1
2
0
2

n
u
J

6
1

]
T
I
.
s
c
[

1
v
0
1
6
8
0
.
6
0
1
2
:
v
i
X
r
a

On the Fragile Rates of Linear Feedback

Coding Schemes of Gaussian Channels with

1

Memory

Charalambos D. Charalambous∗ and Christos Kourtellaris∗ and Themistoklis Charalambous†
∗Department of Electrical and Computer Engineering

University of Cyprus, Cyprus
†Department of Electrical Engineering and Automation, School of Electrical Engineering

Aalto University, Finland

Emails: chadcha@ucy.ac.cy, kourtellaris.christos@ucy.ac.cy, themistoklis.charalambous@aalto.ﬁ

Abstract

In [1] the linear coding scheme is applied, Xt = gt

, t = 2, . . . , n, X1 = g1Θ,
E
−
R, a Gaussian random variable, to derive a lower bound on the feedback rate, for additive

(cid:111)(cid:17)

Θ

Θ

(cid:110)

(cid:16)

1,V0 = v0
−

Y t

with Θ : Ω

→

Gaussian noise (AGN) channels, Yt = Xt + Vt ,t = 1, . . . , n, where Vt is a Gaussian autoregressive (AR)

(cid:12)
(cid:12)
(cid:12)

noise, and κ

[0, ∞) is the total transmitter power. For the unit memory AR noise, with parameters

∈
(c, KW ), where c
[
−
CL,B = 1
2 log χ 2, where χ = limn
gn
gn

∈

−→

1, 1] is the pole and KW is the variance of the Gaussian noise, the lower bound is

∞ χn is the positive root of χ 2 = 1 +

c
1 + |
|χ

2

κ
KW

, and the sequence

1
−

χn (cid:52)=

The conjectured is proved in [2].

, n = 2, 3, . . . , satisﬁes a certain recursion, and conjectured that CL,B is the feedback capacity.
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
In this correspondence, it is observed that the nontrivial lower bound CL,B = 1

2 log χ 2 such that χ > 1,
necessarily implies the scaling coefﬁcients of the feedback code, gn, n = 1, 2, . . ., grow unbounded, in the

(cid:16)

(cid:17)

sense that, limn

∞

= +∞. The unbounded behaviour of gn follows from the ratio limit theorem of

gn
|

|

−→

a sequence of real numbers, and it is veriﬁed by simulations. It is then concluded that such linear codes

are not practical, and fragile with respect to a mismatch between the statistics of the mathematical model

of the channel and the real statistics of the channel. In particular, if the error is perturbed by εn > 0 no
1,V0 = v0
−

matter how small, then Xn = gt

+ gnεn, and

∞, as n

Y t

∞.

Θ

Θ

E

gn
|

εn
|

−→

−→

−

(cid:16)

(cid:110)

(cid:12)
(cid:12)
(cid:12)

(cid:111)(cid:17)

June 17, 2021

DRAFT

 
 
 
 
 
 
2

I. INTRODUCTION, MAIN RESULTS, LITERATURE REVIEW AND OBSERVATIONS

Achievable lower and upper bounds on feedback rates of additive Gaussian (AGN) channels with memory,

driven by autoregressive AR noise, are derived in the early 1970’s, in [1], [3]–[5], using generalizations

of Elias [6], and Schalkwijk and Kailath [7], coding schemes of memoryless AGN channel. Bounds are

also derived in [8], [9], and compared to Butman’s bounds [1]. Variations of the coding schemes [6], [7],

are applied to memoryless AGN channel with feedback, in the context of joint source channel-coding,

using posterior matching feedback schemes in [10]–[12].

In [13], the “maximal information rate” of the AGN channel with unit memory stationary AR noise is

computed (see Corollary 7.1 in [13]), and noted it is identical to Butman’s lower bound. In [2], Butman’s

lower bound is shown to correspond to the feedback capacity of the AGN channel with unit memory

stationary AR noise, while additional generalizations are also obtained for stationary autoregressive

moving average noise.

In this paper, we identify fundamental fragile properties of the linear feedback coding scheme applied in

[1] to derive the lower bound on feedback capacity. To keep our analysis and observations as simple as

possible, our discussion of [1] is focused on AGN channels driven by the simplest noise with memory,

the autoregressive AR unit-memory Gaussian noise. However, our observations are not limited by the

simplicity of the noise.

A. Additive Gaussian Noise Channels Driven by Autoregressive Noise

Bounds on the feedback capacity of AGN channels are derived1 in Tienan’s and Schalkwijk’s 1974 paper

[4], Wolfowitz’s 1975 paper [5] and Butman’s 1976 paper [1], where the authors presuppose the initial

state of the noise is known to the encoder and the decoder2.

Below, we introduce the AGN channel driven by time-varying AR noise, with respect to Butman’s [1]

linear time-varying feedback coding scheme, as shown in Figure I.1. This generalization is considered to

keep our presentation more interesting, and to verify via an alternative derivation, that Butman’s lower

bound on achievable feedback rate, also holds for the more general nonstationary and nonergodic AGN

channels investigated by Cover and Pombra in [14] (even though we show the coefﬁcients of the error

1In [1], [4], [5] the noise is time-invariant, stable or marginally stable.

2 [4, page 311 below the noise model], where z

m+1, . . . , z0 is used, and rest of pages where rates are conditioned on Z m;
−
[5, Section I, second pagagraph], “z0 is the state of the channel at the beginning of transmission; z0 is known to both sender

and receiver”; [1, eqn (17)], where n0 is used.

June 17, 2021

DRAFT

3

of coding scheme grow unbounded).

Butman in [1] considers the restriction ct = c

1, 1], KWt = KW

[
−

(0, ∞),

∈

∈

t (i.e., includes nonasymptoti-

∀

cally stationary noise), while Tienan and Schalkwijk’s, and also Wolfowitz [4], [5] consider the restriction

ct = c

∈

1, 1), KWt = KW

(
−

(0, ∞),

∈

t (i.e., asymptotically stationary noise).

∀

AGN Driven by Time-Varying AR Noise AR(ct; v0).

Yt = Xt +Vt,
n
∑
t=1

1
n

Xt

E

(cid:110)
Vt = ctVt

(cid:0)

(cid:1)

2

−

t = 1, . . . , n,

κ,

≤

[0, ∞),

κ

∈

(cid:111)

V0 = v0
(cid:12)
(cid:12)
(cid:12)

1 +Wt, V0 = v0, ct

Wt

G(0, KWt ), KWt ∈

∈

(0, ∞),

(
−

∞, ∞),

∈
t = 1, . . . , n,

t = 1, . . . , n,

indep. Gaussian seq., indep. of V0

G(0, KV0),

∈

Xt = et(v0, Θ,Y t

1)), et(
) is linear in (v0, Θ,Y t
−
·

1),
−

t = 2, . . . , n,

) is linear in (v0, Θ),
X1 = e1(v0, Θ), e1(
·

Θ : Ω

R is a Gaussian message, Θ

→
t = 1, . . . , n, V0, Θ are mutually independent

∈

Wt,

G(0, KΘ), KΘ > 0,

(I.1)

(I.2)

(I.3)

(I.4)

(I.5)

(I.6)

(I.7)

(I.8)

where the RVs Xt, Yt and Vt are deﬁned as follows.
X n (cid:52)=

X1, X2, . . . , Xn

Y n (cid:52)=

V n (cid:52)=

{
Y1,Y2, . . . ,Yn
{

V1, . . . ,Vn
{

}

is the sequence of channel input random variables (RVs) Xt : Ω

}
is the sequence of channel output RVs Yt : Ω

}
is the sequence of jointly Gaussian distributed RVs Vt : Ω

→

R,

R,

→

R, for ﬁxed V0 = v0,

→

V0 (cid:52)= v0, is the initial state of the channel, i.e., of the noise, known to the encoder and decoder,

G(0, KX ) denotes a Gaussian distribution induced by a Gaussian RV, X : Ω

R, with zero mean and

→

variance KX .

Throughout the paper, we use the following notation.

AR(ct; v0), denotes the time-varying autoregressive unit memory noise V n, with ct

(0, ∞),t = 1, . . . , n, AR(c; vo) denotes its restriction to time-invariant, with ct = c

∈

(
−

(0, ∞),t = 1, . . . , n. The stable AR(c; v0) noise corresponds to c

1, 1).

(
−

∈

A time-varying AR noise without an initial state is denoted by AR(ct), ct

to the case, the RV V0 generates the trivial information, i.e., the sigma

∈
∞, ∞), KWt = KW

∞, ∞), KWt ∈
(
−
∈

(
−

∞, ∞), and corresponds

∈
algebra generated by V0 is

−

.

σ

/0, Ω
}

V0
=
}
{
{
I(Θ;Y n
v0) denotes the mutual information between the Gaussian message Θ and the sequence Y n
|
conditioned on V0 = v0, and I(Θ;Y n) denotes the mutual information between Θ and the sequence Y n.

June 17, 2021

DRAFT

4

Fig. I.1: Mathematical model of the AGN channel driven by a time-varying AR(ct ; v0), ct

∞, ∞), KWt > 0

(
−

t noise,

∀

∈

with Butman’s linear feedback coding scheme. The model in [1] corresponds to the restriction AR(c; v0), KWt = KW

t.

∀

gn, n = 1, 2, . . . is gain sequence multiplying the estimation error.

From the above formulation, follows that, if V0 generates the trivial information, then I(Θ;Y n
I(Θ;Y n); in this case I(Θ;Y n) depends on the distribution of the RV V1, i.e., PV1 and V1 = W1.

V0) =
|

Butman in the 1976 paper [1], considered (I.1)-(I.8), for AR(c; v0), c

1, 1], i.e., ct = c

[
−

t, KΘ = 1,
∀

∈

and derived achievable feedback rates, based on the optimization problem

CL

n (κ, v0) (cid:52)=

Xt =et (v0,Θ,Y t

and its per unit time limit,

sup

) is linear, t=1,...,n: 1
1), et (
·

n E

−

∑n

t=1

Xt

2

V0=v0

(cid:8)

(cid:0)

(cid:1)

(cid:12)
(cid:12)
(cid:12)

CL(κ, v0) = lim
∞
−→

n

CL

n (κ, v0)

1
n

I(Θ;Y n

v0),
|

(I.9)

κ

≤

(cid:9)

(I.10)

),t = 1, . . . , n
provided the supremum and limit exist. In particular, Butman proved that linear strategies et(
·

are given by [1, eqn(17)],

et(v0, Θ,Y t

1) = gt
−

Θ

E

Θ

Y t

1,V0 = v0
−

,

t = 2, . . . , n,

e1(v0, Θ) = g1Θ

(I.11)

−

(cid:16)

(cid:110)

(cid:12)
(cid:12)
(cid:12)

(cid:111)(cid:17)

where g1, g2, . . . , gn is a sequence of nonrandom real numbers. Hence, the supremum in (I.9) is replaced

by the supremum over the sequence g1, g2, . . . , gn that satisﬁes the average power constraint.

Remark I.1. The AGN Channel Driven by Noise without Initial State

The variation of the above AGN channel without an initial state, follows directly from the Tienan

June 17, 2021

DRAFT

<latexit sha1_base64="2FUeNXbfJBwVde6Q5GDYyjFAMio=">AAAB6HicbVBNS8NAEJ34WetX1aOXxSIIQkmkqMeCF48t2A9oQ9lsJ+3azSbsboQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8IBFcG9f9dtbWNza3tgs7xd29/YPD0tFxS8epYthksYhVJ6AaBZfYNNwI7CQKaRQIbAfju5nffkKleSwfzCRBP6JDyUPOqLFS47JfKrsVdw6ySryclCFHvV/66g1ilkYoDRNU667nJsbPqDKcCZwWe6nGhLIxHWLXUkkj1H42P3RKzq0yIGGsbElD5urviYxGWk+iwHZG1Iz0sjcT//O6qQlv/YzLJDUo2WJRmApiYjL7mgy4QmbExBLKFLe3EjaiijJjsynaELzll1dJ66riXVe8RrVcq+ZxFOAUzuACPLiBGtxDHZrAAOEZXuHNeXRenHfnY9G65uQzJ/AHzucPb0eMpw==</latexit>+<latexit sha1_base64="2FUeNXbfJBwVde6Q5GDYyjFAMio=">AAAB6HicbVBNS8NAEJ34WetX1aOXxSIIQkmkqMeCF48t2A9oQ9lsJ+3azSbsboQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8IBFcG9f9dtbWNza3tgs7xd29/YPD0tFxS8epYthksYhVJ6AaBZfYNNwI7CQKaRQIbAfju5nffkKleSwfzCRBP6JDyUPOqLFS47JfKrsVdw6ySryclCFHvV/66g1ilkYoDRNU667nJsbPqDKcCZwWe6nGhLIxHWLXUkkj1H42P3RKzq0yIGGsbElD5urviYxGWk+iwHZG1Iz0sjcT//O6qQlv/YzLJDUo2WJRmApiYjL7mgy4QmbExBLKFLe3EjaiijJjsynaELzll1dJ66riXVe8RrVcq+ZxFOAUzuACPLiBGtxDHZrAAOEZXuHNeXRenHfnY9G65uQzJ/AHzucPb0eMpw==</latexit>+<latexit sha1_base64="9VziXy5Gu0YPvRGa7oF+g65BqFQ=">AAAB7XicbVBNSwMxEJ2tX7V+VT16CRbBi2VXinosePFYwX5Au5Zsmm1js8mSZIW67H/w4kERr/4fb/4b03YP2vpg4PHeDDPzgpgzbVz32ymsrK6tbxQ3S1vbO7t75f2DlpaJIrRJJJeqE2BNORO0aZjhtBMriqOA03Ywvp767UeqNJPizkxi6kd4KFjICDZWaj3dp2de1i9X3Ko7A1omXk4qkKPRL3/1BpIkERWGcKx113Nj46dYGUY4zUq9RNMYkzEe0q6lAkdU++ns2gydWGWAQqlsCYNm6u+JFEdaT6LAdkbYjPSiNxX/87qJCa/8lIk4MVSQ+aIw4chINH0dDZiixPCJJZgoZm9FZIQVJsYGVLIheIsvL5PWedW7qHq3tUq9lsdRhCM4hlPw4BLqcAMNaAKBB3iGV3hzpPPivDsf89aCk88cwh84nz86bY7c</latexit>z 1<latexit sha1_base64="9VziXy5Gu0YPvRGa7oF+g65BqFQ=">AAAB7XicbVBNSwMxEJ2tX7V+VT16CRbBi2VXinosePFYwX5Au5Zsmm1js8mSZIW67H/w4kERr/4fb/4b03YP2vpg4PHeDDPzgpgzbVz32ymsrK6tbxQ3S1vbO7t75f2DlpaJIrRJJJeqE2BNORO0aZjhtBMriqOA03Ywvp767UeqNJPizkxi6kd4KFjICDZWaj3dp2de1i9X3Ko7A1omXk4qkKPRL3/1BpIkERWGcKx113Nj46dYGUY4zUq9RNMYkzEe0q6lAkdU++ns2gydWGWAQqlsCYNm6u+JFEdaT6LAdkbYjPSiNxX/87qJCa/8lIk4MVSQ+aIw4chINH0dDZiixPCJJZgoZm9FZIQVJsYGVLIheIsvL5PWedW7qHq3tUq9lsdRhCM4hlPw4BLqcAMNaAKBB3iGV3hzpPPivDsf89aCk88cwh84nz86bY7c</latexit>z 1<latexit sha1_base64="h3QKvEQOz3m+fU7+9DqQLpZahdQ=">AAAB63icbVDLSgNBEOyNrxhfUY9eBoPgKexKUI8BLx4jmAckS5idzCZD5rHMzAphyS948aCIV3/Im3/jbLIHTSxoKKq66e6KEs6M9f1vr7SxubW9U96t7O0fHB5Vj086RqWa0DZRXOlehA3lTNK2ZZbTXqIpFhGn3Wh6l/vdJ6oNU/LRzhIaCjyWLGYE21wamFQMqzW/7i+A1klQkBoUaA2rX4ORIqmg0hKOjekHfmLDDGvLCKfzyiA1NMFkise076jEgpowW9w6RxdOGaFYaVfSooX6eyLDwpiZiFynwHZiVr1c/M/rpza+DTMmk9RSSZaL4pQjq1D+OBoxTYnlM0cw0czdisgEa0ysi6fiQghWX14nnat6cF0PHhq1ZqOIowxncA6XEMANNOEeWtAGAhN4hld484T34r17H8vWklfMnMIfeJ8/L62OSw==</latexit>X<latexit sha1_base64="PDm4YGeiKt7qwm5T0DIr38Wbgzw=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkVI8FLx4r2g9oQ9lsN+3SzSbsToQS+hO8eFDEq7/Im//GbZuDtj4YeLw3w8y8IJHCoOt+O4WNza3tneJuaW//4PCofHzSNnGqGW+xWMa6G1DDpVC8hQIl7yaa0yiQvBNMbud+54lrI2L1iNOE+xEdKREKRtFKD2yAg3LFrboLkHXi5aQCOZqD8ld/GLM04gqZpMb0PDdBP6MaBZN8VuqnhieUTeiI9yxVNOLGzxanzsiFVYYkjLUthWSh/p7IaGTMNApsZ0RxbFa9ufif10sxvPEzoZIUuWLLRWEqCcZk/jcZCs0ZyqkllGlhbyVsTDVlaNMp2RC81ZfXSfuq6tWr3n2t0qjlcRThDM7hEjy4hgbcQRNawGAEz/AKb450Xpx352PZWnDymVP4A+fzB08AjcY=</latexit>ct<latexit sha1_base64="2FUeNXbfJBwVde6Q5GDYyjFAMio=">AAAB6HicbVBNS8NAEJ34WetX1aOXxSIIQkmkqMeCF48t2A9oQ9lsJ+3azSbsboQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8IBFcG9f9dtbWNza3tgs7xd29/YPD0tFxS8epYthksYhVJ6AaBZfYNNwI7CQKaRQIbAfju5nffkKleSwfzCRBP6JDyUPOqLFS47JfKrsVdw6ySryclCFHvV/66g1ilkYoDRNU667nJsbPqDKcCZwWe6nGhLIxHWLXUkkj1H42P3RKzq0yIGGsbElD5urviYxGWk+iwHZG1Iz0sjcT//O6qQlv/YzLJDUo2WJRmApiYjL7mgy4QmbExBLKFLe3EjaiijJjsynaELzll1dJ66riXVe8RrVcq+ZxFOAUzuACPLiBGtxDHZrAAOEZXuHNeXRenHfnY9G65uQzJ/AHzucPb0eMpw==</latexit>+<latexit sha1_base64="2FUeNXbfJBwVde6Q5GDYyjFAMio=">AAAB6HicbVBNS8NAEJ34WetX1aOXxSIIQkmkqMeCF48t2A9oQ9lsJ+3azSbsboQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8IBFcG9f9dtbWNza3tgs7xd29/YPD0tFxS8epYthksYhVJ6AaBZfYNNwI7CQKaRQIbAfju5nffkKleSwfzCRBP6JDyUPOqLFS47JfKrsVdw6ySryclCFHvV/66g1ilkYoDRNU667nJsbPqDKcCZwWe6nGhLIxHWLXUkkj1H42P3RKzq0yIGGsbElD5urviYxGWk+iwHZG1Iz0sjcT//O6qQlv/YzLJDUo2WJRmApiYjL7mgy4QmbExBLKFLe3EjaiijJjsynaELzll1dJ66riXVe8RrVcq+ZxFOAUzuACPLiBGtxDHZrAAOEZXuHNeXRenHfnY9G65uQzJ/AHzucPb0eMpw==</latexit>+<latexit sha1_base64="4NuQIBzs/a25Cuvt+TItxzhxGUw=">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkqMeCF48V+gVtKJvtpF272YTdjVBC/4MXD4p49f9489+4bXPQ1gcDj/dmmJkXJIJr47rfTmFjc2t7p7hb2ts/ODwqH5+0dZwqhi0Wi1h1A6pRcIktw43AbqKQRoHATjC5m/udJ1Sax7Jppgn6ER1JHnJGjZXa/eYYDR2UK27VXYCsEy8nFcjRGJS/+sOYpRFKwwTVuue5ifEzqgxnAmelfqoxoWxCR9izVNIItZ8trp2RC6sMSRgrW9KQhfp7IqOR1tMosJ0RNWO96s3F/7xeasJbP+MySQ1KtlwUpoKYmMxfJ0OukBkxtYQyxe2thI2poszYgEo2BG/15XXSvqp611XvoVap1/I4inAG53AJHtxAHe6hAS1g8AjP8ApvTuy8OO/Ox7K14OQzp/AHzucPcLmPAA==</latexit>⇥<latexit sha1_base64="csepEzYYbL9y8y3wBFDNIB9Ur2Q=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkVI8FLx4r2g9oQ9lsN+3SzSbsToQS+hO8eFDEq7/Im//GbZuDtj4YeLw3w8y8IJHCoOt+O4WNza3tneJuaW//4PCofHzSNnGqGW+xWMa6G1DDpVC8hQIl7yaa0yiQvBNMbud+54lrI2L1iNOE+xEdKREKRtFKD90BDsoVt+ouQNaJl5MK5GgOyl/9YczSiCtkkhrT89wE/YxqFEzyWamfGp5QNqEj3rNU0YgbP1ucOiMXVhmSMNa2FJKF+nsio5Ex0yiwnRHFsVn15uJ/Xi/F8MbPhEpS5IotF4WpJBiT+d9kKDRnKKeWUKaFvZWwMdWUoU2nZEPwVl9eJ+2rqleveve1SqOWx1GEMziHS/DgGhpwB01oAYMRPMMrvDnSeXHenY9la8HJZ07hD5zPHz4+jbs=</latexit>Xt<latexit sha1_base64="HAdb25fRbk/An9LYveh0IkPGQb8=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkqMeCF48V7Ye0oWy2m3bpZhN2J0IJ/QlePCji1V/kzX/jts1BWx8MPN6bYWZekEhh0HW/ncLa+sbmVnG7tLO7t39QPjxqmTjVjDdZLGPdCajhUijeRIGSdxLNaRRI3g7GNzO//cS1EbF6wEnC/YgOlQgFo2il+8c+9ssVt+rOQVaJl5MK5Gj0y1+9QczSiCtkkhrT9dwE/YxqFEzyaamXGp5QNqZD3rVU0YgbP5ufOiVnVhmQMNa2FJK5+nsio5ExkyiwnRHFkVn2ZuJ/XjfF8NrPhEpS5IotFoWpJBiT2d9kIDRnKCeWUKaFvZWwEdWUoU2nZEPwll9eJa2LqndZ9e5qlXotj6MIJ3AK5+DBFdThFhrQBAZDeIZXeHOk8+K8Ox+L1oKTzxzDHzifPz/Ejbw=</latexit>Yt<latexit sha1_base64="XRlCNEsTmohQHOASj3nc2RCkjEE=">AAAB7nicbVBNS8NAEJ3Ur1q/qh69LBbBiyWRoh4LXjxWsB/ShrLZbtqlm03YnQgl9Ed48aCIV3+PN/+N2zYHbX0w8Hhvhpl5QSKFQdf9dgpr6xubW8Xt0s7u3v5B+fCoZeJUM95ksYx1J6CGS6F4EwVK3kk0p1EgeTsY38789hPXRsTqAScJ9yM6VCIUjKKV2o/9DC+8ab9ccavuHGSVeDmpQI5Gv/zVG8QsjbhCJqkxXc9N0M+oRsEkn5Z6qeEJZWM65F1LFY248bP5uVNyZpUBCWNtSyGZq78nMhoZM4kC2xlRHJllbyb+53VTDG/8TKgkRa7YYlGYSoIxmf1OBkJzhnJiCWVa2FsJG1FNGdqESjYEb/nlVdK6rHpXVe++VqnX8jiKcAKncA4eXEMd7qABTWAwhmd4hTcncV6cd+dj0Vpw8plj+APn8wfgmY86</latexit>Yt 1<latexit sha1_base64="PRUHoyDchvKkpDuQlVY12Bbxaio=">AAAB7nicbVBNS8NAEJ3Ur1q/qh69LBbBiyWRUj0WvHisYD+gDWWz3bRLN5uwOxFK6I/w4kERr/4eb/4bt20O2vpg4PHeDDPzgkQKg6777RQ2Nre2d4q7pb39g8Oj8vFJ28SpZrzFYhnrbkANl0LxFgqUvJtoTqNA8k4wuZv7nSeujYjVI04T7kd0pEQoGEUrddqDDK+82aBccavuAmSdeDmpQI7moPzVH8YsjbhCJqkxPc9N0M+oRsEkn5X6qeEJZRM64j1LFY248bPFuTNyYZUhCWNtSyFZqL8nMhoZM40C2xlRHJtVby7+5/VSDG/9TKgkRa7YclGYSoIxmf9OhkJzhnJqCWVa2FsJG1NNGdqESjYEb/XlddK+rnr1qvdQqzRqeRxFOINzuAQPbqAB99CEFjCYwDO8wpuTOC/Ou/OxbC04+cwp/IHz+QPb+483</latexit>Vt 1<latexit sha1_base64="WYaPU7zwN6h1B8hUzYZArK6G1og=">AAAB7HicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkqMeCF48VTFtoQ9lsN+3SzSbsToQS+hu8eFDEqz/Im//GbZuDtj4YeLw3w8y8MJXCoOt+O6WNza3tnfJuZW//4PCoenzSNkmmGfdZIhPdDanhUijuo0DJu6nmNA4l74STu7nfeeLaiEQ94jTlQUxHSkSCUbSS3x7kOBtUa27dXYCsE68gNSjQGlS/+sOEZTFXyCQ1pue5KQY51SiY5LNKPzM8pWxCR7xnqaIxN0G+OHZGLqwyJFGibSkkC/X3RE5jY6ZxaDtjimOz6s3F/7xehtFtkAuVZsgVWy6KMkkwIfPPyVBozlBOLaFMC3srYWOqKUObT8WG4K2+vE7aV3Xvuu49NGrNRhFHGc7gHC7Bgxtowj20wAcGAp7hFd4c5bw4787HsrXkFDOn8AfO5w//x47F</latexit>Vt<latexit sha1_base64="W9rcVT+TwIEUotdsZeurVVF5jpk=">AAAB7HicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkqMeCF48VTFtoQ9lsN+3SzSbsToQS+hu8eFDEqz/Im//GbZuDtj4YeLw3w8y8MJXCoOt+O6WNza3tnfJuZW//4PCoenzSNkmmGfdZIhPdDanhUijuo0DJu6nmNA4l74STu7nfeeLaiEQ94jTlQUxHSkSCUbSS3xnkOBtUa27dXYCsE68gNSjQGlS/+sOEZTFXyCQ1pue5KQY51SiY5LNKPzM8pWxCR7xnqaIxN0G+OHZGLqwyJFGibSkkC/X3RE5jY6ZxaDtjimOz6s3F/7xehtFtkAuVZsgVWy6KMkkwIfPPyVBozlBOLaFMC3srYWOqKUObT8WG4K2+vE7aV3Xvuu49NGrNRhFHGc7gHC7Bgxtowj20wAcGAp7hFd4c5bw4787HsrXkFDOn8AfO5w8BXo7G</latexit>Wt<latexit sha1_base64="h3QKvEQOz3m+fU7+9DqQLpZahdQ=">AAAB63icbVDLSgNBEOyNrxhfUY9eBoPgKexKUI8BLx4jmAckS5idzCZD5rHMzAphyS948aCIV3/Im3/jbLIHTSxoKKq66e6KEs6M9f1vr7SxubW9U96t7O0fHB5Vj086RqWa0DZRXOlehA3lTNK2ZZbTXqIpFhGn3Wh6l/vdJ6oNU/LRzhIaCjyWLGYE21wamFQMqzW/7i+A1klQkBoUaA2rX4ORIqmg0hKOjekHfmLDDGvLCKfzyiA1NMFkise076jEgpowW9w6RxdOGaFYaVfSooX6eyLDwpiZiFynwHZiVr1c/M/rpza+DTMmk9RSSZaL4pQjq1D+OBoxTYnlM0cw0czdisgEa0ysi6fiQghWX14nnat6cF0PHhq1ZqOIowxncA6XEMANNOEeWtAGAhN4hld484T34r17H8vWklfMnMIfeJ8/L62OSw==</latexit>X<latexit sha1_base64="LZKhXLMiN9OBcs8zrxH2+Nh6J7I=">AAAB6HicbVBNS8NAEJ34WetX1aOXxSJ4sSRS1GPBi8cW7Ae0oWy2k3btZhN2N0IJ/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O2vrG5tb24Wd4u7e/sFh6ei4peNUMWyyWMSqE1CNgktsGm4EdhKFNAoEtoPx3cxvP6HSPJYPZpKgH9Gh5CFn1Fipcdkvld2KOwdZJV5OypCj3i999QYxSyOUhgmqdddzE+NnVBnOBE6LvVRjQtmYDrFrqaQRaj+bHzol51YZkDBWtqQhc/X3REYjrSdRYDsjakZ62ZuJ/3nd1IS3fsZlkhqUbLEoTAUxMZl9TQZcITNiYgllittbCRtRRZmx2RRtCN7yy6ukdVXxriteo1quVfM4CnAKZ3ABHtxADe6hDk1ggPAMr/DmPDovzrvzsWhdc/KZE/gD5/MHck+MqQ==</latexit> <latexit sha1_base64="2FUeNXbfJBwVde6Q5GDYyjFAMio=">AAAB6HicbVBNS8NAEJ34WetX1aOXxSIIQkmkqMeCF48t2A9oQ9lsJ+3azSbsboQS+gu8eFDEqz/Jm//GbZuDtj4YeLw3w8y8IBFcG9f9dtbWNza3tgs7xd29/YPD0tFxS8epYthksYhVJ6AaBZfYNNwI7CQKaRQIbAfju5nffkKleSwfzCRBP6JDyUPOqLFS47JfKrsVdw6ySryclCFHvV/66g1ilkYoDRNU667nJsbPqDKcCZwWe6nGhLIxHWLXUkkj1H42P3RKzq0yIGGsbElD5urviYxGWk+iwHZG1Iz0sjcT//O6qQlv/YzLJDUo2WJRmApiYjL7mgy4QmbExBLKFLe3EjaiijJjsynaELzll1dJ66riXVe8RrVcq+ZxFOAUzuACPLiBGtxDHZrAAOEZXuHNeXRenHfnY9G65uQzJ/AHzucPb0eMpw==</latexit>+<latexit sha1_base64="LTXD+J/7qtfBXhURHEPqGDWxEF4=">AAACE3icbVDLSgMxFM3UV62vqks3wSKIaJmRom6EggguK/QlnTpk0kwbmnmQ3CmUcf7Bjb/ixoUibt24829MHwttPRA4nHMvN+e4keAKTPPbyCwsLi2vZFdza+sbm1v57Z26CmNJWY2GIpRNlygmeMBqwEGwZiQZ8V3BGm7/auQ3BkwqHgZVGEas7ZNuwD1OCWjJyR/ZPoGe6yXXqZ1gu9pjQPBD3THxJR445jG+u0/gxEqxnTr5glk0x8DzxJqSApqi4uS/7E5IY58FQAVRqmWZEbQTIoFTwdKcHSsWEdonXdbSNCA+U+1knCnFB1rpYC+U+gWAx+rvjYT4Sg19V0+OEqhZbyT+57Vi8C7aCQ+iGFhAJ4e8WGAI8agg3OGSURBDTQiVXP8V0x6RhIKuMadLsGYjz5P6adE6K1q3pUK5NK0ji/bQPjpEFjpHZXSDKqiGKHpEz+gVvRlPxovxbnxMRjPGdGcX/YHx+QMvkJxk</latexit>E{⇥|V0=v0,Yt 1}<latexit sha1_base64="Ip07nmB6+WMoB6Ey0i9BII2sA7k=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPBi8eK9gPaUDbbTbp0swm7E6GE/gQvHhTx6i/y5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmltfWNzq7xd2dnd2z+oHh61TZJpxlsskYnuBtRwKRRvoUDJu6nmNA4k7wTj25nfeeLaiEQ94iTlfkwjJULBKFrpIRrgoFpz6+4cZJV4BalBgeag+tUfJiyLuUImqTE9z03Rz6lGwSSfVvqZ4SllYxrxnqWKxtz4+fzUKTmzypCEibalkMzV3xM5jY2ZxIHtjCmOzLI3E//zehmGN34uVJohV2yxKMwkwYTM/iZDoTlDObGEMi3srYSNqKYMbToVG4K3/PIqaV/Uvau6d39Za1wWcZThBE7hHDy4hgbcQRNawCCCZ3iFN0c6L86787FoLTnFzDH8gfP5A1UYjco=</latexit>gt<latexit sha1_base64="h3QKvEQOz3m+fU7+9DqQLpZahdQ=">AAAB63icbVDLSgNBEOyNrxhfUY9eBoPgKexKUI8BLx4jmAckS5idzCZD5rHMzAphyS948aCIV3/Im3/jbLIHTSxoKKq66e6KEs6M9f1vr7SxubW9U96t7O0fHB5Vj086RqWa0DZRXOlehA3lTNK2ZZbTXqIpFhGn3Wh6l/vdJ6oNU/LRzhIaCjyWLGYE21wamFQMqzW/7i+A1klQkBoUaA2rX4ORIqmg0hKOjekHfmLDDGvLCKfzyiA1NMFkise076jEgpowW9w6RxdOGaFYaVfSooX6eyLDwpiZiFynwHZiVr1c/M/rpza+DTMmk9RSSZaL4pQjq1D+OBoxTYnlM0cw0czdisgEa0ysi6fiQghWX14nnat6cF0PHhq1ZqOIowxncA6XEMANNOEeWtAGAhN4hld484T34r17H8vWklfMnMIfeJ8/L62OSw==</latexit>X<latexit sha1_base64="rBvqS2051/f0jJ14zTHqlc/v1Uc=">AAAB7nicbVDLSsNAFL2pr1pfVZdugkVwVZIi6rLgxmUF+4A2lMnkph06mQkzE6GEfoQbF4q49Xvc+TdO2yy09cDA4Zx7mXtOmHKmjed9O6WNza3tnfJuZW//4PCoenzS0TJTFNtUcql6IdHImcC2YYZjL1VIkpBjN5zczf3uEyrNpHg00xSDhIwEixklxkpdFFRGqIbVmlf3FnDXiV+QGhRoDatfg0jSLEFhKCda930vNUFOlGGU46wyyDSmhE7ICPuWCpKgDvLFuTP3wiqRG0tlnzDuQv29kZNE62kS2smEmLFe9ebif14/M/FtkDORZsYGW34UZ9w10p1ndyOmkBo+tYRQxeytLh0TRaixDVVsCf5q5HXSadT967r/0Kg1r4o6ynAG53AJPtxAE+6hBW2gMIFneIU3J3VenHfnYzlacoqdU/gD5/MHb0aPlg==</latexit>encoder<latexit sha1_base64="4wXmS47AcMJtDknjdmPyrkh4RXM=">AAACBnicbZDLSgMxFIYzXmu9jboUIVgEV2WmiLosuNBlBXuBtpQz6WkbmkmGJFMopSs3voobF4q49Rnc+Taml4W2/hD4+M85nJw/SgQ3Ngi+vZXVtfWNzcxWdntnd2/fPzisGJVqhmWmhNK1CAwKLrFsuRVYSzRCHAmsRv2bSb06QG24kg92mGAzhq7kHc7AOqvln0BqlcauRmP4AOktpA5AUqm4wZafC/LBVHQZwjnkyFyllv/VaCuWxigtE2BMPQwS2xyBtpwJHGcbqcEEWB+6WHcoIUbTHE3PGNMz57RpR2n3pKVT9/fECGJjhnHkOmOwPbNYm5j/1eqp7Vw3R1wmqUXJZos6qaBW0UkmtM01MiuGDoBp7v5KWQ80MOuSy7oQwsWTl6FSyIeX+fC+kCtezOPIkGNySs5JSK5IkdyREikTRh7JM3klb96T9+K9ex+z1hVvPnNE/sj7/AHhCJlU</latexit>autoregressiveGaussiannoise<latexit sha1_base64="gWtdU1Tc6NIpUVfgqziYTAzLbNk=">AAAB+3icbVDLSsNAFJ3UV62vWpdugkVwVRIp6rLgxmWFvqAJYTK5aYZOHszcqCX0V9y4UMStP+LOv3H6WGjrgQuHc+7l3nv8THCFlvVtlDY2t7Z3yruVvf2Dw6Pqca2n0lwy6LJUpHLgUwWCJ9BFjgIGmQQa+wL6/vh25vcfQCqeJh2cZODGdJTwkDOKWvKqNeeRBxBRLJxOBEinHnrVutWw5jDXib0kdbJE26t+OUHK8hgSZIIqNbStDN2CSuRMwLTi5AoyysZ0BENNExqDcov57VPzXCuBGaZSV4LmXP09UdBYqUns686YYqRWvZn4nzfMMbxxC55kOULCFovCXJiYmrMgzIBLYCgmmlAmub7VZBGVlKGOq6JDsFdfXie9y4Z91bDvm/VWcxlHmZySM3JBbHJNWuSOtEmXMPJEnskreTOmxovxbnwsWkvGcuaE/IHx+QOFQpS2</latexit>b⇥tand Schalkwijk [4] and Butman [1] formulation, by letting V0 generate the trivial information, i.e.,

5

the sigma

algebra generated by V0 is σ

−

V0
{

}

=

. In such a formulation CL
/0, Ω
}
{

n (κ, v0),CL

n (κ, v0) are

replaced by CL

n (κ, PV1),CL

n (κ, PV1), which emphasize their dependence on the distribution PV1 of V1,

instead of initial state V0 = v0.

[1] is focused on the optimization problem (I.9) and its per unit time limit (I.10), with coding scheme

(I.11), and in particular on the derivation of upper and lower bounds.

It will become apparent (in

subsequent sections) that Butman’s lower bound is derived using a per-symbol average power constraint

at each transmission time, i.e., E

2

Xn

κ, n = 1, 2, . . ., and not 1

n E

∑n

t=1

Xt

2

≤

Over the years, the following result, is used extensively in the literature, such as, [15], [16].

(cid:8)

(cid:0)

(cid:1)

(cid:110)(cid:0)

(cid:1)

V0 = v0
(cid:12)
(cid:12)
(cid:12)

(cid:111)

V0 = v0
(cid:12)
(cid:12)
(cid:12)

κ.

≤

(cid:9)

(R) Butman’s lower bound on CL

n (κ, v0) and CL(κ, v0), and Butman’s Conjecture, that these bounds

correspond to the feedback capacity [1, Abstract].

In particular, [2] proved that Butman’s Conjecture is correct, and the frequency and time-domain charac-

terizations of feedback capacity3 [2, Theorem 4.1 and Theorem 6.1], reproduce Butman’s lower bound

on feedback capacity.

B. Main Results on the Linear Code of [1]

We prove the following conclusion.

(C)

Butman’s [1, Abstract] calculation of the rate limn

∞ CL

n (κ, v0) for the AR(c; v0), c

−→

∞, ∞]

[
−

∈

noise, known as Butman’s lower bound and conjecture on feedback capacity, based on coding

scheme (I.11), corresponds to an unbounded sequence g1, g2, . . ., in the sense that, limn

∞

−→

=

gn
|

|

+∞.

Theorem I.3 presents some of the consequences of (C). To prove (C) we will make use of Theorem I.1

(below), known as the ratio test theorem [17, Theorem 3.34].

Theorem I.1 (The Ratio Test).

Consider any sequence of real numbers

an : n = 1, 2, . . .
}

.

{

(a) Suppose that limn

∞

−→

an+1/an
|

|

= L. If L < 1, then the series ∑∞

n=0 an converges absolutely, if L > 1

the series diverges, and if L = 1 this test gives no information.

(b) If limn

∞

an+1/an
|

|

−→

> 1, then limn

∞

an
|

|

= +∞; if limn

∞

−→

an+1/an
|

|

< 1, then limn

∞

−→

an
|

|

= 0.

−→

3These theorems are used in [2] to obtain Butman’s lower bound, and to validate the Conjecture.

June 17, 2021

DRAFT

Further, as mentioned earlier, to provide additional insight, we present an alternative derivation of

Butman’s lower bound CL

n (κ, v0) for any n = 1, 2, . . ., as stated in Theorem I.2 (below), which is also

valid for the AR(ct; v0), ct

∞, ∞) noise.

(
−

∈

6

Theorem I.2 (Characterizations of CL

n (κ, v0) and CL
Consider the AGN channel deﬁned by (I.1)-(I.8), i.e., with AR(ct; v0) or AR(ct), for any ct

n (κ, PV1)).

∞, ∞),t =

(
−

∈

0, 1, . . ..

(a) Total Average Power Constraint. The maximization over all linear coding schemes, (I.11) (without

initial state, i.e., for AR(ct) noise) of mutual information I(Θ;Y n), subject to total average power,
1
n E

κ, is given by

∑n

t=1

Xt

2

≤

(cid:8)

(cid:0)

(cid:9)

(cid:1)
CL
n (κ, PV1) (cid:52)=

gt , t=1,...,n: 1
n

sup
1KΘ+∑n
g2

1
2

log

(cid:110)

(cid:16)

g2
1KΘ + KV1
KV1

(cid:17)

1
−

κ

≤

+

n
∑
t=2

log

ct

−

(cid:8)
1

(cid:16)

(cid:16)

t Σt

t=2 g2
2

(cid:17)
KWt

gt
1
−
gt

(cid:9)
g2
1 + KWt
t Σt
−

,

(cid:17)(cid:111)

subject to Σt,t = 1, . . . , n that satisﬁes the recursion and controlled by g1, . . . , gn,

Σt =

Σ1 =

KWt Σt
1
−
2
Σt

1
−

gt

ctgt

−
(cid:16)
KΘKV1
g2
1KΘ + KV1

(cid:17)
.

,

t = 2, . . . , n,

1 + KWt
−

(I.12)

(I.13)

(I.14)

(b) Pointwise Average Power Constraint. The maximization over all linear coding schemes, (I.11) (for

AR(ct) noise) of mutual information I(Θ;Y n), subject to pointwise average power, E

1, . . . , n, is given by

CL

n (κ1, . . . , κn, PV1) (cid:52)=

gt , t=1,...,n: g2

1KΘ≤

sup
κ1, g2

t Σt

1
2

log

(cid:110)

(cid:16)

g2
1KΘ + KV1
KV1

(cid:17)

κt , t=2,...,n

ct

gt
1
−
gt

−

+

n
∑
t=2

log

1

(cid:16)

(cid:16)

1≤
−
2
g2
t Σt

(cid:17)
KWt

1 + KWt
−

,

(cid:17)(cid:111)

2

Xt

κt,t =

≤

(cid:8)(cid:0)

(cid:1)

(cid:9)

(I.15)

subject to Σt,t = 1, . . . , n that satisﬁes recursion (I.13), (I.14) and controlled by g1, . . . , gn.

(I.16)

(c) The statements of parts (a) and (b) hold, when the noise is replaced by AR(ct; v0), with V0 = v0 the ini-
tial state known to the encoder, and with KV1 replaced by KW1, and CL
CL
n (κ1, . . . , κn, v0).

n (κ, PV1) = CL

n (κ, v0),CL

n (κ1, . . . , κn, PV1) =

Proof. The proof is given in Section II.

June 17, 2021

DRAFT

In Section I-C, we show that Butman’s lower bound of the AR unit memory noise, corresponds to

7

parameters KWt = KW , ct = c
obtained from Theorem I.2.(c), i.e., CL

[
−

∈

1, 1],t = 1, 2 . . ., Gaussian message Θ

G(0, 1), i.e, KΘ = 1, and it is

∈

n (κ1, . . . , κn, v0), by invoking Butman’s strategy [1, 4 lines below

eqn(19)], sgn(gn) =

sgn(cgn

−

1), n = 2, 3.

−

C. Observations on the Lower Bound on Feedback Capacity of [1], [5]

Now, we turn our attention to Butman’s assumptions, formulation, and the main steps of the derivation

of the lower bound in [1], to verify our observation and claims. It is noted that Butman’s lower bound is

also derived by Wolfowitz [5], by transmitting one of enR messages and maximizing the operational rate

R (i.e., without using information theoretic measures of mutual information etc); rather, by applying the

operational deﬁnition of achievable rates.

Below, we make an observation which is easy to verify.

Observation I.1. By evaluating CL

n (κ1, . . . , κn, v0), described in Theorem I.2.(c), for the parameters KWt =

KW , ct = c

∈
lines below eqn(19)],

[
−

1, 1],t = 1, 2 . . ., Gaussian message Θ

G(0, 1), i.e, KΘ = 1, at Butman’s strategy [1, 4

∈

sgn(gn) =

sgn(cgn

−

1), n = 2, 3,

−

implies

n
∑
j=2

g j

cg j

1
−

−

2

=

n
∑
j=2

(g j)2

1 +

(cid:16)

g j
1
−
g j

c
|

||

2

|
(cid:17)

,

(I.17)

(cid:0)
then by simple algebra we obtain Butman’s and Wolfowitz’s lower bound (this is also veriﬁed in the

(cid:1)

sequel).

Fact 1. Butman [1] and also Wolfowitz [5], considered the AGN channel driven by the AR(c), c

noise, i.e., Wt

G(0, KW )

∈

t, and applied the linear time-varying coding strategy4 of a Gaussian message
∀

G(0, 1), i.e., zero mean and unit variance KΘ = 1, to obtain5

Θ

∈

Xn = gn

Θ

E

Θ

Y n

1,V0 = v0
−

,

n = 2, 3, . . . ,

X1 = g1Θ,

1
n

E

(cid:16)
n
∑
t=1

(cid:110)
Σt (cid:52)= E

−
2

Xt

(cid:0)

Θ

(cid:12)
(cid:110)
(cid:12)
V0 = v0
(cid:12)
(cid:12)
(cid:12)
(cid:12)
E
−

Θ

(cid:1)

(cid:111)

Σ1 =

(cid:110)

(cid:110)(cid:16)
KW
g2
1 + KW

=

1
n

n
∑
t=1

(cid:111)(cid:17)
κt

≤

2

(cid:111)(cid:17)

(cid:111)

Y t,V0 = v0
(cid:12)
(cid:12)
(cid:12)

κ,

=

t = 2, . . . , n, κ1 (cid:52)= g2
1,

κt (cid:52)= g2

t Σt

1,

−

KW

1 + ∑t
g2

j=2

g j

cg j

1
−

−

2

+ KW

(cid:0)

(cid:1)

,

t = 2, . . . , n,

(I.20)

(I.21)

1, 1]

[
−

∈

(I.18)

(I.19)

4A coding strategy is called time-varying coding strategy if the strategy gn that is used to generate Xn is not ﬁxed for all

n = 1, 2, . . ..

5The derivation of Σt is given in Corollary II.1, and Theorem II.1 if no initial state V0 = v0 is assumed.

June 17, 2021

DRAFT

Fact 2. [1, 4 lines below eqn(19)], restricted the strategy gn to the one that increases the signal-to-noise

1+∑t
g2

j=2

ratio,

sgn(gn) =

2

cg j

−

1
−

(cid:1)
sgn(cgn

g j
KW
(cid:0)

−

, as follows.

1),

−

n = 2, 3, . . . =

⇒

n
∑
j=2

Fact 3. [1, eqn(26)–eqn(28)] evaluated I(Θ;Y n

g j

cg j

1
−

−

2

=

(cid:0)

(cid:1)

n
∑
j=2

(g j)2

1 +

(cid:16)

g j
1
−
g j

c
|

||

2

|
(cid:17)

.

(I.22)

v0), by applying the linear coding strategy (I.22) as follows.
|

I(Θ;Y n

v0) =
|

=

1
2

1
2

(cid:110)

gt

log

+

1
2

log

1 +

1 +

κ1
KW

n
∑
t=2
n
∑
t=2
1) and satisﬁes the recursion [1, eqn(23)],

c
1 + |
|
χt

κ1
KW

1 +

1 +

log

cgt

1
−

1
2

+

−

log

(cid:110)

(cid:111)

(cid:110)

(cid:111)

(cid:110)

(cid:16)

(cid:17)

(cid:16)

(cid:17)
2 κt
KW

(cid:111)

2 Σt

1
−
KW

(cid:111)
by (I.22)

where χn is related to (gn, gn

−

gn
gn

1
−
1 +

(cid:12)
(cid:12)
(cid:12)

χn (cid:52)=

χ 2
n =

(cid:110)

(cid:12)
(cid:12)
(cid:12)

(cid:16)

,

n = 3, 4, . . . ,

χ2 = 1 +

κ1
KW

,

c
1 + |
χn

|

2 κn

1
−
KW

1
−

(cid:17)

(cid:111)

κn
κn

1
−

, n = 3, 4, . . .

Fact 4. Butman’s resulting optimization problem, from Fact 3, reduces to

(B1) :

1
n

CL,B1
n

(κ, v0) (cid:52)= sup
g1,...,gn

subject to

n
∑
t=1

κt

≤

1
2n

log

1 +

κ1
KW

+

1
2n

n
∑
t=2

log

1 +

(cid:110)

(cid:110)
κ, κt (cid:52)= g2
t Σt

1,

−

(cid:16)
(cid:111)
t = 2, . . . , n, κ1 = g2
1,

(cid:110)

c
1 + |
|
χt

2 κt
KW

(cid:17)

(cid:111)(cid:111)

χt satisﬁes recursion (I.25), (I.26),

Σt =

Σ1 =

1 + ∑t
g2
KW
g2
1 + KW

KW

j=2(g j)2

c
1 + |
|χ j

2

+ KW

(cid:0)

(cid:1)

.

,

t = 2, . . . , n

by (I.22),

8

(I.23)

(I.24)

(I.25)

(I.26)

(I.27)

(I.28)

(I.29)

(I.30)

(I.31)

Notice that in (B1) one is asked to optimize over

g1, g2, . . . , gn
{

}

subject to constraints, or over

κ1, κ2, . . . , κn
{

.
}

Fact 5. [1] did not provide a solution to CL,B1

n

(κ, v0); instead the statement of Butman’s conjecture [1,

Abstract] on feedback capacity is based on a variation, based on Assumptions I.1 (below).

Assumptions I.1.

[1] Average point-wise power constraints6

Butman’s lower bound on feedback capacity and conjecture [1, Abstract], is based on the single-letter

average power at the transmitter (easily veriﬁed from [1]), given by

E

Xt

2

(cid:110)(cid:0)

(cid:1)

V0 = v0
(cid:12)
(cid:12)
(cid:12)

(cid:111)

where Σt,t = 1, . . . , n satisﬁes (I.30) and (I.31).

= κ,

t = 1, 2, . . . , n =

⇒

κt (cid:52)= g2

t Σt

1 = κ,

−

t = 2, . . . , n, κ1 (cid:52)= g2

1 = κ

(I.32)

6Wolfowitz’s [5] derivation is also based on (I.32).

June 17, 2021

DRAFT

In view of Assumptions I.1, Butman’s lower bound [1, see transition from eqn(23) to eqn(23a) or eqn(28)

and paragraph above it], is given as follows7.

9

(B2) :

CL,B2
n

(κ, v0) =

log

1 +

1
2n

1
n

(cid:110)
χt satisﬁes recursion χ 2
t = 1 +

κ
KW

n
∑
t=2
2

+

1
2n

(cid:111)
c
1 + |
|χn
1
−

(cid:17)

(cid:16)

log

1 +

c
1 + |
|
χt

2 κ
KW

(cid:110)
,

κ
KW

(cid:16)

(cid:17)
t = 3, 4, . . . , χ2 = 1 + κ
KW

(cid:111)

,

g2
t Σt

1 = κ,

−

t = 2, . . . , n, g2

1 = κ, Σt,t = 1, . . . , n satisﬁes (I.30), (I.31).

Unlike (B1), in (B2) there is no optimization over

g1, g2, . . . , gn

or

{

}

{

κ1, κ2, . . . , κn

.
}

Fact 6. Butman’s lower bound [1, Abstract, with m = 1 or eqn(4), eqn(5), eqn(11)] Butman [1, eqn(23a)

and paragraph above eqn(28)], which is based on Assumptions I.1 and strategy (I.22), is stated as follows.

(B) : CL,B(κ) (cid:52)= lim
∞
−→

n

1
n

CL,B2
n

(κ, v0) =

1
2

log χ 2,

where

χ = lim
n
∞
−→

χn = lim
n
∞
−→

(cid:12)
(cid:12)
χ is the positive root of χ 4
(cid:12)

(cid:12)
(cid:12)
(cid:12)

1
−

,

gn
gn

χ 2

κ
KW

−

−

Butman [1] conjectured that CL,B(κ) is the feedback capacity.

χ +

c
|

= 0,

c
|

| ≤

1, KW > 0, κ

0,

≥

2

|
(cid:17)

(cid:16)

(I.33)

(I.34)

(I.35)

(I.36)

(I.37)

(I.38)

Remark I.2. A comparison of CL,B(κ) to an upper bound derived by Tienan and Schalkwijk [4, Section I]

is discussed in [1, eqn(1) and eqn(2)]. Ozarow [9] and Dembo [8] re-visited Butman’s lower bound and

derived upper bounds on feedback rates.

To complete our observations on Butman’s problems we note two more facts.

Fact 7. If we do not impose Butman’s restriction that strategies gn satisfy sgn(gn) =

sgn(cgn

−

1), n =

−

2, 3, . . ., i.e., (I.22) is not assumed, then the optimization problem (B1) and statement (B2), are replaced

by (P1) and (P2), respectively, given below.

(P1) :

1
n

CL,P1
n

(κ, v0) (cid:52)= sup
g1,...,gn

1
2n

log

1 +

κ1
KW

+

1
2n

n
∑
t=2

log

1 +

gt

cgt

1
−

−

2 Σt

1
−
KW

(cid:17)

(cid:111)(cid:111)

(cid:110)
(cid:110)
κ, κt (cid:52)= g2

t Σt

1,

−

(cid:16)
(cid:111)
t = 2, . . . , n, κ1 = g2
1,

(cid:110)

subject to

Σt =

1
n

n
∑
t=1

κt

≤
KW

1 + ∑t
g2

j=2

g j

cg j

1
−

−

2

+ KW

(cid:0)

(cid:1)

,

t = 2, . . . , n,

7There is no optimization over g1, g2, . . . because Assumptions I.1 imply χ1, χ2, . . . does not depend on g1, g2, . . ..

June 17, 2021

(I.39)

(I.40)

(I.41)

DRAFT

Σ1 =

KW
g2
1 + KW

,

(P2) :

1
n

CL,P2
n

(κ, v0) (cid:52)= sup
g1,...,gn

1
2n

log

1 +

κ1
KW

+

1
2n

(cid:110)
1 = κ,

(cid:110)

(cid:111)
t = 2, . . . , n, g2
1 = κ,

subject to g2

t Σt

−

Σt,t = 1, . . . , n satisﬁes (I.41), (I.42).

n
∑
t=2

log

1 +

gt

(cid:110)

(cid:16)

cgt

1
−

−

2 Σt

1
−
KW

(cid:17)

10

(I.42)

(I.43)

(I.44)

(I.45)

(cid:111)(cid:111)

Notice (P1), (P2) are optimization problems, where

{
In Section I-D we compare the numerical solutions of (B2) and (B) to (P2).

}

g1, g2, . . . , gn
{

control

Σ1, Σ2, . . . , Σn

.
}

The next theorem, is an application of the ratio test theorem to Butman’s lower bound CL,B(κ) given by

(I.36)-(I.37).

Theorem I.3. On Butman’s lower bound

Consider the lower bound [1], CL,B(κ) = 1

If limn

If limn

∞

−→

∞

−→

gn/gn
|
gn/gn
|

1
−

1
−

|

|

= χ > 1, then limn

= χ < 1, then limn

2 log χ 2 given by (I.36)-(I.38).
gn
|
gn
|

= +∞.
= 0, and CL,B(κ) = 1

|

|

∞

∞

−→

−→

2 log max

1, χ 2
{

}

= 0

κ

∀

∈

[0, ∞).

Proof. Suppose Butman’s sequence

g1, g2, . . . , gn

{

, corresponding to (I.37), (I.38), i.e., computed using

}

, such that χ is is the real positive root of the quadric equation (I.38),

χ = limn

with

χ

|

|

∞ χn = limn

gn
gn
−→
(cid:12)
> 1. Then by Theorem I.1.(b), sequence
(cid:12)
(cid:12)

−→

1
−

(cid:12)
(cid:12)
(cid:12)

∞

the other hand, Theorem I.1.(b), if

χ

< 1 then

|
using mutual information which takes values in [0, ∞] then the claim holds.

−→

|

g1, g2, . . . , gn
{
gn
|

}
0, as n

| −→

, is such that,

gn
|

| −→

∞, as n

∞. On

−→

∞, and since CL,B(κ) is computed

This implies, Butman ’s lower bound is achieved by an unbounded sequence of coding gains

g1
|

,
|

g2
|

, . . ..
|

Therefore, such coding schemes are not practical, and moreover they are fragile, at least for transmission

of moderate duration n. By fragile, we mean, any mismatch of the estimation error

due to any additional external noise in the Gaussian channel not accounted for,

at any given time of transmission, will be ampliﬁed by the scaling gn (see abstract).

(cid:111)

intervals

E

Θ

−

1, 2, . . . , n
{
}
Y n
Θ

1,V0 = v0
−

(cid:110)

(cid:12)
(cid:12)
(cid:12)

Further to the above, it should be apparent that optimization problem (B1), which uses a total average

power constraint, i.e., (I.27)-(I.29), is also achieved by an unbounded sequence

g1
|

,
|

g2
|

, . . ..
|

Remark I.3. From the above follows that convergence of the sequence χn, n = 1, 2, . . . does not imply

convergence of the sequence

gn
|

, n = 1, 2, . . .. In fact, if
|

gn
|

, n = 1, 2, . . . converges to
|

g
|

|

then necessarily

June 17, 2021

DRAFT

χn, n = 1, 2, . . . convergences to χ = 1, and the value of the lower bound, is 1

2 log χ 2 = 0.

11

Fig. I.2: Numerical example with which it is shown that Butman’s strategy gt ,t = 1, 2, . . . , shown in solid line,

as well as the numerical evaluation of (P2), shown in dashed line, for n = 10 have values of gt ,t = 1, . . . , n that

are similar and grow unbounded. The small differences, however, lead to the gap in the solution, as shown in

Fig. I.3, further supporting our claim about the fragility of Butman’s strategy g1, g2, . . .. For the simulations, we

used KΘ = KW = KV = 1, c = 0.5. The values of the sequence

g1
|

,
|

g2
|

, . . . ,
|

gn
|

|

grows unbounded, as predicted by

Theorem I.3.

D. Simulations of Butman’s Coding Scheme

Figure I.3 is a comparison between the numerical evaluation of (B2), (B) and (P2). Notice that (B2)

and (B) are derived by using Butman’s strategy (I.22). For (B2), once χt,t = 1, 2, . . . , n is computed then

g2
t Σt

1 = κ,t = 2, . . . , n, g2
−

1 = κ, and gt,t = 1, . . . , n is found from Σt,t = 1, . . . , n that satisﬁes (I.30), (I.31)

While the values of the sequence g1, g2, . . . , gn are similar, and their absolute values grow unbounded,

as it is shown in Fig. I.2, the numerical evaluation of optimization problem (P2) for n = 10 performs

better than Butman’s lower bound (B2) (which is based on Butman’s strategy (I.22)), even for n = 20.

Therefore, for ﬁnite n, simulations show that Butman’s strategy (I.22) leading to (B2), is not optimal.

June 17, 2021

DRAFT

12345678910-2.5-2-1.5-1-0.500.511067.97.9588.058.18.158.2-12-10-8-6-4-20210412

Note that for values of n > 10, the numerical optimization problem (P2) is difﬁcult to complete, because

the sequence

gn
|

|

grows unbounded, and the numerical optimization does not converge for the maximum

number of iterations considered. Similarly, it is difﬁcult to determine gt,t = 1, 2, . . . , n of (B2) for large

n. Nevertheless, the main point that Butman’s strategy (I.22) is not optimal for ﬁnite n, can be inferred

from the simulations of (P2) for n = 10.

Fig. I.3: Numerical comparisons of rates based on (B2), (P2) and asymptotic limit, called Batman’s lower bound

based on (B). Problem (P2) is solved using standard MATLAB optimization functions (for n = 10). For the

simulations, we used KΘ = KW = KV = 1, c = 0.5. The asymptotic for Butman’s strategy seems to be optimal

as n

→

∞, but it is not optimal for a ﬁnite n.

On the other hand, the calculation of asymptotic limit based on (B), gives a value which is higher

1

than (B2) calculated for n = 10 and n = 20. This is expected, because the rate based on (B2), i.e.,
nCL,B2
nondecreasing with n, but it was not possible to compute it for large values of n, because i)

(κ, v0), is nondecreasing with n. On other hand, the rate based on (P2), i.e., 1

(κ, v0), is also

nCL,P2

n

n

g1
|

,
|

g2
|

, . . .
|

grows unbounded, and it is difﬁcult to compute it numerically, even for moderate values of n beyond 10,

and ii) an analytic expression of (P2) is not available.

Observation I.2. On Butman’s strategy (I.22)

Simulations show that Butman’s strategy sgn(gn) =

sgn(cgn

1), n = 2, 3, . . ., i.e., which is used to obtain

−

−

June 17, 2021

DRAFT

24681012140.60.811.21.41.613

the recursion χn, n = 1, 2, . . . is not optimal for ﬁnite n, since the numerical optimization problem (P2)

produces another sequence gn with higher value of rate (see Fig. I.3). These simulations highlight the

severe limitations of Butman’s scheme, since it is sub-optimal for small number of transmissions n, and

highly impractical for large number of transmissions n, because

g1
|

,
|

g2
|

, . . . grows unbounded.
|

II. INDEPENDENT DERIVATION OF BUTMAN’S LOWER BOUND AND ADDITIONAL DISCUSSION

In this section we provide the derivation of Theorem I.2, and then we show how to recover, as degenerate

cases, Butman’s lower bounds.

We consider the AGN channel deﬁned by (I.1)- (I.8), without an initial state, and we derive the charac-

terization of CL

n (κ, PV1), with coding scheme deﬁned by (I.9) and (I.11), respectively, (without an initial

state). The analysis below, shows that, Butman’s equations can be obtained for any time-varying noise

with ct

∞, ∞).

(
−

∈

Theorem II.1. Preliminary characterization of CL

n (κ, PV1)

Consider the AGN, driven by a time-varying AR(ct),t

∞, ∞) noise, without initial state.

(
−

∈

Deﬁne the conditional mean and error covariance by

Θt (cid:52)=E

Θ

Y t

,

Σt (cid:52)= E

(cid:12)
(cid:12)
Consider the linear coding scheme
(cid:12)

(cid:110)

(cid:98)

(cid:111)

(cid:110)(cid:16)

Θt

Θ

−

(cid:98)

2

Y t

,

t = 1, . . . , n.

(cid:111)

(cid:17)

(cid:12)
(cid:12)
(cid:12)

Xt =gt

Θ

Yt =gt

=gt

(cid:16)

Θ

(cid:16)

Θ

Θt

1
−

−

Θt
(cid:98)

1
−

−

Θt
(cid:98)

1
−

−

(cid:16)
Y1 =g1Θ +V1,
(cid:98)
n
∑
t=1

Xt

E

2

,

t = 2, . . . , n, X1 = g1Θ,

(cid:17)

(cid:17)

(cid:17)

+Vt,

t = 2, . . . , n,

ctgt

1
−

−

Θ

(cid:16)

Θt

2
−

−

(cid:98)

(cid:17)

+ cYt

1 +Wt

−

by Vt

1 = Yt

−

1
−

−

Xt

1
−

=

1
n

n
∑
t=1

κt

κ,

≤

(cid:9)

κt = g2

t Σt

1,

−

t = 2, . . . , n,

κ1 = g2

1KΘ.

(cid:0)
Then the following hold.

(cid:8)

(cid:1)

(II.46)

(II.47)

(II.48)

(II.49)

(II.50)

(II.51)

(a) The innovations process of Y n denoted by In is an orthogonal Gaussian process, It

G(0, KIt ),t = 1, . . .,

∈

given by

It (cid:52)=Yt

−

=

gt

(cid:16)

E

Yt

Y t

1
−

,

t = 2, . . . , n

(cid:110)
ctgt

(cid:12)
(cid:12)
1
(cid:12)
−

−

(cid:111)
Θ

(cid:17)(cid:16)

Θt

1
−

−

(cid:98)

(cid:17)

+Wt,

I1 =g1Θ +V1.

(II.52)

(II.53)

(II.54)

DRAFT

June 17, 2021

where

It
(cid:110)(cid:0)
gt
−

KIt (cid:52)=E

=

(cid:16)
(cid:52)=E

KI1

2

=

gt

(cid:17)
(cid:111)
ctgt

(cid:16)
2

Σt

1
−
(cid:17)
= g2

2

I1

1KΘ + KV1

2

E

(cid:17)

(cid:110)(cid:16)

Θ

Θt

1
−

−

(cid:98)

2

(cid:9)(cid:17)

(cid:111)

ctgt

1
−

−

1 + KWt
−

+ KWt ,

t = 2, 3, . . . ,

(cid:110)(cid:0)
and where the mean-square error Σt and estimate

(cid:111)

(cid:17)

Θt satisfy the recursions

Σt =

Σ1 =

KWt Σt
1
−
2
Σt

1
−

gt

ctgt

−
(cid:16)
KΘKV1
g2
1KΘ + KV1

(cid:17)
,

t = 2, . . . , n,

,

(cid:98)
1 + KWt
−

Θt =

Θt

1 +

−

gt

gt

ctgt

Σt

−
ctgt

(cid:16)
−

1
−

1
−
2

(cid:17)
Σt

1 + KWt
−

1
−

It,

t = 2, . . . , n,

(cid:16)
g1KΘ
g2
1KΘ + KV1
The mutual information between the Gaussian RV Θ and Y n, is given by

I1.

(cid:17)

(cid:98)

(cid:98)

(cid:98)
Θ1 =

I(Θ;Y n) =H(Y n)

H(V n)

−

=

n
∑
t=1

H(It)

−

H(V1)

n
∑
t=2

−

H(Wt)

=

1
2

log

(cid:16)

g2
1KΘ + KV1
KV1

(cid:17)

+

1
2

n
∑
t=2

log

κ1 + KV1
KV1
(b) Suppose KWt = KW ,t = 2, . . . , n. Then

log

1
2

=

(cid:16)

+

1
2

n
∑
t=2

log

(cid:17)

(cid:16)

(cid:16)

ctgt

−

gt

(cid:16)

2

1
−
(cid:17)
KWt

Σt

1 + KWt
−

ct

−

2

gt
1
−
gt
(cid:17)
KWt

κt + KWt

.

(cid:17)

(cid:17)

(cid:16)
1

14

(II.55)

(II.56)

(II.57)

(II.58)

(II.59)

(II.60)

(II.61)

(II.62)

(II.63)

(II.64)

(II.65)

Σt =

Σ1 =

E

(cid:110)
κt =

κ1 =

June 17, 2021

KW KΘKV1
c jg j
g j

1
−

−

j=2

(cid:0)

1
n

n
∑
t=1

κt

κ,

≤

1KW KΘ + ∑t
g2
KΘKV1
g2
1KΘ + KV1
n
∑
t=1

Xt

=

,

2

(cid:0)

(cid:1)

(cid:111)

2KΘKV1 + KW KV1
(cid:1)

,

t = 2, . . . , n,

(II.66)

(II.67)

(II.68)

1
−
j=2

1KW KΘ + ∑t
g2
g2
1KΘKV1
g2
1KΘ + KV1

g2
t KW KΘKV1

c jg j

1
−

−

g j

(cid:16)

2

(cid:17)

KΘKV1 + KW KV1

,

t = 2, . . . , n,

(II.69)

(II.70)

DRAFT

I(Θ;Y n) =

=

1
2

1
2

log

log

(cid:16)

(cid:16)

g2
1KΘ + KV1
KV1
g2
1KΘ + KV1
KV1

+

+

1
2

1
2

n
∑
t=2
n
∑
t=2

(cid:17)

(cid:17)

log

log

−

1
(cid:16)(cid:0)
χt+1

(cid:16)

ct

gt
1
−
gt

2

κt
KW KΘKV1

(cid:1)

κt
κt+1

(cid:17)

+ 1

(cid:17)

where χt (cid:52)= gt
gt

1
−

, and

χt+1 =

1 +

1

(cid:110)

(cid:16)

1
χt

ct

−

2

(cid:17)

κt
KWt KΘKV1

(cid:111)

κt+1
κt

,

t = 2, . . . , n, χ2 =

g2
g1

.

Proof. (a) By

E

Yt

Y t

1
−

(cid:110)

(cid:12)
(cid:12)
(cid:12)

(cid:111)

=

=

−

−

ctgt

Θt

1
−

1 + ctgt

−

−

1E

Θt

2
−

Y t

1
−

+ ctYt

1
−

ctgt

Θt
(cid:98)

1
−

1 + ctgt

−

1
−

(cid:110)
Θt

(cid:12)
(cid:12)
2 + ctYt
(cid:98)
(cid:12)
−

(cid:111)

1
−

15

(II.71)

(II.72)

(II.73)

(II.74)

(II.75)

and substituting into It we obtain (II.52), (II.54). Then (II.55), (II.57) are directly obtained from the

(cid:98)

(cid:98)

independence of Θ and Wt,t = 2, . . . , n,V1. By mean-square estimation theory, the conditional covariance
of Θ given Y t, denoted by cov(Θ, Θ

cov(Θ,Yt

Y t

cov(Yt,Yt

Y t

(II.76)

,

t = 2, . . . , n,

(II.77)

Y t) is,
(cid:12)
(cid:12)
1)
(cid:12)
−
−

1)
−

− (cid:16)
gt

(cid:16)

(cid:16)

2

1)
−
(cid:17)
2

1
−

1)
−
(cid:17)
2

(cid:12)
(cid:12)
ctgt
(cid:12)

1
−

gt

−

−
ctgt

1
−
2

(cid:16)
cov(Θ, Θ

(cid:12)
(cid:12)
Y t
(cid:12)

1)
−

(cid:17)

(cid:16)
cov(Θ, Θ

(cid:12)
Y t
(cid:12)
−
(cid:12)

1)

(cid:17)
+ KWt

(cid:17)

(cid:16)

(cid:17)

(cid:12)
(cid:12)
(cid:12)

cov(Θ, Θ

Y t

Y t) =cov(Θ, Θ
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
Y t
=cov(Θ, Θ

(cid:12)
(cid:12)
(cid:12)
KΘKV1
g2
1KΘ + KV1

.

cov(Θ, Θ

Y1) =
(cid:12)
(cid:12)
(cid:12)

From the above follows, cov(Θ, Θ

2

Θt

= Σt. Hence, (II.78), reduces to

Σt =Σt

,

t = 2, . . . , n,

Θ

−

Y t) = E
(cid:12)
(cid:12)
(cid:12)
1
−

− (cid:16)
gt
−

(cid:110)(cid:16)
gt

−
ctgt

(cid:17)
(cid:98)
ctgt
1
−
2

(cid:17)
Σt

1
−

(cid:17)

(cid:111)
2
Σ2
t
1
−
1 + KWt
−

Σ1 =

(cid:16)
KΘKV1
g2
1KΘ + KV1

.

(II.78)

(II.79)

(II.80)

From (II.79), (II.80) then follows (II.58), (II.59). The conditional mean of of Θ given Y t is,

E

Θ

Y t

= E

Θ

Y t

1
−

+

cov(Θ,Yt

Y t

1)
−

cov(Yt,Yt

Y t

(cid:111)

(cid:110)

(cid:12)
(cid:12)
(cid:12)

(cid:111)

(cid:16)

= E

Θ

+ cov(Θ,Y1)

(cid:12)
(cid:12)
(cid:12)
cov(Y1,Y1)

(cid:17)(cid:16)

1
−

(cid:111)

(cid:110)

(cid:111)

(cid:16)

(cid:17)

(cid:110)

E

Θ

(cid:110)

(cid:12)
(cid:12)
(cid:12)
Yt
(cid:12)
(cid:12)
(cid:12)

June 17, 2021

1
−

1)
−
(cid:17)

Yt

(cid:16)

−

(cid:12)
(cid:12)
(cid:12)

Y1
(cid:16)

−

E

Y1

.

(cid:110)

(cid:111)(cid:17)

E

Yt

Y t

1
−

,

t = 2, . . . , n,

(cid:110)

(cid:12)
(cid:12)
(cid:12)

(cid:111)(cid:17)

(II.81)

(II.82)

DRAFT

From (II.81), (II.82) and the conditional variance above, then follows (II.60), (II.61). From the above

then follows (II.62)-(II.65). (b) Using the assumptions we deduce (II.66)-(II.71). To show (II.72), (II.73)

16

we follow Butman [3]. Let

SNRt =g2

1KW KΘ +

2

KΘKV1,

t = 1, 2, . . . , n

t
∑
j=2

g j

ctg j

−

1
−

(cid:16)
1

−

(cid:16)

c

gt
1
−
gt

2

(cid:17)

(cid:17)
KΘKV1.

=SNRt

1 + g2
t
−

Hence,

SNRt + KW KV1 =SNRt

=

SNRt

(cid:16)

=

SNRt

(cid:16)

Letting χt = gt
gt

1
−

, then

2

KΘKV1 + KW KV1

1 + g2
t
−

ct

1

−

(cid:16)
1 + KW KV1
−

gt
1
−
gt

(cid:17)
1 +

1 + KW KV1
−

g2
t
KW KV1 + SNRt

κt
KW KΘKV1

1

(cid:0)

1

−

(cid:0)
gt
1
−
gt

1
−
ct

−

(cid:17)

(cid:1)

ct

2

gt
1
−
gt

.

2

(cid:17)

(cid:1)

(cid:17)(cid:16)

1 +

(cid:17)(cid:16)

1 +

κt
KW KΘKV1

1

gt
1
−
gt

ct

−

(cid:16)

From the above follows (II.72), (II.73).

2

=

(cid:17)

KW KV1 + SNRt
KW KV1 + SNRt

1
−

=

κt
κt+1

χt+1.

From Theorem II.1 follows an analogous preliminary characterization, for the AR(ct; v0),t

∞, ∞)

(
−

∈

noise, i.e., when V0 = v0 is the initial state known to the encoder, as stated in the next corollary.

Corollary II.1. Preliminary characterization of CL

n (κ, v0)

Consider the statement of Theorem II.1, with the AGN channel driven by a time-varying AR(ct; vo) noise.

Deﬁne the conditional mean and error covariance for ﬁxed V0 = v0, by

Θt (cid:52)=E

Θ

Y t,V0 = v0
(cid:12)
(cid:12)
(cid:12)

,

(cid:111)

Σt (cid:52)= E

(cid:110)(cid:16)

Θt

Θ

−

(cid:98)

2

(cid:17)

Y t,V0 = v0
(cid:12)
(cid:12)
(cid:12)

,

t = 1, . . . , n.

(II.89)

(cid:111)

(cid:110)

Consider the linear coding scheme
(cid:98)

,

t = 2, . . . , n, X1 = g1Θ,

+ ctVt

1 +Wt,

−

t = 2, . . . , n,

Xt =gt

Θ

Yt =gt

=gt

(cid:16)

Θ

(cid:16)

Θ

Θt

1
−

−

Θt
(cid:98)

1
−

−

Θt
(cid:98)

1
−

−

(cid:16)

(cid:17)

(cid:17)

(cid:17)

Y1 =g1Θ + c0V0 +W1,

(cid:98)

ctgt

1
−

−

Θ

(cid:16)

Θt

2
−

−

+ ctYt

1 +Wt

−

by Vt

1 = Yt

−

1
−

−

Xt

1
−

(cid:17)

κ,

≤

κt = g2

t Σt

1,

−

t = 2, . . . , n,

κ1 = g2

1KΘ.

(cid:98)

κt

n
∑
t=1

E

(cid:8)

2

Xt

(cid:0)

(cid:1)

V0 = v0
(cid:12)
(cid:12)
(cid:12)

(cid:9)

=

1
n

n
∑
t=1

June 17, 2021

(II.83)

(II.84)

(II.85)

(II.86)

(II.87)

(II.88)

(II.90)

(II.91)

(II.92)

(II.93)

(II.94)

DRAFT

Then the statements of Theorem II.1.(a), (b) hold, with the following changes: all conditional expectations

17

are replaced by conditional expectations for a ﬁxed V0 = v0, I(Θ;Y n) is replaced by I(Θ;Y n
H(Y n

H(V n

.

v0) =
|

v0)
|

−

v0), and Σ1 is replaced by Σ1 =
|

KΘKW1
g2
1KΘ+KW1

Proof. This follows by repeating the derivation of Theorem II.1.

From Theorem II.1 follows the validity of Theorem I.2.

Proposition II.1. Theorem I.2 is correct.

Proof. (a) By Theorem II.1.(a) follows directly that optimization problem CL

n (κ, PV1) is as stated.

(b) This also follows from Theorem II.1.(a), by replacing the total average power constraint by the

pointwise average power constraint.

(c) This follows from Corollary II.1, as in parts (a), (b).

By Theorem II.1, follows that I(Θ;Y n) is maximized by the choice of the sequence g1, g2, . . . , gn that

controls the mean-square error Σt,t = 1, . . . , n, and satisﬁes the average power constraint. In general, this

is a dynamic optimization problem, with state variable the sequence Σt,t = 1, . . . , n.

From Theorem II.1, we recover Wolfowitz’s [5] and Butman’s [1] lower bound, and more importantly

the assumptions based on which the lower bound is derived.

Proposition II.2. Reduction of Theorem II.1.(c) to Butman’s lower bound

(a) The statements of Theorem II.1.(c), with KΘ = 1, KV1 = KW , c
(I.22), i.e., sgn(gn) =

[
−
1), n = 2, 3, . . ., reduce to analogous statements derived by Butman [1],

1, 1], and strategy gn that satisﬁes,

sgn(cgn

∈

−

−

i.e., (I.25)-(I.26).

(a) If in addition to (a), i.e., sgn(gn) =

by E

2

Xt

(cid:110)(cid:0)

(cid:111)

(cid:1)

= κ, t = 1, 2, . . . , n (as in Butman [1]) then problem (I.33)-(I.34) is obtained.

sgn(cgn

−

1), n = 2, 3, . . ., the average power contraint is replaced

−

Proof. This is easily veriﬁed.

REFERENCES

[1] S. Butman, “Linear feedback rate bounds for regressive channels,” IEEE Transactions on Information Theory, vol. 22,

no. 3, pp. 363–366, 1976.

June 17, 2021

DRAFT

18

[2] Y.-H. Kim, “Feedback capacity of stationary Gaussian channels,” IEEE Transactions on Information Theory, vol. 56, no. 1,

pp. 57–85, 2010.

[3] S. Butman, “A general formulation of linear feedback communications systems with solutions,” IEEE Transactions on

Information Theory, vol. 15, no. 3, pp. 392–400, 1969.

[4] J. Tienan and J. P. M. Schalkwijk, “An upper bound to the capacity of bandlimited Gaussian autoregressive channel with

noiseless feedback,” IEEE Transactions on Information Theory, vol. IT-14, pp. 311–316, May 1974.

[5] J. Wolfowitz, “Signalling over a gaussian channel with feedback and autoregressive noise,” Journal of Applied Probability,

vol. 12, no. 4, pp. 713–723, 1975.

[6] P. Elias, “Channel capacity without coding,” MIT Research Laboratory of Electronics, Quarterly Report, October 1961,

also in Lectures on Communication System Theory, E. Baghdady, Ed., New York: McGraw Hill, 1961.

[7] J. P. M. Schalkwijk and T. Kailath, “A coding scheme for additive noise channels with feedback-I: no bandwidth constraints,”

IEEE Transactions on Information Theory, vol. 12, no. 2, pp. 172–182, April 1966.

[8] A. Dembo, “On gaussian feedback capacity,” IEEE Transactions on Information Theory, vol. 35, no. 5, pp. 1072–1076,

Sept. 1989.

[9] L. Ozarow, “Upper bounds on the capacity of Gaussian channel with feedback,” IEEE Transactions on Information Theory,

vol. 36, no. 1, pp. 156–161, January 1984.

[10] O. Shayevitz and M. Feder, “Communication with feedback via posterior matching,” in IEEE International Symposium on

Information Theory (ISIT), June 24-29 2007, pp. 391–395.

[11] ——, “The posterior matching feedback scheme: Capacity achieving and error analysis,” in IEEE International Symposium

on Information Theory (ISIT), July 6-11 2008, pp. 900–904.

[12] ——, “Achieving the empirical capacity using feedback: Memoryless additive models,” IEEE Transactions on Information

Theory, vol. 55, no. 3, pp. 1269–1295, March 2009.

[13] S. Yang, A. Kavcic, and S. Tatikonda, “On feedback capacity of power-constrained Gaussian noise channels with memory,”

Information Theory, IEEE Transactions on, vol. 53, no. 3, pp. 929–954, March 2007.

[14] T. Cover and S. Pombra, “Gaussian feedback capacity,” IEEE Transactions on Information Theory, vol. 35, no. 1, pp.

37–43, Jan. 1989.

[15] T. Liu and G. Han, “Feedback capacity of stationary Gaussian channels further examined,” IEEE Transactions on

Information Theory, vol. 64, no. 4, pp. 2494–2506, April 2019.

[16] C. Li and N. Elia, “Youla coding and computation of gaussian feedback capacity,” IEEE Transactions on Information

Theory, vol. 64, no. 4, pp. 3197–3215, April 2019.

[17] W. Rudin, Principles of mathematical analysis, 3rd ed. McGraw-Hill, 1976.

June 17, 2021

DRAFT

