1
2
0
2

l
u
J

8
2

]

V

I
.
s
s
e
e
[

1
v
7
0
4
3
1
.
7
0
1
2
:
v
i
X
r
a

High-speed object detection with a single-photon
time-of-flight image sensor

Germ√°n Mora-Mart√≠n,1,* Alex Turpin,2,3 Alice Ruget,4 Abderrahim
Halimi,4 Robert Henderson,1 Jonathan Leach,4 and Istvan Gyongy1
1School of Engineering, Institute for Integrated Micro and Nano Systems, The University of
Edinburgh, Edinburgh EH9 3FF, UK
2School of Computing Science, University of Glasgow, Glasgow G12 8QQ, UK
3ICFO-Institut de Ciencies Fotoniques, The Barcelona Institute of Science and Technology,
Castelldefels, Barcelona, 08860, Spain
4School of Engineering and Physical Sciences, Heriot-Watt University, Edinburgh EH14 4AS,
UK
*german.mora@ed.ac.uk

Abstract

3D time-of-Ô¨Çight (ToF) imaging is used in a variety of applications such as augmented
reality (AR), computer interfaces, robotics and autonomous systems. Single-photon avalanche
diodes (SPADs) are one of the enabling technologies providing accurate depth data even over
long ranges. By developing SPADs in array format with integrated processing combined
with pulsed, Ô¨Çood-type illumination, high-speed 3D capture is possible. However, array sizes
tend to be relatively small, limiting the lateral resolution of the resulting depth maps, and,
consequently, the information that can be extracted from the image for applications such as
object detection. In this paper, we demonstrate that these limitations can be overcome through
the use of convolutional neural networks (CNNs) for high-performance object detection. We
present outdoor results from a portable SPAD camera system that outputs 16-bin photon
timing histograms with 64√ó32 spatial resolution. The results, obtained with exposure times
down to 2 ms (equivalent to 500 FPS) and in signal-to-background (SBR) ratios as low as
0.05, point to the advantages of providing the CNN with full histogram data rather than
point clouds alone. Alternatively, a combination of point cloud and active intensity data may
be used as input, for a similar level of performance. In either case, the GPU-accelerated
processing time is less than 1 ms per frame, leading to an overall latency (image acquisition
plus processing) in the millisecond range, making the results relevant for safety-critical
computer vision applications which would beneÔ¨Åt from faster than human reaction times.

1.

Introduction

3D imaging is used in a wide range of applications, including computer interfaces, AR/VR [1],
self-driving cars [2], face recognition in smartphones [3], robotics and ballistics [4]. Common
methods of retrieving depth data from a scene include stereoscopy or structured light, although
these can be computationally expensive, and the accuracy reduces with distance. Furthermore,
stereoscopy has diÔ¨Éculties with capturing scenes that are untextured [5]. ToF is an alternative
way to get depth information by using a modulated or pulsed light source (commonly a laser
diode or a LED) typically emitting at near-infrared (NIR) wavelengths [6] and measuring the time
for the back-scattered light to return. ToF cameras usually reach speeds between 10-60 FPS and
these can be divided in two groups: indirect ToF (iToF) and direct ToF (dToF). An iToF sensor
measures the time-dependent intensity to retrieve the delay between transmitted and detected
signal [7]. Photo-demodulator pixels with multiple storage nodes are typically used, which are
synchronised with the illumination source. iToF cameras are widely available commercially, and
oÔ¨Äer high lateral resolutions (e.g. 1024√ó1024 [8]) and millimeter or sub-millimeter precision [9].
One of the main disadvantages is an inherent trade-oÔ¨Ä between range and accuracy, which
typically limits the range to a few meters [10]. Scene-dependent, multi-path issues can also arise,

 
 
 
 
 
 
leading to inaccurate depth data that can potentially confuse computer vision systems. Moreover,
ambient illumination in outdoor use can also severely impact depth estimates [11].

Direct ToF sensors use a sub-nanosecond electronic stopwatch to measure the time for the
light signal, typically a short laser pulse, to return. Highly sensitive avalanche photo-diodes
(APD) or single-photon avalanche diodes (SPAD) are commonly used as detectors. dToF systems
traditionally relied on single-point detectors, complemented by optical scanning to cover a given
Ô¨Åeld of view (FoV), at the expense of a low acquisition rate. However, advances in SPAD
technology have resulted in array format sensors with integrated timing electronics. SPAD-based
dToF devices in line [12] and image sensor format [13, 14] using blade-scanned and Ô¨Çood
illumination sources respectively, have been developed. These devices enable high-speed 3D
imaging, especially when featuring on-chip processing such as histogramming [15]. Moreover,
dToF sensors can maintain a high accuracy over hundreds of meters even under sunlight and do
not suÔ¨Äer from multipath issues like iToF, making them the basis of LiDAR devices [16, 17].

Indeed, automotive LiDAR and autonomous driving have motivated a lot of research into
both dToF sensors, and object detection based on depth data as considered here. Deep learning
using convolutional neural networks (CNNs) reach higher levels of accuracy [18] with respect to
older machine learning techniques like support vector machines (SVMs), k-nearest neighbours or
gradient boosting trees [19] in the context of object detection. High-resolution intensity/RGB
images are the most common choice to perform object detection, as seen in popular neural
networks such as ImageNet or YOLO [20]. LiDAR or point-based data has also been tested
in RangeNet or PointNet networks [21, 22], where raw point clouds are fed into the neural
networks for 3D classiÔ¨Åcation. More recent networks like 3DSSD [23] have used the same idea
to perform 3D object detection by processing eÔ¨Éciently point cloud data or PV-RCNN [24],
using a combination of voxels and point clouds.

This paper targets low latency, CNN-based (U-net [25]), object detection over short ranges
(<10m) based on the output of a high-speed SPAD dToF image sensor. Instead of using point-
cloud or voxel input (as typically done in literature), we investigate the impact of feeding raw
histogram data to the neural network. Indeed, recent works have shown that the amount of
information carried by temporal histograms in ToF measurements is signiÔ¨Åcantly rich [26‚Äì28],
even allowing for obtaining 3D image estimates from pure temporal data gathered with single-pixel
time-resolving detectors [29, 30]. Furthermore, we compare the performance of histogram-based
processing with the result of using higher lateral resolution intensity (photon counting) data
from the same sensor, as input to the CNN. The object detection is made more challenging by
the presence of strong ambient light, as well as a level of occlusion in the dataset. The use of
a U-net in this study as opposed to other state-of-the-art networks [31] that could potentially
provide higher performance (e.g. YOLO) is based on the following: (1) U-net‚Äôs easily adaptable
structure, which allows diÔ¨Äerent data types (in particular the histogram data here) to be readily
introduced (2) the ability to perform well even without large training datasets [25] (3) moderate
model sizes leading to fast processing speeds.

2. SPAD sensor output

We use a recently developed, reconÔ¨Ågurable SPAD image sensor, implemented in 3D-stacked
technology [15]. The sensor contains 256√ó256 SPADs which are grouped in 64√ó64 macropixels,
each containing 4√ó4 SPADs. The readout is over 100 MHz output lines, giving a maximum
frame rate of 760 FPS when the whole array is used and over 1000 FPS when half of it is used,
as in the present study [32].

The SPAD is able to operate in diÔ¨Äerent modes. The modes used here are the intensity or
photon counting mode (SPC), which runs at the SPAD resolution (256√ó128) and time correlated

single photon counting (TCSPC) mode, which operates at the macropixel resolution (64√ó32). The
latter mode generates 16-bin photon timing histograms per macropixel with 14-bit depth and a
minimum temporal resolution of 500 ps per bin (the temporal bin width being programmable). If
the laser pulse is spread over multiple bins and there are suÔ¨Écient photon counts in the histogram,
depth can be estimated with sub-bin precision. With the present sensor, sub-centimeter precision
is achievable. Then, depth is estimated from each histogram with low computational cost by
performing a centre-of-mass calculation following Eq. (1)

ùëë =

(cid:205)min (ùëëùëöùëéùë• +ùë°ùëü ,16)
ùë° max (0, ‚Ñéùë° ‚àí ùëè)
ùë°=max (ùëëùëöùëéùë• ‚àíùë°ùëô ,1)
(cid:205)min (ùëëùëöùëéùë• +ùë°ùëü ,16)
ùë°=max (ùëëùëöùëéùë• ‚àíùë°ùëô ,1) max (0, ‚Ñéùë° ‚àí ùëè)

,

(1)

where ‚Ñéùë° is the histogram bin at a given macropixel (from 1 to 16), ùëëùëöùëéùë• is the index of the bin
with the maximum count, ùëè is the median of the bins (measure of ambient level) and ùë°ùëô, ùë°ùëü are
parameters with a value corresponding to the width of the histogram peak, typically ùë°ùëô, ùë°ùëü = 2.
In this approach, depth is estimated by calculating a centroid around the maximum count and
its neighbours, providing sharp edges between objects in depth maps. Computing the centroid
over all bins instead (after compensating for ambient level) would be akin to the depth map
from an iToF camera, with the pixels seeing multiple returns outputting an averaged depth (so
that edges between two objects appear blurrier). Data acquisition is carried out using an Opal
Kelly XEM7310 FPGA integration module, which relays the SPAD data to a Matlab-based
software interface, where histogram to depth conversion takes place. As in [32], we operate
the camera in a hybrid modality, such that it alternates between histogram and intensity frames
in a time-interleaved manner. In this paper, we consider a target detection neural network and
investigate diÔ¨Äerent data inputs based on the acquired data. Fig. 1 shows examples of the
considered data types:

‚Ä¢ Intensity SPC-256 (256√ó128), obtained under a combination of ambient and active

illumination.

‚Ä¢ Intensity SPC-64 (64√ó32), resized from SPC-256.

‚Ä¢ Active intensity and depth concatenated (Act_I-D, 64√ó32√ó2). Active intensity is obtained
by summing all the bins of the histogram (for a given pixel) after background subtraction.

‚Ä¢ Depth (64√ó32), obtained from histogram via Eq. 1.

‚Ä¢ Histogram (64√ó32√ó16).

The three images given are representative of three diÔ¨Äerent categories of signal-to-background
ratio that we deÔ¨Åne here: very low (SBR < 0.1, Fig. 1a), low (0.1 < SBR < 0.5, Fig. 1b) and
moderate (SBR > 0.5, Fig. 1c). The lower the SBR, the more diÔ¨Écult it is to extract the signal
peak in the histogram, and eventually it gets buried within the ambient photon counts, leading to
the noisy background seen in the depth image of Fig 1a.

We note that SPAD dToF sensors are commonly based on a "Ô¨Årst photon" time-to-digital
converter (TDC) architecture, whereby each pixel only registers the Ô¨Årst detected photon per
frame. This then leads to signiÔ¨Åcant pile-up distortion (or exponential decay in the observed
histogram) in high ambient conditions, leading to inaccurate depth maps [33, 34]. However,
the SPAD used here instead features a multi-event TDC and is therefore more robust to strong
ambient light, as detailed in [32].

Fig. 1. SPC-256, SPC-64, Act_I-D (active illumination representation only), depth and
histogram examples for a) SBR < 0.1. (b) 0.1 < SBR < 0.5 and (c) SBR > 0.5. The
histograms correspond to the indicated pixels on the depth maps (the pixel is shown in
black and encircled). The last bin of the histogram (16) is not usable and is assigned a
value of 0.

3. Object Detection via Neural Networks

Amongst a number of methods for object detection, the use of deep learning has gained popularity
in recent years. CNNs are powerful tools that are able to recognise key features of every object
present in an image and diÔ¨Äerentiate them with a high accuracy. One way to perform this task is
via segmentation, where every pixel in the image is assigned a certain class. U-net is a popular
network that has proved to be eÔ¨Äective in segmentation problems by using small datasets of
hundreds of examples [26].

In this paper, a neural network with the main structure of U-net is used [25]. Certain changes
are applied with regards the size of input/output layers to match the size of our data: depth and
SPC-64 (64√ó32), Act_I-D (64√ó32√ó2), SPC-256 (256√ó128) and histogram (64√ó32√ó16). Table
S5 on the Supplemental Document gives a full description of each layer of the neural network.
The network is Ô¨Årst trained on data with known ground truth, and then deployed to perform target

detection on new data. During training, we provide the network with example frames together
with their corresponding ground truth, here based on manually labelled SPC-256 images and
resized for other data types. The ground truth has the same lateral resolution as the data type to
be trained and X channels, where X is the number of objects to be detected plus the background.
In this study, we have selected 6 objects (bucket, chair, duck, football, cardboard box and statue).
Therefore, ground truths are of the size 64√ó32√ó7 for depth, Act_I-D, histogram and SPC-64
and 256√ó128√ó7 for SPC-256. Ground truths are created by labelling each frame with bounding
boxes around the objects of interest. Each channel of the ground truth‚Äôs array is then Ô¨Ålled with 0
except for labelled regions, which are Ô¨Ålled with 1. Ground truths are one hot encoded to easily
visualize each class, giving each class a diÔ¨Äerent RGB colour. Fig. 2 shows an example of a
frame and its corresponding ground truth and one hot encoding.

Fig. 2. Example of a SPC-256 frame, its corresponding ground truth (split for each
class) and its one hot encoding (associating a color for each class). The ground truth
has pixel values of 0 (black, no object) and 1 (white, object is present).

Each frame must be pre-processed before being fed to the neural network. All data types
are normalized to values between 0 and 1. Depth data is corrected with a calibration frame
to compensate for temporal skew in photon timing across the SPAD array. SPC-256 data is
processed with a median Ô¨Ålter of size 2√ó2 to remove any outliers in the frame. SPC-64 is a
resized version of SPC-256. Histogram data is altered by subtracting the median of each pixel‚Äôs
histogram to remove the background level. When summing all the bins of this histogram, we get
a type of intensity image that preserves the active illumination from the laser but has minimal
contribution from ambient light. This active intensity data, together with the depth frame, are
concatenated to form the data type Act_I-D. The dataset as a whole is augmented by applying a
horizontal Ô¨Çip to each frame, thus doubling the quantity of frames. Finally, the dataset‚Äôs order is
shuÔ¨Ñed randomly to prevent tuning the weights of the neural network speciÔ¨Åcally for a given class
before seeing examples of others. Not doing so can lead to a local minimum in the optimisation
problem far from the absolute minimum.

The neural network is implemented in TensorÔ¨Çow using Keras [35]. The training is performed
using the Adam optimizer [36] on a desktop computer (HP EliteDesk 800 G5 TWR) with the
assistance of a RTX2070 GPU to accelerate the process. The loss function selected to optimise

this segmentation problem is the Focal Tversky loss, often used when there is a class imbalance
(typically the background being much more represented than other classes) [37]. The Focal
Tversky loss or FTL follows the expression of Eq. (2)

ùêπùëá ùêø =

(cid:18)

1 ‚àí

ùëá ùëÉ
ùëá ùëÉ + ùõºùêπ ùëÅ + ùõΩùêπùëÉ

(cid:19) ùõæ

,

(2)

where TP is true positives, FN false negatives, FP false positives, ùõº and ùõΩ are two parameters
which ùõº + ùõΩ = 1 and penalises false negatives when ùõº > ùõΩ and ùõæ is a parameter useful for class
imbalance that forces the model to focus on harder examples when ùõæ > 1. In this network,
ùõº = 0.6, ùõΩ = 0.4 and ùõæ = 1.2. The metric to track the performance of the model is the F-score
(F1), calculated via Eq. (3):

ùêπ1 =

ùëá ùëÉ
2 (ùêπùëÉ + ùêπ ùëÅ)

ùëá ùëÉ + 1

.

(3)

Additional parameters to be considered for the neural network are the number of epochs and
the batch size, set to 100 and 32, respectively. Early stopping of the training process is set
with a patience of 8 using the minimum of the validation loss as metric. In other words, if the
validation loss has not decreased after 8 epochs, the process is terminated and saves a model
with the weights of the best training step. Once the training is complete, an evaluation of the
performance of the model is required. In an object detection problem, we are interested in both
localising and classifying correctly each class in the scene. An object is considered to be well
localised when its intersection over union (IoU) surpasses 50%. IoU is deÔ¨Åned as the area of
overlap between the predicted segmentation and the ground truth divided by the area of union
between the predicted segmentation and the ground truth [38]. Similarly, an object is considered
to be correctly classiÔ¨Åed when the classes of the prediction and the ground truth are matching.
The object is successfully detected only if both criteria are satisÔ¨Åed at the same time.

4. Experimental Set-up

To facilitate the use of the SPAD outdoors, a portable camera setup is built, housed within a
180√ó180√ó88 mm 3D printed enclosure. Fig. 3 shows a rendered image of the SPAD camera box
with all the components.

Fig. 3. Rendered image of the 3D-printed SPAD camera box. The camera contains the
SPAD sensor, a lens, a 850 nm NIR laser and a battery for portability.

The camera includes a NIR laser source (850 nm) that is triggered from the SPAD printed
circuit board (PCB). This laser comes in a compact format (19 mm diameter and 28 mm long,
Iberoptics Time-of-Ô¨Çight illuminator) and provides a peak power of 2 W distributed uniformly
over a FoV of 20¬∫. The laser emits 10 ns pulses with a repetition rate of 6 MHz, which makes it
suitable for short range applications (up to 10 m). Next to the laser, a 6 mm focal length C-mount
lens (MVL6WA) is placed to ensure matching illumination and imaging FoVs. The lens includes
an ambient Ô¨Ålter (Thorlabs FL850-10) to reduce the number of ambient photons that reach to
the sensor. The box provides a hole compatible with tripod mounts and a portable battery that
powers the camera. The SPAD sensor PCB is connected to a laptop via a USB3.0 connection to
display the output in real-time, as explained in Section 2.

5. Results

We evaluate the performance of object detection for the data types oÔ¨Äered by the SPAD camera:
depth, SPC-256, SPC-64, Act_I-D and histogram. To create the necessary dataset, a large number
of frames is captured featuring the objects described in Section 3, which are placed at diÔ¨Äerent
positions and distances from the camera. Exposure times of 2ms, 5ms and 10ms are used. This
is repeated for diÔ¨Äerent ambient conditions and locations within a garden setting.

In the Ô¨Årst trial, a total of 6388 training examples are fed into the neural network to create a
model and a total of 574 examples are used for testing with a SBR < 0.5 on average (the test
data is drawn from a separate subset of the dataset, with a diÔ¨Äerent backdrop). A validation set,
corresponding to 15% of the data from the training set is used to provide and unbiased evaluation
of the model. A RTX2070 GPU is used to speed-up the training and prediction tasks involved in
the neural network. This implementation boosts processing speeds up to 1325 FPS for histograms,
1100 FPS for Act_I-D, 1780 FPS for both depth and SPC-64 and 280 FPS for SPC-256, which
makes it compatible with high-speed vision. The training is run 10 times since the optimization
of the network‚Äôs weights diÔ¨Äers every time, leading to changes in the predictions; multiple runs
thus enable statistical analyses to be performed. Four parameters are used to assess the network‚Äôs
performance in this study. The accuracy reÔ¨Çects the ratio between correct predictions and total
observations. Precision measures the ratio of correct positive predictions and total predicted
positive observations. Recall, or also known as sensitivity, measures the ratio of correct positive
predictions and all observations in a class. Finally, the F-score (Eq. 3) is also measured since
it is a weighted average of precision and recall. This parameter becomes meaningful specially
when the amount of false positives and false negatives is not balanced. Fig. 4 summarizes the
average of the parameters described above for each data type and object. A table with all values
can be found on the Supplemental Document (Table S1).

As seen in Fig. 4, the data suggests that even when capturing frames under high ambient
conditions, the overall object detection accuracy consistently surpasses 70% for all objects and
data types (>80% for histogram and Act_I-D), except for the case of SPC-64. The latter data
type has low lateral resolution, so the contrast between the objects and what is often a complex
background can be poor, making it very challenging to locate and classify objects accurately. In
terms of the performance diÔ¨Äerences between objects, these are thought to arise mainly due to
their relative shapes and sizes. The football has one of the highest performance in most cases
thanks to its simple round shape, whereas the duck presents lower Ô¨Ågures given its more complex
shape which varies signiÔ¨Åcantly with orientation.

High precision values indicate that the neural network is able to identify the right class wherever
there is an object, or in other words, there is a low incidence of false positives. SPC-64 presents
again the lowest numbers for most of the objects and histogram gives the most consistently high
precision. Recall values are in general lower than precision ones, indicating a higher level of
false negatives in the examples. False negatives refer to cases which either there are detections

Fig. 4. Statistics for depth, histogram, SPC-256, SPC-64 and Act_I-D: (a) Accuracy.
(b) Precision. (c) Recall. (d) F-score.

which do not appear in the ground truth or if they do, the IoU is below 50%. Given the signiÔ¨Åcant
diÔ¨Äerence between the incidence of false positives and false negatives, the F-score represents
a more balanced assessment than accuracy when comparing the relative performance of the
diÔ¨Äerent data types for the various objects. Overall, histograms have the best score (92.7%
average F-score) closely followed by Act_I-D (90.9 % average F-score). As before, SPC-64 has
the worst performance (71.3% average F-score)

The F-score values are submitted to a t-student test to assess if the performance of diÔ¨Äerent data
types is signiÔ¨Åcantly diÔ¨Äerent from each other (considering the variability in performance every
time the network is trained). Table 1 summarises the results of the tests looking for statistically
signiÔ¨Åcant diÔ¨Äerences in performance between histograms and the other data types after 20
runs. Histograms are seen to have a clear advantage over other data types, apart from Act_I-D.
Comparisons between other data types can be found on Table S2 in the Supplemental Document.

It is interesting to observe histograms outperforming SPC-256, despite the higher lateral
resolution of the latter (and the high ambient levels beneÔ¨Åting the intensity data, whilst adding
noise to histograms). This indicates that time resolved imaging can capture salient details in a
scene even when the x,y resolution is coarse. In practice, the performance of intensity-based
object detection is expected to depend on the level of contrast in colour and texture between the
objects and the backdrop. Thus, perhaps a more signiÔ¨Åcant observation is histogram-based (and
Act_I-D) processing being considerably more eÔ¨Äective than using depth alone. This points to
active intensity data (equivalent to the number of photons in the histogram peak) providing a rich
set of information. This represents a Ô¨Åne projection of the information available in histogram
data, which can be used for faster processing of results with little compromise on the detection
performance. Cases where histograms outperform Act_I-D may be explained by the multi-peak

Table 1. T-student analysis of F-score results of histogram versus other data types.
Green - histogram has higher F-score, yellow - no diÔ¨Äerence, red - other data type
has higher F-score.

Hist > Depth Hist > SPC-256 Hist > SPC-64 Hist > Act_I-D

Bucket

Chair

Duck

Football

Box

Statue

content for pixels at the edges of objects, which may help in the perception of the contour of
objects with higher Ô¨Ådelity.

A closer examination of detections is carried out with the same test dataset to try and establish
patterns in the failure cases. Histogram data (which performs the best) is mutually compared
with other data types (e.g histogram vs depth), by analysing the percentage of examples where
both predict correctly, where both are wrong and where only one predicts correctly.

Table 2. Failed detection comparison between histograms and other data types.
The parameters are: % of examples where both histogram and the other data
type predict correctly, where both are incorrect, and where one is correct and the
other incorrect.

Hist vs Depth

Hist vs SPC-256

Hist vs SPC-64

Hist vs Act_I-D

Both (cid:51) Both (cid:55) Hist (cid:51) Depth (cid:51) Both (cid:51) Both (cid:55) Hist (cid:51) SPC-256 (cid:51) Both (cid:51) Both (cid:55) Hist (cid:51) SPC-64 (cid:51) Both (cid:51) Both (cid:55) Hist (cid:51) Act_I-D (cid:51)

Bucket

Chair

Duck

Football

Box

Statue

90.6

93.1

93.9

91.6

89.6

96.4

0.0

0.3

0.3

0.0

0.0

0.6

8.5

5.3

3.4

6.8

5.2

1.8

0.9

1.3

2.4

1.6

5.2

1.2

96.9

95.7

82.5

93.8

85.8

93.3

0.0

0.3

0.3

0.0

0.5

0.0

2.2

2.6

14.8

4.7

9.0

4.8

0.9

1.3

2.4

1.6

4.7

1.8

77.5

89.1

49.5

81.7

81.6

93.0

0.0

0.7

1.1

0.0

0.0

0.3

21.6

9.2

47.9

16.8

13.2

5.2

0.9

1.0

1.6

1.6

5.2

1.5

98.4

91.4

93.9

97.5

89.6

96.7

0.0

0.0

0.8

0.0

0.5

0.6

0.6

6.9

3.4

0.9

5.2

1.5

0.9

1.6

1.9

1.6

4.7

1.2

On Table 2 we see that the vast majority of examples are predicted correctly by either one or
both of the paired data types. It is only in a minority of cases (typically <1%) that both data types
fail to detect and object at the same time. The results suggest that the diÔ¨Äerent data types provide
in some ways complementary information on the scene, and there may be potential beneÔ¨Åts in
combining data types for object detection. Fig. 5 shows challenging examples where the neural
network fails to detect appropriately from depth, histogram (represented as active illumination)
and SPC-256. In the Ô¨Årst example, all data types have at least one false detection. In the second
example, the histogram prediction detects all objects correctly, whereas depth and SPC-256
have some failed detections. It can be seen how the examples have a complex background and
contain multiple objects (some of them partially occluded) which leads to a lower prediction
performance.

It is also of interest to assess the impact of SBR being further reduced on the neural network‚Äôs
performance. For this study, the dataset is partitioned in a diÔ¨Äerent way to enable a subset with
lower SBR to be used for testing. The model is trained with 4086 examples and tested with
1151 examples, now with a SBR < 0.05 on average (i.e. 10√ó lower SBR than before). The
discussion of these results can be found in the Supplemental Document, containing a table and

Fig. 5. MisclassiÔ¨Åcation examples (a) and (b). Depth, histogram (active illumination
representation) and SPC-256 frames with its corresponding one hot encoding prediction
below. On the rightmost, the ground truth of the given example.

Ô¨Ågure with all values (Table S3 and Figure S1). We note that due to the considerably lower
SBR than before, histograms are signiÔ¨Åcant noisier, making histogram-based (and by extension
depth-based) perception more challenging. Nevertheless, relative performance of the diÔ¨Äerent
data types is broadly similar, with histograms still oÔ¨Äering the highest, and most consistent
performance overall (85.2 % F-score on average). Act_I-D again produces similar results to
histograms (82.7 % F-score on average).

We also consider object detection of a high-speed moving object under strong ambient
conditions (SBR < 0.05), in this case a football. Several sequences totaling 245 frames are taken
at 500 FPS (2 ms exposure time) and tested with the models generated by the complete dataset
(6388 examples, none of them including high-speed sequences). The video of the sequence can
be found in the supplemental material (Visualization 1). Table 3 shows the average parameters
for football detection for all data types.

Table 3. Average performance parameters after 10 neural network training runs
for a football moving at high-speed as captured by diÔ¨Äerent data types: depth,
histogram, SPC-256, SPC-64 and Act_I-D.

Accuracy [%] Precision [%] Recall [%] F1 [%]

Depth

Hist

SPC-256

SPC-64

Act_I-D

94.3

98.0

99.6

98.8

97.1

100

100

100

100

100

78.8

92.4

98.5

95.5

89.4

88.1

96.1

99.2

97.7

94.4

Fast moving objects may contain motion blur when they are captured (even at high frame
rates), meaning that the object presented to the neural network might look unfamiliar and
remain undetected. This issue combined with low lateral resolution can complicate the detection
performance. However, the detection of a high-speed moving football appears to be successful
for all data types, with histogram-based processing again outperforming depth.

6. Conclusion

This paper presents the application of a dToF SPAD sensor to high-speed object detection
in outdoor conditions. A dataset involving six diÔ¨Äerent objects located at various positions,
with diÔ¨Äerent backdrops, and under varying ambient levels is created to train and test neural
networks for diÔ¨Äerent data types. These include depth, histogram, Act_I-D (intensity under
active illumination, combined with depth), and two types of intensity; SPC-256 and SPC-64
(resized from SPC-256). Processing times are commensurate with the high-speed operation of
the camera, which runs at hundreds of frames per second.

The results from test datasets with signal to background ratios (SBR) of <0.5 and <0.05
indicate that object detection based on histogram data gives the highest performance overall, with
average F-scores of 92.7% and 85.2%, respectively. Act_I-D oÔ¨Äers a similar performance, with
F-score averages of 90.9% and 82.7%, respectively. This compares with 81.8% and 69.7% for
inference based on depth alone.

We note that since the advantage of histogram data versus Act_I-D appears to be relatively
modest, for dToF SPADs with larger number of bins than the 16 bins here, Act_I-D may be
preferable to avoid the need for large neural network models.

We believe that the results presented here are relevant for high-speed situational awareness
in autonomous systems. Whilst these systems often use the fusion of an assortment of sensors
to interpret the surroundings, for the limited number of objects (6), and short range (<10m)
considered here, the SPAD on its own shows a promising level of performance. It would be
interesting to expand the study to more generalised conditions, and to consider, for example, the
addition of RGB data.

Funding

This work was supported by EPSRC through grants EP/M01326X/1 and EP/S001638/1. Also it
is supported by the UK Royal Academy of Engineering Research Fellowship Scheme (Project
RF/201718/17128) and DSTL Dasa project (DSTLX1000147844).

Acknowledgements

The authors are grateful to STMicroelectronics and the ENIAC-POLIS project for chip fabrication.
Preliminary results from this work, using a diÔ¨Äerent neural network model with slower processing,
and indoor data only, were presented at Photonic Instrumentation Engineering VIII in 2021 [39].

Disclosures

The authors declare no conÔ¨Çicts of interest.

Supplemental Document

See Supplement 1 for supporting content.

References
1. C. Steger, M. Ulrich, and C. Wiedemann, Machine vision algorithms and applications (John Wiley & Sons, 2018).
2. X. Chen, H. Ma, J. Wan, B. Li, and T. Xia, ‚ÄúMulti-view 3D object detection network for autonomous driving,‚Äù Proc. -

30th IEEE Conf. on Comput. Vis. Pattern Recognition, CVPR 2017 2017-Janua, 6526‚Äì6534 (2017).

3. G. Pan, S. Han, Z. Wu, and Y. Wang, ‚Äú3D face recognition using mapped depth images,‚Äù in Proceedings of the 2005
IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR‚Äô05) - Workshops - Volume
03, (IEEE Computer Society, USA, 2005), CVPR ‚Äô05, p. 175.

4. C. Jing, J. Potgieter, F. Noble, and R. Wang, ‚ÄúA comparison and analysis of RGB-D cameras‚Äô depth performance
for robotics application,‚Äù 2017 24th Int. Conf. on Mechatronics Mach. Vis. Pract. M2VIP 2017 2017-Decem, 1‚Äì6
(2017).

5. S. Giancola, M. Valenti, and R. Sala, A survey on 3D cameras: Metrological comparison of time-of-Ô¨Çight,

structured-light and active stereoscopy technologies (2018).

6. R. Horaud, M. Hansard, G. Evangelidis, and C. M√©nier, ‚ÄúAn overview of depth cameras and range scanners based on

time-of-Ô¨Çight technologies,‚Äù Mach. Vis. Appl. 27, 1005‚Äì1020 (2016).

7. R. David, B. Allard, X. Branca, and C. Joubert, ‚ÄúStudy and design of an integrated CMOS laser diode driver for an
itof-based 3D image sensor,‚Äù in CIPS 2020; 11th International Conference on Integrated Power Electronics Systems,
(2020), pp. 1‚Äì6.

8. Microsoft, ‚ÄúAzure kinect DK,‚Äù (2020) [retrieved 9 June 2021], https://docs.microsoft.com/en-us/

azure/kinect-dk/hardware-specification.

9. L. V. Labs,

‚ÄúLucid Helios2,‚Äù

(2020)

[retrieved 9 June 2021], https://thinklucid.com/

helios-time-of-flight-tof-camera.

10. L. Pancheri, D. Stoppa, L. Gonzo, and G.-F. D. Betta, ‚ÄúA CMOS range camera based on single photon avalanche
diodes (CMOS-range-kamera mit single-photon-avalanche-photodioden),‚Äù tm - Tech. Messen 74, 57‚Äì62 (2007).
11. J. Marco, Q. Hernandez, A. Mu√±oz, Y. Dong, A. Jarabo, M. H. Kim, X. Tong, and D. Gutierrez, ‚ÄúDeepToF:
oÔ¨Ä-the-shelf real-time correction of multipath interference in time-of-Ô¨Çight imaging,‚Äù ACM Transactions on Graph.
36, 1‚Äì12 (2017).

12. C. Niclass, M. Soga, H. Matsubara, S. Kato, and M. Kagami, ‚ÄúA 100-m range 10-frame/s 340 √ó 96-pixel time-of-Ô¨Çight

depth sensor in 0.18-ùúám CMOS,‚Äù IEEE J. Solid-State Circuits 48, 559‚Äì572 (2013).

13. R. K. Henderson, N. Johnston, F. Mattioli Della Rocca, H. Chen, D. Day-Uei Li, G. Hungerford, R. Hirsch,
D. Mcloskey, P. Yip, and D. J. S. Birch, ‚ÄúA 192 √ó 128 time correlated SPAD image sensor in 40-nm CMOS
technology,‚Äù IEEE J. Solid-State Circuits 54, 1907‚Äì1916 (2019).

14. C. Zhang, S. Lindner, I. M. Antoloviƒá, J. Mata Pavia, M. Wolf, and E. Charbon, ‚ÄúA 30-frames/s, 252 √ó 144 SPAD
Ô¨Çash LIDAR with 1728 dual-clock 48.8-ps tdcs, and pixel-wise integrated histogramming,‚Äù IEEE J. Solid-State
Circuits 54, 1137‚Äì1151 (2019).

15. S. W. Hutchings, N. Johnston, I. Gyongy, T. Al Abbas, N. A. W. Dutton, M. Tyler, S. Chan, J. Leach, and R. K.
Henderson, ‚ÄúA reconÔ¨Ågurable 3-D-stacked SPAD imager with in-pixel histogramming for Ô¨Çash LIDAR or high-speed
time-of-Ô¨Çight imaging,‚Äù IEEE J. Solid-State Circuits 54, 2947‚Äì2956 (2019).

16. S. Afshar, T. J. Hamilton, L. Davis, A. Van Schaik, and D. Delic, ‚ÄúEvent-based processing of single photon avalanche

diode sensors,‚Äù IEEE Sensors J. 20, 7677‚Äì7691 (2020).

17. A. M. Wallace, A. Halimi, and G. S. Buller, ‚ÄúFull waveform LIDAR for adverse weather conditions,‚Äù IEEE

Transactions on Veh. Technol. 69, 7064‚Äì7077 (2020).

18. S. Albawi, T. A. Mohammed, and S. Al-Zawi, ‚ÄúUnderstanding of a convolutional neural network,‚Äù in 2017

International Conference on Engineering and Technology (ICET), (2017), pp. 1‚Äì6.

19. Y. Li and J. Ibanez-Guzman, ‚ÄúLIDAR for autonomous driving: The principles, challenges, and trends for automotive

LIDAR and perception systems,‚Äù IEEE Signal Process. Mag. 37, 50‚Äì61 (2020).
20. J. Redmon and A. Farhadi, ‚ÄúYOLOV3: An incremental improvement,‚Äù (2018).
21. X. Chen, A. Milioto, E. Palazzolo, P. Gigu√®re, J. Behley, and C. Stachniss, ‚ÄúSuma++: EÔ¨Écient LIDAR-based
semantic slam,‚Äù in 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), (2019), pp.
4530‚Äì4537.

22. A. A√ümann, B. Stewart, and A. Wallace, ‚ÄúDeep learning for LIDAR waveforms with multiple returns,‚Äù in 2020 28th
European Signal Processing Conference (EUSIPCO), (IEEE, United States, 2020), European Signal Processing
Conference, pp. 1571‚Äì1575. 28th European Signal Processing Conference, EUSIPCO 2020 ; Conference date:
18-01-2021 Through 22-01-2021.

23. Z. Yang, Y. Sun, S. Liu, and J. Jia, ‚Äú3DSSD: Point-based 3D single stage object detector,‚Äù (2020).
24. S. Shi, C. Guo, L. Jiang, Z. Wang, J. Shi, X. Wang, and H. Li, ‚ÄúPV-RCNN: Point-voxel feature set abstraction for 3D

object detection,‚Äù (2021).

25. O. Ronneberger, P. Fischer, and T. Brox, ‚ÄúU-net: Convolutional networks for biomedical image segmentation,‚Äù

(2015).

26. D. B. Lindell, M. O‚ÄôToole, and G. Wetzstein, ‚ÄúSingle-photon 3D imaging with deep sensor fusion.‚Äù ACM Trans.

Graph. 37, 113‚Äì1 (2018).

27. A. Ruget, S. McLaughlin, R. K. Henderson, I. Gyongy, A. Halimi, and J. Leach, ‚ÄúRobust super-resolution depth

imaging via a multi-feature fusion deep network,‚Äù Opt. Express 29, 11917‚Äì11937 (2021).

28. Z. Sun, D. B. Lindell, O. Solgaard, and G. Wetzstein, ‚ÄúSPADnet: deep RGB-SPAD sensor fusion assisted by

monocular depth estimation,‚Äù Opt. Express 28, 14948‚Äì14962 (2020).

29. A. Turpin, G. Musarra, V. Kapitany, F. Tonolini, A. Lyons, I. Starshynov, F. Villa, E. Conca, F. Fioranelli,

R. Murray-Smith, and D. Faccio, ‚ÄúSpatial images from temporal data,‚Äù Optica 7, 900‚Äì905 (2020).

30. M. Nishimura, D. B. Lindell, C. Metzler, and G. Wetzstein, ‚ÄúDisambiguating monocular depth estimation with a

single transient,‚Äù in European Conference on Computer Vision, (Springer, 2020), pp. 139‚Äì155.

31. I. R. Ward, H. Laga, and M. Bennamoun, ‚ÄúRGB-D image-based object detection: from traditional methods to deep

learning techniques,‚Äù (2019).

32. I. Gyongy, S. W. Hutchings, A. Halimi, M. Tyler, S. Chan, F. Zhu, S. McLaughlin, R. K. Henderson, and J. Leach,

‚ÄúHigh-speed 3D sensing via hybrid-mode imaging and guided upsampling,‚Äù Optica 7, 1253‚Äì1260 (2020).

33. A. Gupta, A. Ingle, and M. Gupta, ‚ÄúAsynchronous single-photon 3D imaging,‚Äù in Proceedings of the IEEE/CVF

International Conference on Computer Vision (ICCV), (2019).

34. J. Rapp, Y. Ma, R. M. A. Dawson, and V. K. Goyal, ‚ÄúHigh-Ô¨Çux single-photon LIDAR,‚Äù Optica 8, 30‚Äì39 (2021).
35. F. Chollet et al., ‚ÄúKeras,‚Äù https://keras.io (2015).
36. D. P. Kingma and J. Ba, ‚ÄúAdam: A method for stochastic optimization,‚Äù (2017).
37. N. Abraham and N. M. Khan, ‚ÄúA novel focal tversky loss function with improved attention U-net for lesion

segmentation,‚Äù (2018).

38. H. RezatoÔ¨Åghi, N. Tsoi, J. Gwak, A. Sadeghian, I. Reid, and S. Savarese, ‚ÄúGeneralized intersection over union: A

metric and a loss for bounding box regression,‚Äù (2019).

39. G. M. Mart√≠n, A. Turpin, A. Ruget, A. Halimi, R. Henderson, J. Leach, and I. Gyongy, ‚ÄúHigh-speed object detection
using SPAD sensors,‚Äù in Photonic Instrumentation Engineering VIII, vol. 11693 (International Society for Optics and
Photonics, 2021), p. 116930L.

