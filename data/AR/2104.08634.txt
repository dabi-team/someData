1
2
0
2

r
p
A
0
2

]

O
R
.
s
c
[

2
v
4
3
6
8
0
.
4
0
1
2
:
v
i
X
r
a

ARES: Accurate, Autonomous, Near Real-time 3D Reconstruction using Drones

Fawad Ahmad
USC

Christina Shin
USC

Eugene Chai
NEC Labs

Karthik Sundaresan
NEC Labs

Ramesh Govindan
USC

Abstract
A 3D model represents an accurate reconstruction of a ob-
ject. This paper explores the design and implementation of
ARES, which provides near real-time, accurate, autonomous
reconstruction of 3D models using a drone-mounted LiDAR;
such a capability can be useful to document building con-
struction or check aircraft integrity between ﬂights. Accurate
reconstruction requires high drone positioning accuracy, and,
because GPS can be inaccurate, ARES uses SLAM. However,
in doing so it must deal with several competing constraints:
drone battery and compute resources, SLAM error accumu-
lation, and LiDAR resolution. ARES uses careful trajectory
design to ﬁnd a sweet spot in this constraint space and a fast
reconnaissance ﬂight to narrow the search area for structures,
and ofﬂoads SLAM to the cloud by streaming compressed
LiDAR data over LTE. It reconstructs buildings to within 10s
of cms and incurs less than 100 ms compute latency.

1 Introduction

Drone-based 3D reconstruction. The last few years have
seen impressive advances in the commoditization of drones.
Today, drones can be equipped with on board compute, a
cellular (LTE) radio and sophisticated sensors (e.g., cameras,
stereo cameras and LiDAR). Given this, in the coming years,
drones will likely revolutionize aerial photography, mapping
and three-dimensional (3D) reconstruction. Recent estimates
put the total market for these drone-based services at 63 billion
US dollars by 2025 [10], with 3D reconstruction, the task of
generating a digital 3D model of the environment, accounting
for nearly a third of the drone services market [44].
What is a 3D model? The term 3D model covers a wide
range of geometric representations of the surfaces of objects,
from coarse-grained approximations (cylinders, cubes, inter-
section of planes), to more ﬁne-grain representations such as
meshes (small-scale surface tesselations that capture struc-
tural variations). In this paper, we seek to extract a ﬁne-grain
point-cloud of a large structure (e.g., a building, blimp or
airplane) which consists of dense points on the surface of the
structure. Each point has an associated 3D position, together

with other attributes (depending on the sensor used to gen-
erate the point-cloud). The point-cloud based 3D model can
generate all other representations.

Applications. 3D models are used in animations in ﬁlms
and video games, for preserving historical and tourist sites,
for archaeology, city planning, and to capture buildings or
rooftops for repair and solar installation etc., and as inputs
to immersive virtual reality (VR) applications. Most prior
research work has explored such ofﬂine reconstruction [19,
30, 32, 34, 51, 54] and report centimeter to sub-meter accuracy.

Towards real-time 3D reconstruction. The nascent drone-
based reconstruction industry is starting to use the time-to-
reconstruction as an important market discriminator. Star-
tups promise to deliver models in hours or within a day [4]
after drone data collection. This trend has inspired newer,
more time-sensitive applications of 3D modeling like (a) post-
disaster reconstruction for search and rescue missions [1, 2,
5, 8, 11], (b) construction site monitoring [4], (c) mapping
mines, cell-towers and tunnels [7, 16], (d) documenting in-
terior ofﬁce spaces [9], and (e) perhaps, most compelling,
between-ﬂight modeling of aircraft fuselages or blimps to
determine structural integrity [38].

Though startups are starting to promise smaller reconstruc-
tion times [7,16], they reconstruct 3D models ofﬂine i.e., after
the drone has landed. Only one company we know of so far
promises real-time reconstruction [9], employing a human-
operator holding a hand-held kit with a stereo camera (instead
of a drone) to reconstruct interior spaces.

The Goal. Given this, we want to accurately reconstruct large
3D structures in near real-time, with no human-intervention.

Photogrammetry based reconstruction. Most existing
work (§4) uses photogrammetry, which takes as input a se-
quence of 2D camera images (and the positions associated
with these images), captured either using drone-mounted cam-
eras [19, 30, 32, 51] or by humans using ordinary mobile
devices [6]. Photogrammetry infers 3D models using a tech-
nique known as multi-view stereo [23], which combines in-
formation from successive images, together with information

1

 
 
 
 
 
 
Figure 1: GPS-derived (left), SLAM-derived (middle), and ARES-
derived (right) models of a large complex on our campus.

about local motion, and inferred camera parameters such as
the focal length. Photogrammetry approaches estimate depth
from 2D images so their reconstruction times are relatively
large i.e., from several hours to days [4] for large structures.

LiDAR-based reconstruction. Unlike photogrammetry,
LiDAR-based reconstruction can more directly infer 3-D mod-
els, because LiDARs directly provide depth information (un-
like cameras). A LiDAR sensor measures distances to surfaces
using lasers mounted on a mechanical rotating platform. The
lasers and mechanical rotator sit within the enclosure of the
LiDAR sensor. With each revolution of the lasers, the sensor
returns a point cloud, or a LiDAR frame. The point cloud
is a set of 3D data points, each corresponding to a distance
measurement of a particular position of the surrounding en-
vironment from the LiDAR. For instance, an Ouster OS1-64
LiDAR has 64 lasers that scan at 20 Hz, with a horizontal and
vertical ﬁeld of view of 360° and 45° respectively.

To reconstruct a structure e.g., a building, it sufﬁces, in the-
ory, to merge points clouds captured from different locations
around the building. To understand what it means to merge
point clouds, consider two successive clouds p and p0. A point
x on the surface of the building may appear both in p and p0.
However, since the drone has moved, this point x appears at
different positions (relative to the LiDAR) in the two point
clouds. If we precisely transform both point clouds to the
same coordinate frame of reference, then the union of points
in p and p0 constitutes part of the 3D model of the building.

Accurate positioning ensures high-quality models. This
requires the precise position of the LiDAR at the positions
where it captured the point clouds. The accuracy of posi-
tioning determines model quality. Two metrics deﬁne quality.
Accuracy, the average distance between the position of a point
in the model and the corresponding point on the surface (see
§2 for a more precise deﬁnition), clearly relies on accurate po-
sitioning. Completeness, the degree to which a model captures
the entire structure, also relies on accurate positioning, since
positioning errors can lead to gaps in the captured model.

Strawman: Using GPS for positioning. Because drones can
carry GPS receivers, GPS can be used to position point clouds.
Unfortunately, GPS errors can be several meters in obstructed
settings, resulting in poor accuracy and completeness of the
3D model. The left image of Fig. 1 shows a 3D model of a
building assembled using a drone ﬂight over a commercial
building; the building’s outline is fuzzy, as are the contours
of the trees surrounding the building. The right image shows
a 3D reconstruction using techniques proposed in this paper,

2

which does not use GPS. (All reconstructions in Fig. 1 use
real data captured from a drone-mounted LiDAR §3).

High-precision GNSS/RTK receivers can provide more
accurate positioning but require additional infrastructure,
are costly, and can perform poorly in urban environments
due to non line-of-sight signals that degrade accuracy (e.g.,
prior work [29] reports that RTK-enabled receivers can ex-
hibit tens of meters error in downtown environments). Prior
work [34,54] has used expensive GPS receivers in the context
of remote sensing, using specialized unmanned aerial vehi-
cles (UAVs) with long-range LiDAR sensors (but for ofﬂine
model reconstruction). In contrast, in this paper we consider
solutions that employ off-the-shelf technologies: drones, and
commodity GPS and LiDAR.
An alternative: Using SLAM for positioning. LiDAR
SLAM (Simultaneous Localization And Mapping [14, 20])
algorithms can provide accurate pose (position and orienta-
tion) estimates, which can be used to position point clouds.
These algorithms use scan or feature matching techniques to
align consecutive LiDAR frames to determine the pose of the
drone throughout its ﬂight. For example, scan matching uses
techniques [15] to ﬁnd the closest match for each point in the
source point cloud A to a point in the reference point cloud B.
It then estimates the transformation (translation and rotation)
that best aligns each point in A to its corresponding point
in B. By repeating this process across consecutive LiDAR
frames, SLAM can position each frame in the ﬁrst frame’s
coordinate frame of reference.
Challenges. However, using SLAM on drone-mounted Li-
DAR is challenging for the following reasons (Table 1).
(cid:73) Reconstruction quality is critically dependent on the design
of the drone trajectory. Fig. 1 (middle) shows reconstruction
using SLAM from a poorly planned drone ﬂight. This recon-
struction is visually worse than GPS-based reconstruction
(Fig. 1 (left)), because the underlying SLAM algorithm is un-
able to track LiDAR frames (i.e., it is unable to match points
in successive point clouds).
(cid:73) SLAM algorithms accumulate drift [14, 20], so position
estimates can degrade on longer ﬂights.
(cid:73) Drones have limited compute because they can carry limited
payloads. For instance, a DJI M600Pro (which we use in this
paper) hexacopter has a maximum payload weight of 5 kg. It
carries an A3Pro ﬂight controller that contains three IMUs and
three GNSS receivers. We have also mounted an LTE radio
and an Ouster OS1-64 LiDAR, as well as a Jetson TX2 board.
This compute capability is far from sufﬁcient to run LiDAR
SLAM1. On the TX2, plane-ﬁtting [22], a primitive used in
our reconstruction pipeline, takes 0.5 seconds per LiDAR
frame (LiDARs generate 20 frames per second) (Fig. A.1)
and plane-ﬁtting accounts for 5% of the execution time of our
entire pipeline (§3).
(cid:73) Drones have limited ﬂight endurance. When fully loaded

1With a 64-beam LiDAR, SLAM processes upto 480 Mbps of 3D data

Challenge

Mechanism

Model accuracy
Limited duration
Limited compute
Drift accumulation

Model collection trajectory planning
Cheap reconnaissance ﬂight
Ofﬂoad compressed point clouds
Flight re-calibration

Table 1: Challenges and contributions

and starting from a full charge, the M600Pro can ﬂy for ap-
proximately 25 minutes. This necessitates careful trajectory
planning to minimize ﬂight duration.
Contributions. This paper presents the design and implemen-
tation of a system called ARES, which makes the following
contributions to address these challenges (Table 1).

ARES’s ﬁrst contribution is the design of model collection
trajectories that navigate the competing constraints i.e., accu-
racy and battery life (§2.1). Model collection uses SLAM, and
SLAM error is sensitive to how the LiDAR is mounted, how
fast and at what height the drone ﬂies. Faster, higher ﬂights
use less of the drone’s battery, but can incur high SLAM error.
The converse is true of slower and lower ﬂights. ARES ﬁnds
a sweet spot in this trade-off space to balance its accuracy
goals with drone battery usage. Even so, SLAM can incur
drift on longer ﬂights. ARES needs to detect excessive drift,
and correct for it in real-time. It uses a novel algorithm that
tracks consistency between GPS traces and SLAM positions
to detect excessive drift, then incorporates a re-calibration
step to correct for it while the drone is in-ﬂight.

Even with ARES’s efﬁcient model collection trajectories,
scans over large areas can exhaust a drone’s battery. To this
end, ARES’s second contribution is a robust and efﬁcient ge-
ometry extraction algorithm (§2.2) that helps focus model
collection only on the structure2 to reconstruct. This algo-
rithm, which runs during a reconnaissance ﬂight before model
collection, works well even with a fast, high ﬂight that mini-
mizes drone ﬂight duration (and hence battery usage). During
the ﬂight, this algorithm extracts the building geometry with-
out constructing the 3D model; it relies on detecting planar
surfaces by using the consistency of surface normals across
points on a plane, then estimates building height and boundary
from the plane forming the rooftop. ARES uses this boundary
to plan the model collection trajectories described above.

ARES’s third contribution is the design of a processing
pipeline that (a) ofﬂoads computation to a cloud server by
compressing point clouds on the drone to the point where they
can be transmitted over LTE (§2.3), and (b) that leverages
GPU acceleration to process these point clouds in near real
time at the LiDAR frame rate and with minimal end to end
processing delay (§3).

Experiments (§3) on real-world drone ﬂights and a photo-
realistic drone simulator (AirSim [48]) , demonstrate that
ARES can achieve 21-30 cm reconstruction accuracy even
after compressing the raw sensor data by almost two orders of
magnitude. Not only is ARES faster than an ofﬂine approach, it

2In this paper, we focus on extracting the boundary of a building and

leave generalization to future work

is also more accurate, since the latter cannot re-calibrate mid-
ﬂight. An experiment with a complete ARES implementation
is able to stream compressed point clouds over LTE, recon-
struct the 3D model on-the-ﬂy and deliver an accurate 3D
model about 100 ms after ﬂight completion. ARES’s choice of
trajectories drains the battery least while achieving the accu-
racy target, and its pipeline can process frames at 20 fps while
incurring a processing latency of less than 100 ms. To our
knowledge, ARES is the ﬁrst to demonstrate on-line, cloud-
based, autonomous, sub-meter, 3D model reconstruction in
near real-time.

2 DART Design

Because drone-based 3-D reconstruction is a complex multi-
dimensional problem (Table 1), we have focused on a geo-
metrically regular, but important, subset of structures for re-
construction: buildings. As this section will make clear, even
this choice poses signiﬁcant challenges. It also brings out the
computation and communication issues in 3-D reconstruction
that are the main focus of this paper. In §2.1, we discuss what
it would take to generalize to other, more complex, structures.
Overview. To use ARES, a user speciﬁes: a) an area of inter-
est, and b) a minimum target point density. Point density is
the number of points per unit area on the surface of a point
cloud; this knob controls the quality of the 3D model. In the
area of interest, ARES guides a drone to automatically dis-
cover buildings, and constructs a 3D model of the buildings
in near real-time (i.e., during the drone ﬂight) while mini-
mizing ﬂight duration at that given minimum point density.
(To a ﬁrst approximation, drone battery usage increases with
ﬂight duration; we have left it to future work to incorporate
drone battery models.) ARES splits its functionality across
two components: (a) a lightweight subsystem that runs on
the drone, and (b) a cloud-based component that discovers
buildings, generates drone trajectories, and reconstructs the
3D models on-the-ﬂy.

ARES’s cloud component (Fig. 2) generates an efﬁcient
reconnaissance trajectory over the area of interest to discover
the rooftop geometry of buildings. Extracting the geometry
from LiDAR data can be computationally intensive (§1), so
ARES streams compressed point clouds to a cloud service dur-
ing ﬂight over a cellular (LTE) connection. The cloud service
extracts the geometry, then prepares a more careful model
collection trajectory that designs a minimal duration ﬂight

Figure 2: ARES architecture

3

Data CollectionRead LiDAR StreamAdaptive CompressionReconnaissance FlightBuilding Detection3D Model Construction3D SLAMDrift Estimation and Re-CalibrationIn the CloudOn the DroneTrajectory PlanningTrajectory PlanningWireless NetworkCloudLiDARTrajectorywhile ensuring high 3D model accuracy. During this second
ﬂight, the drone also streams compressed point clouds, and
the cloud service runs SLAM to estimate point cloud poses,
and composes the received point clouds into the building’s
3D model. Mid-ﬂight, the cloud service may re-calibrate the
trajectory dynamically to minimize drift accumulation.

Below, we ﬁrst describe model collection (§2.1), since that
is the most challenging of ARES’s components. We then de-
scribe how ARES extracts the rooftop geometry (§2.2), then
conclude by describing point-cloud compression (§2.3).

2.1 Model Collection
Given the building geometry (§2.2), ARES designs a model
collection trajectory to capture the 3D model (§1) of the
building.
What constitutes a good 3D model? Prior work on 3D re-
construction [47] has proposed two metrics, accuracy and
completeness. Consider a 3D model M and a corresponding
ground-truth Mg. Accuracy is the Root Mean Square Error
(RMSE) of the distance from each point in M to the nearest
point in Mg. Completeness is the RMSE of the distance from
each point in Mg to the nearest point in M. If both values are
zero, M perfectly matches Mg. If M captures all points in
Mg, but the positions of the points are erroneous, then both
accuracy and completeness will be non-zero. If M captures
only one point in Mg, but positions it correctly, its accuracy
is perfect, but completeness is poor.
Trajectories, SLAM and 3D reconstruction error. As com-
pared to an autonomous vehicle, a drone-mounted LiDAR can
only perceive ( 45°
360° ) of the 3D point cloud. This makes scan
matching more difﬁcult for SLAM. Thus, the trajectory of
the drone ﬂight can impact 3D model completeness and accu-
racy, in part because a poorly designed trajectory can increase
SLAM error. In designing the drone’s trajectory, ARES can
control the following parameters: the actual path of the drone
over the building, its speed, its height, and the orientation of
the LiDAR with respect to the ground. We now discuss the
qualitative impact of these parameter choices; later (§3), we
empirically quantify the best parameter choices.

Orientation impacts accuracy. At a ﬁxed height and speed,
a parallel orientation of the LiDAR (Fig. 3) in which its
scan plane aligns with the drone’s direction of motion, results
in higher overlap between two successive point clouds than
with a perpendicular trajectory, therefore, lower SLAM error
and better accuracy. Fig. 4, obtained using the methodology
described in §3, quantiﬁes this intuition: different orientations
have different degrees of overlap, and as overlap decreases,
SLAM’s positioning error increases. A parallel orientation
(0°) has the lowest SLAM error because it has the highest
visibility lifespan. (Visibility lifespan, the time for which a
point on the building’s surface is visible during ﬂight, is a
proxy for overlap; a longer lifespan indicates greater overlap).
Speed impacts model accuracy. If the drone ﬂies fast, two
successive point clouds will have fewer overlapping points,

resulting in errors in the SLAM’s pose transformations and
(therefore) pose estimates (for a reason similar to Fig. 4)
which leads to poor 3D model accuracy. So, ARES must ﬂy as
slow as possible.

Height impacts both accuracy and completeness. Because
LiDAR beams are radial, the higher a drone ﬂies, the less
dense the points on the surface of the building. Lower density
results in worse completeness. Accuracy is also worse, be-
cause the likelihood of matching the same point on the surface
between two scans decreases with point density. For instance,
the positioning errors for point densities of 2.2 points per m2
and 3.0 points per m2 are 2.5 m and 1.0 m respectively (graph
omitted for brevity). So, ARES must ﬂy as low as possible.

The drone’s path must ensure coverage of the buildings
rooftop and sides. Consider a narrow and wide building. The
drone must ﬂy several times over the building to capture all
its surfaces. If it ﬂies low, slowly, and at a parallel orientation,
the ﬂight duration can be signiﬁcant. Over long durations,
SLAM accumulates drift, which can worsen model accuracy
and completeness.

ARES designs equi-dense trajectories to control model com-
pleteness, and uses ofﬂine data-driven parameter estimation
to ﬁnd the choice of speed, height and orientation. To mini-
mize drift accumulation, ARES performs online drift estima-
tion and re-calibration. We describe these below.

Equi-dense Trajectories. An equi-dense trajectory ensures
that the resulting model is (a) complete, and (b) captures the
building with a point density that is no less than the speciﬁed
minimum target point density d.

Point density depends on LiDAR parameters and height.
The height (more generally, distance for vertical surfaces like
the sides of the building) at which a LIDAR ﬂies from a
surface governs the average density of points it obtains from
that surface; larger heights result in lower point density.

For a given LiDAR conﬁguration, we can compute the
point density as a function of height. For instance, for an
Ouster LiDAR with 64 beams, horizontal resolution of 1024,
and a vertical ﬁeld of view of 45°, to a ﬁrst approximation,
two consecutive beams are at an angular separation of 0.7°
( 45
64 ) and lasers from same beam are 0.35° ( 360
1024 ) apart. Using
geometry, for a surface at a distance h from the drone, we can
compute the projection of the LIDAR on that surface. Using
this projection, we can compute the point density throughout
the whole point cloud. Central regions of the point cloud have
much higher density than regions at the extremities.

Coverage depends on height. The density of points at
which a drone captures a surface depends on its height h.
Given a height h, Fig. 5 shows the coverage of the LiDAR
on a given surface. In general, the drone can only capture
a subset of this full coverage region with a minimum target
point density d (shown by target density region in Fig. 5).
Now, suppose the drone’s trajectory performs a rectilinear
scan over the surface, like the one shown in Fig. 6a. Then,

4

Figure 3: Parallel and perpendicular LiDAR orien-
tation

Figure 4: Impact of orientation on SLAM
positioning error.

Figure 5: Equi-dense trajectory scan
width

(a) The lateral ﬂight

(b) The longitudinal ﬂight

(c) An alternative ﬂight plan (d) Mid-ﬂight re-calibration

Figure 6: Model collection trajectory design

to ensure that ARES captures the entire surface at least at a
density d, the scan width must be equal to or smaller than the
width of the target density coverage region (Fig. 5).

ARES estimates scan width from LiDAR parameters. To
estimate the width of the target-density coverage region, ARES
uses the LiDAR parameters, and models LiDAR geometry,
to derive a function which returns the scan width for a given
target density d, and a given height h. It models the beams
of the LiDAR as having equal angular separation, so it can
compute the points at which these beams intersect with the
plane at a height (or distance) h away. Given a target point
density d, ARES can compute the largest width at this height
that will ensure minimum point density d.

This density guarantee is nominal; in practice, LiDARs
may drop some reﬂections if they are noisy [36]. Future work
can model this noise for better equi-dense trajectory designs.
Trajectory path planning, and parameter selection. ARES
uses a ﬁxed orientation, height and speed for the drone ﬂight;
it uses an ofﬂine data-driven approach to determine these.

Ofﬂine simulations to estimate parameters. As described
above, LiDAR orientation, and drone height, and speed deter-
mine how well SLAM can estimate positions to ensure accu-
racy and completeness. We could have tried to analytically
model the system to derive the optimal parameter choices.
Modeling the LiDAR is feasible (as we discuss above); model-
ing SLAM’s feature matching mathematically is much harder.
So, we resort to exploring the space of parameters using simu-
lation. Speciﬁcally, we use game engine driven photorealistic
simulators like AirSim [48] to sweep the space of parameters.
Then, we validate these results using traces that we capture
from our real-world prototype. We discuss this methodology
in greater detail in §3, where we show that there exists a sweet

spot in the parameter space that ensures high accuracy and
completeness while minimizing ﬂight duration. Speciﬁcally,
§3.4 shows that a parallel orientation, while ﬂying at a dis-
tance of 20 m from the surface (or lower, if necessary, to meet
the point density constraint) at 1 m/s gives the best accuracy.

A ﬁxed trajectory. Given these parameters, the building
geometry and scan width, ARES designs a drone ﬂight path.
Fig. 6 describes this for a building shaped like a rectangu-
lar solid; ARES supports other building shapes (§3). ARES’s
model collection trajectory starts from an origin; this point
deﬁnes the origin of the coordinate system for the resulting
3D model, then laterally traverses the building to capture the
two sides of the building (Fig. 6a). Its ﬂight path extends a
distance δ beyond the building edges to account for errors
in building geometry estimation. As it moves to each side
laterally, it moves up/down on each face to capture them at
the same minimum point density. Then, it returns to the origin,
and traverses longitudinally (Fig. 6b).

Avoiding LiDAR rotations. Why return to the origin?3
This loop closure maneuver is an important technique in
SLAM to correct for drift [20]. If loop closure were not neces-
sary, we could have designed a trajectory as shown in Fig. 6c.
However, this trajectory requires a rotation of the drone at
the dashed segment to ensure that the lateral and longitudinal
segments have the same drone orientation. Rotation can sig-
niﬁcantly increase SLAM drift; Fig. 7 shows an example in
which the green dashed line depicts the actual (ground truth)
drone trajectory, and the blue line SLAM’s estimated pose.
At the bottom right corner of the trajectory, when the drone

3Drones use ﬁducials (e.g., a drone landing pad with a distinctive design)

to identify the origin.

5

Side ViewTop ViewDirection of TravelLiDARLiDARPerpendicularParallelScan PlaneScan PlaneGroundHorizontal Coverage DistanceVertical Coverage Distance030456090LiDAR Orientation (degrees)1.01.52.02.53.03.5RMSE (m)Positioning error3040506070Visibility lifespan (s)Region visibilityTarget Density CoverageFull CoverageScan WidthhOriginOriginOriginExcessive drift!Originrotates, SLAM is completely thrown off.

Drift Estimation and Re-calibration. ARES uses return-to-
origin to re-calibrate SLAM drift. The second, longitudinal,
ﬂight starts a new SLAM session; to “stitch” the two sessions
together, ARES needs to compute a transformation matrix that
transforms the coordinate system of the ﬁrst session to that
of the second. ARES uses standard techniques for this. More
important, ARES designs the longitudinal ﬂight to start close
to the origin, which has two beneﬁts: (a) shorter ﬂight time
resulting in lower overall energy consumption and (b) less
drift accumulation.

Mid-ﬂight re-calibration for accuracy and ﬂight efﬁ-
ciency. Return-to-origin re-calibration might also be neces-
sary in the middle of one of the ﬂights (Fig. 6d), if the envi-
ronment is sparse and SLAM tracking fails. To combat this,
ARES could have added more loop closure maneuvers in the
lateral and longitudinal ﬂights. However, returning to origin is
an expensive operation in terms of the drone’s battery. Instead,
ARES actively monitors drift-error and returns to the origin
only when needed. In that case, the ﬂight resumes at the point
at which it detected excessive drift: the direct path from the
origin to that point is always shorter than the initial segment,
ensuring that the resumed ﬂight starts with a lower drift.

Using deviation from GPS trajectory to detect drift. How-
ever, detecting excessive drift is non-trivial, since ARES has no
way of knowing when SLAM’s position estimates are wrong,
because it does not have accurate ground truth. ARES lever-
ages the GPS readings associated with SLAM poses: each se-
quence of readings gives a GPS trajectory, and ARES attempts
to ﬁnd the best possible match between the GPS trajectory
(e.g., the green line in Fig. 7) and the SLAM-generated trajec-
tory (the blue line in Fig. 7). If there is a signiﬁcant deviation,
ARES assumes there is a drift and invokes re-calibration.

This approach is robust to GPS errors, since it matches
the shape of the two trajectories, not their precise positions
(Algorithm 1). Speciﬁcally, ARES continuously executes 3D
SLAM on the stream of compressed LiDAR frames from the
drone, and estimates the pose of each frame. It synchronizes
the GPS timestamps with the LiDAR timestamps (line 1),
then transforms GPS readings using the Mercator projection
(line 2). It then aligns the GPS trajectory and the SLAM-
generated trajectory using the Umeyama algorithm [53] to
determine the rigid transformation matrices (i.e., translation,
and rotation) that best align the SLAM and GPS poses (lines
3-4). ARES partitions trajectories into ﬁxed length segments
and after alignment, computes the RMSE between the two
trajectories in these segments, and uses these RMSE values
as an indicator of excessive drift: if the RMSE is greater than
a threshold ρ (lines 5-12), ARES invokes return-to-origin.

Generalizing to other structures. Some aspects of model
collection trajectory design depend on the geometry of the
structure whose model we seek to reconstruct. To simplify the
discussion and because geometric manipulations are not the

: SLAM poses S and GPS tags G

Input
Output: Imperfect regions I

0

0

← TimeSynchronization(S, G)

1 S

, G
0
a ← GPSToMercator( G
2 G
3 tcw ← UmeyamaAlignment( G
4 S
0
5 foreach s

a, S
a ← TransformTrajectory( S
a do

0)

0

0

0

0

0

0

0)
, tcw)

0

a, g
a−i, g

a−i in S
ri ← RMSE( s
if IsExcessive(ri) then
I.Append(ga−i)

a−i in G
a−i)

0

6

7

8

9

10

else

pass

end

11
12 end

Algorithm 1: Detecting Excessive Drift

focus of this paper, we have chosen rectangular buildings. We
demonstrate in §3 that our approach generalizes to other regu-
lar building geometries. It also generalizes to other structures
that can be tightly bounded within rectangular solids, such
as aircraft fuselages or blimps (Fig. 8, §3). To accommodate
arbitrary solid geometries, we expect that our techniques for
generating equi-dense trajectories, in-ﬂight re-calibration and
our conclusions about orientation, height and speed will apply,
but the actual trajectory design (Fig. 6) will need to match the
shape of the solid. We leave these extensions to future work.

2.2 Estimating Building Geometry

Given a region of interest, ARES conducts a reconnaissance
(“recon”) ﬂight to determine the boundary of the building’s
roof. It uses this boundary for trajectory design (§2.1).
Goal and requirements. Accurate model reconstruction re-
quires a low, slow, ﬂight which can be battery-intensive. The
recon ﬂight helps ARES scope its model collection to the part
of the region of interest that contains the building to reduce
battery usage. For instance, if in a large campus, buildings
occupy only a small fraction of the surface area, a fast recon
ﬂight can reduce overall drone battery consumption. So, we
require that the recon ﬂight should have as short a duration
as possible. In addition, (a) boundary estimation must not
assume prior existence of a 3D model of the building (prior
work in the area makes this assumption [31,43]); (b) boundary
estimation must be robust to nearby objects like trees that can
introduce error; and (c) buildings come in many shapes (e.g.,
rectangles, squares, hexagons etc.), so boundary estimation
must generalize to these.
The Recon Trajectory. Similar to model collection, recon
uses a rectilinear scan (Fig. 6a, Fig. A.3). Unlike model col-
lection, however, during recon the drone ﬂies fast (4 m/s)
and high (60 m above the building’s roof4), with the LiDAR
mounted in a perpendicular orientation in order to have the

4We assume the nominal building heights in an area are known, for

example, from zoning restrictions.

6

Figure 7: Rotation throws off
SLAM

Figure 8: To reconstruct other
structures (e.g., a blimp), ARES
wraps them in a rectangular
solid and plans a model collec-
tion trajectory for it.

Figure 9: Coverage and scan
widths for different orienta-
tions and heights.

Figure 10: ARES’s building
detector on a real building.

shortest duration ﬂight possible. We justify these parameter
choices in §3, but Fig. 9 depicts the intuition for these choices.
It shows, for an Ouster-64 LiDAR, the ground coverage area
as a function of height. Coverage is highest between 40 and
60 m. At a given height, a perpendicular orientation covers
a wider swathe of ground than a parallel orientation; this al-
lows ARES to use a larger scan width s (Fig. 9), resulting in
a shorter ﬂight. As with model collection, during this ﬂight,
ARES streams point clouds to its cloud component, which
runs the boundary detection algorithms described below.
Challenges and Overview. This ﬂight design poses two chal-
lenges for boundary detection. First, to detect the building’s
boundary, it is still necessary to align all point clouds to the
same coordinate frame of reference. In recon, ARES cannot
use SLAM because fast, high ﬂights can cause SLAM to
lose tracking frequently. We show below that, because bound-
ary detection can afford to be approximate, we can use GPS.
Second, high and fast ﬂights result in low point density, and
boundary detection algorithms must be robust to this.

ARES’s building boundary detection takes as input the area
of interest and outputs the GPS locations that constitute the
boundary of the building. Model collection uses these outputs
(§2.1). Boundary detection runs two different algorithms:
rooftop surface extraction, followed by boundary estimation.
Step 1. Surface Extraction. The cloud component receives
GPS-tagged compressed point clouds from the drone. It ﬁrst
uncompresses them, then computes the surface normal of
every point in the point cloud. A surface normal for a point
determines the direction normal to the surface formed by
points within a ﬁxed radius of the point. Then, ARES uses
RANSAC [22] (a plane-ﬁtting algorithm) to segment the
LiDAR points into groups of points that fall onto planes.
RANSAC is fast, but is not robust to outliers: (a) it combines
into one surface all LiDAR points that satisfy the same planar
equation, including disjoint sets of points (e.g., from trees) at
the same height; (b) point clouds can have multiple detected
planes (e.g., building rooftop, ground surface, vehicles etc.),
and RANSAC cannot distinguish between these.

To address the ﬁrst issue, in each plane, ARES removes
outlying points that are further away from neighboring points
in the same plane using a statistical outlier ﬁlter. Using the sta-
tistical outlier on every point cloud can be compute-intensive.

To this end, ARES tunes the statistical outlier’s parameters to
ﬁnd a sweet spot between ﬁltering accuracy and performance.
To ﬁnd the rooftop among the multiple detected planes, ARES
uses surface normals to compute surface statistics for each
plane (e.g., plane height, 3D centroid, normal variance etc.). It
uses these statistics to ﬁnd the rooftop in the extracted planes.
(As an aside, surface normal computation is computationally
intensive but is parallelizable, so we use a GPU to accelerate
this, as discussed in §3). Intuitively, the rooftop is a large,
uniformly oriented surface (surface normal variance is low)
that lies above the ground plane. ARES can eliminate the
ground plane as that plane whose points are consistent with
the drone’s height. So, it discards all planes that do not satisfy
this deﬁnition (this includes planes with high variances and
the ground surface). At the end of this step, ARES classiﬁes a
single plane as the roof surface.

To remove the possibility of false positives, ARES uses
majority voting to remove erroneous surface detections; it
classiﬁes a plane as the rooftop only if it detects it in multi-
ple consecutive frames. Lastly, even though the outlier ﬁlter
removes small sets of outliers in planes, it is unable to re-
move large clusters of points belonging to objects like trees
found nearby the building. For this, ARES forms clusters of
points based on their spatial relationships such that neigh-
boring points belong to the same cluster. This way, points
belonging to different objects form clusters. Since the roof
is normally a relatively larger surface, ARES simply discards
smaller clusters. To do this in near real-time, ARES ﬁnds the
right set of parameters for the clustering algorithm.

Step 2. Estimating the boundary of the building. In the
previous step, ARES obtains parts of the rooftop from each
point cloud. In this step, it uses the drone’s GPS location
to transform each surface to the same coordinate frame of
reference, then combines all surfaces into a single point cloud
that represents the extracted rooftop of the building. To extract
the boundary of the building, it extracts the alpha shape [13]
of the stitched point cloud. A generalization of a convex hull,
an alpha shape is a sequence of piecewise linear curves in
2-D encompassing the point cloud representing the rooftop.
This allows ARES to generalize to non-convex shapes as well.
Finally, to detect the boundary of multiple buildings, ARES
clusters the rooftop point clouds.

7

050100Altitude (m)0200040006000800010000Coverage area ( m2)50100150200250Scan width (m)Coverage areaParallel widthPerpendicular widthDrone trajectoryStitched rooftopGT boundaryRooftop detection pointsFig. 10 shows results from the building boundary detec-
tion algorithm on real data taken from our drone. The green
rectangle is the ground truth boundary of the building. The
blue points illustrate the drone’s recon trajectory and the grey
points depict locations where ARES detects a rooftop. Some
grey points are beyond the building’s boundary because the
LiDAR has a wide ﬁeld of view and can see the rooftop even
after it has passed it. The red points show the GPS stitched
3D point cloud of the building’s rooftop.

2.3 Point-Cloud Compression

LiDARs generate voluminous 3D data. For instance, the
Ouster OS1-64 LiDAR (which we use in this paper), gen-
erates 20 point clouds per second that add upto 480 Mbps,
well beyond the capabilities of even future cellular standards.
ARES compresses these point clouds to a few Mbps (1.2 to
4.0), using two techniques: viewpoint ﬁltering, and octree
compression. We describe these in §A.2.

3 Evaluation

We evaluate (a) ARES’s ability to reconstruct 3D models in
near real-time, and (b) the accuracy of these 3D models ( §2.1).
We also describe our parameter sensitivity analyses for data
collection, and evaluate boundary detection performance.

Implementation. Not counting external libraries and pack-
ages, ARES is 15,500 lines of code (discussion in §A.3).

Simulations. We evaluate ARES using a photorealistic simu-
lator, AirSim [48], that models realistic physical environments
using a game engine, then simulates drone ﬂights over these
environments and records sensor readings taken from the per-
spective of the drone. AirSim is widely accepted as a leading
simulation platform for autonomous vehicles and drones by
manufacturers, academia and leading industrial players. Air-
Sim has a parametrizable model for a LiDAR; we used the
parameters for the Ouster OS1-64 in our simulation experi-
ments. ARES generates trajectories for the AirSim drone, then
records the data generated by the LiDAR, and processes it to
obtain the 3D model. For computing the metrics above, we
obtain ground truth from AirSim. To build the ground truth
3D model, we ﬂew a drone equipped with a LiDAR several
times over the region of interest in AirSim (using exhaustive
ﬂights) and then stitched all the resulting point clouds using
ground truth positioning information from AirSim.

Real-world Traces. In addition, we have collected data from
nearly 30 ﬂights (each of about 25 minutes) on an M600Pro
drone with an Ouster OS1-64 LiDAR on a commercial com-
plex. For almost all experiments, we evaluated ARES on both
real-world and simulation-driven traces. Simulation-driven
traces give us the ﬂexibility to explore the parameter space
more (as we show below). However, we use real-world traces
to validate all these parameter choices and estimate recon-
struction accuracy in practice.

Metrics. In this section, we quantify end-to-end latency, 3D

Reconstruction
scheme

Accuracy
(m)

Comp.
(m)

Reconstruction
time (s)

Large building (100 m x 50 m x 20 m)

Ofﬂine-SDF
Ofﬂine-TP
ARES
ARES-raw

∞
0.87
0.21
0.14

∞
0.35
0.24
0.17

Small building (50 m x 50 m x 20 m)

Ofﬂine-SDF
Ofﬂine-TP
ARES
ARES-raw

3.36
0.62
0.25
0.21

1.30
0.43
0.14
0.09

3500
3900
719
719

2400
3300
656
656

Table 2: Reconstruction accuracy, completeness (comp.), and
time for two large buildings using three schemes: a) ofﬂine recon-
struction with shortest duration ﬂight (Ofﬂine-SDF), b) ofﬂine
reconstruction with ARES trajectory planning (Ofﬂine-TP), c)
ARES, and d) ARES with raw traces.

Reconstruction
scheme
Ofﬂine SLAM
Online w GPS.
ARES

Bandwidth
(Mbps)
480
3.80
3.80

Accuracy
(m)
2.30
1.60
0.13

Comp.
(m)
1.30
0.53
0.09

Table 3: Reconstruction quality of a real-world 70 x 40 x 20 m
building with online and ofﬂine approaches, relative to an uncom-
pressed trace.

model accuracy and completeness (§2.1), and positioning
error for a number of experimental scenarios. We also quantify
ARES’s energy-efﬁciency (using ﬂight duration as a proxy for
drone battery usage) and the computational capabilities of its
processing pipeline.

3.1

3D Model Reconstruction

Experiment setup. To evaluate the end-to-end performance
of ARES in building an accurate 3D model in near real-time,
we collected and reconstructed the 3D model of two buildings:
a) a large 50m x 100m x 20m (L x W x H) and, b) a small
50m x 50m x 20m building in Airsim. We then compared the
reconstruction accuracy of these models with two baseline
ofﬂine approaches (i.e., approaches that reconstruct the 3D
model after the drone lands): a) ofﬂine reconstruction5 with
the shortest duration ﬂight (Ofﬂine-SDF), b) ofﬂine recon-
struction with ARES’s trajectory planning (Ofﬂine-TP). We
calculated the accuracy and completeness of the models gen-
erated by these approaches by comparing them against ground
truth models generated from AirSim. Lower is better for ac-
curacy and completeness. For these experiments, ARES uses
compressed point clouds with bandwidth requirements that
are compatible with LTE speeds today (i.e., upload bandwidth
of 3.8 Mbps). ARES-raw shows accuracy and completeness
if ARES were to use raw point clouds; we study the effect of
compression on ARES model reconstruction more in §3.3.
ARES builds signiﬁcantly accurate models in less time. As

5We assume both ofﬂine approaches know the exact location of the build-
ing. Without this, SLAM accumulates signiﬁcant drift and reconstructions
are very poor.

8

Structure
type
Blimp
Small rect.
Star-shaped
Large rect.
Plus-shaped
H-shaped
Pentagonal

Flight
duration (s)
586
656
686
719
1024
1044
1361

Accuracy
(m)
0.23
0.25
0.37
0.21
0.31
0.34
0.31

Comp.
(m)
0.03
0.14
0.12
0.24
0.06
0.10
0.12

Table 4: ARES 3D reconstruction times (recon and model collec-
tion) and quality for different structures at low compression

Table 2 shows, ARES achieves lower than 25 cm accuracy
and completeness for both buildings and reconstructs the en-
tire buildings in just 10-12 minutes (the ﬂight duration). For
reconstruction quality, ARES does much better than the two
baseline approaches for two reasons: a) careful trajectory plan-
ning (TP), and b) in-ﬂight re-calibration. Since Ofﬂine-SDF
does neither, its accuracy and completeness values are very
large. To reconstruct the larger building i.e., 100 x 50 x 20m
building, the drone needs to ﬂy more and accumulates signiﬁ-
cant drift (as compared to the smaller building) and has poor
accuracy and completeness (shown by ∞). Ofﬂine-TP does
better because it uses ARES’s trajectory planning, but still
exhibits worse accuracy and completeness than ARES because
it lacks in-ﬂight calibration. This shows the importance of a
real-time quality feedback signal for reconstruction and high-
lights why ofﬂine reconstruction is not accurate even with
uncompressed traces. Though ARES uses compressed point
clouds, with in-ﬂight re-calibration and trajectory planning,
ARES’s models are upto 3.5x more accurate and complete.
If ARES were to use raw traces (ARES-raw) instead, loss of
accuracy and completeness is attributable to SLAM. Relative
to a raw trace, compression accounts for 4-7 cm difference
in accuracy and completeness. Moreover, ARES reconstructs
while the drone is in-ﬂight whereas the other two baseline
approaches do reconstruction ofﬂine on uncompressed point
clouds, incurring up to 4.6× higher reconstruction time6.

To get a visual feel for the degradation resulting from lower
accuracy and completeness, consider Fig. A.6, which shows
the ground-truth model, together with the ARES reconstruc-
tions. With an accuracy of 0.25 m (using 3.8 Mbps upload
bandwidth), the model closely matches the ground truth, but
the textured building surface on the right shows some small
artifacts. These artifacts arise not because of compression, but
because of SLAM imperfections (§3.3).
ARES generalizes to different building shapes. Our results
so far, and the descriptions in §2, have focused on rectangu-
lar buildings. ARES can accurately reconstruct a variety of
building types, as Table 4 shows. For these results, we use
ARES’s default ﬂight parameters and low compression. Larger
buildings (pentagonal, plus, and H-shaped) have larger ﬂight
durations partly because of their size and because they require
two re-calibration steps. Even then, for all buildings, ARES

6In practice, ofﬂine reconstruction will have higher reconstruction times

because we did not consider time to upload data to the cloud.

Compression scheme

Low Medium High

Upload bandwidth (Mbps)
Compression time (ms)
Network latency (ms)
Extraction time (ms)
SLAM time (ms)
Total processing time (ms)

3.80
62.9
15.5
5.05
34.5
117

2.50
65.3
14.6
3.07
30.8
113

1.27
65.3
13.7
3.03
23.9
106

Table 5: ARES enables real-time 3D reconstruction over LTE.
Each row shows per frame latency for that operation.

achieves tens of centimeter accuracy and completeness.
ARES generalizes to other types of structures. To show that
ARES can reconstruct other types of 3D structures e.g., air-
planes, helicopters etc., we modeled a real-world blimp [3]
(15 m x 60 m x 15 m) in AirSim. ARES encloses such struc-
tures within an enclosing rectangular solid (Fig. 8, §3.4). In
less than 10 minutes (Table 4), ARES reconstructed the blimp
with an accuracy of 23 cm and completeness of just 3 cm.
High accuracy is possible on real-world traces. Results
from our drone ﬂights validate that real-world data can
result in comparable performance (Table 3). In these ex-
periments, we reconstructed the 3D model of a real-world
70 m x 40 m x 20 m building. Because we lack a reference
ground truth for real-world data, we use the 3D model gener-
ated from raw, uncompressed traces. Ofﬂine reconstruction
using SLAM after the drone lands fails completely for the
same reasons mentioned above (i.e., no trajectory planning,
and no re-calibration). With GPS, it is possible to do in-ﬂight
reconstruction, however, the accuracy and completeness being
1.60 m and 0.53 m, make such 3D models unusable. With
ARES, on the other hand, we can build accurate, and complete
3D models whose completeness and accuracy are 9 and 13 cm
respectively (top-down view of 3D model in Fig. A.2).

3.2 Performance

Real-time 3D reconstruction over LTE is feasible. To val-
idate that ARES can collect a 3D model end-to-end in near
real-time, we used our implementation to conduct an experi-
ment in which we replayed 15 minutes worth of real-world
data on the drone compute (a Jetson TX2). It then compressed
and streamed point clouds over an LTE connection, to a 16-
core AWS VM with 64 GB RAM and a Nvidia T4 GPU.
(Our experiment only ran model collection; recon also runs
in real-time as discussed below).

To compress point clouds, we used three different levels of
compression (low, medium and high), corresponding to the
following combinations of octree resolution and point resolu-
tion (§2.3): (0.25, 0.10), (0.25, 0.25) and (0.50, 0.50) (effect of
compression is studied in §3.3). In our experiments with our
drone, we have found achievable LTE throughput to range
from 1-4 Mbps; we chose these compression levels to corre-
spond to this range. (In §3.3, we discuss how 5G deployments
would alter these conclusions).

At all three compression modes, ARES was able to stream
point clouds in real time (Table 5), and the total end-to-end

9

ARES component

Sub-component

3D frame compression
3D frame extraction
GPU normal estimation
RANSAC plane-ﬁtting
Outlier removal
Rooftop detection
Rooftop extraction
Rooftop stitching
Total time
LiDAR SLAM

Recon phase

Model collection

3D Reconstruction

Per-frame
execution time
(ms)
13.0 ±1.3
3.0 ±0.3
76.0 ±82
5.0 ±9.0
0.2 ±0.3
0.005
6.0 ±5.0
3.0 ±2.0
100 ±90.0
37.0 ±3.0
10.3 ±2.0

Table 6: Processing times for ARES components.

Structure
type
Star-shaped
Small rect.
Large rect.
Plus-shaped
H-shaped
Pentagonal

Flight dur. (s)
w/o
613
573
694
766
866
1062

w
686
656
719
1024
1044
1361

Accuracy (m)
w/o
1.05
0.63
0.96
0.51
1.10
1.47

w
0.37
0.25
0.21
0.31
0.34
0.31

Comp. (m)
w
w/o
0.12
0.39
0.14
0.40
0.24
0.39
0.06
0.08
0.10
0.27
0.12
0.42

Table 7: Flight duration (dur.) and reconstruction quality for build-
ings at low compression with (w) and without (w/o) re-calibration.

processing time per frame is about 110 ms, of which nearly
65 ms is network latency. Thus, ARES builds the 3D model
whilst the drone is in-ﬂight, adds a frame within 100 ms after
receiving it and can make available a complete 3D model of a
building in about a 100 ms after receiving the last frame!
ARES supports full frame rate processing. We proﬁled the
execution time of each component of ARES on a 15-minute
real-world trace. Point cloud compression executes on the
drone, and other components run on the AWS VM mentioned
above. We use the GPU to ofﬂoad the computation of surface
normals for building detection. During recon, point cloud
compression takes 13 ms per frame (Table 6). Extracting the
building geometry requires 100 ms per frame; with these num-
bers, we can sustain about 10 fps, so with a 20 fps LiDAR, we
process roughly every other frame. Despite this, our building
detector is quite accurate (§3.5). During model collection,
SLAM requires 37 ms per frame, and 3D reconstruction re-
quires about 10 ms (Table 6). The former uses 8 cores, so we
have been able to run these two components in a pipeline to
sustain 20 fps. Thus, a moderately provisioned, cloud VM
sufﬁces to run ARES at full frame rate with an end-to-end
compute latency of about 100 ms for reconnaissance, and
50 ms for model collection.

3.3 Ablation Studies

In this section, we explore how ARES’s techniques contribute
to 3D reconstruction quality and performance.

Re-calibration helps reduce error. To show the effect of
in-ﬂight re-calibration, we built online 3D models of the 7
large buildings mentioned above using ARES with (w) and
without (w/o) re-calibration in Airsim. In these experiments,

Velocity (m/s)

Height (m)

Flight
duration
(s)

Recon
Model coll.
Re-calib.
Total time

Accuracy (m)
Completeness (m)

0.5

30
136
673
476
1285
0.43
0.34

1.0

40
136
343
240
719
0.14
0.17

2.0

50
136
72
272
480
0.91
0.45

Exhaustive
@ 1 m/s
40
-
1520
-
1520
∞
∞

Table 8: ARES’s ﬂight duration for various parameter choices.

we evaluate ﬂight duration and reconstruction quality at low
compression (3.8 Mbps upload bandwidth) using accuracy
and completeness metrics. Table 7 shows that, on average, at
the expense of only 18% (150 seconds) longer ﬂights, ARES
improves accuracy by 65% (65 cm) and completeness by
55% (20 cm) with re-calibration ﬂights. Larger buildings
(plus-shaped, H-shaped, and pentagonal) require longer aerial
ﬂights which accumulate higher drift. This results in relatively
more re-calibration ﬂights and hence higher ﬂight duration.
Even so, ARES is able to reconstruct these buildings accu-
rately, demonstrating the importance of re-calibration.
Short ﬂight durations can produce accurate models.
ARES strives to reduce drone battery depletion in its design by
generating short duration ﬂights without sacriﬁcing accuracy
and completeness. To show that ARES’s defaults of 1 m/s
speed and 40 m height represent the best point in this tradeoff
space, we compare it to a lower, slower ﬂight (30 m, 0.5 m/s),
and a faster, higher ﬂight (50 m, 2 m/s). Table 8 shows that,
on the large building the lower, slower ﬂight has a longer
trajectory, resulting in more re-calibrations. The resulting
model has worse accuracy and completeness; re-calibration
can limit drift error, but not reverse it. A faster, higher ﬂight
has a slightly shorter trajectory, but the resulting model’s accu-
racy is very poor, because there is less overlap between point
clouds at higher speeds (§2.1). Finally, Table 8 also shows
the beneﬁts of a recon ﬂight: an exhaustive ﬂight that uses
the model collection parameters and does not perform recon
is 3× longer than ARES’s ﬂight (and accumulates signiﬁcant
drift, resulting in poor quality 3D models). Results on the
small building are qualitatively similar (omitted for brevity).
ARES builds accurate models at low bandwidths. We ex-
plore the impact of compression on accuracy and complete-
ness using (a) a synthetic building in AirSim and (b) real-
world traces. In addition to the three compression schemes
discussed earlier, we compute accuracy and completeness
for (a) raw point clouds, (b) viewpoint compression and (c)
lossless compression. The ﬁrst two alternatives provide cal-
ibration, while the third alternative explores reconstruction
performance under higher bandwidth as would be available,
for example, in 5G deployments.

As Table 9 shows, viewpoint ﬁltering achieves a 10× com-
pression throughout. Low compression is an order of mag-
nitude more efﬁcient beyond this. Despite this, ARES can
achieve high quality reconstruction. For the AirSim building,
consider accuracy: the raw-point cloud has an accuracy of

10

Compression
proﬁle

Completeness
(m)
Real-world 70 m x 40 m x 20 m large building

Bandwidth
(Mbps)

Accuracy
(m)

Raw
View-point
Lossless
Low
Medium
High

480.0
42.7
7.86
3.80
2.50
1.27

0.00
0.00
0.06
0.13
0.23
0.28

0.00
0.00
0.07
0.09
0.16
0.29

AirSim 50 m x 50 m x 20 m small building

Raw
View-point
Lossless
Low
Medium
High

480.0
42.7
7.86
3.80
2.50
1.27

0.21
0.21
0.22
0.25
0.66
0.73

0.09
0.09
0.10
0.14
0.21
0.24

Table 9: The impact of compression on accuracy/completeness.

0.21 m and 0.09 m, which is attributable entirely to SLAM
error. View-point ﬁltering does not degrade accuracy since it
only omits zero returns. Low compression, with a bandwidth
of 3.8 Mbps (easily achievable over LTE and over 100× more
compact than the raw LiDAR output) only adds 4 cm and
5 cm to accuracy and completeness (respectively). Medium
and high compression have signiﬁcantly poorer accuracy and
completeness. Similar results hold true for the other AirSim
building, so we omit for brevity.

Results from our drone ﬂights validate that real-world data
of a large building (dimensions in Table 9) can result in com-
parable performance (Table 9). Since we lack a reference
ground truth for real-world data, we use the 3D model gen-
erated from raw traces. With real-world traces, we can build
accurate, and complete 3D models that are within 9-13 cm
completeness and accuracy for low compression, and about
16-23 cm for medium compression, with respect to the uncom-
pressed traces. This suggests that highly compressed point
clouds do not signiﬁcantly impact accuracy and completeness.

improve-
Higher bandwidths provide centimeter-level
ments. The emergence of 5G promises larger upload band-
widths. However, as Table 9 illustrates, room for improvement
in accuracy and completeness is small. For the AirSim build-
ing, the gap between raw point clouds and low compression
accuracy (completeness) is only 4 cm (5cm); for the real-
world building, it is 7 cm (2cm). Lossless point cloud com-
pression, which requires 7.86 Mbps bandwidth comes within
1 cm of the raw point cloud accuracy and completeness for the
AirSim building and within 7 cm for the real-world building.
Lower target density worsens completeness. To demon-
strate that users can use the target density tuning knob to
obtain less complete models more quickly, we conducted an
experiment with ARES (with re-calibration) at two different
densities: 7.5 points per m2 and 1 point per m2. For the for-
mer, accuracy and completeness were 0.21 m and 0.14 m, and
for the latter 0.68 m, 0.17 m respectively. The lower density
ﬂight took 20% less time. As expected, completeness is worse
at lower target densities. At the lower density, accuracy is
worse because two adjacent scan lines have smaller overlap.

11

Figure 11: SLAM errors
for LiDAR orientations.

Figure 12: SLAM error for
different speeds and build-
ing distances.

Put another way, a side beneﬁt of specifying higher density is
the higher accuracy from scan line overlap.

3.4 Data Collection

ARES relies on a careful parameter sensitivity analysis (in
both simulation and on real-world traces) to determine model
collection ﬂight parameters: speed, height, and orientation
(§2.1). We have evaluated SLAM error for every combination
of drone speed (ranging from 0.5 m/s to 3 m/s), distance from
building (10 m to 40 m) and orientation (parallel to perpendic-
ular). We present a subset of these results for space reasons.
For these experiments, we use the trajectory described in
Fig. 6c. We report the average numbers for each experiment.
Best choice of orientation is parallel. Fig. 11 plots SLAM
error as a function of LiDAR orientation (Fig. 3) with respect
to the direction of motion. A parallel orientation has lowest
SLAM error (in Fig. 11, yaw 0° corresponds to parallel and
yaw 90° to perpendicular), because it has highest overlap be-
tween successive frames; as yaw increases, overlap decreases,
resulting in higher SLAM error (§2.1).
Best choice of distance is 20 m. Fig. 12 plots the SLAM
error as a function of the drone’s distance from the building
surface for the parallel orientation of the LiDAR. Error in-
creases slowly with height; beyond a 20 m distance from the
building, the error is more than 1 m. Point densities decrease
with height and affect SLAM’s ability to track features/points
across frames (§2.1). Rather than ﬂy lower, ARES operates at
a 20 m distance (or a 40 m height, since in our experiments
buildings are 20 m tall) to reduce ﬂight duration.
Best choice of speed is 1 m/s. Speed impacts SLAM posi-
tioning error signiﬁcantly (Fig. 12). Beyond 1 m/s, SLAM
cannot track frames accurately because of lower overlap be-
tween frames (§2.1). Below 1 m/s i.e., at 0.5 m/s, the ﬂight
duration (in seconds) is twice that of 1 m/s which results in
drift error accumulation. To achieve accurate reconstruction,
ARES chooses to ﬂy the drone at 1 m/s.
Real-world traces conﬁrm these observations. Real-world
traces (§A.5) validate these parameter choices ( Table A.3,
Table A.4) i.e., ﬂy slow, close to the building and in parallel.

3.5 Boundary Detection

Methodology and metrics. We use two metrics for building
boundary estimation: accuracy, and completeness. Accuracy

030456090LIDAR Yaw (degrees)1.01.52.02.53.03.54.0Positioning error (m)10203040Distance from building (m)2.55.07.510.0Positioning error (m)Distance RMSE0.51.02.03.0Speed (m/s)Speed RMSEis the average (2-D) distance between each point (quantized
to 0.1 m) on the predicted boundary and the nearest point on
the actual building boundary. Completeness, is the average
distance between each point on the actual boundary and the
nearest point on ARES’s predicted boundary. Lower values of
accuracy and completeness are better.

We use both real-world traces collected from our ARES
prototype and synthetic traces from AirSim. To compute
ground truth for real-world traces, we pin-pointed the build-
ing’s boundary on Google Maps [24]. For AirSim, we col-
lected the ground truth from the Unreal engine.

Boundary detection can run at full frame rate. Table 6
shows the time taken for each component of boundary de-
tection, on our real-world traces on a single core of a GPU-
equipped desktop. The average processing time per point
cloud is 100 ms, dominated by GPU-accelerated surface nor-
mal estimation (76 ms). This can sustain 10 fps. However,
our LiDAR generates 20 fps, so ARES uses every other frame,
without sacriﬁcing accuracy.

Boundary detection is accurate. To evaluate the accuracy
of ARES’s boundary extraction, we experimented with 3 real-
world traces collected over a 70 m x 60 m x 20 m building.
For these traces, ARES’s average accuracy is 1.42 m and its
completeness is 1.25 m, even at the highest compression and
when it samples every other frames.

Other results. We extensively evaluated ARES’s boundary
detection algorithm’s robustness to different building shapes
(Table A.1), point cloud compression (Table A.2), and point
cloud sub-sampling. Furthermore, we performed an extensive
parameter study to ﬁnd the right ﬂight parameters i.e., speed
(Fig. A.5), height (Fig. A.4) and orientation. For brevity, we
have included results and discussions in the appendix (§A.4).
We summarize two results. First, recon ﬂights can be short
(boundary detection is insensitive to point density and over-
lap). So, it can use perpendicular orientation, ﬂy at 60 m from
the building at 4 m/s. Second, it tolerates sub-sampling upto
one point cloud per second.

4 Related Work

Networked 3D sensing and drone positioning. Some re-
cent work has explored, in the context of cooperative per-
ception [42] and real-time 3D map update [12], transmitting
3D sensor information over wireless networks. Compared
to ARES, they use different techniques to overcome wire-
less capacity constraints. Robotics literature has studied ef-
ﬁcient coverage path-planning for single [52], and multiple
drones [37]. ARES’s trajectory design is inﬂuenced by more in-
tricate constraints like SLAM accuracy and equi-density goals.
Accurately inferring drone motion is important for SLAM-
based positioning [17]. Cartographer [27], which ARES uses
for positioning, utilizes motion models and on-board IMU’s
for estimating motion. In future work, ARES can use drone
orchestration systems [26], for larger campus-scale recon-

struction with multiple drones.

Ofﬂine reconstruction using images. UAV photogramme-
try [21] reconstructs 3D models ofﬂine from 2D photographs.
Several pieces of work [19, 30, 32, 51] study the use of cam-
eras (either RGB or RGB-D) on UAVs for 3D reconstruction.
Prior work [19] has proposed a real-time, interactive inter-
face into the reconstruction process for a human guide. The
most relevant of these [39, 41] predicts the completeness of
3D reconstruction in-ﬂight, using a quality conﬁdence pre-
dictor trained ofﬂine, for a better ofﬂine 3D reconstruction.
However, unlike ARES, this work requires human interven-
tion, computes the 3D model ofﬂine, requires close-up ﬂights,
cannot ensure equi-dense reconstructions, cannot dynamically
re-calibrate for drift and is not an end-to-end system. A body
of work has explored factors affecting reconstruction accu-
racy: sensor error [28], tracking drift, and the degree of image
overlap [19, 35]. Other work [18, 32, 33] has explored tech-
niques to reduce errors by fusing with depth information, or
using image manipulations such as upscaling. Unlike ARES,
almost all of this work reconstructs the 3-D model ofﬂine.

Ofﬂine reconstruction using LiDAR. 3D model reconstruc-
tion using LiDAR [34, 54] relies on additional positioning
infrastructure such as base stations for real-time kinematic
(RTK) positioning, and long-range specialized LiDAR to
achieve tens of centimeters model accuracy. ARES explores a
different part of the design space: online reconstruction with
sub-meter accuracy using commodity drones, GPS and Li-
DAR. More recent work has explored drone-mounted LiDAR
based ofﬂine reconstruction of tunnels and mines, but require
specialized LiDARs and a human-in-the-loop [7,16] for drone
guidance (either manually or by deﬁning a set of waypoints).

Rooftop boundary detection. Prior work has used infrared
sensors, RGB-D cameras [49] and a fusion of LiDAR [43]
with monocular cameras [31,40]. These assume a pre-existing
stitched 3D point cloud [43] or orthophoto [40] and are not
designed to operate in real-time. ARES’s boundary detection
accuracy is comparable to these pieces of work, even though
it does not rely on these assumptions.

5 Conclusions

In this paper, we have taken a step towards accurate, near-
real time 3D reconstruction using drones. Our system, ARES,
uses novel techniques for navigating the tension between
cellular bandwidths, SLAM positioning errors, and compute
constraints on the drone. It contains algorithms for estimating
building geometry, for determining excessive SLAM drift,
and for recovering from excessive drift. It can achieve re-
construction accuracy to within 10s of centimeters in near
real-time, even after compressing LiDAR data enough to ﬁt
within achievable LTE speeds. Future work can include us-
ing more sophisticated drone battery models, cooperative re-
construction of large campuses using multiple drones, and
generalizing further to structures of arbitrary shape.

12

References

[1] 5 Ways Drones are Being Used for Disaster Re-
lief. https://safetymanagement.eku.edu/blog/5-ways-
drones-are-being-used-for-disaster-relief/.

[2] Best Drones

for Natural Disaster Response.
https://www.droneﬂy.com/blogs/news/drones-ﬂooding-
sar-disaster/.

[3] Blimp. https://en.wikipedia.org/wiki/Blimp.

[4] Dronedeploy. https://www.dronedeploy.com/solutions/

rooﬁng/.

[5] Expediting Disaster Relief Services With Drone Tech-
nology. https://www.dronedeploy.com/blog/expediting-
disaster-relief-services-with-drone-technology/.

[6] Hover. https://hover.to/.

[7] Hovermap. https://www.emesent.io/hovermap/.

[8] How

Drones

Disaster Response.
in
https://www.precisionhawk.com/blog/how-drones-aid-
in-disaster-response.

Aid

[9] Kaarta. https://www.kaarta.com/products/contour/.

[10] Markets and markets: The drone service mar-
https://www.marketsandmarkets.com/Market-

ket.
Reports/drone-services-market-80726041.html.

[11] New Technology for Drone-based Emergency Response
Missions. https://cordis.europa.eu/article/id/415444-
new-technology-for-drone-based-emergency-response-
missions.

[12] Fawad Ahmad, Hang Qiu, Ray Eells, Fan Bai, and
Ramesh Govindan. CarMap-Fast 3D Feature Map Up-
dates for Automobiles. In 17th USENIX Symposium on
Networked Systems Design and Implementation (NSDI
20), Santa Clara, CA, 2020. USENIX Association.

[13] Nataraj Akkiraju, Herbert Edelsbrunner, Michael
Facello, Ping Fu, EP Mucke, and Carlos Varela. Alpha
shapes: deﬁnition and software. In Proceedings of the
1st International Computational Geometry Software
Workshop, volume 63, page 66, 1995.

[14] T. Bailey and H. Durrant-Whyte. Simultaneous local-
IEEE Robotics

ization and mapping (slam): part ii.
Automation Magazine, 13(3):108–117, 2006.

[15] P. J. Besl and N. D. McKay. A method for registration
of 3-d shapes. IEEE Transactions on Pattern Analysis
and Machine Intelligence, 14(2):239–256, 1992.

13

[16] Liam Brown, Robert Clarke, Ali Akbari, Ujjar Bhandari,
Sara Bernardini, Puneet Chhabra, Ognjen Marjanovic,
Thomas Richardson, and Simon Watson. The Design of
Prometheus: A Reconﬁgurable UAV for Subterranean
Mine Inspection. Robotics, 9(4):95, 2020.

[17] M. Bryson and S. Sukkarieh. Observability analysis
and active control for airborne slam. IEEE Transactions
on Aerospace and Electronic Systems, 44(1):261–280,
2008.

[18] E. Bylow, R. Maier, F. Kahl, and C. Olsson. Combining
depth fusion and photometric stereo for ﬁne-detailed 3D
models. In Scandinavian Conference on Image Analysis
(SCIA), Norrköping, Sweden, June 2019.

[19] S. Daftry, C. Hoppe, and H. Bischof. Building with
drones: Accurate 3d facade reconstruction using mavs.
In 2015 IEEE International Conference on Robotics and
Automation (ICRA), pages 3487–3494, 2015.

[20] H. Durrant-Whyte and T. Bailey. Simultaneous local-
ization and mapping: part i. IEEE Robotics Automation
Magazine, 13(2):99–110, 2006.

[21] A. Federman, M. Quintero, S. Kretz, J. Gregg, M. Len-
gies, C. Ouimet, and Jeremy Laliberte. Uav photgram-
ISPRS
metric workﬂows: A best practice guideline.
- International Archives of the Photogrammetry, Re-
mote Sensing and Spatial Information Sciences, XLII-
2/W5:237–244, 08 2017.

[22] Martin A. Fischler and Robert C. Bolles. Random sam-
ple consensus: A paradigm for model ﬁtting with appli-
cations to image analysis and automated cartography.
Commun. ACM, 24(6):381–395, June 1981.

[23] Yasutaka Furukawa and Carlos Hernández. Multi-view
stereo: A tutorial. Found. Trends Comput. Graph. Vis.,
9(1-2):1–148, 2015.

[24] Google. Google maps. https://www.google.com/maps.

[25] Michael Grupp.

evo: Python package for the eval-
uation of odometry and SLAM. https://github.com/
MichaelGrupp/evo, 2017.

[26] Songtao He, Favyen Bastani, Arjun Balasingam, Karthik
Gopalakrishnan, Ziwen Jiang, Mohammad Alizadeh,
Hari Balakrishnan, Michael J Cafarella, Tim Kraska,
and Sam Madden. Beecluster: drone orchestration via
predictive optimization. In MobiSys, pages 299–311,
2020.

[27] Wolfgang Hess, Damon Kohler, Holger Rapp, and
Daniel Andor. Real-Time Loop Closure in 2D LIDAR
In 2016 IEEE International Conference on
SLAM.
Robotics and Automation (ICRA), pages 1271–1278,
2016.

[28] S. Jiang, N. Y. Chang, C. Wu, C. Wu, and K. Song. Error
analysis and experiments of 3d reconstruction using a
rgb-d sensor. In 2014 IEEE International Conference
on Automation Science and Engineering (CASE), pages
1020–1025, 2014.

[29] Yurong Jiang, Hang Qiu, Matthew McCartney, Gaurav
Sukhatme, Marco Gruteser, Fan Bai, Donald Grimm,
and Ramesh Govindan. Carloc: Precise positioning of
automobiles. In Proceedings of the 13th ACM Confer-
ence on Embedded Networked Sensor Systems, SenSys
’15, page 253–265, New York, NY, USA, 2015. Associ-
ation for Computing Machinery.

[30] D. Lapandic, J. Velagic, and H. Balta. Framework for
automated reconstruction of 3d model from multiple
In 2017 International Symposium
2d aerial images.
ELMAR, pages 173–176, 2017.

[31] Hui Li, Cheng Zhong, Xiaoguang Hu, Long Xiao, and
Xianfeng Huang. New methodologies for precise build-
ing boundary extraction from lidar data and high resolu-
tion image. Sensor Review, 2013.

[32] J. Li, W. Gao, and Y. Wu. High-quality 3d reconstruc-
tion with depth super-resolution and completion. IEEE
Access, 7:19370–19381, 2019.

[33] Z. Li, P. C. Gogia, and M. Kaess. Dense surface re-
construction from monocular vision and lidar. In 2019
International Conference on Robotics and Automation
(ICRA), pages 6905–6911, 2019.

[34] Y.-C. Lin, Y.-T. Cheng, T. Zhou, R. Ravi, S.M. Hashem-
inasab, J.E. Flatt, C. Troy, and A. Habib. Evaluation of
UAV LiDAR for Mapping Coastal Environments. Re-
mote Sensing, 2019.

[35] Jean Liénard, Andre Vogs, Demetrios Gatziolis, and
Nikolay Strigul. Embedded, real-time uav control for
improved, image-based 3d scene reconstruction. Mea-
surement, 81:264 – 269, 2016.

[36] Sivabalan Manivasagam, Shenlong Wang, Kelvin Wong,
Wenyuan Zeng, Mikita Sazanovich, Shuhan Tan, Bin
Yang, Wei-Chiu Ma, and Raquel Urtasun. Lidarsim: Re-
alistic lidar simulation by leveraging the real world. In
Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition (CVPR), June 2020.

[37] Jalil Modares, Farshad Ghanei, Nicholas Mastronarde,
and Karthik Dantu. Ub-anc planner: Energy efﬁcient
coverage path planning with multiple drones. In 2017
IEEE international conference on robotics and automa-
tion (ICRA), pages 6182–6189. IEEE, 2017.

[38] Trevor Mogg. Austrian airlines is ﬂying a drone around
its planes for a good reason. https://www.digitaltrends.
com/cool-tech/austrian-airlines-is-ﬂying-a-drone-
around-its-planes-for-a-good-reason/.

[39] Christian Mostegel, Markus Rumpler, Friedrich Fraun-
dorfer, and Horst Bischof. UAV-based Autonomous
Image Acquisition with Multi-view Stereo Quality As-
surance by Conﬁdence Prediction. In Proceedings of
the IEEE Conference on Computer Vision and Pattern
Recognition Workshops, pages 1–10, 2016.

[40] Faten Hamed Nahhas, Helmi ZM Shafri, Maher Ibrahim
Sameen, Biswajeet Pradhan, and Shattri Mansor. Deep
learning approach for building detection using lidar–
orthophoto fusion. Journal of Sensors, 2018, 2018.

[41] V. S. Nguyen, T. H. Trinh, and M. H. Tran. Hole bound-
ary detection of a surface of 3d point clouds. In 2015
International Conference on Advanced Computing and
Applications (ACOMP), pages 124–129, 2015.

[42] Hang Qiu, Fawad Ahmad, Fan Bai, Marco Gruteser, and
Ramesh Govindan. Avr: Augmented vehicular real-
ity. In Proceedings of the 16th Annual International
Conference on Mobile Systems, Applications, and Ser-
vices (Mobisys), MobiSys ’18, pages 81–95, Munich,
Germany, 2018. ACM.

[43] Anandakumar M Ramiya, Rama Rao Nidamanuri, and
Ramakrishan Krishnan. Segmentation based building
detection approach from lidar point cloud. The Egyptian
Journal of Remote Sensing and Space Science, 20(1):71–
77, 2017.

[44] Grand View Research. Drone data services market
size and share. https://www.grandviewresearch.com/
industry-analysis/drone-data-services-market.

[45] Boris Schling. The Boost C++ Libraries. XML Press,

2011.

[46] Ruwen Schnabel and Reinhard Klein. Octree-based
In Proceedings of the 3rd
point-cloud compression.
Eurographics / IEEE VGTC Conference on Point-Based
Graphics, SPBG’06, page 111–121, Goslar, DEU, 2006.
Eurographics Association.

[47] Steven M Seitz, Brian Curless, James Diebel, Daniel
Scharstein, and Richard Szeliski. A comparison and
evaluation of multi-view stereo reconstruction algo-
rithms. In 2006 IEEE computer society conference on
computer vision and pattern recognition (CVPR’06),
volume 1, pages 519–528. IEEE, 2006.

[48] Shital Shah, Debadeepta Dey, Chris Lovett, and Ashish
Kapoor. Airsim: High-ﬁdelity visual and physical simu-
lation for autonomous vehicles, 2017.

14

[49] Kritik Soman. Rooftop detection using aerial drone
imagery. In Proceedings of the ACM India Joint Inter-
national Conference on Data Science and Management
of Data, pages 281–284, 2019.

[50] Stanford Artiﬁcial Intelligence Laboratory et al. Robotic

Operating System.

[51] L. Teixeira and M. Chli. Real-time local 3d reconstruc-
tion for aerial inspection using superpixel expansion. In
2017 IEEE International Conference on Robotics and
Automation (ICRA), pages 4560–4567, 2017.

[52] P. Tokekar, J. Vander Hook, D. Mulla, and V. Isler. Sen-
sor planning for a symbiotic uav and ugv system for
precision agriculture. In 2013 IEEE/RSJ International
Conference on Intelligent Robots and Systems, pages
5321–5326, 2013.

[53] Shinji Umeyama. Least-squares estimation of transfor-
mation parameters between two point patterns. IEEE
Transactions on Pattern Analysis & Machine Intelli-
gence, (4):376–380, 1991.

[54] Bin Wu, Bailang Yu, Qiusheng Wu, Shenjun Yao, Feng
Zhao, Weiqing Mao, and Jianping Wu. A graph-based
approach for 3d building model reconstruction from
airborne lidar point clouds. Remote Sensing, 9:92, 01
2017.

[55] Ji Zhang and Sanjiv Singh. Loam: Lidar odometry and
mapping in real-time. In Robotics: Science and Systems,
volume 2, 2014.

A Appendix

A.1 Drone compute

We ran a plane-ﬁtting algorithm, RANSAC, (a module that
we use in our pipeline) on a real-world point cloud trace us-
ing drone compute platform (Jetson TX2). We found that
(Fig. A.1) it takes the TX2, on average, 0.5 seconds to process
a single point cloud. The 64-beam LiDAR generates 20 point
clouds per second whereas plane-ﬁtting accounts for only 5%
of the entire execution time of our reconstruction pipeline.
Thus, the TX2 will take 200 seconds to process a single sec-
ond’s worth of data from the 64-beam LiDAR if we ran it at
20 frames per second. To this end, we ofﬂoad computations
from the drone to the cloud.

A.2 Point cloud compression
ARES uses two techniques (i.e., viewpoint ﬁltering and Octree
compression) to compress LiDAR point clouds to within 1.2
to 4.0 Mbps and transmit them over LTE.
Viewpoint ﬁltering. The OS1-64 LiDAR has a 360° horizon-
tal ﬁeld-of-view (FoV) and a 45° vertical FoV. In a drone-
mounted LiDAR (Fig. 3), only a portion of the full 360° con-
tains useful information. Beams directed towards the sky, or
towards objects beyond LiDAR range, generate zero returns.
Viewpoint ﬁltering removes these, and also removes returns
from the body of the drone. To compress point clouds, ARES
simply removes zero returns. In practice, we have found it to
be important to also ﬁlter out returns from the drone itself,
and also returns further away from the nominal range of the
LiDAR, since these are erroneous. So, ARES ﬁlters all points
closer than 5 m and further than 120 m.
Octree compression. After ﬁltering the point cloud, ARES
compresses the retained data using a standard octree compres-
sion algorithm [46] designed speciﬁcally for point clouds (and
hence this is better than data-agnostic compression techniques
like gzip). An octree is a three-dimensional tree data structure
where each node is a cube that spans a 3D region, and has ex-
actly eight children. The dimensions of the cubes at the leaves
of the tree determine the octree resolution. The numerical
precision used to encode point positions determines the point
resolution. Octree compression efﬁciently encodes empty
leaves or empty tree-internal nodes (those whose descendant

Figure A.1: Plane-ﬁtting on a TX2

15

0100200300400500LIDAR Frames0.30.40.50.60.70.8Execution time (s)Figure A.3: Recon ﬂight trajectory for ARES.

Figure A.2: Top down view of reconstructed 3D model for a large
real-world complex

leaves are empty). It also performs inter-frame compression
(similar to video encoders), efﬁciently encoding unchanged
leaves or internal nodes between two successive point clouds.
As we show in §3, we can parameterize octree compression to
achieve point-cloud transmission rates of 1.2-4 Mbps. ARES
chooses different values of octree resolution and point res-
olution, two parameters that govern the compressibility of
point clouds, to achieve point-cloud transmission rates of
1.2–4 Mbps (§3), well within the range of achievable LTE
speeds.

A.3

Implementation Details.

We have implemented ARES using the Point Cloud Library
(PCL [46]), the Cartographer [27] LiDAR SLAM implementa-
tion7, the Boost C++ libraries [45], and the Robotic Operating
System (ROS [50]). For the recon phase, we used functions
from the Point Cloud Library (PCL [46]) for plane-ﬁtting,
outlier removal and clustering. Our compression and extrac-
tion modules also use PCL and are implemented as ROS
nodes. The drift detection module uses a Python package
for the Umeyama alignment [25]. Not counting libraries and
packages it uses, ARES is 15,500 lines of code.

A.4 Recon Flight

The goal of the recon ﬂight is to survey the area and ﬁnd the
boundary of the structure as fast as possible. ARES uses a
ﬂight trajectory as shown in Fig. A.3 in which parallel scans
of length d are separated by a scan width s. In designing the
recon ﬂight, ARES can change the height, speed and LiDAR
orientation of the drone. To ﬁnd the right set of parameters,
we performed an exhaustive parameter sweep.
Optimum height for recon. To ﬁnd the optimum height for
the recon ﬂight, we planned recon trajectories for a 20 m
building (within a 300 m x 300 m area) in AirSim at different
heights (from 40 m to 90 m). We ﬂew the drone and ran
the boundary estimation on the collected highly compressed
LiDAR point clouds at 10 Hz. For each height, we collected
data and ran the boundary detection module ﬁve times. Higher
ﬂights increase scan width ( Fig. 9) at the expense of point

7We use Cartographer but it can be replaced by other LiDAR SLAM

algorithms like LOAM [55]

Figure A.4: Finding the right height for boundary detection ac-
curacy and battery efﬁciency in the recon ﬂight.

density. However, ARES’s boundary detection algorithm is
robust to lower density point clouds (up till 80 m) and can
accurately estimate the boundary of the building from a height
of upto 80 m. Fig. A.4 shows the 2D boundary detection
accuracy, completeness (lower is good) and ﬂight duration
(as a proxy for battery usage) as a function of the height of
the drone. We ﬁnd that at 80 m (or 60 m from the building),
ARES can jointly optimize for battery efﬁciency and boundary
detection accuracy. At 80 m, ARES can complete the recon
ﬂight in 150 seconds and estimate the boundary to within
2.5 m accuracy and completeness. Beyond 80 m, the scan
width and point density decrease. This results in longer ﬂights
and higher boundary detection accuracy and completeness.

Optimum speed for recon. To ﬁnd the optimum speed for
the recon ﬂight, we planned a recon trajectory for the drone
to ﬂy over the same 20 m building at a height of 80 m from
the ground. We ﬂew the drone in the planned trajectory at
speeds from 1 m/s to 8 m/s and ran boundary detection on
the highly compressed point clouds at 10 Hz. For each speed,
we collected data and ran the boundary detection module
ﬁve times. Fig. A.5 illustrates the effect of drone speed on
the boundary detection accuracy, completeness and the ﬂight
duration. A higher speed results in lower ﬂight duration but at
the expense of boundary detection accuracy and completeness.
Even then, ARES robustly extracts the boundary up till 6 m/s.
At higher speeds, the overlap between consecutive frames is
smaller and hence ARES cannot accurately stitch the frames
together. As such, ARES ﬂies the drone at the sweet spot i.e.,
4 m/s where the ﬂight duration is approximately 150 seconds
and accuracy and completeness are 2.5 m.

Optimum LiDAR orientation. LiDAR orientation controls
scan width and point cloud overlap. A parallel orientation
means larger overlap but small scan width s. On the other
hand, a perpendicular orientation means smaller overlap but
larger scan width s. Larger scan width s means a smaller ﬂight

16

sssssd406080Drone height (m)0246810Average boundary distance (m)180200220240260Time (seconds)AccuracyCompletenessFlight durationStructure
type
Star-shaped
H-shaped
Plus-shaped
Pentagon
Rectangular

Flight
duration (s)
150
150
150
150
150

Accuracy
(m)
1.39
1.31
1.35
2.58
2.50

Comp.
(m)
1.67
1.83
1.55
2.58
2.53

Table A.1: ARES boundary estimation accuracy, completeness
and ﬂight duration for different building types using high com-
pression.

duration (Fig. A.3). A large overlap means better scan match-
ing accuracy. Since ARES uses GPS for stitching in the recon
phase, so it is robust to the overlap. Hence, to minimize ﬂight
duration, it uses a perpendicular orientation of the LiDAR. We
conducted experiments (omitted for brevity) without different
orientations of the LiDAR and conﬁrmed that a perpendicu-
lar orientation minimzes ﬂight duration without any loss in
accuracy/completeness.

Boundary extraction for different buildings. To show that
ARES can accurately extract the 2D boundary of any build-
ing, we collected LiDAR traces of a drone ﬂying over ﬁve
different buildings in Airsim at a height of 80 m and speed of
4 m/s. We collected data over each building ﬁve times. Then,
we ran boundary detection on the highly compressed point
clouds at 10 Hz. We summarize the boundary detection accu-
racy, completeness and the ﬂight duration in Table A.1. As
expected, the ﬂight duration for all buildings is independent
of the underlying building. For all building types, ARES can
accurately extract the boundary of all buildings within 2.5 m
accuracy and completeness. This shows that ARES’s boundary
detection is scalable to all building shapes.

Effect of point cloud compression. To evaluate the effect
of point cloud compression on boundary extraction, we com-
pressed a real-world over the 70 m x 40 m x 20 m building
with the four different compression proﬁles described above.
Then, we ran our boundary extraction algorithm on the com-
pressed traces. Table A.2 shows that ARES’s boundary extrac-
tion algorithm is robust to compression. While bringing down
bandwidth by a factor of 377, for high compression, ARES
only trades off 36 cm in accuracy and 24 cm in completeness.
With higher bandwidths promised with the emergence of 5G,
ARES can achieve the same boundary extraction accuracy as
an uncompressed trace.

Effect of sub-sampling. ARES’s boundary detection algo-

Compression
proﬁle
Uncompressed
View-point
Lossless
Low
Medium
High

Required
bandwidth (Mbps)
480.0
42.7
7.86
3.80
2.50
1.27

Accuracy
(m)
1.09
1.09
1.09
1.09
1.13
1.45

Comp.
(m)
1.09
1.09
1.09
1.10
1.07
1.33

Table A.2: ARES boundary estimation accuracy and completeness
for different levels of compression.

rithm runs at 10 fps. A Ouster-64 beam LiDAR generates 20
point clouds per second. So, the boundary detection algorithm
must be robust to sub-sampling of point clouds. Our evalua-
tions show that, for a drone traveling at 4 m/s, it works well
even when using one point cloud every 3 seconds. Because
ARES’s boundary detection uses GPS for stitching, it does not
need overlap between 3D frames.

A.5 Data Collection
In this section, we perform a parameter sensitivity study to
ﬁnd the optimum parameters for running SLAM accurately
on real-world UAV ﬂights. To do this, we report positioning
error generated by SLAM. For the lack of accurate ground
truth in the real-world, we compare SLAM positions against
a GPS trace. Positioning accuracy is directly related to 3D
model RMSE because these poses are used to position 3D
point cloud in generating a 3D model. A higher positioning
error leads to a higher reconstruction error and vice-versa.
Effect of drone speed. Because GPS is erroneous, we only
draw qualitative conclusions. As Table A.3, taken from our
drone traces, shows, slower ﬂights have lower SLAM error
than faster one, and parallel orientations have lower SLAM
error than perpendicular.
Effect of drone height. Similarly, SLAM error increases with
height and, in real-world traces, the parallel orientation seems
to be signiﬁcantly better than the perpendicular orientation
(Table A.4). At a distance of 20 m from the surface of the
building, the parallel orientation has the minimum positioning
error i.e., 1.25 m. Beyond 20 m for parallel and 40 m for
perpendicular, SLAM loses track completely because of lower
point density.

LiDAR Orientation

Parallel
Perpendicular

Drone model collection speed (m/s)
1.5 m/s
1.25
3.12

3.0 m/s
3.33
7.64

Table A.3: Positioning errors for parallel and perpendicular Li-
DAR orientations at different speeds for real-world traces at a
vertical height of 20 m from the building.

Figure A.5: Finding the right speed for boundary detection accu-
racy and battery efﬁciency in the recon ﬂight.

17

2468Drone speed (m/s)012345678Average boundary distance (m)100200300400500600700Time (seconds)AccuracyCompletenessFlight durationFigure A.6: Reconstructed 3D models at different levels of compression. Top-left: ground truth, top-right: low compression, bottom-left:
medium compression, bottom-right: high compression

Drone model collection height from building (m)
40
5.41
∞

LiDAR
orientation
Parallel
Perpendicular
Table A.4: Positioning errors different LiDAR orientations at
different heights for real-world traces at 1 m/s.

20
1.25
2.18

60
∞
∞

18

Ground Truth 3D ModelAccuracy: 0.25mCompleteness: 0.14mAccuracy: 0.73mCompleteness: 0.24mAccuracy: 0.66mCompleteness: 0.21m