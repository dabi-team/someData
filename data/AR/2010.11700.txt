0
2
0
2

t
c
O
0
2

]

V
C
.
s
c
[

1
v
0
0
7
1
1
.
0
1
0
2
:
v
i
X
r
a

On Benchmarking Iris Recognition within a Head-mounted Display for AR/VR
Applications

Fadi Boutros1,2, Naser Damer1,2, Kiran Raja3,4, Raghavendra Ramachandra4,
Florian Kirchbuchner1,2, Arjan Kuijper1,2
1Fraunhofer Institute for Computer Graphics Research IGD, Darmstadt, Germany
2Mathematical and Applied Visual Computing, TU Darmstadt, Darmstadt, Germany
3 The Norwegian Colour and Visual Computing Laboratory, NTNU, Gjovik, Norway
4 Norwegian Biometrics Laboratory, NTNU, Gjovik, Norway
Email: fadi.boutros@igd.fraunhofer.de

Abstract

Augmented and virtual reality is being deployed in dif-
ferent ﬁelds of applications. Such applications might in-
volve accessing or processing critical and sensitive infor-
mation, which requires strict and continuous access con-
trol. Given that Head-Mounted Displays (HMD) developed
for such applications commonly contains internal cameras
for gaze tracking purposes, we evaluate the suitability of
such setup for verifying the users through iris recognition.
In this work, we ﬁrst evaluate a set of iris recognition algo-
rithms suitable for HMD devices by investigating three well-
established handcrafted feature extraction approaches, and
to complement it, we also present the analysis using four
deep learning models. While taking into consideration the
minimalistic hardware requirements of stand-alone HMD,
we employ and adapt a recently developed miniature seg-
mentation model (EyeMMS) for segmenting the iris. Fur-
ther, to account for non-ideal and non-collaborative capture
of iris, we deﬁne a new iris quality metric that we termed as
Iris Mask Ratio (IMR) to quantify the iris recognition per-
formance. Motivated by the performance of iris recognition,
we also propose the continuous authentication of users in
a non-collaborative capture setting in HMD. Through the
experiments on a publicly available OpenEDS dataset, we
show that performance with EER = 5% can be achieved
using deep learning methods in a general setting, along
with high accuracy for continuous user authentication.

1. Introduction

The commonly implemented security mechanisms in
HMD devices depend on pattern matching or Personal Iden-
tiﬁcation Number (PIN) [17], which is limited to individual

978-1-7281-9186-7/20/$31.00 ©2020 IEEE

knowledge. An alternative for PIN and pattern based pass-
words, earlier works have proposed to use biometric data
to authenticate the Virtual Reality and Augmented Reality
(VR/AR) users based on their natural interactions within the
virtual space [25]. However, this approach is limited to spe-
ciﬁc application scenarios where the user performs a de-
ﬁned physical task. Further, the VR/AR applications, such
as ﬁeld policing and crime scene investigation [36], may
require accessing or transmitting sensitive information. A
user in such a scenario should be properly and continuously
authenticated to prevent anonymous access to sensitive data
and to guarantee the safe use of the system in multi-user
environments. These, among many other application sce-
narios, rise a question regarding the security mechanism in
such headset devices.

Noting that HMD for AR/VR applications is commonly
built with internal cameras to enable gaze interaction with
the virtual environment, our assertion is that such a camera
can be used to verify the user’s identity based on iris pat-
terns, even in a continuous manner. As simplistic idea may
appear, in such a veriﬁcation scenario, and in comparison
to the traditional iris recognition applications, we observe
two main challenges. The ﬁrst one is related to the lim-
ited computational and storage power of HMD devices. The
current HMD device in the market is supplied with mobile
processor with up to 4 GB of memory. For instance, the re-
cent device from HTC Vive has a Qualcomm Snapdragon
835 processor with 4GB of memory. Under such hard-
ware constraints, large segmentation methods or feature ex-
traction models are not realistically deployable, especially
when considering parallel deployment with other applica-
tions. The second challenge concerns the non-cooperative
nature of the image capture process where the eye image
is captured without user cooperation at high frame rate to
enable seamless interaction with VR/AR application [16].

 
 
 
 
 
 
Common iris recognition systems require signiﬁcant coop-
eration of the user to capture high quality iris image with
widely opened eyes. While in the AR/VR scenario, the user
should not be required to continuously and intentionally
collaborate with the identity veriﬁcation sub-system imped-
ing the use of AR/VR, rather the system should run in the
background. Such an interaction result in sub-optimal iris
captures unlike the traditional iris recognition systems lead-
ing to performance degradation.

In this paper, we investigate the possibility of using iris
images captured from internal cameras of HMD for user
veriﬁcation. Considering the hardware constraints of HMD
devices, we ﬁrst utilize and adapt a recently developed
miniature segmentation model [5]. Further, to provide the
evaluation in a holistic manner, we explore three well-used
handcrafted feature extraction methods and four deep learn-
ing models for iris recognition. Considering the challenges
in adapting the iris recognition directly for HMD data, we
propose a new iris quality metric - Iris Mask Ratio (IMR) to
suitable select the iris image for veriﬁcation purposes. Fur-
ther, we propose a continuous authentication model using
iris recognition for verifying the users of HMD devices.

2. Related works on HMD

One of the most accurate and widely deployed ap-
proaches to extract iris features is inspired by the method
proposed by Daugman [14]. Further, iris recognition ap-
proaches have been proposed in the literature, whether they
are derivative of Daugman’s iris features or based on deep
learning techniques. Sun and Tan [45] presented ordi-
nal measures (OM) as a novel iris features. Damer et al.
[12] proposed the transformation of iris features to a rota-
tion invariant space. K. Miyazawa [31] proposed an ap-
proach based on Discrete Fourier Transforms (DFT). More
recently, an approach was presented by Chen J.et al. [8],
where they build a new set of iris features based on Human-
interpreted Crypts Features.

Recently, several works have explored the use of deep
learning techniques for iris recognition. Liu et al [28] pro-
posed DeepIris network to learn pairwise ﬁlters and the
deep representations of heterogeneous iris images. Gang-
war and Joshi [15] proposed the DeepIrisNet network
achieving superior performance on multiple datasets [35]
datasets. Nguyen et al. [47] investigated the performance
of several pre-trained CNNs (on ImageNet) for iris recogni-
tion. Similarly, Zanlorensi [49] ﬁne-tuned ResNet and VGG
models for cross-spectral ocular recognition.

Deep learning approach has been used for iris segmen-
tation. Some of approaches are based on U-Net [29],
Fully Convolutional Network (FCN) [27], Context Encod-
ing Network [9], Cascade Reﬁnement Network (CRN) [5],
Encoder-Decoder [34], Fast-SCNN [37] and SegNet [42].
A few of these works addressed efﬁcient segmentation e.g.

for embedded device. Among the listed models, the Eye-
MMS model proposed Boutros et al. [5] and MinENet pro-
posed by Perry and Fernandez [34] are the smallest models
with 80K and 222K trainable parameters, respectively.

Along the lines of verifying the HMD user, a recently
H2020 EU-funded project is investigating the use of AR
headsets for border guards, in both crowded border-crossing
points and remote locations [1]. Such application scenarios
need users to be veriﬁed in a continuous manner, and with-
out disturbance where the information processed and dis-
played are of secured nature. Lee et al. [26] designed a pro-
totype of gaze estimation and iris recognition using a cam-
era attached inside a wearable headset. However, this ap-
proach required user cooperation as the experimental setup
assumed that the user should look directly to the camera
when authentication is needed. Although the reported veri-
ﬁcation performance was very promising, the performed ex-
periments are different from the real HMD scenario where
the eye is captured on the ﬂy without user cooperation at a
high speed frame rate (around 200Hz [16]).

A recent work by Bastias et al. [3] proposed a method for
iris reconstruction from several 2D near-infrared iris images
from a custom sensor for 2D image capturing mounted on
a wearable headset. However, the work did not target wear-
able headsets speciﬁcally, rather used it to create a captur-
ing setup and proposed a consequent veriﬁcation approach.
Very recently, Boutros et al. [6] evaluated the possibility
of using images captured from an HMD internal camera for
periocular biometric. The work also presented a reference
sample selection strategy to enhance the periocular veriﬁca-
tion performance within HMD environment.

Addressing the internal capture in HMD, recently, the
OpenEDS database was released [16] which is a large scale
eye images dataset captured using a virtual-reality HMD de-
vice with two eye-facing cameras. Based on the OpenEDS,
Facebook hosted competitions for two main challenges,
semantic segmentation, and synthetic eye generation [2],
targeting gaze-tracking solutions. To deal with the ab-
sence of a large scale identity-speciﬁc images captured from
HMD cameras, Damer et al.
[10] proposed an identity-
preserving synthetic ocular image generation model based
on OpenEDS that can be used for training propose. Al-
though the target of OpenEDS is gaze-tracking, it opens
an opportunity to evaluate iris recognition in HMD setups
forming the basis of our work. Motivated by the limited
works investigating iris biometrics in HMD devices, we in-
vestigate the suitability of iris recognition by benchmarking
six different algorithms. Further, we propose a continuous
authentication approach suitable for HMD environment.

3. Traditional iris recognition in HMD

The main goal of this work is to provide a comprehensive
study on iris recognition solutions within AR/VR environ-

Figure 1: Overview of iris recognition workﬂow in HMD.

ment. The generic iris recognition pipeline consists of four
main steps: a) image acquisition, b) image preprocessing,
d) feature extraction, e) comparison and decision making.
These steps are illustrated in Figure 1. The image captured
in the HMD contains areas beyond the iris, therefore, we
segment the iris region and extract the normalized iris. For
different recognition methods, we utilized the normalized
iris or the coarse iris region to extract iris features. This sec-
tion presents the used segmentation model, proposed qual-
ity metric, and the different feature extraction methods.

3.1. Iris segmentation and normalization

HMD captured images contain regions beyond the iris,
as shown in Figure 1. As we baseline iris recognition using
approaches with different requirements, we apply two kinds
of segmentation, ﬁne and coarse segmentation. Consider-
ing the minimalistic computational power of HMD devices,
we employ computationally-light weight segmentation. As
the segmentation should be able to scale-up to images ac-
quired in non-collaborative scenario, we note that Weighted
Adaptive Hough and Ellipso-polar Transform [48] or the
Contrast-adjusted Hough Transform [30] may not be suit-
able as the iris images are non-ideal.

Given the requirements stated above, we opted to use the
Miniature Multi-scale Segmentation Network (Eye-MMS)
approach proposed recently [5]. The compact size of the
Eye-MMS model with only 80K parameters suits minimal-
ist hardware speciﬁcations while achieving high accuracy as
demonstrated on the OpenEDS database [16]. In compari-
son to the one of the latest general light-weight semantic
segmentation methods, the Fast-SCNN model [37] contains
1.1 million parameters. The MinENet proposed by Perry
and Fernandez [34] achieved slightly better performance on
the OpenEDS database, however, with 222K parameters.

We thus employ Eye-MMS model[5] as detailed in the
Section 5 to segment the iris region by labelling the back-
ground, sclera, iris, and pupil areas. To neglect any irregu-
larly labeled pixels, we post-process the semantic segmen-
tation by ﬁtting a convex hull around the largest found con-
tours around each of the considered labels. These hulls rep-
resent the borders of each label and from the labelled iris
region, we extract the iris for normalization. To normalize
the iris region efﬁciently, we start by deﬁning a general cir-
cular border that contains the pupil and the iris. The pupil
circular region is deﬁned around its center of moment and

has the radius of the closest (from the center of the moment)
pupil labeled pixel. The circular border between the iris and
the sclera is also deﬁned centered around the pupil center
of moment and has the radius of the distance between this
center and the furthest (from the center of the moment) iris
labeled pixel. The iris is normalized using the rubber sheet
model by unrolling it to a rectangular image [13]. Simi-
larly, a mask map is created with zero values for each label
not belonging to iris, and ones for each pixel labeled as iris.
Alongside the ﬁne segmentation, we also carry out a coarse
segmentation by cropping the area of eye image containing
only the iris and pupil. To do that, we use the segmenta-
tion labels and calculate the rectangular bounding box that
contains the pupil and the iris. An example of the segmenta-
tion, normalization, masking, and coarsely segmented iris,
is presented as part of the workﬂow in Figure 1.

3.2. Proposed quality metric for iris selection

To account for the non-cooperative nature of the iris im-
age acquisition in HMD devices where the iris images are
non-ideal, we deﬁne a new quality metric for choosing the
iris images prior to feature extraction. We introduce a new
metric by deﬁning the Iris Mask Ratio (IMR) as a ratio of
the actual iris area (mask neglected) size to the whole nor-
malized image size. A higher IMR indicates that a larger
proportion of the iris is visible in the image, making it more
suitable for feature extraction and comparison. The IMR is
used to select the reference image from the reference images
pool of each identity, i.e. the image with the highest IMR is
selected from each reference pool to be the reference.

The deﬁned quality metric is also used as a basis for
choosing the probe images appropriately, i.e. selecting iris
images to be used for veriﬁcation from the series of iris im-
ages. We threshold the IMR value of iris images to neglect
images with low IMR. To validate the applicability of the
proposed quality metric, we correlate it to veriﬁcation per-
formance as explained in the experimental section.

3.3. Iris feature extraction and comparison

We utilize three well-established and complementary
handcrafted iris feature extraction methods owing to the
robustness and time-tested applicability for iris recogni-
tion in various constrained and unconstrained settings [7,
44, 24, 33, 38, 41]. The iriscodes in the ﬁrst handcrafted
method are extracted using the classical Gabor features as
proposed by Daugman [13] and we employ the general-
ized version of the same by using 1D Log-Gabor features
[30]. The second handcrafted approach uses Discrete Co-
sine Transform (DCT) coefﬁcients of overlapped angular
patches from normalized iris images to derive the iriscodes
[32]. The third handcrafted approach extracts the iriscodes
using the Cumulative-Sum-Based Change Analysis [22].
For all the three handcrafted feature extraction schemes,

we employ Hamming Distance (HD) measure to compute
the similarity between two irises. Further, noting the early
works pointed the beneﬁts of the Shifted Hamming Dis-
tance (SHD) [39, 40] to account for rotational and displace-
ment invariance, we use the SHD by shifting the iriscodes
by 8 bit in both directions to obtain the scores. The mini-
mum of all the SHDs computed is further used as compari-
son scores for reporting the performance in this work.

Additionally, we investigate the performance of four
CNN solutions, the DeepIrisNet [15], the MobileNetV3
[19], and transfer learning on ResNet [18] and DenseNet
[21]. DeepIrisNet [15] is a speciﬁcally designed for iris
recognition and provides two architectures, DeepIrisNet-
A and DeepIrisNet-B, both showing excelent performance.
We choose the simpler architecture, DeepIrisNet-A, for our
evaluation. In the second and third approach, we employ
Off-the-Shelf CNN features by applying transfer learning
on ResNet [18] and DenseNet [21] models pre-trained on
the ImageNet [23]. Our choice of ResNet-50 and DenseNet-
201 was based on their promising reported accuracy for
iris recognition [47, 49]. In the fourth approach, we learn
the features using MobileNetV3 [19], which is designed
for low resource application. MobileNetV3 considers sev-
eral optimizations in addition to architecture search to cre-
ate two networks, MobileNetV3-Large and MobileNetV3-
Small. The networks are designed for high and low resource
application and achieved a higher classiﬁcation accuracy
than previous lightweight models such as MobileNet series
[20, 43] and Mnasnet [46]. We employ MobileNetV3-Small
architecture due to its fewer parameters (3 million). We
train and evaluate the CNN models on the normalized iris
to be compatible with earlier works on deep iris recogni-
tion. We additionally evaluate the deep representation ex-
tracted from coarsely segmented iris region as considered
in unconstrained environment by some works [50, 49], es-
pecially when the accurate segmentation is challenging.

For all models, we modiﬁed the number of classes in the
classiﬁcation layer to the number of identities in our train-
ing set (95 identities). To adapt DeepIrisNet and MobileNet
for iris images from HMD devices, we trained these mod-
els from scratch on training data of OpenEDs database [16]
with softmax classiﬁer. Besides, we ﬁne-tuned the entire
DenseNet-201 and ResNet-50 models on the same train-
ing data [16]. During testing, for each model, the softmax
layer is removed and features are extracted f from the last
layer which is of dimension 1 × 1 × 4096 in DeepIrisNet,
7 × 7 × 2048 in ResNet-50, 7 × 7 × 1920 in DenseNet-201
and 1 × 1 × 1280 in MobileNet.

4. Iris continuous authentication in HMD

Given that the iris images are captured continuously dur-
ing the interaction with the HMD, we also propose a frame-
work to continuously authenticate the user employing the

captured iris images. We motivate our work based on the
trust model for keystroke continuous authentication intro-
duced by Bours et al. [4] and adapted for multi-biometrics
in [11]. The trust model measures the conﬁdence of the cur-
rent user being a genuine or imposter user. This trust in the
genuineness of the user is expressed as a trust value. The
adjustment of the trust value depends on the penalty-and-
reward function which adjusts the trust value based on the
sampling over time/actions. In HMD, the images are con-
tinuously captured for eye tracking. Therefore, the penalty-
and-reward function in our model is triggered by each im-
age acquisition. We propose a variable penalty-and-reward
function where the trust value is updated based on the dis-
tance between the comparison scores of iris and the thresh-
old at the Equal Error Rate (EER). If the score exceeds the
threshold, the trust value is increased and the user is treated
as the genuine user, vice versa for imposter. The trust value
is continuously increased (reward) when the score is higher
than the threshold and decreased (penalty) when the score
is lower than the threshold or if there is a gap between
two captured images (e.g. failing to pass the IMR quality,
IMR Th). If the trust value falls below the threshold, the
system logs the user out. The trust value is given as:

T V =





T
max(T V − α, −1)
min(T V + (cs−T ), 1)
max(T V − (T −cs), −1)

at startup
if IM R < IM R T h
if cs ≥ T
if cs ≤ T

where cs is the comparison score and T is the EER thresh-
old. At the start of using HMD device, the trust value is set
to T , where T is the threshold for punishment or reward.
If there is a sample gap, a penalty of α = 0.01 (can be
adjusted for application) is applied. The lower and upper
limits of the trust value are set to −1 and +1, respectively.

5. Experimental setup

To evaluate the applicability of iris recognition and
the proposed continuous authentication model, we employ
OpenEDs database [16]. OpenEDs database is acquired
using a virtual-reality HMD with two synchronized eye-
facing cameras at a frame rate of 200 Hz under controlled
illumination. The semantic segmentation dataset included
12759 images of 152 individuals with a pixel resolution of
640x400. The data is split into 8916 eye images for train-
ing, 2403 for validation, and 1440 for testing as described
in [16]. The test split is not available publicly yet and thus,
is not used in this work. Since the semantic segmentation
labels are available only for both training and validation
splits, the segmentation model is trained on training split
and evaluated on validation split. The deep learning meth-
ods are trained on the training split of the semantic segmen-
tation dataset and tested on the validation split. We ran-
domly selected a subset of 190 images (two per identity) of
the training split to validate the model for early stopping

Figure 2: IMR value histogram of the iris images of full reference pool (a), selected references (b), and all probe samples (c).

to avoid over-ﬁtting the model during the training. The
normalized iris is reshaped to 128 × 128 as proposed in
DeepIrisNet approach. For DenseNet, MobileNetV3 and
ResNet, the normalized iris are resized to 224 × 224 to
match their input layer size. Further, the coarsely seg-
mented iris region was resized to 224 × 224 to match in-
put layer size of the evaluated models. As the handcrafted
feature extraction approaches do not require training, they
are directly evaluated using the validation split. The nor-
malized iris images are resized to a dimension of 512 × 64
pixels for extracting the handcrafted features. The valida-
tion data is identity-disjoint from the training set. Each of
the 28 validation identities contained between 37 and 128
images captured consecutively and on average 86 per iden-
tity. The image set of each identity is split into reference
and probe images. The ﬁrst 10 images for each identity are
considered as reference. Further, the initial reference image
is chosen by the newly deﬁned quality metric i.e., highest
IMR. The consequent ﬁve images are neglected to create a
time gap between reference and probe images. All conse-
quent images for each identity are considered as probes.

Segmentation: We train the Eye-MMS model for 40
epochs with the training parameters as in [5]. The results
are post-processed as described in Section 3.1. The perfor-
mance of the segmentation model is reported as Intersection
over Union (IoU) ratio of each of the four different seg-
mented areas between the ground-truth and predicted label.
The unweighted mean IoU of the areas is also reported to
provide an overall indication of the performance.

Deep learning model training setup: The investigated
models are trained using Adam optimizer with learning rate
of 1e-4 and batch size of 64. Each model is trained twice,
once using coarsely segmented iris as input and once us-
ing normalized iris. We set the initial number of epochs to
100 and the early stopping patience parameter to 5, caus-
ing DeepIrisNet, MobileNetV3, DenseNet, and ResNet to
stop after 41, 19, 21 and 29 epochs, respectively on normal-
ized iris training data, and after 23, 19, 23 and 18 epochs,

IMR 0
IMR 0.1
IMR 0.2
IMR 0.3
IMR 0.4
IMR 0.5
IMR 0.6
IMR 0.7

SG 0-1
1951
1856
1851
1856
1850
1805
1666
1397

SG 2-3
0
13
13
13
16
29
53
57

SG 4-5
0
2
2
2
2
5
15
13

SG 6-7
0
7
7
7
7
7
8
16

SG >8 Max SG

0
1
1
1
1
1
4
17

0
9
9
9
9
10
15
19

Table 1: This table provides a view on the size and amount
of sequence sample gaps (SG) (right most column). Each
column represents a certain gap and the numbers of the ta-
ble represent the occurrences of this gap with a certain IMR
threshold setting e.g. SG 0-1 is the case where two con-
secutive captures do not have any neglected capture or one
neglected capture between them. With a higher IMR thresh-
old, the higher sample gap occurs more often.

respectively for the coarsely segmented iris images.

Iris veriﬁcation: We evaluate the veriﬁcation perfor-
mance of deep learning methods with cosine-distance for
comparison. For each of the deep learning methods, we
evaluated representation extracted from coarsely segmented
iris region and normalized iris images, resulting in evalu-
ation of eight models. For handcrafted feature extraction
methods, the LG and the DCT approaches are evaluated
with HD and SHD distance for comparison, resulting in four
evaluation settings, noted as LG-HD, LG-SHD, DCT-HD,
and DCT-SHD. A ﬁfth setup uses the CSBCA features with
the HD distance for comparison, as the nature of the feature
extraction does not beneﬁt from the computationally more
expensive SHD distance. Each of the settings is evaluated
with IMR thresholds 0.0 and 0.7 computed on the probes.
The veriﬁcation performance is reported as Receiver Op-
erating Characteristic (ROC) curves, Area under the curve
(AUC), False Match Rate (FMR) at ﬁxed False Non-Match
Rate (FNMR) (FMR10, the lowest FNMR for FMR≤10%),
and Equal Error Rate (EER).

6. Results

Several input image sizes were evaluated and the highest veriﬁcation

performance was achieved using input size of 128 × 128

In this section, we present the results of the iris segmen-
tation, followed by a view on iris image selection by the

proposed quality metric. We then discuss iris veriﬁcation
followed by the results of the continuous authentication.

Segmentation results: The Eye-MMS model used in this
work achieved the following IoU values: a) 97.96% on
background, b) 77.85% on sclera, c) 93.72% on iris, d)
92.19% on pupil, and e) 90.43% unweighted mean IoU.

Results on iris selection using proposed IMR: We ana-
lyze the applicability of the proposed quality metric - IMR
on the veriﬁcation performance by using different thresh-
olds. We ﬁrst present a histogram of the IMR values mea-
sured on the reference data and the probe data. Figure 2.a
presents the histogram of the IMR values of the images in
the reference pool. Some samples scored lower than 0.4
IMR indicating a low proportion of visible iris. When the
samples with the highest IMR are selected for each unique
identity, the lowest IMR value corresponds to over 0.7, as
seen in Figure 2.b. On the other hand, Figure 2.c shows the
histogram of the IMR values of probe samples. One can no-
tice that the probe samples contain some images where the
iris was not visible at all, i.e. closed eyes, motivating the
selection of good quality irises. The plot also shows that
most probe samples had an IMR value between 0.6 and 0.9.
Further, this IMR thresholding creates a time (sample)
gap in the veriﬁcation process, which is signiﬁcant if the
veriﬁcation is performed in a continuous nature. Therefore,
we analyze the amount of gap (measured by the number
of images under the threshold between accepted images) in
the probe consequent samples introduced by eight different
IMR thresholds ( 0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6 ,and 0.7).

When samples with lower IMR values are neglected,
this would produce a sample gap (SG) between consecu-
tive frames. Noting that the data is captured at a frame rate
of 200 Hz, every single SG corresponds to 5ms of time.
Having a large SG might affect the applicability to con-
tinuous authentication or, if a large SG is allowed, it will
give an attacker the time frame to gain access. Therefore,
it is important to study the amount and frequency of SG in-
duced by neglecting captures with low IMR. To do that, we
present a thorough analyses in Tables 1. Tables 1 presents
the occurrences of different SGs in each IMR thresholding
setup. As noted from the Tables 1, SG 0-1 indicates the
occurrences of two consecutive captures having no or one
neglected capture between them, SG 2-3 indicates the oc-
currences of two consecutive captures having two or three
neglected captures between them, and so on for SG 4-5, SG
6-7, while the SG>8 indicates the total number of occur-
rences of SG equal to 8 or more. The column indicated by
”MAX SG” points out the maximum gap occurring in the
setting. Table 1 thus shows that increasing the IMR thresh-
old might result in unwanted sample gaps up to 19, in the
case of a 0.7 IMR threshold. However, an IMR threshold of
0.5 will only result in a few sample gaps above 6, and a max

SG of 10.

Results on iris recognition: The veriﬁcation perfor-
mances of the different evaluated deep learning approaches
are presented as ROC curves in Figure 3. Beside, the EER
and FMR10 values are presented in Table 2 and Table 3
for handcraft methods, deep learning methods, respectively.
Each of the Figures 3.a-d shows the ROCs achieved at dif-
ferent IMR thresholds. The ROCs achieved by the hand-
crafted features are not presented for space limitation as
they result in much lower performance compared to deep
learning methods, see Table 2. We make the following main
observations from the set of experiments as noted below:

• One can notice that neglecting captures with low IMR en-
hances the performance of all the evaluated algorithms.
This can be clearly explained as the iris images with high
IMR present more textural information leading to accu-
rate veriﬁcation performance. The same conclusion can
be made when looking at the EER and FMR10 values in
Table 2 and Table 3 where setting the IMR threshold to
0.7 reduces the error rates consistently.

• The achieved veriﬁcation performance by the deep learn-
ing approaches is signiﬁcantly better than handcrafted
features as shown in Table 2 and Table 3 where the best
veriﬁcation performance is achieved by DenseNet when
it is trained and evaluated on coarsely segmented iris.

We also make observations on the used deep learning ap-
proaches. The deep learning approaches achieved slightly
better performance when coarsely segmented iris is used in
comparison to using the normalized iris as shown in Fig-
ure 3. Under the most strict IMR threshold (IMR>0.7),
the best veriﬁcation performance is achieved by DenseNet
model where the EER was 5.80% when it was evaluated on
coarsely segmented iris region and 9.45% when it was eval-
uated on the normalized iris as shown in Table 3. When no
probe images are neglected, the DenseNet also achieved the
best veriﬁcation performance where the EER was 9.73% for
the coarsely segmented iris region and 13.06% for the nor-
malized iris. Although the DenseNet achieved the best re-
sult, the computational complexity of this model (18.5 mil-
lion trainable parameters) is around 6x higher than the Mo-
bileNet (3.1 million trainable parameters). In general, the
ﬁne-tuned models, DenseNet and ResNet, achieved slightly
higher veriﬁcation performance than DeepIrisNet and Mo-
bileNet trained from scratch. This result indicates that ﬁne-
tuned CNN models, originally trained for image classiﬁca-
tion, were able to capture the discriminative features of the
iris image when they are ﬁne-tuned by few training samples
(8916 eye images).

As expected, the veriﬁcation performances of the hand-
crafted approaches were lower than the deep learning ones.
It is noticeable from the error rates in Table 2 that the LG-
SHD and DCT-SHD perform better than the rest of the al-

Figure 3: The ROC curves achieved for different deep learning settings. The plots in the ﬁrst row show the evaluation results
obtained using coarsely segmented iris region and the plots in the second show result obtained from normalized iris.

IMR
threshold
IMR 0.0
IMR 0.7

DCT-SHD

LG-SHD

CSBCA

DCT-HD

LG-HD

EER
0.3469
0.3166

FMR10
0.5644
0.4770

EER
0.3502
0.3132

FMR10
0.5781
0.4596

EER
0.4032
0.3458

FMR10
0.7170
0.6619

EER
0.3750
0.3497

FMR10
0.6397
0.5713

EER
0.3663
0.3347

FMR10
0.6064
0.5313

Table 2: The EER and FMR10 for the different experimental settings and two IMR thresholds 0.0 and 0.7. The lowest FMR10
and EER are in bold for each IMR threshold. One can notice the lower errors achieved by the DCT-SHD and LG-SHD.

Modality

Coarsely
segmented iris

Normalized iris

IMR
threshold
IMR 0.0
IMR 0.7
IMR 0.0
IMR 0.7

DeepIrisNet

DenseNet

MobileNet

ResNet

EER
0.1748
0.1248
0.1741
0.1328

FMR10
0.2728
0.1603
0.2786
0.1751

EER
0.0973
0.0580
0.1306
0.0945

FMR10
0.0947
0.0221
0.1754
0.0853

EER
0.1471
0.1377
0.1619
0.1332

FMR10
0.2144
0.1916
0.2492
0.1796

EER
0.1251
0.0774
0.2058
0.1472

FMR10
0.1418
0.0557
0.3271
0.2189

Table 3: The achieved veriﬁcation performance of different deep learning methods evaluated on the coarsely segmented iris
region and the normalized iris modality.

gorithms. SHD distance achieves generally better veriﬁca-
tion performance than the HD as SHD accounts for rota-
tional shifts. In general, the best achieved EER with hand-
crafted features was 34.69% when no probes are neglected
and 31.66% when probes with IMR lower than 0.7 are ne-
glected, considerably higher than deep learning methods.

6.1. Computational analysis

For the sake of completion and as an indicator for future
works, we present the detailed analysis of computational
efﬁciency in Table 4. The computational efﬁciency of the
deep learning approaches depends on the number of train-
able parameters and the inference latency. Table 4 presents
these factors for the used deep learning models and the

Input size No. parameters

Model
DeepIrisNet-A 224 x 224
DeepIrisNet-A 128 x 128
224 x 224
DenseNet-201
224 x 224
ResNet-50
224 x 224
MobileNet-V3
640 x 480
MMS-Eye

231.9m
55.8m
18.5m
23.7m
3.1m
80k

Inference time
0.36s
0.28s
0.34s
0.73s
0.15s
0.04s

Table 4: Computational efﬁciency of deep learning ap-
proaches in this work.

segmentation model. All evaluations are performed using
Tensorﬂow framework (Version 1.14) running on Linux OS
with Intel(R) Xeon(R) Gold 6130 CPU 2.10GHz processor.
The evaluation only uses single core of the processor. Each
extracted features are stored as four-byte ﬂoating-point re-

(TV <TH)%

(TV >TH)%

(TV<TH)%

(TV>TH)%

(TV<TH)%

(TV>TH)%

User-ID Gen-IMR 0.0 Gen-IMR 0.7

1
2
3
4
5
6
7
8
9
10

0
0
1.53
0
0
40.0
0
0
0
0

0
0
4.61
0
0
65.0
1.90
0
0
2.85

Imp-IMR 0.0
2.93
4.69
3.65
0.10
7.35
0
21.97
20.27
9.88
0.94

Imp-IMR 0.7 User-ID Gen-IMR 0.0 Gen-IMR 0.7

2.34
2.66
2.81
0.92
5.87
0
10.72
10.34
5.34
0

11
12
13
14
15
16
17
18
19
20

0
11.11
9.47
0
0
0
97.89
0
0
0

0
12.69
9.47
30.64
0
3.94
96.84
1.81
0
0

Imp-IMR0.0
3.64
0.31
11.51
21.82
5.51
11.34
6.25
1.66
12.99
15.18

Imp-IMR 0.7 User-ID Gen-IMR 0.0 Gen-IMR 0.7

21
22
23
24
25
26
27
28

91.66
0
0
0
5.55
0
18.57
14.28

81.66
0
0
1.56
22.22
0
5.71
5.49

0.88
0
6.20
15.51
0
11.71
5.62
1.19
3.24
3.31

Imp-IMR 0.0
9.27
8.21
3.79
4.74
0.25
25.78
4.45
0.84

Imp-IMR 0.7
8.59
5.65
0.51
4.74
0
13.78
0
0.89

Table 5: The achieved result by the proposed trust model. The result is reported under two IMR threshold settings, for each
identity and for genuine (Gen) and importer (Imp) user scenarios. For each identity and for each user scenario, the text in
bold indicates that the model has a better performance based on the evaluated IMR thresholds. The result is reported as the
percentage of the trust values (period) where it was lower than the operational threshold (genuine user scenario) and where it
was higher than the operational threshold (imposter user scenario) to all calculated trust values (full session period).

sulting in templates of 16 kilobyte (KB), 392 KB, 367.5
KB, and 4 KB for DeepIrisNet-A, ResNet-50, DenseNet-
201, and MobileNetV3, respectively.

The chosen handcraft approaches are both computation-
ally efﬁcient and optimal for storage purposes. Each of the
feature extraction is completed within 3ms (same processor)
and the templates result in 915 bytes (B), 1022 B, and 336
B for 1D Log-Gabor, DCT Coefﬁcient iriscode and CSBCA
based iriscode, respectively when stored in lossless Portable
Graphics Format (png) format. The comparison of masked
iriscodes using Hamming Distance takes around 2ms while
the shifted version of the same takes around 6ms.

6.2. Continuous authentication results

We evaluated the continuous authentication based on the
comparison scores obtained from MobileNet based on its
aptness for deployability (3.1 million of trainable parame-
ters) suited for low computational power devices in compar-
ison to other evaluated deep learning models. Furthermore,
MobileNet achieved signiﬁcantly higher veriﬁcation perfor-
mance than handcrafted approaches. We reported the result
of the continuous authentication for two separate scenarios,
genuine and imposter user scenarios, motivated by [4, 11].
In the genuine user scenario, the trust value is calculated
based on scores obtained by comparing probes with a refer-
ence of the same identity. In the imposter user scenario, the
trust value is calculated based on scores obtained by com-
paring reference of a subject identity with probes from all
other identities. The result is reported for two different IMR
thresholds, IMR 0.0 and IMR 0.7 to account for the impact
of the proposed quality metric. The T , threshold produc-
ing EER, was 0.5131 when the solution is evaluated using
IMR 0.0 and 0.5452 when evaluated using IMR 0.7. Table
5 summarizes the achieved result for each identity. The re-
sult is reported per identity for genuine user scenario as a
percentage of the trust value updates where it falls below T
to all trust values (false rejection), i.e. the time percentage
of a session where the genuine user is rejected, and for im-
poster user scenario as a percentage of the trust value where
it is higher than the T to all trust values (false acceptance),
i.e. the time percentage of a session where the imposter user

is accepted. Table 5 shows that the model has very similar
behaviour in the genuine user scenario when IMR threshold
is set to 0 or 0.7. In the imposter user scenario, the model
performed better when neglecting captures with IMR lower
than 0.7 than the case when no probe is neglected.

7. Conclusions

Considering the increasing use of AR/VR technologies
in novel ﬁelds and the associated developments in HMD
devices, this work points out the possibility of using the
in-built cameras of such devices for iris recognition. This
work takes into account the limited computational power
commonly associated with such devices and evaluated the
veriﬁcation performance and computational efﬁciency of
the chosen segmentation model, as well as four deep learn-
ing and three handcrafted approaches. We benchmark the
veriﬁcation performance of these algorithms on a realis-
tic database captured using HMD. The overall veriﬁcation
result showed that the deep learning approaches reported
better performance than handcrafted approaches, where the
best performance was achieved by DenseNet. However, the
computational cost of deep learning approaches is higher
than handcrafted features ones due to the millions of train-
able parameters which motivated future work on applying
model compression technique such as parameter pruning
to reduce the computational cost. We proposed a tailored
quality metric for iris image selection based on the relative
proportion of the visible iris, showing the effect on the ver-
iﬁcation performance. The proposed approach has shown
signiﬁcant improvement in veriﬁcation accuracy account-
ing for the non-collaborative capture of the iris. As another
novel component, we also presented a continuous authenti-
cation model based on MobileNet model.

Acknowledgment: This research work has been funded
by the German Federal Ministry of Education and Research
and the Hessen State Ministry for Higher Education, Re-
search and the Arts within their joint support of the National
Research Center for Applied Cybersecurity ATHENE.

References

[1] ARESIBO project: http://aresibo.eu/, 2019.
[2] OpenEDS Challenge: https://research.fb.com/programs/openeds-

challenge/, 2019.

[3] D. Bastias, C. A. Perez, D. P. Benalcazar, and K. W. Bowyer.
A method for 3d iris reconstruction from multiple 2d near-
infrared images. In 2017 IEEE International Joint Confer-
ence on Biometrics (IJCB), pages 503–509. IEEE, 2017.
[4] P. Bours. Continuous keystroke dynamics: A different per-
spective towards biometric evaluation. Information Security
Technical Report, 17(1-2):36–43, 2012.

[5] F. Boutros, N. Damer, F. Kirchbuchner, and A. Kuijper. Eye-
mms: Miniature multi-scale segmentation network of key
eye-regions in embedded applications. In The IEEE Interna-
tional Conference on Computer Vision (ICCV) Workshops,
Oct 2019.

[6] F. Boutros, N. Damer, K. Raja, R. Ramachandra, F. Kirch-
buchner, and A. Kuijper. Periocular biometrics in head-
mounted displays: A sample selection approach for better
recognition. In 2020 8th IWBF, pages 1–6, 2020.
[7] K. W. Bowyer, K. Hollingsworth, and P. J. Flynn.

Image
understanding for iris biometrics: A survey. Computer vision
and image understanding, 110(2):281–307, 2008.

[8] J. Chen, F. Shen, D. Z. Chen, and P. J. Flynn.

Iris recog-
nition based on human-interpretable features. IEEE Trans-
actions on Information Forensics and Security, 11(7):1476–
1485, 2016.

[9] L.-C. Chen, Y. Zhu, G. Papandreou, F. Schroff, and H. Adam.
Encoder-decoder with atrous separable convolution for se-
In Proceedings of the Euro-
mantic image segmentation.
pean conference on computer vision (ECCV), pages 801–
818, 2018.

[10] N. Damer, F. Boutros, F. Kirchbuchner, and A. Kuijper. D-
id-net: Two-stage domain and identity learning for identity-
preserving image generation from semantic segmentation. In
2019 IEEE/CVF International Conference on Computer Vi-
sion Workshop (ICCVW), pages 3677–3682, 2019.

[11] N. Damer, F. Maul, and C. Busch. Multi-biometric continu-
ous authentication: A trust model for an asynchronous sys-
tem. In 19th International Conference on Information Fu-
sion, FUSION 2016, Heidelberg, Germany, July 5-8, 2016,
pages 2192–2199. IEEE, 2016.

[12] N. Damer, P. Terh¨orst, A. Braun, and A. Kuijper. Efﬁcient,
accurate, and rotation-invariant iris code. IEEE Signal Pro-
cess. Lett., 24(8):1233–1237, 2017.

[13] J. Daugman. Iris recognition border-crossing system in the

uae. International Airport Review, 8(2), 2004.

[14] J. Daugman. How iris recognition works. In The essential

guide to image processing, pages 715–739. Elsevier, 2009.

[15] A. K. Gangwar and A. Joshi. Deepirisnet: Deep iris rep-
resentation with applications in iris recognition and cross-
sensor iris recognition. In 2016 IEEE International Confer-
ence on Image Processing, ICIP 2016, Phoenix, AZ, USA,
September 25-28, 2016, pages 2301–2305, 2016.

[16] S. J. Garbin, Y. Shen, I. Schuetz, R. Cavin, G. Hughes,
and S. S. Talathi. Openeds: Open eye dataset. CoRR,
abs/1905.03702, 2019.

[17] C. George, M. Khamis, E. von Zezschwitz, M. Burger,
H. Schmidt, F. Alt, and H. Hussmann. Seamless and se-
cure vr: Adapting and evaluating established authentication
systems for virtual reality. NDSS, 2017.

[18] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning
for image recognition. In 2016 IEEE Conference on Com-
puter Vision and Pattern Recognition, CVPR 2016, Las Ve-
gas, NV, USA, June 27-30, 2016, pages 770–778. IEEE Com-
puter Society, 2016.

[19] A. Howard, M. Sandler, G. Chu, L.-C. Chen, B. Chen,
M. Tan, W. Wang, Y. Zhu, R. Pang, V. Vasudevan, et al.
Searching for mobilenetv3. In Proceedings of the IEEE In-
ternational Conference on Computer Vision, pages 1314–
1324, 2019.

[20] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang,
T. Weyand, M. Andreetto, and H. Adam. Mobilenets: Efﬁ-
cient convolutional neural networks for mobile vision appli-
cations. CoRR, 1704.04861, 2017.

[21] G. Huang, Z. Liu, L. van der Maaten, and K. Q. Weinberger.
In 2017 IEEE
Densely connected convolutional networks.
Conference on Computer Vision and Pattern Recognition,
CVPR 2017, Honolulu, HI, USA, July 21-26, 2017, pages
2261–2269, 2017.

[22] J.-G. Ko, Y.-H. Gil, J.-H. Yoo, and K.-I. Chung. A novel and
efﬁcient feature extraction method for iris recognition. ETRI
journal, 29(3):399–401, 2007.

[23] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet clas-
siﬁcation with deep convolutional neural networks. Com-
mun. ACM, 60(6):84–90, 2017.

[24] A. Kumar and A. Passi. Comparison and combination of iris
matchers for reliable personal authentication. Pattern recog-
nition, 43(3):1016–1026, 2010.

[25] A. Kupin, B. Moeller, Y. Jiang, N. K. Banerjee, and S. Baner-
jee. Task-driven biometric authentication of users in virtual
In International Conference on
reality (vr) environments.
Multimedia Modeling, pages 55–67. Springer, 2019.

[26] J. J. Lee, S. Noh, K. R. Park, and J. Kim. Iris recognition
in wearable computer. In Biometric Authentication, First In-
ternational Conference, ICBA 2004, Hong Kong, China, July
15-17, 2004, Proceedings, pages 475–483, 2004.

[27] N. Liu, H. Li, M. Zhang, J. Liu, Z. Sun, and T. Tan. Accu-
rate iris segmentation in non-cooperative environments using
fully convolutional networks. In 2016 International Confer-
ence on Biometrics (ICB), pages 1–8. IEEE, 2016.

[28] N. Liu, M. Zhang, H. Li, Z. Sun, and T. Tan. Deepiris: Learn-
ing pairwise ﬁlter bank for heterogeneous iris veriﬁcation.
Pattern Recognit. Lett., 82:154–161, 2016.

[29] J. Lozej, B. Meden, V. Struc, and P. Peer. End-to-end iris
segmentation using u-net. In 2018 IEEE International Work
Conference on Bioinspired Intelligence (IWOBI), pages 1–6.
IEEE, 2018.

[30] L. Masek et al. Recognition of human iris patterns for bio-
metric identiﬁcation. PhD thesis, Master’s thesis, University
of Western Australia, 2003.

[31] K. Miyazawa, K. Ito, T. Aoki, K. Kobayashi, and H. Naka-
jima. An effective approach for iris recognition using phase-
based image matching. IEEE transactions on pattern analy-
sis and machine intelligence, 30(10):1741–1756, 2008.

[48] A. Uhl and P. Wild. Weighted adaptive hough and ellipsopo-
lar transforms for real-time iris segmentation. In A. K. Jain,
A. Ross, S. Prabhakar, and J. Kim, editors, 5th IAPR Inter-
national Conference on Biometrics, ICB 2012, New Delhi,
India, March 29 - April 1, 2012, pages 283–290. IEEE, 2012.
[49] L. A. Zanlorensi, D. R. Lucio, A. de Souza Britto Junior,
H. Proenc¸a, and D. Menotti. Deep representations for cross-
IET Biometrics, 9(2):68–77,
spectral ocular biometrics.
2020.

[50] L. A. Zanlorensi, E. Luz, R. Laroca, A. S. Britto, L. S.
Oliveira, and D. Menotti. The impact of preprocessing on
deep representations for iris recognition on unconstrained
In 2018 31st SIBGRAPI Conference on
environments.
Graphics, Patterns and Images (SIBGRAPI), pages 289–296.
IEEE.

[32] D. M. Monro, S. Rakshit, and D. Zhang. Dct-based iris
recognition. IEEE Transactions on Pattern Analysis & Ma-
chine Intelligence, (4):586–595, 2007.

[33] J. Ortega-Garcia, J. Fierrez, F. Alonso-Fernandez, J. Gal-
bally, M. R. Freire, J. Gonzalez-Rodriguez, C. Garcia-
Mateo, J.-L. Alba-Castro, E. Gonzalez-Agulla, E. Otero-
Muras, et al. The multiscenario multienvironment biosecure
multimodal database (bmdb). IEEE Transactions on Pattern
Analysis and Machine Intelligence, 32(6):1097–1111, 2009.
[34] J. Perry and A. Fernandez. Minenet: A dilated cnn for se-
mantic segmentation of eye features. In The IEEE Interna-
tional Conference on Computer Vision (ICCV) Workshops,
Oct 2019.

[35] P. J. Phillips, W. T. Scruggs, A. J. O’Toole, P. J. Flynn, K. W.
Bowyer, C. L. Schott, and M. Sharpe. FRVT 2006 and ICE
2006 large-scale experimental results. IEEE Trans. Pattern
Anal. Mach. Intell., 32(5):831–846, 2010.

[36] R. Poelman, O. Akman, S. Lukosch, and P. Jonker. As if
being there: mediated reality for crime scene investigation.
In Proceedings of the ACM 2012 conference on computer
supported cooperative work, pages 1267–1276. ACM, 2012.
[37] R. P. K. Poudel, S. Liwicki, and R. Cipolla. Fast-scnn: Fast
semantic segmentation network. CoRR, abs/1902.04502,
2019.

[38] H. Proenc¸a. Unconstrained iris recognition in visible wave-
lengths. In Handbook of Iris Recognition, pages 321–358.
Springer, 2016.

[39] C. Rathgeb and A. Uhl. Secure iris recognition based on
local intensity variations. In International Conference Image
Analysis and Recognition, pages 266–275. Springer, 2010.

[40] C. Rathgeb, A. Uhl, and P. Wild. Shifting score fusion: On
exploiting shifting variation in iris recognition. In Proceed-
ings of the 2011 ACM Symposium on Applied Computing,
pages 3–7. ACM, 2011.

[41] A. Ross.

Iris recognition: The path forward. Computer,

43(2):30–35, 2010.

[42] P. Rot,

ˇZ. Emerˇsiˇc, V. Struc, and P. Peer. Deep multi-
class eye segmentation for ocular biometrics. In 2018 IEEE
International Work Conference on Bioinspired Intelligence
(IWOBI), pages 1–8. IEEE, 2018.

[43] M. Sandler, A. G. Howard, M. Zhu, A. Zhmoginov, and
L. Chen.
Inverted residuals and linear bottlenecks: Mo-
bile networks for classiﬁcation, detection and segmentation.
CoRR, abs/1801.04381, 2018.

[44] S. Shah and A. Ross. Iris segmentation using geodesic active
contours. IEEE Transactions on Information Forensics and
Security, 4(4):824–836, 2009.

[45] Z. Sun and T. Tan. Ordinal measures for iris recognition.
IEEE Transactions on pattern analysis and machine intelli-
gence, 31(12):2211–2226, 2008.

[46] M. Tan, B. Chen, R. Pang, V. Vasudevan, M. Sandler,
A. Howard, and Q. V. Le. Mnasnet: Platform-aware neu-
ral architecture search for mobile. In IEEE CVPR 2019, CA,
USA, pages 2820–2828, 2019.

[47] K. N. Thanh, C. Fookes, A. Ross, and S. Sridharan.

Iris
recognition with off-the-shelf CNN features: A deep learn-
ing perspective. IEEE Access, 6:18848–18855, 2018.

