Reduction of detection limit and quantiﬁcation uncertainty due to
interferent by neural classiﬁcation with abstention

Alex Hagena,∗, Ken Jarmana, Jesse Warda, Greg Eidena, Charles Barinagaa, Emily Macea, Craig Aalsetha,
Anthony Caradoa

aPaciﬁc Northwest National Laboratory, Richland, WA, USA

Abstract

Many measurements in the physical sciences can be cast as counting experiments, where the number of
occurrences of a physical phenomenon informs the prevalence of the phenomenon’s source. Often, detection
of the physical phenomenon (termed signal) is diﬃcult to distinguish from naturally occurring phenomena
(termed background). In this case, the discrimination of signal events from background can be performed
using classiﬁers, and they may range from simple, threshold-based classiﬁers to sophisticated neural networks.
These classiﬁers are often trained and validated to obtain optimal accuracy, however we show that the optimal
accuracy classiﬁer does not generally coincide with a classiﬁer that provides the lowest detection limit, nor
the lowest quantiﬁcation uncertainty. We present a derivation of the detection limit and quantiﬁcation
uncertainty in the classiﬁer-based counting experiment case. We also present a novel abstention mechanism
to minimize the detection limit or quantiﬁcation uncertainty a posteriori. We illustrate the method on two
data sets from the physical sciences, discriminating Ar-37 and Ar-39 radioactive decay from non-radioactive
events in a gas proportional counter, and discriminating neutrons from photons in an inorganic scintillator
and report results therefrom.

1. Motivation

Many physical measurements consist of counting
experiments (CEs), where the rate of occurrence of
an event informs quantitative information about a
physical system. These experiments are performed
by discretely counting these events over a desig-
nated counting time. The two main goals of such
CEs are to either detect the presence of a given
phenomenon in a physical system, or to measure
the prevalence of a phenomenon; the performance of
tasks which are best indicated by the detection limit
and measurement uncertainty, respectively. Count-
ing techniques underlie many physical sciences. Ex-
amples include the measurement of mass or speciﬁc
activity of an isotope in a mixed sample, the mea-
surement of the ratio of neutron to photon doses in
radiological experiments, or even the prevalence of
a given pathogen in a population of people.

All but the most trivial counting experiments
suﬀer from the presence of ”background” events,

∗alexander.hagen@pnnl.gov

Preprint submitted to Elsevier

where an unrelated event is detected and thus
counted along side the ”signal” events of interest.
The separation of background from signal can be
eﬀected in many ways. The three main classes
are: physical removal of interference, such as move-
ment of a radiation counting experiment into under-
ground labs to occlude cosmic rays; experiment de-
sign, such as diﬀerential sensitivity measurements
which identify diﬀering sensitivity changes to ana-
lyte versus interferent; and data analytic methods,
such as the removal of events exhibiting the charac-
teristics of radon in radioxenon measurements [1].
In many cases, however, physical removal of in-
terference or careful experiment design to remove
background is either impossible (as is the case in
counting experiments in the dark matter search), or
cost and eﬀort prohibitive (as is the case in many
material separation studies). As machine learn-
ing (ML) techniques have matured, the data ana-
lytic methods for signal and background separation
have become increasingly sophisticated and exhib-
ited higher performance [1–5]. These methods are
also receiving heightened interest as a cost-saving

May 17, 2022

2
2
0
2

r
p
A
2
2

]
n
a
-
a
t
a
d
.
s
c
i
s
y
h
p
[

1
v
9
0
6
7
0
.
5
0
2
2
:
v
i
X
r
a

 
 
 
 
 
 
and throughput-increasing measure.

2. Relevant Literature

An important class of data analytic methods
is that of event-by-event classiﬁcation into signal
or background classes, which we will refer to as
Classiﬁer-Based Counting Experiments (CBCE).
This structure nicely aligns the problem in the
counting statistics domain with a main category
of problems in general data science - that of bi-
nary classiﬁcation. Many advances in binary classi-
ﬁcation are readily applicable to CBCEs. Unfortu-
nately, in the binary classiﬁcation literature, classi-
ﬁers are rarely perfect, and the best are often chosen
as those that have the highest accuracy amongst all
tested methods1. We show that, for a non-perfect
classiﬁer, maximal accuracy is not optimal for min-
imizing quantiﬁcation uncertainty. We also reprise
[1], showing that maximal accuracy does not min-
imize the detection limit. As detection limit and
quantiﬁcation uncertainty are the most important
measures of performance in counting experiments,
we seek to remedy these ﬁndings.

We contribute a novel method for improving both
the quantiﬁcation uncertainty and detection limit
of any classiﬁer by developing an optimal thresh-
old, based on the classiﬁer’s raw output, which
maximizes the metric of interest. This method,
which we call abstention, can be applied to any
CBCE with modest assumptions about the signal-
to-background ratio (SBR). While it is beneﬁcial
at any SBR, it can realize orders of magnitude im-
provement in the quantiﬁcation uncertainty in the
case of very low SBR.

To illustrate the use of abstention in CBCEs,
we structure this work in the following way. We
identify related work and this work’s antecedents.
Then, we illustrate non-optimality of detection limit
using a maximal-accuracy classiﬁer on a data set
consisting of events induced by neutrons and pho-
tons in a scintillator. We follow that with the pro-
cess for determining the thresholds(s) obtaining the
minimal uncertainty for a classiﬁer operating on
events from an ultra-low background proportional
counter measurement. To reinforce our claim that
abstention can be used on any classiﬁer with a con-
tinuous output, we use diﬀerent classiﬁers through-
out. For narrative purposes, details of these classi-
ﬁers are kept to a minimum.

1In this context, accuracy has the narrow deﬁnition of
simply the ratio between the correctly classiﬁed events and
all events.

2

A large uncertainty quantiﬁcation literature, and
in fact a sub-ﬁeld of statistics, exists to determine
the proper way to quantify samples in the presence
of background. The statistical methods for quanti-
fying uncertainty and determining detection limits
given signal and background properties are mature
and well founded, progressing to the point of tech-
nical manuals describing best practices [6]. This,
however, is not true of the methods for CBCE. Sta-
tistical approaches to the uncertainty propagation
through a CBCE scheme are sparse in the liter-
ature, and the literature includes even fewer at-
tempts to improve quantiﬁcation uncertainty in a
CBCE.

Our method for optimizing the quantiﬁcation un-
certainty depends on abstention from certain data
points based on the classiﬁer output (more strictly
the classiﬁer’s conﬁdence in its own output), and
some assumptions about the system in question.
Similar forms of abstention have existed in the ma-
chine learning literature since the 1970s. Hellman
derived a special form of Nearest Neighbors which
used several methods to abstain from classiﬁcation
[7]. More recently, abstention has dealt with in-
creasing the robustness of a classiﬁer to mistakes
within the labels of a data set, while continuing to
measure classiﬁer performance with accuracy. Thu-
lasidasan provides several methods for this, includ-
ing one which modiﬁes the training of a neural net-
work for this robustness [8, 9]. These works devel-
oped the basis for our work: the ability of a ML
based classiﬁer to abstain from classifying certain
data. However, while the previous eﬀorts focused
on developing abstention for speciﬁc ML classiﬁers,
and rely on accuracy for their performance metrics,
we instead focus on tying a simple method for ab-
staining from classiﬁcation to two important phys-
ical metrics.

We develop further on a line of investigation
started by Chow in 1970. Chow described the reject
rate and error tradeoﬀ for an ideal optical charac-
ter recognition system [10]. We use this tradeoﬀ
concept throughout the present work. Recently,
DeStefano generally described that the metric for
any classiﬁer must necessarily be dependent on the
rejection or abstention threshold [11], which Geif-
man applied to neural networks [12]. We modify
these works in two ways, applying them speciﬁcally
to the counting experiment domain, while gener-
alizing to any classiﬁer with a continuous output

(which we call ”scores”).

3. Detection Limit

A common use for counting experiments is to
determine the presence of a given phenomenon;
a use case which pervades many ﬁelds, including
nuclear forensics, beyond-standard-model physics,
and even medical diagnostic applications. For these
cases, the detection limit, or minimum detectable
amount, is the metric of interest for a given detec-
tion methodology. We show below that maximum
accuracy thresholds on CBCEs are in general not
coincident with optimal detection limit thresholds,
and illustrate this fact on an example data set.

3.1. Illustrative Example: Neutron versus photon

discrimination in scintillators

Motivation. The eﬃcient detection of neutrons is
an important subﬁeld of nuclear safety, special nu-
clear material interdiction, and radiotherapy. Of-
ten, however, the neutron detection mechanism
coincides with detection mechanisms for photons;
most neutron detectors thus have a large ”photon
background”. One common way to solve this issue
is to use scintillators, which emit light with diﬀer-
ing properties when a neutron or photon interacts
with the material. Then, the resulting light dis-
tribution is digitized, and so-called ”Pulse Shape
Discrimination” (PSD) methods are employed [13].
The classiﬁcation of an event as originating from a
neutron or a photon is a challenge and the perfor-
mance possible is dependent on the detection ma-
terial’s response to each species, the temperature
and other environmental factors of the experiment,
and, as we subsequently show, the performance of
a classiﬁer. We show that, while creating a classi-
ﬁer for PSD is straightforward, the optimization of
the threshold above which that classiﬁer classiﬁes
an event as neutron can have signiﬁcant eﬀects on
the detection limit.

Data. We obtained a data set of digitized pulses
collected from a Cs2LiYC6 (CLYC) detector when
exposed to mostly photon and mostly neutron
sources. An example of these pulses is shown in
ﬁg. 1. The relevant feature to these pulses is the
curvature around the maximum magnitude of the
pulse: a pulse with a fast rise time and ﬂat area
at the top of the curve is indicative of a neutron
event, an event with no ﬂat area at the top of the

Figure 1: Example pulses emanating from interaction of a
neutron or photon with CLYC scintillator material

curve is indicative of a photon event. This diﬀer-
ence is physically based - a neutron interacts with
CLYC via the 6Li + n → α + 3H reaction. The α
product deposits most of its energy within nanome-
ters of the original event, which in turn creates a
slowly decaying burst of scintillation photons; pho-
tons interact with CLYC by liberating electrons,
which decay away faster, creating a more quickly
decaying burst of scintillation photons.

Classiﬁcation. Discrimination of neutrons and pho-
tons is complicated by the near impossibility of
generating a pure-neutron source: practically, any
experiment will provide detection pulses of mixed
neutron and photon origin. We resolve this ambi-
guity by using a dimensionality reduction method
for identifying neutron and photon sources within
the data set, treating these as our ground-truth la-
bels2. We identify neutrons as ”analyte” and pho-
tons as ”interferent”. We perform standard trans-

2Note that, while we strive for physical accuracy in our
labels, the following derivation applies equally to any set
of labels; therefore exact physical accuracy is a secondary
concern for labeling in the current work. We provide details
about our method in Appendix A

3

02004006008001000Time Step (t) [au]0101Normalized Pulse Voltage (V) [au]Neutron likePhoton likeFigure 2: Tail-vs-total versus total pulse for neutron and
photon like pulses. One can see that in general, a neutron
like pulse has a lower tail-over-total value than those from
photons.

formations to each pulse, to calculate the ”total”
pulse (integrating) and the ”tail-over-total” ratio
(integrating the tail of the pulse and dividing by
the total pulse) as follows:

P =

(cid:90) tp

0

V (t) dt, D =

(cid:90) td

tp

V (t) dt

(1)

r =

D
P + D

(2)

where V (t) is the measured voltage of each pulse at
time t, tp is the end time of some ”prompt” window
in the pulse, and td is ending time of some ”delayed”
window in the pulse. In practice, these integrals are
approximated as simple summations. We compare
this to the energy of the event, which is simply the
maximum value of the pulse above its baseline.

E = max [V (t)]

(3)

These features are visualized in ﬁg. 2.

We then create a classiﬁer, called the tail-over-
total classiﬁer (TOTC). The TOTC uses the two
features shown on ﬁg. 2 as input and attempts to
classify whether an event is neutron-like or photon-
like. TOTC is a multilayer perceptron, and per-
forms well when measured by the area under the re-
ceiver operating characteristic curve. The receiver
operating characteristic (ROC) curve is shown in
ﬁg. 3, alongside a histogram of classiﬁer scores for
analyte and interferent.

The second panel of ﬁg. 3 shows a characteris-
tic typical of counting experiments. While the dis-
tribution of scores for analyte and interferent are
quite diﬀerent, they are not disjoint. This char-
acteristic leads directly to the main thesis of this

4

Figure 3: Receiver operating characteristic for TOTC (top)
and associated histograms (bottom). The classiﬁer performs
well when measured by accuracy or area under the ROC
curve. The scores attributed to neutron versus photon events
are separated but not disjoint.

0200400600Energy (E) [au]0.000.250.500.751.00Tail-to-total pulse(r) [au]Neutron LikePhoton Like020406080100False Positive Rate (rFP) [%]020406080100True Positive Rate (rTP [%]Random ChanceBest accuracy 99.1%with threshold 0.160.00.20.40.60.81.0Classifier score (s) [au]101103Number of instancesanalyteinterferentpaper: that maximum accuracy is not a good met-
ric for CBCEs, and by consequence the threshold
for classiﬁcation should not optimize accuracy. To
show this, we illustrate that the detection limit of
a neutron source given TOTC is not minimized by
the maximal-accuracy threshold.

Detection Limit Optimization. We calculate the
detection limit following [1], which draws from [14]
with some corrections (such as the Stapleton correc-
tion [6]) for low signal counts. Then, the detection
limit ld is given by

(cid:113) k2

4 + lc

lc + k2

2 + k
ηRT

ld =

(4)

where k is Z-score for a false alarm rate of 5%, R
is the expected rate of interferent in a counting ex-
periment of time T , η is the eﬃciency of detecting
analyte given a threshold, and lc is the critical limit
calculated by

√

lc = 2.33

nIA + 0.4 + 1.35

(5)

where nIA is the number of interferent detections
classiﬁed as analyte for a given threshold, and 0.4 is
the Stapleton correction for low analyte detection
rates.

We can then proﬁle the detection limit versus
a varying classiﬁer threshold by calculating ld for
varying thresholds. The results of this calculation
are presented in ﬁg. 4. This ﬁgure illustrates several
expected properties of the detection limit. When
the threshold is too high, very few pulses are clas-
siﬁed as analyte, and a very high detection limit
results. When the threshold is too low, a high de-
tection limit also results because of a high rate of
classiﬁcation interferent pulses as analyte.

Regarding the hypothesis of this paper, ﬁg. 4 ex-
plicitly shows that the maximal-accuracy threshold
In fact,
is not optimal regarding detection limit.
for the example provided, the detection limit for
the maximal-accuracy threshold is twice as high as
the optimal limit.

4. Measurement Uncertainty

Another large class of counting experiments are
those used to quantify a material or phenomenon
of interest, beyond simply detecting it. This tech-
nique is again used in a broad variety of ﬁelds, from
nuclear forensics to prediction of political election
results.

5

Figure 4: Detection limit versus classiﬁer score low limit
when the interferent is 10, 000× more prevalent than the an-
alyte. A classiﬁcation created by visual inspection (dubbed
”box” classiﬁcation”, shown in grey on the top panel) results
in a detection limit close to twice that of the optimal detec-
tion limit (shown in green on the top panel). The maximum
accuracy threshold (shown in red on the top panel) for clas-
siﬁcation as analyte results in a similar detection limit to
that of the box classiﬁcation.

100200300400500600Energy (E) [au]0.650.700.750.800.850.90Tail-to-total pulse(r) [au]Neutron LikePhoton Likemaxaccoptimal0.000.250.500.751.00Classifier score threshold (th) [au]406080Detection limit (ld) [au]MDA: 33.2with optimal MDAthreshold 0.9900MDA: 63.6with optimal accuracythreshold 0.1593MDA: 50.6with boxclassificationThe results of such counting experiments, if ana-
lyzed without an estimate of interferent prevalence,
can lead to extremely biased results. For example,
a classiﬁer discriminating analyte from interferent
which has 90% accuracy, when applied to a pop-
ulation of 100,000 members, 99% of which are in-
terferent, will on average identify 10,000 members
as analyte, 9,000 of which are actually interferent.
This is exactly the base-rate fallacy [15], and uti-
lization of statistical corrections for this fallacy are
essential to accurate quantiﬁcation.

CBCEs are one broad class of correction for
counting experiments with irreducible interference.
The use of a priori measurements of the perfor-
mance of the classiﬁer involved lead to a more ac-
curate estimate of the analyte prevalence. In order
to correct for classiﬁer performance, we can use the
method of moments. We ﬁrst calculate the pro-
portion of correct prediction of analyte (pa) and
interferent (pi) and the proportion of incorrect pre-
diction of analyte (qa) and interferent (qi). Then,
we can state the expected value of counts classiﬁed
as analyte and interferent as

Ya = paµa + qiµi

(6)

Yi = qaµa + qiµi
where µa,i are the prevalence of analyte and inter-
ferent, respectively. Then, this reduces to

(7)

µa =

paYa − qiYi
papi − qaqi

(8)

The uncertainty of this unbiased estimator is
then derived using the conditional variance formula
[16]

Var (µa) = E [Var (µa|Xa, Xi)]+Var (E [µa|Xa, Xi])
(9)
where µa is the mean of counts from analyte, Xa is
a random variate denoting the counts from analyte,
Xi is a random variate denoting the counts from in-
terferent, and E (. . . ) denotes an expectation value.
By accounting for covariances (due to the con-
straint of sums on Xa and Xi), considering counts
to be multinomial random variables, and algebraic
manipulation, we can derive the variance of µa as

Var (µa) = σ2

a +

1
(papi − qaqi)2

· (cid:8)mua
(cid:2)p2
+µi

i pa (1 − pa) + q2

(cid:2)p2
i qi (1 − qi) + q2

i qa (1 − qa) + 2pipaqiqa

i pi (1 − pi) + 2p2

i q2
i

(cid:3)
(cid:3)(cid:9) (10)

A full derivation of eq. (10) is provided in Ap-
pendix B. For many counting experiments, µa can
be assumed to be Poisson distributed, and in those
cases σ2
a is equal to µa. We use this approximation
throughout. With that approximation, we reference
the uncertainty as the standard deviation given that
variance.

u ≡ (cid:112)Var (µa)
While it is not obvious whether the threshold for
maximal accuracy minimizes eq. (10), we illustrate
that it must not through an example from the phys-
ical sciences.

(11)

4.1. Illustrative Example: Interferent rejection in
Ultra Low Background Proportional Counters
Motivation. A low background underground facil-
ity at Paciﬁc Northwest National Laboratory has
collected years of data measuring the prevalence of
37Ar and 39Ar for treaty veriﬁcation and ground-
water age dating, using ultra-low-background pro-
portional counters (ULBPC) [17]. The isotopes of
interest have very low prevalence, and diﬃcult-to-
reduce backgrounds such as micro-discharge events
from signal paths and commercial electronics, along
with natural background events from cosmic rays
can make it challenging to accurately quantify the
activity of each isotope. Mace, Ward, and Aalseth
showed that a neural-network-based classiﬁer could
eﬀectively separate the background noise events
from the radioactive decay or gas-gain events [2].

Data. The ULBPC collects charge avalanches cre-
ated in a gas under high electric ﬁeld by energy
depositions from nuclear decays, cosmic rays, and
other sources. These avalanches diﬀer in total
charge (energy) and timing (shape) due to their ori-
gin, with ”Gas Gain” pulses starting from a baseline
before a sharp rise and slow decay. Other pulses,
from various sources, have other shapes. Example
pulse shapes for each origin are shown in ﬁg. 5.

Data was also collected in two states where the
detector was operated such that a higher number of
bad pulses were collected than in the standard data
set. We denote partitions of the standard data set
as ”Training” and ”Validation” and the two states
with higher bad pulse rate as ”Alt. Mode 1” and
”Alt. Mode 2”. These alternate modes are used
as test sets, to ensure that abstention not only re-
duces the uncertainty, but abstains from pulses in
locations of low probability of correct classiﬁcation
in the general sense.

6

Figure 5: Example pulses from desired (”Gas Gain”) and
undesired (”Other”) operation of the Ultra-Low Background
Proportional Counter

Methods. In order to both discriminate Gas Gain
from Other events, and to generalize well to events
from the alternate modes of operation, we train a
model using the Generalized ODIN method [18].
This architecture is a multilayer perceptron (de-
tails of which are provided in Appendix C), which
we train by minimizing cross-entropy between the
known Gas-Gain/Other labels and the network pre-
dictions.

The resultant network again shows good per-
formance when measured by area under the ROC
curve or accuracy, and again shows separated but
not disjoint score distributions for each class. This
is shown in ﬁg. 6.

Minimization of Uncertainty. Given the trained
ODIN classiﬁer, we can then estimate the activity of
the analyte, and the uncertainty thereof. Calcula-
tion of the activity of the analyte is performed using
eq. (8) and calculation of uncertainty is performed
using eq. (10). Note that both of these equations
require ﬁrst the setting of thresholds ta and ti on
classiﬁer score. Above ta, all pulses are classiﬁed
as analyte, and below ti, all pulses are classiﬁed as
interferent. Then, using pa, pi, qa and qi calculated

7

Figure 6: Performance of and classiﬁer score distribution
from ODIN network trained for ULBPC counting experi-
ments. The network performs well when measured by area
under the Receiver Operating Characteristic curve, with the
analyte and interferent both having broad distributions of
scores.

02004006008001000Time Divisions (t) [au]020,00040,000020,00040,000Voltage Divisions (V) [au]Gas GainOther020406080100False Positive Rate (rFP) [%]020406080100True Positive Rate (rTP [%]Random ChanceBest accuracy 98.2%with threshold 0.480.00.20.40.60.81.0Classifier score (s) [au]101103Number of instancesanalyteinterferentFigure 8: The estimated analyte activity per true analyte
activity and its associated 1σ uncertainty when using the
optimal and maximal-accuracy (”acc”) threshold settings.
The estimated activity of both methods does not diﬀer from
the true activity, but the uncertainty in the optimal case is
much smaller than that in the maximal-accuracy case

Figure 7: Uncertainty calculated given an analyte lower
threshold of ta and interferent upper threshold of ti for ana-
lyte to interferent ratio of 103. The minimal value of uncer-
tainty does not occur along the single threshold line (from
(0, 0) to (1, 1)), and is 3× smaller than the uncertainty when
using the maximal-accuracy threshold

from a validation set, µa and u can be estimated.

It is instructive ﬁrst to examine how changing the
thresholds aﬀects the uncertainty for a given ratio
of analyte to interferent µa
. We start with an ex-
µi
amination of the case where there is 1000× higher
interferent activity than analyte. The uncertainty
for this case is shown in ﬁg. 7, where the color in-
dicates the base 10 logarithm of the uncertainty u,
the location along the x-axis indicates ta and the
location along the y-axis indicates ti. This shows
that many threshold sets with low uncertainty do
not fall along the single threshold region (the line
from (0, 0) to (1, 1)), and in fact the lowest uncer-
tainty in this case is 1
3 that of the uncertainty for
the maximal-accuracy single threshold.

These results are true in general, that the optimal
uncertainty does not fall along the single threshold
line. We then further illustrate this for many diﬀer-
ent analyte to interferent ratios. To do so, for each
analyte to interferent ratio, a simplex based opti-

Figure 9: The ratio of the optimal to the maximal-accuracy
uncertainty for varying analyte to interferent ratios. At
low analyte to interferent ratios, the optimal uncertainty is
several times lower than the maximal-accuracy uncertainty;
throughout the optimal uncertainty is less than the maximal-
accuracy uncertainty

mization method [19] was used to determine the lo-
cation and value of the minimal uncertainty. Then,
the unbiased mean and its uncertainty band was
plotted in ﬁg. 8, and the relative size of the opti-
mal to the maximal-accuracy uncertainty band was
plotted on ﬁg. 9.
These ﬁgures

the optimal and
show that
maximal-accuracy activity estimates do not diﬀer
from each other, nor do they appreciably diﬀer from
the true value. They show that at low analyte
to interferent ratios, the optimal uncertainty is up
to 3× smaller than that for maximal accuracy; at
high analyte to interferent ratios the uncertainty
bands are close to each other, but still smaller in
the optimal case. Also of note is the size of the

8

0.20.40.60.8Analyte Lower Limit (ta)0.20.40.60.8Interferent Upper Limit (ti}Optimal uncertaintyu=1.5ta=0.56ti=0.44Maximal AccuracySingle thresholdu=4.5t=0.480.00.51.01.52.02.53.0Log Relative Uncertainty (log10(u))10-1101Analyte to Interferent Ratio(µaµi) [au]0.51.01.5Normalized Counts(ˆµaˆµa) [au]optimalacctrue10-210-1100101102Analyte to Interferent Ratio(µaµi) [au]0.60.81.0Uncertainty Ratio(uoptuacc) [au]uncertainty bands for small analyte to interferent
ratio. At the lowest analyte to interferent ratios,
the maximal-accuracy uncertainty band includes 0
within two standard deviations. This echos the con-
clusions of section 3: that the maximal-accuracy
thresholds do not have minimal detection limits. It
also adds on stronger conclusions:
in some cases,
using maximal-accuracy thresholds precludes one
from detecting what can be actually quantiﬁed us-
ing optimal thresholds.

Abstention on out of distribution data. As a ﬁnal
check, we are able to look at types of pulses from
which we abstain. To investigate this, we present
ﬁg. 10. In it, we plot each pulse decomposed into
a Pulse Height versus Exemplar Squared Sum of
Errors (ESSE) representation, similar to the pulse
shape discrimination plots in [2]. It can be seen that
the regions of phase space indicative of Gas Gain
versus Other in the training and validation sets are
separated, with some overlap on the bottom left
side of each chart. It can also be seen that pulses
close to that intersection have classiﬁer scores closer
to 1
2 , and that the abstention region neatly splits
those two regions. For the alternate mode data
sets, the Gas Gain region and Other region are not
as well separated; however the abstention region re-
moves many of those points which are in ambiguous
regions of phase space. This shows not only the util-
ity of abstention for traditional in-distribution data
classiﬁcation, but its extension when coupled with
ODIN to an out-of-distribution data set.

5. Conclusions

Citing the utility of using classiﬁers to correct
for diﬃcult-to-reduce backgrounds in counting ex-
periments, we claim that a classiﬁer in the cases
presented here is better judged by detection limit
and minimal uncertainty than mere accuracy. We
demonstrated that these are not the same. We pre-
sented a derivation of the detection limit and mea-
surement uncertainty in such classiﬁer-based count-
ing experiments.

We also presented and showed the utility of ab-
staining from certain events in CBCEs. For detec-
tion limit calculations, direct optimization of the
classiﬁer score threshold above which an event is
classiﬁed as analyte can make large diﬀerences in
the detection limit. For activity measurements, us-
ing two thresholds, one above which all events will
be considered analyte and the other below which

9

all events will be considered interferent, can reduce
the uncertainty by several times, dependent on the
base rate of interferent.

Overall, this paper concludes that caution is
warranted when judging a classiﬁer to be used in
CBCEs, and accuracy should not be the primary
goal. We presented best practices for two metrics
more relevant to metrology, but our concept of ab-
stention applies to and would improve other metrics
as well.

6. Acknowledgement

This research was funded by the National Nuclear
Security Administration, Oﬃce of Defense Nuclear
Nonproliferation Research and Development.

References

[1] A. Hagen, B. Loer, J. Orrell, and R. Saldanha, “Deci-
sion trees for optimizing the minimum detectable con-
centration of radioxenon detectors,” Journal of Envi-
ronmental Radioactivity, vol. 229-230, p. 106542, apr
2021.

[2] E. K. Mace, J. D. Ward, and C. E. Aalseth, “Use of
neural networks to analyze pulse shape data in low-
background detectors,” Journal of Radioanalytical and
Nuclear Chemistry, vol. 318, no. 1, pp. 117–124, 2018.
[3] R. D. Parsons and S. Ohm, “Background rejection in
atmospheric Cherenkov telescopes using recurrent con-
volutional neural networks,” European Physical Journal
C, vol. 80, no. 5, pp. 1–11, 2020.

[4] J. Pearkes, W. Fedorko, A. Lister, and C. Gay, “Jet
Constituents for Deep Neural Network Based Top
Quark Tagging,” 2017.

[5] J. Renner, A. Farbin, J. M. Vidal, J. M. Benlloch-
Rodr´ıguez, A. Botas, P. Ferrario, J. J. G´omez-Cadenas,
V. ´Alvarez, C. D. Azevedo, F. I. Borges, S. C´arcel,
J. V. Carri´on, S. Cebri´an, A. Cervera, C. A. Conde,
J. D´ıaz, M. Diesburg, R. Esteve, L. M. Fernandes, A. L.
Ferreira, E. D. Freitas, A. Goldschmidt, D. Gonz´alez-
D´ıaz, R. M. Guti´errez, J. Hauptman, C. A. Hen-
riques, J. A. Morata, V. Herrero, B. Jones, L. Labarga,
A. Laing, P. Lebrun, I. Liubarsky, N. L´opez-March,
D. Lorca, M. Losada, J. Mart´ın-Albo, G. Mart´ınez-
Lema, A. Mart´ınez, F. Monrabal, C. M. Monteiro, F. J.
Mora, L. M. Moutinho, M. Nebot-Guinot, P. Novella,
D. Nygren, B. Palmeiro, A. Para, J. P´erez, M. Querol,
L. Ripoll, J. Rodr´ıguez, F. P. Santos, J. M. Dos San-
tos, L. Serra, D. Shuman, A. Sim´on, C. Sofka, M. Sorel,
J. F. Toledo, J. Torrent, Z. Tsamalaidze, J. F. Veloso,
J. White, R. Webb, N. Yahlali, and H. Yepes-Ram´ırez,
“Background rejection in NEXT using deep neural net-
works,” Journal of Instrumentation, vol. 12, p. T01004,
jan 2017.

[6] “Detection and Quantiﬁcation Capabilities,” in Multi-
Agency Radiological Laboratory Analytical Protocols
(MARLAP) Manual, vol. III, United States Environ-
mental Protection Agency, July 2004.

Figure 10: ESSE versus Pulse Height for diﬀerent sets of detection events, and corresponding classiﬁer scores for those events.
The events along the boundary between the two classes have classiﬁer scores closer to 1
2 , correspondingly abstention of these
events happens at higher and higher analyte to interferent ratios.

[7] M. E. Hellman, “The Nearest Neighbor Classiﬁcation
Rule with a Reject Option,” IEEE Transactions on
Systems Science and Cybernetics, vol. 6, no. 3, pp. 179–
185, 1970.

[8] S. Thulasidasan, G. Chennupati, J. Bilmes, T. Bhat-
tacharya, and S. Michalak, “On Mixup Training: Im-
proved Calibration and Predictive Uncertainty for Deep
Neural Networks,” arXiv, may 2019.

[9] S. Thulasidasan, T. Bhattacharya, J. Bilmes, G. Chen-
nupati, and J. Mohd-Yusof, “Combating Label Noise in
Deep Learning Using Abstention,” arXiv, may 2019.

[10] C. K. Chow, “On Optimum Recognition Error and
Reject Tradeoﬀ,” IEEE Transactions on Information
Theory, vol. 16, no. 1, pp. 41–46, 1970.

[11] C. De Stefano, C. Sansone, and M. Vento, “To reject
or not to reject: that is the question - an answer in
case of neural classiﬁers,” IEEE Transactions on Sys-
tems, Man and Cybernetics Part C: Applications and
Reviews, vol. 30, no. 1, pp. 84–94, 2000.

[12] Y. Geifman and R. El-Yaniv, “Selective Classiﬁcation
for Deep Neural Networks,” Advances in Neural Infor-
mation Processing Systems, vol. 2017-Decem, pp. 4879–
4888, may 2017.

[13] G. F. Knoll, Radiation detection and measurement.

John Wiley & Sons, 2010.

[14] L. A. Currie, “Limits for qualitative detection and
quantitative determination. Application to radiochem-
istry,” Analytical Chemistry, vol. 40, pp. 586–593, mar
1968.

[15] D. Kahneman and A. Tversky, “On the psychology
of prediction.,” Psychological Review, vol. 80, no. 4,
pp. 237–251, 1973.

[16] S. M. Ross, A ﬁrst course in probability. Pearson, 1988.
[17] C. E. Aalseth, R. M. Bonicalzi, M. G. Cantaloub, A. R.
Day, L. E. Erikson, J. Fast, J. B. Forrester, E. S. Fuller,
B. D. Glasgow, L. R. Greenwood, E. W. Hoppe, T. W.
Hossbach, B. J. Hyronimus, M. E. Keillor, E. K. Mace,
J. I. McIntyre, J. H. Merriman, A. W. Myers, C. T.
Overman, N. R. Overman, M. E. Panisko, A. Seifert,
G. A. Warren, and R. C. Runkle, “A shallow under-
ground laboratory for low-background radiation mea-
surements and materials development,” Review of Sci-
entiﬁc Instruments, vol. 83, p. 113503, nov 2012.
[18] Y. C. Hsu, Y. Shen, H. Jin, and Z. Kira, “General-
ized ODIN: Detecting Out-of-Distribution Image with-
out Learning from Out-of-Distribution Data,” Proceed-
ings of the IEEE Computer Society Conference on
Computer Vision and Pattern Recognition, pp. 10948–
10957, 2020.

[19] C. Troemel, “Simple.” https://https://github.com/

chrisstroemel/Simple, 2018.

[20] L. McInnes, J. Healy, and J. Melville, “UMAP: Uniform
Manifold Approximation and Projection for Dimension
Reduction,” feb 2018.

[21] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer,
R. Weiss, V. Dubourg, J. Vanderplas, A. Passos,
D. Cournapeau, M. Brucher, M. Perrot, and E. Duches-
nay, “Scikit-learn: Machine learning in Python,” Jour-
nal of Machine Learning Research, vol. 12, pp. 2825–
2830, 2011.

[22] I. Goodfellow, Y. Bengio, and A. Courville, Deep Learn-
ing. MIT Press, 2016. http://www.deeplearningbook.
org.

10

[23] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Brad-
bury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein,
L. Antiga, A. Desmaison, A. Kopf, E. Yang, Z. De-
Vito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner,
L. Fang, J. Bai, and S. Chintala, “Pytorch: An imper-
ative style, high-performance deep learning library,” in
Advances in Neural Information Processing Systems 32
(H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch´e-
Buc, E. Fox, and R. Garnett, eds.), pp. 8024–8035, Cur-
ran Associates, Inc., 2019.

Appendices

A. Details of labeling for and design of Tail

Over Total Classiﬁer

In order to determine the diﬀerence in shape
of electrical pulses measured from a Cs2LiYC6
(CLYC) detector when exposed to neutrons and
photons, the detector was exposed to high-intensity
252Cf and 137Cs, respectively.
This does not
generate an environment with only neutrons or
photons present for several reasons: 252Cf, while
mostly a neutron source, also produces photons,
and neutrons and photons are present from nat-
urally occurring radioactive material and cosmo-
genic sources. Therefore, determining associating
the ground truth particle origin with a given event
is diﬃcult or impossible. To determine a set of la-
bels for each event, identifying each as from neu-
trons or photons, we used an approximate process.
We used a Uniform Manifold Approximation and
Projection (UMAP) dimensionality reduction tech-
nique [20] to reduce the pulses into two components.
It was noticed that the ﬁrst component varied with
respect to the curvature around the maximum mag-
nitude of the pulse. We set a threshold on that
component, all pulses above which were labeled as
neutron and below which were labeled as photon.
Through tests with data sets including exposure to
high intensity photon sources, we beleive that this
labeling scheme is > 99% accurate. However, as
noted in the text, the physical accuracy of this la-
beling scheme is not important to this work - ab-
stention and detection limit minimization is equally
applicable to any set of labels.

The classiﬁer for Tail over Total Classiﬁcation
(TOTC) is a multilayer perceptron, of which we
use scikit-learn’s implementation [21]. TOTC
has 5 layers with 10 nodes in each, and uses ReLU
activation.
It is trained by minimizing categori-
cal cross entropy between predictions and the la-

bels as described above using an Adam optimizer
with weight decay of 10−2. These parameters were
chosen empirically to optimize the area under the
receiver operating characteristic curve.

B. Derivation of measurement uncertainty
in a counting based classiﬁer experiment

a and σ2

We perform a measurement and obtain a number
of counts X, generated independently by analyte
(Xa) and interferent (Xi), of which these counts
may be modeled as having means µ, µa and µi; and
variances σ2, σ2
i , respectively. We classify
each count using some classiﬁer, generating counts
Ya|a of truly analyte counts classiﬁed as analyte,
Ya|i truly analyte counts classiﬁed as interferent,
Yi|a truly interferent counts classiﬁed as analyte,
and Yi|i truly interferent counts classiﬁed as inter-
ferent. The probability that a count in X is cor-
rectly classiﬁed as analyte is denoted as pa, and the
probability that it is correctly classiﬁed as inter-
ferent is denoted as pi. We donote their incorrect
counterparts as qa and qi, respectively. Note that
pa + qa ≤ 1 and pi + qi ≤ 1, where the inequality
holds only when there is non-zero abstention.

We write the expected value of analyte count

Ya = Ya|a + Ya|i as

E (Ya) = E (Xapa + Xiqi) = µapa + µiqi

(12)

and the equivalent for interferent as

E (Yi) = E (Xaqa + Xipi) = µaqa + µipi

(13)

Then, the method of moments allows us to obtain
an unbiased estimator by ﬁrst setting the measured
analyte count equal to the expected analyte count
and solving the system of equations for µa (which
we now denote ˆµa because of the estimation of the
expectation value). This obtains

ˆµa =

piYa − qiYi
papi − qaqi

(14)

We estimate the variance in ˆmua using the con-

ditional variance formula [16]:

Var (ˆµa) = E [Var (ˆµa|Xa, Xi)]+Var (E [ˆµa|Xa, Xi])
(15)
Several of the random variables are multinomial
(i.e. Xa is a sum of Ya|a, Ya|i and any unclassiﬁed
counts which are truly analyte Ya|u). We ﬁnd that
the mean and variance for Ya|a in that case is Xapa

11

cross-entropy against the set of labels. The explicit
separation of numerator and denominator follows
the law of conditional probability to decompose the
model output into the conditional probability that
a pulse is both a Gas Gain pulse and is from in-
distribution data (the numerator); and the prob-
ability that the pulse is from in-distribution data
(the denominator). Then, we can use the numera-
tor as our classiﬁer score. The score be close to 1
2 if
the classiﬁer is unsure about which class it belongs
in, or if it comes from out-of-distribution data.

and Xapa (1 − pa), and its covariance with Ya|i is
−Xapaqa. Performing similar substitutions for all
variables, we successively obtain

E [Var (ˆµa|XaXi)] = E

(cid:34)

1
(papi − qaqi)2
(cid:1)
+Var (cid:0)piYi|a − qiYi|i|XaXi

(cid:8)Var (cid:0)piYa|a − qiYa|i|XaXi

(cid:1)(cid:9)(cid:3)

(16)

and

Var (cid:0)piYa|a − qiYa|i|XaXi

(cid:1) = Xa
i qa (1 − qa) + 2pipaqiqa

i pa (1 − pa)
(cid:3)

(cid:2)p2

(17)

+q2

Var (cid:0)piYi|a − qiYi|i|XaXi
+q2

(cid:1) = Xi

(cid:2)p2

i pi (1 − pi) + 2p2

i qi (1 − qi)
i q2
i

(cid:3)

(18)

Finally, we see that

Var (E [ˆµa|XaXi]) = Var (Xa) = σ2
a

(19)

Which results in the overall equation for uncer-
tainty in eq. (10), reprinted below for convenience.

Var (ˆµa) = σ2

a +

1
(papi − qaqi)2

· (cid:8) ˆmua
(cid:2)p2
+µi

i pa (1 − pa) + q2

(cid:2)p2
i qi (1 − qi) + q2

i qa (1 − qa) + 2pipaqiqa

i pi (1 − pi) + 2p2

i q2
i

(cid:3)
(cid:3)(cid:9) (20)

In the text, we use µa and µi as notation for the
estimated means (instead of ˆµa and ˆµi), as we never
reference the true mean in the text.

C. Details of ultra-low background propor-

tional counter classiﬁer

To create the classiﬁer separating Gas-Gain from
Other events in the ultra-low background propor-
tional counter data set, we create a multilayer per-
ceptron of 5 layers, with 100 nodes per layer and ac-
tivated with ReLU, following best practices as laid
out in [22] and other sources. We use the pytorch
framework [23] and implement a Generalized ODIN
architecture [18] to encourage low scores for data
seen during inference which appears to be out-of-
distribution from the training set. This method
requires that the penultimate layer of the neural
network is split explicitly into a numerator and
denominator; their quotient is then minimized by

12

