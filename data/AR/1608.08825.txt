6
1
0
2

g
u
A
1
3

]
E
M

.
t
a
t
s
[

1
v
5
2
8
8
0
.
8
0
6
1
:
v
i
X
r
a

On predictability of ultra short AR(1) sequences ∗

Nikolai Dokuchaev † and Lin-Yee Hin
Department of Mathematics & Statistics, Curtin University,
GPO Box U1987, Perth, 6845 Western Australia

October 17, 2018

Abstract

This paper addresses short term forecast of ultra short AR(1) sequences (4 to 6 terms only)
with a single structural break at an unknown time and of unknown sign and magnitude. As
prediction of autoregressive processes requires estimated coeﬃcients, the eﬃciency of which
relies on the large sample properties of the estimator, it is a common perception that predic-
tion is practically impossible for such short series with structural break. However, we obtain a
heuristic result that some universal predictors represented in the frequency domain allow cer-
tain predictability based on these ultra short sequences. The predictors that we use are universal
in a sense that they are not oriented on particular types of autoregressions and do not require
explicit modelling of structural break. The shorter the sequence, the better the one-step-ahead
forecast performance of the smoothed predicting kernel. If the structural break entails a model
parameter switch from negative to positive value, the forecast performance of the smoothed
predicting kernel is better than that of the linear predictor that utilize AR(1) coeﬃcient esti-
mated from the ultra short sequence without taking the structural break into account regardless
whether the innovation terms in the learning sequences are constructed from independent and
identically distributed random Gaussian or Gamma variables, scaled pseudo-uniform variables,
or ﬁrst-order auto-correlated Gaussian process.

Keywords: predicting, non-stationarity, structural break, autoregressive process.

∗We acknowledge provision of ICTsupport and computing resources by Curtin IT Services http://cits.curtin.edu.au.
Curtin Information Technology Services (CITS) provides Information and Communication Technology systems and
services in support of Curtin’s teaching, learning, research and administrative activities. We acknowledge use of com-
puting resources from the NeCTAR Research Cloud http://www.nectar.org.au. NeCTAR is an Australian Government
project conducted as part of the Super Science initiative and ﬁnanced by the Education Investment Fund. This work
was supported by ARC grant of Australia DP120100928

†Corresponding author. Department of Mathematics and Statistics, Curtin University, GPO Box U1987, Perth,

6845 Western Australia; email: N.Dokuchaev@curtin.edu.au

 
 
 
 
 
 
1 Introduction

In this paper, we readdress the problem of one-step-ahead forecast of a ﬁrst order autoregressive

process, AR(1), with one structural break, i.e., a permanent change, in the AR(1) model parameter.

Speciﬁcally, we consider the scenario where the learning sequence, i.e., the segment of time series

process used for model estimation and forecast, is very short, and where the structural break occurs

at a random time point in the learning sequence.

Forecasting autoregressive process is a well developed area with well known results. One strand

of literature addresses this problem via the time series models that are primarily speciﬁed from the

time-domain modelling perspective (see, among many others, Box and Jenkins, 1976; Abraham and Ledolter,

1986; Stine, 1987; Cryer et al., 1990; Cortez et al., 2004; Hamilton, 1994; Xia and Zheng, 2015,

and the references therein), or via the exponential smoothing and the ﬁltering techniques con-

structed based on state-space approach where the smoothers and ﬁlters are primarily character-

ized in the time-domain (see, e.g., Roberts, 1982; Williams, 1987; Paige and Saunders, 1977;

Chatﬁeld and Yar, 1988; Ord et al., 1997; Chatﬁeld et al., 2001; Hyndman et al., 2002; Berm´udez et al.,

2006; Hyndman et al., 2008, and references therein). A separate yet related strand of litera-

ture addresses this problem via smoothing and ﬁltering techniques where the smoothers and ﬁl-

ters are primarily characterized in the frequency-domain (see, e.g., Cambanis and Soltani, 1984;

Ledolter and Kahl, 1984; Lyman and Edmonson, 2001; Dokuchaev, 2012; 2014; 2016, and refer-

ences therein). In this paper, we address this problem via the convolution of a near-ideal causal

smoothing ﬁlter and a predicting kernel that are primarily characterized in the frequency-domain

(Dokuchaev, 2012; 2014; 2016).

Many strategies have been proposed to address the practical concern of possible model param-

eters structural break in the learning sequence that may compromise modelling eﬃciency and

forecast performance of the time series model (see, among many others, Bagshaw and Johnson,

1977; Sastri, 1986; Andrews, 1993; Bai and Perron, 1998; 2003; Pesaran and Timmermann, 2004;

Clements and Hendry, 2006; Davis et al., 2006; Lin and Wei, 2006; Kim et al., 2009; Rossi, 2013;

1

Pesaran et al., 2013, and the references therein). Implementation of these strategies require the

availability of learning sequences that are considerably longer that those considered in this pa-

per. We cite a few examples. The method proposed by Bai and Perron (1998; 2003) to esti-

mate the timing of structural break requires at least 10 observations on either side of the break.

Pesaran and Timmermann (2007) simulated random processes that each contain 100 to 200 obser-

vations to mimic learning sequences with structural break in AR(1) model parameter in order to

assess the performance of their proposed set of cross-validation and forecast combination proce-

dures that use pre-break and post-break data to perform time series forecast. Giraitis et al. (2013)

simulated time series processes that each contain 200 observations to mimic learning sequences

with structural break in the mean of the simulated random processes in order to assess the perfor-

mance of their proposed one-step-ahead forecast algorithms based on adaptive linear ﬁltering.

In this paper, we consider the scenario when the learning sequence only contain 4 to 6 data points,

and, as such, are too short to eﬀectively apply structural break timing estimation strategies, and

to eﬃciently estimate pre-break and post-break AR(1) model parameters. We consider a family

of linear ﬁlters proposed by Dokuchaev (2016) where the impulse response function is obtained

by inverse Z-transform of the product between the transfer function of a family of near-ideal

causal smoothers (Dokuchaev, 2016) and the transfer function of a family of predicting kernels

(Dokuchaev, 2014). The Monte Carlo experiments reported in Dokuchaev (2016) have demon-

strated clear advantage of using the convolution of the near-ideal causal smoother and the predict-

ing kernel compared to using the predicting kernel alone to generate the impulse response function

for the linear predictor in terms of one-step-ahead forecast performance of AR(2) processes with-

out structural break. However, their relative forecast performance have never been assessed in the

context of AR(1) processes with a single, unknown, random time point structural break in a very

short learning sequence. Additionally, numerical experiments reported in Dokuchaev (2016) uti-

lize 100 observations in the learning sequence. This begs the question whether the proposed linear

predictor will perform well in the context of a very short learning sequence with structural break.

This paper seeks to close this gap in the literature.

Following the choice of benchmark used in, among others, Pesaran and Timmermann (2007) and

2

Giraitis et al. (2013), we use the one-step-ahead forecasts from an AR(1) model that ignores struc-

tural break and utilize all observations, pre-break and post-break, as our benchmark since this is

an appropriate model to use in situations with no breaks. The main contribution of this paper is

our demonstration via simulation experiment that the one-step-ahead forecast performance of this

family of linear ﬁlters is better than that of our chosen benchmark. Additionally, its performance

is comparable to that of the one-step-ahead forecasts from an AR(1) model with the same model

parameter as the synthetic AR(1) model parameter used to simulate the post-break random process.

The rest of the paper is as follows. Section 2 details the problem formulation. Section 3 presents

the Monte Carlo simulation results. Section 4 concludes.

2 Problem setting

Consider a stochastic discrete time process described by AR(1) autoregression

x(t) = β(t) x(t

β1 ,
β2 ,

β(t) =




(βmin, βmax), β2 ∈

t = 0, . . . , d

1

−

x(

1) = 0,

−

(1)

1) + σ η(t) ,

−
t < θ ,
θ .
t

≥

where β1 ∈
innovation term of the time series. This model features a single random structural break to take

(βmin, βmax), βmin < βmax,

), and η(t) is the

βmax|

< 1, σ

βmin|

< 1,

(0,

∞

∈

|

|

place at a random time θ with the values in the set

1, . . . , d

{

2

}

−

. We assume that η(t) are mutually

independent for all t and independent on η.

We consider predicting problem for this process in the case where an ultra short sequences with no

more than six data points are available.

2.1 Special predictors

We investigate the performance of linear time-invariant predictors with an output

y(t) =

t

Xτ=0

τ) x(τ),

h(t

−

d

t

≤

−

1,

3

(2)

≤

where d

6. The process y(t) is supposed to approximate the process x(t + 1), i.e., (2) represents

a one-step-ahead predictor. The predictor is deﬁned by a impulse response function h : Z

R,

→

where Z is the set of integers, and R is the set of real numbers.

In our experiments, we calculate predicting kernels h via their Z-transforms that are represented

explicitly, such that

h(t) =

1
2π Z

Here complex-valued functions H : C

→

π

H

eiω

ei ω tdω ,

t

∈

Z.

(3)

(cid:16)

π
−
C are transfer functions of the corresponding predictors.

(cid:17)

In our experiments, we used two diﬀerent transfer functions

and

Here z

C,

∈

H(z) = K(z) ,

H(z) = K(z) F(z).

(4)

(5)

(6)

.

γ−

r # !

K(z) = z

1

exp

−

"−

γ
z + 1

−

The function K(z) is the transfer function of an one-step predictor from Dokuchaev (2016); r > 0,

γ > 0 are the parameters.

In (5),

F(z) =

exp

(1

a)p

−
z + a

m

+ G(z)
!

,

(7)

G(z) =

ξ(a, p) +

−

ξ(a, p) = exp[

(1

γ(a, p)
N
1] ,
a)p
−

−
a

−
2ξ(a, p) .
p
−

γ(a, p) =

1

|

−

|

1)Nz−

(
−
(cid:16)

N

−

1

,

(cid:17)

Here a

∈

(0, 1), p

∈

(1/2, 1), m

1, and N

≥

≥

1, are the parameters, m, n

Z. The function F(z) is

∈

the transfer function for a smoothing ﬁlter introduced in Dokuchaev (2016).

It can be noted that these linear predictors were constructed for semi-inﬁnite one-sided sequences,

since the corresponding kernels h(t) have inﬁnite support on Z. In theory, the performance of these

4

 
 
predictors is robust with respect to truncation; see the discussion on robustness in Dokuchaev

(2012) and Dokuchaev (2016). However, we found, as a heuristic result of this paper, that their

application to the ultra short series also brings some positive result, meaning that these sequences

feature some predictability. Worthy of note again is that implementation of these predictors does

not involve explicit modelling of and adjustment for structural break, including break time and

magnitude, Moreover, these predictors do not even require that the underlying process is an au-

toregressive process or any other particular kind of a processes.

2.2 Comparison with other predictors

We compare the performance of our predictors with an “ideal” linear predictor

yideal(d

−

1) = β(d) x(d

1) ,

−

(8)

where β(d) = β2 is the post-break AR(1) model parameters that generate the post-break obser-

vations x(d

−

1) and x(d). This predictor is not feasible unless β(d) is supposed to be known.

In our setting, β(d) is unknown and has to be estimated from the observations. We will use the

performance of this predictor as a benchmark.

Additionally. we will compare the performance of our predictors with the performance of the

predictor

yAR(1)(d

−

1) = ˆβ(d) x(d

1) ,

−

(9)

where ˆβ(d) is estimated by ﬁtting an AR(1) model to the sequence

x(τ)
}

{

d
1
τ=0 that involve pre-break
−

and post-break observations using the build-in function ar.ols() in the R computing environment

(R Core Team, 2016) implementing the ordinary least squares model parameter estimation strategy

(pp. 368-370, Luetkepohl, 2008). This is an appropriate model estimation procedure to use if the

sequence

x(τ)
}

{

d
1
τ=0 does not contain structural break. By choosing this predictor as the benchmark
−

for our numerical experiment, we seek to address the question of how costly is it to ignore breaks

when performing one-step-ahead forecasting the direction of a time series using the prediction

algorithms considered, i.e., (4), (5), and (8), relative to (9).

5

3 Simulation experiment

We perform simulation experiments to investigate the one-step ahead forecast performance of (4),

(5), and (8), relative to (9) in predicting x(d), given

x(τ)
}

{

d
1
τ=0 simulated from (1) using four diﬀerent
−

speciﬁcations of (β1, β2)

1. β1 ∈

(0, 1), β2 ∈

(0, 1),

2. β1 ∈

1, 1), β2 ∈
(
−

1, 1),

(
−

3. β1 ∈

1, 0), β2 ∈
(
−

(0, 1),

4. β1 ∈

(0, 1), β2 ∈

1, 0),

(
−

and four diﬀerent speciﬁcations of η(t)

1. Independent and identically distributed (IID) Gaussian innovations: In this setting, we spec-

ify

N (0, 1)

η(t)

∼

as IID random numbers drawn from the standard Gaussian distribution.

2. IID shifted Gamma innovation: In this setting, we specify

η(t) = γ0(t)

√2

−

(10)

(11)

where

γ0(t)
}
{

d
1
t=0 were random numbers drawn randomly from Gamma distribution with
−

shape parameter 2 and scale parameter 2−

1/2, i.e., Γ(2, 2−

1/2).

3. IID scaled pseuo-uniform innovation: In this setting, we specify

η(t) = √12

exp(t + 3 arctan(s)
(cid:0)

− ⌊

exp(t + 3 arctan(s))

⌋ −

1/2

(cid:1)

(12)

where t = 1, . . . , d

−

be performed.

1, s = 1, . . . , Nsim, and where Nsim is the total number of simulations to

6

4. Auto-correlated Gaussian innovation: In this setting, we specify

η(t) = 2−

1/2 (η0(t) + η0(t

1))

−

(13)

1
−

where

t=0 were IID random numbers drawn randomly from N (0, 1) and the lag-one
d
η0(t)
}
{
auto-correlation is E[η(t)η(t

1)] = 0.5.

−

For this simulation experiment, the linear predictors (4) and (5) are implemented in the form of (2)

as

where y(d

where h(t

−

−

1) = yKH(d

τ) = hKH(t

−

−

d

1
−

h(t

−

Xτ=0
1) = yK(d

−
τ) = hK(t

−

1) and y(d

τ) and h(t

−

y(d

−

1) =

τ) x(τ)

x(d) ,

≈

(14)

1) are the one-step-ahead forecasts, and

−
τ) are the impulse response functions for

(4) and (5) respectively. Following the choice of parameters used in Dokuchaev (2016), we set

a = 0.6, p = 0.7, N = 100, m = 2, for the smoothing ﬁlter (7), and set γ = 1.1, for the predicting

kernel (6). We investigate the sensitivity of the predicting kernel for some diﬀerent values of r,

0.8, 1.1, 1.5, 2

}

, and we consider three diﬀerent lengths of ultra short sequence d, where

where r

∈ {
4, 5, 6

.
}

d

∈ {

The ideal linear predictor (8) is implemented as

yideal(d

−

1) = β2(d) x(d

1)

−

≈

x(d) .

(15)

For this predictor, one needs to know β2(d), i.e., the post-break AR(1) model parameters used

to simulate x(d). In practice, it is impossible to know β2(d). We include (15) as it represents a

theoretical ideal benchmark.

The linear predictor (9) is implemented as

yAR(1)(d) = ˆβ(d) x(d

1)

−

≈

x(d) ,

(16)

where ˆβ(d) is estimated by ﬁtting an AR(1) model to the learning sequence

x(τ)
}

{

d
1
τ=0. This is a com-
−

monly used approach in AR(1) time series forecasting, and one that depends on the large-sample

properties of the available time series for eﬃcient model parameter estimation. We are interested

7

to investigate the ﬁnite-sample properties in terms of forecast performance of (14) relative to (16)

in the context of ultra short learning sequences considered in this paper.

For each combination of (β1, β2), η(t), r, and d, we perform Nsim simulations where Nsim ∈ {
105, 2

×
. For each simulation, we simulate an AR(1) processes with a single random

105, 3

105

1

×

×

}

structural break at random unknown time point following the data generation process (1), each

containing d + 1 observations. where β1(t), β2(t), σ, and η(t) are mutually independent. The ﬁrst d

observations are used as the sequence based on which we forecast the (d + 1)-th observation.

Let E[
·

] denote the sample mean across the Nsim Monte Carlo trials performed for each scenario

indexed by s where s = 1, . . . , Nsim. Speciﬁcally, we let

eKH =

E (x(d)

yKH(d

eK =

eideal =

eAR(1) =

(cid:16)

(cid:16)

(cid:16)

(cid:16)

E (x(d)

E (x(d)

E

x(d)

(cid:0)

−

−

−

−

1))2

−
1))2

1/2

(cid:17)
1/2

,

,

(cid:17)
1))2

2

1)
(cid:1)

−

−

1/2

(cid:17)

1/2

,

,

(cid:17)

yK(d

−
yideal(d

yAR(1)(d

be the sample root-mean-squared error (RMSE) for (14) that implement (4) and (5), (15), and (16)

respectively.

We carry out the simulation experiments in the R computing environment (R Core Team, 2016).

Simulation of the learning sequence is carried out by iterative application of (1). The estimation

of AR(1) parameter ˆβ for the implementation of (16) is performed using the ar.ols() script in

R. Numerical integrations carried out to map (4) and (5) to their respective impulse response func-

tions to be used in (14) are implemented via the myintegrate() script in the R add-on package

elliptic proposed in Hankin (2006).

Table 1 depicts the simulation experiments results for the setting with β1, β2 ∈
∼
N (0, 1). For a short sequence of length d = 4, 5, 6, and for the four diﬀerent values of pre-

(0, 1) and η(t)

dicting kernel parameter r, r = 0.8, 1.1, 1.5, 2, the RMSE of the smoothed predicting kernel

linear predictor, is smaller than the RMSE of the linear predictor that utilize AR(1) model pa-

rameter estimated based on the learning sequence ignoring the presence of structural break (16).

8

The shorter the learning sequence, the better the performance of the smoothed predicting kernel

linear predictor. This trend is consistent across three diﬀerent sizes of Monte Carlo simulation

Nsim ∈ {

1

×

104, 2

104, 3

104

.

}

×

×

Worthy of note is that this smoothed predictor does not require explicit modelling of structural

break. In practice, when the available learning sequence is short, and the model parameter struc-

tural break time and magnitude uncertain, it is not possible to eﬃciently apply structural break

estimation and adjustment procedures for parameter estimation and time series forecasting due

to series length constraint. In this context, the smoothed predicting kernel (5) appears to be an

alternative approach that may oﬀer satisfactory forecast performance, circumventing the need of

resorting to model parameter estimation that ignore structure break.

The fact that the RMSE of the predicting kernel linear predictor without smoothing (4) is larger

than the RMSE of the linear predictor (16) highlights the role of the near-ideal causal smoother

(7) in improving the forecast performance of (4). By dampening the high frequency noise, the

smoothed prediction kernel is more able to capture the salient features of the simulated AR(1)

process with random structural break based on a short learning sequence in order to deliver better

one-step-ahead forecast performance than the linear predictor (16). Without the aid of the smooth-

ing kernel, the performance of the predicting kernel (4) is, in general, even poorer than that of the

linear predictor (16) that relies on model parameter estimate from an AR(1) model that ignores

structural break.

It is not surprising that the performance of the linear predictor (16) is poorer than the ideal predictor

(15). Utilizing pre-break and post-break data to estimate post-break model parameter when the

break time and magnitude are unknown inevitably leads to parameter estimation error. Although

cross-validation methods have been proposed to utilize pre-break and post-break data to use pre-

break data to estimate the parameters of the model used to compute out-of sample forecasts (see,

e.g., Pesaran and Timmermann, 2007), the number of observations required to implement these

methodologies is considerably larger than those we consider in this paper.

9

Table 2 depicts the results of simulations with η(t)

N (0, 1) for three other model parameter

∼

settings, i.e., a wider range of possible model parameters with β1, β2 ∈
shift from negative to positive values β1 ∈
(0, 1), β2 ∈
positive to negative values β1 ∈
predicting kernel linear predictor (5) appears to be dependent on the sign of β2. If β2 ∈
performs better than (16), and vice versa.

1, 0), β2 ∈
(
−
1, 0). The forecast performance of the smoothed
(
−

(0, 1), and a model parameter shift from

1, 1), a model parameter

(0, 1), (5)

(
−

Table 3 depicts a subset of the simulation results pertaining to the setting where η(t) is as deﬁned

in (11), while Table 4 depicts those pertaining to the setting where η(t) is as deﬁned in (13). They

shows similar trends as those demonstrated for those of (10) as depicted in Table 1 and Table 2

above.

However, the numerical results pertaining to simulation scenarios with IID innovation terms are in

some ways diﬀerent from those with correlated innovation terms. Table 5 depicts a subset of the

simulation results pertaining to the setting where η(t) is as deﬁned in (12) where E[η(t)η(t

1)] =

−

0.5. While the forecast performance of the smoothed predicting kernel (5) is still better than the

linear predictor (16) for β1 ∈
auto-correlated, it is not so for the remaining three simulation scenarios.

1, 0) and β2 ∈
(
−

(0, 1) in this context where the innovation terms are

10

Table 1: One-step-ahead forecast performance with β1, β2 ∈
θ
∈ {

N (0, 1).

, and η(t)

2, . . . , d

−

∼

2

}

(0, 1), random structural break at

eideal

eAR(1)

eK

eKH

eideal/eAR(1)

eK/eAR(1)

eKH/eAR(1)

r = 0.8, d = 4
r = 0.8, d = 5
r = 0.8, d = 6

r = 1.1, d = 4
r = 1.1, d = 5
r = 1.1, d = 6

r = 1.5, d = 4
r = 1.5, d = 5
r = 1.5, d = 6

r = 2, d = 4
r = 2, d = 5
r = 2, d = 6

r = 0.8, d = 4
r = 0.8, d = 5
r = 0.8, d = 6

r = 1.1, d = 4
r = 1.1, d = 5
r = 1.1, d = 6

r = 1.5, d = 4
r = 1.5, d = 5
r = 1.5, d = 6

r = 2, d = 4
r = 2, d = 5
r = 2, d = 6

r = 0.8, d = 4
r = 0.8, d = 5
r = 0.8, d = 6

r = 1.1, d = 4
r = 1.1, d = 5
r = 1.1, d = 6

r = 1.5, d = 4
r = 1.5, d = 5
r = 1.5, d = 6

r = 2, d = 4
r = 2, d = 5
r = 2, d = 6

Panel (a): β1, β2 ∈

(0, 1), Nsim = 1

0.30031 0.44385 0.39284 0.34568
0.29881 0.37738 0.39343 0.34661
0.29970 0.35256 0.39499 0.34795

×
0.67661
0.79182
0.85004

105

0.88506
1.04254
1.12032

0.77882
0.91847
0.98691

0.30082 0.44232 0.39817 0.34568
0.29971 0.37597 0.39765 0.34586
0.29986 0.35161 0.39970 0.34686

0.68010
0.79717
0.85283

0.90018
1.05767
1.13678

0.78152
0.91991
0.98640

0.30066 0.46608 0.40348 0.34393
0.29979 0.37684 0.40480 0.34393
0.29939 0.35283 0.40506 0.34468

0.64508
0.79554
0.84853

0.86568
1.07420
1.14802

0.73791
0.91267
0.97689

0.30010 0.44740 0.41242 0.34097
0.30127 0.37954 0.41351 0.34376
0.30108 0.35406 0.41589 0.34351

0.67077
0.79379
0.85037

0.92182
1.08950
1.17464

0.76210
0.90572
0.97022

Panel (b): β1, β2 ∈

(0, 1), Nsim = 2

0.29981 0.44456 0.39188 0.34614
0.29951 0.37979 0.39302 0.34690
0.30024 0.35278 0.39404 0.34810

×
0.67439
0.78861
0.85107

105

0.88152
1.03484
1.11696

0.77862
0.91340
0.98673

0.29946 0.44962 0.39626 0.34470
0.29959 0.38039 0.39761 0.34553
0.29981 0.35372 0.39946 0.34672

0.66603
0.78759
0.84759

0.88132
1.04526
1.12934

0.76666
0.90836
0.98021

0.30063 0.44241 0.40416 0.34362
0.30073 0.38054 0.40539 0.34494
0.29988 0.35258 0.40506 0.34588

0.67954
0.79027
0.85053

0.91355
1.06529
1.14885

0.77671
0.90643
0.98100

0.30021 0.44961 0.41343 0.34147
0.29953 0.37829 0.41324 0.34205
0.30112 0.35523 0.41516 0.34450

0.66770
0.79179
0.84766

0.91953
1.09237
1.16869

0.75948
0.90420
0.96978

Panel (c): β1, β2 ∈

(0, 1), Nsim = 3

0.30000 0.44340 0.39318 0.34638
0.29977 0.38060 0.39391 0.34738
0.30012 0.35344 0.39397 0.34852

×
0.67659
0.78762
0.84913

105

0.88675
1.03496
1.11468

0.78119
0.91271
0.98607

0.30019 0.44827 0.39724 0.34511
0.30003 0.37772 0.39832 0.34573
0.30053 0.35391 0.39903 0.34743

0.66967
0.79431
0.84917

0.88617
1.05454
1.12750

0.76988
0.91530
0.98171

0.30025 0.44945 0.40429 0.34404
0.29953 0.37719 0.40392 0.34392
0.29961 0.35373 0.40467 0.34501

0.66804
0.79412
0.84700

0.89953
1.07088
1.14402

0.76547
0.91180
0.97537

0.30004 0.44259 0.41150 0.34183
0.29985 0.37641 0.41410 0.34193
0.29997 0.35304 0.41438 0.34299

0.67793
0.79660
0.84967

0.92975
1.10013
1.17374

0.77233
0.90838
0.97152

11

Table 2: Random structural break at θ

2, . . . , d

∈ {

eideal

eAR(1)

eK

2

}
−
eKH

∼
eideal/eAR(1)

, η(t)

N (0, 1) and Nsim = 3

105.

×

eK/eAR(1)

eKH/eAR(1)

r = 0.8, d = 4
r = 0.8, d = 5
r = 0.8, d = 6

r = 1.1, d = 4
r = 1.1, d = 5
r = 1.1, d = 6

r = 1.5, d = 4
r = 1.5, d = 5
r = 1.5, d = 6

r = 2, d = 4
r = 2, d = 5
r = 2, d = 6

r = 0.8, d = 4
r = 0.8, d = 5
r = 0.8, d = 6

r = 1.1, d = 4
r = 1.1, d = 5
r = 1.1, d = 6

r = 1.5, d = 4
r = 1.5, d = 5
r = 1.5, d = 6

r = 2, d = 4
r = 2, d = 5
r = 2, d = 6

r = 0.8, d = 4
r = 0.8, d = 5
r = 0.8, d = 6

r = 1.1, d = 4
r = 1.1, d = 5
r = 1.1, d = 6

r = 1.5, d = 4
r = 1.5, d = 5
r = 1.5, d = 6

r = 2, d = 4
r = 2, d = 5
r = 2, d = 6

Panel (a): β1, β2 ∈
0.29946 0.44548 0.72830 0.42322
0.29988 0.38167 0.75529 0.42575
0.29970 0.36081 0.77564 0.42754

1, 1)
(
−
0.67221
0.78570
0.83064

1.63486
1.97890
2.14972

0.95002
1.11548
1.18496

0.29967 0.44583 0.73269 0.42220
0.30002 0.38609 0.76609 0.42452
0.29954 0.36060 0.79259 0.42832

0.67215
0.77705
0.83065

1.64341
1.98421
2.19794

0.94699
1.09952
1.18778

0.29981 0.45026 0.75434 0.42250
0.30058 0.38192 0.78852 0.42577
0.29974 0.36132 0.81562 0.42766

0.66587
0.78702
0.82957

1.67533
2.06463
2.25735

0.93834
1.11482
1.18361

0.29950 0.45461 0.77642 0.42232
0.29958 0.38185 0.81207 0.42434
0.29965 0.36041 0.84057 0.42723

0.65880
0.78454
0.83141

1.70790
2.12665
2.33228

0.92899
1.11127
1.18541

Panel (b): β1 ∈

1, 0), β2 ∈
(
−

0.30020 0.44990 0.39408 0.34707
0.30020 0.44990 0.39408 0.34707
0.29935 0.36745 0.39601 0.34919

(0, 1)
0.66727
0.66727
0.81467

0.87593
0.87593
1.07770

0.77143
0.77143
0.95028

0.29977 0.44824 0.39914 0.34585
0.29969 0.38747 0.40141 0.34654
0.30021 0.36857 0.40261 0.34920

0.66876
0.77346
0.81455

0.89047
1.03598
1.09238

0.77157
0.89435
0.94745

0.30021 0.44596 0.40667 0.34529
0.29937 0.38618 0.40881 0.34472
0.30039 0.36888 0.41055 0.34667

0.67318
0.77521
0.81433

0.91189
1.05861
1.11297

0.77426
0.89264
0.93979

0.30062 0.47598 0.41782 0.34428
0.30000 0.38460 0.42137 0.34384
0.29983 0.36798 0.42229 0.34552

0.63158
0.78005
0.81481

0.87781
1.09562
1.14760

0.72331
0.89404
0.93896

Panel (c): β1 ∈

(0, 1), β2 ∈

0.29982 0.44890 0.94541 0.48679
0.29966 0.38576 0.98889 0.49084
0.30018 0.36923 1.01892 0.49363

1, 0)
(
−
0.66790
0.77681
0.81300

2.10604
2.56348
2.75955

1.08439
1.27239
1.33692

0.30018 0.45163 0.95963 0.48872
0.29999 0.38558 1.00126 0.49075
0.29929 0.36789 1.03947 0.49352

0.66466
0.77802
0.81353

2.12480
2.59674
2.82552

1.08212
1.27275
1.34151

0.30073 0.44550 0.97856 0.48733
0.29968 0.38577 1.02990 0.49081
0.29997 0.36922 1.07103 0.49593

0.67504
0.77683
0.81244

2.19655
2.66972
2.90078

1.09391
1.27229
1.34319

0.29985 0.45724 1.00339 0.48925
0.29995 0.38617 1.05628 0.49223
0.29945 0.36725 1.10925 0.49614

0.65579
0.77673
0.81538

2.19445
2.73525
3.02041

1.07001
1.27464
1.35097

12

Table 3: One-step-ahead forecast performance with random structural break at θ
Γ(2, 2−
and η(t) = γ0(t)

1/2), and Nsim = 3

√2, where γ0(t)

105.

2, . . . , d

2

,

}

−

∈ {

−

∼
eAR(1)

×
eideal/eAR(1)

eideal

eK

eKH

eK/eAR(1)

eKH/eAR(1)

r = 0.8, d = 4
r = 0.8, d = 5
r = 0.8, d = 6

r = 2, d = 4
r = 2, d = 5
r = 2, d = 6

r = 0.8, d = 4
r = 0.8, d = 5
r = 0.8, d = 6

r = 2, d = 4
r = 2, d = 5
r = 2, d = 6

r = 0.8, d = 4
r = 0.8, d = 5
r = 0.8, d = 6

r = 2, d = 4
r = 2, d = 5
r = 2, d = 6

r = 0.8, d = 4
r = 0.8, d = 5
r = 0.8, d = 6

r = 2, d = 4
r = 2, d = 5
r = 2, d = 6

Panel (a): β1, β2 ∈
0.30084 0.53991 0.39310 0.34721
0.30016 0.42423 0.39442 0.34722
0.29990 0.37654 0.39329 0.34776

(0, 1)
0.55721
0.70754
0.79645

0.72808
0.92973
1.04447

0.64308
0.81846
0.92357

0.29977 0.52893 0.41234 0.34127
0.30001 0.41651 0.41255 0.34243
0.30028 0.37886 0.41402 0.34331

0.56676
0.72029
0.79259

0.77957
0.99049
1.09281

0.64520
0.82214
0.90616

Panel (b): β1, β2 ∈
0.29997 0.56377 0.72394 0.42278
0.29972 0.42580 0.75337 0.42486
0.29922 0.38669 0.78082 0.42880

1, 1)
(
−
0.53207
0.70390
0.77379

1.28410
1.76931
2.01922

0.74992
0.99780
1.10888

0.29970 0.53578 0.77402 0.42328
0.30021 0.42405 0.81194 0.42479
0.30050 0.38871 0.83975 0.42698

0.55937
0.70796
0.77309

1.44466
1.91473
2.16039

0.79002
1.00174
1.09847

Panel (c): β1 ∈

1, 0), β2 ∈
(
−

0.29942 0.53158 0.39353 0.34666
0.29993 0.43544 0.39468 0.34786
0.30003 0.39344 0.39514 0.34948

(0, 1)
0.56327
0.68881
0.76257

0.74029
0.90640
1.00431

0.65212
0.79888
0.88827

0.30088 0.53712 0.41771 0.34455
0.29959 0.43570 0.42138 0.34349
0.29957 0.39110 0.42320 0.34473

0.56018
0.68760
0.76595

0.77770
0.96712
1.08208

0.64149
0.78835
0.88144

Panel (d): β1 ∈

(0, 1), β2 ∈

0.29991 0.54170 0.94341 0.48621
0.30044 0.42537 0.98715 0.49148
0.30079 0.39497 1.02682 0.49603

1, 0)
(
−
0.55365
0.70630
0.76155

1.74155
2.32067
2.59973

0.89756
1.15542
1.25588

0.30080 0.52253 1.00837 0.49099
0.29956 0.43413 1.06041 0.49239
0.30043 0.39440 1.10692 0.49708

0.57565
0.69004
0.76174

1.92978
2.44263
2.80658

0.93963
1.13420
1.26033

13

Table 4: One-step-ahead forecast performance with random structural break at θ
η(t) = √12

exp(t + 3 arctan(s))

1/2

105.

∈ {

2, . . . , d

2

,

}

−

exp(t + 3 arctan(s)
(cid:0)

− ⌊

eideal

eAR(1)

eK

⌋ −
eKH

, and Nsim = 3
(cid:1)

eideal/eAR(1)

×
eK/eAR(1)

eKH/eAR(1)

r = 0.8, d = 4
r = 0.8, d = 5
r = 0.8, d = 6

r = 2, d = 4
r = 2, d = 5
r = 2, d = 6

r = 0.8, d = 4
r = 0.8, d = 5
r = 0.8, d = 6

r = 2, d = 4
r = 2, d = 5
r = 2, d = 6

r = 0.8, d = 4
r = 0.8, d = 5
r = 0.8, d = 6

r = 2, d = 4
r = 2, d = 5
r = 2, d = 6

r = 0.8, d = 4
r = 0.8, d = 5
r = 0.8, d = 6

r = 2, d = 4
r = 2, d = 5
r = 2, d = 6

Panel (a): β1, β2 ∈
0.27537 0.30305 0.33968 0.22866
0.31588 0.49982 0.39241 0.35066
0.28991 0.36228 0.39589 0.32990

(0, 1)
0.90865
0.63198
0.80024

1.12087
0.78510
1.09278

0.75455
0.70157
0.91063

0.27537 0.30289 0.37362 0.23235
0.31588 0.49950 0.40473 0.34447
0.28991 0.36230 0.41445 0.32531

0.90912
0.63239
0.80019

1.23352
0.81027
1.14393

0.76710
0.68963
0.89790

Panel (b): β1, β2 ∈
0.27537 0.33724 0.52333 0.25020
0.31588 0.45315 0.66214 0.41907
0.28991 0.35271 0.71703 0.42148

1, 1)
(
−
0.81653
0.69707
0.82194

1.55182
1.46117
2.03290

0.74190
0.92478
1.19495

0.27537 0.33766 0.56239 0.25452
0.31588 0.45277 0.70309 0.41578
0.28991 0.35254 0.76555 0.42132

0.81551
0.69766
0.82235

1.66555
1.55286
2.17153

0.75377
0.91829
1.19508

Panel (c): β1 ∈

1, 0), β2 ∈
(
−

0.27537 0.30559 0.34224 0.22801
0.31588 0.48764 0.39469 0.35113
0.28991 0.35639 0.39934 0.33230

(0, 1)
0.90109
0.64777
0.81346

1.11993
0.80937
1.12052

0.74613
0.72005
0.93241

0.27537 0.30552 0.37980 0.23065
0.31588 0.48764 0.40948 0.34395
0.28991 0.35629 0.42379 0.32902

0.90129
0.64778
0.81369

1.24313
0.83973
1.18945

0.75495
0.70535
0.92346

Panel (d): β1 ∈

(0, 1), β2 ∈

0.27537 0.37214 0.62981 0.26768
0.31588 0.42228 0.83730 0.47762
0.28991 0.35600 0.90941 0.49547

1, 0)
(
−
0.73995
0.74804
0.81435

1.69240
1.98282
2.55451

0.71930
1.13106
1.39176

0.27537 0.37196 0.67132 0.27473
0.31588 0.42222 0.89167 0.47730
0.28991 0.35638 0.96375 0.49639

0.74032
0.74815
0.81349

1.80483
2.11187
2.70429

0.73860
1.13045
1.39286

14

Table 5: One-step-ahead forecast performance with random structural break at θ
and θ

2, . . . , d
N (0, 1), and Nsim = 3

1), where η0(t)

1/2η0(t) + 2−

, η(t) = 2−

2, . . . , d

1/2η0(t

∈ {

2

∈ {

−

}

eideal

eAR(1)

eK

eKH

eK/eAR(1)

eKH/eAR(1)

−

∼
eideal/eAR(1)

,
2
−
}
105.
×

r = 0.8, d = 4
r = 0.8, d = 5
r = 0.8, d = 6

r = 2, d = 4
r = 2, d = 5
r = 2, d = 6

r = 0.8, d = 4
r = 0.8, d = 5
r = 0.8, d = 6

r = 2, d = 4
r = 2, d = 5
r = 2, d = 6

r = 0.8, d = 4
r = 0.8, d = 5
r = 0.8, d = 6

r = 2, d = 4
r = 2, d = 5
r = 2, d = 6

r = 0.8, d = 4
r = 0.8, d = 5
r = 0.8, d = 6

r = 2, d = 4
r = 2, d = 5
r = 2, d = 6

Panel (a): β1, β2 ∈
0.30035 0.41511 0.26556 0.33784
0.30030 0.35935 0.26788 0.33906
0.29979 0.33688 0.26966 0.33870

(0, 1)
0.72355
0.83566
0.88989

0.63973
0.74544
0.80046

0.81386
0.94354
1.00540

0.30093 0.41192 0.26972 0.33210
0.29996 0.36100 0.27102 0.33213
0.30007 0.33697 0.27332 0.33336

0.73057
0.83093
0.89049

0.65479
0.75077
0.81111

0.80623
0.92005
0.98927

Panel (b): β1, β2 ∈
0.29964 0.38414 0.36390 0.32763
0.29966 0.33359 0.37153 0.32864
0.30046 0.31771 0.37667 0.33065

1, 1)
(
−
0.78002
0.89830
0.94571

0.94730
1.11373
1.18558

0.85290
0.98518
1.04076

0.30027 0.38045 0.38446 0.32176
0.29993 0.33348 0.39512 0.32317
0.30055 0.31620 0.39959 0.32357

0.78925
0.89940
0.95048

1.01055
1.18484
1.26372

0.84574
0.96909
1.02329

Panel (c): β1 ∈

1, 0), β2 ∈
(
−

0.30001 0.41772 0.26456 0.33607
0.29908 0.37058 0.26511 0.33689
0.30038 0.35256 0.26684 0.33917

(0, 1)
0.71820
0.80706
0.85199

0.63335
0.71538
0.75686

0.80454
0.90910
0.96201

0.30012 0.41655 0.27130 0.33070
0.30027 0.37022 0.27162 0.33141
0.30058 0.35262 0.27223 0.33241

0.72051
0.81107
0.85242

0.65132
0.73367
0.77202

0.79392
0.89518
0.94269

Panel (d): β1 ∈

(0, 1), β2 ∈

0.29979 0.34089 0.48870 0.32159
0.29994 0.30282 0.52276 0.32426
0.30005 0.29188 0.53985 0.32555

1, 0)
(
−
0.87942
0.99050
1.02798

1.43359
1.72634
1.84959

0.94338
1.07081
1.11535

0.29978 0.34135 0.52265 0.31501
0.30037 0.30242 0.56744 0.31838
0.29971 0.29286 0.59408 0.32005

0.87822
0.99322
1.02337

1.53112
1.87633
2.02852

0.92284
1.05277
1.09284

15

4 Conclusions

This paper addresses the problem of one-step-ahead forecast of an AR(1) process with a single

structural break at an unknown time and of unknown sign and magnitude within a very short learn-

ing sequence. We analysed, via simulation experiments, the forecast performance of a smoothed

predicting kernel algorithm relative to that of a linear predictor that utilize the AR(1) model param-

eter estimated from the learning sequence without taking into account the presence of structural

break.

It appears that the shorter the learnings sequence, the better the forecast performance of the

smoothed predicting kernel relative to the linear predictor. Regardless whether the innovation

terms in the learning sequences are constructed from IID random Gaussian variables, IID random

Gamma variables, IID scaled pseudo-uniform variables, or ﬁrst-order auto-correlated Gaussian

process, the forecast performance of the smoothed predicting kernel is better than that of the linear

predictor if the AR(1) model parameter switches from a negative value to a positive value in the

1, 0), β2 ∈
learning sequence, i.e., β1 ∈
(
−
ing scenarios considered in the simulation experiments, i.e., β1, β2 ∈
β1 ∈

(0, 1), β2 ∈

(0, 1).

(0, 1). However, it is not so for the other regime switch-

(0, 1), β1, β2 ∈

1, 1), and

(
−

It could be interesting to explore the forecast performance of the smoothed linear predictor in the

context of random-coeﬃcient AR(1) process (see, among others, Leipus et al., 2006) where the

AR(1) model parameter between any two sequential observations are independent and identically

distributed random variables from the uniform distribution U[0, 1]. Additionally, we may explore

the implementing the smoothed predicting linear predictor in the context of adaptive linear ﬁltering

to perform successive, on-line one-step ahead forecast. We leave this for future work.

16

References

Abraham, B. and Ledolter, J. (1986). Forecast Functions Implied by Autoregressive Integrated

Moving Average Models and Other Related Forecast Procedures, International Statistical Re-

view 54(1): 51–66.

Andrews, D. (1993). Tests for Parameter Instability and Structural Change With Unknown Change

Point, Econometrica 61(4): 821–856.

Bagshaw, M. and Johnson, R. (1977). Sequential procedures for detecting parameter change in a

time-series model, 72(359): 593–597.

Bai, J. and Perron, P. (1998). Estimating and Testing Linear Models with Multiple Structural

Changes, Econometrica 66(1): 47–78.

Bai, J. and Perron, P. (2003). Computation and Analysis of Muptile Structural Change Models,

Journal of Applied Econometrics 18(1): 1–2.

Berm´udez, J., Segura, J. and Vercher, E. (2006). Improving Demand Forecasting Accuracy Using

Nonlinear Programming Software, The Journal fo the Operational Research Society 57(1): 94–

100.

Box, G. and Jenkins, G. (1976). Time Series Analysis: Forecasting and Control, Holden Day, San

Francisco, USA.

Cambanis, S. and Soltani, A. (1984). Prediction of Stable Processes: Spectral and Moving Average

Representations, Zeitschrift f¨ur Wahrscheinlichkeitstheorie und Verwandte Gebiete 66(4): 593–

612.

Chatﬁeld, C., Koehler, A., Ord, K. and Snyder, R. (2001). A New Look at Models for Exponential

Smoothing, Journal of the Royal Statistical Society. Series D (The Statistician 50(2): 147–159.

Chatﬁeld, C. and Yar, M. (1988). Holt-Winters Forecasting: Some Practical Issues, Journal of the

Royal Statistical Society. Series D (The Statistician) 37(2): 129–140.

17

Clements, M. P. and Hendry, D. (2006). Forecasting with breaks, in G. Elliott, C. Granger and

A. Timmermann (eds), Handbook of Economic Forecasting, Volume 1, Elsevier B.V., Amster-

dam, pp. 606–651.

Cortez, P., Rocha, M. and Neves, J. (2004). Evolving time series forecasting arma models, Journal

of Heuristics 10(4): 415–429.

Cryer, J., Nankervis, J. and Savin, N. (1990). Forecast error symmetry in arima models, Journal

of the American Statistical Association 85(411): 724–728.

Davis, R., Lee, T. and Rodriguez-Yam, G. (2006). Structural Break Estimation for Nonstationary

Time Series Models, Journal of the American Statistical Association 101(473): 223–239.

Dokuchaev, N. (2012). Predictor for discrete time processes with energy decay on higher frequen-

cies, IEEE Transaction of Signal Processing 60(11): 6027–6030.

Dokuchaev, N. (2014). Volatility Estimation from Short Time Series of Stock Prices, Journal of

Nonparametric statistics 26(2): 373–384.

Dokuchaev, N. (2016). Near-ideal causal smoothing ﬁlters for the real sequences, Signal Process-

ing 118: 285–293.

Giraitis, L., Kapetanios, G. and Price, S. (2013). Adaptive Forecasting in the Presence of Recent

and Ongoing Structural Change, Journal of Econometrics 177(2): 153–170.

Hamilton, J. D. (1994). Time series analysis, Princenton University Press, New Jersey.

Hankin, R. (2006). Introducing elliptic, an R package for elliptic and modular functions, Journal

of Statistical Software 15(7).

Hyndman, R., Koehler, A., Snyder, R. and Grose, S. (2002). A state space framework for auto-

matic forecasting using exponential smoothing methods, International Journal of Forecasting

18(3): 439–454.

18

Hyndman, R., Koehler, A., Snyder, R. and Ralph, D. (2008). Forecasting with Exponential Smooth-

ing: The State Space Approach, Springer Verlag, Berlin.

Kim, S.-J., Koh, K., Boyd, S. and Gorinevsky, D. (2009).

ℓ1 Trend Filtering, SIAM Review

51(2): 339–360.

Ledolter, J. and Kahl, D. (1984). Adaptive Filtering: An Empirical Evaluation, The Journal of the

Operational Research Society 35(4): 337–345.

Leipus, R., Paulauskas, V. and Surgailis, D. (2006). On a Random-coeﬃcient AR(1) Process

with Heavy-tailed Renewal Switching Coeﬃcient and Heavy-tailed Noise, Journal of Applied

Probability 43(2): 421–440.

Lin, J. and Wei, C.-Z. (2006). Lecture Notes-Monograph Series, Time Series and Related Top-

ics: In Memory of Ching-Zong Wei, Institute of Mathematical Statistics, chapter Forecasting

Unstable Processes, pp. 72–92.

Luetkepohl, H. (2008). Introduction to Multiple Time Series Analysis, Springer Verlag, Berlin.

Lyman, R. J. and Edmonson, W. (2001). Linear Prediction of Bandlimited Processes with Flat

Spectral Densities, IEEE Transactions on Signal Processing 49(7): 1564–1569.

Ord, J., Koehler, A. and Snyder, R. (1997). Estimation and prediction for a class of dynamic

nonlinear statistical models, Journal of the American Statistical Association 92(440): 1621–

1629.

Paige, C. and Saunders, M. (1977). Least sequares estimation of discrete linear dynamic systems

using orthogonal transformations, SIAM Journal on Numerical Analysis 14(2): 180–193.

Pesaran, M., Pick, A. and Pranovich, M. (2013). Optimal forecasts in the presence of structural

breaks, Journal of Econometrics 177(2): 134–152.

Pesaran, M. and Timmermann, A. (2004). How costly is it to ignore breaks when forecasting the

direction of a time series?, International Journal of Forecasting 20(3): 411–425.

19

Pesaran, M. and Timmermann, A. (2007). Selection of estimation window in the presence of

breaks, Journal of Econometrics 137(1): 134–161.

R Core Team (2016). R: A Language and Environment for Statistical Computing, R Foundation

for Statistical Computing, Vienna, Austria.

URL: https://www.R-project.org/

Roberts, S. (1982). A General Class of Holt-Winters Type Forecasting Models, Management

Science 28(7): 808–820.

Rossi, B. (2013). Chapter 21 - Advances in Forecasting under Instability, in G. Elliott and A. Tim-

mermann (eds), Handbook of Economic Forecasting, Vol. 2, Part B of Handbook of Economic

Forecasting, Elsevier, pp. 1203–1324.

Sastri, T. (1986). A Recursive Algorithm for Adaptive Estimation and Parameter Change Detection

of Time Series Models, The Journal of the Operational Research Society 37(10): 987–999.

Stine, R. (1987). Estimating Properties of Autoregressive Forecasts, Journal of the American

Statistical Association 82: 1072–1078.

Williams, T. (1987). Holt-Winters Forecasting, The Journal of the Operational Research Society

38(6): 553–560.

Xia, Y. and Zheng, W. (2015). Novel Parameter Estimation of Autoregressive Signals in the pres-

ence of Noise, Automatica 62: 98–105.

20

