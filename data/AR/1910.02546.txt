9
1
0
2

t
c
O
6

]
T
S
.
h
t
a
m

[

1
v
6
4
5
2
0
.
0
1
9
1
:
v
i
X
r
a

A theorem of Kalman and minimal state-space
realization of Vector Autoregressive Models.

Du Nguyen
nguyendu@post.havard.edu

October 8, 2019

Abstract

We introduce a concept of autoregressive (AR) state-space realization that
could be applied to all transfer functions T (L) with T (0) invertible. We show
that a theorem of Kalman implies each Vector Autoregressive model (with
exogenous variables) has a minimal AR-state-space realization of form yt =
Pp
i=1 HF i−1Gxt−i + (cid:15)t where F is a nilpotent Jordan matrix and H, G satisfy
certain rank conditions. The case varx(1) corresponds to reduced-rank regres-
sion. Similar to that case, for a ﬁxed Jordan form F , H could be estimated by
least square as a function of G. The likelihood function is a determinant ratio
generalizing the Rayleigh quotient. It is unchanged if G is replaced by SG for
an invertible matrix S commuting with F . Using this invariant property, the
search space for maximum likelihood estimate could be constrained to equiva-
lent classes of matrices satisfying a number of orthogonal relations, extending
the results in reduced-rank analysis. Our results could be considered a multi-lag
canonical-correlation-analysis. The method considered here provides a solution
in the general case to the polynomial product regression model in (Velu, Reinsel,
and Wichern 1986). We provide estimation examples with simulated data. We
also explore how the estimates vary with diﬀerent Jordan matrix conﬁgurations
and discuss methods to select a conﬁguration. Our approach could provide an
important dimensional reduction technique with potential applications in time
series analysis and linear system identiﬁcation. In the appendix, we link the
reduced conﬁguration space of G with a geometric object called a vector bundle.

1

Introduction

Traditionally, the state-space approach to time series considers a representation:

yt = Kvt

vt = Cvt−1 + D(cid:15)t

1

 
 
 
 
 
 
Using the lag operator L, vt = (I − CL)−1D(cid:15)t and thus

yt = K(I − CL)−1D(cid:15)t
Let ˜T be a rational matrix function such that ˜T (∞) = 0 ( ˜T is called strictly-
proper in this case.) A realization is a representation of ˜T in the form

(1)

˜T (z) = K(zI − C)−1D

It is known (Gilbert 1963; Kalman 1965) a realization exists for all strictly-
proper ˜T . If yt = T (L)(cid:15)t for a rational matrix T (L) with 0 is not a pole of T
(T (0) is ﬁnite) then ˜T (L) = L−1T (L−1) is strictly proper. Hence:

T (L) = L−1K(L−1I − C)−1D = K(I − CL)−1D

and so y can be represented in state-space form. (T (0) = I in many models,
structure models could have T (0) 6= I.) We note, this traditional state-space
realization (which we will call MA-state-space realization) gives a representation
of y in term of (cid:15). C gives valuable information, for example its eigenvalues could
determine stability of the process. As a moving average representation, it does
not link yt with its lagged values directly. We will take a diﬀerent approach in
this paper.

If T (0) is invertible, we note Z(s) = I − T (0)T (s−1)−1 is strictly proper as

a function of s. We can apply the same realization theorem to express

Z(s) = H(sI − F )−1G

for some {H, F , G}. With s = L−1, this implies:

T (0)T (L)−1 = I − Z(L−1) = I − H(L−1I − F )−1G = I − H(I − F L)−1GL

And the model

could be written as

or

yt = T (L)(cid:15)t

T (L)−1yt = (cid:15)t

T (0)T (L)−1yt = (I − H(I − F L)−1L)yt = T (0)(cid:15)t
yt = H(I − F L)−1GLyt + T (0)(cid:15)t
This is what we call the autoregressive (AR) state-space form. y could be
forecast by its lagged values. This is an important feature we would like to
explore in this paper. Consider the Vector Autoregressive (var) model:

yt =

p
X

i=1

Φiyt−i + (cid:15)t

(2)

In this case, T (L)−1 = I − Pp
i=1 Φis−i. It is clear that
Z(s) is strictly proper. Moreover, it has only one pole of degree p at 0. Kalman

i=1 ΦiLi. So Z(s) = Pp

2

described its minimal realization explicitly. We will see F could be made a
nilpotent Jordan matrix, and so could be classiﬁed by the shape of the Jordan
blocks. For a Jordan form F with F p = 0, this shows:

P (L) := Z(L−1) =

p
X

i=1

ΦiLi =

p
X

i=1

(HF i−1G)Li

and the regression is

yt =

p
X

i=1

(HF i−1G)Liyt−i + (cid:15)t

Here, F does not determine stability of yt, but yt is explicitly expressed in its
lagged values. This approach oﬀers a number of crucial advantages. It turns
out to be a generalization of the reduced-rank regression approach. In that case
F = 0, and we have yt = HGyt−1 + (cid:15)t. We show that we can replicate most of
the reduced-rank analysis here. Fixing G, H could be computed by least square.
The likelihood function could be expressed via the Schur determinant formula
as a determinant ratio which could be considered a generalized Rayleigh quo-
tient. This generalizes the classical result that reduced-rank var(1) models are
related to generalized invariant subspace representations, and to the associated
Rayleigh quotients. Therefore, maximizing the likelihood means minimizing
a determinant ratio. The gradient and hessian of the likelihood function are
very easy to compute, and could be used to estimated model parameters using
standard optimizers in the examples we consider. However, similar to the re-
duced rank case, the likelihood function is unchanged if we replace G by SG
if S commutes with F . So it is possible to restrict the search space to a lower
dimension set. In the reduced-rank case, using QR factorization on β0 we can
assume the rows of G to be orthonormal. We have a similar situation in the
minimal state-space case.

As the structure of F could be classiﬁed by listing all Jordan forms with
F p = 0, we have a very explicit and simple classiﬁcation of possible realizations.
The approach oﬀers a systematic parameter reduction technique that we hope
to compare and combine with other parameter estimation techniques.

Reduced-rank regressions could be deﬁned for any two variables x and y, not
only for an autoregressive y. Our results are valid for a more general forecasting
model with time lagged regressors, the varx model. We restrict ourselves to
consider var and varx in this article. The more general case of varma will be
considered in a future article.

We collect the symbols used and compares our minimal AR-state-space
approach with reduced-rank regression for the reader’s convenience in table 1.
The concepts and symbols will be introduced in subsequent sections.

3

Concept/Symbol

Reduced-Rank

AR-state-space

Dimension of x
Dimension of y
min(m, k)
Lag
Structure params

m
k
h
p = 1 or not applicable
reduced rank l = d < m ˆΨ = [d1, · · · , dp], di ≥ 0; dp > 0; P di ≤

m
k
h
p

l = d

struct.

l = d
l = d

state-space

Total rank alloc.
Min.
dim.
Alt.
params
Parameter
tion
Jordan block
λ = 0 Jordan block K(r, l)
F
Factorization
Minimal criteria

reduc-

J (λ, r, l)

(m − d)(k − d)

F = K(1, l) = 0d,d
β = HG
G, H
(d, m), (k, d),
d

of

size
rank

LQ/Gram-Schmidt GG0 = I l

Num. mat. (A)

XX 0
XY 0(Y Y 0)−1Y X 0

−

Denom. mat. (B) XX 0
κ
Neg Log Likelhood

−

κ(G) = G
log(det(GAG0))
log(det(GBG0))
2 Tr(GAG0)−1GAη0 −
2 Tr(GBG0)−1GBη0
2 Tr((GAG0)−1(GAψ0 +
ψAG0)
−
(GAG0)−1ψAη0)
H(A, G, ψ, η)
H(B, G, ψ, η)

−

Gradient

(H(A, G, ψ, η))

Hessian

No. conﬁgs for F

h

h
l = P di
P jdj

Ψ = [(rg, lg), · · · , (r1, l1))]rg>···>r1,0<li=dri

Pp

i=1(m − P

j≥i dj) Pp

i=1(k − P

j≥i dj)

i=gK(ri, li)

J (λ, r, l)
K(r, l)
F Ψ = ⊕1
P (L) = Z(L−1) = H(I − F L)−1G
Gr,0, H r,0 of size (dr, m), (k, dr), rank
dr. G:,0 = (Gr,0)r and H :,0 = (H r,0)r
are of full row and column rank
G:,0G0
max(0, r − r1 − 1)
X lagX 0

lag − X lagY 0(Y Y 0)−1Y X 0

:,0 = I l; Gr,lG0

r1,0 = 0 for l >

lag

−

X lagX 0
lag
deﬁned in eq. (9)
log(det(κ(G)Aκ(G)0)
log(det(κ(G)Bκ(G)0)
2 Tr(κ(G)Aκ(G)0)−1κ(G)Aκ(η)0
2 Tr(κ(G)Bκ(G)0)−1κ(G)Bκ(η)0
2 Tr((κ(G)Aκ(G)0)−1(κ(G)Aκ(ψ)0 +
κ(ψ)Aκ(G)0)
−
(κ(G)Aκ(G)0)−1κ(ψ)Aκ(η)0)
H(A, G, ψ, η) − H(B, G, ψ, η)

−

!
 p + h − 1
p

Table 1: Main symbols and concepts.

To summarize, in this paper, we:

4

• Introduce a framework for dimensional reduction for varx models under
the concept of minimal AR-state-space. We show in this case, minimal
state-space could be classiﬁed explicitly in term of Jordan forms.

• Compute the likelihood function for each conﬁguration of Jordan form. It
could be considered as a multi-lag canonical correlation analysis. While we
can no longer maximize the likelihood via eigenvectors, its gradient and
hessian are simple to compute, so we can apply standard optimization
techniques.

• Reduce the search domain for the likelihood function to a lower dimen-
sional set, via a generalized LQ/Gram-Schmidt procedure. G could be
made to satisfy two sets of orthogonal relations. This allows us to apply
manifold optimization techniques for large scale problems.

In the appendix, we show the reduced search domain could be considered a
vector bundle on a ﬂag manifold. This geometric concept is not essential to use
our model but potentially will be helpful for higher dimension/autoregressive
order.

The work (Velu, Reinsel, and Wichern 1986) (which mentioned the model
in (Brillinger 1969)) is probably a predecessor to this approach. It is related to
the case F = K(p, dp) (Jordan block of one exponent). The author handled the
case where H p,j = 0 for all j > 0. We will discuss this model in more details in
section 9. We will show our approach can also be used for their model in the
general case.

As we only consider AR-state-space models in this article, we often drop
the preﬁx AR when mentioning state-space models. We will use the wild card
character : to replace running indices on (block) rows and columns of matrices.

2 Review of reduced-rank regression and VAR(1)
minimal realization.

Reduced-rank regression was ﬁrst studied in (Anderson 1951). It found applica-
tions in diﬀerent areas of statistics, notably in time series. Johansen (Johansen
1991) used it in his famous test of cointegration. Reduced-rank regression for
time series has been studied by (Velu, Reinsel, and Wichern 1986; Ahn and
Reinsel 1988; Anderson 1999; Anderson 2002). (Box and Tiao 1977) introduced
canonical-correlation-analysis to time series. We review reduced-rank regres-
sion brieﬂy here, in a less general framework but suﬃcient for the subsequent
analysis. We study a model of form:

y = βx + (cid:15)

y is a k-dimensional random variable, x is a m dimension variable and β is a
k × m matrix of rank d ≤ min(k, m). We can set β = HG with H of size k × d

5

of rank d and G of size d × m. Given sample matrices Y of size k × n and X
of size m × n; for a ﬁxed G, the optimal H is obtained by:

H = Y X 0G0(GXX 0G0)−1

and the residual covariance matrix is

C(G) = Y Y 0 − Y X 0G0(GXX 0G0)−1GXY 0

Following (Johansen 1995), we apply the Schur determinant formula to the
block matrix:

Y X 0G0
Y Y 0
GXY 0 GXX 0G0

!

We have:

det(Y Y 0) det(GXX 0G0 − GXY 0(Y Y 0)−1Y X 0G0) =
det(GXX 0G0) det(Y Y 0 − Y X 0G0(GXX 0G0)−1GXY 0)

So to minimize det(C(G)) (as a function of G) we need to minimize the ratio:

R(G) =

det(G[XX 0 − XY 0(Y Y 0)−1Y X 0]G0)
det(GXX 0G0)

or its logarithm, which has a simple gradient:

∇ηD log(R) = 2 Tr((GAG0)−1GAη0 − (GBG0)−1GBη0)

Where A = [XX 0 − XY 0(Y Y 0)−1Y X 0] and B = XX 0. Here ∇η is the
directional derivative in the direction η. At a critical point we have

(GAG0)−1GA = (GBG0)−1GB

Therefore, GA = γGB, where γ = (GAG0)(GBG0)−1 is a matrix of size d × d.
So this is a generalized invariant subspace problem. Rewrite it as:

GB1/2B−1/2AB−1/2 = γGB1/2

This becomes an invariant subspace problem, where the new matrix is B−1/2AB−1/2
and GB1/2 is the new variable. Alternatively, by comparing gradient it is well-
known that this determinant ratio minimizing problem is equivalent to the trace
ratio problem:

Tr((G[XX 0 − XY 0(Y Y 0)−1Y X 0]G0)(GXX 0G0)−1)

and this leads to the problem of maximizing

Tr((G[XY 0(Y Y 0)−1Y X 0]G0)(GXX 0G0)−1)

which brings us to canonical-correlation-analysis.

6

 
In the time series case, we have y = yt and x = yt−1. The corresponding

regression is:

yt = Φyt−1 + (cid:15)t
This is the vector autoregressive model var(1), with p = 1 as we only have one
lag. If Φ is of reduced-rank d: Φ = HG as above, which can be written with
lag operator symbol as

(I k − ΦL)yt = (cid:15)t
Its transfer function, is T (L) = (I − ΦL)−1, so as in the introduction Z(s) =
I − T (s−1)−1 = s−1Φ has minimal state-space form:

Z(s) = H[sI r]−1G

From our discussion so far, it is clear that the minimal AR-state-space realiza-
tion of the lag polynomial of var(1) model is exactly the reduced-rank regression
model.

3 Minimal state-space realization for VAR(2)

Let us consider the case p = 2. In this case, Z(s) = Φ1s−1 + Φ2s−2 for the
regression:

yt = Φ1yt−1 + Φ2yt−2 + (cid:15)t
With s = L−1, we want to realize P (L) = Φ1L + Φ2L2 in the form H(L−1I −
F )−1G = H(I − F L)−1GL. The later expression will need to be a polynomial
in L. We note if F is nilpotent with F 2 = 0 then (I − F L)−1 = I + F L,
therefore:

Φ1 = HG

Φ2 = HF G

Assume that is the case, we can assume further that F is of Jordan form:

0l2,l2






F =

I l2
0l2,l2






0
0
0l1,l1

We can divide H and G to corresponding blocks, H =

h
H 2,0 H 2,1 H 1,0

i

,

G =




. Expanding:






G2,1
G2,0
G1,0

Φ1 = H 1,0G1,0 + H 2,0G2,1 + H 2,1G2,0
Φ2 = H 2,0G2,0

(3)

Therefore, we have a realization if we can decompose Φ1 and Φ2 to this form.
The minimal requirement would put further restrictions: if G2,0 and G1,0 has

7

zero rows, then we can just drop those rows and get a smaller state-space
realization. So a starting condition is G2,0 and G1,0 should not have zero rows.
The actual condition, speciﬁed by Kalman is that the rows of G2,0 and G1,0
are linearly independent, and so are the columns of H 2,0 and H 1,0. This puts
a constraint l1 + l2 ≤ k where k is the dimension of the vector yt. Further, he
proved our guess is correct: F needs to be a nilpotent matrix. The discussion
here could be generalized to var(p) models and more generally to regression
models with time lag structures as we can see in the next sections.

4 A result of Kalman on rational matrix
function

The results in this section is purely algebraic involving matrix polynomial and
rational functions. Its main result was discovered by Kalman in (Kalman 1965).
Together with earlier results of Gilbert in (Gilbert 1963) they give a complete
picture of minimal state-space realization of all proper rational transfer func-
tions. We will recall a few deﬁnitions but will mostly focus on Proposition 3 of
(Kalman 1965), which is most relevant to our situation. As before, a rational
matrix function Z(s) is called strictly proper if lims→∞ Z(s) = 0. By factoring
out the (scalar) least common denominator ¯q(s) we can write:

Z(s) =

1
¯q(s)

¯P (s)

with degree of ¯P is less than degree of ¯q. Gilbert addressed the case where ¯q
has simple roots. In that case we can expand Z by partial fraction:

Z(s) =

g
X

i=1

H iGi
s − λi

with H i, Gi are of sizes k × di, di × k and of rank di. Note we are assuming that
¯q(s) could be factored to monomials, hence H i, Gi and λi could be complex
numbers (but in the end Z is real.) Gilbert showed that Z admits a minimal
state-space realization with minimal state P
i di and constructed it as a direct
sum of blocks of form H i(sI ri − λi)−1Gi. The case g = 1 and λi = 0 is the
var(1) case above, with L = s−1 as usual.

Kalman addressed the case of roots with multiplicity. He showed in general
the minimal state-space realization could be constructed as direct sum of real-
ization for distinct roots, each could have multiplicity greater than 1. For the
case of one root his result could be summarized in the following proposition,
which is a restatement of Proposition 3 of (Kalman 1965) which works for ma-
trices in X = Rn or Cn as this is purely an algebraic result. We note he used
the term irreducible instead of minimal realization, which is the standard term
today. For our application the root will be zero and all coeﬃcients will turn out
to be real.

8

¯P (s) with ¯P (s) is a polynomial matrix of de-
Proposition 1. Let Z(s) = 1
gree less than p. Let F be an n × n matrix with a single eigenvalue λ. We take a
basis of F in X so that F has the Jordan form: F = diag(J (λ, rg, lg), · · · , J (λ, r1, l1))
with rg > r1 > · · · > r1. Where J (λ, r, l) is deﬁned by:

(s−λ)p

J (λ, r) =

1
· · ·

· · ·
· · ·

· · ·
· · ·



λ








0










1
λ

J (λ, r, l) = J (λ, r) ⊗ I l

and J (λ, r) is of size r. Let G, H be n×m, k ×n matrices expressed with respect
to the same basis. Let l = P li. Then {H, F , G} is a minimal state-space
realization of Z(s) if and only if both of the following conditions are satisﬁed:

1. The l×m matrix G:,0, which consists of rows (rg −1)lg +1, · · · , rglg, rglg +
1 rili

2 rili+(r1−1)l1+1, · · · , Pg

(rg−1−1)lg−1+1, · · · , rglg+rg−1lg−1, · · · , Pg
has rank l.

2. The k×l matrix H :,0, which consists of columns 1, · · · , lg, rglg +1, · · · rglg +

lg−1, · · · , Pg

2 rili + 1, · · · , Pg

2 rili + l1 has rank l.

We have mostly preserved Kalman’s notations, the notable diﬀerences are:
1. Replacing his p with k so not to confuse with the degree of the var model.

2. Use g for the number of terms distinct r’s, as q may be confused with

moving average order.

3. Grouping the blocks with the same r together and order the blocks in
descending order of r’s. This is to conform with the order of the McMillan
denominator. We will see the block with exponent r will contribute to r
coeﬃcients Φ1, · · · , Φr, and the reduction in the rank of the Gp,0 block
contribute the most to reduction of overall parameters. So in a sense it is
an order of importance.

We note the dimension of the minimal realization is

nmin = X rili = δM

δM is deﬁned in term of the Smith normal form.
If a minimal state-space
realization is given, Z(s) is recovered by expanding the terms. Conversely,
given Z(s), Kalman gave an algorithm to recover {H, F , G}. The algorithm
is based on representing ¯P (s) in Smith normal form then expand the terms to
Taylor series of terms s − λ and read the coeﬃcients oﬀ from the representation.
Kalman’s proposition is essentially a translation between the Smith-McMillan
form and the state-space form, a point carried out to all base ﬁelds by the work
of (Ito, Schmale, and Wimmer 2002).

9

Our interest is in the case λ = 0. We will use the notation K(r, l) = J (0, r, l)

going forward. Consider again

Z(s) = Φ1s−1 + · · · Φps−p =

1
sp

p
X

i=1

Φp−isi

with Φp 6= 0, let P (L) = Z(L−1):

P (L) = Φ1L + · · · ΦpLp

Applying the proposition for this case:

Z(s) = H(sI − F )−1G = H(I − F s−1)−1s−1G

and observe since λ = 0, F is a nilpotent Jordan matrix: F p = 0. Therefore
we could rewrite the state-space realization in term of L:

P (L) = Z(L−1) = H(I − F L)−1GL = HGL + HF GL + · · · HF p−1GLp (4)

or

From the fact F rg = 0 and Φp 6= 0 this implies rg = p.

Φi = HF i−1G

5 Detailed description of the minimal real-
ization.

Let us now go deeper to the structure of G and H. Since F is composed of g
blocks K(ri, li), G and H could be decomposed to the corresponding g row or
column blocks respectively. The block corresponding to ri is of size rili and it
has ri sub-blocks, each of equal size li. We call r the exponent of the Jordan
block K(r, l) and l the sub-rank.

We index the sub-blocks corresponding to an exponent r of G in descending
order: Gr,r−1, · · · , Gr,0. The corresponding sub-blocks in H are in ascending
order, H r,0, · · · , H r,r−1. We will see this more explicitly in the next section.
The somewhat mysterious indexing has origin from the correspondence between
the Smith normal form and state-space realization. With this double indexing
convention, it is clear G:,0 above is just the collection of all Gri,0, and the
assumption is the rows of G:,0 are linearly independent of rank l = P li ≤
min(k, m). We have a similar observation for the sub-blocks of H.

Let h = min(k, m). Instead of describing the Jordan blocks by pairs (ri, li)
with li > 0, it is sometime more convenient to allow zero Jordan blocks. To
summarize:

• For a var model the possible choices of F could be classiﬁed by Jordan
matrices such that F p−1 6= 0; F p = 0. In other words, it could be classiﬁed
by a list of tuples Ψ = [(rg, lg), · · · , (r1, l1)] with li > 0, rg = p, rg >
l = P li ≤ h. The
rg−1 > · · · > r1, together with the rank constraint:
corresponding Jordan matrix is F = F Ψ = ⊕i=1

i=gK(ri, li).

10

• An alternative classiﬁcation is by nonnegative integer value vector ˆΨ =
[d1, · · · , dp] with dp > 0, 0 ≤ di and l = P dp ≤ h. Ψ is obtained from ˆΨ
as the list of tuples [(i, di)|di > 0] in reversed order of i. Conversely, we
can obtain ˆΨ from Ψ by patching di = 0 for the exponents i not in Ψ. We
call Ψ and ˆΨ structure parameters.

• To be consistent with the convention of the McMillan denominator, we

will write the Jordan blocks in descending order of exponent.

• For each 1 ≤ i ≤ p, if di > 0, a Jordan block is deﬁned by its exponent i
and its sub-rank di. K(i, di) = J (0, i, di) is of size dii. We will skip blocks
with di = 0.

• The minimal state-space dimension, which is the dimension of F in a
It is equal to the

i=1 dii = P1

minimal realization is nmin = Pp
McMillan degree.

j=g rjlj.

• For a given tuple (k, m, p), and h = min(k, m), the highest possible min-
imal state-space dimension is ph and corresponds to Ψ = [(p, h)]. The
lowest possible dimension is p, corresponds to Ψ = [(p, 1)].

• The number of possible conﬁgurations of Ψ with maximal degree p is

. This follows for a balls-and-urns computation. A more

!

h + p − 1
p

straight forward application of balls-and-urns is the number of conﬁgura-
tions with l = P di ≤ h and degree not exceeding p: in addition to the urns
1 · · · p, we add an urn corresponding to unused dimension d0 = h − l. This
!
.

is an h balls, p + 1 urns problem with number of conﬁgurations

h + p
p

Our count for the maximal degree is exactly p comes from
!

!

!

h + p − 1
p

=

h + p
p

−

h + p − 1
p − 1

In the code, we include a function to list all possible Ψ per a pair (h, p).

• From here the number of conﬁgurations growths polynomially (of degree
p) in m. There are ph possible minimal state-space dimensions, and the
distribution of number of Ψ’s per state-space dimension is a bell-shape.
The analysis suggests that for large m and p, iteration through the set
of all possible Ψ is not practical. We will discuss estimation in the next
section.

As an example, for p = 3, P (L) = P ΦiLi is represented as:

h

L

H [3,:] H [2,:] H [1,:]

i



I − K(3, d3)L




I − K(2, d2)L




I − K(1, d1)L



−1 









G[3,:]
G[2,:]
G[1,:]

As before we can use the nilpotency of K(r, l):

(I − K(r, l)L)−1 = I +

r−1
X

i=1

K(r, l)iLi

11

 
 
 
 
 
We note for the block K(r, l)i is the matrix with the ith oﬀset upper block
diagonal is I l, and other entries are zero. Therefore, the contribution of that
block is

h

H r,0, · · · H r,r−1

i

· · ·


I lL I lL2 I lL3 · · ·








· · ·

· · ·

0

I lLr−1

· · ·

· · ·
· · ·











I lLr

...
I lL2
I lL




 (5)






Gr,r−1
...
Gr,0

So its contribution to Φi = HF i−1G is:

r−i
X

a=0

H r,aGr,p−i−a

And so:

Φi = X

j−i
X

j≥i

a=0

H j,aGj,j−i−a

(6)

It is now simple to recover the formula for the cases p = 1 and p = 2 in
previous sections. While the set up looks more involved to remember we have
the following rules:

• H j1,aGj2,b only appears if j1 = j2. So only terms of form H j,aGj,b are

present. There are no terms linking distinct Jordan blocks.

• H j,aGj,b contributes to Φj−a−b

In the next section we will describe the associated regression model, as well

as estimations.

6 VARX and least square estimates.

We will describe a model of the form:

yt = Φ1xt−1 + Φ2xt−2 · · · + Φpxt−p + (cid:15)t

(7)

We allow yt−i to be part of xt−i, as discussed in (Velu, Reinsel, and Wichern
1986). The classical reduced-rank regression would be a special case of eq. (7)
with p = 1.

Let us now turn to the estimation problem. Assuming we have T +p samples
of data, we organize the data in to matrices Y f of size k × (T + p) and X f
of size m × (T + p). Let Y be the submatrix of Y f of size k × T skipping
the ﬁrst p (columns) samples. Deﬁne LiX to be the submatrix of X f of size
m × T skipping the last i samples and the ﬁrst p − i samples. As we will not
be using the ﬁrst p samples of Y f as well as the last sample of X f , they are
allowed to be null. In the situation where we have autoregression it may be

12

advantageous to share storage of X f and Y f . However we will consider X f
and Y f to be in separate matrices here to simplify the notations. We now look
at the problem of estimating Φi such that the minimal state-space realization
of Z(s) = Φ1s−1 + Φ2s−2 · · · + Φps−p corresponds to ˆΨ = {d1, · · · dp}.

As with classical regressions, the maximum likelihood estimate with Gaus-

sian noise of the parameters H, G, Ω will have the form:

−

T
2

log(det(Ω)) −

1
2

T

X(yt − X Φixt−i)0Ω−1(yt − X Φixt−i)

where Φi is given by eq. (6). We arrive at the condition:

Ω =

1
T

(Y − X ΦiLiX)(Y − X ΦiX)0

Since F is known from the speciﬁcation of ˆΨ, similar to the reduced-rank
case we will need to estimate H and G. Before we proceed with the general
case let us go through the case p = 2. From eq. (3) we have:

yt = (H 1,0G1,0 + H 2,0G2,1 + H 2,1G2,0)xt−1 + H 2,0G2,0xt−2 + (cid:15)t

Similar to the var(1) case, to ﬁnd a least square estimate by minimizing the
determinant of the covariance matrix of (cid:15)t, we ﬁx G and ﬁnd the optimized H.
With the regressor:


G2,1LX + G2,0L2X

G2,0LX

G1,0LX






We can write it as κ(G)X LAG, where:

X LAG =

#

"
L2X
LX

κ(G) =






G2,0 G2,1
G2,0
G1,0






And note that we will write the exponents in descending order as is the case of
the McMillan denominator. We see:

H(G) =

h
H 2,0 H 2,1 H 1,0

i

= Y X 0

LAGκ(G)0(κ(G)X LAGX 0

LAGκ(G)0)−1

So we need to minimize the determinant of residual covariance matrix:

det(Y Y 0 − Y X 0

LAGκ(G)0(κ(G)X LAGX 0

LAGκ(G)0)−1κ(G)X LAGY )

Again, using the Schur complement trick we need to minimize:

R(G) =

det(κ(G)[X LAGX 0

LAG − X LAGY 0(Y Y 0)−1Y X 0

LAG]κ(G)0)

det(κ(G)X LAGX 0

LAGκ(G)0)

13

Write its logarithm as log(det(κ(G)Aκ(G)0 − log(det(κ(G)Bκ(G)0)) with A =
[X LAGX 0
LAG as before.
The logarithm has a simple gradient by Jacobi’s formula:

LAG − X LAGY 0(Y Y 0)−1Y X 0

LAG] and B = X LAGX 0

∇ηlog(det(κ(G)Aκ(G)0)) = 2 Tr((κ(G)Aκ(G)0)−1κ(G)Aκ(η)0

where η is a matrix in the shape of G to specify the direction for the directional
derivative ∇η.

∇η log(R(G)) = Tr((κ(G)Aκ(G)0)−1κ(G)Aκ(η)0−Tr((κ(G)Bκ(G)0)−1κ(G)Bκ(η)0

for all η. So far, we can generalize the steps of the reduced-rank regression in the
introduction. From here, the situation divergences. As κ must be represented
as a tensor, we do not have a matrix equation for G. However, since we know
the gradient (and the hessian is also easy to compute) we can use a hessian-
based optimizer. Later on we will see this is an optimization problem with the
underlying function is invariant under a large group of matrix operations. We
may use manifold optimization techniques for faster convergence. To conclude
the section we note the analysis thus far generalizes to higher p:

Theorem 1. Assume the minimal state-space realization of Pp
i=1 ΦiLi is repre-
sented by a nilpotent Jordan matrix consisting of blocks K(p, dp), · · · , K(1, d1).
Let nmin = P dii. For a ﬁx G:

G =































Gp,p−1
...
Gp,0
...
G2,1
G2,0
G1,0

We deﬁne κ(G) to be the block matrix with row blocks indexed by (r, l) with r
arranged in descending order, l arranged in in ascending order (0 ≤ l ≤ r − 1),
and column blocks ordered from p to 1:

κ(G)(r,l),i =

(

Gr,r−l−i if i ≤ r − l
0 otherwise.

(8)

In other words, the row blocks from κ(G)r,r−1 to κ(G)r,0 has the right most
column ﬁlled by Gr,r−1 to Gr,0 from the top down, then the blocks are propagated

14

up diagonally.



κ(G) =

Gp,0 Gp,1

...
0 Gp,0 Gp,1
...
...
...
· · ·
· · ·
0
...
...
...
· · ·
· · ·
0
· · ·
· · ·
0
· · ·
· · ·
0



















... Gp,p−2 Gp,p−1
...
Gp,p−1
...
...
Gp,0
· · ·
...
...
G2,1
0
· · ·
G2,0
G1,0
· · ·

...
...
0
...
G2,0
0
0





















(p, 0)
(p, 1)
...
(p, p − 1)
...
(2, 0)
(2, 1)
(1, 0)

Set

X LAG =











LpX
...
LX

(9)

(10)

Then the optimal H to minimize the determinant of the residual covariance
matrix is given by:

H(G) =

h
H p,0 · · · H p,p−1
LAGκ(G)0(κ(G)X LAGX 0

Y X 0

· · · H 2,0 H 2,1 H 1,0

i

=

LAGκ(G)0)−1

and the residual determinant is:

det(Y Y 0 − Y X 0

LAGκ(G)0(κ(G)X LAGX 0

LAGκ(G)0)−1κ(G)X LAGY )

(11)

(12)

κ(G) is of full row rank if G:,0 is of full row rank. Minimizing eq. (12) is
equivalent to minimizing:

R(G) =

det(κ(G)[X LAGX 0

LAG − X LAGY 0(Y Y 0)−1Y X 0

LAG]κ(G)0)

det(κ(G)X LAGX 0

LAGκ(G)0)

which has the log-gradient:

2 Tr((κ(G)Aκ(G)0)−1κ(G)Aκ(η)0)
−2 Tr((κ(G)Bκ(G)0)−1κ(G)κ(η)0)

(13)

(14)

with A = X LAGX 0
Deﬁne H(A, G, ψ, η) to be

LAG − X LAGY 0(Y Y 0)−1Y X 0

LAG and B = X LAGX 0

LAG.

2 Tr((κ(G)Aκ(G)0)−1(κ(G)Aκ(ψ)0+κ(ψ)Aκ(G)0)−(κ(G)Aκ(G)0)−1κ(ψ)Aκ(η)0)

(15)

Then its Hessian is H(A, G, ψ, η) − H(B, G, ψ, η).

Proof. The proof is a generalization of the case p = 2. We note as before,
row blocks indices of κ are ordered in order of H, so the row block indices are

15

(r, 0), · · · (r, r − 1); in opposite with convention for G. The columns are indexed
from p to 1 as they correspond to X LAG. First, we need to prove the regressor
to H is κ(G)X LAG. From eq. (6), the block of regressor corresponding to H r,l
is P
i Gr,r−l−iLiX, but this is the (r, l) row block of κ(G)X LAG by eq. (8).
Next, we need to show κ is of full row rank. If vκ(G) = 0 then the columns of v
corresponding to blocks (r, 0) are zeros, as rows of G:,0 are linearly independent.
From the block triangular shape of κ we can show vr,l = 0 inductively in l.
Hence, κ(G)U κ(G)0 does not have zero as an eigenvalue if U is positive deﬁnite.
Therefore, κ(G)U κ(G)0 is also positive deﬁnite, so the determinant ratio and
its logarithm in the theorem are well-deﬁned. The remaining calculations are
routine.

In a sense, our determinant ratio likelihood could be considered as a multiple
lag version of canonical-correlation-analysis, as the gradient equation reduces
to the same calculation in the p = 1 case.

7 Equivalence of state-space realizations.

In the simple reduced-rank case, eq. (13) is unchanged when G is replaced by
SG for any invertible matrix S.
In our general framework, a similar result
holds for an invertible matrix S such that SF = F S. We will describe such
matrices, and show we can normalize G to a form satisfying certain orthogonal
relations. This section is more on linear algebra and geometry, readers can skip
the section if they are not interested in the details of dimensional reduction for
estimation algorithms. The main result to take away is the parameter count of
S, which implies the parameter reduction count for the model. On the other
hand, it is not diﬃcult to work out all the details of the section in the case
m = 2 or m = 3 manually, and that would probably give readers more intuition
on parameter reduction.

As pointed out by Kalman in the same paper, the realizations {H, F , G}
and {HS−1, SF S−1, SG} are equivalent. If S commutes with F , then it is
equivalent to {HS−1, F , SG}. With our least square estimates for a given G,
this implies the models deﬁned by G and SG are equivalent. Let S = Centr(F )
the set (which is a group) of all invertible matrices S of size nmin×nmin such that
SF = F S. We show that we can transform G to normalized forms by applying
an element of S, similar to the Gram-Schmidt process. In particular, we can
make the rows of G:,0 orthonormal, similar to the classical Rayleigh quotient
case. Since there are a few concepts to introduce, it is helpful to examine the




case p = 2 explicitly. In this case, G =

G21
G20
G10





 and F will have the form:






0l1






0l2 I l2
0l2

F =

16

so for S to commute with F , it needs to have the form:

S =






S21,21 S21,20 S21,10
S20,20
S10,20 S10,10






S21,21 = S20,20

"

#

blocks. Since the combined matrix

Each diagonal block is invertible, but there is no restriction on the oﬀ-diagonal
G2,0
G1,0
orthonormal using an LQ factorization (thin QR on its transpose). The end
results are matrices S10,10, S10,20 and S20,20, so that we have (SG)1,0(SG)0
1,0 =
I d1, (SG)2,0(SG)0
2,0 = 0. Therefore we assume
after this step we have Gr,0G0
r1,0 = δr,r1I r,r1. We claim that we can choose the
S21,20, S21,10 blocks of S to make (SG)2,1(SG)0
2,0 = 0.
We have

2,0 = I d2 and (SG)1,0(SG)0

is of full row rank, we can make it

10 = 0, (SG)2,1(SG)0

(SG)2,1 = S21,10G1,0 + S21,21G2,1 + S21,20G2,0

Note that S21,21 = S20,20 is already deﬁned in the ﬁrst step. To make (SG)2,1
orthogonal to G1,0 and G2,0 we only need to set:

S21,10 = −S21,21G2,1G0
S21,20 = −S21,21G2,1G0

1,0

2,0

This is our generalized LQ/Gram-Schmidt for p = 2. While we cannot make G
fully orthogonal like the case p = 1, this helps reduce the search space. Recall
h = min(k, m) and l = d1 + d2 ≤ h. Assume we have chosen G such that we
have G:,0G0
2,1 = 0. Completing G:,0 to an orthonormal basis
by adding m − l row vectors organized in a matrix G⊥, then we can express
G2,1 in that basis:

:,0 = I l and G:,0G0

G2,1 = CGo
⊥
where C is a matrix of size (d2, h − l). So far we have showed that for p = 2 the
choice of minimal state representation is the same as the choice of d1, d2 such
that d1 + d2 ≤ m, and G could be normalized to an orthogonal form. This form








2,0

Go
Go
1,0
Go
⊥


 is an orthonormal

could be represented by a pair (C, O) where O =

basis of Rm and C ∈ Mat(d2, h − l).

We note if Q20,20, Q10,10 are orthogonal square matrices having h − l, d2, d1
rows respectively, then the block diagonal matrix Q = diag(Q20,20, Q20,20, Q10,10)









commutes with F . Hence (C,

Q20,20G2,0
Q10,10G1,0
Go
⊥
G and QG, respectively. The generalized Rayleigh functional


) and (Q20,20C,

G2,0
G1,0
Go
⊥








) represent

R(G, A, B) =

det(κ(G)Aκ(G)0)
det(κ(G)Bκ(G)0)

17

is invariant under multiplication of G by Q. Also if we replace Go
and C by CQ0

⊥ we get the same matrix Go.

⊥ by Q⊥Go
⊥

To illustrate, let us consider the case k = m = 2 and p = 2. Consider the case
⊥ is empty. An orthogonal
"

d2 = d1 = 1. In this case l = 2 and nmin = 3 and Go
cos t
− sin t

matrix could be parameterized under the form

sin t
cos t

. We can

v
w

=

"

#

#

take G =






0
v
w




 and κ(G) =


0
v

0
v

0 w




. The numerator of the generalized Rayleigh

quotient will be:






det(

vL2XA(L2X)0v0 vL2XA(LX)0v0 vL2XA(L1X)0w0
vLXA(L2X)0v0
vLXA(LX)0w0
vLXA(LX)0v0
wLXA(L2X)0v0 wLXA(LX)0v0 wLXA(LX)0w0




)

and the denominator is of the same form with A is replaced by B. In eﬀect we
have an optimization problem on the circle, where the function to optimize is a
rational function of high degree in sin and cos.

The following proposition is rather technical but necessary for the exact pa-
rameter count of S for the general case. The main point to remember is we
can slide diagonally entries in a combined block to a wall. Within a combined
block, the sub-blocks are equal if they are on the same (not necessarily princi-
pal) diagonal, and we only need to deﬁne S on certain entries of vertical walls
corresponding to (ρ, 0) as in the case p = 2.

Proposition 2. Let S = Centr(F ) be the set of all invertible nmin × nmin
matrices commuting with F . We can index the blocks of S ∈ S by Sρ1,j1;ρ2,j2
for 1 ≤ ρi ≤ p and 0 ≤ ji ≤ ρi − 1. Sρ1,j1;ρ2,j2 maps the block Gρ2,j2 to block
(SG)ρ1,j1. Sρ1,j1;ρ2,j2 is of size dρ1 ×dρ2. We have the following characterization
of S:

Sρ1,j1;ρ2,j2 = Sρ1,j1+1;ρ2,j2+1 if ρ1 − 1 > j1 ≥ 0, ρ2 − 1 > j2 ≥ 0

(16)

As a consequence:

Sρ,0;ρ,0 is invertible.
Sρ1,j1;ρ2,ρ2−1 = 0 if ρ1 − 1 > j1 ≥ 0
Sρ1,0;ρ2,j2 = 0 if 0 < j2 < ρ2

Sρ,ρ−1;ρ2,j2 = 0 if j2 > ρ − 1
Sρ1,j1;ρ,0 = 0 if ρ1 − ρ > j1
Sρ1,0;ρ2,0 = 0 if ρ1 > ρ2

The following blocks uniquely deﬁne S:

Sρ1,j;ρ,0 with j ≥ ρ − ρ1

18

(17)

(18)

(19)

Given a collection of blocks as in eq. (19), such that Sρ,0;ρ,0 are invertible, we
can construct a unique S ∈ S using eq. (16).
In particular, the number of
parameters of S is

X

ρ1,ρ

min(ρ1, ρ)dρ1dρ =

p
X

i=1

(X
j≥i

dj)2

(20)

To summarize, to deﬁne S we only need to deﬁne the (:, :, ρ, 0) vertical walls,
and on those walls, for each ρ1, j could take values from max(0, ρ−ρ1) to ρ1 −1.
The remaining cells of S are either zero, or could be ﬁlled by eq. (16). Finally:

κ(SG) = Sκ(G)

(21)

Proof. Note that F ρ1,j1;ρ2,j2 is zero, unless ρ1 = ρ2 and j1 = j2+1. So SF = F S
implies:

(F S)ρ1,j1;ρ2,j2 =

Sρ1,j1−1;ρ2,j2 if j1 > 0
0 if j1 = 0

(SF )ρ1,j1;ρ2,j2 =

Sρ1,j1;ρ2,j2+1 if j2 < ρ2 − 1
0 if j2 = ρ2 − 1

From here eq. (16) and eq. (17) follow.

For ﬁxed (ρ1, ρ2), consider the rectangular combined block:

Sρ1,:;ρ2,: := (Sρ1,j1;ρ2,j2)ρ1>j1≥0;ρ2>j2≥0

It has four walls corresponding to rows Sρ1,0;ρ2,:, Sρ1,ρ1−1;ρ2,: and columns
Sρ1,:;ρ2,0, Sρ1,:;ρ2,ρ2−1. By eq. (16), Sρ1,j1;ρ2,j2 is deﬁned if the surrounding walls
are deﬁned. From eq. (17), the vertical wall (:, :, ρ2, ρ2 − 1) is zero, except for
Sρ1,ρ1−1,ρ2,ρ2−1, and the horizontal wall (ρ1, 0, :, :) is zero, except for Sρ1,0,ρ2,0.
So S is deﬁned by the diagonal blocks and the (ρ, ρ − 1; :, :) horizontal walls as
well as the (:, :, ρ, 0) vertical walls. We have eq. (18) by using eq. (16) to move
these matrix entries to another wall using eq. (16), then apply eq. (17).

The ﬁrst equation of eq. (18) shows the only non-zero entries on the (ρ, ρ −
1) horizontal wall are those with j2 ≤ ρ − 1, but then they are equal to
Sρ,ρ−1−j2;ρ2,0. So the entries of the vertical walls (ρ, 0) alone are suﬃcient to
deﬁne S. Finally, using the second equality of eq. (18), we have the restriction
on j in eq. (19).

We count the number of j’s for a given ordered pair of ρ1, ρ to be min(ρ, ρ1),
while the number of parameters for each such block is dρdρ1. From here the
parameter count for S follows. The relationship between Sκ(G) and κ(SG)
could be veriﬁed by direct substitution. For i ≤ r − l:

κ(SG)(r,l),i = (SG)r,r−l−i = X Sr,r−l−i;r3,l3Gr3,l3
Notice that row blocks (r, l) of κ are indexed in ascending order in l while S is
ordered in descending order in l, so we need:

X

Sr,r−l−1;r2,r2−l2−1κ(G)(r2,l2),i = X

Sr,r−l−1;r2,r2−l2−1Gr2,r2−l2−i

r2,l2;i≤r2−l2

r2,l2;i≤r2−l2

19

(22)

(23)

With the change of variable r2 = r3, l3 = r2 −l2 −i, the right-hand side becomes:

X

r3,l3

Sr,r−l−1;r3,l3+i−1Gr3,l3

and then we apply Sr,r−l−1;r3,l3+i−1 = Sr,r−l−i;r3,l3. We need to pay some
attention to show the various constraints on indices carry through, but we will
leave that to the reader.

The generalized Gram-Schmidt (or LQ) algorithm is described next. It is

purely a linear algebra result, we are not sure if it is already known.

Proposition 3. For any block matrix G ∈ Mat(nmin, m) such that G:,0 is of
full row rank, we can construct an invertible matrix S ∈ S = Centr(F ) such
that Go = SG satisﬁes

Go

:,0Go0

:,0 = I l

Go

ρ,lGo0

ρ1,0 = 0 for l > max(0, ρ − ρ1 − 1)

(Sρ1,0;ρ2,0)ρ1,ρ2 could be chosen to be any block lower triangular matrix (Sρ1,0;ρ2,0 =
0 if ρ1 > ρ2) so that eq. (22) is satisﬁed, that is:

(Sρ1,0;ρ2,0)ρ1,ρ2(Gρ2,0)ρ2(Gρ2,0)0

ρ2(Sρ1,0;ρ2,0)0

ρ1,ρ2 = I l

Once (Sρ1,0;ρ2;0)ρ1,ρ2 is chosen, S is uniquely determined by eq. (23).

Proof. Applying the usual QR/Gram-Schmidt to G0
:,0, then transpose, we get
the LQ factorization of G:,0 = LW 0 with W 0 satisﬁes W 0W 0
0 = I l and L is
lower-triangular, so we can write W 0 = L−1G:,0. We take (Sρ1,0;ρ2,0)ρ1,ρ2 =
L−1, which is lower-triangular, and (Go

ρ,0)ρ = W 0.

From here, for each ρ we get the diagonal blocks Sρ,ρ−1;ρ;ρ−1 = · · · = Sρ,0;ρ,0
ρ1,0 = 0 for

ρ,0 = Sρ,0Gρ,0 satisﬁes: Go

ρ,0 = Ilρ and Go

ρ,0Go0

ρ,0Go0

such that Go
ρ1 6= ρ. We note the block matrix:

(Gρ,0Go0

ρ1,0)p≥ρ≥1;p≥ρ1≥1 = L

is invertible. The remaining constraints on S, for ﬁxed ρ, j, ρ1, with j > 0 and
j ≥ ρ − ρ1 are:

X

Sρ,j;ρ2,j2Gρ2,j2Go0

ρ1,0 = 0

(24)

ρ2,j2

We need to show that we can solve for Sρ,j;ρ2,j2 uniquely from the above
equations. We will back solve in j. The case j = 0 is already done, as
by eq. (17) Sρ,0;ρ2,j2 = 0 if j2 > 0. With ﬁx j, we try to solve eq. (24)
for each ρ’s blocks.
If j2 > 0, by eq. (16), Sρ,j;ρ2,j2 = Sρ,j−1;ρ2,j2−1 has
been solved from the previous step. So we only need to solve for the case
j2 = 0, that is for Sρ,j;ρ2,0. The constraint of S forces Sρ,j;ρ2,0 = 0 unless
p ≥ ρ2 ≥ ρ − j. Therefore we have only p − ρ + j + 1 block variables, corre-
sponding to p−ρ+j +1 equations in eq. (24). The coeﬃcient matrix in this case

20

is (Gρ2,0Go0
ρ1,0)p≥ρ2≥max(1,ρ−j);p≥ρ1≥max(1,ρ−j), which is invertible, and the invert
is (Sρ,0;ρ1,0)ρ,ρ1 skipping the last max(1, ρ − j) − 1 rows and column blocks. So
the solution exists and is unique. From eq. (19), S is uniquely deﬁned once we
have Sρ,j;ρ2,0.

We implement this algorithm in function LQ_multi_lag in the package.
With Ψ and G as inputs, it returns the factors S and W such that G = S−1W ,
S commutes with F Ψ and W satisﬁes the orthogonal relations of eq. (22),
eq. (23). As an example for Ψ = [(3, 1), (1, 1)], we have F = K(3, 1) ⊕ (K(1, 1).
With

G =

the function found Go = SG with:








−2
5
3 −2
1
8
1 −2





























; S =

Go =

0.0
0.
−0.3969112
0.049614
−0.1240347 −0.992278
0.124035
−0.9922779

0.022326 −0.195975
−0.124035 −0.024807
0.000000
0.000000 −0.124035 −0.024807
0.000000
0.000000 −0.124035
0.000000
0.000000 −0.186052 −0.806226
0.000000
The points to note are Go
3,1 = cGo
3,2 = 0 and Go
1,0 are of norm 1, Go
1,0
(c = 0.4 in this case). These are consequences of the orthogonal relations. So,
while G has total dimension 8, Go just needs two parameters, one for the pair
of orthogonal vectors, and one for the proportional constant between Go
3,1 and
1,0. The next proposition clariﬁes further the parameterization of Go:
Go

3,0 and Go








Proposition 4. Let G be the set of matrices G’s of size nmin × m such that
G:,0 is of full row rank. Let GO be the subset of G consisting of all matrices
Go satisfying the constraints in proposition 3. An element Q in S = Centr(F )
maps GO to GO if and only if Q is block diagonal with the diagonal blocks
invertible and satisﬁes:

Qr,0;r,0 = Qr,1;r,1 = · · · = Qr,r−1;r,r−1

Qr,0;r,0Q0

r,0;r,0 = I dr

(25)

Let us call Q the set of all such Q’s. We have a pairing: Q × GO 7→ GO

and the likelihood function is unchanged if Go is replaced by QGo.

Let O(m) be the set of all square orthogonal matrices of size m. Set d0 =

#

"
Go
:,0
G⊥

m−l. For each matrix Go

⊥ ∈ Mat(d0, m) such that

∈ O(m), (Go

⊥ together

with G:,0 form an orthonormal basis) we can ﬁnd for p ≥ r ≥ 1, r > l > 0
matrices Cr,l ∈ Mat((dr, Pr−l−1
dj) such that

j=0









(26)



Go

r−l−1

...
Go
1
Go
⊥

Go

r,l = Cr,l







21

Conversely, given ((Cr,l)p≤r≤1,l>0, O), where C(r, l) ∈ Mat(dr, Pr−l−1


j=0

O ∈ O(m), we can reconstruct an element Go by ﬁrst decompose O to

dj) and

Op,p−1
...
O1,0
O⊥



;










then set Go

r,0 = Or,0; Go

r,l = Cr,l









Or−l−1
...
O1
O⊥









for l > 0. Go constructed that way

is an element of GO ⊂ G.

Let us clarify that when l = r − 1, eq. (26) only has one block Go
⊥.

In
Proof. eq. (25) follows from eq. (16) and the orthogonal relation of GO.
particular this means applying Q, the block Gr,0 is transformed to Qr,0;r,0Gr,0
and thus orthogonal to Gρ,0 if ρ 6= r. The oﬀ diagonal blocks of Q must satisﬁes
an equation of the form eq. (24), and the orthogonal relation just mentioned
means they are zeros. The fact that the likelihood function is unchanged by Q
follows from the fact that it is unchanged under S.

Given Go and Go

⊥, we can take Cr,l as the coeﬃcients of Go

r,l in the basis

Go

r−l−1











...
Go
1
Go
⊥

by the orthogonal relations of proposition 3. On the other hand any







linear combinations of that basis satisﬁes the orthogonal relation required by
GO.

For j > 0, G:,j could be arbitrarily large, G and GO are not bounded.

However, we have the following proposition:

Proposition 5. R is bounded by maximum and minimum of uAu0
uBu0 with u ∈
Mat(nmin, pm) is of full row rank with the later expression has maximum and
minimum calculated by the generalized invariant subspace algorithm.

Proof. This follows from the fact that the image of κ is a subset of Mat(nmin, pm).

The example for p = 2 above suggests that for the general case the likelihood
function could be deﬁned on a geometric object of higher dimension. We will
return to this topic in section 9 as well as the appendix.

8 Parameter reduction

It is well-known that reduced-rank regression reduces the number of parameters
by (m − l)(k − l). We could see this by observing that H and G has kl and

22

ml parameters, and we can replace {H, G} by {HS−1, SG}, where S has l2
parameters, so totally we have (m + k)l − l2 free parameters, a reduction of
mk − (m + k)l + l2 free parameters versus full regression. For the state-space
model we have:

Proposition 6. The total number of parameter reduction for state-space model
of structure ˆΨ = [d1, · · · , dp] is

p
X

i=1

(k − X
j≥i

di)(m − X

di)

j≥i

(27)

From this, we see there is no parameter reduction if dp = h = min(p, k).
The largest possible parameter reduction is p(m − 1)(k − 1), which corresponds
to dp = 1, d1 = · · · = dp−1 = 0. A change in dp has the most eﬀect on the
change in the number of free parameters.

Proof. We count the number of parameters of H and G and subtract by the
number of parameter of S to count the number of free parameter. The reduction
is:

pmk − (m + k)(X jdj) +

p
X

i=1

dj)2

(X
j≥i

which we see is the same expression as in the proposition.

We plot the number of reductions versus minimal state-space dimension,
and allocation rank di in ﬁg. 1 for m = 10, p = 5. The averages are taken over
all possible Ψ. We also ﬁx four structural parameters and plot the ﬁfth. The
graph illustrates the point that we have the most parameter saving for higher
exponent.

9 Examples.

In (Nguyen 2019) we provided the python package implementing this model.
The open source notebook minimal_varx in that package allows one to test the
model on colab environment without installing it on a local machine. We pro-
vided a number of examples in the notebook, which we would like to summarize
the results here.

The main class in the package is varx_minimal_estimator(Ψ, m). It can
evaluate the likelihood function as well as its gradient given a matrix G. Given
data matrices X and Y we provide four data ﬁtting method: simple_ﬁt, gra-
dient_ﬁt, hessian_ﬁt and manifold_ﬁt (We implemented manifold_ﬁt for the
case l = h only, and use the orthogonal constraints but not the full reduction by
symmetry of Q). The ﬁrst three methods vectorize G then just use a standard
optimizer. All methods estimate G to maximize the likelihood. The examples
show that if Ψ is known, the algorithm converges relatively fast. After ﬁtting,

23

Figure 1: Parameter reductions versus space-state dimension and structure parame-
ters.

H, F , G, Φ could be read oﬀ the estimator. To forecast, we can use the predict
method of the same class.

The package contains many utility functions, including those to produce
the Smith-McMillan form for a polynomial matrix, as well as determinants of
polynomial matrices and test for stability. We use a utility function to create
a random stable polynomial of state-space form Ψ. We use this function to
generate the tests samples. We provided a number of examples with diﬀerent
m and p. The examples aim to clarify the concepts here. They also give a ﬂavor
of the behavior of this estimation method when the structure of F changes. In
our examples, we mostly work with 1000 samples.

Let us ﬁrst consider the case m = 2, p = 2. There are

p + m − 1
p

!

= 3

possible conﬁgurations of Ψ. The case Ψ = [(2, 2)], is the full rank case. There
are two reduced-rank cases: Ψ = [(2, 1), (1, 1)] and Ψ = [(2, 1)]:

9.1 The case m = 2, Ψ = [(2, 1), (1, 1)]: a circle

In this case, both G2,0 and G1,0 has full rank 1. In the ﬁrst test, we randomly
generate a number of stable matrices with structure parameter Ψ then try
to recover it using simple_ﬁt. We got reasonable convergence for our test.
Applying proposition 3, G2,0 and G1,0 could be made to be orthogonal, so
G2,0 = [cos(t), sin(t)] and G1,0 = [− sin(t), cos(t)]. The generalized Rayleigh
quotient is thus a ratio of two polynomial functions in cos(t) and sin(t), invariant
when cos(t) is replaced by − cos(t), sin(t) by − sin(t) and hence it is suﬃcient

24

 
to examine for t ∈ [0, π]. The enclosed graph plots the negative log likelihood
for diﬀerent values of t, and shows the minimum negative log likelihood is close
to the likelihood of the data generation G, which is −1.27233. Below is the

Figure 2: m = 2, Ψ = [(2, 1), (1, 1)], parameters versus minus log likelihood.

original versus the ﬁtted Φ

"

"

Φorg

1 =

Φﬁtted
1

=

−0.21712203 −0.5690077
0.43775564 −0.98005326

#

"

Φorg

2 =

0.04440429
0.12523273
−0.25249089 0.17464151

#

−0.19686572 −0.53498222
0.42740042 −0.95142986

#

Φﬁtted
2

=

"

0.12781235
0.05058104
−0.27748382 0.20364765

#

We also generate a random G and show how LQ factorization reduces it to
orthogonal one.

9.2 The case m = 2, Ψ = [(2, 1)]: a circle and its tan-
gents.

In this case G has the form

G2,1
G2,0

!

. As before, in the ﬁrst test we do not put

the orthogonal restriction on G. We generate a number of stable polynomials
with minimal state-space conﬁguration Ψ then recover them using simple_ﬁt.
We get convergence as expected. The notebook also shows an example of LQ
factorization in this case.

This is a good example to illustrate the geometric concepts of the problem.
Note that in this case w = G2,1 and v = G2,0 are both two-dimensional vectors.
By proposition 3, v could be assumed to have norm 1, and v.w = 0. So we
could think of G2,0 as being constrained to the unit circle, while for each v,
G2,1 is being constrained to the line tangent to the unit circle at v. Therefore,
the whole conﬁgurations of G could be restricted to that of pairs (v, w) of a
point on the unit circle and a vector on its tangent line at v, not unlike the con-
ﬁguration space of position and velocity of a circular motion in classical physics
problems. As before, if we parameterize v = (cos t, sin t) then we can write

25

 
w = (−c sin t, c cos t) = cv⊥ with v⊥ = (− sin t, cos t). So O =

"

− sin t
cos t

#

cos t
sin t

as in proposition 4. We plot the likelihood function on a two-dimensional surface
of (t, c), as well as plotting it for a number of ﬁxed t as below. The determinant
ratio In this example, the likelihood corresponding to G used in data generation
is −1.101996, vs estimated value −1.096700

Figure 3: Parameters versus minus log likelihood. Circle with tangent.

"

Φorg

1 =

0.74186938 −0.05524596
0.73052047

[−0.24536459

#

"

Φorg

2 =

0.12980835 −0.05497244
0.14280693
−0.04293259

#

"

Φﬁtted
1

0.74142713 −0.03649409
0.75546506
−0.24909784

#

Φﬁtted
2

=

"

0.10750374 −0.07386304
0.13257722
−0.03611811

#

9.3 VAR(p) with m = 2

This is the generalization of the last two examples. As AR(p) is well understood,
m = k = 2 is also the natural next step. Ψ = [(p, 2)] is the case of full rank
regression, which is already well studied, so we will assume dp = 1. As before,
there are two sub-cases, Ψ = [(p, 1), (ρ, 1)] and Ψ = [(p, 1)]. We show the
reduction of the search space for Go by LQ factorization discussed here in the
notebook on our code page.

26

For the ﬁrst case, the conﬁguration space of Go is a circle with p − ρ − 1 tan-

p,0 and Go

ρ,0 form an orthonormal basis. Go
gent vectors. We have Go
are proportional to Go
ρ,0. In this case the full search space of G is of dimension
2nmin = 2(p + ρ) while the reduced space for Go has dimension p − ρ. For the
second case, Go
p,p−1 are orthogonal
to it. So this case is a circle with p−1 tangent vectors. The minimal state-space
dimension is p and the search space for G is of dimension 2p, while the search
space for Go is of dimension p.

p,0 could be made of norm 1, and Go

p,1, · · · , Go

p,1, · · · , Go

p−ρ−1

This example illustrates the point that even when we can search on the full
space of G for smaller m and p, the reduced space is of just half the dimension,
but it is more complex to describe. We do not implement an optimization
routine here but since we can parameterize the search space for Go explicitly,
it could be done via the chain rule and a standard optimizer.

9.4 Ψ = [(p, d)] and the Velu-Reinsel-Wichern model.

In (Velu, Reinsel, and Wichern 1986), the authors introduced a model of form

yt = A(L)B(L)Lxt + (cid:15)t

xt in their paper is xt−1 in our notation. Here A(L) is a k × d polynomial
matrix function of degree p1 and B(L) is a d × m polynomial matrix function
of degree p2 (we switch p1 and p2 as used in their paper.) Set p = p1 + p2 + 1.
Consider the state-space model with F = K(p, d). Set Gp,i = Bp1−i if i ≤ p2,
and zero otherwise, H p,i = Ap2−i if i ≤ p1, and zero otherwise. From eq. (6):
H(I − F L)−1G = A(L)B(L)L

We note a number of blocks are set to zero in this model. The paper considers
the case when A(L) is constant, corresponding to the case where only H p,0 is
non-zero. We can modify our framework to obtain the likelihood function for
non constant A(L). Note that the regressor for Ai is P BjLj+i+1X we get

[A0, · · · , Ap1] = Y X 0

B(X BX 0

B)−1

where X B = υ(B)X LAG and υ(B) is a block matrix of (p1+1) row ×(p1+p2+1)
column blocks.

υ(B) =










Bp2 Bp2−1
0
Bp2
...
...
0
0

· · ·
· · ·

· · · B0
0
0
· · · B1 B0
0
...
...
...
· · ·
... B1 B0
0

· · ·

· · ·










(28)

We could think of υ as a truncated κ. For the case p1 = 0 considered in their
paper, υ(B) has only one row block and p2 + 1 column blocks. With υ(B) in
place of κ(G), the result of theorem 1 still applies, if υ(B) is of full row rank.
is of full
We will assume this is the case, this means
row rank.

Bp2 Bp2−1

· · · B0

i

h

27

9.5 Likelihood estimation over diﬀerent Ψ’s.
"

For m and p suﬃciently large, the total number of conﬁgurations

#

h + p − 1
p

increases fairly quickly. The number of possible minimal state-space dimensions
only increase linearly between p and hp. A suggested strategy is not to iterate
over all possible Ψ, but rather start with an nmin then search for F (nilpotent
but not necessarily Jordan) using continuous optimization method. However,
it should be instructive to get a sense how Ψ, nmin and likelihood function
interact. We plot the number of conﬁgurations of Ψ per state-space dimension
for h = 10, p = 5. The total number of distinct Ψ’s is 2002. The minimum
states space dimension is between 5 and 50 and the number of Ψ per minimum
state-space dimension can be plotted to be of a bell-shape curve with the middle
dimensions having the most number of Ψ, as in ﬁg. 4

Figure 4: Number of Ψ v.s. McMillan degree

We also generate a stable matrix with m = 5, p = 2, Ψgen = [(2, 2), (1, 2)].
If we do not know Ψgen, we may need to search over the 15 possible Ψ con-
ﬁgurations, and we can summarize the optimization results in table 2. From
the table, once we get the correct Ψ, iteration over more complex Ψ does not
improve the likelihood. This suggests that we may not need to search over all
Ψ, but aim to ﬁnd a Ψ with a suﬃciently small state-space dimension with
likelihood function suﬃciently close to the full regression likelihood.

28

2
1.0
1.0
1.0
1.0
1.0
2.0
2.0
2.0
2.0
3.0
3.0
3.0
4.0
4.0
5.0

1
0.0
1.0
2.0
3.0
4.0
0.0
1.0
2.0
3.0
0.0
1.0
2.0
0.0
1.0
0.0

org_llk ﬁtted_llk success
1.0
-5.575984
1.0
-6.715179
1.0
-7.614981
1.0
-8.290710
1.0
-8.342102
1.0
-7.745144
1.0
-8.569492
1.0
-8.754073
1.0
-8.754607
1.0
-8.754916
1.0
-8.756864
1.0
-8.759571
1.0
-8.759829
1.0
-8.760026
1.0
-8.760028

-8.746828
-8.746828
-8.746828
-8.746828
-8.746828
-8.746828
-8.746828
-8.746828
-8.746828
-8.746828
-8.746828
-8.746828
-8.746828
-8.746828
-8.746828

0
1
2
3
4
5
6
7
8
9
10
11
12
13
14

Table 2: Likelihood function for diﬀerent Ψ.

9.6 Other examples

We ran an example for m = 7, k = 5 with Ψ = [(2, 2), (1, 2)], again the ﬁtted
versus original likelihood are close (19.87 versus −19.91). In a ﬁnal example, we
ran 10 tests with k = 8, p = 3, Ψ = [(3, 1), (2, 1), (1, 2)]. The ﬁtted likelihood
is smaller than the original likelihood as seen in table 3. This seems to be an
issue with autoregressive noise in the noise series used to generate the sample.
We show also the full (i.e. no reduced rank assumption) regression likelihood,
it ﬁts with our minimal state-space likelihood well.

org_llk
-6.778974
-7.664920
-7.909143
-14.679623
-9.291283
-12.626511
-11.045051
-6.679380
-10.012286
-64.521764

ﬁtted_llk
-7.333462
-8.280117
-9.150681
-15.425846
-10.052678
-13.004966
-12.250215
-7.898204
-10.811005
-65.580908

full_llk success
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
0.0

-7.397742
-8.329034
-9.203443
-15.491684
-10.098918
-13.049997
-12.296307
-7.974920
-11.115286
-65.678105

0
1
2
3
4
5
6
7
8
9

Table 3: Case m = 8, p = 3

29

10 Discussion

10.1 An alternative space-space model

We note that L−1T (L−1)−1 is also strictly proper, so we have a state-space
realization:

L−1T (L−1)−1 = H a(LI − F a)Ga

From here

T (L)−1 = H a(I − F aL)−1Ga

For Vector Autoregressive model, we have a representation:

T (L)−1 = I − X ΦiLi = H a(I − F aL)−1Ga

We note also zero is the only pole of L−1T (L−1)−1, so F a is again a Jordan
matrix. Assuming we have the Smith-McMillan form:

L−1T (L−1) = A(L)S(L)B(L)

That means A(L), B(L) are invertible polynomial matrices and S(L) is diagonal
satisfying the Smith-McMillan divisibility requirement. Then

L−1T (L−1)−1 = B(L)−1L−2S(L)−1A(L)−1

L−2S(L)−1 is diagonal but not necessarily satisfying Smith-McMillan divisibil-
ity requirement, but we can make it to be, as in the ﬁnal step of the Smith-
McMillan algorithm. So L−1T (L−1)−1 and the traditional state-space form are
intimately related. However, there is no direct link between our AR-state-space
realization and the traditional one. This alternative model is harder to estimate,
even for p = 1.

10.2 Rank condition on H

So far we recover H by regression. Per Kalman, we should conﬁrm the rank for
H :,0. If it is not of full rank, the structure parameter Ψ that we work with may
not be minimal, and we can replace it by one with more reduced structure.

10.3 Determining the structure parameters

As p and m increases, the number of conﬁgurations for Ψ increases, polynomially
in m and exponentially in p. Given that we have relatively fast convergence, a
parallel search on conﬁgurations is certainly possible for a reasonable range of
p and m. However it may be unnecessary. The objective of the search should
be for the conﬁguration that balance between parameter reduction and close
approximation to the full likelihood. As pointed out in earlier analysis, the
number of parameter saving is impacted more by decreasing di for a higher
i. This motivates a search process where we do a full regression to obtain Φ,

30

then applying a rank test to reduce the rank of G:,0 which penalizing higher
exponents of the Jordan matrix. This could be done sequentially in descending
order of exponent, stopping after a number of steps based on a balance between
likelihood and parameter count. The search on each exponent could be done
using a bisection search if m is suﬃciently large, and would have a log(m)
iterative cost. So this method should be applicable even for large value of m
and p. This analysis could be done with the help of an information criteria or
by a likelihood ratio criteria (AIC or BIC). Another way is to formulate an
objective function that could penalize a norm of F i for higher i. This may be
a future research direction.

10.4 Convergence Analysis

It is well-known that Rayleigh quotient for a positive deﬁnite matrix is convex
and has a unique minimum. By now, we know little about the analytic property
of R(G, A, B), except that its Hessian is known and it is bounded. For general
A and B not necessarily constructed from the regression analysis here, it would
be interesting to analyze the critical points of the R. One question would be if
its minimal value is at a ﬁnite point, or would it be at a direction where G(r, l)
goes to inﬁnite (l > 0). A second question would be if it has more than one
local minima.

10.5 Generalization to VARMA

As Kalman’s result addresses the multiple root case of minimal realization, a
natural question is whether the results presented here has a full Gilbert-Kalman
picture analogue. The answer is yes, which we will address in a forthcoming
article. We hope the full result will give a new eﬀective method in Linear System
Identiﬁcation.

10.6 Further directions

The approach could be adjusted to address the drift and seasonal adjustments.
We have not addressed integration in this paper, however it seems plausible that
it could be done with appropriate modiﬁcation. A motivation for this paper
comes from Johansen’s approach to integration. Fixing a structure Ψ, we can
study other loss functions depending on G to go beyond the Gaussian assump-
tion. Instead of H applying linearly on κ(G)LiX we can assume a non-linear
format. For example, we can use the kernel trick to replace X LAGX 0
LAG, Y Y 0
and Y X 0
LAG with kernel values. We look forward to testing the model with
real data. We also look to improve on the optimization algorithms.

31

Appendix A Vector bundle on ﬂag mani-
folds

To take full advantage of the invariant property of the likelihood function, man-
ifold optimization may be an attractive option. In this appendix we summarize
the results in term of ﬂag manifolds. The uninterested reader can skip the ap-
pendix, consider it as a discussion on a particular optimization technique that
help reduces the search space to a lower dimensional set taking advantage of
the invariant property when replacing Go by QGo. On the other hand, the
geometric picture could be thought of as a high dimensional generalization of
the conﬁgurations of pairs of a particle moving on a circle and its velocity vector
as explained in section 9.2.

Let us ﬁrst ﬁx a few notations.

• Recall GL(m) is the group of all invertible matrices of size m × m, O(m)
is the orthogonal group, SO(m) is the special orthogonal group of all
orthogonal matrices of size m × m with determinant 1, S(O(l1) × O(l2) ×
· · · × O(lp) × O(m − l)) is the block diagonal subgroup of orthogonal group
with block size (l1, · · · , lp, m − l) and determinant 1.

• Let f1 = l1, fi = Pi

j=1 lj and fg+1 = m. Consider F(f1, · · · fg+1; R) =
O(m)/(O(l1) × O(l2) × · · · × O(lp) × O(m − l)).
It is called a real ﬂag
manifold. It has an equivalent representation: SO(m)/S(O(l1) × O(l2) ×
· · · × O(lp) × O(m − l)), see (Ye, Wong, and Lim 2019). (In the literature,
it can also be considered as a quotient of GL(m) by a parabolic subgroup
of GL(m)).

• Let R(G, A, B) = det(κ(G)Aκ(G)0

det(κ(G)Bκ(G)0) be the generalized Rayleigh quotient

corresponding to two symmetric matrices A, B each of size mp × mp.

The following theorem describes the manifold that R is deﬁned on based on the
invariant properties above, it may be just a restatement of results in section 7 in
a fancier language, but it allows us to apply manifold optimization techniques:

Theorem 2. The conﬁguration space GO and the group of orthogonal diagonal
block matrices Q have the following properties:

• Q is isomorphic to O(l1) × · · · O(lg).
• The map from GO to O(m)/O(m − l) given by ﬁrst representing Go as a
pair (C, O) with C = (Cr,l)r,l and O an orthogonal matrix as in propo-
sition 4, then map O to the class O(m − l)O ∈ O(m)/O(m − l) is well-
deﬁned: two representations ((C, O) and (C1, O1) of Go give the same
image in O(m)/O(m − l)).

• The above map induces a ﬁber bundle projection π from GO/Q to F(f1, · · · , fg+1; R).

Each ﬁber is a vector space isomorphic to ⊕p
dj)
where d0 = m − l. So GO/Q is a vector bundle over F(f1, · · · , fg+1; R).
We will call it K(Ψ; m). When p = 1 or l = m, K could be identiﬁed with
F.

r=1 Mat(dr, Pr−1

Pr−l−1
j=0

l=1

32

• The dimension of K is given by m P
i>j≥0 didj + Pp
• It is also given by P

j>0 jdj − P
Pr−1
r=1 dr
l=1

i≥1(P
Pr−l−1
j=0

j≥i dj)2
dj.

• R is a bounded smooth function on K(Ψ, m).

Proof. The ﬁrst statement follows from the diagonal form of Q, and Qρ,l;ρ,l =
Qρ for p−1 ≥ l ≥ 1. The second statement is clear, as another representation of
#

"

Go would have a form (CQ0

⊥,

G:,0
Q⊥Go
⊥

for some Q⊥ ∈ O(m − l). For the next
#

"

statement, if Go is replaced by QGo with Q ∈ Q, (C,

) is transformed to

Go
:,0
G⊥

(QrCr,l diag(Q0

r−l−1 · · · Q0

1, I d0))p≥r≥1,r−1≥l≥0,









QpGp,0
...
Q1G1,0
Go
⊥









So π(QGo) is in the same equivalent class with π(Go) ∈ F(f1, · · · , fg+1; R).
The ﬁber is isomorphic to C = ⊕Cr,l. The ﬁrst expression for dimension of K
is just dim(G) − dim(S), the second expression is sum of the dimensions of ﬂag
manifold and of the vector space ﬁber.

That R is bounded is already proved in proposition 5, and it is clearly

smooth.

Optimizing on K could be advantageous as in many instances its dimension
is much less than dim(G) = m P jdj. We have seen in our examples the search
space dimension could be reduced by half. If there were a manifold optimization
package for K we could take advantage of it. To our knowledge such a package is
not yet available (however, see (Ye, Wong, and Lim 2019). On the other hand,
we can optimize on O(m) × ⊕p
r=1 Mat(dr, Pr−1
dj), instead of taking
quotient down to the ﬂag manifold bundle level. We use the packages (Boumal
et al. 2014), (Townsend, Koep, and Weichwald 2016) to optimize on the O(m)
for the case where l = h and oﬀer a method manifold_ﬁt in our package.

Pr−l−1
j=0

l=1

Bibilography

Ahn, S. K. and G. C. Reinsel (1988). “Nested Reduced-Rank Autoregres-
sive Models for Multiple Time Series”. In: Journal of the American
Statistical Association 83.403, pp. 849–856. doi: 10.1080/01621459.
1988.10478673.

Anderson, T. W. (1999). “Asymptotic distribution of the reduced rank
regression estimator under general conditions”. In: Ann. Statist. 27.4,
pp. 1141–1154. doi: 10.1214/aos/1017938918.

33

— (2002). “Canonical correlation analysis and reduced rank regression
in autoregressive models”. In: Ann. Statist. 30.4, pp. 1134–1154. doi:
10.1214/aos/1031689020.

Anderson, T. W. (1951). “Estimating Linear Restrictions on Regression
Coeﬃcients for Multivariate Normal Distributions”. In: The Annals of
Mathematical Statistics 22.3, pp. 327–351. issn: 00034851.

Boumal, N. et al. (2014). “Manopt, a Matlab Toolbox for Optimization on
Manifolds”. In: Journal of Machine Learning Research 15, pp. 1455–
1459. url: http://www.manopt.org.

Box, G. E. P. and G. C. Tiao (1977). “A canonical analysis of multiple
time series”. In: Biometrika 64.2, pp. 355–365. issn: 0006-3444. doi:
10.1093/biomet/64.2.355.

Brillinger, D. R. (1969). “The canonical analysis of stationary time se-
ries”. In: Multivariate Analysis, 2. Ed. by P. R. Krishnaiah. New York:
Academic Press, pp. 331–350.

Gilbert, E. (1963). “Controllability and Observability in Multivariable
Control Systems”. In: Journal of the Society for Industrial and Ap-
plied Mathematics Series A Control 1.2, pp. 128–151. doi: 10.1137/
0301009.

Ito, Naoharu, Wiland Schmale, and Harald K. Wimmer (2002). “Minimal
state space realizations in Jacobson normal form”. In: International
Journal of Control 75.14, pp. 1092–1099. doi: 10.1080/00207170210158277.

Johansen, Soren (1995). Likelihood-Based Inference in Cointegrated Vector

Autoregressive Models. Oxford University Press.

Johansen, Søren (1991). “Estimation and Hypothesis Testing of Cointegra-
tion Vectors in Gaussian Vector Autoregressive Models”. In: Economet-
rica 59.6, pp. 1551–1580. issn: 00129682, 14680262.

Kalman, R. (1965). “Irreducible Realizations and the Degree of a Rational
Matrix”. In: Journal of the Society for Industrial and Applied Mathe-
matics 13.2, pp. 520–544. doi: 10.1137/0113034.

Nguyen, Du (2019). Project VARX. https : / / github . com / dnguyend /

minimal_varx.

Townsend, James, Niklas Koep, and Sebastian Weichwald (2016). “Py-
manopt: A Python Toolbox for Optimization on Manifolds using Au-
tomatic Diﬀerentiation”. In: Journal of Machine Learning Research
17.137, pp. 1–5. url: http://jmlr.org/papers/v17/16-177.html.
Velu, Raja P., Gregory C. Reinsel, and Dean W. Wichern (1986). “Reduced
rank models for multiple time series”. In: Biometrika 73.1, pp. 105–118.
issn: 0006-3444. doi: 10.1093/biomet/73.1.105.

Ye, Ke, Ken Sze-Wai Wong, and Lek-Heng Lim (2019). Optimization on

ﬂag manifolds. arXiv: 1907.00949 [math.OC].

34

