8
1
0
2

l
u
J

5
2

]

V
C
.
s
c
[

1
v
8
2
5
9
0
.
7
0
8
1
:
v
i
X
r
a

Toward Scale-Invariance and Position-Sensitive
Region Proposal Networks

Hsueh-Fu Lu, Xiaofei Du, and Ping-Lin Chang

Umbo Computer Vision
{topper.lu, xiaofei.du, ping-lin.chang}@umbocv.com
https://umbocv.ai

Abstract. Accurately localising object proposals is an important pre-
condition for high detection rate for the state-of-the-art object detection
frameworks. The accuracy of an object detection method has been shown
highly related to the average recall (AR) of the proposals. In this work,
we propose an advanced object proposal network in favour of translation-
invariance for objectness classiﬁcation, translation-variance for bounding
box regression, large eﬀective receptive ﬁelds for capturing global con-
text and scale-invariance for dealing with a range of object sizes from
extremely small to large. The design of the network architecture aims to
be simple while being eﬀective and with real-time performance. With-
out bells and whistles the proposed object proposal network signiﬁcantly
improves the AR at 1,000 proposals by 35% and 45% on PASCAL VOC
and COCO dataset respectively and has a fast inference time of 44.8 ms
for input image size of 6402. Empirical studies have also shown that the
proposed method is class-agnostic to be generalised for general object
proposal.

Keywords: Object Detection · Region Proposal Networks · Position-
Sensitive Anchors

1

Introduction

Object detection has been a challenging task in computer vision [6,17]. Signif-
icant progress has been achieved in the last decade from traditional sliding-
window paradigms [7,28] to recent top-performance proposal-based [27] detec-
tion frameworks [9,10,11]. A proposal algorithm plays a crucial role in an object
detection pipeline. On one hand, it speeds up the detection process by consid-
erably reducing the search space for image regions to be subsequently classiﬁed.
On the other hand, the average recall (AR) of the object proposal method has
been shown notably correlating with the precision of ﬁnal detection, in which
AR essentially reveals how accurate the detected bounding boxes are localised
comparing with the ground truth [2].

Instead of using low-level image features to heuristically generate the pro-
posals [27,30], trendy methods extract high-level features by using deep convo-
lutional neural networks (ConvNets) [12,26,29] to train a class-agnostic classiﬁer

 
 
 
 
 
 
2

H.-F. Lu, X. Du and P.-L. Chang

with a large number of annotated objects [3,21,6]. For general objectness detec-
tion, such supervised learning approaches make an important assumption that
given enough number of diﬀerent object categories, an objectness classiﬁer can
be suﬃciently generalised to unseen categories. It has been shown that learning-
based methods indeed tend to be unbiased to the dataset categories and learn
the union of features in the annotated object regions [1,2,3,5]. Despite their good
performance [5,12,6], there is still much room to improve the recall especially for
small objects and accuracy for the bounding box localisation [2,14,4,5].

To tackle object detection using ConvNets at various scales and for more ac-
curate localisation, prior works adopted an encoder-decoder architecture with
skip-connections [24] for exploiting low-resolution strong semantic and high-
resolution weak semantic features [4], used position sensitive score maps for
enhancing translation variance and invariance respectively for localisation and
classiﬁcation [5], and used a global convolutional network (GCN) component
for enlarging valid receptive ﬁeld (VRF) particularly for capturing larger image
context [19].

In this paper, we devise an advanced object proposal network which is capa-
ble of handling a large range of object scales and accurately localising proposed
bounding boxes. The proposed network architecture embraces fully convolutional
networks (FCNs) [18] without using fully-connected and pooling layers to pre-
serve spatial information as much as possible. The design takes simplicity into
account, in which the features extracted by ConvNets are entirely shared with
a light-weight network head as shown in Fig. 2.

Ablation studies have been conducted to show the eﬀectiveness of each de-
signed component. We have empirically found that GCN and position-sensitivity
structure can each individually improves the AR at 1,000 proposals. As shown
in Table 2 and 3, evaluating the baseline model on PASCAL VOC and COCO
dataset, GCN brings performance gains from 0.48 and 0.42 to 0.59 (22%) and
to 0.54 (29%) respectively, and, the use of position-sensitivity to 0.61 (26%) and
to 0.45 (6%) respectively. Using them together can furthermore boost the scores
to 0.65 (35%) and to 0.61 (44%) respectively. Together the proposed framework
achieves the state of the art and has a real-time performance.

2 Related works

Traditional object proposal methods take low-level image features to heuristi-
cally propose regions containing objectness. Methods such as Selective Search [27],
CPMC [3] and MCG [1] adopt grouping of multiple hierarchical segmentations
to produce the ﬁnal proposals. Edge boxes [30] on the other hand takes an as-
sumption that objectness is supposed to have clearer contours. Hosang et al. [2]
have comprehensively evaluated diﬀerent proposal methods. Learning-based pro-
posal approaches have gained more attentions recently. DeepBox [3] uses con-
volutional neural network to re-rank object proposals based on other bottom-
up non-learning proposal methods. Faster R-CNN [6] trains a region proposal
network (RPN) on a large number of annotated ground truth bounding boxes

Toward Scale-Inv. and Pos.-Sens. Reg. Propos. Net.

3

Fig. 1. The proposed method. Left: The position-sensitive score maps and windows
with k2 grids (k = 4) in yellow at D2 and D6 shown at the top and bottom respectively.
One can see the D2 activates on small objects while D6 activates on extreme large ones.
Note that D6 is enlarged for visualisation, in which the window size is in fact identical
to the one shown in D2. Right: The windows are mapped into anchors in cyan in the
input image with sizes of the multiple of layer stride s. Both: The bounding box in
orange is the only labeled ground truth (in bike category) on this image from PASCAL
VOC 2007. The large object on the right has no ground truth but the proposed class-
agnostic method can still be generalised to extract its objectness features as shown in
the visualised D6 feature maps.

to obtain high-quality box proposals for object detection. Region-based fully-
convolutional network (R-FCN) [5] introduces position-sensitive score maps to
improve localisation of the bounding boxes at detection stage. Feature pyramid
network (FPN) [4] takes multi-scale feature maps into account to exploit scale-
invariant features for both object proposal and detection stages. Instead of using
feature pyramids and learning from ground truth bounding boxes, DeepMask [5]
and SharpMask [21] use feed-forward ConvNets trained by ground truth masks
and exploit multi-scale input images to perform mask proposals to achieve state-
of-the-art performances.

3 Proposed method

Inspired by FPN [4] and R-FCN [5], the proposed object proposal method is
devised in favour of scale-invariance and position-sensitivity to retain both in-
variance and variance on translation for respectively classifying and localising

D2D3D4D5D6s=64s=44

H.-F. Lu, X. Du and P.-L. Chang

Fig. 2. The overall proposed system architecture. Left: The ResNet together with
the feature pyramid structure form the general backbone of RPN heads. Right: The
structures of diﬀerent RPN heads. Both: Rectangles are components with learnable
parameters to train and ellipses are parameter-free operations. Dash arrow indicates
that the RPN head is shared by all feature pyramid levels.

objects. We also take VRF into account to learn objectness from a larger im-
age spatial context [19]. In addition, instead of regressing and classifying a set
of anchors using default proﬁle (i.e., scale and aspect ratio) by a ﬁxed (3 × 3)
convolutional kernel in certain layers [4,6], we propose directly mapping anchors
from sliding windows in each decoding layer together with sharing the position-
sensitive score maps. The overall ConvNets takes an input image with arbitrary
size to bottom-up encode and top-down decode features with skip connections
to preserve object locality [24]. Scale-invariance as one of the important traits
of the proposed method is thus achieved by extracting multi-scale features from
the input image. These semantically weak to strong features are then feed into
a series of decoding layers being shared by a RPN head. Anchors are generated
by a dense sliding window fashion shared by a bank of position sensitive score
maps. In the end, the network regresses the anchors to localise objects (reg for
short) and classiﬁes the objectness with scores (cls for short).

conv1conv2conv5conv4conv3CBR12048:256CBR11024:256CBR1512:256CBR1256:256updownCBR3256:256upCBR3256:256upCBR3256:256RPN HeadsigmoidPosition-sensitive RoI/ Global average poolingregclsCB1256:4k2CB1256:k2Input imageE2<latexit sha1_base64="x6hAJYGPmt4xFnhHc/zmEiS3wvU=">AAAB63icbVBNS8NAEJ34WetX1aOXxSJ4KkkR1FtRBI8VjC20oWy2k3bpZhN2N0IJ/Q1ePKh49Q9589+4bXPQ1gcDj/dmmJkXpoJr47rfzsrq2vrGZmmrvL2zu7dfOTh81EmmGPosEYlqh1Sj4BJ9w43AdqqQxqHAVji6mfqtJ1SaJ/LBjFMMYjqQPOKMGiv5t728PulVqm7NnYEsE68gVSjQ7FW+uv2EZTFKwwTVuuO5qQlyqgxnAiflbqYxpWxEB9ixVNIYdZDPjp2QU6v0SZQoW9KQmfp7Iqex1uM4tJ0xNUO96E3F/7xOZqLLIOcyzQxKNl8UZYKYhEw/J32ukBkxtoQyxe2thA2poszYfMo2BG/x5WXi12tXNe/+vNq4LtIowTGcwBl4cAENuIMm+MCAwzO8wpsjnRfn3fmYt644xcwR/IHz+QPwsI5O</latexit><latexit sha1_base64="x6hAJYGPmt4xFnhHc/zmEiS3wvU=">AAAB63icbVBNS8NAEJ34WetX1aOXxSJ4KkkR1FtRBI8VjC20oWy2k3bpZhN2N0IJ/Q1ePKh49Q9589+4bXPQ1gcDj/dmmJkXpoJr47rfzsrq2vrGZmmrvL2zu7dfOTh81EmmGPosEYlqh1Sj4BJ9w43AdqqQxqHAVji6mfqtJ1SaJ/LBjFMMYjqQPOKMGiv5t728PulVqm7NnYEsE68gVSjQ7FW+uv2EZTFKwwTVuuO5qQlyqgxnAiflbqYxpWxEB9ixVNIYdZDPjp2QU6v0SZQoW9KQmfp7Iqex1uM4tJ0xNUO96E3F/7xOZqLLIOcyzQxKNl8UZYKYhEw/J32ukBkxtoQyxe2thA2poszYfMo2BG/x5WXi12tXNe/+vNq4LtIowTGcwBl4cAENuIMm+MCAwzO8wpsjnRfn3fmYt644xcwR/IHz+QPwsI5O</latexit><latexit sha1_base64="x6hAJYGPmt4xFnhHc/zmEiS3wvU=">AAAB63icbVBNS8NAEJ34WetX1aOXxSJ4KkkR1FtRBI8VjC20oWy2k3bpZhN2N0IJ/Q1ePKh49Q9589+4bXPQ1gcDj/dmmJkXpoJr47rfzsrq2vrGZmmrvL2zu7dfOTh81EmmGPosEYlqh1Sj4BJ9w43AdqqQxqHAVji6mfqtJ1SaJ/LBjFMMYjqQPOKMGiv5t728PulVqm7NnYEsE68gVSjQ7FW+uv2EZTFKwwTVuuO5qQlyqgxnAiflbqYxpWxEB9ixVNIYdZDPjp2QU6v0SZQoW9KQmfp7Iqex1uM4tJ0xNUO96E3F/7xOZqLLIOcyzQxKNl8UZYKYhEw/J32ukBkxtoQyxe2thA2poszYfMo2BG/x5WXi12tXNe/+vNq4LtIowTGcwBl4cAENuIMm+MCAwzO8wpsjnRfn3fmYt644xcwR/IHz+QPwsI5O</latexit><latexit sha1_base64="x6hAJYGPmt4xFnhHc/zmEiS3wvU=">AAAB63icbVBNS8NAEJ34WetX1aOXxSJ4KkkR1FtRBI8VjC20oWy2k3bpZhN2N0IJ/Q1ePKh49Q9589+4bXPQ1gcDj/dmmJkXpoJr47rfzsrq2vrGZmmrvL2zu7dfOTh81EmmGPosEYlqh1Sj4BJ9w43AdqqQxqHAVji6mfqtJ1SaJ/LBjFMMYjqQPOKMGiv5t728PulVqm7NnYEsE68gVSjQ7FW+uv2EZTFKwwTVuuO5qQlyqgxnAiflbqYxpWxEB9ixVNIYdZDPjp2QU6v0SZQoW9KQmfp7Iqex1uM4tJ0xNUO96E3F/7xOZqLLIOcyzQxKNl8UZYKYhEw/J32ukBkxtoQyxe2thA2poszYfMo2BG/x5WXi12tXNe/+vNq4LtIowTGcwBl4cAENuIMm+MCAwzO8wpsjnRfn3fmYt644xcwR/IHz+QPwsI5O</latexit>E3<latexit sha1_base64="izvhgBUC5lPS3cD5CQbebnhys5Q=">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0lUUG9FETxWMLbQhrLZTtqlm03Y3Qgl9Dd48aDi1T/kzX/jts1Bqw8GHu/NMDMvTAXXxnW/nNLS8srqWnm9srG5tb1T3d170EmmGPosEYlqh1Sj4BJ9w43AdqqQxqHAVji6nvqtR1SaJ/LejFMMYjqQPOKMGiv5N738dNKr1ty6OwP5S7yC1KBAs1f97PYTlsUoDRNU647npibIqTKcCZxUupnGlLIRHWDHUklj1EE+O3ZCjqzSJ1GibElDZurPiZzGWo/j0HbG1Az1ojcV//M6mYkugpzLNDMo2XxRlAliEjL9nPS5QmbE2BLKFLe3EjakijJj86nYELzFl/8S/6R+WffuzmqNqyKNMhzAIRyDB+fQgFtogg8MODzBC7w60nl23pz3eWvJKWb24Recj2/yNI5P</latexit><latexit sha1_base64="izvhgBUC5lPS3cD5CQbebnhys5Q=">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0lUUG9FETxWMLbQhrLZTtqlm03Y3Qgl9Dd48aDi1T/kzX/jts1Bqw8GHu/NMDMvTAXXxnW/nNLS8srqWnm9srG5tb1T3d170EmmGPosEYlqh1Sj4BJ9w43AdqqQxqHAVji6nvqtR1SaJ/LejFMMYjqQPOKMGiv5N738dNKr1ty6OwP5S7yC1KBAs1f97PYTlsUoDRNU647npibIqTKcCZxUupnGlLIRHWDHUklj1EE+O3ZCjqzSJ1GibElDZurPiZzGWo/j0HbG1Az1ojcV//M6mYkugpzLNDMo2XxRlAliEjL9nPS5QmbE2BLKFLe3EjakijJj86nYELzFl/8S/6R+WffuzmqNqyKNMhzAIRyDB+fQgFtogg8MODzBC7w60nl23pz3eWvJKWb24Recj2/yNI5P</latexit><latexit sha1_base64="izvhgBUC5lPS3cD5CQbebnhys5Q=">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0lUUG9FETxWMLbQhrLZTtqlm03Y3Qgl9Dd48aDi1T/kzX/jts1Bqw8GHu/NMDMvTAXXxnW/nNLS8srqWnm9srG5tb1T3d170EmmGPosEYlqh1Sj4BJ9w43AdqqQxqHAVji6nvqtR1SaJ/LejFMMYjqQPOKMGiv5N738dNKr1ty6OwP5S7yC1KBAs1f97PYTlsUoDRNU647npibIqTKcCZxUupnGlLIRHWDHUklj1EE+O3ZCjqzSJ1GibElDZurPiZzGWo/j0HbG1Az1ojcV//M6mYkugpzLNDMo2XxRlAliEjL9nPS5QmbE2BLKFLe3EjakijJj86nYELzFl/8S/6R+WffuzmqNqyKNMhzAIRyDB+fQgFtogg8MODzBC7w60nl23pz3eWvJKWb24Recj2/yNI5P</latexit><latexit sha1_base64="izvhgBUC5lPS3cD5CQbebnhys5Q=">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0lUUG9FETxWMLbQhrLZTtqlm03Y3Qgl9Dd48aDi1T/kzX/jts1Bqw8GHu/NMDMvTAXXxnW/nNLS8srqWnm9srG5tb1T3d170EmmGPosEYlqh1Sj4BJ9w43AdqqQxqHAVji6nvqtR1SaJ/LejFMMYjqQPOKMGiv5N738dNKr1ty6OwP5S7yC1KBAs1f97PYTlsUoDRNU647npibIqTKcCZxUupnGlLIRHWDHUklj1EE+O3ZCjqzSJ1GibElDZurPiZzGWo/j0HbG1Az1ojcV//M6mYkugpzLNDMo2XxRlAliEjL9nPS5QmbE2BLKFLe3EjakijJj86nYELzFl/8S/6R+WffuzmqNqyKNMhzAIRyDB+fQgFtogg8MODzBC7w60nl23pz3eWvJKWb24Recj2/yNI5P</latexit>E4<latexit sha1_base64="gNkaFEs2ZD0k6U13g2BpHM4Bmps=">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0lEUG9FETxWMLbQhrLZTtqlm03Y3Qgl9Dd48aDi1T/kzX/jts1BWx8MPN6bYWZemAqujet+O6WV1bX1jfJmZWt7Z3evun/wqJNMMfRZIhLVDqlGwSX6hhuB7VQhjUOBrXB0M/VbT6g0T+SDGacYxHQgecQZNVbyb3v5+aRXrbl1dwayTLyC1KBAs1f96vYTlsUoDRNU647npibIqTKcCZxUupnGlLIRHWDHUklj1EE+O3ZCTqzSJ1GibElDZurviZzGWo/j0HbG1Az1ojcV//M6mYkug5zLNDMo2XxRlAliEjL9nPS5QmbE2BLKFLe3EjakijJj86nYELzFl5eJf1a/qnv357XGdZFGGY7gGE7BgwtowB00wQcGHJ7hFd4c6bw4787HvLXkFDOH8AfO5w/zuI5Q</latexit><latexit sha1_base64="gNkaFEs2ZD0k6U13g2BpHM4Bmps=">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0lEUG9FETxWMLbQhrLZTtqlm03Y3Qgl9Dd48aDi1T/kzX/jts1BWx8MPN6bYWZemAqujet+O6WV1bX1jfJmZWt7Z3evun/wqJNMMfRZIhLVDqlGwSX6hhuB7VQhjUOBrXB0M/VbT6g0T+SDGacYxHQgecQZNVbyb3v5+aRXrbl1dwayTLyC1KBAs1f96vYTlsUoDRNU647npibIqTKcCZxUupnGlLIRHWDHUklj1EE+O3ZCTqzSJ1GibElDZurviZzGWo/j0HbG1Az1ojcV//M6mYkug5zLNDMo2XxRlAliEjL9nPS5QmbE2BLKFLe3EjakijJj86nYELzFl5eJf1a/qnv357XGdZFGGY7gGE7BgwtowB00wQcGHJ7hFd4c6bw4787HvLXkFDOH8AfO5w/zuI5Q</latexit><latexit sha1_base64="gNkaFEs2ZD0k6U13g2BpHM4Bmps=">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0lEUG9FETxWMLbQhrLZTtqlm03Y3Qgl9Dd48aDi1T/kzX/jts1BWx8MPN6bYWZemAqujet+O6WV1bX1jfJmZWt7Z3evun/wqJNMMfRZIhLVDqlGwSX6hhuB7VQhjUOBrXB0M/VbT6g0T+SDGacYxHQgecQZNVbyb3v5+aRXrbl1dwayTLyC1KBAs1f96vYTlsUoDRNU647npibIqTKcCZxUupnGlLIRHWDHUklj1EE+O3ZCTqzSJ1GibElDZurviZzGWo/j0HbG1Az1ojcV//M6mYkug5zLNDMo2XxRlAliEjL9nPS5QmbE2BLKFLe3EjakijJj86nYELzFl5eJf1a/qnv357XGdZFGGY7gGE7BgwtowB00wQcGHJ7hFd4c6bw4787HvLXkFDOH8AfO5w/zuI5Q</latexit><latexit sha1_base64="gNkaFEs2ZD0k6U13g2BpHM4Bmps=">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0lEUG9FETxWMLbQhrLZTtqlm03Y3Qgl9Dd48aDi1T/kzX/jts1BWx8MPN6bYWZemAqujet+O6WV1bX1jfJmZWt7Z3evun/wqJNMMfRZIhLVDqlGwSX6hhuB7VQhjUOBrXB0M/VbT6g0T+SDGacYxHQgecQZNVbyb3v5+aRXrbl1dwayTLyC1KBAs1f96vYTlsUoDRNU647npibIqTKcCZxUupnGlLIRHWDHUklj1EE+O3ZCTqzSJ1GibElDZurviZzGWo/j0HbG1Az1ojcV//M6mYkug5zLNDMo2XxRlAliEjL9nPS5QmbE2BLKFLe3EjakijJj86nYELzFl5eJf1a/qnv357XGdZFGGY7gGE7BgwtowB00wQcGHJ7hFd4c6bw4787HvLXkFDOH8AfO5w/zuI5Q</latexit>E5<latexit sha1_base64="oVpOM5KiXAGxw+gtqCjufmo2cgA=">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0lEUW9FETxWMLbQhrLZTtqlm03Y3Qgl9Dd48aDi1T/kzX/jts1Bqw8GHu/NMDMvTAXXxnW/nNLS8srqWnm9srG5tb1T3d170EmmGPosEYlqh1Sj4BJ9w43AdqqQxqHAVji6nvqtR1SaJ/LejFMMYjqQPOKMGiv5N738bNKr1ty6OwP5S7yC1KBAs1f97PYTlsUoDRNU647npibIqTKcCZxUupnGlLIRHWDHUklj1EE+O3ZCjqzSJ1GibElDZurPiZzGWo/j0HbG1Az1ojcV//M6mYkugpzLNDMo2XxRlAliEjL9nPS5QmbE2BLKFLe3EjakijJj86nYELzFl/8S/6R+WffuTmuNqyKNMhzAIRyDB+fQgFtogg8MODzBC7w60nl23pz3eWvJKWb24Recj2/1PI5R</latexit><latexit sha1_base64="oVpOM5KiXAGxw+gtqCjufmo2cgA=">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0lEUW9FETxWMLbQhrLZTtqlm03Y3Qgl9Dd48aDi1T/kzX/jts1Bqw8GHu/NMDMvTAXXxnW/nNLS8srqWnm9srG5tb1T3d170EmmGPosEYlqh1Sj4BJ9w43AdqqQxqHAVji6nvqtR1SaJ/LejFMMYjqQPOKMGiv5N738bNKr1ty6OwP5S7yC1KBAs1f97PYTlsUoDRNU647npibIqTKcCZxUupnGlLIRHWDHUklj1EE+O3ZCjqzSJ1GibElDZurPiZzGWo/j0HbG1Az1ojcV//M6mYkugpzLNDMo2XxRlAliEjL9nPS5QmbE2BLKFLe3EjakijJj86nYELzFl/8S/6R+WffuTmuNqyKNMhzAIRyDB+fQgFtogg8MODzBC7w60nl23pz3eWvJKWb24Recj2/1PI5R</latexit><latexit sha1_base64="oVpOM5KiXAGxw+gtqCjufmo2cgA=">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0lEUW9FETxWMLbQhrLZTtqlm03Y3Qgl9Dd48aDi1T/kzX/jts1Bqw8GHu/NMDMvTAXXxnW/nNLS8srqWnm9srG5tb1T3d170EmmGPosEYlqh1Sj4BJ9w43AdqqQxqHAVji6nvqtR1SaJ/LejFMMYjqQPOKMGiv5N738bNKr1ty6OwP5S7yC1KBAs1f97PYTlsUoDRNU647npibIqTKcCZxUupnGlLIRHWDHUklj1EE+O3ZCjqzSJ1GibElDZurPiZzGWo/j0HbG1Az1ojcV//M6mYkugpzLNDMo2XxRlAliEjL9nPS5QmbE2BLKFLe3EjakijJj86nYELzFl/8S/6R+WffuTmuNqyKNMhzAIRyDB+fQgFtogg8MODzBC7w60nl23pz3eWvJKWb24Recj2/1PI5R</latexit><latexit sha1_base64="oVpOM5KiXAGxw+gtqCjufmo2cgA=">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0lEUW9FETxWMLbQhrLZTtqlm03Y3Qgl9Dd48aDi1T/kzX/jts1Bqw8GHu/NMDMvTAXXxnW/nNLS8srqWnm9srG5tb1T3d170EmmGPosEYlqh1Sj4BJ9w43AdqqQxqHAVji6nvqtR1SaJ/LejFMMYjqQPOKMGiv5N738bNKr1ty6OwP5S7yC1KBAs1f97PYTlsUoDRNU647npibIqTKcCZxUupnGlLIRHWDHUklj1EE+O3ZCjqzSJ1GibElDZurPiZzGWo/j0HbG1Az1ojcV//M6mYkugpzLNDMo2XxRlAliEjL9nPS5QmbE2BLKFLe3EjakijJj86nYELzFl/8S/6R+WffuTmuNqyKNMhzAIRyDB+fQgFtogg8MODzBC7w60nl23pz3eWvJKWb24Recj2/1PI5R</latexit>D2<latexit sha1_base64="+za7psNQI6qXEVZ+HSPQVSsFtok=">AAAB63icbVBNS8NAEJ34WetX1aOXxSJ4KkkR1FtRDx4rGFtoQ9lsJ+3SzSbsboQS+hu8eFDx6h/y5r9x2+agrQ8GHu/NMDMvTAXXxnW/nZXVtfWNzdJWeXtnd2+/cnD4qJNMMfRZIhLVDqlGwSX6hhuB7VQhjUOBrXB0M/VbT6g0T+SDGacYxHQgecQZNVbyb3t5fdKrVN2aOwNZJl5BqlCg2at8dfsJy2KUhgmqdcdzUxPkVBnOBE7K3UxjStmIDrBjqaQx6iCfHTshp1bpkyhRtqQhM/X3RE5jrcdxaDtjaoZ60ZuK/3mdzESXQc5lmhmUbL4oygQxCZl+TvpcITNibAllittbCRtSRZmx+ZRtCN7iy8vEr9euat79ebVxXaRRgmM4gTPw4AIacAdN8IEBh2d4hTdHOi/Ou/Mxb11xipkj+APn8wfvKY5N</latexit><latexit sha1_base64="+za7psNQI6qXEVZ+HSPQVSsFtok=">AAAB63icbVBNS8NAEJ34WetX1aOXxSJ4KkkR1FtRDx4rGFtoQ9lsJ+3SzSbsboQS+hu8eFDx6h/y5r9x2+agrQ8GHu/NMDMvTAXXxnW/nZXVtfWNzdJWeXtnd2+/cnD4qJNMMfRZIhLVDqlGwSX6hhuB7VQhjUOBrXB0M/VbT6g0T+SDGacYxHQgecQZNVbyb3t5fdKrVN2aOwNZJl5BqlCg2at8dfsJy2KUhgmqdcdzUxPkVBnOBE7K3UxjStmIDrBjqaQx6iCfHTshp1bpkyhRtqQhM/X3RE5jrcdxaDtjaoZ60ZuK/3mdzESXQc5lmhmUbL4oygQxCZl+TvpcITNibAllittbCRtSRZmx+ZRtCN7iy8vEr9euat79ebVxXaRRgmM4gTPw4AIacAdN8IEBh2d4hTdHOi/Ou/Mxb11xipkj+APn8wfvKY5N</latexit><latexit sha1_base64="+za7psNQI6qXEVZ+HSPQVSsFtok=">AAAB63icbVBNS8NAEJ34WetX1aOXxSJ4KkkR1FtRDx4rGFtoQ9lsJ+3SzSbsboQS+hu8eFDx6h/y5r9x2+agrQ8GHu/NMDMvTAXXxnW/nZXVtfWNzdJWeXtnd2+/cnD4qJNMMfRZIhLVDqlGwSX6hhuB7VQhjUOBrXB0M/VbT6g0T+SDGacYxHQgecQZNVbyb3t5fdKrVN2aOwNZJl5BqlCg2at8dfsJy2KUhgmqdcdzUxPkVBnOBE7K3UxjStmIDrBjqaQx6iCfHTshp1bpkyhRtqQhM/X3RE5jrcdxaDtjaoZ60ZuK/3mdzESXQc5lmhmUbL4oygQxCZl+TvpcITNibAllittbCRtSRZmx+ZRtCN7iy8vEr9euat79ebVxXaRRgmM4gTPw4AIacAdN8IEBh2d4hTdHOi/Ou/Mxb11xipkj+APn8wfvKY5N</latexit><latexit sha1_base64="+za7psNQI6qXEVZ+HSPQVSsFtok=">AAAB63icbVBNS8NAEJ34WetX1aOXxSJ4KkkR1FtRDx4rGFtoQ9lsJ+3SzSbsboQS+hu8eFDx6h/y5r9x2+agrQ8GHu/NMDMvTAXXxnW/nZXVtfWNzdJWeXtnd2+/cnD4qJNMMfRZIhLVDqlGwSX6hhuB7VQhjUOBrXB0M/VbT6g0T+SDGacYxHQgecQZNVbyb3t5fdKrVN2aOwNZJl5BqlCg2at8dfsJy2KUhgmqdcdzUxPkVBnOBE7K3UxjStmIDrBjqaQx6iCfHTshp1bpkyhRtqQhM/X3RE5jrcdxaDtjaoZ60ZuK/3mdzESXQc5lmhmUbL4oygQxCZl+TvpcITNibAllittbCRtSRZmx+ZRtCN7iy8vEr9euat79ebVxXaRRgmM4gTPw4AIacAdN8IEBh2d4hTdHOi/Ou/Mxb11xipkj+APn8wfvKY5N</latexit>D3<latexit sha1_base64="VWwARLL58iJTLCY24Js1NBAUTVc=">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0lUUG9FPXisYGyhDWWznbRLN5uwuxFK6G/w4kHFq3/Im//GbZuDVh8MPN6bYWZemAqujet+OaWl5ZXVtfJ6ZWNza3unurv3oJNMMfRZIhLVDqlGwSX6hhuB7VQhjUOBrXB0PfVbj6g0T+S9GacYxHQgecQZNVbyb3r56aRXrbl1dwbyl3gFqUGBZq/62e0nLItRGiao1h3PTU2QU2U4EzipdDONKWUjOsCOpZLGqIN8duyEHFmlT6JE2ZKGzNSfEzmNtR7Hoe2MqRnqRW8q/ud1MhNdBDmXaWZQsvmiKBPEJGT6OelzhcyIsSWUKW5vJWxIFWXG5lOxIXiLL/8l/kn9su7dndUaV0UaZTiAQzgGD86hAbfQBB8YcHiCF3h1pPPsvDnv89aSU8zswy84H9/wrY5O</latexit><latexit sha1_base64="VWwARLL58iJTLCY24Js1NBAUTVc=">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0lUUG9FPXisYGyhDWWznbRLN5uwuxFK6G/w4kHFq3/Im//GbZuDVh8MPN6bYWZemAqujet+OaWl5ZXVtfJ6ZWNza3unurv3oJNMMfRZIhLVDqlGwSX6hhuB7VQhjUOBrXB0PfVbj6g0T+S9GacYxHQgecQZNVbyb3r56aRXrbl1dwbyl3gFqUGBZq/62e0nLItRGiao1h3PTU2QU2U4EzipdDONKWUjOsCOpZLGqIN8duyEHFmlT6JE2ZKGzNSfEzmNtR7Hoe2MqRnqRW8q/ud1MhNdBDmXaWZQsvmiKBPEJGT6OelzhcyIsSWUKW5vJWxIFWXG5lOxIXiLL/8l/kn9su7dndUaV0UaZTiAQzgGD86hAbfQBB8YcHiCF3h1pPPsvDnv89aSU8zswy84H9/wrY5O</latexit><latexit sha1_base64="VWwARLL58iJTLCY24Js1NBAUTVc=">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0lUUG9FPXisYGyhDWWznbRLN5uwuxFK6G/w4kHFq3/Im//GbZuDVh8MPN6bYWZemAqujet+OaWl5ZXVtfJ6ZWNza3unurv3oJNMMfRZIhLVDqlGwSX6hhuB7VQhjUOBrXB0PfVbj6g0T+S9GacYxHQgecQZNVbyb3r56aRXrbl1dwbyl3gFqUGBZq/62e0nLItRGiao1h3PTU2QU2U4EzipdDONKWUjOsCOpZLGqIN8duyEHFmlT6JE2ZKGzNSfEzmNtR7Hoe2MqRnqRW8q/ud1MhNdBDmXaWZQsvmiKBPEJGT6OelzhcyIsSWUKW5vJWxIFWXG5lOxIXiLL/8l/kn9su7dndUaV0UaZTiAQzgGD86hAbfQBB8YcHiCF3h1pPPsvDnv89aSU8zswy84H9/wrY5O</latexit><latexit sha1_base64="VWwARLL58iJTLCY24Js1NBAUTVc=">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0lUUG9FPXisYGyhDWWznbRLN5uwuxFK6G/w4kHFq3/Im//GbZuDVh8MPN6bYWZemAqujet+OaWl5ZXVtfJ6ZWNza3unurv3oJNMMfRZIhLVDqlGwSX6hhuB7VQhjUOBrXB0PfVbj6g0T+S9GacYxHQgecQZNVbyb3r56aRXrbl1dwbyl3gFqUGBZq/62e0nLItRGiao1h3PTU2QU2U4EzipdDONKWUjOsCOpZLGqIN8duyEHFmlT6JE2ZKGzNSfEzmNtR7Hoe2MqRnqRW8q/ud1MhNdBDmXaWZQsvmiKBPEJGT6OelzhcyIsSWUKW5vJWxIFWXG5lOxIXiLL/8l/kn9su7dndUaV0UaZTiAQzgGD86hAbfQBB8YcHiCF3h1pPPsvDnv89aSU8zswy84H9/wrY5O</latexit>D4<latexit sha1_base64="uPg9mXVRupsnIZTq8NWwdkjrlWg=">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0lEUG9FPXisYGyhDWWznbRLN5uwuxFK6G/w4kHFq3/Im//GbZuDtj4YeLw3w8y8MBVcG9f9dkorq2vrG+XNytb2zu5edf/gUSeZYuizRCSqHVKNgkv0DTcC26lCGocCW+HoZuq3nlBpnsgHM04xiOlA8ogzaqzk3/by80mvWnPr7gxkmXgFqUGBZq/61e0nLItRGiao1h3PTU2QU2U4EzipdDONKWUjOsCOpZLGqIN8duyEnFilT6JE2ZKGzNTfEzmNtR7Hoe2MqRnqRW8q/ud1MhNdBjmXaWZQsvmiKBPEJGT6OelzhcyIsSWUKW5vJWxIFWXG5lOxIXiLLy8T/6x+Vffuz2uN6yKNMhzBMZyCBxfQgDtogg8MODzDK7w50nlx3p2PeWvJKWYO4Q+czx/yMY5P</latexit><latexit sha1_base64="uPg9mXVRupsnIZTq8NWwdkjrlWg=">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0lEUG9FPXisYGyhDWWznbRLN5uwuxFK6G/w4kHFq3/Im//GbZuDtj4YeLw3w8y8MBVcG9f9dkorq2vrG+XNytb2zu5edf/gUSeZYuizRCSqHVKNgkv0DTcC26lCGocCW+HoZuq3nlBpnsgHM04xiOlA8ogzaqzk3/by80mvWnPr7gxkmXgFqUGBZq/61e0nLItRGiao1h3PTU2QU2U4EzipdDONKWUjOsCOpZLGqIN8duyEnFilT6JE2ZKGzNTfEzmNtR7Hoe2MqRnqRW8q/ud1MhNdBjmXaWZQsvmiKBPEJGT6OelzhcyIsSWUKW5vJWxIFWXG5lOxIXiLLy8T/6x+Vffuz2uN6yKNMhzBMZyCBxfQgDtogg8MODzDK7w50nlx3p2PeWvJKWYO4Q+czx/yMY5P</latexit><latexit sha1_base64="uPg9mXVRupsnIZTq8NWwdkjrlWg=">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0lEUG9FPXisYGyhDWWznbRLN5uwuxFK6G/w4kHFq3/Im//GbZuDtj4YeLw3w8y8MBVcG9f9dkorq2vrG+XNytb2zu5edf/gUSeZYuizRCSqHVKNgkv0DTcC26lCGocCW+HoZuq3nlBpnsgHM04xiOlA8ogzaqzk3/by80mvWnPr7gxkmXgFqUGBZq/61e0nLItRGiao1h3PTU2QU2U4EzipdDONKWUjOsCOpZLGqIN8duyEnFilT6JE2ZKGzNTfEzmNtR7Hoe2MqRnqRW8q/ud1MhNdBjmXaWZQsvmiKBPEJGT6OelzhcyIsSWUKW5vJWxIFWXG5lOxIXiLLy8T/6x+Vffuz2uN6yKNMhzBMZyCBxfQgDtogg8MODzDK7w50nlx3p2PeWvJKWYO4Q+czx/yMY5P</latexit><latexit sha1_base64="uPg9mXVRupsnIZTq8NWwdkjrlWg=">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0lEUG9FPXisYGyhDWWznbRLN5uwuxFK6G/w4kHFq3/Im//GbZuDtj4YeLw3w8y8MBVcG9f9dkorq2vrG+XNytb2zu5edf/gUSeZYuizRCSqHVKNgkv0DTcC26lCGocCW+HoZuq3nlBpnsgHM04xiOlA8ogzaqzk3/by80mvWnPr7gxkmXgFqUGBZq/61e0nLItRGiao1h3PTU2QU2U4EzipdDONKWUjOsCOpZLGqIN8duyEnFilT6JE2ZKGzNTfEzmNtR7Hoe2MqRnqRW8q/ud1MhNdBjmXaWZQsvmiKBPEJGT6OelzhcyIsSWUKW5vJWxIFWXG5lOxIXiLLy8T/6x+Vffuz2uN6yKNMhzBMZyCBxfQgDtogg8MODzDK7w50nlx3p2PeWvJKWYO4Q+czx/yMY5P</latexit>D5<latexit sha1_base64="DM6UIvELaHaFxKtKRJ0jMBxEYI0=">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0lEUW9FPXisYGyhDWWznbRLN5uwuxFK6G/w4kHFq3/Im//GbZuDVh8MPN6bYWZemAqujet+OaWl5ZXVtfJ6ZWNza3unurv3oJNMMfRZIhLVDqlGwSX6hhuB7VQhjUOBrXB0PfVbj6g0T+S9GacYxHQgecQZNVbyb3r52aRXrbl1dwbyl3gFqUGBZq/62e0nLItRGiao1h3PTU2QU2U4EzipdDONKWUjOsCOpZLGqIN8duyEHFmlT6JE2ZKGzNSfEzmNtR7Hoe2MqRnqRW8q/ud1MhNdBDmXaWZQsvmiKBPEJGT6OelzhcyIsSWUKW5vJWxIFWXG5lOxIXiLL/8l/kn9su7dndYaV0UaZTiAQzgGD86hAbfQBB8YcHiCF3h1pPPsvDnv89aSU8zswy84H9/ztY5Q</latexit><latexit sha1_base64="DM6UIvELaHaFxKtKRJ0jMBxEYI0=">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0lEUW9FPXisYGyhDWWznbRLN5uwuxFK6G/w4kHFq3/Im//GbZuDVh8MPN6bYWZemAqujet+OaWl5ZXVtfJ6ZWNza3unurv3oJNMMfRZIhLVDqlGwSX6hhuB7VQhjUOBrXB0PfVbj6g0T+S9GacYxHQgecQZNVbyb3r52aRXrbl1dwbyl3gFqUGBZq/62e0nLItRGiao1h3PTU2QU2U4EzipdDONKWUjOsCOpZLGqIN8duyEHFmlT6JE2ZKGzNSfEzmNtR7Hoe2MqRnqRW8q/ud1MhNdBDmXaWZQsvmiKBPEJGT6OelzhcyIsSWUKW5vJWxIFWXG5lOxIXiLL/8l/kn9su7dndYaV0UaZTiAQzgGD86hAbfQBB8YcHiCF3h1pPPsvDnv89aSU8zswy84H9/ztY5Q</latexit><latexit sha1_base64="DM6UIvELaHaFxKtKRJ0jMBxEYI0=">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0lEUW9FPXisYGyhDWWznbRLN5uwuxFK6G/w4kHFq3/Im//GbZuDVh8MPN6bYWZemAqujet+OaWl5ZXVtfJ6ZWNza3unurv3oJNMMfRZIhLVDqlGwSX6hhuB7VQhjUOBrXB0PfVbj6g0T+S9GacYxHQgecQZNVbyb3r52aRXrbl1dwbyl3gFqUGBZq/62e0nLItRGiao1h3PTU2QU2U4EzipdDONKWUjOsCOpZLGqIN8duyEHFmlT6JE2ZKGzNSfEzmNtR7Hoe2MqRnqRW8q/ud1MhNdBDmXaWZQsvmiKBPEJGT6OelzhcyIsSWUKW5vJWxIFWXG5lOxIXiLL/8l/kn9su7dndYaV0UaZTiAQzgGD86hAbfQBB8YcHiCF3h1pPPsvDnv89aSU8zswy84H9/ztY5Q</latexit><latexit sha1_base64="DM6UIvELaHaFxKtKRJ0jMBxEYI0=">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0lEUW9FPXisYGyhDWWznbRLN5uwuxFK6G/w4kHFq3/Im//GbZuDVh8MPN6bYWZemAqujet+OaWl5ZXVtfJ6ZWNza3unurv3oJNMMfRZIhLVDqlGwSX6hhuB7VQhjUOBrXB0PfVbj6g0T+S9GacYxHQgecQZNVbyb3r52aRXrbl1dwbyl3gFqUGBZq/62e0nLItRGiao1h3PTU2QU2U4EzipdDONKWUjOsCOpZLGqIN8duyEHFmlT6JE2ZKGzNSfEzmNtR7Hoe2MqRnqRW8q/ud1MhNdBDmXaWZQsvmiKBPEJGT6OelzhcyIsSWUKW5vJWxIFWXG5lOxIXiLL/8l/kn9su7dndYaV0UaZTiAQzgGD86hAbfQBB8YcHiCF3h1pPPsvDnv89aSU8zswy84H9/ztY5Q</latexit>D6<latexit sha1_base64="0KzNPbIckmMo7NMDy1onzRK1B+I=">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0lE/LgV9eCxgrGFNpTNdtIu3WzC7kYoob/BiwcVr/4hb/4bt20OWn0w8Hhvhpl5YSq4Nq775ZSWlldW18rrlY3Nre2d6u7eg04yxdBniUhUO6QaBZfoG24EtlOFNA4FtsLR9dRvPaLSPJH3ZpxiENOB5BFn1FjJv+nlZ5NetebW3RnIX+IVpAYFmr3qZ7efsCxGaZigWnc8NzVBTpXhTOCk0s00ppSN6AA7lkoaow7y2bETcmSVPokSZUsaMlN/TuQ01noch7YzpmaoF72p+J/XyUx0EeRcpplByeaLokwQk5Dp56TPFTIjxpZQpri9lbAhVZQZm0/FhuAtvvyX+Cf1y7p3d1prXBVplOEADuEYPDiHBtxCE3xgwOEJXuDVkc6z8+a8z1tLTjGzD7/gfHwD9TmOUQ==</latexit><latexit sha1_base64="0KzNPbIckmMo7NMDy1onzRK1B+I=">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0lE/LgV9eCxgrGFNpTNdtIu3WzC7kYoob/BiwcVr/4hb/4bt20OWn0w8Hhvhpl5YSq4Nq775ZSWlldW18rrlY3Nre2d6u7eg04yxdBniUhUO6QaBZfoG24EtlOFNA4FtsLR9dRvPaLSPJH3ZpxiENOB5BFn1FjJv+nlZ5NetebW3RnIX+IVpAYFmr3qZ7efsCxGaZigWnc8NzVBTpXhTOCk0s00ppSN6AA7lkoaow7y2bETcmSVPokSZUsaMlN/TuQ01noch7YzpmaoF72p+J/XyUx0EeRcpplByeaLokwQk5Dp56TPFTIjxpZQpri9lbAhVZQZm0/FhuAtvvyX+Cf1y7p3d1prXBVplOEADuEYPDiHBtxCE3xgwOEJXuDVkc6z8+a8z1tLTjGzD7/gfHwD9TmOUQ==</latexit><latexit sha1_base64="0KzNPbIckmMo7NMDy1onzRK1B+I=">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0lE/LgV9eCxgrGFNpTNdtIu3WzC7kYoob/BiwcVr/4hb/4bt20OWn0w8Hhvhpl5YSq4Nq775ZSWlldW18rrlY3Nre2d6u7eg04yxdBniUhUO6QaBZfoG24EtlOFNA4FtsLR9dRvPaLSPJH3ZpxiENOB5BFn1FjJv+nlZ5NetebW3RnIX+IVpAYFmr3qZ7efsCxGaZigWnc8NzVBTpXhTOCk0s00ppSN6AA7lkoaow7y2bETcmSVPokSZUsaMlN/TuQ01noch7YzpmaoF72p+J/XyUx0EeRcpplByeaLokwQk5Dp56TPFTIjxpZQpri9lbAhVZQZm0/FhuAtvvyX+Cf1y7p3d1prXBVplOEADuEYPDiHBtxCE3xgwOEJXuDVkc6z8+a8z1tLTjGzD7/gfHwD9TmOUQ==</latexit><latexit sha1_base64="0KzNPbIckmMo7NMDy1onzRK1B+I=">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0lE/LgV9eCxgrGFNpTNdtIu3WzC7kYoob/BiwcVr/4hb/4bt20OWn0w8Hhvhpl5YSq4Nq775ZSWlldW18rrlY3Nre2d6u7eg04yxdBniUhUO6QaBZfoG24EtlOFNA4FtsLR9dRvPaLSPJH3ZpxiENOB5BFn1FjJv+nlZ5NetebW3RnIX+IVpAYFmr3qZ7efsCxGaZigWnc8NzVBTpXhTOCk0s00ppSN6AA7lkoaow7y2bETcmSVPokSZUsaMlN/TuQ01noch7YzpmaoF72p+J/XyUx0EeRcpplByeaLokwQk5Dp56TPFTIjxpZQpri9lbAhVZQZm0/FhuAtvvyX+Cf1y7p3d1prXBVplOEADuEYPDiHBtxCE3xgwOEJXuDVkc6z8+a8z1tLTjGzD7/gfHwD9TmOUQ==</latexit>NaïveCBR3256:256CBR3256:256CBR3256:256BaselineCBR3256:256CBR1256:64C1x15 64:64C15x1 64:64C15x1 64:64C1x15 64:64BR64:64CBR164:256GCN shared smoother (GCN-S)Large kernel shared smoother (LK-S)CBR1256:64CBR1564:64CBR164:256GCN non-shared smoother (GCN-NS)CBR1256:64C1x15 64:64C15x1 64:64C15x1 64:64C1x15 64:64BR64:64CBR164:256Large kernel non-shared smoother (LK-NS)CBR1256:64CBR1564:64CBR3256:256CBR3256:256CBR3256:256CBR3256:256CBR3256:256CBR3256:256CBR164:256Toward Scale-Inv. and Pos.-Sens. Reg. Propos. Net.

5

3.1 Encoder

The encoder is a feed-forward ConvNet as the backbone feature extractor, which
scales down by a factor of 2 several times. Although the proposed method can
be equipped with any popular ConvNet architectures [26,29] for the backbone,
ResNet [12] is adopted particularly for its FCN structure being able to retain
the local information as much as possible. ResNets are structured with residual
blocks each consisting of a subset of ConvNets. We note the conv2, conv3, conv4
and conv5 blocks from the original paper [12] as {E2, E3, E4, E5} with the cor-
responding dense sliding window strides s = {4, 8, 16, 32} in regard to the input
image.

3.2 Decoder

The decoder recovers the feature resolution for the strongest semantic feature
maps from low to high with skip connections in between the corresponding en-
coder and decoder layers. The skip connection is substantial for the accuracy of
bounding box proposal as it propagates detail-preserving and position-accurate
features from the encoding process to the decoded features which are later shared
by the RPN head.

Speciﬁcally with ResNet, the decoding process starts from E5 using 1×1 con-
volution and 256 output channels for feature selection followed by batch normal-
isation (BN) and rectiﬁed linear unit (ReLU) layers, which together we brief as
CBR{·} where · is the kernel size. Likewise, each skip connection at a layer takes
a CBR1 with 256 output channels. The bottom-up decoding process is therefore
done by using bilinear upsampling followed by element-wise addition with the
CBR1 selected features from the encoding layers. A CBR3 block is inserted in
each decoding layer right after the addition for the purpose of de-aliasing. We
note the decoding layers as {D2, D3, D4, D5} corresponding to {E2, E3, E4, E5}
in the encoding layers. An extra D6 is added by directly down sampling D5
for gaining an even larger stride s = 64, which is in favor of extremely large
objectness detection.

3.3 RPN heads

A RPN head is in charge of learning features across a range of scales for reg and
cls. The learnable parameters in the RPN head share all features in the decoding
layers to capture diﬀerent levels of semantics for various object sizes. We will
show that the design of a RPN head has a signiﬁcant impact on the ﬁnal proposal
accuracy in Sec. 4. We show a number of diﬀerent RPN head designs in Fig. 2.
Each head takes 256 channel feature map as input and outputs two sibling CB1
blocks for reg and cls with 4 × k2 and k2 channels respectively, where k2 is the
number of regular grids for position-sensitive score maps described in Sec. 3.4.
We regard the state-of-the-art RPN used in FPN [4] as a Baseline method, in
which a CBR3 block is adopted for fusing multi-scale features. Our Baseline

6

H.-F. Lu, X. Du and P.-L. Chang

implementation, which is a bit diﬀerent from [4], uses BN and ReLU which have
been found helpful in converging the end-to-end training.

Inspired by GCN within residual structure [19], we hypothesise that enlarging
VRF to learn from larger image context can improve the overall object proposal
performance. For the GCN shared smoother (GCN-S) and Large kernel shared
smoother (LK-S), a larger convolution kernel (15 × 15) is inserted before the
CBR3 smoothing. Additionally their non-shared smoother counterpart (GCN-
NS and LK-NS) are also compared.

To study the eﬀect of model capacity and the increased number of parame-
ters, a Na¨ıve head is taken into account, which is simply added with more CBR3
blocks to approximately match the number of learnable parameters compared
with other RPN heads. Table 1 lists the number of parameters of all RPN heads.
Compared with the Baseline, the numbers of parameter ratio of the other models
are within a 0.015 standard deviation.

3.4 Position-sensitive anchors

We argue that using a default set of scales and aspect ratios to map anchors
from a constant-size convolution kernel can potentially undermine the accuracy
of reg and cls. This could be due to the mismatch of the receptive ﬁeld of
network and the mapped anchors. Prior works have used such strategy [5,4,6]
with little exploration of other varieties. To improve the ﬁdelity of relationship
between features and anchors with respect to the receptive ﬁeld of ConvNets,
in the proposed method, at each layer, the size of an anchor is calculated by
(w · s) × (h · s) where w and h are the width and height of the sliding window.
Since the anchor and the sliding window are now naturally mapped, position-
sensitive score maps can be further exploited for improving the accuracy of
localisation. Fig. 1 illustrates the stack of score maps for k2 regular grids in
the sliding window. Each grid in the window takes average of its coverage on
the corresponding score map (i.e., average pooling). All k2 grids then undergo a
global average pooling to output 4-channel t and 1-channel o for the ﬁnal reg and
cls result respectively. We further feed o to an activation function sigmoid for
evaluating the objectness score. Details of position-sensitive pooling can be found
in [5]. In this paper we use k = 4 for illustration as well as for all experiments.

3.5 Implementation details

In this paper, all the experiments were conducted with ResNet-50 with the re-
moval of average pooling, fully-connected and softmax layers in the end of the
original model. We do not use conv1 in the pyramid due to the high memory
footage and too low-level features which contribute very little for semantically
representing objectness. The architecture is illustrated in Fig. 2. A set of win-
dow sizes w : h = {8 : 8, 4 : 8, 8 : 4, 3 : 9, 9 : 3} are used for the dense sliding
windows at each layer for generating anchors. At the most top D6 and bottom
D2 layer, additional window sizes {12 : 12, 6 : 12, 12 : 6, 12 : 4, 4 : 12} and

Toward Scale-Inv. and Pos.-Sens. Reg. Propos. Net.

7

{4 : 4, 2 : 4, 4 : 2} are respectively used for discovering extremely large and
small objectness.

The proposed position-sensitive anchors mapped from the windows are all
inside the input image, but the bounding boxes regressed from anchors can
possibly exceed the image boundary. We simply discard those bounding boxes
exceeding the image boundary. In addition, the number of total anchors depends
on the size of input image and the used anchor proﬁle. The eﬀect of anchor
number is discussed in the supplementary material.

Training In each image, a large amount of anchors are generated across all de-
coding layers to be further assigned positive and negative labels. An anchor hav-
ing intersection-over-union (IoU) with any ground truth bounding box greater
than 0.7 is assigned a positive label p and less than 0.3 a negative label n. For each
ground truth bounding box, the anchor with the highest IoU is also assigned to a
positive label, only if the IoU is greater than 0.3. This policy is similar to [6] but
with the additional lower bound for avoiding distraction of outliers. NA anchors
(half positive and half negative anchors) are selected for each training iteration.
The model can be trained end-to-end with NB mini-batch images together with
the sampled anchors using a deﬁned loss:

L =

1
NB · NA

NB(cid:88)

(cid:20) NA(cid:88)

i=1

j=1

(cid:104)
Lreg(tp

i,j, t∗

i,j) + Lcls(op

(cid:105)
i,j)

Lcls(on

(cid:21)
,
i,j)

(1)

+

NA(cid:88)

j=1

where t is the regressed bounding box with t∗ as its ground truth correspondent,
and o is the objectness score. Lreg is the smooth L1 loss taking the diﬀerence
between normalised bounding box coordinates with the ground truth as deﬁned
in [9], and Lcls the cross-entropy loss. We use stochastic gradient descent (SGD)
with momentum of 0.9, weight decay of 10−4 and exponential decay learning
rate le = l0b−λe, in which the e is the epoch number and we set l0 = 0.1 and
λ = 0.1 for the base b = 10.

4 Empirical studies

We have conducted comprehensive experiments for comparing diﬀerent RPN
heads as well as ablation studies to show the impact of position-sensitive score
maps. The experiment platform is equipped with an Intel(R) Xeon(R) CPU E5-
2650 v4@2.20GHz CPU and Nvidia Titan X (Pascal) GPUs with 12 GB memory.
Such hardware spec allowed us to train the models with batch size NB listed
in Table 1. Note that we particularly focus on conducting ablation studies on
diﬀerent components. In all experiments we therefore did not exploit additional
tricks for boosting the performance such as using multi-scale input images for
training [11] and testing [12], iterative regression [8], hard example mining [25],
etc.

8

H.-F. Lu, X. Du and P.-L. Chang

Table 1. The number of parameters in the diﬀerent models and the corresponding
inference time T in ms averaged on the number of all testing images

w/o position-sensitive

w/ position-sensitive

# params NB T07test Tminival # params NB T07test Tminival

Baseline 26,858,334 28

26.6

58.2

26,875,104 18

35.7

79.5

28,039,006 18
Na¨ıve
27,137,630 18
GCN-S
LK-S
27,81,3470 18
GCN-NS 27,727,966 16
28,403,806 16
LK-NS

32.2
34.3
45.2
35.9
48.8

-
76.3
-
83.5
-

28,055,776 14
27,154,400 14
27,830,240 14
27,744,736 12
28,420,576 12

41.5
44.1
55.1
44.8
57.5

-
96.1
-
103.6
-

Baseline model The implementation of our Baseline model, with or without
using position-sensitive score maps, diﬀer from the original FPN [4] in the use of
BN and ReLU in the RPN head, as well as the de-aliasing CBR3 block in each
layer. In addition, the evaluation in their paper was conducted with rescaling
image short side to 800 pixels. The rest of setting such as anchor generation, the
number of pyramid layers, etc. are remained the same. Note that such discrep-
ancy do not aﬀect the ablation studies here to compare the baseline architecture.
The main purpose is to assess performance gains when adding other network
components.

Evaluation protocol All models are evaluated on PASCAL VOC [6] and
COCO [17]. For Pascal VOC we used all train and validation dataset in 2007 and
2012 (denoted as 07+12), which has in total 16,551 images with 40,058 objects,
and report test result on the 2007 test dataset (denoted as 07test) consisting
of 4,952 images with 12,032 objects. For COCO we employed the union of train
and a subset of validation set for in total 109,172 images and 654,212 objects
(denoted as trainval35k), and report test results on the rest of validation set
for 4,589 images and 27,436 objects (denoted as minival). Our evaluation is
consistent with the oﬃcial COCO evaluation protocol, in which areas marked as
”crowds” are ignored and do not aﬀect detector’s scores [17].

In order to perform mini-batch training, we rescaled images in 07+12 with
the long side ﬁxed and zero-pad along the rescaled short side to 640 × 640 for
batching, and for images in trainval35k, the short side were ﬁxed to 768 with
random crop along the rescaled long side to 768 × 768. For testing, images in
07test and minival are padded to have width and height being the closest mul-
tiple of the maximum stride (i.e., s = 64), to avoid rounding errors. All models
were trained for 40 epochs which roughly take 30 and 90 hours for PASCAL
VOC 07+12 and COCO trainval35k respectively.

Following [1,2,17], we evaluated models at AR with diﬀerent proposal num-
bers of 10, 100 and 1,000 {AR10, AR100, AR1k} and the area under the curve

Toward Scale-Inv. and Pos.-Sens. Reg. Propos. Net.

9

Fig. 3. Recall against IoU with diﬀerent proposal numbers of 10, 100 and 1,000 and
average recall against the number of proposals of all models: The results of PASCAL
VOC 07test using models trained on PASCAL VOC 07+12 (Row 1). The results of
COCO minival using models trained on COCO trainval35k (Row 2) and models
trained on PASCAL VOC 07+12 (Row 3).

(AUC) of recall across all proposal numbers. Besides, we also evaluated models
at AR for diﬀerent object area a: small (a < 322), medium (322 < a < 962) and
large (a > 962) with 1,000 proposals {AR1k
l }. It is worth noting
that COCO has more complex scenes with diverse and many more small ob-
jects than PASCAL VOC does [17,22]. We therefore evaluated all models with
PASCAL VOC while selected the top-performance GCN-S and GCN-NS models
for COCO evaluation. In the tables, numbers with underline indicate the high-
est score of a metric among models with diﬀerent RPN heads, and numbers in
bold indicate the highest score of a metric among models with or without using
position-sensitivity.

s , AR1k

m , AR1k

4.1 Impact of using GCN

The results of Table 2 and 3 reveal that by adding GCN in the RPN head, the
overall AR can be remarkably improved regardless the number of considered
proposals or if the position-sensitivity is employed. This can be also observed in
Fig. 3 in which GCN-S and GCN-NS curves are always on the top of others.

AR1k
s

in particular beneﬁts from learning the global image context. One can
observe that compared to Baseline model, the scores have been boosted by
85% from 0.254 to 0.471 on PASCAL VOC with GCN-S model, and by 37%
from 0.308 to 0.422 on COCO with GCN-NS. Therefore, the AR1k has been
overall improved from 0.480 and 0.421 to 0.586 and 0.542, which are 22% and
29% respectively. Fig. 3 also shows that GCN-S and GCN-NS models have the
highest recall scores across all IoU thresholds with diﬀerent proposal numbers.

0.500.550.600.650.700.750.800.850.900.951.00IoU0.00.10.20.30.40.50.60.70.80.91.0Recall10Proposalsw/opsBaselinew/opsNa¨ıvew/opsGCN-Sw/opsLK-Sw/opsGCN-NSw/opsLK-NSw/psBaselinew/psNa¨ıvew/psGCN-Sw/psLK-Sw/psGCN-NSw/psLK-NS0.500.550.600.650.700.750.800.850.900.951.00IoU0.00.10.20.30.40.50.60.70.80.91.0Recall100Proposalsw/opsBaselinew/opsNa¨ıvew/opsGCN-Sw/opsLK-Sw/opsGCN-NSw/opsLK-NSw/psBaselinew/psNa¨ıvew/psGCN-Sw/psLK-Sw/psGCN-NSw/psLK-NS0.500.550.600.650.700.750.800.850.900.951.00IoU0.00.10.20.30.40.50.60.70.80.91.0Recall1000Proposalsw/opsBaselinew/opsNa¨ıvew/opsGCN-Sw/opsLK-Sw/opsGCN-NSw/opsLK-NSw/psBaselinew/psNa¨ıvew/psGCN-Sw/psLK-Sw/psGCN-NSw/psLK-NS100101102103NumberofProposals0.00.10.20.30.40.50.60.70.80.91.0AverageRecallPASCALVOCw/opsBaselinew/opsNa¨ıvew/opsGCN-Sw/opsLK-Sw/opsGCN-NSw/opsLK-NSw/psBaselinew/psNa¨ıvew/psGCN-Sw/psLK-Sw/psGCN-NSw/psLK-NS0.500.550.600.650.700.750.800.850.900.951.00IoU0.00.10.20.30.40.50.60.70.80.91.0Recall10Proposalsw/opsBaselinew/opsGCN-Sw/opsGCN-NSw/psBaselinew/psGCN-Sw/psGCN-NS0.500.550.600.650.700.750.800.850.900.951.00IoU0.00.10.20.30.40.50.60.70.80.91.0Recall100Proposalsw/opsBaselinew/opsGCN-Sw/opsGCN-NSw/psBaselinew/psGCN-Sw/psGCN-NS0.500.550.600.650.700.750.800.850.900.951.00IoU0.00.10.20.30.40.50.60.70.80.91.0Recall1000Proposalsw/opsBaselinew/opsGCN-Sw/opsGCN-NSw/psBaselinew/psGCN-Sw/psGCN-NS100101102103NumberofProposals0.00.10.20.30.40.50.60.70.80.91.0AverageRecallCOCOw/opsBaselinew/opsGCN-Sw/opsGCN-NSw/psBaselinew/psGCN-Sw/psGCN-NS0.500.550.600.650.700.750.800.850.900.951.00IoU0.00.10.20.30.40.50.60.70.80.91.0Recall10Proposalsw/opsBaselinew/opsGCN-Sw/opsGCN-NSw/psBaselinew/psGCN-Sw/psGCN-NS0.500.550.600.650.700.750.800.850.900.951.00IoU0.00.10.20.30.40.50.60.70.80.91.0Recall100Proposalsw/opsBaselinew/opsGCN-Sw/opsGCN-NSw/psBaselinew/psGCN-Sw/psGCN-NS0.500.550.600.650.700.750.800.850.900.951.00IoU0.00.10.20.30.40.50.60.70.80.91.0Recall1000Proposalsw/opsBaselinew/opsGCN-Sw/opsGCN-NSw/psBaselinew/psGCN-Sw/psGCN-NS100101102103NumberofProposals0.00.10.20.30.40.50.60.70.80.91.0AverageRecallTrainedonPASCALVOCandTestedonCOCOw/opsBaselinew/opsGCN-Sw/opsGCN-NSw/psBaselinew/psGCN-Sw/psGCN-NS10

H.-F. Lu, X. Du and P.-L. Chang

Table 2. Object proposal results of all models trained on PASCAL VOC 07+12 and
evaluated on 07test

w/o position-sensitive

w/ position-sensitive

AR10 AR100 AR1k AUC AR1k

s AR1k

m AR1k

l AR10 AR100 AR1k AUC AR1k

s AR1k

m AR1k
l

Baseline

.074 .234

.480 .272 .254 .414 .566

.131 .385

.605 .399 .423 .583 .655

.094 .286
Na¨ıve
.103 .325
GCN-S
LK-S
.113 .333
GCN-NS .136 .365
.084 .290
LK-NS

.515 .313 .410 .418 .596
.584 .356 .471 .558 .623
.562 .356 .441 .547 .595
.586 .383 .445 .569 .625
.551 .326 .420 .553 .575

.613 .435 .466 .593 .655
.182 .434
.644 .471 .445 .603 .709
.212 .479
.630 .446 .463 .613 .674
.178 .447
.238 .490 .653 .484 .453 .593 .730
.645 .450 .429 .582 .728
.179 .447

Table 3. Object proposal results of all models trained on COCO trainval35k and
evaluated on minival

w/o position-sensitive

w/ position-sensitive

AR10 AR100 AR1k AUC AR1k

s AR1k

m AR1k

l AR10 AR100 AR1k AUC AR1k

s AR1k

m AR1k
l

Baseline

.083 .208

.421 .242 .308 .562 .392

.034 .165

.448 .219 .385 .411 .566

.082 .294
GCN-S
GCN-NS .079 .277

.542 .322 .414 .558 .680
.532 .310 .422 .552 .643

.579 .321 .485 .592 .677
.075 .270
.096 .316 .607 .358 .493 .623 .726

One may argue that the improvement in GCN-S and GCN-NS models is
due to the increased number of parameters. From Table 2, LK-S and LK-NS
models have also shown some improvement but considering the extra number of
parameters compared with Baseline model, they are not as eﬀective as GCN-S
and GCN-NS models. This shows that using separable convolution kernel mat-
ters, which aligns with the observation in [19]. Na¨ıve model also exhibits similar
results.

4.2 Impact of using position-sensitivity

As shown in Table 2, on PASCAL VOC, among diﬀerent proposal numbers
and object sizes, models using position-sensitive components generally result in
higher AR. Speciﬁcally, for AR1k, Baseline model shows an improvement from
0.480 to 0.605 (26%) and GCN-NS from 0.586 to 0.653 (11%). As shown in Ta-
ble 3, the experiment on COCO shows similar results, in which Baseline model
has an improvement from 0.421 to 0.448 (6%) and GCN-NS model from 0.532
to 0.607 (14%). To investigate on the small object proposals, AR1k
reveal that
s
training on a large number of annotated small objects in COCO indeed helps in
higher AR1k
scores, compared with the results of PASCAL VOC counterpart.
s
GCN-NS has achieved much higher AR1k
(0.493) score, which is a 17% improve-
s
ment compared to the counterpart. Fig. 4 visualises the distribution heatmap
and hit-and-miss of top 1,000 proposals using GCN-NS models with and without
taking position-sensitivity into account, in which the hits are with a threshold 0.7
for the ground truth IoU. One can qualitatively tell that by using the position-

Toward Scale-Inv. and Pos.-Sens. Reg. Propos. Net.

11

sensitivity, models can generate proposals closer to objects and thus result in
more hits, especially for objects with extremely large or small sizes.

4.3 Inference time

Introducing both GCN structure and position-sensitive score maps in the RPN
head brings in more learnable parameters resulting in more computation. Be-
side the input image size which has a directly impact on the overall inference
time, the number of anchors, the kernel size of GCN and the grid number k of
position-sensitive score maps are also key factors. Table 1 lists the models’ infer-
ence times averaged on all input images in both 07test and minival, in which
Baseline model shows a performance of 26.6/58.2 (denoted for 07test/minival)
ms. Adding the position-sensitive score maps (with grid size k = 4) takes extra
9.1/21.3 ms, 9.8/19.8 ms and 8.9/20.1 ms for Baseline, GCN-S and GCN-NS
model respectively, which show a comparable time diﬀerence. In contrast, intro-
ducing the GCN structure in the Baseline model adds 7.7/18.1 ms for GCN-S
model while additional 9.3/25.3 ms for GCN-NS model. This reveals that using
non-shared smoother generally takes more times than using shared smoother
does. GCN-NS with position-sensitivity, as the best performance model, has a
running time 44.8 ms (∼ 22 fps) for 07+12 and 103.6 ms (∼ 10 fps) for minival.

4.4 Model generalisation

To evaluate the generalisation ability of the proposed method, the models trained
on PASCAL VOC 07+12 are used to evaluate on COCO minival. Note that
compared to COCO (80 categories), PASCAL VOC is a much smaller dataset
with limited object categories (20 categories) and almost all categories in PAS-
CAL VOC are included in COCO. We separate the categories of COCO into
two sets: common and non-common. Common categories are ones in PASCAL
VOC, while non-common are unseen categories. In addition, image scenes in
PASCAL VOC are relatively simple with less small objects annotations. It is
therefore more challenging for a model learned from PASCAL VOC to perfor-
mance object proposal on COCO images. The results are shown in Table 4 and
Fig. 3 (Row 3). Surprisingly, the AR1k of the best performance models, GCN-S
and GCN-NS with position-sensitivity, trained with PASCAL VOC 07+12 can
still outperform Baseline model trained with COCO trainval35k (i.e., 0.438 vs.
0.421). It is expected that the model will not work well on small objects since
PASCAL VOC does not contain many small objects, but nevertheless the model
still shows decent performance on large objects in which the AR1k
is up to 0.693
l
as shown in Table 4. The score is comparable to the other models trained on
COCO trainval35k as shown in Table 3. Delving into the details, we show the
breakdown results of the model generalisation experiment for common and non-
common categories are shown in Table 5. All models have better performance on
common categories overall. However, compared to the Baseline model, the pro-
posed components signiﬁcantly improved the performance on both common and
non-common categories. The per-category AUC performance in Fig. 5 shows

12

H.-F. Lu, X. Du and P.-L. Chang

that non-common categories do not necessarily have worse performance than
common categories (e.g., bear, zebra, and toilet). This indicates that the pro-
posed object proposal networks are able to generalise proposals from a smaller
to a larger and more complex dataset, from common to non-common categories,
and that the proposed architecture can further improve the generalisation for
all categories. Fig. 6 qualitatively demonstrates the generalisation ability of the
proposed method. Although the model trained on PASCAL VOC 07+12 fails at
detecting small objectness, it still exhibits a certain degree of generalisation to
unseen categories (e.g., elephant, teddy bear, or ﬁre hydrant).

Table 4. Object proposal results of all models trained on PASCAL VOC 07+12 and
evaluated on COCO minival

w/o position-sensitive

w/ position-sensitive

AR10 AR100 AR1k AUC AR1k

s AR1k

m AR1k

l AR10 AR100 AR1k AUC AR1k

s AR1k

m AR1k
l

Baseline

.031 .097

.234 .122 .114 .234 .382

.061 .218

.400 .240 .224 .401 .614

GCN-S
.053 .185
GCN-NS .066 .200

.390 .217 .240 .430 .524
.390 .228 .239 .420 .538

.438 .288 .227 .463 .665
.104 .277
.118 .282 .438 .292 .235 .432 .693

Table 5. Object proposal results of all models trained on PASCAL VOC 07+12 and
evaluated on COCO minival for common and non-common categories. Note that ( non)
denotes models evaluated on non-common categories

w/o position-sensitive

w/ position-sensitive

AR10 AR100 AR1k AUC AR1k

s AR1k

m AR1k

l AR10 AR100 AR1k AUC AR1k

s AR1k

m AR1k
l

Baseline
Baseline (non)

.046 .129
.013 .055

.285 .156 .180 .316 .369
.170 .079 .035 .139 .402

.076 .271
.043 .151

.483 .294 .329 .506 .629
.295 .171 .099 .280 .590

.061 .217
GCN-S
GCN-S (non)
.042 .144
GCN-NS
.078 .240
GCN-NS (non) .050 .150

.447 .252 .329 .499 .524
.317 .172 .134 .349 .525
.451 .269 .323 .493 .549
.312 .175 .138 .334 .522

.520 .351 .327 .564 .687
.128 .344
.334 .207 .107 .346 .632
.073 .192
.149 .353 .524 .360 .335 .540 .716
.328 .206 .114 .305 .658
.078 .192

5 Conclusions

In this paper, we have proposed object proposal networks based on the observa-
tion that accurate detection relies on translation-invariance for objectness clas-
siﬁcation, translation-variance for localisation as regression and scale-invariance
for various object sizes. Thorough experiments on PASCAL VOC and COCO
datasets have shown that the adoption of global convolutional network (GCN)
and position-sensitivity components can signiﬁcantly improve object proposal
performance while keeping the network lightweight to achieve real-time perfor-
mance.

Toward Scale-Inv. and Pos.-Sens. Reg. Propos. Net.

13

Fig. 4. The impact of position-sensitivity: visualisation on the distribution heatmap
and hit-and-miss of the top 1,000 proposals by GCN-NS models for a number of se-
lected PASCAL VOC 07test (Row 1-2) and COCO minival (Row 3-5) images. For
each pair of images, model without position-sensitivity is on the left and the one with
position-sensitivity is on the right. Col 1-2: The heatmaps are plotted by stacking
the proposal boxes. Col 3-4: The bounding boxes in orange are ground truth boxes
with their corresponding hit proposals in cyan, in which the IoU threshold is set to
0.7, and the bounding boxes in red are missed cases. Row 6-7: All models tend to fail
in images with complex scenes and diverse object aspect ratios. Nonetheless, note that
models with position-sensitivity generally have higher hit rate. Sec. 4.2 for detailed
discussions.

14

H.-F. Lu, X. Du and P.-L. Chang

Fig. 5. Per-category AUC performance of models trained on PASCAL VOC 07+12 and
evaluated on COCO minival. Common and non-common categories are split in the
white and green region respectively.

Fig. 6. The results of model generalisation experiments visualised in the distribution
heatmap and hit-and-miss of the top 1,000 proposals by GCN-NS models for a number
of selected COCO minival. For each pair of COCO minival images, result of model
trained on PASCAL VOC is on the left and the one of model trained on COCO is on
the right. See Sec. 4.4 for detailed discussions.

airplanebicyclebirdboatbottlebuscarcatchaircowdining tabledoghorsemotorcyclepersonbooksheepcouchtraintvtrucktraffic lightfire hydrantstop signparking meterbenchelephantbearzebragiraffebackpackumbrellahandbagtiesuitcasefrisbeeskissnowboardsports ballkitebaseball batbaseball gloveskateboardsurfboardtennis racketwine glasscupforkknifespoonbowlbananaapplesandwichorangebroccolicarrothot dogpizzadonutcakepotted plantbedtoiletlaptopmouseremotekeyboardcell phonemicrowaveoventoastersinkrefrigeratorclockvasescissorsteddy bearhair driertoothbrush0.00.10.20.30.40.50.60.7AUCw/o ps Baselinew/ ps GCN-NSToward Scale-Inv. and Pos.-Sens. Reg. Propos. Net.

15

References

1. Arbel´aez, P., Pont-Tuset, J., Barron, J.T., Marques, F., Malik, J.: Multiscale com-

binatorial grouping. In: CVPR (2014)

2. Bell, S., Lawrence Zitnick, C., Bala, K., Girshick, R.: Inside-outside net: Detecting
objects in context with skip pooling and recurrent neural networks. In: CVPR
(2016)

3. Carreira, J., Sminchisescu, C.: Cpmc: Automatic object segmentation using con-

strained parametric min-cuts. In: TPAMI (2012)

4. Chavali, N., Agrawal, H., Mahendru, A., Batra, D.: Object-proposal evaluation

protocol is ’gameable’. In: CVPR (2016)

5. Dai, J., Li, Y., He, K., Sun, J.: R-FCN: Object detection via region-based fully

convolutional networks. In: NIPS (2016)

6. Everingham, M., Van Gool, L., Williams, C.K., Winn, J., Zisserman, A.: The pascal

visual object classes (VOC) challenge. In: IJCV (2010)

7. Felzenszwalb, P.F., Girshick, R.B., McAllester, D., Ramanan, D.: Object detection

with discriminatively trained part-based models. In: TPAMI (2010)

8. Gidaris, S., Komodakis, N.: Object detection via a multi-region and semantic

segmentation-aware cnn model. In: ICCV (2015)

9. Girshick, R.: Fast r-cnn. In: ICCV (2015)

10. Girshick, R., Donahue, J., Darrell, T., Malik, J.: Rich feature hierarchies for accu-

rate object detection and semantic segmentation. In: CVPR (2014)

11. He, K., Zhang, X., Ren, S., Sun, J.: Spatial pyramid pooling in deep convolutional

networks for visual recognition. In: ECCV (2014)

12. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.

In: CVPR (2016)

13. Hosang, J., Benenson, R., Doll´ar, P., Schiele, B.: What makes for eﬀective detection

proposals? In: TPAMI (2016)

14. Kong, T., Yao, A., Chen, Y., Sun, F.: Hypernet: Towards accurate region proposal

generation and joint object detection. In: CVPR (2016)

15. Kuo, W., Hariharan, B., Malik, J.: DeepBox: Learning objectness with convolu-

tional networks. In: ICCV (2015)

16. Lin, T.Y., Doll´ar, P., Girshick, R., He, K., Hariharan, B., Belongie, S.: Feature

pyramid networks for object detection. In: CVPR (2017)

17. Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Doll´ar, P.,
Zitnick, C.L.: Microsoft COCO: Common objects in context. In: ECCV (2014)
18. Long, J., Shelhamer, E., Darrell, T.: Fully convolutional networks for semantic

segmentation. In: CVPR (2015)

19. Peng, C., Zhang, X., Yu, G., Luo, G., Sun, J.: Large kernel matters–improve se-

mantic segmentation by global convolutional network. In: CVPR (2017)

20. Pinheiro, P.O., Collobert, R., Doll´ar, P.: Learning to segment object candidates.

In: NIPS (2015)

21. Pinheiro, P.O., Lin, T.Y., Collobert, R., Doll´ar, P.: Learning to reﬁne object seg-

ments. In: ECCV (2016)

22. Pont-Tuset, J., Van Gool, L.: Boosting object proposals: From pascal to coco. In:

ICCV (2015)

23. Ren, S., He, K., Girshick, R., Sun, J.: Faster r-cnn: Towards real-time object detec-
tion with region proposal networks. In: Advances in neural information processing
systems (2015)

16

H.-F. Lu, X. Du and P.-L. Chang

24. Ronneberger, O., Fischer, P., Brox, T.: U-net: Convolutional networks for biomed-

ical image segmentation. In: MICCAI (2015)

25. Shrivastava, A., Gupta, A., Girshick, R.: Training region-based object detectors

with online hard example mining. In: CVPR (2016)

26. Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale

image recognition. In: ICLR (2014)

27. Uijlings, J.R., Van De Sande, K.E., Gevers, T., Smeulders, A.W.: Selective search

for object recognition. In: IJCV (2013)

28. Viola, P., Jones, M.J.: Robust real-time face detection. In: IJCV (2004)
29. Zeiler, M.D., Fergus, R.: Visualizing and understanding convolutional networks.

In: ECCV (2014)

30. Zitnick, C.L., Doll´ar, P.: Edge boxes: Locating object proposals from edges. In:

ECCV (2014)

Toward Scale-Invariance and Position-Sensitive
Object Proposal Networks
Supplementary Material

Hsueh-Fu Lu, Xiaofei Du, and Ping-Lin Chang

Umbo Computer Vision Inc.
{topper.lu, xiaofei.du, ping-lin.chang}@umbocv.com
https://umbocv.ai

1 Overview

In this supplementary material, we present more experiment results to comple-
ment the manuscript.

– The number of anchors generated by non-window (i.e., the original RPN
method [6]) and window mapping can end up being very diﬀerent, depend-
ing on the used anchor proﬁle. The proposed window mapping method us-
ing three aspect ratios [4] increases about twice in the number of anchors
compared with the non-window mapping method. One may argue that the
improvement of the proposed method was due to the large number of an-
chors. To investigate the genuine impact of increasing anchor number, we
added two more anchor aspect ratios {1 : 3, 3 : 1} based on the original
proﬁle (i.e., {1 : 1, 1 : 2, 2 : 1}), which in total forms 5 diﬀerent anchors,
for the models without using position-sensitivity. Table S1 shows the anchor
number for window mapping and non-window using three proﬁles and ﬁve
proﬁles. The total number of anchors and the results of average recall (AR)
of the models (5-anchor models marked with †) are reported in Table S2
and Fig. S1. As one can see that simply increasing the number of diﬀerent
anchor proﬁles, and thus to increase the total number of anchors, does not
necessarily improve the overall AR. This means that the position-sensitive
score maps can be an harmonious adoption for the anchors generated by the
window mapping method.

– We conducted a model capacity experiment by training a model on COCO
trainval35k and test it on PASCAL VOC 07test. The results are shown
in Table S3 and Fig. S2. Compared to the models trained on PASCAL VOC
07+12, the GCN-NS models trained on COCO trainval35k can achieve
even higher AR1k scores from 0.653 (see Table 2 in main paper) to 0.678
(4%). The overall improvement can be broken down into the boost in AR1k
s ,
AR1k
l due to more data with diﬀerent object sizes in COCO. This
indicates that the AR results of the proposed method can still be improved
by using more bounding box training data. Fig. S4 (Row 5-7) shows the
corresponding examples. This observation corroborates the assumption that

m and AR1k

18

H.F. Lu, X. Du and P.L. Chang

Fig. S1. Recall against IoU with diﬀerent proposal numbers of 10, 100, 1,000 and
average recall against the number of proposals on PASCAL VOC 07test using models
trained on PASCAL VOC 07+12. Note that models with † use 5 diﬀerent anchor aspect
ratios to generate more anchors.

Fig. S2. Recall against IoU with diﬀerent proposal numbers of 10, 100 and 1,000 and
average recall against the number of proposals on PASCAL VOC 07test using models
trained on COCO trainval35k.

given enough objects from various categories, the classiﬁer can be generalised
to capture the semantic meaning of objectness, which aligns with precious
works [1,2,3,5].

– Fig. S3 displays more examples of the distribution heatmap and hit-and-
miss at top 1,000 proposals using GCN-NS models with and without taking
position-sensitivity into account. The ground truth IoU threshold is set to
0.7, the same as in the manuscript.

– Fig. S4 shows more examples to display model capacity and model general-

isation of our proposed method.

Table S1. The numbers of anchors in each layer with respect to diﬀerent anchor
proﬁles with ﬁxed input size 640 × 640. Note that the default non-window model uses
3 anchor proﬁles, and models with † use 5 diﬀerent anchor aspect ratios to generate
more anchors

non-window non-window† window

D2
D3
D4
D5
D6
Total

76,800
19,200
4,800
1,200
300
102,300

128,000 194,058
32,000 27,803
5,963
1,043
83
170,500 228,950

8,000
2,000
500

0.500.550.600.650.700.750.800.850.900.951.00IoU0.00.10.20.30.40.50.60.70.80.91.0Recall10Proposalsw/opsBaselinew/opsGCN-Sw/opsGCN-NSw/opsBaseline†w/opsGCN-S†w/opsGCN-NS†w/psBaselinew/psGCN-Sw/psGCN-NS0.500.550.600.650.700.750.800.850.900.951.00IoU0.00.10.20.30.40.50.60.70.80.91.0Recall100Proposalsw/opsBaselinew/opsGCN-Sw/opsGCN-NSw/opsBaseline†w/opsGCN-S†w/opsGCN-NS†w/psBaselinew/psGCN-Sw/psGCN-NS0.500.550.600.650.700.750.800.850.900.951.00IoU0.00.10.20.30.40.50.60.70.80.91.0Recall1000Proposalsw/opsBaselinew/opsGCN-Sw/opsGCN-NSw/opsBaseline†w/opsGCN-S†w/opsGCN-NS†w/psBaselinew/psGCN-Sw/psGCN-NS100101102103NumberofProposals0.00.10.20.30.40.50.60.70.80.91.0AverageRecallPASCALVOCw/opsBaselinew/opsGCN-Sw/opsGCN-NSw/opsBaseline†w/opsGCN-S†w/opsGCN-NS†w/psBaselinew/psGCN-Sw/psGCN-NS0.500.550.600.650.700.750.800.850.900.951.00IoU0.00.10.20.30.40.50.60.70.80.91.0Recall10Proposalsw/opsBaselinew/opsGCN-Sw/opsGCN-NSw/psBaselinew/psGCN-Sw/psGCN-NS0.500.550.600.650.700.750.800.850.900.951.00IoU0.00.10.20.30.40.50.60.70.80.91.0Recall100Proposalsw/opsBaselinew/opsGCN-Sw/opsGCN-NSw/psBaselinew/psGCN-Sw/psGCN-NS0.500.550.600.650.700.750.800.850.900.951.00IoU0.00.10.20.30.40.50.60.70.80.91.0Recall1000Proposalsw/opsBaselinew/opsGCN-Sw/opsGCN-NSw/psBaselinew/psGCN-Sw/psGCN-NS100101102103NumberofProposals0.00.10.20.30.40.50.60.70.80.91.0AverageRecallTrainedonCOCOandTestedonPASCALVOCw/opsBaselinew/opsGCN-Sw/opsGCN-NSw/psBaselinew/psGCN-Sw/psGCN-NSToward Scale-Inv. and Pos.-Sens. Reg. Propos. Net.

19

Table S2. Object proposal results of all models trained on PASCAL VOC 07+12 and
evaluated on 07test using diﬀerent anchor proﬁles

3-anchor w/o position-sensitive
s AR1k

AR10 AR100 AR1k AUC AR1k

m AR1k

5-anchor w/o position-sensitive†
s AR1k

l AR10 AR100 AR1k AUC AR1k

m AR1k
l

Baseline

.074 .234

.480 .272 .254 .414 .566

.032 .157

.404 .204 .200 .252 .539

.092 .293
GCN-S
GCN-NS .136 .365 .586 .383 .445 .569 .625 .057 .219

.584 .356 .471 .558 .623

.103 .325

.572 .335 .451 .553 .609
.509 .272 .442 .516 .518

Table S3. Object proposal results of all models trained on COCO trainval35k and
evaluated on PASCAL VOC 07test

w/o position-sensitive

w/ position-sensitive

AR10 AR100 AR1k AUC AR1k

s AR1k

m AR1k

l AR10 AR100 AR1k AUC AR1k

s AR1k

m AR1k
l

Baseline

.108 .301

.605 .352 .442 .607 .636

.054 .248

.564 .301 .499 .501 .616

GCN-S
.134 .407
GCN-NS .122 .376

.666 .427 .513 .604 .735
.651 .404 .518 .602 .707

.119 .348
.648 .390 .538 .595 .703
.158 .410 .678 .437 .550 .625 .736

20

H.F. Lu, X. Du and P.L. Chang

Fig. S3. The impact of position-sensitivity: visualisation on the distribution heatmap
and hit-and-miss at the top 1,000 proposals by GCN-NS models for a number of se-
lected PASCAL VOC 07test (Row 1-4) and COCO minival (Row 5-8) images. For
each pair of images, model without position-sensitivity is on the left and the one with
position-sensitivity is on the right. Col 1-2: The heatmaps are plotted by stacking the
proposal boxes. Col 3-4: The bounding boxes in orange are ground truth boxes with
their corresponding hit proposals in cyan, in which the IoU threshold is set to 0.7, and
the bounding boxes in red are missed cases.

Toward Scale-Inv. and Pos.-Sens. Reg. Propos. Net.

21

Fig. S4. The results of model generalisation and capacity experiments visualised in the
distribution heatmap and hit-and-miss of the top 1,000 proposals by GCN-NS models
for a number of selected COCO minival (Row 1-4) and PASCAL VOC 07test (Row
5-7) images. Row 1-4: For each pair of COCO minival images, the result of model
trained on PASCAL VOC is on the left and the one of model trained on COCO is
on the right. Row 5-7: For each pair of PASCAL VOC 07test images, the result of
model trained on PASCAL VOC is on the left and the one of model trained on COCO
is on the right.

22

H.F. Lu, X. Du and P.L. Chang

References

1. Chavali, N., Agrawal, H., Mahendru, A., Batra, D.: Object-proposal evaluation pro-

tocol is ’gameable’. In: CVPR (2016)

2. Hosang, J., Benenson, R., Doll´ar, P., Schiele, B.: What makes for eﬀective detection

proposals? In: TPAMI (2016)

3. Kuo, W., Hariharan, B., Malik, J.: DeepBox: Learning objectness with convolutional

networks. In: ICCV (2015)

4. Lin, T.Y., Doll´ar, P., Girshick, R., He, K., Hariharan, B., Belongie, S.: Feature

pyramid networks for object detection. In: CVPR (2017)

5. Pinheiro, P.O., Collobert, R., Doll´ar, P.: Learning to segment object candidates. In:

NIPS (2015)

6. Ren, S., He, K., Girshick, R., Sun, J.: Faster r-cnn: Towards real-time object detec-
tion with region proposal networks. In: Advances in neural information processing
systems (2015)

