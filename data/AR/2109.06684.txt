NON-AUTOREGRESSIVE TRANSFORMER WITH UNIFIED BIDIRECTIONAL DECODER
FOR AUTOMATIC SPEECH RECOGNITION

Chuan-Fei Zhang1,2,Yan Liu1,2,∗, Tian-Hao Zhang3, Song-Lu Chen3, Feng Chen3,4, Xu-Cheng Yin3

1School of Automation and Electrical Engineering, University of Science and Technology Beijing, Beijing 100083, China
2 Key Laboratory of Knowledge Automation for Industrial Processes of Ministry of Education, Beijing 100083, China
3 USTB-EEasyTech Joint Lab of Artiﬁcial Intelligence, University of Science and Technology Beijing, Beijing 100083, China
4 EEasy Technology Company Ltd., Zhuhai 519000, China

1
2
0
2

p
e
S
4
1

]
L
C
.
s
c
[

1
v
4
8
6
6
0
.
9
0
1
2
:
v
i
X
r
a

ABSTRACT

Non-autoregressive (NAR) transformer models have been studied
intensively in automatic speech recognition (ASR), and a substan-
tial part of NAR transformer models is to use the casual mask to
limit token dependencies. However, the casual mask is designed
for the left-to-right decoding process of the non-parallel autoregres-
sive (AR) transformer, which is inappropriate for the parallel NAR
transformer since it ignores the right-to-left contexts. Some models
are proposed to utilize right-to-left contexts with an extra decoder,
but these methods increase the model complexity. To tackle the
above problems, we propose a new non-autoregressive transformer
with a uniﬁed bidirectional decoder (NAT-UBD), which can simul-
taneously utilize left-to-right and right-to-left contexts. However,
direct use of bidirectional contexts will cause information leakage,
which means the decoder output can be affected by the character
information from the input of the same position. To avoid informa-
tion leakage, we propose a novel attention mask and modify vanilla
queries, keys, and values matrices for NAT-UBD. Experimental re-
sults verify that NAT-UBD can achieve character error rates (CERs)
of 5.0%/5.5% on the Aishell1 dev/test sets, outperforming all previ-
ous NAR transformer models. Moreover, NAT-UBD can run 49.8×
faster than the AR transformer baseline when decoding in a single
step.

Index Terms— automatic speech recognition, transformer, non-

autoregressive, bidirectional contexts, information leakage.

1. INTRODUCTION

Recently, transformer models [1, 2] based on encoder-decoder have
shown superior performance in end-to-end automatic speech recog-
nition (ASR) compared with Recurrent Neural Networks (RNNs)
[3, 4] and Connectionist Temporal Classiﬁcation (CTC) [5]. Deﬁ-
ciently, most transformer models predict the next token conditioning
on encoded states and previously generated tokens in an autoregres-
sive (AR) manner, resulting in slow decoding speed.

To accelerate the decoding speed, non-autoregressive (NAR)
transformer models [6, 7, 8] are ﬁrst proposed in machine trans-
lation, which can predict multiple tokens simultaneously and have
been widely studied in ASR recently. To our best knowledge, NAR
transformer models in ASR can be roughly divided into two cate-
gories according to the decoder. The ﬁrst kind of NAR transformer
model [9, 10] regards the decoder as an acoustic model. However,
such NAR transformer models follow the conditional independence
hypothesis between the output tokens and suffer inferior recogni-
tion performance. The second kind of NAR transformer model

* Corresponding author.

Fig. 1: (a) unidirectional decoder [13, 14] with left-to-right contexts
only; (b) unidirectional decoder [16, 17] with right-to-left contexts
only; (c) uniﬁed bidirectional decoder with right-to-left contexts and
left-to-right contexts simultaneously. The grey boxes denote trans-
former decoder, and the dashed lines denote token dependencies.

[11, 12, 13, 14] regards the decoder as a language model, and the
decoder can predict output conditioning on linguistic information.
Notably, the attention mask is widely used in these NAR trans-
formers to limit token dependencies. Especially, the casual mask
proposed in the AR transformer [15] is used in the second kind of
NAR transformers [13, 14] to construct a unidirectional decoder
(Fig. 1 (a)). However, it is inappropriate for NAR transformer mod-
els to use the casual mask. Firstly, the casual mask is designed for
the serial decoding process of the AR transformer while the decod-
ing process of the NAR transformer is parallel. Secondly, the casual
mask only uses left-to-right (L2R) contexts, resulting in discarded
right-to-left (R2L) contexts.

Previously, R2L contexts (Fig. 1 (b)) have been studied in the
AR transformer [16] and the streaming ASR [17]. These models are
composed of one shared encoder and two unidirectional decoders,
i.e., two separate decoders with L2R and R2L contexts, respectively.
Such a framework is complex and inefﬁcient because it needs an ex-
tra unidirectional decoder and the two decoders have no information
exchange.

To tackle the above problems, we propose a new non-autoregre-
ssive transformer with a uniﬁed bidirectional decoder (NAT-UBD),
which can fully utilize both L2R and R2L contexts in a uniﬁed de-
coder (Fig. 1 (c)). However, direct use of bidirectional contexts
will cause information leakage. Concretely, information leakage
means the decoder output can be affected by the character infor-
mation from the input of the same position, and the decoder can
not reﬁne the input during decoding. Since the proposed NAT-UBD
is based on the speech transformer [1], the residual connection and
self-attention mechanism are adopted, and both of them can cause

y2y3y1y1y2y3y2y3y1SOSy1y2y3EOSy2y3y1SOSy1y2y3EOS(c) (b) (a)  
 
 
 
 
 
Fig. 2: The overall architecture of NAT-UBD. First, the input acous-
tic features are downsampled by convolutional subsampling layers
and encoded by the encoder. Then the encoded states and character
sequence are fed into UBD. Finally, UBD predicts all tokens simul-
taneously.

information leakage. To avoid information leakage resulting from
the residual connection, we remove word embedding from vanilla
queries matrix (Q). To avoid information leakage resulting from the
self-attention mechanism, we propose a novel attention mask named
self mask and make both keys matrix (K) and values matirx (V ) in-
dependent of layers, similar to the Disco transformer [18]. This way,
NAT-UBD can outperform all previous NAR transformer models on
the Aishell1 corpus and achieve competitive performance compared
with the AR transformer baseline on the Magicdata corpus. More-
over, NAT-UBD can run much faster than the AR transformer base-
line because UBD can predict all tokens simultaneously.

2. METHODOLOGY

The proposed NAT-UBD can fully utilize both L2R and R2L con-
texts in a uniﬁed decoder. Fig. 2 illustrates the overall architecture of
NAT-UBD. For simplicity, we omit the feed-forward layers and layer
normalization [19]. The convolutional subsampling layers and en-
coder of NAT-UBD are the same as the speech transformer [1], while
the decoder is our proposed uniﬁed bidirectional decoder (UBD).

2.1. Uniﬁed Bidirectional Decoder

UBD takes the character sequence Y as the input and predicts all to-
kens simultaneously conditioning on bidirectional contexts Y(cid:54)=t and
encoded states S, as described in Eq (1).

yt = U BD(Y(cid:54)=t, S)

1 ≤ t ≤ T,

(1)

Fig. 3:
Illustration of information leakage. Both residual con-
nections and multi-head self-attention layers can cause information
leakage. (a) residual connections; (b) indirect attention connections;
(c) direct attention connections. Blue boxes denote multi-head self-
attention layers.

where T is the length of Y . Besides, Y(cid:54)=t represents that the out-
put yt should prevent from utilizing the character information from
the input of the same position. Otherwise, information leakage will
arise.

In the vanilla transformer, the character sequence Y is ﬁrst trans-
formed to QKV s by the character embedding C and positional en-
coding P , as described in Eq (2).

Q1, K 1, V 1 = Linear(C(Y ) + P )

(2)

For simplicity, we use Linear() to represent different linear
layers. This way, QKV s of the ﬁrst multi-head self-attention layer
contain the character information and are transmitted by residual
connections and multi-head self-attention layers. However, both
residual connections (Fig. 3 (a)) and multi-head self-attention layers
(Fig. 3 (b)(c)) can cause information leakage.

To avoid information leakage caused by residual connections,
all queries of Q should not contain the character information of the
current position. So we remove the character embedding and calcu-
late the queries matrix of the ﬁrst multi-head self-attention layer, as
described in Eq (3).

ˆQ1 = Linear(P ),

(3)

where ˆQ represents the queries matrix of NAT-UBD. However, KV s
can not be modiﬁed like ˆQ since the character information must be
retained and utilized in UBD. An alternative method is to remove
residual connections for KV s. Nevertheless, removing residual con-
nections is not enough since the attention connections in multi-head
self-attention layers will also cause information leakage.

To avoid the information leakage caused by indirect attention
connections (Fig. 3 (b)), all keys and values in multi-head self-
attention layers should only contain the character information of
the current position. So we feed the same keys and values matrics

EncoderUnified BidirectionalDecoder(UBD)Multi-headSelf-attentionMulti-headSource-attentionMulti-headSelf-attentionMulti-headSource-attentionCharacterEmbedding. . .ConvolutionSubsamplingAcousticFeaturesOutputSequenceQˆKˆVˆQˆQˆQˆKˆKˆKˆVˆVˆVˆCharacterSequenceEncodedStatesEncodedStatesPositionalEncoding(a)(c)(b). . .. . .(a)(b)(c)y2CharacterEmbeddingPositionalEncodingPositionalEncodingCharacterEmbeddingy1y1y2independent of the previous layers into all multi-head self-attention
layers, as described in Eq (4).

ˆK i, ˆV i = Linear(C(Y ) + P ) 1 ≤ i ≤ I,

(4)

where ˆK and ˆV are keys matrix and values matrix of NAT-UBD.

Besides, I is the number of multi-head self-attention layers.

To avoid the information leakage caused by direct attention con-
nections (Fig. 3), the diagonal elements of the attention weight ma-
trix should be set to 0. Hence, we propose an attention mask named
self mask to multiply with the attention weight matrix. Concretely,
the self mask is an attention mask matrix of which diagonal elements
are 0, and all the other elements are 1. In addition, the self mask also
makes UBD can utilize bidirectional contexts simultaneously.

2.2. Joint Training

To enable the encoder to output preliminary results for UBD during
decoding, we apply the CTC loss [5] to the encoder. Besides, we use
the label sequence as the input character sequence of UBD. Then
NAT-UBD can be jointly trained with both the CTC loss LCT C and
UBD loss LU BD, as described in Eq (5).

L = λLCT C + (1 − λ)LU BD,

(5)

where λ is a hyperparameter to balance two losses. LU BD is the
cross entropy loss [20].

2.3. Decoding by Iterative Reﬁnement and Early Termination

To achieve fast decoding speed, UBD takes the greedy CTC outputs
to substitute the label sequence as the initial input. All decoder in-
puts can use bidirectional contexts to predict, and the whole output
sequence can be predicted by iterative reﬁnement in J iterations, as
described in Eq (6).

(cid:40)

ˆyj
t =

U BD( ˆY j−1,CT C
(cid:54)=t
U BD( ˆY j−1,U BD
(cid:54)=t

, S)
, S)

j = 1,
1 < j ≤ J,

(6)

where ˆY j−1,CT C is the greedy CTC outputs, and ˆY j−1,U BD is the
greedy UBD outputs.

Moreover, we propose a simple but effective stop method named
adaptive termination for NAT-UBD to accelerate decoding speed.
When the output of the jth iteration is the same as the (j − 1)th
iteration, the iteration can be early terminated because the output of
subsequent iteration will remain unchanged.

3. EXPERIMENTS

3.1. Datasets

The experiments are carried out on the 178-hour Aishell1 [21] Man-
darin corpus and 755-hour Magicdata 1 Mandarin corpus. For the
input acoustic features, we extract 80-channel ﬁlterbanks features
splice 3-channel pitch computed from a 25ms window with a stride
of 10ms. The output labels consist of 4231 Chinese characters for
Aishell1 and 4518 Chinese characters for Magicdata, obtained from
the training set.

1 Beijing Magic Data Co., Ltd. www.magicdatatech.com/

Table 1: Character error rate (CER) and real time factor (RTF) on
the Aishell1 corpus. † means SpecAugment is used, and ‡ means
Speed Perturbation is used. J is the maximum number of iterations,
and T is the length of the output sequence.

Model

J

Dev

Test

RTF

Speedup

AR transformer

Transformer [22]‡

T

6.0

6.7

NAR transformer

LASO-small [12]†‡
ST-NAR [9]‡
KERMIT [28]‡
InDIGO [28]‡
CASS-NAT [10]†‡
CTC-enhanced [13]†‡
A-FMLM [29]‡
TSNAT-small [14]‡

AR transformer†‡
NAT-UBD†‡
NAT-UBD†‡

6.0
6.9
6.7
6.0
5.3
5.3
6.2
5.4

1
1
1
1
1
1
1
1
Our work

T
1
10

5.2
5.1
5.0

6.8
7.7
7.5
6.7
5.8
5.9
6.7
5.9

5.6
5.6
5.5

−

−
−
−
−
−
−
−
−

−

−
−
−
−
−
−
−
−

0.4034
0.0081
0.0116

1.00×
49.8×
34.8×

Table 2: Character error rate (CER) and real time factor (RTF) on
the Magicdata corpus. † means SpecAugment is used.

Model

AR transformer†
NAT-UBD†
NAT-UBD†

J

T
1
10

Test

Dev
Our work

RTF

Speedup

4.1
4.4
4.2

4.6
4.9
4.7

0.4703
0.0095
0.0121

1.00×
49.5×
38.9×

3.2. Experimental Setup

We use ESPNet [22] for all experiments. The convolutional sub-
sampling module is comprised of 2 CNN layers with size 3×3, ﬁlter
256, stride 2 on the time dimension for 4× down-sampling. Then
12 encoder layers and 6 decoder layers are stacked. We use 256
dimensions for ˆQ ˆK ˆV s and 4 attention heads for all multi-head at-
tention layers. We set 2048 dimensions for the position-wise feed-
forward networks and use ReLU activation for the hidden layer. La-
bel smoothing [23] with a penalty of 0.1 is applied to prevent over-
ﬁtting. We use Adam [24] optimizer and warm up with β1 = 0.9,
β2 = 0.98, and (cid:15) = 10−9. All models are trained on 2 Titan X GPUs
with batch size 32. Gradients are accumulated [25] over 4 iterations.
SpecAugment [26] is used for data augmentation, and Speed Pertur-
bation [27] is additionally used for the Aishell1 corpus. We use the
dev set for early stopping. We choose the best models of 10 epochs
with the lowest accuracies on the dev set and average them to get
the ﬁnal model. All decoding processes are performed utterance by
utterance on a Titan X GPU without any external language model.
Character error rate (CER), character error rate reduction (CERR),
and real time factor (RTF) are adopted for model evaluation. RTF is
computed as the ratio of the total decoding time to the total duration
of the test set.

3.3. Results

Except for NAT-UBD, we reimplement the AR transformer baseline
with CTC joint training and joint decoding [30]. Especially, beam
search with a width of 10 is used for the AR transformer baseline. As
shown in Table 1, our proposed NAT-UBD achieves the best CERs
than all previous NAR transformer models with different iterations

Table 3: Ablation study of ˆQ ˆK ˆV s and self mask on the Aishell1
corpus. J = 0 means that the greedy CTC output is directly used as
the ﬁnal output.

Table 5: Comparison of the left-to-right (L2R) decoder, right-to-
left (R2L) decoder and uniﬁed bidirectional decoder (UBD) on the
Aishell1 corpus and Magicdata corpus.

Model

NAT-UBD

− ˆQ ˆK ˆV s

−self mask

−both

J

0
10
0
10
0
10
0
10

Dev

Test

CER CERR
5.5
5.0
12.7
12.7
11.1
11.1
10.6
10.6

−
9.1
−
0.0
−
0.0
−
0.0

CER CERR
6.0
5.5
13.6
13.6
12.6
12.6
11.3
11.3

−
8.3
−
0.0
−
0.0
−
0.0

Decoder

L2R

R2L

L2R+R2L

UBD

J

1
10
1
10
1
10
1
10

Aishell1

Magicdata

Dev
5.4
5.5
5.4
5.6
5.6
5.6
5.1
5.0

Test
5.9
5.9
5.9
6.2
6.2
6.3
5.6
5.5

RTF
0.0081
0.0108
0.0081
0.0103
0.0077
0.0091
0.0081
0.0116

Dev
4.7
4.7
4.7
4.7
4.8
4.8
4.4
4.2

Test
5.0
5.0
5.0
5.1
5.2
5.2
4.9
4.7

RTF
0.0095
0.0115
0.0095
0.0110
0.0090
0.0101
0.0095
0.0121

Table 4: Ablation study of adaptive termination on the Aishell1 cor-
pus.

Model
NAT-UBD
−adatpive termination

J
10
10

Dev
5.0
5.0

Test
5.5
5.5

RTF
0.0116
0.0263

and can outperform the AR transformer baseline with a 49.8× faster
decoding speed on the Aishell1 corpus.

We conduct experiments on the Magicdata corpus. Magicdata is
a large Mandarin corpus, and to our best knowledge, we are the ﬁrst
to assess the NAR transformer models on this corpus. From Table
2, we can see that NAT-UBD can achieve competitive CERs with the
AR transformer baseline while maintaining faster decoding speed,
proving the generalizability of NAT-UBD.

3.4. Necessity of Avoiding Information Leakage
We replace ˆQ ˆK ˆV s with vanilla QKV s and self mask with padding
mask, respectively. The padding mask is an attention mask that only
masks the attention connections of padded tokens in a mini-batch
and is widely used in transformer models [11, 12, 13, 14].

After removing ˆQ ˆK ˆV s and self mask all or separately, the ac-
curacies on the dev set approach 100% quickly, and the training pro-
cesses are stopped because of early stopping, resulting in few train-
ing epochs and poor CTC performance. However, we only focus on
the CERR between the greedy CTC output and decoder output. As
shown in Table 3, except for NAT-UBD, the decoder outputs of the
other three models are the same as the greedy CTC output, indicat-
ing that these three decoders have learned identity mapping between
input and output during training. The experimental phenomena ver-
ify that information leakage during training can damage the network
performance, proving both the ˆQ ˆK ˆV s and self mask should be used.

3.5. Effectiveness of Adaptive Termination

We remove the adaptive termination from NAT-UBD during decod-
ing. From Table 4, we can conclude that the adaptive termination
can improve the decoding speed more than 2× times with J = 10
while keeping CERs unchanged.

3.6. Effectiveness of Uniﬁed Bidirectional Decoder

We verify the effectiveness of UBD by replacing UBD with the L2R
decoder and R2L decoder. For fairness, when using L2R and R2L
decoders simultaneously, each decoder has 3 decoder layers, and the

greedy output with the higher average probability is chosen as the
ﬁnal output. In addition, adaptive termination is used during decod-
ing.

As shown in Table 5, UBD achieves the best CERs on two cor-
pora with J = 1. With J = 10, the CERs of UBD can be further
reduced on two corpora while other methods remain unchanged or
even worse. In cases with L2R and R2L decoders simultaneously,
the decoding speed is faster than other methods due to the paral-
lel decoding of two decoders. However, this method yields worse
CERs than only using a unidirectional decoder. It is because the two
decoders have no information exchange, which means their outputs
only depend on unidirectional contexts. Moreover, both decoders
only have 3 decoder layers, which can reduce their respective fea-
ture extraction ability. As a result, we can conclude that the pro-
posed UBD is efﬁcient and can use ample linguistic information for
character prediction.

However, the RTF of UBD is higher than other methods both
on Aishell1 and Magicdata corpora with J = 10. When using
UBD, we observe that the cyclic dependency of adjacent tokens of-
ten arises, making adaptive termination invalid and reducing the de-
coding speed. For example, the ground truth is “stand up”, and the
output of the ﬁrst iteration is “stand down”. Then the outputs of the
second and third iterations might be “sit up” and “stand down”. As
a result, the iteration can not stop until ten iterations. Especially,
cyclic dependency arises with increasing frequency when the pho-
netic pronunciation is similar.

4. CONCLUSION

We propose a new non-autoregressive transformer with a uniﬁed
bidirectional decoder (NAT-UBD), carefully designed to simultane-
ously utilize left-to-right and right-to-left contexts and prevent con-
sequent information leakage. As a result, the proposed NAT-UBD
outperforms all previous NAR transformer models on the Aishell1
corpus and achieves competitive performance with AR transformer
on the Magicdata corpus without any external language model. For
the decoding speed, NAT-UBD can run 49.8× faster than the AR
transformer baseline. Further analysis experiments prove the effec-
tiveness of the uniﬁed bidirectional decoder and the necessity of
avoiding information leakage. We plan to reduce exposure error
from feeding ground truth to the decoder during training and greedy
CTC output during decoding.

[16] Xi Chen, Songyang Zhang, Dandan Song, Peng Ouyang, and
Shouyi Yin,
“Transformer with bidirectional decoder for
speech recognition,” in INTERSPEECH, 2020, pp. 1773–1777.

[17] Di Wu, Binbin Zhang, Chao Yang, Zhendong Peng, Wen-
“U2++: Uniﬁed two-pass bidirectional
arXiv preprint

jing Xia, et al.,
end-to-end model for speech recognition,”
arXiv:2106.05642, 2021.

[18] Jungo Kasai, James Cross, Marjan Ghazvininejad, and Jiatao
Gu, “Non-autoregressive machine translation with disentan-
gled contexts transformer,” in ICML, 2020, pp. 5144–5155.

[19] Lei Jimmy Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton,
“layer normalization,” arXiv preprint arXiv:1607.06450, 2016.

[20] Pieter-Tjerk de Boer, Dirk P.Kroese, Shie Mannor, and Reuven
Y.Rubinstein, “A tutorial on the cross-entropy method,” Annals
of operations research, vol. 134, no. 1, pp. 19–67, 2005.

[21] Hui Bu, Jiayu Du, Xingyu Na, Bengu Wu, and Hao Zheng,
“Aishell-1: An open-source mandarin speech corpus and as-
peech recognition baseline,” in O-COCOSDA, 2017, pp. 1–5.

[22] Shinji Watanabe, Takaaki Hori, Shigeki Karita, Tomoki
Hayashi, Jiro Nishitoba, Yuya Unno, et al., “Espnet: end-to-
end speech processing toolkit,” in ICASSP, 2018, pp. 2207–
2211.

[23] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon
Shlens, and Zbigniew Wojna, “Rethinking the Inception archi-
tecture for computer vision,” in CVPR, 2016, pp. 2818–2826.

[24] Diederik P. Kingma and Jimmy Ba, “Adam: A method for

stochastic optimization,” in ICLR, 2015.

[25] Myle Ott, Sergey Edunov, David Grangier, and Michael Auli,
“Scaling neural machine translation,” in WMT, 2018, pp. 1–9.

[26] Daniel S. Park, William Chan, Yu Zhang, Chung-Cheng Chiu,
Barret Zoph, et al., “Specaugment: A simple data augmen-
tation method for automatic speech recognition,” in INTER-
SPEECH, 2019, pp. 2613–2617.

[27] Tom Ko, Vijayaditya Peddinti, Daniel Povey, and Sanjeev Khu-
danpur, “Audio augmentation for speech recognition,” in IN-
TERSPEECH, 2015, pp. 3586–3589.

[28] Yuya Fujita, Shinji Watanabe, Motoi Omachi, and Xuankai
Chang, “Insertion-based modeling for end-to-end automatic
speech recognition,” in INTERSPEECH, 2020, pp. 3660–3664.

[29] Nanxin Chen, Shinji Watanabe, Jes´us Villalba, and Na-
“Listen and ﬁll in the missing letters: Non-
arXiv

jim Dehak,
autoregressive transformer for speech recognition,”
preprint arXiv:1911.04908, 2020.

[30] Shigeki Karita, Nelson Enrique Yalta Soplin, Shinji Watanabe,
Marc Delcroix, Atsunori Ogawa, and Tomohiro Nakatani, “Im-
proving transformer-based end-to-end speech recognition with
connectionist temporal classiﬁcation and language model inte-
gration,” in INTERSPEECH, 2020, pp. 1408–1412.

5. REFERENCES

[1] Linhao Dong, Shuang Xu, and Bo Xu, “Speech-Transformer:
a no-recurrence sequence-to-sequence model for speech recog-
nition,” in ICASSP, 2018, pp. 5884–5888.

[2] Yingzhu Zhao, Chongjia Ni, Cheung-Chi Leung, Joty Shaﬁq,
Eng Siong Chng, and Bin Ma,
“Speech transformer with
speaker aware persistent memory,” in INTERSPEECH, 2020,
pp. 1261–1265.

[3] Chung-Cheng Chiu, Tara N. Sainath, Yonghui Wu, Rohit Prab-
havalkar, Patrick Nguyen, et al.,
“State-of-the-art speech
recognition with sequence-to-sequence models,” in ICASSP,
2018, pp. 4774–4778.

[4] Bo Li, Shuo-Yiin Chang, Tara N. Sainath, Ruoming Pang,
Yanzhang He, et al., “Towards fast and accurate streaming
end-to-end asr,” in ICASSP, 2020, pp. 6069–6073.

[5] Alex Graves, Santiago Fern´andez, Faustino J. Gomez, and
J¨urgen Schmidhuber, “Connectionist temporal classiﬁcation:
labelling unsegmented sequence data with recurrent neural net-
works,” in ICML, 2006, pp. 369–376.

[6] Jiatao Gu, James Bradbury, Caiming Xiong, Victor O. K. Li,
“Non-autoregressive neural machine

and Richard Socher,
translation,” in ICLR, 2018.

[7] Marjan Ghazvininejad, VladimirLuke Karpukhin, Luke Zettle-
moyer, and Omer Levy,
“Aligned cross entropy for non-
autoregressive machine translation,” in ICML, 2020, pp. 3515–
3523.

[8] Jason Lee, Elman Mansimov, and Kyunghyun Cho, “Deter-
ministic non-autoregressive neural sequence modeling by iter-
ative reﬁnement,” in EMNLP, 2018, pp. 1173–1182.

[9] Zhengkun Tian, Jiangyan Yi, Jianhua Tao, Ye Bai, Shuai
Zhang, et al., “Spike-triggered non-autoregressive transformer
for end-to-end speech recognition,” in INTERSPEECH, 2020,
pp. 5026–5030.

[10] Ruchao Fan, Wei Chu, Peng Chang, and Jing Xiao, “CASS-
NAT: CTC alignment-based single step non-autoregressive
in ICASSP, 2021, pp.
transformer for speech recognition,”
5889–5893.

[11] Yosuke Higuchi, Shinji Watanabe, Chen Nanxin, Tetsuji
Ogawa, et al., “Mask CTC: non-autoregressive end-to-end asr
with CTC and Mask predict,” in INTERSPEECH, 2020, pp.
3655–3659.

[12] Ye Bai, Jiangyan Yi, Jianhua Tao, Zhengkun Tian, Zhengqi
Wen, et al., “Listen attentively, and spell once: Whole sen-
tence generation via a non-autoregressive architecture for low-
in INTERSPEECH, 2020, pp.
latency speech recognition,”
3381–3385.

[13] Xingchen Song, Zhiyong Wu, Yiheng Huang, Chao Weng, Dan
“Non-autoregressive transformer asr with CTC-

Su, et al.,
Enhanced decoder input,” in ICASSP, 2021, pp. 5894–5898.

[14] Zhengkun Tian, Jiangyan Yi, Jianhua Tao, Ye Bai, Shuai
Zhang, Zhengqi Wen, and Xuefei Liu, “TSNAT: two-step non-
autoregressvie transformer models for speech recognition,”
arXiv preprint arXiv:2104.01522, 2021.

[15] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
Llion Jones, et al., “Attention is all you need,” in NIPS, 2017,
pp. 5998–6008.

