Sentence Semantic Regression for Text Generation

Wei Wang1,‚àó‚àó, Piji Li2, Hai-Tao Zheng1
1Shenzhen International Graduate School, Tsinghua University
2Tencent AI Lab
w-w16@mails.tsinghua.edu.cn
lipiji.pz@gmail.com
zheng.haitao@sz.tsinghua.edu.cn

1
2
0
2

g
u
A
6

]
L
C
.
s
c
[

1
v
4
8
9
2
0
.
8
0
1
2
:
v
i
X
r
a

Abstract

Recall the classical text generation works, the
generation framework can be brieÔ¨Çy divided
into two phases: idea reasoning and surface
realization. The target of idea reasoning is to
Ô¨Ågure out the main idea which will be pre-
sented in the following talking/writing peri-
ods. Surface realization aims to arrange the
most appropriate sentence to depict and con-
vey the information distilled from the main
idea. However,
the current popular token-
by-token text generation methods ignore this
crucial process and suffer from many se-
rious issues, such as idea/topic drift.
To
tackle the problems and realize this two-
phase paradigm, we propose a new frame-
work named Sentence Semantic Regression
(SSR) based on sentence-level language mod-
eling. For idea reasoning, two architectures
SSR-AR and SSR-NonAR are designed to
conduct sentence semantic regression autore-
gressively (like GPT2/3) and bidirectionally
(like BERT). In the phase of surface realiza-
tion, a mixed-granularity sentence decoder is
designed to generate text with better consis-
tency by jointly incorporating the predicted
sentence-level main idea as well as the pre-
ceding contextual token-level information. We
conduct experiments on four tasks of story
ending prediction, story ending generation, di-
alogue generation, and sentence inÔ¨Ålling. The
results show that SSR can obtain better perfor-
mance in terms of automatic metrics and hu-
man evaluation.

1

Introduction

The past few years have seen tremendous progress
in the area of text generation, such as dialogue
generation (Shang et al., 2015; Wang et al., 2020;
Zhang et al., 2020c; Roller et al., 2020; Adi-
wardana et al., 2020), abstractive summariza-
tion (Rush et al., 2015; Zhang et al., 2020a),

‚àóWork was done during internship at Tencent AI Lab.

Figure 1: Text producing process.
story generation (Chaturvedi et al., 2017; Fan
et al., 2018; Zhou et al., 2019; See et al., 2019;
Tan et al., 2020), image/video captioning (Vinyals
et al., 2016; You et al., 2016), neural machine
translation (Bahdanau et al., 2015; Vaswani et al.,
2017; Gu et al., 2017), etc. A large number of
researchers have been attracted to this area to Ô¨Åx
the potential problems and improve the perfor-
mance. More recently, the large generative pre-
trained language models, such as GPT2/3 (Rad-
ford et al., 2019; Brown et al., 2020), have greatly
improved the quality of text generation and can be
used to solve different text generation tasks simul-
taneously in a uniÔ¨Åed framework.

Let us recall those pioneer classical text genera-
tion works (Mann, 1983; McKeown, 1992; Reiter
and Dale, 1997; Gatt and Krahmer, 2018), where,
generally and brieÔ¨Çy, the process of text genera-
tion can be divided into two phases, as depicted in
Figure 1: idea reasoning and surface realization.
The target of ‚Äúidea reasoning‚Äù is to Ô¨Ågure out the
main idea/topic which will be presented in the fol-
lowing talking/writing periods. The main idea can
be independent, or dependent on the context such
as the last conversation utterance. After careful
consideration, we obtain the main idea Z. Then,
the objective of the second phase is to arrange the
most appropriate surface symbol sequence to de-
pict and convey the information distilled from the
main idea, which is named ‚Äúsurface realization‚Äù.
Usually, the surface symbol sequence could be one

ùëçùëã3SentencesIdeaSurface RealizationReasoningùëã1ùëã2 
 
 
 
 
 
kind of natural languages, e.g., English.

More importantly, the results of surface realiza-
tion for the same idea Z can be different. We may
jointly consider many factors to select the most
appropriate result for the current scenario. The
factors could be relevance, discourse consistency,
coreference, ethical issues, etc. After those two
phases, the best sentence used to present the main
Indeed, the above-mentioned
idea is produced.
two-phase framework is a natural and crucial man-
ner to conduct the problem of text generation.
Therefore, in this work, we dedicate to investigat-
ing the technical solutions to realize the two-phase
text generation framework.

Let us rethink the current popular text genera-
tion techniques. However, during the investiga-
tions, we Ô¨Ånd that most of the text generation ap-
proaches, such as the pre-trained auto-regressive
language models GPT2/3 (Radford et al., 2019;
Brown et al., 2020) as well as the typical decoders
used in the seq2seq frameworks, are all trained in
the paradigm of token-level language model (here,
token can be BPE (Sennrich et al., 2016) token).
During the inference stage, sentences are gener-
ated in a manner of token-by-token, autoregres-
sively. Even though it has been proved that this
paradigm can generate convincing text with high
Ô¨Çuency and diversity, they are still suffering from
many serious issues such as repetition (Holtz-
man et al., 2019; Welleck et al., 2019; Fu et al.,
2020), hallucinations (Nie et al., 2019; Maynez
et al., 2020), illogical issues (Floridi and Chiriatti,
2020), etc. Therefore, we claim that token-level
generation models cannot capture the high-level
semantic information between sentences. And in
surface realization, the optimization objective at
the token level focuses more on the neighbor to-
ken consistency instead of idea/topic consistency.
Moreover, for some open-ended text generation
tasks such as story generation (Fan et al., 2018;
See et al., 2019), due to the sampling-based de-
coding strategy (such as top-k (Fan et al., 2018)
and top-p (Holtzman et al., 2019)), idea/topic drift
easily occurs, then the generated sentence cannot
depict the main idea/topic faithfully. Actually, for
text producing, humans will get the main idea Ô¨Årst
(idea reasoning), and then translate it using a to-
ken sequence (surface realization), rather than
produce tokens one-by-one by looking back at the
previous tokens.

To tackle the above-mentioned problem, to real-

ize the two-phase text generation framework, we
explore and propose a new generation paradigm
called Sentence Semantic Regression (SSR). Our
approach consists of three components: a sen-
tence encoder, a sentence-level language model
(for idea reasoning), and a mixed-granularity sen-
tence decoder (for surface realization).
In the
idea reasoning phase, the sentence encoder is used
to convert the sentence into a semantic vector. And
the sentence-level language model conducts the
sentence semantic regression on the obtained sen-
tence vector sequence and then predicts the target
sentence vector which is regarded as the main idea
to be described. In the surface realization phase,
the mixed-granularity decoder generates the sen-
tence token by token mainly conditioning on the
predicted sentence vector.
In order to introduce
token-level consistency between sentences, tokens
from the previous sentences are also used as the
context of the decoder. Inspired by the token-level
autoregressive language models (GPT2/3 (Rad-
ford et al., 2019; Brown et al., 2020)) and bidirec-
tional non-autoregressive masked language mod-
els (BERT (Devlin et al., 2019)), we also design
two sentence-level language models, named SSR-
AR (like GPT2/3) and SSR-NonAR (like BERT),
to conduct idea reasoning autoregressively or bidi-
rectionally. We conduct experiments on the tasks
of story ending prediction, story ending genera-
tion, dialogue generation, and sentence inÔ¨Ålling.
The results show that our proposed framework
SSR can obtain better performance in terms of au-
tomatic metrics and human evaluation.

In summary, our contributions are as follows:

‚Ä¢ Text generation generally consists of

two
phases: idea reasoning and surface realization.
To realize this two-phase paradigm, we pro-
pose a new framework named Sentence Seman-
tic Regression (SSR) based on sentence-level
language modeling.

‚Ä¢ For idea reasoning, two architectures SSR-AR
and SSR-NonAR are designed to conduct sen-
tence semantic regression autoregressively and
bidirectionally.

‚Ä¢ For surface realization, a mixed-granularity sen-
tence decoder is designed to generate text with
better consistency by jointly incorporating the
predicted sentence-level main idea as well as the
tokens of the previous context.

‚Ä¢ Experimental results on four different

tasks
demonstrate the effectiveness of our framework.

Figure 2: Illustration of our proposed framework SSR: Sentence Encoder, SSR-AR, SSR-NonAR, and Mixed-
Granularity Sentence Decoder.

2 The Proposed SSR Framework

2.1 Overview

We denote a sentence as X = (x1, x2, . . . , xT ),
where xi represents the ith token, T is the sen-
tence length. A set of sentences is denoted as
X = (X1, X2, . . . , XN ). Our goal is to gener-
ate the last sentence XN given the previous N ‚àí 1
sentences (next sentence generation) or to generate
the sentence Xi given the other surrounding sen-
tences (sentence inÔ¨Ålling). As shown in Figure 2,
our Sentence Semantic Regression (SSR) frame-
work consists of three components: sentence en-
coder, sentence-level language model, and mixed-
granularity sentence decoder. We Ô¨Årst use the
sentence encoder to convert sentences into cor-
responding sentence vectors. Then the sentence-
level language model conducts language model-
ing on sentence vector sequences and learns to
predict sentence vectors to represent semantic in-
formation about the text to be generated. Finally,
the mixed-granularity sentence decoder generates
a sentence token by token conditioning on the pre-
dicted sentence vector. We consider two sentence-
level language models, called SSR-AR and SSR-
NonAR, to utilize two types of objectives of lan-
guage modeling, autoregressive modeling (Rad-
ford et al., 2019; Brown et al., 2020) and non-
autoregressive masked modeling (Devlin et al.,
2019). The SSR-AR is mainly used for next sen-
tence generation task and the SSR-NonAR is used
for sentence inÔ¨Ålling.

2.2 Transformer-based Language Models

Recently, autoregressive language models such
as GPT2/3 (Radford et al., 2019; Brown et al.,
2020) and non-autoregressive bidirectional lan-

guage models such as BERT (Devlin et al., 2019)
are the most typical paradigms designed for lan-
guage generation and understanding, respectively.
The backbones of those two LM branches are
based on Transformer (Vaswani et al., 2017).
Transformer consists of N identical

self-
attention blocks and each block contains two sub-
layers: a multi-head attention layer and a feed-
forward layer. An add & norm layer is employed
around each of those two sub-layers. Formally,
given the input vectors from the previous block
Hn‚àí1, the output Hn of the current block is com-
puted as follows:

Cn = LN (cid:0)SELF-ATT (cid:0)Hn‚àí1(cid:1) + Hn‚àí1(cid:1) (1)
Hn = LN (FFN (Cn) + Cn)
(2)

where SELF-ATT(¬∑), LN(¬∑), and FFN(¬∑) are
respectively self-attention mechanism,
layer
normalization,
network.
and
SELF-ATT(¬∑) conducts attention modeling over
the input Hn‚àí1:

feed-forward

SELF-ATT (cid:0)Hn‚àí1(cid:1) = softmax

(cid:19)

(cid:18) QK(cid:62)
‚àö
dk

V (3)

‚àö

where {Q, K, V} are query, key, and value vec-
tors which are transformed from the input variable
Hn‚àí1.
dk is the scaling factor where the dk is
the dimension size of the query and key vectors.
There are two learnable embedding tables, word
embedding table and positional embedding table.
Given the word embeddings E = {e1, e2, ..., eT }
and corresponding positional embeddings P =
{p1, p2, ..., pT } of a sentence, the input for the
Ô¨Årst block is H0 = E + P.

Finally, a linear function g with softmax acti-
vation is used to compute the probability of next
token xt for GPT2/3 via:

p (xt|x‚â§t‚àí1) = softmax (g (ht))

(4)

Multi-Head Self-Attention LayerFeed-Forward Layerùë•1ùë•2ùë•3Embedding Layerùë•4ùë•5Pooling LayerSentence Semantic VectorMulti-Head Self-Attention LayerFeed-Forward Layerùëß1ùëß2ùëß3ùëß4ùëß5ùëß6ùëß2ùëß3ùëß4ùëß5Multi-Head Self-Attention LayerFeed-Forward Layerùëß1ùëß3ùëß4ùëß2ùëß5maskmaskSentence EncoderSSR-ARSSR-NonARùë•21ùë•22ùë•23ùëß2ùë•11ùë•12ùë•13ùëß1Mixed-Granularity Sentence Decoderùëßor predict the masked token xt for BERT via:

p (xt|x‚â§t‚àí1, x‚â•t+1) = softmax (g (ht))

(5)

We utilize the negative log-likelihood (NLL)
loss as the optimization objective to train the
model:

Llm = ‚àí

1
T

(cid:88)

t

log p (xt)

(6)

where p(xt) can be p(xt|x‚â§t‚àí1) for GPT2/3 or
p(xt|x‚â§t‚àí1, x‚â•t+1) for BERT.

2.3 Sentence Semantic Encoder

Our framework treats a sentence as a semantic unit
and conducts language modeling on sentence level
instead of token level. Sentences are converted
into dense vectors to represent the semantic in-
formation. The quality of sentence representation
directly affects the performance of sentence-level
language modeling. In this work, we use BERT to
obtain sentence vectors, considering that BERT is
good at understanding and is widely used in many
tasks. Certainly, we could use other sentence
representation methods (Reimers and Gurevych,
2019; Zhang et al., 2020b; Li et al., 2020; Gao
et al., 2021). Considering that it is not the main
focus of this work, we leave them for future inves-
tigations.

SpeciÔ¨Åcally, as shown in Figure 2, we Ô¨Årst load
the pre-trained weights of BERT and make the pa-
rameter Ô¨Åxed. Then we feed a sentence into BERT
and obtain hidden states of the second-to-last layer
for each word as HL‚àí1. We apply mean-pooling
on word vectors HL‚àí1 to obtain the sentence vec-
tor zi for the ith sentence.

2.4 Sentence Semantic Regression (SSR)

The core idea of Sentence Semantic Regression is
sentence-level language modeling. The sentence-
level language model conducts language modeling
on sentence vector sequences and learns to pre-
dict the target sentence vector as the main idea
to be presented and conveyed. As shown in Fig-
ure 2, we consider two types of objectives for lan-
guage modeling, autoregressive modeling (Rad-
ford et al., 2019; Brown et al., 2020) and non-
autoregressive masked modeling (Devlin et al.,
2019). The former is called SSR-AR and the
latter is called SSR-NonAR. SSR-AR applies an
architecture similar to GPT2/3 and SSR-NonAR
is similar to BERT. There are several differences
between our sentence-level language models and
GPT2/3 or BERT.

First, after obtaining the sentence semantic vec-
tors Z, we directly feed them to the model of
SSR and our model has no word embedding layer.
The positional embedding table still remains to
indicate sentence location in the original para-
graph. SpeciÔ¨Åcally, given the sentence vectors
Z = {z1, z2, ..., zm} and corresponding posi-
tional embeddings Ps = {ps
m} of a
paragraph with m sentences, the inputs of the Ô¨Årst
block are H0 = Z + Ps.

2, ..., ps

1, ps

Second, different with GPT/BERT, we remove
the softmax activation function in the output
layer and predict a vector with the original dimen-
sion size d as follows:

Àúzt = g(cid:48)(hs
t )

(7)

Third, the loss function is also different with
GPT/BERT considering that the target of SSR is
to predict a vector Àúzt. We employ cosine similar-
ity between the predicted sentence vector Àúzt and
the ground truth vector zt as the optimization ob-
jective, and the calculation is as follows:
1
m

(1 ‚àí cos(Àúzt, zt))

Lssr =

(cid:88)

(8)

t

And this is the main reason we name our frame-
work as ‚ÄúSentence Semantic Regression‚Äù.

Generally, in GPT2/3 and BERT, the cross en-
tropy loss optimizes the positive tokens and the
negative tokens simultaneously because of the su-
perior property of softmax operation. However,
our cosine similarity-based loss only takes into ac-
count the positive sentences, ignoring the negative
ones. Thus, to enhance the performance, inspired
by the contrastive learning paradigm (He et al.,
2020; Chen et al., 2020), we randomly sample n
sentences from other paragraphs for each sample
as negative sentences. Then the contrastive loss
function with negative examples is changed to:

Lcontrast

ssr

=

1
m

2 ‚àí cos(Àúzt, zt) +

1
n

(cid:32)

(cid:88)

t

(cid:33)

(cid:88)

cos(Àúzt, zi)

i

(9)
where Àúzt is the predicted vector, zt is the ground
truth vector, and zi is the negative sample.

SSR-AR Similar to the autoregressive language
models such as GPT2/3, we conduct sentence se-
mantic regression autoregressively. Here, the in-
put is a sequence of sentence semantic vectors, and
the target is the left-shifting of the input.

SSR-NonAR As BERT, for the input sentence
vector sequence, we randomly mask off 15% of
them with zero vectors and then use the rest of the
sentences to predict the masked sentence vectors.

2.5 Surface Realization

duct generation:

For our framework SSR, surface realization can be
implemented via two kinds of approaches: match-
ing and generation.

Surface Realization via Matching Since we
have obtained the predicted sentence vector Àúz (the
main idea to be presented), we can Ô¨Ånd a sentence
X (cid:48) from a candidate corpus whose semantic vector
z(cid:48) is the most matched one with Àúz. The matching
algorithm can be cosine similarity, inner product,
or even a trained matching model. Here, we claim
that matching is just a possible approach to Ô¨Ånd
a candidate sentence (we designed experiments to
clarify this point), and certainly it is not the most
appropriate one due to some issues such as coref-
erence and discourse consistency.

Surface Realization via Generation We be-
lieve generation is a better manner to conduct
surface realization. Thus, we build a Mixed-
Granularity (sentence- and token-granularity) sen-
tence decoder, which is used to recover the cor-
responding sentence token by token condition-
ing on the predicted sentence vector by SSR-AR
or SSR-NonAR. The backbone of the decoder is
still a Transformer-based autoregressive language
model.

SpeciÔ¨Åcally, we use the predicted sentence vec-
tor Àúz as the Ô¨Årst token and <eos> as the second
token, which are regarded as context information
and fed into the decoder to conduct generation:
Àúz, <eos> ‚àí‚Üí y1, y2, . . . , yT (cid:48), <eos>

(10)

We call this version a vanilla sentence decoder.
Although the input format is similar to GPT2,
there is still a big difference between the two meth-
ods. Here, our vanilla sentence decoder only con-
ducts generation according to the sentence vector
z and restores the semantic information as much
as possible. However, we found that the decoder
could not accurately restore some detailed low-
frequency information, such as the named entities
which usually appear in the previous sentences.
Moreover, as mentioned in Section 1, the results
of surface realization for the same idea z can be
different. Therefore, in order to generate an appro-
priate result for the current context, some factors
such as token-level discourse/coreference consis-
tency need to be recalled.

Consistency Enhanced Realization Thus, we
enrich the context by brieÔ¨Çy appending the tokens
from the previous sentences before <eos> to con-

Àúz,..., xl‚àí1,n‚àí1, xl‚àí1,n, <eos>

‚àí‚Üí yl,1, yl,2, ..., yl,T (cid:48), <eos>

(11)

Since both the sentence-level information (Àúz)
and token-level information are used as context to
conduct generation, we call it Mixed-Granularity
In experiments, we interest-
Sentence Decoder.
ingly Ô¨Ånd that the mixed-granularity sentence de-
coder can restore named entities accurately.

2.6 Training

The sentence encoder loads the pre-trained BERT
directly and we do not Ô¨Åne-tune it. We Ô¨Årst repro-
cess the data with the sentence encoder to get all
the sentence vectors. Then we use sentence vec-
tor sequences to train the sentence-level language
models SSR-AR and SSR-NonAR, and regrade
Lcontrast
as the loss function. For the decoder, (the
sentence vector of target sentence, the tokens of
previous sentences, the tokens of target sentence)
triples are used to train the mixed-granularity sen-
tence decoder and the typical NLL loss is regarded
as the optimization objective.

ssr

3 Experimental Setup

In order to evaluate our framework, we conduct
experiments on four different tasks, story ending
prediction, story ending generation, dialogue gen-
eration, and sentence inÔ¨Ålling. Story ending pre-
diction, also called Story Cloze, aims to select
the coherent ending from two candidate sentences.
This task is used to verify the capability of SSR
with surface realization via matching. Story end-
ing generation and dialogue generation belong to
next sentence generation, which aims to gener-
ate a Ô¨Ånal sentence given several previous sen-
tences. We use these two tasks to evaluate SSR-
AR. Sentence inÔ¨Ålling aims to generate an inter-
mediate sentence given the surrounding sentences,
which goal is in line with SSR-NonAR‚Äôs objec-
tive. Therefore, we use this task to validate SSR-
NonAR.

3.1 Datasets

For story ending prediction, story ending gen-
eration, and sentence inÔ¨Ålling, we use the ROC
Stories dataset, which consists of stories focus-
ing on common sense (Mostafazadeh et al., 2016).
The dataset was introduced for the Story Cloze
task. The training set has 98k stories and each

story has Ô¨Åve sentences. The validation set 2016,
test set 2016, and validation set 2018 contain
1.8k/1.8k/1.5k stories consisting of four sentences
followed by two alternative endings: one ending is
coherent with the context; the other is not. Notice
that there are no labels in the training set.

For story ending prediction, we follow the origi-
nal task objective. We conduct unsupervised train-
ing on the training set and then evaluate the model
on validation and test sets. Many works on this
task used a supervised setting and they used the
validation set as a small labeled training set to train
the model (Chaturvedi et al., 2017; Zhou et al.,
2019; Li et al., 2019; Cui et al., 2020). Therefore,
we don‚Äôt compare with these methods. For story
ending generation and sentence inÔ¨Ålling, we split
the original training set into TRAIN, VAL, and
TEST (88k/5k/5k). For dialogue generation, we
use the Dailydialog dataset (Li et al., 2017). Daily-
dialog contains 13k daily conversations under ten
different topics. Statistics show that the speaker
turns are roughly 8, and the average number of
tokens per utterance is about 15. The dataset is
randomly separated into TRAIN, VAL, and TEST
(11k/1k/1k).
In order

framework
to evaluate that our
can improve the performance by pre-training
the sentence-level language model, we use the
BooksCorpus
(Zhu et al., 2015) and English
Wikipedia1 to pre-train SSR-NonAR. SpeciÔ¨Åcally,
we split the text of the two corpora into sentences,
and then use the sentence encoder to obtain all the
sentence vectors. We use 128 consecutive sen-
tences as a paragraph to train SSR-NonAR.

3.2 Baseline Models

For four tasks, we mainly compare our frame-
work with a common token-level language model,
GPT2 (Radford et al., 2019). On the one hand,
GPT2 is better than many task-speciÔ¨Åc models for
many generation tasks. On the other hand, we
mainly want to prove that the sentence-level lan-
guage model is superior to the token-level lan-
guage model in text generation tasks. Therefore,
we do not compare with some other methods. It
should be noted that GPT2 cannot be used directly
for sentence inÔ¨Ålling, while IGPT2 proposed by
Donahue et al. (2020) designs a mask strategy
so that it can be used for Ô¨Ålling blanks with
GPT2. Therefore, we compare our framework

with IGPT2 in sentence inÔ¨Ålling task. GPT2(FT)
and IGPT2(FT) mean that we Ô¨Åne-tune the model
on the corresponding task data.

In order to evaluate each component of SSR, we
conduct a detailed ablation analysis for the fol-
lowing variants. SSR(I) denotes the basic ver-
sion of our framework, which only uses the co-
sine loss Lssr (Eq.(8)) and the vanilla sentence de-
coder (Eq.(10)). SSR(II) denotes our framework
with the contrastive loss Lcontrast
(Eq.(9)) and
the vanilla decoder. SSR is our full framework,
which utilizes the contrastive loss and the mixed-
granularity sentence decoder (Eq.(11)). PT means
that we Ô¨Årst pre-train our SSR on BooksCorpus
and Wikipedia.

ssr

3.3 Evaluation Metrics and Implementation

Details

Automatic Evaluation. For story ending predic-
tion, we evaluate the accuracy of binary classiÔ¨Å-
cation. For story ending generation, dialogue gen-
eration, and sentence inÔ¨Ålling, we use BLEU (Pa-
pineni et al., 2002) and Distinct (Li et al., 2016)
as metrics. BLEU measures the n-gram overlap
between generated text and ground truth. Distinct
aims to evaluate the diversity of generated text.
Human Evaluation. To further evaluate the qual-
ity of generated text in three generation tasks, we
conduct pair-wise comparisons by human evalua-
tion. We evaluate the models from the following
three perspectives: Grammaticality to indicate
whether a story is natural and Ô¨Çuent, Logicality to
indicate whether a story is consistent and coher-
ent in terms of causal dependencies in the context,
and Informativeness to evaluate how much novel
information the generated text contains. Note that
the three aspects are independently evaluated. For
each task, we randomly sample 100 contexts from
the test set and obtain text generated under these
contexts by different models. For each pair of
generated text, three annotators are asked to give
a preference (win, lose, or tie) in terms of three
metrics respectively. We adopt majority voting to
make Ô¨Ånal decisions among the three annotators.
Implementation Details. Our implementation is
based on Transformers2 (Wolf et al., 2020) and
PyTorch (Paszke et al., 2019). For the GPT2
baseline, we use the GPT2-small pre-trained
model (Radford et al., 2019). SpeciÔ¨Åcally, the di-
mension of word embedding and the dimension

1These two datasets are available in huggingface/datasets

2https://github.com/huggingface/transformers

Type

Token-Level LMs

SSR-AR

SSR-NonAR

Model
Schwartz et al. (2017)
GPT2
GPT2(FT)
SSR-AR(I)
SSR-AR(II)
SSR-NonAR(I)
SSR-NonAR(II)
SSR-NonAR(II)+PT

Valid2016 Test2016 Valid2018 Avg

-
57.4
68.5
67.1
72.7
66.4
72.0
73.7

67.7
59.2
69.1
67.0
71.0
66.9
71.7
72.1

-
57.7
69.1
67.7
72.4
66.9
72.1
73.3

-
58.1
68.9
67.3
72.0
66.7
72.0
73.0

Table 1: Automatic evaluation results of story ending prediction.

GPT2(FT)
SSR-AR(I)
SSR-AR(II)
SSR-AR

B-1
0.1180
0.1265
0.1018
0.1281

B-2
0.0223
0.0163
0.0150
0.0271

B-3
0.0053
0.0026
0.0022
0.0061

B-4
0.0015
0.0005
0.0005
0.0014

D-1
0.1379
0.0802
0.1512
0.1617

D-2
0.5185
0.3297
0.5425
0.5650

D-3
0.8329
0.6170
0.8603
0.8728

D-4
0.9546
0.8036
0.9665
0.9709

Table 2: Automatic evaluation results of story ending generation.

of hidden vectors are set to 768. The number of
self-attention blocks is set to 12 and 12 heads are
used in self multi-head attention. For the mixed-
granularity decoder in our framework, we apply
the same model size as GPT2-small. For SSR-
AR, we use a structure similar to GPT2 and we
use a structure similar to the uncased BERT-base
pre-trained model (Devlin et al., 2019) for SSR-
NonAR. We train the model using Adam (Kingma
and Ba, 2014) with learning rate 0.00005. The
dropout rate is set to 0.1 for regularization. Fol-
lowing (Fan et al., 2018) we generate text with ran-
dom top k sampling, where next words are sam-
pled from the top k = 20 candidates rather than
the entire vocabulary distribution. We conduct ex-
periments on 1 NVIDIA Tesla V100 GPU.

4 Results and Discussions

4.1 Main Results

Story Ending Prediction We verify the perfor-
mance of ‚Äúsurface realization via matching‚Äù on
the task of story ending prediction. Automatic
evaluation results are shown in Table 1. We
can see that our models SSR-AR(II) and SSR-
NonAR(II) outperform GPT2(FT) signiÔ¨Åcantly.
This shows the effectiveness of our proposed
framework. GPT2(FT) employs the perplexity
to conduct sentence selection, while SSR-AR(II)
and SSR-NonAR(II) are sentence-level matching.
That is, the perplexity depends on the probabilities
of all tokens in candidate sentences. In contrast,
we Ô¨Årst predict the next sentence vector and then
compute the similarity of the predicted sentence
vector with the candidate sentence vectors. The
evaluation results prove that sentence-level selec-
tion is better than token-level selection. Compared
with SSR-AR(II) and SSR-NonAR(II), the perfor-

mance of SSR-AR(I) and SSR-NonAR(I) drop a
lot, indicating the importance of contrastive loss
during training. In addition, SSR-NonAR(II)+PT
further improves the performance. This demon-
strates that our framework can further improve by
pre-training SSR on the extra corpus.

Story Ending Generation Table 2 reports au-
tomatic evaluation results of story ending gener-
ation. SSR-AR outperforms GPT2(FT) in most
metrics, indicating that the effectiveness of our
framework on generation. In addition, SSR-AR(I)
performs worse than GPT2(FT) in most metrics
while SSR-AR(II) outperforms GPT2(FT) in Dis-
tinct. This indicates that the contrastive loss con-
tributes to improving diversity. Without the con-
trastive loss, SSR-AR(I) tends to generate com-
mon words. Thus, it achieves a higher BLEU-1
score and a lower Distinct score than GPT2(FT).
Compared with SSR-AR(II), SSR-AR improves
BLEU score signiÔ¨Åcantly. This shows that the
mixed-granularity decoder is helpful to reproduce
words in contexts and thus the results have better
relevance to contexts.

Dialogue Generation The trend in Table 3 is
similar to that in Table 2. SSR-AR performs bet-
ter than GPT2(FT) in most metrics, and the con-
trastive loss and the mixed-granularity sentence
decoder improve the performance of Distinct and
BLEU respectively.

Sentence InÔ¨Ålling Automatic evaluation results
of sentence inÔ¨Ålling are shown in Table 4. We can
see that SSR-NonAR outperforms IGPT2(FT) in
all metrics. Similar to SSR-AR(I), SSR-NonAR(I)
also performs worse in Distinct. When adding the
contrastive loss, SSR-NonAR(II) performs better
than IGP2(FT). This demonstrates that the con-
trastive loss is important for both SSR-AR and

(a) BLEU-1

(b) BLEU-2

(c) Distinct-1

(d) Distinct-2

Figure 3: Topic stability analysis of SSR and GPT2 according to the hyperparameter k in top-k sampling.

GPT2(FT)
SSR-AR(I)
SSR-AR(II)
SSR-AR

B-1
0.0658
0.0854
0.0583
0.0693

B-2
0.0207
0.0239
0.0177
0.0232

B-3
0.0049
0.0033
0.0020
0.0054

B-4
0.0029
0.0002
0.0002
0.0017

D-1
0.2347
0.1608
0.2583
0.2565

D-2
0.6950
0.5090
0.7482
0.7597

D-3
0.9206
0.7575
0.9495
0.9549

D-4
0.9808
0.8877
0.9891
0.9914

Table 3: Automatic evaluation results of dialogue generation.

IGPT2(FT)
SSR-NonAR(I)
SSR-NonAR(II)
SSR-NonAR
SSR-NonAR+PT

B-1
0.1191
0.1433
0.1202
0.1326
0.1413

B-2
0.0244
0.0234
0.0246
0.0296
0.0342

B-3
0.0069
0.0070
0.0075
0.0105
0.0114

B-4
0.0021
0.0018
0.0024
0.0042
0.0039

D-1
0.1391
0.0851
0.1615
0.1669
0.1698

D-2
0.5276
0.3412
0.5781
0.5823
0.5832

D-3
0.8480
0.6365
0.8784
0.8787
0.8725

D-4
0.9615
0.8262
0.9705
0.9704
0.9667

Table 4: Automatic evaluation results of sentence inÔ¨Ålling.

Method

Task
story ending gen. SSR-AR vs. GPT2(FT)
SSR-AR vs. GPT2(FT)
dialogue gen.
SSR-NonAR vs. IGPT2(FT)
sentence inÔ¨Å.

Grammaticality

Logicality
Win(%) Tie(%) Lose(%) Win(%) Tie(%) Lose(%) Win(%) Tie(%) Lose(%)
2
4
8

Informativeness

27
21
31

11
9
11

92
89
87

16
18
23

57
61
46

83
86
81

6
7
5

6
5
8

Table 5: Human evaluation results for three generation tasks.

SSR-NonAR. Compared with SSR-NonAR(II),
indicat-
SSR-NonAR improves BLEU further,
ing the effectiveness of the mixed-granularity de-
coder. In addition, when pre-training the sentence-
level language model on the extra corpus, SSR-
NonAR+PT outperforms SSR-NonAR in most
metrics, showing that our framework can beneÔ¨Åt
from pre-training. From the results of three gener-
ation tasks, we can see that the contrastive loss and
the mixed-granularity decoder contribute to Dis-
tinct and BLEU respectively. With these two com-
ponents, SSR obtains better performance than the
token-level method.

Human Evaluation For three generation tasks,
we further utilize human evaluation to evaluate
our framework. The results are shown in Table
5. SSR outperforms GPT2(FT) in terms of Log-
icality and Informativeness. Considering that we
Ô¨Årst predict a sentence vector and then decode it
into a sentence, the text generated by this process
is much more consistent than that produced via a
token by token method. We will prove this in a
more in-depth analysis in the next section. In ad-
dition, SSR performs close to GPT2(FT) in terms
of Grammaticality. We claim that both methods
use an AR-based decoder, so the Ô¨Çuency of gener-

ated sentences is close.

4.2 Analysis of Topic Drift

In order to investigate the degree of topic drift of
different models, we conduct generation with dif-
ferent top-k settings under the same contexts on
story ending generation. From Figure 3 we can
observe that BLEU of GPT2(FT) drops sharply
and Distinct of GPT2(FT) increases a lot when in-
creasing k, showing that the larger k is, the more
serious the topic drift is. While BLEU of our
method drops a little and maintains a stable range.
This indicates that SSR can control the semantic of
the content to be generated through the predicted
sentence vector and effectively avoid topic drift.

Table 6 depicts generated results containing
named entities. With the mixed-granularity de-
coder, SSR-AR can restore named entities accu-
rately, indicating that the mixed-granularity de-
coder contributes to better consistency of the sur-
face realization. Table 7 shows examples gener-
ated by top-k sampling four times. The semantics
of the generated results by GPT2(FT) vary greatly
and are not consistent with the context. The topic
drift phenomenon occurs. In contrast, SSR-AR(II)
predicts context-consistent topics, generating sen-

20406080Top-k0.1150.1200.1250.130BLEU-1GPT2(FT)SSR-AR20406080Top-k0.0180.0200.0220.0240.026BLEU-2GPT2(FT)SSR-AR20406080Top-k0.130.140.150.16Distinct-1GPT2(FT)SSR-AR20406080Top-k0.480.500.520.540.56Distinct-2GPT2(FT)SSR-ARContext: Rex had always wanted to play hockey. He de-
cided that he Ô¨Årst needed to learn to skate. He practiced
skating every day until he was an expert. Finally Rex was
able to begin playing hockey.

GPT2(FT)

1: Rex is now a very good skater.

SSR-AR(II)

1: Andrew is very proud to now experi-
ence his playing ability!

SSR-AR

1: Rex is very proud that he now has great
hockey skills!

Table 6: Generated examples containing named enti-
ties.

Context: Willie had too much stuff. Willie bought a shed
to store all his stuff. Willie had a hard time putting up the
shed. He called some friends for help.

GPT2(FT)

SSR-AR(II)

SSR-AR

1: Willie sold his shed and made enough
money to pay for the house.
2: After a few days they bought the shed.
3: They brought him the shed and Ô¨Åxed the
shed.
4: The shed became a nice storage place.

1: They made it all back in the tools.
2: They helped keep the pile together
plenty.
3: They mopped and hauled the house out.
4: They Ô¨Ånished the shed and put every-
thing back.

1: They helped put up the shed Ô¨Çoor.
2: They helped pile the shed down.
3: They helped put the shed out and usable.
4: They helped haul the shed all out.

Table 7: Generated results by top-k for 4 rounds.

tences about ‚Äúhow to build‚Äù. But the Ô¨Åne-grained
words are quite different from the ones in the con-
text.
Jointly considering the predicted sentence
vector and words from the context, SSR-AR pro-
duces sentences that are more consistent and rele-
vant with the context.
5 Conclusion

We propose a new framework named Sentence Se-
mantic Regression (SSR) for text generation. Two
architectures SSR-AR and SSR-NonAR are de-
signed to conduct sentence semantic regression
autoregressively and bidirectionally. A mixed-
granularity decoder is used to generate sentences
considering the predicted sentence vectors and the
context tokens. We conduct experiments on the
tasks of story ending prediction, story ending gen-
eration, dialogue generation, and sentence inÔ¨Åll-
ing. The results show SSR obtain better perfor-
mance in terms of automatic metrics and human
evaluation.

References

Daniel Adiwardana, Minh-Thang Luong, David R
So, Jamie Hall, Noah Fiedel, Romal Thoppilan,

Zi Yang, Apoorv Kulshreshtha, Gaurav Nemade,
Yifeng Lu, et al. 2020. Towards a human-like open-
domain chatbot. arXiv preprint arXiv:2001.09977.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly
In 3rd Inter-
learning to align and translate.
national Conference on Learning Representations,
ICLR 2015, San Diego, CA, USA, May 7-9, 2015,
Conference Track Proceedings.

Tom B Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, et al. 2020. Language models are few-shot
learners. arXiv preprint arXiv:2005.14165.

Snigdha Chaturvedi, Haoruo Peng, and Dan Roth.
2017. Story comprehension for predicting what hap-
pens next. In Proceedings of the 2017 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 1603‚Äì1614.

Ting Chen, Simon Kornblith, Mohammad Norouzi,
and Geoffrey Hinton. 2020. A simple framework for
contrastive learning of visual representations. In In-
ternational conference on machine learning, pages
1597‚Äì1607. PMLR.

Yiming Cui, Wanxiang Che, Wei-Nan Zhang, Ting Liu,
Shijin Wang, and Guoping Hu. 2020. Discrimina-
tive sentence modeling for story ending prediction.
In Proceedings of the AAAI Conference on ArtiÔ¨Åcial
Intelligence, volume 34, pages 7602‚Äì7609.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. Bert: Pre-training of
deep bidirectional transformers for language under-
standing. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume 1 (Long and Short Papers), pages
4171‚Äì4186.

Chris Donahue, Mina Lee, and Percy Liang. 2020. En-
abling language models to Ô¨Åll in the blanks. In Pro-
ceedings of the 58th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 2492‚Äì
2501, Online. Association for Computational Lin-
guistics.

Angela Fan, Mike Lewis, and Yann Dauphin. 2018.
In Proceed-
Hierarchical neural story generation.
ings of the 56th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), pages 889‚Äì898.

Luciano Floridi and Massimo Chiriatti. 2020. Gpt-3:
Its nature, scope, limits, and consequences. Minds
and Machines, 30(4):681‚Äì694.

Zihao Fu, Wai Lam, Anthony Man-Cho So, and Bei
Shi. 2020. A theoretical analysis of the repeti-
arXiv preprint
tion problem in text generation.
arXiv:2012.14660.

Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021.
SimCSE: Simple contrastive learning of sentence
embeddings. arXiv preprint arXiv:2104.08821.

Albert Gatt and Emiel Krahmer. 2018. Survey of the
state of the art in natural language generation: Core
tasks, applications and evaluation. Journal of ArtiÔ¨Å-
cial Intelligence Research, 61:65‚Äì170.

Jiatao Gu, James Bradbury, Caiming Xiong, Vic-
Non-
arXiv

tor OK Li, and Richard Socher. 2017.
autoregressive neural machine translation.
preprint arXiv:1711.02281.

Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and
Ross Girshick. 2020. Momentum contrast for un-
In Pro-
supervised visual representation learning.
ceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pages 9729‚Äì9738.

Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and
Yejin Choi. 2019. The curious case of neural text
degeneration. arXiv preprint arXiv:1904.09751.

Diederik P Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980.

Bohan Li, Hao Zhou, Junxian He, Mingxuan Wang,
Yiming Yang, and Lei Li. 2020. On the sen-
tence embeddings from pre-trained language mod-
els. arXiv preprint arXiv:2011.05864.

Jiwei Li, Michel Galley, Chris Brockett, Georgios Sp-
ithourakis, Jianfeng Gao, and Bill Dolan. 2016. A
In Pro-
persona-based neural conversation model.
ceedings of the 54th Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long
Papers), pages 994‚Äì1003. Association for Computa-
tional Linguistics.

Yanran Li, Hui Su, Xiaoyu Shen, Wenjie Li, Ziqiang
Cao, and Shuzi Niu. 2017. Dailydialog: A manually
labelled multi-turn dialogue dataset. In Proceedings
of The 8th International Joint Conference on Natural
Language Processing (IJCNLP 2017).

Zhongyang Li, Xiao Ding, and Ting Liu. 2019. Story
arXiv

ending prediction by transferable bert.
preprint arXiv:1905.07504.

William C Mann. 1983. An overview of the penman
text generation system. In AAAI, pages 261‚Äì265.

Joshua Maynez, Shashi Narayan, Bernd Bohnet, and
Ryan McDonald. 2020. On faithfulness and factu-
ality in abstractive summarization. In Proceedings
of the 58th Annual Meeting of the Association for
Computational Linguistics, pages 1906‚Äì1919.

Kathleen McKeown. 1992. Text generation. Cam-

bridge University Press.

Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong
He, Devi Parikh, Dhruv Batra, Lucy Vanderwende,

Pushmeet Kohli, and James Allen. 2016. A cor-
pus and cloze evaluation for deeper understanding of
In Proceedings of the 2016
commonsense stories.
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, pages 839‚Äì849.

Feng Nie, Jin-Ge Yao, Jinpeng Wang, Rong Pan, and
Chin-Yew Lin. 2019. A simple recipe towards re-
ducing hallucination in neural surface realisation.
In Proceedings of the 57th Annual Meeting of the
Association for Computational Linguistics, pages
2673‚Äì2679, Florence, Italy. Association for Compu-
tational Linguistics.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
In Proceedings of
uation of machine translation.
the 40th annual meeting on association for compu-
tational linguistics, pages 311‚Äì318. Association for
Computational Linguistics.

Adam Paszke, Sam Gross, Francisco Massa, Adam
Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca
Antiga, et al. 2019.
Pytorch: An imperative
style, high-performance deep learning library. arXiv
preprint arXiv:1912.01703.

Alec Radford, Jeffrey Wu, Rewon Child, David Luan,
Dario Amodei, and Ilya Sutskever. 2019. Language
models are unsupervised multitask learners. OpenAI
Blog, 1(8).

Nils Reimers and Iryna Gurevych. 2019. Sentence-
BERT: Sentence embeddings using Siamese BERT-
networks. In Proceedings of the 2019 Conference on
Empirical Methods in Natural Language Process-
ing and the 9th International Joint Conference on
Natural Language Processing (EMNLP-IJCNLP),
pages 3982‚Äì3992, Hong Kong, China. Association
for Computational Linguistics.

Ehud Reiter and Robert Dale. 1997. Building applied
natural language generation systems. Natural Lan-
guage Engineering, 3(1):57‚Äì87.

Stephen Roller, Emily Dinan, Naman Goyal, Da Ju,
Mary Williamson, Yinhan Liu, Jing Xu, Myle Ott,
Kurt Shuster, Eric M Smith, et al. 2020. Recipes
for building an open-domain chatbot. arXiv preprint
arXiv:2004.13637.

Alexander M Rush, Sumit Chopra, and Jason Weston.
2015. A neural attention model for abstractive sen-
In Proceedings of the 2015
tence summarization.
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 379‚Äì389.

Abigail See, Aneesh Pappu, Rohun Saxena, Akhila
Yerukola, and Christopher D Manning. 2019. Do
massively pretrained language models make better
In Proceedings of the 23rd Confer-
storytellers?
ence on Computational Natural Language Learning
(CoNLL), pages 843‚Äì861.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016. Neural machine translation of rare words
with subword units. In Proceedings of the 54th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 1715‚Äì
1725.

Yan Zhang, Ruidan He, Zuozhu Liu, Kwan Hui Lim,
and Lidong Bing. 2020b. An unsupervised sentence
embedding method by mutual information maxi-
mization. In Proceedings of the 2020 Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 1601‚Äì1610.

Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun
Chen, Chris Brockett, Xiang Gao, Jianfeng Gao,
Jingjing Liu, and William B Dolan. 2020c. Di-
alogpt: Large-scale generative pre-training for con-
In Proceedings
versational response generation.
of the 58th Annual Meeting of the Association for
Computational Linguistics: System Demonstrations,
pages 270‚Äì278.

Mantong Zhou, Minlie Huang, and Xiaoyan Zhu.
2019. Story ending selection by Ô¨Ånding hints from
IEEE/ACM Transac-
pairwise candidate endings.
tions on Audio, Speech, and Language Processing,
27(4):719‚Äì729.

Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut-
dinov, Raquel Urtasun, Antonio Torralba, and Sanja
Fidler. 2015. Aligning books and movies: Towards
story-like visual explanations by watching movies
In Proceedings of the IEEE
and reading books.
international conference on computer vision, pages
19‚Äì27.

Lifeng Shang, Zhengdong Lu, and Hang Li. 2015.
Neural responding machine for short-text conver-
In Proceedings of the 53rd Annual Meet-
sation.
ing of the Association for Computational Linguistics
and the 7th International Joint Conference on Natu-
ral Language Processing (Volume 1: Long Papers),
pages 1577‚Äì1586.

Bowen Tan, Zichao Yang, Maruan AI-Shedivat, Eric P
Xing, and Zhiting Hu. 2020. Progressive generation
of long text. arXiv preprint arXiv:2006.15720.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, ≈Åukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Proceedings of the 31st International
Conference on Neural Information Processing Sys-
tems, pages 6000‚Äì6010.

Oriol Vinyals, Alexander Toshev, Samy Bengio, and
Dumitru Erhan. 2016.
Show and tell: Lessons
learned from the 2015 mscoco image captioning
IEEE transactions on pattern analysis
challenge.
and machine intelligence, 39(4):652‚Äì663.

Yida Wang, Pei Ke, Yinhe Zheng, Kaili Huang, Yong
Jiang, Xiaoyan Zhu, and Minlie Huang. 2020. A
large-scale chinese short-text conversation dataset.
In CCF International Conference on Natural Lan-
guage Processing and Chinese Computing, pages
91‚Äì103. Springer.

Sean Welleck, Ilia Kulikov, Stephen Roller, Emily
Dinan, Kyunghyun Cho, and Jason Weston. 2019.
Neural text generation with unlikelihood training. In
International Conference on Learning Representa-
tions.

Thomas Wolf, Julien Chaumond, Lysandre Debut, Vic-
tor Sanh, Clement Delangue, Anthony Moi, Pier-
ric Cistac, Morgan Funtowicz, Joe Davison, Sam
Shleifer, et al. 2020. Transformers: State-of-the-
art natural language processing. In Proceedings of
the 2020 Conference on Empirical Methods in Nat-
ural Language Processing: System Demonstrations,
pages 38‚Äì45.

Quanzeng You, Hailin Jin, Zhaowen Wang, Chen Fang,
and Jiebo Luo. 2016. Image captioning with seman-
tic attention. In Proceedings of the IEEE conference
on computer vision and pattern recognition, pages
4651‚Äì4659.

Jingqing Zhang, Yao Zhao, Mohammad Saleh, and Pe-
ter Liu. 2020a. Pegasus: Pre-training with extracted
gap-sentences for abstractive summarization. In In-
ternational Conference on Machine Learning, pages
11328‚Äì11339. PMLR.

