A True AR Authoring Tool for Interactive 
Virtual Museums 

Efstratios Geronikolakis1,3, Paul Zikas1,3, Steve Kateros1,3, Nick Lydatakis1,3, 
Stelios Georgiou1,3, Mike Kentros1,3, George Papagiannakis1,2,3 

1 Foundation for Research and Technology Hellas, 100 N. Plastira Street, 70013 
Heraklion, Greece 

2 Computer Science Department, University of Crete, Voutes Campus, 70013 Heraklion, 
Greece 

3 ORamaVR, 100 N. Plastira Street, 70013 Heraklion, Greece 
{stratos, paul, steve, nick, stelios, mike, 

george.papagiannakis}@oramavr.com 

Abstract 

In this work, a new and innovative way of spatial computing that appeared re-
cently  in  the  bibliography  called  True  Augmented  Reality  (AR),  is  employed  in 
cultural  heritage  preservation.  This  innovation  could  be  adapted  by  the  Virtual 
Museums  of  the  future  to  enhance  the  quality  of  experience.  It  emphasises,  the 
fact that a visitor will not be able to tell, at a first glance, if the artefact that he/she 
is looking at is real or not and it is expected to draw the visitors’ interest. True AR 
is not limited to artefacts but extends even to buildings or life-sized character sim-
ulations of statues. It provides the best visual quality possible so that the users will 
not be able to tell the real objects from the augmented ones. Such applications can 
be beneficial for future museums, as with True AR, 3D models of various exhibits, 
monuments, statues, characters and buildings can be reconstructed and presented 
to the visitors in a realistic and innovative way. We also propose our Virtual Re-
ality Sample  application, a  True  AR playground  featuring  basic components and 
tools  for  generating  interactive  Virtual  Museum  applications,  alongside  a  3D  re-
constructed  character  (the  priest  of  Asinou  church)  facilitating  the  storyteller  of 
the augmented experience.  

Keywords: Augmented Reality, Cultural Heritage, Virtual Museums, Gami-
fication 

 
 
2  

Introduction 

Augmented Reality has revolutionized  many  fields in the  industry, from  medical 
planning,  to  educational  and  training  tools.  It  enhances  the  physical  world  with 
holographic  assets,  extending  the  possibilities  of  new  learning  and  educational 
systems. Specifically, in the field of Digital Cultural Heritage, the rendering quali-
ty needs to be as high as possible to properly illuminate the artifacts, buildings or 
even characters, highlighting their natural beauty.  

In this project,  we followed a holistic approach generating  True AR experiences 
by documenting both tangible and intangible heritage. To develop the holographic 
application, we exploited our M.A.G.E.S platform [11] as the core system archi-
tecture. This platform started as a tool to recreate psychomotor scenarios in VR for 
surgeons to master their skills but quickly expanded into other sectors like emer-
gency scenarios, training courses and mechanical solutions. With this project, we 
aim to expand the capabilities of our system to support Cultural Heritage applica-
tions  for educational purposes [15]. Our system  generates  realistic, gamified and 
interactive applications though Rapid Prototyping and Visual Scripting methodol-
ogies, forming an SDK for development of educational projects. By detecting the 
main restrictions that AR and HoloLens has, we took appropriate actions to trans-
fer this work to AR, in order for the two versions to be as “close” as possible. The 
fact that someone is able to see objects or characters in AR and interact with them 
using their hands, increases the realism of AR (since a highly important element of 
realism is interaction as well, apart from the appearance of the characters, illumi-
nation etc.), thus getting us closer to our goal, which is True AR. 

1.  Previous work 

1.1  The M.A.G.E.S platform 

In this project, we exploited our award winning M.A.G.E.S. platform to generate a 
gamified AR scenario as an example of a holographic Virtual Museum [16]. This 
system can generate a fail-safe, realistic environment for surgeons to practice on 
VR  scenarios,  extending  their  skills  in  an  affordable  and  portable  solution.  The 
key  component  of  M.A.G.E.S  platform  lies  on  the  customizable  SDK  platform 
able  to  generate  educational  training  scenarios  and  immersive  experiences  with 
minimal adaptations and code-free due to the Visual Scripting and Rapid Proto-
typing mechanics.  

 
 
3 

Fig. 1 The M.A.G.E.S. flow diagram 

The Visual Scripting component offers a node-based interface to generate VR be-
haviors and interactive tasks following a consistent educational pipeline. Utilizing 
this authoring tool, we developed a variety of medical training simulations, among 
them Total Knee and Hip Arthroplasties, a Dental Implant placement, an Endotra-
cheal Intubation surgery and emergency medical scenarios (car-motorbike evacua-
tion, trauma operations).  

Rapid Prototyping of training scenarios reflects the classification  methods  to ex-
tract  and  break  down  complex  educational  pipelines  into  reusable  and  flexible 
building blocks. After documenting fundamental interaction tasks (insertion of ob-
jects/tools  into  highlighted  areas,  use  of  tools  to  complete  an  action,  removal  of 
objects  using  other  tools  etc.)  we  integrated  those  features  into  the  M.A.G.E.S. 
platform forming a library of prototyped VR Actions in a user-friendly format, ca-
pable to generate more complex scenarios. Inspired from software design patters, 
we developed new VR design patterns, following interactive principles to replicate 
natural behaviors from real life into the VR world. 

1.2  Holographic Virtual Museums 

Since the advancement of  holographic technology,  AR  headsets are  evolving  in-
cluding  interactive  features  like  gesture  and  voice  recognition,  as  well  as  im-
provements on resolution and FOV. In addition, untethered AR headsets paved the 
way for mobile experiences without the need of external processing power from a 
PC.  Such  embedded  systems,  facilitate  great  tools  to  represent  virtual  museums 
[8] due to their lack of cables and enhanced interactive capabilities. Virtual Muse-
ums  are  institutional  centers  in  the  service  of  society,  open  to  the  public  for  ac-

 
 
 
 
4  

quiring  and  exhibiting  the  tangible  and  intangible  heritage  of  humanity  for  the 
purposes of education, study and enjoyment. In addition, True Augmented Reality 
has recently been defined to be a modification of the user’s perception of their sur-
roundings  that  cannot  be  detected  by  the  user  [13]  due  to  their  realism.  Virtual 
characters  and  objects  should  blend  with  their  surroundings,  achieving  the  “sus-
pension of disbelief”. 

In  recent  years,  many  approaches  on  holographic  cultural  heritage  applications 
emerged, each one focusing on a different aspect of representing the holographic 
exhibits within the real environment. A published survey [5] investigated the im-
pact of Virtual and Augmented reality on the  overall visitor experience in muse-
ums,  highlighting  the  social  presence  of  AR  environments.  [9]  presented  a  com-
parison  of  the  latest  methods  for  rapid  reconstruction  of  real  humans  using  as 
input  RGB and RGB-D images. They also introduce a complete  pipeline to pro-
duce highly realistic reconstructions of virtual characters and digital asserts suita-
ble for VR and AR applications. Another project [2] discussed the development of 
a cross/augmented reality application for the Industrial Museum and Cultural Cen-
ter in the region of Thessaloniki, Greece, to promote the preservation of CH in the 
area  of  Central  Macedonia.  The  presented  Mixed  Reality  application,  integrates 
ARKit and  ARCore to implement portal-based  AR virtual  museum along  with a 
gamified tour guidance and exploration of the museum’s interior. The storytelling 
factor of the application is dominant, focusing on the history of the museum, pre-
vious expeditions and its impact to the society of Thessaloniki in micro and macro 
level. Storytelling, Presence and Gamification are three very important fields that 
should be taken into account, when creating an MR application for cultural herit-
age.  [10]  presented  a  comparison  of  existing  MR  methods  for  virtual  museums 
and pointed out the importance of these three fields for applications that contribute 
to the preservation of cultural heritage [3]. Moreover, in [4] fundamental elements 
for MR applications alongside examples are presented. 

Another recent example [19] presented two Mixed Reality Serious Games in VR 
and AR comparing the two technologies over their capabilities and design princi-
ples. Both applications showcased the ancient palace of Knossos in Minoan Crete, 
Greece  through  interactive  mini  games  and  a  virtual/holographic  tour  of  the  ar-
cheological site using Meta AR glasses. [1] successfully published an AR applica-
tion for visualizing restored ancient artifacts based on algorithm that addresses ge-
ometric constraints of fragments to rebuild the object from the available parts. 

1.3  Platforms for Gamified Content Creation 

Authoring  tools  and  other  content  creation  platforms  emerged  in  recent  years  to 
fulfill the need for interactive MR applications. BricklAyeR [14] is a collaborative 
platform designed for users with limited programming skills that allows the crea-
tion of Intelligent Environments through a building-block interface.  ExProtoVAR 
[12] is a lightweight tool to create interactive virtual prototypes of AR applications 

 
 
 
 
5 

designed for non-programmers lacking experience with AR interfaces. RadEd [17] 
features a new  web-based teaching  framework  with  an integrated smart editor to 
create  case-based  exercises  for  image  interaction,  such  as  taking  measurements, 
attaching labels and select specific parts of the image. It facilitates a framework as 
an  additional  tool  in  complex  training  courses  like  radiology.  ARTIST  [6]  is  a 
platform which provides methods and tools for real-time interaction between hu-
man and non-human characters to generate reusable, low cost and optimized MR 
experiences. Its aim is to develop a code-free system for the deployment and im-
plementation  of  MR  content,  while  using  semantically  data  from  heterogeneous 
resources. The mentioned solutions provide developing environments to generate 
MR experiences, however they lack of advanced authoring tools and educational 
curriculum to support advanced educational - training scenarios. 

2.  Mixed Reality Sample app application 

Our Sample app is a MR application, in which users are presented with basic ex-
amples  of  all  the  functionalities  that  our  SDK  supports.  It  is  considered  to  be  a 
playground  for  MR.  We  consider  it  as  a  room,  where  the  users  can  experiment 
with the basic mechanics of our SDK, try them, and even create their own using 
our tools. In addition, these mechanics can be applied to other scenes as well, as 
they are not bound only on the specific application. The users can experience sim-
ple  examples  of  different  mechanics  interacting  with  many  objects  in  the  scene 
(pick them up, hold them, even throw them), thanks to our “interactable item” util-
ity that our SDK provides. They can use this functionality to possible objects that 
they will create, in order to interact with  them and be able to move them around 
the scene with their virtual hands. 

Another interesting mechanic that our SDK contains, is the virtual hands. They are 
automatically set up to follow and respond correctly to different kinds of control-
lers (Oculus, Vive, Mixed Reality, etc.). Users can easily set these hands to inter-
act with objects of their choice. The hands are animated to perform the appropriate 
gesture that someone would perform in real life when the corresponding controller 
button is pressed. This functionality increases the realism and thus the  feeling of 
presence in the scene.  

An Inverse Kinematic (IK) object, a lamp, can be found in our MR scene, on the 
table. The users are free to interact with it, like any other movable objects in the 
scene  and  note  how  the  IK  mechanic  works  on  the  lamp  (the  base  of  the  lamp 
moves with respect to the top of the lamp and vice versa), adding a great value of 
realism. In addition, users can use the presented IK mechanism to objects of their 
choice, including not only “lifeless” objects but virtual characters as well.  

 
 
 
6  

The Sample app application, offers a variety of interactive tasks (Actions). There 
is a variety of Actions where users can experiment with. Based on these Actions, 
users  are  able  to  create  their  own  custom  actions  in  the  same  or  a  new  project 
simply by importing the SDK to their new project. Moreover, it provides the users 
with some basic tools (scalpel, mallet, scissors), which are in the scene during the 
whole operation and can be used to perform different kinds of actions. Based on 
these tools the  users can even create their own tools to accomplish possible new 
actions that they will generate, according to their needs. 

For  the  completion  of  some  actions,  (insertion  of  objects  into  specific  positions) 
extra information is needed to inform user how to procced and execute this inter-
active task. This is done with another feature that our SDK provides, which is the 
holograms. They can be easily set up and can be used to indicate the correct posi-
tion and rotation of an object during an  “insert action”, or the movement that the 
users have to do with an object in order to complete a  “use action” successfully. 
The types of actions mentioned here are described later in this work. 

A strong feature of our SDK is the UIs and the aidlines that it provides. Users can 
easily  create  their  own  UIs,  based  on  our  prototypes  as  well  adding  advanced 
functionalities to occur after pressing a UI button. The UIs are fully supported and 
interact  with  the  virtual  hands.  The  aidlines  are  mainly  used  to  guide  inexperi-
enced  users  into  tricky  tasks.  They  include  arrows,  which  can  point  to  the  pre-
ferred direction, followed by a message to inform the users of what they need to 
do. A typical example would be the aidline pointing to a tool and a message telling 
the users to pick up the specific tool and use it in a proper way. 

Fig. 2 A basic example of an "insert action" 

In  Fig.  2  above,  a basic  example  of  the  “Insert  Action”  for  Sponza  can  be  seen. 
The class inherits the InsertAction interface and implements the Initialize method, 
which is called at the beginning of the action. The method SetInsertPrefab, instan-
tiates the interactable 3D model of Sponza in the scene and sets its final position 

 
 
 
 
7 

to be the one indicated by the SponzaFinal prefab in this case. SetHoloObject gen-
erates a hologram (of the Sponza model, in this example) to the position where the 
interactable  model  should  be  set.  Finally,  the  SetAidLine  method  generates  the 
aidline prefab, the name of which is passed as an argument that becomes visible in 
the scene. Based on this prototype, the users can create their own actions from any 
3D models, prefabs, holograms, aidlines etc. 

Our UIs also offer another great feature. They can be used as notifications, provid-
ing helpful information to the users, as warnings, to inform them that they are pos-
sibly doing (or already did) something wrong (which may have not severe conse-
quences  but  it  is  advisable  to  avoid  it)  and  as  errors  to  let  them  know  that  their 
actions caused a severe/critical error. With only a few lines of code, the users can 
generate whichever type of UI notification they prefer alongside a message. 

Our  Sample  app  application  also  supports  a  multiplayer  collaborative  environ-
ment. Many users (up to 7+) can join a virtual room, in order to collaborate with 
each  other  and  complete  actions  together.  To  support  a  large  number  of  partici-
pants in the virtual environment, we implemented our custom Conformal Geomet-
ric  Algebra  (CGA)  GPU  interpolation  engine  to  reduce  the  data  transfer  on  the 
network. Furthermore, thanks to dual quaternions, we accomplish smoother trans-
lations and rotations for our virtual objects. Since these functionalities are a part of 
our SDK, any user will be able to use them to create their online sessions of their 
application, without encountering many difficulties.  

The application described in  this  work, is an  AR version  of this playground,  the 
main  objective  of  which  is  to  help  developers  get  started  with  the  basic  compo-
nents of our SDK. 

3.  Integrate AR features into M.A.G.E.S. platform 

The M.A.G.E.S. platform was designed for VR environments, thus integrating AR 
support as a part of the SDK was a challenging task. Virtual Reality reflects a fully 
digital environment to enhance immersion and embodiment, presenting a different 
reality  through  the  virtual  environment.  On  the  other  hand,  Augmented  Reality 
blends the virtual and the real world through the selective rendering of holograph-
ic assets. It is important to pay attention to the augmentations we add on top of the 
real environment, in order for them to blend well with their surroundings, obey the 
laws  of  gravity  and  match  the  environmental  illumination.  As  it  seems,  AR  and 
VR have not much in common. They present the digital content following differ-
ent principles and design patterns. The figure below illustrates a screenshot from 
the VR application featuring the interaction with the North Gate of the palace of 
Knossos. 

 
 
 
 
8  

Fig. 3 Interacting with the north gate of Knossos Palace 

Our goal is to transform our platform into a cross-reality SDK capable to integrate 
multiple technologies (AR/VR/MR) within the same system. In the following sec-
tions we will discuss how we integrated AR support into a platform designed for 
VR. 

3.1 The Modular Device Controller  

The  initial  challenge  is  to  design  the  system  to  manage  and  select  the  deployed 
device. We want our application to be cross-platform, able to generate executables 
for  different  devices  and  operating  systems.  M.A.G.E.S  was  built  on  Unity3D 
game engine to enable this feature. Unity may support exporting to different plat-
forms  but  we  have  to  change  specific  application-wise  parameters  and  design 
principles to truly support this mechanic. Our goal was to develop a modular plat-
form  capable  to  support  different  realities  without  any  parametrization  from  the 
developer’s side, thus we had to integrate the AR support to the core engine of our 
system. 

Our  architecture  contains  different  components  to  support  multiple  devices  and 
technologies.  We  manage  the  deployed  devices  through  the  device  controller 
module,  which  implements  the  callbacks  for  different  buttons,  analog  sticks  and 
gestures. As an example, we integrated SteamVR into our device controller mod-
ule to support all the compatible VR headsets like Oculus, HTC VIVE, Microsoft 
Mixed  Reality  and  many  more.  However,  HoloLens  does  not  support  SteamVR, 
instead they offer  HoloToolKit as the native  API to manage the device. To inte-
grate  HoloLens  into  our  platform,  we  implemented  the  HoloLens  controller,  a 
module  which  derives  from  the  generic  device  controller  and  manages  the 
callbacks  from  the  HoloLens  controller.  However,  HoloLens  does  not  have  a 

 
 
 
 
physical controller. Instead, users interact with the holographic environment using 
hand gestures. For this reason, we implemented HoloLens controller to support all 
the gestures and interactions from HoloToolKit. The diagram below illustrates our 
modular device controller architecture which supports input from various headsets 
and technologies. 

9 

Fig. 4 The Device Controller implementation diagram featuring the AR controller (left) 

and the VR controllers (right) 

This implementation offers great flexibility over the platform enabling the project 
to  run  in  different  devices  with  minimal  changes.  To  utilize  the  modular  device 
controller,  right  before  building  the  executable,  we  need  to  select  the  device  in 
which  we  are  deploying  the  application.  Device  controller  is  represented  as  a 
component in the Unity3D editor, thus we can select the deployed device from a 
convenient drop down menu we designed for this reason. 

3.2 Interaction with holographic objects 

The M.A.G.E.S. platform gives user the ability to interact with the 3D objects and 
the virtual environment through the interaction module we integrated. To support 
runtime interaction with the virtual objects, we integrated NewtonVR physics sys-
tem as the main engine to handle the manipulation of 3D assets.  The interaction 
physics engine of NewtonVR is velocity based, which means that interaction is 
not relied on parenting objects to user’s hand but the objects are connected to 
the hands according to their current velocity. This approach gives the sensation 
of a  more  natural  movement than the  parenting  mechanism.  NewtonVR  has  a 
port  for  Unity  engine  with  an  active  community  and  a  significant  number  of 
successful VR applications. 

Our  interaction  module  is  closely  related  to  the  device  controller  to  support 
different  headsets  and  devices.  To  integrate  HoloLens  platform  into  ours,  we 
had to integrate the interaction system of  HoloToolKit with  NewtonVR. Cur-

 
 
 
 
10  

rently, the most common way to interact with holographic objects through Ho-
loLens is  by  using the  pinch  gesture to  grab  a  hologram and  change  its  posi-
tion  within  the  virtual  environment.  This  method  utilizes  a  simple  parenting 
mechanic to grab the object just by switching the parenting of the object to be 
the user’s hand position. This mechanic is simple, but offers limited function-
ality and poor user experience. To improve the parenting grab mechanic and to 
unify  the  interaction  mechanic  in  our  platform,  we  integrated  the  HoloLens 
gesture grabbing system to the NewtonVR system, that we already support in 
our  platform.  The  diagram  below  illustrates  the  interaction  module  which 
manages the input from HoloLens to send feedback to the platform. 

Fig. 5 Diagram of the interaction module with additions to support HoloLens gestures 

Our  methodology  was  to  create  an  intermediate  module  between  the  device 
controller and the HoloToolKit. This module is the HoloLens Input Handler. It 
implements  three  interfaces  from  HoloToolKit  to  link  the  inputs  from  Ho-
loLens  gestures  and  vocal  controllers  directly  to  our  platform.  For  example, 
OnInputUp method is called when user is rising his pointer, indicating the first 
stage  of  tapping  gesture.  When  HoloLens  Input  Handler  recognizes  this  ges-
ture, it automatically calls the appropriate  Device Controller method to signal 
our platform that the gesture was performed. 

3.3 Port an AR application to a VR system 

At this point, we implemented the interaction module to handle the virtual assets 
with natural gestures. The next step is to reconstruct the augmented environment, 
where the user will interact with. The Sample App application was designed for a 
VR  environment,  thus  the  visualization  of  the  3D  assets  and  the  virtual  room  is 
fully digital. However, in AR applications, the rendered environment blends with 
the virtual and the real world since the augmentations do not cover the entire Field 
of View, but they are placed in key locations respecting physical objects. 

 
 
 
 
 
 
 
 
11 

Fig. 6 The VR version of our Sample Application 

In more detail, to design the AR application, we have to keep only a small amount 
of digital assets and delete the  majority of them  to  make  room  for the  real envi-
ronment.  For  this  reason,  we  kept  only  the  wooden  table  from  the  sample  app 
room  along  with  the  interactable  items  and  the  priest  of  Asinou.  In  addition,  to 
improve the realism of the holographic assets, we integrated a shadow plane under 
each object to replicate a real time shadow. This technique is simple enough but 
enhances the depth of field including an additional layer of illumination. 

Another  module  we  need  to  consider  when  changing  the  deployed  medium 
(AR/VR) is the camera object,  which represents the  used HMD. For this reason, 
we integrated the HoloLens camera from the HoloToolKit into our system to sup-
port  both  cameras  and  technologies.  In  this  way,  developers  can  set  the  camera 
with a single click without the need to import any additional packages or libraries 
transforming sample app into a plug and play system.   

4.  An Interactive AR Cultural Heritage Application 

In this work, an innovative way of supporting AR in the VR version of our cultur-
al heritage application is presented, in which the construction and the restoration 
of archaeological sites is simulated. This application combines education with en-
tertainment  featuring  a  serious  game  experience.  The  users,  who  complete  this 
simulated procedure will learn about the buildings from within the archaeological 
sites,  since  they  experienced  their  reconstruction/restoration.  This  is  very  im-
portant for the preservation of cultural heritage, because in case such an applica-

 
 
12  

tion is installed in the museum, the users will more likely want to visit the muse-
um  again,  in  order  to  try  the  application  once  more  retaining  more  knowledge 
from  their  additional  visit(s).  For  this  reason,  we  enhanced  the  application  with 
gamification elements [7] making the experience more appealing. 

In this application, the construction of Knossos and the restoration of Sponza are 
simulated, but it can also be extended to support more monuments in the future. In 
terms of Knossos, users can pick up the indicated parts of Knossos (for which hol-
ographic representations are used to guide the users to the correct parts) and place 
them in the correct positions, again indicated by the use of holographic representa-
tions. In terms of Sponza, users have to restore the building of Sponza, by placing 
it  in  the  annotated  position  and  then  executing  the  appropriate  actions  to  restore 
the damaged building to perfect condition. If the user makes an error, it is indicat-
ed in the application by applying temporarily red color to the area of the error. All 
the actions that are referenced above, must be executed in a specific order, in order 
to complete the training scenario. 

The featured types of actions in this application are described below: 

 

Insert Action: The user has to pick up an object and place it in the posi-
tion indicated by the corresponding hologram. 

  Remove Action: The user has to pick up the flashing object and move it 

away from its current position. 

  Tool Action: The user has to execute a specific tool-driven action (cut-

ting with scissors or scalpel, for instance). 

  Use Action: The user has to execute an action using an object that is not 
a registered tool in our SDK (wiping over a surface with a cloth, for in-
stance). 

Fig. 7 Main application. Restoration of Knossos (right) or Sponza (left) 

 
 
 
 
 
Quizzes are also a part of the application. When starting the application, users are 
asked “Where is Sponza located?” and  they  have to choose one of the three pro-
posed countries (each represented by its flag) by  gazing at it and performing the 
tap  gesture.  Once  the  users  choose  an  answer,  both  a  visual  (red/green  color)  as 
well as an audio feedback is given to them indicating whether they chose the cor-
rect answer or not. 

13 

Fig. 8 The user (top right) answering the question by performing the "tap" gesture 

The application also features a life-sized priest, from the Asinou church, which is 
located in Cyprus. The priest stands behind the table all the time, watching the us-
ers executing the actions. Also, on the table in front of the priest, there is a minia-
ture version of the Asinou church. The users can pick it up and inspect it by gaz-
ing  at  it  and  using  the  pinch  gesture.  Once  the  church  is  picked  up,  the  priest 
notices it and starts speaking, informing users about the history of the church. As 
he speaks, the priest moves his arms and his body posture in general, to emphasize 
the important information in his sayings.  

Fig. 9 The user moving an object using the "pinch" gesture 

 
 
 
 
14  

Fig. 10 The priest telling history of the Asinou church (model on the table) 

The priest mentioned above, is the real priest of the Asinou church. In fact, he was 
reconstructed by scanning the real priest with the occipital structure sensor, a sen-
sor  that  connects  to  an  iPad  and  is  able  to  scan  real  3D  geometry  [9].  After  the 
scanning procedure, the model of the priest was improved with additional editing 
software (both in terms of geometry as well as texturing) to come correct scanning 
faults. 

This application contributes a lot to the preservation of cultural heritage. Specifi-
cally,  it  provides  the  users  with  the  chance  to  build  or  restore  an  archaeological 
monument themselves. By allowing the users to be actively involved to the recon-
struction/restoration  of  the  archaeological  monuments,  they  gain  even  more 
knowledge by the end of the day as the application becomes more interesting and 
appealing. 

 
 
 
 
15 

Fig. 11 The user restoring the site by performing the action that the hologram indicates 

The  installation  of  gamified  applications  like  this  to  museums,  will  lead  to  even 
greater attraction of people of younger age to museums. Nowadays, younger peo-
ple  believe that a visit to a museum is  tiresome  and only a few of them actually 
want to visit a museum. This is mostly because they do not have something, with 
which they will be able to interact. By offering interaction through immersive ap-
plications, the knowledge that museums offer will spread more quickly to younger 
ages, which will make it easier to preserve cultural heritage across generations. 

5.  Conclusions and Future Work 

In this work, we presented a first integration of AR features in our SDK. By using 
the  HoloToolKit  for  HoloLens  and  implementing  an  input  handler  for  it  that 
“communicated”  with  our  Device  Controller,  we  were  able  to  use  the  gestures 
supported by HoloLens, in order to interact with different objects in our applica-
tion. The main content of this application is the restoration of archaeological sites, 
in which the users can restore or reconstruct Sponza and Knossos respectively, by 
using gestures, which are mapped to the respective buttons of the controllers that 
were used in the VR mode. It is a great approach for the preservation of cultural 
heritage, since it provides the ability to be used in the real monuments, since the 
users are able to see the real world as well. By allowing, for instance, the visitors 
of the Knossos archaeological site to use this application on the site itself, during 
their visit, it will make their experience more interesting and fun, something that 
will increase the chance that they will visit Knossos again (in this specific exam-
ple), or that they will recommend this site to others.  

 
 
 
16  

We also presented a 3D reconstructed virtual character for storytelling. This char-
acter tells the story of a monument (Asinou church in Cyprus), by using appropri-
ate lips and body animations to make the interaction more realistic. This character, 
the priest, was reconstructed out of the real priest of the Asinou church. The fact 
that a virtual person exists in the application and can interact with the users, is an 
element that increases the feeling of presence and the realism of AR, bringing us 
one step closer to True AR. 

During our work, we faced some limitations regarding the hardware of the holo-
graphic device.  HoloLens has a rather small  field of view,  which does not allow 
the users to see the entire virtual scene, something not present in VR or in real life. 
Instead  they  are  able  to  see  the  virtual  scene  through  a  small  window  (about  35 
degrees), which repels them from fully immersing into the virtual experience. An-
other limitation is the processing power, as it is significantly lower than a desktop 
computer, in which the VR version of this application was running flawlessly. As 
a result, a scene that contains a large number of 3D objects (with complex geome-
try) may cause frame drops in HoloLens. Thus, it would be useful to create an al-
gorithm/plugin in the future, which will be able to revise the complex geometry of 
a  3D  object/model  automatically,  that  it  will  be  able  to  run  in  devices  with  less 
processing  power.  Lastly,  we  encountered  a  challenge  during  the  port  of  actions 
that  required  tools,  which  respectively  needed  a  specific  button  to  be  pressed  in 
order  for  them  to  be  used,  i.e.  scissors.  A  possible  solution  for  this  limitation  is 
discussed later in this section. 

There are many interesting and useful goals to be considered in the future. Since it 
is  the  first  time  that  our  application  is  transferred  to  AR,  the  walls,  ceiling  and 
floor that were surrounding the virtual room are removed, as they did not contrib-
ute  in  realizing  the  main  concept  and  features  of  AR.  For  the  future,  it  is  worth 
considering a mechanism, which will be able to detect and do this automatically. 
By choosing, for instance, whether the project/scene is considered to be in AR or 
not, or by marking the objects of the scene that we would like to remain in the AR 
mode, the same version of the application will be able to run correctly both in AR 
and VR. Also, another feature that we would like to add, is a plugin, which will set 
up the scene automatically (setting up all the necessary objects for the reality that 
interests  us  at  the  moment).  The  programmer  will  only  have  to  choose  whether 
this is an AR project or not and the operating system that the project will run on, 
in order for it to set up all the required objects for the specific platform. This fea-
ture will be a meaningful addition to our SDK, as it will allow us to quickly build 
any project to whichever reality we want, without setting it “by hand”.  

Also, another important feature that we would like to add to the AR version is the 
online  multiplayer  part.  In  our  SDK,  more  than  one  users  can  join  the  virtual 
room, in order to cooperate and reconstruct the building at the same time, by help-
ing each other or with the guidance of an expert. That feature is missing from our 
AR version, and it is of high priority to support it in AR as well. Apart from the 

 
 
 
17 

online cooperation of the users from anywhere in the globe, we would also like to 
try mixed online sessions with users that are using the VR version of the applica-
tion  and  users  using  the  AR  version.  The  users  in  VR  will  be  able  to  roam  the 
room and execute the actions with AR users. This cross-reality online session is a 
great novelty both in the field of VR/AR, as well as cultural heritage preservation. 

In  the  future  we  aim  to  conduct  a  qualitative  evaluation  survey  to  examine  and 
document the capabilities of our system in real use. Our evaluation process will be 
based on [18] as this model reflects better our system functionalities and usage. 

Finally, we aim to extend our SDK for HoloLens, in order to use voice commands, 
which are supported by HoloToolKit. Currently, the actions can be completed on-
ly by performing pinch and tap gestures. As HoloLens does not have any control-
lers, it is difficult to simulate the action where the users hold a tool with the grip 
button and press the trigger button to activate it at the  same time. By supporting 
voice commands, the users will be able to hold a tool with their hand and say, for 
instance,  a  word  like  “use”,  in  order  to  use  the  tool.  The  support  of  voice  com-
mands is not limited only to this, but can be more general, like giving the users the 
ability to execute the actions with their voice, or even more interestingly to start a 
conversation with the priest through dialogue-based interaction, asking him ques-
tions about the church, in order for him to provide the answers. 

A video of this work is available here: 
https://www.dropbox.com/s/eqzpsp0xbspzhik/TrueAR.mp4?dl=0 

References 

[1]  Abate,  A.,  Barra,  S.,  Galeotafiore,  G.,  Díaz,  C.,  Aura,  E.,  Sánchez,  M.,  Mas,  X.,  Vendrell 
Vidal, E. (2018). An Augmented Reality Mobile App for Museums: Virtual Restoration of a 
Plate  of  Glass:  7th  International  Conference,  EuroMed  2018,  Nicosia,  Cyprus,  October 29–
November 3, 2018, Proceedings, Part I. 10.1007/978-3-030-01762-0_47.  

[2]  Geronikolakis,  E.,  Tsioumas,  M.,  Bertrand,  S.,  Loupas,  A.,  Zikas,  P.,  Papagiannakis,  G., 
“New Cross/Augmented Reality Experiences for the Virtual Museums of the Future”, in Dig-
ital  Heritage.  Progress  in  Cultural  Heritage:  Documentation,  Preservation,  and  Protection, 
Springer International Publishing, 518—527, 2018. 

[3] Ioannides, Marinos, et al., eds. Digital Heritage. Progress in Cultural Heritage: Documenta-
tion,  Preservation,  and  Protection:  7th  International  Conference,  EuroMed  2018,  Nicosia, 
Cyprus, October 29–November 3, 2018, Proceedings. Vol. 11196. Springer, 2018. 

[4] Ioannides, M., Magnenat-Thalmann, N., Papagiannakis, G., (Eds), Mixed Reality and Gami-
fication for Cultural Heritage, Springer-Nature, DOI: 10.1007/978-3-319-49607-8, 2017 
[5] Jung, Timothy & Tom Dieck, M. Claudia & Lee, Hyunae & Chung, Namho. (2016). Effects 
of Virtual Reality and Augmented Reality on Visitor Experiences in Museum. 10.1007/978-
3-319-28231-2_45. 

[6] Kotis K. ARTIST - a reAl-time low-effoRt mulTi-entity Interaction System for creaTing re-
usable  and  optimized  MR  experiences  (2019).  Research  Ideas  and  Outcomes  5:  e36464. 
https://doi.org/10.3897/rio.5.e36464 

 
 
 
18  

[7] Liarokapis, F., Petridis, P., Andrews, D., de Freitas, S. Multimodal Serious Games Technolo-
gies  for  Cultural  Heritage,  Mixed  Reality  and  Gamification  for  Cultural  Heritage,  Part  V, 
Springer International Publishing, 371-392, 2017. 

[8] Liarokapis, F., Sylaiou, S., Basu, A., Mourkoussis, N., White, M., Lister, P.F. An Interactive 
Visualisation  Interface  for  Virtual  Museums,  Proc.  of  the  5th  International  Symposium  on 
Virtual  Reality,  Archaeology  and  Cultural  Heritage,  Eurographics  Association,  Brussels, 
Belgium, 6-10 Dec, 47-56, 2004. 

 [9] Papaeftymiou M., Kanakis E.M., Geronikolakis E., Nochos A., Zikas P., Papagiannakis G., 
“Rapid Reconstruction and Simulation of Real Characters in Mixed Reality Environments”, 
Digital Cultural Heritage Lecture Notes in Computer Science, Vol. 10605, 267-276, 2018.  
 [10] Papagiannakis, G., Geronikolakis, E., Pateraki, M., Bendicho, V.M., Tsioumas, M. Sylaiou, 
S.,  Liarokapis,  F.,  Grammatikopoulou,  A.,  Dimitropoulos,  K.,  Grammalidis,  N.,  Partarakis, 
N.,  Margetis,  G.,  Drossis,  G.,  Vassiliadi,  M.,  Chalmers,  A.,  Stephanidis,  C.,  Thalmann,  N. 
(2018). Mixed Reality Gamified Presence and Storytelling for Virtual Museums. 

 [11] Papagiannakis, G., Lydatakis, N., Kateros, S., Georgiou, S., Zikas, P. Transforming medi-
cal education and training with vr using M.A.G.E.S. In SIGGRAPH Asia 2018 Posters, SA 
’18, pages 83:1–83:2, New York, NY, USA, 2018. ACM. 

[12]  Pfeiffer-Leßmann,  N.,  Pfeiffer,  T.  ExProtoVAR:  A  Lightweight  Tool  for  Experience-
Focused  Prototyping  of  Augmented  Reality  Applications  Using  Virtual  Reality.  (2018) 
10.1007/978-3-319-92279-9_42.  

 [13]  Sandor,  C.,  Fuchs,  M.,  Cassinelli,  A.,  Li,  H.,  Newcombe,  R.,  Yamamoto,  G.,  Feiner,  S. 

(2015). Breaking the Barriers to True Augmented Reality. 

 [14] Stefanidi, E., Arampatzis, D., Leonidis, A., Papagiannakis, G. (2019). BricklAyeR: A Plat-
form for Building Rules for AmI Environments in AR. 10.1007/978-3-030-22514-8_39. 
[15]  Sylaiou,  S.,  Mania,  K.,  Paliokas,  I.,  Pujol-Tost,  L.,  Kilintzis,  V.,  Liarokapis,  F.  Exploring 
the  educational  impact  of  diverse  technologies  in  online  Virtual  Museums,  International 
Journal of Arts and Technology (IJART), Inderscience Publishers, 10(1): 58-84, 2017. 

[16] White, M., Mourkoussis, N., Darcy, J., Petridis, P., Liarokapis, F., Lister, P.F., Walczak, K., 
Wojciechowski, R., Cellary, W., Chmielewski, J., Stawniak, M., Wiza, W., Patel, M., Steven-
son, J., Manley, J., Giorgini, F., Sayd, P., Gaspard, F. ARCO-An Architecture for Digitiza-
tion,  Management  and  Presentation  of  Virtual  Exhibitions,  Proc.  of  the  22nd  International 
Conference on Computer Graphics (CGI'2004), IEEE Computer Society, Hersonissos, Crete, 
June 16-19, 622-625, 2004. 

 [17]  Pau  Xiberta,  Imma  Boada,  A  new  e-learning  platform  for  radiology  education  (RadEd), 
Computer  Methods  and  Programs  in  Biomedicine,  Volume  126,  2016,  Pages  63-75,  ISSN 
0169-2607, 10.1016/j.cmpb.2015.12.022. 

[18] 

Qualitative 

Evaluation 

https://wmich.edu/sites/default/files/attachments/u350/2018/qual-eval-patton.pdf, 
cessed on 12/10/2019. 

Checklist, 
ac-
last 

[19] Zikas, P., Bachlitzanakis, V., Papaefthymiou, M., Kateros, S., Georgiou, S., Lydatakis, N., 
Papagiannakis G., Mixed reality serious games for smart education. In European Conference 
on Games Based Learning 2016. ECGBL’16, 2016. 

  
 
