8
1
0
2

n
u
J

0
2

]
T
G
.
s
c
[

1
v
6
1
0
8
0
.
6
0
8
1
:
v
i
X
r
a

Equilibrium and Learning in Queues with
Advance Reservations

Eran Simhon and David Starobinski

Boston University, College of Engineering

June 3, 2021

Abstract

Consider a multi-class preemptive-resume M/D/1 queueing system
that supports advance reservations (AR). In this system, strategic cus-
tomers must decide whether to reserve a server in advance (thereby gain-
ing higher priority) or avoid AR. Reserving a server in advance bears a
cost. In this paper, we conduct a game-theoretic analysis of this system,
characterizing the equilibrium strategies. Speciﬁcally, we show that the
game has two types of equilibria.
In one type, none of the customers
makes reservation. In the other type, only customers that realize early
enough that they will need service make reservations. We show that the
types and number of equilibria depend on the parameters of the queue
and on the reservation cost. Speciﬁcally, we prove that the equilibrium
is unique if the server utilization is below 1/2. Otherwise, there may be
multiple equilibria depending on the reservation cost. Next, we assume
that the reservation cost is a fee set by the provider. In that case, we show
that the revenue maximizing fee leads to a unique equilibrium if the uti-
lization is below 2/3, but multiple equilibria if the utilization exceeds 2/3.
Finally, we study a dynamic version of the game, where users learn and
adapt their strategies based on observations of past actions or strategies
of other users. Depending on the type of learning (i.e., action learning vs.
strategy learning), we show that the game converges to an equilibrium in
some cases, while it cycles in other cases.

1 Introduction

Many services, such as health care, cloud computing and banking, combine both
a ﬁrst-come-ﬁrst-served policy and advance reservations (AR). Advance reser-
vations beneﬁt a service provider since knowledge about future demand can im-
prove resource management and quality-of-service (e.g., Charbonneau and Vokkarane
(2012)). Customers are also motivated to reserve in advance, since it decreases
their expected waiting time. However, typically, reservations bear an additional

1

 
 
 
 
 
 
cost for customers. This cost can be a reservation fee, the time or resources
required for making the reservation, the cost of ﬁnancing advance payment, or
the cost of cancellation if needed.

Since the decision of a customer, about reserving a server in advance or not,
aﬀects the waiting time of other customers, game theory is the solution of choice
for studying such systems. Although there exists a rich literature on advance
reservations, works that study advance reservation systems as a game are rare.
The strategic behavior of customers in a system that support AR is studied
in Simhon and Starobinski (2014) and Simhon et al. (2015). These two papers
study a loss system, i.e., a system with no queue. In this paper, instead, we
focus on a queueing system (i.e., customers that encounter a busy server wait
for service). This leads to a diﬀerent model and, interestingly, more explicit
results. We show that the server utilization (traﬃc load) plays in key role in
the behavior of the system and, speciﬁcally, in the number of equilibria.

We assume that the time axis is divided into two time-periods: a reservation
period and a service period. This restriction simpliﬁes the analysis and is com-
mon in the literature of advance reservations (e.g., Virtamo (1992), Yessad et al.
(2007) and Syed et al. (2008)). It can also be found in real life applications. For
example, some service providers do not allow same-day-reservations.

During the reservation period, each customer realizes that he/she will need
service at a speciﬁc future time point. Upon such a realization, the customer
decides whether or not to make a reservation. Customers are assumed to be
strategic and rational. Thus, a customer will make a reservation only if it
reduces his/her expected total cost which consists of the reservation cost (if
making a reservation) and the cost of waiting.

We start the analysis by ﬁnding the equilibrium structure of the game. We
show that there are two possible types of equilibria. In the ﬁrst type, none of
the customers makes AR, while in the second type customers that realize early
enough that they will need future service make AR. We refer to those two types
of equilibria as none-make-AR and some-make-AR, respectively. We show that
if the utilization of the queue (i.e., the ratio between the arrival rate and the
service rate) is smaller than 1/2, then the game has a unique equilibrium. Low
AR costs lead to a some-make-AR equilibrium, while high AR cost lead to a
none-make-AR equilibrium. If the utilization is greater than 1/2, however, there
also exists a middle range of AR cost such that any cost in that range leads to
three equilibria, namely one none-make-AR and two some-make-AR equilibria.
Next, we assume that the AR cost is a fee charged by the service provider.
We analyze the game from the prospective of a provider aiming to maximize its
revenue from AR fees. We show that if the utilization is greater than 2/3, then
the revenue maximizing fee leads to multiple equilibria. Thus, charging that
fee may yield the highest possible revenue for the provider but possibly also no
revenue.

Finally, we study a dynamic version of the game. We use best response
dynamics (as in Fudenberg (1998)) and distinguish between strategy-learning
and action-learning. In strategy-learning, customers obtain information about
strategies adopted at previous steps, while in action-learning, customers esti-

2

mate the previous strategies by obtaining information about the actions taken
at previous steps. Our analysis shows that starting with any initial belief about
customers behavior (i) when implementing strategy-learning, the system always
converges to an equilibrium; (ii) when implementing action-learning, the sys-
tem converges to a none-make-AR equilibrium if it exists and cycles otherwise;
(iii) if the equilibrium is unique, more customers, on average, make reservations
under action-learning than under strategy-learning.
The rest of the paper is structured as follows.

In Section 2, we review
In Section 4, we
related work.
ﬁnd the equilibrium structure of the game. In Section 5, we derive the revenue
maximizing fee and resulting equilibria.
In Section 6, we deﬁne and analyze
dynamic versions of the game. Section 7 concludes the paper and suggests
directions for future research.

In Section 3, we formally deﬁne the game.

2 Related Work

Strategic behavior in queues (also known as queueing games) was pioneered by
Naor (1969) and has been studied extensively since. In that seminal paper, the
author studies an M/M/1 queue where customers decide whether to join or balk
after observing the queue length. Hassin and Haviv (2003) and Hassin (2016)
conduct an extensive review of the ﬁeld of queueing games. Most related to
our work, Balachandran (1972) analyzes strategic behavior in priority queues
and Qiu and Zhang (2016) and Hayel et al. (2016) study strategic behavior in
M/D/1 queues. None of these works consider advance reservations.

Advance reservations have been researched from various other perspectives
in the literature, including scheduling and routing algorithms for communica-
tion networks, methods for revenue maximization, and performance analysis of
queueing systems. The work in Wang et al. (2013) describes a distributed archi-
tecture for advance reservation, while Smith et al. (2000) proposes a scheduling
model that supports AR and evaluates several performance metrics. The work
in Virtamo (1992) analyzes the impact of advance reservations on server uti-
lization under a stochastic arrival model, and Gu´erin and Orda (2000) analyzes
the eﬀect of AR on the complexity of path selection. In Weatherford (1998),
the author reviews models for revenue management of perishable assets, such
as airline seats and hotel rooms, that extend to various industries. The work
in Reiman and Wang (2008) considers admission control strategies in reserva-
tion systems with diﬀerent classes of customers, while Bertsimas and Shioda
(2003) deals with policies for accepting or rejecting restaurant reservations.
The eﬀects of overbooking, cancellations and regrets on advance reservations
are studied in Liberman and Yechiali (1978), Quan (2002), Nasiry and Popescu
(2012). None of these prior works considers the strategic behavior of customers
in making AR, namely, that decisions of customers are not only inﬂuenced by
prices and policies set by providers but also by their beliefs about decisions of
other customers.

Simhon and Starobinski (2014) introduces AR games.

In that paper, the

3

authors consider a loss system (i.e., customers that ﬁnds all servers busy leave).
The authors show that the game may have multiple equilibria, where in one
equilibrium the number of reservations is a random variable, while in the other
equilibrium, none of the customers makes reservation. In Simhon et al. (2015),
the authors study a dynamic version of the game. The main diﬀerence between
the model of our paper and the model presented in Simhon and Starobinski
(2014) and Simhon et al. (2015) is that our paper focus on a queuing system,
while these papers focus on a loss system. Speciﬁcally, our paper shows that
the server utilization plays a key role in determining the number and structure
of equilibria. The characterization of the equilibrium strategies in our paper is
also much more explicit than that provided in Simhon and Starobinski (2014).
The concept of learning an equilibrium is rooted in Cournot’s duopoly model
Cournot (1897) and has been extensively researched since. Traditionally, learn-
ing models are used for ﬁxed-player games (i.e., the same players participate at
each iteration), see Lakshmivarahan (1981), Fudenberg (1998) and Milgrom and Roberts
(1991). Several papers have focused on learning under stochastic settings. For
example, in Liu and van Ryzin (2011) customers choose between buying a prod-
uct at full price or waiting for a discount period. Decisions are made based
on observing past capacities. Altman and Shimkin (1998) analyze a processor
sharing model. In this model, customers choose between joining or balking after
observing the history. Zohar et al. (2002) present a model of abandonment from
unobservable queues. The decision is based on the expected waiting time which
is formed through accumulated experience. Fu and van der Schaar (2009) as-
sume that the same set of players participate in a bid for wireless resources at
each stage. However, the number of packets that need to be transmitted at each
iteration is a random variable.

Diﬀerent learning models diﬀer by their learning rules. A learning rule de-
ﬁnes what kind of information players gain and how they use it. In this paper,
we focus on best response dynamics. According to this rule, which is rooted in
Cournot’s work, players observe the most recent actions adopted by other play-
ers and assume that the same actions will be adopted at the next step. Another
popular learning rule is ﬁctitious play which assumes that at each iteration,
players observe actions made by other players at all previous steps and best-
respond to the empirical frequency of observed actions. This rule was suggested
by Brown (1951). In contrast, Littman (1994) and Tan (1993) assume that play-
ers only observe their own payoﬀs and learn by trial and error. Reinforcement
learning is an example of such a learning rule.

Other relevant work includes Niu et al. (2012), which presents a theoretical
model for pricing cloud bandwidth reservations, in order to maximize social wel-
fare. The reservation fee of each customer is a function of his/her guaranteed
portion instead of the actual amount of resources reserved, as considered in our
models as well as many practical services.
In Menache et al. (2014), the au-
thors consider the problem of deciding which type of service a customer should
buy from a cloud provider. More speciﬁcally, that study considers two options:
on-demand, which means paying a ﬁxed price for service, and spot, a service
oﬀered by Amazon EC2 that allows users to bid for spare instances. They pro-

4

pose a no-regret online learning algorithm to ﬁnd the best policy. Our approach
complements this work in several ways. First, our framework considers advance
reservations (similar to Reserved Instances in Amazon EC2). Second, our mod-
els integrate the strategic behavior of all participants (i.e., both the customers
and the provider).

3 Game Description

We consider a preemptive-resume M/D/1 queue that supports advance reser-
vations. In our model, there is a reservation period which covers [−T, 0]. Each
customer k = 1, 2, ... is associated with a request time −T ≤ tk ≤ 0 and a
desired service starting time (shortly noted as arrival time) sk > 0. That is, if
t1 < t2, then customer 1 has the opportunity to reserve the server before cus-
tomer 2. If s1 < s2, then customer 1 wishes to be served earlier than customer
2. The service period starts only after the reservation period ends. The request
time can be interpreted as how much time in advance a customer realizes that
he/she will need service at a future time point.

The request times are derived from a general continuous distribution with
cumulative distribution function FT (·). The arrivals follow a Poisson process
with rate λ. The service time is 1/µ and we assume that λ < µ.

Each customer, at his/her request time, decides whether to make a reser-
vation or not. We denote those two actions by AR and AR′, respectively. If
a customer makes a reservation but his/her desired service time is already re-
served, the nearest future available time will be reserved for that customer. A
customer that does not make a reservation is served on a ﬁrst-come-ﬁrst-served
basis along periods of times over which the server is not reserved.

The total cost of each customer consists of the reservation cost C (if making
AR) and the cost of waiting which is a linear function of the waiting time.
Without loss of generality, we assume that the cost of waiting is equal to the
waiting time. Note that the waiting time when making AR is smaller than when
not making AR. However, it may be greater than zero, since it is possible that
the server is already reserved at the desired service time. For simpliﬁcation,
we assume that the service period is long enough such that we can ignore the
transient phase before the queue reaches its steady state.

In a preemptive-resume queue, if a job is interrupted, then it later resumes
and is not restarted. Due to this property, if the server is idle and a customer is
waiting for service, the customer will be served even if service cannot be com-
pleted due to an existing reservation (in this case the service will be preempted
and later resumed). Hence, supporting advance reservations in a preemptive-
resume queue does not impact the utilization of the server which is ρ = λ/µ.
Figure 1 illustrates the model.

Note that customers do not know a-priori what will be their waiting time if
making or if not making AR. The decision is based on statistical information
only, namely the values of λ, µ and FT . However, once a customer decides to
make a reservation, the system can provide him/her with the start and end

5

Figure 1: An illustration of the model with three customers. Customer 1 makes
a reservation at time t1, and is served upon arrival at s1. Customer 2 also
makes a reservation and is served upon arrival, but his/her service is preempted
by customer 1 which made a reservation earlier. Customer 3 is served only when
the service of customer 2 is completed.

times of the service.

4 Equilibrium Analysis

We can analyze this system as a priority queue where a priority between 0 (lowest
priority) and 1 (highest priority) is assigned to each customer. A customer with
request time t has priority 0 if not making AR and priority p = 1 − FT (t) if
making AR. Customers that share the same priority are served on a ﬁrst-come-
ﬁrst-served basis. We refer to p as the potential priority. Due to the probability
integral transformation theorem (Dodge 2006, p. 320), we know that p is a
random variable, uniformly distributed in [0, 1].

Since customers are statistically identical, we consider only symmetrical be-
havior. Thus, a decision of a tagged customer is a mapping of his/her potential
priority p to the probability of making AR. We denote this strategy function
by σ(p). Consider a tagged customer with potential priority p. We deﬁne W (·)
to be a mapping of the strategy followed by the rest of the customers and the
priority of the tagged customer to his/her expected waiting time. Thus, the ex-
pected waiting time of the tagged customer is W (σ, p) if making AR and W (σ, 0)
otherwise. Since customers are strategic, a customer with potential priority p
will make AR only if

W (σ, p) + C ≤ W (σ, 0).

(1)

Next, we deﬁne a threshold strategy and show that this is the only strategy

that can lead to equilibria.

Deﬁnition 1. Let τ ∈ (0, 1]. A strategy function σ(p) is said to be a threshold
strategy if it satisﬁes

σ(p) =

1
0

(cid:26)

if p > τ,
if p ≤ τ.

Lemma 1. At equilibrium, all customers follow a threshold strategy.

Proof. Consider any strategy function σ. Since the expected waiting time is
non-increasing with the priority, there is either a single potential priority, or an

6

interval of potential priorities, or no potential priority such that

W (σ, p) + C = W (σ, 0).

(2)

Note that the left hand side of Eq. (2) is the expected total cost if making
AR, while the right hand side is the expected total cost if not making AR. If
Eq. (2) holds for a single value p′, then a customer with potential priority greater
(respectively, smaller) than p′ is better oﬀ making (respectively, not making)
AR. Therefore, σ is an equilibrium strategy only if it is a threshold strategy
with threshold τ = p′.

If Eq. (2) holds for an interval of values [p′, p′′], then all customers with
potential priority p ∈ [p′, p′′] do not make AR (otherwise, W (σ, p) would not be
a constant over that interval). Therefore, σ is an equilibrium strategy only if it
is a threshold strategy, with threshold τ = p′.

Finally, suppose that Eq. (2) does not hold for any p ∈ [0, 1]. If W (σ, p)+C >
W (σ, 0) for all p ∈ [0, 1], then all customers are better oﬀ not making AR.
Therefore, σ is an equilibrium strategy only if it is a threshold strategy, with
threshold τ = 1.

Note that a situation where W (σ, p) + C < W (σ, 0) for all p ∈ [0, 1] does
not exist, since a customer with potential priority zero has the same expected
waiting time if making or avoiding AR.

Next, we deﬁne two types of equilibria.

Deﬁnition 2. An equilibrium strategy with threshold τ is called a some-make-
AR equilibrium if τ < 1.

Deﬁnition 3. An equilibrium strategy with threshold τ is called a none-make-
AR equilibrium if τ = 1.

Since the structure of the equilibrium depends on the reservation cost, we
aim to determine the equilibrium to which a given reservation cost leads. Given
that all customers follow a threshold strategy, we deﬁne a threshold customer to
be a customer with potential priority equals exactly to the threshold followed
by all other customers.

Given a strategy with threshold τ , a threshold customer that makes AR

observes three priority classes:

1. A lower priority class which contains all customers with potential priority
smaller than the threshold customer (none of them makes AR). The arrival
rate of customers belonging to this class is λτ .

2. A priority class which contains only the threshold customer (since the
potential priority is a continuous random variable, the probability that
two customers will have the same potential priority is zero). Thus, the
arrival rate of customers belonging to this class is 0.

3. A higher priority class which contains all customers with greater potential
priority (they all made AR before the threshold customer). The arrival
rate of customers belonging to this class is λ(1 − τ ).

7

A threshold customer that does not make AR only observes two classes:

1. A priority class which contains the threshold customer and all customers
with smaller potential priority. The arrival rate of customers belonging to
this class is λτ .

2. A higher priority class which contains all customers with greater potential
priority. The arrival rate of customers belonging to this class is λ(1 − τ ).

Based on the priority classes deﬁned above, we ﬁnd the expected waiting of
the threshold customer if making or not making AR. We apply the known for-
mula of the waiting time in an M/G/1 queue with preemptive-resume priorities
(Conway et al. 2012, p.175) and obtain the following:

1. The expected waiting time of the threshold customer if making AR is

WAR(τ ) =

µ − λ
2 (1 − τ )
(µ − λ (1 − τ ))

2 −

1
µ

.

(3)

2. The expected waiting time of the threshold customer if not making AR is

WAR′ (τ ) =

µ − λ
2
(µ − λ (1 − τ )) (µ − λ)

−

1
µ

.

The condition for threshold τ < 1 to be a some-make-AR equilibrium is

C + WAR(τ ) = WAR′ (τ ).

(4)

(5)

That is, a customer with potential priority equals to the threshold is indiﬀerent
between the two actions. The condition for threshold τ = 1 to be a none-make-
AR equilibrium is

C + WAR(1) ≥ WAR′ (1).

(6)

That is, a customer with potential priority 1 (and hence, all customers) are
better oﬀ not making AR.

By isolating C in Eq. (5), we deﬁne C(τ ) to be a function that maps a

threshold to the reservation cost that leads to that threshold

C(τ ) ,

λ · µ · τ

2 (µ − λ) · (µ − λ (1 − τ ))

2 .

(7)

We conclude that given reservation cost C, the threshold τ e ∈ (0, 1) represents
a some-make-AR equilibrium if and only if C = C(τ e). The threshold τ e = 1
represents a none-make-AR equilibrium if and only if C ≥ C(1). In order to
ﬁnd the equilibrium structure, we next ﬁnd the properties of C(τ ).

Lemma 2. If ρ ≤ 1/2, then C(τ ) is a monotonically increasing function. If
ρ > 1/2, then C(τ ) is a unimodal function with a global maximum.

8

Proof. First, we compute the derivative of C(τ ):

dC
dτ

=

λµ (λ (1 + τ ) − µ)

2 (λ − µ) (µ − λ (1 − τ ))

3 .

(8)

Since the denominator is negative for any τ , the sign of the derivative is deter-
mined by the sign of λ (1 + τ ) − µ. If ρ ≤ 1/2, then this expression is negative
for any τ ∈ (0, 1) and the derivative of C(τ ) is positive for any τ ∈ (0, 1). If
ρ > 1/2, then the derivative of C(τ ) is positive for any τ < (µ− λ)/λ; is equal to
zero at τ = (µ − λ)/λ; and negative otherwise. Thus, for any value of ρ > 1/2,
C(τ ) is unimodal with a global maximum.

Next, we deﬁne:

and

C , C(1) =

λ
2µ (µ − λ)

,

C ,

µ

8 (λ − µ)

2 .

(9)

(10)

Note that if ρ ≤ 0.5, then C is the maximum value of C(τ ) and if ρ > 0.5, then
C is the maximum value of C(τ ). We can now state the main result of this
section:

Theorem 1. The game has the following equilibrium structure.
When ρ ≤ 1/2:

• If C < C, then there is a unique some-make-AR equilibrium.

• If C > C, then there is a unique none-make-AR equilibrium.

When ρ > 1/2:

• If C < C, then there is a unique some-make-AR equilibrium.

• If C < C < C, then there are two some-make-AR equilibria and a none-

make-AR equilibrium.

• If C > C, then there is a unique none-make-AR equilibrium.

Proof. We begin with ρ ≤ 0.5. If C < C, then there is a single value of τ such
that C = C(τ ) has a solution. Hence, there is one some-make-AR equilibrium.
A none-make-AR equilibrium does not exist since C(1) > C. If C > C, then
there is no value of τ such that C = C(τ ) has a solution. Hence, a some-make-
AR equilibrium does not exist. On the other hand, a none-make-AR equilibrium
exists since C > C(1).

Next, consider ρ > 0.5. In the range [0, C], the function C(τ ) is monotoni-
cally increasing. Thus, if C ∈ [0, C], then there is a single value of τ such that
C = C(τ ) has a solution, and hence there is one some-make-AR equilibrium. In
the range [C, C], the function C(τ ) is unimodal. Thus, if C ∈ [C, C], then there
exist two values of τ that solve C = C(τ ), and hence there are two some-make-
AR equilibria. The condition for the existence of a none-make-AR equilibrium
is the same as in the case of ρ ≤ 0.5.

9

5 Revenue Maximization

In this section, we assume that the reservation cost is a fee determined by the
service provider. We show that the fee that maximizes the revenue leads to a
unique equilibrium if the utilization is smaller than 2/3 and to multiple equilibria
if the utilization is greater than 2/3. We also show that the revenue from AR
fee depends only on the utilization of the queue and the fee itself. Thus, if the
demand and the number of servers increase proportionally, then the revenue
from AR fees does not change.

The revenue per time unit, at equilibrium with threshold τ e, is the number of
customers making AR multiplied by the AR fee that leads to that equilibrium.
The expected revenue is

R(τ e) = λ(1 − τ e)C(τ e)

=

λ2(1 − τ e)τ eµ
2(µ − λ)(λ(τ e − 1) + µ)2 .

(11)

With some manipulation, we get that the revenue function does not depend

on the values of λ and µ but only on the utilization ρ:

R(τ e) =

ρ(1 − τ e)τ e
2(1 − ρ)(1 + ρ(τ e − 1))2 .

(12)

At ﬁrst glance, this result seems surprising since it implies that the revenue
does not increase when scaling the system (i.e., increasing both arrival and
service rates). However, in an M/D/1 queue, the waiting time decreases as
the system gets larger, and hence customers are less motivated to make AR.
Therefore, scaling the system has a trade oﬀ. For a given threshold, as we scale
the system, more customers will make AR but they will pay a smaller fee.

By solving the equation dR/dτ e = 0, we ﬁnd that the optimal threshold
is τ opt = (1 − ρ)/(2 − ρ). By substituting τ opt into Eq. (7), we get that the
optimal fee is

∗

C

=

λ(2µ − λ)
8µ(µ − λ)2 .

(13)

Similarly, by substituting τ opt into Eq. (12), we get that the maximum possible
revenue is

∗

R

=

ρ2
8(1 − ρ)2 .

(14)

Next we ﬁnd the number of equilibria when C = C∗.

Theorem 2. The revenue maximizing fee C∗ leads to a unique some-make-AR
equilibrium if ρ < 2/3 and to multiple equilibria, including a none-make-AR
equilibrium, otherwise.

10

(a) λ = 38, µ = 60

(b) λ = 45, µ = 60

Figure 2: When the utilization ρ < 2/3, the optimal fee C∗ leads to a unique
equilibrium (a). When ρ > 2/3, C∗ leads to multiple equilibria (b).

Proof. The optimal reservation cost C∗ leads to multiple equilibria only if ρ >
0.5 and C∗ > C (see Theorem 1). Using Eq. (9) and Eq. (13), we deduce that
if C∗ > C , then

λ(2µ − λ)
8µ(µ − λ)2 >

λ
2µ (µ − λ)

.

(15)

One can show that the inequality above holds only if ρ > 2/3.

Figure 2 illustrates the game outcome when C = C∗.

5.1 Price of Conservatism

Assuming that ρ > 2/3, the provider can either be risk-averse and charge a
fee that leads to a unique equilibrium with guaranteed revenue, or it can be
risk-taking and charge a higher fee that may lead to greater revenue but also
to zero revenue. To compare between the two options, we use the Price of
Conservatism (PoC) metric, which was introduced in Simhon and Starobinski
(2017). PoC is the ratio between the maximum possible revenue R∗ and the
maximum guaranteed revenue R∗

g, which is deﬁned as follows.

R

∗
g =

R(τ e).

sup
0<τ e<1
s.t. C(τ e) < C.

(16)

Since R(τ e) has exactly one extreme point (which is τ opt), it is increasing
in the range [0, τ opt). Therefore, the maximum guaranteed revenue is achieved
when choosing the largest τ e for which C(τ e) < C. In other words, C should
be slightly smaller than C. By solving C(τ ) = C, we get two solutions: τ e1 = 1
and

τ e
2 =

2

1 − ρ

(cid:18)

ρ (cid:19)

.

11

(17)

Figure 3: The maximum possible revenue and the maximum guaranteed revenue
in a system with parameters λ = 45 and µ = 60.

By substituting τ e

2 into Eq. (12), we get

R

∗
g =

2ρ − 1
2(1 − ρ)

,

and by dividing R∗ by R∗

g, we get

P oC =

ρ2
−8ρ2 + 12ρ − 4

.

(18)

(19)

We conclude with the following theorem.

Theorem 3. If ρ < 2/3, then P oC = 1. Else, P oC =

2
ρ
−8ρ2+12ρ−4 .

Figure 3 shows the maximum possible revenue and the maximum guaranteed

revenue in a system with parameters λ = 45 and µ = 60.

By computing the derivative of PoC with respect to ρ, we get that for any

ρ > 2/3

dP oC
dρ

=

ρ(3ρ − 2)

4(2ρ2 − 3ρ + 1)2 > 0

(20)

Thus, we obtain the following corollary:

Corollary 1. The price of conservatism increases with the utilization.

That is, as the utilization increases, the ratio between the potential revenue
when the provider is risk-taking and the revenue when the provider is risk-
averse increases and tends to ∞ as ρ → 1.

12

6 Dynamic Games

6.1

Learning Models

In this section, we study dynamic versions of the game. In dynamic games (also
known as learning models, since players learn over time the behavior of other
players), it is assumed that the game repeats many times and that initially
players do not necessarily follow an equilibrium strategy. The goal is to ﬁnd
the long-term behavior of the customers. In our analysis, we use a best response
dynamic model which is rooted in Cournot study of duopoly Cournot (1897).
we next describe the learning models.

At each step (game), a new set of customers participate (or the same set
of participants but with new realizations of request times). At the ﬁrst step,
all customers have an initial belief about the strategy that is followed by all
customers. Next, we assume:
Assumption 1. Customers that are indiﬀerent between actions AR and AR′
choose action AR’.

Based on this assumption, and using the proof of Lemma 1, one can show that
the best response of all customers to any initial belief is a threshold strategy.
In order to simplify the analysis and since a threshold strategy is followed at all
steps, we also assume:

Assumption 2. The initial belief is a threshold strategy.

We denote by τi ∈ [0, 1] the threshold of the strategy followed at step i ≥ 1.
We denote by ˆτi the estimation of this strategy and we distinguish between two
types of learning:

1. Strategy learning. In this type of learning, the analysis assumes that

at each step i, ˆτi , τi. That is, customers observe past strategies.

2. Action learning. In this type of learning, customers observe previous
actions and use the proportion of customers that chose AR at the previ-
ous step as an estimation of the strategy that was followed at that step.
Namely, if the demand and the number of reservations at step i are di and
dAR
i

respectively, then

ˆτi , 1 −

dAR
i
di

.

(21)

Since the best response of all customers to any belief is a threshold strategy,
we can deﬁne a joint best response function BR : [0, 1] → [0, 1]. The input is a
belief about the threshold strategy that will be followed by all customers. The
output is the best response threshold to that belief. Thus, we can describe the
best response dynamics of the game as the following process:

ˆτ1 = β,
τi = BR (ˆτi−1) ,

∀i > 1,

(22)
(23)

13

where β ∈ [0, 1] represents the initial belief. Note that under strategy-learning
this process is deterministic, while under action-learning this process is a Markov
process (Gardiner et al. 1985, Chapter 3). In the following sections we analyze
this dynamic process.

Next, we focus on the behavior of customers at a given step. Thus, we
remove the subscript i. We begin the analysis with the following observations:

(i) Given a belief β (i.e., assuming that all other customers follow the thresh-
old β), if a tagged customer with potential priority p > β chooses AR, then
all customers with greater potential priority have higher priority and all
customers with smaller potential priority have lower priority. Therefore,
the (believed) expected waiting time of the tagged customer is equal to
the expected waiting time of a threshold customer that chooses AR in a
system where all customers follow the threshold p. Hence,

W (β, p) = WAR(p),

if p ≥ β,

(24)

where WAR(·) is deﬁned in Eq. (3).

(ii) Given a belief β, if a tagged customer with potential priority p < β chooses
AR, then his/her (believed) expected waiting time is the same as the
expected waiting time of the threshold customer (recall that each customer
believes that he/she is the only one deviating). Hence,

W (β, p) = WAR(β),

if p < β.

(25)

(iii) The expected waiting time of all customers that choose AR′ are equal.

Hence,

W (β, 0) = WAR′ (β),

∀p ∈ [0, 1],

(26)

where WAR′ (·) is deﬁned in Eq. (4).

Those properties will be used later to prove our main results. Next, we sepa-
rately explore the case of a unique some-make-AR equilibriun and the case of
multiple equilibria.

6.2 Learning with Unique Some-make-AR Equilibrium

Consider a some-make-AR equilibrium with equilibrium threshold τ e. By com-
puting the derivative of WAR(β) and WAR′ (β), one can verify that both func-
tions are decreasing with β. This property will be used in the proof of the
following lemma.

Lemma 3. Under a unique some-make-AR equilibrium:

1. If a belief β ∈ [0, τ e) , then BR(β) ∈ (β, τ e).

2. If a belief β ∈ (τ e, 1], then BR(β) = 0.

14

Proof. From Eq. (3) and Eq. (4), we deduce that WAR(0)+C > WAR′ (0). Since,
under unique some-make-AR equilibrium, WAR(0) + C and WAR′ (0) intersect
once, we conclude that

and

WAR(β) + C > WAR′ (β) ∀β ∈ [0, τ e)

WAR(β) + C < WAR′ (β) ∀β ∈ (τ e, 1).

(27)

(28)

For proving part 1, assume that β < τ e. Since WAR(τ e) + C = WAR′ (τ e), and
since WAR(·) is a decreasing function we deduce that

WAR(τ e) + C < WAR′ (β).

(29)

From Eq. (27) and Eq. (29) we deduce that there exists a τ ∈ (β, τ e) such

that

WAR(τ ) + C = WAR′ (β).

(30)

Given the equation above and using Eq. (24) and Eq. (25), one can see that
all customers with potential priority p > τ choose AR, while all customers with
potential priority p ≤ τ choose AR′. This complete the proof of the ﬁrst part
of the lemma.

Now, let assume that β > τ e. Based on Eq. (24) and Eq. (25) and since

WAR(·) is a decreasing function, we deduce that

W (β, p) ≤ WAR(β),

∀p ∈ [0, 1].

From Eq. (28) and Eq. (39), we deduce that

W (β, p) + C < WAR′ (β),

∀p ∈ [0, 1].

(31)

(32)

That is, all customers are better oﬀ choosing AR and the best response to β is
τ = 0.

Next, we study the long-term outcome of the dynamic game and establish

the following result.

Theorem 4. A game with unique some-make-AR equilibrium converges to equi-
librium under strategy-learning and cycles under action-learning.

Proof. We begin with the ﬁrst part of the theorem. Let assume that the initial
belief β ∈ [0, τ e). From Lemma 3, we deduce that τ1 ∈ (β, τ e). Hence, τ2 ∈
(τ1, τ e). By induction, we deduce that, for any i ≥ 0,

τi ≥ τi−1,
τi ≤ τ e.

15

(33)

(34)

The set {τi, i = 1, 2...} is a monotonically increasing sequence bounded by
τ e. Thus, it has a limit, denoted by L.
since limi→∞ τi → L, and sense
limi→∞ BR(τi) = L, we conclude that the limit L is a ﬁxed point of BR, and
hence it must be the equilibrium point τ e.

Next, we assume that β ∈ (τ e, 1]. In this case, based on Lemma 4, τ1 = 0

and the game converges to equilibrium as in the case of β ∈ [0, τ e).

From Eq. (21), we deduce that, under action-learning, at any step i, if τi > 0,
then P (ˆτi > τ e) > 0 (i.e., if the strategy followed at step i is greater than zero
then there is a positive probability that the fraction of customers not making
AR will be greater than τ e). Once ˆτi > τ e, then τi+1 = 0. Thus, customers
strategy cycles between 0 and τ e.

Next, we determine, for a given reservation cost, whether a service provider
who wishes to maximize the number of reservations is better oﬀ under strategy-
learning or under action-learning.

Theorem 5. In a dynamic game with a unique some-make-AR equilibrium, the
average number of customers making AR under action-learning is greater than
under strategy-learning.

Proof. Denote the unique equilibrium by τ e. Consider an arbitrary step i and
assume that action-learning is applied. If at step i the strategy ˆτi ∈ [τ e, 1], then
all customers will choose AR at the next step. If τi ∈ (τi, τ e), then τi+1 ∈ (0, τ e).
Thus, in any realization, the strategy followed by all customers in all steps is a
random variable that takes values between 0 and τ e. In strategy-learning, the
strategy followed by all customers converges to τ e. Thus, the average fraction
of customers not making AR converges to a value between 0 and τ e under
action-learning and to τ e under strategy-learning.

Next, we present a simulated example that compares between the revenue
under action-learning and under strategy-learning. The pseudo-code of the sim-
ulation is given in Algorithm 1. The inputs of the procedure are the arrival rate
λ, the initial belief β, the reservation cost C and the number of steps l.

16

Algorithm 1 Learning Simulation (λ, β, C, l)

ˆτ1 = β
for i ← 1 to l {iterating over all steps} do

DAR ← 0 {variable counting the number of reservations}
D ← generate Poisson random variable {the number of customers}
for j ← 1 to D {iterating over all customers} do

p ← generate random variable from U(0,1) {the potential priority}
if p > ˆτi {check if the potential priority is greater than the current belief}
then

if WAR(p) + C < WAR′ ( ˆτi) {check if the customer is better oﬀ making
AR} then

DAR ← DAR + 1 {increase the number of reservations by one}

end if

else

if WAR( ˆτi) + C < WAR′ ( ˆτi) {check if the customer is better oﬀ making
AR} then

DAR ← DAR + 1 {increase the number of reservations by one}

end if

end if
end for
if strategy-learning then

ˆτi+1 ←

0
τ : WAR(τ ) + C = WAR′ ( ˆτi)

(cid:26)

if ˆτi > τ e,
if ˆτi ≤ τ e,

{compute the current strategy}

end if
if action-learning then

ˆτi+1 ← 1 − DAR

D {estimate the current strategy}

end if
end for

Example 1. Consider a queue with parameters λ = 45 and µ = 60. Let the
reservation cost be C = 0.024. The unique equilibrium (computed using Eq. (5)
and Eq. (6)) is τ e = 0.1026 (i.e., on a static game, on average, 89.74% of the
customers make AR). We run a simulation of 10, 000 steps. Each step lasts for
one time unit (i.e., the average demand at each step is 45). We set the initial
belief to be β = τ e. The average number of reservations per time unit is 40.3
(i.e., on average, 89.5% make AR) under strategy-learning and 42.7 (i.e., on
average, 94.9% make AR) under action-learning. We conclude that, as Theorem
5 states, when customers base their decisions on historic actions and not strate-
gies, more customers make AR. Statistical analysis (one-tailed t-test) shows that
the diﬀerence between the mean number of reservations under action-learning
and under strategy-learning is statistically signiﬁcant, with conﬁdence level of

17

Figure 4: Simulation results.
tomers make AR under action-learning than under strategy-learning.

In a game with unique equilibrium, more cus-

99%. In Figure 4, we plot the number of reservations, under action-learning
and under strategy-learning. We use the same realization of customer arrivals
in each case and we can see that at each iteration, the number of reservations
is greater (or equal) under action-learning.

We conclude that if the provider interest is that as many customers as pos-
sible will make reservations, then it is better oﬀ if customers gain information
about previous actions rather than strategies.

6.3 Learning with Multiple Equilibria

Lemma 4. In a game with multiple equilibria with thresholds τ e1, τ e2 and 1:

1. If a belief β ∈ [0, τ e1) , then BR(β) ∈ (β, τ e1).

2. If a belief β ∈ (τ e1, τ e2), then BR(β) = 0.

3. If a belief β ∈ (τ e2, 1) , then BR(β) ∈ (β, 1].

Proof. From Eq. (3) and Eq. (4), we deduce that WAR(0)+C > WAR′ (0). Since,
under unique some-make-AR equilibrium, WAR(0) + C and WAR′ (0) intersect
twice at τ e1 and τ e2, we conclude that

WAR(β) + C > WAR′ (β) ∀β ∈ {[0, τ e1), (τ 2

e , 1)}

and

WAR(β) + C < WAR′ (β) ∀β ∈ (τ e1, τ e2).

(35)

(36)

For proving part 1, assume that β < τ e1. Since WAR(τ e1) + C = WAR′ (τ e1),

and since WAR(·) is a decreasing function we deduce that

WAR(τ e1) + C < WAR′ (β).

(37)

18

From Eq. (35) and Eq. (37) we deduce that there exists a τ ∈ (β, τ e1) such

that

WAR(τ ) + C = WAR′ (β).

(38)

Given the equation above and using Eq. (24) and Eq. (25), one can see that
all customers with potential priority p > τ choose AR, while all customers with
potential priority p ≤ τ choose AR′. This complete the proof of the ﬁrst part
of the lemma.

Now, assume that β ∈ (τ e1, τ e2). Based on Eq. (24) and Eq. (25) and since

WAR(·) is a decreasing function, we deduce that

W (β, p) ≤ WAR(β),

∀p ∈ [0, 1].

From Eq. (36) and Eq. (39), we deduce that

W (β, p) + C < WAR′ (β),

∀p ∈ [0, 1].

(39)

(40)

That is, all customers are better oﬀ choosing AR and the best response to β is
τ = 0. The third part of the lemma can be proved using the same arguments
as in the proof of the ﬁrst part of the lemma.

Next, we study the long-term outcome of a dynamic game with multiple

equilibria.

Theorem 6. A game with multiple equilibria converges to some-make-AR or
none-make-AR equilibrium (depend on the initial belief ) under strategy-learning
and to none-make-AR equilibrium under action-learning.

Proof. Using the same arguments as in the proof of Theorem 6 and based on
Lemma 4, one can show the following. Under strategy-learning, a game with
initial belief β < τ e2 converges to τ e1, while a game with initial belief β ∈ (τ e2, 1]
converges to 1.

Under action-learning, if at some step i, τi = 1 and none-make-AR is an
equilibrium, then at all future steps all customers will keep not making AR.
Given any threshold strategy τ > 0 followed by all customers, there is a positive
probability that the potential priority of all customers will be smaller than τ ,
and hence none of the customers will make AR. If the game repeats inﬁnite
many times, then with probability one, at some point, none of the customers
will make AR and the game will converge to none-make-AR equilibrium.

Example 2. Consider a queue with parameters λ = 45 and µ = 60. Let the
reservation cost be C = 0.032. Using Eq. (5) and Eq. (6) we compute the set of
equilibria: τ e1 = 0.22, τ e
3 = 1. We set three diﬀerent initial strategies:
β1 = 0.2, β2 = 0.4 and β3 = 0.6. We apply strategy-learning. As Figure 5
shows, within a few steps, the system converges to an equilibrium.

2 = 0.5, τ e

19

Figure 5: Convergence to equilibrium under strategy-learning

6.4 Proﬁt Maximization in Dynamic Games

In this section, we assume that the reservation cost is a fee collected by the
provider. Our goal is to ﬁnd the fee that maximizes the provider revenue in the
dynamic games setting. Under action learning, any fee that leads to multiple
equilibria will eventually lead to zero revenue. Hence, we focus, in this section,
on strategy-learning.

Under strategy-learning with multiple equilibria, the initial belief determines
to which equilibrium the game will converge. To execute the analysis, we assume
that the initial belief β is a continues random variable that takes values between
zero and one.

Consider a game with multiple equilibria {τ e1, τ e2, 1}. From Lemma 3,
Lemma 4 and Theorem 6, we deduce that if the initial belief is in [0, τ e2),
then the game converges to τ e1, otherwise it converges to 1 (i.e., zero reserva-
tions). Thus, with probability P(β < τ e2) the strategy converges to τ e1 and
with probability P(β > τ e2) it converges to 1. Thus, the excepted revenue of
the dynamic game at steady state is

RD(τ e1, τ e2) = P(β < τ e2)R(τ e1).

(41)

where R(·) is deﬁned in Eq. (11). Since the expected revenue depends on both
τ e1 and τ e2, we next ﬁnd the relation between those two thresholds. By ma-
nipulating the equation C(τ e1) = C(τ e2) (see Eq. (7) for deﬁnition of C(·)), we
get the following relation.

τ e2 =

2

1 − ρ

(cid:18)

ρ (cid:19)

1
τ e1 .

(42)

Given the distribution of β and using Eq. (41), Eq. (42) and Eq. (12), one
can ﬁnd the value of τ e1 that maximizes the revenue and, in turn, the optimal
fee. For instance, let assume that β is uniformly distributed in [0, 1]. In this
case, the revenue as a function of τ e1 is

RD(τ e1) =

(1 − τ e1)(1 − ρ)
2ρ(1 − ρ(τ e1 − 1))2 .

(43)

By computing the derivative of RD(τ e1) with respect to τ e1, one can show
that it decreases with τ e1. Thus, when considering multiple equilibria, the

20

optimal value of τ e1 is {min τ e1|C < C(τ e1) < C}. From Eq. (17) we know that
this value is ((1 − ρ)/ρ)2 and is obtained when C = C

Combining this result with Corollary 2 leads to the following theorem:

Theorem 7. Under strategy-learning, if the initial belief is uniformly distributed
between 0 and 1, then the optimal fee is C when ρ > 2/3 and C∗ when ρ < 2/3.

7 Conclusion and future work

In this paper, we analyzed an M/D/1 queue that supports advance reserva-
tions. We associated the act of making reservation with a ﬁxed reservation cost
and studied the impact of this cost on the behavior of customers. First, we
showed that if the utilization of the queue is greater than 1/2, then there is a
range of reservation costs that lead to multiple equilibria including one where
no customer makes a reservation. Furthermore, if the utilization is greater than
2/3 and the reservation cost is a fee charged by the service provider, then the
fee value that maximizes the revenue from AR belongs to the aforementioned
range. In order to evaluate whether the provider should charge a lower fee with
guaranteed revenue or a higher but riskier fee (yielding several equlibria) we
used the price of conservatism (PoC) metric and found the ratio between the
two revenues. Speciﬁcally, when the utilization exceeds 2/3, we showed that
the PoC increases with the utilization and tends to inﬁnity as the utilization
approaches 1.

In the second part of the paper, we studied a dynamic version of the game.
We showed that if the customers observe previous strategies, then the game
converges to an equilibrium. If the customers observe previous actions, then the
game converge to a none-make-AR equilibrium, if such an equilibrium exists,
and cycles otherwise. Finally, we develop a method to derive the revenue-
maximizing fee under dynamic games. This method helps to determine the
optimal control parameters in a game with many equilibria. We expect the
same kind of methods to prove useful for the analysis of other types of dynamic
games with many equilibria.

References

Altman, Eitan, Nahum Shimkin. 1998. Individual equilibrium and learning in processor

sharing systems. Operations Research 46(6) 776–784.

Balachandran, KR. 1972. Purchasing priorities in queues. Management Science 18(5-

Part-1) 319–326.

Bertsimas, Dimitris, Romy Shioda. 2003. Restaurant revenue management. Operations

Research 51(3) 472–486.

Brown, George W. 1951. Iterative solution of games by ﬁctitious play. Activity analysis

of production and allocation 13(1) 374–376.

21

Charbonneau, Neal, Vinod M Vokkarane. 2012. A survey of advance reservation rout-
ing and wavelength assignment in wavelength-routed wdm networks. Commu-
nications Surveys & Tutorials, IEEE 14(4) 1037–1064.

Conway, Richard W, William L Maxwell, Louis W Miller. 2012. Theory of scheduling.

Courier Corporation.

Cournot, A Augustin. 1897. Recherches sur les principes mathematiques de la theorie
des richesses, paris 1838. English transl. by NT Bacon under the title Researches
into the Mathematical Principles of the Theory of Wealth, New York .

Dodge, Yadolah. 2006. The Oxford dictionary of statistical terms. Oxford University

Press on Demand.

Fu, Fangwen, Mihaela van der Schaar. 2009. Learning to compete for resources in
wireless stochastic games. Vehicular Technology, IEEE Transactions on 58(4)
1904–1919.

Fudenberg, Drew. 1998. The theory of learning in games, vol. 2. MIT press.

Gardiner, Crispin W, et al. 1985. Handbook of stochastic methods, vol. 3. Springer

Berlin.

Gu´erin, Roch A, Ariel Orda. 2000. Networks with advance reservations: The routing
perspective. INFOCOM 2000. Nineteenth Annual Joint Conference of the IEEE
Computer and Communications Societies. Proceedings. IEEE , vol. 1. IEEE, 118–
127.

Hassin, Refael. 2016. Rational Queueing. CRC Press.

Hassin, Refael J, Moshe Haviv. 2003. To Queue or Not to Queue: Equilibrium Be-

haviour in Queueing Systems, vol. 59. Kluwer Academic Pub.

Hayel, Yezekael, Dominique Quadri, Tania Jimenez, Luce Brotcorne. 2016. Decentral-
ized optimization of last-mile delivery services with non-cooperative bounded
rational customers. Annals of Operations Research 239(2) 451–469.

Lakshmivarahan, Sivaramakrishnan. 1981. Learning algorithms theory and applica-

tions. Springer-Verlag New York, Inc.

Liberman, Varda, Uri Yechiali. 1978. On the hotel overbooking problem-an inventory

system with stochastic cancellations. Management Science 24(11) 1117–1126.

Littman, Michael L. 1994. Markov games as a framework for multi-agent reinforce-
ment learning. Proceedings of the eleventh international conference on machine
learning, vol. 157. 157–163.

Liu, Qian, Garrett van Ryzin. 2011. Strategic capacity rationing when customers learn.

Manufacturing & Service Operations Management 13(1) 89–107.

Menache, Ishai, Ohad Shamir, Navendu Jain. 2014. On-demand, spot, or both: Dy-
namic resource allocation for executing batch jobs in the cloud. 11th Interna-
tional Conference on Autonomic Computing (ICAC 14). USENIX Association,
177–187.

Milgrom, Paul, John Roberts. 1991. Adaptive and sophisticated learning in normal

form games. Games and economic Behavior 3(1) 82–100.

Naor, Pinhas. 1969. The regulation of queue size by levying tolls. Econometrica:

journal of the Econometric Society 15–24.

Nasiry, Javad, Ioana Popescu. 2012. Advance selling when consumers regret. Manage-

ment Science 58(6) 1160–1177.

22

Niu, Di, Chen Feng, Baochun Li. 2012. Pricing cloud bandwidth reservations un-
der demand uncertainty. ACM SIGMETRICS Performance Evaluation Review ,
vol. 40. ACM, 151–162.

Qiu, Chun Martin, Wenqing Zhang. 2016. Managing long queues for holiday sales

shopping. Journal of Revenue and Pricing Management 15(1) 52–65.

Quan, Daniel C. 2002. The price of a reservation. Cornell Hotel and Restaurant

Administration Quarterly 43(3) 77–86.

Reiman, Martin I, Qiong Wang. 2008. An asymptotically optimal policy for a quantity-
based network revenue management problem. Mathematics of Operations Re-
search 33(2) 257–282.

Simhon, Eran, Carrie Cramer, Zachary Lister, David Starobinski. 2015. Pricing in dy-
namic advance reservation games. Computer Communications Workshops (IN-
FOCOM WKSHPS), 2015 IEEE Conference on. IEEE, 546–551.

Simhon, Eran, David Starobinski. 2014. Game-theoretic analysis of advance reser-
vation services. Information Sciences and Systems (CISS), 2014 48th Annual
Conference on. IEEE, 1–6.

Simhon, Eran, David Starobinski. 2017. Advance reservation games. ACM Transac-
tions on Modeling and Performance Evaluation of Computing Systems (TOM-
PECS) 2(2) 10.

Smith, Warren, Ian Foster, Valerie Taylor. 2000. Scheduling with advanced reser-
vations. Parallel and Distributed Processing Symposium, 2000. IPDPS 2000.
Proceedings. 14th International . IEEE, 127–132.

Syed, Aﬀan A, Wei Ye, John Heidemann. 2008. T-lohi: A new class of mac protocols
for underwater acoustic sensor networks. INFOCOM 2008. The 27th Conference
on Computer Communications. IEEE . IEEE.

Tan, Ming. 1993. Multi-agent reinforcement learning: Independent vs. cooperative
agents. Proceedings of the tenth international conference on machine learning.
330–337.

Virtamo, Jorma T. 1992. A model of reservation systems. Communications, IEEE

Transactions on 40(1) 109–118.

Wang, Wei, Di Niu, Baochun Li, Ben Liang. 2013. Dynamic cloud resource reservation
via cloud brokerage. Distributed Computing Systems (ICDCS), 2013 IEEE 33rd
International Conference on. IEEE, 400–409.

Weatherford, Lawrence R. 1998. A tutorial on optimization in the context of
perishable-asset revenue management problems for the airline industry. Op-
erations research in the airline industry. Springer, 68–100.

Yessad, Samira, Farid Nait-Abdesselam, Tarik Taleb, Brahim Bensaou. 2007. R-mac:
Reservation medium access control protocol for wireless sensor networks. Local
Computer Networks, 2007. LCN 2007. 32nd IEEE Conference on. IEEE, 719–
724.

Zohar, Ety, Avishai Mandelbaum, Nahum Shimkin. 2002. Adaptive behavior of im-
patient customers in tele-queues: Theory and empirical support. Management
Science 48(4) 566–583.

23

