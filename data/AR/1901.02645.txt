9
1
0
2

t
c
O
8
1

]

V
C
.
s
c
[

2
v
5
4
6
2
0
.
1
0
9
1
:
v
i
X
r
a

Weakly Aligned Cross-Modal Learning for Multispectral Pedestrian Detection

Lu Zhang1,3, Xiangyu Zhu2,3, Xiangyu Chen5, Xu Yang1,3, Zhen Lei2,3, Zhiyong Liu1,3,4∗
1 SKL-MCCS, Institute of Automation, Chinese Academy of Sciences
2 CBSR & NLPR, Institute of Automation, Chinese Academy of Sciences
3 University of Chinese Academy of Sciences 4 CEBSIT, Chinese Academy of Sciences
5 Renmin University of China
{zhanglu2016,xu.yang,zhiyong.liu}@ia.ac.cn, {xiangyu.zhu,zlei}@nlpr.ia.ac.cn

Abstract

Multispectral pedestrian detection has shown great ad-
vantages under poor illumination conditions, since the ther-
mal modality provides complementary information for the
color image. However, real multispectral data suffers from
the position shift problem, i.e. the color-thermal image pairs
are not strictly aligned, making one object has different
positions in different modalities.
In deep learning based
methods, this problem makes it difﬁcult to fuse the fea-
ture maps from both modalities and puzzles the CNN train-
ing.
In this paper, we propose a novel Aligned Region
CNN (AR-CNN) to handle the weakly aligned multispec-
tral data in an end-to-end way. Firstly, we design a Re-
gion Feature Alignment (RFA) module to capture the po-
sition shift and adaptively align the region features of the
two modalities. Secondly, we present a new multimodal fu-
sion method, which performs feature re-weighting to select
more reliable features and suppress the useless ones. Be-
sides, we propose a novel RoI jitter strategy to improve the
robustness to unexpected shift patterns of different devices
and system settings. Finally, since our method depends on
a new kind of labelling: bounding boxes that match each
modality, we manually relabel the KAIST dataset by locat-
ing bounding boxes in both modalities and building their
relationships, providing a new KAIST-Paired Annotation.
Extensive experimental validations on existing datasets are
performed, demonstrating the effectiveness and robustness
of the proposed method. Code and data are available at:
https://github.com/luzhang16/AR-CNN .

1. Introduction

Pedestrian detection is an important research topic in
computer vision ﬁeld with various applications, such as
video surveillance, autonomous driving, and robotics. Al-

∗Corresponding author

though great progress has been made by the deep learn-
ing based methods (e.g. [42, 53, 54, 33]), detecting the
pedestrian in adverse illumination conditions, occlusions
and clutter background is still a challenging problem. Re-
cently, many works in robot vision [4, 49], facial expression
recognition [9], material classiﬁcation [41], and object de-
tection [45, 11, 20, 51] show that adopting a novel modality
can improve the performance and offer competitive advan-
tages over single sensor systems. Among the sensors, ther-
mal camera is widely used in face recognition [3, 44, 27],
human tracking [30, 46] and action recognition [59, 15] for
its biometric robustness. Motivated by this, multispectral
pedestrian detection [24, 52, 17, 39] has attracted massive
attention and provides new opportunities for around-the-
clock applications, mainly due to its superiority of comple-
mentary nature between color and thermal modalities.

Challenges A common assumption of multispectral
pedestrian detection is that the color-thermal image pairs
are geometrically aligned [24, 52, 34, 28, 32, 18, 55]. How-
ever, the modalities are just weakly aligned in practice,
which means there is the position shift between modali-
ties, making one object has different positions on differ-
ent modalities, see Figure 1(a). This position shift prob-
lem can be caused by physical properties of different sen-
sors (e.g. parallax, mismatched resolutions and ﬁeld-of-
views), imperfection of alignment algorithms, external dis-
turbance, and hardware aging. Moreover, the calibration for
color-thermal cameras are tortuous, generally require par-
ticular hardware as well as special heated calibration board
[26, 23, 24, 47, 8].

The position shift problem degrades the pedestrian de-
tector in two aspects. First, features from different modal-
ities are mismatched in the corresponding positions, which
puzzles the inference. Second, it is difﬁcult to cover the ob-
jects in both modalities with a single bounding box. In ex-
isting datasets [24, 17], the bounding box is either labelled
on single modality (color or thermal) or a big bounding box
is labelled to cover the objects on both modalities. This la-

 
 
 
 
 
 
approach reduces the degradation from position shift prob-
lem and makes full use of both modalities, achieving the
state-of-the-art performance on the challenging KAIST and
CVC-14 dataset.

2. Related Work

Multispectral Pedestrian Detection As an essential
step for various applications, pedestrian detection has at-
tracted massive attention from the computer vision com-
munity. Over the years, extensive features and algorithms
have been proposed, including both traditional detectors
[14, 12, 38, 56] and the lately dominated CNN-based detec-
tors [37, 22, 1, 50, 58]. Recently, multispectral data have
shown great advantages, especially for the all-day vision
[25, 7, 8]. Hence the release of large-scale multispectral
pedestrian benchmarks [24, 17] is encouraging the research
community to advance the state-of-the-art by efﬁciently ex-
ploiting multispectral input data. Hwang et al. [24] propose
an extended ACF method, leveraging aligned color-thermal
image pairs for around-the-clock pedestrian detection. With
the recent development of deep learning, the CNN-based
methods [52, 48, 6, 55, 19, 32] signiﬁcantly improve the
multispectral pedestrian detection performances. Liu et al.
[34] adopt the Faster R-CNN architecture and analyze dif-
ferent fusion stages within the CNN. K¨onig et al. [28] adapt
the Region Proposal Network (RPN) and Boosted Forest
(BF) framework for multispectral input data. Xu et al. [52]
design a cross-modal representation learning framework to
overcome adverse illumination conditions.

However, most existing methods are employed under
the full alignment assumption, hence directly fuse features
of different modalities in the corresponding pixel position.
This not only hinders the usage of the weakly aligned
dataset (e.g. CVC-14 [17]), but also restricts the further de-
velopment of multispectral pedestrian detection, which is
worthy of attention but still exhibits a lack of study.

Weakly Aligned Image Pair Weakly aligned image pair
is a common phenomenon in multispectral data, since im-
ages from different modalities are usually collected and pro-
cessed independently. A common paradigm to address this
problem is to conduct image registration (i.e. spatial align-
ment) [60, 2, 10, 36] as preprocessing.
It geometrically
aligns two images: the reference and sensed images, which
can be considered as an image-level solution for the posi-
tion shift problem. The standard registration includes four
typical processes: feature detection, mapping function de-
sign, feature matching, and image transformation and re-
sampling. Though well-established, the image registra-
tion mainly focuses on the low-level transformation of the
whole image, which actually introduces time-consuming
preprocessing and disenables the CNN-based detector to be
trained in an end-to-end way.

Figure 1. Overview of our framework. (a) The color-thermal pair
and its annotations, the yellow boxes denote original KAIST anno-
tations, which have position shift between two modalities; the red
boxes are the proposed KAIST-Paired annotations, which have in-
dependent labelling to match each modality. (b) Illustration of the
proposed AR-CNN model. (c) Detection results of both modalities
under the position shift problem.

bel bias will give bad supervision singals and degrade the
detector especially for CNN based methods [40, 35], where
the intersection over union (IoU) overlap is used for fore-
ground/background classiﬁcation. Therefore, how to ro-
bustly localize each individual person on weakly aligned
modalities remains to be a critical issue for multispectral
pedestrian detectors.

Our Contributions (1). To the best of our knowledge,
this is the ﬁrst work that tackles the position shift prob-
lem in multispectral pedestrian detection. In this paper, we
analyse the impacts of the position shift problem and pro-
pose a novel detection framework to merge the information
from both modalities. Speciﬁcally, a novel RFA module
is presented to shift and align the feature maps from two
modalities. Meanwhile, the RoI jitter training strategy is
adopted to randomly jitter the RoIs of the sensed modality,
improving the robustness to the patterns of position shift.
Furthermore, a new conﬁdence-aware fusion method is pre-
sented to effectively merge the modality, which adaptively
performs feature re-weighting to select more reliable fea-
tures and depress the confusing ones. Figure 1 depicts an
overview of the proposed approach.

(2). To realize our method, we manually relabel the
KAIST dataset and provide a novel KAIST-Paired annota-
tion. We ﬁrst ﬁlter the image pairs with original annotations
and obtain 20, 025 valid frames. Then 59, 812 pedestrians
are carefully annotated by locating the bounding boxes in
both modalities and building their relationships.

(3). The experimental results show that the proposed

KAIST-paired annotationsAccurate detections for both color and thermalDetection ResultsOriginal annotationsRoI JitterWeakly AlignedAlinged Region CNNMismatched fusionBiased RoI samplingAmbiguous localizationConfidence-aware FusionRoI PoolingPerformance DropRegion Alignment Color InputThermal Input1122(a)(b)(c)(a)

(b)

Figure 2. The visualization examples of ground truth annotations
in the KAIST (boxes in yellow) and CVC-14 (boxes in orange)
dataset. Image patches are cropped on the same position of color-
thermal image pairs.

3. Motivation

To provide insights into the position shift problem in
weakly aligned image pairs, we start with our analysis of
the KAIST [24] and CVC-14 [17] multispectral pedestrian
dataset. Then we experimentally study how the position
shift problem impacts the detection performance.

3.1. Important Observations

From the multispectral image pairs and the correspond-
ing annotations in the KAIST and CVC-14 dataset, several
issues can be observed.

Weakly Aligned Features As illustrated in Figure 2(a),
the weakly aligned color-thermal image pairs suffer from
position shift problem, which makes it unreasonable to di-
rectly fuse the feature maps of different modalities.

Localization Bias Due to the position shift problem, the
annotation must be reﬁned to match both modalities, see
Figure 2(a). One way is to adopt larger bounding boxes, en-
compassing pedestrians of both modalities, but generating
too big bounding boxes for each modality. Another remedy
is to only focus on one particular modality, while introduc-
ing bias for another modality.

Unpaired Objects Since the image pair of two modal-
ities may have different ﬁeld-of-views due to bad camera
synchronization and calibration, some pedestrians exist in
one modality but are truncated/lost in another, see Figure
2(b). Speciﬁcally, 12.5% (2, 245 of 18, 017) of bounding
boxes are unpaired in the CVC-14 [17] dataset.

3.2. How the Position Shift Impacts?

To quantitatively evaluate how the position shift prob-
lem inﬂuences the detection performance, we conduct ex-
periments on the relatively well-aligned KAIST dataset by
manually simulating the position shift.

(a)

(b)

Figure 3. Surface plot of the detection performances within the po-
sition shift experiments. (a) Baseline detector. (b) The proposed
approach. Horizontal coordinates indicate different step sizes by
which sensed images are shifted along the x-axis and y-axis. Ver-
tical coordinates denote the log-average miss rates (MR) measured
on the reasonable test set of KAIST dataset, lower is better.

Baseline Detector We build our baseline detector based
on the adapted Faster R-CNN [57] framework and adopt
the halfway fusion settings [34] for multispectral pedestrian
detection. To mitigate the negative effect of harsh quanti-
zation on localization, we use RoIAlign [21] instead of the
standard RoIPool [40] for the region feature pooling pro-
cess. Our baseline detector is solid: it has 15.18 MR on
the KAIST reasonable test set, 10.97 better than the 26.15
reported in [34, 32].

Robustness to Position Shift In the testing phase, we
ﬁx the thermal image but spatially shift the color image
along x-axis and y-axis. The shift pixel is selected in
{(∆x, ∆y) | ∆x, ∆y ∈ [−6, 6]; ∆x, ∆y ∈ Z}, which
contains a total of 169 shift modes. As shown in Figure
3(a) and Table 1, the performance dramatically drops as the
absolute shift values increase. Especially, the worst case
(∆x, ∆y) = (−6, 6) suffers ∼ 65.3% relative performance
decrement, i.e. from 15.18 MR to 25.10 MR. Interestingly,
when the image is shifted to a speciﬁc direction (∆x = 1,
∆y = −1), a better result is achieved (15.18 MR to 14.68
MR), which indicates that we can improve the performance
by appropriately handling the position shift problem.

∆MR (%)

∆y

1
0
-1
-4
-6

-6
↓ 6.55
↓ 6.32
↓ 7.19
↓ 8.27
↓ 9.92

-4
↓ 2.62
↓ 2.41
↓ 2.58
↓ 4.01
↓ 5.27

∆x
-1
↓ 0.31
↓ 0.21
↓ 0.19
↓ 0.84
↓ 1.79

0
↓ 0.14
0(15.18)
↑ 0.25
↑ 0.18
↓ 1.37

1
↓ 0.49
↓ 0.14
↑ 0.50
↓ 0.22
↓ 1.21

Table 1. Numerical results of the position shift experiments. The
scores are corresponding with the results in Figure 3(a). Result in
the origin is highlighted in blue, ↓ refers to performance drop and
↑ on the contrary.

Poor AlignmentUnpaired ObjectsPoor AlignmentUnpaired Objects(a) KAIST

(b) CVC-14

Figure 4. The statistics of bounding box shift within color-thermal
image pairs in KAIST and CVC-14 dataset.

4. The Proposed Approach

This section introduces the proposed KAIST-Paired an-
notation (Section 4.1) and Aligned Region CNN. The ar-
chitecture of AR-CNN is shown in Figure 5, which consists
of the region feature alignment module (Section 4.2), the
RoI jitter training strategy (Section 4.3) and the conﬁdence-
aware fusion step (Section 4.4).

4.1. KAIST-Paired Annotation

In order to address the position shift problem, we ﬁrst
manually annotate the color-thermal bounding boxes pairs
on each modality by the following principles:

• Localizing both modalities. The pedestrians are lo-
calized in both color and thermal images, aiming to
clearly indicate the object locations on each modality.
• Adding relationships. A unique index is assigned to
each pedestrian, indicating the pairing information be-
tween modalities.

• Labelling the unpaired objects. The pedestrians that
only appear in one modality are labelled as “unpaired”
to identify such situation.

• Extreme case.

If the image quality of one modality
is beyond human vision, e.g. color image under ex-
tremely bad illumination, we make the bounding box
of color modality consistent with that in the thermal
modality.

Statistics of KAIST-Paired From the new KAIST-
Paired annotation, we can get the statistics information of
shift distance in the original KAIST dataset. As illustrated
in Figure 4(a), more than half of the bounding boxes have
the position shift problem, and the shift distance mostly
ranges from 0 to 10 pixels.

4.2. Region Feature Alignment

In this subsection, we propose the Region Feature Align-
ment (RFA) module to predict the shift between two modal-
ities. Note that the position shift is not simply afﬁne trans-
formation and depends on the cameras. Furthermore, the
shift distance varies from pixels to pixels, always small in
the center and large in the edge. As a result, the shift pre-

diction and alignment process is performed in a region-wise
way.

Reference and Sensed Modality We introduce the con-
cept of the reference and sensed [60, 2] image into the mul-
tispectral setting. In our implementation, we select the ther-
mal image as the reference modality and the color image as
the sensed modality. During training, we ﬁx the reference
modality and perform the learnable feature-level alignment
and RoI jitter process on the sensed one.

Proposals Generation As illustrated in Figure 5, we uti-
lize the Region Proposal Network (RPN) to generate nu-
merous proposals. We aggregate the proposals from both
reference feature map (Conv4 r) and sensed feature map
(Conv4 s) to keep a high recall rate.

Alignment Process The concrete connection scheme of
the RFA module is shown in Figure 6. Firstly, given several
proposals, this module enlarges contextual RoIs to encom-
pass sufﬁcient information of regions. For each modality,
the contextual region features are pooled into a small fea-
ture map with a ﬁxed spatial extent of H × W (e.g. 7 × 7).
Secondly, the feature map from each modality is then con-
catenated to get the multimodal representation. From this
representation, two consecutive fully connected layers are
used to predict the shift targets (i.e. tx and ty) of this re-
gion, so that the new coordinates of the sensed region is
predicted. Finally, we re-pool the sensed feature map on
the new region to get aligned feature representation with the
reference modality. Since we have access to the annotated
bounding boxes pairs on both modalities, the ground truth
shift targets of the two region features can be calculated as
follow:

t∗
x = (xs − xr)/wr

t∗
y = (ys − yr)/hr

(1)

In Equation 1, x, y denote the center coordinates of the
box, w and h indicate the width and height of the box .
Variables xs, xr are for the sensed and reference ground
truth box respectively, t∗
x is the shift target for x coordinate,
and likewise for y.

Multi-task Loss Similar to Fast R-CNN [16], we use
the smooth L1 loss as the regression loss to measure the
accuracy of predicted shift targets, i.e.,

Lshif t({p∗

i }, {ti}, {t∗

i }) =

1
Nshif t

n
(cid:88)

i=1

i smoothL1(ti − t∗
p∗
i )

(2)

where i is the index of RoI in a mini-batch, ti is the pre-
dicted shift target, p∗
i are the associated ground truth
class label (pedestrian p∗
i = 0) and
shift target of the i-th sensed RoI. Nshif t is the total number
of ground truth objects to be aligned (i.e. Nshif t = n).

i = 1 vs. background p∗

i and t∗

For each training example, we minimize an objective

Figure 5. The network structure of Aligned Region CNN (AR-CNN). We adopt the two-stream framework to deal with color-thermal
inputs. Given a pair of images, numerous proposals are generated and aggregated by the Region Proposal Network, then the Region
Feature Alignment module is introduced to align the region features. After alignment, the region features of color and thermal feature
maps are pooled respectively, then the conﬁdence-aware fusion method is performed.

Figure 6. Connection scheme of the RFA module. RF denotes
region feature and ⊕ refers to channel concatenation. The cross-
modal region feature is fed into two fully-connected layers to pre-
dict this region’s shift between two modalities.

i }, {g∗
i }) + Lreg({p∗

i }) = Lcls({pi}, {p∗
i }, {gi}, {g∗

function of Fast R-CNN which is deﬁned as follow:
i }, {t∗
L({pi}, {ti}, {gi}, {p∗
i }, {ti}, {t∗
+λLshif t({p∗

i })
i })
(3)
where pi and gi are the predicted conﬁdence and coordi-
nates of the pedestrian, p∗
i and g∗
i are the associated ground
truth label and the reference ground truth coordinates. Here
the two terms Lshif t and Lreg are weighted by a balancing
parameter λ. In our current implementation, we set λ = 1,
and thus the two terms are roughly equally weighted. For
the RPN module, the loss function is deﬁned as in the liter-
ature [40].

4.3. RoI Jitter Strategy

(a) Reference image

(b) Sensed image

Figure 7. Illustration of the RoI jitter strategy. Red boxes de-
note the ground truths, GTR and GTS stand for the reference and
sensed modality respectively. Blue boxes represents the RoIs, i.e.
the proposals, which are shared by both modalities. RoIj1, RoIj2,
and RoIj3 are three feasible proposal instances after jitter.

introduces stochastic disturbances to the sensed RoIs and
shifts the targets of RFA accordingly, which enriches the
patterns of position shift in the training process, as shown
in Figure 7.

The jitter targets are randomly generated from a normal

distribution,

x, tj
tj

y ∼ N (0, σ2

0;0, σ2

1; 0)

(4)

In reality, the shift patterns are unexpected due to the
changes of devices and system settings. To improve the ro-
bustness to shift patterns, we propose a novel RoI jitter strat-
egy to augment the shift modes. Speciﬁcally, the RoI jitter

where tj denotes jitter targets of x-axis and y-axis, and σ is
the hyperparameter of the radiation extent of jitter. After,
the RoI jitters to the RoIj by using the inverse process of
bounding box transformation of Equation 1.

Reference RFSensed RFCross-Modal RFFC LayersContextualRoI Pooling⊕Region Shiftobjects only exist in one modality, treating them as either
background or foreground will lead to ambiguous classiﬁ-
cation. To mitigate this feature ambiguity, we calculate a
disagreement weight, Wd = 1 − |p1
r − p0
s|,
and perform re-weighting on the sensed features, i.e. the
sensed feature will be depressed if it provides a contradic-
tory prediction with the reference modality.

s| = 1 − |p0

r − p1

5. Experiments

In this section, we conduct several experiments on the
KAIST [24] and CVC-14 [17] dataset. We set the more
reliable thermal input as the reference image and color input
as the sensed one, the opposite conﬁguration is discussed in
the supplementary material. All methods are evaluated on
the “reasonable” setup [13].

5.1. Dataset

KAIST The popular KAIST dataset

[24] contains
95, 328 color and thermal image pairs with 103, 128 dense
annotations and 1, 182 unique pedestrians.
It is recorded
in various scenes at day and night to cover the changes in
light conditions. Detection performance is evaluated on the
test set, which consists of 2, 252 frames sampled every 20th
frame from videos.

CVC-14 The CVC-14 dataset [17] contains visible
(grayscale) plus thermal video sequences, captured by a
car traversing the streets at 10 FPS during day and night
time. The training and testing set contains 7, 085 and 1, 433
frames, respectively. Note that even with post-processing,
the cameras are still not well calibrated. As a result, an-
notations are individually provided in each modality.
It
is worth noting that the CVC-14 dataset has a more seri-
ous position shift problem, see Figure 4(b), which makes
it difﬁcult for state-of-the-art methods to use the dataset
[52, 32, 18, 55, 5].

5.2. Implementation Details

Our AR-CNN detector uses VGG-16 [43] as the back-
bone network, which is pre-trained on the ILSVRC CLS-
LOC dataset [29]. We set the σ0 and σ1 of RoI jitter to 0.05
by default, which can be adjusted to handle wider or nar-
rower misalignment. All the images are horizontally ﬂipped
for data augmentation. We train the detector for 2 epochs
with the learning rate of 0.005 and decay it by 0.1 for an-
other 1 epoch. The network is optimized using the Stochas-
tic Gradient Descent (SGD) algorithm with 0.9 momentum
and 0.0005 weight decay. Multi-scale training and testing
are not applied to ensure fair comparisons with other meth-
ods.

As for evaluation, the log miss rate averaged over the
false positives per image (FPPI) range of [10−2, 100] (MR)
is calculated to measure the detection performance, the
lower score indicates better performance. Since there are

Figure 8. Illustration of the conﬁdence-aware fusion method.
There are three typical situations: (a) at day time, the color and
thermal features are consistent and complementary. (b) under poor
illumination, it is difﬁcult to distinguish the pedestrian in color
modality, hence we pay more weight on the thermal modality. (c)
the pedestrian only exists in the thermal modality due to the posi-
tion shift, so we depress the color feature.

Mini-batch Sampling While training the CNN-based
detector, a small set of samples is randomly selected. We
consistently deﬁne the positive and negative examples with
respect to the reference modality, since the RoI jitter process
is only performed on the sensed modality. Speciﬁcally, the
RoI pair is treated as positive if the reference RoI has the
IoU overlap with reference ground truth box greater than
0.5, and negative if the IoU is between 0.1 and 0.5.

4.4. Conﬁdence-Aware Fusion

In around-the-clock operation, modalities provide vari-
ational qualities of information: the color data is discrim-
inable at day time but fades at night;
the thermal data
presents clear human shape throughout the day and night
while loses ﬁne visual details (e.g. clothing). The naive fu-
sion of features from different modalities is not appropriate
since we want the detector to pay more attention to reli-
able modality. To this end, we propose a conﬁdence-aware
fusion method to make full use of the characteristics be-
tween two different modalitites via re-weighting their fea-
tures, and select the more informative features while sup-
pressing less useful ones.

As shown in Figure 8, the conﬁdence-aware module has
a two-stream architecture and fuses the feature maps from
different modalities. This module adds a branch for each
modality, which is composed of two fully connected layers
for the conﬁdence prediction. We calculate two conﬁdence
s|, in which p1 and
r|, Ws = |p1
weights: Wr = |p1
p0 denote the probability of pedestrian and background, r
and f refer to the reference and sensed modality, respec-
tively. Then, we use multiplication to perform feature re-
weighting (see Figure 8) on the input feature maps to select
more reliable features for estimation.

s − p0

r − p0

Unpaired Objects During training, since the unpaired

******ped: 0.94ped: 0.59Wr =0.88Ws =0.18⊕Wd =0.65Thermal (reference)Color (sensed)Wr =0.98Ws =0.96⊕Wd =0.99ped:0.99ped:0.98Wr =0.78Ws =0.96Wd =0.13ped: 0.02⊕ped: 0.89(a)(b)(c)Method

ACF+T+THOG (optimized) [24]
Halfway Fusion [34]
Fusion RPN [28]
Fusion RPN+BF [28]
Adapted Halfway Fusion
IAF-RCNN [32]
IATDNN+IAMSS [18]
CIAN [55]
MSDS-RCNN [31]
AR-CNN (Ours)

Day
29.59
24.88
19.55
16.49
15.36
14.55
14.67
14.77
10.60
9.94

MR
Night
34.98
26.59
22.12
15.15
14.99
18.26
15.72
11.13
13.73
8.38

All
31.34
25.75
20.67
15.91
15.18
15.73
14.95
14.12
11.63
9.34

Day
29.85
24.29
19.69
16.60
14.56
14.95
14.82
15.13
9.91
9.55

MRC
Night
36.77
26.12
21.83
15.28
15.72
18.11
15.87
12.43
14.21
10.21

All
32.01
25.10
20.52
15.98
15.06
15.65
15.14
14.64
11.28
9.86

Day
30.40
25.20
21.08
17.56
15.48
15.22
15.02
16.21
12.02
9.08

MRT
Night
34.81
24.90
20.88
14.48
14.84
17.56
15.20
9.88
13.01
7.04

All
31.90
25.51
21.43
16.52
15.59
16.00
15.08
14.68
12.51
8.26

Table 2. Comparisons with the state-of-the-art methods on the KAIST dataset. Besides the MR protocol, we also evaluate the detectors on
MRC and MRT in the KAIST-Paired annotation.

Method

SVM [17]
DPM [17]
Random Forest [17]
ACF [39]
Faster R-CNN [39]
MACF [39]
Choi et al. [6]
Halfway Fusion [39]
Park et al. [39]
AR-CNN (Ours)

e
l
b
i
s
i
V

l
a
m
r
e
h
T
+
e
l
b
i
s
i
V

MR
Day Night
76.9
37.6
76.4
25.2
81.2
26.6
83.2
65.0
71.4
43.2
48.2
61.3
43.8
49.3
34.4
38.1
30.8
31.8
18.1
24.7

All
-
-
-
71.3
51.9
60.1
47.3
37.0
31.4
22.1

Table 3. Pedestrian detection results on the CVC-14 dataset. MR
is used to compare the performance of detectors. The ﬁrst column
refers to input modalities of the approach. We use the reimple-
mentation of ACF, Faster R-CNN, MACF, and Halfway Fusion in
literature [39].

some problematic annotations in the original test set of
the KAIST benchmark, we use the widely adopted im-
proved test set annotations provided by Liu et al. [34]. Be-
sides, based on our annotated KAIST-Paired, we propose
the MRC and MRT which indicate the log-average miss rate
on color and thermal modality.

5.3. Comparison Experiments

KAIST. We evaluate our approach and conduct compar-
isons with other published methods, as illustrated in Table
2. Our proposed approach achieves 9.94 MR, 8.38 MR,
and 9.34 MR on the reasonable day, night, and all-day sub-
set respectively, better than other available competitors (i.e.
[24, 34, 28, 32, 18, 55]). Besides, in consideration of the

position shift problem, we also evaluate the state-of-the-art
methods with the KAIST-Paired annotation, i.e. log-average
miss rate associated with color modality (MRC) and ther-
mal modality (MRT ). From Table 2 we can see that our AR-
CNN detector has greater advantages, i.e. 9.86 vs. 11.28
MRC and 8.26 vs. 12.51 MRT , demonstrating the superi-
ority of the proposed approach.

CVC-14. We follow the protocol in [39] to conduct the
experiments. Table 3 shows that the AR-CNN outperforms
all state-of-the-art methods, especially on the night subset
(18.1 vs. 30.8 MR). This validates the contribution of ther-
mal modality, and demonstrates the performance can be sig-
niﬁcantly boosted by correctly utilizing the weakly aligned
modalities.

5.4. Robustness to Position Shift

Following the settings in Section 3.2, we test the robust-
ness of AR-CNN to position shift by evaluating the MRT
on KAIST dataset. Figure 3(b) depicts the visual results by
a surface plot. Compared to the baseline results in Figure
3(a), it can be observed that the robustness to position shift
is signiﬁcantly enhanced, with the overall performance im-
proved. To further evaluate the robustness, we design four
, S135◦
metrics1: S0◦
, where the 0◦-135◦ indi-
cate the shift directions. For each shift direction, we have 21
shift modes, which range from −10 to 10 pixels. The mean
and standard deviation of those 21 results are calculated and

, S90◦

, S45◦

1∆x and ∆y denote the shift pixels, which are selected in the following

sets:
S0◦
S45◦
S90◦
S135◦

: {(∆x, ∆y) | ∆y = 0, ∆x ∈ [−10, 10]; ∆x ∈ Z}

: {(∆x, ∆y) | ∆x = ∆y, ∆x ∈ [−10, 10]; ∆x ∈ Z}

: {(∆x, ∆y) | ∆x = 0, ∆y ∈ [−10, 10]; ∆y ∈ Z}

: {(∆x, ∆y) | ∆x = −∆y, ∆y ∈ [−10, 10]; ∆y ∈ Z}

Method

Halfway Fusion [34]
Fusion RPN [28]
Adapted Halfway Fusion
CIAN [55]
MSDS-RCNN [31]

AR-CNN

RFA RoIJ CAF

(cid:88)
(cid:88)
(cid:88)

(cid:88)
(cid:88)

(cid:88)

O
25.51
21.43
15.59
14.68
12.51

12.94
10.90
9.87
8.26

S0◦
µ
33.73
30.12
23.44
23.64
20.96

21.05
11.91
11.17
9.34

S45◦

S90◦

S135◦

µ
36.25
31.69
26.91
24.07
24.43

15.80
12.38
11.84
9.73

σ
9.66
10.60
11.55
11.50
11.74

9.77
2.59
1.71
1.24

µ
28.30
24.48
17.95
15.07
14.42

13.46
11.00
10.27
8.91

σ
2.26
1.97
2.03
1.35
1.34

1.04
0.21
0.17
0.43

µ
36.71
34.02
27.26
23.98
24.23

16.18
12.34
11.50
9.79

σ
9.87
10.64
11.18
11.57
10.99

6.91
2.27
1.34
1.04

σ
7.17
7.13
7.49
7.69
7.87

7.10
2.81
1.20
0.95

Table 4. Quantitative results of the robustness of detectors to position shift on the KAIST dataset. O denotes the MRT score at the origin,
µ, σ represents the mean and standard deviation of MRT scores respectively. In this testing, we reimplement the ACF+T+THOG, Halfway
Fusion and Fusion RPN, and use the model provided in [55] and [31] for CIAN and MSDS-RCNN.

shown in Table 4. It can be observed that our AR-CNN de-
tector achieves the best mean performance and the smallest
standard deviation on all metrics, demonstrating the robust-
ness of the proposed approach under diverse position shift
conditions.

5.5. Ablation Study

In this section, we perform ablation experiments on the
KAIST dataset for a detailed analysis of our AR-CNN de-
tector. All the ablated detectors are trained using the same
setting of parameters.

Region Feature Alignment Module. To demonstrate
the contribution of the RFA module, we evaluate the perfor-
mance with and without RFA in Table 4. We ﬁnd the RFA
remarkably reduces the MRT and the standard deviation un-
der diverse position shift conditions. Speciﬁcally, for S45◦
,
the standard deviation is reduced by a signiﬁcant 8.53 (from
9.77 to 1.24), and consistent reduction is also observed on
the other three metrics.

RoI Jitter Strategy. Based on the RFA module, we fur-
ther add the RoI jitter strategy and evaluate its contribution.
As shown in Table 4, the RoI jitter further reduces the mean
and standard deviation of results and achieves 9.87 MRT at
the origin. Besides, we can see that RoI jitter works more
on standard deviation than the performance, which demon-
strates that it improves the robustness to shift patterns.

Conﬁdence-Aware Fusion Method. To validate the
effectiveness of the conﬁdence-aware fusion method, we
compared performance with and without it. As shown in
Table 4, the newly added conﬁdence-aware fusion method
slightly depresses the standard deviation, and further re-
duces MRT at the origin by 1.61. This demonstrates
that the detection performance can be further improved by
conﬁdence-aware fusion, since it helps the network to select

more reliable features for adaptive fusion.

6. Conclusion

In this paper, a novel Aligned Region CNN method is
proposed to alleviate the negative effects of position shift
problem in weakly aligned image pairs. Speciﬁcally, we
design a new region feature alignment module, which pre-
dicts the position shift and aligns the region features be-
tween modalities. Besides, an RoI jitter training strategy is
adopted to further improve the robustness to random shift
patterns. Meanwhile, we present a novel conﬁdence-aware
fusion method to enhance the representation ability of fused
feature via adaptively re-weighting the features. To realize
our method, we relabel the large-scale KAIST dataset by lo-
cating the bounding boxes in both modalities and building
their relationships. Our model is trained in an end-to-end
fashion and achieves state-of-the-art accuracy on the chal-
lenging KAIST and CVC-14 dataset. Furthermore, the de-
tector robustness to position shift is improved with a large
margin. It is also worth noting that our method is a generic
solution for multispectral object detection rather than only
the pedestrian problem. In the future, we plan to explore
the generalization of the AR-CNN detector and extend it to
other tasks, considering this weakly aligned characteristic is
widespread and hard to completely avoid when multimodal
inputs are required.

Acknowledgments.
This work was supported by the
National Key Research and Development Plan of China
under Grant 2017YFB1300202, the NSFC under Grants
the
U1613213, 61627808, 61876178, and 61806196,
Strategic Priority Research Program of Chinese Academy
of Science under Grant XDB32050100.

References

[1] Garrick Brazil, Xi Yin, and Xiaoming Liu.

Illuminating
pedestrians via simultaneous detection & segmentation. In
Proceedings of the IEEE International Conference on Com-
puter Vision (ICCV), pages 4950–4959, 2017.

[2] Lisa Gottesfeld Brown. A survey of image registration tech-
niques. ACM computing surveys (CSUR), 24(4):325–376,
1992.

[3] Pradeep Buddharaju, Ioannis T Pavlidis, Panagiotis Tsi-
amyrtzis, and Mike Bazakos. Physiology-based face recog-
IEEE Transac-
nition in the thermal infrared spectrum.
tions on Pattern Analysis and Machine Intelligence (PAMI),
29(4):613–626, 2007.

[4] Frantisek Burian, Petra Kocmanova, and Ludek Zalud.
Robot mapping with range camera, ccd cameras and ther-
In Methods and Models in Automation and
mal imagers.
Robotics (MMAR), 2014 19th International Conference On,
pages 200–205, 2014.

[5] Yanpeng Cao, Dayan Guan, Yulun Wu, Jiangxin Yang, Yan-
long Cao, and Michael Ying Yang. Box-level segmentation
supervised deep neural networks for accurate and real-time
ISPRS Journal of Pho-
multispectral pedestrian detection.
togrammetry and Remote Sensing, 150:70–79, 2019.

[6] Hangil Choi, Seungryong Kim, Kihong Park,

and
Kwanghoon Sohn. Multi-spectral pedestrian detection based
on accumulated object proposal with fully convolutional net-
works. In 2016 23rd IEEE International Conference on Pat-
tern Recognition (ICPR), pages 621–626, 2016.

[7] Yukyung Choi, Namil Kim, Soonmin Hwang, and In So
Kweon. Thermal image enhancement using convolutional
neural network. In 2016 IEEE/RSJ International Conference
on Intelligent Robots and Systems (IROS), pages 223–230,
2016.

[8] Yukyung Choi, Namil Kim, Soonmin Hwang, Kibaek Park,
Jae Shin Yoon, Kyounghwan An, and In So Kweon. Kaist
multi-spectral day/night data set for autonomous and assisted
IEEE Transactions on Intelligent Transportation
driving.
Systems, 19(3):934–948, 2018.

[9] Ciprian Adrian Corneanu, Marc Oliu Sim´on, Jeffrey F Cohn,
and Sergio Escalera Guerrero. Survey on rgb, 3d, thermal,
and multimodal approaches for facial expression recogni-
tion: History, trends, and affect-related applications. IEEE
Transactions on Pattern Analysis and Machine Intelligence
(PAMI), 38(8):1548–1568, 2016.

[10] Suma Dawn, Vikas Saxena, and Bhudev Sharma. Remote
In Inter-
sensing image registration techniques: A survey.
national Conference on Image and Signal Processing, pages
103–112, 2010.

[11] Zhuo Deng and Longin Jan Latecki. Amodal detection of
3d objects: Inferring 3d bounding boxes from 2d ones in
rgb-depth images. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), pages
5762–5770, 2017.

[12] Piotr Doll´ar, Ron Appel, Serge Belongie, and Pietro Perona.
IEEE Transac-
Fast feature pyramids for object detection.
tions on Pattern Analysis and Machine Intelligence (PAMI),
36(8):1532–1545, 2014.

[13] Piotr Dollar, Christian Wojek, Bernt Schiele, and Pietro Per-
ona. Pedestrian detection: An evaluation of the state of the
art. IEEE Transactions on Pattern Analysis and Machine In-
telligence (PAMI), 34(4):743–761, 2012.

[14] Piotr Dollr, Zhuowen Tu, Pietro Perona, and Serge Belongie.
Integral channel features. In British Machine Vision Confer-
ence (BMVC), 2009.

[15] Chenqiang Gao, Yinhe Du, Jiang Liu, Jing Lv, Luyu Yang,
Deyu Meng, and Alexander G Hauptmann. Infar dataset: In-
frared action recognition at different times. Neurocomputing,
212:36–47, 2016.

[16] Ross Girshick. Fast R-CNN.

In Proceedings of the IEEE
International Conference on Computer Vision (ICCV), pages
1440–1448, 2015.

[17] Alejandro Gonz´alez, Zhijie Fang, Yainuvis Socarras, Joan
Serrat, David V´azquez, Jiaolong Xu, and Antonio M L´opez.
Pedestrian detection at day/night time with visible and ﬁr
cameras: A comparison. Sensors, 16(6):820, 2016.

[18] Dayan Guan, Yanpeng Cao, Jun Liang, Yanlong Cao, and
Michael Ying Yang. Fusion of multispectral data through
illumination-aware deep neural networks for pedestrian de-
tection. Information Fusion, 50:148–157, 2019.

[19] Dayan Guan, Yanpeng Cao, Jiangxin Yang, Yanlong Cao,
and Christel-Loic Tisse. Exploiting fusion architectures for
multispectral pedestrian detection and segmentation. Applied
Optics, 57(18):D108–D116, 2018.

[20] Saurabh Gupta, Ross Girshick, Pablo Arbel´aez, and Jiten-
dra Malik. Learning rich features from rgb-d images for ob-
ject detection and segmentation. In Proceedings of the Eu-
ropean conference on Computer Vision (ECCV), pages 345–
360, 2014.

[21] Kaiming He, Georgia Gkioxari, Piotr Doll´ar, and Ross Gir-
shick. Mask r-cnn. In Proceedings of the IEEE International
Conference on Computer Vision (ICCV), pages 2980–2988,
2017.

[22] Jan Hosang, Mohamed Omran, Rodrigo Benenson, and
Bernt Schiele. Taking a deeper look at pedestrians. In Pro-
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), pages 4073–4082, 2015.
[23] Soonmin Hwang, Yukyung Choi, Namil Kim, Kibaek Park,
Jae Shin Yoon, and In So Kweon. Low-cost synchroniza-
In 2015 12th International
tion for multispectral cameras.
Conference on Ubiquitous Robots and Ambient Intelligence
(URAI), pages 435–436. IEEE, 2015.

[24] Soonmin Hwang, Jaesik Park, Namil Kim, Yukyung Choi,
and In So Kweon. Multispectral pedestrian detection:
Benchmark dataset and baseline. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition
(CVPR), pages 1037–1045, 2015.

[25] Namil Kim, Yukyung Choi, Soonmin Hwang, and In So
Kweon. Multispectral transfer network: Unsupervised depth
estimation for all-day vision. In Thirty-Second AAAI Con-
ference on Artiﬁcial Intelligence, 2018.

[26] Namil Kim, Yukyung Choi, Soonmin Hwang, Kibaek Park,
Jae Shin Yoon, and In So Kweon. Geometrical calibration of
multispectral calibration. In 2015 12th International Confer-
ence on Ubiquitous Robots and Ambient Intelligence (URAI),
pages 384–385. IEEE, 2015.

[27] Seong G Kong, Jingu Heo, Faysal Boughorbel, Yue Zheng,
Besma R Abidi, Andreas Koschan, Mingzhong Yi, and
Mongi A Abidi. Multiscale fusion of visible and thermal
ir images for illumination-invariant face recognition. Inter-
national Journal of Computer Vision, 71(2):215–233, 2007.
[28] Daniel K¨onig, Michael Adam, Christian Jarvers, Georg Lay-
her, Heiko Neumann, and Michael Teutsch. Fully convolu-
tional region proposal networks for multispectral person de-
tection. In IEEE Conference on Computer Vision and Pattern
Recognition Workshops (CVPRW), pages 243–250, 2017.

[29] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.
Imagenet classiﬁcation with deep convolutional neural net-
works. In Advances in Neural Information Processing Sys-
tems (NeurIPS), pages 1097–1105, 2012.

[30] Alex Leykin, Yang Ran, and Riad Hammoud. Thermal-
visible video fusion for moving target tracking and pedes-
trian classiﬁcation. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), pages
1–8, 2007.

[31] Chengyang Li, Dan Song, Ruofeng Tong, and Min Tang.
Multispectral pedestrian detection via simultaneous detec-
tion and segmentation. In British Machine Vision Conference
(BMVC), September 2018.

[32] Chengyang Li, Dan Song, Ruofeng Tong, and Min Tang.
Illumination-aware faster r-cnn for robust multispectral
Pattern Recognition, 85:161–171,
pedestrian detection.
2019.

[33] Jianan Li, Xiaodan Liang, ShengMei Shen, Tingfa Xu, Ji-
ashi Feng, and Shuicheng Yan. Scale-aware fast r-cnn for
IEEE Transactions on Multimedia,
pedestrian detection.
20(4):985–996, 2018.

[34] Jingjing Liu, Shaoting Zhang, Shu Wang, and Dimitris N
Metaxas. Multispectral deep neural networks for pedestrian
In British Machine Vision Conference (BMVC),
detection.
2016.

[35] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian
Szegedy, Scott Reed, Cheng-Yang Fu, and Alexander C
Berg. SSD:single shot multibox detector. In Proceedings of
the European conference on Computer Vision (ECCV), pages
21–37, 2016.

[36] JB Antoine Maintz and Max A Viergever. A survey of med-
ical image registration. Medical image analysis, 2(1):1–36,
1998.

[37] Jiayuan Mao, Tete Xiao, Yuning Jiang, and Zhimin Cao.
What can help pedestrian detection? In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), pages 3127–3136, 2017.

[38] Woonhyun Nam, Piotr Doll´ar, and Joon Hee Han. Local
decorrelation for improved pedestrian detection. In Advances
in Neural Information Processing Systems (NeurIPS), pages
424–432, 2014.

[39] Kihong Park, Seungryong Kim, and Kwanghoon Sohn. Uni-
ﬁed multi-spectral pedestrian detection based on probabilis-
tic fusion networks. Pattern Recognition, 80:143–155, 2018.
[40] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
Faster R-CNN: Towards real-time object detection with re-
gion proposal networks. In Advances in Neural Information
Processing Systems (NeurIPS), pages 91–99, 2015.

[41] Philip Saponaro, Scott Sorensen, Abhishek Kolagunda, and
Chandra Kambhamettu. Material classiﬁcation with thermal
imagery. In IEEE Conference on Computer Vision and Pat-
tern Recognition (CVPR), pages 4649–4656, 2015.

[42] Pierre Sermanet, Koray Kavukcuoglu, Soumith Chintala,
and Yann LeCun. Pedestrian detection with unsupervised
In Proceedings of the IEEE
multi-stage feature learning.
Conference on Computer Vision and Pattern Recognition
(CVPR), pages 3626–3633, 2013.

[43] Karen Simonyan and Andrew Zisserman. Very deep convo-
lutional networks for large-scale image recognition. arXiv
preprint arXiv:1409.1556, 2014.

[44] Diego A Socolinsky, Andrea Selinger, and Joshua D
Neuheisel. Face recognition with visible and thermal in-
frared imagery. Computer Vision and Image Understanding,
91(1-2):72–114, 2003.

[45] Shuran Song and Jianxiong Xiao. Deep sliding shapes for
In Proceed-
amodal 3d object detection in rgb-d images.
ings of the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), pages 808–816, 2016.

[46] Atousa Torabi, Guillaume Mass´e, and Guillaume-Alexandre
Bilodeau. An iterative integrated framework for thermal–
visible image registration, sensor fusion, and people tracking
for video surveillance applications. Computer Vision and Im-
age Understanding, 116(2):210–221, 2012.

[47] Wayne Treible, Philip Saponaro, Scott Sorensen, Abhishek
Kolagunda, Michael O’Neal, Brian Phelan, Kelly Sher-
bondy, and Chandra Kambhamettu. Cats: A color and ther-
mal stereo benchmark. In Proceedings of the IEEE Confer-
ence on Computer Vision and Pattern Recognition (CVPR),
pages 2961–2969, 2017.

[48] J¨org Wagner, Volker Fischer, Michael Herman, and Sven
Behnke. Multispectral pedestrian detection using deep fu-
sion convolutional neural networks. In 24th European Sym-
posium on Artiﬁcial Neural Networks, Computational Intel-
ligence and Machine Learning (ESANN), pages 509–514,
2016.

[49] Chen Wang, Danfei Xu, Yuke Zhu, Roberto Mart´ın-Mart´ın,
Cewu Lu, Li Fei-Fei, and Silvio Savarese. Densefusion: 6d
In IEEE
object pose estimation by iterative dense fusion.
Conference on Computer Vision and Pattern Recognition
(CVPR), pages 3343–3352, 2019.

[50] Xinlong Wang, Tete Xiao, Yuning Jiang, Shuai Shao, Jian
Sun, and Chunhua Shen. Repulsion loss: Detecting pedes-
In Proceedings of the IEEE Conference
trians in a crowd.
on Computer Vision and Pattern Recognition (CVPR), pages
7774–7783, 2018.

[51] Danfei Xu, Dragomir Anguelov, and Ashesh Jain. Pointfu-
sion: Deep sensor fusion for 3d bounding box estimation. In
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), pages 244–253, 2018.

[52] Dan Xu, Wanli Ouyang, Elisa Ricci, Xiaogang Wang, and
Nicu Sebe. Learning cross-modal deep representations for
In Proceedings of the IEEE
robust pedestrian detection.
Conference on Computer Vision and Pattern Recognition
(CVPR), pages 5363–5371, 2017.

[53] Bin Yang, Junjie Yan, Zhen Lei, and Stan Z Li. Convolu-
tional channel features. In Proceedings of the IEEE Interna-

tional Conference on Computer Vision (ICCV), pages 82–90,
2015.

[54] Liliang Zhang, Liang Lin, Xiaodan Liang, and Kaiming He.
Is faster r-cnn doing well for pedestrian detection? In Pro-
ceedings of the European conference on Computer Vision
(ECCV), pages 443–457, 2016.

[55] Lu Zhang, Zhiyong Liu, Shifeng Zhang, Xu Yang, Hong
Qiao, Kaizhu Huang, and Amir Hussain. Cross-modality
interactive attention network for multispectral pedestrian de-
tection. Information Fusion, 50:20–29, 2019.

[56] Shanshan Zhang, Rodrigo Benenson, and Bernt Schiele. Fil-
tered channel features for pedestrian detection. In Proceed-
ings of the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), pages 1751–1760, 2015.

[57] Shanshan Zhang, Rodrigo Benenson, and Bernt Schiele.
Citypersons: A diverse dataset for pedestrian detection. In
Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), pages 3213–3221, 2017.

[58] Shifeng Zhang, Longyin Wen, Xiao Bian, Zhen Lei, and
Stan Z Li. Occlusion-aware r-cnn: detecting pedestrians in a
crowd. In Proceedings of the European Conference on Com-
puter Vision (ECCV), pages 637–653, 2018.

[59] Yu Zhu and Guodong Guo. A study on visible to in-
frared action recognition. IEEE Signal Processing Letters,
20(9):897–900, 2013.

[60] Barbara Zitova and Jan Flusser. Image registration methods:
Image and Vision Computing, 21(11):977–1000,

a survey.
2003.

