2
2
0
2

g
u
A
3
1

]

G
L
.
s
c
[

1
v
7
3
5
6
0
.
8
0
2
2
:
v
i
X
r
a

Published as a conference paper at ICLR 2021

DEFENSE AGAINST BACKDOOR ATTACKS VIA IDENTI-
FYING AND PURIFYING BAD NEURONS

Mingyuan Fan 1 Cen Chen 2 Ximeng Liu 1 Wenzhong Guo 1
1 Fuzhou University 2 East China Normal University
fmy2660966@gmail.com cenchen@dase.ecnu.edu.cn
snbnix@gmail.com guowenzhong@fzu.edu.cn

ABSTRACT

The opacity of neural networks leads their vulnerability to backdoor attacks,
where hidden attention of infected neurons is triggered to override normal pre-
dictions to the attacker-chosen ones. In this paper, we propose a novel backdoor
defense method to mark and purify the infected neurons in the backdoored neu-
ral networks. Speciﬁcally, we ﬁrst deﬁne a new metric, called benign salience.
By combining the ﬁrst-order gradient to retain the connections between neurons,
benign salience can identify the infected neurons with higher accuracy than the
commonly used metric in backdoor defense. Then, a new Adaptive Regulariza-
tion (AR) mechanism is proposed to assist in purifying these identiﬁed infected
neurons via ﬁne-tuning. Due to the ability to adapt to different magnitudes of
parameters, AR can provide faster and more stable convergence than the com-
mon regularization mechanism in neuron purifying. Extensive experimental re-
sults demonstrate that our method can erase the backdoor in neural networks with
negligible performance degradation.

1

INTRODUCTION

Beneﬁted from the powerful representation learning ability, neural networks (NNs) play an imper-
ative role in many ﬁelds, especially for image processing Al-Saffar et al. (2017). However, the
brilliant feat of NN also makes it a focal point of many attacks, one of the most threatening among
which is the backdoor attack Gu et al. (2017); Liu et al. (2017); Barni et al. (2019); Chen et al.
(2017). By mixing poisoned data into the training set, backdoor attack can control the victim NN
to output attacker-chosen predictions for triggered inputs, while hardly disturb the predictions of
normal inputs. Moreover, the triggers crafted by the attacker can be only several blocks of pixels Gu
et al. (2017); Liu et al. (2017); Li et al. (2020) or even the invisible noises Zhong et al. (2020); Barni
et al. (2019), which makes the attack notoriously perilous in applications.

By investigating the workﬂows of existing backdoor attacks Gu et al. (2017); Liu et al. (2017); Li
et al. (2020); Barni et al. (2019); Zhong et al. (2020); Chen et al. (2017), it can be observed that all
of the existing attacks involve two common steps. First, the attacker adds a small portion of samples
polluted with triggers (usually less than 5%) into the training set insensibly. Second, during the
training process, the attacker induces some neural neurons in the target neural network to memorize
the trigger patterns. Based on the ﬁndings in Gu et al. (2017), backdoor attacks can succeed because
the neurons that memorize trigger patterns, often called bad neurons, only keep a strong connection
with the triggers but rarely react to normal features. Naturally, to defend the attack, a direct idea
is to remove or mitigate the effect of bad neurons, e.g., via model-pruning or ﬁne-tuning Gu et al.
(2017). However, the existing defense methods are mainly ﬂawed from two perspectives.

Neuron Evaluation. To evaluate which neurons are infected, current approaches are mostly based
on the activation magnitude (AM) of neurons Gu et al. (2017). Neurons with low AMs about normal
inputs are deemed to be “bad”, and pruned during the defense stage. Despite being intuitive and
easy to apply, this metric sometimes leads to the over-pruning of good neurons as it may ignore the
connections between neurons. For instance, consider a case where a clean neuron N in the network
has low AMs but has very high weights on its downstream neurons. N has a strong positive effect
on the ﬁnal predictions but is still judged to be bad based on the above rule.

1

 
 
 
 
 
 
Published as a conference paper at ICLR 2021

Neuron Purifying. As bad neurons are marked in the evaluation stage, the next step is to purify or
prune them. Currently, most pruning based defense methods choose to roughly remove bad neuron
from the backdoored network. However, as pointed out by Liu et al. Liu et al. (2018), some bad
neurons can also be related to the predictions of normal data. Therefore, such a choice can cause the
performance degradation of the target models.

In this paper, we propose a novel backdoor defense method to overcome the two ﬂaws. Speciﬁcally,
instead of utilizing AM, we deﬁne a more effective metric called benign salience (BS, Section 4.1)
to evaluate the importance of neurons. Compared to AM, BS retains the connections between neu-
rons through the ﬁrst-order gradient, hence bad neurons can be identiﬁed more correctly. Then, after
ﬁltering bad neurons, our method chooses to ﬁne-tune but not directly prune them in the neuron pu-
rifying stage (Section 4.2) to avoid unexpected performance degradation. Through a newly designed
adaptive piece-wise regularization mechanism, our ﬁne-tuning method can be far more effective in
mitigating the network’s attention on trigger patterns than the existing ﬁne-tuning method Truong
et al. (2020).

Our contributions are summarized in four folds:

• We propose a novel backdoor defense method (called WIPER), which can combine the

advantages of both model pruning and ﬁne-tuning to identify and purify bad neurons.

• We design a new metric BS to mark the bad neurons whose attention is misled to the
backdoor trigger patterns. Since the connections between neurons are reserved via the
ﬁrst-order gradient, defenders can use BS to distinguish bad neurons with higher accuracy
than the commonly used metric AM.

• We develop a new type of regularization, called adaptive regularization (AR). Compared
to the conventional regularization, AR can better accelerate and stabilize the purifying pro-
cess of bad neurons by adaptively adjusting the penalty degree to different magnitudes of
parameters.

• We conduct extensive experiments on some widely-used datasets to validate the effective-
ness of our defense method. The result shows that our performance signiﬁcantly outper-
forms the state-of-the-art defense methods.

2 RELATED WORKS

In this section, we brieﬂy review the prior works about backdoor attack and defense methods.

Backdoor attack. Backdoor attack Gu et al. (2017); Liu et al. (2017); Nguyen & Tran (2021); Pang
et al. (2020) is one of the most threatening attacks to neural attacks due to its high harmfulness and
stealthiness. A typical backdoor attack is done by injecting a small volume of poison data crafted
with the attacker-chosen triggers into the training set. To maintain the stealthiness of backdoor at-
tacks, the triggers used by attackers are various. For instance, the trigger used in Gu et al. (2017);
Liu et al. (2017); Li et al. (2020) was a simple rect with the resolution of 3 × 3. Blend attack Chen
et al. (2017) adopted common life devices, e.g. glasses, as a trigger so as to evade the inspection of
humans. Moreover, the authors in Zhong et al. (2020) attempted to design a human-imperceptible
yet effective noise. The clean-label attack Shafahi et al. (2018) made the natural features in images
difﬁcultly learnable so that the model was forced to only rely on the trigger to correctly classify with-
out modifying the label. More recently, similar to clean-label attack, sinusoidal signal attack Barni
et al. (2019) designed an easily-learned trigger to conduct the backdoor attack. WIPER is designed
to provide an effective to defend the above-mentioned attacks and ensure model security in applica-
tions.

Backdoor defense. Existing methods against backdoor attacks can be roughly divided into two
categories: detection-based methods Wang et al. (2019); Gao et al. (2019); Wang et al. (2020);
Xu et al. (2021) that aim to detect whether a neural network is backdoored, and purifying-based
methods Liu et al. (2018); Truong et al. (2020); Zhao et al. (2019); Yoshida & Fujino (2020); Li et al.
(2021a) that try to remove the backdoor while maintaining the performance of the target model. Up
to now, the detection-based method has been well developed and many remarkable works Gao et al.
(2019); Wang et al. (2020); Xu et al. (2021) achieve quite a high detection rate. Thus, this paper
mainly focuses on the bad neuron purifying to remove the backdoor. Inspired by the fact that the

2

Published as a conference paper at ICLR 2021

bad neurons were dormant with the presence of clean data, ﬁne-pruning Liu et al. (2018) removed
the backdoor by erasing the neurons with activation values below a certain threshold. However, the
effectiveness of ﬁne-pruning heavily depends on the quality of holding data, and it can be easily
evaded by some state-of-the-art attacks, such as TrojanNN Liu et al. (2017). In Truong et al. (2020),
the authors suggested that referring to catastrophic forgetting Delange et al. (2021), ﬁne-tuning the
model with some clean data was a simple yet effective method to remove the backdoor. Similar
to ﬁne-pruning, ﬁne-tuning also need high-quality clean data to mitigate the effect of bad neurons.
Recently, knowledge distillation Yoshida & Fujino (2020); Li et al. (2021a) was proposed to achieve
backdoor defense by distilling clean knowledge from the infected model to a fresh model but failed
to defend TrojanNN Liu et al. (2017) due to the lack of consideration for its special trigger attention
mechanism.

3 PROBLEM DESCRIPTION

Problem Setting. We consider the typical backdoor attack scenario as discussed in Gu et al. (2017);
Liu et al. (2017); Barni et al. (2019); Li et al. (2020). In the scenario, a model trainer is with the
knowledge of the training set Dt collected from multiple data providers and a self-owned clean
set Dc. The trainer seeks to leverage Dt to train a neural network F. Moreover, among the data
providers, there exist malicious ones A that try to stealthily upload poisoned data to backdoor the
trained model F.

Attack Goal. In the above scenario, the attacker A aims to complete the following goals.

• By adding speciﬁc triggers to the inputs, A can mislead the backdoored network F (cid:48) to
output desired labels that are different from the ground-truth predictions (or predictions of
a clean network F).

• To maintain the stealthiness of the attack, A should make the backdoored network F (cid:48) to

perform similar to F as being given the clean inputs.

Defense Goal. As opposed to the attack goals, the defender, i.e., the model trainer who has full
access to the internal architecture of the target model, aims to achieve two goals.

• The ﬁrst goal is to erase the backdoors from F (cid:48) and make the puriﬁed model perform

correctly as being given with triggered inputs.

• To maintain utility, the second defense goal is to not obviously lower the performance of

the model on normal inputs during the purifying process.

Note, to ensure the feasibility of the defense method in varying applications, the defender is limited
to only utilizing its self-owned validation set to achieve the defense goals.

4 APPROACH

In this section, we present the design details of WIPER. As illustrated in Algorithm 1, WIPER
is mainly composed of two parts. One part is to use the newly proposed metric to evaluate the
importance of neurons and ﬁnd the bad neurons required to be cleansed. The other part leverages a
self-designed loss function to disable the effect of these bad neurons on the target model.

4.1 NEURON IMPORTANCE EVALUATION

WIPER uses a novel metric called BS to evaluate the importance of neurons and distinguish bad
neurons. As mentioned before, BS outperforms the conventional metric AM used in the existing
pruning-based defense method Liu et al. (2018) because instead of evaluating a neuron indepen-
dently, BS additionally considers the connections between neurons in importance evaluation.

4.1.1 THE DEFINITION OF BS

In more details, the deﬁnition of BS basically follows two principles: 1) the importance of a neuron
positively correlates to its contribution to the loss decreasing of the target network; 2) connections

3

Published as a conference paper at ICLR 2021

Protocol 1 WIPER
Input: F (cid:48): the backdoored model; Dc: a set of clean data owned by the defender; A: the set of
neurons needs to be puriﬁed; α:
the hyperparameter to adjust the purifying magnitude; β0:
reserved ratio of neurons; l: the tag of the layer required to be puriﬁed; n: the total number of
epoch of purifying; η: momentum accumulation factor.

Output: F: the puriﬁed model.
1: # Neuron Importance Evaluation.
2: Compute the gradient of all neurons on the l-th layer over Dc.
3: Evaluate the importance for the neurons on the l-th layer of F (cid:48) based on Eq. 2 and Eq. 3.
4: Select β0 neurons with least importance (i.e., the lowest MBS) in the l-th layer and add them

into A.

5: # Neuron Purifying.
6: for each iteration i = 0 to n do
n ).
7:
8:
9:
10:

Update βi ← β0 · (1 − i
for x, y ∈ Dc do

lowest M BS to add into A.

Optimize F (cid:48) based on Lpurif ying.
Update M BSi for each neuron in l-th layer and reset A that select βi neurons with the

end for

11:
12: end for

between neurons are established during the gradient backward propagation process in the training
stage. Following the ﬁrst principle, we ﬁrst deﬁne BS to be the differential loss format.

BS = ∆LDc = LDc(w0) − LDc(w),
(1)
where w denotes the parameters of neurons in a trained (backdoored) network F (cid:48) and w0 is the initial
parameters that usually approximates to 0 Sun (2019). Then, combined with the second principle,
we can rewrite the computation of Eq. 1 through Taylor expansion.

BS ≈ LDc (0) − LDc(w) = (LDc(w) + ∇wLDc(w)T (0 − w) + R1) − LDc(w)

= −∇wLDc(w)T w + R1 ≈ −∇wLDc(w)T w,

(2)

where R1 is the Taylor remainder consisting of 2-th derivative of FDc (w) and quadratic term of
w, which can be omitted in the ﬁnal deduction. Note that, if LDc(·) is a non-linear function, the
errors induced by the ignorance of R1 can be non-negligible. However, for most neural network
architectures, the last a few layers (output layer) are always fully connected, i.e., being linear layers.
At this time, the linear approximation in Eq. 2 becomes reasonable and effective. Therefore, in
practice, WIPER pays more attention to purify the top fully-connected layers with BS to achieve
backdoor defense. We argue that such a setting is effective due to the following reason. Consider
the scenario where the attacker only backdoors the feature extraction layers. The extracted feature
about triggers can be simply blocked by the output layer if the neurons related to the trigger features
are allocated with low weights. Also, the blocking can easily happen because the trigger features are
useless for normal samples. Conversely, if the output layer is backdoored, the attack can be always
successful as soon as the trigger feature is extracted. Therefore, to ensure attack effectiveness, the
objective of most backdoor attacks is focused more on the output layers. The same goes for the
defender. The experimental results in Section 5.5 also prove the above statement.
Intuitively, higher BS indicates higher contribution of the neuron to the loss decreasing of F (cid:48), i.e.,
with more importance. Therefore, as illustrated in Algorithm 1 (Step 4), WIPER marks neurons
with the lowest BS as bad neurons. Moreover, reconsider the backward propagation based gradient
computation process. It can be observed that BS not only considers the activation weight of each
neuron but also involves its connection with downstream neurons into importance evaluation via the
ﬁrst-order gradient ∇wLDc(w)T . Thus, unlike the conventional metric AM1, BS can better avoid
the problem of over-pruning good neurons.

Except for BS, we deﬁne, that can be adopted to evaluate the importance of a neuron, there are many
inspiring metrics that own the similar function, a typical one of which is neuron shapley Ghorbani

1In prior backdoor defense methods Liu et al. (2018), AM is computed by AM = |w · x| where w is the

neuron parameters and x in the input.

4

Published as a conference paper at ICLR 2021

& Zou (2020). Theoretically, neuron shapley can perfectly assess the contribution of a neuron
to the overall performance of the model but seriously suffers from the unaffordable exponential
computation costs (even if with some mitigation measurements such as pruning algorithms or Monte
Carlo estimation Ghorbani & Zou (2020)). For backdoor defense, it requires not only the excellent
ability of backdoor erasing but also acceptable time complexity in applications. Therefore, we argue
that neuron shapley is not a suitable metric to evaluate the importance of a neuron in backdoor
defense scenarios. Thus, at the beginning of our design, we decided to abandon these complex
methods and propose a new metric BS to resolve the problem.

4.1.2 FURTHER DISCUSSION

While implementing WIPER, we notice that although WIPER converges in the expected direction,
its convergence curve suffers from great oscillation. Review the deﬁnition of BS in Eq. 2. It can be
discovered that the unstability is mainly caused by the fact that limited by the real-world computation
resource, WIPER sometimes has to adopt Stochastic Gradient Descent (SGD) to complete model
purifying. With SGD, only one batch of data is taken into computation thereby easily leading the
gradients used to compute BSs to be trapped into local optimum thereby degrading the performance
of WIPER. At this time, we can use the momentum accumulation mechanism to mitigate the problem
as follows.

M BSi(w) = η · M BSi−1(w) + (1 − η) · BS(w), 0 ≤ η ≤ 1,

(3)

where M BSi(w) denotes the accumulated BS of w in the i-th epoch and η is the momentum factor
given by defenders.

Furthermore, in WIPER, the initial proportion β of puriﬁed neurons (with least BS) is an empirical
hyperparameter. Considering that the neurons required to be puriﬁed will be on the decrease along
with the model purifying process, β usually starts with a large value and then is slowly decayed until
the purifying process is completed. Referring to the decaying mechanism of learning rate, we deﬁne
the following decaying function in WIPER.

β = β0 ·

epoch − cur epoch
epoch

,

(4)

where β0 is initial value of β, epoch is the number of training epochs and cur epoch denotes the
current epoch.

4.2 NEURON PURIFYING

We now elaborate how WIPER leverages an improved neuron ﬁne-tuning strategy to achieve bad
neuron purifying.

Without consideration of model performance, a basic idea for neuron purifying is to wipe out bad
neurons by forcing all the parameters of them to be 0. Following the idea and adding the model
performance into consideration, we can derive the following way to achieve bad neuron purifying
like Liu et al. (2018).

Lpurif ying =

n
(cid:88)

i=1

CE(F (cid:48)(xi), yi) + α

(cid:88)

w∈A

||w − 0||k,

(5)

where A denotes the set of bad neurons, CE(·) is the cross-entropy loss function, α is the penalty
coefﬁcient and ||w −0||k is the Lk-norm regularization term (k = 1, 2) that promotes the parameters
of bad neurons to be 0. Although Eq. 5 is straightforward and effective, we notice that the commonly
used Lk regularization always suffers from a too slight (L1) or too strong (L2) punishment degree
and degrades the performance of backdoor defense (detailed in Section 5.6). Diving to the bottom,
such a phenomenon occurs because as w is large (e.g., w (cid:29) 1), L1-regularization usually needs
many rounds of optimization to lower the effect of w and cleanse the attention of bad neurons to
trigger patterns. In contrast, L2-regularization forces w close to 0 in only a couple of rounds but
never makes it to be exactly 0, and thus, easily causes the overshoot of the global optimum.

5

Published as a conference paper at ICLR 2021

To address the issue, we design a novel piece-wise regularization item called adaptive regularization
(AR).

AR(w) =






− e−w−1, w < −1,
|w| ≤ 1,
|w|,
ew−1,

w > 1.

(6)

As shown in Fig. 1, AR can provide a higher
penalty degree than L2 to accelerate conver-
gence when the input parameter is large. Mean-
while, the penalty degree of AR is similar to
the L1-norm regularization as the parameter is
close to 0. In our evaluation, we will demon-
strate that beneﬁted from the ability to process
different magnitudes of parameters, AR ensures
faster convergence rate and high stability than
L1 and L2 regularization in neuron ﬁne-tuning
(see Fig. 2 in Section 5.6). Finally, with AR,
the purifying loss can be rewritten as follows:

Figure 1: The penalty degree of three types of reg-
ularization items over different magnitudes of in-
puts

Lpurif ying =

n
(cid:88)

i=1

CE(F (cid:48)(xi), yi)+α

(cid:88)

j∈A

AR(wj).

(7)

5 EXPERIMENTS

In this section, we mainly experiment with the effectiveness of WIPER in backdoor defense and
answer the following research questions (RQs).

• RQ1 Effectiveness: Can WIPER defend the state-of-the-art backdoor attacks?
• RQ2 A Closer Look at BS: Is BS a good metric in identifying bad neurons?
• RQ3 Improvement: Is AR more suitable for bad neuron purifying than the common reg-

ularization?

• RQ4: Approximation Availability: Is purifying output layers enough to defend backdoor

attacks?

5.1 EXPERIMENT SETTINGS

Baselines. Four state-of-the-art defense methods are considered as baselines, which are Fine-
Pruning Liu et al. (2018), Fine-tuning Truong et al. (2020), KD Yoshida & Fujino (2020), NAD Li
et al. (2021a), IBAU Zeng et al. (2022). We compare their defense effect with WIPER against ten
backdoor attacks, namely badNet Gu et al. (2017), blend attack (BA) Chen et al. (2017), enhanced
trigger attack (ETA) Li et al. (2020), invisible attack (IA) Zhong et al. (2020), sinusoidal signal at-
tack (SIG) Barni et al. (2019), TrojanNN Liu et al. (2017), IMC Pang et al. (2020), WaveNet Nguyen
& Tran (2021), SampleSpeciﬁedAttack (SSA) Li et al. (2021b), and LogitAttack Zhang et al. (2022)
on three benchmark datasets SVHN, CIFAR-10, and CIFAR-100. For fair comparisons, all the con-
ﬁgurations of these attacks and defense follow their corresponding original papers. Moreover, all
defense methods are implemented with access to parts of the clean data randomly selected from the
training set (5% to the size of training sets but never used in the training process like Yoshida &
Fujino (2020); Li et al. (2021a); Liu et al. (2018); Zeng et al. (2022)).

Evaluation Metrics. The performance of backdoor defense is evaluated with two widely-used
metrics: the accuracy of the model on clean samples (ACC), and the attack success rate (ASR). ASR
denotes the ratio of triggered samples that succeed in misleading the target model to output attacker-
chosen predictions. An effective defense method should signiﬁcantly lower ASR and remain high
ACC. All metrics are average values computed with the whole testing set that are never used in the
training or purifying stage.

6

Published as a conference paper at ICLR 2021

Figure 2: The left graph is about experimental result of BS, and the right one is about Fine-Pruning
with AM. For both of the two graphs, the horizontal axis indicates the neurons of the victim model.
Each bar corresponds to one neuron. The BS and AM for each neuron are average values computed
with 1000 testing samples (500 for clean and 500 for poison data). To intuitively compare the
performance of BS and AM, we also visualize the attention of benign and bad neurons on a triggered
image randomly selected from CIFAR10 (an airplane). Note that for both WIPER and previous
pruning-based method Liu et al. (2018), only clean data are used to ﬁnd the bad neurons in the bad
neurons. Here, we plot the BS and AM of poison data only for providing a benchmark and make it
easy to compare. For fairness, the same pruning ratio 0.1 is chosen for both methods, which means
that 10% neurons with lowest BS and AM are plotted in the attention maps about bad neurons. The
rest of neurons are considered good and plotted in another two attention maps. Moreover, the BS
and AM for clean data are sorted in descending order for the convenience of observation.

Defense

Before

Fine-tuning

KD

NAD

Fine-Pruning

IBAU

WIPER

Attack

ACC

ASR

ACC

ASR

ACC ASR ACC

ASR

ACC

ASR

ACC

ASR

ACC ASR

BadNet
BA
ETA
IA
SIG
TrojanNN
IMC
WaveNet
SSA
LogitAttack

94.68
94.62
94.59
94.68
94.38
94.71
94.15
94.13
94.60
94.93

100
100
99.98
100
96.97
100
100
99.36
97.41
100

85.74
86.54
87.61
85.17
84.85
88.75
87.52
84.68
85.22
86.16

3.17
1.44
6.34
3.26
1.31
8.42
10.56
2.81
2.22
3.38

60.16
52.4
53.08
52.45
53.22
49.64
52.62
55.86
52.83
51.68

4.42
2.61
4.98
4.51
6.87
4.81
5.15
6.26
7.59
4.22

86.22
86.47
87.56
86.25
85.3
87.45
86.41
84.84
85.30
86.30

2.71
3.11
6.28
1.87
3.22
31.97
32.53
4.57
4.07
3.55

93.17
92.29
93.41
94.23
93.16
92.31
93.50
92.54
94.95
93.60

69.57
79.65
68.52
87.53
38.53
97.82
98.23
40.86
40.76
68.75

89.70
87.95
91.22
89.71
88.59
88.22
87.52
88.06
87.52
90.01

6.83
9.91
10.98
9.45
2.28
11.34
18.16
3.25
5.27
6.62

94.14
94.09
94.11
94.53
93.69
94.52
93.90
93.90
93.33
93.51

0.49
1.43
4.94
2.48
1.31
2.41
1.92
2.70
3.17
2.14

Table 1: The performance of WIPER compared with four state-of-the-art defense methods over six
backdoor attacks in SVHN.

Other Setups. While implementing WIPER, we ﬁx α, β0, η to be 0.01, 0.5, and 0.9. The number
of epochs for neuron purifying is set to 10 similar to Li et al. (2021a). The puriﬁed layer is the
output layer (usually the last full connection layer in the network) Liu et al. (2018). Moreover, we
adopt standard SGD with a constant learning rate of 0.01 and batch size of 128. All experiments are
conducted using ResNet18 as the target model with a workstation equipped with Nvidia V100. Due
to the space limitation, parts of the results about the ablation study with SVHN and CIFAR-100 are
left in the supplementary material.

5.2 RQ1: EFFECTIVENESS

To validate the effectiveness of WIPER, we ﬁrst test and analyse the defense effect of WIPER
against different backdoor attacks. Then, we compare the performance of WIPER with other de-
fense methods. Table 1, Table 2 and Table 3 summarize the experimental result of ﬁve different
backdoor defense approaches against six state-of-the-art backdoor attacks with SVHN, CIFAR-10,
and CIFAR-100. In the tables, Before means the ACC and ASR of backdoored models before being
puriﬁed.

7

Published as a conference paper at ICLR 2021

Defense

Before

Fine-tuning

KD

NAD

Fine-Pruning

IBAU

WIPER

Attack

ACC

ASR

ACC

ASR

ACC ASR ACC

ASR

ACC

ASR

ACC

ASR

ACC ASR

BadNet
BA
ETA
IA
SIG
TrojanNN
IMC
WaveNet
SSA
LogitAttack

87.42
87.67
87.2
86.83
85.14
86.83
87.10
85.87
86.30
86.92

99.5
99.77
97.24
98.67
91.1
98.67
99.78
97.68
98.21
99.98

78.42
78.69
75.57
78.38
75.82
79.46
78.40
77.24
77.52
79.08

23.31
18.10
28.62
28.10
16.54
73.37
77.56
18.10
18.30
23.92

50.17
50.66
50.93
49.10
51.84
51.27
50.62
51.87
51.41
49.51

5.46
6.94
9.71
7.29
6.56
9.23
8.66
7.55
9.56
5.66

70.54
70.58
71.68
70.71
68.71
71.71
70.86
70.41
71.38
69.81

4.34
13.51
10.42
5.87
3.62
34.98
33.73
13.35
14.32
6.73

80.68
80.77
81.85
78.73
80.69
80.40
81.62
81.45
82.63
80.43

14.70
25.06
15.79
42.40
16.03
67.92
75.63
17.94
16.49
13.96

81.77
80.97
82.85
82.46
80.54
82.63
44.54
80.66
78.82
81.86

11.70
6.14
9.03
10.51
8.42
12.24
15.87
9.96
8.36
10.85

83.20
82.82
82.41
82.04
82.41
84.67
80.92
82.59
81.70
83.03

5.03
5.22
5.80
6.02
3.73
4.68
6.77
5.81
4.76
5.54

Table 2: The performance of WIPER compared with four state-of-the-art defense methods over six
backdoor attacks in CIFAR-10.

Defense

Before

Fine-tuning

KD

NAD

Fine-Pruning

IBAU

WIPER

Attack

ACC

ASR

ACC

ASR

ACC ASR ACC

ASR

ACC

ASR

ACC

ASR

ACC ASR

BadNet
BA
ETA
IA
SIG
TrojanNN
IMC
WaveNet
SSA
LogitAttack

69.66
68.68
68.98
69.12
68.92
70.84
69.08
67.68
68.03
68.35

99.83
99.81
98.43
99.88
90.21
94.66
99.55
98.84
97.76
99.87

60.74
61.68
62.71
59.44
57.48
61.97
62.41
59.49
58.80
61.49

11.01
23.32
17.99
27.79
21.18
56.76
62.15
22.07
22.00
11.26

18.84
15.32
17.52
16.57
15.48
16.61
17.52
16.00
15.50
19.24

1.51
1.59
1.44
1.91
1.58
1.57
1.98
1.29
1.38
2.55

32.80
33.24
31.03
31.37
30.50
30.38
31.19
31.08
30.45
32.90

2.32
2.46
3.11
3.02
1.62
32.03
35.40
2.46
0.21
3.15

63.48
63.08
62.89
60.97
61.43
61.45
61.62
62.04
62.23
62.63

25.53
26.68
37.63
23.30
28.07
78.99
87.98
27.87
28.98
26.09

59.29
61.62
60.82
60.27
59.11
60.58
60.47
59.95
61.00
58.80

3.44
9.08
1.67
5.94
8.12
11.56
15.16
8.37
6.46
4.82

63.71
63.21
62.35
62.11
63.89
63.92
60.91
62.04
60.82
62.00

1.12
0.80
0.83
0.55
0.26
0.20
0.97
0.99
1.38
0.53

Table 3: The performance of WIPER compared with four state-of-the-art defense methods over six
backdoor attacks in CIFAR-100.

Overall, it can be observed that the performance (simultaneously consider both ACC and ASR)
of WIPER considerably outperforms the other backdoor defense methods under different settings.
Speciﬁcally, considering the decrease of ASR, two state-of-the-art methods, KD and NAD, can
achieve competitive performance compared with WIPER but notably suffers from the problem of
model performance degradation. For example, to maintain the ASR to be less than 10%, KD sacri-
ﬁces at most 35% loss of ACC while WIPER only 5%. For NAD, it attains similar performance to
WIPER on most backdoor attacks but fails to defend the TrojanNN attack. This is because besides
adding poisoned data into the training set, TrojanNN allows the attacker to select and manipulate
speciﬁc neurons to enhance the memory of backdoored model about the trigger patterns. The model
distillation method used in NAD make the puriﬁed model inherit the “enhanced memory” from the
backdoored model thereby leading to the low defense effect of NAD on TrojanNN. In contrast, the
bad neuron ﬁltering mechanism of WIPER can accurately mark these backdoored neurons and lower
the ASR of TrojanNN signiﬁcantly. Furthermore, considering performance maintenance, WIPER
can ensure lower ACC degradation of the model compared with other defense methods in almost
all conditions. For instance, in SVHN (Table 1), with only less than 1% degradation of the model
performance, WIPER can drop the ASR of six attacks to less than 5%. However, the other method
still struggles against the attacks, especially for Fine-Pruning.

Attack

Method

BadNet

BA

ETA

IA

SIG

TrojanNN

ACC

ASR

ACC

ASR

ACC

ASR

ACC

ASR

ACC

ASR

ACC

ASR

Before

87.42
Fine-Pruning + AM 80.68
81.11
Fine-Pruning + BS
81.72
Ours + AM
83.20
Ours + BS

99.50
14.70
10.47
12.35
5.03

87.67
80.77
81.70
82.03
82.82

99.77
25.06
14.22
17.28
5.22

87.20
81.85
82.34
81.97
82.41

97.24
15.79
10.72
12.78
5.80

86.83
78.73
80.20
81.51
82.04

98.67
42.40
18.11
30.55
6.02

85.14
80.69
80.79
81.88
82.41

91.10
16.03
11.71
15.17
3.73

86.83
80.40
81.85
83.09
84.67

98.67
67.92
16.50
55.32
4.68

Table 4: Comparison of backdoor defense with different purifying strategies and neuron importance
evaluation metrics on CIFAR10.

8

Published as a conference paper at ICLR 2021

5.3 RQ2: A CLOSER LOOK AT BS

As mentioned before, one of the key factors to make WIPER outperform other methods is the in-
troduction of BS. Here, comprehensive experiments are conducted to explain why BS can achieve
such an improvement on backdoor defense from three perspectives: 1) statistical analysis; 2) at-
tention analysis; 3) effect on neuron purifying. In the experiments, a ResNet18 network is trained
with CIFAR10. During the training process, we backdoor the model according to the widely-used
benchmark attack BadNet Gu et al. (2017) and ensure 100% ASR. The poison label and poison ratio
are set to 0 and 5%. The trigger is a 3×3 square in the upper left corner as suggested by Gu et al.
(2017).

Statistical Analysis. We record the averaged BSs of all neurons by feeding a batch of clean and
poison data, 500 for each, as shown in the left of Figure 2. From the results, an interesting ﬁnding is
that the BSs of neurons are almost opposite for clean and poison data. In fact, such a phenomenon
is precisely what we desire in bad neuron ﬁltering. This is because the phenomenon proves that BS
well describes the principle of backdoor attacks: making some neurons to be strongly activated by
triggers to change the original label output and rarely activated by clean data to avoid lowering the
performance of victim model on normal inputs. Also, we can use BS to accurately mark the neurons
with low importance to normal predictions but high signiﬁcance to trigger activation. Referring to
AM, shown in the right of Figure 2, it can be observed that there is no signiﬁcant difference in AM
between neurons. Such a ﬂaw makes AM unable to provide a clear division between benign and bad
neurons, and leads the bad neurons marked by AM to be mixed in some benign neurons.

Attention Analysis. Further, to validate the above statement, we visualize the attention of neurons
on the original input image in Figure 2. The results show that for both BS and AM, the attention
of benign neurons is mostly focused on the main body of the input image, i.e.,around the airplane.
Then, the upper left corner, the place where triggers are added, draws the attention of bad neurons
marked by BS. Clearly, it is proved that with BS, WIPER accurately identiﬁes which neurons con-
tribute the most to backdoor attacks. Conversely, the attention of the bad neurons marked by AM is
not only focused on the upper left corner but also somewhere else.

Effect on Neuron Purifying. Finally, we conduct experiments to show that the choice of neuron
importance evaluation metric can lead to signiﬁcant effect on neuron purifying. From the results
of Table 4, given the same purifying method (Fine-Pruning or WIPER), the defense effect with BS
outperforms the ones with AM on both ACC and ASR. Especially, with AM, Fine-Pruning fails to
defend the TrojanNN attack. However, with BS, Fine-Pruning succeeds in defending TrojanNN due
to the better ability of BS to identify bad neurons. All the facts prove that BS is a good metric in
backdoor defense.

5.4 RQ3: IMPROVEMENT OF AR ON NEURON PURIFYING

To show the advantages of AR on ﬁne-tuning based neuron purifying, we experimentally compare
it with the other two commonly used regularization methods L1 and L2. Figure 2 illustrates the
performance of WIPER employed with three kinds of regularization mechanisms.

It is observed that the difference of these three kinds of regularization methods mainly stems from
the changing trend of ASR. In more details, the intrinsic property of L2 makes it only “strong”
on penalizing large parameters (more than 1, shown in Figure 1), and unable to actually decrease
parameters to 0.. However, for a backdoored network, the parameters of bad neurons are usually
very small (close to 0). Thus, L2 fails to provide proper penalty on bad neurons and leads to the
lowest ASR decreasing speed. Then, for L1, it can always reduce the ASR to a similar level to
AR . However, since the penalty of L1 is too weak, its convergence speed is still lower than AR.
Finally, the required epochs of AR is just around 2, which is signiﬁcantly lower than L1 and L2
regularization, which requires more than 15 epochs to erase the backdoor. Thus, the adoption of AR
in backdoor defense can greatly improve the performance of neuron purifying.

5.5 RQ4: IS PURIFYING OUTPUT LAYERS ENOUGH TO DEFEND BACKDOOR ATTACKS

In this subsection, we mainly answer the question of whether only purifying the last few layers
is enough for erasing the backdoor from the victim model. Table 6 reports the performance of
WIPER as the target purifying layers are changed. From the results, adding feature extraction layer

9

Published as a conference paper at ICLR 2021

Figure 2: The defense performance of ﬁve defense methods over different data ratio (0.01, 0.05, 0.1,
0.15, 0.2) in CIFAR-10. The images from left to right indicate against BadNet, BA, ETA, IA, SIG,
and TrojanNN.

Figure 2: The performance of WIPER with different regularization items (L1, L2, and AR) on
CIFAR-10. The images from left to right indicate against BadNet, BA, ETA, IA, SIG, and TrojanNN.

Attack

BadNet

BA

ETA

IA

SIG

TrojanNN

Alpha ACC

ASR

ACC

ASR

ACC

ASR

ACC

ASR

ACC

ASR

ACC

ASR

0.001
0.005
0.01
0.05
0.1

83.57
83.55
83.20
82.76
82.20

77.03
21.01
5.03
5.48
5.30

84.09
84.14
82.82
82.89
82.18

86.74
26.20
5.22
5.42
5.07

83.69
84.41
82.41
82.10
81.55

66.43
16.54
5.80
5.48
3.96

84.07
83.44
82.04
81.97
81.33

76.82
25.97
6.02
5.69
4.72

83.57
82.64
82.41
81.78
81.20

28.09
6.69
3.73
4.02
3.44

86.09
86.22
84.67
84.25
84.39

33.18
5.57
4.68
4.86
4.39

Table 5: The performance of WIPER with different α against six backdoor attacks in CIFAR-10.

(i.e., convolution layers) into backdoor purifying shows little improvement on defense effect, only
less 1% more decrease on ASR. In contrast, the performance of the puriﬁed model degrades more
signiﬁcantly when more convolution layers are introduced. As aforementioned, such a phenomenon
occurs because backdoor attacks usually pay less attention on the feature extraction layers than
the output layer to achieve better attack effectiveness. Therefore, forcibly purifying the feature
extraction layers contributes little to the backdoor defense but can easily lead to the over-pruning of
the victim model and lower the model performance.

5.6 OTHER CASES

To better understand WIPER, we perform extensive ablation studies with varying experimental set-
tings, namely data ratio and punishment coefﬁcient α .

Impact of Clean Data. As discussed in the prior works Liu et al. (2018); Truong et al. (2020);
Yoshida & Fujino (2020); Li et al. (2021a), the amount of clean data owned by the defender is a
crucial factor that affects the effect of backdoor defense signiﬁcantly. Here, for the convenience of
statistic and comparison, we use the ratio of clean data set to training set as the indicator to denote
the amount of clean data owned by the defender. Figure 2 demonstrates the performance of ﬁve
defense methods against six attacks over different data ratios. Strikingly, with the strict condition of
1% data ratio, WIPER can still maintain almost 90% ASR drops and less than 3.5% accuracy drops.
In contrast, most of other methods fail to achieve effective backdoor defense (less than 40% ASR
drops on some attacks) or suffer from signiﬁcant ACC drops (more than 20%).

10

Published as a conference paper at ICLR 2021

Attack

BadNet

BA

ETA

IA

SIG

TrojanNN

Puriﬁed Layer ACC ASR ACC ASR ACC ASR ACC ASR ACC ASR ACC ASR

FC only
FC + 1 Conv
FC + 2 Conv
FC + 3 Conv
FC + 4 Conv

83.20
82.19
80.29
77.03
74.49

5.03
5.15
4.79
4.83
4.51

82.82
81.29
79.31
77.84
74.86

5.22
5.93
5.24
4.34
4.35

82.41
81.37
79.42
77.65
71.56

5.80
5.77
5.38
4.60
4.04

82.04
80.46
77.66
76.71
73.33

6.02
5.44
5.69
5.62
4.96

82.41
81.26
79.82
77.96
74.56

3.73
3.98
4.05
3.45
3.22

84.67
82.89
81.00
79.72
72.73

4.68
4.24
4.01
5.36
4.67

Table 6: The performance of WIPER when purifying different layers. Here n Conv indicates puri-
fying the last n convolution layers.

Impact of α. As discussed in Section 4.2, α is a vital factor to determine the degree of WIPER to
punish the bad neurons. Table 5 summarizes the experimental results of WIPER with coarse-tuning
of α. The results show that larger α makes WIPER focus more on backdoor purifying and achieve
lower ASR but is with the expense of slightly decreased ACC. Correspondingly, lower α allows
the target model to maintain higher ACC but results in a higher ASR. Therefore, a trade-off exists
between ASR and ACC. However, the trade between ASR and ACC are not exchanged at equal
values. It can be found that in most cases, only less than 1% ACC drop can leads to the drop of ASR
for about 50% to 60%. As a result, in practice, the defender is suggested to choose a litter higher
magnitude of α to ensure the effectiveness of WIPER.

Impact of Purifying Strategy. Furthermore, in Table 4, we also show the inﬂuence of purifying
strategy choice on backdoor defense. As expected, despite of maintaining the performance of the
puriﬁed model in an acceptable range, Fine-Pruning (ﬁne-tuning with L1 + pruning) still suffers
from poor capacity of backdoor erasing. However, if Fine-Pruning is replaced with our purifying
strategy, the results can be improved signiﬁcantly. For instance, referring to the TrojanNN attack,
substituting Fine-Pruning with our purifying strategy can make the ACC rise from 81.85% to 84.67%
while the ASR drops from 16.50% to 4.68%, even with AM as the importance evaluation metric.

6 CONCLUSION

In this paper, we proposed a backdoor defense approach WIPER that could combine the advantages
of both model pruning and ﬁne-tuning. In more details, WIPER mainly improved current backdoor
defense method from two aspects. The ﬁrst was to propose a new metric called benign salience
to evaluate the importance of neurons in a network and ﬁlter the bad ones. The second was to
design a new regularization item for backdoor purifying, which could accelerate the neuron purifying
process signiﬁcantly. Finally, we conducted extensive experiments to show that the WIPER achieved
ideal performance on both erasing the backdoor and maintaining the model performance after being
equipped with such two techniques. The codes would be opensourced after this paper is publicly
available.

REFERENCES

Ahmed Ali Mohammed Al-Saffar, Hai Tao, and Mohammed Ahmed Talab. Review of deep convolu-
tion neural network in image classiﬁcation. In 2017 International Conference on Radar, Antenna,
Microwave, Electronics, and Telecommunications (ICRAMET), pp. 26–31. IEEE, 2017.

Mauro Barni, Kassem Kallas, and Benedetta Tondi. A new backdoor attack in cnns by training set
corruption without label poisoning. In 2019 IEEE International Conference on Image Processing
(ICIP), pp. 101–105. IEEE, 2019.

Xinyun Chen, Chang Liu, Bo Li, Kimberly Lu, and Dawn Song. Targeted backdoor attacks on deep
learning systems using data poisoning. CoRR, abs/1712.05526, 2017. URL http://arxiv.
org/abs/1712.05526.

Matthias Delange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Ales Leonardis, Greg
Slabaugh, and Tinne Tuytelaars. A continual learning survey: Defying forgetting in classiﬁcation
tasks. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021.

11

Published as a conference paper at ICLR 2021

Yansong Gao, Change Xu, Derui Wang, Shiping Chen, Damith C Ranasinghe, and Surya Nepal.
In Proceedings of the 35th

Strip: A defence against trojan attacks on deep neural networks.
Annual Computer Security Applications Conference, pp. 113–125, 2019.

Amirata Ghorbani and James Zou. Neuron shapley: Discovering the responsible neurons. arXiv

preprint arXiv:2002.09815, 2020.

Tianyu Gu, Brendan Dolan-Gavitt, and Siddharth Garg. Badnets: Identifying vulnerabilities in the

machine learning model supply chain. arXiv preprint arXiv:1708.06733, 2017.

Yige Li, Xixiang Lyu, Nodens Koren, Lingjuan Lyu, Bo Li, and Xingjun Ma. Neural attention
distillation: Erasing backdoor triggers from deep neural networks. In International Conference
on Learning Representations, 2021a.

Yiming Li, Tongqing Zhai, Baoyuan Wu, Yong Jiang, Zhifeng Li, and Shutao Xia. Rethinking the

trigger of backdoor attack. arXiv preprint arXiv:2004.04692, 2020.

Yuezun Li, Yiming Li, Baoyuan Wu, Longkang Li, Ran He, and Siwei Lyu.

Invisible backdoor
attack with sample-speciﬁc triggers. In Proceedings of the IEEE/CVF International Conference
on Computer Vision, pp. 16463–16472, 2021b.

Kang Liu, Brendan Dolan-Gavitt, and Siddharth Garg. Fine-pruning: Defending against back-
dooring attacks on deep neural networks. In International Symposium on Research in Attacks,
Intrusions, and Defenses, pp. 273–294. Springer, 2018.

Yingqi Liu, Shiqing Ma, Yousra Aafer, Wen-Chuan Lee, Juan Zhai, Weihang Wang, and Xiangyu

Zhang. Trojaning attack on neural networks. 2017.

Tuan Anh Nguyen and Anh Tuan Tran. Wanet-imperceptible warping-based backdoor attack. In

International Conference on Learning Representations, 2021.

Ren Pang, Hua Shen, Xinyang Zhang, Shouling Ji, Yevgeniy Vorobeychik, Xiapu Luo, Alex Liu,
and Ting Wang. A tale of evil twins: Adversarial inputs versus poisoned models. In Proceedings
of the 2020 ACM SIGSAC Conference on Computer and Communications Security, pp. 85–99,
2020.

Ali Shafahi, W Ronny Huang, Mahyar Najibi, Octavian Suciu, Christoph Studer, Tudor Dumitras,
and Tom Goldstein. Poison frogs! targeted clean-label poisoning attacks on neural networks. In
Proceedings of the 32nd International Conference on Neural Information Processing Systems, pp.
6106–6116, 2018.

Ruoyu Sun.

Optimization for deep learning:

theory and algorithms.

arXiv preprint

arXiv:1912.08957, 2019.

Loc Truong, Chace Jones, Brian Hutchinson, Andrew August, Brenda Praggastis, Robert Jasper,
Nicole Nichols, and Aaron Tuor. Systematic evaluation of backdoor data poisoning attacks on
image classiﬁers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition Workshops, pp. 788–789, 2020.

Bolun Wang, Yuanshun Yao, Shawn Shan, Huiying Li, Bimal Viswanath, Haitao Zheng, and Ben Y
Zhao. Neural cleanse: Identifying and mitigating backdoor attacks in neural networks. In 2019
IEEE Symposium on Security and Privacy (SP), pp. 707–723. IEEE, 2019.

Ren Wang, Gaoyuan Zhang, Sijia Liu, Pin-Yu Chen, Jinjun Xiong, and Meng Wang. Practical
detection of trojan neural networks: Data-limited and data-free cases. In Computer Vision–ECCV
2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XXIII
16, pp. 222–238. Springer, 2020.

Xiaojun Xu, Qi Wang, Huichen Li, Nikita Borisov, Carl A Gunter, and Bo Li. Detecting ai trojans
using meta neural analysis. In 2021 IEEE Symposium on Security and Privacy (SP), pp. 103–120.
IEEE, 2021.

12

Published as a conference paper at ICLR 2021

Kota Yoshida and Takeshi Fujino. Disabling backdoor and identifying poison data by using knowl-
edge distillation in backdoor attacks on deep neural networks. In Proceedings of the 13th ACM
Workshop on Artiﬁcial Intelligence and Security, pp. 117–127, 2020.

Yi Zeng, Si Chen, Won Park, Z Morley Mao, Ming Jin, and Ruoxi Jia. Adversarial unlearning
of backdoors via implicit hypergradient. International Conference on Learning Representations,
2022.

Zhiyuan Zhang, Lingjuan Lyu, Weiqiang Wang, Lichao Sun, and Xu Sun. How to inject backdoors
with better consistency: Logit anchoring on clean data. International Conference on Learning
Representations, 2022.

Pu Zhao, Pin-Yu Chen, Payel Das, Karthikeyan Natesan Ramamurthy, and Xue Lin. Bridging
In International Conference

mode connectivity in loss landscapes and adversarial robustness.
on Learning Representations, 2019.

Haoti Zhong, Cong Liao, Anna Cinzia Squicciarini, Sencun Zhu, and David Miller. Backdoor
embedding in convolutional neural network models via invisible perturbation. In Proceedings of
the Tenth ACM Conference on Data and Application Security and Privacy, pp. 97–108, 2020.

13

