0
2
0
2

g
u
A
0
2

]
T
S
.
h
t
a
m

[

1
v
8
7
7
8
0
.
8
0
0
2
:
v
i
X
r
a

Strong consistent model selection for general causal

time series

August 21, 2020

William Kengne 1.

CY Cergy Paris Universit´e, CNRS, THEMA, F-95000 Cergy, France.

E-mail: william.kengne@u-cergy.fr

Abstract : We consider the strongly consistent question for model selection in a large class of causal
time series models, including AR(
), ARMA-GARCH and many classical others
), TARCH(
processes. We propose a penalized criterion based on the quasi likelihood of the model. We provide suﬃcient

), ARCH(

∞

∞

∞

conditions that ensure the strong consistency of the proposed procedure. Also, the estimator of the parameter
It appears that, unlike the result of the weak
of the selected model obeys the law of iterated logarithm.

consistency obtained by Bardet et al. [2], a dependence between the regularization parameter and the model
structure is not needed.

Keywords: Model selection, strong consistency, causal processes, quasi-maximum likelihood estimation,

penalized contrast.

1

Introduction

We consider a general class of autoregressive time series in a semiparametric framework. Let M, f : R
R
be two measurable functions and (ξt)t∈Z a sequence of centered independent and identically distributed (iid)
random variables satisfying var(ξ0) = 1. Consider the class of aﬃne causal models,

→

N

Class

AC

(M, f ) : A process X = (Xt)t∈Z belongs to

(M, f ) if it satisﬁes:

AC

Xt = M

(Xt−i)i∈N∗
(cid:0)

(cid:1)

ξt + f

(Xt−i)i∈N∗
(cid:0)

(cid:1)

for any t

Z.

∈

(1)

The existence of a stationary and ergodic solution of the class (1) has been studied by Bardet and Winten-

berger (2009) as a particular case of models considered in Doukhan and Wintenberger (2008). Bardet and
Wintenberger (2009) and Bardet et al. (2017) carried out the inference question in the semiparametric setting

(M, f ), whereas Bardet et al. (2012), Kengne (2012), Bardet and Kengne (2014) focussed on
in the class
the change-point problem in this class. Numerous classical time series models belongs to the class (1): for

AC

instance AR(

), ARCH(

), TARCH(

), ARMA-GARCH or APARCH processes.

∞

∞

∞

1Developed within the ANR BREAKRISK (ANR-17-CE26-0001-01)

1

 
 
 
 
 
 
2

Strong consistent model selection

Consider a trajectory (X1, . . . , Xn) of a process X = (Xt)t∈Z that belongs to

f ∗ are unknown. We consider a ﬁnite collection
corresponds to M ∗ and f ∗. Our main aim is to select a model
to m∗ for large n.

M

of aﬃne causal models, where the true model m∗

(M ∗, f ∗) where M ∗ and

AC

∈ M
) which is ”close”

m (among the collection

M

“

We focus on the semiparametric framework and assume that the distribution of ξ0 is unknown and that
N). That
(Mθ∗, fθ∗). In

the functions f and M are known up to a parameter θ
∈
is, the model m∗ corresponds to the true parameter θ∗
the sequel :

Θ, where Θ is a compact subset of Rd (d

Θ and the process X belongs to

AC

∈

∈

each model m

is considered as a subset of

typically,

m

|

|

∈ M
= #(m);

1, . . . , d
}

{

and denote by

m

|

|

the dimension of the model,

for m
vector associated to m;

∈ M

, the parameter space of m is Θ(m) =

(θi)1≤i≤d

(cid:8)

Θ, θi = 0 if i /
∈

∈

m

; θ(m) is the parameter

(cid:9)

the collection

M

is considered as a subset of the power set of

1, . . . , d
}

{

, i.e.

M ⊂ P(cid:0){

1, . . . , d

.
}(cid:1)

•

•

•

Therefore, for any model m
, m
well as exhaustive families of models.

∈ M

∈ AC

(Mθ, fθ) when θ

∈

Θ(m). Also, we could consider hierarchical as

· · ·

, p′

0, 1,

For instance, assume that (X1, . . . , Xn) is generated from a GARCH(p∗, q∗) process. The collection
could be a family of ARMA(p, q)-GARCH(p′, q′) with (p, q, p′, q′)
, qmax
, p′
0, 1,
max} × {
{
assumed to satisfy p∗
′
max+q
(0,
[0,
∞
parameter space is Θ(m) =

M
} ×
max are the ﬁxed upper bounds of the orders,

≤
≤
′
max . Thus, a model m is a subset of
m

Θ, θi = 0 if i /
∈
The model selection problem for time series has already been considered by several authors; we refer to
the book of McQuarrie and Tsai (1998), the monograph of Rao and Wu (2001), the recent review paper of

qmax. Therefore, consider Θ as a compact subset of Rpmax+qmax

max}
· · ·
pmax and q∗

where pmax, qmax, p′

, pmax + qmax + p′

×
and its

max + q′

(θi)1≤i≤d

max + 1

max, p′

, pmax

} × {

)
∞

{
.
(cid:9)

0, 1,

0, 1,

∈ {

· · ·

· · ·

· · ·

)p

1,

×

∈

(cid:8)

}

Ding et al. (2018), the recent works of Hsu et al. (2019) and the references therein for an overview on this

topic. Hannan (1980) and Hannan and Deistler (2012) provided general conditions for strong consistency
of the order estimator of an ARMA and ARMAX model. Resende and Dorea (2016) proposed the eﬃcient

determination criterion (introduced by Zhao et al. (2001) for the strongly consistent estimation of the order of
multiple Markov chains) for model selection in a general class of multivariate time series. They established the

strong consistency of the procedure under some conditions which may seem a bit strong for some applications,
for instance, the existence of the third order derivative of the contrast function (likelihood), the existence

of moments of order 16 for the BEKK-GARCH model. Recently, Bardet et al. (2020) addressed the model
(Mθ, fθ). They proposed a procedure based on the quasi likelihood
selection question in the class of model

of the model and provided suﬃcient conditions that ensure the weak consistency of the selected model.

AC

In this new contribution, we focus on the model selection in the class of model

contrast which is based on the Gaussian quasi likelihood of the model.

(Mθ, fθ) with a penalized

AC

(i) Under the assumptions that E
|

fθ, Mθ are twice times continu-
ously diﬀerentiable on Θ and satisfy some Lipschitz-type properties, we establish the strong consistency
of the proposed procedure.

with r > 4, the functions θ

ξ0|

7→

∞

r <

W. Kengne

3

(ii) We show that the quasi maximum likelihood estimator (QMLE) of the selected model obeys the law of

iterated logarithm.

The rest of the paper is structured as follows. In Section 2, we set some notations, assumptions and deﬁne

the model selection criterion. The main results are provided in Section 3 whereas Section 4 is devoted to a
concluding remarks. Section 5 focuses on the proofs of the main results.

2 Assumptions and the model selection criterion

2.1 Assumptions on the class of models

(Mθ, fθ)

AC

In the sequel, we will use the norms :

1.

k · k

applied to a vector denotes the Euclidean norm of the vector;

2. for any compact set

Rd and for any g : Θ

Rp,

−→

g

K = supθ∈Θ(
k

g(θ)
k

k

k

).

K ⊆

Throughout the sequel, we assume that the functions θ
diﬀerentiable on Θ. Also, we will use Hθ = M 2
we set ∂θigθ = ∂igθ/∂θi. Let us consider the following assumptions for any compact set
Ψθ = ∂θifθ or ∂θiMθ :

fθ are twice times continuously
θ and for any function gθ which is i times diﬀerentiable on Θ,
Θ, i = 0, 1, 2 and

Mθ and θ

K ⊆

7→

7→

Assumption A(Ψθ,
there exists a sequence of non-negative real numbers
that:

): for any x

K

R

∈

N, the function θ

∞

Ψθ(x) is continuous on Θ with
(cid:1)k≥1 satisfying
)

7→
αk(Ψθ,
(cid:0)

k
∞
k=1 αk(Ψθ,

K

Ψθ(0)

kΘ <
) <
∞
K

∞

P

and

such

Ψθ(x)

k

Ψθ(y)
k

K

−

≤

X
k=1

αk(Ψθ,

xk

)
|

K

yk

|

−

f or all x, y

N.

R

∈

In the sequel we refer to the particular case of the non linear ARCH(
), see Bardet and
Wintenberger(2009)) processes deﬁne when fθ = 0. In this case, we consider the following assumption for
i = 0, 1, 2 :

) (NLARCH(

∞

∞

Assumption A(∂θiHθ,

R∞, the function θ
): for any x
and there exists a sequence of non-negative real numbers

K

∈

∂θiHθ(x) is continuous on Θ with

7→
αk(∂θiHθ,
(cid:0)

(cid:1)k≥1 satisfying
)

K

P

∂θiHθ(0)
∞
k=1 αk(∂θi Hθ,

k

kΘ <
) <
K

∞

∞

such that :

∂θiHθ(x)

k

∂θiHθ(y)
k

K

−

∞

≤

X
k=1

αk(∂θiHθ,

)
|

x2
k −

y2
k|

K

f or all x, y

N.

R

∈

Then deﬁne the set:

Θ(r) = nθ

∈

Rd, A(fθ,

θ

{

}

) and A(Mθ,

) hold with

θ

{

}

∞

X
k=1

αk(fθ,

) +

θ

{

}

ξ0k

k

r

∞

X
k=1

αk(Mθ,

θ

{

}

) < 1o

∪ nθ

∈

Rd, fθ = 0 and A(Hθ,

) holds with

θ

{

}

2
r

ξ0k

k

∞

X
k=1

αk(Hθ,

θ

{

}

) < 1o (2)

The above Lipschitz-type conditions are classical when studying the existence of a stationary and ergodic
Θ(r),
solution of such model, see Doukhan and Wintenberger (2008). In the case of the class

(Mθ, fθ), if θ

AC

∈

4

Strong consistent model selection

then there exists a unique causal, stationary and ergodic solution X = (Xt)t∈Z
moment of order r, see Bardet and Wintenberger (2009).

∈ AC

(Mθ, fθ) with a ﬁnite

The following assumptions are useful in the study of the asymptotic behavior of the QLME .

Assumption D(Θ):

h > 0 such that inf
θ∈Θ

∃

(Hθ(x))

h for all x

≥

N.

R

∈

Assumption Id(Θ): For all (θ, θ′)

Θ2,

∈

(cid:16)fθ(X0, X−1,

· · ·

) = fθ′(X0, X−1,

· · ·

) and Mθ(X0, X−1,

· · ·

) = Mθ′(X0, X−1,

) a.s.(cid:17) ⇒

· · ·

θ = θ′.

Assumption Var(Θ): For all θ
is a.s. linearly independent.

∈

Θ, one of the families

∂fθ
∂θi (X0, X−1,

(cid:0)

)
(cid:1)1≤i≤d

· · ·

or

∂hθ
∂θi (X0, X−1,

(cid:0)

)
(cid:1)1≤i≤d

· · ·

In the following assumption, we make the convention that if A(∂θi Mθ, Θ) holds then αk(∂θiHθ, Θ) = 0 and if
A(∂θiHθ, Θ) holds, then αk(∂θiMθ, Θ) = 0. Set for ℓ = 0, 1, 2,

Assumption Kℓ(Θ): there exists r > 4 such that θ∗
(or A(∂θiHθ, Θ)) hold with

Θ(r)

∩

∈

Θ and for i = 0,

· · ·

, ℓ, A(∂θifθ, Θ), A(∂θi Mθ, Θ)

1

ℓ

X
k≥1

√k log log k X
j≥k

X
i=0

αj(∂θifθ, Θ) + αj(∂θi Mθ, Θ) + αj(∂θiHθ, Θ) <

.
∞

These aforementioned assumptions hold for many classical models, including AR(

)
∞
type processes , see for instance Bardet and Wintenberger (2009), Bardet et al. (2012), Kengne (2012). In the
case of assumption K(Θ), let us consider for ℓ = 0, 1, 2 :

), TARCH(

), ARCH(

∞

∞

1. The geometric case:

ℓ
i=0 αj(∂θifθ, Θ) + αj(∂θi Mθ, Θ) + αj(∂θiHθ, Θ) =

this case, assumption Kℓ(Θ) holds.

P

(aj) for some a

O

[0, 1). In

∈

2. The Riemanian case:
then Kℓ(Θ) holds.

P

ℓ
i=0 αj(∂θi fθ, Θ) + αj(∂θi Mθ, Θ) + αj(∂θi Hθ, Θ) =

(jγ) with γ > 0. If γ > 3/2,

O

2.2 The model selection criterion

Rd. Assume that a trajectory
Consider a model m
(X1, . . . , Xn) is observed. The conditional Gaussian quasi (log)likelihood (up to a constant) Ln is deﬁned for
all θ

(Mθ, fθ) for θ

and the class

Θ(m) by,

Θ(m)

∈ M

AC

⊂

⊂

Θ

∈

∈

Ln(θ) :=

1
2

−

n

X
t=1

qt(θ) , with qt(θ) :=

(Xt

f t
θ)2

−
H t
θ

+ log(H t
θ)

(3)

θ := fθ(Xt−1, Xt−2,

where f t
. Since Ln(θ) depends on
(Xt)t≤0 which are not observed, it is common practice (see [5], [4], [11]) to consider the approximated quasi
Θ(m) by
(log)likelihood given (up to a constant) for all θ

θ := Mθ(Xt−1, Xt−2,

) and H t

), M t

θ =

· · ·

· · ·

(cid:0)

2
M t
θ(cid:1)

∈

Ln(θ) :=
b

1
2

−

n

qt(θ) , with

X
t=1 b

qt(θ) :=
b

(Xt

f t
θ)2
b

−
H t
θ
“

+ log(

H t
θ)
“

(4)

W. Kengne

5

where

f t
θ = fθ(Xt−1, Xt−2,
b

· · ·
”best” parameter associated to the model m if deﬁned by,

· · ·

M t
c

, X1, 0,

θ = Mθ(Xt−1, Xt−2,

),

, X1, 0,

),

· · ·

· · ·

H t
“

M t
c

θ = (

θ)2. Note that, the

θ∗(m) = argmin E[q0(θ)]

.

θ∈Θ(m)

According to [2], θ∗(m) exists and it is unique under Id(Θ(m)). When m = m∗, we have θ∗(m∗) = θ∗. For
any m

, the QMLE of θ∗(m) is given by

∈ M

θ(m) = argmax
b

θ∈Θ(m) b

Ln(θ).

(5)

The selection of the ”best” model

m among the collection

is performed by minimizing the penalized contrast

M

that is

where

“

C(m) =

“

−

2

Ln
b

θ(m)
(cid:1)
(cid:0)
b

+

m

|

|

κn,

m = argmin
m∈M

“

C(m),

“

(6)

(7)

•

(κn)n is the sequence of the regularization parameter (possibly data-dependent) that will be used to
calibrate the penalty term;

• |

is the dimension of the model m, typically, the cardinal of m (considered as a subset of

m
which is also the number of the estimated components of θ (the others are ﬁxed to zero).

|

1, . . . , d
}

{

),

3 Asymptotic results

Recall that, when the model is correctly speciﬁed, Bardet and Wintenberger (2009) have established the
θ(m∗)
consistency and the asymptotic normality of
b

θ(m∗). The following proposition shows that the estimator
b

obeys the law of iterated logarithm.

Proposition 3.1 Let (X1, . . . , Xn) be a trajectory of a process X belonging to

Rd with r > 4. Assume that D(Θ), Id(Θ), V ar(Θ), K2(Θ) hold. Then,

Θ(r)

∩

o
Θ

⊂

θ(m∗)
b

−

θ∗ =

log log n
n

O(cid:16)…

(cid:17) a.s. .

AC

(Mθ∗, fθ∗) where θ∗

∈

(8)

The following theorem provides suﬃcient conditions that ensure the strong consistency of the model selection
procedure.

Theorem 3.1 Let (X1, . . . , Xn) be a trajectory of a process X belonging to
sumptions of Proposition 3.1, and if κn/ log log n

and κn/n

AC
0, then

−→n→∞ ∞

−→n→∞

(Mθ∗, fθ∗)). Under the as-

a.s.
−→n→∞

m∗.

m

“

(9)

6

Strong consistent model selection

Remark 3.1

1. This result, besides it is stronger than those obtained by Bardet et al. (2020), do not
impose any condition on the dependence between the regularization parameter κn and the Lipschitz-type
coeﬃcients αj (∂θifθ, Θ), αj(∂θiMθ, Θ), αj (∂θiHθ, Θ) as have been set by these authors. For instance, in
ℓ
(jγ) for some 3/2 < γ < 2,
the Riemanian case with
i=0 αj(∂θifθ, Θ)+αj(∂θi Mθ, Θ)+αj(∂θiHθ, Θ) =
P
the BIC (κn = log n) is strongly consistent from this Theorem while the result of Bardet et al. (2020)
can not assure the weak consistency of the BIC.

O

2. Hannan and Deistler (2012) have considered the estimation of the order of an ARMAX (including
C is based on the Gaussian likelihood of the model. Under the condition

0, they have established that there exists a constant c1 > 0 such that, if lim inf κn/(2 log log n) >

“

ARMA), where the contrast
κn/n

−→n→∞

c1, then the estimator of the order is strongly consistent. From the proof of Theorem 3.1, one can see
that such result holds for the general class of model considered here; that is, we can ﬁnd a constant c2 > 0
such that if lim inf κn/ log log n > c2, then

m∗.

m

a.s.
−→n→∞

The next corollary show that the estimator of the parameter of the selected model
iterated logarithm.

“

m) obeys the law of

θ(
b

“

Corollary 3.1 Let (X1, . . . , Xn) be a trajectory of a process X belonging to
tions of Theorem 3.1,

AC

(Mθ∗, fθ∗). Under the assump-

θ∗ =

−

O(cid:16)…

log log n
n

(cid:17) a.s. .

(10)

θ(
b

m)

“

4 Concluding remarks

This paper focuses on the model selection in a large class of causal time series models in a semiparametric
framework. The strong consistency of an estimator based on a penalized quasi likelihood contrast is established,
under some classical conditions on the regularization parameters κn.

For the estimation of the order of an ARMAX model, Hannan and Deistler (2012) have established that,
there exists a constant c0 such that, if lim sup κn/(2 log log n) < c0 then the strong consistency of the estimator
of the order fails. A topic of a future works could be to investigate if such result is applied to the general class
of model considered here or to derive an upper bound of κn for which the strong consistency fails.
Another extension of this works is to carry out the model selection problem in the class

(Mθ, fθ) with a
procedure based on a non Gaussian (for instance Laplacian, see Bardet et al. (2017)) quasi likelihood, for the
purpose of reducing the order moment imposed on the process.

AC

5 Proofs of the main results

The following lemma will be useful in the sequel. The proof is carried out by going along similar lines as in
Lemma 2 of [2] by using Corollary 1 of [12]; so, it is then omitted.

Lemma 5.1 Let X
hold. Then,

∈ AC

(Mθ, fθ) and Θ

⊆

Θ(r) with r > 4. Assume that the assumptions D(Θ) and K1(Θ)

1
√n log log n (cid:13)
(cid:13)
(cid:13)

∂

Ln(θ)
b
∂θ −

∂Ln(θ)
∂θ

a.s.
−→n→∞

0.

Θ

(cid:13)
(cid:13)
(cid:13)

(11)

W. Kengne

Proof of Proposition 3.1

According to [5], it holds that

7

θ∗. Also, since θ∗

Θ(m∗)

∩

∈

o
Θ, we

θ(m∗)
b

a.s.
−→n→∞

Ln
∂
b
∂θi

implies

get

∂

Ln(
b

θ(m∗))
b
∂θ

= 0 for n large enough. Thus, for any i = 1,

m∗

,

|

|

· · ·

, the Taylor expansion of

0 =

∂

Ln(
b

θ(m∗))
b
∂θi

=

∂

Ln(θ∗)
b
∂θi

+

∂2

Ln
b

˙θi(m∗)
(cid:0)
(cid:1)
∂θ∂θi

θ(m∗)
(cid:0)
b

θ∗

,
(cid:1)

−

where ˙θi(m∗) lies between

θ(m∗) and θ∗. Therefore,
b

n
log log n (cid:0)

θ(m∗)
b

…

θ∗

(cid:1)

−

=

2
√n log log n

∂

Ln(θ∗)
b
∂θ

F −1
n (m∗)
“

where

Fn(m∗) =
“

∂2

Ln
b

˙θi(m∗)
(cid:1)
(cid:0)
∂θ∂θi

2(cid:16)

−

. (12)

(cid:17)i∈m∗

Note that, by dealing with the ﬁrst (stationary) regime in the Corollary 6.1 of [4] and since ˙θi(m∗)

for i = 1,

m∗

,

|

|

· · ·

, we get

a.s.
−→n→∞

F (θ∗, m∗) where F (θ∗, m∗) = (cid:16)Eh

∂2q0(θ∗)
∂θi∂θj i(cid:17)i,j∈m∗

.

Fn(m∗)
“

Since F (θ∗, m∗) is invertible (see [5]), then for n large enough and with a suﬃciently large probability, the
matrix

Fn(m∗) is invertible. We have from Lemme 5.1, (12) and (13),
“

+ o(1) a.s. .

(14)

n
log log n (cid:0)

θ(m∗)
b

θ∗

=

−

(cid:1)

2
√n log log n

∂Ln(θ∗)
∂θ

n (m∗)
F −1
“

…

We have,

a.s.
−→n→∞

θ∗

(13)

∂Ln(θ∗)
∂θ

=

n

X
t=1

∂qt(θ∗)
∂θ

.

∗

Denote for all t
)
,

) the σ-ﬁeld generated by the whole past at time t. Then,
t(cid:17) is a stationary ergodic square integrable martingale diﬀerence process (see [5]). Therefore, from

t = σ(Xt, Xt−1,

(cid:16)
the law of iterative logarithm for martingales (see [16, 17]), we get,

∂qt(θ
∂θ

· · ·

Z,

F

F

∈

Thus, the proposition follows from (13), (14) and (15).

1
√n log log n

∂Ln(θ∗)
∂θ

=

O

(1) a.s. .

Proof of Theorem 3.1

1. Let m

such as m ) m∗. We have,

∈ M
1
log log n (cid:0)

Let us establish that

C(m∗)
“

=

−

C(m)
(cid:1)

“

2
log log n (cid:0)

Ln
b

θ(m)
(cid:0)
b

(cid:1) −

Ln
b

θ(m∗)
(cid:0)
b

(cid:1) −

κn
log log n

m

(
|

| − |

m∗

).

|

Since θ∗

Θ(m)

∩

∈

o
Θ and

θ(m)
b

expansion of

Ln, we can ﬁnd θ(m) between
b

1
log log n (cid:0)

Ln
b

θ(m)
(cid:0)
b

θ∗, then

∂

a.s.
−→n→∞

Ln(
b

(cid:1) −

Ln
b
θ(m))
b
∂θ

θ(m∗)
(cid:1)
(cid:0)
b

(1) a.s. .

=

O

θ(m) and θ∗ such that
b
θ∗) =
(cid:0)

′ ∂2
(cid:1)

Ln
b

1
2 (cid:0)

θ∗

−

θ(m)
b

θ(m)
(cid:0)
(cid:1)
∂θ2

Ln
b

θ(m)
(cid:0)
b

(cid:1) −

Ln
b

θ(m)
(cid:0)
b

θ∗

.
(cid:1)

−

(18)

= 0 for n large enough. Therefore, from the Taylor

(15)

(cid:4)

(16)

(17)

8

Strong consistent model selection

Also, for any i = 1,

m

,

|

|

· · ·

, we can ﬁnd ˙θi(m) between

0 =

∂

Ln(
b

θ(m))
b
∂θi

=

∂

Ln(θ∗)
b
∂θi

Hence,

+

θ(m) and θ∗ such that, for n large enough,
b
∂2

˙θi(m)
Ln
(cid:1)
(cid:0)
b
∂θ∂θi

θ(m)
(cid:0)
b

θ∗

.
(cid:1)

−

θ∗ =

θ(m)
b
θ(m), θ(m),
b

a.s.
−→n→∞
proof of Proposition 3.1 lead to

−
˙θi(m)

Since

∂

Ln(θ∗)
2
F −1
n (m)
b
n
∂θ
“
θ∗ for i = 1,

,

m

|

|

· · ·

where

Fn(m) =
“

∂2

˙θi(m)
Ln
(cid:1)
(cid:0)
b
∂θ∂θi

2(cid:16)

−

.

(cid:17)i∈m

(19)

, in this case of overﬁtting, the same arguments as in the

Fn(m)
“

a.s.
−→n→∞

2
F (θ∗, m) and −
n

∂2

Ln
b

θ(m)
(cid:0)
(cid:1)
∂θ2

a.s.
−→n→∞

F (θ∗, m) where F (θ∗, m) = (cid:16)Eh

∂2q0(θ∗)
∂θi∂θj i(cid:17)i,j∈m

.

(20)

For the overﬁtted model m, on can deduce from [5] that F (θ∗, m) is invertible, thus for n large enough and
Fn is invertible. From (18), (19), (20) and Lemma 5.1, it holds
with a suﬃciently large probability, the matrix
that
“

1
log log n (cid:0)

Ln
b

2
n2 log log n

θ(m)
(cid:0)
b
∂

1
√n log log n

−(cid:16)

=

=

θ(m∗)
Ln
(cid:1) −
(cid:1)
(cid:0)
b
b
∂2
Ln(θ∗)
F −1
n (m)
∂θ′
b
“
Ln(θ∗)
b
∂θ′
∂Ln(θ∗)

(cid:17)

∂

1
√n log log n

= (cid:16)

∂θ′ + o(1)(cid:17)O
Thus, according to (21) and (15), (17) follows.
m
Therefore, since κn/ log log n

and

Ln
b

θ(m)
(cid:0)
(cid:1)
∂θ2

∂

Ln(θ∗)
b
∂θ

∂2

F −1
n (m)
“
Ln
b

θ(m)
(cid:0)
(cid:1)
∂θ2
1
√n log log n

(cid:17)

F −1
n (m)(cid:16)
“
∂Ln(θ∗)
∂θ

2
F −1
n (m)(cid:16) −
n
“

(1)(cid:16)

1
√n log log n

∂

Ln(θ∗)
b
∂θ

(cid:17)

+ o(1)(cid:17) a.s. .

−→n→∞ ∞

>

m∗

|

|

, then (16) and (17) lead to

|

|

lim
n→∞

1
log log n (cid:0)

C(m∗)
“

=

−

C(m)
(cid:1)

“

−∞

a.s. .

−

C(m)

C(m∗) > 0 a.s.
“
such as m + m∗. We have,
2
n (cid:0)

1
n (cid:0)

“

for large n.

=

C(m)
(cid:1)

θ(m∗)
b
“
E[q0(θ)]. According to the proof of Theorem 3.1 of [2], we get

θ(m)
b

Ln
b

Ln
b

| − |

m∗

(cid:1) −

(cid:1) −

(
|

m

).

(cid:0)

(cid:0)

|

κn
n

This implies,

2. Let m

∈ M

C(m∗)
“

−
1
2

1
n (cid:0)

For all θ

∈

Θ, denote L(θ) =

−

Ln
(cid:1) −
b
Note that, from [5], the function L : Θ
θ∗ /
∈

Θ(m). Hence, L(θ∗(m))

θ(m)
(cid:0)
b

Ln
b

−

→

= L(θ∗(m))

θ(m∗)
(cid:1)
(cid:0)
b
R has a unique maximum at θ∗. Since m + m∗, it holds that

L(θ∗) + o(1) a.s. .

−

L(θ∗) < 0 a.s. . Thus, according to (24) and since κn/n

0, we get

−→n→∞

lim
n→∞

1
n (cid:0)

C(m∗)
“

“
Thus, the strong consistency of

C(m)
(cid:1)

−

< 0 a.s. and

C(m)

“

−

m = argmin
m∈M

“

“

C(m) = argmin
m∈M (cid:0)

C(m∗) > 0 a.s.
“
C(m)

−

C(m∗)
(cid:1)
“

“

for large n.

(25)

follows from (23) and (25).

(cid:4)

(21)

(22)

(23)

(24)

W. Kengne

Proof of Corollary 3.1

9

According to the proof of Theorem 3.1 (equations (23) and (25)) it holds that

m = m∗ a.s. for large n.

Thus, the corollary follows from Proposition 3.1.

“

(cid:4)

References

[1] Bardet, J.-M., Boularouk, Y., and Djaballah, K. Asymptotic behavior of the laplacian quasi-
maximum likelihood estimator of aﬃne causal processes. Electronic journal of statistics 11, 1 (2017),

452–479.

[2] Bardet, J.-M., Kamila, K. and Kengne, W. Consistent model selection criteria and goodness-of-ﬁt

test for common time series models. Electronic Journal of Statistics 14, (2020), 2009–2052.

[3] Bardet, J.M. and Kengne, W. Monitoring procedure for parameter change in causal time series.

Journal of Multivariate Analysis 125, (2014), 204-221.

[4] Bardet, J.-M., Kengne, W., and Wintenberger, O. Detecting multiple change-points in general
causal time series using penalized quasi-likelihood. Electronic journal of statistics 6 (2012), 435–477.

[5] Bardet, J.-M., and Wintenberger, O. Asymptotic normality of the quasi-maximum likelihood
estimator for multidimensional causal processes. The Annals of Statistics 37, 5B (2009), 2730–2759.

[6] Ding, J., Tarokh, V., and Yang, Y. Model selection techniques: An overview. IEEE Signal Processing

Magazine 35, 6 (2018), 16–34.

[7] Doukhan, P., and Wintenberger, O. Weakly dependent chains with inﬁnite memory. Stochastic

Processes and their Applications 118, 11 (2008), 1997–2013.

[8] Hannan, E. J. The estimation of the order of an ARMA process. The Annals of Statistics 8, 5 (1980),

1071–1081.

[9] Hannan, E. J. and Deistler, M. The statistical theory of linear systems. SIAM (2012).

[10] Hsu, H.-L., Ing, C.-K., and Tong, H. On model selection from a ﬁnite family of possibly misspeciﬁed

time series models. The Annals of Statistics 47, 2 (2019), 1061–1087.

[11] Kengne W. Testing for parameter constancy in general causal time-series models. J. Time Ser. Anal.

33, (2012), 503-518.

[12] Kounias, E., and Weng, T. An inequality and almost sure convergence. The Annals of Mathematical

Statistics 40, 3 (1969), 1091–1093.

[13] McQuarrie, A., and Tsai, C. Regression and Time Series Model Selection. World Scientiﬁc Pub Co

Inc, 1998.

[14] Rao, C. R. and Wu, Y. On model selection. IMS Lecture Notes-Monograph Series 38 (2001), 1–64.

10

Strong consistent model selection

[15] Resende, P. A. A. and Dorea, C. C. Y. Model identiﬁcation using the eﬃcient determination

criterion. Journal of Multivariate Analysis 150, (2016), 229–244.

[16] Stout, W. F. The Hartman-Wintner law of the iterated logarithm for martingales. The Annals of

Mathematical Statistics 41 (1970), 2158–2160.

[17] Stout, W. F. Almost sure convergence. Academic press (1974).

[18] Zhao, L. C., Dorea, C. C. Y. and Gonc¸alves, C. R. On determination of the order of a Markov

chain. Statistical inference for stochastic processes 4 (2001), 273–282.

