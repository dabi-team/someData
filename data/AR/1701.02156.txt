Estimating the Competitive Storage Model: A Simulated

Likelihood Approach∗

Tore Selland Kleppe†

Atle Oglend‡

January 10, 2017

Abstract

This paper develops a particle ﬁlter maximum likelihood estimator for the competitive storage model.

The estimator is suitable for inference problems in commodity markets where only reliable price data

is available for estimation, and shocks are temporally dependent. The estimator eﬃciently utilizes the

information present in the conditional distribution of prices when shocks are not iid. Compared to

Deaton and Laroque’s composite quasi-maximum likelihood estimator, simulation experiments and real-

data estimation show substantial improvements in both bias and precision. Simulation experiments also

show that the precision of the particle ﬁlter estimator improves faster than for composite quasi-maximum

likelihood with more price data. To demonstrate the estimator and its relevance to actual data, we ﬁt the

storage model to data set of monthly natural gas prices. It is shown that the storage model estimated

with the particle ﬁlter estimator beats, in terms of log-likelihood, commonly used reduced form time-series

models such as the linear AR(1), AR(1)-GARCH(1,1) and Markov Switching AR(1) models for this data

set.

Keywords: commodity prices, competitive storage model, particle ﬁlter, rational expectations, simulated

likelihood

JEL codes: C13, C15, D22

7
1
0
2

n
a
J

9

]
E
M

.
t
a
t
s
[

1
v
6
5
1
2
0
.
1
0
7
1
:
v
i
X
r
a

∗The authors are indebted to the Editor; Ana Colubi, two anonymous reviewers, Frank Asche, Hans Karlsen, Hans Julius

Skaug and Bård Støve for comments that greatly improved the paper.

†University of Stavanger, Department of Mathematics and Natural Sciences

(Corresponding author:

email:

tore.kleppe@uis.no, address: University of Stavanger, 4036 Stavanger, Norway, telephone: +47 51831717, fax: +47 51831750)

‡University of Stavanger, Department of Industrial Economics

1

 
 
 
 
 
 
1

Introduction

This paper addresses the problem of estimating the structural parameters of the competitive storage model

when only price data is available for estimation and supply shocks are temporally dependent. We propose

and investigate a particle ﬁlter estimator based on recently developed methods in the particle ﬁlter liter-

ature (Gordon et al., 1993; Fernandez-Villaverde and Rubio-Ramirez, 2007; Malik and Pitt, 2011; DeJong

et al., 2013). We demonstrate that this estimator has superior large sample properties and improved param-

eter identiﬁcation properties over the conventional composite pseudo maximum likelihood estimator (CML)

(Deaton and Laroque, 1995, 1996). Compared to the CML, the estimator also displays substantial reduction

in bias when it comes to various predicted price characteristics, including price autocorrelation, where the

CML estimator appears to underestimate price persistence.

Particle ﬁlter methods are suitable to address inference problems in non-linear models with latent vari-

ables, and appear especially relevant for low dimensional models such as the partial equilibrium competitive

storage model in this paper. Our proposed estimator has the added beneﬁt of being an extension of the CML

approach. The CML estimator utilizes invariant distributions to integrate out the latent state variable, in

this case the supply shock. This procedure neglects important information present in the conditional price

distribution when shocks are temporally dependent. It is this information the particle ﬁlter estimator makes

use of when inferring structural parameters.

Improving structural model estimates provides better comparison between competing commodity price

models. We apply our estimator to a natural gas price data set, and compare the structural model to popular

reduced form models such as the linear AR(1) model, AR(1)-GARCH(1,1) model and a Markov Switching

AR(1) model. The comparison to reduced form models allows identiﬁcation of what price features the storage

model is able to account for, and where improvements are needed. For the natural gas market, the storage

model performs better than all the reduced form time-series models considered, in terms of log-likelihood.

This suggests that the non-linearity in the model, arising from the non-negativity constraint on storage, is

relevant for natural gas price characteristics. As support for the relevance of the storage model for natural

gas we also ﬁnd a relatively strong correspondence between observed and model implied storage. The overall

diagnostics show that the storage model addresses features in the higher moments of prices, speciﬁcally linked

to excess kurtosis and ARCH eﬀects.

The competitive storage model is the prevailing economic model explaining price dynamics of storable

commodities. The model incorporates eﬀects of speculative storage behavior on market dynamics in a rational

expectations framework. The model in its current form can trace its origins to the Theory of Storage

(Williams, 1936; Kaldor, 1939; Working, 1949). Since the storage model explicitly recognizes the role of

2

proﬁt maximizing storage behavior, it is often used to examine implications of diﬀerent commodity market

policies and regulations. Miranda and Helmberger (1988) used the storage model to investigate the eﬀects of

government price support programs in the U.S. soybean market. Gouel (2013b) analyzed various food price

stabilization policies (public stocks, state contingent subsidy/tax on production) using an extension of the

competitive storage model with risk averse consumers and incomplete insurance markets. Similarly, Brennan

(2003) used the storage model to analyze the eﬀects of public interventions in the Bangladesh rice market.

Meaningful policy evaluations using structural models require suitable model parameters.

The theoretical foundation of the model is well established by now. What has proved more diﬃcult is con-

fronting the model with data in order to operationalize it for practical policy analysis and hypothesis testing.

Estimation has been limited by two factors: (1) the lack of a closed form solution to the model, necessitat-

ing a computationally demanding subroutine to solve a dynamic optimization problem for each parameter

evaluation, and (2) lack of reliable quantity data. With improved computational power the ﬁrst factor is

becoming less important, although the curse of dimensionality quickly rebalances growth in computational

power. For data availability, some commodity markets do have reliable quantity data. The London Metal

Exchange (LME), for instance, provides information on metal stocks in LME warehouses (see Geman and

Smith (2013) for an analysis of the Theory of Storage as applied to the LME metals). Miranda and Glauber

(1993) estimate the storage model using stock and price data from the U.S. soybean market. Price data

however remains the most accessible, and arguably highest quality, commodity data. Because of this, price

data should in general be used when estimating the storage model. We provide simulation evidence which

shows that price data is suﬃcient to fully identify the structural parameters of the model when the particle

ﬁlter estimator is applied, even when shocks are not iid. For model veriﬁcation and hypothesis testing it is

also desirable to retain some data to test model predictions. We illustrate this in the empirical application

part of the paper when we compare model implied storage, derived using only price data, with actual storage.

An early step towards empirical testing of the model can be found in Wright and Williams (1982). The

authors provide numerical solutions to the model under diﬀerent speciﬁcations, and conﬁrm that model

implied behavior is consistent with observations of some important commodities. One important feature of

the storage model is the non-negativity constraint on storage. This constraint makes the model non-linear;

the no-arbitrage restriction characterizing proﬁt maximizing storage can fail if the commodity is suﬃciently

scarce. Prices will move between two pricing regimes deﬁned by whether or not the no-arbitrage restriction

holds. Which regime is active depends on current price relative to a threshold price marking the cut-oﬀ

point where the no-arbitrage restriction fails. Deaton and Laroque (1992) and Ng (1996) estimate threshold

prices for various commodities using a generalized method of moments estimator and ﬁnd evidence for regime

behavior consistent with model predictions.

3

Full structural estimation without quantity data was achieved in a series of pioneering papers by Deaton

and Laroque (1992, 1995, 1996). Deaton and Laroque estimated the model on price data by means of a

composite pseudo maximum likelihood estimator. Although a signiﬁcant step forward, the eﬃciency and

precision of the estimator has been questioned. Michaelides and Ng (2000) show, using Monte Carlo simula-

tions, that the CML estimator tends to bias the estimates. The authors also ﬁnd that none of the simulation

estimators beat the CML estimator in a mean-squared sense, but that they have improved bias properties.

The particle ﬁlter estimator in this paper addresses the CML estimator bias. Caﬁero et al. (2006) argue

that the crucial kink in the pricing function, due to the non-negativity constraint on storage, is imprecisely

estimated because of the smoothing eﬀects of spline methods used to interpolate between grid points. Caﬁero

et al. (2011) show that precision can be greatly improved by increasing the number of grid points used to

approximate the pricing function. Recently Caﬁero et al. (2015) proposed a maximum likelihood estimator of

the storage model with small properties superior to that of the pseudo maximum likelihood approach. Their

estimator relies of iid normal supply shocks, and as such does not allow for temporal shock dependence. The

estimator considered in this paper provides an alternative to the maximum likelihood estimator of Caﬁero

et al. (2015) in the case of temporal shock dependence.

By allowing for shock dependence, our paper deviates somewhat from the empirical literature on the

model. With the exception of Deaton and Laroque (1995, 1996), most applications assume iid shocks.

One problem iid shocks is that predicted price persistence is lower than what is typically observed. This

can be partly remedied by increasing the number of grid points used to approximate the pricing function

(Caﬁero et al., 2011). Extensions to the basic model such as allowing for an explicit convenience yield

component (Miranda and Rui, 1996; Ng and Ruge-Murcia, 2000), gestation lags in production (Ng and

Ruge-Murcia, 2000), the eﬀect of news on future production (Osborne, 2004) and storage frictions (Mitraille

and Thille, 2009) have also been shown to increase model price autocorrelation. The basic model has also

been extended to a more general equilibrium framework (Funke et al., 2011; Arseneau and Leduc, 2013), and

to consider monopolistic behavior in speculative storage (Mitraille and Thille, 2009). Temporal dependence

in supply and/or demand shocks is not unreasonable for the types of partial equilibrium models that the basic

competitive storage model represents. Any non-modeled stochastic exogenous eﬀects must enter through the

speciﬁed shock processes. Slowly changing macroeconomic conditions, for instance, is likely to give rise to

persistence in demand for the commodity. Overall, it seems unreasonable to expect that speculative storage

alone can fully account for the substantial shock persistence observed in commodity prices. In the additive,

single shock, formulation of the standard competitive storage model, positive demand shocks are equivalent

to negative supply shocks, and supply shocks should be interpreted as net-supply shocks.

Another issue in the Deaton and Laroque CML estimation is the use of observed Fisher information to

4

derive asymptotic standard deviations of estimated parameters. However, we have found that asymptotic

results can be misleading for sample sizes relevant to this methodology. To work around this complication,

we make no eﬀort to make the log-likelihood smooth. Rather we introduce a full parametric speciﬁcation

consistent with the moments of the storage model and rely on parametric bootstrap for estimating statistical

standard errors. The validity of the parametric speciﬁcation is then tested via generalized residuals.

The paper proceeds as follows. In section 2 we give a description of the storage model. Section 3 describes

the estimation methodology. Following this we investigate the performance of the estimator on simulated data

in section 4. Comparing the estimator to the CML estimator we ﬁnd that bias is substantially reduced. In

addition the precision of key structural parameters are greatly improved. In section 5 we apply the estimation

procedure to a natural gas data set and ﬁnd evidence that the non-negativity constraint on storage is relevant

in describing key properties of prices. Finally, section 6 concludes.

2 The Storage Model

Before turning to the issue of estimating structural parameters, we provide a brief description of the basic

competitive storage model with autocorrelated supply shocks. The model is the same as in Deaton and

Laroque (1995, 1996), from now on referred to as DL. For more details on the model see Deaton and Laroque

(1996).

Assume at any time there is exogenous stochastic supply zt, which follows a ﬁrst-order linear autoregres-

sive process: zt = ρzt−1 + (cid:15)t, where (cid:15)t is a standard normal random variable. The supply shock is what

fundamentally drives variations in prices. The market consists of consumers and risk-neutral competitive

speculators that hold inventories. Following DL, we assume a proportional decay of stocks in storage. The

constant depreciation rate δ accounts for the direct cost of storage, and is a structural parameter to be

estimated. Let It be the level of inventories at time t. The amount of stocks on hand xt and supply zt then

follow the laws of motion:

xt = (1 − δ) It−1 + zt,

zt = ρzt−1 + (cid:15)t.

(1)

(2)

The problem facing speculators is choosing the level of inventories to maximize the expected discounted

proﬁts from storage. In this paper, as in DL, consumers are assumed to hold linear inverse demand, repre-

sented by P (z) = a + bz, where a and b < 0 are structural parameters. The opportunity cost of capital tied

5

to storage is assumed a ﬁxed real interest rate r per period. Combined with the depreciation rate, β = 1−δ
1+r

accounts for the cost of storing one unit of the commodity for one period. There are two state variables

relevant to the optimal storage decision at any time, the current stock level xt and the current state of the

supply shock zt. Let V (xt, zt) be the value of the commodity stock at time t given competitive speculators

follow an optimal storage policy. This value function must satisfy the Bellman functional equation

V (xt, zt) = max

It

{pt (xt − It) + βEtV ((1 − δ) It + zt+1, ρzt + (cid:15)t+1.)} ,

(3)

where the maximization is subject to equations (1),(2) and the non-negativity constraint It ≥ 0. Note that

xt − It is the amount supplied to the market, and pt the commodity price at time t. The price is considered

ﬁxed when making storage decisions and speculators are assumed to hold rational expectations. Given

storage is not bounded, It > 0, proﬁt maximizing competitive storage implies the no-arbitrage restriction:

βEtpt+1 = pt (this follows from taking the derivative of equation 3 w.r.t. to It, setting equal to zero, and

applying the envelope theorem). The no-arbitrage relationship will fail if P (xt) ≥ βEtpt+1, in which case

optimal storage must be It = 0 (stock-out). The optimal storage policy implies the following restriction:

pt = max [P (xt) , βEtpt+1]

(4)

The restriction states that if selling the entire stock xt gives a higher price than what could be gained

from storing, zero storage is optimal and current price is dictated by consumer willingness to pay for existing

stocks, P (xt). Otherwise, the no-arbitrage condition holds and proﬁt maximizing storage ensures current

price pt equal to βEtpt+1.

Provided supply has compact support and inventories are costly β < 1, Deaton and Laroque (1992) and

Chambers and Bailey (1996) prove that a solution to (4) exist in the form of a rational expectations pricing

function p = f (x, z). The pricing function f (x, z) gives the competitive equilibrium price p consistent with

optimal storage at states x and z and market clearing. The function in general has no closed form solution

and must be solved for numerically. The mapping p = f (x, z) is the solution to the functional equation:

f (x, z) = max

β

(cid:20)

(cid:90)

(cid:21)
f (cid:0)(cid:15) + ρz + (1 − δ) (cid:0)x − P −1 (f (x, z))(cid:1) , (cid:15) + ρz(cid:1) dΦ ((cid:15)) , P (x)

(5)

where Φ ((cid:15)) is the standard normal distribution function. For given parameters, the function can be found by

conventional numerical procedures, we refer to section 3.1 for the speciﬁc numerical procedure used in this

paper. The price function provides the means to generating the predictive moments necessary for estimation

purposes.

6

3 Estimation Methodology

The problem at hand is to estimate the structural parameters: θ = [ρ, a, b, δ]. By utilizing the price function

(5) we can construct the one-period ahead mean and variance of price as a function of current price pt

and supply shock zt. We deﬁne these quantities respectively as µ(pt, zt) := E(pt+1|pt, zt) and σ2(pt, zt) :=

V ar(pt+1|pt, zt). This procedure allows the construction of the likelihood over all observed prices. For the

model with autocorrelated shocks, the conditional mean and variance depend on the unobserved supply shock

zt. As such zt must be integrated out of the conditional moments. To achieve this, DL integrate over the

invariant distributions of zt conditionally on pt. However, using invariant distributions do not utilize the

full information available from the observables about the state of the system, and is likely to be ineﬃcient,

as was recognized by DL themselves. Simulation evidence provided below also suggest that the invariant

distribution integration does not lead to improved large sample properties of the estimator; the bias of the

estimates does not appear to shrink as more price data becomes available. In the following we propose a

particle ﬁlter approach to deal with this problem. The particle ﬁlter avoids using invariant distributions,

which provides more eﬃcient and precise estimates.

Throughout we let N (x; µ, σ2) denote the N (µ, σ2) density evaluated at x. Moreover, for consistency

with the notation of DL who already use p and f , we use π(x) as the generic symbol for probability density

functions.

3.1 Numerical solution of the price function equation

As for most non-linear rational expectation models, obtaining a closed form solution to the dynamic opti-

mization problem characterized by (5) is not possible, and we must resort to numerical methods. A large

body of literature, surveyed in e.g. Miranda (1997); Aruoba et al. (2006); Gouel (2013a), is devoted to such

numerical solutions. The price function f (x, z) is known to have discontinuous derivatives w.r.t. x at the

point x∗(z), the threshold stock level where the no-arbitrage restriction breaks down for stocks below this

level, i.e. where I = 0 for x ≤ x∗(z). The threshold stock level is characterized by

x∗(z) = sup{x; f (x, z) = P (x)}.

To solve for the price function f (x, z) we iterate on the recursive formulation of equation 5. This provides

a quick and reliable numerical approximation to f (x, z). Perturbation- and spectral methods appear less

applicable for non-smooth solutions, and are also in general more time consuming. Our implementation is

similar in spirit to that of DL, but diﬀers in particular in the choices of grids.

7

3.1.1 Price function iteration grid

When choosing the grid over which the price function is computed, several aspects have to be taken into

consideration. Firstly, the grid has to be ﬁne enough to avoid substantial biases in the parameter estimates

(Caﬁero et al., 2011). Moreover, since the price function is computed for many sets of parameters during

the numerical maximization of the likelihood, computational tractability must be kept in mind. Finally, the

price function must be continuous w.r.t. the parameters, and therefore rules out any adaptive strategies for

ﬁnding grids.

The speciﬁc grids employed are in the z-direction (supply shocks) equally spaced with Mz grid-points

Z1, . . . , ZMz . Throughout most of this work, Mz is set to 64. The grid is chosen to cover 6 standard deviations
of the unconditional distribution of zt, corresponding to Z1 = −6/(cid:112)1 − ρ2 and ZMz = 6/(cid:112)1 − ρ2. Compared

to DL, who used 11 grid points and a discretization similar to that of Tauchen (1986), our grid is more ﬁnely

spaced and covers a larger range. This reﬂect both access to faster- and parallel computing and the fact that

the later described particle ﬁlter sometimes request evaluations of z (cid:55)→ f (x, z) in areas that are very unlikely

to be visited under the marginal distribution of zt.

In the x-direction (stocks), we divide the grid over which f (x, z) is computed into two parts. First, a

ﬁner equally spaced grid with Mx,1 grid points covering the lower range of x known to contain x∗(z) for all

relevant values of z. Second, a coarser equally spaced grid with Mx,2 grid points covering the higher range

of x where x (cid:55)→ f (x, z) is slowly varying. The grid points are denoted by X1, . . . , XMx,1+Mx,2, and for most

of this work, Mx,1 = Mx,2 = 128. The range of the ﬁrst grid is set to

X1 = min (cid:0)P −1(pmax), Z1

(cid:1) , XMx,1 = max

(cid:16)

−

a
b

, ZMz

(cid:17)

.

where pmax is larger than the observed maximum price1. In the deﬁnition of X1, P −1(pmax) ensures that the

numerical representation of f (x, z) spans suﬃciently high prices, and Z1 reﬂects that stock at hand cannot

be smaller than the smallest supply (Deaton and Laroque, 1996). In the upper limit of the ﬁrst grid, −a/b

ensures that XMx,1 ≥ x∗(z) since f (−a/b, z) > P (−a/b) = 0. Moreover, ZMz is chosen less rigorously based

on experience to ensure numerical accuracy in cases when the demand slope b is large in magnitude. The

second grid is uniformly spaced with

XMx,1+1 = XMx,1 + (XMx,1+Mx,2 − XMx,1 )/Mx,2, XMx,1+Mx,2 = cZMz /δ.

The upper x-range of the grid is inspired by DL, who obtain that the stock at hand xt asymptotes to ZMz /δ

1In this work we typically set pmax = 20 as the real data sets are normalized to have unit mean.

8

when the supply is always at maximum ZMz and no stock is consumed. However, we have included the factor

c = 1.5 obtained by trial and error to ensure that suﬃciently low prices are always represented, in particular

when the numerical optimizer tries values of the parameters that are inconsistent with the data at hand.

3.1.2 Price function iteration

Equation (5) deﬁnes a functional ﬁxed point for f (x, z) which we iterate from an initial guess. Our numerical

solution, say ˆf (x, z), obtains as a bilinear interpolation between tabulated values over the previously discussed

grid. The integral in (5) for each grid point (Xi, Zj) is approximated as

(cid:90)

(cid:90)

f (cid:0)(cid:15) + ρZj + (1 − δ) (cid:0)Xi − P −1 (f (Xi, Zj))(cid:1) , (cid:15) + ρZj
Xi − P −1 (cid:16) ˆf (Xi, Zj)

η + (1 − δ)

(cid:17)(cid:17)

, η

ˆf

(cid:16)

(cid:17)

(cid:16)

N (η; ρZj, 1)dη,

(cid:1) dΦ((cid:15)),

ˆf (Zk + (1 − δ)

(cid:16)

Xi − P −1 (cid:16) ˆf (Xi, Zj)

(cid:17)(cid:17)

, Zk)Wj,k,

Mz(cid:88)

k=1

≈

≈

(6)

(7)

(8)

where the weight matrix W has elements

Wj,k =

N (Zk; ρZj, 1)
l=1 N (Zl; ρZj, 1)

(cid:80)Mz

, j, k = 1, . . . , Mz.

I.e. each row in the weight matrix represents discrete distribution over {Z1, . . . , ZMz } having probability

masses proportional to N (Zk; ρZj, 1), k = 1, . . . , Mz. The reason for choosing this particular discretization

follows the insight of DL that it leads to ˆf (x, z) being evaluated only at grid points in the z-direction. Thus

the evaluation of each term in the sum (8) is reduced to a univariate linear interpolation problem in the

x-direction.

Equipped with the integral approximation (6 - 8), the price function iteration is started at ˆf (x, z) =

max(P (x), 0), and proceeds by alternating between

G(Xi, Zj) = β

Mz(cid:88)

ˆf (Zk + (1 − δ)

(cid:16)

Xi − P −1 (cid:16) ˆf (Xi, Zj)

(cid:17)(cid:17)

, Zk)Wj,k,

k=1
ˆf (Xi, Zj) = max(P (Xi), G(Xi, Zj)),

for i = 1, . . . , Mx,1 + Mx,2, j = 1, . . . , Mz. Throughout this work, we perform 400 iterations, as a ﬁxed

number of iterations are required to obtain a continuous likelihood. This number of iterations is typically

suﬃcient to bring maximal absolute change in ˆf at the last iteration to ∼ 1e − 3. The outer loop over i is

performed in parallel over 8 kernels on the computer applied and the overall routine for calculating ˆf requires

∼ 1 second when implemented in Fortran 90.

9

3.1.3 Predictive moments

The predictive moments µ(pt, zt) = E(pt+1|pt, zt) and σ2(pt, zt) = V ar(pt+1|pt, zt) are required to form the

likelihood function of the data. We compute them in two steps. First, the time t stock at hand xt = xt(pt, zt)
is recovered, modulo price function approximation error, from pt and zt by solving pt = ˆf (xt, zt) for xt. In

practice, this is implemented using a binary search in the x-direction. As bilinear interpolation is employed

to compute ˆf (x, z) oﬀ the grid, we have not encountered cases where x (cid:55)→ ˆf (x, z) is not monotone, and

therefore no problems with non-uniqueness of xt was encountered.

Secondly, the predictive mean is calculated using that

µ(pt, zt) = E(pt+1|pt, zt) = E(f (xt+1, zt+1)|pt, zt)

= E (f ((1 − δ)It + zt+1, zt+1) |pt, zt) ,

(cid:90)

=

f ((1 − δ) (cid:2)xt(pt, zt) − P −1(pt)(cid:3) + ρzt + (cid:15)t+1, ρzt + (cid:15)t+1)dΦ((cid:15)t+1),

and a completely analogous argument applies for the conditional variance σ2(pt, zt). Storage It is here derived

as the diﬀerence between implied stock xt(pt, zt) and consumption P −1(pt). In practice, we implement these

integrals using 16-point Gauss-Hermite quadrature and substituting ˆf for f .

3.2

Inference

In their inference, DL do not specify a parametric family for pt+1|pt, zt, and use quasi maximum likelihood2

for estimation. Here we depart in taking pt+1|pt, zt to be Gaussian to obtain a, conditionally on p1, complete

parametric model

pt+1 = µ(pt, zt) + (cid:112)σ2(pt, zt)ηt+1, ηt ∼ i.i.d. N (0, 1), t = 1, . . . , T − 1,

zt+1 = ρzt + (cid:15)t+1, (cid:15)t ∼ i.i.d. N (0, 1), t = 1, . . . , T − 1,

(cid:18)

z1 ∼ N

0,

(cid:19)

,

1
1 − ρ2

(9)

(10)

(11)

that is consistent with the moments of the price process of the storage model.

It is worth noticing that

pt+1|pt, zt ∼ N (µ(pt, zt), σ2(pt, zt)) does not imply that pt+1|pt or pt+1|p1, . . . , pt are Gaussian. Rather, the

unconditional transition laws are complicated mean-variance mixtures of Gaussians that can exhibit skewness

and heavy tails.

The reason for choosing a parametric model is mainly for convenience, as doing so grants us access to

2 In the sense of substituting unspeciﬁed families of distributions with Gaussians (Gourieroux et al., 1984).

10

the likelihood analysis toolbox and parametric bootstrapping. Moreover, the diagnostics reported later allow

us to test whether imposing a Gaussian distribution on ηt is reasonable. Quasi maximum likelihood, on the

other hand, would be complicated due to the lack of reliable derivatives to form sandwich formulas, and is

not easily bootstrapped.

The model (9-11) is, for observations p1, . . . , pT , a dynamic latent variable model, with dependence

structure typical of a time-discretized diﬀusion model, and the conditional likelihood function is expressed in

terms of

L(θ|p1, . . . , pT ) = π(p2, . . . , pt|p1) =

(cid:90)

π(z1)

T
(cid:89)

t=2

π(pt, zt|pt−1, zt−1)dz1 · · · dzT .

(12)

To circumvent integrating over zt, DL uses a composite likelihood technique (Lindsay, 1988) where the

product of likelihoods of consecutive pairs of prices is substituted for the full likelihood. Using composite

likelihoods is known to result in loss of estimation eﬃciency (Varin and Vidoni, 2008) over full likelihood-

based techniques as we propose here. A large body of literature is devoted to fully likelihood-based inference

in models with this structure, and include Bayesian Markov chain Monte Carlo (MCMC) (Eraker, 2001)

and simulated maximum likelihood (Durbin and Koopman, 1997; Shephard and Pitt, 1997; Durham, 2006).

As the calculation of ˆf for each combination of the parameters is expensive, MCMC (including the particle

MCMC approach of Andrieu et al. (2010); Pitt et al. (2012)) would be very time consuming. Moreover,

the fact that zt (cid:55)→ log π(pt+1|pt, zt), implied by (9), is a non-smooth function bars the usage of smoothing-

based importance samplers such as Durbin and Koopman (1997); Shephard and Pitt (1997); Liesenfeld and

Richard (2003); Richard and Zhang (2007). Consequently, we resort to simulated maximum likelihood, where

the likelihood function is calculated using a continuous particle ﬁlter in the spirit of Malik and Pitt (2011)

to obtain eﬃcient estimators and retain computational tractability.

3.2.1 Simulated likelihood

To estimate the marginal log-likelihood l(θ) = log L(θ|p1, . . . , pT ) at a ﬁxed parameter θ, we take as van-

tage point the sampling importance resampling (SIR) particle ﬁlter (Gordon et al., 1993), adapted to time-

discretized diﬀusion structures as described in Durham (2006). The ﬁlter with N particles may be described

as follows:

1. Initialization: set t = 1, ˆl = 0

2. Simulate z(j)

t,pred ∼ π(z1), j = 1, . . . , N .

3. Compute w(j)

t = π(pt+1|pt, z(j)

t,pred) and set Lt = 1
N

(cid:80) w(j)
t

. [Lt is an approximation to π(pt+1|p1, . . . , pt)]

4. Set ˆl ← ˆl + log(Lt). [ˆl is an approximation to log π(p2, . . . , pt+1|p1)]

11

5. Re-sample N particles from z(j)

k=1 w(k)
t,pred with weights w(j),∗
is an equally weighted representation of π(zt|p1, . . . , pt+1)]

t / (cid:80)N

=w(j)

t

t

(cid:110)

z(j)
t,f ilt

[

(cid:111)N

j=1

to form z(j)

t,f ilt, j = 1, . . . , N .

6. Simulate z(j)

t+1,pred ∼ π(zt+1|zt = z(j)

t,f ilt), j = 1, . . . , N .

(cid:110)

z(j)
t+1,pred

[

(cid:111)N

j=1

is an equally weighted repre-

sentation of π(zt+1|p1, . . . , pt+1)]

7. If t < T − 1, set t ← t + 1 and go to step 3.

It is well known (see e.g. Malik and Pitt, 2011) that the resampling in step 5 originate discontinuous likelihood

approximations even if the same random numbers are used for repeated evaluation of ˆl(θ) for diﬀerent values

of the parameters, and thereby renders subsequent numerical maximization diﬃcult. Malik and Pitt (2011)

propose to obtain draws in step 5 based on a smoothed version of the empirical distribution function of the

weighted sample (z(j)

t,pred, w(j),∗

t

). We follow a diﬀerent path to obtain a continuous likelihood, in combining

step 5 and 6. Write

π(zt|p1, . . . , pt+1) ≈

N
(cid:88)

j=1

w(j),∗
t

δ(zt − z(j)

t,pred)

where δ(·) denotes a unit point mass in 0. Then

π(zt+1|p1, . . . , pt+1) ≈

N
(cid:88)

j=1

t N (zt+1|ρz(j)
w(j),∗

t,pred, 1)

which can be sampled from continuously using inversion sampling. We use a fast Fourier transform method

(Kleppe and Skaug, 2016) based on stratiﬁed uniform common numbers described in Appendix A which has

the same computational complexity of O(N log2(N )) as the method of Malik and Pitt (2011). Notice that

advancing to the supply process (step 6) does not involve simulation, which will have desirable eﬀect on

Monte Carlo variability of overall Monte Carlo estimate of the log likelihood ˆl(θ).

A number of improved particle ﬁlters have been proposed in literature (see Cappe et al., 2007, for a

survey) with the prospect of further reducing Monte Carlo variation. However, for relevant ranges of the

parameters, the model (9-11) has a low signal-to-noise ratio. Thus does our simple modiﬁed SIR ﬁlter produce

suﬃciently precise estimates of the log-likelihood for moderate N . E.g.

for T ∼ 1000, N = 4096 and the

ﬁlter implemented in Fortran 90 produces acceptable accuracy while the computing times of an evaluation

of ˆl(θ) is on the same order as the time required to compute ˆf .

3.2.2 Algorithm, estimation and standard errors

Based on the above introduced notation, to evaluate ˆl(θ) for any given θ and price series p1, . . . , pT requires

the following steps:

12

1. Solve numerical the price function ˆf (x, z) as described in section 3.1-3.1.2 for parameter θ.

2. Set t = 1, ˆl = 0 and simulate z(j)

t ∼ N (0, 1/(1 − ρ2)), j = 1, . . . , N .

3. Compute µ(pt, z(j)

t

), σ2(pt, z(j)

t

as described in Section 3.1.3 and set w(j)

) for j = 1, . . . , N based on the numerical solution of the price function
k=1 w(k)

t = N (pt+1|µ(pt, z(j)

), σ2(pt, z(j)

)), w(j),∗
t

t / (cid:80)N

= w(j)

t

t

t

for j = 1, . . . , N .

4. Set Lt = 1
N

(cid:80) w(j)
t

and ˆl ← ˆl + log(Lt).

5. Simulate z(j)

t+1 ∼ (cid:80)N

k=1 w(k),∗

t N (zt+1|ρz(k)

t

, 1), j = 1, . . . , N using the FFT-based method described in

Appendix A.

6. If t < T − 1, set t ← t + 1 and go to step 3. Otherwise return ˆl(θ) = ˆl.

The use of bilinear interpolation for evaluation of ˆf oﬀ the grid results in ˆl(θ) having discontinuous derivatives.

Thus gradient-based optimizers are not suitable, and we therefore ﬁnd the simulated maximum likelihood

estimator ˆθ = arg max ˆl(θ) using the Nelder-Mead type optimizer implemented in MATLAB’s fminsearch

routine.

The lack of continuous derivatives also bars estimating reliable parameter standard errors using the

observed Fisher information matrix. Instead, we rely on parametric bootstrap, where the simulated maximum

likelihood estimator is applied to synthetic data simulated from the model (9-11).

3.2.3 Properties of the simulated maximum likelihood estimator

All structural parameters in the model are statistically identiﬁed given the non-negativity constraint on

storage is not at all times binding. The price process implied by the model moves between two regimes

dependent on the storage constraint. The only parameter unique to the positive storage regime is depreciation

δ. As this tends to one, storage becomes increasingly costly and the storage regime less relevant. We show in

the empirical application below that under zero speculative storage, the price process reduces to a linear AR(1)

process, which is stationary and mean reverting given the supply shock is stationary and mean reverting. At

the other extreme, where depreciation tends to −r, storage becomes costless and the storage constraint less

likely to be binding. The parameters will still be identiﬁed, but the non-linearity arising from the regime-

shifts will be less relevant to the price dynamics. Indeed, the primary feature of the DL formulation of the

storage model is that the market should move between regimes where speculative storage occurs and where

there is a speculative storage stock-out. The relevance of this regime shift feature can be checked by looking

at the probability of model implied stock levels moving below a level where speculative storage is proﬁtable.

We do this in our empirical application below.

13

3.2.4 Diagnostics

When carrying out diagnostics tests on time series models, a standard approach is to carry out a battery

of tests on the residuals. For the dynamic latent variable models, calculating and characterizing residuals

are complicated by the presence of the latent factor. For model (9-11), both pt+1|pt and pt+1|p1, . . . , pt

are complicated non-linear (in pt or p1, . . . , pt) mean-variance mixtures of Gaussians without closed form

expressions. Consequently we resort to generalized residuals as explained in Durham (2006). For a correctly

speciﬁed model, the probability ut of observing pt or smaller, conditionally on p1, . . . , pt−1 should be iid

uniformly distributed. Fortunately, these generalized residuals ut can easily be estimated from particle ﬁlter

output (Durham, 2006) as

ut ≈ ˆut =

1
N

N
(cid:88)

j=1

(cid:16)

Φ

pt|µ(pt−1, z(j)

t−1), σ2(pt−1, z(j)
t−1)

(cid:17)

, t = 2, . . . , T.

(13)

Throughout this work, we use for diagnostics purposes the transformed generalized residuals ˆηt = Φ−1(ˆut), t =
2, . . . , T , from now on referred to as the residuals. Under a correctly speciﬁed model {ˆηt}T
an iid sequence of standard Gaussian variables. However it is worth noticing that {ˆηt}T

t=2 should be

t=2 being Gaus-

sian does not imply that pt+1|p1, . . . , pt, t = 1, . . . , T − 1 are Gaussian, only that the distributions of

pt+1|p1, . . . , pt, t = 1, . . . , T − 1 are correctly captured by the model.

3.3 Composite likelihood estimator

As a benchmark for our proposed simulated maximum likelihood estimator, we also implement a composite

quasi maximum likelihood estimator in the spirit of DL. We use the same numerical solution procedure for

the price function ˆf . This method relies on looking at consecutive pairs of prices (pt, pt+1), t = 1, . . . , T − 1

and ﬁnding the unconditional (with respect to zt) versions of the predictive moments µ, σ2 as

(cid:90)

(cid:90)

µ(pt) =

σ2(pt) =

µ(pt, zt)π(zt|pt)dzt, t = 1, . . . , T − 1,

σ2(pt, zt)π(zt|pt)dzt, t = 1, . . . , T − 1,

(14)

(15)

where the conditional π(zt|pt) derives from the joint stationary distribution π(pt, zt). For this purpose, DL

derive the full joint stationary distribution of (xt, zt) on the grid used for solving for f , and thereafter derive

distributions of zt conditional on each observed pt. In our setup, we advise against this practice, as it involves

factorizing a (Mx,1 + Mx,2)Mz × (Mx,1 + Mx,2)Mz = 16384 × 16384 unstructured matrix. Instead we rely on

a Monte Carlo estimator of π(pt|zt) which is implemented using the following steps:

14

1. Simulate a realization {(ˇpt, ˇzt)}nint

t=1 of the joint (pt, zt) process (9-11) of length nint.

2. Subsample every nt-th observation of the simulated process to form the less autocorrelated sample

˜S = {(˜pi, ˜zi)}ni

i=1 where ˜pi = ˇpnt(i−1)+1, ˜zi = ˇznt(i−1)+1.

3. Calculate observed mean and variance of ˜S, i.e. mp = 1
ni

correspondingly for mz, s2
z.

4. Set up a uniform grid Z = {z(j)}ng

j=1 covering mz ± 4(cid:112)s2
z.

(cid:80)ni

i=1 ˜pi, s2

p = 1

ni−1

(cid:80)ni

i=1(˜pi − mp)2, and

5. For each t = 1, . . . , T − 1; calculate un-normalized kernel estimate with Gaussian kernels to π(zt|pt)

over Z given by

w(j)

t =

ni(cid:88)

i=1

(cid:18)

exp

−

1
2

(pt − ˜pi)2
h2
p

−

1
2

(z(j) − ˜zi)2
h2
z

(cid:19)

.

Here the bandwidths are chosen via the simple plug-in rules hp = 2n− 1

6

i

(cid:113)

p, hz = 2n− 1
s2

6

i

(cid:112)s2
z.

6. For each t = 1, . . . , T − 1; approximate (14, 15) as

µ(pt) ≈ ¯µ(pt) =

σ2(pt) ≈ ¯σ2(pt) =

(cid:80)ng

j=1 w(j)
(cid:80)ng

t

,

t µ(pt, z(j))
j=1 w(j)
t σ2(pt, z(j))
j=1 w(j)

t

.

(cid:80)ng

j=1 w(j)
(cid:80)ng

The composite quasi log-likelihood function then becomes

¯l(θ) =

(cid:32)

−

T −1
(cid:88)

t=1

[pt+1 − ¯µ(pt)]2
2¯σ2(pt)

−

1
2

log (cid:2)2π¯σ2(pt)(cid:3)

(cid:33)

,

and the corresponding composite quasi maximum likelihood estimator is given as ¯θ = arg maxθ

¯l(θ).

In

practice we choose the tuning parameters to be ni = 50, 000, nt = 32 and ng = 128, and therefore this

routine is considerably more expensive than the above described simulated maximum likelihood routine,

but still less costly than computing the full stationary distribution on the (x, z)-grid. As for the simulated

maximum likelihood estimator, smoothness is barred by the interpolation required, and we therefore use

the same Nelder-Mead type optimizer for maximizing ¯l(θ). Moreover, we rely on parametric bootstrap with

(9-11) as data generating process for approximate standard errors.

Modulo Monte Carlo approximation error, it is seen that ˆθ maximizes (cid:80)T

t=2 log π(pt|p1, . . . , pt−1), whereas

¯θ maximizes (cid:80)T

t=2 log π(pt|pt−1). A number of factors determine how much the two diﬀer. In particular are

pt|pt−1and pt|p1, . . . , pt−1 equal in distribution when the non-negativity constraint is binding, and therefore

one would primarily expect the two estimators to diﬀer when δ is small and stock-outs are infrequent.

15

Moreover, one would expect that the two would diﬀer more at higher sampling frequencies or with values of

ρ closer to 1 since then p1, . . . , pt−2 are more informative with respect to pt.

4 Simulation study

To study the performance of the proposed particle ﬁlter-based simulated ML estimator (SML) based on ˆl(θ),

and also to compare with the composite ML (CML) estimator based on ¯l(θ), we conduct a simulation study.

It should be noted that data are simulated using the numerical solution ˆf , and biases incurred by using ˆf

instead of f in the estimation are consequently not visible in this study. See section 5.2 for a study of the

sensitivity of parameter estimates to the grid size. Throughout, we use N = 4096 particles in the ﬁlter,

and the real interest rate r is chosen to correspond to a 5% yearly rate. Each experiment is repeated 100

times and the true parameters in the data generating process (9-11) are taken to resemble those found in

empirical applications. Throughout, the optimizations performed under SML and CML are started at the

true parameters of the data generating process.

4.1 Monthly data experiment

In the ﬁrst simulation study, we consider prices at an equivalent monthly frequency and true parameters

inspired by natural gas real data. The bias, standard deviation and root mean square error (RMSE) are

reported in Table 1 for diﬀerent sample sizes T = {250, 500, 1000}. For the SML estimator, some biases are

seen for the shorter sample size T = 250, but these biases seem to diminish for the larger sample sizes at a

rate that is largely consistent with- or faster than what standard maximum likelihood theory would predict.

Largely the same eﬀects are seen for the standard deviations and RMSEs. Looking at the magnitudes of

the RMSEs, we see that the parameters ρ and δ, which in large part determine the temporal dependence

structure, are estimated with relatively good precision. On the other hand, a and b, which as a rule of thumb

correspond to marginal location and scale of the price process sees relatively large RMSEs, in particular for the

smaller sample sizes. This is related to the fact that the estimation of these parameters is strongly inﬂuenced

by the price spikes, and the number and severity of price spikes vary substantially over the simulated data

sets.

For CML, we obtain RMSEs that are consistently larger than those of SML, with diﬀerences being largest

for a and b at the larger sample sizes. Moreover, in line with Michaelides and Ng (2000), substantial biases

that vanishes only very slowly as T increases are seen for CML. Looking at relative computing times and

errors induced by using Monte Carlo methods, we see that SML is substantially less expensive to evaluate

and also produces consistently smaller Monte Carlo errors than those of CML.

16

True parameters

SML

CML

SML

CML

SML

CML

a
1.5

ρ
0.97

b
-0.4
T = 250, τSM L = 2.4s, τCM L = 7.1s
-0.2173
0.9636
0.9831
0.0069

-0.0146
0.0232
0.0273
0.0010

-0.4821
1.5897
1.6536
0.0315

Bias
Std.dev.
RMSE
MC.Std.dev

Bias
Std.dev.
RMSE
MC.Std.dev

-0.4200
3.3513
3.3606
0.0841

-0.0264
0.0397
0.0475
0.0025

-0.3923
1.3833
1.4311
0.0269
T = 500, τSM L = 3.0s, τCM L = 11.8s
-0.0340
0.2169
0.2185
0.0059

-0.1110
0.5020
0.5116
0.0095

-0.0050
0.0124
0.0133
0.0004

Bias
Std.dev.
RMSE
MC.Std.dev

Bias
Std.dev.
RMSE
MC.Std.dev

-0.1922
1.6313
1.6344
0.0195

-0.0185
0.0256
0.0315
0.0011

-0.3953
0.8874
0.9673
0.0060
T = 1000, τSM L = 3.9s, τCM L = 19.5s
0.0028
0.0664
0.0661
0.0026

-0.0021
0.0065
0.0068
0.0003

-0.0258
0.2280
0.2284
0.0084

Bias
Std.dev.
RMSE
MC.Std.dev

Bias
Std.dev.
RMSE
MC.Std.dev

-0.0109
0.0101
0.0149
0.0020

-0.1408
1.4722
1.4715
0.1067

-0.3801
0.7430
0.8313
0.0280

δ
0.02

0.0011
0.0078
0.0078
0.0007

0.0054
0.0086
0.0102
0.0005

0.0005
0.0050
0.0050
0.0002

0.0048
0.0071
0.0085
0.0005

0.0004
0.0031
0.0031
0.0002

0.0047
0.0054
0.0071
0.0007

(q-)log-like

0.0376

0.0982

0.0522

0.0884

0.0191

0.0970

Note: True parameters indicated in the uppermost row and r corresponding to monthly data. Bias, statistical standard errors
(Std.dev) and root mean squared errors (RMSE) are based on 100 replicas, with all replicas for SML converged. For CML, the
number of failed and ignored replica were 1 (T = 250), 1 (T = 500) and 2 (T = 1000). For both SML and CML, diﬀerent
random number seeds were used in all replica, and N = 4096 particles were employed for SML. Monte Carlo Standard errors
(MC.Std.dev) are calculated from 10 repeated estimations to a simulated data set with diﬀerent random number seeds. τSM L
and τCM L denote the mean (clock-)time of evaluating a (quasi-)log-likelihood function on a Dell Latitude E6510 laptop with
an Intel Core i7 CPU 1.73 GHz CPU with 8 cores running Linux.

Table 1: Simulation study for the model (9-11), monthly data experiment.

17

4.2 Weekly data experiment

Our second simulation study involves prices at an equivalent weekly frequency and true parameters similar

to those obtained for export prices from Norway on fresh farmed Atlantic Salmon3. The design of the

experiment is otherwise equal to the monthly data experiment and the results are provided in Table 2. As in

the previous experiment, SML appear to work satisfactory, with small biases for the larger sample sizes and

RMSEs decaying as T −1/2 or faster. In line with what was found for the monthly data experiment, it is seen

that the temporal dependence parameters ρ and δ are estimated with good precision in all cases, whereas

the location and scale parameters a and b require more data to be accurately determined.

CML has again mostly larger RMSEs than SML in all cases except for b in the T = 250 case, for which

SML results are dominated by a single extreme replica.

In particular we see a non-vanishing bias for a-

parameter for CML that again mirrors what was found by Michaelides and Ng (2000). The diﬀerences in

computing time and Monte Carlo standard errors are also consistent with the previous experiment, with

both higher computational cost and higher Monte Carlo standard errors associated with CML. We therefore

conclude that also in this setup, SML is the better overall estimator.

4.3 Yearly data experiment

Earlier applications of the storage model (Deaton and Laroque, 1995, 1996) have relied on a yearly period

modeling and yearly data. This leads to smaller supply shock autocorrelation, and therefore provide a

situation that is somewhat less in favor of SML. In the yearly data experiment we consider sample sizes of

T = 100, 200 years of data and true dynamics equal to the parameters obtained by Deaton and Laroque

(1996) for Tin using yearly data between 1900 and 1987. Otherwise the design of the experiment is equal to

the monthly data experiment and results are presented in Table 3.

Also here it is seen that SML works very satisfactory with small biases throughout. Moreover, the Monte

Carlo standard errors are substantially smaller than the statistical standard errors. In this situation, it is

seen that the biases for CML vanishes faster than the monthly and weekly situations. Still for CML, we see

larger biases and RMSEs than for SML. Taking into account that CML is more computationally costly than

SML, we conclude that SML is the preferred estimator, even for data at a yearly frequency.

4.4 Eﬀects of estimation bias on implied price characteristics

Table 4 investigates the eﬀects of the estimation bias on implied price characteristics for the monthly, weekly

and yearly data experiments. Using the true and “biased” parameters from the estimations in table 1 and 2,

3The real data used are between week 1, 1995 and week 39, 2012. The prices were normalized to mean 1. Prices can be

found at http://www.nosclearing.com/

18

True Parameters

SML

CML

SML

CML

SML

CML

a
1.65

ρ
0.99

b
-0.09
T = 250, τSM L = 2.5s, τCM L = 7.3s
-0.1473
1.2646
1.2669
0.0008

-0.1401
0.3970
0.4191
0.0044

-0.0079
0.0294
0.0303
0.0002

Bias
Std.dev.
RMSE
MC.Std.dev

Bias
Std.dev.
RMSE
MC.Std.dev

0.1668
0.6931
0.7095
0.0428

-0.0179
0.0268
0.0321
0.0011

-0.0966
0.1865
0.2092
0.0028
T = 500, τSM L = 2.9s, τCM L = 12.1s
-0.0107
0.0470
0.0480
0.0005

-0.0024
0.2189
0.2178
0.0010

-0.0021
0.0068
0.0071
0.0001

Bias
Std.dev.
RMSE
MC.Std.dev

Bias
Std.dev.
RMSE
MC.Std.dev

0.2663
0.5712
0.6277
0.2099

-0.0174
0.0562
0.0585
0.0046

-0.1654
0.6448
0.6626
0.0168
T = 1000, τSM L = 3.9s, τCM L = 19.9s
-0.0015
0.0123
0.0123
0.0002

-0.0083
0.1590
0.1584
0.0057

-0.0006
0.0023
0.0024
1.5e-5

Bias
Std.dev.
RMSE
MC.Std.dev

Bias
Std.dev.
RMSE
MC.Std.dev

-0.0060
0.0063
0.0087
0.0063

0.4690
0.5164
0.6956
0.3497

-0.0754
0.1470
0.1646
0.0434

δ
0.0035

0.0004
0.0025
0.0026
2.8e-5

0.0024
0.0033
0.0041
0.0004

0.0003
0.0013
0.0013
3.1e-5

0.0025
0.0030
0.0039
0.0008

-8.0e-7
0.0007
0.0007
1.3e-5

0.0025
0.0023
0.0034
0.0004

(q-)log-like

0.0096

0.1237

0.0120

0.9993

0.0325

0.5395

Note: True parameters indicated in the uppermost row and r corresponding to weekly data. Bias, statistical standard errors
(Std.dev) and root mean squared errors (RMSE) are based on 100 replicas, with all replicas for SML converged. For both SML
and CML, diﬀerent random number seeds were used in all replica, and N = 4096 particles were employed for SML. Monte
Carlo Standard errors (MC.Std.dev) are calculated from 10 repeated estimations to a simulated data set with diﬀerent random
number seeds. τSM L and τCM L denote the mean (clock-)time of evaluating a (quasi-)log-likelihood function on a Dell Latitude
E6510 laptop with an Intel Core i7 CPU 1.73 GHz CPU with 8 cores running Linux.

Table 2: Simulation study for the model (9-11), weekly data experiment.

19

True Parameters

SML

CML

SML

CML

ρ
0.918

a
0.223

b
-0.038
T = 100, τSM L = 2.1s, τCM L = 4.0s
-0.0008
0.0063
0.0063
3.4e-5

-0.0047
0.0338
0.0339
0.0003

0.0007
0.0391
0.0389
0.0002

Bias
Std.dev.
RMSE
MC.Std.dev

Bias
Std.dev.
RMSE
MC.Std.dev

0.0258
0.0445
0.0512
0.0015

-0.0197
0.0338
0.0390
0.0021

-0.0200
0.0122
0.0234
0.0012
T = 200, τSM L = 2.3s, τCM L = 5.3s
-0.0011
0.0044
0.0045
3.2e-5

-0.0026
0.0219
0.0220
0.0002

0.0008
0.0293
0.0292
0.0010

Bias
Std.dev.
RMSE
MC.Std.dev

Bias
Std.dev.
RMSE
MC.Std.dev

-0.0157
0.0257
0.0300
0.0006

0.0252
0.0359
0.0437
0.0016

-0.0181
0.0071
0.0195
0.0002

δ
0.046

0.0054
0.0234
0.0238
0.0001

-0.0040
0.0318
0.0319
0.0008

-0.0020
0.0144
0.0145
0.0001

-0.0091
0.0140
0.0166
0.0009

(q-)log-like

0.0117

0.0469

0.0447

0.1350

Note: True parameters indicated in the uppermost row and r corresponding to yearly data. Bias, statistical standard errors
(Std.dev) and root mean squared errors (RMSE) are based on 100 replicas, with all replicas for SML converged. For both SML
and CML, diﬀerent random number seeds were used in all replica, and N = 4096 particles were employed for SML. Monte
Carlo Standard errors (MC.Std.dev) are calculated from 10 repeated estimations to a simulated data set with diﬀerent random
number seeds. τSM L and τCM L denote the mean time of evaluating a (quasi-)log-likelihood function on a 2016 imac with an
3.1 Ghz Intel Core i5. Only a single tread was used.

Table 3: Simulation study for the model (9-11), yearly data experiment.

20

E (pt)

SD (pt) SK (pt) KU (pt) AC1 (pt) P(stock out)

Monthly data experiment

True dynamics

0.8583

0.6752

2.3978

10.6107

0.9677

0.0423

0.6641
T=250, SML
0.8047
T=500, SML
0.8517
T=1000, SML
0.7440
T=250, CML
T=500, CML
0.8396
T=1000, CML 0.8423

0.5045
0.6056
0.6473
0.5582
0.6599
0.7309

3.2359
2.5399
2.3629
3.2888
3.2027
3.2759

19.2188
11.8934
10.4207
20.0495
18.5543
18.7207

0.9448
0.9608
0.9653
0.9294
0.9397
0.9492

0.0298
0.0412
0.0454
0.0359
0.0377
0.0333

Weekly data experiment

True dynamics

1.2018

0.4022

1.0890

4.3193

0.9909

0.0119

1.0349
T=250, SML
1.1989
T=500, SML
1.1992
T=1000, SML
1.3514
T=250, CML
T=500, CML
1.3727
T=1000, CML 1.5070

0.3683
0.3820
0.3888
0.3570
0.4125
0.5417

1.8842
1.1783
1.1095
1.4318
1.6494
1.2826

9.3327
4.6949
4.4118
6.1169
7.4317
4.9978

0.9817
0.9887
0.9902
0.9714
0.9716
0.9848

0.0132
0.0145
0.0111
0.0261
0.0257
0.0244

Yearly data experiment

True dynamics

0.1922

0.0875

0.4582

2.8062

0.9063

0.0733

T=100, SML
T=200, SML
T=100, CML
T=200, CML

0.1941
0.1913
0.1928
0.1921

0.0871
0.0876
0.1058
0.1041

0.4462
0.4826
0.8067
0.8074

2.8027
2.8363
3.3511
3.3503

0.8991
0.9050
0.8949
0.8997

0.0846
0.0771
0.0688
0.0717

Note: All ﬁgures are calculated from one million periods of simulated dynamics. “True dynamics” are calculated from dynamics
with parameters equal to those of the “True parameters” rows of Tables 1, 2 and 3. The remaining ﬁgures are calculated from
dynamics with parameters equal to “True parameters” plus “Bias” taken from Tables 1, 2 and 3. SD denotes standard deviations,
SK skewness, KU kurtosis, AC1 ﬁrst order autocorrelation and P(stock out) denotes the marginal probability of being in a
stock-out state, i.e. P (xt) = f (xt, zt).

Table 4: The eﬀect of estimation bias on price characteristics

21

we calculate and compare the mean, standard deviation, skewness, kurtosis and ﬁrst order autocorrelation of

model simulated prices. We also calculate the marginal probability a stock out as implied by the estimated

models. This is done using both the monthly and weekly data experiment parameters. What is immediately

clear from the table is that for the SML estimator, bias declines over all statistics as more price observations

become available. This is not so for the CML estimator. For some of the statistics, for instance the mean

in the weekly data experiment or the kurtosis in the monthly data experiment, the bias increases as more

data becomes available. Overall the SML estimator displays favorable large sample properties and improved

precision over the CML estimator. We also note that the CML estimator consistently underestimates the

price autocorrelation. This is relevant as low implied price autocorrelation has been pointed to as a weakness

of the competitive storage model (Deaton and Laroque, 1996; Caﬁero et al., 2011).

4.5 Robustness to misspeciﬁcation

A reason for opting for composite likelihood methods is that in many cases such methods may be more

robust to model misspeciﬁcation than methods based on the full likelihood. See e.g. Cox and Reid (2004) or

Varin et al. (2011), section 4.3 for a detailed discussion of the robustness properties associated with composite

likelihood. To compare CML and SML under misspeciﬁcation, we carry out a Monte Carlo study where in the

data generating process, ηt in (9) is taken to be a scaled t4-distribution with unit variance. The estimation

methodology is left unchanged, and we consider the an identical setup as for the yearly data experiment

discussed in Section 4.3.

The results are presented in Table 5. It is seen that the changes in performance under misspeciﬁcation is

relatively minor for both methods (i.e. relative to Table 3). In particular, we see that the relative performance

between SML and CML is largely unchanged when misspeciﬁed ηt is added to the data generating process.

Thus, at least for this situation and a heavy-tailed form of misspeciﬁcation, the beneﬁts from employing the

potentially more robust CML are too small to outweigh the statistical accuracy stemming from using a full

likelihood speciﬁcation as in SML.

5 Empirical Application

To illustrate our estimation procedure with real data we apply it to monthly frequency observations on natural

gas spot prices. To compare model ﬁt and precision of parameter estimates we apply both our ﬁlter-based

estimator and the composite likelihood estimator. One way to empirically asses the relevance of speculative

storage model is to compare the storage model ﬁt to a benchmark linear AR(1) model. It is straightforward

to show that zero storage (which might occur if the cost of storage is suﬃciently large) in the storage model

22

True parameters

SML

CML

SML

CML

a
0.223

ρ
0.918
T = 100, τSM L = 2.1s, τCM L = 4.0s
-0.0095
0.0375
0.0385

Bias
Std.dev.
RMSE

0.0041
0.0355
0.0356

0.0237
0.0427
0.0487

Bias
Std.dev.
RMSE

-0.0362
0.0442
0.0570
T = 200, τSM L = 2.3s, τCM L = 5.3s
0.0005
0.0207
0.0206

Bias
Std.dev.
RMSE

0.0015
0.0265
0.0265

b
-0.038

-0.0017
0.0085
0.0086

-0.0220
0.0186
0.0288

0.0005
0.0048
0.0048

δ
0.046

-0.0029
0.0206
0.0207

-0.0072
0.0284
0.0292

-0.0049
0.0142
0.0149

Bias
Std.dev.
RMSE

-0.0218
0.0333
0.0396

0.0218
0.0328
0.0392

-0.0170
0.0127
0.0212

-0.0034
0.0201
0.0203

Note: True parameters indicated in the uppermost row and r corresponding to yearly data. Bias, statistical standard errors
(Std.dev) and root mean squared errors (RMSE) are based on 100 replicas, with all replicas for SML converged. For both SML
and CML, diﬀerent random number seeds were used in all replica, and N = 4096 particles were employed for SML. τSM L and
τCM L denote the mean time of evaluating a (quasi-)log-likelihood function on a 2016 imac with an 3.1 Ghz Intel Core i5. Only
a single tread was used.

Table 5: Simulation study for the model (9-11) subject to misspeciﬁcation in ηt.

implies that prices evolve as

pt+1 = a + ρ(pt − a) + b(cid:15)t+1, (cid:15)t ∼ i.i.d. N (0, 1).

(16)

Rejecting the linear AR(1) model in favor of the storage model does not imply that the storage model is

the “true” model, but provides support for characteristics consistent with speculative storage over a model

where prices are explained by a linear ﬁrst order process. One reason why a linear AR(1) model might be

rejected in favor of the storage model is stochastic volatility in the data. To investigate stochastic volatility

we also estimate the linear AR(1) model with GARCH(1,1) errors. Finally, given the storage model predicts

a two-state regime switching type price dynamics, we estimate a Markov switching (MS) model, where the

AR(1) model in equation 16 is allowed to change between two regimes as determined by a latent two-state

Markov process with constant transition probabilities.

Comparing the ﬁt of the competitive storage model to reduced form time-series models is a strong test.

The reduced form models (such as the linear AR(1) model or the GARCH models) are speciﬁcally designed

to account for speciﬁc features of the data (such as ﬁrst-order autocorrelation or ARCH eﬀects). The value

of such a comparison is that we can identify price features that the model is able to account for, and where

improvements are needed. Finally, we note that all models considered are estimated using commodity price

23

data only.

5.1 Application to Natural Gas at Henry Hub

The natural gas price in this analysis is the spot price at the Henry Hub terminal in Louisiana. Prices are

denoted in US$ per thousand cubic meters of gas, and covers the period 1991 M1 to 2012 M6. Prices can be

obtained from http://www.imf.org/external/np/res/commod/index.aspx. Natural gas prices are inﬂuenced

by a range of factors such as the price of oil, weather, seasonality in demand, shut-in production and storage

(Brown and Yucel, 2008). In regards to storage, inventories play an important role in smoothing production

and balancing demand–supply conditions. The release of information on inventory levels is known to generate

considerable volatility in prices (Mu, 2007). Chiou Wei and Zhu (2006) further demonstrate that the conve-

nience yield (measured as the diﬀerence between the spot and forward natural gas price) is negatively related

to natural gas inventory levels. This suggests that abnormally high prices are related to low inventories, as

implied by the storage model.

The storage model is arguably too simple to capture the full complexities of the real market. Eﬀects

related to energy substitution (the oil price eﬀect) and seasonality in demand is not speciﬁcally modeled, and

is likely captured by the supply shocks. However, if speculative storage eﬀects are present to any substantial

degree, inference should favor the storage model over the linear AR(1) model (equation 16). An added beneﬁt

of analyzing the natural gas market is that detailed information on natural gas storage is available. This

means we can compare the model implied ﬁltered storage, derived using only the price data, to actual storage

levels. A strong correspondence between these series will provide support for the relevance of the storage

model and the estimation procedure.

Prior to estimation the price data was normalized to unit mean. Parameter estimates are provided in

Table 6. We see that SML and CML produce similar results, but that SML has smaller Monte Carlo standard

errors. The statistical- and Monte Carlo standard error for the real data are largely consistent with those

found in the simulation study reported in Table 1, T = 250. Compared to the AR(1) benchmark, the storage

model performs substantially better in terms of model ﬁt. This implies that not all of the model ﬁt comes

from the exogenous latent AR(1) shock; the economic model appears relevant in terms of explaining observed

characteristics.

Adding GARCH(1,1) errors to the AR(1) model gives an improved ﬁt with a log-likelihood of 148.79,

where the AR(1)-GARCH(1,1) model has one more parameter than the storage model. However, the log-

likelihood is still below the ﬁt of the storage model. Due to the non-nested nature of the AR(1)-GARCH(1,1)

model and the storage model (9-11), we also computed the log-likelihood ratios of the storage model against

24

ρ

a
SML

b

δ

log-likelihood

Estimate
MC Std.Dev
Statistical S.E.

0.968
2.9e-5
0.0265

1.471
4.2e-4
1.457

-0.408
1.8e-4
0.489

0.0212
1.0e-5
0.0083

CML

Estimate
MC Std.Dev
Statistical S.E.

0.963
0.0039
0.0465

2.075
0.237
1.213

-0.599
0.0759
1.360

0.0275
0.0016
0.0102

Benchmark AR(1) model (16)

Estimate
Statistical S.E.

0.950
0.0192

1.021
0.625

-0.188
0.0083

Markov-Switching AR(1) model,
P (Regime 1 next period|Regime 1 current period)=0.951,
P (Regime 1 next period|Regime 2 current period)=0.081

Estimate (Regime 1)
Statistical S.E. (Regime 1)
Estimate (Regime 2)
Statistical S.E. (Regime 2)

0.887
0.0164
0.861
0.0492

0.549
0.014
1.725
0.884

-0.064
0.0048
-0.28
0.022

194.32
4.6e-3

192.19
0.288

65.34

164.09

AR(1)-GARCH(1,1) (cond. variance model: σ2

ρ

a

α0

t = α0 + α1(cid:15)2
α1

t−1 + β1σ2
β1

t−1)

Estimate
Statistical S.E.

0.967
0.0155

0.520
0.396

4.1e-4
2.5e-4

0.046
0.095

0.667
0.044

148.79

Note: Standard errors are based on 100 parametric bootstrap replicas, and MC standard errors are based on ﬁtting the model
to the real data 30 times with diﬀerent random number seeds in the particle ﬁlter. The benchmark AR(1) model was ﬁtted
using maximum likelihood, and statistical standard errors are based on observed Fisher information.

Table 6: Parameter estimates for the natural gas data.

25

the AR(1)-GARCH(1,1) model for the 100 parametric bootstrap replica reported in Table 6 4. We ﬁnd

that the observed log-likelihood ratio (2(194.32 − 148.79) = 91.06) fall between the 61st and 62nd simulated

likelihood ratio when the storage model is the true model, which indicate that the observed log-likelihood

ratio is consistent with the storage model being the “true” model.

The MS-AR(1) produces a ﬁt with log-likelihood of 164.09, which is again substantially below the struc-

tural model while containing four more parameters. To assess this number, we ﬁtted the MS-AR(1) model

to the simulated data under the storage model as described above. We ﬁnd that the observed likelihood

ratio (2(194.32 − 163.02) = 60.46) fall between the 13th and 14th simulated likelihood ratio when the storage

model is the true model, which is again consistent with the storage model being the better model.

Finally, we ﬁtted model (9-11) with iid supply shock (i.e. ρ = 0) to compare with a methodology similar

to that of Caﬁero et al. (2011). We used the same price function solver and conditionally Gaussian transition

densities for the price process so that the iid supply shock model is nested under the general model (9-11).

We obtain a log-likelihood of 190.79 for the iid supply shock model, which correspond to a rejection of the iid

model against the general storage model (9-11) with p-value 0.008. It is also worth noticing that the ﬁtted

price function has a very large negative b = −189, and therefore we regard this model ﬁt as nonsensical.

Figure 1 plots the deseasonalized and detrended observed monthly storage of natural gas against the

median model implied storage. Considering that inference on model storage only uses price data, the two

series’ are similar. There is good correspondence between the series’, especially up to the early 2000’s. In later

years, the variation in storage is larger than what is predicted by the storage model. The relatively strong

similarity between the series’ is reassuring as it suggests that the latent model storage is related to actual

storage. The ﬁgure gives support for the relevance of the storage model and the importance of inventories in

accounting for natural gas price movements.

Finally, to explore the eﬀect of the non-negativity constraint on price dynamics we plot the price series

(unit mean) and the ﬁltered model implied probability of a stock-out. This is shown in Figure 2. The

probability of a stock-out is the probability that the next period stock falls below a level where any positive

speculative storage is optimal. The constraint is the source of non-linearity in the model, and as the ﬁgure

shows the constraint is likely to be binding in certain periods. These are periods associated with abnormally

high prices. Looking at the storage data in Figure 1 we observe that periods with higher probability of a

stock-out coincide with periods of low storage. The ﬁgure highlights the importance and relevance of the

non-negativity constraint in accounting for characteristics in the price data.

26

Note: Observed storage is the deseasonalized and detrended U.S. Natural Gas Underground Storage Volume (MMcf)
(http://www.eia.gov/naturalgas/). The series is deseasonalized using trigonometric functions with annual frequency. A lin-
ear trend is used for detrending. All series’ are normalized to have mean zero and unit standard deviation.

Figure 1: Observed and model implied storage.

Note: The ﬁltered probabilities of stock-out at time t are calculated as P rob(f (xt, zt) = P (xt)|p1, . . . , pt) using particle ﬁlter
output.

Figure 2: Natural gas price and model suggested probability of stock-out

27

1991199219931994199519961997199819992000200120022003200420052006200720082009201020112012−4−3−2−101234  Observed storageStorage model implied storage199119921993199419951996199719981999200020012002200320042005200620072008200920102011201200,511,522,533,54Price  00.1250.250.3750.50.6250.750.8751ProbabilityPrice (mean=1, left axis)Probability of stock−out (right axis)Natural gas data
Storage model AR(1)
1.02
0.61
-0.05
0.03
0.95
0.90
0.00

0.86
0.67
2.26
6.46
0.96
0.94
0.40

Data
1
0.61
1.29
1.68
0.95
0.90
0.47

E(pt)
SD(pt)
SK(pt)
EKU (pt)
AC1(pt)
AC2(pt)
AC1(|∆pt|)

Note: The statistics for the estimated models are calculated over simulated realizations of length 100,000. SD denotes
standard deviations, SK skewness, EKU excess kurtosis, ACk the k-th order autocorrelation and in particular AC1(|∆pt|)
denotes the ﬁrst order autocorrelation of the absolute price returns as a measure of volatility clustering.

Table 7: Comparison of statistics from data and estimated models.

Mean
Standard Deviation
Skewness
Excess Kurtosis
First Order Autocorrelation
Jarque-Bera p-value
Kolmogorv-Smirnoﬀ p-value
Ljung-Box test (lag=20) p-value
Engle ARCH test p-value

Residuals from
Natural Gas Data
AR(1)
storage
model
model
-0.0001
0.0175
1.0001
0.9742
-0.2546
0.5999
7.2735
0.5668
0.1877
0.0252
0.0026 <0.0010
0.3493 <0.0001
0.0143
0.0014
0.7648 <0.0001

Note: The residuals for the storage model are calculated by a standard Gaussian quantile transform applied to the
generalized residuals (13). We expect that the residuals are approximately iid standard Gaussian if the model
speciﬁcation is correct.

Table 8: Diagnostics for residuals of the real data set.

5.2 Diagnostics

As a ﬁrst set of diagnostics, we report in table 7 several statistics of the actual natural gas price along with

the corresponding statistics for long realizations of the ﬁtted storage and linear AR(1) models. It is seen

that the AR(1) model captures better the mean, standard deviation and autocorrelation of the price series,

whereas the storage model does a better job at capturing skewness, excess kurtosis and the autocorrelation of

absolute price returns. The latter is used as a measure of volatility clustering, and it is seen that the storage

model matches the data rather well in this sense. The large standard deviation of the storage model for the

salmon data stems from the fact that this model produces infrequent price spikes that are substantially larger

than what is seen in the data.

As a further diagnostic of model ﬁts, we perform a battery of tests on the residuals. The results are given

in Table 8. This diagnostics also helps highlight what features of the data the storage model addresses. Again,

4I.e. (9-11) with parameters found for SML is the data-generating process.

28

Mx,1 Mx,2 Mz
128
256
256
128
128
128
64
256
256
64
128
128

ρ
0.9670
0.9672
0.9671
0.9666

a
1.5257
1.5106
1.4781
1.4819

b
-0.4028
-0.4103
-0.3994
-0.4003

δ
0.0217
0.0217
0.0211
0.0217

log-likelihood
194.2100
194.3057
194.2546
194.3273

Note: The ﬁgures in the table were obtained by estimating the parameters using the real data sets. The random number seed
was kept ﬁxed, and it is seen that errors associated with price function solver discretization are small relative to the statistical
standard deviations.

Table 9: Maximum likelihood estimates and optimal log-likelihoods for diﬀerent resolutions of the rational
expectation solver grids for the real data.

it is seen that the storage model in general does a better job accounting for higher moment characteristics.

The storage model predicts non-linear ﬁrst-order Markov prices. These features will manifest in the higher

moments of prices, and it is not surprising that these are the characteristics the storage model best describes.

Not surprising, the AR(1) benchmark does a better job when it comes to the unconditional mean price and

ﬁrst order autocorrelation, while the storage model does better in terms of excess kurtosis.

In addition,

the storage model better accounts for ARCH eﬀects. In terms of the Gaussian structure of our parametric

speciﬁcation (equations (9-11)), the diagnostic results indicate that residuals are close to normal.

To assess whether we are using a suﬃciently ﬁne grid, we estimate the model on the natural gas and

salmon data using diﬀerent values of Mx,1, Mx,2 and Mz. As was demonstrated by (Caﬁero et al., 2011)

under the ρ = 0 model, using too coarse grids can bias parameter estimates. Parameter estimates obtained

while keeping the random number seed in the particle ﬁlter ﬁxed are provided in Table 9. The settings of

the lowermost rows in each panel of the table are the ones used for the above estimation on natural gas

and salmon prices. For the natural gas data, diﬀerences in parameter estimates for ﬁner grids are minor

comparing to statistical standard errors from the SML panel in Table 6. All in all we conclude that we are

using suﬃciently ﬁne grids for these ranges of parameters.

6 Conclusion

We propose a particle ﬁlter estimator for the competitive storage model with temporal supply shock depen-

dence when only price data is available for estimation. The particle ﬁlter estimator utilizes information in

the conditional distribution of prices when temporal dependence is present in the shocks. This is contrary

to the composite pseudo maximum likelihood estimator of Deaton and Laroque (1995, 1996), which only

utilizes the marginal state distribution to arrive at predictive price moments. To our knowledge this is the

ﬁrst attempt at using particle ﬁlter methods to estimate the competitive storage model. Our results suggest

that the relative simplicity and low-dimensional nature of the partial equilibrium storage model makes it

particularly suitable for particle ﬁlter estimators.

29

We demonstrate through simulation experiments that our particle ﬁlter estimator does a better job in

terms of both the bias and precision of the structural parameter estimates compared to the composite

estimator. Furthermore, our estimator is less computationally demanding than our Monte Carlo based

implementation of the Deaton and Laroque composite maximum likelihood estimator, and also is more

numerically stable.

In addition, simulation evidence indicates that our model has favorable large sample

properties where bias diminishes when more price data becomes available. The composite maximum likelihood

estimator does not display this same general reduction in bias as sample size increases.

As an application and demonstration of the estimator we estimate the storage model to natural gas prices.

As a benchmark, we compare the storage model to a linear AR(1) model, a GARCH(1,1) model and a two-

state Markov Switching AR(1) model. The linear AR(1) model can be thought of as the price representation

that would occur with zero speculative storage in the storage model. The comparison to reduced form

models allows us to identify what features of prices the storage model is able to account for, and where

improvements are needed. For the natural gas market, the storage model performs better than all reduced

form time-series models considered. This suggests that the non-linearity in the model, arising from the

non-negativity constraint on storage, is relevant to account for natural gas price characteristics. As support

for the relevance of the storage model in the natural gas market, we ﬁnd relatively strong correspondence

between observed and model implied storage. Diagnostics show that the storage model addresses features in

the higher moments of prices, speciﬁcally linked to excess kurtosis and ARCH eﬀects.

The particle ﬁlter estimator appears superior to the composite maximum likelihood estimator for the

type of model setting analyzed in this paper. The storage model is used to investigate eﬀects of various

commodity market policies. The value of such policy evaluations depend crucially on the validity of the

structural parameters chosen for the analysis. When price data is the only reliable data available to infer

structural parameters, and shocks are suspected to have temporal dependence, the particle ﬁlter estimator

should be applied to eﬃciently utilize the sparse data.

References

Andrieu, C., A. Doucet, and R. Holenstein (2010). Particle Markov chain Monte Carlo methods. Journal of

the Royal Statistical Society: Series B (Statistical Methodology) 72 (3), 269–342.

Arseneau, D. M. and S. Leduc (2013). Commodity price movements in a general equilibrium model of storage.

IMF Economic Review 61 (1), 199–224.

Aruoba, S. B., J. Fernandez-Villaverde, and J. F. Rubio-Ramirez (2006). Comparing solution methods for

dynamic equilibrium economies. Journal of Economic Dynamics and Control 30 (12), 2477 – 2508.

Brennan, D. (2003). Price dynamics in the Bangladesh rice market:

implications for public intervention.

Agricultural Economics 29 (1), 15–25.

Brown, S. and M. Yucel (2008). What drives natural gas prices? Energy Journal 29 (2), 45.

30

Caﬁero, C., E. Bobenrieth H, J. Bobenrieth H, and B. Wright (2011). The empirical relevance of the

competitive storage model. Journal of Econometrics 162 (1), 44–54.

Caﬁero, C., E. S. Bobenrieth H., J. R. Bobenrieth H., and B. D. Wright (2015). Maximum likelihood
estimation of the standard commodity storage model: Evidence from sugar prices. American Journal of
Agricultural Economics 97 (1), 122–136.

Caﬁero, C., B. Wright, A. Sarris, D. Hallam, et al. (2006). Is the storage model a closed empirical issue?
the empirical ability of the storage model to explain price dynamics. Agricultural Commodity Markets and
Trade. New Approaches to Analyzing Market Structure and Instability 162, 89–114.

Cappe, O., S. Godsill, and E. Moulines (2007). An overview of existing methods and recent advances in

sequential monte carlo. Proceedings of the IEEE 95 (5), 899 –924.

Chambers, M. J. and R. E. Bailey (1996). A theory of commodity price ﬂuctuations. Journal of Political

Economy 104 (5), 924–957.

Chiou Wei, S. and Z. Zhu (2006). Commodity convenience yield and risk premium determination: The case

of the us natural gas market. Energy Economics 28 (4), 523–534.

Cox, D. R. and N. Reid (2004). A note on pseudolikelihood constructed from marginal densities.

Biometrika 91 (3), 729.

Deaton, A. and G. Laroque (1992). On the behaviour of commodity prices. The Review of Economic

Studies 59 (1), 1–23.

Deaton, A. and G. Laroque (1995). Estimating a nonlinear rational expectations commodity price model

with unobservable state variables. Journal of Applied Econometrics 10, S9–S40.

Deaton, A. and G. Laroque (1996). Competitive storage and commodity price dynamics. The Journal of

Political Economy 104(5), 896–923.

DeJong, D. N., R. Liesenfeld, G. V. Moura, J.-F. Richard, and H. Dharmarajan (2013). Eﬃcient likelihood

evaluation of state-space representations. The Review of Economic Studies 80 (2), 538–567.

Durbin, J. and S. J. Koopman (1997). Monte Carlo maximum likelihood estimation for non-Gaussian state

space models. Biometrika 84 (3), 669–684.

Durham, G. B. (2006). Monte Carlo methods for estimating, smoothing, and ﬁltering one and two-factor

stochastic volatility models. Journal of Econometrics 133, 273–305.

Eraker, B. (2001). MCMC analysis of diﬀusion models with application to ﬁnance. Journal of Business and

Economic Statistics 19, 177–191.

Fernandez-Villaverde, J. and J. F. Rubio-Ramirez (2007). Estimating macroeconomic models: A likelihood

approach. Review of Economic Studies 74 (4), 1059–1087.

Funke, N., Y. Miao, and W. Wu (2011). Reviving the competitive storage model: A holistic approach to

food commodity prices. IMF Working Papers 11/64.

Geman, H. and W. O. Smith (2013). Theory of storage, inventory and volatility in the LME base metals.

Resources Policy 38 (1), 18–28.

Gordon, N., D. Salmond, and A. Smith (1993). Novel approach to nonlinear/non-Gaussian Bayesian state

estimation. Radar and Signal Processing, IEE Proceedings F 140 (2), 107 –113.

Gouel, C. (2013a). Comparing numerical methods for solving the competitive storage model. Computational

Economics 41 (2), 267–295.

Gouel, C. (2013b). Optimal food price stabilisation policy. European Economic Review 57, 118–134.

31

Gourieroux, C., A. Monfort, and A. Trognon (1984). Pseudo maximum likelihood methods: Theory. Econo-

metrica 52 (3), pp. 681–700.

Kaldor, N. (1939). Speculation and economic stability. The Review of Economic Studies 7 (1), pp. 1–27.

Kleppe, T. S. and H. J. Skaug (2016). Bandwidth selection in pre-smoothed particle ﬁlters. Statistics and

Computing 26 (5), 1009–1024.

Liesenfeld, R. and J.-F. Richard (2003). Univariate and multivariate stochastic volatility models: estimation

and diagnostics. Journal of Empirical Finance 10, 505–531.

Lindsay, B. G. (1988). Composite likelihood methods. Contemporary Mathematics 80, 221–239.

Malik, S. and M. K. Pitt (2011). Particle ﬁlters for continuous likelihood evaluation and maximisation.

Journal of Econometrics 165 (2), 190 – 209.

Michaelides, A. and S. Ng (2000). Estimating the rational expectations model of speculative storage: a Monte

Carlo comparison of three simulation estimators. Journal of Econometrics 96, 231–66.

Miranda, M. (1997). Numerical strategies for solving the nonlinear rational expectations commodity market

model. Computational Economics 11 (1-2), 71–87.

Miranda, M. J. and J. W. Glauber (1993). Estimation of dynamic nonlinear rational expectations models
of primary commodity markets with private and government stockholding. The Review of Economics and
Statistics 75 (3), 463–470.

Miranda, M. J. and P. G. Helmberger (1988). The eﬀects of commodity price stabilization programs. American

Economic Review 78 (1), 46–58.

Miranda, M. J. and X. Rui (1996). An empirical reassessment of the commodity storage model. Technical

report, Mimeo, Department of Agricultural Economics, Ohio State University.

Mitraille, S. and H. Thille (2009). Monopoly behaviour with speculative storage. Journal of Economic

Dynamics and Control 33 (7), 1451–1468.

Mu, X. (2007). Weather, storage, and natural gas price dynamics: Fundamentals and volatility. Energy

Economics 29 (1), 46–63.

Ng, S. (1996). Looking for evidence of speculative stockholdings in commodity markets. Journal of Economic

Dynamics and Control 20(1-3), 123–43.

Ng, S. and F. J. Ruge-Murcia (2000). Explaining the persistence of commodity prices. Computational

Economics 16 (1-2), 149–171.

Osborne, T. (2004). Market news in commodity price theory: Application to the Ethiopian grain market.

The Review of Economic Studies 71 (1), 133–164.

Pitt, M. K., R. dos Santos Silva, P. Giordani, and R. Kohn (2012). On some properties of Markov chain
Monte Carlo simulation methods based on the particle ﬁlter. Journal of Econometrics 171 (2), 134 – 151.

Richard, J.-F. and W. Zhang (2007). Eﬃcient high-dimensional importance sampling. Journal of Economet-

rics 127 (2), 1385–1411.

Shephard, N. and M. K. Pitt (1997). Likelihood analysis of non-Gaussian measurement time series.

Biometrika 84, 653–667.

Silvermann, B. W. (1986). Density Estimation for Statistics and Data Analysis. New York: Chapman and

Hall.

Tauchen, G. (1986). Finite state Markov-chain approximations to univariate and vector autoregressions.

Economics Letters 20 (2), 177 – 181.

32

Varin, C., N. Reid, and D. Firth (2011). An overview of composite likelihood methods. Statistica Sinica 21 (1),

5–42.

Varin, C. and P. Vidoni (2008). Pairwise likelihood inference for general state space models. Econometric

Reviews 28 (1-3), 170–185.

Williams, J. B. (1936). Speculation and the carryover. The Quarterly Journal of Economics 50 (3), 436–455.

Working, H. (1949). The theory of the price of storage. American Economic Review 39, 1254–1262.

Wright, B. and J. Williams (1982). The economic role of commodity storage. The Economic Journal 92,

596–614.

A Fourier transform-based continuous resampling routine

Suppose we wish to sample from a univariate Gaussian mixture on the form

π(z) =

N
(cid:88)

j=1

w(j)N (z|µ(j), σ2).

Then the practical fast Fourier transform routine consist of the following steps:

• Grid: Find the mean and standard deviation of π(z) and initiate a ng-point regular grid containing say

the mean ± 8 standard deviations. We set ng to 1024 in all computations presented in this paper.

• PDF: As the variance in each component of π(z) is equal, the PDF may be computed using fast Fourier

transform methods on the regular grid as explained thoroughly in Silvermann (1986), section 3.5 (with

the modiﬁcation that each particle weight is now w(j) and not 1/n).

• CDF: Compute the cumulative distribution function (CDF) of the approximate probability density

function on the same grid using a mid-point rule for each grid point.

• Fast inversion: Sample approximate random variables from π(z) based on stratiﬁed uniforms using the

CDF-inversion algorithm provided in Appendix A.3 of Malik and Pitt (2011).

The total operation count of this algorithm is O(N + ng log2(ng)) and thus is it linear in complexity in the

number of particles retained also for this form of continuous sampling with ﬁxed ng. However, it is worth

noticing that to obtain the asymptotically correct random draws (i.e. exact quantiles corresponding to the

stratiﬁed uniform random numbers) as N → ∞, ng must also grow, e.g. as O(N ). Moreover, the area covered

by the grid must also grow, but at a slower rate, e.g. O(log(N )).

33

