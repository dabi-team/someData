6D Object Pose Estimation with Depth Images:
A Seamless Approach for Robotic Interaction and Augmented Reality

David Joseph Tan, Nassir Navab, Federico Tombari
Technische Universit¨at M¨unchen

7
1
0
2

p
e
S
5

]

V
C
.
s
c
[

1
v
9
5
4
1
0
.
9
0
7
1
:
v
i
X
r
a

Figure 1: Results of the framework on different types of objects – from simple to complex geometric structure – on challeng-
ing scenarios.

Abstract

1. Introduction

To determine the 3D orientation and 3D location of ob-
jects in the surroundings of a camera mounted on a robot
or mobile device, we developed two powerful algorithms in
object detection and temporal tracking that are combined
seamlessly for robotic perception and interaction as well as
Augmented Reality (AR). A separate evaluation of, respec-
tively, the object detection and the temporal tracker demon-
strates the important stride in research as well as the impact
on industrial robotic applications and AR. When evaluated
on a standard dataset, the detector produced the highest f1-
score with a large margin while the tracker generated the
best accuracy at a very low latency of approximately 2 ms
per frame with one CPU core – both algorithms outperform-
ing the state of the art. When combined, we achieve a pow-
erful framework that is robust to handle multiple instances
of the same object under occlusion and clutter while attain-
ing real-time performance. Aiming at stepping beyond the
simple scenarios used by current systems, often constrained
by having a single object in absence of clutter, averting to
touch the object to prevent close-range partial occlusion,
selecting brightly colored objects to easily segment them in-
dividually or assuming that the object has simple geometric
structure, we demonstrate the capacity to handle challeng-
ing cases under clutter, partial occlusion and varying light-
ing conditions with objects of different shapes and sizes.

The innovations in robotics have undergone numerous
improvements to maneuver over rough terrains, to avoid
collisions and even to take ﬂight. However, a huge limi-
tation for these autonomous agents is still represented by
the lack of perception of the world around them. Without
perception, modern robots are incapable of autonomously
interacting with the objects present in their surroundings.
Hence, although the advances in hardware to stabilize the
robot’s movement have reached remarkable achievements,
the perception software still falls behind.

Most robotic perception tasks aim at providing an au-
tonomous agent with the skill of either localizing itself in
the surrounding environment, or interacting with nearby ob-
jects through grasping and manipulation. Our work has in-
vestigated on the latter skill, where we develop robotic per-
ception techniques for real-time object detection and track-
ing of multiple 3D objects from RGB-D data acquired using
consumer depth cameras.
In particular, we have focused
on industrial robotic applications, where intelligent robots
have to localize, grasp, assemble and relocate objects on
the production line, for tasks such as pick-and-place and
bin picking. Looking from a wider perspective, the solution
we developed is fundamental for most of the envisioned ap-
plications concerning service and personal robotics, where
robotic assistants help out people for daily tasks in their do-
mestic environments. In addition, it is also fundamental to
AR applications, where the pose of several objects has to be

1

BCDERGB ImageDepth + ResultsFA 
 
 
 
 
 
efﬁciently estimated while the user interacts with them.

Our framework is inspired by the way humans interact
with the objects around them. To be able to interact with
objects in our surrounding, we ﬁrst need to localize each
object in the scene, then keep track of it throughout the pro-
cess. Converting this intuitive idea into an algorithm in-
volves a two-step procedure – object detection and temporal
tracking. These two methods work hand-in-hand such that
the former perceives the object in the scene while the latter
keeps track of the object’s movement during interaction.

2. Seamless Object Detection and Tracking

The goal of the framework is to ﬁnd the objects in the
scene while estimating their orientation and location in the
3D space, then to continuously track them throughout the
following frames. Object detection performs a sliding win-
dow approach to simultaneously ﬁnd the objects in the
scene and estimate their pose. Taking the resulting pose
from the detector as input, the tracker estimates the rela-
tive transformation of the object between two consecutive
frames and temporally relays the pose from one frame to the
next. Intended for autonomous robots, the object detection
and tracking run automatically in order to detect whenever
the object of interest is present and stop tracking when the
object is no longer visible.

Given the 3D CAD model of the object, both algorithms
learn random forests [2] from multiple, synthetically ren-
dered depth images acquired by positioning the camera
around the model. Motivated to perform different tasks, the
detector predicts the rotation and translation parameters for
each region of the sliding window while the tracker pre-
dicts the relative transformation between two consecutive
frames [10, 11, 12].

3. Evaluation

The framework satisﬁes various characteristics that are
required by applications in robotics and AR. These include
(1) the robustness and accuracy to ﬁnd the object in the
scene and to estimate its pose; (2) the efﬁciency to run in
real-time with a minimal computational expense; and, (3)
the cost-effective system requirements.

3.1. Robust and Accurate to Detect and Track

Fundamentally, the goal is to develop two powerful algo-
rithms such that, in the combined approach, each algorithm
can perform their assigned tasks very well.

Robust Detector with 6D Pose Estimate. Evaluated on
the public dataset of [13], the object detection algorithm ac-
quires the best f1-score in Table 1 with a large margin of
18.9% against other methods [1, 4, 5, 6, 7, 13]. We remind
the reader that the f1-score measures the detection rate by

Figure 2: Sample RGB-D frame of the “Milk” sequence
from [13].

incorporating both the precision and recall into one value.
An f1-score of 99.3% then indicates that the detection rate
is almost perfect.

An interesting observation from Table 1 is that all the
other methods [1, 4, 5, 6, 7, 13] have a low f1-score on the
“Milk” sequence. Among them, the highest reached 55.8%
from LineMod [6]. The reason for the low results is because
the authors from [13] added small objects on the object of
interest as shown in Fig. 2.
In effect, this occludes sev-
eral regions of the object of interest and slightly changes its
geometry. Contrary to their results, we can handle these oc-
clusions and achieve an f1-score of 99.3% on this sequence
which is 43.5% higher than the other methods.

Accurate Temporal Tracker. On the other hand, the tem-
poral tracker generates the lowest errors in translation and
rotation in Table 2 on the dataset of [3]. In addition, we also
introduce a version of the tracker for AR [11]. Compared to
[10, 12], the latest version of the temporal tracker [11] aims
at the user experience for AR. Thus, it minimizes jitter by
optimizing through the RGB and depth images to acquire
more accurate pose estimations as evaluated in Table 2.

Combined Performance. Hence, the combined approach
incorporates high detection rates as well as highly accurate
pose. Moving past the public datasets, Fig. 1 demonstrates
the robustness of our algorithm in different types of chal-
lenging scenarios, including clutter, partical occlusions, and
varying lighting conditions. From simple to complex geo-
metrical shapes, the results from Table 2 and Fig. 1 show the
capability of the framework to generalize its performance
for different object shapes and sizes.

3.2. Low Latency and Low Memory Consumption

Table 3 summarizes the detection time and tracking time
with respect to the computational power. Note that, after
detecting the object, only the temporal tracker keeps track
of the object in the subsequent frames. Hence, the latency
of the framework depends on the temporal tracker alone,
which is approximately 2 ms per frame for each object with
a single CPU core. This efﬁciency is a substantial improve-

2

(a) RGB(b) Depth + Our Detection ResultsCoffee Cup

Shampoo

Joystick

Camera

Juice Carton

Milk

LineMod [6]
Point-Pair Features [5]
Coordinate Reg. [1]
Latent Forest [13]
Next-Best-View [4]
Deep Learning [7]

Our Detector

94.2%
86.7%
91.2%
89.1%
93.2%
97.2%

99.8%

92.2%
65.1%
82.4%
79.2%
73.5%
91.0%

99.2%

84.6%
27.7%
75.9%
54.9%
92.4%
89.2%

98.9%

58.9%
40.7%
69.1%
39.4%
90.3%
38.3%

99.0%

59.5%
60.4%
89.7%
88.3%
81.9%
86.6%

99.7%

55.8%
25.9%
47.6%
39.7%
51.0%
46.3%

99.3%

Mean

74.2%
51.1%
75.9%
65.1%
80.4%
74.8%

99.3%

Table 1: Comparison of f1-scores from object detection with 6D pose estimation algorithms, evaluated on the dataset of [13].

Errors

PCL [9]

C&C [3]

Krull [8]

Learner [12]

AR [11]

Our Tracker

x
o
B

t
c
e
n
i
K

)
a
(

k
l
i

M

)
b
(

e
c
i
u
J

e
g
n
a
r
O

)
c
(

e
d
i
T
)
d
(

n
a
e
M

tx
ty
tz
Roll
Pitch
Yaw
Time

tx
ty
tz
Roll
Pitch
Yaw
Time

tx
ty
tz
Roll
Pitch
Yaw
Time

tx
ty
tz
Roll
Pitch
Yaw
Time

Transl.
Rot.
Time

43.99
42.51
55.89
7.62
1.87
8.31
4539.0

13.38
31.45
26.09
59.37
19.58
75.03
2205.0

2.53
2.2
1.91
85.81
42.12
46.37
1637.0

1.46
2.25
0.92
5.15
2.13
2.98
2762.0

18.72
29.7
2786.0

Hardware
Requirement

CPU
(4 cores)

1.84
2.23
1.36
6.41
0.76
6.32
166.0

0.93
1.94
1.09
3.83
1.41
3.26
134.0

0.96
1.44
1.17
1.32
0.75
1.39
117.0

0.83
1.37
1.2
1.78
1.09
1.13
111.0

1.36
2.45
132.0

GPU

0.83
1.67
0.79
1.11
0.55
1.04
143.0

0.51
1.27
0.62
2.19
1.44
1.9
135.0

0.52
0.74
0.63
1.28
1.08
1.2
129.0

0.69
0.81
0.81
2.1
1.38
1.27
116.0

0.82
1.38
131.0

GPU

0.24
0.29
0.18
0.17
0.21
0.16
1.4

0.27
0.25
0.21
0.24
0.33
0.25
1.4

0.22
0.21
0.18
0.2
0.24
0.19
1.5

0.24
0.24
0.17
0.16
0.3
0.19
1.4

0.22
0.22
1.4

0.15
0.19
0.09
0.09
0.06
0.04
2.2

0.09
0.11
0.08
0.07
0.09
0.06
2.1

0.11
0.09
0.09
0.08
0.08
0.08
2.2

0.08
0.09
0.07
0.05
0.12
0.05
2.2

0.1
0.07
2.2

CPU
(1 core)

CPU
(1 core)

Table 2: Comparison of the errors in translation (mm) and rotation (degrees), the failure rate (%) and the runtime (ms) of the
tracking results, evaluated on the dataset of [3].

3

Our Detector

Our Tracker

5. Videos

Time

Computational
Power

872.1 ms

CPU
(8 Cores)

1.4-2.2 ms

CPU
(1 Core)

Table 3: Efﬁciency of the object detection and temporal
tracker.

ment against other temporal trackers from Table 2 that run
at 2786.0 ms for [9], 132.0 ms for [3] and 131.0 ms for [8],
where [3, 8] use GPU. We further evaluated that we achieve
a real-time performance in tracking 108 moving objects at
30 fps with 8 CPU cores [12]. Moreover, considering that
we have a learning-based approach that needs to store the
random forests, we efﬁciently achieved a low memory foot-
print of about 40.5 MB per object.

3.3. Low-Cost Hardware Requirements

The hardware requirements for our framework is (1) a
standard computer; and, (2) a consumer depth sensor (e.g.
Microsoft Kinect and Kinect II, Asus Xtion, PrimeSense
Carmine, Orbbec Astra) that costs about 150 USD. These
sensors are much cheaper than the industrial 3D sensors
In addition, due to the frame-
which are currently used.
work’s efﬁciency,
the cost remains low since powerful
GPU’s are not required because both the tracker and the de-
tector only use the CPU cores. Here, all the quantitative and
qualitative evaluations use an Intel(R) Core(TM) i7 CPU in
a Lenovo W530 laptop.

4. Conclusion

We present a seamless object detection and tracking
framework that automatically ﬁnds the object of interest in
the scene and keeps track of these objects across time. Our
evaluation proves that its robustness, accuracy, efﬁciency
and cost-effective characteristics make it an ideal frame-
work for applications in robotic perception and interaction
for industrial robotics in the production line. Another set
of applications is in Augmented Reality (AR), Mixed Re-
ality (MR) and Virtual Reality (VR), where not only does
the pose estimation play a fundamental role in ﬁnding the
objects in the scene but it is also an enabling technology
in a pipeline composed of multiple modules running on
the same power- and memory-limited hardware platform.
Therefore, this allows applications to have ample of time
to build or render on top of our framework as well as the
capacity to fully utilize the machine’s hardware resources.
Notably, it is also highly suitable for applications in mobile
platforms.

We prepared demonstrative videos12 to show the frame-
work’s performance. For AR applications, we introduce an-
other set of videos34 that uses our new temporal tracker [11]
to estimate a more accurate pose and to handle challenging
scenarios.

This extended abstract was submitted to the demo session
of ISMAR 2017 and the 3rd International Workshop on Re-
covering 6D Object Pose organized at ICCV 2017.

References

[1] E. Brachmann, A. Krull, F. Michel, S. Gumhold, J. Shotton,
and C. Rother. Learning 6d object pose estimation using 3d
object coordinates. In ECCV, 2014.

[2] L. Breiman. Random forests. Machine learning, 45(1):5–32,

2001.

[3] C. Choi and H. I. Christensen. Rgb-d object tracking: A

particle ﬁlter approach on gpu. In IROS, 2013.

[4] A. Doumanoglou, R. Kouskouridas, S. Malassiotis, and T.-
K. Kim. Recovering 6d object pose and predicting next-best-
view in the crowd. In CVPR, 2016.

[5] B. Drost, M. Ulrich, N. Navab, and S. Ilic. Model globally,
match locally: Efﬁcient and robust 3d object recognition. In
CVPR, 2010.

[6] S. Hinterstoisser, V. Lepetit, S. Ilic, S. Holzer, G. Bradski,
K. Konolige, and N. Navab. Model based training, detec-
tion and pose estimation of texture-less 3d objects in heavily
cluttered scenes. In ACCV, 2012.

[7] W. Kehl, F. Milletari, F. Tombari, S. Ilic, and N. Navab. Deep
learning of local rgb-d patches for 3d object detection and 6d
pose estimation. In ECCV, 2016.

[8] A. Krull, F. Michel, E. Brachmann, S. Gumhold, S. Ihrke,
and C. Rother. 6-dof model based tracking via object coor-
dinate regression. In ACCV, 2014.

[9] R. B. Rusu and S. Cousins. 3d is here: Point cloud library

(pcl). In ICRA, 2011.

[10] D. J. Tan and S. Ilic. Multi-forest tracker: A chameleon in

tracking. In CVPR, 2014.

[11] D. J. Tan, N. Navab, and F. Tombari. Looking beyond the
simple scenarios: Combining learners and optimizers in 3d
temporal tracking. TVCG, 2017.

[12] D. J. Tan, F. Tombari, S. Ilic, and N. Navab. A versatile
learning-based 3d temporal tracker: Scalable, robust, online.
In ICCV, 2015.

[13] A. Tejani, D. Tang, R. Kouskouridas, and T.-K. Kim. Latent-
class hough forests for 3d object detection and pose estima-
tion. In ECCV. 2014.

1Link to video: https://youtu.be/7rKBZZHJkFk
2Link to video: https://youtu.be/1P184ZocMo8
3Link to video: https://youtu.be/t-WDIqEPQ3g
4Link to video: https://youtu.be/8-0xsc2abQs

4

