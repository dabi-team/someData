7
1
0
2

y
a
M
5

]
E
M

.
t
a
t
s
[

2
v
8
0
3
1
0
.
5
0
7
1
:
v
i
X
r
a

Fixed eﬀects selection in the linear mixed-eﬀects model using
adaptive ridge procedure for L0 penalty performance

Eric Houngla Adjakossaa,b,∗, Gregory Nuelb

aInternational Chair in Mathematical Physics and Applications (ICMPA-UNESCO Chair) /University of
Abomey-Calavi, 072 B.P. 50 Cotonou, Republic of Benin
b Laboratoire de Probabilit´es et Mod`eles Al´eatoires /Universit´e Pierre et Marie Curie, Case courrier 188 - 4,
Place Jussieu 75252 Paris cedex 05 France

Abstract

This paper is concerned with the selection of ﬁxed eﬀects along with the estimation of ﬁxed

eﬀects, random eﬀects and variance components in the linear mixed-eﬀects model. We introduce

a selection procedure based on an adaptive ridge (AR) penalty of the proﬁled likelihood, where

the covariance matrix of the random eﬀects is Cholesky factorized. This selection procedure is

intended to both low and high-dimensional settings where the number of ﬁxed eﬀects is allowed

to grow exponentially with the total sample size, yielding technical diﬃculties due to the non-

convex optimization problem induced by L0 penalties. Through extensive simulation studies,

the procedure is compared to the LASSO selection and appears to enjoy the model selection

consistency as well as the estimation consistency.

Keywords:

linear mixed-eﬀects model, consistent selection, iteratively weighted ridge, proﬁled

likelihood

1. Introduction

During the last two decades, selection procedures in the linear mixed-eﬀects model have been

an active research topic due to the appealing features of the model and the advent of modern

technologies facilitating the collection of many variables in scientiﬁc studies. Many of these vari-

ables are typically included in the full model at the initial stage of modeling to reduce model

approximation error, and due to the complexity of the mixed-eﬀects models, inferences and inter-

pretations of the estimated models become challenging as the dimension of ﬁxed or random eﬀects

increases (Fan and Li, 2012). The selection of important ﬁxed or random eﬀects has thus become

a fundamental problem in the analysis of grouped data using mixed-eﬀects models, especially in

the high-dimensional settings where the ﬁxed or the random eﬀects vector dimension is allowed

to grow exponentially with the sample size.

∗Corresponding author
Email addresses: ericadjakossah@gmail.com (Eric Houngla Adjakossa), Gregory.Nuel@math.cnrs.fr

(Gregory Nuel)

Preprint submitted to Journal of LATEX Templates

May 8, 2017

 
 
 
 
 
 
Generally, model selection procedures can be viewed as covering three main approaches: the

hypothesis testing procedures, the regularization procedures and other procedures which include

the Bayesian selection methods. The testing procedures include the ordinary hypothesis tests

and the selection methods based on generalized information criteria. Examples of using testing

hypothesis for models selection in the mixed-eﬀects model context include Lin’s works (Lin, 1997)

who proposed a simple global variance component tests, which are locally asymptotically most

precise and are robust in the sense that no assumption about the parametric form of the random

eﬀects is made. Despite their global form expressions which require only the ﬁtting of conventional

generalized linear models, the critical values of the global test statistics, which are based on large

sample theory, are less accurate when the number of levels of each random eﬀect is small, e.g. less

than 15. Edwards and his co-workers (Edwards et al., 2008) extended the traditional coeﬃcient
of determination R2 for the linear mixed-eﬀects model y ∼ N (Xβ, Σ = ZΓZ > + σ2IN ), where
they introduced a statistic R2
(C bβ)> h
C(X > bΣ−1X)−1C >i−1
rank q −1, in testing H0 : Cβ = 0. R2
outcomes and the ﬁxed eﬀects in the context of longitudinal data analysis. This R2

β = (q − 1)ν−1F ( bβ, bΣ)/
C bβ/rank(C), ν = N − rank(X) = N − q, C = (cid:2)0(q−1)×1Iq−1

β measures the multivariate association between the repeated

, with F ( bβ, bΣ) =
(cid:3) of

i
1 + (q − 1)ν−1F ( bβ, bΣ)

h

β statistic arises

as a 1 − 1 function of an appropriate F statistic (i.e., F ( bβ, bΣ)) for testing all the ﬁxed eﬀects,
except the intercept. More precisely, R2

β compares the full model with a null model having no
β is then generalized to deﬁne a partial R2 statistic for
marginal ﬁxed eﬀects of all sorts. Although this testing-based selection procedure of ﬁxed eﬀects

ﬁxed eﬀect except typically the intercept. R2

is very useful, one of its major drawback is that the choice of the denominator of R2

β clearly aﬀects

the rate of convergence as N → ∞, and may change the parameter being estimated. Examples of

R2-based selection of ﬁxed eﬀects in the mixed-eﬀects model include Snijders and Bosker (1994),

Xu (2003), Kramer (2005) and references therein, where generally, too much restrictions are made

on the random eﬀects covariance matrix, and clearly may not be appropriate for a wide range of

data analysis.

Based on testing procedures, a stepwise procedure can be constructed for selecting important

ﬁxed or random eﬀects using generalized information criteria (Pu and Niu, 2006; Fan and Li, 2012),

which are a generalization of Akaike’s information criterion (AIC) (Akaike, 1974) and the Bayesian

information criterion (BIC) (Schwarz et al., 1978). Pu and Niu (2006) extended the Generalized

information criterion (GIC) proposed by Rao and Wu (1989) in order to construct a procedure for

selecting ﬁxed and random eﬀects in the linear mixed-eﬀects model. Here, following Shao (1997),

the asymptotic behavior of the extended GIC method for selecting ﬁxed eﬀects is studied, and the

results from simulations show that if the signal-to-noise ratio is moderate or high, the percentages

of choosing the correct ﬁxed eﬀects by the GIC procedure are close to one for ﬁnite samples.

Another examples of GIC like selection procedures include those proposed by Mallows (1973),

2

Hannan and Quinn (1979) and Bozdogan (1987). These strategies suggest a uniﬁed approach for

choosing a parameters vector β that maximizes the penalized likelihood

n−1‘n(β|y) −

p
X

j=1

pλ(|βj|)

(1)

which arose from the Kullback-Leibler (KL) divergence −‘n( bβ|y) + λkβk0 of the ﬁtted model from

the true model (Akaike, 1973), where ‘n(.|y) is the log-likelihood function, bβ is the maximum

likelihood estimator of β and pλ is the penalty function indexed by the regularization parameter
λ (cid:62) 0. The L0-norm kβk0 of β counts the number of non-vanishing components (βj 6= 0) in

β, and arises naturally in many classical model selection methods. Although involving a nice

interpretation of the best subset selection, and admitting good sampling properties (Barron et al.,

1999), its computation is infeasible in high dimensional statistical endeavors (Fan and Lv, 2010).

Other penalty functions are regularly used in the literature. A natural generalization of L0 penalty
is the so-called bridge (noted Lq) penalty in Frank and Friedman (1993), where pλ(|β|) = λkβkq
Lq
for 0 < q (cid:54) 2. The Lq penalty encompasses L0, L1 - LASSO (Tibshirani, 1996) - and L2 (ridge)

penalties. Since none of the Lq penalties satisﬁes all the required properties (sparsity, approximate

unbiasedness and continuity, see Fan and Li (2001) for more details) for their resulting parameter

estimators, other penalties including SCAD (Fan, 1997; Fan and Li, 2001) and MCP (Zhang, 2007)

are introduced in the literature.

Testing-based stepwise selection procedures, where λ is ﬁxed (λ = 1 for AIC, λ = log(n)/2 for

BIC, λ = log(log n)/2 for HQIC (Hannan and Quinn, 1979), λ = (log n + 1)/2 for CAIC (Bozdo-

gan, 1987), for example) are computationally expensive for high-dimensional settings and ignore

stochastic errors in the variable selection process (Fan and Li, 2001). Another severe drawback is

their lack of stability (Breiman et al., 1996). Penalized likelihood approaches using data-driven

choice of λ are generally preferred to handle the high-dimensional selection problem. These meth-

ods are referred to as regularization procedures. Here, the regularization parameter λ (also called

the tuning parameter) is generally estimated using cross-validation methods (Breiman, 1995; Tib-

shirani, 1996; Fu, 1998; Fan and Li, 2001). Ibrahim et al. (2011) consider the selection of both ﬁxed

and random eﬀects in a general class of mixed eﬀects models by optimizing the penalized likelihood

criterion Qλ(θ|θold) = Q(θ|θold) − n

pλ(|βj|) using the EM algorithm (Dempster et al., 1977),

j=1

where θ is the vector of all the unknown parameters, Q(.|θold) is the resulting function of the EM

algorithm E-step, and pλ is either the SCAD or the adaptive lasso - ALASSO - (Zou, 2006) penalty.

They approximate the integral in Q(θ|θold) by using a Markov chain Monte Carlo and introduce

the ICQ(λ) = −2Q(bθλ|bθ0) + cn(bθλ) statistic (Ibrahim et al., 2008) for selecting the regularization

parameter, where bθ0 = arg max

θ

‘(θ) is the unpenalized maximum likelihood estimate and cn(θ) a

3

p
X

function of the data and ﬁtted model.

Fan and Li (2012) introduce a class of variable selection methods for ﬁxed eﬀects using a

penalized proﬁled likelihood method, where the unknown covariance matrix of the random eﬀects

is replaced with a suitable proxy matrix. Here, the general idea used is as follows. After writing

the joint density f (y, γ) of the response variable y and the random eﬀects vector γ, they consider
dnX

the penalized proﬁled likelihood Ln(β, bγ(β)) − n

pλ(|βj|), where Ln(β, bγ(β)) = f (y, bγ(β)) with

j=1

bγ(β) the empirical Bayes estimate of γ (Harville, 1977) in which the covariance matrix of γ is
replaced with the proxy matrix. dn may increase with the sample size n and pλ is a concave

penalty function (SCAD and LASSO in their simulation and application section). Although the

proxy matrix may be diﬀerent from the true one, it may still yield correct model selection results

at the cost of some additional bias (Fan and Li, 2012). Schelldorfer et al. (2011) deal with

theoretical and computational aspects for high-dimensional selection of ﬁxed eﬀects in the linear

mixed-eﬀects model, where the consistency of the estimator is proven along with a non-asymptotic

oracle result for the adaptive LASSO estimator, under the assumption that the eigenvalues of Z >Z

are bounded. Here, Z is the random eﬀects design matrix, and an explicit analytical expression

of the regularization parameter λ is also given.

Other approches for variable selection in linear mixed-eﬀects models include “fence” procedure

and Bayesian techniques. Jiang et al. (2008) introduce a class of strategies known as a fence

methods intended for variable selection in both linear and generalized linear mixed-eﬀects models.

The fence strategy is based on a measure of lack-of-ﬁt that is a quantity QM = QM (y, θM ), where

y is the response variable, M indicating a candidate model for the selection and θM denotes the

vector of parameters under M . Counting among the typically rare and ad-hoc selection proce-

dures, the fence method is computationally very demanding, particularly because it involves the

estimation of the standard deviation of the diﬀerence of lack-of-ﬁt measures. For more details

on these ad-hoc selection procedures, see the nice review paper of M¨uller et al. (2013) and ref-

erences therein. Bayesian model selection requires to assign a prior distribution over the model

parameters and compute the posterior probabilities of each of them. These computations can be

diﬃcult so are usually carried out by applying sophisticated Markov Chain Monte Carlo (MCMC)

algorithms (M¨uller et al., 2013). Exemples of Bayesian model selection include Chen and Dunson

(2003) and Saville and Herring (2009) who point out that these kinds of MCMC methods are gen-

erally time consuming to implement, requiring special software and depend on subjective choice

of the hyperparameters in the priors.

In this paper, we discuss the selection of ﬁxed eﬀects in the linear mixed-eﬀects model using

an adaptive ridge (AR) penalty of the proﬁled log-likelihood, where the random eﬀect covariance

matrix is Cholesky factorized for solving a preliminary penalized least square problem. The proﬁled

4

likelihood is calculated by slightly modifying the approach proposed by Bates et al. (2007). The

weights matrix of the AR procedure introduced here is updated in such a way that the procedure

converges toward selection with L0 penalty, as have done Frommlet and Nuel (2016). The present

selection strategy is intended to both low and high-dimensional settings.

The rest of the article is organized as follows. The proﬁled log-likelihood is introduced in

Section 2.

In Section 3, we present the weighted ridge procedure, and Section 4 presents the

simulation studies.

2. proﬁled log-likelihood for the linear mixed-eﬀects model

In this section, we consider the classical linear mixed-eﬀect model setting where the number

of observations n is larger than the number of covariates p. By slightly modifying the approach

introduced by Bates et al. (2007), we calculate the proﬁled likelihood function.

2.1. Model and notations

We consider the linear mixed-eﬀects model in which the residual terms are homoscedastic and

independent of the random eﬀects as follows.

Y = Xβ + Zγ + ε,

γ ∼ N (0, Γ),

ε ∼ N (0, σ2In)

and γ ⊥ ε,

(2)

(3)

where Y is the random response variable whose observed value is yobs ∈ Rn, γ ∈ Rq is the
unobserved random eﬀects vector with covariance matrix Γ, ε is the residual term, β ∈ Rp is

the ﬁxed eﬀects vector, and X and Z are the ﬁxed and random eﬀects related design matrices of
dimensions n × p and n × q, respectively. σ2In is the covariance matrix of ε with σ > 0 and In

the n × n identity matrix.

As a variance-covariance matrix, Γ must be positive semideﬁnite. Conveniently, the model is

expressed in terms of a relative covariance factor, Λθ, which is a q × q matrix, depending on the

variance components vector, θ, that generate the symmetric q × q variance-covariance matrix, Γ,

according to

Γ = σ2ΛθΛ>
θ ,

(4)

where σ is the same scale parameter as in Equation (3). This factorization of Γ yields the existence

of a random vector U such that

γ = ΛθU,

(5)

5

with

which is called a spherical random eﬀects1 variable. The model can therefore be re-expressed as

U ∼ N (0, σ2Iq)

(6)

follows

Y = Xβ + ZΛθU + ε,

U ∼ N (0, σ2Iq),

ε ∼ N (0, σ2In)

and

U ⊥ ε.

(7)

(8)

The parameters of the model are the ﬁxed eﬀects vector β and the variance components θ and σ2.

This formulation of the linear mixed-eﬀects model allows, not only, the use of a singular matrix Λθ

which arises in practice, but also for a relatively compact expression for the proﬁled log-likelihood

of β and θ, conditional on yobs.

2.2. proﬁled likelihood

The proﬁled likelihood considered here is expressed through the following theorem.

Theorem 2.1. Suppose that y is a realization of a random vector Y satisfying the linear mixed-

eﬀects model expressed by Equations (7) and (8), where β, θ and σ2 are the parameters to be
θ Lθ = (ZΛθ)>ZΛθ + Iq, ˜u the conditional mean
estimated. Denoting by Lθ the matrix such that L>
of U given that Y = y, and g(˜u) = ky − Xβ − ZΛθ ˜uk2 + k˜uk2, the proﬁled log-likelihood of β and

θ conditional on y is

˜‘(β, θ|y) = −

1
2

log |Lθ|2 −

(cid:20)

1 + log

(cid:18) 2πg(˜u)
n

(cid:19)(cid:21)

.

n
2

(9)

Proof. Denoting by u a realization of U, the density of y is expressed as

f (y) =

Z

Rq

f (y, u)du =

Z

Rq

f (y|u)f (u)du.

f (y|u)f (u) = (2πσ2)−(n+q)/2 exp

(cid:20)

−

ky − Xβ − ZΛθuk2 + kuk2
2σ2

(cid:21)

,

(10)

where kuk2 = u>u, with > denoting the transpose operator.

ky − Xβ − ZΛθuk2 + kuk2 =





(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

y − Xβ

0



 −





ZΛθ

Iq



 u

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

= g(u),

(11)

1Bates et al. (2007) argued that the term “spherical” is related to the fact that the contours of the N (0, σ2Iq)

probability density are spheres.

6

and solving the penalized least squares problem that is to minimize g(u) over u implies

˜u = arg min

u∈Rq

g(u) ⇐⇒





ZΛθ

Iq



> 





ZΛθ

Iq



 ˜u =





ZΛθ

Iq





> 



y − Xβ

0



 .

(12)

Viewed as a function of u, g is C∞ and the ﬁrst and the second diﬀerential of g are

and

This implies that

and

dg(u) = 2 tr (cid:8)−(y − Xβ − ZΛθu)>ZΛθdu + u>du(cid:9)

d2g(u) = 2 tr (cid:8)(ZΛθ)>ZΛθdudu> + dudu>(cid:9) .

∂g
∂u

(u) = −2 (y − Xβ − ZΛθu)> ZΛθ + 2u>

∂2g
∂u∂u> (u) = 2 (ZΛθ)> ZΛθ + 2Iq.

Then, g(u) can be rewritten as

g(u) = g(˜u) +

∂g
∂u

(˜u)(u − ˜u) +

1
2

(u − ˜u)> ∂2g

∂u∂u> (˜u)(u − ˜u).

Referring to Equation (12),





ZΛθ

Iq













y − Xβ

0



 −





ZΛθ

Iq





 ˜u

 = 0 =⇒

∂g
∂u

(˜u) = 0.

(13)

(14)

Then,

and

g(u) = g(˜u) +

(u − ˜u)> ∂2g

1
2
= g(˜u) + (u − ˜u)> h

∂u∂u> (˜u)(u − ˜u)
i
(ZΛθ)> ZΛθ + Iq

(u − ˜u)

= g(˜u) + kLθ(u − ˜u)k2, with L>

θ Lθ = (ZΛθ)> ZΛθ + Iq,

(15)

f (y) = (2πσ2)−(n+q)/2

(cid:20)

exp

−

Z

Rq

= (2πσ2)−(n+q)/2 exp

(cid:20)

−

(cid:21)

du

g(˜u) + kLθ(u − ˜u)k2
2σ2
(cid:20)

(cid:21) Z

g(˜u)
2σ2

exp

−

Rq

kLθ(u − ˜u)k2
2σ2

(cid:21)

du.

(16)

7

v = Lθ(u − ˜u) =⇒ du =

1
|Lθ|

dv, and

f (y) = (2πσ2)−n/2|Lθ|−1 exp

= (2πσ2)−n/2|Lθ|−1 exp

(cid:21) Z

Rq

(cid:21)

(cid:20)

−

(cid:20)

−

g(˜u)
2σ2
g(˜u)
2σ2

(2πσ2)−q/2 exp

(cid:21)

(cid:20)

−

kvk2
2σ2

dv

The log-likelihood of β, θ and σ2 conditional on y is

‘(β, θ, σ2|y) = −

n
2

log(2πσ2) −

1
2

log |Lθ|2 −

g(˜u)
2σ2 .

(17)

(18)

∂‘(β, θ, σ2|y)
∂σ2

= 0 =⇒ σ2 =

g(˜u)
n

. By proﬁling out σ2, the proﬁled log-likelihood, ˜‘(β, θ|y), of β

and θ conditional on y is expressed through

−2˜‘(β, θ|y) = log |Lθ|2 + n

(cid:20)

1 + log

(cid:18) 2πg(˜u)
n

(cid:19)(cid:21)

.

3. L0 estimator of β using iteratively weighted ridge procedure

The L0 estimator of the ﬁxed-eﬀects vector β using iteratively weighted ridge procedure pre-

sented here ﬁts both the low and the high-dimensional settings. Theoretically, this selection

method may enjoy great stability since it uses neither inverse of X (ﬁxed-eﬀects design matrix)

nor inverse of Z (random eﬀects design matrix).

3.1. Adaptive Ridge penalty for the proﬁled likelihood

Let us assume that the true underlying ﬁxed-eﬀects vector βtrue is sparse in the sense that

many of its coeﬃcients are zero. To enforce the sparsity of the estimator of β, we add a weighted

L2 (ridge) penalty for the ﬁxed-eﬀects vector β to the proﬁled log-likelihood function. Thus, we

are considering the objective function

˜‘λ,w(β, θ) = −2˜‘(β, θ|yobs) + λβ>W β,

(19)

where W = diag(w1, . . . , wp) is a p × p diagonal matrix and λ (cid:62) 0 is a regularization parameter.
We aim at estimating β, θ and σ2 by

( ˜βλ,w, ˜θλ,w) = arg min

β,θ

˜‘λ,w(β, θ)

and fσ2

λ,w =

ky − X ˜βλ,w − ZΛ˜θλ,w

˜uk2 + k˜uk2

n

,

(20)

8

where ˜u is calculated by the penalized least squares algorithm that has been hinted at Equa-
tion (12). The criterion ˜‘λ,w(β, θ) is minimized using one of the constrained optimization functions
in R (R Core Team, 2015), to provide the estimators ˜βλ,w, ˜θλ,w and fσ2

λ,w.

3.2. Iteratively Weighted ridge procedure

The L0 penalty for regularization arises naturally in many classical model selection since,

indeed, it counts the number of non-vanishing parameters, giving a nice interpretation of the

best subset selection and admits nice sampling properties (Barron et al., 1999). However, its

computation is infeasible in high dimensional settings and clearly argued to be a combinational

problem with NP-complexity (Fan and Lv, 2010). Some workarounds are reported in the literature,

where the proposed procedure converge toward the L0-penalty based selection. For example,

Frommlet and Nuel (2016) introduced an adaptive ridge procedure that helps to approximate

L0-penalty performances. Here, we are using the same procedure where the weight matrix (W
deﬁned in Equation (19)) diagonal elements wj, 1 (cid:54) j (cid:54) p are iteratively computed and deﬁned

as

w(k)

j =

2

(cid:20)(cid:12)
(cid:12)β(k)
(cid:12)

j

(cid:12)
(cid:12)
(cid:12)

(cid:21)−1

,

+ δ2

(21)

as have done Frommlet and Nuel (2016), taking inspiration from Grandvalet (1998); Biihlmann

and Meier (2008); Candes et al. (2008); Rippe et al. (2012) and their simulation results.

In

Equation (21), k identiﬁes the iteration and βj is the jth component of β. The general expression

of w(k)
j

is w(k)

j =

τ

h(cid:12)
(cid:12)β(k)
(cid:12)

j

(cid:12)
(cid:12)
(cid:12)

+ δτ i q−2

τ

, where q precises the norm k · kLq for the penalty, δ calibrates

which eﬀect sizes are considered relevant and τ determines the quality of the approximation
j ≈ |βj|q. In practice, δ = 10−5 seems to perform well. For more details, see Frommlet and
wjβ2
Nuel (2016). More precisely, the selection procedure performed here is as follows. For a ﬁxed λ,
˜βλ,w is initialized at (1, . . . , 1)>, the vector selection, say, which identiﬁes the selected ﬁxed eﬀects
is initialized at (1, . . . , 1)> and W is initialized at diag(1, . . . , 1). Then the steps come.

1) ˜βλ,old ← ˜βλ,w
2) perform the optimization ( ˜βλ,w, ˜θλ,w) = arg min

β,θ

˜‘λ,w(β, θ), initializing β by ˜βλ,old and θ by

θ0. Here, the components of θ which are variances are initialized by 1 and those which are

not variances are initialized by 0. Thus, θ0 components are 0 or 1.

3) wj ← (cid:0) ˜β2

λ,w,j + δ2(cid:1)−1

, where ˜βλ,w,j is the jth component of ˜βλ,w, and wj is the jth element

of W ’s diagonal.

4) selectionold ← selection and selection ← W · diag( ˜βλ,w) · ˜βλ,w
5) if |selection − selectionold| < tol = 10−5, then the selection can be considered as well
performed for λ. Thus, we choose a new value for λ. If |selection − selectionold| (cid:62) tol, the
selection does not perform well and we go to the item 1) by choosing the current ˜βλ,w as
˜βλ,old, without changing the value of λ.

9

For some λ values, the selection vector may contain 0 or 1 as components. If it is 1, the corre-

sponding ﬁxed eﬀect βj is selected, and if it is 0 the corresponding βj is not selected for λ.

For the choice of the regularization parameter λ, we propose to use the Bayesian Information

Criterion (BIC) criterion deﬁned as

cn,λ = −2‘( ˆβλ,sel, ˆθλ,sel, cσ2

λ,sel|yobs) + log(n) · ˆdλ,

(22)

where ˆβλ,sel, ˆθλ,sel and cσ2
λ,sel are the ML parameters estimators considering the selected variables.
ˆdλ = #{ ˆβλ,sel,j 6= 0 : 1 (cid:54) j (cid:54) p} + dim(θ) + 1 is the sum of the number of nonzero ﬁxed-eﬀects
and the number of variance components. This form of ˆdλ has been suggested by Bates (2010) and

empirically validated by Schelldorfer et al. (2014).

4. Simulation studies

In this section, we assess the performance of our approach using simulated data. We compare

the obtained results with those coming from the Lasso implementation in a situation where the

number of noise variables are excessive.

Since we use one of the optimizers available in the R software for minimizing the ˜‘λ,w(β, θ)

criterion, for too bigger values of p, especially when n (cid:28) p, the convergence of the used algorithm

(“nlminb” for example) is hardly or no more reached. Due to this convergence problem, we restrict

the simulation studies to the low-dimensional setting where p > 40 and will focus on this problem

in another coming paper with the same theoretical approach.

We restrict ourselves to the case of longitudinal data study with N observations coming from

n subjects where each subject i has ni observations. The “working” data sets are simulated under

the following model.

yi = X1iβ∗ + γi1ni + εi,

γi ∼ N (0, Γ),

εi ∼ N (0, σ2Ini ),

for

i = 1, . . . , n;

(23)

where β∗ ∈ Rp1 is the true ﬁxed eﬀects vector, X1 is a N × p1 design covariates matrix, 1ni =
(1, . . . , 1) ∈ Rni. γ ⊥ ε and γi ⊥ γi0 for i 6= i0. γi is an random intercept for the ith subject and ε

is the residual term of the model.

We suppose that we are following up a sample of subjects where the goal is to evaluate how

their weights are inﬂuenced by other variables including the age, the sex and the nutrition score

“nscore”, and which variables govern this inﬂuence. The covariates sex, nscore and age are staked

in the model matrix X1 and all other covariates are staked in another N × p2 model matrix X2

such that X = (X1|X2) with dim(X) = N × p and p = p1 + p2. The components of the vector

(variable) age are randomly sampled from a uniform distribution in [18, 37] and the nscore variable

10

is also uniform in [20, 50]. We would like to ﬁt to the data, the model

yi = Xiβ + γi1ni + εi,

γi ∼ N (0, Γ),

εi ∼ N (0, σ2Ini),

for

i = 1, . . . , n;

(24)

where β = (β>

1 , β>

2 )>, with β1 ∈ Rp1 and β2 = 0 ∈ Rp2 . We are then challenging to identify which

β components are zero.

For the working data sets, we choose p1 = 4, p = 54, N = 300, n = 90, σ = 1, Γ = 1
and β∗ = (1, −1, −1, 1). For the ﬁxed-eﬀects β, we have in fact ﬁfty zeros components and

only four components are not zeros. We simulate 100 data sets for which we perform both the

lasso selection and the IWR (Iteratively Weighted Ridge) procedure. One hundred values of

the regularization parameter λ are chosen in [10−2, 102]. For each replication, the λ value that

minimizes the BIC criterion is retained for selecting the signiﬁcant ﬁxed eﬀects. Denoting by
(with ˆβ coming from

β∗∗ = (1, −1, −1, 1, 0, . . . , 0), the mean squared error MSE = E

i

h

k ˆβ − β∗∗k2
2

lasso or IWR) is computed over the 100 replications. The cardinality of the estimated active set
(i.e. |S( ˆβ)|, with S( ˆβ) = {k : ˆβk 6= 0}) is computed as well as the proportion TP of true positive

(i.e. the selected set is exactly the true one). We also compute the proportion TPC in which the

selected set contains the true one and the proportion ZP in which the true zeros are estimated.

Table 1: Selection performances comparison between LASSO and IWR procedures.

Performance criterion

LASSO

IWR

MSE
|S( ˆβ)|
TP
TPC
ZP

1.200
4(1.614)
0%
16%
98%(0.028)

0.254
5(1.250)
35%
90%
98%(0.023)

The selection results based on the 100 simulated data sets are indeed summarized through ﬁve

performance criteria that are contained in Table 1, where the numbers between parentheses are

the standard deviations related to the criteria mean values (printed just before these parentheses).

Information from Table 1 show that the IWR procedure outperforms the LASSO one. IWR selects

the correct model 35% of the time when LASSO has never found it (see TP values in Table 1).

For information, we use the R software package lmmlasso (Schelldorfer, 2011) to perform LASSO

selection. 90% of the time, the model selected by IWR contains the true one against 16% for

LASSO (see TPC values from Table 1). Obviously, the estimations are of better qualities from

IWR than from LASSO (MSE = 0.254 for IWR, and MSE = 1.200 for LASSO).

The true zero estimation proportions (ZP) are computed in Table 2 which contains also their

number of occurrence over the simulated data sets. The LASSO seems to ﬁnd more often than

11

Table 2: True zero estimation proportions (ZP) with the number of occurrence over the 100 replica-
tions.

LASSO

IWR

ZP
Number

0.88
2

0.90
2

0.92
3

0.94
6

0.96
14

0.98
24

1
49

0.90
2

0.92
2

0.94
8

0.96
16

0.98
32

1
40

IWR all the true zeros (49% for LASSO against 40% for IWR, in Table 2). This may explain
the overﬁtting behavior of IWR (|S( ˆβ)| = 5 for IWR and |S( ˆβ)| = 4 for LASSO in Table 1). It

therefore seems that LASSO has a stronger shrinkage capability than has IWS which shows in

turn a somewhat overﬁtting behavior than LASSO. Through these simulations studies, it appears

that the iteratively weighted aspect of the IWR procedure highers the shrinkage performance of

the ordinary Ridge and results in a selection method having better performance than the LASSO.

Like in the case of LASSO, it is possible for IWR to take advantage of a warm start of the

algorithm to obtain the full regularization path of the selection problem. For instance, Figure 1

Figure 1: Example of a full regularization path for the iteratively weighted ridge selection procedure
with N = 300, n = 90, p = 54, σ = 1 = Γ. Only the ﬁrst four components of β∗ are nonzero. The
dataset is simulated using the R software with set.seed(3). The vertical red dashed bar corresponds
to the minimum of BIC and shows the selected variables.

shows the full regularization path from one of the simulated data. The vertical red dashed bar

corresponds to the minimum of BIC and shows the selected variables. On Figure 1, we clearly
have |S( ˆβ)| = 4, i.e., four variables selected at the end of the procedure.

12

−4−2024−1.0−0.50.00.51.0log(penality)Parameter5. Conclusion

In this paper, we have focused on the ﬁxed-eﬀects selection problem in the linear mixed-eﬀects

model. We have introduced an iteratively weighted ridge procedure which enhances the shrinkage

performance of the ordinary ridge in order to approximate the performance of the L0 based penalty

selection. This selection method is based on an adaptive ridge penalty of the proﬁled likelihood,

where the covariance matrix of the random eﬀects is Cholesky factorized. The procedure ﬁts both

the low and the high-dimensional settings and may enjoy a great numeric stability since it needs no

use of the inverse of the ﬁxed-eﬀects design matrix or the design matrix of the random-eﬀects. Due

the problems of the lack of convergence in the high-dimensional settings using available optimizers,

we restrict the simulations studies to the low-dimensional case where we ﬁnd out that our selection

procedure outperforms the LASSO selection. In another forcoming paper, we will focus on this

convergence problem in the high-dimensional cases with the same theoretical approach.

13

References

References

Akaike, H., 1973.

Information theory and an extension of the maximum likelihood principle.

Second International Symposium on Information Theory (Edited by B. N. Petrov and F. Csaki),

267–281.

Akaike, H., 1974. A new look at the statistical model identiﬁcation.

IEEE transactions on

automatic control 19, 716–723.

Barron, A., Birg´e, L., Massart, P., 1999. Risk bounds for model selection via penalization. Prob-

ability theory and related ﬁelds 113, 301–413.

Bates, D., Sarkar, D., Bates, M.D., Matrix, L., 2007. The lme4 package. R package version 2, 74.

Bates, D.M., 2010.

lme4: Mixed-eﬀects modeling with r. URL http://lme4. r-forge. r-project.

org/book .

Biihlmann, R., Meier, L., 2008. Discussion of” one-step sparse estimates in nonconcave penalized

likelihood models,” by h. zou and r. li. Ann. Statist 36, 1534–1541.

Bozdogan, H., 1987. Model selection and akaike’s information criterion (aic): The general theory

and its analytical extensions. Psychometrika 52, 345–370.

Breiman, L., 1995. Better subset regression using the nonnegative garrote. Technometrics 37,

373–384.

Breiman, L., et al., 1996. Heuristics of instability and stabilization in model selection. The annals

of statistics 24, 2350–2383.

Candes, E.J., Wakin, M.B., Boyd, S.P., 2008. Enhancing sparsity by reweighted ? 1 minimization.

Journal of Fourier analysis and applications 14, 877–905.

Chen, Z., Dunson, D.B., 2003. Random eﬀects selection in linear mixed models. Biometrics 59,

762–769.

Dempster, A.P., Laird, N.M., Rubin, D.B., 1977. Maximum likelihood from incomplete data via

the em algorithm. Journal of the royal statistical society. Series B (methodological) , 1–38.

Edwards, L.J., Muller, K.E., Wolﬁnger, R.D., Qaqish, B.F., Schabenberger, O., 2008. An r2

statistic for ﬁxed eﬀects in the linear mixed model. Statistics in medicine 27, 6137–6157.

Fan, J., 1997. Comments on wavelets in statistics: A review by a. antoniadis. Journal of the

Italian Statistical Society 6, 131–138.

14

Fan, J., Li, R., 2001. Variable selection via nonconcave penalized likelihood and its oracle prop-

erties. Journal of the American statistical Association 96, 1348–1360.

Fan, J., Lv, J., 2010. A selective overview of variable selection in high dimensional feature space.

Statistica Sinica 20, 101.

Fan, Y., Li, R., 2012. Variable selection in linear mixed eﬀects models. Annals of statistics 40,

2043.

Frank, L.E., Friedman, J.H., 1993. A statistical view of some chemometrics regression tools.

Technometrics 35, 109–135.

Frommlet, F., Nuel, G., 2016. An adaptive ridge procedure for l 0 regularization. PloS one 11,

e0148620.

Fu, W.J., 1998. Penalized regressions: the bridge versus the lasso. Journal of computational and

graphical statistics 7, 397–416.

Grandvalet, Y., 1998. Least absolute shrinkage is equivalent to quadratic penalization, in: ICANN

98. Springer, pp. 201–206.

Hannan, E.J., Quinn, B.G., 1979. The determination of the order of an autoregression. Journal

of the Royal Statistical Society. Series B (Methodological) , 190–195.

Harville, D.A., 1977. Maximum likelihood approaches to variance component estimation and to

related problems. Journal of the American Statistical Association 72, 320–338.

Ibrahim, J.G., Zhu, H., Garcia, R.I., Guo, R., 2011. Fixed and random eﬀects selection in mixed

eﬀects models. Biometrics 67, 495–503.

Ibrahim, J.G., Zhu, H., Tang, N., 2008. Model selection criteria for missing-data problems using

the em algorithm. Journal of the American Statistical Association .

Jiang, J., Rao, J.S., Gu, Z., Nguyen, T., et al., 2008. Fence methods for mixed model selection.

The Annals of Statistics 36, 1669–1692.

Kramer, M., 2005. R 2 statistics for mixed models. Statistics in medicine .

Lin, X., 1997. Variance component testing in generalised linear models with random eﬀects.

Biometrika 84, 309–326.

Mallows, C.L., 1973. Some comments on c p. Technometrics 15, 661–675.

M¨uller, S., Scealy, J.L., Welsh, A.H., et al., 2013. Model selection in linear mixed models. Statis-

tical Science 28, 135–167.

15

Pu, W., Niu, X.F., 2006. Selecting mixed-eﬀects models based on a generalized information

criterion. Journal of multivariate analysis 97, 733–758.

R Core Team, 2015. R: A Language and Environment for Statistical Computing. R Foundation

for Statistical Computing. Vienna, Austria. URL: http://www.R-project.org/.

Rao, R., Wu, Y., 1989. A strongly consistent procedure for model selection in a regression problem.

Biometrika 76, 369–374.

Rippe, R.C., Meulman, J.J., Eilers, P.H., 2012. Visualization of genomic changes by segmented

smoothing using an l 0 penalty. PloS one 7, e38230.

Saville, B.R., Herring, A.H., 2009. Testing random eﬀects in the linear mixed model using ap-

proximate bayes factors. Biometrics 65, 369–376.

Schelldorfer, J., 2011. lmmlasso: Linear mixed-eﬀects models with lasso. R package version 0.1-2

.

Schelldorfer, J., B¨uhlmann, P., DE, G., VAN, S., 2011. Estimation for high-dimensional linear

mixed-eﬀects models using l1-penalization. Scandinavian Journal of Statistics 38, 197–214.

Schelldorfer, J., Meier, L., B¨uhlmann, P., 2014. Glmmlasso: an algorithm for high-dimensional

generalized linear mixed models using ?1-penalization. Journal of Computational and Graphical

Statistics 23, 460–477.

Schwarz, G., et al., 1978. Estimating the dimension of a model. The annals of statistics 6, 461–464.

Shao, J., 1997. An asymptotic theory for linear model selection. Statistica Sinica , 221–242.

Snijders, T.A., Bosker, R.J., 1994. Modeled variance in two-level models. Sociological methods &

research 22, 342–363.

Tibshirani, R., 1996. Regression shrinkage and selection via the lasso. Journal of the Royal

Statistical Society. Series B (Methodological) , 267–288.

Xu, R., 2003. Measuring explained variation in linear mixed eﬀects models. Statistics in medicine

22, 3527–3541.

Zhang, C.H., 2007. Penalized linear unbiased selection. Department of Statistics and Bioinfor-

matics, Rutgers University , 2007–003.

Zou, H., 2006. The adaptive lasso and its oracle properties. Journal of the American statistical

association 101, 1418–1429.

16

