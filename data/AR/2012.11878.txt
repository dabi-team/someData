TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, AUGUST 2020

1

Placement Retargeting of Virtual Avatars to
Dissimilar Indoor Environments

Leonard Yoon, Student Member, IEEE, Dongseok Yang, Student Member, IEEE, Jaehyun Kim, Choongho
Chung, Student Member, IEEE, Sung-Hee Lee, Member, IEEE

0
2
0
2
c
e
D
2
2

]

C
H
.
s
c
[

1
v
8
7
8
1
1
.
2
1
0
2
:
v
i
X
r
a

Fig. 1: AR telepresence spaces seen from the perspective views (bottom) of the User X in Space A (left) and User
Y in Space B (right), and the egocentric views of the users seeing the avatar of the other party (top). The avatars
are placed by our method to preserve the semantics of the placement the other party.

Abstract‚ÄîRapidly developing technologies are realizing a 3D telepresence, in which geographically separated users can interact with
each other through their virtual avatars. In this paper, we present novel methods to determine the avatar‚Äôs position in an indoor space
to preserve the semantics of the user‚Äôs position in a dissimilar indoor space with different space conÔ¨Ågurations and furniture layouts. To
this end, we Ô¨Årst perform a user survey on the preferred avatar placements for various indoor conÔ¨Ågurations and user placements, and
identify a set of related attributes, including interpersonal relation, visual attention, pose, and spatial characteristics, and quantify these
attributes with a set of features. By using the obtained dataset and identiÔ¨Åed features, we train a neural network that predicts the
similarity between two placements. Next, we develop an avatar placement method that preserves the semantics of the placement of
the remote user in a different space as much as possible. We show the effectiveness of our methods by implementing a prototype
AR-based telepresence system and user evaluations.

Index Terms‚ÄîTelepresence, Avatar, Augmented reality, Similarity learning.

(cid:70)

1 INTRODUCTION

Rapidly advancing technologies are gradually realizing
immersive 3D telepresence experience,
in which a user
can interact with a remote user through his/her virtual
avatar, an image of the remote user overlaid on the local
physical space, thereby signiÔ¨Åcantly improving the sense of

‚Ä¢

Leonard Yoon, Dongseok Yang, Jaehyun Kim, Choongho Chung and Sung-
Hee Lee are with Korea Advanced Institute of Science and Technology
(KAIST).
E-mail: {lyoon, dsyang, chrisjkim, thegenuine, sunghee.lee}@kaist.ac.kr

Manuscript received March 9, 2020; revised August 19, 2020.

co-presence [1]. For two-way communication, the local user
is also represented by an avatar to be seen by the remote
user.

In this avatar-mediated telepresence, virtual avatar
needs to be placed and animated to deliver the semantics
of the remote user‚Äôs movement to the local user. If the
conÔ¨Ågurations of the two remote spaces are identical, which
allows for identifying a rigid transformation that maps
between the two spaces, it can be achieved by capturing the
user‚Äôs placement and motion and applying them to his/her
virtual avatar at the corresponding remote placement [2].

 
 
 
 
 
 
TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, AUGUST 2020

2

the remote user is visiting his/her place and interact with
him/her as if they were in the same space. Furthermore,
we assume the ‚Äúeveryday telepresence‚Äù [6], [7], in which
two people live a daily life in the telepresence environment:
they interact with each other at times and also may act
independently. Now, suppose that X has moved to a certain
position within Space A. Then where should X (cid:48) in Space B
be located to best represent X?

This question is related to understanding the semantics
of X being at pX in Space A, where pX denotes the place-
ment (position and orientation) of X, and for this a number
of different attributes need to be considered:

‚Ä¢

Interaction: X may be at pX in order to interact with
a person or object near pX . In this case, X (cid:48) needs to
be placed where the same interaction is possible.
‚Ä¢ Pose: X can take a particular pose afforded by pX
(e.g., sitting on a chair), and it would be ideal to place
X (cid:48) where the same pose can be accommodated.
‚Ä¢ Space: An indoor space is divided into different
functional spaces, such as dining and studying areas.
X (cid:48) may need to be placed in the same functional
space as X.

In this paper, we develop a set of features that quantify these
attributes.

After deÔ¨Åning such features, we develop a method to
Ô¨Ånd an appropriate corresponding placement of X (cid:48). This
can be formulated as an optimization problem deÔ¨Åned as:

arg max
pX(cid:48)

Sim(x(pX |pY (cid:48), A), x(pX (cid:48)|pY , B)),

(1)

where x(pX (cid:48)|pY , B) denotes the feature vector of pX (cid:48) rep-
resenting its semantics given pY and Space B. Then Ô¨Ånding
an appropriate similarity function Sim(¬∑, ¬∑), which measures
how close the two features are from the viewpoint of the
telepresence, is a key for the avatar placement.

If the shape and furniture arrangement of two distant
spaces around Y and Y (cid:48) are similar, all the features will
point to similar locations for the placement of X (cid:48). Other-
wise, each feature may give higher preference to different
locations. For instance, when two persons communicate,
interaction-related features may dominate other features
while pose and space-related features will be more impor-
tant when users act independently. Consequently, learning
the similarity measure is related to estimating the relative
importance among the features, and the relative importance
varies with situations. Therefore, the similarity measure will
be a non-linear function of the features.

As the similarity measure is in the realm of human
perception, we take the approach to learn the measure with
a dataset obtained by a user survey on how users would
locate their avatars in various situations. In particular, we
take a ranking approach to solve the optimization problem
and trained a neural network to estimate the similarity of
two placements in different spaces using a triplet loss ar-
chitecture. The trained neural network surpasses a baseline
linear model by a large margin in terms of test accuracy.

To investigate the effectiveness of our method, we con-
struct a prototype telepresence system and conduct a user
study. Figure 1 shows a screenshot of the developed system.
The user study shows that several placement qualities of

Fig. 2: Our target scenario for 3D telepresence. Person X
in Space A and Person Y in Space B are experiencing co-
presence through avatars X (cid:48) and Y (cid:48) representing remote
users. When Person X moves to a new placement, our
method determines the optimal placement of Avatar X (cid:48) to
best preserves the semantics of the placement of users.

However, due to the sheer variety of human living
spaces, two users are more likely in rooms with different
shape and furniture arrangement, where simple mapping
between the placement of two spaces does not exist. In
this case, one needs a non-trivial method that determines
the placement and movement of avatar according to the
conÔ¨Åguration of the local environment so that the local user
can properly recognize what the remote user is doing and
interact with him/her through avatar.

Several methods have been developed to tackle this
problem, e.g., Ô¨Ånding an afÔ¨Åne transformation that maxi-
mizes shared free spaces [3] or matching sittable regions
between two spaces [4], [5] for teleconference. However,
these methods allow only sub-regions within whole space
to be used for telepresence, and do not provide mapping
from an arbitrary placement of one space to the other. In this
paper, we aim to develop a method that allows the users to
fully utilize the two indoor spaces for the purpose of room-
scale telepresence.

Placing and animating telepresence avatar to deliver
the remote user‚Äôs motion semantics must consider various
factors of human motion in real life, including the user‚Äôs
motion, attention, and the relationship to nearby people or
objects. On the higher level, other important aspects should
be considered as well, such as the naturalness of avatar
animation, guarantees of availability and reachability of
interaction resources, and user immersion. Because of the
difÔ¨Åculty of recognizing some latent attributes and compro-
mising among a number of factors, the avatar placement
and animation problem to best preserve the semantics of
the original motion is a challenging task, with its difÔ¨Åculty
increasing with the dissimilarity between the two environ-
ments. Among a number of issues discussed above, this
paper deals with the problem of determining the placement
of an avatar in a room-scale indoor environment.

1.1 Problem DeÔ¨Ånition and Our Approach

The telepresence scenario of our research is illustrated in
Figure 2 showing two remote rooms that have different
layouts and furniture arrangements. By teleporting a Person
X in Space A to Space B as an Avatar X (cid:48), a Person Y in
Space B with an HMD can see X (cid:48). Likewise, X can see
Y ‚Äôs avatar Y (cid:48) in Space A; this allows each user to feel that

TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, AUGUST 2020

3

our method are comparable to those of manual placement
when evaluating the avatar of the other party in our test AR
telepresence environments.

Contributions. We develop a novel framework that de-
termines the placement of a virtual avatar in a general
indoor space to preserve various aspects of the meaning of
the user‚Äôs placement in a dissimilar remote space. To this
end, we identiÔ¨Åed a set of features to be considered for the
placement and developed a neural network-based similarity
predictor by using a training set obtained from a user
survey. We developed a prototype AR-based telepresence
system and show the effectiveness of our methods.

The remainder of the paper proceeds as follows. After
reviewing related work in Section 2, we present our method
for training a similarity predictor between two placements
for generating the placement of the virtual avatar in Section
3. We evaluate our method with respect to the prediction
accuracy and a user study by using the developed prototype
telepresence system in Section 4. Section 5 concludes the
paper with discussion of the limitations of our research and
future research directions.

2 RELATED WORK
In this section, we review related studies on the placement
of the telepresence avatars, motion retargeting, and under-
standing the relationships between a character and nearby
entities.

2.1 Placement of Telepresence Avatar

Recent technical advances have facilitated bidirectional im-
mersive telepresence, which merges two remote spaces into
one.

Maimone and Fuchs [1] proposed an HMD-based telep-
resence system that overlays a remote user‚Äôs image on a
local space. Microsoft Holoportation transmits 3D data of
remote people and objects to a local space [2]. These studies
focused on capturing the remote person and reconstructing
him/her in a designated local space, but they did not
consider the problems arising from the differences between
two spaces, which is the main problem of our research.

This problem was partly addressed by Pejsa et al. [4] who
developed a system that projects a remote user‚Äôs image at a
certain local position for which correspondence is manually
speciÔ¨Åed (e.g., sofa to sofa). Jo et al. [5] proposed a method
to select a suitable furniture object that an avatar should be
located near by constructing the correspondence between
objects in two spaces and adjusted the body pose to Ô¨Åt the
different shapes of the object. Lehment et al. [3] proposed a
method that rigidly aligns two spaces to maximally overlap
free spaces and work surfaces so that a remote person in a
free space could be transmitted to a free space. However,
the rigid alignment approach is valid only if the two spaces
are similar. By contrast, we consider general Ô¨Çoor layouts
that a rigid alignment method cannot handle. More impor-
tantly, we Ô¨Ånd the corresponding positions by considering
many important factors, such as the attention and pose of
a person, the spatial relation between people, and spatial
characteristics of spaces.

In recent years, there have been work on mixed reality
(MR) system that adaptively positioned a resized avatar

within the Ô¨Åeld of view of AR HMD for remote collaboration
[8], and proposed a system realizing transitions between
VR and AR to provide video, audio and spatial capture
for teaching physical tasks [9]. They alleviated the problem
arose from hardware and extended ability of MR system
with integration of existing technology. While these studies
focus on either the interaction only in shared space for col-
laboration or the system that requires additional user input
for teaching task, we deal with general use of everyday
telepresence for people naturally living in different spaces.

2.2 Retargeting Human Movement

Our problem shares the same goal with the motion retarget-
ing problem in that an existing human motion is modiÔ¨Åed
to match different situations. Early methods mostly solved
the problem of modifying a given motion for new characters
with different body dimensions or varied environments [10],
[11]. When it comes to the interaction motions between two
people or between a person and an object, the retargeted
motion can be meaningful only if the interaction semantics
between the two is preserved. For this, many approaches
have deÔ¨Åned the spatial relationships between the interact-
ing entities [12], [13], [14], [15], [16].

Retargeting of a person‚Äôs placement to other environ-
ments in an everyday scenario should also preserve the
semantics of the person‚Äôs location. Compared with previ-
ous motion retargeting studies, this problem must consider
signiÔ¨Åcantly more factors involved in human motion as
mentioned above. This paper takes these factors into con-
sideration to obtain the avatar placement method.

2.3 Character Placement and Action in a Scene

To solve our problem, it is necessary to represent a human‚Äôs
usage of space and actions in relation to the nearby people
and surrounding environment, which has been receiving
growing interest in computer graphics research.

From a single indoor image, Gupta et al. [17] estimated
both the 3D geometries and available human poses in the
scene by simply matching a set of poses to the 3D geome-
tries. Savva et al. [18] used observations of actual humans‚Äô
interactions with objects to learn to predict the likelihood of
actions in 3D scenes. They subsequently developed human-
centric representations of interactions that link attributes of
the human pose with the geometry and layout of the objects,
by which one can generate both human poses and furniture
layouts corresponding to a desired action input [19]. Re-
cently, Li et al. [20] proposed a 3D pose generative model
in indoor environments using the databases of semantic
information from 2D images and geometric information
from 3D models.

On the object interaction level, researchers have de-
veloped various methods to represent and estimate object
affordance [21] or functionality, e.g., [22], [23], [24]. Among
them, Hu et al. [25] proposed a geometric description of the
functionality of a 3D object in the context of a given scene,
derived from interactions between objects. Kim et al. [26]
developed a method that predicts corresponding human
poses for 3D shapes by identifying contactable local shapes
in the 3D shapes and searching for admissible human poses.

TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, AUGUST 2020

4

Fig. 3: User survey screen. Participants can view the Ô¨Çoor plans of Spaces A (bottom left) and B (bottom right) with
placements of humans and avatars concurrently with a 2D monitor. In addition, views of Person X (top left) and Avatar
X (cid:48) (top right) are provided to the participants. Participants select placement of Avatar X (cid:48) corresponding to the placement
of Person X. In the bottom right, placements of Avatar X (cid:48) answered by 10 participants are drawn overlapping. Yellow and
purple gaze lines indicate X and Y , respectively.

Interpersonal distance is another important factor in
human placement. Proxemics [27] is a study of humans‚Äô
behavior in space use according to the egocentric distance,
and a number of studies reported that proxemics theory
remains valid between a person and a virtual avatar [28],
[29], [30]. In addition to interpersonal distance, Pedica and
Vilhj¬¥almsson [31], inspired by theories on human territorial-
ity [32], emphasized the importance of the agents‚Äô orienta-
tion on modeling group dynamics in social interaction.

In this paper, we do not explicitly predict available
actions in a scene or apply proxemics theory for avatar
placement. Rather, we represent the avatar‚Äôs relation to the
scene and the person with low level geometric or categorical
features, which reÔ¨Çect such high level semantics implicitly,
and train a neural network to predict proper placements
based on the low level features.

Planning and generating a motion by taking multiple
factors into account within a scene is a complex task, and
has not been researched extensively in computer graphics.
One notable recent work of this kind is [33], which synthe-
sized a whole-body motion that includes locomotion, body
positioning, action execution and gaze behavior for generic
demonstration tasks in an indoor scene with furniture. To
generate a plausible motion, they considered various condi-
tions including visibility constraints, locomotion accessibil-
ity, and action feasibility among obstacles.

3 METHOD
A central component for Ô¨Ånding the optimal placement
of an avatar is the similarity function that estimates the
similarities of the placements of the avatar and the person

from the telepresence perspective. For this, we train a neural
network that outputs the dissimilarity of two feature vectors
that represent the semantics of the placement. As the simi-
larity depends on human perception, we Ô¨Årst perform a user
survey that collects placement data preferred by people for
a number of different conÔ¨Ågurations of space and human
placements (Sec. 3.1). By examining the collected data, we
identify a set of features that are relevant to the telepresence
(Sec. 3.2). The neural network is trained to learn the sim-
ilarity of the placements using the triplet loss architecture
(Sec. 3.3). Once the similarity estimator is obtained, we Ô¨Ånd
the optimal placement using a sampling-based searching
scheme (Sec. 3.4).

3.1 Data Acquisition by User Survey

We performed a user survey that collected the preferred
avatar location corresponding to a user location in various
scenarios. For this, we Ô¨Årst prepared 24 pairs of house
models and generated a total of 864 questions. The detailed
procedure was as follows. We selected 4 indoor 3D mod-
els available from Google Sketch Up that are adequately
spacious with a reasonable number of furniture items and
functional areas, such as spaces for cooking, studying, and
resting. We then doubled the number of models by changing
furniture arrangements and interior design (e.g., props).
From 28 total pairs of 8 spaces, we excluded 4 pairs with
their own variations to compose 24 space pairs. To learn
the user preferred avatar placements in different indoor
environments, for each space pair, we generated 36 ques-
tions by placing Person X, Person Y , and Avatar Y (cid:48) in
different positions and orientations. We generated questions

TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, AUGUST 2020

5

Fig. 5: Histogram of the average nearest neighbor distances
of the user placements in each user survey question.

Fig. 4: User survey examples. Left: When the Person X and
avatar Y (cid:48) seem to interact, users placed the Avatar X (cid:48) at
similar locations. Right: When the Person X and Avatar
Y (cid:48) act independently, user‚Äôs placements of Avatar X (cid:48) show
higher variance.

on each space so as to have a wide variety of distances
and orientations between the person and avatar, interactions
with objects (watching TV, looking out a window, etc.), pose
(sitting or standing), and functional areas (kitchen, living
room and free space). As a result, a total of 24 √ó 36 = 864
questions were generated.

For each question, we asked participants to place Avatar
X (cid:48) at their preferred locations. Participants were provided
with both a 2D Ô¨Çoor plan of each space and egocentric
views of the person and avatar as shown in Figure 3.
They could explore the space by translating and rotating
the avatar with a mouse. During the experiment, we let
users thoroughly examine the space according to the fol-
lowing instruction: ‚ÄúPlace the avatar at a location that best
represents the person‚Äôs placement in a remote space‚Äù. The
participants were asked to place the avatar only by looking
at the views of the scene and were not provided with any
additional information, such as what exactly X and Y are
looking at or what they are doing. The reason for not giving
additional information was that we wanted to develop an
avatar placement algorithm that did not rely on such high
level contextual information.

A total of 10 responses were obtained for each question,
making total user response 864 √ó 10 = 8640. A total of
210 people participated in the survey and each participant
answered at
least 36 questions while 30 participants
answered additional 36 questions with different house pair.
The survey results will be publicly available online.1

Observation. Figure 4 shows some typical samples from
the user survey. When the two people seem to look at
each other or look at an object together, participants placed
avatars at places that allowed for the same action, and
as a result, the participants‚Äô answers were similar. On the

1. https://github.com/leonyoon/Avatar-Placement-User-Survey

Fig. 6: Features representing placement semantics.

other hand, participants‚Äô answers varied when the con-
Ô¨Ågurations of two spaces were signiÔ¨Åcantly different due
to the separated placements of the person or the avatar,
different layouts of space, or lack of a certain category of
furniture. To understand the clustering/dispersion pattern
of the user data, we calculated the histogram of the average
nearest neighbor distances of the user placements in each
question as shown in Figure 5. The nearest neighbor of
each user placement was identiÔ¨Åed to be the closest one
among the remaining nine user placements for the same
question, with the distance deÔ¨Åned as the weighted sum
of position and angle differences. The average distances
between a user placement and its nearest neighbor were
used for the histogram analysis. The histogram shows the
pattern of unimodal truncated normal distribution, which
shows a continuously varying degree of clustering tendency
of the user survey data.

3.2 Feature Modeling
To determine the placement of Avatar X (cid:48) to correspond to
that of Person X, we need to deÔ¨Åne features that represent
the meaning of the person‚Äôs placement and preserve the fea-
tures when the avatar is placed. After reviewing the results

ùúÉ1ùúÉ2InterpersonalVisualattentionPose accommodationSpatialùëëùëüùëíùëô0.5m0.25m3mùëë1ùëë2ùëë3ùëë440¬∞ùëë1ùëë24m4mCenter of gazeùúÉ1ùúÉ2TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, AUGUST 2020

6

of the user survey, we identiÔ¨Åed the following low level
features to represent the semantics of a person‚Äôs placement
in an indoor environment (Figure 6).

Interaction features

If a person interacts with other people or objects, and the
spatial relationship between the person and an interacting
entity is an important feature to be preserved in the telep-
resence. Distance between two people may reÔ¨Çect their inti-
macy according to Hall‚Äôs proxemics [27]. The orientation of
a person‚Äôs head-forward direction is also important because
the visibility of the other person or an object also strongly
affects the interaction. We model interaction features in two
categories. One is the interpersonal relationship between a
person and an avatar of the other party, which is repre-
sented with the distance and angle between the two, i.e.,
xip = {drel, Œ∏1, Œ∏2}, where drel is the distance between the
two, Œ∏1,2 is the angle difference (thus, always non-negative)
from the frontal direction to the location of the other party2.
The other category is the visual attention feature. In our
experiment, we suppose that a person can attend to 12 object
categories (sofa, chair, table, TV, air-conditioner, refrigerator,
sink,
lamp, piano, cabinet, shelf, window). Rather than
specifying a single object that a person focuses on, which
is difÔ¨Åcult to recognize in a real situation, we assume that
all objects within a narrow visual Ô¨Åeld (40 degrees) within
a certain distance are candidate objects of visual attention,
and nearer objects from the position of person and the center
of gaze have a higher visual attention value than farther
objects. Thus, the i-th element of visual attention feature
xva is deÔ¨Åned as:

xva,i =

(cid:88)

j

( ¬Ødva ‚àí di,j)(¬ØŒ∏va ‚àí Œ∏i,j),

(2)

where di,j and Œ∏i,j are the distance and angle of the j-th
object in the i-th category (i = 1 ¬∑ ¬∑ ¬∑ 12), and ¬Ødva and ¬ØŒ∏va are
the maximum distance and angle (4 meters and 40 degrees
in our experiment), respectively.

Note that, unlike objects, the placement of the other
party is always considered for the avatar placement through
the interpersonal feature xip regardless of its visibility. The
rationale of this choice is that even if the two are not
interacting directly, we assume that people constantly keep
the position of the other party in mind. Once trained, our
optimizer will give a proper importance to this feature
depending on the situation.

Pose accommodation features

The pose of a person, such as sitting or standing, is an
important factor that characterizes human action, and thus

2. We chose to use the angle difference to reÔ¨Çect the behavior that
some survey participants arranged the avatar in a direction that is
reversed from the original interpersonal angle as can be seen in Fig. 4
(left). However, this choice allows asymmetric arrangement between
the user and avatar, which may induce confusion in non-verbal com-
munication if the user‚Äôs pose is directly applied to his/her avatar (e.g.,
X (cid:48) can be on the left of Y while X is on the right of Y (cid:48). If Y looks to the
left to see X (cid:48), Y (cid:48) will look away from X). Therefore, the upper body
and gaze motion should be appropriately modiÔ¨Åed by some motion
retargeting algorithm, which is not investigated in this study. Replacing
Œ∏1 and Œ∏2 with angles will reduce the asymmetric placement.

Fig. 7: Left: Placement of Person X. Right: User selected sim-
ilar placements (brown) and generated less similar place-
ments (red) of Avatar X (cid:48).

it would be ideal if the avatar is placed at a location that
accommodates the user‚Äôs pose. The pose accommodation is
represented with the height Ô¨Åeld around a location because
typical furniture items that accommodate human poses (e.g.,
Ô¨Çoor, chair, bed) have different height Ô¨Åelds. SpeciÔ¨Åcally,
we divide a circular area with a radius of 0.5 m around
a person into a center, which is a circle with a radius of
0.25 m, and 16 surrounding sectors in the outer ring. The
pose accommodation feature xpa is then represented as the
average height of each subdivided area (thus forming a 17-
dimensional vector). In addition, we add a binary feature
value xss to indicate whether a person or avatar is sitting
or standing. In our experiment, xss is set as sit if a person
or avatar is positioned on sittable furniture categories (sofa
and chair), and otherwise stand.

Note that the pose accommodation features not only
characterize the possible poses at a given placement but also
deÔ¨Åne the spatial relation between a person (or avatar) and
nearby furniture items, e.g., standing beside or in front of a
table.

Spatial features

The purpose of our spatial feature is to characterize the
functional characteristic of the surrounding space, and the
distribution of furniture is an important factor for this. To
model the spatial features, we take a rather simple approach
that sums up the distances of furniture items in same
category within a certain distance. Thus, the i-th element
of spatial feature xsp is deÔ¨Åned as:

xsp,i =

(cid:88)

¬Ødsp ‚àí di,j,

(3)

j
where ¬Ødsp is the maximum distance (3 meters in our experi-
ment) of the spatial feature range and di,j is the distance of
the j-th object in the i-th category (i = 1 ¬∑ ¬∑ ¬∑ 12).

In total, the feature vector of a placement is represented

with a 45 dimensional vector x = [xip, xva, xpa, xss, xsp].

3.3 Similarity Learning

Until now we have obtained some samples from the user
survey and deÔ¨Åned the feature vector that characterizes a
placement in an indoor scene. Let us denote the feature of
the placement of Person X by x0 and that of Avatar X (cid:48)
obtained from the user survey by x+. For each sample of
i , we have obtained x+
x0

i,j, (i = 1 ¬∑ ¬∑ ¬∑ 864, j = 1 ¬∑ ¬∑ ¬∑ 10).

TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, AUGUST 2020

7

It is to be noted that the user-selected placements indi-
cate that they are better than other placements, but not the
absolute answers. That is, a user sample x+ might not have
been selected if there had been better positions. Therefore,
this problem needs to be addressed in terms of ranking
between data, i.e., by training a similarity function d(¬∑, ¬∑)
such that it can judge d(x0, x+) < d(x0, x‚àí) where x‚àí is a
feature from the placements not selected by the users.

A basic approach for this is to obtain the similarity
function in bilinear form, i.e., d(x0, x) = (x0‚àíx)T W (x0‚àíx).
Many techniques have been developed for the similarity
learning or the distance metric learning in the bilinear form
[34], [35], among which some support learning relative
similarity as in our problem [36]. This approach would work
Ô¨Åne if the difference in data, x0 ‚àí x, provided enough infor-
mation for ranking. Unfortunately, this is not the case for
our problem because the importance of subfeatures varies
according to the conÔ¨Åguration of x0. In order to reÔ¨Çect the
non-linear characteristic of data similarity in our problem,
we train a deep neural network that learns the similarity
between two placement features, as will be introduced next.

3.3.1 Dataset Preparation

We obtained similar pairs (x0,x+) from the user survey,
but we also need less similar (will be called ‚Äúdissimilar‚Äù
hereafter) pairs (x0,x‚àí) for the training. To generate dissim-
ilar features x‚àí given x0 in a scene, we generated random
placements from the scene and computed corresponding
features to the placements (Figure 7). A random placement
sample that was not too close to one of the user-selected
placements with respect to distance (1 meter), angle (36
degree), and pose (10% of normalized value) was collected
over the whole indoor space for 100 samples, and we
added 10 samples near user-selected placements to make
challenging dissimilar data. Thus, for each x0, we have 10
positive data x+ and 110 negative data x‚àí, which makes a
total of 1100 tuples (x0, x+, x‚àí) used for training.

3.3.2 Learning

We develop a neural network that learns the non-linear
characteristics of the dissimilarity (or distance) between
two placements. The neural network is trained with triplet
loss framework [37], which learns the dissimilarity between
input features in a supervised way (Figure 8(a)). This frame-
work for learning a ranking from relative similarity has been
proved to be effective not only for information retrieval
[38] and recommendation system [39], but also for general
problems including classiÔ¨Åcation and clustering that use
a distance metric [37]. In the training phase, the model
receives datasets of three types of inputs {x}: x0, x+, and
x‚àí, then the network learns the dissimilarity between input
features to predict that x0 is more similar to x+ than x‚àí.
SpeciÔ¨Åcally, the network is optimized to push the distance
d+ between the similar pair (x0, x+) to be low (zero), and the
distance d‚àí between the dissimilar pair (x0, x‚àí) to be high
(one). At the same time, the comparison between the two
distances are utilized so that d+ is less than d‚àí. Following

Model
BBMpa
BBMsp
BBMva
BBM FeatureNet
BBM MetricNet

x
17
12
12
45

h1
14
10
10
38
22 √ó N 44

h2
10
8
8
30
44

y
6
6
6
22
1

TABLE 1: The number of cells in different BBMs; input cells
(x), cells in subsequent layers (h1 and h2), and output cells
(y). N is two for the regular Triplet MatchNet and three for
our model.

[40], in our work the two optimization goals are achieved
with the loss functions œÜ({x}) and œà({x}):
1
|{x}|

{log(1 ‚àí d+) + log(d‚àí)},

œÜ({x}) = ‚àí

(cid:88)

(4)

œà({x}) =

1
|{x}|

x‚àà{x}
(cid:88)

x‚àà{x}

max{0, d+ ‚àí d‚àí},

(5)

where the total loss function is deÔ¨Åned as œÜ({x}) + œà({x}).

3.3.3 Our Network Structure

Typical triplet network models [40], [41] consist of Fea-
tureNet for feature abstraction and MetricNet for the dissim-
ilarity estimation. We slightly expand the triplet structure to
provide an additional hint for the neural network to esti-
mate dissimilarity better for our problem. In addition to the
triplet inputs, our model explicitly computes the differences
between two features |x0 ‚àí x¬±| and uses them as additional
inputs. Figure 8(b) describes the overall structure of the
network model. The explicit distance inputs are passed
through another FeatureNet, dubbed Distance FeatureNet.

Our neural network model consists of two basic mod-
ules: a black box module (BBM) and a subfeature processing
module (SFPM) as shown in Figure 8(c).The SFPM receives
one of the input feature sets, œá ‚àà {x0, x+, x‚àí, |x0‚àíx+|, |x0‚àí
x‚àí|}, and splits the input into the same type of subfeatures,
xip, xpa, xva, xss, and xsp. The high dimensional subfeatures
with local correlation, xpa, xva, and xsp, are processed
through separate BBMs. Then, the outputs of the BBMs and
low dimensional subfeatures, xip and xss, are concatenated,
making the Ô¨Ånal outputs of the SFPM. Two SFPMs are used
as FeatureNet and Distance FeatureNet at the bottom, and a
simple BBM is used as MetricNet at the top.

Our models have Ô¨Åve types of BBMs and their layer sizes
are reported in Table 1. In our experiments, we compared
the BBM with the SFPM as FeatureNet (BBM FeatureNet).
Each layer of this BBM kept the same number of cells with
corresponding layer cells in the SFPM.

3.4 Avatar Placement Algorithm

We use a sampling-based optimization scheme to Ô¨Ånd the
optimal placement of Avatar X (cid:48) given the placements of
Person X in a dissimilar space. For brevity, let us write
d(x(pX (cid:48)|pY , B), x(pX |pY (cid:48), A)),
the dissimilarity between
the placement of Avatar X (cid:48) and Person X, as D(pX (cid:48), pX ).
Then our optimization problem is

arg min
pX(cid:48)

D(pX (cid:48), pX ).

(6)

TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, AUGUST 2020

8

(a) A typical Triplet Matchnet struc-
ture.

(b) Our modiÔ¨Åed Triplet Matchnet structure. (c) BBM (left) and SFPM (right) modules in our neural

network.

Fig. 8: Network structure of our placement similarity predictor.

Fig. 9: CMC curves for Triplet model, our model, and a
linear model (MLR).

We solve this optimization in two stages. First, we sample
each space as a 2D grid map with a grid size of 0.25
meter and for each grid we take 24 orientation samples for
each 15 degrees. We compare the dissimilarity between the
feature vector of Person X and that of Avatar X (cid:48) at every
sample placement to Ô¨Ånd the best sample with the lowest
dissimilarity in the given grid. Next, starting from the best
sample placement we use the particle swarm optimization
(PSO) to Ô¨Ånd the optimal placement of the Avatar X (cid:48). In our
experiment, we used inertia weight w = 0.73, constriction
factors c1, c2 = 1.49 [42], 10 particles and 10 maximum
epochs as to the PSO parameters.

The placement algorithm must run fast to place avatars
promptly in the real-time applications. To this end, we
precompute space-speciÔ¨Åc features such as xpa and xsp
and the 2D obstacle meshes for each space. In the on-line
process, we use multi-threading (16 threads) for operating
the static placement, achieving 0.4 sec (0.3 sec for grid-level
optimization, 0.1 sec for PSO) for each placement.

(a) Average Rank 1%

(b) Average Rank 5%

(c) Average Rank 35%

Fig. 10: Heat maps when the average rank of user-selected
placements are 1, 5, and 35%.

4 RESULTS AND ANALYSIS

4.1 Similarity Learning Accuracy

We compare our triplet-based method with a linear ranking
learning algorithm as a baseline method as well as with the
variations of our own network structure with respect to the
learning accuracy. For the cross-validation, data from half of
all house pairs are used for training and the rest of the data
are used for testing. In addition, we develop a VR and AR
telepresence prototype system and perform user study to
evaluate our system from the user experience perspective.

As a baseline, we use the structural SVM approach for
metric learning to rank (MLR) [36], a well-known method
for the information retrieval. As a generalization of a
multi-class SVM, the structural SVM framework deÔ¨Ånes
the loss function in terms of ranking. The MLR learns
a positive semi deÔ¨Ånite matrix W to maximally satisfy
||x0 ‚àí x+||W < ||x0 ‚àí x‚àí||W for the training set where
||i ‚àí j||W = (cid:112)(i ‚àí j)T W (i ‚àí j). A major drawback is

TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, AUGUST 2020

9

Rank Model

1
2
3
4

SFPM, DFN (proposed)
BBM, DFN
SFPM, No DFN
BBM, No DFN

Avg. Acc.
96.7%
96.3%
95.6%
92.4%

Std. Dev.
0.0008
0.0007
0.0007
0.0050

TABLE 2: Test accuracies of proposed method and its varia-
tions.

that this linear metric cannot reÔ¨Çect the inherent non-linear
characteristic of our problem.

We use the cumulative matching characteristic (CMC)
curve to compare deep neural network variations and the
structural SVM approach. For each test question, we rank
10 user-selected placements as well as all other sample
placements and sort their dissimilarity values in increasing
order. For normalization, we divide the rank by the total
number of placements to represent it as a percentage. For
example, if the rank of one user data is 10 out of 2,000
possible sample placements, we divide 10 by 2,000 and
apply a ceiling function to represent it as a percentile rank
1. Figure 9 shows that our model surpasses the baseline
method by a large margin and is superior to the baseline
triplet network method used in [40].

Figure 10 shows three examples of different matching
characteristics. On the input of Person X (yellow) in the
left space, the colored arrows in the right indicate the
similarity (red: high, blue: low) and optimal direction at
each location computed by our model. Figure 10(a) shows a
case in which the average percentile rank of a user selected
placement is 1%. The heatmap shows that high similarity
placements are concentrated at the sofa, which agrees well
with user-selected placements. Figure 10(b) shows a similar
pattern, but the standing area near Person Y receives in-
creased similarity values due to the interpersonal relation
feature. Lastly, Figure 10(c) shows a case in which the user-
selected placements vary widely, mostly looking at window
while standing near furniture. The heatmap shows a similar
tendency of giving higher values to a wider area, which
produced a lower average rank of user-selected placements.
We also compare our proposed network structure with its
variations on test accuracy. The variations are made in two
aspects: use of the SFPM for FeatureNet and Distance Fea-
tureNet (SFPM vs. BBM), and use of the Distance FeatureNet
(DFN vs. no DFN).

Training and test sets are selected to have the same
number of samples, 150 for each, and not to have the
samples from the same space. As shown in Table 2, our
network model outperforms its variations in terms of the
test accuracy.

4.2 Telepresence System Construction

To validate avatar placement algorithm in immersive VR
and AR environments, we develop a prototype telepresence
system with HTC Vive Pro system and Unity3D game
engine, and conduct experiments. Our telepresence system
consists of two separate stations. In each station, one user
is located in a separate indoor space and can see the avatar
of the other party visiting his/her space. For VR, the users
view the virtual scene with an HMD and can freely move in

(a) House pair #1. Installed objects: Table, TV, sink, chair, sofa, lamp, and
cabinet in both houses. Shelf and window in the right house only.

(b) Perspective view.

Fig. 11: Joint attention case. Both users look at TV.

(a) House pair #2. Installed objects: Table, TV, chair, sofa, lamp, and
cabinet in both houses. Sink, shelf, and window in the right house only.

(b) Perspective view.

Fig. 12: Individual action case.

an empty physical space, with stools placed where virtual
chairs are located to accommodate sitting. For AR, the users
view the real scene with a video see-through HMD and
interact with real furniture in the space. The registration of
the virtual object and the real object is obtained manually
using an input device. A user‚Äôs movement is captured with
three trackers attached on the body (on the waist and feet),
handheld controllers and HMD. Full body pose is obtained
with inverse kinematics. Data transferred between stations
through the Internet are the feature vector of person‚Äôs place-
ment as well as positions and orientations at six parts of the
body.

TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, AUGUST 2020

10

4.3 VR Experiments

While our method was developed for AR telepresence, we
perform test in VR Ô¨Årst as it is more convenient to construct
various spatial conÔ¨Ågurations in VR. For VR telepresence
test, we select 4 studio-type house models from SunCG
database [43] to make two pairs of houses (Figure 11(a) and
12(a)). The Ô¨Årst pair of houses are selected such that both
houses have the same functional areas, kitchen, dining area,
living room, and study area, but with different layouts. The
second pair of houses is made to give a more challenging
situation: both houses have a living room and a dining area,
but a kitchen area exists only in the right house and a study
area exists only in the left house.

For each house pair, we experiment avatar placement
for a number of user positions. Figure 11 shows a case
that our system realizes a joint attention. When a user
approach an avatar of the other party and looks at the same
place, his/her avatar is placed in the vicinity of the other
user and face to the same object. Figure 12 is a case in
which two users interact with objects without regard to each
other. Their avatars Ô¨Ånd their placements in appropriate
locations to preserve the users‚Äô poses and their interaction
with objects. Supplementary video shows more examples
for VR experiments.

4.4 Comparison with Other Approaches

Some studies in telepresence [3] determine afÔ¨Åne transfor-
mation between two spaces and place avatars according the
transformation. This approach keeps only the interpersonal
relationship in shared free space while not supporting other
semantic features. Other studies [4], [5] are more compa-
rable to our work, but they mainly deal with sitting affor-
dance. To show the strength of our method that considers
a wide range of features, we compare the placements by
our method with those that match only a few features
(e.g., sitting affordance, visual attention, and interpersonal
relation) adopted in the previous work.

Figure 13 shows three examples of avatar placement
by our method for comparison. Figure 13(a) shows that
the placement of Avatar X(cid:48) by our method preserves the
context of interaction between two people around the ta-
ble by coordinating interpersonal relation, pose affordance,
spatial and visual attention feature of Person X, compared
to the placement of avatar (X(cid:48) in red) that only considers
interpersonal relation. Second, Figure 13(b) shows that the
placement of Avatar Y(cid:48) by our method preserves the context
of sitting in dining table. In comparison, the result obtained
by matching only the interpersonal relation and sitting affor-
dance (Y(cid:48) in red) does not respect this spatial characteristic.
Third, our method preserves the context of watching TV
together in Figure 13(c). For this case, our method keeps the
visual attention and interpersonal relation over the sitting
affordance (Y(cid:48) in red).

4.5 User Study in AR Environment

We construct AR telepresence environments and conduct a
two-part user study that consists of placement evaluation
and system evaluation on presence and usability. For the
test, we set up two spaces with real furniture (TV, sofa,

(a) Interaction around the table.

(b) Individually sitting in dining table.

(c) Watching TV together.

Fig. 13: Comparison between the results of our method (yel-
low) and the results of the avatar placement that matches
only a few features (red). The right part is the original
conÔ¨Åguration around the person and the left part shows the
results of the avatar placement.

table, and chair) and modify the number and position of
objects to make two different pair of spaces (a total of
4 conÔ¨Ågurations). Figures 1(b) and 14 show snapshots of
AR telepresence experience. In Figure 1(b) both users are
standing, and in Figure 14 one is standing while the other
is sitting. One can see that their avatars are placed appro-
priately to make interpersonal interaction while preserving
the users‚Äô original poses. Supplementary video shows users
experiencing AR telepresence in various positions within
the test spaces.

The Ô¨Årst user study (EXP1) evaluates the quality of place-
ment of our method (Our), in comparison with a simple
heuristic method (Simple) and a manual placement method
(GT). In the simple method, the avatar is placed simply
to preserve the distance and angle between a user and an
entity that is closest from the center of his/her visual Ô¨Åeld.
If the corresponding avatar position is in collision with
an object, its placement is modiÔ¨Åed to a nearby collision-
free position found by random sampling. In the manual
placement method, the participant switches a view from
his/her own AR space to the other party‚Äôs space visualized
with VR objects and move an avatar by hand-held controller
to place avatar in ideal position and orientation. Refer to
supplementary video for the detailed procedure. The second
user study (EXP2) evaluates the presence and usability of
our automatic placement system (Our : Automatic) in com-
parison with the manual placement system (GT : Manual).

TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, AUGUST 2020

11

(a) A local user X is sitting and looking at the avatar Y (cid:48).

(b) A local user Y is standing and looking at the avatar X (cid:48).

Fig. 14: Interaction case in the AR experiment. In each room, perspective (top) and top (bottom left) views of the room, and
the HMD view of the user (bottom right) are shown. The top views show the virtual 3D models corresponding to the real
objects.

We recruited 14 female and 18 male students from
local university community with an average age of 28.13
(SD=2.60). Their level of familiarity with VR or AR interfaces
on a 7 point Likert scale from 1 to 7 was above the average
(M=5.03, SD=1.40). All participants had an experience with
VR or AR interfaces where 15 participants reported that they
used a few times a month and 17 participants had used
them a few times a year. Two participants conducted an
experiment as a pair for both experiments. For EXP1, each
participant from a pair acted as a main participant and a
supporting participant in turn where the main participant
was the evaluator. For EXP2, both participants evaluated
the same questionnaire after using each system.

4.5.1 EXP1: Placement Evaluation

The Ô¨Årst experiment was a within-subject design where we
investigated the effects of avatar placement given four tasks
(Watch TV, SIT on chair, LOOK at avatar, TALK to avatar)
on following criteria: task accomplishment (TA), spatial
similarity (SS) and overall satisfaction (OS). The placement
evaluation was two-fold involving the main participant
and the supporting participant. First, the main participant
evaluated the placement of self-avatar from the Ô¨Årst person
(egocentric) view. Second, the main participant evaluated
the placement of the supporting participant‚Äôs avatar from
the second person view.

Procedure. Both the main participant and the supporting
participant performed each of four tasks in turn where the
order of tasks was counter-balanced. For the Ô¨Årst person
view evaluation, after performing the given task in own
AR space, the main participant switches the view of HMD
to other party‚Äôs VR space to place his/her own avatar and
return back to AR space. Then the main participant knowing
what the given task is evaluated the placement of avatar
from the egocentric view of three different methods (Our,

Simple, GT) presented in counter-balanced order. Three
questions listed below are asked after the placement of each
method. The participant rates each criteria on a 7-point
Likert scale (1: Strongly Disagree - 7: Strongly Agree). For
the second person view, the supporting participant places
his/her own avatar in the same way while the main partici-
pant remains in VR space of the the supporting participant.
After the avatar placement completed, the main participant
returns back to his/her own AR space to evaluate the avatar
placed by three different methods in counter-balanced order
from the second person view. Questions on placement eval-
uation are as follows:

1) The avatar successfully accomplishes the given task

(Task accomplishment).

2) The

furniture

conÔ¨Åguration and arrangement
around the avatar matches well with the my sur-
rounding environment (Spatial similarity).
I am satisÔ¨Åed with the placement of the avatar given
my placement (Satisfaction).

3)

4.5.2 EXP2: System Evaluation

The second experiment is also a within-subject design
which we compare our automatic avatar placement system
(Our: Automatic) with manual avatar placement system
(GT: Manual) on presence and usability. The same size
(16 participants) of new participants are recruited for the
second experiment. The presence questionnaire is based
on Networked Mind Measure of Social Presence [44] and
Temple Presence Inventory [45], while the questions for the
system questionnaire are selected from USE Questionnaire
[46].

Procedure. Two participants complete the same tasks in
turn as in previous experiment. Instead of evaluating each
placement as is done in EXP1, the participants observe the

TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, AUGUST 2020

12

Ô¨Årst person view, but for second person view, which is
actually the case of practical application of our method, the
participants were satisÔ¨Åed with our method in terms of three
criteria.

Table 4 shows that we found signiÔ¨Åcant differences on
all measures except for spatial similarity on LOOK task
from second person view. Post-hoc analysis is provided in
Table 5. For TV task, Our method was not signiÔ¨Åcantly
different from GT from second person view while GT scored
signiÔ¨Åcantly higher for Ô¨Årst person view for all three criteria.
For viewing tasks, viewing angle seems to be considered
important as it tends to be preserved well in GT while less
so in Our method. However, for the second person view,
both GT and Our method were able to produce high scores
because the participants were not sensitive to other party‚Äôs
viewing angle.

For SIT task, post-hoc tests were not signiÔ¨Åcant for GT
and Our method from both Ô¨Årst and second person view.
For both views, as long as the avatar was placed in the chair
or sofa, the scores were high for both GT and Our method.
For the spatial similarity from Ô¨Årst person view, Simple
method was not signiÔ¨Åcantly different from Our method as
it was often able to place the avatar near a chair with a table
whichever was close to the center of gaze of the participant.
For LOOK task from Ô¨Årst person view, post-hoc tests
showed that GT was signiÔ¨Åcantly better than Our method
at looking at other party‚Äôs avatar, and Simple method was
not signiÔ¨Åcantly different with Our method on task accom-
plishment. In terms of second person view, Our method
was not signiÔ¨Åcantly different from GT on all criteria and
Simple method was not signiÔ¨Åcantly different from GT on
task accomplishment and spatial similarity. One possible
interpretation is that all methods were able to place avatar‚Äôs
orientation toward the main participant so that the scores
for Simple method were somewhat comparable to other
methods.

For TALK task, Our method was not signiÔ¨Åcantly dif-
ferent from GT. Simple method was not signiÔ¨Åcantly dif-
ferent from Our method for spatial similarity, but its task
accomplishment and satisfaction scores were signiÔ¨Åcantly
lower than other two methods. Our interpretation is that
the participants were quite sensitive about small difference
in distance and orientation in Talk task.

EXP2: System Evaluation

We performed pairwise Wilcoxon signed-rank test for each
question from questionnaires. For presence questionnaire,
we found signiÔ¨Åcant difference in favor of Manual system
(GT) with Questions 1 and 2 while differences were not
signiÔ¨Åcant for the remaining questions (Table 6). Our in-
terpretation is that the participants were able to visit other
party‚Äôs VR space during the manual placement and under-
stand actual context of the avatar so that the participants can
focus more on partner during the interaction. However, the
participants were able to understand other party under the
given task with good presence using our automatic system.
For usability, we found signiÔ¨Åcant difference in favor
of automatic system (Our) with Questions 5, 6 and 11
(Table 7). This result indicates that Our automatic system
is considered easier and simpler to use, and the participants

Fig. 15: Box plot results of Ô¨Årst person view (top) and second
person view (bottom) evaluation from EXP1.

placement of other party‚Äôs avatar and perform next given
task. After using one of each system given in counter-
balanced order, the participants are given two question-
naires listed in Tables 6 and 7 on presence and usability.
The participants rate each question on a 7-point Likert scale
(1: Strongly Disagree - 7: Strongly Agree).

4.5.3 Experiment Results and Analysis

EXP1: Placement Evaluation

We performed non-paramateric Friedman tests for all mea-
sures between three methods based on the criteria for each
task for two view points. For post-hoc analysis, we used
a pairwise Wilcoxon signed-rank test with Bonferroni cor-
rection. Table 3 shows the mean and standard deviation on
responses for all measures.

Overall, Our method is signiÔ¨Åcantly better than Simple
method, but signiÔ¨Åcantly worse than GT from Ô¨Årst person
view, and it is competitive with GT and signiÔ¨Åcantly better
than Simple method from second person view (Figure 15).
Our interpretation is that the participants were sensitive
about the difference from GT acquired by him/herself in

TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, AUGUST 2020

13

View

Method

First

Second

Our

GT

Simple

Our

GT

Simple

T.A.
4.69
(2.25)
6.75
(0.45)
2.25
(1.18)
6.63
(0.80)
6.75
(0.58)
2.94
(1.88)

TV
S.S
4.56
(1.55)
6.44
(0.81)
2.69
(1.40)
6.25
(0.93)
6.44
(0.89)
3.69
(1.54)

O.S.
4.5
(1.67)
6.69
(0.48)
2.25
(1.13)
6.25
(1.06)
6.50
(0.82)
2.69
(1.58)

T.A.
5.25
(2.38)
6.75
(0.77)
1.94
(1.29)
5.38
(1.54)
6.19
(1.05)
2.50
(1.67)

SIT
S.S
4.81
(2.4)
6.13
(1.26)
3.25
(1.61)
5.75
(1.44)
6.31
(1.01)
3.69
(2.09)

O.S.
5.13
(2.36)
6.50
(0.73)
2.50
(1.63)
5.50
(1.27)
6.19
(1.11)
2.63
(1.54)

T.A.
5.88
(1.50)
6.94
(0.25)
5.06
(1.77)
6.06
(1.06)
5.89
(1.54)
4.13
(2.13)

LOOK
S.S
5.81
(1.17)
6.81
(0.40)
4.44
(1.59)
4.69
(1.66)
5.19
(1.80)
4.13
(1.82)

O.S.
5.75
(1.13)
6.88
(0.34)
4.31
(1.66)
5.31
(1.49)
5.31
(1.81)
3.75
(1.77)

T.A.
6.06
(1.12)
6.63
(0.62)
3.31
(2.06)
6.00
(1.32)
6.63
(0.81)
3.13
(1.75)

TALK
S.S
5.62
(1.36)
6.19
(1.28)
4.38
(2.06)
5.56
(1.26)
6.00
(1.32)
3.63
(1.67)

O.S.
5.88
(1.31)
6.56
(1.09)
3.44
(2.06)
5.69
(1.14)
6.31
(0.70)
2.94
(1.48)

TABLE 3: Mean and standard deviation of rating on three criteria for each task from EXP1.

View

First

Second

Value

Chi-square
p
Chi-square
p

T.A.
26.133
***
25.064
***

TV
S.S
24.667
***
21.708
***

O.S.
26.656
***
21.709
***

T.A.
20.981
***
26.772
***

SIT
S.S
13.000
**
18.681
***

O.S.
18.250
***
26.772
***

T.A.
13.609
**
10.292
**

LOOK
S.S
19.292
***
4.204
0.122

O.S.
21.000
***
8.739
*

T.A.
17.815
***
20.039
***

TALK
S.S
12.378
***
13.440
**

O.S.
22.627
***
26.920
***

TABLE 4: Friedman test results for each task from EXP1. (* < 0.05, ** < 0.01, *** < 0.001)

View

Comparison

First

Second

Our vs GT
Our vs Simple
GT vs Simple
Our vs GT
Our vs Simple
GT vs Simple

T.A.
**
*
***
1.000
**
**

TV
S.S
*
*
***
0.771
**
**

O.S.
**
*
***
1.000
**
**

T.A.
0.148
**
**
0.255
**
***

SIT
S.S
0.173
0.132
**
0.102
**
**

O.S.
0.182
*
**
0.137
**
***

T.A.
*
0.327
**
1.000
*
0.147

LOOK
S.S
*
*
**
0.839
0.837
0.126

O.S.
**
*
**
1.000
*
*

T.A.
0.101
**
**
0.142
**
**

TALK
S.S
0.364
*
*
0.173
0.066
*

O.S.
0.143
**
**
0.069
**
**

TABLE 5: Post-hoc analysis for each task from EXP1: Wilcoxon Signed Rank Test (Pair-wise) with Bonferroni correction (*
< 0.05, ** < 0.01, *** < 0.001).

Question
1. I remained focused on my partner

throughout our interaction.

2. My partner remained focused on
me throughout our interaction.
3. My partner‚Äôs thoughts were clear.

to me.

4. It was easy to understand my

partner.

5. It seems that my partner had come

to the place I was.

6. It seems that I and my partner was

together in the same place.

Our

GT

5.00 (1.10)

5.56 (1.03)

4.69 (1.01)

5.38 (0.96)

P

*

*

5.31 (1.49)

5.00 (0.89)

0.289

5.13 (1.15)

5.06 (1.24)

0.864

5.50 (1.10)

5.56 (1.21)

0.963

5.38 (0.96)

5.50 (1.26)

0.476

Question
1. It helps me be more effective.
2. It is useful.
3. It meets my needs.
4. It is easy to use.
5. It is simple to use.
6. It is user friendly.
7. I learned to use it quickly.
8. I easily remember how to use it.
9. It is easy to learn to use it.
10. I am satisÔ¨Åed with it.
11. I would recommend it to a friend.
12. It is fun to use.
13. It works the way I want it to work.

Our
5.18 (1.33)
5.19 (1.11)
4.94 (1.48)
5.63 (1.71)
6.06 (1.12)
5.31 (1.58)
5.81 (1.33)
6.00 (1.55)
5.44 (1.55)
4.63 (1.31)
4.81 (1.38)
4.81 (1.72)
4.75 (0.86)

GT
4.56 (1.37)
4.88 (1.26)
4.63 (1.71)
4.69 (1.40)
4.94 (1.53)
4.31 (1.49)
5.44 (1.03)
5.25 (1.00)
5.25 (1.00)
4.44 (1.36)
3.94 (1.18)
4.81 (1.56)
5.13 (1.63)

P
0.160
0.751
0.755
0.064
**
*
0.141
0.050
0.524
0.642
*
1.000
0.175

TABLE 6: Presence questionnaire results - Mean and stan-
dard deviation for each condition and p-value in EXP2 (* <
0.05).

want to recommend Our system to a friend more than the
manual system.

TABLE 7: Usability questionnaire results - Mean and stan-
dard deviation for each condition and p-value in EXP2 (* <
0.05, ** < 0.01).

4.5.4 Observation and General Feedback

For the placement evaluation (EXP1), participants found
that our method works well for the context from the given
tasks especially from the second person view. Some partic-
ipants noted that acquiring GT affects memory on the Ô¨Årst
person view, which has a favorable effect on GT method in
the Ô¨Årst person view. However, from the second person view
where the GT placement was acquired by the supporting
participant, the evaluation from the main participant was
not signiÔ¨Åcantly different between GT and Our method.

For the system evaluation (EXP2), many participants
mentioned that Our system was easy and fun to use and

they were surprised that the automatic placement was actu-
ally supporting the given tasks very well. On the other hand,
some participants also noted that GT system that allows
for exploring other party‚Äôs space was also interesting. Many
participants said that manual placement with input device
was time-consuming while some participants enjoyed the
process.

Overall, most participants expressed that they had a
hard time from a heavy device with VR sick as the exper-
iment went on for a while. One participant suggested that
voice chatting would be helpful for real application, but our
focus was mainly on evaluating placement this time.

TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, AUGUST 2020

14

5 DISCUSSION AND FUTURE WORK
In this paper, we presented a method for placing avatars to
preserve the meaning of various aspects of user‚Äôs placement
in a dissimilar indoor environment. A core technical compo-
nent for this was the neural network that estimates the simi-
larity between two placements from different spaces, trained
based on people‚Äôs preference for avatar positions obtained
through a user survey. We have shown the effectiveness of
our method through experiments with a VR and AR-based
telepresence system. In this section, we discuss several lim-
itations of our work and important future research themes.
First, we used only low-level features, such as geometry
and object category information, as inputs to our system.
With this approach, however, it was difÔ¨Åcult for the avatar
to preserve the meaning of the location intended by the
user when it can be interpreted in various ways (i.e., when
the avatar of the other party and many objects are in the
vicinity). Recognizing user‚Äôs actual attention will greatly
help avatar generate motions that reÔ¨Çect the user‚Äôs intended
movement. It would be also worth exploring alternative
feature models than ours (e.g., more sophisticated spatial
features that can distinguish subtle differences of furniture
arrangement) that can improve the placement quality.

Next, we conducted a user survey on a rather limited
number of space pairs. As shown in 5, the data shows a
statistically signiÔ¨Åcant distribution, and we were able to ap-
ply the trained model to indoor spaces from other datasets.
Nevertheless, more extensive user survey data on a wide
variety of spaces and human-avatar placements will help
achieve a more generalized avatar placement method, for
example by training neural networks in an end-to-end way
to discover placement-related features rather than manually
specifying features as done in this work.

In order to realize the avatar-based telepresence to be
used in the real world, additional progress in many di-
rections needs to be made with regard to the avatar an-
imation. In particular, generating avatar‚Äôs locomotion as
well as gesture and gaze animation to preserve the remote
user‚Äôs movement semantics remains as an important and
interesting future research direction.

Lastly, in this work we assumed two users, among whom
only one moves at a time. Further research is necessary to
accommodate more than two concurrently moving users.

ACKNOWLEDGMENTS
This work was partly supported by Samsung Science and
Technology Foundation (SRFC-IT1701-14).

REFERENCES

[2]

[1] A. Maimone, X. Yang, N. Dierk, A. State, M. Dou, and
H. Fuchs, ‚ÄúGeneral-purpose telepresence with head-worn optical
see-through displays and projector-based lighting,‚Äù in 2013 IEEE
Virtual Reality (VR).
IEEE, 2013, pp. 23‚Äì26.
S. Orts-Escolano, C. Rhemann, S. Fanello, W. Chang, A. Kowdle,
Y. Degtyarev, D. Kim, P. L. Davidson, S. Khamis, M. Dou,
V. Tankovich, C. Loop, Q. Cai, P. A. Chou, S. Mennicken,
J. Valentin, V. Pradeep, S. Wang, S. B. Kang, P. Kohli, Y. Lutchyn,
C. Keskin, and S. Izadi, ‚ÄúHoloportation: Virtual 3d teleportation
in real-time,‚Äù in Proceedings of the 29th Annual Symposium on
User Interface Software and Technology, ser. UIST ‚Äô16. New
York, NY, USA: ACM, 2016, pp. 741‚Äì754. [Online]. Available:
http://doi.acm.org/10.1145/2984511.2984517

[3] N. H. Lehment, D. Merget, and G. Rigoll, ‚ÄúCreating automatically
aligned consensus realities for ar videoconferencing,‚Äù in 2014 IEEE
International Symposium on Mixed and Augmented Reality (ISMAR),
Sept 2014, pp. 201‚Äì206.

[4] T. Pejsa, J. Kantor, H. Benko, E. Ofek, and A. Wilson, ‚ÄúRoom2room:
Enabling life-size telepresence in a projected augmented reality
environment,‚Äù in Proceedings of
the 19th ACM Conference on
Computer-Supported Cooperative Work & Social Computing, ser.
CSCW ‚Äô16. New York, NY, USA: ACM, 2016, pp. 1716‚Äì1725.
[Online]. Available: http://doi.acm.org/10.1145/2818048.2819965
[5] D. Jo, K.-H. Kim, and G. J. Kim, ‚ÄúSpacetime: Adaptive control of
the teleported avatar for improved ar tele-conference experience,‚Äù
Comput. Animat. Virtual Worlds, vol. 26, no. 3-4, pp. 259‚Äì269, May
2015. [Online]. Available: http://dx.doi.org/10.1002/cav.1645
I. Rae, B. Mutlu, G. M. Olson,
J. S. Olson, L. Takayama,
and G. Venolia, ‚ÄúEveryday telepresence: Emerging practices
and future research directions,‚Äù in Proceedings of
the 33rd
Annual ACM Conference Extended Abstracts on Human Factors
in Computing Systems, ser. CHI EA ‚Äô15. New York, NY,
USA: ACM, 2015, pp. 2409‚Äì2412.
[Online]. Available: http:
//doi.acm.org/10.1145/2702613.2702639

[6]

[7] M. Schlager and Y. Wang, ‚ÄúReframing and researching everyday
telepresence for families,‚Äù in Proceedings of CHI 2015 Workshop
on Everyday Telepresence: Emerging Practices and Future Research
Directions, 2015.

[8] T. Piumsomboon, G. A. Lee, J. D. Hart, B. Ens, R. W. Lindeman,
B. H. Thomas, and M. Billinghurst, ‚ÄúMini-me: An adaptive avatar
for mixed reality remote collaboration,‚Äù in Proceedings of the 2018
CHI conference on human factors in computing systems, 2018, pp. 1‚Äì13.
[9] B. Thoravi Kumaravel, F. Anderson, G. Fitzmaurice, B. Hartmann,
and T. Grossman, ‚ÄúLoki: Facilitating remote instruction of physical
tasks using bi-directional mixed-reality telepresence,‚Äù in Proceed-
ings of the 32nd Annual ACM Symposium on User Interface Software
and Technology, 2019, pp. 161‚Äì174.

[10] M. Gleicher, ‚ÄúRetargetting motion to new characters,‚Äù in
Proceedings of the 25th Annual Conference on Computer Graphics
and Interactive Techniques, ser. SIGGRAPH ‚Äô98. New York,
[Online]. Available: http:
NY, USA: ACM, 1998, pp. 33‚Äì42.
//doi.acm.org/10.1145/280814.280820

[11] K.-J. Choi and H.-S. Ko, ‚ÄúOnline motion retargetting,‚Äù The Journal
of Visualization and Computer Animation, vol. 11, no. 5, pp. 223‚Äì235,
2000.

[12] E. S. L. Ho, T. Komura, and C.-L. Tai, ‚ÄúSpatial relationship
preserving character motion adaptation,‚Äù ACM Trans. Graph.,
vol. 29, no. 4, pp. 33:1‚Äì33:8,
[Online]. Available:
http://doi.acm.org/10.1145/1778765.1778770

Jul. 2010.

[13] R. A. Al-Asqhar, T. Komura, and M. G. Choi, ‚ÄúRelationship
descriptors for interactive motion adaptation,‚Äù in Proceedings of
the 12th ACM SIGGRAPH/Eurographics Symposium on Computer
Animation, ser. SCA ‚Äô13. New York, NY, USA: ACM, 2013, pp.
45‚Äì53. [Online]. Available: http://doi.acm.org/10.1145/2485895.
2485905

[14] H. Wang, K. A. Sidorov, P. Sandilands, and T. Komura,
‚ÄúHarmonic parameterization by electrostatics,‚Äù ACM Trans.
Graph., vol. 32, no. 5, pp. 155:1‚Äì155:12, Oct. 2013. [Online].
Available: http://doi.acm.org/10.1145/2503177

[15] Y. Kim, H. Park, S. Bang, and S.-H. Lee, ‚ÄúRetargeting human-object
interaction to virtual avatars,‚Äù IEEE Transactions on Visualization
and Computer Graphics, vol. 22, no. 11, pp. 2405‚Äì2412, Nov 2016.

[16] T. Jin, M. Kim, and S.-H. Lee, ‚ÄúAura mesh: Motion retargeting
to preserve the spatial relationships between skinned characters,‚Äù
Computer Graphics Forum, vol. 37, no. 2, pp. 311‚Äì320, 2018.

[17] A. Gupta, S. Satkin, A. A. Efros, and M. Hebert, ‚ÄúFrom 3d scene
geometry to human workspace,‚Äù in Computer Vision and Pattern
Recognition (CVPR), 2011 IEEE Conference on. IEEE, 2011, pp. 1961‚Äì
1968.

[18] M. Savva, A. X. Chang, P. Hanrahan, M. Fisher, and M. Nie√üner,
‚ÄúScenegrok: Inferring action maps in 3d environments,‚Äù ACM
Trans. Graph., vol. 33, no. 6, pp. 212:1‚Äì212:10, Nov. 2014. [Online].
Available: http://doi.acm.org/10.1145/2661229.2661230

[19] M. Savva, A. X. Chang, P. Hanrahan, M. Fisher, and M. Niessner,
‚ÄúPigraphs: Learning interaction snapshots from observations,‚Äù
ACM Transactions on Graphics (TOG), vol. 35, no. 4, p. 139, 2016.

[20] X. Li, S. Liu, K. Kim, X. Wang, M.-H. Yang, and J. Kautz, ‚ÄúPutting
humans in a scene: Learning affordance in 3d indoor environ-
ments,‚Äù in Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, 2019, pp. 12 368‚Äì12 376.

TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, AUGUST 2020

15

[21] J. J. Gibson, The concept of affordances. Perceiving, acting, and know-

[44] C. Harms and F. Biocca, ‚ÄúInternal consistency and reliability of the

ing. Wiley, 1977.

[22] S. Pirk, V. Krs, K. Hu, S. D. Rajasekaran, H. Kang,
Y. Yoshiyasu, B. Benes, and L. J. Guibas, ‚ÄúUnderstanding and
exploiting object interaction landscapes,‚Äù ACM Trans. Graph.,
vol. 36, no. 3, pp. 31:1‚Äì31:14, Jun. 2017. [Online]. Available:
http://doi.acm.org/10.1145/3083725

[23] R. Hu, O. van Kaick, B. Wu, H. Huang, A. Shamir, and H. Zhang,
‚ÄúLearning how objects function via co-analysis of interactions,‚Äù
ACM Trans. Graph., vol. 35, no. 4, pp. 47:1‚Äì47:13, Jul. 2016.
[Online]. Available: http://doi.acm.org/10.1145/2897824.2925870
[24] X. Zhao, M. G. Choi, and T. Komura, ‚ÄúCharacter-object interaction
retrieval using the interaction bisector surface,‚Äù Computer Graphics
Forum, vol. 36, no. 2, pp. 119‚Äì129, May 2017.

[25] R. Hu, C. Zhu, O. van Kaick, L. Liu, A. Shamir, and H. Zhang,
‚ÄúInteraction context (icon): Towards a geometric functionality
descriptor,‚Äù ACM Transactions on Graphics (TOG), vol. 34, no. 4,
p. 83, 2015.

[26] V. G. Kim, S. Chaudhuri, L. Guibas, and T. Funkhouser,
‚ÄúShape2pose: Human-centric shape analysis,‚Äù ACM Trans. Graph.,
vol. 33, no. 4, pp. 120:1‚Äì120:12, Jul. 2014. [Online]. Available:
http://doi.acm.org/10.1145/2601097.2601117

[27] E. T. Hall, ‚ÄúThe hidden dimension,‚Äù 1966.
[28] A. Guye-Vuilleme, T. K. Capin, S. Pandzic, N. M. Thalmann, and
D. Thalmann, ‚ÄúNonverbal communication interface for collabora-
tive virtual environments,‚Äù Virtual Reality, vol. 4, no. 1, pp. 49‚Äì59,
1999.

[29] J. N. Bailenson, J. Blascovich, A. C. Beall, and J. M. Loomis, ‚ÄúInter-
personal distance in immersive virtual environments,‚Äù Personality
Soc. Psychol. Bull., vol. 29, no. 7, pp. 819‚Äì833, 2003.

[30] L. M. Wilcox, R. S. Allison, S. Elfassy, and C. Grelik,
‚ÄúPersonal space in virtual reality,‚Äù ACM Trans. Appl. Percept.,
vol. 3, no. 4, pp. 412‚Äì428, Oct. 2006.
[Online]. Available:
http://doi.acm.org/10.1145/1190036.1190041

[31] C. Pedica and H. H ¬®ogni Vilhj¬¥almsson, ‚ÄúSpontaneous avatar behav-
ior for human territoriality,‚Äù Applied ArtiÔ¨Åcial Intelligence, vol. 24,
no. 6, pp. 575‚Äì593, 2010.

[32] A. E. ScheÔ¨Çen and N. Ashcraft, ‚ÄúHuman territories: How we

behave in space-time.‚Äù 1976.

[33] Y. Huang and M. Kallmann, ‚ÄúPlanning motions and placements
for virtual demonstrators,‚Äù IEEE Transactions on Visualization and
Computer Graphics, vol. 22, no. 5, pp. 1568‚Äì1579, May 2016.

[34] K. Q. Weinberger, J. Blitzer, and L. K. Saul, ‚ÄúDistance metric learn-
ing for large margin nearest neighbor classiÔ¨Åcation,‚Äù in Advances
in neural information processing systems, 2006, pp. 1473‚Äì1480.
[35] J. V. Davis, B. Kulis, P. Jain, S. Sra, and I. S. Dhillon, ‚ÄúInformation-
theoretic metric learning,‚Äù in Proceedings of the 24th international
conference on Machine learning. ACM, 2007, pp. 209‚Äì216.

[36] B. McFee and G. R. Lanckriet, ‚ÄúMetric learning to rank,‚Äù in
Proceedings of the 27th International Conference on Machine Learning
(ICML-10), 2010, pp. 775‚Äì782.

[37] E. Hoffer and N. Ailon, ‚ÄúDeep metric learning using triplet net-
work,‚Äù in International Workshop on Similarity-Based Pattern Recog-
nition. Springer, 2015, pp. 84‚Äì92.

[38] J. Wang, Y. Song, T. Leung, C. Rosenberg, J. Wang, J. Philbin,
B. Chen, and Y. Wu, ‚ÄúLearning Ô¨Åne-grained image similarity with
deep ranking,‚Äù in Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, 2014, pp. 1386‚Äì1393.

[39] C. Lei, D. Liu, W. Li, Z.-J. Zha, and H. Li, ‚ÄúComparative deep
learning of hybrid representations for image recommendations,‚Äù
in Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, 2016, pp. 2545‚Äì2553.

[40] R. Lu, K. Wu, Z. Duan, and C. Zhang, ‚ÄúDeep ranking: Triplet
matchnet for music metric learning,‚Äù in Acoustics, Speech and Signal
Processing (ICASSP), 2017 IEEE International Conference on.
IEEE,
2017, pp. 121‚Äì125.

[41] X. Qi, D. Yang, and X. Chen, ‚ÄúAudio feature learning with triplet-
based embedding network.‚Äù in AAAI, 2017, pp. 4979‚Äì4980.
[42] R. C. Eberhart and Y. Shi, ‚ÄúComparing inertia weights and con-
striction factors in particle swarm optimization,‚Äù in Proceedings
of the 2000 congress on evolutionary computation. CEC00 (Cat. No.
00TH8512), vol. 1.

IEEE, 2000, pp. 84‚Äì88.
[43] S. Song, F. Yu, A. Zeng, A. X. Chang, M. Savva, and T. Funkhouser,
‚ÄúSemantic scene completion from a single depth image,‚Äù Pro-
ceedings of 30th IEEE Conference on Computer Vision and Pattern
Recognition, 2017.

networked minds measure of social presence,‚Äù 2004.

[45] M. Lombard, T. B. Ditton, and L. Weinstein, ‚ÄúMeasuring presence:
the temple presence inventory,‚Äù in Proceedings of the 12th annual
international workshop on presence, 2009, pp. 1‚Äì15.

[46] A. M. Lund, ‚ÄúMeasuring usability with the use questionnaire12,‚Äù

Usability interface, vol. 8, no. 2, pp. 3‚Äì6, 2001.

Leonard Yoon is a Ph.D. candidate with the
Graduate School of Culture Technology at
KAIST. He received the M.S. degree in Electrical
Engineering from KAIST, Korea, in 2014 and
B.S. degree in Electrical and Computer Engi-
neering from University of California, San Diego,
USA, in 2009. His research interests include 3D
scene understanding and AR telepresence.

Dongseok Yang is a Ph.D. candidate with
the Graduate School of Culture Technology at
KAIST. He received the M.S. degree in Culture
Technology from KAIST, Korea, in 2020 and B.S.
degree in Multimedia Engineering from Dongguk
University, Korea, in 2018. His research interests
include real-time character animation synthesis
and AR telepresence.

Jaehyun Kim is a Ph.D. candidate with Program
of Brain and Cognitive Engineering in Depart-
ment of Bio and Brain Engineering at KAIST. His
research interests include brain-inspired artiÔ¨Åcial
intelligence for spatial recognition, path Ô¨Ånding,
and sequential decision making. He received the
M.S. degree in Culture Technology from KAIST,
in 2016 and B.S. degree in Media / Information
and Computer Engineering from Ajou University,
in 2011.

Choongho Chung is a Ph.D. candidate with
the Graduate School of Culture Technology at
KAIST. He received the M.S degree in Culture
Technology from KAIST, Korea, in 2018 and a
B.S degree in Mechanical Engineering in KAIST,
Korea, in 2014. He is interested in telepresence
applications, human attention recognition, and
generation of character animation.

Sung-Hee Lee is an Associate Professor with
the Graduate School of Culture Technology
at KAIST. His research interests include au-
tonomous human animation, avatar motion gen-
eration, and human modeling. He received the
Ph.D. degree in Computer Science from Univer-
sity of California, Los Angeles, USA, in 2008,
and the B.S. and the M.S. degree in Mechani-
cal Engineering from Seoul National University,
Korea, in 1996 and 2000, respectively.

