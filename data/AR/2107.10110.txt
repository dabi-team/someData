1
2
0
2

v
o
N
7

]
L
M

.
t
a
t
s
[

2
v
0
1
1
0
1
.
7
0
1
2
:
v
i
X
r
a

On the Convergence of Prior-Guided Zeroth-Order
Optimization Algorithms

Shuyu Cheng, Guoqiang Wu,

Jun Zhu∗

Dept. of Comp. Sci. and Tech., BNRist Center, State Key Lab for Intell. Tech. & Sys., Institute for AI,
Tsinghua-Bosch Joint Center for ML, Tsinghua University, Beijing, 100084, China
Pazhou Lab, Guangzhou, 510330, China
chengsy18@mails.tsinghua.edu.cn, guoqiangwu90@gmail.com, dcszj@tsinghua.edu.cn

Abstract

Zeroth-order (ZO) optimization is widely used to handle challenging tasks, such
as query-based black-box adversarial attacks and reinforcement learning. Various
attempts have been made to integrate prior information into the gradient estimation
procedure based on ﬁnite differences, with promising empirical results. However,
their convergence properties are not well understood. This paper makes an attempt
to ﬁll up this gap by analyzing the convergence of prior-guided ZO algorithms
under a greedy descent framework with various gradient estimators. We provide
a convergence guarantee for the prior-guided random gradient-free (PRGF) algo-
rithms. Moreover, to further accelerate over greedy descent methods, we present
a new accelerated random search (ARS) algorithm that incorporates prior infor-
mation, together with a convergence analysis. Finally, our theoretical results are
conﬁrmed by experiments on several numerical benchmarks as well as adversarial
attacks. Our code is available at https://github.com/csy530216/pg-zoo.

1

Introduction

Zeroth-order (ZO) optimization [22] provides powerful tools to deal with challenging tasks, such as
query-based black-box adversarial attacks [8, 15], reinforcement learning [29, 21, 10], meta-learning
[1], and hyperparameter tuning [30], where the access to gradient information is either not available or
too costly. ZO methods only assume an oracle access to the function value at any given point, instead
of gradients as in ﬁrst-order methods. The primary goal is to ﬁnd a solution with as few queries to the
function value oracle as possible. Recently, various ZO methods have been proposed in two main
categories. One type is to obtain a gradient estimator and plug in some gradient-based methods. [26]
analyzes the convergence of such methods with a random gradient-free (RGF) estimator obtained
via ﬁnite difference along a random direction. The other type is directed search, where the update
only depends on comparison between function values at different points. Such methods are robust
against monotone transform of the objective function [31, 11, 4]. However, as they do not directly
utilize function values, their query complexity is often higher than that of ﬁnite difference methods.
Therefore, we focus on the ﬁrst type of methods in this paper. Other methods exist such as CMA-ES
[12] which is potentially better on objective functions with a rugged landscape, but lacks a general
convergence guarantee.

ZO methods are usually less efﬁcient than ﬁrst-order algorithms, as they typically take O(d) queries
to reach a given precision, where d is the input dimension (see Table 1 in [11]). Speciﬁcally, for
the methods in [26], the oracle query count is O(d) times larger than that of their corresponding
schemes using gradients. This inefﬁciency stems from random search along uniformly distributed
directions. To improve, various attempts have been made to augment random search with (extra)

∗J.Z. is the Corresponding Author. G.W. is now with School of Software, Shandong University.

35th Conference on Neural Information Processing Systems (NeurIPS 2021).

 
 
 
 
 
 
prior information. For instance, [16, 23] use a time-dependent prior (i.e., the gradient estimated in the
last iteration), while [20, 9, 5] use surrogate gradients obtained from other sources.2 Among these
methods, [16, 20, 9, 23] propose objective functions to describe the quality of a gradient estimator
for justiﬁcation or optimizing its hyperparameters; for example, [23] uses a subspace estimator that
maximizes its squared cosine similarity with the true gradient as the objective, and ﬁnds a better
descent direction than both the prior direction and the randomly sampled direction. However, all
these methods treat gradient estimation and the optimization algorithm separately, and it remains
unclear whether are these gradient estimators and the corresponding prior-exploiting optimization
methods theoretically sound? and what role does the prior play in accelerating convergence?

In this paper, we attempt to answer these questions by establishing a formal connection between
convergence rates and the quality of gradient estimates. Further, we develop a more efﬁcient ZO
algorithm with prior inspired by a theoretical analysis. First, we present a greedy descent framework of
ZO methods and provide its convergence rate under smooth convex optimization, which is positively
related to the squared cosine similarity between the gradient estimate and true gradient. As shown
by [23] and our results, given some ﬁnite-difference queries, the optimal estimator maximizing the
squared cosine similarity is the projection of true gradient on the subspace spanned by those queried
directions. In the case with prior information, a natural such estimator is in the same form as the
one in [23]. We call it PRGF estimator and analyze its convergence rate.3 Our results show that
no matter what the prior is, the convergence rate of PRGF is at least the same as that of the RGF
baseline [26, 18], and it could be signiﬁcantly improved if given a useful prior.4 Such results shed
light on exploring prior information in ZO optimization to accelerate convergence.

Then, as a concrete example, we apply the analysis to History-PRGF [23], which uses historical
information (i.e., gradient estimate in the last iteration) as the prior. [23] presents an analysis on linear
functions, yet still lacking a convergence rate. We analyze on general L-smooth functions, and ﬁnd
that when the learning rate is smaller than the optimal value 1/L in smooth optimization, the expected
squared cosine similarity could converge to a larger value, which compensates for the slowdown of
convergence brought by the inappropriate choice of learning rate. We also show that History-PRGF
admits a convergence rate independent of learning rate as long as it is in a fairly wide range.

Finally, to further accelerate greedy descent methods, we present Prior-Guided ARS (PARS), a new
variant of Accelerated Random Search (ARS) [26] to utilize prior information. Technically, PARS
is a non-trivial extension of ARS, as directly replacing the gradient estimator in ARS by the PRGF
estimator would lose the convergence analysis. Thus, we present necessary extensions to ARS, and
show that PARS has a convergence rate no worse than that of ARS and admits potential acceleration
given a good prior. In particular, when the prior is chosen as the historical information, the resulting
History-PARS is robust to learning rate in experiments. To our knowledge, History-PARS is the
ﬁrst ARS-based method that is empirically robust while retaining the convergence rate as ARS. Our
experiments on numerical benchmarks and adversarial attacks conﬁrm the theoretical results.

2 Setups

Assumptions on the problem class We consider unconstrained optimization, where the objective
function f : Rd → R is convex and L-smooth for L ≥ 0. Optionally, we require f to be τ -strongly
convex for τ > 0. We leave deﬁnitions of these concepts to Appendix A.1.

Directional derivative oracle
In ZO optimization, we follow the ﬁnite difference approach, which
makes more use of the queried function values than direct search and provides better gradient
approximations than alternatives [3]. In particular, we consider the forward difference method:

gµ(v; x) :=

f (x + µv) − f (x)
µ

≈ ∇f (x)(cid:62)v,

(1)

2Some work [8, 16, 32] restricts the random search to a more effective subspace reﬂecting the prior knowledge.

But, this eliminates the possibility of convergence to the optimal solution.
3Note that the estimator is different from the P-RGF estimator in [9].
4In this paper, RGF and PRGF could refer to either a gradient estimator or the greedy descent algorithm with

the corresponding estimator, depending on the context.

2

Algorithm 1 Greedy descent framework
Input: L-smooth convex function f ; initialization x0; upper bound ˆL ( ˆL ≥ L); iteration number T .
Output: xT as the approximate minimizer of f .
1: for t = 0 to T − 1 do
2:
3:
4: end for
5: return xT .

Let vt be a random vector s.t. (cid:107)vt(cid:107) = 1;
xt+1 ← xt − 1
ˆL

gt, where gt ← ∇f (xt)(cid:62)vt · vt;

where v is a vector with unit (cid:96)2 norm (cid:107)v(cid:107) = 1 and µ is a small positive step. As long as the objective
function is smooth, the error between the ﬁnite difference and the directional derivative could be
uniformly bounded, as shown in the following proposition (see Appendix A.2 for its proof).
Proposition 1. If f is L-smooth, then for any (x, v) with (cid:107)v(cid:107) = 1, |gµ(v; x) − ∇f (x)(cid:62)v| ≤ 1
2 Lµ.
Thus, in smooth optimization, the error brought by ﬁnite differences to the convergence bound can be
analyzed in a principled way, and its impact tends to zero as µ → 0. We also choose µ as small as possi-
ble in practice. Hence, in the following analysis we directly assume the directional derivative oracle:
suppose that we can obtain ∇f (x)(cid:62)v for any (x, v) in which (cid:107)v(cid:107) = 1 with one query.

3 Greedy descent framework and PRGF algorithm

We now introduce a greedy descent framework in ZO optimization which can be implemented with
various gradient estimators. We ﬁrst provide a general analysis, followed by a concrete example.

3.1 The greedy descent framework and general analysis

In ﬁrst-order smooth convex optimization, a sufﬁcient single-step decrease of the objective can
guarantee convergence (see Chapter 3.2 in [6]). Inspired by this fact, we design the update in an
iteration to greedily seek for maximum decrease. Suppose we are currently at x, and want to update
along the direction v. Without loss of generality, assume (cid:107)v(cid:107) = 1 and ∇f (x)(cid:62)v > 0. To choose a
suitable step size η that minimizes f (x − ηv), we note that

f (x − ηv) ≤ f (x) − η∇f (x)(cid:62)v +

1
2

Lη2 := F (η)

(2)

L

2L

when η = ∇f (x)(cid:62)v

by smoothness of f . For the r.h.s, we have F (η) = f (x) − (∇f (x)(cid:62)v)2
, which
minimizes F (η). Thus, choosing such η could lead to a largest guaranteed decrease of f (x−ηv) from
f (x). In practice, the value of L is often unknown, but we can verify that as long as 0 < η ≤ ∇f (x)(cid:62)v
,
then f (x − ηv) ≤ F (η) < f (x), i.e., we can guarantee decrease of the objective (regardless of the
direction of v if ∇f (x)(cid:62)v > 0). Based on the above discussion, we further allow v to be random and
present the greedy descent framework in Algorithm 1.
Remark 1. If vt ∼ U(Sd−1), i.e. vt is uniformly sampled from Sd−1 (the unit sphere in Rd), then
Algorithm 1 is similar to the simple random search in [26] except that vt is sampled from a Gaussian
distribution there.
Remark 2. In general, vt could depend on the history (i.e., the randomness before sampling vt). For
example, vt can be biased towards a vector pt that corresponds to prior information, and pt depends
on the history since it depends on xt or the optimization trajectory.

L

Theoretically speaking, vt is sampled from the conditional probability distribution Pr(·|Ft−1) where
Ft−1 is a sub σ-algebra modelling the historical information. Ft−1 is important in our theoretical
analysis since it tells how to perform conditional expectation given the history.

We always require that Ft−1 includes all the randomness before iteration t to ensure that Lemma 1
(and thus Theorems 1 and 2) and Theorem 5 hold. For further theoretical analysis of various
implementations of the framework, Ft−1 remains to be speciﬁed by possibly also including some
randomness in iteration t (see e.g. Example 2 as an implementation of Algorithm 1).5

5In mathematical language, if Ft−1 includes (and only includes) the randomness brought by random vectors
{x1, x2, . . . , xn}, then Ft−1 is the σ-algebra generated by {x1, x2, . . . , xn}: Ft−1 is the smallest σ-algebra
s.t. xi is Ft−1-measurable for all 1 ≤ i ≤ n.

3

By Remark 2, we introduce Et[·] := E[·|Ft−1] to denote the conditional expectation given the history.
In Algorithm 1, under a suitable choice of Ft−1, Et[·] roughly means only taking expectation w.r.t. vt.
We let v := v/(cid:107)v(cid:107) denote the (cid:96)2 normalization of vector v. We let x∗ denote one of the minimizers of
f (we assume such minimizer exists), and δt := f (xt) − f (x∗). Thanks to the descent property in
Algorithm 1, we have the following lemma on single step progress.
(cid:17)2

(cid:16)

(cid:62)

∇f (xt)

vt

and L(cid:48)

:=

L

1−(1− L
ˆL

)2 , then in

Lemma 1 (Proof in Appendix B.1). Let Ct :=
Algorithm 1, we have

Et[δt+1] ≤ δt −

Et[Ct]
2L(cid:48) (cid:107)∇f (xt)(cid:107)2.

(3)

ˆL
2 ≤ L(cid:48) ≤ ˆL and L(cid:48) ≥ L.

We note that L(cid:48) = ˆL/(2−L/ˆL), so
To obtain a bound on E[δT ], one of the classical proofs (see e.g., Theorem 1 in [25]) requires us to take
expectation on both sides of Eq. (3). Allowing the distribution of vt to be dependent on the history
leads to additional technical difﬁculty: in the r.h.s of Eq. (3), Et[Ct] becomes a random variable that
is not independent of (cid:107)∇f (xt)(cid:107)2. Thus, we cannot simplify the term E[Et[Ct](cid:107)∇f (xt)(cid:107)2] if we take
expectation.6 By using other techniques, we obtain the following main theorems on convergence rate.
in Appendix B.1). Let R :=
Theorem 1 (Algorithm 1,
maxx:f (x)≤f (x0) (cid:107)x − x∗(cid:107) and suppose R < ∞. Then, in Algorithm 1, we have

smooth and convex; proof

E[δT ] ≤

(cid:104)

2L(cid:48)R2 (cid:80)T −1
t=0
T (T + 1)

E

1
Et[Ct]

(cid:105)

.

(4)

Theorem 2 (Algorithm 1, smooth and strongly convex; proof in Appendix B.1). If f is also τ -strongly
convex, then we have





E

exp

(cid:16)

− τ
L(cid:48)

δT
(cid:80)T −1
t=0



 ≤ δ0.

(cid:17)
Et[Ct]

(5)

Remark 3. In our results, the convergence rate depends on Et[Ct] in a more complicated way than
only depending on E[Ct] = E[Et[Ct]]. For concrete cases, one may need to study the concentration
properties of Et[Ct] besides its expectation, as in the proofs of Theorems 3 and 4 when we analyze
History-PRGF, a special implementation in the greedy descent framework.
In the strongly convex case, currently we cannot directly obtain a ﬁnal bound of E[δT ]. However,
the form of Eq. (5) is still useful since it roughly tells us that δT converges as the denominator
. For History-PRGF, we will turn this intuition into a rigorous theorem
exp
(Theorem 4) since in that case we can prove that the denominator has a nice concentration property.
Remark 4. If we have a lower bound of Et[Ct], e.g. Et[Ct] ≥ a > 0, then we directly obtain
that E[δT ] ≤ 2L(cid:48)R2
L(cid:48) aT ) for Theorem 2. These could
recover the ideal convergence rate for RGF estimator and the worst-case convergence rate for PRGF
estimator, as explained in Examples 1 and 2.

a(T +1) for Theorem 1, and E[δT ] ≤ δ0 exp(− τ

(cid:80)T −1
t=0

Et[Ct]

− τ
L(cid:48)

(cid:17)

(cid:16)

From Theorems 1 and 2, we see that a larger value of Ct would lead to a better bound. To ﬁnd a
good choice of vt in Algorithm 1, it becomes natural to discuss the following problem. Suppose in an
iteration in Algorithm 1, we query the directional derivative oracle at xt along q directions {ui}q
i=1
(maybe randomly chosen) and obtain the values of {∇f (xt)(cid:62)ui}q
i=1. We could use this information
to construct a vector vt. What is the vt that maximizes Ct s.t. (cid:107)vt(cid:107) = 1? To answer this question, we
give the following proposition based on Proposition 1 in [23] and additional justiﬁcation.
Proposition 2 (Optimality of subspace estimator; proof in Appendix B.2). In one iteration of
Algorithm 1, if we have queried {∇f (xt)(cid:62)ui}q
i=1, then the optimal vt maximizing Ct s.t. (cid:107)vt(cid:107) = 1
should be in the following form: vt = ∇f (xt)A, where A := span{u1, u2, . . . , uq} and ∇f (xt)A
denotes the projection of ∇f (xt) onto A.

6It is also the difﬁculty we faced in our very preliminary attempts of the theoretical analysis when f is not

convex, and we leave its solution or workaround in the non-convex case as future work.

4

Note that in Line 3 of Algorithm 1, we have gt = ∇f (xt)(cid:62)∇f (xt)A · ∇f (xt)A = ∇f (xt)A.
Therefore, the gradient estimator gt is equivalent to the projection of the gradient to the subspace A,
which justiﬁes its name of subspace estimator. Next we discuss some special cases of the subspace
estimator. We leave detailed derivation in following examples to Appendix B.3.
Example 1 (RGF). ui ∼ U(Sd−1) for i = 1, 2, . . . , q. Without loss of generality, we assume
they are orthonormal (e.g., via Gram-Schmidt orthogonalization).7 The corresponding estimator
gt = (cid:80)q
i=1 ∇f (xt)(cid:62)ui · ui (vt = gt). When q = 1, the estimator is similar to the random
gradient-free oracle in [26]. With q ≥ 1, it is essentially the same as the stochastic subspace
estimator with columns from Haar-distributed random orthogonal matrix [18] and similar to the
orthogonal ES estimator [10]. In theoretical analysis, we let Ft−1 only include the randomness
before iteration t, and then we can prove that Et[Ct] = q
d . By Theorems 1 and 2, the convergence rate
q
is E[δT ] ≤
d T ) for smooth and strongly
convex case. The bound is the same as that in [18]. Since the query complexity in each iteration is
proportional to q, the bound for total query complexity is indeed independent of q.

for smooth convex case, and E[δT ] ≤ δ0 exp(− τ
L(cid:48)

2L(cid:48)R2 d
q
T +1

Example 2 (PRGF). With slight notation abuse, we assume the subspace in Proposition 2 is spanned
by {p1, . . . , pk, u1, . . . , uq}, so each iteration takes q + k queries. Let p1, · · · , pk be k non-zero
vectors corresponding to the prior message (e.g. the historical update, or the gradient of a surrogate
model), and ui ∼ U(Sd−1) for i = 1, 2, . . . , q. We note that intuitively we cannot construct a better
subspace since the only extra information we know is the k priors. In our analysis, we assume k = 1
for convenience, and we change the original notation p1 to pt to explicitly show the dependence of pt
on the history. We note that pt could also depend on extra randomness in iteration t (see e.g. the
speciﬁcation of pt in Appendix D.1.1). For convenience of theoretical analysis, we require that pt
is determined before sampling {u1, u2, . . . , uq}, and let Ft−1 also include the extra randomness of
pt in iteration t (not including the randomness of {u1, u2, . . . , uq}) besides the randomness before
iteration t. Then pt is always Ft−1-measurable, i.e. determined by the history. Without loss of
generality, we assume {pt, u1, . . . , uq} are orthonormal (e.g., via Gram-Schmidt orthogonalization).
The corresponding estimator gt = ∇f (xt)(cid:62)pt · pt + (cid:80)q
i=1 ∇f (xt)(cid:62)ui · ui (vt = gt), which is
similar to the estimator in [23]. By [23] (the expected drift of X 2
t in its Theorem 1), we have
Lemma 2 (Proof in Appendix B.3.4). In Algorithm 1 with PRGF estimator, Et[Ct] = Dt + q

d−1 (1 −

Dt) where Dt :=

(cid:16)

∇f (xt)

(cid:62)

(cid:17)2

.

pt

Hence Et[Ct] ≥ q
d holds. By Remark 4, PRGF admits a guaranteed convergence rate of RGF and is
potentially better given a good prior (if Dt is large), but it costs an additional query per iteration.
This shows soundness of the PRGF algorithm. For further theoretical analysis, we need to bound Dt.
This could be done when using the historical prior introduced in Section 3.2 (see Lemma 3). Bounding
Dt is usually challenging when a general prior is adopted, but if the prior is an approximate gradient
(such case appears in [20]), it may be possible. We leave related investigation as future work.

3.2 Analysis on the PRGF algorithm with the historical prior

We apply the above analysis to a concrete example of the History-PRGF estimator [23], which
considers the historical prior in the PRGF estimator.
In this case, Lemma 1, Theorem 1 and
Theorem 2 will manifest themselves by clearly stating the convergence rate which is robust to the
learning rate.

Speciﬁcally, History-PRGF considers the historical prior as follows: we choose pt = gt−1, i.e.,
we let the prior be the direction of the previous gradient estimate.8 This is equivalent to letting
pt = vt−1. Thus, in Algorithm 1, vt = ∇f (xt)A = ∇f (xt)(cid:62)vt−1 · vt−1 + (cid:80)q
i=1 ∇f (xt)(cid:62)ui · ui.
In this form we require {vt−1, u1, . . . , uq} to be orthonormal, so we ﬁrst determine vt−1, and then

7The computational complexity of Gram-Schmidt orthogonalization over q vectors in Rd is O(q2d). There-
fore, with a moderate value of q (e.g. q ∈ [10, 20] in our experiments), its cost is usually much smaller than that
brought by O(q) function evaluations used to approximate the directional derivatives. We note that when using a
numerical computing framework, for orthogonalization one could also adopt an efﬁcient implementation, by
calling the QR decomposition procedure such as torch.linalg.qr in PyTorch.

8One can also utilize multiple historical priors (e.g.
experimented in [23]), but here we only analyze the k = 1 case.

the last k updates with k > 1, as proposed and

5

i=1.

i=1 in A⊥, the (d − 1)-dimensional subspace of Rd perpendicular to vt−1, and then do

sample {ui}q
Gram-Schmidt orthonormalization on {ui}q
To study the convergence rate, we ﬁrst study evolution of Ct under a general L-smooth function. This
extends the analysis on linear functions (corresponding to L = 0) in [23]. Under the framework of
Algorithm 1, intuitively, the change of the gradient should be smaller when the objective function is
very smooth (L is small) or the learning rate is small ( ˆL is large). Since we care about the cosine
similarity between the gradient and the prior, we prove the following lemma:
Lemma 3 (Proof in Appendix B.4). In History-PRGF (pt = vt−1), we have Dt ≥ (1 − L/ˆL)2 Ct−1.

When ˆL = L, i.e., using the optimal learning rate, Lemma 3 does not provide a useful bound, since an
optimal learning rate in smooth optimization could ﬁnd an approximate minimizer along the update
direction, so the update direction may be useless in next iteration. In this case, the historical prior
does not provide acceleration. Hence, Lemma 3 explains the empirical ﬁndings in [23] that past
directions can be less useful when the learning rate is larger. However, in practice we often use a
conservative learning rate, for the following reasons: 1) We usually do not know L, and the cost of
tuning learning rate could be large; 2) Even if L is known, it only provides a global bound, so a ﬁxed
learning rate could be too conservative in the smoother local regions. In scenarios where ˆL is too
conservative ( ˆL > L), History-PRGF could bring more acceleration over RGF.
By Lemma 3, we can assume Dt = (1 − L/ˆL)2 Ct−1 to obtain a lower bound of quantities about Ct.
Meanwhile, since Dt means quality of the prior, the construction in Example 2 tells us relationship
between Ct and Dt. Then we have full knowledge of the evolution of Ct, and thus Et[Ct]. In
Appendix B.4, we discuss about evolution of E[Ct] and show that E[Ct] → O( q
d ≤ L
≤ 1.
ˆL
d
Et[Ct]
L(cid:48) ≈ q
Therefore, by Lemma 1, assuming Et[Ct] concentrates well around E[Ct] and hence
dL ,
PRGF could recover the single step progress with optimal learning rate ( ˆL = L), since Eq. (3) only
Et[Ct]
L(cid:48) which is constant w.r.t. L(cid:48) now. While the above discussion is informal, based on
depends on
Theorems 1 and 2, we prove following theorems which show that convergence rate of History-PRGF
is robust to choice of learning rate.
Theorem 3 (History-PRGF, smooth and convex; proof in Appendix B.5.1). In the setting of The-
((cid:100)·(cid:101) denotes the ceiling function), we
orem 1, assuming d ≥ 4,
have

d−1 ≤ L
ˆL

≤ 1 and T >

L(cid:48)
L ) if q

(cid:108) d
q

(cid:109)

q

E[δT ] ≤

(cid:18) 32
q

+ 2

(cid:19) 2L d
q R2
(cid:108) d
(cid:109)
q

T −

+ 1

.

(6)

Sketch of the proof. The idea of the proof of Theorem 3 is to show that for the random variable
Et[Ct], its standard deviation (cid:112)Var[Et[Ct]] is small relative to its expectation E[Et[Ct]] = E[Ct].
By Chebyshev’s inequality, we can bound E
E[Ct] . In the
actual proof we replace Ct that appears above with a lower bound Et.
Theorem 4 (History-PRGF, smooth and strongly convex; proof in Appendix B.5.3). Under the same
q , we have
conditions as in Theorem 2, then assuming d ≥ 4,

E[Et[Ct]] = 1

τ , and T ≥ 5 d

in Theorem 1 with

d ≤ 0.2 L

1
Et[Ct]

≤ 1, q

(cid:104)

(cid:105)

1

q

E[δT ] ≤ 2 exp

(cid:16)

(cid:17)

δ0.

(7)

T

d−1 ≤ L
ˆL
τ
q
L
d

−0.1

This result seems somewhat surprising since Theorem 2 does not directly give a bound of E[δT ].
Sketch of the proof. The goal is to show that the denominator in the l.h.s of Eq. (5) in Theorem 2,
(cid:80)T −1
Et[Ct])
Et[Ct]), concentrates very well. Indeed, the probability that exp(− τ
exp(− τ
˜L
˜L
t=0
is larger than exp(−0.1 q
τ
L T ) is very small so that its inﬂuence can be bounded by another
d
exp(−0.1 q
τ
L T )δ0, leading to the coefﬁcient 2 in Eq. (7). In our actual analysis we replace Ct
d
that appears above with a lower bound Et.
Remark 5. As stated in Example 2, using RGF with the optimal learning rate, we have E[δT ] ≤
2L d
L T (cid:1) δ0 for smooth and strongly convex

for smooth and convex case, and E[δT ] ≤ exp (cid:0)− q

(cid:80)T −1
t=0

q R2
T +1

d

τ

6

case. Therefore, History-PRGF with a suboptimal learning rate 1
≤
ˆL
1
L could reach similar convergence rate to RGF with optimal learning rate (up to constant factors),
which indicates that History-PRGF is more robust to learning rate than RGF.
Remark 6. We note that the constants in the bounds are loose and have a large potential to be
improved in future work, and empirically the convergence rate of History-PRGF is often not worse
than RGF using the optimal learning rate (see Fig. 2).

under the condition q
d−1

L ≤ 1
ˆL

1

q if we ignore the constants such as 32

As a sidenote, we discuss how to set q in History-PRGF. The iteration complexity given by Theorems 3
and 4 is proportional to 1
q +2 in Eq. (6) by Remark 6. Meanwhile,
we recall that each iteration of PRGF requires q + 1 queries to the directional derivative oracle, so
the total query complexity is roughly proportional to q+1
q . Hence, a very small q (e.g. q = 1) is
suboptimal. On the other hand, Theorems 3 and 4 require 1
, so to enable robustness of
ˆL
History-PRGF to a wider range of the choice of learning rate, q should not be too large. In summary,
it is desirable to set q to a moderate value.

(cid:104) q
d−1

L , 1

∈

(cid:105)

L

1

Note that if we adopt line search in Algorithm 1, then one can adapt the learning rate in a huge
range and reach the convergence guarantee with the optimal learning rate under weak assumptions.
Nevertheless, it is still an intriguing fact that History-PRGF could perform similarly to methods
adapting the learning rate, while its mechanism is very different. Meanwhile, History-PRGF is easier
to be implemented and parallelized compared with methods like line search, since its implementation
is the same as that of the RGF baseline except that it records and uses the historical prior.

4 Extension of ARS framework and PARS algorithm

To further accelerate greedy descent methods, we extend our analysis to a new variant of Accelerated
Random Search (ARS) [26] by incorporating prior information, under the smooth and convex setting.9

By delving into the proof in [26], we present our extension to ARS in Algorithm 2, state its conver-
gence guarantee in Theorem 5 and explain its design in the proof sketch in Appendix C.1.

Algorithm 2 Extended accelerated random search framework10
Input: L-smooth convex function f ; initialization x0; ˆL ≥ L; iteration number T ; γ0 > 0.
Output: xT as the approximate minimizer of f .
1: m0 ← x0;
2: for t = 0 to T − 1 do

3:

4:

Find a θt > 0 such that θt ≤
Step 1: yt ← (1 − αt)xt + αtmt, where αt is a positive root of the equation α2

where yt, vt and g2(yt) are deﬁned in following steps:
t = θt(1 − αt)γt;

Et

(cid:104)(∇f (yt)(cid:62)vt)2(cid:105)
ˆL·Et[(cid:107)g2(yt)(cid:107)2]

γt+1 ← (1 − αt)γt;

Step 2: Let vt be a random vector s.t. (cid:107)vt(cid:107) = 1; g1(yt) ← ∇f (yt)(cid:62)vt · vt;
Step 3: Let g2(yt) be an unbiased estimator of ∇f (yt), i.e., Et[g2(yt)] = ∇f (yt);
xt+1 ← yt − 1
ˆL

g1(yt), mt+1 ← mt − θt
αt

g2(yt);

5:
6:
7:
8: end for
9: return xT .

Theorem 5 (Proof in Appendix C.1). In Algorithm 2, if θt is Ft−1-measurable (see Appendix C.1
for more explanation), then we have


(f (xT ) − f (x∗))

E

(cid:32)

1 +

√

γ0
2

T −1
(cid:88)

t=0

(cid:112)

θt

(cid:33)2

 ≤ f (x0) − f (x∗) +

γ0
2

(cid:107)x0 − x∗(cid:107)2.

(8)

9The procedure of ARS requires knowledge of the strong convexity parameter τ (τ can be 0), but for clarity
we only discuss the case τ = 0 here (i.e., we do not consider strong convexity), and leave the strongly convex
case to Appendix C.6.

10Keys in this extension are: 1) We require Et[g2(yt)] = ∇f (yt). Thus, g2(yt) could not be the PRGF
estimator as it is biased towards the prior; 2) To accelerate convergence, we need to ﬁnd an appropriate θt since
it appears in Eq. (8). If we set θt to the value in ARS baseline, then no potential acceleration is guaranteed.

7

Remark 7. If we let g1(yt) be the RGF estimator in Example 1 and let g2(yt) = d/q · g1(yt), we can
show that E[g2(yt)] = ∇f (yt) and θt could be chosen as q2
ˆLd2 . Then
roughly, the convergence rate ∝ q, so the total query complexity is independent of q. When q = 1,
ARS baseline is recovered. For convenience we call the algorithm ARS regardless of the value of q.
Remark 8. If we have a uniform constant lower bound θ > 0 such that ∀t, θt ≥ θ, then we have

(cid:104)(∇f (yt)(cid:62)vt)2(cid:105)
ˆL·Et[(cid:107)g2(yt)(cid:107)2]

ˆLd2 since

= q2

Et

E [f (xT ) − f (x∗)] ≤

(cid:18)

1 +

√

γ0
2

(cid:19)−2 (cid:16)

√

θ

T

f (x0) − f (x∗) +

(cid:107)x0 − x∗(cid:107)2(cid:17)

.

γ0
2

(9)

Next we present Prior-Guided ARS (PARS) by specifying the choice of g1(yt) and g2(yt) in Al-
gorithm 2 when prior information pt ∈ Rd is available. Since we want to maximize the value
of θt, regarding g1(yt) we want to maximize Et
. By Proposition 2 and Exam-
ple 2, it is natural to let g1(yt) be the PRGF estimator for ∇f (yt). Then by Lemma 2, we have
Et
. The remain-
ing problem is to construct g2(yt), an unbiased estimator of ∇f (yt) (Et[g2(yt)] = ∇f (yt)), and
make Et[(cid:107)g2(yt)(cid:107)2] as small as possible. We leave the construction of g2(yt) in Appendix C.2.
Finally, we calculate the following expression which appears in Line 3 of Algorithm 2 to complete
the description of PARS:

d−1 (1 − Dt)), where Dt :=

= (cid:107)∇f (yt)(cid:107)2(Dt + q

(cid:104)(cid:0)∇f (yt)(cid:62)vt

(cid:104)(cid:0)∇f (yt)(cid:62)vt

∇f (yt)

(cid:1)2(cid:105)

(cid:1)2(cid:105)

(cid:17)2

pt

(cid:16)

(cid:62)

(cid:1)2(cid:105)

(cid:104)(cid:0)∇f (yt)(cid:62)vt
Et
ˆL · Et[(cid:107)g2(yt)(cid:107)2]

=

ˆL

Dt + q
(cid:16)
Dt + d−1

d−1 (1 − Dt)
q (1 − Dt)

(cid:17) .

(10)

Since Dt ≥ 0, the right-hand side is larger than q2/ˆLd2 (by Remark 7, this value corresponds to the
value of θt in ARS), so by Remark 8 PARS is guaranteed a convergence rate of ARS.

(cid:62)

pt)2 = (cid:0)∇f (yt)(cid:62)pt/(cid:107)∇f (yt)(cid:107)(cid:1)2

In implementation of PARS, we note that there remain two problems to solve. The ﬁrst is that Dt is
not accessible through one oracle query, since Dt = (∇f (yt)
, and
(cid:107)∇f (yt)(cid:107) requires estimation. Fortunately, the queries used to construct g1(yt) and g2(yt) can also
be used to estimate Dt. With a moderate value of q, we can prove that considering the error brought
by estimation of Dt, a modiﬁed version of PARS is guaranteed to converge with high probability. We
leave related discussion to Appendix C.3. The second is that Line 3 of Algorithm 2 has a subtlety that
(cid:104)(∇f (yt)(cid:62)vt)2(cid:105)
ˆL·Et[(cid:107)g2(yt)(cid:107)2]

yt depends on θt, so we cannot directly determine an optimal θt satisfying θt ≤
.
Theoretically, we can guess a conservative estimate of θt and verify this inequality, but in practice
we adopt a more aggressive strategy to ﬁnd an approximate solution of θt. We leave the actual
implementation, named PARS-Impl, in Appendix C.4.

Et

In PARS, if we adopt the historical prior as in Section 3.2, i.e., letting pt be the previous PRGF
gradient estimator g1(yt−1), then we arrive at a novel algorithm named History-PARS. Here, we note
that unlike the case in History-PRGF, it is more difﬁcult to derive the evolution of θt theoretically, so
we currently cannot prove theorems corresponding to Theorem 3. However, History-PARS can be
guaranteed the convergence rate of ARS, which is desirable since if we adopt line search in ARS to
reach robustness against learning rate (e.g. in [31]), currently there is no convergence guarantee. We
present the actual implementation History-PARS-Impl in Appendix C.5 and empirically verify that
History-PARS-Impl is robust to learning rate in Section 5.

5 Experiments

5.1 Numerical benchmarks

We ﬁrst experiment on several closed-form test functions to support our theoretical claims. We leave
more details of experimental settings to Appendix D.1.

First, we present experimental results when a general useful prior is provided. The prior-guided
methods include PRGF, PARS (refers to PARS-Impl) and PARS-Naive (simply replacing the RGF
estimator in ARS with the PRGF estimator). We adopt the setting in Section 4.1 of [20] in which the

8

(a) f1

(b) f2

(c) f3

Figure 1: Experimental results using biased gradient as the prior (best viewed in color).

i=1

(cid:80)d

0 = d, x(i)

(cid:0)i · (x(i))2(cid:1), where x(1)

prior is a biased version of the true gradient. Our test functions are as follows: 1) f1 is the “worst-case
smooth convex function” used to construct the lower bound complexity of ﬁrst-order optimization,
as in [26]; 2) f2 is a simple smooth and strongly convex function with a worst-case initialization:
f2(x) = 1
0 = 0 for i ≥ 2; and 3) f3 is the Rosenbrock
d
function (f8 in [13]) which is a well-known non-convex function used to test the performance of
optimization problems. For f1 and f2, we set ˆL to ground truth value L; for f3, we search ˆL for
best performance for each algorithm. We set d = 256 for all test functions and set q such that each
iteration of these algorithms costs 11 queries11 to the directional derivative oracle.12 We plot the
experimental results in Fig. 1, where the horizontal axis represents the number of iterations divided
by (cid:98)d/11(cid:99), and the vertical axis represents log10
. Methods without using the prior
information are shown with dashed lines. We also plot the 95% conﬁdence interval in the colored
region. The results show that for these functions (which have ill-conditioned Hessians), ARS-based
methods perform better than the methods based on greedy descent. Importantly, the utilization of
the prior could signiﬁcantly accelerate convergence for both greedy descent and ARS. We note that
the performance of our proposed PARS algorithm is better than PARS-Naive which naively replaces
the gradient estimator in the original ARS with the PRGF estimator, demonstrating the value of our
algorithm design with convergence analysis.

f (xcurrent)−f (x∗)
f (x0)−f (x∗)

Next, we verify the properties of History-PRGF and History-PARS, i.e., the historical-prior-guided
algorithms. In this part we set d = 500. We ﬁrst verify that they are robust against learning rate
on f1 and f2, and plot the results in Fig. 2(a)(b).13 In the legend, for example, ‘RGF’ means RGF
using the optimal learning rate ( ˆL = L), and ‘RGF-0.02’ means that the learning rate is set to 0.02
times of the optimal one ( ˆL = 50L). We note that for PRGF and PARS, q = 10, so q
d = 0.02. From
Fig. 2(a)(b), we see that: 1) when using the optimal learning rate, the performance of prior-guided
algorithms is not worse than that of its corresponding baseline; and 2) the performance of prior-guided
algorithms under the sub-optimal learning rate such that q
≤ 1 is at least comparable to that of
its corresponding baseline with optimal learning rate. However, for baseline algorithms (RGF and
ARS), the convergence rate signiﬁcantly degrades if a smaller learning rate is set. In summary, we
verify our claims that History-PRGF and History-PARS are robust to learning rate if q
≤ 1.
Moreover, we show that they can provide acceleration over baselines with optimal learning rate on
functions with varying local smoothness. We design a new test function as follows:

d ≤ L
ˆL

d ≤ L
ˆL

f4(x) =

(cid:26) 1

2 r2,
r − 1
2 ,

r ≤ 1
r > 1

, r = (cid:112)f2(x), where x(1)

0 = 5

√

d, x(i)

0 = 0 for i ≥ 2.

(11)

We note that f4 in regions far away from the origin is more smooth than in the region near the origin,
and the global smoothness parameter is determined by the worst-case situation (the region near the
origin). Therefore, baseline methods using an optimal learning rate could also manifest sub-optimal
performance. Fig. 2(c) shows the results. We can see that when utilizing the historical prior, the
algorithm could show behaviors of adapting to the local smoothness, thus accelerating convergence
when the learning rate is locally too conservative.

11That is, for prior-guided algorithms we set q = 10, and for other algorithms (RGF and ARS) we set q = 11.
12The directional derivative is approximated by ﬁnite differences. In PARS, 2 additional queries to the

directional derivative oracle per iteration are required to ﬁnd θt (see Appendix C.4).

13In Fig. 2, the setting of ARS-based methods are different from that in Fig. 1 as explained in Appendix D.1,

which leads to many differences of the ARS curves between Fig. 1 and Fig. 2.

9

0501001502003210RGFPRGFARSPARS-NaivePARS020406080100121086420RGFPRGFARSPARS-NaivePARS01002003004005001.21.00.80.60.40.20.0RGFPRGFARSPARS-NaivePARS(a) f1

(b) f2

(c) f4

Figure 2: Experimental results using the historical prior.

5.2 Black-box adversarial attacks

In this section, we perform ZO optimization on real-world problems. We conduct score-based black-
box targeted adversarial attacks on 500 images from MNIST and leave more details of experimental
settings to Appendix D.2. In view of optimization, this corresponds to performing constrained
maximization over {fi}500
i=1 respectively, where fi denotes the loss function to maximize in attacks.
For each image i, we record the number of queries of fi used in optimization until the attack succeeds
(when using the C&W loss function [7], this means fi > 0). For each optimization method, we
report the median query number over these images (smaller is better) in Table 1. The subscript of the
method name indicates the learning rate 1/ˆL. For all methods we set q to 20. Since [9] has shown that
using PRGF estimator with transfer-based prior signiﬁcantly outperforms using RGF estimator in
adversarial attacks, for prior-guided algorithms here we only include the historical prior case.

METHOD

MEDIAN QUERY METHOD

MEDIAN QUERY

Table 1: Attack results on MNIST.

RGF0.2
RGF0.1
History-PRGF0.2
History-PRGF0.1
History-PRGF0.05

777
1596
484
572
704

ARS0.2
ARS0.1
History-PARS0.2
History-PARS0.1
History-PARS0.05

735
1386
484
550
726

We found that in this task, ARS-based methods perform comparably to RGF-based ones. This could
be because 1) the numbers of iterations until success of attacks are too small to show the advantage of
ARS; 2) currently ARS is not guaranteed to converge faster than RGF under non-convex problems. We
leave more evaluation of ARS-based methods in adversarial attacks and further improvement of their
performance as future work. Experimental results show that History-PRGF is more robust to learning
rate than RGF. However, a small learning rate could still lead to its deteriorated performance due to
non-smoothness of the objective function. The same statement holds for ARS-based algorithms.

6 Conclusion and discussion

In this paper, we present a convergence analysis on existing prior-guided ZO optimization algorithms
including PRGF and History-PRGF. We further propose a novel prior-guided ARS algorithm with
convergence guarantee. Experimental results conﬁrm our theoretical analysis.

Our limitations lie in: 1) we adopt a directional derivative oracle in our analysis, so the error on the
convergence bound brought by ﬁnite-difference approximation has not been clearly stated; and 2) our
implementation of PARS in practice requires an approximate solution of θt, and the accuracy and
inﬂuence of this approximation is not well studied yet. We leave these as future work. Other future
work includes extension of the theoretical analysis to non-convex cases, and more empirical studies
in various application tasks.

10

01002003004005001.501.251.000.750.500.250.00RGFRGF-0.04PRGFPRGF-0.04PRGF-0.020501001502000.60.50.40.30.20.10.0RGFRGF-0.04PRGFPRGF-0.04PRGF-0.020501001502000.30.20.10.0RGFPRGF01002003004005002.52.01.51.00.50.0ARSARS-0.04PARSPARS-0.04PARS-0.0205010015020086420ARSARS-0.04PARSPARS-0.04PARS-0.0205010015020086420ARSPARSAcknowledgements

This work was supported by the National Key Research and Development Program of China (No.
2020AAA0104304), NSFC Projects (Nos. 61620106010, 62061136001, 61621136008, 62076147,
U19B2034, U19A2081, U1811461), Beijing NSF Project (No. JQ19016), Beijing Academy of
Artiﬁcial Intelligence (BAAI), Tsinghua-Huawei Joint Research Program, a grant from Tsinghua
Institute for Guo Qiang, Tiangong Institute for Intelligent Computing, and the NVIDIA NVAIL
Program with GPU/DGX Acceleration.

References

[1] Marcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew W Hoffman, David Pfau, Tom Schaul,
Brendan Shillingford, and Nando De Freitas. Learning to learn by gradient descent by gradient descent.
arXiv preprint arXiv:1606.04474, 2016.

[2] Nikhil Bansal and Anupam Gupta. Potential-function proofs for gradient methods. Theory of Computing,

15(1):1–32, 2019.

[3] Albert S Berahas, Liyuan Cao, Krzysztof Choromanski, and Katya Scheinberg. A theoretical and empirical
comparison of gradient approximations in derivative-free optimization. Foundations of Computational
Mathematics, pages 1–54, 2021.

[4] El Houcine Bergou, Eduard Gorbunov, and Peter Richtarik. Stochastic three points method for uncon-

strained smooth minimization. SIAM Journal on Optimization, 30(4):2726–2749, 2020.

[5] Thomas Brunner, Frederik Diehl, Michael Truong Le, and Alois Knoll. Guessing smart: Biased sampling
for efﬁcient black-box adversarial attacks. In Proceedings of the IEEE/CVF International Conference on
Computer Vision, pages 4958–4966, 2019.

[6] Sébastien Bubeck. Convex optimization: Algorithms and complexity. arXiv preprint arXiv:1405.4980,

2014.

[7] Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In 2017 ieee

symposium on security and privacy (sp), pages 39–57. IEEE, 2017.

[8] Pin-Yu Chen, Huan Zhang, Yash Sharma, Jinfeng Yi, and Cho-Jui Hsieh. Zoo: Zeroth order optimization
based black-box attacks to deep neural networks without training substitute models. In Proceedings of the
10th ACM workshop on artiﬁcial intelligence and security, pages 15–26, 2017.

[9] Shuyu Cheng, Yinpeng Dong, Tianyu Pang, Hang Su, and Jun Zhu. Improving black-box adversarial

attacks with a transfer-based prior. arXiv preprint arXiv:1906.06919, 2019.

[10] Krzysztof Choromanski, Mark Rowland, Vikas Sindhwani, Richard Turner, and Adrian Weller. Structured
evolution with compact architectures for scalable policy optimization. In International Conference on
Machine Learning, pages 970–978. PMLR, 2018.

[11] Daniel Golovin, John Karro, Greg Kochanski, Chansoo Lee, Xingyou Song, and Qiuyi Zhang. Gradientless

descent: High-dimensional zeroth-order optimization. arXiv preprint arXiv:1911.06317, 2019.

[12] Nikolaus Hansen and Andreas Ostermeier. Adapting arbitrary normal mutation distributions in evolution
In Proceedings of IEEE international conference on

strategies: The covariance matrix adaptation.
evolutionary computation, pages 312–317. IEEE, 1996.

[13] Nikolaus Hansen, Steffen Finck, Raymond Ros, and Anne Auger. Real-parameter black-box optimization

benchmarking 2009: Noiseless functions deﬁnitions. PhD thesis, INRIA, 2009.

[14] Risto Heijmans. When does the expectation of a ratio equal the ratio of expectations? Statistical Papers,

40(1):107–115, 1999.

[15] Andrew Ilyas, Logan Engstrom, Anish Athalye, and Jessy Lin. Black-box adversarial attacks with limited
queries and information. In International Conference on Machine Learning, pages 2137–2146. PMLR,
2018.

[16] Andrew Ilyas, Logan Engstrom, and Aleksander Madry. Prior convictions: Black-box adversarial attacks

with bandits and priors. arXiv preprint arXiv:1807.07978, 2018.

11

[17] Hamed Karimi, Julie Nutini, and Mark Schmidt. Linear convergence of gradient and proximal-gradient
methods under the polyak-łojasiewicz condition. In Joint European Conference on Machine Learning and
Knowledge Discovery in Databases, pages 795–811. Springer, 2016.

[18] David Kozak, Stephen Becker, Alireza Doostan, and Luis Tenorio. A stochastic subspace approach to
gradient-free optimization in high dimensions. Computational Optimization and Applications, 79(2):
339–368, 2021.

[19] Stanislaw Lojasiewicz. A topological property of real analytic subsets. Coll. du CNRS, Les équations aux

dérivées partielles, 117:87–89, 1963.

[20] Niru Maheswaranathan, Luke Metz, George Tucker, Dami Choi, and Jascha Sohl-Dickstein. Guided
evolutionary strategies: Augmenting random search with surrogate gradients. In International Conference
on Machine Learning, pages 4264–4273. PMLR, 2019.

[21] Horia Mania, Aurelia Guy, and Benjamin Recht. Simple random search of static linear policies is
competitive for reinforcement learning. In Proceedings of the 32nd International Conference on Neural
Information Processing Systems, pages 1805–1814, 2018.

[22] J Matyas. Random optimization. Automation and Remote control, 26(2):246–253, 1965.

[23] Florian Meier, Asier Mujika, Marcelo Matheus Gauy, and Angelika Steger. Improving gradient estimation

in evolutionary strategies with past descent directions. arXiv preprint arXiv:1910.05268, 2019.

[24] Cristinel Mortici. New approximation formulas for evaluating the ratio of gamma functions. Mathematical

and Computer Modelling, 52(1-2):425–433, 2010.

[25] Yu Nesterov. Efﬁciency of coordinate descent methods on huge-scale optimization problems. SIAM

Journal on Optimization, 22(2):341–362, 2012.

[26] Yurii Nesterov and Vladimir Spokoiny. Random gradient-free minimization of convex functions. Founda-

tions of Computational Mathematics, 17(2):527–566, 2017.

[27] Brendan O’donoghue and Emmanuel Candes. Adaptive restart for accelerated gradient schemes. Founda-

tions of computational mathematics, 15(3):715–732, 2015.

[28] Boris Teodorovich Polyak. Gradient methods for minimizing functionals. Zhurnal Vychislitel’noi Matem-

atiki i Matematicheskoi Fiziki, 3(4):643–653, 1963.

[29] Tim Salimans, Jonathan Ho, Xi Chen, Szymon Sidor, and Ilya Sutskever. Evolution strategies as a scalable

alternative to reinforcement learning. arXiv preprint arXiv:1703.03864, 2017.

[30] Jasper Snoek, Hugo Larochelle, and Ryan P Adams. Practical bayesian optimization of machine learning

algorithms. arXiv preprint arXiv:1206.2944, 2012.

[31] Sebastian U Stich, Christian L Muller, and Bernd Gartner. Optimization of convex functions with random

pursuit. SIAM Journal on Optimization, 23(2):1284–1309, 2013.

[32] Chun-Chen Tu, Paishun Ting, Pin-Yu Chen, Sijia Liu, Huan Zhang, Jinfeng Yi, Cho-Jui Hsieh, and Shin-
Ming Cheng. Autozoom: Autoencoder-based zeroth order optimization method for attacking black-box
neural networks. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 33, pages
742–749, 2019.

[33] Roman Vershynin. High-dimensional probability: An introduction with applications in data science,

volume 47. Cambridge university press, 2018.

12

A Supplemental materials for Section 2

A.1 Deﬁnitions in convex optimization

Deﬁnition 1 (Convexity). A differentiable function f is convex if for every x, y ∈ Rd,

f (y) ≥ f (x) + ∇f (x)(cid:62)(y − x).

Deﬁnition 2 (Smoothness). A differentiable function f is L-smooth for some positive constant L if
its gradient is L-Lipschitz; namely, for every x, y ∈ Rd, we have

Corollary 1. If f is L-smooth, then for every x, y ∈ Rd,

(cid:107)∇f (x) − ∇f (y)(cid:107) ≤ L(cid:107)x − y(cid:107).

|f (y) − f (x) − ∇f (x)(cid:62)(y − x)| ≤

1
2

L(cid:107)y − x(cid:107)2.

(12)

Proof. See Lemma 3.4 in [6].

Deﬁnition 3 (Strong convexity). A differentiable function f is τ -strongly convex for some positive
constant τ , if for all x, y ∈ Rd,

f (y) ≥ f (x) + ∇f (x)(cid:62)(y − x) +

τ
2

(cid:107)y − x(cid:107)2.

A.2 Proof of Proposition 1

Proposition 1. If f is L-smooth, then for any (x, v) with (cid:107)v(cid:107) = 1, |gµ(v; x) − ∇f (x)(cid:62)v| ≤ 1

2 Lµ.

Proof. In Eq. (12), setting y = x + µv and dividing both sides by µ, we complete the proof.

B Supplemental materials for Section 3

B.1 Proofs of Lemma 1, Theorem 1 and Theorem 2

Lemma 1. Let Ct :=

(cid:16)

(cid:62)

∇f (xt)

(cid:17)2

vt

and L(cid:48) :=

L
1−(1− L

ˆL )2 , then in Algorithm 1, we have

Et[δt+1] ≤ δt −

Et[Ct]
2L(cid:48) (cid:107)∇f (xt)(cid:107)2.

Proof. We have

δt+1 − δt = f (xt+1) − f (xt)
(cid:18)

= f

xt −

∇f (xt)(cid:62)vt · vt

(cid:19)

− f (xt)

1
ˆL

≤ −

= −

1
ˆL
1
2L(cid:48)

(cid:0)∇f (xt)(cid:62)vt

(cid:1)2

+

1
2

L ·

(cid:18) 1
ˆL

(cid:19)2

∇f (xt)(cid:62)vt

(cid:0)∇f (xt)(cid:62)vt

(cid:1)2

.

Hence,

Et[δt+1] − δt ≤ −

(cid:104)(cid:0)∇f (xt)(cid:62)vt

(cid:1)2(cid:105)

Et

= −

1
2L(cid:48)

Et[Ct]
2L(cid:48) (cid:107)∇f (xt)(cid:107)2,

(13)

(14)

(15)

(16)

(17)

(18)

where the last equality holds because xt is Ft−1-measurable since we require Ft−1 to include all the
randomness before iteration t in Remark 2 (so (cid:107)∇f (xt)(cid:107)2 is also Ft−1-measurable).

Remark 9. The proof actually does not require f to be convex. It only requires f to be L-smooth.

13

Remark 10. From the proof we see that δt+1 − δt ≤ 0, so f (xt+1) ≤ f (xt). Hence, the sequence
{f (xt)}t≥0 is non-increasing in Algorithm 1.
Theorem 1 (Algorithm 1, smooth and convex). Let R := maxx:f (x)≤f (x0) (cid:107)x − x∗(cid:107) and suppose
R < ∞. Then, in Algorithm 1, we have

E[δT ] ≤

Proof. Since f is convex, we have

(cid:104)

2L(cid:48)R2 (cid:80)T −1
t=0
T (T + 1)

E

1
Et[Ct]

(cid:105)

.

(19)

δt = f (xt) − f (x∗) ≤ ∇f (xt)(cid:62)(xt − x∗) ≤ (cid:107)∇f (xt)(cid:107) · (cid:107)xt − x∗(cid:107) ≤ R(cid:107)∇f (xt)(cid:107),

(20)

where the last inequality follows from the deﬁnition of R and the fact that f (xt) ≤ f (x0) (since
δt+1 ≤ δt for all t). The following proof is adapted from the proof of Theorem 3.2 in [2]. Deﬁne
Φt := t(t + 1)δt. By Lemma 1, we have

Et[Φt+1] − Φt = (t + 1)(t + 2)Et[δt+1] − t(t + 1)δt

= (t + 1)(t + 2)(Et[δt+1] − δt) + 2(t + 1)δt
Et[Ct]
2L(cid:48) (cid:107)∇f (xt)(cid:107)2 + 2(t + 1)R(cid:107)∇f (xt)(cid:107)

≤ −(t + 1)(t + 2)

≤

=

≤

(2(t + 1)R)2

Et[Ct]
2L(cid:48)

4(t + 1)(t + 2)
2L(cid:48)(t + 1)R2
(t + 2)Et[Ct]
2L(cid:48)R2
Et[Ct]

,

where Eq. (24) follows from the fact that −at2 + bt ≤ b2

4a for a > 0. Hence

E[Φt+1] − E[Φt] = E[Et[Φt+1] − Φt] ≤ 2L(cid:48)R2E

(cid:20)

(cid:21)

.

1
Et[Ct]

Since Φ0 = 0, we have E[ΦT ] ≤ 2L(cid:48)R2 (cid:80)T −1

t=0

(cid:104)

E

(cid:105)

1
Et[Ct]

. Therefore,

E[δT ] =

E[ΦT ]
T (T + 1)

≤

(cid:104)

2L(cid:48)R2 (cid:80)T −1
t=0
T (T + 1)

E

1
Et[Ct]

(cid:105)

.

(21)
(22)

(23)

(24)

(25)

(26)

(27)

(28)

Remark 11. By inspecting the proof, we note that Theorem 1 still holds if we replace the ﬁxed
initialization x0 in Algorithm 1 with a random initialization x(cid:48)
0) ≤ f (x0) always
holds. We formally summarize this in the following proposition. This proposition will be useful in the
proof of Theorem 3.
Proposition 3. Let xﬁx be a ﬁxed vector, R := maxx:f (x)≤f (xfix) (cid:107)x − x∗(cid:107) and suppose R < ∞.
Then, in Algorithm 1, using a random initialization x0, if f (x0) ≤ f (xﬁx) always hold, we have

0 for which f (x(cid:48)

E[δT ] ≤

(cid:104)

2L(cid:48)R2 (cid:80)T −1
t=0
T (T + 1)

E

1
Et[Ct]

(cid:105)

.

(29)

Proof. By Remark 10, f (xt) ≤ f (x0). We note that (cid:107)xt − x∗(cid:107) ≤ R since f (xt) ≤ f (x0) ≤ f (xﬁx).
The remaining proof is the same as the proof of Theorem 1.

Next we state the proof regarding the convergence guarantee of Algorithm 1 under smooth and
strongly convex case.

14

(30)

(31)

(32)

(33)

(34)

(35)

(cid:21)

Theorem 2 (Algorithm 1, smooth and strongly convex). In Algorithm 1, if we further assume that f
is τ -strongly convex, then we have




E



exp

(cid:16)

− τ
L(cid:48)

δT
(cid:80)T −1
t=0

 ≤ δ0.

(cid:17)
Et[Ct]

Proof. Since f is τ -strongly convex, we have

δt = f (xt) − f (x∗) ≤ ∇f (xt)(cid:62)(xt − x∗) −

≤ (cid:107)∇f (xt)(cid:107) · (cid:107)xt − x∗(cid:107) −

τ
2

(cid:107)xt − x∗(cid:107)2

τ
2

(cid:107)xt − x∗(cid:107)2

≤

(cid:107)∇f (xt)(cid:107)2
2τ

.

Therefore we have

By Lemma 1 and Eq. (34) we have

(cid:107)∇f (xt)(cid:107)2 ≥ 2τ δt.

Et[δt+1] ≤ δt −

Et[Ct]τ
L(cid:48)

(cid:16)

1 −

δt =

(cid:17)
Et[Ct]

δt.

τ
L(cid:48)

Let αt := τ

L(cid:48) Et[Ct], then Et[δt+1] ≤ (1 − αt)δt. We have

δ0 = E[δ0] ≥ E

(cid:20)

1
1 − α0

(cid:21)

(cid:20)

= E

E0[δ1]

≥ E

(cid:20)

E1[δ2]
(1 − α0)(1 − α1)

(cid:21)

(cid:20)

(cid:20)
E1

= E

≥ . . .
(cid:34)

≥ E

δT
t=0 (1 − αt)

(cid:81)T −1

(cid:35)

.

(cid:21)

(cid:20)

(cid:21)(cid:21)

= E

E0

δ1
1 − α0
δ2
(1 − α0)(1 − α1)

(cid:20)

(cid:21)(cid:21)

δ1
1 − α0
(cid:20)

= E

δ2
(1 − α0)(1 − α1)

Since exp(−x) ≥ 1 − x ≥ 0 when 0 ≤ x ≤ 1, the proof is completed.

Remark 12. Indeed, the proof does not require f to be strongly convex or convex. It only requires
the Polyak-Łojasiewicz condition (Eq. (34)) which is weaker than strong convexity [28, 19, 17].

B.2 Proof of Proposition 2

We note that in Algorithm 1, (cid:107)vt(cid:107) = 1. If vt ∈ A, then we have the following lemma by Proposition 1
in [23].
Lemma 4. Let u1, u2, . . . , uq be q ﬁxed vectors in Rd and A := span{u1, u2, . . . , uq} be the
subspace spanned by u1, u2, . . . , uq. Let ∇f (xt)A denote the projection of ∇f (xt) onto A, then
∇f (xt)A = argmaxvt∈A,(cid:107)vt(cid:107)=1 Ct.

We further note that ∇f (xt)A could be calculated with the values of {∇f (xt)(cid:62)ui}q
Lemma 5. Let A := span{u1, u2, . . . , uq} be the subspace spanned by u1, u2, . . . , uq, and suppose
{u1, u2, . . . , uq} is linearly independent (if they are not, then we choose a subset of these vectors
which is linearly independent). Then ∇f (xt)A = (cid:80)q
i=1 aiui, where a := (a1, a2, · · · , aq)(cid:62) is given
by a = G−1b, where G is a q × q matrix in which Gij = u(cid:62)
i uj, b is a q-dimensional vector in which
bi = ∇f (xt)(cid:62)ui.

i=1:

Proof. Since ∇f (xt)A ∈ span{u1, u2, . . . , uq}, there exists a ∈ Rq such that ∇f (xt)A =
(cid:80)q
i=1 aiui. Since ∇f (xt)A is the projection of ∇f (xt) onto A and u1, u2, . . . , uq ∈ A,
∇f (xt)(cid:62)
Aui = ∇f (xt)(cid:62)ui holds for any i. Therefore, Ga = b. Since {u1, u2, . . . , uq} is lin-
early independent and G is corresponding Gram matrix, G is invertible. Hence a = G−1b.

15

Therefore, if we suppose vt ∈ A, then the optimal vt is given by ∇f (xt)A, which could be calculated
from {∇f (xt)(cid:62)ui}q
i=1. Now we are ready to prove Proposition 2 through an additional justiﬁcation.
Proposition 2 (Optimality of subspace estimator). In one iteration of Algorithm 1, if we have queried
{∇f (xt)(cid:62)ui}q
i=1, then the optimal vt maximizing Ct s.t. (cid:107)vt(cid:107) = 1 should be in the following form:
vt = ∇f (xt)A, where A := span{u1, u2, . . . , uq}.

Proof. It remains to justify the assumption that vt ∈ A. We note that in Line 3 of Algorithm 1,
generally it requires 1 additional call to query the value of ∇f (xt)(cid:62)vt, but if vt ∈ A, then we can al-
ways save this query by calculating ∇f (xt)(cid:62)vt with the values of {∇f (xt)(cid:62)ui}q
i=1, since if vt ∈ A,
then we can write vt in the form vt = (cid:80)q
i=1 aiui, and hence ∇f (xt)(cid:62)vt = (cid:80)q
i=1 ai∇f (xt)(cid:62)ui.
Now suppose we ﬁnally sample a vt /∈ A. Then this additional query of ∇f (xt)(cid:62)vt is neces-
sary. Now we could let A(cid:48) := span{u1, u2, . . . , uq, vt} and calculate v(cid:48)
t = ∇f (xt)A(cid:48). Obviously,
vt)2, suggesting that v(cid:48)
t is better than vt. Therefore, without loss of
(∇f (xt)
generality we can always assume vt ∈ A, and by Lemma 4 the proof is complete.

(cid:62)
t)2 ≥ (∇f (xt)
v(cid:48)

(cid:62)

B.3 Details regarding RGF and PRGF estimators

B.3.1 Construction of RGF estimator

In Example 1, we mentioned that the RGF estimator is given by vt = ∇f (xt)A where A =
span{u1, u2, . . . , uq} (q > 0) and ∀i, ui ∼ U(Sd−1) (u ∼ U(Sd−1) means that u is sampled
uniformly from the (d − 1)-dimensional unit sphere, as a normalized d-dimensional random vector),
and u1, u2, . . . , uq are sampled independently. Now we present the detailed expression of vt by
explicitly orthogonalizing {u1, u2, . . . , uq}:
u1 ∼ U(Sd−1);
u2 = (I − u1u(cid:62)
u3 = (I − u1u(cid:62)

1 )ξ2, ξ2 ∼ U(Sd−1);
1 − u2u(cid:62)

2 )ξ3, ξ3 ∼ U(Sd−1);

...

(cid:32)

uq =

I −

q−1
(cid:88)

i=1

(cid:33)

uiu(cid:62)
i

ξq, ξq ∼ U(Sd−1).

Then we let vt = (cid:80)q
we have gt = (cid:80)q
Algorithm 1 costs q queries to the directional derivative oracle.

i=1 ∇f (xt)(cid:62)ui · ui. Since gt = ∇f (xt)(cid:62)∇f (xt)A · ∇f (xt)A = ∇f (xt)A,
i=1 ∇f (xt)(cid:62)ui · ui. Therefore, when using the RGF estimator, each iteration in

B.3.2 Properties of RGF estimator

In this section we show that for RGF estimator with q queries, Et[Ct] = q
proposition here.
Proposition 4. If vt = (cid:80)q

i=1 ∇f (xt)(cid:62)ui · ui and u1, u2, . . . , uq are orthonormal, then

d . We ﬁrst state a simple

(cid:16)

(cid:62)

∇f (xt)

(cid:17)2

vt

=

q
(cid:88)

(cid:16)

i=1

(cid:62)

∇f (xt)

(cid:17)2

.

ui

(36)

Proof. Since ∇f (xt)(cid:62)vt · vt
(cid:62)
(cid:80)q

i=1 ∇f (xt)

:= gt = (cid:80)q

(cid:62)
i=1 ∇f (xt)(cid:62)ui · ui, we have ∇f (xt)

vt · vt =

ui · ui. Taking inner product with ∇f (xt) to both sides, we obtain the result.

By Proposition 4,
(cid:20)(cid:16)

Et[Ct] = Et

(cid:62)

∇f (xt)

(cid:17)2(cid:21)

vt

=

q
(cid:88)

i=1

(cid:20)(cid:16)

Et

∇f (xt)

(cid:62)

ui

(cid:17)2(cid:21)

=

q
(cid:88)

(cid:16)

∇f (xt)

i=1

(cid:62)Et[uiu(cid:62)

i ]∇f (xt)

(cid:17)

.

(37)

16

In RGF, ui is independent of the history, so in this section we directly write E[uiu(cid:62)
Et[uiu(cid:62)
i ].
For i = 1, since u1 ∼ U(Sd−1), we have E[u1u(cid:62)
is symmetric, hence E[u1u(cid:62)
a = 1/Tr(I) = 1/d.)

1 ] should be something like aI; since Tr(E[u1u(cid:62)

d . (Explanation: the distribution of u1
1 u1] = 1,

1 ]) = E[u(cid:62)

i ] instead of

1 ] = I

For i = 2, we have E[u2u(cid:62)
2 ] = E[E[u2u(cid:62)
E[u2u(cid:62)

2 |u1] = I−u1u(cid:62)
d−1
= I
d .

2 |u1]] = I−E[u1u(cid:62)
1 ]

d−1

1

. (See Section A.2 in [9] for the proof.) Therefore,

Then by induction, we have that ∀1 ≤ i ≤ q, E[uiu(cid:62)

i ] = I

d . Hence by Eq. (37), Et[Ct] = q
d .

B.3.3 Construction of PRGF estimator

In Example 2, we mentioned that the PRGF estimator is given by vt = ∇f (xt)A where
A = span{pt, u1, u2, . . . , uq} (q > 0), where pt is a vector corresponding to the prior message
which is available at the beginning of iteration t, and ∀i, ui ∼ U(Sd−1) (u1, u2, . . . , uq are sam-
pled independently). Now we present the detailed expression of vt by explicitly orthogonalizing
{pt, u1, u2, . . . , uq}. We note that here we leave pt unchanged (we only normalize it, i.e. pt ← pt
(cid:107)pt(cid:107)
if (cid:107)pt(cid:107) (cid:54)= 1) and make {u1, u2, . . . , uq} orthogonal to pt. Speciﬁcally, given a positive integer
q ≤ d − 1,

u1 = (I − ptp(cid:62)
u2 = (I − ptp(cid:62)
u3 = (I − ptp(cid:62)

t )ξ1, ξ1 ∼ U(Sd−1);
t − u1u(cid:62)
t − u1u(cid:62)

1 )ξ2, ξ2 ∼ U(Sd−1);
1 − u2u(cid:62)

2 )ξ3, ξ3 ∼ U(Sd−1);

...

(cid:32)

uq =

I − ptp(cid:62)

t −

q−1
(cid:88)

(cid:33)

uiu(cid:62)
i

ξq, ξq ∼ U(Sd−1).

i=1
Then we let vt = ∇f (xt)(cid:62)pt · pt + (cid:80)q
i=1 ∇f (xt)(cid:62)ui · ui. Since gt = ∇f (xt)(cid:62)∇f (xt)A ·
∇f (xt)A = ∇f (xt)A, we have gt = ∇f (xt)(cid:62)pt ·pt +(cid:80)q
i=1 ∇f (xt)(cid:62)ui ·ui. Therefore, when using
the PRGF estimator, each iteration in Algorithm 1 costs q + 1 queries to the directional derivative
oracle.

B.3.4 Properties of PRGF estimator

Here we prove Lemma 2 in the main article (its proof appears in [23]; we prove it in our language
here), but for later use we give a more useful formula here, which can derive Lemma 2. Let

Dt :=

(cid:16)

(cid:62)

∇f (xt)

pt

(cid:17)2

. We have

Proposition 5. For t ≥ 1,

Ct = Dt + (1 − Dt)ξ2
t ,

(38)

t := (cid:80)q

where ξ2
ui
the vector e onto the (d − 1)-dimensional subspace H, of which pt is a normal vector.

14 in which eH := e − ptp(cid:62)

ti, ξti := ∇f (xt)H

i=1 ξ2

t e denotes the projection of

(cid:62)

Proof. By Proposition 4, we have

(cid:16)

Ct =

∇f (xt)

(cid:62)

(cid:17)2

vt

= Dt +

q
(cid:88)

(cid:16)

i=1

(cid:62)

∇f (xt)

(cid:17)2

.

ui

(39)

By the deﬁnition of u1, u2, . . . , uq, they are in the subspace H. Therefore
(cid:16)

(cid:17)2

(cid:17)2

(cid:17)2

(cid:16)

(cid:62)

(cid:62)

(cid:62)
H ui

= (cid:107)∇f (xt)H (cid:107)2 (cid:16)

∇f (xt)

ui

=

∇f (xt)

∇f (xt)H

ui

= (1 − Dt)

(cid:16)

∇f (xt)H

(cid:62)

(cid:17)2

.

ui
(40)

14Note that in different iterations, {ui} are different. Hence here we explicitly show this dependency on t in

the subscript of ξ.

17

By Eq. (39) and Eq. (40), the proposition is proved.

(cid:16)

t ], the conditional expectation of ξ2

Next we state Et[ξ2
t given the history Ft−1. We can also derive it
in the similar way as in Section B.3.2, but for later use let us describe the distribution of ξ2
t in a more
convenient way. We note that the conditional distribution of ui is the uniform distribution from the
unit sphere in the (d − 1)-dimensional subspace H. Since ξti := ∇f (xt)H
ui, ξti is indeed the inner
product between one ﬁxed unit vector and one uniformly random sampled unit vector in H. Indeed,
(cid:13)
(cid:13)
ξ2
where A(cid:48) := span(u1, u2, · · · , uq) is a random q-dimensional
t is equal to
(cid:13)
A(cid:48)
subspace of H. Therefore, ξ2
t is equal to the squared norm of the projection of a ﬁxed unit vector in
H to a random q-dimensional subspace of H. By the discussion in the proof of Lemma 5.3.2 in [33],
we can view a random projection acting on a ﬁxed vector as a ﬁxed projection acting on a random
vector. Therefore, we state the following proposition.
Proposition 6. The conditional distribution of ξ2
(cid:80)q

i , where (z1, z2, . . . , zd−1)(cid:62) ∼ U(Sd−2), where Sd−2 is the unit sphere in Rd−1.

t given Ft−1 is the same as the distribution of

∇f (xt)H

(cid:13)
2
(cid:13)
(cid:13)

i=1 z2

(cid:17)

(cid:62)

Then it is straightforward to prove the following proposition.
Proposition 7. Et[ξ2

t ] = q

d−1 .

Proof. By symmetry, E[z2
Proposition 6, Et[ξ2

t ] = E[(cid:80)q

i ] = E[z2
i=1 z2

i ] = q

d−1 .

j ] ∀i, j. Since E[(cid:80)d−1

i=1 z2

i ] = 1, E[z2

i ] = 1

d−1 . Hence by

Now we reach Lemma 2.
Lemma 2. In Algorithm 1 with PRGF estimator,

Et[Ct] = Dt +

q
d − 1

(1 − Dt),

(41)

where Dt :=

(cid:16)

∇f (xt)

(cid:62)

(cid:17)2

.

pt

Proof. Since Dt is Ft−1-measurable, by Proposition 5 and Proposition 7, we have

Et[Ct] = Dt + (1 − Dt)Et[ξ2

t ] = Dt +

q
d − 1

(1 − Dt).

Finally, we note that Proposition 6 implies that ξ2
t is independent of the history (indeed, for all i,
ξ2
ti is independent of the history). For convenience, in the following, when we need the conditional
expectation (given some historical information) of quantities only related to ξ2
t , we could directly
write the expectation without conditioning. For example, we directly write E[ξ2
t ] instead of Et[ξ2
t ],
and write Var[ξ2

t ] instead of the conditional variance Vart[ξ2
t ].

B.4 Proof of Lemma 3 and evolution of E[Ct]

In this section, we discuss the key properties of History-PRGF before presenting the theorems in
Section B.5. First we mention that while in History-PRGF we choose the prior pt to be vt−1, we can
choose p0 as any ﬁxed normalized vector. We ﬁrst present a lemma which is useful for the proof of
Lemma 3.
Lemma 6 (Proof in Section B.4.1). Let a, b and c be vectors in Rd, (cid:107)a(cid:107) = (cid:107)c(cid:107) = 1, B := {b :
(cid:107)b − a(cid:107) ≤ k · a(cid:62)c}, 0 ≤ k ≤ 1, a(cid:62)c ≥ 0. Then minb∈B b
Lemma 3. In History-PRGF (pt = vt−1), we have

c ≥ minb∈B b(cid:62)c = (1 − k)a(cid:62)c.

(cid:62)

(cid:18)

Dt ≥

1 −

(cid:19)2

L
ˆL

Ct−1.

18

(42)

Proof. In History-PRGF pt = vt−1, so by the deﬁnitions of Dt and Ct we are going to prove

(cid:16)

∇f (xt)

(cid:62)

vt−1

(cid:17)2

(cid:18)

≥

1 −

(cid:19)2 (cid:16)

L
ˆL

∇f (xt−1)

(cid:62)

(cid:17)2

.

vt−1

Without loss of generality, assume ∇f (xt−1)(cid:62)vt−1 ≥ 0. Since f is L-smooth, we have

(cid:107)∇f (xt) − ∇f (xt−1)(cid:107) ≤ L(cid:107)xt − xt−1(cid:107) =

L
ˆL

∇f (xt−1)(cid:62)vt−1,

which is equivalent to

(cid:13)
(cid:13)
(cid:13)
(cid:13)

∇f (xt)
(cid:107)∇f (xt−1)(cid:107)

− ∇f (xt−1)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

≤

L
ˆL

∇f (xt−1)

(cid:62)

vt−1.

Let a = ∇f (xt−1), b = ∇f (xt)

(cid:107)∇f (xt−1)(cid:107) , c = vt−1. By Lemma 6 we have

(cid:62)

∇f (xt)

(cid:18)

vt−1 ≥

1 −

(cid:19)

L
ˆL

∇f (xt−1)

(cid:62)

vt−1.

(43)

(44)

(45)

(46)

By the deﬁnition of vt, the right-hand side is non-negative. Taking square on both sides, the proof is
completed.

When considering the lower bound related to Ct, we can replace the inequality with equality in
Lemma 3. Therefore, by Proposition 5 and Lemma 3, we now have full knowledge of evolution of
Ct. We summarize the above discussion in the following proposition. We deﬁne a(cid:48) :=
the following.

1 − L
ˆL

(cid:17)2

in

(cid:16)

Proposition 8. Let a(cid:48) :=

(cid:17)2

(cid:16)

1 − L
ˆL

. Then in History-PRGF, we have

Ct ≥ a(cid:48)Ct−1 + (1 − a(cid:48)Ct−1)ξ2
t .

(47)

Proof. By Proposition 5, Ct = (1 − ξ2
obtain the result.

t )Dt + ξ2

t . By Lemma 3, Dt ≥ a(cid:48)Ct−1. Since ξ2

t ≤ 1, we

As an appetizer, we discuss the evolution of E[Ct] here using Lemma 2 and Lemma 3 in the following
proposition.
Proposition 9. Suppose q
for t ≥ n d−1
q .

(k > 0), then in History-PRGF, E[Ct] ≥ (1 − e−n) 2
2+k

d−1 = k L
ˆL

1
1−a(cid:48)

q
d−1

Proof. By Eq. (47), we have

E[Ct] = E[Et[Ct]] ≥ E[a(cid:48)Ct−1 + (1 − a(cid:48)Ct−1)Et[ξ2
t ]]

= E[a(cid:48)Ct−1 + (1 − a(cid:48)Ct−1)E[ξ2
t ]]
(cid:19)
q
d − 1

a(cid:48)E[Ct−1] +

q
d − 1

1 −

=

(cid:18)

.

(48)

(49)

(50)

Letting a := a(cid:48)(1 − q
E[Ct] − b
E[Ct] ≥ b

1−a ≥ a(E[Ct−1] − b
1−a − at( b

d−1 ), b := q

1−a − E[C0]) ≥ (1 − at) b

1−a .

d−1 , then E[Ct] ≥ aE[Ct−1] + b and 0 ≤ a < 1. We have
1−a ), hence

1−a ) ≥ . . . ≥ at(E[C0] − b

1−a ) ≥ a2(E[Ct−2] − b

Since 1 − a = 1 − (1 − q

d−1 )(1 − L
ˆL

)2 = 1 − (1 − k L
ˆL

)(1 − L
ˆL

)2, noting that

1 − (1 − L
ˆL

)2
)(1 − L
ˆL

1 − (1 − k L
ˆL

=

)2

L
ˆL

+ L
ˆL

L
+ L
(1 − L
)
ˆL
ˆL
ˆL
(1 − L
) + k L
(1 − L
ˆL
ˆL
ˆL

≥

)2

2
2 + k

,

(51)

19

we have 1−a(cid:48)

1−a ≥ 2

2+k . Meanwhile, a ≤ 1 − q
)n d−1

at ≤ (1 −

q
d − 1

d−1 . Therefore, if t ≥ n d−1
q
d − 1

q ≤ exp(−

)n d−1

q = e−n.

q , we have

Since 1−a(cid:48)

1−a ≥ 2

2+k and at ≤ e−n, we have
b
1 − a

E[Ct] ≥ (1 − at)

≥

2
2 + k

(1 − e−n)

1
1 − a(cid:48)

q
d − 1

.

Corollary 2. In History-PRGF, lim inf t→∞ E[Ct] ≥ 2
2+k

q
d−1

1
1−a(cid:48) .

(52)

(53)

d ≤ L
ˆL

1−a(cid:48) , the propositions above tell us that E[Ct] tends to O

L(cid:48)
Recalling that L(cid:48) := L
L
as long as k is small, e.g. when q
(which means that the chosen learning rate 1
ˆL
E[Ct]
L ). If E[Ct] ≈ q
≥ q
compared with the optimal learning rate 1
is not dependent
L(cid:48)
d
on L(cid:48) (and thus independent of ˆL). By Lemma 1, Theorem 1 and Theorem 2, this roughly means that
the convergence rate is robust to the choice of ˆL, i.e. robust to the choice of learning rate. Speciﬁcally,
History-PRGF with ˆL > L (but ˆL is not too large) could roughly recover the performance of RGF
with ˆL = L, since

d is the value of Et[Ct] when using the RGF estimator.

L where q

is not too small

in a fast rate,

E[Ct]
L(cid:48) ≈

L(cid:48)
L , then

L : 1
ˆL

q
d

d

1

(cid:17)

(cid:16) q
d

B.4.1 Proof of Lemma 6

In this section, we ﬁrst give a lemma for the proof of Lemma 6.
Lemma 7. Let a and b be vectors in Rd, (cid:107)a(cid:107) = 1, (cid:107)b(cid:107) ≥ 1. Then (cid:107)b − a(cid:107) ≤ (cid:107)b − a(cid:107).

Proof.

(cid:107)b − a(cid:107)2 − (cid:107)b − a(cid:107)2 = (cid:107)b − b(cid:107)2 + 2(b − b)(cid:62)(b − a)

≥ 2(b − b)(cid:62)(b − a)

= 2((cid:107)b(cid:107) − 1)b

(cid:62)

= 2((cid:107)b(cid:107) − 1)(1 − b
≥ 0.

(b − a)
(cid:62)

a)

(54)

(55)

(56)

(57)
(58)

Then, the detailed proof of Lemma 6 is as follows.

Proof. ∀b ∈ B, b(cid:62)c = a(cid:62)c − (a − b)(cid:62)c ≥ a(cid:62)c − (cid:107)a − b(cid:107)(cid:107)c(cid:107) ≥ (1 − k)a(cid:62)c, and both equality
holds when b = a − k · a(cid:62)c · c.

Case 1: (cid:107)b(cid:107) ≥ 1 By Lemma 7 we have (cid:107)b − a(cid:107) ≤ (cid:107)b − a(cid:107), hence if b ∈ B, then b ∈ B, so when
(cid:107)b(cid:107) ≥ 1 we have b

c ≥ minb∈B b(cid:62)c.

(cid:62)

Case 2: (cid:107)b(cid:107) < 1 ∀b ∈ B, if (cid:107)b(cid:107) ≤ 1, then b

(cid:62)

c = b(cid:62)c

(cid:107)b(cid:107) ≥ b(cid:62)c ≥ minb∈B b(cid:62)c.

The proof of the lemma is completed.

B.5 Proofs of Theorem 3 and Theorem 4

B.5.1 Proof of Theorem 3

As mentioned above, we deﬁne a(cid:48) :=
to be used in the proofs. In the analysis, we ﬁrst
try to replace the inequality in Lemma 3 with equality. To do that, similar to Eq. (47), we deﬁne
{Et}T −1

t=0 as follows: E0 = 0, and

1 − L
ˆL

(cid:16)

(cid:17)2

Et = a(cid:48)Et−1 + (1 − a(cid:48)Et−1)ξ2
t ,

(59)

20

where ξ2

t is deﬁned in Proposition 5.

First, we give the following lemmas, which is useful for the proof of Theorem 3.
Lemma 8 (Upper-bounded variance; proof in Section B.5.2). If d ≥ 4, then ∀t, Var[Et[Et]] ≤

1
1−(a(cid:48))2

2q
(d−1)2 .

Lemma 9 (Lower-bounded expectation; proof in Section B.5.2). If

q

d−1 ≤ L
ˆL

and t ≥ d−1

q , then

E[Et] ≥

1
2

1
1 − a(cid:48)

q
d − 1

.

(60)

Lemma 10 (Proof in Section B.5.2). If a random variable X ≥ B > 0 satisﬁes that E[X] ≥ µB,
Var[X] ≤ (σB)2, then

E

(cid:21)

(cid:20) 1
X

≤

1
µB

(cid:18) 4σ2
µ

(cid:19)

+ 2

.

(61)

Then, we provide the proof of Theorem 3 in the following.
Theorem 3 (History-PRGF, smooth and convex). In the setting of Theorem 1, when using the History-
((cid:100)·(cid:101) denotes the ceiling function), we
≤ 1 and T >
PRGF estimator, assuming d ≥ 4,
have

d−1 ≤ L
ˆL

(cid:108) d
q

(cid:109)

q

E[f (xT )] − f (x∗) ≤

(cid:18) 32
q

+ 2

Proof. Since E0 = 0 ≤ C0, and if Et−1 ≤ Ct−1, then

(cid:19) 2L d
q R2
(cid:108) d
(cid:109)
q

T −

+ 1

Et = a(cid:48)Et−1 + (1 − a(cid:48)Et−1)ξ2
t
≤ a(cid:48)Ct−1 + (1 − a(cid:48)Ct−1)ξ2
t
≤ Ct,

.

(62)

(63)

(64)
(65)

in which the ﬁrst inequality is because ξ2
by mathematical induction we have that ∀t, Et ≤ Ct.

t ≤ 1 and the second inequality is due to Eq. (47). Therefore

q

d−1 ≤ L
and t ≥ d−1
Next, if d ≥ 4,
ˆL
E[Et[Et]] = E[Et] ≥ 1
1−a(cid:48) B, and Var[Et[Et]] ≤ 2
1
2
Et[Et] = a(cid:48)(1 − q

q , by Lemma 8 and Lemma 9, if we set B = q

d−1 , then
1−(a(cid:48))2 B2. Meanwhile, if t ≥ 1, then

1

q

d−1 )Et−1 + q
(cid:20)

E

1
Et[Et]

1

(cid:21)

≤

d−1 ≥ B. Therefore, by Lemma 10 we have
(cid:32) 4 2
q
1
2
(cid:18) 32
q

1
1
1−a(cid:48)
2
d − 1
q

1
1−(a(cid:48))2
1
1−a(cid:48)

(1 − a(cid:48))

q
d−1

+ 2

1 − a(cid:48)
1 − (a(cid:48))2 + 2
(cid:19)

(cid:33)

=

(cid:19)

≤

=

d
q
d
q

(1 − a(cid:48))

(cid:18) 32
q

+ 2

L
L(cid:48)

(cid:18) 32
q

(cid:19)

+ 2

.

(66)

(67)

(68)

(69)

(cid:16) 32

(cid:17)

Since Et ≤ Ct, Et[Et] ≤ Et[Ct]. Let s :=

(cid:108) d
q

(cid:109)
, then ∀t ≥ s, E

(cid:104)

1
Et[Ct]

(cid:105)

(cid:104)

≤ E

(cid:105)

≤

1
Et[Et]

L
L(cid:48)

q + 2

d
. Now imagine that we run History-PRGF algorithm with xs as the random initial-
q
ization in Algorithm 1, and set p0 to vs−1. Then quantities in iteration t (e.g. xt, vt, Ct) in the
imaginary setting have the same distribution as quantities in iteration t + s (e.g. xt+s, vt+s, Ct+s)
in the original algorithm (indeed, the quantities before iteration t in the imaginary setting have
the same joint distribution as the quantities from iteration s to iteration t + s − 1 in the original
algorithm), and Ft−1 in the imaginary setting corresponds to Ft+s−1 in the original algorithm. Now
we apply Proposition 3 to the imaginary setting, and we note that if we set xﬁx to the original x0,

21

then the condition in Proposition 3 holds (since by Remark 10, f (xs) ≤ f (x0)). Since quantities in
iteration t in the original algorithm correspond to quantities in iteration t − s in the imaginary setting,
Proposition 3 tells us that if T > s, we have

E[f (xT )] − f (x∗) ≤

so

(cid:104)

2L(cid:48)R2 (cid:80)T −1
1
Et[Ct]
t=s
(T − s)(T − s + 1)

E

E[f (xT )] − f (x∗) ≤

(cid:18) 32
q

+ 2

(cid:19) 2L d
q R2
(cid:109)
(cid:108) d
q

T −

+ 1

(cid:105)

,

.

(70)

(71)

B.5.2 Proofs of Lemma 8, 9 and 10

In this section, we ﬁrst present Lemma 11 for the proof of Lemma 8.

Lemma 11. Suppose d ≥ 3, then Var[ξ2

t ] < 2q

(d−1)2 .

Proof. For convenience, denote D := d − 1 in the following. By Proposition 6, the distribution of
t is the same as the distribution of (cid:80)q
i , where (z1, · · · , zq)(cid:62) ∼ U(SD−1). We note that the
ξ2
distribution of z is the same as the distribution of x

(cid:107)x(cid:107) , where x ∼ N (0, I). Therefore,

i=1 z2



(cid:32) q

(cid:88)

E



(cid:33)2

z2
i

 = E





(cid:32) q

(cid:88)

i=1

i=1

(cid:33)2

 = E






x2
i
(cid:107)x(cid:107)2

(cid:0)(cid:80)q
(cid:16)(cid:80)D

i=1 x2
i
i=1 x2
i

(cid:1)2

(cid:17)2




 .

(72)

By Theorem 1 in [14],
(cid:16)(cid:80)D

(cid:17)2

i=1 x2
i

are independently distributed, which implies

(cid:80)q

i=1 x2
(cid:107)x(cid:107)2

i

and (cid:107)x(cid:107)2 are independently distributed. Therefore,

((cid:80)q
((cid:80)D

i=1 x2
i=1 x2

i )2
i )2 and






E

(cid:0)(cid:80)q
(cid:16)(cid:80)D

i=1 x2
i
i=1 x2
i

(cid:1)2

(cid:17)2




 =

(cid:104)(cid:0)(cid:80)q
E
(cid:20)(cid:16)(cid:80)D

E

i=1 x2
i

i=1 x2
i

(cid:1)2(cid:105)
(cid:17)2(cid:21) .

(73)

We note that (cid:80)q
E (cid:2)(cid:80)q
i=1 x2
i
Var (cid:2)(cid:80)q
i=1 x2
i

(cid:3) = q, and Var (cid:2)(cid:80)q
(cid:3) = q(q + 2). Hence

i=1 x2
i

i=1 x2

i follows the chi-squared distribution with q degrees of freedom. Therefore,
+

(cid:3) = 2q. Therefore, E

= E (cid:2)(cid:80)q

(cid:104)(cid:0)(cid:80)q

(cid:1)2(cid:105)

(cid:3)2

i=1 x2
i

i=1 x2
i

Var

(cid:34) q

(cid:88)

i=1

(cid:35)

z2
i

= E





(cid:32) q

(cid:88)

i=1

(cid:33)2

z2
i

 − E

(cid:34) q

(cid:88)

(cid:35)2

z2
i

=

i=1

q(q + 2)
D(D + 2)

−

q2
D2 =

2q(D − q)
D2(D + 2)

<

2q
D2 .

(74)

Since D = d − 1, the proof is complete.

Then, the detailed proof of Lemma 8 is as follows.

22

Proof. By the law of total variance, using Proposition 7 and Lemma 11, we have

Var[Et] = E[Var[Et|Et−1]] + Var[E[Et|Et−1]]

t ]] + Var[a(cid:48)(1 − E[ξ2
t ]E[(1 − a(cid:48)Et−1)2] + (a(cid:48))2(1 − E[ξ2
t ](E[(1 − a(cid:48)Et−1)]2 + Var[1 − a(cid:48)Et−1]) + (a(cid:48))2(1 − E[ξ2
t ](E[(1 − a(cid:48)Et−1)]2 + (a(cid:48))2Var[Et−1]) + (a(cid:48))2(1 − E[ξ2

t ])Et−1]
t ])2Var[Et−1]

t ])2Var[Et−1]
t ])2Var[Et−1]

= E[(1 − a(cid:48)Et−1)2Var[ξ2
= Var[ξ2
= Var[ξ2
= Var[ξ2
= (a(cid:48))2(Var[ξ2
≤ (a(cid:48))2(Var[ξ2
(cid:32)

t ] + (1 − E[ξ2
t ] + (1 − E[ξ2
(cid:18)
2q
(d − 1)2 +

≤ (a(cid:48))2

1 −

t ]E[(1 − a(cid:48)Et−1)]2
t ])2)Var[Et−1] + Var[ξ2
t ])2)Var[Et−1] + Var[ξ2
t ]

(cid:19)2(cid:33)

q
d − 1

Var[Et−1] +

2q
(d − 1)2 .

(75)

(76)

(77)

(78)

(79)

(80)

(81)

(82)

If d ≥ 4, then
Therefore we have

2q

(d−1)2 + (1 − q

d−1 )2 = 1 − q

(d−1)2 (2(d − 1) − q − 2) ≤ 1 − q

(d−1)2 (d − q) ≤ 1.

Var[Et] ≤ (a(cid:48))2Var[Et−1] +

2q
(d − 1)2 .

(83)

Letting a := (a(cid:48))2, b := 2q
Var[Et] − b
Var[Et] ≤ b
Finally, since Var[Et[Et]] = Var[E[Et|Et−1]] ≤ Var[Et], the proof is completed.

(d−1)2 , then Var[Et] ≤ aVar[Et−1] + b and 0 ≤ a < 1. We have
1−a ), hence

1−a ) ≤ . . . ≤ at(Var[E0] − b
1−a =

1−a ) ≤ a2(Var[Et−2] − b
1−a ≤ b

1−a ≤ a(Var[Et−1] − b
1−a − at( b

1−a − Var[E0]) = (1 − at) b

2q
(d−1)2 .

1
1−(a(cid:48))2

The detailed proof of Lemma 9 is as follows.

Proof. Similar to the proof of Proposition 9, letting a := a(cid:48)(1 − q
(1 − at) b
we have

3 . Meanwhile, since q

1−a , and 1−a(cid:48)

d−1 ≤ L
ˆL

1−a ≥ 2

, a ≤ (1 − q

d−1 ) and b := q

d−1 , then E[Et] =
d−1 )3. Therefore, if t ≥ d−1
q ,

at ≤ (1 −

q
d − 1

)3 d−1

q ≤ exp(−

q
d − 1

)3 d−1

q = e−3.

Since 1−a(cid:48)

1−a ≥ 2

3 and at ≤ e−3, we have

E[Et] = (1 − at)

b
1 − a

≥

2
3

(1 − e−3)

1
1 − a(cid:48)

q
d − 1

≥

1
2

1
1 − a(cid:48)

q
d − 1

.

The detailed proof of Lemma 10 is as follows.

Proof. By Chebyshev’s Inequality, we have

Pr(X <

1
2

E[X]) ≤

Var[X]
E[X])2
( 1
2

≤

4σ2
µ2 .

Hence

E

(cid:21)

(cid:20) 1
X

≤

1
B

(cid:18)

Pr

X <

(cid:19)

E[X]

+

1
2

1
E[X]

1
2

≤

1
B

(cid:18) 4σ2

µ2 +

(cid:19)

2
µ

=

1
µB

(cid:18) 4σ2
µ

(cid:19)

+ 2

.

23

(84)

(85)

(86)

(87)

B.5.3 Proof of Theorem 4

As in Section B.5.1, similar to Eq. (47), we deﬁne {Et}T −1

t=0 as follows: E0 = 0, and

t := (cid:80)q

Et = a(cid:48)Et−1 + (1 − a(cid:48)Et−1)ζ 2
t ,

(88)
d−1 } and ξti is deﬁned in Proposition 5. Here we use ζ 2
t
t in Eq. (88) to obtain a tighter bound when using McDiarmid’s inequality in the proof of

ti := min{ξ2
ti,

i=1 ζ 2

ti, ζ 2

1

where ζ 2
instead of ξ2
Lemma 12.

Here we ﬁrst give Lemma 12 for the proof of Theorem 4.

Lemma 12 (Proof in Section B.5.4). Pr

(cid:16) 1
T

(cid:80)T −1
t=0

Et[Et] < 0.1 q
d

1
1−a(cid:48)

(cid:17)

≤ exp(−0.02T ).

Then, we provide the proof of Theorem 4 in the following.
Theorem 4 (History-PRGF, smooth and strongly convex). Under the same conditions as in Theorem 2
(f is τ -strongly convex), when using the History-PRGF estimator, assuming d ≥ 4,
≤ 1,
q
d ≤ 0.2 L

τ , and T ≥ 5 d

d−1 ≤ L
ˆL

q , we have

q

E[δT ] ≤ 2 exp(−0.1

q
d

τ
L

T )δ0.

Proof. Since E0 = 0 ≤ C0, and if Et−1 ≤ Ct−1, then

Et = a(cid:48)Et−1 + (1 − a(cid:48)Et−1)

≤ a(cid:48)Et−1 + (1 − a(cid:48)Et−1)

q
(cid:88)

i=1
q
(cid:88)

i=1

ζ 2
ti

ξ2
ti

= a(cid:48)Et−1 + (1 − a(cid:48)Et−1)ξ2
t
≤ a(cid:48)Ct−1 + (1 − a(cid:48)Ct−1)ξ2
t
≤ Ct,

(89)

(90)

(91)

(92)

(93)
(94)

in which the second inequality is due to that ξ2
Therefore by mathematical induction we have that ∀t, Et ≤ Ct.
(cid:80)T −1
Et[Et] ≤ 1
Then, since ∀t, Et ≤ Ct, we have 1
t=0
T
T
(cid:80)T −1
Et[Ct] < 0.1 q
≤ exp(−0.02T ). Let kT = exp
we have Pr
t=0
d

(cid:80)T −1
t=0
(cid:17)
1
1−a(cid:48)

Et[Ct]. Therefore, by Lemma 12
(cid:17)
(cid:16) τ
Et[Ct]
.
L(cid:48)

(cid:80)T −1
t=0

t ≤ 1 and the third inequality is due to Eq. (47).

(cid:16) 1
T
1−a(cid:48) = L(cid:48)
L ,

1

Since

(cid:16)

Pr

kT < exp

(cid:16)

0.1

q
d

τ
L

T

(cid:17)(cid:17)

≤ exp(−0.02T ).

Meanwhile, let δt := f (xt) − f (x∗), Theorem 2 tells us that

E[δT kT ] ≤ δ0.

Noting that δ0 ≥ δT , we have
δ0 ≥ E[δT kT ]

τ

τ

d

d

(cid:17)

(cid:17)

L T )]

L T )]
E[δT 1kT ≥exp(0.1 q
(E[δT ] − E[δT 1kT <exp(0.1 q
(E[δT ] − δ0E[1kT <exp(0.1 q
(cid:17) (cid:16)
E[δT ] − δ0 Pr

kT < exp

(cid:17)

(cid:16)

d

d

T

T

(cid:16)

(cid:16)

0.1

0.1

≥ exp

= exp

≥ E[δT kT 1kT ≥exp(0.1 q
q
d
q
d
q
d
q
d
q
d

τ
L
τ
L
τ
L
τ
L
τ
L

= exp

≥ exp

≥ exp

0.1

0.1

0.1

(cid:16)

(cid:17)

(cid:16)

(cid:16)

T

T

T

(E[δT ] − δ0 exp(−0.02T )),

τ

L T )])

τ

L T )])
(cid:16)
q
d

0.1

24

(95)

(96)

(97)
(98)

(99)

(100)

(101)

(102)

(103)

(cid:17)(cid:17)(cid:17)

τ
L

T

in which 1B denotes the indicator function of the event B (1B = 1 when B happens and 1B = 0
when B does not happen). By rearranging we have
(cid:17)

(cid:16)

τ , 0.1 q
When q
d ≤ 0.2 L
E[δT ] ≤ 2 exp (cid:0)−0.1 q

d
τ

d

τ
L

−0.1

E[δT ] ≤ exp

q
d
L T ≤ 0.02T , and hence exp (cid:0)−0.1 q
L T (cid:1) δ0. The proof of the theorem is completed.

δ0 + exp(−0.02T )δ0.

T

d

τ

τ

L T (cid:1) ≥ exp(−0.02T ). Therefore,

(104)

B.5.4 Proof of Lemma 12

In this section, we ﬁrst give two lemmas for the proof of Lemma 12.
Lemma 13. If d ≥ 4, then E[ζ 2

t ] ≥ 0.3 q

d−1 .

Proof. We note that the distribution of ui is the uniform distribution from the unit sphere in the
(d − 1)-dimensional subspace A. Since ξti := ∇f (xt)A
ui, ξti is indeed the inner product between
one ﬁxed unit vector and one uniformly random sampled unit vector. Therefore, its distribution is the
same as z1, where (z1, z2, . . . , zd−1) are uniformly sampled from Sd−2, i.e. the unit sphere in Rd−1.
Now it sufﬁces to prove that E[min{z2
1,

d−1 }] ≥ 0.3
d−1 .

(cid:62)

1

Let p(·) denote the probability density function of z1. For convenience let D := d − 1. We have
D
p(0) = SD−2
, where SD−1 denotes the surface area of SD−1. Since SD−1 = 2π
2
2 ) where Γ(·) is
Γ( D
SD−1
(cid:113) d−1
(cid:113) D−1
the Gamma function, and by [24] we have Γ( D
2 )
2 , we have p(0) ≤
2π .
Γ( D−1
2 )
1−x2)D−2
Meanwhile, we have p(x) = p(0) · (
1 − x2)D−3. If d ≥ 4, then D ≥ 3, and
1−x2 = p(0) · (
√
we have ∀x ∈ [−1, 1], p(0) ≥ p(x). Therefore,

(cid:113) D−1
√

2π ≤

≤

√

(cid:18)

z2
1 ≥

Pr

(cid:19)

1
d − 1

= 1 − Pr

(cid:18)

|z1| <

√

(cid:19)

1
d − 1

≥ 1 −

√

2
d − 1

p(0) = 1 −

(cid:114) 2
π

≥ 0.2.

(105)

Similarly we have

(cid:18)

Pr

z2
1 ≥

(cid:19)

0.25
d − 1

= 1 − Pr

(cid:18)

|z1| <

√

(cid:19)

0.5
d − 1

≥ 1 −

√

p(0) = 1 −

(cid:114) 1
2π

≥ 0.6.

(106)

1
d − 1
(cid:16)

Let z(cid:48)
1

2 := min{z2
1,

1
d−1 }. Then Pr

(cid:16)

z(cid:48)
1

2 ≥ 1
d−1

(cid:17)

≥ 0.2 and Pr

(cid:17)

2 ≥ 0.25
d−1

≥ 0.6. Then

z(cid:48)
1

E[z(cid:48)
1

2] ≥

=

≥

1
d − 1
0.75
d − 1
0.3
d − 1

(cid:18)

(cid:18)

Pr

Pr

2 ≥

z(cid:48)
1

2 ≥

z(cid:48)
1

(cid:19)

(cid:19)

+

+

1
d − 1
1
d − 1

0.25
d − 1
0.25
d − 1

(cid:18) 1

d − 1

(cid:18)

z(cid:48)
1

2 ≥

Pr

Pr

≥ z(cid:48)
1

2 ≥

(cid:19)

0.25
d − 1

(cid:19)

0.25
d − 1

.

(107)

(108)

(109)

Hence E[ζ 2

ti] ≥ 0.3

d−1 . By the deﬁnition of ζ 2

t the lemma is proved.

Lemma 14. If

q

d−1 ≤ L
ˆL

, T ≥ 5 d

q , then 1

T

(cid:80)T −1
t=0

E[Et] ≥ 0.2

q
d−1

1−a(cid:48) .

(cid:16)

1 − 0.3 q
d−1

(cid:17)

a(cid:48)Et−1 + 0.3 q

d−1 . Taking

Proof. By Eq. (88) and Lemma 13, we have Et[Et] ≥
expectation to both sides, we have
(cid:18)

(cid:19)

E[Et] ≥

1 − 0.3

a(cid:48)E[Et−1] + 0.3

q
d − 1

q
d − 1

.

(110)

(cid:16)

Let a :=
E[Et] − b

1 − 0.3 q
d−1

(cid:17)

1−a ≥ a(E[Et−1] − b

a(cid:48), b := 0.3 q

d−1 , then E[Et] ≥ aE[Et−1] + b and 0 ≤ a < 1. We have
1−a ), hence

1−a ) ≥ . . . ≥ at(E[E0] − b

1−a ) ≥ a2(E[Et−2] − b

25

E[Et] ≥ b

1−a − at( b

1−a − E[E0]) = (1 − at) b

1−a . Hence we have

1
T

T −1
(cid:88)

t=0

E[Et] ≥

b
1 − a

(cid:18)

1 −

(cid:19)

1 − aT
(1 − a)T

≥

b
1 − a

(cid:18)

1 −

1
(1 − a)T

(cid:19)

.

Since 1 − a = 1 − (1 − 0.3 q

d−1 )(1 − L
ˆL
1 − (1 − L
)2
ˆL
1 − (1 − 0.3 L
)(1 − L
L
ˆL
ˆL
ˆL
q , then T ≥ 5 1
2.3 . Letting T ≥ 5 d

)2 ≤ 1 − (1 − 0.3 L
ˆL
(1 − L
+ L
)
ˆL
ˆL
(1 − L
) + 0.3 L
ˆL
ˆL
a(cid:48) ≥ 5 1

L
ˆL
(1 − L
ˆL

)(1 − L
ˆL

+ L
ˆL

≥ 5

)2

)2

=

1−

1
√

≥

≥ 5 1
L
ˆL

q
d

)2, noting that

2
2.3

,

we have 1−a(cid:48)
have

1−a ≥ 2

(111)

(112)

1−a . By Eq. (111) we

1
T

T −1
(cid:88)

t=0

E[Et] ≥

2
2.3

b
1 − a(cid:48)

4
5

=

2.4
11.5

q
d−1

1 − a(cid:48) ≥ 0.2

q
d−1

1 − a(cid:48) .

(113)

Finally, the detailed proof of Lemma 12 is as follows.

Proof. Let E := 1
1 , ζ 2
T
a function of them. Now suppose that the value of ζ 2
{ζ 2

Et[Et]. We note that {ζ 2

T −1} are unchanged. Then

1 , . . . , ζ 2

t−1, ζ 2

(cid:80)T −1
t=0

t+1, . . . , ζ 2
∆Es = 0,
∆Es = (1 − a(cid:48)Et−1)∆ζ 2
∆Es = (1 − ζ 2

t ≤ ∆ζ 2
t ,

s )a(cid:48)∆Es−1 ≤ a(cid:48)∆Es−1,

2 , . . . , ζ 2
t is changed by ∆ζ 2

T −1} are independent, and E is
t , while the value of

0 ≤ s ≤ t − 1;

s = t;

s ≥ t + 1.

(114)

(115)

(116)

Therefore, for s ≥ t, ∆Es ≤ (a(cid:48))s−t∆Et ≤ (a(cid:48))s−t∆ζ 2
s ])Es−1 + E[ζ 2
Es[Es] = a(cid:48)(1 − E[ζ 2
T −1
(cid:88)

T −1
(cid:88)

∆E =

1
T

∆Es[Es] ≤

(a(cid:48))s−1−t∆ζ 2

t ≤

s=0

s=t+1

1
T

s ], so ∆Es[Es] ≤ a(cid:48)∆Es−1 ≤ ∆Es−1. Hence

1
T

1

1 − a(cid:48) ∆ζ 2
t .

(117)

t ; for s < t, ∆Es = 0. By Eq. (88),

Since ζ 2
ti := min{ξ2
ti,
armid’s inequality, we have

1

d−1 }, 0 ≤ ζ 2

t ≤ q

d−1 . Therefore ∆E ≤ 1

T

1
1−a(cid:48)

q
d−1 . Therefore, by McDi-

Pr(E < E[E] − (cid:15)) ≤ exp




−

2(cid:15)2

(cid:16) 1
T

T

1
1−a(cid:48)

q
d−1



(cid:32)


 ≤ exp

(cid:17)2

−2T

(cid:18)

(cid:15)(1 − a(cid:48))

d − 1
q

(cid:19)2(cid:33)

.

(118)

Let (cid:15) = 0.1

q
d−1

1−a(cid:48) , we have Pr(E < E[E] − 0.1

q
d−1

1−a(cid:48) ) ≤ exp(−0.02T ). By Lemma 14, E[E] ≥

q
d−1

0.2

1−a(cid:48) . Noting that q

d ≤ q

d−1 , the proof is completed.

C Supplemental materials for Section 4

C.1 Proof of Theorem 5

Theorem 5. In Algorithm 2, if θt is Ft−1-measurable, we have


(f (xT ) − f (x∗))

E

(cid:32)

1 +

√

γ0
2

T −1
(cid:88)

(cid:112)

t=0

(cid:33)2

θt

 ≤ f (x0) − f (x∗) +

γ0
2

(cid:107)x0 − x∗(cid:107)2.

(119)

To help understand the design of Algorithm 2, we present the proof sketch below, where the part
which is the same as the original proof in [26] is omitted.

26

Sketch of the proof. Since xt+1 = yt − 1
ˆL

g1(yt) and g1(yt) = ∇f (yt)(cid:62)vt · vt, by Lemma 1,

Et[f (xt+1)] ≤ f (yt) −

Et

(cid:104)(cid:0)∇f (yt)(cid:62)vt
2L(cid:48)

(cid:1)2(cid:105)

≤ f (yt) −

Et

(cid:104)(cid:0)∇f (yt)(cid:62)vt
2 ˆL

(cid:1)2(cid:105)

.

(120)

Deﬁne ρt := γt

2 (cid:107)mt − x∗(cid:107)2 + f (xt) − f (x∗). The same as in original proof, we have

ρt+1 =

γt+1
2

(cid:107)mt − x∗(cid:107)2 − αtg2(yt)(cid:62)(mt − x∗) +

θt
2

(cid:107)g2(yt)(cid:107)2 + f (xt+1) − f (x∗).

(121)

Then we derive Et[ρt+1]. We mentioned in Remark 2 that the notation Et[·] means the conditional
expectation E[·|Ft−1], where Ft−1 is a sub σ-algebra modelling the historical information, and
we require that Ft−1 includes all the randomness before iteration t. Therefore, γt and mt are
Ft−1-measurable. The assumption in Theorem 5 requires that θt is Ft−1-measurable. Since αt is
determined by γt and θt (through a Borel function), αt is also Ft−1-measurable. We have

Et[ρt+1]
γt+1
2

(cid:107)mt − x∗(cid:107)2 − αtEt[g2(yt)](cid:62)(mt − x∗) +

θt
2

Et[(cid:107)g2(yt)(cid:107)2] + Et[f (xt+1)] − f (x∗)

(122)

(cid:107)mt − x∗(cid:107)2 − αt∇f (yt)(cid:62)(mt − x∗) +

(cid:107)mt − x∗(cid:107)2 − αt∇f (yt)(cid:62)(mt − x∗) +

θt
2
Et

Et[(cid:107)g2(yt)(cid:107)2] + Et[f (xt+1)] − f (x∗) (123)
(cid:104)(cid:0)∇f (yt)(cid:62)vt
2 ˆL

+ Et[f (xt+1)] − f (x∗)

(cid:1)2(cid:105)

(cid:107)mt − x∗(cid:107)2 − αt∇f (yt)(cid:62)(mt − x∗) + f (yt) − f (x∗)

=

=

≤

≤

γt+1
2

γt+1
2

γt+1
2

≤ (1 − αt)ρt,

(124)

(125)

(126)

where the ﬁrst equality is because mt, αt and θt are Ft−1-measurable, the second equality is
because Et[g2(yt)] = ∇f (yt), the ﬁrst inequality is because θt ≤
, the second
inequality is because of Eq. (120), and the last inequality is the same as in original proof. By the
similar reasoning to the proof of Theorem 2, we have E
≤ ρ0. By the original proof,
(cid:81)T −1
1

(cid:104)(∇f (yt)(cid:62)vt)2(cid:105)
ˆL·Et[(cid:107)g2(yt)(cid:107)2]

t=0 (1−αt)

(cid:81)T −1

Et

ρT

(cid:105)

(cid:104)

t=0 (1 − αt) ≤

√

(cid:16)

1+

γ0
2

(cid:80)T −1
t=0

√

θt

(cid:17)2 , completing the proof.

From the proof sketch, we see that

• The requirement that Et[g2(yt)] = ∇f (yt) is to ensure that Eq. (123) holds.

• The constraint on θt in Line 3 of Algorithm 2 is to ensure that Eq. (124) holds.
• The choice of g1(yt) (g1(yt) = ∇f (yt)(cid:62)vt · vt) and update of xt (xt+1 = yt − 1
ˆL

g1(yt))
is the same as in Algorithm 1, i.e. the greedy descent framework. This is since Eq. (125)
requires that Et[f (xt+1)] decreases as much as possible from f (yt).

• From Eq. (121) to Eq. (122), we require Et[αtg2(yt)(cid:62)(mt − x)] = αtEt[g2(yt)](cid:62)(mt − x)
and Et[θt(cid:107)g2(yt)(cid:107)2] = θtEt[(cid:107)g2(yt)(cid:107)2]. Therefore, to make the two above identities hold,
by the property of “pulling out known factors” in taking conditional expectation, we require
that mt, αt and θt are Ft−1-measurable. Since we make sure in Remark 2 that Ft−1 always
includes all the randomness before iteration t, and αt is determined by γt and θt, it sufﬁces to
let θt be Ft−1-measurable. We note that “being Ft−1-measurable” means “being determined
by the history, i.e. ﬁxed given the history”.

Now we present the complete proof of Theorem 5.

27

Proof. Since xt+1 = yt − 1
ˆL

g1(yt) and g1(yt) = ∇f (yt)(cid:62)vt · vt, by Lemma 1,

Et[f (xt+1)] ≤ f (yt) −

Et

(cid:1)2(cid:105)

(cid:104)(cid:0)∇f (yt)(cid:62)vt
2L(cid:48)
(cid:104)(cid:0)∇f (yt)(cid:62)vt
2 ˆL
2 (cid:107)mt − x(cid:107)2 + f (xt) − f (x). Then

(cid:1)2(cid:105)

Et

.

≤ f (yt) −

For an arbitrary ﬁxed x, deﬁne ρt(x) := γt

ρt+1(x) =

=

γt+1
2
γt+1
2

(cid:107)mt+1 − x(cid:107)2 + f (xt+1) − f (x)

(cid:107)mt − x(cid:107)2 −

γt+1θt
αt

g2(yt)(cid:62)(mt − x) +

γt+1θ2
t
2α2
t

(cid:107)g2(yt)(cid:107)2 + f (xt+1) − f (x)

=

γt+1
2

(cid:107)mt − x(cid:107)2 − αtg2(yt)(cid:62)(mt − x) +

θt
2

(cid:107)g2(yt)(cid:107)2 + f (xt+1) − f (x).

We make sure in Remark 2 that Ft−1 always includes all the randomness before iteration t. Therefore,
γt and mt are Ft−1-measurable. The assumption in Theorem 5 requires that θt is Ft−1-measurable.
Since αt is determined by γt and θt (through a Borel function), αt is also Ft−1-measurable. Since
mt, αt and θt are Ft−1-measurable, we have Et[αtg2(yt)(cid:62)(mt − x)] = αtEt[g2(yt)](cid:62)(mt − x) and
Et[θt(cid:107)g2(yt)(cid:107)2] = θtEt[(cid:107)g2(yt)(cid:107)2]. Hence

Et[ρt+1(x)] =

γt+1
2

(cid:107)mt − x∗(cid:107)2 − αtEt[g2(yt)](cid:62)(mt − x∗) +

θt
2

Et[(cid:107)g2(yt)(cid:107)2] + Et[f (xt+1)] − f (x∗)

(132)

(cid:107)mt − x(cid:107)2 − αt∇f (yt)(cid:62)(mt − x) +

θt
2

Et[(cid:107)g2(yt)(cid:107)2] + Et[f (xt+1)] − f (x)
(133)

+ Et[f (xt+1)] − f (x)

(cid:107)mt − x(cid:107)2 − αt∇f (yt)(cid:62)(mt − x) +

(cid:1)2(cid:105)

Et

(cid:104)(cid:0)∇f (yt)(cid:62)vt
2 ˆL

(cid:107)mt − x(cid:107)2 − αt∇f (yt)(cid:62)(mt − x) + f (yt) − f (x)

(cid:107)mt − x(cid:107)2 − ∇f (yt)(cid:62)(αtmt − αtx) + f (yt) − f (x)

(cid:107)mt − x(cid:107)2 + ∇f (yt)(cid:62)(−yt + (1 − αt)xt + αtx) + f (yt) − f (x)

(cid:107)mt − x(cid:107)2 + f ((1 − αt)xt + αtx) − f (x)

(cid:107)mt − x(cid:107)2 + (1 − αt)f (xt) − (1 − αt)f (x)

= (1 − αt)

(cid:107)mt − x(cid:107)2 + f (xt) − f (x)

(cid:17)

(cid:16) γt
2

= (1 − αt)ρt(x).

Therefore,

ρ0(x) = E[ρ0(x)] ≥ E

(cid:21)

= E

(cid:20)
E0

(cid:20) E0[ρ1(x)]
1 − α0
(cid:21)

E1[ρ2(x)]
(1 − α0)(1 − α1)

(cid:20)

(cid:20)
E1

= E

(cid:21)

= E

(cid:21)(cid:21)

(cid:20) ρ1(x)
1 − α0
ρ2(x)
(1 − α0)(1 − α1)

(cid:20) ρ1(x)
1 − α0
(cid:20)

(cid:21)(cid:21)

= E

ρ2(x)
(1 − α0)(1 − α1)

(127)

(128)

(129)

(130)

(131)

(134)

(135)

(136)

(137)

(138)

(139)

(140)

(141)

(cid:21)

=

γt+1
2

≤

≤

=

=

≤

≤

γt+1
2

γt+1
2
γt+1
2
γt+1
2
γt+1
2
γt+1
2

(cid:20)

≥ E

≥ . . .
(cid:34)

≥ E

ρT (x)
t=0 (1 − αt)

(cid:81)T −1

(cid:35)

.

28

We have ρT (x) ≥ f (xT ) − f (x). To prove the theorem, let x = x∗. The remaining is to give an
upper bound of (cid:81)T −1

, we have

t=0 (1 − αt) and ak := 1√
ψk

ak+1 − ak =

t=0 (1 − αt). Let ψk := (cid:81)k−1
1
√
ψk
ψk − ψk+1
√

1
(cid:112)ψk+1

√

=

−

ψk − (cid:112)ψk+1
(cid:112)ψkψk+1

=

ψk − ψk+1

√

(cid:112)ψkψk+1(

ψk + (cid:112)ψk+1)

(142)

(143)

(144)

(145)

=

αk
2(cid:112)ψk+1

=

(cid:112)γk+1θk
2(cid:112)ψk+1

=

√

θk
2

(cid:114) γk+1
ψk+1

√

γ0
2

(cid:80)T −1
t=0

√

θt. Therefore, ψT ≤

(cid:16)

1+

√

γ0
2

1

(cid:80)T −1
t=0

(cid:17)2 .

√

θt

≥

=

=

(cid:112)ψkψk+1(2
ψk − (1 − αk)ψk

ψk)

(cid:112)ψk+1

√

2ψk
γ0θk
2

.

Since ψ0 = 1, a0 = 1. Hence aT ≥ 1 +

The proof is completed.

C.2 Construction of g2(yt)

We ﬁrst note that in PARS, the speciﬁcation of Ft−1 is similar to that in Example 2. That is,
we suppose that pt is determined before sampling {u1, u2, . . . , uq}, but it could depend on extra
randomness in iteration t. We let Ft−1 also includes the extra randomness of pt in iteration t (not
including the randomness of {u1, u2, . . . , uq}) besides the randomness before iteration t. Meanwhile,
we note that the assumption in Theorem 5 requires that θt is Ft−1-measurable, and this is satisﬁed
if the algorithm to ﬁnd θt in Algorithm 2 is deterministic given randomness in Ft−1 (does not
use {u1, u2, . . . , uq} in iteration t). Since Ft−1 includes randomness before iteration t, if θt is
Ft−1-measurable, we can show that yt is Ft−1-measurable.

We also note that in Section 4 and Appendix C, we let Dt :=

(cid:16)

∇f (yt)

(cid:17)2

(cid:62)

pt

, which is different

from the previous deﬁnition Dt :=
∇f (xt)
ARS-based algorithms, we care about gradient estimation at yt instead of that at xt.

in Section 3 and Appendix B. This is because in

pt

(cid:16)

(cid:62)

(cid:17)2

In Algorithm 2, we need to construct g2(yt) as an unbiased estimator of ∇f (yt) satisfying
Et[g2(yt)] = ∇f (yt). Since Theorem 5 tells us that a larger θt could potentially accelerate con-
vergence, by Line 3 of Algorithm 2, we want to make Et[(cid:107)g2(yt)(cid:107)2] as small as possible. To save
queries, we hope that we can reuse the queries ∇f (yt)(cid:62)pt and {∇f (yt)(cid:62)ui}q
i=1 used in the process
of constructing g1(yt).

Here we adopt the construction process in Section B.3.3, and leave the discussion of alternative ways
in Section C.2.1. We note that if we let H be the (d − 1)-dimensional subspace perpendicular to pt,
then

∇f (yt) = ∇f (yt)(cid:62)pt · pt + (I − ptp(cid:62)

(146)
Therefore, we need to obtain an unbiased estimator of ∇f (yt)H . This is straightforward since we
can utilize {ui}q
Proposition 10. For any 1 ≤ i ≤ q, Et[∇f (yt)(cid:62)ui · ui] = 1

i=1 which is uniformly sampled from the (d − 1)-dimensional space H.

t )∇f (yt) = ∇f (yt)(cid:62)pt · pt + ∇f (yt)H .

d−1 ∇f (yt)H .

Proof. We have Et[uiu(cid:62)

i ] = I−ptp(cid:62)

d−1

t

(See Section A.2 in [9] for the proof.). Therefore,

Et[∇f (yt)(cid:62)ui · ui] =

I − ptp(cid:62)
t
d − 1

∇f (yt) =

1
d − 1

∇f (yt)H .

(147)

Therefore,

g2(yt) = ∇f (yt)(cid:62)pt · pt +

d − 1
q

q
(cid:88)

i=1

∇f (yt)(cid:62)ui · ui

(148)

29

satisﬁes that Et[g2(yt)] = ∇f (yt). Then

Et[(cid:107)g2(yt)(cid:107)2] = (cid:107)∇f (yt)(cid:107)2Et

∇f (yt)

(cid:62)

pt · pt +

d − 1
q

(cid:62)

∇f (yt)

ui · ui





(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

= (cid:107)∇f (yt)(cid:107)2

(cid:32)

(cid:16)

(cid:62)

∇f (yt)

(cid:17)2

pt

+

(cid:20)(cid:16)

Et

(cid:62)

∇f (yt)

ui

q
(cid:88)

i=1

q
(cid:88)

i=1

(d − 1)2
q2
(cid:19)

= (cid:107)∇f (yt)(cid:107)2

(cid:18)

Dt +

d − 1
q

(1 − Dt)

,

2



(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:17)2(cid:21)(cid:33)

(149)

(150)

(151)

where the last equality is due to the fact that Et[uiu(cid:62)
∇f (yt)
d−1 ). Meanwhile, if we adopt an RGF estimator as g2(yt), then Et[(cid:107)g2(yt)(cid:107)2] = d
1−Dt
Noting that Dt+ d−1
especially when Dt is closed to 1, since it utilizes the prior information.

q (cid:107)∇f (yt)(cid:107)2.
q , our proposed unbiased estimator results in a smaller Et[(cid:107)g2(yt)(cid:107)2]

q (1−Dt) < d

(hence Et

d−1

i ] = I−ptp(cid:62)

t

(cid:20)(cid:16)

(cid:17)2(cid:21)

(cid:62)

ui

=

Finally, using g2(yt) in Eq. (148), when calculating the following expression which appears in Line 3
of Algorithm 2, the term (cid:107)∇f (yt)(cid:107)2 would be cancelled:

(cid:1)2(cid:105)

(cid:104)(cid:0)∇f (yt)(cid:62)vt
Et
ˆL · Et[(cid:107)g2(yt)(cid:107)2]

=

Et
(cid:16)

(cid:20)(cid:16)

∇f (yt)

(cid:62)

vt

(cid:17)2(cid:21)

ˆL

Dt + d−1

q (1 − Dt)

(cid:17) =

Dt + q
(cid:16)
Dt + d−1

d−1 (1 − Dt)
q (1 − Dt)

ˆL

(cid:17) ,

(152)

where the last equality is due to Lemma 2.

C.2.1 Alternative way to construct g2(yt)

Instead of using the orthogonalization process in Section B.3.3, when constructing g1(yt) as the
PRGF estimator, we can also ﬁrst sample q orthonormal vectors {ui}q
i=1 uniformly from Sd−1, and
then let pt be orthogonal to them with {ui}q
i=1 unchanged. Then we can construct g2(yt) using this
set of {∇f (yt)(cid:62)ui}q
Example 3 (RGF). Since {ui}q
construct an unbiased estimator of ∇f (yt). We let g2(yt) = d
q
it is an unbiased estimator of ∇f (yt), and Et[(cid:107)g2(yt)(cid:107)2] = d

i=1 are uniformly sampled from Sd−1, we can directly use them to
(cid:80)q
i=1 ∇f (yt)(cid:62)ui · ui. We show that

i=1 and ∇f (yt)(cid:62)pt.

q (cid:107)∇f (yt)(cid:107)2.

Proof. In Section B.3.2 we show that E[uiu(cid:62)

i ] = I

d . Therefore

Et[g2(yt)] =

d
q

q
(cid:88)

i=1

Et[uiu(cid:62)

i ]∇f (yt) =

d
q

q
(cid:88)

i=1

1
d

∇f (yt) = ∇f (yt).

Et[(cid:107)g2(yt)(cid:107)2] =

=

d2
q2

d2
q2

q
(cid:88)

i=1
q
(cid:88)

i=1

Et[(∇f (yt)(cid:62)ui)2] =

d2
q2

q
(cid:88)

i=1

∇f (yt)(cid:62)Et[uiu(cid:62)

i ]∇f (yt)

1
d

(cid:107)∇f (yt)(cid:107)2 =

d
q

(cid:107)∇f (yt)(cid:107)2.

We see that Et[(cid:107)g2(yt)(cid:107)2] here is larger than Eq. (151).
Example 4 (Variance reduced RGF). To reduce the variance of RGF estimator, we could use pt to
construct a control variate. Here we use pt to refer to the original pori
before orthogonalization
so that it is ﬁxed w.r.t. randomness of {u1, . . . , uq} (then it requires one additional query to obtain
i=1(∇f (yt)(cid:62)ui·ui−(∇f (yt)(cid:62)pt·pt)(cid:62)ui·ui)+
∇f (yt)(cid:62)pori
(cid:17)
∇f (yt)(cid:62)pt · pt. We show that it is unbiased, and Et[(cid:107)g2(yt)(cid:107)2] = (cid:107)∇f (yt)(cid:107)2 (cid:16)
.
q (1 − Dt)

). Speciﬁcally, we can let g2(yt) = d
q

Dt + d

(cid:80)q

t

t

30

Proof.

Et[(∇f (yt)(cid:62)pt · pt)(cid:62)ui · ui] = Et[uiu(cid:62)

i ]∇f (yt)(cid:62)pt · pt =

1
d

∇f (yt)(cid:62)pt · pt.

Therefore,

Et[g2(yt)] = Et

(cid:34)

d
q

q
(cid:88)

i=1

(cid:35)

∇f (yt)(cid:62)ui · ui

= ∇f (yt).

(cid:80)q

(cid:104) d
q

Let ∇f (yt)H := ∇f (yt) − ∇f (yt)(cid:62)pt · pt. We deﬁne that Var[x] for a stochastic vector x is
such that Var[x] = (cid:80)
i Var[xi]. Then for any stochastic vector x, E[(cid:107)x(cid:107)2] = (cid:107)E[x](cid:107)2 + Var[x].
We have Vart[g2(yt)] = Vart
H ui ·
ui. Then Et[g(cid:48)
q (cid:107)∇f (yt)H (cid:107)2. Therefore, Vart[g2(yt)] =
(cid:17)
Et[(cid:107)g(cid:48)
(cid:107)∇f (yt)(cid:107)2. Hence,
q − 1
(cid:18) d
q

H ui · ui
2(yt)] = ∇f (yt)H , Et[(cid:107)g(cid:48)
2(yt)(cid:107)2] = d
(cid:17)
(cid:107)∇f (yt)H (cid:107)2 = (1 − Dt)
2(yt)(cid:107)2] =
q − 1

Et[(cid:107)g2(yt)(cid:107)2] = (cid:107)Et[g2(yt)](cid:107)2 + Vart[g2(yt)] =

2(yt)(cid:107)2] − (cid:107)Et[g(cid:48)

i=1 ∇f (yt)(cid:62)

i=1 ∇f (yt)(cid:62)

2(yt) := d
q

1 + (1 − Dt)

(cid:107)∇f (yt)(cid:107)2

. Let g(cid:48)

− 1

(cid:19)(cid:19)

(cid:80)q

(cid:16) d

(cid:16) d

(cid:18)

(cid:105)

(cid:18)

=

Dt +

d
q

(cid:19)

(1 − Dt)

(cid:107)∇f (yt)(cid:107)2.

We see that Et[(cid:107)g2(yt)(cid:107)2] here is comparable but slightly worse (slightly larger) than Eq. (151).

In summary, we still favor Eq. (148) as the construction of g2(yt) due to its simplicity and smallest
value of Et[(cid:107)g2(yt)(cid:107)2] among the ones we propose.

C.3 Estimation of Dt and proof of convergence of PARS using this estimator

In zeroth-order optimization, Dt is not accessible since Dt =
. For the term inside
the square, while the numerator can be obtained from the oracle, we do not have access to the
denominator. Therefore, our task is to estimate (cid:107)∇f (yt)(cid:107)2.
To save queries, it is ideal to reuse the oracle query results used to obtain vt and g2(yt): ∇f (yt)(cid:62)pt
and {∇f (yt)(cid:62)ui}i∈{1,2,...,q}. Again, we suppose pt, u1, · · · , uq are obtained from the process in
Section B.3.3. By Eq. (146), we have

(cid:16) ∇f (yt)(cid:62)pt
(cid:107)∇f (yt)(cid:107)

(cid:17)2

(cid:107)∇f (yt)(cid:107)2 = (∇f (yt)(cid:62)pt)2 + (cid:107)∇f (yt)H (cid:107)2.

(153)

Since {ui}q
Proposition 11. For any 1 ≤ i ≤ q, Et[(∇f (yt)(cid:62)ui)2] = 1

i=1 is uniformly sampled from the (d − 1)-dimensional space H,
d−1 (cid:107)∇f (yt)H (cid:107)2.

Proof. By Proposition 10, Et[∇f (yt)(cid:62)ui · ui] = 1

d−1 ∇f (yt)H . Therefore,

Et[(∇f (yt)(cid:62)ui)2] = ∇f (yt)(cid:62)Et[∇f (yt)(cid:62)ui · ui] =

1
d − 1

∇f (yt)(cid:62)∇f (yt)H

=

1
d − 1

(cid:107)∇f (yt)H (cid:107)2.

Thus, we adopt the following unbiased estimate:

(cid:107)∇f (yt)H (cid:107)2 ≈

d − 1
q

q
(cid:88)

i=1

(cid:0)∇f (yt)(cid:62)ui

(cid:1)2

31

(154)

(155)

(156)

By Johnson-Lindenstrauss Lemma (see Lemma 5.3.2 in [33]), this approximation is rather accurate
given a moderate value of q. Therefore, we have

(cid:107)∇f (yt)(cid:107)2 ≈ (cid:0)∇f (yt)(cid:62)pt

(cid:1)2

+

d − 1
q

q
(cid:88)

i=1

(cid:0)∇f (yt)(cid:62)ui

(cid:1)2

and

Dt =

(∇f (yt)(cid:62)pt)2
(cid:107)∇f (yt)(cid:107)2 ≈

(∇f (yt)(cid:62)pt)2 + d−1
q

(cid:0)∇f (yt)(cid:62)pt
(cid:80)q

(cid:1)2

i=1 (∇f (yt)(cid:62)ui)2 .

(157)

(158)

C.3.1 PARS-Est algorithm with theoretical guarantee

In fact, the estimator of Dt concentrates well around the true value of Dt given a moderate value of
q. To reach an algorithm with theoretical guarantee, we could adopt a conservative estimate of Dt,
such that the constraint of θt in Line 3 of Algorithm 2 is satisﬁed with high probability. We show the
prior-guided implementation of Algorithm 2 with estimation of Dt in Algorithm 3, call it PARS-Est,
and show that it admits a theoretical guarantee.

Algorithm 3 Prior-guided ARS with a conservative estimator of Dt (PARS-Est)
Input: L-smooth convex function f ; initialization x0; ˆL ≥ L; Query count per iteration q; iteration number T ;

γ0 > 0.

Output: xT as the approximate minimizer of f .
1: m0 ← x0;
2: for t = 0 to T − 1 do
Obtain the prior pt;
3:
Find a θt such that θt ≤ θ(cid:48)
4:
Step 1: yt ← (1 − αt)xt + αtmt, where αt ≥ 0 is a positive root of the equation α2
5:

t is deﬁned in the following steps:

t in which θ(cid:48)

t = θt(1 − αt)γt;

Step 2: Sample an orthonormal set of {ui}q

i=1 in the subspace perpendicular to pt uniformly, as in

γt+1 ← (1 − αt)γt;

Section B.3.3;

6:

7:

Step 3: ˆDt ←
Resample {ui}q
g1(yt) ← ∇f (yt)(cid:62)vt · vt = (cid:80)q
(cid:80)q
g2(yt) ← d−1
q
xt+1 ← yt − 1
ˆL

8:
9:
10:
11:
12: end for
13: return xT .

(∇f (yt)(cid:62)pt)2
+ 2(d−1)
q

(cid:80)q

(∇f (yt)(cid:62)pt)2
i=1 and calculate vt as in Section B.3.3;

i=1(∇f (yt)(cid:62)ui)2 ; θ(cid:48)

t ←

ˆDt+ q
(cid:16) ˆDt+ d−1

d−1 (1− ˆDt)
q (1− ˆDt)

(cid:17) ;

ˆL

i=1 ∇f (yt)(cid:62)ui · ui + ∇f (yt)(cid:62)pt · pt;

i=1 ∇f (yt)(cid:62)ui · ui + ∇f (yt)(cid:62)pt · pt;
g1(yt), mt+1 ← mt − θt
αt

g2(yt);

Theorem 6. Let

p = Pr

(cid:32) q

(cid:88)

i=1

(cid:33)

x2
i <

q
2(d − 1)

(159)

where (x1, x2, ..., xd−1)(cid:62) ∼ U(Sd−2), i.e. follows a uniform distribution over the unit (d − 1)-
dimensional sphere. Then, in Algorithm 3, for any δ ∈ (0, 1), choosing a q such that p ≤ δ
T , there
exists an event M such that Pr(M ) ≥ 1 − δ, and
(cid:33)2(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)


 ≤ f (x0) − f (x∗) +


(f (xT ) − f (x∗))

(cid:107)x0 − x∗(cid:107)2.

γ0
2

T −1
(cid:88)

(160)

γ0
2

1 +

(cid:112)

M

t=0

θt

√

(cid:32)

E

Proof. We ﬁrst explain the deﬁnition of Ft−1 in the proof (recall that Et[·] is E[·|Ft−1]). Since in
Theorem 5 we require θt to be Ft−1-measurable, we let Ft−1 also includes the randomness in Line 6
of Algorithm 3 in iteration t, besides randomness before iteration t and randomness of pt. We note
that Ft−1 does not include the randomness in Line 8.

32

Let M be the event that: For each t ∈ {0, 1, ..., T − 1}, d−1
q
When M is true, we have that ∀t,

(cid:80)q

i=1

(cid:0)∇f (yt)(cid:62)ui

(cid:1)2

≥ 1

2 (cid:107)∇f (yt)H (cid:107)2.

ˆDt =

≤

=

(cid:0)∇f (yt)(cid:62)pt
(cid:80)q

(cid:1)2
i=1 (∇f (yt)(cid:62)ui)2

(∇f (yt)(cid:62)pt)2 + 2(d−1)
q
(cid:1)2

(cid:0)∇f (yt)(cid:62)pt
(∇f (yt)(cid:62)pt)2 + (cid:107)∇f (yt)H (cid:107)2
(cid:0)∇f (yt)(cid:62)pt
(cid:1)2
(cid:107)∇f (yt)(cid:107)2 = Dt.

(161)

(162)

(163)

Therefore,

θt ≤ θ(cid:48)

t =

ˆDt + q
(cid:16) ˆDt + d−1

d−1 (1 − ˆDt)
q (1 − ˆDt)

ˆL

(cid:17) ≤

Dt + q
(cid:16)
Dt + d−1

d−1 (1 − Dt)
q (1 − Dt)

ˆL

(cid:17) =

(cid:1)2(cid:105)

(cid:104)(cid:0)∇f (yt)(cid:62)vt
Et
ˆL · Et[(cid:107)g2(yt)(cid:107)2]

.

(164)

Since Ft−1 includes the randomness in Line 6 of Algorithm 3 in iteration t, Et[·] refer to only taking
expectation w.r.t. the randomness of vt and g2(yt) in iteration t, i.e. w.r.t. {u1, . . . , uq} in Line 8
of Algorithm 3. Since {u1, . . . , uq} in Line 8 is independent of {u1, . . . , uq} in Line 6, adding
{u1, . . . , uq} in Line 6 to the history does not change the distribution of {u1, . . . , uq} in Line 8 given
the history. Therefore according to the analysis in Section C.2, the last equality of Eq. (164) holds,
and Et[g2(yt)] = ∇f (yt). By Theorem 5, Eq. (160) is proved.
Next we give a lower bound of Pr(M ). Let us ﬁx t. Then

(cid:32)

Pr

d − 1
q

q
(cid:88)

i=1

(cid:0)∇f (yt)(cid:62)ui

(cid:1)2

<

1
2

(cid:33)

(cid:107)∇f (yt)H (cid:107)2

= p.

Since for different t the events inside the brackets are independent, by union bound we have Pr(M ) ≥
1 − pT . Since p ≤ δ

T , the proof is completed.

Remark 13. To save queries, one may think that when constructing vt and g2(yt), we could omit
the procedure of resampling {ui}q
i=1 in Line 8, and reuse {ui}q
i=1 sampled in Line 6 to utilize the
queries of relevant directional derivatives in Line 7. Our theoretical analysis does not support this
yet, as explained below.
If we reuse {ui}q
i=1 sampled in Line 6 to construct vt and g2(yt), then both θt and {g2(yt), vt}
depend on the same set of {ui}q
i=1. Since Theorem 5 requires θt to be Ft−1-measurable, we have
to make Ft−1 include randomness of this set of {ui}q
i=1. Then both g2(yt) and vt are ﬁxed given
the history Ft−1, which is not desired (e.g. Et[g2(yt)] = ∇f (yt) generally does not hold since
Et[g2(yt)] = g2(yt) now, making the proof of Theorem 5 fail).

Remark 14. For given d and q, p can be calculated in closed form with the aid of softwares such as
Mathematica. When d = 2000, if q = 50, then p ≈ 7.5 × 10−4. If q = 100, then p ≈ 3.5 × 10−6.
Hence p is rather small so that a moderate value of q is enough to let p ≤ δ
T .

In fact, p can be bounded by O(exp(−cq)) by Johnson-Lindenstrauss Lemma where c is an absolute
constant (see Lemma 5.3.2 in [33]). Note that the bound is exponentially decayed w.r.t. q and
independent of d.

33

Remark 15. We give an analysis of the inﬂuence of the additional factor 2 in Line 7 of Algorithm 3.
Let

ˆDt2 =

ˆDt1 =

(∇f (yt)(cid:62)pt)2 + 2(d−1)

q

(cid:1)2

(cid:0)∇f (yt)(cid:62)pt
(cid:80)q

i=1 (∇f (yt)(cid:62)ui)2 ,

(∇f (yt)(cid:62)pt)2 + d−1
q

(cid:0)∇f (yt)(cid:62)pt
(cid:80)q

(cid:1)2

i=1 (∇f (yt)(cid:62)ui)2 ,

θt2 =

θt1 =

ˆDt2 + q
(cid:16) ˆDt2 + d−1
ˆDt1 + q
(cid:16) ˆDt1 + d−1

d−1 (1 − ˆDt2)
q (1 − ˆDt2)
d−1 (1 − ˆDt1)
q (1 − ˆDt1)

(cid:17) ,

(cid:17) .

ˆL

ˆL

·

ˆDt1 + d−1
ˆDt2 + d−1

q (1 − ˆDt1)
q (1 − ˆDt2)

Then ˆDt1 ≥ ˆDt2 and 1 − ˆDt1 ≤ 1 − ˆDt2. We have
d−1 (1 − ˆDt2)
d−1 (1 − ˆDt1)
1 − ˆDt1
1 − ˆDt2

θt2
θt1

=

≥

ˆDt2 + q
ˆDt1 + q
ˆDt2
ˆDt1
ˆDt2
1− ˆDt2
ˆDt1
1− ˆDt1

·

=

=

=

(∇f (yt)(cid:62)pt)2
(cid:80)q

i=1(∇f (yt)(cid:62)ui)2

2(d−1)
q

(∇f (yt)(cid:62)pt)2
(cid:80)q

i=1(∇f (yt)(cid:62)ui)2

d−1
q

1
2

.

(165)

(166)

(167)

(168)

(169)

Therefore, we have θt2 ≥ 1
Meanwhile, since ˆDt2 ≥ 0, we have θt2 ≥ q2

2 θt1.

ˆL(d−1)2 . Hence θt2 ≥ max

(cid:110) 1

2 θt1,

q2
ˆL(d−1)2

(cid:111)
.

C.4 Approximate solution of θt and implementation of PARS in practice (PARS-Impl)

We note that in Line 3 of Algorithm 2, it is not straightforward to obtain an ideal solution of θt, since
(cid:104)(∇f (yt)(cid:62)vt)2(cid:105)
ˆL·Et[(cid:107)g2(yt)(cid:107)2]

yt depends on θt. Theoretically speaking, θt > 0 satisfying the inequality θt ≤

Et

ˆL(d−1)2 := θ always holds, so we can always
always exists, since by Eq. (152),
let θt = θ. However, such estimate of θt is too conservative and does not beneﬁt from a good prior
(when Dt is large). Therefore, one can guess a value of Dt, and then compute the value of θt by

Et

(cid:104)(∇f (yt)(cid:62)vt)2(cid:105)
ˆL·Et[(cid:107)g2(yt)(cid:107)2]

≥ q2

Eq. (152), and then estimate the value of Dt and verify that θt ≤
holds. If it does
not hold, we need to try a smaller θt until the inequality is satisﬁed. For example, in Algorithm 3, if
we implement its Line 4 to Line 7 with a guessing procedure,15 we could obtain an runnable algorithm
with convergence guarantee. However, in practice such procedure could require multiple runs from
Line 5 to Line 7 in Algorithm 3, which requires many additional queries; on the other hand, due to
the additional factor 2 in Line 7 of Algorithm 3, we would always ﬁnd a conservative estimate of θt.

Et

(cid:104)(∇f (yt)(cid:62)vt)2(cid:105)
ˆL·Et[(cid:107)g2(yt)(cid:107)2]

15For example, (1) compute θ(cid:48)

t to compute
t by rerunning Line 5 to Line 7, where 0 < κ < 1 is a discount factor to obtain a more conservative

t with θt ← 0 by running Line 5 to Line 7; (2) we guess θt ← κθ(cid:48)

t, then we have found θt as required; else, we go to step (2).

a new θ(cid:48)
estimate of θt; (3) if θt ≤ θ(cid:48)

34

In this section, we introduce the algorithm we use to ﬁnd an approximate solution to ﬁnd θt in Line 3
of Algorithm 2, which does not have theoretical guarantee but empirically performs well. The full
algorithm PARS-Impl is shown in Algorithm 4. It stills follow the PARS framework (Algorithm 2),
and our procedure to ﬁnd θt is shown in Line 5 to Line 7.16 Next we explain the procedure to ﬁnd θt
in detail.

Algorithm 4 Prior-Guided Accelerated Random Search in implementation (PARS-Impl)
Input: L-smooth convex function f ; initialization x0; ˆL ≥ L; Query count per iteration q (cannot be too small);

iteration number T ; γ0 > 0.

Output: xT as the approximate minimizer of f .
1: m0 ← x0;
2: (cid:107) ˆ∇f−1(cid:107)2 ← +∞;
3: for t = 0 to T − 1 do
Obtain the prior pt;
4:
t ← xt; ˆDt ← (∇f (y
y(0)
y(1)
t ← (1 − αt)xt + αtmt, where αt ≥ 0 is a positive root of the equation α2
d−1 (1− ˆDt)
ˆDt ← (∇f (y
q (1− ˆDt)
yt ← (1 − αt)xt + αtmt, where αt ≥ 0 is a positive root of the equation α2

d−1 (1− ˆDt)
q (1− ˆDt)

ˆDt+ q
(cid:16) ˆDt+ d−1

ˆDt+ q
(cid:16) ˆDt+ d−1

(cid:107) ˆ∇ft−1(cid:107)2

(cid:107) ˆ∇ft−1(cid:107)2

; θt ←

; θt ←

)(cid:62)pt)2

)(cid:62)pt)2

(1)
t

(0)
t

(cid:17) ;

(cid:17) ;

8:

5:

7:

6:

ˆL

ˆL

t = θt(1 − αt)γt;

t = θt(1 − αt)γt;

i=1 in the subspace perpendicular to pt uniformly, as in Section B.3.3;

γt+1 ← (1 − αt)γt;

9:
10:
11:

i=1 ∇f (yt)(cid:62)ui · ui + ∇f (yt)(cid:62)pt · pt;

Sample an orthonormal set of {ui}q
g1(yt) ← (cid:80)q
g2(yt) ← d−1
q
(cid:107) ˆ∇ft(cid:107)2 ← (cid:0)∇f (yt)(cid:62)pt
xt+1 ← yt − 1
ˆL

+ d−1
q

(cid:80)q

i=1
g1(yt), mt+1 ← mt − θt
αt

i=1 ∇f (yt)(cid:62)ui · ui + ∇f (yt)(cid:62)pt · pt;
(cid:1)2
(cid:80)q
;

(cid:1)2

(cid:0)∇f (yt)(cid:62)ui
g2(yt);

12:
13:
14: end for
15: return xT .

Speciﬁcally, we try to ﬁnd an approximated solution of θt satisfying the equation θt =
Et

to ﬁnd a θt as large as possible and approximately satisﬁes the inequality

(cid:104)(∇f (yt)(cid:62)vt)2(cid:105)
ˆL·Et[(cid:107)g2(yt)(cid:107)2]

. Since

Et

(cid:104)(∇f (yt)(cid:62)vt)2(cid:105)
ˆL·Et[(cid:107)g2(yt)(cid:107)2]

=

Dt+ q
ˆL(Dt+ d−1

d−1 (1−Dt)

q (1−Dt))

, we try to solve the equa-

(cid:104)(∇f (yt)(cid:62)vt)2(cid:105)
ˆL·Et[(cid:107)g2(yt)(cid:107)2]
Et

θt ≤
tion

θt = g(θt) :=

Dt + q
(cid:16)
Dt + d−1

d−1 (1 − Dt)
q (1 − Dt)

ˆL

(cid:17) ,

(170)

(cid:62)

pt)2 and yt depends on θt. This corresponds to ﬁnding the ﬁxed-point of g,
where Dt = (∇f (yt)
so we apply the ﬁxed-point iteration method. Speciﬁcally, we ﬁrst let θt = 0, then yt = xt, and let
θt ← g(θt) (the above corresponding to Line 5 of Algorithm 4); then we calculate yt again using the
new value of θt (corresponding to Line 6), and let θt ← g(θt) (corresponding to Line 7). We ﬁnd that
two iterations are able to lead to satisfactory performance. Note that then two additional queries to
the directional derivative oracle are required to obtain ∇f (y(0)
)(cid:62)pt used in Line 5
and Line 7.

)(cid:62)pt and ∇f (y(1)

t

t

(cid:62)

pt)2 = (∇f (yt)(cid:62)pt)2
(cid:107)∇f (yt)(cid:107)2
and y(1)
t
)(cid:107)2 and (cid:107)∇f (y(1)

, we need to estimate (cid:107)∇f (yt)(cid:107)2 as introduced in
Since Dt = (∇f (yt)
Section C.3. However, y(0)
in Algorithm 4 are different from both yt and yt−1, so to
estimate (cid:107)∇f (y(0)
)(cid:107)2 as in Section C.3, many additional queries are required
(since the query results of the directional derivative at yt−1 or yt cannot be reused). Therefore, we
introduce one additional approximation: we use the estimate of (cid:107)∇f (yt−1)(cid:107)2 as the approximation
of (cid:107)∇f (y(0)
)(cid:107)2. Since the gradient norm itself is relatively large (compared with

)(cid:107)2 and (cid:107)∇f (y(1)

t

t

t

t

t

16Line 5 and Line 7 require the query of ∇f (y(0)

)(cid:62)pt respectively, so each iteration of
Algorithm 4 requires 2 additional queries to the directional derivative oracle, or requires 4 additional queries to
the function value oracle using ﬁnite difference approximation of the directional derivative.

)(cid:62)pt and ∇f (y(1)

t

t

35

t

t

t

)(cid:107)2 and (cid:107)∇f (y(1)

e.g. directional derivatives) and in zeroth-order optimization, the single-step update is relatively small,
we expect that (cid:107)∇f (y(0)
)(cid:107)2 are closed to (cid:107)∇f (yt−1)(cid:107)2. In Algorithm 4, Line 12
estimates (cid:107)∇f (yt)(cid:107)2 by Eq. (157), and the estimator is denoted (cid:107) ˆ∇ft(cid:107)2. Given this, (cid:107)∇f (y(0)
)(cid:107)2
and (cid:107)∇f (y(1)
)(cid:107)2 are approximated by (cid:107) ˆ∇ft−1(cid:107)2 for calculations of ˆDt in Line 5 and Line 7 as
approximations of (cid:0)∇f (y(0)
(cid:1)2
Finally we note that in the experiments, we ﬁnd that when using Algorithm 4, the error brought by
approximation of (cid:107)∇f (y(0)
)(cid:107)2 sometimes makes the performance of the algorithm
not robust, especially when q is small (e.g. q = 10), which could lead the algorithm to divergence.
Therefore, we propose two tricks to suppress the inﬂuence of approximation error (we note that in
practice, the second trick is more important, while the ﬁrst trick is often not necessary given the
application of the second trick):

)(cid:107)2 and (cid:107)∇f (y(1)

and (cid:0)∇f (y(1)

(cid:62)
)

(cid:62)
)

pt

pt

(cid:1)2

.

t

t

t

t

t

• To reduce the variance of (cid:107) ˆ∇ft(cid:107) when q is small, we let

(cid:107) ˆ∇f avg
t

(cid:107)2 =

1
k

t
(cid:88)

(cid:107) ˆ∇fs(cid:107)2,

s=t−k+1

(171)

t−1(cid:107)2 to replace (cid:107) ˆ∇ft−1(cid:107)2 in Line 5 and Line 7. In our experiments we
and use (cid:107) ˆ∇f avg
choose k = 10. Compared with (cid:107) ˆ∇ft−1(cid:107)2, using (cid:107) ˆ∇f avg
)(cid:107)2 and
(cid:107)∇f (y(1)
)(cid:107)2 could reduce the variance at the cost of increased bias. We note that the
increased bias sometimes brings problems, so one should be careful when applying this
trick.

t−1(cid:107)2 to estimate (cid:107)∇f (y(0)

t

t

• Although Dt ≤ 1, It is possible that ˆDt in Line 5 and Line 7 is larger than 1, which could
lead to a negative θt. Therefore, a clipping of ˆDt is required. In our experiments, we
observe that a ˆDt which is less than but very close to 1 (when caused by the accidental large
approximation error) could also lead to instability of optimization, perhaps because that it
leads to a too large value of θt used to determine yt and to update mt. Therefore, we let
ˆDt ← min{ ˆDt, Bub} in Line 5 and Line 7 before calculating θt, where 0 < Bub ≤ 1 is
ﬁxed. In our experiments we set Bub to 0.6.

We leave a more systematic study of the approximation error as future work.

C.5 Implementation of History-PARS in practice (History-PARS-Impl)

In PARS, when using a speciﬁc prior instead of the prior from a general source, we can utilize some
properties of the prior. When using the historical prior (pt = vt−1), we ﬁnd that Dt is usually similar
to Dt−1, and intuitively it happens when the smoothness of the objective function does not change
quickly along the optimization trajectory. Therefore, the best value of θt should also be similar to the
best value of θt−1. Based on this observation, we can directly use θt−1 as the value of θt in iteration
t, and the value of θt−1 is obtained with yt−1 in iteration t − 1. Following this thread, we present our
implementation of History-PARS, i.e. History-PARS-Impl, in Algorithm 5.

C.6 Full version of Algorithm 2 considering the strong convexity parameter and its

convergence theorem

In fact, the ARS algorithm proposed in [26] requires knowledge of the strong convexity parameter
τ of the objective function, and the original algorithm depends on τ . The ARS algorithm has a
convergence rate for general smooth convex functions, and also have another potentially better
convergence rate if τ > 0. In previous sections, for simplicity, we suppose τ = 0 and illustrate the
corresponding extension in Algorithm 2. In fact, for the general case τ ≥ 0, the original ARS can also
be extended to allow for incorporation of prior information. We present the extension to ARS with
τ ≥ 0 in Algorithm 6. Note that our modiﬁcation is similar to that in Algorithm 2. For Algorithm 6,
we can also provide its convergence guarantee as shown in Theorem 7. Note that after considering
the strong convexity parameter in the algorithm, we have an additional convergence guarantee, i.e.
Eq. (173). In the corresponding PARS algorithm, we have θt ≥ q2
ˆLd2 , so the convergence rate of
PARS is not worse than that of ARS and admits improvement given a good prior.

36

Algorithm 5 History-PARS in implementation (History-PARS-Impl)
Input: L-smooth convex function f ; initialization x0; ˆL ≥ L; Query count per iteration q (cannot be too small);

iteration number T ; γ0 > 0.

Output: xT as the approximate minimizer of f .
1: m0 ← x0;
2: θ−1 ← a very small positive number close to 0;
3: v−1 ∼ U (Sd−1);
4: for t = 0 to T − 1 do
5:

γt+1 ← (1 − αt)γt;

Sample an orthonormal set {ui}q

pt = vt−1;

yt ← (1 − αt)xt + αtmt, where αt ≥ 0 is a positive root of the equation α2

t = θt−1(1 − αt)γt;

i=1 in the subspace perpendicular to vt−1, as in Section B.3.3 with

(cid:80)q

g1(yt) ← (cid:80)q
g2(yt) ← d−1
q
Dt+ q
d−1 (1−Dt)
(cid:16)
Dt+ d−1
ˆL
q (1−Dt)
xt+1 ← yt − 1
ˆL

θt ←

10:
11: end for
12: return xT .

i=1 ∇f (yt)(cid:62)ui · ui + ∇f (yt)(cid:62)vt−1 · vt−1; vt ← g1(yt);

i=1 ∇f (yt)(cid:62)ui · ui + ∇f (yt)(cid:62)vt−1 · vt−1;

(cid:17) , where Dt is estimated using Eq. (158) with pt = vt−1;

g1(yt), mt+1 ← mt − θt−1
αt

g2(yt);

Algorithm 6 Extended accelerated random search framework for τ ≥ 0
Input: L-smooth and τ -strongly convex function f ; initialization x0; ˆL ≥ L; ˆτ such that 0 ≤ ˆτ ≤ τ ; iteration

number T ; a positive γ0 ≥ ˆτ .

Output: xT as the approximate minimizer of f .
1: m0 ← x0;
2: for t = 0 to T − 1 do

Find a θt > 0 such that θt ≤
Step 1: yt ← (1 − βt)xt + βtmt, where βt := αtγt

where θt, yt and g2(yt) are deﬁned in following steps:
γt+αt ˆτ , αt is a positive root of the equation

Et

(cid:104)(∇f (yt)(cid:62)vt)2(cid:105)
ˆL·Et[(cid:107)g2(yt)(cid:107)2]

α2

t = θt((1 − αt)γt + αt ˆτ ); γt+1 ← (1 − αt)γt + αt ˆτ ;

Step 2: Let vt be a random vector s.t. (cid:107)vt(cid:107) = 1; g1(yt) ← ∇f (yt)(cid:62)vt · vt;
Step 3: Let g2(yt) be an unbiased estimator of ∇f (yt), i.e. Et[g2(yt)] = ∇f (yt);
λt ← αt
γt+1
xt+1 ← yt − 1
ˆL

g1(yt), mt+1 ← (1 − λt)mt + λtyt − θt
αt

g2(yt);

ˆτ ;

8:
9: end for
10: return xT .

6:

7:
8:

9:

3:
4:

5:
6:
7:

Theorem 7. In Algorithm 6, if θt is Ft−1-measurable, we have


(f (xT ) − f (x∗))

E

(cid:32)

1 +

√

γ0
2

T −1
(cid:88)

(cid:112)

t=0

(cid:33)2

θt

 ≤ f (x0) − f (x∗) +

γ0
2

(cid:107)x0 − x∗(cid:107)2.

(172)

and

(cid:34)

E

(f (xT ) − f (x∗)) exp

(cid:33)(cid:35)

(cid:32)√

ˆτ

T −1
(cid:88)

t=0

(cid:112)

θt

≤ f (x0) − f (x∗) +

γ0
2

(cid:107)x0 − x∗(cid:107)2.

(173)

Proof. Let Le := ˆL

2 ˆL−L

· ˆL. We still have Eq. (18), so

Et[f (xt+1)] ≤ f (yt) −

≤ f (yt) −

Et

Et

(cid:104)(cid:0)∇f (yt)(cid:62)vt
2Le
(cid:104)(cid:0)∇f (yt)(cid:62)vt
2 ˆL

(cid:1)2(cid:105)

(cid:1)2(cid:105)

.

(174)

(175)

For an arbitrary ﬁxed x, deﬁne ρt(x) := γt
We ﬁrst prove a lemma.

2 (cid:107)mt − x(cid:107)2 + f (xt) − f (x). Let rt := (1 − λt)mt + λtyt.

37

Since (1 − βt)xt + βtmt = yt = (1 − βt)yt + βtyt, we have mt − yt = 1−βt
βt
1 − βt
βt

rt = (1 − λt)mt + λtyt = yt + (1 − λt)(mt − yt) = yt + (1 − λt)

(yt − xt). So

(yt − xt).

(176)

By βt = αtγt
(1 − λt) 1−βt
βt

γt+αt ˆτ , γt+1 = (1 − αt)γt + αt ˆτ and λt = αt
γt+1
. Hence rt = yt + 1−αt
αt

= 1−αt
αt

(yt − xt), which means

ˆτ , after eliminating γt and γt+1, we have

yt = (1 − αt)xt + αtrt.

Now we start the main proof.

ρt+1(x) =

=

γt+1
2
γt+1
2

(cid:107)mt+1 − x(cid:107)2 + f (xt+1) − f (x)

(cid:107)rt − x(cid:107)2 −

γt+1θt
αt

g2(yt)(cid:62)(rt − x) +

γt+1θ2
t
2α2
t

(177)

(178)

(cid:107)g2(yt)(cid:107)2 + f (xt+1) − f (x)

=

γt+1
2

(cid:107)rt − x(cid:107)2 − αtg2(yt)(cid:62)(rt − x) +

θt
2

(cid:107)g2(yt)(cid:107)2 + f (xt+1) − f (x).

We make sure in Remark 2 that Ft−1 always includes all the randomness before iteration t. There-
fore, γt, mt and xt are Ft−1-measurable. The assumption in Theorem 5 requires that θt is
Ft−1-measurable. Since αt, βt, yt and rt are determined by γt, xt, mt and θt (through Borel
functions), they are also Ft−1-measurable. Since θt, αt and rt are Ft−1-measurable, we have
Et[αtg2(yt)(cid:62)(rt − x)] = αtEt[g2(yt)](cid:62)(rt − x) and Et[θt(cid:107)g2(yt)(cid:107)2] = θtEt[(cid:107)g2(yt)(cid:107)2]. Hence

Et[ρt+1(x)] =

γt+1
2

(cid:107)rt − x(cid:107)2 − αtEt[g2(yt)](cid:62)(rt − x) +

=

γt+1
2

(cid:107)rt − x(cid:107)2 − αt∇f (yt)(cid:62)(rt − x) +

θt
2

θt
2

Et[(cid:107)g2(yt)(cid:107)2] + Et[f (xt+1)] − f (x)
(181)

Et[(cid:107)g2(yt)(cid:107)2] + Et[f (xt+1)] − f (x)

(182)

γt+1
2

(cid:107)rt − x(cid:107)2 − αt∇f (yt)(cid:62)(rt − x) +

(cid:1)2(cid:105)

Et

(cid:104)(cid:0)∇f (yt)(cid:62)vt
2 ˆL

+ Et[f (xt+1)] − f (x)

≤

≤

=

=

=

≤

γt+1
2
γt+1
2
γt+1
2
γt+1
2

(cid:107)rt − x(cid:107)2 − αt∇f (yt)(cid:62)(rt − x) + f (yt) − f (x)

(cid:107)rt − x(cid:107)2 − ∇f (yt)(cid:62)(αtrt − αtx) + f (yt) − f (x)

(cid:107)rt − x(cid:107)2 + ∇f (yt)(cid:62)(−yt + (1 − αt)xt + αtx) + f (yt) − f (x)

(cid:107)rt − x(cid:107)2 + αt

(cid:0)f (yt) + ∇f (yt)(cid:62)(x − yt)(cid:1)
+ (1 − αt) (cid:0)f (yt) + ∇f (yt)(cid:62)(xt − yt)(cid:1) − f (x)
γt+1
2

(cid:107)rt − x(cid:107)2 + (1 − αt)f (xt) − (1 − αt)f (x) −

αtτ
2

(cid:107)x − yt(cid:107)2.

We also have

γt+1
2

(cid:107)rt − x(cid:107)2 =

=

≤

=

γt+1
2
γt+1
2

(cid:107)(1 − λt)mt + λtyt − x(cid:107)2

(cid:107)(1 − λt)(mt − x) + λt(yt − x)(cid:107)2

(cid:107)mt − x(cid:107)2 +

(cid:107)yt − x(cid:107)2

γt+1(1 − λt)
2
γt+1(1 − λt)
2

(cid:107)mt − x(cid:107)2 +

= (1 − αt)

γt
2

(cid:107)mt − x(cid:107)2 +

38

γt+1λt
2
αt ˆτ
2
αt ˆτ
(cid:107)x − yt(cid:107)2,
2

(cid:107)yt − x(cid:107)2

(179)

(180)

(183)

(184)

(185)

(186)

(187)

(188)

(189)

(190)

(191)

(192)

(193)

(194)

(195)

(196)

(197)

(cid:21)

(198)

(199)

(200)

(201)

where the inequality is due to Jensen’s inequality applied to the convex function (cid:107) · (cid:107)2, and the
third equality is obtained after substituting λtγt+1 = αt ˆτ by the deﬁnition of λt. Since γt+1 =
(1 − αt)γt + αt ˆτ = (1 − αt)γt + λtγt+1, we have γt+1(1 − λt) = (1 − αt)γt, which leads to the
last equality.

Hence

Et[ρt+1(x)] ≤

γt+1
2

(cid:107)rt − x(cid:107)2 + (1 − αt)f (xt) − (1 − αt)f (x) −

αtτ
2

(cid:107)x − yt(cid:107)2

= (1 − αt)ρt(x) +

≤ (1 − αt)ρt(x).

αt(ˆτ − τ )
2

(cid:107)x − yt(cid:107)2

Therefore,

ρ0(x) = E[ρ0(x)] ≥ E

(cid:21)

= E

(cid:20)
E0

(cid:20) E0[ρ1(x)]
1 − α0
(cid:21)

E1[ρ2(x)]
(1 − α0)(1 − α1)

(cid:20)

(cid:20)
E1

= E

(cid:21)

= E

(cid:21)(cid:21)

(cid:20) ρ1(x)
1 − α0
ρ2(x)
(1 − α0)(1 − α1)

(cid:20) ρ1(x)
1 − α0
(cid:20)

(cid:21)(cid:21)

= E

ρ2(x)
(1 − α0)(1 − α1)

(cid:20)

≥ E

≥ . . .
(cid:34)

≥ E

ρT (x)
t=0 (1 − αt)

(cid:81)T −1

(cid:35)

.

We have ρT (x) ≥ f (xT ) − f (x). To prove the theorem, let x = x∗. The remaining is to give an
upper bound of (cid:81)T −1

t=0 (1 − αt). Let ψk := (cid:81)k−1

t=0 (1 − αt) and ak := 1√
ψk

, we have

ak+1 − ak =

1
(cid:112)ψk+1

−

1
√
ψk

=

√

ψk − (cid:112)ψk+1
(cid:112)ψkψk+1

=

ψk − ψk+1

√

(cid:112)ψkψk+1(

ψk + (cid:112)ψk+1)

≥

=

≥

ψk − ψk+1
√

(cid:112)ψkψk+1(
ψk − (1 − αk)ψk

ψk)

(cid:112)ψk+1

√

2ψk
γ0θk
2

.

=

αk
2(cid:112)ψk+1

=

(cid:112)γk+1θk
2(cid:112)ψk+1

=

√

θk
2

(cid:114) γk+1
ψk+1

The last step is because γt+1 ≥ (1 − αt)γt, so γk+1
γ0
√
θt. Therefore,
a0 = 1. Hence aT ≥ 1 +
1

(cid:80)T −1
t=0

γ0
2

√

≥ (cid:81)k

t=0(1 − αt) = ψk+1. Since ψ0 = 1,

ψT ≤

(cid:16)

1 +

√

γ0
2

(cid:80)T −1
t=0

√

θt

(cid:17)2 .

(202)

Meanwhile, since γ0 ≥ ˆτ and γt+1 = (1 − αt)γt + αt ˆτ , we have that ∀t, γt ≥ ˆτ . Then α2
θt((1 − αt)γt + αt ˆτ ) ≥ θt ˆτ , then we have that αt ≥

√

t =

ψT ≤

T −1
(cid:89)

t=0

(cid:16)

1 −

(cid:17)

(cid:112)

ˆτ θt

ˆτ θt. Therefore,
(cid:32)

T −1
(cid:88)

(cid:112)

√

≤ exp

−

ˆτ

t=0

(cid:33)

.

(203)

θt

The proof is completed.

D Supplemental materials for Section 5

D.1 More experimental settings in Section 5.1

In experiments in this section, we set the step size µ used in ﬁnite differences (Eq. (1)) to 10−6, and
the parameter γ0 in ARS-based methods to ˆL.

39

D.1.1 Experimental settings for Fig. 1

Prior We adopt the setting in Section 4.1 of [20] to mimic the case that the prior is a biased version
of the true gradient. Speciﬁcally, we let pt = ∇f (xt) + (b + nt), where b is a ﬁxed vector and nt is
a random vector uniformly sampled each iteration, (cid:107)b(cid:107) = 1 and (cid:107)nt(cid:107) = 1.5.

Test functions Our test functions are as follows. We choose f1 as the “worst-case smooth convex
function” used to construct the lower bound complexity of ﬁrst-order optimization, as used in [26]:

f1(x) =

1
2

(x(1))2 +

1
2

d−1
(cid:88)

i=1

(x(i+1) − x(i))2 +

1
2

(x(d))2 − x(1), where x0 = 0.

(204)

We choose f2 as a simple smooth and strongly convex function with a worst-case initialization:

f2(x) =

(cid:19)

· (x(i))2

d
(cid:88)

i=1

(cid:18) i
d

, where x(1)

0 = d, x(i)

0 = 0 for i ≥ 2.

(205)

We choose f3 as the Rosenbrock function (f8 in [13]) which is a well-known non-convex function
used to test the performance of optimization problems:
(cid:18)
(x(i))2 − x(i+1)(cid:17)2

+ (x(i) − 1)2

, where x0 = 0.

f3(x) =

(206)

d−1
(cid:88)

100

(cid:19)

(cid:16)

i=1

We note that ARS, PARS-Naive and PARS could depend on a strong convexity parameter (see
Section C.6) when applied to a strongly convex function. Therefore, for f2 we set this parameter to
the ground truth value. For f1 and f3 we set it to zero, i.e. we use Algorithm 2.

D.1.2 Experimental settings for Fig. 2

In this part we set d = 500 and set q such that each iteration of each algorithm costs 11 queries. Since
when using the historical prior, we aim to build algorithms agnostic to parameters of the objective
function, we set the strong convexity parameter in ARS-based methods to 0 even though we know
that e.g. f2 is strongly convex. Correspondingly, we adopt adaptive restart of function scheme [27]
to reach the ideal performance. We introduce our implementation here. In each iteration (suppose
that currently it is iteration t) of Algorithm 5, we check whether f (yt) ≤ f (yt−1). If not, we set
mt+1 ← xt+1 and γt+1 ← γ0 as the restart.

D.2 More experimental settings in Section 5.2

We perform targeted attacks under the (cid:96)2 norm with the perturbation bound set to 3.514 (= 32/255 ×
√
784) if each pixel value has the range [0, 1]. The objective function to maximize for attacking
image x is the C&W loss [7], i.e. f (x) = Z(x)t − maxi(cid:54)=t Z(x)i, where t is the target class
and Z(x) is the logits given the input x. The network architecture is from the PyTorch example
(https://github.com/pytorch/examples/tree/master/mnist).
We set the step size µ used in ﬁnite differences (Eq. (1)) to 10−4, and the parameter γ0 in ARS-based
methods to ˆL. To deal with the constraints in optimization, in each iteration we perform projection
after the update to ensure that the constraints are satisﬁed. In historical-prior-guided methods, to
prevent the prior from pointing to the infeasible region (where the constraints are not satisﬁed), we
let the prior pt be xt − xt−1 for History-PRGF and xt − yt−1 for History-PARS. In unconstrained
optimization, this is equivalent to the original choice of pt (pt = gt−1 for History-PRGF and
pt = g1(yt−1) for History-PARS) up to sign. But in constrained optimization, since xt is further
projected to the feasible region after the update from xt−1 or yt−1, they are not equivalent.

We note that the number of queries for each image does not count queries (one query per iteration) to
check whether the attack has succeeded.

E Potential negative societal impacts

As a theoretical work, we think this paper can provide valuable insights on understanding existing
algorithms and may inspire new algorithms for zeroth-order optimization, while having no signiﬁcant

40

potential negative societal impacts. One may pay attention to its application to query-based black-box
adversarial attacks.

41

