Improved uncertainty quantiﬁcation for Gaussian process regression based
interatomic potentials

Albert P. Bart´ok1, 2 and James R. Kermode2
1)Department of Physics, University of Warwick, Coventry, CV4 7AL, United Kingdom
2)Warwick Centre for Predictive Modelling, School of Engineering, University of Warwick, Coventry, CV4 7AL,
United Kingdom

(Dated: 20 June 2022)

The error estimation capability of machine learning interatomic potentials (MLIPs) based on probabilistic
learning methods such as Gaussian process regression (GPR) is currently under-exploited, because of the
tendancy of the predicted errors to overestimate the true error. We present approaches based on maximising
either the marginal likelihood or an alternative likelihood constructed using leave-one-out cross validation to
provide improved error estimates for interatomic potentials based on GPR. We benchmarked these approaches
on models representing the Ar trimer, showing signiﬁcant improvements in the robustness of the predicted
error estimates.

I.

INTRODUCTION

Multiscale materials modelling has been successful in
many domains but it has not fully delivered on its
promise, in large part because coupling of scales remains
ad hoc, lacking the robust propagation of uncertainties
across scales that is essential for reliable validated mod-
els. Most current research applies interatomic poten-
tial methods, despite severe limitations in accuracy and
transferability. To date, there has been limited system-
atic analysis of the variation in simulation results with
changes in the choice or parameters of the interatomic
potential, mostly using “sloppy models” consisting of en-
sembles of potentials.1–3 These approaches are typically
too costly to allow robust error bars to be placed on the
outputs of complex simulations. Recently, alternative
approaches based on calibrating “committees” of mod-
els, for example trained on subsets of the overall train-
ing data4,5 have seen some success, but so far lack the
rigorous statistical underpinning needed for robust error
propagation to derived quantities. The related “dropout”
artiﬁcial neural network (ANN) approach has a Bayesian
interpretation.6

The more challenging task of quantifying model form
error requires sampling over ensembles of “reasonable”
interatomic potentials that are not constrained by a pre-
deﬁned functional form, allowing them to better reﬂect
the complexity of the underlying quantum mechanics.
This is closely aligned to the broad body of recent lit-
erature that reports eﬀorts to construct machine learn-
ing interatomic potential (MLIP)s that interpolate from
a database of reference ab initio calculations, e.g. widely
used artiﬁcial neural network7 and Gaussian approximate
potentials (GAP)8 approaches, the spectral neigbhbour
anlaysis potential (SNAP) method9 and related Linear
Machine Learning (LML)10 approach, as well asthe re-
cently proposed atomic cluster expansion (ACE)11 frame-
work. Data-driven models avoid many of the pitfalls of
a ﬁxed functional form, but the errors induced by overly
ﬂexible models have not yet been systematically quan-
tiﬁed. While MLIPs based on Gaussian process (GP)

regression are in principle accompanied by posterior vari-
ance estimates, this has been underexploited because of
the tendancy of GP predicted errors to overestimate the
true undercertainty, combined with the high computa-
tional cost and complexity of optimising or sampling hy-
perparameters. For example, a recent GP-based poten-
tial for refractory high entropy alloys12 was limited to
two- and three-body terms only, and then also had to
be remapped from GPs to splines for computational eﬃ-
ciency, removing the UQ capabilities (although a recent
preprint addresses this by also mapping the error bars13).
While MLIPs are already extremely successful, for ex-
ample, they have been applied to complex alloys with
promising results,12,14 their uncertainty quantiﬁcation
(UQ) capabilities have severe limitations at present, with
validation often being conﬁned to test/train split rather
than computing realistic quantities of interest (QoIs).
Here, we present a step towards overcoming these lim-
itations building on prior work from a number of re-
search groups that demonstrates that MLIPs can be in-
vested with UQ capabilities.4,5,7,8,15,16. We use a dataset
consisting of Ar dimers and timers computed at the
CCSD(T) level to demonstrate the principles of our
newly proposed approach.

II. METHODOLOGY

A. Data generation

We have generated a data set of 41 Ar dimers at in-
teratomic distances uniformly spaced between 3 and 7 ˚A.
In addition, a set of 1880 Ar trimers were generated by
varying the bond distances between 3 and 7 ˚A and the
bond angle between 0◦-180◦. We evaluated the ab initio
energies with the Gaussian16 software package17 at the
coupled cluster [CCSD(T)] level using the aug-cc-pVQZ
basis set18. This ensures that dispersion interactions,
which are largely responsible for attraction between the
Ar atoms, are accurately modelled, including their many-
body character. Consequently, the target interaction en-

2
2
0
2

n
u
J

7
1

]
i
c
s
-
l
r
t

m

.
t
a
m
-
d
n
o
c
[

1
v
4
4
7
8
0
.

6
0
2
2
:
v
i
X
r
a

 
 
 
 
 
 
ergy of an Ar trimer conﬁguration is truly a function of
all three atomic positions, and while the interaction is
dominated by pair interactions, the total energy cannot
be exactly decomposed into two-body terms.

B. Gaussian procress regression

We can model any multivariate function f (x) using a
GP. This allows us to deﬁne a prior over functions that is
equivalent to a multivariate normal distribution for any
ﬁnite realisation of the function on a grid of N∗ points
X∗ = [x1, x2, . . . , xN∗ ] so that f = [f (xi), i = 1 . . . N∗],
i.e.

f ∼ GP(m(x), k(x, x(cid:48))) =⇒ f ∼ N (m(x), K)

(1)

where m(x) is a speciﬁed mean function (often taken to
be zero), k(x, x(cid:48)) deﬁnes a kernel for the covariance be-
tween function values at input points x and x(cid:48) and K is
the resulting covariance matrix for all pairs of inputs, i.e.
Kij = k(xi, xj) for i, j = 1 . . . N .

Consider a set of N noisy observations y =
[y1, y2, . . . yN ] at inputs X = [x1, x2, . . . xN ] which are
assumed to follow the model f (x) on average but con-
taminated with additive Gaussian noise

yi = f (xi) + (cid:15) where (cid:15) ∼ N (0, σ2
n)

(2)

where σn is a hyperparameter representing the noise
level. Assuming additive independence, the covariance
matrix for the observations is

cov[y] = Ky = K(X, X) + σ2
nI

(3)

Conditioning the prior GP (1) on the data leads to
a new GP representing our updated knowledge of the
function, referred to as the posterior GP. The posterior
GP can be used to compute the predictive mean and
covariance at input points of interest X ∗ analytically19,
with the results given by

¯f (X ∗) = K(X∗, X)K −1
y y
cov[f ∗] = K(X∗, X∗) − K(X∗, X)K −1

y K(X, X∗)

(4)

(5)

C. Gaussian approximation potentials

The GAP framework has been developed for the spe-
ciﬁc case of modelling an ab initio potential energy sur-
face (PES) using Gaussian process regression (GPR)8,20.
In this framework, the GP inputs x are computed using
descriptors that are invariant to translation, permuta-
tion and rotational symmetries21, and the observations
are the ab initio energies, and often also their derivatives
in the form of forces and stresses. A complication arises
here since we need to infer the decomposition of the total
energy onto individual atoms (since local energies cannot
be directly predicted with ab initio methods)

2

Consider ﬁrst a two-body approximation to the total

energy:

E(r) =

(cid:88)

i<j

V2(rij)

(6)

where V2(r) ∼ GP(0, k2(r, r(cid:48))) and rij = |rj − ri| is the
distance between atoms i and j. Since we only have ob-
servations of the total energy, the target data is the sum
of Gaussian Process models. The covariance between the
total energies of two conﬁgurations A and B is simply
the sum of the covariance functions for each bond

(cid:104)EAEB(cid:105) =

(cid:88)

k2(rij, ri(cid:48)j(cid:48))

(7)

ij∈A,i(cid:48)j(cid:48)∈B

In the more general case the required covariances can
be computed by introducing a linear operator ˆL which
maps from the observations y we have to the local atomic
energies y(cid:48) through the relation y = ˆLT y(cid:48). We refer the
reader to Ref. 20 for a complete discussion of GPR in
the context of interatomic potentials, which for realistic
potentials also requires the use of a sparse GP to reduce
the O(N 3) cost of training; this step neglected here for
simplicity.

In this work, we use a two-body potential which is
a combination of explicit basis functions and a non-
parameteric term, and a fully non-parameteric three-
body potential. The total energy is of the form

E(r) = E0 +

(cid:88)

i<j

V2(rij) +

(cid:88)

i<j<k

V3(rij, rik, rjk)

(8)

where E0 is the average energy per atom across the train-
ing set. We note care must be taken to enforce permuta-
tion symmetry for the three-body term, either by sym-
metrising the descriptor or by summing the term over the
permutation group of three particles; here, we take the
former approach.

The elements of the symmetrised three-body descriptor
d = [d1, d2, d3] are deﬁned as d1 = rij + rik, d2 = (rij −
rik)2 and d3 = rjk, which ensures invariance to rigid
rotations and translations as well as swapping the indices
of j and k, while providing a bijective mapping between
atomic coordinates and the descriptor elements.

D. Explicit basis functions

By analogy with the Kennedy–O’Hagan approach,22
simple models that encode known physics such as short-
range repulsion can be used as a baseline; this has been
explored already in GAP models in deterministic settings
(e.g. to add van der Waals corrections23) but the contri-
butions made by uncertainty in the basis functions have
not previously been included in predictions.
For the two- and three-body GPs, we use

V2(r) = f2(r) + hT (r)β
f2(r) ∼ GP (0, k(r, r(cid:48)))
V3(r) ∼ GP (0, k3(r, r(cid:48)))

(9)
(10)
(11)

This corresponds to the usual zero-mean GP for the
three-body term, but a non-zero mean for the two-body
term which is made up of a linear combination of basis
functions h(r) = [h1(r), h2(r), . . .] with weights β.

Following O’Hagan 24, if we adopt a Gaussian prior on
the weights β, so that β ∼ N (b, B) we can integrate
out the weights rather than optimising them. This yields
another GP for V2(r)

V2(r) ∼ GP (cid:0)h(r)T b, k2(r, r(cid:48)) + h(r)T Bx(r(cid:48))(cid:1)

(12)

Note that, while the mean of the new GP is simply equal
to the product of the mean coeﬃcients and the ﬁxed basis
functions, there is also a contribution to the covariance
caused by the uncertainty in the coeﬃcients. Taking the
limit of an uninformative prior B−1 → 0 gives results for
the predictive mean and covariance which extend (4) and
(5) to

¯V2(X∗) = ¯f2(X∗) + RT ¯β
2] = cov[f ∗
cov[V∗

2 ] + RT (HK −1

y H T )−1R)

(13)

(14)

where the design matrix H = [h(r1), h(r2), . . .] collects
the basis vectors for all training cases, and H ∗ the same
for all test cases, R = H∗ − HK −1
y K∗ and the limiting
¯β = (HK −1
y H T )−1K −1

y y.

E. Basis functions and covariance kernels

It remains to specify the basis functions and covariance
kernels selected in this study. Since atomic interactions
in Ar are largely dominated by van der Waals dispersion,
which is attractive, and repulsion due to the Pauli exclu-
sion principle, we chose basis functions taken from the
Lennard-Jones potential, i.e.

h1(r) =

h2(r) =

1
r12
1
r6

(15)

(16)

For both terms we chose squared-exponential (SE) co-
variance kernels of the forms

k2(r, r(cid:48)) = δ2

2 exp

k3(d, d(cid:48)) = δ2

3 exp

(cid:21)

(cid:20) |r − r(cid:48)|2
(cid:96)2

(cid:35)

(cid:34) 3

(cid:88)

i=1

i|2

|di − d(cid:48)
((cid:96)i
3)2

(17)

(18)

with the inputs r, r and lengthscale hyperparameters
θ2, θ3 are scalars and three-vectors for the two-body and
three-body case, respectively, and the signal variances
δ2 and δ3 are both scalars, giving a total of 7 hyperpa-
rameters including the likelihood noise σn in (3), which
we collect into a vector θ = [δ2, (cid:96)2, δ3, (cid:96)1
3, σn]. The
cutoﬀ distances for the two- and three-body terms were
ﬁxed at 7 ˚A and 5 ˚A, respectively.

3, (cid:96)2

3, (cid:96)3

3

F. Hyperparameter optimisation

For the Bayesian interpretation of a kernel model to be
meaningful, attention must be paid to the choice of hy-
perparameters θ. Up until now these have often been set
heuristically by MLIP practitioners for reasons of sim-
plicity and computational expediency. In this work, ker-
nel hyperparameters are optimised by maximising either
the marginal likelihood (ML), which has previously been
reported to improve predicted variances for GP-based
interatomic potentials.15,16, and the leave-one-out cross
validation (LOO-CV) likelihood, which has the advan-
tage that it does not depend on the validity of modelling
assumptions.

The marginal likelihood is the probability of the ob-
served data given the inputs, and can be computed by
integrating the likelihood multiplied by the prior (i.e.
marginalising over the function values f )), leading to

(cid:90)

p(y|X, θ) =

p(y|f , X, θ)p(f |X, θ)df

(19)

For a zero mean GP the prior and likelihood are both
Gaussian, so integral is analytic and yields

1
2

1
2

N
2

log p(y|X, θ) = −

yT K −1

y y −

log |Ky| −

log 2π,

(20)
which is simply the PDF of the observations y ∼
N (0, Ky). The three terms can be interpreted as a data
ﬁt term, a complexity penalty and a normalisation con-
stant, respectively. For the explicit basis in the limit of
a non-informative prior on the basis coeﬃcients β this
result becomes

log p(y|X, θ) = −

1
2

−

1
2
where A = HK −1
the rank of H T .19

yT K −1

y y +

yT Cy −

1
2

log |A| (21)

log |Ky| −

log 2π

1
2
N − M
2

y H T , C = K −1

y H T A−1HK −1

y

and M is

The LOO-CV approach is based on the idea of leaving
out each piece of training data in turn. The predictive
log probability of the data when leaving out training case
i is

log p(yi|X, y−i, θ) = −

1
2

log σ2

i −

(yi − µi)2
2σ2
i

−

1
2

log 2π

(22)
where y−i means all observations except for number i,
and the predictive mean µi and variance σ2
i are given
by (13) and (14) using reduced training sets {X−i, y−i}.
From this result we can write down the LOO log predic-
tive probability as

LLOO(X, y, θ) =

N
(cid:88)

i=1

log p(yi|X, y−i, θ)

(23)

This can be computed eﬃciently by noting that we re-
quire a series of inverses of the full Ky, each with a row

and column deleted25 which leads to simpliﬁed expres-
sions for the LOO-CV predictive mean and variance of
the form

4

y ]ii

y y]i/[K −1

µi = yi − [K −1
i = 1/[K −1
σ2
y ]ii
Contributions from the explicit basis functions can be
added in an analogous way. The LOO-CV likelihood can
thus be computed with a computational cost that is dom-
inated by a single inversion of Ky.

(24)

(25)

III. RESULTS

We used the CCSD(T) Ar dimer and trimer data to
explore alternative strategies for choosing GP hyperpa-
rameters. To simulate the low-data limit, we trained our
models on a ﬁxed 1% subset of the data (19 structures),
and retained the other 99% (1902 structures) for testing.
One dimensional sweeps for the three hyperparame-
ters for the two-body kernel are shown in Fig. 1, along
with the decomposition of the marginal likelihood into
the terms in (21): data ﬁt (ﬁrst term), the contribution
from the explicit basis set (second and third terms) and
the complexity penalty (fourth term). The plot shows
some diﬀerences in the maxima identiﬁed by the marginal
likelihood and the LOO-CV likelihood, with the latter
being more strongly peaked and for example preferring a
smaller lengthscale and lower noise variance.

We next chose the hyperparameters using three ap-
proaches: (i) adopting the physically informed heuristic
choices of hyperparameters commonly used when ﬁtting
GAP models; (ii) maximising the marginal likelihood of
the training data; (iii) maximising the LOO-CV likeli-
hood of the training data.
In case (i) we ﬁrst ﬁtted
a Lennard-Jones model to the training data and sub-
tracted this from the CCSD(T) data before ﬁtting the
GAP model while in cases (ii) and (iii) we used the ex-
plicit basis function approach described above to ﬁt the
parameteric and non-parametric terms together. Likeli-
hoods were maximised using the BFGS algorithm, with
derivatives with respect to hyperparameters computed
using automatic diﬀerentiation26. Multiple restarts with
random initial choices for the hyperparameters were used
to mitigate local maxima. We found that the best results
are obtained with a single basis function h1(r) to account
for short-range repulsion, without the long-range disper-
sion term h2(r). Table I summarises the hyperparameters
obtained with each approach.

The results shown in Fig. 2 demonstrate that the
ML approach has the highest accuracy on the train-
ing conﬁgurations, but the LOO-CV approach has the
most reliable predicted errors. The GAP heuristics lead
to predicted errors which are often signiﬁcantly over-
estimate the true error (in accordance with previous
observations20). The ML predicted errors do have good
correlation with true error (despite the overall high ac-
curacy). The decomposition of the two-body energies is

FIG. 1. Comparison of the ML and LOO-CV likelihood as a
function of the three hyperparameter in the two-body kernel
k2(r, r(cid:48)); from top to bottom, (cid:96)2, δ2, σn. The decomposition
into the terms of (21) is also shown.

shown in Fig. 3. The explicit basis set makes a large
contribution to the total uncertainty at distances beyond
the cutoﬀ of the non-parametric term; it is notable that
this contribution is larger in the LOO-CV case.

The diﬀerences in performance can be rationalised by
comparing the δ2 and δ3 values in Table 1. Recall that
with a SE kernel the posterior predicted error, evaluated
at a test point that is far from training data, tends to δ.

5

FIG. 2. GP models trained on quantum chemistry CCSD(T) calculations for Ar dimers and trimers, trained on 1% of the data
(orange) and tested on remaining 99% (blue). Three realisations of the model, with hyperparameters chosen using heuristics
as in GAP framework (left), optimised to maximise the marginal likelihood (ML, middle) or the LOO-CV (right). Note
similar accuracy for the mean, but far superior predicted errors with LOO-CV, with GAP heuristics over-estimating and ML
under-estimating true errors.

GAP Opt. ML Opt. LOO-CV

Two-body
δ2 / eV
(cid:96)2 / ˚A
Three-body
δ3 / eV
(cid:96)1 / ˚A
(cid:96)2 / ˚A2
(cid:96)3 / ˚A
σn / eV

0.1
0.5

0.017
1.01

0.0005
1054
499
917

0.05
3.96
3.99
6.50
0.001 0.00077

RMSE / meV/atom 2.1

1.7

1429
2.18

0.0006
239
329
344
0.00077
2.4

TABLE I. Hyperparameter values obtained with each of three
approaches: GAP heuristics, optimising the marginal likeli-
hood (ML) and optimising the LOO-CV likelihood. The ﬁnal
row in the table shows the RMSE of the resulting ﬁts.

This is due to the variance in (5) dominated by the ﬁrst
term, as the second term tends to zero for very dissimilar
pairs of conﬁgurations. The original GAP heuristics as-
sign relatively similar weights to the two- and three-body

terms, while both optimised approaches put more weight
on the two-body term, with the LOO-CV approach lead-
ing to an even greater two-body weighting. There are
also signiﬁcant diﬀerences in the lengthscale parameters,
with LOO-CV leading to a more even split between the
three components of the three-body descriptor than ML.

IV. CONCLUSIONS

Our results demonstrate that maximising the LOO-CV
likelihood signiﬁcantly improves the accuracy of the pre-
dicted errors over the marginal likelihood. To understand
why this might be, recall that the marginal likelihood
tells us the probability of the observations, given the as-
sumptions of the model, while the LOO-CV likelihood
gives an estimate for the predictive probability, whether
or not the assumptions of the model are true, helping to
make it more robust against model limitations27.

Further work is required to extend this proof-of-
principle to more complex manybody descriptors such

0204060CCSD(T) energy / meV/atom0204060Predicted energy / meV/atomGAP (RMSE 2.1 meV/atom)TestTrain0204060CCSD(T) energy / meV/atom0204060ML (RMSE 1.7 meV/atom)0204060CCSD(T) energy / meV/atom0204060LOO-CV (RMSE 2.4 meV/atom)0246810True error / meV/atom0246810Predicted error / meV/atom0246810True error / meV/atom02468100246810True error / meV/atom02468106

search Technology Platform of the University of War-
wick, STFC Scientiﬁc Computing Department’s SCARF
cluster, the EPSRC-funded HPC Midlands+ consortium
(EP/P020232/1, EP/T022108/1) and on ARCHER2
(https://www.archer2.ac.uk/) via the UK Car-Parinello
consortium (EP/P022065/1). For the purpose of Open
Access, the author has applied a CC-BY public copyright
licence to any Author Accepted Manuscript (AAM) ver-
sion arising from this submission.

1S. L. Frederiksen, K. W. Jacobsen, K. S. Brown, and J. P. Sethna,
“Bayesian ensemble approach to error estimation of interatomic
potentials,” Phys. Rev. Lett. 93, 165501 (2004).
2A. Mishra, S. Hong, P. Rajak, C. Sheng, K.-I. Nomura, R. K.
Kalia, A. Nakano, and P. Vashishta, “Multiobjective genetic
training and uncertainty quantiﬁcation of reactive force ﬁelds,”
npj Comp. Mater. 4, 42 (2018).
3K. Tran, W. Neiswanger, J. Yoon, Q. Zhang, E. Xing, and Z. W.
Ulissi, “Methods for comparing uncertainty quantiﬁcations for
material property predictions,” Mach. Learn.: Sci. Technol. 1,
025006 (2020).
4F. Musil, M. J. Willatt, M. A. Langovoy, and M. Ceriotti, “Fast
and accurate uncertainty estimation in chemical machine learn-
ing,” J. Chem. Theory Comput. 15, 906–915 (2019).
5G. Imbalzano, Y. Zhuang, V. Kapil, K. Rossi, E. A. Engel,
F. Grasselli, and M. Ceriotti, “Uncertainty estimation for molecu-
lar dynamics and sampling,” J. Chem. Phys. 154, 074102 (2021).
6M. Wen and E. B. Tadmor, “Uncertainty quantiﬁcation in molec-
ular simulations with dropout neural network potentials,” npj
Comp. Mater. 6, 1–10 (2020).
7J. Behler and M. Parrinello, “Generalized Neural-Network repre-
sentation of High-Dimensional Potential-Energy surfaces,” Phys.
Rev. Lett. 98, 146401 (2007).
8A. P. Bart´ok, M. C. Payne, R. Kondor, and G. Cs´anyi, “Gaussian
approximation potentials: the accuracy of quantum mechanics,
without the electrons,” Phys. Rev. Lett. 104, 136403 (2010).
9A. P. Thompson, L. P. Swiler, C. R. Trott, S. M. Foiles, and G. J.
Tucker, “Spectral neighbor analysis method for automated gen-
eration of quantum-accurate interatomic potentials,” J. Comput.
Phys. 285, 316–330 (2015).

10A. M. Goryaeva, J. D´er`es, C. Lapointe, P. Grigorev, T. D.
Swinburne, J. R. Kermode, L. Ventelon, J. Baima, and M.-C.
Marinica, “Eﬃcient and transferable machine learning potentials
for the simulation of crystal defects in bcc fe and w,” Phys. Rev.
Materials 5, 103803 (2021).

11R. Drautz, “Atomic cluster expansion for accurate and transfer-
able interatomic potentials,” Phys. Rev. B 99, 014104 (2019).
12J. Byggm¨astar, K. Nordlund, and F. Djurabekova, “Modeling re-
fractory high-entropy alloys with eﬃcient machine-learned inter-
atomic potentials: Defects and segregation,” Phys. Rev. B 104,
104101 (2021).

13Y. Xie, J. Vandermause, S. Ramakers, N. H. Protik, A. Johans-
son, and B. Kozinsky, “Uncertainty-aware molecular dynamics
from bayesian active learning: Phase transformations and ther-
mal transport in SiC,” (2022), arXiv:2203.03824 [physics.comp-
ph].

14C. W. Rosenbrock and et al., “Machine-learned interatomic po-
tentials for alloys and alloy phase diagrams,” npj Comp. Mater.
7, 1–9 (2021).

15J. Vandermause, S. B. Torrisi, S. Batzner, Y. Xie, L. Sun, A. M.
Kolpak, and B. Kozinsky, “On-the-ﬂy active learning of inter-
pretable bayesian force ﬁelds for atomistic rare events,” npj
Comp. Mater. 6, 1–11 (2020).

16J. Vandermause, Y. Xie, J. S. Lim, C. J. Owen, and B. Kozin-
sky, “Active learning of reactive bayesian force ﬁelds: Applica-
tion to heterogeneous hydrogen-platinum catalysis dynamics,”
ArXiv:2106.01949 (2021), arXiv:2106.01949 [cond-mat.mtrl-sci].
17M. J. Frisch, G. W. Trucks, H. B. Schlegel, G. E. Scuseria, M. A.
Robb, J. R. Cheeseman, G. Scalmani, V. Barone, G. A. Peters-

FIG. 3. Mean prediction and 95% conﬁdence intervals for
the 2-body component of the ML (above) and LOO-CV (be-
low) models. Note large rise in predicted uncertainty beyond
cutoﬀ of 7 ˚A. Contributions from explicit polynomial basis
and non-parametric regression are shown in red and green,
respectively, with total prediction in blue.

as Smooth Overlap of Atomic Positions (SOAP)21 or
ACE11, and to the sparse GP case where, in principle,
the set of inducing (sparse) points could also be opti-
mised togeher with the hyperparameters. While the ex-
amples presented here used small training sets, LOO-CV
approaches can be scaled to large datasets using Pareto-
smoothed importance sampling,28 giving scope to incor-
porate these ideas in production calculations.

ACKNOWLEDGMENTS

This work was ﬁnancially supported by a Leverhulme
Trust Research Project Grant (RPG-2017-191), the En-
gineering and Physical Science Research Council (EP-
SRC) under grant EP/R043612/1, the CASTEP-USER
project funded by UK Research and Innovation un-
der the grant agreement EP/W030438/1, and the NO-
MAD Centre of Excellence (European Commission grant
agreement ID 951786). We acknowledge computational
resources provided by the Scientiﬁc Computing Re-

7

son, H. Nakatsuji, X. Li, M. Caricato, A. V. Marenich, J. Bloino,
B. G. Janesko, R. Gomperts, B. Mennucci, H. P. Hratchian,
J. V. Ortiz, A. F. Izmaylov, J. L. Sonnenberg, D. Williams-
Young, F. Ding, F. Lipparini, F. Egidi, J. Goings, B. Peng,
A. Petrone, T. Henderson, D. Ranasinghe, V. G. Zakrzewski,
J. Gao, N. Rega, G. Zheng, W. Liang, M. Hada, M. Ehara,
K. Toyota, R. Fukuda, J. Hasegawa, M. Ishida, T. Nakajima,
Y. Honda, O. Kitao, H. Nakai, T. Vreven, K. Throssell, J. A.
Montgomery, Jr., J. E. Peralta, F. Ogliaro, M. J. Bearpark, J. J.
Heyd, E. N. Brothers, K. N. Kudin, V. N. Staroverov, T. A.
Keith, R. Kobayashi, J. Normand, K. Raghavachari, A. P. Ren-
dell, J. C. Burant, S. S. Iyengar, J. Tomasi, M. Cossi, J. M.
Millam, M. Klene, C. Adamo, R. Cammi, J. W. Ochterski, R. L.
Martin, K. Morokuma, O. Farkas, J. B. Foresman, and D. J. Fox,
“Gaussian˜16 Revision C.01,” (2016), gaussian Inc. Wallingford
CT.

18D. E. Woon and T. H. Dunning, “Gaussian basis sets for use
in correlated molecular calculations. iii. the atoms aluminum
through argon,” J. Chem. Phys. 98, 1358–1371 (1993).

19C. E. Rasmussen and C. K. I. Williams, Gaussian processes for

machine learning, Vol. 14 (MIT Press, 2006).

20V. L. Deringer, A. P. Bart´ok, N. Bernstein, D. M. Wilkins, M. Ce-

riotti, and G. Cs´anyi, “Gaussian process regression for materials
and molecules,” Chem. Rev. 121, 10073–10141 (2021).

21A. P. Bart´ok, R. Kondor, and G. Cs´anyi, “On representing chem-

ical environments,” Phys. Rev. B 87, 184115 (2013).

22M. C. Kennedy and A. O’Hagan, “Bayesian calibration of com-
puter models,” J. R. Stat. Soc. Series B Stat. Methodol. 63,
425–464 (2001).

23P. Rowe, V. L. Deringer, P. Gasparotto, G. Cs´anyi, and
A. Michaelides, “An accurate and transferable machine learning
potential for carbon,” J. Chem. Phys. 153, 034702 (2020).

24A. O’Hagan, “Curve ﬁtting and optimal design for prediction,”

J. R. Stat. Soc. 40, 1–24 (1978).

25S. Sundararajan and S. S. Keerthi, “Predictive approaches for
choosing hyperparameters in gaussian processes,” Neural Com-
put. 13, 1103–1118 (2001).

26J. Revels, M. Lubin, and T. Papamarkou, “Forward-mode auto-
matic diﬀerentiation in Julia,” arXiv:1607.07892 [cs.MS] (2016).
27G. Wahba, Spline Models for Observational Data (SIAM, 1990).
28A. Vehtari, A. Gelman, and J. Gabry, “Practical Bayesian model
evaluation using leave-one-out cross-validation and WAIC,” Stat.
Comput. 27, 1413–1432 (2017).

