Engines of Parsimony: Part I
Limits on Computational Rates in Physical Systems

Hannah Earley*

Department of Applied Mathematics and Theoretical Physics, University of Cambridge

Figure 1: An illustration of the primary results of this paper. Consider a spherical region of computational
matter with radius R, convex surface area A and enclosed volume V . The expressions indicate how the
computational rate of each architecture scales, proportionally. From left to right, these regimes are:

√

√

V :
AV :
A :
RA :
R :

reversible computers at absolute zero;
reversible computers at ﬁnite temperature;
irreversible/canonical computers;
critical density (thick shell on the cusp of gravitational collapse);
critical density (thin shell on the cusp of gravitational collapse).

Lay Summary

In recent years, unconventional forms of computing ranging from molecular computers
made out of DNA to quantum computers have started to be realised. Not only that, but they
are becoming increasingly sophisticated and have a lot of potential to inﬂuence the future of
computing. One interesting class of unconventional computers is that of reversible computers,
which includes quantum computers. Reversible computing—wherein state transitions must
be invertible, and therefore must conserve information—is largely neglected outside of
quantum computing, but as we show in this paper this neglect is highly detrimental to
computational performance.

In particular, we consider the maximum sustained computational performance—in terms
of operations per second—that can be extracted from a given region of space, under fairly
general assumptions. Namely, we assume the known laws of physics, and that one will need to

*h.earley@damtp.cam.ac.uk,

orcid.org/0000-0002-6628-2130

1

1
2
0
2

v
o
N
0
3

]
h
c
e
m

-
t
a
t
s
.
t
a
m
-
d
n
o
c
[

6
v
5
0
6
3
0
.

7
0
0
2
:
v
i
X
r
a

 
 
 
 
 
 
supply the system with energy over time in order to keep it going. We show, similarly to early
work by Frank [1], that for any realisable computer reversible computers are strictly better
than irreversible computers at any size. We also derive universal scaling laws describing
just how much better a reversible computer could be compared to an irreversible computer,
proving that the adiabatic Time-Proportionally Reversible Architectures (TPRAs) of Frank
are the best possible, and suggest a path to achieving this bound with molecular computers.
To illustrate these results, summarised in Figure 1, suppose we wish to build the most
powerful computer we can within some spherical region of space, of radius R. If the
computer is irreversible, such as conventional silicon-based processors, then it is found that
essentially only the surface of this sphere can be used for computation and the interior must
be empty or inert. This clearly limits the rate of computation proportional to 4πR2. The
reason for this restriction is thermodynamic, arising from constraints on both supply of
power and rejection of heat. If R is very big, such that the computer threatens to collapse
into a black hole, then it is found that the computational rate can now only scale proportional
to R.

For a reversible computer the situation is substantially improved. In principle, a reversible
computer can operate without producing an increase in entropy and without requiring any
input of energy, and so the rate of computation could scale with the volume. Unfortunately
in practice this is not possible, and a more careful consideration of the system’s entropy is
required. The rate of computation is found to scale proportional to 4√
πR5/2, geometrically
3
between the area and volume. For large R, general relativistic effects become signiﬁcant
and this falls to the more restrictive bound R3/2. For even larger R this eventually falls to
proportional to R, coinciding with the irreversible computer. The reason for this additional
threshold between scaling laws is that the sub-relativistic reversible computers—where
gravitational effects on spacetime are negligible—are not taking full computational advantage
of their volume due to thermodynamic constraints; as the system gets larger beyond the
collapse threshold (when the geometry transitions to a thick shell rather than a sphere) the
thermodynamic constraints gradually relax, allowing a scaling that ‘temporarily’ exceeds the
limit value of R.

Therefore we see that at almost all scales, reversible computers substantially outperform
irreversible ones, whilst at extremely small—typically sub-nanometer—and large—order of
the visible universe in size—scales they coincide.

Technical Abstract

√

We analyse the maximum achievable rate of sustained computation for a given convex
region of three dimensional space subject to geometric constraints on power delivery and
heat dissipation. We ﬁnd a universal upper bound across both quantum and classical systems,
AV where V is the region volume and A its area, verifying and strengthening
scaling as
a result of Frank [1]. Attaining this bound requires the use of reversible computation, else
it falls to scaling as A. By specialising our analysis to the case of Brownian classical
systems, we also give a semi-constructive proof suggestive of an implementation attaining
these bounds by means of molecular computers. For regions of astronomical size, general
AR
relativistic effects become signiﬁcant and more restrictive bounds proportional to
and R are found to apply, where R is its radius. It is also shown that inhomogeneity in
computational structure is generally to be avoided. These results are depicted graphically in
Figure 1.

√

2

1. Introduction

The ubiquity of computer technology—and the increasing demands set upon it by intensive
algorithms from ﬁelds such as machine learning, physics simulations, and cloud computing,
among others—render the question of computer performance of considerable interest. Perhaps
the most well known observation on computer performance was published by Moore [2], who
noticed the trend of microchip component density doubling every 18 months, later dubbed
‘Moore’s law’ in his honour. This law seemed to apply to many quantities in computing, including
clock speed, FLOPS1 of the top 500 supercomputers combined, and inverse storage cost. A
popular debate arose over when, if ever, Moore’s law would stagnate; for clock speed, this point
has come and gone, with consumer processor clock speeds frozen around 3 GHz to 4 GHz.

Whilst it is not surprising that contemporary technology may pose limits on computational
performance, it is pertinent to ask whether the laws of physics impose hard bounds on performance.
Indeed, our knowledge of quantum physics, thermodynamics and relativity reveals the answer
to be afﬁrmative. For a brief overview of some of these constraints see, for example, Lloyd’s
analysis [3].

In this paper, we investigate how these bounds vary as a given computational system is scaled up.
The results we ﬁnd apply, with different constants of proportionality, for any given computational
architecture. A computer constructed from a mix of architectures can be treated as a linear
combination, and therefore these scaling results apply to any computer. This builds upon the work
of Frank [1], proving that his notion of adiabatic Time-Proportionally Reversible Architectures
(TPRAs) are the best possible, conﬁrming the relevant scaling results in the mesoscopic regime (as
well as analysing scaling in very small and very large regimes in more detail), and excluding the
possibility of architectures of lower dissipation. Moreover, the detailed calculations herein yield
speciﬁc constants of proportionality for broad classes of reversible computational architectures.
There are many metrics by which one may wish to measure computational performance. In this
paper, we shall investigate ‘speed’ as deﬁned by the rate of state transitions the computer is able
to execute. As we are more concerned with the general form of scaling law rather than the speciﬁc
constants for a speciﬁc architecture, the exact deﬁnition of ‘state’ and ‘transition’ are not too
important. In general the most ‘obvious’ deﬁnitions can be assumed, for example a conventional
silicon-based computer would have as its states each possible value of all its registers, memory and
any attached storage device, and as its transitions a single machine instruction2. More rigorous
deﬁnitions can be found in quantum mechanics, wherein a state would be an eigenstate of the
computational basis Hamiltonian, and a transition would correspond to the complete evolution of
the state between orthogonal eigenstates.

Other performance metrics of interest might concern synchronisation between distinct compu-
tational elements within a parallel system, and interaction with some arbitrary non-equilibrium
system such as a supply of additional memory resources. These shall be covered in parts II and
III of this paper series, respectively [4, 5].

1FLOPS, or FLoating point Operations Per Second, is a common measure of supercomputer performance.
2To be speciﬁc, we refer to the combination of a single op-code and its parameters.

3

(a)

(b)

Figure 2: An illustration of the three dominant physical constraints that apply to computational systems.
Constraints (a) and (b) are described in Section 1 and (c) in Section 5. (a) The quantum mechanical
constraint bounding the minimum time for a state transition for a system of given average mass-energy
(cid:104)E(cid:105) where E0 = 0. This illustration includes example energy levels of the eigenstates and a cyclic
sequence of orthogonal states {1, . . . , N } which are each a superposition of the eigenstates, with the same
average energy (cid:104)E(cid:105). (b) A combination geometric-thermodynamic constraint. Thermodynamically, each
computational operation generates entropy (generally heat) and this is bounded from below in the case of
irreversible computation, but still non-zero in the reversible case. Geometrically, we can only dissipate this
entropy at a rate scaling with the convex bounding surface, (II). The surface (III), whilst larger than (II), is
not useful as the entropy ﬂux must still pass through surface (II). Surface (I) is also larger, but the ﬂux
must ﬁrst pass through surface (II). The green region is the computational system proper. At steady state
there is an amortised balance between the entropy dissipation ﬂux and input power ﬂux. (continued)

Quantum Constraint on Computer Performance As a ﬁrst approach to bounding computa-
tional performance, we turn to quantum mechanics. Bremermann [6] gave a back of the envelope
calculation in 1962, subsequently reﬁned and elaborated upon by Margolus and Levitin [7] and
summarised in Figure 2a, to show that a system with energy E can change state at a maximum rate
of ν ≤ E/hP where hP is Planck’s constant. Assuming this energy is due primarily to rest mass,
this gives ν ≤ 1.36 × 1050 Hz kg−1. Restricting our attention to a speciﬁc architecture as deﬁned
earlier, we assume a ﬁxed density—or at least that the density is bounded from above—such
that ν ≤ V ρc2/hP . This immediately implies that the maximum rate of computation scales with
volume.

Thermodynamic and Geometric Constraint on Computer Performance This quantum
limit assumes a closed computational system, requiring no external power source. As shall become
clear, sustained processive computation without a driving force is for all practical purposes
impossible. For conventional computing architectures, such as those inside your computer and
smartphone, it is certainly impossible: this is because conventional computing is irreversible in
the sense that information is not generally conserved and so it is not possible to step backwards
through a computation. This is in stark contrast to the laws of physics which are reversible and

4

(c)

Figure 2: (continued) (c) The (general) relativistic constraint which informs the optimal mass distribution
of the system. Each example computational system is shown as a space time diagram, with the horizontal
axis corresponding to space and the vertical to time. Cones are light-cones showing the local causal
structure and indicating spacetime curvature due to the stress-energy tensor. The emanating light waves
correspond to the output of the computer. For small computers such as (i), general relativistic effects are
negligible. For larger computers such as (ii), space is noticeably curved within the vicinity of the system
and so the frequency of emitted photons is red-shifted, meaning that the computer appears slower to a
distant observer. For computers on the cusp of gravitational collapse such as (iii)—even if the density is
lowered to keep the Schwarzschild radius small, as indicated by the lighter shade of the system—light takes
a very long time to escape, diverging to inﬁnity as the system radius approaches 9
8 of the Schwarzschild
radius. By keeping the total mass and radius the same but reconﬁguring it into a spherical shell of the
same density as (i,ii), as depicted in (iv), we can minimise the effect of time dilation to at most a threefold
slowdown, and thus retain our qualitative power-law scaling.

information conserving3, and so in fact such models of computation are incompatible with the
laws of physics. Rather, irreversible computation can only ever be realised in an approximate
sense, leveraging thermodynamic principles to reject entropy to its environment. Irreversible
computation is exhibited by, for example, non-invertible logic gates such as AND, variable mutation
in which the previous state is lost, and merging of control ﬂow. Even if it is possible in principle
to reconstruct the previous computational state by some convoluted mechanism, this is insufﬁcient
as reversibility requires that a process can be readily rewound step-by-step.

The resolution of this physical incompatibility relates to the connection between thermodynam-
ics and information theory, formalised by Szilard [9] and Landauer [10] in the twentieth century.
For every bit of information discarded, a commensurate entropy increase of at least k log 2 is
produced elsewhere, where k is Boltzmann’s constant4. More generally, whenever a quantity of
information I bits is discarded, an entropy increase of ∆S ≥ kI log 2 manifests, with equality
only in the limit of thermodynamic reversibility in which the discarding process takes an inﬁnite
period of time. If this entropy were allowed to remain in the computational system, it would

3This includes quantum mechanics; the Schr¨odinger equation speciﬁes that the time evolution operator is unitary and
hence invertible. Furthermore, it seems likely that the irreversible process of wavefunction collapse is in fact a
reversible one of progressive decoherence, as espoused by the Many-Worlds Interpretation [8].

4When this entropy takes the form of heat, each bit will generate on the order of ∼ 10−21 J ∼ 10−2 eV at room
temperature. Conventional computers typically expend a factor of around 108 times this much for reasons relating
to reliability and speed.

5

eventually accumulate to such an extent that it would interfere with the well ordered operation
of the computational mechanism. As a visceral example, should the entropy take the form of
heat, ∆Q = T ∆S where T is the system temperature, the system will eventually transmute
into a form incompatible with its function—such as melting or turning into a plasma—as the
temperature becomes too great. Even if the system is physically heat-resistant, the increased
entropy will result in errors so frequent that almost all the computational capacity is directed to
error correction, slowing productive computation to a crawl.

In order to sustain computation then, it is necessary to remove the additional entropy at the
same amortised rate as its production. In the case of heat, this is achieved by cooling the system;
this process can be generalised to other forms of entropy, such as disorder in a spin bath [11],
but we shall restrict out attention to heat for conceptual convenience. We must thus enact a ﬂow
of work into the system and heat out in order to sustain computation, but here we encounter a
geometric constraint. This energy ﬂow applies at every bounding surface, assuming that the heat
is ultimately rejected to inﬁnity. Consider a convex bounding surface of area A, and assume our
technology of choice is capable of transporting energy up to a certain ﬂux φ, then the maximal
power that can be exchanged with the system is given by P ≤ φA. As has been established,
however, the rate of heat generation scales with the rate of computational transitions for an
irreversible computer, and therefore the rate of computation is ultimately bounded by the system’s
convex surface area, rather than its volume as contended by the quantum mechanical limit. See
Figure 2b for an illustration.

This is far more restrictive than the volumetric upper bound derived earlier, and suggests that
for large computers only a subvolume equivalent to the outermost shell of the volume can perform
useful computation with the remaining bulk relegated to dormant inactivity5. That is, we take
the more restrictive of the two bounds, which for small systems is the volumetric bound and for
larger (irreversible) systems is this areametric bound.

Ballistic Computation Does this areametric thermodynamic bound render the volumetric
quantum bound inaccessible? A trivial exception is found at small scales, where the surface
area to volume ratio becomes so large that the volumetric bound is in fact more restrictive
than the areametric bound, and thus takes priority. To more robustly improve on the rate
of computation, however, we must reduce the entropic cost associated with a computational
transition. This necessitates conserving information during computation. Landauer, upon coming
to this conclusion, argued that performing any useful computation under this limitation would be
untenable as, instead of raising the entropy of the environment, one would ﬁll the computer’s
memory with the discarded information instead. Fortunately, this objection was unfounded and a
decade later Bennett [12], oft considered the Father of Reversible Computing, explicated a viable
model of reversible computation in his Reversible Turing Machine (RTM). The key property of
RTMs is that they require both the domains and codomains of their rules to be non-overlapping,
rather than just their domains as is the case with standard Turing Machines6. Furthermore, he

5The remaining bulk need not be entirely devoid of purpose. It may, for example, be used for storage and memory. It
must however have negligible entropy generation, such as that resulting from structural deterioration due to cosmic
rays, spontaneous tautomerisation, etc.

6Turing Machines [13] were one of the ﬁrst general models for describing an arbitrary computing machine, the
other being the Lambda Calculus [14]. For an introduction to both, see textbooks on Automata Theory and

6

showed how any irreversible program could be efﬁciently simulated on such a reversible computer
without excessive memory overhead, thus proving that reversible computing was equipotent with
conventional computing. For a brief exposition of reversible programming, see Appendix B.

An idealised reversible computer would produce no entropy in its transitions. Fredkin and
Toffoli [15] described such an idealised system, a physical model of reversible computing that
could operate without being actively powered. This model consisted of a frictionless table upon
which hard elastic billiard balls were projected. The balls would bounce off of each other and some
strategically placed walls, with the precise conﬁguration describing the resultant computation.
As a testament to its capability, Fredkin’s student, Ressler [16], proposed the design of a fully
programmable billiard ball computer, complete with arithmetic logic unit. Given that such a
computer would be powered solely by its initial kinetic energy, the volumetric quantum bound
would be attainable. Unfortunately, Bennett [17] calculated that such a design would ultimately
be infeasible as even the most distant and subtle inﬂuences would be sufﬁcient to rapidly and
completely thermalise the motions of the system:

“Even if classical balls could be shot with perfect accuracy into a perfect apparatus,
ﬂuctuating tidal forces from turbulence in the atmospheres of nearby stars would be
enough to randomise their motion within a few hundred collisions. Needless to say,
the trajectory would be spoiled much sooner if stronger nearby noise sources (e.g.,
— Bennett [17]
thermal radiation and conduction) were not eliminated.”

Such inﬂuences could in principle be suppressed by error correction mechanisms, but error
correction corresponds to the discarding of excess entropy, and would seem to reconstitute the
very issues we were trying to avoid.

Our last refuge against unwarranted entropic inﬂuences is in quantum ground state condensates.
Phenomena such as superﬂuidity and superconductivity arise in a sufﬁciently cooled system,
wherein a macroscopic fraction of particles are found to inhabit their ground state, manifesting
macroscopic quantum behaviours and a subsystem with vanishing entropy and temperature. The
utility of such systems is manifold, but unfortunately it is doubtful that they can be exploited
for dissipationless computation; the reason is that, in order to enact transitions to orthogonal
quantum states, the Schr¨odinger equation requires us to prepare a superposition of (distinct)
energy states which then rotates under the action of the Hamiltonian. The presence of non-ground
eigenstates will necessarily result in a renewed vulnerability to thermodynamic perturbations.
Nevertheless, such cold temperatures are not altogether useless, as unwanted ﬂuctuations would
still be signiﬁcantly suppressed.

Brownian Dynamics The ﬂuctuations and dissipation that result from these thermodynamic
perturbations lead to diffusion of the distribution in phase space, and hence loss of absolute
control over it. Consequently there will be some level of unpredictability to the state of the system
and its (generalised) velocities. In the limit of negligible control, the dynamics are (mostly)
dominated by the noise source; this regime is referred to variously as Brownian or Langevin
dynamics. In Section 4 it shall be shown how to leverage the minimal extant control over the
system to make net progress, and to even robustly surpass irreversible computers in performance.

Computability Theory.

7

Summary Whilst fully dissipationless computation is all but impossible, it is still possible
in principle to tune the system so as to bring the entropic transition cost as close to zero as
desired. In exchange, the rate of computation may itself be diminished. In the paper that follows,
this compromise is evaluated across the range of known physics—from quantum to classical,
non-relativistic to relativistic—covering the full spectrum of feasible computational architectures.
In so doing, a universal scaling limit is discovered, exceeding that of irreversible computers yet
still falling short of the volumetric quantum bound.

2. Quantum Ballistic Architectures and the Quantum Zeno

Effect

We ﬁrst consider a quantum architecture. A viable quantum architecture must be ballistic in the
sense of following an exact prescribed trajectory in phase space, as to do otherwise would lead to
decoherence undermining the computational state. Of course we have established that a ballistic
quantum system cannot truly be isolated from entropic effects; in particular, the third law of
thermodynamics precludes lowering the temperature of any system to absolute zero, and so we
must incorporate a heat bath into our analysis.

Suppose the ballistic Hamiltonian is given by H0 and the perturbative effect of the heat bath
by V (t), such that the true Hamiltonian is H = H0 + V . Further, let the initial density be ρ and
introduce the projector P corresponding to the subspace of valid states, such that P ρ = ρP = ρ.
To ensure processive computation, we must therefore periodically correct errors introduced by
the perturbation. The cost of such error correction will be bounded from below by the entropy
increase due to V , which can be expressed in terms of the probability of an error δp corresponding
to the erroneous subspace P ⊥ = 1 − P .

If corrections are to be made at intervals δt, then this error will be found to be δp(t) =
tr(cid:2)P ⊥ρ(t + δt)(cid:3). For a time-dependent Hamiltonian, the quantum Liouville equation tells us
that ˙ρ = i[ρ, H/(cid:126)P ] where (cid:126)P is Planck’s constant. We can expand ρ as

ρ(t + δt) = ρ + δt

∂ρ
∂t

+ 1

= ρ + i[ρ, ε] + 1

∂t2 + O(cid:0)δt3(cid:1)

2 δt2 ∂2ρ
2 (iδt[ρ, ˙ε] − (ε2ρ − 2ερε + ρε2)) + O(cid:0)δt3(cid:1)

where we have omitted explicit time dependence for brevity and written ε ≡ Hδt/(cid:126)P . We then
ﬁnd

(cid:104)
P ⊥(ρ + i[ρ, ε + 1
δp = tr
(cid:105)
(cid:104)
P ⊥ερε
= tr

2 δt ˙ε] − 1
(cid:104)
+ O(cid:0)ε3(cid:1) ≡ tr

(cid:105)
2 (ε2ρ − 2ερε + ρε2))
+ O(cid:0)ε3(cid:1)
ρεP ⊥ε

(cid:105)

+ O(cid:0)ε3(cid:1)

where the last line uses the fact that P ⊥ρ = ρP ⊥ = 0. Note that if the ρεP ⊥ε term vanishes,
then so do all higher terms, and therefore this term is always the leading order approximant.

In order to evaluate this trace, we ﬁrst write H0 and V as block matrices in the basis (P, P ⊥),

8

yielding

H0 =

(cid:18)h00
0

(cid:19)

,

0
h11

V =

(cid:19)

(cid:18)v00 v01
v10 v11

.

Using the idempotence of projectors, i.e. P ⊥ ≡ P ⊥P ⊥, and the fact ρ ≡ P ρ, we can see that
ρHP ⊥H ≡ ρV P ⊥V = ρ(V 2 − V P V ). Therefore, we have

δp =

(cid:17)2

(cid:16) δt
(cid:126)P

((cid:104)V 2(cid:105) − (cid:104)V P V (cid:105)) .

The ﬁrst thing to notice about this expression is that it varies with the square of δt. This
is characteristic of the Quantum Zeno Effect (QZE [18])—that is, in the limit of constant
measurement (i.e. δt → 0), we can freeze dynamical evolution. Here, we are performing a partial
measurement by projecting onto the subspace of either valid or erroneous computational states,
thus allowing computational evolution to continue. Of course, the Zeno rate itself is subject to
the Margolus-Levitin limit [7] and so such constant measurement is not possible.

To proceed in further determining δp, and thereby δh, it is important to elaborate on certain
architectural details. Margolus and Levitin’s analysis shows that one gets the same E/hP total rate
regardless of whether one considers transitions of the system as a whole (‘serial’) or the combined
transitions of a system partitioned into subsystems (‘parallel’). For a physically realistic system,
measuring large subsystems as a whole is impractical and so a more ﬁne-grained architecture is
likely preferable. Nevertheless, we shall proceed generally, ﬁnding the result is independent of
this detail.

Let the subsystems be indexed by i ∈ I. For a fully serial system, |I| = 1. We allow each
subsystem to evolve independently, assuming that interaction events are negligibly frequent
relative to our Zeno corrections. Specialising our above result for δp, we have

δpi =

(cid:17)2

(cid:16) δti
(cid:126)P

((cid:104)V 2

i (cid:105) − (cid:104)ViPiVi(cid:105))

The strength of the perturbations Vi will be controlled by an effective temperature for that
subsystem, Ti. Suppose the subsystem has ni degrees of freedom7, hereafter referred to as
computational primitives. By the equipartition theorem, the energies of each primitive will be
individually perturbed. For an uncorrelated perturbation, we expect that (cid:104)V (cid:105) = 0 where x is
the time-average value of x. Therefore, (cid:104)V 2(cid:105) ≡ var(cid:104)¯·(cid:105) V . The expected aggregate perturbation
over all primitives, then, is ni(kTi)2 assuming V is Gaussian and where k ≡ kB is Boltzmann’s
constant.

As for the second term, V P V , we expect that on average V will mix (near-)degenerate states
indiscriminately. V P V represents the probability that the perturbation stays within the intended
computational subspace. The state space can be divided into disjoint computational subspaces,
within which the dynamics perform a reversible cycle over computational states. Let the total
state space have cardinality Ωi, and the number of microstates associated with each subspace be
ωij where j indexes the different subspaces such that Ωi = (cid:80)
j ωij. Assuming similar energy

7Excluding any frozen out by low temperatures.

9

distributions between subspaces, the chance of remaining within the original subspace is ωij/Ωi.
We further approximate this by ωi/Ωi where ωi = (cid:104)ωij(cid:105)j is the average subspace cardinality.
Equivalently, we can deﬁne Ωi/ωi ≡ Wi, the number of distinct programs that the subsystem is
capable of executing. Therefore,

δpi = ni

(cid:17)2

(cid:16)

δti

kTi
(cid:126)P

(1 − 1/Wi)

In fact, we may wish to correct the second term errors in which we jump within the same subspace,
as this may interfere with synchronisation between different subsystems (or it may be difﬁcult
to ascertain whether we have jumped within the same subspace). In this case, the expression
reduces to

δpi = ni

(cid:17)2

(cid:16)

δti

kTi
(cid:126)P

(1 − 1/Ωi)

In any case, 1 − 1/Wi is always ﬁnite and ∈ [ 1
2 , 1) for any useful subsystem; that is, a useful
subsystem should have Wi ≥ 2. For composite/non-primitive subsystems, we would like the
number of useful programs to scale exponentially with the number of primitives, i.e. Wi = gni
i
for some gi (typically on the order of unity but greater than 1). For a composite subsystem of
even moderate size, this exponential scaling will render the 1 − 1/Wi term effectively unity.

We are now able to determine the entropy increase between Zeno cycles. The indiscriminate
mixing of the perturbation means that, in the event of an error, the entropy attains its maximum
value. This yields the following expression for the information entropy8,

δhi = [−(1 − δpi) log(1 − δpi) − δpi log δpi + (1 − δpi) log ωi + δpi log Ωi] − [log ωi]

− log δpi) − O(cid:0)δp2
(cid:1)
= δpi(1 + log Ωi
ωi
≡ δpi(log Wi + 1 − log δpi) − O(cid:0)δp2
≥ niδpi log gi − O(cid:0)δp2

(cid:1) .

i

i

(cid:1)

i

This expression assumes ignorance of states within the current computational subspace; again, we
may wish to correct these errors too, in which case we may simply substitute ωi = 1. This would
effectively lead to taking Wi = Ωi = g(cid:48)ni

i ≥ gi, and so the inequality remains valid.

i where g(cid:48)

Putting these together, we get

˙hi ≥

n2
i
rZ,i

ζi

(cid:124)

(cid:17)2

log gi

(cid:16) kTi
(cid:126)P

(cid:123)(cid:122)
≡γi

(cid:125)

where rZ,i ≡ 1/δti is the Zeno measurement rate for the subsystem and ζi = 1 − 1/Wi ∈ [ 1
2 , 1).
The aggregate rate of entropy generation is given by ˙H = (cid:80)
˙hi and is subject to the constraint
k ˆT ˙H ≤ P where ˆT is the system temperature that governs the Landauer bound and P = φA

i

8We use H to refer to the information theoretical entropy, connected to the thermodynamical entropy as S = kBH.

Furthermore, we use the lowercase quantity h to refer to the entropy of a subsystem or particle.

10

is the heat dissipation power, proportional to the system’s surface area. It may seem surprising
that ˙hi ∝ n2
i . In fact, this only applies for sufﬁciently small δpi; when δpi becomes signiﬁcantly
large, ˙hi approaches its maximum of nirZ,i log gi.

We wish to maximise the computational rate, subject to this constrained entropy production. Per
Margolus and Levitin, this rate depends on the combined energy of the computational primitives.
If the average energy per primitive in subsystem i is εi, then the energy available for computation
is EC = (cid:80)

i εini and the computational rate is subject to RC ≤ EC/hP .

Introducing the Lagrangian multiplier α, we wish to maximise Λ with respect to ni;

(cid:88)

Λ =

εini −

i

1
2α

(cid:88)

i

n2
i
rZ,i

γi

=⇒

0 = εi −

1
α

ni
rZ,i

γi .

This gives ˙hi ≥ αεini and so

solving for α and summing over i,

˙H ≥ αEC ;

αrZ,i =

niγi
εi

≡

αRZ = EC

(cid:68) γi
ε2
i

EC

niεiγi
ECε2
i
(cid:69)

.

EC,i

where the average is taken over the computational energy distribution and RZ gives the net Zeno
rate of the system as a whole. Substituting into the ˙H constraint and making use of the fact that
RZ ≤ EZ/hP ,

≥ ˙H ≥

(cid:69)−1

E2
C
RZ
≥ E2
C

P
k ˆT
(cid:68) γ
ε2

P
k ˆT

EZ
hP

(cid:69)

(cid:68) γ
ε2

where ˆT can be called the ‘Szilard’ temperature: the temperature of the system in which entropy
is generated and must be subsequently erased. Finally, we obtain an expression for the net
computational rate RC,

(cid:115)

RC ≤

P
k ˆT

EZ
hP

(cid:28)

(cid:18) 2πkT
ε

ζi

(cid:19)2

(cid:29)−1

log g

=

1
2π

(cid:28) ζi log g
β2ε2

(cid:29)−1/2(cid:114) P
k ˆT

EZ
hP

.

Now, as the upper bound of P is proportional to the bounding surface area, and the upper bound
of EZ is proportional to the system’s volume, we get the scaling law

√

RC (cid:46)

AV ∼ V 5/6 ,

consistent with the scaling law for adiabatic architectures found by Frank [1] and showing it to
be an upper bound.

11

We note that Levitin and Toffoli [19] derive a related expression for the rate of energy dissipa-

tion/heat production within a quantum harmonic oscillator (QHO),

P =

εhP R2
C
(1 − ε)N

,

where ε is the probability of error per state transition and N is the number of QHO energy levels.
Given that their analysis assumes maximal RC, however, this simpliﬁes to

P =

ε∆E
1 − ε

RC

where ∆E is the separation of energy levels. Given that ε is assumed constant in the analysis, this
then yields an areametric bound on RC. Our analysis permits ε to vary, and its optimum value as
a function of system parameters is obtained. We also remain general in the systems studied, and
thus conﬁrm the conjecture that for a ﬁxed error probability the energy-dissipation rate increases
quadratically with the rate of computation.

We also note the recent results9 of Pidaparthi and Lent [20] on the Landau-Zener effect
(LZE) [21, 22]. In a closed quantum system, with no thermal coupling to the external environment,
the Landau-Zener effect predicts that the energy dissipation decays exponentially with the
switching time of the system (the inverse of the computation rate). That this is non-zero even
in the absence of an external environment shows the difﬁcult in achieving ballistic computation
even in ‘idealised’ conditions, but it certainly points towards a route to achieving near-ballistic
computation in regimes in which thermal coupling is negligible. As expected, once thermal
coupling is re-introduced to the system Pidaparthi and Lent show how the adiabatic behaviour
is recovered. Moreover, their numerical results indicate that the LZE does not appear to be a
practicable approach to achieving super-adiabaticity, except perhaps in a very narrow region of
system conditions and size. Moreover, the results of Pidaparthi and Lent show the occurrence
of a local minimum of dissipation with respect to switching time that would lead to engineering
difﬁculties in increasing computer size for this parameter range. In any case, our asymptotic
results stand, although it would be instructive to characterise the parameter range in which the
LZE is exploitable and in which the aforementioned engineering difﬁculties arise.

3. Classical Architectures I: A General Lagrangian Approach

If use of the QZE is not possible, perhaps because of high temperatures or an excessive Zeno rate,
then Fermi’s golden rule applies, giving δp ∝ δt. Consequently ˙h—and thus ˙H—is subject to
a ﬁnite lower bound. We ﬁnd ˙h ≥ n ˙p log g, yielding ˙H ≥ EC(cid:104) ˙p log g/ε(cid:105). Combining with our
constraint

˙H ≤ ˆβP , the inequality in RC becomes

RC ≤

(cid:28) ˙p log g
ε/hP

(cid:29)−1 P
k ˆT

9The author gratefully acknowledges Mike Frank for pointing him towards this body of work.

12

where hP here is Planck’s constant, and so the scaling law falls to RC (cid:46) A ∼ V 2/3. That is,
any ballistic system not making use of the QZE will be subject to the same areametric limit as
irreversible computers.

The breakdown of the QZE corresponds to an increase in thermal coupling. By abandoning
the desire to maintain well deﬁned quantum states, we instead enter the incoherent regime of the
classical realm wherein quantum effects are signiﬁcantly suppressed. Proceeding generally, we
adopt a Lagrangian formalism in order to remain noncommittal in our choice of coordinates (cid:126)q.

We introduce a reasonably general Lagrangian, L = T − V ,

L = [ 1

2 cij ˙qi ˙qj] − [V + wi ˙qi + O( ˙(cid:126)q 3)] ≡ 1

2 ˙q(cid:62)C ˙q − V − W ˙q + O( ˙q3)

where the coefﬁcients c, V , w... may depend continuously on the coordinates (cid:126)q, and Einstein
notation is used for repeated indices. We have rewritten the Lagrangian in matrix form for
concision, where C is a matrix, V a scalar, W a row vector, and q a column vector. We assume
the system is (effectively) closed, self-contained, or otherwise independent of its environment,
and thus L will not formally depend on the time parameter. In addition, this property implies
invariance under global translation of any of its coordinates and thus conservation of all associated
momenta. We obtain the Hamiltonian H and generalised momenta thus

H =

pi =

∂L
∂ ˙qi
∂L
∂ ˙qi

˙qi − L = 1

2 ˙q(cid:62)C ˙q + V + O( ˙q3)

= C ˙q − W + O( ˙q3)

Notice that the terms of our potential linearly dependent on the velocities drop out of the
Hamiltonian. Those dependent on the cube or higher remain, but will turn out to be negligible at
the velocities optimal for computation (and are typically not physically relevant in any case).

For brief collisions, the change in coordinates is negligible and so the spatiotemporal variance
in coefﬁcients can be neglected. In this case, we should conserve the Hamiltonian and momenta.
We introduce superscripts to the coordinates, q(n)
, to represent different particles. We also
introduce notation for the velocities before, u, after, v, and their change, ∆u = v − u. For a
collision between particles m and n,

i

∆H = [ 1
= 1

2 v(m)(cid:62)C(m)v(m) − 1
2 (u(m) + v(m))(cid:62)C(m)∆u(m) + 1

2 v(n)(cid:62)C(n)v(n) − 1
2 (u(n) + v(n))(cid:62)C(n)∆u(n) = 0

2 u(m)(cid:62)C(m)u(m)] + [ 1

2 u(n)(cid:62)C(n)u(n)]

∆p = C(m)∆u(m) + C(n)∆u(n) = 0 ,

13

where we use the fact that C is symmetric. Substituting ∆p into ∆H, we ﬁnd,

2∆H(m,n) = [(u(m) + v(m)) − (u(n) + v(n))](cid:62)C(m)∆u(m)

0 = [(2u(m) + ∆u(m)) − (2u(n) + ∆u(n))](cid:62)C(m)∆u(m)

= [2(u(m) − u(n)) + (1 + C(n)−1C(m))∆u(m)](cid:62)C(m)∆u(m)
= [2(u(m) − u(n)) + (C(m)−1 + C(n)−1)C(m)∆u(m)](cid:62)C(m)∆u(m)
≡ [2(u(m) − u(n)) + µ(m,n)−1C(m)∆u(m)](cid:62)C(m)∆u(m)
= [2 µ(m,n)+1/2(u(m) − u(n))
(cid:125)

+ µ(m,n)−1/2C(m)∆u(m)
](cid:62)µ(m,n)−1/2C(m)∆u(m) ,
(cid:125)
(cid:123)(cid:122)
x

(cid:124)

(cid:124)

(cid:123)(cid:122)
−y

where µ is the generalised reduced mass of the (m, n) system. We can solve for x, and thereby
∆u(m), by rewriting thus,

|x − y|2 = |y|2 .

This form reﬂects the fact that we lack sufﬁcient constraints to unambiguously solve for the
post-collision picture. In Cartesian coordinates for point particles, this manifests as an unknown
direction of motion afterwards. This is usually resolved by introducing the constraint that
momenta perpendicular to the normal vector between the colliding bodies are unchanged. In our
generalised coordinate system, the appropriate constraint is unspeciﬁed. The general solution is
given by

x = y + |y|ˆn = |y|(ˆy + ˆn)

where ˆn is a unit vector of unknown direction. We now ﬁnd an expression for the change in
kinetic energy of our particle of interest, (m),

∆H(m) = (u(m) + 1

2 ∆u(m))(cid:62)C(m)∆u(m)
= u(m)(cid:62)µ(m,n)1/2|y|(ˆy + ˆn) + O(∆u(m)2)
= η|u(m)(cid:62)µ(m,n)(u(m) − u(n))| + O(∆u(m)2)

where we have introduced a factor η ∈ [−2, 2] to take into account our uncertainty in the ﬁnal
direction, and where we have assumed |∆u| (cid:28) u. Being more careful, we can determine ∆u(m)
thus,

∆u(m) = C(m)−1µ(m,n)+1/2|µ(m,n)+1/2(u(n) − u(m))|(ˆy + ˆn)
|∆u(m)| = η(cid:48)|C(m)−1µ(m,n)(u(n) − u(m))|

= η(cid:48)|(C(m) + C(n))−1(C(n)u(n) + C(m)u(m)) − u(m)|
= η(cid:48)|˜u(m,n) − u(m)|

where ˜u(m,n) is the average of prior velocities, weighted by their generalised masses C(·), and
η(cid:48) ∈ [0, 2] is another uncertainty factor related to η. When u(m) is large compared to u(n),

14

(u(m) + ∆u(m)) ≈ u(m). When it is small, (u(m) + ∆u(m)) ≈ u(n). Thus a better approximation
to ∆H is given by

∆H(m) ≈ η|˜u(m,n)(cid:62)µ(m,n)(u(m) − u(n))| .

∆H gives the energy lost from the computational particle (m) to some environmental particle
(n), and thus is the amount of energy that must be supplied to the computational system and
dissipated from the thermal system. In order to proceed we must determine the rate of this energy
transfer; if we assume that the kinetic dynamics of the system are signiﬁcantly faster than the
computational transitions, as suggested by our inability to sufﬁciently control the quantum state,
then we can assume negligible extant correlations between the positions and velocities of different
particles. Thus we can employ a kinetic theory approach to determine this rate.

We ﬁrst determine the mean free path (cid:96), the average distance a particle travels between
collisions. Given the vanishing correlations, we are able to treat collisions between different
species of particle separately. For spherically symmetric particles, we ﬁnd π(r(α) +r(β))2(cid:96)(α;β) =
1/n(β) for the mean free path of α particles between collisions with β particles, where n(β) =
N (β)/V (β) is the number density of β particles. More generally, we can replace π(r(α) + r(β))2
with A(α,β), the effective collision cross-section for arbitrarily shaped particles averaged over
impact orientation. We also need to determine the average relative speed between α and β
particles,

¯v(α,β)2
rel

= (cid:104)(v(α) − v(β))2(cid:105)
= (cid:104)v(α)2(cid:105) + (cid:104)v(β)2(cid:105) − 2(cid:104)v(α) · v(β)(cid:105) ,

¯v(α)2 + ¯v(β)2 ,

(cid:112)

¯v(α,β)
rel =
ν(α;β) = ¯v(α,β)

rel

/(cid:96)(α;β)
= A(α,β)n(β)(cid:112)

¯v(α)2 + ¯v(β)2 ,

where we have used the fact that the velocities are uncorrelated to cancel the cross term. Summing
over these rates for each class, we can ﬁnd the rate of loss of kinetic energy for α particles,
η(α;β)|˜u(α,β)(cid:62)µ(α,β)(u(β) − u(α))|A(α,β)n(β)(cid:112)

¯u(α)2 + ¯u(β)2 .

˙H(α) =

(cid:88)

β

When α particles are heavy and fast, this reduces to

˙H(α) =

=

(cid:88)

β
(cid:88)

β

η(α;β)|u(α)(cid:62)C(β)u(α)|A(α)n(β) ¯u(α)

η(α;β)ρ(β)A(α) ¯u(α)3

where ρ = n|C| is the generalised density. Notice that this takes the same form as the equation
for hydrodynamic drag, taking the drag coefﬁcient to be 2η. Assuming there exists some

15

correspondence between the generalised velocity u(α) and the rate of computation, we ﬁnd

P ≥

(cid:88)

α

N (α)A(α)

(cid:18)

(cid:96)(α) R(α)
N (α)

(cid:19)3

(cid:88)

β

η(α;β)ρ(β)

where α ranges over computational particles and (cid:96) is the characteristic generalised displacement
corresponding to a single computational transition. To maximise the net computational rate R =
(cid:80)
α R(α), we ﬁnd that we need to let ¯u(α) → 0, but this violates our assumption that α particles
are fast. For ﬁnite velocity computational particles, we thus ﬁnd that out net computational rate
scales as R (cid:46) A ∼ V 2/3.

In the limit of slow α particles, |u(α)| (cid:28) |u(β)|,

˙H instead reduces to

˙H(α) =

(cid:88)

β

η(α;β)|˜u(α,β)(cid:62)C(β)u(α)|A(α)n(β) ¯u(β)

where we have taken (cid:104)u(β) − u(α)(cid:105) = ¯u(α). If the generalised mass of the α particles is not
signiﬁcantly heavier than the β particles, then ˜u(a,b) will have a strong dependence on u(β) and
this will lead to the same scaling limit of R (cid:46) A. In order to improve on this, we must let the α
particles be signiﬁcantly heavier. In this limit, ˜u(α,β) ≈ u(α) and we get

˙H(α) =

P ≥

(cid:88)

β

(cid:88)

α

η(α;β)|u(α)(cid:62)C(β)u(α)|A(α)n(β) ¯u(β) ,

N (α)A(α)

(cid:18)

(cid:96)(α) R(α)
N (α)

(cid:19)2

(cid:88)

β

η(α;β)ρ(β) ¯u(β)

(cid:88)

≥

α

R(α)2
N (α)

A(α)(cid:96)(α)2 (cid:88)

η(α;β)ρ(β) ¯u(β)

.

(cid:124)

β

(cid:123)(cid:122)
γ(α)

(cid:125)

To proceed, we maximise the rate subject to this power constraint,

Λ =

(cid:88)

α

R(α) −

1
2λ

(cid:88)

α

R(α)2γ(α)
N (α)

=⇒

0 = 1 −

R(α)γ(α)
λN (α)

n(α)
N γ(α)

N = N

(cid:29)

(cid:28) 1
γ(α)

λR(α) = λR =

(cid:29)−1

R2
N

(cid:28) 1
γ(α)

R
λ

=

P ≥

(cid:88)

α

(cid:88)

α
(cid:115)

R ≤

P N

(cid:29)

(cid:28) 1
γ

16

where N = (cid:80)
α N (α) is the total number of computational particles. To maximise the compu-
tational rate R, then, we let N scale with the volume of the system, leading once again to the
scaling law

√

R (cid:46)

AV ∼ V 5/6 .

4. Classical Architectures II: Brownian Machines

Whilst the previous two sections should sufﬁce to cover all physical computational systems, the
high mass-low speed limit of the classical system is not ideal from an engineering perspective.
For improved practicality, we reformulate this limit in terms of an abstract chemical reaction
network (CRN) near equilibrium, and show that we obtain the same result. This result is thus
more robust in terms of its attainability.

Entropy generation rate We ﬁrst seek a general expression for the rate of entropy generation
for a chemical reaction network. Consider any such dynamical system consisting of a set of
species {Cj : j} and a set of reversible reactions Γ = {νijXj ←→γi ν(cid:48)
ijXj : i}, where we
sum over j and the νij are stoichiometries. By the convergence theorem for reversible Markov
systems, the system will converge to a unique steady state/equilibrium distribution for any initial
conditions. As the reactions are reversible, the steady state will also satisfy detailed balance such
that the forward and backward rates coincide separately for each reaction, i.e. Ri = Ri, where R
is a rate of the whole system rather than per unit volume.

Let us look for a quantity H that measures progress towards equilibrium and satisﬁes the

following properties:

1. H is a state function of the system10, depending only on its instantaneous description;

2. H increases monotonically with time;
3. H is additive, i.e. H(∪iVi) = (cid:80)

i H(Vi) for any set of disjoint regions Vi.

By properties 1 and 2 and the convergence theorem, H will approach a unique maximum at
equilibrium. Microscopically, reactions occur discretely and so the rate of change of H can be
written

˙Hi = Ri∆hi + Ri∆hi = (Ri − Ri)(hreactants − hproducts)

for any reaction i, where we have used reversibility to identify ∆hi = −∆hi = hreactants−hproducts
and where h((cid:80)
j νijXj) is the entropy of the region associated with precisely νij particles of
each species Xj. In order to satisfy property 2, the sign of ∆hi must equal that of Ri − Ri.

Now, consider ﬁxing hproducts and Ri, and varying Ri; this is possible unless the reaction
is trivial with νij = ν(cid:48)
ij for all species Xj, i.e. it does nothing. To maintain property 2, it
must therefore be the case that ∆hi = h(Ri) − h(Ri). To satisfy property 3, we require

10More accurately, on the joint state of an ensemble of iid systems.

17

h(αRi) − h(αRi) = h(Ri) − h(Ri) where α is some arbitrary scaling factor of the system. We
can therefore infer the functional form h(x) = logb ax for some constants a and b.

We have therefore derived an entropy like quantity, unique up to choice of logarithmic unit
via b and entropy at T = 0 via a. Without loss of generality we choose a = 1 and b = e, and
therefore ﬁnd that

˙H =

(cid:88)

(Ri − Ri) log

i

Ri
Ri

(cid:88)

≡ 2

i

Riβi arctanh βi

where βi = (Ri − Ri)/(Ri + Ri) is the effective bias of reaction i and Ri = Ri + Ri is its gross
reaction rate.

Entropy generation rate for elementary chemical reactions We now calculate our en-
tropy quantity for elementary chemical reactions and compare against the expected value. The
rates for elementary chemical reactions are given by collision theory as

ri = ki

(cid:89)

[Xj]νij

j

ri = ki

(cid:89)

[Xj]νij

j

where ki and ki are rate constants and r are rates per unit volume. Assuming no inter-particle
interactions, the canonical entropy is given by the Sackur-Tetrode equation which can be obtained
simply by considering the spatial distribution of the particles along with any other degrees of
freedom. For species Xi, consider an arbitrary volume Vi available to it; if the thermal volume of
the particles is Vi = Λ3 where Λ is the thermal de Broglie wavelength, then there will be Vi/Vi
loci available to the particle, and so the associated entropy within the volume will be

Nih(Xi) = Ni log

Vi
Vi

+Ni(εi − 1)

− log Ni!
(cid:124) (cid:123)(cid:122) (cid:125)
Gibb’s factor

h(Xi) = εi − log[Xi]Vi + O

(cid:19)

(cid:18) log Ni
Ni

where Ni = [Xi]Vi is the number of particles in the volume, and [Xi] the concentration. We have
also taken into account indistinguishability of the particles with the Gibb’s factor, and allowed for
additional degrees of freedom with the εi term. Strictly speaking the spatial distribution should
be calculated combinatorially via the multinomial coefﬁcient

log

(V /V)!
((V /V − (cid:80) Ni))! (cid:81) Ni!

where we have assumed all the particles inhabit the same region and have the same wavelength.
Applying Stirling’s approximation and assuming that (cid:80) Ni (cid:28) V /V, this reduces to (cid:80) Ni(1 −
log[Xi]V) however, so our simplistic derivation is valid.

Therefore we ﬁnd the canonical entropy change due to reaction i is given by

∆hi =

(cid:88)

(νij − νij)h(Xj) =

j

(cid:88)

j

(νij − νij)(εj − log Vj) −

(νij − νij) log[Xj]

(cid:88)

j

18

using a slight abuse of notation (the logarithmands are not unitless, but their combination is).
Comparing with our quantity ∆h = log r/r, we ﬁnd

∆hi = log

(cid:88)

−

j

(νij − νij) log[Xj]

ki
ki

=

V νij −νij
j

(cid:19)(cid:18)

exp

(cid:88)

j

(cid:19)

(νij − νij)εj

ki
ki
(cid:18)(cid:89)

j

which should be compared to the Arrhenius equation. Indeed, the Vi terms confer the appropriate
units to the pre-exponential factor and, by expanding the enthalpy term in terms of kBT as
εi = ε(0)
kBT )2, we can group the roughly constant terms of the enthalpy into
the pre-exponential factor, leaving the exponential term in the form e−∆E/kBT .

i + O( 1

kBT ε(1)

i + 1

Maximising performance in Brownian machines We wish to maximise the net reaction
rate of the system by selecting the biases βi of each individual reaction, subject to bounded
total entropy production. If R = (cid:80) Ri is the total gross reaction rate, then the total net rate is
Rc = (cid:80) Riβi = R(cid:104)β(cid:105) where the expectation value is with respect to the fractional contribution
of each reaction, i.e. weighted by Ri/R. We denote this by Rc assuming that each reaction
contributes to computational progress; if this is not true then Rc will be less than this value. The
entropy rate is similarly given by ˙H = 2R(cid:104)β arctanh β(cid:105). We use a Lagrange multiplier approach
as usual,

Λ = R(cid:104)β(cid:105) − λR(cid:104)β arctanh β(cid:105)

=⇒

Ri = λRi∂βiβi arctanh βi;

that is, the optimum is achieved by setting all the βi equal to a constant, β. This gives ˙H =
2Rβ arctanh β = 2R2

c

R + O( R4

c

R3 ) and hence

Rc ≤

(cid:114)

P R
2kT

,

leading once again to the scaling limit Rc (cid:46)
AV ∼ V 5/6, as the power P scales with area and
the gross computation rate R with volume. Here we have presented a constructive proof of this
scaling limit, as any valid reversible CRN implementation will yield this scaling limit.

√

5. Relativistic Effects at Scale

At different scales, from the microscopic to the cosmic, different constraints predominate in
the analysis of maximum computation rate. For macroscopic systems at worldly scales, the
aforementioned thermodynamic constraints are most directly relevant, yielding an upper bound
of RC (cid:46)
AV . At sufﬁciently small scales, however, the surface area to volume ratio will be
great enough that this thermodynamic constraint will no longer be limiting, with the volumetric
Margolus-Levitin bound instead dominating. This reﬂects the fact that there is simply insufﬁcient

√

19

energy enclosed in the system for the resultant rate of computation to saturate the heat dissipation
capacity at the boundary. If one were to use a denser computational architecture, the computational
capacity of the region could be increased to the point that the power-ﬂux bound is saturated, and
thus recovering the more restrictive

AV bound.

√

It transpires that there are two further regimes. As our systems approach cosmic scales, the
threat of gravitational collapse becomes pertinent. In order to avoid this fate, we must lower the
average density such that the system’s radius always exceeds its Schwarzschild radius11. As the
Schwarzschild radius is proportional to the mass of the system, this implies that the computational
rate of such large systems varies linearly in radius. In fact, there is an intermediate regime: as we
have not been utilising the full computational potential of our mass due to the thermodynamic
constraint, we can gradually increase said utilisation whilst simultaneously reducing our overall
density until the Margolus-Levitin limit is attained, at which point we default to the linear regime.
This intermediate regime is thus still described by the RC (cid:46)
P M limit, which in this post-
Schwarzschild realm equates to RC (cid:46) V 1/2. In summary, the scaling laws of these four regimes
go as V , V 5/6, V 1/2 and V 1/3, in order of increasing scale.

√

A more detailed analysis reveals that these large systems are subject to a further consideration.
So far we have assumed a Galilean invariance worldview. At small and medium scales, this
approximation is very good. At larger scales, however, relativistic effects threaten to reduce
our overall computation rate via time dilation, and at even larger scales they can constrain the
amount of mass we can ﬁt within a volume lest the system undergo gravitational collapse as
mentioned earlier. Special relativistic time dilation is easily avoided by minimising relative
motion within the computational parts of the system (this implies that message packets will have
reduced computational capacity compared to the static computational background, but message
packets can generally be assumed static anyway).

Gravitational time dilation is a more serious issue, which cannot be eluded at large scales. In the
case of a hypothetical supermassive computational system, local computation proceeds unabated,
but distant users interfacing with the system will observe slower than expected operation, as
depicted in Figure 2c. To calculate this slowdown factor, we shall proceed by modelling the
system as spherically symmetric. This is not unreasonable at these scales, where a body’s self-
gravitation makes maintaining other geometries unstable and impractical. We will not consider
rotational systems which could allow for an ellipsoidal shape, as the requisite angular velocities
would almost certainly abrogate nearly all computational progress solely due to special relativistic
effects.

The metric12 of an isotropic and spherically symmetric body in hydrostatic equilibrium can be

11In reality, the threshold radius for gravitational collapse exceeds the Schwarzschild radius by a factor not less than

9
8 . The reasons for this shall be discussed in due course.

12We use a (+ − −−) signature.

20

obtained via the Tolman-Oppenheimer-Volkoff (TOV) equation [23],

(cid:17)(cid:16)

1 +

4πr3P
mc2

(cid:17)(cid:16)

1 −

2Gm
rc2

(cid:17)−1

dP
dr
dν
dr

= −

= −

1 +

(cid:16)
Gm
r2 ρ
(cid:16)
2
P + ρc2
(cid:16)

1 +

P
ρc2
(cid:17) dP
dr
4πr3P
mc2

2Gm
r2c2

=

(cid:17)(cid:16)

1 −

2Gm
rc2

(cid:17)−1

(1)

ds2 = eνc2dt2 −

(cid:16)

1 −

2Gm
rc2

(cid:17)−1

dr2 − r2dΩ2 ,

where P is the pressure, m(r) is the mass enclosed by the concentric spherical shell of radius r,
and ρ is the local mass-energy density. All masses are as observed by a distant observer.
The rate of dynamics of a point P as observed from a point Q is well known to be

R(Q)
R(P )

=

(cid:115)

g00(P )
g00(Q)

,

where gµν is the metric tensor. We assume our distant observer to be moving slowly (relative to
the computer) in approximately ﬂat space, and thus that g00(Q) = 1. This yields a slowdown
factor of f (P ) = (cid:112)g00(P ). The total slowdown for an extended body is hence found to be

f =

=

1
M
1
M

(cid:90)

dV ρ

√

g00

V
(cid:90) r1

0

dr

dm
dr

eν/2 ,

(spherically symmetric)

where M is the total mass and r1 is the least upper bound of its radial extent.

In order to maximise f , we must clearly maximise ν throughout the body. It can be seen
that dν
dr ≥ 0, and in fact within solid regions where ρ > 0 this inequality is strict. Furthermore,
in empty space the TOV equation breaks down; instead, we use our Schwarzschild matching
conditions to ﬁnd

dν
dr

=

(cid:16)

2Gm
r2c2

1 −

∆ν = ∆ log

(cid:16)

1 −

(cid:17)−1

(cid:17)

,

2Gm
rc2
2Gm
rc2

which shows that this inequality is always strict whenever m > 0. ν at the surface is ﬁxed by the
Schwarzschild metric, in accordance with Birkhoff’s theorem, and thus for a ﬁxed extent our only
control over ν is the distribution m between the surface and core. Approaching the core from
the surface, ν strictly decreases until m vanishes. Furthermore dν/dr is at least as great as in
empty space, being strictly greater in massive regions. Therefore it is desirable to concentrate
mass towards the surface.

We assume that the density of our architecture of choice is bounded from above, thus limiting
the degree to which we can concentrate mass towards the surface. The optimum is then achieved

21

by a thick shell of limiting density from the surface inwards. The two extreme cases of a thin
shell and a solid sphere can be treated exactly, but an arbitrarily thick spherical shell must be
treated numerically.

Solid Sphere A solid sphere coincides with the Schwarzschild geometry. The usual Schwarz-
schild metric corresponds to the exterior of the object, instead we must use the interior solution
which can be derived from the TOV equation (1), yielding

√

g00 =

(cid:114)

1 −

3
2

rs
r1

−

1
2

(cid:114)

1 −

(cid:16) r
r1

(cid:17)2 rs
r1

where rs = 2GM/c2 is the Schwarzschild radius. Before computing f , note that g00 vanishes
when rs = 8
9 r1. This limit is in fact of great importance; whilst an object of size r1 = 9
8 rs would
exceed the Schwarzschild radius and be presumed stable against gravitational collapse, this is
not the case. At this point, such an object is unstable towards spontaneous oscillations, its core
pressure diverges, and it readily collapses to a black hole [24]. Furthermore, depending on its heat
capacity ratio γ, a gaseous hydrostatic sphere may be unstable at even larger radii as tabulated
by Chandrasekhar [25]. This r1 ≥ 9
8 rs threshold is thus a stronger bound on the size of massive
objects. Integrating

√

g00 over the mass of this solid sphere, we ﬁnd
√

((1 + 6vs)

1 − vs − v−1/2

s

arcsin

fsolid =

√

vs)

3
16vs

where we have introduced the normalised coordinate v = r/r1. At the vs = 8
fsolid ≈ 0.1699.

9 threshold, we get

Thin Shell For a thin shell, we ﬁrst identify that the pressure on the outer boundary vanishes.
Let the inner radius be r0 and the thickness be δr = r1 − r0 such that M ≈ 4πρr2
1δr and δv (cid:28) 1.
We also introduce the unitless variable u = P/ρc2,

du
dr
du
dv

= −

= −

≈ −

∆u ≈ − 1

(cid:17)(cid:16)

1 −

+ u

(1 + u)

(cid:16) r − r0
r
(cid:16) r − r0
r

4πρrG
c2
4πρrr1G
c2
vs
(u + 1)(u + βδv)(1 − βvs)−1
2δv
2 vs(u + 1)(u + βδv)(1 − βvs)−1 ,

(1 + u)

+ u

(cid:17)(cid:16)

(cid:17)−1

2Gm
rc2
2GM
c2

1 −

(cid:17)−1

r − r0
rδr

where β = r−r0
r1−r0
yielding

∈ [0, 1]. Integrating from the surface inwards, we have u1 = 0 and β = 1,

u0 =

δv
2

vs
1 − vs

+ O(cid:0)δv2(cid:1) .

Therefore in the limit δv → 0, the pressure throughout the thin shell vanishes. As a result, we
also have that ν is constant for all v ∈ [0, 1], taking the value 1 − vs and giving

At the threshold, we get fthin = 1
3 .

fthin =

√

1 − vs .

22

Thick Shell For general δv, we integrate numerically. We rewrite the TOV equations in
normalised and unitless form thus

d log g
dv

=

vvs
2

µ + 3u
µ1 − v2vsµ

,

du
dv

= −(1 + u)

d log g
dv

,

df
dv

= −

3v2g
µ1

,

where µ = 1 − (v0/v)3, µ1 = 1 − v3
to v = v0, with initial conditions u1 = 0, g1 =
given by f0, and explicit results are given in the following section.

0 and g ≡

g00. We integrate the system from v = 1 down
√
1 − vs and f1 = 0. Our slowdown factor is

√

6. Variance in Performance at Different Scales

In order to calculate the maximum performance of a system at different sizes, we compute the
optimum speed within each constraint—thermodynamic and Margolus-Levitin—with relativistic
corrections, and then pick the least of these upper bounds. To apply the relativistic constraint,
we substitute M = 1
2 rsc2/G for mass where rs is the Schwarzschild radius, and then multiply
by our slowdown factor f . For better parametericity, we actually use rs = vs(cid:96) where (cid:96) is the
system’s radius/linear dimension. This yields the two bounds,

Rthermo. = α

(cid:112)

(cid:96)3vsf (vs, v0) ,

Rmarg.lev. = β(cid:96)vsf (vs, v0) ,

where α and β are constants of proportionality that depend on the system architecture. Notice
that there is in fact some choice available in system geometry; we can pick any constant density
thick shell solution, with parameters (v0, vs). As we increase vs, the factor f will decrease, and
so there will be an optimum pair (v0, vs) maximising the given bound. The maximum density
constraint is equivalent to setting a maximum value to the mass and hence vs,

vs =

1
(cid:96)

2GM

c2 = (cid:96)2 8πρG

3c2 (1 − v3
0)

≤ vm ≡ (cid:96)2 8πρG
3c2

≤

8
9

,

where the last constraint is to avoid gravitational collapse. For a given vm then, we have
v0 = 3(cid:112)1 − vs/vm and we maximise the two R bounds in 0 < vs ≤ min( 8

9 , vm).

An illustrative example of these regimes is depicted in Figure 3. Notice how the solid sphere,
though maximising usable computational matter and thus its local computational rate, has its
observable computational rate abrogated as it approaches the point of gravitational self-collapse.
This demonstrates how reducing the available matter can enable the observable computational
rate to increase, though the difference between the dashed and solid lines shows that there
remains a relativistic penalty in the form of gravitational time dilation. We also see that an
irreversible system underperforms with respect to a reversible system across a large range of
length scales, otherwise matching it at the extremes. The exact range again depends on system
architecture speciﬁcs, and for some architectures the irreversible system may give transient
superior performance at certain scales.

23

Quantum
Thermo (Irrev.)
Thermo (Rev.)
Grav. Threshold.

(a)

(b)

(c)

(d)

(e)

(f)

Figure 3: In these plots are shown a series of idealised examples of the relevant bounds on computation
rate. The vertical axes represents rate of computation in arbitrary logarithmic units, and the horizontal axes
the radius of the system in arbitrary logarithmic units. The axis bounds are the same for all Figures (a–g).
The examples are parameterised by mass-energy density, raw rate of computation per unit mass, power
ﬂux through the convex bounding surface, entropy cost of erasing one bit of information, and temperature
of the system.

For a given density, there will exist a threshold radius at which the Schwarzschild radius and system
radius of a solid sphere coincide, leading to gravitational collapse. This threshold is indicated by the
vertical dashed line. As discussed, the geometry must transition to a spherical shell beyond this point. The
lines labelled ‘quantum’ correspond to the quantum mechanical upper bound on computation rate, which
scales as M ∼ R3 below the threshold radius, and M ∼ R beyond. The lines labelled ‘thermo (irrev.)’
correspond to the thermodynamic upper bound on the rate of irreversible computation, which scales as
A ∼ R2. The lines labelled ‘thermo (rev.)’ correspond to the thermodynamic upper bound on the rate
of reversible computation, scaling as R2.5 and R1.5 below and above the threshold radius respectively.
(continued)

24

(g)

Quantum
Quantum†
Thermo (Irrev.)
Thermo (Rev.)
Thermo (Rev.)†
Thermo (Rev.)‡
Net Bound

e
t
a
R

Linear Dimension

Figure 3: (continued) In each of these examples, assume the same parameter values unless otherwise
speciﬁed: (a) the maximum possible performance, as dictated by the Margolus-Levitin and Szilard-
Landauer bounds; (b) the rates for the particular architecture as characterised by the parameter values;
(c) higher raw rate of computation per unit mass (bounded by c2/hP ); (d) greater power ﬂux or lower
entropy cost per erased bit (bounded by kB log 2); (e) lower temperature; (f) higher density.

For each of these examples, the maximum computation rate for a given system size is given by the
least of the three upper bounds. These ﬁgures are also idealised in the sense of ignoring gravitational time
dilation for simplicity.

A more complete picture is given in (g). The solid lines show constraints corrected for time dilation∗,
whilst the dashed variants (†) are uncorrected variants and the (‡) variant shows the case without geometric
optimisation (i.e. maintaining a solid sphere of constant density rather than transitioning to a spherical
shell). The dashed ‘net bound’ line shows the best rate achievable for a given system radius.

∗

Time dilation corrections were computed by integrating the thick shell system of differential equations using dopri5,
and the shell thickness was optimised to maximise the rate using a golden section search. The code was written in
haskell and is available, complete with integration and optimisation routines, at https://github.com/hannah-earley/
revcomp-relativistic-limits.

25

7. Discussion

Additional Entropic Sources The preceding analysis has been restricted to dissipating en-
tropy arising in the course of the computation itself. However, a physical computer will be
subject to other sources of entropy in addition to this. As mentioned in the introduction, Bennett
calculated that the inﬂuence of turbulence in the atmospheres of nearby stars would be sufﬁcient
to thermalise a billiard ball computer within a few hundred collisions. Thus, in general, we
must aim to shield our computer from such external inﬂuences, for example by error correction
procedures. Fortunately, the inﬂuences of such external sources may only interact with our system
via the same boundary through which we dissipate computational entropy, and as such their
rate is at most proportional to this surface area13. Thus, as long as the external sources are not
overwhelmingly strong we should be able to suppress them at all scales with equal ease.

A more challenging source of errors and entropy comes from within. Assuming a non-vanishing
system temperature, the entire system volume will generate thermodynamic ﬂuctuations at a
commensurate rate. These ﬂuctuations can manifest in a variety of ways, perhaps the most
damaging of which is radioactive decay leading to radical generation with the potential to damage
the computational structure. Every event leading to a change in computational state or requiring
repair invokes an entropic cost, and given that these events occur with a rate proportional to the
computationally active volume, this ultimately recovers an areametric bound to our computational
rate,

V ≤

P
kT

1
˙ηint.ﬂuc.

.

This does not necessarily render our reversible computation performance gains unattainable,
however. Providing ˙ηint.ﬂuc. is sufﬁciently small, we can outperform irreversible computers at
scales up to

(cid:115)

(cid:96) ≤

P
kT

1
˙ηint.ﬂuc.

1
δr

where δr is the thickness of the irreversible computational shell that can be supported by such
an architecture. If this threshold size is sufﬁciently large as to coincide with the Schwarzschild
threshold of Figure 3 then such a reversible architecture would in fact outperform its irreversible
analogue at all scales. Furthermore, it is likely that ˙ηint.ﬂuc. is smaller than it would be in an
irreversible system as they have one fewer source of ﬂuctuations to combat; namely, irreversible
computers expend signiﬁcant quantities of energy to ensure unidirectional operation, and any
deviation from this is a potentially fatal error. In contrast, a reversible computer is intrinsically
robust to such ‘errors’, particularly the Brownian architecture described, which actively exploits
this. Expending less energy has the added advantage of permitting lower operating temperatures,
further reducing ˙ηint.ﬂuc..

13Even though such external inﬂuences may indeed permeate the entire volume, such as gravitational waves or
neutrinos, the rate of incidence of these interaction will necessarily scale areametrically. Moreover, gravitational
waves and neutrinos only penetrate the entire volume because they are so weakly interacting. A more strongly
coupled interaction such as ultraviolet light or particulate matter will primarily affect only the ‘skin’ of the system,
and thus illustrates how such external inﬂuences are genuinely only areametric in strength.

26

√

Mixed Architectures Because of the generality of the R (cid:46)
AV scaling law, systems of
maximal computational rate can be constructed using any desired mix of architectures in order to
meet the requirements of a given problem. Providing entropy and input power can be efﬁciently
transported between the surface and the computational bulk, the entropy generation and power
constraints superpose linearly. Thus in principle it is possible to have a mix of quantum and
Brownian architectures within a single system. A caveat is that these two architectures may need
to operate at different temperatures, and a temperature gradient introduces an additional source of
entropy. Thus, the area of the boundary between these subsystems must be at most proportional
to the system’s bounding area in order that the entropy generated can be effectively countered.

This principle is more general: any non-equilibrium inhomogeneity in the system’s struc-
ture will result in entropy generation. To quantify this, we assume that such inhomogeneities
equilibrate via an uncorrelated diffusive process, and we proceed via a Fokker-Planck approach.
The Fokker-Planck equation describes the evolution of a probability distribution in space due
to stochastic dynamics. Speciﬁcally, it pertains to the inﬂuence of a Langevin force with rapidly
decaying correlations in time. In this limit, Brownian particles will exhibit a combination of
drift and diffusion depending on the properties of the system. The evolution of the probability
distribution ϕ is found [26] to obey14

˙ϕ = −∇ · [µϕ − D∇ϕ]

where µ is a drift vector and D is a diffusion matrix. We also identify the probability current
µϕ − D∇ϕ. In order to establish the rate of entropy generation, we must ﬁrst determine the
steady state distribution. We make the assumption that the steady state probability current is
everywhere zero, i.e. there are no persistent current ﬂows or vortices, and therefore ﬁnd that

∇ϕ0
ϕ0

= D−1µ .

Now we obtain an expression for the entropy of the system. As there are many particles, we can
reinterpret ϕ as a concentration of particles (as the Fokker-Planck equation does not impose any
normalisation), and so each particle will have an associated entropy of 1 + ε − log λ where λ ∝ ϕ.
This yields intensional entropy η = ϕ(1 + ε − log λ) and ˙η = ˙ϕ(ε − log λ). At equilibrium, ˙η
cannot have a leading order linear term in ϕ, and so ε = log λ0 which gives

(cid:16)

η = ϕ

1 − log

(cid:17)

ϕ
ϕ0

(cid:90)

H =

(cid:16)

dV ϕ

1 − log

(cid:90)

˙H = −

dV ˙ϕ log

(cid:17)

ϕ
ϕ0

ϕ
ϕ0

.

14In Risken [26], the convention is ˙ϕ = −∂iµiϕ + ∂i∂jDijϕ. We use a different convention here for convenience,

wherein µ(cid:48)

i = µi − ∂jDij.

27

We now substitute the Fokker-Planck equation for ˙ϕ, obtaining

(cid:90)

(cid:90)

(cid:90)

(cid:90)

(cid:90)

˙H =

=

=

=

=

dS · (µϕ − D∇ϕ) log

ϕ
ϕ0

dV (D∇ϕ − µϕ) · ∇ log

(cid:90)

−

ϕ
ϕ0

dV (µϕ − D∇ϕ) · ∇ log

ϕ
ϕ0

(cid:16) ∇ϕ
ϕ
(cid:16) ∇ϕ
ϕ

dV Dϕ

(cid:17)
− D−1µ

· ∇ log

dV Dϕ

−

(cid:16)

dV ϕ

D∇ log

∇ϕ0
ϕ0
(cid:17)

ϕ
ϕ0

(cid:17)

· ∇ log

· ∇ log

ϕ
ϕ0

ϕ
ϕ0
ϕ
ϕ0

(cid:88)

˙H =

i

(cid:68) (cid:12)
(cid:12)D1/2
(cid:12)

i ∇ log

Ni

2(cid:69)

,

ϕi
ϕi,0

(cid:12)
(cid:12)
(cid:12)

where Ni is the number of particles of diffusive species i. In the second line we have assumed
that the probability current across the system boundary vanishes and in the last line we have
generalised to multiple diffusive species.

The consequence is that a system may only sustain the use of spatial variation in its architecture

if at least one of the following three conditions holds for each such species,

1. The region of variation scales with the system’s surface area, such as if the gradient is

localised to a hemispheric plane,

2. The diffusion rate D is vanishingly small,

3. The average relative strength of the gradient, ∇ log(ϕ/ϕ0), has scaling no greater than

(cid:96)−1/2 ∼ V −1/6.

These conditions may be violated only to the extent that another condition is exploited to achieve
a commensurate reduction in the entropy rate.

Modes of Computation The quantum Zeno architecture is inherently processive, meaning
that every transition it makes is useful. This is in contrast to the Brownian architecture, in which
only a vanishingly small fraction of transitions lead to a net advance in computational state. There
are a number of consequences to this distinction that make the quantum architecture preferable.
Firstly, a quantum architecture equivalent to a given Brownian architecture will only use
NQZE. ∼
AV active computational elements compared to NBrown. ∼ V , which signiﬁcantly
reduces the risk of errors due to internal ﬂuctuation. In addition, the properties of the QZE mean
that the ﬂuctuations within the computational subvolume of the quantum architecture can be
rectiﬁed at all scales in contrast to the classical system. Of course, ﬂuctuations in the remaining
bulk of the volume are still problematic, but their rectiﬁcation is less critical.

√

Secondly, whilst the two systems would have the same net rate of computation, the computa-
tional elements of the quantum system operate at the same maximum speed regardless of scale,
AV ) and fully
and the degree of parallelism can be tuned between maximally parallel (N ∼

√

28

√

serial (N = 1). This means that the quantum architecture can operate equally well for parallel
and serial problems. One caveat is that the maximal attainable parallelism of the quantum system,
AV , is lower than for the Brownian, ∼ V . In practice this is not too problematic as parallel
∼
problems can be simulated serially, and because the total computation rate is independent of
the degree of parallelism, the quantum system experiences no penalty for this choice. For the
Brownian system, however, even a problem maximally exploiting the available parallelism will
be limited by the time for each element to perform a single net operation. In addition, whilst the
quantum system has fewer computational foci than the Brownian system, the remaining bulk can
be used for ‘cold’ computation such as data storage.

Thirdly, almost all parallel problems will involve inter-element communication and synchron-
isation. As will be discussed in the next Part [4], these processes present signiﬁcant challenges in
the Brownian architecture that in the worst case would throttle the system down to R ∼ A. This
arises because synchronisation processes map to constrictions in phase space, and Brownian dif-
fusion through such a constriction is very slow. As the quantum architecture evolves processively,
it is less subject to this effect and thus its computational elements can communicate much more
freely.

Fourthly, the quantum architecture permits quantum computation whereas the Brownian
architecture is unlikely to be capable of supporting quantum computation. The reason for this
is that the average time for each computational transition in the Brownian system is typically
large, increasing as the system size does, whereas the decoherence timescale for a quantum
state is ﬁxed and typically small. Furthermore, the decoherence timescale is likely signiﬁcantly
smaller than for the QZE architecture as the Brownian system probably operates at a higher
temperature. Altogether, these factors make it incredibly unlikely that a quantum state can be
reliably maintained through a quantum computation within such an environment.

Despite all these disadvantages, the Brownian architecture is perhaps of more interest as it is
ostensibly easier and cheaper to construct with current technology. The most exciting avenue for
this may lie in the ﬁelds of biological and molecular computing, in which molecules such as DNA
are constructed in such a way that their resulting dynamics encode a computational process [27,
28]. Chemical systems are a natural substrate for Brownian dynamics, and the manipulation of
DNA is becoming ever cheaper and more sophisticated. With improvements in synthetic biology,
we may even be able to readily prepare self-repairing DNA computers in the near future.

We ﬁnish by discussing the timestep for a net computational transition in the Brownian
architecture. This timestep is controlled by the biases, bi. To maximise the system’s computation
rate, we take bi = β ∝ 1/
If, at operating reactant concentrations, the time for any
computational step (forwards or backwards) is t0, then the net timestep is given by

√

(cid:96).

τ =

t0
β

∝ (cid:96)1/2 ∼ V 1/6

which can be seen to get larger as the system gets larger. Fortunately this scaling is sublinear,
and so depending on the available power, the achievable bias may be signiﬁcant. For example,
assuming a 500 W m−2 radiative capacity and a 1 m radius system consisting of fairly conservative
computational particles of size (10 nm)3 operating at a gross rate of 1 Hz, a bias as high as β ∼ 0.4
is possible.

29

Another approach to managing modes of computation in a Brownian computer is to institute a
hierarchy of bias levels at different concentrations. For example, a viable system may consist of
the following tuples ∼ (N, b; RC),

{(V, (cid:96)−1/2; V 5/6), (V 14/15, (cid:96)−2/5; V 4/5), (V 5/6, (cid:96)−1/4; V 3/4), (V 2/3, 1; V 2/3)}

and computations requiring faster steps may use higher biases at the expense of less computational
capacity. Additionally, the highest bias level admits irreversible computation, and so we may
enclose our computer with a thin irreversible shell, whilst the internal bulk consists of reversible
computations at various bias levels. Going further, we could have a hot inner Brownian core, a
cold outer quantum core, and an enveloping irreversible silicon shell. Additionally, a hierarchical
Brownian system need not employ spatial variation; by using different species for each bias, the
subsystems can be mixed homogeneously.

8. Conclusion

√

We have shown that, subject to reasonable geometric constraints on power delivery and heat
dissipation, the rate of computation of a given convex region is subject to a universal bound,
scaling as RC (cid:46)
AV ∼ V 5/6, and that this can only be achieved with reversible computation.
For irreversible computation, this bound falls to A ∼ V 2/3. The scaling laws persist at all
practical scales, only breaking down when the system size approaches the Schwarzschild scale.
For typical densities (∼ 1000 kg m−3), this threshold scale is 4 × 1011 m ≈ 2.7 AU. Beyond this
scale, the maximum rate is attained by localising mass into as thin a shell as possible, reminiscent
of megastructures such as Dyson spheres from the world of science ﬁction. At very small scales,
the rate scales with V as the surface area to volume ratio is negligible. The scaling then falls to
V 5/6, to V 1/2, and ﬁnally to V 1/3 as the system increases in size. At the Schwarzschild regime
and beyond, the computation rate is suppressed by a factor of order unity due to gravitational
time dilation.

This analysis has assumed that each computational element acts independently. In our next
two papers we shall investigate the constraints affecting cooperative reversible architectures, in
particular the thermodynamic cost of synchronisation processes, such as communication and
resource sharing. These costs turn out to be quite signiﬁcant, even prohibitive.

A. Acknowledgements

The author would like to acknowledge the invaluable help and support of his supervisor, Gos
Micklem. This work was supported by the Engineering and Physical Sciences Research Council,
project reference 1781682.

B. Reversible Computing

Irreversible computers are characterised by their non-conservation of information, often in
subtle ways. This may be considered to ultimately originate in early mathematical models

30

of computations such as the Turing Machine and Lambda Calculus in which overwriting and
discarding, respectively, are implicit to the primitive operations of the models. This implicitness
is pervasive across practically all levels of abstraction. At the lowest levels, this manifests
in such forms as transistor logic, non-injective logic gates such as AND, the free overwriting
of memory locations and registers, and jumping to other instructions without a trace of the
instruction pointer’s origin. Semiconductor transistors and logic gates are intrinsically non-
invertible because the state(s)—conductive or not—are properties of the semiconductor regions
themselves; therefore, each time the semiconductor state changes in response to its input, it must
ﬁrst forget its previous state. At higher levels it is even less obvious, from variables going out
of scope, to ignoring the return value of a function, to iteration and recursion without tracking
the full history of such processes, to pure functional languages encouraging rapid turnover of
memory through an underlying garbage collector.

With such ubiquitous application of information-destructive primitives, it is hard to see how
algorithms can be rewritten reversibly. In an upcoming paper, we shall introduce a model of
reversible computation appropriate for the Brownian architecture described earlier, as well as give
an overview of other extant reversible programming languages such as Janus [29], Ψ-Lisp [30] and
Theseus [31]. Our Brownian language is high level, demonstrating that reversible programming
need not be overtly difﬁcult. Adopting this paradigm requires a shift in thinking, in the same way
as moving between imperative and functional styles of programming does.

To program reversibly requires some care. In particular, it is not sufﬁcient for it to be possible
in principle to reconstruct the computational history; instead each computational primitive must
be intrinsically invertible such that the computational history can be readily rewound step-by-
step as easily as it was run forward. Consider a branch in control ﬂow due to a conditional
statement; it is essential that the information used to switch between the branches be retained,
and furthermore that immediately following the branch this information is sufﬁciently accessible
for us to immediately step backwards, reversing the conditional and merging the control ﬂow.
This prohibits, for example, branching on the result of a transient return value from a function;
this value must be retained.

After branching the control ﬂow, one often wishes to merge control ﬂow again. It is imperative
that after such an operation it is explicitly known which branch was taken, typically in the form
of a variable value. In fact, the resolution to this problem showcases the elegant symmetry of
this paradigm: the merging of control ﬂow is simply the inverse of a branch, so one simply takes
a condition that distinguishes the two branches, and plugs it in in reverse. Going further, we
can introduce reversible iteration; a reversible loop has two branch points: an entry point where
control ﬂow either enters the loop or continues to the next iteration, and an exit point where
control ﬂow either continues or exits the loop. The conditions are then, respectively, whether or
not this is the ﬁrst iteration and whether or not this is the last iteration.

More concrete operations, such as an arithmetic primitive for addition, must also be rendered
reversible. Clearly the operation (x, y) (cid:55)→ (x + y) is not invertible as, given 6, it is unclear
whether the addends were (2, 4) or (1, 5). In such cases, one often ﬁnds that there are multiple
ways to render such irreversible primitives reversible. Here, two possible implementations are
(x, y) (cid:55)→ (x, x + y) and (x, y) (cid:55)→ (x − y, x + y).

When rewriting a program in this way, one often ﬁnds that the ‘additional’ information thus

31

obtained can be recycled elsewhere, reducing the need for recalculation or copying of values,
though such parsimonious algorithms are not always easy to identify. Fortunately if it is desired to
discard a value, this does remain possible; one can reversibly discard it by using one of Bennett’s
algorithms [12, 32], or one can introduce an explicit discard primitive to send it to an entropy
dump. Though the latter option may seem antithetical, its explication at least makes clear where
sources of entropy arise in the program, and thus where opportunities for optimisation may
exist. What Bennett showed, however, is that dumping entropy is never necessary. His simplest
algorithm shows how an irreversible program P can be embedded reversibly; suppose that we
keep track of the data discarded by P , then when given an input x, P maps it to the tuple (y, h)
where y is the output of P x and h is the ‘history’ data. Now, make a copy of y, y(cid:48), and set it aside.
We can now run P backwards as we have its discarded data, P −1 : (y, h) (cid:55)→ x. Therefore we are
left with (x, y(cid:48)), i.e. a reversible embedding of P , P (cid:48) : x (cid:55)→ (x, P x). The discarded data h may
be quite large, but Bennett developed more sophisticated algorithms which limit the intermediate
history data recorded to O(log t) where t is the program runtime.

References

[1] Michael Patrick Frank. ‘Reversibility for efﬁcient computing’. PhD thesis. Massachusetts Institute of Techno-

logy, Dept. of Electrical Engineering and Computer Science, 1999.

[2] G E Moore. ‘Cramming more components onto integrated circuits, Reprinted from Electronics, volume 38,
number 8, April 19, 1965, pp.114 ff’. In: IEEE Solid-State Circuits Society Newsletter 11.3 (2006), pp. 33–35.
ISSN: 1098-4232.

[3]

Seth Lloyd. ‘Ultimate physical limits to computation’. In: Nature 406.6799 (2000), pp. 1047–1054.

[4] Hannah Earley. ‘Engines of Parsimony: Part II. Performance Trade-offs for Communicating Reversible

Computers’. In: (2020). arXiv: 2011.04054 [cond-mat.stat-mech].

[5] Hannah Earley. ‘Engines of Parsimony: Part III. Performance Trade-offs for Reversible Computers Sharing

Resources’. 2020. arXiv: 2012.05655 [cond-mat.stat-mech].

[6] Hans J Bremermann. ‘Optimization through evolution and recombination’. In: Self-organizing systems 93

(1962), p. 106.

[7] Norman Margolus and Lev B Levitin. ‘The maximum speed of dynamical evolution’. In: Physica D 120.1

(Sept. 1998), pp. 188–195.

[8] Hugh Everett III et al. The Many-Worlds Interpretation of Quantum Mechanics. Princeton University Press,

[9]

1973.
Leo Szilard. ‘ ¨Uber die Entropieverminderung in einem thermodynamischen System bei Eingriffen intelligenter
Wesen’. In: Zeitschrift f¨ur Physik 53.11-12 (1929), pp. 840–856. English translation: Leo Szilard. ‘On the
decrease of entropy in a thermodynamic system by the intervention of intelligent beings’. In: Behavioral
Science 9.4 (1964), pp. 301–310.

[10] Rolf Landauer. ‘Irreversibility and heat generation in the computing process’. In: IBM J. Res. Dev. 5.3 (1961),

pp. 183–191.

[11]

Joan A Vaccaro and Stephen M Barnett. ‘Information erasure without an energy cost’. In: Proceedings of the
Royal Society A: Mathematical, Physical and Engineering Sciences 467.2130 (2011), pp. 1770–1778.

[12] Charles H Bennett. ‘Logical Reversibility of Computation’. In: IBM J. Res. Dev. 17.6 (Nov. 1973), pp. 525–

532.

[13] Alan Mathison Turing. ‘On computable numbers, with an application to the Entscheidungsproblem’. In: J. of

Math 58.345-363 (1936), p. 5.

32

[14] Alonzo Church. ‘A set of postulates for the foundation of logic’. In: Annals of mathematics (1932), pp. 346–

366.

[15] Edward Fredkin and Tommaso Toffoli. ‘Conservative Logic’. en. In: Collision-Based Computing. Ed. by

Andrew Adamatzky. Springer London, 1981, pp. 47–81.

[16] Andrew Lewis Ressler. ‘The design of a conservative logic computer and a graphical editor simulator’.

PhD thesis. Massachusetts Institute of Technology, 1981.

[17] Charles H Bennett. ‘The thermodynamics of computation—a review’. en. In: Int. J. Theor. Phys. 21.12 (Dec.

1982), pp. 905–940.

[18] Baidyanath Misra and EC George Sudarshan. ‘The Zeno’s paradox in quantum theory’. In: Journal of

Mathematical Physics 18.4 (1977), pp. 756–763.

[19] Lev B Levitin and Tommaso Toffoli. ‘Thermodynamic cost of reversible computing’. In: Physical review

letters 99.11 (2007), p. 110502.

[20]

Subhash S Pidaparthi and Craig S Lent. ‘Energy dissipation during two-state switching for quantum-dot
cellular automata’. In: Journal of Applied Physics (2020).

[21]

L Landau. ‘‘Zur Theorie der Energie¨ubertragung bei St¨ossen II’. In: Phys. Z. Sowjetunion 2 (1932), pp. 46–51.

[22] Clarence Zener. ‘Non-adiabatic crossing of energy levels’. In: Proceedings of the Royal Society of London.
Series A, Containing Papers of a Mathematical and Physical Character 137.833 (1932), pp. 696–702.

[23]

J Robert Oppenheimer and George M Volkoff. ‘On massive neutron cores’. In: Physical Review 55.4 (1939),
p. 374.

[24] Hans A Buchdahl. ‘General relativistic ﬂuid spheres’. In: Physical Review 116.4 (1959), p. 1027.

[25]

S Chandrasekhar. ‘Dynamical instability of gaseous masses approaching the Schwarzschild limit in general
relativity’. In: Physical Review Letters 12.4 (1964), p. 114.

[26] Hannes Risken. Fokker-planck equation. Springer, 1996.

[27] Erik Winfree. Simulations of Computing by Self-Assembly. Tech. rep. California Institute of Technology,

Department of Computer Science, 1998.

[28] Luca Cardelli. ‘Two-Domain DNA Strand Displacement’. In: (June 2010). arXiv: 1006.2993 [cs.LO].

[29] Christopher Lutz and Howard Derby. ‘Janus: a time-reversible language’. In: Caltech class project (1982).

[30] Henry G Baker. ‘NREVERSAL of fortune—the thermodynamics of garbage collection’. In: Memory manage-

ment. Springer, 1992, pp. 507–524.

[31] R P James and A Sabry. Theseus: a high level language for reversible computing, work-in-progress report at

RC (2014). 2014.

[32] Charles H Bennett. ‘Time/space trade-offs for reversible computation’. In: SIAM Journal on Computing 18.4

(1989), pp. 766–776.

33

