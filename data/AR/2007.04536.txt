Attention-based Residual Speech Portrait Model for Speech to Face
Generation

Jianrong Wang1, Xiaosheng Hu1,Li Liu2,∗,Wei Liu1,Mei Yu1,Tianyi Xu1

1College of Intelligence and Computing, Tianjin University, Tianjin 300350, China
2Shenzhen Research Institute of Big Data, the Chinese University of Hong Kong, Shenzhen,
Guangdong
wjr@tju.edu.cn, xiaosheng hu@163.com, liuli@cuhk.edu.cn, 602140606@qq.com,
yueyouyueyou@gmail.com, tianyi.xu@tju.edu.cn

0
2
0
2

l
u
J

9

]

V
C
.
s
c
[

1
v
6
3
5
4
0
.
7
0
0
2
:
v
i
X
r
a

Abstract

Given a speakers speech, it is interesting to see if it is possible
to generate this speakers face. One main challenge in this task
is to alleviate the natural mismatch between face and speech.
To this end, in this paper, we propose a novel Attention-based
Residual Speech Portrait Model (AR-SPM) by introducing the
ideal of the residual into a hybrid encoder-decoder architecture,
where face prior features are merged with the output of speech
encoder to form the ﬁnal face feature. In particular, we inno-
vatively establish a tri-item loss function, which is a weighted
linear combination of the L2-norm, L1-norm and negative co-
sine loss, to train our model by comparing the ﬁnal face feature
and true face feature. Evaluation on AVSpeech dataset shows
that our proposed model accelerates the convergence of train-
ing, outperforms the state-of-the-art in terms of quality of the
generated face, and achieves superior recognition accuracy of
gender and age compared with the ground truth.
Index Terms: Speech to face generation, Face prior, Attention
mechanism, Encoder-decoder, Residual

1. Introduction

Recently, Audio-Visual Cross-Modal Learning becomes more
and more popular [1], and one of the most interesting topics
is to generate face appearances of speakers according to their
audio speech. In fact, the research on “speech to portrait” has
great impacts on our daily life, especially in community secu-
rity. For example, in the real scene of identifying suspects [2],
when there are no images or appearance features but only au-
dio samples of the suspects, police can use the technology of
“speech to portrait” to generate the face images, which are sim-
ilar to suspects’ real faces.

In order to explore the feasibility of generating faces from
speech, some previous studies have shown that a persons voice
is strongly related to his or her facial structures [3, 4]. For ex-
ample, it was shown in [5, 6] that facial bone, joint structures
and the tissues covering them are closely related to the shape
and size of the organs that produce sound. Meanwhile, genetic
factors, biological factors and environmental factors, especially
gender [7, 8], age [9, 10, 11] and ethnicity, can largely inﬂuence
one’s voice and face.

Recently, some research works on speech to face genera-
tion using deep learning methods have emerged [12, 13], among
which Speech2Face [13] is the state-of-the-art (SOTA). It di-
rectly generates a speech by Convolutional Neural Network
(CNN) [14] that learns facial features [15], and then uses this
feature to generate face [16]. This method obtains good re-
sults, but the CNN structure of speech encoder is complicated,

and conventional CNNs may suffer from the gradient vanish-
ing problem. More importantly, due to the natural mismatch
between speech and face [17], there is still room to reduce the
mismatch and improve the performance.

To overcome the shortcomings of previous works and fur-
ther alleviate the mismatch between speech and face, in this
work, we propose a novel Attention-based Residual Speech Por-
trait Model (AR-SPM) within an encoder-decoder architecture
by introducing prior face features, which guides the speech fea-
ture to approach to the original face feature as much as possible.
Besides, Convolutional Block Attention Module (CBAM) [18]
is incorporated into the encoder to extract the key information
of the speech and makes the decoder only focus on the face fea-
ture, ignoring the background noise. Particularly, we propose
a tri-item loss function for encoder, which contains the linear
combination of the L2-norm, L1-norm and the negative cosine
loss to take into account both errors in values and directions.
The decoder loss function is constructed by replacing the co-
sine loss with the Structural Similarity Index (MS-SSIM) [19].
The qualitative and quantitative evaluation on AVSpeech dataset
[20] show the superior performance of our proposed method,
which outperforms the SOTA results [13]. An overview of our
proposed AR-SPM is shown in Figure 1.

Overall, our contributions can be summarized as follows:

• We propose a novel AR-SPM based on an encoder-
decoder architecture. A residual strategy is introduced
by incorporating a prior face feature to make the net-
work capture the most representative face features and
improve the learning effect of speech portrait model.
• We improve the network structure by adding a spatial-
channel attention mechanism and constructing a sym-
metrical face decoder network. Moreover, we innova-
tively establish a tri-item loss function, which contains
L2-norm, L1-norm and negative cosine loss to acceler-
ate the convergence of training.

• Experimental results on AVSpeech dataset [20] show
that our model achieves the SOTA results and signiﬁ-
cantly reduces the cos similarity degree from 40.66◦ to
15.20◦.

2. Related Work

In the literature, three classical methods are used to construct
a mapping between speech and face. The ﬁrst method used
some attributes classiﬁers to predict some ﬁxed attributes from
speakers voice such as gender, age [11] and nationality. Then
it searched the most matching face images automatically based
on the prediction results from the local face images database.

 
 
 
 
 
 
Lastly, it merged the selected face images using the method
in [21]. However, this speech portrait method has some lim-
its in the accuracy of the attributes classiﬁers since it needs to
be trained in a supervised manner and thus limits the correlation
between face and speech within a group of attributes.

The second method that realized a speech portrait model
is based on a direct regression using a Deep Neural Network
(DNN) [22]. Speciﬁcally, the DNN structure is used to con-
struct the mapping between speech and ﬁxed-resolution RGB
face image [23, 24, 25, 26]. However, this method is hard to re-
alize because such a model needs speakers original face image
as a training label. As a result, the model is easily suscepti-
ble to uncontrollable factors in the face image, such as facial
expression, head posture, object occlusion, lighting conditions
and background, which will greatly affect the regression effect
of the model. In addition, the model must learn autonomously
and parse out many complex nonlinear transformations [13].

The third method is an improvement of the second method
by splitting the networks into two parts. The encoder inde-
pendently learns to extract face-related information from the
speech, and the decoder independently learns to restore a stan-
dard resolution face image from face feature as in [13]. Such a
model is trained in a self-supervised way, by utilizing the pair of
face and speech extracted from videos. The description formula
of the network is :

f = F D(SE(s)),

(1)

where s is the speech and f is face image. SE means speech
encoder network and FD means face decoder network. Even
this method [13] is creative and remarkable, it still needs further
improvement. Due to the natural mismatch between speech and
face [17], there is still room to improve the performance and
reduce the mismatch.

To this end, in this work, we propose a novel AR-SPM to
further optimize the SE structure and propose to incorporate a
prior face feature to complement the speech feature.

3. Method

In the following, we describe the details of our proposed AR-
SPM (see Figure 1), which includes the prior face features, the
CBAM and two new loss functions as well as the structure for
SE and FD network, respectively.

3.1. Prior face feature

By introducing the prior face feature, we exploit the idea of the
residual to remove the main similar part of the face (i.e., prior
face feature), thereby highlighting the small changes depicted
by the speech feature. Our AR-SPM converts the speech to face
images by a network φ, which is deﬁned as follows.

φ(s) = F D (SE(s) + fprior) ,

(2)

where φ(s) is the generated face, s means the spectrogram of
input speech, and fprior is the prior face feature, which is calcu-
lated before the training stage. Finally, the merged face feature
is upsampled into RGB face image by FD. Two ways are inves-
tigated to obtain the ﬁnal face feature. The ﬁrst is the sum of
prior face feature and speech feature output from SE, and the
second one is ﬁrst feeding the sum of the output feature of SE
and prior face feature to a fully connected (f c) layer, and then
taking its output as the ﬁnal face feature. Recalled that the goal
of SE is to learn speech features that mimic facial features. Mo-
tivated by [27], SE is designed to learn the residual, which is the

difference between ﬁnal face feature and the face prior, instead
of learning speech face feature directly.

A well-deﬁned prior face feature can be approximate to
speaker’s face feature in many dimensions (e.g., eye contour
and lip contour), and contains the main similar part of face, we
thus take advantage of adding the prior facial features in the SE
to reduce the training difﬁculties and learn more representative
facial features.

Two face priors are investigated in this work. The ﬁrst one
is neutral face prior, which is the arithmetic mean of a large
gender-balance face image dataset:

fprior =

1
n

n
(cid:88)

i=1

VGGFace(f )

(3)

where f denotes the face image and n is the number of face
images. VGGFace denotes the CNN structure in [2], which is
exploited to extract face features from face images by taking n
= 10, 50, 100, 500, 1000, 5000 and 10000. We experimentally
ﬁnd that when n gradually increases, the neutral prior feature
tends to converge, indicating that the calculated prior face fea-
ture is more representative, as shown in Figure 2 and Table 1.
In this work, we ﬁnally take n equals 10000.

The second prior face feature is a gender-dependent prior
by assigning two prior face features to male and female, respec-
tively. To achieve this, a robust classiﬁer network is ﬁrst needed
to predict the gender based on the audio speech. We establish a
lightweight CNN network, which contains ﬁve convolution lay-
ers, three max pooling layers and two fully connected layers to
predict the gender of speakers. It is trained and tested on the
VGGFace dataset, and the gender accuracy reache 97.01% in
test dataset.

Based on the classiﬁcation results, we calculate the male
and female prior face features using the same method as the
neutral prior face feature. The gender prior features tend to con-
verge as well when the number of face images increases (see
Table 1).

It should be noted that these two prior face features are un-
biased on the age due to the AVSpeech dataset contains candi-
dates aged from 20 to 50 years old. Besides, in this work, we
do not take into account some potential attributes such as skin
color and face shape, which do not affect the speaker’s portrait.

3.2. Convolutional Block Attention Module

To make our proposed AR-SPM owes an ability to focus on im-
portant features and ignore the irrelevant information, we utilize
a lightweight general-purpose module CBAM [18], which can
be easily integrated into an end-to-end CNN architecture.

CBAM contains a channel and a spatial attention, which are
embed into both encoder and decoder. Concerning the channel
attention module, each channel of a feature represents a special-
ized detector, so that it can focus on what are important features,
while the spatial attention module is used to determine where
are useful features. The channel attention and spatial attention
can be combined together in a parallel or tandem approach. We
experimentally ﬁnd that “ﬁrst channel attention and then spatial
attention in a tandem manner achieves the best result. Besides,
we also tried channel-focused attention mechanism [28], and it
turns out that CBAM performs better.

As shown in Figure 1, we apply CBAM in both the SE and
FD, so that SE obtain a better feature representation, and FD fo-
cus on the feature information of the face, instead of the back-
ground noise.

Figure 1: The pipeline of our AR-SPM with the neutral prior face.

n1
50
100
500
1000
5000
10000

n2
10
50
100
500
1000
5000

Neutral L1(n1, n2) male L1(n1, n2)

27.861
12.155
10.179
3.815
3.176
1.179

43.908
24.302
14.937
5.059
4.254
1.967

female L1(n1, n2)
36.924
16.318
15.256
5.797
4.505
1.913

Table 1: L1 distance between VGGFace face features with different sample face image number n. In particular, we calculated them in
cases of neutral, male, and female.

Figure 2: The face images restored from neutral prior face-
feature by Face Decoder Network when n=10, 50, 100, 500,
1000, 5000, 10000.

Layer
Fc1
Fc2
ConvTrans1
ConvTrans2
ConvTrans3
ConvTrans4
ConvTrans5
ConvTrans6
ConvTrans7
ConvTrans8
ConvTrans9
ConvTrans10
ConvTrans11
Conv1

Kernel Stride Padding Out Padding Out Channels

1
1
5
5
5
5
5
5
5
5
5
5
5
1

-
-
2
1
1
1
1
2
1
1
2
1
2
1

-
-
2
2
2
2
2
2
2
2
2
2
2
0

-
-
1
0
0
0
0
1
0
0
1
0
1
-

1000
25088
512
512
512
512
512
256
256
256
64
64
32
3

Table 2: The structure of FD network.

3.3. Encoder-decoder structure

In this work, we redesigned the SE network as shown in Table
3, where the Fc2 layer is a fusion layer for speech feature and
prior face feature. The CBAM [18] is added to the CNN struc-
ture. We have also tried the general deep learning structures like
Resnets [27] or Densenets [29] as the SE network, but through
experiments, we found that these kind of networks are not ef-
fective enough as the role of extract face information in speech
portrait model. We hypothesize that Resnets and Densnets are
skip-connected networks, which may introduce noise or silenc-
ing fragments, causing longer training steps to extract effective
speech feature information.

As for the FD network, we exploit the structure of trans-
posed convolutions in [30] and design a network that is sym-
metrical to the VGGFace model. The structure of FD network
is shown in Table 2. To reduce the number of hyperparameters,
instead of using a f c layers (4096 × 25088), we ﬁrst use two
f c layers (4096 × 1000 and 1000 × 25088) to resize the in-
put feature as a 7 × 7 × 512 feature map that is the same as
the last feature map of VGGFace model. Then we use our de-
signed transposed convolutions to upsample the feature map as
a 224 × 224 × 64 feature map. Lastly, a 1 × 1 convolution is
used to convert the feature map to a size 224 × 224 × 3.

3.4. Proposed loss functions

In the following, we introduce our proposed loss functions of
FD and SE, respectively.

3.4.1. Face Decoder loss function

Concerning the loss function of FD networks, we modify the
join loss function, which is similar to the loss function in [30].

generateGenerated Face ImageAttention based DecoderVGG-FacetrainFaceVideoReal Face featureSpeech featureFinal Face featureSpectrogramAttention based EncoderPrior Face feature neutral prior face Figure 3: Overview of FD networks.

As shown in Figure 3, our FD loss function is composed of
an image loss, which is the error between generated image
and original image, and a face feature loss, which is the error
between face features of generated image and original image.
Such a joint loss function can not only penalize the pixel error
between image directly, but also the different of abstract iden-
tity. Speciﬁcally, we adopt the mixed loss of MS-SSIM and (cid:96)1
loss [31] as the image loss:

Limage = α · LMS−SSIM + (1 − α) · G · L(cid:96)1 ,

(4)

where α = 0.84 and G is the parameters of Gaussian distri-
bution in MS-SSIM. It considers the inﬂuence of resolution, so
that it can retain high-frequency information (e.g., image edges
or details) but tend to cause brightness changes and color devi-
ations. In order to tackle this problem, we exploit (cid:96)1 function
to keep brightness and color unchanged. Combining them to-
gether can produce a good penalization for image generation
task.

In order to pay more attention to face contour in the image
rather than the value of the pixel itself, we introduce the cosine
similarity loss, which is generally used to measure the differ-
ence of two embeddings in the feature space:

LCS(A, B) = 1 −

= 1 −

A · B
(cid:107)A(cid:107)(cid:107)B(cid:107)
(cid:80)n

(cid:112)(cid:80)n

i=1 Ai × Bi
i × (cid:112)(cid:80)n

i=1 A2

i=1 B2
i

Finally, the total loss function of the FD is:

LT otal = Limage + LCS.

.

(5)

(6)

3.4.2. Speech Encoder loss function

A tri-item loss function is proposed for SE network. The ﬁrst
item is (cid:96)2 loss between the unitized Sf (i.e., the output speech
feature of SE network) and unitized Ff (i.e., the real speak-
ers face feature obtained by VGGFace). The second item is
(cid:96)1 loss between the output of the FD network’s ﬁrst f c layer
(DF c1: R4096 → R1000), which penalizes the difference of
hidden layer features. The third item is Cosine Similarity
Loss between the output of the VGGFace third f c layer (LCS:
R4096 → R2622), which penalizes the difference of identity fea-
ture. We have also tried the knowledge distillation loss in [13],
and it shows inferior performance to the Cosine Similarity Loss.
Finally, the tri-item loss function is:

Ltotal = λ1

(cid:13)
(cid:13)
(cid:13)
(cid:13)

Ff
(cid:107)Ff (cid:107)

−

Sf
(cid:107)Sf (cid:107)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

2

2

+ λ2 (cid:107)DF c1 (Ff ) − DF c1 (Sf )(cid:107)1
+ λ3LCS (VF c3 (Ff ) , VF c3 (Sf )) ,

(7)

where λ1, λ2, λ3 (λ1 = 1, λ2 = 0.04, λ3 = 1.2) are bal-
anced coefﬁcients which make the gradient of loss items within
a similar scale.

Since FD takes part in the calculation of loss function in SE,
FD will be trained before SE, and the parameters of FD will be
frozen during the training of SE.

4. Experiment

4.1. Datasets and Implementation Details

The AVSpeech dataset [20] is used to evaluate our model, which
is a large-scale speaking video dataset from YouTube. Besides,
we use VGGFace dataset to train our gender prediction network
to classify the gender, and 0.15 million videos are used as train-
ing set and 8 thousand videos as test set. First, we crop the
face image to size 224 × 224 from AVSpeech training set to
train the FD network, whose structure and implementation de-
tails are shown in Table 2. Then, we clip and rescale the audio
from each AVSpeech video to 6s by concatenating if an audio
length is shorter than 6s or cutting if an audio length is longer
than 6s. The audio waveform is resampled at 16 kHz and only
a single channel is used. Spectrograms are computed by the
speaker-independent audio-visual model according to [20].

We use the spectrogram and face-image pairs to train the SE
network, whose structure and implementation details are shown
in Table 3. Our model is implemented by PyTorch 1.1.0 and
optimized by Adam [32] with β1 = 0.5, (cid:15) = 10−4, the learning
rate of 0.001 with the exponentially decay rate of 0.9 at every
2 epochs. We ﬁnally train our model with 50 epochs, and the
batch size is 16.

4.2. Evaluation Metrics

The face image generated by speech portrait model may be in-
terfered with variable face pose, expression, background or non-
critical details. Therefore, a series of pixel level evaluation in-
dex is not applicable. In this work, we adopt a part of evaluation
metrics commonly employed in SOTA work [13].

More precisely, we not directly compared the similarity of
generated face images with original face images, but compare
the similarity of face features extracted from them, because face
feature has good robustness on expression, head posture light
conditions and background [30]. Besides, we compared the
speech features from SE network and the corresponding face
features from the ground truth face image, using the L1 loss,
L2 loss and Cosine Similarity (Cos). When L1 and L2 tends to
0 or Cos tends to 1, the difference between two images tends
to be small or two features are similar in the direction. We also
measure the similarity between the feature of the face image
generated by the AR-SPM and the corresponding face features

generateVGG-FaceReal ImageGenerated ImageDecoderFace-feature of Real ImageImage LossVGG-FaceFace-feature of Generated ImageFace-feature LossLayer
Conv1
MaxPool1
Conv2
MaxPool2
Conv3
Conv4
Conv5
MaxPool3
CBAM
Fc1
AvgPool1
Fc2

Kernel
7
3
5
3
3
3
3
5 × 3
-
1
1 × 1
1

Stride
2
2
2
2
1
1
1
3 × 2
-
-
-
-

Padding Out Channels

1
0
1
0
1
1
1
0
-
-
-
-

64
-
128
-
256
512
512
-
-
4096
-
4096

Table 3: The structure of SE network.

1, L(cid:48)

from ground truth images using the same three metrics denoted
as L(cid:48)
2, Cos(cid:48). Furthermore, we compare the face feature be-
tween the original face features and the features of the face gen-
erated by the original face features. For convenience, we call it
Face-to-Face in this work, which can be regarded as a bench-
mark for comparison, since the speech portrait model relies on
the face feature from original face image for guidance.

4.3. Quantitative and Qualitative Analysis

In the following, we show the quantitative and qualitative results
of the proposed FD network and the AR-SPM.

4.3.1. The Performance of Face Decoder Network

We train the FD network on face images from AVSpeech
dataset, and show qualitative results of face reconstruction in
Figure 4, we can see that the faces generated by the FD are
similar to the ground truth. Besides, since the CBAM attention
module is used in our model, the network will concentrate on
extracting face features and ignore the background, causing a
fuzzy background.

The quantitative results of FD network are shown in the sec-
ond row of Table 4, showing the face feature similarity of face
image generated by FD network using the original face image.

Figure 4: The qualitative results of FD network on the AVSpeech
dataset. The ﬁrst row shows the original face image, and the
second row shows the image generated by FD.

4.3.2. The Performance of AP-SPM

To verify the performance of AR-SPM, we train the SE net-
work on spectrogram and face-image pairs from AVSpeech
dataset. Let Modelneutral and Modelgender denote the model
with neutral prior face feature and the model with gender prior
face feature (given by the proposed automatic gender classiﬁer),
trained with data containing both male and female, respectively.
Modelmale and Modelfemale mean the models using the male
and female face prior, and training with only male and only fe-
male data, respectively. Besides, we denote prior feature mod-
els, which use a f c layer to fuse the prior face feature and output

speech feature as Modelneutral+fc and Modelgender+fc, respec-
tively.

Figure 5: The decline of the loss function, where the blue line
denotes the loss of Modelnon−prior, and the red line denotes the
loss of Modelneutral+fc.

An ablation study is carried out by comparing the sim-
ilarity between speech feature from SE and the correspond-
ing face features from the original face image by measuring
the L1, L2 distance and Cos(deg) (i.e., cosine similarity in
form of degree). Results are shown in Table 4, where we no-
tice that Modelneutral+fc achieves the best performance among
all cases, while Modelnon−prior has the worst performance.
Modelgender and Modelgender+fc have a slight improvement
compared to the result of Modelnon−prior, showing that the
use of gender face prior gains less compared with the using
of neutral face prior. When we switch to the Modelfemale and
Modelmale, we ﬁnd that they are better than Modelgender. This
is reasonable since they train female and male data separately
(i.e., data dependent) and thus make the task easier. More im-
portantly, a fair comparsion in case of the Cos(deg) shows that
our method achieves 15.20◦, outperforming the SOTA [13] re-
sults 40.66◦ by a large margin. Besides, we see the output fea-
ture of f c layer brings evident beneﬁts when combined with the
neutral prior, while it leads to a narrowly worse performance
when combined with the gender prior.

Furthermore, we show the decline of the loss function
of Modelneutral+fc and Modelnon−prior by plotting every
It shows that our
1000 steps in batch training in Figure 5.
Modelneutral+fc converges faster than Modelnon−prior, and it
is able to avoid the unstable training causing by the abnor-
mal data samples. We can see that two abnormal training loss
values are appeared at the beginning of the training stage for
Modelnon−prior, while this phenomenon does not appear in the
case of Modelneutral+fc.

The qualitative results of the ablation study are shown in
Figure 6. We take 10 groups randomly from test set for the
demonstration. We ﬁrst notice that face image generated by
the Face-to-Face is the most similar to the original face image
since it is the reference standard. More importantly, we ﬁnd
that face image generated by the Modelneutral+fc is also highly
similar to the original face image, showing superior quality to
Modelnon−prior. Besides, the Modelneutral+fc performs better
than Modelgender+fc, conﬁrming that the neutral prior is more
robust than the gender prior in this work. Recalled that by us-
ing the CBAM, our model only focus on the face and ignores
the background, leading to blur background in generated face
images.

Model
Face-to-Face

L1
-

L2 Cos(deg)
-

Modelnon−prior 162.957 3.176
Modelneutral+fc 126.524 2.470
Modelneutral
143.335 2.795
Modelgender+fc 155.008 3.023
145.192 2.831
128.641 2.501
130.861 2.542

Modelgender
Modelfemale
Modelmale

L(cid:48)
1

L(cid:48)

2 Cos(cid:48)(deg)

97.158 1.890
179.594 3.499
144.027 2.809
148.610 2.897
171.860 3.350
150.900 2.941
156.399 3.048
152.985 2.981

14.534
30.231
25.842
25.973
29.308
26.104
21.721
21.409

-
29.191
15.204
27.748
28.115
27.994
19.438
19.610

Table 4: Quantitative results of the ablation study. The ﬁrt column represents the different methods, the second, third and fourth
columns represent the L1, L2 and cosine distances between the ﬁnal face features generated by the SE network and the face features
of the original face image, the ﬁfth, sixth and seventh columns represent the L1, L2 and cosine distances between the face features
extracted from the face image generated by our model and the face features of the original image.

Model
Face-to-Face
Modelnon−prior
Modelneutral+fc
Modelfemale
Modelmale

Male
Total
Female
97.4% 92.7% 95.5%
95.6% 89.8% 93.2%
97.9% 89.9% 94.7%
87.7% 93.6% 90.1%
97.4% 68.0% 85.8%

Table 5: The accuracy of gender recognition using the face gen-
erated by the proposed AR-SPM.

Model
Face-to-Face
Modelnon−prior
Modelneutral+fc
Modelfemale
Modelmale

Elder
Total
Young Mid-age
46.7% 70.3%
70.5%
76.0%
53.6%
33.1% 57.1%
65.5%
67.6% 66.1% 51.2% 65.2%
23.5% 63.0%
55.5%
40.4% 62.9%
64.8%

70.4%
64.2%

Table 6: The accuracy of age recognition using the face gener-
ated by the proposed AR-SPM. Young means under 35 year old,
Mid-age means 35 to 65 year old, and Elder means more than
65 years old.

tral and gender prior face features) are explored according to
the gender. In addition, we re-design the encoder and decoder
by incorporating the CBAM into the SE and FD to capture the
spatial and channel relationships and suppress noise. Results
on AVSpeech dataset show that our proposed AR-SPM acceler-
ates the convergence of training and achieves the SOTA perfor-
mance. In the future, our model will be explored to eliminate
the inﬂuence of attributes such as hair and image background
on the speaker’s face reconstruction, or apply in other appli-
cation ﬁelds, such as preliminary medical image generation or
diagnosis from speaker’s speech.

6. References

[1] K. Hoover, S. Chaudhuri, C. Pantofaru, M. Slaney, and I. Sturdy,
“Putting a face to the voice: Fusing audio and visual signals across
a video to determine speakers,” 05 2017.

[2] O. M. Parkhi, A. Vedaldi, A. Zisserman et al., “Deep face recog-
nition,” in Proceedings of British Machine Vision Conference
(BMVC), vol. 1, no. 3, 2015, pp. 6–17.

[3] P. Belin, S. Fecteau, and C. Bdard, “Thinking the voice: Neu-
ral correlates of voice perception,” Trends in cognitive sciences,
vol. 8, pp. 129–35, 04 2004.

[4] M. Kamachi, H. Hill, K. Lander, and E. Vatikiotis-Bateson,

Figure 6: Face generated by the AR-SPM. The 1st row: orig-
inal face images cropped from video, the 2nd row: the Face-
to-Face result, the 3rd row: face images generated from speech
by Modelnon−prior, the 4th row: face images generated from
speech by Modelgender+fc, and the 5th row: face images gen-
erated from speech by Modelneutral+fc.

4.3.3. The performance of gender and age recognition

To further evaluate the quality of our proposed AR-SPM, we
conduct a gender and age recognition experiment based on the
generated face images by our model. Face++ API [33] is uti-
lized to evaluate the gender and age. Results are shown in
Table 5 and Table 6, respectively. Due to Modelmale and
Modelfemale obtains superior performance to Modelgender and
Modelgender+fc as shown in Table 4, we only present the recog-
nition results using Modelmale and Modelfemale. Naturally,
Face-to-Face gives the upper limit of the recognition accuracy.
Except that, we ﬁnd the Modelneutral+fc achieves the highest
accuracy in all cases. Besides, we can see that Modelmale
and Modelfemale are narrowly worse than Modelneutral+fc. It
should be mentioned that the AVSpeech dataset is not age-
balanced (about 35% for Young, 55% for Mid-age and 10% for
Elder). The lower accuracy of the elder age comes from the
limited older images in the dataset.

These results show that introducing the idea of the resid-
ual by using the neutral prior can improve the efﬁciency of the
model training and further enhance the robustness.

5. Conclusion

To alleviate the mismatch between speech and face in the
speech-based face generation, we propose a novel AR-SPM
based on an end-to-end encoder-decoder structure, which uti-
lizes the additional prior face feature to complement the speech
feature in the SE network. Two prior face features (i.e., neu-

“putting the face to the voice: Matching identity across modality.”
Current biology : CB, vol. 13, pp. 1709–14, 10 2003.

[25] S. Reed, Z. Akata, X. Yan, L. Logeswaran, B. Schiele, and H. Lee,

“Generative adversarial text to image synthesis.”

[5] P. Mermelstein, “Determination of the vocal-tract shape from
measured formant frequencies,” The Journal of the Acoustical So-
ciety of America, vol. 41, no. 5, pp. 1283–1294, 1967.

[6] H. Teager and S. Teager, “Evidence for nonlinear sound pro-
duction mechanisms in the vocal tract,” Speech Production and
Speech Modelling, vol. 55, pp. 241–261, 1990.

[7] M. Kotti and C. Kotropoulos, “Gender classiﬁcation in two emo-
tional speech databases,” in International Conference on Pattern
Recognition, 2008.

[8] M. Feld, F. Burkhardt, and C. Mller, “Automatic speaker age and
gender recognition in the car for tailoring dialog and mobile ser-
vices,” 01 2010, pp. 2834–2837.

[9] P. H. Ptacek and E. K. Sander, “Age recognition from voice,” Jour-

nal of Speech & Hearing Research, vol. 9, no. 2, p. 273.

[10] M. Bahari, M. McLaren, H. Van hamme, and D. Van Leeuwen,
“Age estimation from telephone speech using i-vectors,” 13th An-
nual Conference of the International Speech Communication As-
sociation 2012, INTERSPEECH 2012, vol. 1, pp. 506–509, 01
2012.

[11] R. Zazo, P. S. Nidadavolu, N. Chen, J. Gonzalez-Rodriguez, and
N. Dehak, “Age estimation in short speech utterances based on
lstm recurrent neural networks,” IEEE Access, vol. 6, pp. 22 524–
22 530, 2018.

[26] X. Xia, R. Togneri, F. Sohel, and D. Huang, “Auxiliary classi-
ﬁer generative adversarial network with soft labels in imbalanced
acoustic event detection,” IEEE Transactions on Multimedia, pp.
1–1.

[27] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning
for image recognition,” in Proceedings of the IEEE conference on
computer vision and pattern recognition, 2016, pp. 770–778.

[28] J. Hu, L. Shen, S. Albanie, G. Sun, and E. Wu, “Squeeze-and-

excitation networks.”

[29] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger,
“Densely connected convolutional networks,” in Proceedings of
the IEEE conference on computer vision and pattern recognition,
2017, pp. 4700–4708.

[30] F. Cole, D. Belanger, D. Krishnan, A. Sarna, I. Mosseri, and W. T.
Freeman, “Synthesizing normalized faces from facial identity fea-
tures,” in Proceedings of the IEEE Conference on Computer Vi-
sion and Pattern Recognition, 2017, pp. 3703–3712.

[31] H. Zhao, O. Gallo, I. Frosio, and J. Kautz, “Loss functions
for neural networks for image processing,” arXiv preprint, p.
arXiv:1511.08861, 2015.

[32] D. P. Kingma and J. Ba, “Adam: A method for stochastic opti-

mization,” arXiv preprint arXiv:1412.6980, 2014.

[33] MEGVII, “Face analyze api,” https://www.faceplusplus.com.cn/

[12] R. Singh, Proﬁling Humans from their Voice, 01 2019.

attributes/, 2019.

[13] T.-H. Oh, T. Dekel, C. Kim, I. Mosseri, W. T. Freeman, M. Rubin-
stein, and W. Matusik, “Speech2face: Learning the face behind a
voice,” in Proceedings of the IEEE Conference on Computer Vi-
sion and Pattern Recognition, 2019, pp. 7539–7548.

[14] A. Krizhevsky, I. Sutskever, and G. Hinton, “Imagenet classiﬁ-
cation with deep convolutional neural networks,” in International
Conference on Neural Information Processing Systems, 2012.

[15] M. Merler, N. Ratha, R. S. Feris, and J. R. Smith, “Diversity in

faces,” 2019.

[16] F. Cole, D. Belanger, D. Krishnan, A. Sarna, I. Mosseri, and W. T.
Freeman, “Synthesizing normalized faces from facial identity fea-
tures.”

[17] R. Singh, Reconstruction of the Human Persona in 3D from Voice,

and its Reverse. Berlin: Springer, 2019, pp. 325–363.

[18] S. Woo, J. Park, J.-Y. Lee, and I. So Kweon, “Cbam: Convolu-
tional block attention module,” in Proceedings of the European
Conference on Computer Vision (ECCV), 2018, pp. 3–19.

[19] H. Zhao, O. Gallo, I. Frosio, and J. Kautz, “Loss functions for
image restoration with neural networks,” IEEE Transactions on
Computational Imaging, vol. 3, no. 1, pp. 47–57.

[20] A. Ephrat, I. Mosseri, O. Lang, T. Dekel, K. Wilson, A. Hassidim,
W. T. Freeman, and M. Rubinstein, “Looking to listen at the cock-
tail party: a speaker-independent audio-visual model for speech
separation,” ACM Transactions on Graphics (TOG), vol. 37, no. 4,
pp. 112:1–112:11, 2018.

[21] A. Zhmoginov and M. Sandler,

“Inverting face embed-
dings with convolutional neural networks,” arXiv preprint, p.
arXiv:1606.04189, 2016.

[22] D. Yu and L. Deng, “Deep neural networks,” 2015.

[23] A. Duarte, F. Roldan, M. Tubau, J. Escur, S. Pascual, A. Sal-
vador, E. Mohedano, K. McGuinness, J. Torres, and X. Giro-i Ni-
eto, “Wav2pix: speech-conditioned face generation using gener-
ative adversarial networks,” in IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP), vol. 3. IEEE,
2019, pp. 8633–8637.

[24] I. J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-
Farley, S. Ozair, A. Courville, and Y. Bengio, “Generative ad-
versarial networks,” Advances in Neural Information Processing
Systems, vol. 3, pp. 2672–2680, 2014.

