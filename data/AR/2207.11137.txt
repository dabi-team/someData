2
2
0
2

p
e
S
8

]

M
E
.
n
o
c
e
[

2
v
7
3
1
1
1
.
7
0
2
2
:
v
i
X
r
a

A Conditional Linear Combination Test with Many Weak
Instruments∗

Dennis Lim†

Wenjie Wang‡

Yichong Zhang§

Abstract

We consider a linear combination of jackknife Anderson-Rubin (AR), jackknife Lagrangian
multiplier (LM), and orthogonalized jackknife LM tests for inference in IV regressions with many
weak instruments and heteroskedasticity. We choose the weights in the linear combination based
on a decision-theoretic rule that is adaptive to the identiﬁcation strength. Under both weak
and strong identiﬁcation, the proposed linear combination test controls asymptotic size and
is admissible among certain class of tests. Under strong identiﬁcation, we further show that
our linear combination test has optimal power against local alternatives. Simulations and an
empirical application to Angrist and Krueger’s (1991) dataset conﬁrm the good power properties
of our test.

Keywords: Many instruments, power, size, weak identiﬁcation

JEL codes: C12, C36, C55

1

Introduction

Various recent surveys in leading economics journals suggest that weak instruments remain impor-

tant concerns for empirical practice. For instance, I.Andrews, Stock, and Sun (2019) survey 230

instrumental variable (IV) regressions from 17 papers published in the American Economic Review

(AER). They ﬁnd that many of the ﬁrst-stage F-statistics (and non-homoskedastic generalizations)

are in a range that raises such concerns, and virtually all of these papers report at least one ﬁrst-

stage F with a value smaller than 10. Similarly, in Lee, McCrary, Moreira, and Porter’s (2022)

∗Wenjie Wang acknowledges the ﬁnancial support from Singapore Ministry of Education Tier 1 grants RG53/20
and RG104/21. Yichong Zhang acknowledges the ﬁnancial support from a Lee Kong Chian fellowship and the NSFC
under the grant No. 72133002. Any and all errors are our own.

†Singapore Management University. E-mail address: dennis.lim.2019@phdecons.smu.edu.sg.
‡Division of Economics, School of Social Sciences, Nanyang Technological University. HSS-04-65, 14 Nanyang

Drive, Singapore 637332. E-mail address: wang.wj@ntu.edu.sg.

§Singapore Management University. E-mail address: yczhang@smu.edu.sg.

1

 
 
 
 
 
 
survey of 123 AER articles involving IV regressions, 105 out of 847 speciﬁcations have ﬁrst-stage

Fs smaller than 10. Moreover, many IV applications involve a large number of instruments. For

example, in their seminal paper, Angrist and Krueger (1991) study the eﬀect of schooling on wages

by interacting three base instruments (dummies for the quarter of birth) with state and year of

birth, resulting in 180 instruments. Hansen, Hausman, and Newey (2008) show that using the 180

instruments gives tighter conﬁdence intervals than using the base instruments even after adjusting

for the eﬀect of many instruments. In addition, as pointed out by Mikusheva and Sun (2022), in

empirical papers that employ the “judge design” (e.g., see Maestas, Mullen, and Strand (2013),

Sampat and Williams (2019), and Dobbie, Goldin, and Yang (2018)), the number of instruments

(the number of judges) is typically proportional to the sample size, and the famous Fama-MacBeth

two-pass regression in empirical asset pricing (e.g., see Fama and MacBeth (1973), Shanken (1992),

and Anatolyev and Mikusheva (2022)) is equivalent to IV estimation with the number of instru-

ments proportional to the number of assets. Similarly, Belloni, Chen, Chernozhukov, and Hansen

(2012) consider an IV application involving more than one hundred instruments for the study of

the eﬀect of judicial eminent domain decisions on economic outcomes. Carrasco and Tchuente

(2015) used many instruments in the estimation of the elasticity of intertemporal substitution in

consumption. Furthermore, as pointed out by Goldsmith-Pinkham, Sorkin, and Swift (2020), the

shift-share or Bartik instrument (e.g., see Bartik (1991) and Blanchard, Katz, Hall, and Eichen-

green (1992)), which has been widely applied in many ﬁelds such as labor, public, development,

macroeconomics, international trade, and ﬁnance, can be considered as a particular way of com-

bining many instruments. For example, in the canonical setting of estimating the labor supply

elasticity, the corresponding number of instruments is equal to the number of industries, which is

also typically proportional to the sample size.

In this paper, we propose a jackknife conditional linear combination (CLC) test, which is robust

to weak identiﬁcation, many instruments, and heteroskedasticity. The proposed test also achieves

eﬃciency under strong identiﬁcation against local alternatives. The starting point of our analysis is

an observation that, under strong identiﬁcation, an orthogonalized jackknife Lagrangian multiplier

(LM) test is the uniformly most powerful test against local alternatives among the class of tests

that are invariant to sign changes and constructed based on jackknife LM and Anderson-Rubin

(AR) tests only. However, the orthogonalized LM test may not have good power under weak

identiﬁcation or against certain ﬁxed alternatives. We therefore consider a linear combination of

jackknife AR, jackknife LM, and orthogonalized LM tests. Speciﬁcally, we follow I.Andrews (2016)

and determine the linear combination weights by minimizing the maximum power loss, which can

be viewed as a maximum regret and is further calibrated based on the limit experiment of interest

and a suﬃcient statistic for the identiﬁcation strength under many instruments. We show such a

jackknife CLC test is adaptive to the identiﬁcation strength in the sense that (1) it achieves exact

asymptotic size under both weak and strong identiﬁcation, (2) it is asymptotically and conditionally

2

admissible under weak identiﬁcation among some class of tests, (3) it converges to the uniformly

most powerful test mentioned above under strong identiﬁcation against local alternatives, and (4) it

has asymptotic power equal to 1 under strong identiﬁcation against ﬁxed alternatives. Simulations

based on the limit experiment as well as calibrated data conﬁrm the good power properties of our

test. Then, we apply the new jackknife CLC test to Angrist and Krueger’s (1991) dataset with

the speciﬁcations of 180 and 1,530 instruments. We ﬁnd that in both speciﬁcations, our conﬁdence

intervals (CIs) are the shortest among those constructed by weak identiﬁcation robust tests, namely,

the jackknife AR, LM, and CLC tests, and the two-step procedure. Furthermore, our CIs are found

to be even shorter than the non-robust Wald test CIs based on the jackknife IV estimator (JIVE)

proposed by Angrist, Imbens, and Krueger (1999), which is in line with the theoretical result

that the jackknife CLC test is adaptive to the identiﬁcation strength and is eﬃcient under strong

identiﬁcation.

Relation to the literature. The contributions in the present paper relate to two strands of

literature. First, it is related to the literature on many instruments; see, for example, Kunitomo

(1980), Morimune (1983), Bekker (1994), Donald and Newey (2001), Chamberlain and Imbens

(2004), Chao and Swanson (2005), Stock and Yogo (2005a), Han and Phillips (2006), D.Andrews

and Stock (2007), Hansen et al. (2008), Newey and Windmeijer (2009), Anderson, Kunitomo, and

Matsushita (2010), Kuersteiner and Okui (2010), Anatolyev and Gospodinov (2011), Belloni, Cher-

nozhukov, and Hansen (2011), Okui (2011), Belloni et al. (2012), Carrasco (2012), Chao, Swanson,

Hausman, Newey, and Woutersen (2012), Hausman, Newey, Woutersen, Chao, and Swanson (2012),

Hansen and Kozbur (2014), Carrasco and Tchuente (2015), Wang and Kaﬀo (2016), Koles´ar (2018),

Matsushita and Otsu (2020), Sølvsten (2020), Crudu, Mellace, and S´andor (2021), and Mikusheva

and Sun (2022), among others. For implementing inferences in the context of many instruments

and heteroskedasticity, Chao et al. (2012) and Hausman et al. (2012) provide standard errors for

Wald-type inferences that are based on JIVE and a jackknifed version of the limited information

maximum likelihood (LIML) estimator and the Fuller’s (1977) estimator. These estimators are

more robust to many instruments than the commonly used two-stage least squares (TSLS) es-

timator as they are able to correct the bias due to the high dimension of IVs.

In simulations

derived from the data in Angrist and Krueger (1991), which is representative of empirical labor

studies with a many-instrument concern, Angrist and Frandsen (2022, Section IV) show that such

bias-corrected estimators outperform the TSLS that is based on the instruments selected by the

least absolute shrinkage and selection operator (LASSO) introduced in Belloni et al. (2012) or the

random forest-ﬁtted ﬁrst stage introduced in Athey, Tibshirani, and Wager (2019).

However, the Wald inference methods are not valid under weak identiﬁcation, a situation in

which the ratio of the so-called concentration parameter, a measure of the overall instrument

strength, over the square root of the number of instruments stays bounded as the sample size di-

verges to inﬁnity. In this case, even the aforementioned bias-corrected estimators are inconsistent,

3

and there is no consistent test for the structural parameter of interest (see the discussions in Section

3 of Mikusheva and Sun (2022)). For weak identiﬁcation robust inference under many instruments,

D.Andrews and Stock (2007) consider the AR test, the score test introduced in Kleibergen (2002),

and the conditional likelihood ratio test introduced in Moreira (2003). Their IV model is ho-

moskedastic and requires the number of instruments to diverge slower than the cube root of the
sample size (K3/n → 0, where K and n denote the number of instruments and the sample size,
respectively). Anatolyev and Gospodinov (2011) propose a modiﬁed AR test, which allows for the

number of instruments to be proportional to the sample size but also require homoskedastic errors.

Recently, Crudu et al. (2021) and Mikusheva and Sun (2022) propose jackknifed versions of the AR

test in a model with many instruments and heteroskedasticity. Both tests are robust toward weak

identiﬁcation, whereas Mikusheva and Sun’s (2022) jackknife AR test has better power properties

because of the usage of a cross-ﬁt variance estimator. However, the jackknife AR tests may be

ineﬃcient under strong identiﬁcation. Mikusheva and Sun (2022) also propose a new pre-test for

weak identiﬁcation under many instruments and apply it to form a two-stage testing procedure

with a Wald test based on the JIVE introduced in Angrist et al. (1999). The JIVE-Wald test

is more eﬃcient than the jackknife AR under strong identiﬁcation. An empirical researcher can

therefore employ the jackknife AR if the pre-test suggests weak identiﬁcation or the JIVE-Wald

if the pre-test suggests strong identiﬁcation. Furthermore, Matsushita and Otsu (2020) propose a

jackknife LM test, which is also robust to weak identiﬁcation, many instruments, and heteroskedas-

tic errors. Under strong identiﬁcation and local alternatives, our jackknife CLC test proves to be

more eﬃcient than the jackknife AR, the jackknife LM, and the two-step test.

Second, our paper is related to the literature on weak identiﬁcation under the framework of

a ﬁxed number of instruments or moment conditions, in which various robust inference methods

are available for non-homoskedastic errors; see, for example, Stock and Wright (2000), Kleiber-

gen (2005), D.Andrews and Cheng (2012), I.Andrews (2016), I.Andrews and Mikusheva (2016),

I.Andrews (2018), Moreira and Moreira (2019), D.Andrews and Guggenberger (2019), and Lee

et al. (2022). In particular, our jackknife CLC test extends I.Andrews (2016) to the framework

with many weak instruments.

I.Andrews (2016) considers the convex combination between the

generalized AR statistic (S statistic) introduced by Stock and Wright (2000) and the score statistic

(K statistic) introduced by Kleibergen (2005). We ﬁnd that under many weak instruments, the

orthogonalized jackknife LM statistic plays a role similar to the K statistic. However, the trade-oﬀ

between the jackknife AR and orthogonalized LM statistics turns out to be rather diﬀerent from

that between the S and K statistics. As pointed out by I.Andrews (2016), in the case with a ﬁxed

number of weak instruments (or moment conditions), the K statistic picks out a particular (ran-

dom) direction corresponding to the span of a conditioning statistic that measures the identiﬁcation

strength and restricts attention to deviations from the null along this speciﬁc direction. In contrast

to the K statistic, the S statistic treats all deviations from the null equally. Therefore, the trade-oﬀ

4

between the K and S statistics is mainly from the diﬀerence in attention to deviation directions.

We ﬁnd that with many weak instruments, the jackknife AR and orthogonalized LM tests do not

have such diﬀerence in deviation directions. Instead, their trade-oﬀ is mostly between local and

non-local alternatives. Furthermore, although the standard LM test (without orthogonalization) is

not weak identiﬁcation robust under I.Andrews (2016)’s framework, the jackknife LM test is robust

under many instruments. Therefore, we consider a linear combination of jackknife AR, jackknife

LM, and orthogonalized jackknife LM tests, and we ﬁnd that the resulting CLC test has good

power properties in a variety of scenarios.

Notation. We denote Z(µ) as the normal random variable with unit variance and expectation µ

1 + a2(ρZ1 + (1 − ρ2)1/2Z2) + (1 − a1 − a2)Z 2

and [n] = {1, 2, · · · , n}. We further simplify Z(0) as Z, which is just a standard normal random vari-
able. We denote zα as the (1 − α) quantile of a standard normal random variable and Cα(a1, a2; ρ)
as the (1 − α) quantile of random variable a1Z 2
2 where
Z1 and Z2 are two independent standard normal random variables. Furthermore, we let Cα = z2
α/2
and Cα,max(ρ) = sup(a1,a2)∈A0
Cα(a1, a2; ρ), where A0 = {(a1, a2) ∈ [0, 1] × [0, 1], a1 + a2 ≤ a} for
some a < 1. The operators E∗ and P∗ are expectation and probability taken conditionally on data,
respectively. For example, E∗1{Z 2(ˆµ) ≥ Cα}, in which ˆµ is some estimator of the expectation µ
based on data, means the expectation is taken over the normal random variable by treating ˆµ as
deterministic. We use (cid:32) to denote convergence in distribution and U d= V to denote that U and
V share the same distribution.

2 Setup and Limit Problems

We consider the linear IV regression with a scalar outcome Yi, a scalar endogenous variable Xi,
and a K × 1 vector of instruments Zi such that

Yi = Xiβ + ei, Xi = Πi + Vi,

∀i ∈ [n],

(2.1)

where Πi = E(Xi|Zi). We focus on the model with a single endogenous variable, which is prevalent
in empirical research. We let K diverge with sample size n, allowing for the case that K is of the

same order of magnitude as n. For the rest of the paper, we follow the many-instrument literature
and treat {Zi}i∈[n] as ﬁxed so that Πi can also be written as EXi¨ı¼Œwhich is non-random, EVi = 0
by construction, and Eei = 0 by IV exogeneity. We allow (ei, Vi) to be heteroskedastic across i.
Also, following the literature on many instruments, we assume without loss of generality that there
are no controls included in our model as they can be partialled out from (Yi, Xi, Zi).1

We are interested in testing β = β0. Let ei(β0) = Yi − Xiβ0 = ei + Xi∆, where ∆ = β − β0.

1When there are control variables and we partial them out from both Y and X, the residuals for Y and X are
not exactly independent. However, all the analyses in this paper are still valid because they only require (2.2), which
still holds for the residuals of Y and X.

5

We collect the transpose of Zi in each row of Z, an n × K matrix of instruments, and denote
P = Z(Z(cid:62)Z)−1Z(cid:62). In addition, Let Qab =
and C = QΠΠ. Then, as pointed out
by Mikusheva and Sun (2022) point, the (rescaled) C is the concentration parameter that measures

j(cid:54)=i aiPij bj
K

(cid:80)
√

i∈[n]

(cid:80)

the strength of identiﬁcation in the heteroskedastic IV model with many instruments. Speciﬁcally,

the parameter β is weakly identiﬁed if C is bounded and strongly identiﬁed if |C| → ∞. We consider

drifting sequence asymptotics so that all quantities are indexed by the sample size n. We omit such

dependence for notation simplicity.

Throughout the paper, we consider three scenarios: (1) weak identiﬁcation and ﬁxed alterna-

tives in which both C and ∆ are ﬁxed and bounded, (2) strong identiﬁcation and local alternatives
in which C = (cid:101)C/dn, ∆ = (cid:101)∆dn, (cid:101)C and (cid:101)∆ are bounded constants independent of n, and dn → 0 is
a deterministic sequence, and (3) strong identiﬁcation and ﬁxed alternatives in which C = (cid:101)C/dn
and ∆ is ﬁxed and bounded. All the weak identiﬁcation robust tests proposed in the literature

(namely, the jackknife AR tests in Crudu et al. (2021) and Mikusheva and Sun (2022) and the

jackknife LM test in Matsushita and Otsu (2020)) depend on a subset of the following three quan-
tities: (Qe(β0)e(β0), QXe(β0), QXX ). Throughout the paper, we maintain the following high-level
assumption.

Assumption 1. Under both weak and strong identiﬁcation, the following weak convergence holds:











Qee
QXe
QXX − C

(cid:32) N




 ,







0

0

0






Φ1 Φ12 Φ13
Φ12 Ψ
τ
Φ13

Υ

τ









 ,

(2.2)

for some (Φ1, Φ12, Φ13, Ψ, τ, Υ).

Assumption 1 has already been veriﬁed by Chao et al. (2012) and Mikusheva and Sun (2022)

under regularity conditions. It implies that, under both strong and weak identiﬁcation,






Qe(β0)e(β0) − ∆2C
QXe(β0) − ∆C
QXX − C






d= N




 ,







0

0

0

where






Φ1(β0) Φ12(β0) Φ13(β0)
τ (β0)
Φ12(β0) Ψ(β0)
Υ
τ (β0)
Φ13(β0)









 + op(1),

(2.3)

Φ1(β0) = ∆4Υ + 4∆3τ + ∆2(4Ψ + 2Φ13) + 4∆Φ12 + Φ1,
Φ12(β0) = ∆3Υ + 3∆2τ + ∆(2Ψ + Φ13) + Φ12,
Φ13(β0) = ∆2Υ + 2∆τ + Φ13,
Ψ(β0) = ∆2Υ + 2∆τ + Ψ,

τ (β0) = ∆Υ + τ.

(2.4)

6

Speciﬁcally, under strong identiﬁcation, we have QXX dn
Also, under local alternatives, we have ∆ = o(1) so that

p

−→ (cid:101)C, which has a degenerate distribution.

(Φ1(β0), Φ12(β0), Φ13(β0), Ψ(β0), τ (β0)) → (Φ1, Φ12, Φ13, Ψ, τ ).

To describe a feasible version of the test, we assume we have consistent estimates for all the

variance components.

Assumption 2. Let ρ(β0) =

, (cid:98)γ(β0) = ((cid:98)Φ1(β0), (cid:98)Φ12(β0), (cid:98)Φ13(β0), (cid:98)Ψ(β0), (cid:98)τ (β0), (cid:98)Υ, (cid:98)ρ(β0))
be an estimator, and B ∈ (cid:60) be a compact parameter space. Then, we have inf β0∈B Φ1(β0) > 0,
inf β0∈B Ψ(β0) > 0, Υ > 0, and for β0 ∈ B,

Φ1(β0)Ψ(β0)

Φ12(β0)

√

where γ(β0) ≡ (Φ1(β0), Φ12(β0), Φ13(β0), Ψ(β0), τ (β0), Υ, ρ(β0)).

||(cid:98)γ(β0) − γ(β0)||2 = op(1),

Several remarks on Assumption 2 are in order. First, Chao et al. (2012) propose a consistent
estimator of Ψ under strong identiﬁcation and many instruments. It is possible to compute (cid:98)γ(β0)
based on Chao et al.’s (2012) argument with their JIVE-based residuals ˆei from the structure equa-
tion replaced by ei(β0). Under weak identiﬁcation and β0 = β, Crudu et al. (2021) and Matsushita
and Otsu (2021) establish the consistency of such estimators for Φ1(β0) and Ψ(β0), respectively.
Similar arguments can be used to show the consistency of the rest of the elements in (cid:98)γ(β0) under
both weak and strong identiﬁcation. In addition, the consistency can be established under both

local and ﬁxed alternatives. We provide more details in Section A.1 in the Online Supplement. Sec-

ond, motivated by Kline, Saggio, and Sølvsten (2020), Mikusheva and Sun (2022) propose cross-ﬁt
estimators (cid:98)Φ1(β0) and (cid:98)Υ, which are consistent under both weak and strong identiﬁcation and lead
to better power properties. Following their lead, one can write down the cross-ﬁt estimators for
the rest of the elements in γ(β0) and show they are consistent.2 We provide more details in Section
A.2 in the Online Supplement. Note that both Crudu et al.’s (2021) and Mikusheva and Sun’s

(2022) estimators are consistent under heteroskedasticity and allow for K to be of the same order

of n. Third, in order for our jackknife CLC test proposed below to control size under both weak
and strong identiﬁcation, it suﬃces to require (cid:98)γ(β0) to be consistent under the null only. Fourth,
in the following, we study the power properties of jackknife test statistics against local or ﬁxed

alternatives under diﬀerent identiﬁcation scenarios. The power analysis in Lemmas 2.1 and 2.4
below, and subsequently, Theorems 4.1 and 4.2, only requires the consistency of (cid:98)γ(β0) under strong
identiﬁcation with local alternatives and weak identiﬁcation with ﬁxed alternatives, respectively.

2For example, Mikusheva and Sun (2022, p.22) establish the limit of their cross-ﬁt estimator (cid:98)Ψ under weak
identiﬁcation and many instruments when the residual ˆei from the structure equation is computed based on the
JIVE estimator. We can construct (cid:98)Ψ(β0) by replacing ˆei by ei(β0). Then, the argument, as theirs with −QXe/QXX
replaced by ∆, establishes that (cid:98)Ψ(β0)

−→ Ψ(β0).

p

7

Under this framework, Crudu et al. (2021) and Mikusheva and Sun (2022) consider the jackknife

AR test

1{AR(β0) ≥ zα}, AR(β0) =

Qe(β0)e(β0)
(cid:98)Φ1/2
(β0)

1

,

and Matsushita and Otsu (2020) consider the jackknife LM test

1{LM 2(β0) ≥ Cα}, LM (β0) =

QXe(β0)
(cid:98)Ψ1/2(β0)

.

(2.5)

(2.6)

Both tests are robust to weak identiﬁcation, many instruments, and heteroskedasticity. Lemma 2.1
below characterizes the joint limit distribution of (AR(β0), LM (β0))(cid:62) under strong identiﬁcation
and local alternatives.

Lemma 2.1. Suppose Assumptions 1 and 2 hold and we are under strong identiﬁcation with local
alternatives, that is, there exists a deterministic sequence dn → 0 such that C = (cid:101)C/dn and ∆ = (cid:101)∆dn,
where (cid:101)C and (cid:101)∆ are bounded constants independent of n. Then, we have

(cid:32)

(cid:33)

AR(β0)
LM (β0)

(cid:32)

(cid:33)

(cid:32)

N1
N2

d= N

(cid:32)(cid:32)

(cid:33)

(cid:32)

,

(cid:33)(cid:33)

1 ρ

ρ 1

0
(cid:101)∆ (cid:101)C
Ψ1/2

where ρ = Φ12/

√

Φ1Ψ.

Two remarks are in order. First, under strong identiﬁcation, we consider local alternatives so
that β − β0 → 0. This is why we have (Ψ(β0), Φ1(β0), Φ12(β0)) converge to (Ψ, Φ1, Φ12), which
are just the counterparts of (Ψ(β0), Φ1(β0), Φ12(β0)) when β0 is replaced by β. Second, although
AR(β0) has zero mean, and hence, no power, it is correlated with LM (β0) in the current context
of many instruments. It is therefore possible to use AR(β0) to reduce the variance of LM (β0) and
obtain a test that is more powerful than the LM test.

Lemma 2.2. Consider the limit experiment in which researchers observe (N1, N2) with

(cid:33)

(cid:32)

N1
N2

d= N

(cid:32)(cid:32)
0

(cid:33)

θ

,

(cid:32)

(cid:33)(cid:33)

1 ρ

ρ 1

,

know the value of ρ and that EN1 = 0, and want to test for θ = 0 versus the two-sided alternative. In
2 ≥ Cα},
this case, the uniformly most powerful level-α test that is invariant to sign changes is 1{N ∗2
where

N ∗

2 = (1 − ρ2)−1/2(N2 − ρN1)

is the normalized residual from the projection of N2 on N1.

8

Let the orthogonalized jackknife LM statistic be LM ∗(β0) = (1−(cid:98)ρ(β0)2)−1/2(LM (β0)−(cid:98)ρ(β0)AR(β0)).

Then, Lemma 2.1 implies, under strong identiﬁcation and local alternatives,

(cid:32)

AR(β0)
LM ∗(β0)

(cid:33)

(cid:32)

(cid:32)

(cid:33)

N1
N ∗
2

d= N

(cid:32)(cid:32)

(cid:33)

,

0
(cid:101)∆ (cid:101)C
[(1−ρ2)Ψ]1/2

(cid:32)

1 0

(cid:33)(cid:33)

0 1

.

(2.7)

Lemma 2.2 with θ = (cid:101)∆ (cid:101)CΨ−1/2 implies, in this case, that the test 1{LM ∗2(β0) ≥ Cα} is asymptot-
ically strictly more powerful than the jackknife AR and LM tests based on AR(β0) and LM (β0)
against local alternatives as long as ρ (cid:54)= 0. In addition, under strong identiﬁcation and local alter-
natives, Mikusheva and Sun’s (2022) two-step test statistic is asymptotically equivalent to LM (β0),
and thus, is less powerful than LM ∗(β0) too.

Next, we compare the behaviors of AR(β0), LM (β0), and LM ∗(β0) under strong identiﬁcation

and ﬁxed alternatives.

Lemma 2.3. Suppose Assumption 2 holds, (Qe(β0)e(β0) − ∆2C, QXe(β0) − ∆C, QXX − C)(cid:62) = Op(1),
and we are under strong identiﬁcation so that dnC → (cid:101)C for some dn → 0. Then, we have, for any
ﬁxed ∆ (cid:54)= 0,

d2
n











AR2(β0)
LM 2(β0)
LM ∗2(β0)

p
−→






Φ−1
1 (β0)∆4 (cid:101)C2
Ψ−1(β0)∆2 (cid:101)C2




 .

(β0)∆)2∆2 (cid:101)C2

(1 − ρ2(β0))−1(Ψ−1/2(β0) − ρ(β0)Φ−1/2

1

Given dn → 0 and Φ−1

totically. By contrast, LM ∗2(β0) may not have power if ∆ = ∆∗(β0) ≡ Φ1/2

1 (β0)∆4 (cid:101)C2 > 0, AR2(β0) has power 1 against ﬁxed alternatives asymp-
(β0)Ψ−1/2(β0)ρ−1(β0).
Next, we compare the performance of AR(β0) and LM ∗(β0) under weak identiﬁcation and ﬁxed

1

alternatives.

Lemma 2.4. Suppose Assumptions 1 and 2 hold and we are under weak identiﬁcation so that C is

ﬁxed. Then, we have, for any ﬁxed ∆ (cid:54)= 0,

(cid:32)

AR(β0)
LM ∗(β0)

(cid:33)

(cid:32)

(cid:33)

(cid:32)

N1
N ∗
2

d= N

where ρ(β0) =

√

Φ12(β0)

Ψ(β0)Φ1(β0)

and

(cid:32)(cid:32)

m1(∆)
m2(∆)

(cid:33)

(cid:32)

,

1 0

0 1

(cid:33)(cid:33)

,

(2.8)

(cid:32)

(cid:32)

(cid:33)

=

m1(∆)
m2(∆)

Φ−1/2
1
(1 − ρ2(β0))−1/2Ψ−1/2(β0)∆C − ρ(β0)(1 − ρ2(β0))−1/2Φ−1/2

(β0)∆2C

1

(cid:33)

.

(β0)∆2C

9

In particular, as ∆ → ∞, we have

m1(∆) →

C
Υ1/2

and m2(∆) →

C
Υ1/2

ρ23
(1 − ρ2

23)1/2

,

where ρ23 =

τ

(ΨΥ)1/2 is the correlation between QXe and QXX .3

By comparing the means of the normal limit distribution in (2.8), we notice that under weak
identiﬁcation and ﬁxed alternatives, neither LM ∗(β0) dominates AR(β0) or vice versa. We also
notice from Lemma 2.4 that for testing distant alternatives, the power of LM ∗(β0) is diﬀerent from
AR(β0) by a factor of ρ23/(cid:112)1 − ρ2
2. Under weak
identiﬁcation and homoskedasticity,4 we have ρ23 = ρ = Φ12/
ΨΦ1. Therefore, although the test
1{LM ∗2(β0) ≥ Cα} has a power advantage under strong identiﬁcation against local alternatives, it
may lack power under weak identiﬁcation against distant alternatives if the degree of endogeneity
is low. Furthermore, LM ∗(β0) may not have power if ∆ = ∆∗(β0). We notice that such a power
issue of LM ∗(β0) is similar to that of the tests based on the K statistic introduced by Kleibergen
(2002, 2005) under the framework of a ﬁxed number of instruments. Under such a framework,

23, so that it will be lower when |ρ23| ≤ 1/

√

√

the K statistic is eﬃcient under strong identiﬁcation against local alternatives but may have a

non-monotonic power function under weak identiﬁcation (e.g., see the discussions in Section 3.1 of

I.Andrews (2016)).

To achieve the advantages of AR(β0), LM (β0), and LM ∗(β0) in all three scenarios above, we
need to combine them in a way that is adaptive to the identiﬁcation strength. Following I.Andrews
(2016), we consider the linear combination of AR2(β0), LM 2(β0), and LM ∗2(β0). Recall that
2 ) are the limits of (AR(β0), LM ∗(β0)) in either strong or weak identiﬁcation. See (2.7) and
(N1, N ∗
(2.8) for their expressions in these two cases. Then, in the limit experiment, the linear combination

test can be written as

φa1,a2,∞ = 1{a1N 2

1 + a2(˜ρN1 + (1 − ˜ρ2)1/2N ∗

2 )2 + (1 − a1 − a2)N ∗2

2 ≥ Cα(a1, a2; ˜ρ)},

(2.9)

where (a1, a2) ∈ A0 are the combination weights, N1 ∼ Z(θ1), and N ∗
2 ∼ Z(θ2); the mean parame-
ters θ1 and θ2 are deﬁned in Lemmas 2.1 and 2.4 for strong and weak identiﬁcation, respectively; and

˜ρ is the limit of (cid:98)ρ(β0).5 Let the eigenvalue decomposition of the matrix
be

(cid:32)

a1 + a2 ˜ρ2
a2 ˜ρ(1 − ˜ρ2)1/2

a2 ˜ρ(1 − ˜ρ2)1/2
1 − a1 − a2 ˜ρ2

(cid:33)

(cid:32)

a1 + a2 ˜ρ2
a2 ˜ρ(1 − ˜ρ2)1/2

a2 ˜ρ(1 − ˜ρ2)1/2
1 − a1 − a2 ˜ρ2

(cid:33)

= U

(cid:32)

s1(a1, a2)
0

(cid:33)

0
s2(a1, a2)

U (cid:62),

(2.10)

3We suppress the dependence of m1(∆) and m2(∆) on γ(β0) and C for notation simplicity.
4Speciﬁcally, we say the data are homoskedastic if (σi, γi, ηi) deﬁned after (A.1) in Section A of the Online

Supplement are constant across i.

5Under ﬁxed alternatives, ˜ρ = ρ(β0); under local alternatives, ˜ρ = ρ.

10

where, by construction, s1(a1, a2) ≥ s2(a1, a2) ≥ 0 and U is a 2 × 2 unitary matrix. We highlight
the dependence of eigenvalues (s1, s2) on the weights (a1, a2). The dependence of U on (a1, a2) is
suppressed for notation simplicity. Then, we have

a1N 2

1 + a2(˜ρN1 + (1 − ˜ρ2)1/2N ∗

2 )2 + (1 − a1 − a2)N ∗2

2 = s1(a1, a2) (cid:101)N 2

1 + s2(a1, a2) (cid:101)N 2
2

and φa1,a2,∞ = 1{s1(a1, a2) (cid:101)N 2

1 + s2(a1, a2) (cid:101)N 2

2 ≥ Cα(a1, a2; ˜ρ)}, where

(cid:33)

(cid:32)

(cid:101)N1
(cid:101)N2

= U (cid:62)

(cid:33)

(cid:32)

N1
N ∗
2

and (cid:101)N1 and (cid:101)N2 are independent normal random variables with unit variance. This implies that
φa1,a2,∞ can be viewed as a linear combination test of two independent chi-squares random variables
with one degree of freedom, and those two chi-squares random variables are obtained by properly
rotating N1 and N ∗

2 (i.e., the limits of AR(β0) and LM ∗(β0)).

Theorem 2.1 states the key properties of φa1,a2,∞ under the limit experiment. Theorems 4.1-

4.3 further establish that these properties hold asymptotically for our linear combination test.

Theorem 2.1.

(i) Under weak identiﬁcation and ﬁxed alternatives, N1 ∼ Z(θ1), N ∗
2 ∼ Z(θ2),
and they are independent, where θ1 = m1(∆) and θ2 = m2(∆) as in (2.8). We consider the
test of H0 : θ1 = θ2 = 0 against H1 : θ1 (cid:54)= 0 or θ2 (cid:54)= 0. Then, for any (a1, a2) ∈ A0, φa1,a2,∞
deﬁned in (2.9) is admissible among the level-α tests based on test statistics ˜s1 (cid:101)N 2
2 for
(˜s1, ˜s2) ∈ (cid:60)+ × (cid:60)+.

1 + ˜s2 (cid:101)N 2

(cid:101)∆ (cid:101)C

(ii) Under strong identiﬁcation and local alternatives, N 2

2 ∼ Z 2(θ), where θ =
[(1−ρ2)Ψ]1/2 as in (2.7). We consider the test of H0 : θ = 0 against H1 : θ (cid:54)= 0. Then,
φa1,a2,∞ deﬁned in (2.9) is the uniformly most powerful test in the class of tests that depend
on (N1, N ∗
2 ) and are invariant to sign changes if and only if a1 = 0 and a2ρ = 0.

1 ∼ Z 2, N ∗2

(iii) Suppose Assumption 2 holds, (Qe(β0)e(β0) − ∆2C, QXe(β0) − ∆C, QXX − C)(cid:62) = Op(1), and
∗(β0) for some
(β0)Ψ−1/2(β0)ρ−1(β0),

we are under strong identiﬁcation with ﬁxed alternatives. If 1 ≥ a1,n ≥ ˜qΦ1(β0)
constant ˜q > Cα,max(ρ(β0)) and (a1,n, a2,n) ∈ A0, where ∆∗(β0) = Φ1/2
then

C2∆4

1

1{a1,nAR2(β0) + a2,nLM 2(β0) + (1 − a1,n − a2,n)LM ∗2(β0) ≥ Cα(a1,n, a2,n; (cid:98)ρ(β0))}

p

−→ 1.

Five remarks are in order. First, in the case with a ﬁxed number of weak instruments (or

moment conditions), I.Andrews (2016) consider the linear combination of K and S statistics. The

trade-oﬀ between K and S statistics is from the diﬀerence in attention to deviation directions (see
the discussions in Section 3 of I.Andrews (2016)). We notice from Theorem 2.1 that φa1,a2,∞ is

11

constructed based on a quadratic function of AR(β0) and LM ∗(β0), which play roles similar to S
and K, respectively. However, AR(β0) and LM ∗(β0) do not have such a diﬀerence in deviation
Instead, the trade-oﬀ between AR(β0) and LM ∗(β0) is between local and non-local
directions.
alternatives. Additionally, although the standard score test is not weak identiﬁcation robust under
a ﬁxed number of instruments, LM (β0) is robust under many instruments. Therefore, we consider
the linear combination of AR(β0), LM (β0), and LM ∗(β0) to take advantage of the power properties
of all three tests.

Second, unlike the one-sided jackknife AR test proposed by Mikusheva and Sun (2022), we
construct the jackknife CLC test based on AR2(β0) for several reasons. First, under weak identiﬁ-
cation, when the concentration parameter C and, thus, m1(∆) deﬁned in Lemma 2.4 is nonnegative,
the one-sided test has good power. However, even in this case, the power curves simulation in Sec-

tion 5.1 shows that our jackknife CLC test is more powerful than the one-sided AR test in most
scenarios. Second, our jackknife CLC test will have good power even when C is negative.6 Third,
we show below that under strong identiﬁcation and local alternatives, our jackknife CLC test con-
2 > Cα} whereas both the one- and two-sided tests
verges to the uniformly most powerful test 1{N ∗2
based on AR(β0) have no power, as shown in Lemma 2.1. Fourth, under strong identiﬁcation and
ﬁxed alternatives, our jackknife CLC test has asymptotic power equal to 1, as shown in Lemma 2.3

and Theorem 4.3 below. In this case, using the one-sided jackknife AR test cannot further improve
the power. Fifth, combining LM ∗2(β0) with AR2(β0) (and LM 2(β0)), rather than AR(β0), can
substantially mitigate the impact of power loss of LM ∗(β0) at ∆∗(β0), as shown in the numerical
investigation in Section 5.

Third, Theorem 2.1(i) implies that φa1,a2,∞ is admissible among tests that are also quadratic

functions of N1 and N ∗

2 with the same rotation U but diﬀerent eigenvalues (˜s1, ˜s2); that is,

(N1, N ∗

2 )U

(cid:32)

˜s1
0

(cid:33)

(cid:32)

U (cid:62)

(cid:33)

.

N1
N ∗
2

0
˜s2

Speciﬁcally, in the special case with a2 = 0, the rotation matrix U = I2 and φa1,0,∞ is admissible
among level-α tests based on the test statistics of the form a1N 2
for a1 ∈ [0, 1], which
is similar to the result for the linear combination of S and K statistics in I.Andrews (2016).

1 + (1 − a1)N ∗2
2

Fourth, under strong identiﬁcation and local alternatives, a1 = 0 and a2ρ = 0 imply that
2 ≥ Cα}, which is the uniformly most powerful invariant test. When ρ = 0 and
in the second and third terms of φa1,a2,∞ cancels out, which implies

φa1,a2,∞ = 1{N ∗,2
under local alternatives, a2N ∗2
2
that φa1,a2,∞ = 1{N ∗2

2 ≥ Cα} as long as a1 = 0.

Fifth, we note that both the rotation matrix U and the eigenvalues s1 and s2 in (2.10) are func-

(cid:80)

(cid:80)

6We note that C =

, where M = I − P . If Π(cid:62)M Π and (cid:80)
are suﬃciently large, C can be negative. Mikusheva and Sun (2022) further assume that Π(cid:62)M Π ≤ CΠ(cid:62)Π
constant C > 0, which implies that C > 0.

j(cid:54)=i ΠiPij Πj
√

i∈[n]

=

K

K

K

√

i∈[n](1−Pii)Π2

i −Π(cid:62)M Π

(cid:80)

i∈[n] PiiΠ2
for some

i

12

tions of (a1, a2). We choose this speciﬁc parametrization so that φa1,a2,∞ can be written as a linear
combination of AR2(β0), LM 2(β0), and LM ∗2(β0). It is possible to use other parametrizations to
combine AR(β0) and LM ∗(β0). For example, let

(cid:32)

O(ζ) =

(cid:33)

cos(ζ) − sin(ζ)

sin(ζ)

cos(ζ)

be a rotation matrix with angle ζ and

(cid:33)

(cid:32)

AR†(β0, ζ)
LM †(β0, ζ)

= O(ζ)

(cid:32)

AR(β0)
LM ∗(β0)

(cid:33)

. Then, in the limit

experiment, the linear combination test statistic can be written as

aN †2

1 + (1 − a)N †2
2 ,

(2.11)

1 , N †

where (N †
2 ) are the limits of (AR†(β0, ζ), LM †(β0, ζ)) under either weak or strong identiﬁcation.
In the following, we use a minimax procedure to select the weight (a1, a2) in our jackknife CLC
test φa1,a2,∞. We can do the same to select a and ζ for the new parametrization in (2.11). Under
strong identiﬁcation and local alternatives, Lemma 2.2 shows that 1{LM ∗2(β0) ≥ Cα} is the most
powerful test against local alternatives, which is achieved by our jackknife CLC test φa1,a2,∞ with
a1 = 0 and a2ρ = 0. In this setting, the new parametrization does not bring any additional power.

3 A Conditional Linear Combination Test

In this section, we determine the weights (a1, a2) in the jackknife CLC test via a minimax procedure.
Under weak identiﬁcation, the limit power of the jackknife CLC test with weights (a1, a2) is

Eφa1,a2,∞ = E1

(cid:40)

a1Z 2

1 (m1(∆)) + a2(ρ(β0)Z1(m1(∆)) + (1 − ρ2(β0))1/2Z2(m2(∆)))2

+(1 − a1 − a2)Z 2

2 (m2(∆)) ≥ Cα(a1, a2; ρ(β0))

(cid:41)

,

where m1(∆) and m2(∆) are deﬁned in Lemma 2.4, and Z1(·) and Z2(·) are independent. In this
case, we can be explicit and write φa1,a2,∞ = φa1,a2,∞(∆). However, the limit power of the jackknife
CLC test will typically remain unknown as the true parameter β (and hence ∆) is unknown. To
overcome this issue, we follow I.Andrews (2016) and calibrate the power of Eφa1,a2,∞(δ), where
δ ranges over all possible values that ∆ can potentially take; we deﬁne φa1,a2,∞(δ) as well as the
range of potential values of ∆ below.

Let (cid:98)D = QXX − (Qe(β0)e(β0), QXe(β0))

be the residual from

(cid:32)

(cid:98)Φ1(β0)
(cid:98)Φ12(β0)

(cid:98)Φ12(β0)
(cid:98)Ψ(β0)

(cid:33)−1 (cid:32)

(cid:33)

(cid:98)Φ13(β0)
(cid:98)τ (β0)

the projection of QXX on (Qe(β0)e(β0), QXe(β0)). By (2.3), under weak identiﬁcation,

(cid:98)D = D + op(1), D d= N (µD, σ2

D),

13

where


1 − (∆2, ∆)



(cid:32)



µD = C

(cid:33)−1 (cid:32)

Φ1(β0) Φ12(β0)
Φ12(β0) Ψ(β0)

(cid:33)


Φ13(β0)
τ (β0)



 and



(cid:32)

σ2
D = Υ −

(Φ13(β0), τ (β0))

Φ1(β0) Φ12(β0)
Φ12(β0) Ψ(β0)

(cid:33)−1 (cid:32)

(cid:33)
 .

Φ13(β0)
τ (β0)

We note that (cid:98)D is a suﬃcient statistic for µD, which contains information about the concentration
parameter C and is asymptotically independent of AR(β0), LM (β0), and hence LM ∗(β0).

Under weak identiﬁcation, we observe that m1(∆) and m2(∆) in Lemma 2.4 can be written as

(cid:32)

m1(∆)
m2(∆)

(cid:33)

=

(cid:33)

(cid:32)

C1(∆)
C2(∆)

µD,

where

(cid:32)

(cid:33)

≡

C1(∆)
C2(∆)

(cid:32)

Φ−1/2
1
(1 − ρ2(β0))−1/2(Ψ−1/2(β0)∆ − ρ(β0)Φ−1/2

(β0)∆2

1

(3.1)

(cid:33)


1 − (∆2, ∆)



(cid:32)



×

(cid:33)−1 (cid:32)

Φ1(β0) Φ12(β0)
Φ12(β0) Ψ(β0)

(β0)∆2)

(cid:33)




Φ13(β0)
τ (β0)

−1

.

(3.2)

By (3.1), we see that φa1,a2,∞ = φa1,a2,∞(∆) deﬁned in (2.9) can be written as

(cid:40)

a1Z 2

1 (C1(∆)µD) + a2(ρ(β0)Z1(C1(∆)µD) + (1 − ρ2(β0))1/2Z2(C2(∆)µD))2

+(1 − a1 − a2)Z 2

2 (C2(∆)µD) ≥ Cα(a1, a2; ρ(β0))

(cid:41)

.

1

This motivates the deﬁnition that

φa1,a2,∞(δ) = 1

(cid:40)

a1Z 2

1 (C1(δ)µD) + a2(ρ(β0)Z1(C1(δ)µD) + (1 − ρ2(β0))1/2Z2(C2(δ)µD))2

+(1 − a1 − a2)Z 2

2 (C2(δ)µD) ≥ Cα(a1, a2; ρ(β0))

(cid:41)

.

(3.3)

To emphasize the dependence of φa1,a2,∞(δ) on µD and γ(β0), we further write φa1,a2,∞(δ) as
φa1,a2,∞(δ, µD, γ(β0)).

The range of values that ∆ can take is deﬁned as D(β0) = {δ : δ + β0 ∈ B}, where B is the
parameter space. For example, in their empirical application of returns to education, Mikusheva

and Sun (2022) posit that the value of β (i.e., the return to education) is from -0.5 to 0.5 (i.e.,

B = [−0.5, 0.5]). We follow the same practice in the simulation based on calibrated data in Section

5.2 and the empirical application in Section 6.

14

Following the lead of I.Andrews (2016), we deﬁne the highest attainable power for each δ ∈ D(β0)

as Pδ,µD = sup(a1,a2)∈A(µD,γ(β0))

Eφa1,a2,∞(δ, µD, γ(β0)), which means that

Pδ,µD − Eφa1,a2,∞(δ, µD, γ(β0))

is the power loss when the weights are set as (a1, a2). Here we denote the domain of (a1, a2)
as A(µD, γ(β0)) and deﬁne it as A(µD, γ(β0)) = {(a1, a2) ∈ A0, a1 ∈ [a(µD, γ(β0)), 1]} where
A0 = {(a1, a2) ∈ [0, 1] × [0, 1], a1 + a2 ≤ a} for some a < 1,

a(µD, γ(β0)) = min

0.01,

(cid:18)

1.1Cα,max(ρ(β0))Φ1(β0)cB(β0)
∗(β0)µ2
D

∆4

(cid:19)

,

∆∗(β0) = Φ1/2

1

(β0)Ψ−1/2(β0)ρ−1(β0) as deﬁned after Lemma 2.3, and

cB(β0) = sup

δ∈D(β0)


1 − (δ2, δ)



(cid:32)



(cid:33)−1 (cid:32)

Φ1(β0) Φ12(β0)
Φ12(β0) Ψ(β0)

(cid:33)



2



.

Φ13(β0)
τ (β0)

The maximum power loss over δ ∈ D(β0) can be viewed as a maximum regret. Then, we choose

(a1, a2) that minimizes the maximum regret; that is,

(a1(µD, γ(β0)), a2(µD, γ(β0))) ∈

arg min
(a1,a2)∈A(µD,γ(β0))

sup
δ∈D(β0)

(Pδ,µD − Eφa1,a2,∞(δ, µD, γ(β0))).

(3.4)

Four remarks on the domain of (a1, a2) (i.e., A(µD, γ(β0))) are in order. First, the lower bound
a(µD, γ(β0)) is motivated by Theorem 2.1(iii). Second, under weak identiﬁcation, µD is ﬁxed, and
1.1Cα,max(ρ(β0))Φ1(β0)cB(β0)
may be larger than 0.01. In this case, we have A(µD, γ(β0)) = {(a1, a2) ∈
∗(β0)µ2
D
A0, a1 ∈ [0.01, 1]}. In our simulations, the minimax a1 never hits the lower bound so that setting
the lower bound to be 0.01 or 0 does not make any numerical diﬀerence. Third, under strong
identiﬁcation and local alternatives, 1.1Cα,max(ρ(β0))Φ1(β0)cB(β0)

will converge to zero so that

∆4

∆4

∗(β0)µ2
D

A(µD, γ(β0)) =

(cid:26)

(a1, a2) ∈ A0, a1 ∈

(cid:20) 1.1Cα,max(ρ(β0))Φ1(β0)cB(β0)
∗(β0)µ2
D

∆4

(cid:21)(cid:27)

, 1

.

We show in Theorem 4.2 below that in this case, the minimax jackknife CLC test converges to
2 ≥ Cα} deﬁned in Lemma 2.2, which is the uniformly most powerful invariant test. Further-
1{N ∗2
more, the minimax a1 satisﬁes the requirement in Theorem 2.1(iii) with ˜q = 1.1Cα,max(ρ(β0)) so
that under strong identiﬁcation, our CLC test has asymptotic power 1 against ﬁxed alternatives,

as shown in Theorem 4.3. Fourth, we require a < 1 for some technical reason. Again, in our
simulations, we never observe the minimax a1 + a2 hitting the upper bound so that setting the
upper bound to be a or 1 does not make any numerical diﬀerence.

15

In practice, we do not observe µD and γ(β0). Therefore, we follow I.Andrews (2016, Section 6)
and consider the plug-in method. We can replace γ(β0) by its consistent estimator (cid:98)γ(β0) introduced
in Assumption 2. To obtain a proxy of µD,7 we deﬁne


 (cid:98)Υ − ((cid:98)Φ13(β0), (cid:98)τ (β0))

(cid:32)

(cid:98)σD =

(cid:98)Φ1(β0)
(cid:98)Φ12(β0)

(cid:33)−1 (cid:32)

(cid:98)Φ12(β0)
(cid:98)Ψ(β0)

1/2

(cid:33)


,

(cid:98)Φ13(β0)
(cid:98)τ (β0)

which is a function of (cid:98)γ(β0) and a consistent estimator of σD by Assumption 2. Then, under weak
identiﬁcation, we have (cid:98)D2/(cid:98)σ2
D is a suﬃcient
statistic for µ2
D. We consider two estimators for µD as functions of (cid:98)D and (cid:98)σD,
namely, fpp( (cid:98)D, (cid:98)γ(β0)) = (cid:98)σD

(cid:98)rkrs, where (cid:98)rpp = max((cid:98)r − 1, 0) and

(cid:98)rpp and fkrs( (cid:98)D, (cid:98)γ(β0)) = (cid:98)σD

D) + op(1) and D2/σ2

D. Let (cid:98)r = (cid:98)D2/(cid:98)σ2

D + op(1) d= χ2

D = D2/σ2

D/σ2

1(µ2

(cid:112)

√

(cid:98)rkrs = (cid:98)r − 1 + exp

(cid:19)

(cid:18)
− (cid:98)r
2





∞
(cid:88)

j=0

(cid:19)j

(cid:18)
− (cid:98)r
2

1
j!(1 + 2j)



−1



.

Speciﬁcally, Kubokawa, Robert, and Saleh (1993) show that (cid:98)rkrs is positive as long as (cid:98)r > 0 and
(cid:98)r ≥ (cid:98)rkrs ≥ (cid:98)r − 1. It is also possible to consider the MLE based on a single observation (cid:98)D2/(cid:98)σ2
D.
However, such an estimator is harder to use because it does not have a closed-form expression.

In practice, we estimate Eφa1,a2,∞(δ, µD, γ(β0)) by E∗φa1,a2,s(δ, (cid:98)D, (cid:98)γ(β0)) for s ∈ {pp, krs},

where

φa1,a2,s(δ, (cid:98)D, (cid:98)γ(β0))





= 1

+a2

1 ( (cid:98)C1(δ)fs( (cid:98)D, (cid:98)γ(β0)))
(cid:104)
(cid:98)ρ(β0)Z1( (cid:98)C1(δ)fs( (cid:98)D, (cid:98)γ(β0))) + (1 − (cid:98)ρ2(β0))1/2Z2( (cid:98)C2(δ)fs( (cid:98)D, (cid:98)γ(β0)))

a1Z 2

(cid:105)2

+(1 − a1 − a2)Z 2

2 ( (cid:98)C2(δ)fs( (cid:98)D, (cid:98)γ(β0)) ≥ Cα(a1, a2; (cid:98)ρ(β0))





,

(3.5)

and ( (cid:98)C1(δ), (cid:98)C2(δ)) are similarly deﬁned as (C1(δ), C2(δ)) in (3.2) with γ(β0) replaced by (cid:98)γ(β0); that
is,

(cid:32)

(cid:33)

(cid:98)C1(δ)
(cid:98)C2(δ)

(cid:32)

≡

(cid:98)Φ−1/2
(1 − (cid:98)ρ2(β0))−1/2( (cid:98)Ψ−1/2(β0)δ − (cid:98)ρ(β0)(cid:98)Φ−1/2

(β0)δ2

1

1

(cid:33)


1 − (δ2, δ)





×

(cid:32)

(cid:98)Φ1(β0)
(cid:98)Φ12(β0)

(cid:98)Φ12(β0)
(cid:98)Ψ(β0)

(cid:33)−1 (cid:32)

(β0)δ2)

(cid:33)




−1

.

(cid:98)Φ13(β0)
(cid:98)τ (β0)

Let Pδ,s( (cid:98)D, (cid:98)γ(β0)) = sup(a1,a2)∈A(fs( (cid:98)D,(cid:98)γ(β0)),(cid:98)γ(β0))
7In fact, as φa1,a2,∞(δ, µD, γ(β0)) only depends on µ2

E∗φa1,a2,s(δ, (cid:98)D, (cid:98)γ(β0)). Then, for s ∈ {pp, krs},

D, we aim to ﬁnd a good estimator of µ2
D.

16

we can estimate a(µD, γ(β0)) in (3.4) by As( (cid:98)D, (cid:98)γ(β0)) = (A1,s( (cid:98)D, (cid:98)γ(β0)), A2,s( (cid:98)D, (cid:98)γ(β0))) deﬁned as

As( (cid:98)D, (cid:98)γ(β0)) ∈

arg min
(a1,a2)∈A(fs( (cid:98)D,(cid:98)γ(β0)),(cid:98)γ(β0))

sup
δ∈D(β0)

(Pδ,s( (cid:98)D, (cid:98)γ(β0)) − E∗φa1,a2,s(δ, (cid:98)D, (cid:98)γ(β0))),

(3.6)

where φa1,a2,s(δ, (cid:98)D, (cid:98)γ(β0)) is deﬁned in (3.5),

A(fs( (cid:98)D, (cid:98)γ(β0)), (cid:98)γ(β0)) = {(a1, a2) ∈ A0, a1 ∈ [a(fs( (cid:98)D, (cid:98)γ(β0)), (cid:98)γ(β0)), a]},

a(fs( (cid:98)D, (cid:98)γ(β0)), (cid:98)γ(β0)) = min

0.01,

(cid:32)

1.1Cα,max((cid:98)ρ(β0))(cid:98)Φ1(β0)(cid:98)cB(β0)

(cid:98)∆4

∗(β0)f 2

s ( (cid:98)D, (cid:98)γ(β0))

(cid:33)

,

(cid:98)cB(β0) = sup

δ∈D(β0)


1 − (δ2, δ)



(cid:32)



(cid:98)Φ1(β0)
(cid:98)Φ12(β0)

(cid:98)Φ12(β0)
(cid:98)Ψ(β0)

(cid:33)−1 (cid:32)

(cid:33)



2



,

(cid:98)Φ13(β0)
(cid:98)τ (β0)

and (cid:98)∆∗(β0) = (cid:98)Φ1/2

1

(β0) (cid:98)Ψ−1/2(β0)(cid:98)ρ−1(β0). Then, the feasible jackknife CLC test is, for s ∈ {pp, krs},

(cid:40)

(cid:98)φAs( (cid:98)D,(cid:98)γ(β0)) = 1

A1,s( (cid:98)D, (cid:98)γ(β0))AR2(β0) + A2,s( (cid:98)D, (cid:98)γ(β0))LM 2(β0)
+(1 − A1,s( (cid:98)D, (cid:98)γ(β0)) − A2,s( (cid:98)D, (cid:98)γ(β0)))LM ∗2(β0) ≥ Cα(As( (cid:98)D, (cid:98)γ(β0)); (cid:98)ρ(β0))

(cid:41)

,

(3.7)

4 Asymptotic Properties

We ﬁrst consider the asymptotic properties of the jackknife CLC test under weak identiﬁcation and

ﬁxed alternatives, in which C and ∆ are treated as ﬁxed so that we have

(cid:98)D (cid:32) D d= N (µD, σ2

D).

We see from (3.4) and (3.6) that As(d, r) = (a1(fs(d, r), r), a2(fs(d, r), r)) for (d, r) ∈ (cid:60) × Γ, where
Γ is the parameter space for γ(β0) and s ∈ {pp, krs}. We make the following assumption on As(·).

Assumption 3. Suppose we are under weak identiﬁcation with a ﬁxed β0. Let Ss be the set of
discontinuities of As(·, γ(β0)) : (cid:60) (cid:55)→ [0, 1] × [0, 1]. Then, we assume As(d, r) is continuous in r at
r = γ(β0) for any d ∈ (cid:60)/Ss, and the Lebesgue measure of Ss is zero for s ∈ {pp, krs}.

Assumption 3 is a technical condition that allows us to apply the continuous mapping theorem.
It is mild because As(·) is allowed to be discontinuous in its ﬁrst argument. In practice, we can
approximate As(·) by a step function deﬁned over a grid of d so that there is a ﬁnite number of
discontinuities. The continuity of As(·) in its second argument is due to the smoothness of the

17

bivariate normal PDF with respect to the covariance matrix. Therefore, in this case, Assumption

3 holds automatically.

Theorem 4.1. Suppose we are under weak identiﬁcation and ﬁxed alternatives and that Assump-

tions 1, 2, and 3 hold. Then, for s ∈ {pp, krs},

As( (cid:98)D, (cid:98)γ(β0)) (cid:32) As(D, γ(β0)) = (a1(fs(D, γ(β0)), γ(β0)), a2(fs(D, γ(β0)), γ(β0)))

and8

E (cid:98)φAs( (cid:98)D,(cid:98)γ(β0)) → Eφa1(fs(D,γ(β0)),γ(β0)),a2(fs(D,γ(β0)),γ(β0)),∞(∆, µD, γ(β0)),

where φa1,a2,∞ is deﬁned in (3.3) and al(fs(D, γ(β0)), γ(β0)) is interpreted as al(µD, γ(β0)) deﬁned
in (3.4) with µD replaced by fs(D, γ(β0)) for l = 1, 2.

In addition, let BL1 be the class of functions f (·) of D that is bounded and Lipschitz with

Lipschitz constant 1. Then, if the null hypothesis holds such that ∆ = 0, we have

E( (cid:98)φAs( (cid:98)D,(cid:98)γ(β0)) − α)f ( (cid:98)D) → 0,

∀f ∈ BL1.

Several remarks on Theorem 4.1 are in order. First, Theorem 4.1 shows that the asymptotic
power of the CLC test with the weights (a1, a2) selected by the minimax procedure is the same as
that in the limit experiment when the weights equal As(D, γ(β0)), which is a function of D. Given
that D is independent of both normal random variables in φa1,a2,∞(δ) in (3.3), the jackknife CLC
test is asymptotically admissible conditional on (cid:98)D among the tests speciﬁed in Theorem 2.1(i).
Second, we see that the power of our jackknife CLC test is EφAs(D,γ(β0)),∞(∆, µD, γ(β0)), which
does not exactly match the minimax power

Eφa1(µD,γ(β0)),a2(µD,γ(β0)),∞(∆, µD, γ(β0))

in the limit problem. This is because under weak identiﬁcation, it is impossible to consistently
estimate µD, or equivalently, the concentration parameter. A similar result holds under weak
identiﬁcation with a ﬁxed number of moment conditions in I.Andrews (2016). The best we can do
is to approximate µD by reasonable estimators based on D such as fpp(D, γ(β0)) and fkrs(D, γ(β0)).
Last, Theorem 4.1 implies that our jackknife CLC test controls size asymptotically conditionally
on (cid:98)D, and thus, unconditionally.

Next, we consider the performance of (cid:98)φAs( (cid:98)D,(cid:98)γ(β0)) deﬁned in (3.7) under strong identiﬁcation

and local alternatives.

8We assume that C

0 = +∞ if C > 0 and min(C, +∞) = C.

18

Theorem 4.2. Suppose that Assumptions 1 and 2 hold. Further suppose that we are under strong

identiﬁcation and local alternatives as described in Lemma 2.1. Then, for s ∈ {pp, krs}, we have

A1,s( (cid:98)D, (cid:98)γ(β0))

p
−→ 0, A2,s( (cid:98)D, (cid:98)γ(β0))ρ

p

−→ 0,

and

(cid:98)φAs( (cid:98)D,(cid:98)γ(β0))

(cid:32) 1{N ∗2

2 ≥ Cα},

where N ∗
2

d= N

(cid:16)

(cid:17)
(cid:101)∆ (cid:101)C
[(1−ρ2)Ψ]1/2 , 1

.

Three remarks are in order. First, Theorem 4.2 shows that under strong identiﬁcation and

local alternatives, our jackknife CLC test converges to the uniformly most powerful level-α test

characterized in Lemma 2.2. Therefore, it is more powerful than the jackknife AR and LM tests.

Second, under strong identiﬁcation and local alternatives, the JIVE-based Wald test proposed by

Chao et al. (2012) is asymptotically equivalent to the jackknife LM test, which implies that the

jackknife AR and JIVE-Wald-based two-step test in Mikusheva and Sun (2022) is also dominated

by the jackknife CLC test. Third, Theorem 4.2 shows that our jackknife CLC test is adaptive.
In practice, econometricians do not know whether or not the alternative β0 is close to the null
β. Therefore, our jackknife CLC test calibrates the power over all of the values δ can take (i.e.,
δ ∈ D(β0)), which includes both local and ﬁxed alternatives. Yet, Theorem 4.2 shows that the
minimax procedure can produce the most powerful test as if it is known that β0 is under local
alternatives.

Last, we show that, under strong identiﬁcation, the jackknife CLC test (cid:98)φAs( (cid:98)D,(cid:98)γ(β0)) deﬁned in

(3.7) has asymptotic power 1 against ﬁxed alternatives.

Theorem 4.3. Suppose Assumption 2 holds, and (Qe(β0)e(β0) − ∆2C, QXe(β0) − ∆C, QXX − C)(cid:62) =
Op(1). Further suppose that we are under strong identiﬁcation with ﬁxed alternatives so that ∆ =
β − β0 is nonzero and ﬁxed. Then, we have

(cid:98)φAs( (cid:98)D,(cid:98)γ(β0))

p

−→ 1.

5 Simulation

5.1 Power Curve Simulation for the Limit Problem

In this section, we simulate the power behavior of tests under the limit problem described in Section
2. We compare the following tests with a nominal rate of 5%: our jackknife CLC test in which µD is
estimated by the methods pp and krs, respectively, the one-sided jackknife AR test deﬁned in (2.5),

the jackknife LM test deﬁned in (2.6), and the test that is based on the orthogonalized jackknife
LM statistic LM ∗2(β0) deﬁned in this paper. The results below are based on 5,000 simulation
replications.

19

We set the parameter space for β as B = [−6/C, 6/C], where C = 3 and 6 represent weak and

strong identiﬁcation, respectively. The choice of parameter space follows that in I.Andrews (2016,
Section 7.2). We set β0 = 0, and the values of the covariance matrix in (2.2) are set as follows:
Φ1 = Ψ = Υ = 1, and Φ12 = Φ13 = τ = ρ, where ρ ∈ {0.2, 0.4, 0.7, 0.9}. We then compute γ(β0)
based on (2.4) as β ranges over B and generate AR(β0) and LM (β0) based on (2.3). Last, we
implement our CLC test purely based on AR(β0), LM (β0), γ(β0), and B without assuming the
knowledge of (C, β, Φ1, Ψ, Υ, Φ12, Φ13, τ ). We have tried to simulate under alternative settings of
the covariance matrix, and the obtained patterns of the power behavior are very similar.

Figures 1–4 plot the power curves for ρ = 0.2, 0.4, 0.7, and 0.9. In each ﬁgure, we report the

results under both weak and strong identiﬁcation (C = 3 and 6, respectively). We observe that

overall, the two jackknife CLC tests have the best power properties in terms of maximum regret.

Especially when the identiﬁcation is strong (C = 6) and/or the degree of endogeneity is not very low

(ρ = 0.4, 0.7, or 0.9), the jackknife CLC tests outperform their AR and LM counterparts by a large
margin. In addition, we notice that when C = 3, for some parameter values LM ∗(β0) can suﬀer
from substantial declines in power relative to the other tests, which is in line with our theoretical

predictions. By contrast, our jackknife CLC tests are able to guard against such substantial power

loss because of the adaptive nature of their minimax procedure.

Figure 1: Power Curve for ρ = 0.2

5.2 Simulation Based on Calibrated Data

We follow Angrist and Frandsen (2022) and Mikusheva and Sun (2022) and calibrate a data gener-

ating process (DGP) based on the 1980 census dataset from Angrist and Krueger (1991). Let the

instruments be

Zi = (cid:0)(1{Qi = q, Ci = c})q∈{2,3,4},c∈{31,··· ,39}, (1{Qi = q, Pi = p})q∈{2,3,4},p∈{51 states}

(cid:1),

20

−6−30360.00.20.40.60.81.0C = 3(b-b0)CPowerppkrsARLMLM*−6−30360.00.20.40.60.81.0C = 6(b-b0)CFigure 2: Power Curve for ρ = 0.4

Figure 3: Power Curve for ρ = 0.7

where Qi, Ci, Pi are individual i’s quarter of birth (QOB), year of birth (YOB) and place of birth
(POB), respectively, so that there are 180 instruments. Note that the dummy with q = 1 and
c = 30 is omitted in Zi. We denote ˜Yi as income, ˜Xi as the highest grade completed, and ˜Wi as
the full set of YOB-POB interactions; that is,

˜Wi = (cid:0)1{Ci = c, Pi = p}c∈{30,...,39},p∈{51 states}

(cid:1),

which is a 510 × 1 matrix.

As in Angrist and Frandsen (2022), using the full 1980 sample (consisting of 329,509 individuals),
we ﬁrst obtain the average ˜Xi for each QOB-YOB-POB cell; we call this ¯s(q, c, p). Next we use

21

−6−30360.00.20.40.60.81.0C = 3(b-b0)CPower−6−30360.00.20.40.60.81.0C = 6(b-b0)C−6−30360.00.20.40.60.81.0C = 3(b-b0)CPower−6−30360.00.20.40.60.81.0C = 6(b-b0)CFigure 4: Power Curve for ρ = 0.9

LIML to estimate the structural parameters in the following linear IV regression:

˜Yi = ˜XiβX + ˜W (cid:62)

i βW + ei,

˜Xi = Z(cid:62)

i ΓZ + ˜W (cid:62)

i ΓW + Vi,

where ˜X is endogenous and is instrumented by Zi and ˜Wi is the exogenous control variable. Denote
the LIML estimate for βX,W ≡ (β(cid:62)
LIM L,W ). We let (cid:98)y(Ci, Pi) =
˜W (cid:62)

LIM L = ( (cid:98)β(cid:62)

LIM L,X , (cid:98)β(cid:62)

W )(cid:62) as (cid:98)β(cid:62)

X , β(cid:62)

i (cid:98)βLIM L,W and

ω(Qi, Ci, Pi) = ˜Yi − ˜Xi (cid:98)βLIM L,X − ˜W (cid:62)

i (cid:98)βLIM L,W .

Based on the LIML estimate and the calibrated ω(Qi, Ci, Pi), we simulate the following two

DGPs:

1. DGP 1:

(cid:101)yi = ¯y + β(cid:101)si + ω(Qi, Ci, Pi)(νi + κ2ξi)

(5.1)

(cid:101)si ∼ P oisson(µi),

(cid:80)n

where β is the parameter of interest, νi and ξi are independent standard normal, ¯y =
1
Z Zi is the projection of ¯si(q, c, p)
n
onto a constant and Zi. We set κ1 = 1.7 and κ2 = 0.1 as in Mikusheva and Sun (2022).

i=1 (cid:98)y(Ci, Pi), µi ≡ max{1, γ0 + γ(cid:62)

Z Zi + κ1νi}, and γ0 + γ(cid:62)

2. DGP 2: Same as DGP 1 except that κ1 = 2.7 and

(cid:101)si ∼ (cid:98)P oisson(2µi)/2(cid:99).

22

−6−30360.00.20.40.60.81.0C = 3(b-b0)CPower−6−30360.00.20.40.60.81.0C = 6(b-b0)CWe consider varying sample size n based on 0.5%, 1%, and 1.5% of the full sample size. Upon
obtaining n observations, we exclude instruments with (cid:80)n
i=1 Zij < 5. This results in small, medium,
and large samples with 1,648, 3,296, and 4,943 observations and 119, 142, and 150 numbers of IVs,

respectively. Our DGP 1 is exactly the same as that in Mikusheva and Sun (2022), which has
ρ = 0.41. Our DGP 2 has ρ = 0.7. The concentration parameters (deﬁned as C/Υ1/2) for small,
medium, and large samples are 2.15, 3.62, and 4.85, respectively, for DGP 1, and 2.38, 3.97, 5.28,

respectively, for DGP 2.

We emphasize that following Angrist and Frandsen (2022) and Mikusheva and Sun (2022), we
only use ˜Wi to compute the LIML estimator and calibrate ω(Qi, Ci, Pi), but do not use it to generate
new data. Therefore, for the simulated data, the outcome variable is ˜yi, the endogenous variable
is ˜si, the IV Zi is viewed to be ﬁxed, and the exogenous control variable is just an intercept. We
then denote the demeaned versions of ˜yi and ˜si as Yi and Xi, respectively, in (2.1) and implement
various inference methods described below. Following Mikusheva and Sun (2022), we test the null
hypothesis that β = β0 for β0 = 0.1 while varying the true value β ∈ B. The parameter space
is set as B = [−0.5, 0.5], which is consistent with the choice of parameter space for the empirical

application below. The results below are based on 1,000 simulation repetitions. We provide more

details about the implementation in Section B in the Online Supplement.

We compare the following tests with a nominal rate of 5%:

1. pp: our jackknife CLC test when µD is estimated by the method pp.

2. krs: our jackknife CLC test when µD is estimated by the method krs.

3. AR: the one-sided jackknife AR test with the cross-ﬁt variance estimator proposed by Miku-

sheva and Sun (2022).

4. LM CF: Matsushita and Otsu’s (2021) jackknife LM test, but with a cross-ﬁt variance esti-

mator (details are given in Section A.2 in the Online Supplement).

5. 2-step: Mikusheva and Sun’s (2022) two-step estimator in which the overall size is set at 5%.

6. LM∗: LM∗ test deﬁned in this paper.

7. LM MO: Matsushita and Otsu’s (2021) original jackknife LM test.

Figures 5 and 6 plot the power curves of the aforementioned tests. We can make six observations.

First, all methods control size well because they are all weak identiﬁcation robust. Second, the

performance of the jackknife CLC test with krs is slightly better than than that with pp, which is

consistent with the power curve simulation in Section 5.1. Third, in DGP 1 with a small sample

size, the power of the jackknife AR test is about 9.2% higher than that of the krs test when β is

around -0.3. However, for alternatives close to the null (e.g., when β is around 0), the power of

23

Figure 5: Power Curve for DGP 1

the krs test is 24% higher, which implies that the power of the krs test is still better than that

for the jackknife AR test in the minimax sense. The power of the jackknife LM tests is similar

to that of the krs test in DGP 1 with a small sample size. Fourth, for the rest of the scenarios,

the power of the krs test is the highest in most regions of the parameter space. The power of the

jackknife AR and LM is at most 0.7% higher than that of the krs test at some point. For DGP

1 with medium and large sample sizes, the maximum power gaps between our krs test and the

jackknife LM are about 8.6% and 5.6%, and about 43.2% and 50% compared with the jackknife AR.

Furthermore, they are 23.3%, 19.5%, and 18.5% compared with the jackknife LM for DGP 2 with

small, medium, and large sample sizes, respectively, and about 41.5%, 55.3%, and 55.5% compared
with the jackknife AR. Fifth, Figures 7 and 8 show the average values of (a1, a2), the weights of
the jackknife AR and LM for our CLC tests, under DGPs 1 and 2, respectively. We observe that
the minimax procedure does not put all the weights on the LM∗ test. Furthermore, because the
jackknife AR is more powerful on the left side of the parameter space relative to the right, the
minimax weights for AR2(β0) (a1) are higher on the left than on the right. The summation of a1
and a2 is the lowest for alternatives that are close to the null, which is consistent with our theory
that LM∗ is most powerful for local alternatives. Compared with those for DGP 1, the weights for
DGP 2 are lower in general because the identiﬁcation is slightly stronger in this case. Last, although
the power of LM ∗2(β0) drops at both ends of the parameter space, the power of the jackknife CLC
tests remains stable. From Figures 7 and 8, we see that in those regions, more weights are put on
AR2(β0) and LM 2(β0).

24

−0.5−0.3−0.10.10.30.50.00.20.40.60.81.0SmallbProbability of rejection for H0: b0 = 0.1ppkrsARLM_CF2−stepLM*LM_MO−0.5−0.3−0.10.10.30.50.00.20.40.60.81.0Mediumb−0.5−0.3−0.10.10.30.50.00.20.40.60.81.0LargebFigure 6: Power Curve for DGP 2

6 Empirical Application

In this section, we consider the linear IV regressions with the speciﬁcation underlying Angrist and
Krueger (1991, Table VII, column (6)), using the full original dataset.9 The outcome variable Y
and endogenous variable X are log weekly wages and schooling, respectively. We follow Angrist

and Krueger (1991) and focus on two speciﬁcations with 180 and 1,530 instruments. The 180

instruments include 30 quarter and year of birth interactions (QOB-YOB) and 150 quarter and

place of birth interactions (QOB-POB). For the second speciﬁcation with 1530 instruments, we

also include full interactions among QOB-YOB-POB. The exogenous control variables have been

partialled out from the outcome and endogenous variables. More details of the empirical application

are given in Section C in the Online Supplement. The considered tests are similar to those in the
previous section. The jackknife AR test is deﬁned in (2.5) with (cid:98)Φ1 being the cross-ﬁt estimator in
Mikusheva and Sun (2022). The jackknife LM test is deﬁned in (2.6) with the cross-ﬁt estimator
for Ψ(β0). The pp and krs tests are our jackknife CLC tests. The two-step procedure is given
by Mikusheva and Sun (2022, Section 5). Speciﬁcally, the researcher accepts the null if (cid:101)F > 9.98
and W ald(β0) < C0.02
In the case of 180 instruments,
because (cid:101)F = 13.42 > 9.98, the lower and upper bounds of the 95% conﬁdence interval (CI) for
the two-step procedure correspond respectively to the minimum and maximum of the set {β0 ∈
(cid:60) : W ald(β0) < C0.02}; similarly, for the 1,530 instruments, as (cid:101)F = 6.32 ≤ 9.98, the lower and

10 or if (cid:101)F ≤ 9.98 and AR(β0) < z0.02.

9The

from MIT
https://economics.mit.edu/faculty/angrist/data1/data/angkru1991.

downloaded

dataset

can

be

Economics,

Angrist

Data

Archive,

10 (cid:101)F = QXX / (cid:98)Υ, where (cid:98)Υ is the cross-ﬁt estimator. W ald(β0) is deﬁned as

, where ˆβ is the JIVE estimator
and ˆV is a cross-ﬁt estimator of the asymptotic variance of ˆβ. We refer interested readers to Mikusheva and Sun
(2022, Section 5) for more details.

(cid:17)2

(cid:16) ˆβ−β0
ˆV

25

−0.5−0.3−0.10.10.30.50.00.20.40.60.81.0SmallbProbability of rejection for H0: b0 = 0.1−0.5−0.3−0.10.10.30.50.00.20.40.60.81.0Mediumb−0.5−0.3−0.10.10.30.50.00.20.40.60.81.0LargebFigure 7: Average Values of a for DGP 1

upper bounds of the CI for the two-step procedure correspond respectively to the minimum and
maximum of the set {β0 ∈ (cid:60) : AR(β0) < z0.02}. We also report the 95% Wald test CI based on the
JIVE estimator, denoted as JIVE-t. Table 1 reports the 95% CIs by inverting the corresponding

5% tests mentioned above for the parameter space B = [−0.5, 0.5]. Note all CIs except JIVE-t are
robust to weak identiﬁcation. As (cid:101)F ’s are higher than 4.14 in both cases, the JIVE-t (5%) has the
Stock and Yogo (2005b)-type guarantee with at most a 5% size distortion (i.e., the overall size is

less than 10%).

jackknife AR jackknife LM

(5%)
[0.008,0.201]
[-0.035,0.22]

(5%)
[0.067,0.135]
[0.036,0.138]

JIVE-t
(5%)
[0.066,0.132]
[0.035,0.133]

Two-step
(5%)
[0.059,0.139]
[-0.051,0.242]

pp
(5%)
[0.067,0.128]
[0.037,0.133]

krs
(5%)
[0.067,0.128]
[0.037,0.133]

180 IVs
1530 IVs

Table 1: Conﬁdence Intervals
Notes: The (cid:101)F ’s for 180 and 1,530 instruments are 13.42 and 6.32, respectively. The grid-search used for our
conﬁdence interval was over 10,000 equidistant grid-points for β0 ∈ [−0.5, 0.5]. Our jackknife AR conﬁdence
interval for 1530 instruments diﬀers from that in Mikusheva and Sun (2022) because they used year-of-birth
1930-1938 dummies for the QOB-YOB-POB interactions, whereas we used 1930-1939 dummies. More details
are provided in Section C in the Online Supplement.

Table 1 highlights that the CIs generated by our jackknife CLC tests are the shortest among

all the weak identiﬁcation robust CIs (i.e., pp, krs, jackknife AR, jackknife LM, and two-step).

Furthermore, the jackknife CLC CIs are 7.6% and 2.0% shorter than the non-robust JIVE-t CIs

with 180 and 1,530 instruments, respectively, which is in line with our theoretical result that the

CLC tests are adaptive to the identiﬁcation strength and eﬃcient under strong identiﬁcation.

26

−0.5−0.3−0.10.10.30.50.00.20.40.60.8SmallbAverage values of a for DGP1a1_ppa2_ppa1_krsa2_krs−0.5−0.3−0.10.10.30.50.00.20.40.60.8Mediumb−0.5−0.3−0.10.10.30.50.00.20.40.60.8LargebFigure 8: Average Values of a for DGP 2

7 Conclusion

In this paper, we consider a jackknife CLC test that is adaptive to the identiﬁcation strength in

IV regressions with many weak instruments. We show that the proposed test is (i) robust to weak

identiﬁcation, many instruments, and heteroskedasticity, (2) admissible under weak identiﬁcation

among some class of tests, and (3) uniformly most powerful among sign-invariant tests under strong

identiﬁcation against local alternatives. Simulation experiments conﬁrm the good power properties

of the jackknife CLC test.

27

−0.5−0.3−0.10.10.30.50.00.20.40.60.8SmallbAverage values of a for DGP2a1_ppa2_ppa1_krsa2_krs−0.5−0.3−0.10.10.30.50.00.20.40.60.8Mediumb−0.5−0.3−0.10.10.30.50.00.20.40.60.8LargebA Verifying Assumption 2

A.1 Standard Estimators

In this section, we maintain Assumption 4, which is stated below and just Mikusheva and Sun

(2022, Assumption 1).

Assumption 4. The observations (Yi, Xi, Zi)i∈[n] are i.i.d. Suppose P is an n×n projection matrix
of rank K, K → ∞ as n → ∞ and there exists a constant δ such that Pii ≤ δ < 1.

Following the results in Chao et al. (2012) and Mikusheva and Sun (2022), we can show that

under either weak or strong identiﬁcation, Assumption 1 in the paper holds:











Qee
QXe
QXX − C

(cid:32) N














 ,

0

0

0






Φ1 Φ12 Φ13
Φ12 Ψ
τ
Φ13

Υ

τ









 ,

(A.1)

where σ2

i = Ee2

i , η2

i = EV 2

i , γi = EeiVi, ωi = (cid:80)

j(cid:54)=i PijΠj,

Φ1 = lim
n→∞

Φ12 = lim
n→∞

Φ13 = lim
n→∞

2
K

1
K

2
K

(cid:88)

(cid:88)

i∈[n]
(cid:88)

j(cid:54)=i

(cid:88)

i∈[n]
(cid:88)

j(cid:54)=i

(cid:88)

i∈[n]

j(cid:54)=i

ijσ2
P 2

i σ2
j ,

ij(γjσ2
P 2

i + γiσ2

j ),

P 2

ijγiγj,

Ψ = lim
n→∞

τ = lim
n→∞

Υ = lim
n→∞













1
K

2
K

2
K

(cid:88)

(cid:88)

i∈[n]

j(cid:54)=i

(cid:88)

(cid:88)

i∈[n]

j(cid:54)=i

(cid:88)

(cid:88)

i∈[n]

j(cid:54)=i

ij(η2
P 2

i σ2

j + γiγj) +



i σ2
ω2
i

 ,

1
K

(cid:88)

i∈[n]


ijη2
P 2

i γj +

ijη2
P 2

i η2

j +

2
K

4
K

(cid:88)

i∈[n]

(cid:88)

i∈[n]

ω2

i γi

 ,

and



i η2
ω2
i

 .

We note that the standard estimators of the above variance components proposed by Crudu
et al. (2021) are equal to Chao et al.’s (2012) estimators with their residual ˆei replaced by ei(β0).
Speciﬁcally, let

(cid:98)Φ1(β0) =

2
K

(cid:88)

(cid:88)

i∈[n]

j(cid:54)=i

ije2
P 2

i (β0)e2

j (β0),

28

(cid:98)Φ12(β0) =

(cid:98)Φ13(β0) =

(cid:98)Ψ(β0) =

(cid:98)τ (β0) =

(cid:98)Υ =

1
K

2
K

1
K

1
K

2
K

(cid:88)

(cid:88)

i∈[n]
(cid:88)

j(cid:54)=i

(cid:88)

ij(Xjej(β0)e2
P 2

i (β0) + Xiei(β0)e2

j (β0)),

P 2

ijXiei(β0)Xjej(β0),

i∈[n]
(cid:88)

i∈[n]
(cid:88)

j(cid:54)=i

(cid:88)
(

j(cid:54)=i

(cid:88)
(

PijXj)2e2

i (β0) +

1
K

PijXj)2Xiei(β0) +

i∈[n]
(cid:88)

j(cid:54)=i

(cid:88)

i∈[n]

j(cid:54)=i

ijX 2
P 2

i X 2
j .

(cid:88)

(cid:88)

P 2

ijXiei(β0)Xjej(β0)),

j(cid:54)=i

i∈[n]
1
K

(cid:88)

i∈[n]

ijX 2
P 2

i Xjej(β0),

and

(cid:88)

j(cid:54)=i

Assumption 5. Suppose maxi∈[n] |Πi| ≤ C, p1/4
n
maxi∈[n] Pii.

Π(cid:62)Π
K = o(1), and E(e6

i + V 6

i ) < ∞, where pn =

Two remarks on Assumption 5 are in order. First, maxi∈[n] |Πi| ≤ C is mild because Πi = EXi.
Second, Assumption 5 allows for weak identiﬁcation when Π(cid:62)Π/
K → c for a constant c. It also
allows for strong identiﬁcation when Π(cid:62)Π/
K → ∞. In this case, if K/n → 0 so that pn = o(1),
we allow Π(cid:62)Π/K → c for a positive constant c. Otherwise, if K is proportional to n, Assumption
5 requires Π(cid:62)Π/K → 0. Such a restriction is needed because Assumption 2 includes the case of
ﬁxed alternatives (i.e., ﬁxed ∆ (cid:54)= 0), which is not considered in Crudu et al. (2021) and Chao et al.

√

√

(2012).

Theorem A.1. Suppose Assumptions 4 and 5 hold. Then Assumption 2 holds for Crudu et al.’s

(2021) estimators deﬁned above.

A.2 Cross-Fit Estimators

Let M = I − P , Mij be the (i, j) element of M , Mi be the ith row of M , and (cid:101)P 2
Then, Mikusheva and Sun (2022) consider the cross-ﬁt estimators for Φ1(β0), Ψ(β0), and Υ deﬁned
as

ij =

.

P 2
ij
MiiMjj +M 2
ij

(cid:98)Φ1(β0) =

(cid:98)Ψ(β0) =

(cid:98)Υ =

2
K

1
K

2
K

(cid:88)

(cid:88)

(cid:101)P 2
ij[ei(β0)Mie(β0)][ej(β0)Mje(β0)],

j(cid:54)=i

i∈[n]


(cid:88)

(cid:88)
(



i∈[n]

j(cid:54)=i

PijXj)2 ei(β0)Mie(β0)

Mii

(cid:88)

(cid:88)

+

i∈[n]

j(cid:54)=i

(cid:88)

(cid:88)

i∈[n]

j(cid:54)=i

(cid:101)P 2
ij[Xi(β0)MiX][Xj(β0)MjX],

(cid:101)P 2
ijMiXei(β0)MjXej(β0)

 ,

and



29

where X and e(β0) are the column vectors that collect all Xi and ei(β0), respectively. Following
their lead, we can construct the cross-ﬁt estimators for the rest three elements in γ(β0) as follows:

(cid:98)Φ12(β0) =

(cid:98)Φ13(β0) =

(cid:98)τ (β0) =

1
K

2
K

1
K

(cid:88)

(cid:88)

i∈[n]
(cid:88)

j(cid:54)=i

(cid:88)

i∈[n]

j(cid:54)=i

(cid:88)

(cid:88)

i∈[n]

j(cid:54)=i

(cid:101)P 2
ij(MjXej(β0)ei(β0)Mie(β0) + MiXei(β0)ej(β0)Mje(β0)),

(cid:101)P 2
ijMiXei(β0)MjXej(β0),

and

(cid:101)P 2
ij(XiMiX)(MjXej(β0)) +

1
K

(cid:88)

(cid:88)

(

PijXj)2

i∈[n]

j(cid:54)=i

(cid:18) ei(β0)MiX
2Mii

+

XiMie(β0)
2Mii

(cid:19)

,

Assumption 6. Suppose Assumption 5 holds. Further suppose that Π(cid:62)M Π ≤ CΠ(cid:62)Π
constant C > 0.

K

for some

Compared with the assumptions in Mikusheva and Sun (2022), Assumption 6 further requires
that maxi∈[n] |Πi| ≤ C. However, it allows for the case that Π(cid:62)Π/K → c, where c is a nonzero
constant, as long as pn = o(1), which is weaker than those in Mikusheva and Sun (2022) (e.g.,
Theorems 3 and 5 in their paper require Π(cid:62)Π/K → 0 and Π(cid:62)Π/K2/3 → 0, respectively, for the
consistency of the cross-ﬁt variance estimators).

Lemma A.1. Suppose Assumptions 4 and 6 hold. Then, Lemmas 2, 3, S3.1, S3.2 in Mikusheva

and Sun (2022) hold.

Theorem A.2. Suppose Assumptions 4 and 6 hold. Then, Assumption 2 holds for Mikusheva and

Sun’s (2022) cross-ﬁt estimators deﬁned above.

B Details for Simulations Based on Calibrated Data

The DGP contains only the intercept as the control variable. Therefore, we implement our jackknife
CLC test on the demeaned version of (˜yi, ˜si, Zi). The parameter space is B = [−0.5, 0.5]. We test the
null hypothesis that β = β0 for β0 = 0.1 while varying the true value β over 30 equal-spaced grids
over B. The grids for δ is the grid for β minus β0. We generate grids of (a1, a2) as a1 = sin2(t1) and
a2 = cos2(t1) sin2(t2) with t1 taking values over 15 equal-spaced grids over [a1/2(fs( (cid:98)D, (cid:98)γ(β0)), π/2]
and t2 taking values over 15 equal-spaced grids over [0, π/2]. We gauge E∗φa1,a2,s(δ, (cid:98)D, (cid:98)γ(β0)) via a
Monte Carlo integration with N = 2000 draws of independent standard normal random variables. In
practice, it is rare but possible that As( (cid:98)D, (cid:98)γ(β0)) deﬁned in (3.6) is not unique. To increase numeri-
cal stability, we follow I.Andrews (2016) and allow for some slackness in the minimization. Let Ga be
the grid of (a1, a2) mentioned above, (cid:98)Q(a1, a2) = supδ∈D(β0)(Pδ,s( (cid:98)D, (cid:98)γ(β0))−E∗φa1,a2,s(δ, (cid:98)D, (cid:98)γ(β0))),
(cid:98)Qmin = min(a1,a2)∈Ga (cid:98)Q(a1, a2) + 1/n, where n is the sample size, and

Ξ = {(a1, a2) ∈ Ga : (cid:98)Q(a) ≤ (cid:98)Qmin + ( (cid:98)Qmin(1 − (cid:98)Qmin))1/2(2 log(log(N )))1/2N −1/2}.

30

The slackness term in the deﬁnition of Ξ is due to the law of the iterated logarithm for sum of

Bernoulli random variables and captures the randomness of the Monte Carlo integration. Suppose
there are L elements in Ξ, which are denoted as {(a1,l, a2,l)}L
l=1. We then deﬁne As( (cid:98)D, (cid:98)γ(β0)) as
(a1,(cid:98)L/2(cid:99), a2,(cid:98)L/2(cid:99)). We use the cross-ﬁt estimators deﬁned in Section A.2 throughout the simulation.

C Details for Empirical Application

We consider the 1980s census of 329,509 men born in 1930-1939 based on Angrist and Krueger’s

(1991) dataset. The model for 180 instruments follows Mikusheva and Sun (2022), which can be

written explicitly as

ln Wi = Constant + H (cid:62)

i ζ +

Ei = Constant + H (cid:62)

i λ +

38
(cid:88)

c=30

38
(cid:88)

c=30

Y OBi,cξc +

(cid:88)

s(cid:54)=56

P OBi,sηs + βEi + γi

Y OBi,cµc +

(cid:88)

s(cid:54)=56

P OBi,sαs

3
(cid:88)

(cid:88)

+

j=1

s(cid:54)=56

QOBi,jP OBi,sδc,s +

3
(cid:88)

39
(cid:88)

j=1

c=30

QOBi,jY OBi,cθj,c + εi,

where Wi is the weekly wage, Ei is the education of the i-th individual, Hi is a vector of covariates,11
Y OBi,c is a dummy variable indicating whether the individual was born in year c = {30, 31, ..., 39},
while QOBi,j is a dummy variable indicating whether the individual was born in quarter-of-birth
j ∈ {1, 2, 3, 4}. P OBi,s is the dummy variable indicating whether the individual was born in state
s ∈ {51 states}.12 The coeﬃcient β is the return to education. We vary this β across 10,000
equidistant grid-points from -0.5 to 0.5 (i.e., β ∈ {−0.5, −4.9999, −4.9998, ..., 0, ..., 4.9999, 0.5}) and

solve for the range of β where the null hypothesis cannot be rejected. Speciﬁcally, we can write the

above model as

ln Wi = CiΓ + βEi + γi

Ei = Ciτ + ZiΘ + εi,

where Ci is a (329,509×71)-matrix of controls containing the ﬁrst four terms on the right-hand
of the ﬁrst equation, while Zi is the (329,509×180)-matrix of instruments containing the ﬁrst two
terms in the third line. We can then partial out the controls Ci by multiplying each equation by

11The covariates we consider are: RACE, MARRIED, SMSA, NEWENG, MIDATL, ENOCENT, WNOCENT,

SOATL, ESOCENT, WSOCENT, and MT.

12The state numbers are from 1 to 56, excluding (3,7,14,43,52), corresponding to U.S. state codes.

31

the residual matrix I − C(C(cid:62)C)−1C(cid:62) to obtain a form analogous to that in the main text:

Yi = Xiβ + ei,

Xi = Πi + vi.

Then, at each grid-point we take β0 = β and compute AR(β0), LM (β0), W ald(β0), (cid:98)φApp( (cid:98)D,(cid:98)γ(β0))
and (cid:98)φAkrs( (cid:98)D,(cid:98)γ(β0)). We reject the chosen value of β0 for AR(β0) if it exceeds the one-sided 5%-
If LM (β0)2 > C0.05, we reject
quantile of the standard normal (i.e., reject if AR(β0) > z0.05).
the chosen β0 for Jackknife LM. If W ald(β0) > C0.05, we reject for JIVE-t.
If (cid:98)φAs( (cid:98)D,(cid:98)γ(β0)) >
C0.05(As( (cid:98)D, (cid:98)γ(β0)); (cid:98)ρ(β0)) for s ∈ {pp, krs}, we reject accordingly. The two-step procedure depends
on the value of (cid:101)F . If (cid:101)F > 9.98, we reject if W ald(β0) > C0.02; otherwise if (cid:101)F ≤ 9.98, we reject if
AR(β0) > z0.02.

The model for 1,530 instruments can be written explicitly as

ln Wi = Constant + H (cid:62)

i ζ +

Ei = Constant + H (cid:62)

i λ +

38
(cid:88)

c=30

38
(cid:88)

c=30

Y OBi,cξc +

(cid:88)

s(cid:54)=56

P OBi,sηs + βEi + γi.

Y OBi,cµc +

(cid:88)

s(cid:54)=56

P OBi,sαs

3
(cid:88)

39
(cid:88)

+

(cid:88)

j=1

c=30

s∈{51 states}

QOBi,jY OBi,cP OBi,sδj,c,s.

The main diﬀerence between this 1,530-instrument speciﬁcation and the 180-instrument one is that

we now have QOB-YOB-POB interactions as our instruments, compared with QOB-YOB and

QOB-POB interactions in the case of 180 instruments. Note that in both cases, only quarter-of-

birth 1–3 are used; quarter 4 is omitted in order to avoid multicollinearity.

D Proof of Lemma 2.1

Under strong identiﬁcation, by (2.3) and Assumption 2, we have



1 0

0




0 1
0
0 0 dn











Qee
QXe
QXX






(cid:32) N





0












 ,
0
(cid:101)C






Φ1 Φ12 0

Φ12 Ψ 0

0
0
0




 ,

In addition, we note that ei(β0) = ei + Xi∆ with ∆ = dn (cid:101)∆ → 0. Therefore, Under strong

32

identiﬁcation, we have C∆ = (cid:101)C (cid:101)∆,

Qe(β0)e(β0) = Qee + 2∆QXe + ∆2QXX = Qee + op(1),
QXe(β0) = QXe + ∆QXX = QXe + (cid:101)C (cid:101)∆ + op(1).

This implies

(cid:32)

AR(β0)
LM (β0)

(cid:33)

=

(cid:32)

Qe(β0)e(β0)/(cid:98)Φ1/2
QXe(β0)/ (cid:98)Ψ1/2

1

1

(cid:33)

(cid:32)(cid:32)

(cid:32) N

0
(cid:101)C (cid:101)∆
Ψ1/2

(cid:33)

(cid:32)

(cid:33)(cid:33)

1 ρ

,

ρ 1

.

E Proof of Lemma 2.2

Recall N ∗

2 = (1 − ρ2)−1/2(N2 − ρN1) and

(cid:33)

(cid:32)

N1
N ∗
2

d= N

(cid:32)(cid:32)

0
θ
(1−ρ2)1/2

(cid:33)

(cid:32)

,

(cid:33)(cid:33)

.

1 0

0 1

Because ρ is known, it suﬃces to construct the uniformly most powerful invariant test based on
observations (N1, N ∗
2 ). As the null and alternative are invariant to sign changes, the maximum in-
variant is (N1, N ∗2
2 ). Then, Lehmann and Romano (2006, Theorem 6.2.1) implies the invariant test
should be based on the maximum invariant. Note (N1, N ∗2
2 ) are independent, N1 follows a standard
normal distribution, and N ∗
2 follows a noncentral chi-square distribution with one degree of freedom
and noncentrality parameter λ = θ2
1−ρ2 . Therefore, by the Neyman-Pearson’s Lemma (Lehmann
and Romano (2006, Theorem 3.2.1)), the most powerful test based on observations (N1, N ∗2
2 ) is the
likelihood ratio test where the likelihood ratio function evaluated at (N1 = (cid:96)1, N ∗2
2 = (cid:96)2) depends
on (cid:96)2 only and can be written as

LR ((cid:96)2; λ) = −

λ
2

+ log

√

(cid:18) exp(

√

λ(cid:96)2) + exp(−
2

(cid:19)

λ(cid:96)2)

.

In addition, we note that LR ((cid:96)2; λ) is monotone increasing in (cid:96)2 for any λ ≥ 0 and (cid:96)2 ≥ 0. Therefore,
Lehmann and Romano (2006, Theorem 3.4.1) implies the likelihood ratio test is equivalent to
2 ≥ Cα}, which is uniformly most powerful among tests for λ = 0 v.s. λ > 0 and based on
1{N ∗2
observations (N1, N ∗2
2 ) only. This means it is also the uniformly most powerful test that is invariant
to sign changes.

33

F Proof of Lemma 2.3

Under strong identiﬁcation and ﬁxed alternatives, because (Qe(β0)e(β0) − ∆2C, QXe(β0) − ∆C, QXX −
C)(cid:62) = Op(1), we have

(cid:32)

dnAR(β0)
dnLM (β0)

(cid:33)





p
−→



 .

∆2 (cid:101)C

Φ1/2
1

(β0)

∆ (cid:101)C
Ψ1/2(β0)

This implies

dnLM ∗(β0)

p
−→

1
(1 − ρ2(β0))1/2

(cid:32)

∆ (cid:101)C
Ψ1/2(β0)

−

(cid:33)

ρ(β0)∆2 (cid:101)C
Φ1/2
(β0)
1

,

which leads to the desired result.

G Proof of Lemma 2.4

Under weak identiﬁcation, (2.3) implies

(cid:32)

Qe(β0)e(β0)
QXe(β0)

(cid:33)

(cid:32)

=

Qee + 2∆QXe + ∆2QXX
QXe + ∆QXX

(cid:33)

(cid:32) N

(cid:32)(cid:32)

∆2C
∆C

(cid:33)

,

(cid:32)

Φ1(β0) Φ12(β0)
Φ12(β0) Ψ(β0)

(cid:33)(cid:33)

,

which leads to the ﬁrst result.

For the second result, it is obvious that m1(∆) → CΥ−1/2. In addition, we have

m2(∆) =

C (cid:0)∆Φ1(β0) − ∆2Φ12(β0)(cid:1)

(Φ1(β0)(Φ1(β0)Ψ(β0) − Φ2

12(β0)))1/2
ρ23
(1 − ρ2

23)1/2

,

→

τ C
(Υ(ΥΨ − τ 2))1/2

=

C
Υ1/2

where we use the fact that

Φ1(β0)/∆4 → Υ,
(Φ1(β0)Ψ(β0) − Φ2
Φ1(β0) − ∆Φ12(β0)
∆3

→ τ.

12(β0))/∆4 → ΥΨ − τ 2,

H Proof of Theorem 2.1

Theorem 2.1(i) is a direct consequence of Marden (1982, Theorem 2.1) because the acceptance
region A = {(A, B) : s1A2 + s2B2 ≤ Cα(a1, a2; ρ(β0))} is closed, convex, and monotone decreasing

34

in the sense that if (A, B) ∈ A and A(cid:48) ≤ A, B(cid:48) ≤ B, then (A(cid:48), B(cid:48)) ∈ A.
For Theorem 2.1(ii), we note that ˜ρ = ρ under local alternatives and

φa1,a2,∞ = 1

(a1 + a2ρ2)N 2

1 + 2a2ρ(1 − ρ2)1/2N1N ∗

2 + (1 − a1 − a2ρ2)N ∗2

(cid:111)
2 ≥ Cα(a1, a2; ρ)

.

(cid:110)

The “if” part of Theorem 2.1(ii) is a direct consequence of Lemma 2.2. The “only if” part of

Theorem 2.1(ii) is a direct consequence of the the necessary part of Lehmann and Romano (2006,
Theorem 3.2.1). Speciﬁcally, given N1 and N ∗
2 are independent, the “only if” part requires a1 +
a2ρ2 = 0, which implies a1 = 0 and a2ρ = 0.

For Theorem 2.1(iii), we consider two cases of ﬁxed alternatives: (1) ∆ (cid:54)= Φ1/2

(β0)Ψ−1/2(β0)ρ−1(β0)

1

(β0)Ψ−1/2(β0)ρ−1(β0).

In Case (1), by Lemma 2.3, the limits of d2

nAR2(β0),

nLM ∗2(β0) are all positive, implies that for all (a1,n, a2,n) ∈ A0

and (2) ∆ = Φ1/2
nLM 2(β0), d2
d2

1

1{a1,nAR2(β0) + a2,nLM 2(β0) + (1 − a1,n − a2,n)LM ∗2(β0) ≥ Cα(a1,n, a2,n; (cid:98)ρ(β0))}

p

−→ 1.

In Case (2), we have

P (cid:0)a1,nAR2(β0) + a2,nLM 2(β0) + (1 − a1,n − a2,n)LM ∗2(β0) ≥ Cα(a1,n, a2,n; (cid:101)ρ(β0))(cid:1)

(cid:32)

≥ P

˜qΨ2(β0)ρ4(β0)
(cid:101)C2Φ1(β0)

nAR2(β0) ≥ Cα(a1,n, a2,n; (cid:98)ρ(β0)
d2

(cid:33)

= P (˜q + op(1) ≥ Cα,max(ρ(β0))) → 1,

where the ﬁrst inequality follows from the restriction on a1.n and the facts that LM 2(β0) ≥ 0 and
∗(β0) ˜C2 (by Lemma 2.3)
LM ∗2(β0) ≥ 0, the ﬁrst equality follows from d2
nAR2(β0)
p
−→ ρ(β0), and the last convergence follows from the fact that ˜q > Cα,max(ρ(β0)). This
and (cid:98)ρ(β0)
concludes the proof.

1 (β0)∆4

−→ Φ−1

p

I Proof of Theorem 4.1

We are under weak identiﬁcation. By Lemma 2.4 and Assumption 2, we have











AR(β0)
LM ∗(β0)
(cid:98)D






(cid:32) N






m1(∆)
m2(∆)
µD





1 0





0


 ,




0 1
0
0 0 σ2
D





 .

This implies (AR(β0), LM ∗(β0), (cid:98)D) are asymptotically independent. In addition, by Assumption
3, we have

(AR2(β0), LM ∗2(β0), As( (cid:98)D, (cid:98)γ(β0))) (cid:32) (Z 2(m1(∆)), Z 2(m2(∆)), As(D, γ(β0)))

35

where the two normal random variables are independent and independent of D, and by deﬁnition,
As(D, γ(β0))) = (a1(fs(D, γ(β0)), γ(β0)), a2(fs(D, γ(β0)), γ(β0))). In addition, we have (cid:98)ρ(β0)
ρ(β0). By the bounded convergence theorem, this further implies

p
−→

E (cid:98)φAs( (cid:98)D,(cid:98)γ(β0)) → Eφa1(fs(D,γ(β0)),γ(β0)),a2(fs(D,γ(β0)),γ(β0)),∞(∆, µD, γ(β0)).

(I.1)

In addition, suppose the null holds so that ∆ = 0. This implies m1(∆) = m2(∆) = 0. Then,

we have

( (cid:98)φAs( (cid:98)D,(cid:98)γ(β0)) − α)f ( (cid:98)D) (cid:32) (φa1(fs(D,γ(β0)),γ(β0)),a2(fs(D,γ(β0)),γ(β0)),∞(0, µD, γ(β0)) − α)f (D),

where

φa1(fs(D,γ(β0)),γ(β0)),a2(fs(D,γ(β0)),γ(β0)),∞(0, µD, γ(β0))






= 1

a1(fs(D, γ(β0)), γ(β0))Z 2

1 + a2(fs(D, γ(β0)), γ(β0))(ρ(β0)Z1 + (1 − ρ2(β0))1/2Z2)

(1 − a1(fs(D, γ(β0)), γ(β0)) − a2(fs(D, γ(β0)), γ(β0)))Z 2
2
≥ Cα(a1(fs(D, γ(β0)), γ(β0)), a2(fs(D, γ(β0)), γ(β0)); ρ(β0))






,

Z1 and Z2 are independent standard normals, and they are independent of D. Then, by the
deﬁnition of Cα(·), we have

E( (cid:98)φAs( (cid:98)D,(cid:98)γ(β0)) − α)f ( (cid:98)D) → E (cid:2)E (cid:0)φa(fs(D,γ(β0)),γ(β0)),∞(0, µD, γ(β0)) − α|D(cid:1) f (D)(cid:3) = 0.

J Proof of Theorem 4.2

Denote cB = cB(β) and ∆∗ = ∆∗(β). By Assumption 2, Φ1 > 0, which implies |∆∗| > 0. Un-
der strong identiﬁcation and local alternatives, we have ∆ → 0, cB(β0) → cB, ∆∗(β0) → ∆∗,
Cα,max(ρ(β0)) → Cα,max(ρ), and






AR(β0)
LM ∗(β0)
dn (cid:98)D






(cid:32) N













0
(cid:101)∆ (cid:101)C
((1−ρ2)Ψ)1/2
(cid:101)C









,















.

1 0 0

0 1 0

0 0 0

√

This implies dn(cid:98)σD
we note that

(cid:98)r = dn (cid:98)D

p
−→ (cid:101)C, which further implies dnfpp( (cid:98)D, (cid:98)γ(β0))

p
−→ (cid:101)C. For fkrs( (cid:98)D, (cid:98)γ(β0)),

max((cid:98)r − 1, 0) ≤ (cid:98)rkrs ≤ (cid:98)r.

36

Therefore, we also have fkrs( (cid:98)D, (cid:98)γ(β0))dn
Then, for an arbitrary ε > 0, we have P(En(ε)) ≥ 1 − ε when n is suﬃciently large.

p
−→ (cid:101)C. Let En(ε) = {||(cid:98)γ(β0) − γ(β0)|| + |δn (cid:98)D − (cid:101)C| ≤ ε}.

Denote δ = dn(cid:101)δ. We have

As( (cid:98)D, (cid:98)γ(β0)) ∈

arg min
(a1,a2)∈A(fs( (cid:98)D,(cid:98)γ(β0)),(cid:98)γ(β0))

sup
(cid:101)δ∈ (cid:101)Dn

(cid:16)

(cid:17)
Pdn(cid:101)δ,s( (cid:98)D, (cid:98)γ(β0)) − E∗φa1,a2,s(dn(cid:101)δ, (cid:98)D, (cid:98)γ(β0))

,

where (cid:101)Dn = {(cid:101)δ : dn(cid:101)δ ∈ D(β0)}. Let

Qn(a1, a2, (cid:101)δ) = Pdn(cid:101)δ,s( (cid:98)D, (cid:98)γ(β0)) − E∗φa1,a2,s(dn(cid:101)δ, (cid:98)D, (cid:98)γ(β0))
2 ((1 − ρ2)−1/2Ψ−1/2(cid:101)δ (cid:101)C) ≥ Cα}
Q(a1, a2, (cid:101)δ) = E1{Z 2
(cid:16)

ρZ1 + (1 − ρ2)1/2Z2((1 − ρ2)−1/2Ψ−1/2(cid:101)δ (cid:101)C)

and

− E1






1 + a2

a1Z 2
+(1 − a1 − a2)Z 2

2 ((1 − ρ2)−1/2Ψ−1/2(cid:101)δ (cid:101)C) ≥ Cα(a1, a2; ρ)

(cid:17)2






,

where Z1 is standard normal, Z2((1 − ρ2)−1/2Ψ−1/2(cid:101)δ (cid:101)C) is normal with mean (1 − ρ2)−1/2Ψ−1/2(cid:101)δ (cid:101)C
and unit variance, and Z1 and Z2(·) are independent. Then, we aim to show that

sup
(a1,a2)∈A(fs( (cid:98)D,(cid:98)γ(β0)),(cid:98)γ(β0)),(cid:101)δ∈ (cid:101)Dn

(cid:12)
(cid:12)
Qn(a1, a2, (cid:101)δ) − Q(a1, a2, (cid:101)δ)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

p

−→ 0.

(J.1)

We divide (cid:101)Dn into three parts:

(cid:101)Dn,1(ε) = {(cid:101)δ ∈ (cid:101)Dn, |(cid:101)δ| ≤ M1(ε)},
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:101)Dn,2(ε) =

(cid:101)δ ∈ (cid:101)Dn,

− 1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:40)

dn(cid:101)δ
(cid:98)∆∗(β0)
n,1(ε) ∩ (cid:101)Dc

(cid:101)Dn,3(ε) = (cid:101)Dn ∩ (cid:101)Dc

n,2(ε),

(cid:41)

≤ ε

,

and

where M1(ε) is a large constant so that

(cid:32)

(cid:32)

P

(1 − a)Z 2

(cid:33)

M 2
1 (ε)ε2 (cid:101)C2
2(1 − ρ2)ΨcB

(cid:33)

≥ Cα,max(ρ) + 1

= 1 − ε.

(J.2)

When n is suﬃciently large and ε is suﬃciently small, on En(ε), there exists a constant c such that

| (cid:98)∆∗(β0) − ∆∗| ≤ cε,

|(cid:98)Φ1(β0) − Φ1| ≤ cε,

inf
(cid:101)δ∈ (cid:101)Dn,2(ε)
nf 2
|d2

s ( (cid:98)D, (cid:98)γ(β0)) − (cid:101)C2| ≤ cε,

|dn(cid:101)δ| ≥ (1 − ε)(|∆∗| − cε),

37

sup
(cid:101)δ∈ (cid:101)Dn,2(ε)

1 − (∆2

≤


1 − (d2

n(cid:101)δ2, dn(cid:101)δ)





(cid:32)

∗, ∆∗)





(cid:32)

Φ1 Φ12
Φ12 Ψ

(cid:98)Φ1(β0)
(cid:98)Φ12(β0)

(cid:33)−1 (cid:32)

(cid:98)Φ12(β0)
(cid:98)Ψ(β0)
(cid:33)





2

Φ13
τ

(cid:33)−1 (cid:32)

(cid:98)Φ13(β0)
(cid:98)τ (β0)


2

(cid:33)




+ cε ≤ cB + cε,

(J.3)

|(cid:98)cB(β0) − cB| ≤ cε.

This further implies

(cid:101)Dn,1(ε) ∩ (cid:101)Dn,2(ε) = ∅.

Recall φa1,a2,s(δ, (cid:98)D, (cid:98)γ(β0)) deﬁned in (3.5). With δ replaced by dn(cid:101)δ and when (cid:101)δ ∈ (cid:101)Dn,1(ε), we

have

(cid:33)

(cid:32)

d−1
n (cid:98)C1(dn(cid:101)δ)
d−1
n (cid:98)C2(dn(cid:101)δ)

(dnfs( (cid:98)D, (cid:98)γ(β0)))

p
−→

(cid:32)

0
(1 − ρ2)−1/2Ψ−1/2(cid:101)δ (cid:101)C

(cid:33)

,

Therefore, uniformly over (a1, a2) ∈ A0 and (cid:101)δ ∈ (cid:101)Dn,1(ε) and conditional on data, we have

φa1,a2,s(dn(cid:101)δ, (cid:98)D, (cid:98)γ(β0)) (cid:32) 1

This implies






(cid:16)

1 + a2

a1Z 2
+(1 − a1 − a2)Z 2

ρZ1 + (1 − ρ2)1/2Z2((1 − ρ2)−1/2Ψ−1/2(cid:101)δ (cid:101)C)

2 ((1 − ρ2)−1/2Ψ−1/2(cid:101)δ (cid:101)C) ≥ Cα(a1, a2; ρ)

(cid:17)2






.

sup
(a1,a2)∈A0,(cid:101)δ∈ (cid:101)Dn,1(ε)

(cid:12)
(cid:12)
E∗φa1,a2,s(dn(cid:101)δ, (cid:98)D, (cid:98)γ(β0))
(cid:12)
(cid:12)

− E1






(cid:16)

1 + a2

a1Z 2
+(1 − a1 − a2)Z 2

ρZ1 + (1 − ρ2)1/2Z2((1 − ρ2)−1/2Ψ−1/2(cid:101)δ (cid:101)C)

2 ((1 − ρ2)−1/2Ψ−1/2(cid:101)δ (cid:101)C) ≥ Cα(a1, a2; ρ)

(cid:17)2






(cid:12)
(cid:12)
(cid:12)
(cid:12)

p

−→ 0.

In addition, by Lemma 2.2, for any (cid:101)δ, E1






(cid:16)

1 + a2

a1Z 2
+(1 − a1 − a2)Z 2

ρZ1 + (1 − ρ2)1/2Z2((1 − ρ2)−1/2Ψ−1/2(cid:101)δ (cid:101)C)

2 ((1 − ρ2)−1/2Ψ−1/2(cid:101)δ (cid:101)C) ≥ Cα(a1, a2; ρ)

(cid:17)2






is maximized at a1 = 0 and a2ρ = 0. This implies

sup
(cid:101)δ∈ (cid:101)Dn,1(ε)

|Pdn(cid:101)δ,s( (cid:98)D, (cid:98)γ(β0)) − E1{Z 2

2 ((1 − ρ2)−1/2Ψ−1/2(cid:101)δ (cid:101)C) ≥ Cα}|

= sup

(cid:101)δ∈ (cid:101)Dn,1(ε)

|

sup
(a1,a2)∈A(fs( (cid:98)D,(cid:98)γ(β0)),(cid:98)γ(β0))

E∗φa1,a2,s(dn(cid:101)δ, (cid:98)D, (cid:98)γ(β0)) − E1{Z 2

2 ((1 − ρ2)−1/2Ψ−1/2(cid:101)δ (cid:101)C) ≥ Cα}|

38





(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤ sup

(cid:101)δ∈ (cid:101)Dn,1(ε)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

sup
(a1,a2)∈A(fs( (cid:98)D,(cid:98)γ(β0)),(cid:98)γ(β0))

E1

(cid:16)

1 + a2

a1Z 2
+(1 − a1 − a2)Z 2

ρZ1 + (1 − ρ2)1/2Z2((1 − ρ2)−1/2Ψ−1/2(cid:101)δ (cid:101)C)

2 ((1 − ρ2)−1/2Ψ−1/2(cid:101)δ (cid:101)C) ≥ Cα(a1, a2; ρ)

(cid:17)2






− E1{Z 2

2 ((1 − ρ2)−1/2Ψ−1/2(cid:101)δ (cid:101)C) ≥ Cα}

+ op(1),

≤ sup

(cid:101)δ∈ (cid:101)Dn,1(ε)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

sup
(a1,a2)∈A0

E1






(cid:16)

1 + a2

a1Z 2
+(1 − a1 − a2)Z 2

ρZ1 + (1 − ρ2)1/2Z2((1 − ρ2)−1/2Ψ−1/2(cid:101)δ (cid:101)C)

2 ((1 − ρ2)−1/2Ψ−1/2(cid:101)δ (cid:101)C) ≥ Cα(a1, a2; ρ)

(cid:17)2






− E1{Z 2

2 ((1 − ρ2)−1/2Ψ−1/2(cid:101)δ (cid:101)C) ≥ Cα}

(cid:12)
(cid:12)
(cid:12)
(cid:12)

+ op(1) = op(1),

where the second inequality is due to the facts that a(fs( (cid:98)D, (cid:98)γ(β0)), (cid:98)γ(β0)) = op(1) under strong






(cid:16)

1 + a2

a1Z 2
+(1 − a1 − a2)Z 2

ρZ1 + (1 − ρ2)1/2Z2((1 − ρ2)−1/2Ψ−1/2(cid:101)δ (cid:101)C)

2 ((1 − ρ2)−1/2Ψ−1/2(cid:101)δ (cid:101)C) ≥ Cα(a1, a2; ρ)

(cid:17)2






is continuous at

identiﬁcation and E1

a1 = 0 uniformly over |(cid:101)δ| ≤ M1(ε). Therefore, we have

(cid:12)
(cid:12)
Qn(a1, a2, (cid:101)δ) − Q(a1, a2, (cid:101)δ)
sup
(cid:12)
(cid:12)
(a1,a2)∈A(fs( (cid:98)D,(cid:98)γ(β0)),(cid:98)γ(β0)),(cid:101)δ∈ (cid:101)Dn,1(ε)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

p

−→ 0.

(J.4)

Next, we consider the case when (cid:101)δ ∈ (cid:101)Dn,2(ε). We have

φa1,a2,s(dn(cid:101)δ, (cid:98)D, (cid:98)γ(β0))

(cid:16)

+a2

(cid:98)ρ(β0)Z1( (cid:98)C2




(cid:110)
a(fs( (cid:98)D, (cid:98)γ(β0)), (cid:98)γ(β0))Z 2

1 (dn(cid:101)δ)f 2
+(1 − a1 − a2)Z 2

1 ( (cid:98)C2

= 1

≥ 1

a1Z 2

1 ( (cid:98)C2

1 (dn(cid:101)δ)f 2

s ( (cid:98)D, (cid:98)γ(β0)))

s ( (cid:98)D, (cid:98)γ(β0))) + (1 − (cid:98)ρ2(β0))1/2Z2( (cid:98)C2

2 (dn(cid:101)δ)f 2

s ( (cid:98)D, (cid:98)γ(β0)))

2 ( (cid:98)C2
1 (dn(cid:101)δ)f 2

2 (dn(cid:101)δ)f 2

s ( (cid:98)D, (cid:98)γ(β0))) ≥ Cα(a1, a2; (cid:98)ρ(β0))

(cid:111)
s ( (cid:98)D, (cid:98)γ(β0))) ≥ Cα,max((cid:98)ρ(β0))

.

(cid:17)2





By (J.3), on En(ε), there exists a constant c > 0 such that

1 (dn(cid:101)δ)(dnfs( (cid:98)D, (cid:98)γ(β0)))2
(cid:98)C2

=


1 − (d2

n(cid:101)δ2, dn(cid:101)δ)



(cid:98)Φ−1
1 (β0)(dn(cid:101)δ)4(dnfs( (cid:98)D, (cid:98)γ(β0)))2

(cid:33)−1 (cid:32)

(cid:32)

(cid:98)Φ1(β0)
(cid:98)Φ12(β0)

(cid:98)Φ12(β0)
(cid:98)Ψ(β0)


2

(cid:33)




(cid:98)Φ13(β0)
(cid:98)τ (β0)

≥

(Φ1(β0) + cε)−1(1 − ε)4(|∆∗| − cε)4( (cid:101)C2 − cε)
cB + cε

≥ c

and

a(fs( (cid:98)D, (cid:98)γ(β0)), (cid:98)γ(β0)) (cid:98)C2

1 (dn(cid:101)δ)f 2

s ( (cid:98)D, (cid:98)γ(β0))

39

≥

≥

1.1Cα,max((cid:98)ρ(β0))(cid:98)Φ1(β0)(cid:98)cB(β0)
nf 2

s ( (cid:98)D, (cid:98)γ(β0))

∗(β0)d2

(cid:98)∆4

1 (dn(cid:101)δ)(dnfs( (cid:98)D, (cid:98)γ(β0)))2
(cid:98)C2

1.1Cα,max((cid:98)ρ(β0))(Φ1 − cε)(cB − cε)
(|∆∗| + cε)4( (cid:101)C2 + cε)

(Φ1(β0) + cε)−1(1 − ε)4(|∆∗| − cε)4( (cid:101)C2 − cε)
cB + cε

≥ (1.1 − cε)Cα,max((cid:98)ρ(β0)),

where the last inequality holds because ε can be arbitrarily small. This means, on En(ε) and when
(cid:101)δ ∈ (cid:101)Dn,2(ε),

E∗φa1,a2,s(dn(cid:101)δ, (cid:98)D, (cid:98)γ(β0)) ≥ P∗(op(1) + (1.1 − cε)Cα,max((cid:98)ρ(β0)) ≥ Cα,max((cid:98)ρ(β0))) → 1.

As P(En(ε)) → 1, we have

sup
(a1,a2)∈A(fs( (cid:98)D,(cid:98)γ(β0)),(cid:98)γ(β0)),(cid:101)δ∈ (cid:101)Dn,2(ε)

and thus,

(cid:104)
1 − E∗φa1,a2,s(dn(cid:101)δ, (cid:98)D, (cid:98)γ(β0))

(cid:105) p

−→ 0,

sup
(a1,a2)∈A(fs( (cid:98)D,(cid:98)γ(β0)),(cid:98)γ(β0)),(cid:101)δ∈ (cid:101)Dn,2(ε)
≤

sup
(a1,a2)∈A(fs( (cid:98)D,(cid:98)γ(β0)),(cid:98)γ(β0)),(cid:101)δ∈ (cid:101)Dn,2(ε)

(cid:104)

(cid:105)
Pdn(cid:101)δ,s( (cid:98)D, (cid:98)γ(β0)) − E∗φa1,a2,s(dn(cid:101)δ, (cid:98)D, (cid:98)γ(β0))
(cid:104)

(cid:105) p

1 − E∗φa1,a2,s(dn(cid:101)δ, (cid:98)D, (cid:98)γ(β0))

−→ 0.

(J.5)

Furthermore, note that a1 + a2 ≤ a < 1 and when (cid:101)δ ∈ (cid:101)Dn,2(ε), on En(ε), (J.3) implies (cid:101)δ2 → ∞.
Therefore, we have

a1Z 2

1 + a2

(cid:16)

ρZ1 + (1 − ρ2)1/2Z2((1 − ρ2)−1/2Ψ−1/2(cid:101)δ (cid:101)C)

(cid:17)2

+ (1 − a1 − a2)Z 2

2 ((1 − ρ2)−1/2Ψ−1/2(cid:101)δ (cid:101)C)

≥ (1 − a)Z 2

2 ((1 − ρ2)−1/2Ψ−1/2(cid:101)δ (cid:101)C) =

(1 − a)(cid:101)δ2 (cid:101)C2
(1 − ρ2)Ψ

(1 + op(1)) → ∞,

which further implies

sup
(a1,a2)∈A(fs( (cid:98)D,(cid:98)γ(β0)),(cid:98)γ(β0)),(cid:101)δ∈ (cid:101)Dn,2(ε)


1 − E1

and






(cid:16)

1 + a2

a1Z 2
+(1 − a1 − a2)Z 2

ρZ1 + (1 − ρ2)1/2Z2((1 − ρ2)−1/2Ψ−1/2(cid:101)δ (cid:101)C)
2 ((1 − ρ2)−1/2Ψ−1/2(cid:101)δ (cid:101)C) ≥ Cα(a1, a2; ρ)

(cid:17)2










p
−→ 0

sup
(a1,a2)∈A(fs( (cid:98)D,(cid:98)γ(β0)),(cid:98)γ(β0)),(cid:101)δ∈ (cid:101)Dn,2(ε)

(cid:20)
E1{Z 2

2 ((1 − ρ2)−1/2Ψ−1/2(cid:101)δ (cid:101)C) ≥ Cα}

40

− E1






(cid:16)

1 + a2

a1Z 2
+(1 − a1 − a2)Z 2

ρZ1 + (1 − ρ2)1/2Z2((1 − ρ2)−1/2Ψ−1/2(cid:101)δ (cid:101)C)

2 ((1 − ρ2)−1/2Ψ−1/2(cid:101)δ (cid:101)C) ≥ Cα(a1, a2; ρ)

(cid:17)2

(cid:21) p

−→ 0.






(J.6)

Combining (J.5) and (J.6), we have

(cid:12)
(cid:12)
Qn(a1, a2, (cid:101)δ) − Q(a1, a2, (cid:101)δ)
sup
(cid:12)
(cid:12)
(a1,a2)∈A(fs( (cid:98)D,(cid:98)γ(β0)),(cid:98)γ(β0)),(cid:101)δ∈ (cid:101)Dn,2(ε)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

→ 0.

(J.7)

Last, we consider the case in which (cid:101)δ ∈ (cid:101)Dn,3(ε). On En(ε), (J.3) implies

(cid:98)C2

2 (dn(cid:101)δ)f 2

s ( (cid:98)D, (cid:98)γ(β0))
)2

(cid:101)δ2(1 − dn(cid:101)δ
(cid:98)∆∗(β0)

=

≥

≥

(1 − (cid:98)ρ2(β0)) (cid:98)Ψ(β0)


1 − (d2

n(cid:101)δ2, dn(cid:101)δ)



(cid:32)



(1 − cε)M 2

1 (ε)ε2( (cid:101)C2 − cε)

(1 − ρ2)ΨcB

1 (ε)ε2 (cid:101)C2
M 2
2(1 − ρ2)ΨcB

,

nf 2
d2

s ( (cid:98)D, (cid:98)γ(β0))
(cid:98)Φ12(β0)
(cid:98)Ψ(β0)

(cid:98)Φ1(β0)
(cid:98)Φ12(β0)

(cid:33)−1 (cid:32)

(cid:33)



2



(cid:98)Φ13(β0)
(cid:98)τ (β0)

where the second inequality holds when ε is suﬃciently small. In this case,

E∗φa1,a2,s(dn(cid:101)δ, (cid:98)D, (cid:98)γ(β0)) ≥ P∗((1 − a)Z 2

s ( (cid:98)D, (cid:98)γ(β0))) ≥ Cα,max((cid:98)ρ(β0)))

(cid:32)

≥ P∗

(1 − a)Z 2
2

2 ( (cid:98)C2
2 (dn(cid:101)δ)f 2
(cid:32)
1 (ε)ε2 (cid:101)C2
M 2
2(1 − ρ2)ΨcB

(cid:33)

(cid:33)
≥ Cα,max((cid:98)ρ(β0))

(cid:32)

(cid:32)

≥ P∗

(1 − a)Z 2
2

(cid:33)

1 (ε)ε2 (cid:101)C2
M 2
2(1 − ρ2)ΨcB

(cid:33)

≥ Cα,max(ρ) + cε

− ε ≥ 1 − 2ε,

where the second inequality is by the fact that the CDF (survival function) of Z 2(λ) is monotone
decreasing (increasing) in |λ| and the last equality is by the deﬁnition of M1(ε) in (J.2) and the
fact that Cα,max((cid:98)ρ(β0))

p
−→ Cα,max(ρ) . This implies, on En(ε),

sup
(a1,a2)∈A(fs( (cid:98)D,(cid:98)γ(β0)),(cid:98)γ(β0)),(cid:101)δ∈ (cid:101)Dn,3(ε)

(cid:104)
(cid:105)
Pdn(cid:101)δ,s( (cid:98)D, (cid:98)γ(β0)) − E∗φa1,a2,s(dn(cid:101)δ, (cid:98)D, (cid:98)γ(β0))

≤ 2ε.

(J.8)

In addition, we note that (1 − ρ2)−1Ψ−1(cid:101)δ2 (cid:101)C2 satisﬁes

(1 − ρ2)−1Ψ−1(cid:101)δ2 (cid:101)C2 ≥

M 2
1 (ε)ε2 (cid:101)C2
2(1 − ρ2)ΨcB

,

41

where we use the facts that (cid:101)δ2 ≥ M 2
have

1 (ε), cB ≥ 1, and ε < 1. Therefore, by the same argument, we






(cid:16)

1 + a2

a1Z 2
+(1 − a1 − a2)Z 2

ρZ1 + (1 − ρ2)1/2Z2((1 − ρ2)−1/2Ψ−1/2(cid:101)δ (cid:101)C)

2 ((1 − ρ2)−1/2Ψ−1/2(cid:101)δ (cid:101)C) ≥ Cα(a1, a2; ρ)

(cid:17)2






≥ 1 − ε

E1

and

(cid:20)

E1{Z 2

2 ((1 − ρ2)−1/2Ψ−1/2(cid:101)δ (cid:101)C) ≥ Cα}

sup
(a1,a2)∈A(fs( (cid:98)D,(cid:98)γ(β0)),(cid:98)γ(β0)),(cid:101)δ∈ (cid:101)Dn,3(ε)

− E1






(cid:16)

1 + a2

a1Z 2
+(1 − a1 − a2)Z 2

ρZ1 + (1 − ρ2)1/2Z2((1 − ρ2)−1/2Ψ−1/2(cid:101)δ (cid:101)C)

2 ((1 − ρ2)−1/2Ψ−1/2(cid:101)δ (cid:101)C) ≥ Cα(a1, a2; ρ)

(cid:17)2

(cid:21)






≤ ε.

(J.9)

Combining (J.8) and (J.9), we have, on En(ε),

(cid:12)
(cid:12)
Qn(a1, a2, (cid:101)δ) − Q(a1, a2, (cid:101)δ)
sup
(cid:12)
(cid:12)
(a1,a2)∈A(fs( (cid:98)D,(cid:98)γ(β0)),(cid:98)γ(β0)),(cid:101)δ∈ (cid:101)Dn,3(ε)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤ 3ε.

(J.10)

Combining (J.4), (J.7), and (J.10), we have

|Qn(a1, a2, (cid:101)δ) − Q(a1, a2, (cid:101)δ)| > 5ε

(cid:33)

|Qn(a1, a2, (cid:101)δ) − Q(a1, a2, (cid:101)δ)| > ε, En(ε)

(cid:33)

|Qn(a1, a2, (cid:101)δ) − Q(a1, a2, (cid:101)δ)| > ε, En(ε)

(cid:33)

|Qn(a1, a2, (cid:101)δ) − Q(a1, a2, (cid:101)δ)| > 3ε, En(ε)

+ P (E c

n(ε))

(cid:33)

(cid:32)

P

sup
(a1,a2)∈A(fs( (cid:98)D,(cid:98)γ(β0)),(cid:98)γ(β0)),(cid:101)δ∈ (cid:101)Dn
(cid:32)

≤ P

+ P

+ P

sup
(a1,a2)∈A(fs( (cid:98)D,(cid:98)γ(β0)),(cid:98)γ(β0)),(cid:101)δ∈ (cid:101)Dn,1(ε)

(cid:32)

sup
(a1,a2)∈A(fs( (cid:98)D,(cid:98)γ(β0)),(cid:98)γ(β0)),(cid:101)δ∈ (cid:101)Dn,2(ε)

(cid:32)

sup
(a1,a2)∈A(fs( (cid:98)D,(cid:98)γ(β0)),(cid:98)γ(β0)),(cid:101)δ∈ (cid:101)Dn,3(ε)

≤ o(1) + ε.

Since ε is arbitrary, we have

ωn ≡

sup
(a1,a2)∈A(fs( (cid:98)D,(cid:98)γ(β0)),(cid:98)γ(β0)),(cid:101)δ∈ (cid:101)Dn

|Qn(a1, a2, (cid:101)δ) − Q(a1, a2, (cid:101)δ)|

p

−→ 0.

Then we have

0 ≤ sup
(cid:101)δ∈ (cid:101)Dn

Qn(a(fs( (cid:98)D, (cid:98)γ(β0)), (cid:98)γ(β0)), 0, (cid:101)δ) − sup
(cid:101)δ∈ (cid:101)Dn

Qn(As( (cid:98)D, (cid:98)γ(β0)), (cid:101)δ)

42

≤ sup
(cid:101)δ∈ (cid:101)Dn

Q(a(fs( (cid:98)D, (cid:98)γ(β0)), (cid:98)γ(β0)), 0, (cid:101)δ) − sup
(cid:101)δ∈ (cid:101)Dn

Q(As( (cid:98)D, (cid:98)γ(β0)), (cid:101)δ) + 2ωn

= op(1) − sup
(cid:101)δ∈ (cid:101)Dn

Q(As( (cid:98)D, (cid:98)γ(β0)), (cid:101)δ) + 2ωn,

where the equality holds because (1) sup
(cid:101)δ∈(cid:60) Q(a1, 0, (cid:101)δ) is continuous at a1 = 0 as shown in the proof
of I.Andrews (2016, Theorem 5), (2) a(fs( (cid:98)D, (cid:98)γ(β0)), (cid:98)γ(β0)) = op(1) under strong identiﬁcation, and
(3) sup

(cid:101)δ∈(cid:60) Q(0, 0, (cid:101)δ) = 0 by construction.

On the other hand, we have

Q(a1, a2, (cid:101)δ) = E1{Z 2

2 ((1 − ρ2)−1/2Ψ−1/2(cid:101)δ (cid:101)C) ≥ Cα}

ρZ1 + (1 − ρ2)1/2Z2((1 − ρ2)−1/2Ψ−1/2(cid:101)δ (cid:101)C)

2 ((1 − ρ2)−1/2Ψ−1/2(cid:101)δ (cid:101)C) ≥ Cα(a1, a2; ρ)

− E1






(cid:40)

(cid:16)

1 + a2

a1Z 2
+(1 − a1 − a2)Z 2
2 ((1 − ρ2)−1/2Ψ−1/2(cid:101)δ (cid:101)C) ≥ Cα}
(a1 + a2ρ2)Z 2
+(1 − a1 − a2ρ2)Z 2

= E1{Z 2

− E1

1 + a2ρ(1 − ρ2)1/2Z1Z2((1 − ρ2)−1/2Ψ−1/2(cid:101)δ (cid:101)C)
2 ((1 − ρ2)−1/2Ψ−1/2(cid:101)δ (cid:101)C) ≥ Cα(a1, a2; ρ)

(cid:17)2






(cid:41)

Note that a1 = 0 and a2ρ = 0 if and only if a1 + a2ρ2 = 0, given that a1 and a2 are nonnegative.
Therefore, Theorem 2.1(ii) implies, for any constant C > 0, there exists a constant c > 0 such that

inf
(a1,a2)∈A0,a1+a2ρ2≥C

sup
(cid:101)δ∈ (cid:101)Dn

Q(a1, a2, (cid:101)δ) ≥ c > 0.

Therefore,

P

(cid:16)

(cid:17)
A1,s( (cid:98)D, (cid:98)γ(β0)) + A2,s( (cid:98)D, (cid:98)γ(β0))ρ2 ≥ C > 0

≤ P (c ≤ op(1) + 2ωn) → 0.

This implies A1,s( (cid:98)D, (cid:98)γ(β0))

p
−→ 0 and A2,s( (cid:98)D, (cid:98)γ(β0))ρ

p

−→ 0.

K Proof of Theorem 4.3

We consider strong identiﬁcation with ﬁxed alternatives. By construction, we have A1,s( (cid:98)D, (cid:98)γ(β0)) ≥
1.1Cα,max((cid:98)ρ(β0))(cid:98)Φ1(β0)(cid:98)cB(β0)

. By Theorem 2.1(iii), it suﬃces to show that, w.p.a.1,

(cid:98)∆4

∗(β0)f 2

s ( (cid:98)D,(cid:98)γ(β0))

1.1Cα,max((cid:98)ρ(β0))(cid:98)Φ1(β0)(cid:98)cB(β0)

(cid:98)∆4

∗(β0)f 2

s ( (cid:98)D, (cid:98)γ(β0))

≥

˜qΨ2(β0)ρ4(β0)
C2Φ1(β0)

,

43

or equivalently,

1.1Cα,max((cid:98)ρ(β0))(cid:98)Φ1(β0)(cid:98)cB(β0)
nf 2

s ( (cid:98)D, (cid:98)γ(β0))

∗(β0)d2

(cid:98)∆4

≥

˜qΨ2(β0)ρ4(β0)
(cid:101)C2Φ1(β0)

=

˜qΦ1(β0)
(cid:101)C2∆4
∗(β0)

,

(K.1)

for some constant ˜q > Cα,max(ρ(β0)). Under strong identiﬁcation and ﬁxed alternatives, we have


QXX − (Qe(β0)e(β0), QXe(β0))

(cid:32)

dn (cid:98)D = dn

(cid:98)Φ1(β0)
(cid:98)Φ12(β0)

(cid:33)−1 (cid:32)

(cid:98)Φ12(β0)
(cid:98)Ψ(β0)


1 − (∆2, ∆)





p
−→

(cid:32)

(cid:33)−1 (cid:32)

Φ1(β0) Φ12(β0)
Φ12(β0) Ψ(β0)

Φ13(β0)
τ (β0)

(cid:98)Φ13(β0)
(cid:98)τ (β0)
(cid:33)


(cid:33)




 (cid:101)C.

Therefore, we have

dnfs( (cid:98)D, (cid:98)γ(β0)) = dn (cid:98)D + op(1)


1 − (∆2, ∆)

p
−→



(cid:32)



Φ1(β0) Φ12(β0)
Φ12(β0) Ψ(β0)

(cid:33)−1 (cid:32)

(cid:33)




 (cid:101)C

Φ13(β0)
τ (β0)

for s ∈ {pp, krs}. This means for any ε > 0, w.p.a.1,

nf 2
d2

s ( (cid:98)D, (cid:98)γ(β0)) ≤ (cB(β0) + ε) (cid:101)C2.

In addition, we have (cid:98)cB(β0)
p
and (cid:98)Φ1(β0)
Cα,max(ρ(β0)) − cε, and (cid:98)∆4

p
−→ Cα,max(ρ(β0)),
−→ Φ1(β0) > 0, which imply (cid:98)cB(β0) ≥ cB(β0)−cε, (cid:98)Φ1(β0) ≥ Φ1(β0)−cε, Cα,max((cid:98)ρ(β0)) ≥

−→ ∆∗(β0), Cα,max((cid:98)ρ(β0))

−→ cB(β0) ≥ 1, (cid:98)∆∗(β0)

p

p

∗(β0) ≤ ∆4

∗(β0) + cε, w.p.a.1. Therefore, we have, w.p.a.1,

1.1Cα,max((cid:98)ρ(β0))(cid:98)Φ1(β0)(cid:98)cB(β0)
nf 2

s ( (cid:98)D, (cid:98)γ(β0))

∗(β0)d2

(cid:98)∆4

≥

≥

1.1(Cα,max(ρ(β0)) − cε)(cB(β0) − cε)(Φ1(β0) − cε)
∗(β0) + cε)(cB(β0) + ε) (cid:101)C2

(∆4

(1.1 − cε)Cα,max(ρ(β0))Φ1(β0)
∗(β0) (cid:101)C2

∆4

,

where the second inequality holds because ε can be arbitrarily small. Then, we can let ˜q in (K.1)
be (1.1 − cε)Cα,max(ρ(β0)) which is greater than Cα,max(ρ(β0)). This concludes the proof.

L Proof of Theorem A.1

We focus on the consistency of (cid:98)Φ1(β0) and (cid:98)Ψ(β0). The consistency of the rest four estimators
can be established in the same manner. We have ei(β0) = ei + ∆Xi = Ui(∆) + ∆Πi, where

44

Ui(∆) = ei + ∆Vi. Therefore,

(cid:98)Φ1(β0) =

=

=

2
K

2
K

2
K

(cid:88)

(cid:88)

i∈[n]
(cid:88)

j(cid:54)=i

(cid:88)

i∈[n]
(cid:88)

j(cid:54)=i

(cid:88)

i∈[n]

j(cid:54)=i

ije2
P 2

i (β0)e2

j (β0)

ij(∆2Π2
P 2

i + 2∆ΠiUi(∆) + U 2

i (∆))(∆2Π2

j + 2∆ΠjUj(∆) + U 2

j (∆))

ijU 2
P 2

i (∆)U 2

j (∆) + ∆

4
K

(cid:88)

(cid:88)

i∈[n]

j(cid:54)=i

ij(ΠiUi(∆)U 2
P 2

j (∆) + ΠjUj(∆)U 2

i (∆))

ij(Π2
P 2

i U 2

j (∆) + Π2

j U 2

i (∆) + 4ΠiΠjUi(∆)Uj(∆))

ij(Π2
P 2

i ΠjUj(∆) + Π2

j ΠiUi(∆)) + ∆4 2
K

(cid:88)

(cid:88)

i∈[n]

j(cid:54)=i

ijΠ2
P 2

i Π2
j

+ ∆2 2
K

+ ∆3 4
K

(cid:88)

(cid:88)

i∈[n]
(cid:88)

j(cid:54)=i

(cid:88)

i∈[n]

j(cid:54)=i

≡

4
(cid:88)

l=0

∆lTl.

We ﬁrst note that 1
K
this, note that

(cid:80)

i∈[n] ω2

i σ2

i = o(1), 1
K

(cid:80)

i∈[n] ω2

i γi = o(1), and 1
K

(cid:80)

i∈[n] ω2

i η2

i = o(1). To see

1
K

(cid:88)

i∈[n]

i σ2
ω2

i ≤

≤

≤

C
K

C
K

C
K

(cid:88)

ω2
i

i∈[n]

(2Π(cid:62)P 2Π + 2

iiΠ2
P 2
i )

(cid:88)

i∈[n]

(cid:88)

(
i,j∈[n]

|Πi||Pij||Πj| + Π(cid:62)Πpn)

≤ Cp1/2

n

Π(cid:62)Π
K

= o(1),

where the second inequality is shown is the Proof of Mikusheva and Sun (2022, Lemma S1.4) and
last inequality is by the fact that P 2
i γi = o(1) and
1
K

ij ≤ (cid:80)
i = o(1) can be established in the same manner.

ij = Pii. The results for 1
K

j∈[n] P 2

i∈[n] ω2

(cid:80)

(cid:80)

i η2

i∈[n] ω2
We ﬁrst consider T0. Denote ξij = U 2

i (∆)U 2

j (∆) − EU 2

i (∆)U 2

j (∆). We want to show that

1
K

(cid:88)

(cid:88)

i∈[n]

j(cid:54)=i

P 2

ijξij = op(1).

45

Note that





1
K

E

(cid:88)

(cid:88)

i∈[n]

j(cid:54)=i


2

P 2

ijξij



=

1
K2

(cid:88)

(cid:88)

i∈[n]

j(cid:54)=i

P 4
ij

Eξ2

ij +

4
K2

(cid:88)

(cid:88)

(cid:88)

i∈[n]

j(cid:54)=i

i(cid:48)(cid:54)=i,j

ijP 2
P 2

ii(cid:48)Eξijξii(cid:48).

As both Eξ2

ij and |Eξijξii(cid:48)| are bounded, we have

1
K2

(cid:88)

(cid:88)

i∈[n]

j(cid:54)=i

P 4
ij

Eξ2

ij ≤

C
K2

(cid:88)

(cid:88)

i∈[n]

j(cid:54)=i

P 2

ij ≤

C
K

= o(1)

and

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
K2

(cid:88)

(cid:88)

(cid:88)

i∈[n]

j(cid:54)=i

i(cid:48)(cid:54)=i,j

ijP 2
P 2

ii(cid:48)Eξijξii(cid:48)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤

C
K2

(cid:88)

(cid:88)

(cid:88)

i∈[n]

j(cid:54)=i

i(cid:48)(cid:54)=i,j

ijP 2
P 2

ii(cid:48) ≤

C
K2

(cid:88)

(cid:88)

i∈[n]

j(cid:54)=i

P 2

ijPii = o(1).

Therefore, we have

T0 =

2
K

(cid:88)

(cid:88)

P 2
ij

i∈[n]

j(cid:54)=i

E(U 2

i (∆)U 2

j (∆)) + op(1)

(cid:88)

(cid:88)

ijη2
P 2

i η2

j + ∆3 4
K

(cid:88)

(cid:88)

ij(η2
P 2

i γj + η2

j γi) + ∆2 2
K

(cid:88)

(cid:88)

i∈[n]

j(cid:54)=i

ij(η2
P 2

i σ2

j + η2

j σ2

i + 4γiγj)

= ∆4 2
K

+ ∆

4
K

j(cid:54)=i

i∈[n]
(cid:88)

(cid:88)

i∈[n]

j(cid:54)=i

ij(γiσ2
P 2

j + γjσ2

i ) +

ijσ2
P 2

i σ2

j + op(1)

i∈[n]
2
K

j(cid:54)=i

(cid:88)

(cid:88)

i∈[n]

j(cid:54)=i

= Φ1(β0) + op(1).

By the same argument above, we have

T1 = ET1 + op(1) = op(1)

because ET1 = 0. Similarly, we have ET3 = 0 and T3 = op(1). Next, we have

pnT2 = ET2 + oP (1) ≤

C
K

(cid:88)

(cid:88)

i∈[n]

j(cid:54)=i

ijΠ2
P 2

i + op(1) ≤

CpnΠ(cid:62)Π
K

+ op(1) = op(1).

Last, we have

T4 ≤

C
K

(cid:88)

(cid:88)

i∈[n]

j(cid:54)=i

ijΠ2
P 2

i = o(1),

46

where the ﬁrst inequality is by maxi∈[n] |Πi| < C. This implies

(cid:98)Φ1(β0) − Φ1(β0) = op(1).

Next, we consider the consistency of (cid:98)Ψ(β0). By the similar argument above, we have

(cid:88)

(cid:88)

P 2

ijXiei(β0)Xjej(β0))

1
K

=

+

=

j(cid:54)=i

(cid:88)

i∈[n]
1
K

i∈[n]
(cid:88)

1
K

1
K

i∈[n]
(cid:88)

j(cid:54)=i

(cid:88)

i∈[n]

j(cid:54)=i

(cid:88)

j(cid:54)=i

(cid:88)

P 2

ijΠiei(β0)Πjej(β0)) +

P 2

ijViei(β0)Πjej(β0)) +

1
K

1
K

(cid:88)

(cid:88)

P 2

ijΠiei(β0)Vjej(β0))

i∈[n]
(cid:88)

j(cid:54)=i

(cid:88)

P 2

ijViei(β0)Vjej(β0))

i∈[n]

j(cid:54)=i

ij(γi + ∆η2
P 2

i )(γj + ∆η2

j ) + op(1).

(cid:88)

(cid:88)
(

PijXj)2e2

i (β0)

(ωi +

(cid:88)

j(cid:54)=i

PijVj)2e2

i (β0)

In addition, we have

1
K

=

=

=

i∈[n]
1
K

j(cid:54)=i

(cid:88)

i∈[n]
(cid:88)

1
K

1
K

i∈[n]
(cid:88)

(cid:88)

i∈[n]

j(cid:54)=i

ω2
i

Ee2

i (β0) +

1
K

(cid:88)

(cid:88)

i∈[n]

j(cid:54)=i

ijηjEe2
P 2

i (β0) + op(1)

ijη2
P 2

j (σ2

i + 2γi∆ + ∆2η2

i ) + op(1),

(L.1)

(L.2)

where the second equality is due to Mikusheva and Sun (2022, Lemma S3.2). In the next section,

we show the same results hold under Assumption 5. Combining (L.1) and (L.2), we have

ij(γi + ∆η2
P 2

i )(γj + ∆η2

j ) +

1
K

(cid:88)

(cid:88)

i∈[n]

j(cid:54)=i

ijη2
P 2

j (σ2

i + 2γi∆ + ∆2η2

i ) + op(1)

(cid:98)Ψ(β0) =

=

1
K

1
K

(cid:88)

(cid:88)

i∈[n]

j(cid:54)=i

(cid:88)

(cid:88)

i∈[n]

j(cid:54)=i

= Ψ(β0) + op(1).

ij(γiγj + η2
P 2

i η2

j ) +

ijη2
P 2

i η2

j + op(1)

4∆
K

(cid:88)

(cid:88)

i∈[n]

j(cid:54)=i

ijη2
P 2

i γj +

2∆2
K

(cid:88)

(cid:88)

i∈[n]

j(cid:54)=i

47

M Proof of Theorem A.2

Given Lemma A.1, Lemmas 2 and 3 in Mikusheva and Sun (2022) hold under Assumptions 4 and

6. Therefore, Mikusheva and Sun (2022, Theorem 3) shows that

(cid:98)Φ1(β0) −

2
K

(cid:88)

(cid:88)

P 2
ij

i∈[n]

j(cid:54)=i

EU 2

i (∆)EU 2

j (∆) = op(1).

In addition, the proof of Theorem A.1 shows that

2
K

(cid:88)

(cid:88)

P 2
ij

i∈[n]

j(cid:54)=i

EU 2

i (∆)EU 2

j (∆) = Φ1(β0) + o(1),

which implies the consistency of (cid:98)Φ1(β0).

Similarly, given Lemma A.1, Lemma S3.1 in Mikusheva and Sun (2022) holds under Assump-
tions 4 and 6, so that the consistency of (cid:98)Υ to Υ is also shown by using their argument. In addition,
we use the same argument in the proof of Mikusheva and Sun (2022, Theorem 5) to show that

(cid:98)Ψ(β0) =






1
K




+ ∆







1
K

+ ∆2

(cid:88)

(cid:88)
(

i∈[n]

j(cid:54)=i

PijXj)2 eiMie
Mii

+

1
K

(cid:88)

(cid:88)

i∈[n]

j(cid:54)=i

(cid:101)P 2
ijMiXeiMjXej






(cid:88)

(cid:88)

(

PijXj)2

i∈[n]

j(cid:54)=i

(cid:18) eiMiX
Mii

+

XiMie
Mii

(cid:19)

+

2
K

(cid:88)

(cid:88)

i∈[n]

j(cid:54)=i

(cid:101)P 2
ijMiXeiMjXXj






1
K

(cid:88)

(cid:88)
(

PijXj)2 XiMiX
Mii

+

1
K

(cid:88)

(cid:88)

(cid:101)P 2
ijMiXXiMjXXj

i∈[n]

i∈[n]
= Ψ + 2∆τ + ∆2Υ + op(1) = Ψ(β0) + op(1),

j(cid:54)=i

j(cid:54)=i






where the second equality also follows from Lemma S3.1 in Mikusheva and Sun (2022).

Next for (cid:98)Φ12(β0), we have

(cid:88)

(cid:88)

(cid:101)P 2
ijMjXej(β0)ei(β0)Mie(β0)

(cid:101)P 2
ijMjXejeiMie

(cid:88)

j(cid:54)=i

1
K

=

j(cid:54)=i

(cid:88)

i∈[n]
1
K

i∈[n]
1
K

(cid:88)

+ ∆

j(cid:54)=i

i∈[n]
(cid:88)

+ ∆2 1
K

i∈[n]

j(cid:54)=i

48

(cid:88)

(cid:101)P 2
ij (MjXXjeiMie + MjXejXiMie + MjXejeiMiX)

(cid:88)

(cid:101)P 2
ij (MjXXjXiMie + MjXXjeiMiX + MjXejXiMiX)

+ ∆3 1
K

(cid:88)

(cid:88)

i∈[n]

j(cid:54)=i

(cid:101)P 2
ijMjXXjXiMiX.

Note that 1
K

(cid:80)

i∈[n]

(cid:80)

j(cid:54)=i (cid:101)P 2

ijMjXejeiMie = 1
K

(cid:80)

i∈[n]

(cid:80)

j(cid:54)=i (cid:101)P 2

ij(MjV + λi)ejeiMie, where λi =

MiΠ. Then, by Lemma A.1 and Lemma 3 of Mikusheva and Sun (2022),

1
K

(cid:88)

(cid:88)

i∈[n]

j(cid:54)=i

(cid:101)P 2
ijMjXejeiMie −

1
K

(cid:88)

(cid:88)

i∈[n]

j(cid:54)=i

(cid:101)P 2
ijMjV ejeiMie = op(1).

Furthermore, by Lemma A.1 and Lemma 2 of Mikusheva and Sun (2022),

1
K

(cid:88)

(cid:88)

i∈[n]

j(cid:54)=i

(cid:101)P 2
ijMjV ejeiMie −

1
K

(cid:88)

(cid:88)

i∈[n]

j(cid:54)=i

ijγjσ2
P 2

i = op(1).

By using similar arguments, we ﬁnd that

1
K

1
K

1
K

1
K

1
K

1
K

1
K

(cid:88)

(cid:88)

i∈[n]
(cid:88)

j(cid:54)=i

(cid:88)

i∈[n]
(cid:88)

j(cid:54)=i

(cid:88)

i∈[n]
(cid:88)

j(cid:54)=i

(cid:88)

i∈[n]
(cid:88)

j(cid:54)=i

(cid:88)

i∈[n]
(cid:88)

j(cid:54)=i

(cid:88)

i∈[n]
(cid:88)

j(cid:54)=i

(cid:88)

i∈[n]

j(cid:54)=i

(cid:101)P 2
ijMjXXjeiMie =

(cid:101)P 2
ijMjXejXiMie =

(cid:101)P 2
ijMjXejeiMiX =

1
K

1
K

1
K

(cid:88)

(cid:88)

i∈[n]
(cid:88)

j(cid:54)=i

(cid:88)

i∈[n]
(cid:88)

j(cid:54)=i

(cid:88)

ijη2
P 2

j σ2

i + op(1),

P 2

ijγjγi + op(1),

P 2

ijγjγi + op(1),

(cid:101)P 2
ijMjXXjXiMie =

1
K

(cid:101)P 2
ijMjXXjeiMiX =

(cid:101)P 2
ijMjXejXiMiX =

1
K

1
K

(cid:101)P 2
ijMjXXjXiMiX =

i∈[n]
(cid:88)

j(cid:54)=i

(cid:88)

i∈[n]
(cid:88)

j(cid:54)=i

(cid:88)

i∈[n]
(cid:88)

j(cid:54)=i

(cid:88)

ijη2
P 2

j γi + op(1),

ijη2
P 2

j γi + op(1),

ijγjη2
P 2

i + op(1),

i∈[n]
(cid:88)

j(cid:54)=i

(cid:88)

ijη2
P 2

j η2

i + op(1).

1
K

i∈[n]

j(cid:54)=i

Putting these results together, we obtain

(cid:98)Φ12(β0) = Φ12 + ∆(2Ψ + Φ13) + 3∆2τ + ∆3Υ + op(1) = Φ12(β0) + op(1).

We use similar arguments to prove the results for (cid:98)Ψ13(β0) and (cid:98)τ (β0). For (cid:98)Φ13(β0), notice that

1
K

(cid:88)

(cid:88)

i∈[n]

j(cid:54)=i

(cid:101)P 2
ijMiXei(β0)MjXej(β0)

49

(cid:88)

(cid:88)

(cid:101)P 2
ijMiXeiMjXej

(cid:88)

(cid:101)P 2
ij(MiXeiMjXXj + MiXXiMjXej)

(cid:88)

(cid:101)P 2
ijMiXXiMjXXj

=

1
K

+ ∆

j(cid:54)=i

i∈[n]
1
K

(cid:88)

j(cid:54)=i

i∈[n]
(cid:88)

+ ∆2 1
K

=

1
K

j(cid:54)=i

i∈[n]
(cid:88)

(cid:88)

i∈[n]

j(cid:54)=i

P 2

ijγiγj + ∆

1
K

(cid:88)

(cid:88)

i∈[n]

j(cid:54)=i

ij(γiη2
P 2

j + η2

i γj) + ∆2 1
K

(cid:88)

(cid:88)

i∈[n]

j(cid:54)=i

ijη2
P 2

i η2

j + op(1),

which implies that

(cid:98)Φ13(β0) = Φ13 + 2∆τ + ∆2Υ + op(1) = Φ13(β0) + op(1).

Finally, for (cid:98)τ (β0), notice that

1
K

1
K

(cid:88)

(cid:88)

i∈[n]

j(cid:54)=i

(cid:101)P 2
ijXiMiXMjXej(β0) =

(cid:88)

(cid:88)

(

PijXj)2

i∈[n]

j(cid:54)=i

(cid:18) ei(β0)MiX
2Mii

+

which implies that

1
K

(cid:88)

(cid:88)

ijη2
P 2

i γj +

1
K

(cid:88)

(cid:88)

i∈[n]

j(cid:54)=i

ijη2
P 2

i η2

j ∆ + op(1),

j(cid:54)=i
i∈[n]
XiMie(β0)
2Mii

(cid:19)

=

1
K

(cid:88)

(cid:88)

i∈[n]

j(cid:54)=i

ijη2
P 2

i γj +

1
K

(cid:88)

(cid:88)

i∈[n]

j(cid:54)=i

ijη2
P 2

i η2

j ∆ + op(1),

(cid:98)τ (β0) = τ + ∆Υ + op(1) = τ (β0) + op(1).

This completes the proof of the theorem.

N Proof of Lemma A.1

Let pn = maxi Pii. We ﬁrst give some useful bounds:

(cid:88)

i∈[n]

ω2

i ≤ C max

i

P 1/2
ii Π(cid:62)Π = Cp1/2

n Π(cid:62)Π,

max
i∈[n]

ω2

(
i = max
i∈[n]

(cid:88)

j(cid:54)=i

PijΠj)2 ≤ max
i∈[n]

(cid:88)
(

j(cid:54)=i

ij)Π(cid:62)Π ≤ pnΠ(cid:62)Π,
P 2

which imply

(cid:88)

i∈[n]

ω4

i ≤ max
i∈[n]

ω2
i (

(cid:88)

i∈[n]

i ) ≤ Cpn(Π(cid:62)Π)2.
ω2

50

First, we show that Mikusheva and Sun (2022, Lemma S2.1) hold under our conditions following
the lines of argument in their proof. More speciﬁcally, we notice that to show ∆2|EA2| = o(1),
where A2 is deﬁned in the proof of Mikusheva and Sun (2022, Lemme S2.1), it suﬃces to show the
following terms are o(1):

C∆2
K

(cid:88)

(cid:88)

i∈[n]

j(cid:54)=i

P 2

ij|λi||Πj| ≤

C∆2
K

(cid:88)

Piiλ2
i



i∈[n]

(cid:88)



j∈[n]

PjjΠ2
j



≤

C∆2
K

(cid:16)

λ(cid:62)λ

pn

(cid:17)1/2 (cid:16)

Π(cid:62)Π

(cid:17)1/2



1/2 



1/2

≤

C∆2
K3/2

(cid:16)

Π(cid:62)Π

(cid:17)

pn

= o(1) by λ(cid:62)λ ≤ C

Π(cid:62)Π
K

,

C∆2
K

(cid:88)

(cid:88)

i∈[n]

j(cid:54)=i

P 2

ij|Πi||Πj| ≤

C∆2
K

(cid:88)

PiiΠ2
i



i∈[n]

(cid:88)



j∈[n]

PjjΠ2
j



≤

C∆2
K

(cid:16)

Π(cid:62)Π

(cid:17)

pn

= o(1).



1/2 



1/2









Then, we prove the variance of ∆2A2 = o(1) by showing that

C∆4
K2

C∆4
K2

(cid:88)

(cid:88)

ijλ2
P 4

i λ2

j ≤

j∈[n]

i∈[n]




(cid:88)



λ2
i



(cid:88)

P 2
ij

i∈[n]

j∈[n]

C∆4
K2 p2

n

(cid:16)

λ(cid:62)λ

(cid:17)2

≤

C∆4
K2 p2

n

(cid:18) Π(cid:62)Π
K

(cid:19)2

= o(1) by P 2

ij ≤ Pii,


 Π(cid:62)Π + λ(cid:62)λ







2

Pjj|Πj|



 ≤

C∆4
K2

(cid:88)

j∈[n]

(cid:16)

(cid:17)
pn(λ(cid:62)λ)(Π(cid:62)Π) + (λ(cid:62)λ)(pnK)(Π(cid:62)Π)

≤

C∆4
K3

(cid:16)

pn(Π(cid:62)Π)2 + pnK(Π(cid:62)Π)2(cid:17)

= o(1) by

(cid:88)

P 2

jj ≤ pnK,





(cid:88)

(cid:88)

i∈[n]

j∈[n]



2

P 2

ij|ΠiΠj|



≤

C∆4
K2





(cid:88)

i∈[n]

j∈[n]




PiiΠ2
i







PjjΠ2
j

 ≤

(cid:88)

j∈[n]

C∆4
K2 p2

n(Π(cid:62)Π)2 = o(1),

(cid:88)

(cid:88)





(cid:88)

j∈[n]

k∈[n]

i∈[n]



2

P 2

ij|λiΠiMjk|



=

C∆4
K2

(cid:88)

(cid:88)





(cid:88)


2

P 2

ij|λiΠiMjk|



+

C∆4
K2

(cid:88)





(cid:88)

j∈[n]

i∈[n]



2

P 2

ij|λiΠiMjj|



≤

C∆4
K2







(cid:88)

(cid:88)

(cid:88)

M 2
jk



Piiλ2
i





(cid:88)

PiiΠ2
i

 +

j∈[n]

k(cid:54)=j

i∈[n]

C∆4
K2

(cid:88)





(cid:88)

j∈[n]

i∈[n]


2

P 2

ij|λi|



k(cid:54)=j

i∈[n]

j∈[n]


i∈[n]


(cid:88)

(cid:88)



P 4
ij


 λ(cid:62)λ

≤

≤

C∆4
K2 Kp2

n(λ(cid:62)λ)(Π(cid:62)Π) +

C∆4
K2

C∆4Kp2
n(Π(cid:62)Π)2
K2

+

j∈[n]
C∆4pnKΠ(cid:62)Π
K2

i∈[n]

= o(1) by

(cid:88)

(cid:88)

M 2

jk =

(cid:88)

(cid:88)

j∈[n]

k(cid:54)=j

j∈[n]

k(cid:54)=j

P 2
jk ≤ K and P 2

ij ≤ Pii ≤ pn.

51

C∆4
K2

and

C∆4
K2

Second, we show that Mikusheva and Sun (2022, Lemma S2.2) holds under our conditions.

Notice that |∆EA1| = o(1) by

C|∆|
K

(cid:88)

(cid:88)

i∈[n]

j(cid:54)=i

P 2

ij|Πi| ≤

C|∆|
K







1/2

(cid:88)

P 2
ii



(Π(cid:62)Π)1/2 ≤

i∈[n]

C|∆|
K

(pnK)1/2(Π(cid:62)Π)1/2 = o(1),

Then, we show that the variance of ∆A1 is o(1) by showing the following terms are o(1):





(cid:88)





(cid:88)


 λ2

i +

P 2
ij

(cid:88)

(cid:88)

i∈[n]

j∈[n]

i∈[n]

j∈[n]



P 2

ij|λi||λj|

 ≤

(cid:16)

C∆2
K2

pn(λ(cid:62)λ) + pn(λ(cid:62)λ)

(cid:17)

= o(1),





(cid:88)

(cid:88)

ij(λ2
P 4

i + |λi||λj|) +



ijλ2
P 2
i

 ≤

(cid:16)

C∆2
K2

(cid:88)

(cid:88)

i∈[n]

j∈[n]

n(λ(cid:62)λ) + p2
p2

n(λ(cid:62)λ) + pn(λ(cid:62)λ)

(cid:17)

= o(1),

C∆2
K2

C∆2
K2

C∆2
K2

C∆2
K2

C∆2
K2

≤

≤

C∆2
K2

C∆2
K2

i∈[n]

j∈[n]





(cid:88)

(cid:88)

P 2

ik|λi||λk| +

k∈[n]

i∈[n]


(cid:88)

(cid:88)



j∈[n]

i∈[n]



2

P 2

ij|λi|



C∆2
K2

(cid:88)

j∈[n]





(cid:88)



2

P 2

ij|Πi|



i∈[n]




P 2

ij|λi||λj|

 ≤

(cid:88)

(cid:88)

i∈[n]

j∈[n]

C∆2
K2

(cid:16)

(cid:17)
pn(λ(cid:62)λ) + pn(λ(cid:62)λ)

= o(1),

≤

C∆2
K2

≤

C∆2
K2

(cid:88)





(cid:88)


 (λ(cid:62)λ) ≤

P 4
ij

j∈[n]

i∈[n]

C∆2
K2 (pnK)(λ(cid:62)λ) = o(1),





(cid:88)


 (Π(cid:62)Π) ≤

P 4
ij

i∈[n]

(cid:88)

j∈[n]

2

C∆2
K2 (pnK)(Π(cid:62)Π) = o(1),

(cid:88)

(cid:88)

(cid:88)



P 2

ij|ΠiMikMjk|



j∈[n]

k∈[n]

=

C∆2
K2

(cid:88)

(cid:88)

j∈[n]

k(cid:54)=j

i∈[n]


(cid:88)




2

P 2

ij|ΠiMikMjk|



+

C∆2
K2

(cid:88)





(cid:88)

j∈[n]

i∈[n]



2

P 2

ij|ΠiMijMjj|



i∈[n]


C∆2
K2

(cid:88)

(cid:88)

(cid:88)

M 2
jk



j∈[n]

k(cid:54)=j

C∆2
K2 Kp2
(cid:88)
(cid:88)

n(Π(cid:62)Π) +


k∈[n]

i∈[n]

j∈[n]




2

(cid:88)

(cid:88)



i∈[n]

j∈[n]

P 2

ij|Πi|




 Π(cid:62)Π +

P 4
ij

C∆2
K2





(cid:88)

(cid:88)


 Π(cid:62)Π

P 4
ij

i∈[n]
C∆2
K2 Kpn(Π(cid:62)Π) = o(1),




j∈[n]

i∈[n]



(cid:88)



P 2

ij|ΠiMikMjk|





P 2

ik|ΠiMijMjk|

 ≤

(cid:88)

i∈[n]

C∆2
K2 Kpn(Π(cid:62)Π) = o(1),

≤

C∆2
K2 (pnK)(Π(cid:62)Π) = o(1).

Then, to show that Mikusheva and Sun (2022, Lemma 3) holds under our conditions, we show

52

the following terms are o(1):

(cid:88)

(cid:88)

P 2

ij|ΠiλiΠjλj| ≤

C
K

j(cid:54)=i
(cid:16)

i∈[n]
C
K

pn

(cid:88)





(cid:88)

(cid:17) (cid:16)

Π(cid:62)Π

λ(cid:62)λ

(cid:17)

≤

C
K2 pn


2

P 2

ij|Πi||λi|



λ2
j ≤







1/2 



1/2

(cid:88)

(cid:88)

ijΠ2
P 2

i Π2
j





(cid:88)

(cid:88)

ijλ2
P 2

i λ2
j



i∈[n]
(cid:16)

j∈[n]
(cid:17)2

Π(cid:62)Π

(cid:88)

C
K2

= o(1),



pn

(cid:88)

j∈[n]

i∈[n]

i∈[n]

j∈[n]



2

|Πi||λj|



λ2
j ≤

(cid:16)

C
K2 p2

n

Π(cid:62)Π

(cid:17) (cid:18) Π(cid:62)Π

(cid:19)2

K

= o(1),

j∈[n]

i∈[n]

(cid:88)

(cid:88)

(cid:88)

(cid:88)

i∈[n]

i(cid:48)∈[n]

j∈[n]

j(cid:48)∈[n]

ij|ΠiλiΠj|P 2
P 2

i(cid:48)j(cid:48)|Πi(cid:48)λi(cid:48)Πj(cid:48)|

(cid:88)

k∈[n]

|MjkMj(cid:48)k|

C
K

≤

C
K2

C
K2





≤

C
K2

(cid:88)

(cid:88)

i∈[n]

j∈[n]







ijΠ2
P 2

i λ2
i





(cid:88)

(cid:88)

ijΠ2
P 2
j

 ≤

i∈[n]

j∈[n]

C
K2 p2

n(Π(cid:62)Π)(λ(cid:62)λ) ≤

(cid:16)

C
K3 p2

n

(cid:17)2

Π(cid:62)Π

= o(1),

where (cid:80)

k∈[n] |MjkMj(cid:48)k| ≤ 1 by Mikusheva and Sun (2022, Lemma S1.1(ii)).

Now we show that Mikusheva and Sun (2022, Lemma S3.2 ) holds under our conditions, i.e.,

(a)

(b)

(c)

(d)

(e)

1
K

1
K

1
K

1
K

1
K

n
(cid:88)

i=1

(ωi +

n
(cid:88)

(ωi +

i=1
n
(cid:88)

(ωi +

i=1
n
(cid:88)

(ωi +

i=1
n
(cid:88)

i=1

(ωi +

(cid:88)

j(cid:54)=i

(cid:88)

j(cid:54)=i

(cid:88)

j(cid:54)=i

(cid:88)

j(cid:54)=i

(cid:88)

j(cid:54)=i

PijVj)2Ui −





1
K

n
(cid:88)

i=1

ω2
i

E[Ui] +

1
K

(cid:88)

i,j(cid:54)=i



P 2
ij

E[Ui]η2
j



p

−→ 0,

PijVj)2 ξ1,i
Mii

(cid:88)

k(cid:54)=j

Pikξ2,k

p

−→ 0,

PijVj)2aiξ1,i

p

−→ 0,

PijVj)2 ai
Mii

(cid:88)

k(cid:54)=i

Pikξ1,k −

2
K

n
(cid:88)

(cid:88)

i=1

j(cid:54)=i

P 2

ijωi

ai
Mii

E[Vjξ1,j]

p

−→ 0,

PijVj)2Πi

λi
Mii

p

−→ 0,

where ξ1,i, ξ2,i stay for either ei or Vi, Ui stay for e2

i , eiVi, or V 2

i , and ai stay for either Πi or λi
Mii

.

To prove statement (a), following the arguments in Mikusheva and Sun (2022), we just need to

show the following terms are o(1):





1
K

E

(cid:88)

i∈[n]


2

ω2

i Ui



≤

C
K2

(cid:88)

i∈[n]

ω4

i ≤

C
K2 max

i∈[n]





(cid:88)

ω2
i



ω2
i

 ≤

i∈[n]

(cid:16)

C
K2 p3/2

n

(cid:17)2

Π(cid:62)Π

= o(1),

53

C
K

(cid:88)

(cid:88)

i∈[n]

j(cid:54)=i

ij(ω2
P 2

i + |ωi||ωj|) ≤

C
K




(cid:88)

Piiω2

i +



(cid:88)

Piiω2
i



i∈[n]

i∈[n]

(cid:88)



j∈[n]







1/2 

1/2



Pjjω2
j




 ≤

C
K

n Π(cid:62)Π = o(1),
p3/2

where we have used maxi∈[n] ω2
Lemma S1.3(b)).

i ≤ pnΠ(cid:62)Π, (cid:80)

i∈[n] ω2

i ≤ Cp1/2

n Π(cid:62)Π, and Mikusheva and Sun (2022,

To prove statement (b), we show that

(cid:88)

(cid:88)

(P 2

ijω4

i + P 2

ijw2

i w2

j + P 4

ijw2

i + P 4

ij|ωiωj|)

i∈[n]

j(cid:54)=i




pn

(cid:88)

i∈[n]





1/2 



1/2

ω4

i +



(cid:88)

Piiω4
i



i∈[n]

(cid:88)



j∈[n]

Pjjω4
j



(cid:88)

+

i∈[n]





1/2 

Piiω2

i pn + pn



(cid:88)

Piiω2
i



i∈[n]

(cid:88)



j∈[n]

1/2



Pjjω2
j






(cid:16)

n(Π(cid:62)Π)2 + p2
p2

n(Π(cid:62)Π)2 + p5/2

n (Π(cid:62)Π) + p5/2

(cid:17)
n (Π(cid:62)Π)

= o(1),

C
K2

≤

≤

C
K2

C
K2

C
K2


(cid:88)



ω2

i +

(cid:88)

(cid:88)

i∈[n]

i∈[n]

j∈[n]



P 2

ij|ωiωj|

 ≤

(cid:16)

C
K2

n Π(cid:62)Π + p3/2
p1/2

n Π(cid:62)Π

(cid:17)

= o(1),

where we have used (cid:80)

i ≤ Cp1/2
To prove statement (c), we show that, for ai = Πi or λi/Mii,

n Π(cid:62)Π and (cid:80)

i∈[n] ω2

i∈[n] ω4

i ≤ Cpn(Π(cid:62)Π)2.

iia2
P 2

i +

(cid:88)

(cid:88)

P 2

ij |aiaj|

 ≤



C
K2

(cid:16)

(cid:17)
na(cid:62)a + pna(cid:62)a
p2

= o(1),

i∈[n]

j∈[n]
(cid:18)

ω4
i

λ2
i
M 2
ii

≤

C
K2

max
i∈[n]

i Π2
ω4

i ≤

C
K2

(cid:88)

i∈[n]

ω4

i ≤

i∈[n]
(cid:16)

C
K2 pn

(cid:17)2

Π(cid:62)Π

(cid:19)2

(cid:88)

ω2
i

i ≤ Cp2
λ2
n

(cid:19)3

(cid:18) Π(cid:62)Π
K

= o(1),

P 4
ij

(cid:0)a2

i + |ai| |aj|(cid:1) ≤

(cid:16)

C
K2

na(cid:62)a + p2
p2

(cid:17)
na(cid:62)a

= o(1),

= o(1), where we have used max
i∈[n]

|Πi| ≤ C,

C
K2

C
K2

C
K2

C
K2

C
K2





(cid:88)

i∈[n]

(cid:88)

i∈[n]

(cid:88)

i∈[n]

(cid:88)

(cid:88)

i∈[n]

j(cid:54)=i

(cid:88)

(cid:88)

i∈[n]

j(cid:54)=i

ij(ω2
P 2

i a2

i + |ωiai||ωjaj|) ≤

To prove statement (d), we ﬁrst show that

(cid:16)

C
K2

n(Π(cid:62)Π)(a(cid:62)a) + p2
p2

(cid:17)
n(Π(cid:62)Π)(a(cid:62)a)

= o(1).


2





2









C
K2

(cid:88)

i∈[n]

ω2

i |ai|



(cid:88)

+



i∈[n]

|ωiai|



 = o(1).

54

In particular, when ai = Πi, we have









(cid:88)

C
K2

ω2

i |Πi|



i∈[n]


(cid:16)



n Π(cid:62)Π
p1/2

(cid:17)2

+



(cid:88)

i∈[n]



+



(cid:88)

i∈[n]

≤

C
K2



2





2

|ωiΠi|



 ≤

C
K2









(cid:88)



2



ω2
i



+



i∈[n]

(cid:88)

i∈[n]



2

|ωiΠi|







ω2
i



(cid:16)

(cid:17)
Π(cid:62)Π



 ≤

(cid:16)

C
K2

pn(Π(cid:62)Π)2 + p1/2

n (Π(cid:62)Π)2(cid:17)

= o(1),

When ai = λi
Mii

, we have









(cid:88)

ω2
i

C
K2

(cid:12)
(cid:12)
(cid:12)
(cid:12)

λi
Mii

2


(cid:12)
(cid:12)
(cid:12)

(cid:12)



+



(cid:12)
(cid:12)
ωi
(cid:12)
(cid:12)

λi
Mii

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:88)

i∈[n]



2



 ≤

C
K2









(cid:88)


 (λ(cid:62)λ) +





ω4
i

(cid:88)


 (λ(cid:62)λ)





ω2
i

i∈[n]

i∈[n]

i∈[n]
(cid:16)

≤

C
K2

pn(Π(cid:62)Π)2(λ(cid:62)λ) + p1/2

(cid:17)
n (Π(cid:62)Π)(λ(cid:62)λ)

= o(1).

Furthermore, we can show that

C
K2 p1/2


(cid:88)

P 2
ii



i∈[n]

C
K2





(cid:88)

i∈[n]


2

|ωiai|



≤

(cid:88)

Pii |ai| ≤

C
K

i∈[n]






C
K



2

C
K2

(cid:88)



i∈[n]

Pii |ai|



≤

n (Π(cid:62)Π)(a(cid:62)a) = o(1),

1/2

(cid:16)

(cid:17)1/2

a(cid:62)a

≤

C
K

(pnK)1/2 (cid:16)

(cid:17)1/2

a(cid:62)a

= o(1),

C
K2





(cid:88)



P 2
ii



i∈[n]

(cid:16)

(cid:17)
a(cid:62)a

≤

C
K2 pnK

(cid:16)

a(cid:62)a

(cid:17)

= o(1).

To prove statement (e), we show that

(cid:88)

i∈[n]

(cid:88)

ω2

i Πi

λi
Mii

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤

C
K



(cid:88)



PijωiΠi

(cid:88)

ω2
i

i∈[n]


2



≤

λi
Mii

(cid:12)
(cid:12)
(cid:12)
(cid:12)

λi
Mii

(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤







1/2

(cid:88)

ω4
i



i∈[n]

(cid:16)

(cid:17)1/2

λ(cid:62)λ

≤

C
K

n (Π(cid:62)Π)(λ(cid:62)λ)1/2 = o(1),
p1/2


2

(cid:88)

|Pij||ωi||λi|



≤

(cid:88)

C
K2

j∈[n]

i(cid:54)=j

C
K2









(cid:88)

(cid:88)



ω2
i



(cid:88)



ijλ2
P 2
i



j∈[n]

i(cid:54)=j

i(cid:54)=j

C
K





(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

C
K

C
K2

≤

C
K2

i(cid:54)=j

j∈[n]
CKp1/2

n Π(cid:62)Πλ(cid:62)λ

K2



(cid:88)

(cid:88)



P 2

ijΠi

j∈[n]

i(cid:54)=j

= o(1),


2



≤

λi
Mii

C
K2

(cid:88)





(cid:88)

j∈[n]

i(cid:54)=j



2

P 2

ij|λi|



≤

CKpnλ(cid:62)λ
K2

= o(1),

55

C
K

(cid:88)

(cid:88)

j∈[n]

i(cid:54)=j

P 2
ij

(cid:12)
(cid:12)
(cid:12)
(cid:12)

Πi

λi
Mii

(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤

C
K

(cid:88)

(cid:88)

P 2

ij|Πiλi| ≤

C
K

pn(Π(cid:62)Π)1/2(λ(cid:62)λ)1/2 = o(1),





(cid:88)

i(cid:54)=j,k

ijP 2
P 2

ikΠi

λi
Mii

i∈n

j∈[n]

2



≤

C
K2

(cid:88)

(cid:88)





(cid:88)

j∈[n]

k(cid:54)=j

i(cid:54)=j,k



2

ijP 2
P 2

ik|λi|



(cid:88)

(cid:88)

k(cid:54)=j

j∈[n]


C
K2

≤

C
K2

(cid:88)

(cid:88)

(cid:88)



j∈[n]

k(cid:54)=j

i(cid:54)=j,k


 λ(cid:62)λ ≤

ijP 4
P 4
ik

Cp3

nKλ(cid:62)λ
K2

= o(1),

where we have used Mikusheva and Sun (2022, Lemma S1.1(ii)).

Finally, we can show that Mikusheva and Sun (2022, Lemma S3.1) also holds under our condi-

tions by using similar arguments. We omit the details for brevity.

References

Anatolyev, S. and N. Gospodinov (2011). Speciﬁcation testing in models with many instruments.

Econometric Theory 27 (2), 427–441.

Anatolyev, S. and A. Mikusheva (2022). Factor models with many assets: strong factors, weak

factors, and the two-pass procedure. Journal of Econometrics 229 (1), 103–126.

Anderson, T., N. Kunitomo, and Y. Matsushita (2010). On the asymptotic optimality of the liml

estimator with possibly many instruments. Journal of Econometrics 157 (2), 191–204.

Andrews, D. W. and X. Cheng (2012). Estimation and inference with weak, semi-strong, and strong

identiﬁcation. Econometrica 80 (5), 2153–2211.

Andrews, D. W. and P. Guggenberger (2019). Identiﬁcation-and singularity-robust inference for

moment condition models. Quantitative Economics 10 (4), 1703–1746.

Andrews, D. W. K. and J. H. Stock (2007). Testing with many weak instruments. Journal of

Econometrics 138 (1), 24–46.

Andrews, I. (2016). Conditional linear combination tests for weakly identiﬁed models. Economet-

rica 84 (6), 2155–2182.

Andrews, I. (2018). Valid two-step identiﬁcation-robust conﬁdence sets for GMM. Review of

Economics and Statistics 100 (2), 337–348.

Andrews, I. and A. Mikusheva (2016). Conditional inference with a functional nuisance parameter.

Econometrica 84 (4), 1571–1612.

56

Andrews, I., J. H. Stock, and L. Sun (2019). Weak instruments in instrumental variables regression:

Theory and practice. Annual Review of Economics 11 (1), 727–753.

Angrist, J. and B. Frandsen (2022). Machine labor. Journal of Labor Economics 40 (S1), S97–S140.

Angrist, J., G. Imbens, and A. Krueger (1999). Jackknife instrumental variables estimates. Journal

of Applied Econometrics 14 (1), 57–67.

Angrist, J. D. and A. B. Krueger (1991). Does compulsory school attendance aﬀect schooling and

earning? Quarterly Journal of Economics 106 (4), 979–1014.

Athey, S., J. Tibshirani, and S. Wager (2019). Generalized random forests. The Annals of Statis-

tics 47 (2), 1148–1178.

Bartik, T. J. (1991). Who beneﬁts from state and local economic development policies? Kalamazoo,

MI: WE Upjohn Institute for Employment Research.

Bekker, P. (1994). Alternative approximations to the distributions of instrumental variable estima-

tors. Econometrica 62 (3), 657–681.

Belloni, A., D. Chen, V. Chernozhukov, and C. Hansen (2012). Sparse models and methods for

optimal instruments with an application to eminent domain. Econometrica 80 (6), 2369–2429.

Belloni, A., V. Chernozhukov, and C. Hansen (2011). Inference for high-dimensional sparse econo-

metric models. arXiv preprint arXiv:1201.0220 .

Blanchard, O. J., L. F. Katz, R. E. Hall, and B. Eichengreen (1992). Regional evolutions. Brookings

Papers on Economic Activity 1992 (1), 1–75.

Carrasco, M. (2012). A regularization approach to the many instruments problem. Journal of

Econometrics 170 (2), 383–398.

Carrasco, M. and G. Tchuente (2015). Regularized liml for many instruments. Journal of Econo-

metrics 186 (2), 427–442.

Chamberlain, G. and G. Imbens (2004). Random eﬀects estimators with many instrumental vari-

ables. Econometrica 72 (1), 295–306.

Chao, J. C. and N. R. Swanson (2005). Consistent Estimation with a Large Number of Weak

Instruments. Econometrica 73 (5), 1673–1692.

Chao, J. C., N. R. Swanson, J. A. Hausman, W. K. Newey, and T. Woutersen (2012). Asymp-

totic distribution of jive in a heteroskedastic iv regression with many instruments. Econometric

Theory 28 (1), 42–86.

57

Crudu, F., G. Mellace, and Z. S´andor (2021).

Inference in instrumental variable models with

heteroskedasticity and many instruments. Econometric Theory 37 (2), 281–310.

Dobbie, W., J. Goldin, and C. S. Yang (2018). The eﬀects of pretrial detention on conviction,

future crime, and employment: Evidence from randomly assigned judges. American Economic

Review 108 (2), 201–40.

Donald, S. G. and W. K. Newey (2001). Choosing the number of instruments. Econometrica 69 (5),

1161–1191.

Fama, E. F. and J. D. MacBeth (1973). Risk, return, and equilibrium: Empirical tests. Journal of

Political Economy 81 (3), 607–636.

Fuller, W. A. (1977). Some properties of a modiﬁcation of the limited information estimator.

Econometrica 45 (4), 939–953.

Goldsmith-Pinkham, P., I. Sorkin, and H. Swift (2020). Bartik instruments: What, when, why,

and how. American Economic Review 110 (8), 2586–2624.

Han, C. and P. C. Phillips (2006). Gmm with many moment conditions. Econometrica 74 (1),

147–192.

Hansen, C., J. Hausman, and W. Newey (2008). Estimation with many instrumental variables.

Journal of Business and Economic Statistics 26 (4), 398–422.

Hansen, C. and D. Kozbur (2014). Instrumental variables estimation with many weak instruments

using regularized jive. Journal of Econometrics 182 (2), 290–308.

Hausman, J. A., W. K. Newey, T. Woutersen, J. C. Chao, and N. R. Swanson (2012). Instrumental

variable estimation with heteroskedasticity and many instruments. Quantitative Economics 3 (2),

211–255.

Kleibergen, F. (2002). Pivotal statistics for testing structural parameters in instrumental variables

regression. Econometrica 70 (5), 1781–1803.

Kleibergen, F. (2005). Testing parameters in GMM without assuming that they are identiﬁed.

Econometrica 73 (4), 1103–1124.

Kline, P., R. Saggio, and M. Sølvsten (2020). Leave-out estimation of variance components. Econo-

metrica 88 (5), 1859–1898.

Koles´ar, M. (2018). Minimum distance approach to inference with many instruments. Journal of

Econometrics 204 (1), 86–100.

58

Kubokawa, T., C. P. Robert, and A. M. E. Saleh (1993). Estimation of noncentrality parameters.

The Canadian Journal of Statistics 21 (1), 45–57.

Kuersteiner, G. and R. Okui (2010). Constructing optimal instruments by ﬁrst-stage prediction

averaging. Econometrica 78 (2), 697–718.

Kunitomo, N. (1980). Asymptotic expansions of the distributions of estimators in a linear func-

tional relationship and simultaneous equations. Journal of the American Statistical Associa-

tion 75 (371), 693–700.

Lee, D. S., J. McCrary, M. J. Moreira, and J. R. Porter (2022). Valid t-ratio inference for iv.

American Economic Review forthcoming.

Lehmann, E. L. and J. P. Romano (2006). Testing statistical hypotheses. Springer Science &

Business Media.

Maestas, N., K. J. Mullen, and A. Strand (2013). Does disability insurance receipt discourage

work? using examiner assignment to estimate causal eﬀects of ssdi receipt. American Economic

Review 103 (5), 1797–1829.

Marden, J. I. (1982). Combining independent noncentral chi squared or f tests. The Annals of

Statistics 10 (1), 266–277.

Matsushita, Y. and T. Otsu (2020). Jackknife Lagrange multiplier test with many weak instruments.

LSE, STICERD.

Matsushita, Y. and T. Otsu (2021). Jackknife empirical likelihood: small bandwidth, sparse network

and high-dimensional asymptotics. Biometrika 108 (3), 661–674.

Mikusheva, A. and L. Sun (2022). Inference with many weak instruments. Review of Economic

Studies forthcoming.

Moreira, H. and M. J. Moreira (2019). Optimal two-sided tests for instrumental variables regression

with heteroskedastic and autocorrelated errors. Journal of Econometrics 213 (2), 398–433.

Moreira, M. J. (2003). A conditional likelihood ratio test for structural models. Econometrica 71 (4),

1027–1048.

Morimune, K. (1983). Approximate distributions of k-class estimators when the degree of overi-

dentiﬁability is large compared with the sample size. Econometrica 51 (3), 821–841.

Newey, W. K. and F. Windmeijer (2009). Generalized method of moments with many weak moment

conditions. Econometrica 77 (3), 687–719.

59

Okui, R. (2011).

Instrumental variable estimation in the presence of many moment conditions.

Journal of Econometrics 165 (1), 70–86.

Sampat, B. and H. L. Williams (2019). How do patents aﬀect follow-on innovation? evidence from

the human genome. American Economic Review 109 (1), 203–36.

Shanken, J. (1992). On the estimation of beta-pricing models. The Review of Financial Studies 5 (1),

1–33.

Sølvsten, M. (2020). Robust estimation with many instruments. Journal of Econometrics 214 (2),

495–512.

Stock, J. and M. Yogo (2005a). Asymptotic distributions of instrumental variables statistics with

many instruments, Volume 6. Chapter.

Stock, J. H. and J. H. Wright (2000). GMM with weak identiﬁcation. Econometrica 68 (5), 1055–

1096.

Stock, J. H. and M. Yogo (2005b). Testing for weak instruments in linear IV regression. In D. W.

Andrews and J. H. Stock (Eds.), Identiﬁcation and Inference for Econometric Models: Essays in

Honor of Thomas Rothenberg, Chapter 6, pp. 80–108. Cambridge, U.K.: Cambridge University

Press.

Wang, W. and M. Kaﬀo (2016). Bootstrap inference for instrumental variable models with many

weak instruments. Journal of Econometrics 192 (1), 231–268.

60

