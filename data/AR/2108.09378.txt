1
2
0
2

g
u
A
0
2

]

V
C
.
s
c
[

1
v
8
7
3
9
0
.
8
0
1
2
:
v
i
X
r
a

IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS 2021

1

A Multiple-View Geometric Model for Specularity
Prediction on Non-Uniformly Curved Surfaces

Alexandre Morgand, Mohamed Tamaazousti and Adrien Bartoli

Abstract—Specularity prediction is essential to many computer vision applications. In Augmented Reality (AR), it improves the realism
of virtual objects inserted in a live video stream by providing a coherent shape, appearance and motion to specularities. Specularity
prediction also gives important visual cues that could be used in Simultaneous Localisation and Mapping (SLAM), 3D reconstruction
and material modeling, thus improving scene understanding. However, specularity prediction is a challenging task requiring numerous
information from the scene including the camera pose, the geometry of the scene, the light sources and the material properties. Our
previous work have addressed this task by creating an explicit model using an ellipsoid whose projection ﬁts the specularity image
contours for a given camera pose. These ellipsoid-based approaches belong to a family of models called JOint-LIght MAterial
Specularity (JOLIMAS), where we have attempted to gradually remove assumptions on the scene such as the geometry of the
specular surfaces to provide real-time specularity prediction. However, our most recent approach is still limited to uniformly curved
surfaces. This paper builds upon these methods by generalising JOLIMAS to any surface geometry while improving the quality of
specularity prediction, without sacriﬁcing computation performances. The proposed method establishes a link between surface
curvature and specularity shape in order to lift the geometric assumptions from previous work. Contrary to previous work, our new
model is built from a physics-based local illumination model namely Torrance-Sparrow, providing a better model reconstruction.
Specularity prediction using our new model is tested against the most recent JOLIMAS version on both synthetic and real sequences
with objects of varying shape curvatures, reconstructed using a CAD model or the Kinect v2 depth camera. Our method outperforms
previous approaches in specularity prediction, including the real-time setup, as shown in the supplementary material using videos.

Index Terms—Specularity, Prediction, Augmented Reality, Curved, Quadric.

(cid:70)

1 INTRODUCTION

L IGHT is of tremendous interest in many ﬁelds of science,

including physics and computer vision. In computer graphics,
visual effects and AR, illumination plays a crucial role in render-
ing an object realistically. However in AR, mixing virtual objects
and real scene elements is particularly challenging to achieve
because the lighting conditions of the virtual scene have to match
the real ones while maintaining real-time rendering performances.
Among the important lighting elements to render, the specular
reﬂections, also called specular highlights or specularities, are
given as the most important elements to render to improve the
realism of virtually inserted objects [2], [12], [24], [26]. In human
perception, according to Black et al. [3], specularities are essential
elements to distinguish shapes and the motion of shiny objects.
Isolating a specularity in an image is challenging because its
appearance depends on numerous elements in the scene, including
the camera pose, the geometry of the scene, the light sources, the
roughness and material properties of the surface. This dependency
is important because in an AR context, rendering specularities
correctly emphasises the perception of an object material and its
geometry, should the specularity motion be coherent. However,
accessing all these elements is still an open problem since knowing
the lighting conditions requires one to compute many parameters.

• A. Morgand is with SLAMcore ltd, London, UK

E-mail: morgand.alexandre@gmail.com

• M. Tamaazousti is with Université Paris Saclay, CEA, LIST, Gif-sur-Yvette,

France.
E-mail: mohamed.tamaazousti@cea.fr

• A. Bartoli is with IP-UMR 6602 - CNRS/UCA/CHU, Clermont-Ferrand,

France.
E-mail: adrien.bartoli@gmail.com

The specularity’s motion may also vary through the video stream
due to the varying surface geometry (ﬂexible surfaces), different
types of materials present in the scene or light sources that could
the task of specularity
be switched on and off. To sum up,
prediction is challenging but offers a strong potential realism in
AR applications, motivating its study in the current literature [8],
[20], [23], [34], [36].

Generally speaking, the existing methods fall in three cat-
egories: (1) light source modelling methods, including global
the whole lighting context of
illumination, which reconstruct
the scene, and local
illumination models, computing the 3D
position of the illumination and specularity prediction; (2) deep
learning methods using synthetic and real databases of images
for novel viewpoint generation through latent space interpolation
and (3) multiple-view reconstruction approaches. Aside of these
categories methods [45], [46], [53] do not model specularities
explicitly but include them in a process of lighting condition
reconstruction. They require one to compute numerous parame-
ters on the materials and light sources, and cannot predict the
specularity on new viewpoints.

Category (3) is the most recent, which we have proposed. We
have shown in [40]–[43] that a specularity on a planar surface
can be well approximated by an ellipse under a light bulb or
a ﬂuorescent lamp illumination found commonly indoors. We
have proposed a family of models called JOint LIght-MAterial
Specularity (JOLIMAS) to abstract the light-matter interaction and
treat the problem geometrically. This family of models uses a ﬁxed
3D ellipsoid whose projection predicts the specularity’s shape
in new viewpoints. The 3D ellipsoid reconstruction is achieved
from at least three viewpoints. However, the existing approaches
are restricted to planar or slightly curved surfaces, which is not

 
 
 
 
 
 
IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS 2021

2

ideal in a real environment including curved surfaces. JOLIMAS
requires camera pose and scene structure to be available. In
practice owing to the recent advances in visual SLAM, including
numerous methods publicly available, providing real-time camera
localisation [9], [15], [49], [51] and 3D reconstruction [44], [55],
obtaining quality camera pose and 3D geometry of the scene in
real-time as inputs for specularity prediction is now possible.

We describe a twofold contribution in geometric specularity
modelling. First, we present a generalisation of the specularity
prediction model JOLIMAS to any surface geometry. This works
by transforming the specularity contours according to the local
curvature of the surface. The idea behind this transformation is to
model how these contours would be if the surface were deformed
to a plane using the Torrance-Sparrow local illumination model
[54]. This represents the ﬁrst attempt to introduce a physics-based
illumination model to the JOLIMAS modelling approaches. Then,
we reconstruct a canonical representation of the JOLIMAS model
as an ellipsoid from the transformed contours. This ellipsoid will
remain constant for any surface deformation associated to the
specularity of interest. Secondly, we propose an inverse transfor-
mation of these contours when performing specularity prediction.
This prediction is obtained by projecting the ellipsoid from our
canonical model to the image and transforming the contours to
ﬁt the current local curvature. This process runs in real-time for
object with known geometry in the scene and with camera pose as
inputs.

First, we present the related work in specularity prediction
in section 2 to position our method within the state-of-the-art.
We then state the formal specularity prediction problem and give
background in section 3 to propose the canonical dual JOLIMAS
approach in section 4. To assess the efﬁciency of our method,
we conduct two quantitative experiments on synthetic data and
show qualitative results on real sequences for objects of varying
curvature in section 5.

2 RELATED WORK
Modelling lighting effects like shadows and specularities is a
complex problem. This is because they depend on interdependent
elements such as surface material, geometry and roughness. It
is thus relevant to focus ﬁrst on the specularity prediction task
which is less strongly dependent and a logical ﬁrst step for light
conditions understanding. It is also interesting to consider spec-
ularities due to their importance in computer vision. By adding
variations to the image intensity, numerous algorithms are affected
by specularities in the image such as segmentation, reconstruction
and camera localisation methods. However, specularities provide
relevant information on the light sources, the scene geometry
and materials. Due to specularity prediction being a fairly recent
research topic, the state-of-the-art is yet limited. We describe
the three major specularity prediction approaches: light sources
modelling methods using global and local illumination models,
deep learning methods and geometric modelling methods.

2.1 Light Source Modelling Methods

Global illumination models

Methods in this category favour the rendering quality by using
the rendering equation [19], [21]. This equation describes the
total amount of light emitted from a point along a particular
viewing direction, given a function for the incoming light and

a Bidirectional Reﬂectance Distribution Function (BRDF). These
approaches do not generally compute the physical attributes of the
light sources. For instance, [20] captures a 4D light ﬁeld over a
small planar specular surface. By reconstructing the diffuse and
specular components, it achieves convincing rendering. However,
it is unable to predict the specular component for new viewpoints
unseen during initial reconstruction. Moreover, light sources with
changing states (turned on and off) are not handled. Recently,
[53] extended [20] by adding material segmentation for complex
surfaces reconstructed using an RGB-D sensor, but shared the
same limitations as [20]. As a consequence, [20], [53] and similar
approaches such as [36] cannot predict specular reﬂections for
new viewpoints and changing lights.

Despite the quality of the results, these methods lack ﬂex-
ibility. Indeed, they represent an extended light source such as
a ﬂuorescent lamp by several point light sources. Thus, these
methods need to compute the lighting intensity of different point
light sources instead of treating them as a unique light source.
Moreover, dynamic lighting cannot be used and materials for the
predicted specularity are not computed. As a consequence, these
methods are not suited for specularity prediction for unknown
viewpoints Also, due to a long initialisation process, these meth-
ods require numerous images and processing power which is not
adapted to the real-time context of AR.

Local illumination models

In parallel, other works have been proposed on primary light
source reconstruction. Ideally, each light source should be associ-
ated to a geometry (position and shape), a colour and an intensity
value in order to match the scene [31], [33] realistically. Although
numerous light source models exist in computer graphics, the
models used in computer vision are generally divided in two
categories: directional light sources and point light sources. For
external scenes, a directional light source feels more natural to
represent the sun but could also be useful for indoors scenes
(ceiling light sources for instance). [28] describes a method to
compute directional light sources from a ﬁxed viewpoint. The
application is limited because specularities are strongly dependent
on the viewpoint and the light sources need to be estimated for
each pose. Neither the shape and position of the light source, nor
the object material are taken into account, making this method
unable to predict specularities explicitly and accurately. More-
over, extended light sources such as ﬂuorescent lamps cannot
be modeled correctly by point light sources, limiting the method
applicability.

Related approaches [6], [8], [14], [22], [56] suffer from similar
issues and limitations. For example, in light source estimation, [6]
computes a primary light source under the Lambertian assumption
and using an RGB-D camera. From the diffuse term and by
comparing a synthetic rendering of the scene with the current
image, a unique point light source is estimated. However, this
method is not adapted to real-time applications with multiple light
sources that could be point light sources such as a desktop lamp
or a light bulb or extended sources like a ﬂuorescent lamp. By
computing only one light source without knowledge of the shape
and materials of the surfaces of the scene, it is challenging to
perform a realistic and coherent specularity prediction.

Specularity prediction from a parametric light source recon-
struction requires the computation of numerous complex and often
ambiguous light and material parameters. For instance, the joint

IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS 2021

3

estimation of material reﬂectance and light source intensity is an
ill-posed problem, as stated in [34]. In practice, a physics-based
approach like the aforementioned ones cannot effectively predict
a photometric phenomena as complex as a specularity.

2.2 Deep Learning Methods

The impact of deep learning methods in computer vision is
growing rapidly, owing to the recent increase in processing power
thanks to GPU’s and CPU’s, sometimes solely dedicated to ma-
chine learning. The current access to huge labelled image datasets
such as ImageNet [13], COCO [32] and already pretrained models
such as VGG-19 [50] and ResNet [17] for feature extraction
democratised the use of deep learning. The lighting modelling
ﬁeld is no exception to this trend. For deep global illumination,
[35] takes a 3D object with known albedos as inputs. Their
Convolutional Neural Network (CNN) takes the coefﬁcients of
the spherical harmonics function computed as inputs in order
to generalise the illumination after training. This method shows
results from a single image but requires a training time of four
hours per pose and the illumination is limited to the learnt object,
which limits the range of applications of the method. [29] and
more particularly [45] were an interesting continuity of [20],
showing impressive results in terms of specularity prediction for
new viewpoints by modelling high specular objects using an RGB-
D camera to reconstruct a light ﬁeld. [45] was also able to model
inter-reﬂections showing how far novel view synthesis could go
using an adversarial approach. However, this method remains
limited to the object of interest and does not provide strong
insights on scene understanding in case the lighting conditions
change or the camera viewpoint drastically changes.

Another interesting recent approach, perhaps more related to
3D inpainting, is [1]. It generates novel viewpoints from RGB-
D and point clouds data. Without any prior in terms of lighting
modelling, this method interpolates what the lighting would be
for a novel viewpoint. Even though this method is not directly
aiming at realistic lighting conditions, it highlights the interpola-
tion capability of deep learning based methods and shows how
the recent methods in the state-of-the-art require heavy training
and processing. This is not suitable for real-time application and
lacks ﬂexibility when the camera moves too far from the initial
scene or if the scene is too different from the training database.
Additionally, very few image databases contain lighting informa-
tion and more particularly annotated specularity images due to
the difﬁculty to segment specularities in the images manually.
To address this issue, synthetic databases are a decent solution
to have a distinct specular/diffuse component separation but are
sometimes not enough to generalise the network to real images.
More recently, a new family of approaches called Neural Radiance
Fields (NeRF) greatly improved the quality of novel viewpoint
rendering for non-Lambertian reﬂectance [37]. Even though this
area of research is improving rapidly in terms of rendering speed
and quality in the presence of slight object variations, NeRF meth-
ods require a long learning time and requires great computational
power to make the rendering real-time. Moreover, these methods
do not explicitly describe the light interactions but output the RGD
and radiance information for any given point.

2.3 Geometric Modelling Methods

Our previous work initiated a family of approaches called JOLI-
MAS as an original alternative and complement to other methods

in specularity prediction. It addresses several limitations by con-
sidering specularities as a geometric cue in the scene. The ﬁrst
version, Primal JOLIMAS [41], showed that for planar glossy
surfaces, a specularity created from a light bulb or ﬂuorescent
lamp has an elliptical shape. By reconstructing a ﬁxed 3D ellipsoid
from several image ellipses, this method predicts the specular
shape for new viewpoints by simple perspective projection of the
3D ellipsoid. It abstracts light and material interactions, and can
be used for retexturing. Speciﬁcally, primal JOLIMAS uses the
following hypotheses:

Specular reﬂections are elliptical on planar surfaces.
1)
2) A light source is generally associated with a single

specularity on planar surfaces.

3) There is a unique ﬁxed 3D ellipsoid located ‘under’
the planar surface whose perspective projection ﬁts the
specular shape in the image.

Hence, Primal JOLIMAS fails to predict the specularity if the
surface is non-planar, which drastically limits its applicability. For
non-planar surfaces, the reconstructed 3D ellipsoid is not ﬁxed in
space because ellipses are not epipolar consistent, as illustrated
in ﬁgure 1. The problem is more complex because the reﬂected
image of the light source (specularity) through a curved surface
(mirror or specular surface) can be drastically distorted. To solve
this issue, the distortion of the specularity should be included in
the model.

(a)

(b)

Figure 1: Epipolar geometry of the ellipses for three camera poses
Π1, Π2 and Π3 and a point light source L using Primal JOLIMAS
on a sphere. The scene is showed in (a) and the associated
ellipses and epipolar lines in (b). The epipolar geometry is not
respected. This results in an incorrect 3D ellipsoid reconstruction
and incorrect specularity prediction. Figure extracted from [42].

We proposed a second, extended version called Dual JOLI-
MAS [42], which uses virtual cameras computed from the camera
poses reﬂected by the tangent plane at the brightest point of
the specularity observed at the current pose. This virtual camera
system was enough to reconstruct the 3D ellipsoid and predict
specularities on non-planar surfaces, as shown in ﬁgure 2.

A high-level geometric representation of the light sources and
more particularity specularities is beneﬁcial for many applications.
In the current state, the JOLIMAS approaches have a strong
potential for a wide range of applications.

IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS 2021

4

understanding of the specularity behaviour from a geometric
standpoint. In a context where the camera pose and the scene
geometry are known, the specularity movement can thence be
predicted. However, the question of explaining and exploiting the
specularity’s shape transformation due to curvature changes has
been left unresolved. In this paper, we present Canonical Dual
JOLIMAS, a generalisation of the JOLIMAS model family for
specularity prediction obtained by embedding the link between
curvature and specularity shape to alleviate the geometric assump-
tions on the surface shape made by the two previous versions of
JOLIMAS. This widely opens the applicability of JOLIMAS to
real world problems.

3 BACKGROUND AND PROBLEM STATEMENT

In multiple view geometry, we distinguish several cases to re-
construct a static object from perspective projections. The easiest
case is where an object is directly observed. Points, lines or
other geometric primitives are modelled in the image by simple
perspective projection according to camera pose. For an object
observed through a perfect mirror, the visual cues are modelled
in the image by computing the symmetry according to the normal
of the mirror. Estimating the shape and extent of a light source
observed in an image is an ill-posed problem. For a point light
source directly observed in an image, its intensity combined with
the light sensitivity of the camera makes the appearance of the
light source ambiguous, even when the camera sensor parameters
are ﬁxed. When reﬂected on a surface, due to material properties,
the reﬂection of the light source will also be ambiguous and
will not exactly match the original shape of the light source.
As a consequence, geometrically recovering a light source in a
consistent and accurate way is difﬁcult because this light source
shape increases drastically according to the global intensity of the
image. In the presence of a specular surface, in addition to the
intensity of the light source and sensor light sensitivity, the image
of the point light source is observed through a shiny surface with
a mirror-like behaviour and is affected by the surface’s material
(reﬂectance and roughness). This implies that this image does not
match the perspective projection of the light source shape. This
mismatch between the real shape of the light source and the one
observed in the image seems to be, up to scale, the light source
real size. We can link specularity geometric modelling methods to
Structure-from-Motion methods for 3D reconstruction from mirror
reﬂections such as [27], [38], [48]. Most of the theory behind the
JOLIMAS model relies on the existence of a ﬁxed 3D object in
space reﬂected by a specular surface. If we ﬁnd the relationship
between surface curvature (mirror or specular) and 3D object to
reconstruct (ellipsoid in the case of JOLIMAS), it would allow us
to generalise the specularity prediction model to any curvature of
the specular surface.

Objects observed through a planar mirror are deformed in the
case of a curved mirror, as illustrated in ﬁgure 3. The observation
is similar for a specularity observed through the red book with
a plastic cover. We also observed that in the case of the mirror
surface, the ﬁeld of view seems wider despite the surface area
remaining the same. A ﬁrst hypothesis would be that the curvature
change implies ﬁeld of view variation. Since the surface area
remains constant during the deformation, the area of the object
image decreases when the surface becomes concave and increases
when the surface become convex. This link is further highlighted
in our synthetic data in ﬁgure 5. The camera ﬁeld of view is

(a)

(b)

Figure 2: Illustration of Dual JOLIMAS [42], addressing the
issue of specularity prediction on non-planar surfaces. In (a), the
computation of virtual cameras Πs,1, Πs,2 and Πs,3 from the real
cameras Π1, Π2 and Π3 is shown, allowing one to reconstruct an
ellipsoid located near the light source. In (b), as opposed to ﬁgure
1(b), the epipolar lines ﬁt the ellipse. Figure extracted from [42].

Existing applications

In AR, global illumination methods require a consequent initial-
isation process to provide a good rendering but lack ﬂexibility
for new viewpoints. For retexturing applications, JOLIMAS can
be used instead of relying on global or local illumination models
[40], [43]. For scene analysis methods, it is beneﬁcial to have a
quick way to retrieve the state of the light sources (turned on and
off), which is hard to obtain with state of the art methods compared
to JOLIMAS, as shown in [41].

Potential applications

In an autonomous driving assistance system (ADAS), distinguish-
ing efﬁciently specularities from white road lines is important
and can use JOLIMAS as a specularity prediction system to
make the specularity detection less ambiguous and the driving
safer [25]. In Diminished Reality, when removing an object from
an image stream, it is difﬁcult to synthesize light cues such as
specularities to provide a plausible rendering. [47] proposed an
inpainting approach of partially cut specularities but lacks light
source representation in cases when there are not enough data
to recover the specularity due to the object removal. In local
illumination models, it is difﬁcult to accurately triangulate the
light source position without a proper specularity model, as shown
in [16]. In machine learning, collecting a proper image database
with segmented specularities is a long and tedious process which
could be tremendously spedup with a proper specularity geometric
model for both specularity synthesis, segmentation and removal
applications. Current state of the art relies heavily on synthetic
image databases as shown in [30], or build a database from real
images using specularity detection system such as [25], [39].
However, Dual JOLIMAS is limited to uniformly curved surfaces
and fails for changes in the surface curvature.

2.4 Contributions

The relationship between surface curvature and specularity move-
ment was studied in [4] showing promising results for a better

IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS 2021

5

Figure 3: Mirrored images of a 3D scene reﬂected by two types
of surfaces. The top row shows two mirror objects: a plane (left)
and a sphere (right). The bottom row shows a book with a shiny
cover bent from a plane (left) to a cylinder (right). The reﬂected
images of 3D objects (a plastic statue and a smartphone for the
mirror surfaces and a light source for the specular surface) are
deformed according to the local curvature of the surface. During a
specularity prediction process, these deformations must be taken
into account.

deﬁned by the observable world that is seen at any given moment
and is parameterised by two components: an horizontal and a
vertical angles. In the presence of a mirror surface, this ﬁeld of
view can be increased, for instance with a rear view mirror or a
catadioptric camera. Indeed, light rays are reﬂected on the mirror
surface, giving a wider ﬁeld of view. It is possible to change the
ﬁeld of view reﬂected by this mirror by changing its curvature.
More precisely, when one bends a planar mirror to a convex shape,
the ﬁeld of view increases and respectively decreases when bent
into a concave shape. This property is illustrated in ﬁgure 4.

Figure 5: Illustration of the relationship between curvature and
ﬁeld of view using a synthetic 3D scene containing a cube, a
sphere, a light source and a reﬂective surface (mirror and plastic
surface). In order to illustrate the change of curvature of the
surface and change of ﬁeld of view, we curve the reﬂective surface
of the scene progressively and observe changes on the mirror
reﬂections (ﬁrst row) and of the specularity (second row). In the
third row, an aerial view of the 3D scene is shown and ﬁeld of view
limits are drawn in dotted green lines. We observe in a similar way
to ﬁgure 3, that the 3D scene reﬂected by the mirror is deformed
(ﬁrst row) and that the specularity is also deformed (second row).
This experiment shows that the ﬁeld of view increases when the
surfaces change from planar to convex when observing mirror-like
surfaces. As opposed to the deformation applied to the texture of a
surface that we bend which remains at the same physical location,
a reﬂected image is affected by the reﬂected rays of light of the
specular surface.

Figure 4: Relationship between curvature and ﬁeld of view in the
case of planar, convex and concave mirrors (from left to right). The
ﬁeld of view is estimated by computing the reﬂected rays (r) using
the incident rays (i) from the current viewpoint (O). We observe
that the ﬁeld of view increases when the mirror bends convex and
reduces when the mirror bends concave.

4 PROPOSED APPROACH - CANONICAL DUAL

JOLIMAS

We generalise the JOLIMAS model to a canonical form to be able
to predict specularities on any surface geometry regardless of its
curvature. We ﬁrst explain the use of the local curvature to make
the link between curvature and specularity contour changes. This
canonical dual JOLIMAS is illustrated in ﬁgure 6.

(a)

Figure 6: Illustration of the new canonical dual JOLIMAS model.
For each real camera pose (Π1, Π2 and Π3), we create virtual
camera poses (Πs,1, Πs,2 and Πs,3) computed from the brightest
point of each specularity. By synthesizing the specularity shape
on the tangent plane located at the brightest point for each image,
we can reconstruct an ellipsoid located near the light source L in
spite of a varying surface curvature.

ﬁeld of viewLarger ﬁeld of viewConvex mirrorConcave mirrorNarrower ﬁeld of viewIEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS 2021

6

4.1 Use of Local Curvature
From the method [18], which is an application of Shape-from-
Specularity consisting in reconstructing the object geometry from
the movement of a specularity on a surface, several ideas and
hypotheses are interesting to explore in order to ﬁnd the link be-
tween local curvature and specularity shape. Despite the plausible
rendering quality that Phong’s model provides, [18] points out that
the parameters of Phong’s model are not physics-based, which
is unﬁt for a coherent and realistic specularity modelling. More
precisely, [18] use the Torrance-Sparrow model [54] developed by
physicists, giving a detailed formulation on the specular compo-
nent. This model assumes that a surface is composed of randomly
oriented microfacets and having a similar mirror-like behaviour.
This model also quantiﬁes facets occluded by other adjacent facets
using a geometric attenuation factor. The specular model is then
described by three factors:

IS = FDA,

(1)

with F the Fresnel coefﬁcient, D the function describing the
distribution and orientation of a facet and A the geometric at-
tenuation factor. Coefﬁcient F models the light quantity reﬂected
by each facet. In general, F depends on the incident light ray and
a refraction index of the reﬂecting material. According to [11],
F deﬁnes the specularity colour in order to synthesize realistic
images. The distribution function D describes the microfacets
orientation according to the surface normal N. In the case of
Torrance-Sparrow [54], a Gaussian distribution is used to describe
D as:

D = Ke−(α/m)2

,

(2)

where K is a normalisation constant, m a roughness index and
α the incident angle. Thus, for a given α, D is proportional to
ˆH describing
the oriented facets to the direction of the vector
the half-way vector [5]. In order to analyse the specularity shape,
it is not necessary to have all of equation (1) parameters. F is
a nearly constant function of the incident angle for the class of
materials with a large extinction coefﬁcient like metals and other
strongly reﬂective materials. For simplicity, [18] considers F as
constant with respect to the viewing geometry and is valid for most
reﬂective materials. Moreover, [18] shows that the exponential
factor in equation (2) varies rapidly compared to the other terms,
allowing one to simplify equation (1) to:
IS = K(cid:48)e−(α/m)2

(3)

,

where K(cid:48)
is constant. [18] also mentions the existence of a
brightest point such that the intensity is maximal at this point. By
studying the variation of intensity from this point, it is possible to
deduce the surface curvature. The link between intensity IS at a
point P and the incident angle is given by:

(cid:114)

|α| = m

−log

IS
K(cid:48) .

(4)

κ =

The link between the surface curvature and incident angle is given
by:

dα
ds
with κ the local curvature for a given direction, dα a small angle
change of the reﬂected light source and ds the arc length on
the surface at the brightest point PB. In practice, equation (5) is
difﬁcult to compute for non-parametric surfaces such as wireframe
models. Computing the arc length implies computing a geodesic

(cid:12)
(cid:12)
(cid:12)PB

(5)

,

distance, which is computationally expensive and inaccurate for
polygonal or rough surfaces, which often occurs in wireframes.
For simplicity, we deﬁne angle α as α = cos−1( ˆN · ˆH) with ˆN the
normal and ˆH the half-way vector ˆH = ˆL+ ˆV
between the light
(cid:107) ˆL+ ˆV(cid:107)
ray ˆL and the observed ray ˆV.

4.2 Practical Use on a CAD Model - Limit Angles

In order to optimise local curvature computation, we propose to
use the concept of limit angles. These correspond to angle values
α i
max such that for an angle larger than this limit, the specularity
intensity is considered null. Limit angles α i
,
(cid:75)
are computed from the specularity contours and particularly the
normals ˆN associated to the contour points and half-way vectors
ˆH. According to equation (3), the intensity of the specularity is
inﬂuenced predominantly by the angle α. The intensity associated
to the specularity contour points remains approximately indepen-
dent of the curvature of the surface, implying that the limit angles
keep their values in spite of local curvature changes at the contour
points of the specularity.

max, with i ∈

0, n
(cid:74)

In our canonical representation, in order to transform the
current ellipse associated to the specularity, we compute the
brightest point PB in a similar way as [42] and its associated
tangent plane TPB (S). We sample n vectors vi ∈ TPB (S) starting
from the point PB within a range of [0, 2π[.

The choice of n depends on the specularity size and its
shape. In practice, we ﬁxed a value of 36 for the presented
sequences in ﬁgures 10, 13 and 9. This value empirically gives
us the best results. When we reach the external contours of the
max using ˆH and ˆN at the
specularity, we compute a limit angle α i
orthogonally projected point on the surface S. Then, we follow the
same vector vi on TPB (S) until reaching a point where the angle
value is at α i
max, which is the limit associated to the vector. The
transformed ellipse is computed from the new computed contour
points. From the transformed ellipses, we are able to use the same
ellipsoid reconstruction formalism using virtual cameras, as in
dual JOLIMAS [42].

4.3 Specularity Prediction from Limit Angles

During the specularity prediction step, a similar process to [42] is
used to predict the shape of the specularity for a new viewpoint.
We compute the brightest point for the new viewpoint and we
project the reconstructed ellipsoid which gives us the specularity
shape on the tangent plane TPB (S). We then compute the limit
angles on TPB (S) and move the specularity contours on S until
reaching the limit angle on TPB (S) by taking into account the local
curvature of the surface S.

5 EXPERIMENTAL RESULTS
We present quantitative results on synthetic data to assess the
quality of our specularity prediction and qualitative results on real
data compared with the state of the art method [42]. We evaluate
the quality of our specularity prediction ability by comparing the
2D distance between specularity prediction and the contours of the
detected specularity.

5.1 Synthetic Data
Plane/cylinder sequence

In order to test the improvement of our canonical dual model
compared to the dual one, we reuse the experiment from [42]. It

IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS 2021

7

(a)

(b)

Figure 7: Ellipse transformation from its shape on the curved
surface S to the canonical representation on the tangent plane
TPB (S) at the brightest point PB. (a) shows the transformation
process starting from the ellipse ﬁtted to the specularity contours
(blue). We apply this transformation in order to obtained the
corrected ellipse (green) on the tangent plane TPB (S). In (b), we
show that the limit angles keep their values at the specularity
contours without being affected by the surface curvature (blue
for the curved surface and green for the plane).

consists of a video sequence composed by 300 synthetic images
of a cylinder varying in radius as shown in ﬁgure 8(a), in order
to simulate the curvature change from a plane to a cylinder.
is to assess the specularity shape prediction on a
The goal
varying curvature by computing the 2D distance between ellipses
given by the predicted specularity from the compared JOLIMAS
methods and the ﬁtted ellipse from detected specularity contours.
Each curvature change is observed through 6 different synthetic
viewpoints. This test is composed of two experiments. Exp 1
reconstructs the ellipsoid (canonical JOLIMAS model) for each
curvature changes while Exp 2 reconstructs it only once from the
ﬁrst 6 images. The prediction is done from this reconstruction
for every future viewpoints where the plane is morphed into
a cylinder. We present the results in ﬁgure 8(b). For Exp 1,
our model ﬁts almost perfectly the specularity contours with an
average error of 1%. For Exp 2, we observe that our canonical
model can adapt to curvature changes of the surface throughout
the sequence, while keeping an optimal precision of the ellipse
estimation, the predicted specularity, with an average error of 2%.
The previous dual model has an increasing error following the
curvature change, reaching up to 12% with an average error of
6%.

(a)

(b)

Figure 8: Synthetic experiment to evaluate the accuracy of spec-
ularity prediction when curvature changes. (a) illustrates how we
vary the curvature from a plane to a cylinder. In this test, our
ellipsoid is reconstructed from six images from a planar surface.
Exp 1 reconstructs the ellipsoid (the canonical dual JOLIMAS
model) for each curvature changes while Exp 2 reconstructs it only
once from the ﬁrst 6 images. The prediction is conducted from
this reconstruction for every future viewpoint where the plane is
morphed into a cylinder. We observe that canonical JOLIMAS
through Exp 1 (in blue) and Exp 2 (in green) has a low error
in terms of specularity prediction as opposed to dual JOLIMAS
which has an increasing error following the curvature change in
Exp 2.

Ellipsoid sequence

In order to check the validity of the canonical JOLIMAS model on
a simple example, we created a synthetic sequence of 80 images
containing an ellipsoid object which is a shape with non-uniform
curvature. Using the ﬁrst 6 frames, we reconstruct our canonical
dual JOLIMAS model and the dual JOLIMAS model [42]. Then
we use these reconstructions to perform specularity prediction on
the remaining frames of the sequence. These results are illustrated
qualitatively in ﬁgure 9. We can see that our specularity prediction
of canonical dual JOLIMAS outperforms dual JOLIMAS in terms
of position and shape prediction of the ellipse. In this sequence,
our method achieves an average error for specularity prediction of
2.4% while [42]’s error increases rapidly with curvature change,
with an average error of 20.2%.

5.2 Real Data

We present specularity prediction on two real sequences contain-
ing a metal rocket replica and a red book with a shiny cover. The
reconstruction of canonical dual JOLIMAS and dual JOLIMAS is
done from the same six images for each sequence. The specularity
prediction is done for the remaining images of the sequences
following the ones used for reconstruction.

515Image #Exp 2: Dual JOLIMAS with varying curvatureExp 2:     with varying curvatureExp 1: Canonical   dual JOLIMASGeometric ellipse/contours distance (%)Canonical dual JOLIMASIEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS 2021

8

(a)

(b)

(c)

Figure 9: Specularity prediction results for our ellipsoid synthetic
sequence. In (a), we show that the ellipsoid object is not uniform
in curvature. In (b) and (c), we show an image pair of the sequence
and the specularity prediction results (blue ellipses) of canonical
dual JOLIMAS (b) and canonical JOLIMAS (c). The image on the
left corresponds to the curvature used to reconstruct the model and
the one on the right illustrates prediction for stronger curvature.
We see that our canonical JOLIMAS gives better results than dual
JOLIMAS for both curvatures, as dual JOLIMAS performs almost
well on the ﬁrst image but badly on the varying curvature on the
right image.

Rocket replica sequence

We introduced this 1410 frames sequence in [42] and although
the results were already acceptable with an error on average
of 2.1%, the central part of the rocket replica does not have
a constant curvature, which negatively affected the specularity
prediction accuracy of dual JOLIMAS. We observe in ﬁgure 10
that canonical dual JOLIMAS outperforms dual JOLIMAS in
terms of specularity prediction with an average error of 1.3%.

Kinect sequence

To show the potential of our method in a more complex context,
we took a sequence with 146 real images from a Kinect v21,
where depth information is computed from a Time of Flight (ToF)
system. Kinect v2 is mainly supported on Windows 8 and we
had to use the Libfreenect2 library from the OpenKinect2 project.
This library allows one to retrieve infrared information as a 512 ×
424 image and colours as a 1920 × 1080 image from the sensor.
This library also provides tools to synchronise data to obtain a
512 × 424 depth image. In order to reconstruct our canonical dual
JOLIMAS, it is necessary to retrieve surface normal information
where the specularities appear. In the case of Kinect v2, a normal
map needs to be deduced from the point cloud given by the depth
image. However, Kinect v2 does not provide depth for objects
reﬂecting infrared rays such as mirrors and specular surfaces. In
addition, it has holes in occluded areas in the infrared image due to
the baseline between the RGB and infrared cameras. These areas
correspond in general to occluding contours of objects. To address

1. https://developer.microsoft.com/en-us/windows/kinect
2. https://github.com/OpenKinect/libfreenect2

(a)

(b)

(c)

Figure 10: Specularity predictions on the real sequence presented
in [42]. (a) shows the object of interest: a real rocket replica. We
present a pair of images for each model showing the prediction
results (blue ellipses) of dual JOLIMAS (b) and canonical dual
JOLIMAS (c). Our new approach models the position and shape
of the specularities in a more accurate way compared to dual
JOLIMAS, with an average error of 1.3% compared to 2.1%
with dual JOLIMAS, by comparing the 2D distance between
specularity prediction and the contours of the detected specularity.

(a)

(b)

(c)

Figure 11: Colour image from Kinect v2 (a), raw depth image (b)
and corrected by inpainting (c).

these issues, we ﬁrst applied an inpainting algorithm [52] on the
depth image in order to ﬁll the potential holes. For larger areas,
we could also use methods such as [7], [10]. In our sequence,
these holes are generally numerous but small, which allows us to
interpolate neighbouring information in the image. We illustrate
this process in ﬁgure 11.

IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS 2021

9

(a)

(b)

(a)

(b)

(c)

(d)

Figure 12: Computation of the normal map from a point cloud and
the depth map (a) provided by Kinect v2. First we use the PCL
library to compute the normals from neighbour 3D points (b). In
(c), we show a naive normal computation from the depth image
(2D) directly instead of using the point cloud (3D). We can observe
considerable noise, visible on the planar surfaces (wall and book)
unsuitable for optimal specularity prediction. The normal map (d)
obtained is sufﬁcient for our application needs.

Once the depth image is inpainted, we use the Point Cloud
Library (PCL)3 to compute the normal map. This map is obtained
using 3D points within a radius around each point depending on
the point cloud density. The optimal search radius used here in our
experiments is 3 cm. We illustrate this step in ﬁgure 12.

We present in ﬁgure 13 our specularity prediction results for
the Kinect sequence for both canonical dual and dual JOLIMAS.
As expected, dual JOLIMAS is not able to handle the curvature
change of the book as opposed to canonical dual JOLIMAS.

6 LIMITATIONS
Our canonical dual JOLIMAS model proposes a new solution
to the specularity prediction problem for non-uniformly curved
objects (ellipsoid, plastic rocket replica) as well as objects with dy-
namically changing curvature (book bent in real-time throughout
the sequence). Our experiments showed the critical relationship
between 3D reconstruction quality (from the CAD model and
Kinect v2) and stability/accuracy of the prediction. In the case of
Kinect, it is quite difﬁcult to obtain a prediction without jittering
caused by the normal map due to the raw noise of the sensor depth
data. Moreover, due to our method being sensitive to the quality of
camera pose and normal estimation of the surface, the prediction
ability of the specularity shape can vary according to the size of
the detected specularity in the image and the image size in general.

7 FUTURE WORK
In terms of applications, improvements in the specularity predic-
tion capability of canonical dual JOLIMAS will naturally improve

3. http://pointclouds.org

Figure 13: Images used for model reconstruction (a) and results
of specularity prediction on the Kinect sequence for three images
showing different curvatures (for each row) (b). Images on the
ﬁrst column are the source images, the middles ones correspond
to dual JOLIMAS specularity prediction and the ones on the right
are from canonical dual JOLIMAS.

the already presented applications such as retexturing [42]. It will
also provide a better light source position for local illumination
methods to improve the rendering quality of AR applications,
since canonical dual JOLIMAS relies on the Torrance-Sparrow lo-
cal illumination model [54]. After reconstructing the specular term
from the JOLIMAS model, an interesting step would be to jointly
reconstruct the diffuse part through a larger optimisation problem.
The additional information would beneﬁt camera pose estimation
and material estimation methods. It could also be interesting to
study the impact of the material (roughness and reﬂectance) on the
specularity shape, potentially leading to genericity improvement of
the model. Using RGB-D data, it would be interesting to design an
algorithm to ensure temporal and spatial coherence, allowing one
to offer an optimal stability even in difﬁcult cases. A curvature
interpolation of the surface and of the specularity shape change
between two images could be interesting to test. However, RGB-
D data are not mandatory, since one can use constrained SLAM
such as [51], providing the camera pose and the geometry of the
object of interest, as shown with the rocket replica in section
5.2. Another interesting application would be to use JOLIMAS
specularity prediction ability as a labeling helper tool for deep
learning processes, such as specularity detection and light source
modelling.

8 CONCLUSION
We have presented a canonical representation of the previous
dual JOLIMAS model [42], where we address the main limi-
tation of the previous approach: the hypothesis of a ﬁxed and
constant surface curvature. Our method generalises the JOLIMAS
model to any surface geometry while improving the quality of
specularity prediction without compromising performance. We
highlighted the link between surface curvature and specularity
shape provided by [18]. By using the physics-based Torrance-
Sparrow local illumination for modelling the specularity shape as
the light interaction with the material, we computed limit angles
that are angle values at the contours of the specularity constant

IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS 2021

10

for any curvature. This allowed us to deform the specularity to
ﬁt the JOLIMAS reconstruction [42] and adapt the shape of the
predicted specularity for new viewpoints. Specularity prediction
using our new model was tested against [42] on both synthetic
and real sequences with objects of varying shape curvatures given
by the CAD model or using a Kinect v2 depth camera. Our method
outperforms previous approaches for the specularity prediction
task in real-time.

REFERENCES

[1] Kara-Ali Aliev, Artem Sevastopolsky, Maria Kolos, Dmitry Ulyanov, and
Victor Lempitsky. Neural point-based graphics. In European Conference
on Computer Vision, ECCV, 2019.

[2] Alessandro Artusi, Francesco Banterle, and Dmitry Chetverikov. A
survey of specularity removal methods. Computer Graphics Forum,
30(8):2208–2230, 2011.

[3] Andrew Blake. Does the brain know the physics of specular reﬂection?

Nature, 343(6254):165–168, 1990.

[4] Andrew Blake and Gavin Brelstaff. Geometry from specularities.

In

[5]

International Conference on Computer Vision, ICCV, 1988.
James F Blinn. Models of light reﬂection for computer synthesized
pictures. In Special Interest Group on Computer Graphics and Interactive
Techniques, SIGGRAPH, 1977.

[6] Bastiaan J Boom, Sergio Orts-Escolano, Xin X Ning, Steven McDonagh,
Peter Sandilands, and Robert B. Fisher. Point light source estimation
based on scenes recorded by a rgb-d camera. In British Machine Vision
Conference, BMVC, 2013.

[7] Frédéric Bousefsaf, Mohamed Tamaazousti, Souheil Hadj Said, and
Image

Image completion using multispectral imaging.

Rémi Michel.
Processing, 2018.

[8] Paul-Emile Buteau and Hideo Saito. Retrieving lights positions using
plane segmentation with diffuse illumination reinforced with specular
In International Symposium on Mixed and Augmented
component.
Reality, ISMAR, 2015.

[9] Carlos Campos, Richard Elvira, Juan J Gómez Rodríguez, José MM
Montiel, and Juan D Tardós. Orb-slam3: An accurate open-source
library for visual, visual-inertial and multi-map slam. arXiv preprint
arXiv:2007.11898, 2020.

[10] Guillaume Chican and Mohamed Tamaazousti. Constrained patchmatch
for image completion. In International Symposium on Visual Computing,
ISVC, 2014.

[11] Robert L Cook and Kenneth E. Torrance. A reﬂectance model for

computer graphics. Transactions on Graphics, 1(1):7–24, 1982.

[12] Andrey DelPozo and Silvio Savarese. Detecting specular surfaces on
In Computer Vision and Pattern Recognition, CVPR,

natural images.
2007.

[13] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei.
Imagenet: A large-scale hierarchical image database. In Computer Vision
and Pattern Recognition, CVPR, 2009.

[14] Farshad Einabadi and Oliver Grau. Discrete light source estimation
from light probes for photorealistic rendering. In British Machine Vision
Conference, BMVC, 2015.

[15] Jakob Engel, Vladlen Koltun, and Daniel Cremers. Direct sparse
odometry. Pattern Analysis and Machine Intelligence, 40(3):611–625,
2017.

[16] Souheil Hadj-Said, Mohamed Tamaazousti, and Adrien Bartoli. Can we
invert a local reﬂectance model from a single specular highlight with
known scene geometry and camera pose? In International Conference of
the European Association for Computer Graphics (EUROGRAPHICS).
The Eurographics Association, 2019.

[17] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep
residual learning for image recognition. In Computer Vision and Pattern
Recognition, CVPR, pages 770–778, 2016.

[18] Gleen Healey and Thomas O Binford. Local shape from specularity.

volume 42 of CVGIP, pages 62–86, 1988.

[19] David S Immel, Michael F Cohen, and Donald P Greenberg. A radiosity
In Special Interest Group on

method for non-diffuse environments.
Computer Graphics and Interactive Techniques, SIGGRAPH, 1986.
[20] Jan Jachnik, Richard A. Newcombe, and Andrew J. Davison. Real-
time surface light-ﬁeld capture for augmentation of planar specular. In
International Symposium on Mixed and Augmented Reality, ISMAR,
2012.

[21] James T Kajiya. The rendering equation. In Special Interest Group on
Computer Graphics and Interactive Techniques, SIGGRAPH, 1986.

[22] Masayuki Kanbara and Naokazu Yokoya. Real-time estimation of light
source environment for photorealistic augmented reality. In International
Conference on Pattern Recognition, ICPR, 2004.

[23] Kevin Karsch, Varsha Hedau, David Forsyth, and Derek Hoiem. Ren-
dering synthetic objects into legacy photographs. ACM Transactions on
Graphics (TOG), 30(6):157, 2011.

[24] Hyeongwoo Kim, Hailin Jin, Sunil Hadap, and Inso Kweon. Specular

reﬂection separation using dark channel prior. CVPR, 2013.

[25] Seunghyun Kim, Moonsoo Ra, and Whoi-Yul Kim. Specular detection on
glossy surface using geometric characteristics of specularity in top-view
images. Sensors, 21(6):2079, 2021.

[26] Gudrun J Klinker, Steven A Shafer, and Takeo Kanade. A physical ap-
proach to color image understanding. International Journal of Computer
Vision, 4(1):7–38, 1990.

[27] Sujit Kuthirummal and Shree K Nayar. Flexible mirror imaging.

In

International Conference on Computer Vision, ICCV, 2007.

[28] Pascal Lagger and Pascal Fua. Using specularities to recover multiple
light sources in the presence of texture. In International Conference on
Pattern Recognition, ICPR, 2006.

[29] Chloe LeGendre, Wan-Chun Ma, Graham Fyffe, John Flynn, Laurent
Charbonnel, Jay Busch, and Paul Debevec. Deeplight: Learning illumi-
nation for unconstrained mobile mixed reality. In Computer Vision and
Pattern Recognition, CVPR, 2019.

[30] John Lin, Mohamed El Amine Seddik, Mohamed Tamaazousti, Youssef
Tamaazousti, and Adrien Bartoli. Deep multi-class adversarial specu-
larity removal. In Scandinavian Conference on Image Analysis, SCIA,
2019.

[31] John Lin, Mohamed Tamaazousti, Souheil Hadj Said, and Alexandre
Morgand. Color consistency of specular highlights in consumer cameras.
In Virtual Reality Software and Technology, VRST, 2017.

[32] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro
Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick. Microsoft
coco: Common objects in context. In European Conference on Computer
Vision, ECCV, 2014.

[33] Clifford Lindsay and Emmanuel Agu. Automatic multi-light white
balance using illumination gradients and color space projection.
In
International Symposium on Visual Computing, ISVC, 2014.

[34] Stephen Lombardi and Ko Nishino. Reﬂectance and natural illumination
In European Conference on Computer Vision,

from a single image.
ECCV, 2012.

[35] David Mandl, Kwang Moo Yi, Peter Mohr, Peter Roth, Pascal Fua,
Vincent Lepetit, Dieter Schmalstieg, and Denis Kalkofen. Learning
lightprobes for mixed reality illumination. In International Symposium
of Mixed and Augmented Reality, ISMAR, 2017.

[36] Maxime Meilland, Christian Barat, and Andrew Comport.

3d high
dynamic range dense visual slam and its application to real-time object
In International Symposium on Mixed and Augmented
re-lighting.
Reality, ISMAR, 2013.

[37] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron,
Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural
radiance ﬁelds for view synthesis. In European conference on computer
vision, pages 405–421. Springer, 2020.

[38] Hiroshi Mitsumoto, Shinichi Tamura, Kozo Okazaki, Naoki Kajimi,
and Yutaka Fukui. 3-d reconstruction using mirror images based on
a plane symmetry recovering method. Pattern Analysis and Machine
Intelligence, 14(9):941–946, 1992.

[39] Alexandre Morgand and Mohamed Tamaazousti. Generic and real-
In 8th International
time detection of specular reﬂections in images.
Joint Conference on Computer Vision, Imaging and Computer Graphics
Theory and Applications, VISAPP, 2014.

[40] Alexandre Morgand, Mohamed Tamaazousti, and Adrien Bartoli. An
empirical model for specularity prediction with application to dynamic
In International Symposium of Mixed and Augmented
retexturing.
Reality, ISMAR, 2016.

[41] Alexandre Morgand, Mohamed Tamaazousti, and Adrien Bartoli. A
geometric model for specularity prediction on planar surfaces with
multiple light sources. Transactions on Visualization and Computer
Graphics, 24(5):1691–1704, 2017.

[42] Alexandre Morgand, Mohamed Tamaazousti, and Adrien Bartoli. A
multiple-view geometric model of specularities on non-planar shapes
with application to dynamic retexturing. Transactions on Visualization
and Computer Graphics, 23(11):2485–2493, 2017.

[43] Alexandre Morgand, Mohamed Tamaazousti, and Adrien Bartoli. A
multiple-view geometric model of specularities on non-uniformly curved
surfaces. In Virtual Reality Software and Technology, VRST, 2017.
[44] Richard A Newcombe, Andrew J Davison, Shahram Izadi, Pushmeet
Kohli, Otmar Hilliges, Jamie Shotton, David Molyneaux, Steve Hodges,

IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS 2021

11

David Kim, and Andrew Fitzgibbon. Kinectfusion: Real-time dense
In ACM symposium on User interface
surface mapping and tracking.
software and technology,, ISMAR, 2011.

[45] Jeong Joon Park, Aleksander Holynski, and Steven M Seitz. Seeing the
world in a bag of chips. In Computer Vision and Pattern Recognition,
CVPR, 2020.

[46] Jeong Joon Park, Richard Newcombe, and Steve Seitz. Surface light
ﬁeld fusion. In International Conference on 3D Vision, 3DV, pages 12–
21, 2018.

[47] Souheil Hadj Said, Mohamed Tamaazousti, and Adrien Bartoli. Image-
based models for specularity propagation in diminished reality.
In
Transactions on Visualization and Computer Graphics, TVCG, 2017.
[48] Silvio Savarese, Min Chen, and Pietro Perona. Local shape from mirror
International Journal of Computer Vision, 64(1):31–67,

reﬂections.
2005.

[49] T. Schneider, M. T. Dymczyk, M. Fehr, K. Egger, S. Lynen, I. Gilitschen-
ski, and R. Siegwart. maplab: An open framework for research in visual-
inertial mapping and localization. Robotics and Automation Letters,
2018.

[50] Karen Simonyan and Andrew Zisserman. Very deep convolutional
networks for large-scale image recognition. In International Conference
on Learning Representations, ICLR, 2015.

[51] Mohamed Tamaazousti, Sylvie Naudet-Collette, Vincent Gay-Bellile,
Steve Bourgeois, Bassem Besbes, and Michel Dhome. The constrained
slam framework for non-instrumented augmented reality. Multimedia
Tools and Applications, 75(16):9511–9547, 2016.

[52] Alexandru Telea. An image inpainting technique based on the fast

marching method. Journal of graphics tools, 9(1):23–34, 2004.

[53] Richter-Trummer Thomas, Kalkofen Denis, Park Jinwoo, and Schmal-
stieg Dieter.
In
International Symposium of Mixed and Augmented Reality, ISMAR,
2016.

Instant mixed reality lighting from casual scanning.

[54] Kenneth E Torrance and Ephraim M Sparrow. Theory for off-specular
reﬂection from roughened surfaces. Journal of the Optical Society of
America, 57(9):1105–1114, 1967.

[55] Thomas Whelan, Stefan Leutenegger, Renato F Salas-Moreno, Ben
Glocker, and Andrew J Davison. Elasticfusion: Dense slam without a
pose graph. In Robotics: science and systems, volume 11, 2015.

[56] Kwan-Yee K Wong, Dirk Schnieders, and Shuda Li. Recovering light di-
rections and camera poses from a single sphere. In European Conference
on Computer Vision, ECCV. 2008.

