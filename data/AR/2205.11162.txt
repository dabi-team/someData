A Self-Paced Mixed Distillation Method for Non-Autoregressive
Generation

Weizhen Qi1 ∗, Yeyun Gong2 †, Yelong Shen3, Jian Jiao3, Yu Yan3,
Houqiang Li1 , Ruofei Zhang3 , Weizhu Chen3, Nan Duan2
1University of Science and Technology of China, 2Microsoft Research Asia, 3Microsoft
1weizhen@mail.ustc.edu.com, lihq@ustc.edu.com,
2{yegong,nanduan}@microsoft.com,
3{yeshe,jian.jiao,yyua,bzhang,wzchen}@microsoft.com

Abstract

Non-Autoregressive generation is a sequence
generation paradigm, which removes the de-
pendency between target tokens.
It could
efﬁciently reduce the text generation latency
with parallel decoding in place of token-
by-token sequential decoding.
However,
due to the known multi-modality problem,
Non-Autoregressive (NAR) models signiﬁ-
cantly under-perform Auto-regressive (AR)
models on various language generation tasks.
Among the NAR models, BANG is the
ﬁrst
large-scale pre-training model on En-
It con-
glish un-labeled raw text corpus.
siders different generation paradigms as its
pre-training tasks including Auto-regressive
(AR), Non-Autoregressive (NAR), and semi-
Non-Autoregressive (semi-NAR) information
ﬂow with multi-stream strategy.
It achieves
state-of-the-art performance without any dis-
tillation techniques. However, AR distillation
has been shown to be a very effective solution
for improving NAR performance. In this pa-
per, we propose a novel self-paced mixed dis-
tillation method to further improve the genera-
tion quality of BANG. Firstly, we propose the
mixed distillation strategy based on the AR
stream knowledge. Secondly, we encourage
the model to focus on the samples with the
same modality by self-paced learning. The
proposed self-paced mixed distillation algo-
rithm improves the generation quality and has
no inﬂuence on the inference latency. We carry
out extensive experiments on summarization
and question generation tasks to validate the
effectiveness. To further illustrate the commer-
cial value of our approach, we conduct exper-
iments on three generation tasks in real-world
advertisements applications. Experimental re-
sults on commercial data show the effective-
ness of the proposed model. Compared with
BANG, it achieves signiﬁcant BLEU score im-
provement. On the other hand, compared with

∗ Work is done during internship at Microsoft Research

Asia.

† Corresponding Author.

auto-regressive generation method, it achieves
more than 7x speedup. We will make our code
publicly available.

1

Introduction

Non-AutoRegressive (NAR) models have been
studied recently for efﬁcient sequence genera-
tion (Qi et al., 2021; Gu et al., 2017). Different
from classical Autoregressive (AR) approaches
which sequentially decode output tokens (Lewis
et al., 2019; Song et al., 2019; Brown et al., 2020b;
Zou et al., 2021; He et al., 2021), NAR approaches
generate the sequence of tokens in parallel i.e.
BANG (Qi et al., 2021), NAT (Gu et al., 2017)
etc, to largely reduce the inference latency, which
have been successfully applied in query generation,
text summarization tasks (Rajpurkar et al., 2016;
Narayan et al., 2018; Rush et al., 2015).

Despite reducing the inference time dramati-
cally, typical NAR models still signiﬁcantly under-
perform AR models (Qi et al., 2021). Previous
works analyze the issue of performance degrada-
tion by NAR and attribute it to the multi-modality
problem (Kim and Rush, 2016). The multi-
modality problem in NAR is described as gen-
erating target tokens from different possible an-
swers and composing a chaotic confusing target
sequence.
It is not observed in AR models be-
cause they would pick only one possible answer
with step-by-step generation, with all previous gen-
erated tokens as known information. To allevi-
ate the multi-modality problem, sequence distil-
lation (Kim and Rush, 2016; Gu et al., 2017) is
widely used to replace the original training tar-
gets with the generated sequences by a well-trained
AR model. Sequence distillation is analyzed to
prove its ability to improve NAR performance by
reducing the modality (Zhou et al., 2019) and re-
ducing the dependency between target sequence
tokens (Ren et al., 2020). Besides sequence distil-
lation, various techniques are proposed to improve

2
2
0
2

y
a
M
3
2

]
L
C
.
s
c
[

1
v
2
6
1
1
1
.
5
0
2
2
:
v
i
X
r
a

 
 
 
 
 
 
the NAR generation including copy mechanism for
translation (Gu et al., 2017), curriculum learning
(Guo et al., 2020), glancing sampling (Qian et al.,
2020), pre-training (Qi et al., 2021) etc.

In this paper, we propose a novel self-paced
mixed distillation method. Firstly, we propose
to instruct the NAR model to select one modal-
ity to converge and focus on the samples with the
same modality. At the beginning, NAR model will
study all samples equally, then gradually select the
easy samples with self-paced learning. We propose
to use perplexity (PPL) to measure the modality-
matching quality, and give rewards to the samples
that agree with the converged modality. Secondly,
we propose to generate soft labels from the BANG
AR stream for teaching NAR stream. With the soft
labels including rare words knowledge from origi-
nal golden data rather than directly adding original
data into training, it is less possible to hurt the NAR
performance with increased modality problem. On
the contrary, if we say the learned AR model regu-
lates the data distribution to generalize a simpliﬁed
ﬁtting function, instead of the hard outputs from
AR models which are approximately sampled from
beam search, directly predicted words distribution
better describe the AR learned generation function.
The AR teacher model is trained on original golden
data but teaches the student NAR model soft la-
bels with distilled data as contexts. Experimental
results show that the proposed mixed distillation
and self-paced learning signiﬁcantly improve NAR
performance.

The contributions of this paper can be summa-

rized as:

1. We propose a self-paced mixed distillation
method to teach BANG NAR generation with
soft labels knowledge from its AR knowledge
with self-paced learning.

2. We carry out extensive experiments on sum-
marization, question generation with obvious
improvements. It is easy to deploy with sig-
niﬁcant performance improvements and no
inﬂuence on inference latency.

3. We applied the proposed method to commer-
cial tasks.
It achieves signiﬁcantly perfor-
mance improvement compared with BANG
NAR. Compared with AR models, the pro-
posed method meets the online requirement
and also achieves comparable performance.

2 Preliminary

2.1 Non-AutoRegressive Generation

Consider the sequence to sequence generation sce-
nario, we denote the input and output sequence as
(x, y). For a typical neural sequence generation
model, i.e., (Lewis et al., 2019; Song et al., 2019;
Qi et al., 2020), it encodes the input sequence x
into dense representation h in Eqn. 1, and decodes
a sequence of tokens as output y : {yt}T

t=1.

h = Encoder(x)

(1)

In the classical Auto-Regressive generation (AR)
paradigm (Brown et al., 2020b), each token yi in
the output sequence y is predicted with the depen-
dency of h and previous tokens y<t, as in Eqn. 2.

yt = DecoderAR(y<t, h)

(2)

Non-AutoRegressive generation (NAR) models
(Gu et al., 2017; Qi et al., 2021) predict each token
yt of y simultaneously, given h and position t in
Eqn. 3.

yt = DecoderNAR(t, h)

(3)

NAR could greatly reduce the inference complexity
compared with AR by discarding the dependency
between sequence of output tokens. However, it
degrades the performance of AR by introducing the
multi-modality issue (Zhou et al., 2019).

2.2 BANG: Bridging Autoregressive and
Non-autoregressive Generation

BANG (Qi et al., 2021) is a large-scale pre-
trained language model with transformer based
encoder-decoder architecture. It adopts n-stream
self-attention mechanism for integrating AR, NAR
and Semi-NAR generation paradigms into a uni-
ﬁed model. In Figure 1, it illustrates a three-stream
BANG model. The 1st stream in BANG can be uti-
lized for AR generation, and 2nd and 3rd streams
are used for NAR/Semi-NAR generation.

The conditional probabilities of generating tar-
get sequence y given x are shown in Eqn. 4,
where p1(y|x) and pn(y|x) indicate the condi-
tional probabilities computed by the 1st and nth

In this section, we ﬁrst introduce the self-paced
learning and mixed distillation, respectively. Then,
we introduce mixed distillation used in BANG pre-
training.

3.1 Mixed Sequence Distillation

Figure 1: Three-Stream BANG model. In this example,
M is short for [MASK] token. In ith predicting stream,
i − 1 previous tokens are masked out for AR/NAR gen-
eration.

Distillation approaches adopt the “teacher-student”
learning paradigm, where the AR model in BANG
serves as the “teacher” model and the NAR model
is viewed as “student” models.

streams in BANG, respectively.

p1(y|x) =

p2(y|x) =

· · ·

pn(y|x) =

T
(cid:89)

t=1
T
(cid:89)

t=1

T
(cid:89)

t=1

p1(yt|y<t, x)

p2(yt|y<t−1, x)

(4)

pn(yt|y<t−n+1, x)

The pre-training objective for BANG minimizes
the negative log-likelihood of target sequences for
all the n prediction streams, as in Eqn. 5.

LBANG(x, y) = −

n
(cid:88)

s=1

logps(y|x)

(5)

To compute the n prediction streams efﬁciently,
BANG adopts the Cross-stream Visible N-stream
self-attention mechanism to obtain all the n-stream
predictions with one forward pass. Therefore, in
an extreme case when n ≥ T , BANG could de-
code all the output tokens in parallel with the NAR
paradigm.

pNAR(y|x) =

T
(cid:89)

t=1

pt(yt|x)

(6)

In the work, we leverage BANG model architecture
as the test-bed to study AR and NAR mechanisms
for language generation. For the sake of simplicity,
we denote the AR generation model in BANG as
pAR(y|x) which is also named p1(y|x) in Eqn. 4.

3 Method

The vanilla BANG model optimizes the n-stream
predictions independently during training, which
would cause severe multi-modality issue for NAR
generation (Zhou et al., 2019; Kim and Rush, 2016).

Both teacher and student models make predic-
tion over sequence of tokens, the general distilla-
tion function for sequence generation models pAR
and pNAR is given by Eqn. 7:
(cid:0)pAR, pNAR, x(cid:1) = DKL

LDistill

(cid:0)pAR(·|x) (cid:107) pNAR(·|x)(cid:1)

(7)
where DKL(·) is Kullback-Leibler divergence, and
pAR(·|x) and pNAR(·|x) deﬁne the probability dis-
tribution over all possible output sequences by
teacher and student model respectively.

Since it is intractable to compute the Eqn. 7 di-
rectly, we study three alternative ways to approx-
imate the general distillation loss function, to be
elaborated as follows.

Sequence distillation: Sequence distillation ap-
proximates the probability distribution over se-
quences by teacher model with the one-hot dis-
tribution, which is:

∀y∈Y pAR(y|x) ≈

(cid:40)

if y = argmaxˆy pAR(ˆy|x)

1,
0, otherwise

(8)
where Y denotes the set of all possible output se-
quences. In practice, we use beam search decoding
algorithm to obtain sequence ybs to approximate
the sequence with the maximum probability by AR
model:

ybs ≈ argmax

pAR(ˆy|x)

ˆy

(9)

By integrating Eqn. 8 and 9 into Eqn 7, the

distillation loss could be approximated by:

LDistill ≈ LSeq-Distill

(cid:0)pAR, pNAR, x(cid:1) = − log pNAR(ybs|x)

(10)
According to the formulation 10, the distillation
training process can be simply explained as: the
student model pNAR is trained with the sequence-
to-sequence dataset generated by the teacher model
pAR.

Despite the simplicity of the sequence distilla-
tion approach, it omits the token-wised probability

distribution of the teacher model. Thus, another
token-wised teacher forcing distillation approach
is introduced here.

Teacher-Forcing Distillation: We ﬁrst factor-
ize the joint sequence probability pNAR and pAR in
Eqn. 7.

LDistill

(cid:88)

(cid:0)pAR, pNAR, x(cid:1) =
|y|
(cid:88)

pAR(y1:t−1|x)

y∈Y

t=1

(11)

DKL

(cid:0)pAR(·|y<t, x) (cid:107) pNAR(·|t, x)(cid:1)

where pAR(y1:t−1|x) gives the sequence probabil-
ity of y1:t−1 by the teacher AR model.
In the
teacher-forcing distillation approach, it approxi-
mates the distribution pAR(y1:t−1|x) with the one-
hot distribution given by the ground-truth sequence
y∗:

∀y∈Y ∀t≤|y| pAR(y1:t−1|x) ≈

(cid:40)

if y1:t−1 = y∗

1,
0, otherwise

1:t−1

(12)

Therefore, by combining the Eqn. 12 with
Eqn. 11, the teacher-forcing distillation loss could
be given as follows:

LDistill ≈ LTF-Distill

(cid:0)pAR, pNAR, x(cid:1) =
<t, x) (cid:107) pNAR(·|t, x)(cid:1) (13)

(cid:0)pAR(·|y∗

|y∗|
(cid:88)

t=1

DKL

Mixed Sequence Distillation: To leverage the
advantage of both sequence-wise and token-wise
distillation approaches, mixed sequence distillation
instead uses ybs for pAR(y1:t−1|x) approximation
with similar manner as in Eqn. 12.

∀y∈Y ∀t≤|y| pAR(y1:t−1|x) ≈

(cid:40)

if y1:t−1 = ybs

1,
0, otherwise

1:t−1

(14)

Thus, the objective function by mixed sequence
distillation is given as:

LDistill ≈ LMixed-Distill

(cid:0)pAR, pNAR, x(cid:1) =

DKL

(cid:0)pAR(·|ybs

<t, x) (cid:107) pNAR(·|t, x)(cid:1)

(15)

|ybs|
(cid:88)

t=1

In the model training, we combine the distillation
loss with original objective function in BANG, thus
the overall training objective is deﬁned by:

LOverall(x, y) = LBANG(x, y)+
(cid:0)pAR, pNAR, x)(cid:1)

γLDistill

(16)

3.2 Self-Paced Learning

Denote the training corpus for sequence distillation
learning to be {x1, ..., xC}. Classical training al-
gorithms sample instances from the corpus accord-
ing to the static uniform distribution. Curriculum
learning adopts dynamic data sampling strategy
during training (Zhu et al., 2021; Guo et al., 2020;
Qian et al., 2020). For example, it imitates taking
well-designed easy-to-hard training courses, where
“easy” instances are more likely to be sampled at
early training stage, and “hard” instances are with
higher sampling probabilities at late training stage.
In the section, we introduce a self-paced curricu-
lum learning strategy for sequence distillation. In-
stead of human-crafted training courses, self-paced
learning utilizes posterior probability of the student
model to calculate the weight of each instance dur-
ing training. Generally, it assigns an extra weight
to each training instance: {(λ1, x1), ..., (λC, xC)};
λi is the sampling weight of the i-th instance;
which could reﬂect the “easy/hard” degree of the
training case.

Let lossi denote the distillation loss of the i-th

instance:

lossi = LDistill

(cid:0)pAR, pNAR, xi(cid:1)

(17)

lossi measures the discrepancy between teacher
and student models for the i-th sample, and let
λi = exp(−lossi). Intuitively, large value of λi
indicates the instance is easy for distillation learn-
ing, thus it is assigned with a larger weight. In the
practice of the self-paced learning, we adopt the
batch-wise weight normalization to stabilize the
training procedure. Thus, batch-wised self-paced
distillation loss is computed by :

LSP-Distill

(cid:0)pAR, pNAR, {(λi, xi}B
i=1

(cid:1) =

B
(cid:88)

i=1

exp λi
o=1 exp λo

(cid:80)B

LDistill

(cid:0)pAR, pNAR, xi(cid:1)

(18)

In Eqn. 10, 13 and 15, it gives objective func-
tions of sequence distillation, teacher-forcing distil-
lation and mixed sequence distillation respectively.

3.3 Large Scale Pre-training

In previous section § 3.1, we introduced differ-
ent distillation methods to teach the NAR training

with AR knowledge. BANG has a list of predict-
ing streams that can predict tokens in AR, semi-
NAR or NAR information ﬂow for pre-training.
We propose to use LT F −Distill as a self-distillation
method for further pre-training in larger corpus
with nearly no extra cost. The same workﬂow is
used for training self-distillation BANG as previ-
ous work, except that the training targets for NAR
streams are replaced with the predicted distribu-
tions from AR stream. The algorithm is described
in Alg 1.

Algorithm 1 Large Scale Pre-training with Self-
Distillation.

Require: Corpus C; Distillation weight α; Ini-
tialize the model with BANG.
for article A in get_articles(C) do

noised_article, spans = mask_spans(A)
x, y ← make_batch(noised_article, spans)

ˆy = BAN G(x, θ)
ˆy1, ˆy2, ..., ˆyn = split_streams(ˆy)
ysof t = αy + (1 − α)ˆy1.detach()
loss = mean(N LL(y, ˆy1),

KL(ysof t, ˆy2), ..., KL(ysof t, ˆyn))

θ ← loss.backward()

end for
return θ

In Algorithm 1, we can see the procedure to pre-
pare training samples is the same as BANG. Given
an article, a span of continues tokens is masked out
to predict in the decoder, while the noised article is
fed into the encoder as inputs. ˆy is predicted from
BANG multiple stream decoders. For ˆyi in i-th
stream, tokens are predicted with i − 1 previous
tokens replaced with [MASK]. In another word,
tokens in ﬁrst stream ˆy1 are predicted AR infor-
mation ﬂow. Each predicting stream will predict
a distribution with different context to predict the
same sequence. The distribution of AR stream will
be used to calculate NLL loss with the golden hard
targets. The predicted distribution of other predict-
ing streams will be used to calculate KL divergence
loss with the AR stream predictions.

4 Experiments

4.1 Benchmarks

4.1.1 Public Datasets

We evaluate the proposed method on three publicly
available benchmarks: SQuAD 1.1, XSum, and Gi-

gaword for question generation and summarization
tasks.

SQuAD 1.1 (Rajpurkar et al., 2016) is a question
generation dataset, with 98K training samples. The
data is formatted as (cid:104)passage, answer, question(cid:105).
Each passage can be combined with various an-
swers to raise different questions. We follow pre-
vious work (Qi et al., 2020, 2021) to feed (cid:104)answer
[SEP] passage(cid:105) into transformer encoder as the in-
put, with an average length 149.4. The average
output length is 11.5.

XSum (Narayan et al., 2018) is a summarization
dataset, with 204K training samples, 11K valida-
tion samples, and 11K test samples. Each sam-
ple includes an British Broadcasting Corporation
(BBC) article and a professionally written single
sentence summary. The average output length is
21.1.

Gigaword (Rush et al., 2015) is a summariza-
tion dataset, containing 3.8M training pairs, 189k
validation pairs, and 1951 test pairs of (cid:104)passage,
summary(cid:105) examples. They are extracted and
cleaned from the Gigaword corpus (Graff et al.,
2003). To be speciﬁc, it is a headline generation
task, with the ﬁrst sentence of the article as passage
input, and the headline as summary. The average
output length is 9.7.

4.1.2 Real World Benchmarks

We also deploy our proposed model on real world
sponsored search engine applications. For a spon-
sored search engine, advertisers will provide their
websites and their interested keywords, where key-
words can also be auto-generated with a trained
landing page title-to-keyword generation model.
When search engine users search a query, it has
chances to trigger some keywords that advertis-
ers have interest on, and the trigger procedure can
be seen as a query-to-keywords generation task.
We collect three commercial datasets for advertise-
ments query-to-keyword generation and landing
page title-to-keyword generation tasks. The corpus
was collected from En-US market. The corpus size
of each dataset is shown in Table 1. The deﬁnition
and collection details are as following:

Table 1: The corpus size of QKG-EM, QKG-BM, and
ATKG datasets.

Train
72,876

Dataset
QKG-EM
QKG-BM 6,474,865
5,001,037
ATKG

Valid
10,000
10,000
10,000

Test
2,130
492,278
355,824

All
85,006
6,977,143
5,366,861

Table 2: The performance of our methods and baseline methods for non-autoregressive summarization task on
XSum benchmark. “(+x.xx)” means the absolute improvement based on BANG.

MODEL
NAT (Gu et al., 2017)
CMLM (Ghazvininejad et al., 2019)
LevT (Gu et al., 2019)
BANG (Qi et al., 2021)
BANG + LSP
BANG + LTF-Distill
BANG + LSP-TF-Distill
BANG + LBS-Hard-Distill
BANG + LBS-Distill
BANG + LSP-BS-Distill

PRE-TRAIN
No
No
No
Yes
Yes
Yes
Yes
Yes
Yes
Yes

ROUGE-1
24.04
23.82
24.75
32.59
33.01(+0.42)
34.72 (+2.13)
35.02(+2.43)
35.22 (+2.63)
36.13 (+3.54)
36.26 (+3.67)

ROUGE-2
3.88
3.60
4.18
8.98
9.27(+0.29)
10.18 (+1.20)
10.37(+1.39)
11.82(+2.84)
11.73 (+2.75)
12.04(+3.06)

ROUGE-L
20.32
20.15
20.87
27.41
27.76(+0.35)
29.36 (+1.95)
29.52(+2.11)
29.36(+1.95)
30.02 (+2.61)
30.19 (+2.78)

OVERALL
16.08
15.86
16.60
22.99
23.35(+0.36)
24.75 (+1.76)
24.97(+1.98)
25.47(+2.48)
25.96 (+2.97)
26.16(+3.17)

Table 3: Non-autoregressive generation performance on Gigaword summarization. SD is short for sequence distil-
lation, with the AR distilled training set. Soft means with training with AR predicted soft lables. self-paced means
reverse self-paced learning with training samples re-weighting.

Model
BANG (Qi et al., 2021)
BANG + LSP
BANG + LTF-Distill
BANG + LSP-TF-Distill
BANG + LBS-Hard-Distill
BANG + LBS-Distill
BANG + LSP-BS-Distill

PRE-TRAIN
Yes
Yes
Yes
Yes
Yes
Yes
Yes

ROUGE-1
32.61
33.09(+0.48)
33.30 (+0.69)
33.75(+1.14)
36.13(+3.52)
36.32 (+3.71)
36.62(+4.01)

ROUGE-2
13.39
14.12(+0.73)
14.01 (+0.62)
14.50(+1.11)
16.95(+3.56)
17.28(+3.89)
17.74(+4.35)

ROUGE-L
30.76
31.30(+0.54)
31.38(+0.62)
31.80(+1.04)
33.75(+2.99)
34.04 (+3.28)
34.29(+3.53)

OVERALL
25.59
26.17(+0.58)
26.23 (+0.64)
26.68(+1.09)
28.94 (+3.35)
29.21 (+3.62)
29.55(+3.96)

QKG-EM: Query to close variant keywords gen-
eration for exact match. In this task, given a user
query, the model generates a list of keywords that
have exactly the same intent as the source query.
Such a situation usually occurs when advertisers
have a clear targeted audience, judging from the
search queries. To construct QKG-EM, we col-
lect the user query and keywords from clicked ads.
Then, three crowdsourcing annotators are asked to
give a binary label for each query and keyword pair.
We determine the data label when more than two
annotators reach a consensus. The average target
sequence length in the training set and test set is
3.21 and 2.52 respectively. After tokenization into
word pieces, the numbers are 4.07 and 3.42.

QKG-BM: Query to keywords generation for
broad match. In this task, given a user query, the
model generates a list of keywords that is semantic
relevant to the query. This happens when advertis-
ers want to reach to a broader slice of users that may
be interested in their product. Similar to construct
QKG-EM, we collect a set of query and keyword
pairs from clicked data. And because QKG-BM is
harder to judge, we ask ﬁve crowdsourcing anno-
tators to label each pair of QKG-BM. When more
than three people reach a consensus, we determine
the ﬁnal label. The average target sequence length
in the training set and test set is 2.70 and 2.94 re-
spectively. After tokenization into word pieces, the

numbers are 3.68 and 3.91.

ATKG: Ad title to keywords generation. In this
task, given an ad landing page title, the model gen-
erates a list of keywords that are relevant to the ad
title. For many electronic business platforms, there
are lots of products without ready-made keywords
of ad. This task tends to automatically generate
keywords. To construct ATKG, we collect query
and landing page title pairs through clicked data,
and regard the query as the keywords of the landing
page title. Then, three crowdsourcing annotators
are asked to label each pair, and we also determine
the ﬁnal label by consensus. The average target
sequence length in the training set and test set is
3.71 and 4.04 respectively. After tokenization into
word pieces, the numbers are 4.77 and 5.28.

For these tasks, the AR models latency can not
meet the requirements while optimized NAR gen-
eration model can be online used to meet the real
time usage.

4.2 Baselines

We cite the NAR baseline model results from Qi
et al. (2021). The referred baseline models include:
NAT (Gu et al., 2017), CMLM (Ghazvinine-
jad et al., 2019), LevT (Gu et al., 2019), and
BANG (Qi et al., 2021). NAT is the ﬁrst non-
autoregressive translation model based on Trans-
former, it removes the unidirectional information

ﬂow constraint and introduces sequence distillation,
target length prediction, decoder inputs copy tech-
niques. CMLM predicts arbitrary subset of masked
words in a target sequence with the masked lan-
guage model objective. LevT adopts insertion and
deletion as basic operations to edit the draft. BANG
is our most related NAR model and has been thor-
oughly introduced. We follow Qi et al. (2021) to
cite the ﬁrst round outputs of CMLM and LevT,
NAR ﬁnetuning results of BANG as their NAR
results. We carry out improvements on the base
of BANG. The BANG variants with our proposed
techniques are notated as:

BANG+TF-Distill: It uses the teacher-forcing
distillation method for enhancing the model train-
ing, as described in Section § 3.1. In short words,
soft labels with original training data serving as
previous tokens.

BANG+BS-Distill: It uses the beam-search dis-
tillation method in the model training, as described
in Section § 3.1. In short words, soft labels with
beam search output training data serving as previ-
ous tokens.

BANG+BS-Hard-Distill: It also uses the beam-
search distillation method, but instead of using
the predicting score of the autoregressive teacher
model for distillation, it uses one-hot vector for
distillation, this kind of distillation method have
been widely used in non-autoregressive models (Gu
et al., 2017).

BANG+SP-BS-Distill:

It combines the self-
paced learning for teacher-forcing distillation, as
described in Section § 3.2.

4.3 Main Results

We report the performance of our methods and base-
lines for non-autoregressive summarization task on
XSum and Gigaword benchmarks in Table 2 and 3.
From the performance of “BANG” and “BANG +
LTF-Distill”, we see that teacher forcing distillation
achieves 1.76 and 0.64 points absolute performance
improvement on overall score for XSum and giga-
word. It illustrates strong autoregressive teacher
model can help the non-autoregressive learning by
soft labels knowledge without the beam search in-
ference procedure. Comparing the performance
of “BANG” and “BANG+LSP” we see the empha-
sis of easy samples will lead to a better converged
model. Comparing the performance of “BANG +
LBS-Distill” with “BANG + LTF-Distill” and “BANG
+ LBS-Hard-Distill”, we ﬁnd that the proposed mixed

distillation method achieves better performance
than other distillation method. From the perfor-
mance in Table 2 and 3, we see that “BANG +
LSP-BS-Distill” achieves new state-of-the-art perfor-
mance on both XSum and Gigaword benchmarks,
and compared with BANG, it achieves 3.17 and
3.92 points absolute improvement, respectively.
The results demonstrate the proposed self-paced
mixed distillation method for non-autoregressive
generation is effective.

In Table 4, we show the comparison of our meth-
ods and baselines on SQuAD 1.1 for question gen-
eration task. We reach conclusions consistent with
summarization. “BANG + LSP-BS-Distill” achieves
new state-of-the-art performance and improve the
the overall score 2.82 points.

4.4 Ablation Study

4.4.1 Distillation with Soft versus Hard

Target

In the section 3.1,
it presents the distillation
learning with soft target by calculating the KL-
divergence between the teacher and student models’
predictions in Eqn. 16.

We set a combination of hard and soft targets
and show the results in Table 5. We reproduce
the BANG NAR results and set all of the hyper-
parameters the same(including the random seed),
to equally compare the combination of hard and
soft labels’ weight. A consistent improvement can
be seen when increasing the soft weight. It can be
seen that soft labels are more suitable than hard
labels for NAR learning.

4.4.2 Self-paced learning strategy

§ 3.2, we propose to focus on modality-
In
consistency easy samples. Here we present the
results if we focus on the hard samples:

Comparison of how to calculating λi is shown
in Table 6 and 7. Here, λi = P P L = exp(loss),
λi = loss and λi = log(loss) is to focus on hard
examples. λi = 1/P P L = 1/exp(loss) is our
proposed self-paced learning strategy. It can be
observed the hard examples focus sp strategies hurt
the performance for both LTF-Distill in Table 6 and
LSP-BS-Distill in Table 7.
It shows that the NAR
models do not have the capacity to learn from hard
multi-modality training samples, but the modality
consistent easy data will help NAR models learn a
ﬂuent generation pattern.

Table 4: Non-autoregressive generation performance on SQuAD 1.1 question generation. SD is short for sequence
distillation, with the AR distilled training set. Soft means with training with AR predicted soft lables. self-paced
means reverse self-paced learning with training samples re-weighting.

MODEL
NAT (Gu et al., 2017)
CMLM (Ghazvininejad et al., 2019)
LevT (Gu et al., 2019)
BANG (Qi et al., 2021)
BANG + LSP
BANG + LTF-Distill
BANG + LSP-TF-Distill
BANG + LBS-Hard-Distill
BANG + LBS-Distill
BANG + LSP-BS-Distill

PRE-TRAIN
No
No
No
Yes
Yes
Yes
Yes
Yes
Yes
Yes

ROUGE-L
31.51
32.44
31.38
44.07
44.54 (+0.47)
46.14(+2.07)
46.49(+2.42)
46.14 (+2.07)
47.26 (+3.19)
47.41 (+3.34)

BLEU-4
2.46
2.33
2.27
12.75
13.61(+0.86)
13.54(+0.79)
14.14(+1.39)
15.19 (+2.44)
15.30 (+2.55)
15.64 (+2.89)

METEOR
8.86
8.84
9.14
18.99
19.46 (+0.47)
20.06(+1.07)
20.34(+1.35)
21.03 (+2.04)
21.05 (+2.06)
21.22 (+2.23)

OVERALL
14.29
14.54
14.26
25.27
25.87(+0.60)
26.58(+1.31)
26.99(+1.72)
27.45 (+2.18)
27.87 (+2.60)
28.09 (+2.82)

Table 5: The performance on SQuAD 1.1 of different
γ for LTF-Distill. OVL is short for OVERALL score.

γ = ROUGE-L BLEU-4 METEOR
0.00
0.25
0.50
0.75
1.00

19.00
19.18
19.52
20.07
20.06

43.71
43.86
44.43
45.26
46.14

12.30
12.33
13.00
13.52
13.54

OVL
25.00
25.12
25.65
26.28
26.58

Table 6: BANG NAR results with different self-paced
learning λi for LSP. Here if λ is set to None, then the
model is same as BANG NAR. OVL is short for OVER-
ALL score.

λi=
loss
log loss
None
1/PPL

ROUGE-L BLEU-4 METEOR
9.70
10.45
12.75
13.61

17.09
17.75
18.99
19.46

42.25
42.88
44.07
44.54

OVL
23.01
23.69
25.27
25.87

4.4.3 Non-AutoRegressive versus
AutoRegressive generation

The self-paced soft distillation has no inﬂuence on
the inference latency, thus we cited the AR and
NAR latency from Qi et al. (2021) for readers that
are not familiar with NAR performance. We list
the Transformer AR performance and latency to be
compared with BANG NAR model in Table 8 and
Table 9 for SQuAD 1.1 question generation and
XSum summarization.

4.4.4 Multi-stage Finetuning

In previous sections, the NAR student model is
initialized with the pre-trained model. Here we
discuss initializing the NAR model with different
starting points.

In Table 10, we load different models before
ﬁnetuning, as a two-stage training workﬂow. The
two-stage ﬁnetuning experimental results help to
claim these points:

1) No need to specially train the samples

Table 7: BANG NAR results with different self-paced
learning λi for LSP-BS-Distill. Here if λ is set to None,
then the model is same as LBS-Distill. OVL is short for
OVERALL score.

λi=
loss
log loss
None
1/PPL

ROUGE-L BLEU-4 METEOR
15.03
14.21
15.30
15.64

20.85
20.40
21.05
21.22

46.94
46.51
47.26
47.41

OVL
27.61
27.04
27.87
28.09

Table 8: Latency (ms/sample) on SQuAD 1.1 question
generation. In this table, R-L, B-4, MTR are short for
ROUGE-L, BLEU-4, and METEOR respectively.

MODEL
Transformer
BANG
+ LSP-BS-Distill

R-L
29.43
44.07
47.41

B-4 MTR
9.86
4.61
18.99
12.75
21.22
15.64

LATENCY
159.49
15.69
15.69

equally before focusing on the easy samples with
self-paced learning. Comparing the results of
LBS-Hard-Distill + LSP-BS-Distill, we ﬁnd it’s on par
with directly LSP-BS-Distill ﬁnetuning. It is because
that although the modality consistency score is cal-
culated with the PPL (or loss), when starting the
training, the training samples’ losses are very close
and can be seen as equally learning, then gradually
emphasize the easy samples.

2) Comparing the results of LSP-BS-Distill with
NAR+LSP-BS-Distill, and NAR with LSP-BS-Distill +
NAR, we see performance damage on both of
the extra stage 1 pre-ﬁnetuning.
It shows that
the LSP-BS-Distill reinforces the local optimization,
while the converged NAR model on original data
does not agree with the self-paced local optimal.
The LSP-BS-Distill will result in a better performance
modality, which will not help the original training
corpus.

3) Simply adding original training data will hurt
sequence distillation performance, while adding
original knowledge as soft distributions does not,

Table 9: Latency (ms/sample) on XSum summariza-
tion. In this table,R-1,R-2,R-L are short for ROUGE-1,
ROUGE-2, and ROUGE-L respectively.

MODEL
Transformer
BANG
+ LSP-BS-Distill

R-1
30.66
32.59
36.26

R-2
10.80
8.98
12.04

R-L
24.48
27.41
30.19

LATENCY
262.47
15.97
15.97

Table 10: SQuAD 1.1 question generation results. In
this table, R-L, B-4, MTR are short for ROUGE-L,
BLEU-4, and METEOR respectively.

Stage-1
-
AR
LSP-BS-Distill
-
NAR
AR
LBS-Hard-Distill
-
NAR
-

Stage-2
NAR
NAR
NAR
LSP-BS-Distill
LSP-BS-Distill
LSP-BS-Distill
LSP-BS-Distill
LBS-Hard-Distill
LBS-Hard-Distill
LBS-Distill

R-L
44.07
44.77
43.12
47.41
46.71
47.71
47.25
46.14
45.96
47.26

B-4 MTR
19.46
13.61
19.62
13.00
19.10
12.30
21.22
15.64
20.95
15.16
21.52
15.90
21.12
15.58
21.03
15.19
20.79
14.90
21.05
15.30

when observing the performance of LBS-Hard-Distill,
NAR + LBS-Hard-Distill and LBS-Distill . To beneﬁt
from original data, speciﬁc algorithms should be
used (Ding et al., 2021, 2020), otherwise the perfor-
mance may be damaged with the increased modal-
ity as our experimental results. Soft labels learning
could be a simple yet effective choice to keep more
information from raw data.

4) It’s interesting to ﬁnd that by loading the
parameters from AR teacher model, performance
can be further improved for both NAR ﬁnetuning
or LSP-BS-Distill ﬁnetuning. It is probably because
BANG structure supports different generation pat-
tern naturally.

4.4.5 Self-distillation to teacher NAR

generation with shared parameters AR
teacher

In previous sections, the AR teacher models param-
eters are frozen after the AR ﬁnetuning procedure
to act as a stable teacher. Next we want to validate
that will soft labels distillation help NAR perfor-
mance as a self-distillation strategy, then we can
validate the effectiveness before employing it on
large-scale pre-training. Considering that all pre-
dicting streams of BANG share the model param-
eters during pre-training, here we carry out exper-
iments to ﬁnetune a same model for both AR and
NAR generation, with and without the knowledge
from AR stream to NAR stream.

We ﬁnetune a BANG model with 50% batch

of data in AR information ﬂow and 50% batch of
data in NAR information ﬂow on sequence dis-
tilled SQuAD 1.1 question generation benchmark,
which we note as LBS-Hard-Distill. We train another
model with the same setting except that the NAR
targets are AR predicted distributions and note as
BS-Soft-Self-Distill. The results are shown in Ta-
ble 11.

Table 11: SQuAD 1.1 question generation.
Infer is
short for inference type. R-L, B-4, and MTR are short
for ROUGE-L, BLEU-4, and METEOR, respectively.

Infer
R-L
Model
LBS-Hard-Distill
NAR 45.98
LBS-Soft-Self-Distill NAR 46.41
LBS-Hard-Distill
46.77
LBS-Soft-Self-Distill
46.68

AR
AR

B-4 MTR
20.65
14.87
20.91
15.25
22.09
18.18
21.98
17.95

Comparing results in Table 11 and Tabel 4 we
can see that with the same model that able to gen-
erate outputs in both AR and NAR information
ﬂow(Table 11), the outputs are slightly worse than
directly NAR ﬁnetuing (Table 4). It is reasonable
because the same model parameters are shared for
different generation pattern. Comparing the NAR
performance in Table 11 we can see the improve-
ments by teaching knowledge from its AR stream.
It motivates us to improve the NAR performance
of BANG pre-training to use the AR stream pre-
dicted distributions for teaching other streams as
introduced in section § 3.3.

4.5 Results for Real-World Advertisements

Applications

We show the results of BANG AR teacher model,
BANG NAR baseline model and our improvements
with LSP-BS-Distill ﬁnetuning for three real world ad-
vertisements datasets in Table 12, Table 13, and
Table 14. For AR teacher model, the beam size is
set as 5 and length penalty as 1.2 for all the test
set evaluation. Inference batch size is set to 1 to
evaluate the latency to simulate online deployment.
Notice that the ﬁnal deployed BANG NAR genera-
tion model will be further optimized to accelerate,
while for fair comparison, here we keeps the same
code base as previous released BANG model.

Obviously we can see the NAR generation will
signiﬁcantly reduce the inference latency, which
can be deployed on real-world keywords extension
usage. The difference between BANG NAR and
LSP-BS-Distill models can be ignored and resulted
by the machine performance ﬂuctuation because
LSP-BS-Distill has no effect on the inference proce-

Table 12: Performance and latency (ms/sample) on
Query to Keywords Generation dataset QKG-EM. In
this table, B- is short for BLEU-.

Table 14: Performance and latency (ms/sample) on
Ad landing page Title to Keywords Generation dataset
ATKG. In this table, B- is short for BLEU-.

B-1
Model
61.27
BANG AR
BANG NAR 67.07
+LSP-BS-Distill
66.10

B-2
48.90
55.76
56.11

B-4
31.02
28.35
29.61

LATENCY
120.48
16.69
16.60

B-1
Model
40.06
BANG AR
BANG NAR 28.19
+LSP-BS-Distill
39.38

B-2
27.54
21.61
26.91

B-4
11.65
8.17
11.41

LATENCY
144.09
16.73
16.96

Table 13: Performance and latency (ms/sample) on
Query to Keywords Generation dataset QKG-BM. In
this table, B- is short for BLEU-.

Table 15: Non-autoregressive generation performance
on XSum summarization. BANG160g means our pre-
trained model to initialize the model before ﬁnetuning.
Teacher models are the same for fair comparison.

Model
B-1
38.04
BANG AR
BANG NAR 31.53
+LSP-BS-Distill
37.25

B-2
27.12
17.15
26.58

B-4
6.15
2.59
6.14

LATENCY
115.74
17.16
16.60

Pretrain
BANG
BANG160g
BANG160g LSP-BS-Distill

Finetune
NAR
NAR

R-1
32.59
33.55
36.65

R-2
8.98
9.69
12.70

R-L
27.41
28.30
30.61

dure. For QKG-BM and ATKG, LSP-BS-Distill re-
duces the performance gap between NAR model
and AR teacher model signiﬁcantly while keeps the
same latency. It is exciting for sponsored search en-
gine keywords extension tasks. Another interesting
observation is that for query to keywords exten-
sion QKG-EM, BANG NAR generation has better
performance than AR generation for BLEU-1 and
BLEU-2, while worse performance for BLEU-4.
It shows that when the training data is not very
adequate, meantime the output is short keywords,
NAR generation is possible to outperform AR gen-
eration regarding single word and two adjacent
words performance as BLEU-1 and BLEU-2, while
still worse performance regards relatively longer
ﬂuent expresstions as BLEU-4. With LSP-BS-Distill,
the BLEU-4 score is improved while the BLEU-1
and BLEU-2 is hurt, which means that our pro-
posed method will make the NAR student model
more consistent with the AR teacher model rather
than simply improving evaluation metrics. Gener-
ally speaking, with our proposed learning method,
BANG NAR model has satisfying performance
close to AR generation but much lower latency.

4.6 Pre-training Results

We perform further pre-training on 160GB unla-
beled English corpus, including news, books, sto-
ries and web text. It is similar to the corpus of
well-known AR pre-training works such as Prophet-
Net (Qi et al., 2020) and BART (Lewis et al., 2019).
The learning rate is set to 4e-4, 366k steps, batch
size 2048, distillation weight α 0.5 on 16 32GB
memory NVIDIA Tesla V100 GPUs. We show the
reusults for XSum summarization and SQuAD 1.1
question generation in Table 15 and Table 16.

We can see that with self-distillation further
pre-training, performance is consistently improved
among the two benchmarks and different NAR ﬁne-
tuning methods. To ensure the results comparable,
the teacher model for 160 LSP-BS-Distill ﬁnetuning
keeps the same as BANG LSP-BS-Distill baseline. We
will also release the further pretrained model when
our code is open sourced.

5 Related Work

AR generation has been widely developed in recent
years, and pre-training techniques achieve signiﬁ-
cantly performance improvement in AR generation
tasks (Brown et al., 2020a; Lewis et al., 2020; Raf-
fel et al., 2020; Qi et al., 2020). GPT3 (Brown
et al., 2020a) pre-train a large model and generate
the next token from left-to-right. BART (Lewis
et al., 2020), T5 (Raffel et al., 2020), and Prophet-
Net (Qi et al., 2020) are based on encoder-decoder
architecture. BART (Lewis et al., 2020) pre-train
the model through reconstructing the original text
from a noised input. ProphetNet (Qi et al., 2020)
learn to recover a mask span of a input text with
a n-gram prediction mechanism. T5 (Raffel et al.,
2020) investigates different pre-training techniques
and pre-train a generation model with large scale
corpus. Pre-training techniques are well-developed
in AR generation tasks.

Different from AR generation, few pre-training
works focus on NAR generation. BANG (Qi et al.,
2021) is the ﬁrst large scale pre-training work for
NAR generation. It combines AR, NAR, and semi-
NAR in the pre-training. Except pre-training, se-
quence distillation is one powerful method to im-
prove the performance in NAR generation. It has

Table 16: Non-autoregressive generation performance
on SQuAD 1.1 question generation. BANG160g means
our pretrained model to initialize the model before ﬁne-
tuning. Teacher models are the same for fair compari-
son.

Chess, Jack Clark, Christopher Berner, Sam Mc-
Candlish, Alec Radford, Ilya Sutskever, and Dario
Amodei. 2020a. Language models are few-shot
learners. In Annual Conference on Neural Informa-
tion Processing Systems.

Pretrain
BANG
BANG160g
BANG160g LSP-BS-Distill

Finetune
NAR
NAR

R-L
44.07
44.59
47.83

B-4 MTR
18.99
12.75
19.55
12.97
21.59
16.20

been widely studied (Gu et al., 2017; Zhou et al.,
2019; Ren et al., 2020). Zhou et al. (2019) analyze
sequence distillation from reducing the modality
perspective. And Ren et al. (2020) study it from
reducing the dependency between target sequence
tokens perspective. Besides sequence distillation,
glancing sampling (Qian et al., 2020), curriculum
learning from AR model (Guo et al., 2020), and
encoder copy for translation (Gu et al., 2017) are
proposed to reduce the difﬁculty of NAR genera-
tion.

In this work, we propose a new self-paced mixed
distillation method to reduce the difﬁculty of NAR
generation and successfully applied it to BANG.

6 Conclusion

In this paper, we propose several techniques to
improve the non-autoregressive generation perfor-
mance based on BANG. Firstly, we propose to use
mixed distillation to keep the knowledge from orig-
inal corpus rather than completely ignoring them
or simply adding them back. Secondly, self-paced
learning is adopted to focus on the easy samples for
modality-consistent. Then we extend the mixed dis-
tillation into self-distillation pre-training for BANG
to utilize its autoregressive stream knowledge. Ex-
tensive experiments are carried out to support our
claims. We see signiﬁcant improvements on the
public benchmarks including summarization tasks
XSum and gigaword, question generation tasks
SQuAD 1.1. We also deploy our model in real-
world sponsored search engine applications.

References

Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-Voss,
Gretchen Krueger, Tom Henighan, Rewon Child,
Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,
Clemens Winter, Christopher Hesse, Mark Chen,
Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin

Tom B Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, et al. 2020b. Language models are few-shot
learners. arXiv preprint arXiv:2005.14165.

Liang Ding, Longyue Wang, Xuebo Liu, Derek F
Wong, Dacheng Tao, and Zhaopeng Tu. 2020.
Understanding and improving lexical choice in
arXiv preprint
non-autoregressive translation.
arXiv:2012.14583.

Liang Ding, Longyue Wang, Xuebo Liu, Derek F
Wong, Dacheng Tao, and Zhaopeng Tu. 2021. Re-
juvenating low-frequency words: Making the most
of parallel data in non-autoregressive translation.
arXiv preprint arXiv:2106.00903.

Marjan Ghazvininejad, Omer Levy, Yinhan Liu, and
Luke Zettlemoyer. 2019. Mask-predict: Parallel
decoding of conditional masked language models.
arXiv preprint arXiv:1904.09324.

David Graff, Junbo Kong, Ke Chen, and Kazuaki
Maeda. 2003. English gigaword. Linguistic Data
Consortium, Philadelphia, 4(1):34.

Jiatao Gu, James Bradbury, Caiming Xiong, Vic-
Non-
arXiv

tor OK Li, and Richard Socher. 2017.
autoregressive neural machine translation.
preprint arXiv:1711.02281.

Jiatao Gu, Changhan Wang, and Junbo Zhao. 2019.
Levenshtein transformer. In Advances in Neural In-
formation Processing Systems, pages 11181–11191.

Junliang Guo, Xu Tan, Linli Xu, Tao Qin, Enhong
Chen, and Tie-Yan Liu. 2020. Fine-tuning by cur-
riculum learning for non-autoregressive neural ma-
chine translation. In Proceedings of the AAAI Con-
ference on Artiﬁcial Intelligence, volume 34, pages
7839–7846.

Bing He, Mustaque Ahamad, and Srijan Kumar. 2021.
Petgen: Personalized text generation attack on
deep sequence embedding-based classiﬁcation mod-
els. In Proceedings of the 27th ACM SIGKDD Con-
ference on Knowledge Discovery & Data Mining,
pages 575–584.

Yoon Kim and Alexander M Rush. 2016. Sequence-
arXiv preprint

level knowledge distillation.
arXiv:1606.07947.

Mike Lewis, Yinhan Liu, Naman Goyal, Mar-
jan Ghazvininejad, Abdelrahman Mohamed, Omer
Levy, Ves Stoyanov, and Luke Zettlemoyer. 2019.
Bart: Denoising sequence-to-sequence pre-training
for natural language generation,
translation, and
comprehension. arXiv preprint arXiv:1910.13461.

Chunting Zhou, Graham Neubig, and Jiatao Gu.
2019. Understanding knowledge distillation in non-
autoregressive machine translation. arXiv preprint
arXiv:1911.02727.

Qingqing Zhu, Xiuying Chen, Pengfei Wu, JunFei Liu,
and Dongyan Zhao. 2021. Combining curriculum
learning and knowledge distillation for dialogue gen-
eration. In Findings of the Association for Computa-
tional Linguistics: EMNLP 2021, pages 1284–1295,
Punta Cana, Dominican Republic. Association for
Computational Linguistics.

Xu Zou, Da Yin, Qingyang Zhong, Hongxia Yang,
Zhilin Yang, and Jie Tang. 2021. Controllable gen-
eration from pre-trained language models via in-
verse prompting. In Proceedings of the 27th ACM
SIGKDD Conference on Knowledge Discovery &
Data Mining, pages 2450–2460.

Mike Lewis, Yinhan Liu, Naman Goyal, Mar-
jan Ghazvininejad, Abdelrahman Mohamed, Omer
Levy, Veselin Stoyanov, and Luke Zettlemoyer.
2020. BART: denoising sequence-to-sequence pre-
training for natural language generation, translation,
and comprehension. In Proceedings of the 58th An-
nual Meeting of the Association for Computational
Linguistics, pages 7871–7880.

Shashi Narayan, Shay B Cohen, and Mirella Lapata.
2018. Don’t give me the details, just the summary!
topic-aware convolutional neural networks for ex-
In EMNLP, pages 1797–
treme summarization.
1807.

Weizhen Qi, Yeyun Gong, Jian Jiao, Yu Yan, Weizhu
Chen, Dayiheng Liu, Kewen Tang, Houqiang Li,
Jiusheng Chen, Ruofei Zhang, et al. 2021. Bang:
Bridging autoregressive and non-autoregressive gen-
eration with large scale pretraining. In International
Conference on Machine Learning, pages 8630–8639.
PMLR.

Weizhen Qi, Yu Yan, Yeyun Gong, Dayiheng Liu,
Nan Duan, Jiusheng Chen, Ruofei Zhang, and
Ming Zhou. 2020. Prophetnet: Predicting future n-
gram for sequence-to-sequence pre-training. arXiv
preprint arXiv:2001.04063.

Lihua Qian, Hao Zhou, Yu Bao, Mingxuan Wang,
Lin Qiu, Weinan Zhang, Yong Yu, and Lei Li.
2020. Glancing transformer for non-autoregressive
arXiv preprint
neural machine translation.
arXiv:2008.07905.

Colin Raffel, Noam Shazeer, Adam Roberts, Kather-
ine Lee, Sharan Narang, Michael Matena, Yanqi
Zhou, Wei Li, and Peter J. Liu. 2020. Exploring
the limits of transfer learning with a uniﬁed text-to-
text transformer. Journal of Machine Learning Re-
search, 21:140:1–140:67.

Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
Percy Liang. 2016. Squad: 100,000+ questions for
machine comprehension of text. In EMNLP, pages
2383–2392.

Yi Ren, Jinglin Liu, Xu Tan, Zhou Zhao, Sheng
Zhao, and Tie-Yan Liu. 2020.
A study of
non-autoregressive model for sequence generation.
arXiv preprint arXiv:2004.10454.

Alexander M. Rush, Sumit Chopra, and Jason Weston.
2015. A neural attention model for abstractive sen-
In Proceedings of the 2015
tence summarization.
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 379–389, Lisbon, Portugal.
Association for Computational Linguistics.

Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-
Yan Liu. 2019. Mass: Masked sequence to sequence
pre-training for language generation. arXiv preprint
arXiv:1905.02450.

