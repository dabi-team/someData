2
2
0
2

g
u
A
1
1

]
P
S
.
s
s
e
e
[

1
v
7
0
6
5
0
.
8
0
2
2
:
v
i
X
r
a

Real-Time Massive MIMO Channel Prediction: A
Combination of Deep Learning and NeuralProphet

M. Karam Shehzad1, 2, Luca Rose1, M. Furqan Azam3, and Mohamad Assaad2

1Nokia Bell-Labs, France.
2Laboratoire des Signaux et Syst`emes, CentraleSupelec, CNRS, University of Paris-Saclay, France.
3Flemish Institute for Technological Research (VITO), 2400 Mol, Belgium.
{muhammad.shehzad, luca.rose}@nokia.com, muhammadfurqan.azam@vito.be, mohamad.assaad@centralesupelec.fr

Abstract—Channel state information (CSI) is of pivotal im-
portance as it enables wireless systems to adapt transmission
parameters more accurately, thus improving the system’s overall
performance. However,
it becomes challenging to acquire ac-
curate CSI in a highly dynamic environment, mainly due to
multi-path fading. Inaccurate CSI can deteriorate the perfor-
mance, particularly of a massive multiple-input multiple-output
(mMIMO) system. This paper adapts machine learning (ML)
for CSI prediction. Speciﬁcally, we exploit time-series models
of deep learning (DL) such as recurrent neural network (RNN)
and Bidirectional long-short term memory (BiLSTM). Further,
we use NeuralProphet (NP), a recently introduced time-series
model, composed of statistical components, e.g., auto-regression
(AR) and Fourier terms, for CSI prediction. Inspired by statistical
models, we also develop a novel hybrid framework comprising
RNN and NP to achieve better prediction accuracy. The proposed
channel predictors (CPs) performance is evaluated on a real-time
dataset recorded at the Nokia Bell-Labs campus in Stuttgart,
Germany. Numerical results show that DL brings performance
gain when used with statistical models and showcases robustness.

Index Terms—AI/ML,
MIMO, NeuralProphet, 6G.

channel prediction, CSI, massive

I. INTRODUCTION

Artiﬁcial intelligence (AI) and machine learning (ML) are
the deﬁning technologies of next-generation wireless net-
works, called sixth-generation (6G). It is expected that AI/ML
will play a pivotal role in the design phase of 6G wireless
networks [1], [2]. In this regard, standardization of ML has
also begun [1]. Speciﬁcally, it is expected that ML, together
with massive multiple-input multiple-output (mMIMO), dis-
ruptive technology of ﬁfth-generation (5G), can improve, for
instance, precoding gain [1], [3]. Furthermore, the validity of
ML algorithms in a real-time environment has opened up a
new horizon for the consideration of ML in 6G [4].

A mMIMO system can improve signal-to-noise ratio, as
well as throughput, by utilizing diversity and multiplexing
techniques, respectively [5]. However, accurate channel state
information (CSI) is indispensable to get expected gain. In
a highly dynamic wireless communications environment, it is
hard to acquire accurate CSI, e.g., due to reporting compressed
CSI to base station (BS) by user-equipment (UE), and feed-
back/processing delays [6].

To acquire accurate CSI, researchers have started exploiting
an active approach, known as channel prediction [4] as it can
improve the accuracy of CSI without requiring extra radio
resources. The key idea of channel prediction is to forecast
CSI realizations that can mitigate, e.g., compressed CSI and
induced delays. Recently, its application to reduce mMIMO
CSI feedback overhead and accuracy improvement of acquired
CSI at BS have opened new doors for its consideration [3],
[6].

The study of channel prediction has been considered by a
few researchers in the literature [4], [7]–[10], which is mainly
divided into statistical models and ML. For example, auto-
regression (AR) and parametric models have been studied in
[7] and [8], respectively. However, the downside of statistical
models is their iterative re-estimation of parameters that can
expire quickly in a dynamic environment. And due to ma-
nipulation of matrices, parameters re-estimation can be costly
[8]. In contrast, ML has the capability of making multi-step
prediction, as well as can provide huge gains in a diverse envi-
ronment. To this end, [9] and [10] evaluated the performance
of a recurrent neural network (RNN), an ML algorithm, on
a synthetic dataset. To demonstrate the effectiveness of RNN
in a real-world environment, [4] evaluated the performance of
RNN on compressed and uncompressed CSI.

long-short

In this paper, we evaluate the performance of various state-
of-the-art deep learning (DL) models such as RNN [4] and
Bidirectional
term memory (BiLSTM). Also, a
DL inspired statistical algorithm, i.e., NeuralProphet (NP), is
tested on out-of-sample data. Further, getting the inspiration
from using statistical algorithm and DL together, we propose
a novel hybrid framework composed of RNN and NP, which
yields better prediction results than individual models. In
addition to this, we employ hyperparameter tuning for each
of these individual models to select only the best training
parameters. We observe the performance of channel predictors
(CPs) in a realistic environment.

Rest of the paper is organized as follows: Section II provides
details of CPs used in our study. Section III summarizes real-
time dataset. Section IV gives performance comparisons of
CPs, and Section V concludes the paper.

 
 
 
 
 
 
II. CHANNEL PREDICTION

In this section, we highlight the channel prediction models
that are used in this paper. They are divided into three parts:
RNN, BiLSTM, and a hybrid model.

A. Recurrent Neural Network (RNN)

RNN has emerged as a promising technique for time-series
predictions [11]. Due to the presence of recurrent compo-
nents, its prediction capability surpasses feed-forward neural
networks (NNs). In this study, we use RNN to design a hybrid
model. Given the vastity of the available literature on RNN [4],
we do not detail its structure. The interested reader is referred
to [4] and [10] to get deeper understanding. In summary, RNNs
are fed with d-step delayed inputs along with corresponding
labels to generate multi-step ahead predicted CSI realizations.
Later in Section II-C, we will utilize the predicted channel
vector of RNN for the design of the hybrid model.

B. Bidirectional long-short term memory (BiLSTM)

BiLSTM is composed of two independent long-short term
memory (LSTM) networks. In a BiLSTM model, information
is learned from both ends of the input data vector, which
results in better prediction performance than traditional unidi-
rectional LSTM. LSTM is an advanced version of RNN. RNN
suffers from vanishing and exploding gradient problems in the
backpropagation during model training. In the training phase,
when the backpropagation algorithm advances backwards, i.e.,
from output to the input layer of RNN, the gradient often
becomes smaller and smaller. By the time gradient reaches
the input layer, it sometimes results in zero, not updating the
weights of RNN; consequently, RNN cannot learn. Contrar-
ily, exploding gradient turns to increase the gradient, which
results in increasing weights of RNN; hence, gradient descent
diverges. To this end, an LSTM-based NN was developed by
Hochreiter and Schmidhuber [12]. Schematic of an LSTM is
depicted in Fig. 1. The core idea of LSTM is the introduction
of a memory cell and multiplicative gates, which regulate the
ﬂow of information. Brieﬂy, forget gate decides the amount
of information to be stored in the cell by utilizing the current
input, xt, and the output of the previous LSMT cell, denoted
by st−1. Mathematically,

ft = σ(Wf xt + Vf st−1 + bf )

where

σ(x) =

1
1 + e−x

(1)

(2)

is the sigmoid activation function, W and V are the weight
matrices, b is the bias vector, subscript f is associated with
the forget gate.

The input gate determines the amount of information to
be added into cell state ct−1 by exploiting xt and st−1.
Mathematically,

it = σ(Wixt + Vist−1 + bi)
gt = ξ(Wgxt + Vgst−1 + bg)

(3)

where ξ(·) represents hyperbolic tangent activation function,
subscript i and g are associated with input gate. By utilizing
input and forget gates, LSTM can determine the amount of
information to be retained and removed. Finally, the output
gate calculates the output of LSTM cell by using an updated
cell state, ct, and xt; the resultant output, given below, is then
passed to the next LSTM cell of the network.

ot = σ(Woxt + Vost−1 + bo) .

(4)

As a result of the operations above, few information is
dropped and a few is added, this updates the next long-term
state as follows:

ct = (ft ⊗ ct−1 + it ⊗ gt)

(5)

where ⊗ represents Hadamard product. Lastly, short-term
memory state, st, is calculated by passing long-term memory,
ct, through output gate as

st = ot ⊗ ξ(ct) .

(6)

In BiLSTM architecture, as depicted in Fig. 1, input infor-
mation is learned in two directions, i.e., left-to-right (forward
layer) and right-to-left (backward layer). Importantly, notation
t + 1 in BiLSTM architecture is only used for the illustration
purpose, such indexes are based on passed CSI observations.
The output information of each direction, denoted by (cid:126)s and
(cid:126)s, respectively, is passed simultaneously to the output layer,
where output is calculated as

(cid:101)yt = (cid:126)st ⊗ (cid:126)st .

(7)

C. Hybrid Model

In the hybrid model, as shown in Fig. 2, we utilize an RNN-
based CP, summarized in Section II-A, and NP, explained in
the following subsection. In the beginning, dataset is cleaned,
i.e., to check if there is corrupted/duplicate/missing data in the
dataset. Additionally, the data splitting is performed, in which
dataset is divided into three sets i.e., training, validation, and
test. Further, the input sequences are transformed into proper
format, acceptable by ML models. Processed dataset is fed
to the input of RNN, as well as NP. Then, we consider the
predicted channel vector of RNN1, which is fed to NP along
with input feature vector2. NP learns to correct the predicted
output of RNN. Later in Section IV, we demonstrate that NP
can signiﬁcantly correct the predicted outputs of RNN, and it
outperforms all standalone models, notably BiLSTM. In the
following, we explain working functionality of NP.

1It is, however, important to mention that the predicted output of BiLSTM
can also be considered. But for the sake of lower computationally complexity
of the hybrid model, we used RNN, as BiLSTM is computationally more
expensive than RNN. The rationale behind this is two-way learning of
BiLSTM and use of gates, e.g., input, forget.

2In all CPs, input features, and corresponding labels are same.

(a) Single-cell of LSTM.

(b) Fully connected BiLSTM-based NN.

Fig. 1. Graphical illustration of time-series NNs, i.e., LSTM and BiLSTM. Fig. 1 (a) shows single-cell of an LSTM, where input is real-valued CSI realization
for time instant t. Fig. 1 (b) depicts a fully connected BiLSTM-based NN, where inputs are learned in two ways. Circular shapes given in Fig. 1 (b) denote a
single-cell of LSTM, which is given in Fig. 1 (a). The outputs of BiLSTM portray predicted CSI realizations.

Rt: The trend function concerns the overall variation in the
input data. It tries to learn the points where clear variation
in the data occurs;
these points are called change-points,
represented by {n1, n2, . . . , nm}, composed of a total of m
change-points (tuned using grid search). A trend function can
be expressed as

Rt = (ζ 0 + (Γt)†)ζ) · t + (ρ0 + (Γt)†ρ)

(9)

where ζ = {ζ 1, ζ 2, . . . , ζ m}, and ρ = {ρ1, ρ2, . . . , ρm}, are
the vectors of growth rate and offset adjustments, respectively,
and ζ ∈ Rm and ρ ∈ Rm. Besides, ζ 0 and ρ0 are the
initial growth rate and offset values, respectively. And, Γt =
t }, where Γt ∈ Rm, which represents whether
{Γ1
a time t has past each change-point. Also, (·)† representing
transpose. For a jth change-point, Γj

t , . . . , Γm

t is deﬁned as

t , Γ2

Γj

t =

(cid:40)

if t ≥ nj
1,
0, otherwise

.

(10)

Ft: The seasonality function, modeled using Fourier terms

[14], captures periodicity in the dataset, and is expressed as

F p

t =

k
(cid:88)

(cid:18)

r=1

ar · cos

(cid:18) 2πrt
p

(cid:19)

+ br · sin

(cid:18) 2πrt
p

(cid:19)(cid:19)

(11)

where k is the number of Fourier terms, which are deﬁned for
a seasonality having periodicity p. At a time step t, the effect
of all seasonalities can be expressed as

Ft =

F p
t

(cid:88)

p∈P

(12)

where P is the set of periodicities. We considered three
seasonalities, their values are written in Table I.

At: AR is the process of regressing a CSI future realization
against its past realizations. The total number of past CSI
realizations considered in the AR process are referred to as

Fig. 2. Flow diagram of hybrid model.

NeuralProphet (NP): Within a short span of time, NP
has emerged as a promising choice for different time-series
prediction tasks [13]. NP is an explainable, and scalable
prediction framework. It is sometimes important to analyze
the performance of a prediction model in the form of different
components. DL-based models are difﬁcult to interpret due to
their black-box nature. Contrarily, NP is composed of different
components, where each component contributes to predicted
output, and their behaviour is well interpretable.

NP is composed of various components3, which contribute
additively to the prediction. Each component is composed of
individual inputs and modelling methodology. The output of
each component is D-step ahead future CSI realizations. For
the notational convenience, we will explain the model with
D = 1, which will be later extended for multi-step, i.e., D-
future steps. In the context of channel prediction, the predicted
value of NP for time instant t can be written as [13]

(cid:101)zt = Rt + Ft + At
where Rt and Ft represent trend and seasonality functions for
input data, respectively. Also, At is the AR effect for time t
based on previous CSI realizations. Below, we explain each
of them.

(8)

3In the documentation of NP, it is composed of six components. However,
with regard to our application of channel prediction, we dropped a few as
some of them are irrelevant for CSI prediction, e.g., holidays. For more details,
interested reader can refer to [13].

𝜎𝜎𝜎ξ⊗ξ⊗⊕𝒙𝑡𝒔𝑡−1𝒄𝑡−1𝒄𝑡𝒔𝑡Current Hidden StateCurrent Cell StateNext Hidden StateNext Cell State⊗Input𝒇𝑡𝒈𝑡𝒊𝑡𝒐𝑡𝒔𝑡−1𝒔𝑡+1𝒔𝒕−𝟏𝒔𝑡𝒔𝑡+1𝒔𝑡𝒙𝑡−1𝒙𝑡𝒙𝑡+1෥𝒚𝑡−1෥𝒚𝑡෥𝒚𝑡+1Input LayerForward LayerBackward LayerOutput LayerData Cleaning and SplittingInput DataRNN ModelProcessed DataRNN’sPredictionNeuralProphetHyperparameter TuningFinal Predictionthe order, denoted as d, of AR process. A classic AR process
of order d can be modeled as

TABLE I
SIMULATION PARAMETERS

zt = q +

e=d
(cid:88)

e=1

θe · zt−e + (cid:15)t

(13)

where q and (cid:15)t are the intercept and white noise, respectively,
and θ are the coefﬁcients of AR process. The classic AR model
can only make one-step ahead prediction, and to make multi-
step ahead prediction, D distinct AR models are required to ﬁt.
To this end, we utilize feed-forward NN along with AR (built-
in feature of NP), termed as AR-Net [15], to model AR process
dynamics. NP-based AR-Net can produce multi-step future CSI
realizations by using one AR model. AR-Net mimics a classic
AR model, with the only difference of data ﬁtting. AR-Net is
a feed-forward NN that maps AR model.

In the AR-Net, d last observations of CSI realizations are
given as input, denoted as z, which are processed by the
ﬁrst layer and then passed through each hidden layer. Cor-
respondingly, D-step ahead future CSI realizations, denoted
by (cid:101)z = {At, At+1, . . . , At+D}, can be obtained at the output
layer. Mathematically,

ωout
ωout

1 = α(U1z + bNP
1 )
i = α(Uiωout
i−1 + bNP
i )
(cid:101)z = Ul+1ωout

l

for

i ∈ [2, 3, . . . , l]

where α(·) is the rectiﬁed linear unit (ReLu) activation func-
tion, written as

Parameter
{l, β}

Value
{3, 1}

Epochs
m
{(k, p)}

50
30

Parameter
{ג, nh}(NP)
{d, D}
{ג, nh}(RNN & BiLSTM)
{(6, 365.25), (3, 7), (6, 1)}

Value
{0.01, 32}

{48, 24}
0.001, 200

IV. EXPERIMENTAL SETUP AND RESULTS

In this section, we analyze the prediction capability of the
hybrid model and compare its performance with other CPs. To
train CPs, we used dataset of track-1, which is composed of
116k consecutive CSI realizations, i.e., {ht|t = 1, . . . , 116k},
where interval of each recorded value is 0.5 ms. The dataset
(normalized) is passed through necessary pre-processing and
formatting steps using custom-built input pipelines so that it
can easily parsed through each model. Further, the dataset
of track-1 is divided into three parts: training (80%), valida-
tion (10%), and test (10%). For the sake of simplicity, the
dataset of the ﬁrst four transmit antennas is extracted for
performance evaluation. Simulation parameters are given in
Table I unless stated otherwise, and parameter details are given
in Section II-C and IV-A. The training process starts from an
initial state, where weights and biases are randomly initialized.
At the tth time iteration, pre-processed4 channel vector ht
is fed as an input to CPs. Recalling Fig. 1, xt = ht and
(cid:101)yt = (cid:101)ht+D. For notational convenience, let us assume that
input for each CP and corresponding D-step ahead prediction
is denoted by xτ and (cid:101)yτ , respectively. To train CPs, we used
Huber loss as a cost function, which is deﬁned as

α(γ) =

(cid:40)

γ, γ ≥ 0
γ < 0
0,

.

(14)

Lhuber(yτ , (cid:101)yτ ) =

(cid:40) 1

2β (yτ − (cid:101)yτ )2,
|yτ − (cid:101)yτ | − β

for |yτ − (cid:101)yτ | ≤ β

2 , otherwise

Further, l is the number of hidden layers having dimension
of size nh, bNP ∈ Rnh is the vector of biases, U ∈ Rnh×nh
is the weight matrix for hidden layers, except for the ﬁrst
U1 ∈ Rnh×d and last Ul+1 ∈ RD×nh layers. In the AR
component of NP, an important selection parameter is the order
of AR, i.e., d, which is hard to select in practice. In general,
d is chosen such that d = 2D.

III. REAL-TIME CHANNEL MEASUREMENTS

We utilize a dataset, which was recorded at Nokia Bell-Labs.
Brieﬂy, a vehicle (mimicking a UE) was considered, moving
on the sketched tracks, as indicated in Fig. 3. A transmitter
is equipped with 64 antennas placed on a rooftop (height
of approximately 15 meters), while UE was equipped with a
single monopole antenna mounted at the height of 1.5 meters
on the receiver cart, i.e., UE. The UE was moving at a speed of
3 to 5 kmph, and array antennas transmitted 64 time-frequency
orthogonal pilot signals at 2.18 GHz carrier frequency. For
more details of the measurement campaign, interested reader
can refer to [4].

t+1, yτ

t−1, xτ

t+1, (cid:101)yτ

t+2, . . . , yτ

t−2, . . . , xτ

t+2, . . . , (cid:101)yτ

(15)
where yτ = {yτ
t+D} are corresponding labels
for the input CSI realizations xτ = {xτ
t−d},
and (cid:101)yτ = {(cid:101)yτ
t+D} is the D-step ahead pre-
dicted channel vector. By using Huber loss as a cost function,
a batch of 32 samples is fed into CPs, the predicted outcome
is compared with true labels, and error is backpropagated to
update weights and biases using adaptive moment estimation
(Adam) as an optimizer. The training iterations are repeated
until a certain convergence condition is satisﬁed, i.e., cost
function is below a threshold value. Furthermore, open-source
libraries such as TensorFlow, Keras, Scikit-learn, and Pandas,
are used for implementation of CPs. Once the CPs are trained,
they can produce predictions in large batches of data on
desired future steps, i.e., D.

A. Hyperparameter Tuning

Selecting the best training parameters, e.g., learning rate (ג),
number of hidden layers (l) and units (nh), for computational
models is critical for achieving better prediction accuracy. As

4Real and imaginary parts of channel are separated, delayed channel

realizations and corresponding labels are created, etc.

Fig. 3. Pictorial representation of measurement tracks in the Nokia campus. The blue bar on the extreme left side of the ﬁgure shows a mMIMO antenna
located on the rooftop, transmitting in the direction marked with the blue arrow. Tracks on which the UE was moving are drawn with black lines, where the
arrowhead shows the direction of the UE’s movement. Furthermore, numerical values depict the track numbers. Source: [4].

hyperparameters are not directly learned by a model, rather,
we manually deﬁne them before ﬁtting the model. Therefore,
a well-known automated strategy, i.e., grid search, is used for
hyperparameter tuning. For NP, discontinuous growth function
and 90% change-point range gave best results, other tuned
parameters for NP and other CPs are listed in Table I. Also,
the standard NP model only takes uni-variate data to produce
its output. In our hybrid model, we adapted the standard NP
model for multivariate data by adding extra future regressor
into the NP model. Future regressor are the variables, which
are known into the future. In our case, predictions of RNN
are known to us and we included this information as a future
regressor of NP model. In this way, we feed both RNN
predictions and true sequences as multivariate input to NP
model to enhance its prediction capability. Besides, in RNN
and BiLSTM CPs, the dropout layer is adapted, which drops
hidden units randomly with a probability of 0.2, to prevent
over-ﬁtting.

B. Evaluation Parameters

The performance of four CPs, i.e., NP, RNN, BiLSTM, and
hybrid model, is evaluated by using normalized mean-squared
error (NMSE) and cosine similarity. Suppose that multi-step
ahead channel predicted vector and corresponding true labels
are represented by (cid:101)ht+D and ht+D, respectively. Then NMSE,
for a single track, is deﬁned as

NMSE = E






(cid:13)
(cid:13)
(cid:13)(cid:101)ht+D − ht+D
(cid:107)ht+D(cid:107)2
2

(cid:13)
2
(cid:13)
(cid:13)
2






(16)

where E{·} denotes expectation operator, and (cid:107) · (cid:107)2
2 represents
squared Euclidean norm. The second evaluation parameter,
i.e., cosine similarity, is expressed as

Cosine Similarity = E






t+D ht+D |

| (cid:101)h∗
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)((cid:101)ht+D)
(cid:13)2




(17)

C. Performance Comparison

A visual illustration of CPs is shown in Fig. 4, where CSI
realizations are shown for both predicted and the true CSI.
The internal conﬁguration and layers structure of RNN and
BiLSTM are kept similar for an impartial comparison. It can
be observed that NP’s performance is lower, which is due to
the reason for using a simple feed-forward NN. RNN has a
slightly better performance in comparison to NP. However,
BiLSTM and hybrid model give most accurate results. In the
following, we explain results, which are averaged over the
entire test dataset of each track to show a clear winner.

Fig. 5 shows the comparison of four CPs in terms of NMSE.
Particularly, performance is evaluated on three different tracks,
which were unseen by the CPs during the training phase.
The trend reveals that NP, when used standalone, does not
perform well; hence, it gives the worst performance. On the
other hand, BiLSTM outperforms RNN, which is due to the
fact that BiLSTM can retain information in their memory for
longer time periods and learns the input data in both directions,
thereby performing better. In contrast, the hybrid model per-
forms better than other CPs. The rationale behind this is that
NP learns statistical information of RNN’s predicted values
and true inputs and compares them with the corresponding
true labels. Hence, it improves the performance of RNN’s
predictions. For instance, on track-1, there is approximately
4 dB NMSE reduction in the hybrid model as compared to
BiLSTM.

In Table II, the comparison shows that the proposed hybrid
model outperforms BiLSTM, RNN, and NP. For instance, on
track-12 that is signiﬁcantly away from trained track, hybrid
model gives NMSE of 11 × 10−3 whereas NP has 60 × 10−3.
Also, hybrid model brings signiﬁcant gain in comparison to
DL models, i.e., RNN and BiLSTM. Similarly, hybrid model
improves cosine similarity.

where | · | and (·)∗ denote absolute value and conjugate
transpose, respectively.

In this paper, we have developed various time-series models
for mMIMO channel prediction. In particular, we evaluated

(cid:107)ht+D(cid:107)2



V. CONCLUSION

121331514161917456127910112024212322ArrayImagery@2021 GeoBasis-DE/BKG, GeoContent Landeshauptstadt Stuttgart, Maxar Technolo(a) NP

(b) RNN

(c) BiLSTM

(d) Hybrid

Fig. 4. A snapshot of predicted and true channel, where imaginary part of transmit antenna number three is plotted. The UE was moving on track-1, and
each measurement is captured every 0.5 ms.

gain and complexity perspective, of the above models and
others, for instance, Kalman ﬁlter. Furthermore, we aim to
explore NP with different NN architectures, e.g., replacing
feed-forward NN with BiLSTM.

REFERENCES

[1] M. K. Shehzad, L. Rose, M. M. Butt, I. Z. Kovacs, M. Assaad, and
M. Guizani, “Artiﬁcial intelligence for 6G networks: Technology ad-
vancement and standardization,” IEEE Vehicular Technology Magazine,
pp. 2–11, 2022.

[2] J. Hoydis, F. A. Aoudia, A. Valcarce, and H. Viswanathan, “Toward a
6G AI-native air interface,” IEEE Communications Magazine, vol. 59,
no. 5, pp. 76–81, 2021.

[3] M. K. Shehzad, L. Rose, and M. Assaad, “Dealing with CSI compression
to reduce losses and overhead: An artiﬁcial intelligence approach,” in
2021 IEEE International Conference on Communications Workshops
(ICC Workshops), 2021, pp. 1–6.

[4] M. K. Shehzad, L. Rose, S. Wesemann, and M. Assaad, “ML-based
massive MIMO channel prediction: Does it work on real-world data?”
IEEE Wireless Communications Letters, vol. 11, no. 4, pp. 811–815,
2022.

[5] T. L. Marzetta, “Massive MIMO: An introduction,” Bell Labs Technical

Journal, vol. 20, pp. 11–22, 2015.

[6] M. K. Shehzad, L. Rose, and M. Assaad, “A novel algorithm to
report CSI in MIMO-based wireless networks,” in ICC 2021 - IEEE
International Conference on Communications, 2021, pp. 1–6.

[7] K. E. Baddour and N. C. Beaulieu, “Autoregressive modeling for fading
channel simulation,” IEEE Transactions on Wireless Communications,
vol. 4, no. 4, pp. 1650–1662, 2005.

[8] R. O. Adeogun, P. D. Teal, and P. A. Dmochowski, “Parametric channel
prediction for narrowband MIMO systems using polarized antenna
arrays,” in 2014 IEEE 79th Vehicular Technology Conference (VTC
Spring), 2014, pp. 1–5.

[9] W. Jiang and H. D. Schotten, “Neural network-based fading channel pre-
diction: A comprehensive overview,” IEEE Access, vol. 7, pp. 118 112–
118 124, 2019.

[10] W. Liu, L.-L. Yang, and L. Hanzo, “Recurrent neural network based nar-
rowband channel prediction,” in 2006 IEEE 63rd Vehicular Technology
Conference, vol. 5.
IEEE, 2006, pp. 2173–2177.

[11] M. I. Jordan, “Serial order: A parallel distributed processing approach,”
in Advances in psychology. Elsevier, 1997, vol. 121, pp. 471–495.
[12] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural

computation, vol. 9, no. 8, pp. 1735–1780, 1997.

[13] O. Triebe, H. Hewamalage, P. Pilyugina, N. Laptev, C. Bergmeir, and
R. Rajagopal, “NeuralProphet: Explainable forecasting at scale,” arXiv
preprint arXiv:2111.15397, 2021.

[14] A. C. Harvey and N. Shephard, Structural time series models, (edited
by G.S. Maddala, C.R. Rao and H.D. Vinod) ed. Amsterdam: North
Holland, 1993, vol. Vol. 11: Econometrics, pp. 261–302.

[15] O. Triebe, N. Laptev,

“AR-Net: A sim-
ple auto-regressive neural network for time-series,” arXiv preprint
arXiv:1911.12436, 2019.

and R. Rajagopal,

Fig. 5. Comparison of different models on three different tracks, where NMSE
results are independent with respect to track number as each track has different
channel strength.

TABLE II
PERFORMANCE COMPARISON ON DIFFERENT TRACKS AND CPS

Track Number

1

6

12

NMSE
Model
0.006
Hybrid
BiLSTM 0.018
0.030
RNN
0.076
NP
0.003
Hybrid
BiLSTM 0.008
0.009
RNN
0.064
NP
0.011
Hybrid
BiLSTM 0.016
0.024
0.060

RNN
NP

Cosine Similarity
0.997
0.995
0.989
0.978
0.999
0.998
0.996
0.986
0.995
0.994
0.991
0.989

the performance of RNN, BiLSTM, NP, and proposed hy-
brid framework comprising RNN and NP. We learned from
numerical results, which are obtained by exploiting real-time
channel measurements, that the proposed hybrid framework
outperforms standalone RNN, and NP. Furthermore, results
demonstrate the ability of hybrid framework to overcome the
performance of advanced DL model, i.e., BiLSTM.

In the future, we will evaluate the performance, both from

02004006008001000TimeIndex[0.5ms]-0.02-0.0100.010.020.030.04EnvelopeTruePredicted02004006008001000TimeIndex[0.5ms]-0.0100.010.020.030.04EnvelopeTruePredicted02004006008001000TimeIndex[0.5ms]-0.0100.010.020.030.04EnvelopeTruePredicted02004006008001000TimeIndex[0.5ms]-0.0100.010.020.030.04EnvelopeTruePredicted1612TrackNumber00.020.040.060.08NMSEHybridBiLSTMRNNNP