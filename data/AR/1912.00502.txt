Draft version January 24, 2020
Typeset using LATEX default style in AASTeX63

0
2
0
2

n
a
J

3
2

]

R
S
.
h
p
-
o
r
t
s
a
[

2
v
2
0
5
0
0
.
2
1
9
1
:
v
i
X
r
a

Predicting solar ﬂares with machine learning: investigating solar cycle dependence

Xiantong Wang,1 Yang Chen,2 Gabor Toth,1 Ward B. Manchester,1 Tamas I. Gombosi,1 Alfred O. Hero,3
Zhenbang Jiao,2 Hu Sun,2 Meng Jin,4, 5 and Yang Liu6

1Department of Climate and Space Sciences and Engineering, University of Michigan
Ann Arbor, MI, USA
2Department of Statistics, University of Michigan
Ann Arbor, MI, USA
3Department of Electrical Engineering and Computer Science, University of Michigan
Ann Arbor, MI, USA
4Lockheed Martin Solar and Astrophysics Laboratory, Palo Alto, California, USA
5SETI Institute, Mountain View, CA 94043, USA
6Hansen Experimental Physics Laboratory, Stanford University, Stanford, CA 94305, USA

ABSTRACT

A deep learning network, Long-Short Term Memory (LSTM), is used to predict whether an active
region (AR) will produce a ﬂare of class Γ in the next 24 hours. We consider Γ being ≥ M (strong
ﬂare), ≥ C (medium ﬂare) and ≥ A (any ﬂare) class. The essence of using LSTM, which is a recurrent
neural network, is its capability to capture temporal information of the data samples. The input
features are time sequences of 20 magnetic parameters from the Space-weather HMI Active Region
Patches (SHARPs). We analyze active regions from June 2010 to Dec 2018 and their associated ﬂares
identiﬁed in the (Geostationary Operational Environmental Satellite) GOES X-ray ﬂare catalogs. Our
results (i) produce skill scores consistent with recently published results using LSTMs and are better
than the previous results using single time input. (ii) The skill scores from the model show statistically
signiﬁcant variation when diﬀerent years of data are chosen for training and testing. In particular, the
years 2015 to 2018 have better True Skill Statistic (TSS) and Heidke Skill Scores (HSS) for predicting
≥ C medium ﬂares than the years 2011 to 2014 when the diﬀerence in ﬂare occurrence rates is properly
taken into account.

Keywords: magnetic ﬁelds — methods: statistical — Sun: activity — Sun: chromosphere — Sun: ﬂare

1. INTRODUCTION

Solar ﬂares are energetic eruptions of electromagnetic radiation from the Sun lasting from minutes to hours. The
terrestrial impact of small ﬂares is limited, but strong ﬂares have a signiﬁcant on the upper atmosphere. Increased
ionization aﬀects the total electron content, which in turn aﬀects radio wave propagation and global positioning system
(GPS) accuracy. Ionospheric heating causes the atmosphere to expand, increasing the mass density and increasing
drag on satellites altering their orbits. Strong ﬂare are also often accompanied with coronal mass ejections (CMEs)
that can cause substantial impact on the Earth environment. Therefore, it is very worthwhile to improve the prediction
of solar ﬂares, especially larger ones. During solar cycle 24, nearly 800 M or X ﬂares were observed. While posing a
signiﬁcant threat, the rareness of extreme events and the complexity of the ﬂares makes solar ﬂare time and intensity
predictions a very challenging task.

Although the triggering mechanism of solar ﬂares and the factors determining the solar ﬂare strength are far from
being well understood, it is shown by multiple studies that solar ﬂares are caused by the sudden release of free energy
brought by magnetic reconnection in the coronal ﬁeld. What has come to be know as the standard model for ﬂares

Corresponding author: Xiantong Wang
xtwang@umich.edu

 
 
 
 
 
 
2

Wang et al.

and CMEs (Carmichael 1964; Sturrock 1966; Hirayama 1974; Kopp & Pneuman 1976) (also called the CSHKP model),
involves the rise of sheared core or ﬂux rope that results in magnetic reconnection in the surrounding arcade structure.
Several variations of this model have been developed, which incorporate diﬀerent initiation mechanisms (Masuda et al.
1994; Forbes & Acton 1996; Manchester 2003; T¨or¨ok et al. 2004). A number of review papers summarize these works
and many others (Green et al. 2018).

Since the photospheric magnetic ﬁeld drives the coronal ﬁeld, it is possible that the evolution patterns of the
photospheric magnetic ﬁeld may serve as indicators of the triggering process of ﬂares and CMEs. Those features
include the size of the active regions (AR), the integrated magnetic ﬂux, the integrated current helicity, the magnetic
ﬁeld gradient measurements, the shear angle of the magnetic ﬁeld structure and so on. The Helioseismic and Magnetic
Imager (HMI) on the Solar Dynamics Observatory (SDO) satellite, launched a decade ago, has been providing high-
cadence high-resolution photospheric vector magnetic ﬁeld observations starting from 2010. The Space-weather HMI
Active Region Patches, a.k.a. SHARPs (Bobra et al. 2014), contain time series data localized to individual active
regions (ARs) with many pre-calculated quantities based on the AR magnetic. We will use these SHARP quantities
to train our machine learning model.

Machine learning, a sub-ﬁeld of artiﬁcial intelligence, utilizes past data as a ”learning context” for the computer
program that allow it to make predictions of the future state of the system. The advent of the Solar Heliophysics
Observatory (SOHO)/Michelson Doppler Imager (MDI) and SDO/HMI missions provided suﬃcient data for machine
learning algorithms to predict solar activities. At ﬁrst, the line-of-sight (LOS) component of the photospheric mag-
netic ﬁeld measured by the MDI instrument was used by several groups to forecast solar ﬂares using machine learning
algorithms (Ahmed et al. (2013), Huang et al. (2018), Song et al. (2009), Yu et al. (2009), Yuan et al. (2010)). The
support vector machine (SVM) algorithm was used by Boucheron et al. (2015) for a classiﬁcation task on time series of
MDI data from 2000 to 2010. However, the LOS magnetic ﬁeld component does not include all the magnetic ﬁeld infor-
mation, so later studies used the vector magnetic ﬁeld data once it became available from the HMI instrument. Bobra
& Couvidat (2015) used the SVM trained with SHARP parameters for active region classiﬁcation tasks. Nishizuka
et al. (2018) built a residual deep neural network using not only the parameterized photospheric magnetograms but
also using chromospheric images. Jonas et al. (2018) used observations from photosphere, chromosphere, transition
region, and corona as input of the machine learning algorithm, which gave a comparable result to the works done by
Bobra & Couvidat (2015) and Nishizuka et al. (2018).

The machine learning algorithms in these studies mentioned so far do not fully utilize the time dependence of the
input. Among various kinds of machine learning algorithms, recurrent neural networks (RNNs) are suitable to analyze
time series input. Long-short term memory networks (LSTMs) (Hochreiter & Schmidhuber 1997; Gers et al. 1999),
a particular kind of RNNs, have succeeded in many sequence classiﬁcation and prediction tasks, including speech
recognition, time-series forecasting, handwriting recognition and so on (Hastie et al. 2005; Graves et al. 2013). Most
recently, Liu et al. (2019), Chen et al. (2019) and Jiao et al. (2019) used LSTMs on SHARP parameters, which achieved
better performance for predicting solar ﬂares compared to previous works. Sun et al. (2019) identiﬁes key signals for
strong ﬂares from SHARP parameters using time-series clustering on LSTMs predictions.

In this paper, we apply the LSTM algorithm on the SHARP parameters from SDO/HMI vector magnetic ﬁeld to
predict the maximum solar ﬂare class produced by an active region in the next 24 hours. The inputs are 48-hour time
series of SHARP parameters with 12-minute cadence. The observations of ARs are time sequences, hence LSTMs are
suitable for this kind of input. First, our results show consistency with recently published work by Liu et al. (2019)
that also uses the LSTM algorithm. Second, we also ﬁnd that the skill scores vary substantially when using diﬀerent
years of ARs in the training and testing set. This indicates that data samples should be carefully chosen for the model
evaluation. It is also of interest to understand in what respect these years diﬀer in the solar cycle from each other that
may make the solar ﬂare prediction less or more successful.

The rest of this paper is organized as follows. Section 2 describes how we collect data and build the training and
testing sets. Section 3 describes the LSTM architecture we are using in this work. Section 4 explains the metrics
used to evaluate the model performance. Section 5 shows the results of this study and compares them with previous
work. The solar cycle dependence of the prediction skills are also presented in this section. Section 6 describes our
conclusions.

2. DETAILS OF THE DATA PREPARATION

2.1. Dataset

Solar Flare Prediction with Machine Learning

3

Table 1. Number of active regions and
ﬂares of diﬀerent classes observed each
year.

Year ARs A B

C

M X

2011
2012
2013
2014
2015
2016
2017
2018

168
168
183
194
143
109
52
21

1
0
0
0
0
0
0
5

665
475
469
184
446
757
620
255

1002
1115
1197
1627
1274
294
229
12

106
124
97
194
128
15
37
0

9
7
12
16
2
0
4
0

We use SHARP summary parameters as the input data of the prediction model. The Space-weather HMI Active
Region Patches (SHARPs) is a data product derived from vector magnetograms taken from the Helioseismic and
Magnetic Imager (HMI) onboard the Solar Dynamics Observatory (SDO) (Bobra et al. 2014). The summary param-
eters are calculated based on the HMI Active Region Patches (HARPs), which are rectangular boxes surrounding the
active regions that are moving with the solar rotation and the evolution of the active regions. Table 2 lists the 20 key
parameters used in this work. The SHARP summary parameters are downloaded for all active regions from the Joint
Science Operations Center (jsoc.stanford.edu) from 2011 to 2018. The solar ﬂare events are identiﬁed from the
NOAA Geostationary Operational Environmental Satellites (GOES) ﬂare list (Garcia 1994). In the GOES ﬂare list,
ﬂare events are listed with class, start, end, and peak intensity times of each event. The peak time of the ﬂare events
are assigned as the ”event time” when constructing the data samples. The number of active regions and ﬂare events
in diﬀerent years are summarized in Table 1. Note that C ﬂares outnumber the A and B ﬂares suggesting that most
of the A and B ﬂares are missed when their relatively weak signal falls below the X-ray background.

The input of our model are the time sequences of the SHARP summary parameters. To guarantee the quality of the
data, some time sequences are dropped, especially when the active regions are at the limb. The criteria for dropping
unqualiﬁed time sequences are as follows:

1. In order to avoid projection eﬀects, the longitude of the HARP region center is within the range of ±68◦ from

the central meridian,

2. The fraction of missing frames in a time sequence has to be less than 5%,

3. Two time sequences are separated by one hour.

The target value (or label) of each sequence is the maximum ﬂare class produced by the active region in the next 24h
after the end time of the sequence. The NOAA active region number is used to match the HARP and AR numbers
in the GOES ﬂare list. However, while GOES ﬂares are identiﬁed strictly with NOAA ARs, we note that a single AR
may be split among multiple HARPs or that a HARP may contain multiple ARs. Consequently, we ﬁnd that 20% of
HARPs have this mismatch issue, which may lead to a potential error when we assign the maximum ﬂare classes to
the time sequences for ﬂares may be missed or improperly attributed to the HARP.

Because the various SHARP features have diﬀerent scales and units, the original data samples are normalized before
i denote the normalized value of the ith feature in the nth data sample,

input into the machine learning model: let zn
then

where vn
i
the feature i calculated from the entire dataset, respectively.

is the original value of feature i in data sample n, while µi and σi are the mean and standard deviation of

zn
i =

vn
i − µi
σi

,

(1)

4

Wang et al.

Table 2. List of SHARP parameters and brief descriptions

Parameter

Description

Total unsigned ﬂux in Maxwells

Total unsigned vertical current, in Amperes
Total twist parameter, alpha, in 1/Mm

USFLUX
MEANGAM Mean inclination angle, gamma, in degrees
MEANGBT Mean value of the total ﬁeld gradient, in Gauss/Mm
MEANGBZ Mean value of the vertical ﬁeld gradient, in Gauss/Mm
MEANGBH Mean value of the horizontal ﬁeld gradient, in Gauss/Mm
MEANJZD Mean vertical current density, in mA/m2
TOTUSJZ
MEANALP
MEANJZH Mean current helicity in G2/m
TOTUSJH
ABSNJZH
SAVNCPP
MEANPOT Mean photospheric excess magnetic energy density in ergs per cubic centimeter
TOTPOT
MEANSHR Mean shear angle (measured using Btotal) in degrees
SHRGT45
SIZE
SIZE ACR
NACR
NPIX

Percentage of pixels with a mean shear angle greater than 45 degrees in percent
Projected area of patch on image in micro-hemisphere
Projected area of active pixels on image in micro-hemisphere
Number of active pixels in patch
Number of pixels within the patch

Total unsigned current helicity in G2/m
Absolute value of the net current helicity in G2/m
Sum of the Absolute Value of the Net Currents Per Polarity in Amperes

Total photospheric magnetic energy density in ergs per cubic centimeter

2.2. Training/Testing splitting

In order to assess the performance of the machine learning algorithms properly, we need to split the samples (time
sequences of SHARP summary parameters and corresponding maximum ﬂare classes) into a training set and a testing
set. The training set is used for training the machine learning model while the testing set is for assessing the prediction
capability of the model. In the training process, the model learns from the input data and adjusts its parameters to ﬁt
the ground truth. Both variable selection and parameter estimation are included in this process. The samples in the
testing set should be totally separated from the training set, otherwise there will be an artiﬁcial gain of performance
since the information in the training set is leaked to the testing set (Kaufman et al. 2012; Schutt & O’Neil 2013).
Hence, separating the samples based on active regions is necessary to guarantee that sequences from one active region
will not occur in both training and testing sets simultaneously. All the training/testing splitting in this paper are
conducted based on HARPs.

3. ARCHITECTURE OF MACHINE LEARNING MODEL

The Recurrent Neural Network (RNN) is a category of neural networks which can make use of sequential information
(Pearlmutter 1989). This architecture is naturally used in solar ﬂare prediction since the active regions evolve with time
and the occurrence of the solar ﬂares is most likely related to the time-dependent evolution of active regions. RNNs
are called recurrent because they perform the same task for every input from the sequence, but the output depends on
the previous computations. Among various RNN structures, the Long Short Term Memory (LSTM) network is one
of the most commonly used type of RNNs (Hochreiter & Schmidhuber 1997). LSTM networks are explicitly designed
to avoid the long-term dependency problem, which is a major shortcoming for simpler RNNs. The key to LSTMs is
a new cell state variable in the network, which is passed through the whole chain with only minor linear interactions.
This allows the information at a much earlier time to eﬀect the results, which mimics a ‘long-term’ memory.

The structure of the LSTM unit and the LSTM network used in this work is shown in Figure 1. The left panel
shows a single LSTM unit. Each unit takes an input vector x<t> consisting of the input features at a certain time
1> are from the previous LSTM unit. The right panel shows
point t, the hidden state h<t

1> and the cell state c<t

−

−

Solar Flare Prediction with Machine Learning

5

Figure 1. The detailed structure of the LSTM cell (left) and the LSTM network (right).

the structure of the two-layer LSTM network. The LSTM units in the second layer are the same as in the ﬁrst layer,
but their input vectors x<t>
from the ﬁrst layer LSTM units. The relationships between
the unit input, output and internal states are given by the following equations:

are the output vectors o<t>

2

1

1> + bhi)
i<t> = σ(Wiix<t> + bii + Whih<t
−
1> + bhf )
f <t> = σ(Wif x<t> + bif + Whf h<t
−
g<t> = tanh(Wigx<t> + big + Whgh<t
o<t> = σ(Wiox<t> + bio + Whoh<t
−
1> + i<t> ∗ g<t>
c<t> = f <t> ∗ c<t
h<t> = o<t> ∗ tanh(c<t>)

1> + bho)

−

1> + bhg)
−

(2a)

(2b)

(2c)

(2d)

(2e)

(2f)

Here x<t> ∈ Rd is the input vector to the LSTM unit and d is the number of features. i<t>, f <t> and o<t> ∈ Rh
are the activation vectors of the input gates, forget gates and output gates, respectively. g<t> ∈ Rh is an activation
vector from the tanh function. h is the hidden dimension of the LSTM unit, which is a hyperparameter in the model
that reﬂects the model complexity and we use 16 in this work. c<t> ∈ Rh is the cell state vector and there is only
linear relationship between the output and input cell states in a single LSTM unit. The cell state and hidden state
vectors are passed to the next LSTM unit in the same layer. The output vectors in the ﬁrst layer are taken as input in
the second layer. The output vector of the last LSTM unit in the second layer is multiplied by a h vector and passed
to a sigmoid function to generate the ﬁnal prediction value. The tanh and the sigmoid function

1
x
1 + e−
introduce the non-linearity into the neural network. The Wi ∈ Rh
d weight matrices are applied to the input vectors
×
and Wh ∈ Rh
h are applied to the gate activation vectors. b ∈ Rh in the equation are the bias vectors. The weight
matrices and bias vectors are trainable parameters, which are determined during the training process.

σ(x) =

(3)

×

The ”training” in the machine learning is essentially an optimization process for an objective function, also known
as the loss function. The loss function measures the diﬀerence between the model prediction and the ground truth.
An optimization algorithm is used to minimize the loss function so that the trainable parameters in the model can
”encode” some knowledge from the data samples. The prediction can ﬁnally be reduced to a binary classiﬁcation task:
according to the given input, will this AR produce a ﬂare of class Γ in the next 24 hours. The model will generate a
prediction score in the last layer in the network and if this score is larger than a threshold, then the model will make a
positive prediction. In our work, the last layer is a sigmoid function and the output from a sigmoid function is either
close to 0 or 1, so the threshold for binary classiﬁcation is set to be 0.5. The ”Binary Cross Entropy” is typically used
as the loss function for binary classiﬁcation problems. However, this loss function can fail if one category of samples
is dominating the entire data set. Constantly predicting the dominant category in the testing set can result in a small
value of the loss function but the model has no predictive skill in this case. Large energetic solar ﬂares are extremely

σσ(cid:104)(cid:28)(cid:77)(cid:63)σ×(cid:89)××(cid:104)(cid:28)(cid:77)(cid:63)f!t"i!t"g!t"c!t−1"(cid:42)(cid:50)(cid:72)(cid:72)h!t−1"(cid:62)(cid:66)(cid:47)(cid:47)(cid:50)(cid:77)x!t"(cid:65)(cid:77)(cid:84)(cid:109)(cid:105)c!t"h!t"o!t"(cid:71)(cid:97)(cid:104)(cid:74)(cid:82)(cid:71)(cid:97)(cid:104)(cid:74)(cid:107)(cid:71)(cid:97)(cid:104)(cid:74)(cid:82)(cid:71)(cid:97)(cid:104)(cid:74)(cid:107)(cid:71)(cid:97)(cid:104)(cid:74)(cid:82)(cid:71)(cid:97)(cid:104)(cid:74)(cid:107)(cid:71)(cid:97)(cid:104)(cid:74)(cid:82)(cid:71)(cid:97)(cid:104)(cid:74)(cid:107)!(cid:83)(cid:96)(cid:50)(cid:47)(cid:66)(cid:43)(cid:105)(cid:66)(cid:81)(cid:77)(cid:199)(cid:199)······(cid:97)(cid:72)(cid:66)(cid:47)(cid:66)(cid:77)(cid:59)(cid:104)(cid:66)(cid:75)(cid:50)(cid:113)(cid:66)(cid:77)(cid:47)(cid:81)(cid:114)(cid:104)(cid:66)(cid:75)(cid:50)(cid:108)(cid:97)(cid:54)(cid:71)(cid:108)(cid:115)(cid:45)(cid:74)(cid:49)(cid:27)(cid:76)(cid:58)(cid:27)(cid:74)(cid:45)(cid:74)(cid:49)(cid:27)(cid:76)(cid:58)(cid:34)(cid:104)(cid:45)(cid:74)(cid:49)(cid:27)(cid:76)(cid:58)(cid:34)(cid:119)(cid:45)(cid:275)6

Wang et al.

rare events so that the dataset we are using is highly unbalanced. To solve this issue, we used ”Binary Cross Entropy
with Logits Loss” in this work which is deﬁned as:

N

−[pcyn log σ(ˆyn) + (1 − yn) log(1 − σ(ˆyn))]

(4)

L =

1
N

n=1
(cid:88)

Here N is the number of samples in the training or testing set, yn is the target value and ˆyn is the model output.
The coeﬃcient pc and the use of the sigmoid function distinguish this loss function relative to the simple binary cross
entropy loss function. The sigmoid function improves the numerical stability in the optimization process, while pc is
set to the ratio of negative and positive samples to balance the contributions of the two terms in the sum. In this
work, this value is calculated from the training set.

4. MODEL EVALUATION

The four quantities TN, TP, FN, and FP refer to the number of True Negative, True Positive, False Negative and
False Positive predictions, respectively. These four numbers can be combined to calculate the Precision, the Recall
(also known as Probability of Detection, POD), the False Alarm Rate (FAR), the True Skill Statistic (TSS), the Heidke
Skill Score (HSS), and the Accuracy (ACC) deﬁned as

Precision =

TP
TP + FP

POD = Recall =

TP
TP + FN

FAR =

TSS =

HSS =

FP
FP + TN
TP
TP + FN

−

FP
FP + TN
2(TP · TN − FP · FN)
(TP + FN)(FN + TN) + (TP + FP)(FP + TN)

= POD − FAR

ACC =

TP + TN
TP + FP + FN + TN

(5a)

(5b)

(5c)

(5d)

(5e)

(5f)

(5g)

We will use these quantities to evaluate the model performance. The Precision and POD evaluate the model’s ability
to identify positive events (1 being perfect, 0 being worst), while FAR tests the model for correctly identifying negative
events (0 being perfect, 1 being worst). The Accuracy, TSS and HSS evaluate the overall skill with the maximum
value 1 being the perfect score. As we will see, the various skill scores have very diﬀerent dependence on the fraction
of positive and negative events in the training and testing sets. For an imbalanced dataset, the Accuracy becomes less
meaningful because the model’s output will be dominated by the majority of the dataset. Artiﬁcial inﬂation will be
caused to the POD (or the FAR) if the model is assigning all testing samples to be positive (or negative). However,
in both cases, the model will not have any useful prediction skills. TSS approaches the POD when the forecasting is
dominated by correct forecasts of non-occurrence which is the case for solar ﬂare events. A high TSS value therefore
may not really mean that the prediction is reliable, as there can be many false alarms relative to the number of true
predictions. HSS is superior to the TSS in this situation, because it produces 0 value for a model that predicts a random
number with the correct occurrence rate, and positive HSS means that the model is better than that. However, HSS is
sensitive to the ratio of positive versus negative events, which means that the same model can produce very diﬀerent
HSS values depending on the selected data set (for example solar maximum versus solar minimum). In other words,
HSS can be scaled up if there are more positive samples in the testing set (Doswell III et al. 1990). Hence there is no
single skill score that can properly evaluate the forecasting performance, and one needs to be careful when models are
compared. There are only four independent values (TN, TP, FN, and FP) and only their three ratios truly matter.
This means that any three independent values deﬁned above can be used. In practice, we concentrate on the POD,
FAR and HSS values, as these provide complete and intuitive information about the model’s performance. The TSS,
while useful, is not an independent skill score, as it is simply the diﬀerence of POD and FAR.

Solar Flare Prediction with Machine Learning

7

5. RESULTS

5.1. Training Process

The LSTM network is implemented in Python with the PyTorch package. PyTorch is originally a tensor calculation
package for GPU and the auto-gradient feature (Paszke et al. 2017) makes it suitable for machine learning tasks. A
minibatch strategy (Li et al. 2014; Bottou et al. 2018) is used for faster convergence during back-propagation. The
Adam optimizer (Kingma & Ba 2014)is used with the learning rate set to 0.001 and the other parameters are β1 = 0.9
and β2 = 0.999. The batch size is 1000. The model is trained for multiple epochs on the training set. In each epoch,
the model goes through the training samples once. The model is trained for 6 epochs to generate the results presented
in Table 3, Figure 3 and Figure 4. We found no statistically signiﬁcant improvement in the performance after six
epochs. To account for the randomness due to the order of training samples and the initial values of the trainable
parameters, we perform 20 independent runs with diﬀerent random seeds to get evaluate robustness.

5.2. Skill scores for solar ﬂare prediction

The mean values of the skill scores for the 20 runs are reported in Table 3 together with results obtained by earlier
work for comparison. From Table 3, the skill scores for predicting ≥ C ﬂares are larger than those predicting ≥ M
ﬂares in all models. This is expected, because there are many more C ﬂares than M and X ﬂares, which helps the
machine learning algorithm to get better. However, the skill scores for predicting any ﬂares (≥ A) are also smaller
than those predicting ≥ C ﬂares. The reason for this reduced performance is that A and B ﬂares are not properly
observed. The number of ﬂares in diﬀerent energy classes roughly obey a power-law distribution (see Lu & Hamilton
(1991) and Figure 8), thus a large number of class B ﬂares are missing in the GOES ﬂare records (see Figure 2).
The X-ray emission of many B ﬂares can fall below the background emission level once an active region heats up,
which causes those B ﬂares to be unrecorded. Therefore, we are training and testing the model with mislabeled data
samples for predicting any class of ﬂares, hence many weak ﬂares that the model predicts probably are classiﬁed as
false positives, which lowers the skill scores.

Figure 2. The distribution of number of recorded ﬂares for classes B, C, M, X from 2011 to 2018 in the GOES data set. Note
that the number of B ﬂares is much smaller than the number of C ﬂares.

5.3. Comparison with previous results

In this subsection, we will compare the results of this paper to previously published works. In the past decade, there
have been several works that applied machine learning based models to predict solar ﬂares. Those models including
MLP (Multilayer perceptrons) (Florios et al. 2018), SVM (Support Vector Machine) (Qahwaji & Colak 2007; Yuan
et al. 2010; Bobra & Couvidat 2015; Boucheron et al. 2015; Muranushi et al. 2015; Florios et al. 2018), DeFN (Deep
Flare Net) (Nishizuka et al. 2018) and a recently published work (Liu et al. 2019) which also used LSTM network.

The skill scores for diﬀerent models are shown in Table 3. Even though the input and the testing samples may be
diﬀerent in those works using MLP, SVM and DeFN models, the results are representative of the best performance those
models can achieve on the solar ﬂare prediction task. The results from the LSTM models outperform the MLP, SVM

BCMXFlare Class0200040006000Number of Records44576840710508

Wang et al.

and DeFN models substantially, which indicates that taking time series data into account can substantially improve
solar ﬂare forecasting. As discussed in subsection 5.4, we should use similar testing sets when ciging the various models
since diﬀerent train/test splitting can produce diﬀerent skill scores. The notations for LSTM models are deﬁned as
follows: LSTM-15 18 and LSTM-2015 are the models used in this paper. LSTM-15 18 uses ARs from year 2015 to
year 2018 for testing and LSTM-2015 uses ARs in year 2015 for testing. LSTM-Liu uses the model reported in Liu
et al. (2019) which uses ARs in year 2015 for testing. Among those evaluation metrics, 1. Accuracy (ACC) is the least
useful since the data set is highly biased (ﬂare events are rare so the majority samples in the testing set are negative),
predicting those negative samples correctly leads to a high accuracy, however not much predictability for strong ﬂare
events may actually be achieved. 2. Precision and POD reﬂect the model’s ability of making positive predictions:
Precision is the fraction of correctly predicted samples among all predicted positive samples. POD is the fraction of
correctly predicted positives among the actual positive samples in the testing set. Therefore Precision provides more
useful information about predictability of rare events, while POD by itself is not representative of the predictive skills.
From Table 3, our model produces Precision scores within the same range as Liu et al. (2019), and is better than the
previous works listed. 3. HSS and TSS are often considered as good metrics for evaluating model predictability in
binary classiﬁcation tasks. However, TSS beneﬁts from the large number of correctly predicted negative samples (TN)
so that it is much higher than the HSS in all tests. HSS lessens the importance of true negatives (TN) but it is more
sensitive to the fraction of positive samples in the testing set: the value of HSS will be higher if there are more positive
samples in the testing set (which can be artiﬁcially achieved by creating a testing set that contains larger fraction of
positive samples than the actual data). Our model gives better HSS than previous models (MLP, SVM and DeFN)
that do not use time sequences and also has similar performance as the recently published results (Liu et al. 2019)
using LSTM, which validates the correctness of this work. 4. For rare events, small False Alarm Rates (FAR) are
highly indicative of good prediction skills. The FAR for predicting ≥ C and ≥ M ﬂares are all less than 0.1, which
means that more than 90% negative predictions from our model are correct. This contributes greatly to the high HSS
values.

The details of our and the Liu et al. (2019) LSTM models are diﬀerent but they obtain similar skill scores according
to Table 3. This suggests that both LSTM models extract most of the useful information from the SHARP parameters
and further improvement will require using more information from the observation.

5.4. Choosing diﬀerent testing years

As described before in section 2.2, it is important to totally separate the training and testing samples. However,
whether choosing diﬀerent years of ﬂares for testing can have diﬀerent skill scores is still unclear since the previous
works all used data before 2015 for training and after 2015 for testing. In this subsection, we conduct the training-
testing process on diﬀerent combinations of training and testing years. In Figure 3, we present the box plots of skill
scores for twenty independent runs with the testing samples being one of the years from 2011 to 2015, and the other
four years are used for training. (As shown by Table 1, there were very few large ﬂares from 2016 to 2018, so those
years are not suitable for testing and would not contribute much to training either.)

Figure 3 shows that training on 2011 to 2014 and testing on 2015 gives the best HSS and FAR scores for predicting
both ≥ C and ≥ M ﬂares. The trend is diﬀerent for the TSS for predicting ≥ M ﬂares, because TSS is dominated by
the POD values, but this does not mean truly good prediction for rare events, such as ≥ M ﬂares. Good prediction of
rare events requires very few false alarms and this will produce high HSS. Apparently, the model is quite ”restrained”
on making positive predictions for the year 2015 data, which improves its FAR and HSS scores. It is clear from Figure
3 that evaluating the model performance on diﬀerent years will introduce signiﬁcant diﬀerences in the results.

To investigate why the model produces fewer false alarms on year 2015 than other years, we set up two linear
regression models as the baseline. These two baseline models use the same training and testing samples as the LSTM
model. The time sequences of SHARP parameters are reshaped to one-dimensional vectors as the input of the ﬁrst
linear regression model, denoted by ”Linear Regression A”. For the second linear regression model, denoted by ”Linear
Regression B”, the mean values of the time sequences are taken as input. Twenty independent runs are conducted and
the mean values of the skill scores from the LSTM model and two baseline models are shown in Figure 4. The diﬀerence
between the LSTM model and the Linear Regression A is the non-linearity introduced by the LSTM network. The
Linear Regression B eliminates the time sequence information and only inputs the average level of activity into the
model. From Figure 4: 1. For predicting ≥ M ﬂares, the LSTM gives the best HSS, followed by Linear Regression
models A and B. For predicting ≥ C ﬂares, the LSTM model has similar HSS as the Linear Regression A model and

Solar Flare Prediction with Machine Learning

9

Table 3. Comparison of skill scores for diﬀerent models. Lines
with * are results from this work. LSTM-2015 uses the same time
period ase DeFN while LSTM-15 18 uses the same time period as
LSTM-Liu for the testing data set.

Metric

POD

*
*
Precision

*
*
ACC

*
*
HSS

*
*
TSS

*
*
FAR

*
*

Model

≥ M class ≥ C class Any Class

MLP
SVM
DeFN
LSTM-Liu
LSTM-2015
LSTM-15 18
MLP
SVM
DeFN
LSTM-Liu
LSTM-2015
LSTM-15 18
MLP
SVM
DeFN
LSTM-Liu
LSTM-2015
LSTM-15 18
MLP
SVM
DeFN
LSTM-Liu
LSTM-2015
LSTM-15 18
MLP
SVM
DeFN
LSTM-Liu
LSTM-2015
LSTM-15 18
MLP
SVM
DeFN
LSTM-Liu
LSTM-2015
LSTM-15 18

0.812
0.692
0.891
0.881
0.685
0.730
0.143
0.106
0.173
0.222
0.311
0.282
0.855
0.824
0.872
0.909
0.929
0.945
0.204
0.141
0.253
0.347
0.394
0.382
0.669
0.520
0.763
0.790
0.623
0.681
0.143
0.172
0.128
0.091
0.062
0.049

0.637
0.746
0.761
0.762
0.643
0.621
0.451
0.497
0.497
0.544
0.677
0.635
0.778
0.803
0.801
0.829
0.858
0.883
0.389
0.472
0.476
0.539
0.567
0.557
0.449
0.562
0.572
0.607
0.559
0.553
0.188
0.184
0.189
0.155
0.084
0.068

-
-
-
-
0.625
0.530
-
-
-
-
0.670
0.702
-
-
-
-
0.814
0.800
-
-
-
-
0.519
0.473
-
-
-
-
0.509
0.439
-
-
-
-
0.116
0.092

both are better than the Linear Regression B model. This illustrates the importance of the time sequence information.
2. The linear regression models give larger POD and FAR than the LSTM. Therefore, the LSTM model has less
tendency to make positive predictions, which results in better HSS. The optimal case is when the model produces
high POD while also keeping a low FAR, which is not the case for an LSTM. This is the reason why an LSTM cannot
provide a high TSS compared to Linear Regression models.

10

Wang et al.

Figure 3. The box plots of four skill scores for diﬀerent training and testing year choices in the 2011−2015 data set. Note that
smaller FAR means better performance. The left and right columns show results for predicting ﬂares of class ≥ M and ≥ C
ﬂares, respectively. The yellow line is the median and the green triangle is the mean of the 20 independent training runs. The
lower and upper bounds of the boxes correspond to the quartiles, while the red stars are the data points outside the range of
median ±2.698σ. The mean value and median are calculated including the outliers.

All three models give the lowest FAR when testing on year 2015 and for other testing years the thee models show
similar trends. This indicates that the reason for the LSTM producing low FAR is related to the mean values of the
SHARP parameters, which is the information used by the simplest Linear Regression B model. In conclusion, the
LSTM model produces more reliable positive predictions than the simple linear regression models although it will miss
some positive events. The model gives diﬀerent results when being tested/trained on diﬀerent years of data, apparently
due to diﬀerences in the average SHARP parameters.

5.5. Training/Testing on diﬀerent solar cycle phases

According to subsection 5.4, training and testing on diﬀerent years leads to very diﬀerent results. One possible
reason for the diﬀerence could be the changes in the data processing procedure. However, from Hoeksema et al.
(2018), although the data processing techniques were modiﬁed in January, 2015, those changes will not have major
eﬀects on the data products used in this work.

To investigate if there are any intrinsic diﬀerences of the active regions from each year in the solar cycle, we ﬁrst do
the training and testing separately on each year. Since the number of active regions and ﬂares is very small in years
2016 to 2018, those three years are grouped together. Four skill scores are collected to evaluate the model performance:
HSS, TSS, POD and FAR. The process for selecting data samples is:

1. For active regions in each year, randomly select 25% for testing and rest of them for training.

2. Extract 48h time series of SHARP parameters for training and testing from active regions selected in step 1 and

label the time series with the maximum ﬂare class in the next 24 hours from the GOES ﬂare record.

3. Randomly drop negative samples in the training and testing sets to ﬁx the ratio of positive samples and negative

samples to be 0.05 for predicting ≥ M ﬂares and 0.3 for predicting ≥ C ﬂares.

Notice that in step 3, we ﬁx the ratio of positive to negative samples to make the HSS directly comparable across
diﬀerent years. In addition, having a ﬁxed positive/negative sample ratio makes the HSS and TSS behave similarly.

Solar Flare Prediction with Machine Learning

11

Figure 4. The comparison between LSTM model and two baseline linear regression models. Four skill scores are presented.
Left column is the prediction for ≥ M ﬂare and right column is for ≥ C ﬂares. The Linear Regression A takes the whole time
sequence as input (same as LSTM) and the Linear Regression B takes the mean value of the time sequence for each SHARP
parameter.

We perform multiple runs with randomly dropping negative samples, so in fact the runs use diﬀerent data sets. For
each run, the skill scores are the mean values of the model outputs from the third to the tenth epochs. This range of
epochs is chosen based on the typical evolution of skill with epochs: there is an initial rapid improvement, followed
by a plateau with random oscillations, and ﬁnally worsening trends due to overﬁtting. The averaging over multiple
epochs reduces the random variation due to the relatively small data sets. In addition, the mean values better reﬂect
the general performance of the model than picking the best epoch for each run.

The box plots of skill scores for predicting ≥ C ﬂares are shown in Figure 5. Each box contains 100 data points
from 100 runs using randomly selected active regions for training/testing and dropping negative samples in the data
sets. Because there are few active regions and ﬂare events during the three years from 2016 to 2018 (see Table 1), the
skills scores are less centered and the number of outliers is larger than those from other years. The results show that
training and testing on the data after 2015 produces better skill scores than the earlier years. The FAR has the most
substantial diﬀerence for data sets after and before 2015, which is also the major reason for better HSS and TSS since
the POD does not vary much. We are not showing results for predicting ≥ M ﬂares on each year separately because
the ≥ M ﬂares are too rare to give any statistically signiﬁcant results on such small data samples.

The results in Figure 5 show that the prediction model has better performances for the years 2015-2018 than for
2011-2014. To demonstrate this diﬀerence with better statistics, we conduct the training/testing process on two data
sets using these two time periods, and compare the results. The data selecting strategy is the same as what we did for
training/testing on each year but now there are enough M and X ﬂares to produce statistically robust results. The skill
scores for predicting ≥ C and ≥ M ﬂares are shown in Figures 6 and 7, respectively. For predicting ≥ C ﬂares, the
HSS, TSS scores are clearly and signiﬁcantly better for 2015-2018 than for 2011-2014. The box plots of POD overlap
with each other while the box plots of FAR are well separated. This result indicates that for capturing ≥ C ﬂares in
the testing set, the model has similar performance when trained on the data samples from two phases in a solar cycle.
However, the model will produce much fewer false alarms if it is trained on data samples from the declining phase in

12

Wang et al.

Figure 5. The box plots of skill scores for predicting ≥ C ﬂares on diﬀerent years from 2011 to 2018. The symbols in the
ﬁgures are the same with previous box plots.

a solar cycle. For predicting ≥ M ﬂares, according to Figure 7, the diﬀerence of skill scores is less obvious. In general,
the results from 2015-2018 show larger variance because there are fewer ≥ M ﬂares in this time range. The box plots
of HSS, TSS and POD overlap to a large extent except the model gives a higher FAR when trained/tested on year
2015-2018.

There is a signiﬁcant diﬀerence between 2011-2014 and 2015-2018 for predicting ≥ C ﬂares, but not for ≥ M ﬂares.
A simple explanation could be diﬀerent ﬂare intensity distributions in two phases of a solar cycle. For example, if the
frequency of C and M ﬂares are better separated from each other in one of the time periods (i.e. there are relatively
fewer ﬂares with energies near the C/M class boundary) then it will help the model to correctly identify ≥ M ﬂares.
To check this possibility we show the distribution of ﬂare events as a function of energy on a log-log scale for the two
time periods in Figure 8. The histograms are well approximated with straight lines, indicating that the ﬂare intensity
distributions approximately follow power laws (Lu & Hamilton 1991). While the amplitudes are diﬀerent by about a
factor of 3 (there are more ﬂares during solar max than during solar min), the slopes of the two lines are very close to
each other. So there is no obvious diﬀerence in the shape of the ﬂare intensity distributions between the two phases
of a solar cycle. There seems to be some excess of ﬂares near the C/M class boundary for 2015-2018, which may
contribute to the worse performance on predicting ≥ M ﬂares than ≥ C ﬂares for this time period. However, we set
the threshold of positive and negative classes to C8.0. All these excess of ﬂares are labeled as positive. The results
show no diﬀerence in skill scores for two time periods. Thus we conclude that diﬀerence of the skill scores in Figure 6
and 7 cannot be simply explained by diﬀerent ﬂare intensity distributions in the two time periods.

6. CONCLUSION

In this paper, we build a data set covering active regions from 2011 to 2018 from Joint Science Operations Center
(jsoc.stanford.edu). Each data sample is the time sequence of twenty SHARP parameters, which represent the
magnetic ﬁeld properties of an active region. We develop an LSTM network to predict the maximum ﬂare class Γ in
the next 24 hours produced by an active region. The prediction task is reduced to a binary classiﬁcation when Γ is a
combination of classes above a certain threshold. We consider three diﬀerent cases for Γ: ≥ M , ≥ C and ≥ A. The last
case corresponds to predicting any ﬂares. The training/testing splitting is based on active regions, which guarantees
that the model is tested on data samples that it has never seen previously. The skill scores produced by the model
vary substantially for diﬀerent years and we investigate the solar cycle dependence of the model performance. The
main results of this paper are summarized as follows:

1. For evaluating models predicting rare events, such as solar ﬂares, the most relevant skill score is the Heidke Skill
Score (HSS) that is strongly correlated with low false alarm rate (FAR). On the other hand, HSS is sensitive to
the ratio of positive and negative samples in the testing set, which means that comparison of model performance
for diﬀerent data sets requires caution.

2011201220132014201516-180.00.20.40.60.8HSS2011201220132014201516-180.00.20.40.60.8TSS2011201220132014201516-180.00.20.40.60.8POD2011201220132014201516-180.00.10.20.3FARSolar Flare Prediction with Machine Learning

13

Figure 6. The box plots of skill scores for predicting ≥ C ﬂares. The training/testing are conducted within one of two time
periods: 2011-2014 and 2015-2018. The symbols in the ﬁgure are the same as in previous box plots.

Figure 7. The box plots of skill scores for predicting ≥ M ﬂares. The training/testing are conducted within one of two time
periods: 2011-2014 and 2015-2018.. The symbols in the ﬁgure are the same as in previous box plots.

2. The LSTM based model achieves better HSS for predicting solar ﬂares than the previous approaches such as
MLP, SVM and DeFN. Using the time series information improves relevant skills. Our results are also comparable
with the recently published work using a similar LSTM method.

3. Although more than 50% percent of skill scores of LSTM model can be acquired from simple linear regression
models, the non-linearity introduced by LSTM reduces the number of false alarms and improves the prediction
skills of the model.

4. Previous works using active region data after 2015 for testing could introduce bias into the skill scores. If the
model is trained on 2011-2014 and tested on 2015, it produces better skill scores than other combinations of
training and testing years. This appears to be related to the diﬀerence of average level of solar activity in the
training and testing sets.

Based on the results presented in this paper, the LSTM is a valid method for the solar ﬂare prediction task. The skill
scores from this paper are very close to those generated by other diﬀerent LSTM models indicates that the information
contained in the SHARP parameters is limited. In future work, we plan to use more observational information to
further improve the ﬂare prediction skills.

2011-20142015-20180.40.50.60.7HSS2011-20142015-20180.30.40.50.60.7TSS2011-20142015-20180.40.50.60.70.8POD2011-20142015-20180.050.100.150.20FAR2011-20142015-20180.20.30.40.50.60.7HSS2011-20142015-20180.40.60.8TSS2011-20142015-20180.40.60.81.0POD2011-20142015-20180.050.100.15FAR14

Wang et al.

Figure 8. The histogram for C and M ﬂares in two groups of years. The plot is in log-log scale and the histograms are straight
lines which shows the intensity of ﬂares agrees with the power-law distribution.

REFERENCES

Ahmed, O. W., Qahwaji, R., Colak, T., et al. 2013, SoPh,

Hochreiter, S., & Schmidhuber, J. 1997, Neural

283, 157, doi: 10.1007/s11207-011-9896-1

computation, 9, 1735

Bobra, M. G., & Couvidat, S. 2015, ApJ, 798, 135,

Hoeksema, J. T., Baldner, C. S., Bush, R. I., Schou, J., &

doi: 10.1088/0004-637X/798/2/135

Bobra, M. G., Sun, X., Hoeksema, J. T., et al. 2014, SoPh,

Scherrer, P. H. 2018, SoPh, 293, 45,
doi: 10.1007/s11207-018-1259-8

289, 3549, doi: 10.1007/s11207-014-0529-3

Huang, X., Wang, H., Xu, L., et al. 2018, ApJ, 856, 7,

Bottou, L., Curtis, F. E., & Nocedal, J. 2018, Siam Review,

doi: 10.3847/1538-4357/aaae00

60, 223

Jiao, Z., Sun, H., Wang, X., et al. 2019, arXiv preprint

Boucheron, L. E., Al-Ghraibah, A., & McAteer, R. T. J.
2015, ApJ, 812, 51, doi: 10.1088/0004-637X/812/1/51

Carmichael, H. 1964, NASA Special Publication, 50, 451

Chen, Y., Manchester, W. B., Hero, A. O., et al. 2019,

Space Weather, 17, 1404, doi: 10.1029/2019SW002214

Doswell III, C. A., Davies-Jones, R., & Keller, D. L. 1990,

Weather and Forecasting, 5, 576

arXiv:1912.06120

Jonas, E., Bobra, M., Shankar, V., Todd Hoeksema, J., &

Recht, B. 2018, SoPh, 293, 48,
doi: 10.1007/s11207-018-1258-9

Kaufman, S., Rosset, S., Perlich, C., & Stitelman, O. 2012,
ACM Transactions on Knowledge Discovery from Data
(TKDD), 6, 15

Florios, K., Kontogiannis, I., Park, S.-H., et al. 2018, SoPh,

Kingma, D. P., & Ba, J. 2014, arXiv preprint

293, 28, doi: 10.1007/s11207-018-1250-4

arXiv:1412.6980

Forbes, T., & Acton, L. 1996, The Astrophysical Journal,

Kopp, R. A., & Pneuman, G. W. 1976, SoPh, 50, 85,

459, 330

Garcia, H. A. 1994, SoPh, 154, 275,

doi: 10.1007/BF00681100

Gers, F. A., Schmidhuber, J., & Cummins, F. 1999

Graves, A., Mohamed, A.-r., & Hinton, G. 2013, in 2013

IEEE international conference on acoustics, speech and
signal processing, IEEE, 6645–6649

doi: 10.1007/BF00206193

Li, M., Zhang, T., Chen, Y., & Smola, A. J. 2014, in

Proceedings of the 20th ACM SIGKDD international
conference on Knowledge discovery and data mining,
ACM, 661–670

Liu, H., Liu, C., Wang, J. T. L., & Wang, H. 2019, ApJ,

877, 121, doi: 10.3847/1538-4357/ab1b3c

Green, L. M., T¨or¨ok, T., Vrˇsnak, B., Manchester, W., &

Lu, E. T., & Hamilton, R. J. 1991, The astrophysical

Veronig, A. 2018, SSRv, 214, 46,
doi: 10.1007/s11214-017-0462-5

journal, 380, L89

Manchester, W. 2003, Journal of Geophysical Research

Hastie, T., Tibshirani, R., Friedman, J., & Franklin, J.

(Space Physics), 108, 1162, doi: 10.1029/2002JA009252

2005, The Mathematical Intelligencer, 27, 83

Masuda, S., Kosugi, T., Hara, H., Tsuneta, S., & Ogawara,

Hirayama, T. 1974, Solar Physics, 34, 323

Y. 1994, Nature, 371, 495

Solar Flare Prediction with Machine Learning

15

Muranushi, T., Shibayama, T., Muranushi, Y. H., et al.

Song, H., Tan, C., Jing, J., et al. 2009, SoPh, 254, 101,

2015, Space Weather, 13, 778,

doi: 10.1002/2015SW001257

Nishizuka, N., Sugiura, K., Kubo, Y., Den, M., & Ishii, M.

2018, ApJ, 858, 113, doi: 10.3847/1538-4357/aab9a7

doi: 10.1007/s11207-008-9288-3

Sturrock, P. A. 1966, Nature, 211, 695,

doi: 10.1038/211695a0

Sun, H., Manchester, W., Jiao, Z., Wang, X., & Chen, Y.

2019, arXiv preprint arXiv:1912.12360

Paszke, A., Gross, S., Chintala, S., et al. 2017

T¨or¨ok, T., Kliem, B., & Titov, V. S. 2004, A&A, 413, L27,

Pearlmutter, B. A. 1989, Neural Computation, 1, 263

Qahwaji, R., & Colak, T. 2007, SoPh, 241, 195,

doi: 10.1007/s11207-006-0272-5

Schutt, R., & O’Neil, C. 2013, Doing data science: Straight

talk from the frontline (O’Reilly Media, Inc.)

doi: 10.1051/0004-6361:20031691

Yu, D., Huang, X., Wang, H., & Cui, Y. 2009, SoPh, 255,

91, doi: 10.1007/s11207-009-9318-9

Yuan, Y., Shih, F. Y., Jing, J., & Wang, H.-M. 2010,
Research in Astronomy and Astrophysics, 10, 785,
doi: 10.1088/1674-4527/10/8/008

