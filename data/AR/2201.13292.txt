Fragmented ARES: Dynamic Storage
for Large Objects

Chryssis Georgiou
University of Cyprus
Nicosia, Cyprus
chryssis@ucy.ac.cy

Nicolas Nicolaou
Algolysis Ltd
Limassol, Cyprus
nicolas@algolysis.com

Andria Trigeorgi
University of Cyprus
Nicosia, Cyprus
atrige01@cs.ucy.ac.cy

2
2
0
2

n
a
J

1
3

]

C
D
.
s
c
[

1
v
2
9
2
3
1
.
1
0
2
2
:
v
i
X
r
a

the most

Abstract—Data availability is one of

important
features in distributed storage systems, made possible by data
replication. Nowadays data are generated rapidly and the goal to
develop efﬁcient, scalable and reliable storage systems has become
one of the major challenges for high performance computing. In
this work, we develop a dynamic, robust and strongly consistent
distributed storage implementation suitable for handling large
objects (such as ﬁles). We do so by integrating an Adaptive,
Reconﬁgurable, Atomic Storage framework, called ARES, with
a distributed ﬁle system, called COBFS, which relies on a
block fragmentation technique to handle large objects. With the
addition of ARES, we also enable the use of an erasure-coded
algorithm to further split our data and to potentially improve
storage efﬁciency at the replica servers and operation latency.
To put the practicality of our outcomes at test, we conduct an
in-depth experimental evaluation on the Emulab and AWS EC2
testbeds, illustrating the beneﬁts of our approaches, as well as
other interesting tradeoffs.

Index Terms—Distributed storage, Large objects, Strong con-
sistency, High access concurrency, Erasure code, Reconﬁguration

I. INTRODUCTION

Motivation and prior work. Distributed Storage Systems
(DSS) have gained momentum in recent years, following the
demand for available, accessible, and survivable data storage
[25], [27]. To preserve those properties in a harsh, asyn-
chronous, fail prone environment (as a distributed system),
data are replicated in multiple, often geographically separated
devices, raising the challenge on how to preserve consistency
between the replica copies.

For more than two decades, a series of works [7], [11], [13],
[14], [20] suggested solutions for building distributed shared
memory emulations, allowing data to be shared concurrently
i.e. registers, with strict
offering basic memory elements,
consistency guarantees. Linerazibility (atomicity) [15] is the
most challenging, yet intuitive consistency guarantee that such
solutions provide.

The problem of keeping copies consistent becomes even
more challenging when failed replica hosts (or servers) need
to be replaced or new servers need to be added in the system.
Since the data of a DSS should be accessible immediately, it

This work was supported in part by the Cyprus Research and Innovation
Foundation under the grant agreement POST-DOC/0916/0090. The experi-
ments presented in this paper were carried out using the Emulab and AWS
experimental testbeds.

is imperative that the service interruption during a failure or
a repair should be as short as possible. The need to be able
to modify the set of servers while ensuring service liveness
yielded dynamic solutions and reconﬁguration services. Exam-
ples of reconﬁgurable storage algorithms are RAMBO [18],
DYNASTORE [2], SM-STORE [17], SPSNSTORE [12] and
ARES [22].

Currently, such emulations are limited to small-size, ver-
sionless, primitive objects (like registers), hindering the practi-
cality of the solutions when dealing with larger, more common
DSS objects (like ﬁles). A recent work by Anta et al. [4],
introduced a modular solution, called COBFS, which com-
bines a suitable data fragmentation strategy with a distributed
shared memory module to boost concurrency, while maintain-
ing strong consistency guarantees, and minimizing operation
latencies. The architecture of COBFS is shown in Fig. 1 and it
is composed of two main modules: (i) a Fragmentation Module
(FM), and (ii) a Distributed Shared Memory Module (DSMM).
In short, the FM implements a fragmented object, which is a
totally ordered sequence of blocks (where a block is a R/W
object of restricted size), while the DSMM implements an
interface to a shared memory service that allows operations
on individual block objects. To this end a DSMM may
encapsulate any DSM implementation. COBFS implements
coverable linearizable fragmented objects. Coverability [21]
extends linearizability with the additional guarantee that object
writes succeed when associating the written value with the
“current” version of the object. In a different case, a write
operation becomes a read operation and returns the latest
version and the associated value of the object. Coverable
solutions have been proposed only for the static environment.
Thus, COBFS operated over a static architecture, with the
replica hosts predeﬁned in the beginning of its execution.
Contributions. In this work, we propose a dynamic DSS,
that (i) supports versioned objects, (ii) is suitable for large
objects (such as ﬁles), and (iii) is storage-efﬁcient. To achieve
this, we integrate the dynamic distributed shared memory
algorithm ARES with the DSMM module in COBFS. ARES is
the ﬁrst algorithm that enables erasure coded based dynamic
DSM yielding beneﬁts on the storage efﬁciency at the replica
hosts. To support versioning we modify ARES to implement
coverable objects, while high access concurrency is preserved
by introducing support for fragmented objects. Ultimately, we

 
 
 
 
 
 
otherwise, Hξ is concurrent. Finally, Hξ is complete if every
invocation in Hξ has a matching response in Hξ, i.e., each
operation in ξ is complete. An operation π1 precedes an
operation π2 (or π2 succeeds π1), denoted by π1 → π2, in
Hξ, if the response action of π1 appears before the invocation
action of π2 in Hξ. Two operations are concurrent if none
precedes the other.
Clients and servers. We consider a system composed of four
distinct sets of crash-prone, asynchronous processes: a set W
of writers, a set R of readers, a set G of reconﬁguration clients,
and a set S of servers. Let I = W ∪ R ∪ G be the set of
clients. Servers host data elements (replicas or encoded data
fragments). Each writer is allowed to modify the value of a
shared object, and each reader is allowed to obtain the value
of that object. Reconﬁguration clients attempt to introduce
new conﬁguration of servers to the system in order to mask
transient errors and to ensure the longevity of the service.
Conﬁgurations. A conﬁguration, c ∈ C, consists of:
(i) c.Servers ⊆ S: a set of server
identiﬁers; (ii)
c.Quorums: the set of quorums on c.Servers, s.t. ∀Q1, Q2 ∈
c.Quorums, Q1, Q2 ⊆ c.Servers and Q1 ∩ Q2 (cid:54)= ∅; (iii)
DAP (c): the set of data access primitives (operations at level
lower than reads or writes) that clients in I may invoke on
c.Servers (cf. Section III); (iv) c.Con: a consensus instance
with the values from C, implemented and running on top of
the servers in c.Servers; and (v) the pair (c.tag, c.val): the
maximum tag-value pair that clients in I have. A tag consists
of a timestamp ts (sequence number) and a writer id; the
timestamp is used for ordering the operations, and the writer
id is used to break symmetry (when two writers attempt to
write concurrently using the same timestamp) [18]. We refer
to a server s ∈ c.Servers as a member of conﬁguration c.
The consensus instance c.Con in each conﬁguration c is used
as a service that returns the identiﬁer of the conﬁguration that
follows c.
Fragmented objects and fragmented linearizability. As de-
ﬁned in [4], a fragmented object is a totally ordered sequence
of block objects. Let F denote the set of fragmented objects,
and B the set of block objects. A block object (or block) b ∈ B
is a concurrent R/W object with a unique id and is associated
with two structures, val and ver. The unique id of the block
is a triplet (cid:104)f id, clid, clseq(cid:105) ∈ F × I × N, where f id ∈ F
is the id of the fragmented object in which the block belongs
to, clid ∈ I is the id of the client that created the block, and
clseq ∈ N is the client’s local sequence number of blocks that
is incremented every time this client creates a block for this
fragmented object. val(b) is composed of: (i) a pointer that
points to the next block in the sequence (⊥ denotes the null
pointer), and (ii) the data contained in the block (⊥ means
there are no data). ver(b) = (cid:104)wid, bseq(cid:105), where wid ∈ I is
the id of the client that last updated val(b) (initially is the
id of the creator of the block) and bseq ∈ N is a sequence
number of the block (initially 0) that it is incremented every
time val(b) is updated.

A fragmented object f is a concurrent R/W object with
a unique identiﬁer from a set F. Essentially, a fragmented

Fig. 1. Basic architecture of COBFS [4]

aim to make a leap towards dynamic DSS that will be attractive
for practical applications (like highly concurrent and consistent
ﬁle sharing).

In summary, our contributions are the following:
• We propose and prove the correctness of the coverable
version of ARES, COARES, a Fault-tolerant, Reconﬁg-
urable, Erasure coded, Atomic Storage, to support ver-
sioned objects (Section IV).

• We adopt

the idea of

fragmentation as presented
to obtain COARESF, which enables
in COBFS [4],
COARES to handle large shared data objects and in-
creased data access concurrency (Section V). The cor-
rectness of COARESF is rigorously proven.

• To reduce the operational latency of the read/write oper-
ations in the DSMM layer, we apply and prove correct
an optimization in the implementation of the erasure
coded data-access primitives (DAP) used by the ARES
framework (which includes COARES and COARESF).
This optimization has its own interest, as it could be
applicable beyond the ARES framework, i.e., by other
erasure coded algorithms relying on tag-ordered data-
access primitives.

• We present an in-depth experimental evaluation of our
approach over Emulab, a popular emulation testbest, and
Amazon Web Services (AWS) EC2, an overlay (real-
time) testbed (Section VII). Our experiments compare
various versions of our implementation, i.e., with and
without the fragmentation technique or with and with-
out Erasure Code or with and without reconﬁguration,
illustrating tradeoffs and synergies.

II. MODEL AND DEFINITIONS

In this section we present the system setting and deﬁne
necessary terms we use in the rest of the manuscript. As
mentioned, our main goal is to implement a highly consistent
shared storage that supports large shared objects and favors
high access concurrency. We assume read/write (R/W) shared
objects that support two operations: (i) a read operation that
returns the value of the object, and (ii) a write operation that
modiﬁes the value of the object.
Executions and histories An execution ξ of a distributed
algorithm A is an alternating sequence of states and actions
of A reﬂecting the evolution in real time of the execution. A
history Hξ is the subsequence of the actions in ξ. A history
Hξ is sequential if it starts with an invocation action and each
invocation is immediately followed by its matching response;

object is a sequence of blocks from B, with a value val(f ) =
(cid:104)b0, b1, b2, . . .(cid:105), where each bi ∈ B. Initially, each fragmented
object contains an empty block, i.e., val(f ) = (cid:104)b0(cid:105) with
val(b0) = ε; we refer to this as the genesis block. We now
proceed to present the formal deﬁnitions of linearizability and
fragmented linearizability, as given in [4].

Deﬁnition 1 (Linearizability). An object f is linearizable [16]
if, given any complete history H, there exists a permutation
σ of all actions in H such that:

• σ is a sequential history and follows the sequential

speciﬁcation* of f , and

• for operations π1, π2, if π1 → π2 in H, then π1 appears

before π2 in σ.

Given a history H, we denote for an operation π the
history H π which contains the actions extracted from H and
performed during π (including its invocation and response
actions). Hence, if val(f ) is the value returned by read()f ,
then H read()f contains an invocation and matching response
for a read()b operation, for each b ∈ val(f ). Then, from H,
we can construct a history H|f that only contains operations
on the whole fragmented object.

Deﬁnition 2 (Fragmented Linearizability [4]). Let f ∈ F be a
fragmented object, H a complete history on f , and val(f )H ⊆
B the value of f at the end of H. Then, f is fragmented
linearizable if there exists a permutation σb over all the actions
on b in H, ∀b ∈ val(f )H , such that:

• σb is a sequential history that follows the sequential

speciﬁcation of b†, and

• for operations π1, π2 that appear in H|f extracted from
H, if π1 → π2 in H|f , then all operations on b in H π1
appear before any operation on b in H π2 in σb.

Fragmented linearizability guarantees that all concurrent
operations on different blocks prevail, and only concurrent
operations on the same blocks are conﬂicting. The second
point guarantees the total ordering of the operations on the
fragmented object with respect to their real time ordering. For
example, considering two read operations on f , say ρ1 → ρ2,
it must be a case that ρ2 returns a supersequence of the
sequence returned by ρ1.
Coverability and fragmented coverability. Coverability
is deﬁned over a totally ordered set of versions, say
Versions, and introduces the notion of versioned (cover-
able) objects. According to [21], a coverable object
is a
type of R/W object where each value written is assigned
with a version from the set Versions. Denoting a success-
ful write as tr-write(ver)[ver(cid:48), chg]p (updating the object
from version ver to ver(cid:48)), and an unsuccessful write as
tr-write(ver)[ver(cid:48), unchg]p, a coverable implementation sat-
isﬁes the properties consolidation, continuity and evolution as
formally deﬁned below in Deﬁnition 4.

Intuitively, consolidation speciﬁes that write operations may
revise the register with a version larger than any version
modiﬁed by a preceding write operation, and may lead to
a version newer than any version introduced by a preceding
write operation. Continuity requires that a write operation may
revise a version that was introduced by a preceding write
operation, according to the given total order. Finally, evolution
limits the relative increment on the version of a register that
can be introduced by any operation.

We say that a write operation revises a version ver of the
versioned object to a version ver(cid:48) (or produces ver(cid:48)) in an
execution ξ, if tr-write(ver)[ver(cid:48)]pi completes in Hξ. Let the
set of successful write operations on a history Hξ be deﬁned
as Wξ,succ = {π : π = tr-write(ver)[ver(cid:48)]pi completes in Hξ}
The set now of produced versions in the history Hξ is deﬁned
by Versions ξ = {veri : tr-write(ver)[veri]pi ∈ Wξ,succ} ∪
{ver0} where ver0 is the initial version of the object. Observe
that the elements of Versions ξ are totally ordered.

Deﬁnition 3 (Validity). An execution ξ (resp. its history Hξ)
is a valid execution (resp. history) on a versioned object, for
any pi, pj ∈ I:
• ∀cvr-ω(ver)[ver(cid:48)]pi ∈ Wξ,succ, ver < ver(cid:48),
operations
• for

tr-write(∗)[ver(cid:48)]pi
tr-write(∗)[ver(cid:48)(cid:48)]pj in Wξ,succ, ver(cid:48) (cid:54)= ver(cid:48)(cid:48), and

and

any

• for each verk ∈ V ersionsξ there is a sequence of versions
ver0, ver1, . . . , verk, such that tr-write(veri)[veri+1] ∈
Wξ,succ, for 0 ≤ i < k.

Deﬁnition 4 (Coverability [21]). A valid execution ξ is cover-
able with respect to a total order <ξ on operations in Wξ,succ
if:

• (Consolidation)

If π1 = tr-write(∗)[veri], π2 =
tr-write(verj)[∗] ∈ Wξ,succ, and π1 →Hξ π2 in Hξ, then
veri ≤ verj and π1 <ξ π2.

• (Continuity) if π2 = tr-write(ver)[veri] ∈ Wξ,succ, then
there exists π1 ∈ Wξ,succ s.t. π1 = tr-write(∗)[ver] and
π1 <ξ π2, or ver = ver0.

sequences of
2 , . . . , ver(cid:48)(cid:48)

• (Evolution) let ver, ver(cid:48), ver(cid:48)(cid:48) ∈ V ersionsξ. If
versions ver(cid:48)
1, ver(cid:48)
2, . . . , ver(cid:48)
are
1 , ver(cid:48)
1 = ver(cid:48)(cid:48)
(cid:96) , where ver = ver(cid:48)
1 , ver(cid:48)(cid:48)
ver(cid:48)(cid:48)
i)[ver(cid:48)
ver(cid:48), and ver(cid:48)(cid:48)
(cid:96) = ver(cid:48)(cid:48) such that tr-write(ver(cid:48)
∈ Wξ,succ, for 1 ≤ i < k, and tr-write(ver(cid:48)(cid:48)
i )[ver(cid:48)(cid:48)
∈ Wξ,succ, for 1 ≤ i < (cid:96), and k < (cid:96), then ver(cid:48) < ver(cid:48)(cid:48).

there
k and
k =
i+1]
i+1]

If a fragmented object utilizes coverable blocks, instead of
linearizable blocks, then Deﬁnition 2 together with Deﬁni-
tion 4 provide what we would call fragmented coverability:
Concurrent update operations on different blocks would all
prevail (as long as each update is tagged with the latest version
of each block), whereas only one update operation on the same
block would prevail (all the other updates on the same block
that are concurrent with this would become a read operation).

*The sequential speciﬁcation of a concurrent object describes the behavior

of the object when accessed sequentially.

†The sequential speciﬁcation of a block is similar to that of a R/W

register [19], whose value has bounded length.

III. ARES: A FRAMEWORK FOR DYNAMIC STORAGE

ARES [22] is a modular framework, designed to implement
dynamic, reconﬁgurable, fault-tolerant, read/write distributed

to

Similar

linearizable (atomic) shared memory objects.
traditional

implementations, ARES

uses
(cid:104)tag, value(cid:105) pairs to order the operations on a shared object.
In contrast to existing solutions, ARES does not explicitly
deﬁne the exact methodology to access the object replicas.
Rather, it relies on three, so called, data access primitives
(DAPs): (i) the get-tag primitive which returns the tag of an
object, (ii) the get-data primitive which returns a (cid:104)tag, value(cid:105)
pair, and (iii) the put-data((cid:104)τ,v(cid:105)) primitive which accepts a
(cid:104)tag, value(cid:105) as argument.

As seen in [22], these DAPs may be used to express the
data access strategy (i.e., how they retrieve and update the
object data) of different shared memory algorithms (e.g., [6]).
Therefore, the DAPs help ARES to achieve a modular design,
agnostic of the data access strategies, and in turn enables
ARES to use different DAP implementation per conﬁguration
(something impossible for other solutions). Linearizability is
then preserved by ARES given that
the DAPs satisfy the
following property in any given conﬁguration c:

Property 1 (DAP Consistency Properties). In an execution
ξ we say that a DAP operation in ξ is complete if both the
invocation and the matching response step appear in ξ. If Π
is the set of complete DAP operations in execution ξ then for
any φ, π ∈ Π:

C1 If φ is c.put-data((cid:104)τφ, vφ(cid:105)), (cid:104)τφ, vφ(cid:105) ∈ T × V, and π
is c.get-tag() (or c.get-data()) that returns τπ ∈ T (or
(cid:104)τπ, vπ(cid:105) ∈ T × V) and φ → π in ξ, then τπ ≥ τφ.
C2 If φ is a c.get-data() that returns (cid:104)τπ, vπ(cid:105) ∈ T × V, then
there exists π such that π is a c.put-data((cid:104)τπ, vπ(cid:105)) and
φ did not complete before the invocation of π. If no such
π exists in ξ, then (τπ, vπ) is equal to (t0, v0).

DAP Implementations: To demonstrate the ﬂexibility that
DAPs provide, the authors in [10], expressed two different
atomic shared R/W algorithms in terms of DAPs. These are
the DAPs for the well celebrated ABD [6] algorithm, and the
DAPs for an erasure coded based approach presented for the
ﬁrst time in [10]. In the rest of the manuscript we refer to
the two DAP implementations as ABD-DAP and EC-DAP.
Erasure-coded approaches became popular in implementing
atomic R/W objects as they offer fault tolerance and storage
efﬁciency at the replica hosts. In particular, an [n, k]-MDS
erasure coding algorithm (e.g., Reed-Solomon) splits the ob-
ject into k equally sized fragments. Then erasure coding is
applied to these fragments to obtain n coded elements, which
consist of the k encoded data fragments and m encoded parity
fragments. The n coded fragments are distributed among a set
of n different replica servers. Any k of the n coded fragments
can then be used to reconstruct the initial object value. As
servers maintain a fragment instead of the whole object value,
EC based approaches claim signiﬁcant storage beneﬁts. By
utilizing the EC-DAP, ARES became the ﬁrst erasure coded
dynamic algorithm to implement an atomic R/W object.

Given the DAPs we can now provide a high-level descrip-
tion of the two main functionalities supported by ARES: (i)

the reconﬁguration of the data replicas, and (ii) the read/write
operations on the shared object.
Reconﬁguration. Reconﬁguration is the process of changing
the set of servers that hold the object replicas. A conﬁguration
sequence cseq in ARES is deﬁned as a sequence of pairs
(cid:104)c, status(cid:105) where c ∈ C, and status ∈ {P, F } (P stands
for proposed and F for ﬁnalized). Conﬁguration sequences
are constructed and stored in clients, while each server in a
conﬁguration c only maintains the conﬁguration that follows
c in a local variable nextC ∈ C ∪ {⊥} × {P, F }.

To perform a reconﬁguration operation recon(c), a client
follows 4 steps. At ﬁrst, the reconﬁguration client r executes a
sequence traversal to discover the latest conﬁguration sequence
cseq. Then it attempts to add (cid:104)c, P (cid:105) at the end of cseq by
proposing c to a consensus mechanism. The outcome of the
consensus may be a conﬁguration c(cid:48) (possibly different than
c) proposed by some reconﬁguration client. Then the client
determines the maximum tag-value pair of the object, say (cid:104)τ,v(cid:105)
by executing get-data operation starting from the last ﬁnalized
conﬁguration in cseq to the last conﬁguration in cseq, and
transfers the pair to c(cid:48) by performing put-data((cid:104)τ,v(cid:105)) on c(cid:48).
Once the update of the value is complete, the client ﬁnalizes
the proposed conﬁguration by setting nextC = (cid:104)c(cid:48), F (cid:105) on a
quorum of servers of the last conﬁguration in cseq (or c0 if
no other conﬁguration exists).

The traversal and reconﬁguration procedure in ARES pre-
serves three crucial properties: (i) conﬁguration uniqueness,
i.e., the conﬁguration sequences in any two processes have
identical conﬁguration at any index i, (ii) sequence preﬁx,
i.e., the conﬁguration sequence witnessed by an operation is a
preﬁx of the sequence witnessed by any succeeding operation,
and (iii) sequence progress, i.e., if the conﬁguration with index
i is ﬁnalized during an operation, then a conﬁguration j, for
j ≥ i, will be ﬁnalized for a succeeding operation.
Read/Write operations. A write (or read) operation π by a
client p is executed by performing the following actions: (i) π
invokes a read-conﬁg action to obtain the latest conﬁguration
sequence cseq, (ii) π invokes a get-tag (if a write) or get-data
(if a read) in each conﬁguration, starting from the last ﬁnalized
to the last conﬁguration in cseq, and discovers the maximum
τ or (cid:104)τ, v(cid:105) pair respectively, and (iii) repeatedly invokes
put-data((cid:104)τ (cid:48), v(cid:48)(cid:105)), where (cid:104)τ (cid:48), v(cid:48)(cid:105) = (cid:104)τ + 1, v(cid:48)(cid:105) if π is a write
and (cid:104)τ (cid:48), v(cid:48)(cid:105) = (cid:104)τ,v(cid:105) if π is a read in the last conﬁguration in
cseq, and read-conﬁg to discover any new conﬁguration, until
no additional conﬁguration is observed.

IV. COARES: COVERABLE ARES
In this section we replace the R/W objects of ARES [22]
with versioned objects that use coverability (cf. Section II),
yielding the coverable variant of ARES, which we refer as
COARES. We ﬁrst present the algorithms and then its correct-
ness proof.

A. Description

In this section we describe the modiﬁcation that need to
occur on ARES in order to support coverability. The recon-
ﬁguration protocol and the DAP implementations remain the

same as they are not affected by the application of coverability.
The changes occur in the speciﬁcation of read/write operations,
which we detail below.
Read/Write operations. Algorithm 1 speciﬁes the read and
write protocols of COARES. The blue text annotates the
changes when compared to the original ARES read/write pro-
tocols. The local variable f lag ∈ {chg, unchg}, maintained
by the write clients, is set to chg when the write operation is
successful and to unchg otherwise; initially it is set to unchg.
The state variable version is used by the client to maintain
the tag of the coverable object. At ﬁrst, in both cvr-read and
cvr-write operations, the read/write client issues a read-conﬁg
action to obtain the latest introduced conﬁguration; cf. line
Alg. 1:11 (resp. line Alg. 1:40).

In the case of cvr-write, the writer wi ﬁnds the last ﬁnalized
entry in cseq, say µ, and performs a cseq[j].conf.get-data()
action, for µ ≤ j ≤ |cseq| (lines Alg. 1:12–15). Thus, wi re-
trieves all the (cid:104)τ, v(cid:105) pairs from the last ﬁnalized conﬁguration
and all the pending ones. Note that in cvr-write, get-data is
used in the ﬁrst phase instead of a get-tag, as the coverable
version needs both the highest tag and value and not only
the tag, as in the original write protocol. Then, the writer
computes the maximum (cid:104)τ, v(cid:105) pair among all the returned
replies. Lines Alg. 1:16 - 1:20 depict the main difference
between the coverable cvr-write and the original one: if the
maximum τ is equal to the state variable version, meaning
the writer wi has the latest version of the object,
that
it
proceeds to update the state of the object ((cid:104)τ, v(cid:105)) by increasing
τ and assigning (cid:104)τ, v(cid:105) to (cid:104)(cid:104)τ.ts + 1, ωi(cid:105), val(cid:105), where val is the
value it wishes to write (lines Alg. 1:17–18). Otherwise, the
state of the object does not change and the writer keeps the
maximum (cid:104)τ, v(cid:105) pair found in the ﬁrst phase (i.e., the write has
become a read). No matter whether the state changed or not,
the writer updates its version with the value τ (line Alg. 1:21).
the ﬁrst phase is the same as
it discovers the maximum tag-value
the original,
pair among the received replies (lines Alg. 1:43–44). The
propagation of (cid:104)τ, v(cid:105) in both cvr-read (lines Alg. 1:46–53))
and cvr-write (lines Alg. 1:23–30) remains the same as the
original. Finally, the cvr-write operation returns (cid:104)τ, v(cid:105) and the
f lag, whereas the cvr-read operation only returns ((cid:104)τ, v(cid:105)).

In the case of cvr-read,
that

is,

B. Correctness of COARES

COARES is correct if it satisﬁes liveness (termination) and
safety (i.e., linearizable coverability). Termination holds since
read, update and reconﬁg operations on the COARES always
complete given that the DAP complete. As shown in [22],
ARES, implements an atomic object given that the DAP used
satisfy Property 1. Given that COARES uses the same recon-
ﬁguration and read operations, and only the write operation
is sometime converted to read operation then linearizability is
not affected and can be shown that it holds in a similar way
as in [22].

The validity and coverability properties, as deﬁned in Def-
initions 3 and 4, remain to be examined. In COARES, we
use tags to denote the version of the register. Given that the

DAP (c) used in any conﬁguration c ∈ C satisﬁes Property 1,
we will show that any execution ξ of COARES satisﬁes the
properties of Deﬁnitions 3 and 4. In the lemmas that follow
we refer to a successful write operation to the one that is not
converted to a read operation.

We begin with some lemmas that help us show that

COARES satisﬁes Validity.

In any execution ξ of
Lemma 5 (Version Increment).
COARES, if ω is a successful write operation, and ver the
maximum version it discovered during the get-data operation,
then ω propagates a version ver(cid:48) > ver.

Proof. This lemma follows from the fact that COARES uses a
condition before the propagation phase in line Alg. 1:16. The
writer checks if the maximum tag retrieved from the get-data
action is equal to the local version. If that holds, then the
writer generates a new version larger than its local version by
incrementing the tag found.

In any execution ξ of
Lemma 6 (Version Uniqueness).
COARES, if two write operations ω1 and ω2, write values
associated with versions ver1 and ver2 respectively,
then
ver1 (cid:54)= ver2.

Proof. A tag is composed of an integer timestamp ts and the id
of a process wid. Let w1 be the id of the writer that invoked ω1
and w2 the id of the writer that invoked ω2. To show whether
the versions generated by the two write operations are not
equal we need to examine two cases: (a) both ω1 and ω2 are
invoked by the same writer, i.e. w1 = w2, and (b) ω1 and ω2
are invoked by two different writers, i.e. w1 (cid:54)= w2.
Case a: In this case, the uniqueness of the versions is achieved
due to the well-formedness assumption and the C1 term in
Property 1. By well-formdness, writer w1 can only invoke
one operation at a time. Thus, the last put-data(ver1, ∗) of
ω1 completes before the ﬁrst get-data of ω2.

If both operations are invoked and completed in the same
conﬁguration c then by C1, the version ver(cid:48) returned by
c.get-data, is ver(cid:48) ≥ ver1. Since the version is incremented
in ω2 then ver2 = ver(cid:48) + 1 > ver1, and hence ver1 (cid:54)= ver2
as desired.

It remains to examine the case where the put-data was
invoked in a conﬁguration c and the get-data in a conﬁguration
c(cid:48). Since by well-formedness ω1 → ω2, then by the sequence
preﬁx guaranteed by the reconﬁguration protocol of ARES
(second property) the cseq1 obtained during the read-conﬁg
action in ω1 is a preﬁx of the cseq2 obtained during the same
action in ω2. Notice that c(cid:48) is the last ﬁnalized conﬁguration
in cseq2 as this is the conﬁguration where the ﬁrst get-data
action of ω2 is invoked. If c(cid:48) appears before c in cseq2 then
by COARES the write operation ω2 will invoke a get-data
operation in c as well and with the same reasoning as before
will generate a ver2 (cid:54)= ver1. If now c appears before c(cid:48) in
cseq2, then it must be the case that a reconﬁguration operation
r has been invoked concurrently or after ω2 and added c(cid:48). By
ARES [22], r, invoked a put-data(ver(cid:48)) in c(cid:48) before ﬁnalizing
c(cid:48) with ver(cid:48) ≥ ver1. So when ω2 invokes get-data in c(cid:48) by C1

Algorithm 1 Write and Read protocols for COARES.

2:

4:

6:

8:

10:

12:

14:

16:

18:

20:

22:

24:

26:

CVR-Write Operation:
at each writer wi
State Variables:
cseq[]s.t.cseq[j] ∈ C × {F, P }
version ∈ N+ × W initially (cid:104)0, ⊥(cid:105)
Local Variables:
f lag ∈ {chg, unchg} initially unchg
Initialization:
cseq[0] = (cid:104)c0, F (cid:105)

operation cvr-write(val), val ∈ V

cseq ←read-conﬁg(cseq)
µ ← max({i : cseq[i].status = F })
ν ← |cseq|
for i = µ : ν do

(cid:104)τ, v(cid:105) ← max(cseq[i].cf g.get-data(), (cid:104)τ, v(cid:105))

if version = τ then

f lag ← chg
(cid:104)τ, v(cid:105) ← (cid:104)(cid:104)τ.ts + 1, ωi(cid:105), val(cid:105)

else

f lag ← unchg

version ← τ
done ← f alse
while not done do

cseq[ν].cf g.put-data((cid:104)τ, v(cid:105))
cseq ←read-conﬁg(cseq)
if |cseq| = ν then
done ← true

28:

30:

32:

34:

36:

38:

40:

42:

44:

46:

48:

50:

52:

54:

else

ν ← |cseq|

end while
return (cid:104)τ, v(cid:105), f lag

end operation

CVR-Read Operation:
at each reader ri
State Variables:
cseq[]s.t.cseq[j] ∈ C × {F, P }
Initialization:
cseq[0] = (cid:104)c0, F (cid:105)

operation cvr-read( )

cseq ←read-conﬁg(cseq)
µ ← max({j : cseq[j].status = F })
ν ← |cseq|
for i = µ : ν do

(cid:104)τ, v(cid:105) ← max(cseq[i].cf g.get-data(), (cid:104)τ, v(cid:105))

done ← false
while not done do

cseq[ν].cf g.put-data((cid:104)τ, v(cid:105))
cseq ←read-conﬁg(cseq)
if |cseq| = ν then
done ← true

else

ν ← |cseq|

end while
return (cid:104)τ, v(cid:105)

end operation

will obtain a version ver(cid:48)(cid:48) ≥ ver(cid:48) ≥ ver1. Hence ver2 > ver(cid:48)(cid:48)
and thus ver2 (cid:54)= ver1 as needed.
Case b: When w1 (cid:54)= w2 then ω1 generates a version ver1 =
{ts1, w1} and ω2 generates some version ver2 = {ts2, w2}.
Even if ts1 = ts2 the two version differ on the unique id of
the writers and hence ver1 (cid:54)= ver2. This completes the case
and the proof.

Lemma 7. Each version we reach in an execution is derived
(through a chain of operations) from the initial version of the
register ver0. From this point onward we ﬁx ξ to be a valid
execution and Hξ to be its valid history.

Proof. Every tag is generated by extending the tag retrieved
by a get-data operation starting from the initial tag (lines
Alg. 1:17–18). In turn, each get-data operation returns a tag
written by a put-data operation or the initial tag (as per C2 in
Property 1). Then, applying a simple induction, we may show
that there is a sequence of tags leading from the initial tag to
the tag used by the write operation.

Lemma 8. In any execution ξ of COARES, all the coverability
properties of Deﬁnition 4 are satisﬁed.

Proof. For consolidation we need to show that
for two
write operations ω1 = cvr-ω(∗)[τ1, chg] and ω2 =
cvr-ω(τ2)[∗, chg], if ω1 →ξ ω2 then τ1 ≤ τ2. According to
C1 of Property 1, since the get-data of ω2 appears after the
put-data of ω1, the get-data of ω2 returns a tag higher than
the one written by ω1.

Continuity is preserved as a write operation ﬁrst invokes a
get-data action for the latest tag before proceeding to put-data
to write a new value. According to C2 of Property 1, the

get-data action returns a tag already written by a put-data or
the initial tag of the register.

that

To show that evolution is preserved, we take into ac-
count
the version of a register is given by its tag,
where tags are compared lexicographically. A successful write
π1 = cvr-ω(τ)[τ (cid:48)] generates a new tag τ (cid:48) from τ such that
τ (cid:48).ts = τ.ts + 1 (line Alg. 1:18). Consider sequences of tags
τ1, τ2, . . . , τk and τ (cid:48)
(cid:96) such that τ1 = τ (cid:48)
1. Assume
i )[τ (cid:48)
that cvr-ω(τi)[τi+1], for 1 ≤ i < k, and cvr-ω(τ (cid:48)
i+1], for
1 ≤ i < (cid:96), are successful writes. If τ1.ts = τ (cid:48)
1.ts = z, then
τk.ts = z +k and τ (cid:48)

(cid:96).ts = z +(cid:96), and if k < (cid:96) then τk < τ (cid:48)
(cid:96).

2, . . . , τ (cid:48)

1, τ (cid:48)

The main result of this section follows:

Theorem 9. COARES implements an atomic coverable object
given that the DAPs implemented in any conﬁguration c satisfy
Property 1.

Proof. Atomicity follows from the fast that ARES implement
an atomic object if the DAPs satisfy Property 1. Lemmas 5, 6
and 7 show that COARES satisﬁes validity (see Deﬁnition 3),
and Lemma 8 that COARES satisﬁes the coverability proper-
ties (see Deﬁnition 4). Thus the theorem follows.

V. COARESF: INTEGRATE COARES WITH A
FRAGMENTATION APPROACH

The work in [4] developed a distributed storage frame-
work, called COBFS, which utilizes coverable fragmented
objects. COBFS adopts a modular architecture, separating the
object fragmentation process from the shared memory service
allowing it to use different shared memory implementations.
In this section we describe how COARES can be integrated
with COBFS to obtain what we call COARESF, thus yielding a

dynamic consistent storage suitable for large objects. Further-
more, this enables to combine the fragmentation approach of
COBFS with a second level of striping when EC-DAP is used
with COARES, making this version of COARESF more storage
efﬁcient at the replica hosts. A particular challenge of this
integration is how the fragmentation approach should invoke
reconﬁguration operations, since COBFS in [4] considered
only static (non-reconﬁgurable) systems. We ﬁrst describe
COARESF and then we present
its (non-trivial) proof of
correctness.

A. Description

We proceed with a description of the update, read and
reconﬁg operations. From this point onward, we consider ﬁles,
as an example of fragmented objects. To this respect, we view
a ﬁle as a linked-list of data blocks. Here, the ﬁrst block, i.e.,
the genesis block b0, is a special type of a block that contains
speciﬁc ﬁle information (such as the number of blocks, etc);
see [4] for more details.
Update Operation (Fig. 2). The update operation spans two
main modules: (i) the Fragmentation Module (FM), and (ii) the
Distributed Shared Memory Module (DSMM). The FM uses
a Block Identiﬁcation (BI) module, which draws ideas from
the RSYNC (Remote Sync) algorithm [26]. The BI includes
three main modules, the Block Division, the Block Matching
and Block Updates.

1) Block Division: The BI splits a given ﬁle f into data
blocks based on its contents, using rabin ﬁngerprints [24].
BI has to match each hash, generated by the rabin
ﬁngerprint from the previous step, to a block identiﬁer.
2) Block Matching: At ﬁrst, BI uses a string matching
algorithm [9] to ﬁnd the differences between the new
hashes and the old hashes in the form of four statuses:
(i) equality, (ii) modiﬁed, (iii) inserted, (iv) deleted.
3) Block Updates: Based on the hash statuses, the blocks of
the fragmented object are updated. In the case of equality,
no operation is performed. In case of modiﬁcation, an
update operation is then performed to modify the data
of the block. If new hashes are inserted after the hash of
a block, then an update operation is performed to create
the new blocks after that. The deleted one is treated as a
modiﬁcation that sets an empty value.

Subsequently, the FM uses the DSMM as an external service
to execute the block update operations on the shared memory.
As we already mentioned, we use COARES as storage which
is based on the (n, k)-Reed-Solomon code. It splits the value v
of a block into k elements and then creates n coded elements,
and stores one coded element per server.
Read Operation (Fig. 3). When the system receives a read
request from a client, the FM issues a series of read operations
on the ﬁle’s blocks, starting from the genesis block and
proceeding to the last block by following the next block ids.
As blocks are retrieved, they are assembled in a ﬁle.

As in the case of the update operation, the read executes
the block read operations on the shared memory. COARES

regenerates the value of a block using data from parity disks
and surviving data disks.
Reconﬁg Operation. The speciﬁcation of reconﬁg on the DSS
is given in Algorithm 2, while the speciﬁcation of reconﬁg
on a ﬁle (fragmented object) is given in Algorithm 3. When
the system receives a reconﬁg request from a client, the FM
issues a series of reconﬁg operations on the ﬁle’s blocks,
starting from the genesis block and proceeding to the last
block by following the next block ids (Algorithm 3). The
reconf ig operation executes the block reconf ig operations
on the shared memory (Algorithm 2) using dsmm-reconﬁg
operations.

As shown in Theorem 14,

the blocks’ sequence of a
fragmented object remains connected despite the existence of
concurrent read/write and reconﬁguration operations.

Algorithm 2 DSM Module: Operations on a coverable block object
b at client p

1: function dsmm-reconﬁg(c)b,p
2:
3: end function

b.reconﬁg(c)

Algorithm 3 Fragmentation Module: BI and Operations on a ﬁle
f at client p

1: State Variables:
2: Lf a linked-list of blocks, initially

(cid:104)b0(cid:105);

3:
4:
5:

function fm-reconﬁg(c)f,p

b ← val(b0).ptr
Lf ← (cid:104)b0(cid:105)

while b not NULL do

dsmm-reconﬁg(c)b,p
b ← val(b).ptr

6:
7:
8:
9:
end while
10: end function

B. Correctness of COARESF

When a reconﬁg(c) operation is invoked in ARES, a recon-
ﬁguration client is requesting to change the conﬁguration of
the servers hosting the single R/W object. In the case of a ﬁle
(fragmented object) f , which is composed of multiple blocks,
the fragmentation manager attempts to introduce the new
conﬁguration for every block in f . To this end, COARESF,
as presented in Algorithm 3, issues a dsmm-reconﬁg(c)bi,p
operation for each block bi ∈ f . Concurrent write operations
may introduce new blocks in the same ﬁle. So, how can we
ensure that any new value of the blocks are propagated in any
recently introduced conﬁguration? In the rest of this section
we show that fragmented coverability (see Section II) cannot
be violated.

Before we prove any lemmas, we ﬁrst state a claim that

follows directly from the algorithm.

Claim 10. For any block b (cid:54)= b0, where b0 the genesis block,
created by an fm-update operation, it is initialized with a
conﬁguration sequence cseqb = cseq0, where cseq0 is the
initial conﬁguration.

Notive that we assume that a single quorum remains correct
in cseq0 at any point in the execution. This may change in
practical settings by having an external service to maintain

Fig. 2. Update operation.

Fig. 3. Read operation.

and distribute the latest cseq that will be used in a created
block.

We begin with a lemma that states that for any block in the
list obtained by a read operation, there is a successful update
operation that wrote this block.

Lemma 11. In any execution ξ of COARESF, if ρ is a f m −
readf,∗ operation returns a list L, then for any block b ∈ L,
there exists a successful fm-update(∗)f,∗ operation that either
precedes or is concurrent to ρ

Proof. This lemma follows the proof of Lemma 4 presented
in [5].

In the following lemma we show that a reconﬁguration
moves a version of the object larger than any version written
be a preceding write operation to the installed conﬁguration.

Lemma 12. Suppose that ρ is a dsmm-reconﬁg(c2)b,∗ opera-
tion and ω a successful cvr-write(v)b,∗ operation that changes
the version of b to ver, s.t. ω → ρ in an execution ξ of
COARESF. Then ρ invokes c2.put-data((cid:104)ver(cid:48), ∗(cid:105)) in c2, s.t.
ver(cid:48) ≥ ver.

Proof. Let cseqω be the last conﬁguration sequence returned
by the read-conﬁg action at ω (Alg. 1:25), and cseqρ the
conﬁguration sequence returned by the ﬁrst read-conﬁg action
at ρ (see Alg. 2:8 in [10]). By the preﬁx property of the
reconﬁguration protocol, cseqω will be a preﬁx of cseqρ.

Let c(cid:96) the last conﬁguration in cseqω, and c1 the last ﬁnal-
ized conﬁguration in cseqρ. There are two cases to examine:
(i) c1 appears before c(cid:96) in cseqρ, and (ii) c1 appears before c(cid:96)
in cseqρ.

If (i) is the case then during the update-conﬁg action, ρ
will perform a c(cid:96).get-data() action. By term C1 in Property
1, the c(cid:96).get-data() will return a version ver(cid:48)(cid:48) ≥ ver. Since
the ρ function will execute c2.put-data((cid:104)ver(cid:48), ∗(cid:105)), s.t. ver(cid:48) is
the maximum discovered version, then ver(cid:48) ≥ ver(cid:48)(cid:48) ≥ ver.

In case (ii) it follows that the reconﬁguration operation
that proposed c1 has ﬁnalized the conﬁguration. So either
that reconﬁguration operation moved a version ver(cid:48)(cid:48) of b s.t.
ver(cid:48)(cid:48) ≥ ver in the same way as described in case (i) in c1,
or the write operation would observe c1 during a read-conﬁg
action. In the latter case c1 will appear in cseqω and ω will
invoke a c(cid:96).put-data((cid:104)ver, ∗(cid:105)) s.t. either c(cid:96) = c1 or c(cid:96) a

conﬁguration that appears after c1 in cseqω. Since c1 is the
last ﬁnalized conﬁguration in cseqρ, then in any of the cases
described ρ will invoke a c(cid:96).get-data(). Thus, it will discover
and put in c2 a version ver(cid:48) ≥ ver completing our proof.

Next we need to show that any sequence returned by
any read operation is connected, despite any reconﬁguration
operations that may be executed concurrently.

Lemma 13. In any execution ξ of COARESF, if fm-readf,p
is a read operation on f that returns a list of blocks L =
{b0, b1, . . . , bn}, then it must be the case that (i) b0.ptr = b1,
(ii) bi.ptr = bi+1, for i ∈ [1, n − 1], and (iii) bn.ptr = ⊥.

Proof. Assume by contradiction that there exist some bi ∈ L,
s.t. val(bi).ptr (cid:54)= bi+1 (or val(b0).prt (cid:54)= b1). By Lemma 11,
a block bi may appear in the list returned by a read operation
only if it was created by a successful update operation, say π =
update(b, D)f,∗. Let D = (cid:104)D0, . . . , Dk(cid:105) and B = (cid:104)b1, . . . , bk(cid:105)
be the set of k − 1 blocks created in π, with bi ∈ B. Let us
assume w.l.o.g. that all those blocks appear in L as written by
π (i.e., without any other blocks between any pair of them).
By the design of the algorithm π generates a single linked
path from b to bk, by pointing b to b1 and each bj to bj+1,
for 1 ≤ j < k. Block bk points to the block pointed by b
at the invocation of π, say b(cid:48). So there exists a path b →
b1 → . . . → bi that also leads to bi. According again to the
algorithm, bj+1 ∈ B is created and written before bj, for q ≤
j < k. So when the bj.cvr-write is invoked, the operation
bj+1.cvr-write has already been completed, and thus when b
is written successfully all the blocks in the path are written
successfully as well.

By the preﬁx property of the reconﬁguration protocol it
follows that for each bj written by π, ρ will observe a
conﬁguration sequence bj.cseqρ, s.t. bj.cseqπ is a preﬁx of
bj.cseqρ, and hence cπ appears in bj.cseqρ. If cπ appears after
the last ﬁnalized conﬁguration c(cid:96) in bj.cseqρ, then the read
operation will invoke cπ.get-data() and by the coverability
property and property C1, will obtain a version ver(cid:48) ≥ ver.
In case cπ appears before c(cid:96) then a new conﬁguration was
invoked after or concurrently to π and then by Lemma 12 it
follows that the version of b in c(cid:96) is again ver(cid:48) ≥ ver. So we
need to examine the following three cases for bi: (i) bi is b,
(ii) bi is bk, and (iii) bi is one of the blocks bj, for 1 ≤ j < k.

1 and b(cid:48)

1, . . . , b(cid:48)

Case i: If bi is the block b then we should examine if bi.ptr (cid:54)=
b1. Let ver the version of b written by π and ver(cid:48) the version
of b as retrieved by ρ. If ver = ver(cid:48) then ρ retrieved the
block written by ω as the versions by Lemma 6 are unique.
Thus, bi.ptr = b1 in this case contradicting our assumption.
In case ver(cid:48) > ver then there should be a successful update
operation ω(cid:48) that written block b with ver(cid:48). There are two
cases to consider based on whether ω(cid:48) introduced new blocks
or not. If not then the b.ptr = b1 contradicting our assumption.
If it introduced a new list of blocks {b(cid:48)
k}, then it should
have written those blocks before writing b. In that case ρ would
observe b.ptr = b(cid:48)
1 would have been part of L which
is not the case as the next block from b in L is b1, leading to
contradiction.
Case ii: The case (ii) can be proven in the same way as case
(i) for each block bj, for 1 ≤ j < k.
Case iii: If now bi = bk, then we should examine if bi.ptr (cid:54)=
b(cid:48). Since b was pointing to b(cid:48) at the invocation of π then b(cid:48) was
either (i) created during the update operation that also created
b, or (ii) was created before b. In both cases b(cid:48) was written
before b. In case (i), by Lemma 11, the update operation that
created b was successful and thus b(cid:48) must be created as well.
In case (ii) it follows that b is the last inserted block of an
update and is assigned to point to b(cid:48). Since no block is deleted,
then b(cid:48) remains in L when bi is created and thus bi points to
an existing block. Furthermore, since π was successful, then
it successfully written b and hence only the blocks in B were
inserted between b and b(cid:48) at the response of π. In case the
version of bi was ver(cid:48) and larger than the version written on
bk by π then either bk was not extended and contains new data,
or the new block is impossible as L should have included the
blocks extending bk. So b(cid:48) must be the next block after bi in
L at the response of π and there is a path between b and b(cid:48).
This completes the proof.

We conclude with the main result of this section.

Theorem 14. COARESF implements an atomic coverable
fragmented object.

Proof. By the correctness proof in Section IV-B follows that
every block operation in COARESF satisﬁes atomic coverabil-
ity and together with Lemma 13, which shows the connectivity
of blocks, it follows that COARESF implements a coverable
fragmented object satisfying the properties of fragmented
coverability as deﬁned in Section II.

VI. EC-DAP OPTIMIZATION

In this section, we present an optimization in the imple-
mentation of the erasure coded DAP, EC-DAP, to reduce the
operational latency of the read/write operations in DSMM
layer. As we show in this section, this optimized EC-DAP,
which we refer to as EC-DAPopt, satisﬁes all the items in
Property 1, and thus can be used by any algorithm that utilizes
the DAPs, like any variant of ARES. We ﬁrst present the
optimaization and then prove its correctness.

A. Description

The main idea of the optimization stems from the work [4]
which avoids unnecessary object transmissions between the
clients and the servers that host the replicas.

In summary, we apply the following optimization: in the
get-data primitive, each server sends only the tag-value pairs
with a larger or equal tag than the client’s tag. In the case
where the client is a reader, it performs the put-data action
(propagation phase), only if the maximum tag is higher than its
local one. EC-DAPopt is presented in Algorithms 4 and 5. Text
in blue annotates the changed or newly added code, whereas
struck out blue text annotates code that has been removed from
the original implementation.

Following [22], each server si stores a state variable, List,
which is a set of up to (δ + 1) (tag, coded-element) pairs;
δ is the maximum number of concurrent put-data operations,
i.e., the number of writers. In EC-DAPopt, we need another
two state variables, the tag of the conﬁguration (c.tag) and its
associated value (c.val).

2

2

We now proceed with the details of the optimization. Note
that the c.get-tag() primitive remains the same as the original,
that is, the client discovers the highest tag among the servers’
replies in c.Servers and returns it.
Primitive c.get-data(): A client, during the execution of a
c.get-data() primitive, queries all the servers in c.Servers
(cid:7) servers.
for their List, and awaits responses from (cid:6) n+k
Each server generates a new list (List(cid:48)) where it adds every
(tag, coded-element) from the List, if the tag is higher than
the c.tag of the client and the (tag, ⊥) if the tag is equal
to c.tag; otherwise it does not add the pair, as the client
already has a newer version. Once the client receives Lists
from (cid:6) n+k
(cid:7) servers, it selects the highest tag t, such that:
(i) its corresponding value v is decodable from the coded
elements in the lists; and (ii) t is the highest tag seen from
the responses of at least k Lists (see lines Alg. 4:6–8) and
returns the pair (t, v). Note that in the case where any of
the above conditions is not satisﬁed, the corresponding read
operation does not complete. The main difference with the
original code is that in the case where variable c.tag is the
same as the highest decodable tag (tdec
max), the client already
has the latest decodable version and does not need to decode
it again (see line Alg. 4:10).
Primitive c.put-data((cid:104)tw, v(cid:105)): This primitive is executed only
when the incoming tw is greater than c.tag (line Alg. 4:20).
In this case, the client computes the coded elements and sends
the pair (tw, Φi(v)) to each server si ∈ c.Servers. Also, the
client has to update its state (c.tag and c.val). If the condition
does not hold, the client does not perform any of the above, as
it already has the latest version, and so the servers are up-to-
date. When a server si receives a message (PUT-DATA, tw, ci),
it adds the pair in its local List and trims the pairs with the
smallest tags exceeding the length (δ + 1) (see line Alg. 5:17).
Remark. Experimental results conducted on Emulab show
that by using EC-DAPopt over EC-DAP we gain signiﬁcant
reductions especially on read latencies, which concern the
majority of operations in practical systems (see Fig. 4 in

Algorithm 4 EC-DAPopt implementation

at each process pi ∈ I

2: procedure c.get-data()

send (QUERY-LIST,c.tag) to each s ∈ c.Servers
until pi receives Lists from each server s ∈ Sg

(cid:44)→ s.t. |Sg| =

and Sg ⊂ c.Servers

(cid:109)

(cid:108) n+k
2

∗ = set of tags that appears in k lists
dec = set of tags that appears in k lists with values

T ags≥k
T ags≥k
t∗
max ← max T ags≥k
max ← max T ags≥k
tdec
if tdec
max = t∗
max then
if c.tag = tdec
max then
t ← c.tag
v ← c.val

∗

dec

4:

6:

8:

10:

12:

14:

16:

dec (cid:54)= ⊥ then

return (cid:104)t, v(cid:105)
else if T ags≥k
t ← tdec
max
v ← decode value for tdec
max
return (cid:104)t, v(cid:105)

18: end procedure

procedure c.put-data((cid:104)τ, v(cid:105)))

20:

if τ > c.tag then

22:

code-elems = [(τ, e1), . . . , (τ, en)], ei = Φi(v)
send (PUT-DATA, (cid:104)τ, ei(cid:105)) to each si ∈ c.Servers
until pi receives ACK from
c.tag ← τ
c.val ← v
26: end procedure

(cid:108) n+k
2

24:

(cid:109)

servers in c.Servers

Algorithm 5 The response protocols at any server si ∈ S in EC-DAPopt for client requests.

at each server si ∈ S in conﬁguration ck

2: State Variables:

List ⊆ T × Cs, initially {(t0, Φi(v0))}
Local Variables:
List(cid:48) ⊆ T × Cs, initially ⊥

4: Upon receive (QUERY-LIST, tgb) si, ck from q

6:

8:

for τ, v in List do
if τ > tgb then

List(cid:48) ← List(cid:48) ∪ {(cid:104)τ, ei(cid:105)}

else if τ = tgb then

List(cid:48) ← List(cid:48) ∪ {(cid:104)τ, ⊥(cid:105)}

10:

Send List(cid:48) to q

end receive

12: Upon receive (PUT-DATA, (cid:104)τ, ei(cid:105)) si, ck from q

List ← List ∪ {(cid:104)τ, ei(cid:105)}
if |List| > δ + 1 then

τmin ← min{t : (cid:104)t, ∗(cid:105) ∈ List}

/* remove the coded value

List ← List\ {(cid:104)τ, e(cid:105) : τ = τmin ∧ (cid:104)τ, e(cid:105) ∈ List}
List ← List ∪ {(τmin, ⊥)}

Send ACK to q

end receive

14:

16:

18:

Section VII). The great beneﬁts are observed especially in
the fragmented variants of the algorithm and when the objects
are large, as read operations avoid the transmission of many
unchanged blocks.

B. Correctness of EC-DAPopt

To prove the correctness of EC-DAPopt, we need to show
that it is safe, i.e., it ensures the necessary Property 1, and
live, i.e., it allows each operation to terminate. In the following
proof, we will not refer to the get-tag access primitive that
the EC-DAP algorithm uses [22], as the optimization has no
effect on this operation, so it should preserve safety as shown
in [10].

For the following proofs we ﬁx the conﬁguration to c
the DAPs preserve Property 1 in any
as it sufﬁces that
single conﬁguration. Also we assume an [n, k] MDS code,
|c.Servers| = n of which no more than n−k
2 may crash,
and that δ is the maximum number of put-data operations
concurrent with any get-data operation.

We ﬁrst prove Property 1-C2 as it is later being used to

prove Property 1-C1.

Lemma 15 (C2). Let ξ be an execution of an algorithm
A that uses the EC-DAPopt. If φ is a c.get-data() that
returns (cid:104)τπ, vπ(cid:105) ∈ T × V, then there exists π such that π
is a c.put-data((cid:104)τπ, vπ(cid:105)) and φ did not complete before the
invocation of π. If no such π exists in ξ, then (τπ, vπ) is equal
to (t0, v0).

Proof. It is clear that the proof of property C2 of EC-DAPopt
is identical with that of EC-DAP. This happens as the initial

value of the List variable in each servers s in S is still
{(t0, Φs(vπ))}, and the new tags are still added to the List
only via put-data operations. Thus, each server during a
get-data operation includes only written tag-value pairs from
the List to the List(cid:48).

Lemma 16 (C1). Let ξ be an execution of an algorithm A
that uses the EC-DAPopt. If φ is c.put-data((cid:104)τφ, vφ(cid:105)), for
c ∈ C, (cid:104)τφ, vφ(cid:105) ∈ T × V, and π is c.get-data() that returns
(cid:104)τπ, vπ(cid:105) ∈ T × V and φ → π in ξ, then τπ ≥ τφ.

Proof. Let pφ and pπ denote the processes that invokes φ and
(cid:7) servers that
π in ξ. Let Sφ ⊂ S denote the set of (cid:6) n+k
responds to pφ, during φ, and by Sπ the set of (cid:6) n+k
(cid:7) servers
that responds to pπ, during π.

2

2

Per Alg. 5:11, every server s ∈ Sφ, inserts the tag-value
pair received in its local List. Note that once a tag is added
to List, its associated tag-value pair will be removed only
when the List exceeds the length (δ + 1) and the tag is the
smallest in the List (Alg. 5:12–14).

2

When replying to π, each server in Sπ includes a tag in
List(cid:48), only if the tag is larger or equal to the tag associated to
the last value decoded by pπ (lines Alg. 5:6–9). Notice that as
(cid:7), the servers in |Sφ ∩ Sπ| ≥ k reply to
|Sφ| = |Sπ| = (cid:6) n+k
both π and φ. So there are two cases to examine: (a) the pair
(cid:104)τφ, vφ(cid:105) ∈ Lists(cid:48) of at least k servers Sφ ∩ Sπ replied to π,
and (b) the (cid:104)τφ, vφ(cid:105) appeared in fewer than k servers in Sπ.
Case a: In the ﬁrst case, since π discovered τφ in at least k
servers it follows by the algorithm that the value associated
with τφ will be decodable. Hence tdec
max ≤ τφ and τπ ≥ τphi.

Case b: In this case τφ was discovered in less than k servers
in Sπ. Let τ(cid:96) denote the last tag returned by pπ. We can break
this case in two subcases: (i) τ(cid:96) > τφ, and (ii) τ(cid:96) ≤ τφ.

In case (i), no s ∈ Sπ included τφ in List(cid:48)

s before replying
to π. By Lemma 15,
the c.put-data((cid:104)τ(cid:96), ∗(cid:105)) was invoked
before the completion of the ∗.get-data() operation from pπ
that returned τ(cid:96). It is also true that pπ discovered (cid:104)τ(cid:96), ∗(cid:105) in
more than k servers since it managed to decode the value.
Therefore, in this case tdec

max ≥ τ(cid:96) and thus τπ > τφ.

In case (ii), a server s ∈ Sφ ∩ Sπ will not include τφ iff
|Lists(cid:48)
s| = δ + 1, and therefore the local List of s removed
τφ as the smallest tag in the list. According to our assumption
though, no more than δ put-data operations may be concurrent
with a get-data operation. Thus, at least one of the put-data
operations that wrote a tag τ (cid:48) ∈ Lists(cid:48)
s must have completed
before π. Since τ (cid:48)
servers
then |Sπ ∩ S(cid:48)| ≥ k and hence π will be able to decode the
value associated to τ (cid:48), and hence tdec
max ≥ τ(cid:96) and τπ > τφ,
completing the proof of this lemma.

is also written in |S(cid:48)| = n+k
2

Theorem 17 (Safety). Let ξ be an execution of an algorithm
A that contains a set Π of complete get-data and put-data
operations of Algorithm 4. Then every pair of operations
φ, π ∈ Π satisfy Property 1.

Proof. Follows directly from Lemmas 15 and 16.

Liveness requires that any put-data and get-data operation
deﬁned by EC-DAPopt terminates. The following theorem
captures the main result of this section.

Theorem 18 (Liveness). Let ξ be an execution of an algorithm
A that utilises the EC-DAPopt. Then any put-data or get-data
operation π invoked in ξ will eventually terminate.

Proof. Given that no more than n−k
servers may fail, then
2
from Algorithm 4 (lines Alg. 4:19–26) , it is easy to see
that there are at least n+k
servers that remain correct and
2
reply to the put-data operation. Thus, any put-data operation
completes.

2

2

Now we prove the liveness property of any get-data op-
eration π. Let pω and pπ be the processes that invokes the
put-data operation ω and get-data operation π. Let Sω be
(cid:7) servers that responds to pω, in the put-data
the set of (cid:6) n+k
(cid:7) servers that
operations, in ω. Let Sπ be the set of (cid:6) n+k
responds to pπ during the get-data step of π. Note that in
ξ at the point execution T1, just before the execution of π,
none of the write operations in Λ is complete. Let T2 denote
(cid:7)
the earliest point of time when pπ receives all the (cid:6) n+k
responses. Also, the set Λ includes all the put-data operations
that starts before T2 such that tag(λ) > tag(ω)}. Observe
that, by algorithm design, the coded-elements corresponding
to tω are garbage-collected from the List variable of a server
only if more than δ higher tags are introduced by subsequent
writes into the server. Since the number of concurrent writes
|Λ|, s.t. δ > |Λ| the corresponding value of tag tω is not
garbage collected in ξ, at least until execution point T2 in any
of the servers in Sω. Therefore, during the execution fragment

2

2

Next we want to argue that tdec

between the execution points T1 and T2 of the execution ξ,
the tag and coded-element pair is present in the List variable
of every server in Sω that is active. As a result, the tag and
coded-element pairs, (tω, Φs(vω)) exists in the List received
from any s ∈ Sω ∩ Sπ during operation π. Note that since
(cid:7) hence |Sω ∩ Sπ| ≥ k and hence
|Sω| = |Sπ| = (cid:6) n+k
tω ∈ T ags≥k
dec, the set of decode-able tag, i.e., the value vω can
be decoded by pπ in π, which demonstrates that T ags≥k
dec (cid:54)= ∅.
max is the maximum tag that
π discovers via a contradiction: we assume a tag tmax, which
is the maximum tag π discovers, but it is not decode-able, i.e.,
tmax (cid:54)∈ T ags≥k
π ⊂ S be any subset
of k servers that responds with tmax in their List(cid:48) variables
(cid:7) +
to pπ. Note that since k > n/3 hence |Sω ∩ Sk
(cid:6) n+1
π (cid:54)= ∅. Then tmax must be in some
3
servers in Sω at T2 and since tmax > tdec
max ≥ tω. Now since
|Λ| < δ hence (tmax, Φs(vmax)) cannot be removed from any
server at T2 because there are not enough concurrent write
operations (i.e., writes in Λ) to garbage-collect the coded-
elements corresponding to tag tmax. Also since π cannot have
a local tag larger than tmax, according to the lines Alg. 5:6–9
each server in Sπ includes the tmax in its replies. In that case,
tmax must be in T ag≥k
dec, a contradiction.

(cid:7) ≥ 1, i.e., Sω ∩ Sk

dec and tmax > tdec

π| ≥ (cid:6) n+k

max. Let Sk

2

VII. EXPERIMENTAL EVALUATION

Distributed systems are often evaluated on an emulation or
an overlay testbed. Emulation testbeds give users full control
over the host and network environments, their experiments are
repeatable, but their network conditions are artiﬁcial.

The environmental conditions of overlay testbeds are not
repeatable and provide less control over the experiment, how-
ever they provide real network conditions and thus provide
better insight on the performance of the algorithms in a real
deployment. We used Emulab [1] as an emulation testbed and
Amazon Web Services (AWS) EC2 [8] as an overlay testbed.

A. Evaluated Algorithms and Experimental Setup

Evaluated Algorithms. We have implemented and evaluated
the performance of the following algorithms:

• COABD. This is the coverable version of the traditional,
static ABD algorithm [7], [20], as presented in [21]. It
will be used as an overall baseline.

• COABDF. This is the version of COABD that provides
fragmented coverability, as presented in [4]. It can be
considered as a baseline algorithm of the COBFS frame-
work.

• COARESABD. This is a version of COARES that uses the
ABD-DAP implementation [22] (cf. Section III). It can
be considered as the dynamic (reconﬁgurable) version of
COABD.

• COARESABDF. This is COARESF together with the
is the fragmented

i.e.,

it

ABD-DAP implementation,
version of COARESABD.

• COARESEC. This is a version of COARES (see Section
Section IV) that uses the EC-DAPopt implementation
(see Section VI),

• COARESECF. This is the two-level striping algorithm
presented in Section V when used with the EC-DAPopt
implementation of Section VI, i.e., it is the fragmented
version of COARESEC.

Note that we have implemented all the above algorithms
using the same baseline code and communication libraries.All
the modules of the algorithms are written in Python, and the
asynchronous communication between layers is achieved by
using DEALER and ROUTER sockets, from the ZeroMQ
library [28].

In the remainder, for ease of presentation, and when ap-
propriate, we will be referring to algorithms COABD(F)
and COARESABD(F) as the ABD-based algorithms and to
algorithms COARESEC(F) as the EC-based algorithms.
Distributed Experimental Setup on Emulab: All physical
nodes were placed on a single LAN using a DropTail queue
without delay or packet loss. We used nodes with one 2.4 GHz
64-bit Quad Core Xeon E5530 “Nehalem” processor and 12
GB RAM. Each physical machine runs one server or client
process. This guarantees a fair communication delay between
a client and a server node. We have an extra physical node,
the controller, which orchestrates the experiments. A client’s
physical machine has one Daemon that listens for its requests.
Distributed Experimental Setup on AWS: For the File Sizes
and Block Sizes experiments, we create a cluster with 8 node
instances. All of them have the same speciﬁcations, their type
is t2.medium with 4 GB RAM, 2 vCPUs and 20 GB storage.
For the Scalability experiments, we create a cluster with 11
node instances. Ten of them have the same speciﬁcations,
their type is t2.small with 2 GB RAM, 1 vCPU and 20 GB
storage, and one is of type t2.medium. In all experiments one
medium node has also the role of controller to orchestrate
the experiments. In order to guarantee a fair communication
delay between a client and a server node, we placed at most
one server process on each physical machine. Each instance
with clients has one Daemon to listen for clients’ requests.

We used an external implementation of Raft [23] consensus
algorithms, which was used for the service reconﬁguration
and was deployed on top of small RPi devices. Small devices
introduced further delays in the system, reducing the speed
of reconﬁgurations and creating harsh conditions for longer
periods in the service.

For the deployment and remote execution of the experi-
mental tasks on both Emulab and AWS, the controller used
Ansible [3], a tool
to automate different IT tasks. More
speciﬁcally, we used Ansible Playbooks, scripts written in
YAML format. These scripts get pushed to target nodes, do
their work (over SSH) and get removed when ﬁnished.

B. Overview of the experiments

Node Types: During the experiments, we use four distinct
types of nodes, writers, readers, reconﬁgurers and servers.
Their main role is listed below:

• writer w ∈ W ⊆ I: a client that sends write requests to
all servers and waits for a quorum of the servers to reply.

• reader r ∈ R ⊆ I: a client that sends read requests to
servers and waits for a quorum of the servers to reply.
• reconﬁgurer g ∈ G ⊆ I: a client that sends reconﬁgu-
ration requests to servers and waits for a quorum of the
servers to reply. This type of node is used only in any
variant of ARES algorithm.

• server s ∈ S: a server listens for read and write
and reconﬁguration requests, it updates its object replica
according to the DSMM implementation and replies to
the process that originated the request.

Performance Metric: The metric for evaluating the algo-
rithms is operational latency. This includes both communi-
cation and computational delays. The operational latency is
computed as the average of all clients’ average operational
latencies. The performance of COABD shown in the Emulab
results can be used as a reference point in the following
experiments since the rest algorithms combine ideas from it.
The Emulab results are compiled as averages over ﬁve samples
per each scenario. However, the AWS results are complied as
averages over three samples for the Scalability scenario, while
the rest scenarios run only once.

C. Experimental Scenarios

2

2

Here we describe the scenarios we constructed and the
settings for each of them. We executed every experiment
of each scenario in two steps. First, we performed a boot-
up phase where a single client writes a ﬁle of a speciﬁc
initial size and the other readers and writers are informed
about it. Second, operations write and read data to this ﬁle
concurrently and we have measured the performance under
various scenarios. During all the experiments, as the writers
kept updating the ﬁle, its size increased (we generate text ﬁles
with random bytes strings).
Parameters of algorithms: The quorum size of the EC-based
(cid:7), while the quorum size of the ABD-based
algorithms is (cid:6) n+k
(cid:5) + 1. The parameter n is the total number
algorithms is (cid:4) n
of servers, k is the number of encoded data fragments, and m
is the number of parity fragments, i.e. n − k. In relation to
EC-based algorithms, we can conclude that the parameter k
is directly proportional to the quorum size. But as the value
of k and quorum size increase, the size of coded elements
decreases. Also, a high number of k and consequently a
small number of m means less redundancy with the system
tolerating fewer failures. When k = 1 we essentially converge
to replication. Parameter δ in EC-based algorithms is the
maximum number of concurrent put-data operations, i.e., the
number of writers.
Distributed Experiments: For the distributed experiments (in
both testbeds) we use a stochastic invocation scheme in which
readers and writers pick a random time uniformly distributed
(discrete) between intervals to invoke their next operations.
Respectively the intervals are [1...rInt] and [1..wInt], where
rInt, wInt = 3sec. If there is a reconﬁgurer,
it invokes
its next operation every 15sec and performs a total of 5
reconﬁgurations.
We present three main types of scenarios:

• Performance VS. Initial File Sizes: examine performance

when using different initial ﬁle sizes.

• Performance VS. Scalability of nodes under concurrency:
examine performance as the number of service partici-
pants increases.

• Performance VS. Block Sizes: examine performance un-
der different block sizes (only for fragmented algorithms).

D. Performance VS. Initial File Sizes

The ﬁrst scenario is made to measure the performance of
algorithms when the writers update a ﬁle whose size gradually
increases. We varied the fsize from 1 MB to 512 MB by
doubling the ﬁle size in each experimental run. The perfor-
mance of some experiments is missing as the non-fragmented
algorithms crashed when testing larger ﬁle sizes due to an out-
of-memory error. The maximum, minimum and average block
sizes (rabin ﬁngerprints parameters) were set 1 MB, 512 kB
and 512 kB respectively.

a) Emulab parameters: We have 5 writers, 5 readers
and 11 servers. We run twice the EC-based algorithms with
different value of parity, one with m=1 and one with m=5.
Thus, the quorum size of the EC-based algorithms with m=1 is
(cid:7) = 11, while the quorum size of EC-based algorithms
(cid:6) 11+10
2
with m=5 is (cid:6) 11+6
(cid:7) = 9. The quorum size of ABD-based
algorithms is (cid:4) 11
(cid:5) + 1 = 6. In total, each writer performs 20
2
writes and each reader 20 reads.

2

b) AWS parameters: We have 1 writer, 1 reader and 6
servers. We run twice the EC-based algorithms with different
value of parity, one with m=1 and one with m=4. Thus the
quorum size of the EC-based algorithms with m=1 is 6 , while
the quorum size of EC-based algorithms with m=4 is 4. The
quorum size of ABD-based algorithms is 4. In total, each
writer performs 50 writes and each reader 50 reads.

We measure the read and write operation latencies for
both original and the fragmented variant of algorithms; the
results can be seen on Figs. 4 and 5. As shown in Figs. 4(a)
and 5(a), the fragmented algorithms that use the FM achieve
signiﬁcantly smaller write latency, when the ﬁle size increases,
which is a result of the block distribution strategy. In Fig. 4(a),
the lines of fragmented algorithms are very closed to each
other. The fact that the COARESECF with m=1 (Fig. 5(a))
at smaller ﬁle sizes does not beneﬁt so much from the
fragmentation, is because the client waits more responses for
each block request compared to ABD-based algorithms with
fragmentation. However, the update latency exhibited in non-
fragmented algorithms appears to increase linearly with the ﬁle
size. This was expected, since as the ﬁle size increases, it takes
longer latency to update the whole ﬁle. Also, the successful ﬁle
updates achieved by fragmented algorithms are signiﬁcantly
higher as the ﬁle size increases since the probability of two
writes to collide on a single block decreases as the ﬁle size
increases (Fig. 4(a)). On the contrary, the non-fragmented
algorithms do not experience any improvement as it always
manipulates the ﬁle as a whole.

The Block Identiﬁcation (BI) computation latency con-
tributes signiﬁcantly to the increase of fragmented algorithms’

update latency in larger ﬁle sizes, as shown in Fig. 5(c).
We have set the same parameters for the rabin ﬁngerprints
algorithm for all the initial ﬁle sizes, which may have favored
some ﬁle sizes but burdened others.

As shown in Fig. 4(b), all the fragmented algorithms have
smaller read latency than the non-fragmented ones. This hap-
pens since the readers in the shared memory level transmit only
the contents of the blocks that have a newer version. While in
the non-fragmented algorithms, the readers transmit the whole
ﬁle each time a newer version of the ﬁle is discovered. This
explains the increasing curve of non-fragmented compared to
their counterpart with fragmentation.

On the contrary, the read latency of COARES in the corre-
sponding AWS experiment (Fig. 5)(b) has not improved with
the fragmentation strategy. This is due to the fact that the
AWS testbed provides real network conditions. The COARESF
read/write operation has at least two more rounds of com-
munication to perform than COABDF in order to read the
conﬁguration before each of the two phases. As we can see
in Fig. 5(d), the read-conﬁg operations of COARESABDF
during a block read operation have a stable overhead in latency.
Thus, when the FM module sends multiple read block requests,
waiting each time for a reply, the client has this stable overhead
for each block request. The average number of blocks read in
each experiment is shown in the Fig. 5(b). It is also worth
mentioning that the decoding of the read operation in EC-
based algorithms is slower than the encoding of the write as it
requires more computation. It would be interesting to examine
whether the multiple read block requests in COBFS could be
sent in parallel, reducing the overall communication delays.

EC-based algorithms with m=5, k=6 in Emulab and with
m=4, k=2 in AWS results in the generation of smaller number
of data fragments and thus bigger sizes of fragments and
higher redundancy, compared to EC-based algorithms with
m=1. As a result, with a higher number of m (i.e. smaller k)
we achieve higher levels of fault-tolerance, but with wasted
storage efﬁciency. The write latency seems to be less affected
by the number of m since the encoding is faster as it requires
less computation.

In Figs. 4(a)(b), we can additionally observe the write and
read latency of COARESEC and COARESECF (with m=5)
when EC-DAP is used instead of EC-DAPopt in the DSMM
layer. Both algorithms, when using the optimization (i.e., EC-
DAPopt) incur signiﬁcant reductions on the read latency (in
half), especially for large ﬁles. Furthermore, the write latency
of COARESEC is signiﬁcantly reduced (in half); there is no
much gain for the write latency of COARESECF, which was
expected since it is already very low due to fragmentation (the
optimization was aiming the read latency anyway).

E. Performance VS. Scalability of nodes under concurrency

This scenario is constructed to compare the read, write and
recon latency of the algorithms, as the number of service
participants increases. In both Emulab and AWS, we varied
the number of readers |R| and the number of writers |W |
from 5 to 25, while the number of servers |S| varies from

Fig. 4. Emulab results for File Size experiments.

(a)

(a)

(c)

(b)

(b)

(d)

Fig. 5. AWS results for File Size experiments.

3 to 11. In AWS, the clients and servers are distributed in a
round-robin fashion. We calculate all possible combinations of
readers, writers and servers where the number of readers or
writers is kept to 5. In total, each writer performs 20 writes
and each reader 20 reads. The size of the ﬁle used is 4 MB.
The maximum, minimum and average block sizes were set
to 1 MB, 512 kB and 512 kB respectively. For each number
of servers, we set different parity for EC-based algorithms
in order to achieve the same fault-tolerance with ABD-based
algorithms in each case, except in the case of 3 servers (to
avoid replication). With this however, the EC client has to wait
for responses from a larger quorum size. The parity value of
the EC-based algorithms is set to m=1 for 3 servers, m=2 for
5 servers, m=3 for 7 servers, m=4 for 9 servers and m=5 for
11 servers.

The results obtained in this scenario are presented in

Fig. 6 and Fig. 7 for Emulab and AWS respectively. As
expected, COARESEC has the lowest update latency among
non-fragmented algorithms because of the striping level. Each
object is divided into k encoded fragments that reduce the
communication latency (since it transfers less data over the
network) and the storage utilization. The fragmented algo-
rithms perform signiﬁcantly better update latency than the non-
fragmented ones, even when the number of writers increases
(see Figs. 6(a), 7(a)). This is because the non-fragmented
writer updates the whole ﬁle, while each fragmented writer up-
dates a subset of blocks which are modiﬁed or created. We ob-
serve that the update operation latency in algorithms COABD
and COARESABD increases even more as the number of
servers increases, while the operation latency of COARESEC
decreases or stays the same (Figs. 6(c), 7(c)). This is because
when increasing the number of servers, the quorum size grows

(a)

(b)

(c)

Fig. 6. Emulab results for Scalability experiments.

(a)

(b)

(c)

Fig. 7. AWS results for Scalability experiments.

but the message size decreases. Therefore, while both non-
fragmented ABD-based algorithms and COARESEC wait for
responses from more servers, COARESEC gains the advantage
of decreased message size. However, when going from 7 to
9 servers, we ﬁnd that there is a decrease in latency. This
is due the choice of the parity value (parameter of EC-based
algorithms) selected for 7 servers.

Due to the block allocation strategy in fragment algorithms,
more data are successfully written (cf. Fig. 6(a), 6(b)), explain-
ing the slower COARESF read operation (cf. Figs. 6(b), 7(b)).
We built four extra experiments in Emulab to verify the
correctness of the variants of ARES when reconﬁgurations
coexist with read/write operations. The four experiments differ
in the way the reconﬁgurer works; three experiments are based
on the way the reconﬁgurer chooses the next storage algorithm
and one in which the reconﬁgurer changes concurrently the
next storage algorithm and the quorum of servers. In these
experiments the number of servers |S| is ﬁxed to 11 and there
is one reconﬁgurer. All of the scenarios below are run for both
COARES and COARESF.

• Changing to the Same Reconﬁgurations: We execute
two separate runs, one for each DAP . We use only one
reconﬁgurer which requests recon operations that lead to
the same shared memory emulation and server nodes.
• Changing Reconﬁgurations Randomly: The reconﬁg-

DAPs and at
number of servers between [3, 5, 7, 9, 11].

the same time chooses randomly the

As we mentioned earlier, our choice of k minimizes the
coded fragment size but introduces bigger quorums and thus
larger communication overhead. As a result, in smaller ﬁle
sizes, ARES (either fragmented or not) may not beneﬁt so
much from the coding, bringing the delays of the COARESEC
and COARESABD closer to each other (cf. Fig. 8). However,
the read latency of COARESECF is signiﬁcant lower than of
COARESABDF. This is because the COARESECF takes less
time to transfer the blocks to the new conﬁguration.

Fig. 9 illustrates the results of COARESF experiments with
the random storage change. During the experiments, there are
cases where a single read/write operation may access con-
ﬁgurations that implement both ABD-DAP and EC-DAPopt,
when concurrent with a recon operation.

The last scenario in Fig. 10 is constructed to show that
the service is working without interruptions despite the ex-
istence of concurrent read/write and reconﬁguration oper-
ations that may add/remove servers and switch the stor-
age algorithm in the system. Also, we can observe that
COARESF (Fig. 10(b)) has shorter update and read latencies
than COARES (Fig. 10(a)).

F. Performance VS. Block Sizes

urer chooses randomly between the two DAPs.

• Changing Reconﬁgurations with different number
of servers: The reconﬁgurer switches between the two

1) Performance VS. Min/Avg Block Sizes: We varied the
minimum and average bsizes of fragmented algorithms from
8 kB to 1 MB. The size of the initial ﬁle used was set to

Fig. 8. Emulab results when Changing to the Same DAPs.

Fig. 9. Emulab results when Changing DAPs Randomly.

(a)

Fig. 10. Emulab results when Changing DAPs Alternately and Servers Randomly.

(a)

Fig. 11. Emulab results for Min/Avg Block Sizes’ experiments.

(b)

(b)

(a)

(b)

(c)

Fig. 12. AWS results for Min/Avg/Max Block Sizes’ experiments.

4 MB, while the maximum block size was set to 1 MB. In
Emulab, each writer performs 20 writes and each reader 20
reads, whereas in AWS each writer performs 50 writes and
each reader 50 reads.

a) Emulab parameters: We have 5 writers, 5 readers and
11 servers. The parity value of the EC-based algorithms is set
to 1. Thus the quorum size of the EC-based algorithms is 11,
while the quorum size of ABD-based algorithms is 4.

b) AWS parameters: We have 1 writer, 1 reader and 6
servers. The parity value of the EC-based algorithms is set
to 1. Thus the quorum size of the EC-based algorithms is 6,
while the quorum size of ABD-based algorithms is 4.

From Figs. 11(a) , we can infer in general that when larger
min/avg block sizes are used, the update latency reaches its
highest values since larger blocks need to be transferred.
However, too small min/avg block sizes lead to the generation
of more new blocks during update operations, resulting in
more update block operations, and hence slightly higher update
latency. In Figs. 11(b) , smaller block sizes require more read
block operations to obtain the ﬁle’s value. As the minimum and
average bsizes increase, lower number of rather small blocks
need to be read. Thus, further increase of the minimum and
average bsizes forces the decrease of the read latency, reaching
a plateau in the graph. This means that the scenario ﬁnds
optimal minimum and average bsizes and increasing them does
not give better (or worse) read latency. The corresponding
AWS ﬁndings show similar trends.

2) Performance VS. Min/Avg/Max Block Sizes: We varied
the minimum and average bsizes from 2 MB to 64 MB and the
maximum bsize from 4 MB to 1 GB. In Emulab and AWS, this
scenario has the same settings as the prior block size scenario.
In total, each writer performs 20 writes and each reader 20
reads. The size of the initial ﬁle used was set to 512 MB.

This scenario evaluates how the block size impacts the
latencies when having a rather large ﬁle size. As all examined
block sizes are enough to ﬁt the text additions no new blocks
are created. All the algorithms achieve the maximal update
latency as the block size gets larger (Fig 12(a)). COARESECF
has the lower impact as block size increases mainly due to the
extra level of striping. Similar behaviour has the read latency in
Emulab. However, in real time conditions of AWS, the read la-
tency of a higher number of relatively large blocks (Fig. 12(c))
has a signiﬁcant impact on overall latency, resulting in a larger
read latency (Fig. 12(b)).

VIII. CONCLUSIONS

In this paper we have presented a dynamic distributed
ﬁle system that utilizes coverable fragmented objects, which
we call COARESF. To achieve this, we preformed a non-
trivial integration of the ARES framework with the COBFS
distributed ﬁle system. When COARESF is used with a Reed-
Solomon Erasure Coded DAP we obtain a two-level striping
dynamic and robust distributed ﬁle system providing strong
consistency and high access concurrency to large objects
(e.g., ﬁles). We demonstrated the beneﬁts of our approach
through extensive experiments performed on Emulab and AWS

testbeds. Compared to the approach that does not use the
fragmentation layer of COBFS (COARES), the COARESF is
optimized with an efﬁcient access to shared data under heavy
concurrency. Based on these results, we plan to explore in
future work how to optimize our approach to enable low
overhead under read scenarios.

Below we discuss the main trade-offs that we faced during

the implementation and deployment:
Block size of FM. The performance of data striping highly
depends on the block size. There is a trade-off between
splitting the object into smaller blocks, for improving the
concurrency in the system, and paying for the cost of sending
these blocks in a distributed fashion. Therefore, it is crucial to
discover the “golden” spot with the minimum communication
delays (while having a large block size) that will ensure a
small expected probability of collision (as a parameter of the
block size and the delays in the network).
Parity of EC. There is a trade-off between operation latency
and fault-tolerance in the system: the further increase of the
parity (and thus higher fault-tolerance) the larger the latency.
Parameter δ of EC. The value of δ is equal to the number
of writers. As a result, as the number of writers increases,
the latency of the ﬁrst phase of EC also increases, since each
server sends the list with all the concurrent values. In this
point, we can understand the importance of the optimization
in the DSMM layer.

REFERENCES

[1] Emulab network testbed. https://www.emulab.net/.
[2] M.K. Aguilera, I. Keidar, D. Malkhi, and A. Shraer. Dynamic atomic
storage without consensus. In Proceedings of the 28th ACM symposium
on Principles of distributed computing (PODC ’09), pages 17–25, New
York, NY, USA, 2009. ACM.

[3] Ansible. https://www.ansible.com/overview/how-ansible-works.
[4] A.F. Anta, C. Georgiou, T. Hadjistasi, E. Stavrakis, and A. Trigeorgi.
Fragmented Object : Boosting Concurrency of Shared Large Objects. In
Proc.of SIROCCO, pages 1–18, 2021.

[5] Antonio Fern´andez Anta, Chryssis Georgiou, Theophanis Hadjistasi,
Nicolas Nicolaou, Efstathios Stavrakis, and Andria Trigeorgi. Frag-
mented objects: Boosting concurrency of sharedlarge objects. CoRR,
abs/2102.12786, 2021.

[6] H. Attiya. Robust Simulation of Shared Memory: 20 Years After.

Bulletin of the EATCS, 100:99–114, 2010.

[7] H. Attiya, A. Bar-Noy, and D. Dolev. Sharing Memory Robustly in
Message-Passing Systems. Journal of the ACM (JACM), 42(1):124–142,
1995.

[8] AWS EC2. https://aws.amazon.com/ec2/.
[9] Paul Black. Ratcliff pattern recognition. Dictionary of Algorithms and

Data Structures, 2021.

[10] Viveck R. Cadambe, Nicolas C. Nicolaou, Kishori M. Konwar,
N. Prakash, Nancy A. Lynch, and Muriel M´edard. ARES: adaptive,
reconﬁgurable, erasure coded, atomic storage. CoRR, abs/1805.03727,
2018.

[11] P. Dutta, R. Guerraoui, R.R. Levy, and A. Chakraborty. How fast can a

distributed atomic read be? In Prof. of PODC, pages 236–245, 2004.

[12] E. Gafni and D. Malkhi. Elastic conﬁguration maintenance via a
parsimonious speculating snapshot solution. Lecture Notes in Computer
Science (including subseries Lecture Notes in Artiﬁcial Intelligence and
Lecture Notes in Bioinformatics), 9363:140–153, 2015.

[13] C. Georgiou, T. Hadjistasi, N. Nicolaou, and A. Schwarzmann. Un-
leashing and speeding up readers in atomic object implementations. In
Proc. of NETYS, 2018.

[14] C. Georgiou, N. Nicolaou, and A.A. Shvartsman. Fault-tolerant semifast
implementations of atomic read/write registers. Journal of Parallel and
Distributed Computing, 69(1):62–79, 2009.

[15] M.P. Herlihy and J.M. Wing. Linearizability: a correctness condition
for concurrent objects. ACM Transactions on Programming Languages
and Systems, 12(3):463–492, 1990.

[16] M.P. Herlihy and J.M. Wing. Linearizability: A Correctness Condition
for Concurrent Objects. ACM Transactions on Programming Languages
and Systems (TOPLAS), 12(3):463–492, 1990.

[17] L. Jehl, R. Vitenberg, and H. Meling. Smartmerge: A new approach
In International Symposium on

to reconﬁguration for atomic storage.
Distributed Computing, pages 154–169. Springer, 2015.

[18] N. Lynch and A.A. Shvartsman. RAMBO: A reconﬁgurable atomic
memory service for dynamic networks. Lecture Notes in Computer
Science (including subseries Lecture Notes in Artiﬁcial Intelligence and
Lecture Notes in Bioinformatics), 2508(June):173–190, 2002.

[19] N.A. Lynch. Distributed Algorithms. Morgan Kaufmann Publishers,

1996.

[20] N.A. Lynch and A.A. Shvartsman. Robust emulation of shared memory
In Proc. of FTCS,

using dynamic quorum-acknowledged broadcasts.
pages 272–281, 1997.

[21] N. Nicolaou, A.F. Anta, and C. Georgiou. Cover-ability: Consistent
versioning in asynchronous, fail-prone, message-passing environments.
In Proc. of IEEE NCA 2016, pages 224–231. Institute of Electrical and
Electronics Engineers Inc., 2016.

[22] Nicolas Nicolaou, Viveck Cadambe, N. Prakash, Andria Trigeorgi,
Kishori M. Konwar, Muriel Medard, and Nancy Lynch. ARES: Adap-
tive, Reconﬁgurable, Erasure coded, Atomic Storage. ACM Transactions
on Programming Languages and Systems (TOPLAS), 2022.

[23] Diego Ongaro and John Ousterhout.

In search of an understandable
consensus algorithm. In Proceedings of the 2014 USENIX Conference on
USENIX Annual Technical Conference, USENIX ATC’14, pages 305–
320, Berkeley, CA, USA, 2014. USENIX Association.
[24] M O Rabin. Fingerprinting by random polynomials, 1981.
[25] M.V. Steen and A.S. Tanenbaum.
distributed-systems.net, 2017.

Distributed Systems, 3rd ed.

[26] A. Tridgell and P. Mackerras. The rsync algorithm. Imagine, 1996.
[27] P. Viotti and M. Vukolic. Consistency in non-transactional distributed

storage systems. ACM Computing Surveys (CSUR), 49:1 – 34, 2016.

[28] ZeroMQ. https://zeromq.org.

