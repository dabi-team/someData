An eﬃcient plasma-surface interaction surrogate model for

sputtering processes based on autoencoder neural networks

Tobias Gergs,1, 2 Borislav Borislavov,2, 3 and Jan Trieschmann2

1Chair of Applied Electrodynamics and Plasma Technology,

Department of Electrical Engineering and Information Science,

Ruhr University Bochum, 44801 Bochum, Germany

2Electrodynamics and Physical Electronics Group,

Brandenburg University of Technology Cottbus-Senftenberg,

Siemens-Halske-Ring 14, 03046 Cottbus, Germany

3Chair of Communications Engineering,

Brandenburg University of Technology Cottbus-Senftenberg,

Siemens-Halske-Ring 14, 03046 Cottbus, Germany

(Dated: September 7, 2021)

1
2
0
2

p
e
S
6

]
h
p
-
p
m
o
c
.
s
c
i
s
y
h
p
[

2
v
6
0
4
1
0
.
9
0
1
2
:
v
i
X
r
a

1

 
 
 
 
 
 
Abstract

Simulations of thin ﬁlm sputter deposition require the separation of the plasma and material

transport in the gas-phase from the growth/sputtering processes at the bounding surfaces (e.g.,

substrate and target). Interface models based on analytic expressions or look-up tables inherently

restrict this complex interaction to a bare minimum. A machine learning model has recently been

shown to overcome this remedy for Ar ions bombarding a Ti-Al composite target [1]. However, the

chosen network structure (i.e., a multilayer perceptron, MLP) provides approximately 4 million

degrees of freedom, which bears the risk of overﬁtting the relevant dynamics and complicating

the model to an unreliable extend. This work proposes a conceptually more sophisticated but

parameterwise simpliﬁed regression artiﬁcial neural network for an extended scenario, considering

a variable instead of a single ﬁxed Ti-Al stoichiometry. A convolutional β-variational autoencoder

is trained to reduce the high-dimensional energy-angular distribution of sputtered particles to a

latent space representation of only two components. In addition to a primary decoder which is

trained to reconstruct the input energy-angular distribution, a secondary decoder is employed to

reconstruct the mean energy of incident Ar ions as well as the present Ti-Al composition. The

mutual latent space is hence conditioned on these quantities. The trained primary decoder of the

variational autoencoder network is subsequently transferred to a regression network, for which only

the mapping to the particular latent space has to be learned. While obtaining a competitive per-

formance, the number of degrees of freedom is drastically reduced to 15,111 (0.378 % of the MLP)

and 486 (0.012 % of the MLP) parameters for the primary decoder and the remaining regression

network, respectively. The underlying methodology is very general and can easily be extended to

more complex physical descriptions (e.g., taking into account dynamical surface properties) with

a minimal amount of data required.

2

I.

INTRODUCTION

Thin ﬁlm deposition by sputtering is driven by the interplay of two diﬀerent states of

matter (i.e., solid and plasma). The length and time scales of the intrinsic processes span

several orders of magnitude, as detailed elsewhere [1–4]. Consequently, both subsystems and

their interaction with one another are commonly studied in terms of separate modeling and

computer simulation approaches. Concerning the process of sputtering (i.e., the projectile

bombardment of a target surface that leads to a collision cascade in the solid and eventually

to the release of atoms), transport of ions in matter (TRIM), Monte Carlo (MC) or molecu-

lar dynamics (MD) simulations are typically used [5–10]. The collisional transport through

the plasma (including ionization processes) as well as the plasma itself are usually described

by ﬂuid models or particle in cell/Monte Carlo collision (PIC/MCC) simulations, coupled

to test-particle method (TPM) or direct simulation Monte Carlo (DSMC) simulations, re-

spectively [11–16]. The appropriate models need to be selected based on gas pressure regime

as well as physical ﬁdelity of the models (e.g., kinetic vs continuum description). The de-

position of sputtered particles is then again either modeled by molecular dynamics, Monte

Carlo or hybrid simulations [8–10, 17–19].

Joint simulation frameworks that pursue to consistently simulate the plasma and the sur-

face dynamics, substitute detailed surface models by surrogate models for instance based on

analytical expressions (e.g., Sigmund-Thompson theory [20–22]), concentrated coeﬃcients

(e.g., sputter yield), or interpolation from look-up tables. However, these approaches in-

herently require drastic simpliﬁcations of the complex interactions (in particular in case of

reactive plasmas) [19]. In addition, establishing an interface model by manually prioritizing

and implementing all relevant interactions becomes a tedious task [23, 24].

In contrast,

machine learning models (e.g., artiﬁcial neural networks, ANNs), have been used to gener-

alize complex correlations and create surrogate models in the frame of plasma or gas-phase

interactions with surfaces [25–27]. In particular, the feasibility and accuracy of this concept

related to the prediction of energy-angular distributions (EADs) of sputtered particles as a

function of the impinging projectile ion energy distribution (IED) has been demonstrated

[1]. The required data has been gathered for Ar ion bombardment on a Ti-Al composite

surface (ﬁxed stoichiometry x = 0.5), which has been simulated with TRIDYN [7]. To mimic

scenarios with more complex processes or material systems, which require more fundamental

3

physics models (e.g., MD) and consequently bring about a higher computational load, an

intentionally low statistical quality and a small data set size of 439 samples has been chosen.

A multilayer perceptron (MLP) network type has been chosen due to its methodical sim-

plicity. Despite the ANN’s capability to generalize well, due to its large number of degrees

of freedom (i.e., approximately 4 million), the model may rather be considered suboptimal

with respect to the inherent risk of overﬁtting. This holds in particular for the targeted

scenario of even more challenging data set sizes and statistical representations.[1]

This work sets out to improve on the beforehand outlined concept in a threefold way,

namely by i) consideration of an extended data set with Ti-Al stoichiometry as an additional

surface state parameter, ii) dimensionality reduction by means of convolutional β-variational

autoencoders [28–32] conditioned on given input parameters, and iii) transfer learning of

the pretrained decoder network which exploits the established reduced parameter space.

The manuscript is structured as follows: In Section II, applied methods and parameters

are described. The data set generation which encompasses the sputtering simulations as

well as their distribution among subsets required for the machine learning approach are

introduced in Section II A. The dimensionality reduction and regression models are described

in Sections II B and II C, respectively. The corresponding results are gathered in Section III.

Finally, the work is summarized and conclusions are drawn in Section IV.

II. METHODS

In the following, it is ﬁrst explained how the considered data set is established, which

comprises input-output relations (i.e., projectile IEDs to sputtered particle EADs) from

physical sputtering simulations using TRIDYN simulations and how it is split into respective

subsets for the machine learning procedure. Second, a dimensionality reduction of the

species-dependent EADs is described based on an asymmetric convolutional β-variational

autoencoder (β-VAE). Third, a regression artiﬁcial neural network is introduced, mapping

the input to the obtained reduced output parameter space.

4

A. Data set deﬁnition, generation, and split

The data set used to train and evaluate the ANNs consists of a sequence of input tensors

and corresponding output tensors. The former resemble the energy distributions of the

particle ﬂux towards the target fAr(E) (i.e., Ar IED) as well as the Ti1−xAlx target surface

state, that is the stoichiometry x. The latter comprise the EADs of all species, that is Ti,

Al, and Ar.

IEDs fAr[k] are analytically speciﬁed and discretized at the k-th energy bin Ek similar

to [1]. In equal parts, the IED is chosen to be either a mono-energetic, a discrete Gaussian

(standard deviation σ = 20 eV) or bimodal pulsed (width ∆E = 40 eV). For all of which

an energy grid size of 10 eV is used. The mean energies of the bombarding Ar ions are

increased from 0 eV to 1,500 eV for the mentioned sequence of distribution functions. IEDs

at the boundary (0, 1,500 eV) are cropped for the sake of simplicity. This corresponds to a

data set size per stoichiometry of 450. Stoichiometry x ∈ {0.3, 0.5, 0.7} is varied for training

and evaluation, whereas x = 0 signiﬁes pure Ti. In addition, data sets with stoichiometry

x ∈ {0.2, 0.4, 0.8} are selected exclusively for post-training investigation of the generaliza-

tion capabilities of the obtained regression model (i.e., interpolation, extrapolation). These

data sets were excluded from training. Thus, the training input parameter space is ex-

plored by a total of 1,350 samples for this case study. The data set is chosen to be small

to mimic scenarios that are limited by computational resources (e.g., MD). The sputtering

distributions are simulated with TRIDYN [7], providing for each input the corresponding

output tensor of three EADs Y [i, j] with Ei ∈ [0, 30] eV in 30 steps and cos(ϑj) ∈ [0, 1] in

20 steps, equidistantly. For the proceeding training, the data is padded with zeros to obtain

data sizes corresponding to powers of 2 – speciﬁcally an output data point Y [i, j] has shape

(32 × 24 × 3). In TRIDYN, any target material is assumed to be amorphous, with a given

initial stoichiometry x and a maximum Ar incorporation of 10 %. Moreover, the collision

cascade is described by the binary collision approximation. The latter is theoretically only
valid for high kinetic energies ((cid:38) 1 keV), but TRIM based simulations have been shown to

be also applicable in cases of lower energies [33]. Detailed descriptions and reviews of the

TRIM method and its parameters can be found elsewhere [5–7, 33, 34]. The parameters used

in this study are listed in the appendix (Tables III and IV). Two choices for the number of

projectiles are chosen (i.e., 104 and 106). They are meant to resemble challenging statistical

5

EAD representations, as possibly obtained by computationally more demanding methods

for example MD, and estimating “ground truth” EADs, respectively. The absolute frequen-

cies of the latter are divided by 100 to normalize the EADs for evaluation of the network

performance with comparable absolute frequencies. The data has not been normalized or

standardized in any other way.

The data set is shuﬄed and then split into three subsets (i.e., 80 % training, 10 % val-

idation, and 10 % test set). The ﬁrst is used to compute the loss by forward propagation

of the input variables through the network, which is subsequently backward propagated.

The error gradient is accumulated over the size of the chosen mini batch, 32. All train-

able weights are then updated by a stochastic gradient descent algorithm, that is adaptive

moment estimation (Adam) [35]. After each epoch, the validation loss is used to decide

whether the learning rate has to be reduced and eventually the training is stopped. The

test set is ﬁnally used to evaluate the network’s generalization capability and, for instance,

to choose the best set of hyperparameters (HPs) in case of HP studies. Cross validation

(CV) is considered to reduce the potential bias by how the data set is randomly distributed

among its subsets. The validation set is consecutively interchanged with an equivalent sized

fraction of the training set to yield a new combination. In case of three subsets, nested CV

may be the most appropriate choice. However, the nested approach leads to a recognizable

computational burden due to all the combinatorial possibilities (e.g., one obtains K × L

evaluations for a single set of HPs and outer K-fold and inner L-fold CV). As a compromise,

the total data set is consecutively shifted through ﬁxed split markers during the CV, which

means that in the end all data has been used only once for validation and testing over the

course of a 10-fold CV.

For the ﬁnal training with optimal HPs and challenging statistics, and for comparison

with the estimated ground truth (full data set), the test set is evenly distributed among the

training and validation subsets. To obtain an integer number of CV cases, we proceed with

the same 10-fold CV and distribute the original 10 % test set for each CV run (i.e., 85 %

training, 15 % validation set with 5 % overlap of the validation set for subsequent CV runs).

By training/evaluating a corresponding ensemble of K = 10 ANNs with identical HPs, the

model conﬁguration’s total score as well as its variation and sensitivity to the presented

input data and random initialization may be assessed (detailed later).

6

B. Dimensionality reduction

The IEDs of impinging ions and the EADs of sputtered particles are described by his-

tograms with appropriately sized bins. In consequence, a relatively large number of bins

corresponding to input nodes (151 + 1 = 152 for the IED and x) as well as output nodes

(30 × 20 × 3 = 1800 for the EADs of all species) are required. Setting up the targeted regres-

sion model by means of a MLP, as previously proposed [1], inherently leads to a complex

network (high number of degrees of freedom). This may in particular become an issue when

further reducing the data size and (or) the statistical accuracy.

In contrast, a dimensionality reduction may be achieved through the concept of autoen-

coder neural networks [36]. Speciﬁcally, a β-variational autoencoder (β-VAE) is chosen to

generalize trends in the data and reduce the dimensionality at the same time. VAE networks

consist of an encoder, a reparameterization sampler, and a corresponding decoder [28, 29].

β-VAEs introduce the β factor to disentangle the latent space representation [30, 31]. The

encoder maps the EADs of all species to a low dimensional latent space representation with

a corresponding number of mean and variance values. The latter are input to the sampler

model and used to reparameterize a standard normal distribution. The obtained (pseudo-

)random samples are fed into the decoder, which pursues to reconstruct the output signal.

Passing the information through the latent space (bottle neck) favors the generalization

process, which may be further enhanced by choosing simple encoder and decoder networks.

2D convolutional layers (CLs) are utilized as hidden layers in this work. Because of their

shared parameters, CLs inherently require less trainable weights than fully-connected dense

layers (DLs). This may also be understood by the correlation of neighboring EAD bins.

In case of CLs, multiple convolutional kernels (also called channels or ﬁlters) are passed

over the input, producing the output by applying the dot product. Herein, we stick to the

convention that the data structures’ last dimension represents the number of channels of

the CL. The input ﬁeld size is preserve by padding the data with zeros at the boundaries.

The overall operation is meant to extract features from the input signal and provide them

to subsequent layers. To obtain a nonlinear relation, the information is typically passed

through an nonlinear activation function. By using a stride s > 1, the data is sampled down

(e.g., for s = 2 the kernel skips every second input node eﬀectively reducing the input ﬁeld

size by two).

7

Figure 1. Schematic of the VAE network structure.

In the following, the utilized ANNs are described. If not stated diﬀerently, the stride is

chosen to be s = 2 for any hidden CL of the encoder as well as for upsampling deconvolution

layers of the decoder, and s = 1 for the output CLs, respectively. The activation functions

for any hidden layer and output layers are set as rectiﬁed linear unit (ReLU) and linear,

respectively, if not explicitly stated otherwise.

A schematic of the VAE network is depicted in Figure 1, whereas a detailed graph is

presented in the appendix. The encoder takes as input Y [i, j], the stacked EADs with

dimensions (32 × 24 × 3). It begins with a sequence of three 2D CLs with 6, 12 and 24 ﬁlters

as well as (5 × 5), (3 × 3) and (1 × 1) kernel sizes, respectively. The ﬁlter number is doubled

to account for the skipped input nodes in both dimensions, which reduces the information

volume by a factor of two (favoring generalization). The kernel sizes are chosen to initially

extract coarse and later ﬁne features, while also respecting the particular input width and

height of each layer. The extracted features (4 × 3 × 24) are ﬂattened (288) and interpreted

by two dense output layers, each matching the latent space dimensions nl. The ﬁrst of these

DLs represents the array of means µL(Y ) and the second the array of standard deviations

σL(Y ) of nl normal distributions.

Stochastic backpropagation is enabled via the so-called reparameterization trick [28, 30].

Speciﬁcally, a dedicated (pseudo-)random sampling input layer is implemented, which sam-

ples from a standard normal distribution. Its samples are scaled by the standard deviations

σL(Y ) and shifted by the mean values µL(Y ), previously output by the encoder network. It

therefore redeﬁnes the randomness as an input to the model.

The decoder is often chosen to be an equivalent counterpart to the encoder. However,

8

here, an asymmetric β-VAE is found to be superior to a symmetric one. The structure and

topology of the primary decoder is as follows: A single input layer takes nl latent space

coordinates (e.g., generated from the sampling layer). A subsequent DL with 288ca nodes is

used to establish the corresponding initial set of features. The hyperparameter ca is meant

to boost the asymmetry by enabling the decoder to set up more features than the encoder

provides. Thereafter, the output is reshaped to match the width and height of the encoder’s

last CL output. Third, a sequence of three transposed 2D CLs with 24, 12 and 6 ﬁlters as

well as (3 × 3), (5 × 5) and (5 × 5) kernel sizes are used, respectively. As for the output CL,

3 ﬁlters (for 3 species) and a (1 × 1) kernel is utilized in combination with the absolute value

of the linear activation function. The last prohibits the occurrence of negative absolute

frequencies in the predicted/reconstructed EADs Y (cid:48)[i, j]. Here and throughout the text,

primes denote predictions by the neural network.

In addition, a secondary decoder is introduced. This decoder is meant to reconstruct the

mean of the input IED µ(cid:48)

E and the stoichiometry x(cid:48) from the latent space, and to condition
the latter. The submodel consists of two hidden DLs with nid nodes, and two output DLs,

each with 1 node.

Losses are deﬁned by the Kullback-Leibler-Divergenz (KL) loss as a function of the en-

coder output [28, 29], and the weighted mean squared error (MSE) as reconstruction loss

function of both decoder outputs. The weights for the reconstruction of the EADs Y (cid:48) and

the stoichiometry x(cid:48) are 1, whereas 10−6 is used for the mean of the IED µ(cid:48)

E. The β factor of
the β-VAE is used to weight the KL loss with respect to the MSE, disentangling the latent

space representation [30, 31]. Simulated annealing is used over the course of 100 epochs

to gradually increase β from 0 to its target value per batch. The latter is determined in a

hyperparameter study (detailed in Section III A 1).

The coeﬃcient of determination R2 = 1 − SSres
SStot

is considered as an additional metric to

judge the network’s performance with SSres and SStot being the sum of squared residuals

and total sum of squares, respectively. A value of unity indicates fully explained residuals.

The MSE and R2 for multiple outputs are taken as their sum and average, respectively.

Individual metrics are marked by the particular subscript (e.g., MSEEAD).

The learning rate is ﬁxed at 10−3 during the simulated annealing of β. Afterwards, the

learning rate is reduced by 2 whenever the validation loss does not improve on its previous

minimum value by more than 10−3 (less than 0.1 % of the typically observed loss) over the

9

Figure 2. Schematic of the regression network structure. The primary decoder is transferred from

the previously trained β-VAE network and not trainable.

course of 10 epochs. When the validation loss does not improve by more than 10−3 for 25

epochs, early stopping terminates the training phase.

C. Regression model

To ﬁnally establish a regression model, input variables need to be mapped to the VAE’s

latent space. Input for the regression are the discretized Ar IED fAr[k] and the stoichiometry

x of the surface composite. The decoder predicting the EADs Y (cid:48)[i, j] is taken from the β-

VAE, while its weights are set to be non-trainable. Thus, the learning progress from the

preceding step is transferred. A schematic of the combined network is depicted in Figure 2.

The input fAr[k] is initially downsampled with a sequence of six 1D CLs with kernel

sizes and strides of (3) and 2, respectively. The initial channel depth is chosen to be 1 (for

a single species: Ar+) and is consecutively doubled up to the m-th subsequent layer (m

being a hyperparameter), fully compensating the decreasing channel width by increasing

its number. For the remaining layers, the number of ﬁlters is kept constant, halving the

information volume per layer. The output features of are ﬂattened, concatenated with the

input x and processed with a single hidden DL. The number of output nodes of this layer is

selected as the scaled sum of the preceding and following layers’ nodes, cs(of + 1 + nl). The

scaling factor cs is taken into account as a hyperparameter. The ﬁnal output of the mapper

is implemented by a DL with nl nodes and a linear activation function.

MSE and R2 are used as loss function and additional metric, respectively. The learning

rate and termination of the training process is maintained as described in the preceding

Section II B.

10

hyperparameter

nl

β

ca

nid

m

cs

L2

range

[1, 2, 3]

[0.0, 0.1, 0.5, 1.0]

[1.0, 1.5, 2.0, 2.5, 3.0]

[2, 4, 8, 16]

[0, 1, 2, 3, 4, 5, 6]

[0.5, 1.0, 1.5, 2.0]

[0.0, 10−5, 10−4, 10−3, 10−2]

Table I. Parameter range for HP studies of the β-VAE and the regression ANN to optimize dimen-

sionality reduction and generalization. HPs are described in Sections II B and II C.

All ANNs are set up with the TensorFlow framework 2.4.1 and the Keras API included

therein [37, 38].

III. RESULTS

This section begins by studying the optimization of the dimensionality reduction and is

followed by the presentation as well as discussion of the latent space. Subsequently, diﬀerent

network conﬁgurations are studied to obtain the best regression model, whose predictions

are eventually compared with the estimated ground truth.

A. Dimensionality reduction and latent space representation

1. Hyperparameter study

A convolutional β-VAE is employed in a HP study to set up the (presumably) most

optimal network for the reduction of the EAD parameter space to the low-dimensional

latent space. To obtain the best network structure for the given task, a variation of the

latent space dimensions nl, KL loss factor β, asymmetry boost factor ca for the primary

decoder and number of nodes nid per hidden layer of the secondary decoder is investigated.

The total range of HPs are listed in Table I. For computational exploration, the complete

HP space is iterated over in a nested grid-based scheme. To facilitate an intuitive inter-

11

pretation of the results, however, the discussion is restricted to only the best combinations

given a respective HP to be varied. These are examined one after another. The remaining

HPs are chosen to minimize a combined metric (CM),

CM = µMSE + σMSE + cR2(1 − µR2 + σR2),

(1)

with the MSE mean µMSE, MSE standard deviation σMSE, R2 mean µR2, R2 standard devia-

tion σR2 for the 10-fold CV, and the factor cR2 being chosen as 4 to account for the diﬀerent

scales of the metrics. This provides an upper bound for the CM. The contribution of the KL

loss MSEKL is neglected with respect to the reconstruction loss, when evaluating the model

performances.

For all ﬁgures in this section, µMSE ± σMSE and µR2 ± σR2 are displayed by the particular

error bars. Transparent regions are meant to guide the eye.

In Figure 3a, the MSE as well as the R2 metric are shown for HP sets {1, 1.0, 2.0, 16},

{2, 0.1, 3.0, 16} and {3, 0.0, 1.0, 16} as {nl, β, ca, nid}. As apparent from the variation of

nl ∈ {1, 2, 3}, a 1D latent space is too restricted to allow for a reasonable reconstruction.

Although nl = 3 improves the overall VAE performance slightly, the 2D latent space is

favored to obtain a more reduced parameter space and, hence, a simpler regression problem.

This is signiﬁed by the mean value of R2 (µR2 = 0.901) as well as its standard deviation

(σR2 = 0.015) and indicates that the model has learned to generalize successfully.

Second, the KL loss factor β ∈ {0.0, 0.1, 0.5, 1.0} is varied, while nl = 2 is kept constant.

The results are displayed in Figure 3b for HP sets {0.0, 3.0, 16}, {0.1, 3.0, 16}, {0.5, 1.0,

16} and {1.0, 3.0, 16} as {β, ca, nid}. The β-VAE reduces to an ordinary AE for β = 0.

This case provides the smallest reconstruction loss (MSE) at the cost of the lowest R2 mean

and highest R2 standard deviation. Increasing β to 0.1 slightly worsens the reconstruction

loss, but yields an important improvement regarding the coeﬃcient of determination. This

emphasizes the impact of the statistical variation on the training success. Further increments

of β to 0.5 or 1.0 lead to worse-performing networks. Hence, β = 0.1 is considered here to

resemble the best trade oﬀ between R2 and MSE.

In the following, the latent space dimension nl = 2 and the KL loss factor β =

0.1 are maintained.

In Figure 3c the variation of the asymmetry boost factor ca ∈

{1.0, 1.5, 2.0, 2.5, 3.0} for the primary decoder is shown for nid = {16, 8, 16, 16, 16}. The

performance is relatively insensitive with regard to this quantity. Hence, ca = 1.0 is chosen

12

(a) Latent space dimension nl.

(b) KL loss factor β.

(c) Asymmetry boost factor ca.

(d) Number of nodes nid for the secondary decoder.

Figure 3. HP optimization for the β-VAE. Other HPs are chosen to minimize the CM deﬁned in

Eq. (1). The MSE and R2 error bars equal µMSE ± σMSE and µR2 ± σR2, respectively.

to minimize network complexity.

At last, for nl = 2, β = 0.1, and ca = 1.5, the number of nodes nid per hidden layer of

the secondary decoder are varied. The corresponding MSE and R2 metrics are presented

13

metric

µ (.) σ (.) µg (.) σg (.)

MSE

3.591 0.111 0.226 0.119

MSEEAD 3.580 0.109 0.217 0.116

MSEµE

0.009 0.002 0.008 0.003

MSEx

0.001 0.000 0.001 0.000

MSEKL

0.455 0.008 0.452 0.014

R2

R2

EAD

R2
µE
R2
x

0.901 0.014 0.970 0.011

0.798 0.017 0.985 0.008

0.949 0.006 0.955 0.017

0.957 0.020 0.968 0.008

Table II. MSE and R2 metrics for the β-VAE, HPs being nl = 2, β = 0.1, ca = 1.0 and nid = 16.

Metrics with subscript “g” are evaluated against the estimated ground truth data for which the test

sets have been evenly distributed among the training and validation set as detailed in Section II A.

in Figure 3d. A minimum network complexity is apparently required, that is nid ≥ 8.

Further increasing its value to 16 provides a competing and even better performance (i.e.,

µMSE = 0.901, σMSE = 0.014) in comparison to 8 (i.e., µMSE = 0.891, σMSE = 0.023).

The ﬁnal set of HPs (nl = 2, β = 0.1, ca = 1.0, nid = 16) corresponds to 2,584 trainable

weights for the encoder, 15,111 for the primary decoder and 354 for the secondary decoder.

Thus, the β-VAE is built with 18,049 degrees of freedom. The individual contributions to

the MSE and R2 are listed in Table II and indicate a reasonable reconstruction of the EADs.

However, a reliable assessment is limited by the statistics of the test data. A well-generalizing

prediction (which is expected to be comparable to the ground truth) intrinsically deviates

from the test data in terms of the excluded erroneous ﬂuctuations. Consequently, the cor-

responding noise level deﬁnes the limits of the obtained metrics, speciﬁcally MSEEAD ≈ 3.6

and R2

EAD ≈ 0.8. This statistical uncertainty is solely contained in the EADs, not the IED

or the stoichiometry.

In particular, the secondary decoder is determined to reliably interpret the latent space

as indicated by the respective individual metrics (e.g., R2
µE

x ≈ 0.95). This property will
be used in the proceeding analysis to interpret the obtained low-dimensional latent space. A

≈ R2

detailed analysis of the attainable metrics based on the estimated ground truth is provided

14

Figure 4. Latent space (z1, z2) representation of the encoded EADs of sputtered species using the

β-VAE. The stoichiometry x(cid:48) and the mean of the IED µ(cid:48)

E are reconstructed by the secondary

decoder.

in Section III C, whereas the relevant values are already contained in Table II.

2. Latent space

In this section, the low-dimensional latent space representation is studied for the complete

data set (with challenging statistics). This analysis utilizes the optimal HPs as determined

in the previous section. Depicted in Figure 4 is the 2D latent space with coordinates (z1, z2)

for nl = 2 dimensions. The plotted samples correspond to the individual data points,

whereas the distribution of samples corresponds to the encoder projection after training,

and the color coding is the corresponding interpretation of the secondary decoder (µ(cid:48)

E, x(cid:48)).
A number of aspects are noteworthy regarding its interpretation: (1) The displayed latent

space corresponds to what is input to both decoder networks, that is, compressed by the

encoder and statistically evaluated by the sampling layer using the nl mean values µL(Y )

and standard deviations σL(Y ). Therefore, the given markers entail the respective statistical

variation. (2) This aspect is intrinsic to VAE neural networks and is the reason for their

generalization properties. The latent space is eﬀectively explored in between the original

15

samples with the obtained standard deviations σL(Y ). Hence, this facilitates improved in-

terpolation capabilities in between training samples.

It is for instance seen in the lower

plot of Figure 4 presenting the reconstructed stoichiometry x(cid:48). While training is restricted

to x ∈ {0.3, 0.5, 0.7}, the secondary decoder correctly identiﬁes intermediate values. (3)

The KL divergence loss imposes a constraint to attract samples toward a standard normal

distribution. Consequently, the β scaling factor balances between making the latent space

more compact and giving it the freedom to adapt to the reconstruction loss. β = 0 corre-

sponds to a pure reconstruction loss eﬀectively eliminating any latent space regularization.

(4) Irrespective of the choice of β, it is noticeable that the obtained representation mainly

distinguishes a varying stoichiometry at large mean ion energies µ(cid:48)

E (as seen from the upper
plot of Figure 4). At low energies the diﬀerent cases overlap and are no more separable.

This is even the case when considering only the latent distributions’ mean values µL(Y ) (i.e.,

ignoring σL) for all data points (not shown), which present narrow lines that ambiguously

overlap at low energies. It suggests excellent generalization properties at the cost of a limited

degree of certainty and is typical for the proposed β-VAE. This is an aspect that is revisited

in Section III B 2.

B. Regression model

The ﬁnal regression problem to obtain a mapping between the inputs fAr[k] and x to

the output Y (cid:48)[i, j] is drastically simpliﬁed by the dimensionality reduction, provided by the

β-VAE. Speciﬁcally, fAr[k] and x only have to be mapped to the latent space z, discussed in

detail in the preceding section. The corresponding mapping from latent space z to output

EADs Y (cid:48)[i, j] is predeﬁned with the previously trained primary decoder, which is trans-

ferred while its weights are set non-trainable. The training of the mapper network is again

optimized through a HP study as discussed initially. In the following, the mapping to the

latent space is then discussed with respect to the trained representation of the β-VAE model.

Finally, the regression model is completed and its prediction is compared to the estimated

ground truth.

16

(a) m-th convolutional layer.

(b) cs factor (hidden dense layer).

Figure 5. HP optimization for the regression model. Other HPs are chosen to minimize the CM

deﬁned in Eq. (1). The MSE and R2 error bars equal µMSE ± σMSE and µR2 ± σR2, respectively.

1. Hyperparameter study

To obtain the most appropriate ANN for the present problem, a variation of the network

complexity (i.e., m and cs) is considered. The number of channels of consecutive CLs

are doubled up to the m-th layer and kept constant afterwards. The factor cs is meant

to adjust the number of nodes for the DL as a function of extracted features and latent

space dimensions. m ∈ {0, 1, 2, 3, 4, 5, 6} and cs ∈ {0.5, 1.0, 2.0} are varied, respectively.

Both quantities are described in detail in Section II C. The same procedure as outlined in

Section III A 1 is applied and a single HP variation after another is examined, while the

remaining HPs are selected to minimize the CM deﬁned in Eq. (1) over the course of a

10-fold CV. Also for the following ﬁgures, µMSE ± σMSE and µR2 ± σR2 are presented by error

bars. Transparent regions are meant to guide the eye.

In Figure 5a, the MSE and R2 metric are presented for {0, 2.0}, {1, 1.0}, {2, 1.5}, {3,

2.0}, {4, 1.5}, {5, 0.5} and {6, 1.0} as {m, cs}. The minimal complexity for a reasonable

nonlinear projection is satisﬁed for m = 1, whereas m = 2 is considered as the optimal

value. This means that the number of channels starting at a value of 1 is consecutively

17

doubled up to the second CL and remains constant with a value of 4 for all remaining

CLs. This can be reasoned by the balance between feature extraction and data compression

through convolution: With kernel size (3) and stride 2, after the second CL already an energy

range of 70 eV of the initial input ﬁeld is covered – locally integrating the information of 7

bins, due to the respective overlap. It corresponds closely to the predeﬁned features of the

IED fAr[k], which span about 80 eV maximum in comparison. Therefore, subsequent CLs

merely compress the extracted data requiring no additional compensation of the information

volume.

Second, the cs factor is varied for m = 2. The resultant metrics are displayed in Figure 5b

and reveal that the simplest hidden DL considered (i.e., cs=0.5) is insuﬃcient for the targeted

regression task. However, cs ∈ {1.0, 1.5, 2.0} yield a similar performance, so that the least

complex network (i.e., cs = 1) is chosen. This means that the number of nodes for the

hidden DL is the average of the nodes of the preceding (12 extracted IED features and 1

stoichiometry) as well as following (2 latent space dimensions) layer, resulting in 15 nodes.

The mapper network is consequently constructed with 486 trainable weights for the ﬁnal set

of HPs. The corresponding mean and standard deviation of the R2

EAD metric are µR2 = 0.802
and σR2 = 0.017 as well as µMSE = 3.506 and σMSE = 0.123 for the MSEEAD. While this

again hints to a successful regression, a deﬁnite conclusion cannot be drawn as the obtained

metrics are limited by the statistical quality of the test data (cf. Section III A 1). This

aspect will be displayed more clearly in the context of the ground truth metrics presented

in Section III C.

2. Latent space

An additional indication of the training success can be obtained from an analysis of the

latent space that the mapper network projects into. Again the complete data set (with

the challenging statistical quality used for training) is assessed, whereas the input samples

(fAr[k] and x) are directly projected to (z1, z2) coordinates. Figure 6 shows the corresponding

latent space distribution with the IED mean µE and stoichiometry x as color indication. As

neither the input data nor the projection procedure is subject to any statistical variation,

the deﬁnite input values and not the interpretation from the secondary decoder (µ(cid:48)

E, x(cid:48)) are
used for color coding. Notably, all diﬀerent stoichiometries are individually mapped into

18

Figure 6. Latent space (z1, z2) representation of the mapped EADs.

narrow regions, well-discriminated from one another. From a comparison with Figure 4,

their distribution over the latent space and the corresponding color coding (for both µE and

x) closely follows the distribution obtained from the β-VAE. However, a striking diﬀerence

is observed for low mean ion energies µE. In this region the mapper is able to provide a

clear separation for a varying stoichiometry, in stark contrast to the β-VAE. Due to this

separation property and the circumstance that the mapper can adapt to the laid out latent

space distribution, the complete regression network is so eﬀective in the prediction of EADs

for diﬀering stoichiometry. This is further detailed in the subsequent section.

C. Comparison with the estimated ground truth

To verify the hypotheses previously stated, the β-VAE and the regression ANN perfor-

mance and their generalization properties can be evaluated by comparison with the estimated

ground truth. As described at the end of Section II A, the quality of prediction is quanti-

tatively assessed through an ensemble of ANNs with identical HPs. At this point, however,

their MSE and R2 mean values and standard deviations are obtained with reference to the

estimated ground truth data set. Speciﬁcally, the complete data set with a total of 1,350

data samples is used to score the ﬁnal ANNs. Notably, the ground truth data is not re-

19

quired for the training procedure, but its metrics are merely presented here to quantitatively

illustrate the success of our proposed approach.

For the β-VAE, a signiﬁcant diﬀerence between the metrics with challenging statistical

quality and the estimated ground truth metrics included in Table II is immanent. The

MSEEAD mean value µMSE = 0.217 is more than an order of magnitude smaller, while

the corresponding µR2

EAD

value µR2 = 0.985 approaches unity. This conﬁrms the previous

hypothesis that already the β-VAE generalizes well, with a performance comparable to the

estimated ground truth. Consequently, the statistical noise inherent to the training data is

successfully mitigated. This property is most relevant when only limited statistical quality

data is attainable.

It is exploited by the regression model which utilizes its pretrained

decoder network.

The particular values for the regression model (i.e., combined mapper and decoder net-

works with ﬁxed weights) are µMSE = 0.125 and σMSE = 0.059 for MSEEAD as well as
µR2 = 0.991 and σR2 = 0.004 for R2
pared to its counterpart with challenging statistical quality, whereas R2

EAD. Again the MSEEAD is signiﬁcantly lowered com-
EAD is improved and
approaches unity. This signiﬁes that the complete model has indeed learned to diﬀerentiate

between noise and relevant physical features. The latter are eﬀectively extracted and made

available by the regression model. To provide an estimate of the computational eﬀort re-

quired for inference, the evaluation time has been measured on a laptop computer equipped

with an Intel i7-10510U CPU. The mean value and the standard deviation of the ANN

prediction time for a single EAD is 0.310 ms and 0.002 ms, respectively.

In the following, the prediction quality is further assessed in detail for an exemplary case,

speciﬁcally a Gaussian Ar IED with a mean of 1090 eV and standard deviation of 20 eV

bombarding a Ti0.7Al0.3 composite target. The corresponding EADs for all sputtered species

(i.e., Al, Ar and Ti) with i) the challenging statistical representation, ii) the prediction of

the ANN, and iii) the estimated ground truth are shown in Figure 7. The network clearly

predicts the correct output distribution, which can be hardly distinguished from the ground

truth, despite the noisy training reference. Smooth distributions are obtained, whereas

characteristic features like the lack of Ar atoms with energies less than a few eV, or the

energy peak of the Sigmund-Thompson energy distribution are contained and preserved.

The magnitudes of the predicted distributions assessed through integrated quantities

provide further conﬁrmation with reference to the ground truth.

Integrating once over

20

Figure 7. Energy angle distributions per sputtered species (i.e., Al, Ar and Ti) for a Gaussian

Ar IED with a mean of 1090 eV and standard deviation of 20 eV bombarding a Ti0.7Al0.3 com-

posite target. The instance of the training set with its challenging statistics, the prediction of the

regression model as well as the estimated ground truth are displayed.

the polar angle (energy), corresponding energy (angular) distributions are obtained. Both

are presented in Figure 8. Evidently, an excellent agreement of the prediction and the

ground truth is demonstrated with only minimal deviations detectable by visual inspection.

This observation further manifests when integrating over the remaining energy (polar angle)

21

(a) Energy distribution.

(b) Angle distributions.

Figure 8. Energy and angle distributions per sputtered species (i.e., Al, Ar and Ti) for a Gaussian

Ar IED with a mean of 1090 eV and standard deviation of 20 eV bombarding a Ti0.7Al0.3 composite

target. The prediction of the regression model is compared to the estimated ground truth.

coordinate and normalizing the result to the number of impinging ion projectiles. It gives

the sputtering yield YS for each species S separately. Whereas the regression model predicts

yields of YAl = 0.292, YAr = 0.105, and YTi = 0.593 for Al, Ar and Ti, respectively, the

corresponding ground truth values are YAl = 0.293, YAr = 0.102, and YTi = 0.590.

Its

agreement to the second signiﬁcant digit demonstrates a reliable physical description for the

given sputtering scenario.

To obtain a global trend of the model behavior, the yields YS of all species and for all

considered IEDs as well as chemical compositions are shown in Figure 9 as a function of the

IED mean energy µE, along with a comparison to the ground truth. As apparent there is

an overall agreement with all curves closely overlapping. The yield is only slightly oﬀ with a

maximal absolute error of 0.054 and a mean absolute error of 0.004. However, there also is

a noticeable deviation for µE ≈ 0. This may be caused by a certain bias entailed in the data

set (i.e., the signal to noise ratio changes varying energy). However, more probable it may

be attributed to the beforehand outlined discrepancy of the latent space representations of

the β-VAE and regression model for low energies. Aside from the stoichiometry distinction,

22

Figure 9. Yield per sputtered species (i.e., Al, Al and Ti) as a function of the mean of any

considered IED and stoichiometry.

the regression model maps those inputs to a latent space region (i.e., z2 ∈ [−3.76, −3.27])

which has not been seen by the primary decoder during its training as part of the β-VAE.

Moreover, the regression model has to adapt diﬀerent numerical values for all species at

high energy, while at low energy the yields all approach zero. The shape of the distribution

also changes, which needs to be captured regardless (cf. the surface binding energies are

correlated with the EAD maxima). For the steady state situation, this may addressed

by transforming the data set a priori and considering yields normalized to the relative

concentration xS of that species in the solid, which eases the regression problem. Due to ﬂux

balance constraints, YS/xS has to equal out in this case. However, the corresponding values

xS transiently change in the dynamic case, rendering this approach infeasible. Consequently,

it has not been pursued in the present work to not limit the analysis to such speciﬁc cases.

Finally, the capabilities of the regression model to successfully apply interpolation (and

possibly extrapolation) in previously not trained cases is investigated. This is studied by tak-

ing additional Ti1−xAlx composite target stoichiometries x ∈ {0.0, 0.2, 0.4, 0.6, 0.8, 1.0} for

the ground truth data set into account. The network has been trained on x ∈ {0.3, 0.5, 0.7}

only. The corresponding MSEEAD and R2

EAD metrics are summarized in Figure 10. A clear
trend is visible for both metrics: i) The regression ANN is most suitable to successfully per-

23

Figure 10. The loss metrics are displayed as a function of the stoichiometry x of the Ti1−xAlx

composite target. The MSE and R2 error bars equal µMSE ± σMSE and µR2 ± σR2, respectively.

form the interpolation task x ∈ {0.4, 0.6} with a consistent low MSE and nearly unnoticeable

inﬂuence on R2.

ii) In contrast, the proposed model has only a very limited capability to

extrapolate into parameter regions outside the training range x ∈ {0.0, 0.2, 0.8, 1.0}. While

it gives practically useless results far away from the trained domain, it gives better but less

certain results close to the training domain. This is attributed to the models ability to gen-

eralize, paired with the β-VAE approach, eﬀectively statistically sampling also outside the

original parameter space (given the respective latent space standard deviation σL). Conse-

quently, extrapolation is usually discouraged in the frame of machine learning. Notably, the

MSE standard deviation σMSE from the ensemble of ANNs with identical HPs (indicated by

the error bars in Figure 10) provides a metric for the uncertainty of the prediction. This is,

for instance, utilized in the context of active learning to initiate the calculation of additional

data samples when the uncertainty is above a predeﬁned threshold [25].

IV. CONCLUSION

This work aims to extend the procedure proposed in [1] to a wider range of surface

materials, while making it more robust against data limitations (e.g., when using MD). In

24

particular, the stoichiometry x of the Ti-Al composite as the surface state is included as

an additional input parameter. Most importantly, a β-VAE network structure is proposed,

which remedies the limitations of the original MLP. The latter consists of approximately 4

million degrees of freedom, which potentially complicate the model to an unnecessary and

unreliable extent. In contrast, the employed regression model consists of a total of 15,597

weights (0.390 % of the MLP). Out of which 15,111 weights (0.378 %) belong to the EAD

decoder, which is trained as part of the β-VAE. Afterwards, these weights are set non-

trainable and the learning progress is transferred by reusing the EAD decoder in the frame

of the mapper/decoder regression model. Therein, only 486 weights (0.012 %) have to be

trained for the targeted regression task.

The convolutional β-VAE is trained to reduce the dimensionality of the EADs (30 energy

bins × 20 angle bins × 3 species), to a 2 dimensional latent space. In addition to the typical

encoder–decoder pair, a secondary decoder is introduced to condition and simultaneously

interpret the obtained latent space in terms of the incident IED’s mean energy µ(cid:48)

E and Ti-Al
stoichiometry x(cid:48). The utilized set of hyperparameters that minimizes the reconstruction loss

while keeping the model as simple as possible is determined by a hyperparameter study. As

worked out in the discussion of the latent space, the β-VAE trains the model not only at

the given data points, but also in their vicinity. The utilized concept is partially similar to

a conditional VAE [32, 39]. Therein, the secondary decoder is omitted, while the encoder

and the decoder are both provided with the conditional input information. Simultaneously,

the reconstructed output variables are reduced and subsequently reconstructed. Hence, the

latent space is aﬀected by the input variables in both models in diﬀerent ways. Moreover,

the conditional VAE is a regression model itself, while the here presented β-VAE is meant

to reduce the EAD dimensionality. The obtained encoder/decoder networks are prevalently

used to project into/from the low-dimensional latent space, while regression is achieved via

a data-minimalistic mapping through a dedicated ANN component.

The assembled regression model has to be trained to map the input variables to the

latent space. This network has been optimized through a corresponding hyperparameter

study. A ﬁnal test on the estimated ground truth yields a mean µR2 = 0.991 and a standard

deviation σR2 = 0.004 for the coeﬃcient of determination over a 10-fold cross validation.

The corresponding values for the complete data set including the unknown interpolation

stoichiometries x = {0.4, 0.6} are µR2 = 0.987 and σR2 = 0.004. This excellent agreement

25

of the EADs over the complete data set as well as for the exempliﬁed case of the integrated

energy distribution, angular distribution and yield supports that the model is able to describe

the sputtering process for a wide range of IED as well as Ti-Al composites. Notably, it is

in general not feasible to estimate the ground truth (e.g., when using MD simulations or

experimental data). The ground truth data is not required for the training procedure, but

is useful for assessing its performance.

The mean value and the standard deviation of the time required by the regression network

to predict a single EAD are found to be 0.310 ms and 0.002 ms, respectively, on a laptop

computer equipped with an Intel i7-10510U CPU. Thus, it can be readily applied in the

frame of gas-phase simulations as an interface model, as outlined elsewhere [1]. Application

to more sophisticated surface states described by more than only its stoichiometry, as well

as data-limited reactive molecular dynamics simulations potentially causing an even more

challenging statistical representation have to be addressed in a future work. This is suggested

to establish a plasma-surface interface model for complex materials and system dynamics.

Additionally, the options to include inherent physical constraints (e.g., ﬂux balance in the

transient situation with varying stoichiometry) are suggested for exploration.

ACKNOWLEDGEMENT

The authors sincerely thank Professor Dr.-Ing. Thomas Mussenbrock from Ruhr Univer-

sity Bochum for his support. The authors thank Professor Dr. Wolfhard M¨oller from Insti-

tute of Ion Beam Physics and Materials Research, Helmholtz-Zentrum Dresden-Rossendorf

(HZDR) for permission to use the TRIDYN simulation software. Funded by the Deutsche

Forschungsgemeinschaft (DFG, German Research Foundation) – Project-ID 138690629 –

TRR 87 and – Project-ID 434434223 – SFB 1461.

ORCID IDS

T. Gergs: https://orcid.org/0000-0001-5041-2941

B. Borislavov: https://orcid.org/0000-0001-7753-4156

J. Trieschmann: https://orcid.org/0000-0001-9136-8019

26

APPENDIX

Table III. General TRIDYN parameters.

parameter

symbol

value

no. of projectiles

total ﬂuence

angle of irradiation

max. depth

depth interval

Ar max. atomic fraction

Nsp

Φtot

ϑ0

xmax

∆x

xAr

104 and 106
1 ˚A−2

0

600 ˚A

3 ˚A

0.1

27

Table IV. Element speciﬁc TRIDYN parameters. εs,t denotes the surface binding energy matrix

elements, where index combinations s, t (cid:15) {Ar, Al, Ti} iterate over the individual contents of each

target component, Qus is the respective initial atomic fraction, and ns the atomic density.

element εs,Ar (eV) εs,Al (eV) εs,Ti (eV) Qus

ns (˚A−3)

Ar

Al

Ti

0

0

0

0

3.36

4.12

0

4.12

4.89

0

x

0.0249

0.0602

1 − x

0.0558

Figure 11. Schematic of the β-VAE network structure. The shape of the data output by the

intermediate layers is indicated at the data pictographs. The artiﬁcial neural network operations

are indicated by colored arrows, whereas the convolutional operations are detailed above the arrows

by the corresponding kernel size k, number of ﬁlters f , and stride s, for example (k × k × f, s).

28

Figure 12. Schematic of the regression network structure. The shape of the data output by the

intermediate layers is indicated at the data pictographs. The artiﬁcial neural network operations

are indicated by colored arrows, whereas the convolutional operations are detailed above the arrows

by the corresponding kernel size k, number of ﬁlters f , and stride s, for example (k × k × f, s).

29

[1] F. Kr¨uger, T. Gergs, and J. Trieschmann, Plasma Sources Science and Technology 28, 035002

(2019).

[2] G. A. Bird, Molecular Gas Dynamics and the Direct Simulation of Gas Flows (Oxford Uni-

versity Press, New York, USA, 1994).

[3] M. A. Lieberman and A. J. Lichtenberg, Principles of Plasma Discharges and Materials Pro-

cessing, 2nd ed. (Wiley, Hoboken, USA, 2005).

[4] W. D. J. Callister and D. G. Rethwisch, Materials Science and Engineering: An Introduction,

9th ed. (Wiley, Hoboken, USA, 2013).

[5] J. P. Biersack and L. G. Haggmark, Nuclear Instruments and Methods 174, 257 (1980).

[6] W. Eckstein and J. Biersack, Nuclear Instruments and Methods in Physics Research Section

B: Beam Interactions with Materials and Atoms 2, 550 (1984).

[7] W. M¨oller and W. Eckstein, Nuclear Instruments and Methods in Physics Research Section

B: Beam Interactions with Materials and Atoms 2, 814 (1984).

[8] A. F. Voter, in Radiation eﬀects in solids, NATO science series. II, Mathematics, physics and

chemistry No. v. 235 (Springer, Dordrecht, The Netherlands, 2007).

[9] D. B. Graves and P. Brault, Journal of Physics D: Applied Physics 42, 194011 (2009).

[10] E. C. Neyts and P. Brault, Plasma Processes and Polymers 14, 1600145 (2017).

[11] C. K. Birdsall and A. B. Langdon, Plasma Physics via Computer Simulations (IOP Publishing,

Bristol, UK, 1991).

[12] J. v. Dijk, G. M. W. Kroesen, and A. Bogaerts, Journal of Physics D: Applied Physics 42,

190301 (2009).

[13] V. Serikov, S. Kawamoto, and K. Nanbu, IEEE Transactions on Plasma Science 27, 1389

(1999).

[14] R. E. Somekh, Journal of Vacuum Science & Technology A 2, 1285 (1984).

[15] G. M. Turner, I. S. Falconer, B. W. James, and D. R. McKenzie, Journal of Applied Physics

65, 3671 (1989).

[16] J. Trieschmann and T. Mussenbrock, Journal of Applied Physics 118, 033302 (2015).

[17] E. C. Neyts, Y. Shibuta, A. C. T. van Duin, and A. Bogaerts, ACS Nano 4, 6665 (2010).

[18] E. C. Neyts and A. Bogaerts, Theoretical Chemistry Accounts 132, 1320 (2013).

30

[19] R. Tonneau, P. Moskovkin, A. Pﬂug, and S. Lucas, Journal of Physics D: Applied Physics

51, 195202 (2018).

[20] M. W. Thompson, The Philosophical Magazine: A Journal of Theoretical Experimental and

Applied Physics 18, 377 (1968).

[21] P. Sigmund, Physical Review 187, 768 (1969).

[22] P. Sigmund, Physical Review 184, 383 (1969).

[23] S. Berg and T. Nyberg, Thin Solid Films 476, 215 (2005).

[24] D. Depla, S. Mahieu, R. Hull, R. M. Osgood, J. Parisi, and H. Warlimont, eds., Reactive

Sputter Deposition, Springer Series in Materials Science, Vol. 109 (Springer, Berlin, Germany,

2008).

[25] A. Diaw, K. Barros, J. Haack, C. Junghans, B. Keenan, Y. W. Li, D. Livescu, N. Lubbers,

M. McKerns, R. S. Pavel, D. Rosenberger, I. Sagert, and T. C. Germann, Physical Review E

102, 023310 (2020).

[26] Z. W. Ulissi, A. J. Medford, T. Bligaard, and J. K. Nørskov, Nature Communications 8,

14621 (2017).

[27] H. Kino, K. Ikuse, H.-C. Dam, and S. Hamaguchi, Physics of Plasmas 28, 013504 (2021).

[28] D. P. Kingma and M. Welling, in Proceedings of the International Conference on Learning

Representations (Scottsdale, USA, 2013).

[29] D. J. Rezende, S. Mohamed, and D. Wierstra, in Proceedings of the 31st International Con-

ference on Machine Learning, Vol. 32 (PMLR, Beijing, CN, 2014) pp. 1278–1286.

[30] I. Higgins, L. Matthey, A. Pal, C. Burgess, X. Glorot, M. Botvinick, S. Mohamed, and

A. Lerchner, in Proceedings of the 5th International Conference on Learning Representations

(Toulon, FR, 2016).

[31] C. P. Burgess, I. Higgins, A. Pal, L. Matthey, N. Watters, G. Desjardins, and A. Lerchner, in

Proceedings of the 31st Conference on Neural Information Processing Systems (Long Beach,

USA, 2017).

[32] C. Doersch, arXiv:1606.05908 [cs, stat] (2021).

[33] R. Behrisch and W. Eckstein, Sputtering by Particle Bombardment, Topics in Applied Physics,

Vol. 110 (Springer, Berlin, Germany, 2007).

[34] H. Hofs¨ass, K. Zhang, and A. Mutzke, Applied Surface Science 310, 134 (2014).

31

[35] D. P. Kingma and J. Ba, in Proceedings of the 3rd International Conference on Learning

Representations (San Diego, USA, 2015).

[36] G. E. Hinton and R. R. Salakhutdinov, Science 313, 504 (2006).

[37] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin, S. Ghemawat, G. Irving,

M. Isard, M. Kudlur, J. Levenberg, R. Monga, S. Moore, D. G. Murray, B. Steiner, P. Tucker,

V. Vasudevan, P. Warden, M. Wicke, Y. Yu, and X. Zheng, “TensorFlow: An Open Source

Machine Learning Framework for Everyone,” (2016), https://tensorﬂow.org/.

[38] F. Chollet and others, “Keras: The Python Deep Learning library,” (2015), https://keras.io/.

[39] K. Sohn, H. Lee, and X. Yan, in Proceedings of the 29th Conference on Neural Information

Processing Systems, Vol. 28 (Montr´eal, CA, 2015).

32

