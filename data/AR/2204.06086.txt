A POST AUTO-REGRESSIVE GAN VOCODER FOCUSED ON
SPECTRUM FRACTURE

Zhenxing Lu1, Mengnan He2, Ruixiong Zhang2, Caixia Gong2

1Department of Electrical and Computer Engineering, Technical University of Munich, Germany
2Didichuxing, Beijing, China
zhenxing.lu@tum.de, {hemengnan, zhangruixiong, gongcaixia}@didichuxing.com

2
2
0
2

r
p
A
2
1

]
S
A
.
s
s
e
e
[

1
v
6
8
0
6
0
.
4
0
2
2
:
v
i
X
r
a

Abstract

Generative adversarial networks (GANs) have been indicated
their superiority in usage of the real-time speech synthesis.
Nevertheless, most of them make use of deep convolutional
layers as their backbone, which may cause the absence of pre-
vious signal information. However, the generation of speech
signals invariably require preceding waveform samples in its re-
construction, as the lack of this can lead to artifacts in generated
speech. To address this conﬂict, in this paper, we propose an im-
proved model: a post auto-regressive (AR) GAN vocoder with a
self-attention layer, which merging self-attention in an AR loop.
It will not participate in inference, but can assist the generator to
learn temporal dependencies within frames in training. Further-
more, an ablation study was done to conﬁrm the contribution of
each part. Systematic experiments show that our model leads
to a consistent improvement on both objective and subjective
evaluation performance.
Index Terms: speech synthesis, neural vocoder, generative ad-
versarial networks, autoregressive model, self-attention

1. Introduction

With the development of artiﬁcial intelligence, text to speech
(TTS) has made a remarkable progress. Conditional waveform
generation has migrated from traditional digital signal process-
ing (DSP) techniques, such as the Grifﬁn-Lim [1] and the World
[2], to neural networks. There are mainly two types of the state-
of-the-art neural vocoders: AR and Non-AR vocoders. Since
WaveNet [3], the AR vocoder has made a signiﬁcant break-
through, as the AR structure improves the continuity recon-
struction in the generated waveform. On this account, the main
weakness of AR model is the inference speed. WaveRNN [4]
is similar to WaveNet, but a smaller model size and weighting
sparsiﬁcation method are applied to improve inference speed.
LPCNet [5] realizes the efﬁcient and fast neural speech synthe-
sis by linear prediction method.

Non-AR models such as GAN have been demonstrated to
be computationally efﬁcient for conditional waveform genera-
tion. It could learn the probabilistic distribution in the parallel
way, which means less training and inference time cost, such
as MelGAN [6] and HiFi-GAN [7]. A well trained Non-AR
vocoder outperforms the AR vocoder in terms of both speech
quality and inference speed [8, 9]. DiffWave [10] is also a
Non-AR model, which converts the noise signal into waveform
It matches WaveRNN in terms of
through a Markov chain.
speech quality, but the inference speed of a diffusion model is
still not fast enough for real-time synthesis tasks.

Despite advances made in neural vocoders, we ﬁnd that
there is a quality gap between the neural networks generated
speech and the real human voice. In real TTS tasks, especially

Figure 1: A spectrogram comparison of ground truth (a), HiFi-
GAN (b) and our model (c) generated waveform

when applying GAN vocoders, we can hear some vocal tremors
at speciﬁc places, as demonstrated in Fig. 1(b), we could ob-
serve a fracture in HiFi-GAN generated spectrogram, which re-
It has been exhibited
sults in the tremor of generated voice.
[11], that some artifacts of generated speech, such as pitch error
effects and periodicity artifacts, are mainly due to the pitch and
period mismatch caused by the mechanism of non-AR models.
In order to solve this problem, some hybrid models are
brought to the forefront, which combine the advantages of AR
and non-AR model. One productive model is WaveFlow [8],
which uses an AR module to learn short range dependencies
and a non-regressive 2-D convolutional architecture to capture
long range dependencies. CARGAN [11] reduces the pitch and
periodicity errors with an AR loop. However, it still takes 18
times longer on GPU and 3 times longer on CPU by inference
compared with HiFi-GAN.

Inspired by prior works, in this paper, we aim to ﬁnd a
feasible solution on the lack of sequential modeling in GAN
vocoders without any reduction of inference speed. We will
systematically present a series of experiments, then discuss how
GAN can be equipped with self-attention and AR to capture
long-range decencies within frames. Finally, we verify that the
proposed model leads to a better generation performance com-
pared with the baseline and no extra time cost on inference.

We summarize our contributions as follows:

• We propose a self-attention and AR loop based post-
net, which could capture long-term dependencies within
waveform frames in training and will not be used in in-
ference.

• We use a new objective loss function called Teager En-
ergy Operator loss to enhance the interaction of frames.

• We indicate that our proposed post-net is robust and can

 
 
 
 
 
 
Figure 2: Full version of model, the blue trapezoid are the generator and discriminator from original HiFi-GAN V2. The green dash
boxes represent inference and AR loop respectively, where AR loop is applied only in training. To get the next frame of signal (the
blue shadowed rectangle), we use one previous frame of output speech, real speech and one current frame of generated speech (the red
shadowed rectangle), i.e. ”two frames in, one frame out”. The output audio of AR loop will sent to discriminators and inﬂuence the
update of the generator parameters.

be integrated into other GAN vocoders for a performance
gain.

2. Related Works

2.1. Hiﬁ-GAN

In HiFi-GAN, several transposed convolutional layers with di-
verse upsampling rate, and the residual stack, which is made up
of dilation convolutional layers and normal convolutional lay-
ers, are used to transform Mel-spectrogram into waveform. The
multi-receptive ﬁeld fusion in residual stacks could observe pat-
terns of various length in parallel and improve the quality of
speech. Besides, HiFi-GAN upgrades the discriminators from
MelGAN to multi-period (MPD) and multi-scale (MSD) per-
spective, handling the portion of periodic signals and evaluating
audio signal at different levels.

2.2. Chunked autoregressive GAN (CARGAN)

In CARGAN, an extra AR conditioning stack is proposed to
constraint the generation of waveform. The previous samples
and the generated speech are summarized into ﬁxed-length,
injected to an encoder and ﬁnally spliced with the raw Mel-
spectrogram feature to generate a new chunk of speech.

2.3. Multi-resolution STFT Loss

The multi-resolution short-time Fourier transform (STFT) loss
It is the sum of Mel-
proposed in Parallel WaveGAN [12].
spectrogram losses with various STFT analysis parameters. It
consists of spectral convergence loss (LSC ) and magnitude loss
(LM ag):

LSC (x, ˜x) =

(cid:107)|ST F T (x)| − |ST F T (˜x)|(cid:107)F
(cid:107)|ST F T (x)|(cid:107)F

,

(1)

1
N

LM ag(x, ˜x) =

(cid:107)log |ST F T (x)| − log |ST F T (˜x)|(cid:107)1 ,

(2)
where x and ˜x mean target speech and generated speech,
(cid:107)·(cid:107)F and (cid:107)·(cid:107)1 mean F robenius and L1 norm, respectively.
|ST F T (·)| and N denote the STFT magnitudes and the num-
ber of elements in the magnitude, respectively. So the multi-
resolution STFT loss can be converted into:

Lmr stf t(G) = Ex,˜x

(cid:34)

1
M

M
(cid:88)

m=1

(Lm

sc(x, ˜x) + Lm

mag(x, ˜x))

(cid:35)

,

(3)

where M is the number of STFT parameter groups.

3. Our Model

3.1. GAN Architecture

Conforming to GAN’s principle[13], on the one hand, the gen-
erator is designated for learning the reverse mapping from
acoustic features such as Mel-spectrogram to audio waveform.
On the other hand, the discriminator plays the role of a binary
classiﬁer which distinguished the real audio samples from the
dataset as true, and the fake samples produced by generator as
false. Simultaneously, discriminator guides the parameters up-
dating of generator.

3.2. Auto-regressive Loop

As mentioned in the introduction, most GAN vocoders pos-
sess the fracture spectrum problem due to the Non-AR mech-
anism. The continuity between speech signal frames is ignored,
which leads to the phase and periodicity mismatch of the au-
dio samples. Although there exist AR GAN vocoders such as
CARGAN [11] addressing this problem, the inference speed be-
comes their shortcoming. Therefore, we consider using a pos-
terior AR loop to assist the training of generator, but it will not
participate in inference.

The joint probability of a waveform x = {x1, ..., xT },
where xi is a single sample, i is the time index and T is the
length of the waveform. x could be factorized as a product of
conditional distribution as follows:

p(x) =

T
(cid:89)

t=1

p(xt | x1, ..., xt−1),

(4)

we regard each audio sample as the conditioned distribution of
all previous samples. Differing from CARGAN, we utilize the
AR structure only in the time domain (as shown in Fig. 2). One
frame of previous AR-loop’s output ˜x, previous real speech x
and current generated speech ˆx are concatenated, then directly

fed into the post-net, to generate a new frame of waveform, the
Equ. 4 could be reformed as:

p(˜x) =

N
(cid:89)

i=1

p(˜xi·T | ˜x(i−1)·T , x(i−1)·T , ˆxi·T ),

(5)

where N is the number of frames of the whole audio, i is the
current waveform frame, and T is the length of one frame. Here
we handle the waveform into frames, there are two reasons:
ﬁrstly, we believe handling with every sample is inefﬁcient.
Secondly, the speech signal is stationary in a short time frame.
The size of input frames is one of the hyper-parameters and we
will discuss it in the ablation study shortly. This AR loop is
used only in training, and the output will be sent to discrimina-
tor to be classiﬁed as real or fake, then to affect the iteration of
generator parameters through back propagation.

3.3. Post Self-attention Augmented Network

Considering that the self-attention layer can better capture the
context information in time sequence, we develop it according
to the self-attention GAN [14] and the non-local net [15]. As
shown in Fig. 3(b), given the feature map of F ∈ RL×C as
input of the self-attention layer, where L is the time dimension
and C is the number of channels, the query matrix Q, the key
matrix K, and the value matrix V are obtained via matrix trans-
formation:

Q = FWQ, K = FWK , V = FWV ,

(6)

where WQ, WK , WV ∈ RC× C
k denote the learnable weight
matrices of the 1 × 1 convolutional layer. Therefore, the dimen-
sion of the metrics Q, K, V are RL× C
k and the attention map
A is then computed as:

A = softmax(QKT ), A ∈ RL×L,

(7)

aj,i =

exp(sij)
i=1 exp(sij)

(cid:80)L

, si,j = Q(xi)K(xj)T ,

(8)

aj,i denotes the extent to which the model attends to the i-th
location when synthesizing the j-th column vj of V. The output
of the attention layer O is computed as:

O = (AV)WO, WO ∈ R C

(9)
with the weight matrix WO realized by a 1 × 1 convolution
layer of C ﬁlters, the shape of O is restored to the origin shape
L × C. Eventually, there is a learnable scalar weight γ in the
output of the attention layer, so the ﬁnal output is:

k ×C ,

˜F = γO + F.

(10)

We integrate the self-attention layer into the autoencoder
network [16], as illustrated in Fig. 3(a), we also tried to realize
the regressive loop directly with a LSTM layer of 256 hidden
units, but the performance was not satisﬁed. Here we adopt the
traditional autoencoder with the shortcut connection. The en-
coder consists of 2 one-dimensional strided convolutional lay-
ers with the same kernel size of 31 and a stride of 2, and the
increased ﬁlter number of 8, 16. Self-attention layer is applied
in higher-dimensional latent space to enhance the continuity in
adjacent frames. The decoder, on the other hand, reverses the
encoding process by deconvolution layer and restores informa-
tion into waveform.

Figure 3: Illustration of the proposed post-net and the self-
attention layer, with L = 6, C = 6 and k = 2.

3.4. Training Objectives

We replace the Mel-spectrogram loss from original HiFi-GAN
to the multi-resolution STFT loss, the beneﬁt of which is that
the generator is able to learn the features of speech in the
time-frequency domain [17], and could prevent over-ﬁtting
in a ﬁxed STFT representation. Moreover, we propose a new
time-domain loss to enhance the temporal dependencies within
frames.

Teager Energy Operator (TEO) Loss The TEO was proposed
by H. M. Teager [18] while working on non-linear audio signals,
which is widely used to detect voice event:

Ψ(x(t)) = ˙x(t)2 − x(t) ∗ ¨x(t),

(11)

for discrete speech signal, it could be rewritten as:

Ψ (x[n]) = x[n]2 − x[n − 1] ∗ x[n + 1],

(12)

where x[n], x[n − 1] and x[n + 1] represent the current, past
and next audio sample, n is the discrete time index. Normally,
the audio signal can not change suddenly in a short time, the
non-continuity could be captured by this difference Ψ. There-
fore, the TEO loss could constraint signal generation in the time
domain. The TEO loss is deﬁned as:

LT EO(G) = Ex,˜x

(cid:34)

1
N

N
(cid:88)

i=1

(cid:35)

(cid:107)Ψ(xi) − Ψ(˜xi)(cid:107)1

,

(13)

where N is the length of the original speech samples. Here, we
calculate the TEO of original speech x and generated speech ˜x
respectively, then compute them in L1 norm.

Final Loss Combined with all objective loss functions, we
could summarize a jointly loss function:

min
G

Ex,˜x

(cid:34) K
(cid:88)

(cid:35)

(Dk(G) − 1)2

k=1

+ Ex,˜x [Lmr stf t( G) + λLT EO(G)],

(14)

where Dk denotes the k-th sub-discriminator in MPD and
MSD, and Lmr stf t stands for multi-resolution STFT loss.

deconv1deconv2conv1conv2self-attentionoutput(a)1x1 conv1x1 conv!𝑭𝑭𝑸𝑲𝑽𝑨𝒕𝒕𝒆𝒏𝒕𝒊𝒐𝒏𝒕𝒓𝒂𝒏𝒔𝒑𝒐𝒔𝒆𝑨𝑽𝑶𝒔𝒉𝒐𝒓𝒕𝒄𝒖𝒕𝒄𝒐𝒏𝒏𝒆𝒄𝒕𝒊𝒐𝒏1x1 conv1x1 conv(b)Table 1: Comparison of HiFi-GAN V2 (base), MB MelGAN, CARGAN, Our model, MB MelGAN with post-net

Model

Ground Truth

Our model

CARGAN

Hiﬁ−GAN(v2)

MB MelGAN

MB MelGAN with post−net

4. Experiments

Params(M)↓

MOS↑

RTF↓ BCR↓ MOS−TTS↑

−

0.92

25.5

0.92

1.62

1.62

4.43±0.03

−

4.17±0.02

0.385

4.20±0.03

1.035

4.12±0.05

0.385

4.00±0.04

0.069

4.07±0.05

0.069

−

0.13

0.03

0.3

0.27

0.15

4.48±0.03

3.87±0.03

3.90±0.03

3.84±0.04

3.73±0.06

3.73±0.03

4.1. Dataset

In our experiments, we choose Chinese Mandarin Speech Cor-
pus (CSMSC) dataset for training and testing, which consists of
10,000 audios of 12 hours’ recording, and all audios were down-
sampled by 16kHz with 16-bit PCM data format. We preprocess
all raw audios into the 80-dimensional Mel-spectrogram with
hop size as 256.

4.2. Experimental Setup

During training, the Adam optimizer was adopted with a learn-
ing rate varying with number of iterations for both generator and
discriminator. For multi-resolution STFT loss, we applied three
STFT groups (M = 3) with frame size as 512, 1,024, 2,048, the
window size as 240, 600, and 1,200 and the frameshift as 50,
120, and 240. We set the λ of TEO loss as 50.

The synthesis quality was measured on the mean opinion
score (MOS) which were obtained using the crowd sourcing
methodology described in P.808 [19] with 95% conﬁdence in-
tervals. We chose 20 native Chinese speakers and randomly se-
lected 200 sentences to score. Here, in order to clarify that the
proposed model can have better performance on spectral frac-
ture problem, we propose a new subjective evaluation named
bad-case rate BCR, which stands for the rate of speech which
has the spectral fracture problem.
It could be computed as
Gaudio/T otalaudio, where Gaudio represents the number of
the generated audios in which the audio artifacts could be heard,
and the T otalaudio is the total number of original audios. In our
experiment, we use 200 audio examples in total for the BCR
calculation.

The synthesis speed is measured on GPU and CPU based
on the research regarding efﬁciency of neural networks [6]. We
choose 50 sentences to test RTF and averaged over three times.
The devices are single NVIDIA TESLA (R) P40 GPU and an
Intel Xeon (R) CPU E5-2630 v4 @ 2.20 GHz.

4.3. Experimental Results

We benchmark the HiFi-GAN V2, our model, CARGAN,
Multi-band (MB) MelGAN, and MB MelGAN integrated with
proposed post-net, in terms of MOS, number of parameters,
RTF, and BCR. As shown in Tab.1, we ﬁnd that our model
gained an MOS improvement when compared to the baseline.
Meanwhile, our model has a smaller model size when compared
to CARGAN, but has the same RTF as baseline. By utilizing the
post-net, the BCR dropped more than twice as baseline 1.

1https://cookingbear.github.io/research/

As for the TTS task, we also test vocoders with a acous-
tic model of Fastspeech2 [20]. FastSpeech2 predicts mel-
spectrograms, which are fed into the vocoder to generate wave-
form. The results indicate that our model outperforms base
models in TTS tasks.

4.4. Ablation Study

We design the a series of ablation studies to conﬁrm the effect
of each component and the corresponding results are showing
in Tab.2, each model was trained for 600k steps in order to get
stable results. Note that the absence of a regression loop means
that we use a forward structure to replace AR loop but preserve
the same post-net. As shown, every component has a positive
contribution to the inference quality. When we remove the AR
structure, the MOS and BCR fall noticeably. Simultaneously,
the self-attention layer also beneﬁts the connection detail within
frames.

It is worth mentioning that the size of input for the AR post-
net is a vital factor. To evaluate this, we conducted a series of
tests, where the n-frame is the number of input frames of the
post-net, here we choose 1, 2, 4, and as demonstrated in Tab.2.
We ﬁnd that the 2-frame input (our model) has the lowest BCR.

Table 2: MOS result of ablation study

Model

Our model

1-frame

4-frame

w/o AR loop

MOS

BCR

4.17±0.02

4.17±0.02

4.12±0.02

4.10±0.02

0.13

0.16

0.15

0.25

0.16

0.15

w/o self-attention layer

4.18±0.02

w/o TEO loss

4.16±0.02

5. Conclusions

In this paper, we designed a neural vocoder based on AR loop
and self-attention. By using the proposed post-net integrated in
the AR loop, the temporal dependency of generated audios is
improved. Our experiment shows that the model outperforms
base models in both subjective and objective evaluation. Fur-
thermore, our model is generic and can easily be applied in ex-
isting non-AR vocoders to obtain potential improvements.

[18] H. Teager and S. Teager, “Evidence for nonlinear sound produc-
tion mechanisms in the vocal tract,” in Speech production and
speech modelling. Springer, 1990, pp. 241–261.

[19] ITU-T, “Recommendation p.808: Subjective evaluation of speech

quality with a crowdsourcing approach,” 2018.

[20] Y. Ren, Y. Ruan, X. Tan, T. Qin, S. Zhao, Z. Zhao, and T.-Y. Liu,
“Fastspeech: Fast, robust and controllable text to speech,” Ad-
vances in Neural Information Processing Systems, vol. 32, 2019.

6. References
[1] D. Grifﬁn and J. Lim, “Signal estimation from modiﬁed short-
time fourier transform,” IEEE Transactions on acoustics, speech,
and signal processing, vol. 32, no. 2, pp. 236–243, 1984.

[2] M. Morise, F. Yokomori, and K. Ozawa, “World: a vocoder-based
high-quality speech synthesis system for real-time applications,”
IEICE TRANSACTIONS on Information and Systems, vol. 99,
no. 7, pp. 1877–1884, 2016.

[3] A. v. d. Oord, S. Dieleman, H. Zen, K. Simonyan, O. Vinyals,
A. Graves, N. Kalchbrenner, A. Senior, and K. Kavukcuoglu,
“Wavenet: A generative model for raw audio,” arXiv preprint
arXiv:1609.03499, 2016.

[4] N. Kalchbrenner, E. Elsen, K. Simonyan,

S. Noury,
N. Casagrande, E. Lockhart, F. Stimberg, A. Oord, S. Dieleman,
and K. Kavukcuoglu, “Efﬁcient neural audio synthesis,” in
International Conference on Machine Learning.
PMLR, 2018,
pp. 2410–2419.

[5] J. M. Valin and J. Skoglund, “Lpcnet: Improving neural speech
synthesis through linear prediction,” in ICASSP 2019 - 2019 IEEE
International Conference on Acoustics, Speech and Signal Pro-
cessing (ICASSP), 2019.

[6] K. Kumar, R. Kumar, T. de Boissiere, L. Gestin, W. Z. Teoh,
J. Sotelo, A. de Br´ebisson, Y. Bengio, and A. C. Courville, “Mel-
gan: Generative adversarial networks for conditional waveform
synthesis,” Advances in neural information processing systems,
vol. 32, 2019.

[7] J. Kong, J. Kim, and J. Bae, “Hiﬁ-gan: Generative adversarial net-
works for efﬁcient and high ﬁdelity speech synthesis,” Advances
in Neural Information Processing Systems, vol. 33, pp. 17 022–
17 033, 2020.

[8] W. Ping, K. Peng, K. Zhao, and Z. Song, “Waveﬂow: A compact
ﬂow-based model for raw audio,” in International Conference on
Machine Learning. PMLR, 2020, pp. 7706–7716.

[9] R. Prenger, R. Valle, and B. Catanzaro, “Waveglow: A ﬂow-based
generative network for speech synthesis,” in ICASSP 2019-2019
IEEE International Conference on Acoustics, Speech and Signal
Processing (ICASSP).

IEEE, 2019, pp. 3617–3621.

[10] Z. Kong, W. Ping, J. Huang, K. Zhao, and B. Catanzaro, “Dif-
fwave: A versatile diffusion model for audio synthesis,” arXiv
preprint arXiv:2009.09761, 2020.

[11] M. Morrison, R. Kumar, K. Kumar, P. Seetharaman, A. Courville,
and Y. Bengio, “Chunked autoregressive gan for conditional
waveform synthesis,” arXiv preprint arXiv:2110.10139, 2021.

[12] R. Yamamoto, E. Song, and J.-M. Kim, “Parallel wavegan: A fast
waveform generation model based on generative adversarial net-
works with multi-resolution spectrogram,” in ICASSP 2020-2020
IEEE International Conference on Acoustics, Speech and Signal
Processing (ICASSP).

IEEE, 2020, pp. 6199–6203.

[13] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-
Farley, S. Ozair, A. Courville, and Y. Bengio, “Generative adver-
sarial nets,” Advances in neural information processing systems,
vol. 27, 2014.

[14] H. Zhang, I. Goodfellow, D. Metaxas, and A. Odena, “Self-
attention generative adversarial networks,” in International con-
ference on machine learning. PMLR, 2019, pp. 7354–7363.

[15] X. Wang, R. Girshick, A. Gupta, and K. He, “Non-local neural
networks,” in Proceedings of the IEEE conference on computer
vision and pattern recognition, 2018, pp. 7794–7803.

[16] G. E. Hinton and R. Zemel, “Autoencoders, minimum description
length and helmholtz free energy,” Advances in neural informa-
tion processing systems, vol. 6, 1993.

[17] X. Wang, S. Takaki, and J. Yamagishi, “Neural source-ﬁlter-based
waveform model for statistical parametric speech synthesis,” in
ICASSP 2019-2019 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP).
IEEE, 2019, pp. 5916–
5920.

