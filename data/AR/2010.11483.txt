Multilingual Approach to Joint Speech and Accent Recognition with
DNN-HMM Framework

Yizhou Peng1∗, Jicheng Zhang1∗, Haobo Zhang1∗, Haihua Xu2, Hao Huang1, Eng Siong Chng2

1School of Information Science and Engineering, Xinjiang University, Urumqi, China
2School of Computer Science and Engineering, Nanyang Technological University, Singapore

1
2
0
2

y
a
M
8

]
S
A
.
s
s
e
e
[

2
v
3
8
4
1
1
.
0
1
0
2
:
v
i
X
r
a

Abstract

Human can recognize speech, as well as the peculiar accent of
the speech simultaneously. However, present state-of-the-art
ASR system can rarely do that.
In this paper, we propose a
multilingual approach to recognizing English speech, and re-
lated accent that speaker conveys using DNN-HMM frame-
work. Speciﬁcally, we assume different accents of English as
different languages. We then merge them together and train
a multilingual ASR system. During decoding, we conduct
two experiments. One is a monolingual ASR-based decoding,
with the accent information embedded at phone level, realizing
word-based accent recognition (AR), and the other is a multilin-
gual ASR-based decoding, realizing an approximated utterance-
based AR. Experimental results on an 8-accent English speech
recognition show both methods can yield WERs close to the
conventional ASR systems that completely ignore the accent,
as well as desired AR accuracy. Besides, we conduct extensive
analysis for the proposed method, such as transfer learning with
out-domain data exploitation, cross-accent recognition confu-
sion, as well as characteristics of accented-word.
Index Terms: speech recognition, accent recognition, TDNNf,
multilingual, multi-task learning

1. Introduction

Automatic speech recognition (ASR) has been signiﬁcantly im-
proved thanks to the deep neural network techniques [1, 2, 3, 4].
However, there still remains challenges in speech recognition
area. For instance, state-of-the-art ASR performance would be
severely degraded when it recognizes noisy speech [5, 6]. An-
other limitation is that most present ASR systems are only ca-
pable of recognizing monolingual speech. However, as global-
ization trends are deepened, multilingual ASR systems [7, 8, 9]
are greatly required.

Except for the linguistic content that is recognized by ASR
system, speech also conveys other important information about
the speaker, such as voice, emotion, gender, accent information
etc. As a result, recognition of such information is also widely
studied [10, 11, 12, 13, 14, 15, 16] in speech research area.

Though speech related research is widely studied, few ef-
forts are focused on combining different recognition tasks as
mentioned to study jointly. For example, one rarely com-
bines speech recognition with gender recognition, or combines
speech recognition with accent recognition. More often than
not, they are studied separately. This contradicts our human
speech recognition behavior. Intuitively, while human recog-

Students (∗) have joined SCSE MICL Lab, NTU, Singapore as ex-
change students. This work is supported by the National Key R&D
Program of China (2017YFB1402101), Natural Science Foundation of
China (61663044, 61761041), Hao Huang is correspondence author.

nize speech content, they can also recognize the gender or ac-
cent, or even speaker emotion of the incoming speech simulta-
neously.

In this paper, we propose a joint approach to perform-
ing speech and accent recognition using DNN-HMM modeling
framework, namely TDNNf [17]. One of the advantages for the
proposed method is that it can be easily deployed for real-time
speech and accent recognition simultaneously. Our RTF is ∼0.4
under normal 2.6 GHz CPU setting. Meanwhile, we also sub-
mit another paper using Transformer-based End-to-end (E2E)
approach [18]. Though the E2E approach can yield better re-
sults taking advantage of out-domain data, the main limitation
of the method is much slower than TDNNf method,

The paper is organized as follows. Section 2 is to review
prior related work. Section 3 presents the proposed joint speech
and accent recognition method. Section 4 is our data speciﬁca-
tion. Section 5 briefs the overall experimental setup and Sec-
tion 6 presents the results of the proposed methods. After that,
we perform further analysis in Section 7, and draw conclusions
in Section 8.

2. Related work

Recently, multilingual ASR has been drawing wide attention,
but most of the works are only concerned with bilingual case,
such as code-switching ASR [19, 20, 21].
In this paper, our
ASR system is a multilingual ASR system, which is instead
built with 8 accented English corpora. Besides, different from
code-switching ASR, our ASR system is trained with utterances
with only single accent, which is beneﬁcial to utterance-based
accent recognition. For accent recognition, the goal is to clas-
sify speaker’s accent according to their pronunciation peculiar-
ity. One can perform the task using speaker or gender identi-
ﬁcation method, such as i-vector [10], x-vector [11] or End-to-
End (E2E) neural network classiﬁer based methods [12, 22].

Since E2E attention-based framework becomes popular,
multilingual ASR is getting much simpler as the number of
the grapheme-letters/characters for each language is quite lim-
ited (e.g. English has only 26 letters). One can simply merge
the grapheme-letters/characters of different languages as recog-
nition output. For instance, it is reported a 50-language multi-
lingual ASR system is built with E2E method in [7]. However,
it is only focused on speech recognition task. Also using E2E
framework, [23] proposed a method to improve multi-dialect
speech recognition performance by using dialect information as
an external input vector in the LAS ASR model. For joint sys-
tems, [9] proposed a joint language identiﬁcation and speech
recognition method on 10 different languages. Its ASR results
are impressive but far from the state-of-the-art on individual lan-
guages. Besides, its language identiﬁcation results are not com-
pared with conventional i-vector, x-vector, or other E2E meth-

 
 
 
 
 
 
ods. Furthermore, [24] implemented the same structure as [9]
and investigated its performance on many more languages.

Different from prior work, we employ DNN-HMM to
realize a joint speech and accent recognition system. The
framework is only a multilingual ASR system, by appending
grapheme with different accent identiﬁer (we are using position-
dependent grapheme lexicon [3]). To be thorough, we not only
compare our ASR performance with conventional monolingual
DNN-HMM ASR system, we also compare our accent recog-
nition performance with x-vector-based classiﬁer. Besides, we
also conduct a series of analysis on the proposed methods under
different scenarios.

3. Joint speech and accent recognition

Technically, we can think of each accent as different language,
hence given a normal English word, we make the word and
its phone labels different for each individual accent. We then
merge the accented data to train a joint multilingual ASR sys-
tem. From this perspective, not only can such an ASR system
perform speech recognition, it can also recognize the accent.

During training,

assuming word “hello” is

from
British accent, our ASR lexicon should have an item
“hello BRT h BRT WB e BRT l BRT l BRT
like
realizing a British accent-based “hello”,
o BRT WB”,
is represented as “hello BRT” to differentiate with
that
“hello” from other accents. Here the “ BRT” is the accent
identiﬁer, while “ WB” is word boundary grapheme identiﬁer as
recommended in [3].

Korean, Portuguese and Russian respectively. There are two ex-
tra accented English data sets on test set, which are Spanish
and Canadian (they are ignored when we perform accent recog-
nition). Table 1 reports the details of data speciﬁcation. For
clariﬁcation, Figure 1 also plots the utterance length distribu-
tion for the three data sets respectively. The other is out-domain
data which is 960 hours of Librispeech2.

All data are read speech. As will be shown in Section 6, the
WER is quite low for the ASR track. However, as is shown in
Figure 1, majority of utterance length is less than 6 seconds (Ta-
ble 1 also reports that the average utterance length is ∼4s) for
the three data sets. Therefore, one can imagine the accent recog-
nition is rather challenging for such short utterances.

Table 1: Data speciﬁcation for the accented English data sets
including train, dev and test respectively

Total utts
Length (Hrs)
Ave. word (per utt.)
Ave. second (per utt.)

Train
124K
148.51
9.72
4.29

Dev
12k
14.50
9.66
4.35

Test
18k
20.95
9.00
4.15

We have two methods to conduct joint recognition during
decoding. One is to assume our ASR system as a monolin-
gual system, but each word has different pronunciations, de-
noted as “Mono-joint”. Let us use word “hello” as example
again, and one of its pronunciations in decoding lexicon would
be “hello h BRT WB e BRT l BRT l BRT o BRT WB”.
By this means, we demonstrate word-based accent encoding
with the help of accent identiﬁers at phone level. When recog-
nizing speech, we output phone, as well as word sequences. By
simply counting the accent identiﬁers for phone/word, we get
the accent of the recognized utterance. One of the drawbacks
of the method lies in it cannot encode the accent on utterance
level as each word in the utterance can be recognized as dif-
ferent accent. Therefore, we propose another method, that is,
not only is the phone label different, the word label is also dif-
ferent, here we name it as “Multi-joint”. In other words, the
decoding lexicon is the same with the training lexicon, and the
ASR is a multilingual ASR system. The beneﬁt of this method
is that it can approximately realize utterance-based accent en-
coding as the language model (LM) has no direct cross-accent
n-gram ever happened.

4. Data Speciﬁcation

The experimental data is from an accented English speech
recognition workshop[25], sponsored by DataTang Company
in China1. There are 2 challenge tracks, one is for accented En-
glish speech recognition, and the other is for accent recognition.
The data contains two parts. One is in-domain accented English
data, released by DataTang, including train, dev and test
sets speciﬁed by the organizer. There are 8 accented English
data sets among train and dev sets, each with ∼20 hours.
The accents are American, British, Chinese, Indian, Japanese,

Figure 1: Utterance length(s) distribution for accented English
data sets that are train, dev and test respectively

5. Experimental Setup
All the experiments are conducted with Kaldi.3 For DNN-
HMM ASR framework, the acoustic models are trained with
Lattice-free Maximum Mutual Information (LF-MMI) cri-
terion [26] over the Factorized Time Delay Neural Net-
work (TDNNf) [17]. The TDNNf is made up of 15 layers, and
each layer is decomposed as 1536x512x1536, where 512 is the
dimension of the bottleneck layer. To train the TDNNf, we also
perform two kinds of data augmentation (DA) methods, one is
speed perturbation (sp) (x3) [27], and the other is to add four
kinds of noise, such as white noise, music noise, babble noise,
as well as reverberant noise (x4). For baseline system, the lex-
icon is phonetic lexicon with ∼21k word vocabulary. For the
multilingual ASR, the lexicon is position-dependent grapheme
lexicon but with as many as eight times of the vocabulary for ei-
ther two methods as mentioned. From our off-line experiments,
the grapheme lexicon yields slightly worse results, compared
with phonetic lexicon. We stick to use it because we want our

1 https://www.datatang.com/INTERSPEECH2020

2 http://www.openslr.org/12/

3 https://github.com/kaldi-asr/kaldi

5101520253035＜3s[3,4)[4,5)[5,6)[6,7)≥7sPercent(%)Utterence Duration (Seconds)TrainDevTestTable 2: Speech WER (%) and Accent Recognition ACC (%) results of the proposed joint recognition methods on dev set.

Speech
Recognition

Accent
Recognition

Method

TDNNf
Mono-joint
Multi-joint

x-vector
Mono-joint
Multi-joint

AVE (WER%)
7.68
8.66
8.63
AVE (ACC%)
64.86
61.70
63.19

US
8.71
9.46
9.29

55.05
42.64
44.53

UK
8.87
10.31
9.71

88.24
88.43
88.48

CHN
11.08
12.67
12.71

55.44
63.76
66.31

IND
8.67
9.02
9.16

JAP
6.08
6.98
6.68

75.25
86.82
87.66

55.28
42.54
43.88

KOR
6.87
7.28
7.36

56.04
56.65
58.71

POR
6.45
7.43
7.51

RUS
4.47
5.36
5.30

79.58
58.97
61.51

52.91
54.95
55.63

TDNNf system to have the same advantage with E2E ASR sys-
tem, that is, free of lexicon modeling, which is especially useful
for multilingual ASR tasks. For the two methods, we build two
sets of tri-gram language models (LMs) for decoding respec-
tively. One is monolingual LM, and the other is a multilingual
LM, labelling each word with different accent identiﬁers.

Additionally, we also perform x-vector experiment for ac-
cent recognition as a contrast, while we choose logistic regres-
sion as the classiﬁer. We ﬁnd it consistently yields better results
compared with PLDA [28] method in our case. To train the x-
vector extractor, we follow the conﬁguration of [11], and the
resulting x-vector dimension is 512.

6. Results

Table 2 reports the speech recognition and accent recognition
results of the proposed methods, in contrast with conventional
TDNNf, and x-vector systems respectively.

From Table 2, we observe the proposed methods achieve
worse ASR results compared with the baseline TDNNf system.
This is understandable, since for the “Mono-joint” method, we
have 8-times of pronunciations than the baseline system, yield-
ing more confusion to the lexicon. Besides, the pronunciation
difference for each speciﬁc accent is not fully considered. On
the other hand, the TDNNf output of the “Multi-joint” method
has 8-times of senones to handle in theory, compared with the
baseline TDNNf system. Such a big senone-number output is
difﬁcult to obtain by decision-tree-based clustering method, and
it is also hard to learn.

For the accent recognition results in Table 2, the proposed
methods also yield worse accent recognition results, compared
with x-vector method. However, the advantage of the proposed
method lies in two aspects. First to perform accent recognition,
it is easier for the proposed method to exploit out-domain data
(Librispeech here) without accent label, compared with x-
vector method. Such an advantage will be shown in Section 7.2.
Secondly, it can perform both speech and accent recognition
tasks simultaneously.

Besides, from Table 2, we observe the best WERs are from
Russian (RUS) and Japanese (JAP) accented speech respec-
tively, while the worst of all is from Chinese (CHN) accented
speech, which is followed by British (UK) accented speech. For
the accent recognition, the best accuracy is from British accent
(UK), and after that it is Indian accent (IND), while the worst is
American accent (US) which is followed by the Japanese accent
(JAP). One thing worth a note is that the proposed method out-
performs the x-vector method with a big margin for the Chinese
(CHN) and Indian (IND) accented speech respectively, while it
performs much worse on the American (US), Japanese (JAP)
and Portuguese (POR) accented speech.

Furthermore, comparing speech recognition with accent

Table 3: Performance report with different TDNNf outputs (tied-
states/senones), dimension of the ﬁnal bottleneck layer is ﬁxed
at 512.

#PDF

4.3k
8.8k
14.6k
21.8k

Mono-joint

Multi-joint

WER% ACC% WER% ACC%
61.68
63.19
61.99
62.28

60.27
61.70
60.97
60.97

8.78
8.66
8.76
8.76

8.78
8.63
8.84
8.80

recognition results in Table 2, we note that better WER does
not necessarily mean better accent recognition accuracy. For
instance, for the Korean (KOR), Japanese (JAP), and Russian
(RUS) accented speech, the ASR results are signiﬁcantly bet-
ter than the ones of remaining accents, their accent recognition
results are much worse.

Finally, Table 2 shows that the proposed “Multi-joint”
method outperforms the “Mono-joint” method for both speech
and accent recognition results respectively. This suggests
utterance-based accent encoding is superior than the word-
based accent encoding at least for accent recognition.

7. Analysis

7.1. Joint accent acoustic models

For the above experiments, we merge 8 accented English data to
train the joint TDNNf acoustic models, and each accent has its
own phone set. The TDNNf models have ∼8.8k outputs (tied-
states/senones) in total. As a result, we only have about 1k out-
puts for each accent on average. We are curious that if bigger
outputs yield better results. Table 3 reports our experimental re-
sults. From Table 3, bigger outputs of TDNNf don’t bring per-
formance improvement on both tasks. We also report the results
of a smaller output of 4.3k where we only have less than 6 hun-
dred output on average for each accent, and get a slightly worse
performance compared with those of larger scales of outputs.
Our conclusion is that TDNNf is not sensitive to the tied-state
number (perhaps once it is satisﬁed with a minimal tied-state
number). This even suggests we can use TDNNf to train the
joint multilingual models with more languages in future.

7.2. Transfer Learning

To employ out-domain data, we adopt transfer learning (TL)
method [29, 30, 31], with Librispeech as the out-domain
data. We start with a TDNNf model trained with overall out-
domain and in-domain data. We then ﬁne-tune the model with
only the in-domain accented data. Figure 2 plots the curves
of the recognition accuracy versus learning rate factor, with 3

Table 4: Cross-Accent Recognition Confusion on dev set(%)

epochs. From Figure 2, we observe consistent improvements
on both tasks with bigger learning rate factor (≥0.5).

Table 5 reports the results of transfer learning, and RNNLM
rescoring. From Table 5, we see that transfer learning is ef-
fective on both speech and accent recognition performance
improvement, even outperforms x-vector (64.86%) on accent
recognition. However, RNNLM rescoring only improves ASR
WERs, while it yields no accuracy improvement on AR task at
all. These suggest AR accuracy is closely related with the per-
formance of the underlying acoustic model (AM), but less af-
fected with LM. For AM, different acoustic senones are trained
with different accented speech data, as a result it has capabil-
ity to differentiate accents. However, for the LMs (RNNLM
included), they are trained with overall transcripts, and conse-
quently have no accent discriminative capability on either word
or utterance level.

(a) Speech Recognition

(b) Accent Recognition

Figure 2: TL performance versus learning rate factor

Table 5: Recognition results with TL(lr=0.8) and RNNLM
Rescoring for dev and test sets

Method

Mono-joint

+Transfer Learning
+RNNLM Rescoring

Multi-joint

+Transfer Learning
+RNNLM Rescoring

WER(%)
test
10.31
9.83
8.28
10.35
9.74
8.40

dev
8.66
8.31
6.93
8.63
8.17
7.05

ACC(%)

dev
61.70
64.18
64.29
63.19
65.53
65.35

test
61.01
63.70
62.66
63.75
66.68
66.40

7.3. Cross-accent recognition confusion

We are also interested in how each speciﬁc accent is mistak-
enly recognized as another accent, that is, cross-accent recog-
nition confusion pattern. Table 4 shows overall accent recog-
nition confusion results. There are several patterns worth our
notice. First, for British and Indian accents, there are minor
in confusion with other accents. The most confusion counter-
parts for British and Indian accents are Russian and Portuguese
ones, with 3.35% and 13.63% respectively. Second, Japanese

and Korean accents, as well as Portuguese and Russian accents
are mutually confused heavily. For instance, Japanese accent
has 27.96% been miss recognized with Korean accents which
in return has 24.42% been miss recognized with the Japanese
accent. Finally, American accent is widely “overlapped” with
other accents, particularly 27.77% with British, and followed
by Russian (24.47%), and then Portuguese (8.06%) accents re-
spectively.

7.4. Accented-word characteristics

Except for accent confusion, we are also curious about if some
In
words have stronger accented characteristics than others.
other words, if a word appears in some accent and it is cor-
rectly recognized in terms of accent recognition in majority of
times, we think such a word has stronger accented characteris-
tics. Table 6 reveals some exemplar words in terms of highest
and lowest accent recognition accuracy. We can see from Ta-
ble 6, longer words or words with multiple syllables generally
yield better accent recognition results. This seems to be agreed
with our intuition.

Table 6: Exemplar words with highest and lowest accent recog-
nition accuracy respectively

Top 10

most accurate

least accurate

Exemplar words
temperature, strong, feature,
students, adapted, voyage,
branches, value, plans, blind
fake, key, ﬁnish
minute, England, ocean,
orders, sick, either, wash

Ave. Length

7.3

5.4

8. Conclusions

In this paper, we proposed a joint multilingual approach to re-
alizing speech and accent recognition simultaneously using the
DNN-HMM framework. On an 8-accent English speech recog-
nition data set, we demonstrated the effectiveness of the pro-
posed methods on both speech recognition and accent recog-
In addition, we adopted transfer learning to fully ex-
nition.
ploit out-domain data to boost the performance. We found that
transfer learning is beneﬁcial to the performance improvement
for both tasks, particularly for the accent recognition task. We
also tried RNNLM rescoring method, and achieved better WER
as expected however no accent recognition improvement. This
suggests accent recognition is more dependent on AM and less
on LM for our data set. Besides, we also analysed cross-accent
recognition confusion, as well as accented-word characteristics
preliminarily, which are yet to be further studied in future.

                   RecognitionGround TruthUSUKCHNINDJAPKORPORRUSUS37.4527.770.070.000.351.828.0624.47UK2.7292.660.130.000.760.000.383.35CHN7.110.4768.120.0016.172.750.005.37IND2.890.000.3082.790.300.0013.630.08JAP0.000.000.670.0049.4627.960.0021.91KOR0.210.273.360.0024.4265.433.093.22POR1.304.762.230.122.661.3672.7714.79RUS0.314.154.640.000.000.1233.9156.87learning ratefactor91.291.491.691.892.00.050.10.20.30.40.50.60.70.8Recognition Accuracylearning rate factorTL-Mono-joint TL-Multi-joint Mono-joint Multi-joint60616263646566670.050.10.20.30.40.50.60.70.8learning rate factorTL-Mono-jointTL-Multi-joint Mono-joint Multi-joint92.0[18] Jicheng Zhang, Yizhou Peng, Pham Van Tung, Haihua Xu, Hao
Huang, and Eng Siong Chng, “E2e-based multi-task learning ap-
proach to joint speech and accent recognition,” 2021.

[19] Zhiping Zeng, Yerbolat Khassanov, Van Tung Pham, Haihua Xu,
Eng Siong Chng, and Haizhou Li, “On the End-to-End Solution to
Mandarin-English Code-Switching Speech Recognition,” in Proc.
of INTERSPEECH 2019, pp. 2165–2169.

[20] Haihua Xu, Van Tung Pham, Zin Tun Kyaw, Zhi Hao Lim,
Eng Siong Chng, and Haizhou Li,
“Mandarin-english code-
switching speech recognition,” in Proc. of INTERSPEECH 2018,
pp. 554–555.

[21] Pengcheng Guo, Haihua Xu, Lei Xie, and Eng Siong Chng,
“Study of semi-supervised approaches to improving english-
mandarin code-switching speech recognition,” in Proc. of INTER-
SPEECH 2018, pp. 1928–1932.

[22] Pooyan Safari, Miquel India, and Javier Hernando,

Attention Encoding and Pooling for Speaker Recognition,”
Proc. of INTERSPEECH 2020, pp. 941–945.

“Self-
in

[23] Bo Li, Tara N Sainath, Khe Chai Sim, Michiel Bacchiani, Eu-
gene Weinstein, Patrick Nguyen, Zhifeng Chen, Yanghui Wu, and
Kanishka Rao, “Multi-dialect speech recognition with a single
in Proc. of ICASSP 2018), pp.
sequence-to-sequence model,”
4749–4753.

[24] Wenxin Hou, Yue Dong, Bairong Zhuang, Longfei Yang, Jiatong
Shi, and Takahiro Shinozaki, “Large-scale end-to-end multilin-
gual speech recognition and language identiﬁcation with multi-
task learning,” in Proc. INTERSPEECH 2020, 2020, pp. 1037–
1041.

[25] Xian Shi, Fan Yu, Yizhou Lu, Yuhao Liang, Qiangze Feng,
Daliang Wang, Yanmin Qian, and Lei Xie, “The accented english
speech recognition challenge 2020: Open datasets, tracks, base-
lines, results and methods,” arXiv preprint arXiv:2102.10233,
2021.

[26] Daniel Povey, Vijayaditya Peddinti, Daniel Galvez, Pegah
Ghahremani, Vimal Manohar, Xingyu Na, Yiming Wang, and
Sanjeev Khudanpur, “Purely sequence-trained neural networks
for asr based on lattice-free mmi,” in Proc. of INTERSPEECH
2016, pp. 2751–2755.

[27] Tom Ko, Vijayaditya Peddinti, Daniel Povey, and Sanjeev Khu-
danpur, “Audio augmentation for speech recognition,” in Proc. of
INTERSPEECH 2015, 2015.

[28] Yi Wang, Hongjie Bai, Matt Stanton, Wen-Yen Chen, and Ed-
ward Y Chang, “Plda: Parallel latent dirichlet allocation for large-
scale applications,” in International Conference on Algorithmic
Applications in Management. Springer, 2009, pp. 301–314.

[29] Pegah Ghahremani, Vimal Manohar, Hossein Hadian, Daniel
Povey, and Sanjeev Khudanpur, “Investigation of transfer learning
for asr using lf-mmi trained neural networks,” in Proc. of ASRU
2017, pp. 279–286.

[30] Dong Wang and Thomas Fang Zheng,

“Transfer learning for
speech and language processing,” in Proc. of APSIPA 2015, 2015,
pp. 1225–1237.

[31] Zhen Huang, Sabato Marco Siniscalchi, and Chin-Hui Lee, “A
uniﬁed approach to transfer learning of deep neural networks with
applications to speaker adaptation in automatic speech recogni-
tion,” Neurocomputing, vol. 218, pp. 448–459, 2016.

9. References
[1] Chung-Cheng Chiu, Tara N Sainath, Yonghui Wu, Rohit Prab-
havalkar, Patrick Nguyen, Zhifeng Chen, Anjuli Kannan, Ron J
Weiss, Kanishka Rao, Ekaterina Gonina, et al., “State-of-the-art
speech recognition with sequence-to-sequence models,” in Proc.
of ICASSP 2018, pp. 4774–4778.

[2] Hossein Hadian, Hossein Sameti, Daniel Povey, and Sanjeev Khu-
danpur, “End-to-end speech recognition using lattice-free mmi,”
in Proc. of INTERSPEECH 2018, pp. 12–16.

[3] Duc Le, Xiaohui Zhang, Weiyi Zheng, Christian F¨ugen, Geoffrey
Zweig, and Michael L Seltzer, “From senones to chenones: Tied
context-dependent graphemes for hybrid speech recognition,” in
Proc. of ASRU, pp. 457–464.

[4] Shigeki Karita, Nelson Enrique Yalta Soplin, Shinji Watanabe,
Marc Delcroix, Atsunori Ogawa, and Tomohiro Nakatani, “Im-
proving Transformer-Based End-to-End Speech Recognition with
Connectionist Temporal Classiﬁcation and Language Model Inte-
gration,” in Proc. of INTERSPEECH 2019, pp. 1408–1412.

[5] Tian Tan, Yanmin Qian, Hu Hu, Ying Zhou, Wen Ding, and Kai
Yu, “Adaptive very deep convolutional residual network for noise
robust speech recognition,” IEEE/ACM TASLP, vol. 26, no. 8, pp.
1393–1405, 2018.

[6] Michael L Seltzer, Dong Yu, and Yongqiang Wang, “An investiga-
tion of deep neural networks for noise robust speech recognition,”
in Proc. of ICASSP 2013, pp. 7398–7402.

[7] Vineel Pratap, Anuroop Sriram, Paden Tomasello, Awni Han-
nun, Vitaliy Liptchinsky, Gabriel Synnaeve, and Ronan Collobert,
“Massively multilingual asr: 50 languages, 1 model, 1 billion pa-
rameters,” arXiv preprint arXiv:2007.03001, 2020.

[8] Jaejin Cho, Murali Karthick Baskar, Ruizhi Li, Matthew Wies-
ner, Sri Harish Mallidi, Nelson Yalta, Martin Karaﬁat, Shinji
Watanabe, and Takaaki Hori, “Multilingual sequence-to-sequence
speech recognition: Architecture, transfer learning, and language
modeling,” in IEEE SLT 2018, pp. 521–527.

[9] Shinji Watanabe, Takaaki Hori, and John R Hershey, “Language
independent end-to-end architecture for joint language identiﬁca-
tion and speech recognition,” in Proc. of ASRU 2017, pp. 265–
271.

[10] Najim Dehak, Patrick J Kenny, R´eda Dehak, Pierre Dumouchel,
and Pierre Ouellet, “Front-end factor analysis for speaker veriﬁ-
cation,” IEEE TASLP, vol. 19, no. 4, pp. 788–798, 2011.

[11] David Snyder, Daniel Garcia-Romero, Gregory Sell, Daniel
Povey, and Sanjeev Khudanpur,
“X-vectors: Robust dnn em-
beddings for speaker recognition,” in Proc. of ICASSP 2018, pp.
5329–5333.

[12] Yingke Zhu, Tom Ko, David Snyder, Brian Mak, and Daniel
Povey, “Self-attentive speaker embeddings for text-independent
speaker veriﬁcation,” in Proc. of INTERSPEECH 2018, pp. 3573–
3577.

[13] Mousmita Sarma, Pegah Ghahremani, Daniel Povey, Nagen-
dra Kumar Goel, Kandarpa Kumar Sarma, and Najim Dehak,
“Emotion identiﬁcation from raw speech signals using dnns,” in
Proc. of INTERSPEECH 2018, pp. 3097–3101.

[14] Jianyou Wang, Michael Xue, Ryan Culhane, Enmao Diao, Jie
Ding, and Vahid Tarokh, “Speech emotion recognition with dual-
sequence lstm architecture,” in Proc. of ICASSP 2020, pp. 6474–
6478.

[15] Zhong-Qiu Wang and Ivan Tashev, “Learning utterance-level rep-
resentations for speech emotion and age/gender recognition using
deep neural networks,” in Proc. of ICASSP 2017, pp. 5150–5154.

[16] Maryam Najaﬁan and Martin Russell, “Automatic accent identi-
ﬁcation as an analytical tool for accent robust automatic speech
recognition,” Speech Communication, 2020.

[17] Daniel Povey, Gaofeng Cheng, Yiming Wang, Ke Li, Hainan
Xu, Mahsa Yarmohammadi, and Sanjeev Khudanpur,
“Semi-
orthogonal low-rank matrix factorization for deep neural net-
works,” in Proc. of INTERSPEECH 2018, pp. 3743–3747.

