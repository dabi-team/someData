BANG: Bridging Autoregressive and Non-autoregressive Generation with
Large Scale Pretraining

Weizhen Qi 1 2 Yeyun Gong * 3 Jian Jiao * 4 Yu Yan * 4 Weizhu Chen * 4 Dayiheng Liu * 2 5 Kewen Tang 4
Houqiang Li 1 Jiusheng Chen 4 Ruofei Zhang 4 Ming Zhou 3 Nan Duan 3

1
2
0
2

n
u
J

6
1

]
L
C
.
s
c
[

3
v
5
2
5
5
1
.
2
1
0
2
:
v
i
X
r
a

Abstract

In this paper, we propose BANG, a new pre-
to Bridge the gap between
training model
Autoregressive (AR) and Non-autoregressive
(NAR) Generation. AR and NAR generation can
be uniformly regarded as to what extent previ-
ous tokens can be attended, and BANG bridges
AR and NAR generation by designing a novel
model structure for large-scale pretraining. The
pretrained BANG model can simultaneously sup-
port AR, NAR and semi-NAR generation to meet
different requirements. Experiments on question
generation (SQuAD 1.1), summarization (XSum)
and dialogue generation (PersonaChat) show that
BANG improves NAR and semi-NAR perfor-
mance signiﬁcantly as well as attaining compara-
ble performance with strong AR pretrained mod-
els. Compared with the semi-NAR strong base-
lines, BANG achieves absolute improvements of
14.01 and 5.24 in the overall scores of SQuAD
1.1 and XSum, respectively. In addition, BANG
achieves absolute improvements of 10.73, 6.39
and 5.90 in the overall scores of SQuAD, XSUM
and PersonaChat respectively compared with the
strong NAR baselines.

1. Introduction

Various pretraining methods (Song et al., 2019; Lewis et al.,
2019; Qi et al., 2020; Raffel et al., 2020; Zhang et al., 2019a)
have been successfully applied in natural language gener-
ation. Most of the pretraining works are based on Trans-
former and designed with autoregressive (AR) language
model. Transformer based pretraining models show consis-

*Equal contribution 1University of Science and Technology of
China, Hefei, China 2During Internship at MSRA 3Microsoft Re-
search Asia, Beijing, China 4Microsoft, Redmond, USA 5Sichuan
University, Chengdu, China. Correspondence to: Yeyun Gong
<yegong@microsoft.com>.

tent improvements with larger model size and larger pretrain-
ing corpus. Although the autoregressive generation method
achieves high-quality results in many tasks, its latency is a
well-known limitation for online real-time usage.

Non-autoregressive (NAR) models (Gu et al., 2017; Lee
et al., 2018; Ghazvininejad et al., 2019; Raffel et al., 2020;
Zhang et al., 2019a) are proposed to reduce generation la-
tency. Different from AR models which generate tokens
sequentially, NAR models generate tokens in parallel. Com-
pared to AR models, NAR models generally come with a
much lower inference latency, but a decrease in accuracy. In
order to balance latency and accuracy, semi-NAR generation
models (Stern et al., 2019; Lee et al., 2018; Gu et al., 2019;
Ghazvininejad et al., 2019) are proposed. However, most of
the NAR and semi-NAR models focus on translation tasks
rather than general natural langauge generation tasks, which
are proved to signiﬁcantly beneﬁt from pretraining (Qi et al.,
2020; Lewis et al., 2019). Some works (Guo et al., 2020b;
Su et al., 2021) initialize their NAR models with pretrained
natural language understanding model BERT (Devlin et al.,
2018) for better performance. To the best of our knowledge,
this paper proposes the ﬁrst large-scale pretraining model
designed for NAR and semi-NAR generation.

In this paper, we propose a new model named BANG 1 to
bridge the gap between AR and NAR via pretraining a gen-
erative model. Speciﬁcally, we consider pretraining model
using AR, semi-NAR and NAR objectives with different
attention mechanisms, which decide what extent previous
tokens can be attended to. Precisely, BANG is pretrained to
predict each token with arbitrary length of previous golden
tokens replaced with special token [MASK]s. For example,
with complete previous golden tokens, BANG predicts the
next token in the AR manner. With all previous tokens re-
placed by [MASK], BANG predicts the next token in the
NAR manner.

For AR models, the training strategy of teacher-forcing is
commonly used, which uses the golden tokens as previous
context to predict the next token. For NAR models, [MASK]
initialization (Ghazvininejad et al., 2019) or other methods

Proceedings of the 38 th International Conference on Machine
Learning, PMLR 139, 2021. Copyright 2021 by the author(s).

1https://github.com/microsoft/BANG

 
 
 
 
 
 
BANG: Bridging Autoregressive and Non-autoregressive Generation with Large Scale Pretraining

like encoder copy (Gu et al., 2017) and posterior distribu-
tion approximation (Shu et al., 2020) are applied. In BANG
pretraining, we consider the previous context of golden and
[MASK] tokens, with arbitrary golden tokens’ length and
[MASK] tokens’ length. To achieve an efﬁcient implemen-
tation for multiple arbitrary alternatives in a same output
sequence, we propose a new structure named cross-stream
visible n-stream self-attention, which can be used to train
BANG with different golden and [MASK] tokens’ combina-
tions. For usage on downstream tasks, the single pretrained
BANG model can be directly ﬁnetuned for either vanilla AR
models or vanilla NAR models. Additionally, BANG can
also be ﬁnetuned for hybrid semi-NAR models, which sup-
port predicting tokens with arbitrary previous golden tokens
or [MASK]. Concretely, for semi-NAR generation, BANG
predicts the ﬁrst several tokens one by one as a high-quality
sub-sequence hint, then produces all the remaining tokens
simultaneously.

Our main contributions are: 1) BANG bridges the gap
between AR and NAR by considering arbitrary previous
[MASK] length during large-scale pretraining. 2) BANG is
pretrained using an efﬁcient cross-stream visible n-stream
decoder to realize parallelization. Given multiple arbitrary
number of previous tokens replaced with [MASK], every
token is trained to predict simultaneously at each time step.
3) BANG supports NAR, semi-NAR and AR ﬁnetuning to
meet different requirements with the same pretrained model
structure. 4) We pretrain BANG with 16GB English lan-
guage corpora of Wikipedia and BookCorpus, and ﬁnetune
it on 3 popular natural language generation tasks in AR,
semi-NAR and NAR manners, respectively. For NAR and
semi-NAR ﬁnetuning, BANG achieves signiﬁcant perfor-
mance improvements on all the tasks. For AR ﬁnetuning
with the comparison to strong AR pretrained models, BANG
can attain comparable performance.

2. Model Structure

In this section, we ﬁrst introduce fundamental concepts
in § 2.1, and then describe BANG model structure and
implementation in § 2.2. Finally the ﬁnetuning strategies
are given in § 2.3.

2.1. Preliminaries

Natural language generation typically refers to the procedure
of generating an output sequence Y = {y1, y2, ..., y|Y |}
given an input sequence X = {x1, x2, ..., x|X|}. There
are three common generative paradigms: AR, NAR and
semi-NAR generation.

AR generation follows the conditional probability:

P (Y |X) =

|Y |
(cid:89)

t=1

P (yt|y<t, X)

(1)

Each token yt in the target sequence Y is predicted with the
dependency of input sequence X and previous tokens y<t.
Vanilla Transformer realizes this target by shifting decoder
inputs one position, each token attending to its previous
tokens to predict the next token.

NAR generation follows the conditional probability:

P (Y |X) =

|Y |
(cid:89)

t=1

P (yt|X)

(2)

Each token yt in target sequence Y is predicted with the
dependency of input sequence X but no dependency of
previous tokens. Each token in the target sequence should
be predicted simultaneously. An intuitive strategy is to
feed a list of [MASK] tokens as the decoder initialization
(Ghazvininejad et al., 2019).

Semi-NAR generation follows the conditional probabil-
ity:

P (Y |X) =

|Y |
(cid:89)

t=1

P (yt|Yct, X)

(3)

Here, Yct means visible context for yt from target sequence
Y . Yct is designed differently in different semi-NAR mod-
els (Stern et al., 2019; Lee et al., 2018; Ghazvininejad et al.,
2019; Gu et al., 2019).

2.2. BANG

In this section, we will introduce the overall model structure
of BANG in § 2.2.1 and the proposed cross-stream visible
n-stream self-attention in § 2.2.2.

2.2.1. OVERALL

BANG is a sequence to sequence model based on the
Transformer Encoder-Decoder structure. It is composed
of a multi-layer Transformer encoder with vanilla self-
attention mechanism and a multi-layer Transformer de-
coder with proposed cross-stream visible n-stream self-
attention mechanism. We consider the input sequence
X = {x1, x2, . . . , x|X|} and output sequence Y =
{y1, y2, . . . , y|Y |}. On the encoder side, BANG ﬁrstly en-
codes X into a list of hidden states by stacked vanilla Trans-
former encoder layers:

Henc = Encoder(x1, . . . , x|X|),

(4)

BANG: Bridging Autoregressive and Non-autoregressive Generation with Large Scale Pretraining

Figure 1. BANG pretraining. The prediction of y4 is used as an example. “M-S” and “P-S” represent “Main stream” and “Predicting
stream”, respectively. The 1st predicting stream predicts tokens in AR manner. First tokens in each predicting stream compose NAR
prediction.

On the decoder side, BANG predicts each token yt in out-
put sequence Y with cross-stream visible n-stream self-
attention. Besides predicting each token with all previous
golden tokens in the AR manner and all [MASK]s in a NAR
manner, BANG targets at predicting them with arbitrary
length of previous golden tokens replaced with [MASK]s
as the context. Formally, for yt in Y :

P (yt|y<t, x), P (yt|y<t−1, x), P (yt|y<t−2, x),
. . . , P (yt|y1, x), P (yt|x) = Decoder(y<t, Henc)

(5)

The conditional probability of target sequence in BANG can
be described as:

P ( ˆY |X) =

|Y |
(cid:89)

t
(cid:89)

t=1

i=1

P (yt|y≤t−i, X)

(6)

LBANG = logP ( ˆY |X)
|Y |
(cid:88)

t
(cid:88)

=

logP (yt|y≤t−i, X)

t=1

i=1

=

|Y |
(cid:88)

t=1
(cid:124)

logP (yt|y<t, X)

(cid:123)(cid:122)
AR part

(cid:125)

|Y |
(cid:88)

t−1
(cid:88)

+

logP (yt|y≤t−i, X)

t=2
(cid:124)

|Y |
(cid:88)

t=1
(cid:124)

+

i=2

(cid:123)(cid:122)
Bridging part

(cid:125)

logP (yt|X)

(cid:123)(cid:122)
NAR part

(cid:125)

(7)

BANG targets at optimizing ˆY rather than the original out-
put sequence Y . For each token yt in Y , ˆY considers replac-
ing its i previous tokens with [MASK] for any i < t. For
maximum likelihood training with a cross-entropy loss:

We can see the BANG optimizing target is composed of
three parts: AR part, NAR part and bridging part. AR
part and NAR part directly beneﬁt down-stream generation
pattern. The bridging part composes a curriculum learning
path to beneﬁt NAR from high-accuracy AR learning.

𝑦1𝑦3𝑀Decoder𝑦4𝑦2𝑦2𝑦1𝑦3𝑦4𝑀𝑀𝑀𝑀𝑀𝑦3𝑦2𝑦4𝑦5𝑦1M-S1stP-S𝑦2𝑦1𝑦3𝑦4𝑀𝑀𝑀𝑀𝑦3𝑦2𝑦4𝑦5M-S2ndP-S𝑀𝑀𝑀𝑀𝑀𝑦3𝑦2𝑦4𝑦5𝑦11stP-S𝑦1𝑀𝑀Decoder𝑦4𝑦2𝑦3𝑦2𝑦1𝑦3𝑦4M-S𝑀𝑀𝑀𝑀𝑀𝑦3𝑦2𝑦4𝑦5𝑦11stP-S𝑀𝑀𝑀𝑀𝑦3𝑦2𝑦4𝑦52ndP-S𝑀𝑀𝑀𝑦3𝑦4𝑦53rdP-S 𝑀𝑀𝑀Decoder𝑦4𝑀𝑦3𝑦2𝑀𝑀𝑦4𝑦54thP-S𝑦1… …… …ARNAR1stP-SBANG: Bridging Autoregressive and Non-autoregressive Generation with Large Scale Pretraining

2.2.2. CROSS-STREAM VISIBLE N-STREAM

SELF-ATTENTION

To implement the decoder of BANG, we design a cross-
stream visible n-stream self-attention based on the Trans-
former decoder. There are self-attention sub-layers, encoder-
decoder attention sub-layers and feed-forward sub-layers
in Transformer decoder layers. Encoder-decoder attention
sub-layers and feed-forward sub-layers are used to fetch
information from encoded hidden states and do linear calcu-
lation. We keep these two kinds of sub-layers unchanged.
Self-attention sub-layers control the information ﬂow and
we propose cross-stream visible n-stream self-attention to
replace it.

We duplicate decoder into one main stream and n predicting
streams where n equals to target sequence length by sharing
parameters. Main stream is fed with golden tokens as golden
previous context. Predicting streams are fed with [MASK]s
to predict corresponding tokens and these [MASK]s also
serve as previous tokens for NAR generation. For the tokens
in the ith predicting stream, their i − 1 previous tokens are
replaced by [MASK] from its previous predicting streams,
and further previous tokens are golden tokens from the main
stream. Each token in the 1st predicting stream is predicted
in AR pattern with all previous golden tokens from main
stream visible. The ﬁrst tokens in each predicting stream
compose the NAR generation with all previous visible to-
kens as previous predicting streams’ [MASK]s.

Figure 1 illustrates how BANG bridges the AR-NAR gen-
eration with the prediction of y4 as an example. In the 1st
predicting stream, [MASK] for y4 attends to real y1, y2 and
y3 from main stream golden tokens. Each token in the 1st
predicting stream works the same as y4 and is pretrained
in AR manner in parallel. In the 2nd predicting stream,
[MASK] for y4 attends to real y1 and y2 from main stream
golden token and [MASK] for y3 from 1st predicting stream.
y3 in the 1st predicting stream and y4 in the 2nd predict-
ing stream are generated with the conditional probability of
P (y3, y4|y0, y1) as shown on the left of the ﬁgure. We can
see increasing difﬁculty in the 2nd predicting stream and
the transition from AR to NAR as more previous tokens are
replaced with [MASK]s. In the 4th predicting stream, y4 is
predicted with no previous golden tokens visible but only
[MASK]s from its previous predicting streams are visible.
Note that for each token in the target sequence, arbitrary
previous golden tokens replaced with [MASK]s are con-
sidered in BANG, and all predicting streams are computed
simultaneously. At each time step, all tokens in the target
sequence from AR pattern to NAR pattern are predicted
simultaneously.

To reduce GPU memory and computational cost, we adopt
block-wise attention calculation. Since only the previous
predicting streams are visible, each predicting stream is only

concatenated with previous streams to avoid extra computa-
tion cost. The workﬂow in decoder layer l can be described
in Algorithm 1.

Algorithm 1 Cross-stream Visible N-stream Self-attention

Input: hl
Output: hl+1
Kcache ← ∅
Vcache ← ∅
hl+1 ← ∅
Q, K, V = Linear(hl)
for Qi, Ki, Vi in split(Q, K, V) do

Kcache ← Kcache ⊕ Ki
Vcache ← Vcache ⊕ Vi
hl+1 ← hl+1 ⊕ Attn(Qi, Kcache, Vcache)

end for
return hl+1

Function Linear has three outputs: Q, K, V for self-
attention from input hidden states hl. ⊕ denotes the concate-
nate operator, and function Attn is deﬁned in equation 8:

Attn(Q, K, V ) = sof tmax(

QK T
√
d

+ L)V

(8)

, where d represents the dimension for Q,K and V vectors,
L represents the relative positional bias and mask matrix to
make sure only visible tokens’s V can be attended to.

2.3. BANG Finetuning

In § 2.2.1 and § 2.2.2, we introduce the model structure
and its language model for pretraining. In this section, we
will introduce BANG ﬁnetuning strategy for AR, NAR and
semi-NAR generation.

2.3.1. AR GENERATION

BANG AR generation pattern is same as XLNet (Yang et al.,
2019) 2-stream strategy. We take the prediction of y3 as an
example in Figure 2 with the encoder part omitted.

Figure 2. BANG AR generation. The [MASK] can attend to its
previous golden tokens to predict y4.

On the left, BANG predicts y4 with y1, y2 and y3 visible
as P (y4|y1, y2, y3). During model training, to predict each

BANG: Bridging Autoregressive and Non-autoregressive Generation with Large Scale Pretraining

token in parallel, BANG duplicates decoder layers into one
main stream and one predicting stream as shown on the right
of Figure 2. [MASK] in the predicting stream attends to its
previous tokens from main stream to predict corresponding
position’s target token.

2.3.2. NAR GENERATION

BANG NAR generation pattern is same as vanilla Trans-
former. We take the prediction of y4 as an example in
Figure 3 with the encoder part omitted.

Figure 3. BANG NAR generation. The [MASK] can only attend
to its previous [MASK]s to predict y3.

On the left, we can see BANG predicts y4 with no context
information from target sequence Y but a list of [MASK]s.
On the right, a main difference between BANG NAR and
CMLM or NAT is that [MASK] in BANG decoder can only
attend to its previous [MASK]s rather than every token in
the decoder. We design BANG in this attention manner
with two reasons: 1) To be consistent with BANG AR and
beneﬁt from AR-NAR bridging pretraining. 2) If [MASK]
in the decoder has bi-directional attention, the number of fed
[MASK] will inﬂuence the result, under which condition a
length predictor is needed. If the model predicts the wrong
target sequence length, the decoder has to ﬁll all the extra
tokens or terminate unexpectedly (early). In BANG, only
previous [MASK]s are visible, and the ﬁrst generated [EOS]
token is considered the signal of sentence end token as
traditional AR models.

2.3.3. SEMI-NAR GENERATION

For BANG semi-NAR generation, visible context Yct is
designed as complete golden context visible for ﬁrst sev-
eral tokens in AR manner and golden+[MASK] for the rest
tokens. BANG semi-NAR training procedure is same as pre-
training implementation introduced in § 2.2 and we show
the inference procedure in Figure 4.

Figure 4. BANG semi-NAR generation. BANG semi-NAR infer-
ence supports predicting ﬁrst several tokens step by step then gen-
erating rest tokens at one timestep. Here we present the inference
procedure. The training procedure is same to Figure 1.

pressions. High-quality AR generated sub-sequence can
help NAR step about choosing the writing style and other
dependencies. We can also generate the outputs in vanilla
AR or NAR manner with this ﬁnetuning strategy.

3. Experiments and Results

In this section, we ﬁrst introduce the experimental setup and
datasets in § 3.1, and report the main experimental results
in § 3.2. Next, we conduct a comparison between the NAR
pretraining and the BANG pretraining in § 3.3. Last, we put
some case studies for each dataset in the Appendix.

3.1. Setup and Dataset

3.1.1. SETUP

We pretrain BANG using the 16GB corpus (Wikipedia and
BookCorpus). The BANG model has a 6-layer encoder and
a 6-layer decoder, with a dimension of 768 in their hidden
states, to be consistent with AR pretrained models (Song
et al., 2019; Qi et al., 2020; Lewis et al., 2019) base version
model size.

We implement BANG with the span mask and prediction
pretraining task, same as MASS (Song et al., 2019). In each
block of an input sentence, we mask up to n continuous
tokens for prediction, where n is same as the number of
predicting streams. We mask 15% tokens for every 64
tokens in the input sequence span, which makes the target
sequence length in each span as 9. BANG has 9 predicting
streams, same as sequence length. We pretrain BANG from
scratch with a learning rate of 3e-4 for 35 epochs and a
batch size of 2048. BANG uses the same dictionary as
BERT-uncased (Devlin et al., 2018).

We can see that BANG predicts ﬁrst two tokens one by one
in AR manner. Then the AR generated sub-sequence is
used as a hint for the rest tokens. A list of [MASK]s are
appended to be predicted at one time step in NAR manner.
The motivation of this semi-NAR generation pattern is that
NAR models always mix up target sequence different ex-

For AR generation, we load the BANG pretrained model
and ﬁnetune it with teacher forcing strategy. We use the
AR model Transformer (Vaswani et al., 2017) without pre-
training, strong pretrained AR models MASS (Song et al.,
2019), BART (Lewis et al., 2019) and ProphetNet (Qi et al.,
2020) as our baselines. Most of the AR baseline results are

BANG: Bridging Autoregressive and Non-autoregressive Generation with Large Scale Pretraining

collected from GLGE (Liu et al., 2020). We collect the
base version (6-layers encoder, 6-layers decoder, 768 hid-
den sizes) baseline models results to be compared with same
model size BANG for fair comparison. BANG AR ﬁnetun-
ing hyper-parameters are: learning rate 1e-4, warm up steps
of 1000, Adam optimizer, the maximum input and output
length of 512, and a label smoothness of 0.1. We ﬁnetune
BANG on each dataset for 10 epochs and select the best
model according to the performance on their dev sets. For
inference, we set the beam size as 4, length penalty as 1.0
and batch size as 1 to calculate the latency.

For NAR generation, we load the BANG pretrained model
and ﬁnetune it with all [MASK] inputs. We use NAT (Gu
et al., 2017), iNAT (Lee et al., 2018), CMLM (Ghazvinine-
jad et al., 2019) and LevT (Gu et al., 2019) as the NAR
baseline models. For these baselines, we select the out-
puts from the ﬁrst iteration as the NAR outputs if they are
semi-NAR models. For NAR ﬁnetuning experiments, the
hyper-parameters are the same as AR ﬁnetuning except the
number of ﬁnetuning epochs, since NAR ﬁnetuning need
more epochs to converge. We ﬁnetune BANG for 50 epochs
and save a checkpoint for every 10 epochs. We select the
best checkpoint based on the performance on dev set. For
inference, we set the no-repeat-ngram hyper-parameter as
2 to merge consecutive same tokens. Since we consider
the ﬁrst [EOS] token as the end signal rather than predict-
ing the target sentence length, we set the maximal output
length as 50, 85, 30 for SQuAD, XSum and PersonaChat,
respectively.

For semi-NAR generation, we load the BANG pretrained
model and ﬁnetune it with multiple predicting streams to
simultaneously support AR inference and NAR inference.
We set the maximum output length and predicting stream
numbers as 30, 30, 40 for SQuAD, PersonaChat, XSum,
respectively.
In other words, the model produced from
multi-stream ﬁnetuning can simultaneously support AR
inference, NAR inference and semi-NAR inference. For
semi-NAR, we predict the ﬁrst nar tokens using a token-by-
token manner sequentially, followed by predicting the last
nnar = n − nar tokens in parallel via a single step. We set
nar as 5, and nnar as 25, 25, 35 for SQuAD, PersonaChat
and XSum, respectively. The semi-NAR strategy in BANG
is quite ﬂexible to support different sequential and parallel
combinations, and we leave it as future work for further ex-
ploration. For semi-NAR baselines, we choose InsT (Stern
et al., 2019), iNAT (Lee et al., 2018), CMLM (Ghazvinine-
jad et al., 2019), LevT (Gu et al., 2019) and set the maxi-
mum iteration steps as 10 (one decoding followed by up to
nine iterative reﬁnements).

For NAR and semi-NAR experiments, we do not utilize
knowledge distillation (Hinton et al., 2015; Kim & Rush,
2016) from AR models. For NAR generation, knowledge

distillation usually refers to ﬁnetuning with the inference
outputs from an AR model (Kim & Rush, 2016; Gu et al.,
2017), in other words, the ﬁnetuning input keeps unchanged,
while the targt sequence is replaced with the output from an
AR model. We do not utilize knowledge distillation for two
reasons: We want to use the same ﬁne-tuning corpus as AR
models to compare the performance difference; Knowledge
distillation with different generated results and different
AR models will result in different NAR performance and
it’s hard for other researchers to get the same ﬁnetuning
corpus for reproducing our experiments. NAR and semi-
NAR baseline models are integrated into Fairseq library (Ott
et al., 2019) and we use the default hyper-parameters2 to
carry out experiments.

For all downstream tasks, we use 8 NVIDIA Tesla V100
GPUs for ﬁnetuning and one single V100 GPU for inference.
All the experiments are conducted on the Fairseq (Ott et al.,
2019) v0.9.0 codebase and we use the built-in time statistics
function to calculate the per-sample inference latency.

3.1.2. DATASETS

We conduct experiments on following three popular genera-
tion benchmarks:

XSum (Narayan et al., 2018) contains 227K online article
and single sentence summary pairs from the British Broad-
casting Corporation (BBC). The average input and output
lengths are 358.5 and 21.1, respectively.

SQuAD 1.1 (Rajpurkar et al., 2016) is a dataset created
for machine reading comprehension. After preprocessing,
the dataset contains 98K (cid:104)answer, passage, question(cid:105) data
triples. Input is formatted as (cid:104)answer [SEP] passage(cid:105) fol-
lowing GLGE. The average input and output lengths are
149.4 and 11.5, respectively.

PersonaChat (Zhang et al., 2018) is a dataset created for
multi-turn conversation with personalizing proﬁles . After
preprocessing, the dataset contains 150k (cid:104)persona proﬁle
description text, conversation history, response(cid:105) data triples.
Input is formatted as (cid:104)proﬁle [SEP] conversation history(cid:105)
following GLGE. The average input and output lengths are
120.8 and 11.8, respectively.

3.2. Main Results

We present the results for question generation task in Ta-
ble 1, summarization task in Table 2, and dialog task in
Table 3. BANG achieves signiﬁcant performance improve-
ments on all tasks consistently in both the NAR and the
semi-NAR settings. Compared with the best semi-NAR
baselines, BANG achieves absolute improvements of 14.01
and 5.24 in overall scores of SQuAD and XSum, respec-

2Fairseq NAR Baseline Models

BANG: Bridging Autoregressive and Non-autoregressive Generation with Large Scale Pretraining

Pattern

Methods

SQuAD 1.1

Table 1. Results of semi-NAR, NAR and AR on SQuAD 1.1.

Semi-NAR

NAR

AR

InsT (Stern et al., 2019)
iNAT (Lee et al., 2018)
CMLM (Ghazvininejad et al., 2019)
LevT (Gu et al., 2019)
BANG
NAT (Gu et al., 2017)
iNAT (Lee et al., 2018)
CMLM (Ghazvininejad et al., 2019)
LevT (Gu et al., 2019)
BANG
Transformer (Vaswani et al., 2017)
MASS (Song et al., 2019)
BART (Lewis et al., 2019)
ProphetNet (Qi et al., 2020)
BANG

ROUGE-L BLEU-4 METEOR
2.34
3.16
3.89
2.68
17.62
2.46
2.33
2.51
2.27
12.75
4.61
20.16
17.08
19.58
21.40

8.15
9.18
9.70
9.40
21.69
8.86
8.84
8.85
9.14
18.99
9.86
24.41
23.19
23.94
24.25

29.98
32.34
29.60
30.81
47.39
31.51
32.44
31.58
31.38
44.07
29.43
49.48
42.55
48.00
49.32

OVERALL
13.49 (+0.00)
14.89 (+1.40)
14.40 (+0.91)
14.30 (+0.81)
28.90 (+15.41)
14.28 (+0.02)
14.54 (+0.28)
14.31 (+0.05)
14.26 (+0.00)
25.27 (+11.01)
14.63(+0.00)
31.35 (+16.72)
27.61 (+12.98)
30.51 (+15.88)
31.66 (+17.03)

Latency
ms/Sample
67.61 (4.3x)
31.59 (2.0x)
106.84 (6.8x)
116.41 (7.4x)
111.11 (7.1x)
17.11 (1.1x)
16.52 (1.1x)
16.41 (1.0x)
27.52 (1.8x)
15.69 (1.0x)
159.49(10.2x)
N/A
N/A
N/A
N/A

Pattern

Methods

Table 2. Results of semi-NAR, NAR and AR on XSum.

XSum

Semi-NAR

NAR

AR

InsT (Stern et al., 2019)
iNAT (Lee et al., 2018)
CMLM (Ghazvininejad et al., 2019)
LevT (Gu et al., 2019)
BANG
NAT (Gu et al., 2017)
iNAT (Lee et al., 2018)
CMLM (Ghazvininejad et al., 2019)
LevT (Gu et al., 2019)
BANG
Transformer (Vaswani et al., 2017)
MASS (Song et al., 2019)
BART (Lewis et al., 2019)
ProphetNet (Qi et al., 2020)
BANG

ROUGE-1 ROUGE-2 ROUGE-L
5.18
6.88
7.70
7.40
11.71
3.88
3.99
3.60
4.18
8.98
10.80
17.24
16.16
17.12
18.37

16.05
22.43
23.04
21.48
29.16
20.32
20.36
20.15
20.87
27.41
24.48
31.91
30.61
32.07
33.22

17.65
26.95
29.12
25.33
34.71
24.04
24.02
23.82
24.75
32.59
30.66
39.70
38.79
39.89
41.09

OVERALL
12.96 (+0.00)
18.75 (+5.79)
19.95 (+6.99)
18.07 (+5.11)
25.19 (+12.23)
16.08 (+0.22)
16.12 (+0.26)
15.86 (+0.00)
16.60 (+0.74)
22.99 (+7.13)
21.98(+0.00)
29.62 (+7.64)
28.52 (+6.54)
29.69 (+7.71)
30.89 (+8.91)

Latency
ms/Sample
63.37 (4.0x)
31.27 (2.0x)
113.64 (7.1x)
101.01 (6.3x)
109.77 (6.9x)
17.47 (1.1x)
16.94 (1.1x)
16.88 (1.1x)
27.72 (1.7x)
15.97 (1.0x)
262.47(16.4x)
N/A
N/A
N/A
N/A

tively. In addition, BANG achieves absolute improvements
of 10.73, 6.39 and 5.90 in overall scores of SQuAD, XSUM
and PersonaChat compared with the best NAR baselines,
respectively. This clearly demonstrates the effectiveness
of the BANG pretraining. The NAR or semi-NAR results
based on BANG are comparable or even better than the AR
generation methods without pretraining.

From Table 1, we can see that via BANG pretraining, the
results on semi-NAR and NAR generation are signiﬁcantly
improved. Meanwhile, the results on AR generation via
BANG pretraining are comparable with strong pretrained
baselines. On the other hand, both BANG semi-NAR and
NAR models outperforms Transformer AR baselines with
obviously reduced latency. For example, the per sample
inference latency of Transformer model is 159.49ms while
that of BANG NAR is only 15.69ms , which amounts to
a 10.2 times speedup. In terms of the inference speed, we
can see that BANG is slightly better than other NAR gen-
eration models. This speedup is mainly due to the fact that

BANG NAR uses the special token [SEP] as the ending
mark, without the need to predict the length.

Similar to the Question Generation tasks, the improvement
on the XSum in Table 2 is also signiﬁcant. In all the three
categories semi-NAR, NAR and AR, BANG can outperform
all of the baselines consistently. Meanwhile, BANG’s NAR
and semi-NAR results are better than AR generation models
without pretraining. Furthermore, we observe an interesting
result when studying both the SQuAD and XSum together:
BANG semi-NAR consistently outperforms NAR in both
tasks, while the baseline models show that semi-NAR via
iterative reﬁnements can help XSum summarization but
often hurt SQuAD question generation.

For the results on PersonaChat in Table 3, we focus on the
dialog outputs diversity Distinct-1 and Distinct-2 metrics.
For dialog generation, the outputs are very open such that
the outputs diversity is deeply concerned to avoid boring,
or useless dialog responses. We can see the D-1 and D-2

BANG: Bridging Autoregressive and Non-autoregressive Generation with Large Scale Pretraining

Table 3. Results of semi-NAR, NAR, and AR on PersonaChat. D-1(Distinct-1), D-2(Distinct-2) are multiplied by 100 according to GLGE.

Pattern

Methods

PersonaChat

Semi-NAR

NAR

AR

InsT (Stern et al., 2019)
iNAT (Lee et al., 2018)
CMLM (Ghazvininejad et al., 2019)
LevT (Gu et al., 2019)
BANG
NAT (Gu et al., 2017)
iNAT (Lee et al., 2018)
CMLM (Ghazvininejad et al., 2019)
LevT (Gu et al., 2019)
BANG
Transformer (Vaswani et al., 2017)
MASS (Song et al., 2019)
BART (Lewis et al., 2019)
ProphetNet (Qi et al., 2020)
BANG

BLEU-1 BLEU-2 D-1 D-2
0.3
1.1
0.8
0.6
14.2
0.8
0.7
0.6
0.4
22.7
0.8
6.9
6.1
7.3
8.4

9.43
32.13
35.18
18.94
30.72
24.17
23.38
24.06
20.47
23.90
32.95
35.75
39.36
38.40
35.54

12.63
41.17
44.38
24.89
39.82
31.53
30.56
31.44
26.92
31.11
41.56
41.06
47.60
46.00
45.77

0.1
0.1
0.1
0.1
1.9
0.1
0.1
0.1
0.0
2.5
0.3
1.4
1.1
1.3
1.4

OVERALL
5.62 (+0.00)
18.63 (+13.01)
20.12 (+14.50)
11.13 (+5.51)
21.66 (+16.04)
14.15 (+2.20)
13.69 (+1.74)
14.05 (+2.10)
11.95 (+0.00)
20.05 (+8.10)
18.90(+0.00)
21.28 (+2.38)
23.54(+4.64)
23.25 (+4.35)
22.78 (+3.88)

Latency
ms/Sample
65.27 (4.4x)
43.25 (2.9x)
105.82 (7.1x)
80.26 (5.4x)
109.17 (7.3x)
17.86 (1.2x)
16.40 (1.1x)
16.26 (1.1x)
27.56 (1.9x)
14.89 (1.0x)
138.31(9.3x)
N/A
N/A
N/A
N/A

of models without pretraining are very low, which means
even the inputs are different, their outputs are highly over-
lapped or just repetitive. For NAR baseline models, we
observe the outputs are often composed of “i, a, an, the” and
punctuation, which may share some repetitive parts with
target sequence to achieve a reasonable B-1 or B-2 score,
but lacking meaningful responses. For semi-NAR baseline
models, we observe the iterative reﬁnement can help pro-
duce a ﬂuent response. However, the generated responses
are often unrelated to the dialog context with a low accu-
racy. Baseline AR models have high B-1 and B-2 scores via
producing some common phrases like “i have”, “have a”, “a
lot”, but meaningless dialog responses. On contrary, from
BANG, we observe meaningful response and high diversity
in the response, which makes it achieve a good diversity
score consistently in all three categories: the high-speed
NAR generation, the high-performance AR generation, and
the semi-NAR generation. We list more examples in the
Appendix for a detailed illustration.

3.3. NAR Results with AR, NAR, BANG Pretraining

Note that NAR baseline models in § 3.2 are not pretrained.
Here we provide AR and NAR pretrained models to com-
pare with BANG. We load the parameters from MASS as
the AR pretrained model and carry out NAR ﬁnetuning.
NAR pretraining setting is same as BANG pretraining, ex-
cept that we replace the cross-stream visible multi-stream
decoder with a single decoder ﬁlled with [MASK] to make
it consistent with the NAR ﬁnetuning.

The results for NAR ﬁnetuning results with different pre-
training settings are shown in Table 4 and Table 5. First,
pretraining improves NAR ﬁnetuning results signiﬁcantly
and consistently in both tasks. Second, AR pretrained mod-
els perform worse than NAR pretrained models because AR
pretraining has inconsistent language model between pre-

Table 4. NAR results on SQuAD 1.1 question generation with
different pretraining strategy.

Pretrain Strategy ROUGE-L
No-pretrain
AR(MASS)
NAR
BANG

30.69
40.20
41.15
44.07

BLEU-4 METEOR

2.07
9.50
11.38
12.75

8.22
16.00
17.63
18.99

Table 5. NAR results on XSum summarization with different pre-
training strategy.

Pretrain Strategy ROUGE-1 ROUGE-2 ROUGE-L
No-Pretrain
AR(MASS)
NAR
BANG

23.86
26.10
30.43
32.50

20.12
22.50
25.50
27.35

4.17
6.30
7.66
8.92

training and ﬁnetuning. Third, BANG, with the same NAR
ﬁnetuning procedure and model architecture(the n-stream
archtecture difference only exists in the pretraining, and
in the NAR ﬁnetuning procedure, two models have the ex-
actly same architecture), consistently outperforms the NAR
pretrained model. This clearly demonstrates that the pro-
posed pretraining strategy via bridging both AR and NAR
is critical to achieve a better performance in NAR genera-
tion. Lastly, the improvements of BANG are consistent and
signiﬁcant in both tasks across all metrics.

4. Related Work

AR models have been developed for a long time. Recent
works show that pretraining on large scale text will guaran-
tee a consistent improvements on downstream generation
tasks. GPT series work (Radford et al., 2018; 2019; Brown
et al., 2020) pretrains decoders with the task of next token
prediction and convert different downstream tasks into lan-
guage models. MASS (Song et al., 2019) masks continuous

BANG: Bridging Autoregressive and Non-autoregressive Generation with Large Scale Pretraining

words’ spans from input sentences to predict. BART (Lewis
et al., 2019) uses denoising task to pretrain Transformer.
ProphetNet (Qi et al., 2020) deploys future tokens’ predic-
tion to enhance AR generation ability. DialogGPT (Zhang
et al., 2019b) is pretrained for conversational response gen-
eration. XLNet (Yang et al., 2019) utilizes AR pretraining
for downstream natural language understanding tasks, and
we also borrow 2-stream strategy from XLNet for BANG
cross-stream visible n-stream decoder.

NAR and semi-NAR models are proposed to accelerate nat-
ural language generation. NAT (Gu et al., 2017) is proposed
as a NAR generation model to decode the whole target se-
quence at one time step. iNAT (Lee et al., 2018) reﬁnes
outputs with multi-turn post-processing. InsT (Stern et al.,
2019) predicts inserting positions and inserting tokens at
each iteration. CMLM (Ghazvininejad et al., 2019) ﬁrstly
predict all target words with NAR generation and maskout-
regenerate low conﬁdence words. LevT (Gu et al., 2019)
considers two basic operations insertion and deletion at each
iteration.

Some works propose to use AR generation facilitate NAR
generation and some work propose future tokens’ prediction
to facilitate AR generation. Curriculum learning (Bengio
et al., 2009) is used to beneﬁt NAR generation from AR gen-
eration such as Guo et al. (2020a). Future tokens’ prediction
is used to beneﬁt AR generation such as Qi et al. (2020);
Goodman et al. (2020). Recently, algorithms (Tian et al.,
2020; Mansimov et al., 2019) to support different generation
patterns with the same model start to be researched.

BANG beneﬁts from these different NAR and AR genera-
tion models to support NAR, AR, semi-NAR generation.

5. Conclusion

We propose a new natural language generation pretraining
model named BANG. BANG bridges NAR and AR gener-
ation with cross-stream visible n-stream strategy and large
scale pretraining. BANG supports NAR, AR and semi-
NAR generation with the same pretrained model. Experi-
ments show that BANG can signiﬁcant improve the NAR
and semi-NAR generation performance, and provide com-
parable results with strong AR pretraining models. NAR
and semi-NAR generation can be applied to general natu-
ral language generation tasks with acceptable performance
with BANG pretraining. BANG is powerful and ﬂexible to
support more diverse semi-NAR generation strategies and
ﬁnetuning strategies, which we leave as future work.

References

Bengio, Y., Louradour, J., Collobert, R., and Weston, J.
Curriculum learning. In Proceedings of the 26th annual
international conference on machine learning, pp. 41–48,
2009.

Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan,
J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,
Askell, A., et al. Language models are few-shot learners.
arXiv preprint arXiv:2005.14165, 2020.

Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert:
Pre-training of deep bidirectional transformers for lan-
guage understanding. In NAACL, 2018.

Ghazvininejad, M., Levy, O., Liu, Y., and Zettlemoyer, L.
Mask-predict: Parallel decoding of conditional masked
arXiv preprint arXiv:1904.09324,
language models.
2019.

Goodman, S., Ding, N., and Soricut, R. Teaforn: Teacher-
forcing with n-grams. arXiv preprint arXiv:2010.03494,
2020.

Gu, J., Bradbury, J., Xiong, C., Li, V. O., and Socher, R.
Non-autoregressive neural machine translation. arXiv
preprint arXiv:1711.02281, 2017.

Gu, J., Wang, C., and Zhao, J. Levenshtein transformer. In
Advances in Neural Information Processing Systems, pp.
11181–11191, 2019.

Guo, J., Tan, X., Xu, L., Qin, T., Chen, E., and Liu,
T.-Y.
Fine-tuning by curriculum learning for non-
autoregressive neural machine translation. In Proceed-
ings of the AAAI Conference on Artiﬁcial Intelligence,
volume 34, pp. 7839–7846, 2020a.

Guo, J., Zhang, Z., Xu, L., Wei, H.-R., Chen, B., and Chen,
E. Incorporating bert into parallel sequence decoding
with adapters. arXiv preprint arXiv:2010.06138, 2020b.

Hinton, G., Vinyals, O., and Dean, J.
the knowledge in a neural network.
arXiv:1503.02531, 2015.

Distilling
arXiv preprint

Kim, Y. and Rush, A. M. Sequence-level knowledge distil-

lation. arXiv preprint arXiv:1606.07947, 2016.

Lee, J., Mansimov, E., and Cho, K. Deterministic non-
autoregressive neural sequence modeling by iterative re-
ﬁnement. arXiv preprint arXiv:1802.06901, 2018.

Lewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mo-
hamed, A., Levy, O., Stoyanov, V., and Zettlemoyer, L.
Bart: Denoising sequence-to-sequence pre-training for
natural language generation, translation, and comprehen-
sion. arXiv preprint arXiv:1910.13461, 2019.

BANG: Bridging Autoregressive and Non-autoregressive Generation with Large Scale Pretraining

Liu, D., Yan, Y., Gong, Y., Qi, W., Zhang, H., Jiao, J.,
Chen, W., Fu, J., Shou, L., Gong, M., et al. Glge: A
new general language generation evaluation benchmark.
arXiv preprint arXiv:2011.11928, 2020.

Su, Y., Cai, D., Wang, Y., Vandyke, D., Baker, S., Li,
P., and Collier, N. Non-autoregressive text genera-
tion with pre-trained language models. arXiv preprint
arXiv:2102.08220, 2021.

Mansimov, E., Wang, A., Welleck, S., and Cho, K. A
generalized framework of sequence generation with ap-
plication to undirected sequence models. arXiv preprint
arXiv:1905.12790, 2019.

Tian, C., Wang, Y., Cheng, H., Lian, Y., and Zhang, Z. Train
once, and decode as you like. In Proceedings of the 28th
International Conference on Computational Linguistics,
pp. 280–293, 2020.

Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,
L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Attention
is all you need. In NIPS, 2017.

Yang, Z., Dai, Z., Yang, Y., Carbonell, J., Salakhutdinov,
R., and Le, Q. V. Xlnet: Generalized autoregressive
pretraining for language understanding. arXiv preprint
arXiv:1906.08237, 2019.

Zhang, J., Zhao, Y., Saleh, M., and Liu, P. J. Pegasus:
Pre-training with extracted gap-sentences for abstractive
summarization. arXiv preprint arXiv:1912.08777, 2019a.

Zhang, S., Dinan, E., Urbanek, J., Szlam, A., Kiela, D., and
Weston, J. Personalizing dialogue agents: I have a dog,
do you have pets too? In ACL, pp. 2204–2213, 2018.

Zhang, Y., Sun, S., Galley, M., Chen, Y.-C., Brockett, C.,
Gao, X., Gao, J., Liu, J., and Dolan, B. Dialogpt: Large-
scale generative pre-training for conversational response
generation. arXiv preprint arXiv:1911.00536, 2019b.

Narayan, S., Cohen, S. B., and Lapata, M. Don’t give me
the details, just the summary! topic-aware convolutional
neural networks for extreme summarization. In EMNLP,
pp. 1797–1807, 2018.

Ott, M., Edunov, S., Baevski, A., Fan, A., Gross, S., Ng,
N., Grangier, D., and Auli, M.
fairseq: A fast, ex-
tensible toolkit for sequence modeling. arXiv preprint
arXiv:1904.01038, 2019.

Qi, W., Yan, Y., Gong, Y., Liu, D., Duan, N., Chen, J.,
Zhang, R., and Zhou, M. Prophetnet: Predicting fu-
ture n-gram for sequence-to-sequence pre-training. arXiv
preprint arXiv:2001.04063, 2020.

I.

Radford, A., Narasimhan, K., Salimans, T.,

and
Improving language understanding
Sutskever,
URL https://s3-us-
by generative pre-training.
west-2.
com/openai-assets/research-
covers/languageunsupervised/language understanding
paper. pdf, 2018.

amazonaws.

Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and
Sutskever, I. Language models are unsupervised multitask
learners. OpenAI Blog, 1(8), 2019.

Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,
Matena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring
the limits of transfer learning with a uniﬁed text-to-text
transformer. Journal of Machine Learning Research, 21
(140):1–67, 2020.

Rajpurkar, P., Zhang, J., Lopyrev, K., and Liang, P. Squad:
100,000+ questions for machine comprehension of text.
In EMNLP, pp. 2383–2392, 2016.

Shu, R., Lee, J., Nakayama, H., and Cho, K. Latent-variable
non-autoregressive neural machine translation with deter-
ministic inference using a delta posterior. In AAAI, pp.
8846–8853, 2020.

Song, K., Tan, X., Qin, T., Lu, J., and Liu, T.-Y. Mass:
Masked sequence to sequence pre-training for language
generation. arXiv preprint arXiv:1905.02450, 2019.

Stern, M., Chan, W., Kiros, J., and Uszkoreit, J. Insertion
transformer: Flexible sequence generation via insertion
operations. arXiv preprint arXiv:1902.03249, 2019.

