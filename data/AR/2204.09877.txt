2
2
0
2

r
p
A
1
2

]
E
S
.
s
c
[

1
v
7
7
8
9
0
.
4
0
2
2
:
v
i
X
r
a

Non-autoregressive Model for Full-line Code Completion

Fang Liu1,2 , Zhiyi Fu1,2 , Ge Li1,2âˆ— , Zhi Jin1,2âˆ— , Hui Liu3 , Yiyang Hao4
1Key Laboratory of High Conï¬dence Software Technologies (Peking University), Ministry of Education
2 School of Computer Science, Peking University, Beijing, China
3Beijing Institute of Technology
4Silicon Heart Tech Co., Ltd
{liufang816, ypfzy, lige, zhijin}@pku.edu.com, liuhui08@bit.edu.cn, haoyiyang@nnthink.com

Abstract

Code completion tools are frequently used by soft-
ware developers to accelerate software develop-
ment by suggesting the following code elements.
Completing a sequence of code tokens (e.g., a full
line of code) has been proved more efï¬cient than
predicting a single token at a time. To complete the
code sequence, researchers are employing AutoRe-
gressive (AR) decoders to generate tokens in a left-
to-right, token-by-token fashion. Consequently, the
prediction of the next token depends on all previ-
ously generated tokens, which leads to high latency
in inference. To improve the efï¬ciency and accu-
racy of full-line code completion, in this paper, we
propose a Non-AutoRegressive (NAR) model for
code completion boosted by a syntax-aware sam-
pling strategy. Our experimental results on two
widely used datasets suggest that our model out-
performs both AR and NAR baselines on full-line
code completion, and it is faster than the AR model
with up to 9Ã— speed-up.

1 Introduction
Code completion is one of the most useful features in the In-
tegrated Development Environments (IDEs), improving the
software development efï¬ciency by suggesting future code
In recent years, as the development of machine
snippets.
learning and deep learning technologies and easy-to-acquire
open-source codebases, researchers have started to tackle
code completion by learning from large-scale code corpora.
Various Language Models (LM) have been employed for
code completion, including N-gram [Hindle et al., 2016],
RNN [Li et al., 2018], and Transformer based models [Liu
et al., 2020]. Several popular industry code completion tools,
e.g., Tabnine [2018], aiXcoder [2018], and Copilot [2021],
also rely on LMs to provide suggestions for whole lines or en-
tire functions in a token-by-token manner. Recently, Wang et
al. [2020] further explored full-line code completion. Given
a partially completed code snippet, they built a Transformer-
based LM to predict the next line of code. Although such
code completion algorithms/tools can suggest longer code

âˆ—Corresponding authors.

snippets, they intrinsically are suffering from inefï¬ciency be-
cause of the autoregressive decoding process, where each to-
ken is generated conditioned on all the previously generated
tokens. Consequently, the generation is not parallelizable and
thus particularly slow. However, the efï¬ciency of the gener-
ation process is of great importance for code completion
because it is expected to respond instantly on the developerâ€™s
own devices.

Tokens within a code line have the potential to be predicted
concurrently, although existing code completion algorithms
predict them one by one (from left to right) by exploiting
all tokens previously predicted. For example, multiple argu-
ments of the same method are independent of each other, and
thus they could be predicted independently and concurrently.
In Python code, you can even change the order (position) of
the arguments if a function with the keyword â€˜argumentsâ€™ is
called. For example, Func(arg1=a, arg2=b) is equivalent
to Func(arg2=b, arg1=a). We also argue that even if there
exists strong dependency among tokens within a code line,
the dependency is not necessarily left-to-right. A typical ex-
ample is â€œone line IF-ELSE statementâ€ (Python), formed as:
value_1 if condition else value_2. The value 1 is
dependent on the following condition (on its right-hand side),
instead of the verse. We also perform in-depth analysis in
section 2 to verify that the dependency among code tokens
is weaker than that among tokens in natural languages where
parallelized generation has been employed successfully. Our
analysis in section 2 also suggests that predicting code tokens
from right-to-left or the verse (left-to-right) results in compa-
rable performance. Based on those observations, we conclude
that generating tokens in parallel for code completion is pos-
sible and reasonable.

Non-autoregressive (NAR) model, proposed by Gu et al.
[2018], has been successfully applied to Natural Machine
Translation (NMT) to generate tokens non-autoregressively
through a parallel decoding process:

LN AR =

T
(cid:88)

t=1

log p(yt|X; Î¸)

(1)

It allows an order of magnitude lower latency during infer-
ence. There are two kinds of NAR models: Fully NAR mod-
els [Gu et al., 2018; Ma et al., 2019] and iterative NAR mod-
els [Ghazvininejad et al., 2019; Kasai et al., 2020]. These

 
 
 
 
 
 
NAR models can speed up the inference process of NMT sig-
niï¬cantly compared with autoregressive models. However,
NAR models are hard to train without specially designed
training strategies, as the left-to-right inductive bias among
target tokens is ignored during decoding.

Considering the feasibility of parallel generation of code
and inspired by the success of Non-AutoRegressive gener-
ation in NLP, in this paper, we propose a Syntax-Aware
Non-AutoRegressive model (SANAR) for code completion.
Speciï¬cally, we propose an adaptive and syntax-aware sam-
pling strategy to boost the training process of SANAR, which
dynamically glances some code tokens from the target se-
quence according to their difï¬culties and token types. The
better the model is trained, the fewer code snippets would be
glanced. Once the model is well-trained, it can generate the
whole line of tokens in a single pass. Our experimental results
show that our sampling strategy results in obvious improve-
ment in performance on both Python and Java code com-
pletion. The proposed approach is signiï¬cantly faster than
the widely used Auto-Regressive models, achieving 5 âˆ¼ 6Ã—
speed-up on average, and 9Ã— for long targets.

To summarize, the major contributions of our work are as

follows:

â€¢ An empirical study whose results suggest that it is po-
tentially practical to predict code tokens in parallel.

â€¢ A novel approach for full-line code completion. To the
best of our knowledge, it is the ï¬rst non-autoregressive
approach to code completion. We boost the approach by
an adaptive and syntax-aware sampling strategy, that is
specially designed for source code.

â€¢ A large-scale evaluation of the proposed approach
whose results suggest that our approach outperforms
both AR and NAR baselines on full-line code comple-
tion, and signiï¬cantly reduces the inference time com-
pared with the AR model.

2 Dependency among Generated Code Tokens
We argue that the dependency among the generated tokens in
full-line completion is not as strong as in NMT (where NAR
has been successfully applied), and the left-to-right genera-
tion order is not always optimal for code completion. To ver-
ify this assumption, we conduct experiments to analyze the
dependency among target tokens in the full-line code com-
pletion task in this section.

2.1 Reversing the Order of Code Generation
To verify our conjecture of left-to-right is not always the opti-
mal order for code completion, we conduct an experiment to
analyze the impact of different generation orders. We employ
standard auto-regressive Transformer architecture to perform
left-
full-line code completion using two reverse orders:
to-right (Transformer-L2R) and right-to-left (Transformer-
R2L). Table 1 presents the results. For most of the cases, the
performances of the L2R model and R2L model are compara-
ble. For Python language, 2.65% and 2.47% of programs can
only be correctly generated by L2R and R2L, respectively;
6.57% and 7.21% of the programs can only be approximately

Table 1: The percentage of the code sequence that can be correctly
handled by different models. EM stands for exact match, and ES
stands for edit similarity.

only Transformer-L2R

PY

only Transformer-R2L

Both

only Transformer-L2R

JAVA

only Transformer-R2L

Both

Metrics

Percentage

EM
ES>0.5
EM
ES>0.5
EM
ES>0.5

EM
ES>0.5
EM
ES>0.5
EM
ES>0.5

2.65%
6.57%
2.47%
7.21%
13.59%
60.39%

3.28%
6.71%
3.86%
8.22%
26.15%
60.43%

Code Completion

NMT

)
p
(
R

0.7

0.65

0.6

0.55

0.2

0.3

0.4

0.5

P

Figure 1: The attention density ratio R(P) under different masking
probability P in code completion and NMT.

generated (edit similarity >50%) by L2R and R2L, respec-
tively. For Java programs, we can also get similar results.
Therefore, we conclude that left-to-right is not always the op-
timal order for code completion. Thus generating code in
parallel (non-autoregressively) could be a better choice.

2.2 Quantitative Dependency among Code Tokens

In this section, we measure the quantitative dependency
among the generated tokens by attention density and compare
the dependency among code tokens against that among nat-
ural language tokens (in NMT tasks). NMT task is selected
for comparison because NAR models have been successfully
applied to it.

To characterize and quantify the target-token dependency,
inspired by Ren et al. [2020], we build Dependency-Analysis-
Model (DAM) to measure the dependency among target to-
kens using the ratio of the attention weights on target con-
text over that on full (both source and target) context when
predicting a target token, where a bigger ratio indicates a
larger dependency among target tokens. Speciï¬cally, we
use masked language modeling task [Devlin et al., 2019] to
train DAM. We employ mix-attention [He et al., 2018] to cal-
culate the attention weights, where source tokens can only
attend to source tokens with self-attention and target tokens
can attend to all source and target tokens with mix-attention.
During training, we randomly mask the tokens on the target
side, and the model is trained to predict the original value
of these tokens. After the model has been trained, we can
measure the target token dependency based on the ratio of at-

Figure 2: The training procedure with syntax-aware sampling strategy in SAS.

tention weights Î±i on target context over that on full context
when predicting a speciï¬c target token yi, which is deï¬ned
as:

Î±i =

1
N

(cid:80)N

1
N
j=1 Ai,j + 1
M

(cid:80)N

j=1 Ai,j

(cid:80)N +M

j=N +1 Ai,j

(2)

where Ai,j denotes the attention weights from token i to to-
ken j in mix-attention, and j âˆˆ [1, N ] represents the target
token, and j âˆˆ [N + 1, N + M ] represents the source token.
M and N is the length of source and target input, respec-
tively. (cid:80)N +M
j=1 Ai,j = 1. Î±i represents the ratio of attention
density on target context when predicting target token i. For
a given masking probability P , the ï¬nal attention density ra-
tio R(P ) is calculated by averaging Î±i over all test data. As
seen in Figure 1, the attention density ratio for NMT is big-
ger than code generation for all masking probability P , which
demonstrates the dependency among the target tokens in code
completion is less than NMT.

We conclude based on the preceding analysis that it is po-

tentially practical to predict code tokens in parallel.

3 SANAR
3.1 Overview
In this section, we present our model SANAR in detail. We
build SANAR to perform full-line code completion in par-
allel, which uses conditional independent factorization for
the target tokens. Speciï¬cally, given the contextual code se-
quence X = {x1, x2, ..., xm}, the non-autoregressive full-
line code completion task is to predict a sequence of tokens
Y = {y1, y2, ..., yn} that form a complete statement in a non-
autoregressive way:

P (y1, ..., yn|x1, x2, ..., xm) =

n
(cid:89)

t=1

P (yt|x1, x2, ..., xm) (3)

Figure 2 shows the architecture and the trainig procedure of
our model. SANAR adopts the encoder-decoder transformer
architecture: a context encoder that does self-attention, and
a generated-code decoder that has one set of attention heads
over the encoderâ€™s output and another set (self-attention) for
the generated code. During the training procedure, we pro-
pose an adaptive syntax-aware sampling strategy, which en-
able SANAR to generate target code sequences in a single-
pass during inference procedure by gradual training.

3.2 Training
During training, SANAR adopts an adaptive Syntax-Aware
Sampling (SAS) strategy to dynamically glance code snip-
pets from the target sequence depending on its difï¬culty
and the tokensâ€™ syntax types, aiming to incorporate the ex-
plicit syntactic information of the program. To achieve the
sampling strategy, SANAR performs decoding twice dur-
ing training. As shown in Figure 2, the ï¬rst decoding is
performed in a fully-NAR way, where the input to the de-
coder H = {h1, h2, ..., hn} are copied from the encoder
output using soft-copy [Wei et al., 2019], which maps the
encoder embeddings S = {s1, s2, ..., sm} into target length
H = {h1, h2, ..., hn} depending on the distance relationship
between the source position i and the target position j. The
initial predicted tokens Ë†Y are predicted as:

Ë†Y = fdec(soft-copy(fenc(X; Î¸(cid:48))); Î¸)
(4)
The prediction accuracy indicates the difï¬culty of ï¬tting cur-
rent target. In the second decoding, SANAR samples words
of the targets as the extra decoding input by SAS sampling
according to the ï¬rst decoding results and the syntax infor-
mation of the target tokens, and learn to predict the rest words
that are not selected. It is important to note that only the sec-
ond decoding will update the model parameters.

The training objective is to maximize the following:

LSAS =

(cid:88)

log P (yt|X, SAS(Y, Ë†Y , T ); Î¸)

ytâˆˆY \SAS(Y, Ë†Y ,T )

(5)
where Ë†Y is the initially predicted tokens in a fully-NAR de-
coding way. T is the syntax type sequence of the target se-
quence, and SAS(Y, Ë†Y , T ) is a subset of tokens selected via
the adaptive SAS strategy.

3.3 Syntax-Aware Sampling Strategy
Syntax-Aware Sampling (SAS) strategy is an important com-
ponent of SANAR, which adaptively selects the positions of
tokens from the target sequence depending on its difï¬culty
and the syntax types of the tokens. The selected tokens can
provide â€œcorrectâ€ information from the ground-truth target,
thus can reduce the burden of decoder to predict the rest non-
selected tokens in the training phase. SAS samples more to-
kens for SANAR to glance at the beginning, and then reduces
the sampling number gradually, which helps SANAR to learn,
eventually, how to predict the whole line code snippet without
seeing any ground truth tokens in one pass.

Encoderğ‘¥ğ‘¥1ğ‘¥ğ‘¥2ğ‘¥ğ‘¥3ğ‘¥ğ‘¥4ğ‘ ğ‘ 2ğ‘ ğ‘ 3ğ‘ ğ‘ 4ğ‘ ğ‘ 1Shared Decoderâ„2â„3â„4â„1ï¿½ğ‘¦ğ‘¦2ï¿½ğ‘¦ğ‘¦4SAS Moduleğ‘¦ğ‘¦1ğ‘¦ğ‘¦3Soft Copyâ„5ï¿½ğ‘¦ğ‘¦5ï¿½ğ‘¦ğ‘¦2ï¿½ğ‘¦ğ‘¦3ï¿½ğ‘¦ğ‘¦4ï¿½ğ‘¦ğ‘¦1ï¿½ğ‘¦ğ‘¦5â„2â„4â„5(ğ‘¦ğ‘¦1,ğ‘¡ğ‘¡1)(ğ‘¦ğ‘¦5,ğ‘¡ğ‘¡5)â‹¯ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†ğ‘†(ğ‘Œğ‘Œ,ï¿½ğ‘Œğ‘Œ,ğ‘‡ğ‘‡)(ğ‘¦ğ‘¦2,ğ‘¡ğ‘¡2)Shared DecoderFormally, given the context code sequence X, its predicted
token sentence Ë†Y , the ground truth Y , and the tokenâ€™s syn-
tax type sequence of the target T , the goal of SAS strategy
SAS(Y, Ë†Y , T ) is to obtain a subset of tokens sampled from
Y . Speciï¬cally, there are two steps:
1) Deciding the sampling numbers N depending on the difï¬-
culty of correctly generating the target token sequence:

N = Î» Â· dis(Y, Ë†Y )

(6)

N is computed by comparing the difference between Ë†Y and
Y , and we adopt the Hamming distance as the metric follow-
ing Qian et al. [2021], dis(Y, Ë†Y ) = (cid:80)T
t=1(yt (cid:54)= Ë†yt). Î» is the
sampling ratio, which is a hyper-parameter to ï¬‚exibly control
the number of sampled tokens. More tokens will be selected
and fed as input of the second-pass decoding if the networkâ€™s
initial prediction is less accurate. Thus, the sampling num-
ber can be decided adaptively considering the current trained
modelâ€™s prediction capability and the training sampleâ€™s com-
plexity.
2) Sampling N tokens from the target sequence. The most
direct method is to randomly select N tokens from Y like
in Qian et al. [2021]. However, we argue that considering
the syntactic information of the code explicitly during token
selection will be beneï¬cial for understanding the programs,
and thus can help train the decoder to predict the rest tokens
more precisely.
In programs, the keyword, identiï¬ers, and
operators contain more symbolic and syntactic information
than other tokens (for example, literals and separators in Java,
like â€˜;â€™, â€˜{â€™, â€˜}â€™, etc.). Glancing these tokens will help a lot.

To capture the syntactic information of the code during
the sampling procedure, we present a Hybrid-Syntax-Guided
(HSG) sampling strategy. Speciï¬cally, 1 âˆ’ p of the time, the
sampling is performed randomly, where the tokens are ran-
domly selected from the target sequence; and p of the time,
tokens are selected depending on their syntax-types1, we ran-
domly select K â‰¤ N/2 keywords, I â‰¤ N/4 identiï¬ers, and
O â‰¤ N/4 operators from Y , where K + I + O â‰¤ N 2. The
reason for introducing randomness (randomly sampling for
1âˆ’p of the time) in the sampling process is to enable SANAR
to explore more interdependency among target tokens. Com-
pared with the totally randomly sampling strategy in Qian et
al. [2021], our Hybrid-Syntax-Guided sampling strategy can
increase the probability of glancing the keyword, operators,
and identiï¬ers, which can help the SANAR capture the pro-
gramâ€™s syntactic and semantic information better.

Finally, we can obtain a subset of words sampled from Y :

SAS(Y, Ë†Y , T ) = HSG(Y, N, p)

N = Î» Â· dis(Y, Ë†Y )

(7)

Inference

3.4
As SANAR is well-tuned, it will adaptively reduce the per-
centage of sampling, making sure that the trained model
could learn to generate the whole line of code in the single

1p is a hyper-parameter to control the probability of syntax-

guided sampling, the best setting is p = 30%

2The number of these speciï¬c tokens might be not enough

Table 2: Dataset statistics.

Python

Java

# of training pairs
# of testing pairs
Avg. tokens in cxt
Avg. tokens in tgt

7,531,208
3,693,213
90.8
9.4

12,993,112
671,236
71.2
6.9

pass. Thus, the inference of SANAR is fully parallel with
only a single pass.

In traditional left-to-right code completion, where the tar-
get sequence is predicted token by token, the length of the
sequence can be determined by predicting a special EOS
(end of sentence) token. However, for NAR-based generation
where the entire sequence is predicted in parallel, the length
of the target sequence must be determined in advance. To
achieve this, we follow Ghazvininejad et al. [2019] to add an
additional [LENGTH] token to the source input, and the en-
coder output for the [LENGTH] token is used to predict the
length, formed as a max-target-length classiï¬cation task. We
use 1,000 as the max length. The loss is added to the cross-
entropy loss from the target sequence.

4 Experiments
4.1 Experimental Settings
Datasets. We conduct experiments on two program bench-
marks crawled from Github repositories: Py150 [Raychev et
al., 2016] contains 150,000 Python ï¬les, split into a training
set of 100,000 ï¬les and test set of 50,000 ï¬les, GitHub-Java
[Allamanis and Sutton, 2013] contains 14,317 projects, we
follow Hellendoorn and Devanbu [2017] and Karampatsis et
al. [2020] to split validation and test set, and randomly sam-
ple 1,000 projects from the rest projects as the training set.

We use the Python ofï¬cial library tokenizer3 and Javalang4
to split Python and Java programs into lines of tokens, and
extract their syntax types. Then we employ a sliding context
window of 10-lines to create the data pairs, that is, the context
code sequence contains 10-lines of code tokens, and the next
line is considered as the target code sequence. The detailed
information of the datasets is shown in Table 2.
Metrics and Baselines. We use the following metrics to eval-
uate the performance of our approach:

â€¢ Exact Match Accuracy (EM): We compare the ex-
act matching accuracy between the generated code se-
quence and the ground truth.

â€¢ BLEU: We use the BLEU score to measure the n-gram
similarity between the target code sequence and the gen-
erated code sequence.

â€¢ Edit similarity (ES): We use the character-level edit sim-
ilarity of the predicted output Ë†Y and the target output Y ,
which is computed as:

ES = 1 âˆ’

Lev( Ë†Y , Y )
| Ë†Y | + |Y |

(8)

where Lev is Levenshtein distance.

3https://docs.python.org/3/library/tokenize.html
4https://github.com/c2nes/javalang

Table 3: Results on Python benchmark.

Table 4: Results on Java benchmark.

Model

BLEU

EM

ES

Latency

Speedup

Model

BLEU

EM

ES

Latency

Speedup

Transformer

CMLM
GLAT

SANAR

21.93

23.98
26.78

29.07

16.24

62.73

121ms

13.44
16.95

63.82
66.36

18.35

66.90

39ms
20ms

20ms

1.0Ã—

3.1Ã—
6.1Ã—

6.1Ã—

Transformer

CMLM
GLAT

SANAR

25.73

28.97
30.56

32.41

30.33

63.95

101ms

28.36
28.63

63.66
65.80

30.57

66.98

31ms
20ms

19ms

1.0Ã—

3.3Ã—
5.1Ã—

5.3Ã—

â€¢ Latency: The inference latency is computed as the time
to decode a single target code sentence without mini
batching, averaged over the whole test set. The decoding
is implemented in PyTorch on a single NVIDIA Tesla
V100.

We compare our model with the following baselines, includ-
ing both strong AR Transformer and several representative
NAR models:

â€¢ Transformer [Vaswani et al., 2017]: Base Transformer
seq2seq architecture, where the decoder is AR. We also
tried the TransformerLM decoder architecture in Wang
et al. [2020] using the comparable number of param-
eters. The results show that seq2seq architecture can
outperform the LM decoder in performance, and with
a comparable decoding speed. Thus we use Transformer
(seq2seq) model as the strong AR baseline, and also use
seq2seq framework as our base architecture instead of
LM decoder.

â€¢ CMLM [Ghazvininejad et al., 2019]: A strong iterative-
NAR model, which adopts multi-pass decoding strategy
to reï¬ne the target tokens.

â€¢ GLAT [Qian et al., 2021]: A strong fully-NAR model,
which can generate high-quality target only with single-
pass parallel decoding.

Setup. For all the seq2seq baselines and our model, we use
a 6-layers of encoder and decoder with a model size of 5125.
We set the vocabulary size to 50,000 for both Python and Java
datasets. We train the model with batches of 16k tokens us-
ing Nvidia V100 GPUs. We set the dropout rate to 0.1 and
use Adam optimizer with Î² = (0.9, 0.999). The learning rate
warms up to 5e-5 in 4k steps and gradually decays according
to the inverse square root of the step number [Vaswani et al.,
2017]. Following Qian et al. [2021], we set Î» to 0.3. For the
hyper-parameter p in Hybrid-Syntax-Guided sampling strat-
egy, we use 30% as the ï¬nal value, which can achieve the
best results. The speciï¬c syntax types used for guiding the
sampling include keyword, operator, and identiï¬er.

4.2 Main Results
The main results on Python and Java benchmarks are pre-
sented in Table 3 and 4. SANAR signiï¬cantly improves
the full-line code completion quality and outperforms all the
baselines on all evaluation metrics. Especially, SANAR out-
performs strong AR baseline (Transformer) by a large margin
in BLEU. We can observe that the improvements on BLEU

5For TransformerLM [Wang et al., 2020], to keep the compara-
ble number of the parameters, we employ a 12-layers decoder with
a model size of 512.

Table 5: Speed-up on long target sequences.

Model

Latency

Speed up

Transformer
CMLM
GLAT
SANAR

171ms
34ms
19ms
19ms

1.0Ã—
5.0Ã—
9.0Ã—
9.0Ã—

and Edit Similarity are more signiï¬cant than the Exact Match
accuracy. As a recommendation tool, code completion add-in
serves as developersâ€™ cooperator, and developers can accept
to make a few edits to correct the recommended code. Be-
sides, different code snippets can have identical functional-
ity. For example, these two Python return statements have the
same functionality: return True if a==0 else False
and return False if a!=0 else True. Only evaluat-
ing the quality of the generated code by comparing the ex-
act match with the ground truth is not convincing enough.
Thus, BLEU and Edit Similarity which calculate the token
overlapping and the minimum number of operations required
to transform the predicted code sequence into the target se-
quence can evaluate the generated code more thoroughly.
Thus, the signiï¬cant improvements on BLEU and ES fur-
ther demonstrate that SANAR can generate the code sequence
which meets the developersâ€™ expect more.

It is interesting to note that, most of the NAR models
can achieve higher BLEU and ES scores than the AR-based
Transformer model, and can achieve better or comparable EM
accuracy on all the datasets. These results are quite different
from NMT task, where the NAR models can achieve worse
or comparable results with the AR model. The results fur-
ther verify our assumptions in section 2, that is, the non-
autoregressive manner can ï¬t code completion better than
NMT, and can boost the performance on both efï¬ciency and
quality for code completion.

For the inference latency, SANAR can signiï¬cantly reduce
the inference time within 5 âˆ¼ 6Ã— speedup compared with the
AR-based Transformer model. As a fully-NAR model, the in-
ference speed of SANAR is also faster than the iterative-NAR
CMLM model. Compared with NMT, the target sequence in
full-line code completion is shorter, thus the overall speedup
is not as large as in NMT, but is still signiï¬cant. We also
present the latency comparison for completing lines of â‰¥ 10
tokens, which account for about 30% of the test set. The re-
sults are shown in Table 5. As SANAR can generate tokens
in parallel, thus the latency will not be affected by the tar-
get lengths, and still keeps comparable with previous results.
However, for AR models, as the number of generated tokens
increases, the latency increases a lot, and SANAR can achieve
9Ã— speedup. Thus, when completing longer token sequences,
the speedup of SANAR will be more signiï¬cant.

Table 6: Performance of full-line code completion with different
sampling probabilities.

p

0
0.15
0.3
0.5

BLEU

26.78
28.20
29.07
27.37

Python
EM

16.95
16.96
18.35
17.31

ES

BLEU

65.94
66.36
66.90
66.30

30.56
32.16
32.41
31.29

Java
EM

28.63
29.94
30.57
29.82

ES

65.80
66.81
66.98
66.43

Table 7: Token repetition ratio results.

Transformer
CMLM
GLAT
SANAR

Python

Java

3.72% 4.36%
3.66% 4.92%
4.55% 4.56%
5.52% 5.40%

[Li et al., 2018; Liu et al., 2020]. Wang et al. [2020] pro-
pose to use a Transformer-based LM to perform full-line code
completion. In full-line code completion, instead of predict-
ing the next single token, the model predicts a sequence of
tokens that form a complete statement given a partially com-
plete code context. The experimental results show that Trans-
former seq2seq architecture can outperform the LM decoder
in Wang et al. [2020] with comparable number of parameters.
Thus, we use Transformer seq2seq architecture as the AR
baseline, and also employ seq2seq architecture for SANAR
instead of only using a decoder as Wang et al. [2020]. Guo et
al. [2021] proposed GRAMMFORMER, a transformer-based
grammar-guided code generation model that generates code
sketches, i.e. sequences of code tokens with holes. As the
generation target is different from our model, we did not com-
pare with them.

4.3 Analysis

5.2 NAR Models of Text Generation

Inï¬‚uence of Parameter p.
To analyze how the sampling
probability p of our Hybrid-Syntax-Guided sampling strat-
egy affects the modelâ€™s performance, we conduct experiments
with different ps, where a larger p indicates more tokens are
sampled depending on their syntax-types. When p is zero,
the sampling will become totally random, which is the same
with GLAT [Qian et al., 2021]. The results for different ps
are listed in Table 6. When p is increased from 0 to 0.3, the
performance becomes better, demonstrating the effectiveness
of the syntax-guided sampling. When p is further increased,
where the randomness decreases, the performance began to
drop, but still outperforms GLAT. This suggests that it is also
necessary to introduce randomness during the syntax-aware
sampling, which can help SANAR explore more interdepen-
dency among target tokens.
Token Repetition Ratio.
In NMT, one of the known prob-
lems in non-autoregressive models is the high token repeti-
tion ratio, since the interdependency among target tokens is
not considered. We are also interested in whether this prob-
lem also troubles code completion. We measure the percent-
age of repeated tokens on the test set of Python and Java
benchmarks. The results are shown in Table 7. As seen
from the results, the token repetition ratio of the NAR-based
models is not increased obviously when compared with the
AR-based Transformer model, which is quite different from
NMT. These results also demonstrate that the left-to-right in-
terdependency among target tokens in the code completion
task is much weaker than that of NMT task. Thus, the non-
autoregressive decoder can ï¬t code completion better, and
achieve better performance.

5 Related Work

5.1 Code Completion

Since Hindle et al. [2016] have shown that source code is
highly repetitive and predictable, language models began to
be used for source code modeling and code completion [Hin-
dle et al., 2016; Tu et al., 2014; Hellendoorn and Devanbu,
2017; Li et al., 2018; Liu et al., 2020]. Most of the LM-based
code completion models perform next one token completion

Fully NAR The Fully Non-AutoRegressive model consists
of the same encoder as Transformer and a parallel decoder
[Gu et al., 2018]. During training, it uses the conditional in-
dependent factorization for the target sentence by maximizing
the following likelihood:

LN AR = log P (Y |X; Î¸) =

T
(cid:88)

tâˆ’1

log p(yt|X; Î¸)

(9)

During inference, the encoder representation is copied as the
input for the decoder, therefore all tokens on the target side
can be generated in parallel without depending on the previ-
ously generated tokens. Recently, Qian et al. [2021] propose
GLAT, which can achieve parallel text generation with only a
single decoding pass by gradual training.
Iterative NAR The conditional independence assumption
in fully NAR does not hold in general, resulting in inferior
performance.
In order to improve the fully NAR model,
multi-pass iterative decoding approaches such as CMLM
Ghazvininejad et al. [2019] is proposed, which ï¬rst predicts
all of the target words non-autoregressively, and then repeat-
edly mask out and regenerate the words that the model is least
conï¬dent about using the masking scheme.

6 Conclusion

In this paper, we propose SANAR, a syntax-aware non-
autoregressive model to improve the efï¬ciency and accuracy
of full-line code completion. We perform an in-depth analysis
to explore the dependency among the target tokens, and the
results show that the dependency is weaker than the assump-
tion in existing code generation models, suggesting that NAR
can ï¬t code generation better. Experimental results show
that our approach outperforms both NAR and AR baselines,
as well as signiï¬cantly reduces the inference time compared
with AR based code completion models. We are the ï¬rst to
apply Non-autoregressive model for generating code. We be-
lieve this work represents a signiï¬cant advance in code gen-
eration, which will be beneï¬cial as a building block for many
other code generation applications.

sequence generation with generative ï¬‚ow. In EMNLP/IJC-
NLP (1), pages 4281â€“4291. Association for Computational
Linguistics, 2019.

Lihua Qian, Hao Zhou, Yu Bao, Mingxuan Wang, Lin Qiu,
Weinan Zhang, Yong Yu, and Lei Li. Glancing trans-
former for non-autoregressive neural machine translation.
In ACL/IJCNLP (1), pages 1993â€“2003. Association for
Computational Linguistics, 2021.

Veselin Raychev, Pavol Bielik, and Martin T. Vechev. Prob-
abilistic model for code with decision trees. In OOPSLA,
pages 731â€“747. ACM, 2016.

Yi Ren, Jinglin Liu, Xu Tan, Zhou Zhao, Sheng Zhao, and
Tie-Yan Liu. A study of non-autoregressive model for se-
In ACL, pages 149â€“159. Association
quence generation.
for Computational Linguistics, 2020.

Tabnine. Tabnine. https://www.tabnine.com/, 2018.

Zhaopeng Tu, Zhendong Su, and Premkumar Devanbu. On
In Proceedings of the 22nd
the localness of software.
ACM SIGSOFT International Symposium on Foundations
of Software Engineering, pages 269â€“280, 2014.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, Åukasz Kaiser, and Il-
lia Polosukhin. Attention is all you need. In Advances in
neural information processing systems, pages 5998â€“6008,
2017.

Wenhan Wang, Sijie Shen, Ge Li, and Zhi Jin. Towards full-
line code completion with neural language models. arXiv
preprint arXiv:2009.08603, 2020.

Bingzhen Wei, Mingxuan Wang, Hao Zhou, Junyang Lin, and
Xu Sun. Imitation learning for non-autoregressive neural
machine translation. In ACL (1), pages 1304â€“1312. Asso-
ciation for Computational Linguistics, 2019.

References
aiXcoder. aixcoder. https://www.aixcoder.com/, 2018.
Miltiadis Allamanis and Charles Sutton. Mining source code
repositories at massive scale using language modeling. In
MSR, pages 207â€“216. IEEE Computer Society, 2013.

Copilot. Copilot. https://copilot.github.com/, 2021.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
Toutanova. BERT: pre-training of deep bidirectional trans-
formers for language understanding. In NAACL-HLT (1),
pages 4171â€“4186. Association for Computational Linguis-
tics, 2019.

Marjan Ghazvininejad, Omer Levy, Yinhan Liu, and Luke
Zettlemoyer. Mask-predict: Parallel decoding of condi-
tional masked language models. In EMNLP/IJCNLP (1),
pages 6111â€“6120. Association for Computational Linguis-
tics, 2019.

Jiatao Gu, James Bradbury, Caiming Xiong, Victor O. K. Li,
and Richard Socher. Non-autoregressive neural machine
translation. In ICLR (Poster). OpenReview.net, 2018.

Daya Guo, Alexey Svyatkovskiy, Jian Yin, Nan Duan, Marc
Brockschmidt, and Miltiadis Allamanis. Learning to gen-
arXiv preprint arXiv:2106.10158,
erate code sketches.
2021.

Tianyu He, Xu Tan, Yingce Xia, Di He, Tao Qin, Zhibo Chen,
and Tie-Yan Liu. Layer-wise coordination between en-
coder and decoder for neural machine translation. In Pro-
ceedings of the 32Nd International Conference on Neural
Information Processing Systems, pages 7955â€“7965, 2018.
Vincent J Hellendoorn and Premkumar Devanbu. Are deep
neural networks the best choice for modeling source code?
In Proceedings of the 2017 11th Joint Meeting on Founda-
tions of Software Engineering, pages 763â€“773, 2017.

Abram Hindle, Earl T Barr, Mark Gabel, Zhendong Su, and
Premkumar Devanbu. On the naturalness of software.
Communications of the ACM, 59(5):122â€“131, 2016.

Rafael-Michael Karampatsis, Hlib Babii, Romain Robbes,
Charles Sutton, and Andrea Janes. Big code!= big vo-
cabulary: Open-vocabulary models for source code.
In
2020 IEEE/ACM 42nd International Conference on Soft-
ware Engineering (ICSE), pages 1073â€“1085. IEEE, 2020.
Jungo Kasai, James Cross, Marjan Ghazvininejad, and Jiatao
Gu. Non-autoregressive machine translation with disentan-
gled context transformer. In International Conference on
Machine Learning, pages 5144â€“5155. PMLR, 2020.

Jian Li, Yue Wang, Michael R. Lyu, and Irwin King. Code
completion with neural attention and pointer networks. In
IJCAI, pages 4159â€“4165. ijcai.org, 2018.

Fang Liu, Ge Li, Bolin Wei, Xin Xia, Zhiyi Fu, and Zhi Jin.
A self-attentional neural architecture for code completion
In Proceedings of the 28th In-
with multi-task learning.
ternational Conference on Program Comprehension, pages
37â€“47, 2020.

Xuezhe Ma, Chunting Zhou, Xian Li, Graham Neubig, and
Eduard H. Hovy. Flowseq: Non-autoregressive conditional

