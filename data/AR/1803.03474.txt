An end-to-end TextSpotter with Explicit Alignment and Attention

Tong He1,∗, Zhi Tian1,∗, Weilin Huang3, Chunhua Shen1,†, Yu Qiao4, Changming Sun2

1University of Adelaide, Australia 2Data61, CSIRO, Australia 3Malong Technologies
4Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences

2.2. Word Recognition with Character Atten-
tion . . . . . . . . . . . . . . . . . . . .
2.2.1 Attention Mechanism . . . . . .
2.2.2 Attention Alignment and En-
. . . . . . . . . . . .
2.3. Training Strategy . . . . . . . . . . . . .

hancement

3. Experiments

. . . . . . . . . . .
3.1. Evaluation Protocols
3.2. Text-alignment vs. RoI Pooling
. . . . .
3.3. Character Attention . . . . . . . . . . . .
3.4. Joint Training vs. Separate Models
. . .
3.5. Proposed Method vs. State-of-the-art
Methods . . . . . . . . . . . . . . . . . .

6
6

7
8

8
8
9
9
10

10

8
1
0
2

r
a

M
3
2

]

V
C
.
s
c
[

3
v
4
7
4
3
0
.
3
0
8
1
:
v
i
X
r
a

Abstract
Text detection and recognition in natural images
have long been considered as two separate tasks that
are processed sequentially. Training of two tasks in a
uniﬁed framework is non-trivial due to signiﬁcant dif-
ferences in optimisation diﬃculties. In this work, we
present a conceptually simple yet eﬃcient framework
that simultaneously processes the two tasks in one shot.
Our main contributions are three-fold: 1) we propose
a novel text-alignment layer that allows it to precisely
compute convolutional features of a text instance in ar-
bitrary orientation, which is the key to boost the per-
formance; 2) a character attention mechanism is intro-
duced by using character spatial information as explicit
supervision, leading to large improvements in recogni-
tion; 3) two technologies, together with a new RNN
branch for word recognition, are integrated seamlessly
into a single model which is end-to-end trainable. This
allows the two tasks to work collaboratively by shar-
ing convolutional features, which is critical to identify
challenging text instances. Our model achieves im-
pressive results in end-to-end recognition on the IC-
DAR2015 [1] dataset, signiﬁcantly advancing most re-
cent results [2], with improvements of F-measure from
(0.54, 0.51, 0.47) to (0.82, 0.77, 0.63), by using a strong,
weak and generic lexicon respectively. Thanks to joint
training, our method can also serve as a good detec-
tor by achieving a new state-of-the-art detection per-
formance on two datasets.

Contents

1. Introduction

2. Single Shot TextSpotter by Joint Detec-

tion and Recognition
2.1. Text-Alignment Layer . . . . . . . . . . .

2

4
5

Appearing in Proc. IEEE Conf. Computer Vision and Pat-

tern Recognition, 2018.

The ﬁrst two authors contribute equally. C. Shen is the
corresponding author (e-mail: chunhua.shen@adelaide.edu.au).

 
 
 
 
 
 
1. Introduction

The goal of text spotting is to map an input nat-
ural image into a set of character sequences or word
transcripts and corresponding location.
It has at-
tracted increasing attention in the vision community,
due to its numerous potential applications.
It has
made rapid progress riding on the wave of recent deep
learning technologies, as substantiated by recent works
[3, 4, 2, 5, 6, 7, 8, 9, 10, 11]. However, text spotting
in the wild still remains an open problem, since text
instances often exhibit vast diversity in font, scale and
orientation with various illumination aﬀects, which of-
ten come with a highly complicated background.

Past works in text spotting often consider it as
two individual tasks: text detection and word recog-
nition, which are implemented sequentially. The goal
of text detection is to precisely localize all text in-
stances (e.g., words) in a natural
image, and then
a recognition model is processed repeatedly through
all detected regions for recognizing corresponding text
transcripts. Recent approaches for text detection are
mainly extended from general object detectors (such as
Faster R-CNN [12] and SSD [13]) by directly regress-
ing a bounding box for each text instance, or from
semantic segmentation methods (e.g., Fully Convolu-
tional Networks (FCN) [14]) by predicting a text/non-
text probability at each pixel. With careful model de-
sign and development, these approaches can be cus-
tomized properly towards this highly domain-speciﬁc
task, and achieve the state-of-the-art performance
[4, 6, 7, 8, 9, 15]. The word recognition can be cast
into a sequence labeling problem where convolutional
recurrent models have been developed recently [9, 16].
Some of them were further incorporated with an atten-
tion mechanism for improving the performance [17, 18].
However, training two tasks separately does not exploit
the full potential of convolutional networks, where the
convolutional features are not shared. It is natural for
us to make a more reliable decision if we clearly under-
stand or recognize the meaning of a word and all char-
acters within it. Besides, it is also possible to introduce
a number of heuristic rules and hyper-parameters that
are costly to tune, making the whole system highly
complicated.

Recent Mask R-CNN [19] incorporates an instance
segmentation task into the Faster R-CNN [12] de-
tection framework, resulting in a multi-task learning
model that jointly predicts a bounding box and a seg-
mentation mask for each object instance. Our work
draws inspiration from this pipeline, but has a diﬀer-
ent goal of learning a direct mapping between an input
image and a set of character sequences. We create a
recurrent sequence modeling branch for word recogni-

tion within a text detection framework, where the RNN
based word recognition is processed in parallel to the
detection task.

However, the RNN branch, where the gradients are
back-propagated through time, is clearly much more
diﬃcult to optimize than the task of bounding box re-
gression in detection. This naturally leads to signiﬁ-
cant diﬀerences in learning diﬃculties and convergence
rates between two tasks, making the model particularly
hard to be trained jointly. For example, the magnitude
of images for training a text detection model is about
103 (e.g., 1000 training images in the ICDAR 2015 [1])
, but the number is increased signiﬁcantly by many
orders of magnitude when a RNN based text recogni-
tion model is trained, such as the 800K synthetic im-
ages used in [20]. Furthermore, simply using a set of
character sequences as direct supervision may be too
abstractive (high-level) to provide meaningful detailed
information for training such an integrated model ef-
fectively, which will make the model diﬃcult to conver-
gence. In this work, we introduce strong spatial con-
straints in both word and character levels, which allows
the model to be optimized gradually by reducing the
search space at each step.

Contributions In this work, we present a single-
shot textspotter capable of learning a direct mapping
between an input image and a set of character se-
quences or word transcripts. We propose a solution
that combines a text-alignment layer tailed for multi-
orientation text detection, together with a character at-
tention mechanism that explicitly encodes strong spa-
tial information of characters into the RNN branch,
as shown in Fig. 1. These two technologies faithfully
preserve the exact spatial information in both text in-
stance and character levels, playing a key role in boost-
ing the overall performance. We develop a principled
learning strategy that allows the two tasks to be trained
collaboratively by sharing convolutional features. Our
main contributions are described as follows.

Firstly, we develop a text-alignment layer by intro-
ducing a grid sampling scheme instead of conventional
RoI pooling.
It computes ﬁxed-length convolutional
features that precisely align to a detected text region
of arbitrary orientation, successfully avoiding the neg-
ative eﬀects caused by orientation changing and quan-
tization factor of the RoI pooling.

Secondly, we introduce a character attention mecha-
nism by using character spatial information as an addi-
tion supervision. This explicitly encodes strong spatial
attentions of characters into the model, which allows
the RNN to focus on current attentional features in
decoding, leading to performance boost in word recog-
nition.

Figure 1: Illustrations of the results on ICDAR 2015 by our proposed method, which can detect all possible text regions and
recognize relevant transcriptions in just one shot.

Thirdly, both approaches, together with a new RNN
branch for word recognition, are integrated elegantly
into a CNN detection framework, resulting in a single
model that can be trained in an end-to-end manner.
We develop a principled and intuitive learning strategy
that allows the two tasks to be trained eﬀectively by
sharing features, with fast convergence.

Finally, we show by experiments that word recogni-
tion can signiﬁcantly improve detection accuracy in our
model, demonstrating strong complementary nature of
them, which is unique to this highly domain-speciﬁc
application. Our model achieves new state-of-the-art
results on the ICDAR2015 in end-to-end recognition of
multi-orientation texts, largely outperforming the most
recent results in [2], with improvements of F-measure
from (0.54, 0.51, 0.47) to (0.82, 0.77, 0.63) in terms of
using a strong, weak and generic lexicon. Code is avail-
able at https://github.com/tonghe90/textspotter
Related work Here we brieﬂy introduce some re-
lated works on text detection, recognition and end-to-
end wordspotting.

Scene text detection Recently, some methods cast
previous character based detection [21, 22, 23, 24] into
direct text region estimation [25, 8, 15, 26, 4, 27, 28],
avoiding multiple bottom-up post-processing steps by

taking word or text-line as a whole. Tian et al . [7]
modiﬁed Faster-RCNN [12] by applying a recurrent
structure on the convolution feature maps of the top
layer horizontally. The methods in [4, 25] were inspired
from [13]. They both explored the framework from
generic objects and convert to scene text detection by
adjusting the feature extraction process to this domain-
speciﬁc task. However, these methods are based on
prior boxes, which need to be carefully designed in or-
der to fulﬁll the requirements for training. Methods of
direct regression for inclined bounding boxes, instead
of oﬀsets to ﬁxed prior boxes, have been proposed re-
cently. EAST [8] designed a fully convolutional net-
work structure which outputs a pixel-wise prediction
map for text/non-text and ﬁve values for every point
of text region, i.e., distances from the current point to
the four edges with an inclined angle. He et al . [6]
proposed a method to generate arbitrary quadrilater-
als by calculating oﬀsets between every point of text
region and vertex coordinates.

Scene text recognition With the success of recurrent
neural networks on digit recognition and speech trans-
lation, a lot of works have been proposed for text recog-
nition. He et al . [16] and Shi et al . [9, 29] treat text
recognition as a sequence labeling problem by introduc-

Figure 2: The framework of our method. text-alignment layer is proposed to extract accurate sequence features within a
detected quadrilateral of multi-orientation. A novel character attention mechanism is applied to guide the decoding process
with explicit supervision. The whole framework can be trained in an end-to-end manner.

ing LSTM [30] and connectionist temporal classiﬁca-
tion (CTC) [31] into a uniﬁed framework. [17] proposed
an attention-based LSTM for text recognition, which
mainly contains two parts: encoder and decoder.
In
the encoding stage, text images are transformed into a
sequence of feature vectors by CNN/LSTM. Attention
weights, indicating relative importance for recognition,
will be learned during the decoding stage. However,
these weights are totally learned by the distribution of
data and no supervision is provided to guide the learn-
ing process.

End-to-end wordspotting End-to-end wordspotting
is an emerging research area. Previous methods usu-
ally try to solve it by splitting the whole process into
two independent problems: training two cascade mod-
els, one for detection and one for recognition. Detected
text regions are ﬁrstly cropped from original image, fol-
lowed by aﬃne transforming and rescaling. Corrected
images are repeatedly precessed by recognition model
to get corresponding transcripts. However, training er-
rors will be accumulated due to cascading models with-
out sharable features. Li et al . [5] proposed a uniﬁed
network that simultaneously localizes and recognizes
text in one forward pass by sharing convolution fea-
tures under a curriculum strategy. But the existing
RoI pooling operation limits it to detect and recog-
nize only horizontal examples. Busta et al . [2] brought
up deep text spotter, which can solve wordspotting of
multi-orientation problem. However, the method does
not have sharable feature, meaning that the recognition
loss of the later stage has no inﬂuence on the former
localization results.

2. Single Shot TextSpotter by Joint De-

tection and Recognition

In this section, we present the details of the proposed
textspotter which learns a direct mapping between an
input image and a set of word transcripts with cor-

responding bounding boxes of arbitrary orientations.
Our model is a fully convolutional architecture built
on the PVAnet framework [32]. As shown in Fig. 2,
we introduce a new recurrent branch for word recogni-
tion, which is integrated into our CNN model in paral-
lel with the existing detection branch for text bound-
ing box regression. The RNN branch is composed of a
new text-alignment layer and a LSTM-based recurrent
module with a novel character attention embedding
mechanism. The text-alignment layer extracts precise
sequence feature within the detected region, prevent-
ing encoding irrelevant texts or background informa-
tion. The character attention embedding mechanism
regulates the decoding process by providing more de-
tailed supervisions of characters. Our textspotter di-
rectly outputs ﬁnal results in one shot, without any
post-processing step except for a simple non-maximum
suppression (NMS).

Network architecture Our model is a fully convo-
lutional architecture inspired by [8], where a PVA net-
work [32] is utilized as backbone due to its signiﬁcantly
low computational cost. Unlike generic objects, texts
often have a much larger variations in both sizes and
aspect ratios. Thus it not only needs to preserve local
details for small-scale text instances, but also should
maintain a large receptive ﬁeld for very long instances.
Inspired by the success in semantic segmentation [33],
we exploit feature fusion by combining convolutional
features of conv5, conv4, conv3 and conv2 layers grad-
ually, with the goal of maintaining both local detailed
features and high-level context information. This re-
sults in more reliable predictions on multi-scale text
instances. Size of the top layer is 1
4 of the input image
for simplicity.

Text detection This branch is similar to that of
[8], where a multi-task prediction is implemented at
each spatial location on the top convolutional maps,
by adopting an Intersection over Union (IoU) loss de-

Figure 3: Standard RoI pooling (Top) and text-alignment layer (Bottom). Our method can avoid encoding irrelevant texts
and complicated background, which is crucial for the accuracy of text recognition.

scribed in [34].
It contains two sub-branches on the
top convolutional layer designed for joint text/non-text
classiﬁcation and multi-orientation bounding boxes re-
gression. The ﬁrst sub-branch returns a classiﬁcation
map with an equal spatial size of the top feature maps,
indicating the predicted text/non-text probabilities us-
ing a softmax function. The second sub-branch outputs
ﬁve localization maps with the same spatial size, which
estimate ﬁve parameters for each bounding box with
arbitrary orientation at each spatial location of text
regions. The ﬁve parameters represent the distances
of the current point to the top, bottom, left and right
sides of an associated bounding box, together with its
inclined orientation. With these conﬁgurations, the de-
tection branch is able to predict a quadrilateral of arbi-
trary orientation for each text instance. The feature of
the detected quadrilateral region is then feed into the
RNN branch for word recognition via a text-alignment
layer which is described below.

2.1. Text-Alignment Layer

We create a new recurrent branch for word recogni-
tion, where a text-alignment layer is proposed to pre-
cisely compute ﬁxed-size convolutional features from
a quadrilateral region of arbitrary size. The text-
alignment layer is extended from RoI pooling [35] which
is widely used for general objects detection. The RoI
pooling computes a ﬁxed-size convolutional features
(e.g., 7 × 7) from a rectangle region of arbitrary size,
by performing quantization operation.
It can be in-
tegrated into the convolutional layers for in-network
region cropping, which is a key component for end-to-
end training a detection framework. However, directly
applying the RoI pooling to a text region will lead to
a signiﬁcant performance drop in word recognition due
to the issue of misalignment.

– First, unlike object detection and classiﬁcation
where the RoI pooling computes global features
of a RoI region for discriminating an object, word

recognition requires more detailed and accurate lo-
cal features and spatial information for predict-
ing each character sequentially. As pointed out
in [19], the RoI pooling performs quantizations
which inevitably introduce misalignments between
the original RoI region and the extracted features.
Such misalignments have a signiﬁcant negative ef-
fect on predicting characters, particularly on some
small-scale ones such as ‘i’, ‘l’.

– Second, RoI pooling was designed for a rectangle
region which is only capable of localizing horizon-
tal instances.
It will make larger misalignments
when applied to multi-orientation text instances.
Furthermore, a large amount of background in-
formation and irrelevant texts are easily encoded
when a rectangle RoI region is applied to a highly
inclined text instance, as shown in Fig. 3. This
severely reduces the performance on RNN decod-
ing process for recognizing sequential characters.

Recent Mask R-CNN considers explicit per-pixel
spatial correspondence by introducing RoIAlign pool-
ing [19]. This inspires current work that develops
a new text-alignment layer tailored for text instance
which is a quadrilateral shape with arbitrary orien-
tation.
It provides strong word-level alignment with
accurate per-pixel correspondence, which is of critical
importance to extract exact text information from the
convolutional maps, as shown in Fig. 3.

Speciﬁcally, given a quadrilateral region, we ﬁrst
build a sampling grid with size of h × w on the top
convolutional maps. The sampled points are generated
with equidistant interval within the region, and the
feature vector (vp) for a sampled point (p) at spatial
location (px, py), is calculated via a bilinear sampling
[19] as follows,

vp =

4
(cid:88)

i=1

vpi ∗ g(px, pix) ∗ g(py, piy)

(1)

Figure 4: Our proposed sub-net structure for recognition branch, which provides attention guidance during the decoding
process by using character spatial information as supervision.

Where vpi refers to four surrounding points of point
p, g(m, n) is the bilinear interpolation function and
pix and piy refer to the coordinates of point pi. As
presented in [19], an appealing property of the bilinear
sampling is that gradients of the sampled points can be
back-propagated through the networks, by using Eq. 2.

∂grad
∂vpi

(cid:88)

=

g(px, pix) ∗ g(py, piy)

(2)

Grid sampling, by generating a ﬁxed number of sam-
pling points (e.g., w = 64, h = 8 in our experiments),
provides an eﬃcient way to compute ﬁxed-size features
from a quadrilateral region with arbitrary size and ori-
entation. The bilinear sampling allows for exacting
per-pixel alignment, successfully avoiding the quanti-
zation factor.

2.2. Word Recognition with Character Attention

Word recognition module is built on the text-
alignment layer, as shown in Fig. 2. Details of this
module is presented in Fig. 4, where the input is ﬁxed-
size convolutional features output from the text-align
pooling layer with size of w × h × C, where C is the
number of convolutional channels. The convolutional
features are fed into multiple inception modules and
generate a sequence of feature vectors, e.g., 64 ×C-
dimensional features, as shown in Fig. 4. In the next
part, we will brieﬂy introduce attention mechanism and
three strategies to enhance attention alignment.

2.2.1 Attention Mechanism

Recently, attention mechanism has been developed for
word recognition [17, 18], where an implicit attention is

1, he

2, ..., he

It outputs hidden states {he

learned automatically to enhance deep features in de-
coding process. In the encoding process, a bi-direction
LSTM layer is utilized to encode the sequential vec-
w} of the
tors.
same number, which encode strong sequential context
features from both past and future information. Unlike
previous work [9, 16] which decode a character (includ-
ing a non-character label) using each hidden state, the
attention mechanism introduces a new decoding pro-
cess where an attention weights (αt ∈ Rw) is learned
automatically at each decoding iteration, and the de-
coder predicts a character label (yt) by using this at-
tention vector,

yt = Decoder(hd

t , gt, yt−1)

(3)

where hd
time t, computed by:

t is the hidden state vector of the decoder at

t = f (yt−1, hd
hd

t−1, gt)

(4)

is the context vector, which is calculated as a
gt
weighted sum of the input sequence: gt = (cid:80)w
j=1 αt,jhe
j.
The decoder is ended until it encounters an end-of-
sequence (EOS). The attention vector is calculated
by αt,j = sof tmax(et,j), where et,j = z(hd
j) is
an alignment factor measuring matching similarity be-
tween the hidden state and encoding features he
j. How-
ever, these attention vectors are learned automatically
in the training process without an explicit guidance,
giving rise to misalignment problem which severely re-
duces recognition performance, as shown in Fig.5. To
address this problem, we propose new attention align-
ment and enhancement methods that explicitly encode
strong attention of each character.

t−1, he

Figure 5:
A comparison of
the proposed method with tra-
ditional attention LSTM. The
heat map indicates the focus-
ing location at each time step.

2.2.2 Attention Alignment and Enhancement

We introduce a new method which enhance the at-
tention of characters in word recognition. We develop
character-alignment mechanism that explicitly encodes
strong character information, together with a mask su-
pervision task which provides meaningful local details
and spatial information of character for model learn-
ing. Besides, an attention position embedding is also
presented. It identiﬁes the most signiﬁcant spot from
the input sequence which further enhances the corre-
sponding text features in inference. These technical
improvements are integrated seamlessly into a uniﬁed
framework that is end-to-end trainable. Details of each
module are described as follows.

Attention alignment To deal with misalignment
issue raised by existing implicit attention models, we
propose an attention alignment which explicitly en-
codes spatial information of characters, by introducing
an additional loss as supervision.

Speciﬁcally, assuming that pt,1, pt,2, ..., pt,w are cen-
tral points in each column of the sampling grid. At
t-th time step, these central points can be calculated
by Eq. 5,

δt =

w
(cid:88)

j=1

αt,j × pt,j

(5)

Ideally, δt should close to the center of current char-
acter, yt. Without supervision, it is likely to result in
misalignment and therefore incorrect sequence labels.
Intuitively, we can construct a loss function to describe
whether the attention points is focusing on the right lo-
cation.

the distance between the prediction and GT should
be normalized by character width, which we found is
useful for model convergence.

Character mask To further enhance character at-
tention, we introduce another additional supervision by
leveraging character mask, which provides more mean-
ingful information, including both local details and spa-
tial location of a character. A set of binary masks are
generated, with a same spatial size of the last convo-
lutional maps. The number of the masks is equal the
number of character labels. A softmax loss function
is applied at each spatial location, which is referred
as mask loss (cid:96)mask. This explicitly encoding strong
detailed information of characters into the attention
module. Both (cid:96)mask and (cid:96)align losses are optional dur-
ing the training process, and can be ignored on those
images where character level annotations are not pro-
vided.

Position embedding Position embedding was ﬁrst
introduced in [36], aiming to make the model ‘location
aware’ by encoding a one-hot coordinate vector. This
is equivalent to adding a varying bias terms. It is dif-
ﬁcult to directly apply it to our task, as the size of
the feature maps changes according to the size of in-
put image. Instead, we generate a one-hot vector from
the attention vector, uk = arg minj αt,j, which is a
ﬁxed-size binary vector (e.g., 64-D). Then, we directly
concatenate the one-shot vector with the context vec-
tor (gt), which forms a new feature representation with
additional one-hot attention information. Then the de-
coder computed in Eq. 3 can be modiﬁed as,

yt = Decoder(hd

t , gt, yt−1, ut)

(7)

(cid:96)align =

T
(cid:88)

t=0

(cid:13)
(cid:13)
(cid:13)
(cid:13)

δt − kt
0.5 ∗ ¯wt

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)

(6)

Finally, by integrating all these modules into a single
model, we obtains an overall loss function including
four components,

where kt is ground truth (GT) coordinates, and ¯wt is
the GT width of current character, yt. Both of them
are projected onto the axis of text orientation. T is
the number of characters in a sequence. Notice that

L = (cid:96)loc + (cid:96)word + λ1(cid:96)align + λ2(cid:96)mask

(8)

where (cid:96)word is a softmax loss for word recognition, (cid:96)loc
is the loss function for text instance detection, and λ1

and λ2 are corresponding loss weights (both are set to
0.1 in our experiment).

2.3. Training Strategy

Training our model in an end-to-end manner is chal-
lenging due to a number of diﬃculties. First, largely
diﬀerent nature of them, e.g., signiﬁcant diﬀerences
in learning diﬃculties and convergence rates. Second,
the extremely unbalanced distribution of image data.
Our methods require character-level bounding boxes
for generating character coordinates and masks. These
detailed character annotations are not provided in the
standard benchmarks, such as the ICDAR2013 [37] and
ICDAR2015 [1]. Although Gupta et al . [20] developed
a fast and scalable engine to generate synthetic images
of text, providing both word-level and character-level
informations, there is still a large gap between realis-
tic and synthesized images, making the trained model
diﬃcult to generalize well to real-world images.

We ﬁll this gap by developing a principled training
strategy which includes multiple steps.
It is able to
train multiple tasks collaboratively in our single model,
allowing for excellent generalization capability from the
synthesized images to real-world data.

Step One: We randomly select 600k images from
the 800k synthetic images. Word recognition task is
ﬁrstly trained by ﬁxing the detection branch. We pro-
vide the ground truth (GT) bounding boxes of word
instances to the text-align layer. Three losses: (cid:96)word,
(cid:96)align and (cid:96)mask are computed. The training process
last 120k iterations with a learning rate 2 × 10−3.

Step Two: For the next 80k iterations, we open
the detection branch, but still use the GT bounding
boxes for the text-align layer, as the detector performs
poorly at ﬁrst, which will be harmful to the already
trained recognition branch. The learning rate is set to
2×10−4. During the next 20k iterations, sampling grid
are generated from the detection branch. The model is
trained end-to-end in this stage.

Step Three: About 3,000 real-world images from
the ICDAR 2013 [37], ICDAR 2015 [1] and Multi-
lingual1 datasets are utilized in the next 60k iterations.
To enhance generalization ability, data augmentation
is employed. We re-scale the images by keeping aspect
ratio unchanged, followed by random rotation rang-
ing from −20◦ to 20◦, and random cropping 800×800
patches for training. To utilize the character-level su-
pervision, we set the batch size to 4, where an image
from synthetic dataset is included. The learning rate
remained at 2 × 10−4. The whole system is imple-
mented by Caﬀe [38], with TITAN X GPUs.

3. Experiments

In this section, we ﬁrst brieﬂy introduce the datasets
we use and the evaluation protocols, followed by thor-
ough comparison of the proposed method with the
state-of-the-art along with comprehensive ablation ex-
periments.

Datasets The ICDAR2013 dataset focuses more
on horizontal text instances, which contains 229 images
for training and 233 images for testing with word-level
annotation.

The ICDAR2015 dataset is collected by Google
glasses, which has 1,000 images for training and 500
images for testing. Diﬀerent from previous datasets
which are well-captured horizontal English text, it con-
tains texts with more scales, blurring, and orientation.
Multi-lingual scene text dataset2 is built for devel-
oping script-robust text detection methods, which con-
tains about 9,000 images with 9 diﬀerent kinds of tran-
scriptions. We choose about 2000 of them, identiﬁed
with ‘Latin’, to train the end-to-end task.

3.1. Evaluation Protocols

Detection There are two standard protocols for
evaluating detection results: DetEval and ICDAR2013
standard [37]. The main diﬀerence between the two
protocols is that the latter one stress more on individ-
ual words while the former can achieve high score even
when many words are connected into a line.

End-to-end for detection and recognition The
criterion has been used in competition: the evaluation
of the results will be based on a single IoU criterion,
with a threshold of 50%, and correct transcription. Be-
sides, three dictionaries are also provided for testing
reference, i.e., ‘strong’, ‘weak’ and ‘generic’.
‘Strong’
lexicon has 100 entries for every image, and most words
appeared in that image are included.
‘Weak’ lexicon
contains all the words that appeared in the testing
dataset.
’Generic’ lexicon has 90K words. One thing
should be noticed that the length of all the words in
dictionaries are greater than 3 with symbols and num-
bers excluded. There are two protocols for evaluation:
end-to-end and word-spotting. End-to-end needs to
recognize all the words precisely, no matter whether the
dictionary contains these strings. On the other hand,
word-spotting only examine whether the words in the
dictionary appear in images, making it less strict than
end-to-end for ignoring symbols, numbers and words
whose length is less than 3.

1http://rrc.cvc.uab.es/?ch=8&com=introduction

2http://rrc.cvc.uab.es/?ch=8&com=introduction

Figure 6: A comparison of detection performance between joint training (Top) and separate training (Bottom). Joint
training makes it more robust to ﬁnd out text regions as two tasks are highly correlated, where detection can beneﬁt from
training of recognition.

Table 1: Comparisons of the end-to-end task with state-of-the-art on ICDAR2013 and ICDAR2015. The results
are reported with three diﬀerent level lexicons, namely, strong, weak and generic.

Method

Deep2Text II+ [39]
Jaderberg et al . [40]
FCRNall+multi-ﬁlt [20]
TextBoxes [25]
YunosRobot1.0
Li et al . [5]
Deep text spotter [2]
Proposed Method

Method

Stradvision [1]
TextSpotter [41]
Deep TextSpotter [2]
Proposed Method

Year

2014
2015
2016
2017
2017
2017
2017
-

Year

2013
2016
2017
-

3
1
0
2
R
A
D
C

I

5
1
0
2
R
A
D
C

I

Strong
0.85
0.90
−
0.94
0.87
0.94
0.92
0.93

Strong
0.46
0.37
0.58
0.85

Word-Spotting
Weak
0.83
−
−
0.92
−
0.92
0.89
0.92
Word-Spotting
Weak
−
0.21
0.53
0.80

Generic
0.79
0.76
0.85
0.86
0.87
0.88
0.81
0.87

Generic
−
0.16
0.51
0.65

Strong
0.82
0.86
−
0.92
0.84
0.91
0.89
0.91

Strong
0.44
0.35
0.54
0.82

End-to-end
Weak
0.79
−
−
0.90
−
0.90
0.86
0.89
End-to-end
Weak
−
0.20
0.51
0.77

Generic
0.77
−
−
0.84
0.84
0.85
0.77
0.86

Generic
−
0.16
0.47
0.63

3.2. Text-alignment vs. RoI Pooling

We ﬁrst compare the proposed text-alignment with
standard RoI pooling. To make fair comparison, the
detection part is ﬁxed with ground truth and recogni-
tion performance is evaluated on ICDAR2015, which
contains text instances of multi-orientation. Due to
encoding background information and irrelevant text
instances, RoI pooling results in mis-alignment and in-
accurate representation of feature sequences. As shown
in Tab. 2, the accuracy of recognition with proposed
method surpasses standard RoI pooling by a large mar-

gin, boosting from 60.7% to 67.6%. All results are eval-
uated without referring to any lexicon in single scale.

3.3. Character Attention

Diﬀerent from traditional attention-based recogni-
tion models, where attention weights are automatically
learned, we propose a method to regulate the learn-
ing process to prevent mis-alignment in the decoding
stage. To demonstrate the eﬀectiveness of our proposed
method, we conduct two experiments with the detec-
tion part ﬁxed. The ﬁrst one is on VGG synthetic data
[20], where we select 600K for training and 200K for

Table 2: Ablations for the proposed method. We test our model on ICDAR2015. The detection part is replaced
with ground truth for fair comparison.

roi pooling?

(cid:88)
×
×
×
×
×

roi
alignment?
×
(cid:88)
×
×
×
×

text
alignment?
×
×
(cid:88)
(cid:88)
(cid:88)
(cid:88)

supervision?

×
×
×
(cid:88)
×
(cid:88)

position
embedding?
×
×
×
×
(cid:88)
(cid:88)

Accuracy (%)

60.7
61.9
67.6
68.8
68.2
69.5

Table 3: Comparison of detection results with the state-of-the-art methods on ICDAR2013 and ICDAR2015. The
results are reported Recall (R), Precision (P) and F-measure (F). For fair comparison, the detection performance
is achieved without referring to recognition results.

ICDAR2013 dataset

ICDAR2015 dataset

Method

TextFlow [42]
Text-CNN [23]
FCRN [20]
CTPN [7]
He et al. [4]
He et al. [6]

Proposed wo recog
Proposed

Year

2015
2016
2016
2016
2017
2017

-
-

ICDAR standard
F
P
R
0.80
0.85
0.76
0.82
0.73
0.93
0.84
0.94
0.76
0.82
0.93
0.73
0.87
0.88
0.86
0.86
0.92
0.81

DetEval
P
-
0.93
0.92
0.93
0.89
-

R
-
0.76
0.76
0.83
0.86
-

F
-

StradVision2
0.84 MCLAB FCN [15]
EAST [8]
0.83
CTPN [7]
0.88
He et al. [4]
0.88
He et al. [6]
-

0.87
0.88

0.88
0.91

0.88
0.90

0.87
0.89

0.88
0.91

Proposed wo recog

0.88
0.90 Proposed

2015
2016
2016
2016
2017
2017

-
-

0.37
0.43
0.78
0.52
0.73
0.82

0.77
0.71
0.83
0.74
0.80
0.80

0.50
0.54
0.81
0.61
0.77
0.81

0.83
0.86

0.84
0.87

0.83
0.87

Method

Year

R

P

F

testing. The accuracies of character-level and word-
level are evaluated. The method with supervision has
accuracy of 0.95 and 0.88 on two protocols, comparing
to 0.93 and 0.85 on traditional attention-based method.
The other experiment is tested on ICDAR2015 dataset.
As is shown in Fig. 5, the proposed method give more
accurate character localization than attentional LSTM,
leading to about 2% boosting in accuracy.

3.4. Joint Training vs. Separate Models

We believe that text detection and recognition are
not two standalone problems, but highly correlated
where each task can beneﬁt from the training of the
other. Joint training of two tasks in a uniﬁed frame-
work avoids error accumulations among cascade mod-
els. As shown in Tab.
3, the task of recognition
greatly enhances the performance of detection in terms
of recall and precision, leading to a 3% improvement
on F-Measure (noting:
the detection performances
are achieved without referring to recognition results).
As can be seen from Fig.
6, joint training makes
it more robust to text-like background and compli-
cated text instances. We also provide a comparison
with other detection approaches, indicating that our
method achieved new state-of-the-art performance on
ICDAR2013 and ICDAR2015 datasets.

3.5. Proposed Method vs. State-of-the-art Methods

End-to-end results on some extremely challenging
images are presented in Fig. 7. As can be seen in
Fig. 7, our method can correctly detect and recognize
both small text instances and those with large inclined
angles.

eﬀectiveness

ICDAR2015 The

to multi-
orientation texts is testiﬁed on ICDAR2015 dataset.
Our method achieved an F-measure of 0.82, 0.77
and 0.63 respectively in terms of referencing ‘Strong’,
‘Weak’ and ‘Generic’ lexicon under the end-to-end pro-
tocol, which surpasses the state-of-the-art performance
of 0.54, 0.77 and 0.63 by a large margin.

ICDAR2013 The dataset is well-captured for hor-
izontal text instances. The result is shown in Tab. 1,
which is comparable to the state-of-the-art result [5].

Conclusion In this paper we have presented a
novel framework that combines detection and recog-
nition in a uniﬁed network with sharable features. The
model can directly output detection and recognition
results of multi-orientation text instances.

We have proposed a novel text-alignment layer that
can extract precise sequence information without en-
coding irrelevant background or texts. We also im-
prove the accuracy of traditional LSTM by enhancing
the attention of characters during the decoding process.

Figure 7: Examples of textspotting results of the proposed method on ICDAR2013 and ICDAR2015.

Our proposed method achieves state-of-the-art perfor-
mance on two open benchmarks: ICDAR2013 and IC-
DAR2015 and outperforms previous best methods by
a large margin.

Acknowledgments C. Shen’s participation was in

part supported by an ARC Future Fellowship.

References

[1] D. Karatzas, L. Gomez-Bigorda, A. Nicolaou,
S. Ghosh, A. Bagdanov, M. Iwamura, J. Matas,
L. Neumann, V. R. Chandrasekhar, S. Lu, F. Shafait,
S. Uchida, and E. Valveny, “ICDAR 2015 competi-
tion on robust reading,” in Proc. Int. Conf. Document
Analysis & Recognition, 2015.

[2] M. Busta, L. Neumann, and J. Matas, “Deep textspot-
ter: An end-to-end trainable scene text localization
and recognition framework,” in Proc. IEEE Int. Conf.

Comp. Vis., 2017.

[3] M. Jaderberg, K. Simonyan, A. Zisserman, and
K. Kavukcuoglu, “Spatial transformer networks,” in
Proc. Advances in Neural Inf. Process. Syst., 2015.

[4] P. He, W. Huang, T. He, Q. Zhu, Y. Qiao, and X. Li,
“Single shot text detector with regional attention,” in
Proc. IEEE Int. Conf. Comp. Vis., 2017.

[5] H. Li, P. Wang, and C. Shen, “Towards end-to-end
text spotting with convolutional recurrent neural net-
works,” in Proc. IEEE Int. Conf. Comp. Vis., 2017.

[6] W. He, X. Zhang, F. Yin, and C. Liu, “Deep direct re-
gression for multi-oriented scene text detection,” arXiv
preprint arXiv:1703.08289, 2017.

[7] Z. Tian, W. Huang, T. He, P. He, and Y. Qiao, “De-
tecting text in natural image with connectionist text
proposal network,” in Proc. Eur. Conf. Comp. Vis.,
2016.

[8] X. Zhou, C. Yao, H. Wen, Y. Wang, S. Zhou, W. He,
and J. Liang, “EAST: An eﬃcient and accurate scene
text detector,” in Proc. IEEE Conf. Comp. Vis. Patt.
Recogn., 2017.

[9] B. Shi, X. Bai, and C. Yao, “An end-to-end trainable
neural network for image-based sequence recognition
and its application to scene text recognition,” IEEE
Trans. Pattern Anal. Mach. Intell., vol. 39, pp. 2298–
2304, 2017.

[10] S. Tian, S. Lu, and C. Li, “Wetext: Scene text de-
tection under weak supervision,” in Proc. IEEE Int.
Conf. Comp. Vis., 2017.

[11] X. Liu, D. Liang, S. Yan, D. Chen, Y. Qiao, and J. Yan,
“Fots: Fast oriented text spotting with a uniﬁed net-
work,” arXiv preprint arXiv:1801.01671, 2018.

[12] S. Ren, K. He, R. Girshick, and J. Sun, “Faster R-
CNN: Towards real-time object detection with region,”
in Proc. Advances in Neural Inf. Process. Syst., 2015.

[13] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed,
C. Fu, and A. C. Berg, “SSD: Single shot multibox
detector,” in Proc. Eur. Conf. Comp. Vis., 2016.

[14] J. Long, E. Shelhamer, and T. Darrell, “Fully convo-
lutional networks for semantic segmentation,” in Proc.
IEEE Conf. Comp. Vis. Patt. Recogn., 2015.

[15] Z. Zhang, C. Zhang, W. Shen, C. Yao, W. Liu, and
X. Bai, “Multi-oriented text detection with fully con-
volutional networks,” in Proc. IEEE Conf. Comp. Vis.
Patt. Recogn., 2016.

[16] P. He, W. Huang, Y. Qiao, C. C. Loy, and X. Tang,
“Reading scene text in deep convolutional sequences,”
in Proc. AAAI Conf. Artiﬁcial Intell., 2016.

[17] C. Lee and S. Osindero, “Recursive recurrent nets with
attention modeling for OCR in the wild,” in Proc.
IEEE Conf. Comp. Vis. Patt. Recogn., 2016.

[18] D. Bahdanau, K. Cho, and Y. Bengio, “Neural ma-
chine translation by jointly learning to align and trans-
late,” arXiv preprint arXiv:1409.0473, 2016.

[19] K. He, G. Gkioxari, P. Dollar, and R. Grishick, “Mask
R-CNN,” in Proc. IEEE Int. Conf. Comp. Vis., 2017.

[20] A. Gupta, A. Vedaldi, and A. Zisserman, “Synthetic
data for text localisation in natural images,” in Proc.
IEEE Conf. Comp. Vis. Patt. Recogn., 2016.

[21] W. Huang, Z. Lin, J. Yang, and J. Wang, “Text local-
ization in natural images using stroke feature trans-
form and text covariance descriptors,” in Proc. IEEE
Int. Conf. Comp. Vis., 2013.

[22] W. Huang, Y. Qiao, and X. Tang, “Robust scene text
detection with convolutional neural networks induced
MSER trees,” in Proc. Eur. Conf. Comp. Vis., 2014.

[24] C. Yao, X. Bai, W. Liu, Y. Ma, and Z. Tu, “Detecting
texts of arbitrary orientations in natural images,” in
Proc. IEEE Conf. Comp. Vis. Patt. Recogn., 2012.

[25] M. Liao, B. Shi, X. Bai, X. Wang, and W. Liu,
“Textboxes: A fast text detector with a single deep
neural network,” in Proc. AAAI Conf. Artiﬁcial In-
tell., 2017.

[26] T. He, W. Huang, Y. Qiao, and J. Yao, “Accurate text
localization in natural image with cascaded convolu-
tional text network,” arXiv preprint arXiv:1603.09423,
2016.

[27] Z. Zhang, W. Shen, C. Yao, and X. Bai, “Symmetry-
based text line detection in natural scenes,” in Proc.
IEEE Conf. Comp. Vis. Patt. Recogn., 2015.

[28] C. Yao, X. Bai, N. Sang, X. Zhou, S. Zhou, and Z. Cao,
“Scene text detection via holistic, multi-channel pre-
diction,” arXiv preprint arXiv:1606.09002, 2016.

[29] B. Shi, X. Wang, P. Lyu, C. Yao, and X. Bai., “Robust
scene text recognition with automatic rectiﬁcation,” in
Proc. IEEE Conf. Comp. Vis. Patt. Recogn., 2016.

[30] A. Graves and N. Jaitly, “Towards end-to-end speech
recognition with recurrent neural networks,” in Proc.
Int. Conf. Mach. Learn., 2014.

[31] A. Graves, M. Liwicki, S. Fernandez, R. Bertolami,
H. Bunke, and J. Schmidhuber, “A novel connection-
ist system for unconstrained handwriting recognition,”
IEEE Trans. Pattern Anal. Mach. Intell., vol. 31,
pp. 855–868, 2009.

[32] S. Hong, B. Roh, K. Kim, Y. Cheon,

and
M. Park, “PVANet: Lightweight deep neural net-
works for real-time object detection,” arXiv preprint
arXiv:1611.08588, 2016.

[33] O. Ronneberger, P. Fischer, and T. Brox, “U-Net:
Convolutional networks for biomedical image segmen-
tation,” in Proc. Int. Conf. Medical Image Computing
& Computer-Assisted Intervention, 2015.

[34] J. Yu, Y. Jiang, Z. Wang, Z. Cao, and T. Huang,
“UnitBox: An advanced object detection network,” in
ACM Conf. Multimedia, 2016.

[35] R. Grishick, “Fast R-CNN,” in Proc. IEEE Int. Conf.

Comp. Vis., 2015.

[36] Z. Wojna, A. Gorban, D. Lee, K. Murphy, Q. Yu, Y. Li,
and J. Ibarz, “Attention-based extraction of structured
information from street view imagery,” arXiv preprint
arXiv:1704.03549, 2017.

[37] D. Karatzas, F. Shafait, S. Uchida, M. Iwamura,
L. Gomez, S. Robles, J. Mas, D. Fernandez, J. Al-
mazan, and L. de las Heras, “ICDAR 2013 robust
reading competition,” in Proc. Int. Conf. Document
Analysis and Recognition, 2013.

[23] T. He, W. Huang, Y. Qiao, and J. Yao, “Text-
attentional convolutional neural networks for scene
text detection,” IEEE Trans. Image Process., vol. 25,
pp. 2529–2541, 2016.

[38] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long,
R. Girshick, S. Guadarrama, and T. Darrell, “Caﬀe:
Convolutional architecture for fast feature embed-
ding,” in ACM Conf. Multimedia, 2014.

[39] X. Yin, X. Yin, K. Huang, and H. Hao, “Robust text
detection in natural scene images,” IEEE Trans. Pat-
tern Anal. Mach. Intell., vol. 36, pp. 970–983, 2014.

[40] M. Jaderberg, K. Simonyan, A. Vedaldi, and A. Zis-
serman, “Reading text in the wild with convolutional
neural networks,” Int. J. Comput. Vision, vol. 116,
pp. 1–20, 2016.

[41] L. Neumann and J. Matas, “Real-time lexicon-free
scene text localization and recognition,” IEEE Trans.
Pattern Anal. Mach. Intell., vol. 38, pp. 1872–1885,
2016.

[42] S. Tian, Y. Pan, C. Huang, S. Lu, K. Yu, and C. L.
Tan, “Text ﬂow: A uniﬁed text detection system in
natural scene images,” in Proc. IEEE Int. Conf. Comp.
Vis., 2015.

