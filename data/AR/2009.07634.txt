1
2
0
2

r
a

M
9

]
E
M

.
t
a
t
s
[

2
v
4
3
6
7
0
.
9
0
0
2
:
v
i
X
r
a

Time-varying auto-regressive models for count
time-series

Arkaprava Roy, Sayar Karmakar
University of Florida

March 10, 2021

Abstract

Count-valued time series data are routinely collected in many application areas. We are
particularly motivated to study the count time series of daily new cases, arising from COVID-
19 spread. First, we propose a Bayesian framework to study time-varying semiparametric
AR(p) model for count and then extend it to propose a time-varying INGARCH model
considering the rapid changes in the spread. We calculate posterior contraction rates of
the proposed Bayesian methods with respect to average Hellinger metric. Our proposed
structures of the models are amenable to Hamiltonian Monte Carlo (HMC) sampling for
eﬃcient computation. We substantiate our methods by simulations that show superiority
compared to some of the close existing methods. Finally we analyze the daily time series
data of newly conﬁrmed cases to study its spread through diﬀerent government interventions.

Keywords: Autoregressive model, B-splines, COVID-19, Count-valued time series, Hamiltonian
Monte Carlo (HMC), INGARCH, Posterior Contraction Rates, Non-stationary, Poisson Regression

1

Introduction

Modeling count time series is important in many applications such as disease incidence, accident

rates, integer ﬁnancial datasets such as price movement, etc. This relatively new research stream

was introduced in Zeger (1988) and interestingly he analyzed another outbreak namely the US

1970 Polio incidence rate. This stream was furthered by Chan and Ledolter (1995) where Poisson

generalized linear models (GLM) with an autoregressive latent process in the mean are discussed.

A wide range of dependence was explored in Davis et al. (2003) for simple autoregressive (AR)

structure and external covariates. On the other hand, a diﬀerent stream explored integer-valued

time series counts such as ARMA structures as in (Brandt and Williams, 2001; Biswas and Song,

1

 
 
 
 
 
 
2009) or INGARCH structure as done in Zhu (2011, 2012c,a,b). However, from a Bayesian per-

spective, the only work to the best of our knowledge is that of Silveira de Andrade et al. (2015)

where the authors discussed an ARMA model for diﬀerent count series parameters. However,

their treatment of ignoring zero-valued data or putting the MA structure by demeaned Poisson

random variable remains questionable. None of these works focused on the time-varying nature

of the coeﬃcients except for a brief mention in Karmakar et al. (2020+).

Our goals are motivated by both the application and methodological development. To the

best of our knowledge, ours is the ﬁrst attempt to model possibly autoregressive count time series

with time-varying coeﬃcients which can be regarded as the time-varying analog of Fokianos et al.

(2009). We consider a linear link based GLM route instead of the traditional exponential link

(Fokianos and Tjøstheim, 2011), since linear link helps in better interpretability of the coeﬃcient

functions. Linear link however requires more stringent shape restrictions on the functions. We

impose those by putting constraints on the B-spline coeﬃcients while modeling those coeﬃcient

functions. However, it is possible to extend all the computations of the current paper to an

exponential link based GLM framework. The mean function stands for the overall spread and the

autoregressive coeﬃcients stand for the eﬀect of diﬀerent lags. We are particularly motivated to

study the spread of COVID-19 in New York City (NYC) from 23rd January to 14th July using

the daily count data of new cases. In terms of our motivating data application, we wish to identify

which lags are signiﬁcant in our model which can be directly linked to the period of time symptoms

did not show up. We ﬁnd that some higher-order lags like 6, 7, and 8 are also signiﬁcant. These

ﬁndings are in-line with several research articles discussing the incubation length for the novel

coronavirus with a median of 6-7 days and 98% below 11 days. For example, see Lauer et al.

(2020b). We also ﬁnd that after the lockdown or stay-at-home orders it takes about 12-16 days

to reach the peak and then the intercept coeﬃcient function starts decreasing. This is also an

interesting ﬁnd which characterizes the fact that the number of infected but asymptomatic cases

is large compared to the new cases reported. Additional to the time-varying AR model proposal,

we also oﬀer an analysis via a time-varying Bayesian integer-valued generalized autoregressive

conditional heteroscedasticity (TVBINGARCH) model that assumes an additional recursive term

in the conditional expectation (cf. (2.1)). This extension oﬀers some more comprehensiveness

in the modeling part as even BINGARCH with small orders can help us get rid of choosing an

2

appropriate maximum lag value. Since for a Poisson model, the mean is the same as the variance,

this can also be thought of as an extension of the GARCH model in the context of count data.

First introduced by Ferland et al. (2006), these models were thoroughly analyzed in Zhu (2012c,a,

2011, 2012b); Ahmad and Francq (2016). Our proposal for the time-varying TVBINGARCH model

adapts to the non-stationarity theme and also can be viewed as a new contribution. Finally, we

contrast the time-varying AR and the GARCH for both simulations and real-data applications

under diﬀerent metrics of evaluation. Our semiparametric time-varying model provides better

estimates.

Regression models with varying coeﬃcient were introduced by Hastie and Tibshirani (1993).

They modeled the varying coeﬃcients using cubic B-splines. Later, these models has been further

explored in various directions Gu and Wahba (1993); Biller and Fahrmeir (2001); Fan and Zhang

(2008); Franco-Villoria et al. (2019); Yue et al. (2014). Spline bases have been routinely used to

model the time-varying coeﬃcients within non-linear time series models (Cai et al., 2000; Huang

et al., 2002; Huang and Shen, 2004; Amorim et al., 2008). We also consider the B-spline series

based priors to model the time-varying coeﬃcient functions. We develop eﬃcient computational

algorithms for the proposed models.

Apart from developing a computationally tractable hierarchical model, we also establish poste-

rior contraction rates of the proposed models. Ghosal et al. (2007) established posterior contrac-

tion for a general stationary Markov chain with much stricter conditions. However, they relaxed

some of those conditions in Theorem 8.29 of Ghosal and Van der Vaart (2017). To the best of

our knowledge, the posterior contraction rate result of this paper is the ﬁrst for the time-varying

Markov model based on minimal assumptions under Poisson-link. We also consider the strategy,

used to relax the conditions in Theorem 8.29 of Ghosal et al. (2007). Our posterior contraction

rate is with respect to the average Hellinger metric. The primary theoretical hurdle is to construct

exponentially consistent tests in a time-varying Markov setup. Our proposed test construction is

inspired by Jeong et al. (2019); Ning et al. (2020). We construct the test relying on the Neyman-

Pearson lemma with respect to negative average log aﬃnity distance and calculate contraction

rates. Then we show that the same rate holds for the average Hellinger metric as well. We also

discuss a pointwise inferential tool by drawing credible intervals. Such tools are important to

keep an objective perspective in terms of the evolution of the time-varying coeﬃcients without

3

restricting it to some speciﬁc trend models. See Karmakar et al. (2020+) (Karmakar (2018) for

an earlier version) for a comprehensive discussion on time-varying models and their applications.

The rest of the paper is organized as follows. Section 2 describes the proposed Bayesian

models in detail. Section 3 discusses an eﬃcient computational scheme for the proposed method.

We calculate posterior contraction rates in Section 4. The performance of our proposed method in

capturing true coeﬃcient functions are studied in Section 5 and we show excellent performance over

other existing methods. Section 6 deals with an application of the proposed method on COVID-

19 spread for NYC. Then, we end with discussions and possible future directions in Section 7.

Section 8 contains detail theoretical proofs.

2 Modeling

Given the rapidly evolving nature of the pandemic, the patterns and number of new aﬀected

cases were changing rapidly over diﬀerent geographical regions. The rapid change in the observed

counts make all earlier time-constant analysis inappropriate and builds a path where we can explore

methodological and inferential development in tracking down the trajectory of this spread. Thus,

we propose two novel semiparametric time-varying autoregressive models for counts to study the

spread and examine the eﬀects of these interventions in the spread based on the time-varying

coeﬃcient functions. We ﬁrst consider the most general case where we model the data using

a time-varying Bayesian integer-valued generalized autoregressive conditional heteroscedasticity

(TVBINGARCH) model where the conditional mean depends on the past observations as well

as past conditional means. However, the relatively simpler process consisting of a time-varying

mean/intercept function along with time-varying autoregressive coeﬃcient functions upto lag-p

is also important keeping in mind the scope of application to real data and its interpretation.

For example, the particular lags in an AR(p) model for the COVID-19 count data can crave an

interesting phenomenon in the lag-dynamics of the spread. This might be lost if we model the

same using a TVBINGARCH(1,1) model since typically for GARCH type models it is standard

practice to only consider smaller orders.

4

2.1 Time-varying generalized autoregressive conditional heteroscedas-

ticity model for counts

Ferland et al. (2006) proposed integer valued analogue of generalized autoregressive conditional

heteroscedasticity model (GARCH) after observing that the variability in number of cases of

campylobacterosis infections also changes with level. We consider here a time-varying analog of

such process. The conditional distribution for count-valued time-series Xt given Ft−1 = {Xi : i ≤

(t − 1)} and Gt−1 = {λi : i ≤ (t − 1)} is,

Xt|Ft−1, Gt−1 ∼Poisson(λt) where λt = µ(t/T ) +

p
(cid:88)

i=1

ai(t/T )Xt−i +

q
(cid:88)

j=1

bj(t/T )λt−j.

(2.1)

We call our method time-varying Bayesian Integer valued Generalized Auto Regressive Conditional

Heteroscedastic (TVBINGARCH) model. We impose following constraints on the parameter space

similar to Ferreira et al. (2017),

P1 = {µ, ai : 0 < µ(x) < ∞, sup
x

(cid:88)

i,j

(ai(x) + bj(x)) < 1}.

(2.2)

This constraint ensure a unique solution of the time-varying GARCH process as discussed in Davis

and Mikosch (2009); Rohan and Ramanathan (2013); Ferreira et al. (2017). Now, we put priors

on the functions µ(·), ai(·) and bj(·) such that they are supported in P1. Using the B-spline bases,

we put following hierarchical prior on the unknown functions,

µ(x) =

ai(x) =

bk(x) =

K1(cid:88)

j=1

K2(cid:88)

j=1

K3(cid:88)

j=1

αjBj(x)

θijMiBj(x),

0 ≤ θij ≤ 1, 1 ≤ i ≤ p,

ηkjMk+pBj(x),

0 ≤ ηkj ≤ 1, 1 ≤ k ≤ q,

Mi =

τi
k=0 τk

(cid:80)p

,

i = 1, . . . , p + q,

θij ∼U (0, 1) for 1 ≤ i ≤ p, 1 ≤ j ≤ K2,

ηkj ∼U (0, 1) for 1 ≤ k ≤ q, 1 ≤ j ≤ K3,

λ0 ∼Inverse-Gamma(d1, d1),

5

(2.3)

(2.4)

(2.5)

(2.6)

(2.7)

(2.8)

(2.9)

where λ0 is the rate parameter for X0. The speciﬁcation for the density of X0 is required for

computation. Otherwise we need to assume λ0 to be known which is not reasonable for a real data

application. We primarily focus on the special case where p = 1, q = 1. Based on the constraints

on the parameter space we consider following prior for αj’s and τi’s,

αj ∼ TN(0, c2

1, 0, ∞),

τi ∼ U (0, 1),

(2.10)

where TN stands for the truncated normal with mean 0, variance c2
1 and truncated to [0, ∞). In
j=0 Mj = 1. Thus (cid:80)p+q
above construction, (cid:80)P
j=1 Mj < 1 if M0 > 0. As Π(M0 > 0) = 1, we have
Π((cid:80)p+q
j=1 Mj < 1) = 1. Since 0 ≤ θij ≤ 1, we have supx ai(x) ≤ Mi, and supx bj(x) ≤ Mp+j. Thus
(cid:80)p
i=1 Mi < 1. We have (cid:80)p+q
j=1 Mj = 1 if and only if τ0 = 0, which
supx
has zero prior probability. On the other hand, we also have µ(·) ≥ 0 as we have αj ≥ 0. Thus,

j=1 bj(x) ≤ (cid:80)p+q

i=1 ai(x) + (cid:80)q

the induced priors, described in (2.3)− (2.9) are well supported in P1.

2.2 Time-varying auto-regressive model for counts

Although our previous modeling framework is more general, one may only wish to study higher

order lag dependence from the past observations. Thus we consider a simpliﬁed model in this

subsection. The linear Poisson autoregressive model (Zeger, 1988; Brandt and Williams, 2001)

is popular in analyzing higher order lag-dependence in count valued time series. Due to the

assumed non-stationary nature of the data, we propose a time-varying version of this model. The

conditional distribution for count-valued time-series Xt given Ft−1 = {Xi : i ≤ (t − 1)} is,

Xt|Ft−1 ∼Poisson(λt) where λt = µ(t/T ) +

p
(cid:88)

i=1

ai(t/T )Xt−i.

(2.11)

We call our method time-varying Bayesian Auto Regressive model for Counts (TVBARC). The

rescaling of the time-varying parameters to the support [0,1] is usual for in-ﬁlled asymptotics.

Due to the Poisson link in (2.11), both conditional mean and conditional variance depend on the

past observations. The conditional expectation of Xt in the above model (2.11) is E(Xt|Ft−1) =
µ(t/T ) + (cid:80)p

i=1 ai(t/T )Xt−i, which needs to be positive-valued. To ensure that, we impose the

following constraints on parameter space for the time-varying parameters,

P2 = {µ, ai : 0 < µ(x) < ∞, sup
x

(cid:88)

k

ak(x) < 1}.

(2.12)

6

Note that, the conditions imposed (2.12) on the parameters are somewhat motivated by the

stationarity conditions for the time-constant versions of these models. This is not uncommon

in time-varying AR literature. See Dahlhaus and Subba Rao (2006); Fryzlewicz, Sapatinas and

Subba Rao (2008); Karmakar et al. (2020+) for example. Even though the condition on µ(·) seem

restrictive in the light of what we need for invertible time-constant AR(p) process with Gaussian

error, it is not unusual when it is used to model variance parameters to ensure positivity; it was

unanimously imposed for all the literature mentioned above. Additionally, the above references

heavily depend on local stationarity: namely, for every rescaled time 0 < t < 1, they assume
the existence of an ˜Xi process which is close to the observed process. One key advantage of our
proposal is it is free of any such assumption. Our assumption of only the ﬁrst moment is also

very mild for theoretical exploration in Section 4. Moreover, except for a very general linear

model discussed in (Karmakar et al., 2020+), to the best of our knowledge, this is the very ﬁrst

analysis of the time-varying parameter for count time-series modeled by Poisson regression. Thus

we choose to focus on the methodological development rather than proving the optimality of these

conditions. When p = 0, our proposed model reduces to routinely used nonparametric Poisson

regression model as in Shen and Ghosal (2015).

To proceed with Bayesian computation, we put priors on the unknown functions µ(·) and ai(·)’s

such that they are supported in P2. The prior distributions on these functions are induced through

basis expansions in B-splines. Suitable constraints on the coeﬃcients are imposed to ensure the

shape constraints as in P2. Detailed description of the priors are given below,

µ(x) =

ai(x) =

K1(cid:88)

j=1

K2(cid:88)

j=1

αjBj(x)

θijMiBj(x),

0 ≤ θij ≤ 1,

Mi =

τi
k=0 τk

(cid:80)p

,

i = 1, . . . , p,

θij ∼U (0, 1) for 1 ≤ i ≤ p, 1 ≤ j ≤ K2.

(2.13)

(2.14)

(2.15)

(2.16)

Here Bj’s are the B-spline basis functions. The parameters δj’s are unbounded. Based on the

constraints on the parameter space we consider following prior for αj’s and τi’s,

αj ∼ TN(0, c2

1, 0, ∞),

τi ∼ U (0, 1),

(2.17)

7

where TN stands for the truncated normal distribution with mean 0, variance c2

1 and truncated
in [0, ∞). The priors induced by above construction are P2-supported. The veriﬁcation is very

straightforward and similar to the previous subsection.

2.3 Model properties

In this paper, we only consider TVBINGARCH(1,1) which is commonly used for the GARCH

class of models. One drawback of TVBARC is proper selection of lag. To alleviate this, one may

then consider the TVBINGARCH framework. As in the stationary case, TVBINGARCH(1,1) can

be viewed as TVBARC with inﬁnite order. Then the higher values in b1(·)’s is an indication that

there might be important higher lags in TVBARC. Besides, to infer about higher lag dependence

TVBARC is more suitable than TVBINGARCH. In our real data illustration, we ﬁnd that the

TVBARC model identiﬁes three important higher order lags 6,7 and 8 in COVID-19 spread. Such

inference is diﬃcult to obtain from TVBINGARCH. If the CH coeﬃcient b1(·) is uniformly zero,

TVBINGARCH(1,1) reduces to TVBARC(1). However, the computational steps for TVBARC(1)

does not easily follow from TVBINGARCH(1,1). Furthermore, our theoretical result of TVBIN-

GARCH requires a lower bound for the true CH coeﬃcient which is standard for time varying

GARCH class of models. Thus the theoretical result of TVBARC does not easily follow from

TVBINGARCH.

Towards writing the likelihood, note that our proposed models are non-stationary since the

coeﬃcient functions ai(·), bj(·) are possibly not constant. However, we still take a simple product of

individual conditional likelihoods for Xt’s rather than ﬁrst locally approximating it by a stationary

process. The latter approach is more prominent in the frequentist framework and this phenomenon

is known by ‘locally stationary approximation’. This was introduced in some seminal papers by

Dahlhaus et al. (1997, 2000) and were later used in many time-varying literature. See Dahlhaus

and Subba Rao (2006); Dahlhaus (2012); Truquet et al. (2019) among many others. Towards

the Bayesian approach of modelling such approximating phenomenon, interested readers can refer

to Rosen et al. (2009, 2012). However, the assumption of existence of such an approximating

stationary process is somewhat stringent and is probably not required in Bayesian paradigm. For

example, see DeYoreo and Kottas (2017) where the likelihood is formed by taking product of

individual conditional likelihoods for a non-stationary time-series. Other approaches can be found

8

in Hadj-Amar et al. (2020); Yang and Bradley (2020) where the likelihoods for the proposed non-

stationary processes were computed without any local stationary approximation. Moreover, note

that such approximating stationary processes can be shown to exist under the general smoothness

conditions as outlined in Theorem 1 in Dahlhaus and Subba Rao (2006) (for tvARCH case) or

Proposition 2.3 in Rohan and Ramanathan (2013)(for tvGARCH case). These are easily extendible

to the Poisson setting and for more general Holder smooth coeﬃcient functions with probably

an amended approximation rate. So in a sense, our smoothness assumption and the parameter

restriction as (2.2) or (2.12)implies existence of such stationary processes without us implicitly

putting additional assumption.

3 Posterior computation

In this section, we discuss the Markov Chain Monte Carlo (MCMC) sampling method for posterior

computation. Our proposed sampling is dependent on the gradient-based Hamiltonian Monte

Carlo (HMC) sampling algorithm (Neal et al., 2011). Hence, we show the gradient computations

of the likelihood with respect to diﬀerent parameters for TVBARC(p) and TVBINGARCH(p, q)

in the following two subsections.

We obtain the likelihood from the joint density of the data based on our Poisson error model.

Since the joint density can be written as product of conditionals, we can thus write the joint

likelihood of the data as product of conditional densities. Detail expressions for each case are

separately presented below. The likelihoods of the two models are constructed diﬀerently, thus we

present them separately.

3.1 TVBINGARCH structure

We only derive the computational steps for TVBINGARCH(1,1) which is the frequent choice

among GARCH-type models. While ﬁtting this model, we assume for any t < 0 Xt = 0, λt = 0.

The expression for λ1 also involves λ0. Thus, we need to additionally estimate the parameter

λ0, the Poisson rate parameter for X0. Here the likelihood for TVBINGARCH(1,1) is given by
P (X0) (cid:81)T
t=1 P (Xt|Ft−1). We assume that the marginal distribution of X0 is Poisson(λ0) and the
prior for λ0 is Inverse-Gamma(d1, d2) as described in Section 2.1. The complete likelihood L2 of

9

the propose Bayesian method of (2.1) is given by

L2 ∝ exp

(cid:18) T

(cid:88)

t=1

(cid:2) − {µ(t/T ) + a1(t/T )Xt−1 + b1(t/T )λt−1

(cid:9) + Xt log (cid:8)µ(t/T )

+ a1(t/T )Xt−1 + b1(t/T )λt−i}(cid:3) −

K1(cid:88)

j=1

j /(2c2
α2
1)

− (d1 + 1) log λ0 − d1/λ0

10≤θ11,ηij ≤1,,0≤τi≤1,αj ≥0,

(cid:19)

We calculate the gradients of negative log-likelihood (− log L2) with respect to the parameters β,

θ, η and δ. The gradients are given below,

−

d log L2
α1
(cid:18)

=

1 −

(cid:88)

t

d log L2
θ11

d log L2
ηkj
d log L2
τj

−

−

−

(cid:34)

(cid:88)

i≤p

(cid:88)

1≤k≤q

B1(t/T )Xt−j
(µ(t/T ) + a1(t/T )Xt−j) + b1(t/T )λt−1)

(cid:19)

+ αj/(2c2

1),

(cid:18)

= Mi

1 −

(cid:18)

= Mp+k

1 −

(cid:88)

t

B1(t/T )Xt−j
(µ(t/T ) + aj(t/T )Xt−j) + bk(t/T )λt−1)

(cid:19)
,

(cid:88)

t

B1(t/T )λt−j
(µ(t/T ) + aj(t/T )Xt−j) + bk(t/T )λt−1)

(cid:19)
,

(cid:88)

(Mj1{j=k} − MjMk)×

=

k

(cid:18)

θijBj(x)

1 −

(cid:88)

t

Bj(t/T )Xt−j
(µ(t/T ) + aj(t/T )Xt−j) + b1(t/T )λt−1)

(cid:19)

1{j≤p}+

(cid:18)

ηkjBj(x)

1 −

(cid:88)

t

Bj(t/T )λt
(µ(t/T ) + aj(t/T )Xt−1) + b1(t/T )λt−1)

(cid:19)

(cid:35)

1{j>p}

.

The derivative of the likelihood concerning λ0 is calculated numerically by diﬀerentiating from the

ﬁrst principles. Hence, it is sampled using the HMC algorithm too.

3.2 TVBARC structure

Since we do not have any information of the process for t < 0, our computation for TVBARC(p)
is based on the likelihood (cid:81)T

t=p P (Xt|Ft−1). This likelihood may thus be regarded as a quasi-
likelihood as we are looking at the joint density of last T − p + 1 time points given the ﬁrst p

observations and it is similar to the likelihood from DeYoreo and Kottas (2017). This likelihood

also shares some commonality with the objective functions used for computation in Dahlhaus and

10

Subba Rao (2006); Fryzlewicz, Sapatinas, Rao et al. (2008). The complete posterior likelihood L1

of the proposed Bayesian method in (2.11) is given by

L1 ∝ exp

(cid:18) T

(cid:88)

t=p

(cid:2) − {µ(t/T ) +

p
(cid:88)

i=1

ai(t/T )Xt−i

(cid:9) + Xt log (cid:8)µ(t/T )

+

p
(cid:88)

i=1

ai(t/T )Xt−i}(cid:3) −

(cid:19)

j /(2c2
α2
1)

K1(cid:88)

j=1

10≤θij ≤1,0≤τi≤1,αj ≥0,

where we have µ(x) = (cid:80)K1

k=0 exp(δk) . We
develop eﬃcient MCMC algorithm to sample the parameter β, θ and δ from the above likelihood.

j=1 θijMiBj(x) and Mj = exp(δj )

j=1 exp(βj)Bj(x), ai(x) = (cid:80)K2

(cid:80)p

The derivatives of above likelihood with respect to the parameters are easily computable. This

helps us to develop an eﬃcient gradient-based MCMC algorithm to sample these parameters. We

calculate the gradients of negative log-likelihood (− log L1) with respect to the parameters β, θ

and δ. The gradients are given below,

Bj(t/T )Xt

(µ(t/T ) + (cid:80)

j aj(t/T )Xt−j)

(cid:19)

+ αj/(2c2

1),

(cid:18)

=

1 −

(cid:18)

(cid:88)

t

= Mi

1 −

−

−

−

d log L1
αj

d log L1
θij

d log L1
τj

(cid:88)

t

Bj(t/T )Xt

(µ(t/T ) + (cid:80)

(cid:19)
,

j aj(t/T )Xt−j)
(cid:18)
(cid:88)

θijBj(x)

(cid:88)

=

(Mj1{j=k} − MjMk)

k

i

1 −

(cid:88)

t

Bj(t/T )Xt−j

(µ(t/T ) + (cid:80)

j aj(t/T )Xt−j)

(cid:19)
,

where 1{j=k} stands for the indicator function which takes the value one when j = k.

As the parameter spaces of θij’s and ηkj’s have bounded support, we map any Metropolis can-

didate, falling outside of the parameter space back to the nearest boundary point of the parameter

space. To obtain a good acceptance rate, we tune our HMC sampler periodically. There are two

tuning parameters in HMC namely the leapfrog step, and the step size parameter. The step size

parameter is tuned to maintain an acceptance rate within the range of 0.6 to 0.8. The step size

is reduced if the acceptance rate is less than 0.6 and increased if the rate is more than 0.8. This

adjustment is done automatically after every 100 iterations. However, we choose to pre-specify

the leapfrog step at 30 and obtain good results. Due to the increasing complexity of the parameter

space in TVBINGARCH, we consider updating all the parameters involved in ai(·)’s, bk(·)’s, and

λ0 together.

11

4 Large-sample properties

In this section we obtain posterior contraction rates for the two proposed models. Posterior

contraction measures the speed at which we can recover the true parameter from the posterior

distribution with increasing sample size. The notion of recovery is speciﬁed by a semimetric d.

Deﬁnition (Ghosal and Van der Vaart, 2017): The posterior contraction rate at the true pa-

rameter κ0 ∈ A with respect to the semimetric d on A is a sequence (cid:15)T → 0 such that Pκ0Π(κ :
d(κ, κ0) > MT (cid:15)T |X (T )) → 0 for every MT → ∞, where A denotes the parameter space of θ0. Here
X (T ) stands for the complete dataset.

Although TVBINGARCH(1,1) may reduce to TVBARC(1) assuming b1(x) = 0 for all x ∈

[0, 1], the required technical assumptions do not allow us to derive the results for TVBARC as

a special case for TVBINGARCH. For clarity in presenting the assumptions under which the

respective results are established, we will make the conditions in (2.12) and (2.2) more speciﬁc.

Since TVBARC is a simpler model, we ﬁrst develop the theoretical results for this model and then

make modiﬁcations to obtain the results for TVBINGARCH.

4.1 TVBARC structure

We start by studying large sample properties of the simpler AR model in (2.11). For simplicity,

we ﬁx order p at p = 1 for this section however the results are easily generalizable for any ﬁxed

order p with some additional assumptions. The posterior consistency is studied in the asymptotic

regime of increasing sample size T . Let κ = (µ, a1) stands for the complete set of parameters. For

sake of generality of the method, we put a prior on K1 and K2 with probability mass function

given by,

Π(Ki = k) = bi1 exp[−bi2k(log k)bi3],

(4.1)

with bi1, bi2 > 0 and 0 ≤ bi3 ≤ 1 for i = 1, 2. Poisson and geometric probability mass functions

appear as special cases of the above prior density for bi3 = 1 or 0 respectively. These priors

have not been considered while ﬁtting the model as it would require computationally expensive

reversible jump MCMC strategy. We study the posterior consistency with respect to the average

Hellinger distance on the coeﬃcient functions which is
(cid:90)

d2
1,T =

d2
H(κ1, κ2) =

((cid:112)f1 − (cid:112)f2)2,

1
T

1
T

12

where f1 = (cid:81)T
t=1 Pκ1(Xt|Xt−1). Here P stand for the conditional Poisson density deﬁned in (2.11).
The contraction rate will depend on the smoothness of true coeﬃcient functions µ and a and the

parameters b13 and b23 from the prior distributions of K1 and K2. Let κ0 = (µ0, a10) be the truth

of κ.

Assumptions (A): There exists constants 0 < Mµ < MX such that,

(A.1) At time t = 0, Eκ0(X0) < MX.

(A.2) The coeﬃcient functions supx∈[0,1] µ0(x) < Mµ and supx∈[0,1] a10(x) < 1 − Mµ/MX.

(A.3) inf x∈[0,1] min(µ0(x), a10(x)) > ρ for some small ρ > 0.

Assumptions (A.1), (A.2) ensure

Eκ0(Xt) = Eκ0(Eκ0(Xt|Xt−1)) < Mµ +

1 −

(cid:18)

(cid:19)

Mµ
MX

MX < MX

by recursion. Assumption (A.3) is imposed to ensure strict positivity of parameters and is standard

in time-varying literature that deals with such constrained parameters.

Posterior consistency theory studies recovery of the ‘true’ parameter κ0 with increasing sample

size when the data is sampled from the distribution characterized by κ0. Our notion of recovery

is based on the average Hellinger metric d2

1,T deﬁned above.

Theorem 1. Under assumptions (A.1)-(A.3), let the true functions µ0(·) and a10(·) be H¨older

smooth functions with regularity level ι1 and ι2 respectively, then the posterior contraction rate

with respect to the distance d2

1,T is

(cid:26)

T −ι1/(2ι1+1)(log T )ι1/(2ι1+1)+(1−b13)/2, T −ι2/(2ι2+1)(log T )ι2/(2ι2+1)+(1−b23)/2

(cid:27)

.

max

where bij are speciﬁed in (4.1). For the proof, the ﬁrst step is to calculate posterior contrac-

T log (cid:82) f 1/2
tion rate with respect to average log-aﬃnity r2
T (f1, f2) (cid:46) (cid:15)2
T d2
r2
T . The average log-aﬃnity provides a unique advantage to
construct exponentially consistent tests leveraging on the famous Neyman-Pearson Lemma as has

and then show that

T (f1, f2) = − 1

H(f1, f2) (cid:46) (cid:15)2

T implies 1

1 f 1/2

2

also been used in Ning et al. (2020) for a multivariate linear regression setup under group sparsity.

The proof is postponed to Section 8. The proof is based on the general contraction rate result

from Ghosal and Van der Vaart (2017) and some results on B-splines based ﬁnite random series.

13

4.2 TVBINGARCH structure

Next, we discuss the more comprehensive tvBINGARCH model (2.1). To maintain simplicity in

the proof, we again assume p = 1, q = 1. Similar to the previous subsection, we put a prior on the

number of Bspline bases, Ki with probability mass function given by,

Π(Ki = k) = bi1 exp[−bi2k(log k)bi3],

with bi1, bi2 > 0 and 0 ≤ bi3 ≤ 1 for i = 1, 2, 3. Let us assume that ψ = (µ, a1, b1) be the complete

set of parameters. We study the posterior consistency with respect to the Hellinger distance on

the coeﬃcient functions which is

d2
2,T =

1
T

d2
H(ψ1, ψ2) =

(cid:90)

1
T

((cid:112)f1 − (cid:112)f2)2,

where f1 = Pφ1(X0) (cid:81)T
t=1 Pψ1(Xt|Xt−1, λt−1). Here P stands for the conditional Poisson den-
sity deﬁned in (3) and the marginal density of X0, Pφ1(X0) is Poisson(λ10) as described in our

computational steps.

For this structure, we modify the assumptions as

Assumptions(B): There exists constants 0 < Mµ < MX such that,

(B.1) At time t = 0, Eψ0(X0), λ0 < MX.

(B.2) The coeﬃcient functions supx∈[0,1] µ0(x) < Mµ and supx∈[0,1](a10(x) + b10(x)) < 1 − Mµ/MX.

(B.3) inf x∈[0,1] min(µ0(x), a10(x), b10(x)) > ρ for some small ρ > 0.

Assumptions (B.1), (B.2) ensure

Eψ0(Xt) = Eψ0(Eψ0(Xt|Xt−1, λt−1)) < Mµ +

1 −

(cid:18)

(cid:19)

Mµ
MX

MX < MX

by recursion. Thus we have, by Assumption (B.1-B.2)

Eψ0(Xt) < MX, Eψ0(λt) = Eψ0(Xt|Xt−1, λt−1) = Eψ0(Xt) < MX.

Assumption (B.3) is imposed to ensure strict positivity of parameters and is standard in time-

varying literature that deals with such constrained parameters. Now we present our posterior

contraction rate theorem below. The deﬁnition of the contraction rate is the same as before.

14

Theorem 2. Under assumptions (B.1)-(B.3), let the true functions µ0(·), a10(·) and b10(·) be

H¨older smooth functions with regularity level ι1, ι2 and ι3 respectively, then the posterior contrac-

tion rate with respect to the distance d2

2,T is

(cid:26)

T −ι1/(2ι1+1)(log T )ι1/(2ι1+1)+(1−b13)/2, T −ι2/(2ι2+1)(log T )ι2/(2ι2+1)+(1−b23)/2,

max

T −ι3/(2ι3+1)(log T )ι3/(2ι3+1)+(1−b33)/2

(cid:27)

.

The proof follows from a similar strategy as in Theorem 1. An outline of the proof can be found

in the Section 8.

5 Simulation studies

In this section, we study the performance of our proposed Bayesian method in capturing the

true coeﬃcient functions. We compare both TVBARC and TVBINGARCH methods with some

other competing models. It is important to note that, this is to the best of our knowledge ﬁrst

work in Poisson autoregression with a time-varying link. Thus, we compare our method with the

existing time-series models with time-constant coeﬃcients for count data and time-varying AR

with Gaussian error. We also examine the estimation accuracy of the coeﬃcient functions for

estimating the truth.

The hyperparameter c1 of the truncated normal prior is set to 10 to ensure weak informative-

ness. The hyperparameters for Inverse-Gamma prior d1 = 0.1, which is also weakly informative.

We consider 6 equidistant knots for the B-splines based on comparing the AMSE scores. We

choose the knot number after which the AMSE score does not change signiﬁcantly. We collect

10000 MCMC samples and consider the last 5000 as post-burn-in samples for inferences. In ab-

sence of any alternative method for time-varying AR(p) model of count-valued data, we compare

the estimated functions with the true functions in terms of the posterior estimates of functions

along with its 95% pointwise credible bands. The credible bands are calculated from the MCMC

samples at each point t = 1/T, 2/T, . . . , 1. We also compare diﬀerent competing methods in terms

of average MSE (AMSE) score using the INGARCH method of tsglm from R package tscount,

GARMA using tscount as well, tvAR and our proposed Bayesian methods. The AMSE is deﬁned

15

as 1
T

(cid:80)

t(Xt − ˆλt)2. We estimate this in terms of the posterior mean of AMSEs across MCMC as

AM SE =

1
5000

5000
(cid:88)

S=1

1
T

(cid:88)

t

(Xt − ˆλS

t )2,

where ˆλS
t

is the posterior estimate of λt at S-th postburn sample.

5.1 Case 1: TVBARC structure

Here, we consider two model settings p = 1; Xt ∼ Poisson(µ(t/T ) + a1(t/T )Xt−1) and p = 2; Xt ∼

Poisson(µ(t/T ) + a1(t/T )Xt−1 + a2(t/T )Xt−2) for t = 1, . . . , T . Three diﬀerent choices for T have

been considered, T = 100, 500 and 1000. The true functions are for x ∈ [0, 1],

µ0(x) =10 exp (cid:0) − (x − 0.5)2/0.1(cid:1),

a10(x) =0.3(x − 1)2 + 0.1,

a02(x) =0.4x2 + 0.1.

We compare the estimated functions with the truth for sample size 1000 in Figures 1 and

Figure 2 for the models p = 1 and p = 2 respectively. Tables 1 and 2 illustrate the performance

of our method with respect to other competing methods.

Table 1: AMSE comparison for diﬀerent sample sizes across diﬀerent methods when the true

model is (2.11) with p = 1.

INGARCH(1,0) GARMA(1,0) TVAR(1) TVBARC(1)

T = 100

T = 500

T = 1000

11.60

11.35

11.05

11.18

11.04

10.73

11.41

11.24

10.94

8.65

8.12

7.02

5.2 Case 2: TVBINGARCH structure

For the tvBINGARCH case, we only consider one simulation settings p = 1, q = 1; Xt ∼ Poisson(µ(t/T )+

a1(t/T )Xt−1 + b1(t/T )λt−1). Two diﬀerent choices for T have been considered, T = 100 and 200

16

(a) µ()

(b) a1()

Figure 1: Estimated mean function in 1st column and estimated AR(1) coeﬃcient function in

the 2nd column for the case p = 1 and sample size 1000. Red is the true function, black is the

estimated curve along with the 95% pointwise credible bands in green.

Figure 2: Estimated coeﬃcient functions for the simulation case p = 2 and sample size 1000. Red

is the true function, black is the estimated curve along with the 95% pointwise credible bands in

green.

17

Table 2: AMSE comparison for diﬀerent sample sizes across diﬀerent methods when the true

model is (2.11) with p = 2.

INGARCH(2,0) GARMA(2,0) TVAR(2) TVBARC(2)

T = 100

T = 500

T = 1000

18.02

16.42

15.79

17.28

15.86

15.25

13.04

12.61

12.75

11.01

10.79

10.61

Figure 3: Estimated coeﬃcient functions for the TVBINGARCH(1,1) and sample size 1000. Red

is the true function, black is the estimated curve along with the 95% pointwise credible bands in

green.

and for x ∈ [0, 1] the coeﬃcient functions are,

µ0(x) =25 exp (cid:0) − (x − 0.5)2/0.1(cid:1),

a1(x) =0.3(x − 1)2 + 0.1,

b1(x) =0.1x1.5 + 0.1

Figure 3 compares the estimated functions with the truth for sample size 200 for the model in (2.1)

with p = 1, q = 1. The performance of our method is compared to other competing methods in

Tables 3.

Table 3: Average MSE comparison for diﬀerent sample sizes across diﬀerent methods when the

true model is (2.1) with p = 1, q = 1.

INGARCH(1,1) GARMA(1,1)

tvAR(10) TVBINGARCH(1,1)

T = 100

T = 500

T = 1000

27.38

24.02

23.23

24.50

22.90

22.93

27.60

24.07

23.32

18

22.83

21.23

21.19

Figure 1 to 3 shows that our proposed Bayesian method captures the true functions quite well

for both of the two simulation experiments. We ﬁnd that the estimation accuracy improves as the

sample size increases. As the sample size grows, the 95% credible bands are also getting tighter,

implying lower uncertainty in estimation. This gives empirical evidence in favor of the estimation

consistency which has also been veriﬁed theoretically in Section 4. The average mean square error

(AMSE) is always the lowest for our method in Tables 1, 2 and 3.

6 COVID-19 spread at NYC

We collect the data of new aﬀected cases for every day from 23rd January to 14th July from an

open-source platform {https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset}.

The end date 14th July is chosen as around that time NYC started the process of re-opening. The

data on daily new cases are illustrated in Figure 4. We were particularly interested in NYC

data as this city remained an epicenter in US for about a month. With the help of government

interventions and sustained lock-down, the recovery was signiﬁcant in about 3 months. Such a

time-varying nature of the data motivated us to retrospect as how the mean trend and AR trend

behave which can also shed some insight about eﬀects of lockdown or the contagious spread.

Based on the ﬁndings on the incubation of the virus in Lauer et al. (2020a) and others, it is

understood that the symptoms often take some time after the virus aﬀects through contagion. Our

idea is to consider diﬀerent models with varying number of lags for this. We consider TVBARC(1),

TVBARC(10) and TVBINGARCH(1,1) here. The results for the TVBARC(1) are illustrated in

Figure 5. We see that during the spike in daily new cases the function a1(·) is the highest. Figure 6

depicts the estimated mean and coeﬃcient functions from a TVBARC(10) model. We ﬁnd that

the estimated a1(·) functions show a similar trend. On top of that, we see that a6(·), a7(·) and

a8(·) have also some eﬀect. Finally we ﬁt our TVBINGARCH(1,1) which might be considered

TVBARC with inﬁnite order. Figure 7 depicts the estimated functions, the mean {µ(·)}, AR(1)

{a1(·)} and CH(1) {b1(·)} coeﬃcient functions. In Table 4, we compare the AMSE scores across

diﬀerent models. For all the models, we consider 12 equidistant knots based on the AMSE scores

as discussed in Section 5.

Figure 6 suggests that even lag 6, 7, and 8 have some signiﬁcant contribution. The eﬀect of

this lag is suppressed in Figure 5 and is expressed in terms of b1(·) of Figure 7. The estimated

19

Figure 4: Daily new COVID-19 cases from 31st January to 14th of July recorded at NYC.

mean functions also behave similarly for all the three cases. It shows a spike during the rise of

daily new cases. After that, it decreases which can talk about successful containment strategies

in NYC. More speciﬁcally it decreases after around 15 days since the strict implementation of

statewide lockdown on 20-th March. This is consistent with what was found in our unsubmitted

preprint (Roy and Karmakar, 2020) through an empirical early-stage analysis of the spread in

diﬀerent cities and countries.

The eﬀect of Lag 6, 7, and 8 can be attributed to the incubation period of the virus.

It

can also lead to the ﬁnding that there was a weekly periodicity which is probably due to shorter

testing/administrative facilities being available during the weekend. Note that our choice of ﬁtting

an TVBARC(10) model is more general than separately ﬁtting a seasonal/periodic time-series

model. Another important ﬁnding is coming from the overall trend of a1(·). It starts to decrease

when the number of cases starts going down. However later on it varies around 0.6 can be

attributed to the fact that the number of new cases did not vary much and remained around the

same level from the middle of May. The credible bands look very small around the mean function

which is probably due to the large magnitude of the estimated function.

20

(a) NYC-µ(·) function

(b) NYC-a1(·) function

Figure 5: Estimated mean functions in 1st column and estimated AR coeﬃcient functions in

the 2nd column for NYC using TVBARC(1). Black is the estimated curve along with the 95%

pointwise credible bands in green for the mean and AR(1) function.

(a) NYC-µ(·) function

(b) NYC-a(·) functions

Figure 6: Estimated mean functions in 1st column and estimated AR coeﬃcient functions in

the 2nd column for NYC using TVBARC(10). Black is the estimated curve along with the 95%

pointwise credible bands in green for the mean function.

21

Figure 7: Estimated coeﬃcient functions for the TVBINGARCH(1,1) on NYC data. Black is the

estimated curve along with the 95% pointwise credible bands in green.

Table 4: Average MSE comparison for diﬀerent methods on NYC data.

Method

AMSE

Method

AMSE

Method

AMSE

INGARCH(1,1)

318056.3 GARMA(10,0)

1682976.1

TVBARC(1)

210258.9

GARMA(1,1)

329610.1

tvAR(1)

338970.6

TVBARC(10)

185777.9

AR(10,0)

1376133.7

tvAR(10)

274913.7 TVBINGARCH(1,1)

212168.1

7 Discussion

We propose a time-varying Bayesian autoregressive model for counts (TVBARC) and time-varying

Bayesian integer-valued generalized autoregressive conditional heteroskedastic model (TVBIN-

GARCH) with linear link function within Poisson error to study the time series of daily new

conﬁrmed cases of COVID-19. We develop a novel hierarchical Bayesian model that satisﬁes

the stability condition for the respective time-varying models and propose an HMC algorithm

based MCMC sampling scheme. We also establish posterior contraction rate results of the pro-

posed Bayesian methods. The ‘R’ function with an example code can be found at https:

//github.com/royarkaprava/TVBARC. Relying on the proposed hierarchical Bayesian model,

one can develop a time-varying Bayesian model for positive-valued time-series data too. Our anal-

ysis of NYC data shows that there is a time-varying eﬀect of Lag 6, 7, and 8. Some preliminary

analysis on COVID data using our model based on the data until April 24 are archived in our

unpublished pre-print Roy and Karmakar (2020). There are some more interesting ﬁndings related

to signiﬁcant lags for diﬀerent countries.

The deﬁnition of posterior contraction rate we followed involves an diverging sequence MT →

22

∞. If it is possible to replace MT by a large constant M without changing (cid:15)T , the contraction

rate then holds in a slightly stronger sense (see Chapter 8, Ghosal and Van der Vaart (2017)).

Local stationary approximation of the proposed nonstationary process is expected help to establish

such result. Establishing Bernstein von-Mises type theorem to ensure asymptotic normality of the

posterior distribution will also be interesting. However, such results are not yet available for the

corresponding stationary cases. Nevertheless, it is also important direction of future research.

As future work, it will be interesting to include some country-speciﬁc information such as

demographic information, geographical area, the eﬀect of environmental time-series, etc in the

model. These are usually important factors for the spread of any infectious disease. We can

also categorize the diﬀerent types of government intervention eﬀects to elaborate more on the

speciﬁc impacts of the same. In the future we wish to analyze the number of deaths, number of

recovered cases, number of severe/critical cases, etc. for these diseases as those will hopefully have

diﬀerent dynamics than the one considered here and can provide useful insights about the spread

and measures required. For computational ease, we have considered the same level of smoothness

for all the coeﬃcient functions. Fitting this model with diﬀerent levels of smoothness might

be able to provide more insights. Lag selection is a diﬃcult task for time-varying auto-regressive

models. One potential future direction would be to put sparsity inducing prior to the time-varying

coeﬃcient functions in TVBARC for automatic lag detection. Other than building time-varying

autoregressive models for count-valued data using the hierarchical structure from this article, one

interesting future direction is to extend this model for vector-valued count data. In general, it is

diﬃcult to model multivariate count data. There are only a limited number of methods to deal

with multivariate count data (Besag, 1974; Yang et al., 2013; Roy and Dunson, 2019). Building

on these multivariate count data models, one can extend our time-varying univariate AR(p) to a

time-varying vector-valued AR(p). On the same note, even though we imposed Poisson assumption

for increased model interpretation, in the light of the upper bounds for the KL distance, it is not

a necessary criterion and can be applied to a general multiple non-stationary count time-series.

Extending some of the continuous time-series invariance results for nonlinear non-stationary and

multiple series from Karmakar and Wu (2020) to a count series regime will be an interesting

challenge. Finally, we wish to undertake an autoregressive estimation of the basic reproduction

number with the time-varying version of compartmental models in epidemiology.

23

8 Proof of Theorems

We study the frequentist property of the posterior distribution is increasing T regime assuming that

the observations are coming from a true density f0 characterized by the parameter κ0. We follow

the general theory of Ghosal et al. (2000) to study the posterior contraction rate for our problem.

In the Bayesian framework, the density f is itself a random measure and has distribution Π which is

the prior distribution induced by the assumed prior distribution on κ. The posterior distribution of

a neighborhood UT = {f : d(f, f0) < (cid:15)T } around f0 given the observation X (T ) = {X0, X1, . . . , XT }

is

ΠT (U c

T |X (T )) =

(cid:82)
f (X (T ))dΠ(κ)
U c
T
(cid:82) f (X (T ))dΠ(κ)

8.1 General proof strategy

The posterior consistency would hold if above posterior probability almost surely goes to zero in
F (T )
κ0 probability as T goes to ∞, where F (T )
is the true distribution of X (T ). Recall the deﬁnition
κ0
of posterior contraction rate; for a sequence (cid:15)T if ΠT (d(f, f0)|X (T ) ≥ MT (cid:15)T |X (T )) → 0 in F (T )
κ0 -

probability for every sequence MT → ∞, then the sequence (cid:15)T is called the posterior contraction

rate. If the assertion is true for a constant MT = M , then the corresponding contraction rate

becomes slightly stronger.

Note that for two densities f0, f characterized by κ0 and κ respectively, the Kullback-Leibler

divergences are given by

KL(κ0, κ) =

(cid:90)

f0 log

f0
f

(cid:34)

= Eκ0

log

(X0) (cid:81)T
PQκ0
PQκ(X0) (cid:81)T

t=1 Pκ0(Xt|Ft−1, λ0)
t=1 Pκ(Xt|Ft−1, λ0)

(cid:35)

.

Assume that there exists a sieve in parameter space such that Π(W c

T ) ≤ exp(−(CT + 2)T (cid:15)2

T ) and

we have tests χT such that

Eκ0(χT ) ≤ e−LT T (cid:15)2

T /2

sup
κ∈WT :d2(f,f0)>LT (cid:15)2
T

Eκ(1 − χT ) (cid:46) e−LT T (cid:15)2

T

for some LT > CT + 2. Say UT = {f : d2(f, f0) ≤ LT (cid:15)2

T } and ST = {(cid:82) f (X T )

f0(X T ) dΠ(κ) ≥

24

ΠT ( 1

T KL(κ0, κ) < (cid:15)T ) exp(−CT T (cid:15)2

T )}. We can bound the posterior probability from above by,

ΠT (d(f, f0) ≥ MT (cid:15)T |X (T )) ≤ χT + (1 − χT )

= χT + (1 − χT )

(cid:82)
f (X T )dΠ(κ)
U c
T
(cid:82) f (X (T ))dΠ(κ)
(cid:82)
f (X (T ))
f0(X (T )) dΠ(κ)
U c
T
(cid:82) f (X (T ))
f0(X (T )) dΠ(κ)

≤ χT + 1{Sc

T } + (1 − χT )

(cid:82)

f (X (T ))
f0(X (T )) dΠ(κ)

U c
T
T )ΠT { 1
exp(−CT T (cid:15)2

T KL(κ0, κ) < (cid:15)T }

≤ χT + 1{Sc

T } +

exp(CT T (cid:15)2
T )
T KL(κ0, κ) < (cid:15)T }

ΠT { 1

(1 − χT )

(cid:82)

f (X (T ))

U c
T
f0(X (T ))

dΠ(κ)

(8.1)

Taking expectation with respect to κ0, ﬁrst term go to zero by construction of χT . The second
term Eκ01{Sc
sequence CT → ∞. We would require that ΠT { 1

T } goes to zero due to Lemma 8.21 of Ghosal and Van der Vaart (2017) for any
T ). Then for the

T KL(κ0, κ) < (cid:15)T } ≥ exp(−T (cid:15)2

third term,

Eκ0 exp((CT + 1)T (cid:15)2

T )(1 − χT )

(cid:82)

f (X (T ))

U c
T
f0(X (T ))

dΠ(κ) = exp((CT + 1)T (cid:15)2
T )

(cid:90)

U c
T

f (X (T ))(1 − χT )dΠ(κ)

f (X (T ))(1 − χT )dΠ(κ) + Π(W c
T )

(cid:35)

≤ exp(CT + 1)T (cid:15)2
T )

(cid:34)(cid:90)

U c

T ∩WT

(cid:34)

= exp((CT + 1)T (cid:15)2
T )

sup
κ∈WT :d2(f,f0)>LT (cid:15)2
T

(cid:35)
Eκ(1 − χT ) + Π(W c
T )

(cid:46) exp(−T (cid:15)2

T ).

(8.2)

Thus we need three things to calculate posterior contraction rate.

(i) (Prior mass Condition) We would require ΠT { 1

T KL(κ0, κ) < (cid:15)T } ≥ exp(−T (cid:15)2

T ),

(ii) (Sieve) construct the sieve WT such that Π(W c

T ) ≤ exp(−(CT + 2)T (cid:15)2

T ) and

(iii) (Test construction) exponentially consistent tests χT .

We ﬁrst study the contraction properties with respect to d2(f, f0) = r2
and then show that the same rate holds for average Hellinger 1

H(f, f0). Note that LT can
T . With the above general structure, we now proceed to prove individual

T d2

be taken as LT = M 2

T (f, f0) = − 1

T log (cid:82) √

f f0

theorems focusing on the TVBARC and the TVINGARCH cases.

25

8.2 Proof of Theorem 1

For the sake of technical convenience we show our proof for time-varying AR model with 1 lag

only. All the proofs go through for higher lags with the same technical tools.

8.2.1 KL Support

The likelihood based on the parameter space κ is given, Pκ(X0) (cid:81)T
be the distribution of Xt with parameter space κ.

t=1 Pκ(Xt|Xt−1). Let Qκ,t(Xt)

t=1 Pκ0(Xt|Ft−1, λ0)
t=1 Pκ(Xt|Ft−1, λ0)

(cid:81)T

We have
(cid:81)T

R = log

=

T
(cid:88)

t=1

[−{µ0(t/T ) − µ(t/T )} − {a01(t/T ) − a1(t/T )}Xt−1 + Xt{log(µ0(t/T ) + a01(t/T )Xt−1)

− log(µ(t/T ) + a1(t/T )Xt−1)}]

Then KL(κ0, κ) = Eκ0(R). We have in light of MVT,

|R| ≤

T
(cid:88)

t=1

[|µ(t/T ) − µ0(t/T )| + |a1(t/T ) − a01(t/T )|Xt−1

+

Xt
µ∗(t/T ) + a1∗(t/T )Xt−1

{|µ(t/T ) − µ0(t/T )| + |a1(t/T ) − a01(t/T )|Xt−1}]

≤ T (cid:107)µ − µ0(cid:107)∞ + (cid:107)a1 − a01(cid:107)∞

(cid:88)

t

Xt−1 + (cid:107)µ − µ0(cid:107)∞/ρ

(cid:88)

t

Xt + (cid:107)a1 − a01(cid:107)∞/ρ

(8.3)

(8.4)

(cid:88)

Xt,

t

(8.5)

under the assumption that κ(·) = (µ(·), a1(·), b1(·)) and κ0(·) = (µ0(·), a10(·), b10(·)) are close and

also κ∗ is close to both and also in conjunction with Assumption (A.3) to imply inf t a1∗(t/T ) > ρ

and Assumption (A.2) which implies E(Xt) < MX. Then for the ﬁrst term we use the bound

µ∗(t/T ) + a1∗(t/T )Xt−1 > ρ and for the second term the bound µ∗(t/T ) + a1∗(t/T )Xt−1 > ρXt−1
is used to have |µ(t/T )−µ0(t/T )|+|a(t/T )−a(t/T )|Xt−1

≤ (cid:107)µ − µ0(cid:107)∞/ρ + (cid:107)a1 − a01(cid:107)∞/ρ for all t. Thus,

µ∗(t/T )+a1∗(t/T )Xt−1
1
T

E(R) (cid:46) (cid:107)µ − µ0(cid:107)∞ + (cid:107)a1 − a01(cid:107)∞.

(8.6)

8.2.2 Posterior contraction in terms of average negative log-aﬃnity

In this section, we focus on the requirements to calculate posterior contraction rate as in Sec-

tion 8.1.We ﬁrst show posterior consistency in terms of average negative log-aﬃnity which is de-

26

ﬁned as r2

T (f1, f2) = − 1

Then we show that, having r2

T log (cid:82) f 1/2

2

1 f 1/2
T (f1, f0) (cid:46) (cid:15)2

between f1 and f2. Here, we have f1 = (cid:81)T
n implies that our distance metric d2

2,T (f1, f0) (cid:46) (cid:15)2
n.

i=1 Pκ1(Xi|Xi−1).

Proceeding with the rest of the proof of Theorem 1, we use the results of B-Splines, (cid:107)µ−µ0(cid:107)∞ ≤

(cid:107)α − α0(cid:107)∞, where α = {αj} and (cid:107)a1 − a10(cid:107)∞ ≤ (cid:107)γ − γ0(cid:107)∞, where γj = θ1jM1, such that γj < 1.

The H¨older smooth functions with regularity ι can be approximately uniformly up to order K −ι
with K many B-splines. Thus we have (cid:15)T (cid:38) max{K −ι1

1T , K −ι2

2T }.

We need to lower bound the prior probability as required by (i). We have the result (8.6) and

the prior probabilities Π((cid:107)α − α0(cid:107)∞ (cid:46) (cid:15)T , (cid:107)γ − γ0(cid:107)∞ (cid:46) (cid:15)T ) (cid:38) (cid:15)K1T +K2T

T

based on the discussion of

A2 from Shen and Ghosal (2015). The rate of contraction cannot be better than the parametric
rate T −1/2, and so log(1/(cid:15)T ) (cid:46) log T . Thus (i) requires that in terms of pre-rate ¯(cid:15)T , we need
(K1T + K2T ) log T (cid:46) T ¯(cid:15)2
T .

In our problem, we consider following sieve as required by (ii)

WT = {K1, K2, α, γ : K1 ≤ K1T , K2 ≤ K2T , (cid:107)α(cid:107)∞ ≤ AT , min(α, γ) > ρT , γ ≤ 1 − AT /BT ,

λ0 ≤ BT , AT < BT },

(8.7)

where AT , BT are at least polynomial in T and λ0 is the mean of X0 and KT = max{K1T , K2T }.
We take ρT (cid:16) T −a with a < 1, AT (cid:16) T a1, BT (cid:16) T a2 with a2 > a1 for technical need. Note

that, for κ ∈ WT , we have Eκ(Xt) < BT . We need to choose these bounds carefully so that

we have Π(W c

T ) ≤ exp(−(1 + C1)T (cid:15)2

T ), which depend on tail properties of the prior. We have,
T ) = Π[K1 > K1T , K2 > K2T , αK1T ∈ {x : inf x > ρT , sup x < AT , γK2T /∈ {x : inf x >

Π(W c
ρT , sup x < 1 − AT
BT

}, λ0 > BT ].

(cid:104)

Hence we have, Π(W c
(cid:105)K2T

T ) ≤ Π(K1 > K1T ) + Π(K2 > K2T ) + Π{αK1T /∈ [ρT , AT ]K1T } + Π{γK2T /∈
ρT , 1 − AT
} + Π{λ0 > BT } where αK1T is the vector of full set of coeﬃcients of length
BT
K1T and γK2T is the vector of coeﬃcients of length K2T . The quantity Π[αK1T /∈ [ρT , AT ]K1T
can be further upper bounded by K1T Π(α1 /∈ [ρT , AT )]) ≤ K1T exp{−R1T a3}, for some constant

R1, a3 > 0 which can be veriﬁed from the discussion of the assumption A.2 of Shen and Ghosal

(2015) for our choice of prior which exponential. On the other hand, Π{γK2T /∈
K2T Π(γ1 /∈ [ρT , 1 − AT
BT

]) ≤ K2T exp{−R2T a4} for some constant R2, a4 > 0 which can be veriﬁed

≤

(cid:104)
ρT , 1 − AT
BT

(cid:105)K2T

from the proof of Roy et al. (2018). The inverse-gamma prior of λ0 has exponential tail similar

to α1 and thus can be ignored as K1T grows with T . Since BT > AT , the tail of λ0 can be upper

bounded by tail of α1

27

Hence, Π(W c

T ) (cid:46) F1(K1T ) + F2(K2T ) + (K1T + K2T ) exp{−RT a5}. The two functions F1 and F2
in the last expression stand for the tail probabilities of the prior of K1 and K2. We can calculate
their asymptotic order as, F1(x) = Π(K1 > x) (cid:16) exp{−x(log x)b13} and F2(x) = Π(K2 > x) (cid:16)
exp{−x(log x)b23}. We need Π(W c

T }. Hence, we calculate pre-rate from the

T ) (cid:46) exp{−(1 + CT )T (cid:15)2

following equation for some sequence HT → ∞,

K1T (log T )b13 + K2T (log T )b23 (cid:38) HT T ¯(cid:15)2
T ,

log(K1T + K2T ) + HT T ¯(cid:15)2
T

(cid:46) T a5.

(8.8)

Now, we construct test χT such that

Eκ0(χT ) ≤ e−LT T (cid:15)2

T /2

sup
T (κ,κ0)>LT (cid:15)2
T

κ∈WT :r2

Eκ(1 − χT ) (cid:46) e−LT T (cid:15)2

T

for some LT > CT + 2.

To construct the test as required in (iii), we ﬁrst construct the test for point alternative

H0 : κ = κ0 vs H1 : κ = κ1. The most powerful test for such problem is Neyman-Pearson test
φ1T = 1{f1/f0 ≥ 1}. For r2

T , we have

T > LT (cid:15)2

Eκ0φ1T = Eκ0((cid:112)f1/f0 ≥ 1) ≤

(cid:90) (cid:112)f1f0 ≤ exp(−LT T (cid:15)2
T ),

Eκ1(1 − φ1T ) = Eκ1((cid:112)f0/f1 ≥ 1) ≤

(cid:90) (cid:112)f0f1 ≤ exp(−LT T (cid:15)2
T ).

It is natural to have a neighborhood around κ1 such the Type II error remains exponentially

small for all the alternatives in that neighborhood under the test function φ1T . By Cauchy-Schwarz

inequality, we can write that

Eκ(1 − φ1T ) ≤ {Eκ1(1 − φ1T )}1/2{Eκ1(f /f1)2}1/2.

In the above expression, the ﬁrst factor already exponentially decaying. The second factor can be
allowed to grow at most of order ecT (cid:15)2

T for some positive small constant c. We show that Eκ1(f /f1)2

is bounded for every κ such that

(cid:107)µ − µ1(cid:107)∞ ≤

√
ρT√
T

, (cid:107)a − a1(cid:107)∞ ≤

√
ρT√
T BT

.

28

We have, in the light of AM-GM inequality,

Eκ1(f /f1)2 =

(cid:90) f 2
f 2
1

f1 =

(cid:90) f
f1

f = Eκ

f
f1

= Eκ

T
(cid:89)

t=1

f (Xt|Xt−1)
f1(Xt|Xt−1)

≤

1
T

(cid:88)

t=1

Eκ

(cid:18) f (Xt|Xt−1)
f1(Xt|Xt−1)

(cid:19)T

Towards uniformly bounding the summand in the above display, we write

Eκ

(cid:18) f (Xt|Xt−1)
f1(Xt|Xt−1)

(cid:19)T

= EXt−1,κ

∞
(cid:88)

Xt=0

{f (Xt|Xt−1)}T
{f1(Xt|Xt−1)}T f (Xt|Xt−1)
(cid:18) λT +1
λT
1

∞
(cid:88)

Xt=0

(cid:19)Xt

/Xt!

= EXt−1,κ exp[−T (λ − λ1) − λ]

= EXt−1,κ exp[−T (λ − λ1) − λ +

(cid:34)

λT +1
λT
1

]

= EXt−1,κ exp

− T {µ(t/T ) − µ1(t/T )} − T {a1(t/T ) − a11(t/T )}Xt−1

− µ(t/T ) − a1(t/T )Xt−1 +

(µ(t/T ) + a1(t/T )Xt−1)T +1
(µ1(t/T ) + a11(t/T )Xt−1)T

(cid:35)

.

(8.9)

where, λ = µ(t/T ) + a1(t/T )Xt−1,λ1 = µ1(t/T ) + a11(t/T )Xt−1 and EXt−1,κ denotes unconditional

expectation over Xt−1 under the density f with parameter κ. Let us deﬁne r1 = (cid:107)µ − µ1(cid:107)∞ and

r2 = (cid:107)a1 − a11(cid:107)∞

Assuming µ(t/T ) − µ1(t/T ) and a1(t/T ) − a11(t/T ) very small, we can write
(cid:20)(µ(t/T ) + a1(t/T )Xt−1)T +1
(µ1(t/T ) + a11(t/T )Xt−1)T
(cid:26)

(cid:27)T

(cid:21)

=

1 +

(cid:26)

≈

1 + T

µ(t/T ) − µ1(t/T ) + (a1(t/T ) − a11(t/T ))Xt−1
µ1(t/T ) + a11(t/T )Xt−1

µ(t/T ) − µ1(t/T ) + (a1(t/T ) − a11(t/T ))Xt−1
µ1(t/T ) + a11(t/T )Xt−1

(µ(t/T ) + a1(t/T )Xt−1)

(cid:27)

(µ(t/T ) + a1(t/T )Xt−1)

(8.10)

For the above approximation to hold, we need µ(t/T )−µ1(t/T )+(a1(t/T )−a11(t/T ))Xt−1

µ1(t/T )+a11(t/T )Xt−1

to be small. To

verify that, observe that

(cid:12)
(cid:12)
(cid:12)
(cid:12)

µ(t/T ) − µ1(t/T ) + (a1(t/T ) − a11(t/T ))Xt−1
µ1(t/T ) + a11(t/T )Xt−1

(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤

r1
ρT

+

r2
ρT

=

√

1
T ρT

(1 +

1
√
BT

).

As we have ρT = T −a with a < 1, it follows directly. Thus (8.9) before EXt−1,κ applying on (8.10)

29

becomes

(cid:20)[T {µ( t

T ) − µ1( t

T )} + T {a1( t

exp

T ) − a11( t
µ1( t

T )}Xt−1][{µ( t
T ) + a11( t

T )Xt−1

T ) − µ1( t

T )} + {a1( t

T ) − a11( t

T )}Xt−1]

(cid:21)

≤ exp[T r2

1/ρT + 2T r1r2/ρT + T r2

2Xt−1/ρT ]

(8.11)

The bound in (8.11) is obtained by applying a combination of the following inequalities µ(t/T ) +

a1(t/T )Xt−1 > ρT or > ρT Xt−1, |µ(t/T ) − µ1(t/T )| < r1 and |a1(t/T ) − a11(t/T )| < r2. Taking
q = T r2
2/ρT , last part becomes E(eqXt−1) after taking exectation over (8.11). We have E(eqX0) =
eλ0(eq−1) < eBT (eq−1) = eQ for Q = BT (eq − 1) =⇒ (eq − 1) = Q/BT , BT is the upper bound for λ0
in the sieve). We will show E(eqX1) < Q under the above choice of r1 and r2. Then by recursion
it holds for all t. We use the result eq − 1 ≤ 2q for q < 1.

With λ1(X0) = µ(1) + a1(1)X0, we have

E(eqX1) = E(E(eqX1|X0)) = E(eλ1(X0)(eq−1)) = e(eq−1)µ(1)eλ0(e(eq −1)a1(1)−1)

Then choose sieve parameters such that Qa1/BT = a1(1)(eq − 1) ≤ 2a1(1)q is very small which
is ensured as q is very small. Then µ(1)Q/BT + λ0(eQa1(1)/BT − 1) ≈ Qµ(1)/BT + λ0( Qa1(1)

) ≤

BT

Q{µ(1)/BT + a1(1)} < Q as within the sieve µ(1)/BT + a1(1) < AT /BT + (1 − AT /BT ) = 1.

Hence, E(eqX1) < eQ. Recursively, for all t, we can show E(eqXt) < eQ.

Our primary goal of showing Eκ1(f /f1)2 < ∞ can be fulﬁlled if Q is a constant, independent
of T . To ensure Q is independent of T we need BT (eq − 1) is constant. It suﬃces to make qBT
√
ρT√
constant as qBT < BT (eq − 1) < 2qBT . Thus, for r2 ≤
T

and in the light of (8.11) r1 ≤

√
ρT√
T BT

we have Eκ1

bounded.

(cid:17)2

(cid:16) f
f1

The test function χT satisfying exponentially decaying Type I and Type II probabilities is

then obtained by taking maximum over all tests φjT ’s for each ball, having above radius. Thus
χT = maxj φjT . Type I and Type II probabilities are given by P0(χT ) ≤ (cid:80)
j P0φjT ≤ DT P0φjT
T ). Hence, we need to show that log DT (cid:46) T (cid:15)2
P (1 − χT ) ≤ exp(−T LT (cid:15)2
and supκ∈WT :r2
T ,
where DT is the required number of balls of above radius needed to cover our sieve WT . We have

T (κ,κ0)>LT (cid:15)2
T

log DT ≤ log D(r1, (cid:107)α(cid:107)∞ ≤ AT , min(α) > ρT , (cid:107) · (cid:107)∞) + log D(r2, (cid:107)γ(cid:107)∞ ≤ 1 −

≤ K1T log(3K1T AT /r1) + K2T log(3K2T /r2)

AT
BT

, min(γ) > ρT , (cid:107) · (cid:107)∞)

(8.12)

Given our choices of AT , BT and ρT , the two radii r1 and r2 are some fractional polynomials in T .

30

Thus log DT (cid:46) (K1T + K2T ) log T , which is required to be (cid:46) T (cid:15)2

T as in the prior mass condition

due to (i).

Based on (8.8), we have ¯K1T (cid:16) T 1/(2ι1+1)(log T )−1/(2ι1+1), K2T (cid:16) T 1/(2ι2+1)(log T )−1/(2ι2+1)
. The actual

T −ι1/(2ι1+1)(log T )ι1/(2ι1+1), T −ι2/(2ι2+1)(log T )ι2/(2ι2+1)

and a pre-rate ¯(cid:15)T = max

(cid:27)

(cid:26)

rate will be slower that pre-rate. Now, the covering number condition, prior mass conditions
and basis approximation result give us (K1T + K2T ) log T (cid:46) T (cid:15)2
2T }.
Combining all these conditions, we would require K1T (cid:16) T 1/(2ι1+1)(log T )2ι1/(2ι1+1)−b13, K2T (cid:16)
T 1/(2ι2+1)(log T )2ι2/(2ι2+1)−b23. Hence we calculate the posterior contraction rate as (cid:15)T equal to

T and (cid:15)T (cid:38) max{K −ι1

1T , K −ι2

(cid:26)

T −ι1/(2ι1+1)(log T )ι1/(2ι1+1)+(1−b13)/2, T −ι2/(2ι2+1)(log T )ι2/(2ι2+1)+(1−b23)/2

(cid:27)
.

max

8.2.3 Posterior contraction in terms of average Hellinger

We can write Reyni divergence as r2

r2
T

(cid:46) (cid:15)2

2,T (κ0, κ) (cid:46) (cid:15)2
T implies that d2
(cid:113) f1
(cid:16)
(cid:17)−1/T
T ≤ (cid:15)2
If r2
Eκ0
f0
T . By Cauchy-Squarz inequality (cid:0)(cid:82) √

T , we have

1 − (cid:15)2

T = − 1

T log (cid:82) √
T as (cid:15)T goes to zero.

≤ exp((cid:15)2

T ) which implies for small (cid:15)2
(cid:1)2 ≤ (cid:82) f0

(cid:82) f = 1. Thus we have,

T , we have

f0f1

(cid:16)

(cid:113) f1
f0

Eκ0

(cid:17)1/T

≥

f0f1 = − 1

T log Eκ0

(cid:113) f1
f0

. We need to show

(cid:32)

(cid:115)

1 − (cid:15)2

T ≤

Eκ0

(cid:33)1/T

f1
f0

≤ 1,

Since d2

H(f1, f0) = 2(1 − Eκ0

(cid:113) f1
f0

)

(cid:32)

(cid:115)

Eκ0

f1
f0

(cid:33)1/T

(cid:40)

(cid:32)

=

1 −

1 − Eκ0

(cid:33)(cid:41)1/T

(cid:115)

f1
f0

≈ 1 −

1
2T

d2
H(f1, f0).

Thus 1

T d2

H(f1, f0) (cid:46) (cid:15)2

T . Thus it is consistent under average Hellinger distance.

8.3 Proof of Theorem 2

The proof will follow similar path as in the previous section. Thus we just speciﬁcally touch upon

the parts that require diﬀerent treatment. We can rewrite history of the INGARCH process as

{Ft−1, Gt−1} = {Ft−1, λ0}. For the INGARCH case, the likelihood based on the parameter space
κ is diﬀerent from above and is given by, Pψ0(X0, λ0) (cid:81)T

t=1 Pψ(Xt|Ft−1, λ0). Since all the steps are

31

similar for the proof of Theorem 2, we only provide a outline. First to bound KL by the sup norm

distances among functions, we need to tackle |b11(t/T )λ1t − b01(t/T )λ0t|. For this term we have

|b11(t/T )λ1t − b01(t/T )λ0t| ≤ λ0t(cid:107)b11 − b01(cid:107)∞ + max

t

b11(t)|λ1t − λ0t|.

(8.13)

When ψ1 is near ψ0, we have for all t

|λ1t − λ0t| ≤ (cid:107)µ1 − µ0(cid:107)∞ + Xt−1(cid:107)a11 − a01(cid:107)∞ + (1 −

Mµ
MX

)|λ1,t−1 − λ0,t−1| + λ0,t−1|b11 − b01|∞

as we can upper bound maxt b11(t) by (1 − Mµ
MX

) since ψ1 is close to ψ0. We have

T −1
(cid:88)

t=1

Mµ
MX

|λ1t − λ0t| + |λ1T − λ0T |

≤ T (cid:107)µ1 − µ0(cid:107)∞ +

(cid:88)

t

As Mµ < MX,

Xt−1(cid:107)a11 − a01(cid:107)∞ + (1 −

Mµ
MX

)|λ10 − λ00| +

(cid:88)

t

λ0,t−1|b11 − b01|∞

T
(cid:88)

t=1

|λ1t − λ0t| ≤

MX
Mµ

{T (cid:107)µ1 − µ0(cid:107)∞ +

(cid:88)

t

Xt−1(cid:107)a11 − a01(cid:107)∞

+ (1 −

Mµ
MX

)|λ10 − λ00| +

(cid:88)

t

λ0,t−1|b11 − b01|∞}.

which implies,

E

T
(cid:88)

t=1

|λ1t − λ0t| ≤

MX
Mµ

{T (cid:107)µ1 − µ0(cid:107)∞ + T MX(cid:107)a11 − a01(cid:107)∞

+ (1 −

Mµ
MX

)|λ10 − λ00| + T MX|b11 − b01|∞}.

(8.14)

Using the deﬁnition of R as in (8.3), we have

|R| ≤

T
(cid:88)

(cid:20)

t=1

|λ1t − λ0t| +

Xt
µ∗(t/T ) + a1∗(t/T )Xt−1

(cid:21)
|λ1t − λ0t|

(8.15)

The ﬁrst part follows directly. For the second part as ψ1 and ψ0 are close

(cid:88)

(cid:18)

E

E

(cid:18) Xt
λ∗t

|λ1t − λ0t| | Ft

(cid:19)(cid:19)

(cid:88)

≤

t

MX
ρ

E(|λ1t − λ0t|) =

MX
ρ

(cid:88)

E(

t

|λ1t − λ0t|).

T ) can again be bounded by sup-norm diﬀerences in functions as before and |λ10 − λ00|

using (8.14). Next, we need to construct a sieve and construct tests. We consider similar sieve

WT = {K1, K2, K3α, γ1, γ2 : K1 ≤ K1T , K2 ≤ K2T , K3 ≤ K3T , (cid:107)α(cid:107)∞ ≤ AT , min(α, γ1, γ2) > ρT ,

max γ1 + max γ2 ≤ 1 − AT /BT , λ0 ≤ BT },

(8.16)

32

t
Thus E( R

as in the previous problem. Within the sieve, we have EEt−1(max(Xt, λt)) < BT . Here the extra

terms such as K3 stands for number of basis in b1(t) and the vectors γ1 and γ2 correspond to

the B-spline coeﬃcients of the functions a1(t) and b1(t) respectively. Also note that we now
have a lower bound for AT for technical need. We take ρT ≈ T −a with a < 1, AT = BT (1 −
exp(log T /T )ρT ), BT ≈ T a2 for suﬃciently large T such that exp(log T /T )ρT < 1. Within the

sieve again we use a variant of above inequality. Note that within the sieve E(Xt) ≤ BT and

E(λt) ≤ BT .

We have that,

|λ1t − λt| ≤ (cid:107)µ1 − µ(cid:107)∞ + Xt−1(cid:107)a11 − a1(cid:107)∞ + (1 −

AT
BT

)|λ1,t−1 − λt−1| + λt−1|b11 − b01|∞ (8.17)

and also,

|λt − λ1t|
λt

≤

1
ρT

By recursion,

(cid:107)µ − µ1(cid:107)∞ +

1
ρT

(cid:107)a1 − a11(cid:107)∞ +

1 − AT /BT
ρT

|λt−1 − λ1,t−1|
λt−1

+

1
ρT

(cid:107)b1 − b11(cid:107)∞

|λt − λ1t|
λt

≤

Gt

T − 1
(GT − 1)ρT

[(cid:107)µ − µ1(cid:107)∞ + (cid:107)a1 − a11(cid:107)∞ + (cid:107)b1 − b11(cid:107)∞] +

Gt−1
T
ρT

|λ0 − λ01|,

(8.18)

ρT

> 1. Since RHS is increasing in t and we only need to ﬁnd a bound for t = T .

where GT = 1−AT /BT
If AT , BT and ρT are chosen in such a way that GT (cid:16) exp(log T /T ), then GT

T (cid:16) T . Based on that
r1, r2, r3 and r4 can be chosen. For suﬃciently large T (> 1/a) we have (1 − exp(log T /T )ρT ) < 1.

(cid:17)2

(cid:16) f
f1

Let us assume that (cid:107)µ − µ1(cid:107)∞ = r1, (cid:107)a − a1(cid:107)∞ = r2, (cid:107)b − b1(cid:107)∞ = r3, |λ0 − λ01| = r4. Then for
ri ≤ ρT
≤ 1/T a3 for all t with a3 > 0. The choice of a3 is shown later.

T 1+a3 , we have that |λt−λ1t|

λt

Next goal is to ﬁnd the radii for which Eψ

is bounded. Similar steps as before ﬁrst give us

Eψ1

(cid:17)2

(cid:16) f
f1

≤ 1
T

(cid:80)

t=1 Eψ

(cid:16) f (Xt|Ft−1,λ0)
f1(Xt|Ft−1,λ0)

(cid:17)T

and then the following,

(cid:19)T

Eψ

≈ Eψ exp

(cid:18) f (Xt|Ft−1, λ0)
f1(Xt|Ft−1, λ0)

λt
T 2a3−1 .
(cid:3) as λt = Eψ(Xt|Ft−1, λ0). We
We have by Jensen’s inequality, Eψ exp (cid:2)
can again show by induction that within the sieve E(eqXt) < eQ for some constant Q following

T (λ1t − λt)(λ1t − λt)
λt

≤ Eψ exp(T 1−a3|λ1t −λt|) ≤ Eψ exp

(cid:3) ≤ Eψ exp (cid:2) Xt
T 2a3−1

λt
T 2a3−1

similar argument with q = T 1−2a3. We again need qBT independent of T . Hence our choice for a3

33

will be a3 = 1+a2

2 > 1/2. Thus q is small for suﬃciently large T and hence eq − 1 ≈ q. We have

from MGF of Poisson,

E(eqXt) = E(exp{λt(eq − 1))) ≈ E(exp(µ(t)q + a1(t)Xt−1q) + b1(t)λt−1q}

= E(Et−1(exp{µ(t)q + a1(t)Xt−1q})(exp{b1(t)Et−1(Xt−1)q}))

≤ E(Et−1(exp{µ(t)q + a1(t)Xt−1q})Et−1(exp{b1(t)Xt−1q}))

≤ E(Et−1(exp{µ(t)q + a1(t)Xt−1q + b1(t)Xt−1q}))

= E(exp{µ(t)q + (a1(t) + b1(t))Xt−1q}),

(8.19)

by ﬁrst Jensen’s inequality as λt = Eψ(Xt|Ft−1, λ0) and positive correlation between exp{a1(t)Xt−1q}

and exp{b1(t)Xt−1q} under the expectation Et−1. For two positively correlated random variables

Y and Z under the sample space, we have E(Y Z) > E(Y )E(Z). Now using this recurrence result

(8.19) of E(eqXt), we again arrive at similar type of bounds for r1 ≤
E(eqXt) < eQ for some constant Q for all t. We also need that r4 (cid:16) r1, r3 (cid:16) r2, where (cid:16) means

to ensure that

, r2 ≤

√
ρT√
T

√
ρT√
T BT

asymptotically equivalent. Finally we need r1 ≤ min{

ρT
T 1+a3 } and
r4 (cid:16) r1, r3 (cid:16) r2. These radii are also of polynomial order in T . Rest of the pieces of the proof

ρT
T 1+a3 } and r2 ≤ min{

,

,

√
ρT√
T BT

√
ρT√
T

follow similar arguments as before.

References

Ahmad, A., and Francq, C. (2016), “Poisson QMLE of count time series models,” Journal of Time

Series Analysis, 37(3), 291–314. 3

Amorim, L. D., Cai, J., Zeng, D., and Barreto, M. L. (2008), “Regression splines in the time-

dependent coeﬃcient rates model for recurrent event data,” Statistics in medicine, 27(28), 5890–

5906. 3

Besag, J. (1974), “Spatial interaction and the statistical analysis of lattice systems,” Journal of

the Royal Statistical Society. Series B (Methodological), pp. 192–236. 23

Biller, C., and Fahrmeir, L. (2001), “Bayesian varying-coeﬃcient models using adaptive regression

splines,” Statistical Modelling, 1(3), 195–211. 3

34

Biswas, A., and Song, P. X.-K. (2009), “Discrete-valued ARMA processes,” Statistics & probability

letters, 79(17), 1884–1889. 1

Brandt, P. T., and Williams, J. T. (2001), “A linear Poisson autoregressive model: The Poisson

AR (p) model,” Political Analysis, 9(2), 164–184. 1, 6

Cai, Z., Fan, J., and Yao, Q. (2000), “Functional-coeﬃcient regression models for nonlinear time

series,” Journal of the American Statistical Association, 95(451), 941–956. 3

Chan, K., and Ledolter, J. (1995), “Monte Carlo EM estimation for time series models involving

counts,” Journal of the American Statistical Association, 90(429), 242–252. 1

Dahlhaus, R. (2012), “Locally stationary processes,” , 30, 351–413. 8

Dahlhaus, R., and Subba Rao, S. (2006), “Statistical inference for time-varying ARCH processes,”

Ann. Statist., 34(3), 1075–1114. 7, 8, 9, 10

Dahlhaus, R. et al. (1997), “Fitting time series models to nonstationary processes,” Annals of

Statistics, 25(1), 1–37. 8

Dahlhaus, R. et al. (2000), “A likelihood approximation for locally stationary processes,” The

Annals of Statistics, 28(6), 1762–1794. 8

Davis, R. A., Dunsmuir, W. T., and Streett, S. B. (2003), “Observation-driven models for Poisson

counts,” Biometrika, 90(4), 777–790. 1

Davis, R. A., and Mikosch, T. (2009), “Extreme value theory for GARCH processes,” , pp. 187–

200. 5

DeYoreo, M., and Kottas, A. (2017), “A Bayesian nonparametric Markovian model for non-

stationary time series,” Statistics and Computing, 27(6), 1525–1538. 8, 10

Fan, J., and Zhang, W. (2008), “Statistical methods with varying coeﬃcient models,” Statistics

and its Interface, 1(1), 179. 3

Ferland, R., Latour, A., and Oraichi, D. (2006), “Integer-valued GARCH process,” Journal of

Time Series Analysis, 27(6), 923–942. 3, 5

35

Ferreira, G., Navarrete, J. P., Rodr´ıguez-Cort´es, F. J., and Mateu, J. (2017), “Estimation and pre-

diction of time-varying GARCH models through a state-space representation: a computational

approach,” Journal of Statistical Computation and Simulation, 87(12), 2430–2449. 5

Fokianos, K., Rahbek, A., and Tjøstheim, D. (2009), “Poisson autoregression,” Journal of the

American Statistical Association, 104(488), 1430–1439. 2

Fokianos, K., and Tjøstheim, D. (2011), “Log-linear Poisson autoregression,” Journal of Multi-

variate Analysis, 102(3), 563–578. 2

Franco-Villoria, M., Ventrucci, M., Rue, H. et al. (2019), “A uniﬁed view on Bayesian varying

coeﬃcient models,” Electronic Journal of Statistics, 13(2), 5334–5359. 3

Fryzlewicz, P., Sapatinas, T., Rao, S. S. et al. (2008), “Normalized least-squares estimation in

time-varying ARCH models,” Annals of Statistics, 36(2), 742–786. 11

Fryzlewicz, P., Sapatinas, T., and Subba Rao, S. (2008), “Normalized least-squares estimation in

time-varying ARCH models,” Ann. Statist., 36(2), 742–786. 7

Ghosal, S., Ghosh, J. K., Van Der Vaart, A. W. et al. (2000), “Convergence rates of posterior

distributions,” Annals of Statistics, 28(2), 500–531. 24

Ghosal, S., and Van der Vaart, A. (2017), “Fundamentals of nonparametric Bayesian inference,”

, 44. 3, 12, 13, 23, 25

Ghosal, S., Van Der Vaart, A. et al. (2007), “Convergence rates of posterior distributions for noniid

observations,” Annals of Statistics, 35(1), 192–223. 3

Gu, C., and Wahba, G. (1993), “Smoothing spline ANOVA with component-wise Bayesian “con-

ﬁdence intervals”,” Journal of Computational and Graphical Statistics, 2(1), 97–117. 3

Hadj-Amar, B., Rand, B. F., Fiecas, M., L´evi, F., and Huckstepp, R. (2020), “Bayesian Model

Search for Nonstationary Periodic Time Series,” Journal of the American Statistical Association,

115(531), 1320–1335. 9

Hastie, T., and Tibshirani, R. (1993), “Varying-coeﬃcient models,” Journal of the Royal Statistical

Society: Series B (Methodological), 55(4), 757–779. 3

36

Huang, J. Z., and Shen, H. (2004), “Functional coeﬃcient regression models for non-linear time

series: a polynomial spline approach,” Scandinavian journal of statistics, 31(4), 515–534. 3

Huang, J. Z., Wu, C. O., and Zhou, L. (2002), “Varying-coeﬃcient models and basis function

approximations for the analysis of repeated measurements,” Biometrika, 89(1), 111–128. 3

Jeong, S. et al. (2019), “Frequentist Properties of Bayesian Procedures for High-Dimensional

Sparse Regression.,” , . 3

Karmakar, S. (2018), Asymptotic Theory for Simultaneous Inference Under Dependence,, Techni-

cal report, University of Chicago. 4

Karmakar, S., Richter, S., and Wu, W. B. (2020+), “Simultaneous inference for time-varying

models,” In revision, https: // sayarkarmakar. github. io/ publications/ sayar1. pdf , .

2, 4, 7

Karmakar, S., and Wu, W. B. (2020), “Optimal Gaussian Approximation for Multiple Time

Series,” Statistica Sinica, 30(3), 1399–1417. 23

Lauer, S. A., Grantz, K. H., Bi, Q., Jones, F. K., Zheng, Q., Meredith, H. R., Azman, A. S., Reich,

N. G., and Lessler, J. (2020a), “The incubation period of coronavirus disease 2019 (COVID-

19) from publicly reported conﬁrmed cases: estimation and application,” Annals of internal

medicine, . 19

Lauer, S. A., Grantz, K. H., Bi, Q., Jones, F. K., Zheng, Q., Meredith, H. R., Azman, A. S.,

Reich, N. G., and Lessler, J. (2020b), “The Incubation Period of Coronavirus Disease 2019

(COVID-19) From Publicly Reported Conﬁrmed Cases: Estimation and Application,” Annals

of Internal Medicine, .

URL: https://doi.org/10.7326/M20-0504 2

Neal, R. M. et al. (2011), “MCMC using Hamiltonian dynamics,” Handbook of Markov Chain

Monte Carlo, 2(11), 2. 9

Ning, B., Jeong, S., Ghosal, S. et al. (2020), “Bayesian linear regression for multivariate responses

under group sparsity,” Bernoulli, 26(3), 2353–2382. 3, 13

37

Rohan, N., and Ramanathan, T. (2013), “Nonparametric estimation of a time-varying GARCH

model,” Journal of Nonparametric Statistics, 25(1), 33–52. 5, 9

Rosen, O., Stoﬀer, D. S., and Wood, S. (2009), “Local spectral analysis via a Bayesian mixture of

smoothing splines,” Journal of the American Statistical Association, 104(485), 249–262. 8

Rosen, O., Wood, S., and Stoﬀer, D. S. (2012), “AdaptSPEC: Adaptive spectral estimation for

nonstationary time series,” Journal of the American Statistical Association, 107(500), 1575–

1589. 8

Roy, A., and Dunson, D. B. (2019), “Nonparametric graphical model for counts,” arXiv preprint

arXiv:1901.00886, . 23

Roy, A., Ghosal, S., Choudhury, K. R. et al. (2018), “High Dimensional Single-Index Bayesian

Modeling of Brain Atrophy,” Bayesian Analysis, . 27

Roy, A., and Karmakar, S. (2020), “Bayesian semiparametric time varying model for count data

to study the spread of the COVID-19 cases,” arXiv preprint arXiv:2004.02281, . 20, 22

Shen, W., and Ghosal, S. (2015), “Adaptive Bayesian procedures using random series priors,”

Scandinavian Journal of Statistics, 42(4), 1194–1213. 7, 27

Silveira de Andrade, B., Andrade, M. G., and Ehlers, R. S. (2015), “Bayesian GARMA models

for count data,” Communications in Statistics: Case Studies, Data Analysis and Applications,

1(4), 192–205. 2

Truquet, L. et al. (2019), “Local stationarity and time-inhomogeneous Markov chains,” Annals of

Statistics, 47(4), 2023–2050. 8

Yang, E., Ravikumar, P. K., Allen, G. I., and Liu, Z. (2013), On Poisson graphical models,, in

Advances in Neural Information Processing Systems, pp. 1718–1726. 23

Yang, H.-C., and Bradley, J. R. (2020), “Bayesian inference for big spatial data using non-

stationary spectral simulation,” arXiv preprint arXiv:2001.06477, . 9

Yue, Y. R., Simpson, D., Lindgren, F., Rue, H. et al. (2014), “Bayesian adaptive smoothing splines

using stochastic diﬀerential equations,” Bayesian Analysis, 9(2), 397–424. 3

38

Zeger, S. L. (1988), “A regression model for time series of counts,” Biometrika, 75(4), 621–629. 1,

6

Zhu, F. (2011), “A negative binomial integer-valued GARCH model,” Journal of Time Series

Analysis, 32(1), 54–67. 2, 3

Zhu, F. (2012a), “Modeling overdispersed or underdispersed count data with generalized Pois-

son integer-valued GARCH models,” Journal of Mathematical Analysis and Applications,

389(1), 58–71. 2, 3

Zhu, F. (2012b), “Modeling time series of counts with COM-Poisson INGARCH models,” Mathe-

matical and Computer Modelling, 56(9-10), 191–203. 2, 3

Zhu, F. (2012c), “Zero-inﬂated Poisson and negative binomial integer-valued GARCH models,”

Journal of Statistical Planning and Inference, 142(4), 826–839. 2, 3

39

