9
1
0
2

v
o
N
8

]
T
S
.
h
t
a
m

[

2
v
4
0
2
1
1
.
0
1
8
1
:
v
i
X
r
a

Sample covariances of random-coeﬃcient AR(1) panel model

Remigijus Leipus1, Anne Philippe2, Vytaut˙e Pilipauskait˙e3, Donatas Surgailis1

November 11, 2019

1Vilnius University, Faculty of Mathematics and Informatics, Naugarduko 24, 03225 Vilnius, Lithuania
2Universit´e de Nantes, Laboratoire de Math´ematiques Jean Leray, 44322 Nantes Cedex 3, France
3Aarhus University, Department of Mathematics, Ny Munkegade 118, 8000 Aarhus C, Denmark

Abstract

The present paper obtains a complete description of the limit distributions of sample covariances in
N × n panel data when N and n jointly increase, possibly at diﬀerent rate. The panel is formed by N
independent samples of length n from random-coeﬃcient AR(1) process with the tail distribution function
of the random coeﬃcient regularly varying at the unit root with exponent β > 0. We show that for
β ∈ (0, 2) the sample covariances may display a variety of stable and non-stable limit behaviors with
stability parameter depending on β and the mutual increase rate of N and n.

Keywords: Autoregressive model; Panel data; Mixture distribution; Long memory; Sample covariance;

Scaling transition; Poisson random measure; Asymptotic self-similarity.

2010 MSC: 60F05, 62M10.

1 Introduction

Dynamic panels providing information on a large population of heterogeneous individuals such as households,
ﬁrms, etc. observed at regular time periods, are often described by simple autoregressive models with random
parameters near unity. One of the simplest models for individual evolution is the random-coeﬃcient AR(1)
(RCAR(1)) process

X(t) = aX(t − 1) + ε(t),

t ∈ Z,

(1.1)

with standardized i.i.d. innovations {ε(t), t ∈ Z} and a random autoregressive coeﬃcient a ∈ [0, 1) indepen-
dent of {ε(t), t ∈ Z}. Granger [10] observed that in the case when the distribution of a is suﬃciently dense
near unity the stationary solution of RCAR(1) equation in (1.1) may have long memory in the sense that the
sum of its lagged covariances diverges. To be more speciﬁc, assume that the random coeﬃcient a ∈ [0, 1) has
a density function of the following form

φ(x) = ψ(x)(1 − x)β−1,

x ∈ [0, 1),

(1.2)

where β > 0 and ψ(x), x ∈ [0, 1) is a bounded function with limx↑1 ψ(x) =: ψ(1) > 0. Then for β > 1 the
covariance function of stationary solution of RCAR(1) equation in (1.1) with standardized ﬁnite variance
innovations decays as t−(β−1), viz.,

γ(t) := EX(0)X(t) = E

a|t|
1 − a2 ∼ (ψ(1)/2)Γ(β − 1)t−(β−1),

t → ∞,

(1.3)

1

 
 
 
 
 
 
implying (cid:80)
t∈Z | Cov(X(0), X(t))| = ∞ for β ∈ (1, 2]. The same long memory property applies to the
contemporaneous aggregate of N independent individual evolutions {Xi(t)}, i = 1, . . . , N of (1.1) and the
limit Gaussian aggregated process arising when N → ∞. Various properties of the RCAR(1) and more general
RCAR equations were studied in Gon¸calves and Gouri´eroux [9], Zaﬀaroni [32], Celov et al. [3], Oppenheim
and Viano [18], Puplinskait˙e and Surgailis [25], Philippe et al. [19] and other works, see Leipus et al. [14] for
review.

Statistical inference in the RCAR(1) model was discussed in several works. Leipus et al.

[13], Celov et
[4] discussed nonparametric estimation of the mixing density φ(x) using empirical covariances of the
al.
limit aggregated process. For panel RCAR(1) data, Robinson [29] and Beran et al. [1] discussed parametric
estimation of the mixing density.
In nonparametric context, Leipus et al. [15] studied estimation of the
empirical d.f. of a from panel RCAR(1) observations and derived its asymptotic properties as N, n → ∞,
while [16] discussed estimation of β in (1.2) and testing for long memory in the above panel model. For a
N × n panel comprising N samples {Xi(t), t = 1, . . . , n} of length n, i = 1, . . . , N of independent RCAR(1)
processes in (1.1) with mixing distribution in (1.2), Pilipauskait˙e and Surgailis [20] studied the asymptotic
distribution of the sample mean

¯XN,n

:=

1
N n

N
(cid:88)

n
(cid:88)

i=1

t=1

Xi(t)

(1.4)

as N, n → ∞, possibly at a diﬀerent rate.
[20] showed that for 0 < β < 2 the limit distribution of this
statistic depends on whether N/nβ → ∞ or N/nβ → 0 in which cases ¯XN,n is asymptotically stable with
stability parameter depending on β and taking values in the interval (0, 2]. See Table 2 below. As shown in
[20], under the ‘intermediate’ scaling N/nβ → c ∈ (0, ∞) the limit distribution of ¯XN,n is more complicated
and is given by a stochastic integral with respect to a certain Poisson random measure.

The present paper discusses asymptotic distribution of sample covariances (covariance estimates)

(cid:98)γN,n(t, s)

:=

1
N n

(cid:88)

(cid:88)

(Xi(k) − ¯XN,n)(Xi+s(k + t) − ¯XN,n),

(t, s) ∈ Z2,

(1.5)

1≤i,i+s≤N

1≤k,k+t≤n

computed from a similar RCAR(1) panel {Xi(t), t = 1, . . . , n, i = 1, . . . , N } as in [20], as N, n jointly increase,
possibly at a diﬀerent rate, and the lag (t, s) ∈ Z2 is ﬁxed, albeit arbitrary. Particularly, for (t, s) = (0, 0),
(1.5) agrees with the sample variance:

(cid:98)γN,n(0, 0) =

1
N n

N
(cid:88)

n
(cid:88)

i=1

k=1

(Xi(k) − ¯XN,n)2.

(1.6)

The true covariance function γ(t, s) := EXi(k)Xi+s(k + t) of the RCAR(1) panel model with mixing density
in (1.2) exists when β > 1 and is given by

γ(t, s) =




γ(t),



0,

s = 0,

s (cid:54)= 0,

(1.7)

where γ(t) deﬁned in (1.3). Note that γ(t) cannot be recovered from a single realization of the nonergodic
RCAR(1) process {X(t)} in (1.1). However, the covariance function in (1.7) can be consistently estimated
from the RCAR(1) N × n panel when N, n → ∞, together with rates. The limit distribution of the sample
covariance may exist even for 0 < β < 1 when the covariance itself is undeﬁned. As it turns out, the limit

2

distribution of (cid:98)γN,n(t, s) depends on the mutual increase rate of N and n, and is also diﬀerent for temporal,
or iso-sectional lags (s = 0) and cross-sectional lags (s (cid:54)= 0). The distinctions between the cases s = 0 and
s (cid:54)= 0 are due to the fact that, in the latter case, the statistic in (1.5) involves products Xi(k)Xi+s(k + t)
of independent processes Xi and Xi+s, whereas in the former case, Xi(k) and Xi(k + t) are dependent r.v.s.
The main results of this paper are summarized in Table 1 below. Rigorous formulations are given in Sections
3 and 4. For better comparison, Table 2 presents the results of [20] about the sample mean in (1.4) for the
same panel model.

Mutual increase rate of N, n Parameter region Limit distribution
N/nβ → ∞
N/nβ → 0
N/nβ → c ∈ (0, ∞)
Arbitrary

0 < β < 2, β (cid:54)= 1

0 < β < 2, β (cid:54)= 1

0 < β < 2, β (cid:54)= 1

Gaussian

β > 2

asymmetric β-stable

asymmetric β-stable

‘intermediate Poisson’

a) temporal lags (s = 0)

Mutual increase rate of N, n Parameter region Limit distribution
N/n2β → ∞

1 < β < 3/2
1/2 < β < 1

Gaussian
symmetric (2β)-stable

N/n2β → 0
N/n2β → c ∈ (0, ∞)
Arbitrary

3/4 < β < 3/2

symmetric (4β/3)-stable

3/4 < β < 3/2

‘intermediate Poisson’

β > 3/2

Gaussian

b) cross-sectional lags (s (cid:54)= 0)

Table 1: Limit distribution of sample covariances (cid:98)γN,n(t, s) in (1.5)

Mutual increase rate of N, n Parameter region Limit distribution
N/nβ → ∞

1 < β < 2
0 < β < 1

Gaussian
symmetric (2β)-stable

N/nβ → 0
N/nβ → c ∈ (0, ∞)
Arbitrary

0 < β < 2

0 < β < 2

β > 2

symmetric β-stable

‘intermediate Poisson’

Gaussian

Table 2: Limit distribution of the sample mean ¯XN,n in (1.4)

Remark 1.1. (i) β-stable limits in Table 1 a) arising when N/nβ → 0 and N/nβ → ∞ have diﬀerent scale
parameters and hence the limit distribution of temporal sample covariances is diﬀerent in the two cases.

(ii) ‘Intermediate Poisson’ limits in Tables 1–2 refer to inﬁnitely divisible distributions deﬁned through certain
stochastic integrals w.r.t. Poisson random measure. A similar terminology was used in [22].

(iii) It follows from our results (see Theorem 4.1 below) that a scaling transition similar as in the case of
the sample mean [20] arises in the interval 0 < β < 2 for temporal sample covariances and product random

3

ﬁelds Xv(u)Xv(u + t), (u, v) ∈ Z2 involving temporal lags, with the critical rate N ∼ nβ separating regimes
with diﬀerent limit distributions. For ‘cross-sectional’ product ﬁelds Xv(u)Xv+s(u + t), (u, v) ∈ Z2, s (cid:54)= 0
involving cross-sectional lags, a similar scaling transition occurs in the interval 0 < β < 3/2 with the critical
rate N ∼ n2β between diﬀerent scaling regimes, see Theorem 3.1. The notion of scaling transition for long-
range dependent random ﬁelds in Z2 was discussed in Puplinskait˙e and Surgailis [26], [27], Pilipauskait˙e and
Surgailis [22], [23].

(iv) The limit distributions of cross-sectional sample covariances in the missing intervals 0 < β < 1/2 and
0 < β < 3/4 of Table 1 b) are given in Corollary 3.1 below. They are more complicated and not included
in Table 1 b) since the term N n( ¯XN,n)2 due to the centering by the sample mean in (1.5) may play the
dominating role.

(v) We expect that the asymptotic distribution of sample covariances in the RCAR(1) panel model with
common innovations (see [21]) can be analyzed in a similar fashion. Due to the diﬀerences between the two
models (the common and the idiosyncratic innovation cases), the asymptotic behavior of sample covariances
might be quite diﬀerent in these two cases.

(vi) The results in Table 1 a) are obtained under the ﬁnite 4th moment conditions on the innovations, see
Theorems 4.1 and 4.2 below. Although the last condition does not guarantee the existence of the 4th moment
of the RCAR(1) process, it is crucial for the limit results, including the CLT in the case β > 2. Scaling
transition for sample variances of long-range dependent Gaussian and linear random ﬁelds on Z2 with ﬁnite
4th moment was established in Pilipauskait˙e and Surgailis [23]. On the other side, Surgailis [31], Horv´ath
and Kokoszka [12] obtained stable limits of sample variances and autocovariances for long memory moving
averages with ﬁnite 2nd moment and inﬁnite 4th moment. Finally, we mention the important works of Davis
and Resnick [6] and Davis and Mikosch [5] on limit theory for sample covariance and correlation functions of
moving averages and some nonlinear processes with inﬁnite variance, respectively.

The rest of the paper is organized as follows. Section 2 presents some preliminary facts, including the
deﬁnition and properties of the intermediate processes appearing in Table 1. Section 3 contains rigorous
formulations and the proofs of the asymptotic results for cross-sectional sample covariances (1.5), s (cid:54)= 0 and
the corresponding partial sums processes. Analogous results for temporal sample covariances and partial sums
processes are presented in Section 4. Section 4 also contains some applications of these results to estimation
of the autocovariance function γ(t) in (1.3) from panel data. Some auxiliary proofs are given in Appendix.

2 Preliminaries

This section contains some preliminary facts which will be used in the following sections.

2.1. Double stochastic integrals and quadratic forms. Let Bi, i = 1, 2 be independent standard
Brownian motions (BMs) on the real line. Let

Ii(f ) :=

(cid:90)

R

f (s)dBi(s),

Iij(g) :=

(cid:90)

R2

g(s1, s2)dBi(s1)dBj(s2),

i, j = 1, 2,

(2.1)

denote Itˆo-Wiener stochastic integrals (single and double) w.r.t. Bi, Bj. The integrals in (2.1) are jointly

4

deﬁned for any (non-random) integrands f ∈ L2(R), g ∈ L2(R2); moreover, EIi(f ) = EIij(g) = 0 and

EIi(f )Ii(cid:48)(f (cid:48)) =




0,

i (cid:54)= i(cid:48),



(cid:104)f, f (cid:48)(cid:105),

i = i(cid:48),

f, f (cid:48) ∈ L2(R),

(2.2)

EIi(f )Ii(cid:48)j(cid:48)(g) = 0,

0,

(cid:104)g, g(cid:48)(cid:105),

2(cid:104)g, symg(cid:48)(cid:105),

EIij(g)Ii(cid:48)j(cid:48)(g(cid:48)) =

∀i, i(cid:48), j(cid:48),

f ∈ L2(R), g ∈ L2(R2),

(i, j) /∈ {(i(cid:48), j(cid:48)), (j(cid:48), i(cid:48))},

(i, j) ∈ {(i(cid:48), j(cid:48)), (j(cid:48), i(cid:48))}, i (cid:54)= j,

g, g(cid:48) ∈ L2(R2),

i = i(cid:48) = j = j(cid:48),

R f (s)f (cid:48)(s)ds ((cid:107)f (cid:107) := (cid:112)(cid:104)f, f (cid:105)), (cid:104)g, g(cid:48)(cid:105) = (cid:82)

R2 g(s1, s2)g(cid:48)(s1, s2)ds1ds2 ((cid:107)g(cid:107) := (cid:112)(cid:104)g, g(cid:105)) denote
where (cid:104)f, f (cid:48)(cid:105) = (cid:82)
scalar products (norms) in L2(R) and L2(R2), respectively, and sym denotes the symmetrization, see, e.g.,
([7], sec. 11.5, 14.3). Note that for g(s1, s2) = f1(s1)f2(s2), fi ∈ L2(R), i = 1, 2 we have Iii(g) = Ii(f1)Ii(f2) −
(cid:104)f1, f2(cid:105), I12(g) = I1(f1)I2(f2), in particular, I12(g) =d (cid:107)f1(cid:107)(cid:107)f2(cid:107)Z1Z2, where Zi ∼ N (0, 1), i = 1, 2 are
independent standard normal r.v.s.

Let {εi(s), s ∈ Z}, i = 1, 2 be independent sequences of standardized i.i.d. r.v.s, Eεi(s) = 0, Eεi(s)εi(cid:48)(s(cid:48)) = 1
if (i, s) = (i(cid:48), s(cid:48)), Eεi(s)εi(cid:48)(s(cid:48)) = 0 if (i, s) (cid:54)= (i(cid:48), s(cid:48)), i, i(cid:48) = 1, 2, s, s(cid:48) ∈ Z. Consider the centered quadratic form

Qij(h) =

(cid:88)

s1,s2∈Z

h(s1, s2)[εi(s1)εj(s2) − Eεi(s1)εj(s2)],

i, j = 1, 2,

(2.3)

where h ∈ L2(Z2). For i = j we additionally assume Eε4

i (0) < ∞. Then the sum in (2.3) converges in L2 and

var(Qij(h)) ≤ (1 + Eε4

i (0)δij)

(cid:88)

s1,s2∈Z

h2(s1, s2),

(2.4)

see ([7], (4.5.4)). With any h ∈ L2(Z2) and any α1, α2 > 0 we associate its extension to L2(R2), namely,

(cid:101)h(α1,α2)(s1, s2) := (α1α2)1/2h((cid:98)α1s1(cid:99), (cid:98)α2s2(cid:99)),

(s1, s2) ∈ R2,

(2.5)

with (cid:107)(cid:101)h(α1,α2)(cid:107)2 = (cid:80)
of quadratic forms in (2.3) towards double stochastic integrals (2.1).

s1,s2∈Z h2(s1, s2). We shall use the following criterion for the convergence in distribution

Proposition 2.1. ([7], Proposition 11.5.5) Let i, j = 1, 2 and Qij(hα1,α2), α1, α2 > 0 be a family of quadratic
forms as in (2.3) with coeﬃcients hα1,α2 ∈ L2(Z2). For i = j we additionally assume Eε4
i (0) < ∞. Suppose
for some g ∈ L2(R2) we have that

lim
α1,α2→∞

(cid:107)(cid:101)h(α1,α2)

α1,α2 − g(cid:107) = 0.

(2.6)

Then Qij(hα1,α2) →d Iij(g) (α1, α2 → ∞), where Iij(g) is deﬁned as in (2.1).

2.2. The ‘cross-sectional’ intermediate process. Let dMβ ≡ Mβ(dx1, dx2, dB1, dB2) denote
Poisson random measure on (R+ × C(R))2 with mean

dµβ ≡ µβ(dx1, dx2, dB1, dB2) := ψ(1)2(x1x2)β−1dx1dx2PB(dB1)PB(dB2),

(2.7)

where β > 0 is parameter and PB is the Wiener measure on C(R). Let d (cid:102)Mβ := dMβ − dµβ be the centered
Poisson random measure. We shall often use ﬁniteness of the following integrals:

(cid:82)

R2
+
(cid:82)

min (cid:8)1,

min (cid:8)1,

R2
+

1
x1x2(x1+x2)
1
x1+x2

(cid:9)(x1x2)β−1dx1dx2 < ∞,
(cid:9)(x1x2)β−2dx1dx2 < ∞,

∀ 0 < β < 3/2,

∀ 1 < β < 3/2,

(2.8)

(2.9)

5

see Appendix. Let

Yi(u; x) =

(cid:90) u

−∞

e−x(u−s)dBi(s),

u ∈ R, x > 0,

(2.10)

be a family of stationary Ornstein-Uhlenbeck (O-U) processes subordinated to Bi = {Bi(s), s ∈ R}, Bi, i =
1, 2 being independent BMs. Let

z(τ ; x1, x2)

:=

(cid:90) τ

2
(cid:89)

0

i=1

Yi(u; xi)du,

τ ≥ 0,

(2.11)

be a family of integrated products of independent O-U processes indexed by x1, x2 > 0. We use the repre-
sentation of (2.11)

z(τ ; x1, x2) =

(cid:90)

(cid:90) τ

2
(cid:89)

(cid:8)

R2

0

i=1

e−xi(u−si)1(u > si)du(cid:9)dB1(s1)dB2(s2)

(2.12)

as the double Itˆo-Wiener integral in (2.1). The ‘cross-sectional’ intermediate process Zβ is deﬁned as stochastic
integral w.r.t. the Poisson measure Mβ, viz.,

Zβ(τ )

:=

(cid:90)

L1

z(τ ; x1, x2)dMβ +

(cid:90)

Lc
1

z(τ ; x1, x2)d (cid:102)Mβ,

(2.13)

where

L1 := {(x1, x2, B1, B2) ∈ (R+ × C(R))2 : x1x2(x1 + x2) ≤ 1},

1 := (R+ × C(R))2 \ L1
Lc

(2.14)

and µβ(L1) < ∞. For 1/2 < β < 3/2 the two integrals in (2.13) can be combined in a single one:

Zβ(τ ) =

(cid:90)

(R+×C(R))2

z(τ ; x1, x2)d (cid:102)Mβ.

(2.15)

These and other properties of Zβ are stated in the following proposition whose proof is given in the Appendix.
We also refer to [28] and [20] for general properties of stochastic integrals w.r.t. Poisson random measure.

Proposition 2.2. (i) The process Zβ in (2.13) is well-deﬁned for any 0 < β < 3/2.
increments, inﬁnitely divisible ﬁnite-dimensional distributions, and the joint ch.f. given by

It has stationary

E exp (cid:8)i

m
(cid:88)

j=1

θjZβ(τj)(cid:9) = exp

(cid:110) (cid:90)

(R+×C(R))2

(cid:0)ei (cid:80)m

j=1 θj z(τj ;x1,x2) − 1(cid:1)dµβ

(cid:111)
,

(2.16)

where θj ∈ R, τj ≥ 0, j = 1, . . . , m, m ∈ N. Moreover, the distribution of Zβ is symmetric: {Zβ(τ ), τ ≥
0} =fdd {−Zβ(τ ), τ ≥ 0}.
(ii) E|Zβ(τ )|p < ∞ for p < 2β and EZβ(τ ) = 0 for 1/2 < β < 3/2.
(iii) For 1/2 < β < 3/2, Zβ can be deﬁned as in (2.15). Moreover, if 1 < β < 3/2, then EZ 2

β(τ ) < ∞ and

EZβ(τ1)Zβ(τ2) = (σ2

∞/2)(cid:0)τ 2(2−β)

1

+ τ 2(2−β)
2

− |τ2 − τ1|2(2−β)(cid:1),

τ1, τ2 ≥ 0,

(2.17)

where σ2

∞ := ψ(1)2Γ(β − 1)2/(4(2 − β)(3 − 2β)).

(iv) For 1/2 < β < 3/2, the process Zβ has a.s. continuous trajectories.

6

(v) (Asymptotic self-similarity) As b → 0,

bβ−2Zβ(bτ ) →fdd σ∞B2−β(τ ),

if 1 < β < 3/2,

b−1(log b−1)−1/(2β)Zβ(bτ ) →fdd τ V2β,

if 0 < β < 1,

(2.18)

(2.19)

where {B2−β(τ ), τ ≥ 0} is a fractional Brownian motion with E[B2−β(τ )]2 = τ 2(2−β), τ ≥ 0, 2 − β ∈ (1/2, 1),
∞ is given in (2.17), and V2β is a symmetric (2β)-stable r.v. with ch.f. EeiθV2β = e−c∞|θ|2β , θ ∈ R, c∞ :=
σ2
ψ(1)221−2βΓ(β + (1/2))Γ(1 − β)/
π. For any 0 < β < 3/2, as b → ∞,

√

b−1/2Zβ(bτ ) →fdd A1/2B(τ ),

(2.20)

where A > 0 is a (2β/3)-stable r.v. with Laplace transform Ee−θA = exp{−σ0θ2β/3}, θ ≥ 0, σ0 :=
ψ(1)22−2β/3Γ(1 − (2β/3)) B(β/3, β/3)/(2β), and {B(τ ), τ ≥ 0} is a standard BM, independent of A. Finite-
dimensional distributions of the limit process in (2.20) are symmetric (4β/3)-stable.

2.3. The ‘iso-sectional’ intermediate process. Let dM∗
measure on R+ × C(R) with mean

β ≡ M∗

β(dx, dB) denote Poisson random

dµ∗

β ≡ µ∗

β(dx, dB) := ψ(1)xβ−1dxPB(dB),

(2.21)

where 0 < β < 2 is parameter and PB is the Wiener measure on C(R). Let d (cid:102)M∗
centered Poisson random measure. Let Y(·; x) ≡ Y1(·; x) be the family of O-U processes as in (2.10), and

β := dM∗

β − dµ∗

β be the

z∗(τ ; x)

:=

(cid:90) τ

0

Y 2(u; x)du,

τ ≥ 0, x > 0,

(2.22)

be integrated squared O-U processes. Note Ez∗(τ ; x) = τ EY 2(0; x) = τ (cid:82) 0
the representation

−∞ e2xsds = τ /(2x). We will use

z∗(τ ; x) =

(cid:90)

(cid:90) τ

2
(cid:89)

(cid:8)

R2

0

i=1

e−x(u−si)1(u > si)du(cid:9)dB(s1)dB(s2) + τ /(2x)

(2.23)

as the double Itˆo-Wiener integral. The ‘iso-sectional’ intermediate process Z ∗
as stochastic integral w.r.t. the above Poisson measure, viz.,

β is deﬁned for β ∈ (0, 2), β (cid:54)= 1

Z ∗

β(τ )

:=

(cid:90)

R+×C(R)

z∗(τ ; x)






dM∗
d (cid:102)M∗

β, 0 < β < 1,
β, 1 < β < 2,

τ ≥ 0.

(2.24)

Proposition 2.3 stating properties of Z ∗

β is similar to Proposition 2.2.

Proposition 2.3. (i) The process Z ∗
increments, inﬁnitely divisible ﬁnite-dimensional distributions, and the joint ch.f. given by

β in (2.24) is well-deﬁned for any 0 < β < 2, β (cid:54)= 1. It has stationary

E exp (cid:8)i (cid:80)m

j=1 θjZ ∗

β(τj)(cid:9) = exp (cid:8) (cid:82)

R+×C(R)

(cid:0)ei (cid:80)m

j=1 θj z∗(τj ;x) − 1 − i (cid:80)m

j=1 θjz∗(τj; x)1(1 < β < 2)(cid:1)dµ∗

β

(cid:9), (2.25)

where θj ∈ R, τj ≥ 0, j = 1, . . . , m, m ∈ N.
(ii) E|Z ∗

β(τ )|p < ∞ for any 0 < p < β < 2, β (cid:54)= 1 and EZ ∗

β(τ ) = 0 for 1 < β < 2.

(iii) For 1 < β < 2, the process Z ∗

β has a.s. continuous trajectories.

7

(iv) (Asymptotic self-similarity) For any 0 < β < 2, β (cid:54)= 1,

b−1Z ∗

β(bτ ) →fdd






τ V ∗
β
τ V +
β

as b → 0,

as b → ∞,

(2.26)

β , V ∗

where V +
i(θ/(2x))1(1 < β < 2))xβ−1dx}, EeiθV ∗
θ ∈ R and Z ∼ N (0, 1).

β are a completely asymmetric β-stable r.v.s with ch.f.s EeiθV +
β = exp{ψ(1) (cid:82) ∞

0 (eiθ/(2x) − 1 −
0 E(eiθZ2/(2x) − 1 − i(θZ2/(2x))1(1 < β < 2))xβ−1dx},

β = exp{ψ(1) (cid:82) ∞

2.4. Conditional long-run variance of products of RCAR(1) processes. We use some facts
in Proposition 2.4, below, about conditional variance of the partial sums process of the product Yij(t) :=
Xi(t)Xj(t) of two RCAR(1) processes. Split Yij(t) = Y +
1(t ≥
ij (t) = (cid:80)
s1 ∨ s2)εi(s1)εj(s2), Y −
1(t ≥ s1 ∨ s2)εi(s1)εj(s2). For i = j we assume additionally
that Eε4
i (0) < ∞.

ij (t), where Y +

s1∧s2≥1 at−s1

s1∧s2≤0 at−s1

ij (t) = (cid:80)

ij (t)+Y −

at−s2
j

at−s2
j

i

i

Proposition 2.4. We have

var(cid:2)

n
(cid:88)

t=1

Yij(t)|ai, aj

(cid:3) ∼ var(cid:2)

n
(cid:88)

t=1

Y +
ij (t)|ai, aj

(cid:3) ∼ Aijn,

n → ∞,

(2.27)

where

(1−a2
1+a2
i
1−a2
i
with cum4 being the 4th cumulant of εi(0). Moreover, for any n ≥ 1, i, j ∈ Z, ai, aj ∈ [0, 1)

i )(1−a2
2
(
(1−a2

Aij :=

i = j

1−a4
i



),

1+aiaj

j )(1−aiaj ) ,
i )2 + cum4

i (cid:54)= j,




var(cid:2)

n
(cid:88)

t=1

Yij(t)|ai, aj

(cid:3) ≤

Cijn2
(1 − ai)(1 − aj)

min (cid:8)1,

1
n(2 − ai − aj)

(cid:9),

(2.28)

(2.29)

where Cij := 4 (i (cid:54)= j), := 2(2 + |cum4|) (i = j).

Proof. Let i (cid:54)= j. We have E[Yij(t)Yij(s)|ai, aj] = E[Xi(t)Xi(s)|ai]E[Xj(t)Xj(s)|aj] = (aiaj)|t−s|/(1 − a2
a2
j ) and hence

i )(1 −

Jn(ai, aj) := E(cid:2)(cid:0)

n
(cid:88)

t=1

Yij(t)(cid:1)2|ai, aj

(cid:3) =

n
i )(1 − a2
j )

(1 − a2

n
(cid:88)

(aiaj)|t|(cid:0)1 −

t=−n

(cid:1).

|t|
n

(2.30)

Relation (2.30) implies (2.27).
(1/2)((1 − ai) + (1 − aj)). Hence and from (2.30) we obtain

It also implies Jn(ai, aj) ≤ 2n2/((1 − ai)(1 − aj)). Note also 1 − aiaj ≥

Jn(ai, aj) ≤

n
i )(1 − a2
j )

(1 − a2

(cid:0)1 + 2

∞
(cid:88)

t=1

(aiaj)t(cid:1) ≤

2n
(1 − ai)(1 − aj)(1 − aiaj)

≤

4n
(1 − ai)(1 − aj)(2 − ai − aj)

,

proving (2.29). The proof of (2.27)–(2.29) for i = j is similar using cov[Yii(t), Yii(s)|ai] = 2(a|t−s|
cum4a2|t−s|
i

/(1 − a4

i ).

i

/(1 − a2

i ))2 +
(cid:3)

8

3 Asymptotic distribution of cross-sectional sample covariances

Theorems 3.1 and 3.2 discuss the asymptotic distribution of partial sums process

St,s
N,n(τ ) :=

N
(cid:88)

(cid:98)nτ (cid:99)
(cid:88)

i=1

u=1

Xi(u)Xi+s(u + t),

τ ≥ 0,

(3.1)

where t and s ∈ Z, s (cid:54)= 0 are ﬁxed and N and n tend to inﬁnity, possibly at a diﬀerent rate. The asymptotic
behavior of sample covariances (cid:98)γN,n(t, s) is discussed in Corollary 3.1. As it turns out, these limit distributions
do not depend on t, s which is due to the fact that the sectional processes {Xi(t), t ∈ Z}, i ∈ Z are independent
and stationary.

Theorem 3.1. Let the mixing distribution satisfy condition (1.2) with 0 < β < 3/2. Let N, n → ∞ so as

Then the following statements (i)–(iii) hold for St,s

λN,n :=

→ λ∞ ∈ [0, ∞].

N 1/(2β)
n
N,n(τ ), (t, s) ∈ Z2, s (cid:54)= 0 in (3.1) depending on λ∞ in (3.2).

(3.2)

(i) Let λ∞ = ∞. Then

N,nSt,s

n−2λ−β
N,n(τ ) →fdd
N,n(log λN,n)−1/(2β)St,s

n−2λ−1

σ∞B2−β(τ ),

1 < β < 3/2,

N,n(τ ) →fdd τ V2β,

0 < β < 1,

where the limit processes are the same as in (2.18), (2.19).

(ii) Let λ∞ = 0 and E|ε(0)|2p < ∞ for some p > 1. Then

n−2λ−3/2

N,n St,s

N,n(τ ) →fdd A1/2B(τ ),

where the limit process is the same as in (2.20).

(iii) Let 0 < λ∞ < ∞. Then

n−2λ−3/2

N,n St,s

N,n(τ ) →fdd λ1/2

∞ Zβ(τ /λ∞),

(3.3)

(3.4)

(3.5)

(3.6)

where Zβ is the intermediate process in (2.13).

Theorem 3.2. Let the mixing distribution satisfy condition (1.2) with β > 3/2 and assume E|ε(0)|2p < ∞
for some p > 1. Then for any (t, s) ∈ Z2, s (cid:54)= 0 as N, n → ∞ in arbitrary way,

n−1/2N −1/2St,s

N,n(τ ) →fdd σB(τ ),

σ2 := EA12,

(3.7)

where A12 is deﬁned in (2.28).

Remark 3.1. Our proof of Theorem 3.1 (ii) requires establishing the asymptotic normality of a bilinear form
in i.i.d. r.v.s, which has a non-zero diagonal, see the r.h.s. of (3.52). For this purpose, we use the martingale
CLT and impose an additional condition of E|ε(0)|2p < ∞, p > 1. To establish the CLT for quadratic forms
with non-zero diagonal, [2] took similar approach and also needed 2p ﬁnite moments.

In Theorem 3.2 we also assume E|ε(0)|2p < ∞, p > 1. However, it can be proved under Eε2(0) < ∞
applying another technique that is approximation by m-dependent r.v.s. Moreover, this result holds if (1.2)
is replaced by EA12 < ∞.

9

Note that the asymptotic distribution of sample covariances (cid:98)γN,n(t, s) in (1.5) coincides with that of the

statistics

(cid:101)γN,n(t, s) := (N n)−1St,s

N,n(1) − ( ¯XN,n)2.

(3.8)

For s (cid:54)= 0 the limit behavior of the ﬁrst term on the r.h.s. of (3.8) can be obtained from Theorems 3.1 and 3.2.
It turns out that for some values of β, the second term on the r.h.s. can play the dominating role. The limit
behavior of ¯XN,n was identiﬁed in [20] and is given in the following proposition, with some simpliﬁcations.

Proposition 3.1. Let the mixing distribution satisfy condition (1.2) with β > 0.

(i) Let 1 < β < 2 and N/nβ → ∞. Then

N 1/2n(β−1)/2 ¯XN,n →d ¯σβZ,

where Z ∼ N (0, 1) and ¯σ2

β := ψ(1)Γ(β − 1)/((3 − β)(2 − β)).

(ii) Let 0 < β < 1 and N/nβ → ∞. Then

N 1−1/2β ¯XN,n →d ¯V2β,

where ¯V2β is a symmetric (2β)-stable r.v. with ch.f. Eeiθ ¯V2β = e− ¯Kβ |θ|2β , ¯Kβ := ψ(1)4−βΓ(1 − β)/β.

(iii) Let 0 < β < 2 and N/nβ → 0. Then

N 1−1/βn1/2 ¯XN,n →d ¯Wβ,

where ¯Wβ is a symmetric β-stable r.v. with ch.f. Eeiθ ¯Wβ = e−¯kβ |θ|β , ¯kβ := ψ(1)2−β/2Γ(1 − β/2)/β.
(iv) Let β > 2. Then as N, n → ∞ in arbitrary way,

where Z ∼ N (0, 1) and ¯σ2 := E(1 − a)−2.

N 1/2n1/2 ¯XN,n →d ¯σZ,

(3.9)

(3.10)

(3.11)

(3.12)

From Theorems 3.1 and Proposition 3.1 we see that the r.h.s. of (3.8) may exhibit two ‘bifurcation points’
of the limit behavior, viz., as N ∼ n2β and N ∼ nβ. Depending on the value of β the ﬁrst or the second term
may dominate, and the limit behavior of (cid:98)γN,n(t, s) gets more complicated. The following corollary provides
this limit without detailing the ‘intermediate’ situations and also with exception of some particular values of
β where both terms on the r.h.s. may contribute to the limit. Essentially, the corollary follows by comparing
the normalizations in Theorems 3.1 and Proposition 3.1.

Corollary 3.1. Assume that the mixing distribution satisﬁes condition (1.2) with β > 0 and E|ε(0)|2p < ∞
for some p > 1 and (t, s) ∈ Z2, s (cid:54)= 0 be ﬁxed albeit arbitrary.

(i) Let N/n2β → ∞ and 1 < β < 3/2. Then

N 1/2nβ−1

(cid:98)γN,n(t, s) →d σ∞Z,

where Z ∼ N (0, 1) and σ∞ is the same as in Theorem 3.1 (i).

(ii) Let N/n2β → ∞ and 1/2 < β < 1. Then

N 1−1/(2β)

log1/(2β)(N 1/(2β)/n) (cid:98)γN,n(t, s) →d V2β,

10

where V2β is symmetric (2β)-stable r.v. deﬁned in Theorem 3.1 (i).

(iii) Let N/n2β → ∞ and 0 < β < 1/2. Then

N 2−1/β

(cid:98)γN,n(t, s) →d −( ¯V2β)2,

where ¯V2β is symmetric (2β)-stable r.v. deﬁned in Proposition 3.1 (ii).

(iv) Let N/n2β → 0, N/nβ → ∞ and 3/4 < β < 3/2. Then

N 1−3/(4β)n1/2

(cid:98)γN,n(t, s) →d W4β/3,

(3.13)

(3.14)

where W4β/3 is a symmetric (4β/3)-stable r.v. with characteristic function EeiθW4β/3 = e−(σ0/22β/3)|θ|4β/3 and
σ0 is the same constant as in Theorem 3.1 (ii).

(v) Let N/n2β → 0, 1/2 < β < 3/4 and N/n2β/(4β−1) → ∞. Then the convergence in (3.14) holds.

(vi) Let N/nβ → ∞, 1/2 < β < 3/4 and N/n2β/(4β−1) → 0. Then the convergence in (3.13) holds.

(vii) Let N/n2β → 0, N/nβ → ∞ and 0 < β < 1/2. Then the convergence in (3.13) holds.

(viii) Let N/nβ → 0 and 3/4 < β < 3/2. Then the convergence in (3.14) holds.

(ix) Let N/nβ → 0, 0 < β < 3/4 and N/n2β/(5−4β) → ∞. Then

(cid:98)γN,n(t, s) →d −( ¯Wβ)2,
where ¯Wβ is a symmetric β-stable r.v. deﬁned in Proposition 3.1 (iii).

N 2−2/β

(x) Let 0 < β < 3/4 and N/n2β/(5−4β) → 0. Then the convergence in (3.14) holds.

(xi) For 3/2 < β < 2, let N/nβ → [0, ∞] and for β > 2, let N, n → ∞ in arbitrary way. Then

where σ2 is given as in Theorem 3.2.

N 1/2n1/2

(cid:98)γN,n(t, s) →d N (0, σ2),

(3.15)

(3.16)

The proof of Theorem 3.1 in cases (i)–(iii) is given subsections 3.1-3.3. To avoid excessive notation, the
t=1 Xi(t)Xi+1(t).

discussion is limited to the case (t, s) = (0, 1) or the partial sums process SN,n(τ ) := (cid:80)N
i=1
Later on we shall extend them to general case (t, s), s (cid:54)= 0.

(cid:80)(cid:98)nτ (cid:99)

Let us give an outline of the proof of Theorem 3.1. Similarly to [20] we use the method of characteristic
function combined with ‘vertical’ Bernstein’s blocks, due to the fact that SN,n is not a sum of row-independent
summands as in [20]. Write

SN,n(τ ) = SN,n;q(τ ) + S†

N,n;q(τ ) + S‡

N,n;q(τ ),

(3.17)

where the main term

SN,n;q(τ )

:=

˜Nq
(cid:88)

k=1

Yk,n;q(τ ),

Yk,n;q(τ ) :=

(cid:88)

(cid:98)nτ (cid:99)
(cid:88)

(k−1)q<i<kq

t=1

Xi(t)Xi+1(t), 1 ≤ k ≤ ˜Nq := (cid:4) N
q

(cid:5), (3.18)

is a sum of ˜Nq ‘large’ blocks of size q − 1 with

q ≡ qN,n → ∞ as N, n → ∞.

(3.19)

11

The convergence rate of q ∈ N in (3.19) will be slow enough (e.g., q = O(log N )) and speciﬁed later on. The
two other terms in the decomposition (3.17),

N,n;q(τ ) := (cid:80) ˜Nq
S†

k=1

(cid:80)(cid:98)nτ (cid:99)

t=1 Xkq(t)Xkq+1(t), S‡

N,n;q(τ ) := (cid:80)

q ˜Nq<i≤N

(cid:80)(cid:98)nτ (cid:99)

t=1 Xi(t)Xi+1(t),

(3.20)

contain respectively ˜Nq = o(N ) and N − q ˜Nq < q = o(N ) row sums and will be shown to be negligible. More
precisely, we show that in each case (i)–(iii) of Theorem 3.1,

A−1
N,nSN,n;q(τ ) →fdd Sβ(τ ),
N,nS†
A−1
N,n;q(τ ) = op(1),

N,nS‡
A−1

N,n;q(τ ) = op(1),

where AN,n and Sβ denote the normalization and the limit process, respectively, particularly,

AN,n := n2


λβ
N,n,

λN,n(log λN,n)1/(2β), λ∞ = ∞, 0 < β < 1,

λ3/2
N,n,

λ∞ = ∞, 1 < β < 3/2,

λ∞ ∈ [0, ∞), 0 < β < 3/2.

(3.21)

(3.22)

(3.23)

Note that the summands Yk,n;q, 1 ≤ k ≤ ˜Nq in (3.18) are independent and identically distributed, and the
limit Sβ(τ ) is inﬁnitely divisible in cases (i)–(iii) of Theorem 3.1. Hence use of characteristic functions to
prove (3.21) is natural. The proofs are limited to one-dimensional convergence at a given τ > 0 since the
convergence of general ﬁnite-dimensional distributions follows in a similar way. Accordingly, the proof of
(3.21) for ﬁxed τ > 0 reduces to

ΦN,n;q(θ) → Φ(θ),

as N, n → ∞, λN,n → λ∞,

∀θ ∈ R,

ΦN,n;q(θ) := ˜NqE(cid:2)eiθA−1

N,nY1,n;q(τ ) − 1(cid:3),

Φ(θ) := log EeiθSβ (τ ).

(3.24)

(3.25)

where

To prove (3.24) write

N,nY1,n;q(τ ) = (cid:80)q−1
A−1

i=1 yi(τ ), where

yi(τ ) := A−1
N,n

(cid:80)(cid:98)nτ (cid:99)

t=1 Xi(t)Xi+1(t).

(3.26)

We use the identity:

(cid:81)

1≤i<q(1 + wi) − 1 = (cid:80)

1≤i<q wi + (cid:80)

|D|≥2

(cid:81)

i∈D wi,

(3.27)

where the sum (cid:80)
with wi = eiθyi(τ ) − 1 we obtain

|D|≥2 is taken over all subsets D ⊂ {1, . . . , q − 1} of cardinality |D| ≥ 2. Applying (3.27)

ΦN,n;q(θ) := ˜Nq(q − 1)(cid:2)Eeiθy1(τ ) − 1(cid:3) + ˜Nq

(cid:80)

|D|≥2 E (cid:81)

i∈D

(cid:2)eiθyi(τ ) − 1(cid:3).

Thus, since ˜Nq(q − 1)/N → 1, (3.24) follows from

N (cid:2)Eeiθy1(τ ) − 1(cid:3) → Φ(θ),
(cid:89)
(cid:2)eiθyi(τ ) − 1(cid:3) → 0.

E

(cid:88)

N

|D|≥2

i∈D

12

(3.28)

(3.29)

(3.30)

Let us explain the main idea of the proof of (3.29). Assuming φ(x) = (1 − x)β−1 in (1.2) the l.h.s. of (3.29)

can be written as

N (cid:2)Eeiθy1(τ ) − 1(cid:3) = N

(cid:90)

=

where

(0,1]2
(cid:90)

N
B2β
N,n

(0,BN,n]2

E(cid:2)eiθy1(τ ) − 1(cid:12)

(cid:12)ai = 1 − zi, i = 1, 2(cid:3)(z1z2)β−1dz1dz2

E(cid:2)eiθzN,n(τ ;x1,x2) − 1(cid:3)(x1x2)β−1dx1dx2,

(3.31)

zN,n(τ ; x1, x2)

:= A−1
N,n

(cid:88)

s1,s2∈Z

ε1(s1)ε2(s2)

(cid:98)nτ (cid:99)
(cid:88)

2
(cid:89)

t=1

i=1

(cid:0)1 −

xi
BN,n

(cid:1)t−si1(t ≥ si)

(3.32)

R2
+

and BN,n → ∞ is a scaling factor of the autoregressive coeﬃcient. In cases (ii) and (iii) of Theorem 3.1
(proof of (3.5) and (3.6)) we choose this scaling factor BN,n = N 1/(2β) so that N
= 1 and prove that the
B2β
N,n
E(cid:2)eiθz(τ ;x1,x2) − 1(cid:3)(x1x2)β−1dx1dx2 = Φ(θ), where z(τ ; x1, x2) is a random
integral in (3.31) converges to (cid:82)
process and Φ(θ) is the required limit in (3.24). A similar scaling BN,n = (N log λN,n)1/(2β) applies in the
case λ∞ = ∞, 0 < β < 1 (proof of (3.4)) although in this case the factor N/B2β
N,n = 1/ log λN,n in front of
the integral in (3.31) does not trivialize and the proof of the limit in (3.24) is more delicate. On the other
hand, in the case of the Gaussian limit (3.3), the choice BN,n = n leads to N/B2β
N,n → ∞ and (3.31)
tends to (1/2)|θ|2 (cid:82)
Ez2(τ ; x1, x2)(x1x2)β−1dx1dx2 = Φ(θ) with z(τ ; x1, x2) deﬁned in (2.11) as shown in
R2
+
subsection 3.3 below.

N,n = λ2β

To summarize the above discussion: in each case (i)–(iii) of Theorem 3.1, to prove the limit (3.21) of the
main term, it suﬃces to verify relations (3.29) and (3.30). The proof of the ﬁrst relation in (3.22) is very
similar to (3.21) since S†
N,n;q(τ ) is also a sum of i.i.d. r.v.s and the argument of (3.21) applies with small
changes. The proof of the second relation in (3.22) seems even simpler. In the proofs we repeatedly use the
following inequalities:

|eiz − 1| ≤ 2 ∧ |z|,

|eiz − 1 − iz| ≤ (2|z|) ∧ (z2/2),

z ∈ R.

(3.33)

3.1 Proof of Theorem 3.1 (iii): case 0 < λ∞ < ∞

Proof of (3.29). For notational brevity, we assume λN,n = λ∞ = 1 since the general case as in (3.2)
requires unsubstantial changes. Recall from (2.16) that Φ(θ) = (cid:82)
E[eiθz(τ ;x1,x2) − 1](x1x2)β−1dx1dx2, where
z(τ ; x1, x2) is the double Itˆo-Wiener integral in (2.11). Also recall the representation (3.31), (3.32), where
AN,n = n2, BN,n = n and zN,n(τ ; x1, x2) = Q12(hn(·; τ ; x1, x2)) is a quadratic form as in (2.3) with coeﬃcients

R2
+

hn(s1, s2; τ ; x1, x2)

:= n−2

(cid:98)nτ (cid:99)
(cid:88)

2
(cid:89)

t=1

i=1

(cid:0)1 −

xi
n

(cid:1)t−si1(t ≥ si),

s1, s2 ∈ Z.

(3.34)

By Proposition 2.1, with α1 = α2 = n, the point-wise convergence

E[eiθzN,n(τ ;x1,x2) − 1] = E[eiθQ12(hn(·;τ ;x1,x2)) − 1] → E[eiθz(τ ;x1,x2) − 1]

(3.35)

for any ﬁxed x1, x2 ∈ R+ follows from L2-convergence of the kernels:

(cid:107)(cid:101)hn(·; τ ; x1, x2) − h(·; τ ; x1, x2)(cid:107) → 0,

(3.36)

13

where

(cid:101)hn(s1, s2; τ ; x1, x2)

:= nhn((cid:98)ns1(cid:99), (cid:98)ns2(cid:99); τ ; x1, x2) =

1
n

(cid:98)nτ (cid:99)
(cid:88)

2
(cid:89)

t=1

i=1

(cid:0)1 −

xi
n

(cid:1)t−(cid:98)nsi(cid:99)1(t ≥ (cid:98)nsi(cid:99))

→

(cid:90) τ

2
(cid:89)

0

i=1

e−xi(t−si)1(t > si)dt =: h(s1, s2; τ ; x1, x2)

(3.37)

point-wise for any xi > 0, si ∈ R, si (cid:54)= 0, i = 1, 2, τ > 0 ﬁxed. We also use the dominating bound

|(cid:101)hn(s1, s2; τ ; x1, x2)| ≤ Ch(s1, s2; 2τ ; x1, x2),

s1, s2 ∈ R,

0 < x1, x2 < n,

(3.38)

with C > 0 independent of si, xi, i = 1, 2 which follows from the deﬁnition of (cid:101)hn(·; τ ; x1, x2) and the inequality
1 − x ≤ e−x, x > 0. Since h(·; 2τ ; x1, x2) ∈ L2(R2), (3.37), (3.38) and the dominated convergence theorem
imply (3.36) and (3.35).

It remains to show the convergence of the corresponding integrals, viz.,

(cid:90)

(0,n]2

E[eiθzN,n(τ ;x1,x2) − 1](x1x2)β−1dx1dx2 →

(cid:90)

R2
+

From (3.31) and EzN,n(τ ; x1, x2) = 0 we obtain

E[eiθz(τ ;x1,x2) − 1](x1x2)β−1dx1dx2 = Φ(θ).

(3.39)

(cid:12)E(cid:2)eiθzN,n(τ ;x1,x2) − 1(cid:3)(cid:12)
(cid:12)

(cid:12) ≤ C

where




1,



Ez2

N,n(τ ; x1, x2), x1x2(x1 + x2) > 1,

x1x2(x1 + x2) ≤ 1,

(3.40)

Ez2

N,n(τ ; x1, x2) = A−2

N,nE

(cid:104)(cid:16)

(cid:98)nτ (cid:99)
(cid:88)

t=1

(cid:17)2

Y12(t)

|ai = 1 −

(cid:105)

, i = 1, 2

xi
BN,n

= n−4E

(cid:104)(cid:16)

(cid:98)nτ (cid:99)
(cid:88)

t=1

(cid:17)2

Y12(t)

|ai = 1 −

(cid:105)

, i = 1, 2

xi
n

≤

C
n3(x1/n)(x2/n)

min (cid:8)n,

1
(x1 + x2)/n

(cid:9) =

C
x1x2

min (cid:8)1,

1
x1 + x2

(cid:9),

(3.41)

see (3.32) and the bound in (2.29). In view of inequality (2.8), the dominated convergence theorem applies,
proving (3.39) and (3.29).

Proof of (3.30). Choose q = qN,n = (cid:98)log n(cid:99). Let Jq(θ) denote the l.h.s. of (3.30). Using the identity
(cid:80)
i<k<j(1 + wk) with wi = eiθyi(τ ) − 1, see (3.27), we can
D⊂{1,...,q−1}:|D|≥2
rewrite Jq(θ) = (cid:80)

i∈D wi = (cid:80)
1≤i<j<q Tij(θ), where

1≤i<j<q wiwj

(cid:81)

(cid:81)

Tij(θ)

:= N E

(cid:104)
(eiθyi(τ ) − 1)(eiθyj (τ ) − 1) exp

(cid:110)

iθ

(cid:88)

yk(τ )

(cid:111)

(cid:105)
(1(ai < aj+1) + 1(ai > aj+1))

(3.42)

i<k<j

= T (cid:48)

ij(θ) + T (cid:48)(cid:48)

ij(θ).

Since |Jq(θ)| ≤ q2 max1≤i<j<q |Tij(θ)| ≤ (log n)2 max1≤i<j<q |Tij(θ)|, (3.30) follows from

|Tij(θ)| ≤ Cn−δ,

∀ 1 ≤ i < j,

(3.43)

14

with C, δ > 0 independent of n. Using E[yi(τ )|ak, εj(k), k, j ∈ Z, j (cid:54)= i] = 0 and (3.41) we obtain

|T (cid:48)

i (τ )|ak, k ∈ Z](cid:9)1(ai < aj+1)(cid:3)
ij(θ)| ≤ CN E(cid:2) min (cid:8)1, E[y2
(cid:110)

(cid:111)

(cid:90)

min

1,

1
xixi+1(xi + xi+1)

≤ Cn−β

= Cn−β

(0,n]3

(cid:90)

(0,n]2

(cid:110)

1,

min

1
x1x2(x1 + x2)

(cid:111)

x2β−1
1

xβ−1
2

dx1dx2 ≤ Cn−β(T (cid:48)

n + T (cid:48)(cid:48)

n ),

(xixi+1xj+1)β−11(xj+1 < xi)dxidxi+1dxj+1

(3.44)

n := (cid:82)

where T (cid:48)
Next,

0<x1<x2<n min (cid:8)1,

1
x1x2
2

(cid:9)x2β−1
1

xβ−1
2

dx1dx2, T (cid:48)(cid:48)

n := (cid:82)

0<x2<x1<n min (cid:8)1,

1
x2
1x2

(cid:9)x2β−1
1

xβ−1
2

dx1dx2.

T (cid:48)
n ≤

(cid:90) 1

x2β−1
1

0
(cid:104) (cid:90) 1

≤ C

0

Similarly,

√

(cid:104) (cid:90) 1/

x1

dx1

x1

xβ−1
2

dx2 + x−1
1

(cid:90) n

√

1/

x1

xβ−3
2

dx2

(cid:105)

+

(cid:90) n

1

x2β−2
1

dx1

(cid:90) n

x1

xβ−3
2

dx2

x3β/2−1
1

dx1 +

(cid:90) n

1

(cid:105)

x3β−4
1

dx1

≤ Cn3(β−1)∨0(1 + 1(β = 1) log n).

T (cid:48)(cid:48)
n =

(cid:90) 1

0

x2β−1
1

dx1

(cid:90) x1

0

xβ−1
2

dx2 +

(cid:90) n

1

x2β−1
1

dx1

(cid:90) x−2
1

0

xβ−1
2

dx2 +

(cid:90) n

1

x2β−3
1

dx1

(cid:90) x1

x−2
1

xβ−2
2

dx2

≤ C(cid:0)(log n)1(β < 1) + (log n)21(β = 1) + n3(β−1)1(β > 1)(cid:1).

Whence, the bound in (3.43) follows for T (cid:48)
|T (cid:48)(cid:48)
(3.30).

ij(θ)| ≤ CN E[min{1, E[y2

ij(θ) with any 0 < δ < β ∧ (3 − 2β), for 0 < β < 3/2. Since
j (τ )|ak, k ∈ Z]}1(aj+1 < ai)] can be symmetrically handled, this proves (3.43) and

Proof of (3.22). Since A−1
ﬁrst relation in (3.22) follows from

N,nS†

N,n;q(τ ) = (cid:80) ˜Nq

k=1 ykq(τ ) is a sum of ˜Nq i.i.d. r.v.s ykq(τ ), k = 1, . . . , ˜Nq, so the

Clearly, (3.45) is a direct consequence of (3.29) and the fact that ˜Nq/N → 0.

˜NqE[eiθy1(τ ) − 1] → 0,

∀ θ ∈ R.

(3.45)

Consider the second relation in (3.22). Let Lq := N − q ˜Nq be the number of summands in S‡
N,nS‡
A−1

i=1 yi(τ ) and

N,n;q(τ ) =fdd

(cid:80)Lq

N,n;q(τ ). Then

EeiθA−1

N,nS‡

N,n;q(τ ) − 1 = LqE[eiθy1(τ ) − 1] +

(cid:88)

(cid:89)

E

(cid:2)eiθyi(τ ) − 1(cid:3),

|D|≥2

i∈D

(3.46)

where the last sum is taken over all D ⊂ {1, . . . , Lq}, |D| ≥ 2. Since Lq < q = o(N ) from (3.29), (3.30) we
infer that the r.h.s. of (3.46) vanishes, proving (3.22), and thus completing the proof of Theorem 3.1, case
(iii).

3.2 Proof of Theorem 3.1 (ii): case λ∞ = 0, or N = o(n2β).

Proof of (3.29). Note the log-ch.f. of the r.h.s. in (3.5) can be written as

Φ(θ) = log EeiθA1/2B(τ ) = log Ee−(θ2τ /2)A = −σ0(θ2τ /2)2β/3
(cid:110)

(cid:111)(cid:17)

(cid:16)

= −ψ(1)2 (cid:82)

R2
+

1 − exp

−

θ2τ
4x1x2(x1+x2)

(x1x2)β−1dx1dx2

(3.47)

15

with σ0 > 0 given by the integral

σ0 := ψ(1)22−2β/3 (cid:82)

R2
+

(cid:16)

1 − exp

(cid:110)

−

1
x1x2(x1+x2)

(cid:111)(cid:17)

(x1x2)β−1dx1dx2.

(3.48)

Relation (3.47) follows by change of variable xi → (θ2τ /4)1/3xi, i = 1, 2. The convergence of the integral in
(3.48) follows from (2.8). The explicit value of σ0 in (3.48) is given in Proposition 2.2 (v) and computed in
the Appendix. Recall the representation in (3.31) where BN,n = N 1/(2β), N/B2β

N,n = 1 and

zN,n(τ ; x1, x2)

:= N −3/(4β)n−1/2 (cid:88)
s1,s2∈Z

ε1(s1)ε2(s2)

(cid:98)nτ (cid:99)
(cid:88)

2
(cid:89)

(cid:16)

t=1

i=1

1 −

xi
N 1/(2β)

(cid:17)t−si

1(t ≥ si).

(3.49)

Let us prove the (conditional) CLT:

zN,n(τ ; x1, x2) →fdd (2x1x2(x1 + x2))−1/2B(τ ),

implying the point-wise convergence

E[1 − eiθzN,n(τ ;x1,x2)] → 1 − e−θ2τ /(4x1x2(x1+x2))

(3.50)

(3.51)

of the integrands in (3.31) and (3.48), for any ﬁxed (x1, x2) ∈ R2
+. As in the rest of the paper, we restrict the
proof of (3.50) to one-dimensional convergence, and set τ = 1 for concreteness. Split (3.49) as zN,n(1; x1, x2) =
N,n(x1, x2) + z−
z+
s1,s2=1 ε1(s1)ε2(s2) · · · corresponds to the
sum over 1 ≤ s1, s2 ≤ n alone. Thus, we shall prove that

N,n(x1, x2) := N −3/4βn−1/2 (cid:80)n

N,n(x1, x2), where z+

z−
N,n(x1, x2) = op(1)

and z+

N,n(x1, x2) →d N

(cid:16)

0,

1
2x1x2(x1 + x2)

(cid:17)

.

(3.52)

Arguing as in the proof of (2.29) it is easy to show that

E(z−

N,n(x1, x2))2 ≤

C
N 3/(2β)n

(cid:16) x1 + x2
N 1/(2β)

(cid:17)−2(cid:110)(cid:16)

= CλN,n(x1 + x2)−2(cid:8)x−2

1 + x−2

(cid:16)

(cid:17)−2

x1
N 1/(2β)
2 + (x1x2)−1(cid:9),

+

x2
N 1/(2β)

(cid:17)−2

+

(cid:16)

x1
N 1/(2β)

(cid:17)−1(cid:16)

x2
N 1/(2β)

(cid:17)−1(cid:111)

where λN,n → 0, implying the ﬁrst relation in (3.52). To prove the second relation in (3.52) we use the
martingale CLT in Hall and Heyde [11]. (The same approach is used to prove CLT for quadratic forms in
[2].) Towards this aim, write z+
N,n(x1, x2) as a sum of zero-mean square-integrable martingale diﬀerence array

N,n(x1, x2) = (cid:80)n
z+

k=1 Zk, Zk := ε1(k) (cid:80)k−1

s=1 f (k, s) ε2(s) + ε2(k) (cid:80)k−1

s=1 f (s, k) ε1(s) + ε1(k)ε2(k)f (k, k)

with respect to the ﬁltration Fk generated by {εi(s), 1 ≤ s ≤ k, i = 1, 2}, 0 ≤ k ≤ n, where

f (s1, s2)

:= N −3/(4β)n−1/2

n
(cid:88)

2
(cid:89)

t=1

i=1

(cid:0)1 −

xi
N 1/(2β)

(cid:1)t−si1(t ≥ si),

1 ≤ s1, s2 ≤ n.

Accordingly, the second convergence in (3.52) follows from

n
(cid:88)

k=1

E[Z2

k |Fk−1] →p

1
2x1x2(x1 + x2)

and

n
(cid:88)

k=1

E[Z2

k 1(|Zk| > (cid:15))] → 0

for any (cid:15) > 0.

(3.53)

16

Note the conditional variance v2
where

k := E[Z2

k |Fk−1] = (cid:0) (cid:80)k−1

s=1 f (k, s)ε2(s)(cid:1)2 + (cid:0) (cid:80)k−1

s=1 f (s, k)ε1(s)(cid:1)2 + f 2(k, k),

n
(cid:88)

n
(cid:88)

EZ2

k =

Ev2

k =

n
(cid:88)

f 2(s1, s2) = E(z+

N,n(x1, x2))2 →

k=1

k=1

s1,s2=1

1
2x1x2(x1 + x2)

(3.54)

is a direct consequence of the asymptotics in (2.27), where ai = 1−x1/N 1/(2β), aj = 1−x2/N 1/(2β). Therefore
the ﬁrst relation in (3.53) follows from (3.54) and

Rn :=

n
(cid:88)

k=1

(v2

k − Ev2

k) = op(1).

(3.55)

To show (3.55) we split Rn = R(cid:48)

n + R(cid:48)(cid:48)

n into the sum of ‘diagonal’ and ‘oﬀ-diagonal’ parts, viz.,

R(cid:48)
n

:=

2
(cid:88)

(cid:88)

i=1

1≤s<n

ci(s)(ε2

i (s) − 1),

R(cid:48)(cid:48)

n :=

2
(cid:88)

(cid:88)

i=1

1≤s1,s2<n,s1(cid:54)=s2

ci(s1, s2)εi(s1)εi(s2),

:= (cid:80)

s<k≤n f 2(s, k), c2(s) := (cid:80)

where c1(s) := (cid:80)
c2(s1, s2)
(cid:81)2
(cid:80)n
t=1
(cid:80)∞
t=0(a1a2)t = (1 − a1a2)−1 ≤ 2(2 − a1 − a2)−1, we obtain

s1∨s2<k≤n f (k, s1)f (k, s2).

1(t ≥ si) ≤ (cid:0)as1−s2

i=1 at−si

2

i

1(1 ≤ s2 ≤ s1) + as2−s1

1

s<k≤n f 2(k, s), c1(s1, s2) := (cid:80)
s1∨s2<k≤n f (s1, k)f (s2, k),
Using the elementary bound for 1 ≤ s1, s2 ≤ n:
1(1 ≤ s1 ≤ s2)(cid:1)S(a1, a2), S(a1, a2) :=

|ci(s)| ≤ Cn−1x−1

i (x1 + x2)−2,

n
(cid:88)

s1,s2=1

i (s1, s2) ≤ CλN,nx−3
c2

i (x1 + x2)−4,

i = 1, 2.

(3.56)

By (3.56), for 1 < p < 2 and x1, x2 > 0 ﬁxed

E|R(cid:48)

n|p ≤ C

2
(cid:88)

n−1
(cid:88)

|ci(s)|p ≤ Cn−(p−1) = o(1),

E|R(cid:48)(cid:48)

n|2 ≤

i=1
2
(cid:88)

s=1
n
(cid:88)

i=1

s1,s2=1

c2
i (s1, s2) ≤ CλN,n = o(1),

(3.57)

(3.58)

proving (3.55) and the ﬁrst relation in (3.53). The proof of the second relation in (3.53) is similar since it
k=1 E[|Zk|2p] = o(1) for the same 1 < p ≤ 2, where E|Zk|2p ≤ C(cid:0)E| (cid:80)k−1
reduces to Tn := (cid:80)n
s=1 f (k, s) ε2(s)|2p +
s=1 f 2(s, k))p + |f (k, k)|2p(cid:1) by Rosenthal’s
E| (cid:80)k−1
inequality, see e.g. ([7], Lemma 2.5.2), and the sum Tn = O(n−(p−1)) = o(1) similarly to (3.57). This proves
(3.53), (3.52), and the pointwise convergence in (3.51).

s=1 f (s, k) ε1(s)|2p + |f (k, k)|2p(cid:1) ≤ C(cid:0)((cid:80)k−1

s=1 f 2(k, s))p + ((cid:80)k−1

Now we return to the proof of (3.29), whose both sides are written as respective integrals (3.31) and (3.47).
Due to the convergence of the integrands (see (3.51)), it suﬃces to justify the passage to the limit using a
dominated convergence theorem argument. The dominating function independent of N, n is obtained from
(3.31) and EzN,n(τ ; x1, x2) = 0 and from (3.40), (3.41), (2.8) similarly as in the case λ∞ ∈ (0, ∞) above. This
proves (3.29).

Proofs of (3.30) and (3.22) are completely analogous to those in the case λ∞ ∈ (0, ∞) except that we now
choose q = (cid:98)log N (cid:99) and replace n in (3.43) and elsewhere in the proof of (3.30) and (3.22), case λ∞ ∈ (0, ∞),
by N 1/2β. This ends the proof of Theorem 3.1, case (ii).

17

3.3 Proof of Theorem 3.1 (i): case λ∞ = ∞, or n = o(N 1/(2β))

Case 1 < β < 3/2. Proof of (3.29). In this case, Φ(θ) := −σ2
n2−βN 1/2. Rewrite the l.h.s. of (3.29) as

∞τ 2(2−β)θ2/2, BN,n = n and AN,n = n2λβ

N,n =

N (cid:2)Eeiθy1(τ ) − 1(cid:3) = (cid:82)

[0,n)2 EΛN,n(θ; τ ; x1, x2)(x1x2)β−1dx1dx2,

ΛN,n(θ; τ ; x1, x2) := λ2β
N,n

(cid:2)eiθλ−β

N,n ˜zN,n(τ ;x1,x2) − 1 − iθλ−β

where
N,n ˜zN,n(τ ; x1, x2)(cid:3)

(3.59)

and where ˜zN,n(τ ; x1, x2) is deﬁned as in (3.32) with AN,n replaced by ˜AN,n := n2 = AN,n/λβ
in the proof of Case (iii) (the ‘intermediate limit’), for any x1, x2 > 0

N,n. As shown

˜zN,n(τ ; x1, x2) →d z(τ ; x1, x2)

and E˜z2

N,n(τ ; x1, x2) → Ez2(τ ; x1, x2),

(3.60)

see (3.35), where z(τ ; x1, x2) is deﬁned in (2.11) and the last expectation in (3.60) is given in (A.2).
Then using Skorohod’s representation we extend (3.60) to ˜zN,n(τ ; x1, x2) → z(τ ; x1, x2) a.s.
implying also
ΛN,n(τ ; x1, x2) → −(θ2/2)z2(τ ; x1, x2) a.s. Since |ΛN,n(θ; τ ; x1, x2)| ≤ C ˜z2
N,n(τ ; x1, x2) and (3.60) holds, by
Pratt’s lemma we obtain

GN,n(τ ; x1, x2) → − 1

2 Ez2(τ ; x1, x2),

∀ (x1, x2) ∈ R2
+.

(3.61)

Relation (3.29) follows from (3.59), (3.61) and the dominated convergence theorem, using the dominating
bound

|GN,n(τ ; x1, x2)| ≤ CE˜z2

N,n(τ ; x1, x2) ≤ C
x1x2

min (cid:8)1,

1
x1+x2

(cid:9) =: ¯G(x1, x2),

(3.62)

see (3.41), and integrability of ¯G, see (2.9).

Proof of (3.30) is similar to that in case (iii) 0 < λ∞ < ∞ above with q = (cid:98)log n(cid:99).
the bound (3.43) for Tij(θ) = T (cid:48)
|T (cid:48)

It suﬃces to check
ij(θ) given in (3.42). By the same argument as in (3.44), we obtain

ij(θ) + T (cid:48)(cid:48)
i (τ )1(ai < aj+1)]. The bound on E˜z2

N,n(τ ; x1, x2) in (3.62) further implies

ij(θ)| ≤ CN E[y2

|T (cid:48)

ij(θ)| ≤ Cn−β

(cid:90)

(0,n]3

1
x1x2

(cid:110)

1,

min

1
x1 + x2

(cid:111)

(x1x2x3)β−11(x3 < x1)dx1dx2dx3 ≤ Cn−β(T (cid:48)

n + T (cid:48)(cid:48)

n ),

(cid:90) n

1

(cid:90) n

1

(cid:17)

x3β−4
1

dx1

≤ Cn3β−3

(cid:17)

x3β−4
2

dx2

≤ Cn3β−3.

where

and

T (cid:48)
n :=

(cid:110)

1,

min

(cid:90) n

0

(cid:111)

x2β−2
1

dx1

1
x1

(cid:90) x1

0

xβ−2
2

dx2 = C

(cid:16) (cid:90) 1

0

x3β−3
1

dx1 +

(cid:90) n

(cid:110)

1,

min

T (cid:48)(cid:48)
n :=

0
ij(θ)| ≤ CN E[y2

(cid:111)

xβ−2
2

dx2

1
x2

(cid:90) x2

0

x2β−2
1

dx1 = C

(cid:16) (cid:90) 1

0

x3β−3
2

dx2 +

Then |T (cid:48)(cid:48)
j (τ )1(ai > aj+1)] can be handled in the same way. Whence, the bound in (3.43)
follows with any 0 < δ < 3 − 2β, for 1 < β < 3/2. This proves (3.30). Proof of (3.22) using ˜Nq/N → 0 and
Lq = N − q ˜Nq < q = o(N ) is completely analogous to that in case (iii) 0 < λ∞ < ∞. This completes the
proof of Theorem 3.1, case (i) for 1 < β < 3/2.

Case 0 < β < 1. Proof of (3.29). In the rest of this proof, write λ ≡ λN,n = N 1/(2β)/n → ∞ for brevity.
Also denote λ(cid:48) := λ(log λ)1/2β, log λ(cid:48)/ log λ → 1. Let BN,n := λ(cid:48)n, then

zN,n(τ ; x1, x2)

:=

1
λ(cid:48)n2

(cid:88)

s1,s2∈Z

ε1(s1)ε2(s2)

(cid:98)nτ (cid:99)
(cid:88)

2
(cid:89)

t=1

i=1

(cid:0)1 −

xi
λ(cid:48)n

(cid:1)t−si1(t ≥ si).

(3.63)

18

Split the r.h.s. of (3.29) as follows:

N E(cid:2)eiθy1(τ ) − 1(cid:3) =

1
log λ

(cid:90)

(0,λ(cid:48)n]2

(cid:0)1(1 < x1 + x2 < λ) + 1(x1 + x2 > λ) + 1(x1 + x2 < 1)(cid:1)

×E(cid:2)eiθzN,n(τ ;x1,x2) − 1(cid:3)(x1x2)β−1dx1dx2 =:

3
(cid:88)

i=1

Li.

Here, L1 is the main term and Li, i = 2, 3 are remainders. Indeed, |L3| = O(1/ log λ) = o(1). To estimate L2
we need the bound

Ez2

N,n(τ ; x1, x2) ≤

(cid:110)

1,

min

C
x1x2

λ(cid:48)
x1 + x2

(cid:111)
,

which follows from (2.29) similarly to (3.41). Using (3.64) we obtain

|L2| ≤

C
log λ

(cid:90)

x1+x2>λ

(cid:110)

1,

min

λ(cid:48)
x1x2(x1 + x2)

(cid:111)

(x1x2)β−1dx1dx2 =

C
log λ

where, by change of variables: x1 + x2 = y, x1 = yz,

(J (cid:48)

λ + J (cid:48)(cid:48)

λ ),

(3.64)

(3.65)

J (cid:48)
λ

:=

=

(cid:90)

1(x1x2(x1 + x2) < λ(cid:48))(x1x2)β−1dx1dx2

x1+x2>λ
(cid:90) 1

(cid:90) ∞

1(y3z(1 − z) < λ(cid:48))y2β−1(z(1 − z))β−1dzdy

0
λ
(cid:90) ∞

≤ C

λ

y2β−1dy

(cid:90) 1/2

0

zβ−11(y3z < 2λ(cid:48))dz ≤ C(λ(cid:48))β

(cid:90) ∞

λ

y−β−1dy = C(log λ)1/2

since 0 < β < 1. Similarly,

J (cid:48)(cid:48)
λ

:= λ(cid:48)

This proves |L2| = O(1/ log λ) = o(1).

λ

(cid:90)

1(x1x2(x1 + x2) > λ(cid:48))(x1 + x2)−1(x1x2)β−2dx1dx2

x1+x2>λ
(cid:90) ∞

≤ Cλ(cid:48)

y2β−4dy

(cid:90) 1/2

0

zβ−21(y3z > λ(cid:48))dz ≤ C(log λ)1/2.

Consider the main term L1. Although EeiθzN,n(τ ;x1,x2) and hence the integrand in L1 point-wise converge for
any (x1, x2) ∈ R2
+, see below, this fact is not very useful since the contribution to the limit of L1 from bounded
xi’s is negligible due to the presence of the factor 1/ log λ → 0 in front of this integral. It turns out that the
main (non-negligible) contribution to this integral comes from unbounded x1, x2 with x1/x2 + x2/x1 → ∞
and x1x2 → z ∈ R+. To see this, by change of variables y = x1 + x2, x1 = yw and then w = z/y2 we rewrite

L1 =

1
log λ

(cid:90) λ

1

VN,n(θ; y)

dy
y

,

(3.66)

where

VN,n(θ; y)

:= 2

(cid:90) y2/2

0

(cid:104)
E

exp

(cid:110)

iθzN,n(τ ;

(cid:16)

, y

1 −

z
y

z
y2

(cid:17)(cid:111)

(cid:105)
− 1

zβ−1(cid:16)

1 −

(cid:17)β−1

dz.

z
y2

(3.67)

In view of Li = o(1), i = 2, 3 relation (3.29) follows from representation (3.66) and the existence of the limit:

lim
y→∞,y=O(λ)

VN,n(θ; y) = V (θ) := −k∞|θ|2θτ 2β,

(3.68)

19

where the constant k∞ > 0 is deﬁned below in (3.71). More precisely, (3.68) says that for any (cid:15) > 0 there
exists K > 0 such that for any N, n, y ≥ K satisfying y ≤ λ, λ ≥ K

|VN,n(θ; y) − V (θ)| < (cid:15).

(3.69)

To show that (3.69) implies L1 → V (θ) it suﬃces to split L1 − V (θ) = (log λ)−1 (cid:82) λ
(log λ)−1 (cid:82) K
uniformly in N, n, y.

y +
y and use (3.69) together with the fact that |VN,n(θ; y)| ≤ C is bounded

1 (VN,n(θ; y) − V (θ)) dy

K(VN,n(θ; y) − V (θ)) dy

To prove (3.69), rewrite V (θ) of (3.68) as the integral

V (θ) = 2

(cid:90) ∞

0

zβ−1E(eiθτ Z1Z2/(2

√

z) − 1)dz = −2E

(cid:90) ∞

0

with Z1, Z2 ∼ N (0, 1) independent normals and

zβ−1(1 − e−θ2τ 2Z2

1 /(8z))dz = −k∞|θ|2βτ 2β

(3.70)

k∞ = 2E (cid:82) ∞

0 zβ−1(1 − e−Z2

1 /(8z))dz = 21−3βE|Z1|2β (cid:82) ∞

0 zβ−1(1 − e−1/z)dz

= 21−2βΓ(β + 1

2 )Γ(1 − β)/(

√

πβ).

(3.71)

Let ΛN,n(z; y) := E(cid:2) exp (cid:8)iθzN,n(τ ; z
expectations in (3.67), (3.70). Clearly, (3.69) follows from

y , y(cid:0)1 − z

y2

(cid:1)(cid:9) − 1(cid:3), Λ(z) := E[eiθτ Z1Z2/(2

√

z) − 1] denote the corresponding

limy→∞,y=O(λ) ΛN,n(z; y) = Λ(z),

∀ z > 0,

and

|ΛN,n(z; y)| ≤ C(1 ∧ (1/z)),

∀ 0 < y < λ, 0 < z < y2/2.

(3.72)

(3.73)

The dominating bound in (3.73) is a consequence of (3.64). To show (3.72) use Proposition 2.1 by writing
zN,n(τ ; z/y, y(cid:48)), y(cid:48) := y(1 − z/y2) in (3.67) as the quadratic form: zN,n(τ ; z/y, y(cid:48)) = Q12(hα1,α2(·; τ ; z)) with

hα1,α2(s1, s2; τ ; z)

:=

(cid:114) y
zy(cid:48)

√

α1

:= λ(cid:48)ny/z,

1
α1α2

2
(cid:89)

(cid:98)nτ (cid:99)
(cid:88)

(cid:0)1 −

1
n
i=1
α2 := λ(cid:48)n/y(cid:48).

t=1

(cid:1)t−si1(t ≥ si),

s1, s2 ∈ Z,

(3.74)

1
αi

If

then

n, α1, α2, y, y(cid:48) → ∞ so that

y/y(cid:48) → 1

and n = o(αi), i = 1, 2,

(3.75)

(cid:101)h(α1,α2)
α1,α2 (s1, s2; τ ; z)

:=

√

α1α2hα1,α2((cid:98)α1s1(cid:99), (cid:98)α2s2(cid:99); τ ; z)

=

→

(cid:114) y
zy(cid:48)

1
n

(cid:98)nτ (cid:99)
(cid:88)

2
(cid:89)

t=1

i=1

(cid:0)1 −

1
αi

(cid:1)t−(cid:98)αisi(cid:99)1(t ≥ (cid:98)αisi(cid:99))

τ
√

z

2
(cid:89)

i=1

esi1(si < 0) =: h(s1, s2; τ ; z)

(3.76)

point-wise for any τ > 0, z > 0, si ∈ R, si
(3.75), (cid:107)(cid:101)h(α1,α2)

(cid:54)= 0, i = 1, 2 ﬁxed. Moreover, under the same conditions
α1,α2 (·; τ ; z) − h(·; τ ; z)(cid:107) → 0, implying the convergence Q12(hα1,α2(·; τ ; z)) →d I12(h(·; τ ; z)) =d

20

√

z), Zi ∼ N (0, 1), i = 1, 2 by Proposition 2.1. Conditions on n, y, y(cid:48), λ(cid:48) in (3.75) are obviously
τ Z1Z2/(2
satisﬁed due to y, y(cid:48) = O(λ) = o(λ(cid:48)). This proves (3.72) and (3.68), thereby completing the proof of of (3.29).
Proof of (3.30). For Tij(θ) deﬁned by (3.42) let us prove (3.43). Denote N (cid:48)
λ := (N log λ)1/2β. Similarly to
(3.44) we have that

|Tij(θ)| ≤

C
N 1/2(log λ)3/2

(cid:90)

(0,N (cid:48)

λ]3

min (cid:8)1, Ez2

N,n(τ ; x1, x2)(cid:9)(x1x2x3)β−11(x3 < x1)dx1dx2dx3

with zN,n(τ ; x1, x2) deﬁned by (3.63). Whence using (3.64) similarly as in the proof of case (i) we obtain

|Tij(θ)| ≤

C
N 1/2(log λ)3/2

(cid:82)
(0,N (cid:48)
(cid:80)3

λ]2 min
i=1 Tλ,i,

C
N 1/2(log λ)3/2

(cid:110)

1,

1
x1x2

min (cid:8)1,

λ(cid:48)
x1+x2

(cid:9)(cid:111)

x2β−1
1

xβ−1
2

dx1dx2

where

Tλ,1

:=

Tλ,2

:=

=

(cid:90)

(0,N (cid:48)

λ]2

(cid:90)

λ]2

(0,N (cid:48)
(cid:90)

(0,N (cid:48)

λ]2

1(x1 + x2 < λ(cid:48)) min (cid:8)1,

1
x1x2

(cid:9)x2β−1
1

xβ−1
2

dx1dx2,

1(x1x2(x1 + x2) < λ(cid:48), x1 + x2 > λ(cid:48))x2β−1

1

xβ−1
2

dx1dx2,

Tλ,3

:= λ(cid:48)

1(x1x2(x1 + x2) > λ(cid:48), x1 + x2 > λ(cid:48))x2β−2

1

xβ−2
2

dx1dx2/(x1 + x2).

By changing variables x1, x2 as in (3.66)-(3.67) we get Tλ,1 ≤ C (cid:82) λ(cid:48)
estimation of J (cid:48)
we conclude that

λ , following (3.65) we obtain Tλ,2 + Tλ,3 ≤ C(λ(cid:48))β (cid:82) 2N (cid:48)

λ, J (cid:48)(cid:48)

λ(cid:48)

0 yβ−1dy ≤ C(λ(cid:48))β. Also, similarly to the
λ/λ(cid:48)). Hence,

y−1dy ≤ C(λ(cid:48))β log(N (cid:48)

λ

|Tij(θ)| ≤

C(λ(cid:48))β log(N (cid:48)

λ/λ(cid:48))

N 1/2(log λ)3/2

≤

C log n
nβ log λ

,

proving (3.43) with any 0 < δ < β. This proves (3.30). We omit the proof of (3.22) which is completely
similar to that in case (iii) and elsewhere. This completes the proof of Theorem 3.1 for (t, s) = (0, 1).
Proof of Theorem 3.1 in the general case (t, s) ∈ Z2, s ≥ 1. Similarly to (3.17) we decompose St,s
as

N,n(τ ) in (3.1)

N,n(τ ) = St,s
St,s

N,n;q(τ ) + St,s;†

N,n;q(τ ) + St,s;‡

N,n;q(τ ),

(3.77)

where the main term

St,s
N,n;q(τ )

:=

˜Nq
(cid:88)

k=1

Y t,s
k,n;q(τ ),

Y t,s
k,n;q(τ ) :=

(cid:88)

(cid:98)nτ (cid:99)
(cid:88)

(k−1)q<i≤kq−s

u=1

Xi(u)Xi+s(u + t)

(3.78)

is a sum of independent ˜Nq = (cid:98)N/q(cid:99) blocks of size q − s = qN,n − s → ∞, and

St,s;†
N,n;q(τ ) :=

˜Nq
(cid:88)

(cid:88)

(cid:98)nτ (cid:99)
(cid:88)

k=1

kq−s<i≤kq

u=1

Xi(u)Xi+s(u + t), St,s;‡

N,n;q(τ ) :=

(cid:88)

(cid:98)nτ (cid:99)
(cid:88)

q ˜Nq<i≤N

u=1

Xi(u)Xi+s(u + t)

The proof of

(3.29)–(3.30) for A−1

N,nY t,s

1,n;q(τ ) = (cid:80)q−s

u=1 Xi(u)Xi+s(u + t) is completely analogous since the distribution of yt,s

i=1 yt,s

i (τ ), yt,s
i (τ ) does not depend on

i (τ )

:=

are remainder terms.
A−1
N,n
t and s (cid:54)= 0.

(cid:80)(cid:98)nτ (cid:99)

21

3.4 Proof of Theorem 3.2

The proof uses the following result of [23].

Lemma 3.1. ([23], Lemma 7.1) Let {ξni, 1 ≤ i ≤ Nn}, n ≥ 1, be a triangular array of m-dependent r.v.s
with zero mean and ﬁnite variance. Assume that: (L1) ξni, 1 ≤ i ≤ Nn, are identically distributed for any
n1 → Eξ2 < ∞ for some r.v. ξ and (L3) var((cid:80)Nn
i=1 ξni) ∼ σ2Nn, σ2 > 0. Then
n ≥ 1, (L2) ξn1 →d ξ, Eξ2
N −1/2
i=1 ξni →d N (0, σ2).
n

(cid:80)Nn

i=1 ξni, where ξni := n−1/2 (cid:80)(cid:98)nτ (cid:99)

For notational simplicity, we consider only one-dimensional convergence at τ > 0. Let (N n)−1/2St,s

N,n(τ ) =
N −1/2 (cid:80)N
u=1 Xi(u)Xi+s(u + t), 1 ≤ i ≤ N are |s|-dependent, identically dis-
tributed random variables with zero mean and ﬁnite variance. Since ξni, 1 ≤ i ≤ N are uncorrelated, it
follows that E((cid:80)N
u=1 X1(u)X2(u). Proposition 2.4 implies
E[ξ2
A12B(τ ),
where A12 is independent of B(τ ). This follows from the martingale CLT similarly to (3.50). By the lemma
above, we conclude that (N n)−1/2St,s
(cid:3)

n ∼ τ σ2, where σ2 := EA12 < ∞. It remains to show that ξn →d

n1, where ξn1 =d ξn := n−1/2 (cid:80)(cid:98)nτ (cid:99)

n|a1, a2] ∼ τ A12, and so Eξ2

i=1 ξni)2 = N Eξ2

N n(τ ) →d σB(τ ). Theorem 3.2 is proved.

√

4 Asymptotic distribution of temporal (iso-sectional) sample covariances

The limit distribution of iso-sectional sample covariances (cid:98)γN,n(t, 0) in (1.5) and the corresponding partial sums
process St,0
N,n(τ ) of (3.1) is obtained similarly as in the cross-sectional case, with certain diﬀerences which
are discussed below. Since the conditional expectation E[St,0
N,n(τ ) (cid:54)= 0, a natural
decomposition is

N,n(τ )|a1, · · · , aN ] =: T t,0

N,n(τ ) = (cid:101)St,0
St,0

N,n(τ ) + T t,0

N,n(τ ),

(4.1)

where (cid:101)St,0

N,n(τ ) := St,0

N,n(τ ) − T t,0

N,n(τ ) is the conditionally centered term with E[ (cid:101)St,0

N,n(τ )|a1, · · · , aN ] = 0, and

T t,0
N,n(τ ) := (cid:98)nτ (cid:99)

N
(cid:88)

i=1

i/(1 − a2
at

i ),

t ≥ 0,

(4.2)

is proportional to a sum of i.i.d. r.v.s at

i/(1 − a2

i ), 1 ≤ i ≤ N with regularly decaying tail distribution function

P(cid:0)at/(1 − a2) > x) ∼ P(a > 1 −

1
2x

) ∼ cax−β,

x → ∞,

ca := ψ(1)/2ββ,

see condition (1.2). Accordingly, the limit distribution of appropriately normalized and centered term T t,0
N,n(τ )
does not depend on t and can be found from the classical CLT and turns out to be a (β ∧ 2)-stable line, under
normalization nN 1/(β∧2) (β (cid:54)= 2). The other term, (cid:101)St,0
N,n(τ ), in (4.1), is a sum of mutually independent partial
sums processes Y t,0
u=1 (Xi(u)Xi(u + t) − E[Xi(u)Xi(u + t)|ai]), 1 ≤ i ≤ N with conditional variance

i,n (τ ) := (cid:80)(cid:98)nτ (cid:99)

var[Y t,0

i,n (1)|ai] ∼ nAt,0

ii , n → ∞,

where At,0
ii

:= 1+a2
i
1−a2
i

(cid:16) 1+a2|t|
i
(1−a2

i )2 + a2|t|

i

(2|t|+cum4)

1−a4
i

(cid:17)

.

The proof of the last fact follows similarly to that of (2.28) and is omitted. As ai ↑ 1, At,0
the limit distribution of (cid:101)St,0
on the limit λ∗
(cid:101)St,0
N,n(τ ) in all cases of λ∗

ii ∼ 1/(2(1−ai)3) and
N,n(τ ) can be shown to exhibit a trichotomy on the interval 0 < β < 3 depending
N,n(τ ) dominates
N,n(τ ) have the same convergence

∞ in (4.3). It turns out that for β > 2 the asymptotically Gaussian term T t,0

∞, while in the interval 0 < β < 2 T t,0

N,n(τ ) and (cid:101)St,0

22

N,n(τ ) is a β-stable line in both cases λ∗

rate. Somewhat surprisingly, the limit distribution of St,0
λ∗
∞ = 0 with diﬀerent scale parameters of the random slope coeﬃcient of this line.
Rigorous description of the above limit results is given in the following Theorems 4.1 and 4.2. The proofs
of these theorems are similar and actually simpler than the corresponding Theorems 3.1 and 3.2 dealing
with non-horizontal sample covariances, due to the fact that St,0
N,n(τ ) is a sum of row-independent summands
contrary to St,s
N,n(τ ), s (cid:54)= 0. Because of this, we omit some details of the proof of Theorems 4.1 and 4.2.
We also omit the more delicate cases β = 1 and β = 2 where the limit results may require a change of
normalization or additional centering.

∞ = ∞ and

Theorem 4.1. Let the mixing distribution satisfy condition (1.2) with 0 < β < 2, β (cid:54)= 1. Let N, n → ∞ so
that

λ∗
N,n :=

→ λ∗

∞ ∈ [0, ∞].

(4.3)

N 1/β
n

In addition, assume Eε4(0) < ∞. Then the following statements (i)–(iii) hold for St,0
depending on λ∗

∞ in (4.3).

N,n(τ ), t ∈ Z in (3.1)

(i) Let λ∗

∞ = ∞. Then

n−1N −1/β(cid:0)St,0

N,n(τ ) − ESt,0

N,n(τ )1(1 < β < 2)(cid:1) →fdd τ V ∗
β ,

where V ∗

β is a completely asymmetric β-stable r.v. with characteristic function in (4.7) below.

(ii) Let λ∗

∞ = 0. Then

n−1N −1/β(cid:0)St,0

N,n(τ ) − ESt,0

N,n(τ )1(1 < β < 2)(cid:1) →fdd τ V +
β ,

where V +

β is a completely asymmetric β-stable r.v. with characteristic function in (4.8) below.

(iii) Let 0 < λ∗

∞ < ∞. Then

n−1N −1/β(cid:0)St,0

N,n(τ ) − ESt,0

N,n(τ )1(1 < β < 2)(cid:1) →fdd λ∗

∞Z ∗

β(τ /λ∗

∞),

where Z ∗

β is the ‘diagonal intermediate’ process in (2.24).

(4.4)

(4.5)

(4.6)

Remark 4.1. The r.v.s V ∗

β and V +

β in (4.4) and (4.5) have respective stochastic integral representations

V ∗
β =

V +
β =

(cid:90)

R+×C(R)

(cid:90)

R+×C(R)

(cid:90) 0

(cid:8)

−∞

exsdB(s)(cid:9)2d(M∗

β − EM∗

β1(1 < β < 2)),

(2x)−1d(M∗

β − EM∗

β1(1 < β < 2))

w.r.t. Poisson random measure M∗
both V ∗

β and V +

β have completely asymmetric β-stable distribution follows from their ch.f.s:

β in (2.21). Note (cid:82) 0

−∞ exsdB(s) =law Z/

√

2x, Z ∼ N (0, 1). The fact that

EeiθV ∗

β = exp (cid:8)ψ(1)

(cid:90) ∞

E(cid:0)eiθZ2/(2x) − 1 − i(θZ2/(2x))1(1 < β < 2)(cid:1)xβ−1dx(cid:9)

= exp (cid:8) − c∗

0

β|θ|β(1 − i sign(θ) tan(πβ/2))(cid:9),
(cid:90) ∞

EeiθV +

β = exp (cid:8)ψ(1)

(cid:0)eiθ/(2x) − 1 − i(θ/(2x))1(1 < β < 2)(cid:1)xβ−1dx(cid:9)

0

= exp (cid:8) − c+

β |θ|β(1 − i sign(θ) tan(πβ/2))(cid:9),

θ ∈ R,

23

(4.7)

(4.8)

where

c+
β :=

ψ(1)Γ(2 − β) cos(πβ/2)
2ββ(1 − β)

,

β := c+
c∗

β E|Z|2β

(4.9)

with E|Z|2β = 2βΓ(β + 1/2)/

π (cid:54)= 1 unless β = 1, implying that V ∗

β and V +

β have diﬀerent distributions.

√

Theorem 4.2. Let the mixing distribution satisfy condition (1.2) with β > 2. In addition, assume Eε4(0) <
∞. Then for any t ∈ Z, as N, n → ∞ in arbitrary way,

n−1N −1/2(cid:0)St,0

N,n(τ ) − ESt,0

N,n(τ )(cid:1) →fdd τ σ∗

t Z,

(4.10)

where Z ∼ N (0, 1) and (σ∗

t )2 := var(a|t|/(1 − a2)).

Remark 4.2. If β < 1, then γ(t, 0) is undeﬁned for any t ∈ Z. Using the convention γ(t, 0)1(1 < β < 2) := 0
if β < 1 and γ(t, 0) if β > 1.

Corollary 4.1. (i) Let the conditions of Theorem 4.1 (i) be satisﬁed. Then for any t ∈ Z

N 1−1/β((cid:98)γN,n(t, 0) − γ(t, 0)1(1 < β < 2)) →d V ∗
β .

(ii) Let the conditions of Theorem 4.1 (ii) be satisﬁed. Then for any t ∈ Z

N 1−1/β((cid:98)γN,n(t, 0) − γ(t, 0)1(1 < β < 2)) →d V +
β .

(iii) Let the conditions of Theorem 4.1 (iii) be satisﬁed. Then for any t ∈ Z

N 1−1/β((cid:98)γN,n(t, 0) − γ(t, 0)1(1 < β < 2)) →d λ∗

∞Z ∗

β(1/λ∗

∞).

(iv) Let the conditions of Theorem 4.2 be satisﬁed. Then for any t ∈ Z

N 1/2((cid:98)γN,n(t, 0) − γ(t, 0)) →d σ∗

t Z,

Z ∼ N (0, 1).

Proof of Theorem 4.1. Let t ≥ 0 and

yt,0(τ )

:=

1
nN 1/β

(cid:98)nτ (cid:99)
(cid:88)

u=1

(X(u)X(u + t) − EX(u)X(u + t)1(1 < β < 2)).

(4.11)

It suﬃces to prove that

Φt,0

N,n(θ) → Φ∗(θ),

where, using Eyt,0(τ )1(1 < β < 2) = 0,

as N, n → ∞, λ∗

N,n → λ∗

∞,

∀θ ∈ R,

(4.12)

N,n(θ) := N E(cid:2)eiθyt,0(τ ) − 1 − iθyt,0(τ )1(1 < β < 2)(cid:3),
Φt,0

Φ∗(θ) := log EeiθS∗

β (τ ),

(4.13)

and S ∗

β(τ ) denotes the limit process in (4.4)–(4.6). Similarly to (3.31),

Φt,0

N,n(θ) = ψ(1)

(cid:90)

(0,1/N 1/β ]

E(cid:2)eiθzt,0

N,n(τ ;x) − 1 − iθzt,0

N,n(τ ; x)1(1 < β < 2)(cid:3)xβ−1dx,

(4.14)

24

where zt,0

N,n(τ ; x) := yt,0(τ )|a=1−x/N 1/β . Next we decompose yt,0(τ ) = y∗(τ ) + y+(τ ), where

y∗(τ )

:=

y+(τ )

:=

1
nN 1/β

(cid:98)nτ (cid:99)
nN 1/β

(cid:98)nτ (cid:99)
(cid:88)

u=1

(X(u)X(u + t) − E[X(u)X(u + t)|a]),

(E[X(0)X(t)|a] − E[X(0)X(t)1(1 < β < 2)]) =

(cid:98)nτ (cid:99)
nN 1/β

(cid:16) at
1 − a2 − E

(cid:104) at1(1 < β < 2)
1 − a2

(cid:105)(cid:17)

.

Accordingly, we decompose zt,0

N,n(τ ; x) = z∗

N,n(τ ; x) + z+

N,n(τ ; x), where

z∗
N,n(τ ; x)

:=

z+
N,n(τ ; x)

:=

1
nN 1/β

(cid:98)nτ (cid:99)
nN 1/β

(cid:88)

ε(s1)ε(s2)

(cid:98)nτ (cid:99)
(cid:88)

s1,s2∈Z
(cid:16) (1 − xN −1/β)t

u=1

1 − (1 − xN −1/β)2

− E

(cid:104) at1(1 < β < 2)
1 − a2

(cid:105)(cid:17)

,

(cid:0)1 − x
N 1/β

(cid:1)2u+t−s1−s21(u ≥ s1, u + t ≥ s2),

(4.15)

where ε(s1)ε(s2) := ε(s1)ε(s2) − Eε(s1)ε(s2).

Proof of (4.12), case 0 < λ∗

Φ∗(θ) = ψ(1) (cid:82) ∞

∞ < ∞. We have
0 E(cid:2)eiθλ∗

∞z∗(τ /λ∗

∞;x) − 1 − iθλ∗

∞z∗(τ /λ∗

∞; x)1(1 < β < 2)(cid:3)xβ−1dx,

(4.16)

where the last expectation is taken w.r.t. the Wiener measure PB. Similarly as in the proof of (3.29) we
prove the point-wise convergence of the integrands in (4.14) and (4.16): for any x > 0

Λt,0

N,n(θ; x)

:= E(cid:2)eiθzt,0
→ E(cid:2)eiθλ∗

N,n(τ ;x) − 1 − iθzt,0
∞z∗(τ /λ∗

∞;x) − 1 − iθλ∗

N,n(τ ; x)1(1 < β < 2)(cid:3)
∞z∗(τ /λ∗

∞; x)1(1 < β < 2)(cid:3).

(4.17)

The proof of (4.17) using Proposition 2.1 is very similar to that of (3.35) and we omit the details. Using
(4.17) and the dominated convergence theorem we can prove the convergence of integrals, or (4.12). The
application of the dominated convergence theorem is guaranteed by the dominating bound

|Λt,0

N,n(θ; x)| ≤ C(1 ∧ (1/x)){1(0 < β < 1) + (1/x)1(1 < β < 2)},

(4.18)

which is a consequence of |z+
we get |Λt,0
hence (4.18) follows. For 1 < β < 2 (4.18) follows similarly. This proves (4.12) for 0 < λ∗

N,n(τ ; x))2 ≤ Cx−2, see (2.29). Particularly, for 0 < β < 1
E|z∗
N,n(τ ; x)|2 + (1/x)) ≤ C/x,

N,n(τ ; x)| ≤ C/x, E(z∗
N,n(θ; x)| ≤ E(|z∗

N,n(θ; x)| ≤ 2 and |Λt,0

N,n(τ ; x)| + |z+

N,n(τ ; x)|) ≤ C(

(cid:113)

∞ < ∞.

Proof of (4.12), case λ∗

∞ = 0.

In this case

Φ∗(θ) = ψ(1) (cid:82)

R+

(cid:2)eiθ(τ /(2x)) − 1 − iθ(τ /(2x))1(1 < β < 2)(cid:3)xβ−1dx,

see (4.8). From (2.29) we have E(z∗

N,n(τ ; x))2 ≤ Cx−2 min{1, λ∗

N,n/x} → 0 and hence

Λt,0
N,n(θ; x) → eiθτ /(2x) − 1 − iθ(τ /(2x))1(1 < β < 2)

for any x > 0 similarly as in (4.17). Finally, the use of the dominating bound in (4.18) which is also valid in
this case completes the proof of (4.12) for λ∗

∞ = 0.

Proof of (4.12), case λ∗

∞ = ∞.

In this case,

Φ∗(θ) = ψ(1) (cid:82)

R+

E(cid:2)eiθ(τ Z2/(2x)) − 1 − iθ(τ Z2/(2x))1(1 < β < 2)(cid:3)xβ−1dx,

(4.19)

25

N,n(τ ; x) in (4.15) as quadratic form: z∗

see (4.7). Write z∗
N,n(τ ; x) = Q11(h(τ ; x; ·)) in (2.3) and apply Propo-
sition 2.1 with α1 = α2 ≡ α := N 1/β. Note (cid:101)h(α,α)(τ ; x; s1, s2) = n−1 (cid:80)(cid:98)nτ (cid:99)
u=1 (1 − x/N 1/β)u−(cid:98)N 1/β s1(cid:99)(1 −
x/N 1/β)t+u−(cid:98)N 1/β s2(cid:99)1(u ≥ (cid:98)N 1/βs1(cid:99), u + t ≥ (cid:98)N 1/βs2(cid:99)) → g(s1, s2) := τ ex(s1+s2)1(s1 ∨ s2 ≤ 0) point-wise a.e.
in (s1, s2) ∈ R2 and also in L2(R2). Then conclude z∗
R2 g(s1, s2)dB(s1)dB(s2) =d
N,n(τ ; x) →d I11(g) =d
τ (cid:8)(cid:0) (cid:82) 0
−∞ esxdB(s)(cid:1)2(cid:9) =d τ (Z2 − 1)/(2x) for any x > 0, where Z ∼ N (0, 1). On the
other hand, z+

−∞ esxdB(s)(cid:1)2 − E(cid:0) (cid:82) 0

N,n(τ ; x) → τ /2x and therefore

(cid:82)

N,n(θ; x) → E(cid:2)eiθτ Z2/(2x) − 1 − iθ(τ Z2/(2x))1(1 < β < 2)(cid:3)
Λt,0

for any x > 0, proving the point-wise convergence of the integrands in (4.14) and (4.19). The remaining
(cid:3)
details are similar as in the previous cases and omitted. This ends the proof of Theorem 4.1.

Proof of Theorem 4.2. Consider the decomposition in (4.1), where n−1T t,0
is a sum of i.i.d. r.v.s with ﬁnite variance (σ∗

t )2 = var(a|t|/(1 − a2)) and therefore

N,n(τ ) = ((cid:98)nτ (cid:99)/n) (cid:80)N

i=1 at

i/(1 − a2
i )

n−1N −1/2(cid:0)T t,0

N,n(τ ) − ET t,0

N,n(τ )(cid:1) →fdd τ σ∗
t Z

holds by the classical CLT as N, n → ∞ in arbitrary way and where Z ∼ N (0, 1). Hence, the statement
of the theorem follows from (cid:101)St,0
N,n(1)) =
u=1 X(u)X(u + t)|a] ≤ CN n2E(cid:2)(1 − a)−2 min{1, (n(1 − a))−1}(cid:3), where the last expectation vanishes
N Evar[(cid:80)n
(cid:3)
as n → ∞, due to E(1 − a)−2 < ∞. Theorem 4.2 is proved.

N,n(1) = op(nN 1/2). By Proposition 2.4 (2.29) we have that var( (cid:101)St,0

Figure 1: Density of the limiting random variables in cases [left] (i),(ii), [right] (iv) of Corollary 4.1 for t = 0
and their kernel density estimates constructed from a random sample of size 1000 from (cid:98)γN,n(0, 0) in (1.6)
with N = 5000, a2 ∼ Beta(2, β), ε(0) ∼ N (0, 1).

To illustrate our results, we use a2 ∼ Beta(α, β), α, β > 0, as in [10]. Then condition (1.2) holds with
the same β and we can explicitly compute parameters of the limit distributions in cases (i), (ii), (iv) of
Corollary 4.1. Figure 1 shows the density of the corresponding limiting random variables for α = 2, β = 1.5,

26

0.0000.0250.0500.075−20−1001020Vb*Vb+n = 100n = 5000b = 1.50.000.050.100.15−10−50510s0*Zn = 100n = 5000b = 2.52.5 and t = 0. We also plot the kernel density estimates constructed using 1000 RCAR(1) panels with N =
5000, n = 100, 5000, ε(0) ∼ N (0, 1). More speciﬁcally, we use a random sample of N 1/β((cid:98)γN,n(0, 0) − γ(0, 0))
if β = 1.5 and N 1/2((cid:98)γN,n(0, 0) − γ(0, 0)) if β = 2.5. On the l.h.s. we can see that the empirical distribution
of (cid:98)γN,n(0, 0) is diﬀerent for n = 100, 5000, whereas on the r.h.s. both kernel density estimates are quite close
to the limiting normal density.

In the ﬁnite variance case β > 1, Corollary 4.1 can be used for statistical inference about the covariance
γ(t, 0) = γ(t) in (1.3), provided parameters of the limit distributions are consistently estimated. Denote by

β,ψ(x) := P(V ∗
F ∗

β ≤ x), F +

β,ψ(x) := P(V +

β ≤ x),

x ∈ R,

(4.20)

the c.d.f.s of the above stable r.v.s, which are uniquely determined by β, ψ(1) ≡ ψ in (1.2), see (4.7)–(4.9).
The same is true for the (marginal) distribution Z ∗
β(τ ) of the ‘diagonal intermediate’ process in (2.24). In
Corollary 4.2 we suppose the existence of estimators

ˆβN,n = β + op(1/log N ),
N,n,t = (σ∗
ˆσ2

t )2 + op(1),

ˆψN,n = ψ + op(1),

(4.21)

(4.22)

which is discussed in Remark 4.4 below. Corollary 4.2 omits the ‘intermediate’ case λ∗
because in this case the limit distribution is less tractable and depends on λ∗
a ﬁnite sample.

∞ ∈ (0, ∞), partly
∞ which is diﬃcult to assess in

Corollary 4.2. (i) Let the conditions of Theorem 4.1 (i) be satisﬁed, 1 < β < 2, and ˆβN,n, ˆψN,n be estimators
as in (4.21). Then for any t ∈ Z

supx∈R

(cid:12)
(cid:12)P(cid:0)N 1−1/ ˆβN,n((cid:98)γN,n(t, 0) − γ(t)) ≤ x(cid:1) − F ∗

ˆβN,n, ˆψN,n

(x)(cid:12)

(cid:12) = op(1).

(4.23)

(ii) Let the conditions of Theorem 4.1 (ii) be satisﬁed, 1 < β < 2, and ˆβN,n, ˆψN,n be estimators as in (4.21).
Then for any t ∈ Z

supx∈R

(cid:12)
(cid:12)P(cid:0)N 1−1/ ˆβN,n((cid:98)γN,n(t, 0) − γ(t)) ≤ x(cid:1) − F +

ˆβN,n, ˆψN,n

(x)(cid:12)

(cid:12) = op(1).

(4.24)

(iii) Let the conditions of Theorem 4.2 be satisﬁed, β > 2, and ˆσ2
any t ∈ Z

N,n,t be an estimator as in (4.22). Then for

supx∈R

(cid:12)
(cid:12)P(cid:0)(cid:0) N
ˆσ2

N,n,t

(cid:1)1/2((cid:98)γN,n(t, 0) − γ(t)) ≤ x(cid:1) − P(Z ≤ x)(cid:12)

(cid:12) = op(1),

Z ∼ N (0, 1).

(4.25)

Proof. Consider (4.23). Write N 1−1/ ˆβN,n((cid:98)γN,n(t, 0) − γ(t)) = N 1−1/β((cid:98)γN,n(t, 0) − γ(t)) + ξN,n, where
ξN,n := (N (1/β)−(1/ ˆβN,n) − 1)N 1−1/β((cid:98)γN,n(t, 0) − γ(t)) = op(1) due to (4.21) and Corollary 4.1(i). Therefore,
supx∈R |P(N 1−1/ ˆβN,n((cid:98)γN,n(t, 0) − γ(t)) ≤ x) − F ∗
(x)| = op(1)
follows from (4.21) and continuity of continuity of the c.d.f. F ∗
β,ψ in β, ψ. This proves (4.23). The proof of
(cid:3)
(4.24), (4.25) is analogous.

β,ψ(x)| → 0. Relation supx∈R |F ∗

β,ψ(x) − F ∗

ˆβN,n, ˆψN,n

Remark 4.3. Using Corollary 4.2 we can construct asymptotic conﬁdence intervals for γ(t), as fol-
lows. For α ∈ (0, 1) denote by qβ,ψ(α) the α-quantile of the c.d.f. F ∗
β,ψ in (4.20). Then, since α =
(α)) a.s., P(N 1−1/ ˆβN,n((cid:98)γN,n(t, 0) − γ(t)) ≤ q ˆβN,n, ˆψN,n
F ∗
(α)) − α = op(1) follows from

(q ˆβN,n, ˆψN,n

ˆβN,n, ˆψN,n

27

(4.23); moreover since the above quantity is non-random, we get that |P(N 1−1/ ˆβN,n((cid:98)γN,n(t, 0) − γ(t)) ≤
q ˆβN,n, ˆψN,n

(α)) − α| = o(1), implying that

(cid:104)
(cid:98)γN,n(t, 0) − N (1/ ˆβN,n)−1q ˆβN,n, ˆψN,n

(1 − α/2), (cid:98)γN,n(t, 0) − N (1/ ˆβN,n)−1q ˆβN,n, ˆψN,n

(cid:105)
(α/2)

is the asymptotic conﬁdence interval for γ(t), for any conﬁdence level α ∈ (0, 1). Analogous conﬁdence
intervals for γ(t) can be deﬁned in the case (4.24); in the case (4.25) they follow in a standard way.

Remark 4.4. Estimation of the tail parameter β in the RCAR(1) panel model was studied in [16]. Particu-
larly, [16] developed a modiﬁed version ˆβN,n of the Goldie–Smith estimator in [8] and proved its asymptotic
normality, under additional (rather stringent) conditions on the mutual increase rate of N and n. A similar
estimator ˆψN,n can be deﬁned following [8]. We expect that these estimators satisfy the consistency as in
(4.21) under much weaker assumptions on N, n. Finally, for t ≥ 0 the estimator ˆσ2
N,n,t in (4.22) can be deﬁned
(see the proof in Appendix) as

ˆσ2
N,n,t

:=

1
N

N
(cid:88)

i=1

(cid:16) 1
n

n−t
(cid:88)

k=1

Xi(k)Xi(k + t)

(cid:17)2

−

(cid:16) 1
N n

N
(cid:88)

n−t
(cid:88)

i=1

k=1

Xi(k)Xi(k + t)

(cid:17)2

.

(4.26)

Remark 4.5. In general, in the RCAR(1) model the autoregressive coeﬃcient a can take a value from (−1, 1).
In the latter case if the distribution of a is suﬃciently dense at −1, the (unconditional) autocovariance function
of the RCAR(1) process oscillates when decaying slowly, which is usually referred to as seasonal long memory.
The restriction a ∈ [0, 1) in the present paper (as well as in [23], [16] and some other papers) is basically
due to technical reasons. We expect that, under assumption (1.2), most of our results hold in the general
case a ∈ (−1, 1) provided the concentration of the mixing distribution near −1 is not too strong, e.g., if
E(1 + a)−β(cid:48) < ∞ for some β(cid:48) > β.

A Appendix

Proof of Proposition 2.2. (i) The existence of Zβ follows from

Jβ :=

(cid:90)

Lc
1

|z(τ ; x1, x2)|2dµβ < ∞

(A.1)

and µβ(L1) < ∞. We have µβ(L1) = ψ(1)2 (cid:82)
(cid:82) x1
(cid:82) x1
0 xβ−1
0 1(x2 < 1/x2
(cid:82) ∞
1 x−β−1
Consider (A.1). Then

(cid:1) < ∞ since β > 0.

dx2 = C(cid:0) (cid:82) 1

0 xβ−1

1)xβ−1

dx1

dx1

R2
+

1

2

1

2

1(x1x2(x1 + x2) < 1)(x1x2)β−1dx1dx2 ≤ C (cid:82) ∞
(cid:82) 1/x2
1
0

dx2 + (cid:82) ∞

(cid:1) ≤ C(cid:0) (cid:82) 1

1 xβ−1

xβ−1
2

dx1

dx2

0 x2β−1

0 xβ−1

1

1

1

dx1 +

dx1

Jβ = C

(cid:90)

R2
+

1(x1x2(x1 + x2) > 1)E|z(τ ; x1, x2)|2(x1x2)β−1dx1dx2,

where

E|z(τ ; x1, x2)|2 =

=

(cid:90)

(0,τ ]2

1
4x1x2

2
(cid:89)

i=1
(cid:90)

(0,τ ]2

E[Yi(u1; xi)Yi(u2; xi)]du1du2

e−(x1+x2)|u1−u2|du1du2 ≤

(cid:16)

Cτ 2
x1x2

1 ∧

1
τ (x1 + x2)

(cid:17)

.

(A.2)

28

Hence,

Jβ ≤ C

≤ C

= C

(cid:90)

R2
+

(cid:90)

R2
+
(cid:16) (cid:90) 1

1(x1x2(x1 + x2) > 1)(x1 + x2)−1(x1x2)β−2dx1dx2

1(x2 > x1, x1x2

2 > 1)xβ−2

1 xβ−3

2

dx1dx2

xβ−2
1

dx1

(cid:90) ∞

x−1/2
1

xβ−3
2

dx2 +

(cid:90) ∞

1

xβ−2
1

dx1

(cid:90) ∞

x1

(cid:17)

xβ−3
2

dx2

< ∞

0

if 0 < β < 3/2. The remaining facts in (i) are easy and we omit the details.

(ii) Similarly as in ([20], proof of Proposition 3.1(ii)) it suﬃces to show for any 0 < p < 2β that

∞ > Jp,β(τ ) :=


(cid:82)

R2
+
(cid:82)

R2
+

E|z(τ ; x1, x2)|p(x1x2)β−1dx1dx2,
E(cid:2)|z(τ ; x1, x2)|p ∨ |z(τ ; x1, x2)|2(cid:3)(x1x2)β−1dx1dx2, p > 2.

0 < p ≤ 2,

(A.3)

Let ﬁrst 0 < p ≤ 2. Using E|z(τ ; x1, x2)|p ≤ (E|z(τ ; x1, x2)|2)p/2 and (A.2), we obtain

Jp,β(τ ) ≤ C

(cid:90)

(cid:16) (cid:90)

R2
+

(0,τ ]2

where

e−(x1+x2)|u1−u2|du1du2

(cid:17)p/2

(x1x2)β−1−p/2dx1dx2 =: Cτ 2(p−β)Ip,β,

(A.4)

Ip,β ≤

(cid:90)

(cid:16)

1 ∧

R2
+
(cid:90) ∞

(cid:90) x1

1
x1 + x2
(cid:16)

1 ∧

≤ C

0
(cid:90) ∞

0
(cid:16)

1 ∧

0

1
x1

= C

1
x1
(cid:17)p/2

(cid:17)p/2

(x1x2)β−1−p/2dx1dx2

(cid:17)p/2

(x1x2)β−1−p/2dx1dx2

x2β−p−1
1

dx1 < ∞

(A.5)

if p/2 < β < 3p/4, thus proving (A.3) for 0 < p ≤ 2.

Next for 2 < p < 3 we need the inequality for double Itˆo-Wiener integrals: for any p ≥ 2, g ∈ L2(R2)

E(cid:12)
(cid:12)

p ≤ C(cid:0)E(cid:12)
R2 g(s1, s2)dB1(s1)dB2(s2)(cid:12)
(cid:82)
(cid:12)
(cid:12)

(cid:82)

R2 g(s1, s2)dB1(s1)dB2(s2)(cid:12)
2(cid:1)p/2 = C(cid:0) (cid:82)
(cid:12)

R2 |g(s1, s2)|2ds1ds2

(cid:1)p/2.(A.6)

Indeed, by using Gaussianity and independence of B1, B2 and Minkowski
(cid:82)
R2 g(s1, s2)dB1(s1)dB2(s2) we obtain

inequality for I2(g)

:=

(cid:0)E|I2(g)|p(cid:1)2/p = (cid:0)EB1EB2

≤ CEB2

(cid:8)EB1
= CEB2EB1

(cid:2)|I2(g)|p(cid:12)

(cid:12)B1
(cid:2)|I2(g)|p(cid:12)
(cid:2)|I2(g)|2(cid:12)
(cid:12)B2

(cid:3)(cid:1)2/p ≤ C(cid:0)EB1(EB2
(cid:3)}2/p ≤ CEB2
(cid:12)B2
(cid:3) = CE|I2(g)|2.

(cid:2)|I2(g)|2(cid:12)

(cid:12)B1
(cid:2)|I2(g)|2(cid:12)

(cid:8)(cid:0)EB1

(cid:12)B2

(cid:3))p/2(cid:1)2/p

(cid:3)(cid:1)p/2(cid:9)2/p

Using inequality (A.6) and (A.4), (A.5) we obtain

Jp,β(τ ) ≤ C

(cid:16) (cid:90)

R2
+

E|z(τ ; x1, x2)|p(x1x2)β−1dx1dx2 +

E|z(τ ; x1, x2)|2(x1x2)β−1dx1dx2

(cid:17)

(cid:90)

R2
+

≤ C(Ip,β(τ ) + I2,β(τ )) < ∞

if p/2 < β < 3p/4, thus proving (A.3) and part (ii).

29

(iii) Follows from stationarity of increments of Zβ (part (i)) and J2,β(τ ) = σ2
(A.2),

∞τ 2(2−β), where according to

σ2
∞ =

(cid:90)

R2
+

Ez2(1; x1, x2)dµβ

= (ψ(1)/2)2

(cid:90)

(0,1]2

du1du2

(cid:90) ∞

(cid:0)

0

e−x|u1−u2|xβ−2dx(cid:1)2

= (ψ(1)/2)2Γ(β − 1)2

(cid:90)

(0,1]2

|u1 − u2|2(1−β)du1du2 = (ψ(1)/2)2Γ(β − 1)2/((2 − β)(3 − 2β)).

(iv) Follows from stationarity of increments, E|Zβ(τ )|p ≤ CJp,β(τ ), 1 < p ≤ 2, where Jp,β(τ ) is the same as
in (A.3), and Kolmogorov’s criterion; c.f ([20], proof of Proposition 3.1(iv)).

(v) The proofs are very similar to those of Theorem 3.1 (i), (ii), hence we omit some details. For notational
simplicity, we only prove one-dimensional convergence at τ > 0.

Proof of (2.18). As b → 0, consider

Φb(θ) := log E exp{iθbβ−2Zβ(bτ )} = ψ(1)2

(cid:90)

R2
+

EΨ(θbβ−2z(bτ ; x1, x2))(x1x2)β−1dx1dx2,

where Ψ(z) := eiz − 1 − iz, z ∈ R. Since b−2z(bτ ; x1, x2) =d z(τ ; bx1, bx2), rewrite

Φb(θ) = ψ(1)2b−2β

(cid:90)

R2
+

EΨ(θbβz(τ ; x1, x2))(x1x2)β−1dx1dx2,

where b−2βΨ(θbβz(τ ; x1, x2)) → −(θ2/2)z2(τ ; x1, x2) a.s. Note |b−2βΨ(θbβz(τ ; x1, x2))| ≤ (θ2/2)z2(τ ; x1, x2),
where the dominating function satisﬁes (A.2) and (2.9). Hence, by the dominated convergence theorem,

Φb(θ) → −(θ2/2)ψ(1)2

(cid:90)

R2
+

which ﬁnishes the proof.

Ez2(τ ; x1, x2)(x1x2)β−1dx1dx2 = log E{iθσ∞B2−β(τ )},

Proof of (2.19) follows that of Thm. 3.1(i), case 0 < β < 1. As b → 0, consider

Φb(θ) := log Eeiθb−1(log b−1)−1/2β Zβ (bτ ) =

ψ(1)2
log b−1

(cid:90)

R2
+

E[eiθzb(τ ;x1,x2) − 1](x1x2)β−1dx1dx2,

where

satisﬁes

see (A.2). Split

zb(τ ; x1, x2) := b−1(log b−1)−1/(2β)z(cid:0)bτ ; (log b−1)−1/(2β)x1, (log b−1)−1/(2β)x2

(cid:1)

E|zb(τ ; x1, x2)|2 ≤

(cid:16)

C
x1x2

1 ∧

b−1(log b−1)1/(2β)
x1 + x2

(cid:17)

,

(A.7)

Φb(θ) =

ψ(1)2
log b−1

(cid:90)

R2
+

(cid:0)1(1 < x1 + x2 < b−1) + 1(x1 + x2 > b−1) + 1(x1 + x2 < 1)(cid:1)

× E[eiθzb(τ ;x1,x2) − 1](x1x2)β−1dx1dx2 =:

3
(cid:88)

i=1

Li.

30

Using (A.7), we can show that Li, i = 2, 3 are remainders. By change of variables: y = x1 + x2, x1 = yw and
then w = z/y2, we rewrite the main term

L1 =

1
log b−1

(cid:90) b−1

1

Vb(θ; y)

dy
y

,

Vb(θ; y) := 2ψ(1)2

(cid:90) y2/2

0

Λb(z; y)zβ−1(cid:0)1 −

(cid:1)β−1dz

z
y2

(A.8)

with Λb(z; y) := E[exp{iθzb(τ ; z
y , y(1 − z
0 < y < b−1. Here the dominating bound is a consequence of (A.7). Then

y2 ))} − 1], which satisﬁes |Λb(z; y)| ≤ C(1 ∧ 1

z ) for all 0 < z

y2 < 1
2 ,

L1 → log Eeiθτ V2β = 2ψ(1)2

(cid:90) ∞

0

Λ(z)zβ−1dz,

where Λ(z) := E[eiθτ Z1Z2/(2

√

z) − 1] with Zi ∼ N (0, 1), i = 1, 2 being independent r.v.s, follows from

lim
y→∞,y=O(b−1)

Λb(z; y) = Λ(z),

∀z > 0,

(A.9)

(A.10)

for more details we refer the reader to the proof of Thm. 3.1 (i) case 0 < β < 1. More precisely, (A.10)
says that for every (cid:15) > 0 there exists a small δ > 0 such that for all 0 < b < δ, if δ−1 < y < b−1, then
|Λb(z; y) − Λ(z)| < (cid:15). To show (A.10), note zb(τ ; z
y2 )) = I12(hb(·; τ ; z)) is a double Itˆo-Wiener
stochastic integral w.r.t. independent standard Brownian motions {Bi(s), s ∈ R}, i = 1, 2 for

y , y(1 − z

hb(s1, s2; τ ; z) := (log b−1)−1/(2β)

(cid:90) τ

2
(cid:89)

0

i=1

e− 1

αi

(bu−si)1(si < bu)du,

s1, s2 ∈ R,

α1 := (log b−1)1/(2β)y/z, α2 := (log b−1)1/(2β)/y(cid:48),

We have that zb(τ ; z

y , y(1 − z

y2 )) =d I12((cid:101)hb(·; τ ; z)), where

y(cid:48) := y(cid:0)1 −

(cid:1).

z
y2

(cid:101)hb(s1, s2; τ ; z) :=

√

α1α2hb(α1s1, α2s2; τ ; z)

=

(cid:114) y
zy(cid:48)

(cid:90) τ

2
(cid:89)

0

i=1

e− 1

αi

(bu−αisi)1(αisi < bu)du,

s1, s2 ∈ R.

If b → 0, y, y(cid:48) → ∞ so that y/y(cid:48) → 1 and b/αi → 0, i = 1, 2, then (cid:107)(cid:101)hb(·; τ ; z) − h(·; τ ; z)(cid:107) → 0 with

h(s1, s2; τ ; z) :=

τ
√

z

2
(cid:89)

i=1

esi1(si < 0),

s1, s2 ∈ R,

(A.11)

implies the convergence zb(τ ; z
z. Conditions on b, y, y(cid:48) are obvi-
ously satisﬁed due to y, y(cid:48) = O(b−1) = o(b−1(log b−1)1/(2β)). This proves (A.10) and (A.9), thereby completing
the proof of of (2.19).

y2 )) →d I12(h(·; τ ; z)) =d τ Z1Z2/2

y , y(1 − z

√

Proof of (2.20) follows that of Theorem 3.1 (ii). We will prove that as b → ∞,

log Eeiθb−1/2Zβ (bτ ) = ψ(1)2

(cid:90)

R2
+
(cid:90)

R2
+

→ ψ(1)2

E(cid:2) exp (cid:8)iθb−1/2z(bτ ; x1, x2)(cid:9) − 1(cid:3)(x1x2)β−1dx1dx2

(A.12)

(cid:104)

(cid:110)

−

exp

θ2τ
4x1x2(x1 + x2)

(cid:111)

(cid:105)
− 1

(x1x2)β−1dx1dx2 = log EeiθA1/2B(τ ).

31

By (A.2), we have that E[exp{iθb−1/2z(bτ ; x1, x2)} − 1] ≤ C min{1, (x1x2(x1 + x2))−1}. In view of (2.8), the
dominated convergence theorem applies if the integrands on the r.h.s. of (A.12) converge pointwise, i.e. for
every (x1, x2) ∈ R2
+,

To simplify notation, let τ = 1 and all b ∈ N. Deﬁne

b−1/2z(bτ ; x1, x2) →d

B(τ )
(cid:112)2x1x2(x1 + x2)

.

(A.13)

z+
b (x1, x2) :=

(cid:90) b

(cid:90) b

0

0

f (s1, s2)dB1(s1)dB2(s2),

f (s1, s2) := b−1/2

(cid:90) b

2
(cid:89)

0

i=1

e−xi(u−si)1(u > si)du,

b (x1, x2) := b−1/2z(b; x1, x2) − z+

and z−
we only need to prove that

b (x1, x2). Since E(z−

b (x1, x2))2 = O(b−1) implies z−

b (x1, x2) = op(1),

z+
b (x1, x2) →d N

(cid:16)

0,

1
2x1x2(x1 + x2)

(cid:17)

as b → ∞.

(A.14)

Write z+

b (x1, x2) = (cid:80)b
(cid:90) k

Zk :=

k=1 Zk as a sum of a sum of a zero-mean square-integrable martingale diﬀerence array

(cid:90) k−1

f (s1, s2)dB1(s1)dB2(s2) +

k−1
(cid:90) k

0
(cid:90) k

+

k−1

k−1

f (s1, s2)dB1(s1)dB2(s2)

(cid:90) k−1

(cid:90) k

0

k−1

f (s1, s2)dB1(s1)dB2(s2)

w.r.t. the ﬁltration Fk generated by {Bi(s), 0 ≤ s ≤ k, i = 1, 2}, k = 0, . . . , b. By the martingale CLT in Hall
and Heyde [11], (A.14) then follows from

b
(cid:88)

k=1

E[Z2

k |Fk−1] →p

1
2x1x2(x1 + x2)

and

b
(cid:88)

k=1

E[Z2

k 1(|Zk| > (cid:15))] → 0

for any (cid:15) > 0.

(A.15)

Since (cid:80)b
(cid:80)b

k=1(E[Z2

k = (cid:82) b
k=1 EZ2
0
k |Fk−1] − EZ2
k ), where

(cid:82) b
0 f 2(s1, s2)ds1ds2 = E(z+

b (x1, x2))2 → (2x1x2(x1 + x2))−1, consider Rb

:=

E[Z2

k |Fk−1] =

(cid:90) k

(cid:16) (cid:90) k−1

f (s1, s2)dB2(s2)

(cid:17)2

ds1 +

k−1
(cid:90) k

+

0
(cid:90) k

k−1

k−1

f 2(s1, s2)ds1ds2.

(cid:90) k

(cid:16) (cid:90) k−1

k−1

0

f (s1, s2)dB1(s1)

(cid:17)2

ds2

By rewriting Rb =d
c2(s1, s2) = (cid:82) b

(cid:100)s1∨s2(cid:101) f (s, s1)f (s, s2)ds and using the elementary bound:

(cid:80)2

i=1

(cid:82) b
0

0 ci(s1, s2)dBi(s1)dBi(s2) with c1(s1, s2) = (cid:82) b
(cid:82) b

(cid:100)s1∨s2(cid:101) f (s1, s)f (s2, s)ds,

f (s1, s2) ≤ Cb−1/2(cid:0)e−x1(s2−s1)1(s1 < s2) + e−x2(s1−s2)1(s1 ≥ s2)(cid:1),

0 ≤ s1, s2 ≤ b,

(A.16)

(cid:82) b
0

(cid:82) b
0 c2

i=1

we obtain E|Rb|2 = (cid:80)2
proof of the ﬁrst relation in (A.15). Finally, using (A.6), (A.16), we obtain (cid:80)b
which implies the second relation in (A.15) and completes the proof of (A.14).
Proposition 2.2 is proved.

i (s1, s2)ds1ds2 = O(b−1) = o(1), which proves Rb = op(1) and completes the
k=1 E|Zk|4 = O(b−1) = o(1),

(cid:3)

32

Proof of Proposition 2.3. (i) Split Z ∗

β(τ ) = (cid:101)Z ∗

β(τ ) + τ V +

β with

(cid:101)Z ∗

β(τ ) :=

V +
β :=

(cid:90)

R+×C(R)

(cid:90)

R+×C(R)

(cid:16)

z∗(τ ; x) −

(cid:17)

τ
2x

d(M∗

β − EM∗

β1(1 < β < 2)),

1
2x

d(M∗

β − EM∗

β1(1 < β < 2)),

β is a Poisson random measure on R+ × C(R) with mean µ∗
0 min{1, x−1}xβ−1dx < ∞ if β ∈ (0, 1) and (cid:82) ∞

β follows from (cid:82) ∞

β = EM∗

β given in (2.21). The existence
0 min{x−1, x−2}xβ−1dx < ∞ if β ∈ (1, 2).

where M∗
of V +
The process (cid:101)Z ∗

β is well-deﬁned if

J ∗
p,β(τ ) :=

(cid:90)

R+×C(R)

|z∗(τ ; x) − τ /2x|pdµ∗

β = C

(cid:90) ∞

0

E|z∗(τ ; x) − τ /2x|pxβ−1dx < ∞,

(A.17)

where 0 < p ≤ 1 for β ∈ (0, 1) and 1 ≤ p ≤ 2 for β ∈ (1, 2). We have E|z∗(τ ; x) − τ /2x|p ≤ (var(z∗(τ ; x)))p/2,
where

var(z∗(τ ; x)) =

(cid:90)

(0,τ ]2
(cid:90)

= 2

cov(Y 2(u1; x), Y 2(u2; x))du1du2

(cid:90)

R2

ds1ds2e−2x(u1+u2−s1−s2)1(s1 ∨ s2 < u1 ∧ u2)

(0,τ ]2
(cid:90)

=

1
2x2

(0,τ ]2

e−2x|u1−u2|du1du2 =

1
8x4 (2xτ − 1 + e−2xτ ) ≤ C

(cid:16)

τ 2
x2

1 ∧

(cid:17)

,

1
xτ

(A.18)

p,β(τ ) ≤ Cτ 2p−β < ∞ for p < β < 3p/2. This completes the proof of part (i).
β |p < ∞ for 0 < p < β, since V +

β(τ )|p < ∞
p,β(τ ) < ∞ in (A.17), where p is suﬃciently close to β and such that 0 < p < β < 3p/2. This

β is a β-stable random variable. Similarly to (A.3), E| (cid:101)Z ∗

hence, J ∗
(ii) E|V +
follows from J ∗
proves part (ii).

(iii) Follows from part (ii) by Kolmogorov’s criterion, similarly as in the proof of Proposition 2.2.

(iv) For notational simplicity, we only prove one-dimensional convergence at τ > 0. We have
log E exp{iθb−1Zβ(bτ )} = ψ(1) (cid:82)

Λb(x)xβ−1dx, where

R+

Λb(x) := E(cid:2) exp (cid:8)iθb−1z∗(bτ ; x)(cid:9) − 1 − iθb−1z∗(bτ ; x)1(1 < β < 2)(cid:3).

Substituting E|z∗(bτ ; x)| ≤ (E|z∗(bτ ; x)|2)1/2 and E|z∗(bτ ; x)|2 = var(z∗(bτ ; x)) + (bτ /2x)2 ≤ C(b/x)2 by
(A.18) into

|Λb(x)| ≤ C






min (cid:8)1, b−1E|z∗(bτ ; x)|(cid:9),
0 < β < 1,
min (cid:8)b−1E|z∗(bτ ; x)|, b−2E|z∗(bτ ; x)|2(cid:9), 1 < β < 2,

we obtain the bounds: |Λb(x)| ≤ C min{1, x−1} if 0 < β < 1, and |Λb(x)| ≤ C min{x−1, x−2} if 1 < β < 2.
The result then follows from the dominated convergence theorem once we show that for all x ∈ R+,

Λb(x) →




exp{iθτ /(2x)} − 1 − (iθτ /(2x))1(1 < β < 2)

as b → ∞,



E[exp{iθZ2τ /(2x)} − 1 − (iθZ2τ /(2x))1(1 < β < 2)]

as b → 0,

(A.19)

33

where Z ∼ N (0, 1). Using (A.18), we get E|b−1z∗(bτ ; x) − (τ /2x)|2 = b−2 var(z∗(bτ ; x)) ≤ Cb−1 = o(1) as
b → ∞, which implies the ﬁrst convergence in (A.19). To prove the second convergence in (A.19), note
Z/

2x =d Y(0; x). It suﬃces to show that as b → 0,

√

(cid:12)
E|b−1z∗(bτ ; x) − τ Y 2(0; x)| = E
(cid:12)
(cid:12)

(cid:90) τ

0

(cid:12)
(Y 2(bu; x) − Y 2(0; x))du
(cid:12)
(cid:12) ≤

(cid:90) τ

0

E|Y 2(bu; x) − Y 2(0; x)|du = o(1).

Factorizing the diﬀerence of squares and applying the Cauchy-Schwarz inequality, this follows from

E|Y(bu; x) − Y(0; x)|2 =

(cid:90) bu

0

e−2xsds +

1
2x

(e−xbu − 1)2 ≤ Cbu.

Prop. 2.3 is proved.

(cid:3)

Calculation of the constant σ0 in Proposition 2.2 (v). We have

σ0 ·

22β/3
ψ(1)2

=

=
u2=u1v2

=
u1=v−1/3
1

=

=

=

=
v2=s−1−1

=

(cid:90)

R2
+

(cid:90)

R2
+
(cid:90)

1
3

R2
+

0

(cid:90)

1
3
R2
+
Γ(1 − 2β
3 )
3
Γ(1 − 2β
3 )
2β
Γ(1 − 2β
3 )
2β
Γ(1 − 2β

(cid:0)1 − exp{−(u1 + u2)−1(u1u2)−1}(cid:1)(u1u2)β−1du1du2

(cid:0)1 − exp{−u−3

1 (1 + v2)−1v−1

2 }(cid:1)u2β−1

1

vβ−1
2

du1dv2

(cid:0)1 − exp{−v1(1 + v2)−1v−1

2 }(cid:1)v−2β/3−1

1

vβ−1
2

dv1dv2

(cid:16) (cid:90) 1/((1+v2)v2)

(cid:17)
e−v1tdt

v−2β/3
1

vβ−1
2

dv1dv2

(cid:90) ∞

0
(cid:90) ∞

vβ−1
2

dv2

(cid:90) 1/((1+v2)v2)

t2β/3−1dt

0

(1 + v2)−2β/3vβ/3−1

2

dv2

s2β/3(s−1 − 1)β/3−1s−2ds

0
(cid:90) 1

0

3 , β
3 )

3 ) B( β
2β

.

Proof of (4.22). By Corollary 4.1 (iv),
(4.22) for (4.26) follows from

1
N n

(cid:80)N

i=1

(cid:80)n−t

k=1 Xi(k)Xi(k + t) →p γ(t) = E at

1−a2 . Hence, relation

By the LLN, 1
N
show that

(cid:80)N

i=1( at

i
1−a2
i

1
N

N
(cid:88)

i=1

(cid:16) 1
n

n−t
(cid:88)

k=1

Xi(k)Xi(k + t)

(cid:17)2

→p E

(cid:16) at

1 − a2

(cid:17)2

.

(A.20)

)2 →p E( at

1−a2 )2. Therefore by Minkowski’s inequality, for (A.20) we only need to

1
N

N
(cid:88)

i=1

(cid:16) 1
n

n
(cid:88)

k=1

Xi(k)Xi(k + t) −

(cid:17)2

at
i
1 − a2
i

= op(1).

By taking expectations this follows from

E

(cid:16) 1
n

n
(cid:88)

k=1

Xi(k)Xi(k + t) −

at
i
1 − a2
i

(cid:17)2

=

1
n2 E var

(cid:104) n
(cid:88)

k=1

Xi(k)Xi(k + t)

(cid:105)

(cid:12)
(cid:12)
(cid:12)ai

= o(1),

(A.21)

34

Using cov[Xi(k)Xi(k + t), Xi(k(cid:48))Xi(k(cid:48) + t)|ai] = a2(|k−k(cid:48)|+t)
as in (2.29) we see that the l.h.s. of (A.21) does not exceed CE[
n → ∞ by the dominated convergence theorem, due to E(1 − a)−2 < ∞.

cum4 + a2|k−k(cid:48)|+a2 max{|k−k(cid:48)|,t}
(1−a2)2
(1−ai)2 min{1,

1−a4

and the same bound
n(1−ai) }] which vanishes as
(cid:3)

1

1

Acknowledgments

The authors are grateful to an anonymous referee and associate editor for useful comments. Vytaut˙e Pili-
pauskait˙e acknowledges the ﬁnancial support from the project “Ambit ﬁelds: probabilistic properties and
statistical inference” funded by Villum Fonden.

References

[1] Beran, J., Sch¨utzner, M. and Ghosh, S. (2010) From short to long memory: Aggregation and estimation. Comput.

Stat. Data Anal. 54, 2432–2442.

[2] Bhansali, R.J., Giraitis, L. and Kokoszka, P.S. (2007) Convergence of quadratic forms with nonvanishing diagonal.

Statist. Probab. Lett. 77, 726–734.

[3] Celov, D., Leipus, R. and Philippe, A. (2007) Time series aggregation, disaggregation and long memory. Lithuanian

Math. J. 47, 379–393.

[4] Celov, D., Leipus, R. and Philippe, A. (2010) Asymptotic normality of the mixture density estimator in a disaggre-

gation scheme. J. Nonparametric Statist. 22, 425–442.

[5] Davis, R.A. and Mikosch, T. (1998) The sample autocorrelations of heavy-tailed processes with applications to

ARCH. Ann. Statist. 26, 2049–2080.

[6] Davis, R.A. and Resnick, S.I. (1986) Limit theory for the sample covariance and correlation functions of moving

averages. Ann. Statist. 14, 533–588.

[7] Giraitis, L., Koul, H.L. and Surgailis, D. (2012) Large Sample Inference for Long Memory Processes. London:

Imperial College Press.

[8] Goldie, C.M. and Smith, R.L. (1987) Slow variation with remainder: theory and applications. Q. J. Math. 38,

45–71.

[9] Gon¸calves, E. and Gouri´eroux, C. (1988) Aggr´egation de processus autoregressifs d’ordre 1. Annales d’Economie et

de Statistique 12, 127–149.

[10] Granger, C.W.J. (1980) Long memory relationship and the aggregation of dynamic models. J. Econometrics 14,

227–238.

[11] Hall, P. and Heyde, C.C. (1980) Martingale Limit Theory and Its Applications. Academic Press, New York.

[12] Horv´ath, L. and Kokoszka, P. (2008) Sample autocovariances of long-memory time series. Bernoulli 14, 405–418.

[13] Leipus, R., Oppenheim, G., Philippe, A. and Viano, M.-C. (2006) Orthogonal series density estimation in a

disaggregation scheme. J. Statist. Plan. Inf. 136, 2547–2571.

[14] Leipus, R., Philippe, A., Puplinskait˙e, D. and Surgailis, D. (2014) Aggregation and long memory: recent develop-

ments. J. Indian Statist. Assoc. 52, 71–101.

[15] Leipus, R., Philippe, A., Pilipauskait˙e, V. and Surgailis, D. (2017). Nonparametric estimation of the distribution

of the autoregressive coeﬃcient from panel random-coeﬃcient AR(1) data. J. Multiv. Anal. 153, 121–135.

35

[16] Leipus, R., Philippe, A., Pilipauskait˙e, V. and Surgailis, D. (2019) Estimating long memory in panel random-

coeﬃcient AR(1) data. Preprint. arXiv:1710.09735

[17] Mikosch, T., Resnick, S., Rootz´en, H. and Stegeman, A. (2002) Is network traﬃc approximated by stable L´evy

motion or fractional Brownian motion? Ann. Appl. Probab. 12, 23–68.

[18] Oppenheim, G. and Viano, M.-C. (2004) Aggregation of random parameters Ornstein-Uhlenbeck or AR processes:

some convergence results. J. Time Ser. Anal. 25, 335–350.

[19] Philippe, A., Puplinskait˙e, D. and Surgailis, D. (2014) Contemporaneous aggregation of triangular array of random-

coeﬃcient AR(1) processes. J. Time Series Anal. 35, 16–39.

[20] Pilipauskait˙e, V. and Surgailis, D. (2014) Joint temporal and contemporaneous aggregation of random-coeﬃcient

AR(1) processes. Stochastic Process. Appl. 124, 1011–1035.

[21] Pilipauskait˙e, V. and Surgailis, D. (2015) Joint aggregation of random-coeﬃcient AR(1) processes with common

innovations.Statist. Probab. Letters 101, 73–82.

[22] Pilipauskait˙e, V. and Surgailis, D. (2016) Anisotropic scaling of random grain model with application to network

traﬃc. J. Appl. Probab. 53, 857–879.

[23] Pilipauskait˙e, V. and Surgailis, D. (2017) Scaling transition for nonlinear random ﬁelds with long-range dependence.

Stochastic Process. Appl. 127, 2751–2779.

[24] Puplinskait˙e, D. and Surgailis, D. (2009) Aggregation of random coeﬃcient AR(1) process with inﬁnite variance

and common innovations. Lithuanian Math. J. 49, 446–463.

[25] Puplinskait˙e, D. and Surgailis, D. (2010) Aggregation of random coeﬃcient AR(1) process with inﬁnite variance

and idiosyncratic innovations. Adv. Appl. Probab. 42, 509–527.

[26] Puplinskait˙e, D. and Surgailis, D. (2015) Scaling transition for long-range dependent Gaussian random ﬁelds. Stoch.

Process. Appl. 125, 2256–2271.

[27] Puplinskait˙e, D. and Surgailis, D. (2016) Aggregation of autoregressive random ﬁelds and anisotropic long-range

dependence. Bernoulli 22, 2401–2441.

[28] Rajput, B.S. and Rosinski, J. (1989) Spectral representations of inﬁnitely divisible processes. Probab. Theory

Related Fields 82, 451–487.

[29] Robinson, P.M. (1978) Statistical inference for a random coeﬃcient autoregressive model. Scand. J. Stat. 5, 163–

168.

[30] Samorodnitsky, G. and Taqqu, M.S. (1994) Stable Non-Gaussian Random Processes. Chapman and Hall, New

York.

[31] Surgailis, D. (2004) Stable limits of sums of bounded functions of long memory moving averages with ﬁnite variance.

Bernoulli 10, 327–355.

[32] Zaﬀaroni, P. (2004) Contemporaneous aggregation of linear dynamic models in large economies. J. Econometrics

120, 75–102.

[33] Zaﬀaroni, P. (2007) Aggregation and memory of models of changing volatility. J. Econometrics 136, 237–249.

36

