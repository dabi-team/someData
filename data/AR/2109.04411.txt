Non-autoregressive End-to-end Speech Translation
with Parallel Autoregressive Rescoring

Hirofumi Inaguma, Student Member, IEEE, Yosuke Higuchi, Student Member, IEEE,
Kevin Duh, Member, IEEE Tatsuya Kawahara, Fellow, IEEE and Shinji Watanabe, Senior Member, IEEE

1

1
2
0
2

p
e
S
9

]
S
A
.
s
s
e
e
[

1
v
1
1
4
4
0
.
9
0
1
2
:
v
i
X
r
a

Abstract—This article describes an efﬁcient end-to-end speech
translation (E2E-ST) framework based on non-autoregressive
(NAR) models. End-to-end speech translation models have several
advantages over traditional cascade systems such as inference
latency reduction. However, conventional AR decoding methods
are not fast enough because each token is generated incremen-
tally. NAR models, however, can accelerate the decoding speed
by generating multiple tokens in parallel on the basis of the
token-wise conditional independence assumption. We propose a
uniﬁed NAR E2E-ST framework called Orthros, which has an
NAR decoder and an auxiliary shallow AR decoder on top of
the shared encoder. The auxiliary shallow AR decoder selects the
best hypothesis by rescoring multiple candidates generated from
the NAR decoder in parallel (parallel AR rescoring). We adopt
conditional masked language model (CMLM) and a connectionist
temporal classiﬁcation (CTC)-based model as NAR decoders
for Orthros, referred to as Orthros-CMLM and Orthros-CTC,
respectively. We also propose two training methods to enhance the
CMLM decoder. Experimental evaluations on three benchmark
datasets with six language directions demonstrated that Orthros
achieved large improvements in translation quality with a very
small overhead compared with the baseline NAR model. More-
over, the Conformer encoder architecture enabled large quality
improvements, especially for CTC-based models. Orthros-CTC
with the Conformer encoder increased decoding speed by 3.63×
on CPU with translation quality comparable to that of an AR
model.

Index

Terms—End-to-end

speech

translation,

non-

autoregressive decoding, rescoring

I. INTRODUCTION

B REAKING language barriers by using machines is an

ultimate goal for international communications. Auto-
matic speech translation (ST) has been studied for this purpose
for decades [1]–[4]. Cascade approaches combining auto-
matic speech recognition (ASR) and machine translation (MT)
systems have been the de facto standard, but the research
paradigm is shifting to end-to-end speech translation (E2E-ST)
models thanks to advances in deep learning [5]–[7]. End-to-
end models have several attractive properties, such as avoiding
ASR error propagation and low-latency decoding. However,
the translation quality of E2E models still lags behind that

H. Inaguma and T. Kawahara are with the Graduate School of In-
formatics, Kyoto University, Kyoto 606-8501, Japan (e-mail: {inaguma,
kawahara}@sap.ist.i.kyoto-u.ac.jp).

Y. Higuchi is with Waseda University, Tokyo 162-0042, Japan (e-mail:

higuchi@pcl.cs.waseda.ac.jp).

K. Duh is with HLTCOE, Johns Hopkins University, Baltimore, MD 21211-

2840, USA (e-mail: kevinduh@cs.jhu.edu).

S. Watanabe is with Language Technologies Institute, Carnegie Mellon

University, Pittsburgh, PA 15213-3891, USA (e-mail: shinjiw@cmu.edu).

of cascade systems when additional resources are available,
although the gap is closing [8]–[10].

Various solutions have been proposed to bridge the quality
gap between cascade and E2E models, such as multi-task
learning with auxiliary tasks [6], [7], pre-training [7], [11]–
[13], knowledge distillation [14], [15], and semi-supervised
training [16], [17]. Most studies on E2E systems have fo-
cused on autoregressive (AR) models, which generate a target
sequence from left to right. However, the decoding speed is
not fast enough for real-world applications because of the
incremental update of decoder states. This slows the decoding
speed, especially when the Transformer architecture [18] is
adopted because of the self-attention operation to all past
tokens in the decoder at each generation step.

Recently, non-AR (NAR) models have attracted attention to
increase the decoding speed by discarding the conditional de-
pendency of outputs in AR models. Gu et al. [19] proposed the
ﬁrst single-step NAR MT model, but it sacriﬁces translation
quality. Various models have been proposed to address this
issue; improved single-step NAR models [20]–[28], iterative-
reﬁnement models [29]–[32], latent alignment models [33]–
[36],
insertion-based models [37]–[39], and latent variable
models [40]–[43]. NAR models have been successfully ex-
tended to other tasks such as text-to-speech synthesis [44],
[45], ASR [46]–[48] and ST [49], [50].

However, what differentiates the NAR ST task from other
NAR tasks is many-to-many non-monotonic mapping. In other
words, source inputs, even though they correspond to the same
word sequence, can vary signiﬁcantly depending on speaker
attributes, speaking rate, recording conditions and so on. These
problems do not exist in text-based tasks because discrete
tokens represent the input instead of continuous signals. It
is also challenging to determine the target length from the
speech in advance because it includes many silent frames and
the input length is much longer than that of text.

To increase the decoding speed for the E2E-ST task, we
propose an efﬁcient uniﬁed NAR framework called Orthros,
which introduces an auxiliary shallow AR decoder on top of
the shared speech encoder (see Fig. 1). The AR decoder is
jointly trained with the NAR decoder and is used to rescore
multiple candidates of different lengths generated from the
NAR decoder, referred to as parallel AR rescoring. This is
based on an observation that sequence-level scores from the
NAR decoder are not sufﬁcient for accurately selecting the
best candidate. Because outputs from the NAR decoder can
be fed to the AR decoder in parallel, the NAR decoding is
still maintained. Because a shallow AR decoder works for

 
 
 
 
 
 
2

which will be helpful for future E2E-ST studies.

• We conducted various analyses to evaluate the effective-
ness of Orthros in terms of model capacity, robustness
against long-form speech, and searchability.

II. BACKGROUND

In this section, we review AR and NAR E2E-ST models.
Let X = (x1, . . . , xU ) denote input speech features in a source
language, Y = (y1, . . . , yN ) denote the target translation text,
where U and N denote the input length and output length,
respectively.

A. AR sequence model

Sequence-to-sequence models, so-called encoder-decoder
models, are typically using AR sequence models such as the
Transformer [18]. An AR model factorizes a conditional prob-
ability of Y given X into a chain of conditional probabilities
from left to right as follows:

P (Y |X) =

N
(cid:89)

i=1

PAR(yi|y<i, X),

(1)

where PAR is a probability density function of the AR model,
and y<i are all previous tokens before the i-th token. The
parameters are updated with a cross-entropy (CE) loss LAR
formulated as

LAR = − log PAR(Y |X)
N
(cid:88)

= −

log PAR(yi|y<i, X).

i=1

training can be
When using the Transformer architecture,
carried out efﬁciently by feeding all ground-truth tokens to the
decoder in parallel, i.e., teacher-forcing [54]. During inference,
however, beam search is used as a heuristic to ﬁnd the
most plausible sequence. Because this incrementally expands
a preﬁx of each hypothesis token by token, the decoding speed
is typically not fast enough. Speciﬁcally,
the Transformer
decoder slows the speed because it performs self-attention to
all past tokens generated thus far. The decoding complexity is
O(U N ).

B. NAR sequence model

To increase the decoding of AR models by generating
all tokens in a target sequence in parallel, the conditional
independence for each token position in the output probability
is assumed with NAR models. The conditional probability in
Eq. (1) is decomposed as

P (Y |X) =

N
(cid:89)

i=1

PNAR(yi|X),

(2)

where PNAR is a probability density function of the NAR
model. Because PNAR is not conditioned on y<i, this for-
mulation enables the generation of a sequence with a single
iteration, which can achieve large increases in decoding speed.
However, such a strong assumption degrades translation qual-
ity because of the multimodality problem, in which multiple

Fig. 1: Overview of Orthros

the rescoring purpose only, additional computation cost is
minimal.

[30] and connectionist

We investigate the conditional masked language model
(CMLM)
temporal classiﬁcation
(CTC)-based model [51] as NAR decoders for Orthros, re-
ferred to as Orthros-CMLM and Orthros-CTC, respectively.
The CMLM is an iterative reﬁnement model while the CTC-
based model is a single-step latent alignment model. However,
any other NAR decoder topology can be used for Orthros
in theory as long as multiple candidates can be generated.
To enhance the training of the CMLM decoder, we further
propose multi-mask training (MMT) and joint training with
an auxiliary text-input NAR MT task. With these methods, the
CMLM decoder can take decoder inputs from multiple views
per sample for improving translation quality. To enhance the
encoder representation, we also investigated the Conformer
encoder [52].

Experimental evaluations on three benchmark corpora, in-
cluding six language directions, are conducted to show that
Orthros signiﬁcantly improves the BLEU scores with small
additional latency. We show that the rescoring is more effective
than reﬁning predictions through further iterations. We also
argue that joint training with the auxiliary shallow AR decoder
improves the BLEU scores in most cases. MMT and joint
training with the NAR MT task also boosts the BLEU score
for the Transformer encoder while maintaining the decoding
speed. The Conformer encoder improves BLEU scores across
models, especially for CTC-based models. We show that
Orthros-CTC with the Conformer encoder achieved the best
BLEU scores among NAR models. Compared with the AR
model, it achieved 3.63× faster decoding speed on a CPU
with comparable BLEU scores.

The contributions of this article are summarized as follows:1
• We propose Orthros and show its effectiveness with two
i.e., CMLM and CTC-based model as

NAR models,
examples.

• We propose two enhanced training methods for the

CMLM.

• We thoroughly compare AR and NAR models on the
basis of both Transformer and Conformer encoder archi-
tectures.

• We present strong AR baselines with and without
[53],

sequence-level knowledge distillation (SeqKD)

1This study is an extension of our previous study [49], in which only the

ﬁrst item was investigated.

(cid:6)(cid:20)(cid:19)(cid:28)(cid:10)(cid:25)(cid:24)(cid:20)(cid:22)(cid:13)(cid:14)(cid:22)(cid:13)(cid:23)(cid:23)(cid:16)(cid:26)(cid:13)(cid:1)(cid:29)(cid:6)(cid:2)(cid:8)(cid:30)(cid:1)(cid:12)(cid:13)(cid:11)(cid:20)(cid:12)(cid:13)(cid:22)(cid:9)(cid:21)(cid:13)(cid:13)(cid:11)(cid:15)(cid:1)(cid:13)(cid:19)(cid:11)(cid:20)(cid:12)(cid:13)(cid:22)(cid:4)(cid:13)(cid:19)(cid:14)(cid:24)(cid:15)(cid:1)(cid:21)(cid:22)(cid:13)(cid:12)(cid:16)(cid:11)(cid:24)(cid:20)(cid:22)(cid:9)(cid:15)(cid:10)(cid:17)(cid:17)(cid:20)(cid:27)(cid:10)(cid:25)(cid:24)(cid:20)(cid:22)(cid:13)(cid:14)(cid:22)(cid:13)(cid:23)(cid:23)(cid:16)(cid:26)(cid:13)(cid:1)(cid:29)(cid:2)(cid:8)(cid:30)(cid:1)(cid:12)(cid:13)(cid:11)(cid:20)(cid:12)(cid:13)(cid:22)(cid:7)(cid:10)(cid:22)(cid:10)(cid:17)(cid:17)(cid:13)(cid:17)(cid:1)(cid:2)(cid:8)(cid:1)(cid:22)(cid:13)(cid:23)(cid:11)(cid:20)(cid:22)(cid:16)(cid:19)(cid:14)(cid:4)(cid:13)(cid:19)(cid:14)(cid:24)(cid:15)(cid:1)(cid:13)(cid:23)(cid:24)(cid:16)(cid:18)(cid:10)(cid:24)(cid:16)(cid:20)(cid:19)(cid:1)(cid:4)(cid:1)(cid:5)(cid:1)(cid:7)(cid:1)(cid:9)(cid:1)(cid:11)(cid:1)(cid:3)(cid:3)(cid:1)(cid:3)(cid:31)(cid:5)(cid:2)(cid:9)(cid:3)(cid:32)(cid:31)(cid:5)(cid:2)(cid:9)(cid:3)(cid:32)(cid:31)(cid:5)(cid:2)(cid:9)(cid:3)(cid:32)(cid:31)(cid:5)(cid:2)(cid:9)(cid:3)(cid:32)(cid:1)(cid:4)(cid:1)(cid:5)(cid:1)(cid:7)(cid:1)(cid:9)(cid:1)(cid:11)(cid:1)(cid:3)(cid:3)(cid:1)(cid:3)(cid:4)(cid:1)(cid:3)(cid:31)(cid:5)(cid:2)(cid:9)(cid:3)(cid:32)(cid:31)(cid:5)(cid:2)(cid:9)(cid:3)(cid:32)(cid:31)(cid:5)(cid:2)(cid:9)(cid:3)(cid:32)(cid:31)(cid:5)(cid:2)(cid:9)(cid:3)(cid:32)(cid:1)(cid:4)(cid:1)(cid:5)(cid:1)(cid:7)(cid:1)(cid:9)(cid:1)(cid:11)(cid:1)(cid:3)(cid:3)(cid:1)(cid:3)(cid:4)(cid:1)(cid:3)(cid:31)(cid:5)(cid:2)(cid:9)(cid:3)(cid:32)(cid:31)(cid:5)(cid:2)(cid:9)(cid:3)(cid:32)(cid:31)(cid:5)(cid:2)(cid:9)(cid:3)(cid:32)(cid:31)(cid:5)(cid:2)(cid:9)(cid:3)(cid:32)(cid:31)(cid:5)(cid:2)(cid:9)(cid:3)(cid:32)(cid:1)(cid:4)(cid:1)(cid:5)(cid:1)(cid:6)(cid:1)(cid:7)(cid:1)(cid:8)(cid:1)(cid:9)(cid:1)(cid:10)(cid:1)(cid:11)(cid:1)(cid:3)(cid:2)(cid:1)(cid:3)(cid:3)(cid:1)(cid:3)(cid:1)(cid:4)(cid:1)(cid:5)(cid:1)(cid:6)(cid:1)(cid:7)(cid:1)(cid:8)(cid:1)(cid:9)(cid:1)(cid:10)(cid:1)(cid:11)(cid:1)(cid:3)(cid:2)(cid:1)(cid:3)(cid:3)(cid:1)(cid:3)(cid:4)(cid:1)(cid:3)(cid:1)(cid:4)(cid:1)(cid:5)(cid:1)(cid:6)(cid:1)(cid:7)(cid:1)(cid:8)(cid:1)(cid:9)(cid:1)(cid:10)(cid:1)(cid:11)(cid:1)(cid:3)(cid:2)(cid:1)(cid:3)(cid:3)(cid:1)(cid:3)(cid:4)(cid:1)(cid:3)(cid:5)(cid:1)(cid:3)(cid:1)(cid:4)(cid:1)(cid:5)(cid:1)(cid:6)(cid:1)(cid:7)(cid:1)(cid:8)(cid:1)(cid:9)(cid:1)(cid:10)(cid:1)(cid:11)(cid:1)(cid:3)(cid:2)(cid:1)(cid:3)(cid:3)(cid:1)(cid:3)(cid:4)(cid:1)(cid:3)correct translations are predicted given the same source sen-
tence [19]. SeqKD is an effective method for mitigating this
problem by transforming reference translations in the training
data into a more deterministic form by using a teacher AR
model [55].

To relax the conditional independence assumption, iterative
reﬁnement methods [29] have also been studied by modifying
Eq. (2) to a chain of T iterations as

mask and ˆY (t)

Inference Let ˆY (t)
obs be masked and observed to-
kens in the prediction ˆY (t) at the t-th iteration (0 ≤ t ≤ T ),
respectively. Given a target length ˆN estimated using a length
predictor stacked on the encoder, the CMLM starts generation
from a placeholder ﬁlled with a [MASK] token at all ˆN
positions (t = 0). In the Predict step, the most plausible token
is selected from the vocabulary according to the probability
of the CMLM PCMLM at each masked position i in ˆY (t−1)
mask as

3

P (Y |X) =

T
(cid:89)

t=1

PNAR(Y (t)|Y (t−1), X)

T
(cid:89)

(cid:18) N (t)
(cid:89)

=

t=1

i=1

PNAR(y(t)

i

|Y (t−1), X)

(cid:19)
,

1 , · · · , y(t)

where Y (t) = (y(t)
N (t)) is a sequence of tokens at the t-
th iteration, where N (t) is the output length at t-th iteration and
can be changed in some models [37]–[39]. Although iterative
reﬁnement methods slow the decoding speed of pure NAR
models, they can achieve better translation quality in general
and ﬂexibly control the speed by changing T .

1) CTC: CTC is a loss function for a single-step latent
alignment model, which eliminates the necessity of frame-
level supervision for learning input-output mapping [51]. A
linear projection layer is stacked on the encoder to generate
a probability distribution PCTC so the model is typically a
decoder-free architecture. The conditional independence per
encoder frame is assumed with CTC, and CTC marginalizes
posterior probabilities of all possible alignment paths efﬁ-
ciently with the forward-backward algorithm. The gap in the
sequence lengths between the input and output is ﬁlled by
introducing blanks labels. The CTC loss LCTC is deﬁned as
the negative log-likelihood:

LCTC = − log PCTC(Y |X).

During inference, CTC-based models can be regarded as
fully NAR models if the best label class is taken from the
vocabulary at every encoder frame, which is known as greedy
search. The decoding complexity is O(U ). CTC assumes the
length of X to be longer than that of Y (i.e., |X| ≥ |Y |),
so previous studies using CTC for the text-input NAR MT
task adopted the upsampling technique to expand the input
sequence length [34]–[36]. However, this is not necessary for
speech-to-text generation tasks because |X| is generally much
longer than |Y |. Instead, we usually compress input sequence
lengths for the E2E-ST task [56]–[59]. Another advantage
of CTC for NAR models is that an explicit target length
prediction is not necessary. It can be obtained as a by-product
after collapsing frame-level outputs.

2) CMLM: The CMLM [30] is an iterative reﬁnement-
based model with a token in-ﬁlling adopted in Bidirectional
Encoder Representations from Transformers (BERT) [60].
During inference, CMLM adopts the Mask-Predict algorithm,
which alternates Mask and Predict steps at every iteration
and reﬁnes predictions through T iterations. Therefore, the
decoding complexity is O(U T ).

ˆy(t)
i = argmax

PCMLM(yi = w| ˆY (t)

obs, X),

w∈V
p(t)
i,CMLM ← max
w∈V

PCMLM(yi = w| ˆY (t)

obs, X),

where V is the output vocabulary, and p(t)
the i-th token at the t-th iteration. Note that p(t)
for masked positions only.

i,CMLM is a score of
i,CMLM is updated

In the Mask step, k(t) tokens having the lowest conﬁdence
scores in the previous prediction ˆY (t) are replaced with
[MASK], where k(t) is a linear decay function deﬁned as

k(t) = (cid:98) ˆN ·

T − t
t

(cid:99).

As the target length must be determined before starting the
token generation, length parallel decoding (LPD) [23], [30]
is typically used by generating multiple length candidates in
parallel. The l length candidates are used by selecting top-l
classes from a length predictor.2 After T iterations, a candidate
having the highest sequence-level scores 1
i,CMLM is
ˆN
selected as the best translation output.

i log P (t)

(cid:80)

Training The training objective of the CMLM is formulated
as a CE loss calculated at masked positions as

LCMLM = −

(cid:88)

y∈Ymask

log PCMLM(y|Yobs, X),

(3)

where Ymask ⊂ Y are partially masked ground-truth tokens,
and Yobs = Y \ Ymask. The number of masked tokens is sam-
pled from a uniform distribution U(1, N ), and the positions
are also determined randomly.

A length predictor is implemented as a linear classiﬁer and
trained to predict the ground-truth target sequence length N
given X as

Llp = − log Plp(N |X),

where Llp is a length prediction loss. Unlike the text-input MT
task, where the encoder output corresponding to a spacial to-
ken [LENGTH] is used as an input to the linear classiﬁer [30],
time-averaged encoder outputs are used for the E2E-ST task.
The total objective Ltotal is formulated as follows:

Ltotal = LCMLM + λlpLlp,

where λlp is a weight for the length-prediction loss.

2We use top-l classes instead of [ˆl − ∆, ˆl + ∆] centering on the best class

ˆl as in [23].

Training:

3) Semi-autoregressive

Semi-AR training
(SMART) is an improved training method to mitigate the
gap in the behaviors of the CMLM between training and
test [31]. To resemble the test-time behavior based on the
Mask-Predict algorithm during training, the decoder input is
replaced with the model prediction at each training step. To
obtain the prediction, the most plausible token is taken at all
N positions by adopting the same masking as the original
CMLM training while the gradients are truncated. Part of the
resultant tokens ˆY = (ˆy1, . . . , ˆyN ) are masked out to generate
a new decoder input ˆYobs, which is fed to the decoder again
to calculate the CE loss at all positions as

4

Macaron-Net [62]. Relative positional encoding is also used
in each self-attention module. In our study, we investigated
how the Conformer impacts the translation quality of both AR
and NAR models. Although the Conformer encoder introduces
additional parameters, it is not a bottleneck of decoding speed.

III. PROPOSED FRAMEWORK: ORTHROS

In this section, we propose a uniﬁed NAR E2E-ST frame-
work, Orthros, which enhances the NAR decoder by joint
training and parallel rescoring with an auxiliary shallow AR
decoder.

LCMLM = −

(cid:88)

y∈Y

log PCMLM(y| ˆYobs, X).

A. Model architecture

During inference, the Mask-Predict algorithm is used, but
tokens at all positions are updated at every iteration, unlike
the original CMLM training. Although SMART slightly slows
the training speed by doubling the forward pass, it does not
increase the decoding cost during inference.

4) Mask-CTC: Since the CMLM is designed for iterative
reﬁnement starting from [MASK] tokens at all positions, the
translation quality at the early decoding iterations is likely to
be poor. To provide useful contexts from a single-step NAR
E2E-ST model based on CTC, Mask-CTC initializes the de-
coder input of the CMLM with a greedy CTC prediction [47].
The CTC module is attached to the top encoder layer and
trained jointly with the CMLM decoder. The less conﬁdent
CTC predictions, the probabilities of which are smaller than
pthres, are replaced with [MASK] tokens. However, unlike
Mask-CTC for the ASR task, masked positions change de-
pending on the score at each iteration because of the non-
monotonic sequence generation in the ST task [48]. To keep
from modifying most CTC outputs by using the original Mask-
Predict algorithm, the restricted Mask-Predict algorithm was
proposed [48], where k(t) is truncated by the number of
masked tokens at t = 0. We refer to this type of CMLM as
CTC-CMLM in this paper.

Because CTC can also be used as a length predictor, a
separate length-prediction layer or LPD is not necessary. The
total training objective is formulated by interpolating LCMLM
and LCTC as

Ltotal = (1 − λCTC)LCMLM + λCTCLCTC,

where λCTC is a CTC loss weight.

C. Conformer

The Conformer is a Transformer-based encoder architecture
augmented by the convolution module in each block [52]. It
was introduced in the ASR task to capture global and local
features by self-attention and convolution, respectively. Be-
cause this property is compatible with speech, its effectiveness
has been demonstrated in various speech-related tasks [61],
including E2E-ST. Each Conformer block introduces an addi-
tional convolution module right after the self-attention module.
An additional position-wise feed-forward network (FFN) is
introduced right before the self-attention module, following the

An overview of Orthros is presented in Fig. 1. Orthros has
three main components: speech encoder, NAR decoder, and
auxiliary shallow AR decoder. The motivation to introduce
the AR decoder is based on an observation that sequence-
level scores obtained from the NAR decoder are not suitable
for selecting the best translation from multiple candidates.
This is because of the conditional independence assumption
made with the NAR decoder, which makes the NAR decoder
not fully leverage the effectiveness of generating multiple
candidates.

In Orthros, the speech encoder is shared between the NAR
and AR decoders, which greatly reduces the model size
and computational overhead compared with using another
AR model such as that proposed by Gu et al. [19]. For
models except for a CTC-based one, a length predictor is also
stacked on the speech encoder. The entire architecture is jointly
trained.

For Orthros, we mainly focus on the CMLM as the NAR
decoder because we can control the decoding speed by chang-
ing the number of iterations, and it is widely used in the MT
literature [26], [27], [30], [31], [63]–[68]. We refer to this
model as Orthros-CMLM. Because of powerful Conformer-
CTC, we also investigated equipping the AR decoder of with
CTC, referred to as Orthros-CTC. Orthros can be applied to
any NAR decoder topology as long as multiple candidates can
be generated.

B. Inference: parallel AR rescoring

During inference, we use sequence-level scores from the AR
decoder to select the most plausible translation after generating
multiple candidates with the NAR decoder. Because we can
feed tokens at all positions to the AR decoder simultaneously
in Eq. (1), we refer to this as parallel AR rescoring.

When using the CMLM as the NAR decoder, we use
the Mask-Predict algorithm described in Section II-B2 to
generate l candidates through T iterations. When using a
CTC-based model as the NAR decoder, we use left-to-right
beam search with a beam width of l. Although this is not
purely NAR decoding, there is no matrix multiplication with
weight parameters. After the NAR decoding, we immediately
feed the resulting tokens to the auxiliary AR decoder. We use
the sequence-level log probabilities 1
i log PAR(yi|y<i, X)
ˆN
obtained from the AR decoder to select the best translation

(cid:80)

5

among the l candidates. Note that scores from the NAR
decoder are not used for the rescoring.3

For Orthros-CMLM,

this rescoring step corresponds to
performing one more decoding iteration. However, as shown in
Section V, parallel AR rescoring with T − 1 is more effective
than the Mask-Predict with T without the rescoring. A shallow
AR decoder is used for rescoring purpose, so the additional
decoding cost is much smaller than a single iteration of the
CMLM decoder.

NAR E2E-ST task. This also has an effect of augmenting the
training data twice similar to MMT. However, we show that
these two methods are complementary and their combination4
is effective.

3) Training objective: We optimize an entire network with

an end-to-end training objective as

Ltotal = LCMLM + λlpLlp + λARLAR + λMTLMT,

(6)

where λ∗ are the corresponding loss weights.5

C. Orthros-CMLM

D. Orthros-CTC

In this section, we present Orthros-CMLM, which uses the
CMLM as the NAR decoder in Orthros. We also propose two
enhanced training methods for the CMLM.

1) MMT: In the training of the CMLM, the decoder ob-
serves a single set of tokens per sample, and the CE loss is cal-
culated at only masked positions. This is not as data-efﬁcient
as AR models, especially when training data are sparse such
as in the E2E-ST task. To mitigate this problem, we introduce
MMT by calculating CE losses for M forward passes with
different Yobs = Y \ Ymask. At each forward pass, a random
mask for Ymask is sampled independently. Accordingly, the
CE loss in Eq. (3) is modiﬁed to the average CE losses in all
forward passes as

LCMLM =

=

1
M

1
M

M
(cid:88)

m=1
M
(cid:88)

LCMLM(Y (m)

mask|Y (m)

obs , X)

(cid:88)

− log PCMLM(y|Y (m)

obs , X),

(4)

In this section, we describe Orthros-CTC, which uses a
CTC-based model as the NAR decoder. Unlike Orthros-
CMLM, Orthros-CTC does not have any NAR decoder pa-
rameters except for the linear projection layer, so it is more
parameter-efﬁcient. Note again that CTC does not require a
separate length predictor. As discussed in Section V, trans-
lation quality of CTC-based models greatly improves using
the Conformer encoder. Therefore, we focus only on the
Conformer encoder. The total objective Ltotal is formulated
as

Ltotal = LCTC + λARLAR.

(7)

To generate multiple candidates, left-to-right preﬁx beam
search can be used in the CTC branch. Although this is no
longer NAR decoding, we can use an efﬁcient implementation
with C++6 as done in a previous study [36] because there is
no matrix multiplication.

IV. EXPERIMENTAL SETTING

m=1

y∈Y (m)
mask

A. Dataset

mask and Y (m)

where Y (m)
are masked and observed tokens in
the m-th forward pass, respectively. This method corresponds
to augmenting the training data by a factor of M in total.
However, the number of parameter updates does not increase.

obs

2) Auxiliary NAR MT objective: To further enhance the
CMLM decoder further, we train the NAR E2E-ST model
jointly with an auxiliary text-input NAR MT task by sharing
the parameters of the CMLM decoder. The effectiveness of an
auxiliary MT task has been studied to improve the translation
quality of AR E2E-ST models by leveraging source transcrip-
tions Z [7], [56], [69]. However, the task is still AR sequence
generation, and none of the previous studies investigated the
effectiveness of the auxiliary MT task for the NAR E2E-ST
task. The CE loss for the NAR MT task LMT is formulated as

LMT =

1
M

M
(cid:88)

(cid:88)

m=1

y∈Y (m)
mask

− log PCMLM(y|Y (m)

obs , Z).

(5)

To encode source transcriptions, an additional text encoder
is introduced. However, this encoder can be removed during
inference, so the decoding cost does not change.

Regarding masking strategies, we use separate masks for
the NAR MT task by randomly sampling them independently,
which is more effective than reusing the same mask used in the

3We also investigated linearly interpolating scores from the AR and NAR

decoders in the log domain, but it did not lead to improvement.

We used Must-C [70] En→{De, Nl, Fr, Es, It}, Libri-
trans En→Fr [71], and Fisher-CallHome Spanish Es→En [72]
corpora for the experimental evaluations. We used the recipes
provided in the ESPnet-ST toolkit [73].

1) Must-C En→{De, Nl, Fr, Es, It}: We used En→De
(399 h), En→Nl (433 h), En→Fr (483 h), En→Es (495 h),
and En→It (456 h) directions on Must-C. This corpus con-
tains spontaneous English lecture speech extracted from TED
talks, the corresponding source transcriptions, and the target
translations. Non-verbal speech labels, such as “(Applause)”
and “(Laughter)”, were kept during training but removed
during evaluation as post-processing. We report case-sensitive
detokenized BLEU scores [74] on the tst-COMMON set.

2) Libri-trans (En→Fr): This corpus contains 100 h of
English read speech, the corresponding English transcriptions,
and the French translations. The French translation in the
training set
is augmented with Google Translate for each
utterance, and we used both references following the standard
practice. We report case-insensitive detokenized BLEU scores
on the test set.

4This corresponds to augmenting the training data by a factor of 4.
5Unlike our previous study [49], we removed the auxiliary CTC-based ASR
objective because it can be removed without quality degradation when pre-
training the encoder with the ASR task. Instead, an auxiliary NAR MT task
was newly introduced in this article.

6https://github.com/parlance/ctcdecode

3) Fisher-CallHome Spanish (Es→En): This corpus con-
tains 171 h of Spanish conversational telephone speech, the
corresponding Spanish transcriptions as well as the English
translations. Following the standard practice of this corpus, all
punctuation marks except for apostrophes were removed from
both transcriptions and translations. We report case-insensitive
BLEU scores on Fisher-{dev, dev2, test}, and CallHome-
{devtest, evltest}. We report case-insensitive detok-
enized BLEU scores on these ﬁve sets. Note that BLEU scores
on the Fisher splits were evaluated with four references. We
used the Fisher-dev set as the validation set.

B. Pre-processing

We extracted 80-channel

log-mel ﬁlterbank coefﬁcients
computed with 25-ms window size and shifted every
10ms with three-dimensional pitch features with the Kaldi
toolkit [75]. We augmented speech data with speed pertur-
bation [76] and SpecAugment [77] for both ASR and E2E-ST
task. Utterances having more than 3000 speech frames or more
than 400 characters were removed from the training data to ﬁt
the GPU memory.

We tokenized all sentences with the tokenizer.perl
script in the Moses toolkit [78]. Source transcriptions were
lowercased, and the punctuation marks except for apostro-
phes were removed. We constructed shared source and target
vocabularies on the basis of the byte pair encoding (BPE)
algorithm [79] with the Sentencepiece toolkit [80]. ASR
vocabularies were built only on source transcriptions. For
AR models, we used 1k units for all tasks on the Fisher-
CallHome Spanish and Libri-trans corpora, while 5k and 8k
units were used for ASR and E2E-ST/MT tasks on the Must-
C corpus, respectively. For NAR models, we used 16k units
on all corpora, unless otherwise noted. These vocabulary sizes
were selected to achieve the best performance for each model
following [49].

C. Architecture

We implemented models based on the ESPnet-ST toolkit.
All models in the ASR and E2E-ST tasks consisted of 12
encoder blocks and 6 decoder blocks, while MT models used
6 Transformer encoder blocks. We also used the Transformer
decoder when using the Conformer encoder. The speech
encoder in the ASR and E2E-ST tasks had two convolutional
neural network (CNN) blocks before the ﬁrst encoder block,
which had a kernel size of 3 and channel size of 256. Each
CNN block down-sampled features on both time and frequent
axes with a stride of two, which resulted in the four-fold time
reduction. The dimensions of the self-attention layer dmodel
and FFN dﬀ were set to 256 and 2048, respectively, and the
number of attention heads H was set to four. The kernel
size of depthwise separable convolution in each Conformer
block was set to 15. The CTC-based models had the same
encoder architecture while the decoder was replaced with a
linear projection layer.

D. Training

The Adam optimizer [83] was used for training with β1 =
0.9, β2 = 0.98, and (cid:15) = 10−9. We used the Noam learning

6

rate schedule [18] with warmup steps of 25k and learning-
rate constant of 5.0. The effective batch size was set to 256
utterances for the NAR models. We used dropout and label
smoothing [84] with a probability of 0.1 and 0.1, respectively.
We set (λlp, λAR, λMT) to (0.1, 0.3, 0.3) in Eq. (6) throughout
the experiments and empirically conﬁrmed that they work well
under various conditions. The number of masks for MMT M
was set to 2. We trained AR models for 30 epochs, CTC-based
models and CTC-CMLM for 100 epochs, the other models
for 50 epochs. The last ﬁve best checkpoints based on the
validation score were used for model averaging, except that
the last ten best checkpoints were used for the CTC-based
models.

We initialized encoder parameters of all E2E-ST models
with those of the pre-trained ASR model trained on the same
speech data. We did not use any external resources for pre-
training. The decoder parameters of all E2E-ST models were
initialized on the basis of a strategy in BERT [60], where
weight parameters were sampled from N (0, 0.02), biases were
set to zero, and layer normalization parameters were set to β =
0, γ = 1.7 This technique was used for the CMLM models in
the MT task [30].

Following the standard practice in NAR models [19], [30],
we used SeqKD with an AR Transformer MT model as a
teacher, except for Libri-trans.8 The teacher MT models used
a beam width of 5.

E. Decoding

For the AR models, we used a beam width bST ∈ {1, 4, 10}.
The ASR models used the joint CTC/Attention decoding [85]
with shallow fusion of an external long short-term memory
(LSTM) LM. For the CMLM decoders, we used a length beam
size l = 5 and number of iterations T ∈ {4, 10} as default
settings. We mainly used a relatively small l to avoid slowing
the decoding speed on the CPU. We also used a de-duplication
technique [27], [29], [86] for the CMLM decoders, where
repeated tokens were collapsed to a single token as post-
processing, except for the Fisher-CallHome Spanish corpus.9
For Orthros-CTC, we used l = 20 without de-duplication. The
decoding speed was measured with a batch size of 1 on an
NVIDIA TITAN RTX GPU and Intel(R) Xeon(R) Gold 6128
CPU @ 3.4GHz by averaging on ﬁve runs. We calculated
BLEU scores with SacreBLEU10 [87].

V. MAIN RESULTS

A. Must-C

1) Transformer encoder: Results with the Transformer
encoder on Must-C are shown in Table I. The single step
NAR decoding with CTC showed poor BLEU scores although

7Unlike [49], we did not use a pre-trained MT model for initialization of

decoder parameters.

8We did not observe any improvement in BLEU scores with SeqKD for
both AR and NAR models on this corpus. This is probably because the teacher
MT model was too weak due to a small amount of the training bitext.

9De-duplication was effective for corpora having long-form speech, such

as Must-C and Libri-trans.

10case.mixed+numrefs.1+smooth.exp+tok.13a+version.1.5.1

TABLE I: BLEU scores with Transformer encoder on Must-C tst-COMMON. Decoding speed was measured on En→De
with batch size of 1. NAR was set to 1. MMT: multi-mask training. NAR MT: auxiliary text-input NAR MT objective.

7

E2E AR

E2E NAR

Model

ESPnet-ST [73] (bST = 10)
Fairseq S2T [81]
NeurST [82]

Transformer (bST = 1)
Transformer (bST = 4)
Transformer + SeqKD (bST = 1)
Transformer + SeqKD (bST = 4)

CTC

CMLM
SMART
Orthros-CMLM
+ MMT + NAR MT

CMLM
SMART
Orthros-CMLM
+ MMT + NAR MT

T

N

N

1

4

10

De

22.9
22.7
22.8

21.0
22.8
23.8
24.3

19.8

20.1
20.4
21.8
22.5

21.5
21.4
22.9
23.5

Cascade AR

Trf ASR → Trf MT

2N 23.5

BLEU (↑)

Fr

32.8
32.9
33.3

31.6
33.3
33.7
34.5

28.9

28.9
29.7
30.0
31.5

30.7
31.0
31.8
33.1

33.9

Es

28.0
27.2
27.4

26.2
27.8
28.0
28.9

22.8

24.0
24.9
25.3
26.0

25.3
25.4
26.1
27.1

28.6

It

23.8
22.7
22.9

22.1
23.3
23.3
24.2

19.5

20.5
21.0
21.5
22.7

21.8
22.1
22.5
23.3

24.3

Avg

27.0
26.6
26.7

25.4
26.9
27.3
28.1

23.0

23.4
24.1
24.8
25.8

24.9
25.0
26.0
26.8

27.8

Speedup (↑)

GPU

CPU

–
–
–

1.64×
1.00×
1.64×
1.00×

–
–
–

3.26×
1.00×
3.26×
1.00×

24.51× 14.62×

6.45×
6.45×
5.69×
5.69×

3.13×
3.13×
2.99×
2.99×

0.45×

5.43×
5.43×
5.10×
5.10×

2.75×
2.75×
2.64×
2.64×

0.68×

Nl

27.4
27.3
27.2

26.1
27.3
27.7
28.4

23.9

23.6
24.3
25.3
26.1

25.1
25.2
26.4
27.1

28.5

TABLE II: BLEU scores with Conformer encoder on Must-C tst-COMMON

Model

E2E AR

Conformer (bST = 1)
Conformer (bST = 4)
Conformer + SeqKD (bST = 1)
Conformer + SeqKD (bST = 4)

E2E NAR

CTC (l = 1)
Orthros-CTC (l = 20)

CMLM
CTC-CMLM
Orthros-CMLM
+ MMT + NAR MT

CMLM
CTC-CMLM
Orthros-CMLM
+ MMT + NAR MT

T

N

1

4

10

De

23.2
25.0
25.7
26.3

24.1
25.3

22.6
24.1
23.4
23.3

23.5
24.2
24.5
24.1

Cascade AR

Cfm ASR → Trf MT

2N 24.1

BLEU (↑)

Speedup (↑)

Nl

28.3
29.7
29.7
30.6

28.5
29.9

26.3
27.8
27.6
27.5

27.6
28.2
28.5
28.6

29.4

Fr

34.5
35.5
35.8
36.4

34.6
36.2

31.3
34.7
33.3
34.0

33.0
34.8
34.6
35.1

35.0

Es

29.7
30.5
30.6
31.0

29.0
30.4

26.5
29.1
28.3
28.7

27.5
29.3
29.1
29.2

29.5

It

24.6
25.4
25.5
25.9

24.3
25.4

21.6
24.5
23.2
23.7

22.7
24.6
24.0
24.4

24.7

Avg

28.1
29.2
29.5
30.0

28.1
29.4

25.7
28.0
27.2
27.4

26.9
28.1
28.1
28.3

28.5

GPU

1.59×
1.00×
1.59×
1.00×

13.83×
1.14×

5.44×
5.07×
4.92×
4.92×

2.89×
3.06×
2.73×
2.73×

0.46×

CPU

2.82×
1.00×
2.82×
1.00×

8.32×
3.63×

4.20×
5.97×
4.03×
4.03×

2.33×
4.43×
2.31×
2.31×

0.67×

the speed increases were very large; 24.51× and 14.62× on
the GPU and CPU, respectively. The iterative NAR models,
however, improved BLEU scores at the cost of speed. We
ﬁrst compared the CMLM and SMART without the auxiliary
AR decoder, and the SMART outperformed the CMLM when
T = 4. However, the gains were diminished when T = 10.
Therefore, we did not use SMART for Orthros. Orthros-
CMLM consistently improved the CMLM for all language
pairs regardless of T . The additional latency with the parallel
AR rescoring was about 5 and 100 ms on the GPU and
CPU, respectively, which are negligible when we take the
gains of BLEU scores into account. The enhanced training
with MMT and multi-task with the auxiliary NAR-MT task
further improved the BLEU scores for all language pairs and
T . Compared with the baseline CMLM, the enhanced Orthros-
CMLM brought the gains of 2.4 and 1.9 BLEU scores on
average for T = 4 and T = 10, respectively. The enhanced
Orthros-CMLM with T = 10 also achieved BLEU scores

comparable to those of the AR Transformer (bST = 4) with
2.99× and 2.64× increase in speed on the GPU and CPU,
respectively. However, we found that SeqKD improved the AR
model and the gains were much larger than those reported in
the MT literature [36]. Compared with the cascade system, the
enhanced Orthros-CMLM with T = 10 achieved 6.44× and
3.88× faster decoding on GPU and CPU while the average
BLEU score underperformed by 1.0.

2) Conformer encoder: Results with the Conformer en-
coder on Must-C are shown in Table II. We observed that
the Conformer encoder consistently improved BLEU scores
across models. The CTC-based models showed the largest
gains of BLEU scores by more than 4 BLEU. One limitation
of the CTC-based models was that
took more training
steps to converge. We trained them for 100 epochs while
we trained the CMLM models for 50 epochs. We evalu-
ated Orthros-CTC considering the strong BLEU scores of
Conformer-CTC. It resulted in further gains by parallel AR

it

8

TABLE III: BLEU scores with Transformer encoder on
Libri-trans En→Fr test. NAR was set to 1.

TABLE IV: BLEU scores with Conformer encoder on
Libri-trans En→Fr test

Model

T BLEU (↑)

WordKD [14]
TCEN-LSTM [12]
NeurST [82]
Curriculum PT [13]
LUT [88]
STAST [56]
COSTT [58]
SATE [89]

Transformer (bST = 1)
Transformer (bST = 4)

CTC (l = 1)

CMLM
SMART
Orthros-CMLM
+ MMT + NAR MT

CMLM
SMART
Orthros-CMLM
+ MMT + NAR MT

N

N

1

4

10

17.02
17.05
17.2
17.66
17.75
17.81
17.83
18.3

16.6
16.7

13.4

14.3
13.6
15.0
15.9

15.5
14.6
16.2
16.7

Speedup (↑)

GPU

CPU

–
–
–
–
–
–
–
–

–
–
–
–
–
–
–
–

1.64× 4.29×
1.00× 1.00×

35.03× 26.61×

9.40× 10.09×
9.40× 10.09×
8.47× 9.52×
8.47× 9.52×

4.61× 5.18×
4.61× 5.18×
4.39× 4.99×
4.39× 4.99×

E2E AR

E2E NAR

Cascade AR Trf ASR → Trf MT

2N 17.0

0.43× 0.77×

rescoring and addressed the slow convergence issue. In terms
of decoding speed, Orthros-CTC achieved a large gain for
the CPU (3.63×) while that for the GPU was small (1.14×).
We also investigated CTC-CMLM, which reﬁned a CTC
output through multiple iterations. Although it outperformed
Orthros-CMLM in speed with similar BLEU scores, we did
not observe any BLEU improvement from the pure CTC-
based model. Therefore, increasing the number of candidates
was more effective than reﬁning the single output multiple
times. Similar to the Transformer encoder, Orthros-CMLM
improved the CMLM by 1.5 and 1.2 BLEU scores on average
for T = 4 and T = 10, respectively. The enhanced training
slightly improved the BLEU scores (+0.2) except for En→De
and En→Nl. We reason that the Conformer encoder already
enhanced the CMLM decoder by providing better encoder
representations and augmenting masks for the CMLM training
was not complementary. Comparing the enhanced Orthros-
CMLM (T = 10) with the cascade system, the gap in the
average BLEU score was reduced from 1.0 to 0.2 by using
the Conformer encoder. To summarize, Orthros-CTC showed
the best BLEU scores in all language pairs.

B. Libri-trans

The results from the Transformer and Conformer encoders
on Libri-trans are listed in Tables III and IV, respectively. We
did not use SeqKD for this corpus because it was not effective
in our preliminary experiments. We also referred to the results
of previous studies that did not leverage additional resources.
For Transformer-based models, we conﬁrmed that SMART
was not effective and Orthros outperformed both CMLM and
SMART regardless of T . The enhanced training was also
helpful, boosting the BLEU score by 0.9 for T = 4 and 0.5
for T = 10. The enhanced Orthros with T = 10 matched the
AR model in terms of BLEU score with 4.39× and 4.99×

Model

T BLEU (↑)

E2E AR

E2E NAR

Conformer (bST = 1)
Conformer (bST = 4)

CTC (l = 1)
Orthros-CTC (l = 20)

CMLM
CTC-CMLM
Orthros-CMLM
+ MMT + NAR MT

CMLM
CTC-CMLM
Orthros-CMLM
+ MMT + NAR MT

N

1

4

10

18.8
19.2

17.5
18.5

15.3
17.0
16.7
16.7

15.8
17.2
17.6
17.4

Speedup (↑)

GPU

CPU

1.61× 3.90×
1.00× 1.00×

19.24× 15.53×
1.37× 3.52×

7.92× 8.23×
7.03× 11.34×
7.11× 7.82×
7.11× 7.82×

4.32× 4.77×
4.49× 8.59×
4.12× 4.61×
4.12× 4.61×

Cascade AR Cfm ASR → Trf MT 2N 17.3

0.42× 0.79×

increase in decoding speed on the GPU and CPU, respectively.
The relative increase in speed were much larger than those on
Must-C because the test set on Libri-trans had many long-
form utterances up to about 50 s.

For Conformer-based models, we observed large improve-
ments across all models. The AR model achieved the state-of-
the-art BLEU score of 19.2. The other trend was consistent
with that on Must-C. Conformer-CTC improved upon the
Transformer-CTC by 4.1 BLEU, and Orthros-CTC further
boosted it by 1.0 BLEU. Orthros-CMLM outperformed the
CMLM by 1.4 BLEU for T = 4 and 1.8 BLEU for T = 10,
while the enhanced training did not improve it further.

C. Fisher-CallHome Spanish

The results with the Transformer and Conformer encoders
on Fisher-CallHome Spanish are listed in Tables V and VI,
respectively. Note that the CallHome sets are out-of-domain
because the models were trained in the Fisher domain. We
conﬁrmed similar trends for Must-C and Libri-trans. However,
unlike previous experiments, we observed that Conformer-
based Orthros-CMLM outperformed Conformer-CTC on the
CallHome test sets by a large margin. Considering the fact
that the word error rates on the CallHome test sets were much
larger than those on the Fisher test sets (40% vs. 20% [61],
[90]), we can conclude that Orthros is more robust in noisy
acoustic conditions.

VI. ANALYSIS

A. Effective number of auxiliary AR decoder layers

We ﬁrst investigated the effective number of auxiliary AR
decoder layers NAR for Orthros-CMLM. We increased NAR
from zero to six.11 We used the Conformer encoder without
any enhanced training method. The results on the dev set
of Must-C En→De, Libri-trans, and Fisher-CallHome Spanish
in Table VII indicate that a shallow AR decoder works

11In our previous study [49], we used NAR = 6 and did not study its impact

on BLEU scores.

TABLE V: BLEU scores with Transformer encoder on Fisher-CallHome Spanish Es→En. NAR was set to 3. Decoding speed
was measured on Fisher-test set.

BLEU (↑)

Speedup (↑)

9

Model

E2E AR

Transformer (bST = 1)
Transformer (bST = 4)
Transformer + SeqKD (bST = 1)
Transformer + SeqKD (bST = 4)

E2E NAR

CTC (l = 1)

CMLM
SMART
Orthros-CMLM
+ MMT + NAR MT

CMLM
SMART
Orthros-CMLM
+ MMT + NAR MT

T

N

1

4

10

Fisher

CallHome

dev

dev2

test

devtest

evltest

47.3
49.4
49.6
51.1

45.8

45.3
45.3
47.4
49.1

47.7
46.3
49.9
50.8

48.5
50.6
49.9
51.4

46.4

46.2
46.1
48.2
49.9

48.5
47.1
50.4
51.3

43.3

47.5
49.4
49.4
50.8

46.2

45.4
45.9
47.4
48.5

48.0
47.0
49.6
50.1

42.3

18.0
18.6
18.5
19.6

15.6

17.5
17.2
18.6
19.0

18.5
17.9
19.3
19.5

20.4

17.3
18.4
18.6
19.2

16.0

16.8
17.0
17.8
18.9

18.6
17.8
18.9
19.6

19.9

GPU

CPU

1.84×
1.00×
1.84×
1.00×

3.07×
1.00×
3.07×
1.00×

22.11× 11.55×

5.76×
5.76×
4.95×
4.95×

3.19×
3.19×
2.93×
2.93×

0.50×

4.50×
4.50×
4.05×
4.05×

2.47×
2.47×
2.34×
2.34×

0.64×

Cascade AR

Trf ASR → Trf MT

2N 41.4

TABLE VI: BLEU scores with Conformer encoder on Fisher-CallHome Spanish Es→En

Model

E2E AR

Conformer (bST = 1)
Conformer (bST = 4)
Conformer + SeqKD (bST = 1)
Conformer + SeqKD (bST = 4)

E2E NAR

CTC (l = 1)
Orthros-CTC (l = 20)

CMLM
CTC-CMLM
Orthros-CMLM
+ MMT + NAR MT

CMLM
CTC-CMLM
Orthros-CMLM
+ MMT + NAR MT

T

N

1

4

10

Cascade AR

Cfm ASR → Trf MT

2N 42.6

BLEU (↑)

Speedup (↑)

Fisher

CallHome

GPU

CPU

dev

dev2

test

devtest

evltest

53.3
54.4
54.1
54.7

51.0
54.0

47.7
50.9
50.3
50.3

49.8
51.3
51.8
51.3

53.9
55.1
54.6
55.4

51.6
54.8

48.1
51.4
50.7
50.7

49.7
51.7
52.3
52.2

44.4

52.5
53.6
53.9
54.1

50.8
54.1

46.9
50.5
48.9
49.0

48.9
50.7
50.9
51.2

43.2

20.6
21.1
21.4
21.5

18.0
21.0

18.8
17.7
20.0
20.3

19.7
17.9
20.7
20.9

21.7

20.8
21.1
21.5
21.0

18.7
20.8

18.5
17.9
18.9
20.0

19.4
18.3
20.0
20.4

21.0

1.82×
1.00×
1.82×
1.00×

11.80×
1.09×

4.70×
4.79×
4.18×
4.18×

2.89×
3.40×
2.70×
2.70×

0.50×

2.71×
1.00×
2.71×
1.00×

6.91×
2.80×

3.68×
4.96×
3.44×
3.44×

2.30×
3.89×
2.20×
2.20×

0.66×

TABLE VII: Effective number of auxiliary AR decoders NAR
for Orthros-CMLM on dev sets of Must-C En→De, Libri-
trans, and Fisher-CallHome Spanish. Conformer encoder was
used. All models used parallel AR rescoring. No enhanced
training method, such as MMT, was used.

NAR

Must-C

BLEU (↑)

Libri-trans

Fisher

T = 4

T = 10

T = 4

T = 10

T = 4

T = 10

0

1
2
3
4
5
6

21.4

22.2
21.9
22.0
21.0
21.8
21.9

22.7

23.2
23.1
23.2
22.4
23.1
22.8

16.2

17.4
17.3
16.8
17.6
17.3
16.9

16.9

18.4
18.2
18.1
18.5
18.4
17.8

47.7

48.7
49.2
50.3
49.5
47.9
49.1

49.8

50.1
51.0
51.8
51.6
49.5
51.1

well across corpora. Speciﬁcally, NAR = 1 was best on Must-
C, NAR = 4 was best on Libri-trans while NAR = 1 showed

comparable BLEU scores. However, Fisher-CallHome Spanish
reached a peak at NAR = 3. This was probably because the
input speech was much noisy compared with other corpora;
therefore, stacking multiple AR decoder layers was more
effective for providing better sequence-level scores. Otherwise,
a single layer was enough, which minimized the additional
decoding cost by the rescoring. Hence, we used NAR = 3 on
Fisher-CallHome Spanish and NAR = 1 on the other corpora
in the other experiments, regardless of the encoder type. We
also used the same NAR for Orthros-CTC.

B. Ablation study

We conducted an ablation study of Orthros-CMLM in
terms of parallel AR rescoring and the enhanced training
methods on the dev sets of Must-C En→De, Libri-trans, and
Fisher-CallHome Spanish shown in Table VIII. We used the
Transformer encoder and ﬁrst observed that joint training with
an auxiliary shallow AR decoder improved the BLEU scores

TABLE VIII: Ablation study of parallel AR rescoring and enhanced training methods on dev sets of Must-C En→De, Libri-
trans, and Fisher-CallHome Spanish. Transformer encoder was used.

10

ID Model

Must-C

BLEU (↑)

Libri-trans

Fisher

T = 4

T = 10

T = 4

T = 10

T = 4

T = 10

A1

A2
A3

A4
A5
A6
A7
A8
A9

CMLM

Orthros-CMLM
- parallel AR rescoring

Orthros-CMLM + MMT + NAR MT
- parallel AR rescoring
- MMT (M = 2)
- NAR MT

+ MMT (M = 3)
+ MMT (M = 4)

19.8

21.1
20.4

21.9
21.5
21.8
21.2
21.8
21.4

21.4

22.0
21.3

22.8
22.4
23.0
22.4
22.6
22.4

14.6

15.7
15.1

16.3
15.7
15.6
16.4
16.1
16.1

15.8

16.9
16.3

17.3
16.7
16.3
17.2
17.2
17.0

45.3

47.4
45.7

49.1
47.2
48.6
47.5
47.9
48.7

47.7

49.9
48.3

50.8
49.0
50.2
49.9
50.1
50.4

TABLE IX: BLEU scores of large models with (dmodel, H) =
(512, 8) on Must-C En→De tst-COMMON. Transformer en-
coder was used.

Model

T BLEU (↑)

Transformer AR + SeqKD (bST = 1)
Transformer AR + SeqKD (bST = 4)

Orthros-CMLM
+ MMT + NAR MT

Orthros-CMLM
+ MMT + NAR MT

N

4

10

24.0
24.7

23.0
23.8

24.0
24.4

Speedup (↑)

GPU CPU

1.62× 3.30×
1.00× 1.00×

5.89× 5.63×

3.02× 2.92×

Fig. 2: Robustness against
long-form speech on IWSLT
tst2019 set. X-axis denotes maximum input length thresh-
old in segment-merging algorithm [91].

(A1 vs. A3), except for T = 10 on Must-C. A3 was trained
jointly with the auxiliary AR decoder but did not use it during
inference. Parallel AR rescoring further improved the BLEU
scores in all settings (A2 vs. A3).

The effectiveness of parallel AR rescoring remained even
when using the enhanced training combining MMT (M = 2)
and the auxiliary NAR-MT task (A4 vs. A5). Each of these
training methods was beneﬁcial (A4 vs. A6 vs. A7). Because
A4 increased the number of masks for the CMLM training by
a factor of 4 per sample, we also investigated increasing M in
MMT without the auxiliary NAR-MT task (A9). However, we
conﬁrmed that this was less effective, indicating that providing
source information from different modalities was helpful to
some extent.

C. Large model

In the above experiments, we used (dmodel, H) = (256, 4)
in all models. They are much smaller than those used in
the NAR MT literature [36], where (dmodel, H) = (512, 8)
was typically used. Therefore, our baseline AR model was
relatively lightweight. This was because using a large AR
model did not lead to a notable BLEU improvement in our
preliminary experiments due to the limited parallel data in
the ST corpora. However, when additional data are avail-
able, increasing model capacity would be typically beneﬁcial.
Therefore, we also compared decoding speeds of AR and
NAR models based on a large Transformer architecture. We
used (dmodel, H) = (512, 8) in both the encoder and decoder
architectures of AR and NAR models, except that the size

of the auxiliary AR decoder in Orthros-CMLM was kept
to (dmodel, H) = (256, 4). The results on Must-C En→De
tst-COMMON in Table IX indicate that using the large
architecture improved the BLUE score of the AR model by
only 0.4. The inference speed was not so different from that
in Table I, but we observed a large BLEU improvement in
Orthros-CMLM by increasing the model capacity. Compared
with Table I, it improved BLEU scores by 1.2 and 1.1 when
T = 4 and T = 10, respectively. The quality was further
boosted by the enhanced training by 0.8 and 0.4 for T = 4 and
T = 10, respectively. Finally, the enhanced Orthros-CMLM
almost matched the strong AR model trained with SeqKD in
quality with 3× faster decoding.

D. Robustness against long-form speech

is important

In speech translation, audio segmentation has a large impact
on the ﬁnal translation quality [91]–[95] because acoustic-
driven segmentation does not necessarily correspond to sen-
tence segmentation based on punctuation marks. Therefore,
to translate long-form speech robustly by
it
incorporating long context [91]. Therefore, we investigated
the robustness of E2E-ST models against long-form data on
the International Conference on Spoken Language Translation
(IWSLT) tst2019 set (En→De).12 Following the segment-
merging algorithm [91], we split the entire speech using a
neural voice activity detection model [96] then concatenated
multiple adjacent segments until reaching the desired input

12The IWSLT test sets have reference translation over the entire session,

unlike Must-C.

TABLE X: Oracle BLEU scores on Must-C En→De
tst-COMMON. Numbers inside brackets denote gains from
parallel AR rescoring.

11

Model

Transformer encoder
Orthros-CMLM

+ MMT + NAR MT

+ large

Conformer encoder
Orthros-CTC

Orthros-CMLM

T

4
4
10
10

4
4
10
10

4
4
10
10

1

4
4
10
10

l

5
9
5
9

5
9
5
9

5
9
5
9

20

5
9
5
9

Oracle BLEU (∆) (↑)

24.6 (+2.8)
26.1 (+3.9)
25.9 (+3.0)
27.2 (+4.2)

25.7 (+3.2)
27.2 (+4.4)
26.9 (+3.4)
28.4 (+4.6)

26.1 (+3.1)
27.5 (+4.4)
27.4 (+3.4)
28.8 (+4.6)

28.6 (+3.3)

26.2 (+2.4)
27.5 (+4.1)
27.1 (+2.5)
28.5 (+4.0)

VII. CONCLUSIONS

To increase the decoding speed of E2E-ST models, we
proposed Orthros, which introduces an auxiliary shallow AR
decoder on top of the shared encoder to assist
the NAR
decoder and optimizes the entire network end-to-end. The AR
decoder is used for rescoring outputs from the NAR decoder
with a very small additional decoding cost. We investigated the
CMLM and CTC as the NAR decoder for Orthros. We also
compared Transformer and Conformer encoder architectures.
We introduced MMT and joint training with an auxiliary text-
input NAR-MT task to enhance the CMLM decoder. Experi-
mental evaluations on three benchmark corpora including six
language pairs conﬁrmed the consistent effectiveness of Or-
thros; parallel AR rescoring improved the BLUE scores of the
NAR model regardless of the encoder and decoder topologies.
Enhanced training methods improved the quality of Orthros-
CMLM by a large margin when the Transformer encoder
was used. When increasing the model capacity, the enhanced
Orthros-CMLM matched the strong large AR model trained
with SeqKD in terms of BLEU score with 3× faster decoding.
The Conformer encoder improved the overall quality across
models while the effectiveness of the parallel AR rescoring
was maintained. Orthros-CTC showed the best BLEU score
while achieving a 3.63× increase in decoding speed on a CPU
compared with the AR model.

For future work, we will further improve the translation
quality of Conformer-CTC by adopting a deep encoder ar-
chitecture [97]. We believe that improving the selection of a
better hypothesis in NAR models would bridge the quality gap
between AR and NAR models.

REFERENCES

Fig. 3: Effectiveness of length beam size l with T = 10 on
Must-C En→De tst-COMMON. Conformer encoder was
used.

length.13 Figure 2 shows BLEU scores as a function of the
maximum input length threshold for the segment merging. We
evaluated models trained on Must-C En→De. Note that we did
not change any of the decoding hyperparameters of the E2E-
ST models for this experiment. We observed that both the AR
model and Orthros-CMLM were not robust against long-form
speech, regardless of the encoder architecture. In contrast,
Orthros-CTC could robustly translate long-form speech up
to 60 s, outperforming the Conformer AR model on speech
longer than 30 s.

E. Searchability

In this section, we analyze the searchability of Orthros by
changing a length beam size l. We investigate how it impacts
the quality of hypotheses.

1) Length beam size: Figure 3 shows BLEU scores as
a function of length beam size l with T = 10 on Must-C
En→De tst-COMMON. We used the Conformer encoder and
conﬁrmed that parallel AR rescoring consistently improved
the quality of NAR models when l > 1 and the gain increased
using a larger l. The baseline CMLM and CTC-based model
did not improve by increasing l because the NAR decoders
could not provide effective sequence-level scores. Because
parallel AR rescoring introduces an additional iteration in
terms of T , we also investigated reducing T in Orthros-CMLM
by 1. We conﬁrmed that BLEU scores of Orthros-CMLM with
T = 9 was much better than those of CMLM with T regardless
of l and the gap was enlarged as l increased. Therefore, we
conclude that parallel AR rescoring improves the searchability
of NAR models.

2) Oracle BLEU: We next

investigated oracle BLEU
scores, where the best hypothesis was selected from candidates
generated from the NAR decoder based on the sentence-level
BLEU score with the reference translation. It is regarded as the
upper bound that can be achieved by parallel AR rescoring. We
conducted this experiment to identify a room for improvement
of translation quality of NAR models. Table X indicates
that increasing the number of length candidates can generate
translations of higher quality than reﬁning the predictions for
more iterations. However, NAR models were still affected by
selecting a better translation even with parallel AR rescoring.
This was more severe when using a large l. Better training
and architecture also led to higher oracle BLEU scores.

13We did not merge adjacent segments with interval above 100ms.

[1] F. W. Stentiford and M. G. Steer, “Machine translation of speech,”

British Telecom technology journal, vol. 6, no. 2, pp. 116–122, 1988.

[2] A. Waibel, A. N. Jain, A. E. McNair, H. Saito, A. G. Hauptmann,
and J. Tebelskis, “JANUS: a speech-to-speech translation system using
connectionist and symbolic processing strategies,” in Acoustics, Speech,
and Signal Processing, IEEE International Conference on.
IEEE
Computer Society, 1991, pp. 793–796.

[3] H. Ney, “Speech translation: Coupling of recognition and translation,”

in Proceedings of ICASSP.

IEEE, 1999, pp. 517–520.

[4] C. F¨ugen, “A system for simultaneous translation of lectures and
speeches,” Ph.D. dissertation, Karlsruhe Institute of Technology, 2009.
[5] A. B´erard, O. Pietquin, C. Servan, and L. Besacier, “Listen and translate:
A proof of concept for end-to-end speech-to-text translation,” in Pro-
ceedings of NeurIPS 2016 End-to-end Learning for Speech and Audio
Processing Workshop, 2016.

[6] R. J. Weiss, J. Chorowski, N. Jaitly, Y. Wu, and Z. Chen, “Sequence-to-
sequence models can directly translate foreign speech,” in Proceedings
of Interspeech, 2017, pp. 2625–2629.

[7] A. B´erard, L. Besacier, A. C. Kocabiyikoglu, and O. Pietquin, “End-
to-end automatic speech translation of audiobooks,” in Proceedings of
ICASSP.

IEEE, 2018, pp. 6224–6228.

[8] E. Ansari, A. Axelrod, N. Bach, O. Bojar, R. Cattoni, F. Dalvi,
N. Durrani, M. Federico, C. Federmann, J. Gu et al., “Findings of the
IWSLT 2020 evaluation campaign,” in Proceedings of IWSLT, 2020, pp.
1–34.

[9] M. Sperber and M. Paulik, “Speech translation and the end-to-end
promise: Taking stock of where we are,” in Proceedings of ACL, 2020.
[10] L. Bentivogli, M. Cettolo, M. Gaido, A. Karakanta, A. Martinelli,
M. Negri, and M. Turchi, “Cascade versus direct speech translation:
Do the differences still make a difference?” in Proceedings of ACL,
2021, pp. 2873–2887.

[11] S. Bansal, H. Kamper, K. Livescu, A. Lopez, and S. Goldwater, “Pre-
training on high-resource speech recognition improves low-resource
speech-to-text translation,” in Proceedings of NAACL-HLT, 2019, pp.
58–68.

[12] C. Wang, Y. Wu, S. Liu, Z. Yang, and M. Zhou, “Bridging the gap
between pre-training and ﬁne-tuning for end-to-end speech translation,”
in Proceedings of AAAI, 2020, pp. 9161–9168.

[13] C. Wang, Y. Wu, S. Liu, M. Zhou, and Z. Yang, “Curriculum pre-training
for end-to-end speech translation,” in Proceedings of ACL, 2020, pp.
3728–3738.

[14] Y. Liu, H. Xiong, Z. He, J. Zhang, H. Wu, H. Wang, and C. Zong, “End-
to-end speech translation with knowledge distillation,” in Proceedings
of Interspeech, 2019, pp. 1128–1132.

[15] H. Inaguma, T. Kawahara, and S. Watanabe, “Source and target bidi-
rectional knowledge distillation for end-to-end speech translation,” in
Proceedings of NAACL-HLT, 2021, pp. 1872–1881.

[16] Y. Jia, M. Johnson, W. Macherey, R. J. Weiss, Y. Cao, C.-C. Chiu, N. Ari,
S. Laurenzo, and Y. Wu, “Leveraging weakly supervised data to improve
end-to-end speech-to-text translation,” in Proceedings of ICASSP. IEEE,
2019, pp. 7180–7184.

[17] J. Pino, L. Puzon, J. Gu, X. Ma, A. D. McCarthy, and D. Gopinath,
training data for end-to-end automatic speech

“Harnessing indirect
translation: Tricks of the trade,” in Proceedings of IWSLT, 2019.
[18] A. Vaswani et al., “Attention is all you need,” in Proceedings of NIPS,

2017, pp. 5998–6008.

[19] J. Gu, J. Bradbury, C. Xiong, V. O. Li, and R. Socher, “Non-
autoregressive neural machine translation,” in Proceedings of ICLR,
2018.

[20] J. Guo, X. Tan, D. He, T. Qin, L. Xu, and T.-Y. Liu, “Non-autoregressive
neural machine translation with enhanced decoder input,” in Proceedings
of AAAI, 2019, pp. 3723–3730.

[21] Z. Li, Z. Lin, D. He, F. Tian, T. Qin, L. Wang, and T.-Y. Liu, “Hint-based
training for non-autoregressive machine translation,” in Proceedings of
EMNLP, 2019, pp. 5708–5713.

[22] Y. Wang, F. Tian, D. He, T. Qin, C. Zhai, and T.-Y. Liu, “Non-
autoregressive machine translation with auxiliary regularization,” in
Proceedings of AAAI, 2019, pp. 5377–5384.

[23] B. Wei, M. Wang, H. Zhou, J. Lin, and X. Sun, “Imitation learning for
non-autoregressive neural machine translation,” in Proceedings of ACL,
2019, pp. 1304–1312.

[24] C. Shao, J. Zhang, Y. Feng, F. Meng, and J. Zhou, “Minimizing the bag-
of-ngrams difference for non-autoregressive neural machine translation,”
in Proceedings of AAAI, 2020, pp. 198–205.

[25] J. Liu, Y. Ren, X. Tan, C. Zhang, T. Qin, Z. Zhao, and T.-Y. Liu,
“Task-level curriculum learning for non-autoregressive neural machine
translation,” in Proceedings of IJCAI, 2020, pp. 3861–3867.

12

[26] M. Ghazvininejad, O. Levy, and L. Zettlemoyer, “Aligned cross entropy
for non-autoregressive machine translation,” in Proceedings of ICML,
2020.

[27] C. Du, Z. Tu, and J. Jiang, “Order-agnostic cross entropy for non-
autoregressive machine translation,” in Proceedings of ICML, 2021.
[28] L. Qian, H. Zhou, Y. Bao, M. Wang, L. Qiu, W. Zhang, Y. Yu, and
L. Li, “Glancing Transformer for non-autoregressive neural machine
translation,” in Proceedings of ACL, 2021, pp. 1993–2003.

[29] J. Lee, E. Mansimov, and K. Cho, “Deterministic non-autoregressive
neural sequence modeling by iterative reﬁnement,” in Proceedings of
EMNLP, 2018, pp. 1173–1182.

[30] M. Ghazvininejad, O. Levy, Y. Liu, and L. Zettlemoyer, “Mask-predict:
Parallel decoding of conditional masked language models,” in Proceed-
ings of EMNLP, 2019, pp. 6112–6121.

[31] M. Ghazvininejad, V. Karpukhin, L. Zettlemoyer, and O. Levy, “Semi-
autoregressive training improves mask-predict decoding,” arXiv preprint
arXiv:2001.08785, 2020.

[32] J. Kasai, J. Cross, M. Ghazvininejad, and J. Gu, “Non-autoregressive ma-
chine translation with disentangled context Transformer,” in Proceedings
of ICML, 2020, pp. 5144–5155.

[33] Z. Sun, Z. Li, H. Wang, D. He, Z. Lin, and Z. Deng, “Fast structured
decoding for sequence models,” in Proceedings of NeurIPS, H. Wallach,
H. Larochelle, A. Beygelzimer, F. d'Alch´e-Buc, E. Fox, and R. Garnett,
Eds., 2019.

[34] J. Libovick`y and J. Helcl, “End-to-end non-autoregressive neural ma-
chine translation with connectionist temporal classiﬁcation,” in Proceed-
ings of EMNLP, 2018, pp. 3016–3021.

[35] C. Saharia, W. Chan, S. Saxena, and M. Norouzi, “Non-autoregressive
machine translation with latent alignments,” in Proceedings of EMNLP,
2020, pp. 1098–1108.

[36] J. Gu and X. Kong, “Fully non-autoregressive neural machine transla-
tion: Tricks of the trade,” in Findings of the Association for Computa-
tional Linguistics: ACL-IJCNLP 2021, 2021, pp. 120–133.

[37] J. Gu, C. Wang, and J. Zhao, “Levenshtein Transformer,” in Proceedings

of NeurIPS, 2019, pp. 11 181–11 191.

[38] M. Stern, W. Chan, J. Kiros, and J. Uszkoreit, “Insertion Transformer:
Flexible sequence generation via insertion operations,” in Proceedings
of ICML, 2019, pp. 5976–5985.

[39] W. Chan, N. Kitaev, K. Guu, M. Stern, and J. Uszkoreit, “KERMIT:
Generative insertion-based modeling for sequences,” arXiv preprint
arXiv:1906.01604, 2019.

[40] L. Kaiser, S. Bengio, A. Roy, A. Vaswani, N. Parmar, J. Uszkoreit, and
N. Shazeer, “Fast decoding in sequence models using discrete latent
variables,” in Proceedings of ICML, 2018, pp. 2390–2399.

[41] X. Ma, C. Zhou, X. Li, G. Neubig, and E. Hovy, “FlowSeq: Non-
autoregressive conditional sequence generation with generative ﬂow,”
in Proceedings of EMNLP, 2019, pp. 4282–4292.

[42] R. Shu, J. Lee, H. Nakayama, and K. Cho, “Latent-variable non-
autoregressive neural machine translation with deterministic inference
using a delta posterior,” in Proceedings of AAAI, 2020, pp. 8846–8853.
[43] L. Tu, R. Y. Pang, S. Wiseman, and K. Gimpel, “ENGINE: Energy-
based inference networks for non-autoregressive machine translation,”
in Proceedings of ACL, 2020, pp. 2819–2826.

[44] A. Oord, Y. Li,

I. Babuschkin, K. Simonyan, O. Vinyals,
K. Kavukcuoglu, G. Driessche, E. Lockhart, L. Cobo, F. Stimberg et al.,
“Parallel WaveNet: Fast high-ﬁdelity speech synthesis,” in Proceedings
of ICML, 2018, pp. 3918–3926.

[45] Y. Ren, Y. Ruan, X. Tan, T. Qin, S. Zhao, Z. Zhao, and T.-Y. Liu, “Fast-
Speech: Fast, robust and controllable text to speech,” in Proceedings of
NeurIPS, 2019, pp. 3171–3180.

[46] W. Chan, C. Saharia, G. Hinton, M. Norouzi, and N. Jaitly, “Imputer:
Sequence modelling via imputation and dynamic programming,” in
Proceedings of ICML, 2020, pp. 1403–1413.

[47] Y. Higuchi, S. Watanabe, N. Chen, T. Ogawa, and T. Kobayashi, “Mask
CTC: Non-autoregressive end-to-end ASR with CTC and mask predict,”
in Proceedings of Interspeech, 2020, pp. 3655–3659.

[48] Y. Higuchi, H. Inaguma, S. Watanabe, T. Ogawa, and T. Kobayashi,
“Improved Mask-CTC for non-autoregressive end-to-end ASR,” in Pro-
ceedings of ICASSP.

IEEE, 2021, pp. 8363–8367.

[49] H. Inaguma, Y. Higuchi, K. Duh, T. Kawahara, and S. Watanabe,
“Orthros: Non-autoregressive end-to-end speech translation with dual-
decoder,” in Proceedings of ICASSP.

IEEE, 2021, pp. 7503–7507.

[50] S.-P. Chuang, Y.-S. Chuang, C.-C. Chang, and H.-y. Lee, “Investigating
the reordering capability in CTC-based non-autoregressive end-to-end
speech translation,” in Findings of the Association for Computational
Linguistics: ACL-IJCNLP 2021, 2021, pp. 1068–1077.

[51] A. Graves, S. Fern´andez, F. Gomez, and J. Schmidhuber, “Connectionist
temporal classiﬁcation: Labelling unsegmented sequence data with re-
current neural networks,” in Proceedings of ICML, 2006, pp. 369–376.
[52] A. Gulati, J. Qin, C.-C. Chiu, N. Parmar, Y. Zhang, J. Yu, W. Han,
S. Wang, Z. Zhang, Y. Wu, and R. Pang, “Conformer: Convolution-
augmented Transformer for speech recognition,” in Proceedings of
Interspeech, 2020, pp. 5036–5040.

[53] Y. Kim and A. M. Rush, “Sequence-level knowledge distillation,” in

Proceedings of EMNLP, 2016, pp. 1317–1327.

[54] R. J. Williams and D. Zipser, “A learning algorithm for continually
running fully recurrent neural networks,” Neural computation, vol. 1,
no. 2, pp. 270–280, 1989.

[55] C. Zhou, J. Gu, and G. Neubig, “Understanding knowledge distillation in
non-autoregressive machine translation,” in Proceedings of ICLR, 2019.
[56] Y. Liu, J. Zhu, J. Zhang, and C. Zong, “Bridging the modality gap for
speech-to-text translation,” arXiv preprint arXiv:2010.14920, 2020.
[57] M. Gaido, M. Cettolo, M. Negri, and M. Turchi, “CTC-based compres-
sion for direct speech translation,” in Proceedings of EACL, 2021, pp.
690–696.

[58] Q. Dong, M. Wang, H. Zhou, S. Xu, B. Xu, and L. Li, “Consecutive
decoding for speech-to-text translation,” in Proceedings of AAAI, 2021.
[59] X. Zeng, L. Li, and Q. Liu, “RealTranS: End-to-end simultaneous
speech translation with convolutional weighted-shrinking Transformer,”
in Findings of the Association for Computational Linguistics: ACL-
IJCNLP 2021, 2021, pp. 2461–2474.

[60] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-
training of deep bidirectional Transformers for language understanding,”
in Proceedings of NAACL-HLT, 2019, pp. 4171–4186.

[61] P. Guo, F. Boyer, X. Chang, T. Hayashi, Y. Higuchi, H. Inaguma,
N. Kamo, C. Li, D. Garcia-Romero, J. Shi et al., “Recent developments
on ESPnet toolkit boosted by Conformer,” in Proceedings of ICASSP.
IEEE, 2021, pp. 5874–5878.

[62] Y. Lu, Z. Li, D. He, Z. Sun, B. Dong, T. Qin, L. Wang, and T.-Y.
Liu, “Understanding and improving Transformer from a multi-particle
dynamic system point of view,” arXiv preprint arXiv:1906.02762, 2019.
[63] J. Guo, L. Xu, and E. Chen, “Jointly masked sequence-to-sequence
model for non-autoregressive neural machine translation,” in Proceed-
ings of ACL, 2020, pp. 376–385.

[64] X. Kong, Z. Zhang, and E. Hovy, “Incorporating a local

transla-
tion mechanism into non-autoregressive translation,” in Proceedings of
EMNLP, 2020, pp. 1067–1073.

[65] L. Ding, L. Wang, D. Wu, D. Tao, and Z. Tu, “Context-aware cross-
attention for non-autoregressive translation,” in Proceedings of COLING,
2020, pp. 4396–4402.

[66] P. Xie, Z. Cui, X. Chen, X. Hu, J. Cui, and B. Wang, “Infusing sequential
information into conditional masked translation model with self-review
mechanism,” in Proceedings of COLING, 2020, pp. 15–25.

[67] Y. Hao, S. He, W. Jiao, Z. Tu, M. Lyu, and X. Wang, “Multi-task learning
with shared encoder for non-autoregressive machine translation,” in
Proceedings of NAACL-HLT, 2021, pp. 3989–3996.

[68] Q. Wang, H. Yu, S. Kuang, and W. Luo, “Hybrid-regressive neural
machine translation,” 2021. [Online]. Available: https://openreview.net/
forum?id=jYVY piet7m

[69] P. Bahar, T. Bieschke, and H. Ney, “A comparative study on end-to-end
IEEE, 2019, pp.

speech to text translation,” in Proceedings of ASRU.
792–799.

[70] M. A. Di Gangi, R. Cattoni, L. Bentivogli, M. Negri, and M. Turchi,
“MuST-C: a Multilingual Speech Translation Corpus,” in Proceedings
of NAACL-HLT, 2019, pp. 2012–2017.

[71] A. C. Kocabiyikoglu, L. Besacier, and O. Kraif, “Augmenting Lib-
rispeech with French translations: A multimodal corpus for direct speech
translation evaluation,” in Proceedings of LREC, 2018.

[72] M. Post, G. Kumar, A. Lopez, D. Karakos, C. Callison-Burch, and
S. Khudanpur, “Improved speech-to-text translation with the Fisher and
Callhome Spanish–English speech translation corpus,” in Proceedings
of IWSLT, 2013.

[73] H. Inaguma, S. Kiyono, K. Duh, S. Karita, N. Yalta, T. Hayashi, and
S. Watanabe, “ESPnet-ST: All-in-one speech translation toolkit,” in
Proceedings of ACL: System Demonstrations, 2020, pp. 302–311.
[74] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, “Bleu: a method for
automatic evaluation of machine translation,” in Proceedings of ACL,
2002, pp. 311–318.

[75] D. Povey, A. Ghoshal, G. Boulianne, L. Burget, O. Glembek, N. Goel,
M. Hannemann, P. Motlicek, Y. Qian, P. Schwarz et al., “The kaldi
speech recognition toolkit,” in Proceedings of ASRU.

IEEE, 2011.

13

[76] T. Ko, V. Peddinti, D. Povey, and S. Khudanpur, “Audio augmentation
for speech recognition,” in Proceedings of Interspeech, 2015, pp. 3586–
3589.

[77] D. S. Park, W. Chan, Y. Zhang, C.-C. Chiu, B. Zoph, E. D. Cubuk,
and Q. V. Le, “SpecAugment: A simple data augmentation method for
automatic speech recognition,” in Proceedings of Interspeech, 2019, pp.
2613–2617.

[78] P. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. Federico,
N. Bertoldi, B. Cowan, W. Shen, C. Moran, R. Zens, C. Dyer, O. Bojar,
A. Constantin, and E. Herbst, “Moses: Open source toolkit for statis-
tical machine translation,” in Proceedings of the 45th Annual Meeting
of the Association for Computational Linguistics Companion Volume
Proceedings of the Demo and Poster Sessions, 2007, pp. 177–180.
[79] R. Sennrich, B. Haddow, and A. Birch, “Neural machine translation
of rare words with subword units,” in Proceedings of ACL, 2016, pp.
1715–1725.

[80] T. Kudo and J. Richardson, “SentencePiece: A simple and language inde-
pendent subword tokenizer and detokenizer for neural text processing,”
in Proceedings of EMNLP: System Demonstrations, 2018, pp. 66–71.

[81] C. Wang, Y. Tang, X. Ma, A. Wu, D. Okhonko, and J. Pino, “Fairseq
S2T: Fast speech-to-text modeling with fairseq,” in Proceedings of
AACL: System Demonstrations, 2020, pp. 33–39.

[82] C. Zhao, M. Wang, Q. Dong, R. Ye, and L. Li, “NeurST: Neural speech
translation toolkit,” in Proceedings of ACL: System Demonstrations,
2021, pp. 55–62.

[83] D. Kingma and J. Ba, “Adam: A method for stochastic optimization,”

Proceedings of ICLR, 2015.

[84] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, “Rethinking
the inception architecture for computer vision,” in Proceedings of CVPR,
2016, pp. 2818–2826.

[85] S. Watanabe, T. Hori, S. Kim, J. R. Hershey, and T. Hayashi, “Hybrid
CTC/attention architecture for end-to-end speech recognition,” IEEE
Journal of Selected Topics in Signal Processing, vol. 11, no. 8, pp.
1240–1253, 2017.

[86] Z. Sun and Y. Yang, “An EM approach to non-autoregressive conditional
sequence generation,” in Proceedings of ICML, 2020, pp. 9249–9258.
[87] M. Post, “A call for clarity in reporting BLEU scores,” in Proceedings of
the Third Conference on Machine Translation: Research Papers, 2018,
pp. 186–191.

[88] Q. Dong, R. Ye, M. Wang, H. Zhou, S. Xu, B. Xu, and L. Li, ““Listen,
Understand and Translate”: Triple supervision decouples end-to-end
speech-to-text translation,” in Proceedings of AAAI, 2021.

[89] C. Xu, B. Hu, Y. Li, Y. Zhang, S. Huang, Q. Ju, T. Xiao, and
J. Zhu, “Stacked acoustic-and-textual encoding: Integrating the pre-
trained models into speech translation encoders,” in Proceedings of ACL,
2021, pp. 2619–2630.

[90] S. Karita, N. Chen, T. Hayashi, T. Hori, H. Inaguma, Z. Jiang,
M. Someki, N. E. Y. Soplin, R. Yamamoto, X. Wang et al., “A
comparative study on Transformer vs RNN in speech applications,” in
Proceedings of ASRU.

IEEE, 2019, pp. 499–456.

[91] H. Inaguma, B. Yan, S. Dalmia, P. Guo, J. Shi, K. Duh, and S. Watanabe,
“ESPnet-ST IWSLT 2021 ofﬂine speech translation system,” in Proceed-
ings of IWSLT, 2021, pp. 100–109.

[92] M. Gaido, M. A. D. Gangi, M. Negri, M. Cettolo, and M. Turchi,
“Contextualized translation of automatically segmented speech,” in
Proceedings of Interspeech, 2020, pp. 1471–1475.

[93] N.-Q. Pham, T.-L. Ha, T.-N. Nguyen, T.-S. Nguyen, E. Salesky,
S. St¨uker, J. Niehues, and A. Waibel, “Relative positional encoding for
speech recognition and direct translation,” in Proceedings of Interspeech,
2020, pp. 31–35.

[94] T. Potapczyk and P. Przybysz, “SRPOL’s system for the IWSLT 2020
end-to-end speech translation task,” in Proceedings of IWSLT, 2020, pp.
89–94.

[95] M. Gaido, M. Negri, M. Cettolo, and M. Turchi, “Beyond voice activity
detection: Hybrid audio segmentation for direct speech translation,”
arXiv preprint arXiv:2104.11710, 2021.

[96] H. Bredin, R. Yin, J. M. Coria, G. Gelly, P. Korshunov, M. Lavechin,
D. Fustes, H. Titeux, W. Bouaziz, and M.-P. Gill, “Pyannote.audio: neu-
ral building blocks for speaker diarization,” in Proceedings of ICASSP.
IEEE, 2020, pp. 7124–7128.

[97] J. Kasai, N. Pappas, H. Peng, J. Cross, and N. Smith, “Deep encoder,
shallow decoder: Reevaluating non-autoregressive machine translation,”
in Proceedings of ICLR, 2021.

