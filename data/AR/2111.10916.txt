Video Content Swapping Using GAN

Tingfung Lau (tingfunl) 1 Sailun Xu (sailunx) 1 Xinze Wang (xinzew) 1

1
2
0
2

v
o
N
1
2

]

V
C
.
s
c
[

1
v
6
1
9
0
1
.
1
1
1
2
:
v
i
X
r
a

1. Introduction

Video generation is an interesting problem in computer vi-
sion. It is quite popular for data augmentation, movie spe-
cial effects, AR/VR and so on. With the advances of deep
learning, many deep generative models have been proposed
to solve this task. These deep generative models provide a
way to utilize all the unlabeled images and videos online,
since it can learn deep feature representations with unsu-
pervised manner. And these models can also generate dif-
ferent kinds of images, which have great value for visual
application. However, video generation still has long way
to go. Directly generating a video from scratch could be
very challenging since we need to model not only the ap-
pearances of objects in the video but also their temporal
motion.

There are three main challenges for video generation
(Tulyakov et al., 2018). Firstly, the system needs to build
appearance models and physical motion models to get
both the time and space information. Any model that
doesn’t work well will lead to the generated video contain-
ing objects with physically impossible motion. Secondly,
even when objects perform some basic motion, there are
so many variations considering time dimension, such as
speech. Thirdly, motion artifacts are particularly percep-
tible for human beings.

In this project, we focus on a particular sub-task of video
generation: video content swapping. Since visual signals in
a video can be divided into content and motion, which rep-
resent objects and their dynamics respectively. The goal of
our project is to swap the appearance object in an video but
keep the motion being the same. For example, if we have
a video clip of a professional dancer performing ballet, we
would like to generate video of ourselves performing a fan-
tastic ballet show, with the same pose as the professional
dancer in the original video. Users without professional
knowledge can also create video with the content as they
want. All they need is an original video, and a photo in-
cluding the content that they want to replace. We want to
develop such a system based on some conditional genera-
tive models.

This project requires to solve three major problems. First
of all, we need to train a constrained generative model to
make sure the output is corresponding to the input objects.

It is not enough to randomly generate a normal video. Sec-
ondly, the system should recognize between foreground
and background images of the video. And it can auto-
matically adjust the screen according to the interactions be-
tween objects and context, such as occlusion. Thirdly, with
the increase of the number of people in the image, the sys-
tem can still maintain real-time performance. (Cao et al.,
2018)

A wide range of generative methods having been proposed
for image generation, including Variational Autoencoders
(VAE) (Kingma and Welling, 2013), Generative Adversar-
ial Networks (GAN) (Goodfellow et al., 2014), and Con-
ditional Generative Adversarial Networks (CGAN) (Mirza
and Osindero, 2014). For video generation, there are sev-
eral approaches using GAN, such as Temporal GANs con-
ditioning on Captions (TGANs-C) (Pan et al., 2017), Mo-
tion and Content decomposed Generative Adversarial Net-
work (MoCoGAN) (Tulyakov et al., 2018), and Seman-
tic Consistent Generative Adversarial Network (SCGAN)
(Yang et al., 2018). (Denton et al., 2017) propose an alter-
native way using disentangled auto encoder.

As we focus on video content swapping, our video gen-
eration should be conditioned on content input and pose
in a reference video. We ﬁrst extract the pose information
from a video using a pre-trained human pose detection (Cao
et al., 2018) trained on large scale data set. Then we will
use a generative model to synthesize a video using the ex-
tracted pose and content image. This could be done using a
disentangled auto encoder that learns to reconstruct an im-
age based on content code and pose code or using a cGAN
that conditioned on pose and content image. We will do ex-
periments with these two methods on 3 data sets to ﬁnd out
the best method for solving this task. We plan to make up
a tool for user to create their own synthesized video based
on our method.

2. Related Works

2.1. Video Content Swapping.

The swapping problem in videos, speciﬁcally human faces,
has been studied via various approaches. Traditional meth-
ods such as (Thies et al., 2018) and (Dale et al., 2011)
deliver impressive results, and the former one even offers

 
 
 
 
 
 
Video Content Swapping Using GAN

Figure 1. Architecture for Video Generation

real-time video reenactment. However, these methods rely
heavily on domain-speciﬁc knowledge and ﬁne-tuned fea-
tures speciﬁcally designed for face, and thus could not be
generalized for non-face tasks.

A recent unpublished work based on deep network called
DeepFake takes on this problem by training a shared en-
coder, and decoding using the target-speciﬁc decoder. This
approach also gives impressive results and does not require
domain-speciﬁc knowledge. However, it requires large
number of training samples of the target face. Even worse,
we need to train a different encoder for every different tar-
get face that we want to map to.

2.2. Generative Model

A huge amount of deep generative models have been pro-
posed for image generation and video generation. There
are two basic structures for these models, which is VAE
and GAN. Kingma and Welling (2013) proposed a stochas-
tic variational inference and learning algorithm, which
can be applied to large datasets and intractable cases ef-
ﬁciently, (Rezende et al., 2014) introduced a recognition
model to represent an approximate posterior distribution
and uses this for optimisation of a variational lower bound.
(Tulyakov et al., 2017) presented a principled framework,
which is Hybrid VAE (H-VAE), to capitalize on unlabeled
data. Goodfellow et al. (2014) proposed GAN to solve im-
age generation by learning a generator to fool a discrimi-
nator for classifying whether image is real/fake. Mirza and
Osindero (2014) further extend the idea and proposes con-
ditional generative adversarial nets (CGAN). The generator
models the conditional distribution of data and the discrim-
inator takes the variables conditioned on as an additional
input. We also use the same idea of CGAN in this project.

2.3. Video Generations

Denton et al. (2017) proposes a different idea by taking into
account the temporal coherence of a video, albeit not for
the face-swapping task but for video prediction task. It dis-
entangles the video representation into two components–
content, which remains unchanged throughout a single clip
or a duration of time, and pose, which captures the dynamic
aspects of the clip and thus varies over time. Concretely, for
the pose constraint, they introduce the adversarial loss for a
discriminator network C and pose encoder Ep, and the i-th
clip at t’s frame xt
i:

Ladversarial(C) = − log(C(Ep(xt
i), Ep(xt+k
− log(C(Ep(xt
Ladversarial(Ep) = − log(C(Ep(xt
i), Ep(xt+k
− log(C(Ep(xt

j

i

i), Ep(xt+k

i

)))

)))
i), Ep(xt+k
)))

i

)))

Effectively, the loss encourages the pose encoder to pro-
duce pose embedding that is indistinguishable for a dis-
criminator. The whole architecture is shown in Figure
1. The discriminator in the left part is trained with bi-
nary cross entropy (BCE) loss to predict whether a pair
of pose vectors comes from the same (top portion) or dif-
ferent (lower portion) scenes. When the discriminator is
trained, the pose encoder Ep is ﬁxed. The right part is the
whole model. And when the pose encoder Ep is updated,
the scene discriminator is held ﬁxed.

Yang et al. (2018) divides the human video generation pro-
cess into two steps, the ﬁrst step is to synthesize the human
pose sequence from an initial pose at the ﬁrst frame, the
second step is to generate the video frames from an human
pose sequences and initial frame of the video. They train
their pose generator and video generator using the GAN

Video Content Swapping Using GAN

loss to classify whether the generated pose sequences and
video are real or fake.

Tulyakov et al. (2018) also proposes the idea of replac-
ing the video entity by swapping the content embedding.
However, their content embedding is sampled once and as-
sumed ﬁxed for the entirety of the clip, which is overly
constrained as there might be object occulation and rotation
in the video, which might change the content embedding.
Moreover, they could not specify a target content as they do
not have the encoder part, as they are only relying on the
unconditional distribution of the latent content embedding.

2.4. Pose Extraction

Pose extraction is an important problem for a wide range
of scenarios, and people have proposed and optimized dif-
ferent kinds of models for different situations. In the mul-
tiple person pose estimation, there are two types of meth-
ods, which are part based and two steps. In the part-based
framework, the model ﬁrst detect all body parts, and then
label and assemble these parts to detect the pose. But since
only small local regions are considered, the body-part de-
tectors are vulnerable.

The two-step framework extracts pose based on object de-
tection and single person pose estimation, and it has bet-
In this project, we utilized AlphaPose
ter performance.
(Fang et al., 2017), which follows two-step framework. The
whole process can be divided into three steps, ﬁrstly, the
human detector bounds humans in the images with boxes.
Secondly, these boxes are fed into the ’Symmetric STN’
and ’SPPE’ module, and the pose proposal are generated
automatically. At last, the generated pose proposals are
reﬁned by parametric Pose NMS to obtain the estimated
human poses. Especially, AlphaPose introduces ’Parallel
SPPE’ to avoid local minimums and leverage the power
of SSTN in the training process. And designing a pose-
guided proposals generator to augment the existing training
samples. This method have better performance for multi-
person human pose estimation in terms of accuracy and ef-
ﬁciency. Therefore, we extract human poses in videos with
AlphaPose, and the process is shown in Fig.2.

Figure 2. Architecture for Pose Extractor

3. Methods

3.1. Disentangled Representation

t=1 to a content code {C}T

Inspired by Denton et al. (2017), we want to solve this
problem by learning a disentangled representation of video.
The idea is that we use a auto-encoder like model to encode
a video {I}T
t=1 and a pose code
{Z}T
t=1 and generate the video from the hidden code. We
denote the t-th frame of the i-th video by I t
i , and similarly
for all other sequential collection of variables. The model
have a pose encoder Fpose and a content encoder Fcontent
and a generator G, the hidden code and reconstruction is
computed as

Ct
i = Fcontent (I t
i )
i = Fpose (I t
Z t
i )
ˆI t
i , Z t
i = G(Ct
i ).

for each time frame I t

i in a video sequence.

We want to ensure the learned content code and the pose
code captures exactly the appearance and motion features
respectively. For the pose code, since we focus on video
with a human as its subject, so a natural idea is to utilize
the off-the-shelf deep human pose estimators pretrained on
large scale image data sets (Cao et al., 2018), as already
mentioned in section 2.4. Therefore the Fpose would be
a ﬁxed pre-trained model throughout the training process.
We could ﬁrst extract Z t
i in one pass in the preprocessing
step and use it for later training.

For the content code, intuitively the content code should not
vary much throughout the consecutive frames. So we use
the following consistency loss to ensure the content encod-
ing of frame t and t + k for k random sampled from some
time-window [−w, w] is consistent(Denton et al., 2017).

Lconsist = (cid:107)Fcontent (I t+k

i

) − Fcontent (I t

i )(cid:107)2
2

(1)

As a standard measurement of the quality of decoder and
encode, we compare the self-reconstructed frame ˆI t
i =
G(Ct

i ) against the original frame I t
i :

i , Z t

Lrec =

(cid:13)
(cid:13)I t
(cid:13)

i − ˆI t

i

(cid:13)
2
(cid:13)
(cid:13)
2

Apart from the standard self-reconstruction error, accord-
ing to our assumption, if we use the content code at time
t + k: Ct+k
, and the pose code at time t: Z t
i to reconstruct
the image, the image should be close to I t
i , therefore we
impose the following temporal-shifted reconstruction loss
on our encoder-decoder network as in Denton et al. (2017).

i

Lrec = (cid:107)I t

i − G(Fcontent(I t+k

i

), Z t

i ))(cid:107)2
2

(2)

Video Content Swapping Using GAN

The whole model is shown in Figure 3. It could be trained
on human video data set to learn the disentangled represen-
tation.

Figure 3. The whole pipeline of disentangled video representation
learning.

During inference time, we ﬁrst compute the pose codes
{Zi}T
t=1 for a video using the pretrained human pose en-
coder Fpose and then compute the content code ˜C for some
an reference image for content swapping. Then we use the
generator to produce images sequence ˜I t = G( ˜C, Z t
i ) as
the edited video. Since the encode disentangle the content
and pose, so the resulting system will be able to achieve the
goal of video content swap.

The content encoder in the original work is a shallow 5
layer convolutional network to produce features at different
resolutions. The decoder uses 5 layer transposed convolu-
tion and combines hidden code at different resolutions to
generate a image. The structure of the network is describe
in Section 3.3.

3.2. Conditional GAN

GAN is a popular generation model, however, simply us-
ing GAN cannot solve the video content swapping problem
because we require the output being conditioned on both
the content and pose image. Mirza and Osindero (2014)’s
work has inspired us to come up with our conditional GAN
framework. We will train a generator G that generates a
frame conditioned on a content code I and a pose code
Z. The generator is the same encoder-decoder structure as
we use in previous approach, but we optimize the network
using the GAN loss. Since we condition the generation
upon two hidden codes, we train two separate discrimina-
tors, the pose discriminator Dpose and content discrimina-
tor Dcontent . The pose discriminator takes a frame I t
i and
a corresponding pose Z t
i to classify whether this is a true
pair from the data or a frame generated by generator con-
ditioned on the pose code Z. This give us the loss of pose

discriminator

Lpose = E[− log Dpose (I t

i |Z t

i )]

+ E[− log(1 − Dpose (G(C t

i , Z t(cid:48)

i )|Z t(cid:48)

i ))]

The pose coordinates are converted to a heat map with the
same size as the input image, before putting into the dis-
criminator. Speciﬁcally speaking, we use 2D Gaussian dis-
tributions N ((xi, yi)T , σ2I) to encode human key points
(xi, yi) for i = 1, · · · , 17. The Gaussian distribution is
then converted to a 17 × 128 × 128 feature map by calcu-
lating the density function at each grid point. The resuling
feature map is concatenated with the 3 × 128 × 128 as the
input.

We also train a discriminator Dcontent to ensure the output
looks like real image and has the same appearance as the
content image. Dcontent will take a pair of image, and de-
cide whether it is two real samples from the same video, or
a generated frame and the content image the generator con-
ditioned on. This give us the loss of content discriminator

Lcontent = E[− log Dcontent (I t

i , I t(cid:48)

i )]

+ E[− log(1 − Dcontent (G(I t

i , Z t(cid:48)

i ), I t

i ))

The generator optimize the GAN loss

LGAN = E[− log(Dcontent (G(I t
+ E[− log(Dpose (G(I t

i , Z t(cid:48)
i , Z t(cid:48)

i ), I t
i ), Z t(cid:48)

i ))]
i ))]

The whole model could be concluded by Figure 4

Triplet Content Discriminator The content discrimi-
nator in the previous conditional GAN method decides
whether two frames are from the same video. Our ﬁrst ap-
proach simply concatenates two images and pass it through
a convolutional network. To better compare the similarity
in content between two frames, we proposes another struc-
ture, that passes the original frame through the convolu-
tional network to get a content embedding. Moreover, we
make use of the triplet loss that encourages the similarity
between an anchor and its positive pair and dissimilarity
between the anchor and its negative pair.
To illustrate, in our case, the anchor and the positive con-
tent code would be two content codes from two different
frames of the same video canchor (cid:44) Ct
i , and
the negative content code would be the content code from
a reconstructed frame cnegative (cid:44) Fcontent(G(C t
i )) then
the triplet loss is deﬁned as

i , cpositive (cid:44) Ct(cid:48)

i , Z t(cid:48)

max(0, (cid:107)canchor−cpositive(cid:107)2+m−(cid:107)canchor−cnegative(cid:107)2). (3)

where m is the margin hyperparameter, by which we want
different classes to be separated

Video Content Swapping Using GAN

Figure 4. The whole pipeline of our proposed Conditional GAN model.

3.3. Encoder and Decoder

We build the encoder and decoder as convolutional mod-
els, both of which contain six convolutional layers. The ar-
chitecture is shown in Fig.5. Using more complex models
like adding additional convolutional layers or using resid-
ual blocks as in Zhu et al. (2017) may further improve the
capacity of the generator. However, this project is focused
on the probabilistic formulation of the video frame genera-
tion task, and this structure is effective enough for our task.
Thus we stick on this structure.

difﬁcult to extract the pose of other moving objects. There-
fore, we searched for several datasets containing moving
people to evaluate our model from different aspects. We
have also searched for some other datasets, however, many
videos contains more than one subject and the number of
subjects varies in a single video from one frame to another.
We therefore adhere only to the two datasets below for all
our experiments.

• KTH Action. 1

Figure 5. The Architecture for encoder and decoder

4. Experiment

4.1. Data

Figure 6. KTH Action Samples

The most important point for the dataset is that the videos
and images should contain people in motion. Since in en-
coding process, pose estimator would extract the pose of
people based on pretrained human pose frame. It will be

KTH Action is one of the most popular datasets for
recognition of human actions. This video database
contains six types of human actions, which are walk-

1http://www.nada.kth.se/cvap/actions/

Video Content Swapping Using GAN

ing, jogging, running, boxing, hand waving and hand
clapping. These actions are performed several times
by 25 subjects in four different scenarios: outdoors s1,
outdoors with scale variation s2, outdoors with differ-
ent clothes s3 and indoors s4. Currently the database
contains 2391 sequences. All sequences were taken
over homogeneous backgrounds with a static camera
with 25fps frame rate. The sequences were down-
sampled to the spatial resolution of 160 × 120 pixels
and have a length of four seconds in average.

• UCF101. 2 This dataset is commonly used for video

key-point, e.g: head, hands, and etc. The coordinate is nor-
malized to range −1 to 1 with respect to the image width
and height. In some image the human is fully or partially
outside the image, the pose detector will not detect the pose
in this case. To tackle this problem, we also add one binary
variable to indicate whether the human is visible. There-
fore we have a 35 dimension vector in total per person per
frame. For the sake of simplicity, we will be dealing with
video with single subject (human).

4.3. Experiment Details

For a fair comparison of four different methods, including
the baseline and ones we proposed above:

(a) Disentagled Baseline

(b) Disentangled with Pretrained Pose

(c) CGAN

(d) CGAN with Triplet Loss

We implemented these methods based on the original im-
plementation 3 of Denton et al. (2017). We further develop
the pose extraction part and the CGAN method, our code is
released4.

Due to the limitation of computation resources, we stick
to the KTH dataset for all our experiments. We train each
proposed method for 100 epochs, using the same optimizer
conﬁguration: Adam with default learning rate at 0.001 for
all different models.

4.4. Experiment Results

We evaluate these different methods by the MSE recon-
struction loss mentioned in 3.1, as listed in 1. For the qual-
ity of the disentangled learning, we have also visualized
the result of swapping the content code of a video with the
content code extracted from a different video, in hope of
seeing the very same person performing different actions.
In ﬁgure 8, we have shown two content swapping exam-
ples. The ﬁrst example has different subject magnitudes
for the content image and pose video but poses are on the
easier side, whereas the second example has roughly the
same size of both subjects yet the pose are much harder.
Each example consists of four different methods. The blue
row of each example shows the pose video from which we
extracted the pose code, and the green row of each example
shows the key-points that we have extracted using Alpha-
Pose and will use in method (b),(c),(d). The red column of
each example shows the (single) content image from which
we extract the content code and swap into the pose video.

3https://github.com/edenton/drnet
4https://github.com/ldf921/drnet-py

Figure 7. UCF-101 Samples

action recognition. It includes 13,220 videos of 101
different action categories, such as basketball shoot-
ing, biking/cycling, diving, golf swinging and so on.
This data set is very challenging due to large varia-
tions in camera motion, object appearance and pose,
object scale, viewpoint, cluttered background, illumi-
nation conditions, etc. For each category, the videos
are grouped into 25 groups with more than 4 action
clips in it. The video clips in the same group share
some common features, such as the same actor, simi-
lar background, similar viewpoint, and so on.

For some datasets, the scaled videos are really small, so
quantitative evaluation may not faithfully reﬂect the quality
of the generation. Therefore, we will provide more visual
results in the sequel sections.

4.2. Pose Extraction

We preprocess each video with the AlphaPose (Fang et al.,
2017). The return format is a list containing 2-D coordi-
nates of 17 key points: (xi, yi)M
i=1, where i denotes the i-t h

2https://www.crcv.ucf.edu/data/UCF101.php

Video Content Swapping Using GAN

Method

Disentangled Baseline Disentangled with Pretrained Pose CGAN CGAN with Triplet Loss

MSE Loss

0.0036

0.0040

0.0048

0.0061

Table 1. Reconstruction mean squared error of the 4 methods.

Figure 8. Visualization of video content swap results with 4 methods. (a) Disentangled Baseline (b) Disentangled with Pretrained Pose
(c) CGAN (d) CGAN with Triplet Loss.

Video Content Swapping Using GAN

Disentangled Baseline
In the ﬁrst example, we can see
that the network has marginally learned disentangled rep-
resentation, and the swap-reconstruction look close to the
expected goal. However, the quality of generated image is
low as the image is blurry, and details of body parts are
obviously lacking: we could barely see the head of the per-
son from the content image. In the second example, the
result is much worse, we could see that the subjects from
both videos get mixed up in the ﬁnal result and are both
blurry, which indicates that the content information is not
ﬂowing through the content code solely. Therefore the pose
code will be polluted by the content information of the pose
video and the content codes of both pose video and content
image are salient in the swap-reconstruction result. Addi-
tionally, although the MSE Loss is the lowest for this base-
line method, the swap-reconstruction result is actually in-
ferior to methods we have proposed, showing that (cid:96)2 is not
a accurate indicator of the quality of the result.

Disentangled with Pretrained Pose Apparently, the ﬁ-
delity of the swap-reconstructed video has signiﬁcantly
In the
improved from the Baseline Disentangled Model.
ﬁrst example, we can see that the details of the sub-
ject of the content image are well preserved in the swap-
reconstruction. Moreover, as a sign of better learned disen-
tangled representation, we no longer see the content clutter
as before in the second example. Interestingly, as a result
of using a pretrained pose, we see that when we have dras-
tically different subject magnitudes in content image and
pose video (as in the ﬁrst example), the subject of the con-
tent image will be re-sized to the magnitude of the subject
of the pose video.

CGAN CGAN as well as CGAN with triplet-loss below,
has much sharper swap-reconstruction than both the Dis-
entangled Baseline and the Disentangled with Pretrained
Pose. However, in the second example, this method again
suffers the problem of cluttering the content information
from both content image and pose video, which is salient
in the third to last frame.

CGAN with triplet-loss Although we have added the
triplet-loss to encourage similarity between content codes
of the same video and to simultaneously encourage dis-
similarity between content codes of different videos with
a separation of margin as mentioned in 3.2, the swap-
reconstruction result is no better than the CGAN–actually
the reconstruction is blurrier than the ordinary CGAN in
the ﬁrst video. In the second experiment, the subject clut-
tering problem also deteriorates as we can see the subjects
of both videos overlapping each other (notice a shadow of
the running pose from the content image) in every single
frame. We think that further ﬁne-tuning the margin hyper-
parameter could ﬁx the problem, or maybe a (cid:96)2 norm in the

representational space is not

5. Conclusion and Future Work

In this project, we try to solve the video generation problem
from the video content swap side. We propose two meth-
ods to train a disentangled frame generator using content
and pose information, disentangled encoder-decoder train-
ing and CGAN training. Both auto-ecnoding and CGAN
methods can generate video frames with good quality in a
small action recognition data set, which support our idea
of using disentangled representation for solving video con-
tent swap task. We also found the frame generated using
CGAN looks more photo realistic and CGAN appoarch is
better at this problem.

Future direction of project is to experiment with the meth-
ods on large action data set with various poses, background
and different human appearances. The large data set will
improve the model generalization to the a new pose and
content in the test data. With more complex data set, we
would have a more accurate evaluation of the strength and
weakness of each method and develop better model for
video content swap task.

References

Sergey Tulyakov, Ming-Yu Liu, Xiaodong Yang, and Jan
Kautz. Mocogan: Decomposing motion and content for
video generation. 2018 IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition, pages 1526–1535,
2018.

Zhe Cao, Gines Hidalgo, Tomas Simon, Shih-En Wei, and
Yaser Sheikh. OpenPose: realtime multi-person 2D pose
estimation using Part Afﬁnity Fields. In arXiv preprint
arXiv:1812.08008, 2018.

Diederik P Kingma and Max Welling. Auto-encoding vari-
ational bayes. arXiv preprint arXiv:1312.6114, 2013.

Ian Goodfellow,

and Yoshua Bengio.

Jean Pouget-Abadie, Mehdi Mirza,
Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron
Courville,
Generative ad-
In Z. Ghahramani, M. Welling,
versarial nets.
C. Cortes, N. D. Lawrence, and K. Q. Weinberger, ed-
itors, Advances in Neural Information Processing Sys-
tems 27, pages 2672–2680. Curran Associates, Inc.,
2014. URL http://papers.nips.cc/paper/
5423-generative-adversarial-nets.pdf.

Mehdi Mirza and Simon Osindero. Conditional generative
adversarial nets. Computer Science, pages 2672–2680,
2014.

Yingwei Pan, Zhaofan Qiu, Ting Yao, Houqiang Li, and
Tao Mei. To create what you tell: Generating videos

Video Content Swapping Using GAN

In Proceedings of the 25th ACM inter-
from captions.
national conference on Multimedia, pages 1789–1798.
ACM, 2017.

Ceyuan Yang, Zhe Wang, Xinge Zhu, Chen Huang, Jian-
ping Shi, and Dahua Lin. Pose guided human video gen-
eration. In Proceedings of the European Conference on
Computer Vision (ECCV), pages 201–216, 2018.

Emily L Denton et al. Unsupervised learning of disentan-
In Advances in Neu-
gled representations from video.
ral Information Processing Systems, pages 4414–4423,
2017.

Justus Thies, Michael Zollh¨ofer, Marc Stamminger, Chris-
tian Theobalt, and Matthias Niessner. Face2face: Real-
time face capture and reenactment of rgb videos. Com-
mun. ACM, 62(1):96–104, December 2018. ISSN 0001-
0782. doi: 10.1145/3292039. URL http://doi.
acm.org/10.1145/3292039.

Kevin Dale, Kalyan Sunkavalli, Micah K. Johnson, Daniel
Vlasic, Wojciech Matusik, and Hanspeter Pﬁster. Video
In Proceedings of the 2011 SIG-
face replacement.
GRAPH Asia Conference, volume 30, pages 130:1–
130:10, 2011.

Danilo Jimenez Rezende, Shakir Mohamed, and Daan
Wierstra. Stochastic backpropagation and approximate
arXiv preprint
inference in deep generative models.
arXiv:1401.4082, 2014.

Sergey Tulyakov, Andrew Fitzgibbon, and Sebastian
Improving deep generative
arXiv preprint

Nowozin. Hybrid vae:
models using partial observations.
arXiv:1711.11566, 2017.

Hao-Shu Fang, Shuqin Xie, Yu-Wing Tai, and Cewu Lu.
Rmpe: Regional multi-person pose estimation. In Pro-
ceedings of the IEEE International Conference on Com-
puter Vision, pages 2334–2343, 2017.

Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A
Efros. Unpaired image-to-image translation using cycle-
In Proceedings of the
consistent adversarial networks.
IEEE International Conference on Computer Vision,
pages 2223–2232, 2017.

