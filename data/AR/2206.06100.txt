AR-NeRF: Unsupervised Learning of Depth and Defocus Effects
from Natural Images with Aperture Rendering Neural Radiance Fields

Takuhiro Kaneko

NTT Communication Science Laboratories, NTT Corporation

Figure 1. Unsupervised learning of depth and defocus effects from unstructured (and view-limited) natural images. (a) During
training, we used only a collection of unstructured (and view-limited) single natural images and did not use any supervision (e.g., ground-
truth depth, pairs of multiview images, defocus supervision, or pretrained models). (b) After training, our model, called AR-NeRF, can
generate sets of images and depths. In particular, in the generation of an image, AR-NeRF can adjust the defocus strength and focus
distance intuitively and continuously using photometric constraints. The project page is available at https://www.kecl.ntt.co.
jp/people/kaneko.takuhiro/projects/ar-nerf/.

Abstract

Fully unsupervised 3D representation learning has
gained attention owing to its advantages in data collec-
tion. A successful approach involves a viewpoint-aware ap-
proach that learns an image distribution based on genera-
tive models (e.g., generative adversarial networks (GANs))
while generating various view images based on 3D-aware
models (e.g., neural radiance fields (NeRFs)). However,
they require images with various views for training, and
consequently, their application to datasets with few or lim-
ited viewpoints remains a challenge. As a complementary
approach, an aperture rendering GAN (AR-GAN) that em-
ploys a defocus cue was proposed. However, an AR-GAN is
a CNN-based model and represents a defocus independently
from a viewpoint change despite its high correlation, which
is one of the reasons for its performance. As an alternative
to an AR-GAN, we propose an aperture rendering NeRF
(AR-NeRF), which can utilize viewpoint and defocus cues
in a unified manner by representing both factors in a com-
mon ray-tracing framework. Moreover, to learn defocus-
aware and defocus-independent representations in a disen-
tangled manner, we propose aperture randomized training,
for which we learn to generate images while randomizing
the aperture size and latent codes independently. During

our experiments, we applied AR-NeRF to various natural
image datasets, including flower, bird, and face images, the
results of which demonstrate the utility of AR-NeRF for un-
supervised learning of the depth and defocus effects.

1. Introduction

Natural images are 2D projections of the 3D world.
Solving the inverse problem, i.e., understanding the 3D
world from natural images, is a principal challenge in com-
puter vision and graphics and has been actively studied in
various fields owing to its diverse applications, such as en-
vironmental understanding in robotics, content creation in
advertisements, and photo editing in the arts.

After collecting pairs of 2D and 3D data or sets of mul-
tiview images, a successful approach is to learn the 3D
predictor using direct or photometric-driven supervision.
This approach demonstrates promising results in terms of
fidelity. However, the collection of such data is often diffi-
cult or impractical. To reduce the collection costs, learning
from single images (i.e., from a dataset that includes a sin-
gle image per training instance) has been actively studied.

To obtain clues under such setting, several studies [21,
35, 86, 88, 104] have introduced object-specific shape mod-
els, including 3DMM [5] and SMPL [58], and searched

1

Defocus strengthFocus distanceDepthWeakStrongNearFar(a) Training imagesUnstructuredsingle images(b) Generated images and depthsfor solutions within the shape model constraints. Other
studies have utilized auxiliary information such as 2D key-
points [36, 98] or 2D silhouettes [10, 24, 30, 49] to sim-
plify the problem by aligning the object parts or separating
the target objects from the background. These studies also
demonstrate remarkable results; however, the construction
of the shape model is not always easy and narrows the appli-
cable objects, and auxiliary information incurs extra costs in
terms of data collection.

To alleviate such restrictions, a fully unsupervised ap-
proach, which learns 3D representations from single im-
ages without any additional supervision (including auxil-
iary information and pre-trained models), has gained atten-
tion. Under this setting, the viewpoint is a principal clue,
which typical methods utilize by learning an image distri-
bution using a generative model (e.g., a generative adver-
sarial network (GAN) [25]) while generating various view-
point images based on viewpoint-aware 3D models, such
as voxels [30, 67, 68], primitives [51], and neural radiance
fields (NeRFs) [9, 26, 65, 69, 70, 87]. This allows learning a
viewpoint-aware 3D representation; however, owing to the
diverse viewpoints needed, the application to a dataset in
which viewpoint cues are limited or unavailable without the
use of a preprocessing (e.g., natural flower or bird images,
as shown in Figure 1) remains a challenge.

As a complement to a viewpoint cue, an aperture render-
ing GAN (AR-GAN) [37] was proposed to exploit a defo-
cus cue by equipping the aperture rendering [94] on top of
a CNN GAN. This constraint allows the learning of both
depth and depth-of-field (DoF) effects in an unsupervised
manner. However, as a limitation, an AR-GAN employs
the defocus cue independently from the viewpoint cue and
cannot utilize both factors jointly despite these two factors
being highly correlated with the ability to help each other.1
Consequently, the quality of the depth prediction when us-
ing AR-GAN remains limited.

We thus aim to construct a unified model that can lever-
age defocus and viewpoint cues jointly by considering the
application of unsupervised learning of the 3D representa-
tion (particularly depth and defocus effects) from natural un-
structured (and view-limited) images (Figure 1). To achieve
this, we propose a new extension of NeRF called aperture
rendering NeRF (AR-NeRF), which can represent defocus
effects and viewpoint changes in a unified manner by repre-
senting both factors through a common ray-tracing frame-
work. More precisely, in contrast to the standard NeRF,
which represents each pixel using a single ray under the pin-
hole camera assumption, AR-NeRF employs an aperture
camera [90] that represents each pixel using a collection of
rays that converge at the focus plane and whose scale is de-

in

[37],

1More precisely,

the combinations of an AR-GAN
and viewpoint-aware GANs (particularly, HoloGAN [67] and RGBD-
GAN [74]) are provided. These models can learn the defocus and
viewpoint-aware representations simultaneously but individually; there-
fore, such models cannot utilize the learning of one representation for the
learning of another.

termined according to the aperture size. Through such mod-
eling, we can represent both viewpoint changes and defocus
effects by simply changing the inputs and the integration of
the implicit function (multilayer perceptron (MLP)), which
converts the point position and view direction into the RGB
color and volume density. Consequently, through training,
we can optimize the MLP while reflecting both factors.

Moreover, to disentangle defocus-aware and defocus-
independent representations in an unsupervised manner, we
introduce aperture randomized training, in which we learn
to generate images in a GAN framework while changing the
aperture size and latent codes both randomly and indepen-
dently. A similar technique is commonly used in viewpoint-
aware representation learning [9, 26, 30, 51, 65, 67–70, 87],
and this training is useful for disentangling the effect of the
corresponding factor from latent codes.

We applied AR-NeRF to natural image datasets, includ-
ing view-limited (Oxford Flowers [73] (flower) and CUB-
200-2011 [101] (bird)) datasets and a dataset with various
views (FFHQ [42] (face)), and demonstrated that AR-NeRF
is better than or comparable to the baseline models, includ-
ing a state-of-art fully unsupervised depth-learning model
(i.e., AR-GAN [37]) and generative NeRF (particularly pi-
GAN [9]), in terms of the depth prediction accuracy. We
also demonstrated that AR-NeRF can manipulate the defo-
cus effects (i.e., defocus strength and focus distance) intu-
itively and continuously while retaining the image quality,
whereas AR-GAN has difficulty doing so.

Overall, our contributions can be summarized as follows:

• To achieve an unsupervised learning of the depth and
defocus effects, we propose a new extension of NeRF
called AR-NeRF, which can employ viewpoint and de-
focus cues in a unified manner by representing both
factors in a common ray-tracing framework.

• To

and

disentangle

defocus-aware

representations

defocus-
independent
unsupervised
conditions, we introduce aperture randomized train-
ing, by which we learn to generate images while
changing the aperture size and latent codes both
randomly and independently.

under

• We empirically demonstrate the utility of AR-NeRF
for the unsupervised learning of the depth and defocus
effects using various natural image datasets, including
view-limited (flower and bird) datasets and a dataset of
various views (face). We provide detailed analyses and
extended results in the Appendices.2

2. Related work

Implicit neural representations. Owing to their 3D-
aware, continuous, and memory-efficient nature,
im-
plicit neural representations have gained attention in both
learning-based 3D [2, 11, 12, 22, 63, 64, 71, 75, 77, 85] and

2The project page is available at https://www.kecl.ntt.co.

jp/people/kaneko.takuhiro/projects/ar-nerf/.

2

scene [8,13,34,78] reconstructions. Typical representations
are supervised using 3D data; however, to eliminate the
need for 3D supervision, the incorporation of differentiable
rendering has also been proposed [55, 56, 72, 93, 112]. The
most relevant model is NeRF [65], which combines implicit
neural representations with volume rendering for a novel
view synthesis. Our AR-NeRF is based on NeRF and ob-
tains a viewpoint-aware functionality by inheriting it. How-
ever, to obtain the defocus-aware functionality, AR-NeRF
employs an aperture camera model instead of a pinhole
camera model, which is typically used in NeRF. Moreover,
the described studies aimed to learn a single network per
scene using a set of multiview images, whereas we aimed to
construct a generative model from the collection of unstruc-
tured single images. Owing to this difference, we do not
aim to compare AR-NeRF with NeRF in this study; how-
ever, reimporting our idea (i.e., the usage of an aperture
camera) to the original task remains for future research.
Generative adversarial networks. GANs [25] have shown
remarkable results in 2D image modeling through a se-
ries of advancements (e.g., [7, 40–43]). A strong prop-
erty of GANs is their ability to learn the data distribution
through random sampling without directly defining the dis-
tribution. This property allows GANs to learn the data dis-
tribution through measurements [6, 38, 39, 48, 76] and ar-
chitectural constraints [99, 105, 107, 111, 120]. Using the
same logic, unsupervised 3D-aware GANs [9, 26, 30, 37,
51, 65, 67–70, 74, 87, 95] have succeeded in learning 3D-
aware representations by incorporating 3D-2D projection
modules and/or 3D-aware constraints. More specifically,
most studies address viewpoint-aware representation learn-
ing using 3D representations based on voxels [30, 67, 68],
primitives [51], and NeRFs [9, 26, 65, 69, 70, 87], and a few
studies [37] have addressed the learning of defocus-aware
representations. Herein, we introduce a unified model that
can jointly leverage both defocus and viewpoint cues to
strengthen the latter category model. We demonstrate the
utility of their joint usage in the experiments described in
Section 5.3.
Learning 3D representations from single images. As dis-
cussed in Section 1, to eliminate the cost of collecting 3D
data or multiview images, the learning of a 3D represen-
tation from single images has garnered attention. Promis-
ing approaches involve the use of shape models [21, 35, 86,
88,104] and the incorporation of auxiliary information such
as 2D keypoints [36, 98] or 2D silhouettes [10, 24, 30, 49].
Although such approaches have yielded remarkable results,
the requirement for shape models or auxiliary information
remains a bottleneck. To eliminate this bottleneck, a fully
unsupervised learning approach based on generative mod-
els has been actively studied. The learning targets differ
according to the studies applied, and to date, the unsuper-
vised learning of viewpoints [9, 26, 30, 51, 65, 67–70, 74,
84, 87, 95, 108], albedo [108], texture [95], light [108], 3D
meshes [84,95], depth [37,74,108], and defocus effects [37]
has been proposed. Among these approaches, AR-NeRF

shares the motivation with AR-GAN [37] with the aim of
learning the depth and defocus effects. However, as the
main difference, an AR-GAN represents an aperture ren-
derer in discretized CNNs and is specific to the learning of
the depth and defocus effects, whereas our AR-NeRF rep-
resents an aperture renderer with continuous radiance fields
and can explain and utilize other ray-tracing-related phe-
nomena (e.g., viewpoints) in a unified manner. We empiri-
cally demonstrate these merits in Section 5.2.
Learning of depth and defocus effects. There is a large
body of studies conducted on depth learning. Representa-
tive approaches involve training the depth predictor using
pairs or sets of data, such as image and depth pairs [17, 18,
46, 47, 53, 110], multiview image pairs [20, 23, 109], and
consecutive frame sets [102, 113, 117]. Defocus synthesis
has also garnered interest in computer vision and graph-
ics, and both model-based [3, 28, 33, 89, 100] and learning-
based [32, 80, 94, 103] defocus synthesizers have been pro-
posed. Based on the high correlation between the depth and
defocus strength, some studies [27,94] have proposed learn-
ing the depth while reconstructing focused images from all-
in-focus images under the assumption that pairs of focused
and all-in-focus images are available for training. Although
our study is motivated by the success of such studies, the
main difference is that we address a challenging but practi-
cally important situation in which there are no training data
available other than natural unstructured (and view-limited)
images. The latest model addressing this problem is an AR-
GAN [37]. As stated previously, we investigate the quanti-
tative and qualitative differences in Section 5.2.

3. Preliminaries

3.1. GAN

We begin by describing the two previous work upon
which our model is built. The first is a GAN [25], which
learns the data distribution implicitly through a two-player
min-max game using the following objective function:

LGAN = EIr∼pr(I)[log D(Ir)]

+ Ez∼pg(z)[log(1 − D(G(z)))],

(1)

where, given a latent code z ∼ pg(z), a generator G gen-
erates an image Ig that fools the discriminator D by mini-
mizing LGAN, whereas D distinguishes Ig from a real im-
age Ir by maximizing LGAN. Here, the superscripts r and
g represent real and generated data, respectively. Through
adversarial training, pg(I) reaches close to pr(I).

3.2. NeRF

NeRF [65] (in particular, we consider generative vari-
ants [9,87] relevant to our study) represents a scene using an
MLP that takes the 3D position x ∈ R3 and view direction
d ∈ S2 as inputs and predicts the RGB color c(x, d) ∈ R3
and volume density σ(x) ∈ R+. More precisely, in [9, 87],
positional encoding [65,97] and sine nonlinearity [91] were

3

σ(r(s))ds

.

(2)

4.2. Aperture rendering with NeRF

used prior to or during the application of the MLP to encode
positional information; however, we omit them for a general
representation. Moreover, in a generative variant, the MLP
also takes the latent code z ∈ RLz as input to represent a
variety of data. However, this is omitted for simplicity.

NeRF employs a pinhole camera (Figure 2(a)) and pre-
dicts the color of each pixel C(r) and the corresponding
depth Z(r) by integrating over a single camera ray r(t) =
o + td (where o and d are the camera origin and direction,
respectively) within a distance t ∈ [tn, tf ] using the volume
rendering equation [61]:

(cid:90) tf

tn
(cid:90) tf

C(r) =

Z(r) =

T (t)σ(r(t))c(r(t), d)dt,

T (t)σ(r(t))tdt,

tn

(cid:18)

where T (t) = exp

−

(cid:90) t

tn

(cid:19)

In practice, the integral is intractable; thus, a discretized
form with stratified and hierarchical sampling [65] is used.

4. Aperture rendering NeRF: AR-NeRF

4.1. Problem statement

We first clarify the problem statement. We address fully
unsupervised learning of the depth and defocus effects,
where no supervision or pretrained models are available and
only a collection of unstructured single images are accessi-
ble during training. Owing to the lack of explicit super-
vision, it is difficult to learn a conditional model that can
directly predict the depth and defocus effects from an in-
put image. As an alternative, we aim to construct an un-
conditional generator G(z) that can generate the image and
depth as a set while varying the defocus effects. Similar
to viewpoint-aware representation learning, which requires
a dataset that includes various view images to acquire the
viewpoint cue, our defocus-aware representation learning
requires a dataset that includes variously defocused images
to obtain a defocus cue. More formally, we impose the fol-
lowing assumption for the dataset:

Assumption 1 The training images are captured using var-
ious aperture-sized cameras, and the dataset includes di-
versely defocused images.

Two factors affecting the defocus effects (as detailed in
Section 4.2) are the aperture size and focus distance (dis-
tance between the ray origin and the plane where all objects
are in focus). Hence, we can also impose the assumption
of diversity of the focus distance instead of or in addition to
Assumption 1. However, under a practical scenario, the fo-
cused target tends to be fixed when the scene is determined.
Hence, in this study, only Assumption 1 is imposed.3

Figure 2. Comparison of ray tracing on NeRF and AR-NeRF.

Note that we assume the existence of diversely defo-
cused images but do not assume the existence of their
pairs/sets. We observed that this assumption is satisfied in
a typical natural image dataset, as shown in Figure 1.

As described in Section 3.2, NeRF is a strongly 3D-
aware model that can jointly represent an image and depth
at the design level (Equation 2). To utilize this strong prop-
erty in our problem (Section 4.1), we consider representing
aperture rendering in a ray-tracing framework, which is the
basis of NeRF. This is achieved by replacing pinhole cam-
era-based ray tracing (Figure 2(a)), which is used in a stan-
dard NeRF, with aperture camera-based ray tracing [90]
(Figure 2(b)).

For pinhole-camera-based ray tracing, we cast all rays
from a single point o. By contrast, with aperture camera-
based ray tracing, we cast rays from an aperture of radius s.
More formally, the origin of a ray from the aperture (o′) is
written as

o′ = o + u,

(3)

where |u| ∈ [0, s], and the direction of u is orthogonal to o.
A bundle of rays emitted from the aperture converges to
a point on the plane at a focal distance of f . Based on this
definition, the direction of the ray from the aperture (d′) is
calculated as follows:

d′ = (o + f d − o′)/f,

(4)

Based on Equations 3 and 4, we can calculate the ray from
the origin o′, that is, r′(t) = o′ + td′, and render the corre-
sponding color C(r′) and depth Z(r′) using volume render-
ing (Equation 2). The final color and depth are calculated by
integrating over C(r′) and Z(r′) for all rays in |u| ∈ [0, s].4
However, similar to volume rendering, the integral is in-
tractable in practice; therefore, a discretized form is used.
More precisely, we generate a finite bundle of rays from the
sampled |u| ∈ [0, s] and calculate the final output by taking
the average of the corresponding C(r′) and Z(r′).

3A similar assumption (diversity of the DoF settings) has also been in-
troduced in an AR-GAN [37]. However, it does not distinguish the effects
of the two factors, and for a stricter assumption, we redefine it here.

4The depth does not need to be integrated, but we used this formulation
to account for the ambiguity derived from the defocus blur. However, we
empirically found that the effect on the depth accuracy is subtle.

4

(a) Pinhole camera-based ray tracing(b) Aperture camera-based ray tracingon NeRF (baseline)on AR-NeRF (ours)odPinhole camerafsoo’udd’Aperture cameraFocus distanceAperture4.3. Aperture randomized training

To learn defocus-aware and defocus-independent repre-
sentations in a disentangled manner, we introduce aperture
randomized training, in which we learn to generate images
by varying the aperture size and latent codes both randomly
and independently. More formally, we rewrite the GAN ob-
jective function (Equation 1) as follows:5

LAR-NeRF = EIr∼pr(I)[log D(Ir)]

+ Ez∼pg(z),s∼pg(s)[log(1 − D(G(z, s)))],

(5)

where the latent code z and aperture size s are sampled ran-
domly and independently. In practice, we represent pg(s)
using a half-normal distribution and parameterize its stan-
dard deviation σs to determine the range of aperture sizes
in a data-driven manner. As a side note, we represent the
focus distance f , another variable in aperture rendering, us-
ing an MLP, taking z as the input under the assumption that
f is determined according to the rendered target.

As discussed in Section 4.2, our aperture rendering has
a strong 3D constraint based on ray tracing, and therefore
when we train a model using Equation 5, z must capture
the representations that are independent and robust to the
change in defocus driven by the fluctuation of s.

4.4. Advanced techniques for practice

To the best of our knowledge, unsupervised learning of
the depth and defocus effects is a relatively new task (e.g.,
the first attempt was at CVPR 2021 [37]), and practical tech-
niques (in particular, those specific to NeRF) have yet to be
sufficiently developed. To advance this research direction,
we discuss practical techniques considered for this task.

of

Representation
background with
unbounded
NeRF++. A typical generative NeRF [9, 87] renders an
entire scene in a tightly bounded 3D space to efficiently
model the foreground. However, this strategy is problem-
atic when training images with an unbounded background
(e.g., bird images in Figure 1). In particular, this problem
is critical in terms of learning the defocus effects because
its strength is determined according to depth. We cannot
represent a strong defocus effect at the design level when
using a tightly bounded 3D space. To address this problem,
we implemented a synthesis network using NeRF++ [115],
which is composed of a foreground NeRF in a unit sphere
and background NeRF modeled using inverted sphere
parameterization. This implementation allows representing
a strong defocus effect in a far background. For a fair
description, we note that concurrent approaches [26, 69]

5The relevant training scheme is DoF-mixture learning [37], in which
the image generation is learned while generating various defocused im-
ages. The main difference from our approach is that their method manipu-
lates the depth scale instead of the aperture size because they cannot scale
the aperture directly owing to the discretized formulation. Consequently,
their learned depth is relative, and therefore they must carefully tune the
value when changing the focus distance by adding an offset.

have also incorporated NeRF++ for image generation to
represent an unbounded background.
Learning depth and defocus effects with changes in
viewpoint. Fully unsupervised learning of the depth and
defocus effects is a challenging and ill-posed problem, al-
though our aperture randomized training alleviates this dif-
ficulty. To obtain a hint from another source, we jointly
learn viewpoint-aware and view-independent representa-
tions by randomly sampling the camera poses during train-
ing [9, 87]. To prevent sampled camera parameters beyond
a real distribution, we restrict its range (using a standard de-
viation of 0.1 radian in practice). We found that this setting
works reasonably well for both datasets, including limited
and wide viewpoints (Section 5.3).
Aperture ray sampling scheme. In typical ray tracing used
in computer graphics [90], a large number of rays (e.g., 100)
are sampled per pixel in aperture rendering (Section 4.2)
to improve the synthesis fidelity. However, this increases
both the processing time and memory. To efficiently repre-
sent the aperture using limited rays, we used stratified sam-
pling [65]. More concretely, we used five rays; the origin
of one ray was placed at the center of the aperture, and the
origins of the others were placed along the circumference
of the aperture with equal intervals. We examine the effect
of this approximation in Appendix A.2.

5. Experiments

5.1. Experimental settings

We conducted two experiments to verify the effective-
ness of AR-NeRF from multiple perspectives: a compara-
tive study (Section 5.2) in which we compared AR-NeRF to
AR-GAN [37], which is a pioneering model with a similar
objective, and an ablation study (Section 5.3) in which we
investigated the importance of our ideas. In this section, we
present the common settings and discuss the details of each
in the next sections.
Dataset. Following the AR-GAN study [37], we evaluated
AR-NeRF using three natural image datasets:
two view-
limited datasets, i.e., Oxford Flowers [73] (8,189 flower
images with 102 categories) and CUB-200-2011 [101]
(11,788 bird images with 200 categories), and a view-
various dataset, that is, FFHQ [42] (70,000 face images).
To effectively examine various cases, we resized the images
to a pixel resolution of 64 × 64. This strategy was also used
in the AR-GAN study [37]. Therefore, we can compare
AR-NeRF to AR-GAN under fair conditions. We provide
detailed information about the datasets in Appendix C.1.
Evaluation metrics. We evaluated the effectiveness of AR-
NeRF quantitatively using the same two metrics used in the
AR-GAN study [37] for a direct comparison. The first is
the kernel inception distance (KID) [4], which measures
the maximum mean discrepancy between real and generated
images within the inception model [96]. We used the KID
to evaluate the visual fidelity of the generated images. We

5

calculated this score using 20,000 generated images and all
real images. Based on our objective (i.e., training an uncon-
ditional model on unstructured natural images), preparing
the ground truth depth is nontrivial. Following [37], as an
alternative, we calculated the depth accuracy by (1) train-
ing the depth predictor using pairs of images and depths
generated through GANs, (2) predicting the depths of real
images using the trained depth predictor, and (3) comparing
the predicted depths to those predicted by a highly general-
izable monocular depth estimator [109] trained using stereo
pairs.6 To measure the differences in depth, we used the
scale-invariant depth error (SIDE) [17], which measures
the difference between depths in a scale-invariant manner
and is useful for comparing the depths predicted by differ-
ent representation systems. For both metrics, the smaller
the value, the better the performance.

Implementation. We implemented AR-NeRF based on pi-
GAN [9],7 which is a state-of-the-art generative variant of
NeRF. Because the original pi-GAN was not applied to the
dataset used in our experiments, we carefully tuned the con-
figurations and hyperparameters such that the baseline pi-
GAN could generate images reasonably well. Next, we
incorporated a background synthesis network into pi-GAN
based on NeRF++ [115]8 (Section 4.4). Hereafter, we refer
to this model as pi-GAN++. Subsequently, we incorporated
aperture rendering (Section 4.2) and aperture randomized
training (Section 4.3) into pi-GAN++. This is the model
denoted by AR-NeRF below. We provide implementation
details in Appendix C.1.

5.2. Comparative study

To determine the validity of AR-NeRF for unsupervised
learning of the depth and defocus effects, we first investi-
gated the comparative performance between AR-NeRF and
AR-GAN [37], which is a state-of-art model for this prob-
lem. The main difference between AR-NeRF and AR-GAN
is the architectural difference, where AR-NeRF is con-
structed based on the continuous radiance fields, whereas
AR-GAN is constructed based on discretized CNNs. An-
other small but significant difference is that AR-NeRF rep-
resents the defocus distribution (i.e., aperture size distribu-
tion) using a half-normal distribution (Equation 5), whereas
AR-GAN represents it (i.e., depth scale distribution in this
case) using a binomial distribution (Equation 6 in [37]). To
confirm the effects of this difference, we also evaluated a
variant of AR-GAN (referred to as AR-GAN++), in which
the defocus distribution was expressed using a half-normal
distribution, similar to AR-NeRF. Furthermore, as a ref-
erence, we report the scores of RGBD-GAN [74], which

6We used the official pretrained model: https://github.com/

KexianHust/Structure-Guided-Ranking-Loss.

7We implemented it based on the official code: https://github.

com/marcoamonteiro/pi-GAN.

8We implemented this while referring to the official code: https:

//github.com/Kai-46/nerfplusplus.

Oxford Flowers

CUB-200-2011

FFHQ

KID↓

SIDE↓

KID↓

SIDE↓

KID↓

SIDE↓

AR-GAN
AR-GAN++
RGBD-GAN

11.23
10.18
12.04

AR-NeRF (ours)

7.86

4.46
4.42
7.01

3.94

14.30
13.91
14.92

6.81

3.58
3.61
7.06

3.63

5.75
5.43
6.73

3.67

4.21
4.88
5.81

2.61

Table 1. Comparison of KID↓ (×103) and SIDE↓ (×102) be-
tween baseline GANs and AR-NeRF (ours).

learns the depth information using viewpoint cues.9
Quantitative comparisons. We summarize quantitative
comparison results in Table 1. AR-NeRF outperformed
the baseline GANs in terms of the KID and SIDE, except
for SIDE on CUB-200-2011, where AR-GAN/AR-GAN++
was comparable to AR-NeRF.10 These results validate the
utility of AR-NeRF for unsupervised learning of the depth.
We believe that the strengths of AR-NeRF, i.e., the joint us-
age of the viewpoint and defocus cues and continuous rep-
resentations based on implicit functions, contribute to this
improvement. We present qualitative comparisons of the
predicted depths in Figures 13–15 (Appendix B).
Qualitative comparisons. We conducted qualitative com-
parisons to validate the effectiveness of unsupervised learn-
ing of the defocus effects. We present examples of generated
images and depths in Figure 3. In AR-NeRF, we manipu-
lated the defocus strength and focus distance by changing s
and f (Figure 2(b)). As discussed above, the original AR-
GAN discretely represents the defocus distribution. There-
fore, differently from AR-NeRF, it is unsuitable for con-
ducting continuous operations. Alternatively, we examine
the performance of AR-GAN++, which represents a con-
tinuous defocus distribution. In AR-GAN++, we manipu-
lated the defocus strength and focus distance by changing
the scale and offset of depth, respectively.

The results indicate that AR-NeRF can manipulate both
the defocus strength and focus distance without generating
significant artifacts. In particular, in the manipulation of the
focus distance, AR-NeRF succeeds in refocusing on both
the foreground and background, the appearances of which
are the same as those in the all-in-focus images (in the left-
most column). By contrast, AR-GAN++ often generates
unexpected artifacts, particularly when it attempts to refo-
cus on the background (in the second-to-last column). As
possible causes for this phenomenon, (1) AR-GAN++ dis-

9In our preliminary experiments, we also examined the performances
of AR-HoloGAN/AR-RGBD-GAN (combinations of AR-GAN and Holo-
GAN [67]/RGBD-GAN [74]) on FFHQ. We found that the SIDE scores
for AR-HoloGAN and AR-RGBD-GAN were 4.79 and 4.40, respectively,
and were worse than those for AR-GAN. This result indicates that the si-
multaneous but individual usage of the viewpoint and defocus cues in AR-
HoloGAN/AR-RGBD-GAN does not improve the depth learning.

10SIDE has a limitation in that it can ignore certain types of degradation
because it measures the difference based on l2, causing statistical aver-
aging. This may be why SIDE is comparable on CUB-200-2011 despite
the qualitative differences (Figure 3). To validate this hypothesis, we ana-
lyzed the gradient of the difference between the ground truth and predicted
depths [16] and found that AR-NeRF outperformed AR-GAN/AR-GAN++
on this metric. We discuss the details in Appendix A.7.

6

Figure 3. Comparison of generated images and depths between AR-GAN++ and AR-NeRF (ours). To manipulate the defocus
strength, we varied the strength within [0, σs, 2σs, 3σs], where σs indicates the standard deviation of a half-normal distribution, which is
used to represent the defocus distribution during training. To manipulate the focus distance, we used a range in which the foregrounds and
backgrounds were focused.

cretely represents light fields in a 2D space; thus, the dis-
cretization error becomes critical when large manipulations
are counted and (2) the predicted depths include artifacts
(e.g., holes appearing in objects), causing errors when im-
ages are rendered based on depth. The properties of AR-
NeRF, that is, (1) a continuous representation in a 3D space
and (2) joint usage of defocus and viewpoint cues, are use-
ful for addressing these defects. As another advantage of
AR-NeRF, it can increase the resolution of the generated
images by increasing the density of the input points owing
to the nature of the implicit function [9]. We demonstrate
this strength in Figure 1, where 128 × 128 images are gen-
erated using the same models as those used in Figure 3.

5.3. Ablation study

We conducted ablation studies to examine the utility of
AR-NeRF as a generative variant of the NeRF. We com-
pared AR-NeRF to five baselines: pi-GAN [9], where a
background synthesis network and aperture rendering are
ablated; pi-GAN++, where aperture rendering is ablated;
AR-NeRF-0, where viewpoint changes (Section 4.4) are not
applied during training; AR-NeRF-F, where the full view-
point changes that are optimized to the face dataset (FFHQ)
are used; and pi-GAN++-F, where aperture rendering is
ablated from AR-NeRF-F. We tested the last two models
on FFHQ only because viewpoint cues were limited on the
other datasets. For pi-GAN++, we set the number of rays to
be the same as that in AR-NeRF by an ensemble of multiple
rays with an aperture size of s = 0. We used this imple-

Oxford Flowers

CUB-200-2011

FFHQ

(B) (D) (V) KID↓

SIDE↓ KID↓

SIDE↓ KID↓

SIDE↓

L
pi-GAN
pi-GAN++ ✓
L
AR-NeRF-0 ✓ ✓ 0
AR-NeRF-F ✓ ✓ F
pi-GAN++-F ✓
F
AR-NeRF ✓ ✓ L

3.69
8.30
6.81
–
–

7.86

5.23
4.83
4.03
–
–

3.94

5.04
9.84
8.67
–
–

6.81

4.87
3.88
3.74
–
–

3.63

4.29
4.43
3.83
4.59
5.06

3.67

3.03
2.69
3.61
2.75
2.78

2.61

Table 2. Comparison of KID↓ (×103) and SIDE↓ (×102) be-
tween AR-NeRF and ablated models. Check marks (B) and (D)
indicate the use of a background synthesis network and defocus
cue, respectively. In column (V), L, F, and 0 indicate the use of
local, full, and no viewpoint changes, respectively.

mentation to investigate the pure performance differences
between the models with and without aperture rendering.

Results. We list the quantitative results in Table 2 and pro-
vide a qualitative comparison of the predicted depths in Fig-
ures 13–15 (Appendix B). Our findings are as follows:

(1) Effects of the background synthesis network (pi-GAN
vs. pi-GAN++). We found that pi-GAN outperforms pi-
GAN++ in terms of the KID. We consider that the com-
pact representation of the pi-GAN is advantageous for effi-
ciently learning 2D image distributions. However, pi-GAN
was outperformed by pi-GAN++ in terms of the SIDE. This
result indicates that pi-GAN is unsuitable for our aims (i.e.,
unsupervised learning of the depth) despite its ability to
generate high-fidelity images.

(2) Effects of aperture rendering (pi-GAN++ vs. AR-

7

Defocus strengthFocus distanceDepthWeakStrongNearFarOxford FlowersAR-GAN++CUB-200-2011FFHQAR-GAN++AR-GAN++AR-NeRF(ours)AR-NeRF(ours)AR-NeRF(ours)NeRF). We found that AR-NeRF outperformed pi-GAN++
on both metrics, except for SIDE on FFHQ, where pi-
GAN++ was comparable to AR-NeRF. The same tendency
holds for the comparison between AR-NeRF-F and pi-
GAN++–F. This is because FFHQ includes sufficient view-
point variations to leverage the viewpoint cues. By contrast,
Oxford Flowers and CUB-200-2011 do not contain them. In
this case, the defocus cues used in AR-NeRF contributed to
an improvement.
(3) Comparison between viewpoint and defocus cues (pi-
GAN++ vs. AR-NeRF-0). With these models, the defocus
and viewpoint manipulations are ablated. Therefore, we can
analyze each effect by comparing them. We found that, in
FFHQ, pi-GAN++ outperformed AR-NeRF-0 in terms of
SIDE, whereas in the other datasets, AR-NeRF-0 outper-
formed pi-GAN++. This can be explained by differences in
the availability of the viewpoint cues, as discussed in (2).
(4) Comparison between local and full viewpoint changes
(AR-NeRF vs. AR-NeRF-F). We found that AR-NeRF out-
performed AR-NeRF-F on both metrics. This result indi-
cates that we do not need to carefully tune the camera pa-
rameters for unsupervised depth learning. The same ten-
dency was observed in the comparison between pi-GAN++
and pi-GAN++-F. Note that AR-NeRF-F has an advan-
tage in the viewpoint manipulation capability because it can
learn full view variations, whereas AR-NeRF can only learn
local view variations.
Detailed analyses. For further analyses, we examined (1)
the importance of learning defocus effects from images
(Appendix A.1), (2) the effect of the aperture ray sampling
scheme (Appendix A.2), (3) simultaneous control of the
viewpoint and defocus (Appendix A.3), (4) generation of
higher-resolution images (Appendix A.4), (5) application to
defocus renderer (Appendix A.5), (6) the Fr´echet inception
distance (FID) [31] (Appendix A.6), and (7) the gradient of
the difference in depth [16] (Appendix A.7). See the Ap-
pendices for further details.

6. Discussion

6.1. Limitations and future work

AR-NeRF has two limitations, stemming from radiance

field representations and fully unsupervised learning.
Limitations caused by radiance field representations. In
radiance field representations, the computational complex-
ity increases not only with the image size but also with the
depth along each ray. Consequently, the calculation cost
is higher than that of a CNN GAN (e.g., AR-GAN [37]);
therefore, application to high-resolution images is diffi-
cult. AR-NeRF requires multiple rays per pixel to repre-
sent aperture rendering. Therefore, it incurs a larger calcu-
lation cost than the standard NeRF, which represents each
pixel using a single ray. Through our experiments, we
found that AR-NeRF outperforms the baseline NeRF, which
has a similar calculation cost (in particular, pi-GAN++).

Figure 4. Failure case.

This demonstrates the validity of our research direction.
However, improving the calculation cost remains an im-
portant topic for future research. Recent concurrent stud-
ies [19, 26, 52, 81, 92, 114] have addressed reducing the cal-
culation cost of NeRF, and the incorporation of these meth-
ods is also a promising research area.
Limitations caused by fully unsupervised learning. Fully
unsupervised learning of the depth and defocus effects is
highly challenging, and some limitations remain. During
our experiments, we found that our model is better than
or comparable to the models trained under the same con-
ditions. However, its performance is lower than that of
supervised models.
In particular, application to complex
images will be difficult because AR-NeRF is a generative
approach that assumes that it can learn the image genera-
tion reasonably well. Furthermore, the use of an unbounded
background based on NeRF++ [115] allows strong defocus
effects that occur in the far plane to be represented. How-
ever, it is still difficult to distinguish the defocus blur from
the flat texture when the defocus blur is extremely strong
(e.g., Figure 4). Addressing these problems is a possible
direction for future research.

6.2. Potential negative social impact

The method presented in this paper enables the creation
of realistic images. This poses a potential risk to the cre-
ation of misleading content (e.g., deepfake). In particular,
our model can increase the credibility of fake content in
terms of 3D consistency and may potentially deceive sys-
tems that rely on 3D structures, such as face recognition
systems. Therefore, we believe that it is essential for the
community to develop technology to distinguish fake im-
ages from real images and carefully monitor advancements
in the corresponding research fields [1, 50, 60, 66, 83].

7. Conclusion

To advance the research on the fully unsupervised learn-
ing of depth and defocus effects, we introduced AR-NeRF,
which extends NeRF by incorporating aperture rendering.
AR-NeRF is noteworthy because it can employ defocus and
viewpoint cues in a unified manner by representing both
factors through a common ray-tracing framework. We em-
pirically demonstrated the effectiveness of AR-NeRF for
unsupervised learning of the depth and defocus effects. Al-
though we focused on a generative variant of NeRF in this
study, our idea, that is, the incorporation of aperture ren-
dering in NeRF, is general, and we expect that its usage will
broaden the applications of NeRF under practical scenarios.

8

Focus distanceDepthNearFarReferences

[1] Shruti Agarwal, Hany Farid, Yuming Gu, Mingming He,
Koki Nagano, and Hao Li. Protecting world leaders against
deep fakes. In CVPR Workshops, 2019. 8

[2] Matan Atzmon and Yaron Lipman. SAL: Sign agnostic
learning of shapes from raw data. In CVPR, 2020. 2
[3] Jonathan T. Barron, Andrew Adams, YiChang Shih, and
Carlos Hern´andez. Fast bilateral-space stereo for synthetic
defocus. In CVPR, 2015. 3

[4] Mikołaj Bi´nkowski, Dougal J. Sutherland, Michael Arbel,
and Arthur Gretton. Demystifying MMD GANs. In ICLR,
2018. 5

[5] Volker Blanz and Thomas Vetter. A morphable model for

the synthesis of 3D faces. In SIGGRAPH, 1999. 1

[6] Ashish Bora, Eric Price, and Alexandros G. Dimakis. Am-
bientGAN: Generative models from lossy measurements.
In ICLR, 2018. 3

[7] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large
scale GAN training for high fidelity natural image synthe-
sis. In ICLR, 2019. 3

[8] Rohan Chabra, Jan E. Lenssen, Eddy Ilg, Tanner Schmidt,
Julian Straub, Steven Lovegrove, and Richard Newcombe.
Deep local shapes: Learning local SDF priors for detailed
3D reconstruction. In ECCV, 2020. 3

[9] Eric R. Chan, Marco Monteiro, Petr Kellnhofer, Jiajun Wu,
and Gordon Wetzstein. pi-GAN: Periodic implicit genera-
tive adversarial networks for 3D-aware image synthesis. In
CVPR, 2021. 2, 3, 5, 6, 7, 14, 28

[10] Wenzheng Chen, Huan Ling, Jun Gao, Edward Smith,
Jaakko Lehtinen, Alec Jacobson, and Sanja Fidler. Learn-
ing to predict 3D objects with an interpolation-based differ-
entiable renderer. In NeurIPS, 2019. 2, 3

[11] Zhiqin Chen, Kangxue Yin, Matthew Fisher, Siddhartha
Chaudhuri, and Hao Zhang. BAE-NET: Branched autoen-
coder for shape co-segmentation. In ICCV, 2019. 2
[12] Zhiqin Chen and Hao Zhang. Learning implicit fields for

generative shape modeling. In CVPR, 2019. 2

[13] Julian Chibane, Aymen Mir, and Gerard Pons-Moll. Neural
unsigned distance fields for implicit function learning. In
NeurIPS, 2020. 3

[14] Terrance DeVries and Graham W. Taylor.

Improved reg-
ularization of convolutional neural networks with cutout.
arXiv preprint arXiv:1708.04552, 2017. 29

[15] Vincent Dumoulin, Ethan Perez, Nathan Schucher, Florian
Strub, Harm de Vries, Aaron Courville, and Yoshua Ben-
gio. Feature-wise transformations. Distill, 3(7):e11, 2018.
28

[16] David Eigen and Rob Fergus. Predicting depth, surface nor-
mals and semantic labels with a common multi-scale con-
volutional architecture. In ICCV, 2015. 6, 8, 15

[17] David Eigen, Christian Puhrsch, and Rob Fergus. Depth
map prediction from a single image using a multi-scale
deep network. In NIPS, 2014. 3, 6

[18] Huan Fu, Mingming Gong, Chaohui Wang, Kayhan Bat-
manghelich, and Dacheng Tao. Deep ordinal regression
network for monocular depth estimation. In CVPR, 2018. 3
[19] Stephan J. Garbin, Marek Kowalski, Matthew Johnson,
FastNeRF: High-

Jamie Shotton, and Julien Valentin.
fidelity neural rendering at 200FPS. In ICCV, 2021. 8

9

[20] Ravi Garg, Vijay Kumar B. G., Gustavo Carneiro, and Ian
Reid. Unsupervised CNN for single view depth estimation:
Geometry to the rescue. In ECCV, 2016. 3

[21] Baris Gecer, Stylianos Ploumpis, Irene Kotsia, and Stefanos
Zafeiriou. GANFIT: Generative adversarial network fitting
for high fidelity 3D face reconstruction. In CVPR, 2019. 1,
3

[22] Kyle Genova, Forrester Cole, Daniel Vlasic, Aaron Sarna,
William T. Freeman, and Thomas Funkhouser. Learn-
ing shape templates with structured implicit functions. In
ICCV, 2019. 2

[23] Cl´ement Godard, Oisin Mac Aodha, and Gabriel J. Bros-
tow. Unsupervised monocular depth estimation with left-
right consistency. In CVPR, 2017. 3

[24] Shubham Goel, Angjoo Kanazawa, and Jitendra Malik.
Shape and viewpoint without keypoints. In ECCV, 2020.
2, 3

[25] Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville,
and Yoshua Bengio. Generative adversarial nets. In NIPS,
2014. 2, 3, 28

[26] Jiatao Gu, Lingjie Liu, Peng Wang, and Christian Theobalt.
StyleNeRF: A style-based 3D-aware generator for high-
resolution image synthesis. In ICLR, 2022. 2, 3, 5, 8
[27] Shir Gur and Lior Wolf. Single image depth estimation
trained via depth from defocus cues. In CVPR, 2019. 3
[28] Samuel W. Hasinoff and Kiriakos N. Kutulakos. A layer-
based restoration framework for variable-aperture photog-
raphy. In ICCV, 2007. 3

[29] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
In CVPR,

Deep residual learning for image recognition.
2016. 28

[30] Philipp Henzler, Niloy Mitra, and Tobias Ritschel. Escap-
ing Plato’s cave using adversarial training: 3D shape from
unstructured 2D image collections. In ICCV, 2019. 2, 3
[31] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,
Bernhard Nessler, G¨unter Klambauer, and Sepp Hochreiter.
GANs trained by a two time-scale update rule converge to
a Nash equilibrium. In NIPS, 2017. 8, 15

[32] Andrey Ignatov, Jagruti Patel, and Radu Timofte. Ren-
dering natural camera bokeh effect with deep learning. In
CVPR Workshops, 2020. 3

[33] David E. Jacobs, Jongmin Baek, and Marc Levoy. Focal
stack compositing for depth of field control. Stanford Com-
puter Graphics Laboratory Technical Report, 1(1):2012,
2012. 3

[34] Chiyu Max Jiang, Avneesh Sud, Ameesh Makadia, Jingwei
Huang, Matthias Nießner, and Thomas Funkhouser. Local
implicit grid representations for 3D scenes. In CVPR, 2020.
3

[35] Angjoo Kanazawa, Michael J. Black, David W. Jacobs, and
Jitendra Malik. End-to-end recovery of human shape and
pose. In CVPR, 2018. 1, 3

[36] Angjoo Kanazawa, Shubham Tulsiani, Alexei A. Efros, and
Jitendra Malik. Learning category-specific mesh recon-
struction from image collections. In ECCV, 2018. 2, 3
[37] Takuhiro Kaneko. Unsupervised learning of depth and
depth-of-field effect from natural images with aperture ren-
dering generative adversarial networks. In CVPR, 2021. 2,
3, 4, 5, 6, 8, 14, 29

[38] Takuhiro Kaneko and Tatsuya Harada. Noise robust gener-

ative adversarial networks. In CVPR, 2020. 3

[39] Takuhiro Kaneko and Tatsuya Harada. Blur, noise, and
compression robust generative adversarial networks.
In
CVPR, 2021. 3

[40] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehti-
nen. Progressive growing of GANs for improved quality,
stability, and variation. In ICLR, 2017. 3, 29

[41] Tero Karras, Miika Aittala, Samuli Laine, Erik H¨ark¨onen,
Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Alias-free
generative adversarial networks. In NeurIPS, 2021. 3
[42] Tero Karras, Samuli Laine, and Timo Aila. A style-based
generator architecture for generative adversarial networks.
In CVPR, 2019. 2, 3, 5, 28

[43] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten,
Jaakko Lehtinen, and Timo Aila. Analyzing and improving
the image quality of StyleGAN. In CVPR, 2020. 3

[44] Diederik P. Kingma and Jimmy Ba. Adam: A method for

stochastic optimization. In ICLR, 2015. 28, 29

[45] Karol Kurach, Mario Luˇci´c, Xiaohua Zhai, Marcin Michal-
ski, and Sylvain Gelly. A large-scale study on regularization
and normalization in GANs. In ICML, 2019. 15

[46] Yevhen Kuznietsov, Jorg Stuckler, and Bastian Leibe.
Semi-supervised deep learning for monocular depth map
prediction. In CVPR, 2017. 3

[47] Iro Laina, Christian Rupprecht, Vasileios Belagiannis, Fed-
erico Tombari, and Nassir Navab. Deeper depth prediction
with fully convolutional residual networks. In 3DV, 2016.
3

[48] Steven Cheng-Xian Li, Bo Jiang, and Benjamin Marlin.
MisGAN: Learning from incomplete data with generative
adversarial networks. In ICLR, 2019. 3

[49] Xueting Li, Sifei Liu, Kihwan Kim, Shalini De Mello,
Varun Jampani, Ming-Hsuan Yang, and Jan Kautz. Self-
supervised single-view 3D reconstruction via semantic con-
sistency. In ECCV, 2020. 2, 3

[50] Yuezun Li, Xin Yang, Pu Sun, Honggang Qi, and Siwei
Lyu. Celeb-DF: A large-scale challenging dataset for Deep-
Fake forensics. In CVPR, 2020. 8

[51] Yiyi Liao, Katja Schwarz, Lars Mescheder, and Andreas
Geiger. Towards unsupervised learning of generative mod-
els for 3D controllable image synthesis. In CVPR, 2020. 2,
3

[52] David B. Lindell, Julien N. P. Martel, and Gordon Wet-
zstein. AutoInt: Automatic integration for fast neural vol-
ume rendering. In CVPR, 2021. 8

[53] Fayao Liu, Chunhua Shen, Guosheng Lin, and Ian Reid.
Learning depth from single monocular images using deep
IEEE Trans. Pattern Anal.
convolutional neural fields.
Mach. Intell., 38(10):2024–2039, 2015. 3

[54] Rosanne Liu, Joel Lehman, Piero Molino, Felipe Petroski
Such, Eric Frank, Alex Sergeev, and Jason Yosinski. An
intriguing failing of convolutional neural networks and the
CoordConv solution. In NeurIPS, 2018. 28

[55] Shichen Liu, Shunsuke Saito, Weikai Chen, and Hao Li.
Learning to infer implicit surfaces without 3D supervision.
In NeurIPS, 2019. 3

[56] Shaohui Liu, Yinda Zhang, Songyou Peng, Boxin Shi, Marc
Pollefeys, and Zhaopeng Cui. DIST: Rendering deep im-
plicit signed distance function with differentiable sphere
tracing. In CVPR, 2020. 3

[57] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang.
Deep learning face attributes in the wild. In ICCV, 2015.
28

[58] Matthew Loper, Naureen Mahmood, Javier Romero, Ger-
ard Pons-Moll, and Michael J. Black. SMPL: A skinned
multi-person linear model. ACM Trans. Graph., 34(6):1–
16, 2015. 1

[59] Andrew L. Maas, Awni Y. Hannun, and Andrew Y. Ng.
Rectifier nonlinearities improve neural network acoustic
models. In ICML Workshops, 2013. 28

[60] Francesco Marra, Diego Gragnaniello, Davide Cozzolino,
and Luisa Verdoliva. Detection of GAN-generated fake im-
ages over social networks. In MIPR, 2018. 8

[61] Nelson Max. Optical models for direct volume rendering.
IEEE Trans. Vis. Comput. Graph, 1(2):99–108, 1995. 4
[62] Lars Mescheder, Andreas Geiger, and Sebastian Nowozin.
Which training methods for GANs do actually converge?
In ICML, 2018. 28

[63] Lars Mescheder, Michael Oechsle, Michael Niemeyer, Se-
bastian Nowozin, and Andreas Geiger. Occupancy net-
works: Learning 3D reconstruction in function space.
In
CVPR, 2019. 2

[64] Mateusz Michalkiewicz, Jhony K. Pontes, Dominic Jack,
Mahsa Baktashmotlagh, and Anders Eriksson. Implicit sur-
face representations as layers in neural networks. In ICCV,
2019. 2

[65] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,
Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng.
NeRF: Representing scenes as neural radiance fields for
view synthesis. In ECCV, 2020. 2, 3, 4, 5, 13

[66] Lakshmanan Nataraj, Tajuddin Manhar Mohammed, B. S.
Manjunath, Shivkumar Chandrasekaran, Arjuna Flenner,
Jawadul H. Bappy, and Amit K. Roy-Chowdhury. Detecting
GAN generated fake images using co-occurrence matrices.
In EI, 2019. 8

[67] Thu Nguyen-Phuoc, Chuan Li, Lucas Theis, Christian
Richardt, and Yong-Liang Yang. HoloGAN: Unsupervised
In
learning of 3D representations from natural images.
ICCV, 2019. 2, 3, 6, 28, 29

[68] Thu Nguyen-Phuoc, Christian Richardt, Long Mai, Yong-
Liang Yang, and Niloy Mitra. BlockGAN: Learning 3D
object-aware scene representations from unlabelled images.
In NeurIPS, 2020. 2, 3

[69] Michael Niemeyer and Andreas Geiger.

Camera-aware decomposed generative neural
fields. In 3DV, 2021. 2, 3, 5

CAMPARI:
radiance

[70] Michael Niemeyer and Andreas Geiger. GIRAFFE: Rep-
resenting scenes as compositional generative neural feature
fields. In CVPR, 2021. 2, 3

[71] Michael Niemeyer, Lars Mescheder, Michael Oechsle, and
Andreas Geiger. Occupancy flow: 4D reconstruction by
learning particle dynamics. In ICCV, 2019. 2

[72] Michael Niemeyer, Lars Mescheder, Michael Oechsle,
and Andreas Geiger. Differentiable volumetric rendering:
Learning implicit 3D representations without 3D supervi-
sion. In CVPR, 2020. 3

[73] Maria-Elena Nilsback and Andrew Zisserman. Automated
flower classification over a large number of classes.
In
ICVGIP, 2008. 2, 5, 28

10

[74] Atsuhiro Noguchi and Tatsuya Harada. RGBD-GAN: Un-
supervised 3D representation learning from natural image
datasets via RGBD image synthesis. In ICLR, 2020. 2, 3, 6
[75] Michael Oechsle, Lars Mescheder, Michael Niemeyer,
Thilo Strauss, and Andreas Geiger. Texture fields: Learning
texture representations in function space. In ICCV, 2019. 2
[76] Arthur Pajot, Emmanuel de Bezenac, and Patrick Gallinari.
In ICLR,

Unsupervised adversarial image reconstruction.
2018. 3

[77] Jeong Joon Park, Peter Florence, Julian Straub, Richard
Newcombe, and Steven Lovegrove. DeepSDF: Learning
continuous signed distance functions for shape representa-
tion. In CVPR, 2019. 2

[78] Songyou Peng, Michael Niemeyer, Lars Mescheder, Marc
Pollefeys, and Andreas Geiger. Convolutional occupancy
networks. In ECCV, 2020. 3

[79] Ethan Perez, Florian Strub, Harm de Vries, Vincent Du-
moulin, and Aaron Courville. FiLM: Visual reasoning with
a general conditioning layer. In AAAI, 2018. 28

[80] Ming Qian, Congyu Qiao, Jiamin Lin, Zhenyu Guo,
Chenghua Li, Cong Leng, and Jian Cheng. BGGAN:
Bokeh-glass generative adversarial network for rendering
realistic bokeh. In ECCV Workshops, 2020. 3

[81] Christian Reiser, Songyou Peng, Yiyi Liao, and Andreas
Geiger. KiloNeRF: Speeding up neural radiance fields with
thousands of tiny MLPs. In ICCV, 2021. 8

[82] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-
net: Convolutional networks for biomedical image segmen-
tation. In MICCAI, 2015. 14, 29

[83] Andreas Rossler, Davide Cozzolino, Luisa Verdoliva,
Christian Riess, Justus Thies, and Matthias Nießner. Face-
Forensics++: Learning to detect manipulated facial images.
In ICCV, 2019. 8

[84] Mihir Sahasrabudhe, Zhixin Shu, Edward Bartrum, Riza
Alp Guler, Dimitris Samaras, and Iasonas Kokkinos. Lift-
ing AutoEncoders: Unsupervised learning of a fully-
disentangled 3D morphable model using deep non-rigid
structure from motion. In ICCV Workshops, 2019. 3
[85] Shunsuke Saito, Zeng Huang, Ryota Natsume, Shigeo Mor-
ishima, Angjoo Kanazawa, and Hao Li. PIFu: Pixel-aligned
implicit function for high-resolution clothed human digiti-
zation. In ICCV, 2019. 2

[86] Soubhik Sanyal, Timo Bolkart, Haiwen Feng,

and
Michael J. Black. Learning to regress 3D face shape and ex-
pression from an image without 3D supervision. In CVPR,
2019. 1, 3

[87] Katja Schwarz, Yiyi Liao, Michael Niemeyer, and Andreas
Geiger. GRAF: Generative radiance fields for 3D-aware
image synthesis. In NeurIPS, 2020. 2, 3, 5, 14

[88] Jiaxiang Shang, Tianwei Shen, Shiwei Li, Lei Zhou, Ming-
min Zhen, Tian Fang, and Long Quan. Self-supervised
monocular 3D face reconstruction by occlusion-aware
multi-view geometry consistency. In ECCV, 2020. 1, 3
[89] Xiaoyong Shen, Xin Tao, Hongyun Gao, Chao Zhou, and
Jiaya Jia. Deep automatic portrait matting. In ECCV, 2016.
3

[90] Peter Shirley.

Ray

one weekend.
https : / / raytracing . github . io / books /
RayTracingInOneWeekend.html, December 2020.
2, 4, 5

tracing

in

[91] Vincent Sitzmann, Julien Martel, Alexander Bergman,
David Lindell, and Gordon Wetzstein. Implicit neural rep-
resentations with periodic activation functions. In NeurIPS,
2020. 3, 28

[92] Vincent Sitzmann, Semon Rezchikov, William T. Freeman,
Joshua B. Tenenbaum, and Fredo Durand. Light field net-
works: Neural scene representations with single-evaluation
rendering. In NeurIPS, 2021. 8

[93] Vincent Sitzmann, Michael Zollh¨ofer, and Gordon Wet-
zstein. Scene representation networks: Continuous 3D-
structure-aware neural scene representations. In NeurIPS,
2019. 3

[94] Pratul P. Srinivasan, Rahul Garg, Neal Wadhwa, Ren Ng,
and Jonathan T. Barron. Aperture supervision for monocu-
lar depth estimation. In CVPR, 2018. 2, 3

[95] Attila Szab´o, Givi Meishvili, and Paolo Favaro. Unsuper-
vised generative 3D shape learning from natural images.
arXiv preprint arXiv:1910.00287, 2019. 3

[96] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon
Shlens, and Zbigniew Wojna. Rethinking the Inception ar-
chitecture for computer vision. In CVPR, 2016. 5

[97] Matthew Tancik, Pratul P. Srinivasan, Ben Mildenhall, Sara
Fridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi
Ramamoorthi, Jonathan T. Barron, and Ren Ng. Fourier
features let networks learn high frequency functions in low
dimensional domains. In NeurIPS, 2020. 3

[98] Luan Tran and Xiaoming Liu. Nonlinear 3D face mor-

phable model. In CVPR, 2018. 2, 3

[99] Carl Vondrick, Hamed Pirsiavash, and Antonio Torralba.
Generating videos with scene dynamics. In NIPS, 2016. 3
[100] Neal Wadhwa, Rahul Garg, David E. Jacobs, Bryan E. Feld-
man, Nori Kanazawa, Robert Carroll, Yair Movshovitz-
Attias, Jonathan T. Barron, Yael Pritch, and Marc Levoy.
Synthetic depth-of-field with a single-camera mobile
phone. ACM Trans. Graph., 37(4):1–13, 2018. 3

[101] Catherine Wah, Steve Branson, Peter Welinder, Pietro Per-
ona, and Serge Belongie. The Caltech-UCSD Birds-200-
2011 Dataset. Technical Report CNS-TR-2011-001, Cali-
fornia Institute of Technology, 2011. 2, 5, 28

[102] Chaoyang Wang, Jos´e Miguel Buenaposada, Rui Zhu, and
Simon Lucey. Learning depth from monocular videos using
direct methods. In CVPR, 2018. 3

[103] Lijun Wang, Xiaohui Shen, Jianming Zhang, Oliver Wang,
Zhe Lin, Chih-Yao Hsieh, Sarah Kong, and Huchuan Lu.
DeepLens: Shallow depth of field from a single image.
ACM Trans. Graph., 37(6):1–11, 2018. 3

[104] Mengjiao Wang, Zhixin Shu, Shiyang Cheng, Yannis Pana-
gakis, Dimitris Samaras, and Stefanos Zafeiriou. An ad-
versarial neuro-tensorial approach for learning disentangled
Int. J. Comput. Vis., 127(6-7):743–762,
representations.
2019. 1, 3

[105] Xiaolong Wang and Abhinav Gupta. Generative image
modeling using style and structure adversarial networks. In
ECCV, 2016. 3

[106] Peter Welinder, Steve Branson, Pietro Perona, and Serge
In

Belongie. The multidimensional wisdom of crowds.
NIPS, 2010. 28

[107] Jiajun Wu, Chengkai Zhang, Tianfan Xue, Bill Freeman,
and Josh Tenenbaum. Learning a probabilistic latent space
of object shapes via 3D generative-adversarial modeling. In
NIPS, 2016. 3

11

[108] Shangzhe Wu, Christian Rupprecht, and Andrea Vedaldi.
Unsupervised learning of probably symmetric deformable
3D objects from images in the wild. In CVPR, 2020. 3
[109] Ke Xian, Jianming Zhang, Oliver Wang, Long Mai, Zhe
Lin, and Zhiguo Cao. Structure-guided ranking loss for
single image depth prediction. In CVPR, 2020. 3, 6, 25,
26, 27, 29

[110] Dan Xu, Elisa Ricci, Wanli Ouyang, Xiaogang Wang, and
Nicu Sebe. Multi-scale continuous CRFs as sequential deep
networks for monocular depth estimation. In CVPR, 2017.
3

[111] Jianwei Yang, Anitha Kannan, Dhruv Batra, and Devi
Parikh. LR-GAN: Layered recursive generative adversar-
ial networks for image generation. In ICLR, 2017. 3
[112] Lior Yariv, Yoni Kasten, Dror Moran, Meirav Galun, Matan
Atzmon, Ronen Basri, and Yaron Lipman. Multiview neu-
ral surface reconstruction by disentangling geometry and
appearance. In NeurIPS, 2020. 3

[113] Zhichao Yin and Jianping Shi. GeoNet: Unsupervised
learning of dense depth, optical flow and camera pose. In
CVPR, 2018. 3

[114] Alex Yu, Ruilong Li, Matthew Tancik, Hao Li, Ren Ng, and
Angjoo Kanazawa. PlenOctrees for real-time rendering of
neural radiance fields. In ICCV, 2021. 8

[115] Kai Zhang, Gernot Riegler, Noah Snavely, and Vladlen
Koltun. NeRF++: Analyzing and improving neural radi-
ance fields. arXiv preprint arXiv:2010.07492, 2020. 5, 6,
8, 29

[116] Shengyu Zhao, Zhijian Liu, Ji Lin, Jun-Yan Zhu, and Song
Han. Differentiable augmentation for data-efficient GAN
training. In NeurIPS, 2020. 29

[117] Tinghui Zhou, Matthew Brown, Noah Snavely, and
David G. Lowe. Unsupervised learning of depth and ego-
motion from video. In CVPR, 2017. 3

[118] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A.
Efros. Unpaired image-to-image translation using cycle-
consistent adversarial networks. In ICCV, 2017. 14, 29

[119] Jun-Yan Zhu, Richard Zhang, Deepak Pathak, Trevor Dar-
rell, Alexei A. Efros, Oliver Wang, and Eli Shechtman.
Toward multimodal image-to-image translation. In NIPS,
2017. 14, 29

[120] Jun-Yan Zhu, Zhoutong Zhang, Chengkai Zhang, Jia-
jun Wu, Antonio Torralba, Joshua B. Tenenbaum, and
William T. Freeman. Visual object networks: Image gen-
eration with disentangled 3D representation. In NeurIPS,
2018. 3

12

Appendix

This appendix provides detailed analyses (Appendix A),
additional qualitative results (Appendix B), and implemen-
tation details (Appendix C).

A. Detailed analyses

In this section, we present seven detailed analyses to pro-

vide a deeper understanding of the proposed model.

• Appendix A.1: Importance of learning defocus effects.

• Appendix A.2: Effect of aperture ray sampling

scheme.

• Appendix A.3: Simultaneous control of viewpoint and

defocus.

• Appendix A.4: Generation of higher-resolution im-

ages.

• Appendix A.5: Application to defocus renderer.

• Appendix A.6: Fr´echet inception distance.

• Appendix A.7: Gradient of difference in depth.

A.1. Importance of learning defocus effects

As discussed in Section 4.2, we represent aperture ren-
dering in a ray-tracing framework, which is the basis of
its application is not
NeRF. Thus, our idea is general;
restricted to AR-NeRF and can be incorporated into any
NeRF-based model, including existing or pretrained mod-
els. This fact raises the question of whether it is necessary
to learn the defocus effect from the data because we can at-
tach the defocus effects virtually by incorporating aperture
rendering into the model, even if aperture rendering is not
used in the training.

In the main text, we present evidence verifying the im-
portance of learning defocus effects from some perspec-
tives. More specifically, the results in Table 2 indicate that
the learning of the defocus effects is useful for improv-
ing model performance in terms of both image quality and
depth accuracy. In this section, we examine the tolerance to
defocus manipulation.

When a model can manipulate the defocus strength to the
greatest extent possible, we believe that it can generate an
image that is close to a real image even when large defocus
effects are imposed. Based on this consideration, we ex-
amined the KID when large defocus effects were imposed
on pi-GAN++ (i.e., a model without learning the defocus
effects) and AR-NeRF (i.e., a model that learns the defo-
cus effects). More precisely, we calculated the KID when
the aperture size s ∈ {σs, 2σs, 3σs, 4σs, 5σs}, where σs is
the standard deviation of the aperture size obtained through
AR-NeRF training.
Results. We summarize the results in Table 3. We found
that for every dataset and aperture size, AR-NeRF outper-
formed pi-GAN++ in terms of the KID score. In particu-

Oxford Flowers

pi-GAN++
AR-NeRF

CUB-200-2011

pi-GAN++
AR-NeRF

FFHQ

pi-GAN++
AR-NeRF

σs

8.27
7.86

σs

9.90
6.81

σs

4.64
3.66

2σs

10.63
9.07

2σs

11.15
7.26

2σs

6.94
5.30

3σs

19.40
14.21

3σs

14.26
9.43

3σs

11.54
9.51

4σs

32.04
22.09

4σs

19.95
13.26

4σs

17.10
14.98

5σs

43.94
31.91

5σs

29.66
19.77

5σs

23.81
21.60

Table 3. Changes in the KID↓ (×103) when varying the aper-
ture size to s ∈ {σs, 2σs, 3σs, 4σs, 5σs}. σs is the standard de-
viation of the aperture size obtained by training AR-NeRF.

lar, we found that the difference is significant in the Ox-
ford Flowers and CUB-200-2011 datasets, where viewpoint
cues are limited or difficult to obtain and defocus cues play
a critical role in obtaining a better SIDE (as shown in Ta-
ble 2). These results indicate that (1) neural radiance fields
need to be optimized for defocus effects to obtain defocus-
tolerant representations that can be jointly used in various
ranges of defocus strength, and (2) AR-NeRF (i.e., a model
that learns the defocus effects) is useful for addressing this
problem, particularly when other cues (e.g., viewpoint cues)
are limited.

A.2. Effect of aperture ray sampling scheme

As discussed in Section 4.4, stratified sampling [65] was
used to represent the aperture using a limited number of
rays. We used five rays in particular: the origin of one ray
was placed at the center of the aperture, and the origins of
the others were placed along the circumference of the aper-
ture at equal intervals. In this section, we examine the effect
of this approximation.

Increasing the number of rays in the training phase is
costly; thus, we examined the effect of an aperture ray sam-
pling scheme in the inference phase. More specifically, we
compared stratified sampling, which was used in the main
experiments, with random sampling, where the offsets of
ray origins, that is, u in Equation 3, were randomly sam-
pled in a disk of radius s (i.e., |u| ∈ [0, s]). To examine
the effect of the number of rays, we investigated the dif-
ference in performance when changing the number of rays
in {1, 2, 5, 10, 20}. As mentioned above, we examined the
difference in performance in an inference phase. Hence, the
base-trained model was the same as that used in Section 5
and was common across all settings.

Results. We summarize the results in Table 4. We found
that, when we use random sampling, (1) the performance is
improved as the number of rays increases until reaching ap-

13

Oxford Flowers

1

2

5

AR-NeRF

18.80

12.17

8.51

CUB-200-2011

1

2

5

AR-NeRF

13.25

8.92

6.97

FFHQ

1

2

5

AR-NeRF

15.92

8.26

4.26

10

7.65

10

6.72

10

3.79

20

7.37

20

7.19

20

4.08

(5)

(7.86)

(5)

(6.81)

(5)

(3.67)

Table 4. Changes in the KID↓ (×103) when the number of rays
varies within {1, 2, 5, 10, 20}. When calculating the scores from
the second to sixth columns, we randomly sampled the offsets of
ray origins, that is, u (Equation 3), in a disk of radius s (i.e., |u| ∈
[0, s]). Bold font indicates the best scores. When calculating the
scores in the last column, we used stratified sampling, as detailed
in Section 4.4. To distinguish them, we used parentheses for the
latter.

proximately 10,11 (2) the model with five rays (in the fourth
column) performs worse than the model with the same num-
ber of rays with stratified sampling (in the last column), and
(3) we need to increase the number of rays to 10 (in the
fifth column) to obtain a performance comparable to that
of the model with stratified sampling (in the last column).
Considering that processing time and memory increase as
the number of rays increases, we believe that stratified sam-
pling with five rays is a reasonable choice in our experimen-
tal settings.

It is a possible that we will need to use more rays when
applying to higher-resolution images, and in that case, we
will need to increase the number of points along the ray.

A.3. Simultaneous control of viewpoint and defocus

AR-NeRF is a natural extension of NeRF, in which pin-
hole camera-based ray tracing is replaced with aperture
camera-based ray tracing. This extension does not con-
tradict the basic functionalities of NeRF. Thus, AR-NeRF
can learn viewpoint-aware representations by taking over
the characteristics of NeRF.
Results. We demonstrate this strength in Figure 5. These
results indicate that by using AR-NeRF, we can manipulate
viewpoints and defocus effects simultaneously and indepen-
dently in a unified framework.

A.4. Generation of higher-resolution images

In the experiments in the main text (Section 5), we used
64 × 64 images to examine various cases efficiently, fol-
lowing an AR-GAN study [37]. However, as discussed in
Appendix A.3, AR-NeRF is a natural extension of NeRF;

11The reason why the performance degrades when the number of rays
increases too much (particularly in the cases where the number of rays
is 20 on CUB-200-2011 and FFHQ) is that, in training, NeRF (including
AR-NeRF) is optimized using finite sample points. Consequently, overly
dense sampling in the inference phase can cause discrepancies from opti-
mized conditions. This may lead to degradation when the number of rays
increases too much. In other experiments, we found that the same phe-
nomenon occurs when the number of sample points along the ray increases
significantly.

therefore, it can be applied to higher-resolution images with
an increase in calculation cost, similar to other generative
variants of NeRF, such as [9,87]. To validate this statement,
we applied AR-NeRF to the 128 × 128 images.

Results. We provide the examples of generated images and
depths in Figure 6. Following the experimental settings in
the pi-GAN study [9], we trained the model using 128×128
images and rendered the final results by sampling 512×512
pixels. We found that AR-NeRF can render the defocus ef-
fects, including changes in defocus strength and focus dis-
tance, reasonably well, even in higher-resolution images.

A.5. Application to defocus renderer

After training, AR-NeRF can generate images from ran-
domly sampled latent codes while varying the defocus
strength and focus distance with photometric constraints.
By utilizing these images, we can train a defocus renderer,
which, given an image, manipulates the defocus strength
and focus distance intuitively and continuously. We call this
renderer AR-NeRF-R. In particular, we implemented AR-
NeRF-R using a conditional extension of U-Net [82, 119],
which incorporates the aperture size s and focus distance
f as auxiliary information to control the image generation
based on them. We provide the implementation details in
Appendix C.2. In this section, we empirically investigate
the effectiveness of AR-NeRF-R.

Dataset. We used the Oxford Flowers dataset to train
AR-NeRF and AR-NeRF-generated images to train AR-
NeRF-R. In particular, for AR-NeRF, we used the model
discussed in Section 5. When training AR-NeRF-R, we
used 128 × 128 images generated by AR-NeRF, where
we increased the resolution of the generated images from
64 × 64 to 128 × 128 by increasing the density of the input
points (Section 5.2), to allow AR-NeRF-R to be applied to
128 × 128 images. To confirm the generality of the learned
model, we evaluated it on a different dataset (iPhone2DSLR
Flower [118]), including photographs of flowers taken by
smartphones. We provide the details regarding the dataset
in Appendix C.2.

Comparison model. To the best of our knowledge, no pre-
vious method can learn the continuous representations of
defocus strength and focus distance from natural images
in the same setting as our own (i.e., without any supervi-
sion and any predefined model). Therefore, we used two
baselines that have partially the same objective. The first
baseline is CycleGAN [118], which trains a defocus ren-
derer using set level supervision.12 In contrast to AR-NeRF-
R, CycleGAN requires additional supervision to determine
whether each training image is an all-in-focus or focused
image. The second baseline is AR-GAN-DR [37], which
can train a defocus renderer without any supervision or pre-
trained model, similar to AR-NeRF-R; however, its conver-

12We used the pretrained model provided by the authors: https://

github.com/junyanz/pytorch-CycleGAN-and-pix2pix.

14

Oxford Flowers

CUB-200-2011

FFHQ

FID↓

KID↓

FID↓

KID↓

FID↓

KID↓

AR-GAN
AR-GAN++
RGBD-GAN

AR-NeRF

20.4
19.0
20.8

17.1

11.23
10.18
12.04

7.86

24.0
23.0
24.6

17.0

14.30
13.91
14.92

6.81

10.4
9.9
11.6

7.8

5.75
5.43
6.73

3.67

Table 5. Comparison of FID↓ and KID↓ (×103) between base-
line GANs and AR-NeRF (ours). This table supplements Table 1.

Oxford Flowers

CUB-200-2011

FFHQ

(B) (D) (V) FID↓

KID↓

FID↓

KID↓

FID↓

KID↓

L
pi-GAN
pi-GAN++ ✓
L
AR-NeRF-0 ✓ ✓ 0
AR-NeRF-F ✓ ✓ F
pi-GAN++-F ✓
F
AR-NeRF ✓ ✓ L

12.6
18.2
15.2
–
–

17.1

3.69
8.30
6.81
–
–

7.86

14.8
21.5
20.1
–
–

17.0

5.04
9.84
8.67
–
–

6.81

9.7
8.6
8.0
8.8
9.8

7.8

4.29
4.43
3.83
4.59
5.06

3.67

Table 6. Comparison of FID↓ and KID↓ (×103) between AR-
NeRF and ablated models. This table supplements Table 2.
Check marks (B) and (D) indicate the use of a background syn-
thesis network and defocus cue, respectively. In column (V), L,
F, and 0 indicate the use of local, full, and no viewpoint changes,
respectively.

Oxford Flowers

CUB-200-2011

FFHQ

KID↓ SIDE↓ ∇d↓ KID↓ SIDE↓ ∇d↓ KID↓ SIDE↓ ∇d↓

AR-GAN 11.23
AR-GAN++ 10.18

AR-NeRF

7.86

4.46
4.42

3.94

6.94
7.01

3.54

14.30
13.91

6.81

3.58
3.61

3.63

4.99
4.99

3.39

5.75
5.43

3.67

4.21
4.88

2.61

5.73
7.37

2.24

Table 7. Comparison of KID↓ (×103), SIDE↓ (×102), and ∇d↓
(×102) among AR-GAN, AR-GAN++, and AR-NeRF (ours).
This table supplements Table 1.

sion is one-to-one, and it cannot adjust the defocus strength
and focus distance continuously.
Results. We present examples of the rendered images in
Figure 7. We found that CycleGAN often performs un-
necessary changes (e.g., color changes in the fifth row),
whereas AR-NeRF-R and AR-GAN-DR do not. We in-
fer that the aperture-rendering mechanisms in AR-NeRF
and AR-GAN contributed to this phenomenon. The dif-
ference between AR-GAN-DR and AR-NeRF-R is that in
AR-GAN-DR, the defocus strength and focus distance are
uniquely determined according to the input image, whereas
in AR-NeRF-R, we can change them continuously by vary-
ing the auxiliary information of aperture size s and focus
distance f . This new AR-NeRF-R functionality allows for
interactive selection of defocused images.

A.6. Fr´echet inception distance

In the main text, we used KID because it has an unbiased
estimator and complements the flaws of other representative
metrics (i.e., Fr´echet inception distance (FID) [31] and in-
ception score (IS) [31]). However, the FID is a widely used
metric. For reference, we report the FID in Tables 5 and 6.
As also shown in a previous study [45], we found that KID
and FID had high correlations in this case. These results do
not contradict the statements in the main text.

A.7. Gradient of difference in depth

Figures 13–15 show that AR-GAN/AR-GAN++ yields
unexpected artifacts around the edge and surface; however,
SIDE can ignore this degradation because it measures the
difference based on l2, causing statistical averaging. This
may explain why the improvement in depth prediction by
AR-NeRF is not reflected in SIDE on the CUB-200-2011
dataset (Table 1), despite the qualitative difference (Fig-
ure 14). To validate this hypothesis, we calculated the gra-
dient of the difference between the ground truth and pre-
dicted depths (∇d), which is commonly used to examine
local structural similarity [16]. Table 7 lists the results and
shows that AR-NeRF can improve depth prediction even on
the CUB-200-2011 dataset in this metric.

15

Figure 5. Simultaneous control of viewpoint and defocus.

16

Defocus strengthWeakStrongYawDepthFocus distanceNearFarPitchDepthDefocus strengthWeakStrongDepthFocus distanceNearFarDepthDefocus strengthWeakStrongDepthFocus distanceNearFarDepthOxford FlowersCUB-200-2011FFHQFigure 6. Examples of 512 × 512 image and depth generation using AR-NeRF.

17

Defocus strengthFocus distanceDepthWeakStrongFarNearDefocus strengthDepthWeakStrongFocus distanceFarNearFigure 7. Comparison of defocus rendering among CycleGAN, AR-GAN-DR, and AR-NeRF-R (ours).

18

Defocus strengthFocus distanceWeakStrongNearFarInput(c) AR-NeRF-R (ours)+Defocus(a) CycleGANInput+DefocusInput(b) AR-GAN-DRB. Additional qualitative results

In this appendix, we provide additional qualitative re-
sults that correspond to those presented in the main text.
Figure captions and their relationship to the results in the
main text are as follows:

• Figure 8: Unsupervised learning of depth and defo-
cus effects from unstructured (and view-limited) natu-
ral images. This figure is an extended version of Fig-
ure 1.

• Figure 9: Simultaneous control of defocus and latent
codes on the Oxford Flowers dataset. This figure is an
extension of Figure 1.

• Figure 10: Simultaneous control of defocus and latent
codes on the CUB-200-2011 dataset. This figure ex-
tends Figure 1.

• Figure 11: Simultaneous control of defocus and latent
codes on the FFHQ dataset. This figure is an extended
version of Figure 1.

• Figure 12: Comparison of generated images and
depths between AR-GAN++ and AR-NeRF. This fig-
ure extends Figure 3.

• Figure 13: Comparison of depth prediction on the Ox-
ford Flowers dataset. The depths are used to calculate
the SIDEs in Tables 1 and 2.

• Figure 14: Comparison of depth prediction on the
CUB-200-2011 dataset. The depths are used to cal-
culate the SIDEs in Tables 1 and 2.

• Figure 15: Comparison of depth prediction on the
FFHQ dataset. The depths are used to calculate the
SIDEs in Tables 1 and 2.

19

Figure 8. Unsupervised learning of depth and defocus effects from unstructured (and view-limited) natural images. This figure is an
extended version of Figure 1. As shown here, our objective is to acquire a generator that can generate sets of images and depths using only
a collection of unstructured (and view-limited) single images and without any supervision (e.g., ground-truth depth, pairs of multiview
images, defocus supervision, and pretrained models). In particular, in the generation of an image, we aim to obtain a generator that can
intuitively and continuously adjust the defocus strength and focus distance with photometric constraints.

20

Defocus strengthFocus distanceDepthWeakStrongNearFarOxford FlowersCUB-200-2011FFHQFigure 9. Simultaneous control of defocus and latent codes on the Oxford Flowers dataset. This figure is an extended version of
Figure 1. In AR-NeRF, aperture randomized training (Section 4.3) encourages the defocus effects and latent codes to capture independent
representations. By employing this characteristic, we can manipulate defocus effects and latent codes independently and simultaneously.

21

Defocus strengthWeakStrongz1z2Latent codeDepthFocus distanceNearFarFigure 10. Simultaneous control of defocus and latent codes on the CUB-200-2011 dataset. This figure is an extended version of
Figure 1. In AR-NeRF, aperture randomized training (Section 4.3) encourages the defocus effects and latent codes to capture independent
representations. By employing this characteristic, we can manipulate defocus effects and latent codes independently and simultaneously.

22

Defocus strengthWeakStrongz1z2Latent codeDepthFocus distanceNearFarFigure 11. Simultaneous control of defocus and latent codes on the FFHQ dataset. This figure is an extended version of Figure 1. In
AR-NeRF, aperture randomized training (Section 4.3) encourages the defocus effects and latent codes to capture independent representa-
tions. By employing this characteristic, we can manipulate defocus effects and latent codes independently and simultaneously.

23

Defocus strengthWeakStrongz1z2Latent codeDepthFocus distanceNearFarFigure 12. Comparison of generated images and depths between AR-GAN++ and AR-NeRF (ours). This figure is an extended version
of Figure 3. We found that AR-NeRF can manipulate both the defocus strength and focus distance without producing significant artifacts.
In particular, it is worth noting that AR-NeRF can refocus on both the foreground (shown in the fifth column) and background (shown in the
second-to-last column), which are almost the same as those in the all-in-focus images (shown in the first column), by manipulating the focus
distance. By contrast, AR-GAN++ tends to yield unexpected artifacts (e.g., over-smoothing or discretization artifacts), particularly when
there is a strong defocus (shown in the fourth column) or refocus on the background (shown in the second-to-last column). As discussed
in the main text, the possible causes for these phenomena are: (1) AR-GAN++ discretely represents light fields in a 2D space; thus, the
discretization error becomes critical when a large manipulation is performed, and (2) the predicted depths (shown in the last column)
contain artifacts (e.g., holes emerge in the objects), resulting in errors when images are rendered based on the depths. The properties of
AR-NeRF, that is, (1) continuous representation in a 3D space and (2) joint optimization using defocus and viewpoint cues, are useful for
addressing these weaknesses.

24

Defocus strengthFocus distanceDepthWeakStrongNearFarOxford FlowersAR-GAN++CUB-200-2011FFHQAR-GAN++AR-GAN++AR-NeRF(ours)AR-NeRF(ours)AR-NeRF(ours)Figure 13. Comparison of depth prediction on the Oxford Flowers dataset. These depths are used to calculate the SIDEs in Tables 1
and 2. AR-GAN (b), AR-GAN++ (c), and RGBD-GAN (d) are CNN-based and are trained in a fully unsupervised manner. In addition, pi-
GAN (e), pi-GAN++ (f), AR-NeRF-0 (g), and AR-NeRF (h) are NeRF-based and trained in a fully unsupervised manner. By contrast, the
model in (i) [109] was trained using stereo supervision and applied as the ground truth in the evaluation. The SIDEs in Tables 1 and 2 were
calculated by comparing the depths in (b)–(h) with the depths in (i). Our findings are summarized as follows: (1) Because AR-GAN (b) and
AR-GAN++ (c) only employ defocus (appearance) cues, their predicted depths are affected by their appearance. For example, in the second
row, the pattern in the petals affects depth prediction, despite its non-necessity. (2) RGBD-GAN (d) utilizes viewpoint (geometric) cues for
3D representation learning. However, there are few viewpoint cues in this dataset; consequently, this model has difficulty in learning depth.
(3) For the same reason, pi-GAN (e) and pi-GAN++ (f), which only employ viewpoint cues, suffer from learning difficulties, although
NeRF itself has a strong 3D consistency at the design level. In particular, they fail to consistently distinguish between the foreground
(flower) and background (surroundings), parts of which are often missing. (4) Although the results of AR-NeRF-0 (g) are closest to those
of AR-NeRF (h), AR-NeRF-0 (g) is often affected by the appearance (e.g., in the second row, similar to AR-GAN (b) and AR-GAN++ (c))
because it also leverages only focus cues. (5) AR-NeRF (h) overcomes the limitations of pi-GAN (e), pi-GAN++ (f), and AR-NeRF-0 (g)
by utilizing both the viewpoint and defocus cues.

25

(a)(b)(c)(d)(e)(f)(g)(h)(i)ImageStereoAR-NeRFpi-GAN++pi-GANAR-NeRF-0AR-GANAR-GAN++RGBD-GAN(ours)(ours)Figure 14. Comparison of depth prediction on the CUB-200-2011 dataset. These depths are used to calculate the SIDEs in Tables 1
and 2. AR-GAN (b), AR-GAN++ (c), and RGBD-GAN (d) are CNN-based and are trained in a fully unsupervised manner. In addition,
pi-GAN (e), pi-GAN++ (f), AR-NeRF-0 (g), and AR-NeRF (h) are NeRF-based and trained in a fully unsupervised manner. By contrast,
the model in (i) [109] was trained using stereo supervision and was applied as the ground truth in the evaluation. The SIDEs in Tables 1 and
2 were calculated by comparing the depths in (b)–(h) with the depths in (i). Our findings are summarized as follows: (1) Because AR-GAN
(b) and AR-GAN++ (c) only employ defocus (appearance) cues, their predicted depths are affected by their appearance. For example, in
the fifth row, the horizontal boundary in the background is emphasized despite its non-necessity. (2) RGBD-GAN (d) utilizes viewpoint
(geometric) cues for 3D representation learning. However, there are few viewpoint cues in this dataset; consequently, this model has
difficulty in learning depth. (3) For the same reason, pi-GAN (e) and pi-GAN++ (f), which also only employ viewpoint cues, suffer from
learning difficulty, although NeRF itself has a strong 3D consistency at the design level. In particular, they fail to consistently distinguish
between the foreground (bird) and background (surroundings), parts of which are often mixed (e.g., in the third and fourth rows). (4)
Although the results of AR-NeRF-0 (g) are closest to those of AR-NeRF (h), AR-NeRF-0 (g) is often affected by the appearance (e.g.,
in the fifth row, similar to AR-GAN (b) and AR-GAN++ (c)) because it also only leverages focus cues. (5) AR-NeRF (h) overcomes the
limitations of pi-GAN (e), pi-GAN++ (f), and AR-NeRF-0 (g) by utilizing both the viewpoint and defocus cues.

26

(a)(b)(c)(d)(e)(f)(g)(h)(i)ImageStereoAR-NeRFpi-GAN++pi-GANAR-NeRF-0AR-GANAR-GAN++RGBD-GAN(ours)(ours)Figure 15. Comparison of depth prediction on the FFHQ dataset. These depths are used to calculate the SIDEs in Tables 1 and 2. AR-
GAN (b), AR-GAN++ (c), and RGBD-GAN (d) are CNN-based and are trained in a fully unsupervised manner. In addition, pi-GAN (e),
pi-GAN++ (f), pi-GAN++-F (g), AR-NeRF-0 (h), AR-NeRF (i), and AR-NeRF-F (j) are NeRF-based and trained in a fully unsupervised
manner. By contrast, the model in (k) [109] was trained using stereo supervision and was applied as the ground truth in the evaluation. The
SIDEs in Tables 1 and 2 were calculated by comparing the depths in (b)–(j) with those in (k). Our findings are summarized as follows: (1)
In the depths predicted using AR-GAN (b) and AR-GAN++ (c), holes appeared in the face regions. This is because they can only adopt a
defocus cue, which is insufficient to distinguish a flat surface in a face from the blur caused by the defocus. (2) RGBD-GAN succeeded in
capturing the face direction in depth by leveraging the viewpoint cue (e.g., in the sixth and seventh rows); however, the depth fidelity was
low. This can occur because RGBD-GAN imposes 3D consistency only at a loss level and not at an architectural level. (3) By contrast, the
NeRF-based models (e)–(j) have 3D consistency at the architectural level, allowing high fidelity and consistent 3D depths to be predicted.
In particular, we found that the models utilizing viewpoint cues ((e)–(g), (i), and (j)) demonstrated similar performance. This is because
this dataset includes sufficiently varying viewpoints. (4) However, AR-NeRF-0 (h), which can only use the defocus cue, fails to capture
structures around the eyes. This is possibly because there is a large variety of appearances around the eyes, and it is difficult to model
the corresponding depth using only a defocus cue. We can overcome this limitation by jointly using viewpoint and defocus cues, as in
AR-NeRF (i) or AR-NeRF-F (j).

27

ImageStereopi-GAN++pi-GANAR-NeRF-0AR-GANAR-GAN++RGBD-GAN(ours)(a)(b)(c)(d)(e)(f)(h)AR-NeRF(ours)(i)(k)AR-NeRF-F(ours)(j)(g)pi-GAN++-FC. Implementation details

In this appendix, we provide implementation details re-

garding the following items:

• Appendix C.1: Details of the main experiments (Sec-

tion 5).

• Appendix C.2: Details of defocus renderer (Ap-

pendix A.5).

C.1. Details of main experiments (Section 5)

C.1.1 Dataset

In the experiments, we used three datasets, the detailed in-
formation of which is as follows:

Oxford Flowers [73]. The dataset consists of 8,189 images
with 102 flower categories. Each category includes 40 or
more images. The images were obtained by searching the
web and taking photographs. We downloaded the data from
an official website.13 More detailed information is provided
in the README file available on the website.

CUB-200-2011 [101]. The dataset contains 11,788 images
of 200 bird species. The images were collected using a
Flickr image search and then filtered by presenting each im-
age to multiple users of Mechanical Turk [106]. We down-
loaded the data from an official website.14 More detailed
information is provided in the technical report [101].

FFHQ (Flickr-Faces-HQ) [42]. The dataset consists of
70,000 face images. The images were crawled from Flickr.
Therefore, as the dataset creators [42] mention, the dataset
inherits all the biases of that website. The images were fil-
tered using automatic filters and Amazon Mechanical Turk.
Only images under permissive licenses (Creative Commons
BY 2.0, Creative Commons BY-NC 2.0, Public Domain
Mark 1.0, Public Domain CC0 1.0, or U.S. Government
Works license) were collected. The dataset itself is avail-
able under a Creative Commons BY-NC-SA 4.0 license by
the NVIDIA Corporation. We downloaded the data from an
official website.15 More detailed information is provided in
the README file available on the website.

C.1.2 Network architectures

As explained in Section 5.1, we implemented AR-NeRF
based on the pi-GAN [9],16 which is a state-of-the-art gener-
ative variant of NeRF. Because the original pi-GAN was not
applied to the datasets used in our experiments, we carefully
tuned the configurations and hyperparameters so that the
baseline pi-GAN could generate images sufficiently well.

13https://www.robots.ox.ac.uk/˜vgg/data/flowers/

102/.

14http://www.vision.caltech.edu/visipedia/CUB-

200-2011.html.

15https://github.com/NVlabs/ffhq-dataset.
16https://github.com/marcoamonteiro/pi-GAN.

In particular, we used the configuration of CelebA [57]17 as
the default and tuned depending on the dataset. We explain
the details of each network below.
Mapping network. In pi-GAN, a StyleGAN [42]-inspired
mapping network was introduced to efficiently propagate
information in the latent code to each layer. We imple-
mented this network using an MLP with three hidden layers
(256 units each). We used leaky rectified linear units (LRe-
LUs) [59] with a negative slope of 0.2 as activation func-
tions. The dimension of the latent code was set to 256. This
architecture is the same as that of the original pi-GAN [9].
Synthesis network. In pi-GAN, SIREN [91]-based implicit
radiance fields are used as a synthesis network. We imple-
mented this network using an MLP with eight FiLM [15,
79]-SIREN hidden layers of 128 units each. In the origi-
nal pi-GAN [9], 256 units were used in each layer; how-
ever, in our preliminary experiments, we found that the
reduction in the units did not significantly affect the per-
formance.
In addition, this reduction allowed the use of
a larger number of points along the ray, which is critical
for improving performance. Considering this, we used 128
units in our experiments. In pi-GAN++ and AR-NeRF, we
used the above-mentioned network as a foreground synthe-
sis network and implemented a background synthesis net-
work using an MLP with eight FiLM-SIREN hidden layers
of 64 units each. We used fewer parameters in the back-
ground synthesis network under the assumption that the
background is simpler than the foreground.
Discriminator. We used different discriminators accord-
ing to the dataset. For FFHQ, we used the same discrim-
inator as that used in pi-GAN for CelebA.17 The discrim-
inator was implemented using CoordConv layers [54] and
residual blocks [29]. In our preliminary experiments, we
found that the CoordConv layers yield negative effects for
Oxford Flowers and CUB-200-2011. A possible cause is
that in FFHQ, faces are aligned based on facial landmarks,
whereas in Oxford Flowers and CUB-200-2011, flowers
and birds are not strictly aligned. Based on this finding,
we removed the CoordConv layers from the discriminator
when applied to Oxford Flowers and CUB-200-2011.

C.1.3 Training settings

We used different training settings according to the dataset.
For FFHQ, we used the same setting as that in pi-GAN for
CelebA.17 More specifically, as the GAN objective func-
tion, we used the non-saturating GAN loss [25] with real
gradient penalty (R1) regularization [62], where the weight
parameter of the R1 regularization was set to 0.2. Addition-
ally, we used an identity regularizer [67] with the weight
parameter of 15 to keep the identity across different view-
points. The network was trained for 200,000 iterations us-
ing the Adam optimizer [44], with learning rates of 0.00006

17https://github.com/marcoamonteiro/pi-GAN/blob/

master/curriculums.py.

28

as follows:
iPhone2DSLR Flower [118]. The dataset includes 2,381
smartphone images and 3,805 DSLR images. The smart-
phone images were collected from Flickr by searching for
photos taken by Apple iPhone 5, 5s, or 6, with the search
text “flower.” DSLR images with a shallow depth-of-field
(DoF) were also collected from Flickr using the search tags
“flower” and “dof.” We downloaded the data from an of-
ficial website.20 More detailed information is provided on
the website and appendix of the corresponding paper [118].

C.2.2 Network architectures

We implemented AR-NeRF-R using basically the same net-
work as AR-GAN-DR [37], that is, we used the U-Net ar-
chitecture [82]. A difference from AR-GAN-DR is that we
extended U-Net to a conditional setting [119]. Specifically,
we injected the aperture size s and focus distance f into ev-
ery intermediate layer in the encoder after expanding them
to the corresponding feature map size.

C.2.3 Training settings

We generated training data (i.e., pairs of all-in-focus and
focused images with auxiliary information on aperture size
s and focus distance f ) using AR-NeRF, which was trained
using 64 × 64 images on the Oxford Flowers dataset. AR-
NeRF was the same as that used to generate the samples in
Figures 1 and 3. When training AR-NeRF-R, we used 128×
128 images generated by AR-NeRF, where we increased the
resolution of the generated images from 64 × 64 to 128 ×
128 by increasing the density of input points (Section 5.2).
We trained the defocus renderer for 300,000 iterations using
the Adam optimizer [44] with a learning rate of 0.0003 and
momentum terms β1 and β2 of 0.9 and 0.99, respectively.
The batch size was set to 4. The learning rate was kept
constant during training, except for the last 30% iterations,
where the learning rate was smoothly ramped down to zero.

and 0.0002 for the generator and discriminator, respectively,
and momentum terms β1 and β2 of 0 and 0.9, respectively.
The batch size was set to 16. We used an exponential mov-
ing average [40] with a decay of 0.999 over the weights to
generate the final generator. In pi-GAN, we set the number
of sample points along the ray to 48, where 32 and 16 points
were used for stratified sampling and hierarchical sampling,
respectively. In pi-GAN++ and AR-NeRF, we set the val-
ues for the foreground and background synthesis networks
as 48 and 24, respectively. In the foreground synthesis, 32
and 16 points were used for stratified sampling and hierar-
chical sampling, respectively. In the background synthesis,
16 and 8 points were used for stratified sampling and hi-
erarchical sampling, respectively. A field of view was set
to 12◦. As discussed in Section 5.3, we used the same
number of rays in all models to investigate the pure per-
formance differences between the models with and without
aperture rendering. Specifically, we used five stratified sam-
pled rays (Section 4.4) in AR-NeRF and five ensemble rays
(i.e., five rays with an aperture size s = 0) in pi-GAN and
pi-GAN++.

For Oxford Flowers and CUB-200-2011, we also used
differentiable augmentation [116]18 to stabilize the train-
ing. In particular, we used color jittering, translation, and
cutout [14] for Oxford Flowers, and translation for CUB-
200-2011 because we found that they were the best choice.
We removed the identity regularizer [67] because we found
that it yields negative effects for Oxford Flowers and CUB-
200-2011. The other settings were the same as those for
FFHQ.

C.1.4 Evaluation

As described in Section 5.1, we calculated KID using
20,000 generated images and all real images. We imple-
mented a KID calculator based on the official code.19 The
depth predictor, which was used when calculating the SIDE,
was implemented using U-Net architecture [82]. In particu-
lar, we used the same network and training settings as those
used in the AR-GAN study [37] for direct comparison. pi-
GAN++ and AR-NeRF can synthesize the unbounded back-
ground (or depth) by using a NeRF++ [115]-based back-
ground synthesis network; however, the predictable depth
range is bounded in a typical depth predictor, including the
model [109] that was used as “ground truth” in our exper-
iment. Therefore, we used only the foreground synthesis
network when calculating the SIDE.

C.2. Details of defocus renderer (Appendix A.5)

C.2.1 Dataset

In the experiment, we used a test set of the iPhone2DSLR
Flower [118] for the evaluation. Its detailed information is

18https : / / github . com / mit - han - lab / data -

efficient-gans.

19https://github.com/mbinkowski/MMD-GAN.

20https://github.com/junyanz/CycleGAN.

29

