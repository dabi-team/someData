Low-Rank Autoregressive Tensor Completion for
Spatiotemporal Trafﬁc Data Imputation

Xinyu Chen, Mengying Lei, Nicolas Saunier, Lijun Sun*

1

1
2
0
2

r
p
A
0
3

]

G
L
.
s
c
[

1
v
6
3
9
4
1
.
4
0
1
2
:
v
i
X
r
a

Abstract—Spatiotemporal trafﬁc time series (e.g., trafﬁc vol-
ume/speed) collected from sensing systems are often incomplete
with considerable corruption and large amounts of missing
values, preventing users from harnessing the full power of
the data. Missing data imputation has been a long-standing
research topic and critical application for real-world intelligent
transportation systems. A widely applied imputation method
is low-rank matrix/tensor completion; however, the low-rank
assumption only preserves the global structure while ignores
the strong local consistency in spatiotemporal data. In this
paper, we propose a low-rank autoregressive tensor completion
(LATC) framework by introducing temporal variation as a new
regularization term into the completion of a third-order (sensor ×
time of day × day) tensor. The third-order tensor structure allows
us to better capture the global consistency of trafﬁc data, such
as the inherent seasonality and day-to-day similarity. To achieve
local consistency, we design the temporal variation by imposing
an AR(p) model for each time series with coefﬁcients as learnable
parameters. Different from previous spatial and temporal regu-
larization schemes, the minimization of temporal variation can
better characterize temporal generative mechanisms beyond local
smoothness, allowing us to deal with more challenging scenarios
such “blackout” missing. To solve the optimization problem in
LATC, we introduce an alternating minimization scheme that
estimates the low-rank tensor and autoregressive coefﬁcients it-
eratively. We conduct extensive numerical experiments on several
real-world trafﬁc data sets, and our results demonstrate the
effectiveness of LATC in diverse missing scenarios.

Index Terms—Spatiotemporal trafﬁc data, missing data im-
putation, low-rank tensor completion, truncated nuclear norm,
autoregressive time series model

I. INTRODUCTION

Spatiotemporal trafﬁc data collected from various sensing
systems (e.g. loop detectors and ﬂoating cars) serve as the
foundation to a wide range of applications and decision-
making processes in intelligent transportation systems. The
emerging “big” data is often large-scale, high-dimensional,
and incomplete, posing new challenges to modeling spa-
tiotemporal trafﬁc data. Missing data imputation is one of
the most important research questions in spatiotemporal data
analysis, since accurate and reliable imputation can help various

Xinyu Chen and Nicolas Saunier are with the Civil, Geological and
Mining Engineering Department, Polytechnique Montreal, Montreal, QC
H3T 1J4, Canada. E-mail: chenxy346@gmail.com (Xinyu Chen), nico-
las.saunier@polymtl.ca (Nicolas Saunier).

Mengying Lei and Lijun Sun are with the Department of Civil En-
gineering, McGill University, Montreal, QC H3A 0C3, Canada. E-mail:
mengying.lei@mail.mcgill.ca (Mengying Lei), lijun.sun@mcgill.ca (Lijun
Sun).

* Corresponding author. Address: 492-817 Sherbrooke Street West,

Macdonald Engineering Building, Montreal, Quebec H3A 0C3, Canada

Manuscript received xx; revised xx.

downstream applications such as trafﬁc forecasting and trafﬁc
control/management.

×

×

×

The key to missing data imputation is to efﬁciently charac-
terize and leverage the complex dependencies and correlations
across both spatial and temporal dimensions [1]. Different from
point-referenced systems, trafﬁc state data (e.g., speed and ﬂow)
is individual sensor-based with a ﬁxed temporal resolution. This
allows us to summarize spatiotemporal trafﬁc state data in the
time) or a tensor (e.g.,
format of a matrix (e.g., sensor
sensor
day) [2], and low-rank matrix/tensor
time of day
completion becomes a natural solution to solve the imputation
problem. Over the past decade, extensive effort has been made
on developing low-rank models through principle component
analysis, matrix/tensor factorization (with predeﬁned rank) and
nuclear norm minimization (see e.g., [2]–[4]). However, the
default low-rank structure (e.g., nuclear norm) purely relies
on the algebraic property of the data, which is invariant to
permutation in the spatial and temporal dimensions. In other
words, with the low-rank assumption alone, we essentially
overlook the strong “local” spatial and temporal consistency
in the data. For instance, we expect trafﬁc ﬂow data collected
in a short period to be similar and adjacent sensors to show
similar patterns. To this end, some recent studies have tried to
encode such “local” consistency by introducing total/quadratic
variation and graph regularization as a “smoothness” prior into
low-rank factorization models [1], [5]–[7] and imposing time
series dynamics on the temporal latent factor in the factorization
framework [8]–[10]. However, these studies essentially adopt
a bilinear/multilinear factorization model, which requires a
predeﬁned rank as a hyperparameter.

In this paper, we propose a low-rank autoregressive tensor
completion (LATC) framework to impute missing values in
spatiotemporal trafﬁc data. For each completed time series,
we deﬁne temporal variation as the accumulated sum of
autoregressive errors. To model the low-rankness property,
we use truncated nuclear norm as an effective approximation
to avoid the rank determination problem in factorization
models. The ﬁnal objective function of LATC consists of two
components, i.e., the truncated nuclear of the completed tensor
and the temporal variation deﬁned on the unfolded time series
matrix. The combination allows us to effectively characterize
both global patterns and local consistency in spatiotemporal
trafﬁc data. The overall contribution of this work is threefold:
1) We integrate the autoregressive time series process into
a low-rank tensor completion model to capture both
global and local trends in spatiotemporal trafﬁc data.
By minimizing the truncated nuclear norm of the third-
day) tensor, we can better
order (sensor

time of day

×

×

 
 
 
 
 
 
characterize day-to-day similarity, which is a unique
property of trafﬁc time series data [11].

2) We develop an alternating learning algorithm to update
tensor and coefﬁcient matrix separately. The tensor is
updated via ADMM, and the coefﬁcient matrix is updated
by least squares with closed-form solution.

3) We conduct extensive numerical experiments on four
trafﬁc data sets. Imputation results show the superiority
and advantage of LATC over recent state-of-the-art
models.

The remainder of this paper is organized as follows. We in-
troduce related work and notations in Section II and Section III,
respectively. Section IV introduces in detail the proposed LATC
model. In Section V, we conduct extensive experiments on
some trafﬁc data sets and make comparison with some baseline
models. Finally, we summarize the study in Section VI.

II. RELATED WORK

There are two types of low-rank models to solve the

spatiotemporal missing data imputation problem.
Temporal matrix factorization. Factorization models ap-
proximate the complete spatiotemporal matrix/tensor using
bilinear/multilinear factorization models with a predeﬁned rank
parameter. To encode temporal consistency, recent studies
have introduced local smoothness and time series dynamics to
regularize the temporal factor (see e.g., [5], [8]–[10]). The
introduction of generative mechanism (e.g., autoregressive
model) not only offers better interpolation/imputation accuracy,
but also enable the factorization models to perform forecasting.
However, a major limitation of these models is that they often
require careful tuning and selection of the rank parameter.
Tensor representation. Another approach is to fold a time
series matrix into a third-order tensor (sensor
×
day) by introducing an additional “day” dimension (e.g., [4],
[12], [13]). This is a particular case for trafﬁc data given the
clear day-to-day similarity, but many real-world time series data
resulted from human behavior/activities (e.g., energy/electricity
consumption) also exhibit similar patterns. It is expected that
the third-order representation captures more information, given
that the multivariate time series matrix is in fact one of the
unfoldings of the third-order tensor. As a result, the tensor
structure not only preserves the dependencies among sensors
but also provides an alternative to capture both local and
global temporal patterns (e.g., trafﬁc speed data at 9:00 am
on Monday might be similar to that of 9:00 am on Tuesday).
These tensor-based models have shown superior performance
over matrix-based models in missing data imputation tasks.

time of day

×

III. NOTATIONS

×

Throughout this work, we use boldface uppercase letters to
RM
N , boldface lowercase letters
RM , and lowercase letters to denote
N , we denote the
×
t) to
R(N
−
t entries
RN . The Frobenius norm of X is deﬁned as
m,n, and the (cid:96)2-norm of x is deﬁned

denote matrices, e.g., X
∈
to denote vectors, e.g., x
∈
scalars, e.g., x. Given a matrix X
(m, n)th entry in X by xm,n, and use xm,[t+1:] ∈
denote the sub-vector that consists of the last N
−
of xm ∈
(cid:107)F =
X
(cid:107)

m,n x2

RM

∈

(cid:113)(cid:80)

2

I

×

as

m x2

×
(cid:112)(cid:80)

(cid:107)
X ∈
of
X
foldk(
·
mode. Thus, we have foldk(

m. We denote a third-order tensor by
(cid:107)2 =
x
RM
J and the kth-mode (k = 1, 2, 3) unfolding
X (k) [14]. Correspondingly, the folding operator
by
) converts a matrix to a third-order tensor in the kth-
for any tensor
X (k)) =
X
RM
I
J , its Frobenius norm is deﬁned as
×
×
m,i,j x2
m,i,j and its inner product with another
and
m,i,j xm,i,jym,i,j where
,

. For
X
(cid:107)X (cid:107)F =
tensor is given by

X ∈

=

(cid:113)(cid:80)
are of the same size.

(cid:104)X

Y(cid:105)

Y

X

(cid:80)

IV. METHODOLOGY

A. Tensorization for Global Consistency

We denote the true spatiotemporal trafﬁc data collected from
M sensors over J days by Y , whose columns correspond to
time points and rows correspond to sensors:

Y =





|
|
y1 y2

|

|

· · ·

|
yIJ

|



M

(IJ),

×

R

(1)



∈

where I is the number of time points per day. The ob-
PΩ(Y ) with
served/incomplete matrix can be written as
observed entries on the support Ω:

PΩ(Y )]m,n =
[

(cid:26)

ym,n,
0,

if (m, n)
∈
otherwise,

Ω,

where m = 1, . . . , M and n = 1, . . . , IJ.

I

)

−

×

×

−

×

∈

=

Q

X

Q

Q

X

(Y )

RM

1(
·

(IJ) where

We next introduce the forward tensorization operator

)
(
·
that converts the multivariate time series matrix into a third-
order tensor. Temporal dimension of trafﬁc time series is
divided into two dimensions, i.e., time of day and day. For-
mally, a third-order tensor can be generated by the forward
J . Conversely,
tensorization operator as
Q
the resulted tensor can also be converted into the original
RM
1(
matrix by Y =
) denotes
∈
).
the inverse operator of
(
·
Q

[15] or truncated nuclear norms

The tensorization step transforms matrix-based imputation
problem to a low-rank tensor completion problem. Global
consistency can be achieved by minimizing tensor rank. In
practice, tensor rank is often approximated using sum of nuclear
norms
[4], where
r is a truncation parameter (see section IV-C). Our motivation
for doing so is that the spatiotemporal trafﬁc data can be
characterized by both long-term global trends and short-term
local trends. The long-term trends refer to certain periodic,
seasonal, and cyclical patterns. Trafﬁc ﬂow data over 24 hours
on a typical weekday often shows a systematic “M” shape
resulted from travelers’ behavioral rhythms, with two peaks
during morning and evening rush hours [16]. The pattern also
exists at the weekly level with substantial differences from
weekdays to weekends. The short-term trends capture certain
temporary volatility/perturbation that deviates from the global
patterns (e.g., due to incident or special event). The short-term
trends seem to be more “random”, but they are common and
ubiquitous in reality. LATC leverages both global and local
patterns by using matrix and tensor simultaneously.

(cid:107)X (cid:107)r,

(cid:107)X (cid:107)∗

∗

B. Temporal Variation for Local Consistency

We deﬁne temporal variation of a time series matrix Z
d and a time lag set

given a coefﬁcient matrix A

RM

×

∈

=

H

h1, . . . , hd}
{
Z
(cid:107)

A,

(cid:107)

H

as

=

m,t
(cid:88)
Z
(cid:107)

(zm,t −

i
(cid:88)

am,izm,t

−

hi)2.

(2)

H

A,
(cid:107)

As can be seen,

quantiﬁes the total squared error
when ﬁtting each individual time series zm with an autore-
gressive model with coefﬁcient am. Given an estimated A,
minimizing the temporal variation will encourage the time
series data Z to show stronger temporal consistency. In other
words, the multivariate time series matrix Z will be better
explained by a series of autoregressive models parameterized
by A. It should be noted that both Z and A are variables in
the proposed temporal variation term.

C. Low-rank Autoregressive Tensor Completion (LATC)

The ensure both global consistency and local consistency,

we propose LATC as the following optimization model

A,

Z

λ
2 (cid:107)
(cid:107)
(Z) ,

min
X ,Z,A (cid:107)X (cid:107)r,

+

H

s.t.

∗
=
X
Q
PΩ(Z) =
is the partially observed time se-
×
N+ is the truncation which satisﬁes r <

PΩ(Y ),

(cid:26)
(IJ)

(3)

where Y
∈
ries matrix. r
.
min
}
{

M, I, J

RM

∈

The formulation of LATC ensures both global consistency
and local consistency by combining truncated nuclear norm
minimization with temporal variation minimization. The weight
parameter λ in the objective function controls the trade-
off between truncated nuclear norm and temporal variation.
Fig. 1 shows that Y can be reconstructed with both low-rank
properties and time series dynamics because the constraint in
(3), i.e.,
(Z), is closely related to the partially observed
X
matrix Y .

Q

=

Most nuclear norm-based tensor completion models employ
the Alternating Direction Method of Multipliers (ADMM)
algorithm to solve the optimization problem. However, due
to the introduction of autoregression coefﬁcient matrix, we
can no longer apply the default ADMM algorithm to solve
the optimization problem (3). Here we consider applying an
alternating minimization scheme by separating the original
optimization into two subproblems. Starting with some given
initial values (
N
by solving the two subproblems in an iterative manner. In the
implementation, we ﬁrst ﬁx A(cid:96) and solve the following problem
to update the variables

0, Z0, A0), we can update

(cid:96)+1 and Z(cid:96)+1:

(cid:96), Z(cid:96), A(cid:96))

(
{

}(cid:96)

X

X

∈

X

(cid:96)+1, Z(cid:96)+1 := arg min

X

X ,Z (cid:107)X (cid:107)r,
=
(Z),
X
Q
PΩ(Z) =

PΩ(Y ).

s.t.

(cid:26)

+

∗

λ
2 (cid:107)

Z

(cid:107)A(cid:96),

H

(4)

where (cid:96) denotes the count of iteration in the alternating
minimization scheme. Then, we ﬁx Z(cid:96)+1 and solve the

following least square problem to estimate the coefﬁcient matrix
A(cid:96)+1:

3

A(cid:96)+1 := arg min

Z(cid:96)+1

A (cid:107)

.

A,

H

(cid:107)

(5)

When A(cid:96) is ﬁxed, the subproblem in Eq. (4) becomes a
general low-rank tensor problem, can it can be solved using
ADMM in a similar way as in [15] and [17]. The augmented
Lagrangian function of the optimization in Eq. (4) can be
written as

, Z, A(cid:96),

(
X

L

T

) =

+

λ
2 (cid:107)

Z

∗

(cid:107)X (cid:107)r,
ρ
+
2 (cid:107)X − Q

(cid:107)A(cid:96),
2
(Z)
F
(cid:107)

H

(6)

+

X − Q
(cid:10)

(Z),

,

T

RM
J is
where ρ is the learning rate of ADMM, and
the dual variable. In particular, we keep
PΩ(Y ) as a
ﬁxed constraint to maintain observation consistency. According
to the augmented Lagrangian function, ADMM can transform
the problem in Eq. (4) into the following subproblems in an
iterative manner:

(cid:11)
T ∈
PΩ(Z) =

×

×

I

(cid:96)+1,k+1 : = arg min

X
Z(cid:96)+1,k+1 : = arg min

(
X
X L
(
X
Z L
(cid:96)+1,k + ρ(

, Z(cid:96)+1,k, A(cid:96),

T
(cid:96)+1,k+1, Z, A(cid:96),

(cid:96)+1,k),

(cid:96)+1,k),

T

(7)

(8)

(cid:96)+1,k+1

(Z(cid:96)+1,k+1)),

(9)

(cid:96)+1,k+1 : =

T

T

X
where k denotes the count of iteration in the ADMM. In the
following, we discuss the detailed solutions to Eqs. (7) and
(8).

− Q

X

1) Update Variable

: The optimization over

is a trun-
cated nuclear norm minimization problem. Truncated nuclear
norm of any given tensor is the weighted sum of truncated
nuclear norm on the unfolding matrices of the tensor, which
takes the form:

X

3

=

(cid:107)X (cid:107)r,

∗

αp(cid:107)X (p)(cid:107)r,

∗

(10)

I

×

×

RM

X ∈

p=1
(cid:88)
3
J with
for tensor
p=1 αp = 1. For the
minimization of truncated nuclear norm on tensor, the above
formula is not in its appropriate form because unfolding a
tensor in different modes cannot guarantee the dependencies of
variables [15]. Therefore, we introduce
X 3 and they
X 1,
X 2,
. Accordingly, it is possible
correspond to the unfoldings of
X p:
to obtain the closed-form solution for each
(cid:13)Q−1(X ) − Z (cid:96)+1,k(cid:13)

αp(cid:107)X (p)(cid:107)r,∗ +

X p := arg min

(cid:80)

X

(cid:13)
(cid:13)

(cid:13)
(cid:13)

2

X

F

ρ
2

+ (cid:10)Q−1(X ) − Z (cid:96)+1,k, Q−1(T (cid:96)+1,k)(cid:11)

= arg min

αp(cid:107)X (p)(cid:107)r,∗

X
(cid:13)
(cid:13)
(cid:13)X −
(cid:16)

+

ρ
2
= foldp

(cid:16)

Q(Z (cid:96)+1,k) − T (cid:96)+1,k/ρ

(cid:17)(cid:13)
(cid:13)
(cid:13)
F
(cid:16)
Q(Z (cid:96)+1,k)(p) − T (cid:96)+1,k

2

(p)

/ρ

Dr,αp/ρ

(11)

(cid:17)(cid:17)

,

(

D·

where
) denotes the generalized singular value thresholding
·
that associated with truncated nuclear norm minimization as
shown in Lemma 1.

4

=

1, 2
{

. Each
}

H

Fig. 1: Illustration of the proposed LATC framework for spatiotemporal trafﬁc data imputation with time lags
time series ym,

is modeled by the autoregressive coefﬁcients

1, 2, . . . , M

m

.
am1, am2}

{

∀

∈ {

}

N+ where
Lemma 1. For any α, ρ > 0, Z
×
, an optimal solution to the truncated nuclear
r < min
m, n
}
{
norm minimization problem

n, and r

Rm

∈

∈

min
X

α

X
(cid:107)

(cid:107)r,

∗

+

ρ
2 (cid:107)

X

Z

2
F ,
(cid:107)

−

(12)

is given by the generalized singular value thresholding [18]–
[20]:

−

ˆX =

α/ρ]+) V (cid:62),

Dr,α/ρ(Z) = U diag ([σ

1r ·
]+ denotes the positive
where U diag(σ)V (cid:62) is the SVD of Z. [
·
truncation at 0 which satisﬁes [σ
.
σ
α/ρ]+ = max
α/ρ, 0
}
{
1r ∈ {
} is a binary indicator vector whose ﬁrst r
entries are 0 and other entries are 1.
X 2,

X 3 in Eq. (11), we can

Gathering the results of

min
0, 1
}

X 1,

(13)

m,n

−

−

{

update the variable

by

X
(cid:96)+1,k+1 :=

3

X

p=1
(cid:88)

αpX p.

(14)

X

Z (cid:96)+1,k+1 := arg min
Z

2) Update Variable Z: Given that

=
rewrite Eq. (8) with respect to Z as follows,
λ
2
− (cid:10)Q(Z), T (cid:96)+1,k(cid:11)
λ
2

(cid:107)Z(cid:107)A(cid:96),H +

(cid:107)Z(cid:107)A(cid:96),H

= arg min

ρ
2

(Z), we can

Q

(cid:13)
(cid:13)X (cid:96)+1,k+1 − Q(Z)
(cid:13)

(cid:13)
(cid:13)
(cid:13)

2

F

Z
(cid:13)
(cid:13)
(cid:13)Z − Q−1(X (cid:96)+1,k+1 + T (cid:96)+1,k/ρ)
(cid:13)
(cid:13)
(cid:13)

2

F

.

+

ρ
2

(15)
We use the following Lemma 2 to solve this optimization

problem.

T which
Lemma 2. For any multivariate time series Z
consists of M time series over T consecutive time points, the
autoregressive process for any (m, t)th element of Z takes

RM

∈

×

d

zm,t ≈

am,izm,t

−

hi,

(16)

i=1
(cid:88)

with autoregressive coefﬁcient A

=

h1, h2, . . . , hd}
H
{
the following general formula:

d and time lag set
×
. This autoregressive process also takes

RM

∈

and for each time series zm ∈

d

RT ,

m, we have

∀

Ψ0zm ≈

i=1
(cid:88)

am,iΨizm,

(18)

where

denotes the Khatri-Rao product, and

(cid:12)
Ψ0 =
Ψi =

∈
Ψ =

0(T
0(T
(T

−

(cid:2)
R
(cid:2)

(cid:2)

hd)

×

−

hd

I T

−

−
hd)

hd)

(hd

(cid:3)
hi)
−
×
T , i = 1, 2, . . . , d,

−

×

hd
I T

(T

R
−
∈
hd 0(T

hd)

T ,

×

hd)

−

hi

×

Ψ1 Ψ2

Ψd

· · ·

are matrices deﬁned based on time lag set

(T

R

−

hd)

×

(cid:3)

(dT ),

∈

(cid:3)

.

H

H

According to Lemma 2, there are two options for updating
Z when A and
are known. The ﬁrst is to minimize the
errors in the form of matrix as described in Eq. (17), and
the second is to minimize the errors in the form of vector as
described in Eq. (18). The ﬁrst solution involves complicated
operations and possibly high computational cost (see Theorem 1
in Appendix A for details). We follow the second approach
which takes the vector form for optimizing Z. This yields a
closed-form solution in Lemma 3.
Lemma 3. Suppose Ψ0, Ψ1, . . . , Ψd ∈
RM
toregressive coefﬁcient A
×
Lemma 2, then for any m
1, 2, . . . , M
to the problem

T and au-
hd)
d are known as deﬁned in
, an optimal solution

∈
∈ {

R(T

}

−

×

zm := arg min

z

is given by

d

−

i=1
(cid:88)

1
Ψ0z
2 (cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

am,iΨiz

zm := α(B(cid:62)mBm + αI T )−

α
2 (cid:107)

z

2

+

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
1xm,

2
2,
xm(cid:107)

−

(19)

(20)

d
i=1 am,iΨi.

where Bm = Ψ0 −
Remark. Lemma 3 in fact provides a least squares solution for
zm. It is also helpful to deﬁne Bm, m = 1, 2, . . . , M as sparse
matrices and interpret zm as the solution of the following linear
equation:

(cid:80)

(B(cid:62)mBm + αI T )zm = αxm.

(21)

Ψ0Z(cid:62)

d

≈

i=1
(cid:88)

Ψi(a(cid:62)i (cid:12)

Z(cid:62)) = Ψ(A(cid:62)

(cid:12)

Z(cid:62)),

(17)

This can help avoid the expensive inverse operation on the
T -by-T matrix since T is a possibly large value.

···a11a12...y1...yMaM1···aM2IJtimepointsj=1j=JCorrelatingObservedUnobservedY∈RM×(IJ)Itimepoints|{z}Msensors|{z}Jdays|{z}X∈RM×I×JQ−1(·)Q(·), ρ, λ, r)

V. EXPERIMENTS

5

0,0 as zeros and A0 as small random values.
PΩ(Y ), α1 = α2 = α3 = 1
3 , K = 3,

Algorithm 1: imputer(Y ,

H

Initialize
Set
and (cid:96) = 0.

T
PΩ(Z0,0) =
while not converged do
for k = 0 to K
−
ρ = min
1.05
{
for j = 1 to J do
Compute

1 do

ρ, ρmax

;
}

×
X j by Eq. (11);
(cid:96)+1,k+1 by Eq. (14);

Update
X
for m = 1 to M do
Update z(cid:96)+1,k+1

m

by Eq. (22);

(cid:96)+1,k+1 by Eq. (9);

Update
T
Transform observation information by letting
PΩ(Z(cid:96)+1,k+1) =
PΩ(Y );
for m = 1 to M do
Update a(cid:96)+1

m by Eq. (24);

(cid:96) := (cid:96) + 1;

return recovered matrix ˆX.

According to Lemma 3, for any m

∈ {
closed-form solution to Eq. (15) is given by

1, 2, . . . , M

, the

}

z(cid:96)+1,k+1

m

:=

1

−

ρ
λ

I T

ρ
λ
(cid:96)+1,k+1 +

B(cid:62)mBm +
(cid:16)
1
m (
−
X
· Q
d
i=1 a(cid:96)
m,iΨi in which Ψ0, Ψ1, . . . , Ψd

(cid:96)+1,k/ρ),

(cid:17)
T

(22)

where Bm = Ψ0 −
follow the same deﬁnition as in Lemma 2.
(cid:80)

3) Update Variable A: As mentioned above, A

is the coefﬁcient matrix in the deﬁned temporal variation term.
To estimate A, we solve the following problem derived from
Eq. (5):

RM

d

×

∈

A(cid:96)+1 : = arg min
A

(z(cid:96)+1,K

m,t −

m,t
(cid:88)

i
(cid:88)

am,iz(cid:96)+1,K
m,t
−

hi)2

(23)

= arg min
A

z(cid:96)+1,K
m,[hd+1:] −

V mam

2

,

2
m (cid:13)
(cid:13)
(cid:88)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
d and vt =
hd)
where V m = (vhd+1,
, vIJ )(cid:62)
×
−
· · ·
(z(cid:96)+1,K
Rd, t = hd +1, . . . , IJ are formed
)(cid:62)
m,t
∈
−
by Z(cid:96)+1,K. Obviously, this optimization has a closed-form
solution, which is given by

, z(cid:96)+1,K
hd
m,t
−

R(IJ

h1,

· · ·

∈

m := V †mz(cid:96)+1,K
a(cid:96)+1

m,[hd+1:],

m,

∀

(24)

where

† denotes the pseudo-inverse.
·

Algorithm 1 shows the overall algorithm for solving LATC.
The algorithm has three parameters ρ, λ and r. Parameter
ρ controls the ADMM and the singular value thresholding.
Parameter λ is a trade-off between truncated nuclear norm and
ρ.
temporal variation, which can be typically set to λ = c
Thus, c = 1 implies that these two norms have the same
importance in the objective. The recovered matrix is computed
by ˆX
(cid:96),K) at each outer iteration. The algorithm
returns the converged ˆX as the ﬁnal result, if the convergence
criteria is met.

1(

X

Q

=

−

·

(cid:96)

In this section, we evaluate the proposed LATC model
on several real-world trafﬁc data sets with different missing
patterns.

A. Trafﬁc Data Sets

We use the following four spatiotemporal trafﬁc sets for our

benchmark experiment.

•

•

•

•

×

×

×

144

61).

(G): Guangzhou urban trafﬁc speed data set.1 This data set
contains trafﬁc speed collected from 214 road segments
over two months (from August 1 to September 30, 2016)
with a 10-minute resolution (i.e., 144 time intervals per
day) in Guangzhou, China. The prepared data is of size
8784 in the form of multivariate time series matrix
214
(or tensor of size 214
(H): Hangzhou metro passenger ﬂow data set.2 This data
set provides incoming passenger ﬂow of 80 metro stations
over 25 days (from January 1 to January 25, 2019) with
a 10-minute resolution in Hangzhou, China. We discard
the interval 0:00 a.m. 6:00 a.m. with no services, and
only consider the remaining 108 time intervals of a day.
2700 in the form of
The prepared data is of size 80
multivariate time series (or tensor of size 80
25).
(S): Seattle freeway trafﬁc speed data set.3 This data set
contains freeway trafﬁc speed from 323 loop detectors
with a 5-minute resolution (i.e., 288 time intervals per day)
over the ﬁrst four weeks of January, 2015 in Seattle, USA.
8064 in the form of
The prepared data is of size 323
multivariate time series (or tensor of size 323
28).
(P): Portland highway trafﬁc volume data set.4 This data
set is collected from highways in the Portland-Vancouver
Metropolitan region, which contains trafﬁc volume from
1156 loop detectors with a 15-minute resolution (i.e., 96
time intervals per day) in January, 2021. The prepared
2976 in the form of multivariate
data is of size 1156
×
time series matrix (or tensor of size 1156

31).

108

288

96

×

×

×

×

×

×

×

×

Note that the adapted data sets and Python codes for our

experiments are available on Github.5

B. Missing Data Generation

To evaluate the performance of LATC for missing trafﬁc
data imputation thoroughly, we take into account three missing
data patterns as shown in Fig. 2, i.e., random missing (RM),
non-random missing (NM), and blackout missing (BM). RM
and NM data are generated by referring to our prior work
[2]. According to the mechanism of RM and NM data, we
mask certain amount of observations as missing values (e.g.,
30%, 70%, 90%), and the remaining partial observations are
input data for learning a well-behaved model. BM pattern is
different from RM and NM patterns, which masks observations
of all spatial sensors/locations as missing values with certain

1https://doi.org/10.5281/zenodo.1205229
2https://tianchi.aliyun.com/competition/entrance/231708/information
3https://github.com/zhiyongc/Seattle-Loop-Data
4https://portal.its.pdx.edu/home
5https://github.com/xinychen/transdim

I

2I

3I

4I

5I

6I

7I

(a) Random missing (RM).

I

2I

3I

4I

5I

6I

7I

(b) Non-random missing (NM).

I

2I

3I

4I

5I

6I

7I

(c) Blackout missing (BM).

Fig. 2: Illustration of three missing data patterns for spatiotem-
poral trafﬁc data (e.g., trafﬁc speed). Each time series represent
the collected data from a given sensor. In these graphics, two
curves correspond to two different time series. (a) Data are
missing at random. Small circles indicate the missing values.
(b) Data are missing continuously during a few time periods.
Segments in gray indicate missing values. (c) No sensors are
available (i.e., blackout) over a certain time window.

window length. BM is a challenging scenario with complete
column-wise missing. We set the missing rate in the following
experiments to 30%.

To assess the imputation performance, we use the actual
values of the masked missing entries as the ground truth to
compute MAPE and RMSE:

MAPE =

1
n

ˆyi

yi −
yi

RMSE =

(yi −

n

i=1 (cid:12)
(cid:88)
(cid:12)
(cid:12)
n
1
(cid:12)
n

(cid:118)
(cid:117)
(cid:117)
(cid:116)

100,

×

(cid:12)
(cid:12)
(cid:12)
(cid:12)
ˆyi)2,

(25)

i=1
(cid:88)
where yi and ˆyi are actual values and imputed values, respec-
tively.

C. Baseline Models

For comparison, we take into account the following baseline:

•

•

Low-Rank Autoregressive Matrix Completion (LAMC).
This is a matrix-form variant of the LATC model.
Low-Rank Tensor Completion with Truncation Nuclear
Norm minimization (LRTC-TNN, [4]). This is a low-
rank completion model in which truncated nuclear norm
minimization can help maintain the most important low-
rank patterns. Since the truncation in LRTC-TNN is a
deﬁned as a rate parameter, we adapt LRTC-TNN to use

6

(a) 30%, RM.

(b) 70%, RM.

(c) 90%, RM.

(d) 30%, NM.

(e) 70%, NM.

(f) 30%, BM.

Fig. 3: RMSEs of LATC imputation on Guangzhou urban trafﬁc
5
speed data where ρ = 1
10−
for NM/BM data. The smallest RMSE is achieved by: (a)
c = 10, r = 30; (b) c = 10, r = 20, 25; (c) c = 10, r = 15; (d)
r = 10; (e) r = 5; (f) c = 10, r = 15.

4 for RM data and ρ = 1

10−

×

×

integer truncation in order to make it consistent with
LATC.
Bayesian Temporal Matrix Factorization (BTMF, [10]).
This is a fully Bayesian temporal factorization framework
which builds the correlation of temporal dynamics on
latent factors by vector autoregressive process. Due to
the temporal modeling, it outperforms the standard matrix
factorization in the missing data imputation tasks [10].
Smooth PARAFAC Tensor Completion (SPC, [7]). This
is a tensor decomposition based completion model with
total variation smoothness constraints.

•

•

D. Results

There are several parameters in LATC, including learning
.
rate ρ, weight parameter λ, truncation r, and time lag set
H
The most important parameters are the coefﬁcient c = λ/ρ
and the truncation r. For other parameters including ρ and
, we conduct preliminary test for choosing them.
time lag set
4
5, 1
for all data sets. To
ρ is chosen from
assess the sensitivity of the model over c and r, we develop
the following setting for our imputation experiments:

10−

10−

H

×

×

{

}

1

Time lag set is set as
data, and
λ = c

1, 2, 3, 4
}
{
∈ {

ρ where c

1, 2, . . . , 6
{
}
for (P) data;
1
1
5 , 1, 5, 10
10
}

·

•

•

for (G), (H), and (S)

;

51015202530Truncation parameter r110151510Coefficient c=λ/ρ3.283.183.13.043.012.993.263.163.083.022.992.973.133.032.962.912.872.852.762.712.672.642.622.612.622.592.572.562.552.542.02.22.42.62.83.03.23.451015202530Truncation parameter r110151510Coefficient c=λ/ρ3.853.73.623.593.613.673.843.693.613.583.63.643.763.613.533.493.493.513.473.373.33.273.263.273.33.233.193.183.183.192.62.83.03.23.43.63.84.051015202530Truncation parameter r110151510Coefficient c=λ/ρ4.294.094.14.184.34.454.284.094.074.154.264.394.244.0544.054.14.194.143.973.913.923.954.014.063.913.863.883.923.983.603.753.904.054.204.354.5051015202530Truncation parameter r110151510Coefficient c=λ/ρ4.134.094.114.194.284.414.134.094.114.184.284.414.134.094.114.184.284.44.134.094.14.174.264.384.134.094.14.174.254.354.024.084.144.204.264.324.384.444.5051015202530Truncation parameter r110151510Coefficient c=λ/ρ4.354.384.574.694.895.064.354.374.554.714.915.064.354.374.524.74.875.054.354.374.514.74.835.024.354.374.534.654.834.954.04.24.44.64.85.05.25.451015202530Truncation parameter r110151510Coefficient c=λ/ρ4.083.983.994.034.14.194.083.983.994.034.094.194.073.973.984.014.074.164.053.943.953.974.014.084.023.923.913.933.964.023.73.83.94.04.14.24.34.44.57

As mentioned above, despite the truncated nuclear norm
built on tensor, the results also show the advantage of temporal
variation built on the multivariate time series matrix. Due to
the temporal modeling, temporal variation can improve the
imputation performance for missing trafﬁc data imputation.
Table I shows the overall imputation performance of LATC
and baseline models on the four selected trafﬁc data sets with
various missing scenarios. Of these results, NM and BM data
seem to be more difﬁcult to reconstruct with all these imputation
models than RM data. In most cases, LATC outperforms
other baseline models. Comparing LATC with LAMC shows
the advantage of tensor structure, i.e., LATC with tensor
structure performs better than LAMC with matrix structure.
Comparing LATC with LRTC-TNN shows the advantage of
temporal variation, i.e., temporal modeling with autoregressive
process has positive inﬂuence for improving the imputation
performance. For volume data sets (H) and (P), the relative
errors are quite high because some volume values are close
to 0 or relatively small and estimating these values would
accumulate relatively large relative errors.

Figs. 5, 6, and 7 show some imputation examples with
different missing scenarios that achieved by LATC. In these
examples, we can see explicit temporal dependencies underly-
ing trafﬁc time series data. For all missing scenarios, LATC
can achieve accurate imputation and learn the true signals from
observations even with severe missing data (e.g., NM/BM data).
In Fig. 5, it shows that the time series signal of passenger ﬂow
is not complex. By referring to Table I, we can see that LRTC-
TNN without temporal variation outperforms the proposed
LATC model on Hangzhou metro passenger ﬂow data, and this
demonstrates that not all multivariate time series imputation
cases require temporal modeling, for some cases that the signal
does not show strong temporal dependencies, purely low-rank
model can also provide accurate imputation.

VI. CONCLUSION

Spatiotemporal trafﬁc data imputation is of great signiﬁcance
in data-driven intelligent transportation systems. Fortunately, for
analyzing and modeling trafﬁc data, there are some fundamental
features such as low-rank properties and temporal dynamics
that can be taken into account. In this work, the proposed LATC
model builds both low-rank structure (i.e., truncated nuclear
norm) and time series autoregressive process on certain data
representations. By doing so, numerical experiments on some
real-world trafﬁc data sets show the advantages of LATC over
other low-rank models. In addition to the imputation capability
of LATC, LATC can also be applied to spatiotemporal trafﬁc
forecasting in the presence of missing values.

APPENDIX A
SUPPLEMENTARY THEOREM

Theorem 1. Suppose Φ0 ∈
−
and autoregressive coefﬁcient A
Lemma 2, then an optimal solution to the problem

T , Φ
RM

R(T

hd)

∈

×

×

R(T

(dT ),
hd)
−
∈
d as deﬁned in

×

min
Z

1
2

Φ0Z(cid:62)
(cid:13)
(cid:13)
(cid:13)

Φ(A(cid:62)

−

(cid:12)

Z(cid:62))

2

F

+

α
2 (cid:107)

Z

X

2
F ,

(cid:107)

−

(cid:13)
(cid:13)
(cid:13)

(a) 30%, RM.

(b) 70%, RM.

(c) 90%, RM.

(d) 30%, NM.

(e) 70%, NM.

(f) 30%, BM.

Fig. 4: RMSEs of LATC imputation on Hangzhou metro
5). The smallest RMSE
passenger ﬂow data (ρ = 1
×
is achieved by: (a) c = 1, r = 15; (b-c) c = 1, r = 10; (d)
c = 1

5 , r = 5; (f) c = 1, r = 10.

10 , r = 5; (e) c = 1

10−

and r < min

M, I, J
{

.

r

5, 10, 15, 20, 25, 30
}

}

∈ {

•
Fig. 3 shows the heatmaps of imputation RMSE values
achieved by LATC model on Guangzhou urban trafﬁc speed
data. It demonstrates that: 1) for RM and BM data, when
c = 10, LATC model achieves the best imputation performance
and the truncation r has little impact on the ﬁnal results; 2) for
NM data, the coefﬁcient c is less important than the truncation r.
LATC model achieves the best performance when the truncation
is a relatively small value (e.g., 5, 10). These results veriﬁes
the importance of temporal variation minimization for RM and
BM imputation.

Fig. 4 shows similar heatmaps for Hangzhou metro passenger
ﬂow data. It can be seen that: 1) for RM and BM data, when
c = 1, LATC model achieves the best imputation performance;
2) for NM data, LATC model achieves the best performance
with small coefﬁcient c and truncation r (e.g., 5).

By testing the LATC model in the similar way, it can indicate
the importance of temporal variation on other two data sets.
On Seattle freeway trafﬁc speed data, we observe that the
coefﬁcient c has little impact on the ﬁnal imputation for the
RM and NM data. However, there show the positive inﬂuence
of temporal variation in LATC for BM data. On Portland
highway trafﬁc volume data, a relatively large coefﬁcient c
(e.g., 5 and 10) can make the model less sensitive to the various
truncation values for RM and BM data.

5101520Truncation parameter r110151510Coefficient c=λ/ρ25.5225.5426.328.1425.4725.4125.9227.6525.3325.0824.9726.3626.0625.9926.427.5227.2227.3627.9428.9520222426283032345101520Truncation parameter r110151510Coefficient c=λ/ρ28.5728.8329.6531.1928.5328.4429.5631.0928.4528.2529.4331.3729.329.8631.3833.4830.2931.0632.7834.8220222426283032345101520Truncation parameter r110151510Coefficient c=λ/ρ36.1934.8136.5537.8336.2334.6836.7437.8635.2134.4435.937.336.2636.2237.8339.5636.9138.5543.3447.3925.027.530.032.535.037.540.042.545.047.550.05101520Truncation parameter r110151510Coefficient c=λ/ρ47.3850.1550.3352.2847.4450.4249.5752.3448.5353.554.5456.7857.0461.864.4665.676468.2872.0173.4340424446485052545658605101520Truncation parameter r110151510Coefficient c=λ/ρ47.5350.9852.7554.2847.350.8952.4653.6951.1655.5556.0756.9167.1170.7371.3872.32110.7111.8112.8113.640424446485052545658605101520Truncation parameter r110151510Coefficient c=λ/ρ28.8328.7329.8232.0628.8128.729.6231.6528.8428.6430.1632.4330.4733.3837.0440.133.4238.342.545.182024283236404448TABLE I: Performance comparison (in MAPE/RMSE) of LATC and baseline models for RM, NM, and BM data imputation.

8

Data Missing

LATC

LAMC

LRTC-TNN

BTMF

SPC

(G)

(H)

(S)

30%, RM
70%, RM
90%, RM
30%, NM
70%, NM
30%, BM-6

30%, RM
70%, RM
90%, RM
30%, NM
70%, NM
30%, BM-6

30%, RM
70%, RM
90%, RM
30%, NM
70%, NM
30%, BM-12

5.71/2.54
7.22/3.18
9.11/3.86
9.63/4.09
10.37/4.35
9.23/3.91

19.12/24.97
20.25/28.25
24.32/34.44
19.93/47.38
24.30/47.30
21.93/28.64

4.90/3.16
5.96/3.71
7.47/4.51
7.11/4.33
9.46/5.42
9.44/5.36

9.51/4.04
10.40/4.37
11.65/4.79
10.11/4.23
11.15/4.60
12.15/5.17

22.65/42.94
25.30/51.26
32.30/66.13
22.93/67.08
29.23/63.95
30.78/66.03

5.98/3.73
8.02/4.70
10.56/5.91
6.99/4.25
9.75/5.60
27.05/13.66

6.99/3.00
8.38/3.59
9.55/4.05
9.61/4.07
10.36/4.34
9.45/3.97

18.87/24.90
20.07/28.13
23.46/35.84
19.94/50.12
23.88/45.06
21.40/27.83

4.99/3.20
6.10/3.77
8.08/4.80
6.85/4.21
9.23/5.35
9.52/5.41

7.54/3.27
8.75/3.73
10.02/4.21
10.32/4.33
11.36/4.85
12.43/7.04

22.37/28.66
25.65/32.23
31.51/46.24
25.61/77.00
34.50/70.11
52.15/57.61

5.91/3.72
6.47/3.98
8.17/4.81
9.26/5.36
10.47/6.15
14.33/13.60

7.37/5.06
8.91/4.44
10.60/4.85
9.13/5.29
11.15/5.17
11.14/5.13

19.82/26.21
21.02/31.91
24.97/49.68
27.46/68.56
46.86/98.81
22.49/37.53

5.92/3.62
7.38/4.30
9.75/5.31
8.87/4.99
11.32/5.92
11.30/5.84

(P)

30%, RM
70%, RM
90%, RM
30%, NM
70%, NM
30%, BM-4

21.29/56.73
24.35/43.32
28.45/39.65
26.96/60.33
33.42/47.34
31.01/60.33
Best results are highlighted in bold fonts. The number next to the BM denotes the window length.

18.22/19.14
19.96/22.21
23.90/25.71
19.55/20.38
23.86/26.74
27.85/25.68

17.93/16.03
21.26/19.37
25.64/23.75
19.93/19.69
25.75/28.25
29.21/27.60

17.27/16.08
19.99/18.73
22.90/22.68
19.59/18.91
30.26/60.85
31.74/74.42

17.46/15.89
19.56/18.70
23.47/22.74
18.90/18.84
24.67/31.74
24.04/23.52

Fig. 5: Imputed values by LATC for Hangzhou metro passenger ﬂow data. This example corresponds to metro station #3 and
the 4th day of the data set. Black dots/curves indicate the partially observed data, gray rectangles indicate blackout missing,
while red curves indicate the imputed values.

Fig. 6: Imputed values by LATC for Seattle freeway trafﬁc speed data. This example corresponds to detector #3 and the 7th
day of the data set.

6:009:0012:0015:0018:0021:0024:000200400600Passenger flow30% RM6:009:0012:0015:0018:0021:0024:000200400600Passenger flow70% RM6:009:0012:0015:0018:0021:0024:000200400600Passenger flow90% RM6:009:0012:0015:0018:0021:0024:000200400600Passenger flow30% NM6:009:0012:0015:0018:0021:0024:000200400600Passenger flow70% NM6:009:0012:0015:0018:0021:0024:000200400600Passenger flow30% BM-60:006:0012:0018:0024:0020406080Speed30% RM0:006:0012:0018:0024:0020406080Speed70% RM0:006:0012:0018:0024:0020406080Speed90% RM0:006:0012:0018:0024:0020406080Speed30% NM0:006:0012:0018:0024:0020406080Speed70% NM0:006:0012:0018:0024:0020406080Speed30% BM-129

Fig. 7: Imputed values by LATC for Portland trafﬁc volume data. This example corresponds to detector #3 and the 8th day of
the data set.

is given by

C) + αI M T ]−

C)(cid:62)(B

vec(Z(cid:62)) := α[(B
where B = (I M ⊗
⊗
Proof. In this case, we can use vectorization:

−
−
Φ0) and C = (I M ⊗

denotes the Kronecker product.

·
Φ)[(I M (cid:12)

1

vec(X (cid:62)),

A(cid:62))

I T ].

⊗

vec(Z(cid:62)),

vec(Φ(A(cid:62)

vec(Φ0Z(cid:62)) =(I M ⊗
Z(cid:62))) =(I M ⊗
=(I M ⊗

Φ0)
·
Φ)
vec(A(cid:62)
·
Φ)[(I M (cid:12)
) denotes the vectorization operator for any given
·

where vec(
matrix. Denote by f the objective of problem (1):

(cid:12)
A(cid:62))

Z(cid:62))

I T ]

(cid:12)

⊗

·

vec(Z(cid:62)),

(B

f =

1
2 (cid:107)
By letting

C)

·

−

vec(Z(cid:62))

2
2 +
(cid:107)

α
2 (cid:107)

vec(Z(cid:62))

vec(X (cid:62))

2
2.
(cid:107)

−

df
d vec(Z(cid:62))

we have

=(B

−

C)(cid:62)(B

+ α[vec(Z(cid:62))

−

−

C) vec(Z(cid:62))

vec(X (cid:62))] = 0,

vec(Z(cid:62)) =α[(B

C)(cid:62)(B

−

−

C) + αI M T ]−

vec(X (cid:62)).

1

·

ACKNOWLEDGEMENT

This research is supported by the Natural Sciences and
Engineering Research Council (NSERC) of Canada, the Fonds
de recherche du Quebec – Nature et technologies (FRQNT),
and the Canada Foundation for Innovation (CFI). X. Chen and
M. Lei would like to thank the Institute for Data Valorisation
(IVADO) for providing the PhD Excellence Scholarship to
support this study.

REFERENCES

[1] M. T. Bahadori, Q. R. Yu, and Y. Liu, “Fast multivariate spatio-temporal
analysis via low rank tensor learning,” in Advances in Neural Information
Processing Systems, 2014, pp. 3491–3499.

[2] X. Chen, Z. He, and L. Sun, “A bayesian tensor decomposition approach
for spatiotemporal trafﬁc data imputation,” Transportation Research Part
C: Emerging Technologies, vol. 98, pp. 73 – 84, 2019.

[3] L. Li, J. McCann, N. S. Pollard, and C. Faloutsos, “Dynammo: Mining
and summarization of coevolving sequences with missing values,” in
Proceedings of the 15th ACM SIGKDD international conference on
Knowledge discovery and data mining, 2009, pp. 507–516.

for spatiotemporal

[4] X. Chen, J. Yang, and L. Sun, “A nonconvex low-rank tensor completion
trafﬁc data imputation,” Transportation

model
Research Part C: Emerging Technologies, vol. 117, p. 102673, 2020.
[5] L. Xiong, X. Chen, T.-K. Huang, J. Schneider, and J. G. Carbonell,
“Temporal collaborative ﬁltering with bayesian probabilistic tensor
factorization,” in SIAM International Conference on Data Mining, 2010,
pp. 211–222.

[6] N. Rao, H.-F. Yu, P. Ravikumar, and I. S. Dhillon, “Collaborative ﬁltering
with graph information: Consistency and scalable methods.” in NIPS,
vol. 2, no. 4. Citeseer, 2015, p. 7.

[7] T. Yokota, Q. Zhao, and A. Cichocki, “Smooth parafac decomposition
for tensor completion,” IEEE Transactions on Signal Processing, vol. 64,
no. 20, pp. 5423–5436, 2016.

[8] H.-F. Yu, N. Rao, and I. S. Dhillon, “Temporal regularized matrix
factorization for high-dimensional time series prediction,” in Advances
in Neural Information Processing Systems, 2016, pp. 847–855.

[9] R. Sen, H.-F. Yu, and I. S. Dhillon, “Think globally, act locally: A deep
neural network approach to high-dimensional time series forecasting,” in
Advances in Neural Information Processing Systems, 2019, pp. 4838–
4847.

[10] X. Chen and L. Sun, “Bayesian temporal factorization for multidimen-
sional time series prediction,” IEEE Transactions on Pattern Analysis
and Machine Intelligence, pp. 1–1, 2021.

[11] L. Li, X. Su, Y. Zhang, Y. Lin, and Z. Li, “Trend modeling for trafﬁc time
series analysis: An integrated study,” IEEE Transactions on Intelligent
Transportation Systems, vol. 16, no. 6, pp. 3430–3439, 2015.

[12] H. Tan, Y. Wu, B. Shen, P. J. Jin, and B. Ran, “Short-term trafﬁc
prediction based on dynamic tensor completion,” IEEE Transactions on
Intelligent Transportation Systems, vol. 17, no. 8, pp. 2123–2133, 2016.
[13] Z. Li, N. D. Sergin, H. Yan, C. Zhang, and F. Tsung, “Tensor completion
for weakly-dependent data on graph for metro passenger ﬂow prediction,”
arXiv preprint arXiv:1912.05693, 2019.

[14] T. G. Kolda and B. W. Bader, “Tensor decompositions and applications,”

SIAM Review, vol. 51, no. 3, pp. 455–500, 2009.

[15] J. Liu, P. Musialski, P. Wonka, and J. Ye, “Tensor completion for
estimating missing values in visual data,” IEEE Transactions on Pattern
Analysis and Machine Intelligence, vol. 35, no. 1, pp. 208–220, 2013.
[16] G. Lai, W.-C. Chang, Y. Yang, and H. Liu, “Modeling long-and short-term
temporal patterns with deep neural networks,” in ACM SIGIR Conference
on Research & Development in Information Retrieval, 2018, pp. 95–104.
[17] Y. Hu, D. Zhang, J. Ye, X. Li, and X. He, “Fast and accurate matrix
completion via truncated nuclear norm regularization,” IEEE Transactions
on Pattern Analysis and Machine Intelligence, vol. 35, no. 9, pp. 2117–
2130, 2013.

[18] Y. Zhang and Z. Lu, “Penalty decomposition methods for rank
minimization,” in Advances in Neural Information Processing Systems,
2011, pp. 46–54.

[19] K. Chen, H. Dong, and K.-S. Chan, “Reduced rank regression via adaptive
nuclear norm penalization,” Biometrika, vol. 100, no. 4, pp. 901–920,
2013.

0:004:008:0012:0016:0020:0024:00100200300Volume30% RM0:004:008:0012:0016:0020:0024:00100200300Volume70% RM0:004:008:0012:0016:0020:0024:00100200300Volume90% RM0:004:008:0012:0016:0020:0024:00100200300Volume30% NM0:004:008:0012:0016:0020:0024:00100200300Volume70% NM0:004:008:0012:0016:0020:0024:00100200300Volume30% BM-4[20] C. Lu, C. Zhu, C. Xu, S. Yan, and Z. Lin, “Generalized singular value
thresholding,” in AAAI Conference on Artiﬁcial Intelligence (AAAI), 2015.

10

Xinyu Chen received the B.S. degree in Trafﬁc
Engineering from Guangzhou University, Guangzhou,
China, in 2016, and M.S. degree in Transportation
Information Engineering & Control from Sun Yat-
Sen University, Guangzhou, China, in 2019. He is
currently a PhD student with the Civil, Geological
and Mining Engineering Department at Polytech-
nique Montreal, Montreal, QC, Canada. His current
research centers on machine learning, spatiotemporal
data modeling, and intelligent transportation systems.

Mengying Lei received the B.S. degree in automa-
tion from Huazhong Agricultural University, in 2016,
and the M.S. degree from the school of automation
science and electrical engineering, Beihang Univer-
sity, Beijing, China, in 2019. She is now a Ph.D.
student with the Department of Civil Engineering at
McGill University, Montreal, Quebec, Canada. Her
research currently focuses on spatiotemporal data
modelling and intelligent transportation systems.

Nicolas Saunier received an engineering degree
and a Doctorate (Ph.D.) in computer science from
Telecom ParisTech, Paris, France, respectively in
2001 and 2005. He is currently a Full Professor
with the Civil, Geological and Mining Engineering
Department at Polytechnique Montreal, Montreal,
QC, Canada. His research interests include intelligent
transportation, road safety, and data science for
transportation.

Lijun Sun received the B.S. degree in Civil Engi-
neering from Tsinghua University, Beijing, China,
in 2011, and Ph.D. degree in Civil Engineering
(Transportation) from National University of Singa-
pore in 2015. He is currently an Assistant Professor
with the Department of Civil Engineering at McGill
University, Montreal, QC, Canada. His research
centers on intelligent transportation systems, machine
learning, spatiotemporal modeling, travel behavior,
and agent-based simulation. He is a member of the
IEEE.

