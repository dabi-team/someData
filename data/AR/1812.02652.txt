8
1
0
2
c
e
D
6

]

R
S
.
h
p
-
o
r
t
s
a
[

1
v
2
5
6
2
0
.
2
1
8
1
:
v
i
X
r
a

Solar Physics
DOI: 10.1007/•••••-•••-•••-••••-•

Solar ﬂare forecasting from magnetic feature
properties generated by Solar Monitor Active Region
Tracker
Solar Physics

Katarina Domijan1
D.Shaun Bloomﬁeld2

·

· François Pitié3

c(cid:13) Springer ••••

Abstract We study the predictive capabilities of magnetic feature proper-
ties (MF) generated by Solar Monitor Active Region Tracker (SMART) Hig-
gins et al.(Adv. Space Res. 47, 2105, 2011) for solar ﬂare forecasting from two
datasets: the full dataset of SMART detections from 1996 to 2010 that has
been previously studied by Ahmed et al. (Solar Phys. 283(1), 179, 2011) and
a subset of that dataset which only includes detections that are NOAA active
regions (ARs).
Main contributions: we use marginal relevance as a ﬁlter feature selection method
to identify most useful SMART MF properties for separating ﬂaring from non-
ﬂaring detections and logistic regression to derive classiﬁcation rules to predict
future observations. For comparison, we employ a Random Forest, Support Vec-
tor Machine and a set of Deep Neural Network models, as well as Lasso for feature
selection. Using the linear model with three features we obtain signiﬁcantly bet-
ter results (TSS=0.84) to those reported by Ahmed et al. (Solar Phys. 283(1),
179, 2011) for the full dataset of SMART detections. The same model produced
competitive results (TSS=0.67) for the dataset of SMART detections that are
NOAA ARs which can be compared to a broader section of ﬂare forecasting
literature. We show that more complex models are not required for this data.

Keywords: Active Regions, magnetic ﬁelds; Flares, Dynamics; Photosphere;
Space weather; Feature Selection; Machine Learning; Support vector machines,
Random forests; Deep Neural Networks

(cid:66) K. Domijan

katarina.domijan@mu.ie

1 Maynooth University

2 Northumbria University

3 Trinity College Dublin

SOLA: sola_KD.tex; 7 December 2018; 1:29; p. 1

 
 
 
 
 
 
Domijan et al.

————————————————-

1. Introduction

Solar ﬂares strongly inﬂuence space weather and their prediction using pho-
tospheric magnetic ﬁeld observations has been studied extensively in recent
years (e.g. Abramenko (2005); McAteer, Gallagher, and Ireland (2005); Schrijver
(2007); Leka and Barnes (2007); Georgoulis and Rust (2007); Qahwaji et al.
(2008); Colak and Qahwaji (2008); Barnes, and Leka (2008); Colak and Qah-
waji (2009); Mason and Hoeksema (2010); Yu et al. (2010); Yang et al. (2013);
Al-Ghraibah, Boucheron, and McAteer (2015); Boucheron, Al-Ghraibah, and
McAteer (2015); Bobra, and Couvidat (2015); Liu et al. (2017b); Daei, Safari,
and Dadashi (2017); Gheibi, Safari, and Javaherian (2017); Liu et al. (2017a);
Raboonik et al. (2017); Nishizukaet al. (2017); Nishizuka et al. (2018); Huang
et al. (2018))

In this paper we analysed a dataset of magnetic feature (MF) properties gener-
ated by Solar Monitor Active Region Tracker (SMART) (Higgins et al., 2011), an
automated system for detecting and tracking active regions (AR) from SOHO
Michelson Doppler Interferometer (MDI) magnetograms. SMART determines
MF properties such as region size, total ﬂux, ﬂux imbalance, ﬂux emergence
rate, Schrijver’s R-value and Falconer’s measurement of non-potentiality. Each
MF detection was classed as ﬂaring or non-ﬂaring if it produced a C-class or
above ﬂare within the 24 hours following the observation.

This dataset was previously analysed by Ahmed et al. (2011) and in this
paper one of the aims was to improve on their results. We considered a number
of classiﬁcation approaches: binary logistic regression (LR) (Cox, 1958), which is
a linear classiﬁer, as well as the classiﬁers that allow for nonlinear classiﬁcation
rules, namely: Random Forests (RF) (Breiman, 2001), support vector machines
(SVMs) (Vapnik, 1998) and a set of Deep Feedforward Neural Network (DNN)
architectures. We considered feature selection for the linear model: the LR clas-
siﬁer was applied to a small subset of MF properties selected by a marginal
relevance (MR) criteria (Dudoit, Fridlyand, and Speed, 2002) and the full fea-
ture set. We also used Lasso (Tibshirani, 1996), a model related to LR that
simultaneously performs classiﬁcation and feature selection.

To assess how the results of a predictive model will generalize to an inde-
pendent dataset we used cross-validation where the training of algorithms and
feature selection are carried out on the training set and the presented results
are shown for the test set. True Skill Scores (TSS), Heidke Skill Score (HSS),
Receiver Operating Characteristic (ROC) curves and Area Under ROC curve
(AUC) were used as measures of classiﬁer performance.

For the dataset analysed by Ahmed et al. (2011) we found that the linear clas-
siﬁer using only the top three features selected by MR yielded good classiﬁcation
rates with the highest TSS of 0.84, sensitivity (recall) of 95% and speciﬁcity of
89%. This is a signiﬁcant improvement on the previous analysis of this data.
None of the other approaches that we considered exceeded this performance.

SOLA: sola_KD.tex; 7 December 2018; 1:29; p. 2

Solar ﬂare forecasting from MF properties generated by SMART

SMART detects MFs automatically and independently from NOAA active
regions. A large number of detections are small magnetic ﬂux regions that have
no associated sunspot structure and do not possess many of the properties that
SMART calculates, yielding values close to zero for some of the features. These
detections never ﬂare and it is relatively easy for a forecasting system to get
them correct. In order to compare our results to a broader section of the ﬂare
forecasting literature, we analysed a second set of results that correspond to
SMART detections which are NOAA active regions by initially ﬁltering the
SMART dataset. For this reduced dataset, the same linear classiﬁer with the
top three features selected by MR yielded TSS of 0.67 with corresponding sensi-
tivity and speciﬁcity of 87% and 80%. None of the other models, including more
comprehensive searches of the feature space and nonlinear classiﬁers, were able
to improve on this performance.

Based on the classiﬁcation results as well as the visualization of the data we
show that there is no advantage in including a larger number of features or ﬁtting
more complex, non-linear models for these datasets.

For comparison with Ahmed et al. (2011) we used the same split of the data
into training and testing sets. For the full dataset of all SMART detection, the
training set is large, comprising of 330,000 instances and as such puts constraints
on the choice of classiﬁer methodology. For example, kernel based classiﬁers such
as SVM require the computation of an n × n dimensional kernel matrix and do
not scale well to data where n, the number of instances is large. To evade this
issue we took the approach of subsampling from the full training set to construct
50 smaller training sets of 400 instances each. The SVMs were trained on these
small training sets whereas DNNs involve highly parameterized models and were
trained on the full training set. LR and RF were trained on the small subsampled
training sets and the full training set. We found that using the entire training
set to train the algorithms gives no improvement in test classiﬁcation rates.

The analysis of the data was done in R, a free software environment for
statistical computing and graphics (R Core Team, 2017). The graphical displays
were produced using ggplot2 (Wickham, 2009) and plot3D (Soetaert, 2017)
packages. The code to reproduce the analysis and graphics can be accessed at
https://github.com/domijan/Sola.

The paper is organized as follows: in Section 2 we describe the dataset, in
Section 3 we brieﬂy outline the method used for feature selection; the classiﬁca-
tion algorithms; the cross-validation settings for assessing classiﬁer performance
and the forecast performance measures. Section 4 presents the results and in
Section 5 we make some concluding comments.

2. Data

Data are line-of-sight magnetograms from SOHO/MDI. Magnetic feature prop-
erties were extracted by Solar Monitor Active Region Tracking algorithm (Hig-
gins et al., 2011). Flares are from Geostationary Operational Environmental
Satellite (GOES) soft X-ray (1–8 Å) ﬂare lists provided by NOAA/SWPC.

SOLA: sola_KD.tex; 7 December 2018; 1:29; p. 3

Domijan et al.

SMART detects MFs automatically and independently from NOAA active
regions. Following (Ahmed et al., 2011) we deﬁned an “MF detection” as an
individual SMART MF detected in one MDI magnetogram. Each MF detection
was classiﬁed as ﬂaring or non-ﬂaring if it produced a C-class or above ﬂare
within the 24 hours following the observation. In order to minimize the error
caused by projection eﬀects, only MF detections located within 45 deg from solar
disc center were considered. The dataset comprises of MF detections generated
by SMART from April 1996 - Dec 2010. A list of SMART MF features used in
this analysis with descriptions is given in Table 4.

In this paper we study two datasets: the “full SMART dataset” of all MF
detections generated by SMART from 1996 April 1 to 2010 December 31 and a
“NOAA AR dataset” containing of only those SMART MF detections that can
be associated with NOAA ARs. The second dataset is derived by retaining only
those SMART MF detections whose boundaries encompass the coordinates of
one or more NOAA ARs after these were time-rotated to the MDI observation
times used by SMART.

3. Methods

3.1. Classiﬁcation algorithms

Logistic regression is a well established framework for modelling and predic-
tion of data where the response variable of interest is binary. It is a subset of
the Generalized Linear Models (GLMs)(Nelder and Wedderburn, 1972) that are
widely used across a range of scientiﬁc disciplines and are available in almost all
statistical software packages.

For each MF detection in a training dataset, we have a feature vector Xi
and an observed class label Yi ∈ {0, 1}, denoting if it produced a ﬂare. The
distribution of of Yi is modeled by a Bernoulli (pi) distribution, where pi =
P (Yi = 1|Xi, β) denotes the probability of ﬂare and β is the parameter vector.
In this model we use the logit link, where pi is the logistic function of a linear
combination of the explanatory features:

.

pi =

1
1 + eβ0+βXi
The model coeﬃcients β are estimated using maximum likelihood and are
used to estimate pi. The class of a MF detection i can be predicted by thresh-
olding the estimated pi at a particular value therefore giving a linear classiﬁcation
algorithm. As such, LR is a related model to Fisher’s linear discriminant analysis
(LDA) (Fisher, 1936) and SVMs with linear kernels. Other classiﬁcation methods
that we considered were chosen because they take very diﬀerent approaches to
inducing nonlinearity, feature selection and model-ﬁtting.

LR is an example of a feedforward neural network architecture with a single
neuronal unit, single layer, and sigmoid activation function. By adding units and
layers, the neural networks extend the LR model to complex models with non-
linear classiﬁcation boundaries. The architectures with two or more hidden layers

SOLA: sola_KD.tex; 7 December 2018; 1:29; p. 4

Solar ﬂare forecasting from MF properties generated by SMART

are generally called deep neural networks (DNNs) (Géron, 2018). There are
many types of DNN architectures and these models that have been successfully
applied in the domains of text and image analysis. In this paper we employed a
multi-layer perceptron (MLP) which consists of a sequence of densly connected
layers of neurons. This is the classic architecture for the data where the feature
vector does not have a hierarchical structure, as is the case for image or text
data.

In this paper we considered a range of fully connected layers architectures: two
hidden layers with 8 and 4 units (DNN_8_4), two hidden layers with 16 units
each (DNN_16_16), two hidden layers with 256 and 32 units (DNN_256_32)
and three hidden layers with 13, 6 and 6 units each (DNN_13_6_6). We chose
tanh activations for the hidden units. The output layer for each of the networks is
a single sigmoid unit and the loss function is set as the binary cross-entropy. The
networks have been trained over 200 epochs with a mini-batch size of 1024 and
using the ADAM optimization strategy. We did consider deeper architectures,
but they were overﬁtting the data and we also considered diﬀerent activation
functions but there was no diﬀerence in the algorithm performance.

A support vector machine (SVM) is a kernel extension of a binary linear
classiﬁer that constructs a hyperplane to separate two classes. The hyperplane
is chosen so that the smallest perpendicular distance of the training data to
the hyperplane (margin) is maximized. A tuning parameter (cost) controls the
number of observations that are allowed to violate the margin or the hyperplane.
Kernel trick is a general technique that can be applied to any optimization
problem which can be rewritten so that it takes the inner products between pairs
of the training observations as opposed to the observations themselves. When
the inner product is replaced with a more general kernel, the observations are
implicitly mapped to a higher dimensional feature space where the optimization
takes place. For linear classiﬁers, this has the eﬀect of ﬁtting nonlinear decision
boundaries in the original feature space. The shape of the boundary is determined
by the choice of kernel and its parameterization.

Random Forests (RF) provide a diﬀerent approach to the classiﬁcation
problem and to feature selection. They grow a number of decision trees on
bootstrapped samples of the training set. Each tree recursively partitions the
feature space into rectangular subregions where the predicted class is the most
common occurring class. At each iteration, a tree algorithm searches through
all the possible split-points along a randomly selected subset of features to ﬁnd
a partition which minimizes the region impurity, measured by the Gini index.
For a binary problem, the Gini index is given by: 2ˆpm(1 − ˆpm), where ˆpm is the
proportion of ﬂaring observations in region m. A single consensus prediction is
obtained from all the trees using majority vote which allows for very complex
and nonlinear decision boundaries. The total decrease in the Gini index from
splitting on a feature, averaged over all trees can be taken as an estimate of that
feature’s importance.

3.2. Feature selection

For this study, we use the Marginal Relevance (MR) score to rank the features
in order of their capability to discriminate between the two classes (ﬂare/non-

SOLA: sola_KD.tex; 7 December 2018; 1:29; p. 5

Domijan et al.

ﬂare). The MR score for each feature is the ratio of the between-class to within-
class sum of squares. This idea underpins many statistical methodologies and
is frequently used in genetics to screen out a large number of spurious features,
see, for example, Dudoit, Fridlyand, and Speed (2002).

The approach to feature selection using MR screens out the unnecessary
features before applying logistic regression. MR considers the information in
each feature independently so the highest ranked features can be correlated and
do not necessarily form the optimal subset for the purposes of classiﬁcation.

For these datasets we also ﬁtted Lasso (Tibshirani, 1996), a model related
to LR, but where the coeﬃcients β are simultaneously shrunk to zero using
a penalty which is controlled by a tuning parameter. Lasso provides a more
sophisticated approach to feature selection than MR and simultaneously re-
duces dimensionality of the feature space and performs classiﬁcation. Lasso is
implemented in R package glmnet (Friedman, Hastie, and Tibshirani, 2009).

All features were used to train the nonlinear classiﬁers. SVMs combine all the
feature information into a distance matrix (kernel) and can cope with correlated
inputs and a small number of spurious features. In RF all the features are used
to grow the trees and at each iteration randomly selected subsets are jointly
considered for subdividing the feature space.

3.3. Cross-validation

For consistency and comparison with Ahmed et al. (2011) we use the MF detec-
tions from April 1996 - December 2000 and January 2003 - December 2008 to
train the classiﬁcation algorithms and the MF detections from January 2001 -
December 2002 and January 2009 to December 2010 comprise the test set.

The number of ﬂaring/non-ﬂaring SMART detections in the training and

testing sets for both the full and NOAA AR data are shown in Table 1.

The training set is further subsampled by randomly drawing 200 instances of

ﬂares and 200 instances of non-ﬂares to form 50 smaller training sets.

The full SMART dataset contains 490,997 non-ﬂaring and 27,244 (5.4%) ﬂar-
ing instances and therefore exhibits a large class imbalance. In the NOAA AR
dataset 18.6% of detections were classed as ﬂares. Class imbalance is a common
problem and has received a great deal of attention in classiﬁcation literature,
see, for example Chawla, Japkowicz, and Kotcz (2004). In construction of the
subsampled training sets we uniformly sampled instances of ﬂares and non-ﬂares
but adjusted the mixture of the classes, an approach known as case-control
sampling. Logistic regression models ﬁtted to subsamples can be converted to a
valid model using a simple adjustment to the intercept, see Fithian and Hastie
(2014).

3.4. Forecast performance measures

In a binary classiﬁcation problem we can designate one outcome as positive
(ﬂare) and the other as negative (no ﬂare). For algorithms with probabilistic
outputs, binary forecasts are obtained by thresholding p, e.g. predicting a ﬂare if
the estimated p > 0.5. A confusion matrix is constructed by cross-tabulating the

SOLA: sola_KD.tex; 7 December 2018; 1:29; p. 6

Solar ﬂare forecasting from MF properties generated by SMART

Table 1. The number of ﬂaring/non-ﬂaring SMART detections in the
full and reduced datasets.

Full SMART dataset

NOAA AR dataset

Training set Testing set Training set Testing set

ﬂare
non-ﬂare

16673
313617

10571
177380

1137
5272

707
2789

predicted with the observed classes. This presents the number of true positives
TP (ﬂare predicted and observed), false positives FP (ﬂare predicted but not
observed), true negatives TN (no ﬂare predicted and none observed) and false
negatives FN (no ﬂare predicted but observed).

The true positive rate (TPR), or sensitivity, is the proportion of correctly
classiﬁed ﬂares out of all the ﬂares observed in the sample TPR = TP/ (TP+FN).
The true negative rate (TNR), or speciﬁcity, is the proportion of true negatives
out of all the non-ﬂaring instances. The false positive rate is FPR = 1 - TNR
and the false negative rate is FNR = 1 - TPR.

A classiﬁer that performs well will give a high TPR and TNR and, con-
sequently, low FPR and FNR. For classiﬁers that give probabilistic outputs,
sensitivity (TPR) can be increased by lowering the threshold of p, but this
automatically increases the FPR. An optimal choice of the threshold is context
dependent: the cost of FNs might be higher than FPs. For a (0, 1) range of
thresholds, receiver operating characteristic (ROC) curve plots the TPR vs FPR.
ROC curve and the corresponding area under the ROC curve (AUC) are used
for comparing the performance of algorithms over the entire range of thresholds.
The ideal ROC curve is in the top left corner, giving high TPR and a low FPR,
and the maximum possible value for AUC is 1.

For a single threshold or for classiﬁers with non-probabilistic outputs, the
elements of the confusion matrix can be combined in a number of ways to obtain
a single measure of the performance of a given method.

Accuracy (ACC) gives the proportion of correctly classiﬁed observations over

both classes.

True skill statistic (TSS), (Youden, 1950; Hanssen and Kuipers, 1965) com-

bines the sensitivity and speciﬁcity by taking TSS = TPR + TNR - 1.

Heidke skill score (HSS) (Heidke, 1926) measures the fraction of correct pre-
dictions after adjusting for the predictions which would be correct due to random
chance.

For more details on forecast performance measures used in solar ﬂare literature

see Bloomﬁeld et al. (2012); Barnes, and Leka (2008); Barnes et al. (2016).

4. Results

LR with top three features selected by MR (LR) and SVMs were trained on
50 subsampled training sets. The results for the SVM were obtained from R

SOLA: sola_KD.tex; 7 December 2018; 1:29; p. 7

Domijan et al.

package e1071 (Meyer et al., 2017), with a Gaussian kernel with the bandwidth
parameter set to 0.03 for the full SMART dataset and 0.01 for the NOAA AR
dataset. The cost of constraints violation was set to 1.

LR with top three features selected by MR (LR3), the full set of features
(LR13), RF and DNN were trained on the full training set. The DNN architec-
tures were: two layers with 8 and 4 units (DNN_8_4), two layers with 16 units
each (DNN_16_16), two layers, 256 and 32 units (DNN_256_32) and three
layers 13,6 and 6 units each (DNN_13_6_6).

For RF, 500 trees were grown, where at each iteration three variables were
randomly sampled as candidates for each split. The tuning parameters of RF
and SVMs were tuned over grids using cross-validation of the training set. For
support vector machines (SVMs) we tried out Gaussian, Anova and Laplacian
kernels and Bayesian Kernel Projection Classiﬁer (BKPC) (Domijan and Wilson,
2011), a sparse Bayesian variant using lower dimensional projections of the data
in the feature space. All three kernels and BKPC performed equally well in the
training set at the optimal values of their kernel parameters so, in the paper
we present the results of the Gaussian kernel as it is best known. RF is imple-
mented in R library randomForest (Liaw and Wiener, 2002). MR algorithm is
implemented in R library BKPC (Domijan, 2016). DNNs were ﬁtted using Keras
library in R (Allaire, and Chollet, 2017).

For the purposes of analysis, some of the features were log transformed (high-
gradient neutral-line length in the region, neutral-line length in the region,
Falconer’s W LSG value, Schrijver’s R value, total un-signed magnetic ﬂux and
ﬂux emergence rate). Same transformations were found to be adequate for both
the full SMART dataset and the NOAA AR dataset. For both datasets, the
features in the training sets were scaled to have zero mean and unit variance
and the same scaling was then applied to the test sets.

The forecast performance measures (TPR, TNR, TSS, ACC and HSS), de-
scribed in Section 3.4, were calculated for the test set at a range of thresholds
of p.

For the classiﬁers trained on 50 subsampled training sets, 50 classiﬁcation
rules were obtained and consequently the median values, 2.5th and 97.5th per-
centiles of the resulting forecast performance measures are reported. This can
be used to assess the sensitivity of the algorithms to the choice of the training
sets.

4.1. Full SMART dataset

Figure 1(a) shows the ROC curves plotted for all the classiﬁers for the full
SMART dataset. For the algorithms trained on 50 subsampled training sets (LR
and SVM), the ROC curve is obtained from the median TPR and FPR over the
(0, 1) range of thresholds. The ROC curves are very close and show that after
careful tuning, all models perform equally well and converge to the same results
in terms of the performance measures.

AUC, the highest TSS and HSS for all classiﬁers for the full SMART data
are given in Table 2. For algorithms trained on 50 subsamples, the reported
value is the median with 2.5th and 97.5th percentiles given in brackets. AUC

SOLA: sola_KD.tex; 7 December 2018; 1:29; p. 8

Solar ﬂare forecasting from MF properties generated by SMART

(a)

(b)

Figure 1. ROC curves, calculated for the training set for LR (LR with 3 features trained
on subsamples), LR3 (LR with 3 features trained on the full training set), LR13 (LR with
all features trained on the full training set), RF, SVM and the DNN architectures. (a) Full
SMART dataset, (b) NOAA ARs dataset.

Table 2. AUC, the highest TSS and HSS for all the classiﬁers. The ﬁrst two
(LR and SVM) were ﬁtted to subsampled training sets. The rest were trained on
the full training set.

Classiﬁer

TSS

HSS

AUC

LR
SVM
LR3
LR13
RF
DNN_8_4
DNN_16_16
DNN_256_32
DNN_13_6_6

0.84 (0.83, 0.84)
0.83 (0.83, 0.84)
0.84
0.84
0.83
0.83
0.83
0.83
0.83

0.63 (0.60, 0.64)
0.56 (0.51, 0.60)
0.64
0.64
0.63
0.63
0.63
0.63
0.64

0.966 (0.962, 0.968)
0.949 (0.942, 0.956)
0.967
0.967
0.964
0.966
0.966
0.966
0.965

ranged from 0.949 to 0.967, TSS ranged from 0.83 to 0.84 and HSS ranged from
0.56 to 0.64. This again shows that no model convincingly outperformed the
others in terms of predictive ability for this dataset. The linear model with only
three features works as well as the more complex models that allow for nonlinear
classiﬁcation boundaries. Likewise, including extra features in the linear model
did not improve performance. The results for LR with three features are the same
for the algorithm trained on the subsampled training sets and the full training
set (LR and LR3), showing that small datasets of 400 instances are suﬃcient to
train this model. Narrow conﬁdence bands for LR and SVM indicate that the
classiﬁcation results are consistent across the subsampled training sets.

For the logistic regression algorithm with the three input features trained on
the subsampled training sets (LR) the median values for TSS, TPR, TNR, ACC
and HSS at each threshold are presented in Table 5 in the Appendix. The 2.5th
and 97.5th percentiles for TSS and HSS are given in brackets. For comparison,

SOLA: sola_KD.tex; 7 December 2018; 1:29; p. 9

0.000.250.500.751.000.000.250.500.751.00FPRTPRClassifierLRLR3LR13RFSVMDNN_8_4DNN_16_16DNN_256_32DNN_13_6_6ROC: full SMART dataset0.000.250.500.751.000.000.250.500.751.00FPRTPRClassifierLRLR3LR13RFSVMDNN_8_4DNN_16_16DNN_256_32DNN_13_6_6ROC: NOAA ARsDomijan et al.

Table 6 presents the results for the same algorithm trained on the full training
set (LR3).

For LR, the highest TSS of 0.84 is obtained at the thresholds between 0.04
to 0.08 which give the TPR in the range of 0.96 and 0.92 and TNR of 0.88
and 0.91 respectively. The results from Ahmed et al. (2011) give a TPR of
0.523, TNR of 0.989 with HSS of 0.595 for the machine learning algorithm and
TPR of 0.814 and HSS of 0.512 (TNR is not reported) for the automated solar
activity prediction (ASAP). Area under ROC curve (not reported by Ahmed
et al. (2011)) was calculated for the 50 curves and the median AUC value for LR
is 0.966 with 2.5th and 97.5th percentiles of 0.962, 0.968. For the same model
trained on the full training set using three features (LR3), the AUC was 0.967.
Using all features in the model (LR13) did not increase AUC from 0.967.

4.2. Choice of skill scores and threshold

The LR model ﬁts a sigmoid surface over the range of X and the decision
boundary separating the two predicted classes at any threshold is linear.

Note that in this dataset a single MF is tracked though time and will be
recorded multiple times throughout its lifetime. For the purpose of this analysis,
all MF detections are treated as individual measurements. This can pose a prob-
lem for interpreting the probabilistic inference of LR which is underpinned by the
independence assumption: the standard error estimates for the coeﬃcients are no
longer reliable and p cannot be interpreted as probability. However, in this paper,
we do not make use of probabilistic inference and we treat the logistic regression
model as a deterministic linear classiﬁer, where the choice of thresholding value
of estimated p is based on context requirements: comparing the acceptable levels
of true positive and true negative rates for diﬀerent thresholds. Furthermore, by
training algorithms on very small subsets of the original data (100 instances
of ﬂares randomly drawn from 16,673 and 300 non-ﬂares from 313,617) one is
unlikely to get many detections of the same MF in the same sub-sample, which
helps evade this problem. Alternatively, one could enforce the randomly drawn
detections to have large enough time cadences between them (e.g. more than
two weeks) in order to ensure that the same MF is not recorded in the same
training set multiple times throughout its lifetime.

Figure 2 shows the calculated sensitivity (TPR), speciﬁcity (TNR), ACC, TSS
and HSS over a range of probability thresholds (0.01 to 0.99 in steps of 0.01),
where ˆp was estimated from the LR3 model. The proportion of ﬂaring instances
in the full training set is 0.05, shown as the vertical line on the graph. This ﬁgure
shows increase in TNR at the expense of TPR with increase of the threshold. At
the lowest threshold p = 0.01, the 82% of non-ﬂares are correctly identiﬁed and
this increases to 89% at 0.05. Likewise, 98% of ﬂares are correctly predicted at
the threshold of 0.01 and 95% are correctly identiﬁed at the threshold of 0.05.
The maximum estimated TSS = TNR + TPR - 1 is 0.84. Accuracy, a measure
of overall error rate is maximized at the threshold of p = 0.5, which gives TNR
of 99%, but misclassiﬁes 50% of the ﬂares. This illustrates how ACC is a very
poor choice of metric for data with a large class imbalance. At threshold of 1,
classifying all observations as negative, one will still get a 95% accuracy score

SOLA: sola_KD.tex; 7 December 2018; 1:29; p. 10

Solar ﬂare forecasting from MF properties generated by SMART

Figure 2. Sensitivity (TPR), speciﬁcity (TNR), accuracy, TSS and HSS over a range of
probability thresholds. Estimated probability was obtained from the logistic regression with
three features on the full SMART dataset. The proportion of ﬂares in the training dataset is
0.05 (vertical line).

and choosing a threshold of p = 0.5 will misclassify over half the ﬂare detections.
Given that the assumptions for probabilistic inference in LR are met, the p
estimates the likelihood that an observation is going to ﬂare given its feature
information. Before ﬁtting the model and utilizing the feature information, the
probability that a randomly selected detection will ﬂare is 0.05. Unlike LDA,
the prior information about ﬂare prevalence is not incorporated in the model.
Thus it is sensible to take this prior as a threshold as opposed to 0.5. For this
threshold, the algorithm will classify a detection as ﬂare if the estimated p is
greater than the probability we would assign to a randomly selected detection.
Figure 2 shows that this threshold strikes a sensible balance between TPR and
TNR and indicates that TSS is a better choice than HSS for imbalanced data.
HSS is maximized at a higher threshold of 0.3 with TPR of 0.72.

ROC curves allow for comparison of classiﬁers with probabilistic outputs over
the range of thresholds. AUC summarises the forecast performance in a single
score, however, one could argue that comparing algorithm performance at an
‘optimal’ threshold is more useful than over the entire range. When comparing
classiﬁer performance using skill scores, we argue that it is useful to plot skill
score curves over the threshold range.

Figure 3 shows the TSS curves for diﬀerent classiﬁers (for LR the curve is
median TSS with 2.5th and 97.5th percentile band). Figure 3(a) shows the TSS
curves from the algorithms trained and tested on the full SMART dataset. The
maximum TSS values obtained from all classiﬁers are very close (ranging from
0.82-0.84) but are obtained at diﬀerent thresholds for p since the TSS curves
diﬀer in shape. This is due to the fact that the probability of ﬂare is estimated
diﬀerently in these models. For the RF the estimate of p is non-parametric.

SOLA: sola_KD.tex; 7 December 2018; 1:29; p. 11

0.000.250.500.751.000.000.250.500.751.00pACCHSSTNRTPRTSSPerformance Measures(a)

(b)

Domijan et al.

Figure 3. TSS or median TSS curve for all the classiﬁers. LR (LR with 3 features trained
on subsamples), LR3 (LR with 3 features trained on the full training set), LR13 (LR with all
features trained on the full training set), RF, SVM and four DNN architectures. TSS curve
for LR (full line) has 2.5th and 97.5th percentile band. (a) Full SMART dataset, (b) NOAA
AR dataset.

By default SVM produces categorical outputs, however probabilistic extensions
exist and the R implementation ﬁts a probabilistic regression model that assumes
(zero-mean) Laplace-distributed errors for the predictions. For DNNs, in order
to balance the classes, the class weight of 20 was used for the ﬂare class labels
in the computation of the loss function, which is equivalent to upsampling to
match the majority class. Therefore, for the DNN models, TSS is optimised at
a threshold of 0.5.

4.3. NOAA AR dataset

Figure 1(b) shows the ROC curves plotted for all the classiﬁers for the NOAA
AR data. TSS curves are given in Figure 3(b). For LR the curve is median TSS
with 2.5th and 97.5th percentile band. Compared to the results of the algorithms
trained and tested on the full SMART data, the performance of all algorithms
is signiﬁcantly weaker if trained and tested on the NOAA AR dataset, however
this is still competitive with the results reported elsewhere in the solar ﬂare
forecasting literature, for example see Barnes et al. (2016).

For the NOAA AR dataset, AUC, the highest TSS and HSS for all classiﬁers
are given in Table 3. AUC ranged from 0.9 to 0.91, TSS ranged from 0.64 to
0.67 and HSS ranged from 0.57 to 0.59. For logistic regression algorithms (LR,
LR3, LR13) and RF, the TSS is optimised at the threshold of p ≈ 0.18 which is
the prevalence of ﬂares in the training set of this data. For DNN architectures
the class weight of 5 was used for the ﬂare class labels in the computation of the
loss function. For full output of LR and LR13 results, see Table 7 and Table 8
in the Appendix).

The results show that the algorithms with top three features (LR and LR3)
perform as well as the linear model with all features (LR13) and all the nonlinear
algorithms (DNNs, RF and SVM). In addition, small datasets of 400 instances
are suﬃcient to train the linear model. Narrow conﬁdence bands for LR and

SOLA: sola_KD.tex; 7 December 2018; 1:29; p. 12

0.00.20.40.60.80.000.250.500.751.00pTSSClassifierLRLR3LR13RFSVMDNN_8_4DNN_16_16DNN_256_32DNN_13_6_6TSS: full SMART dataset0.00.20.40.60.80.000.250.500.751.00pTSSClassifierLRLR3LR13RFSVMDNN_8_4DNN_16_16DNN_256_32DNN_13_6_6TSS: NOAA ARsSolar ﬂare forecasting from MF properties generated by SMART

Table 3. NOAA AR data: AUC, the highest TSS and HSS for all the classiﬁers.
The ﬁrst two (LR and SVM) were ﬁtted to subsampled training sets. The rest
were trained on the full training set.

Classiﬁer

TSS

HSS

AUC

LR
SVM
LR3
LR13
RF
DNN_8_4
DNN_16_16
DNN_256_32
DNN_13_6_6

0.66 (0.658, 0.664)
0.64 (0.63, 0.66)
0.66
0.67
0.64
0.66
0.67
0.66
0.66

0.59 (0.57, 0.59)
0.57 (0.55, 0.58)
0.59
0.58
0.57
0.57
0.59
0.59
0.58

0.90 (0.90, 0.90)
0.90 (0.89, 0.90)
0.91
0.91
0.90
0.90
0.90
0.90
0.90

SVM indicate that the classiﬁcation results are consistent across the subsampled
training sets.

4.4. Feature analysis and selection

The marginal relevance score for each feature was derived from the data used to
train the classiﬁcation algorithm (detections recorded from April 1996 - Decem-
ber 2000 and January 2003 - December 2008). Features in order of their marginal
relevance derived from the full and the NOAA AR dataset are given in Table 4.
The third and fourth columns give the importance order of the top six features
obtained from the Random Forest in both datasets. The top features selected
by Lasso are given in columns ﬁve and six. Column titles with (R) denotes the
importance measures were derived for the reduced NOAA AR dataset.

MR selects high-gradient neutral-line length in the region (LsgMm), max-
imum gradient along polarity inversion line (MxGradGpMm) and neutral-line
length in the region (LnlMm) as the top three features for both full SMART and
NOAA AR dataset. For both datasets, the best performing and most parsimo-
nious Lasso model had three features, but selected area of the region (AreaMmsq)
or total un-signed magnetic ﬂux (BﬂuxMx) instead of maximum gradient along
PIL (MxGradGpMm). These algorithms had the same classiﬁcation performance
as LR3 and LR13. In addition to neutral-line length in the region (LnlMm), fea-
tures with highest importance selected by RF were Schrijver’s R value (RvalMx),
total un-signed magnetic ﬂux (BﬂuxMx) and Falconer’s W LSG value (WLsg-
GpMm).

The forecast performance measures for the Lasso models, RFs and LR with
three or more features were similar in both datasets. Many other approaches to
feature selection exist, but this indicates that there is little scope for improve-
ment with more thorough exploration of the feature space.

Figure 5 and 6 in the Appendix plot the marginal densities from the training
set of the full SMART dataset and NOAA AR dataset of some of the top features

SOLA: sola_KD.tex; 7 December 2018; 1:29; p. 13

Domijan et al.

Figure 4. MF detections in the (a) one training dataset and (b) testing dataset,
dimensions (3 features with the highest marginal relevance), red = ﬂare, black = no ﬂare.

in 3

selected by MR, Lasso and RF. Large peaks close to 0 in the non-ﬂare distribu-
tion show that the full SMART dataset is dominated by the small magnetic ﬂux
regions that do possess many of the properties that SMART calculates and never
ﬂare. All the displayed features contain information on whether an observation
is likely to be a ﬂare, however, all contain a signiﬁcant amount of overlap in
the distributions of ﬂaring and non-ﬂaring regions. Schrijver’s R and Falconer’s
W LSG values show a clear separation between two clusters, one of which has a
very low proportion of ﬂares, whereas the density plots for most other features
indicate a steady increase in the proportion of ﬂares, which gives an insight into
why splitting on these variables would lead to a decrease in the Gini index.

Figure 4 shows one subsampled training set and test set of the full dataset
of all SMART detections in three dimensions corresponding to features with the
highest marginal relevance. The detections are colored by their class (ﬂare/non-
ﬂare). In the test set, a large proportion of detections is located around 0, but due
to over-plotting, it is harder to see how these observations dominate the dataset
compared to density plots. These plots show that whereas the features contain
information about classes, there in an overlap between them in this feature space
- the classes are not perfectly separable. The shape of the classiﬁcation boundary
will not change this and the scope for improvement when using more complex
algorithms in this feature space is limited.

SOLA: sola_KD.tex; 7 December 2018; 1:29; p. 14

Solar ﬂare forecasting from MF properties generated by SMART

5. Discussion

In order to classify MF detections we used a number of classiﬁcation approaches
including binary logistic regression, SVMs, RFs and a set of DNN architectures.
Categorical forecasts were obtained by thresholding the estimated probability
from these models. Skill scores, curves of TSS, ROC curves and area under the
ROC were used to compare the performance of the classiﬁcation approaches.
We discussed the choice of skill scores and optimal thresholds for various model
settings.

The ﬂare prediction results that we obtained from the linear classiﬁer with
a very sparse subset of features compare favorably to those found elsewhere in
the literature and show a signiﬁcant improvement on the results of the previous
analysis of this data. We found that, in terms of classiﬁcation performance,
there was no beneﬁt in using more features or more ﬂexible models that allow
for nonlinear classiﬁcation boundaries as all approaches converged to the same
result. Furthermore, we found no decrease in performance when training the
algorithms on very small subsampled training sets.

By plotting the data in the selected dimensions we see that the classes are
not perfectly separable in the space of SMART features and that there is limited
scope for improvement in using more complex algorithms on this dataset.

A better performance, however, might be obtained by using the deep learn-
ing networks to learn the forecasting patterns directly from magnetograms of
solar active regions as opposed to using the features computed from the mag-
netograms. Some work on DNNs for solar ﬂare prediction has been done by
Nishizuka et al. (2018) and Huang et al. (2018).

Direct comparisons with other published methods are diﬃcult because of
diﬀerences in data sets, the deﬁnition of an event, and evaluation and reporting
of classiﬁcation results Barnes et al. (2016). It would be of interest to carry
out a comparative study of classiﬁcation algorithms, such as presented here,
to the Space-weather HMI Active Region Patch (SHARP) (Bobra et al., 2014)
data from the Solar Dynamics Observatory’s(SDO) Helioseismic and Magnetic
Imager (HMI). The data have previously been analysed by Bobra, and Couvidat
(2015) and Liu et al. (2017a) who used SVMs and Random Forests.

The work presented in this paper is fully reproducible with code for variable

selection, sub-sampling and classiﬁcation available via GitHub.

Appendix

References

Abramenko, V.I.: 2005, Relationship between magnetic power spectrum and ﬂare productivity

in solar active regions, Astrophys. J.,629, 1141.

Ahmed, O.W., Qahwaji, R., Colak, T., Higgins, P.A., Gallagher, P.T., Bloomﬁeld, D.S.: 2011,
Solar ﬂare prediction using advanced feature extraction, machine learning, and feature
selection. Solar Phys. 283, 1, 157.

Al-Ghraibah, A., Boucheron, L.E., McAteer, R.T.J.: 2015, An automated classiﬁcation ap-
proach to ranking photospheric proxies of magnetic energy build-up. Astron. Astrophys.
579, A64.

SOLA: sola_KD.tex; 7 December 2018; 1:29; p. 15

Domijan et al.

Figure 5. The marginal densities of the top features in the full SMART dataset selected by
MR, Lasso and RF algorithms.

Allaire, J., Chollet, F.: 2017, keras: R interface to ‘keras’. R package version 2.1.5.9002. https:

//keras.rstudio.com.

Barnes, G., Leka, K.D.: 2008, Evaluating the performance of solar ﬂare forecasting methods.

Astrophys. J. Lett. 688, L107.

Barnes, G., Leka, K.D., Schrijver, C.J., Colak, T., Qahwaji, R., Ashamari, O.W., Yuan, Y.,
Zhang, J., McAteer, R.T.J., Bloomﬁeld, D.S., Higgins, P.A., Gallagher, P.T., Falconer, D.A.,
Georgoulis, M.K., Wheatland, M.S., Balch, C., Dunn, T., Wagner, E.L.: 2016, A comparison
of ﬂare forecasting methods. i. results from the all-clear workshop. Astrophys. J., 829, 2,
89.

SOLA: sola_KD.tex; 7 December 2018; 1:29; p. 16

024602468log(Lsg_Mm + 1)densityflare01012302468log(Lnl_Mm + 1)densityflare010.000.010.020.03050010001500MxGrad_GpMmdensityflare010.00.10.20.30.40.50102030log(Rval_Mx + 1)densityflare010.00.20.44045505560log(Bflux_Mx)densityflare010.00.51.01.52.00510log(WLsg_GpMm + 1)densityflare01Solar ﬂare forecasting from MF properties generated by SMART

Figure 6. The marginal densities of the top features in the NOAA AR dataset selected by
MR, Lasso and RF algorithms.

Bloomﬁeld, D.S., Higgins, P.A., McAteer, R.T.J., Gallagher, P.T.: 2012, Toward reliable

benchmarking of solar ﬂare forecasting methods. Astrophys. J. Lett. 747.

Bobra, M.G., Couvidat, S.: 2015, Solar ﬂare prediction using SDO/HMI vector magnetic ﬁeld

data with a machine-learning algorithm. Astrophys. J., 798, 2, 135.

Bobra, M.G., Sun, X., Hoeksema, J.T., Turmon, M., Liu, Y., Hayashi, K., Barnes, G., Leka,
K.D.: 2014, The Helioseismic and Magnetic Imager (HMI) Vector Magnetic Field Pipeline:
SHARPs – Space-Weather HMI Active Region Patches. Solar Phys. 289, 9, 3549.

Boucheron, L.E., Al-Ghraibah, A., McAteer, R.T.J.: 2015, Prediction of solar ﬂare size and

time-to-ﬂare using support vector machine regression. Astrophys. J., 812, 1, 51.

SOLA: sola_KD.tex; 7 December 2018; 1:29; p. 17

0.000.250.500.750246log(Lsg_Mm + 1)densityflare010.00.10.20.30.40.502468log(Lnl_Mm + 1)densityflare010.0000.0010.0020.0030500100015002000MxGrad_GpMmdensityflare010.000.050.100.150.200.250102030log(Rval_Mx + 1)densityflare010.00.10.20.30.40.545485154log(Bflux_Mx)densityflare010.00.10.20.30.02.55.07.510.012.5log(WLsg_GpMm + 1)densityflare01Domijan et al.

Breiman, L.: 2001, Random forests. Machine Learning 45, 1, 5.
Chawla, N.V., Japkowicz, N., Kotcz, A.: 2004, Editorial: Special

imbalanced data sets. SIGKDD Explor. Newsl. 6, 1, 1.

issue on learning from

Colak, T., Qahwaji, R.: 2008, Automated McIntosh-based classiﬁcation of sunspot groups using

MDI images. Solar Phys. 248, 277.

Colak, T., Qahwaji, R.: 2009, Automated solar activity prediction: A hybrid computer platform
using machine learning and solar imaging for automated prediction of solar ﬂares. Space
Weather 7, S06001.

Cox, D.R.: 1958, The regression analysis of binary sequences (with discussion). J Roy Stat Soc

B 20, 215.

Daei, F., Safari, H., Dadashi, N.: 2017, Complex network for solar active regions. Astrophys.

J., 845, 1, 36.

Domijan, K.: 2016, BKPC: Bayesian Kernel Projection Classiﬁer. R package version 1.0.

https://CRAN.R-project.org/package=BKPC.

Domijan, K., Wilson, S.P.: 2011, Bayesian kernel projections for classiﬁcation of high

dimensional data. Statistics and Computing 21, 2, 203.

Dudoit, S., Fridlyand, J., Speed, T.P.: 2002, Comparison of discrimination methods for the
classiﬁcation of tumors using gene expression data. Journal of the American Statistical
Association 97, 457, 77.

Fisher, R.A.: 1936, The use of multiple measurements in taxonomic problems. Annals of

Eugenics 7, 179.

Fithian, W., Hastie, T.: 2014, Local case-control sampling: eﬃcient subsampling in imbalanced

data sets. The Annals of Statistics 42, 5, 1693.

Friedman, J., Hastie, T., Tibshirani, R.: 2009, glmnet: Lasso and elastic-net regularized
generalized linear models. R package version 1.1-4. https://CRAN.R-project.org/package=
glmnet.

Georgoulis, M.K., Rust, D.M.: 2007, Quantitative forecasting of major solar ﬂares. Astrophys.

J. Lett. 661, 1, L109.

Géron, A.: 2018, Neural networks and deep learning, OâĂŹReilly Media, Inc, Sebastopol, CA,

USA.

Gheibi, A., Safari, H., Javaherian, M.: 2017, The solar ﬂare complex network. Astrophys. J.,

847, 2, 115.

Hanssen, A.W., Kuipers, W.J.A.: 1965, On the relationship between the frequency of rain
and various meteorological parameters: (with reference to the problem of objective fore-
casting), Koninkl. Nederlands Meterologisch Institut. Mededelingen en Verhandelingen ,81,
Staatsdrukerij, Netherlands.

Heidke, P.: 1926, Berechnung des erfolges und der gÃĳte der windstÃďrkevorhersagen im

sturmwarnungsdienst. Geograﬁska Annaler 8, 301.

Higgins, P.A., Gallagher, P.T., McAteer, R.T.J., Bloomﬁeld, D.S.: 2011, Solar magnetic feature

detection and tracking for space weather monitoring. Adv. Space Res. 47, 2105.

Huang, X., Wang, H., Xu, L., Liu, J., Li, R., Dai, X.: 2018, Deep learning based solar ﬂare

forecasting model. I. results for line-of-sight magnetograms. Astrophys. J., 856, 1, 7.

Leka, K.D., Barnes, G.: 2007, Photospheric magnetic ﬁeld properties of ﬂaring versus ﬂare-quiet

active regions. iv. a statistically signiﬁcant sample. Astrophys. J., 656, 2, 1173.

Liaw, A., Wiener, M.: 2002, Classiﬁcation and regression by randomforest. R News 2, 3, 18.

http://CRAN.R-project.org/doc/Rnews/.

Liu, C., Deng, N., Wang, J.T.L., Wang, H.: 2017a, Predicting solar ﬂares using SDO/HMI
vector magnetic data products and the random forest algorithm. Astrophys. J., 843, 2,
104.

Liu, J.-F., Li, F., Zhang, H.-P., Yu, D.-R.: 2017b, Short-term solar ﬂare prediction using

image-case-based reasoning. Research in Astronomy and Astrophysics 17, 11, 116.

Mason, J.P., Hoeksema, J.T.: 2010, Testing automated solar ﬂare forecasting with 13 years of

michelson doppler imager magnetograms. Astrophys. J., 723, 1, 634.

McAteer, R.T.J., Gallagher, P.T., Ireland, J.: 2005, Statistics of active region complexity: A

large-scale fractal dimension survey. Astrophys. J., 631, 628.

Meyer, D., Dimitriadou, E., Hornik, K., Weingessel, A., Leisch, F.: 2017, e1071: Misc functions
of the department of statistics, probability theory group (formerly: E1071), TU Wien. R
package version 1.6-8. https://CRAN.R-project.org/package=e1071.

Nelder, J.A., Wedderburn, R.W.M.: 1972, Generalized linear models. Journal of the Royal

Statistical Society, Series A, General 135, 370.

SOLA: sola_KD.tex; 7 December 2018; 1:29; p. 18

Solar ﬂare forecasting from MF properties generated by SMART

Nishizuka, N., Sugiura, K., Kubo, Y., Den, M., Watari, S., Ishii, M.: 2017, Solar Flare Pre-
diction Model with Three Machine-learning Algorithms using Ultraviolet Brightening and
Vector Magnetograms. Astrophys. J., 835, 2, 156.

Nishizuka, N., Sugiura, K., Kubo, Y., Den, M., Ishii, M.: 2018, Deep ﬂare net (DeFN) model

for solar ﬂare prediction. Astrophys. J., 858, 2, 113.

Qahwaji, R., Colak, T., Al-Omari, M., Ipson, S.: 2008, Automated prediction of CMEs using

machine learning of CME - ﬂare associations. Solar Phys. 248, 471.

R Core Team: 2017, R: A language and environment for statistical computing. R Foundation

for Statistical Computing, Vienna, Austria. https://www.R-project.org/.

Raboonik, A., Safari, H., Alipour, N., Wheatland, M.S.: 2017, Prediction of solar ﬂares using

unique signatures of magnetic ﬁeld images. Astrophys. J., 834, 1, 11.

Schrijver, C.J.: 2007, A characteristic magnetic ﬁeld pattern associated with all major solar

ﬂares and its use in ﬂare forecasting. Astrophys. J. Lett. 655, 2, L117.

Soetaert, K.: 2017, plot3d: Plotting multi-dimensional data. R package version 1.1.1. https:

//CRAN.R-project.org/package=plot3D.

Tibshirani, R.: 1996, Regression shrinkage and selection via the Lasso. Journal of the Royal

Statistical Society B, 58, 267.

Vapnik, V.: 1998, Statistical learning theory, Wiley-Interscience, New York.
Wickham, H.: 2009, ggplot2: Elegant graphics for data analysis, Springer, New York. http:

//ggplot2.org.

Yang, X., Lin, G., Zhang, H., Mao, X.: 2013, Magnetic nonpotentiality in photospheric active

regions as a predictor of solar ﬂares. Astrophys. J. Lett. 774, 2, L27.

Youden, W.J.: 1950, Index for rating diagnostic tests. Cancer 3, 32.
Yu, D., Huang, X., Wang, H., Cui, Y., Hu, Q., Zhou, R.: 2010, Short-term solar ﬂare level

prediction using a bayesian network approach. Astrophys. J., 710, 1, 869.

SOLA: sola_KD.tex; 7 December 2018; 1:29; p. 19

M
R

M
R
(
R
)

R
F

R
F
(
R
)

L
a
s
s
o

L
a
s
s
o
(
R
)

F
e
a
t
u
r
e

D
e
s
c
r
i
p
t
i
o
n

1
3

1
2

1
1

1
0

9

8

7

6

5

4

3

2

1

1
3

1
2

1
1

1
0

4

8

9

7

6

5

2

3

1

−

−

−

−

3

−

−

2

6

4

1

−

5

−

−

−

−

4

−

−

2

−

3

1

6

5

−

−

−

−

−

−

−

−

1

−

1

−

1

−

−

−

−

1

−

−

−

−

−

1

−

1

A
r
e
a
M
m
s
q

A
r
e
a

o
f

t
h
e

r
e
g
i
o
n

W
L
s
g
G
p
M
m

F
a
l
c
o
n
e
r
’
s

W
L
S
G

v
a
l
u
e

B

R
v
a
l
M
x

S
c
h
r
i
j
v
e
r
’
s
R
v
a
l
u
e

L
a
r
g
e
s
t

m
a
g
n
e
t
i
c

ﬁ
e
l
d

v
a
l
u
e

D
B
ﬂ
u
x
D
t
M
x

F
l
u
x

e
m
e
r
g
e
n
c
e

r
a
t
e

H
G
l
o
n
w
d
t
h

H
e
l
i
o
g
r
a
p
h
i
c

l
o
n
g
i
t
u
d
i
n
a
l

e
x
t
e
n
t

B
ﬂ
u
x
i
m
b

M
e
d
n
G
r
a
d

H
G
l
a
t
w
d
t
h

B
ﬂ
u
x
M
x

H
e
l
i
o
g
r
a
p
h
i
c

l
a
t
i
t
u
d
i
n
a
l

e
x
t
e
n
t

T
o
t
a
l

u
n
-
s
i
g
n
e
d
m
a
g
n
e
t
i
c

ﬂ
u
x

F
l
u
x

i

m
b
a
l
a
n
c
e

f
r
a
c
t
i
o
n

i
n

t
h
e

r
e
g
i
o
n

M
e
d
i
a
n

g
r
a
d
i
e
n
t

a
l
o
n
g

t
h
e

n
e
u
t
r
a
l

l
i
n
e

L
n
l
M
m

N
e
u
t
r
a
l
-
l
i
n
e

l
e
n
g
t
h

i
n

t
h
e

r
e
g
i
o
n

L
s
g
M
m

H
i
g
h
-
g
r
a
d
i
e
n
t

n
e
u
t
r
a
l
-
l
i
n
e

l
e
n
g
t
h

i
n

t
h
e

r
e
g
i
o
n

M
x
G
r
a
d
G
p
M
m

M
a
x
i
m
u
m
g
r
a
d
i
e
n
t

a
l
o
n
g

p
o
l
a
r
i
t
y

i
n
v
e
r
s
i
o
n

l
i

n
e

Domijan et al.

s
p
a
r
s
e
s
t

m
o
d
e
l
s

ﬁ
t
t
e
d

t
o

t
h
e

t
r
a
i
n
i
n
g

s
e
t

o
f

t
h
e

f
u
l
l

a
n
d
N
O
A
A
A
R
(
R
)

d
a
t
a
s
e
t
s
.

t
h
e

t
r
a
i
n
i
n
g

s
e
t

o
f

t
h
e

f
u
l
l

a
n
d
N
O
A
A
A
R

(
R
)

d
a
t
a
s
e
t
s
.

C
o
l
u
m
n
s

L
a
s
s
o

a
n
d

L
a
s
s
o

(
R
)

p
r
e
s
e
n
t

t
h
e

f
e
a
t
u
r
e
s

s
e
l
e
c
t
e
d

i

n

t
h
e

f
o
u
r

d
e
t
e
c
t
i
o
n
s

o
n
l
y
.

C
o
l
u
m
n
s

R
F

a
n
d

R
F
(
R
)

p
r
e
s
e
n
t

t
h
e

v
a
r
i
a
b
l
e

i

m
p
o
r
t
a
n
c
e

o
r
d
e
r

o
b
t
a
i
n
e
d

f
r
o
m

r
u
n
n
i
n
g

t
h
e

R
a
n
d
o
m
F
o
r
e
s
t

o
n

a
l
l

S
M
A
R
T

d
e
t
e
c
t
i
o
n
s
.

T
h
e

s
e
c
o
n
d

c
o
l
u
m
n
M
R
(
R
)

i
s

t
h
e

M
R

f
e
a
t
u
r
e

r
a
n
k
i
n
g

o
b
t
a
i
n
e
d

f
r
o
m

t
h
e

t
r
a
i
n
i
n
g

s
e
t

o
f

t
h
e
N
O
A
A
A
R

T
a
b
l
e

4
.
:

S
M
A
R
T
m
a
g
n
e
t
i
c

f
e
a
t
u
r
e
s

i
n

o
r
d
e
r

o
f

t
h
e
i
r

m
a
r
g
i
n
a
l

r
e
l
e
v
a
n
c
e

o
b
t
a
i
n
e
d

f
r
o
m

t
h
e

t
r
a
i
n
i
n
g

d
a
t
a
s
e
t

f
o
r

t
h
e

d
a
t
a
s
e
t

o
f

SOLA: sola_KD.tex; 7 December 2018; 1:29; p. 20

Solar ﬂare forecasting from MF properties generated by SMART

Table 5. Results from the full SMART dataset for logistic regression 3 fea-
tures (LR): median values for TSS, TPR, TNR, ACC and HSS for the testing
set. 2.5th and 97.5th percentiles are given in brackets. The classiﬁcation rules
were obtained from 50 randomly sampled training subsets of 400 instances
each.

p

TSS

TPR TNR ACC HSS

0.01
0.02
0.03
0.04
0.05
0.06
0.07
0.08
0.09
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9

0.8
0.82
0.83
0.84
0.84
0.84
0.84
0.84
0.83
0.83
0.78
0.69
0.58
0.49
0.37
0.26
0.14
0.04

(0.79, 0.82)
(0.81, 0.83)
(0.82, 0.84)
(0.83, 0.84)
(0.83, 0.84)
(0.83, 0.84)
(0.83, 0.84)
(0.83, 0.84)
(0.83, 0.84)
(0.82, 0.84)
(0.74, 0.82)
(0.61, 0.77)
(0.5, 0.71)
(0.38, 0.65)
(0.26, 0.57)
(0.15, 0.48)
(0.06, 0.36)
(0, 0.19)

0.98
0.97
0.97
0.96
0.95
0.94
0.93
0.92
0.92
0.91
0.83
0.72
0.6
0.5
0.38
0.26
0.14
0.04

0.82
0.85
0.87
0.88
0.89
0.9
0.91
0.91
0.92
0.92
0.95
0.97
0.98
0.99
0.99
1
1
1

0.83
0.86
0.87
0.88
0.89
0.9
0.91
0.91
0.92
0.92
0.95
0.96
0.96
0.96
0.96
0.96
0.95
0.95

0.34
0.38
0.41
0.43
0.46
0.47
0.49
0.5
0.52
0.53
0.61
0.63
0.61
0.57
0.49
0.38
0.24
0.07

(0.31, 0.37)
(0.35, 0.42)
(0.38, 0.45)
(0.4, 0.47)
(0.41, 0.49)
(0.43, 0.51)
(0.45, 0.52)
(0.46, 0.54)
(0.48, 0.55)
(0.49, 0.56)
(0.57, 0.64)
(0.6, 0.64)
(0.58, 0.64)
(0.5, 0.63)
(0.38, 0.61)
(0.24, 0.57)
(0.11, 0.48)
(0.01, 0.31)

Table 6. Results from the full SMART dataset
for logistic regression with 3 features trained on
the full training dataset: TSS, TPR, TNR, ACC
and HSS for the testing set.

p TSS TPR TNR ACC HSS

0.01
0.03
0.05
0.07
0.09
0.10
0.20
0.30
0.40
0.50
0.60
0.70
0.80
0.90

0.80
0.83
0.84
0.84
0.84
0.83
0.79
0.70
0.60
0.50
0.39
0.27
0.14
0.04

0.98
0.97
0.95
0.93
0.92
0.91
0.83
0.72
0.62
0.51
0.40
0.27
0.14
0.04

0.82
0.87
0.89
0.91
0.92
0.92
0.96
0.97
0.98
0.99
0.99
1.00
1.00
1.00

0.83
0.87
0.89
0.91
0.92
0.92
0.95
0.96
0.96
0.96
0.96
0.96
0.95
0.95

0.33
0.41
0.46
0.49
0.52
0.53
0.62
0.64
0.62
0.59
0.52
0.39
0.24
0.07

SOLA: sola_KD.tex; 7 December 2018; 1:29; p. 21

Domijan et al.

Table 7. Results from the NOAA AR dataset for logistic regression 3
features (LR): median values for TSS, TPR, TNR, ACC and HSS for the
testing set. 2.5th and 97.5th percentiles are given in brackets. The classi-
ﬁcation rules were obtained from 50 randomly sampled training subsets of
400 instances each.

p

TSS

TPR TNR ACC HSS

0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9

0.59
0.66
0.63
0.57
0.47
0.34
0.22
0.11
0.02

(0.57, 0.63)
(0.65, 0.66)
(0.6, 0.65)
(0.5, 0.61)
(0.37, 0.55)
(0.24, 0.47)
(0.12, 0.35)
(0.04, 0.24)
(0, 0.11)

0.93
0.86
0.75
0.64
0.51
0.37
0.23
0.11
0.02

0.65
0.8
0.88
0.93
0.96
0.98
0.99
1
1

0.71
0.81
0.85
0.87
0.87
0.85
0.84
0.82
0.8

0.4
0.53
0.58
0.58
0.53
0.43
0.31
0.16
0.04

(0.37, 0.45)
(0.5, 0.55)
(0.56, 0.59)
(0.55, 0.59)
(0.45, 0.58)
(0.32, 0.53)
(0.18, 0.44)
(0.07, 0.33)
(0, 0.16)

Table 8. TSS, TPR, TNR, ACC and HSS for
LR13 for the NOAA AR dataset.

p TSS TPR TNR ACC HSS

0.10
0.20
0.30
0.40
0.50
0.60
0.70
0.80
0.90

0.60
0.67
0.63
0.57
0.52
0.41
0.30
0.17
0.04

0.93
0.87
0.76
0.66
0.57
0.44
0.31
0.17
0.04

0.67
0.80
0.87
0.92
0.95
0.97
0.98
1.00
1.00

0.72
0.81
0.85
0.86
0.87
0.86
0.85
0.83
0.81

0.41
0.54
0.57
0.58
0.56
0.49
0.39
0.24
0.07

SOLA: sola_KD.tex; 7 December 2018; 1:29; p. 22

