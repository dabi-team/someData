VoteHMR: Occlusion-Aware Voting Network for Robust 3D
Human Mesh Recovery from Partial Point Clouds

Guanze Liu
College of Software, Beihang
University
Beijing, China
sy1921112@buaa.edu.cn

Yu Rong
The Chinese University of Hong Kong
Hong Kong SAR, China
rongyu9124@gmail.com

Lu Sheng∗
College of Software, Beihang
University
Beijing, China
lsheng@buaa.edu.cn

1
2
0
2

t
c
O
7
1

]

V
C
.
s
c
[

1
v
9
2
7
8
0
.
0
1
1
2
:
v
i
X
r
a

ABSTRACT
3D human mesh recovery from point clouds is essential for vari-
ous tasks, including AR/VR and human behavior understanding.
Previous works in this field either require high-quality 3D human
scans or sequential point clouds, which cannot be easily applied
to low-quality 3D scans captured by consumer-level depth sensors.
In this paper, we make the first attempt to reconstruct reliable 3D
human shapes from single-frame partial point clouds. To achieve
this, we propose an end-to-end learnable method, named VoteHMR.
The core of VoteHMR is a novel occlusion-aware voting network
that can first reliably produce visible joint-level features from the
input partial point clouds, and then complete the joint-level features
through the kinematic tree of the human skeleton. Compared with
holistic features used by previous works, the joint-level features
can not only effectively encode the human geometry information
but also be robust to noisy inputs with self-occlusions and miss-
ing areas. By exploiting the rich complementary clues from the
joint-level features and global features from the input point clouds,
the proposed method encourages reliable and disentangled param-
eter predictions for statistical 3D human models, such as SMPL.
The proposed method achieves state-of-the-art performances on
two large-scale datasets, namely SURREAL and DFAUST. Further-
more, VoteHMR also demonstrates superior generalization ability
on real-world datasets, such as Berkeley MHAD.

CCS CONCEPTS
• Computing methodologies → Point-based models; Shape
representations; Shape inference; Reconstruction.

KEYWORDS
3D human shape reconstruction, occlusion handling, hough voting
in point clouds

ACM Reference Format:
Guanze Liu, Yu Rong, and Lu Sheng. 2021. VoteHMR: Occlusion-Aware
Voting Network for Robust 3D Human Mesh Recovery from Partial Point

∗Lu Sheng is the corresponding author.

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
MM ’21, October 20–24, 2021, Virtual Event, China
© 2021 Association for Computing Machinery.
ACM ISBN 978-1-4503-8651-7/21/10. . . $15.00
https://doi.org/10.1145/3474085.3475309

Figure 1: Human mesh recovery from a single frame of par-
tial point clouds usually suffers from occluded human body
parts and noises. In our VoteHMR, we adopt a robust voting
network to reliably cluster visible joint-level features, show-
ing impressive reconstruction onto challenging samples in
the Berkerly MHAD dataset [26].

Clouds. In Proceedings of the 29th ACM International Conference on Multi-
media (MM ’21), October 20–24, 2021, Virtual Event, China. ACM, New York,
NY, USA, 10 pages. https://doi.org/10.1145/3474085.3475309

1 INTRODUCTION
Recovering 3D human shapes is an appealing yet challenging task in
the research community of 3D computer vision and multimedia. It
can significantly benefit various downstream applications including
AR/VR [46], robotics [33, 34] and etc. This task aims at predicting
the personalized 3D human models of the captured targets into non-
parametric meshes [22] or parametric descriptions [10, 15, 17, 56]
of a statistical body model [1, 23, 28]. In this paper, we focus on
human mesh recovery from the point clouds captured by consumer-
level depth sensors, such as Kinect and ToF cameras. The irregular,
sparse and orderless nature of this special 3D input brings additional
challenges to designing a robust point-based reconstruction system,
which cannot be achieved by simply adopting the ideas of recent
success in RGB-based approaches [17, 22].

Although promising reconstruction results have been demon-
strated by early attempts that leverage a complete point cloud of
a human scan [15] or a sequence of point clouds [48], existing ap-
proaches are not yet able to reliably cope with a single frame of
partial point clouds that are affected with severe self-occlusions
and noises. Many applications (such as on mobile devices) may
not afford to generate a complete scan of the captured targets or

PointCloudSegmentVotePredict MeshRGB 
 
 
 
 
 
store a sequence of point clouds before the process of 3D human
shape reconstruction. Thus, it is meaningful to study robust 3D
human mesh recovery from a single frame of partial point clouds.
To achieve this, there are several key challenges that need to be
addressed: (1) Coping with complex self-occlusions and missing
parts for robust pose estimation, (2) Sustaining reasonable shape
reconstruction without the interference of noises.

In this paper, we propose an occlusion-aware voting network
for robust human shape reconstruction from a single frame of
partial point clouds, named VoteHMR, that tackles the aforemen-
tioned challenges using an end-to-end learnable framework. Our
VoteHMR is designed to regress the parameters for statistical hu-
man body models, such as SMPL [23], rather than directly learning
the regression of the 3D coordinates of a high-dimensional human
mesh. This design can eliminate the distortions on the generated
meshes when interfered with by the flaws of the input point clouds.
Moreover, inspired that the Hough voting [13] has been success-
fully applied for object detection on the point clouds [35], we adopt
a similar mechanism to gather sparse points from visible human
parts for discovering observed skeleton joints and their shape rep-
resentations. These occlusion-aware joint-level features are then
completed by leveraging the kinematic tree of human skeletons.
Compared with holistic features [17, 20], these joint-level features
not only explicitly encode the human poses but also implicitly ex-
tract the local geometric contexts in each body part, thus benefit
for comprehensive but disentangled predictions of the shape and
pose of the captured targets.

To be specific, in our VoteHMR, we first propose a partial voting
generation and clustering module to gather enough reliable points
to discover the joint-level shape representations around the visible
3D skeleton joints. After that, an occlusion-aware joint completion
module is leveraged to complete the joint-level representations
throughout the kinematic tree. Thereafter, we propose two parallel
global parameter regression and local parameter regression modules
to disentangle the prediction of shape and pose parameters for the
SMPL model. The whole network is fully supervised by synthetic
datasets such as SURREAL [43] and DFAUST [4], where the ground-
truth (GT) human meshes, 3D coordinates of skeleton joints, and
body part segmentation labels can be generated for each input point
cloud. It is worth noting that our method can be readily tested on
real point clouds captured by consumer-level depth sensors with
superior reconstruction performances than the existing methods [9,
15, 48]. To demonstrate this, we evaluate the trained models on
Berkeley MHAD dataset [26]. Several results are shown in Figure 1.
It is revealed that VoteHMR can reliably vote the visible joints
with their associated body part segments, and then recover the
human meshes with diverse poses from raw point clouds captured
by Kinect. Note that VoteHMR is just tested on this dataset, which
validates that the proposed method is robust and generalizable to
real-world scenarios.

In summary, the contributions of this paper are threefold: (1) The
first method, named as VoteHMR, that successfully apply Hough
voting to achieve parametric human shape reconstruction from a
single frame of partial point clouds. (2) Certain robustness over
complex occlusions, noises, and sparsity that frequently occur
in the point clouds. (3) The state-of-the-art human mesh recov-
ery performances on two benchmark datasets, SURREAL [43] and

DFAUST [4], with additional nice generalization towards real-world
datasets such as Berkeley MHAD [26].

2 RELATED WORKS
2.1 Human Mesh Recovery from 2D Images
Human mesh recovery has been extensively studied from RGB
images, either by template-based approaches to fit the 2D annota-
tions of the RGB images [18, 21, 29–31, 39, 40, 42, 45, 47], such as
keypoints [3], silhouettes [31], and dense annotations [11, 38], or
template-less approaches [30, 31, 41, 58, 59] that directly regress
the 3D coordinates of vertices of the human meshes. The template-
based methods often make use of human priors such as skeletons
or parametric models like SCAPE [1] and SMPL [23], thus can
alleviate the twisted and rugged human shapes that may occur
by template-less methods. Recent works also try to process depth
data [2, 9, 15, 30, 48, 50] for human mesh recovery, in which the
existing methods can also be divided into template-based [2, 52, 53]
and template-less methods [6, 7, 14, 24], in which the template-less
methods create the 3D human meshes without any prior knowledge
about the body shape and usually volumetrically fuse all captured
depth maps to reconstruct 3D models. Some works [54, 55, 57] try
to combine template-priors and template-less methods for more
robust mesh recovery with large human motions.

2.2 Human Mesh Recovery from Point Clouds
Recent works have also tried to recover 3D human meshes from
point clouds. These works often utilize a pointnet [36] or point-
net++ [37] backbone to extract features from orderless point clouds.
Jiang et al [15] propose a skeleton-aware network to predict hu-
man mesh from complete human point cloud scans but failed to
generalize well to partial human scans. Groueix et al [9] deformed
a template model to a given point cloud but often produce rugged
human mesh. Wang et al [48] used a coarse-to-fine spatial-temporal
mesh attention convolution module to predict human meshes from
sequences of point clouds. In their framework, temporal dependen-
cies are vital to rectify the flaws caused by partial scans. Few works
have been proposed to tackle occlusions from a single frame of
point clouds. Since existing methods are usually trained on syn-
thetic datasets like [4, 43], these methods [9, 15, 48] usually fail to
generalize well to real scenarios and require a weakly supervised
fine-tuning scheme. Unlike these methods, We provide an occlusion-
aware voting-based human mesh recovery network that proves to
be robust against noise and occlusions from a single frame of partial
point clouds. We train our VoteHMR on synthetic datasets [4, 43],
but it can generalize well to real-world scenarios.

2.3 Voting Methods in Deep Learning
Traditional Hough voting [13] translates the problem of detecting
patterns to detecting peaks in the parametric space. A similar voting
method is combined with deep learning and widely used in the
field of object detection [5, 35, 51] and instance segmentation [16]
from point clouds. VoteNet [35] was the first method to combine
traditional hough voting with deep learning methods, which utilized
an MLP module to simulate voting procedure and predict offset
coordinates and features for each seed point. The vote features and
coordinates are grouped and aggregated to generate box proposals.

Figure 2: Overview of the proposed VoteHMR method. Best viewed on screen.

The voting method can also be used in 6-DOF pose estimation.
PVN [32] predicted the unit vector field for a given image and
PVN3D [12] predicted the keypoints offsets and instance semantic
segmentation label for each pixel on a given RGBD image. A similar
strategy was also used for hand pose estimation from point clouds,
Ge et al [8] used a point-to-point regression method to predict the
heatmap and unit vector field for each input hand point, which
utilizes a similar strategy as voting. To our best knowledge, we are
the first attempt to use a voting module in the field of human mesh
recovery, and we tailor this module to solve complex occlusions
and noises when dealing with human mesh recovery in a single
frame of partial point clouds.

3 METHODOLOGY
3.1 Preliminaries
3D Paramteric Human Model. The Skinned Multi-Person Lin-
3.1.1
ear Model (SMPL) [23] is used in this paper to represent 3D human
bodies. It includes two sets of parameters, i.e., the shape parameters
𝜷 ∈ R10 for controlling height, weight, limb proportions, and the
pose parameters 𝜽 ∈ R3𝐾 denotes the axis-angle representations
of the relative 3D rotations of 𝐾 skeleton joints with respect to
their parents in the kinematic tree. To be specific, 𝐾 = 23 + 1, in-
cluding one additional root joint indicates the global rotation of
the human body model. Given the pose and shape parameters, the
SMPL model deforms a triangulated mesh template and generates a
new mesh with 𝑀 = 6890 vertices, i.e., M (𝜽, 𝜷). The deformation
process M (𝜽, 𝜷) is differentiable w.r.t. 𝜽 and 𝜷, benefiting the inte-
gration of the SMPL model into a neural network with end-to-end
training. The SMPL model also provides 𝐾 body part labels associ-
ated with the skeleton joints, thus each vertex has a deterministic
correspondence with its belonging body part.

3.1.2 Deep Hough Voting for 3D Object Detection. VoteNet [35]
has shown great success in designing end-to-end 3D object detec-
tion networks for raw point clouds. It reformulates the traditional
Hough voting into a point-wise deep regression problem through
shared MLPs. VoteNet generates an object proposal by sampling the
seed points from the input point cloud whose votes are within the
same cluster. It is more compatible with sparse point sets than re-
gion proposals and encourages proposal generation within adaptive
receptive areas. As it accumulates partial information to form con-
fident detection, a voting-based approach would benefit skeleton
discovery even with severe noises, missed areas, and occlusions.

3.2 Overview
As illustrated in Figure 2, the input of our VoteHMR is a point
cloud P ∈ R𝑁 ×3, with a 3D coordinate for each of the 𝑁 points.
Such an input typically comes from depth sensors that scan partial
views of the 3D human surfaces. The outputs are two sets of SMPL
parameters, one set includes the global parameters 𝝓𝑔 = {𝜷, 𝜽 0},
and the other set has the local parameters 𝝓𝑙 = {𝜽 1, . . . , 𝜽 𝐾−1}.
The former controls the body shape and the global rotation, and
the latter indicates the relative rotations of the articulated skele-
ton joints. These parameters deform a 3D human shape model
M (𝝓𝑔, 𝝓𝑙 ) (mimicking the SMPL deformation process M (Θ, 𝜷)) as
a personalized triangulated mesh. VoteHMR consists of four main
modules: (1) a partial vote generation and clustering module, (2)
an occlusion-aware joint completion module, (3) a local parameter
regressor, and (4) a global parameter regressor. We will elaborate
on these four modules in the following parts.

3.3 Partial Vote Generation and Clustering
From an input point cloud P ∈ R𝑁 ×3, we aim at generating 𝑁 votes,
where each vote has a body part segmentation score vector s𝑖 ∈ R𝐾 ,
a 3D coordinate offset o𝑖 ∈ R3 towards its nearest skeleton joint,
and a high dimensional feature vector f𝑖 ∈ R𝐹1 . These votes within
the same segmentation results are then grouped into joint-level
vote clusters, representing the skeleton joint of the corresponding
segment. There are three major steps: (1) point cloud feature learn-
ing through a backbone network, (2) simultaneously predicting
three components of each vote, and (3) clustering votes for skeleton
joint-level feature association. Note that the input point clouds may
only contain the partial human body scans, thus the output joint
features may also partially fill the kinematic tree.

3.3.1 Point Cloud Feature Learning. We leverage PointNet++ [37]
as our feature extraction backbone due to its widespread success on
point cloud analysis. Following 2D pose estimation [25], this back-
bone network has multiple set abstraction (downsampling) layers
and feature propagation (upsampling) layers with skip connections,
thus encourage more contextual clues to enrich the geometric fea-
tures. It outputs the 3D point coordinate x𝑖 and a 𝐹1-dimensional
feature vector b𝑖 for each input point p𝑖, 𝑖 = 1, . . . , 𝑁 , while each
point generates one vote.

3.3.2 Partial Vote Generation. We apply a shared voting module
network to generate the vote {s𝑖, o𝑖, f𝑖 } from each input base feature
b𝑖, 𝑖 = 1, . . . , 𝑁 independently. To be specific, the voting module

InputPoint CloudVoteClusteringMLPSharedGlobal Param Regression<latexit sha1_base64="+kMUnRUdwGyquZbjIhxbl2BEASM=">AAAB73icbVA9SwNBEJ3zM8avqKXNYRCswl0EtTNgYyUR8gXJEfY2e8mSvb1zd04IR36CFjYWitja+0vsbPwfdm4+Ck18MPB4b4Y3M34suEbH+bQWFpeWV1Yza9n1jc2t7dzObk1HiaKsSiMRqYZPNBNcsipyFKwRK0ZCX7C6378Y+fVbpjSPZAUHMfNC0pU84JSgkRpXLeQh08ftXN4pOGPY88Sdkvz5d3z/VXm/K7dzH61ORJOQSaSCaN10nRi9lCjkVLBhtpVoFhPaJ13WNFQSk+Kl432H9qFROnYQKVMS7bH6eyIlodaD0DedIcGenvVG4n9eM8HgzEu5jBNkkk6CgkTYGNmj4+0OV4yiGBhCqOJmV5v2iCIUzYuy5gnu7MnzpFYsuCcF99rJl4owQQb24QCOwIVTKMEllKEKFAQ8wBM8WzfWo/VivU5aF6zpzB78gfX2AwmXlGQ=</latexit>N⇥3<latexit sha1_base64="4nAn2VXDVQkuT1bGblqS2Y8xxfI=">AAAB8nicbVA9SwNBEN2LGmP8ilraLAbBKtylUMuAIFaSgPmASwh7m71kyd7usTsnhCM/wkILC0UsbPwllnbin3HzUWjig4HHezPMzAtiwQ247peTWVldy67nNvKbW9s7u4W9/YZRiaasTpVQuhUQwwSXrA4cBGvFmpEoEKwZDC8mfvOWacOVvIFRzDoR6UseckrASv51G3jEDL7set1C0S25U+Bl4s1JsZKtfX883L1Vu4XPdk/RJGISqCDG+J4bQyclGjgVbJxvJ4bFhA5Jn/mWSmIXddLpyWN8bJUeDpW2JQFP1d8TKYmMGUWB7YwIDMyiNxH/8/wEwvNOymWcAJN0tihMBAaFJ//jHteMghhZQqjm9lZMB0QTCjalvA3BW3x5mTTKJe+05NVsGmU0Qw4doiN0gjx0hiroClVRHVGk0D16Qs8OOI/Oi/M6a80485kD9AfO+w/Q9pSm</latexit>N⇥F1<latexit sha1_base64="OHhX+FdDcH/Q0VFcrBpm6GQBBlY=">AAAB83icbVDJSgNBEK1xiTFuUY9eGoPgKczkoB4DgnhMwCyQGUJPpydp0rPQXSOEIR/hJRcPigie/BGP3sSfsbMcNPFBweO9Kqrq+YkUGm37y1pb39jMbeW3Czu7e/sHxcOjpo5TxXiDxTJWbZ9qLkXEGyhQ8naiOA19yVv+8Hrqt+650iKO7nCUcC+k/UgEglE0kusQF0XINbnpVrrFkl22ZyCrxFmQUjVX//6YPLzVusVPtxezNOQRMkm17jh2gl5GFQom+bjgpponlA1pn3cMjahZ5GWzm8fkzCg9EsTKVIRkpv6eyGio9Sj0TWdIcaCXvan4n9dJMbjyMhElKfKIzRcFqSQYk2kApCcUZyhHhlCmhLmVsAFVlKGJqWBCcJZfXiXNStm5KDt1k0YF5sjDCZzCOThwCVW4hRo0gEECE3iCZyu1Hq0X63XeumYtZo7hD6z3H/xNlLQ=</latexit>1⇥F2<latexit sha1_base64="4nAn2VXDVQkuT1bGblqS2Y8xxfI=">AAAB8nicbVA9SwNBEN2LGmP8ilraLAbBKtylUMuAIFaSgPmASwh7m71kyd7usTsnhCM/wkILC0UsbPwllnbin3HzUWjig4HHezPMzAtiwQ247peTWVldy67nNvKbW9s7u4W9/YZRiaasTpVQuhUQwwSXrA4cBGvFmpEoEKwZDC8mfvOWacOVvIFRzDoR6UseckrASv51G3jEDL7set1C0S25U+Bl4s1JsZKtfX883L1Vu4XPdk/RJGISqCDG+J4bQyclGjgVbJxvJ4bFhA5Jn/mWSmIXddLpyWN8bJUeDpW2JQFP1d8TKYmMGUWB7YwIDMyiNxH/8/wEwvNOymWcAJN0tihMBAaFJ//jHteMghhZQqjm9lZMB0QTCjalvA3BW3x5mTTKJe+05NVsGmU0Qw4doiN0gjx0hiroClVRHVGk0D16Qs8OOI/Oi/M6a80485kD9AfO+w/Q9pSm</latexit>N⇥F1MLPMLP<latexit sha1_base64="8uX8RpieG8u6EtEtG271zInOd78=">AAAB8XicbVA9SwNBEJ3zM8avqKXNYhCswl0KtTNgIwgSIV+YHGFvs0mW7O0du3NCCPkJWtlYKGJr6y+xs/F/2Ln5KDTxwcDjvRnezASxFAZd99NZWFxaXllNraXXNza3tjM7uxUTJZrxMotkpGsBNVwKxcsoUPJarDkNA8mrQe985FdvuTYiUiXsx9wPaUeJtmAUrXRzRRooQm7IZTOTdXPuGGSeeFOSPfuO779K73fFZuaj0YpYEnKFTFJj6p4boz+gGgWTfJhuJIbHlPVoh9ctVdTG+IPxxkNyaJUWaUfalkIyVn9PDGhoTD8MbGdIsWtmvZH4n1dPsH3qD4SKE+SKTYLaiSQYkdH5pCU0Zyj7llCmhd2VsC7VlKF9Uto+wZs9eZ5U8jnvOOddu9lCHiZIwT4cwBF4cAIFuIAilIGBggd4gmfHOI/Oi/M6aV1wpjN78AfO2w/dF5TQ</latexit>N⇥K<latexit sha1_base64="+kMUnRUdwGyquZbjIhxbl2BEASM=">AAAB73icbVA9SwNBEJ3zM8avqKXNYRCswl0EtTNgYyUR8gXJEfY2e8mSvb1zd04IR36CFjYWitja+0vsbPwfdm4+Ck18MPB4b4Y3M34suEbH+bQWFpeWV1Yza9n1jc2t7dzObk1HiaKsSiMRqYZPNBNcsipyFKwRK0ZCX7C6378Y+fVbpjSPZAUHMfNC0pU84JSgkRpXLeQh08ftXN4pOGPY88Sdkvz5d3z/VXm/K7dzH61ORJOQSaSCaN10nRi9lCjkVLBhtpVoFhPaJ13WNFQSk+Kl432H9qFROnYQKVMS7bH6eyIlodaD0DedIcGenvVG4n9eM8HgzEu5jBNkkk6CgkTYGNmj4+0OV4yiGBhCqOJmV5v2iCIUzYuy5gnu7MnzpFYsuCcF99rJl4owQQb24QCOwIVTKMEllKEKFAQ8wBM8WzfWo/VivU5aF6zpzB78gfX2AwmXlGQ=</latexit>N⇥3<latexit sha1_base64="+dyFIoChYYYgsimUOepah9ffVAQ=">AAAB8nicbVA9SwNBEN2LGmP8ilraLAbBKtylUMuAIIJNAuYDLiHsbfaSJXu7x+6cEI78CAstLBSxsPGXWNqJf8bNR6GJDwYe780wMy+IBTfgul9OZmV1Lbue28hvbm3v7Bb29htGJZqyOlVC6VZADBNcsjpwEKwVa0aiQLBmMLyY+M1bpg1X8gZGMetEpC95yCkBK/nXbeARM/iyW+4Wim7JnQIvE29OipVs7fvj4e6t2i18tnuKJhGTQAUxxvfcGDop0cCpYON8OzEsJnRI+sy3VBK7qJNOTx7jY6v0cKi0LQl4qv6eSElkzCgKbGdEYGAWvYn4n+cnEJ53Ui7jBJiks0VhIjAoPPkf97hmFMTIEkI1t7diOiCaULAp5W0I3uLLy6RRLnmnJa9m0yijGXLoEB2hE+ShM1RBV6iK6ogihe7RE3p2wHl0XpzXWWvGmc8coD9w3n8AzdCUpA==</latexit>K⇥F2Occlusion-aware Joint Completion<latexit sha1_base64="+dyFIoChYYYgsimUOepah9ffVAQ=">AAAB8nicbVA9SwNBEN2LGmP8ilraLAbBKtylUMuAIIJNAuYDLiHsbfaSJXu7x+6cEI78CAstLBSxsPGXWNqJf8bNR6GJDwYe780wMy+IBTfgul9OZmV1Lbue28hvbm3v7Bb29htGJZqyOlVC6VZADBNcsjpwEKwVa0aiQLBmMLyY+M1bpg1X8gZGMetEpC95yCkBK/nXbeARM/iyW+4Wim7JnQIvE29OipVs7fvj4e6t2i18tnuKJhGTQAUxxvfcGDop0cCpYON8OzEsJnRI+sy3VBK7qJNOTx7jY6v0cKi0LQl4qv6eSElkzCgKbGdEYGAWvYn4n+cnEJ53Ui7jBJiks0VhIjAoPPkf97hmFMTIEkI1t7diOiCaULAp5W0I3uLLy6RRLnmnJa9m0yijGXLoEB2hE+ShM1RBV6iK6ogihe7RE3p2wHl0XpzXWWvGmc8coD9w3n8AzdCUpA==</latexit>K⇥F2Local Param Regression<latexit sha1_base64="s8BRtpJDAiHcdIcyjO6grz+U778=">AAAB+nicbVA7T8MwGHR4lvJKgY3FokJiqpIOwFiJhbFI9CG1UeQ4TmPVsSPbAVWhP4WFAYQYgT/Cxl9gZ8dpO0DLSZZPd98nny9IGVXacT6tpeWV1bX10kZ5c2t7Z9eu7LWVyCQmLSyYkN0AKcIoJy1NNSPdVBKUBIx0guFF4XduiFRU8Gs9SomXoAGnEcVIG8m3K/1AsFCNEnP105j6A9+uOjVnArhI3BmpNg6+39XXa9z07Y9+KHCWEK4xQ0r1XCfVXo6kppiRcbmfKZIiPEQD0jOUo4QoL59EH8Njo4QwEtIcruFE/b2Ro0QV6cxkgnSs5r1C/M/rZTo693LK00wTjqcPRRmDWsCiBxhSSbBmI0MQltRkhThGEmFt2iqbEtz5Ly+Sdr3mntbcK9NGHUxRAofgCJwAF5yBBrgETdACGNyCe/AInqw768F6tl6mo0vWbGcf/IH19gO4Dpiz</latexit> g<latexit sha1_base64="PjpntYJJRKbx5gAzT92WJEmfB38=">AAAB+nicbVC9TsMwGHT4LaVACiMSsqiQmKqkAzBWYmFsJfojNVHlOE5r1bEj2wFVoSOPwcIAQqwMfQ42noGXwGk7QMtJlk933yefL0gYVdpxvqy19Y3Nre3CTnG3tLd/YJcP20qkEpMWFkzIboAUYZSTlqaakW4iCYoDRjrB6Dr3O3dEKir4rR4nxI/RgNOIYqSN1LfLXiBYqMaxubxkSPusb1ecqjMDXCXuglTqpWnz+/Fk2ujbn14ocBoTrjFDSvVcJ9F+hqSmmJFJ0UsVSRAeoQHpGcpRTJSfzaJP4JlRQhgJaQ7XcKb+3shQrPJ0ZjJGeqiWvVz8z+ulOrryM8qTVBOO5w9FKYNawLwHGFJJsGZjQxCW1GSFeIgkwtq0VTQluMtfXiXtWtW9qLpN00YNzFEAx+AUnAMXXII6uAEN0AIY3IMn8AJerQfr2Xqz3ueja9Zi5wj8gfXxA5j0l90=</latexit> lSMPL<latexit sha1_base64="eZUd79PLw3nnb6JWs3Lgt6AyStE=">AAAB8HicbVC7SgNBFJ31GeMramkzGASrsBtB7QzYCDYR8pJkCbOT2WTIzOwyc1cISz5BGxsLRWyt/RI7G//Dzsmj0MQDFw7n3Mu59wax4AZc99NZWFxaXlnNrGXXNza3tnM7uzUTJZqyKo1EpBsBMUxwxarAQbBGrBmRgWD1oH8x8uu3TBseqQoMYuZL0lU85JSAlW6uWsAlM/i4ncu7BXcMPE+8Kcmff8f3X5X3u3I799HqRDSRTAEVxJim58bgp0QDp4INs63EsJjQPumypqWK2Bg/HS88xIdW6eAw0rYU4LH6eyIl0piBDGynJNAzs95I/M9rJhCe+SlXcQJM0UlQmAgMER5djztcMwpiYAmhmttdMe0RTSjYH2XtE7zZk+dJrVjwTgretZsvFdEEGbSPDtAR8tApKqFLVEZVRJFED+gJPTvaeXRenNdJ64IzndlDf+C8/QBdUZSL</latexit>K⇥3Complete JointsPredicted MeshPartial Vote Generation and Clusteringconsists of a multi-layer perceptron (MLP) network, which is fol-
lowed by three independent heads for body part segmentation,
offset regression, and feature updating, respectively.

The segmentation head employs a fully connected (FC) layer
with softmax after the voting module to predict the body part
segmentation score vector s𝑖 ∈ R𝐾 for each input point p𝑖 . Our
method employs the cross-entropy loss Lseg = − (cid:205)𝑁
in-
dependently for each point to train the segmentation head. 𝑡𝑖 is
𝑡𝑖
𝑖 means
the ground truth (GT) body part label for the point p𝑖 , s
the 𝑡𝑖 -th entry of the score vector s𝑖 . The GT segmentation label
of each point is annotated by copying the body part label from the
nearest vertex from the ground truth human shape model.

𝑡𝑖
𝑖=1 log s
𝑖

The offset regression head also employ an FC layer to output
the Euclidean offsets o𝑖 ∈ R3 for each input point p𝑖 . It aims to
generate a vote offset towards the skeleton joint that the input point
belongs to, so as to gather more valid contexts to reliably define
the joint positions and their geometric features. The predicted 3D
offset o𝑖 is explicitly supervised by a regression loss

Lvote-reg =

1
𝑁

𝑁
∑︁

𝐾
∑︁

𝑖

𝑘=0

∥o𝑖 − (c∗

𝑘 − p𝑖 )∥𝜌 · I[p𝑖 on body part 𝑘], (1)

where I[p𝑖 on body part 𝑘] indicates whether an input point p𝑖 is
on a body part 𝑘. c∗
𝑘 ∈ R3 is the GT 3D coordinates for the 𝑘-th
skeleton joint. 𝜌 (·) is the smooth-ℓ1 norm for robust regression.

The feature updating head uses a residual connection to up-
date the vote features as f𝑖 = b𝑖 + Δf𝑖, 𝑖 = 1, . . . , 𝑁 . Δf𝑖 is extracted
by the shared MLP network with another independent FC layer.
The feature updating head does not receive explicit supervision.

The partial vote generation has an overall vote generation loss

Lvote-gen = 𝜆11Lvote-reg + 𝜆12Lseg,

(2)

which is accompanied with subsequent losses for human mesh
recovery.

3.3.3 Vote Clustering. Given the predicted votes for all input points,
we are ready to group and aggregate them into predicted skeleton
joints and their associated features. To be specific, we have the
predicted position and feature for each joint as

c𝑘 =

1
(cid:205)𝑁
𝑖=1 s𝑘

𝑖

𝑁
∑︁

𝑖=1

𝑘
𝑖 , and q𝑘 =
(p𝑖 + o𝑖 )s

1
(cid:205)𝑁
𝑖=1 s𝑘

𝑖

𝑁
∑︁

𝑖=1

𝑘
𝑖 .
f𝑖 s

(3)

To be specific, we assume joints whose confidence score is below a
pre-defined threshold as occluded and thus set the corresponding
joint positions and features to be zero.

3.4 Occlusion-aware Joint Completion
The input point clouds captured by depth sensors contain self-
occlusions and missing areas, thus it is necessary to complete the
skeleton joints in the kinematic tree for reliable pose estimation
and global shape recovery. In this part, we use an occlusion-aware
joint completion module to fill in the missing joint features as well
as the joint positions from the previous stage.

At first, we reorganize the joint features and positions of all
skeleton joints as a tensor ˜J ∈ R𝐾×(3+𝐹1) . We first concatenate the
joint position and feature in the channel dimensions and then list
them in the order of the kinematic tree. Note that we set joints with

Figure 3: Global parameter regression module.

a confidence score below a pre-defined threshold as occluded and
thus set the corresponding joint positions and features to be zero,
as indicated in Section 3.3.3. The occlusion-aware joint completion
module infers the position and features for the occluded joints from
the visible ones. We use a two-layer MLP network to transform ˜J
into the completed joint feature ˜Q ∈ R𝐾×𝐹2 , and then an additional
FC layer for predicting refined joint positions ˜C ∈ R𝐾×3. The
completed 3D coordinates and features for all joints are fed into
the global and local parameter regressors for SMPL-based HMR.

3.5 Global Parameter Regression
The global parameter regression is to predict the shape parameter
𝜷 and the global rotation 𝜽 0. We argue that the combination of
global features and skeleton-based features can well describe the
global shape information, i.e., the skeleton joints may partially
describe the height, limb proportions and etc., while the holistic
perception of the input point clouds can indicate the weight or size
of the captured target. To this end, a feature aggregation method
similar to the edge convolution [49] is adopted to obtain a skeleton-
aware global parameter regression, as shown in Figure 3. The inputs
of this module are the completed joint features ˜Q ∈ R𝐾×𝐹2 , and
the global feature g ∈ R1×𝐹2 extracted at the bottleneck layer of
the backbone network for point cloud feature learning (shown in
Section 3.3.1). Since the global features and the local joint features
are heterogeneous, we apply a cross-attention module [44] to align
them, where the query is the global feature g, the key and value
are the joint features ˜Q, thus the attentive global feature is

˜g = softmax

(cid:16)

𝝍𝑄 (g)𝝍𝐾 ( ˜Q)⊤/√︁𝐹2

(cid:17)

𝝍𝑉 ( ˜Q).

(4)

𝝍𝑄 (·), 𝝍𝐾 (·), 𝝍𝑉 (·) are three projection layers for query, key and
value, respectively. Then the edge convolution operation is per-
formed between ˜g and each joint feature ˜q𝑖 , 𝑖 = 0, . . . , 𝐾, written
as 𝝍EdgeConv ([˜q𝑖 ; ˜g − ˜q𝑖 ]), where 𝝍EdgeConv (·) is one MLP layer
that is shared for each pair. The resultant features are further fed
into another FC layer to predict the global parameters 𝝓𝑔 ∈ R19, in-
cluding 𝜽 ∈ R10 and a vectorized root rotation matrix vec(R(𝜽 0)),
following the recent successes [27, 31, 56].

3.6 Local Parameter Regression
The local parameter regression is to predict the relative rotations
{𝜽 𝑘 }𝐾−1
of the articulated skeleton joints. We employ two lay-
𝑘=1
ers graph convolution network (GCNs) onto the skeleton graph
to incorporate contextual relations, where the graph nodes are
the completed joint features ˜Q, the graph edges are the edges in
the kinematic tree. The output is the vectorized rotation matrices
for all skeleton joints except that of the root joint. Thus the local
parameters take the format of 𝝓𝑙 ∈ R(𝐾−1)×9.

Cross-AttentionUnitMLPsharedKeyValueQuery<latexit sha1_base64="cddtMRNzMGMUukSIedL0B56+08Y=">AAAB8nicbVBNS8NAEJ34WetX1aOXxSJ4KkkR9VgURPBSwX5AGspmu2mXbjZhdyKU0p/hxYMiXv013vw3btsctPXBwOO9GWbmhakUBl3321lZXVvf2CxsFbd3dvf2SweHTZNkmvEGS2Si2yE1XArFGyhQ8naqOY1DyVvh8Gbqt564NiJRjzhKeRDTvhKRYBSt5N93UMTckNtutVsquxV3BrJMvJyUIUe9W/rq9BKWxVwhk9QY33NTDMZUo2CST4qdzPCUsiHtc99SRe2iYDw7eUJOrdIjUaJtKSQz9ffEmMbGjOLQdsYUB2bRm4r/eX6G0VUwFirNkCs2XxRlkmBCpv+TntCcoRxZQpkW9lbCBlRThjalog3BW3x5mTSrFe+i4j2cl2vXeRwFOIYTOAMPLqEGd1CHBjBI4Ble4c1B58V5dz7mrStOPnMEf+B8/gBnQ5Cu</latexit>K⇥F2<latexit sha1_base64="cHc7nfJY2oIfNMjHNJkAhxdcfsk=">AAAB6nicbVBNS8NAEJ34WetX1aOXxSJ4KkkR9VgUxGNF+wFtKJvtpF262YTdjVBCf4IXD4p49Rd589+4bXPQ1gcDj/dmmJkXJIJr47rfzsrq2vrGZmGruL2zu7dfOjhs6jhVDBssFrFqB1Sj4BIbhhuB7UQhjQKBrWB0M/VbT6g0j+WjGSfoR3QgecgZNVZ6uO1Ve6WyW3FnIMvEy0kZctR7pa9uP2ZphNIwQbXueG5i/Iwqw5nASbGbakwoG9EBdiyVNELtZ7NTJ+TUKn0SxsqWNGSm/p7IaKT1OApsZ0TNUC96U/E/r5Oa8MrPuExSg5LNF4WpICYm079JnytkRowtoUxxeythQ6ooMzadog3BW3x5mTSrFe+i4t2fl2vXeRwFOIYTOAMPLqEGd1CHBjAYwDO8wpsjnBfn3fmYt644+cwR/IHz+QPC8Y11</latexit>F2<latexit sha1_base64="cHc7nfJY2oIfNMjHNJkAhxdcfsk=">AAAB6nicbVBNS8NAEJ34WetX1aOXxSJ4KkkR9VgUxGNF+wFtKJvtpF262YTdjVBCf4IXD4p49Rd589+4bXPQ1gcDj/dmmJkXJIJr47rfzsrq2vrGZmGruL2zu7dfOjhs6jhVDBssFrFqB1Sj4BIbhhuB7UQhjQKBrWB0M/VbT6g0j+WjGSfoR3QgecgZNVZ6uO1Ve6WyW3FnIMvEy0kZctR7pa9uP2ZphNIwQbXueG5i/Iwqw5nASbGbakwoG9EBdiyVNELtZ7NTJ+TUKn0SxsqWNGSm/p7IaKT1OApsZ0TNUC96U/E/r5Oa8MrPuExSg5LNF4WpICYm079JnytkRowtoUxxeythQ6ooMzadog3BW3x5mTSrFe+i4t2fl2vXeRwFOIYTOAMPLqEGd1CHBjAYwDO8wpsjnBfn3fmYt644+cwR/IHz+QPC8Y11</latexit>F2<latexit sha1_base64="cddtMRNzMGMUukSIedL0B56+08Y=">AAAB8nicbVBNS8NAEJ34WetX1aOXxSJ4KkkR9VgURPBSwX5AGspmu2mXbjZhdyKU0p/hxYMiXv013vw3btsctPXBwOO9GWbmhakUBl3321lZXVvf2CxsFbd3dvf2SweHTZNkmvEGS2Si2yE1XArFGyhQ8naqOY1DyVvh8Gbqt564NiJRjzhKeRDTvhKRYBSt5N93UMTckNtutVsquxV3BrJMvJyUIUe9W/rq9BKWxVwhk9QY33NTDMZUo2CST4qdzPCUsiHtc99SRe2iYDw7eUJOrdIjUaJtKSQz9ffEmMbGjOLQdsYUB2bRm4r/eX6G0VUwFirNkCs2XxRlkmBCpv+TntCcoRxZQpkW9lbCBlRThjalog3BW3x5mTSrFe+i4j2cl2vXeRwFOIYTOAMPLqEGd1CHBjBI4Ble4c1B58V5dz7mrStOPnMEf+B8/gBnQ5Cu</latexit>K⇥F2<latexit sha1_base64="zzo0ia6kN1qmv0rzJjNfBF2ZX9I=">AAAB83icbVBNS8NAEJ34WetX1aOXxSJ4KkkR9VgURPBSwX5AE8pmu2mXbjZhdyKU0r/hxYMiXv0z3vw3btsctPXBwOO9GWbmhakUBl3321lZXVvf2CxsFbd3dvf2SweHTZNkmvEGS2Si2yE1XArFGyhQ8naqOY1DyVvh8Gbqt564NiJRjzhKeRDTvhKRYBSt5N/7KGJuSPW2W+2Wym7FnYEsEy8nZchR75a+/F7CspgrZJIa0/HcFIMx1SiY5JOinxmeUjakfd6xVFG7KRjPbp6QU6v0SJRoWwrJTP09MaaxMaM4tJ0xxYFZ9Kbif14nw+gqGAuVZsgVmy+KMkkwIdMASE9ozlCOLKFMC3srYQOqKUMbU9GG4C2+vEya1Yp3UfEezsu16zyOAhzDCZyBB5dQgzuoQwMYpPAMr/DmZM6L8+58zFtXnHzmCP7A+fwB24KQ6g==</latexit>K⇥2F2MLPMLP<latexit sha1_base64="cddtMRNzMGMUukSIedL0B56+08Y=">AAAB8nicbVBNS8NAEJ34WetX1aOXxSJ4KkkR9VgURPBSwX5AGspmu2mXbjZhdyKU0p/hxYMiXv013vw3btsctPXBwOO9GWbmhakUBl3321lZXVvf2CxsFbd3dvf2SweHTZNkmvEGS2Si2yE1XArFGyhQ8naqOY1DyVvh8Gbqt564NiJRjzhKeRDTvhKRYBSt5N93UMTckNtutVsquxV3BrJMvJyUIUe9W/rq9BKWxVwhk9QY33NTDMZUo2CST4qdzPCUsiHtc99SRe2iYDw7eUJOrdIjUaJtKSQz9ffEmMbGjOLQdsYUB2bRm4r/eX6G0VUwFirNkCs2XxRlkmBCpv+TntCcoRxZQpkW9lbCBlRThjalog3BW3x5mTSrFe+i4j2cl2vXeRwFOIYTOAMPLqEGd1CHBjBI4Ble4c1B58V5dz7mrStOPnMEf+B8/gBnQ5Cu</latexit>K⇥F2<latexit sha1_base64="+CQNZBGdY6IehZiSJFBo/WTEKqA=">AAAB+nicbVC7TsMwFL3hWcorhZHFokJiqhKEgLGChbFI9CG1UeQ4TmvViSPbAVWhn8LCAEKsfAkbf4PTZoCWI1k+Oude+fgEKWdKO863tbK6tr6xWdmqbu/s7u3btYOOEpkktE0EF7IXYEU5S2hbM81pL5UUxwGn3WB8U/jdByoVE8m9nqTUi/EwYREjWBvJt2uDQPBQTWJzDdIR84e+XXcazgxombglqUOJlm9/DUJBspgmmnCsVN91Uu3lWGpGOJ1WB5miKSZjPKR9QxMcU+Xls+hTdGKUEEVCmpNoNFN/b+Q4VkU6MxljPVKLXiH+5/UzHV15OUvSTNOEzB+KMo60QEUPKGSSEs0nhmAimcmKyAhLTLRpq2pKcBe/vEw6Zw33ouHendeb12UdFTiCYzgFFy6hCbfQgjYQeIRneIU368l6sd6tj/noilXuHMIfWJ8/vuGUUQ==</latexit> gGlobal FeatureComplete Joint FeatureAttentiveGlobal Feature3.7 Losses
The overall loss function for our VoteHMR can be divided into the
vote generation loss Lvote-gen as defined in Equ. (2), the parameter
regression loss for the global and local parameters, and the model
fitting loss, written as

L = 𝜆1Lvote-gen + 𝜆2Lparam-reg + 𝜆3Lmodel-fit.

(5)

Lparam-reg and Lmodel-fit will be depicted in the following parts.

3.7.1 Parameter Regression Loss. In order to achieve higher pose
prediction accuracies, following [27, 31, 56], we directly predict the
rotation matrix R𝑘 ∈ R3×3, 𝑘 = 0, . . . , 𝐾 for each joint, instead of
its axis-angle representation. Therefore, We apply SMPL parame-
ter regression loss to SURREAL dataset with ground truth SMPL
parameters, which is the combinational losses for shape and pose,
LSMPL = ∥𝜷 − 𝜷 ∗ ∥1 + (cid:205)𝐾−1
𝑘 ∥1. To ensure the orthog-
𝑘=0
onality of the generated rotation matrices, we explicitly include
an orthogonality regularization for the predicted rotation matrices,
such as Lorth = (cid:205)𝐾−1
𝑘 − I∥2. Therefore, the parameter
𝑘=0
regression loss is Lparam-reg = 𝜆21LSMPL + 𝜆22Lorth.

∥R𝑘 − R∗

∥R𝑘 R⊤

(cid:205)𝐾
𝑖=0 ∥c𝑘 − c∗

3.7.2 Model Fitting Loss. The model fitting loss includes three
parts. The first one is the skeleton fitting loss, i.e., Lskeleton =
𝑘 ∥1, which matches the predicted joint positions
1
𝐾
and the GT joint positions without occlusions. The second loss is the
vertex fitting loss, which is implemented as the per-vertex Euclidean
error between the predicted human shape model and the GT model,
𝑀 ∥M (𝝓𝑔, 𝝓𝑙 ) − M∗ ∥1, where M∗ indicate the
such as Lvertex = 1
3D coordinates of the vertices of the GT human model, 𝑀 is the
number of vertices in the SMPL model. The third loss is the half-
term chamfer distance between the predicted human shape model
and the input point cloud, i.e., LCD = 1
𝑖=1 min𝑗=1,...,𝑀 ∥p𝑖 −
𝑁
m𝑗 ∥2. It finds the closest vertex in the predicted model for each
point p𝑖 in the input point cloud. m𝑖 ∈ R3 is the 3D coordinate of
the 𝑖th vertex. Therefore, the overall model fitting loss is Lmodel-fit =
𝜆31Lvertex + 𝜆32LCD + 𝜆33Lskeleton.

(cid:205)𝑁

4 EXPERIMENTS
4.1 Setup
4.1.1 Datasets. We conduct our experiments on two synthetic
datasets, i.e., SURREAL [43] and DFAUST [4], and a real-world
dataset, Berkeley MHAD [26]. VoteHMR is trained on SURREAL
and DFAUST datasets since they provide GT SMPL models for each
input point cloud. The Berkeley MHAD dataset is applied for testing
the reconstruction performance in the real scenario.
- SURREAL [43]. The training set of the SURREAL dataset contains
55, 001 depth clips of synthetic human motions. We follow the
strategy of Wang et al [48] to uniformly sample 1, 000, 000 depth
images as the training set, and 10, 000 frames as the testing set. We
re-projected the depth images to obtain the partial point clouds.
- DFAUST [4]. It contains 40, 000 real undressed human meshes
for 65 action sequences. The training set of DFAUST is constructed
by projecting human meshes from the former 55 action sequences
onto 10 camera views, and then randomly sampling 250, 000 depth
images. The testing set is sampled from the latter 10 action se-
quences, and also randomly samples 10, 000 frames as the testing

Table 1: Per vertex reconstruction errors (mm) of different
methods tested on synthetic datasets.

Method
HMR [17]
GraphCMR [22]
3DCODED [9]
Skeleton [15]
Sequential (Non Param.) [48]
Sequential (Param.) [48]
Our Method

SURREAL [43] DFAUST [4]

75.85
73.81
41.8
80.5
21.2
24.3
20.2

76.87
75.31
43.5
82.6
22.2
25.2
21.5

set, following the setting in [48]. We apply the same point cloud
generation strategy as that for the SURREAL dataset.
- Berkeley MHAD [26]. This dataset is composed of two-view
depth sequences captured from two synchronized Kinect cameras.
It has a wide range of poses, containing 12 action sequences per-
formed by 12 actors. Berkeley MHAD also provides per-frame 3D
human keypoints annotations, which are also used to generate
bounding boxes to crop out the human regions. We sample about
2400 frames from the first eight sequences to serve as the test
set. Note that two depth views of the same action frame are both
converted into point clouds as two separate test samples.

4.1.2 Evaluation Metrics. We consider several evaluation metrics
to validate the performance of our method in different aspects.
For synthetic datasets that contain GT human models, we use the
widely adopted per-vertex error (PVE) to compute the average
reconstruction error between the corresponding vertices from two
SMPL models. We also report the maximum PVE (PVE max) error to
evaluate the robustness of our method when dealing with extreme
poses. Moreover, we also calculate the mean per-joint position error
(MPJPE) to indicate the prediction accuracy of the 3D joint locations.
For real datasets that we do not have GT human models, we report
the average Chamfer Distance (CD) as described in Section 3.7.

Implementation Details. We randomly sample the raw point
4.1.3
clouds to a fixed size with 𝑁 = 2, 500 as input. The PointNet++
backbone consists of 4 layers of set abstraction and feature propaga-
tion modules, respectively. The dimension of the per-point feature
from the last layer of PointNet++ is 128, the dimension of the clus-
ter joint feature is 131. In the partial vote generation and clustering
stage, we empirically set the occluded pre-defined threshold as 0.1.
In the training process, we use the Adam optimizer [19] with a
learning rate of 1e−4. We set the loss weights of first three loss
terms as 𝜆1 = 1 and 𝜆2 = 𝜆3 = 10. For vote generation loss, loss
weights are set as 𝜆11 = 0.1 and 𝜆12 = 1. We set 𝜆21 = 1 when
training SURREAL, and 𝜆21 = 0 for training DFAUST. The rest
loss weights are all set to be 1. During testing, we downsample the
predicted shape model to 1723 vertices for the calculation of PVE
and PVE max, so as to fairly compare with the baseline method [48].

4.2 Comparison to State-of-the-art Methods
4.2.1 Comparison on Synthetic Data. In this part, we report the
quantitative and qualitative comparisons between our method and
the reference methods in Table 1 and Figure 4. The quantitative
evaluations are both conducted on the SURREAL dataset [43] and

Figure 4: Qualitative comparison of different methods on
the SURREAL dataset [43]. (a) The input partial point clouds,
visualized as partially visible meshes. (b) The ground truth
human meshes. From (c) to (i), the reconstructed results by
(c) VoteHMR, (d) the non-parametric and (e) parametric vari-
ants of Wang et al [48], (f) 3D-CODED [9], (g) Jiang et al [15],
(h) GraphCMR [22] and (i) HMR [17]. The color indicates the
degree of reconstruction error in each vertex.

the DFAUST dataset [4]. The qualitative evaluations are performed
on the SURREAL dataset. The reference methods are all retrained
on these datasets for a fair comparison.

We first compare VoteHMR with two depth image-based hu-
man mesh recovery methods by retraining the recently RGB-based
HMR [17] and GraphCMR [22] methods onto depth data. Both
the quantitative and qualitative comparisons demonstrate that our
method outperforms them by a large margin. It shows that the
point clouds have unique benefits towards 3D human shape recon-
struction, in addition to data in the 2D modality.

We also compare our model with 3D human mesh recovery
methods upon the point clouds [9, 15, 48]. To fairly compare our
method with them, we add the SMPL parameter supervision to
Jiang et al [15], and reproduce a parametric version of Wang et
al [48] through an optimization-based SMPL fitting on predicted
vertices. Note that we just employ the single frame-based frame-
work of Wang et al [48]. The quantitative results demonstrate that
the proposed VoteHMR tremendously reduces reconstruction errors
than these reference methods. The visualization comparisons show
that Jiang et al [15] produce results with large pose mismatches,
especially the global poses, the non-parametric version of Wang et
al [48] distorts the human shapes when facing occlusion or hard
poses. 3D-CODED [9] can also roughly reconstruct the human
shapes, but its results may suffer from rugged or over-bent meshes,
as shown in Figure 4.

4.2.2 Comparison on Real Data. We also evaluate the generaliza-
tion ability of our VoteHMR on the real data, in comparison to the
reference methods. In this study, all methods are tested on the Berke-
ley MHAD dataset [26] but trained on the SURREAL dataset [43]
and DFAUST [4]. As visualized in Figure 5, the proposed method

Figure 5: Reconstruction results on different samples from
the Berkeley MHAD dataset [26], without the weakly super-
vised fine-tuning scheme [48]. (a) and (b) shows the input
point clouds and their corresponding RGB images. (c) are
the predicted meshes of our VoteHMR. (d) to (f) are the re-
sults by Wang et al [48], 3D-CODED [9], and Jiang et al [15].

Table 2: Mean Chamfer Distance (mm) of different methods
tested on MHAD dataset. Initial and Final suggest results be-
fore and after weakly supervised fine tuning.

Method
3DCODED [9]
Jiang et al [15]
Wang et al [48]
Our Method

Initial
56.29
121.63
81.29
51.76

Final
37.62
29.28
36.79
24.44

achieves more reliable reconstructions than the reference methods,
even though the input point clouds are heavily contaminated by
occlusions and noises. Note that we report both results before and
after the weakly supervised fine-tuning scheme as frequently used
in the previous methods [9, 15, 48], the reported results before the
weakly supervised fine-tuning can better indicate the capabilities
of different feed-forward networks of these methods, without the
interference of the aforementioned post-processing. Without the
weakly supervised fine-tuning scheme, the methods by Wang et
al [48] and Jiang et al [15] usually predict incorrect poses, manifest-
ing that their methods are sensitive to the domains of the training
data, and much less robust to changes in the point clouds. We also
report the results after weakly supervised fine-tuning results used
in previous methods in [48]. The overall weakly supervised loss

0cm10cm(a)(b)(c)(d)(e)(f)(g)(h)(i)(a)(b)(c)(d)(e)(f)Table 3: Ablation study of VoteHMR on the SURREAL
dataset [43]. The performances are evaluated on the PVE,
MAJPE and PVE max, respectively.

Method
Ours
w/o Voting
w/o Joint Completion
w/o Global Parameter Regressor

PVE MPJPE PVE max
20.14
57.51
59.44
26.2

220.78
678.4
444.7
275.9

17.7
49.2
49.47
22.93

Figure 7: Reconstruction accuracies with and without the
Occlusion-aware Joint Completion module. (a) is the point
cloud input. (b) is the vote results. (c) is the reconstruction er-
ror with the Occlusion-aware Joint Completion module. (d)
is the reconstruction error without this module. (e) is the
ground truth human mesh.

Figure 6: Reconstruction accuracies with and without the
Partial Vote Generation and Clustering module. (a) is the
point cloud input. (b) is the vote results. (c) is the reconstruc-
tion error with the Partial Vote Generation and Clustering
module. (d) is the reconstruction error without this module.
(e) is the ground truth human mesh.

term including a Chamfer distance loss to fit the mesh to input point
clouds, a Laplacian loss to preserve surface smoothness, an edge
loss to penalize unnatural edges and enforce length consistency.
We optimize the overall loss term on MHAD for about 100 epochs.
Comparison results are also shown in Table 2. In this case, the
proposed method still achieves significantly better performances
than other methods.

Our method may fail in cases of rare poses, while the reference
methods may also fail as well. We also notice that our method often
fails to predict correct shape parameters for real data, one possible
reason may be the limited shape variance of the synthetic training
dataset, while the noise in the MHAD [26] data also adds additional
difficulties to the shape estimation.

4.3 Ablation Study
We conduct a list of ablation studies of our VoteHMR, the PVE,
MPJPE and PVE max results are reported in Table 3.

4.3.1 Partial Vote Generation and Clustering. We first evaluate the
effectiveness of our partial vote generation and clustering module
by creating a baseline method named “w/o voting”, by replacing
these two modules with the skeleton-aware attention module as
in [15]. In “w/o voting” baseline, the pointnet++ downsamples point
cloud to 64 seed point features, and an attention module is used
to map unordered seed point features into skeleton joint features.
The skeleton joint features are further fed to the joint completion
module for feature refinement. The results in Table 3 and Figure 6
show that our method with partial vote generation and clustering
can produce results with much lower errors than this baseline
method does. Compared to attention-based solutions, our method

Figure 8: Reconstruction accuracies with and without the
Global Parameter Regression. (a) is the point cloud input.
(b) is the GT mesh. (c) is the reconstruction error with the
Global Parameter Regression module. (d) is the reconstruc-
tion error without this module.

can capture a more accurate skeleton structure, thus benefiting
from a more reliable pose estimation. Note that we also report the
maximum PVE value during evaluation. VoteHMR without voting
module has a much larger maximum reconstruction error on the
SURREAL dataset, indicating the robustness of our partial vote
generation and clustering module on hard poses.

4.3.2 Occlusion-aware Joint Completion. To evaluate the effective-
ness of our occlusion-aware joint completion module, we also con-
duct a baseline “w/o joint completion” by feeding incomplete joint
coordinates and features into the global and local parameter regres-
sors. The comparison results of all evaluation metrics in Table 3
demonstrate that our joint completion module is essential to achieve
better performances. As shown in Figure 7, the occluded hand fails
to be accurately recovered without the joint completion module. In
contrast, our VoteHMR can better handle the occlusions and guess
the most likely human poses from the partial input point clouds.

4.3.3 Global Parameter Regression. We further evaluate the effec-
tiveness of our global parameter regression. For baseline method
“w/o global parameter regression”, we apply a single layer MLP on
top of the concatenated complete joint coordinates and features to
estimate the SMPL global parameters 𝝓𝑔 ∈ R19. The reconstruction
errors are listed in Table 3. As shown in Figure 8, the result without

(a)(b)(c)(d)(e)0cm10cm(a)(b)(c)(d)(e)10cm(a)(b)(c)(d)0cm10cmFigure 9: Per Vertex Error of point cloud based methods
against varying noise levels (left), and varying numbers of
points (right) on the SURREAL testset.

Figure 10: Qualitative results of our VoteHMR under differ-
ent noise variation levels in the input point clouds. For each
result, from left to right, we show the reconstruction results
by applying our model to inputs with noise standard devia-
tion of 10mm, 20mm, 30mm, respectively.

global parameter regression estimates giant shape error compared
to the result with our complete method.

4.4 More Experiments about Robustness
4.4.1 Reconstruction against Varying Noise Levels. Figure 9 (left)
compares different methods about the PVEs at different noise levels
on the SURREAL test set. In this experiment, each point in a test
sample is added a random 3D noise offset according to a given
standard deviation. We retrain our VoteHMR and the previous

Figure 11: Qualitative results by applying our VoteHMR to
varying numbers of points. For each result, from left to right,
we show the reconstruction results when the input point
clouds have 2, 500, 1, 500, 1, 000, 500, 250 points, respectively.

methods [9, 15, 48] with a noise standard deviation of 10mm. During
the evaluation stage, we increase the noise standard deviation of
the test set to up to 40mm. The performance of Wang et al [48]
drops dramatically as the standard deviation of the added noises
increase. While the proposed methods VoteHMR usually present
the lowest PVE scores in all the noise levels. We also visualize the
results of our method in Figure 10, where different levels of noises
do not significantly distort the pose predictions. The increase of
PVE scores is mainly caused by inevitable prediction errors of shape
parameters due to large noise variance.

4.4.2 Reconstruction against Varying Point Numbers. Figure 9 (right)
shows the PVE scores of different point cloud based methods against
the varying sizes of the input point clouds, on the SURREAL test set.
The input point clouds are randomly sampled to different numbers.
And each method is trained when the input point cloud has 2, 500
points. The comparison results demonstrate the robustness of our
model even when only a fraction of points exists in the test set. The
visualization results in Figure 11 also show that our model performs
well when the input point clouds only have 250 points.

5 CONCLUSION
In this paper, we address the problem of robust human mesh recov-
ery from a single-frame partial point cloud, by a novel occlusion-
aware voting network, named as VoteHMR, to handle the com-
plex self-occlusion and noises of point cloud data captured from
commodity-level depth sensors. The experimental results have
demonstrated that our method can achieve state-of-the-art per-
formance on SURREAL and DFAUST datasets, and generalize well
to real data captured by depth sensors like Berkeley MHAD dataset,
without the aid of further weakly supervised fine-tuning.

Acknowledgement
This work was supported by the Key Research and Development
Program of Guangdong Province, China (No. 2019B010154003), and
the National Natural Science Foundation of China (No. 61906012).

10152025303540Std of Noise50100150200250PVEOursWang et al.3D-CODEDJiang et al.5001000150020002500Number of Points2030405060708090PVEOursWang et al.3D-CODEDJiang et al.10mm20mm30mm10mm20mm30mmInputsOurs2500 points1500 points250 points2500 points250points1000 points500 points1500 points1000 points500 pointsREFERENCES
[1] Dragomir Anguelov, Praveen Srinivasan, Daphne Koller, Sebastian Thrun, Jim
Rodgers, and James Davis. 2005. SCAPE: shape completion and animation of
people. ACM Trans. Graph. 24, 3 (2005), 408–416. https://doi.org/10.1145/1073204.
1073207

[2] Federica Bogo, Michael J. Black, Matthew Loper, and Javier Romero. 2015. Detailed
Full-Body Reconstructions of Moving People from Monocular RGB-D Sequences.
In 2015 IEEE International Conference on Computer Vision, ICCV 2015, Santiago,
Chile, December 7-13, 2015. IEEE Computer Society, Santiago, 2300–2308. https:
//doi.org/10.1109/ICCV.2015.265

[3] Federica Bogo, Angjoo Kanazawa, Christoph Lassner, Peter V. Gehler, Javier
Romero, and Michael J. Black. 2016. Keep It SMPL: Automatic Estimation of 3D
Human Pose and Shape from a Single Image. In Computer Vision - ECCV 2016
- 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016,
Proceedings, Part V (Lecture Notes in Computer Science, Vol. 9909), Bastian Leibe,
Jiri Matas, Nicu Sebe, and Max Welling (Eds.). Springer, Amsterdam, 561–578.
https://doi.org/10.1007/978-3-319-46454-1_34

[4] Federica Bogo, Javier Romero, Gerard Pons-Moll, and Michael J. Black. 2017.
Dynamic FAUST: Registering Human Bodies in Motion. In 2017 IEEE Conference
on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July
21-26, 2017. IEEE Computer Society, Honolulu, 5573–5582. https://doi.org/10.
1109/CVPR.2017.591

[5] Bowen Cheng, Lu Sheng, Shaoshuai Shi, Ming Yang, and Dong Xu. 2021. Back-
Tracing Representative Points for Voting-Based 3D Object Detection in Point
Clouds. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR). IEEE, virtual, 8963–8972.

[6] Mingsong Dou, Sameh Khamis, Yury Degtyarev, Philip L. Davidson, Sean Ryan
Fanello, Adarsh Kowdle, Sergio Orts-Escolano, Christoph Rhemann, David Kim,
Jonathan Taylor, Pushmeet Kohli, Vladimir Tankovich, and Shahram Izadi. 2016.
Fusion4D: real-time performance capture of challenging scenes. ACM Trans.
Graph. 35, 4 (2016), 114:1–114:13. https://doi.org/10.1145/2897824.2925969
[7] Mingsong Dou, Jonathan Taylor, Henry Fuchs, Andrew W. Fitzgibbon, and
Shahram Izadi. 2015. 3D scanning deformable objects with a single RGBD
sensor. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR
2015, Boston, MA, USA, June 7-12, 2015. IEEE Computer Society, Boston, 493–501.
https://doi.org/10.1109/CVPR.2015.7298647

[8] Liuhao Ge, Zhou Ren, and Junsong Yuan. 2018. Point-to-Point Regression Point-
Net for 3D Hand Pose Estimation. In Computer Vision - ECCV 2018 - 15th Eu-
ropean Conference, Munich, Germany, September 8-14, 2018, Proceedings, Part
XIII (Lecture Notes in Computer Science, Vol. 11217), Vittorio Ferrari, Martial
Hebert, Cristian Sminchisescu, and Yair Weiss (Eds.). Springer, Munich, 489–505.
https://doi.org/10.1007/978-3-030-01261-8_29

[9] Thibault Groueix, Matthew Fisher, Vladimir G. Kim, Bryan C. Russell, and Math-
ieu Aubry. 2018. 3D-CODED: 3D Correspondences by Deep Deformation. In Com-
puter Vision - ECCV 2018 - 15th European Conference, Munich, Germany, September
8-14, 2018, Proceedings, Part II (Lecture Notes in Computer Science, Vol. 11206),
Vittorio Ferrari, Martial Hebert, Cristian Sminchisescu, and Yair Weiss (Eds.).
Springer, Munich, 235–251. https://doi.org/10.1007/978-3-030-01216-8_15
[10] Riza Alp Güler and Iasonas Kokkinos. 2019. HoloPose: Holistic 3D Human
Reconstruction In-The-Wild. In IEEE Conference on Computer Vision and Pattern
Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019. Computer Vision
Foundation / IEEE, Long Beach, 10884–10894. https://doi.org/10.1109/CVPR.
2019.01114

[11] Riza Alp Güler, Natalia Neverova, and Iasonas Kokkinos. 2018. DensePose: Dense
Human Pose Estimation in the Wild. In 2018 IEEE Conference on Computer Vision
and Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018.
IEEE Computer Society, Salt Lake City, 7297–7306. https://doi.org/10.1109/CVPR.
2018.00762

[12] Yisheng He, Wei Sun, Haibin Huang, Jianran Liu, Haoqiang Fan, and Jian Sun.
2020. PVN3D: A Deep Point-Wise 3D Keypoints Voting Network for 6DoF
Pose Estimation. In 2020 IEEE/CVF Conference on Computer Vision and Pattern
Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020. IEEE, Seattle, 11629–
11638. https://doi.org/10.1109/CVPR42600.2020.01165

[13] Paul VC Hough. 1959. Machine analysis of bubble chamber pictures. In Proc.
of the International Conference on High Energy Accelerators and Instrumentation,
Sept. 1959. 554–556.

[14] Matthias Innmann, Michael Zollhöfer, Matthias Nießner, Christian Theobalt,
and Marc Stamminger. 2016. VolumeDeform: Real-Time Volumetric Non-rigid
Reconstruction. In Computer Vision - ECCV 2016 - 14th European Conference,
Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part VIII (Lecture
Notes in Computer Science, Vol. 9912), Bastian Leibe, Jiri Matas, Nicu Sebe, and
Max Welling (Eds.). Springer, Amsterdam, 362–379. https://doi.org/10.1007/978-
3-319-46484-8_22

[15] Haiyong Jiang, Jianfei Cai, and Jianmin Zheng. 2019. Skeleton-Aware 3D Human
Shape Reconstruction From Point Clouds. In 2019 IEEE/CVF International Confer-
ence on Computer Vision, ICCV 2019, Seoul, Korea (South), October 27 - November
2, 2019. IEEE, Seoul, 5430–5440. https://doi.org/10.1109/ICCV.2019.00553

[16] Li Jiang, Hengshuang Zhao, Shaoshuai Shi, Shu Liu, Chi-Wing Fu, and Jiaya Jia.
2020. PointGroup: Dual-Set Point Grouping for 3D Instance Segmentation. In
2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020,
Seattle, WA, USA, June 13-19, 2020. IEEE, Seattle, 4866–4875. https://doi.org/10.
1109/CVPR42600.2020.00492

[17] Angjoo Kanazawa, Michael J. Black, David W. Jacobs, and Jitendra Malik. 2018.
End-to-End Recovery of Human Shape and Pose. In 2018 IEEE Conference on
Computer Vision and Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA,
June 18-22, 2018. IEEE Computer Society, Salt Lake City, 7122–7131. https:
//doi.org/10.1109/CVPR.2018.00744

[18] Angjoo Kanazawa, Jason Y. Zhang, Panna Felsen, and Jitendra Malik. 2019.
Learning 3D Human Dynamics From Video. In IEEE Conference on Computer
Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20,
2019. Computer Vision Foundation / IEEE, Long Beach, 5614–5623.
https:
//doi.org/10.1109/CVPR.2019.00576

[19] Diederik P. Kingma and Jimmy Ba. 2015. Adam: A Method for Stochastic Opti-
mization. In 3rd International Conference on Learning Representations, ICLR 2015,
San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, Yoshua Bengio
and Yann LeCun (Eds.). San Diego. http://arxiv.org/abs/1412.6980

[20] Muhammed Kocabas, Nikos Athanasiou, and Michael J. Black. 2020. VIBE: Video
Inference for Human Body Pose and Shape Estimation. In 2020 IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA,
June 13-19, 2020. IEEE, Seattle, 5252–5262. https://doi.org/10.1109/CVPR42600.
2020.00530

[21] Nikos Kolotouros, Georgios Pavlakos, Michael J. Black, and Kostas Daniilidis.
2019. Learning to Reconstruct 3D Human Pose and Shape via Model-Fitting in
the Loop. In 2019 IEEE/CVF International Conference on Computer Vision, ICCV
2019, Seoul, Korea (South), October 27 - November 2, 2019. IEEE, Seoul, 2252–2261.
https://doi.org/10.1109/ICCV.2019.00234

[22] Nikos Kolotouros, Georgios Pavlakos, and Kostas Daniilidis. 2019. Convolutional
Mesh Regression for Single-Image Human Shape Reconstruction. In IEEE Confer-
ence on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA,
June 16-20, 2019. Computer Vision Foundation / IEEE, Long Beach, 4501–4510.
https://doi.org/10.1109/CVPR.2019.00463

[23] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll, and
Michael J. Black. 2015. SMPL: a skinned multi-person linear model. ACM Trans.
Graph. 34, 6 (2015), 248:1–248:16. https://doi.org/10.1145/2816795.2818013
[24] Richard A. Newcombe, Dieter Fox, and Steven M. Seitz. 2015. DynamicFusion:
Reconstruction and tracking of non-rigid scenes in real-time. In IEEE Conference
on Computer Vision and Pattern Recognition, CVPR 2015, Boston, MA, USA, June
7-12, 2015. IEEE Computer Society, Boston, 343–352. https://doi.org/10.1109/
CVPR.2015.7298631

[25] Alejandro Newell, Kaiyu Yang, and Jia Deng. 2016. Stacked Hourglass Networks
for Human Pose Estimation. In Computer Vision - ECCV 2016 - 14th European
Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part
VIII (Lecture Notes in Computer Science, Vol. 9912), Bastian Leibe, Jiri Matas, Nicu
Sebe, and Max Welling (Eds.). Springer, Amsterdam, 483–499. https://doi.org/10.
1007/978-3-319-46484-8_29

[26] Ferda Ofli, Rizwan Chaudhry, Gregorij Kurillo, René Vidal, and Ruzena Bajcsy.
2013. Berkeley MHAD: A comprehensive Multimodal Human Action Database.
In 2013 IEEE Workshop on Applications of Computer Vision, WACV 2013, Clearwater
Beach, FL, USA, January 15-17, 2013. IEEE Computer Society, Clearwater Beach,
53–60. https://doi.org/10.1109/WACV.2013.6474999

[27] Mohamed Omran, Christoph Lassner, Gerard Pons-Moll, Peter V. Gehler, and
Bernt Schiele. 2018. Neural Body Fitting: Unifying Deep Learning and Model
Based Human Pose and Shape Estimation. In 2018 International Conference on
3D Vision, 3DV 2018, Verona, Italy, September 5-8, 2018. IEEE Computer Society,
Verona, 484–494. https://doi.org/10.1109/3DV.2018.00062

[28] Ahmed A. A. Osman, Timo Bolkart, and Michael J. Black. 2020. STAR: Sparse
Trained Articulated Human Body Regressor. In Computer Vision - ECCV 2020 -
16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part VI
(Lecture Notes in Computer Science, Vol. 12351), Andrea Vedaldi, Horst Bischof,
Thomas Brox, and Jan-Michael Frahm (Eds.). Springer, Glasgow, 598–613. https:
//doi.org/10.1007/978-3-030-58539-6_36

[29] Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani, Timo Bolkart, Ahmed
A. A. Osman, Dimitrios Tzionas, and Michael J. Black. 2019. Expressive Body
Capture: 3D Hands, Face, and Body From a Single Image. In IEEE Conference
on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA,
June 16-20, 2019. Computer Vision Foundation / IEEE, Long Beach, 10975–10985.
https://doi.org/10.1109/CVPR.2019.01123

[30] Georgios Pavlakos, Xiaowei Zhou, Konstantinos G. Derpanis, and Kostas Dani-
ilidis. 2017. Coarse-to-Fine Volumetric Prediction for Single-Image 3D Human
Pose. In 2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR
2017, Honolulu, HI, USA, July 21-26, 2017. IEEE Computer Society, Honolulu,
1263–1272. https://doi.org/10.1109/CVPR.2017.139

[31] Georgios Pavlakos, Luyang Zhu, Xiaowei Zhou, and Kostas Daniilidis. 2018.
Learning to Estimate 3D Human Pose and Shape From a Single Color Image. In
2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018,
Salt Lake City, UT, USA, June 18-22, 2018. IEEE Computer Society, Salt Lake City,

459–468. https://doi.org/10.1109/CVPR.2018.00055

//doi.org/10.1109/CVPR.2018.00275

[32] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. 2019. PVNet:
Pixel-Wise Voting Network for 6DoF Pose Estimation. In IEEE Conference on
Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June
16-20, 2019. Computer Vision Foundation / IEEE, Long Beach, 4561–4570. https:
//doi.org/10.1109/CVPR.2019.00469

[33] Xue Bin Peng, Angjoo Kanazawa, Sam Toyer, Pieter Abbeel, and Sergey Levine.
2019. Variational Discriminator Bottleneck: Improving Imitation Learning, In-
verse RL, and GANs by Constraining Information Flow. In 7th International Confer-
ence on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.
OpenReview.net, New Orleans. https://openreview.net/forum?id=HyxPx3R9tm
[34] Xue Bin Peng, Ze Ma, Pieter Abbeel, Sergey Levine, and Angjoo Kanazawa. 2021.
AMP: Adversarial Motion Priors for Stylized Physics-Based Character Control.
CoRR abs/2104.02180 (2021). arXiv:2104.02180 https://arxiv.org/abs/2104.02180
[35] Charles R. Qi, Or Litany, Kaiming He, and Leonidas J. Guibas. 2019. Deep Hough
Voting for 3D Object Detection in Point Clouds. In 2019 IEEE/CVF International
Conference on Computer Vision, ICCV 2019, Seoul, Korea (South), October 27 -
November 2, 2019. IEEE, Seoul, 9276–9285. https://doi.org/10.1109/ICCV.2019.
00937

[36] Charles Ruizhongtai Qi, Hao Su, Kaichun Mo, and Leonidas J. Guibas. 2017.
PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation.
In 2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017,
Honolulu, HI, USA, July 21-26, 2017. IEEE Computer Society, Honolulu, 77–85.
https://doi.org/10.1109/CVPR.2017.16

[37] Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J. Guibas. 2017. Point-
Net++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space.
In Advances in Neural Information Processing Systems 30: Annual Confer-
ence on Neural Information Processing Systems 2017, December 4-9, 2017, Long
Beach, CA, USA, Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M.
Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett (Eds.).
Long Beach, 5099–5108.
https://proceedings.neurips.cc/paper/2017/hash/
d8bf84be3800d12f74d8b05e9b89836f-Abstract.html

[38] Yu Rong, Ziwei Liu, Cheng Li, Kaidi Cao, and Chen Change Loy. 2019. Delving
Deep Into Hybrid Annotations for 3D Human Recovery in the Wild. In 2019
IEEE/CVF International Conference on Computer Vision, ICCV 2019, Seoul, Korea
(South), October 27 - November 2, 2019. IEEE, Seoul, 5339–5347. https://doi.org/
10.1109/ICCV.2019.00544

[39] Yu Rong, Ziwei Liu, and Chen Change Loy. 2020. Chasing the Tail in Monocular
3D Human Reconstruction with Prototype Memory. CoRR abs/2012.14739 (2020).
arXiv:2012.14739 https://arxiv.org/abs/2012.14739

[40] Yu Rong, Takaaki Shiratori, and Hanbyul Joo. 2020. FrankMocap: Fast Monoc-
ular 3D Hand and Body Motion Capture by Regression and Integration. CoRR
abs/2008.08324 (2020). arXiv:2008.08324 https://arxiv.org/abs/2008.08324
[41] Shunsuke Saito, Zeng Huang, Ryota Natsume, Shigeo Morishima, Hao Li,
and Angjoo Kanazawa. 2019. PIFu: Pixel-Aligned Implicit Function for High-
Resolution Clothed Human Digitization. In 2019 IEEE/CVF International Confer-
ence on Computer Vision, ICCV 2019, Seoul, Korea (South), October 27 - November
2, 2019. IEEE, Seoul, 2304–2314. https://doi.org/10.1109/ICCV.2019.00239
[42] Gül Varol, Duygu Ceylan, Bryan C. Russell, Jimei Yang, Ersin Yumer, Ivan Laptev,
and Cordelia Schmid. 2018. BodyNet: Volumetric Inference of 3D Human Body
Shapes. In Computer Vision - ECCV 2018 - 15th European Conference, Munich,
Germany, September 8-14, 2018, Proceedings, Part VII (Lecture Notes in Computer
Science, Vol. 11211), Vittorio Ferrari, Martial Hebert, Cristian Sminchisescu, and
Yair Weiss (Eds.). Springer, Munich, 20–38. https://doi.org/10.1007/978-3-030-
01234-2_2

[43] Gül Varol, Javier Romero, Xavier Martin, Naureen Mahmood, Michael J. Black,
Ivan Laptev, and Cordelia Schmid. 2017. Learning from Synthetic Humans. In
2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017,
Honolulu, HI, USA, July 21-26, 2017. IEEE Computer Society, Honolulu, 4627–4635.
https://doi.org/10.1109/CVPR.2017.492

[44] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention Is All
You Need. CoRR abs/1706.03762 (2017). arXiv:1706.03762 http://arxiv.org/abs/
1706.03762

[45] Nitika Verma, Edmond Boyer, and Jakob Verbeek. 2018. FeaStNet: Feature-
Steered Graph Convolutions for 3D Shape Analysis. In 2018 IEEE Conference on
Computer Vision and Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA,
June 18-22, 2018. IEEE Computer Society, Salt Lake City, 2598–2606. https:

[46] Jingbo Wang, Sijie Yan, Bo Dai, and Dahua Lin. 2021. Scene-aware Generative Net-
work for Human Motion Synthesis. CoRR abs/2105.14804 (2021). arXiv:2105.14804
https://arxiv.org/abs/2105.14804

[47] Jingbo Wang, Sijie Yan, Yuanjun Xiong, and Dahua Lin. 2020. Motion Guided 3D
Pose Estimation from Videos. In Computer Vision - ECCV 2020 - 16th European
Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part XIII (Lecture Notes
in Computer Science, Vol. 12358), Andrea Vedaldi, Horst Bischof, Thomas Brox,
and Jan-Michael Frahm (Eds.). Springer, Glasgow, 764–780. https://doi.org/10.
1007/978-3-030-58601-0_45

[48] Kangkan Wang, Jin Xie, Guofeng Zhang, Lei Liu, and Jian Yang. 2020. Sequential
3D Human Pose and Shape Estimation From Point Clouds. In 2020 IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA,
June 13-19, 2020. IEEE, Seattle, 7273–7282. https://doi.org/10.1109/CVPR42600.
2020.00730

[49] Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay E. Sarma, Michael M. Bronstein, and
Justin M. Solomon. 2019. Dynamic Graph CNN for Learning on Point Clouds.
ACM Trans. Graph. 38, 5 (2019), 146:1–146:12. https://doi.org/10.1145/3326362
[50] Lingyu Wei, Qixing Huang, Duygu Ceylan, Etienne Vouga, and Hao Li. 2016.
Dense Human Body Correspondences Using Convolutional Networks. In 2016
IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las
Vegas, NV, USA, June 27-30, 2016. IEEE Computer Society, Las Vegas, 1544–1553.
https://doi.org/10.1109/CVPR.2016.171

[51] Qian Xie, Yu-Kun Lai, Jing Wu, Zhoutao Wang, Yiming Zhang, Kai Xu, and Jun
Wang. 2020. MLCVNet: Multi-Level Context VoteNet for 3D Object Detection.
In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR
2020, Seattle, WA, USA, June 13-19, 2020. IEEE, Seattle, 10444–10453. https:
//doi.org/10.1109/CVPR42600.2020.01046

[52] Genzhi Ye, Yebin Liu, Nils Hasler, Xiangyang Ji, Qionghai Dai, and Christian
Theobalt. 2012. Performance Capture of Interacting Characters with Handheld
Kinects. In Computer Vision - ECCV 2012 - 12th European Conference on Computer
Vision, Florence, Italy, October 7-13, 2012, Proceedings, Part II (Lecture Notes in
Computer Science, Vol. 7573), Andrew W. Fitzgibbon, Svetlana Lazebnik, Pietro
Perona, Yoichi Sato, and Cordelia Schmid (Eds.). Springer, Florence, 828–841.
https://doi.org/10.1007/978-3-642-33709-3_59

[53] Mao Ye, Yang Shen, Chao Du, Zhigeng Pan, and Ruigang Yang. 2016. Real-Time
Simultaneous Pose and Shape Estimation for Articulated Objects Using a Single
Depth Camera. IEEE Trans. Pattern Anal. Mach. Intell. 38, 8 (2016), 1517–1532.
https://doi.org/10.1109/TPAMI.2016.2557783

[54] Tao Yu, Kaiwen Guo, Feng Xu, Yuan Dong, Zhaoqi Su, Jianhui Zhao, Jianguo Li,
Qionghai Dai, and Yebin Liu. 2017. BodyFusion: Real-Time Capture of Human
Motion and Surface Geometry Using a Single Depth Camera. In IEEE International
Conference on Computer Vision, ICCV 2017, Venice, Italy, October 22-29, 2017. IEEE
Computer Society, Venice, 910–919. https://doi.org/10.1109/ICCV.2017.104
[55] Tao Yu, Jianhui Zhao, Zerong Zheng, Kaiwen Guo, Qionghai Dai, Hao Li, Gerard
Pons-Moll, and Yebin Liu. 2020. DoubleFusion: Real-Time Capture of Human
Performances with Inner Body Shapes from a Single Depth Sensor. IEEE Trans.
Pattern Anal. Mach. Intell. 42, 10 (2020), 2523–2539. https://doi.org/10.1109/
TPAMI.2019.2928296

[56] Hongwen Zhang, Jie Cao, Guo Lu, Wanli Ouyang, and Zhenan Sun. 2019. Learning
3D Human Shape and Pose from Dense Body Parts. CoRR abs/1912.13344 (2019).
arXiv:1912.13344 http://arxiv.org/abs/1912.13344

[57] Zerong Zheng, Tao Yu, Hao Li, Kaiwen Guo, Qionghai Dai, Lu Fang, and
Yebin Liu. 2018. HybridFusion: Real-Time Performance Capture Using a Single
Depth Sensor and Sparse IMUs. In Computer Vision - ECCV 2018 - 15th Eu-
ropean Conference, Munich, Germany, September 8-14, 2018, Proceedings, Part
IX (Lecture Notes in Computer Science, Vol. 11213), Vittorio Ferrari, Martial
Hebert, Cristian Sminchisescu, and Yair Weiss (Eds.). Springer, Munich, 389–
406. https://doi.org/10.1007/978-3-030-01240-3_24

[58] Zerong Zheng, Tao Yu, Yixuan Wei, Qionghai Dai, and Yebin Liu. 2019. Dee-
pHuman: 3D Human Reconstruction From a Single Image. In 2019 IEEE/CVF
International Conference on Computer Vision, ICCV 2019, Seoul, Korea (South),
October 27 - November 2, 2019. IEEE, Seoul, 7738–7748. https://doi.org/10.1109/
ICCV.2019.00783

[59] Hao Zhu, Xinxin Zuo, Sen Wang, Xun Cao, and Ruigang Yang. 2019. Detailed
Human Shape Estimation From a Single Image by Hierarchical Mesh Deformation.
In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long
Beach, CA, USA, June 16-20, 2019. Computer Vision Foundation / IEEE, Long
Beach, 4491–4500. https://doi.org/10.1109/CVPR.2019.00462

