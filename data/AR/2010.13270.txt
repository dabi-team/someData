IMPROVED MASK-CTC FOR NON-AUTOREGRESSIVE END-TO-END ASR

Yosuke Higuchi1, Hirofumi Inaguma2, Shinji Watanabe3, Tetsuji Ogawa1, Tetsunori Kobayashi1

1 Waseda University, Japan 2 Kyoto University, Japan 3 Johns Hopkins University, USA

1
2
0
2

b
e
F
6
1

]
S
A
.
s
s
e
e
[

2
v
0
7
2
3
1
.
0
1
0
2
:
v
i
X
r
a

ABSTRACT

For real-world deployment of automatic speech recognition (ASR),
the system is desired to be capable of fast inference while relieving
the requirement of computational resources. The recently proposed
end-to-end ASR system based on mask-predict with connectionist
temporal classiﬁcation (CTC), Mask-CTC, fulﬁlls this demand by
generating tokens in a non-autoregressive fashion. While Mask-
CTC achieves remarkably fast inference speed, its recognition per-
formance falls behind that of conventional autoregressive (AR) sys-
tems. To boost the performance of Mask-CTC, we ﬁrst propose
to enhance the encoder network architecture by employing a re-
cently proposed architecture called Conformer. Next, we propose
new training and decoding methods by introducing auxiliary ob-
jective to predict the length of a partial target sequence, which al-
lows the model to delete or insert tokens during inference. Ex-
perimental results on different ASR tasks show that the proposed
approaches improve Mask-CTC signiﬁcantly, outperforming a stan-
dard CTC model (15.5% → 9.1% WER on WSJ). Moreover, Mask-
CTC now achieves competitive results to AR models with no degra-
dation of inference speed (< 0.1 RTF using CPU). We also show a
potential application of Mask-CTC to end-to-end speech translation.

Index Terms— Non-autoregressive sequence generation, con-
nectionist temporal classiﬁcation, end-to-end speech recognition,
end-to-end speech translation

1. INTRODUCTION

End-to-end automatic speech recognition (E2E-ASR) is a task which
directly converts speech into text based on sequence-to-sequence
modeling [1–4]. Compared with the traditional hybrid system built
upon separately trained modules [5], E2E-ASR greatly simpliﬁes
its model training and inference, reducing the cost of model devel-
opment and the requirement of computational resources. Besides,
E2E-ASR has achieved comparable results with those of the hybrid
system in diverse tasks [6–9]. Recent model architectures and tech-
niques have further enhanced the performance of E2E-ASR [10–15].
Many of the previous studies on E2E-ASR focus on an autore-
gressive (AR) model [3, 4], which estimates the likelihood of a se-
quence generation based on a left-to-right probabilistic chain rule.
The AR model often performs the best among other E2E-ASR archi-
tectures [6]. However, it suffers from slow inference speed, requiring
L-step incremental calculations of the model to generate L tokens.
While some neural models, including Transformer [16], permit for
efﬁcient inference using GPU, it becomes highly computation inten-
sive in production environments. For example, mobile devices are
often constrained by limited on-device resources, where only CPU
is generally available for the model inference1. If the inference algo-

1It may be argued that the inference can be done on a cloud server but run-
ning the model entirely on-device is important from privacy perspective [9].

rithm is light and simple, ASR can be performed on-device at low-
power consumption with fast recognition speed. Building E2E-ASR
models with fast inference speed while suppressing the model calcu-
lations is a critical factor for the real-world deployment.

Contrary to the AR framework, a non-autoregressive (NAR)
model generates a sequence within a constant number of the infer-
ence steps [1, 17]. NAR models in neural machine translation have
achieved competitive performance to AR models [18–25]. Several
approaches have been proposed to realize a NAR model in E2E-
ASR [26–31]. Connectionist temporal classiﬁcation (CTC) predicts
a frame-wise latent alignment between input speech frames and
output tokens, and generates a target sequence based on a condi-
tional independence assumption between the frame predictions [26].
However, such assumption limits the performance of CTC compared
with that of AR models [32]. Imputer [28] effectively models the
contextual dependencies by iteratively predicting the latent align-
ments of CTC based on mask prediction [22, 33]. Despite achieving
comparable performance with that of AR models, Imputer processes
the frame-level sequence, consisting of hundreds of units, using the
self-attention layers [16], which cost computations proportional to
the square of a sequence length. On the other hand, Mask-CTC [29]
generates a sequence shorter than Imputer by reﬁning a token-level
CTC output with the mask prediction, achieving fast inference speed
of less than 0.1 real time factor (RTF) using CPU.

In this work, we focus on improving Mask-CTC based E2E-
ASR, which suits for the production owing to its fast inference.
Mask-CTC starts a sequence generation from an output of CTC to
avoid the cumbersome length prediction of a target sequence, which
is required in the previous NAR framework [17]. However, Mask-
CTC is confronted with some problems related to this dependence on
the CTC output. Firstly, the performance of Mask-CTC is strongly
limited to that of CTC since only minor errors in the CTC output,
such as spelling mistakes, are subjected to the reﬁnement. Secondly,
the length of a target sequence is ﬁxed with that of the CTC out-
put throughout the decoding, making it difﬁcult to recover deletion
or insertion errors in the CTC output. To overcome the former, we
adopt Conformer [15], which yields better speech processing than
Transformer, to improve the CTC performance. For the latter, we in-
troduce new training and decoding strategies to handle the deletion
and insertion errors during inference. We also explore the potential
of Mask-CTC to the end-to-end speech translation (E2E-ST) task.

2. NON-AUTOREGRESSIVE END-TO-END ASR

E2E-ASR aims to model a conditional probability P (Y |X), which
directly maps a T -length input sequence X = (xt ∈ RD|t =
1, ..., T ) into a L-length output sequence Y = (yl ∈ V|l =
1, ..., L). Here, xt is a D-dimensional acoustic feature at frame t,
yl is an output token at position l, and V is a vocabulary.

In this section, we review non-autoregressive (NAR) models,
which perform sequence generation within a constant number of in-
ference steps in parallel with token positions.

 
 
 
 
 
 
2.1. Connectionist temporal classiﬁcation (CTC)

CTC predicts a frame-level input-output alignment A = (at ∈ V ∪
{ǫ}|t = 1, ..., T ) by introducing a special blank symbol ǫ [1]. Based
on a conditional independence assumption per frame, CTC models
the probability P (Y |X) by marginalizing over all possible paths as:

Pctc(Y |X) = X

P (A|X),

(1)

A∈β−1(Y )

where β−1(Y ) denotes all possible alignments compatible with Y .
The conditional independence assumption enables the model to per-
form NAR sequence generation in a single forward pass.

2.2. Mask-CTC

Due to the conditional independence assumption, the CTC-based
model generally suffers from poor recognition performance [6].
Mask-CTC has been proposed to mitigate this problem by itera-
tively reﬁning an output of CTC with bi-directional contexts of
tokens [29]. Mask-CTC adopts an encoder-decoder model built
upon Transformer blocks [16]. CTC is applied to the encoder out-
put [12, 34] and the decoder is trained via the masked language
model (MLM) objective [22, 33]. For training the MLM decoder,
randomly sampled tokens Ymask are replaced by a special mask
token <MASK>. Ymask are then predicted conditioning on the rest
observed (unmasked) tokens Yobs (= Y \ Ymask) as follows:

Pmlm(Ymask|Yobs, X) = Y

P (y|Yobs, X).

(2)

y∈Ymask

The number of masked tokens Nmask (= |Ymask|) are sampled from
a uniform distribution of 1 to L as in [22, 23]. With the CTC objec-
tive in Eq. (1), Mask-CTC optimizes model parameters by maximiz-
ing the following log-likelihood as:

LNAR = α log Pctc(Y |X)+

(1 − α) log Pmlm(Ymask|Yobs, X),

(3)

where α (0 ≤ α ≤ 1) is a tunable parameter.

During inference, an output of CTC is obtained through greedy
decoding by suppressing repeated tokens and removing blank sym-
bols. Then, low-conﬁdence tokens are replaced with <MASK> by
thresholding the posterior probabilities of CTC with Pthres. The
resulting unmasked tokens are then fed into the MLM decoder to
predict the masked tokens. With this two-pass inference based on
the CTC and MLM decoding, the errors in the CTC output, caused
by the conditional independence assumption, are expected to be cor-
rected conditioning on the high-conﬁdence tokens in the entire out-
put sequence. The CTC output can be further improved by gradually
ﬁlling in the masked tokens in multiple steps K. At the n-th step,
yl ∈ Y (n)

mask is predicted as follows:

yl = argmax

y

Pmlm(yl = y|Y (n)

obs , X),

(4)

where top C positions with the highest decoder probabilities are se-
lected to be predicted at each iteration. By setting C = ⌊Nmask/K⌋,
inference can be completed in constant K steps.

3. PROPOSED IMPROVEMENTS OF MASK-CTC

Making use of the CTC output during inference, Mask-CTC ef-
fectively avoids the cumbersome length prediction of a target se-
quence [17], which is rather challenging in ASR [27, 29]. However,
there are some drawbacks to this dependence on the CTC output.

Firstly, the performance of CTC limits that of Mask-CTC because
the MLM decoder can only make minor changes to the CTC out-
put, such as correcting spelling errors. Secondly, as the target length
is ﬁxed with that of the CTC output throughout the decoding, it is
difﬁcult to recover deletion or insertion errors.

To tackle these problems, we propose to (1) enhance the en-
coder architecture by adopting the state-of-the-art encoder architec-
ture, Conformer [15], and (2) introduce new training and decoding
methods for the MLM decoder to handle the insertion and deletion
errors during inference.

3.1. Conformer

Conformer integrates a convolution module and Macaron-Net style
feed-forward network (FFN) [35] into the Transformer encoder
block [16]. While the self-attention layer is effective at modeling
long-range global context, the convolution layer increases the abil-
ity to capture local patterns in a sequence, which is effective for
speech processing. We expect the Conformer encoder to boost the
performance of CTC and thus improve the overall performance of
Mask-CTC accordingly.

3.2. Dynamic length prediction (DLP)

3.2.1. Training

Inspired by [21, 36], we propose dynamic length prediction (DLP),
in which the MLM decoder is trained so as to dynamically predict
the length of a partial target sequence from a corresponding masked
token at each iteration. In addition to the mask prediction task in Eq.
(2), which is analogous to solving substitution errors, the decoder is
trained to solve simulated deletion and insertion errors.
Deletion-simulated task makes the decoder predict the length of a
partial target sequence, having “one or more” tokens, from a corre-
sponding masked token. For example, given a ground-truth sequence
Y = [y1, y2, y3, y4] and its masked sequence [y1, <MASK>, y4] by
merging y2 and y3 to <MASK>, the decoder is trained to predict a
symbol 2 from the masked position, which corresponds to the length
of the partial sequence [y2, y3]. This task makes the decoder aware
of the possibility that the masked token has one or more correspond-
ing output tokens, simulating a recover from deletion error in the
decoder inputs. To generate such masks, partial tokens in Y are
randomly sampled and replaced with <MASK> as Ymask in Eq. (2).
Then the consecutive masks are integrated into one single mask to
form a masked sequence Y del (|Y del|≤ |Y |), consisting of masked
tokens Y del
mask). The
target length labels, Ddel = {di ∈ Z|i = 1, ..., |Y del
mask|}, are ob-
tained from the above mask integration process. Ddel is predicted
conditioning on the observed tokens Y del

mask and observed tokens Y del

obs (= Y del \ Y del

obs as:

Plp(Ddel|Y del

obs , X) = Y
i

P (di|Y del

obs , X),

(5)

where Plp is a posterior probability distribution of the mask length.
Insertion-simulated task makes the decoder predict “zero” from
a masked token, indicating the mask corresponds to no partial tar-
get sequence. For example, given a ground-truth sequence Y =
[y1, y2, y3, y4] and a masked sequence [y1, y2, y3, <MASK>, y4], the
decoder is trained to predict a symbol 0 from the masked position
as there is no corresponding tokens in Y . This way, we can make
the decoder aware of the redundant masked token, simulating a re-
cover from insertion error in the decoder inputs. To obtain masks
for this task, we randomly insert <MASK> into Y to form a masked
sequence Y ins (|Y ins|> |Y |), consisting of masked tokens Y ins
mask
obs (= Y ins \ Y ins
and observed tokens Y ins
mask). The target lengths

Algorithm 1 Shrink-and-Expand Decoding
Input: K: iteration step, X: intput speech, ˆY = {ˆyl}L
1: Calculate Pl,mlm = Pmlm(yl = ˆyl| ˆY , X) for each ˆyl ∈ ˆY
2: Obtain ˆYmask = {ˆyl|Pl,mlm < Pthres}L

l=1: CTC output

3: Mask ˆY , where ˆY =

(

l=1
∅ (ˆyl ∈ ˆYmask) // <MASK>
(ˆyl ∈ ˆYobs = ˆY \ ˆYmask)
ˆyl

4: C = ⌊| ˆYmask|/K⌋ // #masks predicted in each loop
5: while stopping criterion not met do
6:
7:

// Shrink (Ex. [ˆy1, ∅, ∅, ∅, ˆy5, ∅, ˆy7] → [ˆy1, ∅, ˆy5, ∅, ˆy7])
Shrink masks in ˆY and update ˆYmask, ˆYobs accordingly
// Expand (Ex. [ˆy1, ∅, ˆy5, ∅, ˆy7] → [ˆy1, ∅, ∅, ˆy5, ˆy7])
Calculate Pl,lp(dl| ˆYobs, X) for each ˆyl ∈ ˆYmask
Expand masks in ˆY based on argmaxd Pl,lp(dl = d| ˆYobs, X)

and update ˆYmask, ˆYobs accordingly
Calculate Pl,mlm(yl| ˆYobs, X) for each ˆyl ∈ ˆYmask
Predict masks in ˆY as argmaxy Pl,mlm(yl = y| ˆYobs, X),
where ˆyl with top-C highest probabilities are selected

8:
9:
10:

11:
12:

Update ˆYmask, ˆYobs

13:
14: end while
15: return ˆY

Dins = {di = 0|i = 1, ..., |Y ins
dicted as:

mask|} of the inserted masks are pre-

dicted as the previous way in Eq. (4). Note that shrink-and-expand
decoding requires two forward calculations of the decoder for each
inference step: one for predicting the length of each mask and one
for predicting target tokens.

4. EXPERIMENTS

To evaluate the effectiveness of the proposed improvements of
Mask-CTC, we conducted experiments on E2E-ASR models using
ESPnet [37]. The recognition performance and the inference speed
were evaluated based on word error rate (WER) and real time factor
(RTF), respectively. All of the decodings were done without using
external language models (LMs).

4.1. Datasets

The experiments were carried out using three tasks: the 81 hours
Wall Street Journal (WSJ) [38], the 210 hours TEDLIUM2 [39]
and the 16 hours Voxforge in Italian [40]. For the network inputs,
we used 80 mel-scale ﬁlterbank coefﬁcients with three-dimensional
pitch features extracted using Kaldi [41]. To avoid overﬁtting, we
chose data augmentation techniques from speed perturbation [42]
and SpecAugment [43], depending on the tasks and models. For the
tokenization of target texts, we used characters for WSJ and Vox-
forge. For TEDLIUM2, we generated a 500 subword vocabulary
based on Byte Pair Encoding (BPE) algorithm [44].

Plp(Dins|Y ins

obs , X) = Y
i

P (di|Y ins

obs , X).

(6)

4.2. Experimental setup

Both tasks are trained jointly by a shared single linear layer fol-
lowed by a softmax classiﬁer (we set the maximum class to 50) on
top of the decoder. The objective of the overall DLP is formulated
by combining Eqs. (5) and (6) as:
LLP = log Plp(Ddel|Y del

obs , X) + log Plp(Dins|Y ins

obs , X).

(7)

Finally, a new Mask-CTC model is trained with a loss combining the
conventional objective LNAR from Eq. (3) and the objective of the
proposed DLP LLP from Eq. (7) as follows:

LNAR−LP = LNAR + βLLP,

(8)

where β (> 0) is a tunable parameter.

3.2.2. Inference

Alg. 1 shows the proposed shrink-and-expand decoding algorithm,
which allows delete and insert tokens during sequence generation.
Compared with the conventional method [29] (explained in Sec.
2.2), the proposed decoding differs in the masking process of the
CTC output and the prediction process of the masked tokens.

While the previous method detects low-conﬁdence tokens in a
CTC output by thresholding the posterior probabilities of CTC, the
proposed decoding refers to the probabilities of the MLM decoder
(line 2 in Alg. 1). Taking advantage of the bi-directional contexts of
tokens, it appeared that the decoder probabilities are more suitable
for detecting the CTC errors.

Shrink-and-expand decoding dynamically changes the target se-
quence length by deleting or inserting masks at each iteration. Shrink
step (line 7 in Alg. 1) integrates consecutive masks into one single
mask. The integrated masks are then fed to the decoder to predict the
length of each mask (line 9 in Alg. 1) required to match the length of
an expected target sequence. Expand step (line 10 in Alg. 1) mod-
iﬁes the number of each mask based on the predicted length. For
example, if the length of a mask is predicted as 2, we insert a mask
to form two consecutive masks, and if predicted as 0, we delete the
mask from the target sequence. Finally, the masked tokens are pre-

For all the tasks, we adopted the same Transformer [16] architec-
ture as in [12], where the number of heads H, the dimension of a
self-attention layer datt, the dimension of a feed-forward layer dﬀ
were set to 4, 256, and 2048, respectively. The encoder consisted
of 2 CNN-based downsampling layers followed by 12 self-attention
layers and the decoder consisted of 6 self-attention layers. For Con-
former encoder [15], we used the same conﬁguration for the self-
attention layer as in Transformer, except dﬀ was set to 1024.2 For
the model training, we tuned hyper-parameters (e.g., training epochs,
etc.) following the recipes provided by ESPnet and we will make our
conﬁgurations publicly available on ESPnet to ensure reproducibil-
ity. The loss weight α in Eq. (3) was set to 0.3 and β in Eq. (8) was
set to 0.1 for Voxforge and 1.0 for WSJ and TEDLIUM2. After the
training, a ﬁnal model was obtained by averaging model parameters
over the 10 – 50 checkpoints with the best validation performance.
During inference, the threshold Pthres for the conventional Mask-
CTC (explained in Sec. 2.2) was tuned from values of 0.9, 0.99,
0.999. For the proposed decoding in Sec. 3.2, we ﬁxed Pthres to 0.5
for all tasks. RTF was measured using utterances in dev93 of WSJ
using Intel(R) Xeon(R) Gold 6148 CPU, 2.40GHz.

4.3. Evaluated models

We evaluated different E2E-ASR models, each of which can either
be autoregressive (AR) or non-autoregressive (NAR). AR indicates
an AR model trained with the joint CTC-attention objective [12, 34]
and, for inference, we used the joint CTC-attention decoding [45]
with beam search. CTC indicates a NAR model simply trained with
the CTC objective as in Eq. (1) [26]. Mask-CTC indicates a NAR
model trained with the conventional Mask-CTC framework [29] as
explained in Sec. 2.2. We also applied the proposed dynamic length
prediction (DLP, explained in Sec. 3.2) to Mask-CTC.

For each model, we compared the results between Transformer

(TF) and Conformer (CF) encoders.

2We adjusted dﬀ to avoid the Conformer-based model becoming slow

during inference due to the increase in number of parameters.

Table 1. Word error rate (WER) and real time factor (RTF) on WSJ. The proposed improvements on Mask-CTC (the use of Conformer and
dynamic length prediction) are compared with CTC and autoregressive (AR) models. K denotes the number of inference steps required to
generate each output token. RTF was measured on dev93 using CPU. For each Mask-CTC model, RTF was calculated for K = 10.

Model

Autoregressive

A1 Transformer-AR [12]
A2
+ beam search
A3 Conformer-AR
A4
+ beam search

Non-autoregressive

B1 Transformer-CTC
B2 Transformer-Mask-CTC [29]
B3
+ dynamic length prediction

C1 Conformer-CTC
C2 Conformer-Mask-CTC
C3

+ dynamic length prediction

#Params
(M)

WSJ (WER)

dev93

eval92

RTF

Speedup

K = L (avg. 99.9)

K = L (avg. 100.3)

13.5
12.8
11.4
11.1

10.8
10.6
8.8
8.5

0.456±0.005
5.067±0.012
0.474±0.009
5.094±0.031

1.00×
0.09×
0.96×
0.09×

K ≤ C (C: const.)

K ≤ C (C: const.)

0

1

5

19.4
15.5
15.5

13.0
11.9
11.8

–
15.2
14.0

−
11.8
11.3

−
14.9
13.9

−
11.7
11.3

10

−
14.9
13.8

−
11.7
11.3

0

15.5
12.5
12.4

10.8
9.4
9.6

1

5

−
12.2
11.1

−
9.2
9.3

−
12.0
10.8

−
9.2
9.1

10

−
12.0
10.8

−
9.1
9.1

0.021±0.000
0.063±0.001
0.074±0.001

0.033±0.000
0.063±0.000
0.080±0.000

21.71×
7.24×
6.16×

13.81×
7.24×
5.70×

27.2
27.2
30.4
30.4

17.7
27.2
27.2

20.9
30.4
30.4

Table 2. Word error rates (WER) on Voxforge Italian and
TEDLIUM2. Results with beam search are reported in parentheses.

Table 3. Speech translation results on Fisher-CallHome Spanish

#itr K Voxforge

TEDLIUM2

Model

Fisher (BLEU)

CallHome (BLEU)

dev

dev2

test

devtest

evltest

Model

TF-AR [12]
CF-AR

TF-CTC
TF-Mask-CTC [29]

+ DLP

CF-CTC
CF-Mask-CTC

+ DLP

L
L

0
10
5

0
10
5

35.5 (35.7)
29.8 (29.8)

9.5 (8.9)
8.4 (7.9)

56.1
38.3
35.1

31.8
29.2
29.0

16.6
10.9
10.6

9.5
8.6
9.7

4.4. Results

Table 1 shows the results on WSJ. By comparing the results among
NAR models using Transformer (B*), the proposed DLP (B3) out-
performed the conventional Mask-CTC (B2). We can conclude that
DLP successfully recovered insertion and deletion errors in the CTC
output, looking at B2 and B3 having the same CTC results (K = 0)
and B3 resulted in better improvement. The performance of CTC
signiﬁcantly improved by using Conformer (B1, C1) and Mask-CTC
greatly beneﬁted from it (C2). The errors were further reduced by
applying DLP (C3), achieving 9.1% in eval92 which was the best
among the NAR models and better than that of the state-of-the-art
model without LM [46,47]. By comparing results between NAR and
AR models, Mask-CTC achieved highly competitive performance to
AR models for both Transformer (A1, B3) and Conformer (A3, C3),
demonstrating the effectiveness of the proposed methods for improv-
ing the original Mask-CTC.

In terms of the decoding speed using CPU, all NAR models (B*,
C*) were at least 5.7 times faster than the AR models (A*), achiev-
ing RTF of under 0.1. The speed of Mask-CTC slowed down by
applying DLP (B3, C3) due to the increase in number of decoder
calculations (explained in Sec. 3.2.2). However, with DLP, the error
rates converged faster in less iterations (K = 5) and thus the infer-
ence speed was the same or even faster than the original Mask-CTC.
Table 2 shows the results on Voxforge and TEDLIUM2. Mask-
CTC achieved comparable results with those of AR models by the
proposed improvements. However, DLP did not improve CF-Mask-
CTC on TEDLIUM2. We observed the performance of CF-Mask-
CTC was accurate enough and DLP was not effective.

TF-AR
TF-CTC
TF-Mask-CTC
+ CTC greedy
+ original decoding
+ mask-predict (MP)
+ restricted MP

47.01
45.57

47.89
46.97

47.19
45.97

45.93
44.80
47.43
49.94

46.82
45.40
48.14
49.42

46.17
44.39
46.96
48.66

18.11
15.99

15.73
14.14
16.52
16.96

17.95
15.91

15.60
14.14
16.42
16.79

4.5. End-to-end speech translation (E2E-ST)

To see a potential application to other speech tasks, we applied
Mask-CTC framework to the E2E-ST task, following [48]. For
NAR models, we used sequence-level knowledge distillation [49].
Table 3 shows the results on Fisher-CallHome Spanish corpus [50].
Since input-output alignments are non-monotonic in this task, we
observed the conﬁdence ﬁltering based on the CTC probabilities did
not work well, unlike ASR. Next, we performed the mask-predict
(MP) decoding proposed in MT [22] by starting from all <MASK>
and conﬁrmed some gains over CTC. Finally, we initialized a target
sequence with the ﬁltered CTC output as in the ASR task and then
performed the MP decoding. Here, the number of masked tokens at
each iteration are truncated by the number of initial masked tokens
Nmask (restricted MP) to keep information from the CTC output for
the later iterations. This way, the results were further improved from
the CTC greedy results by a large margin. Moreover, interestingly,
Mask-CTC outperformed the AR model on this corpus.

5. CONCLUSION

This paper proposed improvements of Mask-CTC based non-
autoregressive E2E-ASR. We adopted the Conformer encoder to
boost the performance of CTC and introduced new training tasks
for the model to dynamically delete or insert tokens during infer-
ence. The experimental results demonstrated the effectiveness of the
improved Mask-CTC, achieving competitive performance to autore-
gressive models with no degradation of inference speed. We also
showed Mask-CTC framework can be used for end-to-end speech
translation. Our future plan is to integrate an extrenal language
model into Mask-CTC while keeping the decoding speed fast.

6. REFERENCES

[1] Alex Graves et al., “Connectionist temporal classiﬁcation: la-
belling unsegmented sequence data with recurrent neural net-
works,” in Proc. of ICML, 2006, pp. 369–376.

[2] Alex Graves, “Sequence transduction with recurrent neural

networks,” arXiv preprint arXiv:1211.3711, 2012.

[4] Dzmitry Bahdanau et al.,

[3] Ilya Sutskever and other, “Sequence to sequence learning with
neural networks,” in Proc. of NeurIPS, 2014, pp. 3104–3112.
“Neural machine translation by
jointly learning to align and translate,” in Proc. of ICLR, 2015.
[5] Geoffrey Hinton et al., “Deep neural networks for acoustic
modeling in speech recognition: The shared views of four re-
search groups,” IEEE Signal processing magazine, vol. 29, no.
6, pp. 82–97, 2012.

[6] Chung-Cheng Chiu et al., “State-of-the-art speech recognition
with sequence-to-sequence models,” in Proc. of ICASSP, 2018,
pp. 4774–4778.

[7] Christoph L¨uscher et al., “RWTH ASR systems for librispeech:
Hybrid vs attention,” in Proc. of Interspeech, 2019, pp. 231–
235.

[8] Shigeki Karita et al., “A comparative study on Transformer
vs RNN in speech applications,” in Proc. of ASRU, 2019, pp.
449–456.

[9] Tara N Sainath et al.,

“A streaming on-device end-to-end
model surpassing server-side conventional model quality and
latency,” in Proc. of ICASSP, 2020, pp. 6059–6063.

[10] Linhao Dong et al., “Speech-Transformer: a no-recurrence
sequence-to-sequence model for speech recognition,” in Proc.
of ICASSP, 2018, pp. 5884–5888.

[11] Tara N Sainath et al., “Two-pass end-to-end speech recogni-

tion,” in Proc. of Interspeech, 2019, pp. 2773–2777.

[12] Shigeki Karita et al., “Improving Transformer-based end-to-
end speech recognition with connectionist temporal classiﬁca-
tion and language model integration,” in Proc. of Interspeech,
2019, pp. 1408–1412.

[13] Samuel Kriman et al., “Quartznet: Deep automatic speech
recognition with 1d time-channel separable convolutions,” in
Proc. of ICASSP, 2020, pp. 6124–6128.

[14] Wei Han et al., “ContextNet: Improving convolutional neu-
ral networks for automatic speech recognition with global con-
text,” in Proc. of Interspeech, 2020.

[15] Anmol Gulati et al.,

“Conformer: Convolution-augmented
Transformer for speech recognition,” in Proc. of Interspeech,
2020.

[16] Ashish Vaswani et al., “Attention is all you need,” in Proc. of

NeurIPS, 2017, pp. 5998–6008.

[17] Jiatao Gu et al., “Non-autoregressive neural machine transla-

tion,” in Proc. of ICLR, 2018.

[18] Jindˇrich Libovick´y and Jindˇrich Helcl,

“End-to-end non-
autoregressive neural machine translation with connectionist
temporal classiﬁcation,” in Proc. of EMNLP, 2018, pp. 3016–
3021.

[19] Jason Lee et al., “Deterministic non-autoregressive neural se-
quence modeling by iterative reﬁnement,” in Proc. of EMNLP,
2018, pp. 1173–1182.
[20] Mitchell Stern et al.,

“Insertion Transformer: Flexible se-
quence generation via insertion operations,” in Proc. of ICML,
2019, pp. 5976–5985.

[21] Jiatao Gu et al.,

“Levenshtein Transformer,”

NeurIPS, 2019, pp. 11181–11191.

in Proc. of

[22] Marjan Ghazvininejad et al., “Mask-predict: Parallel decoding
of conditional masked language models,” in Proc. of EMNLP-
IJCNLP, 2019, pp. 6114–6123.
[23] Marjan Ghazvininejad et al.,

“Semi-autoregressive train-
arXiv preprint

ing improves mask-predict decoding,”
arXiv:2001.08785, 2020.

[24] Chitwan Saharia et al., “Non-autoregressive machine transla-
tion with latent alignments,” arXiv preprint arXiv:2004.07437,
2020.

[25] Xuezhe Ma et al.,

“FlowSeq: Non-autoregressive condi-
tional sequence generation with generative ﬂow,” in Proc. of
EMNLP-IJCNLP, 2019, pp. 4273–4283.

[26] Alex Graves and Navdeep Jaitly, “Towards end-to-end speech
recognition with recurrent neural networks,” in Proceedings of
ICML, 2014, pp. 1764–1772.

[27] Nanxin Chen et al., “Listen and ﬁll in the missing letters: Non-
arXiv

autoregressive Transformer for speech recognition,”
preprint arXiv:1911.04908, 2019.

[28] William Chan et al., “Imputer: Sequence modelling via impu-

tation and dynamic programming,” in Proc. of ICML, 2020.

[29] Yosuke Higuchi et al., “Mask CTC: Non-autoregressive end-
to-end ASR with CTC and mask predict,” in Proc. of Inter-
speech, 2020.

[30] Yuya Fujita et al., “Insertion-based modeling for end-to-end
automatic speech recognition,” in Proc. of Interspeech, 2020.
“Spike-triggered non-autoregressive
Transformer for end-to-end speech recognition,” in Proc. of
Interspeech, 2020.

[31] Zhengkun Tian et al.,

[32] Eric Battenberg et al., “Exploring neural transducers for end-
to-end speech recognition,” in Proc. of ASRU, 2017, pp. 206–
213.

[33] Jacob Devlin et al., “BERT: Pre-training of deep bidirectional
transformers for language understanding,” in Proc. of NAACL-
HLT, 2019, pp. 4171–4186.

[34] Suyoun Kim et al.,

speech recognition using multi-task learning,”
ICASSP, 2017, pp. 4835–4839.

“Joint CTC-attention based end-to-end
in Proc. of

[35] Yiping Lu et al., “Understanding and improving transformer
from a multi-particle dynamic system point of view,” in Proc.
ICLR, 2020.

[36] Yi Ren et al., “FastSpeech: Fast, robust and controllable text

to speech,” in Proc. of NeurIPS, 2019.

[37] Shinji Watanabe et al., “ESPnet: End-to-end speech processing
toolkit,” in Proc. of Interspeech, 2018, pp. 2207–2211.
[38] Douglas B Paul and Janet M Baker, “The design for the wall
street journal-based CSR corpus,” in Proc. of Workshop on
Speech and Natural Language, 1992, pp. 357–362.

[39] Anthony Rousseau et al., “Enhancing the TED-LIUM corpus
with selected data for language modeling and more TED talks,”
in Porc. of LREC, May 2014, pp. 3935–3939.

[40] “Voxforge,” http://www.voxforge.org.
[41] Daniel Povey et al., “The Kaldi speech recognition toolkit,” in

Proc. of ASRU, 2011.

[42] Tom Ko et al., “Audio augmentation for speech recognition,”

in Proc. of Interspeech, 2015.

[43] Daniel S Park et al., “SpecAugment: A simple data augmen-
tation method for automatic speech recognition,” in Proc. of
Interspeech, 2019, pp. 2613–2617.

[44] Rico Sennrich et al., “Neural machine translation of rare words

with subword units,” in Proc. of ACL, 2016, pp. 1715–1725.

[45] Takaaki Hori et al., “Joint CTC/attention decoding for end-to-
end speech recognition,” in Proc. of ACL, 2017, pp. 518–529.
[46] Sara Sabour et al., “Optimal completion distillation for se-

quence learning,” in Proc. of ICLR, 2019.

[47] Lasse Borgholt et al., “Do end-to-end speech recognition mod-
els care about context?,” in Proc. of Interspeech, 2020.
[48] Hirofumi Inaguma et al., “ESPnet-ST: All-in-one speech trans-
lation toolkit,” in Proc. ACL: System Demonstrations, 2020,
pp. 302–311.

[49] Yoon Kim et al., “Sequence-level knowledge distillation,” in

Proc. of EMNLP, 2016, pp. 1317–1327.

[50] Matt Post et al., “Improved speech-to-text translation with the
Fisher and Callhome Spanish–English speech translation cor-
pus,” in Proc. of IWSLT, 2013.

