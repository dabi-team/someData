HAVEN: A Unity-based Virtual Robot Environment to Showcase HRI-based
Augmented Reality

Andre Cleaver1, Darren Tang2, Victoria Chen2, Jivko Sinapov2
1Department of Mechanical Engineering
2Department of Computer Science

0
2
0
2

v
o
N
6

]

O
R
.
s
c
[

1
v
4
6
4
3
0
.
1
1
0
2
:
v
i
X
r
a

Abstract

Due to the COVID-19 pandemic, conducting Human-Robot
Interaction (HRI) studies in person is not permissible due to
social distancing practices to limit the spread of the virus.
Therefore, a virtual reality (VR) simulation with a virtual
robot may offer an alternative to real-life HRI studies. Like a
real intelligent robot, a virtual robot can utilize the same ad-
vanced algorithms to behave autonomously. This paper intro-
duces HAVEN (HRI-based Augmentation in a Virtual robot
Environment using uNity), a VR simulation that enables users
to interact with a virtual robot. The goal of this system de-
sign is to enable researchers to conduct HRI Augmented Re-
ality studies using a virtual robot without being in a real envi-
ronment. This framework also introduces two common HRI
experiment designs: a hallway passing scenario and human-
robot team object retrieval scenario. Both reﬂect HAVEN’s
potential as a tool for future AR-based HRI studies.

INTRODUCTION
To establish common ground between robot and human, the
human needs to understand the robot’s intention and be-
haviors. However, it is difﬁcult for robots to interpret hu-
man intentions and effectively express their own intentions
(Williams et al., 2019). To break the communication barrier,
there are studies that utilize Augmented Reality (AR) to en-
able robot data that would typically be “hidden”

to be visualized by rendering graphic images over the
real world using AR-supported devices (e.g., smartphones,
tablets, the Microsoft HoloLens, etc.) (Frank, Moorhead,
and Kapila, 2017). However, AR technology requires access
to a physical robot and a real-life location. When real-life
HRI experiment is not accessible, a VR simulation with a
virtual robot offers an alternative way to conduct HRI stud-
ies remotely.

VR is a computer-generated simulation where a person
can interact within an artiﬁcial 3D environment. In VR based
HRI, a virtual robot and graphic images can resemble the
presence in a real-life location (Fang, Ong, and Nee, 2014)
There are several advantages to using a virtual robot. A
virtual robot can utilize advanced algorithms to behave au-

Figure 1: An AR visualization of the robot (Turtlebot2)’s
path. White cubes represent the arc motion while the white
spheres show straight motion.

tonomously like a real robot. For example, studies have com-
pared the interactive behaviors of robots in a VR environ-
ment and a real environment. The results conﬁrm that the
behaviors of a virtual robot and a robot in a real environment
are the same (Mizuchi and Inamura, 2017). Virtual robots
can also reduce the cost of experiments. Since multiple par-
ticipants can log in to the VR environment and interact with
the virtual robot at the same time, this can minimize the re-
sources needed to conduct the studies in a real environment
(Mizuchi and Inamura, 2017).

In this paper, we introduce HAVEN (HRI-based Augmen-
tation in a Virtual robot Environment using uNity), a VR
simulation that enables users to interact with a virtual robot.
We use Unity1

, a game development engine, to create an interactive VR
environment containing a virtual robot and its sensory data
and cognitive output that is portrayed with a different type
of graphic visualizations. In a technical demonstration of the
system, we show that we can create a virtual robot that can

Copyright © 2020, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.

1https://unity.com/

 
 
 
 
 
 
SYSTEM ARCHITECTURE AND DESIGN
HAVEN2 is a modular virtual environment designed to sup-
port many different aspects of robotic AR simulation. The
functional capabilities of HAVEN include:

• Simulate various robot agents of choice. Currently the

TurtleBot2 robot is used.

• Host a variety of HRI scenarios. HAVEN includes Hall-
way Passing and Object Collection in single room scenar-
ios. Different environments are in development.

• Customizable scripts for data visualization and robotic be-

haviors.

Software
Built fully in the Unity 2018.4 game development engine, all
robot AI and other functionality are fully implemented using
Unity’s native C# script. Researchers who have Unity can
clone the repository and edit the source code to implement
their own algorithms and virtual hardware. The ﬁnal envi-
ronment can either be hosted on a cloud service or partici-
pants can download a zip that itself contains the executable
and all dependencies based on operating system. This ﬂex-
ibility in distribution allows a wide variety of potential par-
ticipants across several different platforms.

Robot Agent
The desired robot will be able to be chosen from a list.
The given design for these scenarios is a Turtlebot2, cur-
rently only equipped with autononmous navigation. The AI
is scripted to replicate its motion closely to its real world
counterpart.

Movement Scripting The controller script of the Turtle-
Bot2 agent is built on Unity’s Navigation Mesh (NavMesh)
System. NavMesh scans the entire environment and lays a
grid atop all navigatable surfaces. NavMeshAgents can then
take that grid and mark a destination so long as there ex-
ists a valid path and will start to approach it. However, the
agent will simultaneously move directly toward the destina-
tion and rotate such that to face the destination. To create a
more realistic movement, the TurtleBot2 ﬁrst calculates the
signed angle between the direction it is facing and the di-
rection it must face to be looking at the next destination. If
the angle’s absolute value is above a certain threshold or the
destination is too close, then the TurtleBot2 will stop, rotate
until it is facing its next destination, then continue forward.
Otherwise, the TurtleBot2 will diminish the angle between
itself and the destination by traveling in an arc motion until
it faces the destination.

Battery Level An optional battery feature, designed to add
an extra layer of depth to navigation, is included that is cur-
rently designed to decrease in charge relative to translational
motion, however different actions . For mobile robots they
will always keeps track of how far it is away from its charg-
ing base. There are two different modes:

2The HAVEN repository is available at https://github.com/tufts-

ai-robotics-group/HAVEN

Figure 2: Examples of AR visualizations projected by the
Turtlebot2 robot. Thought bubble is rendered on top. Battery
indicator is in green in the middle, and the yellow arrow are
used for motion intent.

act autonomously with advanced algorithms relying only on
Unity. We also demonstrated two popular experimental stud-
ies in HRI, such as the hallway passing scenario and the
human-robot object retrieval scenario.

RELATED WORK

AR provides computer-generated graphics on a user’s view
of the real world. In the context of mobile robots, the AR
robotics system enables users and robots to communicate
through a shared reality space that merges the robot’s vi-
sual world with the human’s (Muhammad et al., 2019). The
system consists of different visualizations that represent sen-
sory and cognitive data that is rendered on the user interface.
Other studies utilize AR interface to support human-multi-
robot collaboration, where the robots collaborate with both
the robot and human teammates (Chandan, Li, and Zhang,
2019).

Virtual robots have been utilized in VR-based HRI stud-
ies. For example, previous frameworks have combined both
robot control (e.g., ROS) and virtual environment display
and interaction (e.g., Unity) (Codd-Downey et al., 2014).
VR technology enables the reduction of cost in software
development as well as other applications such as cloud-
based robotics (Mizuchi and Inamura, 2017) and teleoper-
ation tasks (Codd-Downey et al., 2014). Other studies use
Unity as the primary authoring tool for a functional virtual
robot (Mattingly et al., 2012).

HAVEN is a continuation of these previous works, merg-
ing the AR graphic interface and a functional virtual robot in
VR environment. Compared to the other systems, HAVEN
allows the robot to leverage the AR virtualization to commu-
nicate with the users. Using only Unity 3D, HAVEN offers
simplicity when it comes to building the virtual robot and
the VR environment, allowing non-programmers to create
VR simulation environment to ﬁt their needs.

• After reaching a destination, before calculating the path
to the next destination the TurtleBot2 will calculate the
following:

– How far it is from the next destination
– How far the next destination is from the base

If the sum of those calculations exceeds the distance re-
maining on the battery, the TurtleBot2’s next destination
becomes the charging base. From there, it will resume
reaching the rest of the destinations.

• Every frame the TurtleBot2 compares the distance from
the charging base and the remaining distance the battery
can travel. As soon as the remaining battery life can travel
the distance to the charging base, the TurtleBot2 abandons
its current path and reroutes to the charging base.

Visualization Options
HAVEN is currently equipped with 4 different types of visu-
alization: Path Projection, Turn Signals, Thought Bubbles,
and Battery Level Indicator

Path Projection (Path, Steps to Project) Given the
TurtleBot2’s destination and its rotation requirements, the
path is broken up into small segments (the length of the des-
ignated markers) and a marker is placed on each segment.
The location of each of those markers acts as an intermittent
destination when given to NavMesh. The number of mark-
ers that appear on-screen at a given time can be speciﬁed as
a parameter, and there are different markers for a linear and
curved motion (See ﬁgure 1).

Turn Signals (Angle) The sign of the angle calculated be-
tween the TurtleBot2 and the destination determines the di-
rection the TurtleBot2 will turn or rotate, and the appropriate
direction arrow will appear (See ﬁgure 2).

Thought Bubbles (Message string to project) When the
human agent gets close enough to the TurtleBot2, the mes-
sage for the TurtleBot2 to display appears above it in a comic
style thought bubble that always faces the human agent (See
ﬁgure 2).

Battery Level Indicator (Battery Level) Similar to the
thought bubble, battery level is indicated by a 2D image on
on the TurtleBot2 that always faces the human agent (See
ﬁgure 2).

APPLICATIONS
HAVEN simulates real-world objectives related to HRI
in a format that allows for additions, modiﬁcations, and
widespread distribution for various questions and research
purposes. Scenarios are provided as separate Unity scenes
built off an initial scenario template. Two example scenar-
ios are included: Hallway traversal and Shared room object
retrieval.

Hallway Passing Scenario
The Hallway Passing Scenario requires both agents to co-
ordinate their movements in order to avoid collision with

Figure 3: First person view of the hallway passing scenario.
The TurtleBot2 detects the human (obstacle) blocking its
path and therefore re-plans its path trajectory to the goal. The
direction it decides to move around the obstacle is given by
the rendered yellow arrow to communicate its motion intent.

each other while attempting to reach the other side. HRI re-
searchers have approached human prediction of robot mo-
tion intent through external robot modiﬁcations, movement
behavior scripts, and many other methods. (Pacchierotti,
Christensen, and Jensfelt, 2005; Watanabe et al., 2015; Fer-
nandez et al., 2018). In HAVEN, the TurtleBot2’s motion
behavior can be modiﬁed using scripts, and it can use visu-
alizations to inform the others of its decisions and intent.

TurtleBot2’s goal
In this scenario, the only visualization
utilized is the turn signal. The TurtleBot2’s main objective
is to reach the center of every room. The TurtleBot2 travels
down the center of every hallway unless the human agent is
blocking the center path, in which case the TurtleBot2 will
start to curve to avoid the human, using the turn signals to
indicate its desired direction (see ﬁgure 3).

Human’s goal The human’s main goal is also to reach the
center of every room. In harder scenarios, the human must
reach the center of a room before the TurtleBot2 does in
order to remove an obstruction that sits in the TurtleBot2’s
path. The human will always start in a different room as the
TurtleBot2 and does not know where the TurtleBot2 starts.

Shared Room Object Retrieval
This scenario was inspired by Walker et al. (2018) who con-
ducts a study in which a participant collects beads from a
set of stations with a monitor drone. Both agents share the
goal of collecting their respective set of gems that are scat-
tered around the room. The TurtleBot2 selects a gem to set
its destination to and proceeds to collect it. It will not collect
gems it collides with on the way. The human must stand in
the gem for a given duration before it is collected. If the hu-
man collides with Turtlebot2’s path projection it will reroute
to curve around the human, unless the human is within a
certain distance of either the Turtlebot2 or its destination, in
which case it must wait until the human moves away from
the path. If the human moves away before the Turtlebot2
reaches the rerouted section it will revert back to its original
path (See ﬁgure 4).

In addition, we will consider the types of information that
are useful for data collection. We based all our initial work
on the TurtleBot2, we plan on extending support for dif-
ferent types of robots as well i.e. drones and manipulators.
We expect our system to become a useful tool for HRI re-
searchers to further investigate the effectiveness of proposed
algorithms in regard to a robot’s movement and how visual-
izing various robot data can improve human-robot collabo-
ration.

References
Chandan, K.; Li, X.; and Zhang, S. 2019. Negotiation-based
human-robot collaboration via augmented reality. arXiv
preprint arXiv:1909.11227.

Codd-Downey, R.; Forooshani, P. M.; Speers, A.; Wang, H.;
and Jenkin, M. 2014. From ros to unity: Leveraging robot
and virtual environment middleware for immersive tele-
operation. In 2014 IEEE (ICIA), 932–936. IEEE.

Fang, H.; Ong, S.-K.; and Nee, A. Y. 2014. Novel ar-based
interface for human-robot interaction and visualization.
Advances in Manufacturing 2(4):275–288.

Fernandez, R.; John, N.; Kirmani, S.; Hart, J.; Sinapov, J.;
and Stone, P. 2018. Passive demonstrations of light-based
robot signals for improved human interpretability. In 2018
27th IEEE (RO-MAN), 234–239. IEEE.

Frank, J. A.; Moorhead, M.; and Kapila, V. 2017. Mobile
mixed-reality interfaces that enhance human–robot inter-
action in shared spaces. Frontiers in Robotics and AI 4:20.
Mattingly, W. A.; Chang, D.-j.; Paris, R.; Smith, N.; Blevins,
J.; and Ouyang, M. 2012. Robot design using unity for
In 2012 17th
computer games and robotic simulations.
(CGAMES), 56–59. IEEE.

Mizuchi, Y., and Inamura, T. 2017. Cloud-based multimodal
human-robot interaction simulator utilizing ros and unity
frameworks. In 2017 IEEE/SICE (SII), 948–955. IEEE.
Muhammad, F.; Hassan, A.; Cleaver, A.; and Sinapov, J.
In 2019
ISSN:

2019. Creating a Shared Reality with Robots.
14th ACM/IEEE Int. Con. on (HRI), 614–615.
2167-2121.

Pacchierotti, E.; Christensen, H. I.; and Jensfelt, P. 2005.
Human-robot embodied interaction in hallway settings: a
pilot user study. In ROMAN 2005, 164–171. IEEE.

Walker, M.; Hedayati, H.; Lee, J.; and Szaﬁr, D. 2018. Com-
municating robot motion intent with augmented reality. In
Proceedings of the 2018 ACM/IEEE Int. Conf. on (HRI),
316–324.

Watanabe, A.; Ikeda, T.; Morales, Y.; Shinozawa, K.;
2015. Communicat-
In 2015 IEEE/RSJ

Miyashita, T.; and Hagita, N.
ing robotic navigational intentions.
(IROS), 5763–5769. IEEE.

Williams, T.; Szaﬁr, D.; Chakraborti, T.; and Phillips, E.
2019. Virtual, augmented, and mixed reality for human-
robot interaction (vam-hri). In 2019 14th ACM/IEEE Int.
Conf. on (HRI), 671–672. IEEE.

Figure 4: First person view of object retrieval scenario. The
red circle on the ﬂoor is the TurtleBot2’s charging base.
The white spheres show the TurtleBot2’s path trajectory.
Blue gems are robot collectibles. Red gems are human col-
lectibles.

DISCUSSION
Especially given the circumstances of COIVD-19, there are
many beneﬁts to working in simulation over a real-world
machine. Robots and visualizations that are currently in the
theoretical side can be tested, and studies can be performed.
Without being restricted to a lab, participants can access and
perform the study from anywhere, either hosted on the web
or from downloading the executable and dependencies along
with instructions in single zip ﬁle. Each study can be fully
encapsulated in its own program, given sufﬁcient instruc-
tions to participants through text prompts and manuals. The
visualizations and scenarios shown in this paper are only the
basics of what HAVEN can be capable of. Although not as
realistically accurate as most other simulations built on more
robotics-oriented software, the versatility of building solely
in Unity removes many entry barriers for participants and
can be reached by a signiﬁcantly larger audience as either the
technical expertise or the proper equipment are required. Not
only in terms of participants, but to researchers and develop-
ers as well. With Unity knowledge and experience, current
robots, visualizations, and scenarios can either be modiﬁed
or built upon to ﬁt speciﬁc needs or new ones can be added
all together. With more control over the environment, the
range of collectible data goes beyond that of robot sensory
input, and many environmental variables can be controlled.

CONCLUSION AND FUTURE WORK
This paper introduced HAVEN, a VR simulation that en-
ables researchers to conduct AR-based HRI studies to over-
come the challenges of social-distancing mandates. With
this tool, users can interact with a virtual intelligent robot
and see its sensory and cognitive output that is rendered with
graphic visualizations projected over a VR world. Then we
described two common popular scenarios for experimental
AR studies in HRI such as the hallway passing scenarios
and the human-robot object retrieval scenarios.

For future work, we plan to develop a more immersive
user experience by building HAVEN into head-mounted dis-
plays. We also plan on introducing a teleoperation frame-
work for real-time studies that will grant researchers con-
trol of the robot, allowing them to interact with participants.

