ON THE PARAMETER ESTIMATION OF ARMA(p,q) MODEL BY 

APPROXIMATE BAYESIAN COMPUTATION 

LINGHUI LI1, ANSHUI LI2, and HUIZENG ZHANG3* 

1Department of mathematics, school of science, Hangzhou Normal University, Hangzhou Zhejiang, 
P.R. China 
2Department of mathematics, school of science, Hangzhou Normal University, Hangzhou Zhejiang, 
P.R. China 
3Department of mathematics, school of science, Hangzhou Normal University, Hangzhou Zhejiang, 
P.R. China 

Keywords: ARMA(p,q) model, ABC algorithm, Maximum Likelihood estimation, Time Series 

*Corresponding author 

Abstract.  In  this  paper,  the  parameter  estimation  of  ARMA(p,q)  model  is  given  by  approximate 
Bayesian  computation  algorithm.  In  order  to  improve  the  sampling  efficiency  of  the  algorithm, 
approximate  Bayesian  computation  should  select  as  many  statistics  as  possible  with  parameter 
information in low dimension. Firstly, we use the autocorrelation coefficient of the first p+q order 
sample  as  the  statistic  and  obtain  an  approximate  Bayesian  estimation  of  the  AR  coefficient, 
transforming  the  ARMA(p,q)  model  into  the  MA(q)  model.  Considering  the  first  q  order  sample 
autocorrelation functions and sample variance as the statistics, the approximate Bayesian estimation 
of  MA  coefficient  and  white  noise  variances  can  be  given. The  method  mentioned  above  is  more 
accurate and powerful than the maximum likelihood estimation, which is verified by the numerical 
simulations and experiment study. 

1.  INTRODUCTION 

Maximum likelihood estimation is one common parameter estimation method for time series 
ARMA(p,q) model. This method is relatively complex with large estimation error in high-order case. 
Graupe et.al proposed to fit the ARMA model into a higher order AR model by the least square method 
[1]. By solving the compatible algebraic equations, the parameter estimates of the ARMA model can 
be obtained, but the accuracy of the estimations is not high. In order to improve the accuracy, two-
stage least squares method is given by Z Deng et al [2], but the convergence rate was relatively slow. 
Based on affine geometry algorithm [3], L.Shen obtained accurate linear and non-linear ARMA model 
parameter estimation. 

Approximate Bayesian computation (ABC in short) is a novel algorithm in Bayesian statistic, 
sampling the approximate posterior distribution of parameters with rejection method. Rubin first gave 
the basic idea of approximate Bayesian [6] in 1984, Tavaré et. al brought the algorithm into statistical 
inference  of  DNA  data  [7]  later.  Pritchard  and  others  extended  this  algorithm  [8]  and  used  the 
extended version of  ABC algorithm to  study the variation of human  Y chromosome. Approximate 
Bayesian computation, regarded as a powerful method of parameter estimation for complex models, 
has  been  widely  used  in  population  genetics,  systems  biology,  epidemiology  and  systematic 
geography  [9,10]  since  2003.  In  order  to  improve  the  sampling  efficiency  of  the ABC  algorithm, 
sufficient  statistics  of  parameters  should  be  selected.  It  is  generally  difficult  to  obtain  sufficient 
statistics of parameters in most cases. Jean-Michel Marin et.al proposed the ABC method to estimate 

the parameters of the MA(q) model [11]. 

In this paper, the autocovariance function is selected as the statistic, and the estimation accuracy 
is  not  high  when  the  white  noise  variance  is  known. According  to  the  relationship  between  the 
autocorrelation function and the model coefficient of ARMA(p,q) model, this paper first obtain the 
estimation of the AR coefficient with ABC algorithm, transforming the ARMA(p,q) model into the 
MA(q) model. We then take the q order sample autocorrelation function and the sample variance as 
statistics, and get the estimation of the coefficient of MA(q) model and the variance of the white noise. 
Our method greatly improves the accuracy of estimation for the ARMA(p,q) model. In other words, 
this paper implement a simple and powerful approximate Bayesian algorithm to estimate ARMA(p,q) 
model parameters. 

This  paper  is  organized  as  follows:  some  necessary  techniques  of  ARMA(p,q)  model  and  the 
ABC  algorithm  which  will  be  used  in  the  sequel  are  briefly  described  in  Section  2  and  Section  3 
respectively; our main results are shown in Section 4; simulations and experimental study will be 
given in Section 5 and Section 6. 

2.  ARMA(p,q) MODEL 

Let us begin with some basic definitions and properties of the ARMA(p,q) model which will used 

in the sequel. Most of the definitions and properties list below can be found in [12]. 

Definition 1. Let {Yt : t∈T} be a time series in which T={0,  ±1,  ±2,

!

}, if 

"

t∈T, 

 t
e
Y
= +-
µq
t

e
-
q
1
t
1
-

e
-
2
t
-

2

q

-!

e
q t q

-

, 

we call such a series a moving average model of order q and abbreviate the name to MA(q), 

where

qq ¹

0

, 

te

  is a white noise series. The series {Yt : t∈T} is called the centralized MA(q) model 

when 

µ

=0. 

Let 

( ) 1
x
-
Q=-

2
x
x
- - !
q q
2

1

q

q
q

x

,  the 

xQ=

( ) 0

  is  called  the  characteristic  equation  of  the 

MA(q)  model,  the  MA(q)  model  is  reversible  if  and  only  if  the  root  of  the  characteristic  equation 

xQ=

( ) 0

  is outside the unit circle. 

Proposition 1. If MA(q) process is reversible, then 

iq

  satisfies: 

q <
i

i

qC i
,

,
1, 2,
= !

q

. 

Proof: Assume characteristic equation is 

( )
x
Q=-

1
-

x
- - =-
q q
2

x

1

2

!

q

x
-
q
q

(
1

x
-
l
1

)(
1

x
-
l
2

)(
1

l
3

x

)

(
1
!

l
q

x

)

the right-hand term of the upper formula is expanded 

1

æ
-+
ç
è

q

å

i

1
=

-
l
i

x

ö
÷
ø

æ
- ç
è

å
+
i
<

j

+
ll
i
j

2

x

ö
÷
ø

æ
ç
è

å

j k
< <

i

lll
j
k

i

ö
÷
ø

3

x

!

(

q

)
1

compared with the left-hand formula 

q
1

=

q
å
,
- =
l q
2
i
1
=

i

=

,
-
ll q
j
3

i

å

i

<

j

,
lll q
k
q

!

,

i

j

(

q

)
1

=
i

å

j k
< <

æ
ç
è

æ
ç
è

q

Õ

i

1
=

l
i

ö
÷
ø

q

x

,

q

Õ

i

1
=

l
i

. 

ö
÷
ø

 
 
for the MA(q) process is reversible, the root of the equation 

q
i

=

1
l
i

is outside the unit circle, 

i

1, 2,
= !

,

q

,

  so 

l <
i

1,

i

1, 2,
= !

q

.Then

(1
q <£ £

qC

i

i

i q

)

. 

q < =

1
q C

,q

1

,qCq <

2

2

,qCq <

3

3

,q
qCq <!

,

q

thus 

Proposition  2. The  relationship  between  the  autocorrelation  coefficient 

kr

  of  MA(q)  model 

and 

q £ £

i q

(
1i

)

  is as follows: 

r
k

q k
-

å

qq
i

k i
+

1
i
-
+
!

,1

k

q

2

+

q
q

=

0

k

1,
ì
ï
ï -+
q
k
ï
=£ £í
1
+
ï
ï
ï
î

0,

k

2
q
1
>

q

Definition 2. Let {Yt : t∈T} be a time series in which T={0,  ±1,  ±2,

!

}, if 

"

t∈T, 

Y
t

c
= +

Y
f
1
t

1
-

+

Y
f
2
t

-

2

+

!

+

f

Y
p t p

-

+

e
t

we call such a series an autoregressive model of order p and abbreviate the name to AR(p), where, 

pf ¹

0

  ,{ } is a white noise series. The series {Yt : t∈T} is called the centralized AR(p) model 

te

when c=0. 

Let 

( )
x
F=-

p

x

p

1
-

x
-
f
1

2
p
-
- - !
f
2

x

f
p

, the 

xF=

( ) 0

  is called the characteristic equation of 

the AR(p) model, the AR(p) model is stationary if and only if the root of the characteristic equation 

xF=

( ) 0

  is in the unit circle. 

Proposition 3. If AR(p) process is stationary, then 

if

  satisfies: 

f <
i

i

pC i
,

1, 2,
,
= !

p

. 

Proof: Similar to the proof of Proposition 1, we will not repeat it here. 

Definition 3. Let {Yt : t∈T} be a time series in which T={0,  ±1,  ±2,

!

}, if 

"

t∈T, 

Y
t

c
= +

Y
f
1
t

1
-

+

Y
f
2
t

-

2

+

!

+

f

Y
p t p

-

+

e q
e
-
1
t
t

1
-

-

q
2

e
t

-

2

-

!

-

q

e
q t q

-

We  call  such  a  series  an  autoregressive  moving  average  model  and  abbreviate  the  name  to 

ARMA(p,q), where 

pf ¹

0

,
qq ¹

0

,{ } is a white noise series and 

te

te

!

2
N s
(0,

)

,

2s

  is the variance 

of the white noise. The series {Yt : t∈T} is called the centralized ARMA(p,q) model when c=0. The 

ARMA(p,q) model studied in this paper refers to the centralized ARMA(p,q) model. 

Obviously, the ARMA(p,q) model degenerates into the MA(q) model when p=0; it analogously 
degenerates into a AR(p) model when q=0. So the AR(p) and MA(q) model are special cases of the 

 
 
 
ARMA(p,q) model, and the statistical properties of the ARMA(p,q) model are also a combination of 
the AR(p) and MA(q) model. 

Proposition 4. If ARMA(p,q) model is stationary, then 

if

  satisfies: 

f <
i

i

pC i
,

1, 2,
,
= !

p

. 

Proof: If ARMA(p,q) model is stationary, then the root of the equation 

xF=

( ) 0

  is in the unit 

circle. According to the Proposition 3, 

f <
i

i

pC i
,

,
1, 2,
= !

p

. 

Proposition 5. If ARMA(p,q) model is reversible, then 

iq

  satisfies: 

q <
i

i

qC i
,

1, 2,
,
= !

q

. 

Proof: If ARMA(p,q) model is reversible, then the root of equation 

xQ=

( ) 0

  is outside the unit 

circle. According to the Proposition 1, 

q <
i

i

qC i
,

,
1, 2,
= !

q

. 

Proposition  6.  The  relationship  between  the  autocorrelation  coefficient 

kr

  of  stationary 

ARMA(p,q) model and 

f £ £

(1

i

i

p

)

  is as follows: 

+

!

+

+

!

+

f r
1
p
f r
2
p

p

2

+

1
+-

=

=

+

r fr fr
ì
1
p
p
1
2
ï
r
+
fr fr
ï
1
p
p
+
í
!!!
ï
ï
r
=
î
-+
-

fr
1

p q
+

p q
+

+

2

p

1

1

fr
2

p q

+

!

+

f r
p
p

2

If two stationary ARMA(p,q) models have the same autocorrelation of order p+q, and 
r r
r
1
p
1
-
r
r
r
p
p
2
! ! ! !
r
q

r
p

!

!

!

p q
+-

1
+

p

1

é
ê
ê
ê
ê
r
ê
ë

ù
ú
ú
ú
ú
ú
û

is reversible, the AR coefficients of the two ARMA(p,q) models are the same. 

The above propositions are well-known properties, and no proof is given here. 

3.  APPROXIMATE BAYESISN COMPUTATION 

In order to illustrate the approximate Bayesian computation method, let's start with the Bayesian 

model. Assume  the  density  of 

X

=

(

X X
,
1

2

,

,
!

X

n

T

)

  is 

Xp

(
)
xq q

,  where 

q

  is  the  parameter  of 

model, 

x

=

(

x x
2,
1

,

,
!

x
n

T

)

.

q

  has  a  prior  density  of 

(
)
pq q

,  in  the  case  of 

X x=

  the  posterior 

density of 

q

  is 

p
q

X

(
)
x
q =

p

X

q

p

X

ò

q

(
x
(

p
q

(
)
q
( )
x t p t dt
q

)
q
)

According to the above formula, the calculation of posterior density involves the calculation of 
( )

. In complex models, this is very difficult. 

x t p t dt
q

(
)
xq q

  and 

likelihood function 

Xp

Xp

)

(

q

ò

 
 
 
Approximate Bayesian computation can avoid these two problems. Here we give a brief introduction. 
Approximate Bayesian computation, abbreviated as ABC, is a sampling simulation method of 
approximate posterior distribution. The most remarkable feature of this method is that it's easy to 

implement and does not need to calculate the likelihood function 

Xp

(
)
xq q

. Generally speaking, it 

is difficult to calculate the likelihood function and posterior density in many cases, while there is no 
specific or closed expression. In order to estimate the parameters of the complex statistical model, we 
can use the ABC algorithm to simulate posterior samples and replace the calculation of likelihood 
function with the information of these samples to obtain the estimation of model parameters. 

First, we give the basic ABC algorithm. 

Algorithm 1. (Basic ABC algorithm) 

input: samples 

*

x

n

RÎ

; the prior distribution 

( )
pq ×

  of 

q

; likelihood function 

(
);
Xp q q×

the distance 

),r × ×
(

  of 

nR

; an approximation parameter 

0d >

. 

output: samples (

,
, n
q q q!

2,

1

) of approximate posterior distribution

(
Xp
q q

x

)*

. 

algorithm steps: 
for i=1 to n do 
repeat 

generation 

'q

  based on prior distribution 

( )
pq ×

; 

generation 

x

  based on likelihood function 

(
)'
Xp q q×

. 

until 

)*
(
,x xr

d£

iq q=

'

end for 

The disadvantage of the basic ABC algorithm is that the probability of rejection of 

  is very 
high when the dimension of n is large, which greatly increases the computational difficulty. To solve 

q

this issue, the low-dimensional sufficient statistic 

( )
S x

d

RÎ

  of parameter 

q

  instead of x (d<<n) 

is used, which leads to an improved ABC algorithm. 

Algorithm 2. (Improved ABC algorithm) 

input: samples 

*

x

n

RÎ

; the prior distribution 

( )
pq ×

  of  ；likelihood function 

q

(
)
Xp q q×

; 

  the distance 

d × ×
( , )

  of 

dR

; an approximation parameter 

0d >

. 

output: samples (

, n
,
q q q!

2,

1

) of approximate posterior distribution 

| (
Xp
q q

|

*
x

)

. 

algorithm steps: 
  for i=1 to n do 
repeat 

 
 
 
generation 

'q

  based on prior distribution 

( )pq ×

; 

generation 

x

  based on    likelihood function 

Xp q q×
| ( |

'

)

. 

until 

*
d S x S x
( ( ),

(

))

d£

iq q=

'

end for 

It can be proved that the samples obtained is from posterior distribution when 

0d ®

. 

4.  ABC ALGORITHMS OF THE PARAMETER ESTIMATION OF ARMA(p,q) MODEL 

In this section, we will give our main algorithms for the parameters estimations of ARMA(p,q) 

model. 

Let {Yt : t∈T} is a stationary ARMA(p,q) model, 

Y
t

c
= +

Y
f
1
t

1
-

+

Y
f
2
t

-

2

+

!

+

f

Y
p t p

-

+

e q
e
-
1
t
t

1
-

-

q
2

e
t

-

2

-

!

-

q

e
q t q

-

where T={0,  ±1,  ±2,

!

}, 

pf ¹

0

,
qq ¹

0

,{ }  is  white  noise  series,  and 

te

te

!

2
N s
(0,

)

,

2s

is  the 

variance of the white noise. 

In  order  to  guarantee  the  uniqueness  of  the  model  coefficient  estimates,  we  assume  that  the 

model ARMA(p,q) is reversible.   

Let   

,
)p
f f f f

!

=

(

,

,

1

2

, 

,
)q
q q q q

!

=

(

,

,

2

1

, 

D f=
1 { :

the root of 

p

x

-

x
f
1

p

1
-

-

f
2

x

p

-

2

- -=!

f
p

0

is in the unit circle}, 

D q=
{ :

2

the root of 

1

-

x
-
q q
2

1

2

x

x
- - =!
q
q

q

0

  is outside the unit circle}.   

Obviously, the ARMA(p,q) model is stationary and reversible if and only if 

1DfÎ

,
2DqÎ

. 

In this paper, we assume that the prior distribution of 

f

  is a uniform distribution on 

1D

, the 

prior  distribution  of 

q

  is  a  uniform  distribution  on 

2D

,  the  prior  distribution  of 

s

  is  inverse 

Gamma  distribution,  that  is 

s

=

1
t

,
)
(
t a bG!

,

,

,
a b

is  a  parameter  of  gamma  distribution  and  is 

known. 

t q f

,

,

  is independence from one another. 

In the ABC algorithm, the first step is to generate a uniformly distributed 

f

  on 

1D

  and 

q

  on 

2D

.  According  to  the  Proposition  4,  if  the  ARMA(p,q)  model  is  stationary,  we  have

f

{
ÎD = ÎÂ
f

1

p

:

f
i

<

i

pC i
,

,
1, 2,
= !

}
p

. Thus we first simulate a random vector 

f

  which obeys the 

 
 
 
uniform  distribution  on 

1D

, 

then  determine  whether 

f

  satisfies  {the 

root  of

p

x

-

x
f
1

p

1
-

-

f
2

x

p

-

2

- -=!

f
p

0

is in the unit circle}, if satisfied then accept it, else refuse it. Thus an 

algorithm for simulating uniformly distributed random vectors on 

1D

  is obtained. According to the 

Proposition  5,  if  ARMA(p,q)  model  is  reversible,  then 

q

{
ÎD = ÎÂ
q

2

q

:

q
i

<

i

pC i
,

1, 2,
,
= !

}
q

. 

Similarly, we first simulate a random vector 

q

, which obeys the uniform distribution on 

2D

, then 

determine whether 

q

  is satisfied {the root of 

1

-

x
-
q q
2

1

2

x

x
- - =!
q
q

q

0

  is outside the unit circle}, 

if  satisfied  then  accept  it  ,else  refuse  it.  Thus  an  algorithm  for  simulating  uniformly  distributed 

random vectors on 

2D

  is obtained as follows. 

Algorithm 3. 
input: order p,q of the model 

output: generate a random vector 

f

  that obeys the uniform distribution on 

generate a random vector 

q

  that obeys the uniform distribution on 

1D

; 

2D

. 

Algorithm steps: 
1. repeat 

  generate a random vector 

f

  that obeys the uniform distribution on 

1D

until 

f

  satisfies: {the root of

p

x

-

x
f
1

p

1
-

-

f
2

x

p

-

2

- -=!

f
p

0

is in the unit circle} 

output 

f

2. repeat 

generate a random vector 

q

  that obeys the uniform distribution on 

2D

until 

q

  satisfies: {the root of 

1

-

x
-
q q
2

1

2

x

x
- - =!
q
q

q

0

  is outside the unit circle} 

q

output 
According  to  the  Proposition  6,  if  two  stationary  ARMA(p,q)  models  have  the  same 

autocorrelation of order p+q, and 

p

1
+

!

r r
r
1
p
1
-
r
r
r
p
p
2
! ! ! !
r
q

é
ê
ê
ê
ê
r
ê
ë

r
p

!

!

p q
+-

1

ù
ú
ú
ú
ú
ú
û

is  reversible,  the  AR  coefficients  of  the  two  ARMA(p,q)  models  are  the  same.  So  we  use  the 
autocorrelation coefficient of the first order p+q sample as statistics and use the ABC algorithm to 

estimate 

,
)p
f f f f

!

=

(

,

,

2

1

, the specific algorithm is as follows. 

 
 
 
 
 
Algorithm 4. 

input:  samples

*

y

=

(

*
y y
,
1

*
2

,

!

,

y

*

)n

;  order  p,q  of  the  model;  sample  size  N;  an  approximation 

parameter 

e

; the distance 

d × ×
( , )

; the variance of white noise series 

2s

. 

output: approximate Bayesian estimation 

Ù
f

  of 

. 
f

Algorithm steps: 

1. compute the autocorrelation coefficient of sample 

*y

, 

Ù
(
r

*

y

)

=

2. for i=1 to N do 
repeat 

according to the Algorithm 3, random generation 

)fq
( ,

*

*
Ù
Ù
(
y
( ),
r r
2

1

y
( ),

!

,

*
Ù
r

p q
+

y
( ))

. 

random  generation  the  time  Series 

y

=

(

y y
,
1
2

,

!

,

y
)n

  of  ARMA(p,q)  model  with 

coefficient  of 

)fq
( ,

,  white  noise  variance  of 

2s

,  calculating  the  autocorrelation 

coefficient of the series 

Ù
r

y
( )

=

Ù
(
r
i

y i
( ),

=

1, 2,

,
+!

p q

)

. 

until 

Ù
Ù
(
y
( ),
(
r r

d

*

y

))

£

e

if f=

end for 

3. approximate Bayesian estimation 

Ù
f

  of 

, 
f

Ù
f

1 N
= å
iN

1
=

f
i

. 

Let 

X

t

Y
=-
t

Y
-
f
t
1
1
-

Y
-
f
t
2
-

2

-!

f

Y
p t p

-

, then 

X

t

=

e
t

-

e
q
1
t

1
-

-

q
2

e
t

-

2

-

!

-

q
q

e
t q
-

is  a  MA(q)  model.  Assume 

Ù
f

  is  the  estimate  of 

f

  in  Algorithm  4,  bring  samples 

*

,

y

!

)n
}
i n

*

y

=

(

*
y y
,
1

*
2

,

  into 

X

t

Y
=-
t

Ù
f
1

Y
-
t
1
-

Ù
f
2

Y
-
2
t
-

-!

Ù
f
p

Y
t p
-

,t=p+1,p+2,

!

,n. We get the samples 

1
+£ £

* :
ix p

{
process is determined by the autocorrelation coefficient of the first q order, which is independent of 

.  According  to  the  Proposition  2,  The  model  coefficient 

  of  MA(q) 

  of 

tX

q

the  variance  of  white  noise.  So  when  estimating  the  parameter 

2
)qs
( ,

,  take  the  autocorrelation 

coefficient  of  the  sample's  first  q  order  as  statistics,  and  estimated  by  ABC  algorithm 

,
)q
q q q q

!

=

(

,

,

1

2

,  then  an  estimate  of 

2s

  is  given  by  using  sample  variance  as  a  statistic.  The 

specific algorithm is as follows. 

 
 
 
 
Algorithm 5. 

input: 

samples

*
x

=

(

*
x
p

1
+

,

*
x
p

+

2

,

,
!

*
x
n

)

,where 

*
x
p i
+

y
=-

*
p i
+

-

*
y
-
f
p i
1
- +
+

1

f
2

*
y
- - !
p i
2

f

*
y
p i

i
(

1, 2,
=- !

,

n p

)

;  order  q  of  the  model;  parameters 

,a b

  of  Gamma  distribution; 

sample size N; an approximation parameter 

e

; the distance 

d × ×
( , )

  of 

dR

. 

output: approximate Bayesian estimation 

Ù
Ù
2
,qs

  of 

2
,qs

. 

Algorithm steps: 

1. compute the autocorrelation coefficient of sample 

*x

, 

Ù
(
r

*
x

)

=

2. for i=1 to N do 
repeat 

*

*
Ù
Ù
(
x
( ),
r r
2

1

x
( ),

!

,

*
Ù
r
q

x
( ))

according to the Algorithm 3, random generation 

q

  random generation the time series 

x

=

(

x x
,
1
2

,

!

,

x -

n q

)

  of MA(q) model with coefficient 

of 

q

, white noise variance of 

2s

, calculating the autocorrelation coefficient of the series 

Ù
r

x
( )

=

Ù
(
r
i

x i
( ),

1, 2,
= !

q
, )

until 

Ù
Ù
( ( ),
x
(
r r

d

*
x

))

£

e
1

iq q=

'

end for   

3. compute approximate Bayesian estimation of 

, 
q

Ù
q

1 N
= å
iN

1
=

q
i

. 

4. compute the variance 

2*s

of samples 

*x

. 

5. for i=1 to N do 
repeat 

  generating random numbers 

(
)
t a bG!

,

. Let 

s

=

1
t

, 

  random generation the time series 

x

=

(

x x
,
1
2

,

!

,

x -

n q

)

  of MA(q) model with coefficient 

Ù
q

, white noise variance of 

of 

2s

, calculating the variance 

2s

  of samples; 

until 

2*

s s e
2

£

-

2

is s=

end for   

6. compute approximate Bayesian estimation for the variance of white noise

2
Ù
s

1 N
= å
iN

1
=

s
i

2

. 

 
 
 
 
 
 
 
 
5.  NUMERICAL SIMULATION 

To illustrate the effect of ABC parameter estimation, we choose ARMA(2,2) model for numerical 

simulation. Let a ARMA(2,2) model   

Y
t

=

Y
0.6
t

1
-

+

Y
0.2
+-
t
2
-

e
t

0.3
e
-
t

1
-

0.4
e
Î± ±
t

-

2

,

t

{
0, 1, 2,

}
!

where white noise series 

te

N!

2
(0, 2 )

. 

Firstly, time series samples 

*

x

=

(

*
*
x x
,
1
2

,

!

*
x
1000

)

  are generated according to the model above. 

According to the Algorithm 4, simulated generated samples 

f

  with size of 100000, and the threshold 

parameter is 

e=

0.0005

. We will choose the autocorrelation coefficient as parameter statistic, and 

the distance is ranked from small to large. The first 50 

f

  of the smallest distance is taken as the 

posterior sample, and the average value of the posterior sample is taken to obtain the estimated value 

of 

f

.  Then  by  Algorithm  5,  we  generate  a  sample  of   

q

  with  size  of  100000,  with  threshold 

parameter
nearest posterior sample of 30 

0.0003

e=

. The autocorrelation coefficient is used as parameter statistic and we take the 

q

, which is used to estimate the parameter 

. 
q

We also select a threshold parameter 

e=

0.0001

was simulated with the mean of this sample as the estimate of 

s

  and a posteriori sample of 

  with size of 10 
s
. Figure 1 and Figure 2 respectively 

represent  the  sample  scatter  plots  of 

(
f f f=
2

,

1

)

  and 

(
q q q=
2

,

1

)

  approximated  by  the  ABC 

algorithm.  Figure  3  and  Figure  4  respectively  represent  histograms  of 

f

  and 

q

  for  posterior 

distribution sampling. To verify the effectiveness of the ABC algorithm, we compare our results with 
the maximum likelihood method. The results are shown in Table 1. In conclusion, the sample points 

obtained  by  the  ABC  algorithm  are  closely  concentrated  in  the  vicinity  of 

f=

(0.6, 0.2)

, 

q=

(0.3, 0.4)

, which is more accurate than maximum likelihood estimation. 

Figure 1                                                                                                    Figure 2 

Figure 2. The sample scatter plots of 

f

  approximated by the ABC algorithm 

Figure 2. The sample scatter plots of 

q

  approximated by the ABC algorithm 

 
                           
 
Figure 3                                                                                              Figure 4 

Figure 3. histograms of 

f

  for posterior distribution sampling 

Figure 4. histograms of 

q

  for posterior distribution sampling 

Table 1. Comparison of the maximum likelihood estimation and ABC algorithm 

Maximum Likelihood Estimation 

ABC Estimation 

Estimation 

Relative Error 

Estimation 

Relative Error 

0.6236 

3.93% 

0.5932088 

1.13% 

0.1783 

10.85% 

0.2035063 

1.75% 

0.2729 

9.03% 

0.3015688 

0.52% 

0.4034 

0.85% 

0.4053529 

1.34% 

3.949 

1.28% 

3.953303 

1.17% 

1f

2f

1q

2q

2s

6.  EXAMPLE ANALYSIS 

The primary industry is an industry that produces natural objects and plays an important role in 
daily life. Figure 5 gives the time series diagram of the year-by-year growth of the primary industry. 
With ADF test of this sequence, we can show that the sequence is approximate stationary. We can 
select the alternative model through the ACF chart and PACF chart, such as Figure 6, and rank the 
model as ARMA(1,1). We take the year-by-year growth of the first industry from the first quarter in 
2006  to  the  first  quarter  in  2018  as  experiment  data.  Using  the  ARMA(1,1)  model  to  forecast  the 
quarters of 1~2,1~3,1~4 in 2018, and then comparing with the actual value to illustrate the effect of 
our model. 

                 
 
 
 
 
 
 
 
Figure 5                                                                                                  Figure 6 

Figure 5. The time series diagram of the year-by-year growth of the primary industry. 
Figure 6. The ACF and PACF of the year-by-year growth of the primary industry. 

The ARMA(1,1) model is as follows, 

"

t∈T: 

Y
t

c
= +

Y
f
t

1
-

e
+ -
t

e
q
t

1
-

, 

where  the  estimated  value  of 

te

~

2
N s
(0,

)

,  c  is  sample  average 

Ù
c

=

3.744

.  The  parameters  are 

estimated by algorithm 5 and we can obtain the estimated values of 

,fqs

,

  as follows: 

Y
t

=

3.744 0.03925049

+

Y
+-
1
t
-

e
t

e
0.6014705
t

1
-

, 

Where 

te N

2
~ (0, 0.5356927 )

. 

The fitted model was evaluated and its residual satisfies the condition of independent normal 
distribution (see Figure 7). It shows that the ARMA model can fit the data well. Next, the model is 
predicted. Table 2 gives the actual value, forecast value, relative error and confidence interval of year-
by-year growth of primary industry in the quarter of 1~2,1~3,1~4 in 2018, and the prediction effect 
is good. 

                 
 
 
Figure 7. The Q-Q Plot fitted by ARMA model 

Table 2. The prediction of the year-by-year growth of primary industry   

Time 

Q1~2,2018 

Q1~3,2018 

Q1~4,2018 

Actual 
Value 

Predicted 
Value 

Relative 
Error 

80% Confidence 
Interval 

95% Confidence 
Interval 

3.2 

3.4 

3.5 

3.276426 

2.388% 

(2.40,4.15) 

(1.94,4.61) 

3.416563 

0.487% 

(2.66,4.17) 

(2.26,4.57) 

3.438341 

1.762% 

(2.72,4.16) 

(2.34,4.53) 

Acknowledgement 

This  research  was  financially  supported  by  Zhejiang  Provincial  Natural  Science  Foundation  of 
China(No. LQ18A010007). 

References 

[1] Graupe D, Krause DJ, Moore J B. Identification of autoregressive moving-average parameter of 
time series. IEEE Trans Automatic Control, 1975;20:104-107. 

[2] Zili Deng, Jianwei Ma, Hongyue Du. Two-stage least squares method for parameter estimation of 
ARMA model. Science and Technology and Engineering, 2002 (05): 3-5. 

[3] Sheng Lu, Ki Hwan Ju, and Ki H. Chon. A New Algorithm for Linear and Nonlinear ARMA 
Model Parameter Estimation Using Affine Geometry. IEEE Transactions on Biomedical Engineering, 
Vol. 48, NO. 10, October 2001. 

[4] Voss. An Introduction to Statistical Computing, a simulation-based Approach. John Wiley Sons, 
Ltd, United Kingdom, WILEY, 2014:15-18,181-192. 

[5]  Beaumont,  MA,  Cornuet,  JM,  Marin,  JM,  et  al.  Adaptive  approximate  Bayesian  computation. 
Biometrika, 2009(4):983-990. 

[6] Rubin D. Bayesianly justifiable and relevant frequency calculations for the applied statistician. 
Ann. Stat,1984,12(4):1151-1172. 

[7] Tavaré S, Balding D, Griffith R, Donnelly P. Inferring coalescence times from DNA sequence 
data. Genetics,1997:505-518. 

[8]  Pritchard  J,  Seielstad  M,  Perez-Lezaun  A,Feldman  M.  Population  growth  of  human  Y 
chromosomes: a study of Y chromosome microsatellites. Mol.Biol.Evol,1999:1791-1798. 

[9]  Beaumont  MA,  Zhang  W,  Balding  DJ.  Approximate  Bayesian  Computation  in  population 
genetics. Genetics 2002(162):2025-2035. 

[10] Drovandi CC, Pettitt AN. Estimation of parameters for macroparasite population evolution using 
approximate bayesian computation. Biometrics: Journal of the Biometric Society : An International 
Society Devoted to the Mathematical and Statistical Aspects of Biology, 2011(1):225-233. 

[11]  Jean-Michel  Marin,  Pierre  Pudlo,  Christian  P.  Robert,  et  al.  Approximate  Bayesian 
computational methods. Statistics and computing, 2012(6):1167-1180. 

[12] James Douglas Hamilton. Time Series Analysis, Princeton University Press,1994. 

 
