Predicting Human Performance in Vertical
Hierarchical Menu Selection in Immersive AR
Using Hand-gesture and Head-gaze

Majid Pourmemar †, Yashas Joshi ‡, Charalambos Poullis
Immersive and Creative Technologies Lab, Department of Computer Science and Software Engineering
Concordia University
Montreal, Canada
majid.pourmemar@concordia.ca

2
2
0
2

n
u
J

9
1

]

C
H
.
s
c
[

1
v
0
8
4
9
0
.
6
0
2
2
:
v
i
X
r
a

Abstract—There are currently limited guidelines on designing
user interfaces (UI) for immersive augmented reality (AR) appli-
cations. Designers must reﬂect on their experience designing UI
for desktop and mobile applications and conjecture how a UI will
inﬂuence AR users’ performance. In this work, we introduce a
predictive model for determining users’ performance for a target
UI without the subsequent involvement of participants in user
studies. The model is trained on participants’ responses to ob-
jective performance measures such as consumed endurance (CE)
and pointing time (PT) using hierarchical drop-down menus.
Large variability in the depth and context of the menus is ensured
by randomly and dynamically creating the hierarchical drop-
down menus and associated user tasks from words contained
in the lexical database WordNet. Subjective performance bias
is reduced by incorporating the users’ non-verbal standard
performance WAIS-IV during the model training. The semantic
information of the menu is encoded using the Universal Sentence
Encoder. We present the results of a user study that demonstrates
that the proposed predictive model achieves high accuracy in
predicting the CE on hierarchical menus of users with various
cognitive abilities. To the best of our knowledge, this is the
ﬁrst work on predicting CE in designing UI for immersive AR
applications.

Index Terms—human performance, augmented reality, hierar-

chical menus

I. INTRODUCTION

In recent years, Augmented Reality (AR) has emerged as
one of the most promising technologies with applications in
gaming, education, and medicine [1], [2], [3]. Currently, AR
hardware is categorized in terms of: (a) mobile AR, where
the AR content is shown through hand-held tablets or smart-
phones, (b) spatial AR, which is based on projecting digital
content on physical objects, and (c) immersive AR, where see-
through head-mounted displays (HMDs) are used to augment
the user’s ﬁeld-of-view. Each category of devices allows users
to experience the real world overlaid with virtual, computer-
generated content. One of the most important characteristics
of AR is the interactions with virtual content [4]. In immersive
AR, this feature is supported through graphical User Interfaces
(GUI) and input modalities relying on natural interactions such
† Majid Pourmemar: conceptualization, methodology, software, data curation,
investigation, validation,
the manuscript.
Corresponding author. ‡ Yashas Joshi: software.

formal analysis, and writing of

as hand-gestures, head-gaze, eye-gaze, and voice recognition.
Thus, the design of GUIs and the choice of input modality
is of paramount importance for immersive AR applications,
in
and a determining factor for their usability. Moreover,
immersive AR, the interaction with the virtual content involves
considerable physical or mental activity [5], [6]. Therefore, it
is important to design these interactions in a way that reduces
the amount of overall workload on the users. This will be more
important for prolonged usage of an immersive headset, i.e.
performing a sequence of interactions to reach a ﬁnal goal.
As the applications become more complicated, containing
numerous actions and options, the popular choice amongst
designers and developers is to combine the aforementioned
menu options into groups and hierarchies(i.e. hierarchical
drop-down menus as shown in Figure 1).

Fig. 1: We present an AI model for predicting human perfor-
mance (pointing time and consumed endurance) in hierarchical
menu selection in immersive Augmented Reality applications.

Although drop-down menus in immersive AR can be re-
placed with more user-friendly menus like radial [7] or tile
menus [8], they are still widely used due to their familiar na-
ture. This is especially the case for developers who transfer the
two dimensional desktop menu layouts to three dimensional
immersive AR. This means that optimizing the placement of
the menu items in hierarchical drop-down menus can lead to
improved interactions in immersive AR. To achieve this, one

 
 
 
 
 
 
has to evaluate the performance of users of different cognitive
capacities on the menu.

User studies must be conducted during and after the menu
design process to provide insights into the usability concerning
the effectiveness, efﬁciency, and overall satisfaction of the
end-users. However, conducting user studies for immersive
AR applications is a time-consuming and costly process. The
participants must have access to an AR-capable HMD, e.g.
HoloLens, Magic Leap, etc, which considering the current
cost of HMDs is very unlikely. This restricts the type of
user studies to on-site, face-to-face usability testing. This can
be very time-consuming and expensive since participants are
often compensated for their time. This is especially the case
when an iterative design process is followed.

To overcome these challenges, we propose a predictive
model that determines the users’ performance for a target
GUI without any involvement of participants during the user
studies. The model
is trained on data acquired during a
user study involving a large number of hierarchical drop-
down menus, and the responses of participants to objective
performance measures.

A. Human Performance on Hierarchical Menu Selection

Hierarchical menus are widely used in a variety of ap-
plications ranging from simple web pages such as portals
and websites, to highly professional desktop software like 3D
authoring applications (i.e., Autodesk Maya and Unity3D).
Each phase of the software contains hundreds of actions and
options in a hierarchical menu format. According to the related
literature, [5], [9] selecting a desired menu item, especially
when it appears deep in the hierarchy, can be challenging.
Based on several studies [10], [11], a user’s performance on
menu selection in a desktop conﬁguration, depends on the
pointing time (PT). The PT is deﬁned as the time it takes
for the user to ﬁnd the desired menu option and point to it,
i.e. the time duration between the ﬁrst and the last click. This
is different for immersive AR/VR, since the input modalities
for interacting with the GUIs are different. VR mainly uses
controllers to interact with the GUI, while AR relies on head-
gaze or hand-gestures. Studies speciﬁc to immersive AR/VR
applications [5], [12] have shown that the user’s performance
for menu selection also depends on additional factors like
workload and error rate. The workload is deﬁned as physical
or mental effort required to point the cursor at the desired
menu item and select it. Since errors are more likely to occur
in immersive AR than in the desktop environment [6], the
error rate is also an important measure for evaluating the
performance. Increased error rates are attributed to the fact
that most interactions in immersive AR, are performed using
hand-gestures and pointing the head-gaze. This increases the
workload and ampliﬁes the possibility of an error during a
selection.

In this work, the user’s performance on a hierarchical GUI
is predicted considering the factors like expected PT and
physical workload corresponding to selecting a menu item
from a hierarchical drop-down menu through a sequence of

menu selection tasks in immersive AR. Inspired by related
work in the area, we consider the semantics and organization
of the menu items (i.e. number of levels, number of items
in each level, and number of characters for each target in
the every level) and the users’ cognitive ability in performing
visual tasks (WAIS-IV test).

Previous works on performance prediction in HCI tasks
categorize the measures into two sub-groups, user-related and
GUI-related factors. A brief description of such factors is
presented in this section.

1) User-related Factors: The user-related factors depend
on the capabilities of the users performing selection tasks.
Their learning ability, cognitive performance, and fatigue
are important factors that affect the performance of menu
selection. These factors are recorded during the user study
and considered in the ﬁnal neural network for predicting
the human performance. We applied a standard cognitive
approach, WAIS-IV test [13], to estimate each user’s mind
Processing Speed Index(PSI).

2) GUI-related Factors: The appearance of a menu is an
essential factor in the user performance during a selection task.
Shape of the menu (i.e., vertical list, radial) [5], saliency of
items within, their semantic meaning, and the organization in
general (i.e., number of levels) [11] are the most important
factors. In this work, we focus on the vertical lists such as
hierarchical menus with the maximum depth of 3 and consider
the saliency, semantics, and organization of menu items as a
part of the machine learning model input.

B. Contributions

The contributions of this work are as follows:
• To the best of our knowledge, this paper is the ﬁrst work
on predicting human performance of vertical hierarchical
menu selection in immersive AR incorporating both user-
related and GUI-related factors.

• This work is the ﬁrst work that considers the semantic re-
lationship of the targets at different levels in the hierarchy.
Previously, Li et al. [11], focused only on the semantics
of the words in vertical menus. We demonstrate that when
it comes to a hierarchical menu selection, the semantic
relationship between the targets in each menu level is an
important factor to consider.

II. LITERATURE REVIEW

Human performance is an important factor in designing
and developing user interfaces(UIs). The performance of a UI
can be calculated using statistical methods and user studies
with a certain number of participants [14]. Although the
performance of a UI can be evaluated using user studies,
predicting the performance using extensive mathematical and
machine learning models without testing it with real users
will be less time-consuming and less expensive. The related
work in predicting human performance in HCI is divided
into two broad categories, namely the traditional methods that
mostly use statistical and mathematical approaches such as
Fitts’ law [15] and Hick’s law [16] and the novel machine

learning methods such as [11], [17], [18] which use deep
neural networks to consider all user-related and GUI-related
factors.

A. Traditional Methods

There are a plethora of mathematical models presented to
predict human performance in HCI. Fitts’ [15], and Hick’s
law [16] are the two fundamental laws for predicting task
completion time and form the basis for numerous prediction
methods. The complex models such as [19] in the literature,
predict visual search time based on Fitts’ law. However, the
model [19] predicts visual search time in vertical menus
without considering the saliency of the items (i.e., number
of characters in a word), which is one of the important factors
in vertical menu selection. The work also does not consider
the semantic meaning of each item as an important factor.

Considering the visual appearance of GUIs, the authors in
[20] presented a computational model of visual search on
graphical layouts. They gave several examples and user studies
indicating how users learn GUIs and how changes in visual
design will affect the performance of the UI. Bailly et al. [19]
and Li et al. [11] call this measure as a learning effect and
consider it in vertical menu selection.

Regarding the immersive environments, in [21], the authors
proposed a mathematical model for predicting task execution
time for mid-air and touch-based human hand-gestures. Al-
though, it predicts the task completion time with R2 = 0.936
based on their own dataset gathered in a user study, they do
not consider predicting the performance in a sequence of tasks
with factors such as arm fatigue [12], learning effect [11] and
GUI-related factors i.e. semantics and menu appearance.

B. Machine Learning-based Methods

Recent advances in machine learning and deep learning
motivated HCI researchers to provide machine learning models
for HCI applications. Therefore, predictive models of human
performance are of signiﬁcant interest to many researchers.
Although traditional models perform very well in predicting
human performance in simple cases with the condition of
removing factors such as learning effect, semantics, task load
and user’s cognitive performance, they do not work well if all
possible effective factors are considered. In such situations,
the machine learning methods appeared to be resolving the
issues [22].

The authors in [11] presented a novel deep recurrent neural
network to capture semantics, learning effect and saliency of
menu items to predict visual search time in vertical menu
selection. The method outperformed previous traditional works
[19] in terms of accuracy and goodness of ﬁt (R2). However,
the method does not explain the performance of hierarchical
menu selection which is most common in many real use-
cases. Another work that uses recurrent neural networks, Wei
et al. [23] presented a deep model to predict the mouse-click
position in a sequence of interactions. The LSTM model was
trained on previous paths of the mouse and could predict the

future mouse-click area with a better accuracy and error rate
compared to previous works including [24] and [25].

The aforementioned methods perform well in desktop en-
vironments. However, there is no indication that using the
current 2D based models can also predict the performance
in menu selection in 3D immersive environments using hand-
gestures and head-gaze as the input modalities. In such en-
vironments the workload of the tasks is higher compared to
desktop environments [6] and it can affect the performance of
the menu selections by user.

In conclusion, since in the immersive AR environment
the possibility of noise and error and workload are higher,
more complicated tools have to be applied to predict
the
performance of user. In the following sections, we present
our method for predicting human performance in vertical
hierarchical menu selection in immersive AR.

III. METHODOLOGY
This research aims to provide a validated predictive model
of CE and PT for hierarchical menu selection in immersive
AR. Two user studies were conducted to gather the required
training data, and evaluate the machine learning model. The
proposed model was trained on the data from the ﬁrst user
study and evaluated through a set of unseen data collected
from the second user study. In the following sections, we
present details for these user studies. The diagram in Figure
2 summarizes the components of data collection procedure
during the user studies.

A. Application

We developed an immersive AR application on MS
HoloLens I, coupled with a Microsoft Kinect I for data acqui-
sition. The integrated application is developed with Unity3D
and includes two separate modes for training and evaluation.
Microsoft’s Mixed Reality Toolkit (MRTK) is employed to
develop immersive AR features and interactions. The applied
input modality was the combination of hand-gestures (for
menu selection) and head-gaze (for moving the cursor), as
they are most common in immersive AR headsets. The GUI
consists of a regular hierarchical drop-down menu as this is
widely used in AR/VR applications. The hierarchical menu
contains hundreds of drop-down menu selections for different
tasks in an immersive AR environment. Since the usual menu
depths in any typical software varies between 2 to 3 (i.e., in 3D
authoring applications such as Autodesk Maya or 3D Studio
Max, or Unity3D), we have selected 3 as the maximum menu
depth in our application. Each user selection is randomized
and the task depth varies from 2 to 3. A display prompt on
the top showed the item to be selected from the hierarchy,
and the users were instructed to perform that selection using
either hand-gestures or head-gaze. The PT and user’s hand
movements for each selection were recorded using the built-
in application’s procedure with Microsoft Kinect. In order
to have a complete hierarchy of words in our menu, we
used the taxonomy of WordNet [27], which is an extensive
lexical database of English nouns, verbs, and adjectives. For
example, ”sport → cycling → dune cycling” is showed on the

Fig. 2: The diagram shows the data acquisition procedure during the user studies. An integrated AR application is developed
on MS HoloLens I with hundreds of hierarchical drop-down menu selection tasks. The menu hierarchies were extracted from
WordNet [26], a lexical database of semantic relations between the words in English. The users performed the menu selection
tasks through two separate user studies using hand-gestures and head-gaze with MS HoloLes I. A Microsoft Kinect I recorded
the user’s hand movements during the user studies. Finally, the data from the whole procedure is used to train and validate
our machine learning model.

display prompt, asking the user to select dune cycling from
the hierarchy, as a single task. The semantics of the words on
each level and the relation between the words are applied in
the performance prediction pipeline.

B. Measures

In addition to measuring the pointing time (PT) for each
menu selection task, we also measure CE for evaluating phys-
ical workload, WAIS-IV test for assessing the participants’
cognitive performance, Semantics, and Organization of menus
for assessing the GUI-related aspects of menu selection. This
section discusses these measures.

1) Consumed Endurance (CE): The authors in [12] pro-
posed a measure for evaluating workload in tasks with mid-
air hand-gestures. This method can be used in immersive
AR/VR applications that rely on mid-air hand-gestures as input
modalities. CE requires a Microsoft Kinect to track the user’s
hand movements in real-time and assigns a workload value
to the 3D manipulation task. The model is derived from the
biomechanical structure of the upper arm for both male and
female users, and it is widely used as a metric for evaluating
the Gorilla Arm effect. Our proposed model predicts CE for
hierarchical menu selection tasks.

2) WAIS-IV Test: Prior to performing the menu selection
tasks, the participants were asked to complete a paper-based
subtest of WAIS-IV [28] to measure their cognitive ability
to perform visual menu tasks. This data lets the machine
learning model discriminate between users in performing
menu selection tasks and provide the designers and developers
with insights on their design’s personalized performance. The
complete WAIS-IV test
that covers all
aspects of human intelligence, including Verbal IQ (VIQ) and
Performance IQ (PIQ). For our experiments, the Processing
Speed Index(PSI) is measured as part of the PIQ. The PSI test
consists of two subtests: Symbol Search and Symbol Coding.

is a standard test

The symbol search test includes 63 symbol search matching
questions printed on seven one-sided A4 papers. The users
were asked to ﬁnd similar symbols in each line and mark
them during the standard 120 seconds to complete the test.
On the symbol coding test, the user had 120 seconds to copy
symbols paired with the numbers. The output will be two
integer numbers, symbol search results between 0 and 63 and
symbol coding between 0 and 135.

3) Semantics and Menu organization: Based on the results
of [11] presented by Li et al., the semantics of menus affect the
performance of vertical menu selection. Therefore, including
improves the
the semantics of menu items forming a list
accuracy of the predictions. In our work, we also extract the
semantics of the menu items using word embedding tech-
niques. A novel encoding approach named Universal Sentence
Encoder (USE) [29] is applied on the menu items to encode
the varied length menu lists to a ﬁxed length one. USE takes
as input a sentence and produces a 512-dimensional vector
representing the items’ semantic meaning.

The hierarchical menu organization is also considered as
one of the inputs for the machine learning model. For each
target in each task, the menu item’s position in the list coupled
with the number of whole menu items in that list is considered.
The saliency of the targets is also considered and is calculated
as the number of characters for each word.

C. User Study Procedure

This research received approval from the University’s Re-
search Ethics Committee. Two user studies were conducted to
gather training data and evaluate the machine learning model.
The participants were assigned through a random process to do
either the ﬁrst or second user study. The participants were ﬁrst
asked to complete a pre-test questionnaire containing questions
relating to their demographics and background experience in
using immersive AR technologies through each user study.

Immersive AR ApplicationInput Modalities Head Gaze    EnglishLexicalDatabaseWordNetHierarchyFilter  (max depth = 3)Automated and  randomized hierarchical menu  creationTrainingDataUser-related Measures:Consumed EnduranceMind ProcessingSpeed(PSI)Learning effectFatiguePointing timeGUI-related Measures:Semantics Saliency of TargetsMenu Organiztion         Hand Gesture    Microsoft Kinect-Consumed Endurance(CE)          WAIS IV Test-Symbol Search Test-Symbol Coding TestSentence Encoder -Universal Sentence Encoder (USE)TasksThen they were asked to complete the WAIS-IV test in the
researcher’s presence. A 10-minute calibration of HoloLens I
and a 10-minute instructional tutorial were the following steps
before starting the main hierarchical menu selection attempts.
Participants were asked to perform menu selection tasks with
MS HoloLens I during the main test. This part included ﬁve
menu selection attempts consisting of seven menu selection
tasks. The number of attempts and tasks resulted from a pilot
study conducted before the main study considering the user’s
subjective feedback of the test. Considering the high physical
demand of immersive AR tasks using hand-gestures, the users
were given a 2-minute break between performing each menu
selection attempt. The menu selection tasks were presented in
a single line of instruction at the top-right corner in the user’s
view, informing the user which menu item should be selected.
A Microsoft Kinect recorded the participant’s hand movements
during the activity, and the CE was calculated per task. Figure
3 shows the procedure followed and Figure 4 shows the setup
of environment during the user studies.

D. Participant Demographics

A total number of 26 users (12 female, 14 male) participated
in our user study for both training and evaluation parts. The
participants were randomly assigned to the training/evaluation
user study. The participants ranged from 24-39 years old with
the majority of 73% within the age group 27 to 35 and 11%
within group of 35 to 39 and 16% within the age group of 23
to 27.

IV. MODEL DESIGN AND LEARNING
We trained a recurrent neural network on user-related
and GUI-related measures extracted from the user studies
to predict
the PT and CE of hierarchical menu selection
tasks in a sequence of interactions. The input to the model
consists of three parts, including normalized WAIS-IV test
data representing the user-related measures, organization of
menu items, and semantic vectors as it is shown in the Figure
6. The model’s output is the desired CE and PT through a
sequence of tasks. We took advantage of a recurrent neural
network to assess the user’s activity through a series of tasks
and consider the effects of the learning effect and the user’s
fatigue while performing the tasks. In this section, ﬁrst, we
will explain the data preprocessing for preparing the data for
feeding to the machine learning model, Section IV-A, second
the architecture of the machine learning model in Section IV-B
and ﬁnally, the training of the model in Section IV-C.

A. Data Prepossessing

Menus and hierarchies have variable length and depth. To
address this problem, we converted the sequence of menu
items to ﬁxed-length vectors using word embedding techniques
as discussed in [11] and employed the USE technique [30].
The preprocessing was applied to the inputs as it is indicated
in Figure 6. Since the USE receives sequences of words,
we created comma-separated sentences from menu items,
including all menus from the start to the end of the list,
namely Input Vector 1. Another sentence was also produced
from targets in each level, separated by a single comma to

emphasize each level’s target, emphasizing semantic relation
between the targets, namely Input Vector 2. The word vectors
were later fed to USE to be converted to ﬁxed-length vectors.
Figure 5 shows an example of the similarity of the ﬁxed-length
embedding vectors for nine different tasks.

B. Neural Network Model

The machine learning model consists of the encoder and
the prediction networks, as shown in Figure 6. To encode
the varied length hierarchies, we used USE [30] which is
a transformer-based model designed to convert the varied-
length paragraph to ﬁxed-length ﬂoat value vectors. After
converting the tasks to ﬁxed-length vectors, the prediction
network is trained on the user’s performance on a sequence of
tasks to predict the PT and CE. We used an LSTM model
with one hidden layer, an input layer size of 523, and an
output layer size of 2 (CE and PT). The training window
size for considering the previous tasks was 15, meaning that
the prediction of performance for each menu selection task
is based on the previous 15 tasks performed by each user.
Therefore, the recurrent model was able to predict the learning
effect through the tasks.

C. Training

We trained our model on a workstation with NVIDIA
Geforce RTX 2060 GPU using the PyTorch library. The train-
ing data size is 840, and the test data size is 140, including the
menu selection tasks done by users. Since this is a regression
problem, we optimized the Mean Squared Error (MSE) during
the training. This is equivalent to optimizing the R2, which
is a measure for determining the goodness of ﬁt. We use the
Adam optimizer with a learning rate of 0.00002. In addition,
we found that the dropout rate of 0.3 reduces over-ﬁtting.

V. DISCUSSION AND ANALYSIS OF THE RESULTS
We have evaluated our model using test data extracted
from the evaluation user study. The model was trained on
optimizing R2 as the goodness of ﬁt. In other words, the
aim is to regress the PT and CE for a sequence of unseen
tasks from the test data gathered from the user study with
four random participants having presented to them completely
different menu hierarchies than the ones in the training data.
Therefore, the model was trained to capture different aspects
that affect performance including semantics, menu organiza-
tion and users’ cognitive performance. The Figure 7 shows the
predictions and actual values for PT and CE for one of the
users from the evaluation user study. Table I shows the results
of the predictions for each of the users who participated in the
evaluation user study. Each table row consists of the R2 and
MSE for each user. The results show that our model effectively
predicts the CE and PT in a sequence of hierarchical menu
tasks in immersive AR. Our investigations also show that
the model performs better after learning the users behaviors
in selecting the menu options i.e after a number of tasks
performed by the user. We conﬁrmed this by evaluating the
average of error for the ﬁrst ﬁve tasks and last ﬁve tasks of
each user.

Fig. 3: The participants had to ﬁll out a pre-test questionnaire followed by a 5 minutes standard WAIS-IV Test and a 10-minute
calibration to adjust the headset. A 10-minute instructional tutorial to familiarize the participants with hand-gestures used in
immersive AR, e.g., air-tapping, was presented as the next step. Then, the participants had to do the main part of the test: the
menu selection tasks. The section consists of ﬁve menu selection attempts, including seven hierarchical menu selection tasks.
In order to avoid users’ fatigue, each attempt was followed by a 2-minute break.

Fig. 4: The participant is performing menu selection tasks
while the MS Kinect records the hand motions. The red dots
on the ground and on the wall in front of user are used for
having similar initiation conﬁguration for all users.

Although we achieved lower R2s compared to the previous
works such as [11], our work has some speciﬁcations that
make it different from other works. Firstly we trained our
model on the data gathered in an immersive AR application.
Interacting with virtual content in an AR application includes
a lot of inconsistency and noise since the input modalities
are head-gaze and hand-gesture as the work [5] shows that
the error rate in menu selection in such an environment is
high. On top of that, most of the users who participated in
our user study had almost no previous experience in working
with immersive AR/VR headsets. As we asked from users in
the pretest questionnaire, only 7% of them claimed they had
good experience using AR headsets. That was why we added
an instructional tutorial for each participant before starting
the menu selection tasks. Secondly, in this work, we employ
hierarchical menu designs instead of a simple drop-down
menu with only one depth level. Having a hierarchical task
may increase the chance of error and inconsistency by itself.
Thirdly, our experiments on the test data for immersive AR
tasks, shows a positive correlation between the accuracy of the
model and increasing the size of the training data.

A. Ablation

In order to evaluate the effect of each input component on
the predictions, an ablation study is conducted by removing
from the input vector the WAIS-IV data, and semantic and

(a) Semantic similarity map for
input vector 1 for 9 selected tasks

(b) Semantic similarity map for
input vector 2 for 9 selected tasks

(c) Semantic similarity map for
average value of input vectors 1
& 2

(d) The targets for each menu selection tasks

Fig. 5: USE [30] is applied on the tasks to capture the semantic
similarity by converting it to a ﬁxed-length vector. The colors
along with the diagonal show semantic similarity between the
vectors encoding the tasks.

organization of menus. The results of training the network
without these components are shown in Table II. The ablation
study shows that the combination of semantics, WAIS, and the
organization of menus appeared to be accounts for much of
the increase in the accuracy of prediction.

The analysis of the results also shows a positive correlation

Pre-testQuestionnaire 3 MinutesWAIS  Test 5 minutesInstructionalTutorial 10 minutesCalibration 10 minutesHierarchical Menu Selection Attempts    45~55 minutesBreak 2 minutesAttempt 1 7 TasksAttempt 2 7 TasksAttempt 3 7 TasksBreak 2 minutesBreak 2 minutesAttempt 4 7 TasksBreak 2 minutesAttempt 5 7 TasksTask 1    "sport, skating, skateboarding" Task 2    "sport, skiing, cross-country"Task 3    "sport, rowing row, sculling"Task 4    "natural, rock stone, pebble"Task 5    "geological, ice mass, ice floe"Task 6    "natural, rock stone, wall rock"Task 7    "artifact, Ready-made" Task 8    "natural, Consolidation"Task 9    "artifact, button"Fig. 6: The data preprocessing and machine learning pipeline. In order to convert varied length menu items to the ﬁxed length
vectors ﬁrstly, a sequence of all items starting from the beginning of the ﬁrst-level to the end of last level d (d=2, 3) separated
by single comma between each two words created, Input Vector 1. Secondly a sequence consisting of all target items in
each level, separated by single comma is created, Input Vector 2. The two sequences were passed to the USE network [30]
to be converted to a ﬁxed-length 512-dimensional vectors. We applied the pre-trained models of USE which is available on
TensorFlow Hub . Finally, we found that the element-wise average of two vectors lead to better results for the validation set.
The WAIS-IV data and the organization of the tasks are also concatenated to the result of the semantic vector.

TABLE I: Results of Average R2 and MSE for predictions PT
and CE on the evaluation dataset

User

1
2
3
4
Average

R2 CE

MSE CE

R2 PT

MSE PT

0.32
0.08
0.31
0.26
0.24

5.34
9.92
7.46
6.06
7.19

0.45
0.28
0.4
0.31
0.36

5.99
7.03
8.17
6.37
6.89

TABLE II: Results of Ablation study to determine the impor-
tance of each feature

Feature to Remove

R2 CE

MSE CE

R2 PT

MSE PT

No Remove
Without WAIS
Without Semantics
Without Organization

0.24
0.11
0.03
0.0

7.19
11.23
8.74
15.67

0.36
0.25
0.14
0.1

6.89
8.50
7.11
12.37

result and the PT and amount of physical activity for each
task. A Pearson’s correlation analysis has been applied on the
results of the WAIS-IV (Symbol Search and Symbol coding)
for different participants and their CE measurements, PT and
error rate. The results clearly showed that there is a moderate
degree of negative correlation between the WAIS-IV results
and the CE values as well as PT. The Table III shows the
correlation between PT, CE, WAIS data, and the age of the
participants.

TABLE III: Pearson’s correlation coefﬁcient, ρ, indicating the
correlation between the WAIS-IV data and our performance
measures

(a)

(b)

Fig. 7: The predicted CE/PT versus the actual CE/PT through
35 tasks done by a participant in the evaluation user study. The
vertical axis is the CE/PT and the horizontal axis represents
the order of task from 0 to 34 (as it is depicted in Figure 3
each participant performed 35 tasks in 5 attempts each one
included 7 tasks).

between the WAIS-IV test results and the performance of
users in immersive AR, and thus integrating the WAIS results
for a user improves the accuracy of the prediction. This also
conﬁrms the reported correlation between the WAIS-IV test

Performance Factor

CE
PT

Total
WAIS

-0.324
-0.284

Symbol
Search

-0.443
-0.295

Symbol
Coding

-0.167
-0.214

Age

-0.11
-0.434

              Hierarchical Menu Selection Task inImmersive AR12.27.5sStage 2:PredictionNet variable-length word vector of allmenu items seperated by comaLevel 1 menu itemsLevel 2 menu itemsLevel 3 menu itemsfixed-length  hierarchica menuembeddingLSTMNetworkConsumed Endurance Pointing TimeLevel 1 (5)Level 2 (7)Level 3 (7)Append allmenu itemsand targetsseperatedby comaWechsler AdultIntelligence Scale(WAIS IV) Embedvariable-length word vector of alltargets seperated by comaTaget1 (1)Target 2 (1)Target 3 (1)EmbedAVGfixed-length target embedding[WIAS][Organization][Semantic Vector] (512)(512)(2)(9)(512)Data PreprocessingMachine Learning ModelStage 1: Semantic Encoder (USE) variable-to-fixed length conversionVI. CONCLUSION AND FUTURE WORK
Predicting human performance is beneﬁcial in designing and
developing user interfaces. Various works currently predict
human performance in desktop or smartphones that works
perfectly but not any recent work in immersive AR. Interacting
with virtual content in immersive environments is mainly done
by natural interactions such as hand-gestures and head-gaze.
These natural interactions can increase the possibility of error,
noise, and user fatigue. In this work, we presented a method for
predicting human performance of hierarchical menu selection
in immersive AR using hand-gestures and head-gaze as the
input modalities. Our model builds upon previous work in the
area for vertical menu selection in desktop environments [11].
The features involved in the performance of menu selection
are divided into two categories: user-related and GUI-related
measures. We have applied both categories to reach better
results for predicting human performance through a sequence
of tasks. To this end, we conducted two user studies to gather
the data used for training and evaluating the machine learning
model. The results show that applying the cognitive WAIS-
IV test improves the predictions and reduces the effect of
environmental noise and users’ fatigue. A natural extension of
this work is its application to other graphical user interfaces
such as radial menus and input modalities such as hand-
gestures and head-gaze.

ACKNOWLEDGEMENTS
This research is based upon work supported by the Natural
Sciences and Engineering Research Council of Canada Grants
No. N01670 (Discovery Grant).

REFERENCES

[1] Tim Althoff, Ryen W White, and Eric Horvitz. Inﬂuence of pok´emon go
on physical activity: study and implications. Journal of medical Internet
research, 18(12):e315, 2016.

[2] Kuo-En Chang, Jia Zhang, Yang-Sheng Huang, Tzu-Chien Liu, and Yao-
Ting Sung. Applying augmented reality in physical education on motor
skills learning. Interactive Learning Environments, pages 1–13, 2019.

[3] Philip Pratt, Matthew Ives, Graham Lawton, Jonathan Simmons, Nasko
Radev, Liana Spyropoulou, and Dimitri Amiras. Through the hololens™
looking glass: augmented reality for extremity reconstruction surgery
using 3d vascular models with perforating vessels. European radiology
experimental, 2(1):2, 2018.

[4] Dimitris Chatzopoulos, Carlos Bermejo, Zhanpeng Huang, and Pan Hui.
Mobile augmented reality survey: From where we are to where we go.
IEEE Access, 5:6917–6950, 2017.

[5] Majid Pourmemar and Charalambos Poullis. Visualizing and interacting
In ACM
with hierarchical menus in immersive augmented reality.
SIGGRAPH International Conference on Virtual Reality Continuum and
its Applications in Industry. ACM, 2019.

[6] Benjamin Bach, Ronell Sicat, Johanna Beyer, Maxime Cordeil, and
Hanspeter Pﬁster. The hologram in my hand: How effective is inter-
active exploration of 3d visualizations in immersive tangible augmented
IEEE transactions on visualization and computer graphics,
reality?
24(1):457–467, 2018.

[7] Sascha Gebhardt, Sebastian Pick, Franziska Leithold, Bernd Hentschel,
and Torsten Kuhlen. Extended pie menus for immersive virtual envi-
ronments. IEEE transactions on visualization and computer graphics,
19(4):644–651, 2013.

[8] Frederik Brudy. Interactive menus in augmented reality environments.

Beyond the Desktop, page 1, 2013.

[9] Krystian Samp and Stefan Decker. Supporting menu design with radial
layouts. In Proceedings of the International Conference on Advanced
Visual Interfaces, pages 155–162. ACM, 2010.

[10] Leah Findlater, Karyn Moffatt, Joanna McGrenere, and Jessica Dawson.
to improve menu
Ephemeral adaptation: The use of gradual onset
In Proceedings of the SIGCHI Conference on
selection performance.
Human Factors in Computing Systems, pages 1655–1664. ACM, 2009.
[11] Yang Li, Samy Bengio, and Gilles Bailly. Predicting human performance
In Proceedings of
in vertical menu selection using deep learning.
the 2018 CHI Conference on Human Factors in Computing Systems,
page 29. ACM, 2018.

[12] Juan David Hincapi´e-Ramos, Xiang Guo, Paymahn Moghadasian, and
Pourang Irani. Consumed endurance: a metric to quantify arm fatigue
of mid-air interactions. In Proceedings of the SIGCHI Conference on
Human Factors in Computing Systems, pages 1063–1072. ACM, 2014.
[13] David Wechsler. Wechsler adult intelligence scale–fourth edition (wais–

iv). San Antonio, TX: NCS Pearson, 22:498, 2008.

[14] Kevin Browne and Christopher Anand. An empirical evaluation of user
interfaces for a mobile video game. Entertainment Computing, 3(1):1–
10, 2012.

[15] Paul M Fitts. The information capacity of the human motor system
Journal of experimental

in controlling the amplitude of movement.
psychology, 47(6):381, 1954.

[16] William E Hick. On the rate of gain of information. Quarterly Journal

of experimental psychology, 4(1):11–26, 1952.

[17] Arianna Yuan and Yang Li. Modeling human visual search performance
on realistic webpages using analytical and deep learning methods.
In Proceedings of the 2020 CHI Conference on Human Factors in
Computing Systems, pages 1–12, 2020.

[18] Amanda Swearngin and Yang Li. Modeling mobile interface tappability
In Artiﬁcial Intelligence
using crowdsourcing and deep learning.
for Human Computer Interaction: A Modern Approach, pages 73–96.
Springer, 2021.

[19] Gilles Bailly, Antti Oulasvirta, Duncan P Brumby, and Andrew Howes.
In Pro-
Model of visual search and selection time in linear menus.
ceedings of the SIGCHI Conference on Human Factors in Computing
Systems, pages 3865–3874. ACM, 2014.

[20] Jussi PP Jokinen, Zhenxin Wang, Sayan Sarcar, Antti Oulasvirta, and
Xiangshi Ren. Adaptive feature guidance: Modelling visual search with
International Journal of Human-Computer Studies,
graphical layouts.
136:102376, 2020.

[21] Orlando Erazo and Jos´e A Pino. Predicting task execution time on
natural user interfaces based on touchless hand gestures. In Proceedings
of the 20th International Conference on Intelligent User Interfaces,
pages 97–109. ACM, 2015.

[22] Arianna Yuan, Ken Pfeuffer, and Yang Li. Human performance modeling
In Artiﬁcial Intelligence for Human Computer

with deep learning.
Interaction: A Modern Approach, pages 3–31. Springer, 2021.

[23] Datong Wei, Chaofan Yang, Xiaolong Zhang, and Xiaoru Yuan. Predict-
ing mouse click position using long short-term memory model trained by
joint loss function. In Extended Abstracts of the 2021 CHI Conference
on Human Factors in Computing Systems, pages 1–6, 2021.
[24] Edward Lank, Yi-Chun Nikko Cheng, and Jaime Ruiz.

Endpoint
In Proceedings of the SIGCHI
prediction using motion kinematics.
conference on Human Factors in Computing Systems, pages 637–646,
2007.

[25] Phillip T Pasqual and Jacob O Wobbrock. Mouse pointing endpoint
In Proceedings of the
prediction using kinematic template matching.
SIGCHI Conference on Human Factors in Computing Systems, pages
743–752, 2014.

[26] Christiane Fellbaum. Wordnet. The encyclopedia of applied linguistics,

2012.

[27] George A Miller. Wordnet: a lexical database for english. Communica-

tions of the ACM, 38(11):39–41, 1995.

[28] David Wechsler. The measurement and appraisal of adult intelligence.

1958.

[29] Daniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua, Nicole Limtiaco,
Rhomni St. John, Noah Constant, Mario Guajardo-Cespedes, Steve
Yuan, Chris Tar, Brian Strope, and Ray Kurzweil. Universal sentence
encoder for English. In Proceedings of the 2018 Conference on Empir-
ical Methods in Natural Language Processing: System Demonstrations,
pages 169–174, Brussels, Belgium, November 2018. Association for
Computational Linguistics.

[30] Daniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua, Nicole Limtiaco,
Rhomni St John, Noah Constant, Mario Guajardo-C´espedes, Steve
Yuan, Chris Tar, et al. Universal sentence encoder. arXiv preprint
arXiv:1803.11175, 2018.

