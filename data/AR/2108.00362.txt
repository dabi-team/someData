1
2
0
2

g
u
A
3

]

V
C
.
s
c
[

2
v
2
6
3
0
0
.
8
0
1
2
:
v
i
X
r
a

Neural Free-Viewpoint Performance Rendering under Complex
Human-object Interactions

Guoxing Sun1, Xin Chen1,2,3, Yizhang Chen1, Anqi Pang1,2,3, Pei Lin1, Yuheng Jiang1,

Lan Xu1, Jingya Wang1†

, Jingyi Yu1†

1ShanghaiTech University, School of Information Science and Technology, Shanghai Engineering Research Center of
Intelligent Vision and Imaging, China
2Shanghai Institute of Microsystem and Information Technology, Chinese Academy of Sciences, China
3University of Chinese Academy of Sciences, China
{sungx, chenxin2, chenyzh, pangaq, linpei, jiangyh2, xulan1, wangjingya, yujingyi } @shanghaitech.edu.cn

Figure 1: Our approach achieves photo-realistic reconstruction results of human activities in novel views under challenging
human-object interactions, using only six RGB cameras. (a) Capture setting. (b) Our results.

ABSTRACT
4D reconstruction of human-object interaction is critical for immer-
sive VR/AR experience and human activity understanding. Recent
advances still fail to recover fine geometry and texture results from
sparse RGB inputs, especially under challenging human-object in-
teractions scenarios. In this paper, we propose a neural human
performance capture and rendering system to generate both high-
quality geometry and photo-realistic texture of both human and
objects under challenging interaction scenarios in arbitrary novel
views, from only sparse RGB streams. To deal with complex occlu-
sions raised by human-object interactions, we adopt a layer-wise
scene decoupling strategy and perform volumetric reconstruction
and neural rendering of the human and object. Specifically, for
geometry reconstruction, we propose an interaction-aware human-
object capture scheme that jointly considers the human reconstruc-
tion and object reconstruction with their correlations. Occlusion-
aware human reconstruction and robust human-aware object track-
ing are proposed for consistent 4D human-object dynamic recon-
struction. For neural texture rendering, we propose a layer-wise

†Corresponding author.

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
MM ’21, October 20–24, 2021, Virtual Event, China
© 2021 Association for Computing Machinery.
ACM ISBN 978-1-4503-8651-7/21/10. . . $15.00
https://doi.org/10.1145/3474085.3475442

human-object rendering scheme, which combines direction-aware
neural blending weight learning and spatial-temporal texture com-
pletion to provide high-resolution and photo-realistic texture re-
sults in the occluded scenarios. Extensive experiments demonstrate
the effectiveness of our approach to achieve high-quality geome-
try and texture reconstruction in free viewpoints for challenging
human-object interactions.

CCS CONCEPTS
• Computing methodologies → Image-based rendering.

KEYWORDS
Human-object Interaction; Implicit Reconstruction; Dynamic Re-
construction; Neural Rendering

ACM Reference Format:
Guoxing Sun, Xin Chen, Yizhang Chen, Anqi Pang, Pei Lin, Yuheng Jiang,
Lan Xu, Jingya Wang, Jingyi Yu. 2021. Neural Free-Viewpoint Performance
Rendering under Complex Human-object Interactions. In Proceedings of
the 29th ACM International Conference on Multimedia (MM ’21), October
20–24, 2021, Virtual Event, China. ACM, New York, NY, USA, 11 pages.
https://doi.org/10.1145/3474085.3475442

1 INTRODUCTION
The rise of virtual and augmented reality (VR and AR) has in-
creased the the demand of the human-centric 4D (3D spatial plus
1D time) content generation, with numerous applications from
entertainment to commerce, from gaming to education. Further

(b)164532HumanObjects(a)Cameras 
 
 
 
 
 
reconstructing the 4D models and providing photo-realistic render-
ing of challenging human-object interaction scenarios conveniently
evolves as a cutting-edge yet bottleneck technique.

Early high-end solutions [12, 13, 24, 69] rely on multi-view dome-
based setup to achieve high-fidelity reconstruction and rendering,
which are expensive and difficult to be deployed. The recent low-end
volumetric approaches [55, 56, 69–72] have enabled light-weight
performance reconstruction by leveraging the RGBD sensors and
modern GPUs. However, they still suffer from unrealistic textur-
ing results and heavily rely on depth cameras which are not as
ubiquitous as color cameras. The recent neural rendering tech-
niques [30, 36, 64] bring huge potential for human reconstruction
and photo-realistic rendering from only RGB inputs. In particu-
lar, the approaches with implicit function [50, 52, 73] reconstruct
clothed humans with fine geometry details but are restricted to only
human without modeling human-object interactions. For photo-
realistic human performance rendering, various data representa-
tion have been explored, such as point-clouds [42, 64], voxels [30],
implicit representations [36, 47, 48, 62] or hybrid neural textur-
ing [57]. However, existing solutions rely on doom-level dense RGB
sensors or are limited to human priors without considering the
joint rendering of human-object interactions. Besides, various re-
searchers [15, 19, 45, 59, 75–78] models the interaction between
humans and the objects or the surrounding environments. However,
they only recover the naked human bodies or a visually plausible
but not accurate spatial arrangement. Few researchers explore to
combine volumetric modeling and photo-realistic novel view syn-
thesis under the human-object interaction scenarios, especially for
the light-weight sparse RGB setting.

In this paper, we attack these challenges and present a human-
object neural volumetric rendering using only sparse RGB cameras
surrounding the performer and objects. As illustrated in Fig. 1, our
novel approach generates both high-quality geometry and photo-
realistic texture of human activities in novel views for both the
performers and objects under challenging interaction scenarios,
whilst maintaining the light-weight setup.

Generating such a free-viewpoint video and achieving robust
volumetric reconstruction of human activities under challenging
human-object interactions is non-trivial. Our key idea is to em-
brace a layer-wise scene decoupling strategy and perform volumet-
ric reconstruction and rendering of the target human and object
separately, so as to model the challenging occlusion explicitly for
human-object interactions. To this end, we first apply an off-the-
shelf instance segmentation approach to the six RGB input streams
to distinguish the human and object separately. Then, for robust ge-
ometry reconstruction, we introduce a novel implicit human-object
capture scheme to model the mutual influence between human
and object. For the human reconstruction, we perform a neural im-
plicit geometry generation to jointly utilize both the pixel-aligned
image features and global human motion priors with the aid of
an occlusion-aware training data augmentation. For the objects,
we perform a template-based object alignment for the first frame
and human-aware tracking to maintain temporal consistency and
prevent the segmentation uncertainty caused by interaction. Fi-
nally, for photo-realistic rendering, based on the geometry proxy
above, a layer-wise human-object rendering scheme is proposed to
disentangle the human and object separately. Specifically, we adopt

template-based texturing with color correction for the object and
extend the neural texturing scheme [57] into our interaction sce-
narios with severe human-object occlusion. We propose a direction-
aware neural texturing blending scheme that encodes the occlusion
information explicitly, and adopts a spatial-temporal texture com-
pletion for the occluded regions based on the human motion priors.
With the aid of such occlusion analysis, our texturing scheme maps
the input adjacent images into a photo-realistic texture output of
human-object activities in the target view through efficient blend-
ing weight learning, without requiring further per-scene training.
To summarize, our main technical contributions include:

• We present a neural human performance rendering scheme
for challenging human-object interaction scenarios using
only sparse RGB cameras, which can reconstruct high-quality
texture and geometry results of human activities, achieving
significant superiority to existing state-of-the-art.

• We propose an interaction-aware human-object capture sch-
eme that combines occlusion-aware neural implicit human
geometry generation and robust human-aware object track-
ing for consistent 4D human-object dynamic reconstruction.
• We introduce a layer-wise human-object rendering scheme,
which utilizes occlusion-aware neural blending weight learn-
ing and spatial-temporal texture completion to provide high-
resolution and photo-realistic texture results.

2 RELATED WORK
Human Performance Capture. Markerless human performance
capture techniques have been widely investigated to achieve hu-
man free-viewpoint video or reconstruct the geometry. The high-
end solutions [9, 12, 13, 24] adopt studio-setup with dense cam-
eras to produce high-quality reconstruction and surface motion,
but the synchronized and calibrated multi-camera systems are
both difficult to deploy and expensive. The recent low-end ap-
proaches [10, 16, 21, 66] enable light-weight performance capture
under the single-view setup or even hand-held capture setup or
drone-based capture setup [68]. However, these methods require a
naked human model or pre-scanned template. Volumetric fusion
based methods [38, 70, 72, 79] enables free-form dynamic recon-
struction. But they still suffer from careful and orchestrated motions,
especially for a self-scanning process where the performer turns
around carefully to obtain complete reconstruction. [55] breaks
self-scanning constraint by introducing implicit occupancy method.
All these methods suffer from the limited mesh resolution lead-
ing to uncanny texturing output. Recent method [37] leverages
unsupervised temporally coherent human reconstruction to gen-
erate free-viewpoint rendering. It is still hard for this method to
get photo-realistic rendering results. Comparably, our approach
enables the high-fidelity capture of human-object interactions and
eliminates the additional motion constraint under the sparse view
RGB camera settings.
Neural Rendering. The recent progress of differentiable neural
rendering brings huge potential for 3D scene modeling and photo-
realistic novel view synthesis. Researchers explore various data
representations to pursue better performance and characteristics,
such as point-clouds [2, 58, 64], voxels [31], texture meshes [27, 60]
or implicit functions [7, 33, 34, 36, 43, 63]. However, these methods

Figure 2: The overview of our approach. Given the six RGB stream inputs surrounding the performer and objects, our approach
generates high-quality human-object meshes and free-view rendering results. “DR” indicates differentiable rendering.

require inevitable pre-scene training to a new scene. For neural
modeling and rendering of dynamic scenes, NHR [64] embeds spa-
tial features into sparse dynamic point-clouds, Neural Volumes [30]
transforms input images into a 3D volume representation by a VAE
network. More recently, [26, 44, 47, 48, 61, 65, 74] extend neural
radiance field (NeRF) [36] into the dynamic setting. They learn a
spatial mapping from the canonical scene to the current scene at
each time step and regress the canonical radiance field. However,
for all the dynamic approaches above, dense spatial views or full
temporal frames are required in training for high fidelity novel
view rendering, leading to deployment difficulty and unacceptable
training time overhead. Recent approaches [47] and [57] adopt a
sparse set of camera views to synthesize photo-realistic novel views
of a performer. However, in the scenario of human-object interac-
tion, these methods fail to generate both realistic performers and
realistic objects. Comparably, our approach explores the sparse cap-
ture setup and fast generates photo-realistic texture of challenging
human-object interaction in novel views.
Human-object capture. Early high-end work [12] captures both
human and objects by reconstruction and rendering with dense
cameras. Recently, several works explore the relation between hu-
man and scene to estimate 3D human pose and locate human po-
sition [15, 18, 28], naturally place human [20, 76, 78] or predict
human motion [4]. Another related direction [17, 29, 59] models
the relationship between hand and objects for generation or cap-
ture. PHOSA [75] runs human-object capture without any scene-
or object-level 3D supervision using constraints to resolve ambi-
guity. However, they only recover the naked human bodies and
produce a visually reasonable spatial arrangement. A concurrent
close work is RobustFusion(journal) [56]. They capture human and
objects by volumetric fusion respectively, and track object by Itera-
tive Closest Point (ICP). However, their texturing quality is limited
by mesh resolution and color representation, and the occluded re-
gion is ambiguous in 3D space. Comparably, our approach enables
photo-realistic novel view synthesis and accurate human object

arrangement in 3D world space under the human-object interaction
for the light-weight sparse RGB settings.

3 OVERVIEW
An overview of the proposed architecture is depicted in Fig. 2.
Given the sparse-view RGB video inputs, we introduce a coarse-to-
fine multi-stage neural human-object rendering scheme to handle
challenging scenarios with severe occlusions and multi objects.
Our approach captures high-fidelity human-object geometry and
arrangement by interaction-aware human-object capture and gener-
ates photo-realistic novel view rendering results by layered human-
object rendering. A brief introduction of our main components is
provided as follows:
Occlusion-aware Implicit Human Reconstruction (Fig. 2 (a)).
We perform a neural implicit geometry generation to utilize both
the pixel-aligned image features and global human motion priors for
the human reconstruction. Through the occlusion-aware human
reconstruction scheme, our approach reconstructs high-fidelity
human geometry under different occlusion scenarios.
Human-aware Object Tracking (Fig. 2 (b)). For the objects inter-
acted with human, we perform a template-based object alignment
and human-aware object tracking to maintain temporal consis-
tency. Efficient differentiable renderers are used to predict the 6DoF
pose of the object by comparing rendered masks. Our continuous
object tracking works robustly under the world space by jointly
considering object masks, occlusion, and mesh-intersection.
Direction-aware Neural Texture Blending (Fig. 2 (c)). For photo-
realistic rendering, a layer-wise human-object rendering scheme
is proposed to disentangle the human and object separately. We
adopt template-based texturing with color correction for the object
and extend the neural texturing scheme [57] into our interaction
scenarios with severe human-object occlusion. To deal with oc-
clusion, we propose a direction-aware neural texturing blending
scheme to explicitly encode the occlusion information and balance

Interaction-aware Human-Object CaptureSparse-viewRGB VideosMulti-view Keypoints……2DEncoderMLP(a)Occlusion-aware Implicit Human Reconstruction…Human Masks/RGBsObjects Masks/RGBs(b)Human-aware Object TrackingDROptimizer……Human-ObjectMeshesObjects MovementsHumanMeshTimeSMPLFitting3DEncoderLayered Human-Object RenderingNeuralBlending(c)Direction-aware Neural Texture Blending(d)Spatial-temporal Texture CompletionCanonical SpaceRGBsMasksWarpedImages…View 1View 2RegistrationNovelView……RenderingResultsBlendedImagethe quality of the warped images with the angle map between two
source views and the novel view.
Spatial-temporal Texture Completion (Fig. 2 (d)). Based on the
geometry from the results of the human-object capture, we adopt a
spatial-temporal texture completion for the occluded regions based
on the human motion priors. We generate a texture-completed
proxy in the canonical human space firstly. We thus use this infor-
mation to complete the missing texture in a novel view.
We describe more details for each component in Sec.4

4 METHOD
4.1 Interaction-aware Human-Object Capture
Classical multi-view stereo reconstruction approaches [12, 14, 41,
54] and recent neural rendering approaches [30, 36, 64] rely on
multi-view dome based setup to achieve high-fidelity reconstruc-
tion and rendering results. However, they suffer from both sparse-
view inputs and occlusion of objects. To this end, we propose a
novel implicit human-object capture scheme to model the mutual
influence between human and object from only sparse-view RGB
inputs.
(a) Occlusion-aware Implicit Human Reconstruction. For the
human reconstruction, we perform a neural implicit geometry gen-
eration to jointly utilize both the pixel-aligned image features and
global human motion priors with the aid of an occlusion-aware
training data augmentation.

Without dense RGB cameras and depth cameras, traditional
multi-view stereo approaches [12, 13] and depth-fusion approaches
[40, 55, 69] can hardly reconstruct high-quality human meshes.
With implicit function approaches [51, 53], we can generate fine-
detailed human meshes with sparse-view RGB inputs. However, the
occlusion from human-object interaction can still cause severe arti-
facts. To end this, we thus utilize the pixel-aligned image features
and global human motion priors.

Specifically, we adopt the off-the-shelf instance segmentation
approach [3] to obtain human and object masks, thus distinguishing
the human and object separately from the sparse-view RGB input
streams. Meanwhile, we apply the parametric model estimation to
provide human motion priors for our implicit human reconstruction.
We voxelized the mesh of this estimated human model to represent
it with a volume field.

We give both the pixel-aligned image features and global human
motion priors in volume representation to two different encoders
of our implicit function, as shown in Fig. 2 (a). Different from
[75] with only a single RGB input, we use pixel-aligned image
features from the multi-view inputs and concatenate them with our
encoded voxel-aligned features. We finally decode the pixel-aligned
and voxel-aligned feature to occupancy values with a multilayer
perceptron (MLP).

For each query 3D point 𝑃 on the volume grid, we follow PIFu

[50] to formulate the implicit function 𝑓 as:

𝑓 (Φ(𝑃), Ψ(𝑃), 𝑍 (𝑃)) = 𝜎 : 𝜎 ∈ [0.0, 1.0],

Φ(𝑃) =

1
𝑛

𝑛
∑︁

𝑖

𝐹𝐼𝑖 (𝜋𝑖 (𝑃)),

Ψ(𝑃) = 𝐺 (𝐹𝑉 , 𝑃),

(1)

(2)

(3)

Figure 3: Illustration of our synthetic 3D data with both hu-
man and objects.
where 𝑝 = 𝜋𝑖 (𝑃) denotes the projection of 3D point to camera
view 𝑖, 𝐹𝐼𝑖 (𝑥) = 𝑔(𝐼𝑖 (𝑝)) is the image feature at 𝑝. Ψ(𝑃) = 𝐺 (𝐹𝑉 , 𝑃)
denotes the voxel aligned features at 𝑃, 𝐹𝑉 is the voxel feature.
To better deal with occlusion, we introduce an occlusion-aware
reconstruction loss to enhance the prediction at the occluded part
of human. It is formulated as:
𝑇
∑︁

𝑇
∑︁

(cid:13)
𝑜𝑐𝑐 − 𝜎𝑝𝑟𝑒𝑑
𝜎𝑔𝑡
(cid:13)
𝑜𝑐𝑐
(cid:13)

(cid:13)
2
(cid:13)
(cid:13)
2

+ 𝜆𝑣𝑖𝑠

(cid:13)
𝑣𝑖𝑠 − 𝜎𝑝𝑟𝑒𝑑
𝜎𝑔𝑡
(cid:13)
(cid:13)

𝑣𝑖𝑠

(cid:13)
2
(cid:13)
(cid:13)
2

.

L𝜎 = 𝜆𝑜𝑐𝑐

(4)

𝑡 =1

𝑡 =1

Here, 𝜆𝑜𝑐𝑐 and 𝜆𝑣𝑖𝑠 represent the weight of occlusion points and
visible points, respectively. 𝜎𝑜𝑐𝑐 and 𝜎𝑣𝑖𝑠 are the training sampling
points at the occlusion area and visible area.

For the detail of the parametric model estimation, we fit the
parametric human model, SMPL [32], to capture occluded human
with the predicted 2D keypoints. Specifically, we use Openpose [5]
as our joint detector to estimate 2D human keypoints from sparse-
view RGB inputs. To estimate the pose/shape parameters of SMPL
as our human prior for occluded human, we formulate the energy
function 𝑬 prior of this optimization as:

(5)

𝑬 prior (𝜽 𝑡 , 𝜷) = 𝑬 2D + 𝜆T𝑬 T
Here, 𝑬 2D represents the re-projection constraint on 2D keypoints
detected from sparse-view RGB inputs, while 𝑬 T enforces the final
pose and shape to be temporally smooth. 𝜽 𝑡 is the pose parameters
of frame 𝑡, while 𝜷 is the shape parameters. Note that this temporal
smoothing enables globally consistent capture during the whole
sequence, and benefits the parametric model estimation when some
part of the body is gradually occluded. We follow [21] to formulate
the 2D term 𝑬 2D and the temporal term 𝑬 T under the sparse-view
setting.

Moreover, we apply an occlusion-aware data augmentation to
reduce the domain gap between our training set and the challenging
human-object interaction testing set. Specially, we randomly sample
some objects from ShapeNet dataset [6]. We then randomly rotate
and place them around human before training, as shown in Fig.
3. By simulating the occlusion of human-object interaction, our
network is more robust to occluded human features.

With both the pixel-aligned image features and the statistical
human motion priors under this occlusion-aware data augmenta-
tion training, our implicit function generates high-quality human
meshes with only spare RGB inputs and occlusions from human-
object interaction.
(b) Human-aware Object Tracking. For the objects around the
human, people recover them from depth maps [39], implicit fields [35],
or semantic parts [8]. We perform a template-based object align-
ment for the first frame and human-aware tracking to maintain
temporal consistency and prevent the segmentation uncertainty

Figure 4: Illustration of our layered human-object rendering approach, which not only includes a direction-aware neural
texture blending scheme to encode the occlusion information explicitly but also adopts a spatial-temporal texture completion
for the occluded regions based on the human motion priors.

caused by interaction. With the inspiration of PHOSA [75], we
consider each object as a rigid body mesh.

To faithfully and robustly capture object in 3D space as time
going, we introduce a human-aware tracking method. Expressly,
we assume objects are rigid bodies and transforming rigidly in
the human-object interaction activities. So the object mesh 𝑂𝑡 at
frame 𝑡 can be represented as: 𝑂𝑡 = 𝑅𝑡𝑂𝑡 −1 + 𝑇𝑡 . Based on the soft
rasterization rendering [49], the rotation 𝑅𝑡 and the translation
𝑇𝑡 can be naively optimized by comparing L2 norm between the
𝑡 and object mask M𝑜𝑖
rendered silhouette 𝑆𝑖
𝑡 .

Human is also an important cue to locate the object position.
From the 2D perspective, when objects are occluded by the human
at a camera view, the L2 loss between rendered silhouette and
occluded mask will lead to the wrong object location due to the
wrong guidelines at the occluded area. So we remove the occluded
area affected by human mask Mℎ𝑖
𝑡 when computing the L2 loss.
From the 3D perspective, human can not interpenetrate an rigid
object, so we also add an interpenetration loss L𝑃 [22] to regularize
optimization. Our total object tracking loss is:

L𝑡𝑟𝑎𝑐𝑘 = 𝜆1

𝑛
∑︁

𝑖=0

∥B (Mℎ𝑖

𝑡 == 0) ⊙ 𝑆𝑖

𝑡 − M𝑜𝑖

𝑡 ∥ + 𝜆2L𝑃 ,

(6)

where n denotes view numbers, 𝜆1 denotes weight of silhouette
loss, 𝜆2 denotes weight of interpenetration loss, B represents an
binary operation, it returns 0 when the condition is true, else 1.

Our implicit human-object capture utilizes both the pixel-aligned
image features and global human motion priors with the aid of an
occlusion-aware training data augmentation, and captures objects
with the template-based alignment and the human-aware tracking
to maintain temporal consistency and prevent the segmentation

uncertainty caused by interaction. Thus, our approaches can gen-
erate high-quality human-object geometry with sparse inputs and
occlusions.

4.2 Layered Human-Object Rendering
We introduce a neural human-object rendering pipeline to encode
local fine-detailed human geometry and texture features from adja-
cent input views, so as to produce photo-realistic layered output in
the target view, as illustrated in Fig. 4.
(c) Direction-aware Neural Texture Blending. While traditional
image-based rendering approaches always show the artifacts with
the sparse-view texture blending, we follow [57] to propose a
direction-aware neural texture blending approach to render photo-
realistic human in the novel view. For a novel view image 𝐼𝑛, the
linear combination of two source view 𝐼1 and 𝐼2 with blending
weight map 𝑊 is formulate as:

𝐼𝑛 = 𝑊 · 𝐼1 + (1 − 𝑊 ) · 𝐼2.
However, in the sparse-view setting, the neural blending approach
[57] can still generate unsmooth results. As the reason of these
artifacts, the imbalance of angles between two source views with a
novel view will lead to the imbalance wrapped image quality.

(7)

Different from Suo et al. [57], we thus propose a direction-aware
neural texture blending to eliminate such artifacts, as shown in
Fig. 4. The direction and angle between the two source views and
target view will be an important cue for neural rendering quality,
especially under occluded scenarios. Given novel view depth 𝐷𝑛 and
source view depth 𝐷1, 𝐷2, we wrap them to the novel view 𝐷𝑛
1 and
𝐷𝑛
2 , then compute the occlusion map 𝑂𝑖 = 𝐷𝑛 − 𝐷𝑛
𝑖 (𝑖 = 1, 2). Then,
we unproject 𝐷𝑖 to point-clouds. For each point 𝑃, we compute the
−−→
−−→
𝑐𝑛𝑃 to get angle map 𝐴𝑖 , where 𝑐𝑖
𝑐𝑖𝑃 and
cosine value between
denotes the optical center of source camera 𝑖, 𝑐𝑛 denotes the optical

NovelViewWarpedImagesOcclusionMapsAngleMapsDecoderWeight MapWarped Image1Warped Image2Direction-aware Neural Texture BlendingSpatial-temporal Texture CompletionBlendedImageShared WeightsMaskRGBMeshesView 1View 2Non-RigidDeformationCanonical Space Texture FusionOcclusion MapRGBLayeredHuman-ObjectViewEncoderViewEncoderMultiframesFigure 5: The geometry and texture results of our proposed approach, which generates photo-realistic rendering results and
high fidelity geometry on a various of sequences, such as rolling a box, playing with balls.

center of novel view camera. Thus, we introduce a direction-aware
blending network Θ𝐷𝐴𝑁 to utilize global feature from image and
local feature from human geometry to generate the blending weight
map 𝑊 , which can be formulated as:

𝑊 = Θ𝐷𝐴𝑁 (𝐼1, 𝑂1, 𝐴1, 𝐼2, 𝑂2, 𝐴2),

(8)

(d) Spatial-temporal Texture Completion. While human-object
interaction activities consistently lead to occlusion, the missing tex-
ture on human, therefore, leads to severe artifacts for free-viewpoint
rendering. To end this, we propose a spatial-temporal texture com-
pletion method to generate a texture-completed proxy in the canon-
ical human space. We use the temporal and spatial information to
complete the missing texture at view 𝑖 and time 𝑡 from different
times and different views.

Specifically, we first use the non-rigid deformation to register
an up-sampled SMPL model (41330 vertices) with the captured
human meshes. Then, for each point on the proxy, we find the
nearest visible points along with all views and all frames, then
assign an interpolation color to this point. We thus generate a
canonical human space with the fused texture. For the occluded
part of human in novel view, we render the texture-completed
image and blend it with the neural rendering results in Sec. 4.2 (c).
We utilize a layered human-object rendering strategy to render
human-object together with the reconstruction and tracking of
object. For each frame, we render human with our novel neural tex-
ture blending while rendering objects through a classical graphics
pipeline with color correction matrix (CCM). To combine human
and object rendering results, we utilize the depth buffer from the
geometry of our human-object capture.
Training Strategy. To enable our sparse-view neural human per-
formance rendering under human-object interaction, we need to
train the direction-aware blending network Θ𝐷𝐴𝑁 properly.

We follow Suo et al. [57] to utilize 1457 scans from the Twin-
dom dataset [1] to train our DAN Θ𝐷𝐴𝑁 properly. Differently, we
randomly place the performers inside the virtual camera views
and augment this dataset by randomly placing some objects from
ShapeNet dataset [6]. By simulating the occlusion of human-object
interaction, we make our network more robust to occluded human.
Our training dataset contains RGB images, depth maps and angle
maps for all the views and models.

For the training of our direction-aware blending network Θ𝐷𝐴𝑁 ,
we set out to apply the following learning scheme to enable more
robust blending weight learning. The appearance loss function with
the perceptual term [23] is to make the blended texture as close as
possible to the ground truth, which is formulated as:

L𝑟𝑔𝑏 =

1
𝑛

𝑛
∑︁

𝑗

(cid:18)(cid:13)
𝑟 − 𝐼 𝑗
𝐼 𝑗
(cid:13)
𝑔𝑡
(cid:13)

(cid:13)
2
(cid:13)
(cid:13)
2

+

(cid:13)
𝜑
(cid:13)
(cid:13)

(cid:17)

(cid:16)

𝐼 𝑗
𝑟

− 𝜑

(cid:16)

𝐼 𝑗
𝑔𝑡

(cid:19)

(cid:17)(cid:13)
2
(cid:13)
(cid:13)
2

(9)

where 𝐼𝑔𝑡 is the ground truth RGB images; 𝜑 (·) denotes the output
features of the third-layer of pre-trained VGG-19.

With the aid of such occlusion analysis, our texturing scheme
maps the input adjacent images into a photo-realistic texture output
of human-object activities in the target view through efficient blend-
ing weight learning, without requiring further per-scene training.

5 EXPERIMENT
In this section, we report the details of our approach and evaluate
our method on a variety of complex human-object interaction sce-
narios. All of our experiments are run on a PC with 2.2GHz Intel
Xeon 4214 CPU, 32GB RAM, and Nvidia GeForce TITAN RTX GPUs.
The inputs of our system are from a multi-camera system with six
synchronized RGB cameras. Fig. 5 demonstrates that our approach
faithfully reconstructs the geometry and texture of both human
and object under interactions, and even handles severe occlusion

Table 1: Quantitative comparison against various neural
rendering methods on the rendering results. Our method
achieves consistently better metric results.

Method
NHFVV [57]
NHR [64]
NeRF [36]
Ours

PSNR↑
17.545
23.869
24.022
25.323

SSIM↑ MAE ↓
12.949
0.966
10.204
0.964
9.612
0.970
4.787
0.985

Figure 7: Qualitative evaluation of our occlusion-aware im-
plicit human reconstruction scheme. (a)Input images. (b)w/o
implicit; (c)w/o explicit, aug; (d)w/o explicit; (e)ours; (f)ours
and object from side view.

5.2 Evaluation
Object-aware human reconstruction. Here we evaluate our hu-
man geometry reconstruction stage. Let w/o implicit denote the
variation which only uses explicit parametric human model us-
ing the same SMPL fitting process in Sec. 4.1 (a). Besides, let w/o
explicit and let w/o explicit, aug denote the variation without
explicit 3D human prior and without both the 3D human prior and
occlusion augmentation, respectively. Fig. 7 provides the qualitative
comparison against all these variations in real-world sequences.
Note that without the implicit geometry inference, only paramet-
ric naked human models are recovered without details, while the
results without explicit 3D human prior or occlusion augmenta-
tion suffer from severe geometry artifacts due to the challenging
human-object interactions, especially for the occluded regions and
thin structure like arms. Differently, our approach achieves detailed
human geometry reconstruction under challenging occlusions and
interactions and can further enable consistent human-object cap-
ture through our lay-wise design. For further quantitative analysis,
we evaluate on our synthetic dataset adopt Chamfer distance (CD)
in centimeters and Point to Surface distance (P2S) in centimeters, as
well as the Cosine) and L2 distances upon the re-projection normal
as our evaluation metrics The corresponding Table. 2 highlights the
contribution of each part of our geometry generation component
and shows that our full method can produce good geometry under
the occlusion scenario.
Layered human-object rendering. We further evaluate our human-
object rendering stage against various texturing schemes using the
same geometry proxy from our previous stage for a fair evaluation.
Let Per-vertex denote the per-vertex color scheme of PIFu [51]

Figure 6: Qualitative comparison on rendering results with
various neural rendering approaches. Our approach gener-
ates more reasonable and photo-realistic texture. (a) Input
images; (b) NHFVV; (c) NHR; (d) NeRF; (e) Ours.

and multi-object scenarios, such as pulling a chair and catching
two balls.
Implementation Details. We assume the perspective camera model
in our pipeline. We adopt a U-net architecture following NeuralHu-
manFVV [57] for the image encoder and a 3D convolution network
like IF-Net [11] with fewer feature dimensions at each resolution
for the voxel encoder. U-net architecture is adopted in our Θ𝐷𝐴𝑁
similar to Siamese Network [25] which takes the input from two
source view separately with shared parameters during the encoding
process. We train our network with 1457 scans from Twindom [1]
augmented with rigging models of different poses and random
occlusions.

5.1 Comparison
To the best of our knowledge, our approach is the first neural
layer-wise free-viewpoint performance rendering approach with
human-object interactions using only sparse RGB input. For thor-
ough comparison, we compare our approach against existing neural
rendering methods, including the point-based NHR [64], implicit
method NeRF [36] and the hybrid texturing-based NHFVV [57]
using the same training data for a fair comparison. Note that for
the training of NHR we obtain the point-clouds from the depth
sensors in our capture system, and we extend the NeRF to dynamic
setting using per-frame multi-view RGB images for training. Be-
sides, the human and object segmented masks are all utilized during
the training process of these methods for a fair comparison.

As shown in Fig. 6, other method suffers from severe rendering
artifacts under our interaction and sparse view setting. Differently,
our approach achieves significantly sharper and more realistic ren-
dering results of human-object interaction scenarios even when
serve occlusion appears. Note that our approach can also enable
layer-wise rendering effect and get rid of the tedious per-scene
training process. For quantitative comparison, we adopt the peak
signal-to-noise ratio (PSNR), structural similarity index (SSIM), the
Mean Absolute Error (MAE) as metrics on the whole testing dataset
by comparing the rendering results with source view inputs. As
shown in Tab. 1, our approach outperforms other methods on all the
metrics above, which illustrates the effectiveness of our approach
for free-viewpoint performance rendering under our human-object
interaction and sparse-view setting.

(a)(b)(c)(d)(e)(a)(b)(c)(d)(e)(f)Table 2: Quantitative evaluation of occlusion-aware human
reconstruction scheme on synthetic data.

Method

w/o implicit
w/o explicit, aug
w/o explicit
Ours

Mesh

Normal

CD↓
9.680
3.551
3.839
1.819

P2S↓ Cosine↓
8.736
3.698
6.275
2.255

0.362
0.215
0.150
0.134

L2↓
0.595
0.443
0.380
0.353

Figure 9: Evaluation of texturing schemes. (a) Our geometry
results; (b) Per-pixel texturing results from PIFu; (c) Neural
blending results from NHFVV; (d,e) Our texturing results
with and w/o object; (f,g) Quantitative results of different
texturing schemes with and w/o object, respectively.

extreme poses and occlusions unseen during training. A large-scale
and high-quality dataset for 4D human-object interaction analysis
will be critical for such generalization. Furthermore, our approach
will fail to capture transparent objects which is hard to be seg-
mented. Our current pipeline models the geometry generation and
layer-wise texture rendering separately, and it’s an interesting di-
rection to build an end-to-end learning framework such as neural
radiance field [36] for complex human-object interaction scenarios.

6 CONCLUSION
We have presented a neural performance rendering system to gen-
erate high-quality geometry and photo-realistic textures of human-
object interaction activities in novel views using sparse RGB cam-
eras only. Our layer-wise scene decoupling strategy enables explicit
disentanglement of human and object for robust reconstruction and
photo-realistic rendering under challenging occlusion caused by
interactions. Specifically, the proposed implicit human-object cap-
ture scheme with occlusion-aware human implicit regression and
human-aware object tracking enables consistent 4D human-object
dynamic geometry reconstruction. Additionally, our layer-wise
human-object rendering scheme encodes the occlusion information
and human motion priors to provide high-resolution and photo-
realistic texture results of interaction activities in the novel views.
Extensive experimental results demonstrate the effectiveness of
our approach for compelling performance capture and rendering
in various challenging scenarios with human-object interactions
under the sparse setting. We believe that it is a critical step for dy-
namic reconstruction under human-object interactions and neural
human performance analysis, with many potential applications in
VR/AR, entertainment, human behavior analysis and immersive
telepresence.

7 ACKNOWLEDGMENTS
This work was supported by NSFC programs (61976138, 61977047),
the National Key Research and Development Program (2018YFB2100
500), STCSM (2015F0203-000-06), SHMEC (2019-01-07-00-01-E00003)
and Shanghai YangFan Program (21YF1429500).

Figure 8: Qualitative evaluation of texturing scheme. (a) In-
put images; (b) Geometry of our approach; (c) Per-pixel tex-
turing results from PIFu; (d) Neural blending results from
NeuralHumanFVV; (e) Our texturing results.

and Neural Blending denote the hybrid texturing scheme in Neu-
ralHumanFVV [57]. For qualitative evaluation, as shown in Fig. 8,
both baseline methods suffer from blur texturing results or severe
occlusion artifacts near the boundary regions. In contrast, our layer-
wised rendering scheme utilizes both the direction information in
an occlusion-aware manner, which generates much sharper and
photo-realistic texture rendering results, comparing favorably to
the other texturing methods. Furthermore, we quantitative evaluate
against various rendering schemes under the interaction scenarios
with objects. As shown in Fig. 9, the per-vertex scheme suffers from
blur texturing artifact while the baseline neural blending scheme
suffers from occlusion artifacts near the boundary regions especially
on the objects. Differently, our approach generates photo-realistic
rendering results under human-object interactions, yielding the
lowest mean average error(MAE) of the whole sequence with or
without taking the object into error calculation. These evaluations
illustrate the effectiveness of our texturing scheme to utilize the
direction information and perform occlusion analysis for complete,
photo-realistic, and layer-wise texturing.

5.3 Limitation and Discussion
As a trial for free-viewpoint performance rendering under human-
object interactions, the proposed system still owns some limitations.
First, we cannot handle objects with non-rigid deformation or topol-
ogy changes like tearing a paper, which restricts the application of
our systems. We plan to address it by incorporating the key-volume
update technique [67]. Besides, due to the low image resolution and
limited mesh diversity of training data, our method cannot generate
good results for fine-grained regions like fingers and handle those

(b)(c)(d)(e)(a)(a)(b)(c)(d)(e)(f)(g)REFERENCES
[1] [n.d.]. Twindom Dataset. https://web.twindom.com/.
[2] Kara-Ali Aliev, Artem Sevastopolsky, Maria Kolos, Dmitry Ulyanov, and Victor
Lempitsky. 2019. Neural point-based graphics. arXiv preprint arXiv:1906.08240
(2019).

[3] Daniel Bolya, Chong Zhou, Fanyi Xiao, and Yong Jae Lee. 2019. YOLACT: Real-
Time Instance Segmentation. In Proceedings of the IEEE/CVF International Confer-
ence on Computer Vision (ICCV).

[4] Zhe Cao, Hang Gao, Karttikeya Mangalam, Qi-Zhi Cai, Minh Vo, and Jitendra
Malik. 2020. Long-term human motion prediction with scene context. In European
Conference on Computer Vision. Springer, 387–404.

[5] Zhe Cao, Tomas Simon, Shih-En Wei, and Yaser Sheikh. 2017. Realtime Multi-
Person 2D Pose Estimation Using Part Affinity Fields. In Computer Vision and
Pattern Recognition (CVPR).

[6] Angel X Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing
Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, et al.
2015. Shapenet: An information-rich 3d model repository. arXiv preprint
arXiv:1512.03012 (2015).

[7] Anpei Chen, Zexiang Xu, Fuqiang Zhao, Xiaoshuai Zhang, Fanbo Xiang, Jingyi Yu,
and Hao Su. 2021. MVSNeRF: Fast Generalizable Radiance Field Reconstruction
from Multi-View Stereo. arXiv preprint arXiv:2103.15595 (2021).

[8] Xin Chen, Yuwei Li, Xi Luo, Tianjia Shao, Jingyi Yu, Kun Zhou, and Youyi Zheng.
2018. Autosweep: Recovering 3d editable objects from a single photograph. IEEE
transactions on visualization and computer graphics 26, 3 (2018), 1466–1475.
[9] Xin Chen, Anqi Pang, Yang Wei, Lan Xui, and Jingyi Yu. 2019. TightCap: 3D
human shape capture with clothing tightness. arXiv preprint arXiv:1904.02601
(2019).

[10] Xin Chen, Anqi Pang, Wei Yang, Yuexin Ma, Lan Xu, and Jingyi Yu. 2021.
SportsCap: Monocular 3D Human Motion Capture and Fine-grained Under-
standing in Challenging Sports Videos. arXiv preprint arXiv:2104.11452 (2021).
[11] Julian Chibane, Thiemo Alldieck, and Gerard Pons-Moll. 2020. Implicit functions
in feature space for 3d shape reconstruction and completion. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 6970–6981.
[12] Alvaro Collet, Ming Chuang, Pat Sweeney, Don Gillett, Dennis Evseev, David
Calabrese, Hugues Hoppe, Adam Kirk, and Steve Sullivan. 2015. High-quality
streamable free-viewpoint video. ACM Transactions on Graphics (TOG) 34, 4
(2015), 69.

[13] Mingsong Dou, Philip Davidson, Sean Ryan Fanello, Sameh Khamis, Adarsh
Kowdle, Christoph Rhemann, Vladimir Tankovich, and Shahram Izadi. 2017.
Motion2Fusion: Real-time Volumetric Performance Capture. ACM Trans. Graph.
36, 6, Article 246 (Nov. 2017), 16 pages.

[14] Yasutaka Furukawa, Carlos Hernández, et al. 2013. Multi-view Stereo: A Tutorial.
Foundations and Trends® in Computer Graphics and Vision 9, 1-2 (2013), 1–148.
[15] Vladimir Guzov, Aymen Mir, Torsten Sattler, and Gerard Pons-Moll. 2021. Human
POSEitioning System (HPS): 3D Human Pose Estimation and Self-localization
in Large Scenes from Body-Mounted Sensors. In IEEE Conference on Computer
Vision and Pattern Recognition (CVPR). IEEE.

[16] Marc Habermann, Weipeng Xu, Michael Zollhöfer, Gerard Pons-Moll, and Chris-
tian Theobalt. 2019. LiveCap: Real-Time Human Performance Capture From
Monocular Video. ACM Transactions on Graphics (TOG) 38, 2, Article 14 (2019),
17 pages.

[17] Shreyas Hampali, Sayan Deb Sarkar, Mahdi Rad, and Vincent Lepetit. 2021. Hands-
Former: Keypoint Transformer for Monocular 3D Pose Estimation ofHands and
Object in Interaction. arXiv preprint arXiv:2104.14639 (2021).

[18] Mohamed Hassan, Vasileios Choutas, Dimitrios Tzionas, and Michael J Black.
2019. Resolving 3D human pose ambiguities with 3D scene constraints. In
Proceedings of the IEEE/CVF International Conference on Computer Vision. 2282–
2292.

[19] Mohamed Hassan, Partha Ghosh, Joachim Tesch, Dimitrios Tzionas, and Michael J.
Black. 2021. Populating 3D Scenes by Learning Human-Scene Interaction. In
Proceedings IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR).
[20] Mohamed Hassan, Partha Ghosh, Joachim Tesch, Dimitrios Tzionas, and Michael J
Black. 2021. Populating 3D Scenes by Learning Human-Scene Interaction. In
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.
14708–14718.

[21] Yannan He, Anqi Pang, Xin Chen, Han Liang, Minye Wu, Yuexin Ma, and Lan
Xu. 2021. Challencap: Monocular 3d capture of challenging human performances
using multi-modal references. In Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition. 11400–11411.

[22] Wen Jiang, Nikos Kolotouros, Georgios Pavlakos, Xiaowei Zhou, and Kostas
Daniilidis. 2020. Coherent Reconstruction of Multiple Humans from a Single
Image. In CVPR.

[23] Justin Johnson, Alexandre Alahi, and Li Fei-Fei. 2016. Perceptual losses for real-
time style transfer and super-resolution. In European conference on computer
vision. Springer, 694–711.

[24] Hanbyul Joo, Tomas Simon, and Yaser Sheikh. 2018. Total Capture: A 3D Defor-
mation Model for Tracking Faces, Hands, and Bodies. In The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR).

[25] Gregory Koch, Richard Zemel, and Ruslan Salakhutdinov. 2015. Siamese neural
networks for one-shot image recognition. In ICML deep learning workshop, Vol. 2.
Lille.

[26] Zhengqi Li, Simon Niklaus, Noah Snavely, and Oliver Wang. 2020. Neural Scene
Flow Fields for Space-Time View Synthesis of Dynamic Scenes. arXiv preprint
arXiv:2011.13084 (2020).

[27] Lingjie Liu, Weipeng Xu, Michael Zollhoefer, Hyeongwoo Kim, Florian Bernard,
Marc Habermann, Wenping Wang, and Christian Theobalt. 2019. Neural ren-
dering and reenactment of human actor videos. ACM Transactions on Graphics
(TOG) 38, 5 (2019), 1–14.

[28] Miao Liu, Dexin Yang, Yan Zhang, Zhaopeng Cui, James M Rehg, and Siyu Tang.
2020. 4d human body capture from egocentric video via 3d scene grounding.
arXiv preprint arXiv:2011.13341 (2020).

[29] Shaowei Liu, Hanwen Jiang, Jiarui Xu, Sifei Liu, and Xiaolong Wang. 2021. Semi-
supervised 3d hand-object poses estimation with interactions in time. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.
14687–14697.

[30] Stephen Lombardi, Tomas Simon, Jason Saragih, Gabriel Schwartz, Andreas
Lehrmann, and Yaser Sheikh. 2019. Neural Volumes: Learning Dynamic Ren-
derable Volumes from Images. ACM Trans. Graph. 38, 4, Article 65 (July 2019),
14 pages. https://doi.org/10.1145/3306346.3323020

[31] Stephen Lombardi, Tomas Simon, Jason Saragih, Gabriel Schwartz, Andreas
Lehrmann, and Yaser Sheikh. 2019. Neural volumes: Learning dynamic renderable
volumes from images. arXiv preprint arXiv:1906.07751 (2019).

[32] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll, and
Michael J. Black. 2015. SMPL: A Skinned Multi-person Linear Model. ACM
Trans. Graph. 34, 6, Article 248 (Oct. 2015), 16 pages.

[33] Haimin Luo, Anpei Chen, Qixuan Zhang, Bai Pang, Minye Wu, Lan Xu, and
Jingyi Yu. 2021. Convolutional Neural Opacity Radiance Fields. arXiv preprint
arXiv:2104.01772 (2021).

[34] Quan Meng, Anpei Chen, Haimin Luo, Minye Wu, Hao Su, Lan Xu, Xuming He,
and Jingyi Yu. 2021. GNeRF: GAN-based Neural Radiance Field without Posed
Camera. arXiv preprint arXiv:2103.15606 (2021).

[35] Lars Mescheder, Michael Oechsle, Michael Niemeyer, Sebastian Nowozin, and
Andreas Geiger. 2019. Occupancy networks: Learning 3d reconstruction in
function space. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition. 4460–4470.

[36] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi
Ramamoorthi, and Ren Ng. 2020. NeRF: Representing Scenes as Neural Radiance
Fields for View Synthesis. In Computer Vision – ECCV 2020, Andrea Vedaldi, Horst
Bischof, Thomas Brox, and Jan-Michael Frahm (Eds.). Springer International
Publishing, Cham, 405–421.

[37] Armin Mustafa, Marco Volino, Hansung Kim, Jean-Yves Guillemaut, and Adrian
Hilton. 2020. Temporally coherent general dynamic scene reconstruction. Inter-
national Journal of Computer Vision (2020), 1–19.

[38] Richard A Newcombe, Dieter Fox, and Steven M Seitz. 2015. Dynamicfusion:
Reconstruction and Tracking of Non-rigid Scenes in Real-time. In Proceedings of
the IEEE Conference on Computer Vision and Pattern Recognition. 343–352.
[39] Richard A Newcombe, Shahram Izadi, Otmar Hilliges, David Molyneaux, David
Kim, Andrew J Davison, Pushmeet Kohi, Jamie Shotton, Steve Hodges, and
Andrew Fitzgibbon. 2011. KinectFusion: Real-time Dense Surface Mapping
and Tracking. In Proceedings of the IEEE International Symposium on Mixed and
Augmented Reality. IEEE, 127–136.

[40] Richard A. Newcombe, Shahram Izadi, Otmar Hilliges, David Molyneaux, David
Kim, Andrew J. Davison, Pushmeet Kohli, Jamie Shotton, Steve Hodges, and
Andrew Fitzgibbon. 2011. KinectFusion: Real-Time Dense Surface Mapping and
Tracking. In Proc. of ISMAR. 127–136.

[41] Richard A Newcombe, Steven J Lovegrove, and Andrew J Davison. 2011. DTAM:
Dense Tracking and Mapping in Real-time. In Proceedings of the IEEE International
Conference on Computer Vision. IEEE, 2320–2327.

[42] Anqi Pang, Xin Chen, Haimin Luo, Minye Wu, Jingyi Yu, and Lan Xu. 2021.
Few-shot Neural Human Performance Rendering from Sparse RGBD Videos.
arXiv:2107.06505 [cs.CV]

[43] Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, and Steven
Lovegrove. 2019. Deepsdf: Learning continuous signed distance functions for
shape representation. In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition. 165–174.

[44] Keunhong Park, Utkarsh Sinha, Jonathan T Barron, Sofien Bouaziz, Dan B Gold-
man, Steven M Seitz, and Ricardo-Martin Brualla. 2020. Deformable Neural
Radiance Fields. arXiv preprint arXiv:2011.12948 (2020).

[45] Priyanka Patel, Chun-Hao P. Huang, Joachim Tesch, David T. Hoffmann,
Shashank Tripathi, and Michael J. Black. 2021. AGORA: Avatars in Geography
Optimized for Regression Analysis. In Proceedings IEEE/CVF Conf. on Computer
Vision and Pattern Recognition (CVPR).

[46] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hujun Bao. 2019. PVNet:

Pixel-wise Voting Network for 6DoF Pose Estimation. In CVPR.

[47] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun
Bao, and Xiaowei Zhou. 2021. Neural Body: Implicit Neural Representations

with Structured Latent Codes for Novel View Synthesis of Dynamic Humans. In
CVPR.

[48] Albert Pumarola, Enric Corona, Gerard Pons-Moll, and Francesc Moreno-Noguer.
2020. D-NeRF: Neural Radiance Fields for Dynamic Scenes. arXiv preprint
arXiv:2011.13961 (2020).

[49] Nikhila Ravi, Jeremy Reizenstein, David Novotny, Taylor Gordon, Wan-Yen Lo,
Justin Johnson, and Georgia Gkioxari. 2020. Accelerating 3d deep learning with
pytorch3d. arXiv preprint arXiv:2007.08501 (2020).

[50] Shunsuke Saito, Zeng Huang, Ryota Natsume, Shigeo Morishima, Angjoo
Kanazawa, and Hao Li. 2019. PIFu: Pixel-aligned implicit function for high-
resolution clothed human digitization. In Proceedings of the IEEE International
Conference on Computer Vision. 2304–2314.

[51] Shunsuke Saito, Zeng Huang, Ryota Natsume, Shigeo Morishima, Angjoo
Kanazawa, and Hao Li. 2019. PIFu: Pixel-Aligned Implicit Function for High-
Resolution Clothed Human Digitization. In The IEEE International Conference on
Computer Vision (ICCV).

[52] Shunsuke Saito, Tomas Simon, Jason Saragih, and Hanbyul Joo. 2020. PIFuHD:
Multi-Level Pixel-Aligned Implicit Function for High-Resolution 3D Human
Digitization. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition. 84–93.

[53] Shunsuke Saito, Tomas Simon, Jason Saragih, and Hanbyul Joo. 2020. PIFuHD:
Multi-Level Pixel-Aligned Implicit Function for High-Resolution 3D Human
Digitization. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR).

[54] Christoph Strecha, Wolfgang Von Hansen, Luc Van Gool, Pascal Fua, and Ulrich
Thoennessen. 2008. On Benchmarking Camera Calibration and Multi-view Stereo
for High Resolution Imagery. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition. Ieee, 1–8.

[55] Zhuo Su, Lan Xu, Zerong Zheng, Tao Yu, Yebin Liu, and Lu Fang. 2020. RobustFu-
sion: Human Volumetric Capture with Data-Driven Visual Cues Using a RGBD
Camera. In Computer Vision – ECCV 2020, Andrea Vedaldi, Horst Bischof, Thomas
Brox, and Jan-Michael Frahm (Eds.). Springer International Publishing, Cham,
246–264.

[56] Zhuo Su, Lan Xu, Dawei Zhong, Zhong Li, Fan Deng, Shuxue Quan, and Lu
Fang. 2021. RobustFusion: Robust Volumetric Performance Reconstruction un-
der Human-object Interactions from Monocular RGBD Stream. arXiv preprint
arXiv:2104.14837 (2021).

[57] Xin Suo, Yuheng Jiang, Pei Lin, Yingliang Zhang, Minye Wu, Kaiwen Guo, and Lan
Xu. 2021. NeuralHumanFVV: Real-Time Neural Volumetric Human Performance
Rendering using RGB Cameras. In IEEE Conference on Computer Vision and Pattern
Recognition (CVPR). IEEE.

[58] Xin Suo, Minye Wu, Yanshun Zhang, Yingliang Zhang, Lan Xu, Qiang Hu, and
Jingyi Yu. 2020. Neural3D: Light-weight Neural Portrait Scanning via Context-
aware Correspondence Learning. In Proceedings of the 28th ACM International
Conference on Multimedia. 3651–3660.

[59] Omid Taheri, Nima Ghorbani, Michael J. Black, and Dimitrios Tzionas. 2020.
GRAB: A Dataset of Whole-Body Human Grasping of Objects. In Computer
Vision – ECCV 2020, Vol. LNCS 12355. Springer International Publishing, Cham,
581–600. https://grab.is.tue.mpg.de

[60] Justus Thies, Michael Zollhöfer, and Matthias Nießner. 2019. Deferred neural
rendering: Image synthesis using neural textures. ACM Transactions on Graphics
(TOG) 38, 4 (2019), 1–12.

[61] Edgar Tretschk, Ayush Tewari, Vladislav Golyanik, Michael Zollhöfer, Christoph
Lassner, and Christian Theobalt. 2020. Non-Rigid Neural Radiance Fields: Re-
construction and Novel View Synthesis of a Deforming Scene from Monocular
Video. arXiv preprint arXiv:2012.12247 (2020).

[62] Edgar Tretschk, Ayush Tewari, Vladislav Golyanik, Michael Zollhöfer, Christoph
Lassner, and Christian Theobalt. 2020. Non-Rigid Neural Radiance Fields: Recon-
struction and Novel View Synthesis of a Dynamic Scene From Monocular Video.

arXiv:2012.12247 [cs.CV]

[63] Ziyu Wang, Liao Wang, Fuqiang Zhao, Minye Wu, Lan Xu, and Jingyi Yu. 2021.
MirrorNeRF: One-shot Neural Portrait Radiance Field from Multi-mirror Cata-
dioptric Imaging. arXiv preprint arXiv:2104.02607 (2021).

[64] Minye Wu, Yuehao Wang, Qiang Hu, and Jingyi Yu. 2020. Multi-View Neural
Human Rendering. In Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR).

[65] Wenqi Xian, Jia-Bin Huang, Johannes Kopf, and Changil Kim. 2020. Space-time
Neural Irradiance Fields for Free-Viewpoint Video. arXiv preprint arXiv:2011.12950
(2020).

[66] Donglai Xiang, Hanbyul Joo, and Yaser Sheikh. 2019. Monocular Total Capture:
Posing Face, Body, and Hands in the Wild. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR).

[67] L. Xu, W. Cheng, K. Guo, L. Han, Y. Liu, and L. Fang. 2019. FlyFusion: Realtime
Dynamic Scene Reconstruction Using a Flying Depth Camera. IEEE Transactions
on Visualization and Computer Graphics (2019), 1–1.

[68] Lan Xu, Yebin Liu, Wei Cheng, Kaiwen Guo, Guyue Zhou, Qionghai Dai, and
Lu Fang. 2017. Flycap: Markerless motion capture using multiple autonomous
flying cameras. IEEE transactions on visualization and computer graphics 24, 8
(2017), 2284–2297.

[69] L. Xu, Z. Su, L. Han, T. Yu, Y. Liu, and L. FANG. 2019. UnstructuredFusion:
Realtime 4D Geometry and Texture Reconstruction using CommercialRGBD
Cameras. IEEE Transactions on Pattern Analysis and Machine Intelligence (2019),
1–1.

[70] Tao Yu, Kaiwen Guo, Feng Xu, Yuan Dong, Zhaoqi Su, Jianhui Zhao, Jianguo
Li, Qionghai Dai, and Yebin Liu. 2017. BodyFusion: Real-time Capture of Hu-
man Motion and Surface Geometry Using a Single Depth Camera. In The IEEE
International Conference on Computer Vision (ICCV). ACM.

[71] Tao Yu, Zerong Zheng, Kaiwen Guo, Pengpeng Liu, Qionghai Dai, and Yebin
Liu. 2021. Function4D: Real-time Human Volumetric Capture from Very Sparse
Consumer RGBD Sensors. In IEEE Conference on Computer Vision and Pattern
Recognition (CVPR2021).

[72] Tao Yu, Zerong Zheng, Kaiwen Guo, Jianhui Zhao, Qionghai Dai, Hao Li, Gerard
Pons-Moll, and Yebin Liu. 2019. DoubleFusion: Real-time Capture of Human
Performances with Inner Body Shapes from a Single Depth Sensor. Transactions
on Pattern Analysis and Machine Intelligence (TPAMI) (2019).

[73] Yebin Liu Qionghai Dai Zerong Zheng, Tao Yu. 2021. PaMIR: Parametric Model-

Conditioned Implicit Representation for Image-based Human Reconstruction.

[74] Jiakai Zhang, Xinhang Liu, Xinyi Ye, Fuqiang Zhao, Yanshun Zhang, Minye Wu,
Yingliang Zhang, Lan Xu, and Jingyi Yu. 2021. Editable Free-viewpoint Video
Using a Layered Neural Representation. arXiv preprint arXiv:2104.14786 (2021).
[75] Jason Y. Zhang, Sam Pepose, Hanbyul Joo, Deva Ramanan, Jitendra Malik, and
Angjoo Kanazawa. 2020. Perceiving 3D Human-Object Spatial Arrangements
from a Single Image in the Wild. In European Conference on Computer Vision
(ECCV).

[76] Siwei Zhang, Yan Zhang, Qianli Ma, Michael J. Black, and Siyu Tang. 2020.
PLACE: Proximity Learning of Articulation and Contact in 3D Environments. In
International Conference on 3D Vision (3DV).

[77] Tianshu Zhang, Buzhen Huang, and Yangang Wang. 2020. Object-occluded
human shape and pose estimation from a single color image. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 7376–7385.
[78] Yan Zhang, Mohamed Hassan, Heiko Neumann, Michael J. Black, and Siyu Tang.
2020. Generating 3D People in Scenes without People. In Computer Vision and
Pattern Recognition (CVPR). 6194–6204.

[79] Zerong Zheng, Tao Yu, Hao Li, Kaiwen Guo, Qionghai Dai, Lu Fang, and Yebin
Liu. 2018. HybridFusion: Real-time Performance Capture Using a Single Depth
Sensor and Sparse IMUs. In European Conference on Computer Vision (ECCV).

A MORE IMPLEMENTATION DETAILS
We normalize the camera system to 𝑚 unit and transform it to align
the system center with the coordinate origin. Our voxel encoder
is a forward 3D CNN like [11], outputs 1, 16, 32, 32, 32 dimension
features from each layer. Our image encoder outputs 128 dimen-
sion feature in the final layer. A 1-d depth feature is produced by
perspective projection like [51]. We concatenate these features and
pass them through a MLP like [51] with dimensions of 242, 1024,
512, 256, 128, 1. For the training data of human reconstruction, we
sample points 2 𝑐𝑚, 3 𝑐𝑚, 6 𝑐𝑚 and uniformly in the 3D space. We
group the surface samplings, and combine it with uniform sampling
using a ratio of 16 : 1. We train the human reconstruction network
with Adam optimizer at the batch size of 4. The learning rate is
1e-4 and sampling strategy is (3 𝑐𝑚,6 𝑐𝑚, uniform) for the first 20
epochs. Then we reduce learning rate to 1e-5 and change sampling
strategy to (2 𝑐𝑚,3 𝑐𝑚, uniform) for further 100 epochs. In the object
tracking stage, we use silhouette rendering in pytorch3d [49] under
perspective projection. The Adam optimizer with learning rate of
1e-2 is used for a 100-iteration tracking optimization. For the neural
blending network, we train it with Adam optimizer at the learning
rate of 1e-4 and the batch size of 4. We train this network 44,000
iterations.

B CAMERA SETTING
Pioneer work [57] evaluates the influence of sparse view number
in their section 5, and finds that six cameras serve as a good com-
promise between rendering artifacts and the camera number. Since
human-object interaction is even more challenging, we choose “six”
for stable object tracking, robust human tracking and high-fidelity
rendering. Finally, we place the cameras uniformly to minimize the
average artifacts in circle rendering.

C EFFICIENCY
Compared with SOTAs, our method doesn’t need per-scene training
which improves efficiency in practice greatly(save several hours
per-scene). However, one promising direction is to add the real-time
ability into our framework to enable immersive telepresence under
the human-object interaction scenario. Our method takes 0.2s to
estimate SMPL, 7s to generate mesh, 7s to track object, and 0.25s to
render in single frame. We believe the coarse-to-fine strategy [57]
and the end-to-end 6DoF estimation [46] can accelerate human
reconstruction and object tracking respectively.

