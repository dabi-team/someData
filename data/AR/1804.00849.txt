9
1
0
2

v
o
N
8

]
T
S
.
h
t
a
m

[

2
v
9
4
8
0
0
.
4
0
8
1
:
v
i
X
r
a

Robust estimation of stationary
continuous-time ARMA models
via indirect inference

Vicky Fasen-Hartmann †

Sebastian Kimmig ‡

−

≥

2p

In this paper we present a robust estimator for the parameters of a stationary continuous-
time ARMA(p, q) (CARMA(p, q)) process sampled equidistantly which is not necessar-
ily Gaussian. Therefore, an indirect estimation procedure is used. It is an indirect es-
timation because we ﬁrst estimate the parameters of the auxiliary AR(r) representation
(r
1) of the sampled CARMA process using a generalized M- (GM-)estimator.
Since the map which maps the parameters of the auxiliary AR(r) representation to the
parameters of the CARMA process is not given explicitly, a separate simulation part is
necessary where the parameters of the AR(r) representation are estimated from simulated
CARMA processes. Then, the parameter which takes the minimum distance between the
estimated AR parameters and the simulated AR parameters gives an estimator for the
CARMA parameters. First, we show that under some standard assumptions the GM-
estimator for the AR(r) parameters is consistent and asymptotically normally distributed.
Next, we prove that the indirect estimator is consistent and asymptotically normally dis-
tributed as well using in the simulation part the asymptotically normally distributed LS-
estimator. The indirect estimator satisﬁes several important robustness properties such as
weak resistance, πdn-robustness and it has a bounded inﬂuence functional. The practical
applicability of our method is demonstrated through a simulation study with replacement
outliers and compared to the non-robust quasi-maximum-likelihood estimation method.

AMS Subject Classiﬁcation 2010: Primary: 62F10, 62F12, 62M10

Secondary: 60G10, 60G51

Keywords: AR process, CARMA process, indirect estimator, inﬂuence functional, GM-estimator,
LS-estimator, outlier, resistance, robustness

†Institute of Stochastics, Englerstraße 2, D-76131 Karlsruhe, Germany.
‡Württembergische Versicherung AG, Gutenbergstraße 30 D-70176 Stuttgart, Germany.

1

 
 
 
 
 
 
1 Introduction

The paper presents a robust estimator for the parameters of a discretely observed stationary continuous-
time ARMA (CARMA) process. A weak ARMA(p, q) process in discrete-time is a weakly stationary
solution of the stochastic difference equation

∈
where B denotes the backward shift operator (i.e. BXm = Xm

φ(B)Xm = θ(B)Zm, m

Z,

1),

−

φ(z) = 1

φ1z

. . .

−

−

−

φpzp

and θ(z) = 1 + θ1z + . . . + θqzq

(1.1)

= 0 and (Zm)m

are the autoregressive and the moving average polynomials, respectively, with φ1, . . . ,φp, θ1, . . . ,θq
R, φp,θq
mean and constant variance.
sequence then we call (Xm)m
difference equation with i.i.d. noise (Zm)m

∈
Z is an uncorrelated sequence with constant
∈
If (Zm)m
Z is even an independent and identically distributed (i.i.d.)
∈
Z a strong ARMA process. A natural continuous-time analog of this
∈
Z is the formal p-th order stochastic differential equation
∈

Z a weak white noise, i.e., (Zm)m
∈

∈
where D denotes the differential operator with respect to t,

a(D)Yt = c(D)DLt,

t

R,

(1.2)

a(z) = zp + a1zp
−

1 + . . . + ap

and

c(z) = c0zq + c1zq
−

1 + . . . + cq

∈

R, ap, c0

= 0. The process (Lt)t

respectively, with p > q, and
are the autoregressive and the moving average polynomials,
a1, . . . , ap, c0, . . . , cq
R is a Lévy process, i.e., a stochastic process
∈
with L0 = 0 almost surely, independent and stationary increments and almost surely càdlàg sample
paths. However, this is not the formal deﬁnition of a CARMA(p, q) process because a Lévy pro-
cess is not differentiable. The idea is more that the differential operator on the autoregressive side
act like an integration operator on the moving average side. The precise deﬁnition of a CARMA
process is given later. A rigorous foundation for CARMA(p, 0) processes is provided in Bergstrom
(1983, 1984) and for CARMA(p, q) processes in Brockwell (2001). A Lévy driven CARMA process
can be deﬁned via a controller canonical state space representation. Necessary and sufﬁcient condi-
tions for the existence of strictly stationary CARMA processes are given in Brockwell and Lindner
(2009). From Brockwell and Lindner (2009) (see as well Thornton and Chambers (2017)) it is also
well known that a discretely sampled stationary CARMA process (Ymh)m
Z (h > 0 ﬁxed) admits a
∈
weak ARMA representation, but unfortunately this is in general for Lévy driven models not a strong
ARMA representation. For an overview and a comprehensive list of references on CARMA processes
we refer to Brockwell (2014) and Chambers and Thornton (2018).

In many situations it is more appropriate to specify a model in continuous time rather than in
discrete time. In recent years the interest in these models has increased with the availability of high-
frequency data in ﬁnance and turbulence but as well by irregularly spaced data, missing observations
or situations when estimation and inference at various frequencies is to be carried out. It is not sur-
prising that stationary CARMA processes are applied in many areas as, e.g., signal processing and
control (cf. Garnier and Wang (2008); Larsson et al. (2006)), high-frequency ﬁnancial econometrics
(cf. Todorov (2009)) and ﬁnancial mathematics (cf. Benth et al. (2014a,b)). The ﬁrst attempts for
maximum-likelihood estimation of Gaussian stationary and non-stationary MCAR(p) models are go-
ing back Harvey and Stock (1985a,b, 1989) and were further explored in the well-known paper of

2

6
6
Zadrozny (1988). Zadrozny (1988) investigates continuous-time Brownian motion driven ARMAX
models and allows stocks and ﬂows at different frequencies, and higher order integration. There
exist a few papers dealing with the asymptotic properties of parameter estimators of discretely sam-
pled stationary CARMA models as Schlemm and Stelzer (2012); Brockwell et al. (2011) and for non-
stationary CARMA models Fasen-Hartmann and Scholz (2017). The papers have in common that
they use a quasi maximum likelihood estimator (QMLE). However, it is well known that QMLE are
sensitive to outliers and irregularities in the data. Hence, we are looking for an alternative robust
approach.

In statistics the most fundamental question when considering robustness of an estimator is how the
estimator behaves when the data does not satisfy the model assumptions (cf. Huber and Ronchetti
(2009); Maronna et al. (2006); Olive (2017)). In the case of small deviations from the model assump-
tions a robust estimator should give estimations not too far away from the estimations of the original
model. The most common and best understood robustness property is distributional robustness where
the shape of the true underlying distribution deviates slightly from the assumed model. The amount of
measures for robustness is huge, e.g., qualitative robustness, quantitative robustness, optimal robust-
ness, efﬁciency robustness and the breakdown point, to mention only a few. In contrast to the case of
i.i.d. random variables, in the case of time series, there exist several types of possible contamination
of the data which makes it more difﬁcult to characterize robustness. In particular, for AR processes
it is well-known that the GM-estimator (cf. Boente et al. (1987); Künsch (1984); Martin (1980)) and
the RA-estimator (cf. Ben et al. (1999)) satisfy different robustness properties in contrast to M- or
LS-estimators which are sensitive to the presence of additive outliers (cf. Denby and Martin (1979)).
However, for general ARMA models the GM-estimator and the RA-estimator are again sensitive to
outliers and hence, non-robust (cf. Bustos and Yohai (1986)). Muler et al. (2009) develop a robust es-
timation procedure for ARMA models by calculating the residuals of the ARMA models with the help
of BIP-ARMA models. For their result it is essential to have a strong ARMA model. Unfortunately
the results can not easily be extended to weak ARMA models which we have in our context.

In this paper we use the indirect inference method originally proposed by Smith (1993) for non-
linear dynamic economic models. That paper was extended by Gallant and Tauchen (1996) and
Gouriéroux et al. (1993) (see also the overview in Gouriéroux and Monfort (1997)) for models with
intractable likelihood functions and moments. If the likelihood function and moments are intractable
maximum likelihood estimation and generalized methods of moments are infeasible. The authors
applied the indirect inference method to macroeconomics, microeconomics, ﬁnance and auction mod-
els; see as well Monfort (1996); Phillips and Yu (2009) for applications to continuous-time models,
Gouriéroux et al. (2000); Kyriacou et al. (2017) for applications to time series models and Monfardini
(1998) for applications to stochastic volatility models. In addition indirect inference is used for bias re-
duction in ﬁnite samples as, e.g., in Gouriéroux et al. (2000, 2010); Yu (2011); Kyriacou et al. (2017);
do Rêgo Sousa et al. (2019). Our motivation for the indirect inference method is robust estimation
(cf. de Luna and Genton (2001, 2000); Kyriacou et al. (2017)). An alternative approach for bias cor-
rection is given in Wang, Phillips and Yu (2011) for univariate and multivariate diffusion models. For
estimators of the mean revision parameter based on the Euler approximation and the trapezoidal ap-
proximation for discretization the authors calculate the bias and relate it to the estimation bias and
discretization bias.

The core idea of the indirect estimation method is to avoid estimating the parameters of interest
directly and instead ﬁt an auxiliary model to the data, estimate the parameters of this auxiliary model
and then use this estimates with simulated data to construct an estimator for the original parame-
ter of interest (see de Luna and Genton (2001) for a schematic overview over the indirect estimation
method). de Luna and Genton (2001, 2000) recognized that it is possible to construct robust estima-

3

tors via this approach, even for model classes where direct robust estimation is difﬁcult. The reason
is that it is sufﬁcient if the parameters of the auxiliary model are estimated by a robust estimation
method. Therefore, de Luna and Genton (2001) present an indirect estimation procedure for strong
ARMA processes (without detailed assumptions and rigorous proofs). They ﬁt an AR(r) process to
the ARMA model and estimate the parameters of the AR(r) process with a GM-estimator. We present
a similar approach in our paper for the estimation of the CARMA parameters. Since the discretely
sampled stationary CARMA process admits a weak ARMA representation instead of a strong ARMA
representation several proofs have to be added and identiﬁability issues have to be taken into account.
The paper is structured as follows. In Section 2, we ﬁrst present our parametric family of stationary
CARMA processes and our model assumptions. Furthermore, we motivate that for any r
1
any stationary CARMA process has an AR(r) representation. Then, in Section 3, we introduce the
indirect estimation procedure and give sufﬁcient criteria for indirect estimators to be consistent and
asymptotically normally distributed independent of the model; we have to assume at least consistent
and asymptotically normally distributed estimators in the estimation part and in the simulation part of
the indirect estimation method. Since the auxiliary AR(r) parameters of the sampled CARMA process
are estimated by a GM-estimator we give an introduction into GM-estimators in Section 4 and derive
consistency and asymptotic normality of this estimator in our setup. Moreover, we see that the GM-
estimator is still asymptotically normally distributed for CARMA processes with outliers as additive
outliers and replacement outliers. Our conclusions extend the results of Bustos (1982). Finally, in
Section 5, we are able to show that the indirect estimator for the parameters of the discretely observed
stationary CARMA process is consistent and asymptotically normally distributed using in the estima-
tion part a GM-estimator and in the simulation part a LS-estimator. Several robustness properties of
this estimator are derived as well as qualitative robustness and a bounded inﬂuence functional. After
all, the simulation part, in Section 6, shows the practical applicability of our indirect estimator and its
robustness properties. We compare our estimator with the non-robust QMLE. Conclusions are given
in Section 7. The paper ends with the proofs of the results in Section 8.

2p

≥

−

Notation
We use as norms the Euclidean norm
For a matrix A
ϑ
similarly ∇2
convergence in probability. In general C denotes a constant which may change from line to line.

d we denote by AT its transpose. For a matrix function f (ϑ) in Rm
×
Rdm

d which is submultiplicative.
d with
s and
P
→

Rs the gradient with respect to the parameter vector ϑ is ∇ϑ f (ϑ) = ∂vec( f (ϑ))

∂ϑT
for weak convergence and

s. Finally, we write D
−→

ϑ f (ϑ) = ∂vec(∇ϑ f (ϑ))

in Rd and its operator

in Rm

Rdms

Rm

k·k

k·k

for

∂ϑT

∈

∈

∈

∈

×

×

×

×

2 Preliminaries
2.1 The CARMA model
In this paper we consider a parametric family of stationary CARMA processes. Let Θ
(N(Θ)
c0(ϑ), . . . , cp

N) be a parameter space, p

N be ﬁxed and for any ϑ

= 0 and c j(ϑ)

= 0 for some j

R, ap(ϑ)

0, . . . , p

1(ϑ)

∈

∈

∈

RN(Θ)
Θ let a1(ϑ), . . . , ap(ϑ),
. Furthermore, de-
1
}

−

⊆

−

∈

∈ {

4

6
6
ﬁne

0

1

Aϑ :=

0
...
0
ap(ϑ)











−

−

0
...
0
1(ϑ)

ap

−

0

1
. . .
. . .
. . .

. . .
. . .
. . .
0
. . .

0
...
0
1
a1(ϑ)











−

Rp
×

p,

∈

q(ϑ) = sup

j

0, . . . , p

{
cϑ := (cq(ϑ)(ϑ), cq(ϑ)

∈ {

1
}

: cl(ϑ) = 0
−
}
1(ϑ), . . . , c0(ϑ), 0, . . . , 0)T

l > j

∀

with
Rp.

sup /0 := p

1,

−

The CARMA process (Yt (ϑ))t
tion: Let (Xt(ϑ))t

−

∈
R is then deﬁned via the controller canonical state space representa-
∈

R be a strictly stationary solution to the stochastic differential equation
∈

where ep denotes the p-th unit vector in Rp. Then the process

dXt(ϑ) = AϑXt(ϑ) dt + ep dLt,

R,

t

∈

Yt(ϑ) := cT

ϑXt(ϑ),

R,

t

∈

(2.1a)

(2.1b)

is said to be a (stationary) CARMA process of order (p, q(ϑ)). Rewriting (2.1) line by line (Yt (ϑ))t
can be interpreted as solution of the differential equation (1.2); see Brockwell (2001); Marquardt and Stelzer
(2007). This means that in our parametric family of CARMA processes the order of the autoregres-
sive polynomial is ﬁxed to p but the order of the moving average polynomial q(ϑ) may change. In
addition, we investigate only stationary CARMA processes.

R

∈

Furthermore, we have the discrete-time observations Yh, . . . ,Ynh of the CARMA process (Yt )t

R =
∈
R with ﬁxed grid distance h > 0. Hence, the true model parameter is ϑ0. The aim of this
(Yt (ϑ0))t
∈
paper is to receive from the observations Yh, . . . ,Ynh an estimator for ϑ0. Throughout the paper we will
assume that the following Assumption A holds.

Assumption A.

(A.1) The parameter space Θ is a compact subset of RN(Θ).

(A.2) The true parameter ϑ0 is an element of the interior of Θ.

(A.3) E[L1] = 0, 0 < EL2

1 = σ2

L < ∞ and there exists a δ > 0 such that E

L1
|

4+δ < ∞.
|

(A.4) The eigenvalues of Aϑ have strictly negative real parts.

(A.5) For all ϑ

Θ the zeros of cϑ(z) = c0(ϑ)zq(ϑ) + c1(ϑ)zq(ϑ)

1 + . . . + cq(ϑ) are different from

−

∈
the eigenvalues of Aϑ.

Θ we have (cϑ, Aϑ)

= (cϑ′, Aϑ′).

(A.6) For any ϑ,ϑ′ ∈
(A.7) For all ϑ

∈
the imaginary part of z.

Θ the spectrum of Aϑ is a subset of

C :

z
{

∈

h < Im(z) < π
π
h }

−

where Im(z) denotes

(A.8) The maps ϑ

Aϑ and ϑ

7→

7→

cϑ are three times continuous differentiable.

5

6
Remark 2.1.

(i) (A.1) and (A.2) are standard assumptions in point estimation theory.

(ii) (A.4) guarantees that there exists a stationary solution of the state process (2.1a) and hence, a
R (see Marquardt and Stelzer (2007)). For this reason we
∈
R is stationary. The assumption of a
∈
R is essential for the indirect estimation approach of this
∈

stationary CARMA process (Yt (ϑ))t
can and will assume throughout the paper that (Yt (ϑ))t
stationary CARMA process (Yt (ϑ))t
paper.

(iii) A consequence of (A.4), (A.8), the compactness of Θ and the fact that the eigenvalues of a ma-
λ
|

Θ max
trix are continuous functions of its entries (cf. Bernstein (2009, Fact 10.11.2)) is supϑ
∈
ρu for some C,ρ > 0.
λ is eigenvalue of eAϑ

< 1 and hence, supϑ
Θ k
∈
(iv) Due to (A.5) the state space representation (2.1) of the CARMA process is minimal (cf. Bernstein

C e−

eAϑu

k ≤

{|

}

:

(2009, Proposition 12.9.3) and Hannan and Deistler (2012, Theorem 2.3.3)).

(v) A consequence of (A.5) and (A.6) is that the family of stationary CARMA processes (Yt (ϑ))t

R
is identiﬁable from their spectral densities and in combination with (A.7) that the same is true
for the discrete-time process (Ymh(ϑ))m

Z (cf. Schlemm and Stelzer (2012, Theorem 3.13)).
∈

∈

(vi) The CARMA process has to be sampled sufﬁciently ﬁnely to ensure that (A.7) holds so that the

parameters can be identiﬁed from the discrete data.

In the following we denote the autocovariance function of the stationary CARMA process (Yt (ϑ))t

as (γϑ(t))t

R which has by Schlemm and Stelzer (2012, Proposition 3.1) the form
∈

R

∈

γϑ(t) = Cov(Ys+t(ϑ),Ys(ϑ)) = cT

ϑ eAϑt Σϑcϑ,

R, t

s

∈

≥

0,

(2.2)

with Σϑ = σ2
L
continuous differentiable as well.

∞
0 eAϑu epeT
R

p eAϑu du. Due to Assumption A the autocovariance function is three times

2.2 The AR(r) representation of a stationary CARMA process

First, we deﬁne the auxiliary AR(r) representation of the sampled CARMA process (Ymh(ϑ))m

Z.
∈

Proposition 2.2. For every ϑ

Θ and every r

2p

−

≥

∈

1, there exists a unique

such that

π(ϑ) := (π1(ϑ), . . . ,πr(ϑ),σ(ϑ))

Rr

∈

×

[0, ∞)

Um(ϑ) := Ymh(ϑ)

r
∑
k=1

−

πk(ϑ)Y(m

k)h(ϑ)

−

is stationary with E[U1(ϑ)] = 0, Var(U1(ϑ)) = σ2(ϑ) and

E

Um(ϑ)Y(m

−

k)h(ϑ)

= 0

for k = 1, . . . , r.

We call π(ϑ) the auxiliary parameter of the AR(r) representation of (Ymh(ϑ))m

(cid:2)

(cid:3)

(2.3)

(2.4)

Z.
∈

Remark 2.3. Um(ϑ) can be interpreted as the error of the best linear predictor of Yϑ(mh) in terms of
Y(m
r)h(ϑ). Per construction, however, the sequence (Um(ϑ))m
Z is not an uncorre-
∈
lated sequence, Um(ϑ) is only uncorrelated with Y(m

1)h(ϑ), . . . ,Y(m

1)h(ϑ), . . . ,Y(m

r)h(ϑ).

−

−

−

−

6

Deﬁnition 2.4. Let Π
stationary AR(r) processes. The map π : Θ
→
is called the link function or binding function.

⊆

Rr+1 be the parameter space containing all possible parameter vectors of
π(ϑ) and π(ϑ) as given in Theorem 2.2

Π with ϑ

7→

Lemma 2.5. Let r

2p

−

≥

1. Then, π(ϑ) is injective and three times continuously differentiable.

Finally, due to Lemma 2.5 we suppose throughout the paper:

Assumption B. Let r

2p

1.

−

≥

3 Indirect estimation

b

b

1(

πn an estimator of π(ϑ0) that

πn) would be an estimator for ϑ0 = π−

For ﬁxed r, denote by
is calculated from the observations
Y n = (Yh, . . . ,Ynh). If we were able to analytically invert the link function π and calculate π−
πn),
1(π(ϑ0)). However, this is not possible in gen-
then π−
1 exists. To overcome this problem, we perform a second
eral since no analytic representation of π−
estimation, which is based on simulations, and constitutes the other building block of indirect esti-
mation. We ﬁx a number s
∈
1 = 0 and E(LS
with ELS
1)2 = σ2
associated CARMA process (Y S
“pseudo–observations” Y sn
we estimate again π(ϑ) by an estimator
mator for ϑ0 which minimizes a suitable distance between
follows.

N and simulate a sample path of length sn of a Lévy process (LS
R
L . Then, for a ﬁxed parameter ϑ
Θ we generate a sample path of the
R using the simulated path (LS
R. This gives us a vector of
t (ϑ))t
∈
∈
snh(ϑ)) of length sn. From this observation Y sn
h (ϑ), . . . ,Y S
S (ϑ) = (Y S
S (ϑ)
πS
sn(ϑ). The idea is now to choose that value of ϑ as esti-
πS
n (ϑ). The formal deﬁnition is as

πn and

t )t

t )t

1(

∈

b

∈

b

Deﬁnition 3.1. Let
estimator for π(ϑ) calculated from the pseudo–observations Y sn
RN(Θ)
Ω
deﬁned as

πn be an estimator for π(ϑ0) calculated from the data Y n, let
h (ϑ), . . . ,Y S

N(Θ) be a symmetric positive deﬁnite weighting matrix. The function LInd : Θ
b

S (ϑ) = (Y S

∈

b

×

πS
sn(ϑ) be an
ϑ(snh)) and let
[0, ∞) is

→

b

b

LInd(ϑ, Y n) := [

πn

πS
sn(ϑ)]T Ω[

πn

πS
sn(ϑ)].

−

−

Then, the indirect estimator for ϑ0 is

b
b
n = arg min

ϑInd

b

b

LInd(ϑ, Y n).

ϑ

Θ

∈

We are able to present general conditions under which this indirect estimator is consistent and

b

asymptotically normally distributed.

Theorem 3.2.

(a) Suppose that the following assumptions are satisﬁed:

∞.

→
π(ϑ)
k

−

P
→

0 as n

∞.

→

πn

(C.1)

π(ϑ0) as n

P
→
(C.2) supϑ
Θ k
b
∈
Deﬁne the map
b
QInd : Θ

n (ϑ)

πS

[0, ∞)

as ϑ

[π(ϑ)

−

7→

→

π(ϑ0)]T Ω[π(ϑ)

π(ϑ0)].

−

(3.1)

7

Then

LInd(ϑ, Y n)

sup
Θ|
ϑ
∈

QInd(ϑ)
|

−

P
→

0

and

ϑInd
n

P
→

ϑ0.

If we replace in (C.1) and (C.2) convergence in probability by almost sure convergence then we
can replace in the statement convergence in probability by almost sure convergence as well.

b

(b) Assume additionally to (C.1) and (C.2):

(C.4) √n(

(C.3) √n(

πS
n (ϑ)

−

N (0,ΞS(ϑ)) as n

π(ϑ)) D
−→
N (0,ΞD(ϑ0)) as n

π(ϑ0)) D
−→

→
∞.

−
(C.5) For any sequence (ϑn)n
N with ϑn
∈

→
ϑ0 as n

P
→

πn
b

b

∞ for any ϑ

Θ.

∈

∞ the asymptotic behaviors

→

hold as n

→
∞,

Then, as n

→

where

with

P
πS
n (ϑn)
→
πS
n (ϑn) = OP(1),
b
∞ and ∇ϑπ(ϑ0) has full column rank N(Θ).
b

∇ϑ
∇2
ϑ

∇ϑπ(ϑ0),

√n(

ϑInd

n −

ϑ0) D
−→

N (0,ΞInd(ϑ0)),

b

ΞInd(ϑ0) = JInd(ϑ0)−

1IInd(ϑ0)JInd(ϑ0)−

1

JInd(ϑ0) = [∇ϑπ(ϑ0)]T Ω[∇ϑπ(ϑ0)]

and

IInd(ϑ0) = [∇ϑπ(ϑ0)]T Ω

ΞD(ϑ0) +
(cid:20)

1
s

ΞS(ϑ0)
(cid:21)

Ω[∇ϑπ(ϑ0)].

Gouriéroux et al. (1993) develop for a dynamic model as well the consistency and the asymptotic
normality of the indirect estimator but under different assumptions mainly based on LInd(ϑ, Y n) (see
as well Smith (1993)). These results are again summarized in Gouriéroux and Monfort (1997). In
the context of indirect estimation of ARMA models, de Luna and Genton (2001, p.22) mention the
asymptotic normality of their indirect estimator but without stating any regularity conditions and only
referring to Gouriéroux and Monfort (1997, Proposition 4.2).

Remark 3.3.

(a) The asymptotic covariance matrix can be written as

ΞInd(ϑ0) = H (ϑ0)

ΞD(ϑ0) +

(cid:18)

1
s

ΞS(ϑ0)

H (ϑ0)T ,

(cid:19)

where H (ϑ0) = [∇ϑπ(ϑ0)T Ω∇ϑπ(ϑ0)]−
de Luna and Genton (2001, Eq. (4.4)).

1[∇ϑπ(ϑ0)]T Ω. This is the analog form of

(b) Note that the asymptotic results hold for any r

1. But increasing the auxiliary AR order
≥
does not necessarily yield better results. On the other hand, increasing s increases the efﬁciency.
H (ϑ0)ΞD(ϑ0)H (ϑ0)T . The best efﬁciency is received for
For s
→
Ω = [ΞD(ϑ0)]−

1 in which case ΞInd(ϑ0)

∇ϑπ(ϑ0)T ΞD(ϑ0)−

∞ we receive ΞInd(ϑ0)

1∇ϑπ(ϑ0)

2p

→

−

−

.

1

s
∞
→
→

(cid:2)

8

(cid:3)

Remark 3.4. A fundamental assumption for Proposition 2.2 is (A.4) resulting in the existence of
stationary CARMA processes. In particular, in the case of integrated CARMA processes (Yt (ϑ))t
R,
∈
where Aϑ has eigenvalue 0, the result of Proposition 2.2 does not hold in general. For this reason
the indirect estimation approach of this paper can not be extended to integrated CARMA processes
which are non-stationary. Even for integrated CARMA processes it is well known that estimators
for the parameter determining the integration have a n convergence instead of a √n convergence
(cf. Chambers and McCrorie (2007); Fasen-Hartmann and Scholz (2017); Chambers and Thornton
(2018)).

Remark 3.5. The discretely observed stationary CARMA(p, q(ϑ)) process (Ymh(ϑ))m
Z admits a
∈
representation as a stationary ARMA(p, p
1) process with weak white noise of the form

−

φ(B)Ymh(ϑ) = θ(B)εm(ϑ),

(3.2)

−

Z is a weak ARMA(p, p
∈

p
ehλi z) (the λi being the eigenvalues of Aϑ), θ(z) is a monic, Schur–stable poly-
where φ(z) = ∏
i=1(1
−
Z is a weak white noise (see Brockwell and Lindner (2009, Lemma 2.1)), i.e.
nomial and (εm(ϑ))m
∈
(Ymh(ϑ))m
1) process. Such an exact discrete-time ARMA representation
for multivariate CARMA processes was generalized in (Thornton and Chambers, 2017, Theorem 1) to
possible non-stationary multivariate CARMA processes. Thus, it is as well possible to do an indirect
estimation procedure by estimating the parameters of the discrete-time ARMA(p, p
1) represen-
tation, e.g., using maximum-likelihood, instead of estimating the parameters of the auxiliary AR(r)
model. Then the map π is replaced by the map π1 which maps the parameters of the CARMA process
to the coefﬁcients of the weak ARMA(p, p
1) representation of its sampled version (3.2). Using
π1(ϑ) instead of π(ϑ) in Theorem 3.2, Theorem 3.2 can be adapted under the same assumptions giv-
ing asymptotic normality of the indirect estimator based on the discrete-time ARMA representation
of the CARMA process. In particular, it is as well possible to derive an estimation procedure for
non-stationary CARMA processes. However, until now there does not exist robust estimators for the
parameters of weak ARMA processes such that this approach does not give robust estimators for the
parameters of the stationary CARMA process, which is the topic of this paper.

−

−

4 Estimating the auxiliary AR(r) parameters of a CARMA

process with outliers

In order to apply the indirect estimator to a discretely sampled stationary CARMA process we need
strongly consistent and asymptotically normally distributed estimators for the parameters of the aux-
iliary AR(r) representation.
In this section we will study generalized M- (GM-) estimators. The
GM-estimator will be applied to a stationary CARMA process afﬂicted by outliers because we want
to study some robustness properties of our estimator as well. Outliers can be thought as typical ob-
servations that do not arise because of the model structure but due to some external inﬂuence, e.g.,
measurement errors. Therefore, a whole sample of observations which contains outliers does not
come from the true model anymore but it is still close to it as long as the total number of outliers is
not overwhelmingly large.

Deﬁnition 4.1. Let g : [0, 1]
→
be a stochastic process taking only the values 0 and 1 with

[0, 1] be a function that satisﬁes g(γ)

−

γ = o(γ) for γ

0. Let (Vm)m

Z

∈

→

P(Vm = 1) = g(γ)

9

and let (Zm)m

Z be a real-valued stochastic process. The disturbed process (Y γ
∈

mh(ϑ))m

Z is deﬁned as
∈

Y γ
mh(ϑ) = (1

−

Vm)Ymh(ϑ) +VmZm.

(4.1)

The disturbed process (Y γ

mh(ϑ))m

Z is in general not a sampled CARMA process anymore.
∈

Remark 4.2.

∈

(a) The interpretation of this model is that at each point m

Z an outlier is observed with probability
g(γ) while the true value Ymh(ϑ) is observed with probability 1
g(γ). The model has the
advantage that one can obtain both additive and replacement outliers by choosing the processes
Z adequately. Speciﬁcally, to model replacement outliers, one assumes
Z and (Vm)m
(Zm)m
∈
∈
Z are jointly independent. Then, if the realization of Vm
that (Zm)m
Z and (Ymh(ϑ))m
Z, (Vm)m
∈
∈
∈
is equal to 1, the value Ymh(ϑ) will be replaced by the realization of Zm justifying the use of
the name replacement outliers. On the other hand, modeling additive outliers can be achieved
by taking Zm = Ymh(ϑ) + Wm for some process (Wm)m
Z is
∈
independent from (Vm)m
mh(ϑ) = Ymh(ϑ) + VmWm such that the realization
of Wm is added to the realization of Ymh(ϑ) if Vm is 1.

Z and assuming that (Ymh(ϑ))m
∈

Z. Then we have Y γ
∈

−

(b) Another advantage of this general outlier model is that one can easily model the temporal struc-
Z is chosen as an i.i.d. sequence with P(Vm = 1) = γ,
ture of outliers. On the one hand, if (Vm)m
∈
then outliers typically appear isolated, i.e., between two outliers there is usually a period of time
where no outliers are present. On the other hand, one can also model patchy outliers by letting
(Bm)m
Z be an i.i.d. process of Bernoulli random variables with success probability ε and
∈
setting Vm = max(Bm

l, . . . , Bm) for a ﬁxed l

N. Then as ε

0,

−

−
which results in γ = lε. For ε sufﬁciently small, outliers then appear in a block of size l.

−

P(Vm = 1) = 1

ε)l = lε+ o(ε),

∈
(1

→

Recall the following notion:

Deﬁnition 4.3. A stationary stochastic process Y = (Yt )t
α-) mixing if

I with I = R or I = Z is called strongly (or
∈

αl := sup

(cid:8)
where F 0
0) and F ∞
−
0 < α < 1 we call Y = (Yt )t

∞ = σ(Yt : t

≤

∩

−

B)

: A

P(A
|

P(A)P(B)
|

F 0
∈
−
l). If αl
I exponentially strongly mixing.
∈

l = σ(Yt : t

≥

≤

∞, B

F ∞
l

l

∞
→
→

0

∈
Cαl for some constants C > 0 and

(cid:9)

Assumption D.

(D.1) The processes (Vm)m

Z and (Zm)m
∈

Z are strictly stationary with E
∈

V1
|

|

(D.2) Either we have the replacement model where the processes (Ymh(ϑ))m

Z, (Vm)m
Z
∈
Z are exponentially strongly mixing, i.e.,
are jointly independent, and (Vm)m
∈
Cρm and αZ(m)
N. Or we have
αV (m)
∈
the additive model with Zm = Ymh(ϑ) + Wm where the processes (Ymh(ϑ))m
Z and
Z, (Vm)m
∈
∈
(Wm)m
Z are exponentially strongly mix-
∈
ing.

Z are jointly independent, and (Vm)m
∈

Cρm for some C > 0, ρ

Z and (Zm)m
∈

Z and (Wm)m
∈

(0, 1) and every m

≤

≤

∈

∈

< ∞ and E

< ∞.

Z1
|
|
Z and (Zm)m
∈

10

(D.3) For all a

R,π

∈

∈

Rr with

+

> 0:

a
π
|
|
k
k
(r+1)h(ϑ) + π1Y γ
P(aY γ

rh(ϑ) + . . . + πrY γ

h (ϑ) = 0) = 0.

We largely follow the ideas of Bustos (1982) for the GM-estimation of AR(r) parameters, however
our model and our assumptions are slightly different. Assumption D corresponds to Bustos (1982,
Assumption (M2),(M4),(M5)). The main difference is that the sampled stationary CARMA process
(Ymh)m
Z is in Bustos (1982) an inﬁnite-order moving average process whose noise is Φ–mixing
∈
which is in general not satisﬁed for a sampled stationary CARMA process. However, we already
know from Marquardt and Stelzer (2007, Proposition 3.34) that a stationary CARMA process is ex-
ponentially strongly mixing which is weaker than Φ-mixing. Therefore, we assume that (Vm)m
Z,
∈
(Zm)m
Z are exponentially strongly mixing instead of Φ-mixing as in Bustos (1982).
∈

Z and (Wm)m
∈

In the following we deﬁne GM-estimators. Let two functions φ : Rr

R and χ : R

R
×
→
2h(ϑ), . . . ,Y γ
h (ϑ),Y γ

R be
→
nh(ϑ)) from the

given. Moreover, assume that we have observations Y n,γ(ϑ) = (Y γ
disturbed process in (4.1). The parameter

πGM(ϑγ) = (πGM

1

(ϑγ), . . . ,πGM

r

(ϑγ),σGM(ϑγ))

is deﬁned as the solution of the equations

Y γ
h (ϑ)
...
Y γ
rh(ϑ)

E 

φ












,






Y γ
(r+1)h(ϑ)

−

π1Y γ

rh(ϑ)
σ

. . .

−

−

πrY γ

h (ϑ)





Y γ
(r+1)h(ϑ)

−

π1Y γ

rh(ϑ)
σ

. . .

−

−

E

χ









Y γ
h (ϑ)
...
Y γ
rh(ϑ)
2





πrY γ
h (ϑ)

= 0,

(4.2a)











= 0

(4.2b)





!





(0, ∞). The idea is again that these are the parameters of the auxiliary AR

Rr
for (π1, . . . ,πr,σ)
∈
×
representation of (Y γ
mh(ϑ))m
Z
as well. We choose not to indicate this in the notation to make the exposition more readable. For
Z we also write πGM(ϑ) instead of πGM(ϑ0). Now, the GM-
the uncontaminated process (Ymh(ϑ))m
∈
πGM
πGM
n,r (ϑγ),
n,1 (ϑγ), . . . ,
(ϑγ) = (
estimator

Z. Note that πGM(ϑγ) depends on the processes (Vm)m
∈

(ϑγ)) based on φ and χ is deﬁned to satisfy

Z and (Zm)m
∈

σGM
n

πGM
n

∈



Y γ
kh(ϑ)
b
...

Y γ
1)h(ϑ)


(k+r

−

Y γ
(k+r)h(ϑγ)


,






b
r
n
−
φ
∑

k=1





n
r
−
∑
k=1

χ

1

n

r

−

1

n

r

−





Y

γ
(k+r)h

b
πGM
n,1 (ϑγ)Y
(ϑ)
−

b

b
γ
(k+r
1)h
−
σGM
(ϑγ)
n

b

n,1 (ϑγ)Y γ
πGM

1)h(ϑ)
(ϑγ)

(k+r
−
σGM
n

−

b

(ϑ)

...

−

−

πGM
n,r (ϑγ)Y

γ
kh(ϑ)





b

. . .

−

−



Y γ




(k+r




n,r (ϑγ)Y γ
πGM

b

Y γ
kh(ϑ)
...
1)h(ϑ)
2

−
kh(ϑ)

= 0, (4.3a)









= 0. (4.3b)



!



Throughout the paper we assume that there exists a solution of (4.3) although this is not always the
case in practice.

b

Example 4.4.

(a) There are two main classes of GM-estimators, the so–called Mallows estimators and the Hampel–
Krasker–Welsch estimators. More information on them can be found in Bustos (1982);

11

 
 
Denby and Martin (1979); Martin (1980); Martin and Yohai (1986). In the literature, this kind
of estimators sometimes appear under the name BIF (for bounded inﬂuence) estimators. The
class of Mallows estimators are deﬁned as φ(y, u) = w(y)ψ(u), where w is a strictly positive
weight function and ψ is a suitably chosen robustifying function. The Hampel–Krasker–Welsch
estimators are of the form

φ(y, u) =

ψ(w(y)u)
w(y)

,

where w is a weight function and ψ is again a suitably chosen bounded function.

(b) Typical choices for ψ are the Huber ψk–functions (cf. Maronna et al. (2006, Eq. (2.28))). Those
for a constant k > 0. A possibility for w is,
for a Huber function ψk. Another choice for ψ is the so-called Tukey

functions are deﬁned as ψk(u) = sign(u) min
e.g., w(y) = ψk(
y
y
)/
|
|
|
|
bisquare (or biweight) function which is given by

, k
u
|
{|

}

ψ(u) = u

1

(cid:18)

2

1

u2
k2

−

(cid:19)

,

u
|≤
{|

k

}

where k is a tuning constant.

(c) For the function χ, a possibility is χ(x2) = ψ2(x)

the deﬁnition of φ. The random variable Z is suitably distributed.

EZ[ψ2(Z)] with the same ψ function as in

−

In order to develop an asymptotic theory and to obtain a robust estimator it is necessary to impose
assumptions on φ and χ which we will do next analogous to Bustos (1982, (E1) - (E6)):

Assumption E.

Suppose φ : Rr

(E.1) For each y

Rr, the map u

∈

×

7→

R and χ : R

R

→

→

R satisfy the following assumptions:

φ(y, u) is odd, uniformly continuous and φ(y, u)

0 for u

0.

≥

≥

(E.2) (y, u)

7→

φ(y, u)y is bounded and there exists a c > 0 such that

φ(y, u)y
|

−

φ(z, u)z

| ≤

y
c
k

−

z
k

for all u

R.

∈

(E.3) The map u

φ(y,u)
u

7→

is non-increasing for y

Rr and there exists a u0

∈

R such that φ(y,u0)

u0

> 0.

∈

(E.4) φ(y, u) is differentiable with respect to u and the map u

∂φ(y,u)

∂u y is bounded.

∂φ(y,u)
∂u

7→

is continuous, while (y, u)

7→

Y γ
h (ϑ)
...
Y γ
rh(ϑ)

, u



Y γ
h (ϑ)
...
Y γ
rh(ϑ)

(E.5) E 

sup
R
u
∈




u 

∂
∂u

φ










(E.6) χ is bounded and increasing on



x :
{
Furthermore, χ is differentiable and x









< ∞.












(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

χ(x) < b
(cid:13)
}

(cid:13)
(cid:13)
(cid:13)


(cid:13)
(cid:13)


(cid:13)
χ(0).
a
(cid:13)
−
≤
−
xχ′(x2) is continuous and bounded. Lastly, χ(u2
0) > 0.
7→



R χ(x) and a =
where b = supx
∈




In the remaining of this section we always assume that Assumption D and E are satisﬁed.

Remark 4.5. As pointed out in Bustos (1982, p. 497) one can deduce from Maronna and Yohai
(1981, Theorem 2.1) that there exists a solution πGM(ϑγ)
(0, ∞) of equation (4.2) if
Assumption E holds. Moreover, there exists a compact set K
K and
for any π

Kc equation (4.2) does not hold (see Bustos (1982, p. 500)).

×
(0, ∞) with πGM(ϑγ)

∈
Rr

Rr

×

⊂

∈

∈

12

In general it is not easy to verify that πGM(ϑγ) is unique. Additionally, one would like to have
that πGM(ϑ0) = πGM(ϑ) = π(ϑ) are the parameters of the auxiliary AR(r) model in the case that the
GM-estimator is applied to realizations of an uncontaminated sampled stationary CARMA process
(Ymh(ϑ))m

Z. The following proposition gives a sufﬁcient condition.
∈

Proposition 4.6. Suppose that Ur+1(ϑ) as deﬁned in equation (2.3) satisﬁes

(Ur+1(ϑ),Yrh(ϑ), . . . ,Yh(ϑ)) D= (

Ur+1(ϑ),Yrh(ϑ), . . . ,Yh(ϑ)).

−

(4.4)

u
Assume further that the function u
| ≤
|
where u0 satisﬁes Assumptions (E.3) and (E.6), and the function χ is chosen in such a way that

φ(y, u) is nondecreasing and strictly increasing for

7→

u0,

U1(ϑ)
σ(ϑ)

E

χ

"

 (cid:18)

2

!#

(cid:19)

= 0.

(4.5)

Finally, assume that γ = 0 so that (Y γ
mh(ϑ))m
as deﬁned in Theorem 2.2 is the unique solution of (4.2), i.e., πGM(ϑ0) = π(ϑ).

Z = (Ymh(ϑ))m
∈

Z. Then the auxiliary parameter π(ϑ)
∈

Remark 4.7.

(a) Assumption (4.4) holds if the distribution of Ur+1(ϑ) is symmetric and Ur+1(ϑ) is independent

of (Yrh(ϑ), . . . ,Yh(ϑ)). This again is satisﬁed if (Lt )t

R is a Brownian motion.
∈

(b) The monotonicity assumption on φ is valid, e.g., for both the Mallows and Hampel–Krasker–

Welsch estimators when the function ψ is chosen as a Huber ψk–function with u0 = k.

(c) The assumption on χ is fulﬁlled, e.g.,

if χ is chosen as in Example 4.4(c) with
Var(U1(ϑ)). In the case that the driving Lévy process is a Brownian motion this
N (0, 1).

Z D= U1(ϑ)/
means that Z

p
∼

Theorem 4.8. Suppose
n
∞
πGM
→
n
→

that
πGM(ϑγ) P-a.s.

(ϑγ)

there exists a unique solution πGM(ϑγ) of

(4.2).

Then

The proof goes in the same vein as the proof of Bustos (1982, Theorem 2.1) and is therefore omitted.
b
Next, we would like to deduce the asymptotic normality of the GM-estimator. Let the set K be

given as in Remark 4.5 and for π = (π1, . . . ,πr,σ)

K deﬁne

∈

QGM(π,ϑγ) =

Y γ
h (ϑ)
...
Y γ
rh(ϑ)




E

χ

"

 (cid:18)

E 

φ






















Y γ
(r+1)h(ϑ)

π1Y γ
rh(ϑ)
σ

−

−

,



πrY γ

h (ϑ)

...

−

Y γ
(r+1)h(ϑ)

π1Y γ
rh(ϑ)
σ

−

−

πrY γ

h (ϑ)

...

−

Y γ
h (ϑ)
...
Y γ
rh(ϑ)









2


!#

(cid:19)





















.

(4.6)

For the proof of the asymptotic normality of the GM estimator we use a Taylor expansion of
QGM(π,ϑγ) at πGM(ϑγ). With the knowledge of the asymptotic behavior QGM(
(ϑγ),ϑγ) and
(ϑγ),ϑγ) it is then straightforward to derive the asymptotic behavior of the GM-estimator
∇πQGM(
πGM
(ϑγ).
n
b

We need the following auxiliary result which is the analog of Bustos (1982, Lemma 3.1) under our

πGM
n

πGM
n

b

different model assumptions.
b

13

Lemma 4.9. Deﬁne the map Ψ : Rr+1

×

Rr

(0, ∞)

→

Rr+1 as

×
y1
...
yr
χ






, yr+1

π1yr
−
σ

−

...

πry1

−

yr+1

π1yr
−
σ

−

...

πry1

−

y1
...
yr






2









.










Ψ(y,π) =

φ
















stochastic

Furthermore,
= Ψ(Y γ

kh(ϑ), . . . ,Y γ

(cid:16)(cid:0)
deﬁne
(k+r+1)h(ϑ),πGM(ϑγ)). Then

the

(cid:1)
process Ψ(ϑγ) = (Ψk(ϑγ))k

(cid:17)

as Ψk(ϑγ)

N

∈

1
√n

−

r

n
r
−
∑
k=1

Ψk(ϑγ) D
→

N (0, IGM(ϑγ)),

where the (i, j)-th component of IGM(ϑγ) is

[IGM(ϑγ)]i j = E [Ψ1,i(ϑγ)Ψ1, j(ϑγ)] + 2

∞
∑
k=1

E

Ψ1,i(ϑγ)Ψ1+k, j(ϑγ)

(4.7)

(cid:2)

(cid:3)

and Ψk,i(ϑγ) denotes the i–th component of Ψk(ϑγ), i = 1, . . . , r + 1. Especially, each [IGM(ϑγ)]i j is
ﬁnite for i, j

.
1, . . . , r + 1
}

∈ {

First, we derive the asymptotic behavior of the gradient ∇πQGM(πn,ϑγ).

Lemma 4.10. Let QGM(π,ϑγ) be deﬁned as in (4.6). Then the gradient ∇πQGM(π,ϑγ) exists. More-
πGM(ϑγ) as n
N with πn
over, for any sequence (πn)n
∈

∞ we have as n

∞,

→

P
→
∇πQGM(πn,ϑγ)

P
→

→
∇πQGM(πGM(ϑγ),ϑγ).

Next, we deduce the asymptotic normality of QGM(

πGM
n

(ϑγ),ϑγ).

Lemma 4.11. Let QGM(π,ϑγ) be deﬁned as in (4.6) and suppose that ∇πQGM(π,ϑγ) is non-singular.
Furthermore, let IGM(ϑγ) be given as in (4.7) and suppose that
∞.
Then, as n

πGM(ϑγ) as n

πGM
n

(ϑγ)

P
→

→

b

∞,

√n

−

rQGM(

πGM
n

(ϑγ),ϑγ) D
−→

N (0, IGM(ϑγ)).
b

The following analog version of Bustos (1982, Theorem 2.2) holds in our setting which gives the

b

asymptotic normality of the GM-estimator.

Theorem 4.12. Let QGM(π,ϑγ) be deﬁned as in (4.6) and suppose that JGM(ϑγ) := ∇πQGM(π,ϑγ)
in (4.7) and suppose that
is non-singular.
πGM
n

let IGM(ϑγ) be given as

πGM(ϑγ) as n

∞. Then, as n

Furthermore,

(ϑγ)

∞,

→

P
→

→

√n

−

r(

πGM
n

(ϑγ)

−

→
πGM(ϑγ)) D
−→

N (0,ΞGM(ϑγ)),

b

where

b

ΞGM(ϑγ) := [JGM(ϑγ)]−

1IGM(ϑγ)[JGM(ϑγ)]−

1.

(4.8)

14

5 The indirect estimator for the CARMA parameters

5.1 Asymptotic normality

In Section 3 we already introduced the indirect estimator and presented in Theorem 3.2 sufﬁcient
In the
criteria for the indirect estimator to be consistent and asymptotically normally distributed.
following we want to show that these assumptions are satisﬁed in the setting of discretely sampled
πS
n (ϑ) in the simulation part the least-squares- (LS-)
CARMA processes when we use as estimator
πGM
estimator
πn the GM-estimator
n
b
S (ϑ) = (Y S
b

Deﬁnition 5.1. Based on the sample Y sn
πLS
sn,1(ϑ), . . . ,
(

b
σLS
sn (ϑ)) of π(ϑ) minimizes

(ϑ0).
h (ϑ), . . . ,Y S

snh(ϑ)) the LS-estimator

πLS
n (ϑ) and for

πLS
sn (ϑ) =

πLS
sn,r(ϑ),

b

b

LLS(π, Y sn

b
b
S (ϑ)) :=

1

sn

r

−

sn
r
−
∑
k=1

Y S
(k+r)h(ϑ)

(cid:16)

π1Y S

(k+r

−

1)h(ϑ)

. . .

−

−

−

2

πrY S

kh(ϑ)
(cid:17)

b

(5.1)

in Π′ := π(Θ) and

σLS

sn (ϑ) is deﬁned as

σ2

b
LS,sn(ϑ) =

1

sn

r

−

sn
r
−
∑
k=1

Y S
(k+r)h(ϑ)

(cid:16)

πLS
sn,1(ϑ)Y S

(k+r

−

1)h(ϑ)

. . .

−

−

−

πLS
sn,r(ϑ)Y S

kh(ϑ)
(cid:17)

2

.

Remark 5.2. The quasi ML-function for the auxiliary AR(r) parameters of the discretely sampled
CARMA process is deﬁned as

b

b

b

LQMLE(π, Y sn

S (ϑ)) =

1

sn

r

−

sn
r
−
∑
k=1  

log(σ2) +

(Y S

(k+r)h(ϑ)

π1Y S

(k+r

−

1)h(ϑ)
−
σ2

. . .

−

−

πrY S

kh(ϑ))2

!

and the quasi ML-estimator as
S (ϑ)). It is well known that
for the estimation of AR(r) parameters the ML-estimator and the LS-estimator are equivalent (this
can be seen by straightforward calculations taking the derivatives of the ML-function LQMLE which
are proportional to the derivatives of LLS).

(ϑ) = arg minπ
∈

Π′ LQMLE(π, Y sn

b

πQMLE
sn

Theorem 5.3. Let Assumption A, B, D and E hold. Suppose that the unique solution πGM(ϑ0) of (4.2)
Z is π(ϑ0), that ∇ϑπ(ϑ0) has full column rank N(Θ) and that JGM(ϑ0) is non-singular.
for (Ymh)m
∈
Further, assume that E
n (ϑ)
πGM
and
n

N with 2N∗ > max(N(Θ), 4 + δ). If
ϑInd
is weakly consistent and
n

2N∗ for some N∗ ∈
LS
1|
|
(ϑ0) then the indirect estimator

n (ϑ) =

πn =

πLS

πS

b

b

where

with

√n(

ϑInd

n −

ϑ0) D
b
−→

N (0,ΞInd(ϑ0)),

b

b

b

ΞInd(ϑ0) = JInd(ϑ0)−

1IInd(ϑ0)JInd(ϑ0)−

1

JInd(ϑ0) = [∇ϑπ(ϑ0)]T Ω[∇ϑπ(ϑ0)]

IInd(ϑ0) = [∇ϑπ(ϑ0)]T Ω

ΞGM(ϑ0) +
(cid:20)

and
1
s

ΞLS(ϑ0)
(cid:21)

Ω[∇ϑπ(ϑ0)],

where the matrix ΞLS(ϑ) is deﬁned as in (4.8) with φ(y, u) = u and χ(x) = x

1.

−

15

We have already proven that (C.1) and (C.4) of Theorem 3.2 are satisﬁed. To show the remaining
πLS
n (ϑ) we require several auxiliary results. The remaining of this
conditions on the LS-estimator
section is devoted to that.

Sufﬁcient conditions for (C.2) and (C.5) are the weak uniform convergence of the LS-estimator
and its derivatives. Since the LS-estimator is deﬁned via the sample autocovariance function we ﬁrst
derive the uniform weak convergence of the sample autocovariance function and its derivatives.

b

Proposition 5.4. For j, l

0, . . . , r

∈ {

deﬁne

}

γϑ,n(l, j) =

1

n

r

−

n
r
−
∑
k=1

Y(k+l)h(ϑ)Y(k+ j)h(ϑ).

Then for i, u

b
1, . . . , N(Θ)
}

∈ {

the following statements hold.

(a) supϑ
Θ |
∈

γϑ,n(l, j)

−

γϑ(l

j)
|

P
→

0.

(b) supϑ
∈

Θ

(c) supϑ
∈

Θ

∂
b
∂ϑi
(cid:12)
(cid:12)
(cid:12)

∂2
b
∂ϑi∂ϑu

γϑ,n(l, j)

−

γϑ(l

j)

−

P
→

0.

γϑ,n(l, j)

∂
∂ϑi∂ϑu

−

(cid:12)
(cid:12)
γϑ(l
(cid:12)

j)

−

P
→

0.

−
∂
∂ϑi

Then the proof of (C.2) follows from Proposition 5.5.

b

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

Proposition 5.5.

(a) supϑ
Θ |
∈

πLS

n (ϑ)

π(ϑ)
|

−

P
→

0.

(b) supϑ
Θ |
∈

∇ϑ
b

πLS
n (ϑ)

∇ϑπ(ϑ)
|

−

(c) supϑ
Θ |
∈

∇2
ϑ

πLS
n (ϑ)
b

∇2

ϑπ(ϑ)
|

−

P
→
P
→

0.

0.

A direct consequence from this is the next corollary.

b

Corollary 5.6. Let ϑn be a sequence in Θ with ϑn

P
→

ϑ0. Then the following statements hold:

(a)

πLS

n (ϑn)

P
→

π(ϑ0).

(b) ∇ϑ

b

πLS
n (ϑn)

(c) ∇2
ϑ

πLS
n (ϑn)
b

P
→
P
→

∇ϑπ(ϑ0).

∇2

ϑπ(ϑ0).

This corollary already gives (C.5).

b

Finally, (C.3) is a consequence of Proposition 5.7 which gives the asymptotic normality of the the
LS-estimator. In principle this follows from Theorem 4.12 by interpreting the least squares estimator
as a particular GM-estimator with φ(y, u) = u and χ(x) = x

1.

−

Proposition 5.7. For any ϑ

∈

Θ the LS-estimator

πLS
n (ϑ) is strongly consistent and as n

∞,

→

√n(

πLS
n (ϑ)

π(ϑ)) D
b
−→

−

N (0,ΞLS(ϑ)) .

b

16

5.2 Robustness properties

Roughly speaking an estimator is robust when small deviations from the nominal model have not
much effect on the estimator. This property is known as qualitative robustness or resistance of the
estimator and was originally introduced in Hampel (1971) for i.i.d. sequences. The same article also
gives a slight extension to the case of data that are generated by permutation–invariant distributions,
introducing the term π–robustness (Hampel (1971, p.1893)). Of course, time series do not satisfy
the assumption of permutation invariance in general. Therefore, there have been various attempts
to generalize the concept of qualitative robustness to the time series setting. Boente et al. (1987,
Theorem 3.1) prove that their πdn–robustness for time series is equivalent to Hampel’s π–robustness
for i.i.d. random variables and therefore, extends Hampel’s π–robustness. They go ahead and deﬁne
the term resistance as well. The concept of resistance has the intuitive appeal of making a statement
about changes in the values of the estimator when comparing two deterministic samples. In contrast,
πdn–robustness is only a statement concerning the distribution of the estimator, which is in general not
easily tractable. The indirect estimator is weakly resistant and πdn–robust. The explicit deﬁnitions and
the derivation of these properties for our indirect estimator are given in Section 9.1 of the Supporting
Information.

Intuitively speaking, the inﬂuence functional measures the change in the asymptotic bias of an esti-
mator caused by an inﬁnitesimal amount of contamination in the data. This measure of robustness was
originally introduced as inﬂuence curve by Hampel (1974) for i.i.d. processes. It was later generalized
to the time series context by Künsch (1984) who explicitly studies the estimation of autoregressive
processes. However, in the paper of Künsch only estimators which depend on a ﬁnite–dimensional
marginal distribution of the data–generating process and a very speciﬁc form of contaminations are
considered. To remedy this, a further generalization was then made by Martin and Yohai (1986) who
consider the inﬂuence functional and explicitly allow for the estimators to depend on the measure of
the process which makes more sense in the time series setup (cf. Martin and Yohai (1986, Section
4)). In the sense of Martin and Yohai (1986, Section 4) the indirect estimator has a bounded inﬂuence
functional; see Section 9.2 in the Supporting Information.

The breakdown point is (for a sample of data with ﬁxed length n) the maximum percentage of out-
liers which can be contained in the data without ”ruining” the estimator. In this sense, it measures
how much the observed data can deviate from the nominal model before catastrophic effects in the
estimation procedure happen. However, the formal deﬁnition depends on the model and the estima-
tor. Maronna and Yohai (1991) and Maronna et al. (1979) deal explicitly with the breakdown point of
GM-estimators in regression models and Martin and Yohai (1985) and Martin (1980) study it in the
time series context. A very general deﬁnition of the breakdown point is given in Genton and Lucas
(2003, Deﬁnition 1 and Deﬁnition 2). Heuristically speaking, the fundamental idea of that deﬁnition
is that the breakdown point is the smallest amount of outlier contamination with the property that
the performance of the estimator does not get worse anymore if the contamination is increased fur-
ther. As already mentioned in Martin (1980, p. 239) (the proof is given in the unpublished paper
of Martin and Jong (1977)), and later in de Luna and Genton (2001, p. 377) and Genton and Lucas
(2003, p. 89), the breakdown point of the GM-estimator applied to estimate the parameters of an
AR(r) process is 1/(r + 1). Hence, the breakdown point of our indirect estimator is as well 1/(r + 1)
πS
n (ϑ) is applied to a simulated
since the other building block of the indirect estimator, the estimator
outlier–free sample.

b

17

6 Simulation study

We simulate CARMA processes on the interval [0, 1000] and choose a sampling distance of h = 1,
resulting in n = 1000 observations of the discrete–time process. The simulated processes are driven
either by a standard Brownian motion or by a univariate NIG (normal inverse Gaussian) Lévy process.
The increments of a NIG-Lévy process L(t)

1) have the density

L(t

−

−

fNIG(x;µ,α,β,δ) =

αδ
π

exp(δ

α2

β2 + βx)

−

K1(α√δ2 + x2)
√δ2 + x2

,

R,

x

∈

p

≥

−

β2)

R is a location parameter, α

R is a symmetry parameter and K1
0 is a shape parameter, β
µ
∈
is the modiﬁed Bessel function of the third kind with index 1. The variance of the process is then
3
σ2
L = δα2/(α2
2 . For the NIG Lévy process we use the parameters α = 3, β = 1, δ = 2.5145
and µ =
0.8890. These parameters result in a zero–mean Lévy process with variance approximately
1 which allows for comparison of the results to the standard Brownian motion case. For the outlier
model we choose additive outliers where the process (Vm)m
Z is a sequence of i.i.d. Bernoulli random
∈
Z where ξ and γ take different
variables with P(V1 = 1) = γ. The process (Zm)m
Z is Zm = ξ for m
∈
values in different simulations.

−

∈

∈

πGM
n

b

πn as GM-estimator

The indirect estimator is deﬁned as in Section 5. We take

(ϑ0) using the
R software which provides the pre–built function arGM in the package robKalman for applying GM-
estimators to AR processes. This function uses a Mallows estimator as in Example 4.4(a). The weight
b
function w(y) is the Tukey bisquare function from Example 4.4(b) applied to
y
, for the function
k
k
ψ(u) the user can choose between the Huber ψk–function and the bisquare function. The function
is implemented as an iterative least squares procedure as described by Martin (1980, p. 231ff.). We
do 6 iterations using the Huber function and then 50 iterations with the bisquare function, which is
the maximum number of iterations where the algorithm stops earlier if convergence is achieved. In
our experiments we use k = 4 for the tuning constant of the ψk–function. In general, we set s = 75
to obtain the simulation–based observations Y sn
snh(ϑ)) in the simulation part of
the indirect procedure. The type of Lévy process used for the simulation part is of the same type as
πS
the Lévy process driving the CARMA process. For the estimator
n (ϑ) we apply the least squares
estimator and as weighting matrix Ω we take the identity matrix for convenience reasons. In some
experiments we ﬁrst estimated the asymptotic covariance matrix of the GM-estimator by the empirical
b
πn. Setting Ω as the inverse of
covariance matrix of a suitable number of independent realizations of
that estimate did not signiﬁcantly affect the procedure positively or negatively so that the use of the
convenient identity matrix seems justiﬁed. In each experiment, we calculate the indirect estimator
and, for comparison purposes, the QMLE as deﬁned in Schlemm and Stelzer (2012). For the indirect
estimator as well for the QMLE we use 50 independent samples and report on the average estimated
value, the bias and the empirical variance of the parameter estimates.

h (ϑ), . . . ,Y S

S (ϑ) = (Y S

b

ξ = 0, γ = 0 (uncontaminated)
Bias
Mean
-0.1187
2.1187
-0.1238
-2.1238
-0.1214
-2.1214

Var
0.1008
0.0956
0.0937

r = 1
r = 2
r = 3

ξ = 10, γ = 0.1
Bias
-0.0027
-0.1121
-0.4828

Mean
-2.0027
-2.1121
-2.4828

Var
0.1004
0.1681
0.4811

ξ = 5, γ = 0.15
Bias
-0.0711
0.0445
-0.4838

Mean
-2.0711
-1.9555
-2.4838

Var
0.0905
0.1494
0.3367

Table 6.1: Indirect estimation of a CARMA(1, 0) process with parameter ϑ0 =

nian motion with n = 1000.

2 driven by a Brow-

−

18

∈

(
−

First, CARMA(1,0) processes with parameter ϑ0

∞, 0) are studied where Aϑ0 = ϑ0 and cϑ0 = 1.
These processes are of particular interest because their discretely sampled version admit an AR(1) rep-
resentation. For this reason, one would expect the indirect procedure to work very well as the auxiliary
representation is actually exact. Initially, in Table 6.1, we estimate contaminated and uncontaminated
2 driven by a Brownian motion using in the indirect estimation
CARMA(1,0) processes with ϑ0 =
method an auxiliary AR(r) process with r = 1, 2, 3. For uncontaminated CARMA(1,0) processes the
parameter r = 1 gives the lowest absolute bias where the variance is the highest. However, the bias and
the variances are very similar. By contrast with contaminated CARMA(1,0) processes, if we increase
r the variances increase. That is not surprising because r = 1 reﬂects the true model and including
more parameters than necessary results in more estimation errors. As well the bias is quite low for
r = 1.

−

Next, in Table 6.2, we compare the indirect estimator with r = 1 and the QMLE for a Brown-
0.2 the
ian motion driven CARMA(1, 0) process with either ϑ0 =
CARMA(1, 0) process is not so far away from a non-stationary process. In both cases we see that the
QMLE and the indirect estimator work quite well for uncontaminated CARMA(1,0) processes (top
2 the absolute bias of
of Table 6.2). The QMLE has a lower variance in both cases where for ϑ0 =
the indirect estimator and for ϑ0 =
0.2 the bias of the QMLE is lower. But still for both estimation
procedures the values are comparable. If we allow additionally outliers in the CARMA(1,0) model,

0.2 . For ϑ0 =

2 or ϑ0 =

−

−

−

−

−

ξ = 0, γ = 0 (uncontaminated)

Mean
-2.1424
-0.2031

Mean
-2.4017
-2.4513

Mean
-4.7942
-4.9139

Mean
-2.1207
-2.9511

QMLE
Bias
-0.1424
-0.0031

QMLE
Bias
-0.4017
-2.2513

QMLE
Bias
-2.7942
-4.7139

QMLE
Bias
-0.1207
-2.7511

Mean
-2.1187
-0.2100

Var
0.0913
0.0003
ξ = 5, γ = 0.1

Mean
-2.0027
-0.1981

Var
0.1487
0.0093
ξ = 10, γ = 0.1

Mean
-1.8070
-0.1981

Var
0.0315
0.0440
ξ = 5, γ = 0.15

Var
0.2592
0.0102

Mean
-2.0711
-0.1772

Indirect
Bias
-0.1187
-0.0100

Indirect
Bias
-0.0027
0.0019

Indirect
Bias
0.1930
0.0019

Indirect
Bias
-0.0711
0.0228

Var
0.1008
0.0009

Var
0.1004
0.0010

Var
0.0655
0.0010

Var
0.0905
0.0008

ϑ0 =
ϑ0 =

2
0.2

−
−

ϑ0 =
ϑ0 =

2
0.2

−
−

ϑ0 =
ϑ0 =

2
0.2

−
−

ϑ0 =
ϑ0 =

2
0.2

−
−

Table 6.2: Estimation results for CARMA(1, 0) processes with parameter ϑ0 driven by a Brownian

motion with n = 1000 and r = 1.

already in the case ξ = 5 and γ = 0.1, the indirect estimator performs vastly better than the QMLE
giving a much less biased estimate and lower variance. For ξ = 10 and γ = 0.1 the QMLE is far
away from the true values where the indirect estimator still gives good results. Increasing γ to 0.15
but keeping ξ = 5 shows that both estimators perform worse than in the situation with γ = 0.1, which
is to be expected. But once again, the indirect estimator gives excellent results. However, the QMLE
runs much faster than the indirect estimator.

19

In a further study we investigate CARMA(3,1) processes. This especially means that the sampled
such

process is not a weak AR process anymore. The true parameter is ϑ0 =
that

ϑ1 ϑ2 ϑ3 ϑ4 ϑ5

(cid:0)

(cid:1)

Aϑ0 =

0
1
0
1
0
0
ϑ1 ϑ2 ϑ3



R3
×

3



∈

and

cϑ0 = (ϑ4, ϑ5, 0).





For the CARMA(3,1) model we choose r = 5, which is also the minimum order of the auxiliary AR
representation to satisfy Assumption B. We also tried different values of r but they didn’t give better
results (see Table 11.1 in the Supporting Information). In contrast, for contaminated CARMA(3,1)
processes it seems that the absolute bias and variance are the lowest for r = 5.

Mean
-1.02574
-1.99600
-1.98396
-0.00688
0.99773

Mean
-1.01492
-1.99192
-1.99376
0.01404
1.01311

Mean
-1.00647
-1.99953
-1.99398
0.00843
1.00849

QMLE
Bias
-0.02574
0.00400
0.01604
-0.00688
-0.00227

QMLE
Bias
-0.01492
0.00808
0.00624
0.01404
0.01311

QMLE
Bias
-0.00647
0.00047
0.00602
0.00843
0.00849

n = 200

Var
0.01814
0.01105
0.02499
0.01309
0.01599

Mean
-1.32232
-2.28763
-2.11439
0.00287
0.88711

n = 1000

Var
0.00129
0.00310
0.00411
0.00109
0.00044

Mean
-1.02333
-1.98112
-2.00188
0.01253
1.00248

n = 5000

Var
0.00016
0.00005
0.00025
0.00004
0.00003

Mean
-1.00534
-1.99576
-1.99930
0.00089
0.99864

Indirect
Bias
-0.32232
-0.28763
-0.11439
0.00287
-0.11289

Indirect
Bias
-0.02333
0.01888
-0.00188
0.01253
0.00248

Indirect
Bias
-0.00534
0.00424
0.00070
0.00089
-0.00136

Var
0.94063
1.29387
0.42097
0.02619
0.08930

Var
0.00515
0.00905
0.01286
0.00436
0.00356

Var
0.00046
0.00034
0.00103
0.00035
0.00027

1
2
2

ϑ1 =
−
ϑ2 =
−
ϑ3 =
−
ϑ4 = 0
ϑ5 = 1

1
2
2

ϑ1 =
−
ϑ2 =
−
ϑ3 =
−
ϑ4 = 0
ϑ5 = 1

1
2
2

ϑ1 =
−
ϑ2 =
−
ϑ3 =
−
ϑ4 = 0
ϑ5 = 1

Table 6.3: Estimation results for an uncontamined CARMA(3, 1) process with parameter ϑ0 =

(ϑ1,ϑ2,ϑ3,ϑ4,ϑ5) driven by a Brownian motion with r = 5.

In the ﬁrst instance, we compare the QMLE and the indirect estimator for uncontaminated
CARMA(3, 1) processes in Table 6.3 and Table 6.4. In Table 6.3 the driving Lévy process is a Brow-
nian motion where in Table 6.4 it is a NIG-Lévy process. The results are very similar. The QMLE
has in general a lower variance than the indirect estimator. For n = 200 and n = 1000 it seems as well
that the QMLE has a lower absolute bias. But for n = 5000 this changes and the indirect estimator has
a lower absolute bias. However, both estimator perform excellent. For the Brownian motion driven
model the QML optimization failed for n = 200, 1000 and 5000 in 5, 4, and 5 cases, respectively,
where for the NIG driven model it failed in 6, 5, and 2 cases, respectively. The indirect estimator
never failed. The error occurs when the estimated value of ϑ0 is not an element of Θ anymore. The

20

Mean
-1.01143
-2.03219
-1.95505
0.01161
1.01040

Mean
-1.01502
-2.00152
-1.98346
0.00235
1.00318

Mean
-1.00640
-1.99918
-1.99441
0.00831
1.00850

QMLE
Bias
-0.01143
-0.03219
0.04495
0.01161
0.01040

QMLE
Bias
-0.01502
-0.00152
0.01654
0.00235
0.00318

QMLE
Bias
-0.00640
0.00082
0.00559
0.00831
0.00850

n = 200

Var
0.00735
0.01168
0.01746
0.00766
0.00462

Mean
-1.21354
-2.10578
-2.02967
0.02754
0.89459

n = 1000

Var
0.00123
0.00125
0.00232
0.00052
0.00045

Mean
-1.03184
-1.98345
-1.99289
-0.01551
0.99433

n = 5000

Var
0.00005
0.00006
0.00010
0.00004
0.00004

Mean
-1.00584
-1.99993
-1.99437
-0.00309
0.99607

Indirect
Bias
-0.21354
-0.10578
-0.02967
0.02754
-0.10541

Indirect
Bias
-0.03184
0.01655
0.00711
-0.01551
-0.00567

Indirect
Bias
-0.00584
0.00007
0.00563
-0.00309
-0.00393

Var
0.48999
0.67909
0.20588
0.03439
0.17048

Var
0.00510
0.00682
0.00944
0.00544
0.00201

Var
0.00049
0.00020
0.00074
0.00021
0.00031

1
2
2

ϑ1 =
−
ϑ2 =
−
ϑ3 =
−
ϑ4 = 0
ϑ5 = 1

1
2
2

ϑ1 =
−
ϑ2 =
−
ϑ3 =
−
ϑ4 = 0
ϑ5 = 1

1
2
2

ϑ1 =
−
ϑ2 =
−
ϑ3 =
−
ϑ4 = 0
ϑ5 = 1

Table 6.4: Estimation results for an uncontaminated CARMA(3, 1) process with parameter ϑ0 =

(ϑ1,ϑ2,ϑ3,ϑ4,ϑ5) driven by a NIG Lévy process with r = 5.

results in the table are averaged over experiments in which the algorithm did deliver a result, the
failed attempts were discarded. We obtained similar results as in Table 6.3 and Table 6.4 for different
parameter values.

≤

Further, for a Brownian motion driven CARMA(3,1) process we estimate ϑ0 for each of the follow-
ing contamination conﬁgurations in Table 6.5 (see as well Table 11.2 in the Supporting Information
for different values of n): ξ = 5 and γ = 0.1, ξ = 10 and γ = 0.1, ξ = 5 and γ = 1/6, and ξ = 5
and γ = 0.25. In this situation, the breakdown point has an upper bound of 1/6 since we have r = 5.
Hence, γ = 0.25 lies above the breakdown point and we expect to encounter problems in the estimation
procedure, while for γ

1/6 these problems should not occur. This is indeed the case.

For the ﬁrst two experiments, where γ = 0.1, we immediately recognize the maximum likelihood
estimate is severely biased and far from the true parameter value. Especially the inclusion of a zero
component in the true parameter seems to pose a major problem since this component is affected by
the most bias. On the other hand, the indirect estimator is still very close to the true parameter value
in all components including the zero component. Increasing ξ to 10 while keeping γ = 0.1 results in
a very similar performance of the indirect estimator. The increase of γ from 0.1 to 1/6 also affects the
performance of the indirect estimator. For all components of ϑ0 the absolute bias and the variance of
the indirect estimator increase. However, the loss in quality of the indirect estimator is manageable
and the calculated estimates still resemble the true parameter. This means that even at the breakdown
point of 1/6, the performance of the indirect estimator is satisfying, although of course not as good as
for lower contamination probabilities.

21

Mean
-0.4288
-2.9691
-2.4261
1.9267
1.8987

Mean
-0.0812
-3.6798
-3.8853
3.9956
2.3854

Mean
-0.1784
-4.8587
-12.0764
2.8032
2.1475

Mean
-0.1427
-6.1993
-13.7370
3.1533
1.5824

QMLE
Bias
0.5712
-0.9691
-0.4261
1.9267
0.8987

QMLE
Bias
0.9188
-1.6798
-1.8853
3.9956
1.3854

QMLE
Bias
0.8216
-2.8587
-10.0764
2.8032
1.1475

QMLE
Bias
0.8573
-4.1993
-11.7370
3.1533
0.5824

1
2
2

ϑ1 =
−
ϑ2 =
−
ϑ3 =
−
ϑ4 = 0
ϑ5 = 1

1
2
2

ϑ1 =
−
ϑ2 =
−
ϑ3 =
−
ϑ4 = 0
ϑ5 = 1

1
2
2

ϑ1 =
−
ϑ2 =
−
ϑ3 =
−
ϑ4 = 0
ϑ5 = 1

1
2
2

ϑ1 =
−
ϑ2 =
−
ϑ3 =
−
ϑ4 = 0
ϑ5 = 1

ξ = 5,γ = 0.1

Var
0.0055
0.0088
0.6193
0.0173
0.0028
ξ = 10,γ = 0.1

Mean
-1.0031
-2.0444
-1.9969
-0.0157
0.9147

Var
0.0012
0.4044
21.1039
0.1610
0.1283

Mean
-1.0031
-2.0446
-1.9966
-0.0157
0.9144

ξ = 5,γ = 1
6

Var
0.0136
15.9100
268.3894
0.2815
0.2928
ξ = 5,γ = 0.25

Mean
-0.9476
-2.1688
-1.9481
-0.0491
0.6996

Var
0.0525
52.7257
767.4184
0.2231
1.8174

Mean
-0.6035
-3.8476
-6.0640
2.1462
0.7653

Indirect
Bias
-0.0031
-0.0444
0.0031
-0.0157
-0.0853

Indirect
Bias
-0.0031
-0.0446
0.0034
-0.0157
-0.0856

Indirect
Bias
0.0524
-0.1688
0.0519
-0.0491
-0.3004

Indirect
Bias
0.3965
-1.8476
-4.0640
2.1462
-0.2347

Var
0.0131
0.0606
0.0325
0.0070
0.0243

Var
0.0131
0.0608
0.0325
0.0070
0.0243

Var
0.0426
0.1375
0.0469
0.0101
0.0705

Var
7.5738
25.0781
417.3265
11.3420
26.1110

Table 6.5: Estimation results for a CARMA(3, 1) process with parameter ϑ0 = (ϑ1,ϑ2,ϑ3,ϑ4,ϑ5)

driven by a Brownian motion with n = 1000 and r = 5.

The situation is vastly different in the experiment with γ = 0.25 where γ is above the breakdown
point. Here, we see not surprisingly that the indirect estimator, too, gives estimates which are severely
biased and quite far away from the true parameters. We also observe that the numerical procedure
used to obtain the parameter estimates quite often fails to deliver a result. The ratio of successful
to unsuccessful experiments is roughly equal to 1:2, i.e., the algorithm failed about twice as often
as it succeeded. In this sense, we can say that the estimator has broken down: for a given outlier–
contaminated sample, it either does not return an admissible estimate at all, or, if it does, the estimate
is far away from the true parameter. The latter statement is also evident from the fact that the variances
of the indirect estimates are far smaller in this case than in other experiments which intuitively means
that the algorithm typically returns very similar bad estimates if it returns a result at all.

22

7 Conclusion

πn =

πGM
n

(ϑ0) and

In this paper we presented an indirect estimation procedure for the parameters of a discretely observed
CARMA process by estimating the parameters of its auxiliary AR(r) representation using a GM-
estimator. Since there does not exist an explicit form of the map between the AR parameters and
the CARMA parameters, an additional simulation step to get back from the AR parameters to the
CARMA parameters was necessary. Sufﬁcient conditions were given such that the indirect estimator
is consistent and asymptotically normally distributed, on the one hand, in a general context, but on
πLS
n (ϑ). Moreover,
the other hand, as well for the special case where
the indirect estimator satisﬁes different robustness properties as weakly resistant, πdn-robustness and
it has a bounded inﬂuence functional.

b
Summarizing the simulation studies, the indirect estimator performs convincingly for various orders
p and q of the CARMA process, for different driving Lévy processes and for a variety of outlier
conﬁgurations. The QMLE failed in some simulations but the indirect estimator could be used always.
In contrast to the QMLE, the indirect estimator is robust against outliers where the QMLE is severely
biased. For uncontaminated CARMA processes the indirect estimator is less biased for large n where
for small n it is opposite. But in this situation both estimators work quite well. Obviously the bias in
the indirect estimation procedure can be decreased by using in the estimation and in the simulation
part the same type of estimator because then the bias from the estimation part and the simulation part
are cancelling out (cf. Gouriéroux et al. (2000, 2010)). But proving the asymptotic normality of the
indirect estimator using as well in the simulation part the GM estimator is involved and topic of some
future research.

πS
n (ϑ) =

b

b

b

8 Proofs

8.1 Proofs of Section 2

ϑ eAϑhm Σϑcϑ, m

Z is γϑ(mh) = cT
∈

N the covariance matrix of
Proof of Proposition 2.2. First, we need to show that for any r
(Yh(ϑ), . . . ,Y(r+1)h(ϑ)) is non–singular. To see this, note that the autocovariance function of
(Ymh(ϑ))m
Since Σϑ is non–singular
(cf. Schlemm and Stelzer (2012, Corollary 3.9)) and cϑ
= 0p we have that γϑ(0) > 0. Moreover,
the eigenvalues of Aϑ have strictly negative real parts by (A.4) and therefore, γϑ(mh)
∞
holds. By Brockwell and Davis (1991, Proposition 5.1.1), it follows that the covariance matrix of
N. Thus, a conclusion of Brockwell and Davis
(Yh(ϑ), . . . ,Y(r+1)h(ϑ)) is non–singular for every r
(1991, §8.1) is that there exist unique π1(ϑ), . . . ,πr(ϑ),σ2(ϑ) which solve the set of r + 1 Yule–
Walker equations, namely

N0 (see (2.2)).

0 as m

→

→

∈

∈

∈

π∗(ϑ) := 

π1(ϑ)
...
πr(ϑ)









= 




=: Γ(r

γϑ((r

−
σ2(ϑ) = γϑ(0)

γϑ(0)
γϑ(h)
...

γϑ(h)
γϑ(0)
...

2)h)

−
1)(ϑ)−

1)h) γϑ((r
1γ(r
π∗(ϑ)T γ(r

−
1)(ϑ),

−

−

1)(ϑ).

−

γϑ((r
γϑ((r

1)h)
2)h)

−
−
...
γϑ(0)

· · ·
· · ·

· · ·

1

−








γϑ(h)
...
γϑ(rh)











(8.1a)

(8.1b)

23

6
Proof of Lemma 2.5. We make use of the fact that the discretely observed stationary CARMA(p, q(ϑ))
process (Ymh(ϑ))m
1) process with weak white
noise as is given in (3.2). Then we can decompose the map π : Θ
Π into three separate maps for
which we deﬁne the following spaces:

Z admits a representation as a stationary ARMA(p, p
∈

→

−

M :=

{

(φ1, . . . ,φp,θ1, . . . ,θp
model as in (1.1) for which φ(z) and θ(z) have no common zeros

1,σ)

R2p : The coefﬁcients deﬁne a weak ARMA(p, p

R2p,

∈

−

1)

−

Rr+1 : The coefﬁcients deﬁne the autocovariances up to order

G :=

Π :=

∈

γ = (γ0, . . . ,γr)
{
r of a stationary stochastic process where Γ(r
(π1, . . . ,πr,σ)
AR(r) process and σ2 is the variance of the noise

Rr

×

∈

{

−

(0, ∞) : (π1, . . . ,πr) are the coefﬁcients of a stationary

1) is non-singular

Rr+1,

} ⊆

} ⊆

Rr+1,

} ⊆

−

−

1) is deﬁned as Γ(r

1)(ϑ) in (8.1a). Denote by π1 : Θ

where Γ(r
M the map which maps the
parameters of a CARMA process to the coefﬁcients of the weak ARMA(p, p
1) representation of its
−
G the map which maps the parameters of a weak
sampled version as in (3.2). Denote by π2 : M
→
ARMA(p, p
1) process to its autocovariances of lags 0, . . . , r. Lastly, denote by π3 : G
Π the map
which maps a vector of autocovariances (γ0, . . . ,γr) to the parameters of the auxiliary AR(r) model.
π1. We will show that πi is injective for i = 1, 2, 3 and receive from
Then we have that π = π3
this the injectivity of π. The three-times continuous-differentiability of the map π follows from the
representation (8.1) and the three-times continuous-differentiability of the autocovariance function γϑ.

π2

→

→

−

◦

◦

Step 1: π1 is injective.

∈

1.

−

≥

2p

Θ)
}

Z : ϑ
∈

(Ymh(ϑ)m
{

Step 2: π2 is injective if r

Θ the parameters of the weak ARMA process in (3.2) differ.

is identiﬁable from their spectral densities and hence, for any ϑ

Due to Assumption A and Schlemm and Stelzer (2012, Theorem 3.13) the family of sampled pro-
cesses
=
ϑ′ ∈
The reason is that by the method of Brockwell and Davis (1991, p. 93), the autocovariances of
1) processes are completely determined as solutions of difference equations with p
ARMA(p, p
1,
boundary conditions which depend on the coefﬁcient vector (φ1, . . . ,φp,θ1, . . . ,θp
≥
−
the number of equations r is greater than or equal to the number of variables 2p
1 which results
in the injectivity of π2 (see also de Luna and Genton (2001, Section 4.1)). To be more precise, let
θ = (φ1, . . . ,φp,θ1, . . . ,θp
φ1, . . . ,
1,σ)
∈
−
φp). Deﬁne Γ(p)(θ)
Case 1. (φ1, . . . ,φp)
φ1, . . . ,
= (
e
Due to Brockwell and Davis (1991, (3.3.9))

σ)
(p+1) similarly to Γ(p
−

φp,
R(p+1)
e
e

1)(ϑ) in (8.1a).

1,σ). If r

M and

θ1, . . . ,

θ = (

M .

2p

θp

−

−

−

1,

∈

∈

e

e

−

×

e

e
φp

φp

(
−
(
−

e
. . .

. . .

φ1

φ1

−

−

1)Γ(p)(θ) = (0
1)Γ(p)(

θ) = (0

0

0

0),

0).

· · ·

· · ·

But since the vectors (
−
only possible if Γ(p)(θ)

φp
. . .
e
= Γ(p)(
φ1, . . . ,
N0 = (γ
∈
Step 3: π3 is injective.

1991, (3.3.9)), (γθ(k))k

Case 2. (φ1, . . . ,φp) = (

e

−

φ1
e

φp
e
−

1) and (

. . .
θ) which implies π2(θ)
= π2(
φp). Assume that π2(θ) = π2(
e
θ(k))k
N0 and hence, θ =
∈
e

θ.

e

1) are linear independent this is

φ1
−
θ).
e
θ). But then due to (Brockwell and Davis,
e
e

We can also rewrite the linear equations (8.1) as a linear system with (r + 1) equations and the (r + 1)
unknown variables γ0, . . . ,γr which gives the injectivity of π3.

e

e

24

6
6
6
6
8.2 Proofs of Section 3

Proof of Theorem 3.2.
(a) We ﬁrst start by proving the consistency. With the deﬁnition of QInd we obtain

LInd(ϑ, Y n)

sup
Θ|
ϑ
∈
= sup
πn
[
Θ |
ϑ
∈
πT
n Ω

πn
b

≤ |

QInd(ϑ)
|

−
sn(ϑ)]T Ω[
πS

πn

−
π(ϑ0)T Ωπ(ϑ0)
b
|

−

−
πT
+ sup
n Ω
b
b
Θ|
ϑ
∈

πS
sn(ϑ)

−

πS
sn(ϑ)]

[π(ϑ)

−
−
sn(ϑ)T Ω
πS

b

+ sup
b
Θ|
ϑ
∈
π(ϑ0)T Ωπ(ϑ)
b
|

π(ϑ0)]
|

π(ϑ0)]T Ω[π(ϑ)

πn

−
π(ϑ)T Ωπ(ϑ0)
|
π(ϑ)T Ωπ(ϑ)
.
|

πS
sn(ϑ)

−
sn(ϑ)T Ω
πS
b

−

+ sup
Θ |
ϑ
∈

The four summands on the right–hand side converge in probability to zero as n
∞. For the ﬁrst one,
this is a consequence of (C.1). For the remaining three terms, the arguments are similar, so that we
treat only the second one exemplary. We have

→

b

b

b

b

sup
Θ|
ϑ
∈

≤

sn(ϑ)T Ω
πS

πn

−

π(ϑ)T Ωπ(ϑ0)
|
π(ϑ)T Ω

πn

−

sn(ϑ)T Ω
πS
b

sup
b
Θ|
ϑ
∈
Ω
k

≤ k

πn

|

+ sup
Θ|
ϑ
∈
Ω
k
k

πS
sn(ϑ)
b

π(ϑ)

πn
b
k

kk

−

+

sup
b
ϑ

Θ k
∈

sup
ϑ

Θ k
∈

π(ϑ)
b
kk

πn

π(ϑ0)
k

−

P
→

0.

π(ϑ)T Ω

πn

π(ϑ)T Ωπ(ϑ0)
|

−

b

π(ϑ)
k

b
b
Here, we used the fact that supϑ
is ﬁnite due to the continuity of the map π and the compact-
Θ k
∈
ness of Θ as well as both (C.1) and (C.2). Therefore, the function LInd(ϑ, Y n) converges uniformly
n minimizes LInd(ϑ, Y n)
ϑInd
in ϑ in probability to the limiting function QInd(ϑ). Per construction,
and QInd(ϑ) has a unique minimum at ϑ = ϑ0. Therefore, weak consistency of
follows by argu-
ing as in the proof of Schlemm and Stelzer (2012, Theorem 2.4); although in their proof convergence
in probability is replaced by almost sure convergence, this doesn’t matter because we can use the
subsequence criterion which says that a sequence converges in probability iff any subsequence has a
further subsequence which converges almost surely.

ϑInd
n

b

b

The proof of strong consistency goes similarly by replacing convergence in probability by almost

sure convergence.

(b) For the asymptotic normality, note that

√n(

πn

−

πS
sn(ϑ0)) = √n(

πn

−

π(ϑ0)) + √n(π(ϑ0)

πS
sn(ϑ0)).

−

Since both estimators are independent from each other, we obtain with (C.3) and (C.4) that
b

b

b

b

√n(

πn

sn(ϑ0)) D
πS
−→

−

N

0,ΞD(ϑ0) +

(cid:18)

1
s

ΞS(ϑ0)

.

(cid:19)

(8.2)

Moreover,

b
0 = ∇ϑLInd(ϑ, Y n)

b

= 2[∇ϑ

πS
sn(

ϑInd
n

ϑ=

ϑInd
n

)]T Ω[

πS
sn(

ϑInd
n

πn].

)

−

(cid:12)
We now use a Taylor expansion of order 1 around the true value ϑ0 to obtain
(cid:12)

b

b

b

b

b

b

0 = √n∇ϑLInd(

Ind, Y n)
ϑn
= √n∇ϑLInd(ϑ0, Y n) + √n∇2
b

ϑLInd(ϑn, Y n)(

ϑInd

ϑ0)

n −

b

25

= 2[∇ϑ

sn(ϑ0)]T Ω√n[
πS
+2[∇ϑ
b

sn(ϑn)]T Ω[∇ϑ
πS

−

πS
sn(ϑ0)
πS
sn(ϑn)]√n(
b
ϑ0

sn(ϑn)]T Ω[
πS
πn] + 2[∇2
ϑ
ϑInd
ϑ0).
n −
b
and hence, ϑn

b

−

ϑn
b
k
πS
sn(ϑn)

ϑ0

k ≤ k

πn] + [∇ϑ

−

b

ϑInd
b
n −
sn(ϑn)]T Ω[∇ϑ
πS

b

k

Here, ϑn is such that

[∇2
ϑ

n
πS
sn(ϑ

)]T Ω[

πS
sn(ϑn)

πn]√n(

ϑInd

n −

−

ϑ0)

b
ϑ0 as n

b

b

∞. Moreover,

→

P
→

πS
sn(ϑn)]

P
→

[∇ϑπ(ϑ0)]T Ω[∇ϑπ(ϑ0)]

(8.3)

due to (C.1), (C.2), (C.5) and the continuity of π(ϑ). Furthermore, the right-hand side is non-singular
since ∇ϑπ(ϑ0) has full column rank and Ω is non-singular. Finally, we write

b

b

b

b

b

√n(

ϑInd

ϑ0)

n −
n
πS
[∇2
sn(ϑ
b
ϑ
(cid:16)

=

)]T Ω[

πS
sn(ϑn)

−

πn] + [∇ϑ

sn(ϑn)]T Ω[∇ϑ
πS

πS
sn(ϑn)]

1

−

[∇ϑ

sn(ϑ0)]T Ω√n(
πS

πS
sn(ϑ0)

πn)

−

(cid:17)

b

b

b

b

and use (8.2), (8.3) and (C.5) to obtain as n
b
b

b

b

∞,

→

√n(

ϑInd

n −

ϑ0) D
−→

[∇ϑπ(ϑ0)]T Ω[∇ϑπ(ϑ0)]

1

−

[∇ϑπ(ϑ0)]T Ω

This completes the proof.

b

(cid:0)

(cid:1)

0,ΞD(ϑ0) +

N

·

(cid:18)

1
s

ΞS(ϑ0)

.

(cid:19)

8.3 Proofs of Section 4

0 χ(x) < 0, limx

Proof of Proposition 4.6. Using similar arguments as in Maronna and Yohai (1981, Lemma 2.1)
∞ χ(x) = ∞, the continuity and boundedness of χ and the Intermediate Value
(limx
Rr there exists a unique solution σ of the
Theorem) we can show that for each ﬁxed (π1, . . . ,πr)
equation

∈

→

→

Y(r+1)h(ϑ)

−

π1Yrh(ϑ)

σ

E

χ

"

 (cid:18)

. . .

−

−

πrYh(ϑ)

2

!#

(cid:19)

= 0.

By assumption (4.5), the function χ is chosen in such a way that for (π1(ϑ), . . . ,πr(ϑ)) this unique
solution is σ(ϑ). Therefore, we have that π(ϑ) is a solution of (4.2b). Next, we show that π(ϑ) is a
solution of (4.2a) as well. Since the function φ(y, u) is odd in u by Assumption (E.1), it holds that

Yh(ϑ)
...
Yrh(ϑ)

Ur+1(ϑ)
σ(ϑ) 



,






Yh(ϑ)
...
Yrh(ϑ)
















E 

φ












Yh(ϑ)
...
Yrh(ϑ)

= E 

φ



−

,



−



Yh(ϑ)
...
Yrh(ϑ)







,








Ur+1(ϑ)
σ(ϑ) 








=

−

E 

φ












Ur+1(ϑ)
σ(ϑ) 



Yh(ϑ)
...
Yrh(ϑ)

Yh(ϑ)
...
Yrh(ϑ)
















(8.4)

,











where the last equality follows from (4.4). From this equation we can conclude that

Yh(ϑ)
...
Yrh(ϑ)






E 

φ







Yh(ϑ)
...
Yrh(ϑ)






= 0,











Ur+1(ϑ)
σ(ϑ) 



,






26

and therefore, π(ϑ) is a solution of equation (4.2a).

Next, we show similarly to Maronna and Yohai (1981, Theorem 2.2(a)) for regression models that
π(ϑ) is the unique solution. Assume that another solution π′ = (π′1, . . . ,π′r,σ′) of (4.2) exists. But
= (π1, . . . ,πr). Note that the arguments in the derivation of (8.4) still hold if we
then (π′1, . . . ,π′r)
replace σ(ϑ) in the denominator of the second argument of φ by σ′. Thus, we obtain that

Yh(ϑ)
...
Yrh(ϑ)






Ur+1(ϑ)
σ′






,






Yh(ϑ)
...
Yrh(ϑ)






= 0,











E 

φ







and therefore

E 

Yh(ϑ)
...
Yrh(ϑ)

φ


















Yh(ϑ)
...
Yrh(ϑ)

φ

−









Y(r+1)h(ϑ)

,



−

π′1Yrh(ϑ)
σ′

. . .

−

−

π′rYh(ϑ)

Yh(ϑ)
...
Yrh(ϑ)
















,



Ur+1(ϑ)
σ′







Yh(ϑ)
...
Yrh(ϑ)



















= 0.





(8.5)

Since P((Yh(ϑ), . . . ,Yrh(ϑ)) = (0, . . . , 0)) = 0 and P(Y(r+1)h(ϑ)
= Ur+1(ϑ)) = 0 due to (D.3) for γ = 0 and u
Rr we have that
for every y

7→

−
φ(y, u) is strictly increasing on the interval (

−

−

π′1Yrh(ϑ)

. . .

π′rYh(ϑ)
u0, u0),

−

∈

Y(r+1)h(ϑ)

−

π′1Yrh(ϑ)
σ′

. . .

−

−

π′rYh(ϑ)

u0

≥

P-a.s.

(8.6)

because otherwise (8.5) cannot hold. Now, π′ is by assumption also a solution of (4.2b) and hence,
we have due to (8.6) and (E.6)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

0 = E

χ

"

 (cid:18)

Y(r+1)h(ϑ)

−

π′1Yrh(ϑ)
σ′

. . .

−

−

π′rYh(ϑ)

2

!# ≥

(cid:19)

χ(u2

0) > 0

which is a contradiction.

Proof of Lemma 4.9. By the Cramer–Wold device, the statement of the lemma is equivalent to the
1
k=1Ψk(ϑγ) converges to a univariate normal distribution with mean 0 and
assertion that
−
√n
−
Rr+1. According to Ibragimov (1962, Theorem 1.7), this holds if
variance xT IGM(ϑγ)x for every x
we can show that

r xT ∑n

∈

r

and that (xTΨk(ϑγ))k

N is strongly mixing with mixing coefﬁcients αxTΨ(ϑγ)(m) satisfying
∈

E

xTΨk(ϑγ)
2+δ < ∞
|
|

∞
∑
m=1

αδ/(2+δ)
xTΨ(ϑγ) (m) < ∞ for some δ > 0.

(8.7)

(8.8)

The same theorem then also states that xT IGM(ϑγ)x < ∞ from which we then deduce that for i, j

∈

27

6
1, . . . , r + 1
}
{
We start to show the existence of the (2 +δ)–th moment of xTΨk(ϑγ) in (8.7). Therefore, note that

the entry [IGM(ϑγ)]i j is ﬁnite and therefore, IGM(ϑγ) is well–deﬁned.

E

xTΨk(ϑγ)
2+δ
|
|

C

2+δ
x
k
k

≤

r+1
∑
i=1

E

2+δ < ∞,
Ψk,i(ϑγ)
k
k

(8.9)

where the last inequality holds since Ψk,i(ϑγ) is bounded by (E.2) and (E.6).

Finally, the process (Y γ

mh(ϑ))m

Z is strongly mixing and the mixing coefﬁcients satisfy the above
∈
condition (8.8) for the following reason. Either we have in the case of replacement outliers that
Y γ
mh(ϑ) = G(Vm, Zm,Ymh(ϑ)) for some measurable function G and the three processes (Vm), (Zm) and
(Ymh(ϑ)) are independent, or in the case of additive outliers we have Y γ
mh(ϑ) = G(Vm,Wm,Ymh(ϑ))
for some measurable function G and the three processes (Vm), (Wm) and (Ymh(ϑ)) are independent.
Hence, by Bradley (2007, Theorem 6.6(II)), Assumption (D.2) and Marquardt and Stelzer (2007,
Proposition 3.34) we receive

αY γ(ϑ)(m)

≤

αV (m) + αZ(m) + αY (ϑ)(m)

Cρm

≤

≤

αV (m) + αW (m) + αY (ϑ)(m)

respectively, αY γ(ϑ)(m)
(0, 1). Fur-
∈
thermore, Ψk(ϑγ) depends only on the ﬁnitely many values Y γ
(k+r)h(ϑ) and by Bradley
Cρm. Thus, the strong mix-
(2007, Remark 1.8(b)) this ensures that αΨ(ϑγ)(m)
ing coefﬁcients αxTΨ(ϑγ)(m) of xTΨ(ϑγ) satisfy the summability condition (8.8) and the lemma is
proven.

kh(ϑ), . . . ,Y γ

Cρm for some C > 0 and ρ

αY γ(ϑ)(m + r)

≤

≤

≤

Proof of Lemma 4.10. Note, ﬁrst that for i, j = 1, . . . , r,

sup
K
π

∈

∂
∂πi

φ




(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

= sup
K
π

∈





∂
∂u

Y γ
h (ϑ)
...
Y γ
rh(ϑ)




φ

(cid:19)











∂
∂u

φ



Y γ
h (ϑ)
...
Y γ
rh(ϑ)


Y γ
h (ϑ)
...
Y γ
rh(ϑ)

(cid:18)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
C (cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

Y γ
(r+1)h(ϑ)

−

π1Y γ

rh(ϑ)
σ

. . .

−

−

πrY γ

h (ϑ)



,



Y γ
jh(ϑ)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
h (ϑ)
(cid:12)




πrY γ

Y γ
(r+1)h(ϑ)

,

−

π1Y γ

rh(ϑ)
σ

. . .

−

−

Y γ
h (ϑ)
...
Y γ
rh(ϑ)

Y γ
h (ϑ)
...
Y γ
rh(ϑ)

sup
R
u
∈

(cid:18)

(cid:19)

≤







(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
due to Assumption (E.4) and the boundedness of 1/σ on the compact set K. By Assumption (D.1)
(cid:13)
and (A.3) the expectation on the right-hand side is ﬁnite. Similarly,

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)



































≤

C (cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

, u

Y γ
(r+1

i)h(ϑ)
−
σ

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Y γ
jh(ϑ)(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)



Y γ
h (ϑ)
...
Y γ
rh(ϑ)




Y γ
h (ϑ)
...
Y γ
rh(ϑ)
















(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

∂
∂σ

sup
K
π

∈

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
≤

Y γ
h (ϑ)
...
Y γ
rh(ϑ)






,






φ




∂
∂u

φ

(cid:19)

(cid:18)

C sup
R
u
∈

u
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)











Y γ
(r+1)h(ϑ)

−

π1Y γ

rh(ϑ)
σ

. . .

−

−

πrY γ

h (ϑ)

Y γ
h (ϑ)
...
Y γ
rh(ϑ)

Y γ
h (ϑ)
...
Y γ
rh(ϑ)






.






(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

, u









28

(cid:19)

!(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

∂
∂πi

χ

 (cid:18)

(cid:12)
(cid:12)
2
(cid:12)
(cid:12)
(cid:12)
(cid:19)

The expectation on the right-hand side is ﬁnite due to Assumption (E.5). Similar arguments, us-
Y γ
(r+1)h(ϑ)

πrY γ

h (ϑ)

...

2

ing Assumption (E.6), also show that

for i = 1, . . . , r and

π1Y γ
rh(ϑ)
σ

−

−

−

∂
∂σχ

Y γ
(r+1)h(ϑ)

π1Y γ
rh(ϑ)
σ

−

πrY γ

h (ϑ)

...

−

−

 (cid:18)

(cid:12)
(cid:12)
Therefore, by (Billingsley, 1999, Theorem 16.8(ii)) (that is an application of dominated convergence)
(cid:12)
(cid:12)
∇πQGM(π,ϑγ) exists on K and the order of differentiation and expectation can be changed.
(cid:12)

!(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
Moreover, due (E.4), (E.6) and (Billingsley, 1999, Theorem 16.8(i)) the map π

∇πQGM(π,ϑγ)

are uniformly dominated by integrable random variables.

is continuous. Hence, if πn

P
→

πGM(ϑγ)

∈

K then ∇πQGM(πn,ϑγ)

P
→

Proof of Lemma 4.11. We use the decomposition

7→
∇πQGM(πGM(ϑγ),ϑγ).

√n

−

pQGM(

πGM
n

(ϑγ),ϑγ) =

1
√n

−

n
r
−
∑
k=1

r

[QGM(

πGM
n

(ϑγ),ϑγ) +Ψk(ϑγ)]

1
√n

−

n
r
−
∑
k=1

r

−

Ψk(ϑγ).

b

The ﬁrst term is of order oP(1) due to (Bustos, 1982, Lemma 3.5) (cf. Kimmig (2016, Lemma A.5)
in our setting). The second term converges to N (0, IGM(ϑγ)) due to Lemma 4.9. Hence, we receive
the statement.

b

Proof of Theorem 4.12. Due to (4.2) we have QGM(π(ϑγ),ϑγ) = 0. Next, a ﬁrst-order Taylor expan-
sion around

(ϑγ) gives

πGM
n

0 = √n
b
= √n

−

−

rQGM(π(ϑγ),ϑγ)
πGM
rQGM(
n

(ϑγ),ϑγ) + √n

r∇πQGM(πGM

n

(ϑγ),ϑγ)(πGM(ϑγ)

πGM
n

(ϑγ)),

−

−

where
nation of Lemma 4.10 and Lemma 4.11.

πGM(ϑγ)
k

πGM
(ϑγ)
b
n

k ≤ k

−

πGM(ϑγ)

πGM
n

(ϑγ)
. The statement follows then from a combi-
k

b

−

8.4 Proofs of Section 5

b

For the ease of notation we write in the following for the Lévy process (LS
R shortly (Lt)t
t )t
∈
2N∗ for some 2N∗ > max(N(Θ), 4 + δ); similarly (Y S
hence, assume that E
R.
t )t
|
∈

L1
|

R and
∈

R is (Yt )t
∈
ϑ eAϑu ep1[0,∞)(u) and

Lemma 8.1. Deﬁne for any ϑ

∈

Θ the function fϑ(u) = cT

Gϑ(u) =

fϑ(u), ∇ϑ fϑ(u), ∇2

ϑ fϑ(u)

.

Then P-a.s. we have

(cid:0)

(cid:1)

Ymh(ϑ), ∇ϑYmh(ϑ), ∇2

ϑYmh(ϑ)

which is strongly mixing and ergodic.

(cid:0)

(cid:1)

mh

m

Z =
∈

∞

(cid:18)Z

−

Gϑ(mh

u) dLu

−

(cid:19)m

Z

∈

The proof is moved to Section 10 in the Supporting Information.

Proof of Proposition 5.4. (a) First, we prove the pointwise convergence of the sample autocovari-
γϑ,n(l, j) is locally Hölder-continuous which results in a stochastic
ance function and second, that

b

29

equicontinuity condition. Then we are able to apply Pollard (1990, Theorem 10.2) which gives the
uniform convergence.
Step 1. Pointwise convergence. From Lemma 8.1 we already know that (Ymh(ϑ))m
and ergodic sequence with E
as n

Z is a stationary
∈
2 < ∞. Then Birkoff’s Ergodic Theorem gives
|

2 < ∞ due to E
Ymh(ϑ)
|
|

L1
|

∞,

→

γϑ,n(l, j)

P
→

γϑ(l

j).

−

Step 2.

γϑ,n(l, j) is locally Hölder-continuous. Let γ

[0, 1

N(Θ)/(2N∗)) and

b

b

Uk := sup
0<
ϑ2k
ϑ1−
k
Θ
ϑ1,ϑ2 ∈

<1

∈
Ykh(ϑ1)
|
ϑ1
k

−
−

−
Ykh(ϑ2)
|
γ
ϑ2
k

.

Z is a stationary sequence, Uk
Since ((Ymh(ϑ))ϑ
Θ)m
∈
∈
Information, EU 2N∗
1 < ∞. In particular, for any ϑ1,ϑ2

and hence,

Ykh(ϑ1)
|

−

Ykh(ϑ2)

| ≤

d
= U1 and due to Lemma 10.3 in the Supporting

∈
Uk

Θ with

ϑ1
k

ϑ2

−

ϑ2

k

−

< 1 the upper bound

ϑ1
k
γ
k

Y(k+l)h(ϑ1)Y(k+ j)h(ϑ1)
|

Y(k+l)h(ϑ2)Y(k+ j)h(ϑ2)
|

−
Y(k+l)h(ϑ)
|

≤

sup
Θ |
ϑ
∈

(cid:18)

Y(k+ j)h(ϑ)
|
(cid:19)

+ sup
Θ |
ϑ
∈
=:U ∗k+l,k+ j

(Uk+l +Uk+ j)

ϑ1
k

−

ϑ2

γ
k

hold. Finally,

|

{z

}

γϑ1,n(l, j)
|

−

γϑ2,n(l, j)

| ≤

1

n

r

−

n
r
−
∑
k=1

with

b

b

U ∗k+l,k+ jk

ϑ1

−

ϑ2

γ
k

for

ϑ1
k

−

ϑ2

k

< 1

(8.10)

E(U ∗k+l,k+ j) = E(U ∗1+l,1+ j)

≤

4

E

(cid:18)

sup
Θ
ϑ

∈

(cid:18)

Yh(ϑ)2

EU 2
1

(cid:19)

(cid:19)

1/2

< ∞

where we used Lemma 10.3 in the Supporting Information to get the ﬁnite expectation.
Step 3. Let ε,η > 0. Take 0 < δ < min
give

1/γ. Then (8.10) and Markov’s inequality
1,ηε/E(U ∗1+l,1+ j)
}
{

γϑ1,n(l, j)
|

γϑ2,n(l, j)
|

−

> η



≤

E(U ∗1+l,1+ j)

δγ
η

< ε.

P



0<

sup
ϑ2k
ϑ1−
k
Θ
ϑ1 ,ϑ2∈

<δ



A conclusion of this stochastic equicontinuity condition, the pointwise convergence in Step 1 and
Pollard (1990, Theorem 10.2) is the uniform convergence.

b

b



The proof of (b,c) goes in the same vein as the proof of (a).

30

Proof of Proposition 5.5. Deﬁne

1)

γ(r
n

−

b
Then

(ϑ) = 

1)

−

γϑ,n(r, r
...
γϑ,n(r, 0)

b

and

Γ(r
n

−

1)

(ϑ) = 

b

b









γϑ,n(r

1)

−

1, r
−
...

γϑ,n(r

1, 0)

−

1)

−

γϑ,n(0, r
...
γϑ,n(0, 0)

b

· · ·

· · ·




b

b

b

πLS
n,1(ϑ)
...
b
πLS
n,r (ϑ)

π∗n (ϑ) := 

= [

Γ(r
n

−

1)

(ϑ)]−

1

γ(r
n

−

1)

(ϑ),



b
σ2
LS,n(ϑ) =



γϑ,n(r, r)
b


b

π∗n (ϑ)]T
[
−

1)

γ(r
n

−

b
(ϑ).

.






(8.11)

A conclusion of Proposition 5.4(a) and the deﬁnition of Γ(r

b

b

b

1)(ϑ) and γ(r

−

−

1)(ϑ) in (8.1) is that

Γ(r
n

−

1)

(ϑ)

sup
ϑ

Γ(r

−

1)(ϑ)
k

P
→

0

and

sup
ϑ

γ(r
n

1)

−

(ϑ)

γ(r

−

1)(ϑ)
k

P
→

0.

−

(8.12)

−

Θk
∈

Θk
∈
Due to the continuity and the positive deﬁniteness of Γ(r
compactness of Θ we receive supϑ
Θ k
∈
(8.11)-(8.12) and (8.1).

1)(ϑ)−

Γ(r

b

k

−

1

b

−

1)(ϑ) (cf. proof of Proposition 2.2), and the
< ∞. Hence, statement (a) is a consequence of

(b) Note that

∂
∂ϑi

∂
∂ϑi

π∗n (ϑ) =

π∗(ϑ) =
b

−

−

Γ(r
[
n

−

1)

(ϑ)]−

1

∂
∂ϑi

Γ(r
n

−

1)

(ϑ)

Γ(r
[
n

−

1)

(ϑ)]−

1

γ(r
n

−

1)

(ϑ) + [

Γ(r
n

1)

−

(ϑ)]−

1

[Γ(r
b

1)(ϑ)]−

1

−

h

h

∂
∂ϑi

Γ(r
b

−

i
1)(ϑ)

i

[Γ(r
b

1)(ϑ)]−

−

1γ(r
b

1)(ϑ) + [Γ(r
b

−

1)(ϑ)]−

1

−

1)

γ(r
n

−

(ϑ)

∂
∂ϑi

∂
∂ϑi

γ(r
b

−

i
1)(ϑ)

,

.

i
(8.13)

h

h

Applying Proposition 5.4(b) we receive that

∂
∂ϑi

Γ(r
n

−

1)

(ϑ)

∂
∂ϑi

−

Γ(r

−

0 and sup

∂
∂ϑi

1)

γ(r
n

−

(ϑ)

∂
∂ϑi

−

γ(r

−

P
→

1)(ϑ)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

ϑ

Θ(cid:13)
∈
(cid:13)
(cid:13)
(cid:13)

Then the same arguments as in (a) and (8.12)-(8.14) lead to statement (b).

b

b

(c) The proof goes in analog lines as in (a) and (b).

sup
ϑ

Θ(cid:13)
∈
(cid:13)
(cid:13)
(cid:13)

0.(8.14)

P
→

1)(ϑ)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

Proof of Corollary 5.6. (a) We use the upper bound

πLS
n (ϑn)
k

−

π(ϑ0)

k ≤

πLS
n (ϑ)

sup
ϑ

Θk
∈

π(ϑ)
k

−

+

π(ϑn)
k

π(ϑ0)
.
k

−

The ﬁrst term converges in probability to 0 due Proposition 5.5(a) and the second term because π(ϑ)
is continuous (see Lemma 2.5) and ϑn

ϑ0. The proof of (b,c) goes on the same way.

b

b

P
→

πLS
Proof of Proposition 5.7. Due to Proposition 5.5(a) we already know that the LS-estimator
n (ϑ) is
πLS
n (ϑ) follows in principle from Theorem 4.12 by interpret-
consistent. The asymptotic normality of
ing the least squares estimator as a particular GM-estimator with φ(y, u) = u and χ(x) = x
1. An
b
−
assumption of Theorem 4.12 is that the Jacobian JGM(ϑ) = ∇πQGM(π(ϑ),ϑ) is non–singular. For

b

31

the LS-estimator this can be veriﬁed by direct calculations because

∇πQLS(π,ϑ) = JLS(ϑ) =

1
σ(ϑ)

−

γϑ(0)
γϑ(h)
...

γϑ((r

1)h)

−
0

γϑ(h)
γϑ(0)
...
. . .
0

. . . γϑ((r
. . . γϑ((r
. . .
. . .
. . .

−
−
...
γϑ(0)
0

1)h) 0
2)h) 0
...
0
2










.










r block is non-singular. How-
Hence, JLS(ϑ) is non–singular if and only if the upper left r
ever, the upper left block is up to a positive factor the covariance matrix of the random vector
(Yh(ϑ), . . . ,Yrh(ϑ)) which is non–singular (cf. proof of Proposition 2.2).

×

Still, we need to be careful because the function φ and χ do not satisfy Assumptions (E.2), (E.4)
and (E.6) with respect to boundedness. However, a close inspection of the proof of Theorem 4.12
reveals that the boundedness is only used at two points. First, in Lemma 4.9 where we deduce the
ﬁniteness of the expectation in (8.9). However, for the LS-estimator

Ψk,i(ϑ) =

Y(k+r)h(ϑ)

π1(ϑ0)Y(k+r

1)h(ϑ)

. . .

−

−

−

−

πr(ϑ0)Ykh(ϑ)

Y(k+i
−

1)h(ϑ)

for i = 1, . . . , r and

Ψk,r+1(ϑ) =

(cid:2)

(cid:18)

Y(k+r)h(ϑ)

−

π1(ϑ)Y(k+r

1)h(ϑ)

−
σ(ϑ)

(cid:3)

. . .

−

−

πr(ϑ)Ykh(ϑ)

2

(cid:19)

1.

−

Therefore, inequality (8.9) follows since the Lévy process (Lt)
R has ﬁnite (4 +δ)–th moment which
∈
then transfers to (Yt (ϑ))t
R by (Marquardt and Stelzer, 2007, Proposition 3.30) and subsequently the
∈
(2 + δ/2)-moment of Ψk,i(ϑ).

Second, the boundedness assumptions are used in the proof of Lemma 4.10 to deduce the existence
of ∇πQLS(π,ϑ) and its continuity. But by the above calculations ∇πQLS(π,ϑ) exists obviously and
is continuous.

Proof of Theorem 5.3. (C.1) and (C.4) follow from Theorem 4.8 and Theorem 4.12. (C.2) is proven in
Proposition 5.5. (C.3) is a consequence of Proposition 5.7. Finally, (C.5) is derived in Corollary 5.6.

Acknowledgement

The authors take pleasure to thank Thiago do Rˆego Sousa for helping with the simulation study. They
thank two anonymous referees for useful comments improving the paper. Financial support by the
Deutsche Forschungsgemeinschaft through the research grant FA 809/2-2 is gratefully acknowledged.

Data Availability Statement

The data that support the ﬁndings of this study are available from the corresponding author upon
reasonable request.

References

M.G. Ben, E.J. Martinez, and V.J. Yohai. Robust estimation in vector autoregressive moving-average

models. J. Time Ser. Anal., 20(4):381–399, 1999.

32

F.E Benth, C. Klüppelberg, G. Müller, and L. Vos. Futures pricing in electricity markets based on

stable CARMA spot models. Energy Economics, 44:392–406, 2014a.

F.E. Benth, S. Koekebakker, and V. Zakamouline. The CARMA interest rate model. International

Journal of Theoretical and Applied Finance, 17(2), 2014b.

A.R. Bergstrom. Gaussian estimation of structural parameters in higher order continuous time dy-

namic models. Econometrica, 51:117–152, 1983.

A.R. Bergstrom. Continuous time stochastic models and issues of aggregation over time.

In Z.

Griliches and M.D. Intriligator, Handbook of Econometrics, 1145–1212. North-Holland, 1984.

D.S. Bernstein. Matrix Mathematics: Theory, Facts, and Formulas. Princeton Univ. Press, 2009.

P. Billingsley. Convergence of probability measures. John Wiley & Sons, New York, 2. edition, 1999.

G. Boente, R. Fraiman, and V. Yohai. Qualitative robustness for stochastic processes. Ann. Statist.,

15(3):1293–1312, 1987.

R.C. Bradley. Introduction to Strong Mixing Conditions. Volume 1. Kendrick Press, Heber City, 2007.

P.J. Brockwell. Lévy-driven CARMA processes. Ann. Inst. Statist. Math., 53:113–124, 2001.

P.J. Brockwell. Recent results in the theory and applications of CARMA processes. Ann. Inst. Statist.

Math., 66(4):647–685, 2014.

P.J. Brockwell and R. Davis. Time Series: Theory and Methods. Springer, New York, 2 edition, 1991.

P.J. Brockwell and A. Lindner. Existence and uniqueness of stationary Lévy-driven CARMA pro-

cesses. Stochastic Process. Appl., 119(8):2660–2681, 2009.

P.J. Brockwell, R. Davis, and Y. Yang. Estimation for non-negative Lévy-driven CARMA processes.

J. Bus. Econom. Statist., 29(2):250–259, 2011.

O.H. Bustos. General M-estimates for contaminated pth-order autoregressive processes: consistency

and asymptotic normality. Z. Wahrscheinlichkeit, 59(4):491–504, 1982.

O.H. Bustos and V.J. Yohai. Robust estimates for ARMA models. J. Amer. Statist. Assoc., 81(393):

155–168, 1986.

A.J. Chambers and J. R. McCrorie. Frequency domain estimation of temporally aggregated Gaussian

cointegrated systems. J. Econometrics, 136:1–29, 2007.

McCrorie JR. Chambers, M. and MA. Thornton. Continuous time modelling based on an exact dis-
crete time representation. In K. van Montfort, J. Oud, and M. Voelkle, editors, Continuous Time
Modeling in the Behavioral and Related Sciences, pages 317–357. Springer, 2018.

X. de Luna and M.G. Genton. Robust simulation-based estimation. Statist. Probab. Lett., 48(3):

253–259, 2000.

X. de Luna and M.G. Genton. Robust simulation-based estimation of ARMA models. J. Comput.

Graph. Stat., 10(2):370–387, 2001.

33

L. Denby and R.D. Martin. Robust estimation of the ﬁrst-order autoregressive parameter. J. Amer.

Statis. Assoc., 74(365):140–146, 1979.

T. do Rêgo Sousa, S. Haug, and C. Klüppelberg. Indirect Inference for Lévy-driven continuous-time

GARCH models. Scand. J. Statist., 2019.

V. Fasen-Hartmann and M. Scholz. Quasi-maximum likelihood estimation for cointegrated solutions

of continuous-time state space models observed at discrete time points. Preprint, 2018.

R.A. Gallant and G. Tauchen. Which moments to match? Econometric Theory, 12:657–668, 1996.

H. Garnier and L. Wang, editors. Identiﬁcation of Continuous-time Models from Sampled Data. Ad-

vances in Industrial Control. Springer, London, 2008.

M.G. Genton and A. Lucas. Comprehensive deﬁnitions of breakdown points for independent and

dependent observations. J.R. Stat. Soc. Ser. B, 65(1):81–94, 2003.

C. Gouriéroux and A. Monfort. Simulation-based Econometric Methods. Oxford University Press,

Oxford, 1997.

C. Gouriéroux, A. Monfort, and E. Renault. Indirect inference. J. Appl. Econometr., 8:85–118, 1993.

C. Gouriéroux, E. Renault, and N. Touzi. Calibration by simulation for small sample bias correction.
In R. Mariano, T. Schuermann, and M. J. Weeks, editors, Simulation-based Inference in Economet-
rics: Methods and Applications, pages 328–358. Cambridge University Press, Cambridge, 2000.

C. Gouriéroux, P.C.B Phillips, and J. Yug. Indirect inference for dynamic panel models. J. Economet-

rics, 157:68–77, 2010.

F.R. Hampel. A general qualitative deﬁnition of robustness. Ann. Math. Statist., 1887–1896, 1971.

F.R. Hampel. The inﬂuence curve and its role in robust estimation. J. Amer. Statist. Assoc., 69(346):

383–393, 1974.

E.J. Hannan and M. Deistler. The Statistical Theory of Linear Systems. Society for Industrial and

Applied Mathematics, 2012. Reprint of the 1988 original.

A.C. Harvey and J.H. Stock. The estimation of higher-order continuous time autoregressive models.

Econometric Theory, 1:97–112, 1985a.

A.C. Harvey and J.H. Stock. Continuous time autoregressive models with common stochastic trends.

J. Econom. Dynam. Control., 12:365–384, 1985b.

A.C. Harvey and J.H. Stock. Estimating integrated higher-order continuous time autoregressions with

an application to money-income causality. J. Econometrics, 42(3):319–336, 1989.

P.J. Huber and E. Ronchetti. Robust Statistics. Wiley, 2. edition, 2009.

I. Ibragimov. Some limit theorems for stationary processes. Theory Probab. Appl., 7(4):349–382,

1962.

S. Kimmig. Statistical Inference for MCARMA Processes. PhD thesis, Karlsruher Institut of Technol-

ogy, Karlsruhe, 2016.

34

H. Künsch. Inﬁnitesimal robustness for autoregressive processes. Ann. Statist., 12(3):843–863, 1984.

M. Kyriacou, P.C.B. Phillips, and F. Rossi. Indirect inference in spatial autoregression. Econometrics

Journal, 20:168–189, 2017.

E.K. Larsson, M. Mossberg, and T. Söderström. An overview of important practical aspects of
continuous-time ARMA system identiﬁcation. Circuits Systems Signal Process., 25:17–46, 2006.

R.A. Maronna and V.J. Yohai. Asymptotic behavior of general M-estimates for regression and scale

with random carriers. Z. Wahrscheinlichkeit, 58(1):7–20, 1981.

R.A. Maronna and V.J. Yohai. The breakdown point of simultaneous general M estimates of regression

and scale. J. Amer. Statist. Assoc., 86(415):699–703, 1991.

R.A. Maronna, O.H. Bustos, and V.J. Yohai. Bias-and efﬁciency-robustness of general M-estimators
for regression with random carriers. In Smoothing techniques for curve estimation, pages 91–116.
Springer, 1979.

R.A. Maronna, D.R. Martin, and V.J. Yohai. Robust Statistics. Wiley, 2006.

T. Marquardt and R. Stelzer. Multivariate CARMA processes. Stochastic Process. Appl., 117(1):

96–120, 2007.

D.R. Martin. Robust estimation of autoregressive models. In D.R. Brillinger and G.C. Tiao, editors,

Directions in time series, 228–262. Institute of Mathematical Statistics, Hayward, CA, 1980.

D.R. Martin and J. Jong. Asymptotic properties of robust generalized M-estimates for the ﬁrst order

autoregressive parameter. Unpublished Bell Labs Technical Memo, Murray Hill, NJ, 1977.

D.R. Martin and V.J. Yohai. Robustness in time series and estimating ARMA models. In E.J. Hannan,

P.R. Krishnaiah, and M.M. Rao, editors, Handbook of statistics, 119–155. Elsevier, 1985.

D.R. Martin and V.J. Yohai. Inﬂuence functionals for time series. Ann. Statist., 14(3):781–818, 1986.

C. Monfardini. Estimating stochastic volatility models through indirect inference. The Econometrics

Journal, pages C113–C128, 1998.

A. Monfort. A reappraisal of misspeciﬁed econometric models. Econometric Theory, 12(12):597–

619, 1996.

N. Muler, D. Peña, and V.J. Yohai. Robust estimation for ARMA models. Ann. Statist., 37(2):816–

840, 2009.

D.J. Olive. Robust Multivariate Analysis. Springer, Cham, 2017.

P.C.B. Phillips and J. Yu. Maximum likelihood and Gaussian estimation of continuous time models in
ﬁnance. In T. Mikosch, J.-P. Kreiß, R.A. Davis, and T.G. Andersen, editors, Handbook of Financial
Time Series, pages 497–530, Berlin, Heidelberg, 2009. Springer.

D. Pollard. Empirical processes: theory and applications, volume 2 Institute of Mathematical Statis-

tics, Hayward, CA, 1990.

E. Schlemm and R. Stelzer. Quasi maximum likelihood estimation for strongly mixing state space
models and multivariate Lévy-driven CARMA processes. Electron. J. Stat., 6:2185–2234, 2012.

35

A.A. Smith. Estimating nonlinear time-series models using simulated vector autoregressions. J. Appl.

Econometr., 8(S1):S63–S84, 1993.

M.A. Thornton and M.J. Chambers. Continuous time ARMA processes: discrete time representation

and likelihood evaluation. J. Econom. Dynam. Control, 79:48–65, 2017.

V. Todorov. Estimation of continuous-time stochastic volatility models with jumps using high-

frequency data. J. Econometrics, 148:131–148, 2009.

X. Wang, P.C.B. Phillips and J. Yu. Bias in estimating multivariate and univariate diffusions J.

Econometrics, 161:228-245, 2011.

J. Yu. Simulation-based estimation methods for ﬁnancial time series models. In J.-C. Duan, J.E. Gen-
tle, and W. Härdle, editors, Handbook of Computational Finance, 427–465, Berlin, 2011. Springer.

P. Zadrozny. Gaussian likelihood of continuous-time ARMAX models when data are stocks and ﬂows

at different frequencies. Econometric Theory, 4(1):108–124, 1988.

36

Supporting Information

9 Robustness properties of the indirect estimator

In this section we study the robustness properties of the indirect estimator for the CARMA parameters
of (Ymh)m
(ϑ0) is the GM-estimator satisfying Assumptions A, B,
Z where we assume that
∈
E, (D.3) for (Ymh)m
Z and that π(ϑ0) is the unique solution of (4.2) for (Ymh)m
Z (a sufﬁcient criterium
∈
∈
for this is given in Proposition 4.6). Moreover, we require similarly to (E.2):

πGM
n

πn =

b

b

(E2’) (y, u)

7→

φ(y, u) is bounded and there exists c > 0 such that

φ(y1, u)y1
k
−
φ(y, u1)y
k

−

φ(y2, u)y2
φ(y, u2)y

k ≤

k ≤

y1
c
k
u1
c
|

y2
u2

y2
y1
/ min(
),
,
k
k
k
k
k
u2
u1
/ min(
).
,
|
|
|
|
|

−

−

Under some mild assumptions on ψ(u) and w(y) both the Mallows estimator and the Hampel–
Krasker–Welsch estimator satisfy these conditions (cf. Boente et al. (1987, p.1305)). For the simula-
tion part we take some estimator

πS
n (ϑ), e.g., the LS-estimator, such that as n

∞,

→

b

πS
n (ϑ)

sup
ϑ

Θk
∈

π(ϑ)
k

−

P
→

0

holds.

b

9.1 Resistance and qualitative robustness

To this end, let y be a (inﬁnite-length) realization of the discretely sampled CARMA process (Ymh)m
Z.
∈
R∞, where R∞ denotes the inﬁnite cartesian product of
Formally, we can write that y = (ymh)m
R with itself. On this space, equipped with the Borel σ-ﬁeld B∞ we denote the set of all probability
R∞ as above by yn = (yh, y2h, . . . , ynh) the
measures by P(R∞). In the following, we denote for y
vector of the ﬁrst n coordinates. Finally, PY (h) denotes the probability measure of the discrete-sampled
CARMA process (Ymh)m

∈

∈

N

∈

Deﬁnition 9.1. Let y
of

∈

ϑn when it is calculated using the deterministic realization zn

Rn.

N be a sequence of estimators. Denote by
ϑn)n
∈

(a) (
b

N is called resistant at y if for every ε > 0 there exists a δ > 0 such that
ϑn)n
∈

ϑn(zn) the value

b

Z.
∈
R∞ and let (

b

b

sup

ϑn(zn)
k

ϑn(wn)
k

−

: zn, wn

∈

ε

≤

N,

n

∀

∈

(9.1)

where Bδ(x) denotes an open ball with center x and radius δ with respect to the metric

b

b

n

∈

Bδ(yn)
o

dn(zn, wn) = inf

ε :

(cid:26)

i
#
{

∈ {

1, . . . , n
:
}
n

zn
i −
|

wn

i | ≥

ε
}

≤

ε

.

(cid:27)

(b) (

N is called asymptotically resistant at y if for any ε > 0 there exists a δ > 0 and N0(ε, y)
ϑn)n
∈
N such that (9.1) holds for n

N0(ε, y).

∈

b

≥

1

(c) For Q

∈

P(R∞) we say that (

N is strongly resistant at Q if
ϑn)n
∈

Q

b
y
∈

(cid:16)n

R∞ : (

N is resistant at y
ϑn)n
∈

= 1.

o(cid:17)

(d) (

N is called asymptotically strongly resistant at Q if
ϑn)n
∈

b

b

Q

y

∈

(cid:16)n

R∞ : (

N is asymptotically resistant at y
ϑn)n
∈

= 1.

o(cid:17)

(e) (

N is called weakly resistant at Q if for any ε > 0 there exists a δ > 0 such that
ϑn)n
∈

b

b

Q

y

R∞ : sup

∈

ϑn(zn)
k
n

ϑn(wn)
k

−

: zn, wn

∈

Bδ(yn)
o

ε

≤

(f) (

(cid:16)n
N is called asymptotically weakly resistant at Q if for any ε > 0 there exist a δ > 0 and
ϑn)n
∈
N such that
N(ε)
∈
b

o(cid:17)

b

b

1

ε

≥

−

n

∀

∈

N.

R∞ : sup

Q

y

(cid:16)n

∈

ϑn(zn)
k
n

ϑn(wn)
k

−

: zn, wn

∈

Bδ(yn)
o

≤

ε

o(cid:17)

ε

1

−

≥

n

∀

≥

N(ε).

b

b

With this deﬁnition at hand, we want to study the question whether our indirect estimator for the pa-
rameters of a CARMA processes is resistant. We will make use of the fact that the indirect estimator
consists of two independent parts, the GM-estimator for the parameters of the auxiliary AR represen-
tation, which deals with possible outliers in the observations, and the outlier–free estimator of the AR
parameters based on simulated data.

Theorem 9.2. The GM-estimator (

πGM
n

N is strongly resistant at PY (h).
(ϑ0))n
∈

πGM
n

N is asymptotically strongly resistant at PY (h). This follows from
Proof. First of all, (
(ϑ0))n
∈
Boente et al. (1987, Theorem 5.1). The theorem requires that φ and χ fulﬁll Assumption E, (E2’) and
that the limiting equation has a unique solution, which we assumed. Moreover, (Ymh)m
Z is stationary
∈
and ergodic due to Marquardt and Stelzer (2007, Proposition 3.34) and fulﬁlls (D.3).

b

b

Next, by Cox (1981, Lemma 5)

πGM
n

combination with the asymptotically strongly resistance of (
resistance due to Boente et al. (1987, Proposition 4.2).

b

(ϑ0) is a continuous function of Y n for every n
πGM
n

N. This in
N at PY (h) implies the strong
(ϑ0))n
∈

∈

Theorem 9.3. The indirect estimator (
tant at PY (h).

ϑInd
n

b

b

N is weakly resistant and asymptotically weakly resis-
)n
∈

Proof. Let ε > 0. Since QInd has a unique minimum in ϑ0

η := 3−

1

inf
>ε/4
ϑ0
k

ϑ
k

−

QInd(ϑ) > 0.

Moreover, the map x
7→
Π′ = π(Θ). Thus, there exists an ε′ > 0 such that

xT Ωx is continuous and hence, uniformly continuous on the compact set

πT Ωπ
|

−

T Ωπ′

π′

η
8

.

| ≤

sup
Π′
π,π′∈
ε′
π′ k≤
π
−
k

2

Deﬁne

Ω0(ε,δ) :=

y
N {
\n
∈

∈

R∞ : sup

πn(zn)

{k

πn(wn)
k

−

: zn, wn

∈

Bδ(yn)

} ≤

ε/2
}

.

Due to the strong resistance of the GM-estimator
1987,
(Boente et al.,
ε,ε′,η/(8
ε := min
Ω
k
k
{

Proposition
supπ
π
)
Π′ k
}
k
∈

4.1)

there
b

b

b
πn =

(ϑ0) at PY (h) given in Theorem 9.2 and
for

an δ > 0

such

that

πGM
n
exists
b

e

Let y

∈

P(Ω0(

ε,δ))

1

−

≥

Bδ(yn). Then for any n

e

∈

ε
2

.

1

−

ε
2 ≥
e
N,

Ω0(

ε,δ) and zn

∈
LInd(ϑ, yn)

sup
e
Θ|
ϑ
∈
= sup
ϑ

Θ| −
∈
sup
Π′
π

2
π
k

∈

≤

LInd(ϑ, zn)
|
πn(zn)]T Ω

−
πn(yn)

2[

−
πn(yn)
b

Ω

b
kk

kk

πn(zn)
b
k

−

+

πn(yn)T Ω
|

b

πn(yn)
b

−

πS
n (ϑ) +

πn(yn)T Ω

πn(yn)

πn(zn)T Ω

πn(zn)
|

−
πn(zn)TΩ

b

πn(zn)
b

η
4

.

(9.2)

| ≤

b
On the other hand, from Theorem 3.2(a) we know that supϑ
Θ |
∈
N such that
there exists an N(ε)

b

b

b

b
LInd(ϑ, Y n)

b

QInd(ϑ)
|

P
→

−

0. Hence,

∈

LInd(ϑ, Y n)

P

sup
ϑ

Θ k
(cid:18)
∈
R∞ : supϑ
Θ |
∈

Deﬁne Ωn :=
Then

y
{

∈

QInd(ϑ)

| ≤

−

ε
2

−

η

> 1

(cid:19)

n

∀

≥

N(ε).

LInd(ϑ, yn)

QInd(ϑ)

| ≤

η
}

and let y

Ωn

∩

∈

−

Ω0(

ε,δ) and n

N(ε).

≥

LInd(ϑ0, yn)
|
|

=

LInd(ϑ0, yn)
|

−

QInd(ϑ0)

| ≤

η,

e

(9.3)

and with the deﬁnition of η we receive

inf
ϑ0
−

ϑ
|

|≥

ε

4 |

LInd(ϑ, yn)

| ≥

inf
ϑ0
−

ϑ
|

|≥

ε

4 |

QInd(ϑ)

| −

LInd(ϑ, yn)

sup
Θ|
ϑ
∈

QInd(ϑ)

| ≥

3η

−

−

η = 2η.

(9.4)

Since

n (yn) minimizes LInd(ϑ, yn) we can deduce from (9.3) and (9.4) that
ϑInd

ϑInd
n
k

(yn)

ϑ0

k

−

<

ε
4

.

(9.5)

Bδ(yn). Due to (9.2) and (9.3) we obtain

b

b

∈

Let zn

LInd(ϑ0, zn)
|

| ≤

LInd(ϑ, zn)

sup
Θ|
ϑ
∈

Likewise, (9.2) and (9.4) give us that

LInd(ϑ, yn)
|

+

−

LInd(ϑ0, yn)
|

| ≤

η
4

+ η =

5η
4

.

(9.6)

LInd(ϑ, zn)

inf
ϑ0
−

ϑ
|

|≥

ε

4 |

| ≥

inf
ϑ0
−

ϑ
|

|≥

ε

4 |

LInd(ϑ, yn)

LInd(ϑ, zn)

sup
Θ|
ϑ
∈

LInd(ϑ, yn)

| ≥

−

7η
4

.

(9.7)

| −

3

Since

n (zn) minimizes LInd(ϑ, zn) we can conclude from (9.6) and (9.7) that
ϑInd

b

ϑInd
n
k

(zn)

ϑ0

k

−

<

ε
4

.

Finally, (9.5) and (9.8) result in

b

(9.8)

ϑInd
n
k

(zn)

−

n (yn)
ϑInd

k ≤ k

ϑInd
n

(zn)

ϑ0

+

n (yn)
ϑInd
k

−

k

−

ϑ0

k

<

ε
2

.

b
To summarize, for n
zn, wn

≥
Bδ(yn) we have

∈

N(ε) we have PY (h)(Ω0(

b

b

ε,δ)

Ωn)

b
1

−

≥

∩

ε, and for y

Ω0(

ε,δ)

Ωn and

∩

∈

ϑInd
n
k

(zn)

−

n (wn)
ϑInd

k ≤ k

ϑInd
n

(zn)

e
n (yn)
ϑInd
k

−

+

n (yn)
ϑInd
k

n (wn)
ϑInd
k

−

e
< ε.

ϑInd
n

b

b

By deﬁnition,

This gives the asymptotically weakly resistance of (
b
b

b
n depends on Y n through a continuous function applied to
ϑInd
is a continuous function in Y n. This and the asymptotically weakly resistance of (

ϑInd
n
PY (h) imply the weakly resistance at PY (h) by Boente et al. (1987, Proposition 4.2).
b
πS
Remark 9.4. If the stronger version supϑ
Θ k
∈
show on a similar way that the indirect estimator (

n (ϑ)
π(ϑ)
−
ϑInd
N is even strongly resistant.
)n
n
∈

N at PY (h).
)n
b
∈

πGM
n

k →

b

b

b

b

(ϑ0) and therefore,
ϑInd
N at
)n
n
∈

0 P-a.s. holds then it is possible to

As already mentioned, one could also deﬁne qualitative robustness of a sequence of estimators by
b
demanding that the distribution of the estimator does not change too much when the data is changed
slightly. To make this notion explicit, we ﬁrst deﬁne a pseudometric for measures on metric spaces.

b

Deﬁnition 9.5. For a metric space (M, d) with Borel sets B(M), the Prokhorov distance πd between
two measures µ,ν on B(M) with respect to d is deﬁned as

πd(µ,ν) := inf

ε > 0 : µ(A)
{

x
ν(
{

M : d(x, A) < ε
) + ε
}

A

B(M)
.
}

∈

∀

∈

≤

This pseudometric is a key component of the deﬁnition of qualitative robustness.
Deﬁnition 9.6. Let dΘ be a metric on Θ and let ρn be a pseudometric on P(Rn), n
P(R∞) denote by Pn the n–th order marginal of P. Finally, P
estimator
ε > 0 there exists a δ > 0 such that for every Qn

∈
P(Θ) is the distribution of the
ϑn ∈
N is called ρn–robust at P if for every
ϑn)n
b
∈
P(Rn) with ρn(Pn, Qn) < δ:

ϑn under Pn. Then the sequence of estimators (

N. For P

∈

b

πdΘ(P

ϑn

∈
, Q

b
ε.

)

≤

ϑn

As shown in Boente et al. (1987, Theorem 3.1), this is a direct generalization of the deﬁnition of
b

b

π–robustness given by Hampel (1971) for i.i.d. processes.
Theorem 9.7. On Rn we deﬁne the metric

dn(xn, yn) = inf

i :
ε : #
{
{

xi
|

−

yi

| ≥

/n
ε
}

ε
}

≤

and use the Prokhorov distance with respect to dn,

πdn(Pn, Qn) = inf

xn
Qn(
ε > 0 : Pn(A)
{
∈
{
on B(Rn). Then the sequence of estimators (
ϑInd

≤

Rn : dn(xn, A) < ε
) + ε
}

∀
N is πdn –robust at PY (h).
n )n
∈

A

B(Rn)
}

∈

b

4

Regarding the metric dn two points are close if all coordinates except a small fraction are close.

Proof. Due to Theorem 9.3 the estimator (
Boente et al. (1987, Theorem 4.2(i)) is then the πdn–robustness of (

N is weakly resistant at PY (h). A conclusion of
)n
∈

ϑInd
n

ϑInd
n

N at PY (h).
)n
∈

In summary, we can say that our indirect estimator is weakly resistant at PY (h) as well as πdn–robust.
This is in contrast to, e.g., M–estimators, which are not qualitatively robust even in the case of linear
regression (cf. Maronna and Yohai (1981, p.8)).

b

b

9.2 The inﬂuence functional

In addition to the assumptions given at the beginning of the Supporting Information we assume
throughout this section that there exists a unique solution πGM(ϑγ
mh(ϑ0))m
∈
1, and JGM(ϑ0) is non-singular. We denote the probability measure associated
for any 0
γ
to the distribution of (Y γ
1. Note that γ = 0 corresponds to the case
γ
≤
where there are no outliers, i.e., we can observe the nominal process without error and then write
P0
Y (h) = PY (h). Similarly PZ is the distribution of (Zm)m
Z. We
∈
write
γ

Z and PW is the distribution of (Wm)m
∈

P(R∞) and introduce the statistical functional

0 ) of (4.2) for (Y γ

Z by Pγ
∈

Z = (Y γ
∈

Y (h) for 0

mh)m

mh)m

:=

≤

≤

≤

Z

Pγ
Y (h)}
{

Pγ
Y (h), 0
{

≤

≤

1
} ⊆
Pγ
Y (h)} →
{

TGM :

Π as

Pγ
Y (h) 7→

πGM(ϑγ

0 ).

Then, the deﬁnition of the inﬂuence functional for the GM-estimator is

IFGM(PZ,

Pγ
Y (h)}
{

) := lim
0
γ
→

TGM(Pγ

Y (h))

−
γ

TGM(PY (h))

πGM(ϑγ
0 )
γ

−

π(ϑ0)

= lim
0
γ
→

(9.9)

whenever this limit is well–deﬁned. Note that the inﬂuence functional depends on the whole “arc”
. This is the most important difference to the deﬁnition
1
of contaminated measures
}
used by Künsch (1984), because in that paper the approximation Pγ
γ)PY (h) + γν for some
Y (h) = (1
P(R∞) is used (Künsch (1984, Eq. (1.11))). The inﬂuence functional measures the effect
ﬁxed ν
of an inﬁnitesimal contamination of the true process by the process (Zm) on the asymptotic estimate
deﬁned via the functional TGM.

Pγ
Y (h), 0
{

−

≤

≤

∈

γ

In a similar vein, we can deﬁne the inﬂuence functional for the estimation of the parameter ϑ0 of

our CARMA process. Analogous to TGM, we ﬁrst deﬁne a suitable statistical functional

TInd :

Pγ
Y (h)} →
{

Θ as

Pγ
Y (h) 7→

ϑInd

0 (γ) := arg min

ϑ

Θ

∈

[π(ϑ)

−

πGM(ϑγ

0 )]T Ω[π(ϑ)

πGM(ϑγ

0 )].

−

This is the analog of ϑ0 = arg minϑ
Θ[π(ϑ)
∈
(3.1)). With this and ϑInd
(cf.
functional of the indirect estimator is

0

π(ϑ0)] in the uncontaminated case
(0) = ϑ0 due to πGM(ϑ0) = π(ϑ0) the deﬁnition of the inﬂuence

π(ϑ0)]T Ω[π(ϑ)

−

−

IFInd(PZ,

Pγ
Y (h)}
{

) = lim
0
γ
→

TInd(Pγ

Y (h))

−
γ

TInd(PY (h))

ϑInd

0 (γ)
γ

ϑ0

.

−

= lim
0
γ
→

We are interested in properties of this functional, in particular, if it is bounded. Boundedness of the
inﬂuence functional implies that the estimate arising from the contaminated process cannot move too
far away from the one in the uncontaminated case if the rate of contamination is inﬁnitesimal. This
property is well–known for the inﬂuence functional for GM-estimators of AR processes. Since these

5

estimators are an integral building block of the indirect estimator, one can hope that it carries over to
our scenario and indeed it does, since the two functionals are proportional.

Proposition 9.8. Assume that ∇ϑπ(ϑ0) has full column rank N(Θ). Then if IFGM exists then IFInd
exists and

IFInd(PZ,

where

Pγ
Y (h)}
{

) = H (TInd(PY (h)))IFGM(PZ,

Pγ
Y (h)}
{

),

H (TInd(PY (h))) = H (ϑ0) = [∇ϑπ(ϑ0)T Ω∇ϑπ(ϑ0)]−

1∇ϑπ(ϑ0)T Ω.

Proof. This follows from de Luna and Genton (2000, Theorem 1) as special case.

From this theorem we see that the question of the boundedness of the inﬂuence functional for the
indirect estimator of a discretely sampled CARMA process reduces to the question of the boundedness
of the inﬂuence functional for the GM-estimatior of the auxiliary AR representation of the sampled
CARMA process.

Theorem 9.9. Let the additive outlier model be given and let Assumption D hold. Then there exists a
constant K > 0 such that

IFGM(PW ,
k

Pγ
)
Y (h)}
{
k ≤

2(r + 1)K

JGM(ϑ0)−

1

.

Proof. The plan is to apply Martin and Yohai (1986, Theorem 4.3). Therefore, we have to check
that (Martin and Yohai, 1986, Eq.
(4.6)) hold. Sufﬁcient conditions for this equation are given
in Martin and Yohai (1986, Theorem 4.2) which are obviously satisﬁed in our case due to (E.2),
(E.6) and due to the fact that πGM(ϑγ
0 ) depends only on the distribution of the ﬁnite random vec-
tor (Y γ
(r+1)h). For the same reasons and with our assumption that JGM(ϑ0) is non-singular the
other conditions in (Martin and Yohai, 1986, Theorem 4.3) are satisﬁed as well.

h , . . . ,Y γ

(cid:13)
(cid:13)

(cid:13)
(cid:13)

As well due to Martin and Yohai (1986, Theorem 4.3) it is possible to give an explicit (but not a very
handy) representation of IFGM(PW ,
).

) and hence, though Proposition 9.8 for IFInd(PW ,

Pγ
Y (h)}
{

Pγ
Y (h)}
{

10 Proof of Lemma 8.1

Before we state the proof we require some auxiliary results.

Lemma 10.1. For any i, j, l

the following conditions hold:

(a) supϑ
Θ |
∈

fϑ(u)

| ≤

C e−

∈ {
ρu and

1, . . . , N(Θ)
}
∞
Θ fϑ(u)2 du < ∞.
0 supϑ
∈
R
ρu and

2

∂ fϑ(u)
∂ϑj

du < ∞.

(b) supϑ
∈

Θ

(c) supϑ
∈

Θ

(d) supϑ
∈

Θ

∂ fϑ(u)
∂ϑj

(cid:12)
(cid:12)
∂2 fϑ(u)
(cid:12)
∂ϑj∂ϑi

(cid:12)
(cid:12)
∂3 fϑ(u)
(cid:12)
∂ϑj∂ϑi∂ϑl

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

Cu e−

≤

Cu2 e−

≤

Cu3 e−

≤

Θ

∞
0 supϑ
∈
R
ρu and

∞
0 supϑ
∈
R
ρu and

(cid:16)

Θ

(cid:17)
∂2 fϑ(u)
∂ϑj∂ϑi

2

du < ∞.

(cid:16)

Θ

(cid:17)
∂3 fϑ(u)
∂ϑj∂ϑi∂ϑl

2

du < ∞.

(cid:17)

∞
0 supϑ
∈
R

(cid:12)
(cid:12)
(cid:16)
(cid:12)
(cid:12)
Proof. (a) Due to Remark 2.1(iii) we have that supϑ
(cid:12)
(cid:12)
Θ k
∈
eAϑu
cϑ
supϑ
supϑ
Θ k
Θ k
∞
∈
∈
Θ fϑ(u)2 du < ∞.
0 supϑ
∈

C e−

k ≤

k

R

6

C e−
| ≤
ρu using the continuity of cϑ on the compact set Θ. Finally,

ρu and hence, supϑ
Θ |
∈

fϑ(u)

eAϑu

k ≤

(b) A consequence of Wilcox (1967) is that

∂ eAϑu
∂ϑj

=

0
Z

u

eAϑ(u
−

s)

∂
∂ϑj

(cid:18)

Aϑ

eAϑs ds,

(cid:19)

and hence, supϑ
Θ k
∈

∂ eAϑu
∂ϑj k ≤

Cu e−

ρu. From this and

∂ fϑ(u)
∂ϑj

=

∂cϑ
∂ϑj (cid:19)

(cid:18)

eAϑu ep + cϑ

∂ eAϑu
∂ϑj (cid:19)

(cid:18)

ep

we receive

∞

∂ fϑ(u)
∂ϑj (cid:19)
(c,d) can be proven similarly to (a) and (b).

sup
ϑ

Θ(cid:18)
∈

0
Z

2

∞

du

C

≤

0

Z

u2 e−

2ρu du < ∞.

R be a Lévy process with E
Lemma 10.2. Let (Lt)t
L1
|
∈
Furthermore, let φ : R
R be a measurable map with φ
→
and there exist ﬁnite constants c j1,..., jk such that

T

2N∗ < ∞ for some N∗ ∈
|
N∗
j=1 L2 j(R). Then E
∈
|

N and E(L1) = 0.
∞
2N∗ < ∞
∞ φ(u) dLu
|

−
R

E

∞

∞

(cid:18)Z

−

φ(u) dLu

2N∗

(cid:19)

=

N∗
∑
k=1

c j1,..., jk

∞

∞

(cid:18)Z

−

∑
j1+...+ jk=N∗
N0
j1,..., jk∈

φ2 j1 (u) du

(cid:19)

∞

· · ·

∞

(cid:18)Z

−

φ2 jk (u) du

.

(cid:19)

Proof. For N∗ = 2 the result was already derived in Cohen and Lindner (2013, Lemma 3.2). The
proof for general N∗ uses the same ideas. Let ν be the Lévy measure of (Lt)t
R and V its Gaussian
∈
parameter. Deﬁne

ψ(s) =

1
2

−

V 2s2

∞

∞

Z

−

φ2(u) du +

∞

∞

Z

−

∞ Z
−

∞

[eisφ(u)x

1

−

−

isφ(u)x]ν(dx) ds,

and write ψ(i)(s) for the i-th derivative of ψ(s). Due to the Lévy-Khintchine formula we get

ξ(s) := E

exp

is

(cid:18)

(cid:18)

∞

∞

Z

−

φ(u) dLu

= exp (ψ(s)) .

(cid:19)(cid:19)

2N∗ is obtained as (2N∗)-th derivative of ξ(s) at s = 0 times (
Then E
forward calculations yield that the (2N∗)-th derivative ξ(2N∗)(s) of ξ(s) has the form

∞
∞ φ(u) dLu

−

−

(cid:0)R

(cid:1)

1)N∗. Straight-

2N∗
∑
k=1

ξ(2N∗)(s) = 



∑
i1+...+ik=2N∗
i1,...,ik∈

N0

ci1,...,ikψ(i1)(s)

· · ·

e

ψ(ik)(s)



exp(ψ(s)).

Plugging in for s = 0 and taking into account that ψ(0) = 1 and ψ(1)(0) = 0 gives

E

∞

∞

(cid:18)Z

−

φ(u) dLu

2N∗

(cid:19)

= (

−

1)N∗ξ(2N∗)(0) =

N∗
∑
k=1

∑
j1+...+ jk =N∗
N0
j1,..., jk ∈

c j1,..., jkψ(2 j1)(0)

ψ(2 jk)(0).

· · ·

7

Finally, with

ψ(2)(0) =

(cid:20)
ψ(2i)(0) = (

V 2

−

1)i1

−

∞

∞

− Z
−
∞

∞

(cid:20)Z

−

x2 ν(dx)

(cid:21) (cid:20)Z

x2i ν(dx)

∞

∞
−
∞

∞

(cid:21) (cid:20)Z

−

,

φ2(u) du
(cid:21)
φ2i(u) du
(cid:21)

,

2,

i

≥

we obtain the result.

N be such that 2N∗ > N(Θ) and E
Lemma 10.3. Let N∗ ∈
L1
|
the following maps are P-a.s. Hölder-continuous of order γ

2N∗ < ∞. For any i, j, l
|
N(Θ)/(2N∗)):
[0, 1
∈

−

1, . . . , N(Θ)
}

∈ {

(a) ϑ

(b) ϑ

(c) ϑ

7→

7→

7→

R

R

∞
0 fϑ(u) dLu =: Z(ϑ),
∞
0

fϑ(u) dLu =: Z( j)(ϑ),

∂
∂ϑj

∞
0

∂
∂ϑj∂ϑi

fϑ(u) dLu =: Z( j,i)(ϑ).

R

Moreover, E(supϑ
Θ |
∈
The same is true for Z( j)(ϑ) and Z( j,i)(ϑ).

2N∗ ) < ∞ and for U := sup 0<
Z(ϑ)
|

<1

ϑ2k
ϑ1−
k
Θ
ϑ1 ,ϑ2∈

Z(ϑ1)
|
ϑ1
k

Z(ϑ2)
−
ϑ2
−

γ we have EU 2N∗ < ∞.
k

|

Proof.
(a) Let ϑ1,ϑ2

∈

Θ and deﬁne φ(u) := fϑ1(u)

fϑ2(u). Due to a Taylor expansion we obtain

φ(u) = fϑ1(u)

−

ϑ(u)(u)(ϑ1

ϑ2)

−

−
fϑ2(u) = ∇ϑ f

for some

ϑ(u)

Θ with

ϑ(u)
k

−

∈

ϑ2

e

φ(u)
|

e
| ≤

sup
ϑ

Θ k
∈

Plugging this into Lemma 10.2 gives

ϑ1

ϑ2

k ≤ k
∇ϑ fϑ(u)

−
ϑ1

e

. Hence, we receive by Lemma 10.1
k
ϑ2

Cu e−

ϑ2

ρu

ϑ1
k

−

.
k

kk

−

k ≤

E(Z(ϑ1)

Z(ϑ2))2N∗

−
N∗
∑
k=1

C

C

ϑ1
k

≤

≤

∞

ϑ1

∞ k

(cid:18)Z

−

ϑ2

2 j1 u2 j1 e−
k

−

2 j1ρ du

(cid:19)

∞

ϑ1

∞ k

· · ·

(cid:18)Z

−

ϑ2

2 jk u2 jk e−
k

−

2 jkρ du

(cid:19)

c j1,..., jk |
|

∑
j1 +...+ jk=N∗
N0
j1 ,..., jk∈
2N∗ .
ϑ2
k

−

Then, an application of Kolmogorov-Chentsov Theorem (cf. Schilling and Partzsch (2014, Theorem
10.1)) yield the Hölder continuity and EU 2N∗ < ∞. Since Θ is compact, some straightforward calcu-
lations yield E(supϑ
Θ |
∈

2N∗ ) < ∞.
Z(ϑ)
|

The proofs of (b)-(c) are similarly to the proof of (a) and thus, skipped.

Lemma 10.4. Let [a, b]
⊆
moments and let g : [a, b]
Moreover, assume

×

→

R be a bounded interval, (Lt)t
R

R be a Lévy process with ﬁnite second
∈
R be differentiable in the ﬁrst component with derivative ∂g(ϑ,u)
∂ϑ .

(a)

∂g(ϑ, u)
∂ϑ

is bounded B([a, b])

B([

−

⊗

u1, u2])-measurable for all u1, u2 > 0,

8

(b)

lim
M
∞
→

ϑ

sup
>M (cid:12)
u
[a,b] Z|
|
∈
(cid:12)
(cid:12)
∞
(cid:12)

(c) ϑ

(d) ϑ

7→

Z

∞

−
∞

7→

∞

Z

−

∂g(ϑ, u)
∂ϑ

du = 0 and lim
u
∞
|→
|

(cid:12)
(cid:12)
(cid:12)
(cid:12)

ϑ

sup
[a,b] (cid:12)
∈
(cid:12)
(cid:12)
(cid:12)

∂g(ϑ, u)
∂ϑ

= 0,

(cid:12)
(cid:12)
(cid:12)
(cid:12)

g(ϑ, u) dLu is P-a.s. continuous,

∂g(ϑ, u)
∂ϑ

dLu is P-a.s. continuous.

Then, outside a P-zero set, Z(ϑ,ω) :=
val (a, b) and

R

∞
∞ g(ϑ, u) dLu(ω) is continuous differentiable over the inter-

−

∂
∂ϑ

∞

∞

Z

−

g(ϑ, u) dLu(ω) =

∞

∞

Z

−

∂g(ϑ, u)
∂ϑ

dLu(ω).

Proof. An application of Fubini’s Theorem for Lévy-integrals (see (Brockwell and Schlemm, 2013,
Theorem 2.4)) gives that for

[a, b]

ϑ

∈

ϑ

∞

a Z
Z e

∞

∂g(ϑ, u)
e
∂ϑ

dLu dϑ =

∞

ϑ

a
∞ Z e

∂g(ϑ, u)
∂ϑ

dϑdLu P-a.s.

Z
−
The remaining of the proof follows the same line as Hutton and Nelson (1984, Theorem 2.2).

−

Proof of Lemma 8.1. A combination of Lemma 10.1-Lemma 10.4 result in the MA representation. A
conclusion of the MA representation and Fuchs and Stelzer (2013, Theorem 3.5) is that the process is
mixing and therefore, in particular, ergodic.

9

11 Simulation results

Mean
-1.02333
-1.98112
-2.00188
0.01253
1.00248

r = 5
Bias
-0.02333
0.01888
-0.00188
0.01253
0.00248

Var
0.00515
0.00905
0.01286
0.00436
0.00356

Mean
-1.0031
-2.0444
-1.9969
-0.0157
0.9147

Mean
-1.0031
-2.0446
-1.9966
-0.0157
0.9144

Mean
-0.9476
-2.1688
-1.9481
-0.0491
0.6996

Mean
-0.6035
-3.8476
-6.0640
2.1462
0.7653

r = 5
Bias
-0.0031
-0.0444
0.0031
-0.0157
-0.0853

r = 5
Bias
-0.0031
-0.0446
0.0034
-0.0157
-0.0856

r = 5
Bias
0.0524
-0.1688
0.0519
-0.0491
-0.3004

r = 5
Bias
0.3965
-1.8476
-4.0640
2.1462
-0.2347

Var
0.0131
0.0606
0.0325
0.0070
0.0243

Var
0.0131
0.0608
0.0325
0.0070
0.0243

Var
0.0426
0.1375
0.0469
0.0101
0.0705

Var
7.5738
25.0781
417.3265
11.3420
26.1110

1
2
2

ϑ1 =
−
ϑ2 =
−
ϑ3 =
−
ϑ4 = 0
ϑ5 = 1

1
2
2

ϑ1 =
−
ϑ2 =
−
ϑ3 =
−
ϑ4 = 0
ϑ5 = 1

1
2
2

ϑ1 =
−
ϑ2 =
−
ϑ3 =
−
ϑ4 = 0
ϑ5 = 1

1
2
2

ϑ1 =
−
ϑ2 =
−
ϑ3 =
−
ϑ4 = 0
ϑ5 = 1

1
2
2

ϑ1 =
−
ϑ2 =
−
ϑ3 =
−
ϑ4 = 0
ϑ5 = 1

Var
0.00501
0.00865
0.01263
0.00470
0.00198

Var
0.4248
1.0920
0.2113
0.0130
0.0536

Mean
-1.1226
-2.2005
-2.0514
-0.0279
0.8876

Mean
-1.02802
-1.97851
-2.00098
0.01781
0.99551

Indirect estimation
ξ = 0,γ = 0 (uncontaminated)
r = 6
Bias
-0.02802
0.02149
-0.00098
0.01781
-0.00449
ξ = 5,γ = 0.1
r = 6
Bias
-0.1226
-0.2005
-0.0514
-0.0279
-0.1124
ξ = 10,γ = 0.1
r = 6
Bias
-0.0204
-0.0581
0.0006
-0.0193
-0.0993
ξ = 5,γ = 1/6
r = 6
Bias
-0.0935
-0.4429
0.0582
-0.0743
-0.4054
ξ = 5,γ = 0.25
r = 6
Bias
0.7638
-1.1737
-1.2305
1.7850
0.4551

Mean
-1.0204
-2.0581
-1.9994
-0.0193
0.9007

Mean
-1.0935
-2.4429
-1.9418
-0.0743
0.5946

Mean
-0.2362
-3.1737
-3.2305
1.7850
1.4551

Var
0.0137
0.0980
0.0341
0.0104
0.0466

Var
0.1174
1.1446
0.1021
0.0231
0.1107

Var
0.1676
0.7282
5.8199
2.7106
0.8809

Mean
-1.02079
-2.00511
-1.99128
0.01541
0.98748

r = 7
Bias
-0.02079
-0.00511
0.00872
0.01541
-0.01252

Var
0.00681
0.02403
0.01429
0.00552
0.00830

Mean
-1.0670
-2.1946
-2.0483
-0.0104
0.8370

Mean
-1.0319
-2.1881
-2.0324
-0.0082
0.8583

Mean
-0.9831
-2.4168
-1.9096
-0.0481
0.5980

Mean
-0.2224
-3.1487
-3.1563
1.6978
1.4777

r = 7
Bias
-0.0670
-0.1946
-0.0483
-0.0104
-0.1630

r = 7
Bias
-0.0319
-0.1881
-0.0324
-0.0082
-0.1417

r = 7
Bias
0.0169
-0.4168
0.0904
-0.0481
-0.4020

r = 7
Bias
0.7776
-1.1487
-1.1563
1.6978
0.4777

Var
0.1802
0.6932
0.1112
0.0109
0.1106

Var
0.0445
0.7340
0.0532
0.0098
0.0747

Var
0.0561
0.5622
0.1023
0.0192
0.1281

Var
0.1255
0.6338
4.8466
1.7545
0.7939

Table 11.1: Estimation results for a CARMA(3, 1) process with parameter ϑ0 = (ϑ1,ϑ2,ϑ3,ϑ4,ϑ5)

driven by a Brownian motion with n = 1000.

10

Mean
-1.32232
-2.28763
-2.11439
0.00287
0.88711

n = 200
Bias
-0.32232
-0.28763
-0.11439
0.00287
-0.11289

Var
0.94063
1.29387
0.42097
0.02619
0.08930

Mean
-1.0290
-2.2015
-2.0014
0.0044
0.7710

Mean
-1.0244
-2.2144
-1.9920
0.0026
0.7643

Mean
0.9337
-2.3455
-2.0464
0.0009
0.5380

Mean
-0.4340
-3.0477
-4.0795
1.9005
1.0938

n = 200
Bias
-0.0290
-0.2015
-0.0014
0.0044
-0.2290

n = 200
Bias
-0.0244
-0.2144
0.0080
0.0026
-0.2357

n = 200
Bias
0.0663
-0.3455
-0.0464
0.0009
-0.4620

n = 200
Bias
0.5660
-1.0477
-2.0795
1.9005
0.0938

Var
0.0589
0.4188
0.1333
0.0302
0.1279

Var
0.0611
0.4986
0.1456
0.0296
0.1375

Var
0.1275
0.7441
0.6561
0.0850
0.2457

Var
1.5176
0.8088
55.4861
18.3389
3.3676

1
2
2

ϑ1 =
−
ϑ2 =
−
ϑ3 =
−
ϑ4 = 0
ϑ5 = 1

1
2
2

ϑ1 =
−
ϑ2 =
−
ϑ3 =
−
ϑ4 = 0
ϑ51

1
2
2

ϑ1 =
−
ϑ2 =
−
ϑ3 =
−
ϑ4 = 0
ϑ5 = 1

1
2
2

ϑ1 =
−
ϑ2 =
−
ϑ3 =
−
ϑ4 = 0
ϑ5 = 1

1
2

ϑ1 =
−
ϑ2 =
−
ϑ3 = 2
ϑ4 = 0
ϑ5 = 1

Var
0.01299
0.02302
0.01541
0.00969
0.02011

Mean
-1.06069
-2.03660
-1.94386
-0.03115
0.93227

Var
0.0206
0.1148
0.0292
0.0077
0.0463

Mean
-1.0561
-2.1196
-1.9090
-0.0641
0.8764

Indirect estimation
ξ = 0,γ = 0 (uncontaminated)
n = 500
Bias
-0.06069
-0.03660
0.05614
-0.03115
-0.06773
ξ = 5,γ = 0.1
n = 500
Bias
-0.0561
-0.1196
0.0910
-0.0641
-0.1236
ξ = 10,γ = 0.1
n = 500
Bias
-0.0559
-0.1197
0.0911
-0.0641
-0.1240
ξ = 5,γ = 1/6
n = 500
Bias
-0.0080
-0.2549
0.1066
-0.0617
-0.3467
ξ = 5,γ = 0.25
n = 500
Bias
0.8775
-0.9516
-0.5088
1.3724
0.7279

Mean
-1.0080
-2.2549
-1.8934
-0.0617
0.6533

Mean
-0.1225
-2.9516
-2.5088
1.3724
1.7279

Mean
-1.0559
-2.1197
-1.9089
-0.0641
0.8760

Var
0.0325
0.1925
0.0527
0.0119
0.1024

Var
0.0497
0.1455
0.6350
0.3679
0.1833

Var
0.0206
0.1158
0.0292
0.0077
0.0467

Mean
-1.00534
-1.99576
-1.99930
0.00089
0.99864

Mean
-1.0011
-2.0141
-1.9866
-0.0140
0.9651

Mean
-1.0026
-2.0113
-1.9880
-0.0141
0.9672

Mean
-0.9521
-2.0912
-1.9667
-0.0370
0.7963

Mean
-0.0498
-2.8767
-2.3780
1.2918
1.8691

n = 5000
Bias
-0.00534
0.00424
0.00070
0.00089
-0.00136

n = 5000
Bias
-0.0011
-0.0141
0.0134
-0.0140
-0.0349

n = 5000
Bias
-0.0026
-0.0113
0.0120
-0.0141
-0.0328

n = 5000
Bias
0.0479
-0.0912
0.0333
-0.0370
-0.2037

n = 5000
Bias
0.9502
-0.8767
-0.3780
1.2918
0.8691

Var
0.00046
0.00034
0.00103
0.00035
0.00027

Var
0.0013
0.0026
0.0029
0.0004
0.0035

Var
0.0012
0.0022
0.0028
0.0004
0.0033

Var
0.0085
0.0263
0.0114
0.0026
0.0301

Var
0.0262
0.0451
0.6102
0.3086
0.0508

Table 11.2: Estimation results for a CARMA(3, 1) process with parameter ϑ0 = (ϑ1,ϑ2,ϑ3,ϑ4,ϑ5)

driven by a Brownian motion with r = 5.

11

ξ = 0,γ = 0 (uncontaminated)

Mean
-0.10186
-1.20025
-2.09793
0.00080
1.00766

QMLE
Bias
-0.00186
-0.00025
0.00207
0.00080
0.00766

Var
0.00064
0.00025
0.00019
0.00001
0.00051

Mean
-0.12204
-1.23417
-2.04382
-0.00368
0.98731

Indirect
Bias
-0.02204
-0.03417
0.05618
-0.00368
-0.01269

Var
0.00365
0.00846
0.02458
0.00212
0.01179

0.1
1.2
2.1

ϑ1 =
−
ϑ2 =
−
ϑ3 =
−
ϑ4 = 0
ϑ5 = 1

Table 11.3: Estimation results for a CARMA(3, 1) process with parameter ϑ0 = (ϑ1,ϑ2,ϑ3,ϑ4,ϑ5)

driven by a Brownian motion with n = 1000 and r = 5.

References

G. Boente, R. Fraiman, and V. Yohai. Qualitative robustness for stochastic processes. Ann. Statist.,

15(3):1293–1312, 1987.

P.J. Brockwell and E. Schlemm. Parametric estimation of the driving Lévy process of multivariate

CARMA processes from discrete observations. J. Multivariate Anal., 115:217–251, 2013.

S. Cohen and A. Lindner. A central limit theorem for the sample autocorrelations of a Lévy driven
continuous time moving average process. J. Statist. Plann. Inference, 143(8):1295–1306, 2013.

D. Cox. Metrics on stochastic processes and qualitative robustness. University of Washington, Dept.

of Statistics, Techn. Rep, 3, 1981.

X. de Luna and M.G. Genton. Robust simulation-based estimation. Statist. Probab. Lett., 48(3):

253–259, 2000.

F. Fuchs and R. Stelzer. Mixing conditions for multivariate inﬁnitely divisible processes with an ap-
plication to mixed moving averages and the supOU stochastic volatility model. ESAIM: Probability
and Statistics, 17:455–471, 2013.

F.R. Hampel. A general qualitative deﬁnition of robustness. Ann. Math. Statist., pages 1887–1896,

1971.

J.E. Hutton and P.I. Nelson.

Interchanging the order of differentiation and stochastic integration.

Stochastic Process. Appl., 18(2):371–377, 1984.

H. Künsch. Inﬁnitesimal robustness for autoregressive processes. Ann. Statist., 12(3):843–863, 1984.

R.A. Maronna and V.J. Yohai. Asymptotic behavior of general M-estimates for regression and scale

with random carriers. Z. Wahrscheinlichkeit, 58(1):7–20, 1981.

T. Marquardt and R. Stelzer. Multivariate CARMA processes. Stochastic Process. Appl., 117(1):

96–120, 2007.

D.R. Martin and V.J. Yohai. Inﬂuence functionals for time series. Ann. Statist., 14(3):781–818, 1986.

R.L. Schilling and L. Partzsch. Brownian motion. De Gruyter, Berlin, 2. edition, 2014.

R.M. Wilcox. Exponential operators and parameter differentiation in quantum physics. J. Math.

Physics, 8:962–982, 1967.

12

