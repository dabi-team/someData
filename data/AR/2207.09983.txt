JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021

1

Diffsound: Discrete Diffusion Model for
Text-to-sound Generation

Dongchao Yang, Jianwei Yu, Helin Wang, Wen Wang, Chao Weng, Yuexian Zou, Senior Member, IEEE and
Dong Yu, Fellow, IEEE

2
2
0
2

l
u
J

0
2

]

D
S
.
s
c
[

1
v
3
8
9
9
0
.
7
0
2
2
:
v
i
X
r
a

Abstract—Generating sound effects that humans want is an
important topic. However, there are few studies in this area for
sound generation. In this study, we investigate generating sound
conditioned on a text prompt and propose a novel text-to-sound
generation framework that consists of a text encoder, a Vector
Quantized Variational Autoencoder (VQ-VAE), a decoder, and
a vocoder. The framework ﬁrst uses the decoder to transfer
the text features extracted from the text encoder to a mel-
spectrogram with the help of VQ-VAE, and then the vocoder
is used to transform the generated mel-spectrogram into a
waveform. We found that the decoder signiﬁcantly inﬂuences
the generation performance. Thus, we focus on designing a
good decoder in this study. We begin with the traditional
autoregressive decoder, which has been proved as a state-of-
the-art method in previous sound generation works. However,
the AR decoder always predicts the mel-spectrogram tokens
one by one in order, which introduces the unidirectional bias
and accumulation of errors problems. Moreover, with the AR
decoder, the sound generation time increases linearly with the
sound duration. To overcome the shortcomings introduced by
AR decoders, we propose a non-autoregressive decoder based
on the discrete diffusion model, named Diffsound. Speciﬁcally,
the Diffsound predicts all of the mel-spectrogram tokens in one
step and then reﬁnes the predicted tokens in the next step, so
the best-predicted results can be obtained after several steps.
Our experiments show that our proposed Diffsound not only
produces better text-to-sound generation results when compared
with the AR decoder but also has a faster generation speed, e.g.,
MOS: 3.56 v.s 2.786, and the generation speed is ﬁve times faster
than the AR decoder. Furthermore, to automatically assess the
quality of generated samples, we deﬁne three different objective
evaluation metrics (e.g., FID, KL, and audio caption loss), which
can comprehensively assess the relevance and ﬁdelity of the
generated samples. Code, pre-trained models, and generated
samples are released 1.

Index Terms—Text-to-sound generation, autoregressive model,

diffusion model, vocoder

I. INTRODUCTION

U SER controlled sound generation has a lot of potential

applications, e.g., movie and music productions, game
scene sound effects, and so on. With the development of virtual
reality (VR) technology, it is very important to generate the
sound effects that users want. Research on sound generation is
very limited. Chen et al. [1], Zhou et al. [2] and Iashin et al.

Dongchao Yang, Helin Wang, Wen Wang and Yuexian Zou are with the
Advanced Data and Signal Processing laboratory, School of Electronic and
Computer Engineering, Peking University, China. This work was done when
Dongchao Yang was an intern at Tencent AI Lab.

Jianwei Yu, Chao Weng and Dong Yu are with Tencent AI Lab.
Yuexian Zou and Jianwei Yu are the corresponding authors.

(zouyx@pku.edu.cn; tomasyu@tencent.com)

1http://dongchaoyang.top/text-to-sound-synthesis-demo/

[3] proposed to generate sound related to a video. Liu et al.
[4] and Kong et al. [5] attempted to generate environmental
sound conditioned on a one-hot label. No one, however, has
looked into generating sound from text descriptions. Text-
to-sound generation has a wide range of uses, e.g., adding
background sound for speech synthesis systems. Nowadays,
speech synthesis systems have been applied to poetry or
novel reading. The user experience could be improved by
adding background sound to scenarios represented in text.
Furthermore, many music or movie designers are required to
ﬁnd a suitable sound for a scene. A simple approach is that
they describe the scene with a sentence, and then use the text-
to-sound model to generate the corresponding sound. In this
work, we focus on directly generating audio based on human-
written descriptions, such as “An audience cheers and applauds
while a man talks”.

The state-of-the-art methods [3], [4] in the sound gen-
eration both employ a two-stage generation strategy, which
ﬁrst uses autoregressive (AR) decoder to generate a mel-
spectrogram conditioned on the one-hot label or the video
and then employs a vocoder (e.g. MelGAN [6]) to transform
the generated mel-spectrogram into waveform. To improve the
generation efﬁciency, they propose to learn a prior in the form
of the Vector Quantized Variational Autoencoder (VQ-VAE)
codebook [7], which aims to compress the mel-spectrogram
into a group of discrete tokens with a ﬁxed number. By using
VQ-VAE, the mel-spectrogram generation problem transfers
to predicting a group of discrete tokens corresponding to the
mel-spectrogram. Inspired by [3], [4], we propose a text-to-
sound generation framework, which consists of a text encoder,
a VQ-VAE, a decoder, and a vocoder. The diagram of the text-
to-sound framework is shown in Figure 1. We found that the
decoder signiﬁcantly inﬂuences the generation performance.
Thus, we focus on designing a good decoder in this paper.

We start by looking into the AR decoder. However, we
discovered that the AR decoder is unable to produce high-
ﬁdelity and high-relevance sound with text input. There are
two issues that have an impact on the generation results.
First, while the AR decoder has been shown to be a powerful
method in sound generation [3], [4], it has two ﬂaws: (1)
Mel-spectrogram tokens are always predicted in order (e.g.,
from left to right) by the AR decoder. This ﬁxed order intro-
duces unidirectional bias in the synthesized mel-spectrogram
because important contextual information can come from any
part of the mel-spectrogram; (2) During the inference phase,
incorrectly predicted tokens from previous steps propagate to
subsequent tokens, resulting in accumulated prediction errors.

 
 
 
 
 
 
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021

2

Fig. 1. The diagram of the text-to-sound generation framework includes
four parts: a text encoder that extracts text features from the text input, a
decoder that generates mel-spectrogram tokens, a pre-trained VQ-VAE that
transforms the tokens into mel-spectrogram, and a vocoder that transforms the
generated mel-spectrogram into waveform. We explore two kinds of decoders,
an autoregressive decoder and a non-autoregressive decoder (Diffsound).

Second, the available text-audio data is limited. The largest
open-source text-audio dataset is Audiocaps [8], which only
includes about 49K training data. In contrast, Iashin et al. [3]
trains their model using Vggsound dataset [9], which has over
200K audio-video pairs.

To address the weaknesses in the AR decoder, we propose
a non-autoregressive decoder based on diffusion probabilistic
models (diffusion models for short) [10]–[13], named Diff-
sound. Instead of predicting the mel-spectrogram tokens one
by one in order, Diffsound predicts all of the mel-spectrogram
tokens in one step, and then reﬁnes the predicted tokens in the
next step, so that the best results can be obtained after several
steps. In each step, the Diffsound leverages the contextual
information of all tokens predicted in the previous step to
estimate a new probability density distribution and uses this
distribution to sample the tokens in the current step. Due
to the fact that Diffsound can make use of the contextual
information of all tokens and revise any token in each step, it
effectively avoids the unidirectional bias and the accumulated
prediction error problems. To realize this, we adopt the idea
from diffusion models, which use a forward process to corrupt
the original mel-spectrogram tokens in T steps, and then let
the model learn to recover the original tokens in a reverse
process. Speciﬁcally, in the forward process, we deliberately
use both mask token and random token to replace the original
tokens x0 ∼ q(x0) into a stationary distribution p(xT ). In
the reverse process, we let the network learn to recover the
original tokens from xT ∼ p(xT ) conditioned on the text
features. Figure 1 shows an example of non-autoregressive
mel-spectrogram tokens generation.

To solve the data deﬁciency problem in the text-to-sound
generation tasks. We propose to let the Diffsound learn knowl-
edge from the Audioset dataset [14] and then ﬁne-tune the
pre-trained Diffsound on a small-scale text-audio dataset (e.g.,
Audiocaps). Audioset is the largest open-source dataset in the
audio ﬁeld, but it only provides the event labels for each

Fig. 2. The overall architecture of VQ-VAE, which consists of four parts:
an encoder that extracts the representation ˆz from the mel-spectrogram, a
codebook that contains a ﬁnite number of embedding vectors, a decoder
that reconstructs the mel-spectrogram based on mel-spectrogram tokens,
and a discriminator that distinguishes the mel-spectrogram is original or
reconstructed. Q(.) denotes a spatial-wise quantizer that maps each features
ˆzij into its closest codebook entry zk to obtain the mel-spectrogram tokens.

audio. To utilize the Audioset dataset, we propose a mask-
based text generation strategy (MBTG) that can generate a text
description according to the event labels so that a new text-
audio dataset is built. Furthermore, we observe a phenomenon:
it is easier to generate audio that only includes one single event
than audio that includes multiple events. To help the Diffsound
learn better, we mimic the human learning process by let the
Diffsound learn from easy samples and gradually advance to
complex samples and knowledge. Speciﬁcally, we propose a
curriculum learning strategy in our pre-training stage, that is,
we ﬁrst select the audios that only include one event (easy
sample) to the training set, and gradually add the audios that
include multiple events (hard sample) to the training set.

Human evaluation of sound generation models is an ex-
pensive and tedious procedure. Thus, objective evaluation
metrics are necessary for sound generation tasks. We explore
three objective evaluation metrics: Frechet Inception Distance
(FID) [15], KL-divergence [3] and audio caption loss. We
demonstrate that these metrics can effectively evaluate the
ﬁdelity and relevance of the generated sound. Furthermore,
we also use the Mean Opinion Score (MOS) to assess our
methods.

Experiments show that our text-to-sound generation frame-
work can generate high-quality sound, e.g., MOS: 3.56 (ours)
v.s 4.11 (ground truth), and our proposed Diffsound has
better generation performance and speed compared to the AR
decoder, e.g., MOS: 3.56 v.s 2.786, and the generation speed is
ﬁve times faster than the AR decoder. Our main contributions
are listed as follows:

(1) For the ﬁrst time, we investigate how to generate sound
based on text description and offer a text-to-sound generation
framework. Furthermore, we propose a novel decoder (Diff-
sound) based on a discrete diffusion model that outperforms
the AR decoder in terms of generation performance and speed.
(2) To solve the data deﬁciency problem in the text-to-sound
generation task, we propose a mask-based text generation
strategy (MBTG), which helps build a large-scale text-audio
dataset based on the Audioset dataset. We demonstrate the
effectiveness of pre-training the Diffsound on the Audioset,
which gives the insight to improve the performance of the
generation model under a data deﬁciency situation.

(3) We initially explore three objective evaluation metrics
for the text-to-sound generation task. We demonstrate these

TextinputTextEncoder VQ-VAEVocoderAutoregressivedecoder51739Non-autoregressivedecoder51739Text featuresAn example of  autoregressivespectrogram tokens generationAn example of  non-autoregressive spectrogram tokens generation. AR51739157Mel-spectrogram tokens51739M1739M123MMM23MFroward processmel-spectrogram tokensReverse processmeaninglesstoken sequenceCorrupt the mel-spectrogram tokensusing mask (M) and random tokensUse Diffsound learn to recover the mel-spectrogram tokensMM23MDiffsoundM123MDiffsoundM123MM1739DiffsoundM173951739VQ-VAEDecoderSpectrogram CodebookZ2Z3Z1Z4ZKDiscriminatorspectrogramtokensQ(.)Encoder51739JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021

3

metrics can comprehensively assess the relevance and ﬁdelity
of the generated sound.

information. It

II. RELATED WORK
GAN-based Content Generation In the past few years, Gen-
erative Adversarial Networks (GANs) have shown promising
results in image generation [16]–[18], speech synthesis [6],
[19], [20] and music generation [21]. GAN-based models are
capable of synthesizing high-ﬁdelity images/sounds. However,
they suffer from well-known training instability and mode
collapse issues, which lead to a lack of sample diversity. Most
related to our work is RegNet [1], which aims to generate
sound conditioned by visual
is noted that
RegNet only generates sounds on a single domain dataset (e.g.,
dog bark or drum), which means that it struggles on complex
scenes with multiple sound events. Furthermore, Iashin et al.
[3] have proved that using an autoregressive generation model
can produce better results than RegNet.
Autoregressive Models AR models [22], [23] have shown
powerful generation capability and have been applied for
image generation [7], [24]–[29], speech synthesis [30] and
sound generation [3], [4]. To generate high-resolution images,
VQ-VAE [7], [27], VQGAN [24] and ImageBART [31] train
an encoder to compress the image into a low-dimensional
discrete latent space. After that, the AR models learn from
low-dimensional discrete latent space directly, which greatly
reduces the time complexity and improves the performance.
Liu et al. [4] and Iashin et al. [3] also apply the similar idea
to generate sound, and achieve good generation performance.
Diffusion Probabilistic Models Diffusion generative models
were ﬁrst proposed in [11] and achieved strong results on
image generation [13], [31]–[33] and speech synthesis [34]–
[37]. Diffusion models with discrete state spaces were ﬁrst
introduced by Sohl-Dickstein et al. [11], who considered a
diffusion process over binary random variables. Hoogeboom
et al. [38] extended the model to categorical random variables
with transition matrices characterized by uniform transition
probabilities. Jacob et al. [12] further improve and extend dis-
crete diffusion models by using a more structured categorical
corruption process to corrupt the forward process. D3PMs [12]
and VQ-Diffusion [13] have applied discrete diffusion models
to image generation.

III. PROPOSED TEXT-TO-SOUND FRAMEWORK

The overall architecture of

the proposed text-to-sound
framework is demonstrated in Figure 1, which consists of four
parts including a text encoder, a VQ-VAE, a decoder, and a
vocoder. The detailed design of each part will be introduced
in this section.

A. Text encoder

The ﬁrst step in the text-to-sound generation task is de-
signing a good text encoder to extract the sound event infor-
mation from the context while other information should be
excluded. In this study, we employed the pretrained BERT
[39] and the text encoder of pretrained CLIP model [40] to
extract the text features (a vector to represent the contextual

information) respectively and found that the latter brings better
generation performance. We conjecture that the CLIP model
is more suitable for text-to-sound generation tasks for the
reason that CLIP is trained by contrastive learning between
the representations of images and text, which makes the text
representations include more the concept of semantic, e.g., dog
barks and birds sing. Note that the text encoder is ﬁxed in our
training process.

B. Learning Discrete Latent Space of Mel-spectrograms Via
VQ-VAE

In this part, we introduce the vector quantized variational
autoencoder (VQ-VAE) [7] to simplify the process of decoder
generates the mel-spectrograms.

Most of the text-to-speech (TTS) methods [34], [35], [41]
directly learn the mapping from text to wave samples or raw
spectrogram pixels for the reason that the pronunciation of
speech depends on the words of text. Unlike TTS, there is no
direct correspondence between text and sound in the sound
generation task. To this end, the text-to-sound task needs to
extract the event information from the text input and then
generate the corresponding events. Considering a sound may
consist of multiple events and each event has its own unique
characteristics. We propose to use the VQ-VAE to learn a
codebook to encode the characteristic of events, and then
generate the mel-spectrogram based on the codebook. Liu et
al. [4] and Iashin et al. [3] also employ the same strategy to
generate sound, which proves that VQ-VAE not only brings
good generation performance but also reduces computation
costs. As Figure 2 shows, a mel-spectrogram can be repre-
sented by a group of mel-spectrogram tokens. Thus, the mel-
spectrogram generation problem transfers to predicting a group
of tokens. In the following, we will introduce the details of
VQ-VAE.

VQ-VAE is trained to approximate an input using a com-
pressed intermediate representation, retrieved from a discrete
codebook. VQ-VAE consists of an encoder Evq, a decoder
k=1 ∈ RK×nz containing a
G and a codebook Z = {zk}K
ﬁnite number of embedding vectors, where K is the size of
the codebook and nz is the dimension of codes. Given a
spectrogram s ∈ RF ×L, the input s is ﬁrstly encoded into
a small-scale representation (encoder Evq consists of multiple
convolution and pooling layers) ˆz = Evq(s) ∈ RF (cid:48)×L(cid:48)×nz
where F (cid:48) × L(cid:48) represents the reduced frequency and time
dimension . Then we use a spatial-wise quantizer Q(.) which
maps each spatial feature ˆzij into its closest codebook entry
zk to obtain a spatial collection of spectrogram tokens zq
zq = Q(ˆz) := (cid:0) arg min
zk∈Z

2 for all (i, j) in (F (cid:48), L(cid:48))(cid:1)
(1)
Then the spectrogram can be faithfully reconstructed via the
decoder i.e., ˆs = G(zq). Note that the spectrogram tokens are
quantized latent variables in the sense that they take discrete
values. The encoder Evq, the decoder G, and the codebook Z
can be trained end-to-end via the following loss function

||ˆzij −zk||2

LVQVAE = ||s−ˆs||1+||sg[Evq(s)]−zq||2

2+||sg[zq]−Evq(s)||2
2
(2)

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021

4

where sg is the stop-gradient operation that acts as an identity
during the forward pass but has zero gradients at the backward
pass. To preserve the reconstruction quality when upsampled
from a smaller-scale representation, we follow the setting of
VQGAN [24], which adds a patch-based adversarial loss [42]
to the ﬁnal training loss

Lf = LVQVAE + λd(log(D(s)) + log(1 − D(ˆs)))

(3)

where D is a discriminator, λd is a hyper-parameter to control
the weight of adversarial loss.

C. Decoder

The decoder in our framework is proposed to transfer the
text features to the quantized features obtained from VQ-VAE
(mel-spectrogram tokens). An autoregressive decoder is ﬁrst
investigated.

Given the text-audio pairs, we ﬁrst use the text encoder to
extract text features y from the text description. After that,
we obtain the discrete mel-spectrogram tokens x ∈ ZN from
the mel-specrogram of the audio with a pre-trained VQ-VAE,
where N = F (cid:48) × L(cid:48) represents the sequence length of tokens.
By using the VQ-VAE, the decoding process can be viewed
as an autoregressive next-token prediction: Given tokens x<i,
the decoder learns to predict the distribution of possible next
tokens, i.e., p(xi|x<i, y) to compute the likelihood of the
full representation as p(x|y) = (cid:81)
i p(xi|x<i, y). The decoder
is trained with a cross-entropy (CE) loss, comparing the
predicted mel-spectrogram tokens to those obtained from the
ground truth. Due to the wrongly predicted results of previ-
ous steps inﬂuencing the current step, the “teacher-forcing”
strategy [31] is used to guarantee the stability of training.
Lastly, we use the decoder of VQ-VAE (G) to transform the
predicted mel-spectrogram tokens into the mel-spectrogram.
In the inference stage, we can set the number of predicted
mel-spectrogram tokens N to determine the duration of the
generated sound.

However, the AR decoder suffers from the unidirectional
bias and accumulation of errors problems due to it always
predicts tokens in order. To this end, a non-autoregressive de-
coder based on discrete diffusion model is proposed. (Detials
will be given in Section IV.)

D. Vocoder

at

for

the

aims

Vocoder

transforming

generated mel-
spectrogram into waveform ˆw, which is a hot
research
topic. Grifﬁn-Lim [43], WaveNet [44], MelGAN [6], and
HiFi-GAN [45] are very popular vocoders
speech
synthesis task. The Grifﬁn-Lim is a classic signal processing
method that is very fast and easy to implement. However,
Grifﬁn-Lim produces low-ﬁdelity results when operating
on mel-spectrograms [3]. WaveNet provides high-quality
results but remains relatively slow in generation time. In this
study, considering its generation efﬁciency and quality, we
employ MelGAN which is a non-autoregressive approach
to reconstructing the waveform. MelGAN has been widely
used in speech synthesis ﬁelds. However, many pre-trained
MelGAN models are trained on speech or music data, so they

are not suitable for environmental sound generation. We train
a MelGAN on a large-scale audio event dataset (Audioset)
[14], which contains 527 unique sound events.

IV. DIFFUSION-BASED DECODER

In

our

this

proposed

introduce

section, we

non-
autoregressive decoder based on discrete diffusion model,
named Diffsound. As discussed in Section III-C, Diffsound is
proposed to address the unidirectional bias and accumulation
of errors issues in AR decoders. In the following, we ﬁrst
introduce the diffusion models. Then we discuss the details
of the training and inference of the Diffsound. Lastly, we
discuss how to use the pre-train strategy to further improve
the performance of the Diffsound.

A. Diffusion Probabilistic Models

Diffusion probabilistic (diffusion for short) models [11]
have been proved as a powerful generation model in image and
speech ﬁelds [13], [32]. In this section, we brieﬂy introduce
some basic principles of the diffusion models.

two processes:

1) Vanilla Diffusion Model: A diffusion model consists
the forward process with steps t ∈
of
{0, 1, 2, ..., T } and the reverse process t ∈ {T, T − 1, ..., 1, 0}.
The forward process corrupts the original data x0 into the
noisy latent variable xT which belongs to a stationary dis-
tribution (e.g., Gaussian distribution), and the reverse process
learns to recover original data x0 from xT .
Forward process Given the audio data x0,
the forward
process aims to corrupt the data x0 ∼ q(x0) into a sequence
of increasingly noisy latent variables x1:T = x1, x2, ..., xT .
Each of noisy latent variables xt has the same dimension with
x0. The forward process from data x0 to the variable xT can
be formulated as a ﬁxed Markov chain

q(x1:T |x0) =

T
(cid:89)

t=1

q(xt|xt−1)

(4)

√

Following [11], Gaussian noise is selected in each step, so that
the conditional probability distribution can be q(xt|xt−1) =
1 − βtxt−1, βtI), where βt is a small positive con-
N (xt;
stant. According to the pre-deﬁned schedule β1, β2, ..., βT , the
overall process gradually converts clean x0 to a latent variable
with an isotropic Gaussian distribution of p(xT ) = N (0, I).
Due to the properties of Markov chain, the probability distri-
bution q(xt|x0) can be

q(xt|x0) = N (xt;

αtx0, (1 − αt)I)

(5)

√

where αt = 1 − βt and αt = (cid:81)t
Reverse process The reverse process converts the latent
variable xT ∼ N (0, I) into x0

s=1 αs.

pθ(x0:T ) = p(xT )

T
(cid:89)

t=1

pθ(xt−1|xt)

(6)

where pθ(·) is the distribution of the reverse process with
learnable parameters θ. The posterior q(xt−1|xt, x0) can be
derived according to Bayes formula. In order to optimize the
generative model pθ(x0) to ﬁt the data distribution q(x0), one

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021

5

typically optimizes a variational upper bound on the negative
log-likelihood:

(cid:104)
DKL[q(xT |x0)||p(xT )]+

Lvb = Eq(x0)
(cid:2)DKL[q(xt−1|xt, x0)||pθ(xt−1|xt)](cid:3)(cid:105)

Eq(xt|x0)

(7)

T
(cid:88)

t=1

0 in x0, xi

2) Discrete Diffusion model: One limitation of vanilla
diffusion model is that, for original data x0 in discrete space,
e.g., for any element xi
0 ∈ {1, 2, ..., P }, we cannot
the x0 by adding Gaussian noise in the forward
corrupt
process since the range of xi
0 belongs to P different discrete
values. To solve this issue, discrete diffusion model [11],
[12] is proposed. In discrete diffusion model, a transition
probability matrix is deﬁned to indicate how x0 transits to xt
for each step of forward process. Assuming that x0 ∈ ZN
and xk
0 ∈ {1, 2, ..., P }. Without introducing confusion, we
omit superscripts k in the following description. The matrices
[Qt]ij = q(xt = i|xt−1 = j) ∈ RP ×P deﬁnes the
probabilities that xt−1 transits to xt. Then the forward process
for the whole token sequence can be written as

q(xt|xt−1) = c(cid:62)(xt)Qtc(xt−1)

(8)

where c(x) denotes the one-hot column vector of x. The cate-
gorical distribution over xt is given by the vector Qtc(xt−1).
Due to the property of Markov chain, one can marginalize
out the intermediate steps and derive the probability of xt at
arbitrary timestep directly from x0

q(xt|x0) = c(cid:62)(xt)Qtc(x0), with Qt = Qt . . . Q1

(9)

Besides, q(xt−1|xt, x0) can be derived according to Bayes
formula

q(xt|xt−1, x0)q(xt−1|x0)
q(xt|x0)

q(xt−1|xt, x0) =
(cid:0)c(cid:62)(xt)Qtc(xt−1)(cid:1)(cid:0)c(cid:62)(xt−1)Qt−1c(x0)(cid:1)
c(cid:62)(xt)Qtc(x0)

=

(10)

B. Non-autoregressive Mel-spectrograms Generation Via Diff-
sound

In contrast to the AR decoder, which predicts the mel-
spectrogram tokens one by one, we expect Diffsound can
predict all of the tokens in one step, and then reﬁne the
predicted results in the next step, so that the best results can
be obtained after several steps. In other words, we expect the
predicted results can be improved through T -step iterations.
The Diffsound can make use of the contextual information
of all tokens and revise any token in each step. It effectively
avoids the unidirectional bias and the accumulated prediction
error problems. To realize the process, we adapt the idea from
discrete diffusion model [11]–[13], which designs a strategy
to corrupt the original mel-spectrogram token sequence x0 ∼
q(x0) into a totally meaningless sequence xT ∼ p(xT ) in
T steps, and then let network learn to recover the original
sequence x0 based on the text information in T steps. Figure
1 shows an example of the forward and reverse processes. In
the inference stage, we randomly sample a token sequence

xT from p(xT ), and then let the network predict a new mel-
spectrogram token sequence based on xT and the text features.
According to the previous description in Section IV-A, the
key point of training a discrete diffusion model is to design a
suitable strategy to pre-deﬁne Markov transition matrices Qt.
As discussed in Section III-B, the codebook of VQVAE
encodes the characteristics of events, and each vector of the
codebook encodes the different
information. According to
this property, we propose three strategies to corrupt the mel-
spectrogram tokens. Firstly, changing the context by randomly
replacing the original token. Secondly, masking the context
by introducing an extra mask token. However, we ﬁnd that
changing the context by randomly replacing tokens results in
the reverse process is hard to learn because a mel-spectrogram
token may be replaced with a completely unrelated category
token, e.g., a token representing the dog barking may be
replaced by a token representing the man speaking. Further-
more, there is a context relationship between adjacent tokens.
Only using the mask token makes the model tend to focus
on the mask token and ignore the context relationship. Thus,
we propose to combine the changing and masking context
strategies. We deﬁne three transition matrices according to the
three strategies, respectively: Uniform transition matrix, mask
transition matrix, and mask and uniform transition matrix.

In the following, we ﬁrst

introduce the three transition
matrices. Then we discuss the noise schedule strategy and loss
function. Lastly, we introduce the reparameterization trick and
fast inference strategy.
Uniform transition matrix Uniform transition matrix was
ﬁrst proposed by Sohl-Dickstein et al. [11] for binary random
variables, e.g., variable zero can transfer to one or zero with
a uniform distribution. Hoogeboom et al. [38] later extended
this to categorical variables. The core idea is that each variable
can transfer to all the pre-deﬁned categories with a uniform
distribution. In practice, the transition matrix Qt ∈ RK×K can
be deﬁned as

Qt =








αt + βt
βt
...
βt

βt
αt + βt
...
βt

βt
βt
...

· · ·
· · ·
. . .
· · · αt + βt








(11)

where βt ∈ [0, 1] and αt = 1 − Kβt. This transition matrix
denotes that each token has a probability of Kβt
to be
resampled uniformly over all the K categories, while with a
probability of αt to remain the previous value at the present
step. The stationary distribution p(xT ) can be derived as:

p(xT ) = [βT , βT , · · · , βT ]

(12)

where αT = (cid:81)T
t=1 αt, βT = (1 − αT )/K. As Section IV-A2
described, we could calculate q(xt|x0) according to formula
(9), q(xt|x0) = c(cid:62)(xt)Qtc(x0). However, when the number
of categories K and time step T is too large, it can quickly
become impractical to store all of the transition matrices Qt in
memory, as the memory usage grows like O(K 2T ). Inspired
by [13], we ﬁnd that it is unnecessary to store all of the
transition matrices. Instead we only store all of αt and βt

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021

6

Algorithm 1 Training of the Diffsound.
Require:

Algorithm 2 Inference of the Diffsound.
Require:

A transition matrix Qt, timestep T , network parameters
θ, training epoch N , text-audio dataset D, the encoder of
VQ-VAE Evq.
1: for i = 1 to N do
2:

for (text, audio) in D do

3:
4:
5:
6:
7:
8:

s = get mel spectrogram(audio);
x0 = Evq(s), y =TextEncoder(text);
sample t from Uniform(1, 2, 3, ..., T );
sample xt from q(xt|x0) based on formula (19);
estimate pθ(xt−1|xt, y);
calculate loss according to formula (20-23);
update network θ;

end for

9:
10:
11: end for
12: return network θ.

Time stride ∆t, timestep T , input text, the decoder of
VQ-VAE G, network θ, stationary distribution p(xT );

1: t = T , y =TextEncoder(text);
2: sample xt from p(xT );
3: while t>0 do
4:
5:
6: end while
7: return G(xt).

xt ← sample from pθ(xt−∆t|xt, y)
t ← (t − ∆t)

probability of γt to transition to [MASK] token, a probability
of Kβt be resampled uniformly over all the K categories and
a probability of αt = 1 − Kβt − γt to stay the same token.
The transition matrices Qt ∈ R(K+1)×(K+1) is deﬁned as

in advance, because we can calculate q(xt|x0) according to
following formula:

Qt =

Qtc(x0) = αtc(x0) + βt

(13)








αt + βt
βt
...
γt

βt

βt
αt + βt βt
...
γt

...
γt

· · ·
· · ·
. . .
· · ·


0
0


...


1

(17)

The Proof can be found in Appendix A.
Mask transition matrix Previous work [13], [38] proposed
introducing an additional absorbing state, such that each token
either stays the same or transitions to the absorbing state.
In this paper, we add an additional token [MASK] into the
codebook (the index is K +1) to represent the absorbing state,
thus the model is asked to recover the original tokens from the
mask tokens. The mask transition matrix Qt ∈ R(K+1)×(K+1)
is

Qt =










βt
0
0
...
γt

0
βt
0
...
γt

0
0
βt
...
γt

· · ·
· · ·
· · ·
. . .
· · ·


0
0


0


...


1

(14)

where γt ∈ [0, 1]. The mask transition matrix denotes that each
token either stays the same with probability βt = (1 − γt) or
transitions to an additional token [MASK] with probability
γt. According to Markov transition formula, the stationary
distribution p(xT ) can be derived as:

p(xT ) = [βT , βT , · · · , γT ]

(15)

where γT = 1 − βT , and βT = (cid:81)T
t=1 βt. Similarly, we only
store all of γt and βt in advance, and we calculate q(xt|x0)
according to following formula:

Qtc(x0) = βtc(x0) + γtc(K + 1)

(16)

The Proof can be found in Appendix B.
Mask and uniform transition matrix As in the previous
discussion, using the uniform transition matrix brings the
problem of semantic change, so the reverse process is hard to
learn. Using the mask transition matrix makes the model tend
to focus on the mask token and ignore the context information.
To combat these problems, a simple idea is to combine mask
and uniform transition matrices. Speciﬁcally, each token has a

Similarly, the stationary distribution p(xT ) can be derived as

p(xT ) = [βT , βT , · · · , γT ]
t=1 αt, γT = 1 − (cid:81)T

where αT = (cid:81)T
t=1(1 − γt) and βT =
(1 − αT − γT )/K. According to previous discussions, we can
calculate q(xt|x0) according to following formula:

(18)

Qtc(x0) = αtc(x0) + (γt − βt)c(K + 1) + βt

(19)

Noise schedule Noise schedule is used to pre-deﬁne the
value of transition matrices (pre-deﬁne αt, βt, and γt
in
our study). Many noise schedules have been proposed, such
as the linear schedule, the consine schedule [46], and the
mutual-information-based noise schedule [12]. In this study,
we adapted the linear schedule for all of the experiments.
Loss function Similar to the training objective of a continuous
diffusion model, we also train a network pθ(xt−1|xt, y) to
estimate the posterior transition distribution q(xt−1|xt, x0).
The network is trained to minimize the variational lower bound
(VLB).

Lvlb =

T −1
(cid:88)

t=1

(cid:2)DKL[q(xt−1|xt, x0)||pθ(xt−1|xt, y)](cid:3)

(20)

+ DKL(q(xT |x0)||p(xT ))

where p(xT ) is the stationary distribution, which can be
derived in advance. The completed training algorithm is sum-
marized in Algorithm 1.
Reparameterization trick Recent works [13], [46] found that
approximating some surrogate variables, e.g., the noiseless tar-
get data q(x0) gives better results comparing with directly pre-
dicting the posterior q(xt−1|xt, x0). In this study, we follow
the reparameterization trick proposed in [13], which lets the
network predict the noiseless token distribution pθ(ˆx0|xt, y)

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021

7

Algorithm 3 Pre-training the Diffsound on Audioset.
Require:

The audio and its corresponding label {A, L}, the number
of training epoch n, initial network parameters θ;
1: Count the number of events for each audio-label pair;
2: Split {A, L} into two subset according the number of

events, {ASES, LSES}, {AM ES, LM ES};

for (audio, l) in {ASES, LSES} do

text = MBTG(l);
Train the model θ according to (text, audio);

end for

3: for i = 1 to n do
4:
5:
6:
7:
8: end for
9: for i = 1 to 2 · n do
10:
11:
12:
13:
14: end for
15: return Diffsound θ.

end for

for (audio, l) in {AM ES, LM ES} do

text = MBTG(l);
Train the model θ according to (text, audio);

at each reverse step, and then compute pθ(xt−1|xt, y) accord-
ing to the following formula:

pθ(xt−1|xt, y) =

K
(cid:88)

ˆx0=1

q(xt−1|xt, ˆx0)pθ(ˆx0|xt, y)

(21)

Based on the formula (21), an auxiliary denoising objective
loss is introduced, which encourages the network to predict
noiseless token x0:

Lx0 = −logpθ(x0|xt, y)

(22)

Experimental results indicate that combining Lx0 and Lvlb
could get better performance. Thus our ﬁnal loss function is
deﬁned as:

L = λLx0 + Lvlb

(23)

where λ is a hyper-parameter to control the weight of the
auxiliary loss Lx0 .
Fast inference strategy We can see that the inference speed
is related to the timestep T . By
of the Diffsound model
leveraging the reparameterization trick, we can skip some
steps in the Diffsound model to achieve a faster inference.
Usually, we sample the spectrogram tokens in the chain
of xT , xT −1, xT −2, ..., x0. Now, we can use a larger time
stride ∇t,
thus we sample the spectrogram tokens in the
chain of xT , xT −∇t, xT −2∇t, ..., x0. We found this strategy
makes the inference stage more efﬁcient, which only causes
little decrease to quality. The whole inference algorithm is
summarized in Algorithm 2.

C. Pre-training Diffsound on Audioset dataset

Recently, image generation has got great success, one of
the reasons is that they collect large-scale text-image data,
e.g., CogView [25] collects more than 30 million high-quality
(Chinese) text-image pairs. However, collecting large-scale

text-audio pairs is time-consuming. Audioset [14] is the largest
open-source dataset in the audio ﬁeld, but it only provides
the event labels for each audio. To utilize these data, we
propose a mask-based text generation method to generate text
descriptions according to event labels.
Mask-based text generation method (MBTG) Our method
is based on a basic observation of how humans write text
descriptions for audio: Humans ﬁrst listen to the audio to ﬁnd
out which events happen, and then they add some detailed
descriptions to compose a sentence. For example, if the label is
“dog barks, man speaks”, one can generate the text description
like that “a dog is barking when a man is speaking” or “a dog
barks after a man speaks over”. The ﬁrst sentence indicates that
the events of dog barking and man speaking are simultaneously
happening. The latter shows that we ﬁrst listen to a man
speaks, and then a dog barks. Although the keywords are the
same, the generated two texts correspond to different audio due
to different detailed descriptions. It means that automatically
generating text descriptions according to the label information
is a tough task. Instead of generating speciﬁc text, we propose
to use ‘[MASK]’ token to replace the detailed description. We
can generate text descriptions like that “[MASK] [MASK] dog
bark [MASK] man speaking [MASK]”. We expect the model
to learn the relationship of events rather than directly obtain it
from the text description. The generation algorithm is easy to
implement. We randomly insert m ∈ {1, 2} mask tokens on
either side of the label.
Curriculum Learning Strategy We found that it is easier to
generate audio that only includes a single event than audio that
includes multiple events. To help the Diffsound learn better,
we mimic the human learning process by letting the Diffsound
learn from easy samples, and gradually advance to complex
samples and knowledge. Thus, a curriculum learning strategy
is proposed. Speciﬁcally, we rank the Audioset according to
the number of events in the audio, and then we split the
Audioset into two subsets: one only includes the audio of a
single event (we refer to it as the Single Event Set (SES)), and
the other includes the audio of multiple events (we refer to it as
the Multiple Event Set (MES)). We ﬁrst train the Diffsound
on the SES in the ﬁrst few epochs. After that we train the
Diffsound on the MES. The whole algorithm is summarized
in Algorithm 3.

V. EVALUATION METRIC

In this study, we investigate Humans Mean Opinion Score

(MOS) and objective assessment metrics.

A. MOS

We randomly choose 15 groups generated sound by AR de-
coder and Diffsound. Each group includes one text description,
one real sample, 1-2 generated sounds by AR decoder, and 1-2
generated sounds by the Diffsound. We let 10 people give the
grades for these sounds in three aspects: relevance, ﬁdelity, and
intelligibility. Note that the test person never knows whether
the sound is real or generated. We ask people to give 0-5
grades on the three aspects. Finally, we use the average score
of the three aspects as the MOS.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021

8

B. Objective Assessment Metrics

Human evaluation of the performance of the sound gen-
eration model is an expensive and tedious procedure. Thus,
designing a proper evaluation metric that can measure the gap
between generated and real samples is very important for the
generation task. In this paper, we employed three objective
quality assessment metrics: Fr´echet Inception Distance (FID),
Kullback-Leibler (KL) divergence, and Audio Caption Loss.
We also conducted a group of ablation experiments to validate
that our proposed metrics are effective.
FID. Fr´echet Inception Distance (FID) [15] is often used to
evaluate the ﬁdelity of the generated samples in the image
generation domain. Iashin et al. [3] also use FID as one
metric to evaluate the generated sound. FID is deﬁned by
the distance between the distributions of the pre-classiﬁcation
layer’s features of InceptionV3 [47] between fake and real
samples, and InceptionV3 is usually pre-trained on ImageNet
[48]. Considering the difference between images and spec-
trograms, we adapt the InceptionV3 architecture [47] for the
spectrogram and train the model on the Audioset dataset
[14]. Speciﬁcally, we modify the input convolutional layer and
change the channel from 3 to 1. We do not use the max pooling
operations to preserve spatial resolution at higher layers. We
train the InceptionV3 on the Audioset with a batch size of 64
mel-spectrograms using Adam optimizer, the learning rate is
3e − 3 with weight decay is 1e − 3.
KL. For the text-to-sound generation task, one important thing
is to evaluate the relevance between generated samples and
conditional text. Considering a sound comprises of multiple
events, we can use a pre-trained classiﬁer (pre-trained Incep-
tionV3 on the Audioset dataset) to get the probability of gen-
erated and real samples, and then calculate Kullback-Leibler
(KL) divergence between the two probability distributions.
Audio caption loss. Text-to-sound generation task can be
seen as a reverse audio caption [49], [50] task. Intuitively,
if the generated sample has high ﬁdelity and relevance with
text description s, the generated sample can be audio caption
system translated to ˆs, and the difference between s and ˆs
should be small. Thus we propose to use the gap between
s and ˆs as one metric for the text-to-sound generation task.
Speciﬁcally, we ﬁrst train an audio caption transformer (ACT)
[51] on Audiocaps dataset [8]. We follow the basic model
structure proposed in [51]. The difference is that we use a
860 × 80 log-mel spectrogram as input. Then we use SPICE
[52] and CIDEr [53], which are common evaluation metrics
for audio caption tasks, to evaluate the quality of generated
samples. The SPICE score guarantees the generated captions
are semantically faithful to the audio, while the CIDEr score
guarantees captions are syntactically ﬂuent. The higher SPICE
and CIDEr scores indicate better generation quality.

C. The Effectiveness of FID and KL

We try to validate whether the FID and KL scores can
measure the gap between the generated and real samples. To
realize this, we randomly choose a group of audio X o from
the AudioCaps [8] validation set and try to generate a new set
X f from three different aspects, respectively: add Gaussian

Fig. 3. FID and KL are evaluated for (a): add Gaussian noise, (b): mask
part of audio content, (c): mix with other interfere sound. The ﬁrst row is
the simple visualization of how the spectrograms changed when different
disturbance level is used. The disturbance level rises from zero and increases
to the highest level. We can see that The FID and KL scores capture the
disturbance level very well.

noise, mask part of the audio content, and add interfere sound.
Finally, we calculate the FID and KL scores between X o and
X f to verify whether the FID and KL metrics are sensitive to
these factors. The visualization results are shown in Figure 3.
Add Gaussian noise. Intuitively,
if the generated sample
contains too much noise, which may not be very well. Thus,
we add Gaussian noise to X o to generate a new set X f , and
then we calculate the FID and KL scores between X o and
X f . As Figure 3 (a) shows, the FID and KL scores gradually
increase when we add more noise, which indicates that FID
and KL metrics are sensitive to extra noise.
Mask part of audio content. If the generated sample only
contains part of the acoustic information compared to the real
sample, the generated sample is suboptimal. Thus, we mask
part of the audio content to generate a new set X f . As Figure
3 (b) shows, when we gradually increase the proportion of
masked parts, FID and KL scores also increase.
Add interfere sound. The generated sound should not contain
irrelevant acoustic information with the text description. We
randomly choose an interfere audio from the AudioCaps
training set, and then we directly mix the interfere sound with
X o to build a new set X f . According to Figure 3 (c), we can
see that the FID and KL scores gradually increase when the
interfering sound increases.

VI. EXPERIMENTAL SETUP

A. Dataset and Data Pre-processing

Audioset [14] and Audiocaps [8] dataset are used in our

experiments.

1) Audioset: An ontology comprising 527 sound classes is
used in the large-scale audio dataset known as Audioset. More
than 2 million 10-second audio snippets from YouTube videos
are included in the Audioset collection. There are roughly 19M
audio clips in our training set because some audio clips can
no longer be downloaded. Each audio clip may have one or
more labels for the presented audio events.

2) Audiocaps: Audiocaps is the largest audio captioning
dataset currently available with around 50k audio clips sourced

                         (a)                                                                 (b)                                                                   (c)                                                                 JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021

9

TABLE I
THE MEAN OPINION SCORE COMPARASION BETWEEN AR AND
DIFFSOUND MODELS. GT DENOTES THE GROUND TRUTH SOUND.

Decoder
GT
AR
Diffsound

Relevance↑
4.307
2.747
3.833

Fidelity↑
4.167
2.7
3.487

Intelligibility↑ MOS↑
4.116
2.786
3.56

3.873
2.913
3.36

from AudioSet. Audiocaps includes three sets: training, vali-
dation, test sets. There are 49256, 494, and 957 audio clips
in our training, validation and test sets. Each audio clip in the
training set contains one human-annotated caption, while each
contains ﬁve captions in the validation and test set. We use the
Audiocaps training set to train our models. We evaluate our
methods on the Audiocaps validation set.

3) Data pre-processing: All audio clips in the two datasets
are converted to 22.05k Hz and padded to 10-second long.
Log mel-spectrograms extracted using a 1024-points Hanning
window with 256-points hop size and 80 mel bins are used
as the input features. Finally, we can extract a 860 × 80 mel-
spectrogram from 10-second audio.

B. Implementation Details

Our proposed text-to-sound generation framework is not
end-to-end. We train each part separately. For text encoder, we
directly use the pre-trained BERT or CLIP models. We ﬁrst
train VQ-VAE and vocoder. Then we train the decoder with
the help of the text encoder and pre-trained VQ-VAE. Note
that the VQ-VAE and text encoder are ﬁxed when we train
the decoder. In the following, we will introduce the details of
network structure and training strategy.

1) VQ-VAE:

In this study, our VQ-VAE’s encoder Evq,
decoder G, and discriminator D follow the setting of VQ-
GAN [3], [24], which is a variant version of VQ-VAE [7]. For
codebook Z, the dimension of each vector nz is set as 256, and
the codebook size K is set as 256. VQ-VAE converts 860×80
spectrogram into 53 × 5 tokens. We train our VQ-VAE model
on Audiocaps and Audioset datasets, respectively. We ﬁnd that
training on Audioset can achieve better performance. Unless
speciﬁcally stated, we default to using the VQ-VAE pre-trained
on Audioset. The learning rate is ﬁxed and determined as a
product of a base learning rate, a number of GPUs, and a
batch size. In our experiments, the base learning rate is set as
1e − 6, and Adam optimizer [54] (betas are (0.5,0.9)) is used.
We train VQ-VAE with batches of 20 mel-spectrograms on 8
Nvidia V100 GPUs. The training takes about 9 days on the
Audioset. To stabilize the training procedure, we zero out the
adversarial part of the loss in formula (3) for the ﬁrst 2 training
epochs, after that the adversarial loss is used, and λd = 0.8.
2) Autoregressive decoder: Inspired by the success in au-
toregressive sound generation [3], [4], we follow the SOTA
backbone of the video-to-sound generation task, and employ
a transformer-based network to learn the mapping from text
to the spectrogram tokens. Speciﬁcally,
the autoregressive
decoder is a 19-layer 16-head transformer with a dimension
of 1024. To utilize the text features, we transform it into
the transformer’s hidden dimension space (1024) using one

TABLE II
THE OBJECTIVE METRICS COMPARISON BETWEEN AR DECODER AND
DIFFSOUND. CB DENOTES THAT WE TRAIN THE CODEBOOK ON AUDIOSET
OR AUDIOCAPS (CAPS FOR SHORT) DATASETS. TE DENOTES THE TYPE OF
TEXT ENCODER.

Decoder

AR

Diffsound

CB
Caps
Caps
Audioset
Caps
Audioset

TE
BERT
CLIP
CLIP
CLIP
CLIP

FID↓
18.01
17.94
16.87
13.47
9.76

KL↓
6.8
5.98
5.31
4.95
4.21

SPICE↑
0.055
0.082
0.088
0.093
0.103

CIDEr↑
0.1
0.2
0.22
0.28
0.36

dense layer and then input the transformer. The output of the
transformer is passed through a K-way softmax classiﬁer. The
base learning rate is 1e − 6, and the AdamW optimizer [55]
is used. The batch size is set as 16 for each GPU. The model
is trained until the loss on the validation set has not improved
for 2 consecutive epochs. Training the autoregressive decoder
takes about 2 days on 8 Nvidia P40 GPUs.

3) Diffsound: For a fair comparison with the autoregressive
decoder under similar parameters, we also built a 19-layer 16-
head transformer with a dimension of 1024 for the Diffsound.
Each transformer block contains a full attention, a cross
attention to combine text features and a feed-forward network
block. The current timestep t is injected into the network with
Adaptive Layer Normalization [56](AdaLN) operator.
Noise schedule setting. For the uniform transition matrix, we
linearly increase βt from 0 to 0.1, and decrease αt from 1
to 0. For the mask transition matrix, we linearly increase γt
from 0 to 1, and decrease βt from 1 to 0. For the mask and
uniform transition matrix, we linearly increase γt and βt from
0 to 0.9 and 0.1, and decrease αt from 1 to 0.
Training details. For the default setting, we set timesteps
T = 100 and loss weight λ = 1e − 4 in formula (23). The
mask and uniform transition matrix is used, because we ﬁnd
it can get the best generation performance. We optimize our
network using AdamW [55] with β1 = 0.9 and β2 = 0.94.
The basic learning rate is 3e − 6, and batch size is 16 for
each GPU. We train Diffsound on the Audiocaps, which takes
about 2 days on 16 Nvidia V100 GPUs (training epoch is set
as 400). If we pre-train Diffsound on the Audioset, we use 32
Nvidia V100 GPUs, which takes about 8 days (training epoch
is set as 200).

4) Vocoder.: We rely on the ofﬁcial implementation of the
MelGAN [6]. During training, the model inputs a random
sequence of 8192 audio samples (the sample rate is 22050).
The vocoder is trained for 200 epochs with a batch size of 256
mel-spectrograms on one P40 GPU for approximately 20 days
on the Audioset dataset. Considering the time complexity,
we do not use all of the Audioset data, we randomly choose
40% audio clips to train the MelGAN.

5) The duration of the generated sound: Consider that the
Audiocaps dataset contains 10 seconds of audio. We ﬁxed the
duration of the generated sound to 10 seconds to ensure a
fair comparison of generated and real sound. As a result, the
number of generated mel-spectrogram tokens is ﬁxed at 265
for both the AR decoder and Diffsound.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021

10

TABLE III
THE IMPACT OF THE THREE DIFFERENT TRANSITION MATRICES FOR THE
DIFFSOUND.

TABLE V
ABLATION STUDY ON TRAINING AND INFERENCE STEPS. EACH COLUMN
USES THE SAME TRAINING STEPS WHILE EACH ROW USES THE SAME
INFERENCE STEPS. WE ONLY REPORT THE FID IN THIS TABLE.

Decoder

Diffsound

FID↓
Matrices
Uniform 10.14
11.5
9.76

Mask
Both

KL↓
4.39
4.46
4.21

SPICE↑
0.101
0.103
0.103

CIDEr↑
0.349
0.342
0.359

TABLE IV
ABLATION STUDY ON THE EFFECT OF THE FINAL MASK RATE γT ON
MASK AND UNIFORM MATRIX.

Decoder

Diffsound

Mask rate γT
0
0.1
0.3
0.5
0.7
0.9
1

FID↓
10.14
10.63
10.64
10.75
9.84
9.76
11.5

KL↓
4.31
4.47
4.35
4.31
4.37
4.21
4.46

SPICE↑
0.101
0.093
0.099
0.096
0.102
0.103
0.103

CIDEr↑
0.35
0.29
0.34
0.32
0.34
0.36
0.34

VII. RESULTS AND ANALYSIS

In this section, we conduct experiments to verify the effec-
tiveness of our text-to-sound generation framework. Table I
shows the MOS comparison between the generated and real
sound. We can see that our text-to-sound generation framework
could achieve good performance regardless of whether the
decoder is AR or Diffsound. Let’s start with a detailed com-
parison between the AR decoder and our proposed Diffsound.
Then we conduct ablation studies for the Diffsound.

A. The comparison between the AR decoder and Diffsound

1) Subjective and objective metrics: Table I shows the
subjective metrics (MOS) comparison between the AR decoder
and Diffsound. We can see that our Diffsound signiﬁcantly
improves the MOS compared to the AR decoder, e.g., MOS
3.56 v.s 2.786. By comparing the MOS of Ground Truth and
that of Diffsound, we see that our method gets comparable
results in relevance and intelligibility aspects. Due to many
audio clips including background noise, the intelligibility of
Ground Truth is relatively low. Table II shows the objec-
tive metrics comparison between the AR decoder and the
Diffsound. Firstly, by comparing rows 1 and 2, we can see
that using the CLIP model as the text encoder brings better
generation performance than the BERT model. Secondly, we
can see that using the VQ-VAE trained on the Audioset dataset
brings better generation quality than on the Audiocaps dataset,
for the reason that training on a large-scale dataset improves
the ability of VQ-VAE. Lastly, by comparing the AR decoder
and Diffsound, we can see that
the Diffsound gets better
performance on all of the metrics, e.g., FID 9.76 v.s 16.87, KL
4.21 v.s 5.31, CIDEr 0.36 v.s 0.22. In summary, the objective
and subjective metrics both indicate the effectiveness of our
Diffsound.

2) The generation speed: Generation speed is also an im-
portant metric to evaluate the generation model. To investigate
the generation speed between AR decoder and Diffsound, we
conducted a group of ablation experiments, and the results are
shown in Table VI. Note that we conducted these experiments
on a single Nvidia P40 GPU. We ﬁx the generated sound

Inference
step

25
50
100

Training step
25
12.22
-
-

50
11.58
11.08
-

100
10.91
10.48
9.76

The visualization of generated samples by the Diffsound and
Fig. 4.
AR decoder. The ﬁrst line is the text input. The second line is the mel-
spectrograms of real audio. The last two lines are the mel-spectrograms of
generated audio by Diffsound and AR decoder respectively.

duration as 10-second. We only calculate the time to generate
the mel-spectrograms and ignore the vocoder’s costs. Firstly,
we can see that using the AR decoder to generate a mel-
spectrogram needs about 23-second, but using our Diffsound
only takes about 5-second. Moreover, our Diffsound also has
better generation performance. Secondly, the generation speed
of our Diffsound can be further improved by using smaller
timesteps T and larger time stride ∆t but decreases the
generation quality. The fastest generation speed is obtained
when timesteps T = 25 and ∆t = 7. The Diffsound only
needs 0.53 seconds to generate a mel-spectrogram, which is
43 times faster than the AR decoder with a similar FID score.
3) Visualization: Figure 4 shows some generated samples
by the Diffsound and AR decoder. We can see that the Diff-
sound can generate a scene with complete semantics compared
to the AR decoder, e.g., Figure 4 (a) shows that the sound
generated by AR decoder only includes the man speaks event
and the crickets sing is missing. Furthermore, as Figure 4
(b) and (c) show, our Diffsound has better detailed modelling
ability than the AR decoder.

B. Ablation study for Diffsound

1) Impact of different transition matrices for Diffsound: In
this section, we explore the impact of three transition matri-
ces on Diffsound: uniform transition matrix, mask transition
matrix, mask and uniform transition matrix. Table III presents

A man speaks as crickets singA person is snoring while sleepingBirds and insects make noise during the daytimeText OriginalDiffsoundMan speaksCrickets sing    Snoring    Birds and insects sing                         (a)                                                               (b)                                                             (c)                                                          ARJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021

11

TABLE VI
THE GENERATION SPEED COMPARISON BETWEEN THE AR DECODER AND DIFFSOUND. TIMESTEPS T DENOTES THE START STEP IN INFERENCE STAGE.
TIME STRIDE ∆t > 1 INDICATES THAT WE USE FAST INFERENCE STRATEGY.

Decoder
AR

Timestep (T )
-

Diffusion

25

50

100

Time stride ∆t
-
7
5
3
1
7
5
3
1
7
5
3
1

FID (↓)
16.87
16.76
16.51
12.54
10.91
15.26
14.04
11.06
10.48
11.87
12.71
10.13
9.76

KL (↓)
5.31
4.68
4.7
4.38
4.2
4.66
4.54
4.25
4.24
4.35
4.44
4.3
4.21

SPICE (↑)
0.054
0.088
0.088
0.099
0.104
0.094
0.092
0.102
0.104
0.103
0.095
0.103
0.102

CIDEr (↑)
0.16
0.25
0.26
0.32
0.34
0.29
0.28
0.32
0.35
0.34
0.30
0.35
0.36

Speed(spec/s) (↓)
23.24
0.53
0.59
0.72
1.49
0.67
0.82
1.15
2.77
1.02
1.28
1.86
4.96

TABLE VII
THE EFFECTIVENESS OF PRE-TRAINING THE DIFFSOUND ON AUDIOSET.
CL DENOTES THE CURRICULUM LEARNING STRATEGY.

Decoder

Pretrain

CL

Diffsound

(cid:88)
(cid:88)

(cid:88)

FID↓
9.76
8.78
8.27

KL↓
4.21
4.15
4.11

SPICE↑
0.103
0.101
0.105

CIDEr↑
0.36
0.35
0.36

the results of Diffsound with three different transition matrices.
We can see that the best results are obtained when the mask
and uniform matrix is used. The experimental results verify
the effectiveness of the mask and uniform transition matrix:
Only using a uniform transition matrix brings the problem
of semantic change, which leads to the reverse process being
hard to learn; Only using the mask transition matrix makes the
model tend to focus on the mask token and ignore the context
information.

2) Impact of the ﬁnal mask rate γT for the mask and
uniform transition matrix: We conduct ablation studies to
investigate the impact of the ﬁnal mask rate (γT ) on the
mask and uniform transition matrix. Note that when γT = 0,
the mask and uniform transition matrix can be seen as a
uniform transition matrix, and when γT = 1, the mask and
uniform transition matrix can be seen as a mask transition
matrix. From Table IV, we see that the best performance
is obtained when γT = 0.9. If γT is too large (more than
0.9), the Diffsound may pay more attention to the mask token
and ignore the context information, so that the performance
decreases. Instead, if γT is too small (less than 0.1), the
Diffsound also cannot perform well.

3) Number of timesteps: We conduct ablation studies to
investigate the impact of the timesteps T of the training and
inference stages for the Diffsound, with the results shown
in Table V. In this study, considering the generation speed,
we set the maximal timesteps T as 100 in both training and
inference. We can see that using larger timesteps in the training
and inference stage could get better performance, but it costs
larger time complexity. Furthermore, we can ﬁnd that it still
maintains a good performance when dropping 75 inference
steps (e.g., training step is 100, but inference step is 25), which
gives us a direction to boost the generation speed.

C. The effectiveness of pre-training the Diffsound on Audioset

In this section, we validate whether pre-training the Diff-
sound on the Audioset dataset can improve the generation
performance. Note that considering the time complexity, we
only use about 45% Audioset training set. Table VII shows
the experimental results. We can see that using the pre-trained
Diffsound can improve the generation performance, e.g., the
score of FID and KL from 9.76 and 4.21 to 8.78 and 4.15.
Furthermore, the best performance was obtained when our
proposed curriculum learning strategy is used, e.g., FID score:
8.27, KL score: 4.11. We believe that the performance can be
further improved when we use more data.

D. Choose high-relevance sound with text description based
on audio caption loss

Due to the random sample process in the inference stage
of the Diffsound and AR decoder, the generated sound may
be different in multiple sampling processes even using the
same text description. We generated 10 samples for each text
in this study. To quickly choose high-relevance samples with
the input text, we can rank the samples according to the sum
of their SPICE and CIDEr scores, and then we only keep
part of them. As Table VIII shows, if we only keep the top 2
samples for each text, the AR method’s KL score will improve
from 5.31 to 5.03, and the Diffsound method’s KL score will
improve from 4.21 to 3.86. We conjecture that this strategy can
help us quickly choose high-relevance samples with the text
description. Furthermore, we also observe that the FID score
of the Diffsound will slightly decrease when top k from 10 to
2. We think one of the reasons is that the Diffsound generates
some high-ﬁdelity samples but these samples are irrelevant to
the text description.

VIII. CONCLUSION

In this work, we present a framework for text-to-sound
generation tasks, and propose a novel non-autoregressive de-
coder (Diffsound) based on discrete diffusion model, which
signiﬁcantly improves the generation performance and speed
compared to the AR decoder. We also explore a simple
pre-training strategy to further improve the performance of
Diffsound. To effectively evaluate the quality of generated

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021

12

TABLE VIII
CHOOSE HIGH-RELEVANCE SOUND WITH TEXT DESCRIPTION BASED ON
AUDIO CAPTION LOSS. TOP K DENOTES THAT WE KEEP THE TOP K
SAMPLES ACCORDING TO THE SPICE AND CIDER SCORES.

Decoder

AR

Diffsound

Top k
10
5
2
10
5
2

FID↓
16.87
16.28
15.72
9.76
9.83
10.14

KL↓
5.31
5.22
5.03
4.21
4.03
3.86

SPICE↑
0.088
0.094
0.145
0.103
0.164
0.218

CIDEr↑
0.22
0.26
0.41
0.36
0.52
0.71

samples, we designed three objective evaluation metrics for
this task. Both objective and subjective metrics veriﬁed the
effectiveness of Diffsound.

This work still has some limitations that need to be ad-
dressed in our future work, e.g., our generation framework is
not end-to-end, we separately train the VQ-VAE, the decoder,
and the vocoder, which may not be optimal. In the future, we
will explore an end-to-end sound generation framework.

APPENDIX A
THE PROOF OF FORMULA (13)

We use mathematical induction to prove formula (13). We

have following conditional information:

βt ∈ [0, 1], αt = 1 − Kβt, αt =

t
(cid:89)

αi, βt = (1 − αt)/K

(24)
Now we want to prove that Qtc(x0) = αtc(x0) + βt. Firstly,
when t = 1, we have:

i=1

Q1c(x0) =

(cid:26) α1 + β1, x = x0
x (cid:54)= x0

β1,
which is clearly hold. Suppose the formula (13) is hold at step
t, then for t = t + 1, we have:

(25)

Qt+1c(x0) = Qt+1Qtc(x0)

Now we consider two conditions:
(1) when x = x0 in step t + 1, we have two situations in step
t, that is, x = x0 and x (cid:54)= x0, so that we have:
Qt+1c(x0)(x) = (αt + βt)(αt+1 + βt+1) + (K − 1)βtβt+1

= αt+1 + αtβt+1 + βtαt+1 + Kβtβt+1
= αt+1 + αtβt+1 + βtαt+1 + (1 − αt)βt+1

= αt+1 +

(1 − αt)
K
= αt+1 + βt+1

αt+1 +

(1 − αt+1)
K

(26)
(2) when x (cid:54)= x0 in step t + 1, we also have two situations in
step t, that is, x = x0 and x (cid:54)= x0, so that we have:
Qt+1c(x0)(x) = βt(αt+1 + βt+1) + βtβt+1(K − 1) + αtβt+1

= βt(αt+1 + βt+1 + βt+1(K − 1)) + αtβt+1
= βt + αtβt+1
(1 − αt)
K
= βt+1

αt(1 − αt+1)
K

+

=

(27)

So the proof done.

APPENDIX B
THE PROOF OF FORMULA (16)

We also use mathematical induction to prove formula (16).

We have following conditional information:

γt ∈ [0, 1], βt = 1 − γt, γt = 1 − βt, βt =

t
(cid:89)

i=1

βi.

(28)

Now we want to prove that Qtc(x0) = βtc(x0)+γtc(K +1).
Firstly, when t = 1, we have:

Q1c(x0) =

(cid:26) β1,

x = x0
γ1, x = K + 1

(29)

which is clearly hold. Suppose the formula (16) is hold at step
t, then for t = t + 1, we have:

Qt+1c(x0) = Qt+1Qtc(x0)

Now we consider two conditions:
(1) when x = x0 in step t + 1, we only have one situation in
step t, that is, x = x0, so that we have:

Qt+1c(x0)(x) = βt+1βt

= βt+1

(30)

(2) when x = K + 1 in step t + 1, we have two situations in
step t, that is, x = x0 and x = K + 1, so that we have:

Qt+1c(x0)(x) = γt + βtγt+1

= 1 − βt + βtγt+1
= 1 − βt(1 − γt+1)
= 1 − βt+1
= γt+1

(31)

So the proof done.

REFERENCES

[1] P. Chen, Y. Zhang, M. Tan, H. Xiao, D. Huang, and C. Gan, “Generating
visually aligned sound from videos,” IEEE Transactions on Image
Processing, vol. 29, pp. 8292–8302, 2020.

[2] Y. Zhou, Z. Wang, C. Fang, T. Bui, and T. L. Berg, “Visual to sound:
Generating natural sound for videos in the wild,” in Proceedings of
the IEEE Conference on Computer Vision and Pattern Recognition,
pp. 3550–3558, 2018.

[3] V. Iashin and E. Rahtu, “Taming visually guided sound generation,” in

British Machine Vision Conference (BMVC), 2021.

[4] X. Liu, T. Iqbal, J. Zhao, Q. Huang, M. D. Plumbley, and W. Wang,
“Conditional sound generation using neural discrete time-frequency
representation learning,” in IEEE International Workshop on Machine
Learning for Signal Processing (MLSP), pp. 1–6, IEEE, 2021.

[5] Q. Kong, Y. Xu, T. Iqbal, Y. Cao, W. Wang, and M. D. Plumbley,
“Acoustic scene generation with conditional samplernn,” in IEEE In-
ternational Conference on Acoustics, Speech and Signal Processing
(ICASSP), pp. 925–929, IEEE, 2019.

[6] K. Kumar, R. Kumar, T. de Boissiere, L. Gestin, W. Z. Teoh, J. Sotelo,
A. de Br´ebisson, Y. Bengio, and A. C. Courville, “Melgan: Generative
adversarial networks for conditional waveform synthesis,” Advances in
Neural Information Processing Systems, vol. 32, 2019.

[7] A. Van Den Oord, O. Vinyals, et al., “Neural discrete representation
learning,” Advances in Neural Information Processing Systems, vol. 30,
2017.

[8] C. D. Kim, B. Kim, H. Lee, and G. Kim, “Audiocaps: Generating
captions for audios in the wild,” in Proceedings of the Conference
of the North American Chapter of the Association for Computational
Linguistics: Human Language Technologies, Volume 1 (Long and Short
Papers), pp. 119–132, 2019.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021

13

[9] H. Chen, W. Xie, A. Vedaldi, and A. Zisserman, “Vggsound: A
large-scale audio-visual dataset,” in IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP), pp. 721–725, IEEE,
2020.

[10] J. Ho, A. Jain, and P. Abbeel, “Denoising diffusion probabilistic models,”
Advances in Neural Information Processing Systems, vol. 33, pp. 6840–
6851, 2020.

[11] J. Sohl-Dickstein, E. Weiss, N. Maheswaranathan, and S. Ganguli,
“Deep unsupervised learning using nonequilibrium thermodynamics,” in
International Conference on Machine Learning, pp. 2256–2265, PMLR,
2015.

[12] J. Austin, D. Johnson, J. Ho, D. Tarlow, and R. van den Berg, “Structured
denoising diffusion models in discrete state-spaces,” Advances in Neural
Information Processing Systems, vol. 34, 2021.

[13] S. Gu, D. Chen, J. Bao, F. Wen, B. Zhang, D. Chen, L. Yuan, and
B. Guo, “Vector quantized diffusion model for text-to-image synthesis,”
arXiv preprint arXiv:2111.14822, 2021.

[14] J. F. Gemmeke, D. P. Ellis, D. Freedman, A. Jansen, W. Lawrence, R. C.
Moore, M. Plakal, and M. Ritter, “Audio set: An ontology and human-
labeled dataset for audio events,” in IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP), pp. 776–780, IEEE,
2017.

[15] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter,
“Gans trained by a two time-scale update rule converge to a local
nash equilibrium,” Advances in Neural Information Processing Systems,
vol. 30, 2017.

[16] S. Reed, Z. Akata, X. Yan, L. Logeswaran, B. Schiele, and H. Lee,
“Generative adversarial text to image synthesis,” in International Con-
ference on Machine Learning, pp. 1060–1069, PMLR, 2016.

[17] Z. Zhang, Y. Xie, and L. Yang, “Photographic text-to-image synthesis
with a hierarchically-nested adversarial network,” in Proceedings of
the IEEE Conference on Computer Vision and Pattern Recognition,
pp. 6199–6208, 2018.

[18] A. Brock, J. Donahue, and K. Simonyan, “Large scale gan training for
high ﬁdelity natural image synthesis,” arXiv preprint arXiv:1809.11096,
2018.

[19] S.-H. Lee, H.-W. Yoon, H.-R. Noh, J.-H. Kim, and S.-W. Lee, “Multi-
spectrogan: High-diversity and high-ﬁdelity spectrogram generation with
adversarial style combination for speech synthesis,” arXiv preprint
arXiv:2012.07267, 2020.

[20] H. Guo, F. K. Soong, L. He, and L. Xie, “A new gan-based end-to-end
tts training algorithm,” arXiv preprint arXiv:1904.04775, 2019.
[21] J. Nistal, S. Lattner, and G. Richard, “Drumgan: Synthesis of drum
sounds with timbral feature conditioning using generative adversarial
networks,” arXiv preprint arXiv:2008.12073, 2020.

[22] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal,
A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al., “Language mod-
els are few-shot learners,” Advances in Neural Information Processing
Systems, vol. 33, pp. 1877–1901, 2020.

[23] A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever, “Improving

language understanding by generative pre-training,” 2018.

[24] P. Esser, R. Rombach, and B. Ommer, “Taming transformers for high-
resolution image synthesis,” in Proceedings of the IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition, pp. 12873–12883,
2021.

[25] M. Ding, Z. Yang, W. Hong, W. Zheng, C. Zhou, D. Yin, J. Lin, X. Zou,
Z. Shao, H. Yang, et al., “Cogview: Mastering text-to-image generation
via transformers,” Advances in Neural Information Processing Systems,
vol. 34, 2021.

[26] N. Parmar, A. Vaswani, J. Uszkoreit, L. Kaiser, N. Shazeer, A. Ku, and
D. Tran, “Image transformer,” in International Conference on Machine
Learning, pp. 4055–4064, PMLR, 2018.

[27] A. Razavi, A. Van den Oord, and O. Vinyals, “Generating diverse
high-ﬁdelity images with vq-vae-2,” Advances in Neural Information
Processing Systems, vol. 32, 2019.

[28] A. Ramesh, M. Pavlov, G. Goh, S. Gray, C. Voss, A. Radford, M. Chen,
and I. Sutskever, “Zero-shot text-to-image generation,” in International
Conference on Machine Learning, pp. 8821–8831, PMLR, 2021.
[29] A. Ramesh, P. Dhariwal, A. Nichol, C. Chu, and M. Chen, “Hierarchical

text-conditional image generation with clip latents,” 2022.

[30] Y. Wang, R. Skerry-Ryan, D. Stanton, Y. Wu, R. J. Weiss, N. Jaitly,
Z. Yang, Y. Xiao, Z. Chen, S. Bengio, et al., “Tacotron: Towards end-
to-end speech synthesis,” arXiv preprint arXiv:1703.10135, 2017.
[31] P. Esser, R. Rombach, A. Blattmann, and B. Ommer, “Imagebart: Bidi-
rectional context with multinomial diffusion for autoregressive image
synthesis,” Advances in Neural Information Processing Systems, vol. 34,
2021.

[32] P. Dhariwal and A. Nichol, “Diffusion models beat gans on image
synthesis,” Advances in Neural Information Processing Systems, vol. 34,
2021.

[33] A. Nichol, P. Dhariwal, A. Ramesh, P. Shyam, P. Mishkin, B. McGrew,
I. Sutskever, and M. Chen, “Glide: Towards photorealistic image gen-
eration and editing with text-guided diffusion models,” arXiv preprint
arXiv:2112.10741, 2021.

[34] Z. Kong, W. Ping, J. Huang, K. Zhao, and B. Catanzaro, “Dif-
fwave: A versatile diffusion model for audio synthesis,” arXiv preprint
arXiv:2009.09761, 2020.

[35] M. Jeong, H. Kim, S. J. Cheon, B. J. Choi, and N. S. Kim, “Diff-
tts: A denoising diffusion model for text-to-speech,” arXiv preprint
arXiv:2104.01409, 2021.

[36] V. Popov, I. Vovk, V. Gogoryan, T. Sadekova, and M. Kudinov, “Grad-
tts: A diffusion probabilistic model for text-to-speech,” in International
Conference on Machine Learning, pp. 8599–8608, PMLR, 2021.
[37] S.-g. Lee, H. Kim, C. Shin, X. Tan, C. Liu, Q. Meng, T. Qin, W. Chen,
S. Yoon, and T.-Y. Liu, “Priorgrad: Improving conditional denois-
ing diffusion models with data-driven adaptive prior,” arXiv preprint
arXiv:2106.06406, 2021.

[38] E. Hoogeboom, D. Nielsen, P. Jaini, P. Forr´e, and M. Welling, “Argmax
ﬂows and multinomial diffusion: Towards non-autoregressive language
models,” arXiv preprint, 2021.

[39] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training
of deep bidirectional transformers for language understanding,” arXiv
preprint arXiv:1810.04805, 2018.

[40] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal,
G. Sastry, A. Askell, P. Mishkin, J. Clark, et al., “Learning transferable
language supervision,” in International
visual models from natural
Conference on Machine Learning, pp. 8748–8763, PMLR, 2021.
[41] X. Tan, T. Qin, F. Soong, and T.-Y. Liu, “A survey on neural speech

synthesis,” arXiv preprint arXiv:2106.15561, 2021.

[42] P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros, “Image-to-image translation
with conditional adversarial networks,” in Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition, pp. 1125–
1134, 2017.

[43] D. Grifﬁn and J. Lim, “Signal estimation from modiﬁed short-time
fourier transform,” IEEE Transactions on Acoustics, Speech, and Signal
Processing, vol. 32, no. 2, pp. 236–243, 1984.

[44] A. v. d. Oord, S. Dieleman, H. Zen, K. Simonyan, O. Vinyals, A. Graves,
N. Kalchbrenner, A. Senior, and K. Kavukcuoglu, “Wavenet: A gener-
ative model for raw audio,” arXiv preprint arXiv:1609.03499, 2016.

[45] J. Kong, J. Kim, and J. Bae, “Hiﬁ-gan: Generative adversarial networks
for efﬁcient and high ﬁdelity speech synthesis,” Advances in Neural
Information Processing Systems, vol. 33, pp. 17022–17033, 2020.
[46] A. Q. Nichol and P. Dhariwal, “Improved denoising diffusion prob-
abilistic models,” in International Conference on Machine Learning,
pp. 8162–8171, PMLR, 2021.

[47] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, “Rethinking
the inception architecture for computer vision,” in Proceedings of
the IEEE Conference on Computer Vision and Pattern Recognition,
pp. 2818–2826, 2016.

[48] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “Imagenet:
image database,” in IEEE conference on

A large-scale hierarchical
computer vision and pattern recognition, pp. 248–255, Ieee, 2009.
[49] M. Wu, H. Dinkel, and K. Yu, “Audio caption: Listen and tell,” in
ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech
and Signal Processing (ICASSP), pp. 830–834, IEEE, 2019.

[50] K. Drossos, S. Lipping, and T. Virtanen, “Clotho: An audio captioning
dataset,” in IEEE International Conference on Acoustics, Speech and
Signal Processing (ICASSP), pp. 736–740, IEEE, 2020.

[51] X. Mei, X. Liu, Q. Huang, M. D. Plumbley, and W. Wang, “Audio
captioning transformer,” arXiv preprint arXiv:2107.09817, 2021.
[52] P. Anderson, B. Fernando, M. Johnson, and S. Gould, “Spice: Semantic
propositional image caption evaluation,” in European Conference on
Computer Vision, pp. 382–398, Springer, 2016.

[53] R. Vedantam, C. Lawrence Zitnick, and D. Parikh, “Cider: Consensus-
based image description evaluation,” in Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition, pp. 4566–4575,
2015.

[54] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”

arXiv preprint arXiv:1412.6980, 2014.

[55] I. Loshchilov and F. Hutter, “Decoupled weight decay regularization,”

arXiv preprint arXiv:1711.05101, 2017.

[56] J. L. Ba, J. R. Kiros, and G. E. Hinton, “Layer normalization,” arXiv

preprint arXiv:1607.06450, 2016.

