2
2
0
2

l
u
J

4
1

]

R
P
.
h
t
a
m

[

1
v
9
6
0
7
0
.
7
0
2
2
:
v
i
X
r
a

MOMENT CONDITIONS FOR RANDOM COEFFICIENT
AR(∞) UNDER NON-NEGATIVITY ASSUMPTIONS

PASCAL MAILLARD AND OLIVIER WINTENBERGER

Abstract. We consider random coeﬃcient autoregressive models of in-
ﬁnite order (AR(∞)) under the assumption of non-negativity of the co-
eﬃcients. We develop novel methods yielding suﬃcient or necessary con-
ditions for ﬁniteness of moments, based on combinatorial expressions of
ﬁrst and second moments. The methods based on ﬁrst moments recover
previous suﬃcient conditions by Doukhan and Wintenberger [6] in our
setting. The second moment method provides in particular a necessary
and suﬃcient condition for ﬁniteness of second moments which is diﬀer-
ent, but shown to be equivalent to the classical criterion of Nicholls and
Quinn [11] in the case of ﬁnite order. We further illustrate our results
through two examples.

1. Introduction

Consider the recurrence

(1.1)

Xt =

∞
(cid:88)

j=1

At,jXt−j + Bt ,

t ∈ Z ,

where ((At,j)j, Bt) is an iid family of sequences of non-negative random vari-
ables. In the case where there exists p ∈ N such that A1,j = 0 for all j > p,
this recurrence is known in the literature under the name random coeﬃ-
cient autoregressive model of p-th order, symbolically AR(p) in Buraczewski,
Damek and Mikosch [4, Section 4.4.9], or also random diﬀerence equation
It is natural to call the general recurrence
of p-th order in Kesten [10].
the random coeﬃcient autoregressive model of inﬁnite order, symbolically
AR(∞).

Date: July 14, 2022.
2020 Mathematics Subject Classiﬁcation. Primary: 60G70, Secondary: 39A50, 60F25,

60G10, 60H25, 62M10.

Key words and phrases. random coeﬃcient autoregressive model, stochastic recurrence

equations, heavy tails, power-law tails, second moment method.

PM is aﬃliated to Institut de Math´ematiques de Toulouse (IMT), Universit´e de
Toulouse, CNRS UMR5219 and Institut Universitaire de France. Supported in part by
grants ANR-20-CE92-0010-01 (REMECO project) and ANR-11-LABX-0040 (ANR pro-
gram “Investissements d’Avenir”).

OW acknowledges support of the French Agence Nationale de la Recherche (ANR)

under reference ANR20-CE40-0025-01 (T-REX project).

1

 
 
 
 
 
 
2

PASCAL MAILLARD AND OLIVIER WINTENBERGER

The aim of this paper is to ﬁnd explicit necessary and suﬃcient conditions
for the existence of moments of the non-anticipative stationary solution of
(1.1) which admits the (possibly inﬁnite) expression
(cid:88)

˜At0,t1−t0 · · · ˜Atn−1,tn−tn−1B−tn.

(1.2)

˜X :=

0=t0<t1<···<tn, n(cid:62)0

where ˜At,j := A−t,j, t ∈ Z, j (cid:62) 1. We will always assume that the random
variable B satisﬁes P(B = 0) < 1 to avoid any degeneracy. Moreover we are
interested only in the cases where B has a lighter tail than ˜X. We refer to
Hult and Samorodnistky [9] for an analysis of the cases where B and ˜X are
tail equivalent.

The ﬁnite order case AR(p), p < ∞. Let us recall what is known in the
ﬁnite order case, following Kesten [10] and the recent book by Buraczewski,
Damek and Mikosch [4]. One typically reformulates the problem as a random
diﬀerence equation of ﬁrst order on p × p matrices: Deﬁne



At,1 At,2

· · · At,p

At =

1






. . .

1








,

Bt =








Bt
0
...
0








,

Xt =






Xt
...
Xt−p+1




 ,

with all non-appearing entries equal to zero in the deﬁnition of At. Then
(1.1) is equivalent to

(1.3)

Xt = AtXt−1 + Bt.
This equation has been studied by many authors starting from Kesten
[10]. Let us recall how it is typically solved. First, if one seeks stationary
solutions of (1.1) one needs to assume that the top Lyapounov exponent of
the matrix At is negative, i.e.,

(1.4)

lim
n→∞

1
n

log (cid:107)An · · · A1(cid:107) < 0,

a.s.,

where we can of course choose (cid:107) · (cid:107) to be any norm. Next, one deﬁnes the
following function [4, (4.4.35)]:

(1.5)

h(θ) = lim
n→∞

1
n

log E

(cid:104)

(cid:107)An · · · A1(cid:107)θ(cid:105)

(existence of the limit follows from a submultiplicativity argument). The
tail behavior of the stationary solution (1.2) depend on the form of this
function. The canonical case is the one where there exists α > 0, such that
h(α) = 0 and α(cid:48) > α such that h(α(cid:48)) < ∞. In this case, and under mild
assumptions on Bt, it is known that there exists a stationary solution X to
(1.3) that is power law tailed of order α, i.e. there exists C > 0 such that

P((cid:107)X(cid:107) > x) ∼ C x−α,

x → ∞ ,

see Buraczewski, Damek and Mikosch [4]. One easily deduces the moment
properties E[X θ] < ∞ for θ < α and E[X θ] = ∞ otherwise. In other words,

MOMENT CONDITIONS FOR RANDOM COEFFICIENT AR(∞)

3

using the convexity of he function h, the condition h(θ) < 0 is necessary and
suﬃcient for the existence of moments of order θ > 0 for the AR(p) model.
The aim of the paper is to extend such necessary and suﬃcient conditions
when p = ∞.

The root equation h(α) = 0 is impossible to solve for general stochastic
matrices At. Even the simpler top-Lyapunov condition (1.4) is rarely ex-
plicit. The speciﬁcity of the matrix At in the AR(p) model allows Nicholls
and Quinn [11], see also Buraczewski, Damek and Mikosch [4, Section 4.4.9],
to provide an explicit suﬃcient condition for the existence of a stationary
solution with ﬁnite second moment, in the case of centered coeﬃcients At,j
(which therefore does not include the case of non-negative coeﬃcients con-
sidered here). Denoting by ρ the spectral radius and ⊗ the tensor product,
the suﬃcient condition of (1.4) is

(1.6)

ρ(E[A ⊗ A]) < 1 .

Nicholls and Quinn [11, Corollary 2.2.2] also show that this assumption is
necessary and suﬃcient for having ﬁnite moments of order 2 when B has
ﬁnite moments of order 2 and is independent of A. Thus the condition
ρ(E[A ⊗ A]) < 1 is equivalent to the condition h(2) < 0. The advantage of
the former is its relative explicitness and we seek at ﬁnding such relatively
explicit conditions when p = ∞.

Back to AR(∞). Very few is known in the inﬁnite-order case p = ∞.
Doukhan and Wintenberger [6] studied non-linear processes with inﬁnite
memory, which includes the AR(∞) as a special case. For nonlinear pro-
cesses with inﬁnite memories satisfying

Xt = F (Xt−1, Xt−2, . . . ; ξt) ,

t ∈ Z ,

where the iid process (ξt) can be taken equal to ((At,j)j, Bt), they give
suﬃcient conditions for ﬁnite moments of order θ using a coupling approach
in Lθ, θ (cid:62) 1. In particular, the contraction condition (3.2) in Doukhan and
Wintenberger [6] turns into

˜φ1(θ) := log

∞
(cid:88)

j=1

E(cid:2)Aθ
j

(cid:3)1/θ < 0 ,

θ (cid:62) 1

for model (1.1). Thus, if ˜φ1(θ) < 0 and E[Bθ] < ∞ for some θ (cid:62) 1,
Theorem 3.1 in Doukhan and Wintenberger [6] ensures the existence of a
stationary solution X admitting ﬁnite moments of order θ. It seems that no
other explicit condition appears in the literature.

Relation with the smoothing transform. Equation (1.1) looks simi-
lar to an equation known in the literature as the ﬁxed point equation of

4

PASCAL MAILLARD AND OLIVIER WINTENBERGER

the (non-homogeneous) smoothing transform. This is the following distribu-
tional equation (we let the inhomogeneity be equal to 1 for simplicity):

(1.7)

Y d=

∞
(cid:88)

j=1

AjY (j) + 1,

where (Aj)j(cid:62)1 is a random inﬁnite vector of non-negative entries and (Y (j))j(cid:62)1
are iid copies of Y , independent of (Aj)j(cid:62)1. Deﬁne

φ1(θ) := log

∞
(cid:88)

j=1

E(cid:2)Aθ
j

(cid:3) ,

θ > 0 .

It is known that a solution to (1.7) exists if φ1(θ) < 0 for some θ ∈ (0, 1],
furthermore, this criterion is close to being optimal [4, Section 5.2.4]. In this
case, if there exists α (cid:62) θ such that φ1(α) = 0 and some non-arithmeticity
condition is met, then P(Y > x) ∼ Cx−α as x → ∞, for some constant
C ∈ (0, ∞). Note that the characteristic exponent α does not depend on
the dependencies between the Aj’s, only on the marginal laws.

Description of our results. Our ﬁrst result is an extension of the above-
mentioned suﬃciency condition from Doukhan and Wintenberger [6] to the
case θ (cid:54) 1. More precisely we prove that φ1(θ) < 0 is a suﬃcient condition
for E[X θ] < ∞ when 0 < θ (cid:54) 1. Then we show that φ1(θ) < 0 is a necessary
condition for E[X θ] < ∞ when θ (cid:62) 1. Finally we also show that ˜φ1(θ) < 0
is a necessary condition for E[X θ] < ∞ when 0 < θ < 1. Since the functions
φ1 and ˜φ1 coincide at θ = 1, we deduce that φ1(1) = ˜φ1(1) < 0 is a necessary
and suﬃcient condition for the existence of moments of order 1. We thus
obtain necessary conditions as well as suﬃcient conditions for any θ > 0,
and these are asymptotically sharp when θ is close to one. The special
role of θ = 1 is not surprising in view of linearity of the model and of the
expectation.

Our main contribution in this paper is a generalisation of these methods
to higher moments. Speciﬁcally, we obtain necessary conditions and suf-
ﬁcient conditions of moments which are better than the previous ones for
θ (cid:62) 2 and for some values of θ in the interval [1, 2], furthermore, they are
asymptotically sharp when θ is close to 2. To do so we introduce two other
functions φ2 and ˜φ2 that have the following properties (we omit some extra
conditions below to simplify the presentation):

• for 0 < θ < 2 the conditions φ2(θ) < 0 and ˜φ2(θ) < 0 are suﬃcient
and necessary, respectively, for the existence of moments E[X θ] < ∞,
• for θ > 2 the conditions φ2(θ) < 0 and ˜φ2(θ) < 0 are necessary and
suﬃcient, respectively, for the existence of moments E[X θ] < ∞,
• for θ = 2 the condition φ2(θ) = ˜φ2(θ) < 0 is necessary and suﬃcient

for the existence of moments E[X 2] < ∞

The statement above is only a partial summary omitting extra conditions
In particular, conditions on the functions φ1 and ˜φ1 are
in some cases.

MOMENT CONDITIONS FOR RANDOM COEFFICIENT AR(∞)

5

required for ensuring the existence of moments; see Theorem 3.1 for a precise
statement. We furthermore verify that this last criterion coincides with the
classical one of Nicholls and Quinn [11] in the case θ = 2, although it is
of a very diﬀerent form. Indeed our approach uses linearity together with
combinatorics on pairs instead of a matrix representation as the one of [11].

We mention that our approach also covers models of the form

(1.8)

Xt =

∞
(cid:88)

j=1

A(cid:48)

t−j,jXt−j + B(cid:48)

t−1 ,

t ∈ Z ,

t,j)j, B(cid:48)

where ((A(cid:48)
t) is an iid family of sequences of non-negative random
variables. The non-anticipative solution of (1.8) is a previsible process with
respect to the natural ﬁltration generated by (A(cid:48)
t. The volatility pro-
cess (σ2
t ) of a GARCH(1,1) model εt = Ztσt for iid (Zt) is a typical example.
It satisﬁes the recursion

t,j)j, B(cid:48)

t−1 ,

t ∈ Z .

t = ω + α1ε2
σ2

t−1 + β1σ2
After expanding, this turns into a recursion of inﬁnite order of the form (1.8).
We check that for the volatility process of a GARCH(1,1) model our con-
ditions provide necessary and suﬃcient conditions of ﬁnite order moments.
In particular we recover the optimal second order moment condition of the
GARCH(1,1) volatility using our approach. Note that inﬁnite memory mod-
els (AR and ARCH) are necessary for writing the invertible form (depending
only on the past observations εt−1, εt−2, . . .) of some ﬁnite memory models
such as ARMA or GARCH.

There is a natural extension of some our methods to higher moments of
order k (cid:62) 3. However, while this easily yields necessary criteria for the
ﬁniteness of moments, we were not able to obtain simple suﬃcient criteria
in that case. See Section 3.1 for details.

Finally, notice also that suﬃcient conditions for ﬁniteness of moments for
some non-linear models (such as Galton-Watson process with immigration,
see [3]) can be deduced from Theorem 3.1 using a stochastic domination
argument similar as in [6].

2. First moment – comparison to smoothing transform

Let ((Aj)j(cid:62)1, B) be a random sequence with non-negative entries. In the

remainder of the paper, we let (Xt)t(cid:62)0 be the solution to the equation

(2.9)

(cid:40)

Xt = (cid:80)∞
X0 = 1, X−1 = X−2 = · · · = 0,

j=1 At,jXt−j + Bt,

t (cid:62) 1

where the family ((At,j)j(cid:62)1)t∈Z satisﬁes one of the following two assump-
tions:
(A1) ((At,j)j(cid:62)1, Bt)t∈Z are iid copies of the random sequence ((Aj)j(cid:62)1, B),

or

6

PASCAL MAILLARD AND OLIVIER WINTENBERGER

(A2) At,j = A(cid:48)

t−j,j for any t ∈ Z and j (cid:62) 0, where ((A(cid:48)

t,j)j(cid:62)1)t∈Z are iid
copies of the random sequence (Aj)j(cid:62)1. Furthermore, (Bt)t∈Z are iid
copies of B, independent of (At,j)t∈Z,j(cid:62)1.

Note that under (A1) or (A2), the sequence of random vectors ((At,j)j(cid:62)1)t∈Z
is stationary in t. Under (A1) it is also reversible, however, under (A2) it
is not in general. We therefore deﬁne
˜At,j := A−t,j,

(2.10)
Remark 2.1. One readily checks that ˜At,j and ˜At(cid:48),j(cid:48) are independent if

t ∈ Z, j (cid:62) 1.

• t (cid:54)= t(cid:48) (under (A1)), or
• t + j (cid:54)= t(cid:48) + j(cid:48) (under (A2)),

with obvious generalizations for larger collections of r.v. Moreover, ˜At,j and
B−t(cid:48) are independent if t (cid:54)= t(cid:48) under (A1) and for all t, t(cid:48) under (A2).

The following result is the main result of this section.

Theorem 2.2. Let Xt be deﬁned by (2.9).

(1) As t → ∞, Xt converges in law to a (possibly inﬁnite) limit X.
(2) Assume φ1(θ) < 0 and E[Bθ] < ∞ for some θ ∈ (0, 1]. Then E[X θ] <

∞.

(3) Assume φ1(θ) (cid:62) 0 for some θ (cid:62) 1. Then E[X θ] = ∞.
(4) Assume ˜φ1(θ) < 0 and E[Bθ] < ∞ for some θ (cid:62) 1. Then E[X θ] <

∞.

(5) Assume ˜φ1(θ) (cid:62) 0 for some 0 < θ < 1. Then E[X θ] = ∞.

Remark 2.3. Consider the case θ = 1. By deﬁnition, φ1(1) = ˜φ1(1). By
Theorem 2.2, if E[B] < ∞, then φ1(1) = ˜φ1(1) < 0 is a necessary and
suﬃcient condition for E[X] < ∞. This condition rewrites explicitly as

∞
(cid:88)

j=1

E[Aj] < 1

and does not depend on the dependence among the Aj.
Proof. We begin by proving 1. Unravelling the recurrence (2.9), we get (with
the convention (cid:81)

∅ = 1)

Xt =

=

(cid:88)

Atn,tn−tn−1 · · · At1,t1−t0Bt0

0(cid:54)t0<t1<···<tn=t, n(cid:62)0
(cid:88)

0=t0<t1<···<tn(cid:54)t, n(cid:62)0

At−t0,t1−t0 · · · At−tn−1,tn−tn−1Bt0

Using stationarity in t and (2.10), we get

Xt

d=

(cid:88)

0=t0<t1<···<tn(cid:54)t, n(cid:62)0

=: ˜Xt.

˜At0,t1−t0 · · · ˜Atn−1,tn−tn−1B−tn

MOMENT CONDITIONS FOR RANDOM COEFFICIENT AR(∞)

7

By non-negativity of the A’s, the sequence ˜Xt is increasing in t and therefore
converges almost surely as t → ∞ to the (possibly inﬁnite) limit

(2.11)

˜X :=

(cid:88)

˜At0,t1−t0 · · · ˜Atn−1,tn−tn−1B−tn.

0=t0<t1<···<tn, n(cid:62)0

d= ˜Xt for every t, this shows that Xt converges in law to a (possibly

Since Xt
inﬁnite) limit X. Furthermore, X d= ˜X.

We now prove 2. showing that with θ from the assumption, i.e. θ ∈ (0, 1]
and φ1(θ) < 0, E[X θ] = E[ ˜X θ] < ∞ (note that this implies in particular
that X is ﬁnite almost surely). By subadditivity of the function x (cid:55)→ xθ, we
have

E[ ˜X θ] (cid:54)

(cid:88)

0=t0<t1<···<tn, n(cid:62)0

E[( ˜At0,t1−t0 · · · ˜Atn−1,tn−tn−1B−tn)θ].

By independence (see Remark 2.1),

E[ ˜X θ] (cid:54)

(cid:88)

E[Aθ

t1−t0] · · · E[Aθ

tn−tn−1]E[Bθ]

0=t0<t1<···<tn, n(cid:62)0
E[Aθ

(cid:88)

i1] · · · E[Aθ

in]E[Bθ]

=

i1,...,in(cid:62)1, n(cid:62)0

= E[Bθ]

∞
(cid:88)

(cid:32) ∞
(cid:88)

(cid:33)n

E[Aθ
i ]

i=1

enφ1(θ)

n=0
∞
(cid:88)

n=0

= E[Bθ]

< ∞,

since φ1(θ) < 0 by hypothesis. This ﬁnishes the proof of 2.

The proof of 3. is similar. Let θ (cid:62) 1 and φ1(θ) (cid:62) 0. By superadditivity

of the function x (cid:55)→ xθ, we have, similarly as above,

E[ ˜X θ] (cid:62) E[Bθ]

∞
(cid:88)

n=0

enφ1(θ) = ∞.

The assertion 4. follows from Doukhan and Wintenberger [6] in the cases
where (A1) is satisﬁed. We provide here an alternative proof that also works
under (A2). Using the Minkowski inequality (θ (cid:62) 1) and by independence

8

PASCAL MAILLARD AND OLIVIER WINTENBERGER

(see Remark 2.1),

E[ ˜X θ]1/θ (cid:54)

(cid:88)

E[(At1−t0 · · · Atn−tn−1B−tn)θ]1/θ

0=t0<t1<···<tn, n(cid:62)0
(cid:88)

(cid:54)

E[Aθ

t1−t0]1/θ · · · E[Aθ

tn−tn−1]1/θE[Bθ]1/θ

0=t0<t1<···<tn, n(cid:62)0
E[Aθ

(cid:54) (cid:88)

i1]1/θ · · · E[Aθ

in]1/θE[Bθ]1/θ

i1,...,in(cid:62)1, n(cid:62)0
∞
(cid:88)

(cid:54) E[Bθ]1/θ

n=0
∞
(cid:88)

n=0

(cid:54) E[Bθ]1/θ

< ∞,

(cid:33)n

E[Aθ

i ]1/θ

(cid:32) ∞
(cid:88)

i=1

en ˜φ1(θ)

since ˜φ1(θ) < 0 by hypothesis.

The proof of 5. is similar than the preceding one, replacing the Minkowski
inequality with the reverse Minkowski inequality holding for 0 < θ (cid:54) 1 and
non-negative random variables:

E[ ˜X θ]1/θ (cid:62)

(cid:88)

E[(At1−t0 · · · Atn−tn−1B−tn)θ]1/θ .

0=t0<t1<···<tn, n(cid:62)0

We conclude easily – this ﬁnishes the proof of 5.

(cid:3)

3. Second moment – a combinatorial formula

From this section on, we let X be the random variable from the statement
of Theorem 2.2 and ˜X the random variable from (2.11). We now calculate
the second moment E[X 2]. For this, it is useful to introduce some notation.
Let T denote the set of ﬁnite increasing integer-valued sequences 0 = t0 <
t1 < · · · < tn for some n (cid:62) 0. The trivial sequence is denoted by 0 = (0). For
t = (t0, . . . , tn) ∈ T , we write n(t) = n and ˜At = ˜At0,t1−t0 · · · ˜Atn−1,tn−tn−1,
with ˜A0 = 1 by convention. We also denote ˜Bt = B−tn. With this notation,
we have

(cid:88)

˜X =

˜At ˜Bt,

and hence

(3.12)

t∈T

˜X 2 =

(cid:88)

s,t∈T

˜As ˜Bs ˜At ˜Bt.

Deﬁne the concatenation of a ﬁnite number of sequences t1, . . . , tk by

t1 · · · tk := (t1

0, . . . , t1

n(t1), t1

n(t1) + t2

1, . . . , t1

n(t1) + t2

n(t2), . . . , t1

n(t1) + · · · + tk

n(tk))

MOMENT CONDITIONS FOR RANDOM COEFFICIENT AR(∞)

9

Now deﬁne the following two sets of pairs of elements in T :

C = {(s, t) ∈ T × T : n(s) > 0, n(t) > 0, sn(s) = tn(t),

si (cid:54)= tj, 0 < i < n(s), 0 < j < n(t)}

O = {(s, t) ∈ T × T : ∀0 < i (cid:54) n(s), 0 < j (cid:54) n(t) : si (cid:54)= tj}.
We call the pairs in C and O closed and open, respectively. Note that the
trivial pair is open by deﬁnition: (0, 0) ∈ O.
It is easy to see that any
pair (s, t) ∈ T × T can be written as a concatenation of a ﬁnite number of
closed pairs and an open pair: for every (s, t) ∈ T × T there exists k ∈ N0,
(s1, t1), . . . , (sk, tk) ∈ C and (so, to) ∈ O, such that

(s, t) = (s1 · · · skso, t1 · · · tkto).

Furthermore, this decomposition is unique. See Figure 1 for an illustration.

Figure 1. Schematic illustration of the decomposition of a
pair (s, t) ∈ T × T into ﬁnitely many (here, three) closed
pairs (s1, t1), (s2, t2), (s3, t3) and an open pair (so, to).

In the same spirit as the functions φ1 and ˜φ1 from the last section, we

now introduce two functions φ2 and ˜φ2, deﬁned as follows:
(cid:88)

E[ ˜Aθ/2
s

˜Aθ/2
t

] ,

θ > 0

φ2(θ) := log

˜φ2(θ) := log

(s,t)∈C
(cid:88)

(s,t)∈C

E[ ˜Aθ/2
s

˜Aθ/2
t

]2/θ ,

θ > 0 .

Note that we have φ2(θ) (cid:62) φ1(θ) for all θ > 0, because the set C contains
the closed pairs ((0, i), (0, i)) for each i (cid:62) 1 and summing over these pairs
only yields φ1(θ).

The following theorem is the main result of this article:

Theorem 3.1. Let θ > 0.

(1) Assume φ2(θ) (cid:62) 0 if θ (cid:62) 2 or ˜φ2(θ) (cid:62) 0 if θ (cid:54) 2. Then E[X θ] = ∞.
(2) Assume the following:

(a) either: θ (cid:54) 2, φ1(θ/2) < 0 and φ2(θ) < 0,
or: θ (cid:62) 2, ˜φ1(θ/2) < 0 and ˜φ2(θ) < 0,
i Aθ/2

]min(1,2/θ)(cid:105)

E[Aθ/2

(b) E

(cid:104)(cid:80)

i,j(cid:62)1

j

< ∞ (only under (A1)),

s1t1s2s3sot3t2to10

PASCAL MAILLARD AND OLIVIER WINTENBERGER

(c) E[Bθ] < ∞.
Then E[X θ] < ∞.

Proof. We ﬁrst consider θ = 2. By the decomposition of arbitrary pairs into
a concatenation of closed pairs and an open pair, we have

(3.13)

∞
(cid:88)

˜X 2 =

(cid:88)

(cid:88)

˜As1···skso ˜Bs1···skso ˜At1···tkto ˜Bt1···tkto.

k=0

(s1,t1),...,(sk,tk)∈C

(so,to)∈O

We now claim the following:

(3.14) E[ ˜As1···skso ˜Bs1···skso ˜At1···tkto ˜Bt1···tkto]

= E[ ˜As1 ˜At1] · · · E[ ˜Ask ˜Atk ]E[ ˜Aso ˜Bso ˜Ato ˜Bto].

To see this, set τ0 = 0 and for i = 1, . . . , k:
n(sk) = t1

n(s1) + · · · + sk

τi := s1

n(t1) + · · · + tk

n(tk),

where the equality comes from the deﬁnition of closed pairs. In Figure 1,
the τi correspond to the abscissae of the dotted lines. We now write

˜As1···skso ˜At1···tkto = Π1 · · · ΠkΠo, where

Πi =

˜As1···si ˜At1···ti
˜As1···si−1 ˜At1···ti−1

,

i = 1, . . . , k,

and Πo =

˜As1···skso ˜Bs1···skso ˜At1···tkto ˜Bt1···skto
˜As1···sk ˜At1···tk

.

Now note that for every i = 1, . . . , k, Πi is a product of terms ˜At,j with

• t ∈ {τi−1, . . . , τi − 1}, and
• t + j ∈ {τi−1 + 1, . . . , τi}.

Furthermore, Πo is a product of terms ˜At,j and B−t with

• t ∈ {τk, τk + 1, . . .}, and
• t + j ∈ {τk + 1, τk + 2, . . .}.

Hence, under either (A1) or (A2), the products Π1, . . . , Πk, Πo are inde-
pendent, see Remark 2.1. Furthermore, by stationarity of the sequence
(( ˜At,j)j(cid:62)1)t(cid:62)0, we have E[Πi] = E[ ˜Asi ˜Ati] for all i = 1, . . . , k and E[Πo] =
E[ ˜Aso ˜Bso ˜Ato ˜Bto]. This yields (3.14).

Equations (3.13) and (3.14) together now give

(3.15)

E[X 2] =





(cid:88)

(s,t)∈O

E[ ˜As ˜Bs ˜At ˜Bt]





∞
(cid:88)






k

(cid:88)

E[ ˜As ˜At]



.

k=0

(s,t)∈C

MOMENT CONDITIONS FOR RANDOM COEFFICIENT AR(∞)

11

Noting that

(cid:88)

(s,t)∈O

and

E[ ˜As ˜Bs ˜At ˜Bt] (cid:62) E[ ˜A0 ˜B0 ˜A0 ˜B0] = E[B2] > 0,

E[ ˜As ˜At] = eφ2(2) = e

˜φ2(2),

(cid:88)

(s,t)∈C

the ﬁrst statement of the lemma follows from (3.15) (in the case θ = 2).

In the case θ (cid:62) 2, we use superadditivity of the function x (cid:55)→ xθ/2 and

the deﬁnition of ˜X to bound



(cid:32)

E[X θ] (cid:62) E



(cid:88)

t∈T

(cid:33)2
 ,

˜Aθ/2
t

˜Bθ/2
t

and then use the case θ = 2 with At,j and Bj replaced by Aθ/2
all t, j.

t,j and Bθ/2

j

for

In the case θ (cid:54) 2, similarly to the proof of (3.15), but using moreover the

reverse Minkowski’s inequality, we get

(3.16)

E[X θ]2/θ = E[(X 2)θ/2]2/θ




(cid:88)

(cid:62)



(s,t)∈O

E[ ˜Aθ/2
s

˜Bθ/2
s

˜Aθ/2
t

˜Bθ/2
t

]2/θ



∞
(cid:88)





(cid:88)

k=0

(s,t)∈C



k

E[ ˜Aθ/2
s

˜Aθ/2
t

]2/θ



.

This yields the ﬁrst statement in the case θ (cid:54) 2 and thus ﬁnishes the proof
of the ﬁrst statement.

We now turn to the proof of the second statement (ﬁniteness of E[X θ]).
Again, we ﬁrst consider the case θ = 2 (note that φ1(1) = ˜φ1(1) and φ2(2) =
˜φ2(2) by deﬁnition). By assumption (a), the geometric series in (3.15) is
ﬁnite. It suﬃces to show that (cid:80)

(s,t)∈O E[ ˜As ˜Bs ˜AtBt] is ﬁnite as well.

We separately consider the cases (A1) and (A2). We start with the
simpler case (A2). Let (s, t) ∈ O. By deﬁnition, ˜As is a product of terms
˜At,j with t + j ∈ S := {s1, . . . , sn(s)}. Similarly, ˜At is a product of terms ˜At,j
with t + j ∈ T := {t1, . . . , tn(t)}. By deﬁnition of O, the sets S and T are
disjoint. Hence, under assumption (A2), ˜As and ˜At are independent, see
Remark 2.1. Thus, using the non-negativity of the A’s and the independence

12

PASCAL MAILLARD AND OLIVIER WINTENBERGER

assumptions,

(cid:88)

(s,t)∈O

E[ ˜As ˜Bs ˜At ˜Bt] = E[B2] +

(cid:88)

E[ ˜As]E[ ˜At]E[B]2

(cid:54) E[B2]

(s,t)∈O\(0,0)
(cid:88)

E[ ˜As(cid:48)]E[ ˜At(cid:48)]

s(cid:48),t(cid:48)∈T
= E[B2]E[X (cid:48)]2 < ∞.

We now treat the more delicate case (A1). Let again (s, t) ∈ O. We
decompose s and t according to their ﬁrst jump (if it exists): s = (i)s(cid:48) and
t = (j)t(cid:48), with i, j ∈ N0 and s(cid:48), t(cid:48) ∈ T , and with the short-hand notation

(i) =

(cid:40)

(0, i)
(0)

if i (cid:62) 1
if i = 0.

Using (A1) and the deﬁnition of O, we get, setting A0 = ˜A0 = 1,

E[ ˜As ˜At] = E[AiAj]E[ ˜As(cid:48)]E[ ˜At(cid:48)].

Furthermore, the map

(cid:40)O → N2
0 × T 2
(s, t) (cid:55)→ (i, j, s(cid:48), t(cid:48))

is obviously injective. Hence, by non-negativity of the A’s and the indepen-
dence assumptions, we get

(cid:88)

(s,t)∈O

E[ ˜As ˜Bs ˜At ˜Bt] (cid:54) E[B2]

(cid:88)

(cid:88)

s(cid:48),t(cid:48)∈T

i,j∈N0


E[AiAj]E[ ˜As(cid:48)]E[ ˜At(cid:48)]

= E[B2]



(cid:88)

E[AiAj]





(cid:32)

(cid:33)2

(cid:88)

E[ ˜At]

(3.17)

i,j∈N0


1 + 2eφ1(1) +

= E[B2]

t∈T


 E[X (cid:48)]2.

E[AiAj]

(cid:88)

i,j(cid:62)1

By hypothesis (c), we have E[B2] < ∞. Using hypotheses (a) and (b), one
gets ﬁniteness of the term in brackets on the RHS of (3.17). Finiteness of
E[X (cid:48)] follows from the hypothesis φ1(1) < ∞ and Theorem 2.2 (2). Alto-
gether, this yields (cid:80)

(s,t)∈O E[ ˜As ˜Bs ˜At ˜Bt] < ∞, which was to be proven.

We now treat the case θ (cid:54) 2. By subadditivity, we have

X θ/2
t

(cid:54)

∞
(cid:88)

j=1

t,j X θ/2
Aθ/2

t−j + Bθ/2

t

,

t ∈ Z .

MOMENT CONDITIONS FOR RANDOM COEFFICIENT AR(∞)

13

Thus E[X θ] (cid:54) E[(X (θ))2] where X (θ) is the non-anticipative stationary so-
lution of

∞
(cid:88)

X (θ)

t =

t,j X (θ)
Aθ/2

t−j + Bθ/2

t

,

t ∈ Z ,

j=1

and the general result for θ (cid:54) 2 follow from the one for θ = 2 considering
an alternative AR model.

The case θ (cid:62) 2 on the other hand is treated by following the proof for
θ = 2, but starting from the reverse inequality of (3.16), which holds by
Minkowski’s inequality, instead of (3.15).

(cid:3)

3.1. Extension to higher moments. It is natural to try to extend the
above methods to higher moments. One could deﬁne for every k (cid:62) 2 two sets
of closed and open k-tuples as follows (in the deﬁnition below, we identify
t = (t0, . . . , tn) ∈ T with the set {t0, . . . , tn} and note that we have 0 ∈ t
for every t ∈ T ):

C (k) = {(t1, . . . , tk) ∈ T k : t1 ∩ . . . ∩ tk = {0, m},

where m = t1

n(t1) = · · · = tk

n(tk) > 0},

O (k) = {(t1, . . . , tk) ∈ T k : t1 ∩ . . . ∩ tk = {0}}.

One can then again uniquely write any k-tuple (t1, . . . , tk) ∈ T k as a con-
catenation of a ﬁnite number of closed tuples and one open tuple. Moreover,
an analogue of (3.15) holds: Assuming that B ≡ 1 for simplicity, we have

(3.18)

E[X k] =





(cid:88)

E[ ˜At1 · · · ˜Atk ]





∞
(cid:88)





(cid:88)

E[ ˜At1 · · · ˜Atk ]



.



n

(t1,...,tk)∈O (k)

n=0

(t1,...,tk)∈C (k)

A natural guess for a suﬃcient and necessary condition for the ﬁniteness
of E[X k] is that (cid:80)
(t1,...,tk)∈C (k) E[ ˜At1 · · · ˜Atk ] < 1, plus probably some ad-
ditional conditions on moments of lower order. However, we were not able
to prove this.
Indeed, while it is immediate that the above condition is
necessary for the ﬁniteness of E[X k] (for, otherwise, the sum over n on the
right-hand side of (3.18) is inﬁnite), we were not able to prove suﬃciency.
The problem is caused by the term on the right-hand side of (3.18) involv-
ing O (k): we were not able to give simple hypotheses under which this term
is ﬁnite, as it is not clear how to generalize the proof for k = 2 to gen-
eral k. Indeed, while in the case of k = 2, ˜As and ˜At are independent for
(s, t) ∈ O(2) (at least under assumption (A2)), this is not the case anymore
for k > 2. We believe that the extension of our methods to higher moments
is an interesting question for further research and hope to be able to address
it in the future.

14

PASCAL MAILLARD AND OLIVIER WINTENBERGER

4. Second moment – comparison to a classical criterion

There exists an extensive statistics literature on the random coeﬃcient
autoregressive equation (1.1), see for instance the book of Nicholls and Quinn
[11]. One usually considers autoregressive equations with a centred noise Bt
(unlike the case of non-negative noise considered in this article) and under
assumption (A1). In this context, and in the case p < ∞, there exists a
classical criterion which is necessary and suﬃcient for the stationary solution
to have ﬁnite second moment. In this section, we show that this criterion,
which makes sense for arbitrary non-homogeneities, is actually equivalent to
the criterion from Theorems 3.1.

Throughout the section, we are in the context from the beginning of Sec-
tion 2, i.e. we consider (2.9) with non-negative coeﬃcients ((At,j)j, Bt)t(cid:62)0,
and we assume that (A1) holds. We further assume that the equation is of
ﬁnite order p, i.e.

(p) There exists p < ∞, such that At,j = 0 almost surely for all t ∈ Z

and all j > p.

Recall the deﬁnition of the matrices

At =










1

At,1 At,2
0
. . .
. . .
· · ·

0
...
0

· · ·
· · ·
. . .
. . .
0










· · · At,p
0
· · ·
...
...
0

. . .
1

,

and set A = A1,t and Aj = A1,j, j (cid:62) 1.

For simplicity, we will furthermore assume the following:
(+) At,j > 0 almost surely for all t ∈ Z, 1 (cid:54) j (cid:54) p.
(B) B is independent of (Aj)1(cid:54)j(cid:54)p and E[B2] < ∞.
We denote by ⊗ the tensor (or Kronecker) product of matrices. Recall
that ⊗ is bilinear and associative and that it satisﬁes the mixed product
property with respect to the usual matrix product:

(M ⊗ N )(P ⊗ Q) = M P ⊗ N Q.

For a square matrix M , we further denote by ρ(M ) its spectral radius,
i.e. the largest absolute value of its eigenvalues. We furthermore denote by
I the identity matrix of dimension p.

Theorem 4.1. Assume (A1), (p), (+) and (B). Then (cid:80)
1 and (cid:80)∞
j=1

E[Aj] < 1 if and only if ρ(E[A ⊗ A]) < 1.

(s,t)∈C E[ ˜As ˜At] <

Proof. By the assumption on B, we can assume with no loss of general-
ity that B = 1 a.s. By Theorem 3.1, we have (cid:80)
(s,t)∈C E[ ˜As ˜At] < 1 and
(cid:80)∞
E[Aj] < 1 if and only if E[X 2] < ∞ if and only if E[ (cid:101)X 2] < ∞, where

j=1

MOMENT CONDITIONS FOR RANDOM COEFFICIENT AR(∞)

15

we recall that ˜X is deﬁned in (2.11). It follows from this deﬁnition that

(cid:101)X =

(cid:16) (cid:88)

n(cid:62)0

A0 · · · A−n+1

(cid:17)

11

(cid:16) (cid:88)

d=

n(cid:62)0

A1 · · · An

(cid:17)

,

11

where the equality in law follows from (A1). Hence, deﬁning the matrix

M := E

(cid:33)

(cid:32)

A1 · · · An

⊗

(cid:34)(cid:32)

(cid:88)

n(cid:62)0

(cid:88)

n(cid:62)0

(cid:33)(cid:35)

A1 · · · An

,

and denoting by (Mij)i,j=1,...,p2 its coordinates, we have

E[ (cid:101)X 2] < ∞ ⇐⇒ M11 < ∞.
We now claim that M11 is ﬁnite if and only if ρ(E[A ⊗ A]) < 1. To see

this, we ﬁrst express the matrix M as follows:
(cid:33)(cid:35)

(cid:34)(cid:32)

(cid:32)

(cid:33)

M = E

(cid:88)

A1 · · · An

⊗

A1 · · · An

(cid:88)

n(cid:62)0

(cid:88)





= E

n,m(cid:62)0

n(cid:62)0

A1 · · · An ⊗ A1 · · · Am





(cid:104) (cid:88)

= E

(A1 ⊗ A1) · · · (An ⊗ An)

(cid:16)

I ⊗ I

n(cid:62)0

+

(cid:88)

k(cid:62)1

(cid:0)(I ⊗ An+1) · · · (I ⊗ An+k) + (An+1 ⊗ I) · · · (An+k ⊗ I)(cid:1)(cid:17)(cid:105)

=

=

(cid:32)

(cid:32)

(cid:88)

n(cid:62)0

(cid:88)

n(cid:62)0

(cid:33) 

E[A ⊗ A]n

I ⊗ I +

(cid:88)

k(cid:62)1

(cid:33) 

(cid:0)E[I ⊗ A]k + E[A ⊗ I]k(cid:1)















E[A ⊗ A]n

I ⊗ I + I ⊗



(cid:88)

k(cid:62)1

E[A]k

 +



(cid:88)

k(cid:62)1

E[A]k

 ⊗ I

 .

Now, note that ρ(E[A]) (cid:54) (cid:112)ρ(E[A ⊗ A]) by Lemma 4.2 below. Hence, if
ρ(E[A ⊗ A]) < 1, all the above series are ﬁnite. Thus all the entries of M
are ﬁnite, in particular M11.

On the other hand, suppose ρ(E[A ⊗ A]) (cid:62) 1; we will show that the ﬁrst
series is inﬁnite. Note that, almost surely, the matrix A is irreducible and
aperiodic by assumption (+). Hence, Ak > 0 entrywise for k large enough,
i.e. the matrix is almost surely primitive in the language of Seneta [12]. Since
(A ⊗ A)k = Ak ⊗ Ak, it follows that A ⊗ A is almost surely primitive as
well, hence E[A ⊗ A] is primitive by non-negativity. We can therefore apply
the strong version of the Perron-Frobenius theorem for primitive matrices
(see Theorem 1.2 of Seneta [12]) to the matrix E[A ⊗ A]. It follows that
there exist right and left eigenvectors u and v, respectively, with positive

16

PASCAL MAILLARD AND OLIVIER WINTENBERGER

entries and associated to the largest eigenvalue ρ = ρ(E[A ⊗ A]) (cid:62) 1 so that

ρ−nE[A ⊗ A]n = uvT + o(1) ,

n → ∞ .

One deduces that, entrywise,

m
(cid:88)

n=0

E[A ⊗ A]n (cid:62)

m
(cid:88)

n=0

ρ−nE[A ⊗ A]n = muvT + o(m) ,

m → ∞ ,

Since all the entries of uvT are positive, this yields for every m (cid:62) 0,

E[ ˜X 2] = M11 (cid:62)

(cid:32) m
(cid:88)

n=0

(cid:33)

E[A ⊗ A]n

(cid:62) mu1v1 + o(m),

11

which implies that E[ ˜X 2] = ∞. This proves the result.

(cid:3)

The following lemma was needed in the proof of Theorem 4.1 above and

is included here for completeness.

Lemma 4.2. Let A be a random m × m matrix with non-negative entries,
m (cid:62) 1. Then, ρ(E[A]) (cid:54) (cid:112)ρ(E[A ⊗ A]).

Proof. Denote by (cid:107) · (cid:107)∞ the entry-wise maximum norm. Note that for every
m × m matrix M , we have

(cid:107)E[M ](cid:107)∞ (cid:54) E[(cid:107)M (cid:107)∞] (cid:54) m2(cid:107)E[M ](cid:107)∞.
Indeed, the ﬁrst inequality comes from the non-negativity of the entries of
M , and the second inequality comes from bounding (cid:107)M (cid:107)∞ by the sum over
the entries of M . Furthermore, from the deﬁnition of the tensor product,
we have

(cid:107)M ⊗ M (cid:107)∞ = (cid:107)M (cid:107)2
All this, together with the Cauchy-Schwarz inequality, now implies for every
k (cid:62) 1,

∞.

(cid:107)E[Ak](cid:107)∞ (cid:54) E[(cid:107)Ak(cid:107)∞]

(cid:113)

(cid:113)

= E[

(cid:107)Ak ⊗ Ak(cid:107)∞]

= E[
(cid:113)

(cid:54)

(cid:107)(A ⊗ A)k(cid:107)∞]

E[(cid:107)(A ⊗ A)k(cid:107)∞]

(cid:54)

(cid:113)

m4(cid:107)E[(A ⊗ A)k](cid:107)∞.

By Gelfand’s formula, we now have

ρ(E[Ak]) = lim
k→∞

(cid:107)E[Ak](cid:107)1/k
∞

m4/k(cid:113)
(cid:54) lim
k→∞
= (cid:112)ρ((cid:107)E[A ⊗ A]).

(cid:107)E[(A ⊗ A)k](cid:107)∞

1/k

This ﬁnishes the proof.

(cid:3)

MOMENT CONDITIONS FOR RANDOM COEFFICIENT AR(∞)

17

5. Examples

In this section we work out two toy examples. The special feature of these
examples is that explicit necessary and suﬃcient conditions for ﬁniteness of
moments are available. Note that necessary conditions are rarely discussed
in the literature. Two notable exceptions are [5] for the ARCH model and
[8] for the LARCH model. The ﬁrst one has positive coeﬃcients as in the
present article. Using our approach, we recover and extend the results of
[5].

5.1. A simple example of order 2. We consider the case where A1, A2
are iid copies of a r.v. A, E[A] < 1, and Ak = 0 for k (cid:62) 3. Furthermore,
we assume that condition (A1) is satisﬁed. We wish to make explicit the
criterion for ﬁniteness of second moments from Theorem 3.1 and compare it
to the classical criterion 1.6 from Nicholls and Quinn [11, Corollary 2.2.2].
By Theorem 4.1 these are equivalent.

The set of closed pairs C is very simple: if (s, t) ∈ C , then either
(1) (s, t) ∈ {((0, 1), (0, 1)), ((0, 2), (0, 2))}, or
(2) there exists k (cid:62) 1, such that s starts with a jump of size 1, then t
and s alternately do jumps of size 2 (k times in total), followed by
a last jump of size 1 (from either s or t depending on whether k is
odd or even), or

(3) same as the previous case, but with the roles of s and t exchanged.

If (s, t) is of the ﬁrst type, then

E[AsAt] = E[A2].

If (s, t) is of the second or third type, with the corresponding k, then

E[AsAt] = E[A]k+2.

Hence,

(cid:88)

(s,t)∈C

E[AsAt] = 2E[A2] + 2

E[A]k+2 = 2

(cid:18)

E[A2] +

E[A]3
1 − E[A]

(cid:19)

.

(cid:88)

k(cid:62)1

Writing a = E[A] and b = E[A2], this gives

E[AsAt] (cid:62) 1 ⇐⇒ 2

(cid:18)

b +

(cid:19)

a3
1 − a

(cid:62) 1

(cid:88)

(s,t)∈C

(5.19)

⇐⇒ (2b − 1)(1 − a) + 2a3 (cid:62) 0.

We now calculate E[A ⊗ A]. First, we have

A ⊗ A =







A2
A1
A1
1

1 A1A2 A2A1 A2
2
0
0
0

A2
0
0

0
A2
0







,

18

PASCAL MAILLARD AND OLIVIER WINTENBERGER

and so

E[A ⊗ A] =







b a2 a2
a
0
a
0
a
a
0
0
1



b
0


0

0

.

The characteristic polynomial of this matrix is easily calculated to be

det(E[A ⊗ A] − X(I ⊗ I)) = (X + a)P (X),

where the polynomial P (X) is deﬁned as

P (X) = X 3 − (a + b)X 2 + (−2a2 − b + ab)X + ab.
Hence, the eigenvalues of the matrix E[A ⊗ A] are −a as well as the roots
of the degree-3 polynomial P (X). Recall that a ∈ (0, 1). We have

P (−1) = −1 − a + 2a3 < 0

P (0) = ab > 0
P (a) = −2a4 < 0.

Therefore, the smallest root of the polynomial P is greater than −1 and,
furthermore, the largest root is greater or equal than 1 if and only if P (1) (cid:54)
0. Hence,

ρ(E[A ⊗ A]) (cid:62) 1 ⇐⇒ P (1) (cid:54) 0 ⇐⇒ (2b − 1)(1 − a) + 2a3 (cid:62) 0,

which is exactly (5.19), as expected.

5.2. A degenerate inﬁnite memory case. We consider the following ex-
ample motivated by the GARCH(1,1) model that also admits an ARCH(∞)
representation. Consider the inﬁnite memory recursion

(5.20)

Xt =

1
1 − β

+

(cid:88)

k(cid:62)0

βk+1Zt−kXt−1−k ,

t ∈ Z .

where (Zt)t∈Z is an iid sequence of copies of a non-negative random variable
Z. Setting At,j = βjZt−j+1, we see that (5.20) is of the form (1.1) with this
choice of At,j. Furthermore, assumption (A2) is veriﬁed.

On the other hand, it is well-known and can easily be checked that the sta-
tionary solution of the above equation, if it exists, also satisﬁes the Markov
equation

(5.21)

Xt = 1 + β(1 + Zt)Xt−1 ,

t ∈ Z .

As before, denote by X the limit in law of Xt as t → ∞. From the latter
recursion, one can obtain that, for every θ > 0,

E[X θ] < ∞ ⇐⇒ E[βθ(1 + Z)θ] < 1.
(5.22)
In fact, it is known that there exists C > 0 such that P(X > x) ∼ C/xα
as x → ∞ for α > 0 satisfying the equation E[βα(1 + Z)α] = 1, see [4] for
more details. Comparing the necessary and suﬃcient condition (5.22) with

MOMENT CONDITIONS FOR RANDOM COEFFICIENT AR(∞)

19

the conditions obtained by computing the functions φ1, ˜φ1, φ2 and ˜φ2, we
thus get an explicit benchmark for our conditions.

The functions φ1 and ˜φ1 are easily calculated from (5.20), which yields,

φ1(θ) = log

˜φ1(θ) = log

(cid:17)

(cid:16) βθE[Zθ]
1 − βθ
(cid:16) βE[Zθ]1/θ
1 − β

,

(cid:17)

.

We now calculate φ2. For s = (s0, . . . , sn) ∈ T , write Zs = Zs1 · · · Zsn.

Then it is easily checked that

(cid:88)

(s,t)∈C

E[ ˜Aθ/2
s

˜Aθ/2
t

] =

(cid:88)

(s,t)∈C

βθ/2(sn(s)+tn(t))E[Zθ/2

s Zθ/2
t

].

Setting

Cm = {(s, t) ∈ C : sn(s) = tn(t) = m} , m (cid:62) 1 ,

the above equality is expressed as

(cid:88)

(s,t)∈C

E[ ˜Aθ/2
s

˜Aθ/2
t

] =

∞
(cid:88)

m=1

βθm (cid:88)

E[Zθ/2

s Zθ/2
t

] ,

(s,t)∈Cm

Fix m (cid:62) 1 and let (s, t) ∈ Cm. By deﬁnition, we have

E[Zθ/2

s Zθ/2
t

] = E[Zθ]E[Zθ/2]n(s)+n(t)−2.

Note that the exponent j = n(s) + n(t) − 2 is equal to the number of points
(cid:1) ways
in {1, . . . , m − 1} which are contained in either s or t. There are (cid:0)m−1
to choose these points and, given the choice of these points there are 2j ways
to distribute them among s and t. It follows that

j

(cid:88)

(s,t)∈Cm

E[Zθ/2

s Zθ/2
t

] = E[Zθ]

m−1
(cid:88)

j=0

(cid:19)

(cid:18)m − 1
j

(2E[Zθ/2])j

= E[Zθ](1 + 2E[Zθ/2])m−1.

Summing over m yields,

(cid:88)

(s,t)∈C

E[ ˜Aθ/2
s

˜Aθ/2
t

] = E[Zθ]

∞
(cid:88)

m=1

βθm(1 + 2E[Zθ/2])m−1.

It follows that

φ2(θ) =




log

(cid:16)

βθE[Zθ]
1 − βθ(1 + 2E[Zθ/2])

(cid:17)

,



+∞,

if βθ(1 + 2E[Zθ/2]) < 1

otherwise.

20

PASCAL MAILLARD AND OLIVIER WINTENBERGER

A similar argument shows that

˜φ2(θ) =




log

(cid:16)



+∞,

β2E[Zθ]2/θ
1 − β2(1 + 2E[Zθ/2]2/θ)

(cid:17)

,

if β2(1 + 2E[Zθ/2]2/θ) < 1

otherwise.

With the explicit expressions of φ1, ˜φ1, φ2 and ˜φ2 at hand, we can now
compare the conditions for ﬁniteness of E[X θ] provided by Theorem 2.2 and
Theorem 3.1 with the condition (5.22). To do this, for every θ ∈ (0, 3],
we consider the critical value βθ, such that E[X θ] is ﬁnite for β < βθ and
inﬁnite for β > βθ (the existence and uniqueness of βθ is easily seen by
monotonicity in β of the involved functions). Our theorems provide upper
and lower bounds in the phases θ ∈ (0, 1], θ ∈ [1, 2] and θ ∈ [2, 3], which are
furthermore sharp for θ ∈ {1, 2}. In Figure 2, we compare these bounds with
the exact value for βθ obtained from the equation log(E[βθ(1 + Z)θ]) = 0.
One can notice that the use of second moment methods allows to greatly
improve the quality of the bounds obtained by ﬁrst moment methods, as
soon as θ > 1.2.

Conditions of moments are crucial for proving the asymptotic normality
of estimators of the parameters driving time series models.
In particular
for GARCH models it is important to assume the sharpest condition of mo-
ments in order to apply the estimators on plausibly heavy-tailed ﬁnancial
time series; see [7] for a reference textbook on GARCH modeling. For in-
stance Bardet and Wintenberger [2] used second moment methods in inﬁnite
memory processes in order to prove the asymptotic normality of the Quasi-
Maximum Likelihood Estimator (QMLE) of the parameters θ = (ω, α1, β1)
in the GARCH(1,1) model εt = Ztσt for iid (Zt) and (σt) satisfying the
recursion

t−1 ,

t−1 + β1σ2

t ∈ Z .
t /ω, β = β1 and Zt =
t /β1. Then the necessary and suﬃcient condition of moments of or-

t = ω + α1ε2
σ2
This equation coincides with (5.21) for Xt = σ2
α1Z2
der 2 on σ2

t (and then 4 on εt) is
φ2(2) = ˜φ2(2) < 0 ⇐⇒ α2
1
However the suﬃcient condition used in Bardet and Wintenberger [2] is

E[Z4] + 2α1β1E[Z2] + β2

1 < 1 .

φ1(2) < 0 ⇐⇒ α1E[Z4]1/2 + β1 < 1 .
In Figure 3 we illustrate both conditions on the coeﬃcients (α1, β1) for stan-
dard gaussian Z. Note however that the asymptotic normality of the QMLE
holds under much weaker log-moments conditions than the second moment
condition φ2(2) = ˜φ2(2) < 0 by using directly the recursion (5.21) rather
than (5.20); see Francq and Zakoian [7]. The inﬁnite memory approach
is only required in more complex settings such as in [1] for time varying
parameters GARCH(1,1) models. There, the best known condition for as-
ymptotic normality is φ(2) < 0 and our approach might improve on the
existing literature.

MOMENT CONDITIONS FOR RANDOM COEFFICIENT AR(∞)

21

Figure 2. Illustration of our necessary and suﬃcient con-
ditions of moments. Our suﬃcient conditions of moments
correspond to the curves in red (for 0 < θ (cid:54) 1), blue (for
0 < θ (cid:54) 2, green (for θ (cid:62) 1) and purple (for θ (cid:62) 2). Our
necessary conditions of moments correspond to the curves in
green (for 0 < θ (cid:54) 1), purple (for 0 < θ (cid:54) 2), red (for θ (cid:62) 1)
and blue (for θ (cid:62) 2).

References

[1] Jean-Marc Bardet, Paul Doukhan, and Olivier Wintenberger. Contrast estimation of
general locally stationary processes using coupling. arXiv preprint arXiv:2005.07397,
2020.

[2] Jean-Marc Bardet and Olivier Wintenberger. Asymptotic normality of the quasi-
maximum likelihood estimator for multidimensional causal processes. The Annals of
Statistics, 37(5B):2730–2759, 2009.

[3] Bojan Basrak, Rafa(cid:32)l Kulik, and Zbigniew Palmowski. Heavy-tailed branching process

with immigration. Stochastic Models, 29(4):413–434, 2013.

[4] Dariusz Buraczewski, Ewa Damek, and Thomas Mikosch. Stochastic models with
power-law tails. Springer Series in Operations Research and Financial Engineering.
Springer, [Cham], 2016.

[5] Randal Douc, Fran¸cois Roueﬀ, and Philippe Soulier. On the existence of some
arch(∞) processes. Stochastic Processes and Their Applications, 118(5):755–761,
2008.

0.00.51.01.52.02.53.00.350.400.450.500.55θβφ1=0φ~1=0φ2=0φ~2=022

PASCAL MAILLARD AND OLIVIER WINTENBERGER

Figure 3. Illustration of diﬀerent suﬃcient conditions of
second order moments on the coeﬃcients (α1, β1) of the
GARCH(1,1) model (below the black and red curves, respec-
tively).

[6] Paul Doukhan and Olivier Wintenberger. Weakly dependent chains with inﬁnite mem-

ory. Stochastic Processes and their Applications, 118(11):1997–2013, 2008.

[7] Christian Francq and Jean-Michel Zakoian. GARCH models: structure, statistical

inference and ﬁnancial applications. John Wiley & Sons, 2019.

[8] Liudas Giraitis, Remigijus Leipus, Peter M Robinson, and Donatas Surgailis.
LARCH, leverage, and long memory. Journal of Financial Econometrics, 2(2):177–
210, 2004.

[9] Henrik Hult and Gennady Samorodnitsky. Tail probabilities for inﬁnite series of reg-

ularly varying random vectors. Bernoulli, 14(3):838–864, 2008.

[10] Harry Kesten. Random diﬀerence equations and Renewal theory for products of ran-

dom matrices. Acta Mathematica, 131(1):207–248, 1973.

[11] Des F. Nicholls and Barry G. Quinn. Random Coeﬃcient Autoregressive Models:
An Introduction. Lecture notes in statistics, vol. 11. Springer-Verlag, New York -
Heidelberg - Berlin, 1982.

[12] Eugene Seneta. Non-negative matrices and Markov chains. Springer Science & Busi-

ness Media, 2006.

0.00.20.40.60.81.00.00.10.20.30.40.5β1α1φ2(2)=0φ1(2)=0MOMENT CONDITIONS FOR RANDOM COEFFICIENT AR(∞)

23

Institut de Math´ematiques de Toulouse, Universit´e Toulouse 3 Paul Sabatier,

118 Route de Narbonne, 31062 Toulouse Cedex 9, France
Email address: firstname.lastname@math.univ-toulouse.fr
URL: https://www.math.univ-toulouse.fr/~pmaillar

LPSM, Sorbonne Universit´e, 4 place Jussieu, 75005 Paris, France
Email address: firstname.lastname@sorbonne-universite.fr
URL: http://wintenberger.fr

