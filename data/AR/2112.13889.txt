Free-ViewpointRGB-DHumanPerformanceCaptureandRenderingPhongNguyen-Ha1∗,NikolaosSarafianos2,ChristophLassner2,JanneHeikkil¨a1,andTonyTung21CenterforMachineVisionandSignalAnalysis,UniversityofOulu,Finland2MetaRealityLabsResearch,Sausalitohttps://www.phongnhhn.info/HVSNetAbstract.CapturingandfaithfullyrenderingphotorealistichumansfromnovelviewsisafundamentalproblemforAR/VRapplications.Whilepriorworkhasshownimpressiveperformancecaptureresultsinlaboratorysettings,itisnon-trivialtoachievecasualfree-viewpointhumancaptureandrenderingforunseenidentitieswithhighfidelity,especiallyforfacialexpressions,hands,andclothes.Totacklethesechallengesweintroduceanovelviewsynthesisframeworkthatgeneratesrealisticrendersfromunseenviewsofanyhumancapturedfromasingle-viewandsparseRGB-Dsensor,similartoalow-costdepthcamera,andwithoutactor-specificmodels.Weproposeanarchitecturetocreatedensefea-turemapsinnovelviewsobtainedbysphere-basedneuralrendering,andcreatecompleterendersusingaglobalcontextinpaintingmodel.Additionally,anen-hancernetworkleveragestheoverallfidelity,eveninoccludedareasfromtheoriginalview,producingcrisprenderswithfinedetails.Weshowthatourmethodgenerateshigh-qualitynovelviewsofsyntheticandrealhumanactorsgivenasingle-stream,sparseRGB-Dinput.Itgeneralizestounseenidentities,andnewposesandfaithfullyreconstructsfacialexpressions.Ourapproachoutperformspriorviewsynthesismethodsandisrobusttodifferentlevelsofdepthsparsity.1IntroductionNovelviewsynthesisofrigidobjectsordynamicsceneshasbeenaveryactivetopicofresearchrecentlywithimpressiveresultsacrossvarioustasks[42,45,62].However,syn-thesizingnovelviewsofhumansinmotionrequiresmethodstohandledynamicsceneswithvariousdeformationswhichisachallengingtask[62,67];especiallyinthosere-gionswithfinedetailssuchasthefaceortheclothes[46,50,63,66].Inaddition,priorworkusuallyreliesonalargeamountofcameras[5,42],expensivecapturesetups[51],orinferencetimeontheorderofseveralminutesperframe.Thisworkaimstotacklethesechallengesusingacompact,yeteffectiveformulation.WeproposeanovelHumanViewSynthesisNetwork(HVS-Net)thatgenerateshigh-fidelityrenderedimagesofclothedhumansusingacommodityRGB-Dsensor.Thechallengingrequirementsthatweimposeare:i)generalizationtonewsubjectsattest-timeasopposedtomodelstrainedpersubject,ii)theabilitytohandledynamic*ThisworkwasconductedduringaninternshipatMetaRealityLabsResearch.arXiv:2112.13889v4  [cs.CV]  2 Aug 20222P.Nguyenetal.Input viewNovel ViewsInput viewNovel ViewsDetailsSynthetic DataReal DataFig.1.Overview.WepresentaHumanViewSynthesismodelthatpredictsnovelviewsofhumansfromasingle-view,sparseRGB-Dinput.Ourmethodrendershighqualitynovelviewsofboth,syntheticandrealhumansat1Kresolutionwithoutper-subjectfinetuning.behaviorofhumansinunseenposesasopposedtoanimatinghumansusingthesameposesseenattraining,iii)theabilitytohandleocclusions(eitherfromobjectsorself-occlusion),iv)capturingfacialexpressionsandv)thegenerationofhigh-fidelityimagesinalivesetupgivenasingle-stream,sparseRGB-Dinput(similartoalow-cost,off-the-shelfdepthcamera).HVS-Nettakesasinputasingle,sparseRGB-Dimageoftheupperbodyofahu-manandatargetcameraposeandgeneratesahigh-resolutionrenderingfromthetargetviewpoint(seeFig.1).Thefirstkeydifferentiatingfactorofourproposedapproachcomparedtopreviousapproachesisthatweutilizedepthasanadditionalinputstream.Whiletheinputdepthissparseandnoisyitstillenablesustoutilizetheinformationseenintheinputviewandhencesimplifyingthesynthesisofnovelviews.Toaccountforthesparsenessoftheinput,weoptedforasphere-basedneuralrendererthatusesalearn-ableradiustocreateadenser,warpedimagecomparedtosimplyperforminggeometrywarpingfromoneviewtotheother.Whencombinedwithanencoder-decoderarchitec-tureandtrainedend-to-end,ourapproachisabletosynthesizenovelviewsofunseenindividualsandtoin-paintareasthatarenotvisiblefromthemaininputview.However,weobservedthatwhilethisapproachworkswellwithminimalocclusionsithasahardtimegeneratinghigh-qualityrenderingswhentherearesevereocclusions,eitherfromthepersonmovingtheirhandsinfrontoftheirbodyorifthey’reholdingvariousob-jects.Thus,weproposetoutilizeasingleadditionalocclusion-freeimageandwarpittothetargetnovelviewbyestablishingaccuratedensecorrespondencesbetweenthetwoinputs.Acompactnetworkcanbeusedforthispurpose,whichissufficienttorefinethefinalresultandgeneratetheoutputprediction.Wetraintheentirepipelineend-to-endusingphotometriclossesbetweenthegeneratedandground-truthpairofimages.Inaddition,weusestereoscopicrenderingtoencourageview-consistentresultsbetweenclose-byviewpoints.TotrainHVS-Net,werelyonhigh-qualitysyntheticscansofhu-Free-ViewpointRGB-DHumanPerformanceCaptureandRendering3mansthatweanimatedandrenderedfromvariousviews.Akeyfindingofourworkisthatitgeneralizesverywelltorealdatacapturedbya3dMDscannersystemwithalevelofdetailinthefaceortheclothesthatarenotseeninpriorworks[31,32,51].Insummary,thecontributionsofthisworkare:–Arobustsphere-basedsynthesisnetworkthatgeneralizestomultipleidentitieswith-outper-humanoptimization.–Arefinementmodulethatenhancestheself-occludedregionsoftheinitialestimatednovelviews.ThisisaccomplishedbyintroducinganovelyetsimpleapproachtoestablishdensesurfacecorrespondencesfortheclothedhumanbodythataddresseskeylimitationsofDensePosewhichisusuallyusedforthistask.–State-of-the-artresultsondynamichumanswearingvariousclothes,oraccessoriesandwithavarietyoffacialexpressionsofboth,syntheticandreal-captureddata.2RelatedWorkViewsynthesisfordynamicscenes,inparticularforhumans,isawell-establishedfieldthatprovidesthebasisforthiswork.Ourapproachbuildsonideasfrompoint-basedrendering,warping,andimage-basedrepresentations.ViewSynthesis.Forasurveyofearlyimage-basedrenderingmethods,wereferto[56,60].Oneofthefirstmethodstoworkwithvideointhisfieldispresentedin[9]andusesapre-recordedperformanceinamulti-viewcapturingsetuptocreatethefree-viewpointillusion.Zitnicketal.[70]similarlyuseamulti-viewcapturesetupforview-pointinterpolation.Theseapproachesinterpolatebetweenrecordedimagesorvideos.Ballanetal.[4]cointheterm‘video-basedrendering’:theyuseittointerpolatebetweenhand-heldcameraviewsofperformances.Thestronggenerativecapabilitiesofneuralnetworksenablefurtherextrapolationandrelaxationofconstraints[18,22,28,41].Zhouetal.[69]introduceMulti-PlaneImages(MPIs)forviewpointsynthesisanduseamodeltopredictthemfromlow-baselinestereoinputand[17,57]improveovertheoriginalbaselineandadditionallyworkwithcameraarraysandlightfields.Broxtonetal.[8]extendtheideatolayered,dynamicmeshesforimmersivevideoexperienceswhereasBansaletal.[5]usefreecameraviewpoints,butmultiplecameras.Withevenstrongerdeepneuralnetworkpriors,[64]performsviewpointextrapolationfromasin-gleview,butforstaticscenes,whereas[62,67]canworkwithasingleviewindynamicsettingswithlimitedmotion.Bemanaetal.[6]worksinstaticsettingsbutpredictsnotonlytheradiancefieldbutalsolightinggivenvaryingilluminationdata.Chibaneetal.[14]tradeinstantdepthpredictionsandsynthesisfortherequirementofmul-tipleimages.Alternatively,volumetricrepresentations[38,39]canalsobeingutilizedforcapturingdynamicscenes.Alltheseworksrequiresignificantcomputationtimeforoptimization,multipleviewsorofflineprocessingfortheentiresequence.3D&4DPerformanceCapture.Whiletheaforementionedworksareusuallyscene-agnostic,employingpriorknowledgecanhelpintheviewpointextrapolationtask:thishasbeenwellexploredintheareaof3D&4DHumanPerformanceCapture.AgreatoverviewofthedevelopmentoftheVirtualizedRealitysystemdevelopedatCMUinthe90sispresentedin[29].Itisoneofthefirstsuchsystemsandusesmultiplecamerasforfull4Dcapture.Startingfromthiswork,thereisacontinuouslineofworkrefining4P.Nguyenetal.andimprovingovermulti-viewcaptureofhumanperformances[1,15,35,70].Relighta-bles[21]usesagainamulti-camerasystemandaddscontrolledlightingtothecapturesetup,sothattheresultingreconstructedperformancescanbereplayedinnewlightingconditions.Theauthorsof[27]takeadifferentroute:theyfindawaytousebundlead-justmentfortriangulationoftracked3Dpointsandobtainresultswithsub-frametimeaccuracy.Broxtonetal.[8]isoneofthelatestsystemsforgeneral-purposeviewinter-polationandusesamulti-viewcapturesystemtocreatealayeredmeshrepresentationofthescene.Manyrecentworksapplyneuralradiancefields[42,65]torenderhumansatnovelviews.Lietal.[36]useasimilarmulti-viewcapturesystemtotrainadynamicNeuralRadianceField.Kwonetal.[31]learngeneralizableneuralradiancefieldsbasedonaparametrichumanbodymodeltoperformnovelviewsynthesis.However,thismethodfailstorenderhigh-qualityclothdetailsorfacialexpressionsofthehuman.Bothofthesesystemsusemultiplecamerasandareunabletotransmitperformanceinreal-time.Givenmulti-viewinputframesorvideos,recentworksonrenderingani-matehumansfromnovelviewsshowimpressiveresults[46,50,51,66].Howeversuchmethodscanbeprohibitivelyexpensivetorun([46]runsat1minute/frame)andcannotgeneralizetounseenhumansbutinsteadcreateadedicatedmodelforeachhumanthattheyneedtorender.HumanViewSynthesisusingRGB-D.Afewmethodshavebeenpublishedrecentlythattacklesimilarscenarios:LookingGood[40]re-rendersnovelviewpointsofacap-turedindividualgivenasingleRGB-Dinput.However,theircapturesetupproducesdensegeometrywhichmakesthisacomparativelyeasytask:thetargetviewsdonotdeviatesignificantlyfromtheinputviews.Arecentapproach[48]usesafrontalinputviewandalargenumberofcalibrationimagestoextrapolatenovelviews.Thismethodreliesonakeypointestimatortowarptheselectedcalibratedimagetothetargetpose,whichleadstounrealisticresultsforhands,occludedlimbs,orforlargebodyshapes.Point-basedRendering.WeassumeasingleinputRGB-Dsensorasadatasourceforourmethod.Thisnaturallyallowsustoworkwiththedepthdatainapoint-cloudformat.Tousethisforend-to-endoptimization,webuildontopofideasfromdif-ferentiablepointcloudrendering.Someofthefirstmethodsrenderedpointcloudsbyblendingdiscretesamplesusinglocalblurringkernels:[25,37,54].Usingthediffer-entiablepointcloudrenderingtogetherwithconvolutionalneuralnetworksnaturallyenablestheuseoflatentfeaturesandadeferredrenderinglayer,whichhasbeenex-ploredin[33,64].Recentworksonpoint-basedrendering[2,30]useapointrendererimplementedinOpenGL,thenuseaneuralnetworkimagespacetocreatenovelviews.Ruckertetal.[55]usepurelypixel-sizedpointsandfinitedifferencesforoptimiza-tion.WearedirectlybuildingonthesemethodsandusethePulsarrenderer[33]inourmethodtogetherwithanadditionalmodeltoimprovethepointclouddensity.WarpingRepresentations.Tocorrectlyrenderoccludedregions,wewarptherespec-tiveimageregionsfromanunoccludedposturetotherequiredposture.Debevecetal.[16]isoneofthefirstmethodstouse“projectivetexture-mapping”forviewsyn-thesis.Chaurasiaetal.[11]usesdepthsynthesisandlocalwarpstoimproveoverimage-basedrendering.Theauthorsof[19]takeviewsynthesisthroughwarpingtoitsextreme:theysolelyusewarpstocreatenovelviewsorsynthesizegaze.Recentmethods[45,53,61]use3DproxiestogetherwithwarpingandaCNNtogeneratenovelFree-ViewpointRGB-DHumanPerformanceCaptureandRendering5InputDepth-basedwarpingSynSin(point-based)Pulsar(sphere-based)GTFig.2.Comparisonof3Dpointcloudtransformations.FromasingleRGB-Dinput,weobtainthewarpedimageusing:adepth-basedwarpingtransformation[34,40],theneuralpoint-basedrendererSynSin[64]andtheneuralsphere-basedPulsarrenderer[33].ThenovelimagewarpedbyPulsarissignificantlydenser.views.Allthesemethodsrequireeithercreationofanexplicit3Dproxyfirst,oruseofimage-basedrendering.Instead,weusethedynamicper-framepointcloudtogetherwithapre-captured,unoccludedimagetowarpnecessaryinformationintothetargetviewduringonlineprocessing.3HVS-NetMethodologyThegoalofourmethodistocreaterealisticnovelviewsofahumancapturedbyasingleRGB-Dsensor(withsparsedepth,similartoalow-costRGB-Dcamera),asfaithfulandfastaspossible.Weassumethatthecameraparameterizationoftheviewtogenerateisknown.Still,thisposesseveralchallenges:1)theinformationweareworkingwithisincomplete,sincenotallregionsthatarevisiblefromthenovelviewcanbeobservedbytheRGB-Dsensor;2)occlusionaddsadditionalregionswithunknowninformation;3)eventhepixelsthatarecorrectlyobservedbytheoriginalsensoraresparseandex-hibitholeswhenviewedfromadifferentangle.Wetackletheaforementionedproblemsusinganend-to-endtrainableneuralnetworkwithtwocomponents.First,givenanRGB-DimageparameterizedasitstwocomponentsRGBIvandsparsedepthDvtakenfromtheinputviewv,asphere-basedviewsynthesismodelSproducesdensefeaturesofthetargetviewandrenderstheresultingRGBimagefromthetargetcameraviewusingaglobalcontextinpaintingnetworkG(seeSec.3.1).However,thisfirstnetworkcannotfullyresolveallocclusions:informationfromfullyoccludedregionsismissing(e.g.,renderingapatternonaT-shirtthatisoccludedbyahand).Toaccountforsuchcases,weoptionallyextendourmodelwithanenhancermoduleE(seeSec.3.2).Itusesinformationfromanunoccludedsnapshotofthesameperson,estimatesthedensecorrespondencesbetweenthepredictednovelviewandocclusion-freeinputview,andthenrefinethepredictedresult.6P.Nguyenetal.Input viewSparse depthFeature extractor FprojectionTarget camera TInput feature MTarget feature MtSphere-basedrenderer Ωx NGlobal Context Inpainting Model GPredicted Mask, Confidence and Novel ViewGTFast FourierConvolution BlocksrifiFig.3.Sphere-basedviewsynthesisnetworkarchitecture.ThefeaturepredictorFlearnsradiusandfeaturevectorsofthespheresetS.Wethenusethesphere-baseddifferentiablerendererΩtodensifythelearnedinputfeaturesMandwarpthemtothetargetcameraT.TheprojectedfeaturesMtarepassedthroughtheglobalcontextinpaintingmoduleGtogeneratetheforegroundmask,confidencemapandnovelimage.Brighterconfidencemapcolorsindicatelowerconfidence.3.1Sphere-basedViewSynthesisThegoalofthisfirstpartofourpipelineistorenderasparseRGB-Dviewofahumanasfaithfullyaspossiblefromadifferentperspective.Oftheaforementionedartifacts,itcanmostlydealwiththeinherentsparsityofspherescausedduetothedepthforeshortening:fromasingleviewpointintwoneighboringpixels,weonlygetasignalattheirtworespectivedepths—nomatterhowmuchtheydiffer.Thismeansthatforeverytwopixelsthathavealargedifferenceindepthandareseenfromtheside,largegapsoccur.Forrenderinghumansubjects,these“gaps”areoflimitedsize,andwecanaddresstheproblemtoacertainextentbyusingasphere-basedrendererforviewsynthesis.Sphere-basedrenderer.Giventhedepthofeverypixelfromtheoriginalviewpointaswellasthecameraparameters,thesepointscannaturallybeprojectedintoanovelview.Thismakestheuseofdepth-basedwarpingorofadifferentiablepoint-orsphere-rendereranaturalchoiceforthefirststepinthedevelopmentoftheviewsynthesismodel.Thebetterthisrenderercantransformtheinitialinformationintothenovelview,thebetter;thisprojectionstepisautomaticallycorrect(exceptforsensornoise)andnotsubjecttotrainingerrors.InFig.2,wecomparethedensityofthewarpedimagesfromasinglesparseRGB-Dinputusingthreedifferentmethods:depth-basedwarping[34],point-basedrender-ing[64]andsphere-basedrendering[33].Depthbasedwarping[34]representstheRGD-Dinputasasetofpixel-sized3Dpointsandthus,thecorrectlyprojectedpixelsinthenovelviewareverysensitivetothedensityoftheinputview.Thewidely-useddifferentiablepoint-basedrenderer[64]introducesaglobalradius-per-pointparameterwhichallowstoproduceasomewhatdenserimages.Sinceitusesthesameradiusforallpoints,thiscomes,however,withatrade-off:iftheradiusisselectedtoolarge,de-tailsindenseregionsoftheinputimagearelost;iftheradiusisselectedtoosmall,theresultingimagesgetsparserinsparseregions.Therecentlyintroduced,sphere-basedPulsarrenderer[33]notonlyprovidestheoptiontouseaper-sphereradiusparameter,butitalsoprovidesgradientsfortheseradiuses,whichenablesustosetthemdynam-ically.AsdepictedinFig.2,thisallowsustoproducedenserimagescomparedtotheothermethods.Fig.3showsanoverviewoftheoverallarchitectureofourmethod.Inafirststep,weuseashallowsetofconvolutionallayersFtoencodetheinputimageIvtoad-dimensionalfeaturemapM=F(Iv).Fromthisfeaturemap,wecreateasphererepresentationthatcanberenderedusingthePulsarrenderer.ThismeansthatFree-ViewpointRGB-DHumanPerformanceCaptureandRendering7wehavetofindpositionpi,featurevectorfi,andradiusriforeveryspherei∈1,..,NwhenusingNspheres(forfurtherdetailsabouttherenderingstep,wereferto[33]).Thespherepositionspicantriviallybeinferredfromcameraparameters,pixelindexanddepthforeachofthepixels.WechoosethefeaturesfiasthevaluesofMattherespectivepixelposition;weinferribypassingMtoanotherconvolutionlayerwithasigmoidactivationfunctiontobounditsrange.Thisleadstoanas-dense-as-possibleprojectionoffeaturesintothetargetview,whichisthebasisforthefollowingsteps.Globalcontextinpainitingmodel.Next,theprojectedfeaturesareconvertedtothefi-nalimage.Thisremainsachallengingproblemsinceseveral“gaps”inthere-projectedfeatureimagesMtcannotbeavoided.Toaddressthis,wedesignanefficientencoder-decoder-basedinpaintingmodelGtoproducethefinalrenders.Theencodingbottle-neckseverelyincreasesthereceptivefieldsizeofthemodel,whichinturnallowsittocorrectlyfillinmoreofthemissinginformation.Additionally,weemployaseriesofFastFourierConvolutions(FFC)[13]totakeintoaccounttheimage-widereceptivefield.Themodelisabletohallucinatemissingpixelsmuchmoreaccuratelycomparedtoregularconvolutionlayers[58].PhotometricLosses.Thesphere-basedviewsynthesisnetworkSnotonlypredictsanRGBimageIpofthetargetview,butalsoaforegroundmaskImandaconfidencemapIcwhichcanbeusedforcompositinganderrorcorrection,respectively.Wethenmultiplythepredictedforegroundmaskandconfidencemapwiththepredictednovelimage:Ip=Ip∗Im∗Ic.However,animperfectmaskImmaybiasthenetworktowardsunimportantareas.Therefore,wepredictaconfidencemaskIcasaside-productoftheGnetworktodynamicallyassignlessweightto“easy”pixels,whereas“hard”pixelsgethigherimportance[40].Alloftheaforementionedmodelcomponentsaretrainedend-to-endusingthepho-tometriclossLphoto,whichisdefinedas:Lphoto=Li+Lm.Liisthecombinationofanℓ1,perceptual[12]andhingeGAN[20]lossbetweentheestimatednewviewIpandtheground-truthimageIGT.Lmisthebinarycross-entropylossbetweenthepredictedandground-truthforegroundmask.Wefoundthatthislossencouragesthemodeltopredictsharpcontoursinthenovelimage.Thetwolossesleadtohigh-qualityrecon-structionresultsforsingleimages.However,wenotethatstereoscopicrenderingofnovelviewsrequiresmatchingleftandrightimagesforbothviews.Whereastheabovelossesleadtoplausiblereconstructions,theydonotnecessarilyleadtosufficientlycon-sistentreconstructionsforclose-byviewpoints.Wefoundatwo-stepstrategytoaddressthisissue:1)Insteadofpredictinganovelimageofasingleviewpoint,wetrainthemodeltopredicttwonearbynovelviews.Toobtainperfectlyconsistentdepthbetweenbothviews,weusethewarpingoperatorWfrom[26]towarpthepredictedimageandthedepthfromonetothenearbypairedviewpoint.2)Inthesecondstep,wedefineamulti-viewconsistencylossLcas:Lc=||ILp−W(IRp)||1,(1)whereILpandIRparepredictedleftandrightnovelviews.Withthis,wedefinethephotometriclossasfollows:Lphoto=Li+0.5×Lm+0.5×Lc.(2)8P.Nguyenetal.An AdditionalPose-free InputAuto-encoderSPADEResblocksIUV warpingSphere-based View Synthesis SEstimated Novel ViewEstimated HD-IUVRefined Novel ViewWarped InputGround-truth+Input ViewIUVPredictorFig.4.IUV-basedimagerefinement.Usinganadditionalocclusion-freeinput,werefinetheinitialestimatednovelviewbytrainingtheEnhancernetworkE.Weinferthedensecorrespondencesofboth,predictednovelviewandocclusion-freeimage,usinganovelHD-IUVmodule.Theocclusion-freeimageiswarpedtothetargetviewandthenrefinedbyanauto-encoder.Therefinednovelviewshowscrisperresultsontheoccludedareacomparedtotheinitiallyestimatedrender.3.2HandlingOcclusionsThesphere-basedviewsynthesisnetworkSpredictsplausiblenovelviewswithhighquality.However,ifthepersonisholdinganobjectsuchasawallet(c.t.Fig.4)oriftheirhandsareobstructinglargepartsoftheirtorso,thenthewarpedtransformationwillresultinmissingpointsinthisregion(asdiscussedinFig2).Thisleadstolow-fidelitytextureestimatesforthoseoccludedregionswhenperformingnovelviewsynthesiswithatargetcamerathatisnotclosetotheinputview.Hence,tofurtherenhancethequalityofthenovelviews,weintroducetwoadditionalmodules:i)anHD-IUVpredictorDtopredictdensecorrespondencesbetweenanRGBimage(renderofahuman)andthe3Dsurfaceofahumanbodytemplate,andii)arefinementmoduleRtowarpanadditionalocclusion-freeinput(e.g.,aselfieinapracticalapplication)tothetargetcameraandenhancetheinitialestimatednovelviewtotacklingtheself-occlusionissue.HD-IUVPredictorD.WefirstestimatearepresentationthatmapsanRGBimageofahumantothe3Dsurfaceofabodytemplate[24,43,44,59].OnecoulduseDense-Pose[43]forthistaskbuttheestimatedIUV(whereIreflectsthebodypart)predictionscoveronlythenakedbodyinsteadoftheclothedhumanandareinaccurateastheyaretrainedbasedonsparseandnoisyhumanannotations.Instead,webuildourownIUVpredictorandtrainitonsyntheticdataforwhichwecanobtainaccurateground-truthcorrespondences.WithpairsofsyntheticRGBimagesandground-truthdensesurfacecorrespondences,wetrainaUNet-likenetworkthatprovidesdensesurface(i.e.IUV)estimatesforeachpixeloftheclothedhumanbody.Foreachpixelpintheforegroundimage,wepredict3-channeled(RGB)colorp′whichrepresentsthecorrespondence(thecolorsinsucharepresentationareuniquewhichmakessubsequentwarpingeasy).Thus,wetreatthewholeproblemasamulti-taskclassificationproblemwhereeachtask(predictionsfortheI,U,andVchannels)istrainedwiththefollowingsetoflosses:a)multi-taskclassificationlossforeachofthe3channels(per-pixelclassificationlabel)andb)silhouetteloss.InFig.5weshowthat,unlikeDensePose,theproposedHD-IUVmoduleaccuratelyestablishesfinelevelcorrespondencesforthefaceandhandregionsFree-ViewpointRGB-DHumanPerformanceCaptureandRendering9Input imageAdditional pose-free inputWarped image using HD-IUVRefined image(target view)Estimated HD-IUV (ours)Ground-truth IUVEstimatedIUVusing DensePoseWarped image using DensePoseFig.5.Densecorrespondencevisualization.TexturewarpingwithDensePoseresultsininaccu-rateanddistortedimagesinthetargetviewduetoincorrectIUVestimates(enhancedbythefactthatittargetsthenakedbody).OurproposedHD-IUVrepresentationcoversthehumanbodyin-cludingclothing,capturesfacialandhanddetailswithhighaccuracy,andresultsinlessdistortedrenderingsinthetargetview.Westackthiswarpedimagewiththeinitiallyestimatedtarget-viewsynthesizedimageandprovideitasinputfortheEnhancernetworktoobtainthefinalresults.whilecapturingthewholeclothedhumanandthusmakingitapplicableforsuchappli-cations.Oncethismodelispre-trained,wemergeitwiththerestofthepipelineandcontinuethetrainingprocedurebyusingtheinitiallyestimatednovelviewIpadaninputtoanencoder-decoderarchitecturethatcontainsthreepredictionheads(fortheI,U,andVchannels).Anin-depthdiscussiononthedatageneration,networkdesign,andtrainingisprovidedinthesupplementarymaterial.WarpingRepresentationsandViewRefinement.ThepredictedHD-IUVinisolationwouldnotbeusefulforthetaskofhumanviewsynthesis.However,whenusedalongwiththeocclusion-freeRGBinput,itallowsustowarpallvisiblepixelstothehumaninthetargetcameraTandobtainapartialwarpedimageIw.Forrealapplicationsthisocclusion-freeinputcanbeaselfieimage—therearenospecificrequirementstothebodyposefortheimage.InFig.5wecompareDensePoseresultswiththeproposedHD-IUVmodule.DensePoseclearlyproduceslessaccurateandmoredistortedtextures.Inthenextstep,westackIpandIwandpasstheresultingtensortoarefinementmodule.Thismoduleaddressestwokeydetails:a)itlearnstoberobusttoartifactsthatareoriginatingeitherfromtheoccludedregionsoftheinitiallysynthesizednovelviewaswellastextureartifactsthatmightappearduetothefactthatwerelyonHD-IUVdensecorrespondencesforwarpingandb)itiscapableofsynthesizingcrisperresultsintheoccludedregionsasitreliesonboththeinitiallysynthesizedimageaswellasthewarpedimagetothetargetviewbasedonHD-IUV.TherefinementmoduleistrainedusingthephotometriclossLphotobetweentherefinednovelimagesandgroundtruths.Alldetailsregardingtrainingandimagewarping,aswellasthefullnetworkarchitecture,canbefoundinthesupplementarymaterial.10P.Nguyenetal.4ExperimentsDatasets.Theproposedapproachistrainedsolelyonsyntheticdataandevaluatedquan-titativelyandqualitativelyonboth,syntheticandrealdata.Fortraining,weusetheRenderPeopledataset[52],whichhasbeenusedextensively[3,7,10,23,47,49,59]forhumanreconstructionandgenerationtasks.Overall,weuseasubsetof1000watertightmeshesofpeoplewearingavarietyofgarmentsandinsomecasesholdingobjectssuchasmugs,bagsormobilephones.Whereasthiscoversavarietyofpersonalappearancesandobjectinteraction,allofthesemeshesarestatic—thecoverageoftheposespaceislacking.Hence,weaugmentthedatasetbyintroducingadditionalposevariations:weperformnon-rigidregistrationforallmeshes,rigthemforanimationanduseasetofpre-definedmotionstoanimatethem.Withthissetofmeshesandanimations,weareabletoassembleasetofhigh-qualityground-truthRGB-Drendersaswellastheircor-respondingIUVmapsfor25viewsperframeusingBlender.Weusea90/10train/testsplitbasedonidentitiestoevaluatewhetherourmodelcangeneralizewelltounseenindividuals.Inadditiontothesynthetictestset,wealsoassembleareal-worldtestdatasetcon-sistingof3dMD4Dscansofpeopleinmotion.The3dMD4Dscannerisafull-bodyscannerthatcapturesunregisteredvolumetricpointcloudsat60Hz.Weusethisdatasetsolelyfortestingtoinvestigatehowwellourmethodhandlesthedomaingapbetweensyntheticandrealdata.The3dMDdatadoesnotincludeobjectinteractions,butisgenerallynoisierandhascomplexfacialexpressions.Tosummarize:ourtrainingsetcomprises950staticscansintheiroriginalposeand∼10000posedscansafteranima-tion.Ourtestsetincludes50staticunseenidentitiesalongwith1000animatedrendersand3000framesoftwohumanscapturedwitha3dMDfull-bodyscanner.NovelViewpointRange.Weassumeascenariowithacameraviewpointatalowerlevelinfrontofaperson(e.g.,thecamerasittingonadeskinfrontoftheuser).ThisisamorechallengingscenariothanLookingGood[40]orVolumetricCapture[48]use,butalsoarealisticone:itcorrespondstoeverydayvideoconferencesettings.Atthesametime,thetargetcameraismovingfreelyinthefrontalhemispherearoundtheperson(Pitch&Roll:[−45o,45o],Lx:[−1.8m,1.8m],Ly:[1.8m,2.7m],Lz:[0.1m,2.7m]inaBlendercoordinatesystem).Thus,theviewpointrangeissignificantlylargerperinputviewthaninpriorwork.Baselines.Inthisevaluation,wecompareourapproachtotwonovelviewsynthesisbaselinesbycomparingtheperformanceingeneratingsingle,novel-viewRGBimages.ToevaluatethegeneralizationofHVS-Net,wecompareitwithLookingGood[40].SincethereisnoavailablesourcecodeofLookingGood,wereimplementedthemethodforthiscomparisonandvalidatedinvarioussyntheticandreal-worldsettingsthatthisimplementationisqualitativelyequivalenttowhatisreportedintheoriginalpaper(weincludecomparisonimagesinthesupp.mat.).WefollowedthestereosetupofLooking-Goodanduseadensedepthmaptopredictthenovelviews.Furthermore,wecompareHVS-NetwiththerecentlyproposedviewsynthesismethodSynSin[64],whichesti-matesmonoculardepthusingadepthpredictor.Tocreatefairevaluationconditions,wereplacethisdepthpredictorandeitherprovidedenseorsparsedepthmapsasin-putsdirectly.Whilethereareseveralrecentlyproposedmethodsinthetopicofhuman-viewsynthesis;almostallarerelyingoneitherproprietarydatacapturedinlabenviron-Free-ViewpointRGB-DHumanPerformanceCaptureandRendering11InputGTLookingGoodSynSinOursFig.6.Qualitativecomparison.ExamplesofgeneratednovelviewsbyHVS-Netandstate-of-the-artmethodsonthetestsetoftheRenderPeople[52]dataset.Asopposedtoallothermethods,LookingGood[40]usesdenseinputdepth.ments[48],multi-viewinputstreams[31,36,51,66]andmostimportantlynoneoftheseworkscangeneralizetonewhumanidentities(orforthecaseofNeuralBody[51]notevennewposes)attestingtimewhichourproposedHVS-Netcanaccomplish.Further-more,inferringnewviewsinareal-timemannerisfarfromsolvedformosttheseworks.Incontrast,ourmethodfocusesmoreonapracticalapproachofsingleviewsynthesis,aimingtogeneralizetonewidentitiesandunseenposeswhilebeingfastatinferencetime.HencewesticktoperformingquantitativecomparisonsagainstLookingGood[40]andSynSin[64]andwedonotcompareitwithNeRF-basedapproaches[31,36,51,66]assuchcomparisonsarenotapplicable.Metrics.WereportthePSNR,SSIM,andperceptualsimilarity(LPIPS)[68]ofviewsynthesisbetweenHVS-Netandotherstate-of-the-artmethods.4.1ResultsInTab.1andFig.6,wesummarizethequantitativeandqualitativeresultsforsamplesfromtheRenderPeopledataset.WefirstcomparethefullmodelHVS-NetagainstavariantHVS-Net†,whichutilizesadensemapasaninput.WeobservenosignificantdifferencesbetweenthepredictednovelviewsproducedbyHVS-Netwhentrainedus-ingeithersparseordensedepthinput.Thisconfirmstheeffectivenessofthesphereradiuspredictor:itmakesHVS-Netmorerobustw.r.t.inputpointclouddensity.Inanextstep,weevaluateHVS-Netagainstthecurrenttopperformingsingleviewhumansynthesismethods[40,64],whichdonotrequireper-subjectfinetuning.Even12P.Nguyenetal.MethodRenderPeople(static)RenderPeople(animated)Real3dMDDataLPIPS↓SSIM↑PSNR↑LPIPS↓SSIM↑PSNR↑LPIPS↓SSIM↑PSNR↑LookingGood†[40]0.240.92525.320.250.91224.530.290.86325.12SynSin†[64]0.310.85124.180.350.93723.640.350.93722.18SynSin[64]0.520.82422.450.550.85320.860.650.81919.92HVS-Net(w/oEnhancer)0.180.98628.540.190.92626.240.200.91026.25HVS-Net†0.140.98628.560.170.95827.410.200.91826.47HVS-Net0.150.98628.540.170.95527.450.200.91826.47Table1.Quantitativeresultsonsyntheticandrealimages.Foralldatasets,themetricsareav-eragedacrossallviews.Methodswitha†symbolareusingdenseinputdepth.BothHVS-NetandHVS-Net†achievethebestresultscomparedtootherviewsynthesismethods.WeobserveaslightdropofperformancewithoutusingtheproposedEnhancermodule.Input PredictionFig.7.Generalizationtoreal-worldexamples.Ourmethodgeneralizeswelltoreal-world4Ddataandshowsrobustnessw.r.ttodifferenttargetposes.TheseresultsareproducedusingHVS-Net,trainedsolelyonsyntheticdatawithoutfurtherfine-tuning.thoughweusedensedepthmapsasinputtoLookingGood†[40],themethodstillstrug-glestoproducerealisticresultsifthetargetposedeviatessignificantlyfromtheinputviewpoint.Inthe1strowofFig.6,LookingGood†[40]alsostrugglestorecovercleanandaccuratetexturesoftheoccludedregionsbehindthehandsoftheperson.AlthoughbothSynSin[64]andHVS-Netutilizethesamesparsedepthinput,therenderedtargetimagesarenotablydifferent.SynSin[64]notonlyperformspoorlyontheoccludedregionsbutalsoproducesartifactsaroundtheneckoftheperson,visibleinthe2ndrowofFig.6.Incontrast,ourmethodisnotonlyabletorenderplausibleandrealisticnovelviews,butcreatesthemalsofaithfulw.r.t.theinputviews.NoticethatHVS-Netisabletopredictfairlyaccuratehairforbothsubjectsgivenverylittleinformation.Inalastexperiment,wetestthegeneralizationabilityofourmethodonreal-world4Ddata,showninFig.7.Beingtrainedonlyonsyntheticdata,thisrequiresgeneraliza-tiontonovelidentity,novelposes,andbridgingthedomaingap.Inthe4Dscans,thesubjectsareabletomovefreelywithinthecapturevolume.Weuseafixed,virtual3DsensorpositiontocreatethesparseRGB-DinputstreamforHVS-Net.Theinputcam-eraisplacednearthefeetofthesubjectsandisfacingup.AscanbeseeninFig.1andFig.7,HVS-Netisstillabletoperformnovelviewsynthesiswithhighquality.Despiteusingsparseinputdepth,ourmethodisabletorenderrealistictexturesontheclothesofbothsubjects.Inaddition,facialexpressionssuchasopeningthemouthorsmilingFree-ViewpointRGB-DHumanPerformanceCaptureandRendering13MethodVariantLPIPS↓SSIM↑PSNR↑NoSphereRepres.0.220.93426.15NoGlobalContext0.210.95426.82NoEnhancer0.180.96727.92HVS-Net(full)0.150.98628.54Inputdepth(%)Run-time↑(fps)LPIPS↓SSIM↑PSNR↑5250.170.98528.2710220.150.98628.5425210.140.98628.55100200.140.98628.56Table2.Left:Ablationstudy.ReconstructionaccuracyontheRenderPeopletestingset.Right:Reconstructionaccuracyandinferencespeedusingdifferentlevelsofinputdepthsparsity.arealsowell-reconstructed,despitethefactthatthestaticoranimatedscansusedtotrainournetworkdidnothaveavarietyoffacialexpressions.ThequalityoftheresultsobtainedinFig.7demonstratesthatourapproachcanrenderhigh-fidelitynovelviewsofrealhumansinmotion.Weobservethatthegeneratednovelviewsarealsotempo-rallyconsistentacrossdifferenttargetviewtrajectories.Foradditionalresultsandvideoexamples,werefertothesupplementarymaterial.4.2AblationStudiesandDiscussionModelDesign.Tab.2(left)andFig.8summarizethequantitativeandqualitativeper-formancefordifferentmodelvariantsonthetestsetoftheRenderPeopledataset[52].HVS-Netwithoutthesphere-basedrepresentationdoesnotproduceplausibletargetviews(see,forexample,therenderedface,whichisblurrycomparedtothefullmodel).Thisisduetothehighlevelofsparsityoftheinputdepth,whichleadstoaharderinpaintingproblemfortheneuralnetworkthataddressesthistask.ReplacingtheFastFourierConvolutionresidualblocksoftheglobalcontextinpaintingmodelwithregularconvolutionlayersleadstoadropinrenderqualityintheoccludedregion(redbox).Usingtheproposedmodelarchitecture,butwithouttheenhancer(5thcolumnofFig.8)leadstoalossofdetailintexture.Incontrast,thefullproposedmodelusingtheEn-hancernetworkrendersthelogoaccurately.Notethatthislogoiscompletelyoccludedbythehuman’shandssoitisnon-trivialtorenderthelogousingasingleinputimage.SparseDepthRobustness.InFig.9,weshownovelviewsynthesisresultsusingdiffer-entlevelsofsparsityoftheinputdepthmaps.WefirstrandomlysampleseveralversionsofthesparseinputdepthandHVS-Nettoprocessthem.Ourmethodisabletomain-tainthequalityofviewsynthesisdespitestrongreductionsinpointclouddensity.Thishighlightstheimportanceoftheproposedsphere-basedrenderingcomponentandtheenhancermodule.AscanbeseeninTab.2(right),weobserveaslightdropofperfor-mancewhenusing5%or10%oftheinputmaps.Tobalancebetweenvisualqualityandrenderingspeed,wesuggestthatusing25%oftheinputdepthdataissufficienttoachievesimilarresultscomparedtousingthefulldata.InferenceSpeed.ForAR/VRapplications,aprimetargetforamethodliketheonepro-posed,runtimeperformanceiscritical.Attesttime,HVS-Netgenerates1024×1024imagesat21FPSusingasingleNVIDIAV100GPU.Thisspeedcanbefurtherin-creasedwithmoreefficientdataloadersandanoptimizedimplementationthatusesthe14P.Nguyenetal.GTNo sphere-basedrepresentationNo global contextauto-encoderNo EnhancerFull ModelInputAdditionalpose-free inputFig.8.Qualitativeablationstudy.Comparisonoftheground-truthwithpredictednovelviewsbyseveralvariantsoftheproposedHVS-Net.(a) 5% foreground points(b) 10% foreground points(c) 25% foreground pointsFig.9.HVS-Netsparsityrobustness.Werandomlysample(a)5%,(b)10%and(c)25%ofdensedepthpointsasinputdepthmapanduseitasaninputforHVS-Nettopredictnovelviews.ThetextintheT-shirtisreconstructedathigh-fidelitywith25%ofthedepthpointsutilized.NVIDIATensorRTengine.Finally,differentdepthsparsitylevelsdonotsignificantlyaffecttheaverageruntimeofHVS-Net,whichisapluscomparedtopriorwork.5ConclusionWepresentedHVS-Net,amethodthatperformsnovelviewsynthesisofhumansinmo-tiongivenasingle,sparseRGB-Dsource.HVS-Netusesasphere-basedviewsynthesismodelthatproducesdensefeaturesofthetargetview;thesearethenutilizedalongwithanautoencodertocompletethemissingdetailsofthetargetviewpoints.Toaccountforheavilyoccludedregions,weproposeanenhancermodulethatusesanadditionalunoc-cludedviewofthehumantoprovideadditionalinformationandproducehigh-qualityresultsbasedonannovelIUVmapping.Ourapproachgenerateshigh-fidelityrendersatnewviewsofunseenhumansinvariousnewposesandcanfaithfullycaptureandrenderfacialexpressionsthatwerenotpresentintraining.Thisisespeciallyremark-able,sincewetrainHVS-Netonlyonsyntheticdata;yetitachieveshigh-qualityresultsacrosssyntheticandreal-worldexamples.Acknowledgements:TheauthorswouldliketothankAlbertParaPozzo,SamJohnsonandRonaldMalletfortheinitialdiscussionsrelatedtotheproject.Free-ViewpointRGB-DHumanPerformanceCaptureandRendering15References1.deAguiar,E.,Stoll,C.,Theobalt,C.,Ahmed,N.,Seidel,H.P.,Thrun,S.:Performancecap-turefromsparsemulti-viewvideo.TOG(2008)42.Aliev,K.A.,Sevastopolsky,A.,Kolos,M.,Ulyanov,D.,Lempitsky,V.:Neuralpoint-basedgraphics.In:ECCV(2020)43.Alldieck,T.,Pons-Moll,G.,Theobalt,C.,Magnor,M.:Tex2shape:Detailedfullhumanbodygeometryfromasingleimage.In:ICCV(2019)104.Ballan,L.,Brostow,G.J.,Puwein,J.,Pollefeys,M.:Unstructuredvideo-basedrendering:Interactiveexplorationofcasuallycapturedvideos.In:SIGGRAPH(2010)35.Bansal,A.,Vo,M.,Sheikh,Y.,Ramanan,D.,Narasimhan,S.:4dvisualizationofdynamiceventsfromunconstrainedmulti-viewvideos.In:CVPR(2020)1,36.Bemana,M.,Myszkowski,K.,Seidel,H.P.,Ritschel,T.:X-fields:Implicitneuralview-,light-andtime-imageinterpolation.In:SIGGRAPHAsia(2020)37.Bhatnagar,B.L.,Tiwari,G.,Theobalt,C.,Pons-Moll,G.:Multi-garmentnet:Learningtodress3Dpeoplefromimages.In:ICCV(2019)108.Broxton,M.,Flynn,J.,Overbeck,R.,Erickson,D.,Hedman,P.,Duvall,M.,Dourgarian,J.,Busch,J.,Whalen,M.,Debevec,P.:Immersivelightfieldvideowithalayeredmeshrepresentation.TOG(2020)3,49.Carranza,J.,Theobalt,C.,Magnor,M.A.,Seidel,H.P.:Free-viewpointvideoofhumanac-tors.TOG(2003)310.Chaudhuri,B.,Sarafianos,N.,Shapiro,L.,Tung,T.:Semi-supervisedsynthesisofhigh-resolutioneditabletexturesfor3dhumans.In:CVPR(2021)1011.Chaurasia,G.,Duchene,S.,Sorkine-Hornung,O.,Drettakis,G.:Depthsynthesisandlocalwarpsforplausibleimage-basednavigation.TOG(2013)412.Chen,Q.,Koltun,V.:Photographicimagesynthesiswithcascadedrefinementnetworks.In:ICCV(2017)713.Chi,L.,Jiang,B.,Mu,Y.:Fastfourierconvolution.In:NeurIPS(2020)714.Chibane,J.,Bansal,A.,Lazova,V.,Pons-Moll,G.:Stereoradiancefields(srf):Learningviewsynthesisfromsparseviewsofnovelscenes.In:CVPR(2021)315.Collet,A.,Chuang,M.,Sweeney,P.,Gillett,D.,Evseev,D.,Calabrese,D.,Hoppe,H.,Kirk,A.,Sullivan,S.:High-qualitystreamablefree-viewpointvideo.TOG(2015)416.Debevec,P.,Yu,Y.,Borshukov,G.:EfficientView-DependentImage-BasedRenderingwithProjectiveTexture-Mapping.EurographicsRenderingWorkshop(1998)417.Flynn,J.,Broxton,M.,Debevec,P.,Duvall,M.,Fyffe,G.,Overbeck,R.,Snavely,N.,Tucker,R.:Deepview:Viewsynthesiswithlearnedgradientdescent.In:CVPR(2019)318.Flynn,J.,Neulander,I.,Philbin,J.,Snavely,N.:Deepstereo:Learningtopredictnewviewsfromtheworld’simagery.In:CVPR(2016)319.Ganin,Y.,Kononenko,D.,Sungatullina,D.,Lempitsky,V.S.:Deepwarp:Photorealisticim-ageresynthesisforgazemanipulation.In:ECCV(2016)420.Goodfellow,I.,Pouget-Abadie,J.,Mirza,M.,Xu,B.,Warde-Farley,D.,Ozair,S.,Courville,A.,Bengio,Y.:Generativeadversarialnets.In:NeurIPS(2014)721.Guo,K.,Lincoln,P.,Davidson,P.,Busch,J.,Yu,X.,Whalen,M.,Harvey,G.,Orts-Escolano,S.,Pandey,R.,Dourgarian,J.,Tang,D.,Tkach,A.,Kowdle,A.,Cooper,E.,Dou,M.,Fanello,S.,Fyffe,G.,Rhemann,C.,Taylor,J.,Debevec,P.,Izadi,S.:Therelightables:Volumetricperformancecaptureofhumanswithrealisticrelighting.TOG(2019)422.Huang,Z.,Li,T.,Chen,W.,Zhao,Y.,Xing,J.,LeGendre,C.,Luo,L.,Ma,C.,Li,H.:Deepvolumetricvideofromverysparsemulti-viewperformancecapture.In:ECCV(2018)323.Huang,Z.,Xu,Y.,Lassner,C.,Li,H.,Tung,T.:ARCH:Animatablereconstructionofclothedhumans.In:CVPR(2020)1016P.Nguyenetal.24.Ianina,A.,Sarafianos,N.,Xu,Y.,Rocco,I.,Tung,T.:BodyMap:Learningfull-bodydensecorrespondencemap.In:CVPR(2022)825.Insafutdinov,E.,Dosovitskiy,A.:Unsupervisedlearningofshapeandposewithdifferen-tiablepointclouds.In:NeurIPS(2018)426.Jaderberg,M.,Simonyan,K.,Zisserman,A.,Kavukcuoglu,K.:Spatialtransformernet-works.In:NeurIPS(2015)727.Joo,H.,Liu,H.,Tan,L.,Gui,L.,Nabbe,B.,Matthews,I.,Kanade,T.,Nobuhara,S.,Sheikh,Y.:Panopticstudio:Amassivelymultiviewsystemforsocialmotioncapture.In:ICCV(2015)428.Kalantari,N.K.,Wang,T.C.,Ramamoorthi,R.:Learning-basedviewsynthesisforlightfieldcameras.TOG(2016)329.Kanade,T.,Rander,P.,Narayanan,P.:Virtualizedreality:constructingvirtualworldsfromrealscenes.IEEEMultiMedia(1997)330.Kopanas,G.,Philip,J.,Leimk¨uhler,T.,Drettakis,G.:Point-basedneuralrenderingwithper-viewoptimization.ComputerGraphicsForum(2021)431.Kwon,Y.,Kim,D.,Ceylan,D.,Fuchs,H.:Neuralhumanperformer:Learninggeneralizableradiancefieldsforhumanperformancerendering.In:NeurIPS(2021)3,4,1132.Kwon,Y.,Petrangeli,S.,Kim,D.,Wang,H.,Park,E.,Swaminathan,V.,Fuchs,H.:Rotationally-temporallyconsistentnovelviewsynthesisofhumanperformancevideo.In:ECCV(2020)333.Lassner,C.,Zollhofer,M.:Pulsar:Efficientsphere-basedneuralrendering.In:CVPR(2021)4,5,6,734.Le,H.A.,Mensink,T.,Das,P.,Gevers,T.:Novelviewsynthesisfromasingleimageviapointcloudtransformation.In:BMVC(2020)5,635.Li,H.,Luo,L.,Vlasic,D.,Peers,P.,Popovi´c,J.,Pauly,M.,Rusinkiewicz,S.:Temporallycoherentcompletionofdynamicshapes.TOG(2012)436.Li,T.,Slavcheva,M.,Zollh¨ofer,M.,Green,S.,Lassner,C.,Kim,C.,Schmidt,T.,Lovegrove,S.,Goesele,M.,Lv,Z.:Neural3dvideosynthesis.In:CVPR(2021)4,1137.Lin,C.H.,Kong,C.,Lucey,S.:Learningefficientpointcloudgenerationfordense3dobjectreconstruction.In:AAAI(2018)438.Lombardi,S.,Simon,T.,Saragih,J.,Schwartz,G.,Lehrmann,A.,Sheikh,Y.:Neuralvol-umes:Learningdynamicrenderablevolumesfromimages.TOG(2019)339.Lombardi,S.,Simon,T.,Schwartz,G.,Zollhoefer,M.,Sheikh,Y.,Saragih,J.:Mixtureofvolumetricprimitivesforefficientneuralrendering.TOG(2021)340.Martin-Brualla,R.,Pandey,R.,Yang,S.,Pidlypenskyi,P.,Taylor,J.,Valentin,J.,Khamis,S.,Davidson,P.,Tkach,A.,Lincoln,P.,Kowdle,A.,Rhemann,C.,Goldman,D.B.,Keskin,C.,Seitz,S.,Izadi,S.,Fanello,S.:Lookingood:Enhancingperformancecapturewithreal-timeneuralre-rendering.TOG(2018)4,5,7,10,11,1241.Meshry,M.,Goldman,D.B.,Khamis,S.,Hoppe,H.,Pandey,R.,Snavely,N.,Martin-Brualla,R.:Neuralrerenderinginthewild.In:CVPR(2019)342.Mildenhall,B.,Srinivasan,P.P.,Tancik,M.,Barron,J.T.,Ramamoorthi,R.,Ng,R.:Nerf:Representingscenesasneuralradiancefieldsforviewsynthesis.In:ECCV(2020)1,443.Neverova,N.,AlpGuler,R.,Kokkinos,I.:Denseposetransfer.In:ECCV(2018)844.Neverova,N.,Novotny,D.,Khalidov,V.,Szafraniec,M.,Labatut,P.,Vedaldi,A.:Continuoussurfaceembeddings.In:NeurIPS(2020)845.Nguyen,P.,Karnewar,A.,Huynh,L.,Rahtu,E.,Matas,J.,Heikkila,J.:Rgbd-net:Predictingcoloranddepthimagesfornovelviewssynthesis.In:3DV(2021)1,446.Noguchi,A.,Sun,X.,Lin,S.,Harada,T.:Neuralarticulatedradiancefield.In:ICCV(2021)1,447.Palafox,P.,Sarafianos,N.,Tung,T.,Dai,A.:SPAMs:Structuredimplicitparametricmodels.In:CVPR(2022)10Free-ViewpointRGB-DHumanPerformanceCaptureandRendering1748.Pandey,R.,Keskin,C.,Izadi,S.,Fanello,S.,Tkach,A.,Yang,S.,Pidlypenskyi,P.,Taylor,J.,Martin-Brualla,R.,Tagliasacchi,A.,Papandreou,G.,Davidson,P.:Volumetriccaptureofhumanswithasinglergbdcameraviasemi-parametriclearning.In:CVPR(2019)4,10,1149.Patel,P.,Huang,C.H.P.,Tesch,J.,Hoffmann,D.T.,Tripathi,S.,Black,M.J.:AGORA:Avatarsingeographyoptimizedforregressionanalysis.In:CVPR(2021)1050.Peng,S.,Dong,J.,Wang,Q.,Zhang,S.,Shuai,Q.,Zhou,X.,Bao,H.:Animatableneuralradiancefieldsformodelingdynamichumanbodies.In:ICCV(2021)1,451.Peng,S.,Zhang,Y.,Xu,Y.,Wang,Q.,Shuai,Q.,Bao,H.,Zhou,X.:Neuralbody:Implicitneuralrepresentationswithstructuredlatentcodesfornovelviewsynthesisofdynamichu-mans.In:CVPR(2021)1,3,4,1152.RenderPeople:http://renderpeople.com/10,11,1353.Riegler,G.,Koltun,V.:Freeviewsynthesis.In:ECCV(2020)454.Roveri,R.,Rahmann,L.,Oztireli,C.,Gross,M.:Anetworkarchitectureforpointcloudclassificationviaautomaticdepthimagesgeneration.In:CVPR(2018)455.R¨uckert,D.,Franke,L.,Stamminger,M.:Adop:Approximatedifferentiableone-pixelpointrendering.arXivpreprintarXiv:2110.06635(2021)456.Shum,H.,Kang,S.B.:Reviewofimage-basedrenderingtechniques.In:VisualCommuni-cationsandImageProcessing(2000)357.Srinivasan,P.P.,Tucker,R.,Barron,J.T.,Ramamoorthi,R.,Ng,R.,Snavely,N.:Pushingtheboundariesofviewextrapolationwithmultiplaneimages.In:CVPR(2019)358.Suvorov,R.,Logacheva,E.,Mashikhin,A.,Remizova,A.,Ashukha,A.,Silvestrov,A.,Kong,N.,Goka,H.,Park,K.,Lempitsky,V.:Resolution-robustlargemaskinpaintingwithfourierconvolutions.In:WACV(2022)759.Tan,F.,Tang,D.,Mingsong,D.,Kaiwen,G.,Pandey,R.,Keskin,C.,Du,R.,Sun,D.,Bouaziz,S.,Fanello,S.,Tan,P.,Zhang,Y.:Humangps:Geodesicpreservingfeaturefordensehumancorrespondences.In:CVPR(2021)8,1060.Tewari,A.,Fried,O.,Thies,J.,Sitzmann,V.,Lombardi,S.,Sunkavalli,K.,Martin-Brualla,R.,Simon,T.,Saragih,J.,Nießner,M.,Pandey,R.,Fanello,S.,Wetzstein,G.,Zhu,J.Y.,Theobalt,C.,Agrawala,M.,Shechtman,E.,Goldman,D.B.,Zollh¨ofer,M.:Stateoftheartonneuralrendering.ComputerGraphicsForum(2020)361.Thies,J.,Zollh¨ofer,M.,Theobalt,C.,Stamminger,M.,Nießner,M.:IGNOR:Image-guidedNeuralObjectRendering.In:ICLR(2020)462.Tretschk,E.,Tewari,A.,Golyanik,V.,Zollh¨ofer,M.,Lassner,C.,Theobalt,C.:Non-rigidneuralradiancefields:Reconstructionandnovelviewsynthesisofadynamicscenefrommonocularvideo.In:ICCV(2021)1,363.Wang,T.,Sarafianos,N.,Yang,M.H.,Tung,T.:Animatableneuralradiancefieldsfrommonocularrgb-d.arXivpreprintarXiv:2204.01218(2022)164.Wiles,O.,Gkioxari,G.,Szeliski,R.,Johnson,J.:Synsin:End-to-endviewsynthesisfromasingleimage.In:CVPR(2020)3,4,5,6,10,11,1265.Xie,Y.,Takikawa,T.,Saito,S.,Litany,O.,Yan,S.,Khan,N.,Tombari,F.,Tompkin,J.,Sitzmann,V.,Sridhar,S.:Neuralfieldsinvisualcomputingandbeyond(2021)466.Xu,H.,Alldieck,T.,Sminchisescu,C.:H-nerf:Neuralradiancefieldsforrenderingandtemporalreconstructionofhumansinmotion.In:NeurIPS(2021)1,4,1167.Yoon,J.S.,Kim,K.,Gallo,O.,Park,H.S.,Kautz,J.:Novelviewsynthesisofdynamicsceneswithgloballycoherentdepthsfromamonocularcamera.In:CVPR(2020)1,368.Zhang,R.,Isola,P.,Efros,A.A.,Shechtman,E.,Wang,O.:Theunreasonableeffectivenessofdeepfeaturesasaperceptualmetric.In:CVPR(2018)1169.Zhou,T.,Tucker,R.,Flynn,J.,Fyffe,G.,Snavely,N.:Stereomagnification:Learningviewsynthesisusingmultiplaneimages.TOG(2018)370.Zitnick,C.,Kang,S.B.,Uyttendaele,M.,Winder,S.,Szeliski,R.:High-qualityvideoviewinterpolationusingalayeredrepresentation.TOG(2004)3,4Free-ViewpointRGB-DHumanPerformanceCaptureandRenderingPhongNguyen-Ha1∗,NikolaosSarafianos2,ChristophLassner2,JanneHeikkil¨a1,andTonyTung21CenterforMachineVisionandSignalAnalysis,UniversityofOulu,Finland2MetaRealityLabsResearch,Sausalitohttps://www.phongnhhn.info/HVSNet3x3-Conv-1283x3-Conv-643x3-Conv-2563x3-Conv-5123x3-Conv-163x3-Conv-641x1-Conv-641x1-Conv-64SigmoidFFCx9projectionSparse depthFeature extractor FSphere-basedrenderer ΩInput viewExtracted feature MTarget camera TTarget feature Mt3x3-Conv-1283x3-Conv-643x3-Conv-2563x3-Conv-512+++Global Context InpaintingModel G3x3-Conv3x3-Conv3x3-ConvSpectralTransform1x1-Conv-C(local)1x1-Conv-C(global)Fast Fourier Convolution (FFC)concatNovel view3x3-Conv-3Fig.1.Detailedarchitectureofthesphere-basedviewsynthesisnetwork.ThefeatureextractorFfirstusethreeconvolutionlayerswithstride1toextractthefeaturesoftheinputview.Wetheninfertheradiusofeachspherebypassingthelearnedfeaturesthroughanotherconvolutionlayerandthesigmoidactivationfunction.ThegreenandredconvolutionlayersofGmodulescaleupanddownthefeaturemapsrespectively.Inthissupplementarymaterialweprovideadditionaldetailsregardingournetworkdesigns(Sec.A),aswellasimplementationdetails(Sec.B).Additionalqualitativeevaluationsandresultsareshowninthesupplementalvideo.Finally,wediscussthelimitationofourapproach(Sec.C).ANetworkDesignsInthissection,wedescribethetechnicaldetailsoftwosub-networksofourproposedHVS-Net:asphere-basedviewsynthesisSandaenhancermodelE.A.1Sphere-basedviewsynthesismodelSSphere-basedfeaturewarping.Thearchitectureofthesphere-basedviewsynthesismodelSisshowninFig.1.InsteadofdirectlyrenderingnovelviewsusingtheRGBinputimage,wefirstpasseditthroughafeatureextractorF*ThisworkwasconductedduringaninternshipatMetaRealityLabsResearch.arXiv:2112.13889v4  [cs.CV]  2 Aug 20222P.Nguyenetal.(a) Point-based rendering(b) Sphere-based renderingFig.2.Visualizationoftherenderedfeaturesbetween(a)pointand(b)sphere-basedrenderingmethods.Point-basedmethod[?]canonlyrenderpixels(orangeboxes)thathavevalid3Dcoordinates.Incontrast,sphere-basedmethod[?]useslearnedradiusriofeachpointpitorenderneighboringpixelswhichleadstoadenserfeaturemap.whichconsistsofthreeconvolutionlayerswithstride1tomaintainthespa-tialresolution.WechoosethefeaturesfiasthevaluesofMwherethereisavaliddepthvalue.Weestimateper-sphereradiusribypassingMtoanotherconvolutionlayerwithsigmoidactivationfunction.InFig.2,weshowthevisu-alizationofrenderedfeaturemapsfromasetofsparsepointsusingpointandsphere-basedrenderers.Incaseofpoint-basedrendering[?],each3Dpointpicanrenderasinglepixel.Therefore,alargeamountofpixelscannotberen-deredbecausethereisnorayconnectingthosepixelswithvalid3Dpoints.Incontrast,thesphere-basedneuralrenderer[?]Ωrendersapixelbyblendingthecolorsofanyintersectedsphereswiththegivenray.Sinceweestimateradiusriofeachsphere(dashedcircle)usingashallownetwork,thisallowsustorenderpixelsthatdonothaveavalid3Dcoordinates.Asaresult,weobtainamuchdenserfeaturemapsascanbeseenintheFig.2ofthemainpaper.Notethat,Ωisfully-differentiableandrenderstargetfeaturemapsveryefficientlyusingPyTorch3D[?].Globalcontextinpaintingmodel.WerenderthenovelviewusingaglobalcontextinpaintingmodelG.WedesignthearchitectureoftheGmodulebasedontheencoder-decoderstructurewithskipconnectionsandnineresidualblocksarealsoutilizedinthebottleneck.Ineachresidualblock,wereplacetheregularconvolutionlayerswiththerecentlyproposedFastFourierConvolution(FFC)[?]whichpossessesthenon-localreceptivefields.AccordingtothespectralconvolutiontheoreminFouriertheory,point-wiseupdateinthespectraldomaingloballyaffectsallfeaturesin-volvedintheFouriertransform.TheFFClayersplitstheinputfeaturesintoFree-ViewpointRGB-DHumanPerformanceCaptureandRendering3localandglobalbranches.Thelocalbranchutilizesconventionalconvolutionlayerstoobtainlocalfeatures.Incontrast,theglobalbranchincludesaSpectralTransformblock[?]whichuseschannel-wiseFastFourierTransform[?]toen-ableimage-widereceptivefield.Theoutputofbothbranchesarethensummed,aggregatedbeforeaddingtotheresiduals.Outputs.TheviewsynthesismodelSnotonlypredictsanRGBimageIpofthetargetviewbutalsoaforegroundmaskImandaconfidencemapIc.Weemploythreedifferent3×3convolutionlayerstopredictthoseoutputsusingtheoutputofthefinallayeroftheGmodule.Thus,weapplythepredictedforegroundmaskandconfidencemaptothepredictednovelimageasfollow:Ip=Ip×Im×Ic.WetrainthemodelSusingthephotometriclossLphotoasdefinedinthemainpaper.A.2EnhancermodelEGround-truthData:WeusetheRenderPeopledataset[?]totrainallourmodels;whichcomprisesof1000watertightrawmeshes.ToobtainIUVground-truthwefirstfitanSMPL-likeparametricbodymodeltothescansandthenperformnon-rigidregistrationforallmeshesandrigthemforanimation.Inthatwayweobtain1000riggedmodelstowhichwecanapplythesameIUVmapduringrenderingwithanemissionshaderinBlenderCyclesandthusobtainper-pixelperfectIUVground-truthgivenanRGBinput.ThisprocessisdepictedinFig.3.HD-IUVpredictorD:NowthatwehavegeneratedpairsofRGBimagesandground-truthIUVmapsthenextstepistotrainanetworkthatgivenanRGBimageofahuman,canestablishaccurateper-pixelcorrespondencesforeachpixelcorrespondingtotheclothedhuman(seeFig.4).NotethatthekeydifferencebetweenthisapproachandwhatmethodssuchasDensePose[?]orCSE[?]aredoingwhichisdensecorrespondenceestimatestotheunclothedhumanbody.InadditionbecausemostapproachesaretrainedontheDensePose-COCOdataset[?]whichcomprisessparse(only∼100discretepointsperhuman)andnoisyannotationssuchpredictionsareusuallyinaccurateandnotapplicabletoourapplicationthattargetsclothedhumans.ThisisalsodepictedinFig.5ofthemainpaperwhereitsclearthatDensePoseIUVestimatesresultintopoortexturewarpings.TotrainourmodelwhichwetermasHD-IUV(thatstandsforHigh-DefinitionIUV)weemployedanencoder-decoderarchitecturewithfourdownsamplingandupsamplingconvolutionlayersalongwithskipconnectionsbetweenthemwhilethebottleneckcomprises3residualblocks.Thisdesignisjustifiedbythefactthatourinput-outputpairsarealwayswellalignedduetothedensecorrespondencesestablishedbyHD-IUVwhichisnotthecasewithpriorwork.ForHD-IUV,weutilizeinstancenormalization[?]andtheReLUactivationfunctioninalllayersofthenetworkbesidesthe3outputbranchesforeachtask(I,U,Voutputs).TheUVbrancheshave256outputchannels(sincetheUVpredictionscantakeanypossiblevalue),whereastheIchannelhas25channelswhichcorrespondto24bodypartsandbackground.Inallbranchesa1×1convolutionisappliedand4P.Nguyenetal.itsoutputisanunnormalizedlogitthatisthenfedtothecross-entropylosses.Eachtask’sscoresarefedtotheirrespectiveclassificationlosseswhichareusedtotrainthenetworkas:LIUV=λI∗LI+λU∗LU+λV∗LV(1)whereλi,LiaretherespectiveweightingparametersandlossfunctionsfortheI,U,Vchannels.Framingthisproblemasamulti-tasklearningproblem(3tasks)wheretheU,VandItasksare(256D,256D,25D)per-pixelclassificationprob-lemsrespectively,endedupbeingaveryeffectiveapproachtoenforcestrongsupervisionsforthesurfacecorrespondencesthatotherlossesweexperimentedwithcouldnotachieve.Inadditionweemployedasilhouettelosstoensurethatdensecorrespondenceestimatesareprovidedforeachpixeloftheforegroundclothedhuman.Finally,usingthepredictedIUV,wecanwarptheocclusion-freeinputimagetothetargetcamerausingthetexturetransfertechnique3fromDensePose[?].RefinementmoduleInthissection,weutilizethewarpedimageIwfromprevioussteptoenhancetheinitiallyestimatedtargetviewIpusingarefinementmoduleR.Basedonthepredictedconfidenceoftheviewsynthesisnetwork,wecombinebothimagesasfollows:ˆI=Ip+(1−Ic)∗IwwhereˆIisfedtoaencoder-decodernetworkfortherefinementpurposes.Inthiswork,wetrytogeneratehumansatthenovelviewpointssorenderingrealistichumanbodypartsisrequired.WeobservethatthepredictedsemanticIcontainsvaluableinformationaboutthesemanticinformationofthehumaninthetargetcamera.Therefore,weusetheSPADEnormalization[?]toinjectthesemanticsItothedecoderoftherefinementmodule.Ascanbeseeninthequalitativeresults,therefinedimageisphoto-realisticcomparedtotheground-truthimage.Notethat,weusethesamediscriminatorwith[?]toperformadversarialtrainingbetweenbothbeforeandafterrefinedimagesandtheground-truthnovelviews.DiscussionHerewediscusstheeffectivenessofourproposedHD-IUVoverDensePose[?]representationstorefinethetargetviews.AscanbeseenintheFig.8ofthemainpaper,ourEnhancermodelcanhandleheavyocclusionsusingjustasinglephoto.WeemphasizethattheHD-IUVrepresentationiscrucialforthisrefinementstepbecausewecanobtainpixel-alignedwarpedimagesatthetargetviewpointscomparedtotheground-truthdata.ThereforeourwarpedimageshavehigherqualitycomparedtothoseproducedbyDensePose.BImplementationDetailsThemodelsweretrainedwiththeAdamoptimizerusinga0.004learningrateforthediscriminator,0.001forboththeviewsynthesismodelRandtheenhancermoduleEandmomentumparameters(0,0.9).Theinput/outputofourmethodare1024×1024.WeimplementHVS-NetinPyTorchandthetrainingacrossourlarge-scaledatasetwithallidentitiesandviewstook2daystoconvergeon4NVIDIAV100GPUs.3TextureTransferUsingEstimatedDenseCoordinatesFree-ViewpointRGB-DHumanPerformanceCaptureandRendering5CLimitationsDespiteproducingappealingresultsonreal-worlddata,theproposedmethodistrainedsolelyonsyntheticdata.Itmanagestobridgethedomaingapre-markablywell,howeverwebelieveitsperformancecouldbefurtherimprovedbyintegratingreal-worlddataintothetrainingset.However,gatheringsuchdataisnottrivial:generating(closeto)noise-freepointcloudsfortrainingrequireselaboratemulti-viewcapturesystems,possiblyenhancedwithcontrolledlightingtosimulatevaryinglightingconditions.Awaytocircumventthispartiallyistotrainonalarge-scalesyntheticdataset[?]andthenfine-tuningonasmaller-scalereal-worlddataset.This,atleast,reducestheamountofdatathathastobecaptured.Anotherlimitationweidentifiedisthatthewarpedimageusedasinputtotheenhancermodelhaslowerqualitycomparedtotheinitialestimatednovelview.ThisisindependentofthequalityoftheIUVmappingandisaninherentproblemofthedifferentiablewarpingoperation.Improvingthisoperationcouldbeapromisingdirectionforfutureworkthatcouldincreasetheupperboundinqualityforthenovelviewsynthesisoffinestructuresinocclusionscenarios.6P.Nguyenetal.Fig.3.ProcessforIUVground-truthgenerationGivenarawsyntheticscanofaclothedhuman(topleft)weperformnon-rigidregistrationwith2Dkeypointsasadditionalconstraints(top-middle)andobtaintheregisteredscantothebodytemplate(bottomleft)andtheriggedscan(topright)whichisanimationready.UsingthecorrespondingUVmapwecannowobtainaccurateIUVground-truth(bottomright)thatweusetotraintheproposedHD-IUVmodel.WeprovidethecorrespondingDensePoseestimatetodemonstratethestarkdifferencebetweenthetwointermsofqualityaswellascoverage.Free-ViewpointRGB-DHumanPerformanceCaptureandRendering7Estimated Novel View3x3-Conv-1283x3-Conv-643x3-Conv-2563x3-Conv-5123x3-Conv-1283x3-Conv-643x3-Conv-2563x3-Conv-512Estimated IUVResidual blocksx33x3-Conv-1283x3-Conv-643x3-Conv-2563x3-Conv-5123x3-Conv-1283x3-Conv-643x3-Conv-2563x3-Conv-512Residual blocksSPADE Resblocksx3IUVHD-IUV Predictor DRefineNet RAn additionalocclusion-free imageIIUVWarpingRefined novel viewGTVUI+Fig.4.IUV-basedimagerefinement.Usinganadditionalocclusion-freeinput,werefinetheinitialestimatednovelviewbytrainingtheEnhancerEnetwork.Weinferthedensecorrespondencesofbothpredictednovelviewandocclusion-freeimageusinganovelHD-IUVmodule.Theocclusion-freeimageiswarpedtothetargetviewandthenrefinedbyanauto-encoder.Therefinednovelviewshowsbetterresultontheoccludedareacomparedtotheinitialestimated.