Testing for high-dimensional network parameters in

auto-regressive models

Lili Zheng1 and Garvesh Raskutti1

Abstract

High-dimensional auto-regressive models provide a natural way to model inﬂuence be-
tween M actors given multi-variate time series data for T time intervals. While there has
been considerable work on network estimation, there is limited work in the context of infer-
ence and hypothesis testing. In particular, prior work on hypothesis testing in time series
has been restricted to linear Gaussian auto-regressive models. From a practical perspec-
tive, it is important to determine suitable statistical tests for connections between actors
In the context of high-dimensional time series
that go beyond the Gaussian assumption.
models, conﬁdence intervals present additional estimators since most estimators such as
the Lasso and Dantzig selectors are biased which has led to de-biased estimators. In this
paper we address these challenges and provide convergence in distribution results and conﬁ-
dence intervals for the multi-variate AR(p) model with sub-Gaussian noise, a generalization
of Gaussian noise that broadens applicability and presents numerous technical challenges.
The main technical challenge lies in the fact that unlike Gaussian random vectors, for sub-
Gaussian vectors zero correlation does not imply independence. The proof relies on using
an intricate truncation argument to develop novel concentration bounds for quadratic forms
of dependent sub-Gaussian random variables. Our convergence in distribution results hold
provided T = Ω((s ∨ ρ)2 log2 M ), where s and ρ refer to sparsity parameters which matches
existed results for hypothesis testing with i.i.d. samples. We validate our theoretical results
with simulation results for both block-structured and chain-structured networks.

8
1
0
2

c
e
D
1
1

]
T
S
.
h
t
a
m

[

2
v
9
5
6
3
0
.
2
1
8
1
:
v
i
X
r
a

1

Introduction

Vector autoregressive models arise in a number of applications including macroeconomics (see
e.g.Ang and Piazzesi [2003],Hansen [2003],Shan [2005]), computational neuroscience (see e.g.Goebel
et al. [2003],Seth et al. [2015],Harrison et al. [2003], Bressler et al. [2007]), and many others (see
e.g.Michailidis and dAlch´e Buc [2013],Fujita et al. [2007]). Recent years has seen substantial

1Department of Statistics, University of Wisconsin-Madison

1

 
 
 
 
 
 
development in the theory and methodology of high-dimensional auto-regressive models with
respect to parameter estimation (see e.g. Song and Bickel [2011],Basu et al. [2015],Davis et al.
[2016],Medeiros and Mendes [2016], Mark B. and R. [2018]). In particular if there are M depen-
dent time series (e.g. voxels in the brain, actors in a social network, measurements at diﬀerent
spatial locations), time series network models allow us to model temporal dependence between
actors/nodes in a network.

More precisely, consider the following time series auto-regressive network model with lag p,

Xt+1 =

p
(cid:88)

j=1

A∗(j)Xt+1−j + (cid:15)t,

(1)

t=0 ∈ RM is the time series data we have access to, {A∗(j) ∈ RM ×M , j = 1, . . . , p}
where {Xt}T
are the network parameters of interest and (cid:15)t ∈ RM is zero-mean noise. We are considering the
high-dimensional setting where the number of nodes M in the network is much larger than the
sample size T . Prior work in Basu et al. [2015] has addressed the question of how to estimate the
network parameter A∗ with Gaussian noise (cid:15)t under sparsity assumptions and various structural
constraints. In this paper, we focus on inference and hypothesis testing for the parameter A∗
given the data (Xt)T

t=0.

In high-dimensional statistics, there has recently been a growing body of work on conﬁdence
intervals and hypothesis testing under structural assumptions such as sparsity. Since the widely
used Lasso estimator for sparse linear regression is asymptotically biased, one-step estimators
based on bias-correction have been studied in works such as Zhang and Zhang [2014], Van de Geer
et al. [2014] and Javanmard and Montanari [2014] which are referred to as LDPE, de-sparsifying
and de-biasing estimator respectively. Low-dimensional components of these estimators have
asymptotic normality and thus can be used for constructing hypothesis testing and conﬁdence
intervals.

In this paper, we adopt the framework of Ning and Liu (Ning et al. [2017]) who propose a
high dimensional test statistic based on score function, called the decorrelated score function
which we brieﬂy describe here. Formally, consider a statistical model P = {Pβ : β ∈ Ω} with
high-dimensional parameter vector β = (θ, γ(cid:62))(cid:62) ∈ Rd. Suppose we are interested in the scalar
parameter θ and γ ∈ Rd−1 is the nuisance parameter. Suppose data {U i, i = 1, . . . , n} are i.i.d.
data following distribution Pβ, then the negative log-likelihood function is deﬁned as

1
n

n
(cid:88)

i=1

log f (U i; θ, γ).

(cid:96)(θ, γ) = −

√

n∇θ(cid:96)(0, γ∗) is asymptotically normal if the true parameter
It is known that the score function
β∗ = (0, γ∗). If γ∗ is substituted by some estimator ˆγ, the estimation induced error can be
approximated as the following:

√

n∇θ(cid:96)(0, ˆγ) −

√

n∇θ(cid:96)(0, γ∗) ≈

√

n∇2

θγ(cid:96)(0, γ∗)(ˆγ − γ∗),

2

when ˆγ − γ∗ is small enough. Although ˆγ − γ∗ converge to 0 with properly chosen ˆγ, e.g.
n∇2
Lasso estimator,
(cid:54)= 0. This fact
motivates the decorrelated score function:

θγ(cid:96)(0, γ∗)(ˆγ − γ∗) would not vanish if Eβ

θγ(cid:96)(0, γ∗)

∇2

√

(cid:17)

(cid:16)

S(θ, γ) = ∇θ(cid:96)(θ, γ) − IθγI−1

γγ∇γ(cid:96)(θ, γ),

with Fisher information matrix I = Eβ

(cid:0)∇2(cid:96)(β)(cid:1). One can check that

E (∇γS(θ, γ)) = 0.

Both γ and IθγI−1
the decorrelated score function is asymptotically normal.

γγ are substituted by some estimator, and it is shown in Ning et al. [2017] that

In the linear regression case, the test statistic generated by the decorrelated score function in
Ning et al. [2017] is equivalent to that constructed by de-biased estimator in Van de Geer et al.
[2014]. However, Ning et al. [2017] allow a more general form, and thus is easier to adapt to
the time series case. In fact Neykov et al.Neykov et al. [2018] consider amongst other examples,
high-dimensional time series with Gaussian error innovations. While Gaussian error innovations
are widely used, many time series models include data that has bounded range or discrete data,
for which the Gaussian distribution is not a natural ﬁt.
In this paper, we address the more
general and technically challenging setting in which the noise (cid:15)t is sub-Gaussian.

One of the important technical challenges in going from the Gaussian to the sub-Gaussian
case is that dependent Gaussian vectors can be rotated to be independent, while such a result
does not hold for sub-Gaussian vectors. Prior work in Wong et al. [2016] addresses this challenges
by imposing stationarity and β-mixing conditions. In order to avoid these conditions, we develop
novel concentration bounds for sub-Gaussian random vectors.

In this paper, we investigate the hypothesis testing and conﬁdence region with respect to a
low-dimensional component of parameter matrices {A∗(j), j = 1, . . . , p} for sub-Gaussian data,
using the testing framework in Ning et al. [2017]. Our major contributions are as follows:

• Extending theoretical results in Ning et al. [2017] for high-dimensional hypothesis testing
from Gaussian to sub-Gaussian temporal dependent data (VAR model), both under null
and alternative hypothesis. We also show that our techniques lead to similar results to
Neykov et al.Neykov et al. [2018] in the Gaussian case but under less restrictive conditions;

• A novel concentration bound for quadratic forms of sub-Gaussian time series data. Note
that unlike Gaussian vectors which can be rotated to be independent, sub-Gaussian vectors
can not which present additional technical challenges. Our analysis also leads to estima-
tors for covariance and regression parameters for time series data under sub-Gaussian
assumptions which are of independent interest.

3

• We also construct semi-parametric eﬃcient conﬁdence region for multivariate parameters

with ﬁxed dimension;

• Finally we support our theoretical guarantees with a simulation study on bounded noise,

which is sub-Gaussian but not Gaussian.

1.1 Related Work

In the literature on inference for high-dimensional VAR models, most work focuses on the
estimation problem. Song and Bickel (Song and Bickel [2011]) investigate penalized least squares
algorithms for diﬀerent penalties, with some externally imposed assumptions on the temporal
dependence. Theoretical guarantees on Dantzig type and Lasso type estimators are studied
in Han et al. [2015] and Basu et al. [2015], but with Gaussian noise. Barigozzi and Brownlees
(Barigozzi and Brownlees [2018]) consider the inference for stationary dependence structure built
among variables, other than the parameters in the VAR model. In our work, we control the
error bounds of Lasso and Dantzig type estimators for parameter matrices, with sub-Gaussian
noise. Then we establish asymptotic distribution of test statistic based on this.

In the high-dimensional hypothesis testing literature, there is some work regarding to test-
ing for high-dimensional mean vector (Srivastava [2009]), covariance matrices (Chen et al.
[2010],Zhang et al. [2013]) and independence among variables (Schott [2005]). While for testing
on regression parameters, most work assumes i.i.d samples. Lockhart et al. [2014], Taylor et al.
[2014] and Lee et al. [2016] proposes methods to test whether a covariate should be selected
conditioning on the selection of some other covariates. A penalized score test depending on the
tuning parameter λ is considered in Voorman et al. [2014]. Our work follows the a line of work
by Zhang and Zhang [2014], Van de Geer et al. [2014], Javanmard and Montanari [2014] and
Ning et al. [2017], the de-sparsifying or decorrelated literature. We construct a VAR version
of decorrelated score test proposed by Ning et al. [2017]. Chen and Wu (Chen and Wu [2018])
tackles the hypothesis testing problem for time series data as well, but they are testing the trend
in a time series, instead of the autoregressive parameter which encodes the inﬂuence structure
among variables.

As mentioned earlier, our work is most closely related to the prior work of Neykov et
al.Neykov et al. [2018], which provides a hypothesis testing framework with high-dimensional
Gaussian time series as a special case. In our work, we consider the more general and techni-
cally challenging case of sub-Gaussian vector auto-regressive models. Throughout this paper,
we provide a comparison to results derived in this work for the Gaussian case.

4

1.2 Organization of the Paper

Section 2 explains the problem set up and proposes our test statistic. Theoretical guarantee is
shown in section 3. Speciﬁcally, section 3.1 and 3.2 present the weak convergence rate of test
statistic under the null and alternative hypothesis H0 and HA. Section 3.3 propose some feasible
estimators, which satisfy the assumptions required and can be plugged into the test statistic.
Section 3.4 considers the case when the variance of noise are unknown, and we construct a
conﬁdence region for multivariate parameter vectors in Section 3.5. We consider the special case
of the AR(1) model with Gaussian noise, a detailed comparison with Neykov et al. [2018] is
provided in section 3.6. Section 4 provides simulation results and section 5 includes the proofs
for the two main theorems. Much of the proof is deferred to Appendices.

1.3 Notation

We deﬁne the following norms for vectors and matrices: For a vector u = (u1, . . . , ud)(cid:62) ∈ Rd, we
p . For a matrix U ∈ Rm×n, the (cid:96)p norm and

deﬁne the p-norm where p ≥ 1,(cid:107)u(cid:107)p =

(cid:16)(cid:80)d

(cid:17) 1

i=1 up

i

Frobenius norm of U is deﬁned as (cid:107)U (cid:107)p = supv
(cid:107)U (cid:107)F =
use notation (cid:107)U (cid:107)1,1 to denote the (cid:96)1 penalty on U , which is (cid:80)m
i=1
U is symmetric the trace norm of U is (cid:107)U (cid:107)tr = tr(

U 2).

√

,

(cid:107)U v(cid:107)p
(cid:107)v(cid:107)p

(cid:17) 1

(cid:80)n

(cid:16)(cid:80)m
2 . We also
j=1 U 2
ij
i=1
(cid:80)n
j=1 |Ui,j|. Furthermore, if

Throughout the paper, we assume that the entries of noise vectors {(cid:15)ti, 1 ≤ i ≤ M }∞

t=−∞ are
independent sub-Gaussian variables with constant scale factor. A univariate centered random
variable X has a sub-Gaussian distribution with scale factor τ if

MX (t) (cid:44) E [exp(tX)] ≤ exp(τ 2t2/2).

2 Problem Setup

We consider a general vector auto-regressive time series with lag p, where p is known and ﬁnite
and independent of T or other dimensions:

Xt+1 =

p
(cid:88)

j=1

A(j)Xt−j+1 + (cid:15)t,

(2)

where Xt ∈ RM , (cid:15)t ∈ RM is zero-mean entry-wise independent sub-Gaussian noise with identity
covariance matrix, and A(j) ∈ RM ×M , j = 1, · · · , p are parameters of interest. Deﬁne the matrix
A∗ = (A(1), · · · , A(p)) ∈ RM ×pM and Xt = (X (cid:62)
t−p+1)(cid:62) ∈ RpM , then we can also write
(2) as

t , · · · , X (cid:62)

Xt+1 = A∗Xt + (cid:15)t.

(3)

5

For notational convenience, we assume that time series data Xt has time range 1 − p ≤ t ≤ T .

Based on data (Xt)T

t=1−p, we test the hypothesis of whether a subset of entries in A∗ are 0.
i be the ith row vector of A∗. Without loss of generality, suppose the entries we test are
Let A∗
in rows 1, · · · , k. Deﬁne Dm ⊂ {1, · · · , pM } as the columns we test in mth row with dm = |Dm|,
and D = {(i, j) : 1 ≤ i ≤ k, j ∈ Di}, with d = |D| = (cid:80)k

m=1 dm. We test the null hypothesis:

H0 : (cid:101)AD = 0

(4)

where (cid:101)AD = ((A∗
with T . In the work of of Neykov et al.Neykov et al. [2018], d is assumed to be 1.

)(cid:62) ∈ Rd. We also assume that d is ﬁnite and not increasing

, · · · , (A∗

k)(cid:62)
Dk

1)(cid:62)
D1

2.1 Stationary distribution

Since we are developing a hypothesis testing framework based on the decorrelated score test,
it is important to specify a stationary distribution for Xt Using standard notation from auto-
regressive time series models, deﬁne the polynomial A(z) = IM − (cid:80)p
j=1 A(j)zj, where IM is an
M × M identity matrix, and z is a complex number. To guarantee the existence of a stationary
solution to (3), we assume

Then we can write

det(A(z)) (cid:54)= 0,

|z| ≤ 1.

(A(z))−1 =

∞
(cid:88)

j=0

Ψjzj,

where Ψj, j ≥ 0 are all real valued matrices which are polynomial functions of A(i), 1 ≤ i ≤ p.
Note that in the special case where p = 1, Ψj = (A∗)j.

It can be shown that the unique stationary solution to (2) is

Xt =

∞
(cid:88)

j=0

Ψj(cid:15)t−j−1,

and the covariance matrix Σ of Xt satisﬁes

Σ = Cov(Xt) =

∞
(cid:88)

j=0

ΨjΨ(cid:62)
j .

(5)

2.2 Decorrelated Score Function

Using the frameworks developed in Ning et al. [2017] for independent design, we consider the
decorrelated score test. First we deﬁne the score function S(A∗) ∈ RM ×M , with each entry
deﬁned as follows:

[S(A∗)]jk = −

1
T

T −1
(cid:88)

(Xt+1,j − a∗(cid:62)

j Xt)Xtk = −

t=0

6

1
T

T −1
(cid:88)

t=0

(cid:15)t,jXtk.

As pointed out in Ning et al. [2017], the standard score function is infeasible and we need to
consider the decorrelated score function

with each Sm ∈ Rdm corresponding to the tested row (m, Dm):

S = (S(cid:62)

1 , S(cid:62)

2 , · · · , S(cid:62)

k )(cid:62) ∈ Rd,

Sm = −

1
T

T −1
(cid:88)

t=0

(cid:15)t,m(Xt,Dm − w∗(cid:62)

m Xt,Dc

m),

where Xt,Dm ∈ Rdm is composed of the entries of Xt whose indices are within set Dm. Xt,Dc
RpM −dm is also deﬁned similarly and w∗

m ∈ R(pM −dm)×dm is chosen to satisfy

m ∈

Cov(Xt,Dm − w∗(cid:62)

m Xt,Dc

m, Xt,Dc

m) = 0.

Speciﬁcally, w∗

m is deﬁned as a function of Υ = Cov(Xt) ∈ RpM ×pM :

w∗

m = (ΥDc

m,Dc

m)−1ΥDc

m,Dm.

2.3 Test Statistic

Based on the decorrelated score function Sm, we ﬁrst deﬁne the statistic VT,m ∈ Rdm:
√

VT,m (cid:44)

T (Υ(m))− 1

2 Sm,

with Υ(m) ∈ Rdm×dm being deﬁned as:

Υ(m) (cid:44) Cov(Xt,Dm − w∗(cid:62)
m Xt,Dc
= Cov(Xt,Dm|Xt,Dc
m)
= ΥDm,Dm − ΥDm,Dc

m)

m(ΥDc

m,Dc

m)−1ΥDc

m,Dm.

(6)

(7)

(8)

Let VT be the d-dimensional vector concatenated by VT,m’s:

VT = (V (cid:62)

T,1, · · · , V (cid:62)

T,k)(cid:62).

One of the main results of the paper is to show that VT is asymptotically Gaussian. Deﬁne
m, and Υ(m), we later
UT = (cid:107)VT (cid:107)2
deﬁne estimators for these quantities. Formally, we deﬁne our test statistic (cid:98)UT as

d. Since we do not know (cid:15)t, w∗

2, then UT is asymptotically χ2

(cid:98)UT = T

k
(cid:88)

m=1

Υ(m)(cid:17)−1
(cid:16)(cid:91)

(cid:98)S(cid:62)
m

(cid:98)Sm,

(9)

where

(cid:91)
Υ(m) ∈ Rdm×dm is an estimator for Υ(m) and (cid:98)Sm ∈ Rdm is deﬁned as

(cid:98)Sm = −

1
T

T −1
(cid:88)

(cid:16)

t=0

Xt+1,m − ( (cid:98)Am)(cid:62)
Dc
m

Xt,Dc

m

(cid:17)

7

(Xt,Dm − ˆw(cid:62)

mXt,Dc

m),

with (cid:98)Am ∈ RpM and ˆwm ∈ R(pM −dm)×dm estimating A∗
m and w∗
m. Here we are not worried about
(cid:91)
Υ(m), since Υ(m) is a low dimensional covariance matrix. To guarantee a
the invertible issue of
good estimation of the high-dimensional parameter A∗
m and w∗
m, we impose sparsity conditions
upon them. Speciﬁcally, for each 1 ≤ m ≤ M , 1 ≤ i ≤ k deﬁne

ρm (cid:44) (cid:107)A∗

m(cid:107)0,

si (cid:44) (cid:107)w∗

i (cid:107)0,

(10)

and note that they both depend on A∗.

The sparsity of w∗

m can be implied by the sparsity of Υ−1, which is a common condition in
high-dimensional hypothesis testing literature (e.g. see Van de Geer et al. [2014]). Speciﬁcally,
the following Lemma shows that when lag p = 1 and A∗ is symmetric, the sparsity of w∗
m is
implied by the sparsity of A∗:

Lemma 2.1. If p = 1, A∗ ∈ RM ×M is symmetric, then sm deﬁned in (10) satisﬁes

sm ≤ d2

m max
1≤i≤M

ρi,

for 1 ≤ m ≤ k.

The proof for Lemma 2.1 is included in Appendix E.

3 Theoretical guarantee

In this section, we present uniform convergence results for test statistic (cid:98)UT under H0 and HA,
with A∗ and estimators satisfying conditions. We also provide feasible estimators, and prove
that they satisfy corresponding conditions in Section 3.3. Unknown variance and conﬁdence
region construction is discussed in Section 3.4 and 3.5. In Section 3.6 we provide consequences
of our theory under AR(1) model with Gaussian noise and compare our results with Neykov et
al.Neykov et al. [2018].

Recall that the null hypothesis is

H0 : (cid:101)AD = 0,

(11)

with (cid:101)AD ∈ Rd being concatenated by (A∗
like in Ning et al. [2017], we consider

1)D1, . . . , (A∗

k)Dk . While for the alternative hypothesis,

with some constant φ > 0 and constant vector ∆ ∈ Rd. Write

HA : (cid:101)AD = T −φ∆,

(12)

∆ = (∆(cid:62)

1 , · · · .∆(cid:62)

k )(cid:62),

8

where each ∆m ∈ Rdm. The reason why T −φ∆ instead of ∆ is considered in (12) is that we
expect the test to be more sensitive as sample size increases. We will see how the value of φ
inﬂuences the convergence of (cid:98)UT in Theorem 3.2.

We still assume (cid:15)ti’s are i.i.d. sub-Gaussian random variables, and also consider a special
case, where (cid:15)t ∼ N (0, I). We compare our result in the Gaussian case to results in Neykov et
al.Neykov et al. [2018].

First we deﬁne the sets Ω0 and Ω1 of feasible parameter matrices A∗ under H0 and HA

respectively. To control the stability of {Xt} in model (3), we impose the condition:

∞
(cid:88)





∞
(cid:88)

i=0

j=0



1
2

(cid:107)Ψi+j(cid:107)2
2



≤ β,

for some constant β > 0. In the case p = 1, condition (13) reduces to

∞
(cid:88)





∞
(cid:88)

i=0

j=0



1
2

(cid:13)
(cid:13)(A∗)i+j(cid:13)
2
(cid:13)
2



≤ β,

(13)

(14)

which is implied by (cid:107)A∗(cid:107)2 ≤ 1 − (cid:15) for some 0 < (cid:15) < 1, a typical condition assumed (see
e.g. Neykov et al. [2018]). Then deﬁne sets Ω0 and Ω1 for any β, ρ, s, M, T, φ > 0, set D of size
d and vector ∆ = (∆(cid:62)

1 , · · · , ∆(cid:62)

k )(cid:62) ∈ Rd:

Ω0 = {A∗ ∈ RM ×pM : (cid:101)AD = 0,

∞
(cid:88)





∞
(cid:88)

i=0

j=0



1
2

(cid:107)Ψi+j(cid:107)2
2



≤ β,

max
m

ρm(A∗) ≤ ρ, max
m

sm(A∗) ≤ s},

Ω1 = {A∗ ∈ RM ×pM : (cid:101)AD = T −φ∆,

∞
(cid:88)





∞
(cid:88)



1
2

(cid:107)Ψi+j(cid:107)2
2



≤ β,

i=0

j=0

max
m

ρm(A∗) ≤ ρ, max
m

sm(A∗) ≤ s}.

(15)

(16)

Note here ρm(A∗) and sm(A∗) are still functions of A∗, since Υ is determined by A∗. Clearly
(cid:91)
Σ(m) with 1 ≤ m ≤ k, to guarantee the weak
we need reliable estimators for (cid:98)Am, ˆwm and
convergence of (cid:98)UT . We present the following assumptions for these estimators, which we will
verify in section 3.3. Note that constants C may depend on p, d, β and τ , but do not depend on
either M or T .

Assumption 3.1 (Estimation Error for A∗

m). For each A∗ ∈ Ω0 ∪ Ω1,
(cid:114)

(cid:13)
(cid:13) (cid:98)Am − A∗
(cid:13)

m

(cid:13)
(cid:13)
(cid:13)2

≤ C

ρm log M
T

,

(17)

(cid:13)
(cid:13) (cid:98)Am − A∗
(cid:13)

m

(cid:13)
(cid:13)
(cid:13)1

≤ Cρm

(cid:114)

( (cid:98)Am − A∗

m)(cid:62)

(cid:32)

1
T

T −1
(cid:88)

t=0

log M
T

,

(cid:33)

XtX (cid:62)
t

( (cid:98)Am − A∗

m) ≤ C

9

ρm log M
T

,

hold for 1 ≤ m ≤ k, with probability at least 1 − c1 exp{−c2 log M }.

These are standard error bounds for Lasso estimator and Dantzig Selector with independent
design. In this paper we verify Assumption 3.1 in section 3.3 and the remaining two assumptions
when we have dependent sub-Gaussian random variables, as we do for our vector auto-regressive
model setting.

Assumption 3.2 (Estimation Error for w∗

m). For each A∗ ∈ Ω0 ∪ Ω1:
(cid:114)

(cid:107) ˆwm − w∗

m(cid:107)1 ≤ Csm
(cid:33)

log M
T

,

(cid:35)

Xt,Dc

mX (cid:62)
t,Dc
m

( ˆwm − w∗

m)

≤ C

(cid:34)

tr

( ˆwm − w∗

m)(cid:62)

(cid:32)

1
T

T −1
(cid:88)

t=0

sm log M
T

,

(18)

hold for 1 ≤ m ≤ k, with probability at least 1 − c1 exp{−c2 log M }.

Similar to Assumption 3.1, we will show that both Lasso estimator and Dantzig selector

under model (3) satisfy Assumption 3.2.

Assumption 3.3 (Estimation Error for Υ(m)). For each A∗ ∈ Ω0 ∪ Ω1,
(cid:16)(cid:91)
Υ(m)(cid:17)−1

Υ(m) 1

2 − I

≤ C

2

,

(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

(s ∨ ρ) log M
√
T

(cid:13)
(cid:13)
Υ(m) 1
(cid:13)
(cid:13)

(19)

hold for 1 ≤ m ≤ k, with probability at least 1 − c1 exp{−c2 log M }.

Note that Υ(m) ∈ Rdm×dm is a low-dimensional matrix, and thus it is computationally feasible
(cid:91)
Υ(m). We show in

to use the sample covariance matrix of Xt,Dm − ˆw(cid:62)
(cid:91)
section 3.3 that, as long as ˆwm is a reliable estimator for w∗
Υ(m) would satisfy a tighter bound
m,
than (19). This looser bound in Assumption 3.3 actually allows more choices for estimators for
(Υ(m))−1, as shown in section 3.5.

m as an estimator for

mXt,Dc

3.1 Uniform convergence under null hypothesis

Based on these assumptions, we have the following main theorem.

Theorem 3.1. Consider the model (3) with i.i.d. sub-Gaussian noise (cid:15)ti with sub-Gaussian
parameter τ . If Assumptions 3.1-3.3 are satisﬁed, and (ρ ∨ s) log M = o(
T ), then (cid:98)UT deﬁned
in (9) satisﬁes

√

sup
x∈R,A∗∈Ω0

(cid:12)
(cid:12)
(cid:12)

(cid:12)
P( (cid:98)UT ≤ x) − Fd(x)
(cid:12)
(cid:12)

≤

C1
T

1
8

+ C2

(cid:18) (s ∨ ρ) log M
√

T

(cid:19) 1
2

+

C3
M C4

,

(20)

when T > C for some constant C. Here the constants Ci’s depend on p, d, β, τ .

10

8 rather than T − 1

Theorem 3.1 proves weak convergence of (cid:98)UT to χ2

d. The uniform convergence rate can be
understood as follows: the ﬁrst term is due to the rate obtained by martingale CLT, where
we require T − 1
2 due to the dependence; the remaining two terms arise from
estimation error, with the second one being the error bounds, and third being the probability
that the error bounds do not hold. If we assume Gaussianity, we can improve the ﬁrst term in
the rate of convergence from T − 1
4 +α for any α > 0. To the best of our knowledge, ours
is the ﬁrst work that formally attempts to characterize the rates of convergence.

8 to T − 1

Remark 3.1. Compared to the theoretical result for independent design in Ning et al. [2017],
(cid:16)(cid:80)∞
the only additional condition we add is (cid:80)∞
2 ≤ β, which is used to control the
i=0
strength of dependence uniformly. Also, we consider multivariate testing which is more general,
and derive the explicit convergence rate.

j=0 (cid:107)Ψi+j(cid:107)2

(cid:17) 1

2

Remark 3.2. The test statistic proposed in Van de Geer et al. [2014] and Javanmard and
Montanari [2014] for the independent design share similar ideas with our test statistic. Instead
m, Van de Geer et al. [2014] assumes Υ−1 to be row
of imposing a sparsity assumption upon w∗
wise sparse. This is actually equivalent to the sparsity assumption on w∗
m in the univariate case.
Javanmard and Montanari [2014] does not require the sparsity condition on Υ−1, but it is hard
to extend their theory to the time series setting, due to a diﬃculty in applying the martingale
CLT.

Remark 3.3. The theoretical guarantee we obtained here, is more general and stronger than the
result achieved in Neykov et al. [2018]. A more detailed comparison is presented in section 3.6.

3.2 Uniform convergence under alternative hypothesis

Recall the deﬁnition of ΩA in (16). The following theorem establishes the asymptotic behavior
of (cid:98)UT for A∗ ∈ ΩA, with diﬀerent values of φ. First deﬁne

(cid:101)∆ = ( (cid:101)∆(cid:62)

1 , · · · , (cid:101)∆(cid:62)

k )(cid:62),

(cid:101)∆m = (Υ(m))

1

2 ∆m,

(21)

where Υ(m) is deﬁned in (8).

Theorem 3.2. Consider the model (3) with i.i.d. sub-Gaussian noise (cid:15)ti and sub-Gaussian
parameter τ . If Assumptions 3.1-3.3 are satisﬁed, and (ρ ∨ s) log M = o(
T ), then when T > C
for some constant C,

√

(1) φ = 1
2

sup
x∈R,A∗∈Ω1

(cid:12)
P( (cid:98)UT ≤ x) − Fd,(cid:107) (cid:101)∆(cid:107)2
(cid:12)
(cid:12)

2

(cid:12)
(cid:12)
(x)
(cid:12)

≤

C1
T

1
8

+ C2

(cid:18) (s ∨ ρ) log M
√

T

(cid:19) 1
2

+

C3
M C4

.

11

(22)

(2) 0 < φ < 1
2

(3) φ > 1
2

≤

sup
A∗∈Ω1
C1
T

+

1
8

|P( (cid:98)UT ≤ x)|

C2
M C3

+ C4 exp{−C5T

1

2 −φ + C6

√

x}.

sup
x∈R,A∗∈Ω1

(cid:12)
P( (cid:98)UT ≤ x) − Fd(x)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

≤

C1
T

1
8

+ C2

(cid:18) (s ∨ ρ) log M
√

T

(cid:19) 1
2

+

C3
M C4

+ C3T

1−2φ
3

.

Here Ci’s are constants depending on p, d, β, ∆, τ .

(23)

(24)

Theorem 3.2 shows the threshold value of φ for HA to be detectable. When φ > 1

2 , we cannot
distinguish H0 and HA since under both cases (cid:98)UT converges to χ2
2 , (cid:98)UT diverges
to +∞ in probability, thus it would be very easy to detect HA; When φ = 1
2 , (cid:98)UT converges to a
non-central χ2
d with noncentrality parameter determined by constant vector ∆ and Υ = Cov(Xt),
which implies the power of the test. Note here, (23) holds also for the trivial case φ < 0, since
we do not use the fact φ > 0 in the proof.

d; When φ < 1

Remark 3.4. Theorem 3.2 is also consistent with the threshold value of φ given by Ning et al.
[2017] for linear regression with i.i.d samples. However, Ning et al. [2017] assumes additional
conditions on the scaling of sample size, number of covariates and sparsity of w∗
m for proving
asymptotic power. Our conditions are exactly the same as the ones for H0, due to a more speciﬁc
model and careful analysis.

3.3 Feasible Estimators

Both the estimation of w∗
thus we can use the Lasso or Dantzig selector. Formally, deﬁne

m and A∗ can be viewed as high-dimensional sparse regression problems,

(cid:98)A(L) = arg min
A∈RM ×pM

1
T

T −1
(cid:88)

t=0

(cid:107)Xt+1 − AXt(cid:107)2

2 + λA(cid:107)A(cid:107)1,1,

(25)

as the Lasso estimator for A∗, and

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)∞
as the Dantzig selector estimator for A∗. Similarly, for 1 ≤ m ≤ k, deﬁne

(cid:98)A(D) = arg min
A∈RM ×pM

(Xt+1 − AXt)X (cid:62)
t

(cid:107)A(cid:107)1,1,

T −1
(cid:88)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
T

s.t.

t=0

≤ λA,

(26)

ˆw(L)

m =

arg min
w∈R(pM −dm)×dm

1
T

T −1
(cid:88)

t=0

(cid:107)Xt,Dm − w(cid:62)Xt,Dc

m(cid:107)2

2 + λw(cid:107)w(cid:107)1,1,

(27)

12

and

ˆw(D)

m =

arg min
w∈R(pM −dm)×dm

(cid:107)w(cid:107)1,1,

s.t.

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
T

T −1
(cid:88)

(Xt,Dm − w(cid:62)Xt,Dc

m)X (cid:62)
t,Dc
m

t=0

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

≤ λw.

(28)

While for estimating Υ(m), since this is a low dimensional covariance matrix for Xt,Dm −
w∗(cid:62)

m, we can directly use sample covariance of Xt,Dm − ˆw(cid:62)

m Xt,Dc

(cid:91)
Υ(m):

mXt,Dc

m as

(cid:91)
Υ(m) =

1
T

T −1
(cid:88)

t=0

(Xt,Dm − ˆw(cid:62)

mXt,Dc

m)(Xt,Dm − ˆw(cid:62)

mXt,Dc

m)(cid:62),

(29)

for 1 ≤ m ≤ k. Here ˆwm in the deﬁnition of (29) is either ˆw(L)

m or ˆw(D)
m .

As shown in the following, estimators (25) to (29) all satisfy Assumptions 3.1 to 3.3, under

the model setting stated in (3):

Lemma 3.1. If (cid:98)A = (cid:98)A(L), or (cid:98)A = (cid:98)A(D), which are deﬁned as in (25) and (26) with λA (cid:16)
then (cid:98)A satisﬁes Assumption 3.1 when T > Cρ log M .
Lemma 3.2. If ˆwm = ˆw(L)
(cid:113) log M
T

m or ˆwm = ˆw(D)
, then ˆwm’s satisfy Assumption 3.2 when T > Cs log M .

m , which are deﬁned as in (27) and (28) with λw (cid:16)

(cid:113) log M
T

,

Lemma 3.3. If
1 − c1 exp{−c2 log M }, then

(cid:91)
Υ(m)’s are deﬁned as in (29), where ˆwm satisﬁes (18) with probability at least

(cid:13)
(cid:13)
Υ(m) 1
(cid:13)
(cid:13)

2

(cid:16)(cid:91)
Υ(m)(cid:17)−1

Υ(m) 1

2 − I

(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

(cid:114)

≤ C

log M
T

,

with probability at least 1 − c1 exp{−c2 log M }, when T > Cs2 log M .

Note here Lemma 3.3 is stronger than Assumption 3.3. The proof of these Lemmas are
deferred to Appendix A. By these lemmas and Theorem 3.1, 3.2, we arrive at following Corollary.

Corollary 3.1. Under model (3) with i.i.d sub-Gaussian noise (cid:15)ti with parameter τ , if (cid:98)A = (cid:98)A(L)
(cid:91)
or (cid:98)A(D), ˆwm = ˆw(L)
Υ(m)’s are deﬁned as in (29) for 1 ≤ m ≤ k with λA (cid:16) λw (cid:16)
√
(cid:113) log M
T ) and T > C for some constant C > 0, bounds (20) to (24)
T

m or ˆw(D)

m , and

, then if (ρ ∨ s) log M = o(
from Theorems 3.1 and 3.2 hold.

3.4 Variance Estimation

In this section, we consider the case where σ∗2 = Var((cid:15)ti) is unknown under model (3). Actually,
if σ∗ (cid:54)= 1 is known, it is straightforward to extend Theorem 3.1 to Theorem 3.2 for (cid:98)UT deﬁned
as follows:

k
(cid:88)

(cid:98)UT = T

(cid:98)S(cid:62)
m(

(cid:91)
Υ(m))−1 (cid:98)Sm/σ∗2.

(30)

m=1

13

This follows since if we consider Yt = Xt/σ∗, time series data Yt would satisfy the same model
but with unit variance noise.

When σ∗2 is unknown, we apply the estimator

and deﬁne the test statistic

ˆσ2 =

1
M T

T −1
(cid:88)

t=0

(cid:107)Xt+1 − (cid:98)AXt(cid:107)2
2,

(cid:101)UT = T

k
(cid:88)

m=1

(cid:98)S(cid:62)
m(

(cid:91)
Υ(m))−1 (cid:98)Sm/ˆσ2.

(31)

(32)

We show that (cid:101)UT has the same convergence results we derive for the unit variance noise case.

Theorem 3.3. Consider the model (3) with i.i.d. sub-Gaussian noise (cid:15)ti of variance σ∗2 =
0 > 0 and scale factor τ σ∗. Then Theorem 3.1 and 3.2 hold for (cid:101)UT under each
Var((cid:15)ti) ≥ σ2
corresponding condition, and constants Ci’s also depend on σ0.

Theorem 3.3 shows that when we have to estimate the unknown σ∗2, test statistic (cid:101)UT main-
tains the same asymptotic behavior as (cid:98)UT under the known variance case, given that all the
assumptions for estimation errors are satisﬁed and σ∗ is lower bounded by some constant.

Remark 3.5. With sub-Gaussian noise (cid:15)ti, if we still assume the scale factor τ σ∗ of (cid:15)ti to be
bounded by constant, then Lemma 3.1 to 3.3 would still hold. Thus the assumptions imposed on
(cid:91)
Υ(m) are all satisﬁed. However, if we don’t assume σ∗ to be
estimation errors of (cid:98)A, ˆwm and
bounded, then the tuning parameters λA and λw have to scale with σ∗.

Remark 3.6. Neykov et al. [2018] proposes another estimator for the variance of (cid:15)ti, based on
the fact that Σ = AΣA(cid:62) + Cov((cid:15)t). Both these estimators are consistent and lead to convergence
in distribution results.

3.5 Semi-parametric Optimal Conﬁdence Region

In this section, we construct a conﬁdence region for (cid:101)AD, under model (3) with unknown noise
variance σ∗2. Similar to Ning et al. [2017], we consider the one-step estimator ˆa(m) for each
(A∗

m)Dm, based on the decorrelated score function:

ˆa(m) = ( (cid:98)Am)Dm −

Υ(m)(cid:17)−1
(cid:16)(cid:93)

(cid:101)Sm,

(33)

where (cid:98)Am is any estimator satisfying the Assumptions 3.1 on error bounds for (cid:98)Am − A∗
both the Lasso or Dantzig Estimator for A∗

(cid:93)
Υ(m) takes the form:

m are suitable.

m, and

(cid:93)
Υ(m) =

1
T

T −1
(cid:88)

(cid:16)

t=0

Xt,Dm − ˆw(cid:62)

mXt,Dc

m

(cid:17)

X (cid:62)

t,Dm,

(34)

14

which is another estimator for Υ(m), and

(cid:101)Sm = −

1
T

T −1
(cid:88)

(cid:16)

t=0

Xt+1,m − (cid:98)A(cid:62)

mXt

(cid:17) (cid:16)

Xt,Dm − ˆw(cid:62)

mXt,Dc

m

(cid:17)

.

We will show that ˆa(m) − (A∗
m)Dm is asymptotically Gaussian with covariance matrix (Υ(m))−1.
Thus we construct the following conﬁdence region for (cid:101)AD, with asymptotic conﬁdence coeﬃcient
1 − α:

(cid:26)

CR(α) =

θ = (θ(cid:62)

1 , . . . , θ(cid:62)

k )(cid:62) : θm ∈ Rdm,

(ˆa(m) − θm)(cid:62)(cid:91)

Υ(m)(ˆa(m) − θm) ≤ χ2

d(1 − α)

.

(cid:27)

(35)

T
ˆσ2

k
(cid:88)

m=1

This is a d dimensional elliptical ball with center vector (ˆa(1)(cid:62), . . . ˆa(k)(cid:62))(cid:62). The following
theorem shows the weak convergence result of

(cid:98)RT (cid:44) T
ˆσ2

k
(cid:88)

(ˆa(m) − (A∗

m)Dm)(cid:62)(cid:91)

Υ(m)(ˆa(m) − (A∗

m)Dm).

(36)

m=1

Theorem 3.4. Under model (3) with i.i.d. sub-Gaussian noise (cid:15)ti with variance σ∗2 = Var((cid:15)ti) ≥
0 > 0 and sub-Gaussian parameter τ σ∗, then Theorem 3.1 and 3.2 hold for (cid:98)RT under each cor-
σ2
responding condition, and the constants Ci’s also depend on σ0.

(cid:91)
Υ(m) for
Remark 3.7. In the deﬁnition of one-step estimator ˆa(m), we use
theoretical convenience. Theorem 3.4 would still hold true if ˆa(m) is deﬁned as ( (cid:98)Am)Dm −
(cid:16)(cid:91)
Υ(m)(cid:17)−1

(cid:93)
Υ(m) instead of

(cid:101)Sm.

Remark 3.8. We have exactly the same theoretical result for (cid:101)UT and (cid:98)RT , and this is due to
the close relationship between these two quantities. In particular,

(cid:98)RT = T

k
(cid:88)

m=1

(cid:18)(cid:93)
Υ(m)

(cid:98)S(cid:62)
m

(cid:62)(cid:19)−1

Υ(m) (cid:16)(cid:93)
(cid:91)

Υ(m)(cid:17)−1

(cid:98)Sm/ˆσ2,

compared to (cid:101)UT = T (cid:80)k
(cid:18)(cid:93)
Υ(m)(cid:17)−1
Υ(m)

Υ(m) (cid:16)(cid:93)
(cid:91)

(cid:62)(cid:19)−1

m=1 (cid:98)S(cid:62)
m(

(cid:91)
Υ(m))−1 (cid:98)Sm/ˆσ2. We show in the proof of Theorem 3.4 that

also satisﬁes Assumption 3.3 as an estimator for (cid:0)Υ(m)(cid:1)−1

.

Remark 3.9. The one-step estimator ˆa(m) is asymptotically unbiased, and shares a similar
form to the de-biased estimator proposed by Zhang and Zhang [2014], Van de Geer et al. [2014].
The de-biased estimator in Van de Geer et al. [2014] would take the following form under our
setting:

(cid:98)bm = ( (cid:98)Am)Dm + (cid:98)ΘDm,·

Xt+1,m − X (cid:62)

t (cid:98)Am

(cid:17)

,

1
T

T −1
(cid:88)

t=0

(cid:16)

Xt

15

where (cid:98)Θ is computed by node-wise regression, as an estimator for Υ−1. When dm = |Dm| =
1, this is essentially the same as our estimator ˆa(m), but would be slightly diﬀerent in the
multivariate case. Note that the asymptotic covariance matrix for ˆa(m) equals to the partial
m), and thus is semi-parametric eﬃcient, while ˆbm is only
information matrix I ∗(Am,Dm|Am,Dc
eﬃcient when it is a scalar.

Remark 3.10. (cid:98)RT is also very similar to the test statistic proposed by Neykov et al. [2018]
for VAR model with lag 1. The only diﬀerence lies in the estimation of Var((cid:15)ti), and they only
consider Dantzig selector for estimating A∗ and w∗
m. We will provide a detailed comparison
between their theoretical result with ours in section 3.6.

3.6 Special case: AR(1) with Gaussian noise

Our theoretical guarantee covers VAR models with lag p and sub-Gaussian noise, of which AR(1)
model and Gaussian noise are special cases. Here we explain the consequences of our result under
this special case and provide comparison with Neykov et al. [2018].

When we consider lag p = 1, the constraint for A∗ becomes

∞
(cid:88)





∞
(cid:88)

i=0

j=0



1
2

(cid:13)(A∗)i+j(cid:13)
(cid:13)
2
(cid:13)
2



≤ β, max

m

ρm(A∗) ≤ ρ, max
m

sm(A∗) ≤ s,

T ). The two sparsity conditions and sample size requirement are
with (ρ ∨ s) log M = o(
included in the conditions Neykov et al. [2018] proposes. In addition, they assume the following:

√

(cid:107)A∗(cid:107)1 ≤ C, (cid:107)A∗(cid:107)2 ≤ 1 − ε, (cid:13)

(cid:13)Σ−1(cid:13)

(cid:13)1 ≤ C.

for some 0 < ε < 1. Note that we don’t require these conditions, among which the ﬁrst
and third are quite strong, and the second one (cid:107)A∗(cid:107)2 ≤ 1 − ε is suﬃcient for our condition
(cid:80)∞
2 ≤ β. This follows since if (cid:107)A∗(cid:107)2 ≤ 1 − ε,
i=0

(cid:16)(cid:80)∞
j=0

(cid:13)(A∗)i+j(cid:13)
(cid:13)
2
(cid:13)
2

(cid:17) 1

∞
(cid:88)





∞
(cid:88)

i=0

j=0



1
2

(cid:13)(A∗)i+j(cid:13)
(cid:13)
2
(cid:13)
2



∞
(cid:88)

≤





∞
(cid:88)

i=0

j=0



1
2

(cid:107)A∗(cid:107)2(i+j)
2



≤

(cid:80)∞
i=0(1 − ε)i
(cid:112)1 − (1 − ε)2

≤ (cid:0)2ε − ε2(cid:1)− 1
2 .

Until now the discussion focuses on the case where (cid:15)ti are i.i.d. sub-Gaussian noise of scale
factor Cσ∗, with (σ∗)2 being the variance of (cid:15)ti and lower bounded by some constant. Thus our
setting covers the case where (cid:15)t ∼ N (0, (σ∗)2I) with σ∗ ≥ c. If (cid:15)t ∼ N (0, Ψ) with Ψii ≥ c as
assumed in Neykov et al. [2018], we can still prove the same theoretical guarantee, under even
weaker condition based on spectral density, due to established concentration bounds in Basu
et al. [2015].

16

4 Numerical Experiments

In this section, we provide a simulation study to validate our theoretical results. For simplicity,
our simulation is based on the AR(1) model:

Xt+1 = A∗Xt + (cid:15)t,

t = 0, . . . , T,

(37)

where A∗ ∈ RM ×M is set to be row-wise sparse. Symmetricity is not required in our theory,
but in order to ensure the sparsity of w∗
m, we focus on symmetric matrices under H0, and
slightly asymmetric ones under HA. The eigenvalues of A∗ all fall in the unit circle of the
complex plane, which ensures the existence of stationary solution to this model. White noise (cid:15)ti
is simulated as independent Uniform(−1, 1) in order to satisfy the sub-Gaussianity condition.
Other distributions were also used but not reported since the results were very similar.

To consider multi-variate test sets, throughout the simulation we test the index set D with

d = |D| = 6, which involves three diﬀerent rows and two columns in each row:

D = {(1, 3), (1, 5), (3, 3), (3, 4), (5, 4), (5, 8)}.

The null hypothesis takes the form H0 : (cid:101)AD = µ with some d-dimensional vector µ. Correspond-
ingly, we consider alternative hypothesis HA : (cid:101)AD = µ + T −φ∆, with ∆ randomly selected from
d-dimensional Gaussian distribution, and φ ranges from 0.25 to 1.2.

Under H0, we generate A∗ with diﬀerent row-wise sparsity levels and structures, and for
each A∗, vector µ may diﬀer depending on the corresponding (cid:101)AD. Under HA, A∗ are still the
same matrices as under H0, but only adding the tested indices (cid:101)AD by T −φ∆. The experiments
are repeated under diﬀerent settings of A∗, ∆, M, T and φ.

We use Lasso estimators deﬁned in (25), (27) for the estimation of A∗ and w∗

m, 1 ≤ m ≤ k,
and tuning parameters λA, λw are selected using cross validation. In cross validation, the training
sets are composed of consecutive time series data, with the remaining 10% of the original data
set being testing sets. Under H0, 1000 simulations are carried out under each parameter setting,
while under HA, we have 100 simulations. In the following sections, we look into false positive
rates (FPR) and true positive rates (TPR) of test statistics (cid:101)UT and (cid:98)RT as deﬁned in (32) and
(36), when we set the level of test as α = 0.05.

4.1 Under the Null Hypothesis

(1) Varying sparsity

Here we summarize the experiments with randomly generated A∗, that are symmetric and
row-wise sparse, with diﬀerent sparsity levels ρ deﬁned in (10). Figure 1 shows how FPR
T . We can see that when T
of (cid:101)UT and (cid:98)RT averaged over 1000 experiments vary with

√

17

Figure 1: False positive rate (FPR) of (cid:101)UT and (cid:98)RT v.s.
sparsity level ρ. The red line is the signiﬁcance level α = 0.05.

√

T , with various dimension M and

increases to about 500, the FPR becomes stable and close to α = 0.05 regardless of ρ, M ,
choice between (cid:101)UT and (cid:98)RT .

When the sample size T is small, the test tends to be conservative, which is the consequence
of estimating variance σ∗2 and covariances Υ(m)’s. In the simulation we use naive estimators
for these two quantities, as deﬁned in (31) and (29) which tend to be smaller than the true
parameters. This is because we usually ﬁt noise in the regression, as noticed by Fan et al.
[2012]. As shown in these two ﬁgures, (cid:98)RT is less conservative than (cid:101)UT when T is small,
(cid:101)Υ(m)(cid:17)−1
since the magnitude of (cid:101)Υ(m) is larger than (cid:98)Υ(m), which makes
probably a better estimator for Υ(m). We also summarize the FPR when the variance σ∗2 of
(cid:15)ti is known in Figure 2. We can see from these ﬁgures that (cid:98)UT is still a little conservative
when T is small, while (cid:98)RT with ˆσ2 substituted by σ∗2 is not conservative.

(cid:101)Υ(m)(cid:62)(cid:17)−1

(cid:98)Υ(m) (cid:16)

(cid:16)

(2) Diﬀerent Graph Structures

If we consider the M actors in the time series as nodes in a network, and a nonzero A∗
ij
represents an directed edge from j to i, then each matrix A∗ corresponds to a M -dimensional
directed graph. We experiment with diﬀerent structures of A∗, which also correspond to
diﬀerent graph structure, including block graph or chain graph. Speciﬁcally, we consider

18

Figure 2: FPR of (cid:101)UT and (cid:98)RT when residual variance is known.

matrices with (cid:96)2 norm equal to 0.75:

A(1) =


















1/4 1/2
1/2 1/4

0
0

0
0

· · ·
· · ·

0

0
...
0
0

0

0
...
0
0

1/4 1/2 · · ·

1/2 1/4 · · ·
. . .
. . .
. . .
· · ·
· · ·
· · ·
· · ·
· · ·
· · ·


















0
0
0
0
...
...
...
...
...
...
1/4 1/2
1/2 1/4

,

which is a block graph;

A(2) =














c
c
0
c
c
0
...
. . .
0 · · ·
0 · · ·

0
c
0
. . .
· · ·
· · ·

· · ·
· · ·
c
. . .
c
· · ·

· · ·
· · ·
· · ·
. . .
0
c


0

0


0


...




c

0

,

with constant c chosen to ensure (cid:13)
(cid:13)2 = 0.75, which is a chain graph; and A(3) being
randomly generated symmetric matrix of sparsity level ρ = 2, and largest eigenvalue equal
to 0.75. Figure 3 shows the diﬀerence among these three diﬀerent structures. We can see

(cid:13)A(2)(cid:13)

19

Figure 3: FPR under diﬀerent graph structure. Block refers to A(1), chain refers to A(2) and
random refers to A(3).

that block graph is less accurate than the other two, which is due to a larger variance for
each Xt,Dm − w∗(cid:62)
Investigating the question of how graph structure theoretically
inﬂuences testing performance remains an open and interesting direction.

m Xt,Dc

m.

4.2 Alternative Hypothesis

First we look into how the true positive rate (TPR) varies with (cid:107)T −φ∆(cid:107)2, since we set HA as
(cid:101)AD = µ+T −φ∆ and (cid:107)T −φ∆(cid:107)2 may be viewed as a measure of distance from the null hypothesis.
Fig. 4 only presents the simulation results when A∗ = A(1) and M = 300, while the other choices
of A∗ and M generate very similar results. We can see from these two ﬁgures that as (cid:107)T −φ∆(cid:107)2
increases, TPR approaches 1. The slope increases when sample size T gets larger, or when the
test statistic changes from (cid:98)RT to (cid:101)UT . This aligns with intuition, since when T increases, we are
supposed to distinguish between H0 and HA better, and (cid:101)UT is more conservative than (cid:98)RT as we
show in subsection 4.1.

(cid:13)
(cid:13)
(cid:13) (cid:101)∆

(cid:13)
(cid:13)
(cid:13)2

We also check the inﬂuence of φ. Figure 5 reveals how TPR changes when T increases, if
and φ ﬁxed. If φ < 0.5, TPR converges to 1 very quickly, while if φ > 0.5, TPR

we set

converges to 0.05, but the convergence is slower when φ or

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13) (cid:101)∆
(cid:13)2
Theorem 3.3 and 3.4 states that (cid:101)UT and (cid:98)RT would converge to χ
d,(cid:107) (cid:101)∆(cid:107)2
(cid:13)
(cid:13)
2
(cid:13)
(cid:13)
(cid:13) (cid:101)∆
(cid:13)
2

. The black lines in ﬁgure
converge to some value between 0.05 and 1, depending on d and
5 indicate this convergence value, but since the test tends to be conservative when T is not large
enough, TPR when φ = 0.5 is usually above the black line. The conservative issue is more severe
under HA since the deviation (cid:101)∆ is also multiplied by the estimated variances, which exaggerates

increases. When φ = 0.5,

, thus the TPR should

2

20

Figure 4: True positive rate of (cid:101)UT and (cid:98)RT , when A∗ = A(1) and M = 300

the conservative tendency. However, this may not be a big concern under HA, since we always
want the TPR to be large.

5 Proof Overview

One of the main contributions of this work is the proof technique, which addresses a number
of technical challenges and develops novel concentration bounds for dependent sub-Gaussian
random vectors. In this section, we present and discuss key lemmas for the proof and provide
the main steps for proving Theorems 3.1 and 3.2, deferring the more technically intensive steps
to the supplement.

5.1 Key Lemmas

The major technical challenge lies in proving the following two concentration bounds for depen-
dent sub-Gaussian random vectors.

Lemma 5.1 (Deviation Bound for A∗). Under model (3), when (cid:15)ti are sub-Gaussian noise with
scale factor τ , and A∗ ∈ Ω0 ∪ Ω1,

P

(cid:32)(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
T

T −1
(cid:88)

t=0

(cid:15)tX (cid:62)
t

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

(cid:33)

(cid:114)

log M
T

> C

≤ c1 exp{−c2 log M },

When T ≥ C log M .

Lemma 5.1 is a standard deviation bound for proving estimation error bound of Lasso type
or Dantzig selector type estimators. We apply this lemma both in the proof of Theorem 3.1, 3.2
and Lemma 3.1.

21

= 1, A∗ = A(1). Results for diﬀerent graph size M
Figure 5: TPR of (cid:101)UT and (cid:98)RT when
from 30 to 300 are combined together and average TPR is taken. Red line is signiﬁcance level
α, the value that TPR should converge to when φ < 0.5; while the black line is the convergence
point speciﬁed in Theorem 3.2 when φ = 0.5.

(cid:13)
˜∆
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)2

Lemma 5.2. Under model (3), when (cid:15)ti are sub-Gaussian noise with constant scale factor τ ,
and A∗ ∈ Ω0 ∪ Ω1, if B ∈ RpM ×pM is a symmetric matrix, we have

P

(cid:32)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
T

T −1
(cid:88)

t=0

X (cid:62)

(cid:12)
(cid:12)
(cid:12)
t BXt − tr(BΥ)
(cid:12)
(cid:12)

(cid:33)

(cid:26)

> δ

≤ c1 exp

−c2T min

(cid:26) δ

(cid:107)B(cid:107)2

,

δ2
(cid:107)B(cid:107)tr(cid:107)B(cid:107)2

(cid:27)(cid:27)

.

Lemma 5.2 provides concentration bound for the sample average of general quadratic form
t BXt, and is very helpful in proving martingale CLT under our setting, REC, Lemma 3.3,

X (cid:62)
etc.

In the Gaussian case, both these lemmas follow from prior work in Basu et al. [2015] which
relies on the fact that dependent Gaussian vectors can be rotated to be independent. Since
dependent sub-Gaussian random variables cannot be rotated to be independent (only uncorre-
lated), we exploit the independence of (cid:15)t by representing each Xt by linear function of the inﬁnite
series {(cid:15)i}i=t
i=−∞ and then use a careful truncation argument. We analyze suﬃciently many terms
in the summation, and control the inﬁnite residues.

5.2 Proof of Theorem 3.1

Proof. Suppose A∗ ∈ Ω0. We will use Ci, ci to refer to constants that only depend on p, d, β, τ
(not M or T ), and diﬀerent constants might share the same notation.

The proof can be divided into two major parts: showing the convergence of UT to χ2

d, and

22

bounding the estimation error

(cid:12)
(cid:12)
(cid:12) (cid:98)UT − UT

(cid:12)
(cid:12)
(cid:12). Formally, for any ε > 0,

P( (cid:98)UT ≤ x) − Fd(x)
(cid:12)
≤P(UT ≤ x + ε) + P(
(cid:12)
(cid:12) (cid:98)UT − UT
≤ |P(UT ≤ x + ε) − Fd(x + ε)| + Fd(x + ε) − Fd(x) + P

(cid:12)
(cid:12)
(cid:12) > ε) − Fd(x)

(cid:16)(cid:12)
(cid:12)
(cid:12) (cid:98)UT − UT

(cid:12)
(cid:17)
(cid:12)
(cid:12) > ε

,

and

Fd(x) − P( (cid:98)UT ≤ x)

=P( (cid:98)UT > x) − (1 − Fd(x))
(cid:12)
≤P(UT > x − ε) + P(
(cid:12)
(cid:12) (cid:98)UT − UT

(cid:12)
(cid:12)
(cid:12) > ε) − 1 + Fd(x)

≤ |Fd(x − ε) − P (UT ≤ x − ε)| + Fd(x) − Fd(x − ε) + P

(cid:16)(cid:12)
(cid:12)
(cid:12) (cid:98)UT − UT

(cid:12)
(cid:17)
(cid:12)
(cid:12) > ε

,

which implies

(cid:12)
(cid:12)
P( (cid:98)UT ≤ x) − Fd(x)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

|P(UT ≤ y) − Fd(y)| + Fd(x + ε) − Fd(x − ε) + P

≤ sup
y∈R

(cid:16)(cid:12)
(cid:12)
(cid:12) (cid:98)UT − UT

(cid:12)
(cid:17)
(cid:12)
(cid:12) > ε

.

(38)

In the following, we provide bounds on each of the three terms. The following lemma
, of which the convergence of

shows the uniform weak convergence rate of (cid:107)VT + µ(cid:107)2
UT = (cid:107)VT (cid:107)2

d is a special case.

2 to χ2

2 to χ2

d,(cid:107)µ(cid:107)2
2

Lemma 5.3 (Convergence Rate of (cid:107)VT + µ(cid:107)2
noise of scale factor τ , then for any A∗ ∈ Ω0, ∀µ ∈ Rd,

2). Under model (3) with (cid:15)ti being sub-Gaussian

(cid:12)
P((cid:107)VT + µ(cid:107)2
(cid:12)
(cid:12)

sup
x∈R

2 ≤ x) − Fd,(cid:107)µ(cid:107)2

(x)

(cid:12)
(cid:12) ≤ C((cid:107)µ(cid:107)2)T − 1
(cid:12)
8 ,

(39)

2

when T > C for some absolute constant C, where C((cid:107)µ(cid:107)2) is a constant depending on and is
non-decreasing with respect to (cid:107)µ(cid:107)2.

This Lemma is proved in section C, by applying a uniform martingale central limit theorem

result. Thus, by Lemma 5.3, if T > C for some constant C,

|P(UT ≤ y) − Fd(y)| ≤ CT − 1
8 .

sup
y∈R

Fd(x + ε) − Fd(x − ε) ≤ C2ε

Meanwhile,

since χ2

d has bounded density.

23

Now we only need to choose a proper ε and bound P

(cid:16)(cid:12)
(cid:12)
(cid:12) (cid:98)UT − UT

(cid:12)
(cid:17)
(cid:12)
(cid:12) > ε

.

(cid:12)
(cid:12)
(cid:12) (cid:98)UT − UT

(cid:12)
(cid:12)
(cid:12) ≤

≤

≤

(cid:16)

T (cid:98)S(cid:62)
m

(cid:12)
(cid:12)T (cid:98)S(cid:62)
(cid:12)
m(
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:13)
(cid:13)
Υ(m) 1
(cid:13)
(cid:13)
√

2

k
(cid:88)

m=1
k
(cid:88)

m=1
k
(cid:88)

m=1
(cid:13)
(cid:13)
(cid:13)

+

(cid:91)
Υ(m))−1 (cid:98)Sm − (cid:107)VT,m(cid:107)2
2

(cid:12)
(cid:12)
(cid:12)

Υ(m))−1 − (Υ(m))−1(cid:17)
(cid:91)

(

(cid:98)Sm +

√

(cid:13)
(cid:13)
(cid:13)

T (Υ(m))− 1

2 (cid:98)Sm

(cid:13)
2
(cid:13)
(cid:13)
2

− (cid:107)VT,m(cid:107)2
2

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(40)

Υ(m)(cid:17)−1
(cid:16)(cid:91)

Υ(m) 1

2 − I

(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

√

(cid:13)
(cid:13)
(cid:13)

T (Υ(m))− 1

2 (cid:98)Sm

(cid:13)
2
(cid:13)
(cid:13)
1

T (Υ(m))− 1

2 ( (cid:98)Sm − Sm)

(cid:13)
2
(cid:13)
(cid:13)
2

+ 2 (cid:107)VT,m(cid:107)2

√

(cid:13)
(cid:13)
(cid:13)

T (Υ(m))− 1

(cid:13)
(cid:13)
2 ( (cid:98)Sm − Sm)
(cid:13)2

.

Deﬁne Em =

√

T (Υ(m))− 1

2

(cid:12)
(cid:12)
(cid:12) (cid:98)UT − UT

(cid:12)
(cid:12)
(cid:12) ≤

(cid:16)

(cid:98)Sm − Sm

(cid:17)

, then (40) turns into

k
(cid:88)

(cid:107)Em(cid:107)2

2 + 2 (cid:107)VT,m(cid:107)2 (cid:107)Em(cid:107)2

m=1
(cid:13)
(cid:13)
Υ(m) 1
(cid:13)
(cid:13)

+

2

Υ(m)(cid:17)−1
(cid:16)(cid:91)

Υ(m) 1

2 − I

(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

(cid:0)(cid:107)VT,m(cid:107)2 + (cid:107)Em(cid:107)2

(cid:1)2 .

(41)

We can bound (cid:107)VT,m(cid:107)2 using Lemma 5.3 and

(cid:13)
(cid:13)
Υ(m) 1
(cid:13)
(cid:13)

2

Υ(m)(cid:17)−1
(cid:16)(cid:91)

Υ(m) 1

2 − I

(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

using Lemma

19, while for bounding the estimation induced error (cid:107)Em(cid:107)2, we ﬁrst apply the following lemma
to bound the eigenvalues of Υ(m).

Lemma 5.4. Consider the model (2) with independent noise (cid:15)ti of unit variance, A∗ satisﬁes
(13), then the eigenvalues of Υ can be bounded as follows:

0 < C1(β) ≤ Λmin (Υ) ≤ Λmax (Υ) ≤ C2(β).

Lemma 5.4 is proved based on established results in Basu et al. [2015]. Note that we assumed
=

unit variance in Theorem 3.1 and 3.2, so we can apply Lemma 5.4 here. Since (cid:0)Υ(m)(cid:1)−1
(cid:0)Υ−1(cid:1)

, applying Lemma 5.4 would lead us to the following:

Dm,Dm

(cid:16)

(cid:16)

(Υ(m))−1(cid:17)
(Υ(m))−1(cid:17)

Λmin

Λmax

≥ Λmin(Υ−1) = Λmax(Υ)−1 ≥ C,

≤ Λmax(Υ−1) = Λmin(Υ)−1 ≤ C.

(42)

Thus we have

(cid:107)Em(cid:107)2 ≤ C

√

T

(cid:13)
(cid:13)
(cid:13) (cid:98)Sm − Sm

(cid:13)
(cid:13)
(cid:13)2

,

24

with

(cid:98)Sm − Sm =( ˆwm − w∗

m)(cid:62) 1
T

T −1
(cid:88)

t=0

Xt,Dc

m(cid:15)t,m

+

1
T

T −1
(cid:88)

(Xt,Dm − w∗(cid:62)

m Xt,Dc

m)X (cid:62)
t,Dc
m

(cid:16)

( (cid:98)Am)Dc

m − (A∗

m)Dc

m

(cid:17)

(43)

t=0

− ( ˆwm − w∗

m)(cid:62)

(cid:32)

1
T

T −1
(cid:88)

t=0

(cid:33)

(cid:16)

Xt,Dc

mX (cid:62)
t,Dc
m

( (cid:98)Am)Dc

m − (A∗

m)Dc

m

(cid:17)

.

The following two lemmas provide bounds for

(cid:13)
(cid:13)
(cid:13)

1
T

(cid:80)T −1

t=0 Xt,Dc

m(cid:15)t,m

(cid:13)
(cid:13)
(cid:13)∞

, and

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
T

T −1
(cid:88)

(Xt,Dm − w∗(cid:62)

m Xt,Dc

m)X (cid:62)
t,Dc
m

t=0

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

.

Lemma 5.5. When T ≥ C log M ,

P

(cid:32)(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
T

T −1
(cid:88)

t=0

(cid:15)tX (cid:62)
t

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

(cid:33)

(cid:114)

log M
T

> C

≤ c1 exp{−c2 log M }.

Lemma 5.1 is a common condition in high-dimensional regression problems, and is usually

referred to as deviation bound. We will prove it in Section C.

Lemma 5.6 (Deviation Bound for w∗
1 ≤ m ≤ k,

m). With probability at least 1 − c1 exp{−c2 log M }, for all

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
T

T −1
(cid:88)

(Xt,Dm − w∗(cid:62)

m Xt,Dc

m)X (cid:62)
t,Dc
m

t=0

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

(cid:114)

≤ C

log M
T

.

Lemma 5.6 can also be viewed as a deviation bound, if we consider a regression problem
m as covariates. This is also proved in Section C. Applying

with Xt,Dm as response and Xt,Dc
Assumptions 3.1 and 3.2, with probability at least 1 − c1 exp{−c2 log M },

(cid:107)Em(cid:107)2 ≤ C

(sm ∨ ρm) log M
√
T

√

+

1
2

1
2

T Q

1 Q

2 ≤ C

(sm ∨ ρm) log M
√
T

,

where

(cid:18)(cid:16)

(cid:17)

(cid:98)Am

Q1 =

Dc
m

(cid:34)

Q2 =tr

( ˆwm − w∗

m)(cid:62)

− (A∗

m)Dc

m

(cid:19)(cid:62) (cid:32)

1
T

T −1
(cid:88)

t=0

Xt,Dc

mX (cid:62)
t,Dc
m

(cid:33) (cid:18)(cid:16)

(cid:17)

(cid:98)Am

Dc
m

(cid:19)

− (A∗

m)Dc

m

(cid:32)

1
T

T −1
(cid:88)

t=0

(cid:33)

(cid:35)

Xt,Dc

mX (cid:62)
t,Dc
m

( ˆwm − w∗

m)

,

25

and Assumption 3.1 and 3.2 implies Q1 ≤ C ρm log M
straightforward: to see why it holds true, let ˆhm = (cid:98)Am − A∗
have

T

and Q2 ≤ C sm log M

T
m and H = 1
T

. The former is not
t=0 XtX (cid:62)
t , then we

(cid:80)T −1

X (cid:62)

t,Dc
m

(cid:17)

(cid:16)ˆhm

(cid:21)2

Dc
m

X (cid:62)
t

ˆhm − X (cid:62)

t,Dm

(cid:21)2

(cid:17)

(cid:16)ˆhm

Dm

T −1
(cid:88)

(cid:20)

t=0
T −1
(cid:88)

(cid:20)

t=0
T −1
(cid:88)

(cid:34)

(cid:16)

Q1 =

=

≤

1
T

1
T

2
T

t=0
mHˆhm + 2
ρm log M
T

.

=2ˆh(cid:62)

≤C

X (cid:62)
t

ˆhm

(cid:17)2

+

(cid:18)

X (cid:62)

t,Dm

(cid:17)

(cid:16)ˆhm

(cid:19)2(cid:35)

Dm

(44)

(cid:16)ˆhm

(cid:17)(cid:62)

Dm

HDm,Dm

(cid:17)

(cid:16)ˆhm

Dm

Here we apply Assumption 3.1, and the fact that

(cid:17)(cid:62)

(cid:16)ˆhm

HDm,Dm

Dm
≤dm(cid:107)H(cid:107)∞(cid:107)ˆhm(cid:107)2
2

(cid:17)

(cid:16)ˆhm

Dm

≤dm ((cid:107)H − Υ(cid:107)∞ + Λmax(Υ))

ρm log M
T

≤C

ρm log M
T

.

The last inequality is due to Lemma 5.4 and the following lemma:

Lemma 5.7. With probability at least 1 − c1 exp{−c2 log M },

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
T

T −1
(cid:88)

t=0

XtX (cid:62)

(cid:13)
(cid:13)
(cid:13)
t − Υ
(cid:13)
(cid:13)∞

(cid:114)

≤ C

log M
T

.

Therefore, by taking a union bound, we show that

(cid:107)Em(cid:107)2 ≤ C

(sm ∨ ρm) log M
√
T

,

for any 1 ≤ m ≤ k, with probability at least 1 − c1 exp{−c2 log M }.

Meanwhile, by applying Lemma 5.3, one can show that for y >

√

5d,

P (cid:0)(cid:107)VT,m(cid:107)2 > y(cid:1) ≤CT − 1
≤CT − 1
≤CT − 1

8 + 1 − Fd(y2)
8 + exp{−(y2 − d)/4}

8 + Cy−2,

(45)

26

where the second inequality is due to a χ2
d tail bound established in Laurent and Massart [2000]
(see Lemma 1 in Laurent and Massart [2000]), and the third inequality comes from the fact
that, ∀ constant C1 > 0, ∃ constant C2 such that

(cid:16) (s∨ρ) log M
√

Let y =
probability at least

T

y2e−C1y2

≤ C2.

sup
y≥0

(cid:17)− 1

4 and plug it into (41), then with Assumption 3.3, we can show that with

1 − c1 exp{−c2 log M } − c3T − 1

8 − c4

(cid:18) (s ∨ ρ) log M
√

T

(cid:19) 1
2

,

(cid:18) (s ∨ ρ) log M
√

T

(cid:19)− 1
2

+ C2

(cid:18) (s ∨ ρ) log M
√

T

(cid:19) 3
4

the following holds:

(cid:12)
(cid:12)
(cid:12) (cid:98)UT − UT

(cid:12)
(cid:12)
(cid:12) ≤C1

(s ∨ ρ) log M
√
T
(cid:18) (s ∨ ρ) log M
√

T

(cid:19) 1
2

,

≤C

√

if (s ∨ ρ) log M = o(
(cid:16) (s∨ρ) log M
√

(cid:17) 1
2 ,

ε = C

T

T ) and T > C for some constant C. Therefore, applying (38) with

(cid:12)
P( (cid:98)UT ≤ x) − Fd(x)
(cid:12)
(cid:12)

(cid:12)
(cid:12) ≤ C1T − 1
(cid:12)

8 + C2

(cid:18) (s ∨ ρ) log M
√

T

(cid:19) 1
2

+ C3 exp{−c log M }.

Since constants Ci only depend on d, β and τ , this bound also holds for supremum over A∗ ∈ Ω0
and x ∈ R. Note that for a clear presentation, we are not showing the sharpest bound, which
can be obtained by choosing a diﬀerent y.

5.3 Proof of Theorem 3.2

proof of Theorem 3.2. We prove this case by case. We will use Ci, ci to refer to constants that
only depend on d, β, ∆, φ, and diﬀerent constants might share the same notation.

Similar from the proof of Theorem 3.1, the major part of the proof is devoted to bounding

(cid:12)
(cid:12) (cid:98)UT − (cid:107)VT + µ(cid:107)2
(cid:12)

2

(cid:12)
(cid:12) with high probability for some vector µ ∈ Rd.
(cid:12)

(1) φ = 1
2

Suppose A∗ ∈ Ω1. Using similar deduction as in the proof of Theorem 3.1, for any ε > 0,

(cid:12)
P( (cid:98)UT ≤ x) − Fd,(cid:107) (cid:101)∆(cid:107)2
(cid:12)
(cid:12)
2 ≤ y

(cid:107)VT − (cid:101)∆(cid:107)2

(cid:16)

(cid:12)
(cid:12)
(x)
(cid:12)
(cid:17)
− Fd,(cid:107) (cid:101)∆(cid:107)2

(cid:12)
P
(cid:12)
(cid:12)

2

2

≤ sup
y∈R

(cid:12)
(cid:12)
(y)
(cid:12)

(46)

+ Fd,(cid:107) (cid:101)∆(cid:107)2

2

(x + ε) − Fd,(cid:107) (cid:101)∆(cid:107)2

2

(x − ε) + P

(cid:18)(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:98)UT −

(cid:13)
(cid:13)
(cid:13)VT − (cid:101)∆

(cid:13)
2
(cid:13)
(cid:13)
2

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:19)

> ε

.

27

(a) Bounding the ﬁrst two terms

The ﬁrst term is the convergence rate of (cid:107)VT − (cid:101)∆(cid:107)2

2 to χ2

d,(cid:107) (cid:101)∆(cid:107)2
2

. By Lemma 5.3,

(cid:16)

(cid:12)
P
(cid:12)
(cid:12)

sup
y∈R

(cid:107)VT − (cid:101)∆(cid:107)2

2 ≤ y

(cid:17)

− Fd,(cid:107) (cid:101)∆(cid:107)2

2

(y)

(cid:12)
(cid:12) ≤ C((cid:107) (cid:101)∆(cid:107)2)T − 1
(cid:12)

8 ≤ C(cid:107)∆(cid:107)2T − 1
8 .

The last inequality is due to

(cid:107) (cid:101)∆(cid:107)2

2 =

k
(cid:88)

m=1

(cid:107) (cid:101)∆m(cid:107)2

2 ≤

k
(cid:88)

m=1

(cid:16)

Υ(m)(cid:17)

Λmax

(cid:107)∆(cid:107)2
2,

and an upper bound for Λmax

(cid:0)Υ(m)(cid:1) in (42).

Bounding the second term in (46) is not straightforward as bounding Fd(x + ε) − Fd(x − ε)
in the proof of Theorem 3.1, since (cid:101)∆ is not a constant vector when A∗ takes diﬀerent
values in Ω∗
as shown above. One can show that

1. We only have a uniform bound of

(cid:13)
(cid:13)
(cid:13) (cid:101)∆

(cid:13)
(cid:13)
(cid:13)2

2

Fd,(cid:107) (cid:101)∆(cid:107)2



C(d)

(x + ε) − Fd,(cid:107) (cid:101)∆(cid:107)2
(cid:16)

2

(x + ε)

d
2 − (x − ε)

d
2

(cid:16)



C(d)

(x + ε)

d
2 − (x − ε)

d
2

≤

(x − ε) = P

(cid:18)(cid:13)
(cid:13)
(cid:13)Z + (cid:101)∆

(cid:13)
2
(cid:13)
(cid:13)
2

∈ (x − ε, x + ε]

(cid:19)

(cid:17)

(cid:17)

√

e−(

x−ε−(cid:107) (cid:101)∆(cid:107)2)2/2,

,

√

√

x − ε ≥ 2(cid:107) (cid:101)∆(cid:107)2

x − ε < 2(cid:107) (cid:101)∆(cid:107)2

,

where Z is a d-dimensional standard Gaussian random vector with density φ(z) = C(d) exp{−(cid:107)z(cid:107)2
The last inequality holds because that, for any set C ⊂ Rd,

2/2}.

P (Z ∈ C) ≤ sup
z∈C

φ(z)

(cid:90)

z∈C

dz.

Suppose 0 < ε ≤ 1, then if

√

x − ε ≥ 2(cid:107) (cid:101)∆(cid:107)2,

(cid:16)

(x + ε)

d
2 − (x − ε)

d
2

(cid:17)

exp

√

(cid:110)

−(

x − ε − (cid:107) (cid:101)∆(cid:107)2)2/2

(cid:111)

otherwise,

Thus,

(b) Bounding

(cid:12)
(cid:12)
(cid:98)UT −
(cid:12)
(cid:12)

≤dε(x + ε)

d

2 −1 exp{−(x − ε)/8}

≤dεe

ε
4 sup
y≥0

y

d

2 −1 exp{−y/8} ≤ C(d)ε,

(cid:16)

(x + ε)

d
2 − (x − ε)

d
2

(cid:17)

≤ dε(x + ε)

d

2 −1 ≤ C(d)ε.

2

Fd,(cid:107) (cid:101)∆(cid:107)2
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:13)
2
(cid:13)
(cid:13)
2

(cid:13)
(cid:13)
(cid:13)VT − (cid:101)∆

(x + ε) − Fd,(cid:107) (cid:101)∆(cid:107)2

2

(x − ε) ≤ C(d)ε.

28

Similar from (41) in the proof of Theorem 3.1, it is straightforward to show that

(cid:12)
(cid:12)
(cid:98)UT −
(cid:12)
(cid:12)

(cid:13)
(cid:13)
(cid:13)VT − (cid:101)∆

(cid:13)
2
(cid:13)
(cid:13)
2

(cid:12)
(cid:12)
(cid:12)
(cid:12)

k
(cid:88)

≤

(cid:107)Em(cid:107)2

2 + 2

(cid:13)
(cid:13)
(cid:13)VT,m − (cid:101)∆m

(cid:13)
(cid:13)
(cid:13)2

(cid:107)Em(cid:107)2

(47)

m=1
(cid:13)
(cid:13)
Υ(m) 1
(cid:13)
(cid:13)

+

2

Υ(m)(cid:17)−1
(cid:16)(cid:91)

Υ(m) 1

2 − I

(cid:16)

(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

(cid:107)VT,m − (cid:101)∆m(cid:107)2 + (cid:107)Em(cid:107)2

(cid:17)2

,

where Em =

√

T (Υ(m))− 1

VT,m − (cid:101)∆m =

√

2 (cid:98)Sm − VT,m + (cid:101)∆m. To bound (cid:107)Em(cid:107)2, note that
√
T (Υ(m))− 1

T (Υ(m))− 1

2 Sm − (cid:101)∆m =

2 (Sm − Υ(m)(A∗

m)Dm),

and

Sm − Υ(m)(A∗

m)Dm =

(cid:34)

−

1
T

1
T

t
T −1
(cid:88)

t=0

(cid:88)

(Xt,Dm − w∗(cid:62)

m Xt,Dc

m)X (cid:62)

t,Dm − Υ(m)

(cid:35)

(A∗

m)Dm

(cid:16)

Xt+1,m − (A∗

m)(cid:62)
Dc
m

Xt,Dc

m

(cid:17) (cid:16)

Xt,Dm − w∗(cid:62)

m Xt,Dc

m

= (cid:101)Sm + W ∗
m

(cid:32)

1
T

(cid:88)

t

(cid:33)

XtX (cid:62)

t,Dm − Υ·,Dm

(A∗

m)Dm,

with (cid:101)Sm ∈ Rdm and W ∗

m ∈ Rdm×M deﬁned as follows:

(cid:101)Sm = −

1
T

T −1
(cid:88)

(Xt+1,m − (A∗

m)(cid:62)
Dc
m

Xt,Dc

m)(Xt,Dm − w∗(cid:62)

m Xt,Dc

m),

t=0
(W ∗

m)·,Dm = Idm×dm,

(W ∗

m)·,Dc

m = w∗(cid:62)
m .

(cid:17)

(48)

Therefore,

(cid:107)Em(cid:107)2 ≤

√

(cid:13)
(cid:13)
(cid:13)

+

T (Υ(m))− 1
(cid:13)
(cid:13)
(Υ(m))− 1
(cid:13)
(cid:13)
(cid:13)
√

(cid:13)
(cid:13)
2 ( (cid:98)Sm − (cid:101)Sm)
(cid:13)2
(cid:32)

2 W ∗
m

(cid:88)

1
T

≤C

T

(cid:13)
(cid:13)
(cid:13) (cid:98)Sm − (cid:101)Sm

(cid:13)
(cid:13)
(cid:13)2

+ C

(cid:33)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)2

XtX (cid:62)

t,Dm − Υ·,Dm

∆m

t

(cid:112)

dm max

i

(cid:107)(W ∗

m)i·(cid:107)1

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
T

XtX (cid:62)

(cid:13)
(cid:13)
(cid:13)
t − Υ
(cid:13)
(cid:13)∞

.

(cid:88)

t

The last inequality applies (42). Meanwhile,

max
i

(cid:107)(W ∗

m)i·(cid:107)1 =1 + max

i

m)·i(cid:107)1

(cid:107)(w∗
√

sm (cid:107)(w∗

m)·i(cid:107)2

≤1 + max

i
√

≤1 +

≤1 + C

√

≤1 + C

29

m)−1 max

m,Dc

i

smΛmin(ΥDc
√

(cid:112)

sm max

i

(Υ2)ii

smΛmax(Υ) ≤ C

√

sm.

(cid:107)Υ·i(cid:107)2

(49)

The ﬁrst equality and second inequality come from the deﬁnition of W ∗
m; the third
2 = (cid:0)Υ2(cid:1)
inequality is because that (cid:107)Υ·i(cid:107)2
ii =
e(cid:62)
i Υ2ei ≤ Λmax(Υ)2; and the last inequality is obtained from Lemma 5.4. Applying Lemma
5.7 leads us to

ii; the fourth inequality is due to that (cid:0)Υ2(cid:1)

m and w∗

(cid:107)Em(cid:107)2 ≤ C

We can write (cid:98)Sm − (cid:101)Sm as

(cid:114)

sm log M
T

+ C

√

T (cid:107) (cid:98)Sm − (cid:101)Sm(cid:107)2.

(cid:98)Sm − (cid:101)Sm =( ˆwm − w∗

m)(cid:62) 1
T

T −1
(cid:88)

t=0

(cid:16)

Xt,Dc

m

(cid:15)t,m + T − 1

2 ∆(cid:62)

mXt,Dm

(cid:17)

(cid:18)(cid:16)

(cid:98)Am

(cid:18)(cid:16)

(cid:98)Am

+

−

(cid:17)

(cid:17)

Dc
m

Dc
m

− (A∗

m)Dc

m

− (A∗

m)Dc

m

(cid:19)(cid:62) 1
T

(cid:19)(cid:62) 1
T

T −1
(cid:88)

t=0
T −1
(cid:88)

t=0

(cid:16)

Xt,Dc

m

Xt,Dm − w∗(cid:62)

m Xt,Dc

m

(cid:17)(cid:62)

Xt,Dc

mX (cid:62)
t,Dc
m

( ˆwm − w∗(cid:62)

m ).

Note that

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
T

T −1
(cid:88)

t=0

Xt,Dc

mX (cid:62)

t,Dm

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

≤

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
T
(cid:114)

≤C

XtX (cid:62)

(cid:13)
(cid:13)
(cid:13)
t − Υ
(cid:13)
(cid:13)∞

(cid:88)

t

+ (cid:107)Υ(cid:107)∞

log M
T

+ (cid:107)Υ(cid:107)2 ≤ C,

(50)

due to Lemma 5.4 and 5.7, which further implies

(cid:13)
(cid:13)
( ˆwm − w∗
(cid:13)
(cid:13)
(cid:13)

m)(cid:62) 1
T

T −1
(cid:88)

t=0

Xt,Dc

mX (cid:62)

t,Dm∆m

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)2

≤ C(cid:107) ˆwm − w∗

m(cid:107)1.

Applying Assumption 3.1 to 3.3, Lemma 5.1, 5.6, one can show that with probability at
least 1 − c1 exp{−c2 log M },

(cid:107)Em(cid:107)2 ≤ C

(sm ∨ ρm) log M
√
T

,

(51)

(cid:13)
(cid:13)VT,m − (Υ(m))
(cid:13)

with the same arguments as bounding (cid:107) (cid:98)Sm − Sm(cid:107)2 under H0.
(cid:13)
(cid:13)
While for
(cid:13)2
(cid:16)(cid:13)
(cid:13)VT,m − (Υ(m))
(cid:13)

, applying Lemma 5.3 leads us to

2 ∆m

2 ∆m

> y

P

(cid:17)

1

≤C1T − 1
=C1T − 1
≤C1T − 1

8 + 1 − Fd,(cid:107) (cid:101)∆(cid:107)2
(cid:16)
8 + P

(cid:107)Z + (cid:101)∆(cid:107)2

(cid:16)

8 + P

(cid:107)Z(cid:107)2

1

(cid:13)
(cid:13)
(cid:13)2
(y2)
2 > y2(cid:17)
2 > (y − C(cid:107)∆(cid:107)2)2(cid:17)

2

,

30

for any y ≥ 0, where Z ∼ N (0, Id). We apply the tail bound for χ2
and Massart [2000]) as in (45), and obtain
2 > (y − C(cid:107)∆(cid:107)2)2(cid:17)

≤ C (y − C(cid:107)∆(cid:107)2)−2 ≤ Cy−2,

(cid:107)Z(cid:107)2

P

(cid:16)

d (Lemma 1 in Laurent

when y > C for some constant C. Let y =
y, (51) and (19) into (47), one can show that

(cid:16) (s∨ρ) log M
√

T

(cid:17)− 1

4 , and plug

(cid:13)
(cid:13)VT,m − (Υ(m))
(cid:13)

1

2 ∆m

(cid:13)
(cid:13)
(cid:13)2

≤

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:98)UT −

(cid:13)
(cid:13)
2
(cid:13)
(cid:13)
(cid:13)VT − (cid:101)∆
(cid:13)
2
(cid:18) (s ∨ ρ) log M
√

≤C1

T

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:19) 3
4

+ C2

(s ∨ ρ) log M
√
T

(cid:18) (s ∨ ρ) log M
√

T

(cid:19)− 1
2

≤C

(cid:18) (s ∨ ρ) log M
√

T

(cid:19) 1
2

,

with probability at least

1 − c1 exp{−c2 log M } − c3T − 1

8 − c4

(cid:18) (s ∨ ρ) log M
√

T

(cid:19) 1
2

,

if (s ∨ ρ) log M = o(T ) and T > C.

Therefore, applying (46) with ε = C

(cid:16) (s∨ρ) log M
√

T

(cid:17) 1

2 leads to

(cid:12)
P( (cid:98)UT ≤ x) − Fd(x)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:18) (s ∨ ρ) log M
√

T

≤C1T − 1

8 + C2

(cid:19) 1
2

+ C3 exp{−C4 log M }.

Since constants Ci only depend on d, β, ∆, τ , this bound also holds for supremum over
A∗ ∈ Ω1 and x ∈ R.

(2) 0 < φ < 1
2

First we provide a lower bound for (cid:98)UT with high probability. Since bounds in Assumption
3.1 to 3.3, Lemma 5.1 to 5.7 hold with probability at least 1 − c1 exp{−c2 log M }, we apply
these bounds directly in following deduction. Meanwhile, we always assume (ρ∨s) log M =

31

√

o(

T ) and T > C for desired constant C. With these conditions, one can show that

(cid:98)UT =

≥

k
(cid:88)

m=1
k
(cid:88)

m=1

T (cid:98)S(cid:62)
m(

(cid:91)
Υ(m))−1 (cid:98)Sm

T (cid:107)Υ(m)− 1

2 (cid:98)Sm(cid:107)2
2

(cid:16)

1 − dm

(cid:13)
(cid:13)Υ(m) 1
(cid:13)
2 (

(cid:91)
Υ(m))−1Υ(m) 1

2 − I

≥CT

k
(cid:88)

m=1



(cid:32)

≥C



T

(cid:13)
(cid:13)(Υ(m))− 1
(cid:13)

2 (cid:98)Sm

(cid:13)
2
(cid:13)
(cid:13)
2

k
(cid:88)

m=1

(cid:13)
(cid:13)(Υ(m))− 1
(cid:13)

2 ( (cid:98)Sm − Sm)

(cid:33) 1

2

(cid:13)
2
(cid:13)
(cid:13)
2

2



− (cid:107)VT (cid:107)2



.

(cid:17)

(cid:13)
(cid:13)
(cid:13)∞

(52)

(cid:13)
(cid:13)Υ(m) 1
(cid:13)
2 (

(cid:91)
Υ(m))−1Υ(m) 1

2 − I

(cid:13)
(cid:13)
(cid:13)∞

con-

2 ( (cid:98)Sm − Sm)

in the following. First write (cid:98)Sm−Sm

The third line is due to Assumption 3.3, which implies

verges to 0 under our scaling (ρ ∨ s) log M = o(

We provide a lower bound for
as

(cid:13)
(cid:13)(Υ(m))− 1
(cid:13)

√

T ).
(cid:13)
2
(cid:13)
(cid:13)
2

(cid:98)Sm − Sm =( ˆwm − w∗

m)(cid:62)

(cid:32)

1
T

T −1
(cid:88)

t=0

(cid:33)

(cid:15)t,mXt,Dc

m

−

1
T

+

1
T

T −1
(cid:88)

(Xt,Dm − ˆw(cid:62)

mXt,Dc

m)X (cid:62)

t,Dm(A∗

m)Dm

t=0
T −1
(cid:88)

(Xt,Dm − ˆw(cid:62)

mXt,Dc

m)X (cid:62)
t,Dc
m

(( (cid:98)Am)Dc

m − (A∗

m)Dc

m)

(cid:44)E(1)

t=0
m + E(3)
m + E(2)
m ,
(cid:13)
(cid:13)
(cid:13)
(cid:13)E(1)
(cid:13)E(3)
(cid:13)
(cid:13)
(cid:13)
(cid:13)2

we ﬁnd the upper bounds for

(cid:13)
(cid:13)
(cid:13)2
ing. Applying Assumption 3.2 and Lemma 5.1 provides an upper bound for

and lower bound for

(cid:13)
(cid:13)E(2)
(cid:13)

(cid:13)
(cid:13)
(cid:13)2

m

m

m

,

in the follow-
(cid:13)
(cid:13)
(cid:13)E(1)
(cid:13)
(cid:13)
(cid:13)2

m

:

(cid:107)E(1)

m (cid:107)2 ≤ (cid:107) ˆwm − w∗

m(cid:107)1

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
T

T −1
(cid:88)

t=0

Xt(cid:15)(cid:62)
t

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

≤

sm log M
T

.

Since

(cid:13)
(cid:13)E(3)
(cid:13)

m

(cid:13)
(cid:13)
(cid:13)2

≤

(cid:13)
(cid:13)
( ˆwm − w∗
(cid:13)
(cid:13)
(cid:13)

m)(cid:62) 1
T

T −1
(cid:88)

t=0

Xt,Dc

mX (cid:62)
t,Dc
m

(( (cid:98)Am)Dc

m − (A∗

m)Dc

(cid:13)
(cid:13)
(cid:13)
m)
(cid:13)
(cid:13)2

(cid:112)

dm

+

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
T

T −1
(cid:88)

(Xt,Dm − w∗(cid:62)

m Xt,Dc

m)X (cid:62)
t,Dc
m

t=0

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

(cid:13)
(cid:13)
(cid:13)( (cid:98)Am)Dc

m − (A∗

m)Dc

(cid:13)
(cid:13)
m)
(cid:13)1

,

then using the same argument as bounding (cid:107) (cid:98)Sm − Sm(cid:107)2 when proving Theorem 3.1, we
have

(cid:13)
(cid:13)E(3)
(cid:13)

m

(cid:13)
(cid:13)
(cid:13)2

≤ C

(sm ∨ ρm) log M
T

.

32

To lower bound (cid:107)E(2)

m (cid:107)2, ﬁrst note that

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
T

T −1
(cid:88)

(cid:16)

t=0

Xt,Dm − ˆw(cid:62)

mXt,Dm

(cid:17)

X (cid:62)

t,Dc
m

− Υ(m)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
T

T −1
(cid:88)

t=0

XtX (cid:62)

(cid:13)
(cid:13)
(cid:13)
t − Υ
(cid:13)
(cid:13)∞

≤ max
i

(cid:107)(W ∗

m)i·(cid:107)1

≤Csm

(cid:114)

log M
T

,

+ (cid:107) ˆwm − w∗

m(cid:107)1

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
T

T −1
(cid:88)

t=0

XtX (cid:62)
t

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

(53)

where we apply (49), Lemma 5.7, Assumption 3.2, and bound
the same argument as in (50). Thus,

(cid:13)
(cid:13)
(cid:13)

1
T

(cid:80)T −1

t=0 XtX (cid:62)
t

(cid:13)
(cid:13)
(cid:13)∞

using

(cid:107)E(2)

m (cid:107)2 ≥ T −φ (cid:13)
(cid:13)Υ(m)∆m
(cid:13)

(cid:13)
(cid:13)
(cid:13)2

− Csm

(cid:114)

log M
T

T −φ ≥ CT −φ,

since ∆m is a constant vector, and Λmin(Υ(m) is lower bounded by constant as in (42).
Applying these bounds for (cid:107)E(i)

m (cid:107)2, 1 ≤ i ≤ 3, one can show that,

T

k
(cid:88)

m=1

(cid:13)
(cid:13)(Υ(m))− 1
(cid:13)

(cid:13)
2
(cid:13)
2 ( (cid:98)Sm − Sm)
(cid:13)
2

≥

k
(cid:88)

m=1

(cid:18)

C1T

1

2 −φ − C2

(cid:19)2

(s ∨ ρ) log M
√
T

≥ CT 1−2φ.

Plug this into (52) and apply Lemma 5.3, we have

P( (cid:98)UT ≤ x) ≤ C exp{−c log M } + P

(cid:16)

(cid:107)VT (cid:107)2 ≥ C1T

≤C1 exp{−c log M } + C2T − 1
≤C1 exp{−c log M } + C2T − 1

8 + 1 − Fd((C3T

1

2 −φ − C4

8 + C3 exp{−(C3T

1

2 −φ − C4

x)2},

√

(cid:17)

x

1

2 −φ − C2
√

x)2)
√

where in the last line we apply the χ2
depend on d, β, ∆, τ , this bound holds when taking supremum over A∗ ∈ Ω1 and x ∈ R.

d tail bound as in (45). Since the constants here only

(3) φ > 1
2

The proof of this case is similar to that of Theorem 3.1. The only thing diﬀerent lies in
(cid:12)
(cid:12)
(cid:17)
the choice of ε and bounding P
(cid:12)
(cid:12)
(cid:12) still holds
(cid:12) > ε
T (Υ(m))− 1
2 ( (cid:98)Sm − Sm). We directly apply the bounds in Assumptions

(cid:16)(cid:12)
(cid:12)
(cid:12) (cid:98)UT − UT

. The bound (41) for

(cid:12)
(cid:12)
(cid:12) (cid:98)UT − UT

here, with Em =

√

33

3.1 to 3.3, and Lemma 5.1 to Lemma 5.7 in the following. First we write

(cid:98)Sm − Sm =( ˆwm − w∗

m)(cid:62) 1
T

T −1
(cid:88)

t=0

Xt,Dc

m(cid:15)t,m

+

1
T

T −1
(cid:88)

(cid:16)

t=0

Xt,Dm − w∗(cid:62)

m Xt,Dc

m

(cid:17)

X (cid:62)

t,Dc
m

(cid:16)

( (cid:98)Am)Dc

m − (A∗

m)Dc

m

(cid:17)

− ( ˆwm − w∗

m)(cid:62)

(cid:32)

1
T

T −1
(cid:88)

t=0

(cid:33)

(cid:16)

Xt,Dc

mX (cid:62)
t,Dc
m

( (cid:98)Am)Dc

m − (A∗

m)Dc

m

(cid:17)

− T −(1+φ)

T −1
(cid:88)

(Xt,Dm − ˆw(cid:62)

mXt,Dc

m)X (cid:62)

t,Dm∆m.

t=0

Note here that the ﬁrst three terms are exactly the same as in (43), and thus can be
bounded as in the proof of Theorem 3.1. We only have to tackle the last term. By (53),
one can show that,

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
T

T −1
(cid:88)

(Xt,Dm − ˆw(cid:62)

mXt,Dc

m)X (cid:62)

t,Dm∆m

t=0

≤

(cid:13)
(cid:13)Υ(m)∆m
(cid:13)

(cid:13)
(cid:13)
(cid:13)2

+ Csm

(cid:114)

log M
T

≤ C,

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)2

Thus, going through the same arguments as bounding

(cid:13)
(cid:13)
(cid:13) (cid:98)Sm − Sm

(cid:13)
(cid:13)
(cid:13)2

under H0, we have

(cid:107)Em(cid:107)2 ≤ C1

(s ∨ ρ) log M
√
T

+ C2T

1

2 −φ,

with probability at least 1 − C exp{−c log M }. Recall that in (45), when y > C for some
constant C,

P((cid:107)VT,m(cid:107)2 ≥ y) ≤ C1T − 1

8 + C2y−2.

Let y =

(cid:16) (s∨ρ) log M
√

T

(cid:17)− 1

4 ∧ T

2φ−1
6

, then by (41) one can show that

(cid:12)
(cid:12)
(cid:12) (cid:98)UT − UT

(cid:12)
(cid:12)
(cid:12)

(cid:18) (s ∨ ρ) log M
√

T

(cid:19)− 1
2

+ C2

(cid:18) (s ∨ ρ) log M
√

T

(cid:19) 3
4

+ C3T

1−2φ
3

(s ∨ ρ) log M
√
T
(cid:18) (s ∨ ρ) log M
√

≤C1

≤C1

T

(cid:19) 1
2

+ C2T

1−2φ
3

,

with probability at least

1 − c1 exp{−c2 log M } − c3T − 1

8 − c4

(cid:18) (s ∨ ρ) log M
√

T

(cid:19) 1
2

− c5T

1−2φ
3

,

T ) and T > C for some constant C. Therefore, applying (38) with

√

if (s ∨ ρ) log M = o(
(cid:17) 1

(cid:16) (s∨ρ) log M
√

ε = C1

2 + C2T

1−2φ
3

,

T
(cid:12)
(cid:12)
(cid:12)

P( (cid:98)UT ≤ x) − Fd(x)

(cid:12)
(cid:12)
(cid:12)

34

≤C1T − 1

8 + C2

(cid:18) (s ∨ ρ) log M
√

T

(cid:19) 1
2

+ C3T

1−2φ

3 + C4 exp{−C5 log M }.

Since constants Ci only depend on d, β, τ, ∆, this bound also holds for supremum over
A∗ ∈ Ω1 and x ∈ R.

6 Conclusion

In this paper, we have provided theoretical guarantees for hypothesis tests for sparse high-
dimensional auto-regressive models with sub-Gaussian innovations. Speciﬁc upper bounds for
the convergence rates of test statistics are given. Importantly, our results go beyond the Gaussian
assumption and do not rely on mixing assumptions. As a consequence of our theory, we also
develop novel concentration bounds for quadratic forms of dependent sub-Gaussian random
variables using a careful truncation argument.

It would be of interest to consider other variance estimation method, e.g., scaled Lasso Sun
and Zhang [2012], or cross-validation based method Fan et al. [2012], and establish corresponding
theoretical guarantee. There also remain a number of open questions/challenges including exten-
sions to generalized linear models, heavy-tailed innovations and incorporating hidden variables
under time series setting.

Acknowledgements

We would like to thank both Sumanta Basu and Yiming Sun for useful discussions and comments.
LZ and GR were supported by ARO W911NF-17-1-0357 and NGA HM0476-17-1-2003. GR was
also supported by NSF DMS-1811767.

References

A. Ang and M. Piazzesi. A no-arbitrage vector autoregression of term structure dynamics with
macroeconomic and latent variables. Journal of Monetary economics, 50(4):745–787, 2003.

M. Barigozzi and C. T. Brownlees. Nets: Network estimation for time series. 2018.

S. Basu, G. Michailidis, et al. Regularized estimation in sparse high-dimensional time series

models. The Annals of Statistics, 43(4):1535–1567, 2015.

35

S. L. Bressler, C. G. Richter, Y. Chen, and M. Ding. Cortical functional network organization
from autoregressive modeling of local ﬁeld potential oscillations. Statistics in medicine, 26
(21):3875–3885, 2007.

L. Chen and W. B. Wu. Testing for trends in high-dimensional time series. Journal of the

American Statistical Association, (just-accepted):1–37, 2018.

S. X. Chen, L.-X. Zhang, and P.-S. Zhong. Tests for high-dimensional covariance matrices.

Journal of the American Statistical Association, 105(490):810–819, 2010.

R. A. Davis, P. Zang, and T. Zheng. Sparse vector autoregressive modeling. Journal of Com-

putational and Graphical Statistics, 25(4):1077–1096, 2016.

J. Fan, S. Guo, and N. Hao. Variance estimation using reﬁtted cross-validation in ultrahigh
dimensional regression. Journal of the Royal Statistical Society: Series B (Statistical Method-
ology), 74(1):37–65, 2012.

A. Fujita, J. R. Sato, H. M. Garay-Malpartida, R. Yamaguchi, S. Miyano, M. C. Sogayar,
and C. E. Ferreira. Modeling gene expression regulatory networks with the sparse vector
autoregressive model. BMC systems biology, 1(1):39, 2007.

R. Goebel, A. Roebroeck, D.-S. Kim, and E. Formisano. Investigating directed cortical interac-
tions in time-resolved fmri data using vector autoregressive modeling and granger causality
mapping. Magnetic resonance imaging, 21(10):1251–1261, 2003.

I. Grama and E. Haeusler. An asymptotic expansion for probabilities of moderate deviations

for multivariate martingales. Journal of Theoretical Probability, 19(1):1–44, 2006.

R. M. Gray et al. Toeplitz and circulant matrices: A review. Foundations and Trends R(cid:13) in

Communications and Information Theory, 2(3):155–239, 2006.

F. Han, H. Lu, and H. Liu. A direct estimation of high dimensional stationary vector autore-

gressions. The Journal of Machine Learning Research, 16(1):3115–3150, 2015.

P. R. Hansen. Structural changes in the cointegrated vector autoregressive model. Journal of

Econometrics, 114(2):261–295, 2003.

M. Hardy. Combinatorics of partial derivatives. the electronic journal of combinatorics, 13(1):

1, 2006.

L. Harrison, W. D. Penny, and K. Friston. Multivariate autoregressive modeling of fmri time

series. Neuroimage, 19(4):1477–1491, 2003.

A. Javanmard and A. Montanari. Conﬁdence intervals and hypothesis testing for high-
dimensional regression. The Journal of Machine Learning Research, 15(1):2869–2909, 2014.

36

B. Laurent and P. Massart. Adaptive estimation of a quadratic functional by model selection.

Annals of Statistics, pages 1302–1338, 2000.

J. D. Lee, D. L. Sun, Y. Sun, J. E. Taylor, et al. Exact post-selection inference, with application

to the lasso. The Annals of Statistics, 44(3):907–927, 2016.

R. Lockhart, J. Taylor, R. J. Tibshirani, and R. Tibshirani. A signiﬁcance test for the lasso.

Annals of statistics, 42(2):413, 2014.

R. G. Mark B. and W. R. Network estimation from point process data. IEEE Trans. of Info.

Theory, 2018. To appear.

M. C. Medeiros and E. F. Mendes. 1-regularization of high-dimensional time-series models with
non-gaussian and heteroskedastic errors. Journal of Econometrics, 191(1):255–271, 2016.

G. Michailidis and F. dAlch´e Buc. Autoregressive models for gene regulatory network inference:

Sparsity, stability and causality issues. Mathematical biosciences, 246(2):326–334, 2013.

M. Neykov, Y. Ning, J. S. Liu, H. Liu, et al. A uniﬁed theory of conﬁdence regions and testing

for high-dimensional estimating equations. Statistical Science, 33(3):427–443, 2018.

Y. Ning, H. Liu, et al. A general theory of hypothesis tests and conﬁdence regions for sparse

high dimensional models. The Annals of Statistics, 45(1):158–195, 2017.

M. Rudelson, R. Vershynin, et al. Hanson-wright inequality and sub-gaussian concentration.

Electronic Communications in Probability, 18, 2013.

J. R. Schott. Testing for complete independence in high dimensions. Biometrika, 92(4):951–956,

2005.

A. K. Seth, A. B. Barrett, and L. Barnett. Granger causality analysis in neuroscience and

neuroimaging. Journal of Neuroscience, 35(8):3293–3297, 2015.

J. Shan. Does ﬁnancial development lead’economic growth? a vector auto-regression appraisal.

Applied Economics, 37(12):1353–1367, 2005.

S. Song and P. J. Bickel. Large vector auto regressions. arXiv preprint arXiv:1106.3915, 2011.

M. S. Srivastava. A test for the mean vector with fewer observations than the dimension under

non-normality. Journal of Multivariate Analysis, 100(3):518–532, 2009.

T. Sun and C.-H. Zhang. Scaled sparse linear regression. Biometrika, 99(4):879–898, 2012.

J. Taylor, R. Lockhart, R. J. Tibshirani, and R. Tibshirani. Post-selection adaptive inference

for least angle regression and the lasso. arXiv preprint, 2014.

37

S. Van de Geer, P. B¨uhlmann, Y. Ritov, R. Dezeure, et al. On asymptotically optimal conﬁdence
regions and tests for high-dimensional models. The Annals of Statistics, 42(3):1166–1202,
2014.

R. Vershynin. Introduction to the non-asymptotic analysis of random matrices. arXiv preprint

arXiv:1011.3027, 2010.

A. Voorman, A. Shojaie, and D. Witten. Inference in high dimensions with the penalized score

test. arXiv preprint arXiv:1401.2678, 2014.

K. C. Wong, Z. Li, and A. Tewari. Lasso guarantees for time series estimation under subgaussian

tails and β-mixing. arXiv preprint arXiv:1602.04265, 2016.

C.-H. Zhang and S. S. Zhang. Conﬁdence intervals for low dimensional parameters in high
dimensional linear models. Journal of the Royal Statistical Society: Series B (Statistical
Methodology), 76(1):217–242, 2014.

R. Zhang, L. Peng, R. Wang, et al. Tests for covariance matrix with ﬁxed or divergent dimension.

The Annals of Statistics, 41(4):2075–2096, 2013.

A Proof of Lemmas in Section 3.3

Proof of Lemma 3.1. We prove the error bounds for each (cid:98)Am and then take a union bound.
1 ∈ RM . With a little abuse of
Without loss of generality, we consider the estimation of A∗
notation, let S = supp(A∗
1, S = supp(A∗
(S is not the
decorrelated score function we deﬁned in section 9). We would like to bound (cid:107)ˆh(cid:107)1, (cid:107)ˆh(cid:107)2 and
ˆh(cid:62)Hˆh under two cases separately:

1), ˆh = (cid:98)A1 − A∗

1), and H = 1
T

t=0 XtX (cid:62)
t

(cid:80)T −1

(1) (cid:98)A = (cid:98)A(L).

Here we adopt the standard proof framework for Lasso. By (25) we know that (cid:98)A1 ∈ RM
satisﬁes

(cid:98)A1 = arg min

β∈RM

1
T

which implies

T −1
(cid:88)

t=0

(Xt+1,1 − X (cid:62)

t β)2 + λA(cid:107)β(cid:107)1,

T −1
(cid:88)

(Xt+1,1 − X (cid:62)

t A∗

1)2 + λA(cid:107)A∗

1(cid:107)1.

t=0

1
T

1
T

T −1
(cid:88)

(Xt+1,1 − X (cid:62)

t (cid:98)A1)2 + λA(cid:107) (cid:98)A1(cid:107)1 ≤

t=0

38

Rearranging the terms, we have
(cid:32)

ˆh(cid:62)Hˆh ≤ 2ˆh(cid:62)

(cid:33)

(cid:15)t,1Xt

+ λA(cid:107)A∗

1(cid:107)1 − λA(cid:107) (cid:98)A1(cid:107)1

1
T

T −1
(cid:88)

t=0

≤ 2

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
T

T −1
(cid:88)

t=0

(cid:15)tX (cid:62)
t

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

(cid:107)ˆh(cid:107)1 + λA(cid:107)ˆhS(cid:107)1 − λA(cid:107)ˆhSc(cid:107)1.

The last line is due to that

(cid:107)A∗

1(cid:107)1 − (cid:107) (cid:98)A1(cid:107)1 = (cid:107)(A∗
= (cid:107)(A∗
≤ (cid:107)ˆhS(cid:107)1 − (cid:107)ˆhSc(cid:107)1.

1)S(cid:107)1 − (cid:107)( (cid:98)A1)S(cid:107)1 − (cid:107)( (cid:98)A1)Sc(cid:107)1
1)S(cid:107)1 − (cid:107)( (cid:98)A1)S(cid:107)1 − (cid:107)ˆhSc(cid:107)1

By Lemma 5.1, with probability at least 1 − c1 exp{−c2 log M },

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
T

T −1
(cid:88)

t=0

(cid:15)tX (cid:62)
t

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

≤

1
4

λA = C

(cid:114)

log M
T

.

Meanwhile, since H is positive semi-deﬁnite,
3λA
2

0 ≤ ˆh(cid:62)Hˆh ≤

(cid:107)ˆhS(cid:107)1 −

λA
2

(cid:107)ˆhSc(cid:107)1,

We have the following restricted eigenvalue condition for H.

(cid:107)ˆhSc(cid:107)1 ≤ 3(cid:107)ˆhS(cid:107)1.

Lemma A.1. Under the model speciﬁed in (3) with independent sub-Gaussian noise (cid:15)ti of
constant scale factor, and A∗ ∈ Ω0 ∪ Ω1, for any set J ⊂ {1, 2, · · · , pM }, positive integer
κ > 0, H satisﬁes the following REC:

inf{v(cid:62)Hv : v ∈ C(J, κ), (cid:107)v(cid:107)2 ≤ 1} ≥ C1 > 0,

with probability at least 1 − 2 exp {−cT }, when |J| log pM ≤ C2T . Here C(J, κ) = {v :
(cid:107)vJ c(cid:107)1 ≤ κ(cid:107)vJ (cid:107)1}, constant C1 depends on β, c and C2 depend on κ and β.

Here ˆh ∈ C(S, 3), |S| = ρ1, by Lemma A.1, when T ≥ Cρ log M ,

ˆh(cid:62)Hˆh ≥ C(cid:107)ˆh(cid:107)2
2,

with probability at least 1 − 2 exp{−cT }, when T > Cρ log M . Thus

(cid:107)ˆh(cid:107)2

2 ≤ Cˆh(cid:62)Hˆh ≤ CλA(cid:107)ˆhS(cid:107)1 ≤ C

(cid:114)

ρ1 log M
T

(cid:107)ˆh(cid:107)2,

(54)

which implies

(cid:107)ˆh(cid:107)2 ≤C

(cid:114)

ρ1 log M
T

,

(cid:107)ˆh(cid:107)1 ≤4(cid:107)ˆhS(cid:107)1 ≤ 4

√

ρ1(cid:107)ˆh(cid:107)2 ≤ Cρ1

with probability at least 1 − c1 exp{−c2 log M }.

39

ˆh(cid:62)Hˆh ≤ C

,

ρ1 log M
T
log M
T

(cid:114)

,

(2) (cid:98)A = (cid:98)A(D).

Here we adopt the standard proof framework for Dantzig selector. By (26),

(cid:98)A1 = arg min

β∈RM

(cid:107)β(cid:107)1,

s.t.

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
T

T −1
(cid:88)

(Xt+1,1 − X (cid:62)

t β)Xt

t=0

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

≤ λA.

(55)

By Lemma 5.1, when T ≥ C log M , with probability at least 1 − c1 exp{−c2 log M },

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
T

T −1
(cid:88)

t=0

which implies

Meanwhile, by (55),

(Xt+1,1 − X (cid:62)

t A∗

1)Xt

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

=

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
T

T −1
(cid:88)

t=0

(cid:15)t,1Xt

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

≤ λA,

(cid:107)Hˆh(cid:107)∞ ≤ C

(cid:114)

log M
T

.

(cid:107) (cid:98)A1(cid:107)1 ≤ (cid:107)A∗

1(cid:107)1,

(cid:107)ˆhSc(cid:107)1 ≤ (cid:107)ˆhS(cid:107)1.

Here ˆh ∈ C(S, 1), |S| = ρ1, by Lemma A.1, when T ≥ Cρ log M ,

ˆh(cid:62)Hˆh ≥ C(cid:107)ˆh(cid:107)2
2,

with probability at least 1 − 2 exp{−cT }, when T > Cρ log M . Thus

(cid:107)ˆh(cid:107)2

2 ≤ Cˆh(cid:62)Hˆh ≤ (cid:107)Hˆh(cid:107)∞(cid:107)ˆh(cid:107)1 ≤ C

(cid:114)

log M
T

(cid:114)

(cid:107)ˆh(cid:107)1 ≤ C

ρ1 log M
T

(cid:107)ˆh(cid:107)2,

(56)

which implies

(cid:107)ˆh(cid:107)2 ≤C

(cid:114)

ρ1 log M
T

,

(cid:107)ˆh(cid:107)1 ≤4(cid:107)ˆhS(cid:107)1 ≤ 4

√

ρ1(cid:107)ˆh(cid:107)2 ≤ Cρ1

with probability at least 1 − c1 exp{−c2 log M }.

ˆh(cid:62)Hˆh ≤ C

,

ρ1 log M
T
log M
T

(cid:114)

,

Therefore, after taking a union bound over m = 1, · · · , k, proof complete.

Proof of Lemma 3.2. Without loss of generality, we consider the estimation of (w∗
take a union bound. Let v∗ = (w∗
we prove upper bounds for (cid:107)ˆh(cid:107)1 and ˆh(cid:62)HDc

1)·,1 and then
1)·,1, ˆv = ( ˆw1)·,1, ˆh = ˆv − v∗ ∈ RM −d1 and S = supp(v∗). Then

ˆh with high probability under two cases.

1,Dc
1

(1) ˆwm = ˆw(L)
m .

Looking into the deﬁnition (27) of ˆw1, it is clear that the optimization can be viewed as
d1 separate optimization problems, in terms of each column of ˆw1. Thus

ˆv = arg min

v∈RM −d1

1
T

T −1
(cid:88)

(cid:16)

t=0

(Xt,D1)1 − X (cid:62)
t,Dc
1

v

(cid:17)2

+ λw(cid:107)v(cid:107)1.

40

The following proof is almost identical to the proof in Lemma 3.1 under (cid:98)A = (cid:98)A(L), except
some diﬀerence in notation and application of Lemmas. One can show that,

1
T

1
T

T −1
(cid:88)

(cid:16)

t=0
T −1
(cid:88)

(cid:16)

t=0

≤

(Xt,D1)1 − X (cid:62)
t,Dc
1

(cid:17)2

ˆv

+ λw(cid:107)ˆv(cid:107)1

(Xt,D1)1 − X (cid:62)
t,Dc
1

v∗(cid:17)2

+ λw(cid:107)v∗(cid:107)1,

Rearranging the inequality gives us

ˆh(cid:62)HDc

1,Dc
1

ˆh ≤2ˆh(cid:62)

(cid:32)

1
T

T −1
(cid:88)

t=0

(cid:0)(Xt,D1)1 − Xt,Dc

1

v∗(cid:1) XDc

1

(cid:33)

+ λw(cid:107)v∗(cid:107)1 − λw(cid:107)ˆv(cid:107)1

≤2

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
T

T −1
(cid:88)

(Xt,D1 − w∗(cid:62)

1 Xt,Dc

1

t=0

)X (cid:62)
Dc
1

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

(cid:107)ˆh(cid:107)1 + λw(cid:107)ˆhS(cid:107)1 − λA(cid:107)ˆhSc(cid:107)1.

By Lemma 5.6, with probability at least 1 − c1 exp{−c2 log M },

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
T

which implies,

T −1
(cid:88)

(Xt,D1 − w∗(cid:62)

1 Xt,Dc

1

t=0

)X (cid:62)
Dc
1

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

≤

1
4

λw = C

(cid:114)

log M
T

,

0 ≤ ˆh(cid:62)HDc

1,Dc
1

ˆh ≤

3λw
2

(cid:107)ˆhS(cid:107)1 −

λw
2

(cid:107)ˆhSc(cid:107)1,

(cid:107)ˆhSc(cid:107)1 ≤ 3(cid:107)ˆhS(cid:107)1.

Let ˜h ∈ RM be deﬁned as the following:

By Lemma A.1, when T ≥ Cs log M , with probability at least 1 − 2 exp{−cT },

˜hD1 = 0,

˜hDc

1

= ˆh,

(57)

(cid:107)ˆh(cid:107)2

2 = (cid:107)˜h(cid:107)2

2 ≤C˜h(cid:62)H˜h = 2ˆh(cid:62)HDc

1,Dc
1

ˆh ≤ Cλw(cid:107)ˆhS(cid:107)1 ≤ C

(cid:114)

s1 log M
T

(cid:107)ˆh(cid:107)2,

which implies

and

ˆh(cid:62)Hˆh ≤ C

s1 log M
T

,

(cid:107)ˆh(cid:107)1 ≤ 4(cid:107)ˆhS(cid:107)1 ≤ 4

√

s1(cid:107)ˆh(cid:107)2 ≤ Cs1

(cid:114)

log M
T

,

with probability at least 1 − c1 exp{−c2 log M }.

41

(2) ˆwm = ˆw(D)
m .
By (28),

ˆv = arg min
v∈RM −d1

(cid:107)v(cid:107)1,

s.t.

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
T

T −1
(cid:88)

(cid:16)

t=0

(Xt,D1)1 − v(cid:62)Xt,Dc

1

(cid:17)

Xt,Dc

1

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

≤ λw.

(58)

This proof is also pretty similar to the proof of Lemma 3.1 under the case where (cid:98)A = (cid:98)A(D).
By Lemma 5.6,

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
T

T −1
(cid:88)

(cid:16)

t=0

(Xt,D1)1 − v∗(cid:62)Xt,Dc

1

(cid:17)

1

Xt,Dc

1

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

≤ λw = C

(cid:114)

log M
T

,

with probability at least 1 − c1 exp{−c2 log M }. Thus,

(cid:13)
(cid:13)H (cid:62)
(cid:13)
Dc

1,Dc
1

ˆh

(cid:13)
(cid:13)
(cid:13)∞

≤ C

(cid:114)

log M
T

.

Meanwhile, by (58),

which further implies

(cid:107)ˆv(cid:107)1 = (cid:107)ˆvS(cid:107)1 + (cid:107)ˆvSc(cid:107)1 ≤ (cid:107)v∗(cid:107)1 = (cid:107)v∗

S(cid:107)1 ,

Recall the deﬁnition of ˜h in (57),then by Lemma A.1, (59) and (57), when T ≥ Cs log M ,

(cid:13)
ˆhSc
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)1

≤

(cid:13)
ˆhS
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)1

.

(59)

(cid:107)ˆh(cid:107)2

2 = (cid:107)˜h(cid:107)2

2 ≤C˜h(cid:62)H˜h
=Cˆh(cid:62)HDc
ˆh
1,Dc
1
(cid:13)
(cid:13)
(cid:13)
ˆh
(cid:13)H (cid:62)
(cid:13)
(cid:13)
(cid:13)
Dc
(cid:13)1
(cid:13)
(cid:114)
log M
T

≤C

≤C

ˆh

(cid:13)
(cid:13)
(cid:13)∞

1,Dc
1

(cid:107)ˆhS(cid:107)1

(cid:114)

≤C

s1 log M
T

(cid:107)ˆh(cid:107)2,

which implies

and

ˆh(cid:62)HDc

1,Dc
1

ˆh ≤ C

s1 log M
T

,

(cid:107)ˆh(cid:107)1 ≤ C

√

s1(cid:107)ˆh(cid:107)2 ≤ Cs1

(cid:114)

log M
T

,

with probability at least 1 − c1 exp{−c2 log M }.

42

Since

and

(cid:107)( ˆw1) − (w∗

1)(cid:107)1 =

d1(cid:88)

j=1

(cid:107)( ˆw1)·,j − (w∗

1)·,j(cid:107)1,

(cid:40)

tr

( ˆw1 − w∗

1)(cid:62)

(cid:32)

1
T

T −1
(cid:88)

t=0

(cid:33)

(cid:41)

Xt,Dc

mX (cid:62)
t,Dc
m

( ˆw1 − w∗
1)

=

d1(cid:88)

j=1

(( ˆw1)·,j − (w∗

1)·,j)(cid:62)

(cid:32)

1
T

T −1
(cid:88)

t=0

(cid:33)

Xt,Dc

1

X (cid:62)

t,Dc
1

(( ˆw1)·,j − (w∗

1)·,j) ,

taking a union bound over { ˆwm : m = 1, · · · , k} and all columns of ˆwm, proof is complete.

Proof of Lemma 3.3. The following established result can be applied here:

Lemma A.2. For any invertible matrix B, if B + ∆ is also invertible, then

(cid:107)(B + ∆)−1 − B−1(cid:107)2 ≤

(cid:107)B−1(cid:107)2

2(cid:107)∆(cid:107)2

1 − (cid:107)B−1(cid:107)2(cid:107)∆(cid:107)2

.

(60)

Since (cid:107)I(cid:107)2 = 1, one can show that for 1 ≤ m ≤ k,

(cid:13)
(cid:13)
Υ(m) 1
(cid:13)
(cid:13)

2

−1

(cid:91)
Υ(m)

Υ(m) 1

2 − I

(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

≤

(cid:13)
(cid:13)Υ(m) 1
(cid:13)

2

(cid:91)
Υ(m))−1Υ(m) 1

2 − I

(cid:13)
(cid:13)
(cid:13)2

≤

(cid:107)∆(cid:107)2
1 − (cid:107)∆(cid:107)2

,

where ∆ = Υ(m)− 1

2

(cid:91)
Υ(m)Υ(m)− 1

2 − I. Due to (42),

≤C

(cid:107)∆(cid:107)2 ≤
Υ(m) − Υ(m)(cid:13)
(cid:13)
(cid:91)
(cid:13)
(cid:13)
(cid:13)
(cid:13)F
Υ(m) − Υ(m)(cid:13)
(cid:13)
(cid:91)
(cid:13)
(cid:13)
(cid:13)∞
(cid:13)

(cid:16)

Υ(m) − Υ(m)(cid:13)
ΛminΥ(m)(cid:17)−1 (cid:13)
(cid:91)
(cid:13)
(cid:13)
(cid:13)2
(cid:13)
Υ(m) − Υ(m)(cid:13)
(cid:13)
(cid:91)
(cid:13)
(cid:13)
(cid:13)∞
(cid:13)

≤ dm

.

. Write

(cid:91)
Υ(m) − Υ(m) as

In the following we bound

(cid:91)
Υ(m) − Υ(m) =W ∗
m

(cid:32)

1
T

T −1
(cid:88)

t=0

(cid:33)

XtX (cid:62)

t − Υ

W ∗(cid:62)
m

− ( ˆwm − w∗

m)(cid:62) 1
T

T −1
(cid:88)

t=0

Xt,Dc

m(Xt,Dm − w∗(cid:62)

m Xt,Dc

m)(cid:62)

−

1
T

T −1
(cid:88)

(Xt,Dm − w∗(cid:62)

m Xt,Dc

m)X (cid:62)
t,Dc
m

t=0

+ ( ˆwm − w∗

m)(cid:62)

(cid:32)

1
T

T −1
(cid:88)

t=0
(cid:17)(cid:62)

(cid:44)E(m)

1 − E(m)

2 −

(cid:16)

E(m)
2

+ E(m)
3

,

( ˆwm − w∗

m)

(cid:33)

Xt,Dc

mX (cid:62)
t,Dc
m

( ˆwm − w∗

m)

43

where W ∗

m is deﬁned as in (48). Actually,

(cid:107)E(m)
1

(cid:107)∞ = max

i,j

= max

i,j

(cid:12)
(cid:12)
W ∗
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
T

(cid:32)

1
T

T −1
(cid:88)

t=0

(cid:33)

XtX (cid:62)

t − Υ

W ∗(cid:62)
m,j·

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

m,i·

T −1
(cid:88)

t=0

X (cid:62)

t W ∗(cid:62)

m,i·W ∗

m,j·Xt − tr(W ∗(cid:62)

m,i·W ∗

m,j·Υ)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

,

which is the maximum over deviations of some quadratic forms from their expectation. The
t BXt, with B ∈ RM ×M being
following lemma provides a bound for quadratic form 1
T
any symmetric matrix.

t=0 X (cid:62)

(cid:80)T −1

By Lemma 5.2, we only need to bound the trace norm and operator norm of

(W ∗

m)(cid:62)

i· (W ∗

m)j· + (W ∗

m)(cid:62)

j·(W ∗

m)i·

(cid:17)

.

(cid:16)

1
2

The following lemma establishes the relationship between (cid:107)·(cid:107)tr and (cid:107)·(cid:107)2 for symmetric matrices.

Lemma A.3. For any symmetric matrix U of rank r, (cid:107)U (cid:107)tr ≤ r(cid:107)U (cid:107)2.

(cid:16)

Since 1
2

(W ∗

m)(cid:62)

i· (W ∗

m)j· + (W ∗

m)(cid:62)

j·(W ∗

m)i·

(cid:17)

is of rank 2,

(cid:16)

(cid:13)
1
(cid:13)
(cid:13)
2
(cid:13)
(cid:13)
1
(cid:13)
(cid:13)
2
(cid:13)
(cid:13)
(cid:13)(W ∗
(cid:13)

(cid:16)

≤2

≤2

(W ∗

m)(cid:62)

i· (W ∗

m)j· + (W ∗

m)(cid:62)

j·(W ∗

m)i·

(W ∗

m)i·)(cid:62)(W ∗
(cid:13)
(cid:13)
(cid:13)2

m)j·

i· (W ∗

m)(cid:62)

m)j· + (W ∗

m)(cid:62)

j·(W ∗

m)i·

= 2(cid:107)(W ∗

m)i·(cid:107)2(cid:107)(W ∗

(cid:17)(cid:13)
(cid:13)
(cid:13)
(cid:13)tr

(cid:17)(cid:13)
(cid:13)
(cid:13)
(cid:13)2
m)j·(cid:107)2.

Meanwhile, similar from (49), we bound maxi (cid:107)(W ∗

m)i·(cid:107)2

2 by

(cid:107)(W ∗

m)i·(cid:107)2

2 =1 + (cid:107)(w∗

m)·,i(cid:107)2
2
≤1 + Λmax(Υ−1
Dc
m,Dc
m
≤1 + Λmin(Υ)−2Λmax(Υ)2 ≤ C,

)2 (cid:107)Υ·,i(cid:107)2
2

(61)

(62)

where the second inequality is due to that (cid:107)Υ·,i(cid:107)2
both the trace norm and (cid:96)2 norm of 1
2
and applying Lemma 5.2 gives us

m,i·W ∗

W ∗(cid:62)

(cid:16)

2 = (Υ2)ii ≤ Λmax(Υ2) ≤ Λmax(Υ)2. Thus,
can be bounded by constant,

m,j· + W ∗(cid:62)

m,j·W ∗

m,i·

(cid:17)

(cid:32)

(cid:107)E(m)
1

P

(cid:107)∞ > C

(cid:33)

(cid:114)

log M
T

≤ c1 exp{−c2 log M }.

Meanwhile, by Lemma 5.6 and Assumption 3.2, with probability at least 1 − c1 exp{−c2 log M },

(cid:107)E(m)
2

(cid:107)∞ ≤

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
T

T −1
(cid:88)

(Xt,Dm − w∗(cid:62)

m Xt,Dc

m)Xt,Dc

m

t=0

44

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

(cid:107) ˆwm − w∗

m(cid:107)1

≤ C

sm log M
T

,

and

(cid:13)
(cid:13)E(m)
(cid:13)

3

(cid:13)
(cid:13)
(cid:13)∞

( ˆwm − w∗

m)(cid:62)

·,iHDc

m( ˆwm − w∗

m)·,j

m,Dc

= max
i,j

≤ max
l
(cid:110)

≤tr

3

)ij

(cid:12)
(cid:12)(E(m)
(cid:12)
( ˆwm − w∗

(cid:12)
(cid:12)
(cid:12) = max
i,j
m)(cid:62)

·,lHDc

m( ˆwm − w∗

m,Dc

m)·,l
(cid:111)

( ˆwm − w∗

m)(cid:62)HDc

m( ˆwm − w∗
m)

m,Dc

≤C

sm log M
T

.

Here the second line is because that HDc
m,Dc
semi-deﬁnite, thus we can apply Cauchey-Schwartz inequality. When T ≥ Cs2 log M .

t=0 Xt,Dc

t,Dc
m

is symmetric and positive

mX (cid:62)

m = 1
T

(cid:80)T −1

sm log M
T

≤

(cid:114)

log M
T

,

which implies

Υ(m) − Υ(m)(cid:13)
(cid:13)
(cid:91)
(cid:13)
(cid:13)
(cid:13)2
(cid:13)

≤ (cid:107)

(cid:91)
Υ(m) − Υ(m)(cid:107)F ≤ dm(cid:107)

(cid:91)
Υ(m) − Υ(m)(cid:107)∞ ≤ C

(cid:114)

log M
T

.

Therefore, take a union bound over 1 ≤ m ≤ k, with probability at least 1 − c1 exp{−c2 log M },

(cid:13)
(cid:13)
Υ(m) 1
(cid:13)
(cid:13)

2

−1

(cid:91)
Υ(m)

Υ(m) 1

2 − I

(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

(cid:114)

≤ C

log M
T

.

when T ≥ Cs2 log M .

B Proof of Theorem 3.3 and Theorem 3.4

Proof of Theorem 3.3. Now we consider model (3), with unknown σ∗2 = Var((cid:15)ti) ≥ σ2
this model, we use the notation (cid:98)UT for the quantity deﬁned in the following:

0. Under

(cid:98)UT = T

k
(cid:88)

m=1

(cid:98)S(cid:62)
m(

(cid:91)
Υ(m))−1 (cid:98)Sm/σ∗2.

As explained in Section 3.4, (cid:98)UT satisﬁes Theorem 3.1 and 3.2 under each corresponding condition.
We show in the following that we only need to control the estimation error of ˆσ2. Note that for
any 0 < δ < 1,

(cid:16)

P

(cid:101)UT ≤ x

(cid:17)

≤ P

(cid:18)

(cid:98)UT ≤

(cid:19)

x
1 − δ

+ P

(cid:18) σ∗2

ˆσ2 < 1 − δ

and

(cid:16)

P

(cid:17)

(cid:101)UT > x

(cid:18)

≤ P

(cid:98)UT >

(cid:19)

x
1 + δ

+ P

(cid:18) σ∗2

ˆσ2 > 1 + δ

45

(cid:19)

,

.

(cid:19)

For any distribution function F (x),

(cid:16)

(cid:12)
(cid:12)
(cid:12)

P

(cid:101)UT ≤ x

(cid:17)

(cid:12)
(cid:12)
(cid:12) ≤ sup
− F (x)
y

+ P

P

(cid:12)
(cid:12)
(cid:12)
(cid:18)

(cid:16)

(cid:98)UT ≤ y

(cid:17)

− F (y)

ˆσ2 <

(cid:19)

σ∗2
1 + δ

+ P

(cid:12)
(cid:12)
(cid:12) + sup
y
(cid:18)

ˆσ2 >

(cid:19)

.

σ∗2
1 − δ

|F (y) − F (y(1 − δ))|

Recall that Theorem 3.1 and 3.2 establish bounds for P

(cid:16)

(cid:17)

(cid:98)UT ≤ x

2 , for P

HA with φ > 1
Thus we only need to bound P
F (x) = Fd(x) or F (x) = Fd,(cid:107) (cid:101)∆(cid:107)2

2

2

(x) when φ = 1
(cid:16)
(cid:17)
ˆσ2 > σ∗2
1−δ

− Fd,(cid:107) (cid:101)∆(cid:107)2
(cid:16)
ˆσ2 < σ∗2
1+δ
(x). Since 0 < δ < 1,

, P

(cid:16)

(cid:17)

(cid:98)UT ≤ x
2 , and for P

− Fd(x) under H0, or under
(cid:16)

(cid:17)

(cid:98)UT ≤ x

when 0 < φ < 1
2 .
and supy |F (y) − F (y(1 − δ))| with

(cid:17)

(cid:18)

ˆσ2 <

P

(cid:19)

σ∗2
1 + δ

(cid:18)

+ P

ˆσ2 >

(cid:19)

σ∗2
1 − δ

≤ P

(cid:18)

|ˆσ2 − σ∗2| >

(cid:19)

δσ∗2
2

≤ P

(cid:18)

|ˆσ2 − σ∗2| >

(cid:19)

.

δσ2
0
2

Meanwhile,

ˆσ2 − σ∗2 =

1
M T

=

1
M T

T −1
(cid:88)

t=0
T −1
(cid:88)

t=0

(cid:13)
(cid:13)
(cid:13)Xt+1 − (cid:98)AXt

(cid:13)
2
(cid:13)
(cid:13)
2

− σ∗2

(cid:107)(cid:15)t(cid:107)2

2 − σ∗2 +

1
M T

T −1
(cid:88)

t=0

(cid:13)
(cid:13)( (cid:98)A − A∗)Xt
(cid:13)

(cid:13)
2
(cid:13)
(cid:13)
2

+

2
M T

T −1
(cid:88)

t=0

(cid:12)
t ( (cid:98)A − A∗)Xt
(cid:12)(cid:15)(cid:62)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

=

1
M T

+

2
M

T −1
(cid:88)

(cid:107)(cid:15)t(cid:107)2

2 − σ∗2 +

t=0
M
(cid:88)

i=1

(cid:32)

( (cid:98)Ai − A∗

i )(cid:62)

1
M

1
T

M
(cid:88)

( (cid:98)Ai − A∗

i )(cid:62)H( (cid:98)Ai − A∗
i )

i=1
T −1
(cid:88)

t=0

(cid:33)

(cid:15)tiXt

.

By Assumption 3.1 and Lemma 5.1, with probability at least 1 − c1 exp{−c2 log M },

1
M

M
(cid:88)

i=1

and

( (cid:98)Ai − A∗

i )(cid:62)H( (cid:98)Ai − A∗

i ) ≤ C

ρ log M
T

(cid:114)

≤ C

ρ log M
T

,

2
M

M
(cid:88)

i=1

( (cid:98)Ai − A∗

i )(cid:62)H

(cid:32)

1
T

T −1
(cid:88)

t=0

(cid:33)

(cid:15)tiXt

≤2 max

i

(cid:13)
(cid:13)
(cid:13) (cid:98)Ai − A∗
(cid:13)
(cid:13)
(cid:13)1
(cid:114)

i

(cid:32)

1
T

T −1
(cid:88)

t=0

(cid:33)

(cid:15)tX (cid:62)
t

≤C

ρ log M
T

≤ C

ρ log M
T

.

Also, since (cid:15)ti are independent sub-Gaussian random variables with scale factor Cσ∗, the ﬁrst
term can be bounded by Bernstein type inequality of sub-exponential random variables(see

46

proposition 5.16 in Vershynin [2010]):

P

(cid:32)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
M T

T −1
(cid:88)

t=0

(cid:107)(cid:15)t(cid:107)2

2 − σ∗2

(cid:33)

>

δσ∗2
2

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤ 2 exp (cid:8)−cM T min{δ2, δ}(cid:9) .

Let δ = C

(cid:113) ρ log M
T

, then

(cid:18)

ˆσ2 <

P

(cid:19)

σ∗2
1 + δ

(cid:18)

+ P

ˆσ2 >

(cid:19)

σ∗2
1 − δ

≤2 exp {−c1ρM log M } + c2 exp{−c3 log M }.

While for supx Fd,(cid:107)µ(cid:107)2

2

(x) − Fd,(cid:107)µ(cid:107)2

2

(x(1 − δ)) with any µ ∈ Rd satisfying (cid:107)µ(cid:107)2 ≤ C, if δ < 1
2 ,

Fd,(cid:107)µ(cid:107)2
(cid:16)

2

(x) − Fd,(cid:107)µ(cid:107)2

(x(1 − δ))
(cid:17)
2 ∈ (x(1 − δ), x]

2

(cid:107)Z + µ(cid:107)2

=P

(cid:16)

≤C(d)

d
2 − (x(1 − δ))

d
2

x

(cid:17)

≤C(d)δx

d
2 exp

(cid:26)

−

1
2

e−(cid:107)z(cid:107)2

2/2

(cid:107)z+µ(cid:107)2

sup
2∈(x(1−δ),x]
(cid:17)2

(cid:16)(cid:112)x(1 − δ) − (cid:107)µ(cid:107)2

1((cid:112)x(1 − δ) ≥ (cid:107)µ(cid:107)2)

(cid:27)

.

Here Z ∈ Rd is a standard Gaussian random vector, the third line is due to that the density of
Z is (2π)− d
2/2, and the fourth line applies the fact that when 0 < δ < 1
2 ,

2 e−(cid:107)z(cid:107)2

(cid:104)
1 − (1 − δ)

d
2

(cid:105)

≤

d
2

sup
ξ∈(1−δ,1)

d

2 −1δ =

ξ

d
2

(1 − δ)( d

2 −1)1(d≤2)δ ≤ C(d)δ.

Meanwhile, when (cid:112)x(1 − δ) < (cid:107)µ(cid:107)2,

and when (cid:112)x(1 − δ) ≥ (cid:107)µ(cid:107)2,
(cid:26)

d
2 exp

x

−

d
2 ≤

x

(cid:107)µ(cid:107)d
2
(1 − δ)

d
2

≤ C(d),

(cid:16)(cid:112)x(1 − δ) − (cid:107)µ(cid:107)2

(cid:17)2

(cid:27)
1((cid:112)x(1 − δ) ≥ (cid:107)µ(cid:107)2)

1
2

(y + C)de−y2/2 ≤ C(d),

≤ sup
y≥0

which implies

Fd,(cid:107)µ(cid:107)2

2

(x) − Fd,(cid:107)µ(cid:107)2

2

(x(1 − δ)) ≤ C(d)δ.

To see why all the bounds for (cid:98)UT still hold for (cid:101)UT , note that we only need to add C
T +
2 exp {−c1ρM log M } + c2 exp{−c3 log M } to the bounds under H0, and under HA when φ ≥ 1
2 ,
which only changes the constant factors of the previous bounds. For the bound under HA
when 0 < φ < 1
, and add 2 exp {−c1ρM log M } +
c2 exp{−c3 log M }, which only changes the constant factors as well. Therefore, all the conclusions
for (cid:98)UT in Theorem 3.1 and 3.2 still hold for (cid:101)UT under each corresponding condition.

2 , we substitute x by x

1−δ with δ = C

(cid:113) log M
T

(cid:113) ρ log M

47

Proof of Theorem 3.4. First we show the connection between RT and (cid:101)UT . Note that

(cid:101)Sm = −

1
T

T −1
(cid:88)

(cid:16)

t=0

Xt,Dm − ˆw(cid:62)

mXt,Dc

m

(cid:17) (cid:16)

Xt+1,m − (cid:98)A(cid:62)

mXt

(cid:17)

T −1
(cid:88)

(cid:16)

Xt,Dm − ˆw(cid:62)

mXt,Dc

m

(cid:17)

X (cid:62)

t,Dm

(cid:35) (cid:18)(cid:16)

(cid:17)

(cid:98)Am

(cid:19)

− (A∗

m)Dm

Dm

(cid:34)

1
T

t=0
(cid:18)(cid:16)

(cid:93)
Υ(m)

= (cid:98)Sm +

= (cid:98)Sm +

which implies

(cid:17)

(cid:98)Am

Dm

− (A∗

m)Dm

(cid:19)

,

ˆa(m) − (A∗

m)Dm =((cid:100)Am)Dm − (A∗

m)Dm −

Υ(m)(cid:17)−1
(cid:16)(cid:93)

(cid:101)Sm = −

Υ(m)(cid:17)−1
(cid:16)(cid:93)

(cid:98)Sm.

Thus

RT =

=

T
ˆσ2

T
ˆσ2

k
(cid:88)

m=1
k
(cid:88)

m=1

(ˆa(m) − (A∗

m)Dm)(cid:62) (cid:91)

Υ(m) (ˆa(m) − (A∗

m)Dm)

(cid:18)(cid:93)
Υ(m)

(cid:98)S(cid:62)
m

(cid:62)(cid:19)−1

Υ(m) (cid:16)(cid:93)
(cid:91)

Υ(m)(cid:17)−1

(cid:98)Sm,

and the only diﬀerence between RT and (cid:101)UT is that we substitute
(cid:18)(cid:93)
Υ(m)

We only need to prove that

Υ(m) (cid:16)(cid:93)
(cid:91)

Υ(m)(cid:17)−1

(cid:62)(cid:19)−1

is very similar to the proof of Lemma 3.3, but we need to bound

satisﬁes Assumption 3.3. The argument
Υ(m)(cid:17)−1 (cid:93)
Υ(m)

− Υ(m)

(cid:13)
Υ(m) (cid:16)(cid:91)
(cid:93)
(cid:13)
(cid:13)
(cid:13)

(cid:62)

(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

Υ(m)(cid:17)−1
(cid:16)(cid:91)

by

(cid:18)(cid:93)
Υ(m)

(cid:62)(cid:19)−1

Υ(m) (cid:16)(cid:93)
(cid:91)

Υ(m)(cid:17)−1

.

instead of

Υ(m) − Υ(m)(cid:13)
(cid:13)
(cid:91)
(cid:13)
(cid:13)
(cid:13)∞
(cid:13)

here.

Let E =

(cid:93)
Υ(m) −

(cid:91)
Υ(m), then

(cid:62)

Υ(m) (cid:16)(cid:91)
(cid:93)
(cid:16)(cid:91)

Υ(m) + E

Υ(m)(cid:17)−1 (cid:93)
Υ(m)
Υ(m)(cid:17)−1 (cid:16)(cid:91)
(cid:17) (cid:16)(cid:91)
Υ(m)(cid:17)−1
(cid:16)(cid:91)

(cid:91)
Υ(m) + E + E(cid:62) + E

=

=

Υ(m) + E(cid:62)(cid:17)

E(cid:62).

Recall that when proving Lemma 3.3, we already upper bound
with probability at least 1 − c1 exp{−c2 log M }. Thus for any vector u ∈ Rdm s.t (cid:107)u(cid:107)2 = 1,

by C

Υ(m) − Υ(m)(cid:13)
(cid:13)
(cid:91)
(cid:13)
(cid:13)
(cid:13)∞
(cid:13)

(cid:113) log M
T

u(cid:62)(cid:91)

Υ(m)u =u(cid:62)Υ(m)u + u(cid:62) (cid:16)(cid:91)
Υ(m) − Υ(m)(cid:17)
u
Υ(m) − Υ(m)(cid:13)
(cid:13)
(cid:16)
(cid:91)
(cid:13)
(cid:13)
(cid:13)∞
(cid:13)

Υ(m)(cid:17)

≥Λmin

− dm

≥ C,

48

which implies Λmax

(cid:18)(cid:16)(cid:91)

Υ(m)(cid:17)−1(cid:19)

≤ C, and

in the following. One can show that

(cid:13)
(cid:13)
(cid:13)
(cid:13)

E

Υ(m)(cid:17)−1
(cid:16)(cid:91)

E(cid:62)

(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

≤ Cdm(cid:107)E(cid:107)∞. We bound (cid:107)E(cid:107)∞

(cid:107)E(cid:107)∞ =

≤

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
T

1
T

T −1
(cid:88)

(cid:16)

t=0

T −1
(cid:88)

(cid:16)

t=0

Xt,Dm − ˆw(cid:62)

mXt,Dc

m

(cid:17)

X (cid:62)

t,Dc
m

ˆwm

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

Xt,Dm − w∗(cid:62)

m Xt,Dc

m

(cid:17)

X (cid:62)

t,Dc
m

((cid:107)w∗

m(cid:107)1 + (cid:107) ˆwm − w∗

m(cid:107)1)

m))(cid:62)
·i

1
T

(cid:12)
(cid:12)
(( ˆwm − w∗
(cid:12)
(cid:12)
(cid:12)
T −1
(cid:88)

+ max
i,j

+

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
T

t=0

Xt,Dc

mX (cid:62)
t,Dc
m

w∗
m

T −1
(cid:88)

(cid:16)

Xt,Dc

mX (cid:62)
t,Dc
m

(cid:17)

(( ˆwm − w∗

m))·j

t=0
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

(cid:107) ˆwm − w∗

m(cid:107)1.

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Applying (42), (62), Lemma 5.7, we have
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

mX (cid:62)
t,Dc
m

Xt,Dc

T −1
(cid:88)

1
T

w∗
m

t=0

≤ (cid:13)

(cid:13)ΥDc

mw∗
m

m,Dc

(cid:13)
(cid:13)∞ +

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
T

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)∞
T −1
(cid:88)

t=0

Xt,Dc

mX (cid:62)
t,Dc
m

− ΥDc

m,Dc
m

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

(cid:107)w∗

m(cid:107)1

(63)

≤Λmax(Υ) max

i

(cid:107)(w∗

m)·,i(cid:107)2 + C

sm log M
T

≤ C.

Thus, with Lemma 5.6, Assumption 3.2, and (63), we show that with probability at least 1 −
c1 exp{−c2 log M },

(cid:107)E(cid:107)∞ ≤ C

(cid:114)

log M
T

+ C

sm log M
T

+ Csm

(cid:114)

log M
T

≤ Csm

(cid:114)

log M
T

.

Therefore, using the same arguments as in the proof of Lemma 3.3,
(cid:13)
(cid:13)
(cid:13)
(cid:13)2

Υ(m)(cid:17)−1 (cid:93)
Υ(m)

(cid:13)
(cid:13)
Υ(m) 1
(cid:13)
(cid:13)

Υ(m) (cid:16)(cid:91)
(cid:93)

Υ(m) 1

2 − I

(cid:62)

2

Υ(m)(cid:17)−1 (cid:93)
Υ(m)

(cid:62)

− Υ(m)

Υ(m)(cid:17)−1 (cid:93)
Υ(m)

(cid:62)

− Υ(m)

(cid:13)
(cid:13)
(cid:13)
(cid:13)2

(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

+ C (cid:107)E(cid:107)∞

(cid:13)
Υ(m) (cid:16)(cid:91)
(cid:93)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
Υ(m) (cid:16)(cid:91)
(cid:93)
(cid:13)
(cid:13)
(cid:13)
Υ(m) − Υ(m)(cid:13)
(cid:13)
(cid:91)
(cid:13)
(cid:13)
(cid:13)∞
(cid:13)
(cid:114)

≤C

≤Cdm

≤C

≤Csm

log M
T

.

By Lemma A.2,

(cid:13)
(cid:13)
Υ(m)− 1
(cid:13)
(cid:13)
(cid:13)

2

(cid:62)(cid:19)−1

(cid:18)(cid:93)
Υ(m)

Υ(m) (cid:16)(cid:93)
(cid:91)

Υ(m)(cid:17)−1

Υ(m)− 1

2 − I

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

≤ Csm

(cid:114)

log M
T

.

49

C Proof of Lemmas in Section 5

Proof of Lemma 5.3. Let







1
√
T

ξT,t = −

(cid:15)t,1(Υ(1))− 1

2 W ∗

1 Xt

...

(cid:15)t,k(Υ(k))− 1

2 W ∗

k Xt







.

Deﬁne ﬁltration FT,t = σ(X−p+1, X−p+2, · · · , Xt+1), then (ξT t, FT t)0≤t≤T −1 is a martingale
diﬀerence sequence, and VT = (cid:80)T −1
t=0 ξT,t. To bound the convergence rate, we are going to use a
modiﬁed version of Lemma 4 in Grama and Haeusler (2006).

Lemma C.1. Let (ξni, Fni)0≤i≤n be a martingale diﬀerence sequence taking values in Rd. Let
X n

i=1 ξni, and (cid:104)X n(cid:105)k = (cid:80)k

ni|Fn,i−1). Deﬁne Rn,d

i=1 ani (cid:44) (cid:80)k

δ + N n,d

δ = Ln,d

k = (cid:80)k

E(ξniξ(cid:62)

i=1

,

δ

Ln,d

δ =

n
(cid:88)

i=1

E(cid:107)ξni(cid:107)2+2δ

2

, N n,d

δ = E(cid:107) (cid:104)X n(cid:105)n − I(cid:107)1+δ

tr

.

Then ∀µ ∈ Rd, r ≥ 0, 0 < δ ≤ 1

2 , when Rn,d

δ ≤ 1,

P((cid:107)X n

n + µ(cid:107)2 ≥ r) − P((cid:107)Z + µ(cid:107)2 ≥ r) ≤ C((cid:107)µ(cid:107)2, d, δ)

(cid:16)

Rn,d
δ

(cid:17) 1

3+2δ ,

where Zd×1 ∼ N (0, I), C((cid:107)µ(cid:107)2, d, δ) is non-decreasing as (cid:107)µ(cid:107)2 increases.

By Lemma C.1, to bound supx>0,
.

δ + N T,d

δ = LT,d
RT,d

δ

(cid:12)
P((cid:107)VT + µ(cid:107)2
(cid:12)
(cid:12)

2 ≤ x) − Fd,(cid:107)µ(cid:107)2

(x)

(cid:12)
(cid:12)
(cid:12), we only need to bound

2

LT,d

δ =

T −1
(cid:88)

(cid:16)

E

(cid:17)

(cid:107)ξT,t(cid:107)2+2δ
2

t=0

≤ CT −(1+δ)

T
(cid:88)

E

(cid:32) k

(cid:88)

(cid:33)1+δ

(cid:107)W ∗

mXt(cid:107)2

2(cid:15)2

t,m

t=1
T −1
(cid:88)

kδ

m=1
k
(cid:88)

E

(cid:16)

≤ CT −(1+δ)

|(cid:15)t,m|2+2δ(cid:107)W ∗

mXt(cid:107)2+2δ
2

(cid:17)

t=0
k
(cid:88)

= T −δkδC(δ)

m=1

(cid:16)

E

(cid:107)W ∗

mX0(cid:107)2+2δ
2

(cid:17)

m=1

Here the second line is due to Λmin(Υ(m)) ≥ 1, and the third line is due to f (x) = x1+δ is a
convex function. More speciﬁcally,

(cid:32) k

(cid:88)

m=1

(cid:107)W ∗

mXt(cid:107)2

2(cid:15)2

t,K

(cid:33)1+δ

≤

1
k

k
(cid:88)

m=1

(cid:0)k(cid:107)W ∗

mXt(cid:107)2

2(cid:15)2

t,K

(cid:1)1+δ

= kδ

k
(cid:88)

(cid:16)

m=1

(cid:107)W ∗

mXt(cid:107)2+2δ
2

(cid:15)2+2δ
t,K

(cid:17)

.

50

While for the last line, since (cid:15)t,m is sub-Gaussian with parameter τ , E|(cid:15)t,m|2+2δ ≤ C(δ). Note
that d, β, τ are all viewed as constants here. Due to the sub-Gaussianity of (cid:15)t,i’s, we have the
following lemma.

Lemma C.2.

Therefore,

which implies

E ((cid:107)W ∗

mXt(cid:107)q
2)

1
q ≤ Cq

for all q ≥ 1.

(cid:16)

E

(cid:107)W ∗

mX0(cid:107)2+2δ
2

(cid:17)

≤ C(δ),

LT,d
δ ≤ C(δ)T −δ.

While for N T,d

δ

, since

T −1
(cid:88)

(cid:16)

E

ξT,tξ(cid:62)

T,t|FT,t−1

(cid:17)

− I

t=0


(Υ(1))− 1

2 B1(Υ(1))− 1

2

=







0
...
0

(Υ(2))− 1

2

· · ·
2 B2(Υ(2))− 1
. . .
· · ·

· · ·
· · ·
. . .
· · ·

0
0
...

(Υ(k))− 1

2 Bk(Υ(k))− 1

2









,

where Bm = W ∗
m

(cid:16) 1
T

(cid:80)T −1

t=0 XtX (cid:62)

(cid:17)
t − Υ

W ∗(cid:62)
m ,

N T,d

δ =E

≤E

≤E

(cid:32) k

(cid:88)

m=1

(cid:32) k

(cid:88)

m=1

(cid:32) k

(cid:88)













m=1

(cid:13)
(cid:13)(Υ(m))− 1
(cid:13)

2 Bm(Υ(m))− 1

2

(cid:33)1+δ


(cid:13)
(cid:13)
(cid:13)tr

dm

(cid:13)
(cid:13)(Υ(m))− 1
(cid:13)

2 Bm(Υ(m))− 1

2

(cid:33)1+δ


(cid:13)
(cid:13)
(cid:13)2

d2
m(cid:107)Bm(cid:107)∞

(cid:33)1+δ
 ,

where the second line is because that (Υ(m))− 1
apply Lemma A.3; the last line is due to

2 Bm(Υ(m))− 1

2 is of rank at most dm, and we can

(cid:107)Bm(cid:107)2 = sup
(cid:107)u(cid:107)2=1

(cid:107)Bmu(cid:107)2 ≤ sup
(cid:107)u(cid:107)2=1

(cid:112)

dm(cid:107)Bmu(cid:107)∞ ≤ sup
(cid:107)u(cid:107)2=1

(cid:112)

dm(cid:107)Bm(cid:107)∞(cid:107)u(cid:107)1 = dm(cid:107)Bm(cid:107)∞.

Since

(Bm)ij =

1
T

T −1
(cid:88)

t=0

X (cid:62)

t (W ∗

m)(cid:62)

i· (W ∗

m)j·Xt − tr

(cid:16)

51

(W ∗

m)(cid:62)

i· (W ∗

(cid:17)
m)j·Υ

,

by Lemma 5.2, we only need to bound the operator norm and trace norm of

(W ∗

m)(cid:62)

i· (W ∗

m)j· + (W ∗

m)(cid:62)

j·(W ∗

m)i·

(cid:17)

.

(cid:16)

1
2

By (61) and (62), we have the following:

(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

≤2

(cid:16)

1
2

(cid:16)

(W ∗

m)(cid:62)

i· (W ∗

m)j· + (W ∗

m)(cid:62)

j·(W ∗

m)i·

(W ∗

m)(cid:62)

i· (W ∗

m)j· + (W ∗

m)(cid:62)

j·(W ∗

m)i·

≤ C.

(cid:17)(cid:13)
(cid:13)
(cid:13)
(cid:13)tr
(cid:17)(cid:13)
(cid:13)
(cid:13)
(cid:13)2

Therefore, applying Lemma 5.2 leads us to





P

(cid:32) k

(cid:88)

d2
m(cid:107)Bm(cid:107)∞

(cid:33)1+δ



> x



m=1
(cid:32)

P

(cid:107)Bm(cid:107)∞ >

(cid:33)

1
1+δ

x
d2

≤

k
(cid:88)

m=1

(cid:110)

−c2T min

(cid:110)

x

1
1+δ , x

2
1+δ

(cid:111)(cid:111)

,

≤c1 exp

which implies

N T,d

δ ≤

≤

(cid:90) ∞

0
(cid:90) ∞

0

≤C(δ)

≤C(δ)





P

(cid:32) k

(cid:88)

(cid:33)1+δ



d2
m(cid:107)Bm(cid:107)∞

> x

 dx

m=1
(cid:110)

c1 exp

−c2T min

(cid:110)

x

2
1+δ , x

1
1+δ

(cid:111)(cid:111)

dx

(cid:18)(cid:90) 1

uδ exp{−cT u2}du +

uδ exp{−cT u}du

(cid:19)

(cid:90) ∞

1

(cid:18)

0
T − 1+δ

2 Γ

(cid:19)

(cid:18) 1 + δ
2

+ T −1−δΓ(1 + δ)

(cid:19)

≤C(δ)T − 1+δ
2 .

Thus,

RT,d
δ = N T,d

δ + LT,d

δ ≤ C(δ)

(cid:16)

T −δ + T − 1+δ

2

(cid:17)

.

By Lemma C.1, for any x ≥ 0, µ ∈ Rd, and 0 ≤ δ ≤ 1

2 , when T > C(δ),

(cid:12)
P (cid:0)(cid:107)VT + µ(cid:107)2
(cid:12)
(cid:12)

2 ≤ x(cid:1) − Fd,(cid:107)µ(cid:107)2

(x)

(cid:12)
(cid:12)
(cid:12) ≤ C((cid:107)µ(cid:107)2, δ)

(cid:16)

RT,d
δ

(cid:17) 1

3+2δ .

2

The best rate is achieved when δ = 1

2 , and thus when T > C,

(cid:12)
(cid:12)
(cid:12)

sup
x≥0

P (cid:0)(cid:107)VT + µ(cid:107)2

2 ≤ x(cid:1) − Fd,(cid:107)µ(cid:107)2

2

(x)

(cid:12)
(cid:12) ≤ C((cid:107)µ(cid:107)2)T − 1
(cid:12)
8 ,

52

Proof of Lemma 42. We prove the lower and upper bounds for eigenvalues of Υ, by establishing
a connection between our stability condition (13) and another spectral density based condition
proposed in Basu et al. [2015]. First we introduce the following lemma, which is a direct result
of proposition 2.3 and (2.6) in Basu et al. [2015] under our setting.

Lemma C.3. Under the model speciﬁed in (3) with independent noise (cid:15)ti of unit variance, the
eigenvalues of Υ can be bounded as follows:

(µmax(A))−1 ≤ Λmin(Υ) ≤ Λmax(Υ) ≤ (µmin(A))−1 ,

where µmin(A) = min|z|=1 Λmin (A∗(z)A(z)), and µmax(A) = max|z|=1 Λmax (A∗(z)A(z)).

By Lemma C.3, we only need to prove that condition (13) implies a lower bound for µmin(A)

and upper bound for µmax(A). First note that

µmin(A) = min
|z|=1

Λmin (A(z)A∗(z))

= min
|z|=1

= min
|z|=1

= min
|z|=1

inf
u

inf
v

(cid:107)A∗(z)u(cid:107)2
2
(cid:107)u(cid:107)2
2
(cid:107)v(cid:107)2
2
(cid:13)
(cid:13)(A∗(z))−1 v
(cid:13)
(cid:13)(A∗(z))−1(cid:13)
(cid:13)
(cid:13)2

(cid:16)(cid:13)
(cid:13)

(cid:13)
2
(cid:13)
(cid:13)
2
(cid:17)−2

,

where the last equality is due to that
|z| = 1,

(cid:13)
(cid:13)

(cid:13)(A∗(z))−1(cid:13)
(cid:13)
(cid:13)2

= supv

(cid:107)(A∗(z))−1v(cid:107)2
(cid:107)v(cid:107)2

. Meanwhile, for any

(cid:13)
(cid:13)

(cid:13)(A∗(z))−1(cid:13)
(cid:13)
(cid:13)2

= (cid:13)

(cid:13)A−1(z)(cid:13)

(cid:13)2 =

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

∞
(cid:88)

j=0

Ψjzj

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)2

≤

∞
(cid:88)

j=0

(cid:107)Ψj(cid:107)2 ≤ β,

where we apply condition (13) in the last inequality. Thus µmin(A) ≥ β−2.

While for bounding µmax(A), we start by bounding (cid:107)An(cid:107)2 for 0 ≤ n ≤ p. Here we deﬁne

A0 = IM ×M , and An = 0 for all n > p. Since




I = A−1(z)A(z) =

Ψjzj





∞
(cid:88)

j=0

(cid:32) p

(cid:88)

i=0

(cid:33)

Aizi

=

∞
(cid:88)

(cid:32) ∞
(cid:88)

(cid:33)

ΨiAn−i

zn,

n=0

i=0

one can show that Ψ0 = I, and (cid:80)n

i=0 ΨiAn−i = 0 for n ≥ 1. Thus

An = −

n
(cid:88)

i=1

ΨiAn−i for n ≥ 1,

and (cid:107)An(cid:107)2 ≤ (cid:80)n

i=1 (cid:107)Ψi(cid:107)2(cid:107)An−i(cid:107)2. We have the following claim:

For 0 ≤ n ≤ p,

(cid:107)An(cid:107)2 ≤ βn ∨ 1.

(64)

53

This can be proved by induction.
0 ≤ n = k ≤ p,

It is clear that (cid:107)A0(cid:107)2 = (cid:107)I(cid:107)2 = β0, and if (64) holds for

(cid:107)Ak+1(cid:107)2 ≤

n
(cid:88)

i=1

(cid:107)Ψi(cid:107)2(βn−i ∨ 1) ≤ β max

i

(βn−i ∨ 1) ≤ βn ∨ 1.

Therefore, µmax(A) can be bounded in the following:
Λmax (A(z)A∗(z))

µmax(A) = max
|z|=1

= max
|z|=1
(cid:32) p

(cid:88)

≤

(cid:107)A∗(z)(cid:107)2
2

(cid:33)2

(cid:107)Ai(cid:107)2

i=0

(cid:18) βp+1 − 1
β − 1

≤

(cid:19)2

1(β > 1) + (p + 1)21(0 ≤ β ≤ 1).

With Lemma C.3, we conclude that

where C1(β) =

(cid:16) 1−β

(cid:17)2

1−βp+1

1(β > 1) + (p + 1)−21(0 ≤ β ≤ 1), and C2(β) = β2.

C1(β) ≤ Λmin(Υ) ≤ Λmax(Υ) ≤ C2(β),

Proof of Lemma 5.1. Recall that Xt = (cid:80)∞

j=0 Ψj(cid:15)t−j−1. Deﬁne Ψ(p)

j ∈ RpM ×M as the following:

,

(65)







Ψ(p)

j =







Ψj1(j ≥ 0)
...
Ψj−p+11(j − p + 1 ≥ 0)
j=0 Ψ(p)

t=0 (cid:15)tX (cid:62)
t :
T −1
(cid:88)

(cid:15)t,1

1
T

t=0

j=0

then we can also write Xt as an inﬁnite sum Xt = (cid:80)∞
we consider the ﬁrst entry of 1
T

(cid:80)T −1

j (cid:15)t−j−1. Without loss of generality,

∞
(cid:88)

(Ψj)1·(cid:15)t−j−1.

(66)

In the following, we tackle the inﬁnite sum in (66), by focusing our analysis on the ﬁnite sum
and let the residue converges to 0. Rigorously, for any positive integer m, let

˜(cid:15) = ((cid:15)(cid:62)

−m−1, . . . , (cid:15)(cid:62)

T −1)(cid:62),

η(t) = ((Ψt+m)1·, . . . , (Ψ0)1,·, 0, . . . , 0)(cid:62) ∈ R(T +m+1)M ,

and e(t) ∈ R(T +m+1)M satisfying e(t)

i = 1(i = (t + m)M + 1), then we have

1
T

=˜(cid:15)(cid:62)

T −1
(cid:88)

t=0
(cid:32)

1
T

(cid:15)t,1

∞
(cid:88)

j=0

(Ψj)1·(cid:15)t−j−1

e(t)η(t)(cid:62)

(cid:33)

˜(cid:15) +

1
T

T −1
(cid:88)

t=0

T −1
(cid:88)

(cid:15)t,1

∞
(cid:88)

(Ψj)1·(cid:15)t−j−1

t=0

j=t+m+1

(cid:44)E1 + E2.

54

We will let m be suﬃciently large in later argument. The following arguments are devided into
two parts: bounding E1 and E2.

(1) Bounding E1

Since all entries of ˜(cid:15) are independent sub-Gaussian with constant parameter, we can apply
the following Hanson-Wright inequality:

Lemma C.4. Let X = (X1, . . . , Xn) ∈ Rn be a random vector with independent components
Xi which satisfy E(Xi) = 0 and (cid:107)Xi(cid:107)ψ2 ≤ K. Let A be an n × n matrix. Then, for every
t ≥ 0,

(cid:16)

(cid:17)
|X (cid:62)AX − EX (cid:62)AX| > t

P

(cid:26)

(cid:18)

≤ 2 exp

−c min

t2
K4(cid:107)A(cid:107)2
F

,

t
K2(cid:107)A(cid:107)2

(cid:19)(cid:27)

This lemma is a result in Rudelson et al. [2013].By Lemma C.4, we only need to bound the
norms of 1
T

t=0 e(t)η(t)(cid:62).

(cid:80)T −1

First note that

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
T

T −1
(cid:88)

t=0

e(t)η(t)(cid:62)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)2

=

sup
(cid:107)u(cid:107)2=(cid:107)v(cid:107)2=1

1
T

T −1
(cid:88)

t=0

u(cid:62)e(t)η(t)(cid:62)v.

For any u, v ∈ R(T +m+1)M with unit (cid:96)2 norm, one can show that

T −1
(cid:88)

t=0
T −1
(cid:88)

t=0
T −1
(cid:88)

t=0

1
T

1
T

1
T

1
T

=

≤

≤

u(cid:62)e(t)η(t)(cid:62)v

u(t+m)M +1

t+m
(cid:88)

(Ψt+m−i)1·v(i+1)

u(t+m)M +1

i=0
t+m
(cid:88)

i=0

αt+m−i(cid:107)v(i+1)(cid:107)2

(umM +1, · · · , u(T +m−1)M +1)Γ







(cid:107)v(1)(cid:107)2
...
(cid:107)v(T +m)(cid:107)2







≤

(cid:107)Γ(cid:107)2
T

,

where v(i) = (v(i−1)M +1, . . . , viM )(cid:62), αi = (cid:107)Ψi(cid:107)2 ≥ (cid:107)(Ψi)1·(cid:107)2, and Γ ∈ RT ×(T +m) is a matrix
with each entry Γij = αm+i−j1(m + i − j ≥ 0). Since Γ is a Toeplitz matrix, we will use the
following lemma to bound its (cid:96)2 norm.
Lemma C.5. Let f (λ) be a Fourier series deﬁned as f (λ) = (cid:80)∞
(cid:80)∞

t=−∞ tk exp{ikλ}, with
k=−∞ |tk| < ∞. We deﬁne a sequence of Toeplitz matrices Tn with (Tn)i,j = ti−j, then

the operator norm of Tn is bounded by

where ess sup f the essential supremum.

(cid:107)Tn(cid:107)2 ≤ 2ess sup f.

55

This is actually Lemma 4.1 in Gray et al. [2006], and we directly apply it here. By Lemma
C.5,

(cid:107)Γ(cid:107)2 ≤ 2 sup
λ

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

∞
(cid:88)

k=−m

αm+keikλ

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤ 2

∞
(cid:88)

k=0

αk ≤

∞
(cid:88)





∞
(cid:88)

i=0

j=0



1
2

α2

i+j



≤ β.

Thus

(cid:13)
(cid:13)
(cid:13)

1
T

(cid:80)T −1

t=0 e(t)η(t)(cid:62)(cid:13)
(cid:13)
(cid:13)2

≤ β

T . While for the Frobenius norm, we have

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
T

T −1
(cid:88)

t=0

e(t)η(t)(cid:62)

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
F

(cid:32)(cid:32)

1
T

T −1
(cid:88)

t=0

η(t)e(t)(cid:62)

(cid:33) (cid:32)

1
T

T −1
(cid:88)

l=0

(cid:33)(cid:33)

e(t)η(t)(cid:62)

=tr

=

≤

1
T 2

1
T 2

T −1
(cid:88)

t=0
T −1
(cid:88)

(cid:107)η(t)(cid:107)2
2

t+m
(cid:88)

α2

i ≤

t=0

i=0

β2
T

.

Therefore, by Lemma C.4, for any δ > 0,

P (|E1| > δ) ≤ 2 exp (cid:8)−cT min{δ, δ2}(cid:9) .

(2) Bounding E2

First note that

|E2| =

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
T

T −1
(cid:88)

(cid:15)t,1

∞
(cid:88)

(Ψj)1·(cid:15)t−j−1

t=0

j=t+m+1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤

1
2T

T −1
(cid:88)

t=0

(cid:15)2
t,1 +

1
2T

T −1
(cid:88)





∞
(cid:88)


2

(Ψj)1·(cid:15)t−j−1



.

t=0

j=t+m+1

Recall the deﬁnition of (cid:107) · (cid:107)ψ1 and (cid:107) · (cid:107)ψ2 in the proof of Lemma C.2. Since (cid:107)(cid:15)2
2(cid:107)(cid:15)t,1(cid:107)2
ψ2

≤ 2τ 2,

t,1(cid:107)ψ1 ≤

P

(cid:32)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
2T

T −1
(cid:88)

t=0

(cid:15)2
t,1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:33)

> δ

≤ 2 exp{−cT min{δ, δ2}},

by Bernstein type inequality of sub-exponential random variables(see proposition 5.16 in Ver-
shynin [2010]).

Now we bound the second term 1
2T

(cid:80)T −1
t=0

(cid:16)(cid:80)∞

j=t+m+1(Ψj)1·(cid:15)t−j−1

(cid:17)2

. Since

∞
(cid:88)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
j=t+m+1

(Ψj)1·(cid:15)t−j−1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤

∞
(cid:88)

j=t+m+1

αj(cid:107)(cid:15)t−j−1(cid:107)2,

56

one can show that

≤

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
2T

T −1
(cid:88)





∞
(cid:88)

(Ψj)1·(cid:15)t−j−1

t=0

j=t+m+1





2(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)ψ1

∞
(cid:88)

(Ψj)1·(cid:15)t−j−1

j=t+m+1

≤CM τ 2





∞
(cid:88)



αj



,

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
2

ψ2

j=t+m+1
√

where we apply the fact that (cid:107)(cid:107)(cid:15)t(cid:107)2(cid:107)ψ2
Thus we have

≤ C

M τ , which is shown in the proof of Lemma C.2.





P

1
2T

T −1
(cid:88)





∞
(cid:88)



2



(Ψj)1·(cid:15)t−j−1



> δ



t=0

j=t+m+1

≤C exp




−



cδ
M τ 2 (cid:16)(cid:80)∞

j=t+m+1 αj






.

(cid:17)2

due to the tail bound of sub-exponential r.v. (also see Vershynin [2010]). Since

∞
(cid:88)

i=0

∞
(cid:88)

αi ≤





∞
(cid:88)



1
2

α2

i+j



≤ β,

i=0

j=0





∞
(cid:88)

j=t+m+1



2

αj



= 0.

lim
m→∞

Let m be suﬃciently large such that

(cid:16)(cid:80)∞

j=t+m+1 αj

(cid:17)2

≤ 1

M T , then we arrive at the following

(cid:32)

P

1
T

T −1
(cid:88)

t=0

(cid:33)

(cid:15)t,1(Xt)1

≤ C exp{−cT min{δ, δ2}}.

√

Let δ = C
conclusion follows.

log M T and take a union bound over the pM 2 entries of 1
T

(cid:80)T −1

t=0 (cid:15)tX (cid:62)

t , the

Proof of Lemma 5.6. Without loss of generality, consider

1
T

T −1
(cid:88)

(cid:16)

t=0

Xt,Dm − w∗(cid:62)

m Xt,Dc

m

(cid:17)

i

Xt,j

57

for any 1 ≤ i ≤ dm, and j ∈ Dc
quadratic form

m. Similar from the proof of Lemma 5.6, We can write it as a

1
T

T −1
(cid:88)

t=0

X (cid:62)
t

(cid:16)

1
2

(W ∗

m)(cid:62)

i· e(cid:62)

j + ej(W ∗

m)i·

(cid:17)

Xt,

m is deﬁned as in (48). Since 1
2

(cid:16)

(W ∗

m)(cid:62)

i· e(cid:62)

j + ej(W ∗

m)i·

(cid:17)

is of rank 2, and we have

where W ∗
bounded (cid:107)(W ∗

m)i·(cid:107)2 in (62), applying Lemma A.3 leads to

(cid:16)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
2

(W ∗

m)(cid:62)

i· e(cid:62)

j + ej(W ∗

m)i·

(cid:17)(cid:13)
(cid:13)
(cid:13)
(cid:13)tr

(W ∗

m)(cid:62)

i· e(cid:62)

j + ej(W ∗

m)i·

(cid:17)(cid:13)
(cid:13)
(cid:13)
(cid:13)2

(cid:16)

1
2

≤2

(cid:13)
(cid:13)
(cid:13)
(cid:13)
≤ (cid:107)(W ∗

m)i·(cid:107)2 ≤ C.

Applying Lemma 5.2, and taking a union bound over all entries of

1
T

T −1
(cid:88)

(cid:16)

t=0

the conclusion follows.

Xt,Dm − w∗(cid:62)

m Xt,Dc

m

(cid:17)

Xt,

Proof of Lemma 5.7. Similar from the proof of Lemma 5.1, we consider
Since

T −1
(cid:88)

1
T

XtiXtj =

X (cid:62)
t

(eie(cid:62)

j + eje(cid:62)
i )

Xt,

T −1
(cid:88)

1
T

(cid:18) 1
2

(cid:19)

t=0
by Lemma 5.2, we need to bound norms of 1
2 (eie(cid:62)
show that

t=0

j + eje(cid:62)

i ), which is of rank at most 2. One can

(cid:12)
(cid:12)
(cid:12)

1
T

(cid:80)T −1

t=0 XtiXtj − Υij

(cid:12)
(cid:12)
(cid:12).

(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
2

(eie(cid:62)

(cid:13)
(cid:13)
j + eje(cid:62)
i )
(cid:13)
(cid:13)tr

≤ 2

(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
2

(eie(cid:62)

j + eje(cid:62)
i )

(cid:13)
(cid:13)
(cid:13)
(cid:13)2

≤ 2(cid:107)ei(cid:107)2(cid:107)ej(cid:107)2,

with Lemma A.3. Therefore, by taking a union bound, it is clear that

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
T

T −1
(cid:88)

t=0

XtX (cid:62)

(cid:13)
(cid:13)
(cid:13)
t − Υ
(cid:13)
(cid:13)∞

(cid:114)

≤ C

log M
T

,

with probability at least 1 − c1 exp{−c2 log M }.

D Proof of Lemmas in Section A and Appendix C

Proof of Lemma C.1. Here we adopt the proof framework for Lemma 4 in Grama and Haeusler
[2006], but with some small adjustments. First we construct a new martingale diﬀerence sequence
(mnk, Gnk)1≤k≤n+1, sum of whose covariances equal to Id×d. Random projections are used for
construction. The following lemma on random projections is stated as Lemma 3 in Grama and
Haeusler [2006].

58

Lemma D.1. Let V and a1, · · · , an be positive semi-deﬁnite d × d matrices. Set Ak = a1 +
· · · + ak, for k = 1, · · · , n. Then there exist a sequence of integers 1 ≤ τ1 ≤ · · · ≤ τd ≤ n and
a corresponding sequence S1 ⊇ · · · ⊇ Sd of subspaces of Rd such that, with Pk deﬁned as the
projection matrix of subspace Si, for τi ≤ k < τi+1 (where τ0 = 1, τd+1 = n + 1, S0 = Rd), the
following statements hold true for k = 1, · · · , n:
(a)V − (cid:98)Ak is non-negative deﬁnite, where (cid:98)Ak = P1a1P1 + · · · + PkakPk;
(b)x(cid:62)( (cid:98)Ak − Ak)x = 0, for all x ∈ Πk (cid:44) {Pkx : x ∈ Rd};
(c)x(cid:62)( (cid:98)Ak − V + αkI)x ≥ 0 for all x ∈ Π(cid:62)
Meanwhile, Pk is determined by a1, · · · , ak and V .

k , where αk = max{(cid:107)aτj (cid:107)2 : τj ≤ k}.

Given this claim, mnk can be constructed as follows:

Recall the martingale sequence we consider is (ξnk, Fnk)1≤k≤n+1, and ank = E (cid:0)ξnkξ(cid:62)
the fact with V = I, ak = ank, and let {Pnk}n
Dn = I − (cid:80)n

(cid:1). Apply
k=1 be the corresponding projection matrices. Let

k=1 PnkankPnk, which is non-negative deﬁnite. Deﬁne

nk

M n

k =

n
(cid:88)

k=1

mnk,

1 ≤ k ≤ n + 1,

where

mnk = Pnkξnk, for 1 ≤ k ≤ n, mn,n+1 = D

1
2

n ηn,n+1.

Since Pnk ∈ Fn,k−1, mnk ∈ Fnk for 1 ≤ k ≤ n.Thus (mnk, Gnk) is also a martingale diﬀerence
sequence with Gnk = Fnk, when 1 ≤ k ≤ n, and Gn,n+1 = σ(Fnn, ηn,n+1). Meanwhile,

(cid:104)M n(cid:105)n+1 =

n+1
(cid:88)

k=1

E(mnkm(cid:62)

nk|Fn,k−1) = Id×d.

This construction is from Grama and Haeusler [2006]. They also prove that, for any ε, δ > 0,
n+1(cid:107)2 ≥ ε(cid:1) ≤ C(d, δ)ε−2−2δ (cid:16)

n − M n

δ + N n,d
Ln,d

P (cid:0)(cid:107)X n

(cid:17)

,

δ

(67)

Since

− P((cid:107)X n
+ P((cid:107)M n

n+1(cid:107)2 > ε) − P((cid:107)Z + µ(cid:107)2 ≥ r + 2ε)

n − M n
n+1 + µ(cid:107)2 ≥ r + ε) − P(Z ∈ [r, r + 2ε))

≤P((cid:107)X n
≤P((cid:107)M n

n + µ(cid:107)2 ≥ r) − P((cid:107)Z + µ(cid:107)2 ≥ r)
n+1 + µ(cid:107)2 ≥ r − ε) − P((cid:107)Z + µ(cid:107)2 ≥ r − 2ε)

(68)

+ P((cid:107)X n

n − M n

n+1(cid:107)2 > ε) + P(Z ∈ [r − 2ε, r)),

for any µ ∈ Rd, r ≥ 0, ε > 0, we need to bound

E(1((cid:107)Z + µ(cid:107)2 ≥ r + 2ε)) − E(1((cid:107)M n

n+1 + µ(cid:107)2 ≥ r + ε))

59

and

E(1((cid:107)M n

n+1 + µ(cid:107)2 ≥ r − ε)) − E(1((cid:107)Z + µ(cid:107)2 ≥ r − 2ε)).

The following functions are deﬁned as a smooth relaxation for indicator function. Let

f∗(z) =

(cid:90) z− 1
2

−∞

φ(t)dt, with φ(t) =

1
C

exp{−

4

1 − 4t2 }1(− 1
2 , 1

2 )(t),

(69)

where C is a normalizing constant s.t. (cid:82) φ(t)dt = 1. Then we have f∗(z) = 0 if z ≤ 0,
0 ≤ f∗(z) ≤ 1 if 0 ≤ z ≤ 1, and f∗(z) = 1 if z ≥ 1. f∗(z) is inﬁnitely many times diﬀerentiable
on R, and since f∗(z) is constant when z ≤ 0 or z ≥ 1, for any ﬁxed order, the derivative of
f∗(z) is bounded. For any z ∈ Rd, let

fl,µ,r,ε(z) = f∗(gl,µ,r,ε(z)),

where

g1,µ,r,ε(z) =

(cid:107)z + µ(cid:107)2 − r − ε
ε

,

g2,µ,r,ε(z) =

(cid:107)z + µ(cid:107)2 − r + 2ε
ε

.

(70)

(71)

In the following proof, we will denote fl,µ,r,ε(z) and gl,µ,r,ε(z) as fl(z) and gl(z), l = 1, 2 for
brevity. Therefore,

E(1((cid:107)Z + µ(cid:107)2 ≥ r + 2ε)) − E(1((cid:107)M n

n+1 + µ(cid:107)2 ≥ r + ε)) ≤ E(f1(Z) − f1(M n

n+1)),

E(1((cid:107)M n

n+1 + µ(cid:107)2 ≥ r − ε)) − E(1((cid:107)Z + µ(cid:107)2 ≥ r − 2ε)) ≤ E(f2(M n

n+1) − f1(Z)).

Thus,

|P((cid:107)X n

n + µ(cid:107)2 ≥ r) − P((cid:107)Z + µ(cid:107)2 ≥ r)|
|E(fl(M n

n+1) − fl(Z))| + P((cid:107)X n

≤ max
l=1,2
+ P((cid:107)Z + µ(cid:107)2 ∈ [r − 2ε, r + 2ε]).

n − M n

n+1(cid:107)2 > ε)

Actually, when r ≤ 3ε, the right hand side of (68) can be substituted by

P((cid:107)Z + µ(cid:107)2 < 3ε),

and

|P((cid:107)X n

≤ max{|E(f1(M n

n + µ(cid:107)2 ≥ r) − P((cid:107)Z + µ(cid:107)2 ≥ r)|
n+1) − f1(Z))| + P((cid:107)X n
+ P((cid:107)Z + µ(cid:107)2 ∈ [r, r + 2ε]), P((cid:107)Z + µ(cid:107)2 ∈ [0, 3ε))}.

n − M n

n+1(cid:107)2 > ε)

(72)

To bound E(fl(M n

n+1) − fl(Z)), we will use the following lemma.

Lemma D.2. For fl(·) deﬁned as in (70),
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
1≤i1,··· ,ik≤d

yi1 · · · yik

(cid:88)

∂k
∂zi1 · · · ∂zik

(cid:12)
(cid:12)
(cid:12)
fl(z)
(cid:12)
(cid:12)
(cid:12)

≤ C(k)ε−k(cid:107)y(cid:107)k
2,

(73)

for any k ∈ Z∗, y, z ∈ Rd, when l = 1, or when l = 2 and r > 3ε.

60

The proof of this lemma is deferred to Appendix E. In the following proof, we will always

assume the condition l = 1 or l = 2 and r > 3ε hold. Therefore, for any m ∈ Z∗,

(cid:12)
(cid:12)
(cid:12)
fl(z + y) − fl(y) −
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
1≤i1,··· ,im+1≤d

(cid:88)

=

yi1 · · · yim+1

m
(cid:88)

(cid:88)

k=1

1≤i1,··· ,ik≤d

yi1 · · · yik

∂k
∂zi1 · · · ∂zik

(cid:12)
(cid:12)
(cid:12)
fl(z)
(cid:12)
(cid:12)
(cid:12)

∂m+1
∂ui1 · · · ∂uim+1

(cid:12)
(cid:12)
(cid:12)
fl(u)
(cid:12)
(cid:12)
(cid:12)

≤C(m + 1)ε−m−1(cid:107)y(cid:107)m+1

2

,

where u = z + t1y for some 0 ≤ t1 ≤ 1. Meanwhile,

m
(cid:88)

(cid:88)

k=1

1≤i1,··· ,ik≤d

yi1 · · · yik

∂k
∂zi1 · · · ∂zik

(cid:12)
(cid:12)
(cid:12)
fl(z)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
fl(z + y) − fl(y) −
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
=
(cid:12)
(cid:12)

1≤i1,··· ,im≤d

(cid:88)

yi1 · · · yi(m)

(cid:88)

−

yi1 · · · yim

1≤i1,··· ,im≤d
≤2C(m)ε−m(cid:107)y(cid:107)m
2 ,

∂m
∂vi1 · · · ∂vim

fl(v)

∂m
∂zi1 · · · ∂zim

(cid:12)
(cid:12)
(cid:12)
fl(z)
(cid:12)
(cid:12)

where v = z + t2y for some 0 ≤ t2 ≤ 1. Thus, for any δ > 0,

(cid:12)
(cid:12)
(cid:12)
fl(z + y) − fl(y) −
(cid:12)
(cid:12)
(cid:12)

(cid:100)2+2δ(cid:101)−1
(cid:88)

(cid:88)

k=1

1≤i1,··· ,ik≤d

yi1 · · · yik

∂k
∂zi1 · · · ∂zik

(cid:12)
(cid:12)
(cid:12)
fl(z)|
(cid:12)
(cid:12)
(cid:12)

≤C(δ) max{ε−(cid:100)2+2δ(cid:101)+1(cid:107)y(cid:107)(cid:100)2+2δ(cid:101)−1
≤C(δ)ε−2−2δ(cid:107)y(cid:107)2+2δ

2

.

2

, ε−(cid:100)2+2δ(cid:101)(cid:107)y(cid:107)(cid:100)2+2δ(cid:101)

2

}

Let ˜wnk, 1 ≤ k ≤ n be i.i.d. standard Gaussian random vectors that are independent of Gn,n+1,
wnk = (bnk)

2 ˜wnk, for k = 1, · · · , n + 1, where bnk = E(mnkm(cid:62)

nk|Gn,k−1). Deﬁne

1

W n

n+2 = 0, W n

k =

n+1
(cid:88)

i=k

wni,

1 ≤ k ≤ n + 1.

61

Then W n

1 follows standard Gaussian distribution. Let U n
n+1) − fl(Z))(cid:12)
(cid:12)
1 ))(cid:12)
n+1) − fl(W n
(cid:12)

k = M n

k−1 + W n

k+1, then

(cid:12)
(cid:12)E(fl(M n
= (cid:12)
(cid:12)E(fl(M n
(cid:12)
n+1
(cid:12)
(cid:88)
(cid:12)
(cid:12)
(cid:12)
k=1
(cid:12)
n+1
(cid:12)
(cid:88)
E(fl(U n
(cid:12)
(cid:12)
(cid:12)

E(fl(U n

=

≤

k=1

k + mnk) − fl(U n

(cid:12)
(cid:12)
(cid:12)
k + wnk))
(cid:12)
(cid:12)

k + mnk) − fl(U n
k )

(cid:100)2+2δ(cid:101)−1
(cid:88)

−

(cid:88)

(mnk)i1 · · · (mnk)ij

j=1

1≤i1,··· ,ij ≤d

∂j
∂zi1 · · · ∂zij

fl(U n

(cid:12)
(cid:12)
(cid:12)
k ))
(cid:12)
(cid:12)

+

n+1
(cid:88)

k=1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

E(fl(U n

k + wnk) − fl(U n
k )

(cid:100)2+2δ(cid:101)−1
(cid:88)

−

(cid:88)

(wnk)i1 · · · (wnk)ij

j=1

1≤i1,··· ,ij ≤d

∂j
∂zi1 · · · ∂zij )

fl(U n

(cid:12)
(cid:12)
(cid:12)
k ))
(cid:12)
(cid:12)

≤

n+1
(cid:88)

k=1

C(δ)ε−2−2δE((cid:107)mnk(cid:107)2+2δ

2

).

Generally this inequality holds for δ ∈ (0, 1
2 ], since wnk and mnk have the same second order
moments, which justiﬁes the fourth line. By the proof of Lemma 4 in Grama and Haeusler
[2006],

n+1
(cid:88)

k=1

E((cid:107)mnk(cid:107)2+2δ

2

thus

) ≤ C(d, δ)(Ln,d

δ + N n,d

δ

),

(cid:12)
(cid:12)E (cid:0)fl(M n

n+1) − fl(z)(cid:1)(cid:12)

(cid:12) ≤ C(d, δ)ε−2−2δRn,d

δ

.

(74)

Now we only need to bound P ((cid:107)Z + µ(cid:107)2 ∈ [r − 2ε, r + 2ε]) and P ((cid:107)Z + µ(cid:107)2 ∈ [0, 3ε)). Assume
ε ≤ 1, then

P ((cid:107)Z + µ(cid:107)2 ∈ [0, 3ε)) = P (Z ∈ B3ε(−µ)) ≤ C(d)εd ≤ C(d)ε.

Meanwhile,

P((cid:107)Z + µ(cid:107)2 ∈ [r − 2ε, r + 2ε])
=P(Z ∈ Br+2ε(−µ)\Br−2ε(−µ))
C(d) (cid:0)(r + 2ε)d − (r − 2ε)d(cid:1) ,
C(d) exp{−(r − 2ε − (cid:107)µ(cid:107)2)2/2} (cid:0)(r + 2ε)d − (r − 2ε)d(cid:1) ,






≤

r ≤ 2ε + (cid:107)µ(cid:107)

r > 2ε + (cid:107)µ(cid:107)2

≤C(d, (cid:107)µ(cid:107)2)ε.

The last line is due to that

(r + 2ε)d − (r − 2ε)d ≤4εd(r + 2ε)d−1 ≤ 4dε(4 + (cid:107)µ(cid:107)2)d−1,

62

when r ≤ 2ε + (cid:107)µ(cid:107)2, and

exp{−(r − 2ε − (cid:107)µ(cid:107)2)2/2}

(cid:16)

(r + 2ε)d − (r − 2ε)d(cid:17)

≤4εd sup
x>0

(x + 4ε + (cid:107)µ(cid:107)2)d−1 exp{−x2/2}

≤4d sup
x>0

(x + 4 + (cid:107)µ(cid:107)2)d−1 exp{−x2/2}ε.

Here clearly C(d, (cid:107)µ(cid:107)2) is non-decreasing with respect to (cid:107)µ(cid:107)2. Therefore, by (72), (67) and
(74), when Rn,d

δ ≤ 1, for any µ ∈ Rd, r ≥ 0, 0 < δ ≤ 1

2 , with ε = (Rn,d

1
3+2δ ,

)

δ

P((cid:107)X n

n + µ(cid:107)2 ≥ r) − P((cid:107)Z + µ(cid:107)2 ≥ r) ≤ C(d, δ, (cid:107)µ(cid:107)2)

(cid:16)

Rn,d
δ

(cid:17) 1

3+2δ ,

where C(d, δ, (cid:107)µ(cid:107)2) is non-decreasing with respect to (cid:107)µ(cid:107)2.

Proof of Lemma C.2. First we introduce the following two norms:
For any random variable X,

(cid:107)X(cid:107)ψ1 = sup
p≥1

(cid:107)X(cid:107)ψ2 = sup
p≥1

p−1E (|X|p)

1
p ,

p− 1

2 E (|X|p)

1
p .

These two norms are related to sub-exponential and sub-Gaussian random variables, and the
following lemma shows the connections between the two norms and the scale factor for sub-
Gaussian r.v.

Lemma D.3. For any sub-Gaussian r.v. X with scale factor τ , the following hold:

with some absolute constants c, C, and

cτ ≤ (cid:107)X(cid:107)ψ2 ≤ Cτ,

(cid:107)X(cid:107)2

ψ2 ≤ (cid:107)X(cid:107)ψ1 ≤ 2(cid:107)X(cid:107)2

ψ2.

This is an established result in Vershynin [2010]. By Lemma D.3, bounding

would be suﬃcient, and we start from bounding E (exp {λ (W ∗
that Xt = Ψ(p)
j deﬁned as in (65), we can write

j εt−j−1, with Ψ(p)

(cid:13)
(cid:13)
(cid:13)ψ1
m)i· Xt}) for any λ ∈ R. Recall

(cid:13)
(cid:13)(cid:107)W ∗
(cid:13)

mXt(cid:107)2
2

(W ∗

m)i·Xt = (W ∗

m)i·

∞
(cid:88)

k=0

Ψ(p)

k (cid:15)t−k−1 = lim
N →∞

N
(cid:88)

(W ∗

m)i·Ψ(p)

k (cid:15)t−k−1,

k=0

exp {λ (W ∗

m)i· Xt} = lim
N →∞

(cid:40)

exp

λ

N
(cid:88)

k=0

63

(W ∗

m)i·Ψ(p)

k (cid:15)t−k−1

(cid:41)

,

and

(cid:40)

exp

λ

N
(cid:88)

k=0

(W ∗

m)i· Ψ(p)

k (cid:15)t−k−1

(cid:41)

(cid:40)

≤ exp

|λ|

∞
(cid:88)

k=0

(cid:107)(W ∗

m)i·(cid:107)2 ˜αk (cid:107)(cid:15)t−k−1(cid:107)2

,

(cid:41)

where ˜αk is deﬁned as
as follows:

(cid:13)
(cid:13)Ψ(p)
(cid:13)

k

(cid:13)
(cid:13)
(cid:13)2

. The relationship between ˜αk and αk = (cid:107)Ψk(cid:107)2 can be established

˜αk = sup
(cid:107)u(cid:107)2=1

(cid:13)
(cid:13)
(cid:13)Ψ(p)
(cid:13)
(cid:13)
k u
(cid:13)2

= sup

(cid:107)u(cid:107)2=1





(p−1)∧j
(cid:88)

n=0



1
2

(cid:107)Ψk−nu(cid:107)2
2



≤

(cid:33) 1

2

α2

k−n

,

(cid:32)p−1
(cid:88)

n=0

(75)

if we deﬁne αi = 0 when i < 0. We now prove that exp {|λ| (cid:80)∞
m)i·(cid:107)2 ˜αk (cid:107)(cid:15)t−k(cid:107)2} is
integrable so that we can use Dominated Convergence Theorem. Since (cid:15)ti’s are all independent
sub-Gaussian random variables with parameter τ ,

k=0 (cid:107)(W ∗

(cid:107)(cid:107)(cid:15)t(cid:107)2(cid:107)ψ2

≤ (cid:13)

(cid:13)(cid:107)(cid:15)t(cid:107)2
2

(cid:13)
(cid:13)

1
2
ψ1

≤ (cid:0)M (cid:107)(cid:15)2

ti(cid:107)ψ1

(cid:1) 1

2 ≤ C

√

M τ,

(76)

where the second inequality is due to Minkowski’s inequality. Thus,

(cid:32)

(cid:40)

E

exp

|λ|

(cid:32)

∞
(cid:88)

k=0
(cid:40)

= lim
N →∞

E

exp

|λ|

(cid:40)

(cid:107)(W ∗

m)i·(cid:107)2 ˜αk (cid:107)(cid:15)t−k(cid:107)2

(cid:41)(cid:33)

(cid:107)(W ∗

m)i·(cid:107)2 ˜αk (cid:107)(cid:15)t−k(cid:107)2

(cid:41)(cid:33)

N
(cid:88)

k=0

≤ lim
N →∞

exp

CM λ2 (cid:107)(W ∗

m)i·(cid:107)2
2

(cid:41)

N
(cid:88)

k=0

˜α2
k

≤ exp (cid:8)CM λ2(cid:9) ,

where the ﬁrst equality is due to Monotone Convergence Theorem, and the last line is due to
(62) and the fact that

N
(cid:88)

k=0

˜α2

k ≤

N
(cid:88)

p−1
(cid:88)

k=0

n=0

α2

k−n ≤ p

N
(cid:88)

k=0

k ≤ β2.
α2

Therefore, by Dominated Convergence Theorem,

E (exp {λ (W ∗

(cid:32)

m)i· Xt})
(cid:40)

N
(cid:88)

= lim
N →∞

E

exp

λ

(cid:40)

k=0

(cid:41)(cid:33)

(W ∗

m)i·Ψ(p)

k (cid:15)t−k

(cid:41)

∞
(cid:88)

k=0

˜α2
k

≤ exp

Cλ2(cid:107)(W ∗

m)i·(cid:107)2
2

= exp (cid:8)Cλ2(cid:9) .

By Lemma D.3, (cid:107)(W ∗

m)i· Xt(cid:107)ψ2

≤ C, and

(cid:13)
(cid:13)(cid:107)W ∗
(cid:13)

mXt(cid:107)2
2

(cid:13)
(cid:13)
(cid:13)ψ1

≤

dm(cid:88)

i=1

(cid:13)
(cid:13)((W ∗
(cid:13)

m)i· Xt)2(cid:13)
(cid:13)
(cid:13)ψ1

≤ 2

dm(cid:88)

i=1

(cid:107)(W ∗

m)i· Xt(cid:107)2
ψ2

≤ C.

64

Thus

E ((cid:107)W ∗

mXt(cid:107)p
2)

1
p ≤ C

√

p.

Proof of Lemma 5.2. Recall that Xt = (cid:80)∞
from the proof of Lemma 5.1, for any positive integer m, we can write down 1
T
the following:

j (cid:15)t−j−1, where Ψ(p)

j=0 Ψ(p)

j

is deﬁned in (65). Similar
(cid:80)T −1
t BXt as

t=0 X (cid:62)

1
T

T −1
(cid:88)

t=0

X (cid:62)

t BXt =

=

1
T

1
T

T −1
(cid:88)





∞
(cid:88)



(cid:62)



Ψ(p)

j (cid:15)t−j−1



B



∞
(cid:88)



Ψ(p)

j (cid:15)t−j−1



t=0

j=0

T −1
(cid:88)

t=0





t+m−1
(cid:88)

j=0




∞
(cid:88)

T −1
(cid:88)

j=0




(cid:62)

Ψ(p)

j (cid:15)t−j−1



B



t+m−1
(cid:88)



Ψ(p)

j (cid:15)t−j−1





(cid:62)

j=0


∞
(cid:88)

Ψ(p)

j (cid:15)t−j−1



B





Ψ(p)

j (cid:15)t−j−1



t=0

j=t+m

j=t+m

T −1
(cid:88)





t+m−1
(cid:88)



(cid:62)



Ψ(p)

j (cid:15)t−j−1



B





Ψ(p)

j (cid:15)t−j−1



∞
(cid:88)

j=t+m

+

1
T

+

2
T

t=0
j=0
(cid:44)E1 + E2 + E3.

Then we can bound each Ei from its expectation separately, and m will be chosen to be suﬃ-
ciently large later.

(1) Bounding E1 − E(E1)

Let Θ(t) ∈ RpM ×(T +m)M and ˜(cid:15) ∈ R(T +m)M be deﬁned as

(cid:16)

Θ(t) =

Ψ(p)

t+m−1

· · · Ψ(p)
0

0 · · ·

(cid:17)
0

,

Then E1 = ˜(cid:15)(cid:62) (cid:16) 1
T
norm and Frobenius norm of 1
T

(cid:80)T −1

˜(cid:15) =
t=0 Θ(t)(cid:62)BΘ(t)(cid:17)
(cid:80)T −1

t=0 Θ(t)(cid:62)BΘ(t).

(cid:16)

(cid:15)(cid:62)
−m · · ·

(cid:15)(cid:62)
T −1

(cid:17)(cid:62)

.

˜(cid:15), and by Lemma C.4 we only need to bound the operator

i. Bounding

(cid:13)
(cid:13)
(cid:13)

1
T

(cid:80)T −1

t=0 Θ(t)(cid:62)BΘ(t)(cid:13)
(cid:13)
(cid:13)2

65

For any unit vector u, v ∈ R(t+m)M ,

u(cid:62) 1
T

T −1
(cid:88)

t=0

Θ(t)(cid:62)BΘ(t)v =

=

≤

1
T

1
T

1
T

T −1
(cid:88)

t+m
(cid:88)

t=0

i,j=1

u(i)(cid:62)Ψ(p)(cid:62)

t+m−1BΨ(p)

t+m−jv(j)

T +m−1
(cid:88)

i,j=1

T +m−1
(cid:88)

i,j=1





u(i)(cid:62)

T −1
(cid:88)

t=(i∨j−m)∨0

Ψ(p)(cid:62)

t+m−1BΨ(p)

t+m−j


 v(j)

(cid:107)u(i)(cid:107)2(cid:107)v(j)(cid:107)2(cid:107)B(cid:107)2

∞
(cid:88)

l=0

(cid:13)
(cid:13)Ψ(p)
(cid:13)

|i−j|+l

(cid:13)
(cid:13)
(cid:13)2

(cid:13)
(cid:13)Ψ(p)
(cid:13)

l

(cid:13)
(cid:13)
(cid:13)2

,

where u(i) = (u(i−1)M +1, . . . , uiM ). Let ˜αi =
Γij = (cid:80)∞

k=0 ˜α|i−j|+k ˜αk, then

(cid:13)
(cid:13)Ψ(p)
(cid:13)

i

(cid:13)
(cid:13)
(cid:13)2

, and Γ ∈ R(t+m)×(t+m) be deﬁned as

u(cid:62) 1
T

T −1
(cid:88)

t=0

Θ(t)(cid:62)BΘ(t)v ≤

(cid:107)B(cid:107)2
T

((cid:107)u(1)(cid:107)2, . . . , (cid:107)u(t+m)(cid:107)2)Γ

≤

(cid:107)B(cid:107)2Λmax(Γ)
T

.







(cid:107)v(1)(cid:107)2
...
(cid:107)v(t+m)(cid:107)2







Thus we only need to bound Λmax(Γ). Applying Lemma C.5, the largest eigenvalue of
Toeplitz matrix Γ can be bounded by

Λmax(Γ) ≤ess sup
λ

∞
(cid:88)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
l=−∞

∞
(cid:88)

j=0

˜α|l|+j ˜αjeilλ

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤

∞
(cid:88)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
l=−∞

∞
(cid:88)

j=0

∞
(cid:88)

≤2





∞
(cid:88)

l=0

j=0

˜α|l|+j ˜αj

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
1
2 



˜α2

l+j







1
2

˜α2
j



.

∞
(cid:88)

j=0

where the third inequality is due to Cauchey-Schwartz inequality. Due to (75), we can
further obtain

Λmax(Γ) ≤2

∞
(cid:88)





∞
(cid:88)

p−1
(cid:88)

l=0

j=0

n=0



1
2 

α2

l+j−n







1
2

α2

j−n



∞
(cid:88)

p−1
(cid:88)

j=0

n=0

≤2p

(cid:32) ∞
(cid:88)

i=0

α2

1−p+i

(cid:33) 1

2 ∞
(cid:88)

(cid:32) ∞
(cid:88)

l=0

i=0

(cid:33) 1
2

α2

l+1−p+i

≤ C(β).

and we deﬁne αi = 0 when i < 0 for convenience. Therefore,
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

C(cid:107)B(cid:107)2
T

Θ(t)(cid:62)BΘ(t)

T −1
(cid:88)

1
T

≤

t=0

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)2

.

66

ii. Bounding

(cid:13)
(cid:13)
(cid:13)

1
T

(cid:80)T −1

t=0 Θ(t)(cid:62)BΘ(t)(cid:13)
2
(cid:13)
(cid:13)
F

First note that

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
T

T −1
(cid:88)

t=0

Θ(t)(cid:62)BΘ(t)

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
F

≤

1
T 2

T −1
(cid:88)

s,t=0

(cid:12)
(cid:12)
(cid:12)tr

(cid:16)

Θ(s)(cid:62)BΘ(s)Θ(t)(cid:62)BΘ(t)(cid:17)(cid:12)
(cid:12)
(cid:12) ,

and if we write B = P (cid:62)ΛP with orthogonal P and diagonal Λ (since B is symmetric),

(cid:16)

(cid:16)

(cid:12)
(cid:12)
(cid:12)tr
(cid:12)
(cid:12)
(cid:12)tr
≤(cid:107)B(cid:107)tr

=

Θ(s)(cid:62)BΘ(s)Θ(t)(cid:62)BΘ(t)(cid:17)(cid:12)
(cid:12)
(cid:12)

P Θ(s)Θ(t)(cid:62)BΘ(t)Θ(s)(cid:62)P (cid:62)Λ
(cid:13)Θ(s)Θ(t)(cid:62)BΘ(t)Θ(s)(cid:62)(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)2
(cid:13)Θ(s)Θ(t)(cid:62)(cid:13)

(cid:13)
(cid:13)

.

2
(cid:13)
(cid:13)
2

≤(cid:107)B(cid:107)tr(cid:107)B(cid:107)2

(cid:17)(cid:12)
(cid:12)
(cid:12)

Meanwhile, due to that ˜αi =

(cid:13)
(cid:13)Ψ(p)
(cid:13)

i

(cid:13)
(cid:13)
(cid:13)2

and (75),

T −1
(cid:88)

s,t=0

(cid:13)
(cid:13)

(cid:13)Θ(s)Θ(t)(cid:62)(cid:13)

2
(cid:13)
(cid:13)
2

=

≤

=

≤

T −1
(cid:88)

s,t=0

T −1
(cid:88)

t∧s+m
(cid:88)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
i=1
(cid:32)t∧s+m
(cid:88)

s,t=0

i=1

Ψ(p)
t+m−iΨ(p)

s+m−i

˜αt+m−i ˜αs+m−i

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
2
(cid:33)2





(cid:32)

T −1
(cid:88)

s,t=0

T −1
(cid:88)


2

˜αi ˜α|t−s|+i



(t∧s)+m−1
(cid:88)

i=0

∞
(cid:88)

(cid:33) 
p

α2
i

∞
(cid:88)



 .

α2

|t−s|+i

p

Note that (cid:80)∞
i=0

(cid:16)(cid:80)∞

j=0 α2

i+j

s,t=0

i=0

i=1−p

(cid:17) 1

2 ≤ β,

T −1
(cid:88)

s,t=0

(cid:13)
(cid:13)

(cid:13)Θ(s)Θ(t)(cid:62)(cid:13)

2
(cid:13)
(cid:13)
2

≤Cp2

T −1
(cid:88)





∞
(cid:88)

s,t=0

i=1−p





α2

|t−s|+i

≤Cp2

T −1
(cid:88)

l=0

2(T − l)





∞
(cid:88)



α2

l+i



i=1−p

∞
(cid:88)

(cid:32) ∞
(cid:88)

(cid:33)

α2

l+i

≤CT

l=0


∞
(cid:88)

≤CT



i=0

(cid:32) ∞
(cid:88)

2


(cid:33) 1
2

α2

l+i



≤ CT,

l=0

i=0

67

where the fourth line is due to Cauchey-Schwartz inequality. Therefore,

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
T

T −1
(cid:88)

t=0

Θ(t)(cid:62)BΘ(t)

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
F

≤

C(cid:107)B(cid:107)2(cid:107)B(cid:107)tr
T

.

Now we apply Lemma C.4, and arrive at

P (|E1 − E(E1)| > δ) ≤ 2 exp

(cid:26)

−cT min

(cid:26) δ

(cid:107)B(cid:107)2

,

δ2
(cid:107)B(cid:107)2(cid:107)B(cid:107)tr

(cid:27)(cid:27)

.

(2) Bounding E2 − E(E2)

We will show that |E2 − E(E2)| vanishes when m is large enough. First we bound (cid:107)E2(cid:107)ψ1.
Since

|E2| ≤

1
T

T −1
(cid:88)

t=0



(cid:107)B(cid:107)2



∞
(cid:88)

j=t+m



2

˜αj(cid:107)(cid:15)t−j−1(cid:107)2



,

by (75) and (76),

(cid:107)E2(cid:107)ψ1 ≤

2
T

T −1
(cid:88)

t=0



(cid:107)B(cid:107)2



∞
(cid:88)



2

˜αj

(cid:13)
(cid:13)(cid:107)(cid:15)t−j−1(cid:107)2

(cid:13)
(cid:13)ψ2



j=t+m


∞
(cid:88)



T −1
(cid:88)


2

˜αj



≤

CM (cid:107)B(cid:107)2
T

j=t+m

2

˜αj



t=0


∞
(cid:88)

j=m


≤CM (cid:107)B(cid:107)2



≤CM (cid:107)B(cid:107)2p2



∞
(cid:88)



2

αj



.

Meanwhile,

j=m−p

|E(E2)| =

≤

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
T

1
T

T −1
(cid:88)

t=0

T −1
(cid:88)

t=0



tr

B

∞
(cid:88)

j Ψ(p)(cid:62)
Ψ(p)

j

j=t+m

∞
(cid:88)

j=t+m

˜α2
j

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:107)B(cid:107)tr

(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)

≤p(cid:107)B(cid:107)tr

∞
(cid:88)

α2
j .

j=m−p

For any δ > 0, let m be suﬃciently large such that (cid:80)∞
then by tail bound of sub-exponential random variable (see Vershynin [2010]),

j=m−p α2

δ
2p(cid:107)B(cid:107)tr

j <

, (cid:107)E2(cid:107)ψ1 ≤ C(cid:107)B(cid:107)2

,

T

P (|E2 − E(E2)| > δ) ≤ C exp

(cid:26)

−

(cid:27)

.

cδT
(cid:107)B(cid:107)2

68

(3) Bounding E3 − E(E3)
One can show that

|E3| ≤

2(cid:107)B(cid:107)2
T

T −1
(cid:88)

∞
(cid:88)

t=0

j=t+m

˜αj(cid:107)(cid:15)t−j−1(cid:107)2

∞
(cid:88)

j=0

˜αj(cid:107)(cid:15)t−j−1(cid:107)2,

and

Thus

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

∞
(cid:88)

j=n

˜αj(cid:107)(cid:15)t−j−1(cid:107)2

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)ψ2

√

M τ

≤ C

∞
(cid:88)

j=n

√

˜αj ≤ Cp

∞
(cid:88)

αj.

M τ

j=n−p

(cid:107)E3(cid:107)ψ1 ≤

4(cid:107)B(cid:107)2
T

T −1
(cid:88)

t=0

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

∞
(cid:88)

j=t+m


˜αj(cid:107)(cid:15)t−j−1(cid:107)2

√

≤C(cid:107)B(cid:107)2

M pτ



∞
(cid:88)





αj





αj



˜αj(cid:107)(cid:15)t−j−1(cid:107)2

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)ψ2

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

∞
(cid:88)

j=0


(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)ψ2
∞
(cid:88)

j=m−p

j=0

≤C(cid:107)B(cid:107)2

√

M

∞
(cid:88)

αj.

j=m−p

The ﬁrst line is due to the following fact: For any two sub-Gaussian random variables X and
Y , (cid:107)XY (cid:107)ψ1

≤ 2(cid:107)X(cid:107)ψ2(cid:107)Y (cid:107)ψ2. We can prove this in the following:

q−1 (E|XY |q)

sup
q≥1

1
q ≤ sup
q≥1

q−1 (cid:0)E|X|2q(cid:1) 1

2q (cid:0)E|Y |2q(cid:1) 1

2q

≤2 sup
q≥1

q− 1

2 (E|X|q)

q− 1

2 (E|Y |q)

1
q

1
q sup
q≥1

=2(cid:107)X(cid:107)ψ2(cid:107)Y (cid:107)ψ2,

where the ﬁrst line applies Cauchey-Schwartz inequality. Thus, with large enough m, (cid:107)E3(cid:107)ψ1 ≤
(cid:107)B(cid:107)2
T . Also, E(E3) = 0, therefore implies the same bound for E3 − E(E3) as the one for
E2 − E(E2):

P (|E3 − E(E3)| > δ) ≤ C exp

(cid:26)

−

cδT
(cid:107)B(cid:107)2

(cid:27)

.

In conclusion, for any δ > 0, if we choose some m accordingly,
(cid:32)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
t BXt − tr(BΥ)
(cid:12)
(cid:12)

T −1
(cid:88)

X (cid:62)

> δ

1
T

P

t=0

(cid:33)

3
(cid:88)

P

≤

i=1

(cid:18)

|Ei − E(Ei)| >

(cid:19)

δ
3

(cid:26)

≤C exp

−cT min

(cid:26) δ

(cid:107)B(cid:107)2

,

δ2
(cid:107)B(cid:107)2(cid:107)B(cid:107)tr

(cid:27)(cid:27)

.

69

Proof of Lemma A.1. Here we apply some results in Basu et al. [2015] with a little change in no-
(cid:12)v(cid:62)(H − Υ)v(cid:12)
tation. These results simpliﬁes the original problem to ﬁnding a upper bound for (cid:12)
(cid:12)
with any ﬁxed unit vector v. Speciﬁcally, the following lemmas are useful:

Lemma D.4. For any J ⊂ {1, · · · , pM }, and κ > 0,

C(J, κ) ∩ {v ∈ RpM : (cid:107)v(cid:107)2 ≤ 1} ⊂ (κ + 2)cl {conv {K (|J|)}} ,

where K(l) = {v ∈ RpM : (cid:107)v(cid:107)0 ≤ l, (cid:107)v(cid:107)2 ≤ 1} for any positive integer l.

Lemma D.5.

sup
v∈cl{conv(K(l))}

(cid:12)
(cid:12)v(cid:62)Dv
(cid:12)

(cid:12)
(cid:12)
(cid:12) ≤ 3 sup
v∈K(2l)

(cid:12)
(cid:12)v(cid:62)Dv
(cid:12)

(cid:12)
(cid:12)
(cid:12) .

Lemma D.6. Consider a symmetric matrix D ∈ RpM ×pM . If for any vector v ∈ RpM with
(cid:107)v(cid:107)2 ≤ 1, and any η ≥ 0,

P

(cid:16)(cid:12)
(cid:12)v(cid:62)Dv
(cid:12)

(cid:12)
(cid:12)
(cid:12) > η

(cid:17)

≤ c1 exp (cid:8)−c2T min (cid:8)η, η2(cid:9)(cid:9) ,

then for any integer l ≥ 1,

(cid:32)

P

sup
v∈K(l)

(cid:33)

(cid:12)
(cid:12)v(cid:62)Dv
(cid:12)

(cid:12)
(cid:12)
(cid:12) > η

≤ c1 exp (cid:8)−c2T min (cid:8)η, η2(cid:9) + l min {log(pM ), log(21epM/l)}(cid:9) .

By Lemma D.4 and Lemma D.5,

sup

≤ sup

(cid:110)(cid:12)
(cid:12)v(cid:62)(H − Υ)v
(cid:12)
(cid:110)(cid:12)
(cid:12)v(cid:62)(H − Υ)v
(cid:12)

(cid:111)

(cid:12)
(cid:12)
(cid:12) : v ∈ C(J, κ), (cid:107)v(cid:107)2 ≤ 1
(cid:12)
(cid:12)
(cid:12) : v ∈ (κ + 2)cl {conv{K(|J|)}}
(cid:12)
(cid:12)
(cid:12) : v ∈ K(2|J|)

(cid:111)

.

(cid:110)(cid:12)
(cid:12)v(cid:62)(H − Υ)v
(cid:12)

(cid:111)

≤3(κ + 2)2 sup

For any unit vector v ∈ RpM ,

v(cid:62)(H − Υ)v =

1
T

T −1
(cid:88)

t=0

X (cid:62)

t vv(cid:62)Xt − tr

(cid:16)

(cid:17)
vv(cid:62)Υ

,

Thus (cid:12)

(cid:12)v(cid:62)(H − Υ)v(cid:12)

(cid:12) can be bounded by Lemma 5.2.
(cid:13)vv(cid:62)(cid:13)
(cid:13)vv(cid:62)(cid:13)
(cid:13)
(cid:13)
(cid:13)2
(cid:13)tr

(cid:13)
(cid:13)

(cid:13)
(cid:13)

=

= (cid:107)v(cid:107)2

2 = 1,

which implies

(cid:12)
(cid:12)
(cid:12) > η
By Lemma D.6, when |J| log pM ≤ C(η)T ,

(cid:16)(cid:12)
(cid:12)v(cid:62)(H − Υ)v
(cid:12)

P

(cid:17)

≤ c1 exp{−c2T min{η, η2}}.

sup

(cid:110)(cid:12)
(cid:12)v(cid:62)(H − Υ)v
(cid:12)

(cid:12)
(cid:12)
(cid:12) : v ∈ K(2|J|)

(cid:111)

≤ η,

70

with probability at least 1 − c1 exp{−c2T min{η, η2}}. Let η = [6(κ + 2)2]−1Λmin(Υ) ≥ C(κ, β),
then

(cid:110)

inf

v(cid:62)Hv : v ∈ C(J, κ), (cid:107)v(cid:107)2 ≤ 1

(cid:111)

≥Λmin(Υ) − sup

(cid:110)(cid:12)
(cid:12)v(cid:62)(H − Υ)v
(cid:12)

(cid:12)
(cid:111)
(cid:12)
(cid:12) : v ∈ C(J, κ), (cid:107)v(cid:107)2 ≤ 1

≥

1
2

Λmin(Υ) ≥ C(β),

with probability at least 1 − c1 exp{−c2T }, when |J| log pM ≤ C(κ, β)T , and c2 depends on κ
and β. Here we apply Lemma 5.4 to lower bound the eigenvalues of Υ.

E Proof of Lemma D.2, 2.1, A.2, and A.3

Proof of Lemma D.2. Recall that fl(z) = f∗(gl(z)), with f∗(z) = (cid:82) z− 1
and g2(z) = ((cid:107)Z + µ(cid:107)2 − r + 2ε) /ε. In order to bound the partial derivatives of composite func-
tion, we apply the following lemma which is a direct result of Proposition 1 and 2 in Hardy
[2006].

2

−∞ φ(z)dz, g1(z) = ((cid:107)Z + µ(cid:107)2 − r − ε) /ε,

Lemma E.1. Suppose univariate function f and g: Rn → R have derivatives and partial
derivatives of orders up to k, then ∀{i1, . . . , ik} ⊂ {1, . . . , n},

∂k
∂xi1 · · · ∂xik

f (g(x)) =

(cid:88)

f (|π|)(g(x))

π∈Π(k)

(cid:89)

B∈π

∂|B|g(x)
(cid:81)
j∈B ∂xij

,

where Π(k) is the set of partitions for {1, · · · , k}, and B ∈ π is a block in π. Formally,

Π(k) = {{B1, B2, · · · , Bn} : Bi ∩ Bj = ∅, ∪iBi = {1, 2, · · · , k}}.

By Lemma E.1, we can write out the kth order partial derivatives of fl:

∂k
∂zi1 · · · ∂zik

fl(z) =

(cid:88)

f (|π|)
∗

(gl(z))

π∈Π(k)

(cid:89)

B∈π

∂|B|gl(z)
(cid:81)
j∈B ∂zij

.

Moreover, we can also write gl(z) as a composite function ϕl(ψ(z)), with ϕ1(x) =
ϕ2(x) =
2. Then applying Lemma E.1 on gl(z) gives us

, and ψ(z) = (cid:107)z + µ(cid:107)2

x−r+2ε
ε

√

Note that

∂n
∂zi1 · · · ∂zin

gl(z) =

(cid:88)

ϕ(|π|)
l

(ψ(z))

π∈Π(n)

(cid:89)

B∈π

∂|B|ψ(z)
(cid:81)
j∈B ∂zij

.

∂|B|ψ(z)
(cid:81)
j∈B ∂zij

=


zij + µij

1(ij = il)

0

if B = {j} for any j

if B = {j, l} for any j, l

if |B| > 2,

71

√

x−r−ε
ε

,

(77)

which means that we only need to consider the partitions with all blocks of size 1 or 2, when
calculating the partial derivative of gl(z) using (77). Also note that we need partitions for
blocks within an original partition π, we deﬁne the following partition set C(π) for any partition
π = {B1, . . . , Bn} of size n:

C(π) = {∪n

i=1˜πi : ˜πi ∈ Π(Bi)s.t.∀C ∈ ˜πi, |C| ≤ 2} .

This set C(π) include the unions of partitions for each block Bi within π, and each block within
the partition of Bi has size bounded by 2. Let S(˜π) = {i : {i} ∈ ˜π}, and P (˜π) = {{i, j} : {i, j} ∈
˜π}, then the partial derivative of fl(z) can be expanded as

∂k
∂zi1 · · · ∂zik

fl(z) =

(cid:88)

π∈Π(n)
˜π∈C(π)

f |π|)
∗

(gl(z))C(π, ˜π)

Πj∈S(˜π)(zij + µij )Π{j,l}∈P (˜π)1(ij = il)
ε|π|(cid:107)z + µ(cid:107)2|˜π|−|π|

2

,

(78)

where we apply the fact that ϕ(k)

l

(x) = C(k)
εxk− 1

2

. For each ﬁxed π ∈ Π(k) and ˜π ∈ C(π),

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

=

(cid:88)

yi1 · · · yik Πj∈S(˜π)(zij + µij )Π{j,l}∈P (˜π)1(ij = il)

1≤i1,··· ,ik≤d
(cid:16)

y(cid:62)(z + µ)

(cid:17)|S(˜π)|

(cid:107)y(cid:107)2|P (˜π)|
2

(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤ (cid:107)y(cid:107)k

2(cid:107)z + µ(cid:107)|S(˜π)|

2

,

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

then combine this with (78), we have

(cid:88)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
1≤i1,··· ,ik≤d

yi1 · · · yik

∂k
∂zi1 · · · ∂zik

fl(z)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:88)

≤

π∈Π(n)
˜π∈C(π)

f (|π|)
∗

(gl(z))C(π, ˜π)(cid:107)y(cid:107)k
2
ε|π|(cid:107)z + µ(cid:107)k−|π|

.

2

In addition, note that f (k)
2 ) = 0 when x ≤ 0 or x ≥ 1, and is bounded on
(0, 1).Thus we only have to consider (cid:107)z + µ(cid:107)2 > r + ε when l = 1 and (cid:107)z + µ(cid:107)2 > r − 2ε when
l = 2. If r > 3ε and l = 2, (cid:107)z + µ(cid:107)2 > r − 2ε > ε. Therefore,

(x) = φ(k−1)(x − 1

∗

(cid:88)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
1≤i1,··· ,ik≤d

y(i1) · · · y(ik)

∂k
∂z(i1) · · · ∂z(ik)

(cid:12)
(cid:12)
(cid:12)
fl(z)
(cid:12)
(cid:12)
(cid:12)

(cid:88)

≤

(cid:88)

π∈Π(k)

(Si,Pi)|π|

i=1∈C(π)

C(|π|)(cid:107)y(cid:107)k
εk

≤ C(k)ε−k(cid:107)y(cid:107)k.

Proof of Lemma 2.1. Note that

w∗

m = Υ−1
Dc

m,Dc
m

ΥDc

m,Dm = −(Υ−1)Dc

m,Dm

(cid:2)(Υ−1)Dm,Dm

(cid:3)−1

.

72

When A∗ is symmetric, Υ−1 = I − (A∗)2, thus

m = (cid:0)(A∗)2(cid:1)
w∗

Dc

m,Dm

(cid:104)

I − (cid:0)(A∗)2(cid:1)

Dm,Dm

(cid:105)−1

∈ R(M −dm)×dm.

It is clear that

sm = (cid:107)w∗
(cid:12){i : [(A∗)2]i,Dm (cid:54)= 0}(cid:12)

Let Rm = (cid:12)

(cid:12) and Cm = {j : A∗

j,Dm

(cid:54)= 0}, then

m(cid:107)0 ≤dm |{i : (w∗

m)i· (cid:54)= 0}| ≤ dm

(cid:12){i : [(A∗)2]i,Dm (cid:54)= 0}(cid:12)
(cid:12)
(cid:12) .

and

Therefore,

|Cm| ≤ dm max
1≤i≤M

(cid:107)a∗

i (cid:107)0

Rm ⊂ {i : supp(A∗

i·) ∩ Cm (cid:54)= ∅}.

sm ≤ dm|Rm| ≤ dm

(cid:88)

j∈Cm

|supp(A∗

·j)| ≤ d2

m( max
1≤i≤M

(cid:107)a∗

i (cid:107)0)2.

Proof of Lemma A.2. Let Y = (B + ∆)−1, then immediately we have Y B − I = −Y ∆, which
is equivalent to Y − B−1 = −Y ∆B−1. Thus the (cid:96)2 norm of Y − B−1 can be bounded by
(cid:107)Y (cid:107)2(cid:107)∆(cid:107)2(cid:107)B−1(cid:107)2. Moreover, note that (cid:107)Y (cid:107)2 ≤ (cid:107)Y − B−1(cid:107) + (cid:107)B−1(cid:107), we have

(cid:107)Y (cid:107)2 ≤ (cid:107)B−1(cid:107)2 + (cid:107)Y (cid:107)2(cid:107)∆(cid:107)2(cid:107)B−1(cid:107)2,

and rearranging terms gives us

Therefore,

(cid:107)Y (cid:107)2 ≤

(cid:107)B−1(cid:107)2
1 − (cid:107)B−1(cid:107)2(cid:107)∆(cid:107)2

.

(cid:107)Y − B−1(cid:107)2 ≤

(cid:107)B−1(cid:107)2

2(cid:107)∆(cid:107)2

1 − (cid:107)B−1(cid:107)2(cid:107)∆(cid:107)2

.

Proof of Lemma A.3. First note that for any symmetric matrix U , we can write it as U = P (cid:62)ΛP ,
with orthogonal matrix P and diagonal matrix Λ. By the deﬁnition of trace norm,

(cid:107)U (cid:107)tr = tr

(cid:16)√

U 2(cid:17)

= tr

(cid:16)√

P (cid:62)Λ2P

(cid:17)

√

(cid:16)

P (cid:62)

Λ2P

(cid:17)

= tr

(cid:16)√

Λ2(cid:17)

.

= tr

If we denote the non-zero eigenvalues of U as λ1, . . . , λr, then

(cid:107)U (cid:107)tr = tr

(cid:16)√

Λ2(cid:17)

≤ r max

i

|λi| ≤ r(cid:107)U (cid:107)2.

73

