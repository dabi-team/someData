6
1
0
2

t
c
O
1
1

]
T
S
.
h
t
a
m

[

1
v
5
1
2
3
0
.
0
1
6
1
:
v
i
X
r
a

Speciﬁcation testing in nonparametric
AR-ARCH models∗

Marie Huˇskov´a,

Natalie Neumeyer, Tobias Niebuhr, Leonie Selk

Department of Statistics

Department of Mathematics

Charles University of Prague

University of Hamburg

July 11, 2018

Abstract

In this paper an autoregressive time series model with conditional heteroscedasticity is
considered, where both conditional mean and conditional variance function are modeled
nonparametrically. A test for the model assumption of independence of innovations from
past time series values is suggested. The test is based on an weighted L2-distance of
empirical characteristic functions. The asymptotic distribution under the null hypothesis
of independence is derived and consistency against ﬁxed alternatives is shown. A smooth
autoregressive residual bootstrap procedure is suggested and its performance is shown in
a simulation study.

AMS 2010 Classiﬁcation: Primary 62M10, Secondary 62G10
Keywords and Phrases: autoregression, conditional heteroscedasticity, empirical character-

istic function, kernel estimation, nonparametric CHARN model, testing independence

Running title: Nonparametric AR-ARCH models

1

Introduction

Assume we have observations from a one-dimensional stationary weakly dependent time series
Z. Nonparametric modeling avoids misspeciﬁcation problems and thus such models
Xj, j
have gained much attention over the last years, see Fan and Yao (2003) and Gao (2007) for

∈

∗Financial support of the DFG (Research Unit FOR 1735 Structural Inference in Statistics: Adaptation
and Eﬀciency) and GA ˇCR 15-09663S is gratefully acknowledged. Corresponding author: Natalie Neumeyer,
University of Hamburg, Department of Mathematics, Bundesstrasse 55, 20146 Hamburg, Germany, e-mail:
neumeyer@math.uni-hamburg.de

1

 
 
 
 
 
 
extensive overviews. One popular possibility is to analyze data by ﬁtting a nonparametric
AR(1)-ARCH(1)-model (also called CHARN-model), i. e.

Xj = m(Xj−1) + σ(Xj−1)εj,

j

Z,

∈

Xj−1 = x], conditional variance function σ2(x) =
with autoregression function m(x) = E[Xj |
Xj−1 = x), and innovations εj, independent from past time series values Xj−1, Xj−2, . . . .
Var(Xj |
Before applying any procedure developed for a time series model like the one deﬁned, model
assumptions need to be tested. Thus we are interested in testing the hypothesis

H0 : εj and (Xj−1, Xj−2 . . .) are stochastically independent.

Although testing for this model assumption is essential for applications in order to obtain
correct forecasts, it seems that the problem has not been considered before in the literature
for the nonparametric case. The reason is presumably that tests for hypotheses involving the
innovation distribution would typically be based on the empirical distribution function of non-
parametrically estimated innovations (residuals). Only recently, asymptotic results for such
processes in nonparametric autoregressive models are available. M¨uller et al. (2009) consider
the above model in the homoscedastic case with constant σ. They prove an asymptotic ex-
pansion of the empirical process of residuals obtained from local-polynomial estimation of the
autoregression function m. Further, Dette et al. (2009) base a test for the multiplicativity hy-
pothesis m = cσ on the estimated innovation distribution. Selk and Neumeyer (2013) consider
sequential empirical process of residuals and apply it to test for a change-point in the innova-
tion distribution. In order to test an implication of the null hypothesis H0 one could consider,
N, test statistics based on an estimated diﬀerence of the
for some ﬁxed and prespeciﬁed k
joint empirical distribution function of εj and (Xj−1, . . . , Xj−k) and the product of the marginal
distributions. Asymptotic theory could be derived similar to the considerations in M¨uller et al.
(2009), Dette et al. (2009), and Selk and Neumeyer (2013). Note, however, that the assump-
tions for deriving asymptotic distributions of residual-based processes as in the aforementioned
literature are very restrictive. To avoid unnecessarily strong assumptions we follow a diﬀerent
path in the paper at hand and base our test on an estimated weighted L2-distance between the
joint and the marginal characteristic functions of εj and (Xj−1, . . . , Xj−k). In an iid context
a test for independence of errors and covariates in nonparametric regression models based on
residual empirical characteristic functions was suggested by Hl´avka et al. (2011). Relatedly,
in a time series context but for a parametric model Hl´avka et al. (2012) test for a change in
the innovation distribution of a linear autoregression model based on residual empirical char-
acteristic functions. Another motivation for considering the empirical characteristic functions
instead of empirical distribution functions is that in other contexts it has been observed that
those tests inhabit better power properties, e.g., see Hl´avka et al. (2016). A survey of testing
procedures based on empirical characteristic functions is given in Meintanis (2016).

∈

The remainder of the paper is organized as follows. In section 2 we deﬁne our estimators and
the test statistic. In section 3 we state model assumptions and give the asymptotic distribution

2

of the test statistic under the null hypothesis, whereas consistency under ﬁxed alternatives is
discussed in section 4. A bootstrap procedure is suggested in section 5, where also the ﬁnite
sample performance is investigated in a simulation study. Section 6 concludes the paper, while
all proofs are presented in an appendix.

2 The test statistic

Z, considered in
Assume we have observations X−k+1, . . . , Xn from the time series Xj, j
section 1. As test statistic for independence of innovations and past time series values we
consider the weighted L2-distance

∈

Tn = n

ˆϕˆε, ¯Xk(t0, t1, . . . , tk)

ˆϕˆε(t0) ˆϕ ¯Xk(t1, . . . , tk)

2

W (t0, . . . , tk) d(t0, . . . , tk).

−

Here W denotes some weight function fulﬁlling assumption (A8) in Section 3. Furthermore

(cid:12)
(cid:12)

Z

(cid:12)
(cid:12)

ˆϕˆε, ¯Xk(t0, t1 . . . , tk) =

¯wj exp

i

t0 ˆεj +

tνXj−ν

n

k

j=1
X
estimates the joint characteristic function of εj and ¯Xk,j = (Xj−1, . . . , Xj−k), whereas

ν=1
X

!!

n

ˆϕˆε(t) =

¯wj exp (itˆεj) ,

j=1
X
n

k

ˆϕ ¯Xk(t1, . . . , tk) =

¯wj exp

i

tνXj−ν

j=1
X

ν=1
X

!

→

P

[0, 1] which vanish outside [

estimate the marginal characteristic functions of εj and ¯Xk,j, respectively. Here the weights
n
l=1 wn(Xl−1)), where we choose a weight function wn(x) =
are deﬁned as ¯wj = wn(Xj−1)/(
. Here and throughout IA denotes the indicator function
I[−an,an](x) for some sequence an → ∞
of set A. Other weight functions wn : R
an, an] are possible as
well but require slightly adapted assumptions. The weights are included in the deﬁnition of the
empirical characteristic functions to avoid problems of kernel estimation in areas where only
ˆm(Xj−1))/ˆσ(Xj−1)
few data are available. Furthermore the residuals are deﬁned as ˆεj = (Xj −
and we use Nadaraya-Watson type estimators for the conditional mean and variance functions,
j=1 K( x−Xj−1
ˆfX (x)
j=1 K( x−Xj−1
cn
ˆfX (x)

)(Xj −

ˆσ2(x) =

ˆm(x) =

ˆm(x))2

P
n

1
ncn

1
ncn

)Xj

−

cn

n

P

with kernel function K and sequence of bandwidths cn, n

N. Here

ˆfX (x) =

1
ncn

n

x

K

j=1
X

(cid:16)

3

∈
Xj−1
cn

−

(cid:17)

 
 
 
denotes a kernel estimator for the marginal density fX of Xj. See, e. g., Robinson (1983), Masry
and Tjøstheim (1995), H¨ardle and Tsybakov (1997) and Hansen (2008) for properties of these
estimators in the time series context.

3 Assumptions and asymptotic results under the null

hypothesis

Under the null hypothesis we state the following assumptions. Please note that throughout we
write t = (t0, t1, . . . , tk) and use the notation g(t) for simplicity also for functions g that only
depend on (t1, . . . , tk) (see e. g. ψ(t, x) from assumption (A4)).

(A1) The process (Xt)t∈Z is strictly stationary and α-mixing with mixing coeﬃcient α that
for some q > 0, where s > 2

and β > 1+(s−1)(2+1/q)

satisﬁes α(i)
≤
s <
and E
X1 has bounded marginal density fX such that for some constant B1,

Ai−β for some A <
.

X0|

∞

∞

s−2

|

sup
x

E(

s

X1|

|

|

X0 = x)fX (x)

B1.

≤

Furthermore (X0, Xj) has bounded joint density fj and there exists a constant B2, such
that for some j∗,

sup
x0,xj

E(

X1Xj+1|

|

X0 = x0, Xj = xj)fj(x0, xj)

B2

≤

for all j

j∗.

≥

(A2) Let m, σ2 and fX be diﬀerentiable. Let there exist some r
σ2 , 1
fX

) such that the
X are of order O((log n)r) uniformly on the interval
Ccn, an + Ccn] (with C from assumption (A5)). Further we assume Lipschitz
X, m′ and (σ2)′ in the following sense,

functions m, m′, σ2, (σ2)′, 1
In = [
continuity of the derivatives f ′

an −
−

and f ′

(0,

∞

∈

sup
x,y∈In
|x−y|≤cn

|

g(x)

g(y)

|

−

= O(cn(log n)r) for g

X, m′, (σ2)′
f ′

.

}

∈ {

(A3) The innovations (εt)t∈Z are independent, centered and identically distributed. For each

Z, εt is independent from the past Xt−1, Xt−2, . . . .

t

∈

For some δ > 2
ε1|
uniformly in j with r and In from assumption (A2).

β−2 let E[
|

and supx∈In E[
|

(2+2δ)∨4] <

∞

2(1+δ)

εj|

|

X0 = x] = O((log n)r)

(A4) Deﬁne ψ(t, x) = E[Y1(t)

that

X0 = x]

|

−

E[Y1(t)] with Y1(t) = cos(

k
ν=1 tνX1−ν) and assume

sup
x,z∈In
|x−z|≤Ccn

sup
t

|

ψ(t, x)

ψ(t, z)

|

−

4

P
= O((log n)rcd
n)

for some d > 0 with r from assumption (A2) and C from assumption (A5). As-
sume the same condition holds for ˜ψ(t, x) = E[Z1(t)
E[Z1(t)] with Z1(t) =
sin(

X0 = x]

−

|

k
ν=1 tνX1−ν).

(A5) The kernel K is a symmetric and Lipschitz continuous density with compact support

P

C, C] and

[
−

K(u)u du = 0.

(A6) For q, s and β from (A1) we have an = O(n1/(2q) log n), and for θ =

R

that log n = o(nθcn). Let

β−2− 1

q − 1+β
s−1
β+2− 1+β
s−1

it holds

then a∗

n = O(∆nn−1/4) with ∆n = inf |x|≤an fX (x).

a∗
n =

1/2

log n
ncn (cid:19)

(cid:18)

+ c2
n,

(A7) Let the sequence of bandwidths fulﬁll nc2

n(log n)−D

, nc4

n(log n)D

→

→ ∞

0 for all D > 0.

(A8) The weight function W is nonnegative and symmetric such that W (

= W (t0, . . . , tk). Further

t4
0W (t0, . . . , tk)d(t0 . . . , tk) <

.

∞

t0,

±

t1, . . . ,

±

tk)

±

R

Remark 3.1 Apart from the typical assumptions on the kernel, bandwidths and weight func-
tions we need smoothness assumptions on the unkown functions as well as moment assumptions
and the mixing property, e. g. in order to obtain uniform rates of convergence for the kernel
estimators, similar to Hansen (2008). Note that for (A6) and (A7) both to be satisﬁed one
needs θ > 1
4.

We have the following asymptotic distribution of the test statistic under the null.

Theorem 3.2 Under the assumptions (A1)–(A8) the test statistic Tn converges in distribution
Rk+1, denotes a centered Gaussian process with the
to T =
same covariance structure as

Rk+1 S2(t)W (t) dt, where S(t), t

∈

R

˜S(t0, . . . , tk)

(cid:16)
+

(cid:16)
+ t0

−

=

cos(t0ε1)

E

cos(t0ε1)

Y1(t) + Z1(t)

E[Y1(t) + Z1(t)]

−
sin(t0ε1)

(cid:2)
E[sin(t0ε1)]
−

(cid:3)(cid:17)(cid:16)

Y1(t)

−

−
Z1(t)

−

E[Y1(t)

−

(cid:17)
Z1(t)]

1
2
1
2

(ε2

(cid:17)(cid:16)
1 −

(ε2

1 −

ε1E[sin(t0ε1)] +

1)E[sin(t0ε1)ε1]

(cid:17)
E[Y1(t) + Z1(t)
|

X0]

−

E[Y1(t) + Z1(t)]

(cid:16)

t0

ε1E[cos(t0ε1)] +

(cid:17)(cid:16)

1)E[cos(t0ε1)ε1]

E[Y1(t)

Z1(t)

X0]

E[Y1(t)

−

|

−

−

(cid:17)
Z1(t)]

.

(cid:16)

(cid:17)
The proof is given in the appendix. An asymptotic level-α test is obtained by rejecting H0
whenever Tn > c1−α, where P (T > c1−α) = α. Due to the complicated distribution of T we
suggest a bootstrap procedure to estimate the critical value c1−α in section 5.

(cid:17)(cid:16)

5

Remarks 3.3 (a) The replacement of true but unknown innovations εj by the estimated resid-
uals ˆεj changes the asymptotic distribution drastically. Were the true innovations known and
used in the test statistic instead of residuals the statistic ˜S in Theorem 3.2 would simplify to

˜S(t0, . . . , tk) =

cos(t0ε1)

E

cos(t0ε1)

Y1(t) + Z1(t)

E[Y1(t) + Z1(t)]

−
sin(t0ε1)

(cid:16)
+

(cid:16)

(cid:2)
E[sin(t0ε1)]
−

(cid:3)(cid:17)(cid:16)

Y1(t)

−

(cid:17)(cid:16)

E[Y1(t)

−

−

(cid:17)
Z1(t)]

.

(cid:17)

−
Z1(t)

(b) If the aim is to test for independence of innovations and past time series values in a
(homoscedastic) AR(1) model

Xj = m(Xj−1) + εj,

1 in the deﬁnition of the residuals. Then the statistic ˜S in Theorem 3.2

one simply sets ˆσ
changes to

≡

˜S(t0, . . . , tk) =

cos(t0ε1)

E

cos(t0ε1)

Y1(t) + Z1(t)

E[Y1(t) + Z1(t)]

+ t0ε1E[sin(t0ε1)]

E[Y1(t) + Z1(t)

E[Y1(t) + Z1(t)]

−
sin(t0ε1)

(cid:16)
+

(cid:2)
E[sin(t0ε1)]
−

(cid:3)(cid:17)(cid:16)

Y1(t)

(cid:16)

−

(cid:16)
t0ε1E[cos(t0ε1)]

E[Y1(t)

(cid:16)

−
Z1(t)

−
X0]

E[Y1(t)

−

−

(cid:17)
Z1(t)]

(cid:17)

(cid:17)(cid:16)

−

|
Z1(t)

−

−

X0]

|

E[Y1(t)

−

(cid:17)
Z1(t)]

.

(cid:17)

(c) As mentioned in the introduction alternative testing procedures would be given by, e. g.,
Kolmogorov-Smirnov or Cram´er-von Mises type statistics based on the ˆFˆε, ¯Xk −
F ¯Xk , i. e. the
weighted empirical joint distribution function of ˆεj and ¯Xk,j = (Xj−1, . . . , Xj−k) (j = 1, . . . , n)
and the product of the marginals. Following M¨uller et al. (2009), Dette et al. (2009), and Selk
and Neumeyer (2013) to derive the asymptotic distribution would, however, require stronger
assumptions on the data generating process.

Fˆε ⊗

4 Fixed alternatives

Note that by construction the test statistic Tn cannot detect alternatives where the innovation
εj is independent of (Xj−1, . . . , Xj−k), but depends on some Xj−ℓ for ℓ > k. However, the test
is consistent against any ﬁxed alternative

H1 : εj and Xj−ℓ are stochastically dependent for some ℓ

1, . . . , k

}

∈ {

under the following model. Assume that (Xj)j∈Z is a strictly stationary and weakly dependent
Xj = x] and
time series that fulﬁlls assumption (A1). Further deﬁne m(x) = E[Xj+1 |
Xj = x). Let m, σ2 and the marginal density fX fulﬁll assumption (A2).
σ2(x) = Var(Xj+1 |
Let the kernel, weight function and sequence of bandwidth fulﬁll (A5)–(A8). Then we have
the following result.

6

Theorem 4.1 Under the assumptions listed in this section, Tn/n converges to

˜T =

Z

(cid:12)
(cid:12)

ϕε, ¯Xk(t0, t1, . . . , tk)

−

ϕε(t0)ϕ ¯Xk(t1, . . . , tk)

2 W (t0, . . . , tk) d(t0, . . . , tk)

in probability, where ϕε, ¯Xk is the joint characteristic function of εj and (Xj−1, . . . , Xj−k), and
ϕε and ϕ ¯Xk are the corresponding marginal characteristic functions.

(cid:12)
(cid:12)

The proof is given in the appendix. Note that under H1 one has ˜T > 0 and hence Tn −→ ∞
→ ∞

.

for n

From rejection of H0 one should conclude that the AR(1)-ARCH(1) model is not suitable

to describe the data. Possible reasons are explained in the following example.

Example 4.2 (a) Consider the conditional distribution of εj, given Xj−1. The ﬁrst two mo-
ments of this distribution do not depend on Xj−1 by construction. Higher order moments could
depend on Xj−1, i. e. E[εℓ
3. In the simulation study we
will consider a skew normal innovation distribution with mean zero, variance one and skewness
dependent on Xj−1.

Xj−1] = hℓ(Xj−1) for some ℓ

j |

≥

(b) The conditional distribution of εj, given ¯Xk = (Xj−1, . . . , Xj−k) may still depend on ¯Xk.
If this distribution does still depend on the ﬁrst component Xj−1, but only on this component,
modeling the autoregression and conditional variance function with lag 1 is appropriate, but one
should not apply any procedures that assume independence of innovations and past time series
values.

(c) An AR(ℓ)-ARCH(ℓ) model could be appropriate for the data for some ℓ > 1, i. e.

Xj = ˜m(Xj−1, . . . , Xj−ℓ) + ˜σ(Xj−1, . . . , Xj−ℓ)ηj

with innovations ηj independent from Xj−1, Xj−2, . . . .

5 Bootstrap and ﬁnite sample performance

In this section we investigate the ﬁnite-sample performance of our test by simulations. Due
to the complicated limiting distribution of T from Theorem 3.2, we suggest to use a smooth
autoregressive residual bootstrap instead. Our bootstrap strategy is as follows.
Firstly, based on the estimators as introduced in section 2, generate bootstrap innovations ε∗
j
from a smooth estimate of the innovation distribution, i. e. given the original data X−k+1, . . . , Xn
the distribution of ε∗

j reads

Fn(x) =

x

˜εi

−
hn

L

(cid:16)

(cid:17)

1
n

n

i=1
X
7

where hn denotes a positive bandwidth, L is some smooth distribution function and ˜ε1, . . . , ˜εn
denote the standardized versions of the residuals ˆε1, . . . , ˆεn. Secondly, compute the bootstrap
process via

X ∗

j−1)ε∗

j−1) + ˆσ(X ∗

j , j = 1, . . . , n,

j = ˆm(X ∗
0 and a suﬃciently large number of forerunnings to ensure the pro-

with some starting value X ∗
cess is in balance. Thirdly, calculate the bootstrap analogue of the test statistic Tn, say T ∗
n .
Frequent repetitions of these steps give the distribution of T ∗
bution of Tn. By using the 1
of independence then is rejected if Tn > c∗
X−k+1, . . . , Xn, the bootstrap innovations ε∗
bootstrap data fulﬁlls the null hypothesis.
The simulations are restricted to the hypothesis ’H0: εj and Xj−1 are stochastically indepen-
dent’, i.e. only the case k = 1 is investigated. To examine the performance of the test for ﬁnite
sample sizes, we consider the following two AR-ARCH models:

n which approximates the distri-
1−α, the hypothesis
1−α. It is worth noting that, given the original data
j−2, . . . and thus the

α percentile of the distribution of T ∗

j are independent of X ∗

n , say c∗

j−1, X ∗

−

(i) m(x) = 0.9x, σ(x)

1,

(ii) m(x)

≡

≡

0, σ(x) = √1 + 0.25x2.

Obviously, model (i) corresponds to an AR series and model (ii) represents an ARCH model.
For both models, the performance under the null and under the alternative is investigated.
We distinguish the null from the alternative by the choice of the innovation sequence. Under
the null we use standard normally distributed innovations. Under the alternative we choose
standardized skew normally distributed innovations, where the skewness parameter depends on
past time series values. In particular the skewness parameter of εt+1 was set to 10X 2
t for all
relevant time points t, according to the notation of Fern´andez and Steel (1998).

Table 1: Rejection probabilities for the AR model (i) under the null hypothesis (left) and the under the alternative (right).

α = 0.01 α = 0.05 α = 0.1

n = 50

n = 100

n = 200

n = 300

n = 400

0.0000

0.0000

0.0005

0.0000

0.0000

0.0275

0.0475

0.0125

0.0225

0.0175

0.0425

0.0200

0.0375

0.0125

0.0425

α = 0.01 α = 0.05 α = 0.1

n = 50

n = 100

n = 200

n = 300

n = 400

0.0150

0.0450

0.1550

0.5100

0.8350

0.0600

0.1125

0.1375

0.2200

0.3225

0.5200

0.7700

0.8675

0.9525

0.9900

Table 2: Rejection probabilities for the ARCH model (ii) under the null hypothesis (left) and under the alternative (right).

α = 0.01 α = 0.05 α = 0.1

n = 50

n = 100

n = 200

n = 300

n = 400

0.0075

0.0000

0.0150

0.0025

0.0075

0.0150

0.0225

0.0125

0.0325

0.0325

0.0700

0.0225

0.0625

0.0475

0.0825

α = 0.01 α = 0.05 α = 0.1

n = 50

n = 100

n = 200

n = 300

n = 400

0.0325

0.1000

0.3125

0.4900

0.5575

0.0900

0.1525

0.2125

0.2775

0.4750

0.5275

0.5750

0.6375

0.6025

0.6450

Tables 1 and 2 state the rejection probabilities for 400 Monte Carlo simulations each with
400 bootstrap repetitions for several sample sizes n and signiﬁcance levels α. We chose L as
the standard normal distribution, hn was set to n−1/4 for reasons given in Neumeyer (2006),

8

and the bandwidth cn was chosen by Silverman’s rule by thumb, see Silverman (1986).
The tables show that under the null hypothesis the test yields the given level of signiﬁcance.
While for model (ii) the test performance is very likely, for model (i) the test seems to be some-
how over-conservative for the sample sizes used. Under the alternative the test power increases
with increasing sample size in both models as to be expected. It is worth to note that the test
power increases faster for model (i) than for model (ii). Altogether, the procedure performs
satisfying in our simulations, however, it has to be noticed that the test performance depends
on the time series at hand.
For practitioners the computation of the test statistic Tn, and T ∗
n respectively, might be chal-
lenging. For that reason, we suppose using another representation of Tn, and T ∗
n , which avoids
for solving complicated integrals. The alternative representation is stated in the following
lemma.

Lemma 5.1 Let
tion (A8) and if W yields W (t0, . . . , tk) = V0(t0)
and the test statistic Tn can be represented by

[V ](x) denote the Fourier transformation of V at point x. Under Assump-
cos(tx)V (t)dt

k
i=1 Vi(ti), it holds

[V ](x) =

F

F

Q

n

k

¯ws3 ¯ws4

R

Xs4−j)

n

Tn =n

s1,s2=1
X

n

¯ws1 ¯ws2F

[V0](ˆεs1 −

ˆεs2)

[Vj](Xs3−j −

F

s3,s4=1
X
k

j=1
Y
[Vj](Xs1−j −

F

[V0](ˆεs1 −

ˆεs2)

Xs2−j)

¯ws1 ¯ws2F

+ n

s1,s2=1
X

n

2n

−

s1,s2,s3=1
X

¯ws1 ¯ws2 ¯ws3F

j=1
Y
[V0](ˆεs1 −

ˆεs2)

k

j=1
Y

[Vj](Xs1−j −

F

Xs3−j).

Since the choice of the weighting function W belongs to the user, the additional assumption
on its multiplicative form is very weak. If one further chooses W such that the Fourier trans-
formations of the corresponding functions Vi, i = 0, . . . , k, are known, the test statistic Tn can
straightforwardly be computed. Even more important, the implementation then simpliﬁes a lot
since the computation of the (k + 1)-fold integral is omitted.

Example 5.2 Some choices of W fulﬁlling the assumptions of the lemma are:

(a) W (t0, . . . , tk) = e−γ0|t0|

j = 0, . . . , k, is given by

k
j=1 e−γj |tj|, where the Fourier transformation of Vj(tj) := e−γj |tj |,
[Vj](tj) = 2γj

j +4π2x2 .
γ2

Q

F

(b) W (t0, . . . , tk) = e−γ0t2
0, . . . , k, is given by

0

F

k

j=1 e−γj t2
[Vj](x) =
Q

e−(πx)2/γj .

π
γj

q

9

j , where the Fourier transformation of Vj := e−γj t2

j , j =

6 Concluding remarks and outlook

In this paper we suggested a test for independence of innovations and past time series ob-
servations in an AR-ARCH model, where both the conditional mean and conditional volatility
function are modeled nonparametrically. The test is based on empirical characteristic functions.
For simplicity of presentation we considered the AR(1)-ARCH(1) case. However, generaliza-
tions to AR(p)-ARCH(p) models are straightforward, while then local polynomial estimators
for the mean and variance function should be used. Facing the curse of dimensionality also
semiparametric models might be of interest, see e. g. Yang et al. (1999) for a model with an
additive autoregression function and multiplicative volatility function. Including covariates is
possible as well. Then one considers a model of type Xj = m(Tj) + σ(Tj)εj, where the vector
Tj may include past observations. Testing independence of εj from Tj, Tj−1, . . . would be of
interest here and can be conducted in an analogous manner.

A question related to the one considered in the paper at hand is whether the innovations
really form an iid sequence. Corresponding tests for parametric times series models have been
considered by Ghoudi et al. (2001), among others. Presumably with the methods developed in
the paper at hand, such hypotheses tests for nonparametric time series models can be derived.
We leave the consideration for future research.

References

Dette, H., Pardo-Fern´andez, J. C. & Van Keilegom, I. (2009). Goodness-of-Fit Tests for

Multiplicative Models with Dependent Data. Scand. J. Statist. 36, 782–799.

Fan, J. & Yao, Q. (2003). Nonlinear Time Series: Nonparametric and Parametric Methods.

Springer Series in Statistics, New York.

Fern´andez, C. and Steel, F.J. (1998). On Bayesian modeling of fat tails and skewness. J.

American Stat. Assoc. 93, 259–371.

Gao, J. (2007). Nonlinear Time Series: Semiparametric and Nonparametric Methods. Chap-

man & Hall/CRC, Boca Raton.

Ghoudi, K., Kulperger, R. J. & R´emillard, B. (2001). A Nonparametric Test of Serial Inde-

pendence for Time Series and Residuals. J. Multivariate Anal. 79, 191–218.

Hansen, B.E. (2008). Uniform Convergence Rates for Kernel Estimation with Dependent Data.

Econom. Theory 24, 726-748.

H¨ardle, W. & Tsybakov, A. (1997). Local polynomial estimators of the volatility function in

nonparametric autoregression. J. Econometrics 81, 223-242.

10

Hl´avka, Z., Huˇskov´a, M., Kirch, C. & Meintanis, S. G. (2012). Monitoring changes in the
error distribution of autoregressive models based on Fourier methods. TEST 21, 605–634.

Hl´avka, Z., Huˇskov´a, M., Kirch, C. & Meintanis, S. G. (2014). Fourier-Type Tests Involving

Margingale Diﬀerence Processes. Econometric Reviews.
DOI: 10.1080/07474938.2014.977074

Hl´avka, Z., Huˇskov´a, M., Kirch, C. & Meintanis, S. G. (2016). Bootstrap procedures for on-line
monitoring of changes in autoregressive models. Commun. Stat. Simul. Comput. 45,
2471–2490.

Hl´avka, Z., Huˇskov´a, M. & Meintanis, S. G. (2011). Tests for independence in non-parametric

heteroscedastic regression models. J. Multivariate Anal. 102, 816–827.

Ibragimov, I.A. & Khasminskii, R.Z. (1981). Statistical Estimation: Asymptotic Theory.

Springer, New York.

Masry, E. & Tjøstheim, D. (1995). Nonparametric estimation and identiﬁcation of nonlinear

ARCH time series. Econometric Theory 11, 258–289.

Meintanis, S. G. (2016). A review of testing procedures based on the empirical characteristic

function. South African Statistical Journal 50, 1–14.

M¨uller, U. U., Schick, A. & Wefelmeyer, W. (2009). Estimating the innovation distribution in

nonparametric autoregression. Probab. Theory Relat. Fields 144, 53–77.

Neumeyer, N. (2009). Smooth residual bootstrap for empirical processes of nonparametric

regression residuals. Scand. J. Statist. 36, 204–228.

Robinson, P. M. (1983). Nonparametric estimators for time series. J. Time Ser. Anal. 4,

185-207.

Selk, L. & Neumeyer, N. (2013). Testing for a change of the innovation distribution in non-
parametric autoregression - the sequential empirical process approach. Scand. J. Statist.
40, 770–788

Silverman, B.W. (1986). Density estimation for statistics and data analysis. Chapman and

Hall, New York.

Su, L. & Xiao, Z. (2008). Testing structural change in time-series nonparametric regression

models. Statistics and Its Interface. Vol. 1, 347-366.

Sun, S. & Chiang, C-Y. (1997). Limiting behavior of the perturbed empirical distribution
functions evaluated at U-statistics for strongly mixing sequences of random variables. J.
Appl. Math. Stoch. Anal. 10, 3–20.

11

Yang, L., H¨ardle, W. & Nielsen, J. P. (1999). Nonparametric autoregression with multiplicative

volatility and additive mean. J. Time Ser. Anal. 20, 579–604.

Yokoyama, R. (1980). Moment Bounds for Stationary Mixing Sequences. Z. Wahrschein-

lichkeitstheorie verw. Gebiete 52, 45–57.

A Proofs: main results

Throughout the proof D denotes some generic positive constant, independent of t, that may
diﬀer from line to line.

Proof of Theorem 3.2.
Note that for the test statistic we have

n

k

n

n

k

Tn = n

¯wj exp

i

t0 ˆεj +

tνXj−ν

Z (cid:12)
j=1
X
(cid:12)
(cid:12)
×

(cid:16)

(cid:16)
W (t0, . . . , tk) d(t0, . . . , tk)
n

k

ν=1
X

−

(cid:17)(cid:17)

j=1
X

Xℓ=1

¯wj ¯wℓ exp

i

t0 ˆεj +

tνXℓ−ν

(cid:16)

(cid:16)

ν=1
X

n

n

k

= n

¯wj cos

t0 ˆεj +

tνXj−ν

Z h(cid:16)

j=1
X
n

(cid:16)

ν=1
X
k

−

(cid:17)

j=1
X
n

Xℓ=1
n

¯wj ¯wℓ cos

t0 ˆεj +

tνXℓ−ν

(cid:16)

ν=1
X
k

¯wj sin

t0 ˆεj +

tνXj−ν

j=1
X

(cid:16)
(cid:16)
W (t0, . . . , tk) d(t0, . . . , tk)

ν=1
X

+

×

−

(cid:17)

j=1
X

Xℓ=1

¯wj ¯wℓ sin

t0 ˆεj +

tνXℓ−ν

(cid:16)

ν=1
X

and with the addition theorems for trigonometric functions one obtains

n

k

n

k

Tn = n

Z (
h

j=1
X
n

¯wj cos(t0 ˆεj)

cos

tνXj−ν

(cid:16)

(cid:16)

ν=1
X
k

−

(cid:17)

Xℓ=1
n

ν=1
(cid:16)
X
k

¯wℓ cos

tνXℓ−ν

2

(cid:17)(cid:17)(cid:12)
(cid:12)
(cid:12)

2

(cid:17)(cid:17)

2

(cid:17)(cid:17)

i

(cid:17)(cid:17)
2

(cid:17)(cid:17)i

−

¯wj sin(t0 ˆεj)

sin

tνXj−ν

j=1
X
n

(cid:16)

(cid:16)

ν=1
X

k

−

(cid:17)

¯wℓ sin

tνXℓ−ν

Xℓ=1

n

(cid:16)

ν=1
X

k

¯wj sin(t0 ˆεj)

cos

tνXj−ν

¯wℓ cos

tνXℓ−ν

+

+

h

j=1
X
n

(cid:16)

(cid:16)

ν=1
X
k

¯wj cos(t0 ˆεj)

sin

tνXj−ν

W (t0, . . . , tk) d(t0, . . . , tk).

(cid:16)

(cid:16)

ν=1
X

j=1
X

×

−

(cid:17)

Xℓ=1
n

ν=1
(cid:16)
X
k

−

(cid:17)

Xℓ=1

¯wℓ sin

tνXℓ−ν

(cid:16)

ν=1
X

(cid:17)(cid:17)
2

)

(cid:17)(cid:17)i

From assumption (A8) by symmetry properties of cosine and sine we obtain

Tn =

(Sn(t))2W (t) dt,

Z

12

where

n

Sn(t) = √n

¯wj cos(t0 ˆεj)

j=1
X

n

k

cos

"

n

(cid:16)

ν=1
X

k

tνXj−ν

+ sin

tνXj−ν

(cid:17)

k

(cid:16)

ν=1
X

(cid:17)

k

¯wℓ

cos

tνXℓ−ν

+ sin

tνXℓ−ν

−

Xℓ=1

(cid:16)

k

(cid:16)

ν=1
X

#

(cid:17)(cid:17)

(cid:17)

sin

(cid:16)

ν=1
X

k

tνXj−ν

(cid:16)

ν=1
X

(cid:17)

k

−

(cid:17)

+ √n

¯wj sin(t0 ˆεj)

j=1
X

cos

tνXj−ν

(cid:16)

ν=1
X

k

"

n

−

¯wℓ

cos

tνXℓ−ν

Xℓ=1

(cid:16)

(cid:16)

ν=1
X

−

(cid:17)

sin

tνXℓ−ν

(cid:16)

ν=1
X

.
#

(cid:17)(cid:17)

For simplicity for the moment we consider only

S(1)
n (t) = √n

n

j=1
X

k

n

k

¯wj cos(t0 ˆεj)

cos

"

tνXj−ν

−

(cid:17)

Xℓ=1

(cid:16)

ν=1
X

¯wℓ cos

tνXℓ−ν

(cid:16)

ν=1
X

.
#

(cid:17)

By a second order Taylor expansion for

cos (t0 ˆεj) = cos

t0

εj + εj

(cid:18)

(cid:18)

σ

ˆσ

−
ˆσ

(Xj−1) +

ˆm

m

−
ˆσ

(Xj−1)

(cid:19)(cid:19)

and introducing the notations

ˆκn =

1
n

n

wn(Xi−1)

i=1
X

k

Yj(t) = cos

tνXj−ν

,

j = 1, . . . , n,

(A.1)

we obtain the expansion S(1)

n = S(1,1)

ν=1
(cid:16)
X
n + S(1,2)

n

(cid:17)
2S(1,3)

n

1

−

, where

S(1,1)
n

(t) =

=

S(1,2)
n

(t) =

1
ˆκn

1
√n

1
ˆκn

1
√n

1
ˆκn

1
√n

n

j=1
X
n

j=1
X
n

j=1
X

wn(Xj−1) cos(t0εj)

Yj(t)

(cid:16)

1
ˆκn

1
n

−

n

Xℓ=1

wn(Xℓ−1)Yℓ(t)

wn(Xj−1)

cos(t0εj)

(cid:16)

wn(Xj−1) sin(t0εj)t0

−

(cid:18)

E[cos(t0εj)]

Yj(t)

(cid:17)(cid:16)

−

ˆm

m

−
ˆσ

(Xj−1) + εj

ˆσ

σ

−
ˆσ

(cid:17)
n

1
ˆκn

1
n

Xℓ=1
(Xj−1)

wn(Xℓ−1)Yℓ(t)

(cid:17)

(cid:19)

×

1
ˆκn

S(1,3)
n

(t) =

1
√n

Yj(t)
h

n

−

1
ˆκn

1
n

n

Xℓ=1

wn(Xℓ−1)Yℓ(t)

wn(Xj−1) cos(t0ξj)t2
0

j=1
X

(Xj−1) + εj

ˆσ

σ

−
ˆσ

(Xj−1)

2

(cid:19)

i
ˆm

−
ˆσ

m

(cid:18)

13

×

Yj(t)

(cid:16)

1
ˆκn

1
n

−

n

Xℓ=1

wn(Xℓ−1)Yℓ(t)

(cid:17)

(with ξj between εj and ˆεj, j = 1, . . . , n). The last term is negligible because

(S(1,3)
n

(t))2W (t) dt

Z

t4
0W (t) dt

≤

Z

1
n

(cid:16)

n

i=1
X

ε2
i + 1

2

nOP

(cid:17)

(cid:16)(cid:16)

a∗
n
∆n

4

(cid:17)

(cid:17)

= oP (1)

by assumptions (A3) and (A6), Proposition B.1 and (B.1). Lemmata B.2, B.3 and B.4
give further expansions of S(1,1)
˜S(1)
n (t))2W (t) dt = oP (1), where

. With this we obtain altogether that

and S(1,2)

(S(1)

n (t)

−

n

n

R

˜S(1)
n (t) =

1
√n

n

wn(Xj−1)

j=1 "
X

cos(t0εj)

−

(cid:16)

E[cos(t0εj)]

(Yj(t)

(cid:17)

E[Yj(t)])

−

1
2

(ε2

j −

+ t0

(cid:16)

1)E[sin(t0ε1)ε1] + εjE[sin(t0ε1)]

E[Yj(t)

(cid:17)(cid:16)

Xj−1]

|

−

E[Yj(t)]

.

#

(cid:17)

Analogously it follows that Sn = ˜Sn + Rn, where

R2

n(t)W (t) dt = oP (1) and

R

E

cos(t0εj)

Yj(t) + Zj(t)

−

E[Yj(t) + Zj(t)]

(cid:17)

˜Sn(t)
1
√n

=

+

(cid:16)
+ t0

n

wn(Xj−1)

j=1
X
sin(t0εj)

cos(t0εj)

−

"

(cid:16)

(cid:2)
Yj(t)

(ε2

(cid:17)(cid:16)
j −

1
2
1
2

(cid:16)

t0

εjE[cos(t0ε1)] +

(cid:16)

−

with

E[sin(t0εj)]

−

Zj(t)

−

−

E[Yj(t)

Zj(t)]

−

εjE[sin(t0ε1)] +

1)E[sin(t0ε1)ε1]

(cid:17)
E[Yj(t) + Zj(t)
|

Xj−1]

−

E[Yj(t) + Zj(t)]

(ε2

j −

1)E[cos(t0ε1)ε1]

E[Yj(t)

(cid:17)(cid:16)

k

Zj(t)

|

−

Xj−1]

−

E[Yj(t)

−

(cid:17)
Zj(t)]

,
#

(cid:17)

(cid:3)(cid:17)(cid:16)

(cid:17)(cid:16)

Zj(t) = sin

tνXj−ν

j = 1, . . . , n.

To ﬁnish the proof of Theorem 3.2 we apply Theorem 22 (pages 380, 381) in Ibragimov and
Chasminskij (1981). In order to verify the assumptions it suﬃces to show:

(cid:16)

ν=1
X

(cid:17)

(i) ˜Sn(t) has asymptotically normal distribution with zero mean and ﬁnite variance;

(ii) for any compact set F in Rk+1,

•

•

E

sup
n

ZF

˜S2
n(t)W (t)dt <

;

∞

14

(iii)

•

˜S2
n(t1)

E

|

−

˜S2
n(t2)

D

t1 −

|

t2|

γ

| ≤

t1, t2

∀

for some γ > 0 and some D > 0;

(iv) for all η > 0 there exists some compact set Fη in Rk+1 with

•

E

Rk+1\Fη

Z

˜S2
n(t)W (t)dt < η

n, E

∀

Rk+1\Fη

Z

˜S2(t)W (t)dt < η.

Since ˜Sn(t) is the sums of martingale diﬀerences for each t and the the central limit theorem
for martingale diﬀerences can be applied which further implies (i). Direct calculations gives
(ii). Concerning (iii) we have

˜S2
n(t1)

E

|

˜S2
n(t2)

E

˜Sn(t1)

˜Sn(t2)

| ≤
|
h
2
˜Sn(t2)

−

−
˜Sn(t1)

| ×

+

|

|

˜S2
n(t1)
2

|
(cid:16)
˜Sn(t2)

˜Sn(t2)

|

+

|
1/2

|

(cid:17)i

|

(cid:17)

(cid:17)

−
˜Sn(t1)
(cid:12)
(cid:12)
(cid:12)

E

×

|
(cid:16)

(cid:12)
(cid:12)
(cid:12)

E( ˜Sn(t)2)

D,

≤

t
∀

E

≤

(cid:16)

and since

it suﬃces to study

We show here the needed inequality only for one of the terms in ˜Sn(t1)
treated in the same way. Particularly,

˜Sn(t2) all others are

−

E

˜Sn(t1)
(cid:12)
(cid:12)
(cid:12)

−

˜Sn(t2)

2

.

(cid:12)
(cid:12)
(cid:12)

1
√n

E

(cid:16)

n

j=1
X

wn(Xj−1)

(cos(t01εj)

(cid:16)

E cos(t01εj))(Yj(t1)

EYj(t1))

−

−

(cos(t02εj)

E cos(t02εj))(Yj(t2)

−

−

(cos(t01εj)

E cos(t01εj))(Yj(t1)

−

−

2

EYj(t2))

(cid:17)(cid:17)
EYj(t1))

E cos(t02εj))(Yj(t2)

EYj(t2))

−

−

2

(cid:17)(cid:17)

−
wn(Xj−1)

(cid:16)
(cos(t02εj)

−
t2||
t1 −

2

= E

(cid:16)

D

||

≤

where we used smoothness of cosine and moment assumptions. Proceeding similarly with other
terms and putting all together we conclude

˜S2
n(t1)

E

|

−

˜S2
n(t2)

D

t1 −

t2||

||

| ≤

This implies the item (iii). Item (iv) follows straightforwardly by our moment assumptions and
integrability of W .

15

Combining all the above arguments we can infer that the assertion of Theorem 3.2 holds true;
see Lemma 7.1 and proof of Theorem 4.1 (a) in Hl´avka et al. (2014) for a similar argumentation.
✷

Proof of Theorem 4.1.
We use the same decomposition of Tn =

(Sn(t))2W (t) dt as in the proof of Theorem
3.2. Please note that Lemma B.1 remains true under the assumptions of Theorem 4.1. A
R
careful inspection of the proof of Theorem 3.2 shows that applying this Lemma one obtains
Sn = ˜Sn + Rn, where

n(t)W (t) dt = oP (n) and

R2

˜Sn(t)
√n

=

1
n

n

R
wn(Xj−1)

j=1
X

cos(t0εj)

−

"

(cid:16)

E

cos(t0εj)

Yj(t) + Zj(t)

(cid:2)

(cid:3)(cid:17)(cid:16)

−

E[Yj(t) + Zj(t)]

(cid:17)

+

sin(t0εj)

E[sin(t0εj)]

Yj(t)

−

Zj(t)

−

−

E[Yj(t)

−

Zj(t)]

(cid:16)

(cid:17)(cid:16)

.
#

(cid:17)

The proof is ﬁnished as the end of the proof of Theorem 3.2 applying Theorem 22 (pages 380,
381) in Ibragimov and Chasminskij (1981). To this end, condition (i) is replaced by convergence
in probability of ˜Sn(t)/n1/2 to

¯S(t) = E

cos(t0εj)

−

"

(cid:16)

E

cos(t0εj)

Yj(t) + Zj(t)

(cid:2)

(cid:3)(cid:17)(cid:16)

−

E[Yj(t) + Zj(t)]

(cid:17)

+

sin(t0εj)

E[sin(t0εj)]

Yj(t)

Zj(t)

E[Yj(t)

Zj(t)]

−

−

−

#

−

(cid:16)

(cid:17)(cid:16)
for all t, whereas in conditions (ii)–(iv) ˜Sn is replaced by ˜Sn/n1/2. Thus we obtain convergence of
( ¯S(t))2W (t) dt in probability. Note further that by the addition theorems for trigono-
Tn/n to
( ¯S(t))2W (t)dt. This
metric functions and symmetry properties of cosine and sine it holds ˜T =
✷
completes the proof.

(cid:17)

R

R

Proof of Lemma 5.1.
Using assumption (A8),
Rk+1 |
k
i=1 Vi(ti) by assumption, one obtains
W (t0, . . . , tk) = V0(t0)
R

it follows that

t4
0W (t0, . . . , tk)

d(t0, . . . , tk) <

|

∞

and since

>

∞

Rk+1 |

Z

Q
t4
0W (t0, . . . , tk)

|

d(t0, . . . , tk) =

t4
0V0(t0)

Rk+1

Z

Vi(ti)d(t0, . . . , tk)

k

i=1
Y
k

t4
0V0(t0)dt0

=

R

Z

i=1 Z
Y

Vi(ti)dti

R

F

L1(R) for any i = 0, . . . , k. Hence, the Fourier transformation of any Vi,
which gives that Vi ∈
[Vi], exists. The representation of the test statistic is now straightforwardly computed by
say
using the deﬁnition of the Fourier transformation and of the empirical characteristic functions
besides the multiplicative structure of W . Since the computation is tedious but without further
✷
insights, this part of the proof is omitted here.

16

B Auxiliary results

First note that for ˆκn deﬁned in (A.1) one obtains directly that E[(ˆκn −
by 1

an
2 ) = o(1) and thus we have

FX1( an
2 )

FX1(

−

−

−

1)2] can be bounded

ˆκn = 1 + oP (1).

(B.1)

Proposition B.1 Let (Xj)j∈Z be a strictly stationary time series with marginal density fX.
Xj = x) and assume (A1),
Xj = x] and σ2(x) = Var(Xj+1 |
Deﬁne m(x) = E[Xj+1 |
(A2), (A5), (A6). Let ∆n = inf |x|≤an fX(x), a∗
n =
n)(log n)D. Here, D > 0 is some multiple of r from assumption (A2) and may
((cn/n)1/2 + c2
diﬀer from line to line. We then have

n = ((log n)/(ncn))1/2 + (log n)Dc2

n and b∗

(i)

(ii)

(iii)

ˆfX(x)

sup
|x|≤an |

fX (x)

|

−

= OP (a∗
n)

sup
|x|≤an |

ˆm(x)

m(x)

|

−

= OP (

)

sup
|x|≤an |

ˆσ(x)

−

σ(x)

|

= OP (

a∗
n
∆n
a∗
n
∆n

)

sup
|x|≤an

sup
|x|≤an

sup
|x|≤an

1
ncn

1
ncn

1
ncn

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

K

K

K

n

j=1
X
n

j=1
X
n

j=1
X

(cid:18)

(cid:18)

(cid:18)

x

x

x

Xj−1 −
cn

Xj−1 −
cn

Xj−1 −
cn

(cid:19)

(cid:19)

(cid:19)

(m(Xj−1)

(σ2(Xj−1)

−

−

m(x))

= OP (b∗
n)

(cid:12)
(cid:12)
(cid:12)
σ2(x))
|

= OP (b∗
n)

(m2(Xj−1)

m2(x))

|

−

= OP (b∗
n)

1
ncn

n

K

j=1
X

(cid:18)

x

Xj−1 −
cn

(cid:19)

σ(Xj−1)εj(m(Xj−1)

m(x))

|

−

= OP (b∗

n).

sup
|x|≤an

(cid:12)
(cid:12)
(cid:12)

Proof. The ﬁrst two results of (i) are stated in Theorems 6 and 8 by Hansen (2008) without
the (log n)D factor of the c2
n term. In comparison to Hansen (2008) we use a diﬀerent bounding
for the expectation terms since we do not assume second derivatives. E. g. we obtain, making
use of the mean value theorem, the properties of the kernel function and our assumption (A2),

E[ ˆfX(x)

sup
|x|≤an |

fX (x)]

= sup

|x|≤cn |

|

−

cnu)

−

−

fX (x)) du

|

K(u)(fX(x

Z

17

cn

K(u)

sup
|x|≤an

≤

= O(c2

Z
n(log n)r).

u

|

|

sup
ξ between
x and x−cnu

|

f ′(ξ)

f ′(x)

du

|

−

The result on ˆσ follows similarly to the derivations by Hansen (2008) by noting that ˆσ2(x) =
ˆs(x)
Xj = x] based on
the observation pairs (Xj−1, X 2

ˆm2(x), where ˆs is the Nadaraya-Watson estimator for s(x) = E[X 2

j ), j = 1, . . . , n.

j |

−

Towards the results in (ii) we treat only the ﬁrst one since the others follow analogously.

Note that by the mean value theorem

−

m(Xi−1)

m(x) = (Xi−1 −

x)m′(x) + (Xi−1 −
for some ξXi−1,x between [min(Xi−1, x), max(Xi−1, x)], where the absolute value of the second
x)2(log n)r due to assumption (A2). It thus suﬃces to
summand can be bounded by (Xi−1 −
show

x)(m′(ξXi−1,x)

m′(x))

−

n

K

1
ncn
n

i=1
X
K

(cid:18)
Xi−1 −
cn

i=1
X

(cid:18)

sup
|x|≤an

(cid:12)
(cid:12)
(cid:12)

1
ncn

(cid:12)
(cid:12)
(cid:12)

sup
|x|≤an

Xi−1 −
cn

x

(cid:19)

(Xi−1 −

x

(cid:19)

(Xi−1 −

x)m′(x)

x)2

= OP (c2
n)

(cid:12)
(cid:12)
(cid:12)

= OP (b∗

n).

(cid:12)
(cid:12)
(cid:12)

The ﬁrst relation is straightforward by assumption (A2) and applying Theorem 2 in Hansen
K(u)u2. For the latter one we receive with the same
(2008) with Yi = 1 and the kernel u
theorem applied with Yi = 1 and kernel u

K(u)u

7→

7→

x

−

Xi−1
cn

(cid:19)

(x

−

Xi−1)

−

sup
|x|≤an

1
ncn

(cid:12)
(cid:12)
(cid:12)
= OP

n

K

i=1 (cid:16)
X

(cid:18)
log n
ncn

1/2

cn

.

(cid:17)

(cid:17)
Further by direct calculation

(cid:16)(cid:16)

E

K

(cid:18)

h

x

−

Xi−1
cn

(cid:19)

(x

−

Xi−1)

i(cid:17)(cid:12)
(cid:12)
(cid:12)

1
ncn

n

i=1
X

E

K

h
= cn

(cid:18)

Z

x

−

Xi−1
cn

(x

−

(cid:19)

Xi−1)

=

i

1
cn Z

K

(cid:18)

K(z)zfXi−1 (x

−

zcn))dz = O(c2
n

Z

x

−

(x

y
−
cn (cid:19)
z2K(z)dz sup
x∈ ˜In |

y)fXi−1(y)dy

f ′
X(x)

|

) = O(c2

n(log n)r)

where we utilize assumptions (A2) and (A5).

The result (iii) can be proved in the same way as the results in (ii). Just set Yi =

for the
ﬁrst and Yi = σ(Xi−1)εi for the second relation (when applying Theorem 2 by Hansen, 2008)
and note that

εi|

|

x

−

Xi−1
cn

E

K

(cid:18)

h

Xi−1)σ(Xi−1)εi

= 0.

i

✷

(x

−

(cid:19)

18

Lemma B.2 Under the assumptions of Theorem 3.2 we have

where

˜S(1,1)
n

(t) =

1
√n

(S(1,1)
n

(t)

−

˜S(1,1)
n

(t))2W (t) dt = oP (1),

Z

n

wn(Xj−1)

cos(t0εj)

E[cos(t0εj)]

(Yj(t)

E[Yj(t)]).

−

−

(cid:0)

(cid:1)

j=1
X

Proof. Due to (B.1) we have

S(1,1)
n

(t) = (1 + oP (1))( ˜S(1,1)

n

(t)

Jn(t)J (1)

n (t)

−

−

Jn(t)J (2)

n (t)),

where

Jn(t) =

E[cos(t0εj)]

−

(cid:17)

1
√n

j=1
X
n

n

wn(Xj−1)

cos(t0εj)

(cid:16)

E[Yℓ(t)]

Yℓ(t)

Xℓ=1 (cid:16)
n

−

Yℓ(t)(wn(Xℓ−1)

Xℓ=1

(cid:17)

ˆκn)

−

1
ˆκn

.

J (1)
n (t) =

J (2)
n (t) =

1
n

1
n

Note that from assumption (A1) it follows that β > 2 and thus
. From
this, centeredness of the summands (under the null) and the boundedness of cosine analogously
to the proof of Theorem 2 by Yokoyama (1980) one obtains

P

∞

∞
i=0(i + 1)α(i) <

E[(Jn(t))4]

≤
n (t))4]

E[(J (1)

D

1
n2 D.

≤

(B.2)

(B.3)

The constant D can be chosen independent of t due to the boundedness of the cosine function.
Thus from the Cauchy Schwarz inequality we obtain directly

E

(Jn(t)J (1)

n (t))2W (t) dt

= O(

h Z

i

1
n

).

Now note that

J (2)
n (t) = (1 + oP (1))

Yℓ(t)(wn(Xℓ−1)

E[wn(Xℓ−1)])

−

n

1
n

Xℓ=1
n
1
n

n

Yℓ(t)

1
n

−

Xℓ=1
and thus by boundedness of Yℓ we have, uniformly with respect to t,

j=1
X

(wn(Xj−1)

E[wn(Xj−1)])

−

!

J (2)
n (t)

|

|

= OP (1)

1
n

n

|

j=1
X

wn(Xj−1)

E[wn(Xj−1)]

= oP (1)

|

−

(B.4)

19

 
by a consideration of the expectation of the sum due to the properties of the weight function.
We obtain

(Jn(t)J (2)

n (t))2W (t) dt = oP (1)

(Jn(t))2W (t) dt = oP (1)

Z

Z

by an application of (B.2).

✷

Lemma B.3 Under the assumptions of Theorem 3.2 we have

(S(1,2)
n

(t)

−

Z

˜S(1,2)
n

(t))2W (t) dt = oP (1),

where

˜S(1,2)
n

(t) =

1
√n

n

j=1
X

wn(Xj−1) sin(t0εj)t0

m

ˆm

−
ˆσ

(cid:18)

(Xj−1) + εj

ˆσ

σ

−
ˆσ

(Xj−1)

(Yj(t)

(cid:19)

E[Yj(t)]).

−

Proof. Due to (B.1) we have

S(1,2)
n

(t) = (1 + oP (1))( ˜S(1,2)

n

(t)

t0In(t)J (1)

n (t)

−

−

t0In(t)J (2)

n (t))

with J (1)

n and J (2)

n as in Lemma B.2 and

In(t) =

1
√n

n

j=1
X

wn(Xj−1) sin(t0εj)

m

ˆm

−
ˆσ

(cid:18)

(Xj−1) + εj

ˆσ

σ

−
ˆσ

(Xj−1)

.

(cid:19)

Now

In(t)

|

| ≤

1
√n

n

(

εj|

|

+ 1)OP (

) = oP (√n)

a∗
n
∆n

j=1
X
uniformly with respect to t by assumption (A3) and Proposition B.1 (i). Thus

(t0In(t)J (1)

n (t))2W (t) dt = oP (n)

0(J (1)
t2

n (t))2W (t) dt = oP (1)

Z

by (B.3).

Further, by (B.4) we obtain

Z

(t0In(t)J (2)

n (t))2W (t) dt = oP (1)

0(In(t))2W (t) dt = oP (1),
t2

Z

Z

where one yields the last equality as follows. Similarly to the proof of Lemma B.4 one can ﬁrst
replace the random denominators ˆσ ˆfX in the deﬁnition of In by their true counterparts σfX
0( ˜In(t))2W (t) dt] = O(1)
applying Proposition B.1. Let ˜In denote the resulting term, then E[
t2
✷
is shown by straightforward calculations.

R

20

Lemma B.4 Under the assumptions of Theorem 3.2 we have

˜S(1,2)
n

(t)

−

˜S(1,2,1)
n

(t)

−

˜S(1,2,2)
n

(t)

Z (cid:16)

2

(cid:17)

W (t) dt = oP (1),

where

˜S(1,2,1)
n

(t) =

˜S(1,2,2)
n

(t) =

t0
ˆκnn3/2

t0
ˆκnn3/2

n

n

j=1
X

i=1
X

n

n

j=1
X

i=1
X

1
cn

K

(cid:18)
(Yj(t)
×
1
cn

K

(cid:18)
(Yj(t)

Xi−1

Xj−1 −
cn

E[Yj(t)])
Xi−1

−
Xj−1 −
cn

(cid:19)

(cid:19)

wn(Xj−1)σ(Xi−1)εi
σ(Xj−1)fX (Xj−1)

sin(t0εj)

wn(Xj−1)σ2(Xi−1)(ε2

i −
2σ2(Xj−1)fX(Xj−1)

1)

sin(t0εj)εj

−
Proof. Recall the deﬁnition of ˜S(1,2)

×

n

in Lemma B.3 and note that

E[Yj(t)])

wn(Xj−1)
ˆκn

sin(t0εj)

wn(Xj−1)
ˆκn

sin(t0εj)

ˆm

ˆm

m

m

−
ˆσ

−
σ

(Xj−1)(Yj(t)

E[Yj(t)])

−

(Xj−1)

ˆfX
fX

(Xj−1)(Yj(t)

−

E[Yj(t)]) + R(1)

n (t)

n

j=1
X
n

t0
√n

t0
√n

=

j=1
X
+ R(2)
n (t)
= ˜S(1,2,1)
n

(t) + R(1)

n (t) + R(2)

n (t) + R(3)

n (t),

where

R(1)

n (t) =

R(2)

n (t) =

R(3)

n (t) =

t0
√n

t0
√n

n

j=1
X
n

j=1
X

wn(Xj−1)
ˆκn

wn(Xj−1)
ˆκn

t0
ˆκn√n

n

j=1
X

sin(t0εj)

sin(t0εj)

ˆm

ˆm

m

m

−
ˆσ

−
σ

(Xj−1)

σ

ˆσ

−
σ

(Xj−1)(Yj(t)

E[Yj(t)])

−

(Xj−1)

ˆfX

fX −
fX

(Xj−1)(Yj(t)

E[Yj(t)])

−

wn(Xj−1)
σ(Xj−1)

sin(t0εj)

1
ncn

n
i=1 K

Xj−1−Xi−1
cn

(m(Xi−1)



P

(cid:16)

(cid:17)
fX(Xj−1)

m(Xj−1))

−

(Yj(t)

×


E[Yj(t)])).

−

By Proposition B.1 (i) one directly obtains that
OP (n(a∗

n/∆n)4) = oP (1).

Concerning

(R(3)

n (t))2W (t) dt notice that

(R(j)

n (t))2W (t) dt for j = 1, 2 is of rate

R

R(3)

n (t)

|

| ≤

D

R

t0
ˆκn√n

n

j=1
X

wn(Xj−1)
σ(Xj−1)fX(Xj−1)

sup
|x|≤an

1
ncn

n

K

i=1
X

(cid:18)

x

−

Xi−1
cn

(cid:19)

(m(Xi−1)

m(x))

−

uniformly in t which together with assertion B.1 (ii) implies the rate
OP (n(log n)4r(b∗

n)2) = oP (1), where the latter equality follows from assumption (A7).

(R(3)

n (t))2W (t) dt =

R





(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

21

Concerning the second term in the deﬁnition of ˜S(1,2)
σ = (ˆσ2

σ2)/(ˆσ + σ), analogous to before one shows that

n

ˆσ

−

−

in Lemma B.3 note that due to

t0
√n

t0
√n

n

j=1
X
n

j=1
X

wn(Xj−1)
ˆκn

wn(Xj−1)
ˆκn

=

sin(t0εj)εj

ˆσ

σ

−
ˆσ

(Xj−1)(Yj(t)

E[Yj(t)])

−

sin(t0εj)εj

(ˆσ2

σ2) ˆfX

−
2σ2fX

(Xj−1)(Yj(t)

−

E[Yj(t)]) + R(4)

n (t),

where

(R(4)

n (t))2W (t) dt = oP (1) now follows from Proposition B.1 (i).

To treat the remaining term further we ﬁrst insert the deﬁnition of ˆσ2 and then use the fact
ˆm)2 and insert the deﬁnition of ˆm. With this one obtains

ˆm2 = 2m(m

R
that m2

(m

ˆm)

−

−

−

−

n

t0
√n

wn(Xj−1)
ˆκn

sin(t0εj)εj

(ˆσ2

σ2) ˆfX

−
2σ2fX

(Xj−1)(Yj(t)

E[Yj(t)])

−

j=1
X
= ˜S(1,2,2)
n

(t) + R(5)

n (t) + R(6)

n (t) + R(7)

n (t) + R(8)

n (t)

R(9)

n (t),

−

where

R(5)

n (t) =

t0
ˆκnn3/2

R(6)

n (t) =

t0
ˆκnn3/2

R(7)

n (t) =

t0
ˆκnn3/2

R(8)

n (t) =

t0
ˆκnn3/2

R(9)

n (t) =

t0
ˆκnn3/2

j=1
X

×
n

j=1
X

×
n

j=1
X
×
n

j=1
X
×
n

n

n

1
cn

K

Xi−1

Xj−1 −
cn

(cid:19)

wn(Xj−1)
2σ2(Xj−1)fX(Xj−1)

sin(t0εj)εj

K

(cid:18)
i=1
X
(σ2(Xi−1)
n
1
cn

(cid:18)
i=1
X
(m2(Xi−1)
n
1
cn

K

σ2(Xj−1))(Yj(t)
−
Xi−1
Xj−1 −
cn

E[Yj(t)])
wn(Xj−1)
2σ2(Xj−1)fX(Xj−1)

−

m2(Xj−1))(Yj(t)
Xi−1

−
Xj−1 −
cn

E[Yj(t)])

−
wn(Xj−1)
σ2(Xj−1)fX(Xj−1)

sin(t0εj)εj

sin(t0εj)εj

K

1
cn

(cid:18)
i=1
X
m(Xj−1)(m(Xj−1)
n
Xj−1 −
cn

(cid:18)
i=1
X
σ(Xi−1)εi(m(Xi−1)
n
Xj−1 −
cn

1
cn

K

−
Xi−1

−
Xi−1

m(Xi−1))(Yj(t)

E[Yj(t)])

−
wn(Xj−1)
σ2(Xj−1)fX(Xj−1)

sin(t0εj)εj

m(Xj−1))(Yj(t)

E[Yj(t)])

−
wn(Xj−1)
2σ2(Xj−1)fX(Xj−1)

sin(t0εj)εj

(cid:19)

(cid:19)

(cid:19)

(cid:19)

j=1
X

(cid:18)
i=1
X
(m(Xj−1)

ˆm(Xj−1))2(Yj(t)

E[Yj(t)])

−

×

−
n (t))2W (t) dt = oP (1) completely analogous to the treatment of R(3)
n
✷
n for j = 9.

(R(j)
and one can show
for j = 5, 6, 7, 8 and R(1)

R

22

Lemma B.5 Under the assumptions of Theorem 3.2 we have

( ˜S(1,2,1)
n

(t)

( ˜S(1,2,2)
n

(t)

−

−

Z

Z

¯S(1,2,1)
n

(t))2W (t) dt = oP (1)

¯S(1,2,2)
n

(t))2W (t) dt = oP (1),

where

¯S(1,2,1)
n

(t) =

t0E[sin(t0ε1)]
√n

¯S(1,2,2)
n

(t) =

t0E[sin(t0ε1)ε1]
2√n

j=1
X
n

n

wn(Xj−1)εj(E[Yj(t)

Xj−1]

|

−

E[Yj(t)])

wn(Xj−1)(ε2

j −

1)(E[Yj(t)

Xj−1]

|

−

E[Yj(t)]).

j=1
X

Proof. We only prove the ﬁrst assertion, the second one can be shown completely analogous.

We have the expansion

˜S(1,2,1)
n

(t)

¯S(1,2,1)
n

−

(t) = Un(t) + t0E[sin(t0ε1)]Vn(t),

where

with ζj = (Xj−1, . . . , Xj−k),

ϕ(t, εi, Xi−1, ζj)

Un(t) =

1
n3/2

n

n

j=1
X

i=1
X

ϕ(t, εi, Xi−1, ζj)

K

Xi−1

Xj−1 −
cn

wn(Xj−1)
fX (Xj−1)σ(Xj−1)

sin(t0εj)(Yj(t)

E[Yj(t)])

−

= σ(Xi−1)εi

1
cn

−

Z

and

(cid:16)
1
cn

K

x

−

Xi−1
cn

(cid:0)

(cid:1)

(cid:17)
wn(x)
σ(x)

E[sin(t0ε1)](E[Yj(t)

Xj−1 = x]

|

−

E[Yj(t)]) dx

!

Vn(t) =

1
√n

n

i=1
X

σ(Xi−1)εi

Z

1
cn

K

x

−

Xi−1
cn

(cid:0)

(cid:1)(cid:16)

wn(x)ψ(t, x)
σ(x)

wn(Xi−1)ψ(t, Xi−1)
σ(Xi−1)

−

dx

(cid:17)

with ψ(t, x) = E[Yi(t)
considering the expectation

|

Xi−1 = x]

E[Yi(t)]. Straightforwardly we obtain negligibility of Vn by

−

E

0V 2
t2

n (t)W (t) dt

Z
(cid:2)
t2
0

=

Z

Z

σ2(z)

(cid:16) Z

(cid:3)
K

1
cn

z

x

−
cn

(cid:0)

(cid:1)(cid:16)

wn(x)ψ(t, x)
σ(x)

wn(z)ψ(t, z)
σ(z)

−

2

dx

(cid:17)

(cid:17)

fX (z) dz W (t) dt.

23

 
Kn = [

Note that the inner integral is zero for z
an + cnC, an −
z
−
0V 2
t2
n (t)W (t) dt

cnC] and z

E

∈

∈

6∈
In \

Kn to obtain

In. We further separately consider the cases

Z
(cid:2)
t2
0

Z

x

≤

(cid:3)
K

h Z

σ2(z)

(cid:16) Z
fX (z)I

1
−
cn
cn
(cid:0)
Kn}
z
{
x
1
−
cn
cn
(cid:0)
= O((log n)5r(cn + cD
n )) = o(1)

×
σ2(z)

∈
K

(cid:16) Z

+

Z

dz
z

(cid:1)(cid:16)

z

ψ(t, x)

ψ(t, z)

−
σ(x)

|

(cid:1)(cid:16)

|

+

1
σ(x)

+

1
σ(z)

2

dx

(cid:17)

(cid:17)

1

σ(x) −

1
σ(z)

2

dx

(cid:12)
(cid:12)
(cid:12)
fX (z)I

(cid:17)

(cid:12)
(cid:17)
(cid:12)
(cid:12)
Kn}
In \

z

{

∈

dz

W (t) dt

i

by assumptions (A2) and (A4).

We will now prove E[

U 2

n(t)W (t) dt] = o(1). To this end note that

R

U 2

n(t)W (t) dt] =

E[

Z

1
n3

n

n

n

n

E

g(ξi1, ξi2, ξj1, ξj2)

,

(B.5)

j1=1
X

i1=1
X

j2=1
X

i2=1
X

h

i

where ξi = (εi, ζi) and

g(ξi1, ξi2, ξj1, ξj2) =

ϕ(t, εi1, Xi1−1, ζj1)ϕ(t, εi2, Xi2−1, ζj2)W (t) dt

Z

We ﬁrst consider the case where all indices i1, j1, i2, j2 are diﬀerent. Then the expecta-
tion is zero if either i1 or i2 is the largest index because E[εi] = 0 and εi is independent of
= i). All other cases are treated similarly and thus we only discuss
εj, Xi−1, Xi−2, . . . (for j
the case i1 < i2 < j1 < j2 in detail. We will apply a version of Lemma 2.1 by Sun and Chiang
(1997) for multivariate random variables (see Su and Xiao’s (2008) Lemma D.1) in two separate
Z, an independent copy of
i2. Denote by the process ξ∗
subcases. First let i2 −
Z i. e. a process with the same distributional properties, but independent of the original
ξi, i
data. Then E[g(ξ∗

i1, ξi2, ξj1, ξj2)] = 0 and, for δ > 0,

j1 −

i1 ≥

i , i

∈

∈

E

g(ξ∗

i1, ξi2, ξj1, ξj2)

1+δ

k1

h(cid:12)
sup
(cid:12)
(cid:12)
x∈[−an−Ccn,an+Ccn]

i
σ2+2δ(x)
(cid:12)
(cid:12)

≤

sup
x∈[−an−Ccn,an+Ccn]

σ−2−2δ(x)E[
|

ε1|

1+δ]

E

×

" Z  
1
cn

(cid:12)
(cid:12)
(cid:12)
K

× 

1
cn

K

x

−
cn

(cid:16)

1
cn

K

y

Xj1−1 −
cn

wn(Xj1−1)
fX(Xj1−1)

+

(cid:17)

Xi2−1

(cid:16)
Xj2−1 −
cn
(cid:17)
σ2+2δ(x)

Z (cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
wn(Xj2−1)
fX(Xj2−1)

1
cn
Z (cid:12)
(cid:16)
(cid:12)
σ−2−2δ(x)
sup
(cid:12)
x∈[−an−Ccn,an+Ccn]

K

+

(cid:12)
(cid:12)
(cid:12)

x

−

y

wn(x)

(cid:12)
(cid:12)
(cid:12)

(cid:17)
Xi2−1
cn

(cid:17)

(cid:12)
(cid:12)
(cid:12)

(cid:16)
sup
x∈[−an−Ccn,an+Ccn]

k2

≤

1+δ

dx

!

fX(y) dy

1+δ

wn(x)

dx

!

1+δ

εi2|

|

#

(cid:12)
(cid:12)
(cid:12)

sup
x∈[−an−Ccn,an+Ccn]

(fX (x))−1−δ + 1

×

(cid:16)

(cid:17)

24

6
c−δ
n E

×

" 

1
cn

K

(cid:16)

Xj2−1 −
cn

Xi2−1

wn(Xj2−1)
fX(Xj2−1)

(cid:17)

(cid:12)
(cid:12)
(cid:12)

1+δ

+ 1

!

1+δ

εi2|

|

#

(cid:12)
(cid:12)
(cid:12)

for some constants k1, k2. This is of order O((log n)˜rc−2δ
(A2)–(A5). An application of the aforementioned inequality gives

n ) for ˜r = 5r(1 + δ) by assumptions

1
n3

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

E

g(ξi1, ξi2, ξj1, ξj2)

i1<i2<j1<j2
X
i2−i1≥j1−i2

h

(cid:12)
(cid:12)
i
(cid:12)
(cid:12)
(cid:12)

= O((log n)˜r/(1+δ))

1
n3c2δ/(1+δ)
n

(α(i2 −

i1))δ/(1+δ)

i1<i2<j1<j2
X
i2−i1≥j1−i2

(log n)5r
nc2δ/(1+δ)

n

j(α(j))δ/(1+δ)

n

j=1
X

(cid:17)
j1− βδ

1+δ = o(1)

= O

(cid:16)

o(1)

≤

∞

j=1
X

by assumptions (A1), (A3) and (A7). Here the mixing coeﬃcient α of ξi, i
as the mixing coeﬃcient of Xi, i
we apply the same inequality but by considering E[g(ξ∗

Z, see Fan and Yao (2003). In the subcase i2 −
i1, ξ∗
i2, ξj1, ξj2)] = 0 and obtain

∈

∈

Z, is the same
j2
i1 < j1 −

1
n3

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

E

g(ξi1, ξi2, ξj1, ξj2)

i1<i2<j1<j2
X
i2−i1≥j1−i2

h

(cid:12)
(cid:12)
i
(cid:12)
(cid:12)
(cid:12)

= O((log n)˜r/(1+δ))

1
n3c2δ/(1+δ)
n

(α(j1 −

i2))δ/(1+δ)

i1<i2<j1<j2
X
i2−i1<j1−i2

(log n)5r
nc2δ/(1+δ)

n

= O

(cid:16)

n

(cid:17)

j=1
X

j(α(j))δ/(1+δ) = o(1).

For the case i1 = i2 we exemplarily consider the subcase i1 = i2 < j1 < j2, other sub-
cases are treated similarly. Note that E[g(ξi1, ξi1, ξj1, ξ∗
), and
E[
|

j2)] = 0 by the deﬁnition of ϕ(

n ) as before. Thus we obtain

1+δ] = O((log n)˜rc−2δ

g(ξi1, ξi1, ξj1, ξ∗

j2)

·

|

1
n3

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

E

g(ξi1, ξi1, ξj1, ξj2)
h

i1<j1<j2
X

(cid:12)
(cid:12)
i
(cid:12)
(cid:12)
(cid:12)

= O((log n)˜r/(1+δ))

1
n3c2δ/(1+δ)
n
n

(α(j2 −

j1))δ/(1+δ)

i1<j1<j2
X

(log n)5r
nc2δ/(1+δ)

n

= O

(cid:16)

(α(j))δ/(1+δ) = o(1).

(cid:17)

j=1
X

Finally, the cases where more than two indices in i1, i2, j1, j2 are equal always lead to neg-
= j2 = i2 in the
ligible terms by direct calculation. E. g. consider the term for j1 = i1 6
sum (B.5). Applying assumption (A2) its absolute value can straightforwardly be bounded
by n−1O((log n)4r)(E[
n = o(1) by assumption (A7). The remaining terms are
|
✷
treated analogously.

])2K 2(0)/c2

ε1|

25

