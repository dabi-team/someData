0
2
0
2

l
u
J

1
3

]

V
C
.
s
c
[

1
v
6
9
7
5
1
.
7
0
0
2
:
v
i
X
r
a

AR-Net: Adaptive Frame Resolution for
Eﬃcient Action Recognition

Yue Meng1, Chung-Ching Lin1, Rameswar Panda1, Prasanna Sattigeri1,
Leonid Karlinsky1, Aude Oliva1,3, Kate Saenko1,2, and Rogerio Feris1

1 MIT-IBM Watson AI Lab, IBM Research
2 Boston University
3 Massachusetts Institute of Technology

Abstract. Action recognition is an open and challenging problem in
computer vision. While current state-of-the-art models oﬀer excellent
recognition results, their computational expense limits their impact for
many real-world applications. In this paper, we propose a novel approach,
called AR-Net (Adaptive Resolution Network), that selects on-the-ﬂy
the optimal resolution for each frame conditioned on the input for eﬃ-
cient action recognition in long untrimmed videos. Speciﬁcally, given a
video frame, a policy network is used to decide what input resolution
should be used for processing by the action recognition model, with the
goal of improving both accuracy and eﬃciency. We eﬃciently train the
policy network jointly with the recognition model using standard back-
propagation. Extensive experiments on several challenging action recog-
nition benchmark datasets well demonstrate the eﬃcacy of our proposed
approach over state-of-the-art methods. The project page can be found
at https://mengyuest.github.io/AR-Net

Keywords: Eﬃcient Action Recognition, Multi-Resolution Processing,
Adaptive Learning

1 Introduction

Action recognition has attracted intense attention in recent years. Much progress
has been made in developing a variety of ways to recognize complex actions,
by either applying 2D-CNNs with additional temporal modeling [31,54,15] or
3D-CNNs that model the space and time dimensions jointly [49,7,25]. Despite
impressive results on commonly used benchmark datasets, the accuracy obtained
by most of these models usually grows proportionally with their complexity and
computational cost. This poses an issue for deploying these models in many
resource-limited applications such as autonomous vehicles and mobile platforms.
Motivated by these applications, extensive studies have been recently con-
ducted for designing compact architectures [44,27,28,64,2] or compressing mod-
els [13,57,9,35]. However, most of the existing methods process all the frames in
a given video at the same resolution. In particular, orthogonal to the design of
compact models, the computational cost of a CNN model also has much to do

 
 
 
 
 
 
2

Y. Meng et al.

Figure 1. A conceptual overview of our approach. Rather than processing all
the frames at the same resolution, our approach learns a policy to select the optimal
resolution (or skip) per frame, that is needed to correctly recognize an action in a given
video. As can be seen from the ﬁgure, the seventh frame is the most useful frame for
recognition, therefore could be processed only with the highest resolution, while the
rest of the frames could be processed at lower resolutions or even skipped without
losing any accuracy. Best viewed in color.
with the input frame size. To illustrate this, let us consider the video in Figure 1,
represented by eight uniformly sampled frames. We ask, Do all the frames need
to be processed at the highest resolution (e.g., 224×224) to recognize the action
as “Making a sandwich” in this video? The answer is clear: No, the seventh
frame is the most useful frame for recognition, therefore we could process only
this frame at the highest resolution, while the rest of the frames could be pro-
cessed at lower resolutions or even skipped (i.e., resolution set to zero) without
losing any accuracy, resulting in large computational savings compared to pro-
cessing all the frames with the same 224×224 resolution. Thus, in contrast to
the commonly used one-size-ﬁts-all scheme, we would like these decisions to be
made individually per input frame, leading to diﬀerent amounts of computation
for diﬀerent videos. Based on this intuition, we present a new perspective for
eﬃcient action recognition by adaptively selecting input resolutions, on a per
frame basis, for recognizing complex actions.

In this paper, we propose AR-Net, a novel and diﬀerentiable approach to learn
a decision policy that selects optimal frame resolution conditioned on inputs for
eﬃcient action recognition. The policy is sampled from a discrete distribution
parameterized by the output of a lightweight neural network (referred to as the
policy network), which decides on-the-ﬂy what input resolution should be used on
a per frame basis. As these decision functions are discrete and non-diﬀerentiable,
we rely on a recent Gumbel Softmax sampling approach [29] to learn the policy
jointly with the network parameters through standard back-propagation, with-
out resorting to complex reinforcement learning as in [62,14,63]. We design the
loss to achieve both competitive performance and resource eﬃciency required
for action recognition. We demonstrate that adaptively selecting the frame res-
olution by a lightweight policy network yields not only signiﬁcant savings in
FLOPS (e.g., about 45% less computation over a state-of-the-art method [61] on

Adaptive Resolution for Eﬃcient Action Recognition

3

ActivityNet-v1.3 dataset [5]), but also consistent improvement in action recog-
nition accuracy.

The main contributions of our work are as follows:

– We propose a novel approach that automatically determines what resolutions

to use per target instance for eﬃcient action recognition.

– We train the policy network jointly with the recognition models using back-
propagation through Gumbel Softmax sampling, making it highly eﬃcient.
– We conduct extensive experiments on three benchmark datasets (ActivityNet-
V1.3 [5], FCVID [30] and Mini-Kinetics [7]) to demonstrate the superiority
of our proposed approach over state-of-the-art methods.

2 Related Works

Eﬃcient Action Recognition. Action recognition has made rapid progress
with the introduction of a number of large-scale datasets such as Kinetics [7]
and Moments-In-Time [39,40]. Early methods have studied action recognition
using shallow classiﬁcation models such as SVM on top of local visual features
extracted from a video [34,53]. In the context of deep neural networks, it is
typically performed by either 2D-CNNs [31,46,10,17,21] or 3D-CNNs [49,7,25].
A straightforward but popular approach is the use of 2D-CNNs to extract frame-
level features and then model the temporal causality across frames using diﬀerent
aggregation modules such as temporal averaging in TSN [54], a bag of features
scheme in TRN [65], channel shifting in TSM [36], depthwise convolutions in
TAM [15], non-local neural networks [55], and LSTMs [12]. Many variants of 3D-
CNNs such as C3D [49], I3D [7] and ResNet3D [25], that use 3D convolutions to
model space and time jointly, have also been introduced for action recognition.
While extensive studies have been conducted in the last few years, limited
eﬀorts have been made towards eﬃcient action recognition [62,61,20]. Speciﬁ-
cally, methods for eﬃcient recognition focus on either designing new lightweight
architectures (e.g., R(2+1)D [51], Tiny Video Networks [44], channel-separated
CNNs [50]) or selecting salient frames/clips conditioned on the input [63,62,33,20].
Our approach is most related to the latter which focuses on adaptive data sam-
pling and is agnostic to the network architecture used for recognizing actions.
Representative methods typically use Reinforcement Learning (RL) where an
agent [62,14,63] or multiple agents [59] are trained with policy gradient meth-
ods to select relevant video frames, without deciding frame resolution as in our
approach. More recently, audio has also been used as an eﬃcient way to select
salient frames for action recognition [33,20]. Unlike existing works, our frame-
work requires neither complex RL policy gradients nor additional modalities such
as audio. LiteEval [61] proposes a coarse-to-ﬁne framework for resource eﬃcient
action recognition that uses a binary gate for selecting either coarse or ﬁne fea-
tures. In contrast, we address both the selection of optimal frame resolutions and
skipping in an uniﬁed framework and jointly learn the selection and recognition
mechanisms in a fully diﬀerentiable manner. Moreover, unlike binary sequential
decision being made at every step in LiteEval, our proposed approach has the

4

Y. Meng et al.

ﬂexibility in deciding multiple actions in a single step and also the scalability
towards long untrimmed videos via multi-step skipping operations. We include
a comprehensive comparison to LiteEval in our experiments.

Adaptive Computation. Many adaptive computation methods have been re-
cently proposed with the goal of improving computational eﬃciency [3,4,52,56,23].
Several works have been proposed that add decision branches to diﬀerent layers
of CNNs to learn whether to exit the network for faster inference [18,38]. Block-
Drop [60] eﬀectively reduces the inference time by learning to dynamically select
which layers to execute per sample during inference. Adaptive computation time
for recurrent neural networks is also presented in [23]. SpotTune [24] learns to
adaptively route information through ﬁnetuned or pre-trained layers. Reinforce-
ment learning has been used to adaptively select diﬀerent regions for fast object
detection in large images [41,19]. While our approach is inspired by these meth-
ods, in this paper, we focus on adaptive computation in videos, where our goal
is to adaptively select optimal frame resolutions for eﬃcient action recognition.

Multi-Resolution Processing. Multi-resolution feature representations have
a long history in computer vision. Traditional methods include image pyra-
mids [1], scale-space representations [43], and coarse-to-ﬁne approaches [42].
More recently, in the context of deep learning, multi-scale feature representa-
tions have been used for detection and recognition of objects at multiple scales
[6,37], as well as to speed up deep neural networks [37,8]. Very few approaches
have explored multi-scale recognition for eﬃcient video understanding. A two-
branch network that fuses the information of high-resolution and low-resolution
video frames is proposed in [31]. bLVNet-TAM [15] also uses a two-branch multi-
resolution architecture based on the Big-Little Net model [8], while learning
long-term temporal dependencies across frames. SlowFast Networks [16] rely on
a similar two-branch model, but each branch encodes diﬀerent frame rates (i.e.,
diﬀerent temporal resolutions), as opposed to frames with diﬀerent spatial res-
olutions. Unlike these methods, rather than processing video frames at multiple
resolutions with specialized network branches, our approach determines optimal
resolution for each frame, with the goal of improving accuracy and eﬃciency.

3 Proposed Method

Given a video dataset D = {(Vi, yi)}N
i=1, where each video Vi contains frames
with spatial resolution 3 × H0 × W0 and is labelled from the predeﬁned classes:
yi ∈ Y = {0, 1, ..., C −1}, our goal is to create an adaptive selection strategy that
decides, per input frame, which resolution to use for processing by the classiﬁer
F : V → Y with the goal of improving accuracy and eﬃciency. To this end, we
ﬁrst present an overview of our approach in Section 3.1. Then, we show how we
learn the decision policy using Gumbel Softmax sampling in Section 3.2. Finally,
we discuss the loss functions used for learning the decision policy in Section 3.3.

Adaptive Resolution for Eﬃcient Action Recognition

5

Figure 2. Illustration of our approach. AR-Net consists of a policy network and
diﬀerent backbone networks corresponding to diﬀerent resolutions. The policy network
decides what resolution (or skip) to use on a per frame basis to achieve accuracy and
eﬃciency. In training, policies are sampled from a Gumbel Softmax distribution, which
allows to optimize the policy network via backpropagation. During inference, input
frames are ﬁrst fed into the policy network to decide the proper resolutions, then the
rescaled frames are routed to corresponding backbones to generate predictions. Finally
the network averages all the predictions for action classiﬁcation. Best viewed in color.
3.1 Approach Overview

Figure 2 illustrates an overview of our approach, which consists of a policy net-
work and backbone networks for classifying actions. The policy network contains
a lightweight feature extractor and an LSTM module that decides what reso-
lutions (or skipping) to use per input frame, for eﬃcient action recognition.
Inspired by the compound scaling method [48], we adopt diﬀerent network sizes
to handle diﬀerent resolutions, as a frame with a higher resolution should be
processed by a heavier network because of its capability to handle the detailed
visual information and vice versa.Furthermore, it is often unnecessary and inef-
ﬁcient to process every frame in a video due to large redundancy coming from
static scenes or the frame quality being very low (blur, low-light condition, etc).
Thus, we design a skipping mechanism in addition to the adaptive selection of
frame resolutions in an uniﬁed framework to skip frames (i.e., resolution set to
zero) whenever necessary to further improve the eﬃciency in action recognition.
During training, the policy network is jointly trained with the recognition
models using Gumbel Softmax sampling, as we will describe next. At test time,
an input frame is ﬁrst fed into a policy network, whose output decides the proper
resolutions, and then the resized frames are routed to the corresponding models
to generate the predictions. Finally, the network averages all the predictions as
the action classiﬁcation result. Note that the additional computational cost is
incurred by resizing operations and the policy network, which are negligible in

6

Y. Meng et al.

comparison to the original recognition models (the policy network is designed to
be very lightweight, e.g., MobileNetv2 in our case).

3.2 Learning the Adaptive Resolution Policy

Adaptive Resolution. AR-Net adaptively chooses diﬀerent frame scales to
achieve eﬃciency. Denote a sequence of resolutions in descending order as {si}L−1
i=0 ,
where s0 = (H0, W0) stands for the original (also the highest) frame resolution,
and sL−1 = (HL−1, WL−1) is the lowest resolution. The frame at time t in the
lth scale (resolution sl = (Hl, Wl)) is denoted as I l
t. We consider skipping frames
as a special case “choosing resolutions s∞”. We deﬁne the skippings sequence
(ascending order) as {Fi}M −1
i=0 , where the ith operation means to skip the cur-
rent frame and the following (Fi − 1) frames from predictions. The choices for
resolutions and skippings formulate our action space Ω.
Policy Network. The policy network contains a lightweight feature extractor
Φ(·; θΦ) and an LSTM module. At time step t < T we resize the frame It to the
lowest resolution I L−1

(for eﬃciency) and send it to the feature extractor,

t

ft = Φ(I L−1

t

; θΦ)

(1)

where ft is a feature vector and θΦ denotes learnable parameters (we use θname
for the learnable parameters in the rest of this section). The LSTM updates
hidden state ht and outputs ot using the extracted feature and previous states,

ht, ot = LSTM(ft, ht−1, ot−1; θLST M )

(2)

Given the hidden state, the policy network estimates the policy distribution
and samples the action at ∈ Ω = {0, 1, ...L + M − 1} via the Gumbel Softmax
operation (will be discussed in the next section),

at ∼ GUMBEL(ht, θG)

(3)

If at < L, we resize the frame to spatial resolution 3×Hat ×Wat and forward it to
the corresponding backbone network Ψat (·; θΨat
) to get a frame-level prediction,

yat
t = Ψat(I at

t ; θΨat

)

(4)

t ∈ R3×Hat ×Wat is the resized frame and yat

where I at
t ∈ RC is the prediction.
Finally, all the frame-level predictions are averaged to generate the video-level
prediction y for the given video V .

When the action at >= L, the backbone networks will skip the current frame
for prediction, and the following (Fat−L −1) frames will be skipped by the policy
network. Moreover, to save the computation, we share the policy network for
generating both policy and predictions for the lowest resolution, i.e., ΨL−1 = Φ4.

4 The notation here is for brevity. Actually, the output for Φ is a feature vector, whereas
the output for ΨL−1 is a prediction. In implementation, we use a fully connected layer
after the feature vector to get the prediction

Adaptive Resolution for Eﬃcient Action Recognition

7

Training using Gumbel Softmax Sampling. AR-Net makes decisions about
which resolutions (or skipping) to use per training example. However, the fact
that the decision policy is discrete makes the network non-diﬀerentiable and
therefore diﬃcult to optimize via backpropagation. One common practice is
to use a score function estimator (e.g., REINFORCE [58,22]) to avoid back-
propagating through the discrete samples. However, due to the undesirable fact
that the variance of the score function estimator scales linearly with the discrete
variable dimension (even when a variance reduction method is adopted), it is
slow to converge in many applications [61,29]. As an alternative, in this paper,
we adopt Gumbel-Softmax Sampling [29] to resolve this non-diﬀerentiability and
enable direct optimization of the discrete policy in an eﬃcient way.

The Gumbel Softmax trick [29] is a simple and eﬀective way to substitute
the original non-diﬀerentiable sample from a discrete distribution with a diﬀer-
entiable sample from a corresponding Gumbel-Softmax distribution. Speciﬁcally,
at each time step t, we ﬁrst generate the logits z ∈ RL+M −1 from hidden states
ht by a fully-connected layer z = FC(ht, θF C). Then we use Softmax to generate
a categorical distribution πt,

(cid:40)

πt =

pi

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

pi =

exp(zi)

(cid:80)L+M −1
j=0

exp(zj)

(cid:41)

(5)

With the Gumbel-Max trick [29], the discrete samples from a categorical distri-
bution are drawn as follows:

ˆp = arg max

i

(log pi + Gi),

(6)

where Gi = − log(− log Ui) is a standard Gumbel distribution with Ui sampled
from a uniform i.i.d distribution U nif (0, 1). Due to the non-diﬀerentiable prop-
erty of arg max operation in Equation 6, the Gumbel Softmax distribution [29]
is thus used as a continuous relaxation to arg max. Accordingly, sampling from
a Gumbel Softmax distribution allows us to backpropagate from the discrete
samples to the policy network. Let ˆP be a one hot vector [ ˆP0, ..., ˆPL+M −1]:

ˆPi =

(cid:40)

1,
if i = ˆp
0, otherwise

(7)

The one-hot coding of vector ˆP is relaxed to a real-valued vector P using softmax:

Pi =

exp((log pi + Gi)/τ )

(cid:80)L+M −1
j=0

exp((log pj + Gj)/τ )

,

i ∈ [0, ..., L + M − 1]

(8)

where τ is a temperature parameter, which controls the ‘smoothness‘ of the dis-
P becomes
tribution P , as

P converges to a uniform distribution and lim
τ →0

lim
τ →+∞

a one-hot vector. We set τ = 5 as the initial value and gradually anneal it down
to 0 during the training, as in [24].

8

Y. Meng et al.

To summarize, during the forward pass, we sample the decision policy us-
ing Equation 6 (this is equivalent to the process mentioned in Equation 3 and
θF C = θG) and during the backward pass, we approximate the gradient of the
discrete samples by computing the gradient of the continuous softmax relaxation
in Equation 8.

3.3 Loss Functions

During training, we use the standard cross-entropy loss to measure the classiﬁ-
cation quality as:

Lacc = E(V,y)∼Dtrain [−y log(F(V ; Θ))]

(9)

where Θ = {θΦ, θLST M , θG, θΨ0, ..., θΨL−2} and (V, y) is the training video sample
with associated one-hot encoded label vector. The above loss only optimizes
for accuracy without taking eﬃciency into account. To address computational
eﬃciency, we compute the GFLOPS for each individual module (and speciﬁc
resolution of frames) oﬄine and formulate a lookup table. We estimate the overall
runtime GFLOPS for our network based on the oﬄine lookup table GFLOPSF :
Ω → R+and online policy aV,t for each training video (V, y) ∼ Dtrain. We use the
GFLOPS per frame as a loss term to punish for high-computation operations,

Lf lops = E(V,y)∼Dtrain

(cid:34)

1
T

T −1
(cid:88)

t=0

(cid:35)

FLOPSF (aV,t)

(10)

Furthermore, to encourage the policy learning to choose more frames for
skipping, we add an additional regularization term to enforce a balanced policy
usage,

Luni =

L+M −1
(cid:88)

i=0

(cid:32)

E(V,y)∼Dtrain

(cid:34)

1
T

T −1
(cid:88)

t=0

(cid:35)
1(aV,t = i)

−

(cid:33)2

1
L + M

(11)

where 1(·) is the indicator function. Here E(V,y)∼Dtrain
resents the frequency of action i being made through the dataset. Intuitively,
this loss function term drives the network to balance the policy usage in order
to obtain a high entropy for the action distribution. To sum up, our ﬁnal loss
function for the training becomes:

rep-

1
T

(cid:20)

T −1
(cid:80)
t=0

(cid:21)
1(aV,t = i)

L = (1 − α) · Lacc + α · Lf lops + β · Luni

(12)

where α denotes respective loss weight for the computing eﬃciency, and β con-
trols the weight for the regularization term.

Adaptive Resolution for Eﬃcient Action Recognition

9

4 Experiments

In this section, we conduct extensive experiments to show that our model out-
performs many strong baselines while signiﬁcantly reducing the computation
budget. We ﬁrst show that our model-agnostic AR-Net boosts the performance
of existing 2D CNN architectures (ResNet [26], EﬃcientNet [48]) and then show
our method outperforms the State-of-the-art approaches for eﬃcient video un-
derstanding. Finally, we conduct comprehensive experiments on ablation studies
and qualitative analysis to verify the eﬀectiveness of our policy learning.

4.1 Experimental Setup

Datasets. We evaluate our approach on three large-scale action recognition
datasets: ActivityNet-v1.3 [5], FCVID(Fudan-Columbia Video Dataset)
[30]
[5] is labelled with 200 action categories
and Mini-Kinetics [32]. ActivityNet
and contains 10,024 videos for training and 4,926 videos for validation with an
average duration of 117 seconds. FCVID [30] has 91,223 videos (45,611 videos
for training and 45,612 videos for testing) with 239 label classes and the aver-
age length is 167 seconds. Mini-Kinetics dataset contains randomly selected 200
classes and 131,082 videos from Kinetics dataset [32]. We use 121,215 videos for
training and 9,867 videos for testing. The average duration is 10 seconds.
Implementation Details. We uniformly sample T = 16 frames from each
video. During training, images are randomly cropped to 224 × 224 patches with
augmentation. At the inference stage, the images are rescaled to 256 × 256 and
center-cropped to 224 × 224. We use four diﬀerent frame resolutions (L = 4) and
three skipping strategies (M = 3) as the action space. Our backbones network
consists of ResNet-50 [26], ResNet-34 [26], ResNet-18 [26], and MobileNetv2
[45], corresponding to the input resolutions 224 × 224, 168 × 168, 112 × 112,
and 84 × 84 respectively. The MobileNetv2 [45] is re-used and combined with a
single-layer LSTM (with 512 hidden units) to serve as the policy network. The
policy network can choose to skip 1, 2 or 4 frames.

Policy learning in the ﬁrst stage is extremely sensitive to initialization of the
policy. We observe that optimizing for both accuracy and eﬃciency is not eﬀec-
tive with a randomly initialized policy. Thus, we divide The training process into
3 stages: warm-up, joint-training and ﬁne-tuning. For warm-up, we ﬁx the policy
network and only train the backbones network (pretrained from ImageNet [11])
for 10 epochs with learning rate 0.02. Then the whole pipeline is jointly trained
for 50 epochs with learning rate 0.001. After that, we ﬁx the policy network
parameters and ﬁne-tune the backbones networks for 50 epochs with a lower
learning rate of 0.0005. We set the initial temperature τ to 5, and gradually an-
neal it with an exponential decay factor of -0.045 in every epoch [29]. We choose
α = 0.1 and β = 0.3 for the loss function and use SGD [47] with momentum 0.9
for optimization. We will make our source code and models publicly available.
Baselines. We compare with the following baselines and existing approaches:

• UNIFORM: averages the frame-level predictions at the highest resolution

224 × 224 from ResNet-50 as the video-level prediction.

10

Y. Meng et al.

• LSTM: updates ResNet-50 predictions at the highest resolution 224 × 224
by hidden states and averages all predictions as the video-level prediction.
• RANDOM: uses our backbone framework but randomly samples policy ac-
tions from uniform distribution (instead of using learned policy distribution).
• Multi-Scale: gathers the frame-level predictions by processing diﬀerent res-
olutions through our backbone framework (instead of selecting an optimal
resolution with one corresponding backbone at each time step). This serves
as a very strong baseline for classiﬁcation, at the cost of heavy computation.
• AdaFrame[62]: uses MobileNetV2/ResNet-101 as lightweight CNN/backbone.
• LiteEval [61]: uses MobileNetV2/ResNet-101 as Policy Network/backbone.
• ListenToLook(Image) [20]: we compared with a variant of their approach
with only the visual modality (MobileNetv2|ResNet-101). We also report
other results obtained by using audio data as an extra modality in Figure 3.
• SCSampler [33]: as oﬃcial code is not available, we re-implemented the SC-
Sampler using AC loss as mentioned in [33]. We choose MobileNetv2 as the
sampler network and use ResNet-50 as the backbone. We select 10 frames
out of 16 frames for prediction, as in [33].

Metrics. We compute the mAP (mean average precision) and estimate the
GFLOPS(gigabyte ﬂoating point operations per second) to reﬂect the perfor-
mance for eﬃcient video understanding. Ideally, a good system should have a
high mAP with only a small amount of GFLOPS used during the inference stage.
Since diﬀerent baseline methods use diﬀerent number of frames for classiﬁcation,
we calculate both GFLOPS per frame (denoted as GFLOPS/f) and GFLOPS
per video (denoted as GFLOPS/V) in the following experiments.

4.2 Main Results

Adaptive Resolution Policy improves 2D CNN. We ﬁrst compare our
AR-Net with several simple baselines on ActivityNet and FCVID datasets to
show how much performance our adaptive approach can boost in 2D convolution
networks. We verify our method on both ResNet [26] and EﬃcientNet [48] to
show the improvement is not limited to model architectures. As shown in Table 1,
comparing to traditional “Uniform” and “LSTM” methods, we save 50% of the
computation while getting a better classiﬁcation performance.

We further show that it is the adaptively choosing resolutions and skippings
that helps the most for eﬃcient video understanding tasks. Taking ResNet ar-
chitecture as an example, “Random Policy” can only reach 65.0% mAP on Ac-
tivityNet and 75.3% on FCVID, whereas AR-Net using learned policy can reach
73.8% and 81.3% respectively. Speciﬁcally, “Multi-Scale” can be a very strong
baseline because it gathers all the predictions from multi-scale inputs through
multiple backbones. It is noticeable that AR-Net’s classiﬁcation performance is
comparable to the “Multi-Scale” baseline, while using 70% less computation.
One possible explanation is that there exist noisy and misleading frames in the
videos, and AR-Net learns to skip those frames and uses the rest of the frames
for prediction. Similar conclusion can also be drawn from using EﬃcientNet
architectures, which shows our approach is model-agnostic.

Adaptive Resolution for Eﬃcient Action Recognition

11

Table 1. Action recognition results (in mAP and GFLOPS) on ActivityNet-v1.3 and
FCVID. Our method consistently outperforms all simple baselines

Approach

Arch

ActivityNet-v1.3

FCVID

mAP(%) GFLOPS/f GFLOPS/V mAP(%) GFLOPS/f GFLOPS/V

Uniform
LSTM
Random Policy
Multi-Scale
AR-Net

Uniform
LSTM
Random Policy
Multi-Scale
AR-Net

ResNet

Eﬃcient
Net

72.5
71.2
65.0
73.5
73.8

78.8
78.0
72.5
79.5
79.7

4.11
4.12
1.04
6.90
2.09

1.80
1.81
0.38
2.35
0.96

65.76
65.89
16.57
110.43
33.47

28.80
28.88
6.11
37.56
15.29

81.0
81.1
75.3
81.3
81.3

83.5
83.7
79.7
84.2
84.4

4.11
4.12
1.03
6.90
2.19

1.80
1.81
0.38
2.35
0.88

65.76
65.89
16.49
110.43
35.12

28.80
28.88
6.11
37.56
14.06

Table 2. SOTA eﬃcient methods comparison on ActivityNet-v1.3 and FCVID

Approach

ActivityNet-v1.3

FCVID

mAP(%) GFLOPS/f GFLOPS/V mAP(%) GFLOPS/f GFLOPS/V

AdaFrame [62]
LiteEval [61]
ListenToLook(Image) [20]
SCSampler [33]
AR-Net(ResNet)

AR-Net(EﬃcientNet)

71.5
72.7
72.3
72.9
73.8

79.7

3.16
3.80
5.09
2.62
2.09

0.96

78.97
95.10
81.36
41.95
33.47

15.29

80.2
80.0
-
81.0
81.3

84.4

3.01
3.77
-
2.62
2.19

0.88

75.13
94.30
-
41.95
35.12

14.06

Adaptive Resolution Policy outperforms state-of-the-art methods. We
compare the performance of AR-Net with several state-of-the-art methods on
ActivityNet and FCVID in Table 2. The result section of the table is divided
into two parts. The upper part contains all the methods using Residual Network
architecture, whereas the lower part shows the best result we have achieved by
using the latest EﬃcientNet [48] architecture. Usually it is hard to improve the
classiﬁcation accuracy while maintaining a low computation cost, but our “AR-
Net(ResNet)” outperforms all the state-of-the-art methods in terms of mAP
scores, frame-level GFLOPS and video-level GFLOPS. Our method achieves
73.8% mAP on ActivityNet and 81.3% mAP on FCVID while using 17% ∼
64% less computation budgets compared with other approaches. This shows the
power of our adaptive resolution learning approach in eﬃcient video understand-
ing tasks. When integrated with EﬃcientNet [48], our “AR-Net(EﬃcientNet)”
further gains 5.9% in mAP on ActivityNet and 3.1% on FCVID, with 54%∼60%
less computation compared to “AR-Net(ResNet)”. Since there is no published
result using EﬃcientNet for eﬃcient video understanding, these results can serve
as the new baselines for future research.

Figure 3 illustrates the GFLOPS-mAP curve on ActivityNet dataset, where
our AR-Net obtains signiﬁcant computational eﬃciency and action recognition
accuracy with much fewer GFLOPS than other baseline methods. We quote
the reported results on MultiAgent [59], AdaFrame[62] and ListenToLook[20]
(here “(IA|R)” and “(MN|R)” are short for “(Image-Audio|ResNet-101)” and
“(MobileNetV2|ResNet-101)” mentioned in [20]). The results of LiteEval [61]

12

Y. Meng et al.

with

Figure 3.
Comparisons
state-of-the-art
alternatives on Ac-
tivityNet
dataset.
Our proposed AR-
Net obtains the best
recognition accuracy
with much
fewer
GFLOPS than the
compared methods.
We
quote
the numbers reported
in published papers
and
possible
when
the mAP
compare
the average
against
test
GFLOPs
for
video. See text
more details.

directly

per

)

%

(

n
o
i
s
i
c
e
r
P
e
g
a
r
e
v
A
n
a
e
m

80

75

70

65

60

0

50

100

150

MultiAgent [59]
LiteEval [61]
AdaFrame10 [62]
AdaFrame5 [62]
ListenToLook(MN|R) [20]
ListenToLook(IA|R) [20]
ListenToLook(IA|IA) [20]
SCSampler [33]
Ours (ResNet)
Ours (EﬃcientNet)
200

250

300

GFLOPS/Video

Table 3. Results for video classiﬁcation on Mini-Kinetics dataset

Approach

Mini-Kinetics
Top1(%) GFLOPS/f GFLOPS/V

LiteEval [61]
SCSampler [33]
AR-Net(ResNet)

61.0
70.8
71.7

AR-Net(EﬃcientNet)

74.8

3.96
2.62
2.00

1.02

99.00
41.95
32.00

16.32

are generated through the codes shared by the authors, and the results of SC-
Sampler [33] are obtained by our re-implementation following their reported
details. ListenToLook (IA|R) denotes models using both visual and audio data
as inputs. Given the same ResNet architectural family, our approach achieves
substantial improvement compared to the best competitors, demonstrating the
superiority of our method. Additionally, our best performing model, which em-
ploys EﬃcientNet [48] architecture, yields more than 5% improvement in mAP
at the same computation budgets. It shows that our approach of adaptively se-
lecting proper resolutions on a per frame basis is able to yield signiﬁcant savings
in computation budget and to improve recognition precision. Further Ex-
periment on Mini-Kinetics. To further test the capability of our method,
we conduct experiments on Mini-Kinetics dataset. Compared with the recent
methods LiteEval [61] and SCSampler [33], our method achieves better Top-1
accuracy and the computation cost is reduced with noticeable margin. In brief,
our method consistently outperform the existing methods in terms of accuracy
and speed on diﬀerent datasets, which implies our AR-Net provides an eﬀective
framework for various action recognition applications.

Adaptive Resolution for Eﬃcient Action Recognition

13

Table 4. Results of diﬀerent policy settings on ActivityNet-v1.3

Policy Settings
Uniform
LSTM
Resolution Only
Skipping Only
Resolution+Skipping

mAP(%) GFLOPS/f GFLOPS/V

72.5
71.2
73.4
72.7
73.8

4.11
4.12
2.13
2.21
2.09

65.76
65.89
34.08
34.90
33.47

Table 5. Results of diﬀerent losses on ActivityNet-v1.3

Losses
Acc
Acc+Eﬀ

α
0.0
0.1
Acc+Eﬀ+Uni 0.1

β mAP(%) GFLOPS/f GFLOPS/V
3.75
0.0
2.28
0.0
2.09
0.3

60.06
36.48
33.47

74.5
73.8
73.8

Table 6. Results of diﬀerent training strategies on ActivityNet-v1.3

Training Strategy
Joint
(cid:51)
(cid:51)
(cid:51)

Finetuning
(cid:55)
(cid:55)
(cid:51)

Warm-Up
(cid:55)
(cid:51)
(cid:51)

mAP(%) GFLOPS/f GFLOPS/V

67.1
73.3
73.8

1.16
2.03
2.09

17.86
32.40
33.47

4.3 Ablation Studies

Eﬀectiveness of choosing resolution and skipping. Here we inspect how
each type of operation enhances the eﬃcient video understanding. We deﬁne
three diﬀerent action spaces: “Resolution Only” (the policy network can only
choose diﬀerent resolutions), “Skipping Only”(the policy network can only de-
cide how many frames to skip) and “Resolution+Skipping”. We follow the same
training procedures as illustrated in Section 4.1 and evaluate each approach on
ActivityNet dataset. We adjust the training loss to keep their GFLOPS at the
same level and we only compare the diﬀerences in classiﬁcation performances. As
shown in Table 4, comparing with baseline methods (“Uniform” and “LSTM”),
they all improve the performance, and the best strategy is to combine skippings
and choosing resolutions. Intuitively, skipping can be seen as “choosing zero res-
olution” for the current frame, hence gives more ﬂexibility in decision-making.
Trade-oﬀ between accuracy and eﬃciency. As discussed in Section 3.3,
hyper-parameters α and β in Equation 12 aﬀect the classiﬁcation performance,
eﬃciency and policy distribution. Here we train our model using 3 diﬀerent
weighted combinations: “Acc” (only using accuracy-related loss), “Acc+Eﬀ”(using
accuracy and eﬃciency losses) and “Acc+Eﬀ+Uni”(using all the losses). As
shown in Table 5, training with “Acc” will achieve the highest mAP, but the com-
putation cost will be similar to “Uniform” method (GFLOPS/V=65.76). Adding
the eﬃciency loss term will decrease the computation cost drastically, whereas
training with “Acc+Eﬀ+Uni” will drop the GFLOPS even further. One reason
is that the network tends to skip more frames in the inference stage. Finally, we
use hyper-parameters α = 0.1, β = 0.3 in our training.
Diﬀerent training strategies. We explore several strategies for training the
adaptive learning framework. As shown in Table 6, the best practice comes from
“Warm-Up+Joint+Finetuning” so we adopt it in training our models.

14

Y. Meng et al.

Figure 4. Qualitative examples from ActivityNet and FCVID. We uniformly sample
8 frames per video and AR-Net chooses the proper resolutions or skipping. Relevant
frames are kept in original resolution whereas non-informative frames are resized to
lower resolution or skipped for computation eﬃciency.
4.4 Qualitative Analysis

An intuitive view of how AR-Net achieves eﬃciency is shown in Figure 4. We
conduct experiments on ActivityNet-v1.3 and FCVID testing sets. Videos are
uniformly sampled in 8 frames. The upper row of each example shows original
input frames, and the lower row shows the frames processed by our policy net-
work for predictions. AR-Net keeps the most indicative frames (e.g. Futsal and
Fencing) in original resolution and resizes or skips frames that are irrelevant or
in low quality (blurriness). After being conﬁdent about the predictions, AR-Net
will avoid using the original resolution even if informative contents appear again
(e.g. Pitching a tent/Windsurﬁng). The last two examples show that AR-Net is
able to capture both object-interaction (clipper-dog) and background changes.

5 Conclusion

We have demonstrated the power of adaptive resolution learning on a per frame
basis for eﬃcient video action recognition. Comprehensive experiments show that
our method can work in a full range of accuracy-speed operating points, from a
version that is both faster and more accurate than comparable visual-only models
to a new, state-of-the-art accuracy-throughput version based on the EﬃcientNet
[48] architecture. The proposed learning framework is model-agnostic, which
allows applications to various sophisticated backbone networks and the idea can
be generally adopted to explore other complex video understanding tasks.

Acknowledgement

This work is supported by IARPA via DOI/IBC contract number D17PC00341.
The U.S. Government is authorized to reproduce and distribute reprints for

Adaptive Resolution for Eﬃcient Action Recognition

15

Governmental purposes notwithstanding any copyright annotation thereon. This
work is partly supported by the MIT-IBM Watson AI Lab.
Disclaimer: The views and conclusions contained herein are those of the au-
thors and should not be interpreted as necessarily representing the oﬃcial policies
or endorsements, either expressed or implied, of IARPA, DOI/IBC, or the U.S.
Government.

References

1. Adelson, E.H., Anderson, C.H., Bergen, J.R., Burt, P.J., Ogden, J.M.: Pyramid

methods in image processing. RCA engineer 29(6), 33–41 (1984)

2. Araujo, A., Negrevergne, B., Chevaleyre, Y., Atif, J.: Training compact deep learn-
ing models for video classiﬁcation using circulant matrices. In: Proceedings of the
European Conference on Computer Vision (ECCV). pp. 0–0 (2018)

3. Bengio, E., Bacon, P.L., Pineau, J., Precup, D.: Conditional computation in neural

networks for faster models. arXiv preprint arXiv:1511.06297 (2015)

4. Bengio, Y., L´eonard, N., Courville, A.: Estimating or propagating gradi-
ents through stochastic neurons for conditional computation. arXiv preprint
arXiv:1308.3432 (2013)

5. Caba Heilbron, F., Escorcia, V., Ghanem, B., Carlos Niebles, J.: Activitynet: A
large-scale video benchmark for human activity understanding. In: Proceedings of
the ieee conference on computer vision and pattern recognition. pp. 961–970 (2015)
6. Cai, Z., Fan, Q., Feris, R.S., Vasconcelos, N.: A uniﬁed multi-scale deep convolu-
tional neural network for fast object detection. In: European conference on com-
puter vision. pp. 354–370. Springer (2016)

7. Carreira, J., Zisserman, A.: Quo vadis, action recognition? a new model and the
kinetics dataset. In: proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition. pp. 6299–6308 (2017)

8. Chen, C.F., Fan, Q., Mallinar, N., Sercu, T., Feris, R.: Big-little net: An eﬃcient
multi-scale feature representation for visual and speech recognition. arXiv preprint
arXiv:1807.03848 (2018)

9. Chen, W., Wilson, J., Tyree, S., Weinberger, K., Chen, Y.: Compressing neural
networks with the hashing trick. In: International conference on machine learning.
pp. 2285–2294 (2015)

10. Ch´eron, G., Laptev, I., Schmid, C.: P-cnn: Pose-based cnn features for action
recognition. In: Proceedings of the IEEE international conference on computer
vision. pp. 3218–3226 (2015)

11. Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet: A large-
scale hierarchical image database. In: 2009 IEEE conference on computer vision
and pattern recognition. pp. 248–255. Ieee (2009)

12. Donahue, J., Anne Hendricks, L., Guadarrama, S., Rohrbach, M., Venugopalan,
S., Saenko, K., Darrell, T.: Long-term recurrent convolutional networks for visual
recognition and description. In: Proceedings of the IEEE conference on computer
vision and pattern recognition. pp. 2625–2634 (2015)

13. Dong, X., Huang, J., Yang, Y., Yan, S.: More is less: A more complicated net-
work with less inference complexity. In: Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition. pp. 5840–5848 (2017)

14. Fan, H., Xu, Z., Zhu, L., Yan, C., Ge, J., Yang, Y.: Watching a small portion
could be as good as watching all: Towards eﬃcient video classiﬁcation. In: IJCAI
International Joint Conference on Artiﬁcial Intelligence (2018)

16

Y. Meng et al.

15. Fan, Q., Chen, C.F.R., Kuehne, H., Pistoia, M., Cox, D.: More is less: Learning
eﬃcient video representations by big-little network and depthwise temporal ag-
gregation. In: Advances in Neural Information Processing Systems. pp. 2261–2270
(2019)

16. Feichtenhofer, C., Fan, H., Malik, J., He, K.: Slowfast networks for video recogni-
tion. In: Proceedings of the IEEE International Conference on Computer Vision.
pp. 6202–6211 (2019)

17. Feichtenhofer, C., Pinz, A., Wildes, R.P.: Spatiotemporal multiplier networks for
video action recognition. In: Proceedings of the IEEE conference on computer
vision and pattern recognition. pp. 4768–4777 (2017)

18. Figurnov, M., Collins, M.D., Zhu, Y., Zhang, L., Huang, J., Vetrov, D., Salakhut-
dinov, R.: Spatially adaptive computation time for residual networks. In: Proceed-
ings of the IEEE Conference on Computer Vision and Pattern Recognition. pp.
1039–1048 (2017)

19. Gao, M., Yu, R., Li, A., Morariu, V.I., Davis, L.S.: Dynamic zoom-in network for
fast object detection in large images. In: Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition. pp. 6926–6935 (2018)

20. Gao, R., Oh, T.H., Grauman, K., Torresani, L.: Listen to look: Action recognition

by previewing audio. arXiv preprint arXiv:1912.04487 (2019)

21. Gkioxari, G., Malik, J.: Finding action tubes. In: Proceedings of the IEEE confer-

ence on computer vision and pattern recognition. pp. 759–768 (2015)

22. Glynn, P.W.: Likelihood ratio gradient estimation for stochastic systems. Commu-

nications of the ACM 33(10), 75–84 (1990)

23. Graves, A.: Adaptive computation time for recurrent neural networks. arXiv

preprint arXiv:1603.08983 (2016)

24. Guo, Y., Shi, H., Kumar, A., Grauman, K., Rosing, T., Feris, R.: Spottune: transfer
learning through adaptive ﬁne-tuning. In: Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition. pp. 4805–4814 (2019)

25. Hara, K., Kataoka, H., Satoh, Y.: Can spatiotemporal 3d cnns retrace the history
of 2d cnns and imagenet? In: Proceedings of the IEEE conference on Computer
Vision and Pattern Recognition. pp. 6546–6555 (2018)

26. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In:
Proceedings of the IEEE conference on computer vision and pattern recognition.
pp. 770–778 (2016)

27. Howard, A.G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T., An-
dreetto, M., Adam, H.: Mobilenets: Eﬃcient convolutional neural networks for
mobile vision applications. arXiv preprint arXiv:1704.04861 (2017)

28. Iandola, F.N., Han, S., Moskewicz, M.W., Ashraf, K., Dally, W.J., Keutzer, K.:
Squeezenet: Alexnet-level accuracy with 50x fewer parameters and¡ 0.5 mb model
size. arXiv preprint arXiv:1602.07360 (2016)

29. Jang, E., Gu, S., Poole, B.: Categorical reparameterization with gumbel-softmax.

arXiv preprint arXiv:1611.01144 (2016)

30. Jiang, Y.G., Wu, Z., Wang, J., Xue, X., Chang, S.F.: Exploiting feature and class
relationships in video categorization with regularized deep neural networks. IEEE
transactions on pattern analysis and machine intelligence 40(2), 352–364 (2017)

31. Karpathy, A., Toderici, G., Shetty, S., Leung, T., Sukthankar, R., Fei-Fei, L.: Large-
scale video classiﬁcation with convolutional neural networks. In: Proceedings of
the IEEE conference on Computer Vision and Pattern Recognition. pp. 1725–1732
(2014)

Adaptive Resolution for Eﬃcient Action Recognition

17

32. Kay, W., Carreira, J., Simonyan, K., Zhang, B., Hillier, C., Vijayanarasimhan, S.,
Viola, F., Green, T., Back, T., Natsev, P., et al.: The kinetics human action video
dataset. arXiv preprint arXiv:1705.06950 (2017)

33. Korbar, B., Tran, D., Torresani, L.: Scsampler: Sampling salient clips from video for
eﬃcient action recognition. In: Proceedings of the IEEE International Conference
on Computer Vision. pp. 6232–6242 (2019)

34. Laptev, I., Marszalek, M., Schmid, C., Rozenfeld, B.: Learning realistic human
actions from movies. In: 2008 IEEE Conference on Computer Vision and Pattern
Recognition. pp. 1–8. IEEE (2008)

35. Li, H., Kadav, A., Durdanovic, I., Samet, H., Graf, H.P.: Pruning ﬁlters for eﬃcient

convnets. arXiv preprint arXiv:1608.08710 (2016)

36. Lin, J., Gan, C., Han, S.: Tsm: Temporal shift module for eﬃcient video un-
derstanding. In: Proceedings of the IEEE International Conference on Computer
Vision. pp. 7083–7093 (2019)

37. Lin, T.Y., Doll´ar, P., Girshick, R., He, K., Hariharan, B., Belongie, S.: Feature
pyramid networks for object detection. In: Proceedings of the IEEE conference on
computer vision and pattern recognition. pp. 2117–2125 (2017)

38. McGill, M., Perona, P.: Deciding how to decide: Dynamic routing in artiﬁcial neu-
ral networks. In: Proceedings of the 34th International Conference on Machine
Learning-Volume 70. pp. 2363–2372 (2017)

39. Monfort, M., Andonian, A., Zhou, B., Ramakrishnan, K., Bargal, S.A., Yan, T.,
Brown, L., Fan, Q., Gutfreund, D., Vondrick, C., et al.: Moments in time dataset:
one million videos for event understanding. IEEE transactions on pattern analysis
and machine intelligence 42(2), 502–508 (2019)

40. Monfort, M., Ramakrishnan, K., Andonian, A., McNamara, B.A., Lascelles, A.,
Pan, B., Gutfreund, D., Feris, R., Oliva, A.: Multi-moments in time: Learn-
ing and interpreting models for multi-action video understanding. arXiv preprint
arXiv:1911.00232 (2019)

41. Najibi, M., Singh, B., Davis, L.S.: Autofocus: Eﬃcient multi-scale inference. In:
Proceedings of the IEEE International Conference on Computer Vision. pp. 9745–
9755 (2019)

42. Pedersoli, M., Vedaldi, A., Gonzalez, J., Roca, X.: A coarse-to-ﬁne approach for
fast deformable object detection. Pattern Recognition 48(5), 1844–1853 (2015)
43. Perona, P., Malik, J.: Scale-space and edge detection using anisotropic diﬀusion.
IEEE Transactions on pattern analysis and machine intelligence 12(7), 629–639
(1990)

44. Piergiovanni, A., Angelova, A., Ryoo, M.S.: Tiny video networks. arXiv preprint

arXiv:1910.06961 (2019)

45. Sandler, M., Howard, A., Zhu, M., Zhmoginov, A., Chen, L.C.: Mobilenetv2: In-
verted residuals and linear bottlenecks. In: Proceedings of the IEEE conference on
computer vision and pattern recognition. pp. 4510–4520 (2018)

46. Simonyan, K., Zisserman, A.: Two-stream convolutional networks for action recog-
nition in videos. In: Advances in neural information processing systems. pp. 568–
576 (2014)

47. Sutskever, I., Martens, J., Dahl, G., Hinton, G.: On the importance of initialization
and momentum in deep learning. In: International conference on machine learning.
pp. 1139–1147 (2013)

48. Tan, M., Le, Q.V.: Eﬃcientnet: Rethinking model scaling for convolutional neural

networks. arXiv preprint arXiv:1905.11946 (2019)

18

Y. Meng et al.

49. Tran, D., Bourdev, L., Fergus, R., Torresani, L., Paluri, M.: Learning spatiotem-
poral features with 3d convolutional networks. In: Proceedings of the IEEE inter-
national conference on computer vision. pp. 4489–4497 (2015)

50. Tran, D., Wang, H., Torresani, L., Feiszli, M.: Video classiﬁcation with channel-
separated convolutional networks. In: Proceedings of the IEEE International Con-
ference on Computer Vision. pp. 5552–5561 (2019)

51. Tran, D., Wang, H., Torresani, L., Ray, J., LeCun, Y., Paluri, M.: A closer look
at spatiotemporal convolutions for action recognition. In: Proceedings of the IEEE
conference on Computer Vision and Pattern Recognition. pp. 6450–6459 (2018)
52. Veit, A., Belongie, S.: Convolutional networks with adaptive inference graphs. In:
Proceedings of the European Conference on Computer Vision (ECCV). pp. 3–18
(2018)

53. Wang, H., Kl¨aser, A., Schmid, C., Liu, C.L.: Action recognition by dense trajecto-

ries. In: CVPR 2011. pp. 3169–3176. IEEE (2011)

54. Wang, L., Xiong, Y., Wang, Z., Qiao, Y., Lin, D., Tang, X., Van Gool, L.: Tem-
poral segment networks: Towards good practices for deep action recognition. In:
European conference on computer vision. pp. 20–36. Springer (2016)

55. Wang, X., Girshick, R., Gupta, A., He, K.: Non-local neural networks. In: Pro-
ceedings of the IEEE conference on computer vision and pattern recognition. pp.
7794–7803 (2018)

56. Wang, X., Yu, F., Dou, Z.Y., Darrell, T., Gonzalez, J.E.: Skipnet: Learning dy-
namic routing in convolutional networks. In: Proceedings of the European Confer-
ence on Computer Vision (ECCV). pp. 409–424 (2018)

57. Wen, W., Xu, C., Wu, C., Wang, Y., Chen, Y., Li, H.: Coordinating ﬁlters for
faster deep neural networks. In: Proceedings of the IEEE International Conference
on Computer Vision. pp. 658–666 (2017)

58. Williams, R.J.: Simple statistical gradient-following algorithms for connectionist

reinforcement learning. Machine learning 8(3-4), 229–256 (1992)

59. Wu, W., He, D., Tan, X., Chen, S., Wen, S.: Multi-agent reinforcement learning
based frame sampling for eﬀective untrimmed video recognition. In: Proceedings
of the IEEE International Conference on Computer Vision. pp. 6222–6231 (2019)
60. Wu, Z., Nagarajan, T., Kumar, A., Rennie, S., Davis, L.S., Grauman, K., Feris,
R.: Blockdrop: Dynamic inference paths in residual networks. In: Proceedings of
the IEEE Conference on Computer Vision and Pattern Recognition. pp. 8817–8826
(2018)

61. Wu, Z., Xiong, C., Jiang, Y.G., Davis, L.S.: Liteeval: A coarse-to-ﬁne framework for
resource eﬃcient video recognition. In: Advances in Neural Information Processing
Systems. pp. 7778–7787 (2019)

62. Wu, Z., Xiong, C., Ma, C.Y., Socher, R., Davis, L.S.: Adaframe: Adaptive frame
selection for fast video recognition. In: Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition. pp. 1278–1287 (2019)

63. Yeung, S., Russakovsky, O., Mori, G., Fei-Fei, L.: End-to-end learning of action
detection from frame glimpses in videos. In: Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition. pp. 2678–2687 (2016)

64. Zhang, X., Zhou, X., Lin, M., Sun, J.: Shuﬄenet: An extremely eﬃcient convolu-
tional neural network for mobile devices. In: Proceedings of the IEEE conference
on computer vision and pattern recognition. pp. 6848–6856 (2018)

65. Zhou, B., Andonian, A., Oliva, A., Torralba, A.: Temporal relational reasoning in
videos. In: Proceedings of the European Conference on Computer Vision (ECCV).
pp. 803–818 (2018)

Adaptive Resolution for Eﬃcient Action
Recognition (Supplementary)

Table 1. Supplementary Material Overview

Section
A
B
C
D
E

Content
Details on Mini-Kinetics Dataset
GFLOPS Estimation
Policy Distributions
Qualitative Analysis
RL vs Gumbel Softmax in Policy Learning

A Mini-Kinetics

Kinetics is a large dataset containing 400 action classes and 240K training videos
that are collected from YouTube. Since the full Kinetics dataset is quite large, we
have created a smaller dataset that we call Mini-Kinetics by randomly selecting
half of the categories of Kinetics-400 dataset. The mini-Kinetics dataset contains
121K videos for training and 10K videos for testing, with each video lasting 6-10
seconds. We will make the splits publicly available to enable future comparisons.

B GFLOPS Estimation

Table 2. GFLOPS for diﬀerent backbones and resolutions

Network

Resolution GFLOPS Feature Dim

MobileNet−v2

84×84

0.0529

ResNet−18

112×112

0.4683

ResNet−34

168×168

2.2490

ResNet−50

224×224

4.1103

EﬃcientNet−b0 112×112

0.0975

EﬃcientNet−b1 168×168

0.3937

EﬃcientNet−b3 224×224

1.8000

1280

512

512

2048

1280

1280

1536

20

To estimate the overall GFLOPS for our framework, we compute a weighted
sum based on online policy distribution and an oﬄine GFLOPS look up table.
The method to compute online policy distribution is summarized in Equation
11. To generate the look up table for GFLOPS with respect to diﬀerent modules
and resolutions, we ﬁrst instantiate the speciﬁc network and then use THOP
(https://pypi.org/project/thop/) to measure the GFLOPS. The example
code snippet for computing the FLOPS for ResNet−50 at 224 × 224 frame res-
olution is given below.

t o r c h v i s i o n ,

import t o r c h ,
model = getattr ( t o r c h v i s i o n . models , ” r e s n e t 5 0 ” ) ( True )
data =( t o r c h . randn ( 1 , 3 , 2 2 4 , 2 2 4 ) , )
f l o p s ,

= thop . p r o f i l e ( model ,

i n p u t s=data )

thop

Table 2 presents all the results we need for computing GFLOPS. The GFLOPS
for LSTM is approximated by “square of the input feature dimension”, since we
only need matrix-vector multiplications. Note that when feature dimension is
around 1000 ∼ 2000, this value is normally smaller than 0.01, and hence negli-
gible to other operations.

C Distributions

Figure 1 shows the dataset-speciﬁc and category-speciﬁc policy usages for “AR-
Net(ResNet)”. Videos are uniformly sampled in 8 frames. We present policy
distribution (choosing 224×224/168×168/112×112 resolution or skipping 1/2/4
frames) in Figure 1(a), present a subset of classes sorted in relative high res-
olution usage (ratio of “choosing 224×224” over “choosing 224×224/168×168/
112×112”) in Figure 1(b) and list a subset of classes sorted in resolution us-
age ratio (ratio of “choosing 224×224/168×168/112×112” over all policies) in
Figure 1(c). Only less than 1% of frames are used in 84×84 resolution in our
experiments, so we omit “resolution 84” in Figure 1(a). On dataset level, we
observe that AR-Net skips relatively more frames on Mini-Kinetics compared to
ActivityNet and FCVID, indicating that videos in Mini-Kinetics are less motion-
informative. Moreover, on the class level, samples with complex procedures (e.g.
“making a sandwich” from ActivityNet in Figure 1(b)) are using more frames
with high resolution, compared to the samples with static objects, scenes (“light-
ning” from FCVID in Figure 1(b) and (c)) or scene-related actions (“ballet” or
“building cabinet” in Figure 1(c)), indicating that our learned decision policy
often corresponds to the diﬃculty in making predictions (i.e., diﬃcult samples
require more frames with high resolution).

Adaptive Resolution for Eﬃcient Action Recognition (Supplementary)

21

(a) Overall policy distribution on ActivityNet, FCVID and Mini-Kinetics

(b) Relative high resolution usage on ActivityNet, FCVID and Mini-Kinetics

(c) Resolution usage on ActivityNet, FCVID and Mini-Kinetics

Figure 1. Dataset-speciﬁc and category-speciﬁc policy usage for AR-Net(ResNet).

22

D Additional Qualitative Analysis

Figure 2 ∼ 4 show more qualitative results that AR-Net predicts on ActivityNet,
FCVID and Mini-Kinetics. We deﬁne “diﬃculties” (Easy, Medium and Hard)
based on their computation budgets. In general, AR-Net saves the computation
greatly for examples that contain clear appearance or actions with less motion.

Figure 2. Qualitative results on ActivityNet dataset. Videos are uniformly sampled
in 8 frames. The ﬁrst row in each example is the original video input, and the second
row represents the resolutions or skipping decisions that AR-NET chooses. We shows
ground truth labels and deﬁne “diﬃculties” (Easy, Medium and Hard) based on their
computation budgets. AR-Net can save the computation greatly for examples which
contain clear appearance or actions with less motion. Best viewed in color.

Adaptive Resolution for Eﬃcient Action Recognition (Supplementary)

23

Figure 3. Qualitative results on FCVID dataset. Videos are uniformly sampled in
8 frames. The ﬁrst row in each example is the original video input, and the second
row represents the resolutions or skipping decisions that AR-NET chooses. We shows
ground truth labels and deﬁne “diﬃculties” (Easy, Medium and Hard) based on their
computation budgets. AR-Net can save the computation greatly for examples which
contain clear appearance or actions with less motion. Best viewed in color.

24

Figure 4. Qualitative results on Mini-Kinetics dataset. Videos are uniformly sampled
in 8 frames. The ﬁrst row in each example is the original video input, and the second
row represents the resolutions or skipping decisions that AR-NET chooses. We shows
ground truth labels and deﬁne “diﬃculties” (Easy, Medium and Hard) based on their
computation budgets. AR-Net can save the computation greatly for examples which
contain clear appearance or actions with less motion. Best viewed in color.

Adaptive Resolution for Eﬃcient Action Recognition (Supplementary)

25

E RL vs Gumbel Softmax in Policy Learning

We conduct an experiment to compare diﬀerent policy learning approaches. For
the Reinforcement Learning method, we adopt the policy gradient approach and
follow the same training procedures and number of epochs used in the Gumbel
Softmax experiment. Based on the hyperparameters provided from the previous
experiment, we further tune learning rates (0.001 → 0.002 in joint-training stage;
0.0005 → 0.001 in ﬁnetuning) to get the best performance for the RL-based
method. As shown in Table 3, Gumbel Softmax approach can achieve a better
trade-oﬀ in recognition performance (less GFLOPS usage with higher mAP),
showing its eﬀectiveness over the RL-based approach.

Table 3. Performances for diﬀerent learning approaches on ActivityNet-v1.3

Approach

mAP GFLOPS/f GFLOPS/v

Policy Gradient 72.4

Gumbel Softmax 73.8

3.17

2.09

50.69

33.47

