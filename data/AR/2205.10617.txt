2
2
0
2

y
a
M
1
2

]

V
C
.
s
c
[

1
v
7
1
6
0
1
.
5
0
2
2
:
v
i
X
r
a

Gradient Concealment: Free Lunch for Defending
Adversarial Attacks

Sen Pei
NLPR, Institute of Automation
Chinese Academy of Science
peisen2020@ia.ac.cn

Xiaopeng Zhang
NLPR, Institute of Automation
Chinese Academy of Science
xpzhang@nlpr.ia.ac.cn

Jiaxi Sun
NLPR, Institute of Automation
Chinese Academy of Science
sunjiaxi2020@ia.ac.cn

Gaofeng Meng
NLPR, Institute of Automation
Chinese Academy of Science
gfmeng@nlpr.ia.ac.cn

Abstract

Recent studies show that the deep neural networks (DNNs) have achieved great
success in various tasks. However, even the state-of-the-art deep learning based
classiﬁers are extremely vulnerable to adversarial examples, resulting in sharp
decay of discrimination accuracy in the presence of enormous unknown attacks.
Given the fact that neural networks are widely used in the open world scenario
which can be safety-critical situations, mitigating the adversarial effects of deep
learning methods has become an urgent need. Generally, conventional DNNs can
be attacked with a dramatically high success rate since their gradient is exposed
thoroughly in the white-box scenario, making it effortless to ruin a well trained
classiﬁer with only imperceptible perturbations in the raw data space. For tackling
this problem, we propose a plug-and-play layer that is training-free, termed as
Gradient Concealment Module (GCM), concealing the vulnerable direction of
gradient while guaranteeing the classiﬁcation accuracy during the inference time.
GCM reports superior defense results on the ImageNet classiﬁcation benchmark,
improving up to 63.41% top-1 attack robustness (AR) when faced with adversarial
inputs compared to the vanilla DNNs. Moreover, we use GCM in the CVPR 2022
Robust Classiﬁcation Challenge 1, currently achieving 2nd place in Phase II with
only a tiny version of ConvNext. The code will be made available.

1

Introduction

The studies towards model robustness are highly valued in security-critical applications where the
reliability of DNNs against unknown inputs must be treated carefully[51]. From the current research
in [19, 40, 10, 12], DNNs have been proved to generalize well when the input data are clean and
drawn i.i.d. from the same distribution as the training set. However, in the open world scenario, DNNs
can be attacked maliciously with visually unnoticeable perturbations, resulting in severe performance
decay and security risk (see Figure 1). For remedying DNNs from vulnerability when being attacked,
previous works in [7, 21, 47, 32, 14, 37, 49] have made a great contribution to this line of research,
trying to promote DNNs much more robust in the presence of perturbed images. Unfortunately, these
methods only focus on some speciﬁc attacks which can not be generally used in various applications,
and as a result, leave potential risks in complicated unknown situations. In this paper, we propose to

1https://aisafety.sensetime.com

Preprint. Under review.

 
 
 
 
 
 
conceal the vulnerable direction of the classiﬁer’s gradient for improving its reliability, protecting
DNNs from gradient based attacks.

Figure 1: Attacking DNNs with FGSM [7]. Top: Vanilla ResNet50 [10] trained on ImageNet 2012
dataset [36]. Bottom: Identical ResNet50 with our proposed GCM. The left images are correctly
classiﬁed as go-cart. The center images are adversarial perturbations generated by FGSM. We
randomly choose crossword puzzle as the target class after being attacked, and the perturbations
are obtained by minimizing the cross entropy loss between the output of ResNet and the target
one-hot label. It can be seen that the perturbed image in the ﬁrst line is misclassiﬁed as forklift with a
conﬁdence up to 0.4854 by the vanilla ResNet50. For contrast, with our proposed GCM, the identical
ResNet50 can defend these attacks effortlessly.

Currently, there is now a sizable body of work proposing various attack schemes to ruin the superior
performance of DNNs, and unfortunately, most adversarial attacks meet their need. Generally
speaking, the adversarial attack can be viewed in two manners, which are the black-box attack and
the white-box attack. In the case of the former fashion, noisy images generated with only a limited
(or without) knowledge of the model are fed into the classiﬁer for testing, and the data augmentation
methods can help to deal with this situation [1]. By comparison, the white-box attack is much more
aggressive since it has access to the complete details of the target model, including its parameter
values, architecture, training method, and in some cases its training data as well. In all but a few
cases, white-box attacks are gradient based attack methods which can be roughly divided into two
categories: the single-step attack and the iterative attack. As in [47], the perturbed images generated
by iterative methods such as DeepFool [29] may easily get over-ﬁtted to the target neural network,
resulting in stronger but less transferable attacks. On the contrary, the single-step attack such as
FGSM [7] narrowly suffers from the aforementioned issue, but its generated perturbations may not
be aggressive enough to fool the target classiﬁer. All told, concealing the gradients of DNNs is one of
the key solutions to prevent classiﬁers from being attacked, leading to both stable performance and
better generalization ability in the presence of unknown adversarial attacks.

Towards this end, we propose using GCM to hide the direction of gradients for mitigating adversarial
effects, protecting DNNs from malicious attacks. GCM is a plug-and-play module placed in an
arbitrary layer of DNNs which generates marginal magnitude and enormous gradient, resulting in a
negligible change in feature maps while signiﬁcant disturbance in the sign of gradient. The proposed
GCM is parameter-free, introducing no classiﬁcation performance decay during both the training and
inference time. The main contributions of this work are summarized as follows:

• A parameter-free module termed GCM is proposed for defending adversarial attacks without
costly training, increasing up to 63.41% attack robustness (AR) in the presence of gradient
based attacks compared to previous methods.

2

• We test the proposed GCM on ImageNet classiﬁcation benchmarks with ResNet [21],
WideResNet [53], DenseNet [12], EfﬁcientNet [43] and different transformer architectures
such as ViT [5] and Swin-Transformer [25]. These sufﬁcient results suggest that GCM
improves the performance of adversarial defense stably, making it more reliable to apply
neural networks in security-critical areas.

• Unlike prior works, we pose a brand new perspective to explore the solution of adversarial
defense in deep learning, leading to defense schemes with better generalization ability.

2 Related Work

In this section, we give an overview of adversarial attacks and some current defense methods for
improving the robustness of deep neural networks.

Adversarial Attacks are severe threats for deploying machine learning systems in the security-
critical scenario. In [42], adversarial examples were ﬁrst noticed. It is found that adding visually
undetectable noise to the clean images can result in the misclassiﬁcation of deep neural networks.
With L2 distance metric, these adversarial images can be obtained through box-constrained L-BFGS
[23]. Later, in [7], a single-step attack approach named Fast Gradient Sign Method (FGSM) is
proposed to produce adversarial examples efﬁciently using the sign of gradient under the constraint of
L∞ distance metric. In [20], FGSM is reﬁned in an iterative manner taking multiple aggressive steps
for attacking, reporting superior results compared to the vanilla FGSM. PGD [27] is also an iterative
attack technique using the ﬁrst-order gradient of the target classiﬁers. It clips the perturbations in each
iteration within a (cid:15) ball. Jacobian-based Saliency Map Attack (JSMA) introduced in [31] is a greedy
attack method optimized with L0 distance. The proposed saliency map models the impact each pixel
has on the corresponding classiﬁcation result, and JSMA modiﬁes the pixel with the highest saliency
for improving the likelihood of the desired label. Similarly, [41] ﬁnds that modifying only one pixel
can also destroy the vulnerable deep neural networks, raising more concern about AI Safety. In [29],
DeepFool proposes generating closer adversarial examples under the constraint of L2 distance metric,
providing a better understanding of adversarial effects. C&W [2] aims to ﬁnd some imperceptible
noise that minimizes the difference between the perturbed image and its clean counterpart under
some desired distance metric while fooling the well-trained classiﬁers. C&W is an iterative gradient
based attack method with an extremely high success rate for ruining current defense approaches such
as FGSM regularization [7] and defensive distillation [32]. Alongside, studies in [28, 24, 52] focus
on the transferability of adversarial examples. UAP [28] is a systematic algorithm for computing
architecture-agnostic perturbations, revealing the geometric correlation between different parts of the
decision boundary. More recently, a plug-and-play module named MGAA [52] proposes generating
universal noise iteratively with the meta-train step and the meta-test step, improving the cross-model
transferability of these perturbations. Different from the aforementioned approaches, AdvDrop [6]
crafts adversarial samples by dropping existing details of images, making defense a much more
difﬁcult task.

Adversarial Defense is an opposite research direction to mitigate the adversarial effects. The
progress in defending against attacks is not such signiﬁcant compared to the generation of adversarial
examples. In [50], feature squeezing is proposed to detect adversarial examples by comparing DNN
model’s prediction on the original input with that of squeezed inputs. Defense-GAN [38] proposes
using a generator to denoise the adversarial examples by minimizing the reconstruction error between
these perturbed images and the synthetic images generated by the aforementioned generator. Unlike
previous adversarial training schemes such as TRADES [55], [54] generates adversarial images
for training through feature scattering in latent space, avoiding label leaking. [15] claims that the
vulnerability of DNNs is caused directly by the non-robust features in images, and capturing these
features can reduce the success rate of adversarial attacks. Defensive distillation proposed in [33]
ﬁnds that model distillation can also reduce the effectiveness of adversarial samples on DNNs. [16]
proposes a recursive generator that can produce much stronger adversarial perturbations for training,
revealing the vulnerability of the target classiﬁer. [30] argues that the close proximity of different
class samples in the learned feature space causes the poor performance of DNNs in the presence
of imperceptible adversarial noise, and thus restricting the hidden space of DNNs help to defend
against these attacks. Sparse coding [22] ﬁnds that projecting images into its proposed quasi-natural
image space can remove the adversarial perturbations, and this method is attack-agnostic which
has good generalization ability. HGD [22] is a denoiser that uses a loss function deﬁned as the

3

difference between the target classiﬁer’s outputs activated by the clean image and its corresponding
denoised image, removing adversarial perturbations adaptively. Different from the aforementioned
training schemes, the randomization layer proposed in [48] is a post-processing method without
trainable parameters. It invalidates the adversarial attacks by changing the attacked location and pixel
values through image resizing and random padding. Compared to the above techniques, the proposed
training-free GCM has better generalization ability and much stronger defense capability, making
gradient based attacks ineffective.

3 Approach

At the beginning of this section, we detail the adversarial attack methods used in the following
experiments. Immediately, we elaborate the proposed Gradient Concealment Module formally.

3.1 Gradient based Adversarial Attacks

Before introducing the proposed GCM, we give an overview of gradient based adversarial attack
methods formally. Suppose x and y are the legitimate (i.e., clean) image without perturbations
and its corresponding ground truth label. Let L(x, y, θ) denote the loss function of target classiﬁer
parameterized by θ. The perturbed image ˆx = x + r is obtained through different attack methods
where r indicates the adversarial noise with the constraint of ||r||p ≤ (cid:15). Here, we use || · ||p to
represent the Lp norm. The desired label after being attacked is ˆy. We consider three adversarial
attacks in the white-box fashion which are FGSM [7], PGD [27] and C&W [2] respectively.

Fast Gradient Sign Method (FGSM) [7] is a basic single-step attack method using the sign of
gradient. Usually, FGSM is treated in two fashions which are target-attack and untarget-attack. The
former case desires to change the predicted label of input x to some speciﬁc target labels. The latter
case only wants to make the model misclassify the perturbed images. Formally, the untarget-attack is
built as follows with the constraint of inﬁnite norm (cid:15):

ˆx = x + (cid:15) · sgn(∇xL(x, y, θ))
Opposite to the above equation, target-attack can be established using gradient descend method as:
ˆx = x − (cid:15) · sgn(∇xL(x, ˆy, θ))

(2)

(1)

Projected Gradient Descent Method (PGD) [27] proposes an uniﬁed framework for generating
adversarial examples together with the adversarial training process. The objective of PGD is to solve
a min-max optimization problem shown as follows:

min
θ

ρ(θ) = E(x,y)∼D max
||r||p≤(cid:15)

L(x + r, y, θ)

(3)

where D is the training data, θ is the parameters of the target classiﬁer and p is the order of norm.

C&W [2] is an iterative attack method. With an auxiliary variable w, it ﬁnds adversarial perturbations
r as:

1
2
where the perturbed image ˆx = x + r = 1
2 (tanh(w) + 1). C&W aims to minimize the Lp norm of
noise r while making the model misclassify the aforementioned image ˆx. Formally, the uniﬁed loss
function is in the following form [48]:

(tanh(w) + 1) − x

r =

(4)

min
w

||r||p + c · max{f (ˆx)y − f (ˆx)(cid:54)=y, −k}

(5)

where f (x)y denotes the predicted conﬁdence of x matching with label y, and k is a desired conﬁdence
gap between the ground truth label y and any other misclassiﬁed labels.

3.2 Gradient Concealment Module

GCM is used to conceal the vulnerable gradient direction of the vanilla classiﬁers, making gradient
based attacks completely ineffective. During the training phase, the classiﬁer is trained as usual with
the backpropagation method. In the inference time, one just needs to place the GCM at the top (or any
other layers) of the trained neural networks. Since the magnitude of output from GCM is extremely
minute so that no obvious change is introduced to the classiﬁcation accuracy.

4

Figure 2: The proposed GCM framework. Gradient concealment module is a plug-and-play layer,
and it can greatly reduce the magnitude of input data while generating enormous gradient. w is the
frequency of trigonometric function, and ε is a extremely tiny weight. When setting w · ε far greater
than 1, the vulnerable direction of classiﬁer’s gradient is concealed, providing stable and superior
performance in the presence of adversarial attacks. {xi} and {yi} are the clean training images and
labels, fθ(·) is a deep neural network parameterized by θ and Lcls is the category cross entropy loss.

Formally, let x be the input image, and let f (·)
denote the deep neural network. In Figure 3, the
upper part shows the calculation ﬂow of conven-
tional DNNs, and the lower half of the ﬁgure
describes that of our proposed GCM. Since the
parameter ε is extremely small, the forward pro-
cess is almost identical in both the vanilla model
and its GCM counterpart. We consider the back-
ward process in the attack scenario. For the
vanilla model, we have:

Figure 3: Calculation ﬂow of the vanilla model
and its GCM counterpart. f (·) is the classiﬁer
and g(·) indicates the proposed GCM.

(

∂L
∂x

)vanilla =

∂L
∂f

·

∂f
∂x

(6)

where L denotes the category cross entropy loss. Similarly, the gradient that L towards input x in
GCM is obtained with chain rule:
∂g
∂x

)vanilla · (1 + εw cos(wx))

)vanilla ·

∂L
∂f

∂L
∂x

∂L
∂x

∂L
∂x

)GCM =

∂g
∂x

∂f
∂g

= (

≈ (

(7)

(

·

·

When setting εw far greater than 1, the last term in Eq.(7) is dominated by εw cos(wx), and therefore,
concealing the original gradient’s direction shown in Eq.(6). GCM has no trainable parameters which
can be used during the inference time directly. At a high level, the overall pipeline of the proposed
GCM is shown in Algorithm 1.

Algorithm 1: Gradient concealment module for defending adversarial attacks.
Input: training data D, classiﬁer fθ(·) parameterized by θ, w and ε for GCM
Output: trained neural network fθ(·)

1 while Training do
2

Sample a batch data {xi, yi}i=1:K from D;
Get the corresponding output logits : y∗
i = fθ(xi);
Calculate the cross entropy loss Lcls between (y∗
Update θ with gradient descent method.

i , yi)i=1:K;

6 while Inference do
7

Sample {x, y} from the testing set;
Cascade : gθ(x) = [x + GCM(x), fθ(·)];
Adversarial attack : ˆx = attack(x, gθ, y);
Evaluation : ˆy = gθ(ˆx).

3

4

5

8

9

10

5

4 Experiments

Starting with the experimental setup, we tell the evaluation metrics and report the adversarial defense
results on ImagNet [36] and CIFAR-10 [18] with different model architectures including CNN-based
methods and attention-based methods immediately. Following this, we detail the ablation study and
give some auxiliary visualization results.

4.1 Experimental Setup

Dataset. We use the whole testing set from ImageNet [36] 2012 for evaluation. This testing set
contains 50k images in 1k categories. The short edge of all images is resized to 256 ﬁrst, maintaining
the aspect ratio, and then the testing inputs are obtained through cropping at the center of images.
The size of inputs after pre-processing is 224 × 224 × 3 with RGB channels. Besides, we also report
the defense results on CIFAR-10 [18] compared to different defending methods, and no modiﬁcations
are introduced to these images. The size of inputs is 32 × 32 × 3 with RGB channels.

Model Architectures. We employ sufﬁcient model architectures to test the reliability and gen-
eralization performance of the proposed GCM. For convolution based models, we adopt ResNet-
50 [10], WideResNet-50 [53], DenseNet-121 [12], ShufﬂeNetV2 [26], EfﬁcientNet-B4 [43] and
MobileNetV3 [11] from 2016 to 2019. For the attention based architecture which is currently a hot
research topic, we use DeiT-S [45], ViT-B/16 [5] and Swin-Transformer-S [25] for comparison. All
these models are trained on ImageNet 2012 training set, and their weights can be downloaded at this
web page 2. The details of training process are stated clearly in RobustART [44].

Evaluation Metrics. We use the top-1 classiﬁcation accuracy (ACC) and the attack robustness
(AR) [44] to evaluate the defense ability of target classiﬁers. To formalize these metrics, let
D = {(x1, y1), (x2, y2), ..., (xN , yN )} denote the testing set, and let fθ(·) indicate the classiﬁer
parameterized by θ. The corresponding perturbed image of xi is ˆxi. With these notations, the
classiﬁcation accuracy is deﬁned as:

ACC =

1
N

N
(cid:88)

i=1

I(fθ(xi) = yi)

(8)

where I(·) is a binary indicator function whose value equals to 1 if and only if the inner condition
is true. It is meaningless to attack the images which are already misclassiﬁed by the target model.
Consequently, attack robustness (AR) only concerns the defense performance of model on the
correctly classiﬁed sub testing set. AR is formulated as follows:

(cid:80)N

i=1

AR =

I(fθ(xi) = yi) · I(fθ( ˆxi) = yi)
I(fθ(xi) = yi)

(cid:80)N

i=1

(9)

Adversarial Attack Methods. We consider three adversarial attack methods which are FGSM [7],
PGD [27] and C&W [2]. For single-step adversarial attack method FGSM, the perturbations r are
constrained with L∞ norm. For PGD, we report results under the constraint of L1 norm, L2 norm
and L∞ norm respectively. For iterative attack method C&W, we set the binary search steps to 10
with a learning rate 1e-2. The iteration steps of optimization is limited to 10.

4.2 Main Results of Adversarial Defense

We report the adversarial defense results on CIFAR-10 [18] and ImageNet [36] 2012 Testing Set.
Top-1 classiﬁcation accuracy (ACC) is used to evaluate the performance of classiﬁers on clean images,
and attack robustness (AR) is used to measure the stability and reliability of models in presence of
adversarial attacks.

Adversarial Defense on ImageNet [36]. Table 1 shows the classiﬁcation result and the attack
robustness result on ImageNet. ACC on clean images indicates the top-1 classiﬁcation accuracy
of the following models on the ImageNet testing set. All other metrics are attack robustness (AR)
evaluated only on the correctly classiﬁed testing images. For each model in the ﬁrst half of Table 1,

2http://robust.art/source

6

we set a corresponding GCM counterpart for comparison. One can notice the superior performance
of GCM clearly by comparing the groups with and without GCM. The hyper-parameters w and ε
in GCM are set to 1e20 and 1e-8 respectively. In particular, with FGSM attack method under the
constraint of L∞ ≤ 8
255 , GCM dramatically improves the AR metric up to 63.41% compared to the
vanilla ResNet. Specially, due to the relatively high time cost in C&W attack, we randomly choose
20 images per category in the ImageNet testing set for evaluation.

Table 1: Adversarial defense results on ImageNet [36]. It can be seen clearly that GCM signiﬁ-
cantly improves the defense ability towards gradient based attacks while maintaining the classiﬁcation
accuracy of the input images. The table in light gray background denotes the results obtained with
GCM. Values below are attack robustness if not speciﬁed.

ImageNet [36]

2012 Testing Set

ACC on

Clean Images

ResNet-50 (2016) [10]

WideResNet-50 (2016) [53]

DenseNet-121 (2017) [12]

ShufﬂeNetV2-x2.0 (2018) [26]

EfﬁcientNet-B4 (2019) [43]

MobileNetV3-x1.4 (2019) [11]

DeiT-S (2021) [45]

ViT-B/16 (2021) [5]

Swin-Transformer-S (2021) [25]

ResNet-50 (Ours) [10]

WideResNet-50 (Ours) [53]

DenseNet-121 (Ours) [12]

ShufﬂeNetV2-x2.0 (Ours) [26]

EfﬁcientNet-B4 (Ours) [43]

MobileNetV3-x1.4 (Ours) [11]

DeiT-S (Ours) [45]

ViT-B/16 (Ours) [5]

Swin-Transformer-S (Ours) [25]

77.890

78.212

74.858

72.286

71.520

73.636

79.900

79.462

82.930

78.571

78.082

74.711

71.910

71.759

73.389

79.931

79.466

82.785

L∞
33.560

26.077

20.016

8.662

2.052

10.568

20.684

31.640

25.425

98.797

99.122

98.900

98.571

98.328

98.225

97.907

98.275

98.271

FGSM [7]

PGD [27]

C&W [2]

2
255 L∞

8
255 L1 400 L1 1600 L2 2.0 L2 8.0 L∞

2
255 L∞

31.773

20.879

16.815

7.500

1.233

9.245

8.828

15.862

16.926

3.286

3.250

1.130

0.138

5.819

0.399

1.585

3.522

0.766

0.012

0.358

0.035

0.000

0.357

0.000

0.000

0.003

0.200

2.128

0.010

2.408

0.611

0.587

0.053

0.088

0.000

0.769

0.276

0.105

0.000

0.881

0.000

1.929

0.000

0.656

0.000

1.450

2.250

0.408

0.085

0.363

0.152

0.990

2.511

0.964

95.183

98.855

94.819

98.923 94.406

97.579

96.057

98.752

94.463

98.876 94.505

98.240

94.977

98.858

94.314

98.829 94.081

97.738

92.151

98.525

92.114

98.625 92.206

97.320

94.677

98.831

89.953

98.993 90.865

98.321

93.358

99.094

92.432

99.025 91.962

98.055

91.469

99.046

93.875

99.123 93.950

98.286

92.238

99.290

94.937

99.287 95.071

98.602

94.377

98.156

90.708

98.350 91.044

98.178

8
255 L2 8.0
0.211

0.000

0.496

0.056

0.000

0.198

0.000

0.000

0.000

0.000

97.376

97.687

97.156

97.054

97.968

97.303

97.984

98.239

96.766

0.213

0.116

0.119

1.884

0.236

0.644

0.902

0.757

95.108

95.658

95.486

98.566

93.068

92.761

91.512

93.314

92.306

Adversarial Defense on CIFAR-10 [18]. In the previous section, we compare the model robustness
in the presence of adversarial attacks between the vanilla classiﬁer and its GCM counterpart. In this
part, we further discuss the performance of the proposed GCM and some existing defending methods.
Formally, we report the results of several canonical defending schemes with and without GCM
including Unlabeled [3], Limits Uncovering [8], Weight Perturbation [46], Fixing Data Aug [35],
Extra Helper [34], Stable Neural ODE [17], Exploring EMA [13], Proxy Distributions [39] and Data
Generation [9]. We use the pre-trained models from RobustBench [4] which can be downloaded at
this web page 3. Only the classiﬁcation accuracy (ACC) is adopted as the evaluation metric since no
attack robustness (AR) values are given in [4]. Besides, all these pre-trained models are designed
for defending against PGD attack under the constraint of L∞ norm, and therefore, we only compare
these classiﬁers in the single PGD attack manner. w and ε are set to 1e20 and 1e-8 respectively.

From the results shown in Table 2, it can be noticed that the proposed GCM is stable in presence of
perturbations with different magnitudes. In particular, when limited the L∞ norm is no more than
10
255 , GCM achieves a superiority with up to 35.02% improvement in classiﬁcation accuracy compared
to the Unlabeled [3] scheme. Generally, FGSM [7] is less aggressive compared to PGD [27] attack,
and therefore, we eliminate these redundancy experiments on CIFAR-10 [18].

4.3 Ablation Study

Ablation on magnitude ε. First, we ﬁx the value of frequency w to 1e20, and the ε is situated
between 1e-8 and 1e-3. It is worth noting that the value of ε should be sufﬁciently small in case of
introducing intolerable perturbations. We use ResNet-50 [10] and ViT [5] to test the sensitivity of the
model to parameter ε. We report the AR metrics for evaluating the model’s robustness.

3https://github.com/RobustBench/robustbench

7

Table 2: Adversarial defense results on CIFAR-10 [18]. Compared to the current defending
methods shown in the following table, GCM is much more stable in handling perturbations with
different magnitudes, achieving a superior performance when stated with perturbed images. Values
below are classiﬁcation accuracy on CIFAR-10.

CIFAR-10 [18]
Testing Set
Unlabeled (2019) [3]
Uncovering Limits (2020) [8]
Weight Perturbation (2020) [46]
Fixing Data Aug (2021) [35]
Extra Helper (2021) [34]
Stable Neural ODE (2021) [17]
Exploring EMA (2021) [13]
Proxy Distributions (2021) [39]
Data Generation (2021) [9]
Unlabeled (Ours) [3]
Uncovering Limits (Ours) [8]
Weight Perturbation (Ours) [46]
Fixing Data Aug (Ours) [35]
Extra Helper (Ours) [34]
Stable Neural ODE (Ours) [17]
Exploring EMA (Ours) [13]
Proxy Distributions (Ours) [39]
Data Generation (Ours) [9]

ACC on
Clean Images
89.69
91.10
88.25
92.23
91.47
93.73
91.23
86.68
87.50
90.30
90.82
88.36
92.82
91.51
94.09
91.45
86.91
87.91

2
255
85.90
88.60
86.00
88.80
87.30
94.00
87.80
84.20
83.70
90.31
90.50
88.50
92.90
91.50
94.10
91.30
86.80
87.90

PGD [27] L∞
6
255
72.95
81.70
77.00
78.10
75.00
93.30
80.00
77.90
74.90
90.27
90.70
88.10
92.50
91.79
94.30
90.90
87.00
87.70

4
255
79.10
85.50
82.40
84.70
82.10
93.80
84.30
81.20
79.30
90.52
90.40
88.60
92.70
91.60
94.10
91.30
86.90
87.90

8
255
64.61
76.60
72.10
70.20
66.70
92.40
75.80
73.10
70.60
90.21
90.80
88.00
92.60
91.73
93.90
91.00
86.90
87.60

10
255
55.17
71.80
67.20
63.30
57.30
91.90
70.00
68.60
65.60
90.19
90.30
87.90
92.50
91.52
93.90
90.70
86.60
87.40

Table 3: Ablation on magnitude ε. The frequency w is set to 1e20. The magnitude of perturbations
is limited to 8

255 with the L∞ norm. Values below are AR results on ImageNet.

ε

1e-3

ResNet-50 [10]
1e-4

1e-5

1e-6

1e-7

1e-8

1e-3

1e-4

ViT-B/16 [5]
1e-6
1e-5

1e-7

1e-8

FGSM [7] 90.29 90.92 91.02 91.02 91.01 91.04 81.74 83.33 83.52 83.53 83.52 83.54
PGD [27] 78.96 94.90 97.03 97.24 97.19 97.25 77.21 95.41 96.76 96.98 96.96 97.00

It can be concluded based on the results shown in Table 3 that GCM is insensitive to the magnitude
ε, leading to better stability and reliability in complicated scenarios. Generally, randomly chosen ε
from 1e-8 to 1e-4 is enough to yield a promising result.

Ablation on frequency w. We set the value of magnitude ε to 1e-8, and the frequency w is situated
from 1e10 to 1e20. The value of w should be sufﬁciently great for concealing the gradient direction
of the vanilla classiﬁers. We use the identical models and evaluation metrics as Table 3.

Table 4: Ablation on frequency w. The magnitude of perturbations is limited to 8
norm. Values below are AR results on ImageNet.

255 with the L∞

ResNet-50 [10]

w

1e10 1e12 1e14 1e16 1e18 1e20

ViT-B/16 [5]
1e10 1e12 1e14 1e16 1e18 1e20

FGSM [7] 95.07 94.52 94.91 95.65 93.12 94.02 76.49 74.80 73.44 76.76 74.54 74.52
PGD [27] 96.64 95.79 95.08 95.47 96.58 97.24 76.97 74.22 75.67 75.73 75.09 77.92

In Table 4, we notice that the changing tendency of AR metrics along with the frequency w is not
such clear when the magnitude of w is sufﬁciently large. Randomly chosen frequency w from 1e10

8

to 1e20 yields considerable performance on defending unknown attacks. If not speciﬁed, we set ε
and w to 1e-8 and 1e20.

Ablation on the position of GCM. GCM can be placed at an arbitrary layer of the deep neural
networks. We consider two fashions to place the GCM: (1) Front: set the GCM as the top layer of
DNNs; (2) Middle: place GCM immediately after some speciﬁc convolutional layers.

Table 5: Ablation on the position of GCM. The magnitude of perturbations are limited within 8
255
under the L∞ norm. Block indicates the ConvBlock in ResNet-50 [10]. Values below are ACC on
ImageNet. Placing GCM in all layers of the classiﬁer yields the best result.

Location

FGSM [7]
PGD [27]

Front

Block1 Block2 Block3 Block4 Block5 All Layers

77.721
77.857

73.892
77.051

72.682
76.093

70.333
73.690

67.284
67.542

65.478
65.303

78.572
78.574

4.4 Auxiliary Visualization Results

GCM defends deep neural networks against the adversarial attacks through concealing the gradient’s
direction of the target models. With the goal of visualizing the modiﬁcation GCM resulted, we use
black points (pixel 0) to indicate the negative direction (i.e.,-1) of the gradient, and let white points
(pixel 1) denote the positive direction (i.e.,1) of that. Occasionally, the gradient of some pixels may be
zero, and we set these positions with a value 1
2 . The perturbations got through FGSM are illustrated
as a gray map within each RGB channel, and the result merging these three channels is still shown as
a color image. Following ﬁgure shows the visualization results.

Figure 4: Visualization of the sign of gradients. It can be seen that the gradient generated by GCM
is much more dispersed compared to the vanilla classiﬁer, weakening the adversarial effects.

5 Conclusion

In this paper, with the goal of mitigating adversarial effects of DNNs, we propose a novel module
termed as GCM to conceal the sign of models’ gradient, protecting conventional neural networks
from adversarial perturbations. Sufﬁcient experiments on different architectures with and without
defending schemes indicate the superior defense ability and stability of our proposed GCM. We are
looking forward to see more innovative studies removing obstacles for reliable deployment of DNNs.

9

References

[1] Naveed Akhtar and Ajmal S. Mian. Threat of adversarial attacks on deep learning in computer vision: A

survey. IEEE Access, 6:14410–14430, 2018.

[2] Nicholas Carlini and David A. Wagner. Towards evaluating the robustness of neural networks. In 2017
IEEE Symposium on Security and Privacy, SP 2017, San Jose, CA, USA, May 22-26, 2017, pages 39–57.
IEEE Computer Society, 2017.

[3] Yair Carmon, Aditi Raghunathan, Ludwig Schmidt, John C. Duchi, and Percy Liang. Unlabeled data
improves adversarial robustness. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence
d’Alché-Buc, Emily B. Fox, and Roman Garnett, editors, Advances in Neural Information Processing
Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December
8-14, 2019, Vancouver, BC, Canada, pages 11190–11201, 2019.

[4] Francesco Croce, Maksym Andriushchenko, Vikash Sehwag, Edoardo Debenedetti, Nicolas Flammarion,
Mung Chiang, Prateek Mittal, and Matthias Hein. Robustbench: a standardized adversarial robustness
benchmark. In Thirty-ﬁfth Conference on Neural Information Processing Systems Datasets and Benchmarks
Track, 2021.

[5] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas
Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and
Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In 9th
International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021.
OpenReview.net, 2021.

[6] Ranjie Duan, Yuefeng Chen, Dantong Niu, Yun Yang, A. Kai Qin, and Yuan He. Advdrop: Adversarial
attack to dnns by dropping information. In 2021 IEEE/CVF International Conference on Computer Vision,
ICCV 2021, Montreal, QC, Canada, October 10-17, 2021, pages 7486–7495. IEEE, 2021.

[7] Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples.
In Yoshua Bengio and Yann LeCun, editors, 3rd International Conference on Learning Representations,
ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015.

[8] Sven Gowal, Chongli Qin, Jonathan Uesato, Timothy A. Mann, and Pushmeet Kohli. Uncovering the limits
of adversarial training against norm-bounded adversarial examples. CoRR, abs/2010.03593, 2020.

[9] Sven Gowal, Sylvestre-Alvise Rebufﬁ, Olivia Wiles, Florian Stimberg, Dan Andrei Calian, and Timothy A.

Mann. Improving robustness using generated data. CoRR, abs/2110.09468, 2021.

[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.
In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA,
June 27-30, 2016, pages 770–778. IEEE Computer Society, 2016.

[11] Andrew Howard, Ruoming Pang, Hartwig Adam, Quoc V. Le, Mark Sandler, Bo Chen, Weijun Wang, Liang-
Chieh Chen, Mingxing Tan, Grace Chu, Vijay Vasudevan, and Yukun Zhu. Searching for mobilenetv3. In
2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019, Seoul, Korea (South), October
27 - November 2, 2019, pages 1314–1324. IEEE, 2019.

[12] Gao Huang, Zhuang Liu, Laurens van der Maaten, and Kilian Q. Weinberger. Densely connected
convolutional networks. In 2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR
2017, Honolulu, HI, USA, July 21-26, 2017, pages 2261–2269. IEEE Computer Society, 2017.

[13] Hanxun Huang, Yisen Wang, Sarah Monazam Erfani, Quanquan Gu, James Bailey, and Xingjun Ma.
Exploring architectural ingredients of adversarially robust deep neural networks. CoRR, abs/2110.03825,
2021.

[14] Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan Engstrom, Brandon Tran, and Aleksander Madry.
Adversarial examples are not bugs, they are features. In Hanna M. Wallach, Hugo Larochelle, Alina
Beygelzimer, Florence d’Alché-Buc, Emily B. Fox, and Roman Garnett, editors, Advances in Neural
Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019,
NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages 125–136, 2019.

[15] Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan Engstrom, Brandon Tran, and Aleksander Madry.
Adversarial examples are not bugs, they are features. In Hanna M. Wallach, Hugo Larochelle, Alina
Beygelzimer, Florence d’Alché-Buc, Emily B. Fox, and Roman Garnett, editors, Advances in Neural
Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019,
NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages 125–136, 2019.

10

[16] Yunseok Jang, Tianchen Zhao, Seunghoon Hong, and Honglak Lee. Adversarial defense via learning to
generate diverse attacks. In 2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019,
Seoul, Korea (South), October 27 - November 2, 2019, pages 2740–2749. IEEE, 2019.

[17] Qiyu Kang, Yang Song, Qinxu Ding, and Wee Peng Tay. Stable neural ODE with lyapunov-stable

equilibrium points for defending against adversarial attacks. CoRR, abs/2110.12976, 2021.

[18] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.

[19] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classiﬁcation with deep convolutional
neural networks. In Peter L. Bartlett, Fernando C. N. Pereira, Christopher J. C. Burges, Léon Bottou,
and Kilian Q. Weinberger, editors, Advances in Neural Information Processing Systems 25: 26th Annual
Conference on Neural Information Processing Systems 2012. Proceedings of a meeting held December 3-6,
2012, Lake Tahoe, Nevada, United States, pages 1106–1114, 2012.

[20] Alexey Kurakin, Ian J. Goodfellow, and Samy Bengio. Adversarial examples in the physical world. In 5th
International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017,
Workshop Track Proceedings. OpenReview.net, 2017.

[21] Alexey Kurakin, Ian J. Goodfellow, and Samy Bengio. Adversarial machine learning at scale. In 5th
International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017,
Conference Track Proceedings. OpenReview.net, 2017.

[22] Fangzhou Liao, Ming Liang, Yinpeng Dong, Tianyu Pang, Xiaolin Hu, and Jun Zhu. Defense against
adversarial attacks using high-level representation guided denoiser. In 2018 IEEE Conference on Computer
Vision and Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018, pages 1778–1787.
Computer Vision Foundation / IEEE Computer Society, 2018.

[23] Dong C. Liu and Jorge Nocedal. On the limited memory BFGS method for large scale optimization. Math.

Program., 45(1-3):503–528, 1989.

[24] Yanpei Liu, Xinyun Chen, Chang Liu, and Dawn Song. Delving into transferable adversarial examples
and black-box attacks. In 5th International Conference on Learning Representations, ICLR 2017, Toulon,
France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017.

[25] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin
transformer: Hierarchical vision transformer using shifted windows. In 2021 IEEE/CVF International
Conference on Computer Vision, ICCV 2021, Montreal, QC, Canada, October 10-17, 2021, pages 9992–
10002. IEEE, 2021.

[26] Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng, and Jian Sun. Shufﬂenet V2: practical guidelines for
efﬁcient CNN architecture design. In Vittorio Ferrari, Martial Hebert, Cristian Sminchisescu, and Yair
Weiss, editors, Computer Vision - ECCV 2018 - 15th European Conference, Munich, Germany, September
8-14, 2018, Proceedings, Part XIV, volume 11218 of Lecture Notes in Computer Science, pages 122–138.
Springer, 2018.

[27] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards
deep learning models resistant to adversarial attacks. In 6th International Conference on Learning Repre-
sentations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings.
OpenReview.net, 2018.

[28] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, and Pascal Frossard. Universal adversar-
ial perturbations. In 2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017,
Honolulu, HI, USA, July 21-26, 2017, pages 86–94. IEEE Computer Society, 2017.

[29] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. Deepfool: A simple and
accurate method to fool deep neural networks. In 2016 IEEE Conference on Computer Vision and Pattern
Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016, pages 2574–2582. IEEE Computer
Society, 2016.

[30] Aamir Mustafa, Salman H. Khan, Munawar Hayat, Roland Goecke, Jianbing Shen, and Ling Shao. Adver-
sarial defense by restricting the hidden space of deep neural networks. In 2019 IEEE/CVF International
Conference on Computer Vision, ICCV 2019, Seoul, Korea (South), October 27 - November 2, 2019, pages
3384–3393. IEEE, 2019.

[31] Nicolas Papernot, Patrick D. McDaniel, Somesh Jha, Matt Fredrikson, Z. Berkay Celik, and Ananthram
Swami. The limitations of deep learning in adversarial settings. In IEEE European Symposium on Security
and Privacy, EuroS&P 2016, Saarbrücken, Germany, March 21-24, 2016, pages 372–387. IEEE, 2016.

11

[32] Nicolas Papernot, Patrick D. McDaniel, Xi Wu, Somesh Jha, and Ananthram Swami. Distillation as a
defense to adversarial perturbations against deep neural networks. In IEEE Symposium on Security and
Privacy, SP 2016, San Jose, CA, USA, May 22-26, 2016, pages 582–597. IEEE Computer Society, 2016.

[33] Nicolas Papernot, Patrick D. McDaniel, Xi Wu, Somesh Jha, and Ananthram Swami. Distillation as a
defense to adversarial perturbations against deep neural networks. In IEEE Symposium on Security and
Privacy, SP 2016, San Jose, CA, USA, May 22-26, 2016, pages 582–597. IEEE Computer Society, 2016.

[34] Rahul Rade and Seyed-Mohsen Moosavi-Dezfooli. Helper-based adversarial training: Reducing excessive
margin to achieve a better accuracy vs. robustness trade-off. In ICML 2021 Workshop on Adversarial
Machine Learning, 2021.

[35] Sylvestre-Alvise Rebufﬁ, Sven Gowal, Dan A. Calian, Florian Stimberg, Olivia Wiles, and Timothy A.
Mann. Fixing data augmentation to improve adversarial robustness. CoRR, abs/2103.01946, 2021.

[36] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang,
Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet Large
Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV), 115(3):211–252,
2015.

[37] Pouya Samangouei, Maya Kabkab, and Rama Chellappa. Defense-gan: Protecting classiﬁers against adver-
sarial attacks using generative models. In 6th International Conference on Learning Representations, ICLR
2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net,
2018.

[38] Pouya Samangouei, Maya Kabkab, and Rama Chellappa. Defense-gan: Protecting classiﬁers against adver-
sarial attacks using generative models. In 6th International Conference on Learning Representations, ICLR
2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net,
2018.

[39] Vikash Sehwag, Saeed Mahloujifar, Tinashe Handina, Sihui Dai, Chong Xiang, Mung Chiang, and Prateek
Mittal. Improving adversarial robustness using proxy distributions. CoRR, abs/2104.09425, 2021.

[40] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recogni-
tion. In Yoshua Bengio and Yann LeCun, editors, 3rd International Conference on Learning Representa-
tions, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015.

[41] Jiawei Su, Danilo Vasconcellos Vargas, and Kouichi Sakurai. One pixel attack for fooling deep neural

networks. IEEE Trans. Evol. Comput., 23(5):828–841, 2019.

[42] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian J. Goodfellow, and
Rob Fergus. Intriguing properties of neural networks. In Yoshua Bengio and Yann LeCun, editors, 2nd
International Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014,
Conference Track Proceedings, 2014.

[43] Mingxing Tan and Quoc V. Le. Efﬁcientnet: Rethinking model scaling for convolutional neural networks.
In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Con-
ference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, volume 97 of
Proceedings of Machine Learning Research, pages 6105–6114. PMLR, 2019.

[44] Shiyu Tang, Ruihao Gong, Yan Wang, Aishan Liu, Jiakai Wang, Xinyun Chen, Fengwei Yu, Xianglong
Liu, Dawn Song, Alan Yuille, Philip H.S. Torr, and Dacheng Tao. Robustart: Benchmarking robustness on
architecture design and training techniques. https://arxiv.org/pdf/2109.05211.pdf, 2021.

[45] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Hervé
Jégou. Training data-efﬁcient image transformers & distillation through attention. In Marina Meila and
Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, ICML
2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pages
10347–10357. PMLR, 2021.

[46] Dongxian Wu, Shu-Tao Xia, and Yisen Wang. Adversarial weight perturbation helps robust generalization.
In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin,
editors, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information
Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020.

[47] Cihang Xie, Jianyu Wang, Zhishuai Zhang, Zhou Ren, and Alan L. Yuille. Mitigating adversarial
effects through randomization. In 6th International Conference on Learning Representations, ICLR 2018,
Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018.

12

[48] Cihang Xie, Jianyu Wang, Zhishuai Zhang, Zhou Ren, and Alan L. Yuille. Mitigating adversarial
effects through randomization. In 6th International Conference on Learning Representations, ICLR 2018,
Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018.

[49] Weilin Xu, David Evans, and Yanjun Qi. Feature squeezing: Detecting adversarial examples in deep neural
networks. In 25th Annual Network and Distributed System Security Symposium, NDSS 2018, San Diego,
California, USA, February 18-21, 2018. The Internet Society, 2018.

[50] Weilin Xu, David Evans, and Yanjun Qi. Feature squeezing: Detecting adversarial examples in deep neural
networks. In 25th Annual Network and Distributed System Security Symposium, NDSS 2018, San Diego,
California, USA, February 18-21, 2018. The Internet Society, 2018.

[51] Jingkang Yang, Kaiyang Zhou, Yixuan Li, and Ziwei Liu. Generalized out-of-distribution detection: A

survey. CoRR, abs/2110.11334, 2021.

[52] Zheng Yuan, Jie Zhang, Yunpei Jia, Chuanqi Tan, Tao Xue, and Shiguang Shan. Meta gradient adversarial
attack. In 2021 IEEE/CVF International Conference on Computer Vision, ICCV 2021, Montreal, QC,
Canada, October 10-17, 2021, pages 7728–7737. IEEE, 2021.

[53] Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. In Richard C. Wilson, Edwin R.
Hancock, and William A. P. Smith, editors, Proceedings of the British Machine Vision Conference 2016,
BMVC 2016, York, UK, September 19-22, 2016. BMVA Press, 2016.

[54] Haichao Zhang and Jianyu Wang. Defense against adversarial attacks using feature scattering-based
adversarial training. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d’Alché-Buc,
Emily B. Fox, and Roman Garnett, editors, Advances in Neural Information Processing Systems 32:
Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019,
Vancouver, BC, Canada, pages 1829–1839, 2019.

[55] Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric P. Xing, Laurent El Ghaoui, and Michael I. Jordan.
Theoretically principled trade-off between robustness and accuracy. In Kamalika Chaudhuri and Ruslan
Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning, ICML
2019, 9-15 June 2019, Long Beach, California, USA, volume 97 of Proceedings of Machine Learning
Research, pages 7472–7482. PMLR, 2019.

13

