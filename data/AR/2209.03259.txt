A Ridge-Regularised Jackknifed Anderson-Rubin Test∗

Max-Sebastian Dov`ıa, Anders Bredahl Kocka,b, and Sophocles Mavroeidisa

aDepartment of Economics, University of Oxford, Manor Road, OX1 3UQ, Oxford, UK
bDepartment of Economics and CREATES, Aarhus University, Denmark
@economics.ox.ac.uk
max-sebastian.dovi,anders.kock,sophocles.mavroeidis
}

{

September 8, 2022

Abstract

We consider hypothesis testing in instrumental variable regression models with
few included exogenous covariates but many instruments – possibly more than the
number of observations. We show that a ridge-regularised version of the jackknifed
Anderson and Rubin (1949, henceforth AR) test controls asymptotic size in the
presence of heteroskedasticity, and when the instruments may be arbitrarily weak.
Asymptotic size control is established under weaker assumptions than those im-
posed for recently proposed jackknifed AR tests in the literature. Furthermore,
ridge-regularisation extends the scope of jackknifed AR tests to situations in which
there are more instruments than observations. Monte-Carlo simulations indicate
that our method has favourable ﬁnite-sample size and power properties compared
to recently proposed alternative approaches in the literature. An empirical appli-
cation on the elasticity of substitution between immigrants and natives in the US
illustrates the usefulness of the proposed method for practitioners.

Keywords: instrumental variables, weak identiﬁcation, high dimensional mod-

els, ridge regression.

JEL codes: C12, C36, C55.

∗This research is funded by the European Research Council via Consolidator grant number 647152
and the German National Merit Foundation. We thank Frank Kleibergen, Damian Kozbur, Anna
Mikusheva, and Bent Nielsen for useful comments and discussions.

2
2
0
2

p
e
S
7

]

M
E
.
n
o
c
e
[

1
v
9
5
2
3
0
.
9
0
2
2
:
v
i
X
r
a

1

 
 
 
 
 
 
1

Introduction

Instrumental variables (IVs) are commonly employed in economics and related ﬁelds to

estimate causal eﬀects from observational data. Interest in inference with (very) many

IVs that are potentially weak has recently received increased attention for at least two

reasons. First, when identiﬁcation is weak, researchers may attempt to obtain more

precise inference by using a large number of IVs to capture the limited exogenous

variation in the endogenous covariates. Second, recent econometric approaches and

theoretical results make a number of IVs that approaches or even exceeds the num-

ber of observations more common. For instance, the so-called granular IV approach

proposed by Gabaix and Koijen (2020) leads to a number of IVs equal to the size of

the cross-section of the panel considered. This number can approach, and potentially

outstrip, the number of available observations. Another example is the so-called satu-

ration approach to the identiﬁcation of (weighted) local average treatment eﬀects as in

Blandhol et al. (2022), which involves considering as IVs a suﬃcient number of inter-

actions between the original IVs and the covariates. The number of IVs generated in

this way can easily outstrip the number of observations available. However, there is no

guarantee that a vast number of IVs will be jointly informative about the causal eﬀects
of interest.1 Hence, it is important to have methods of inference that remain reliable
when using (very) many weak IVs. The present paper contributes to the literature on

the development of such methods.

We propose a ridge-regularised jackknifed Anderson-Rubin (RJAR) test to construct

conﬁdence sets for the coeﬃcients of endogenous variables in weakly-identiﬁed and

heteroskedastic IV models when the number of IVs is large. Jackknife-based methods

have recently been used in this context because they are applicable in an asymptotic

framework where the number of IVs diverges with the number of observations. How-

ever, by relying on existing central limit theorems developed for standard projection

matrices, these methods require that the number of IVs be less than the number of

observations, and often perform poorly when the number of IVs is close to (but still

less than) the number of observations. Recently-proposed regularisation approaches

for inference under many IVs require strong identiﬁcation or a sparse relationship be-

tween the endogenous variables and the IVs to work well. By combining jackkniﬁng

with ridge regularisation, we provide a test that remains valid under heteroskedastic-

ity, arbitrarily weak identiﬁcation, and more IVs than observations, all while achieving

good power both when the relationship between the endogenous variables and the IVs

is sparse and when it is dense.

In the context of fully weak-identiﬁcation robust inference with an increasing number

1Pretesting the strength of IVs, though practical, could be rather ineﬃcient as a way of controlling
the size of subsequent inference on causal eﬀects. Moreover, weak-IV pretests are not available when
the number of IVs exceeds the number of observations.

2

of IVs, it is helpful to distinguish three diﬀerent asymptotic regimes. The ﬁrst ‘mod-
erately many’ IVs regime allows the number of IVs, kn, to grow with the sample size,
n, but still requires it to be asymptotically negligible with respect to the sample size.
Examples include Andrews and Stock (2007), who require kn/n1/3
and Gao (2017), who require kn/n

0, and Phillips

→

0.

→

The second ‘many’ IVs regime allows the number of IVs to be of the same order of

magnitude as the number of observations. Anatolyev and Gospodinov (2011) provide
an Anderson and Rubin (1949, henceforth AR) test that remains valid for kn/n
→
λ, 0
, provided that the error terms are homoskedastic and a
strong balanced-design assumption of the IVs is satisﬁed.2,3 Bun, Farbmacher, and
Poldermans (2020) provide analogous results for the GMM version of the AR statistic

λ < 1, n

→ ∞

≤

under the assumption of independently and identically distributed (i.i.d.) data. These

results were recently extended in the form of a jackknifed AR statistic by Crudu,

Mellace, and S´andor (2020, henceforth CMS) and Mikusheva and Sun (2022, henceforth

MS) to the case where errors are allowed to display arbitrary heteroskedasticity, and

the only assumption on the IVs is that the diagonal entries of the projection matrix

of the IVs are bounded away from unity from above.

The third ‘very many’ IVs regime allows the number of IVs to grow with n, and further

allows the number of IVs to be greater than the number of observations. Belloni et

al. (2012, henceforth BCCH) propose a Sup Score test that remains valid under mild

conditions, and allows the number of IVs to increase exponentially with the sample size.

Carrasco and Tchuente (2016) propose a ridge-regularised AR statistic that allows for

more IVs than observations under the assumption of i.i.d. data. Kapetanios, Khalaf,

and Marcellino (2015) extend Bai and Ng (2010) by proposing weak-identiﬁcation

robust factor-based statistics that in principle allow for the number of IVs to be larger

than the number of observations, provided that the factor structure of the IVs is

suﬃciently strong.

Using the notation of the model introduced in the next section, Table 1 provides a

schematic overview of the main assumptions and results in the literature on inference

that is robust to many weak IVs.

Our test provides a twofold extension of the existing literature. First, it allows for valid

inference under many IVs while further weakening the assumptions of similar tests

proposed by CMS and MS. This is made possible by deriving the limiting behaviour

of the RJAR statistic from the bottom up, without relying on the existing results in

Chao et al. (2012) or Hansen and Kozbur (2014). Second, this test allows for more

2See Anatolyev (2012), Anatolyev and Yaskov (2017), and Crudu, Mellace, and S´andor (2020) for

a discussion of (the restrictiveness of) the strong balanced-design assumption.

3See Kaﬀo and Wang (2017) for a bootstrapped version of the Anatolyev and Gospodinov (2011)

AR test.

3

IVs than observations. The only other approach currently available in the literature

that is robust to heteroskedastic error terms and more IVs than observations is the

Sup Score test of BCCH. Simulations show that the RJAR test has power comparable

to the Sup Score test of BCCH whenever the signal in the ﬁrst stage is sparse (i.e.,

only a few of the IVs are informative), and substantially more power when the signal

in the ﬁrst stage is dense (i.e., not sparse). Finally, we provide a comparison of the

most recently proposed approaches to conducting inference in possibly heteroskedastic

linear IV models when the number of IVs is not negligible with respect to the sample

size. Indeed, using extensive simulation evidence and an empirical application based on

Card (2009), we provide a comparison between these existing ‘state-of-the-art’ methods

in a controlled and comparable setting.

Notation. Ip denotes the p
is denoted as Aij for 1
j
≤
||
≤
norm of A, and tr(B) := (cid:80)m
i=1 Bii for any m
for 1

m, 1

i, j

p.

×

≤

i

p matrix A
p identity matrix. The entry (i, j) for an m
tr(A(cid:48)A) denotes the Frobenius
F :=
||
m matrix B with entries given by Bij

(cid:112)

≤

×

A

m. The remaining notation follows standard conventions.

×

≤

≤

Organisation of the paper. Section 2 speciﬁes the linear IV model considered
throughout. Section 3 introduces the RJAR test, and provides the main asymptotic

results. Section 4 provides simulation evidence on the size and power of our RJAR test

and compares it with the tests proposed by BCCH, CMS and MS. Section 5 considers

an empirical application based on Card (2009). Section 6 concludes. All proofs are

given in the Appendix.

4

Table 1: Weak-identiﬁcation robust inference with many IVs: schematic comparison

of main assumptions and results in the literature.

kn

Z

ε

Anatolyev and
Gospodinov (2011) and
Kaﬀo and Wang (2017)

kn/n → λ, 0 ≤ λ < 1,
n → ∞

Z is ﬁxed, obeys
strong balanced
design assumption

i.i.d., E[εi] = 0,

E[ε4

i ] < ∞

Belloni et al. (2012)

log(max(kn, n)) =
o(n1/3), n → ∞

Z is ﬁxed

independent,
E[εi] = 0,
E[ε3
i ] < ∞

Kapetanios, Khalaf,
and Marcellino (2015)

Factor-dependent

Potentially dependent data,
il] < ∞, l = 1, . . . , kn, E[ε4

i ] < ∞

E[Z4

Carrasco and Tchuente
(2016)

n → ∞

Ziεi is i.i.d., E[Ziεi] = 0, E[X 2

ig] < ∞

Bun, Farbmacher, and
Poldermans (2020)

kn/n → λ, 0 ≤ λ < 1,
n → ∞

Ziεi is i.i.d., Z obeys strong balanced
design assumption, E[Ziεi] = 0,
i ε8

i ] < ∞

E[Z8

Crudu, Mellace, and
S´andor (2020)

kn/n → λ, 0 ≤ λ < 1,
kn → ∞ as n → ∞

Z is ﬁxed,
Pii ≤ 1 − δ, 0 < δ < 1

Mikusheva and Sun
(2022)

kn/n → λ, 0 ≤ λ < 1,
kn → ∞ as n → ∞

Z is ﬁxed,
Pii ≤ 1 − δ, 0 < δ < 1

RJAR

rn → ∞ as n → ∞

(cid:80)n

Z is ﬁxed,
(cid:80)
j(cid:54)=i(P γn
n , c > 0, ω ≥ 1

ij )2 ≥

i=1
crω

independent,
E[εi] = 0,
E[ε4
i ] < ∞

independent,
E[εi] = 0,
E[ε6
i ] < ∞

independent,
E[εi] = 0,
E[ε4
i ] < ∞

Notes:
n is the number of observations, kn is the number of IVs, and g is the number of endogenous variables.
Z is the n × kn matrix of IVs. X is the n × g matrix of endogenous variables. ε is the n × 1 vector of
structural error terms. rn := rank(Z). See also Equation (1) below.
P := Z(Z(cid:48)Z)−1Z(cid:48), P γn := Z(Z(cid:48)Z + γnIkn )−1Z(cid:48) for γn ≥ 0 if rn = kn and γn ≥ γ− > 0 if rn < kn.

2 Model

We consider the heteroskedastic linear IV model given by

y = Xβ + ε

X = ZΠ + V,

(1a)

(1b)

g matrix

×

where y is an n

1 vector containing the dependent variable, X is an n

containing the endogenous variables, β is a g

×
containing the structural error terms, Z is an n
kn

g coeﬃcient matrix, and V is an n

×
1 coeﬃcient vector, ε is an n

1 vector
kn matrix containing the IVs, Π is a
g matrix containing the ﬁrst-stage errors. kn

×

×

×

×

5

can diverge with n, but g is ﬁxed. Also, let yi, Xi, εi, Zi, and Vi denote the ith row of
y, X, ε, Z, and V , respectively. As in BCCH, CMS and MS, we treat Z as ﬁxed (non-

stochastic), and assume that any exogenous covariates have been partialled out (see
the discussion of Assumption 3 below). We exclusively consider methods that allow for
arbitrarily weak identiﬁcation, i.e., methods that control asymptotic size irrespective

of the value of Π.

Inference is conducted on the coeﬃcient vector β by testing hypotheses of the following
type for a prespeciﬁed β0

g:

∈ (cid:60)

H0 : β = β0 vs. H1 : β

= β0.

(2)

For a given non-randomised test of asymptotic size α

(0, 1), a conﬁdence set of
α can be constructed as the collection of those β0 for which

∈

asymptotic coverage 1
H0 in Equation (2) is not rejected. For convenience, deﬁne

−

e(β0) := (cid:2)e1(β0), . . . , en(β0)(cid:3)(cid:48)

,

ei(β0) := yi

Xiβ0,

−

i = 1, . . . , n,

(3)

which we refer to as the structural error under the null hypothesis.

It should be noted that so long as the error term remains additive, the linearity in the

structural equation (1a) and the ﬁrst-stage equation (1b) can be relaxed to allow for

any (known) real-valued function, without aﬀecting the asymptotic properties of the

RJAR test.

3 The RJAR test

This section introduces our proposed RJAR test, derives its large sample properties

under the null hypothesis in Equation (2), and discusses its relationship to the most

closely related tests in the literature: the jackknifed AR tests of CMS and MS, and

the Sup Score test of BCCH.

3.1 Deﬁnition of the RJAR test

The original AR test of the null hypothesis in Equation (2) can be thought of as a

test that the IVs are exogenous using the implied structural errors in Equation (3).

More speciﬁcally, the AR test is a Wald test of the signiﬁcance of the IVs in the
auxiliary regression of the structural errors under the null, ei(β0), on the IVs, Zi.4

√

4The AR statistic is deﬁned as e(β0)(cid:48)Z (cid:98)ΣZ (cid:48)e(β0)/n, where (cid:98)Σ is a consistent estimator of
Var[Z (cid:48)e(β0)/
n]. The weak-IV-robust AR test of the null hypothesis in Equation (2) with asymptotic
size α rejects the null if and only if the AR statistic exceeds the 1 − α quantile of a χ2 distribution
with kn degrees of freedom.

6

(cid:54)
When the number of IVs kn grows with n, i.e., when there are many (potentially
weak) IVs, the χ2 approximation of the original AR statistic breaks down. The recent
papers by CMS and MS, which we shall brieﬂy review in the next subsection, propose
jackknifed versions of the AR test that remain valid when kn grows with n, but kn < n.
Our proposed method combines jackkniﬁng with ridge regularisation of the auxiliary
regression of ei(β0) on Zi.

We ﬁrst standardise the IVs in-sample (after partialling out any covariates) as in BCCH

(p. 2393) so that

1
n

n
(cid:88)

i=1

Z2

ij = 1

for

j = 1, . . . , kn.

(4)

The RJAR test for the testing problem in Equation (2) is then based on the following

statistic:

RJARγn(β0) :=

(cid:113)

1
ˆΦγn(β0)

n
(cid:88)

(cid:88)

i=1

j(cid:54)=i

√rn

P γn
ij ei(β0)ej(β0),

where rn := rank(Z) (assumed to be positive without loss of generality),

ˆΦγn(β0) :=

2
rn

n
(cid:88)

(cid:88)

i=1

j(cid:54)=i

(P γn

ij )2e2

i (β0)e2

j (β0),

(5)

(6)

and P γn := Z(Z(cid:48)Z + γnIkn)−1Z(cid:48) is the ridge-regularised projection matrix for γn
0
if rn = kn and γn > 0 if rn < kn. γn is a (sequence of) regularisation parameter(s)
whose choice we discuss next.

≥

We set the penalty parameter γn to

γ∗
n := max arg max

γn∈Γn

n
(cid:88)

(cid:88)

(P γn

ij )2,

i=1

j(cid:54)=i

(7)

γn

: γn
where Γn :=
0 if rn = kn, and γn
constant γ− not depending on n. The existence of γ∗
(cid:80)
pendix. We let γ∗

n be an element of arg max

∈ (cid:60)

(cid:80)n

≥

{

≥

γ− > 0 if rn < kn

for some
n is shown in Lemma 3 in the Ap-
j(cid:54)=i(P γn
ij )2 out of conservativeness,

}

i=1

γn∈Γn

i.e., to make Assumption 3 below as plausible as possible given the IVs. Further-
more, we let γ∗
n be the maiximal element of this set because the maximiser of the ﬁrst
argument is not necessarily unique without imposing additional assumptions on the

singular values and left-singular vectors of the IVs (although in practice we only found

unique maximisers). We choose to take the maximum of the maximisers to make the
smallest eigenvalues of the ridge-regularised Gram matrix, Z(cid:48)Z + γnIkn, as far away
from zero as possible when rn < kn.

7

We now have all the ingredients to deﬁne our new test.

Deﬁnition 1. The RJAR test rejects H0 : β = β0 in Equation (2) at signiﬁcance
level α

(0, 1) if and only if

∈

RJARγ∗

n(β0) >

(1

Q

−

α),

(8)

where RJARγn(β0) is deﬁned in Equation (5), γ∗

n is deﬁned in Equation (7), and

(1

Q

−

α) is the (1

−

α) quantile of the Standard Normal distribution.

3.2 Asymptotic properties of the RJAR test

We make the following assumptions to derive the limiting distribution of RJARγ∗
under the null hypothesis in Equation (2).

n(β0)

Assumption 1.
i∈N is a sequence of independent random variables satisfying
εi
}
{
E[εi] = 0, inf i∈N Var[εi] > 0, and supi∈N E[ε4

i ] <

.
∞

Assumption 2.

rn = rank(Z)

→ ∞

as n

.
→ ∞

Assumption 3. There exist γ− > 0, c > 0, ω

1, N

2 such that for all n

1. If rn = kn, then there exists a γn

2. If rn < kn, then there exists a γn

∈

∈

[0,

∞

[γ−,

∞

≥

≥
) such that (cid:80)n

i=1
) such that (cid:80)n

(cid:80)

j(cid:54)=i(P γn
ij )2
(cid:80)
j(cid:54)=i(P γn

≥
ij )2

i=1

N

≥
crω
n

crω
n

≥

Assumption 1 is a mild condition on the structural error terms, and allows for condi-

tional heteroskedasticity. It is the same as in CMS. It is slightly less restrictive than

the one in MS (who require ﬁnite sixth moments on the structural error terms), and

slightly more restrictive than the one in BCCH (who require ﬁnite third moments).

Assumption 2 is a weak technical assumption. It implies that both kn and n diverge.
It also allows for the sum of the number of IVs and the number of exogenous covariates

to be larger than the number of observations, provided that the number of exogenous

covariates that have been partialled out be suﬃciently small (so that the rank of the

matrix of partialled IVs continues to diverge). This assumption is weaker than the
restriction on the dimensionality in MS and CMS, who require rn = kn, kn < n for
. BCCH prove asymptotic size control of their Sup
as n
each n
Score test under the assumption that log kn = o(n1/3).

N, and kn

→ ∞

→ ∞

∈

Assumption 3 is a high-level assumption on the number of IVs and their correlation
structure. Assumption 3 implies that γ∗
n . When
kn < n (and there are no exogenous covariates, as in CMS and MS), Assumption 3

n satisﬁes (cid:80)n

γ∗
ij )2
n

j(cid:54)=i(P

crω

(cid:80)

i=1

≥

8

is weaker than the balanced-design assumption in CMS and MS which requires for
P := Z(Z(cid:48)Z)−1Z(cid:48)

max
1≤i≤n

Pii

δ,

1

−

≤

(9)

for some 0 < δ < 1. The balanced-design assumption implies Assumption 3.

Proposition 1. Suppose (as in CMS and MS) that rank(P ) = kn and that there
exists a 0 < δ < 1 such that max
1≤i≤n

δ. Then Assumption 3 is satisﬁed.

Pii

1

≤

−

It is diﬃcult to give more primitive conditions for Assumption 3 that remain both
easily-veriﬁable and suﬃciently general when kn > n. Figure 1 shows that for the
Gaussian data-generating process (DGP) design used in the simulations in Section 4,
the ratio of (cid:80)n
ij )2 to rn is constant, lending plausibility to Assumption 3
in this context. Figure 1 also shows that γ∗
n grows with the number of observations,
which implies that our penalty parameter may grow with the sample size (in contrast

j(cid:54)=i(P γn

(cid:80)

i=1

to the one proposed in Carrasco and Tchuente (2016)). In practice, and similarly to

the heuristics recommended in CMS, MS, and Hansen and Kozbur (2014), we rec-

ommend that in a given application, practitioners check the implied minimum value

of c in Assumption 3. If very small values are observed, then Assumption 3 may be

questionable.

9000

6000

∗n
γ

3000

0

2
)
n
j
γ
i
P
(

i

=
j

(cid:80)

1
=
ni
(cid:80)

1
−
n
r
0
>
n
γ

x
a
m

0.05

0.04

0.03

0.02

0.01

0.00

7500 10000

0

2500

0

2500

5000
n

(a)

7500 10000

5000
n

(b)

Figure 1: Numerical Evidence on Assumption 3 for kn = 1.9n. Gaussian IVs drawn
with mean 0 and Var [Zil] = 0.3, Corr [Zil, Zim] = 0.5|l−m|, and indepen-
dently across i.

We are now in a position to derive the asymptotic distribution of the RJAR test under

the null hypothesis.

9

(cid:54)
Theorem 1. Suppose Assumptions 1–3 and the null hypothesis in Equation (2) hold.
Consider any sequence γn such that, for all n suﬃciently large, (cid:80)n
crω
n
1. Then, the statistic RJARγn(β0) deﬁned in Equation (5)
for some c > 0 and ω

j(cid:54)=i(P γn

ij )2

(cid:80)

i=1

≥

satisﬁes

≥

RJARγn(β0)

d
→ N

[0, 1].

Corollary 1. Under Assumptions 1–3 and the null hypothesis in Equation (2), the
RJAR test given in Deﬁnition 1 has asymptotic size α.

Notice that we did not impose any assumption on the coeﬃcients of the instruments

Π in the ﬁrst-stage regression in Equation (1). Therefore, the RJAR test is robust to

arbitrarily weak identiﬁcation.

3.3 Closest alternatives in the literature

The RJAR test is similar to the jackknifed AR tests proposed in CMS and MS. The
distinguishing feature is the use of a ridge-regularised projection matrix P γn, which
makes the RJAR test applicable also when rn < kn, unlike the aforementioned two
tests that use the normal projection matrix P , and cannot be computed when kn > n.

CMS assume rn = kn < n, and propose the jackknifed AR statistic given by

ARCM S(β0) :=

(cid:113)

1
ˆΦCM S(β0)

√kn

n
(cid:88)

(cid:88)

i=1

j(cid:54)=i

Cijei(β0)ej(β0),

−

−

1
2 P D(In

B, A := P + ∆, B := (In
1
2 D(In

where C := A
−
D)−1P
D)−1P , and D is the diagonal matrix containing
D)−1
the diagonal elements of P . ˆΦCM S(β0) := 2
ije2
j (β0). Under the
kn
null hypothesis in Equation (2), CMS show that ARCM S(β0) converges to a Standard
Normal distribution. The CLT underlying this result is a modiﬁed version of Lemma

P ), ∆ := P D(In

i (β0)e2

j(cid:54)=i C2

P )D(In

(cid:80)n

(cid:80)

i=1

−

−

−

−

−

−

D)−1(In

A2 of Chao et al. (2012) proposed in Bekker and Crudu (2015, Appendix A.4).

MS also assume that rn = kn < n, and propose a diﬀerent jackknifed AR statistic that
can be obtained from RJARγn(β0) in Equation (5) by setting γn = 0, and replacing
ˆΦγn(β0) with

ˆΦM S(β0) :=

2
kn

n
(cid:88)

(cid:88)

i=1

j(cid:54)=i

P 2
ij
MiiMjj + M 2
ij

(cid:2)ei(β0)Mie(β0)(cid:3) (cid:2)ej(β0)Mje(β0)(cid:3) ,

(10)

P , and Mi is the ith row of M . The reason why the unregularised
where M = In
jackknifed AR test in MS uses ˆΦM S(β0) instead of the variance estimator given in
Equation (6) evaluated at γn = 0, is because, according to their Theorem 4, the

−

10

former yields higher power than the latter. It follows that the unregularised version of
our RJAR test, which arises when γ∗
n = 0 in Equation (7), will be dominated by the
jackknifed AR test of MS in terms of power, and practitioners may prefer the latter

in those cases. We note, however, that the asymptotic size control and superior power

of the jackknifed AR test of MS are proven under the balanced design assumption

given in Equation (9) which is strictly stronger than our Assumption 3. This can

have implications for the ﬁnite-sample performance of the jackknifed AR test of MS

compared to the RJAR, which we investigate in Section 4.

When kn is larger than n, the AR tests in CMS and MS are not applicable, since
Z(cid:48)Z does not have full rank. In this case, the Sup Score test of BCCH remains valid.
BCCH ﬁrst standardise the IVs as in Equation (4), and then propose the Sup Score

statistic given by

S(β0) = max
1≤j≤kn

(cid:12)
1√
(cid:12)
(cid:12)
n
(cid:113) 1
n

(cid:80)n

i=1 ei(β0)Zij
(cid:0)ei(β0)(cid:1)2

.

Z2
ij

(cid:80)n

i=1

(cid:12)
(cid:12)
(cid:12)

BCCH propose to use the critical value να = cBCCH √n
and α

α/(2kn)(cid:1), cBCCH > 1
(0, 1). They show that comparing their Sup Score statistic to this critical

(cid:0)1

Q

−

value yields a test of the null hypothesis in Equation (2) that has asymptotic size less

∈

than or equal to α. Being a supremum-norm test suggests that the BCCH Sup Score

test will work well with a sparse ﬁrst stage (i.e. where only a few elements of Π are

zero), but may have lower power than the RJAR test when the ﬁrst stage is dense.

This is veriﬁed in the simulations in Section 4.

4 Simulations

We now investigate the size and power properties of the RJAR test and compare them

to those of the tests proposed in BCCH, CMS and MS. We take our simulation setup

from Hansen and Kozbur (2014) who in turn take theirs from BCCH. The DGP is

given by





εi
vi

(11a)

(11b)

yi = Xiβ + εi
Xi = Z(cid:48)

iπ + vi,





(cid:20)

0,

(cid:104) σ2

ε σεv
σεv σ2
v

(cid:105)(cid:21)

,

∼ N

11

for i = 1, . . . , n = 100, with σ2
v = 1 and σεv = 0.6σεσv.5 The IVs are
independent Gaussian with mean 0 and Var [Zil] = 0.3 and Corr [Zil, Zim] = 0.5|l−m|.6
π = (cid:37)κ, where κ is a vector of zeros and ones that varies with the type of DGP

ε = 2, σ2

considered (sparse or dense, as modelled below), and (cid:37) is some scalar that ensures
that for a given concentration parameter, µ2, the following relationship is satisﬁed:

This implies that

µ2 =

nπ(cid:48)E (cid:2)ZiZ(cid:48)
σ2
v

i

(cid:3) π

.

(cid:115)

(cid:37) =

vµ2
σ2
nκ(cid:48)E (cid:2)ZiZ(cid:48)

i

.

(cid:3) κ

To illustrate how the sparsity structure of the ﬁrst stage in Equation (11b) can aﬀect

the size and power of the studied tests, we consider both a sparse ﬁrst stage and a
kn−5](cid:48),
dense ﬁrst stage. Sparsity in the ﬁrst stage is modelled by setting κ = [ι(cid:48)
where ιq is a q
1 vector of zeros. Density in the ﬁrst
stage is modelled by setting κ = [ι(cid:48)

1 vector of ones, and 0q is a q
×
0.6kn](cid:48).

0.4kn, 0(cid:48)

5, 0(cid:48)

×

We consider kn = 30, 90, 190. Throughout, γ− in Assumption 3 is set equal to 1. For
the case of 30 IVs, the RJAR test does not impose any regularisation (γ∗
n = 0). For
γ∗
n = 12.048. We note that (cid:80)n
ij )2 = 11.123 > 8.845 =
the case of 90 IVs, γ∗
n
(cid:80)n
j(cid:54)=i(Pij)2. This shows that even in the case where rn < n, ridge regularisation
can make Assumption 3 strictly more plausible. For the case of 190 IVs, γ∗
n = 109.187.
The variance estimator of MS occasionally yields a negative value. These cases are

j(cid:54)=i(P

(cid:80)

(cid:80)

i=1

i=1

conservatively interpreted as a failure to reject the null hypothesis. As recommended
by BCCH, cBCCH = 1.1. The number of Monte Carlo replications is 10,000.

4.1 Size

Figure 2 shows the simulation results for the sparse ﬁrst stage for tests of size 0.01 to
0.99, that is the rejection frequency under H0 : β0 = 1. As far as the illustration of
the tests’ size properties is concerned, the dense ﬁrst stage yields virtually the same

results, and is hence omitted. Since all tests are robust to weak IVs, the rejection

frequencies of the tests are not aﬀected by the value of the concentration parameter.

For the case of 30 IVs, the AR tests of CMS and MS and the RJAR test have correct

size, while the BCCH Sup Score test is undersized.

5Heteroskedasticity could be incorporated in standard ways, e.g., by scaling the structural error
by the value of a randomly selected IV for every observation. As is to be expected given that all the
approaches considered here are robust to heteroskedasticity of arbitrary form, such modiﬁcations do
not qualitatively aﬀect the simulations.

6Unreported simulations show that the results remain qualitatively unchanged if uncorrelated Gaus-

sian IVs are considered instead.

12

For the case of 90 IVs, the AR test of CMS appears to control size for common small

nominal test sizes (e.g., 0.05 or 0.1). The AR test of MS appears to be generally

oversized. For example, at nominal level 0.05, the rejection frequency of the test is
0.189.7 The BCCH Sup Score test continues to be undersized, while the RJAR test
continues to have correct size irrespective of the value of µ2.

For the case of 190 IVs, only the BCCH Sup Score test and the RJAR test are feasible.

As before, the BCCH Sup Score test is undersized, while the RJAR test has correct

size.

y
c
n
e
u
q
e
r
F

n
o
i
t
c
e
j
e
R

1.00

0.75

0.50

0.25

0.00

y
c
n
e
u
q
e
r
F

n
o
i
t
c
e
j
e
R

1.00

0.75

0.50

0.25

0.00

0.00

0.25
0.75
0.50
Nominal Test Size

1.00

0.00

0.25
0.75
0.50
Nominal Test Size

1.00

(a) kn = 30

(b) kn = 90

1.00

y
c
n
e
u
q
e
r
F

n
o
i
t
c
e
j
e
R

0.75

0.50

0.25

0.00

0.00

0.25
0.75
0.50
Nominal Test Size

1.00

(c) kn = 190

Figure 2: PP Plots for Sparse IVs, µ2 = 0, H0 : β0 = 1.

7Unreported simulations conﬁrm that if n and kn are increased to about 1,000 and 900, respectively,

there is no longer any size distortion.

13

ARCMSARMSSRJAR4.2 Power

Figures 3–8 show the power of the tests when the number of IVs and the sparsity
pattern of the ﬁrst stage is varied. It is still the case that H0 : β0 = 1. For the case of
30 IVs (Figure 3 and Figure 4), the AR test of CMS has similar power to the RJAR

test, while MS is slightly more powerful than the RJAR test. The BCCH Sup Score

test is less powerful than all other tests. For the case of 90 sparse IVs (Figure 5), the

RJAR test is slightly more powerful than the BCCH Sup Score test. The AR test

of MS fails to control the size, while the AR test of CMS exhibits power properties

substantially worse than those of the BCCH Sup Score test and the RJAR test. For

the case of 90 dense IVs (Figure 6), the AR tests of CMS and MS exhibit the same

lower power and failure to control the test size as in the case of 90 sparse IVs. In this

dense setting, the RJAR test is substantially more powerful than the BCCH Sup Score

test. For the case of 190 sparse and dense IVs (Figure 7 and Figure 8), the RJAR

test is more powerful than the BCCH Sup Score test. Thus, for all the DGPs that are

considered here, the RJAR test is as powerful as existing methods whenever these are

applicable, and sometimes much more powerful.

14

y
c
n
e
u
q
e
r
F

n
o
i
t
c
e
j
e
R

1.00

0.75

0.50

0.25

0.00

1.00

y
c
n
e
u
q
e
r
F

n
o
i
t
c
e
j
e
R

0.75

0.50

0.25

0.00

y
c
n
e
u
q
e
r
F

n
o
i
t
c
e
j
e
R

1.00

0.75

0.50

0.25

0.00

-1

0

1
β

(a) µ2 = 0

2

3

-1

0

2

3

1
β

(b) µ2 = 30

y
c
n
e
u
q
e
r
F

n
o
i
t
c
e
j
e
R

1.00

0.75

0.50

0.25

0.00

-1

0

1
β

2

3

-1

0

2

3

1
β

(c) µ2 = 60

(d) µ2 = 180

Figure 3: Power Curves for 30 Sparse IVs. Nominal test size of 5% indicated by the

grey horizontal line.

15

ARCMSARMSSRJARy
c
n
e
u
q
e
r
F

n
o
i
t
c
e
j
e
R

1.00

0.75

0.50

0.25

0.00

1.00

y
c
n
e
u
q
e
r
F

n
o
i
t
c
e
j
e
R

0.75

0.50

0.25

0.00

y
c
n
e
u
q
e
r
F

n
o
i
t
c
e
j
e
R

1.00

0.75

0.50

0.25

0.00

-1

0

1
β

(a) µ2 = 0

2

3

-1

0

2

3

1
β

(b) µ2 = 30

y
c
n
e
u
q
e
r
F

n
o
i
t
c
e
j
e
R

1.00

0.75

0.50

0.25

0.00

-1

0

1
β

2

3

-1

0

2

3

1
β

(c) µ2 = 60

(d) µ2 = 180

Figure 4: Power Curves for 30 Dense IVs. Nominal test size of 5% indicated by the

grey horizontal line.

16

ARCMSARMSSRJARy
c
n
e
u
q
e
r
F

n
o
i
t
c
e
j
e
R

1.00

0.75

0.50

0.25

0.00

y
c
n
e
u
q
e
r
F

n
o
i
t
c
e
j
e
R

1.00

0.75

0.50

0.25

0.00

y
c
n
e
u
q
e
r
F

n
o
i
t
c
e
j
e
R

1.00

0.75

0.50

0.25

0.00

-1

0

1
β

(a) µ2 = 0

2

3

-1

0

2

3

1
β

(b) µ2 = 30

1.00

y
c
n
e
u
q
e
r
F

n
o
i
t
c
e
j
e
R

0.75

0.50

0.25

0.00

-1

0

1
β

2

3

-1

0

2

3

1
β

(c) µ2 = 60

(d) µ2 = 180

Figure 5: Power Curves for 90 Sparse IVs. Nominal test size of 5% indicated by the

grey horizontal line.

17

ARCMSARMSSRJARy
c
n
e
u
q
e
r
F

n
o
i
t
c
e
j
e
R

1.00

0.75

0.50

0.25

0.00

y
c
n
e
u
q
e
r
F

n
o
i
t
c
e
j
e
R

1.00

0.75

0.50

0.25

0.00

y
c
n
e
u
q
e
r
F

n
o
i
t
c
e
j
e
R

1.00

0.75

0.50

0.25

0.00

-1

0

1
β

(a) µ2 = 0

2

3

-1

0

2

3

1
β

(b) µ2 = 30

1.00

y
c
n
e
u
q
e
r
F

n
o
i
t
c
e
j
e
R

0.75

0.50

0.25

0.00

-1

0

1
β

2

3

-1

0

2

3

1
β

(c) µ2 = 60

(d) µ2 = 180

Figure 6: Power Curves for 90 Dense IVs. Nominal test size of 5% indicated by the

grey horizontal line.

18

ARCMSARMSSRJARy
c
n
e
u
q
e
r
F

n
o
i
t
c
e
j
e
R

1.00

0.75

0.50

0.25

0.00

y
c
n
e
u
q
e
r
F

n
o
i
t
c
e
j
e
R

1.00

0.75

0.50

0.25

0.00

y
c
n
e
u
q
e
r
F

n
o
i
t
c
e
j
e
R

1.00

0.75

0.50

0.25

0.00

-1

0

1
β

(a) µ2 = 0

2

3

-1

0

2

3

1
β

(b) µ2 = 30

1.00

y
c
n
e
u
q
e
r
F

n
o
i
t
c
e
j
e
R

0.75

0.50

0.25

0.00

-1

0

1
β0

(c) µ2 = 60

2

3

-1

0

2

3

1
β

(d) µ2 = 180

Figure 7: Power Curves for 190 Sparse IVs. Nominal test size of 5% indicated by the

grey horizontal line.

19

SRJARy
c
n
e
u
q
e
r
F

n
o
i
t
c
e
j
e
R

1.00

0.75

0.50

0.25

0.00

1.00

y
c
n
e
u
q
e
r
F

n
o
i
t
c
e
j
e
R

0.75

0.50

0.25

0.00

y
c
n
e
u
q
e
r
F

n
o
i
t
c
e
j
e
R

1.00

0.75

0.50

0.25

0.00

-1

0

1
β

(a) µ2 = 0

2

3

-1

0

2

3

1
β

(b) µ2 = 30

1.00

y
c
n
e
u
q
e
r
F

n
o
i
t
c
e
j
e
R

0.75

0.50

0.25

0.00

-1

0

1
β

2

3

-1

0

2

3

1
β

(c) µ2 = 60

(d) µ2 = 180

Figure 8: Power Curves for 190 Dense IVs. Nominal test size of 5% indicated by the

grey horizontal line.

5 Empirical Application

We consider an empirical application based on Card (2009). The coeﬃcient of interest
is given by βs in the following model:

yis = βsXis + δ(cid:48)

sWi + εis,

(12)

20

SRJARwhere yis is the diﬀerence between residual log wages for immigrant and native men in
skill group s in city i,8 Xis is the log ratio of immigrant to native hours worked in skill
group s of both men and women in city i, and Wi is a vector of city-level controls with
coeﬃcient vector δs, and εis is the structural error. In the context of the production
function speciﬁed in Card (2009, Section I), βs can be interpreted as the (negative)
inverse elasticity of substitution between immigrants and natives in the US in their

respective skill group. As in Card (2009), we consider two skill groups s = h, c (high

school or college equivalent) separately.

Card (2009) raises the concern that unobserved factors in a city may lead to both higher
wages and higher employment levels of immigrants relative to natives, causing Xis to
be endogenous. Card (2009) proposes to use the ratio of the number of immigrants

from country l in city i to the total number of immigrants from foreign country l

in the US as an IV. The rationale for these IVs is that existing immigrant enclaves

are likely to attract additional immigrant labour through social and cultural channels

unrelated to labour market outcomes. We consider two sets of IVs. First, we consider
the original setup of Card (2009), using as IVs the kn = 38 diﬀerent countries of origin
of the immigrants. Second, motivated by the saturation approach of Blandhol et al.

(2022), we consider the setup where these 38 original IVs are interacted with the q = 9
available controls (including a constant). This yields kn = 342 IVs. In both cases, the
number of observations (i.e., the number of cities) is n = 124.9

We construct (weak-identiﬁcation robust) conﬁdence sets for βs by inverting the AR
tests of CMS and MS, the Sup Score test, and the RJAR test. Thus, the 95% conﬁdence
set for any test is obtained as the collection of βs,0 for which that test does not reject
the null at 5% level of signiﬁcance. As in the simulation exercise in Section 4, γ− = 1,
and cBCCH = 1.1. A grid of 100 values for βs,0 is used for s = h, c. Data is taken from
a single cross section, as made available by Goldsmith-Pinkham, Sorkin, and Swift

(2020).

Figure 9 shows the conﬁdence sets when kn = 38 for high-school workers and college
workers. We ﬁnd that γ∗
n = 0, implying that no regularisation is needed. The 95%
conﬁdence sets for each test are given by all the points below the grey horizontal line.

This is in line with our simulations in Section 4, where we found that regularisation was
not needed to maximise the sum in Assumption 3 when kn/n = 0.3. The conﬁdence
sets for both skill groups broadly conﬁrm the results in Card (2009). We ﬁnd that

the conﬁdence sets for high-school workers is smallest for the jackknifed AR statistic

8As discussed in Card (2009, p. 11, footnote 17), residual wages are wages once observed charac-

teristics of the entire US workforce are controlled for.

9Note that the controls and the IVs are at the city level, and hence the same for the applications
to high-school workers and college workers. Therefore, the projection matrix and the ridge-regularised
projection matrix will also be the same across these two skill groups.

21

of CMS, whereas the BCCH Sup Score test yields the smallest conﬁdence interval

for the application to college workers. Based on the power results in Section 4, this

suggests a very sparse ﬁrst stage for college workers, i.e., a few nationalities being

highly predictive of inﬂows of immigrant labour. In line with the simulation evidence
on power for kn/n = 0.3 in Section 4, the diﬀerences in conﬁdence sets across the four
diﬀerent tests considered appear to be reasonably small.

(a) High-school workers

(b) College workers

Figure 9: 95 % conﬁdence sets for βs for the application in Equation (12) with kn = 38

IVs. maxiPii = 0.944. γ∗

n = 0, r−1
n

(cid:80)n

(cid:80)

i=1

j(cid:54)=i(P

γ∗
ij )2 = 0.513.
n

Figure 10 shows the conﬁdence sets when kn = 342 for high-school workers and college
9 = 115 < 342 = kn, the jackknifed
workers, respectively. Since rn = n
q = 124
AR statistics of CMS and MS are not applicable. We ﬁnd that γ∗
n = 5.299 and
γ∗
ij )2 = 0.106. In line with the simulation results on power reported
r−1
n
n
in Section 4, the RJAR test yields smaller conﬁdence intervals than the BCCH sup

j(cid:54)=i(P

(cid:80)n

(cid:80)

i=1

−

−

score test. The qualitative conclusions with respect to the case of 38 IVs remain

unchanged.

22

0.000.250.500.751.00-0.10-0.050.000.05βh,01−p-value0.000.250.500.751.00-0.2-0.10.00.1βc,01−p-valueARCMSARMSSRJAR(a) High-school workers

(b) College workers

Figure 10: 95 % conﬁdence sets for βs for the application in Equation (12) with kn =
(cid:80)

(cid:80)n

342 IVs. γ∗

n = 5.299, r−1
n

i=1

j(cid:54)=i(P

γ∗
ij )2 = 0.106.
n

6 Conclusion

We contributed to the literature on (very) many IVs in the cross-sectional linear IV

model by proposing a new, ridge-regularised jackknifed AR test. Our test compares

favourably with existing methods in the literature both theoretically, by allowing for

high-dimensional IVs and weakening a common assumption on the IVs’ projection

matrix, and practically, by having correct asymptotic size and displaying favourable

power properties even when the number of IVs approaches or exceeds the number of

observations.

23

0.000.250.500.751.00-0.10.00.00.0βh,01−p-value0.000.250.500.751.00-0.2-0.2-0.10.00.00.00.1βc,01−p-valueSRJARAppendix A Proofs

Throughout this appendix, C > 0 denotes a universal constant that can change

across lines. CSHNW refers to Chao et al. (2012) from where we also borrow our

summation conventions. Since all the proofs in this appendix are under the null
hypothesis, we write εi and ˆΦγn for ei(β0) and ˆΦγn, respectively. We also deﬁne
Φγn := 2
rn

ij )2E[ε2

j(cid:54)=i(P γn

i ]E[ε2
j ],

(cid:80)n

(cid:80)

i=1

The following singular value decomposition of the n
rank rn will be used frequently (see, e.g., L¨utkepohl (1996, p. 60)):

×

kn matrix of instruments Z of

Z = U SQ(cid:48),

where U is an n
×
that Q(cid:48)Q = QQ(cid:48) = Ikn, and S is the n

n matrix such that U (cid:48)U = U U (cid:48) = In, Q is a kn

kn matrix given by

×

kn matrix such

×





D
0(n−rn)×rn

0rn×(kn−rn)
0(n−rn)×(k−rn)



 ,

S =

where D is the diagonal rn
one can write

×

rn matrix containing the singular values of Z. Hence,

P γn = U SQ(cid:48)(QS(cid:48)SQ(cid:48) + QγnIknQ(cid:48))−1QS(cid:48)U (cid:48) = U S(S(cid:48)S + γnIkn)−1S(cid:48)U (cid:48) = U ˜DU (cid:48),

(A.1)

where ˜D = S(S(cid:48)S + γnIkn)−1S(cid:48) is the diagonal n
D2
given by given by ˜Dll =
ll
ll+γn ≤
the diagonal entries of ˜D are also the the eigenvalues of P γn.

n matrix with diagonal entries
1 for l = 1, . . . , rn, and zero otherwise. Note that

D2

×

A.1 Lemmas

The following lemma collects some properties of P γn. Recall that Z has rank rn.

Lemma 1. Fix n
rn < kn one has

≥

3. For all h, m = 1, . . . , n and γn

0 if rn = kn and γn > 0 if

≥

1 for all positive integers j,

(i) 0

(P γn)j

≤
(ii) (cid:80)n
i=1(P γn
i=1 P γn

(iii) (cid:80)n

hh ≤

P γn
hh ,

P γn
hh ≤
hi )2 = (P γn)2
hh ≤
D2
ii = (cid:80)rn
ll
ll+γn ≤
D4
ll
ll+γn)2 ≤

ii = (cid:80)rn

(D2

rn,

l=1

l=1

D2

(iv) (cid:80)n

i=1(P γn)2

(v)

P γn
hm| ≤
|

1,

rn,

24

2 and any

}

3
I

1, . . . , n

⊆ {

3,
}

(vi) For any

1, . . . , n

2
⊆ {
I
I2(P γn
ij )4
≤
ij )2(P γn
I3(P γn

rn,

jl )2

(a) (cid:80)

(b) (cid:80)

rn.

≤

D2

D2
ll
ll+γn

Proof. By the arguments prior to this lemma, the non-zero eigenvalues of P γn are
˜Dll =
for l = 1, . . . , rn. (iii) now follows from the trace of P γn equaling the
sum of its non-zero eigenvalues. Furthermore, the eigenvalues of (P γn)2 are given by
D4
˜D2
ll
ll+γn)2 for l = 1, . . . , rn, and zero otherwise. (iv) now follows in the same way
ll =
as (iii).

(D2

By Equation (A.1) one has that

(P γn)j

hh =

n
(cid:88)

l=1

˜Dj

llU 2

hl ≤

n
(cid:88)

l=1

˜DllU 2

hl = P γn

hh ≤

1,

(A.2)

which veriﬁes (i) since 0
1; the last inequality following from the largest
diagonal entry of P γn being bounded from above by its largest eigenvalue, which is no
greater than one. Next, by (i),

≤

≤

˜Dll

n
(cid:88)

i=1

(P γn

hi )2 = (P γn)2

hh ≤

P γn
hh ≤

1,

such that (ii) and (v) follow. Furthermore, by (v) and (iv),

(cid:88)

(P γn

ij )4

I2

and (vi) follows since

n
(cid:88)

n
(cid:88)

≤

i=1

j=1

(P γn

ij )2 =

n
(cid:88)

i=1

(P γn)2

ii ≤

rn,

(P γn

ij )2(P γn

jl )2

(cid:88)

I3

n
(cid:88)

n
(cid:88)

≤

j=1

i=1

(P γn

ij )2

n
(cid:88)

l=1

(P γn

jl )2 =

n
(cid:88)

(cid:16)

j=1

(cid:17)2

(P γn)2
jj

n
(cid:88)

j=1

≤

(P γn)2

jj ≤

rn,

the penultimate inequality being a consequence of (i) and the last of (iv).

Lemma 2. Fix n

4. For all γn

≥

≥

0 if rn = kn and γn > 0 if rn < kn one has

(cid:88)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
i<j<l<m

P γn
il P γn

jl P γn

im P γn

jm

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Crn.

≤

Proof. The proof follows Lemma B2 in CSHNW closely. For G := diag(P γn

11 , . . . , P γn
nn )

25

observe that (P γn

G)4 equals

−

(P γn)2GP γn + (P γn)2G2

P γnG(P γn)2 + P γnGP γnG

(P γn)3G

(P γn)4
−
+P γnG2P γn

−
P γnG3
GP γnG2 + G2(P γn)2

−

−

−

−

−

G(P γn)3 + G(P γn)2G + GP γnGP γn
G3P γn + G4.
G2P γnG

−

Since tr(A(cid:48)) = tr(A) and tr(AB) = tr(BA) for square matrices A and B,

tr((P γn

−

G)4) =tr((P γn)4)

4tr((P γn)3G) + 4tr((P γn)2G2) + 2tr(P γnGP γnG)

−

4tr(P γnG3) + tr(G4)

−
tr((P γn)3G)
tr((P γn)4) + 4
|
|
+ tr(G4).
tr(P γnG3)
|
|

+ 4

≤

+ 4tr((P γn)2G2) + 2tr(P γnGP γnG)

Next, because 0
Gii

0, one gets

≥

(P γn)j

ii ≤

≤

P γn
ii

for all positive integers j by Lemma 1 (i), and

tr((P γn

G)4)

−

≤

tr(P γn) + 4tr(P γnG) + 4tr(P γnG2) + 2tr(P γnGP γnG)
+ 4tr(P γnG3) + tr(G4).

Since 0
tr(P γn)
tr(P γn)

≤

≤

≤

P γn
1 and Gj
ii ≤
rn, tr(P γnGP γnG)
rn. Hence,

≤

In (elementwise) for all positive integers j, tr(P γnGj)

tr((P γn)2)

≤

≤

rn. Furthermore, tr(G4)

≤

≤
tr(G) =

tr((P γn

G)4)

−

16rn.

≤

(A.3)

As in the proof of Lemma B2 in CSHNW, deﬁne the lower-triangular matrix L with
entries Lij = P γn

ij 1i>j, so that P γn = L + L(cid:48) + G. Then

(P γn

−

G)4 =(L + L(cid:48))4

=L4 + L2LL(cid:48) + L2L(cid:48)L + L2L(cid:48)2 + LL(cid:48)L2 + LL(cid:48)LL(cid:48)

+ LL(cid:48)L(cid:48)L + LL(cid:48)3 + L(cid:48)LL2 + L(cid:48)LLL(cid:48) + L(cid:48)LL(cid:48)L + L(cid:48)LL(cid:48)2
+ L(cid:48)2L2 + L(cid:48)2LL(cid:48) + L(cid:48)2L(cid:48)L + L(cid:48)4.

Note for all positive integers j, [(L(cid:48))j](cid:48) = Lj. Since tr(A(cid:48)) = tr(A) and tr(AB) =
tr(BA) for any square matrices A and B,

tr((P γn

−

G)4) = 2tr(L4) + 8tr(L3L(cid:48)) + 4tr(L2L(cid:48)2) + 2tr(L(cid:48)LL(cid:48)L).

(A.4)

We consider each of the terms on the right-hand side of Equation (A.4) separately.

26

Before proceeding, note that:

(L)2

ab =

(L(cid:48))2

ab =

(L(cid:48)L)ab =

n
(cid:88)

l=1
n
(cid:88)

l=1
n
(cid:88)

LalLlb,

LlaLbl =

n
(cid:88)

l=1

LlaLlb,

LblLla = (L)2

ba,

l=1
n
(cid:88)

(L)2

alLlb =

n
(cid:88)

n
(cid:88)

LamLmlLlb,

(L)3

ab =

(L)4

ab =

l=1
n
(cid:88)

l=1

m=1
n
n
(cid:88)
(cid:88)

(L)2

al(L)2

lb =

n
(cid:88)

LamLmlLljLjb,

(A.5)

l=1
n
(cid:88)

l=1
n
(cid:88)

(L3L(cid:48))ab =

(L2L(cid:48)2)ab =

l=1
n
(cid:88)

m=1
n
(cid:88)

j=1
n
(cid:88)

(L)3

al(L(cid:48))lb =

LamLmjLjlLbl,

(L)2

al(L(cid:48))2

lb =

l=1
n
(cid:88)

l=1

m=1

j=1

(L)2

al(L)2

bl =

n
(cid:88)

n
(cid:88)

n
(cid:88)

l=1

m=1

j=1

LamLmlLbjLjl,

(L(cid:48)LL(cid:48)L)ab =

(L(cid:48)L)al(L(cid:48)L)lb =

n
(cid:88)

n
(cid:88)

n
(cid:88)

l=1

m=1

j=1

LmaLmlLjlLjb.

l=1
n
(cid:88)

l=1

Using the expression for (L)4

ab in Equation (A.5),

tr(L4) =

n
(cid:88)

n
(cid:88)

n
(cid:88)

n
(cid:88)

i=1

l=1

m=1

j=1

LimLmlLljLji =

(cid:88)

i,j,l,m

= 0,

P γn
ji 1j>iP γn

im 1i>mP γn

ml 1m>lP γn

lj 1l>j

since there is no combination of indices i, j, l, m that jointly satisfy each of the indicator

(A.6)

functions, as this would require m < i < j < l < m.

Using the expression for (L3L(cid:48))ab in Equation (A.5),

tr(L3L(cid:48)) =

n
(cid:88)

n
(cid:88)

n
(cid:88)

n
(cid:88)

LimLmjLjlLil =

(cid:88)

P γn
im 1i>mP γn

mj1m>jP γn

jl 1j>lP γn

il 1i>l

i=1

l=1

(cid:88)

j=1

m=1
P γn
im P γn

mjP γn

jl P γn

il =

(cid:88)

i,j,l,m
P γn
ml P γn

lj P γn

ji P γn

mi

l<j<m<i
(cid:88)

i<j<l<m

P γn
ij P γn

jl P γn

im P γn
lm .

i<j<l<m

=

=

The third equality follows since the product within the summations is non-zero only

for l < j < m < i. The fourth equality follows by replacing the indices according to

(A.7)

27

the dictionary
P γn.

{

i : m, j : j, l : i, m : l

. The last equality follows from the symmetry of
}

Using the expression for (L2L(cid:48)2)ab in Equation (A.5),

tr(L2L(cid:48)2) =

n
(cid:88)

n
(cid:88)

n
(cid:88)

n
(cid:88)

LimLmlLijLjl

i=1
(cid:88)

j=1

m=1

l=1
im 1i>mP γn
P γn

ml 1m>lP γn

ij 1i>jP γn

jl 1j>l

=

i,j,l,m
(cid:88)

im P γn
P γn

ml P γn

ij P γn

jl =

(cid:88)

ij P γn
P γn

jl P γn

lm P γn

mi

i>j>l,i>m>l
ij P γn
P γn

(cid:88)

jl P γn

lm P γn

mi +

i>j>l,i>m>l
(cid:88)
ij P γn
P γn

jl P γn

lm P γn

mi

i>j=m>l
(cid:88)

+

ij P γn
P γn

jl P γn

lm P γn

mi

i>j>m>l

(cid:88)

i>m>j>l
ij P γn
P γn

jl P γn

lj P γn

ji +

(cid:88)

i<j<l<m

ml P γn
P γn

li P γn

ij P γn

jm

(A.8)

P γn
ij P γn

jl P γn

lm P γn

mi

(cid:88)

i>m>j>l
ij P γn
P γn

jl P γn

lj P γn

ji +

i>j>l

(cid:88)

+

i>j>l

(cid:88)

+

mjP γn
P γn

ji P γn

il P γn

lm

(cid:88)

i<j<l<m

ml P γn
P γn

li P γn

ij P γn

jm

i<j<l<m
(P γn

(cid:88)

ij )2(P γn

jl )2 + 2

i<j<l

(cid:88)

i<j<l<m

P γn
ij P γn

il P γn

jmP γn
lm .

=

=

=

=

=

The third equality follows since the product within the summations is non-zero only

when both i > j > l and i > m > l. The fourth equality follows from the symmetry
of P γn. The sixth equality follows by replacing the indices in the second summation
. The seventh equality follows by
according to the dictionary
}
i : m, j :

replacing the indices in the third summation according to the dictionary
j, l : i, m : l

. The last equality follows from the symmetry of P γn.

i : m, j : l, l : i, m : j
{

{

}

28

Using the expression for (L(cid:48)LL(cid:48)L)ab in Equation (A.5),

tr(L(cid:48)LL(cid:48)L) =

n
(cid:88)

n
(cid:88)

n
(cid:88)

n
(cid:88)

LmiLmlLjlLji

=

=

=

=

i=1
(cid:88)

j=1

m=1

l=1
mi 1m>iP γn
P γn

ml 1m>lP γn

jl 1j>lP γn

ji 1j>i

i,j,l,m
(cid:88)

ij 1i>jP γn
P γn

l>j1l>jP γn

lm 1l>mP γn

im 1i>m

i,j,l,m
(cid:88)

ij P γn
P γn

lj P γn

lm P γn

im +

(cid:88)

ij P γn
P γn

lj P γn

lm P γn

im

m=j<i=l
(cid:88)

+

m=j<i<l
(cid:88)

m<j<i=l
(cid:88)

m<j<i<l
(cid:88)

+

+

+

ij P γn
P γn

lj P γn

lm P γn

im +

m=j<l<i
(cid:88)

j<m<i=l
(cid:88)

m<j<l<i
(cid:88)

j<m<l<i

ij P γn
P γn

lj P γn

lm P γn

im +

ij P γn
P γn

lj P γn

lm P γn

im +

ij P γn
P γn

lj P γn

lm P γn

im

ij P γn
P γn

lj P γn

lm P γn

im

ij P γn
P γn

lj P γn

lm P γn

im

ij P γn
P γn

lj P γn

lm P γn

im

(A.9)

(cid:88)

j<m<i<l
(P γn

ij )4 +

j<i

+

(cid:88)

(cid:88)

(P γn

ij )2(P γn

lj )2 +

j<l<i

(cid:88)

j<i<l

(P γn

ij )2(P γn

lj )2

(P γn

ij )2(P γn

im )2 +

(cid:88)

(P γn

ij )2(P γn

im )2

j<m<i

(cid:88)

+ 4

m<j<i

li P γn
P γn

mi P γn

lj P γn

mj

i<j<l<m
(P γn

ij )4 + 2

=

(cid:88)

j<i

(cid:88)

i<j<l

(P γn

li )2(P γn

ji )2

(cid:88)

+

(P γn

li )2(P γn

lj )2 + 4

(cid:88)

i<j<l<m

li P γn
P γn

mi P γn

lj P γn

mj

(cid:88)

(cid:16)

i<j<l

(P γn

ij )2(P γn

il )2 + (P γn

il )2(P γn

jl )2(cid:17)

i<j<l
(P γn

(cid:88)

ij )4 + 2

=

i<j

+ 4

(cid:88)

i<j<l<m

P γn
il P γn

jl P γn

im P γn
jm.

The third equality follows by replacing the indices in the summation according to the

dictionary

i : j, j : i, l : m, m : l

. The fourth equality follows since the product

{

}

within the summation is non-zero only when both i > j, m and l > j, m. The ﬁfth

equality follows from replacing the indices in the last four summations in the previous

line according to the dictionaries
i : m, j : i, l : l, m : j
{
follows from replacing the indices in the second, third, fourth and ﬁfth summations

,
}
, respectively. The sixth equality
}

i : m, j : j, l : l, m : i
}

i : l, j : i, l : m, m : j

i : l, j : j, l : m, m : i

{

}

{

{

,

,

in the previous line according to the dictionaries

i : l, j : i, l : j

i : j, j : i, l : l

,

}

{

,
}

{

29

i : l, j : i, m : j
{
}
symmetry of P γn.

,

i : l, j : j, m : i
{

, respectively. The last equality follows by the
}

Let S := (cid:80)
il P γn
im P γn
the expressions in Equations (A.6)–(A.9) into Equation (A.4) yields

i<j<l<m P γn

lm + P γn

jm + P γn

jl P γn

il P γn

jl P γn

ij P γn

ij P γn

im P γn

jmP γn

lm . Substituting

tr((P γn

−

G)4) =8

(cid:88)

ij P γn
P γn

jl P γn

im P γn

lm

i<j<l<m


(cid:88)

+ 4



i<j<l

(P γn

ij )2(P γn

jl )2 + 2

(cid:88)

i<j<l<m

ij P γn
P γn

il P γn

jmP γn

lm







(cid:88)



i<j

+ 2

(P γn

ij )4 + 2

(cid:88)

(cid:16)

i<j<l

(P γn

ij )2(P γn

il )2 + (P γn

il )2(P γn

jl )2(cid:17)

il P γn
P γn

jl P γn

im P γn

jm





(cid:88)

+ 4

i<j<l<m

=2

(cid:88)

i<j

(P γn

ij )4

(P γn

ij )2(P γn

jl )2 + (P γn

il )2(P γn

jl )2 + (P γn

ij )2(P γn

il )2(cid:17)

(cid:88)

(cid:16)

+ 4

i<j<l

+ 8S.

Next, by the triangle inequality and Lemma 1 (vi),

(cid:88)

(cid:16)

(P γn

ij )2(P γn

jl )2 + (P γn

il )2(P γn

jl )2 + (P γn

ij )2(P γn

il )2(cid:17)

(A.10)

S

| ≤

|

1
4

(cid:88)

(P γn

ij )4

i<j
1
2

+

+

1
8
Crn.

≤

i<j<l

tr((P γn

G)4)

−

Take

ui
{
Deﬁne

}

to be a sequence of i.i.d. mean-zero and unit variance random variables.

∆1 :=

∆2 :=

(cid:88)

(cid:16)

i<j<l
(cid:88)

(cid:16)

i<j<l

P γn
ij P γn

il ujul + P γn

ij P γn

jl uiul + P γn

il P γn

jl uiuj

(cid:17)

,

P γn
ij P γn

il ujul + P γn

ij P γn

jl uiul

(cid:17)

, ∆3 :=

(cid:88)

(cid:16)

P γn
il P γn

jl uiuj

(cid:17)

.

i<j<l

30

Then by Lemma 1 (vi),

E[∆2

3] =

(cid:88)

(P γn

il )2(P γn

jl )2 + 2

(cid:88)

i<j<l<m

il P γn
P γn

jl P γn

im P γn

jm

i<j<l

rn + 2

≤

(cid:88)

i<j<l<m

il P γn
P γn

jl P γn

im P γn
jm.

Furthermore,

E[∆2∆3] =

(cid:88)

i<j<l<m

ij P γn
P γn

il P γn

jmP γn

lm +

(cid:88)

i<j<l<m

ij P γn
P γn

jl P γn

im P γn
lm ,

and

E[∆2

2] =

(cid:88)

{i,l}<j<k
(cid:88)

+

ij P γn
P γn

il P γn

mjP γn

ml +

(cid:88)

ij P γn
P γn

jmP γn

il P γn

lm

ij P γn
P γn

im P γn

jl P γn

lm +

i<{j,l}<m
(cid:88)

ij P γn
P γn

jmP γn

li P γn

lm

i<j<l<m
(P γn

(cid:88)

ij )2(P γn

il )2 +

l<i<j<m

(P γn

ij )2(P γn

jl )2 + 2

(cid:88)

i<j<l

(cid:88)

i<m<j<l

ij P γn
P γn

il P γn

mjP γn

ml

=

=

i<j<l

+ 2

(cid:88)

P γn
ij P γn

jmP γn

il P γn

lm

i<j<l<m
(cid:88)

ij P γn
P γn

im P γn

jl P γn

lm +

+

i<j<l<m
(P γn

(cid:88)

ij )2(P γn

il )2 +

i<j<l

Crn,

≤

(cid:88)

i<j<l<m

jl P γn
P γn

lm P γn

ij P γn

im

(cid:88)

i<j<l

(P γn

ij )2(P γn

jl )2 + 2S

where the last inequality follows from Lemma 1 (vi) and Equation (A.10). Since
3] + 2E[∆2∆3]
1] = E[∆2
∆1 = ∆2 + ∆3, E[∆2
Crn. Hence by the
triangle inequality and the expression for E[∆2
3],

2] + E[∆2

Crn + 2S

≤

≤

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:88)

i<j<l<m

il P γn
P γn

jl P γn

im P γn

jm

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤

≤

≤

(cid:16)

C

E[∆2

3] + rn

(cid:17)

(cid:16)

C

E[(∆1

−

≤

∆2)2] + rn

(cid:17)

C(E[∆2

1] + E[∆2

2] + rn)

Crn.

Lemma 3. Let γ− > 0 and Γn = Γ(γ−) :=

γn
{

∈ (cid:60)

: γn

≥

0 if rn = kn, and γn

≥

31

γ− > 0 if rn < kn

}

. Then if P γn is a non-diagonal matrix,10

n
(cid:88)

(cid:88)

(P γn

ij )2

i=1

j(cid:54)=i

[0,

)
∞

∈

γ∗
n := max arg max

γn∈Γn

exists.

Proof. Notice that

n
(cid:88)

(cid:88)

i=1

j(cid:54)=i

(P γn

ij )2 =

=

=

n
(cid:88)

i=1
n
(cid:88)

i=1

rn(cid:88)

l=1

n
(cid:88)

(P γn

ij )2

j=1

(P γn)2

ii −

(cid:32)

D2
ll
ll + γn

D2

n
(cid:88)

−

i=1

(P γn

ii )2

n
(cid:88)

(P γn

ii )2

i=1
(cid:33)2

n
(cid:88)





rn(cid:88)

i=1

l=1


2

U 2
il



,

D2
ll
ll + γn

D2

−

where the third equality follows from Lemma 1 (iv) and Equation (A.2). Since

lim
γn→∞

D2
ll
ll + γn

D2

= 0,

and U 2

il ≤

1 for l = 1, . . . , rn and i = 1, . . . , n, it follows that

lim
γn→∞

n
(cid:88)

(cid:88)

i=1

j(cid:54)=i

(P γn

ij )2 = 0.

Hence, the maximum is not attained for arbitrarily large γn. Since P γn is a non-
diagonal matrix by assumption, (cid:80)n
j(cid:54)=i(P γn
ij )2 is strictly positive. This leaves
(cid:80)
a compact set over which the non-zero (cid:80)n
j(cid:54)=i(P γn
ij )2 is maximised, such that a
maximiser exists.

(cid:80)

i=1

i=1

Lemma 4. Under Assumptions 1, 2 and the null hypothesis in Equation (2),
Φγn|

p
→

0.

ˆΦγn −
|

Proof. Deﬁning ηi := ε2

i −

E[ε2

i ], one can write

ˆΦγn −

Φγn =

2
rn

n
(cid:88)

(cid:88)

(P γn

ij )2 (cid:16)

i=1

j(cid:54)=i

ηiηj + E[ε2

j ]ηi + E[ε2

i ]ηj

(cid:17)

,

10We note that Assumption 3 excludes the case where P γn is diagonal, so that γ∗

n exists under the

assumptions made in this paper.

32

and it follows that

(cid:12)
ˆΦγn −
(cid:12)
(cid:12)

Φγn

(cid:12)
(cid:12)
(cid:12) ≤

2
rn

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

n
(cid:88)

(cid:88)

(P γn

ij )2ηiηj

j(cid:54)=i

i=1
(cid:12)
(cid:12)
n
(cid:88)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

i=1

2
rn

+

n
(cid:88)

(P γn

ij )2E[ε2

i ]ηj

j(cid:54)=i

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

+

2
rn

(cid:12)
(cid:12)
n
(cid:88)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

i=1

(cid:88)

(P γn

ij )2E[ε2

j ]ηi

j(cid:54)=i

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(A.11)

Consider each of E[A2

2], and E[A2

3] in turn.

A1 + A2 + A3.

≡
1], E[A2

E[A2

1] =

=

=

+

=

=

=

≤

≤

4
r2
n

4
r2
n

+

4
r2
n

4
r2
n

+

8
r2
n

8
r2
n

+

8
r2
n

C
r2
n

C
rn

n
(cid:88)

(cid:88)

n
(cid:88)

(cid:88)

(P γn

ij )2(P γn

hg )2E[ηiηjηhηg]

i=1
n
(cid:88)

j(cid:54)=i

h=1

g(cid:54)=h

(cid:88)

(cid:88)

(cid:88)

(P γn

ij )2(P γn

hg )2E[ηiηjηhηg]

i=1

h(cid:54)=i

g(cid:54)=h

j(cid:54)=i
n
(cid:88)

4
r2
n

i=1

j(cid:54)=i

g(cid:54)=i

(cid:88)

(cid:88)

(P γn

ij )2(P γn

ig )2E[η2

i ηjηg]

n
(cid:88)

(cid:88)

(cid:88)

(cid:88)

i=1

j(cid:54)=i

h(cid:54)=i

g /∈{h,i}

(P γn

ij )2(P γn

hg )2E[ηi]E[ηjηhηg]

n
(cid:88)

(cid:88)

(cid:88)

(P γn

ij )2(P γn

ih )2E[η2

i ηjηh]

i=1

h(cid:54)=i

j(cid:54)=i
n
(cid:88)

4
r2
n

i=1

j(cid:54)=i

g(cid:54)=i

(cid:88)

(cid:88)

(P γn

ij )2(P γn

ig )2E[η2

i ηjηg]

n
(cid:88)

(cid:88)

(cid:88)

(P γn

ij )2(P γn

ih )2E[η2

i ηjηh]

i=1
n
(cid:88)

j(cid:54)=i

h(cid:54)=i

(cid:88)

(cid:88)

(P γn

ij )2(P γn

ih )2E[η2

i ]E[ηj]E[ηh]

i=1

j(cid:54)=i

h /∈{i,j}

8
r2
n

n
(cid:88)

(cid:88)

i=1

j(cid:54)=i

(P γn

ij )4E[η2

i ]E[η2
j ]

(P γn

ij )4E[η2

i ]E[η2
j ]

(P γn

ij )4

n
(cid:88)

(cid:88)

i=1
n
(cid:88)

j(cid:54)=i

(cid:88)

i=1

j(cid:54)=i

,

The fourth equality follows from E[ηi] = 0 and the symmetry of P γn. The sixth
equality follows from E[ηj] = 0. The ﬁrst inequality follows from Assumption 1, which

33

implies supi∈NE[η2
concerning E[A2

i ] <
∞
2] one has

. The second inequality follows from Lemma 1 (vi). Next,

E[A2

2] =

=

+

=

≤

≤

4
r2
n

4
r2
n

4
r2
n

4
r2
n

C
r2
n

C
rn

n
(cid:88)

(cid:88)

n
(cid:88)

(cid:88)

(P γn

ij )2(P γn

hg )2E[ε2

j ]E[ε2

g]E[ηiηh]

i=1
n
(cid:88)

i=1
n
(cid:88)

i=1
n
(cid:88)

i=1
n
(cid:88)

j(cid:54)=i

h=1

g(cid:54)=h

(cid:88)

(cid:88)

(cid:88)

(P γn

ij )2(P γn

hg )2E[ε2

j ]E[ε2

g]E[ηi]E[ηh]

j(cid:54)=i

h(cid:54)=i

g(cid:54)=h

(cid:88)

(cid:88)

j(cid:54)=i

g(cid:54)=i

(cid:88)

(cid:88)

j(cid:54)=i

g(cid:54)=i

(cid:88)

(cid:88)

(P γn

ij )2(P γn

ig )2E[ε2

j ]E[ε2

g]E[η2
i ]

(P γn

ij )2(P γn

ig )2E[ε2

j ]E[ε2

g]E[η2
i ]

(P γn

ij )2(P γn

ig )2

i=1

j(cid:54)=i

g(cid:54)=i

,

by the same reasoning that led to the penultimate display. Finally, similar arguments
imply that E[A2
3]
that ˆΦγn −

such that by Markov’s inequality and Equation (A.11) it follows
≤
−1/2
Φγn = Op(r
n

). Thus, by Assumption 2,

ˆΦγn −
|

Φγn|

p
→

C
rn

0.

A.2 Proof of Proposition 1

By the assumptions in the statement, P has rank kn and Pii
that kn < n. Notice that

1

−

≤

δ, which implies

1
kn

n
(cid:88)

(cid:88)

(Pij)2 =

i=1

j(cid:54)=i

=

1
kn

1
kn





n
(cid:88)

n
(cid:88)

(Pij)2

i=1

j=1



(Pii)2

 =

1
kn

n
(cid:88)

i=1

−





n
(cid:88)

i=1

(P 2)ii

n
(cid:88)

−

i=1

(Pii)2









n
(cid:88)

i=1

Pii

−

n
(cid:88)

i=1



(Pii)2



1
kn

≥





n
(cid:88)

i=1

Pii

(1

δ)

−

−



Pii

 = δ,

n
(cid:88)

i=1

where the third equality follows from the idempotency of P , the ﬁrst inequality follows
since maxiPii
1
≤
since tr(P ) = kn.

δ by assumption of the statement, and the last equality follows

−

Next, observe that

n
(cid:88)

(cid:88)

i=1

j(cid:54)=i

(P γn

ij )2 =

=

n
(cid:88)

(cid:88)

i=1
n
(cid:88)

j(cid:54)=i

(cid:88)

i=1

j(cid:54)=i

(Pij)2 +

(Pij)2 +

n
(cid:88)

(cid:88)

i=1
n
(cid:88)

j(cid:54)=i
n
(cid:88)

i=1

j=1

(P γn

ij )2

(P γn

ij )2

n
(cid:88)

i=1
n
(cid:88)

(cid:88)

(Pij)2

j(cid:54)=i
n
(cid:88)

(Pij)2

i=1

j=1

−

−

34

n
(cid:88)

(Pii)2

+

i=1

n
(cid:88)

i=1

−

(P γn

ii )2.

Furthermore, by Equation (A.1) Pii
in Equation (A.2)) such that (cid:80)n

P γn
ii ≥
≥
i=1(Pii)2
−

0 for all γn
i=1(P γn
ii )2

(cid:80)n

0 (see also the last equality

0. It follows that

≥

≥

n
(cid:88)

(cid:88)

i=1

j(cid:54)=i

(P γn

ij )2

≥

n
(cid:88)

(cid:88)

(Pij)2

i=1

j(cid:54)=i

P

(
||

2
F − ||
||

P γn

2
F ).
||

−

(A.12)

In addition,

P

||

2
F − ||
||

P γn

P γn
||
P

||

F )(

P
||
P γn

F
||
F )

P γn

F )

||

− ||

(A.13)

2
F = (
||
||
2

≤

2

≤

P

F +

||

P
||
(cid:112)

||
kn

F (

||
P

||

−

− ||

F
||
P γn

F ,

||

||

the inequalities following from
P γn
||

F and

P

||

||

||

≤ ||
F ). Using Equation (A.1) once more yields that

P

F = √kn (using Equation (A.1) compare
||

P γn
||

F
||

P

−

P γn = U ( ˇD

˜D)U (cid:48),

−

where ˇD is the n
×
matrix. Therefore,

n diagonal matrix with upper left block being the kn

kn identity

×

P γn

P

||

−

2
F = tr([P
||
kn(cid:88)
(cid:18)

=

i=1

P γn](cid:48)[P

−

−

D2
ii
ii + γn

D2

1

−

P γn]) = tr(U ( ˇD
(cid:19)2

−

˜D)2U (cid:48))

.

(A.14)

Combining Equations (A.12)–(A.14) implies that

n
(cid:88)

(cid:88)

i=1

j(cid:54)=i

(P γn

ij )2

≥

n
(cid:88)

(cid:88)

(Pij)2

i=1

j(cid:54)=i

(cid:112)
2

kn

−

(cid:118)
(cid:117)
(cid:117)
(cid:116)

kn(cid:88)

(cid:18)
1

i=1

D2
ii
ii + γn

D2

−

(cid:19)2

,

such that

Since

1
kn

n
(cid:88)

(cid:88)

i=1

j(cid:54)=i

(P γn

ij )2

1
kn

≥

n
(cid:88)

(cid:88)

(Pij)2

i=1

j(cid:54)=i

−

(cid:118)
(cid:117)
(cid:117)
(cid:116)
2

1
kn

kn(cid:88)

(cid:18)
1

i=1

D2
ii
ii + γn

D2

−

(cid:19)2

.

1
kn

n
(cid:88)

(cid:88)

i=1

j(cid:54)=i

(Pij)2

1
kn

≤

n
(cid:88)

n
(cid:88)

(Pij)2 =

i=1

j=1

1
kn

tr(P 2) =

1
kn

tr(P ) = 1,

35

and γn
(cid:55)→
there exists a γ0

1
kn

(cid:0)1

D2
(cid:80)kn
ii
i=1
ii+γn
−
n > 0 such that

D2

(cid:1)2

is continuous, 0 at 0, and tends to 1 as γn

,
→ ∞

(cid:118)
(cid:117)
(cid:117)
(cid:116)
2

1
kn

kn(cid:88)

(cid:18)
1

i=1

D2
ii
ii + γ0
n

D2

−

(cid:19)2

=

1
2kn

n
(cid:88)

(cid:88)

i=1

j(cid:54)=i

(Pij)2

ensuring that

1
kn

n
(cid:88)

(cid:88)

(P

γ0
ij )2
n

i=1

j(cid:54)=i

1
2kn

≥

n
(cid:88)

(cid:88)

(Pij)2

i=1

j(cid:54)=i

δ/2.

≥

(A.15)

(cid:80)

j(cid:54)=i(P γn

ij )2,

Finally, since γ∗

n is the penalty parameter that maximises γn

(cid:80)n

i=1

(cid:55)→

n
(cid:88)

(cid:88)

i=1

j(cid:54)=i

(P

γ∗
ij )2
n

≥

n
(cid:88)

(cid:88)

(P

γ0
ij )2
n

i=1

j(cid:54)=i

kδ
2

,

≥

so that Assumption 3 is satisﬁed for ω = 1.

A.3 Proof of Theorem 1

We begin by showing that

RJAR∗

γn :=

1
(cid:112)Φγn

√rn

n
(cid:88)

(cid:88)

i=1

j(cid:54)=i

P γn
ij εiεj

(A.16)

converges in distribution to a standard normal for all sequences of γn that satisfy
n := 2 (cid:80)n
Assumption 3.11 To this end, deﬁne
2
n] and note
i=2
U
that by the symmetry of P γn, RJAR∗
γn can be written as

ij εiεj, s2

j=1 P γn

n := E[

(cid:80)i−1

U

RJAR∗

γn = s−1
n U

n.

We proceed by establishing that s−1
n
from Hall and Heyde (1980, Corollary 3.1)) upon verifying that i) for all (cid:15) > 0

[0, 1] as n

→ ∞

n U

. This, in turn, follows

d
→ N

s−2
n

n
(cid:88)

i=2

E[Y 2

niI(
|

Yni

|

> (cid:15)sn)]

0

→

as n

,
→ ∞

(A.17)

where Yni = 2 (cid:80)i−1

j=1 P γn

ij εiεj, and ii)

2
s−2
n
n V

p
→

1

as n

,
→ ∞

(A.18)

11We do not directly invoke Lemma 2 in Hansen and Kozbur (2014) because it would require
strengthening the assumptions used in this paper. In particular, we would have to replace the condition
in Assumption 3 that (cid:80)n
ij )2 ≥
ckn.

n with the stronger condition that (cid:80)n

ij )2 ≥ crω

j(cid:54)=i(P γn

j(cid:54)=i(P γn

(cid:80)

(cid:80)

i=1

i=1

36

where

2

n = (cid:80)n
V

i=2

and write

E[Y 2
ni|

ε1, . . . , εi−1]. Consider ﬁrst the condition in Equation (A.17)

E[Y 2

ni] = 4

i−1
(cid:88)

i−1
(cid:88)

j=1

h=1

ij P γn
P γn

ih

E[ε2

i εjεh] = 4

i−1
(cid:88)

(P γn

ij )2E[ε2

i ]E[ε2
j ],

j=1

so that, upon using that

U

n = (cid:80)n

i=2 Yni and E[YniYnj] = 0 for i

= j, one gets

s2
n =

n
(cid:88)

i=2

E[Y 2

ni] = 4

n
(cid:88)

i−1
(cid:88)

i=2

j=1

(P γn

ij )2E[ε2

i ]E[ε2
j ].

Furthermore,

E[Y 4

ni] =16

i−1
(cid:88)

i−1
(cid:88)

i−1
(cid:88)

i−1
(cid:88)

j=1

h=1

m=1

l=1

ij P γn
P γn

ih P γn

im P γn

il

E[ε4

i εjεhεmεl]

=16

i−1
(cid:88)

j=1

(P γn

ij )4E[ε4

i ]E[ε4

j ] + 48

i−1
(cid:88)

(cid:88)

(P γn

ij )2(P γn

ih )2E[ε4

i ]E[ε2

j ]E[ε2
h],

j=1

h(cid:54)=j

so that

n
(cid:88)

i=2

E[Y 4

ni] =16

n
(cid:88)

i−1
(cid:88)

i=2

j=1

(P γn

ij )4E[ε4

i ]E[ε4
j ]

+ 48

n
(cid:88)

i−1
(cid:88)

(cid:88)

i=2

j=1

h(cid:54)=j

It follows that

(P γn

ij )2(P γn

ih )2E[ε4

i ]E[ε2

j ]E[ε2
h].

h(cid:54)=j(P γn

ij )2(P γn

ih )2

ij )2 (cid:80)n

h=1(P γn

ih )2

(cid:80)n

i=2

(cid:80)n

i=1

(cid:80)i−1

j=1(P γn

(cid:80)n

j=1(P γn

ij )4 + (cid:80)n
i=2
(cid:16)(cid:80)n
(cid:80)

i=1

ij )2 + (cid:80)n
i=1
(cid:16)(cid:80)n
(cid:80)

i=1

(cid:80)i−1
j=1

j(cid:54)=i(P γn
(cid:80)n

(cid:80)
ij )2(cid:17)2
j=1(P γn
ij )2(cid:17)2
j(cid:54)=i(P γn
(cid:1)2

(cid:80)n

(cid:80)n

i=1(P γn)2
(cid:16)(cid:80)n

ii + (cid:80)n
(cid:80)

i=1
ii + (cid:80)n
i=1(P γn)2
(cid:16)(cid:80)n
(cid:80)
j(cid:54)=i(P γn

i=1

(cid:0)(P γn)2
ii
ij )2(cid:17)2
i=1(P γn)2
ii
ij )2(cid:17)2

j(cid:54)=i(P γn

(cid:80)n

i=2

E[Y 4
ni]
s4
n

C

≤

C

≤

= C

C

≤

C

≤

(cid:80)n

i=1
i=1(P γn)2
ii
r2ω
n

Cr1−2ω
n

0,

≤

→

37

(A.19)

(A.20)

(cid:54)
where the ﬁrst inequality follows from Assumption 1. The second inequality follows

from Lemma 1 (v). The third inequality follows from Lemma 1 (i). The fourth

inequality follows from Assumption 3. The ﬁfth inequality follows from Lemma 1

(iii). The limit holds by Assumption 2 and Assumption 3. The condition in Equation

(A.17) follows from Equation (A.20).

In order to verify the convergence in Equation (A.18) it suﬃces to show that

(cid:104)

s−4
n

E

2
(
n −
V

n)2(cid:105)
s2

=

E[
n] + s4
4
V

n −
s4
n

2s2
n

E[

2
n]

V

=

4
n]

E[
V
s4
n

+ 1

2

E[
2
n]
V
s2
n →

−

0.

Since

2

n = (cid:80)n
V

i=2

E[Y 2
ni|

ε1, . . . , εi−1] it follows that

2
n] =

E[
V

n
(cid:88)

i=2

E[Y 2

ni] = s2
n,

the last equality following from Equation (A.19). It remains to be veriﬁed that

E[
4
n]
V
s4
n →

1.

(A.21)

To this end, observe that

2
n =

V

n
(cid:88)

i=2

E[Y 2
ni|

ε1, . . . , εi−1] = 4

n
(cid:88)

i−1
(cid:88)

i−1
(cid:88)

i=2

j=1

h=1

ij P γn
P γn

ih

E[ε2

i ]εjεh,

such that

4
n = 16

V

n
(cid:88)

n
(cid:88)

i−1
(cid:88)

i−1
(cid:88)

j−1
(cid:88)

j−1
(cid:88)

i=2

j=2

h=1

l=1

m=1

w=1

ih P γn
P γn

il P γn

jmP γn

jw

E[ε2

i ]E[ε2

j ]εhεlεmεw.

j, tedious but straightforward calculations included for completeness yield

For i

E

≤




i−1
(cid:88)

i−1
(cid:88)

j−1
(cid:88)

j−1
(cid:88)

ih P γn
P γn

il P γn

jmP γn

jw εhεlεmεw





h=1

l=1

m=1

w=1

i−1
(cid:88)

i−1
(cid:88)

j−1
(cid:88)

j−1
(cid:88)

=

l=1

h=1

m=1

w(cid:54)=m

P γn
il P γn

ih P γn

jmP γn

jw

E[εlεhεmεw]

+

+

i−1
(cid:88)

i−1
(cid:88)

j−1
(cid:88)

l=1

h(cid:54)=l

m(cid:54)=l

P γn
il P γn

ih (P γn

jm)2E[εl]E[εhε2
m]

i−1
(cid:88)

i−1
(cid:88)

l=1

h(cid:54)=l

P γn
il P γn

ih (P γn

jl )2E[ε3

l ]E[εh] +

i−1
(cid:88)

j−1
(cid:88)

(P γn

il )2(P γn

jm)2E[ε2

l ε2
m]

l=1

m=1

38

=

=

i−1
(cid:88)

i−1
(cid:88)

j−1
(cid:88)

j−1
(cid:88)

l=1

h=1

m=1

w(cid:54)=m

i−1
(cid:88)

i−1
(cid:88)

j−1
(cid:88)

j−1
(cid:88)

l=1

h(cid:54)=l

m=1

w(cid:54)=m

il P γn
P γn

ih P γn

jmP γn

jw

E[εlεhεmεw] +

i−1
(cid:88)

j−1
(cid:88)

(P γn

il )2(P γn

jm)2E[ε2

l ε2
m]

il P γn
P γn

ih P γn

jmP γn

jw

l=1

m=1

E[εlεhεmεw]

i−1
(cid:88)

j−1
(cid:88)

j−1
(cid:88)

(P γn

il )2P γn

jmP γn

jw

E[ε2

l εw]E[εm]

l=1

m(cid:54)=l

w(cid:54)=m

i−1
(cid:88)

j−1
(cid:88)

(P γn

il )2P γn

jl P γn

jw

E[ε3

l ]E[εw] +

i−1
(cid:88)

j−1
(cid:88)

(P γn

il )2(P γn

jm)2E[ε2

l ε2
m]

+

+

l=1

w(cid:54)=l

i−1
(cid:88)

i−1
(cid:88)

j−1
(cid:88)

j−1
(cid:88)

l=1

h(cid:54)=l

m=1

w(cid:54)=m

i−1
(cid:88)

i−1
(cid:88)

j−1
(cid:88)

j−1
(cid:88)

l=1

h(cid:54)=l

m(cid:54)=h

w(cid:54)=m

=

=

il P γn
P γn

ih P γn

jmP γn

jw

il P γn
P γn

ih P γn

jmP γn

jw

i−1
(cid:88)

i−1
(cid:88)

j−1
(cid:88)

l=1

h(cid:54)=l

w /∈{h,l}

il P γn
P γn

ih P γn

jh P γn

jw

l=1

m=1

E[εlεhεmεw] +

i−1
(cid:88)

j−1
(cid:88)

(P γn

il )2(P γn

jm)2E[ε2

l ε2
m]

l=1

m=1

E[εlεhεmεw]

E[εl]E[εw]E[ε2
h]

i−1
(cid:88)

i−1
(cid:88)

l=1

h(cid:54)=l

P γn
il P γn

ih P γn

jh P γn

jl

E[ε2

l ]E[ε2

h] +

i−1
(cid:88)

j−1
(cid:88)

(P γn

il )2(P γn

jm)2E[ε2

l ε2
m]

+

+

i−1
(cid:88)

i−1
(cid:88)

j−1
(cid:88)

j−1
(cid:88)

l=1

h(cid:54)=l

m(cid:54)=h

w(cid:54)=m

P γn
il P γn

ih P γn

jmP γn

jw

i−1
(cid:88)

j−1
(cid:88)

+

(P γn

il )2(P γn

jm)2E[ε2

l ε2
m]

l=1

m=1

E[εlεhεmεw] +

i−1
(cid:88)

i−1
(cid:88)

l=1

h(cid:54)=l

P γn
il P γn

ih P γn

jh P γn

jl

E[ε2

l ]E[ε2
h]

=

=

=

=

l=1

m=1

i−1
(cid:88)

i−1
(cid:88)

j−1
(cid:88)

j−1
(cid:88)

l=1

h(cid:54)=l

m(cid:54)=h

w /∈{m,h}

il P γn
P γn

ih P γn

jmP γn

jw

E[εlεhεmεw]

i−1
(cid:88)

i−1
(cid:88)

j−1
(cid:88)

+

l=1

h(cid:54)=l

m /∈{h,l}

P γn
il P γn

ih P γn

jmP γn

jh

E[εl]E[εm]E[ε2
h]

+ 2

i−1
(cid:88)

i−1
(cid:88)

l=1

h(cid:54)=l

P γn
il P γn

ih P γn

jl P γn

jh

E[ε2

l ]E[ε2

h] +

i−1
(cid:88)

j−1
(cid:88)

(P γn

il )2(P γn

jm)2E[ε2

l ε2
m]

i−1
(cid:88)

i−1
(cid:88)

j−1
(cid:88)

j−1
(cid:88)

l=1

h(cid:54)=l

m(cid:54)=h

w /∈{m,h}

P γn
il P γn

ih P γn

jmP γn

jw

l=1

m=1

E[εlεhεmεw]

+ 2

i−1
(cid:88)

i−1
(cid:88)

l=1

h(cid:54)=l

P γn
il P γn

ih P γn

jl P γn

jh

E[ε2

l ]E[ε2

h] +

i−1
(cid:88)

j−1
(cid:88)

(P γn

il )2(P γn

jm)2E[ε2

l ε2
m]

i−1
(cid:88)

i−1
(cid:88)

j−1
(cid:88)

j−1
(cid:88)

l=1

h(cid:54)=l

m(cid:54)=h

w /∈{m,h,l}

il P γn
P γn

ih P γn

jmP γn

jw

l=1

m=1

E[εlεhεm]E[εw]

39

i−1
(cid:88)

i−1
(cid:88)

j−1
(cid:88)

l=1

h(cid:54)=l

m /∈{h,l}

il P γn
P γn

ih P γn

jmP γn

jl

E[ε2

l ]E[εh]E[εm]

i−1
(cid:88)

i−1
(cid:88)

l=1

h(cid:54)=l

i−1
(cid:88)

j−1
(cid:88)

il P γn
P γn

ih (P γn

jl )2E[ε3

l ]E[εh] + 2

i−1
(cid:88)

i−1
(cid:88)

l=1

h(cid:54)=l

il P γn
P γn

ih P γn

jl P γn

jh

E[ε2

l ]E[ε2
h]

(P γn

il )2(P γn

jm)2E[ε2

l ε2
m]

+

+

+

l=1

m=1

=2

=2

i−1
(cid:88)

i−1
(cid:88)

l=1

h(cid:54)=l

i−1
(cid:88)

i−1
(cid:88)

l=1

h(cid:54)=l

il P γn
P γn

ih P γn

jl P γn

jh

il P γn
P γn

ih P γn

jl P γn

jh

+

i−1
(cid:88)

l=1

(P γn

il )2(P γn

jl )2E[ε4
l ]

E[ε2

l ]E[ε2

h] +

E[ε2

l ]E[ε2

h] +

i−1
(cid:88)

j−1
(cid:88)

(P γn

il )2(P γn

jm)2E[ε2

l ε2
m]

l=1

m=1

i−1
(cid:88)

j−1
(cid:88)

l=1

m(cid:54)=l

(P γn

il )2(P γn

jm)2E[ε2

l ]E[ε2
m]

=2

i−1
(cid:88)

(cid:88)

h=1

l(cid:54)=h

ih P γn
P γn

il P γn

jh P γn

jl

E[ε2

h]E[ε2
l ]

+

i−1
(cid:88)

h=1

(P γn

ih )2(P γn

jh )2Var[ε2

h] +

i−1
(cid:88)

j−1
(cid:88)

h=1

l=1

(P γn

ih )2(P γn

jl )2E[ε2

h]E[ε2
l ],

where Var[ε2

h] := E[ε4
h]

E[ε2

h]E[ε2
h].

−

Deﬁning q := min
{

i, j

, the above display in turn implies
}

4
n] =32

E[
V

n
(cid:88)

i=2

E[ε2
i ]

n
(cid:88)

j=2

E[ε2
j ]

q−1
(cid:88)

(cid:88)

h=1

l(cid:54)=h

P γn
ih P γn

il P γn

jh P γn

jl

E[ε2

h]E[ε2
l ]

n
(cid:88)

i=2

n
(cid:88)

n
(cid:88)

j=2

n
(cid:88)

E[ε2
j ]

E[ε2
j ]

q−1
(cid:88)

h=1

i−1
(cid:88)

E[ε2
i ]

E[ε2
i ]

+ 16

+ 16

(P γn

ih )2(P γn

jh )2Var[ε2
h]

j−1
(cid:88)

(P γn

ih )2(P γn

jl )2E[ε2

h]E[ε2
l ]

j=2

h=1

l=1

i=2
An + Bn + s4
n.

≡

In order to establish the convergence in Equation (A.21), it is suﬃcient to show that

40

An+Bn
s4
n →

0. To this end, note that

n
(cid:88)

q−1
(cid:88)

(cid:88)

l(cid:54)=h

h=1

i=1


(P γn

ih )2(P γn

il )2





(A.22)

n
(cid:88)

n
(cid:88)

q−1
(cid:88)

(cid:88)

ih P γn
P γn

il P γn

jh P γn

jl

An

C

≤

j=1

h=1

l(cid:54)=h

i=1


n
(cid:88)

q−1
(cid:88)

(cid:88)

(cid:88)

ih P γn
P γn

il P γn

jh P γn

jl +

= C





C



≤

i=1

j(cid:54)=i

h=1

l(cid:54)=h

n
(cid:88)

(cid:88)

(cid:88)

(cid:88)

i=1

j<i

h<j

l(cid:54)=h

ih P γn
P γn

il P γn

jh P γn

jl + rn





n
(cid:88)

(cid:88)

(cid:88)

(cid:88)

ih P γn
P γn

il P γn

jh P γn

jl + rn

j<i

h<j

l<h

(cid:88)

(cid:88)

(cid:88)

j<i

h<j

l<h

ih P γn
P γn

il P γn

jh P γn

jl



+ rn



(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

C

≤

i=1
(cid:12)

(cid:12)
n
(cid:88)
(cid:12)
(cid:12)

(cid:12)
(cid:12)

i=1

C

Crn

≤

≤







The ﬁrst inequality follows from Assumption 1. The second inequality follows from
Lemma 1 (vi). The third inequality follows from the symmetry of P γn. The ﬁfth
inequality follows from Lemma 2. Next,

Bn

C

≤

n
(cid:88)

n
(cid:88)

q−1
(cid:88)

(P γn

ih )2(P γn

jh )2

i=1

j=1

h=1

Crn,

≤

(A.23)

the ﬁrst inequality following from Assumption 1 and the second from Lemma 1 (vi).

Thus,

An + Bn
s4
n

C

≤

(cid:16)(cid:80)n

i=1

rn
j(cid:54)=i(P γn

ij )2(cid:17)2 ≤

(cid:80)

Cr1−2ω

n →

0.

(A.24)

where the second inequality follows from Assumption 3 and the convergence from

Assumption 2 and Assumption 3. Equation (A.24) veriﬁes the condition in Equation

(A.21), which in turn implies that the condition condition in Equation (A.18) holds.

Having veriﬁed the conditions in Equation (A.17) and Equation (A.18) we conclude
ˆΦγn −
that RJAR∗
γn
under Assumptions 1 and 3. Hence, under the null hypothesis, the continuous mapping
theorem implies that RJARγn(β0)

0 and hence also ˆΦγn/Φγn

[0, 1]. By Lemma 4

d
→ N

Φγn|

[0, 1].

p
→

p
→

1

|

d
→ N

41

References

Anatolyev, Stanislav (2012). “Inference in regression models with many regressors”.

In: Journal of Econometrics 170.2, pp. 368–382.

Anatolyev, Stanislav and Nikolay Gospodinov (2011). “Speciﬁcation Testing in Models

with Many Instruments”. In: Econometric Theory 27.2, pp. 427–441.

Anatolyev, Stanislav and Pavel Yaskov (2017). “Asymptotics of Diagonal Elements of
Projection Matrices Under Many Instruments/Regressors”. In: Econometric Theory
33, pp. 717–738.

Anderson, Theodore and Herman Rubin (1949). “Estimation of the Parameters of a
Single Equation in a Complete System of Stochastic Equations”. In: The Annals of
Mathematical Statistics 20, pp. 46–63.

Andrews, Donald and James Stock (2007). “Testing with many weak instruments”. In:

Journal of Econometrics 138.1, pp. 24–46.

Bai, Jushan and Serena Ng (2010). “Instrumental Variable Estimation in a Data Rich

Environment”. In: Econometric Theory 26.6, pp. 1577–1606.

Bekker, Paul and Federico Crudu (2015). “Jackknife instrumental variable estimation

with heteroskedasticity”. In: Journal of Econometrics 185.2, pp. 332–342.

Belloni, Alexandre, Daniel Chen, Victor Chernozhukov, and Christian Hansen (2012).

“Sparse Models and Methods for Optimal Instruments With an Application to Em-
inent Domain”. In: Econometrica 80.6, pp. 2369–2429.

Blandhol, Christine, John Bonney, Magne Mogstad, and Alexander Torgovitsky (2022).
“When is TSLS Actually LATE?” In: Becker Friedman Institute Working Papers
2022-16, pp. 1–68.

Bun, Maurice, Helmut Farbmacher, and Rutger Poldermans (2020). “Finite sam-
ple properties of the GMM Anderson-Rubin test”. In: Econometric Reviews 39.10,
pp. 1042–1056.

Card, David (2009). “Immigration and Inequality”. In: American Economic Review

Papers and Proceedings 99.2, pp. 1–21.

Carrasco, Marine and Guy Tchuente (2016). “Regularization Based Anderson Rubin
Tests for Many Instruments”. In: University of Kent, School of Economics Discus-
sion Papers 1608, pp. 1–34.

Chao, John, Norman Swanson, Jerry Hausmann, Whitney Newey, and Tiemen Woutersen

(2012). “Asymptotic Distribution of JIVE in a Heteroskedastic IV Regression with
Many Instruments”. In: Econometric Theory 28, pp. 42–86.

Crudu, Federico, Giovanni Mellace, and Zsolt S´andor (2020). “Inference in Instrumen-
tal Variable Models with Heteroskedasticity and Many Instruments”. In: Economet-
ric Theory 77, pp. 1–30.

Gabaix, Xavier and Ralph Koijen (2020). “Granular Instrumental Variables”. In: NBER

Working Paper Series 28204, pp. 1–98.

42

Goldsmith-Pinkham, Paul, Isaac Sorkin, and Henry Swift (2020). “Bartik Instruments:
What, When, Why, and How”. In: American Economic Review 110.8, pp. 2586–2624.
Hall, Peter and Chris Heyde (1980). Martingale Limit Theory and Its Application.

Academic Press.

Hansen, Christian and Damian Kozbur (2014). “Instrumental variables estimation with
many weak instruments using regularized JIVE”. In: Journal of Econometrics 182.2,
pp. 290–308.

Kaﬀo, Maximilien and Wenjie Wang (2017). “On bootstrap validity for speciﬁcation
testing with many weak instruments”. In: Economics Letters 157, pp. 107–111.
Kapetanios, Georges, Lynda Khalaf, and Massimiliano Marcellino (2015). “Factor-
Based Identiﬁcation-Robust Interference in IV Regressions”. In: Journal of Applied
Econometrics 31.5, pp. 821–842.

L¨utkepohl, Helmut (1996). Handbook of Matrices. Wiley.
Mikusheva, Anna and Liyang Sun (2022). “Inference with Many Weak Instruments”.

In: Review of Economic Studies (Forthcoming), pp. 1–37.

Phillips, Peter C B and Wayne Yuan Gao (2017). “Structural inference from reduced
forms with many instruments”. In: Journal of Econometrics 199.2, pp. 96–116.

43

