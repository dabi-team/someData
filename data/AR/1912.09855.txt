Explainability and Adversarial Robustness for RNNs

Alexander Hartl, Maximilian Bachl, Joachim Fabini, Tanja Zseby
Technische Universit¨at Wien
ﬁrstname.lastname@tuwien.ac.at

0
2
0
2

b
e
F
9
1

]

G
L
.
s
c
[

2
v
5
5
8
9
0
.
2
1
9
1
:
v
i
X
r
a

Abstract—Recurrent Neural Networks (RNNs) yield attractive
properties for constructing Intrusion Detection Systems (IDSs)
for network data. With the rise of ubiquitous Machine Learning
(ML) systems, malicious actors have been catching up quickly to
ﬁnd new ways to exploit ML vulnerabilities for proﬁt. Recently
developed adversarial ML techniques focus on computer vision
and their applicability to network trafﬁc is not straightforward:
Network packets expose fewer features than an image, are
sequential and impose several constraints on their features.

We show that despite these completely different characteristics,
adversarial samples can be generated reliably for RNNs. To
understand a classiﬁer’s potential for misclassiﬁcation, we extend
existing explainability techniques and propose new ones, suitable
particularly for sequential data. Applying them shows that
already the ﬁrst packets of a communication ﬂow are of crucial
importance and are likely to be targeted by attackers. Feature im-
portance methods show that even relatively unimportant features
can be effectively abused to generate adversarial samples. We
thus introduce the concept of feature sensitivity which quantiﬁes
how much potential a feature has to cause misclassiﬁcation.

Since traditional evaluation metrics such as accuracy are not
sufﬁcient for quantifying the adversarial threat, we propose the
Adversarial Robustness Score (ARS) for comparing IDSs and
show that an adversarial training procedure can signiﬁcantly
and successfully reduce the attack surface.

I. INTRODUCTION

There is a signiﬁcant body of scientiﬁc work focusing on
the detection of unwanted behavior in networks. In the past, a
viable way of performing intrusion detection was to inspect the
content of packets themselves and detect if a packet delivers
potentially harmful content. More recently, with the increasing
deployment of encryption, the focus now lies on features that
are always available to network monitoring equipment like
packet sizes, protocol ﬂags or port numbers when encrypting
above the transport layer.

Network communication is usually aggregated into ﬂows,
which are commonly deﬁned as a sequence of packets shar-
ing certain properties. When analyzing ﬂows, not only the
aforementioned features are available but also features related
to the timing of the individual packets. Various approaches
have been proposed to extract features from ﬂows and then
perform anomaly detection with the extracted ﬂows [1]. While
these approaches frequently work well, it is problematic that
the whole ﬂow has to be received ﬁrst and only afterwards
anomaly detection is applied, revealing attack ﬂows. Thus, we
design a network IDS that operates on a per-packet basis and
decides if a packet is anomalous based on features that are
available even for trafﬁc that is encrypted above the transport
layer, like for example TLS or QUIC. At the same time, an

RNN-based IDS has the beneﬁt of providing any available
information to the classiﬁer while avoiding tedious feature
engineering procedures, which derive statistical measures from
the sequence of packet features. We show that our system
has similar performance to other ﬂow-based anomaly detection
systems but can detect anomalies before the ﬂow terminates.
However, for practical use, high detection accuracy is not
enough. With the recent rise of interest in Adversarial Machine
Learning (AML) techniques, also AML for IDSs has been in-
vestigated. For example, [2] investigates remedies for poison-
ing attacks on IDSs and [3] investigate adversarial robustness
of common network IDSs. In this research, we investigate
whether adversarial samples, i.e. minimally different attack
ﬂows which are classiﬁed as benign, can be found for our
RNN-based model. This is not a trivially answerable question
since adversarial samples have mostly been analyzed in the
context of computer vision. Our scenario signiﬁcantly deviates
from computer vision because 1) the number of features
is signiﬁcantly smaller and 2) only certain features can be
manipulated if the ﬂow should remain valid.

Surprisingly, we can conﬁrm that adversarial samples can be
found even when considering these tight constraints. Thus, we
argue that traditional performance metrics like, e.g., accuracy
are not sufﬁcient for evaluating security-sensitive ML systems.
Despite the well-known threat of adversarial samples, we ﬁnd
that related literature lacks metrics for adversarial robustness
and, in particular, no measure has been proposed for quanti-
fying the robustness of ML-based IDSs.

Due to the threat of adversarial attacks, but also as a basic
requirement for social acceptance of an ML-based system,
a further crucial requirement for ML-based IDSs is that the
classiﬁer’s decisions are interpretable by humans. This recently
stirred up increased interest in explainability methods. Also in
this case, common methods are designed to work with images
or tabular data, but not with sequences.

In this paper, we attempt

to provide a comprehensive
discussion of these security-related topics in the context of
RNNs. Our main contributions are:

• We analyze several methods for generating adversarial
samples and show that adversarial samples can be gen-
erated efﬁciently for an RNN-based classiﬁer.

• Based on common robustness notions of related works, we
propose the Adversarial Robustness Score (ARS) as a new
performance score for IDSs, which captures the notion of
how easily an adversary can generate adversarial samples.
We demonstrate that the ARS can be computed efﬁciently.

c(cid:13)2020 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including
reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or
reuse of any copyrighted component of this work in other works.

 
 
 
 
 
 
• We review methods for evaluating which features have
a signiﬁcant
impact on the classiﬁer’s prediction, both
picking up methods that have been proposed in the liter-
ature, extending them for RNNs, and devising new meth-
ods. Astonishingly, feature importance methods reveal that
features, which are manipulated for successful adversarial
ﬂows, are not even particularly important for the RNN’s
classiﬁcation outcome. Thus, we propose feature sensitivity
methods, which show how prone a feature is to cause
misclassiﬁcation.

• Going further, we investigate which packets have a sig-
niﬁcant contribution to the classiﬁer’s decision and which
values of these features lead to classiﬁcation as attack.
Hence, we extend existing explainability methods such as
Partial Dependence Plots (PDPs) [4] for sequential data.
• Based on the insights gained by the feature importance
and explainability methods, we ﬁnally propose two defense
methods. First, by simply leaving out manipulable features,
we obtain a classiﬁer which is slightly less accurate but
is still useful to be deployed in a real scenario. Second,
by deploying an adversarial training procedure, we can
reduce the attack surface and harden the resulting IDS
while retaining all features and similar classiﬁcation per-
formance. We show that the ARS is signiﬁcantly higher
for the hardened model.
We make all the source code, data, trained machine learning
models and ﬁgures freely available to enable reproducibility
and encourage experimentation at https://github.com/CN-TU/
adversarial-recurrent-ids.

II. AN RNN-BASED CLASSIFIER
We implemented a three-layer LSTM-based classiﬁer with
512 neurons at each layer. For a large-enough network, we do
not expect these architectural parameters to have a severe im-
pact on classiﬁcation accuracy, so we chose these parameters
to obtain a good performance while keeping training duration
at an acceptable level.

As the input features we use source port, destination port,
protocol identiﬁer, packet length, Interarrival time (IAT) to the

TABLE I: Flow occurrence frequency of attack types.

(a) CIC-IDS-2017

(b) UNSW-NB15

Attack type

Proportion

Attack type

Proportion

Exploits
Fuzzers
Reconnaissance
Generic
DoS
Shellcode
Analysis
Backdoors
Worms

1.42%
1.01%
0.58%
0.21%
0.19%
0.08%
0.03%
0.02%
0.01%

DoS Hulk
PortScan, Firewall
DDoS LOIT
Inﬁltration
DoS GoldenEye
DoS SlowHTTPTest
DoS Slowloris
Brute-force SSH
Botnet ARES
XSS attack
PortScan, no Fw.
Brute-force FTP
SQL injection
Heartbleed

10.10%
6.90%
4.08%
3.30%
0.32%
0.18%
0.17%
0.11%
0.03%
0.03%
0.02%
0.01%
<0.01%
<0.01%

TABLE II: Performance metrics per packet and per ﬂow. MLP
values from [2] are presented for comparison.

CIC-IDS-2017
Flow

Packet

MLP

UNSW-NB15
Flow

Packet

MLP

Accuracy
Precision
Recall
F1
Youden’s J

99.1% 99.7% 99.8% 99.5% 98.3% 98.9%
97.0% 99.7% 99.9% 83.4% 78.6% 84.5%
97.8% 99.1% 99.2% 87.6% 72.6% 82.9%
97.4% 99.4% 99.5% 85.5% 75.5% 83.7%
97.2% 99.0% 99.1% 87.3% 71.9% 82.3%

previous packet in the ﬂow, packet direction (i.e. forward or
reverse path) and all TCP ﬂags (0 if the ﬂow is not TCP).
We omitted Time-to-Live (TTL) values, as they are likely to
lead to unwanted prediction behavior [2]. Among the used
features, source port, destination port and protocol identiﬁer
are constant over the whole ﬂow while the other features vary.
During ﬂow extraction we used the usual 5-tuple ﬂow key,
which distinguishes ﬂows based on the protocol they use and
their source and destination port and IP address. We use Z-
score normalization to transform feature values to the range
appropriate for neural network training. We ensured that our
classiﬁers do not suffer from overﬁtting using a train/test split
of 2:1.

For evaluation, we use the CIC-IDS-2017 [5] and UNSW-
NB15 [6] datasets which each include more than 2 million
ﬂows of network data, containing both benign trafﬁc and a
large number of different attacks. Attacks contained in the
datasets are shown in Table I. Table II shows the achieved
classiﬁcation performance when evaluating metrics per packet
and per ﬂow and includes performance results for an MLP
classiﬁer from [2] for comparison. As depicted, our RNN-
based classiﬁers achieve an accuracy that is similar to previous
work based on these datasets [1, 2]. However, unlike these
classiﬁers, our recurrent classiﬁer has the advantage of being
able to detect attacks already before the attack ﬂows terminate.

III. ADVERSARIAL ATTACKS

We now investigate whether known AML methods can
be used for generating adversarial ﬂows for our RNN. [3]
previously studied the behavior of several Network IDSs under
adversarial attacks but unlike us did not investigate RNNs.

A network packet contains signiﬁcantly less features than,
e.g., an image and, furthermore, most features such as TCP
ﬂags cannot be easily manipulated, as their manipulation might
violate the protocol speciﬁcations and thus cause communi-
cation to fail. We identify the packet length and the IAT as
features which are most likely to be exploited and thus choose
them to be modiﬁed by the adversary. But even these features
cannot be manipulated due to problem-speciﬁc constraints:

• Only packets can be manipulated which are transmitted by
the attacker, except for botnet and backdoor trafﬁc, which
is entirely controlled by an attacker and thus only packets
travelling in one direction can be manipulated.

• Packets must not be smaller than initially, as otherwise less

information could be transmitted.

• IATs must not decrease, as otherwise the physical speed
of data transmission can be violated in some cases. An
in-depth analysis of cases in which reduction of IATs is
legitimate is complex, so we generally disallowed IAT
reductions.

Several AML methods have been proposed in the recent lit-
erature, achieving different speed-quality tradeoffs: [7] shows
that it is possible to create adversarial samples for an image,
which look similar to the original image but are classiﬁed
wrongly. [8] develops the Fast Gradient Sign Method (FGSM),
which makes easy generation of adversarial samples possible.
[9] explores the use of AML for RNNs, but lacks important
AML methods. We implemented the following AML methods.

A. Carlini-Wagner

We implemented the Carlini-Wagner method (CW) [10],

performing gradient descent on the optimization objective

d(X, ˜X) + κ max(Z( ˜X), δ).

(1)

Here, d(·) is a distance metric and κ ∈ R+ is a parameter
governing the tradeoff achieved between attack success and
distance from the original ﬂow. Furthermore, Z(·) denotes the
neural network’s logit output, X denotes the original ﬂow and
˜X the adversarial ﬂow optimized by CW.

δ ∈ R is a parameter that determines how far an adversary
wants to exceed the decision boundary. In the original publica-
tion δ = 0, meaning that the network’s decision for adversarial
samples is just between attack and benign trafﬁc. In the present
context, we need to make sure that the classiﬁer’s prediction
would actually be benign, even though a certain level of noise
will be added to IATs due to the network between attacker
and victim. Hence, we introduced δ = −0.2, corresponding to
a prediction conﬁdence of 55% for the sample to be benign
after the sigmoid activation function.

We used L1 as distance metric, as we consider L1 distance
to represent practically relevant differences of network ﬂows
best. We used Projected Gradient Descent (PGD) for meeting
the real-world constraints discussed above.

B. L∞-bounded Projected Gradient Descent

[11] uses a method for generating adversarial samples
which deploys PGD to maximize the network’s loss function
while constraining the achieved L∞ distance from the original
samples. Hence, an important difference to CW is that for this
method the network’s loss instead of its logit output is con-
sidered. We consider this method an interesting combination
of CW and FGSM. Since its functioning is somewhat similar
to CW, we expected the generated adversarial samples to be
similarly close to the original samples.

C. Fast Gradient Sign Method

Finally, we also tested the FGSM [8], which is the ﬁrst
method for generating adversarial samples. FGSM can be
considered a single pass of PGD on the loss function with

an equality constraint on the L∞ distance, i.e. the adversarial
sample is found as

˜X = X + (cid:15) sgn(∇X L(X)),

(2)

where L(X) denotes the network’s loss function and (cid:15) denotes
the achieved L∞ distance.

Fig. 1: Attack success ratios for both datasets and per attack
type for CIC-IDS-2017. Depicted are ﬂow detection accuracies
for adversarial ﬂows and for unmodiﬁed attack ﬂows.

D. Evaluation

Figure 1 depicts the performance for the investigated algo-
rithms for both datasets and for each attack type in CIC-IDS-
2017. We used CW with a tradeoff of 1 and, to provide a
fair comparison, for FGSM and PGD we used an average L∞
distance as observed for CW.

CW delivers the best performance and FGSM, while being
the fastest of all investigated algorithms, delivers the worst
performance. The ﬁgure shows signiﬁcant differences for
detecting different attack families in the ﬁrst place. Also for
generating adversarial ﬂows some attack families are more
susceptible than others. However, the results match to a high
degree with our expectations. For example, SQL injection
attacks apparently are closer to benign trafﬁc than Denial-of-
Service (DoS) attacks and, hence, ﬁnding adversarial samples
should be easier.

Interestingly, any increase of the L∞ bound for PGD did not
yield signiﬁcantly improved performance. CW thus generally
achieved superior results to L∞-bounded PGD and we can
conﬁrm the recommendation by [10] to perform gradient
descent on the logit output instead of the loss for good results.
Figure 2 depicts the success ratio and average distance
for CW for different tradeoffs κ. Evidently, the larger κ, the
better the attack works, but also distances from original ﬂows
become higher. With acceptable distances, we were able to
generate adversarial samples for about 80% of attack samples.
For successful adversarial samples, the second term in Equa-
tion 1 becomes constant and, hence, is no longer relevant for
optimization. Hence, κ is irrelevant for the achieved distance,
but governs if for one sample an adversarial sample is found.
However, when using a large κ in Equation 1, to avoid overly
large steps that impede convergence, it is necessary to reduce

CIC-IDS-2017UNSW-NB15SQL injectionBrute-force FTPXSS attackPortScan, no Fw.Botnet ARESInfiltrationBrute-force SSHPortScan, FirewallDoS SlowlorisDoS SlowHTTPTestDoS GoldenEyeDoS HulkDDoS LOIT0.00.51.0RecallUnmodifiedCWPGDFGSMthe gradient descent learning rate. Since the distance term is
not affected by κ, it is furthermore necessary to scale the
iteration count inversely with the learning rate. CW with a
large κ therefore needs more time than with a small κ.

IV. EVALUATING ADVERSARIAL ROBUSTNESS

As Figure 1 shows, for most attack types, most samples can
be modiﬁed by an adversary to be classiﬁed as benign by using
CW. High recall for a particular attack type does not imply that
adversarial samples are hard to ﬁnd for these attacks. Thus we
argue that beside the classical metrics such as accuracy, false
positives etc., a metric is needed, which quantiﬁes how easily
an IDS classiﬁer can be fooled.

Such a metric should be easy to compute and easy to
interpret. While in general adversarial robustness for ML
models is frequently quantiﬁed as the average or median of
minimum distances of adversarial samples [12], we found no
generally agreed-upon metric for IDSs in the literature. If we
consider ﬂows, for which no adversarial sample can be found,
to have inﬁnite distance, the median has the advantage of
ignoring such unsuccessful samples and outliers. On the other
hand, the median might depict expected distances badly if they
have a very uneven distribution.

smallest distance of non-adversarial samples is not smaller
than the (cid:100) N
2 (cid:101)th smallest distance of adversarial samples, stop
and compute the ARS. Otherwise increase κ and repeat. If
after a predeﬁned number of iterations – e.g. 100 – still not
more than half of the samples are adversarial, also break and
set the ARS to ∞.

The larger the ARS is, the more robust the model is, as
then an adversary needs to modify the samples more to make
them adversarial. If not more than half of the samples could
be made adversarial, our metric is ∞ since then apparently it
is not possible for an adversary to reliably conduct adversarial
attacks on the majority of samples. In this case, the ratio of
samples that could be made adversarial can be a useful metric
to determine the exact extent of the adversarial threat.

Setting the threshold for the ARS to 50% is arbitrary, but
reasonable as outliers with very large distances are ignored
and because if an attacker can make the majority of samples
adversarial, we argue that its attack is “successful”.

ARS =

We thus propose the ARS as follows. Let S denote the set
of samples, N = |S| ∈ N the total number of samples and
ds ∈ R+
0 the distance of an adversarial ﬂow from the original
ﬂow for a sample s ∈ S, assigning unsuccessful samples a
distance of ∞. We deﬁne the ARS as
(cid:88)

1
(cid:100)N/2(cid:101)
with ˜S ⊂ S denoting a set of size | ˜S| = (cid:100)N/2(cid:101), so that d˜s ≤
ds for all ˜s ∈ ˜S, s ∈ S \ ˜S. The ARS is thus approximately
the average of distances not larger than the median distance.
We consider an adversary successful if he can cause at least
50% of all samples to be misclassiﬁed. In this case, the ARS
is the average of the distances of the adversarial samples to
the original samples for the 50% of all samples which have
the smallest distances. If the adversary doesn’t manage to
manipulate enough samples, the ARS is ∞.

d˜s,

˜s∈ ˜S

(3)

CW is able to ﬁnd adversarial samples with minimum
distance, but becomes slow for large κ. Hence, it can be used
for ﬁnding the ARS efﬁciently as follows: Use CW with a
small tradeoff, trying to generate adversarial samples for an
attack type. If at least N/2 are wrongly classiﬁed, and the

Fig. 2: Performance of CW with different κ values.

Fig. 3: Recall for unmodiﬁed ﬂows and ARS for attacks
in CIC-IDS-2017. For “PortScan, Firewall” recall never falls
below 66%: The attack does not succeed and the ARS is ∞.

Figure 3 shows that certain attack types,

like “DoS
SlowHTTPTest”, are easy to classify for an IDS but still are
also surprisingly vulnerable to adversarial modiﬁcations by an
attacker aiming to make them look benign.

V. EXPLAINING RECURRENT NEURAL NETWORKS

Having veriﬁed the effectiveness of AML for our RNNs, we
now investigate how the classiﬁers come to a decision. From
a naive perspective, one might be tempted to reuse existing
explainability methods for RNNs by considering a ﬂow the
sum of its packet features. We identify several difﬁculties
which occur when trying to explain decisions made by RNNs.
• Feature quantity. The number of features fed into an RNN
is the number of packet features times the length of the ﬂow.
For long ﬂows, the total number of inputs can become very
large.
• Variable sequence lengths. The length of different ﬂows
might differ tremendously. Hence, features at one particular
time step might be important for the network’s outcome for
one ﬂow, but not even exist for other ﬂows.

012345Tradeoff 0.40.60.8Success ratio0510L1 distanceCIC-IDS-2017UNSW-NB15DDoS LOITDoS HulkDoS GoldenEyeDoS SlowHTTPTestDoS SlowlorisBrute-force SSHInfiltrationBotnet ARESPortScan, no Fw.XSS attackBrute-force FTPSQL injection0.000.250.500.751.00Recall020406080ARSTABLE III: Accuracy drop for:

(a) Input perturbation

(b) Feature dropout

Feature

Accuracy drop

Feature

Accuracy drop

Protocol
Packet Length
SYN Flag
ACK Flag
Direction
Destination port
Source port
RST Flag
PSH Flag
Interarrival time
FIN Flag
URG Flag
ECE Flag
CWR Flag
NS Flag

0.207
0.165
0.099
0.084
0.073
0.071
0.060
0.057
0.056
0.024
0.012
0
0
0
0

Destination port
Source port
RST Flag
ACK Flag
Protocol
Packet Length
Direction
SYN Flag
Interarrival time
FIN Flag
ECE Flag
URG Flag
CWR Flag
NS Flag
PSH Flag

0.025
0.003
0.001
0.001
0.001
0.001
0.001
0
0
0
0
0
0
0
0

• Lack of a distance measure. However, even if we restricted
the analysis to ﬂows of a constant length, a ﬂow is different
from the plain concatenation of its packet features. For exam-
ple, in a sentence, which is sequence of words, parts can be
rearranged, giving a different sequence with possibly the exact
same meaning.
• Multiple prediction outputs. Often an RNN produces
an output at each time step. When applying explainability
methods, the question arises which output to consider for the
method. The natural choice is to base the methods on the
prediction output which occurs at the same time step as the
feature under investigation: This approach is less complex
compared to considering also features of all earlier time
the current prediction outcome to
steps. Also, we expect
more dependent on the current feature, compared to a feature
from many steps ago. However, due to the complex decision
processes of deep neural networks, this is not always true and
a feature might inﬂuence a decision many time steps later.

Many explainability methods proposed recently are local
and thus provide explanations for a particular data sample
[13, 14, 15, 16]. However, for the particular problem of
network trafﬁc, due to the high number of ﬂows and the
characteristics of data, analyzing individual samples is of
low interest. Explainability methods presented in this paper
therefore aim to understand a model by analyzing which
features are important, at which time step they are important
and which feature values lead to classiﬁcation as attack.

A. Feature Importance Metrics

As a ﬁrst step to understanding the neural network’s deci-
sions, we estimate how important individual features are for
the model’s predictions. When investigating an ML classiﬁer,
it is natural to ask which inputs have a large inﬂuence on the
classiﬁer’s prediction. We feel the need to distinguish metrics
for this purpose based on their main aim:

A large amount of research has been spent on ﬁnding
feature importance metrics, which allow the selection of high-

importance features, providing a reasonably good classiﬁcation
performance while resulting in a more light-weight classiﬁer.
Conversely, both adversarial machine learning and explain-
able machine learning bring up the question to what extent
individual features are able to change the prediction outcome.
While appearing similar, traditional feature importance can
yield markedly wrong results for this case. To distinguish,
we propose the term feature sensitivity for such metrics. To
analyze features, we use the following approaches:

1) Neural Network Weights: In previous works [17, 18], a
simple method for determining feature importance in neural
networks has been summing up neural network weights lead-
ing from a certain input to the prediction outcome. The weights
method can be considered for both feature importance and
feature sensitivity, however, clearly, especially in the case of
complex network architectures, this method is likely to provide
wrong results. Hence, we provide results for the weights
method mainly for comparison. Note that an LSTM cell alone
has four weights leading from one input to an output.

2) Input Perturbation: The most commonly deployed fea-
ture importance techniques, used by practitioners for RNNs
[19] and Deep Learning (DL) [20, 21, 17], are based on adding
noise to a particular feature and evaluating the drop in accuracy
that occurs. We argue that it is hard to determine the “correct”
intensity of noise to add. Hence, we sample the value for a
feature from the distribution of all values of this feature in
the dataset. This makes the method non-parametric since the
noise distribution does not need to be chosen. We ensured
that features which stay constant for a ﬂow, i.e. source port,
destination port and protocol, also stay constant throughout
the ﬂow when randomizing the feature.

3) Feature Dropout: While the perturbation method is
convenient since it is easy to implement and understand, we
argue that it has some shortcomings: The RNN was never
trained for dealing with “garbage” values that the random-
ization creates. For example, completely unrealistic feature
combinations could occur that were never observed during
training. Furthermore, sequences of features could occur that
cannot occur in reality.

To evaluate true feature importance, we thus develop a more
sound method called feature dropout: When training a model,
for each sample, we leave out each feature with independent
n , n ∈ N being the number of features, by setting
probability 1
it to zero. On average, one feature gets zeroed out but it is
also possible that none or more than one are left out. This
procedure is equivalent to using dropout [22] with probability
1
n before the ﬁrst layer.

An important implementation detail is that for each feature
we add another input which is 1 if the feature is suppressed
and 0 otherwise. This is necessary for the neural network to
be able to distinguish between a feature missing and a feature
genuinely being zero. The overall outcome is a classiﬁer being
able to deal with missing features. The results in Table III show
that, unlike input perturbation, feature dropout does not vastly
overestimate features’ importance. With feature dropout, it
becomes apparent that only very few features actually contain

unique information, affecting accuracy when left out:
destination port and the source port.

the

the right-side case the prediction deterministically depends on
the feature value.

A model

trained with feature dropout

typically yields
slightly lower accuracy than a regularly trained model, even if
no features are left out (ﬂow accuracy of 99.43% vs. 99.65%).
We thus recommend training two models: One regular one and
one with feature dropout to use for the feature importance.

Another method that uses dropout for feature importance
is [23], applying a technique called Variational Dropout to
learn the optimal dropout rate for each feature. It tries to leave
out as many features as possible and at the same time keep
accuracy high. Thus important features are going to be left out
less often and one can then extract the dropout probabilities
for each feature and assess their signiﬁcance based on them.
While this method looks seemingly related to feature dropout,
it is signiﬁcantly more complex and does not aim to show the
accuracy drop that occurs when omitting a feature but instead
returns a unique feature importance value between 0 and 1.

For feature dropout, it can also happen that several features
are left out for a sample and so our method can also be
used to analyze the effect of multiple features missing and
can thus show possibly correlated features or – more general
– features that contain common information, relevant for the
classiﬁcation task:

score =

accbase − acc-both features
(accbase − acc-feature 1) + (accbase − acc-feature 2)

(4)

With accbase we denote the accuracy of the classiﬁer with all
features included, with acc-feature i the accuracy if feature i is
omitted and with acc-both features the accuracy if both features
are omitted. Assuming a non-negative accuracy drop when
omitting a feature, the resulting score is ≥ 0.5. The higher it
is, the larger the information that both features share.

For example,

the score between RST and the protocol
identiﬁer is 8.5; the highest of all pairs of features. While
this may be unintuitive at the ﬁrst glance, it likely stems from
the fact that if the protocol identiﬁer (TCP or UDP) is missing,
then RST being 1 at some point indicates that the ﬂow is TCP.
Feature dropout might constitute a building block for meth-

ods based on Shapley values [13] like KernelSHAP [14].

4) Mutual

Information:

Input perturbation and feature
dropout mainly address feature importance. For example,
assuming that the test dataset is representative for production
use, for feature importance it is reasonable to equate the dis-
tribution for perturbing a feature with the feature distribution
itself. However, when evaluating feature sensitivity, e.g. for
analyzing potential for adversarial samples, the attacker is not
limited by this distribution and frequently is able to choose
arbitrary values in the feature space.

Furthermore, we argue that accuracy drop depicts an im-
portance measure which might be misleading for evaluat-
ing feature sensitivity. To see this, let f (A, x) denote the
joint probability for classiﬁcation as attack and a feature
value x. Accuracy then is (cid:82)
R f (A, x)dx for attacks and 1 −
(cid:82)
R f (A, x)dx for benign trafﬁc. Figure 5 shows two different
distributions yielding the same accuracy, but clearly the right-
side distribution has a larger inﬂuence on the prediction, as in

To capture such dependencies, we propose to use mutual in-
formation to determine feature sensitivity. Mutual Information
between two random variables X, Y is deﬁned as
(cid:26)

(cid:19)(cid:27)

IX,Y = E

log

(cid:18) fX,Y (x, y)
fX (x)fY (y)

,

(5)

with fX (x), fY (y) and fX,Y (x, y) denoting the distribution
of X, Y and their joint distribution, respectively. To obtain
feature sensitivity, we compute mutual information between
an input variable and the prediction output for one ﬂow at one
particular time step, averaging over the result for the test set.
5) Comparison: Figure 4 shows the results, which match to
a large extent with domain-speciﬁc expectations for classifying
network ﬂows. In particular, rarely used TCP ﬂags like NS or
URG are unimportant for the classiﬁer. On the other hand,
destination port and protocol are essential for characterizing
ﬂows by hinting at the type of trafﬁc. IAT and packet length are
important for estimating the amount of transferred information
and several ﬂags hint at certain attacks like DoS attacks.

The weights method is able to reveal features with a very
low importance to a certain degree, but disagrees with the other
methods to a large extent. Less anticipated, however, also input
perturbation does not exhibit a considerable correlation with
feature dropout. Considering its functioning of completely re-
moving individual features, feature dropout is the most reliable
method for evaluating importance for removing features. It is
remarkable that none of the other methods is able to depict
the distinct peak of importance for the destination port visible
for feature dropout in Figure 4 and Table III.

It is not surprising that mutual information disagrees with
feature dropout, since both their aim and their functioning
are substantially different. For example, mutual information
shows that the protocol ﬁeld can have a substantial impact
on the classiﬁcation even though an identical accuracy can be
achieved when omitting it.

Metrics for UNSW-NB15 differ substantially from CIC-
IDS-2017. However, due to the large number of different
network attacks and network conﬁgurations, it is easily pos-
sible that relevant features are very different. We consider the
question of model transferability of substantial importance for
IDS applications, but out of scope for the present research.

B. Explainability Plots

Knowing which features to investigate, it is important to
analyze which feature values lead to classiﬁcation as attack. In
literature, the use of Partial Dependency Plots (PDP) has been
proposed [4]. To inspect attack types in detail, in this research
we use a conditional form of the PDP. If X ∈ Rn denotes a
random vector drawn from feature space, f (X) ∈ [0, 1] the
neural network’s prediction and c the attack type, we deﬁne
the conditional PDP for feature i as

PDPc,i(w) = EX|C

(cid:16)

f (X1, . . . , Xi−1, w, Xi+1, . . . Xn) |c

(cid:17)
,
(6)

Fig. 4: Feature importance metrics for the ﬂow prediction for CIC-IDS-2017 (left side) and UNSW-NB15 (right side).

empirically approximating the conditional distribution by the
observed subset that belongs to a certain attack type.

By using a classical PDP we would likely lose information
due to the averaging over very different
types of trafﬁc.
However, for network trafﬁc in particular, investigating each
sample individually is not possible. Hence, the conditional
PDP provides the ideal compromise for our application.

Due to the use of a 5-tuple ﬂow key, port numbers and
the protocol
identiﬁer are constant for all packets in one
ﬂow. Hence, we can consider the RNN a regular classiﬁer
and reuse PDPs, which have been proposed for non-recurrent
classiﬁcation methods, by plotting the RNN’s ﬂow prediction
outcome over one of these features. The results show that
for some attack types the port numbers play an important
role. When looking at the PDP for benign trafﬁc samples in
Figure 6, it becomes apparent that trafﬁc destined to a high
destination port is generally indicative of an attack. We argue
that this is because most services that regular users use have
low port numbers.

C. Plots for Sequences

Intuitively, features at the beginning of a ﬂow should be
the most important while the classiﬁer’s predictions should
not vary signiﬁcantly anymore, as soon as it has come to a
decision.

To evaluate this hypothesis, Figure 7 depicts the classiﬁer’s
prediction conﬁdence for each time step, along with the
number of samples having at least this length, which were used
for evaluating the ﬁgure. While at the ﬁrst couple of packets
the conﬁdence is not very high, towards the end of the ﬂow
it reaches values close to 1 and stays there. Hence, not only
is the classiﬁer able to make a reasonable classiﬁcation after
just a few packets, the ﬁgure also suggests that indeed later
packets have a negligible inﬂuence on the prediction.

Fig. 5: Two distributions yielding an identical accuracy drop.

Fig. 6: PD plot for the source and destination port features.

Fig. 7: Classiﬁer conﬁdence per time step for CIC-IDS-2017.
For the majority of attack types, conﬁdence increases in the
ﬁrst few steps and then stays almost constant at 1.

For investigating in more detail how features inﬂuence the
prediction outcome, we extend PDPs to the sequential setting.
Denoting as X = {X 1, . . . , X m} the series of packet features
X t ∈ Rn and ht(X) the network’s hidden state after time step
t, we deﬁne the sequential PDP as

seqPDPc,i(t, w) =
(cid:16)

EX|C

f (cid:0)ht−1(X), X t

1, . . . , X t

i−1, w, X t

i+1, . . . X t
n

(7)
(cid:17)
(cid:1) |c

.

Figure 8 shows an example together with the mean values
for both unmodiﬁed samples and adversarial samples. Also in
this ﬁgure we notice that mainly the ﬁrst few packets are able
to inﬂuence the prediction outcome and modifying features at
a later time step does not change its conﬁdence any longer. In
many cases, the adversarial sample generation indeed moves
packet features to areas where the network is less likely to
be classiﬁed as attacks, conﬁrming the effectiveness of PDPs.
In other cases, however, we did not observe an agreement

Dest. portIATProtocolPkt. lengthSource portRST FlagSYN FlagACK FlagDirectionPSH FlagFIN FlagECE FlagCWR FlagURG FlagNS Flag0.00.20.40.60.8Normalized metricWeights methodInput perturbationFeature dropoutMutual informationMeanPkt. lengthACK FlagIATPSH FlagSYN FlagProtocolDest. portSource portFIN FlagDirectionRST FlagURG FlagECE FlagCWR FlagNS Flag0.00.10.20.30.4Normalized metricWeights methodInput perturbationFeature dropoutMutual informationMean0.00.51.0x0.00.51.0f(A,x)0.00.51.0x0.00.51.0f(A,x)0100002000030000400005000060000Port number100101102103Flow number0.00.20.40.6Partial dependenceDestination portSource port051015Time step t0500010000Flow numberMedian1st and 3rd quartile10th and 90th percentile0.980.991.00ConfidenceFig. 8: Exemplary sequential PD plot and adversarial ﬂows for
the DoS Slowloris attack in CIC-IDS-2017. The lines show the
feature’s mean values. The shaded region shows the change in
conﬁdence that occurs when the feature is varied.

between PDP and adversarial modiﬁcations, hinting at de-
pendencies which cannot be presented by PDPs. Since the
IAT is undeﬁned for the ﬁrst packet, we always set it to 0.
Still, interestingly, the plot shows, that the classiﬁer considers
packets with a higher IAT to be more likely to be attacks than
those with a smaller one.

Finally, we investigated whether our classiﬁcation task
involves recognizing complex patterns in the feature space.
As example, Figure 9 shows that attack types indeed have a
characteristic pattern in which they send packets by which they
are easily recognizable. Other attack families similarly show
characteristic patterns.

VI. DEFENSES

We now investigate two approaches to increase robustness
of the RNNs against adversarial attacks. Several methods
have been proposed to improve robustness in the context of
computer vision [24]. We chose the methods presented below,
because they are simple and can be applied to our recurrent
setting in a straight-forward fashion.

A. Reducing Features

The most obvious defense is to simply omit features an
attacker can manipulate. We try two different approaches of
this defense strategy:

• Leaving out all manipulable features, i.e. packet size and

IAT, in both directions.

• Leaving them out only in the direction from the attacker
to the victim. This, however, does not prevent adversarial
samples for botnets, for which the attacker has control over
both sides.
Both approaches lead to complete resistance to adversarial
samples, except for botnets, which can still operate when only
leaving out manipulable features in one direction. The results
show that – surprisingly – there is only a small difference
in classiﬁcation performance when looking at ﬂow accuracy:
It is slightly lower at 99.3% compared to 99.7% originally
for CIC-IDS-2017. However, packet accuracy is only 98%

Fig. 9: IAT and packet length for SSH brute-force attacks in
CIC-IDS-2017.

when leaving out the features in one direction and 96.7%
when leaving them out in both directions. Thus, apparently the
IAT and packet size are especially important for determining
whether a ﬂow is malicious in the ﬁrst packets of a ﬂow.

B. Adversarial Training

As alternative to omitting manipulable features, we at-
tempted to make the classiﬁer more robust against adversarial
samples by augmenting the training set by adversarial ﬂows
generated using CW, labeled as additional attacks. This ap-
proach can be considered an adversarial training procedure,
which is a common defense method in the related litera-
ture [24, 11]. We added one adversarial sample per attack
sample to the training set. Since CW is deterministic, this is
the maximum number of adversarial samples possible. With
this augmented training set we then alternated retraining of the
neural network and optimization of the adversarial samples.

A question which occurs in this process, is how many CPU
cycles to spend on network training and adversarial sample op-
timization. We decided to use as many backpropagation steps
for training as we use for adversarial sample optimization. For
each adversarial sample optimization step, we performed 10
iterations of gradient descent. Hence, all adversarial samples
were optimized each 10 epochs of neural network training.

Figure 10 shows the ARS throughout adversarial training for
several attack categories and clearly indicates that adversarial
training is effective, as the distance gradually increases. Hence,
an attacker would have to modify attack samples more and
more, eventually rendering the attack unpractical.

For both datasets, after a small number of epochs, it was no
longer possible to create a signiﬁcant number of adversarial
samples for most attack categories. After 50 epochs of training,
accuracy is essentially identical to the results presented in

Fig. 10: ARS during adversarial training for UNSW-NB15.

102103104Pkt. length (B)05101520Time step t106103100IAT (s)Original flowsAdversarial flows0102030405060Time step t5001000Pkt. length (B)010002000IAT (ms)Median1st and 3rd quartile0.20.40.60.8Training duration in epochs0255075ARSDoSExploitsFuzzersGenericReconnaissance[12] N. Carlini, A. Athalye, N. Papernot, W. Brendel, J. Rauber,
D. Tsipras, I. Goodfellow, A. Madry, and A. Kurakin, “On
Evaluating Adversarial Robustness,” arXiv:1902.06705, Feb.
2019.

[13] L. S. Shapley, “A value for n-person games,” Contributions to
the Theory of Games, vol. 2, no. 28, pp. 307–317, 1953.
[14] S. M. Lundberg and S.-I. Lee, “A Uniﬁed Approach to Inter-
preting Model Predictions,” in Advances in Neural Information
Processing Systems 30, pp. 4765–4774, Curran Associates, Inc.,
2017.

[15] A. Dhurandhar, P.-Y. Chen, K. Shanmugam, T. Pedapati, A. Bal-
akrishnan, and R. Puri, “Model Agnostic Contrastive Explana-
tions for Machine Learning Classiﬁcation Models,” 2018.
[16] M. T. Ribeiro, S. Singh, and C. Guestrin, “”Why Should I Trust
You?”: Explaining the Predictions of Any Classiﬁer,” in KDD,
(San Francisco, California, USA), pp. 1135–1144, ACM, 2016.
[17] J. D. Olden, M. K. Joy, and R. G. Death, “An accurate
comparison of methods for quantifying variable importance
in artiﬁcial neural networks using simulated data,” Ecological
Modelling, vol. 178, pp. 389–397, Nov. 2004.

[18] J. D. Olden and D. A. Jackson, “Illuminating the “black box”:
a randomization approach for understanding variable contri-
butions in artiﬁcial neural networks,” Ecological Modelling,
vol. 154, pp. 135–150, Aug. 2002.

[19] StackExchange Cross Validated,
importance

Variable
in RNN or LSTM,” 2019.
tps://stats.stackexchange.com/questions/191855/variable-
importance-in-rnn-or-lstm.

“neural

networks

-
ht

[20] C. Molnar, Interpretable Machine Learning: A Guide for Mak-

ing Black Box Models Explainable. 2019.

[21] StackExchange

Cross

Validated,

selection

ture
https://stats.stackexchange.com/questions/250381/feature-
selection-using-deep-learning.

learning?,”

using

deep

“Fea-
2016.

[22] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and
R. Salakhutdinov, “Dropout: A Simple Way to Prevent Neu-
ral Networks from Overﬁtting,” Journal of Machine Learning
Research, vol. 15, pp. 1929–1958, 2014.

[23] C.-H. Chang, L. Rampasek, and A. Goldenberg, “Dropout
feature ranking for deep learning models,” arXiv:1712.08645,
2017.

[24] N. Akhtar and A. Mian, “Threat of Adversarial Attacks on Deep
Learning in Computer Vision: A Survey,” IEEE Access, vol. 6,
pp. 14410–14430, 2018.

Table II. Recall and informedness increased and precision
slightly decreased. However, this is due to the higher propor-
tion of attack samples in training data and, hence, expected.
We conclude that adversarial training is effective for reducing
the attack surface for adversarial attacks in our scenario.

VII. CONCLUSION

We have implemented a recurrent classiﬁer based on LSTMs
to detect network attacks, which is able to detect attacks
already before they are over. The recurrent approach allowed
us to inspect the inﬂuence of single packets on the detection
performance and shows which packets are characteristic for
attacks. Even though the interpretation of RNNs poses several
difﬁculties, we have demonstrated methods for gaining insights
into the model’s functioning.

We showed that even though our use case is very different
from computer vision, adversarial samples can be found ef-
ﬁciently, even if only ostensibly unimportant features can be
modiﬁed. We introduced feature sensitivity methods to show
which features can easily be manipulated by an adversary
to cause a wrong classiﬁcation. We proposed the ARS for
quantifying and comparing the adversarial threat for IDSs.
Deploying an adversarial training procedure, we could sig-
niﬁcantly reduce the adversarial threat.

ACKNOWLEDGEMENTS

The Titan Xp used for this research was donated by the

NVIDIA Corporation.

REFERENCES
[1] F. Meghdouri, T. Zseby, and F.

Iglesias, “Analysis of
Lightweight Feature Vectors for Attack Detection in Network
Trafﬁc,” Applied Sciences, vol. 8, p. 2196, Nov. 2018.

[2] M. Bachl, A. Hartl, J. Fabini, and T. Zseby, “Walling Up
Backdoors in Intrusion Detection Systems,” in Big-DAMA ’19,
(Orlando, FL, USA), pp. 8–13, ACM, 2019.

[3] M. J. Hashemi, G. Cusack, and E. Keller, “Towards Evaluation
of NIDSs in Adversarial Setting,” in Big-DAMA ’19, (Orlando,
FL, USA), pp. 14–21, ACM, 2019.

[4] J. H. Friedman, “Greedy Function Approximation: A Gradient
Boosting Machine,” The Annals of Statistics, vol. 29, no. 5,
pp. 1189–1232, 2001.

[5] I. Sharafaldin, A. Habibi Lashkari, and A. A. Ghorbani, “To-
ward Generating a New Intrusion Detection Dataset and Intru-
sion Trafﬁc Characterization,” in ICISSP, (Funchal, Madeira,
Portugal), pp. 108–116, SCITEPRESS, 2018.

[6] N. Moustafa and J. Slay, “UNSW-NB15: a comprehensive
data set for network intrusion detection systems (UNSW-NB15
network data set),” in MilCIS, pp. 1–6, Nov. 2015.

[7] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan,
I. Goodfellow, and R. Fergus, “Intriguing properties of neural
networks,” in ICLR, 2014.

[8] I. Goodfellow, J. Shlens, and C. Szegedy, “Explaining and

Harnessing Adversarial Examples,” in ICLR, 2015.

[9] N. Papernot, P. McDaniel, A. Swami, and R. Harang, “Crafting
Adversarial Input Sequences for Recurrent Neural Networks,”
arXiv:1604.08275, Apr. 2016.

[10] N. Carlini and D. Wagner, “Towards Evaluating the Robustness

of Neural Networks,” in S&P, pp. 39–57, IEEE, May 2017.

[11] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu,
“Towards Deep Learning Models Resistant to Adversarial At-
tacks,” ICLR, 2018.

