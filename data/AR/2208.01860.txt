2
2
0
2

g
u
A
3

]

Y
S
.
s
s
e
e
[

1
v
0
6
8
1
0
.
8
0
2
2
:
v
i
X
r
a

Joint Optimization of DNN Inference Delay and
Energy under Accuracy Constraints for AR
Applications

Guangjin Pan†, Heng Zhang†, Shugong Xu†, Shunqing Zhang†, and Xiaojing Chen†
† Key laboratory of Specialty Fiber Optics and Optical Access Networks,
Shanghai University, Shanghai, 200444, China
Email: {guangjin_pan, hengzhang, shugong, shunqing, jodiechen}@shu.edu.cn

Abstract—The high computational complexity and high energy
consumption of artiﬁcial intelligence (AI) algorithms hinder their
application in augmented reality (AR) systems. This paper con-
siders the scene of completing video-based AI inference tasks in
the mobile edge computing (MEC) system. We use multiply-and-
accumulate operations (MACs) for problem analysis and optimize
delay and energy consumption under accuracy constraints. To
solve this problem, we ﬁrst assume that ofﬂoading policy is
known and decouple the problem into two subproblems. After
solving these two subproblems, we propose an iterative-based
scheduling algorithm to obtain the optimal ofﬂoading policy. We
also experimentally discuss the relationship between delay, energy
consumption, and inference accuracy.

Index Terms—Mobile augmented reality, edge intelligence,

mobile edge computing, resource allocation.

I. INTRODUCTION

The metaverse requires people to interact between the
real world and the virtual world. Augmented reality (AR)
is an essential technology for this to happen. With artiﬁcial
intelligence (AI) technology, AR can carry out more profound
scene understanding and more immersive interactions.

AI has played an essential role in many ﬁelds, such as
automatic speech recognition (ASR), natural language process-
ing (NLP), computer vision (CV), and so on. However, the
computational complexity of AI algorithms, especially deep
neural networks (DNN), is usually very high. It is challenging
to complete DNN inference timely and reliably on mobile
devices with limited computation and energy capacity. In [1],
experiments show that a typical single-frame image processing
AI inference task takes about 600 ms even with speedup from
the mobile GPU. In addition, continuously executing the above
inference tasks can only last up to 2.5 hours on commodity
devices. The above issues result in only a few AR applications
currently using deep learning. To reduce the inference time of
DNNs, one way is to perform network pruning on the neural
network [2]–[4]. However, it could be destructive to the model
if pruning too many channels, and it may not be possible to
recover a satisfactory accuracy by ﬁne-tuning [2].

Edge AI [5]–[7] is another approach to solving these prob-
lems. The integration of mobile edge computing (MEC) and
AI technology has recently become a promising paradigm for
supporting computationally intensive tasks. The edge DNN

inference tasks need to consider three Key Performance Indi-
cators (KPIs), i.e., delay, energy consumption, and accuracy.
[8] and [9] optimize inference task selection to minimize the
energy consumption in wireless networks. To optimize the
inference delay, [10] uses a tandem queueing model to analyze
the queueing and processing delays of DL tasks in multiple
DNN partitions. At the same time, [11] and [12] optimize the
accuracy of inference tasks under the constraints of delay and
energy consumption. What’s more, [13] joint optimizes the
service placement, computational and radio resource allocation
to minimize the users’ total delay and energy consumption.

In this paper, we consider a multi-user MEC system and
assume that each device executes the video-based DNN infer-
ence task. Each device can be AR glasses, mobile robots, and
so on. Different from the ofﬂoading problem of still-based
(image-based or single-frame-video-based) DNN inference
tasks in existing studies [10]–[12], we focus on the video-
based DNN inference tasks, which have higher computational
complexity and require more computing energy. We model the
problem as a multi-objective optimization problem to optimize
delay and energy consumption with the constraint of inference
accuracy. The main contributions of this paper are summarized
as follows,

• Multi-dimensional target optimization. We formulate the
video-based ofﬂoading problem as a mixed-integer non-
linear programming problem (MINLP). Unlike existing
work [13], we optimize latency and energy consumption
under the constraints of DNN inference accuracy. To
the best of our knowledge, this is the ﬁrst work that
considers the relationship between delay, energy, and
accuracy at the same time. At the same time, we also
explore the trade-off relationship between latency, energy
consumption and inference accuracy.

• MAC-based analysis model. We explored the relationship
between the number of input video frames and delay,
energy consumption and inference accuracy. We also use
multiply-and-accumulate operations (MACs) for reﬁned
modeling for the delay and energy consumption. To our
knowledge, this is the ﬁrst work using MACs to analyze
the AI inference ofﬂoading problem.

 
 
 
 
 
 
impact of the number of input video frames Mn on recognition
accuracy and the allocation of communication and computing
resources. We assume that both dn and Θn are same for
different devices. Therefore, we simplify the expression of the
computational complexity to C(Mn). C(Mn) also means the
number of MACs that DNN inference needs to complete, when
the number of input video frames is Mn.

Then we give the expression for the inference delay and
energy consumption. The computation delay of the device n
and MEC can be respectively expressed as,

Dmd

n =

De

n =

,

ρC(Mn)
f md
n
ρC(Mn)
f e
n

,

(1)

(2)

n

where ρ (cycle/MAC) represents the number of CPU cycles
required to complete a multiplication and addition, which
depends on the CPU model. f e
(in CPU cycle/s)
represent the computing resources allocated by the edge server
and the device, respectively. Denote f e,max and f md,max
to be the total computation resource of the edge server and
the mobile device n, respectively. Therefore, the computing
resources satisfy (cid:80)

n ≤ f e,max and f md

n ≤ f md,max.

n and f md

n∈N f e

As for energy consumption, denote κ to be a coefﬁcient de-
termined by the corresponding device [11]. The computational
energy consumption of device n can be expressed as,

n

Emd

n = κρC(Mn)f md

n

2

.

(3)

B. Delay and Energy Models for Transmission

We consider a time-division multiple access (TDMA)
method for channel access. Denote Rn to be the achievable
data rate of device n. The delay and energy consumption of
transmission can be written as,

Dt

n =

Mndn
Rntn

,

(4)

Et

Mndn
Rn
where tn is the proportion of time that device n transmits, and
pn is the transmission power.

n =

pn.

(5)

C. Inference Tasks Accuracy Model

As mentioned above, we mainly focus on the impact of the
number of input video frames Mn on recognition accuracy.
We assume that the quality of the input video is the same
for different devices. For a certain task and DNN model, the
accuracy is only determined by the number of input frames.
According to [14], more frames will lead to better inference
accuracy, and as the input frames continue to increase, the
performance gain will gradually decrease. Take gesture and
action recognition as examples. Fig. 2 shows the relationship
between the inference accuracy and the number of input video
frames. Therefore, we deﬁne Φ(Mn) as a monotone non-
decreasing function to describe the relationship between the
accuracy and the number of input frames.

Fig. 1. The overview of the video sampling and computing ofﬂoading
system. The video sampling management module can control the sampling
rate of the captured video and determine the number of video frames used
the video to the edge server or
for AI inference. Devices can transmit
perform inference tasks locally based on the wireless channel information
and computing capabilities.

• Iteration-based scheduling scheme. To solve the opti-
mization problem, we decompose the original problem.
First, assuming that the ofﬂoading decision is given, we
respectively solve optimization problems of the device set
that completes the inference locally and the device set that
is ofﬂoaded to the edge server. Then, to obtain the optimal
ofﬂoading policy, we propose a iterative-based algorithm
and solve the original problem through iteration.

The rest of this paper is organized as follows. In Section
II, we introduce system models,
including delay, energy,
and accuracy models. In Section III, we formulate the joint
optimization problem and convert
the original problem to
make it more tractable. Section IV gives the solution algorithm
of the proposed problem. Numerical results and analysis are
presented in Section V. Finally, the paper is concluded in
Section VI.

II. SYSTEM MODEL

We consider a multi-user MEC system with one base
station (BS) and N mobile devices, denoted by the set
N = {1, 2, . . . N }. Each device has a camera and needs to
accomplish DNN inference tasks. Due to the limitation of
device computational resources, DNN inference tasks can be
placed on local or edge servers. The limited computational
resource will lead to longer computing delay and greater power
consumption when the inference task is executed locally.
However, when the inference task is executed on the edge
server, it will bring additional wireless transmission delay.

A. Delay and Energy Models for Inference

The inference delay depends on the DNN model’s architec-
ture, the input size of the DNN model, and the device’s or
server’s computing power. The computational complexity of
the nth device’s task can be expressed as C(Mn, dn, Θn),
where Mn is the number of input video frames, dn is the data
size of one video frame, and Θn represents the architecture and
parameters of the DNN. In this paper, we mainly focus on the

limit for the input video, Z is the set of integers, and M max
is
the maximum number of frames of the input video. (8c) and
(8d) represent the communication and computation resource
limitation, respectively. (8f) limits the computation resource
of each device.

n

Problem P1 is a non-convex MINLP problem and is difﬁcult
to be solved. First, C(Mn) is an discrete function. As the
number of input frames Mn increases,
the computational
complexity also increases. This kind of increase is irregular
because it is affected by the structure of DNN layers, such
as the stride and padding size of 3DCNN. Therefore, C(Mn)
cannot be used for optimization directly. Second, both Mn and
xn are integers, making the problem difﬁcult to be solved.

Fig. 2. The experimental and ﬁtted curves of gesture recognition task and
action recognition task.

B. Problem Conversion

III. PROBLEM FORMULATION

In this section, we formulate the optimization problem to
reduce the system’s delay and devices’ energy consumption
under the constraint of recognition accuracy. We analyze the
difﬁculty of solving the problem. To simplify the problem, we
make a reasonable conversion of the problem.

A. Original Problem Formulation

Based on the above analysis, the nth device’s delay and

energy consumption can be expressed as,

Dn = (1 − xn)

ρC(Mn)
f md
n

+ xn(

ρC(Mn)
f e
n

+

Mndn
Rntn

),

(6)

En = (1 − xn)κρC(Mn)f md

n

2

+ xn(

Mndn
Rn

pn).

(7)

where xn indicates whether the inference task is executed on
local or edge servers.

Given the system model described previously, our goal is
to reduce end-to-end delay and energy consumption under
the constraint of recognition accuracy. Each device follows
the binary ofﬂoading policy. The mathematical optimization
problem of the total cost can be expressed as,

Problem P1 (Original Problem):

minimize
n ,f e

{Mn,tn,f md

n,xn}

(cid:18)

(cid:19)

β1Dn + β2En

,

(cid:88)

n∈N

subject to Φ(Mn) ≥ αn, ∀n ∈ N ,
, Mn ∈ Z,
Mn ≤ M max
(cid:88)

n
xntn ≤ 1,

n∈N
(cid:88)

xnf e

n ≤ f e,max,

n∈N
tn, f e
n ≥ 0, ∀n ∈ N ,
0 ≤ f md
n ≤ f md,max
n
xn ∈ {0, 1} , ∀n ∈ N ,

, ∀n ∈ N ,

(8)

(8a)

(8b)

(8c)

(8d)

(8e)

(8f)

(8g)

To make the problem P1 more tractable, we convert the
problem. First, we give an approximate expression of the
computational complexity function C(Mn), and assume it is
a continuous function.

Second, considering that Φ(Mn) is a monotone non-
decreasing function. In order not to lose generality, deﬁne
n = arg minMn Φ(Mn), Φ(Mn) ≥ αn, Mn ∈ Z. M ∗
M ∗
n is
the minimum number of input frames under the requirement
of accuracy.

We deﬁne two sets of devices, i.e. N0 = {n | xn = 0, n ∈
N } and N1 = {n | xn = 1, n ∈ N }. F0,n and F1,n are the
cost function of the device n in sets N0 and N1, respectively.
As mentioned above, the problem P1 can be rewritten as,

Problem P2 (Converted Problem):

minimize
n ,f e

{tn,f md

n,xn}

(cid:88)

(1 − xn)F0,n(M ∗

n, f md
n )

n∈N0

(cid:88)

+

n∈N1

xnF1,n(M ∗

n, f e

n, tn),

(9)

subject to (8c) − (8g),

where

F0,n(M ∗

n, f md

n ) = β1

F1,n(M ∗

n, f e

n, tn) = β1

ρC(M ∗
n)
f md
n
ρC(M ∗
n)
f e
n

+ β2κρC(M ∗

+ β1

M ∗
ndn
Rntn

n)f md2
n
M ∗

+ β2

, (10)

ndnpn
Rn

.

(11)

IV. OPTIMIZATION PROBLEM SOLVING
In this section, we decompose the problem P2 and propose
a iterative-based greedy scheme to solve it. First, supposing
that the ofﬂoading decision (i.e. {xn}) is given, we solve opti-
mization problems of sets N0 and N1, respectively. Then, we
propose a iterative-based algorithm to optimize the ofﬂoading
decision {xn}.

A. Optimization Problem Solving for N0

For set N0, i.e., when the device executes inference tasks

locally, the optimization problem becomes,

where αn represents the recognition accuracy requirement,
β1, β2 are the weight factors. (8a) represents the recognition
accuracy requirement of each device. (8b) indicates the frame

Problem PN0 (Problem for N0):
(cid:44) (cid:88)
n∈N0

minimize
{f md
n }

FPN0

F0,n(M ∗

n, f md

n ),

(12)

0481216The number of input video frames0.20.40.60.81AccuracyFitted curve, gestureFitted curve, actionExperiment, gestureExperiment, actionTheorem 2: The function expressions of f e∗

n and t∗

n relative

subject to

(8f).

The optimization variables in PN0 is the local computation
n . We can derive the optimal solution to PN0 in

resource f md
a closed-form expression.

Theorem 1: The optimal solution to PN0 is given by,
(cid:115)
f md∗
n = min{ 3

), f md,max

}

(

n

β1
2β2κ

(13)

to M ∗

n are given by,

f e∗
n =

t∗
n =

f e,max(cid:112)C(M ∗
n)
(cid:112)C(M ∗
i )

(cid:80)
i∈N1

(cid:113) M ∗
n
Rn
(cid:113) M ∗
i
Ri

.

(cid:80)
i∈N1

,

(19)

(20)

Proof: Please refer to Appendix A.
The partial derivative of FPN0

with respect to f md

n

is,

Proof: Please refer to Appendix B.
According to the KKT conditions, we can obtain the fol-

∂FPN0
∂f md
n

= −β1

ρC(M ∗
n)
f md2
n

+ 2β2κρC(M ∗

n)f md
n ,

(14)

By setting

∂FPN0
∂f md
n

= 0, we have,

(cid:115)
f md
n = 3
(

β1
2β2κ

).

(15)

Considering the value range of f md
be given by,

n , the optimal solution can

(cid:115)
f md∗
n = min{ 3

(

β1
2β2κ

), f md,max

n

}

(16)

which completes the proof.

From Theorem 1, we can see that the optimal local CPU-
cycle frequency f md
is determined by the weight factors β1,
β2, the coefﬁcient of CPU energy consumption κ, and is
limited by its corresponding upper bound f md,max

n

.

n

B. Optimization Problem Solving for N1

Then we solve the optimization problem of N1. The prob-

lem P2 can be written as,

Problem PN1 (Problem for N1):

minimize
n,tn}

{f e

subject to

(cid:88)

F1,n(M ∗

n, f e

n, tn),

(17)

n∈N1
(8c), (8d), (8e).

The optimization variables in the the problem PN1 are
the edge computation resource f e
n, and the proportion of
transmission time tn. We can obtain the optimal solution to
PN1 using the method of Lagrange multiplier. The partial
Lagrangian function can be written as,

LPN1

=

(cid:18) β1ρC(M ∗
n)
f e
n

+

(cid:88)

n∈N1

(cid:19)

+

β2M ∗
ndnpn
Rn

tn − 1) + µ1(

n − f e,max),
f e

(18)

ndn

β1M ∗
Rntn
(cid:88)

n∈N1

(cid:88)

+ µ0(

n∈N1

First of all, according to (18), supposing that M ∗

n is given,
we can solve the problem PN1 based on the Karush-Kuhn-
Tucker (KKT) condition. We can obtain the function expres-
sions of f e∗
n, as shown in the following
theorem.

n relative to M ∗

n and t∗

(21)

(22)

(23)

(24)

(25)

(26)

(27)

(28)

lowing necessary and sufﬁcient conditions,

+ u∗

1 = 0, f e∗

n > 0,

+ u∗

0 = 0, t∗

n > 0,

∂LPN1
∂f e∗
n
∂LPN1
∂t∗
n
(cid:88)

µ∗
0(

= −

β1ρC(M ∗
n)
f e∗2
n
β1M ∗
ndn
Rnt∗2
n
t∗
n − 1) = 0,

= −

n∈N ∗
(cid:88)

µ∗
1(

n∈N ∗

n − f e,max) = 0,
f e∗

1 ≥ 0.

0, µ∗
µ∗
Because β1ρC(M ∗
n)
1 are also positive. Therefore, we can obtain,

and β1M ∗
ndn
Rnt∗2
n

f e∗2
n

µ∗

are both positive, µ∗

0 and

(cid:88)

n∈N
(cid:88)

n∈N

f e∗
n − f e,max = 0,

t∗
n − 1 = 0,

f e∗
n =

(cid:115)

β1ρC(M ∗
n)
Rnµ∗
1

,

(cid:115)

β1M ∗
nd
Rnµ∗
0
Combining (26) and (28), we can get the expression of f e∗
n

t∗
n =

(29)

.

corresponding to M ∗
n,

f e∗
n =

f e,max(cid:112)C(M ∗
n)
(cid:112)C(M ∗
i )

(cid:80)
i∈N1

.

(30)

Similarly, combining (27) and (29), we can get the expression
of t∗

n corresponding to M ∗
n,

t∗
n =

(cid:113) M ∗
n
Rn
(cid:113) M ∗
i
Ri

(cid:80)
i∈N1

.

(31)

which completes the proof.

C. Optimization of Ofﬂoading Policy {xn}

We can use the Enumeration method to search for all
ofﬂoading strategies and get the optimal solution. However,
the complexity of Search-based ofﬂoading decision algorithm
becomes high when the number of devices N grows large.

In this section, we propose a iterative-based greedy algorithm
to optimize the ofﬂoading decision {xn}. Then, we analyze
the computational complexity of our proposed algorithms.
Inspired by the Theorem 1 and Theorem 2, when executing
the cost function F0,n and optimization
inference locally,
variables f md
n , Mn only depend on the device’s own param-
eters and are not affected by the parameters of other devices.
However, for edge set N1, the cost function is related to
the number and parameters of devices in the set N1. The
principle of the iterative-based algorithm is introduced as
follows. First, calculate the cost function {F0,n} of set N0
when each device’s task is executed locally. Second, assuming
that all devices are ofﬂoaded to the edge server for inference
and |N1| = N . In each iteration, the cost function {F1,n}
corresponding to each device of N1 is obtained. Comparing
{F0,n} and {F1,n} of the devices in the N1 set, we can get the
difference between {F0,n} and {F1,n} and select the device
with the largest difference as k. Try to put the device k from
the set N1 into the set N0 and compute the cost of new sets. If
the total cost of new sets is reduced, continue the next iteration.
Otherwise, put the device k back to the set N1.

V. NUMERICAL RESULTS

In this section, we evaluate the performance of the proposed
algorithms via simulations. For all
the simulation results,
unless speciﬁed otherwise, we set the downlink bandwidth
as Bw = 5 MHz and the power spectral as N0 = −174
dBm/Hz [11]. According to [8], the path loss is modelled
as P L = 128.1 + 37.6 log10(D) dB, where D is the dis-
tance between the device and the BS in kilometres. Devices
randomly distributed in the area within [500m 500m]. The
computational resource of the MEC server and devices are
set to be 1.8 GHz and 22 GHz, respectively. The recognition
accuracy requirement and the maximum number of input video
frames are set to α = 0.90, M max
= 16, respectively. The
coefﬁcient κ is determined by the corresponding device and
is set to be 10−28 in this paper according to [11]. The size of
the input video is 112 ∗ 112 ∗ Mn. In addition, the coefﬁcient
of computational complexity ρ is set to be 0.12 cycle/MAC,
which is obtained through several experiments. Weights β1, β2
are set to be 0.5, 0.5, respectively. The default number of
users is 20. We perform experimental analysis with the gesture
recognition task and the Resnet-18 network.

n

A. Simulation Results of Average Cost

In this section, we compare proposed algorithms and some
baseline algorithms. We mainly compare the average cost. We
run 100 tests and can calculate the average cost of each device
and the average running time of each test.

In Fig. 3, we plot the average cost of the proposed scheme
and some baseline schemes under a different number of
devices. The proposed scheme is compared with the local
inference scheme (Local), the edge inference scheme (Edge),
and the random ofﬂoading scheme (Random). When the
number of devices is less than 8, the cost of the scheme that
executes tasks only at the edge is almost equal to the cost of

Fig. 3. Comparison of the proposed scheme and three baseline schemes. When
there are only a small number of devices, the average cost of the proposed
scheme and the scheme that devices are randomly ofﬂoaded to the edge server
to complete inference tasks with 50% probability.

the proposed scheme. It is because all devices can beneﬁt from
performing inference on the edge server when the number of
devices is small. If the inference task is only executed locally,
the average cost of the device will not change because the
local resources among the equipment do not affect each other.
The curve for the Edge scheme is linear because we assume
that the AI model is the same for all users.

B. Simulation Results of Delay, Energy, and Accuracy

This section compares the average delay, energy consump-
tion, and the ofﬂoading rate (the proportion of devices that
perform inference on the edge server). We set the default
requirement for inference accuracy to be 0.9. We consider the
different number of devices, different bandwidths, different
edge computing resources, and different weights β1, β2.

Fig. 4 shows the average delay, energy, and ofﬂoading rate
under different numbers of devices, different bandwidths, and
different edge computing resources. In Fig. 4(a), we plot
results with different numbers of devices. As shown in Fig.
4(a), when the number of devices is small (less than 8), with
the number of devices increasing, the average delay increases,
and the average energy consumption decrease. This is because
when the number of devices is less than 8, all devices ofﬂoad
the task to the edge server (the ofﬂoading rate is equal to
1). With the number of devices increasing, communication
resources and the edge server’s computation resources are
shared by more devices. It leads to an increase in the delay and
a decrease in the number of input video frames for inference.
When the number of devices exceeds 8, the average energy
consumption increase, and the average delay and ofﬂoad
rate gradually decrease. Considering different bandwidths and
different edge computing resources, we plot Fig. 4(b) and Fig.
4(c). As shown in Fig. 4(b) and Fig. 4(c), the ofﬂoading rate
increases with the bandwidth and edge computing resource
increasing, indicating that more devices are ofﬂoading tasks
to the edge server.

412202836The number of devices00.511.5Average costProposedRandomLocalEdgeFig. 4. The average delay, energy, and ofﬂoading rate under different numbers of devices, different bandwidths, and different edge computing resources.

Innovation Program of Shanghai Municipal Science and Tech-
nology Commission grants 20JC1416400 and 21ZR1422400,
Pudong New Area Science & Technology Development Fund,
Key-Area Research and Development Program of Guang-
dong Province grant 2020B0101130012, Foshan Science and
Technology Innovation Team Project grant FS0AA-KJ919-
4402-0060, and research funds from Shanghai Institute for
Advanced Communication and Data Science (SICS).

REFERENCES

[1] L. N. Huynh, R. K. Balan, and Y. Lee, “Deepsense: A gpu-based deep
convolutional neural network framework on commodity mobile devices,”
in Proc. ACM WearSys’16, 2016, p. 25–30.

[2] Z. Liu, J. Li, Z. Shen, G. Huang, S. Yan, and C. Zhang, “Learning
efﬁcient convolutional networks through network slimming,” in Proc.
IEEE ICCV’17, Oct 2017, pp. 2736–2744.

[3] Z. Liu, M. Sun, T. Zhou, G. Huang, and T. Darrell, “Rethinking the
value of network pruning,” in Proc. ICLR’19, May 2019, pp. 1–21.
[4] W. Shi, Y. Hou, S. Zhou, Z. Niu, Y. Zhang, and L. Geng, “Improving
device-edge cooperative inference of deep learning via 2-step pruning,”
in Proc. IEEE INFOCOM WKSHPS’19, 2019, pp. 1–6.

[5] Y. Shi, K. Yang, T. Jiang, J. Zhang, and K. B. Letaief, “Communication-
efﬁcient edge AI: Algorithms and systems,” IEEE Commun. Surveys
Tuts., vol. 22, no. 4, pp. 2167–2191, 2020.

[6] X. Wang, Y. Han, C. Wang, Q. Zhao, X. Chen, and M. Chen, “In-edge
AI: Intelligentizing mobile edge computing, caching and communication
by federated learning,” IEEE Netw., vol. 33, no. 5, pp. 156–165, 2019.
[7] K. B. Letaief, Y. Shi, J. Lu, and J. Lu, “Edge artiﬁcial intelligence for
6G: Vision, enabling technologies, and applications,” IEEE J. Sel. Areas
Commun, vol. 40, no. 1, pp. 5–36, 2022.

[8] K. Yang, Y. Shi, W. Yu, and Z. Ding, “Energy-efﬁcient processing
and robust wireless cooperative transmission for edge inference,” IEEE
Internet Things J., vol. 7, no. 10, pp. 9456–9470, 2020.

[9] S. Hua, Y. Zhou, K. Yang, Y. Shi, and K. Wang, “Reconﬁgurable
intelligent surface for green edge inference,” IEEE Transactions on
Green Communications and Networking, vol. 5, no. 2, pp. 964–979,
2021.

[10] W. He, S. Guo, S. Guo, X. Qiu, and F. Qi, “Joint DNN partition
deployment and resource allocation for delay-sensitive deep learning
inference in IoT,” IEEE Internet Things J., vol. 7, no. 10, pp. 9241–
9254, 2020.

[11] Y. He, J. Ren, G. Yu, and Y. Cai, “Optimizing the learning performance
in mobile augmented reality systems with CNN,” IEEE Trans. Wireless
Commun., vol. 19, no. 8, pp. 5333–5344, 2020.

[12] E. Li, L. Zeng, Z. Zhou, and X. Chen, “Edge AI: On-demand acceler-
ating deep neural network inference via edge computing,” IEEE Trans.
Wireless Commun., vol. 19, no. 1, pp. 447–457, 2020.

[13] Z. Lin, S. Bi, and Y.-J. A. Zhang, “Optimizing AI service placement and
resource allocation in mobile edge intelligence systems,” IEEE Trans.
Wireless Commun., vol. 20, no. 11, pp. 7257–7271, 2021.

[14] C. Wang, S. Zhang, Y. Chen, Z. Qian, J. Wu, and M. Xiao, “Joint
conﬁguration adaptation and bandwidth allocation for edge-based real-
time video analytics,” in Proc. IEEE INFOCOM’20, 2020, pp. 257–266.

Fig. 5. The relationship between the delay, energy consumption, and accuracy.

We use different weights, β1, β2 to study the trade-off re-
lationship between the average delay and energy consumption
with accuracy requirement. The constraint is β1 + β2 = 1.
Fig. 5 shows the delay, energy consumption, and accuracy
are mutually limited. Lower energy consumption leads to
higher delay when the accuracy is constant. From another
perspective, to improve the accuracy, it is necessary to sacriﬁce
the performance of delay and energy consumption.

VI. CONCLUSION

This paper considers optimizing video-based AI inference
tasks in a multi-user MEC system. An MINLP is formulated to
minimize the total delay and energy consumption, with the re-
quirement of accuracy. We use a MAC-based model to analyze
the problem. We propose an iterative-based scheduling method
to solve this problem. We analyze the experimental results
with the different number of devices, different bandwidths
and different edge computing capabilities. We also plot the
relationship curve of the average delay, energy consumption,
and accuracy.

ACKNOWLEDGEMENT

This work was supported in part by the National Natural
Science Foundation of China (NSFC) under Grant 61871262,
62071284, and 61901251, the National Key R&D Program
of China grants 2017YFE0121400 and 2019YFE0196600, the

412202836The number of devices00.250.50.751DelayEnergyOffloading rate26101418Bandwidth / MHz00.250.50.751DelayEnergyOffloading rate412202836Edge computing resoure / GHz00.250.50.751DelayEnergyOffloading rate00.40.81.21.6Energy / J00.40.81.21.6Delay / sAccuracy 0.78Accuracy 0.87Accuracy 0.90Accuracy 0.93