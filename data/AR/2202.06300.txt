Deep Graph Learning for Spatially-Varying Indoor Lighting Prediction

JIAYANG BAI, JIE GUOâ€ , CHENCHEN WAN, ZHENYU CHEN, ZHEN HE, SHAN YANG, PIAOPIAO YU,
YAN ZHANG AND YANWEN GUOâ€ , State Key Lab for Novel Software Technology, Nanjing University, China

2
2
0
2

b
e
F
3
1

]

V
C
.
s
c
[

1
v
0
0
3
6
0
.
2
0
2
2
:
v
i
X
r
a

Fig. 1. Our method takes a single image as the input and predicts the full and spatially-varying indoor illumination that can be used to generate consistent
shading and realistic shadows after inserting virtual objects into the image.

Lighting prediction from a single image is becoming increasingly important
in many vision and augmented reality (AR) applications in which shad-
ing and shadow consistency between virtual and real objects should be
guaranteed. However, this is a notoriously ill-posed problem, especially for
indoor scenarios, because of the complexity of indoor luminaires and the
limited information involved in 2D images. In this paper, we propose a graph
learning-based framework for indoor lighting estimation. At its core is a
new lighting model (dubbed DSGLight) based on depth-augmented Spherical
Gaussians (SG) and a Graph Convolutional Network (GCN) that infers the
new lighting representation from a single LDR image of limited field-of-
view. Our lighting model builds 128 evenly distributed SGs over the indoor
panorama, where each SG encoding the lighting and the depth around that
node. The proposed GCN then learns the mapping from the input image to
DSGLight. Compared with existing lighting models, our DSGLight encodes
both direct lighting and indirect environmental lighting more faithfully and
compactly. It also makes network training and inference more stable. The
estimated depth distribution enables temporally stable shading and shadows
under spatially-varying lighting. Through thorough experiments, we show
that our method obviously outperforms existing methods both qualitatively
and quantitatively.

CCS Concepts: â€¢ Computing methodologies â†’ Mixed / augmented re-
ality; Scene understanding.

Additional Key Words and Phrases: Lighting, Graph learning, Rendering

1

INTRODUCTION

Predicting indoor illumination from a single image is a challenging
but meaningful task [Einabadi et al. 2021]. It is a fundamental step
for many applications, ranging from indoor scene understanding to
augmented reality (AR). However, this problem is highly ill-posed
due to the following reasons. First, indoor illumination stems from
a wide range of different luminaires including spot lights, large area
lights and indirect lighting. This is quite different from outdoor
lighting which is dominated by the sun [Hold-Geoffroy et al. 2019,
2017; Liu et al. 2015; Yu et al. 2021; Zhang et al. 2019]. Second, a
single image captured by an ordinary camera has limited field-of-
view (FoV) that may fail to provide sufficient cues of indoor light

â€  is the corresponding author.
Authorâ€™s address: Jiayang Bai, Jie Guoâ€ , Chenchen Wan, Zhenyu Chen, Zhen He, Shan
Yang, Piaopiao Yu, Yan Zhang and Yanwen Guoâ€  , State Key Lab for Novel Software
Technology, Nanjing University, Nanjing, China.

sources. In fact, many indoor light sources have a local impact
and yield spatially-varying lighting effects. This makes them hard
to be identified from a single FoV-limited image. Moreover, strong
lighting-reflectance ambiguities exist in indoor scenes since different
combinations of lighting and surface reflectance may produce the
same pixel intensities.

Currently, this problem is addressed in two ways. One line of
work assumes the availability of some prior knowledge about the
scene, e.g., scene geometries [Barron and Malik 2013; Maier et al.
2017; Weber et al. 2018] obtained by depth sensors or multi-view
inputs. Prior knowledge is effective for overcoming the ill-posedness
of the problem but will limit the applicability of the methods. A
recent trend is to leverage deep learning based solutions accom-
panied with large training datasets to infer lighting from a single,
casually captured image [Gardner et al. 2019, 2017b; Garon et al.
2019; Song and Funkhouser 2019; Srinivasan et al. 2020; Zhan et al.
2021a,b]. The lighting models used in these solutions can be divided
into two categories. In the first category, the indoor lighting of sur-
rounding environment is stored in a densely sampled environment
map [Gardner et al. 2017b], i.e., image-based lighting (IBL). Unfortu-
nately, inferring a full environment map directly is very difficult due
to the high dimensionality of the representation. Alternatively, the
second category leverages analytical models with a small number
of parameters to fit real-world illumination. For instance, indoor
illumination can be approximated by a set of Spherical Harmonics
(SH) [Cheng et al. 2018; Garon et al. 2019; Green 2003] or Spherical
Gaussians (SG) [Gardner et al. 2019; Tsai and Shih 2006a,b; Wang
et al. 2009; Zhan et al. 2021b]. Generally, SH works well for low-
frequency lighting, but is plagued with ringing artifacts as the SH
order increases. Gardner et al. [2019] proposed using 2-5 SGs to
model high-frequency light sources in an indoor scene. Although it
outperforms SH-based methods, regressing the positions of float-
ing SGs is unstable for deep learning. Consequently, this method
achieves degraded performance as the number of SGs increases.

To retain the advantages of SGs in compactly encoding high-
frequency lighting and enable stable training, we introduce DSG-
Light, a novel SG-based lighting model augmented with depth values.
In this model, ğ‘ SGs are assumed to have fixed positions and are
evenly distributed over a unit sphere. We also fix the bandwidth of

 
 
 
 
 
 
2

â€¢

Jiayang Bai, Jie Guoâ€ , Chenchen Wan, Zhenyu Chen, Zhen He, Shan Yang, Piaopiao Yu, Yan Zhang and Yanwen Guoâ€ 

each SG such that only its color and depth values vary. In this case,
predicting indoor lighting boils down to estimating ğ‘ RGBD values,
which is more stable and robust than regressing several free SGs
on the sphere. The augmented depth values allow us to generate
spatially-varying shading and shadows with respect to the insert
location.

Considering the non-Euclidean nature of DSGLight, we design a
graph convolution network (GCN) for feature extraction and light-
ing prediction. GCNs have gained impressive successes in many
vision tasks, but are less exploited in lighting prediction. In our
network, we first extract features from an input image with vanilla
convolutional layers. Then, these features are connected to GCN
features with several fully-connected layers. After information ex-
changing across neighboring nodes, we finally obtain the color and
depth for each SG. We demonstrate through extensive experiments
that our method outperforms existing methods both qualitatively
and quantitatively. The state-of-the-art performance on indoor light-
ing prediction allows us to achieve more realistic shading results
when inserting virtual objects into real indoor scenes (see Fig. 1).

In summary, our main contributions in this paper are:

â€¢ A depth-augmented indoor lighting model, i.e., DSGLight, to
compactly and faithfully encode spatially-varying lighting in
an indoor scene.

â€¢ A dedicated graph convolutional network for predicting DS-

GLight from a single image.

â€¢ A new dataset with supervision of DSGLight parameters that

is constructed to train our network.

2 RELATED WORK
2.1

Indoor lighting prediction

Inferring the illumination from 2D images is a classic problem that
has been extensively studied in the past decades. Early progress on
illumination estimation relied on prior knowledge of scene geome-
try. For instance, Barron and Malik [2013] adopted depth sensors
to capture geometric information, Wu et al. [2011] reconstructed
geometry with multi-view stereo, and Larsch et al. [2011] required
users to annotate the geometry. Some other works manage to predict
illumination from purely 2D image(s). Khan et al. [2006] presented a
method to flip the given image to approximate the rest of the whole
panorama, which are out of view. Unfortunately, this only works
well in some special cases.

While deep learning is significantly successful in many graphi-
cal and vision tasks, some learning-based methods are proposed to
estimate illumination. Hold-Geoffroy et al. [2017] presented an end-
to-end approach that leveraged CNN to predict outdoor illumination
represented by a low-dimensional analytical model, significantly
outperforming previous traditional methods. Gardner [2017b] also
adopted an end-to-end CNN to regress a non-parametric environ-
ment map to represent accurate illumination. Song et al. [2019]
proposed â€œNeural Illuminationâ€ that can predict indoor lighting at a
specific locale. Another recent work [Srinivasan et al. 2020] lever-
aged a multi-scale volumetric lighting representation to estimate
spatially-coherent illumination from images.

Fig. 2. Illustration of DSGLight. Left: centering directions of 128 evenly
distributed SGs. Middle: fitted DSGLight (RGB channels) representation for
an indoor panorama. Right: fitted DSGLight of depth representation.

However, these methods only capture dominated lights and do
not account for spatially-varying environmental lights, resulting in
fewer details for rendering specular objects and inconsistent shad-
ows. Moreover, these methods usually leverage CNNs to establish
the mapping between input images and SGs. However, CNNs, with
a limited receptive field, fail to explore the long-range correlations
between SGs.

2.2 Graph convolution networks

To deal with non-Euclidean data like graphs, Graph convolution net-
works (GCNs) provide well-suited solutions which can be divided
into two types: spatial-based [Atwood and Towsley 2015; Micheli
2009; Niepert et al. 2016] and spectral-based [Defferrard et al. 2016;
Hamilton et al. 2017; Kipf and Welling 2017]. Spatial-based methods
used in many works are mainly based on the spatial relations of
vertices in graphs and they apply spectral convolutions on graph
using ideas from graph signal processing [Shuman et al. 2012]. As
transforming the signal to spectral domains can be expensive, many
recent works [Defferrard et al. 2016; Kipf and Welling 2016] ap-
proximate the spectral convolutions using Chebyshev polynomials.
Similarly, our network also uses Chebyshev polynomials for spectral
convolutions in GCN layers. We use GCNs to extract features from
indoor illumination encoded in a non-Euclidean form and establish
the connection between 2D images and a SG-based lighting model.

3 DSGLIGHT
3.1 Representation

DSGLight is a depth-augmented parametric lighting model that
leverages a set of discrete SG functions to represent the spatially-
varying indoor illumination, as illustrated in Fig. 2. We denote DS-
GLight by M = {ğ‘šğ‘– |ğ‘– = 1, . . . , ğ‘ } which contains ğ‘ vertices. Each
vertex in DSGLight is associated with a spherical Gaussian [Wang
et al. 2009] defined as follows:

(1)

ğº (v; a, ğœ†, ğœ‡) = ağ‘’ğœ† (vğœ‡âˆ’1)
where ğœ‡ is the lobe axis (the centering direction), ğœ† âˆˆ (0, +âˆ) is the
lobe sharpness (the bandwidth), and a is the lobe amplitude (a âˆˆ
R4 for RGBD information). The direction v denotes the spherical
parameter of the resulting function. Given ğ‘šğ‘– = {ağ‘–, ğœ†ğ‘–, ğœ‡ğ‘– } for ğ‘–ğ‘¡â„
vertex, the overall indoor illumination encoded in a given panorama
is compressed into an arbitrary spherical function ğ‘“ (v) as follows:
ğ‘
âˆ‘ï¸

ğ‘“ (v) =

ğº (v; ğ‘šğ‘– ).

(2)

ğ‘–

Close to our work, several methods also leverage multiple SGs to
represent indoor lighting [Gardner et al. 2019; Zhan et al. 2021b].

The corresponding depth panorama is treated in a similar way.
Currently, we set ğ‘ to 128.

Deep Graph Learning for Spatially-Varying Indoor Lighting Prediction â€¢

3

(a) Original

(b) DSGLight (RGB channels) (c) [Gardner et al. 2019] (3 SGs)(d)

[Gardner et al. 2019] (5

(e) [Garon et al. 2019] (SH)

SGs)

Fig. 3. Visual comparisons of different lighting representations for an indoor scene.

To further reduce the number of parameters, we fix the position
ğœ‡ and the bandwidth ğœ† of each SG. Specifically, 128 nodes are placed
evenly over a unit sphere, and they share the same bandwidth. This
allows ğœ‡ and ğ‘£ to be preset for our model. The distribution of these
nodes is shown in Fig. 2. Besides being a more compact representa-
tion, such a design stabilizes network training and inferences. As
pointed out in [Gardner et al. 2019], regressing several floating SGs
with floating positions and unconstrained bandwidth is unstable,
especially for many SGs. As a result, performance decreases from 3
to 5 SGs. With fixed position and bandwidth, inferring DSGLight
boils down to predicting ğ‘ RGBD values, which is easier and more
stable than inferring floating SGs.

To make ğ‘ SGs cover the whole panorama as much as possible
and avoid overlapping, we set the bandwidth parameter ğœ† as follows:

ğœ† =

ln 0.6
âˆš
cos(arctan(2/

.

ğ‘ )) âˆ’ 1

(3)

Details are provided in the supplemental material.

3.2 Comparison and analysis

Previous works have proposed many parametric methods to reduce
the number of parameters in representing panoramic lighting, such
as SH, SG and IBL. Here, we compare these representations with
our DSGLight. One example comparing these representations is
provided in Fig. 3.

Among these methods, IBL is the most accurate. However, regress-
ing the high-dimensional IBL directly may lead to local minimal
and it is difficult to converge during training. With much fewer
parameters, SH is a low-dimensional lighting representation and
can be easily predicted with a compact decoder. Unfortunately, SH
is too sensitive to its parametersâ€™ variation. Moreover, the network
has a tendency to overfit to the ringing artifacts when employing
high-order SH functions to preserve high-frequency information.
SG is able to represent high-frequency lighting and avoid ringing
artifacts. However, Gardner et al. [2019] show that it is hard to
optimize the positions of SGs, especially when the number of SGs is
large. In comparison, DSGLight represents an indoor panorama with
a certain number of SG functions which have fixed positions and
bandwidth. It is more stable and effective than predicting arbitrary
and floating light sources. In addition, as the vertices are spread
evenly on the unit sphere and cover the entire sphere, DSGLight can
capture both dominated light sources and detailed environmental
lighting. This allows high-quality AR rendering with consistent
shading and shadows, as well as fine details on specular objects.

3.3 Dataset preparation

To generate the dataset for training, we have to prepare a large num-
ber of images and their corresponding DSGLight parameters. Given
a HDR panorama, generating ground truth DSGLight can be solved
as an optimization problem, i.e., minimizing the difference between
illumination/depth constructed by DSGLight and the ground truth
panorama:

ğ‘¨âˆ— = arg min

ğ‘¨

(cid:32)

ğ‘ âˆ’

ğ‘
âˆ‘ï¸

ğ‘–=1

âˆ‘ï¸

ğ‘ âˆˆ P

(cid:33)2

ğº (ğ¹ (ğ‘); ağ‘–, ğœ†ğ‘–, ğœ‡ğ‘– )

(4)

where ğ‘¨ = {a1, a2, . . . , ağ‘ } is the set of lobe amplitudes of SGs,
ğ‘ is a pixel in the ground truth panoramas P, and ğ¹ maps ğ‘ to a
direction v in the sphere. When fixing P, ğœ‡ and ğœ†, the problem boils
down to a linear optimization problem, which can be solved easily
by nonnegative least square algorithms.

We currently perform the optimization on two datasets:

â€¢ Laval dataset [Gardner et al. 2017a] contains 2,200 high
resolution indoor panoramas which are fully HDR without
any saturation. This dataset also contains depth values for
each panorama.

â€¢ SUN360 Dataset [Xiao et al. 2012] contains 12,000 indoor
LDR panoramas captured in a wide range of indoor scenes.
For this dataset, we first convert each LDR panorama into a
HDR panorama with the trained network of [Eilertsen et al.
2017]. This dataset does not contain depth values. Therefore,
we can only handle DSGLight with RGB channels when using
examples from this dataset.

We further employ Mitsuba renderer [Nimier-David et al. 2019] to
sample several Fov-limited images from each panorama at random
elevation between -20Â°and 20Â°, azimuth between -180Â°and 180Â°and
FoV between 60Â°and 80Â°. Meanwhile, the corresponding panorama
is captured by aligning its center with the view direction.

4 DEEP GRAPH LEARNING FOR DSGLIGHT

Our goal is to recover DSGLight of the environment in which the
photograph has been taken. It does not simply assign light intensity
and depth value to each visible light source, but must extrapolate
large portions of the invisible illumination. Thus, it requires wide
context information and must exploit very specific relationships
between light sources to guide estimation. To this end, we exploit
a graph convolutional network (GCN) to use prior knowledge on
the non-Euclidean nature of graph structure of DSGLight and cope
with significant amounts of potential relationships between nodes.

4

â€¢

Jiayang Bai, Jie Guoâ€ , Chenchen Wan, Zhenyu Chen, Zhen He, Shan Yang, Piaopiao Yu, Yan Zhang and Yanwen Guoâ€ 

Fig. 4. The overall pipeline of our method. Given an input image, we first leverage a backbone CNN as a feature extraction module (a) to learn the feature of
nodes in DSGLight. Then, we construct a graph with feature vectors initialized by the feature extraction module. A GCN (b) is employed to exchange feature
vectors across adjacent nodes and finally predicts the values (color or depth) of the SG nodes.

Running graph convolutions updates the indoor-specific perceptual
features extracted from the input image, which is equivalent as
driving light sourcesâ€™ intensities towards the correct light intensities
in the RGB color space.

4.1 Network architecture

Our framework consists of two parts: a feature extraction mod-
ule and a graph convolutional network (GCN). Fig. 4 shows the
architecture of our network.

In the feature extraction module, we can use any CNN based
backbone to extract the features of an input image. Currently, we
adopt the VGG-19 [Simonyan and Zisserman 2015] as the backbone
and generate a 512 Ã— 7 Ã— 11 feature matrix given an image with a
resolution of 240 Ã— 360. This feature matrix is further connected to
GCN with two fully-connected layers, which transform the feature
map to the features of each node in DSGLight.

In our framework, a graph convolutional layer takes a graph as
input and outputs a graph with a different feature matrix. It can be
written as the following non-linear function:

ğ‘™+1 = ğœ
H

(cid:16)

ğ‘™
EH

ğ‘™ (cid:17)

W

(5)

where Hğ‘™ is the input of the ğ‘™ğ‘¡â„ layer, Wğ‘™ is a trainable weight matrix,
E is the adjacency matrix describing the graph structure in the
matrix form, and ğœ is a non-linear activation function (ReLU in our
experiments). Stacking graph convolutional networks can combine
graph node features and graph topological structural information
to make final predictions.

GCN layer takes the node representations Hğ‘™ from previous layer
as inputs and outputs new node representations. Thus we construct
the graph G = {V, F , E} as the input of GCN. GCNs are vertex-
based and perform convolution on vertices with edges denoting
connections. V is the set of nodes corresponding to vertices in
DSGLight. F is the features of vertices learned from CNN. E is the
adjacency matrix in Eq. (5). To get the adjacency matrix, we use k-
Nearest Neighbors (k-NN) to construct the directed dynamic edges
between nodes.

Our GCN module comprises 4 graph convolutional layers. Li et
al. [2019] stated that stacking more layers into a GCN would easily
lead to the vanishing gradient problem and over-smoothing. Our
experiments show that four GCN layers are sufficient for multi-scale
feature extraction and are stable for training.

4.2 Loss functions

In the design of loss functions, we handle color and depth separately.
For depth, we only use L2 loss. For color, we design a hybrid loss
that consists of a reconstruction loss Lğ‘…, a perceptual loss Lğ‘‰ ğºğº
and a weighted L2 loss Lğ‘Š for the SG parameters, i.e.,

L = Lğ‘Š + ğ›¼ Lğ‘… + ğ›½Lğ‘‰ ğºğº
(6)
where ğ›¼ and ğ›½ are the weights to balance different losses. In our
training, we set ğ›¼ and ğ›½ to 0.2 and 0.1, respectively.

Reconstruction loss penalizes the per-pixel intensity distance
between the ground truth and the predicted environment map re-
constructed with Eq. (2). The perceptual loss aims to enhance the
perceptual similarity in the predicted illumination. We adopt a VGG-
19 architecture which is pre-trained in the image classification task.
Let Î¦ be the output of the last pooling layer in VGG-19, and ğ‘Î¦
represents the total number of pixels in this feature space. The
perceptual loss is defined as:

Lğ‘‰ ğºğº (ğ‘, Ë†ğ‘) =

1
ğ‘Î¦
where ğ‘ and Ë†ğ‘ denote the ground truth and the predicted environ-
ment map, separately.

â„“2 (Î¦(ğ‘), Î¦( Ë†ğ‘))

(7)

L2 loss is used mostly in regression problems but it will lose
high-frequency information in training. Since the DSGLight has a
wide range of values, we reweight SGs in the DSGLight to boost the
importance of salient SGs. Inspired by ACESFilm (Academy Color
Encoding System) curve, which distributes the weights to pixels of
HDR images empirically and achieves good tone mapping result,
we use the following weighted function:

ğ‘“ğ‘¤ (ğ‘¥) =

ğ‘¥ (2.51ğ‘¥ + 0.03)
ğ‘¥ (2.43ğ‘¥ + 0.59) + 0.14

.

(8)

The weighted L2 loss for SG parameters is calculated as :

Lğ‘Š (ğ‘¨, Ë†ğ‘¨) = ğ‘“ğ‘¤ (ğ‘¨)â„“2 (ğ‘¨, Ë†ğ‘¨)
(9)
where ğ‘¨ and Ë†ğ‘¨ represent the ground truth and our predicted pa-
rameters, respectively.

4.3 Discussion on the locality of indoor lighting

Most lights in indoor scenes have a local influence. This means we
not only need to predict the distribution of lighting around current
point of view, but also have to estimate the relative positions of the
lights. To handle depth, we augment DSGLight with depth values.

In this case, the lobe amplitude parameter of each SG in DSGLight
contains four channels (three for color and one for depth).

performed with a physically-based Mitsuba renderer [Nimier-David
et al. 2019].

Deep Graph Learning for Spatially-Varying Indoor Lighting Prediction â€¢

5

The training requires a large number of panoramas and their
ground truth depth maps. However, among our two datasets, only
Laval dataset has manually annotated per-pixel depth. From these
annotated depth maps, we optimize the SGs of depth according to
Eq. (4) where P is replaced by the depth map.

Some previous methods, such as [Li et al. 2020] and [Garon et al.
2019], also try to handle spatial variations of lighting for indoor
scenes. Unlike ours, they estimate indoor lighting for each inserted
point independently, yielding a local representation of lighting that
varies spatially. Although they produce finer spacial light prediction,
their strategy easily incurs temporal flickering when the inserted
virtual object moves in the scene. In contrast, our global representa-
tion of indoor lighting guarantees temporal consistency for moving
objects.

4.4 Training details

Our network is implemented on top of Tensorflow [Abadi et al.
2015]. We train it using SGD and the Adam solver with the moment
parameters ğ›½1 = 0.9 and ğ›½2 = 0.999. The learning rate is initially
set to 0.001 and halves every 40 epochs. The weights of all layers
in our network are initialized with the Xavier uniform. Training
examples are fed into our network with a mini-batch size of 5. We
train the network for 150 epochs which takes about two days on
one NVIDIA V100 GPU.

5 EVALUATION

In this section, we evaluate the performance of our method in indoor
illumination prediction and make comparisons with the state-of-the-
arts. More results and comparisons can be found in the supplemental
material.

5.1 Dataset and metrics

To generate our dataset, HDR panoramas are subdivided into two
parts, one for training and the other for testing. Therefore, the
panoramas in the test set will not appear during training. As men-
tioned in Sec. 3.3, we then randomly sample FOV-limited images
and generate the ground truth DSGLights from the panoramas. The
SUN360 dataset contains 50,000 images for training and the test
set contains 2,000 images. The Laval dataset has 10,000 images for
training and 2,000 for testing. Visual comparisons are conducted on
both predicted environment maps and rendered images with virtual
object insertion. PSNR is adopted as the quantitative measure.

5.2 Comparisons to previous methods

We predict indoor illumination with different methods and render
two scenes in Fig. 5 and Fig. 6, respectively. For the methods of [Gard-
ner et al. 2019] and [Zhan et al. 2021b], we implemented and trained
their networks with two datasets separately. Lighthouse [Srinivasan
et al. 2020] requires a held-out perspective view near the input
stereo pair and the depth. Since SUN360 lacks ground truth depth,
we only retrain Lighthouse on the Laval dataset. As for evaluation
on SUN360, we use their pre-trained model. More training details of
Lighthouse are provided in the supplemental material. Rendering is

Qualitative evaluation on the Laval dataset: In Fig. 5, we ren-
der the Armadillo scene using the predicted illumination of four
recent methods, as well as the ground truth and our method. The
renderings of the Armadillo on a plane highlight the accuracy of
estimated light directions. Note that our generated shadows and
ambient lighting are closer to the ground truth, as compared with
other methods.

Gardner et al. [2017b] proposed a CNN-based pipeline to predict
a RGB panorama and a light mask for the indoor illumination. The
predicted panorama is usually very smooth and may have color drift.
Therefore, the synthesized images are quite different from those
generated by the ground-truth lighting. Gardner et al. [2019] use a
parametric form based on SGs to encode indoor light sources, but
the centers of the light sources are allowed to vary wildly, leading to
unstable results. For the method of Srinivasan et al. [2020], it fails to
predict the lighting distribution outside the known regions, leading
to a large bias on rendering. Zhan et al. [2021b] have proved the
effectiveness of SGs by generating accurate illumination maps under
the guidance of predicted SG parameters. However, they do not sup-
port spatially-varying lighting prediction. Most importantly, their
CNN networks do not explore the long-range correlation between
SGs with a limited receptive field.

Qualitative evaluation on the SUN360 dataset: In Fig. 6, we
further compare these methods by rendering a specular Dragon
model lit by the estimated lighting. Since the model is highly specu-
lar, it can clearly reflect the environment around it. As expected, our
method captures details in the surroundings (e.g., the white wall and
the red floor) quite well. The predicted panorama is plausible and the
details provide better visual coherence. In comparison, Gardner et
al. [2017b] capture the hue of the scene, but the lighting distribution
is overly smooth. In contrast, Zhan et al. [2021b] hallucinate many
details different with the ground truth. For the method of [Gardner
et al. 2019], it hardly infers and represents lighting details.

Quantitative evaluation: We also provide the quantitative met-
rics in terms of PSNR for each method in Table 1. In this table, we
list the average PSNR values for the estimated environment map,
the rendered Armadillo scene and the Dragon scene. Obviously,
our method achieves higher PSNR values than others under ev-
ery testing scenario. This further shows that our method achieves
state-of-the-art performance on indoor lighting prediction.

5.3 Ablation study

To validate the importance of graph convolutional layers, we remove
them from the whole pipeline (-GCN). Now, the features from feature
extraction module are fed to a fully connected layer which predicts
the parameters directly. Fig. 7 reports the performances of these
models. We observe that environment maps generated by the model
without GCN module lack fine details for the scenes. It can not
exchange information across neighboring vertices through GCN
layers and vertices work independently. Therefore, it is hard to infer
illumination from the known region of panoramas. Tab. 1 shows that
PSNR drops slightly when our network removes the GCN module.

6

â€¢

Jiayang Bai, Jie Guoâ€ , Chenchen Wan, Zhenyu Chen, Zhen He, Shan Yang, Piaopiao Yu, Yan Zhang and Yanwen Guoâ€ 

Input Image

[Gardner et al. 2017b] [Gardner et al. 2019] [Srinivasan et al. 2020]

[Zhan et al. 2021b]

Ours

Ground Truth

18.98 dB

19.19 dB

17.09 dB

21.57 dB

21.74 dB

âˆ dB

18.34 dB

18.36 dB

24.51 dB

23.19 dB

25.60 dB

âˆ dB

Fig. 5. Rendering the Armadillo scene (purely diffuse) with the indoor illumination predicted by different methods. Quantitative metrics in terms of PSNR for
the rendered images are displayed below each method.

Input Image

[Gardner et al. 2017b] [Gardner et al. 2019] [Srinivasan et al. 2020]

[Zhan et al. 2021b]

Ours

Ground Truth

19.50 dB

17.88 dB

18.33 dB

12.30 dB

23.37 dB

âˆ dB

19.59 dB

20.09 dB

20.28 dB

18.18 dB

28.61 dB

âˆ dB

Fig. 6. Rendering the Dragon scene (specular) with the indoor illumination predicted by different methods. Quantitative metrics in terms of PSNR for the
rendered images are displayed below each method. Depth values are not concerned in these cases.

It also shows that weighted L2 loss is beneficial for improving the
performance.

5.4 Validation of spatially-varying lighting

As aforementioned, our method supports spatially-varying lighting
when inserting virtual objects into different locations of an indoor
scene, due to the availability of the depth information. To demon-
strate this, we insert virtual objects at different locations in real

images of [Garon et al. 2019]. Two examples are provided in Fig. 8.
Here, we compare our method with [Li et al. 2020] that also enables
spatially-varying indoor lighting. As seen, our results are closer to
the ground truth. More importantly, our method guarantees tem-
poral consistency of lighting when the virtual object moves in the
scene, since our DSGLight is unique and global for a given image.
However, Li et al.â€™s method [Li et al. 2020] estimates local lighting

Table 1. Quantitative comparisons in terms of PSNR (dB). The best scores
are highlighted in bold. -GCN and -Lğ‘Š represent the network trained
without GCN module and weighted L2 loss, respectively.

Dataset

Method

Map Armadillo Dragon

Laval

SUN360

[Gardner et al. 2017b]
[Gardner et al. 2019]
[Srinivasan et al. 2020]
[Zhan et al. 2021b]
Ours
-GCN
-Lğ‘Š

[Gardner et al. 2017b]
[Gardner et al. 2019]
[Srinivasan et al. 2020]
[Zhan et al. 2021b]
Ours
-GCN
-Lğ‘Š

14.45
16.49
17.50
13.80
22.54
20.90
21.87

11.90
12.36
13.63
9.02
13.75
13.16
13.70

20.60
22.21
21.42
16.63
26.16
24.83
20.30

23.83
23.58
21.10
20.17
24.82
23.39
16.85

18.35
21.14
23.00
18.76
24.00
22.11
22.61

20.15
19.04
21.04
15.97
21.81
21.21
21.54

T
G

s
r
u
O

N
C
G

-

Fig. 7. Validating the contribution of GCN to the performance of the pre-
sented model.

(a) [Li et al. 2020]

(b) DSGLight

(c) Ground truth

Deep Graph Learning for Spatially-Varying Indoor Lighting Prediction â€¢

7

Fig. 9. Each scene in the user study is shown as a column, where different
colors indicate that users preferred the corresponding method instead of
the ground truth.

scene, we generate a reference composite by relighting a bunny
model with its ground truth light probe. We compare these results
with objects that were relit with estimated illumination from [Li
et al. 2020] and our method. Users are asked to pick which rendering
is more realistic between image pairs rendered with ground truth
and the estimated lighting. We gather responses from 138 unique
participants and the results are shown in Fig. 9. Both of [Li et al.
2020] and our method beat each other in half of the scenarios, which
suggests that they are of equal visual quality. However, our method
outperforms [Li et al. 2020] slightly on the average confusion, with
a 32.00% vs 30.75% confusion.

5.5 Limitations

Despite the state-of-the-art performance, our method still has some
limitations. First, DSGLight is an approximation to the detailed
environment map, lacking sharp features. This prevents specular
objects from generating sharp textures reflecting the surrounding.
Moreover, the out-of-view depth generated by our method is not
very accurate due to the high ill-posedness. This would be solved
by multiple input images.

6 CONCLUSION

In this paper, we have proposed the first graph learning-based strat-
egy for inferring high-quality and spatially-varying indoor illumina-
tion from a single image. To represent both dominated local lights
and detailed environmental lighting with high fidelity, we encode
the panoramic indoor illumination in DSGLight, a depth-augmented
lighting model with 128 SGs evenly distributed over a sphere. A
GCN is adopted to predict non-Euclidean DSGLight from a 2D im-
age. Through thorough comparisons, we have demonstrated that
our method outperforms previous state-of-the-art methods both
qualitatively and quantitatively.

Fig. 8. Demonstration of spatially-varying lighting on real images of [Garon
et al. 2019].

REFERENCES

independently for each inserted position, which may easily cause
temporal flickering.

To further show the superiority of our method, we conduct a
perceptual user study on 20 scenes of [Garon et al. 2019]. For each

MartÃ­n Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro,
Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat,
Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal
Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dan ManÃ©, Rajat
Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens,
Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay
Vasudevan, Fernanda ViÃ©gas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin
Wicke, Yuan Yu, and Xiaoqiang Zheng. 2015. TensorFlow: Large-Scale Machine

8

â€¢

Jiayang Bai, Jie Guoâ€ , Chenchen Wan, Zhenyu Chen, Zhen He, Shan Yang, Piaopiao Yu, Yan Zhang and Yanwen Guoâ€ 

Learning on Heterogeneous Systems. http://tensorflow.org/ Software available
from tensorflow.org.

James Atwood and Don Towsley. 2015. Diffusion-Convolutional Neural Networks.

Arxiv (01 2015).

Jonathan Barron and Jitendra Malik. 2013. Intrinsic Scene Properties from a Single
RGB-D Image. Proceedings / CVPR, IEEE Computer Society Conference on Computer
Vision and Pattern Recognition. IEEE Computer Society Conference on Computer Vision
and Pattern Recognition 38 (06 2013). https://doi.org/10.1109/CVPR.2013.10

Dachuan Cheng, Jian Shi, Yanyun Chen, Xiaoming Deng, and Xiaopeng. Zhang. 2018.
Learning Scene Illumination by Pairwise Photos from Rear and Front Mobile Cam-
eras. Computer Graphics Forum 37, 7 (2018), 213â€“221.

MichaÃ«l Defferrard, Xavier Bresson, and Pierre Vandergheynst. 2016. Convolutional
Neural Networks on Graphs with Fast Localized Spectral Filtering. In Proceedings of
the 30th International Conference on Neural Information Processing Systems (Barcelona,
Spain) (NIPSâ€™16). Curran Associates Inc., Red Hook, NY, USA, 3844â€“3852.

Gabriel Eilertsen, Joel Kronander, Gyorgy Denes, Rafal K. Mantiuk, and Jonas Unger.
2017. HDR image reconstruction from a single exposure using deep CNNs. ACM
Trans. Graph. 36, 6 (2017), 178:1â€“178:15. https://doi.org/10.1145/3130800.3130816
Farshad Einabadi, Jean-Yves Guillemaut, and Adrian Hilton. 2021. Deep Neural Models
for Illumination Estimation and Relighting: A Survey. Computer Graphics Forum 40,
6 (2021), 315â€“331.

Marc-AndrÃ© Gardner, Yannick Hold-Geoffroy, Kalyan Sunkavalli, Christian GagnÃ©,
and Jean-FranÃ§ois Lalonde. 2019. Deep Parametric Indoor Lighting Estimation. In
2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019, Seoul, Korea
(South), October 27 - November 2, 2019. IEEE, 7174â€“7182. https://doi.org/10.1109/
ICCV.2019.00727

Marc-AndrÃ© Gardner, Kalyan Sunkavalli, Ersin Yumer, Xiaohui Shen, Emiliano Gam-
baretto, Christian GagnÃ©, and Jean-FranÃ§ois Lalonde. 2017a. Learning to predict
indoor illumination from a single image. ACM Trans. Graph. 36, 6 (2017), 176:1â€“
176:14. https://doi.org/10.1145/3130800.3130891

Marc-AndrÃ© Gardner, Kalyan Sunkavalli, Ersin Yumer, Xiaohui Shen, Emiliano Gam-
baretto, Christian GagnÃ©, and Jean-FranÃ§ois Lalonde. 2017b. Learning to Predict
Indoor Illumination from a Single Image. ACM Trans. Graph. 36, 6, Article 176 (Nov.
2017), 14 pages.

M. Garon, K. Sunkavalli, S. Hadap, N. Carr, and J. Lalonde. 2019. Fast Spatially-Varying
Indoor Lighting Estimation. In 2019 IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR). 6901â€“6910. https://doi.org/10.1109/CVPR.2019.00707

R. Green. 2003. Spherical Harmonic Lighting: The Gritty Details.
William L. Hamilton, Rex Ying, and Jure Leskovec. 2017. Inductive Representation
Learning on Large Graphs. CoRR abs/1706.02216 (2017). arXiv:1706.02216 http:
//arxiv.org/abs/1706.02216

Y. Hold-Geoffroy, A. Athawale, and J. Lalonde. 2019. Deep Sky Modeling for Single
Image Outdoor Lighting Estimation. In 2019 IEEE/CVF Conference on Computer
Vision and Pattern Recognition (CVPR). 6920â€“6928. https://doi.org/10.1109/CVPR.
2019.00709

Y. Hold-Geoffroy, K. Sunkavalli, S. Hadap, E. Gambaretto, and J. Lalonde. 2017. Deep
outdoor illumination estimation. In 2017 IEEE Conference on Computer Vision and
Pattern Recognition (CVPR). 2373â€“2382. https://doi.org/10.1109/CVPR.2017.255
Yannick Hold-Geoffroy, Kalyan Sunkavalli, Sunil Hadap, Emiliano Gambaretto, and
Jean-FranÃ§ois Lalonde. 2017. Deep Outdoor Illumination Estimation. 2373â€“2382.
https://doi.org/10.1109/CVPR.2017.255

Kevin Karsch, Varsha Hedau, David Forsyth, and Derek Hoiem. 2011. Rendering
Synthetic Objects into Legacy Photographs (SA â€™11). Association for Computing
Machinery, New York, NY, USA, Article 157, 12 pages. https://doi.org/10.1145/
2024156.2024191

Erum Arif Khan, Erik Reinhard, Roland W. Fleming, and Heinrich H. BÃ¼lthoff. 2006.
Image-Based Material Editing (SIGGRAPH â€™06). Association for Computing Machin-
ery, New York, NY, USA, 654â€“663. https://doi.org/10.1145/1179352.1141937

Thomas N. Kipf and Max Welling. 2016. Semi-Supervised Classification with Graph
Convolutional Networks. CoRR abs/1609.02907 (2016). arXiv:1609.02907 http:
//arxiv.org/abs/1609.02907

Thomas N. Kipf and Max Welling. 2017. Semi-Supervised Classification with Graph
Convolutional Networks. In 5th International Conference on Learning Representa-
tions, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings.
OpenReview.net. https://openreview.net/forum?id=SJU4ayYgl

Guohao Li, Matthias MÃ¼ller, Ali K. Thabet, and Bernard Ghanem. 2019. DeepGCNs:
Can GCNs Go As Deep As CNNs?. In 2019 IEEE/CVF International Conference on
Computer Vision, ICCV 2019, Seoul, Korea (South), October 27 - November 2, 2019.
IEEE, 9266â€“9275. https://doi.org/10.1109/ICCV.2019.00936

Zhengqin Li, Mohammad Shafiei, Ravi Ramamoorthi, Kalyan Sunkavalli, and Man-
mohan Chandraker. 2020.
Inverse rendering for complex indoor scenes: Shape,
spatially-varying lighting and svbrdf from a single image. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2475â€“2484.
Yang Liu, Theo Gevers, and Xueqing Li. 2015. Estimation of Sunlight Direction Using
3D Object Models. IEEE Transactions on Image Processing 24, 3 (2015), 932â€“942.

Robert Maier, Kihwan Kim, Daniel Cremers, Jan Kautz, and Matthias NieÃŸner. 2017.
Intrinsic3D: High-Quality 3D Reconstruction by Joint Appearance and Geometry
Optimization with Spatially-Varying Lighting. In IEEE International Conference on
Computer Vision, ICCV 2017, Venice, Italy, October 22-29, 2017. IEEE Computer Society,
3133â€“3141. https://doi.org/10.1109/ICCV.2017.338

A. Micheli. 2009. Neural Network for Graphs: A Contextual Constructive Approach.
IEEE Transactions on Neural Networks 20, 3 (2009), 498â€“511. https://doi.org/10.1109/
TNN.2008.2010350

Mathias Niepert, Mohamed Ahmed, and Konstantin Kutzkov. 2016. Learning Convolu-
tional Neural Networks for Graphs. CoRR abs/1605.05273 (2016). arXiv:1605.05273
http://arxiv.org/abs/1605.05273

Merlin Nimier-David, Delio Vicini, Tizian Zeltner, and Wenzel Jakob. 2019. Mitsuba 2:
A Retargetable Forward and Inverse Renderer. Transactions on Graphics (Proceedings
of SIGGRAPH Asia) 38, 6 (Dec. 2019). https://doi.org/10.1145/3355089.3356498
David Shuman, Sunil K. Narang, Pascal Frossard, Antonio Ortega, and Pierre Van-
dergheynst. 2012. The Emerging Field of Signal Processing on Graphs: Extending
High-Dimensional Data Analysis to Networks and Other Irregular Domains. IEEE
Signal Processing Magazine 30 (10 2012). https://doi.org/10.1109/MSP.2012.2235192
Karen Simonyan and Andrew Zisserman. 2015. Very Deep Convolutional Networks
for Large-Scale Image Recognition. In 3rd International Conference on Learning
Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track
Proceedings, Yoshua Bengio and Yann LeCun (Eds.). http://arxiv.org/abs/1409.1556
Shuran Song and Thomas A. Funkhouser. 2019. Neural Illumination: Lighting Predic-
tion for Indoor Environments. In IEEE Conference on Computer Vision and Pattern
Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019. Computer Vision
Foundation / IEEE, 6918â€“6926.

Pratul P. Srinivasan, Ben Mildenhall, Matthew Tancik, Jonathan T. Barron, Richard
Tucker, and Noah Snavely. 2020. Lighthouse: Predicting Lighting Volumes for
Spatially-Coherent Illumination. In 2020 IEEE/CVF Conference on Computer Vision
and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020. IEEE, 8077â€“
8086. https://doi.org/10.1109/CVPR42600.2020.00810

Yu-Ting Tsai and Zen-Chung Shih. 2006a. All-frequency precomputed radiance transfer
using spherical radial basis functions and clustered tensor approximation. ACM
Trans. Graph. 25, 3 (2006), 967â€“976. https://doi.org/10.1145/1141911.1141981
Yu Ting Tsai and Zen-Chung Shih. 2006b. All-frequency precomputed radiance transfer
using spherical radial basis functions and clustered tensor approximation. ACM
Transactions on Graphics 25, 3 (1 July 2006), 967â€“976. https://doi.org/10.1145/
1141911.1141981 null ; Conference date: 30-07-2006 Through 03-08-2006.

Jiaping Wang, Peiran Ren, Minmin Gong, John Snyder, and Baining Guo. 2009. All-
frequency rendering of dynamic, spatially-varying reflectance. ACM Trans. Graph.
28, 5 (2009), 133. https://doi.org/10.1145/1618452.1618479

Henrique Weber, Donald PrÃ©vost, and Jean-FranÃ§ois Lalonde. 2018. Learning to Estimate
Indoor Lighting from 3D Objects. In International Conference on 3D Vision, Verona,
Italy. 199â€“207.

C. Wu, B. Wilburn, Y. Matsushita, and C. Theobalt. 2011. High-quality shape from
multi-view stereo and shading under general illumination. In CVPR 2011. 969â€“976.
https://doi.org/10.1109/CVPR.2011.5995388

Jianxiong Xiao, Krista A. Ehinger, Aude Oliva, and Antonio Torralba. 2012. Recognizing
scene viewpoint using panoramic place representation. In 2012 IEEE Conference on
Computer Vision and Pattern Recognition, Providence, RI, USA, June 16-21, 2012. IEEE
Computer Society, 2695â€“2702. https://doi.org/10.1109/CVPR.2012.6247991

Piaopiao Yu, Jie Guo, Fan Huang, Cheng Zhou, Hongwei Che, Xiao Ling, and Yan-
wen Guo. 2021. Hierarchical Disentangled Representation Learning for Outdoor
Illumination Estimation and Editing. In Proceedings of the IEEE/CVF International
Conference on Computer Vision (ICCV). 15313â€“15322.

Fangneng Zhan, Changgong Zhang, Wenbo Hu, Shijian Lu, Feiying Ma, Xuansong
Xie, and Ling Shao. 2021a. Sparse Needlets for Lighting Estimation with Spherical
Transport Loss. In Proceedings of the IEEE International Conference on Computer
Vision.

Fangneng Zhan, Changgong Zhang, Yingchen Yu, Yuan Chang, Shijian Lu, Feiying Ma,
and Xuansong Xie. 2021b. EMLight: Lighting Estimation via Spherical Distribution
Approximation. In Proceedings of the AAAI Conference on Artificial Intelligence.
J. Zhang, K. Sunkavalli, Y. Hold-Geoffroy, S. Hadap, J. Eisenman, and J. Lalonde. 2019.
All-Weather Deep Outdoor Lighting Estimation. In 2019 IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR). 10150â€“10158. https://doi.org/10.
1109/CVPR.2019.01040

