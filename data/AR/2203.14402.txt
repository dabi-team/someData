2
2
0
2

n
u
J

7
1

]

V
C
.
s
c
[

3
v
2
0
4
4
1
.
3
0
2
2
:
v
i
X
r
a

UV Volumes for Real-time Rendering of Editable Free-view Human
Performance

YUE CHEN, Xiâ€™an Jiaotong University, China
XUAN WANG, Tencent AI Lab, China
XINGYU CHEN, Xiâ€™an Jiaotong University, China
QI ZHANG, Tencent AI Lab, China
XIAOYU LI, Tencent AI Lab, China
YU GUO, Xiâ€™an Jiaotong University, China
JUE WANG, Tencent AI Lab, China
FEI WANG, Xiâ€™an Jiaotong University, China

Fig. 1. We decompose the dynamic human into 3D UV Volumes and a 2D texture. The disentanglement of appearance from geometry enables us to achieve (a)
real-time novel view synthesis with (b) the UV avatar, (c) retexturing of 3D human by editing the 2D texture, (d) reshaping and (e) reposing by changing the
parameters of human model while keeping the texture untouched.

Neural volume rendering enables photo-realistic renderings of a human
performer in free-view, a critical task in immersive VR/AR applications. But
the practice is severely limited by high computational costs in the rendering
process. To solve this problem, we propose the UV Volumes, a new approach
that can render an editable free-view video of a human performer in real-
time. It separates the high-frequency (i.e., non-smooth) human appearance
from the 3D volume, and encodes them into 2D neural texture stacks (NTS).

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
SIGGRAPH Conference Proceedings, Dec 2022, Daegu
Â© 2022 Association for Computing Machinery.
ACM ISBN 978-1-4503-1234-5/22/07. . . $15.00
https://doi.org/10.1145/8888888.7777777

The smooth UV volumes allow much smaller and shallower neural networks
to obtain densities and texture coordinates in 3D while capturing detailed
appearance in 2D NTS. For editability, the mapping between the parameter-
ized human model and the smooth texture coordinates allows us a better
generalization on novel poses and shapes. Furthermore, the use of NTS
enables interesting applications, e.g., retexturing. Extensive experiments on
CMU Panoptic, ZJU Mocap, and H36M datasets show that our model can
render 960 Ã— 540 images in 30FPS on average with comparable photo-realism
to state-of-the-art methods. The project and supplementary materials are
available at https://github.com/fanegg/UV-Volumes.
CCS Concepts: â€¢ Computing methodologies â†’ Rendering.
ACM Reference Format:
Yue Chen, Xuan Wang, Xingyu Chen, Qi Zhang, Xiaoyu Li, Yu Guo, Jue
Wang, and Fei Wang. 2022. UV Volumes for Real-time Rendering of Editable
Free-view Human Performance. In Proceedings of SIGGRAPH Conference
Proceedings. ACM, New York, NY, USA, 8 pages. https://doi.org/10.1145/
8888888.7777777

1

(c) Retexturing(e) Reposing(d) Reshaping(a) Novel View SynthesisDyNeRFNeuralBodyAnimatable-NeRFOurs1.5FPS1.3FPS0.3FPS42FPS(b) UV Avatar 
 
 
 
 
 
SIGGRAPH Conference Proceedings, Dec 2022, Daegu

Yue Chen, Xuan Wang, Xingyu Chen, Qi Zhang, Xiaoyu Li, Yu Guo, Jue Wang, and Fei Wang

INTRODUCTION

1
Synthesizing free-view videos of a human performer in motion is a
long-standing problem in computer graphics. Early approaches [Col-
let et al. 2015a] rely on obtaining an accurate 3D mesh sequence
through multi-view stereo. However, the computed 3D mesh often
fails to depict the complex geometry structure, resulting in limited
photorealism. Methods (e.g., NeRF [Mildenhall et al. 2020]) that
make use of volumetric representation and differentiable ray cast-
ing have shown promising results for novel view synthesis. These
techniques have been further extended to tackle dynamic scenes.

Nonetheless, NeRF and its variants require a large number of
queries against a deep Multi-Layer Perceptron (MLP). Such time-
consuming computation prevents them from being applied to appli-
cations that require high rendering efficiency. In the case of static
NeRF, a few methods [Garbin et al. 2021; Reiser et al. 2021; Yu
et al. 2021b] have already achieved real-time performance. However,
for dynamic NeRF, solutions for real-time rendering of volumetric
free-view video are still lacking.

In this work, we present UV Volumes, a novel framework that
can produce editable free-view videos of a human performer in
motion and render it in real-time. Specifically, we take advantage
of a pre-defined UV-unwrapping (e.g., SMPL or DensePose) of the
human body to tackle the geometry (with texture coordinates) and
textures in two branches. We employ a sparse 3D Convolutional
Neural Networks (CNN) to transform the voxelized and structured
latent codes anchored with a posed SMPL model to a 3D feature
volume, in which only smooth and view-independent densities and
UV coordinates are encoded. For rendering efficiency, we use a
shallow MLP to decode the density and integrate the feature into the
image plane by volume rendering. Each feature in the image plane
is then converted to the UV coordinates individually. Accordingly,
we utilize the yielded UV coordinates to query the RGB value from
a pose-dependent neural texture stack (NTS). This process greatly
reduces the number of queries against MLPs and enables real-time
rendering.

It is worth noting that the network modules in the proposed
framework only need to approximate relatively "smooth" functions.
As shown in Figure 2, the magnitude spectrum of the RGB image and
the corresponding UV image indicates that UV is much smoother
than RGB. That is, we only model the low-frequency density and UV
coordinate in the 3D volumes, and then detail the appearance in the
2D NTS, which is also spatially aligned across different poses. This
feature also increases the generalization power of such modules and
supports various editing operations.

We perform extensive experiments on three widely-used datasets:
CMU Panoptic, ZJU Mocap, and H36M datasets. The results show
that the proposed approach can effectively generate editable free-
view video from both dense and sparse views. The produced free-
view video can be rendered in real-time with comparable photoreal-
ism to the state-of-the-art methods that have much higher compu-
tational costs. In summary, our major contributions are:

â€¢ A novel system for rendering editable human performance

video in free-view and real-time.

â€¢ UV Volumes, a method that can accelerate the rendering

process while preserving the high-frequency details.

Fig. 2. Discrete Fourier Transform (DFT) for RGB and UV image. In the
magnitude spectrum, the distance from each point to the midpoint describes
the frequency, the direction from each point to the midpoint describes the
direction of the plane wave, and the value of the point describes its ampli-
tude. The distribution of the UV magnitude spectrum is more concentrated
in the center, which indicates that the frequency of the UV image is lower.

â€¢ Extended editing applications enabled by this framework,

such as reposing, retexturing, and reshaping.

2 RELATED WORK

Novel View Synthesis for Static Scenes. Novel view synthesis
for static scenes is a well-explored problem. Early image-based ren-
dering approaches [Davis et al. 2012; Gortler et al. 1996; Levoy and
Hanrahan 1996] utilize densely sampled images to obtain novel
views with light fields instead of explicit or accurate geometry esti-
mation. The learning-based methods [Flynn et al. 2019; Kalantari
et al. 2016; Mildenhall et al. 2019; Srinivasan et al. 2019] apply neu-
ral networks to reuse input pixels from observed viewpoints. In
recent years, dramatic improvements have been achieved by neu-
ral volume rendering techniques. For instance, NeRF [Mildenhall
et al. 2020] represents a static scene using a deep MLP, mapping 3D
spatial locations and 2D viewing directions to volumetric density
and radiance. However, rendering high-resolution scenes via NeRF
is time-consuming since it requires millions of queries to obtain the
density and radiance. Subsequent works [Garbin et al. 2021; Reiser
et al. 2021; Yu et al. 2021a,b] attempt to accelerate the inference of
vanilla NeRF in various ways, some of which achieve the real-time
rendering performance, but only for static scenes.

Free-View Video Synthesis. Early methods [Collet et al. 2015b;
Mustafa et al. 2016] rely on accurate 3D reconstruction and texture
rendering captured by dome-based multi-camera systems to syn-
thesize novel views of a dynamic scene. Recently, various neural
representations have been employed in differentiable rendering to
depict dynamic scenes, such as the voxels [Lombardi et al. 2019],
point clouds [Wu et al. 2020], textured meshes [Thies et al. 2019],
and implicit functions [Li et al. 2021b; Liu et al. 2020; Park et al.
2021a,b; Pumarola et al. 2021]. Particularly, DyNeRF [Li et al. 2021b]
directly takes the latent code as the condition for time-varying
scenes. Other deformation-based NeRF variants [Li et al. 2021a;
Park et al. 2021a,b; Pumarola et al. 2021; Tretschk et al. 2021] take
as input the monocular video, as a result, they fail to synthesize the
free-view spatio-temporal visual effects. Besides, they also suffer
from the high computational cost in inference and the lack of editing
abilities. Geometric constraints are exploited in methods [Shao et al.
2021; Yu et al. 2021c], and a hybrid scene representation is used
for efficiency in a very recent approach [Lombardi et al. 2021], but
non-editable models still.

2

UV magnitude spectrumUV imageRGB magnitude spectrumRGB image0.00.20.40.60.81.0UV Volumes for Real-time Rendering of Editable Free-view Human Performance

SIGGRAPH Conference Proceedings, Dec 2022, Daegu

Fig. 3. Overall pipeline of proposed framework. Our model has two main branches: 1) Based on a human pose ğœƒ , a volume generator constructs UV volumes
involving the feature of UV information. Then a feature map can be rendered via differentiable raymarching and decoded to texture coordinates (UV)
pixel-by-pixel. 2) A texture generator produces a pose-dependent Neural Texture Stack(NTS) E encoding the highly-detailed appearance information. The UV
coordinates and the texture embedding interpolated by it are passed into an MLP to predict a color ^C at the desired ray direction d.

Editable Free-View Videos. There exist previous works that fo-
cus on the problem of producing editable free-view video or ani-
matable avatars. ST-NeRF [Zhang et al. 2021] exploits the layered
neural representation in order to move, rotate and resize individual
objects in free-view videos. Methods [Peng et al. 2021a; Weng et al.
2022; Xu et al. 2021] decompose a dynamic human into a canonical
neural radiance field and a skeleton-driven warp field that backward
map observation-space points to canonical space. However, learn-
ing a backward warp field is highly under-constrained, since the
backward warp field is pose-dependent [Chen et al. 2021]. Neural
Actor [Liu et al. 2021] takes the texture map as latent variables.
Unfortunately, the ground-truth texture map usually can not be
obtained without high-fidelity 3D reconstruction. In contrast, with-
out using any high-fidelity 3D reconstructions, our approach can
produce editable (including reposing, reshaping and retexturing)
free-view videos in real-time from both dense and sparse views.

3 METHOD
Given multi-view videos of a performer, our model generates an
editable free-view video that supports real-time rendering. We use
the availability of an off-the-shelf SMPL model and the pre-defined
UV unwrap in Densepose [GÃ¼ler et al. 2018] to introduce proper
priors into our framework. In this section, we describe the details of
our framework, which is shown in Figure 3. The two main branches
in our framework are presented in turn. One is to generate the UV
volumes (Sec. 3.1), and the other is the generation of NTS (Sec. 3.2).
Then we provide a more detailed description of the training process
in Sec. 3.3.

3.1 UV Volumes
Neural radiance fields [Mildenhall et al. 2020] have been proven
to produce free-viewpoint images with view consistency and high

3

fidelity. Nonetheless, capturing the high-fidelity appearance in a
dynamic scene is time-consuming and difficult. To this end, we
propose the UV volumes in which only the density and texture
coordinate (i.e., UV coordinate) are encoded instead of human ap-
pearance. Given the UV image rendered by ray casting, we can use
the UV coordinate to query the corresponding RGB values from the
2D NTS by employing the UV unwrap defined in Densepose.

We utilize the volume generator to construct UV volumes. First,
the time-invariant latent codes anchored to a posed SMPL model
are voxelized and taken as the input. Then we use the 3D sparse
CNN to encode the voxelized latent codes to a 3D feature volume
named the UV volumes, which contains UV information.

Given a sample image I of multi-view videos, we provide a posed
SMPL parameterized by human pose ğœƒ and a set of latent codes z
anchored on its vertices and then query the feature vector ğ‘“ (x, z, ğœƒ )
at point x from the generated UV volumes. The feature vector is fed
into a shallow MLP ğ‘€ğœ to predict the volume density:

ğœ (x) = ğ‘€ğœ (ğ‘“ (x, z, ğœƒ )) .

(1)

We then apply the volume rendering [Kajiya and Von Herzen
1984] technique to render the UV feature volume into a 2D feature
ğ‘ğ‘–
map. We sample ğ‘ğ‘– points {xğ‘– }
along the camera ray r between
ğ‘–=1
near and far bounds based on the posed SMPL model in 3D space.
The feature at the pixel can be calculated as:

F (r) =

ğ‘ğ‘–
âˆ‘ï¸

ğ‘–=1

ğ‘‡ğ‘– (1 âˆ’ exp(âˆ’ğœ (xğ‘– ) ğ›¿ğ‘– )) ğ‘“ (xi, z, ğœƒ ) ,

ğ‘–âˆ’1
âˆ‘ï¸

where ğ‘‡ğ‘– = exp (cid:169)
(cid:173)
(cid:171)
where ğ›¿ğ‘– = âˆ¥xğ‘–+1 âˆ’ xğ‘– âˆ¥2 is the distance between adjacent sampled
points. An MLP ğ‘€ğ‘¢ğ‘£ is used to individually decode all the pixels

ğœ (cid:0)xğ‘— (cid:1) ğ›¿ ğ‘— (cid:170)
(cid:174)
(cid:172)

ğ‘—=1

âˆ’

,

(2)

Neural Texture Stack (NTS)Volume GeneratorUV VolumesTexture GeneratorTexture StackUVDensePoseUVRGBFeature MapRGBGTSIGGRAPH Conference Proceedings, Dec 2022, Daegu

Yue Chen, Xuan Wang, Xingyu Chen, Qi Zhang, Xiaoyu Li, Yu Guo, Jue Wang, and Fei Wang

Fig. 4. The novel view synthesis results of our model on different dynamic humans.

in the yielded view-invariant feature image to their corresponding
texture coordinates to generate the UV image. In specific, the texture
coordinates can be represented as:
(cid:16) ^P (r), ^U (r), ^V (r)

= ğ‘€ğ‘¢ğ‘£ (F (r)) ,

(3)

(cid:17)

where ^P and ^U, ^V respectively are the corresponding part assign-
ments and UV coordinates.

3.2 Neural Texture Stack
Given the generated UV image, we employ the continuous texture
stack encoded in the implicit neural representation to recover the
color image. To extract the local relation of neural texture stack
with respect to the human pose, we use a CNN texture generator ğº
to produce the pose-dependent NTS:

(4)

Eğ‘˜ = ğº (ğœƒ, k) ,
where we subdivide the body surface into ğ‘ğ‘˜ = 24 parts, and k is a
one-hot label vector representing the ğ‘˜-th body part. At a foreground
pixel, the part assignments ^P predicted from UV volumes (referred
in Equation (3)) can be interpreted as the probability of the pixel
belonging to the ğ‘˜-th body part, which is defined as (cid:205)ğ‘ğ‘˜
^Pğ‘˜ (r) =
ğ‘˜=1
1. For each human body part ğ‘˜, the texture generator generates
corresponding neural texture stack Eğ‘˜ . We forward propagate the
generator network ğº once to predict all the 24 neural textures
with a batch size of 24. Let ^Uğ‘˜ and ^Vğ‘˜ denote the predicted UV
coordinates of the ğ‘˜-th body part. We sample the texture embeddings
at non-integer locations ( ^Uğ‘˜(r), ^Vğ‘˜(r)) in a piecewise-differentiable
manner using bilinear interpolation [Jaderberg et al. 2015]:

eğ‘˜ (r) = Eğ‘˜

(cid:104) ^Uğ‘˜ (r), ^Vğ‘˜ (r)

(cid:105)

.

(5)

To model high-frequency color of human performances, we apply
the positional encoding ğ›¾ (Â·) [Rahaman et al. 2019] to UV coordi-
nates and viewing direction, and pass the encoded UV map with
the sampled texture embedding into an MLP ğ‘€ğ‘ to decode the
view-dependent color ^Cğ‘˜ (r) of camera ray r at the desired viewing
direction d:

^Cğ‘˜ (r) = ğ‘€ğ‘

(cid:16)
ğ›¾ ( ^Uğ‘˜ (r), ^Vğ‘˜ (r)), eğ‘˜ (r), k, ğ›¾ (d)

(cid:17)

.

(6)

Following that, the color ^C(r) at each pixel is reconstructed via a
weighted combination of decoded colors at ğ‘ğ‘˜ body parts, where

4

Fig. 5. Given noisy UV and semantic labels (e.g., the red circles), we can
reconstruct correct UV volumes (e.g., the blue circles) under the intrinsic
multi-view constraint of RGB loss (e.g., the green circles).
the weights are prescribed by part assignments ^Pğ‘˜ :

^C(r) =

ğ‘ğ‘˜
âˆ‘ï¸

ğ‘˜=1

^Pğ‘˜ (r) ^Cğ‘˜ (r).

(7)

3.3 Training
Collecting the results of all rays { ^C(r)}ğ» Ã—ğ‘Š , we denote the full
rendered image as ^I âˆˆ Rğ» Ã—ğ‘Š Ã—3. To learn the parameters of our
model, we optimize the L2 loss between the rendered ^I and observed
images I:

Lrgb =

(cid:13)
^I âˆ’ I
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)

2

2

.

(8)

In contrast to [Mildenhall et al. 2020; Peng et al. 2021b], we can
render an entire image requiring a single pass through our model.
Thus, we also compare the rendered images against the ground-
truth using perceptual loss [Gatys et al. 2016; Johnson et al. 2016;
Ulyanov et al. 2016], which extracts feature maps by pretrained
fixed VGG network ğœ“ (Â·) [Simonyan and Zisserman 2014] from both
images and minimizes the L1-norm between them:

Lvgg =

(cid:13)
ğœ“ ( ^I) âˆ’ ğœ“ (I)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)1

.

(9)

DensePoseOursDensePoseOursGround-truthOurs(a) UV images(b) Semantic labels(c) RGB imagesUV Volumes for Real-time Rendering of Editable Free-view Human Performance

SIGGRAPH Conference Proceedings, Dec 2022, Daegu

Datasets

CMU
(960Ã—540)

ZJU
(512Ã—512)

H36M
(500Ã—500)

p1
p2
p3
313
377
386
s9p
s11p
s1p

DN
30.04
25.56
27.04
29.67
27.13
30.29
21.53
21.27
18.91

NB
29.78
25.68
27.12
28.82
28.12
30.12
25.11
24.39
23.24

PSNRâ†‘
AN w/o Lğ‘ Ours
30.38
30.09
27.12
28.78
28.51
26.13
29.38
29.36
24.20
29.11
28.44
27.50
26.28
26.18
25.71
28.48
28.38
28.51
26.19
26.03
26.08
25.82
25.20
25.21
23.98
23.83
23.43

DN
0.968
0.939
0.955
0.958
0.933
0.938
0.824
0.828
0.781

NB
0.962
0.942
0.956
0.952
0.949
0.939
0.912
0.899
0.909

SSIMâ†‘
AN w/o Lğ‘ Ours
0.966
0.963
0.936
0.953
0.952
0.903
0.962
0.962
0.874
0.958
0.956
0.939
0.930
0.931
0.923
0.916
0.919
0.915
0.916
0.915
0.917
0.911
0.905
0.906
0.911
0.911
0.901

DN
0.088
0.137
0.154
0.084
0.112
0.122
0.242
0.313
0.332

NB
0.099
0.139
0.142
0.088
0.088
0.112
0.136
0.193
0.149

FPSâ†‘

LPIPSâ†“
AN w/o Lğ‘ Ours DN NB AN Ours
44.76
0.135
37.30
0.204
34.60
0.259
51.39
0.124
38.70
0.152
35.88
0.163
40.00
0.139
33.41
0.174
41.43
0.162

0.055
0.062
0.062
0.068
0.094
0.103
0.085
0.118
0.094

0.036
0.044
0.047
0.053
0.085
0.078
0.084
0.111
0.093

0.76
1.28
1.28
1.51
2.02
4.89
2.19
1.02
0.97

1.01
1.45
2.12
2.07
2.41
3.00
1.06
1.18
1.38

0.21
0.34
0.33
0.62
0.76
0.91
0.30
0.67
0.50

Table 1. Quantitative results of novel view synthesis. We achieve competitive PSNR and SSIM while outperforming the others on LPIPS and FPS.

Fig. 6. Qualitative results of novel view synthesis on CMU Panoptic, ZJU
Mocap. It is illustrated that our method produces the high-fidelity novel syn-
thesis results. All the baseline methods suffer from blurry textures, especially
letters and wrinkles.

To warm-start the UV volumes and regularize its solution space,
we leverage the pre-trained DensePose model. In particular, we
perform the DensePose on the training data to produce the Denspose
UV image as pseudo supervision, to regularize the UV volumes under
semantic loss Lp and UV-metric loss Luv between the DensePose
outputs and our UV predictions:

Lp =

Luv =

ğ‘ğ‘˜
âˆ‘ï¸

ğ‘˜=1
ğ‘ğ‘˜
âˆ‘ï¸

ğ‘˜=1

Pğ‘˜ log( ^Pğ‘˜ ),

Pğ‘˜

(cid:18)(cid:13)
^Uğ‘˜ âˆ’ Uğ‘˜
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)

2

2

+

(cid:13)
(cid:13)
(cid:13)

^Vğ‘˜ âˆ’ Vğ‘˜

(cid:13)
(cid:13)
(cid:13)

(cid:19)

2

2

,

(10)

where ğ‘ğ‘˜ is the number of body parts, and Pğ‘˜ and ^Pğ‘˜ are respec-
tively the multi-class semantic probability at the ğ‘˜-th part of Dense-
Pose outputs and UV volumes predictions. Similarly, Uğ‘˜ ,Vğ‘˜ and
^Uğ‘˜ , ^Vğ‘˜ are the DensePose and UV volumes predicted UV coordi-
nates at the ğ‘˜-th part. Lp is chosen as a multi-class cross-entropy
loss to encourage rendered part labels to be consistent with pro-
vided DensePose labels, and Luv promotes to generate inter-frame
consistent UV coordinates.

Fig. 7. Qualitative results of novel pose generalization on CMU Panoptic,
ZJU Mocap. Benefiting from the UV volumes and NTS that give our models
better generalization ability, our method performs better on novel poses,
especially for preserving sharp image details.

We present the UV images predicted by our UV volumes and the
pseudo supervision of DensePose in Figure 5. Given noisy semantic
and UV labels (e.g., the blue circles), we can reconstruct correct
UV volumes (e.g., the red circles) under the intrinsic multi-view
constraint of RGB loss (e.g., the green circles). As shown in the
second row of Figure 5, it can be observed that UV volumes success-
fully recovered the UV images even though the provided DensePose
supervision is incorrect.

Given the binary human mask S for the observed image I, we
propose a silhouette loss to facilitate UV volumes modeling a more
fine-grained geometry:

ğ‘ğ‘– âˆ’1
âˆ‘ï¸

âˆ’

ğ‘‡ (r) = exp (cid:169)
(cid:173)
(cid:171)
(S(r)(1 âˆ’ ğ‘‡ (r)) + (1 âˆ’ S(r))ğ‘‡ (r)) ,

ğœ(xğ‘— )ğ›¿ ğ‘— (cid:170)
(cid:174)
(cid:172)

Ls =

âˆ‘ï¸

ğ‘—=1

,

(11)

râˆˆR

5

CMU-171026p2s1ZJU-313Ground-truthOursDyNeRFNeuralBodyAnimatable-NeRFCMU-171204p4s6Ground-truthNeuralBodyAnimatable-NeRFOursCMU-171026p2s1ZJU-313CMU-171204p4s6SIGGRAPH Conference Proceedings, Dec 2022, Daegu

Yue Chen, Xuan Wang, Xingyu Chen, Qi Zhang, Xiaoyu Li, Yu Guo, Jue Wang, and Fei Wang

Methods

NB
AN
Ours

CMU(960Ã—540)
SSIMâ†‘
0.918
0.883
0.927

LPIPSâ†“
0.146
0.208
0.073

PSNRâ†‘
25.94
23.65
26.20

ZJU(512Ã—512)
SSIMâ†‘
0.918
0.911
0.910

LPIPSâ†“
0.120
0.153
0.104

PSNRâ†‘
24.51
24.55
23.69

H36M(500Ã—500)
SSIMâ†‘
0.884
0.873
0.874

LPIPSâ†“
0.170
0.170
0.141

PSNRâ†‘
25.54
25.00
25.04

Table 2. Quantitative results of novel pose generalization. We achieve com-
petitive PSNR and SSIM while outperforming the others on LPIPS.

Fig. 8. Qualitative results of reshaping. By changing the SMPL parameters
ğ›½, we can conveniently make the human performer fatter, slimmer, or larger.
In each pair of results, the result of NeuralBody is shown on the left, and our
result is on the right. Obviously, more details and consistency are preserved
in our results in varying shapes.
where ğ‘‡ (r) is accumulated transmittance. The value of mask S(r)
in the foreground is zero, and the background is one.

We combine the aforementioned losses and jointly train our model

to optimize the full objective:

L = Lrgb + ğœ†vggLvgg + ğœ†pLp + ğœ†uvLuv + ğœ†sLs.

(12)

4 EXPERIMENTS
To demonstrate the effectiveness and efficiency of our method, we
perform extensive experiments. We report quantitative results using
four standard metrics: PSNR, SSIM, LPIPS, and FPS. And the quali-
tative experiments illustrate our method produces photo-realistic
images in different tasks, e.g., novel view synthesis, reposing, re-
shaping, and retexturing.
Dataset. We perform experiments on several types of datasets
which consist of calibrated and synchronized multi-view videos.
We use 26 and 20 training views on CMU Panoptic dataset [Joo et al.
2017] with 960 Ã— 540 resolution and ZJU Mocap dataset [Peng et al.
2021b] with 512 Ã— 512 resolution, respectively. The most challeng-
ing one is the H36M dataset [Ionescu et al. 2013] with 500 Ã— 500
resolution, where only three cameras are available for training. The
evaluation is done on the hold-out cameras (novel views) or hold-out
segments of the sequence (novel poses).
Baselines. To validate our method, we compare it against several
state-of-the-art free-view video synthesis techniques: 1) DN: DyN-
eRF [Li et al. 2021b], which takes time-varying latent codes as the
conditions for dynamic scenes; and 2) NB: NeuralBody [Peng et al.
2021b], which takes as input the posed human model with structured
time-invariant latent codes and generates a pose-conditioned neural
radiance field; 3) AN: Animatable-NeRF, which uses neural blend
weight fields to generate correspondences between observation and
canonical space.
Novel View Synthesis. For comparison, we synthesize images of
training poses in hold-out test views. Table 1 shows the compari-
son of our method against baselines, which demonstrates that our
method performs best LPIPS and FPS among all methods. Specifi-
cally, we achieve rendering the free-view videos of human perfor-
mances in 30FPS with the help of UV volumes. Note that LIPIS agrees

6

Fig. 9. Qualitative results of retexturing. The presented framework allows
us to edit the texture by drawing patterns on the NTS conveniently. The
rich texture patterns are well preserved and transferred to correct semantic
areas in different poses, which demonstrates that UV Volumes not only
successfully change the texture under the edited frame but also can be
transferred to a novel one with the modeled dynamics.

surprisingly well with human visual perception, which indicates
that our synthesis is more visually similar to ground-truth.

Figure 6 presents the qualitative comparison of our method with
baselines. Baselines fail to preserve the sharp image details, whose
rendering is blurry and even split. In contrast, our method can
accurately capture the high-frequency details like letters, numbers
and wrinkles on shirts and the belt on pants benefiting from our NTS
model. Furthermore, we show the view synthesis results of dynamic
humans in Figure 4, which indicate that our method generates high-
quality appearance results even with rich textures and challenging
motions. Note the rightmost example is from the H36M dataset with
only 4 views. Please refer to the supplementary material and video
for more results.
Reposing. We perform reposing on the human performer with
novel motions. As DyNeRF is not designed for editing tasks, we
compare our method against NeuralBody and Animatable-NeRF. As
shown in Table 2, quantitative results demonstrate that our method
achieves competitive PSNR and SSIM while outperforming others
on LPIPS.

The qualitative results are shown in Figure 7. For novel human
poses, NeuralBody gives blurry and distorted rendering results while
Animatable-NeRF even produces split humans due to a highly under-
constrained backward warp field from observation to canonical
space. In contrast, synthesized images of our method achieve better
visual quality with reasonable high-definition dynamic textures.
The results indicate that using smooth UV volumes in 3D and en-
coding texture in 2D has better controllability on the novel pose
generalization than directly modeling a pose-conditioned neural
radiance field.
Reshaping. We demonstrate that our approach can edit the shape
of reconstructed human performance by changing the shape param-
eters of the SMPL model. We report the qualitative results in Figure
1 and Figure 8. NeuralBody fails to infer the reasonable changes of
the cloth, while our method generalizes well on novel shapes.

OriginalFatSlimLargeNBOursNBOursNBOursNBOursUV Volumes for Real-time Rendering of Editable Free-view Human Performance

SIGGRAPH Conference Proceedings, Dec 2022, Daegu

Novel Pose Generation

Novel View Synthesis

Method

Sparse CNN

Ours

48.78

Density

7.08

52.04

84.38

NB

Color Model
UV NTS RGB
7.52
1.60
1.53
9.12
19.46

68.23

546.81

663.84

715.88

Rendering

1.73

32.65

Fig. 10. Given any arbitrary artistic style or cloth appearance, we can render
(a) a 3D dynamic human with the transferred texture or perform (b) a 3D
virtual try-on in real-time.

No Warm-start Loss 30.37
30.09
No Perceptual Loss
17.47
No Silhouette Loss
30.38
Complete Model

Novel View Synthesis Novel Pose Generation
PSNRâ†‘ SSIMâ†‘ LPIPSâ†“ PSNRâ†‘ SSIMâ†‘ LPIPSâ†“
0.076
0.079
0.218
0.073

0.060
0.055
0.207
0.036

0.964
0.963
0.874
0.966

0.917
0.919
0.860
0.927

26.14
26.05
16.95
26.20

Table 3. Results of models under different training settings.

Retexturing. With the learned dense correspondence of UV vol-
umes and neural texture, we can edit the 3D cloth with a user-
provided 2D texture, as shown in Figure 9. Visually inspected, the
rich texture patterns are well preserved and transferred to correct
semantic areas in different poses. Moreover, our model supports
changing texturesâ€™ style and appearance, which are presented in
Figure 10. Thanks to the style transfer network [Ghiasi et al. 2017],
we can perform arbitrary artistic stylizations on 3D human perfor-
mance. Given any fabric texture, we can even dress the performer in
various appearances, which enables 3D virtual try-on in real-time.

4.1 Ablation Studies
We conduct ablation studies on performer p1 of the CMU dataset.
As shown in Table 3, we analyze the effects of different losses for
the proposed approach by removing warm-start loss, perceptual
loss and silhouette loss, respectively. Then, we analyze the time
consumption of each module. We encourage the reader to see the
supplement for additional ablations, discussion of model design, and
other experimental results.
Impact of warm-start loss. We present using semantic and UV-
metric loss to warm-start the UV volumes and constrain its solution
space. To prove the effectiveness of this process, we train an ablation
(No Warm-start Loss) built upon our full model by eliminating the
warm-start loss. It gives lower performance in all metrics, especially
the LPIPS increased a lot when rendering novel views. This com-
parison indicates that the warm-start loss yields better information
reuse of different frames by transforming the observation XYZ co-
ordinates to canonical UV coordinates defined by the consistent
semantic and UV-metric loss.

7

Table 4. Time analysis of each module in milliseconds(ms).

Impact of perceptual loss. Thanks to the UV volumes, we can
render an entire image by a single pass during training, allowing us
to use perceptual loss. Using the same model but training without
the perceptual loss (No Perceptual Loss) gives a lower performance
in all metrics, especially the PSNR and LPIPS. This comparison
illustrates that perceptual loss can improve the visual quality of
synthesized images.
Impact of silhouette loss. To facilitate the UV volumes modeling
a more fine-grained geometry, we employ a silhouette loss by using
the 2D binary mask of the human performer. We present an ablation
(No Silhouette Loss) built upon our full model by eliminating the
silhouette loss, as shown in Table 3. It is obvious that No Silhouette
Loss gives the worst performance in all metrics among all ablations.
This comparison demonstrates that our geometry does benefit from
the silhouette loss, which can be seen in the supplement to get an
intuitive visual experience.

4.2 Time Consumption
We analyze the time consumption of each module in our framework
and the corresponding module in NeuralBody [Peng et al. 2021b]
on ZJU Mocap performer 313, as shown in Table 4. On average,
it takes 48.78 ms for us to obtain the UV volumes from the posed
human model. Then, our method takes only 19.46 ms (51FPS) to
access the free-view renderings, which benefits from the smooth
UV volumes that allow the much smaller and shallower to obtain
densities and texture coordinates in 3D while capturing detailed
appearance in 2D NTS. On the contrary, NeuralBody spends 663.84
ms (1.5FPS) to synthesize novel views, which prevents it from being
used in applications that require running in real-time. Even on
the novel pose generalization task, our method can reach 68.23
milliseconds per frame (14FPS) as well. All experiments are run on
a single NVIDIA A100 GPU.

5 CONCLUSIONS
We have presented the UV volumes for free-view video synthesis
of a human performer. It is the first method to generate a real-time
free-view video with editing ability. The key idea is to employ the
smooth UV volumes and highly-detailed textures in an implicit
neural texture stack. Extensive experiments have demonstrated
both the effectiveness and efficiency of our method. In addition to
improving efficiency, our approach can also support editing, e.g.,
reposing, reshaping, or retexturing the human performer in the
free-view videos.

(a) Texture transfer(b) Try-onTextureStyleTransferred textureTextureAppearanceNew textureSIGGRAPH Conference Proceedings, Dec 2022, Daegu

Yue Chen, Xuan Wang, Xingyu Chen, Qi Zhang, Xiaoyu Li, Yu Guo, Jue Wang, and Fei Wang

REFERENCES
Xu Chen, Yufeng Zheng, Michael J Black, Otmar Hilliges, and Andreas Geiger. 2021.
SNARF: Differentiable Forward Skinning for Animating Non-Rigid Neural Implicit
Shapes. In International Conference on Computer Vision (ICCV).

Alvaro Collet, Ming Chuang, Pat Sweeney, Don Gillett, Dennis Evseev, David Calabrese,
Hugues Hoppe, Adam Kirk, and Steve Sullivan. 2015a. High-quality streamable
free-viewpoint video. ACM Transactions on Graphics (ToG) 34, 4 (2015), 1â€“13.
Alvaro Collet, Ming Chuang, Pat Sweeney, Don Gillett, Dennis Evseev, David Calabrese,
Hugues Hoppe, Adam Kirk, and Steve Sullivan. 2015b. High-quality streamable
free-viewpoint video. ACM Transactions on Graphics (ToG) 34, 4 (2015), 1â€“13.
Abe Davis, Marc Levoy, and Fredo Durand. 2012. Unstructured light fields. In Computer

Graphics Forum, Vol. 31. Wiley Online Library, 305â€“314.

John Flynn, Michael Broxton, Paul Debevec, Matthew DuVall, Graham Fyffe, Ryan
Overbeck, Noah Snavely, and Richard Tucker. 2019. Deepview: View synthesis with
learned gradient descent. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition. 2367â€“2376.

Stephan J Garbin, Marek Kowalski, Matthew Johnson, Jamie Shotton, and Julien
Valentin. 2021. Fastnerf: High-fidelity neural rendering at 200fps. arXiv preprint
arXiv:2103.10380 (2021).

Leon A Gatys, Alexander S Ecker, and Matthias Bethge. 2016. Image style transfer using
convolutional neural networks. In Proceedings of the IEEE conference on computer
vision and pattern recognition. 2414â€“2423.

Golnaz Ghiasi, Honglak Lee, Manjunath Kudlur, Vincent Dumoulin, and Jonathon
Shlens. 2017. Exploring the structure of a real-time, arbitrary neural artistic styliza-
tion network. arXiv preprint arXiv:1705.06830 (2017).

Steven J Gortler, Radek Grzeszczuk, Richard Szeliski, and Michael F Cohen. 1996. The
lumigraph. In Proceedings of the 23rd annual conference on Computer graphics and
interactive techniques. 43â€“54.

RÄ±za Alp GÃ¼ler, Natalia Neverova, and Iasonas Kokkinos. 2018. Densepose: Dense
human pose estimation in the wild. In Proceedings of the IEEE conference on computer
vision and pattern recognition. 7297â€“7306.

Catalin Ionescu, Dragos Papava, Vlad Olaru, and Cristian Sminchisescu. 2013. Human3.
6m: Large scale datasets and predictive methods for 3d human sensing in natural
environments. IEEE transactions on pattern analysis and machine intelligence 36, 7
(2013), 1325â€“1339.

Max Jaderberg, Karen Simonyan, Andrew Zisserman, et al. 2015. Spatial transformer
networks. Advances in neural information processing systems 28 (2015), 2017â€“2025.
Justin Johnson, Alexandre Alahi, and Li Fei-Fei. 2016. Perceptual losses for real-time
style transfer and super-resolution. In European conference on computer vision.
Springer, 694â€“711.

Hanbyul Joo, Tomas Simon, Xulong Li, Hao Liu, Lei Tan, Lin Gui, Sean Banerjee,
Timothy Godisart, Bart Nabbe, Iain Matthews, et al. 2017. Panoptic studio: A
massively multiview system for social interaction capture. IEEE transactions on
pattern analysis and machine intelligence 41, 1 (2017), 190â€“204.

James T Kajiya and Brian P Von Herzen. 1984. Ray tracing volume densities. ACM

SIGGRAPH computer graphics 18, 3 (1984), 165â€“174.

Nima Khademi Kalantari, Ting-Chun Wang, and Ravi Ramamoorthi. 2016. Learning-
based view synthesis for light field cameras. ACM Transactions on Graphics (TOG)
35, 6 (2016), 1â€“10.

Marc Levoy and Pat Hanrahan. 1996. Light field rendering. In Proceedings of the 23rd

annual conference on Computer graphics and interactive techniques. 31â€“42.

Tianye Li, Mira Slavcheva, Michael Zollhoefer, Simon Green, Christoph Lassner, Changil
Kim, Tanner Schmidt, Steven Lovegrove, Michael Goesele, and Zhaoyang Lv. 2021b.
Neural 3d video synthesis. arXiv preprint arXiv:2103.02597 (2021).

Zhengqi Li, Simon Niklaus, Noah Snavely, and Oliver Wang. 2021a. Neural scene
flow fields for space-time view synthesis of dynamic scenes. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition. 6498â€“6508.
Lingjie Liu, Jiatao Gu, Kyaw Zaw Lin, Tat-Seng Chua, and Christian Theobalt. 2020.

Neural sparse voxel fields. arXiv preprint arXiv:2007.11571 (2020).

Lingjie Liu, Marc Habermann, Viktor Rudnev, Kripasindhu Sarkar, Jiatao Gu, and
Christian Theobalt. 2021. Neural actor: Neural free-view synthesis of human actors
with pose control. ACM Transactions on Graphics (TOG) 40, 6 (2021), 1â€“16.

Stephen Lombardi, Tomas Simon, Jason Saragih, Gabriel Schwartz, Andreas Lehrmann,
and Yaser Sheikh. 2019. Neural volumes: Learning dynamic renderable volumes
from images. arXiv preprint arXiv:1906.07751 (2019).

Stephen Lombardi, Tomas Simon, Gabriel Schwartz, Michael Zollhoefer, Yaser Sheikh,
and Jason Saragih. 2021. Mixture of volumetric primitives for efficient neural
rendering. ACM Transactions on Graphics (TOG) 40, 4 (2021), 1â€“13.

Ben Mildenhall, Pratul P Srinivasan, Rodrigo Ortiz-Cayon, Nima Khademi Kalantari,
Ravi Ramamoorthi, Ren Ng, and Abhishek Kar. 2019. Local light field fusion: Practical
view synthesis with prescriptive sampling guidelines. ACM Transactions on Graphics
(TOG) 38, 4 (2019), 1â€“14.

Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ra-
mamoorthi, and Ren Ng. 2020. Nerf: Representing scenes as neural radiance fields
for view synthesis. In European conference on computer vision. Springer, 405â€“421.

Armin Mustafa, Hansung Kim, Jean-Yves Guillemaut, and Adrian Hilton. 2016. Tempo-
rally coherent 4d reconstruction of complex dynamic scenes. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition. 4660â€“4669.

Keunhong Park, Utkarsh Sinha, Jonathan T Barron, Sofien Bouaziz, Dan B Goldman,
Steven M Seitz, and Ricardo Martin-Brualla. 2021a. Nerfies: Deformable neural
radiance fields. In Proceedings of the IEEE/CVF International Conference on Computer
Vision. 5865â€“5874.

Keunhong Park, Utkarsh Sinha, Peter Hedman, Jonathan T Barron, Sofien Bouaziz,
Dan B Goldman, Ricardo Martin-Brualla, and Steven M Seitz. 2021b. Hypernerf: A
higher-dimensional representation for topologically varying neural radiance fields.
arXiv preprint arXiv:2106.13228 (2021).

Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei
Zhou, and Hujun Bao. 2021a. Animatable Neural Radiance Fields for Modeling
Dynamic Human Bodies. In ICCV.

Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and
Xiaowei Zhou. 2021b. Neural body: Implicit neural representations with structured
latent codes for novel view synthesis of dynamic humans. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition. 9054â€“9063.
Albert Pumarola, Enric Corona, Gerard Pons-Moll, and Francesc Moreno-Noguer. 2021.
D-nerf: Neural radiance fields for dynamic scenes. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition. 10318â€“10327.

Nasim Rahaman, Aristide Baratin, Devansh Arpit, Felix Draxler, Min Lin, Fred Ham-
precht, Yoshua Bengio, and Aaron Courville. 2019. On the spectral bias of neural
networks. In International Conference on Machine Learning. PMLR, 5301â€“5310.
Christian Reiser, Songyou Peng, Yiyi Liao, and Andreas Geiger. 2021. KiloNeRF: Speed-
ing up Neural Radiance Fields with Thousands of Tiny MLPs. arXiv preprint
arXiv:2103.13744 (2021).

Ruizhi Shao, Hongwen Zhang, He Zhang, Yanpei Cao, Tao Yu, and Yebin Liu. 2021.
Doublefield: Bridging the neural surface and radiance fields for high-fidelity human
rendering. arXiv preprint arXiv:2106.03798 (2021).

Karen Simonyan and Andrew Zisserman. 2014. Very deep convolutional networks for

large-scale image recognition. arXiv preprint arXiv:1409.1556 (2014).

Pratul P Srinivasan, Richard Tucker, Jonathan T Barron, Ravi Ramamoorthi, Ren Ng, and
Noah Snavely. 2019. Pushing the boundaries of view extrapolation with multiplane
images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition. 175â€“184.

Justus Thies, Michael ZollhÃ¶fer, and Matthias NieÃŸner. 2019. Deferred neural rendering:
Image synthesis using neural textures. ACM Transactions on Graphics (TOG) 38, 4
(2019), 1â€“12.

Edgar Tretschk, Ayush Tewari, Vladislav Golyanik, Michael ZollhÃ¶fer, Christoph Lass-
ner, and Christian Theobalt. 2021. Non-rigid neural radiance fields: Reconstruction
and novel view synthesis of a dynamic scene from monocular video. In Proceedings
of the IEEE/CVF International Conference on Computer Vision. 12959â€“12970.

Dmitry Ulyanov, Vadim Lebedev, Andrea Vedaldi, and Victor S Lempitsky. 2016. Texture
networks: Feed-forward synthesis of textures and stylized images.. In ICML, Vol. 1.
4.

Chung-Yi Weng, Brian Curless, Pratul P Srinivasan, Jonathan T Barron, and Ira
Kemelmacher-Shlizerman. 2022. HumanNeRF: Free-viewpoint Rendering of Moving
People from Monocular Video. arXiv preprint arXiv:2201.04127 (2022).

Minye Wu, Yuehao Wang, Qiang Hu, and Jingyi Yu. 2020. Multi-view neural human
rendering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition. 1682â€“1691.

Hongyi Xu, Thiemo Alldieck, and Cristian Sminchisescu. 2021. H-nerf: Neural radiance
fields for rendering and temporal reconstruction of humans in motion. Advances in
Neural Information Processing Systems 34 (2021).

Alex Yu, Sara Fridovich-Keil, Matthew Tancik, Qinhong Chen, Benjamin Recht, and
Angjoo Kanazawa. 2021a. Plenoxels: Radiance Fields without Neural Networks.
arXiv preprint arXiv:2112.05131 (2021).

Alex Yu, Ruilong Li, Matthew Tancik, Hao Li, Ren Ng, and Angjoo Kanazawa. 2021b.
arXiv preprint

Plenoctrees for real-time rendering of neural radiance fields.
arXiv:2103.14024 (2021).

Tao Yu, Zerong Zheng, Kaiwen Guo, Pengpeng Liu, Qionghai Dai, and Yebin Liu. 2021c.
Function4d: Real-time human volumetric capture from very sparse consumer rgbd
sensors. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition. 5746â€“5756.

Jiakai Zhang, Xinhang Liu, Xinyi Ye, Fuqiang Zhao, Yanshun Zhang, Minye Wu,
Yingliang Zhang, Lan Xu, and Jingyi Yu. 2021. Editable free-viewpoint video using
a layered neural representation. ACM Transactions on Graphics (TOG) 40, 4 (2021),
1â€“18.

8

