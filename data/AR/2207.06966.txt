2
2
0
2

l
u
J

4
1

]

V
C
.
s
c
[

1
v
6
6
9
6
0
.
7
0
2
2
:
v
i
X
r
a

Scene Text Recognition with
Permuted Autoregressive Sequence Models

Darwin Bautista

and Rowel Atienza

Electrical and Electronics Engineering Institute,
University of the Philippines, Diliman
{darwin.bautista,rowel}@eee.upd.edu.ph

Abstract. Context-aware STR methods typically use internal autore-
gressive (AR) language models (LM). Inherent limitations of AR mod-
els motivated two-stage methods which employ an external LM. The
conditional independence of the external LM on the input image may
cause it to erroneously rectify correct predictions, leading to signifi-
cant inefficiencies. Our method, PARSeq, learns an ensemble of internal
AR LMs with shared weights using Permutation Language Modeling.
It unifies context-free non-AR and context-aware AR inference, and it-
erative refinement using bidirectional context. Using synthetic training
data, PARSeq achieves state-of-the-art (SOTA) results in STR bench-
marks (91.9% accuracy) and more challenging datasets. It establishes
new SOTA results (96.0% accuracy) when trained on real data. PARSeq
is optimal on accuracy vs parameter count, FLOPS, and latency because
of its simple, unified structure and parallel token processing. Due to its
extensive use of attention, it is robust on arbitrarily-oriented text which
is common in real-world images. Code, pretrained weights, and data are
available at: https://github.com/baudm/parseq.

Keywords: scene text recognition, permutation language modeling, au-
toregressive modeling, cross-modal attention, transformer

1

Introduction

Machines read text in natural scenes by first detecting text regions, then recog-
nizing text in those regions. The task of recognizing text from the cropped regions
is called Scene Text Recognition (STR). STR enables reading of road signs, bill-
boards, paper bills, product labels, logos, printed shirts, etc. It has practical
applications in self-driving cars, augmented reality, retail, education, and de-
vices for the visually-impaired, among others. In contrast to Optical Character
Recognition (OCR) in documents where the text attributes are more uniform,
STR has to deal with varying font styles, orientations, text shapes, illumination,
amount of occlusion, and inconsistent sensor conditions. Images captured in nat-
ural environments could also be noisy, blurry, or distorted. In essence, STR is
an important but very challenging problem.

STR is mainly a vision task, but in cases where parts of the text are impossi-
ble to read, e.g. due to an occluder, the image features alone will not be enough

 
 
 
 
 
 
2

D. Bautista, R. Atienza

to make accurate inferences. In such cases, language semantics is typically used
to aid the recognition process. Context-aware STR methods incorporate seman-
tic priors from a word representation model [56] or dictionary [53], or learned
from data [60,37,3,58,38,80,24,61,10] using sequence modeling [6,69].

Sequence modeling has the advantage of learning end-to-end trainable lan-
guage models (LM). STR methods with internal LMs jointly process image
features and language context. They are trained by enforcing an autoregres-
sive (AR) constraint on the language context where future tokens are condi-
tioned on past tokens but not the other way around, resulting in the model
P (y|x) = (cid:81)T
t=1 P (yt|y<t, x) where y is the T -length text label of the image x.
AR models have two inherent limitations arising from this constraint. First, the
model is able to learn the token dependencies in one direction only—usually the
left-to-right (LTR) direction. This unidirectionality causes AR models to be bi-
ased towards a single reading direction, resulting in spurious addition of suffixes
or direction-dependent predictions (illustrated in Appendix A). Second, during
inference, the AR model can only be used to output tokens serially in the same
direction used for training. This is called next-token or monotonic AR decoding.

(a) ABINet

(b) Unified STR model (Ours)

Fig. 1. (a) State-of-the-art method ABINet [24] uses a combination of context-free
vision and context-aware language models. The language model functions as a spell
checker but is prone to erroneous rectification of correct initial predictions due to its
conditional independence on the image features. (b) Our proposed method performs
both initial decoding and iterative refinement by jointly processing image and con-
text features, resulting in a single holistic output. This eschews the need for separate
language and fusion models resulting in a more efficient and robust STR method

To address these limitations, prior works have combined left-to-right and
right-to-left (RTL) AR models [61,10], or opted for a two-stage approach using
an ensemble of a context-free STR model with a standalone or external LM
[80,24]. A combined LTR and RTL AR model still suffers from unidirectional
context but works around it by performing two separate decoding streams—
one for each direction—then choosing the prediction with the higher likelihood.
Naturally, this results in increased decoding time and complexity. Meanwhile,
two-stage ensemble approaches like in Figure 1a obtain their initial predictions
using parallel non-AR decoding. The initial context-less prediction is decoded
directly from the image using the context-free model P (y|x) = (cid:81)T
t=1 P (yt|x).
This enables the external LM, P (y) = (cid:81)T
t=1 P (yt|y̸=t) in ABINet [24] for exam-
ple, to use bidirectional context since all characters are available at once. The
LM functions as a spell checker and rectifies the initial prediction, producing a

Scene Text Recognition with Permuted Autoregressive Sequence Models

3

context-based output. The conditional independence of the LM from the input
image may cause it to erroneously rectify correct predictions if they appear mis-
spelled, or if a similar word with a higher likelihood exists. This is evident in
the low word accuracy of the LM in SRN (27.6%) and in ABINet (41.9%) when
used as a spell checker [24]. Hence, a separate fusion layer is used to combine
the features from the initial prediction and the LM prediction to get the final
output. A closer look at the LM of ABINet (Appendix B) reveals that it is ineffi-
cient for STR. It is underutilized relative to its parameter count, and it exhibits
dismal word accuracy despite using a significant chunk of the overall compute
requirements of the full ABINet model.

In sequence model literature, there has been recent interest in generalized
models of sequence generation. Various neural sequence models, such as AR
and refinement-based non-AR, were shown to be special cases in the generalized
framework proposed by Mansimov et al . [47]. This result posits that the same
generalization can be done in STR models, unifying context-free and context-
aware STR. While the advantages of this unification are not apparent, we shall
show later that such a generalized model enables the use of an internal LM while
maintaining the refinement capabilities of an external LM.

Permutation Language Modeling (PLM) was originally proposed for large-
scale language pretraining [79], but recent works [66,55] have adapted it for
learning Transformer-based generalized sequence models capable of different de-
coding schemes. In this work, we adapt PLM for STR. PLM can be considered
a generalization of AR modeling, and a PLM-trained model can be seen as an
ensemble of AR models with shared architecture and weights [68]. With the
use of attention masks for dynamically specifying token dependencies, such a
model, illustrated in Figure 2, can learn and use conditional character proba-
bilities given an arbitrary subset of the input context, enabling monotonic AR
decoding, parallel non-AR decoding, and even iterative refinement.

Fig. 2. Illustration of NAR and iterative refinement (cloze) models in relation to an
ensemble of AR models for an image x with a three-element text label y. Four different
factorizations of P (y|x) (out of six possible) are shown, with each one determined by
the factorization order shown in the subscript

In summary, state-of-the-art (SOTA) STR methods [80,24] opted for a two-
stage ensemble approach in order to use bidirectional language context. The low

4

D. Bautista, R. Atienza

word accuracy of their external LMs, despite increased training and runtime
requirements, highlights the need for a more efficient approach. To this end, we
propose a permuted autoregressive sequence (PARSeq) model for STR. Trained
with PLM, PARSeq is a unified STR model with a simple structure, but is
capable of both context-free and context-aware inference, as well as iterative
refinement using bidirectional (cloze) context. PARSeq achieves SOTA results
on the STR benchmarks for both synthetic and real training data (Table 6)
across all character sets (Table 4), while being optimal in its use of parameters,
FLOPS, and runtime (Figure 5). For a more comprehensive comparison, we also
benchmark on larger and more difficult real datasets which contain occluded and
arbitrarily-oriented text (Figure 4b). PARSeq likewise achieves SOTA results in
these datasets (Table 5).

2 Related Work

The recent surveys of Long et al . [45] and Chen et al . [13] provide comprehensive
discussions on different approaches in STR. In this section, we focus on the use
of language semantics in STR.

Context-free STR methods directly predict the characters from image fea-
tures. The output characters are conditionally-independent of each other. The
most prominent approaches are CTC-based [27] methods [59,44,72,11], with a
few using different approaches such as self-attention [23] for pooling features into
character positions [2], or casting STR as a multi-instance classification problem
[30,12]. Ensemble methods [80,24] use an attention mechanism [6,69] to produce
the initial context-less predictions. Since context-free methods rely solely on the
image features for prediction, they are less robust against corruptions like oc-
cluded or incomplete characters. This limitation motivated the use of language
semantics for making the recognition model more robust.

Context-aware STR methods typically use semantics learned from data to
aid in recognition. Most approaches [3,37,60,14,61] use RNNs with attention [6]
or Transformers [58,38,10] to learn internal LMs using the standard AR training.
These methods are limited to monotonic AR decoding. Ensemble methods [80,24]
use bidirectional context via an external LM for prediction refinement. The con-
ditional independence of the external LM on image features makes it prone to
erroneous rectification, limiting usefulness while incurring significant overhead.
VisionLAN [75] learns semantics by selectively masking image features of indi-
vidual characters during training, akin to denoising autoencoders and Masked
Language Modeling (MLM) [21]. In contrast to prior work, PARSeq learns an
internal LM using PLM instead of the standard AR modeling. It supports flex-
ible decoding by using a parameterization which decouples the target decoding
position from the input context, similar to the query stream of two-stream at-
tention [79]. Unlike ABINet [24] which uses the cloze context for both training
and inference, PARSeq uses it for iterative refinement only. Moreover, as said
earlier, the refinement model of ABINet is conditionally independent of the in-

Scene Text Recognition with Permuted Autoregressive Sequence Models

5

put image, while PARSeq considers both input image and language context in
the refinement process.

Generation from Sequence Models can be categorized into two contrast-
ing schemes: autoregressive (one token at a time) and non-autoregressive (all to-
kens predicted at once). Mansimov et al . [47] proposed a generalized framework
for sequence generation which unifies the said schemes. BANG [55] adapted
two-stream attention [79] for use with MLM, in contrast to our use of PLM.
PMLM [40] is trained using a generalization of MLM where the masking ratio
is stochastic. A variant which uses a uniform prior was shown to be equivalent
to a PLM-trained model. Closest to our work is Tian et al . [66] which adapts
the two-stream attention parameterization [79] to decoders by interspersing the
content and query streams from different layers. In contrast, our decoder does
not use self-attention and does not intersperse the two streams. This allows our
single layer decoder to use the query stream only, and avoid the overhead of the
unused content stream.

3 Permuted Autoregressive Sequence Models

In this section, we first present the Transformer-based model architecture of
PARSeq. Next, we discuss how to train it using Permutation Language Modeling.
Lastly, we show how to use the trained model for inference by discussing the
different decoding schemes and the iterative refinement procedure.

3.1 Model Architecture

Multi-head Attention (MHA) [69] is extensively used by PARSeq. We denote it
as M HA(q, k, v, m), where q, k, and v refer to the required parameters query,
key, and value, while m refers to the optional attention mask. We provide the
background material on MHA in Appendix C.

PARSeq follows an encoder-decoder architecture, shown in Figure 3, com-
monly used in sequence modeling tasks. The encoder has 12 layers while the
decoder is only a single layer. This deep-shallow configuration [33] is a deliber-
ate design choice which minimizes the overall computational requirements of the
model while having a negligible impact in performance. Details in Appendix D.

ViT Encoder. Vision Transformer (ViT) [23] is the direct extension of the
Transformer to images. A ViT layer contains one MHA module used for self-
attention, i.e. q = k = v. The encoder is a 12-layer ViT without the classi-
fication head and the [CLS] token. An image x ∈ RW ×H×C, with width W ,
height H, and number of channels C, is tokenized by evenly dividing it into
pw ×ph patches, flattening each patch, then linearly projecting them into dmodel-
dimensional tokens using a patch embedding matrix Wp ∈ RpwphC×dmodel , re-
sulting in (W H)/(pwph) tokens. Learned position embeddings of equal dimen-
sion are added to the tokens prior to being processed by the first ViT layer.

6

D. Bautista, R. Atienza

Fig. 3. PARSeq architecture and training overview. LayerN orm and Dropout layers
are omitted due to space constraints. [B], [E], and [P] stand for beginning-of-sequence
(BOS), end-of-sequence (EOS), and padding tokens, respectively. T = 25 results in 26
distinct position tokens. The position tokens both serve as query vectors and position
embeddings for the input context. For [B], no position embedding is added. Attention
masks are generated from the given permutations and are used only for the context-
position attention. Lce pertains to the cross-entropy loss

In contrast to the standard ViT, all output tokens z are used as input to the
decoder:

z = Enc(x) ∈ R W H

pw ph

×dmodel

(1)

Visio-lingual Decoder. The decoder follows the same architecture as the pre-
LayerN orm [5,74] Transformer decoder but uses twice the number of attention
heads, i.e. nhead = dmodel/32. It has three required inputs consisting of position,
context, and image tokens, and an optional attention mask.

In the following equations, we omit LayerN orm and Dropout for brevity.

The first MHA module is used for context–position attention:

hc = p + M HA(p, c, c, m) ∈ R(T +1)×dmodel

(2)

where T is the context length, p ∈ R(T +1)×dmodel are the position tokens,
c ∈ R(T +1)×dmodel are the context embeddings with positional information, and
m ∈ R(T +1)×(T +1) is the optional attention mask. Note that the use of special
delimiter tokens ([B] or [E]) increases the total sequence length to T + 1.

The position tokens encode the target position to be predicted, each one
having a direct correspondence to a specific position in the output. This pa-
rameterization is similar to the query stream of two-stream attention [79]. It
decouples the context from the target position, allowing the model to learn from
PLM. Without the position tokens, i.e. if the context tokens are used as queries
themselves like in standard Transformers, the model will not learn anything
meaningful from PLM and will simply function like a standard AR model.

The supplied mask varies depending on how the model is used. During train-
ing, masks are generated from random permutations (Section 3.2). At inference

Scene Text Recognition with Permuted Autoregressive Sequence Models

7

(Section 3.3), it could be a standard left-to-right lookahead mask (AR decoding),
a cloze mask (iterative refinement), or no mask at all (NAR decoding).

The second MHA is used for image–position attention:

hi = hc + M HA(hc, z, z) ∈ R(T +1)×dmodel

(3)

where no attention mask is used. The last decoder hidden state is the output of
the MLP, hdec = hi + M LP (hi) ∈ R(T +1)×dmodel .

Finally, the output logits are y = Linear(hdec) ∈ R(T +1)×(S+1) where S is
the size of the character set (charset) used for training. The additional character
pertains to the [E] token (which marks the end of the sequence). In summary,
given an attention mask m, the decoder is a function which takes the form:

y = Dec(z, p, c, m) ∈ R(T +1)×(S+1)

(4)

3.2 Permutation Language Modeling

Given an image x, we want to maximize the likelihood of its text label y =
[y1, y2, . . . , yT ] under the set of model parameters θ. In standard AR modeling,
the likelihood is factorized using the chain rule according to the canonical or-
dering, [1, 2, . . . , T ], resulting in the model log p(y|x) = (cid:80)T
t=1 log pθ(yt|y<t, x).
However, Transformers process all tokens in parallel, allowing the output tokens
to access or be conditionally-dependent on all the input tokens. In order to have
a valid AR model, past tokens cannot have access to future tokens. The AR
property is enforced in Transformers with the use of attention masks. For exam-
ple, a standard AR model for a three-element sequence y will have the attention
mask shown in Table 1a.

The key idea behind PLM is to train on all T ! factorizations of the likelihood:

log p(y|x) = Ez∼ZT

(cid:34) T

(cid:88)

t=1

(cid:35)

log pθ(yzt |yz<t, x)

(5)

where ZT denotes the set of all possible permutations of the index sequence [1,
2, . . . , T ], and zt and z<t denote the t-th element and the first t − 1 elements,
respectively, of a permutation z ∈ ZT . Each permutation z specifies an ordering
which corresponds to a distinct factorization of the likelihood.

To implement PLM in Transformers, we do not need to actually permute
the text label y. Rather, we craft the attention mask to enforce the ordering
specified by z. As a concrete example, shown in Table 1 are attention masks for
four different permutations of a three-element sequence. Notice that while the
order of the input and output sequences remains constant, all four correspond
to distinct AR models specified by the given permutation or factorization order.
With this in mind, it can be seen that the standard AR training is just a special
case of PLM where only one permutation, [1, 2, . . . , T ], is used.

In practice, we cannot train on all T ! factorizations due to the exponential
increase in computational requirements. As a compromise, we only use K of

8

D. Bautista, R. Atienza

the possible T ! permutations. Instead of sampling uniformly, we choose the K
permutations in a specific way. We use K/2 permutation pairs. The first half
consists of the left-to-right permutation, [1, 2, . . . , T ], and K/2 − 1 randomly
sampled permutations. The other half consists of flipped versions of the first. We
found that this sampling procedure results in a more stable training.

With K permutations and the ground truth label ˆy, the full training loss
is the mean of the individual cross-entropy losses for each permutation-derived
attention mask mk:

L =

1
K

K
(cid:88)

k=1

Lce(yk, ˆy)

(6)

where yk = Dec(z, p, c, mk). Padding tokens are ignored in the loss computa-
tion. More PLM details are in Appendix E.

Table 1. Illustration of AR attention masks for each permutation. The table header
(with the [B] token) pertains to the input context, while the header column (with
the [E] token) corresponds to the output tokens. 1 means that the output token has
conditional dependency on the corresponding input token. 0 means that no information
flows from input to output

(a) [1, 2, 3]

(b) [3, 2, 1]

(c) [1, 3, 2]

(d) [2, 3, 1]

[B] y1 y2 y3

[B] y1 y2 y3

[B] y1 y2 y3

[B] y1 y2 y3

1
y1
1
y2
1
y3
[E] 1

0
1
1
1

0
0
1
1

0
0
0
1

1
y1
1
y2
1
y3
[E] 1

0
0
0
1

1
0
0
1

1
1
0
1

1
y1
1
y2
1
y3
[E] 1

0
1
1
1

0
0
0
1

0
1
0
1

1
y1
1
y2
1
y3
[E] 1

0
0
0
1

1
0
1
1

1
0
0
1

3.3 Decoding Schemes

PLM training coupled with the correct parameterization allows PARSeq to be
used with various decoding schemes. In this work, we only use two contrasting
schemes even though more are theoretically supported. Specifically, we elaborate
the use of monotonic AR and NAR decoding, as well as iterative refinement.

Autoregressive (AR) decoding generates one new token per iteration. The
left-to-right attention mask (Table 2a) is always used. For the first iteration, the
context is set to [B], and only the first position query token p1 is used. For any
succeeding iteration i, position queries [p1, . . . , pi] are used, while the context is
set to the previous output, argmax(y) prepended with [B].

Non-autoregressive (NAR) decoding generates all output tokens at the
same time. All position queries [p1, . . . , pT +1] are used but no attention mask is
used (Table 2b). The context is always [B].

Scene Text Recognition with Permuted Autoregressive Sequence Models

9

Iterative refinement can be performed regardless of the initial decoding
method (AR or NAR). The previous output (truncated at [E]) serves as the
context for the current iteration similar to AR decoding, but all position queries
[p1, . . . , pT +1] are always used. The cloze attention mask (Table 2c) is used. It is
created by starting with an all-one mask, then masking out the matching token
positions.

Table 2. Illustration of information flow for the different decoding schemes. Conven-
tions follow Table 1. In NAR decoding, no mask is used; this is equivalent to using an
all-one mask. ”. . . ” pertains to elements y3 to yT −1

(a) left-to-right AR mask

(b) NAR mask

(c) cloze mask

[B] y1 y2 . . . yT

1
y1
1
y2
1
. . .
yT
1
[E] 1

0
1
1
1
1

0
0
0
0
0
0
1 . . . 0
0
1
1
1
1
1

[B]

1
y1
1
y2
1
. . .
yT
1
[E] 1

[B] y1 y2 . . . yT

1
y1
1
y2
1
. . .
yT
1
[E] 1

0
1
1
1
1

1
1
1
0
1
1
1 . . . 1
0
1
1
1
1
1

4 Results and Analysis

In this section, we first discuss the experimental setup including the datasets, pre-
processing methods, training and evaluation protocols, and metrics used. Next,
we present our results and compare PARSeq to SOTA methods in terms of the
said metrics and commonly used computational cost indicators.

4.1 Datasets

STR models are traditionally trained on large-scale synthetic datasets because of
the relative scarcity of labelled real data [3]. However, in recent years, the amount
of labelled real data has become sufficient for training STR models. In fact, train-
ing on real data was shown to be more sample-efficient than on synthetic data [4].
Hence, in addition to the commonly used synthetic training datasets MJSynth
(MJ) [30] and SynthText (ST) [28], we also use real data for training. Specifi-
cally, we use COCO-Text (COCO) [70], RCTW17 [62], Uber-Text (Uber) [84],
ArT [16], LSVT [65], MLT19 [52], and ReCTS [83]. A comprehensive discussion
about these datasets is available in Baek et al . [4]. In addition, we also use two
recent large-scale real datasets based on Open Images [35]: TextOCR [63] and
annotations from the OpenVINO toolkit [36]. More details in Appendix F.

Following prior works [3], we use IIIT 5k-word (IIIT5k) [49], CUTE80 (CUTE)
[57], Street View Text (SVT) [73], SVT-Perspective (SVTP) [54], ICDAR 2013
(IC13) [32], and ICDAR 2015 (IC15) [31] as the datasets for evaluation. Baek

10

D. Bautista, R. Atienza

et al . [3] provides an in-depth discussion of these datasets. We use the case-
sensitive annotations of Long and Yao [46] for IIIT5k, CUTE, SVT, and SVTP.
Note that IC13 and IC15 have two versions of their respective test splits com-
monly used in the literature—857 and 1,015 for IC13; 1,811 and 2,077 for IC15.
To avoid confusion, we refer to the benchmark as the union of IIIT5k, CUTE,
SVT, SVTP, IC13 (1,015), and IC15 (2,077).

These six benchmark datasets only have a total of 7,672 test samples. This
amount pales in comparison to benchmark datasets used in other vision tasks
such as ImageNet [20] (classification, 50k samples) and COCO [42] (detection,
40k samples). Furthermore, the said datasets largely contain horizontal text only,
as shown in Figure 4a, except for SVT, SVTP, and IC15 2,077 which contain
a number of rotated text. In the real world, the conditions are less ideal, and
captured text will most likely be blurry, vertically-oriented, rotated, or even
occluded. In order to have a more comprehensive comparison, we also use the
test sets of more recent datasets, shown in Figure 4b, such as COCO-Text (9.8k
samples; low-resolution, occluded text), ArT [16] (35.1k samples; curved and
rotated text), and Uber-Text [84] (80.6k samples; vertical and rotated text).

(a) Samples from the benchmark datasets

(b) Samples from Uber, COCO, ArT

Fig. 4. Sample test images from the datasets used

4.2 Training Protocol and Model Selection

All models are trained in a mixed-precision, dual-GPU setup using PyTorch DDP
for 169,680 iterations with a batch size of 384. Learning rates vary per model
(Appendix G.2). The Adam [34] optimizer is used together with the 1cycle [64]
learning rate scheduler. At iteration 127,260 (75% of total), Stochastic Weight
Averaging (SWA) [29] is used and the 1cycle scheduler is replaced by the SWA
scheduler. Validation is performed every 1,000 training steps. Since SWA aver-
ages weights at the end of each epoch, the last checkpoint at the end of training
is selected. For PARSeq, K = 6 permutations are used (Section 4.4). A patch
size of 8 × 4 is used for PARSeq and ViTSTR. More details are in Appendix G.
Label preprocessing is done following prior work [61]. For training, we set
a maximum label length of T = 25, and use a charset of size S = 94 which
contains mixed-case alphanumeric characters and punctuation marks.

Image preprocessing is done like so: images are first augmented, resized,
then finally normalized to the interval [−1, 1]. The set of augmentation opera-
tions consists primarily of RandAugment [18] operations, excluding Sharpness.

Scene Text Recognition with Permuted Autoregressive Sequence Models

11

Invert is added due to its effectiveness in house number data [17]. GaussianBlur
and PoissonNoise are also used due to their effectiveness in STR data augmen-
tation [1]. A RandAugment policy with 3 layers and a magnitude of 5 is used.
Images are resized unconditionally to 128×32 pixels.

4.3 Evaluation Protocol and Metrics

All experiments are performed on an NVIDIA Tesla A100 GPU system. Reported
mean±SD values are obtained from four replicates per model. A t-test (α = 0.05)
is used to determine if model differences are statistically-significant. There can be
multiple best results in a column if the differences are not statistically-significant.
PARSeq results are obtained from the same model using two different decoding
schemes: PARSeqA denotes AR decoding with one refinement iteration, while
PARSeqN denotes NAR decoding with two refinement iterations (ablation study
in Appendix H).

Word accuracy is the primary metric for STR benchmarks. A prediction

is considered correct if and only if characters at all positions match.

Charset may vary at inference time. Subsets of the training charset can
be used for evaluation. Specifically, the following charsets are used: 36-character
(lowercase alphanumeric), 62-character (mixed-case alphanumeric), and 94-char-
acter (mixed-case alphanumeric with punctuation). In Python, these correspond
to array slices [:36], [:62], and [:94] of string.printable, respectively.

4.4 Ablation on training permutations vs test accuracy

As discussed in Section 3.2, training on all possible permutations is not feasible
in practice due to the exponential increase in computational requirements. We
instead sample a number of permutations from the pool of all possible permuta-
tions. Table 3 shows the effect of the number of training permutations on the test
accuracy for all decoding schemes. With K = 1, only the left-to-right ordering
is used and the training simplifies to the standard AR modeling. In this setup,
NAR decoding does not work at all, while AR decoding works well as expected.
Meanwhile, the refinement or cloze accuracy is at a dismal 71.14% (this is very
low considering that the ground truth itself is used as the initial prediction). All
decoding schemes start to perform satisfactorily only at K >= 6. This result
shows that PLM is indeed required to achieve a unified STR model. Intuitively,
NAR decoding will not work when training on just the forward and/or reverse
orderings (K <= 2) because the variety of training contexts is insufficient. NAR
decoding relies on the priors for each character which could only be sufficiently
trained if all characters in the charset naturally exist as the first character of a
sequence. Ultimately, K = 6 provides the best balance between decoding accu-
racy and training time. The very high cloze accuracy (∼94%) of our internal LM
highlights the advantage of jointly using image features and language context for
prediction refinement. After all, the primary input signal in STR is the image,
not the language context.

12

D. Bautista, R. Atienza

Table 3. 94-char word accuracy on the benchmark vs number of permutations (K)
used for training PARSeq. No refinement iterations were used for both AR and NAR
decoding. cloze acc. pertains to the word accuracy of one refinement iteration. It was
measured by using the ground truth label as the initial prediction

K AR acc. NAR acc.

cloze acc. Training hours

1
2
6
12
24

93.04
93.48
93.34
92.91
92.67

0.01
22.69
92.22
91.71
91.72

71.14
94.55
94.81
94.59
94.36

5.86
7.30
8.48
10.10
13.53

4.5 Comparison to state-of-the-art (SOTA)

We compare PARSeq to popular and recent SOTA methods. In addition to the
published results, we reproduce a select number of methods for a fair compari-
son [3]. In Table 6, most reproduced methods attain higher accuracy compared
to the original results. The exception is ABINet (around 1.4% decline in com-
bined accuracy) which originally used a much longer training schedule (with pre-
training of 80 and 8 epochs for LM and VM, respectively) and additional data
(WikiText-103). For both synthetic and real data, PARSeqA achieves the highest
word accuracies, while PARSeqN consistently places second or third. When real
data is used, all reproduced models attain much higher accuracy compared to
the original reported results, while PARSeqA establishes new SOTA results.

In Table 4, we show the mean accuracy for each charset. When synthetic
data is used for training, there is a steep decline in accuracy from the 36- to the
62- and 94-charsets. This suggests that diversity of cased characters is lacking in
the synthetic datasets. Meanwhile, PARSeqA consistently achieves the highest
accuracy on all charset sizes. Finally in Table 5, PARSeq is the most robust
against occlusion and text orientation variability. Appendix J contains more
experiments on arbitrarily-oriented text. Notice that the accuracy gap between
methods is better revealed by these larger and more challenging datasets.

Figure 5 shows the cost-quality trade-offs in terms of accuracy and commonly
used cost indicators like parameter count, FLOPS, and latency. PARSeq-S is the
base model used for all results, while -Ti is its scaled down variant (details in
Appendix D). Note that for PARSeq, the parameter count is fixed regardless of
the decoding scheme. PARSeq-S achieves the highest mean word accuracy and
exhibits very competitive cost–quality characteristics across the three indicators.
Compared to ABINet and TRBA, PARSeq-S uses significantly less parameters
and FLOPS. In terms of latency (Appendix I), PARSeq-S with AR decoding is
slightly slower than TRBA, but is still significantly faster than ABINet. Mean-
while, PARSeq-Ti achieves a much higher word accuracy vs CRNN in spite of
similar parameter count and FLOPS. PARSeq-S is Pareto-optimal, while -Ti is
a compelling alternative for low-resource applications.

Scene Text Recognition with Permuted Autoregressive Sequence Models

13

Table 4. Mean word accuracy on the benchmark vs evaluation charset size

Method

Train data

36-char

62-char

94-char

CRNN
ViTSTR-S
TRBA
ABINet
PARSeqN
PARSeqA

CRNN
ViTSTR-S
TRBA
ABINet
PARSeqN
PARSeqA

S
S
S
S
S
S

R
R
R
R
R
R

83.2±0.2
88.6±0.0
90.6±0.1
89.8±0.2
90.7±0.2
91.9±0.2

88.5±0.1
94.3±0.1
95.2±0.2
95.2±0.1
95.2±0.1
96.0±0.0

56.5±0.3
69.5±1.0
71.9±0.9
68.5±1.1
72.5±1.1
75.5±0.6

87.2±0.1
92.8±0.1
93.7±0.1
93.7±0.1
93.7±0.1
94.6±0.0

54.8±0.2
67.7±1.0
69.9±0.8
66.4±1.0
70.5±1.1
73.0±0.7

85.8±0.1
91.8±0.1
92.5±0.1
92.4±0.1
92.7±0.1
93.3±0.1

Table 5. 36-char word accuracy on larger and more challenging datasets

Test datasets and # of samples

Train
data

ArT
35,149

COCO
9,825

Uber
80,551

Total
125,525

S
S
S
S
S
S

R
R
R
R
R
R

57.3±0.1
66.1±0.1
68.2±0.1
65.4±0.4
69.1±0.2
70.7±0.1

66.8±0.2
81.1±0.1
82.5±0.2
81.2±0.1
83.0±0.2
84.5±0.1

49.3±0.6
56.4±0.5
61.4±0.4
57.1±0.8
60.2±0.8
64.0±0.9

62.2±0.3
74.1±0.4
77.5±0.2
76.4±0.1
77.0±0.2
79.8±0.1

33.1±0.3
37.6±0.3
38.0±0.3
34.9±0.3
39.9±0.5
42.0±0.5

51.0±0.2
78.2±0.1
81.2±0.3
71.5±0.7
82.4±0.3
84.5±0.1

41.1±0.3
47.0±0.2
48.3±0.2
45.2±0.3
49.7±0.3
51.8±0.4

56.3±0.2
78.7±0.1
81.3±0.2
74.6±0.4
82.1±0.2
84.1±0.0

Method

CRNN
ViTSTR-S
TRBA
ABINet
PARSeqN
PARSeqA

CRNN
ViTSTR-S
TRBA
ABINet
PARSeqN
PARSeqA

P-SA

P-SA

P-TiA

P-SN

ABINet

TRBA

P-TiA

P-SN

ViTSTR-S

ViTSTR-S

TRBA

P-SA

P-SN

TRBA

ABINet

ViTSTR-S

ABINet

P-TiA

P-TiN

P-TiN

P-TiN

CRNN

CRNN

CRNN

20

40

Parameters [M]

2

4

6

FLOPS [G]

10

20

Latency [msec/image]

94

92

90

88

86

]

%

[

y
c
a
r
u
c
c
A

Fig. 5. Mean word accuracy (94-char) vs computational cost. P-S and P-Ti are short-
hands for PARSeq-S and PARSeq-Ti, respectively. For TRBA and PARSeqA, FLOPS
and latency correspond to mean values measured on the benchmark

14

D. Bautista, R. Atienza

Table 6. Word accuracy on the six benchmark datasets (36-char). For Train data:
Synthetic datasets (S) - MJ [30] and ST [28]; Benchmark datasets (B) - SVT, IIIT5k,
IC13, and IC15; Real datasets (R) - COCO, RCTW17, Uber, ArT, LSVT, MLT19,
ReCTS, TextOCR, and OpenVINO; ”*” denotes usage of character-level labels. In our
experiments, bold indicates the highest word accuracy per column. 1Used with SCAT-
TER [43]. 2SynthText without special characters (5.5M samples). 3LM pretrained on
WikiText-103 [48]. Combined accuracy values are available in Appendix K

Test datasets and # of samples

Method

Train IIIT5k
3,000
data

SVT
647

PlugNet [50]
SRN [80]

S
S

RobustScanner [81] S,B
S*
S
S,B
S
S
S
S
S
S2
S
S
S3

TextScanner [71]
AutoSTR [82]
RCEED [19]
PREN2D [77]
VisionLAN [75]
Bhunia et al. [9]
CVAE-Feed.1 [8]
STN-CSTR [12]
ViTSTR-B [2]
CRNN [4]
TRBA [4]
ABINet [24]

94.4
94.8
95.4
95.7
94.7
94.9
95.6
95.8
95.2
95.2
94.2
88.4
84.3
92.1
96.2

92.3
91.5
89.3
92.7
90.9
91.8
94.0
91.7
92.2
–
92.3
87.7
78.9
88.9
93.5

857

–
95.5
–
–
–
–
96.4
95.7
–
–
96.3
93.2
–
–
97.4

IC13

IC15

1,015

1,811

2,077

SVTP
645

CUTE
288

95.0
–
94.1
94.9
94.2
–
–
–
95.5
95.7
94.1
92.4
88.8
93.1
–

–
82.7
–
–
81.8
–
83.0
83.7
–
–
86.1
78.5
–
–
86.0

82.2
–
79.2
83.5
–
82.2
–
–
84.0
84.6
82.0
72.6
61.5
74.7
–

84.3
85.1
82.9
84.8
81.7
83.6
87.6
86.0
85.7
88.9
86.2
81.8
64.8
79.5
89.3

85.0
87.8
92.4
91.6
–
91.7
91.7
88.5
89.7
89.7
–
81.3
61.3
78.2
89.2

ViTSTR-S
CRNN
TRBA
ABINet
PARSeqN (Ours)
PARSeqA (Ours)

ViTSTR-S
CRNN
TRBA
ABINet
PARSeqN (Ours)
PARSeqA (Ours)

S
S
S
S
S
S

94.0±0.2 91.7±0.4 95.1±0.7 94.2±0.7 82.7±0.1 78.7±0.1 83.9±0.6 88.2±0.6
91.2±0.2 85.7±0.7 92.1±0.7 90.9±0.5 74.4±1.0 70.8±0.9 73.5±0.6 78.7±0.7
96.3±0.2 92.8±0.9 96.3±0.3 95.0±0.4 84.3±0.1 80.6±0.2 86.9±1.3 91.3±1.6
95.3±0.2 93.4±0.2 97.1±0.4 95.0±0.3 83.1±0.3 79.1±0.2 87.1±0.6 89.7±2.3
95.7±0.2 92.6±0.3 96.3±0.4 95.5±0.6 85.1±0.1 81.4±0.1 87.9±0.9 91.4±1.5
97.0±0.2 93.6±0.4 97.0±0.3 96.2±0.4 86.5±0.2 82.9±0.2 88.9±0.9 92.2±1.2

R 98.1±0.2 95.8±0.4 97.6±0.3 97.7±0.3 88.4±0.4 87.1±0.3 91.4±0.2 96.1±0.4
R 94.6±0.2 90.7±0.4 94.1±0.4 94.5±0.3 82.0±0.2 78.5±0.2 80.6±0.3 89.1±0.4
R 98.6±0.1 97.0±0.2 97.6±0.3 97.6±0.2 89.8±0.4 88.7±0.4 93.7±0.3 97.7±0.2
R 98.6±0.2 97.8±0.3 98.0±0.4 97.8±0.2 90.2±0.2 88.5±0.2 93.9±0.8 97.7±0.7
R 98.3±0.1 97.5±0.4 98.0±0.1 98.1±0.1 89.6±0.2 88.4±0.4 94.6±1.0 97.7±0.9
R 99.1±0.1 97.9±0.2 98.3±0.2 98.4±0.2 90.7±0.3 89.6±0.3 95.7±0.9 98.3±0.6

s
t
l
u
s
e
R
d
e
h
s
i
l
b
u
P

s
t
n
e
m

i
r
e
p
x
E

5 Conclusion

We adapted PLM for STR in order to learn PARSeq, a unified STR model
capable of context-free and -aware decoding, and iterative refinement. PARSeq
achieves SOTA results in different charset sizes and real-world datasets by jointly
conditioning on both image and text representations. By unifying different de-
coding schemes into a single model and taking advantage of the parallel com-
putations in Transformers, PARSeq is optimal on accuracy vs parameter count,
FLOPS, and latency. Due to its extensive use of attention, it also demonstrates
robustness on vertical and rotated text common in many real-world images.

Acknowledgments. This work was funded in part by CHED-PCARI IIID-
2016-005 (Project AIRSCAN). We are also grateful to the PCARI Prime team,
led by Roel Ocampo, who ensured the uptime of our GPU servers.

Scene Text Recognition with Permuted Autoregressive Sequence Models

15

References

1. Atienza, R.: Data augmentation for scene text recognition. In: 2021 IEEE/CVF
International Conference on Computer Vision Workshops (ICCVW). pp. 1561–
1570 (2021). https://doi.org/10.1109/ICCVW54120.2021.00181

2. Atienza, R.: Vision transformer for fast and efficient scene text recognition. In:
International Conference on Document Analysis and Recognition (ICDAR) (2021)
3. Baek, J., Kim, G., Lee, J., Park, S., Han, D., Yun, S., Oh, S.J., Lee, H.: What is
wrong with scene text recognition model comparisons? dataset and model analysis.
In: Proceedings of the IEEE/CVF International Conference on Computer Vision
(ICCV) (10 2019)

4. Baek, J., Matsui, Y., Aizawa, K.: What if we only use real datasets for scene text
recognition? toward scene text recognition with fewer labels. In: Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp.
3113–3122 (6 2021)

5. Baevski, A., Auli, M.: Adaptive input representations for neural language mod-
eling. In: International Conference on Learning Representations (2019), https:
//openreview.net/forum?id=ByxZX20qFQ

6. Bahdanau, D., Cho, K., Bengio, Y.: Neural machine translation by jointly learning
to align and translate. In: Bengio, Y., LeCun, Y. (eds.) 3rd International Con-
ference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9,
2015, Conference Track Proceedings (2015)

7. Balandat, M., Karrer, B., Jiang, D.R., Daulton, S., Letham, B., Wilson, A.G.,
Bakshy, E.: BoTorch: A Framework for Efficient Monte-Carlo Bayesian Opti-
mization. In: Advances in Neural Information Processing Systems 33 (2020),
http://arxiv.org/abs/1910.06403

8. Bhunia, A.K., Chowdhury, P.N., Sain, A., Song, Y.Z.: Towards the unseen: Itera-
tive text recognition by distilling from errors. In: Proceedings of the IEEE/CVF
International Conference on Computer Vision (ICCV). pp. 14950–14959 (10 2021)
9. Bhunia, A.K., Sain, A., Kumar, A., Ghose, S., Chowdhury, P.N., Song, Y.Z.: Joint
visual semantic reasoning: Multi-stage decoder for text recognition. In: Proceed-
ings of the IEEE/CVF International Conference on Computer Vision (ICCV). pp.
14940–14949 (10 2021)

10. Bleeker, M., de Rijke, M.: Bidirectional scene text recognition with a single decoder.

In: ECAI 2020, pp. 2664–2671. IOS Press (2020)

11. Borisyuk, F., Gordo, A., Sivakumar, V.: Rosetta: Large scale system for text de-
tection and recognition in images. In: Proceedings of the 24th ACM SIGKDD In-
ternational Conference on Knowledge Discovery & Data Mining. pp. 71–79 (2018)
12. Cai, H., Sun, J., Xiong, Y.: Revisiting classification perspective on scene text recog-

nition (2021), https://arxiv.org/abs/2102.10884

13. Chen, X., Jin, L., Zhu, Y., Luo, C., Wang, T.: Text recognition in the wild: A

survey. ACM Computing Surveys (CSUR) 54(2), 1–35 (2021)

14. Cheng, Z., Bai, F., Xu, Y., Zheng, G., Pu, S., Zhou, S.: Focusing attention: To-
wards accurate text recognition in natural images. In: Proceedings of the IEEE
international conference on computer vision. pp. 5076–5084 (2017)

15. Cheng, Z., Xu, Y., Bai, F., Niu, Y., Pu, S., Zhou, S.: Aon: Towards arbitrarily-
oriented text recognition. In: Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition. pp. 5571–5579 (2018)

16

D. Bautista, R. Atienza

16. Chng, C.K., Liu, Y., Sun, Y., Ng, C.C., Luo, C., Ni, Z., Fang, C., Zhang, S., Han,
J., Ding, E., et al.: Icdar2019 robust reading challenge on arbitrary-shaped text-
rrc-art. In: 2019 International Conference on Document Analysis and Recognition
(ICDAR). pp. 1571–1576. IEEE (2019)

17. Cubuk, E.D., Zoph, B., Man´e, D., Vasudevan, V., Le, Q.V.: Autoaugment:
Learning augmentation strategies from data. In: 2019 IEEE/CVF Conference
on Computer Vision and Pattern Recognition (CVPR). pp. 113–123 (2019).
https://doi.org/10.1109/CVPR.2019.00020

18. Cubuk, E.D., Zoph, B., Shlens, J., Le, Q.V.: Randaugment: Practical automated
data augmentation with a reduced search space. In: Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition Workshops. pp. 702–703
(2020)

19. Cui, M., Wang, W., Zhang, J., Wang, L.: Representation and correlation enhanced
encoder-decoder framework for scene text recognition. In: Llad´os, J., Lopresti, D.,
Uchida, S. (eds.) Document Analysis and Recognition – ICDAR 2021. pp. 156–170.
Springer International Publishing, Cham (2021)

20. Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: ImageNet: A Large-

Scale Hierarchical Image Database. In: CVPR09 (2009)

21. Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: BERT: Pre-training of deep
bidirectional transformers for language understanding. In: Proceedings of the 2019
Conference of the North American Chapter of the Association for Computational
Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers).
pp. 4171–4186. Association for Computational Linguistics, Minneapolis, Minnesota
(Jun 2019). https://doi.org/10.18653/v1/N19-1423, https://aclanthology.org/
N19-1423

22. Doll´ar, P., Singh, M., Girshick, R.: Fast and accurate model scaling. In: Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp.
924–932 (2021)

23. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner,
T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al.: An image is worth
16x16 words: Transformers for image recognition at scale. In: International Con-
ference on Learning Representations (2020)

24. Fang, S., Xie, H., Wang, Y., Mao, Z., Zhang, Y.: Read like humans: Autonomous,
bidirectional and iterative language modeling for scene text recognition. In: Pro-
ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recogni-
tion (CVPR). pp. 7098–7107 (6 2021)

25. Golovin, D.,

Solnik, B., Moitra,

(eds.): Google Vizier: A Service

Sculley, D.
timization
google-vizier-a-service-for-black-box-optimization

(2017),

S., Kochanski, G., Karro,

J.E.,
for Black-Box Op-
http://www.kdd.org/kdd2017/papers/view/

26. Goyal, P., Doll´ar, P., Girshick, R., Noordhuis, P., Wesolowski, L., Kyrola, A., Tul-
loch, A., Jia, Y., He, K.: Accurate, large minibatch sgd: Training imagenet in 1
hour. arXiv preprint arXiv:1706.02677 (2017)

27. Graves, A., Fern´andez, S., Gomez, F., Schmidhuber, J.: Connectionist temporal
classification: labelling unsegmented sequence data with recurrent neural networks.
In: Proceedings of the 23rd international conference on Machine learning. pp. 369–
376 (2006)

28. Gupta, A., Vedaldi, A., Zisserman, A.: Synthetic data for text localisation in nat-
ural images. In: IEEE Conference on Computer Vision and Pattern Recognition
(2016)

Scene Text Recognition with Permuted Autoregressive Sequence Models

17

29. Izmailov, P., Podoprikhin, D., Garipov, T., Vetrov, D., Wilson, A.: Averaging
weights leads to wider optima and better generalization. In: Silva, R., Globerson,
A., Globerson, A. (eds.) 34th Conference on Uncertainty in Artificial Intelligence
2018, UAI 2018. pp. 876–885. 34th Conference on Uncertainty in Artificial In-
telligence 2018, UAI 2018, Association For Uncertainty in Artificial Intelligence
(AUAI) (2018)

30. Jaderberg, M., Simonyan, K., Vedaldi, A., Zisserman, A.: Synthetic data and ar-
tificial neural networks for natural scene text recognition. In: Workshop on Deep
Learning, NIPS (2014)

31. Karatzas, D., Gomez-Bigorda, L., Nicolaou, A., Ghosh, S., Bagdanov, A., Iwamura,
M., Matas, J., Neumann, L., Chandrasekhar, V.R., Lu, S., et al.: Icdar 2015 com-
petition on robust reading. In: 2015 13th International Conference on Document
Analysis and Recognition (ICDAR). pp. 1156–1160. IEEE (2015)

32. Karatzas, D., Shafait, F., Uchida, S., Iwamura, M., i Bigorda, L.G., Mestre, S.R.,
Mas, J., Mota, D.F., Almazan, J.A., De Las Heras, L.P.: Icdar 2013 robust reading
competition. In: 2013 12th International Conference on Document Analysis and
Recognition. pp. 1484–1493. IEEE (2013)

33. Kasai, J., Pappas, N., Peng, H., Cross, J., Smith, N.: Deep encoder, shallow de-
coder: Reevaluating non-autoregressive machine translation. In: International Con-
ference on Learning Representations (2021), https://openreview.net/forum?id=
KpfasTaLUpq

34. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. In: Interna-

tional Conference on Learning Representations (ICLR) (2015)

35. Krasin, I., Duerig, T., Alldrin, N., Ferrari, V., Abu-El-Haija, S., Kuznetsova,
A., Rom, H., Uijlings, J., Popov, S., Kamali, S., Malloci, M., Pont-Tuset,
J., Veit, A., Belongie, S., Gomes, V., Gupta, A., Sun, C., Chechik, G., Cai,
D., Feng, Z., Narayanan, D., Murphy, K.: Openimages: A public dataset for
large-scale multi-label and multi-class image classification. Dataset available
from https://github.com/openimages (2017), https://storage.googleapis.com/
openimages/web/index.html

36. Krylov, I., Nosov, S., Sovrasov, V.: Open images v5 text annotation and yet another
mask text spotter. In: Balasubramanian, V.N., Tsang, I. (eds.) Proceedings of The
13th Asian Conference on Machine Learning. Proceedings of Machine Learning
Research, vol. 157, pp. 379–389. PMLR (17–19 Nov 2021), https://proceedings.
mlr.press/v157/krylov21a.html

37. Lee, C.Y., Osindero, S.: Recursive recurrent nets with attention modeling for ocr in
the wild. In: Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR) (6 2016)

38. Lee, J., Park, S., Baek, J., Oh, S.J., Kim, S., Lee, H.: On recognizing texts of arbi-
trary shapes with 2d self-attention. In: Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition Workshops. pp. 546–547 (2020)
39. Li, Z., Wallace, E., Shen, S., Lin, K., Keutzer, K., Klein, D., Gonzalez, J.: Train
big, then compress: Rethinking model size for efficient training and inference of
transformers. In: International Conference on Machine Learning. pp. 5958–5968.
PMLR (2020)

40. Liao, Y., Jiang, X., Liu, Q.: Probabilistically masked language model capable of au-
toregressive generation in arbitrary word order. In: Proceedings of the 58th Annual
Meeting of the Association for Computational Linguistics. pp. 263–274 (2020)
41. Liaw, R., Liang, E., Nishihara, R., Moritz, P., Gonzalez, J.E., Stoica, I.: Tune:
A research platform for distributed model selection and training. arXiv preprint
arXiv:1807.05118 (2018)

18

D. Bautista, R. Atienza

42. Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Doll´ar, P.,
Zitnick, C.L.: Microsoft coco: Common objects in context. In: European conference
on computer vision. pp. 740–755. Springer (2014)

43. Litman, R., Anschel, O., Tsiper, S., Litman, R., Mazor, S., Manmatha, R.: Scatter:
Selective context attentional scene text recognizer. In: IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR) (June 2020)

44. Liu, W., Chen, C., Wong, K.Y.K., Su, Z., Han, J.: Star-net: a spatial attention

residue network for scene text recognition. In: BMVC. vol. 2, p. 7 (2016)

45. Long, S., He, X., Yao, C.: Scene text detection and recognition: The deep learning

era. International Journal of Computer Vision 129(1), 161–184 (2021)

46. Long, S., Yao, C.: Unrealtext: Synthesizing realistic scene text images from the
unreal world. In: Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR) (2020)

47. Mansimov, E., Wang, A., Welleck, S., Cho, K.: A generalized framework of se-
quence generation with application to undirected sequence models. arXiv preprint
arXiv:1905.12790 (2019)

48. Merity, S., Xiong, C., Bradbury, J., Socher, R.: Pointer sentinel mixture models.
In: 5th International Conference on Learning Representations, ICLR 2017, Toulon,
France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net (2017),
https://openreview.net/forum?id=Byj72udxe

49. Mishra, A., Alahari, K., Jawahar, C.: Scene text recognition using higher order
language priors. In: BMVC-British Machine Vision Conference. BMVA (2012)
50. Mou, Y., Tan, L., Yang, H., Chen, J., Liu, L., Yan, R., Huang, Y.: Plugnet: Degra-
dation aware scene text recognition supervised by a pluggable super-resolution
unit. In: Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK,
August 23–28, 2020, Proceedings, Part XV 16. pp. 158–174. Springer (2020)
51. Munjal, R.S., Prabhu, A.D., Arora, N., Moharana, S., Ramena, G.: Stride: Scene
text recognition in-device. In: 2021 International Joint Conference on Neural Net-
works (IJCNN). pp. 1–8. IEEE (2021)

52. Nayef, N., Patel, Y., Busta, M., Chowdhury, P.N., Karatzas, D., Khlif, W., Matas,
J., Pal, U., Burie, J.C., Liu, C.l., et al.: Icdar2019 robust reading challenge on
multi-lingual scene text detection and recognition—rrc-mlt-2019. In: 2019 Inter-
national Conference on Document Analysis and Recognition (ICDAR). pp. 1582–
1587. IEEE (2019)

53. Nguyen, N., Nguyen, T., Tran, V., Tran, M.T., Ngo, T.D., Nguyen, T.H., Hoai,
M.: Dictionary-guided scene text recognition. In: Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition (CVPR). pp. 7383–7392
(6 2021)

54. Phan, T.Q., Shivakumara, P., Tian, S., Tan, C.L.: Recognizing text with per-
spective distortion in natural scenes. In: Proceedings of the IEEE International
Conference on Computer Vision. pp. 569–576 (2013)

55. Qi, W., Gong, Y., Jiao, J., Yan, Y., Chen, W., Liu, D., Tang, K., Li, H., Chen, J.,
Zhang, R., et al.: Bang: Bridging autoregressive and non-autoregressive generation
with large scale pretraining. In: International Conference on Machine Learning.
pp. 8630–8639. PMLR (2021)

56. Qiao, Z., Zhou, Y., Yang, D., Zhou, Y., Wang, W.: Seed: Semantics enhanced
encoder-decoder framework for scene text recognition. In: IEEE/CVF Conference
on Computer Vision and Pattern Recognition (CVPR) (6 2020)

57. Risnumawan, A., Shivakumara, P., Chan, C.S., Tan, C.L.: A robust arbitrary
text detection system for natural scene images. Expert Systems with Applications
41(18), 8027–8048 (2014)

Scene Text Recognition with Permuted Autoregressive Sequence Models

19

58. Sheng, F., Chen, Z., Xu, B.: Nrtr: A no-recurrence sequence-to-sequence model for
scene text recognition. In: 2019 International Conference on Document Analysis
and Recognition (ICDAR). pp. 781–786. IEEE (2019)

59. Shi, B., Bai, X., Yao, C.: An end-to-end trainable neural network for image-based
sequence recognition and its application to scene text recognition. IEEE transac-
tions on pattern analysis and machine intelligence 39(11), 2298–2304 (2016)
60. Shi, B., Wang, X., Lyu, P., Yao, C., Bai, X.: Robust scene text recognition with
automatic rectification. In: Proceedings of the IEEE conference on computer vision
and pattern recognition. pp. 4168–4176 (2016)

61. Shi, B., Yang, M., Wang, X., Lyu, P., Yao, C., Bai, X.: Aster: An attentional scene
text recognizer with flexible rectification. IEEE transactions on pattern analysis
and machine intelligence 41(9), 2035–2048 (2018)

62. Shi, B., Yao, C., Liao, M., Yang, M., Xu, P., Cui, L., Belongie, S., Lu, S., Bai, X.:
Icdar2017 competition on reading chinese text in the wild (rctw-17). In: 2017 14th
IAPR International Conference on Document Analysis and Recognition (ICDAR).
vol. 1, pp. 1429–1434. IEEE (2017)

63. Singh, A., Pang, G., Toh, M., Huang, J., Galuba, W., Hassner, T.: Textocr: Towards
large-scale end-to-end reasoning for arbitrary-shaped scene text. In: Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp.
8802–8812 (2021)

64. Smith, L.N., Topin, N.: Super-convergence: Very fast training of neural networks
using large learning rates. In: Artificial Intelligence and Machine Learning for
Multi-Domain Operations Applications. vol. 11006, p. 1100612. International So-
ciety for Optics and Photonics (2019)

65. Sun, Y., Ni, Z., Chng, C.K., Liu, Y., Luo, C., Ng, C.C., Han, J., Ding, E., Liu,
J., Karatzas, D., et al.: Icdar 2019 competition on large-scale street view text with
partial labeling-rrc-lsvt. In: 2019 International Conference on Document Analysis
and Recognition (ICDAR). pp. 1557–1562. IEEE (2019)

66. Tian, C., Wang, Y., Cheng, H., Lian, Y., Zhang, Z.: Train once, and decode as
you like. In: Proceedings of the 28th International Conference on Computational
Linguistics. pp. 280–293 (2020)

67. Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., J´egou, H.: Training
data-efficient image transformers & distillation through attention. In: International
Conference on Machine Learning. pp. 10347–10357. PMLR (2021)

68. Uria, B., Murray, I., Larochelle, H.: A deep and tractable density estimator. In:

International Conference on Machine Learning. pp. 467–475. PMLR (2014)

69. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser,
L.u., Polosukhin, I.: Attention is all you need. In: Guyon, I., Luxburg, U.V., Bengio,
S., Wallach, H., Fergus, R., Vishwanathan, S., Garnett, R. (eds.) Advances in
Neural Information Processing Systems. vol. 30. Curran Associates, Inc. (2017)
70. Veit, A., Matera, T., Neumann, L., Matas, J., Belongie, S.: Coco-text:
Dataset and benchmark for text detection and recognition in natural images.
In: arXiv preprint arXiv:1601.07140 (2016), http://vision.cornell.edu/se3/
wp-content/uploads/2016/01/1601.07140v1.pdf

71. Wan, Z., He, M., Chen, H., Bai, X., Yao, C.: Textscanner: Reading characters in
order for robust scene text recognition. In: Proceedings of the AAAI Conference
on Artificial Intelligence. vol. 34, pp. 12120–12127 (2020)

72. Wang, J., Hu, X.: Gated recurrent convolution neural network for ocr. In: Pro-
ceedings of the 31st International Conference on Neural Information Processing
Systems. pp. 334–343 (2017)

20

D. Bautista, R. Atienza

73. Wang, K., Babenko, B., Belongie, S.: End-to-end scene text recognition. In: 2011
International Conference on Computer Vision. pp. 1457–1464. IEEE (2011)
74. Wang, Q., Li, B., Xiao, T., Zhu, J., Li, C., Wong, D.F., Chao, L.S.: Learning deep
transformer models for machine translation. In: Proceedings of the 57th Annual
Meeting of the Association for Computational Linguistics. pp. 1810–1822 (2019)

75. Wang, Y., Xie, H., Fang, S., Wang, J., Zhu, S., Zhang, Y.: From two to one: A new
scene text recognizer with visual language modeling network. In: Proceedings of
the IEEE/CVF International Conference on Computer Vision (ICCV). pp. 14194–
14203 (10 2021)

76. Xiao, T., Dollar, P., Singh, M., Mintun, E., Darrell, T., Girshick, R.: Early convo-
lutions help transformers see better. Advances in Neural Information Processing
Systems 34 (2021)

77. Yan, R., Peng, L., Xiao, S., Yao, G.: Primitive representation learning for scene text
recognition. In: Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR). pp. 284–293 (6 2021)

78. Yan, R., Peng, L., Xiao, S., Yao, G., Min, J.: Mean: Multi-element attention net-
work for scene text recognition. In: 2020 25th International Conference on Pattern
Recognition (ICPR). pp. 1–8. IEEE (2021)

79. Yang, Z., Dai, Z., Yang, Y., Carbonell, J., Salakhutdinov, R.R., Le, Q.V.: Xlnet:
Generalized autoregressive pretraining for language understanding. Advances in
neural information processing systems 32 (2019)

80. Yu, D., Li, X., Zhang, C., Liu, T., Han, J., Liu, J., Ding, E.: Towards accurate
scene text recognition with semantic reasoning networks. In: Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 12113–
12122 (2020)

81. Yue, X., Kuang, Z., Lin, C., Sun, H., Zhang, W.: Robustscanner: Dynamically
enhancing positional clues for robust text recognition. In: European Conference on
Computer Vision. pp. 135–151. Springer (2020)

82. Zhang, H., Yao, Q., Yang, M., Xu, Y., Bai, X.: Autostr: Efficient backbone search
for scene text recognition. In: Computer Vision–ECCV 2020: 16th European Con-
ference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XXIV 16. pp. 751–
767. Springer (2020)

83. Zhang, R., Zhou, Y., Jiang, Q., Song, Q., Li, N., Zhou, K., Wang, L., Wang, D.,
Liao, M., Yang, M., et al.: Icdar 2019 robust reading challenge on reading chinese
text on signboard. In: 2019 international conference on document analysis and
recognition (ICDAR). pp. 1577–1581. IEEE (2019)

84. Zhang, Y., Gueguen, L., Zharkov, I., Zhang, P., Seifert, K., Kadlec, B.: Uber-text:
A large-scale dataset for optical character recognition from street-level imagery.
In: SUNw: Scene Understanding Workshop - CVPR 2017. Hawaii, U.S.A. (2017),
http://sunw.csail.mit.edu/abstract/uberText.pdf

Scene Text Recognition with Permuted Autoregressive Sequence Models

21

A Issues with unidirectionality of AR models in STR

As discussed in the main text, the unidirectionality of AR models could result in
spurious addition of suffixes and direction-dependent decoding. Shown in Table 7
is a sample output of a left-to-right (LTR) AR model trained on a 36-character
lowercase charset. Since the input is fairly clear and horizontal, the model was
very confident in the predictions for the first 10 characters. However, since it
was trained on alphanumeric characters only, it did not know how to recognize
the exclamation mark. The language context swayed the output of the model
to add the -ly suffix in order to make sense of the unrecognized character. A
right-to-left (RTL) AR model would not add the suffix due to the lack of context
(since the right-most characters would have to be predicted first). This direction-
dependent decoding is further illustrated in Table 8 where two AR models trained
on opposing directions produce different outputs. In this case, the input contains
ambiguity on the uppercase N character. If read from left to right, the context
of the earlier characters can be used to infer that the ambiguous character is
N. However, when read in the opposite direction, the context of OPE is not yet
available, prompting the RTL model to recognize two l ’s in place of a single N
character.

Table 7. Example of a spurious suffix from a left-to-right AR model. GT refers to the
ground truth label, while Confidence pertains to per-character prediction confidence

Input

GT

Prediction

Confidence

terrifying

terrifyingly

[1.00, . . . , 1.00, 0.97, 0.72]

Table 8. Example of direction-dependent decoding with two AR models

Input

GT Direction Prediction

Confidence

open

LTR
RTL

open
opell

[1.00, 1.00, 1.00, 0.66]
[1.00, 1.00, 0.52, 0.57, 0.94]

B Inefficiency of External Language Models in STR

As mentioned in the main text, ensemble methods such as ABINet [24] and SRN
[80] utilize a standalone or external Language Model (LM). In Table 9, we show
the cost measurements of fvcore on the full ABINet model for a single input, as
well as the measurement breakdown for its component models. We can see that
while the LM accounts for around 34.48% of the parameter count, it only uses

22

D. Bautista, R. Atienza

13.65% of the overall FLOPS and 15.78% of the overall activations (a measure
shown to be correlated with model runtime [22,76]). When evaluated in spelling
correction on the 36-character set, the LM achieves a top-5 word accuracy of
only 41.9% [24]. With the ground truth label itself as input (Table 10), the same
model gets a top-1 word accuracy of only 50.44% (36-char). This means that even
if the Vision Model (VM) is perfect (always predicting the correct label), the LM
will produce a wrong output 50% of the time. In summary, the external LM’s
dedicated compute cost, underutilization relative to its parameter and memory
requirements, and dismal word accuracy show the inefficiency of this approach.
For STR, an internal LM might be more appropriate since the primary input
signal is the image, not the language context.

Table 9. Commonly used cost indicators as measured by fvcore for ABINet. Full
Model pertains to the overall measurements

Module

# of Parameters (M)

FLOPS (G) # of Activations (M)

Full Model

36.858 (100.00%)

7.289 (100.00%)

10.785 (100.00%)

- Vision
- Language
- Alignment

23.577 (63.97%)
12.707 (34.48%)
0.574 (1.55%)

6.249 (85.73%)
0.995 (13.65%)
0.045 (0.62%)

9.036 (83.78%)
1.702 (15.78%)
0.047 (0.44%)

Table 10. Performance of ABINet’s LM when the ground truth label itself is used as
the input

Dataset # of samples Word acc. (%) 1 - NED

IIIT5k
SVT
IC13
IC15
SVTP
CUTE80

3,000
647
1,015
2,077
645
288

47.33
65.38
62.07
40.49
65.27
46.88

69.50
83.48
78.77
67.72
83.08
68.65

Combined

7,672

50.44

72.54

C Multi-head Attention

The attention mechanism is central to the operation of Transformers [69]. In
scaled dot-product attention, the similarity scores between two dk-dimensional
vectors q (query) and k (key), computed using their dot-product, are used to

Scene Text Recognition with Permuted Autoregressive Sequence Models

23

transform a dv-dimensional vector v (value). Formally, scaled dot-product at-
tention is defined as:

Attn(q, k, v) = sof tmax

(cid:18) qkT
√
dk

(cid:19)

v

(7)

It accepts an optional attention mask that limits which keys the queries could
attend to. In a Transformer with token dimensionality of dmodel, dk = dv =
dmodel.

Multi-head Attention (MHA) is the extension of scaled dot-product attention
to multiple representation subspaces or heads. To keep the computational cost of
MHA practically constant regardless of the number of heads, the dimensionality
of the vectors are reduced to dhead = dmodel/h, where h is the number of heads. A
head corresponds to an invocation of Equation (7) on projected versions of q, k,
and v using parameter matrices Wq ∈ Rdmodel×dhead, Wk ∈ Rdmodel×dhead, and
Wv ∈ Rdmodel×dhead, respectively, as shown in Equation (8). The final output
is obtained in Equation (9) by concatenating the heads and multiplying by the
output projection matrix Wo ∈ Rdmodel×dmodel .

headi = Attn(qWq

i , kWk

i , vWv
i )

M HA(q, k, v) = Concat(head1, ..., headh)Wo

(8)

(9)

D Model Architecture

PARSeq uses an encoder which largely follows the original ViT [23], and a pre-
LayerN orm [5,74] decoder with more heads. The architectures are practically
unchanged but are reproduced here for the convenience of the reader.

D.1 ViT Encoder

The encoder is composed of 12 layers. All layers share the same architecture
shown in Figure 6. The output of the last encoder layer goes through a final
LayerN orm.

D.2 Visio-lingual Decoder

The decoder (Figure 7) consists of only a single layer. The immediate outputs
of all M HA and M LP layers go through Dropout (p = 0.1, not shown). Image
Features are already LayerN orm’d by the encoder (hence no LayerN orm prior
to input).

24

D. Bautista, R. Atienza

Fig. 6. Illustration of a ViT layer from Dosovitskiy et al . [23]. Norm pertains to
LayerN orm.

D.3 Architecture Configuration

The main results are obtained from the base model, PARSeq-S, which has a
similar configuration to DeiT-S [67] but uses an image size of 128×32 and a
patch size of 8×4 (a change also adapted in our reproduction of ViTSTR-S).
Based on our experiments, scaling up the model only marginally improves word
accuracy on the benchmark. We instead explore scaling down the model to make
it more suitable for edge devices. PARSeq-Ti, which uses a configuration similar
to DeiT-Ti [67], is more similar to CRNN [59] in terms of parameter count and
FLOPS. The detailed configuration parameters are shown in Table 11.

Table 11. Configurations for the base (PARSeq-S) and smaller (PARSeq-Ti) model
variants. dmodel refers to the dimensionality of the model which dictates the dimensions
of the vectors and feature maps. h refers to the number of attention heads used in MHA
layers. dM LP refers to the dimension of the intermediate features within the MLP layer.
depth refers to the number of encoder or decoder layers used

Variants

dmodel

PARSeq-Ti
PARSeq-S

192
384

h

3
6

encoder

dM LP

depth

768
1536

12
12

decoder
dM LP

depth

768
1536

1
1

h

6
12

Scene Text Recognition with Permuted Autoregressive Sequence Models

25

Fig. 7. Visio-lingual decoder architecture with LayerN orm layers shown.

E Permutation Language Modeling

In this section, we provide additional details about the adaptation of PLM for
use in PARSeq. We give a concrete illustration of masked multi-head attention
first. Next, the intuition behind the usage of permutation pairs is discussed.
Lastly, implementation details and considerations about the training procedure
are discussed.

E.1

Illustration of attention masking

As discussed in the main text, Transformers process all tokens in parallel. In
order to enforce the AR constraint which limits the conditional dependencies
for each token, attention masking is used. Figure 8 shows a concrete example of
masked multi-head attention for a sequence y. The position tokens always serve
as the query vectors, while the context tokens (context embeddings with position
information) serve as the key and value vectors. Note that the sequence order is
fixed, and that only the AR factorization order (specified by the attention mask)
is permuted.

E.2 Permutation Sampling

As discussed in the main text, we sample permutations in a specific way. We use
pairs of permutations, and the left-to-right permutation is always used. Thus, we
only sample K/2 − 1 permutations every training step. To illustrate the intuition
behind the usage of flipped permutation pairs, we give the following example.
Given a three-element text label y = [y1, y2, y3] and K = 4 permutations: [1, 2, 3],
[3, 2, 1], [1, 3, 2], and [2, 3, 1]. The first two permutations are the left-to-right and
right-to-left orderings, respectively. Both are always used as long as K > 1. The
corresponding factorizations of the joint probability per pair are as follows:

p(y)[1,2,3] = p(y1)p(y2|y1)p(y3|y1, y2)
p(y)[3,2,1] = p(y3)p(y2|y3)p(y1|y2, y3)

26

D. Bautista, R. Atienza

(a) MHA for output token y1

(b) MHA for output token y2

(c) MHA for output token y3

(d) MHA for output token [E]

Fig. 8. Masked MHA for a three-element sequence y = [y1, y2, y3] given the factoriza-
tion order [1, 3, 2]. c are context embeddings with position information

Scene Text Recognition with Permuted Autoregressive Sequence Models

27

p(y)[1,3,2] = p(y1)p(y3|y1)p(y2|y1, y3)
p(y)[2,3,1] = p(y2)p(y3|y2)p(y1|y2, y3)

For each permutation pair, if we group the probabilities per element, we get
Table 12. Notice that the probabilities of each element for every permutation pair
consists of disjoint sets of conditioning variables. For example, the probabilities
of element y1 for [1, 2, 3] (left-to-right permutation) and [3, 2, 1] (right-to-left
permutation) are p(y1) and p(y1|y2, y3), respectively. The first term is the prior
probability of y1. It is not conditioned on any other element of the text label,
unlike the second term which is conditioned on all other elements, y2 and y3.
Similarly for y2, the first term is conditioned only on y1 while the second term
is conditioned only on y3. In our experiments, we find that using flipped permu-
tation pairs results in more stable training dynamics where the loss is smoother
and less erratic.

Table 12. Probability terms grouped by permutation pairs.

Perm.

y1

y2

y3

[1, 2, 3]
[3, 2, 1]

[1, 3, 2]
[2, 3, 1]

p(y1)
p(y1|y2, y3)

p(y2|y1)
p(y2|y3)

p(y3|y1, y2)
p(y3)

p(y1)
p(y1|y2, y3)

p(y2|y1, y3)
p(y2)

p(y3|y1)
p(y3|y2)

E.3 Special handling of end-of-sequence [E] token

Although the [E] token is part of the sequence, it is handled in a specific way
in order to make training simpler. First, no character c ∈ C, where C is the
training charset, is conditioned on [E]. Intuitively, it means that [E] marks the
end of the sequence (hence its name) since no more characters are expected after
it is produced by the model. More formally, it means that p(c|[E]) = 0. This is
achieved by masking the positions of [E] in the input context. Second, we train
[E] on only two permutations, left-to-right and right-to-left. The left-to-right
lookahead mask provides the longest context to [E] (conditioned on all other
characters in the sequence), while the right-to-left mask provides no context,
which is necessary for NAR decoding. We could also train [E] on different subsets
of the input context, but doing so needlessly complicates the training procedure
without offering any advantages.

E.4 Considerations for batched training

Text labels of varying lengths can be included in a mini-batch. However, the sam-
pled permutations for the mini-batch are always based on the longest sequence.

28

D. Bautista, R. Atienza

Hence, it is possible that after accounting for padding, multiple permutations
would become equivalent. To see why this is the case, consider a mini-batch
containing two samples: the first label has a single character, while the second
label has four characters. The first label has a sequence length of one and total
number of permutations also equal to one. On the other hand, the second label
has a sequence length of four which corresponds to 24 total permutations. If
we use K = 6 permutations, then it means that the permutations for the first
label would be oversampled since there is only one valid permutation for T = 1.
We find that this oversampling actually helps training. We experimented with a
modified training procedure wherein sequences with T < 4 are grouped together
(i.e. 1-, 2-, and 3-character sequences are grouped separately). This training pro-
cedure results in increased training time due to the mini-batch being split further
into smaller batches, but it does not improve accuracy nor hasten convergence.
Thus, we stick with the simpler batched training procedure.

F Dataset Matters

F.1 Open Images Datasets

TextOCR and OpenVINO are datasets both derived from Open Images—a large
dataset with very diverse images often containing complex scenes with several
objects (8.4 per image on average). Open Images is not specifically collected for
STR. Thus, it contains text of varying resolutions, orientations, and quality, as
shown in cropped word boxes in Figure 9. TextOCR and OpenVINO significantly
overlap in terms of source scene images, as shown in Table 13. Samples of source
scene images common to both are shown in Figure 10. Only the validation set
of OpenVINO and the test set of TextOCR do not overlap any other image set.
The labels of TextOCR’s test set are kept private.

Table 13. Overlap between TextOCR and OpenVINO in terms of the number of
common source scene images.

I

O
N
V
n
e
p
O

train 1
train 2
train 5
train f
validation

TextOCR
val

test

225
230
184
157
0

0
0
0
0
0

train

1,612
1,444
1,302
1,068
0

Scene Text Recognition with Permuted Autoregressive Sequence Models

29

Fig. 9. Cropped word boxes from Open Images.

Fig. 10. Examples of source scene images common to TextOCR and OpenVINO.

30

D. Bautista, R. Atienza

F.2 Data preparation for LMDB storage

We use the archives released by Baek et al . [4] for RCTW17, Uber-Text, ArT,
LSVT, MLT19, and ReCTS. Thus, we only preprocess data for the remaining
datasets.

For COCO-Text, we use the v1.4 test annotations released as part of the
ICDAR 2017 challenge. For train and val, we use the latest (v2.0) annotations.
We preprocess TextOCR, OpenVINO, and COCO-Text with minimal filtering
and modifications, in contrast to the usual practice of removing non-horizontal
text and special characters. We only filter illegible and non-machine printed text.
The only modification we perform is the removal of whitespace on either side of
the label, or duplicate whitespace between non-whitespace characters.

For IC13 and IC15, we use the original data from the ICDAR competition
website and perform no modifications to the data. We emulate the previous
filtering methods [73,14] to create the subsets used for evaluation.

Long and Yao [46] have reannotated IIIT5k, CUTE, SVT, and SVTP be-
cause the original annotations are case-insensitive and lack punctuation marks.
However, both the reannotations and the originals contain some errors. Hence,
we review inconsistencies between the two versions and manually reconcile them
to correct the errors.

Table 14 provides a detailed summary of how each dataset was used.

Table 14. Summary of dataset usage after on-the-fly filtering for the 94-character set.
Numbers indicate how many samples were used from each dataset. t and v refer to
splits that were repurposed as training and validation data, respectively. * indicates
private ground truth labels. – indicates that the dataset does not have a particular
split. IC13 and IC15 have two versions of their respective test splits commonly used
in the literature.

Dataset

train

val

MJSynth
SynthText

LSVT
MLT19
RCTW17
ReCTS
TextOCR
OpenVINO

ArT
COCO
Uber

IIIT5k
SVT
IC13
IC15
SVTP
CUTE

7,224,586
6,975,301

41,439
56,727
10,284
21,589
710,994
1,912,784

32,028
59,733
91,732

2,000v
257v
848v
4,468v
–
–

802,731t
–

–
–
–
–
107,093t
158,757t

–
13,394t
36,188t

test

891,924t
–

–
–
–
2,467t
0*
–

35,149
9,825
80,587

–
–
–
–
–
–

3,000
647
857 / 1,015
1,811 / 2,077
645
288

Scene Text Recognition with Permuted Autoregressive Sequence Models

31

G Training Details

In the main text, the 169,680 training iterations (batch size of 384) is equivalent
to 20 epochs on the combined real training dataset (3,257,585 samples). The
same exact number of training iterations is used when training on synthetic
data (MJ+ST, 15.89M samples), resulting in just over 4 epochs of training. As
shown in Table 15, this training schedule is more than twice as long as Baek et al .
[4] but is still much shorter than ABINet’s original training schedule of 8 epochs
VM pretraining on MJ+ST, 80 epochs LM pretraining on WikiText-103, and 10
epochs full model training on MJ+ST. This explains why our reproduction of
CRNN and TRBA obtain higher accuracy than the originals, and why ABINet
gets a slightly lower (1.4%) accuracy compared to the original results.

Table 15. Training schedule comparison vs reproduced methods. Sorted from shortest
to longest schedule based on the sample count (essentially batch size × number of
iterations)

Method

Batch size # of iterations Sample count (M)

CRNN and TRBA [4]
ViTSTR [2]
Ours
ABINet (VM + full) [24]
ABINet (LM) [24]

128
192
384
384
4,096

200,000
300,000
169,680
745,074
1,688,720

25.6
57.6
65.2
286.1
6,917.0

G.1 Label preprocessing

Preprocessing and filtering are done as follows. Whitespace characters are re-
moved from the labels. Unicode characters are normalized using the NFKD
normalization form and then converted to ASCII. Next, labels longer than T
characters are filtered. Case-sensitivity is inferred from the charset. If all letters
in the charset are lowercase, the label is transformed to its lowercase version. If
the charset consists of purely uppercase letters, the label is converted to its up-
percase version. If the charset is mixed-case, no case conversion is done. Lastly,
all characters not specified in the charset are removed from the labels.

G.2 Learning Rate Optimization

For fair comparison during evaluation, all training hyperparameters—except the
learning rate—are kept constant across models. The learning rate is varied be-
cause different architectures and model sizes train differently [39]. Ray Tune
[41] was used to automatically search for the optimum maximum learning rate
given the fixed training schedule. Specifically, we used a combination of Me-
dian Stopping [25] and Bayesian Optimization [7] to efficiently narrow down

32

D. Bautista, R. Atienza

Table 16. Learning rates used for training. The Base LR is the raw value set in
the configuration, while the Effective LR is the actual value used for training. During
pretraining, ABINet (LM) is used for ABINet’s language model

Model

Base LR Effective LR

CRNN
ViTSTR-S
TRBC
TRBA
ABINet (LM)
ABINet
PARSeq

5.10 × 10−4
8.90 × 10−4
1.00 × 10−4
6.90 × 10−4
3.00 × 10−4
3.40 × 10−4
7.00 × 10−4

1.08 × 10−3
1.89 × 10−3
2.12 × 10−4
1.46 × 10−3
6.36 × 10−4
7.21 × 10−4
1.48 × 10−3

the configuration space. Finally, a grid search over the narrowed down learning
rate range was performed with models trained to completion. The final learn-
ing rates used for training are shown in Table 16. The base learning rates are
scaled using two multipliers: the DDP factor (
nGP U ) and the batch size linear
scaling rule (bsize/256) [26], where nGP U refers to the number of GPUs used
(i.e. nGP U = 2 for a dual-GPU setup) and bsize refers to the batch size (i.e.
bsize = 384).

√

H Accuracy of decoding schemes vs latency

Figure 11 shows how the word accuracy and latency evolve as functions of the
number of refinement iterations. For AR decoding, refinement iterations after
the first provide negligible increase in accuracy. For NAR decoding, the accuracy
increase becomes insignificant after the second iteration. Hence, we use one and
two refinement iterations for the AR and NAR decoding schemes, respectively.

93.5

93

92.5

]

%

[

y
c
a
r
u
c
c
A

1

2

3

2

3

0

1

0

8

14
12
10
Latency [msec/image]

NAR
AR

16

Fig. 11. PARSeq word accuracy and single-image latency for each decoding scheme.
The number of refinement iterations used is indicated for each point.

Scene Text Recognition with Permuted Autoregressive Sequence Models

33

I Detailed Latency Measurements

]
e
g
a
m
i
/
c
e
s
m

[

y
c
n
e
t
a
L

35

30

25

20

15

10

5

0

1

Mean word length

CRNN
ViTSTR-S
TRBA
ABINet
PARSeqN
PARSeqA

5

17
13
9
Output label length

21

25

Fig. 12. Model latency vs output label length as measured by PyTorch’s benchmark
timer on an NVIDIA Tesla A100 GPU. Each point corresponds to a mean of five runs of
Timer.blocked autorange(). Lower is better. Mean word length is the average length
of the labels from all test datasets.

We measure model latency in an isolated manner. By doing so, we can reliably
factor out the effects of data loading, storage, and CPU latency. We use the built-
in benchmarking tool of PyTorch and measure latency for different label lengths,
as shown in Figure 12. As expected, NAR methods including PARSeqN exhibit
near-constant latency regardless of output label length. Meanwhile, the latency
of AR methods increases linearly as a function of the output label length. The
latency increase of PARSeqA is steeper than TRBA. However, since the average
length of words in the test datasets is quite short at 5.4, the actual difference in
mean latency between TRBA and PARSeqA is only about 2.3 msec.

J Experiments on arbitrarily-oriented text

In STR, the focus has mainly been on horizontal text, with a few explicitly tack-
ling arbitrarily-oriented text [54,15,51,77,78]. In our experiments, we observe
that existing attention-based models are capable of recognizing text in arbitrary
orientation, as shown in Table 17. Only CRNN, a CTC-based model, exhibits
dismal orientation robustness. We conjecture that the direct correspondence of
visual feature positions to textual feature positions in CTC-based models causes
this poor performance. On the other hand, attention-based models compute fea-
ture similarity scores on-the-fly, resulting in a more dynamic alignment between
visual and textual features.

We hypothesize that regardless of architecture and training procedure, the
attention mechanism makes STR models generally robust against orientation

34

D. Bautista, R. Atienza

Table 17. Orientation robustness benchmark. Word accuracy (94-char) on rotated
versions of the six benchmark datasets. %dec refers to the percentage decrease of the
Mean accuracy w.r.t. the 0° accuracy.

0°↑

85.8
91.8
92.5
92.4

Method

CRNN
ViTSTR-S
TRBA
ABINet

PARSeqN
PARSeqA

90°↑

180°↑

270°↑ Mean↑ %dec↓

11.8±0.1
87.9±0.2
84.6±0.1
66.0±1.5

6.4±0.5
78.9±0.3
83.5±0.2
77.1±0.9

10.7±0.2
80.6±0.9
78.6±0.3
65.5±1.8

92.7
93.3

86.7±0.3
88.0±0.1

83.2±0.9
86.6±0.3

81.1±0.3
84.1±0.1

9.6
82.4
82.3
69.6

83.7
86.2

88.8
10.2
11.0
24.7

9.7
7.6

variations. To test our hypothesis, we created a pose-corrected version of Tex-
tOCR which contains text in canonical orientation (practically horizontal), as
opposed to the original version which contains text in arbitrary orientation. One
possible contributor to the orientation robustness of TRBA is its image rectifica-
tion module [60]. To test if this is the case, we also train TRBC, the CTC-based
version of TRBA. We train all models on either TextOCR variants exclusively
and show the results in Table 18.

Table 18. Effect of training on horizontally-oriented (H) vs arbitrarily-oriented (A)
variants of TextOCR. 0° pertains to model accuracy (94-char) on non-rotated bench-
mark datasets. Rotated refers to the mean accuracy on 90°, 180°, and 270° rotations of
the benchmark datasets. In H vs A per row, bold indicates significantly higher accuracy.

Method

CRNN
ViTSTR-S
TRBC
TRBA
ABINet

PARSeqN
PARSeqA

0°

H

A

Rotated

H

A

84.7±0.4
87.8±0.7
87.0±0.2
89.7±0.0
90.3±0.2

89.8±0.3
90.6±0.0

84.0±0.4
88.1±0.3
87.3±0.2
90.1±0.2
90.7±0.2

90.4±0.0
91.3±0.2

0.8±0.0
2.5±0.2
1.3±0.1
2.7±0.1
3.0±0.3

6.4±0.4
8.4±0.4

8.7±0.4
72.8±1.3
17.3±1.9
76.3±0.4
63.9±1.5

77.9±0.6
80.7±0.3

We observe that both CTC-based and attention-based models can be trained
on arbitrarily-oriented text. The mean 0° accuracy decreased for CRNN but the
decrease was not statistically-significant. For other models, the mean accuracy
even increased with TRBA and PARSeq showing statistically-significant im-
provements. This suggests that the common practice of filtering non-horizontal
text might be unnecessary. As far as arbitrarily-oriented text recognition is con-
cerned, training on arbitrarily-oriented text expectedly improves the accuracy
across all models. However, the improvement is minimal in CTC-based models
compared to attention-based models. Moreover, TRBC exhibits slightly better
orientation robustness compared to CRNN, but it still performs badly compared
to TRBA. This suggests that the contribution of the image rectification module

Scene Text Recognition with Permuted Autoregressive Sequence Models

35

to orientation robustness is minimal, and that the attention mechanism is the
primary contributor to orientation robustness.

K Combined word accuracy

Six small datasets are typically used to benchmark STR methods, resulting in
six different mean values for word accuracy. The combined word accuracy is
typically reported too, but we did not include it in Table 4 of the main text
because of space constraints and possible confusion due to inconsistencies in test
sets used. Table 19 shows the combined word accuracy on the benchmark (7,672
samples) and on the smaller test subset (consisting of IC13 857 and IC15 1,811)
with a total of 7,248 samples.

Table 19. Word accuracy on the six benchmark datasets (36-character set). For Train
data: Synthetic datasets (S) - MJ [30] and ST [28]; Benchmark datasets (B) - SVT,
IIIT5k, IC13, and IC15; Real datasets (R) - COCO, RCTW17, Uber, ArT, LSVT,
MLT19, ReCTS, TextOCR, and OpenVINO; ”*” denotes usage of character-level la-
bels. In our experiments, bold indicates the highest word accuracy per column. 1Used
with SCATTER [43]. 2SynthText without special characters (5.5M samples). 3LM Pre-
trained on WikiText-103 [48]

Test datasets and # of samples

Method

Train Total
7,248
data

Total (benchmark )
7,672

s
t
l
u
s
e
R
d
e
h
s
i
l
b
u
P

s
t
n
e
m

i
r
e
p
x
E

PlugNet [50]
SRN [80]

S
S

RobustScanner [81] S,B
S*
S
S,B
S
S
S
S
S
S2
S
S
S3

TextScanner [71]
AutoSTR [82]
RCEED [19]
PREN2D [77]
VisionLAN [75]
Bhunia et al. [9]
CVAE-Feed.1 [8]
STN-CSTR [12]
ViTSTR-B [2]
CRNN [4]
TRBA [4]
ABINet [24]

–
90.4
–
–
–
–
91.5
91.2
–
–
–
85.6
–
–
92.7

ViTSTR-S
CRNN
TRBA
ABINet
PARSeqN (Ours)
PARSeqA (Ours)

ViTSTR-S
CRNN
TRBA
ABINet
PARSeqN (Ours)
PARSeqA (Ours)

S
S
S
S
S
S

90.0±0.1
84.5±0.2
92.0±0.2
91.3±0.2
92.0±0.2
93.2±0.2

R 94.7±0.1
R 89.6±0.1
R 95.7±0.1
R 95.9±0.2
R 95.7±0.1
R 96.4±0.0

89.8
–
89.2
91.0
–
–
–
–
90.9
–
–
83.8
75.8
85.7
–

88.6±0.0
83.2±0.2
90.6±0.1
89.8±0.1
90.7±0.2
91.9±0.2

94.3±0.1
88.5±0.0
95.2±0.1
95.2±0.1
95.2±0.1
96.0±0.0

36

D. Bautista, R. Atienza

L Qualitative Results

In the following tables, shown are qualitative results for all test datasets and for
some images obtained from the internet. The input images are shown in their
original orientations and in aspect ratios close to their original. For predictions
which are roughly aligned to the ground truth, wrong characters are highlighted
in red while missing characters are indicated by a red underscore .

Table 20 shows the results for samples from regular datasets like IIIT5k,
SVT, and IC13. Most of the models did not have a problem recognizing the
fairly clear, horizontal, and high-resolution input images. The only exception is
CRNN failing to recognize any character from the tilted CITY image sample
of SVT. No model was able to correctly recognize Verbandstoffe due to the
ambiguity caused by motion blur, making the character o look like an e.

Table 20. Qualitative results on samples from regular datasets IIIT5k, SVT, and IC13.
GT refers to the ground truth label.

Input

GT

PARSeqA

ABINet

TRBA

ViTSTR-S

CRNN

Predictions

k
5
T
I
I
I

T
V
S

3
1
C
I

Kellimar

Kellimar

Kellimar

Kellimar

Kellimar

Kellimar

TIDE

TIDE

TIDE

TIDE

TIDE

TIDE

Coca-Cola

Coca-Cola

Coca-Cola

Coca-Cola

Coca-Cola

Coca-Cola

NESCAFE NESCAFE NESCAFE NESCAFE NESCAFE NESCAFE

ICEBOX

ICEBOX

ICEBOX

ICEBOX

OCESOX

IREBOX

CITY

CITY

CITY

CITY

CITY

—

BREWERY BREWERY BREWERY BREWERY BREWERY BREWERY

THE

THE

THE

THE

THE

THE

Distributed Distributed Distributed Distribated Distributed Distrm uted

Verbandstoffe Verbandsteffe Verbandsteffe Verbandstelle Verbandsteffe Verbandsteffe

GALORE

GALORE

GALORE

GALORE

CALORE

GALORE

Input

GT

PARSeqA

ABINet

TRBA

ViTSTR-S

CRNN

Predictions

Table 21 shows the qualitative results for samples from the IC15 dataset.
Context-free methods, ViTSTR and CRNN, were not able to correctly predict
Kappa possibly due to the ambiguity caused by distortion on the first p char-

Scene Text Recognition with Permuted Autoregressive Sequence Models

37

acter. ABINet and CRNN both have difficulty in recognizing vertically-oriented
(CONCIERGE ) and rotated text (UNSEEN ). No model correctly predicted epi-
Centre due to the case ambiguity of the character C. Only PARSeq and CRNN
were able to correctly read the telephone number.

Table 21. Qualitative results from IC15 samples.

Input

GT

PARSeqA

ABINet

TRBA

ViTSTR-S

CRNN

Predictions

Kappa

Kappa

Kappa

Kappa

Kaopa

Kadpa

UNSEEN

UNSEIN

UNITIN

UNSEEN

UNSEEN

MATA

CONCIERGE CONCIERGE ONNIIEOO CONCIERGE CONCIERGE

—

epiCentre
Tel:7778100

epicentre

epicentre
Tel:7778100 Tel:77778100 Teles7778100 Tel:17778100 Tel:7778100

eplcentre

epicentre

epicentre

Table 22. Qualitative results from SVTP samples.

Input

GT

PARSeqA ABINet

TRBA

ViTSTR-S CRNN

Predictions

MINT

MINT

AINT

MINT

MINT

MINT

REDWOOD REDWA D maCyyro Programmer REDWOBD

Pe

HOUSE

HOUCE

HOUSE

HOUSE

HOUCE

HOUSE

Restaurant Restaurant Restaurant Restaurant Restaurant Restaurant

CARLTON CARLTON CARLTON CARLTON CARLTON

ANO

Table 22 shows the qualitative results for SVTP samples. All models ex-
cept ABINet were able to recognize MINT. No model correctly recognized the
vertically-oriented text, REDWOOD, with ViTSTR and PARSeq producing the
two closest predictions. Surprisingly, both PARSeq and ViTSTR fail at the rel-
atively easy HOUSE, where the character S is occluded. In PARSeq, the visual

38

D. Bautista, R. Atienza

features have a stronger effect on the final output than the textual features due
to the image–position MHA being closer to the final decoder hidden state. Thus,
a low-confidence visual feature might sway the output to the wrong character
given enough magnitude relative to the textual features. All models correctly
recognized Restaurant even though the image is relatively blurry. All models
except CRNN correctly recognized the vertically-oriented text, CARLTON.

Table 23. Qualitative results from CUTE80 samples.

Input

GT

PARSeqA ABINet TRBA ViTSTR-S CRNN

Predictions

BALLYS BALLYS BALLY’S BALLYS BALLYS BALLYS

eBizu

eBizu

eBizu

eBizu

eBizu

eBizu

CLUB

CLUB

CLUB

CLUB

CLUB

2U1

SALMON SALMON SALMON SALMON SALMON SA NON

Table 23 shows the results for CUTE80, a dataset which primarily contains
curved text. The samples are high-resolution and of good quality resulting in
generally accurate recognition across models. The only exceptions are BALLYS
for ABINet and the relatively vertical texts CLUB, and SALMON for CRNN.

Table 24. Qualitative results from ArT samples.

Predictions

Input

GT

PARSeqA ABINet

TRBA ViTSTR-S CRNN

FONDENTE FONDENIE FONDENIE FONDEN S FONDENTE

FOMEON

cuisine

cuisine

cuisine

cuisine

cuisine

culsi e

COFFEE COFFEE COFFEE COFFEE

COFFEE COFFEE

Franziskaner Franziskaner Franziskaner Franziskaner Franziskaner Ranzishanes

TOMORROW’S TOMORROV’S TOMORRO ’SS TOMORROW’S TOMORROW’S

TO

Scene Text Recognition with Permuted Autoregressive Sequence Models

39

Table 24 shows the results for samples from ArT, a dataset of arbitrarily-
oriented and curved text. CRNN fails to recognize text which are vertically-
oriented. Only ViTSTR is able to recognize FONDENTE correctly, with PARSeq
and ABINet both predicting I in place of T. For the almost upside down TO-
MORROW’S, only TRBA and ViTSTR are able to recognize it. PARSeq possibly
mistook W for a V due to aspect ratio distortion (vertical image being rescaled
into a horizontal one).

Table 25 shows the results for COCO-Text samples. No model was able to
recognize ANTS, possibly due to the presence of small stray characters around
the main text. All models were able to recognize XT-862K in spite of the blurry
image, and People in spite of the occluded o and p characters. Chevron is a
particularly hard sample due to the last two characters being occluded by two
different objects. Only PARseq was able to detect the last two characters and
correctly recognize the o character, while all other models only recognized the
first five characters. GUNNESS is another hard sample due to its low resolution
and occluded character. Only PARSeq was able to infer the occluded character
correctly.

Table 25. Qualitative results from COCO-Text samples.

Input

GT

PARSeqA ABINet

TRBA ViTSTR-S

CRNN

Predictions

ANTS

ANTSTS

KANTER BANTSEN AATSSE

N

XT-862K XT-862K

XT-862K

XT-862K

XT-862K

XT-862K

People

People

People

People

People

People

Chevron

Chevrol

Chevr

Chevr

Chevr

Chevr

GUNNESS GUNNESS GSNNNSSS AWNESS GONNESSS GOWNESS

Table 26 shows the qualitative results for ReCTS, a dataset which contains
fairly high-resolution text with unconventional font styles. Model performance
across all samples is generally good since they are clear and horizontally-oriented.
No model correctly predicted the string of digits, with PARSeq and TRBA
producing the closest predictions with only one wrong character. Most models
correctly predicted AWON except for PARSeq and CRNN which mistook the
occluded W for an N.

Table 27 shows the results for Uber-Text, a dataset which contains many
vertical or rotated text from outdoor signages. PARSeq is the only model to
correctly recognize all samples.

40

D. Bautista, R. Atienza

Table 26. Qualitative results from ReCTS samples.

Input

GT

PARSeqA ABINet

TRBA

ViTSTR-S

CRNN

Predictions

SEVEN

SEVEN

SEVEN

SEVEN

SEVEN

SEVEN

TEA

TEA

TEA

TEA

TEA

TEA

13031863597 13031867597 13071863599 13031863599 13071967597 19091967599

Bubble

Bubble

Bubble

Bubble

Bubble

Bubble

AWON

ANON

AWON

AWON

AWON

A’NON

Table 27. Qualitative results from Uber-Text samples.

Input

GT

PARSeqA

ABINet

TRBA

ViTSTR-S CRNN

Predictions

MeridethLn MeridethLn Merittteww..... MeridethLn MeridethLn

wata

5811

5811

3911

5011

5811

40

PARKING PARKING

2017

2017

PARKING
2017

PARKING PARKING POME

2017

2017

2017

Table 28 shows results from additional samples obtained from the Internet.
Overall, the samples are high-resolution, horizontally-oriented, and use uncon-
ventional fonts similar to ReCTS. PARSeq is the only model to correctly recog-
nize all samples, particularly Creative which uses a cursive handwriting type of
font.

Table 28. Qualitative results from samples obtained from the internet.

Input

GT

PARSeqA

ABINet

TRBA

ViTSTR-S

CRNN

Predictions

COCKTAILS COCKTAILS COCKTAILS COCKTAHIS COCKTAILS COCKTAILS

Creative Creative Creat ne Crestire Creat ee Cedrre

TOGARASHI TOGARASHI TOGARASHI TOGARASHI TOGARASHI TOGARASH!

