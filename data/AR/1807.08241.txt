8
1
0
2

l
u
J

2
2

]

G
L
.
s
c
[

1
v
1
4
2
8
0
.
7
0
8
1
:
v
i
X
r
a

NAVREN-RL:Learningtoﬂyinrealenvironmentviaend-to-enddeepreinforcementlearningusingmonocularimagesMalikAqeelAnwar1,ArijitRaychowdhury2DepartmentofElectricalandComputerEngineeringGeorgiaInstituteofTechnology,Atlanta,GA,USAaqeel.anwar@gatech.edu1,arijit.raychowdhury@ece.gatech.edu2Abstract—WepresentNAVREN-RL,anapproachtoNAVigateanunmannedaerialvehicleinanindoorRealENvironmentviaend-to-endreinforcementlearning(RL).Asuitablerewardfunctionisdesignedkeepinginmindthecostandweightconstraintsformicrodronewithminimumnumberofsensingmodalities.CollectionofsmallnumberofexpertdataandknowledgebaseddataaggregationisintegratedintotheRLprocesstoaidconvergence.ExperimentationiscarriedoutonaParrotARdroneindifferentindoorarenasandtheresultsarecomparedwithotherbaselinetechnologies.Wedemonstratehowthedronesuccessfullyavoidsobstaclesandnavigatesacrossdifferentarenas.Videoofthedronenavigatingusingtheproposedapproachcanbeseenathttps://youtu.be/yOTkTHUPNVYI.INTRODUCTIONOverthepastdecade,therehasbeenconsiderablesuccessinusingUnmannedAerialVehicles(UAVs)ordronesinvar-iedapplicationssuchasreconnaissance,surveying,rescuingandmapping.Irrespectiveoftheapplication,navigatingau-tonomouslyisoneofthekeydesirablefeaturesofUAVsbothindoorsandoutdoors.Severalsolutionshavebeenproposedtomakedronesautonomousinanindoorenvironment.TherehasbeensigniﬁcantworktowardsusingadditionaldedicatedsensingmodalitiessuchasRADAR[1]andLIDAR[2],whichprovidehighaccuracyinnavigationandobstacleavoidancethusenablingautonomousﬂightspossible.Butwhenthepay-loadandthecostistakenintoaccount,suchsystemsareheavyandexpensive,makingthemalmostimpossibletobeusedinlowcostMicroAerialVehicles(MAV).UltrasonicSONARisacheapalternativebutsuffersfromlackofaccuracyandreducedﬁeldofview(FOV).HenceforMAVs,usingtheon-boardandrelativelycheapsensors,inparticularcameras,isanattractiveoptionforautonomousindoornavigation.Inrecentyears,RLhasbeenextensivelyexploredfordifferenttypeofrobotictasks.Themodel-freenatureofRLmakesitsuitableintheproblemswherelittleornothingisknownabouttheenvironment.RLhasbeensuccessfullyimplementedingamesandhasshownbeyondhumanlevelperformance[3],[4].However,RLisadata-hungrymethodandoftenrequiresmoredatacomparedtoothersupervisedtechniquestogeneratecomparableresults.Therequirementofalargetrainingdata-sizeisoftenaddressedbytraininginasimulatedenvironment.However,iftheenvironmentisunknown,off-linetrainingpresentslowaccuracyandhighercrashrates.Sofar,limitedsuccesshasbeenachievedtraininginrealenvironments.Further,ensuringsafetyoftheagentduringtrainingisalsochallenging.Inthispaperweexploreasingle-camera-basedautonomousnavigationandobstacleavoidanceforMAVsinrealenvi-ronments.Traditionalsystemsemployhandcraftedsensingandcontrolalgorithmstoallownavigationandhasledtosigniﬁcantprogressinthisﬁeld[5],[6].Recently,thesuccessofdeepneuralnetworkshaveenticedresearcherstostudyneuromorphicmodelsofautonomousnavigation[7]–[9].Inspiteofthesuccessofsuchmachinelearningmodels,wealsorecognizethattrueautonomyinintelligentagentswillonlyemergewhenbio-mimeticsystemscanperformcontinuouslearningthroughinteractionswiththeenvironment.Themaincontributionsofthepaperareasfollows:•Demonstrationofend-to-endDeepRLforcollisionavoidanceusingmonocularimagesonlyandwithouttheuseofanyothersensingmodality.•Overcomingtheissuesassociatedwiththeimplementa-tionofRLinrealenvironmentsbydesigningasuitablerewardfunctionthattakesintoaccountboththesafetyandsensorconstraints.•UsingexpertdataandknowledgebaseddataaggregationtoimprovetheRLconvergenceinrealtime.II.RELATEDWORKOurprincipalgoalistoenabletheUAVtoﬂybyitselfinarealenvironment,withoutincurringanyadditionalhardwareorsensorcost.Mostofthelow-costMAVscomeequippedwithanon-boardcameraandInertialMeasuringUnit(IMU).Sotheuseofimageframesfornavigationisanareaofactiveresearch.Wehavestudiedsupervisedlearningfordronenav-igation.[10]collectsadata-setof11,500videosofcrashingandlearnsaneuralnetworkthatclassiﬁesanimageas“crash”or“nocrash”.Basedonthatknowledge,theauthorsuseahandcraftedalgorithmtosteerandnavigatethedroneawayfromobstacles.[11]usesanindoordata-setandclassiﬁestheimagesaccordingtotheactiontakenbythedrone.Theydeﬁneasetofﬁveactionsintheactionspaceofthedrone,henceposingtheproblemasaclassiﬁcationproblemwithﬁveclasses.Asupervisedimageclassiﬁerwiththreeclassesisusedin[12]totrainadeepneuralnetworkforforestnavigation.Thedata-setiscollectedbymountingthreecamerasonahiker’sheadfacingforward,leftandright.[13]usesRLastheonlinelearningmechanismtonavigateadroneinaforest.Acameraframeistakenandispre-processedbeforeitisfedtotheRLsystem.Thispre-processinguseshandcraftedalgorithmstoextractlowerdimensionalfeaturesfromthecameraimage. 
 
 
 
 
 
EnvironmentTraining data generationstst+1atCamera FramesFunctional blocksConvolutional Neural NetworkTraining Updatest+1strt st(st ,yt)RL NetworkDepth NetworkRewardCalculationReward GeneratedTraining DataFig.1.Blockdiagramofthekeyalgorithmiccomponentsthatenableend-to-endRLforobstacleavoidanceandautonomousﬂightinadrone.[14]usessimulatedenvironmentswithalargersetofactionspace(1681actions).Theagentistrainedforadeepneuralnetworkin9simulatedenvironmentsandtheperformanceisreported.Theneuralnetworktrainedinthesimulationsisthenalsotestedintherealenvironmentwithoutanyﬁne-tuningandhasshowntoperformwell.Unfortunately,theperformanceofthisapproachgreatlydependsuponthecorrelationofthesimulatedandrealenvironment.Forthecasesofunknownenvironmentswhichhaslimitedsimilaritywiththesimulatedtrainingenvironments,theagentisexpectedtobehavepoorly.Allofthesepreviousdemonstrationsandapproaches,inspiteoftheirmanysuccesses,eitherrequireconsiderablehuman/expertintervention,handcraftedalgorithmsorareim-plementedofﬂineinsimulations,wherethesimulatedandtherealendowmentsneedtobenearlyidentical.III.DEEPREINFORCEMENTLEARNING(DRL)A.BackgroundonReinforcementLearning(RL)TheideaofRListolearnacontrolpolicybyinteractingwiththeenvironment.Inthispaper,thegoalachievedthroughRListotakeactionsthatleadtoacollisionfreeﬂightofthedroneinarealenvironment.Thereisnogoalpositionandtheobjectiveistonavigatethroughthearenasafely.Considerthetaskofobstacleavoidancewherethedroneinteractswiththeenvironmentinasequenceofactions,observationsandrewardcalculations.Ateachtimeinstant,thedroneobservesthecurrentcameraframes.IttakesanactionafromanactionspaceAandimplementsit.Implementingtheactionmovesthedronetoanewpositionwhereitobservesanewcameraframes0.Thisnewcameraframealongwiththeactiontakenwillquantifyarewardr.Thegoalofthesystemistotakeactionsmaximizingthelongtermreward,i.e.ateachtimestept,weneedtotakeanactionthateventuallyleadsustoasequenceofstatessiwithrewardsrifori∈{t+1,t+2,...}suchthatthefuturediscountedreturnRt=∑Ti=tγi−triismaximized,whereγ∈[0,1]isthediscountfactor.Eachofthestate-actionpairisassignedaQvalueQ(s,a).DuringthelearningphasetheseQvaluesareupdatedaccordingtotheBellmanoptimalityequationasfollowsQ(s,a)=r+γmaxa0Q(s0,a0)(1)Bellmanequationupdateensuresthatinagivenstatestse-lectinganactionat=maxa0Q(st,a0)willresultinmaximizingthefuturediscountedrewardRt.TheseQvaluesarestoredasanapproximationofafunctionwithstatesasinput.InDeepReinforcementLearning(DRL)thefunctiontoestimatetheseQvaluesisadeepneuralnetwork.B.ChallengesofimplementingDRLinrealenvironmentsRLinrealenvironmentsforcollisionavoidanceischalleng-ing,aslistedbelow.Themethodologiesadoptedinthispapertoaddressthemaredescribedinthenextsection.1)Rewardgeneration:Inrealenvironments,thepositionoftheagentanditsdistancefromobstaclesisnotknown.Henceextrasensingcapabilitiesneedtobeaddedtotheagentgivingitanotionofdepthwhichnotonlyaddstothecomputationcostbutalsototheweightoftheagent.Inthispaper,wedemonstrateDRLusingasinglemonocularcamera.2)Safetyissues:RLworksviaatrialanderrormethod.Itisdesignedtolearnfrommistakes.Forthetaskofcollisionavoidance,itmeansthattheagenthastocollideintotheobstaclestolearn.Thiscollisioncannotonlyharmtheagent,butalsotheenvironment.Weproposeamethodofvirtualcrashandacrashrewardtoaddressthisissue.3)Resettingtheagenttoasuitableinitialposition:RLrequiresthattheagentmustbeplacedatproperinitialposition(usuallythesame)everytimeitcrasheswithanobstacle.Insimulations,itistriviallyachievedwhileinrealenvironmentsitposesachallenge.Wedemonstrateamethodofun-doingthedrone’sactionstoachievethesameeffectasresettingthedrone’sposition.4)Largeonlinedata-setrequirement:TheamountofdatarequiredforimplementingRLislarge.Suchtrainingdatarequirementstemsfromthefactthattheagentstartswithlittleknowledgeoftheenvironmentandtakesrandomactionstoexploreit.Asopposedtosimulationswhereyoucaneasilycollectalargenumberofdata-points,thedata-setthatcanbecollectedinarealenvironmentislimited.Weuseseveraltechniquestoaddressthisissue,asdescribedinthenextsection.IV.NAVIGATIONINREALENVIRONMENTSVIARL(NAVREN-RL)Weproposeanend-to-enddronenavigationmethodologyusingexpertdataaideddeepreinforcementlearningonimagesacquiredbyasinglecamera.Theend-to-endapproachhasbeensummarizedintheblockdiagramshowninFigure1.WelimittheactionspacetothreeactionsA={aF,aL,aR}whereundertheactionaFthedronemovesforward(by0.25m),aLthedroneturnsleft(45o)andaRthedroneturnsright(45o).Toaddresstheissuesofreal-timeDRL,weexplorethefollowingsolutionskeepinginmindtheagent’sweight,cost,limitedsensingcapabilities,andenvironmentalconstraints.A.RewardgenerationSincewearenotusinganyexternalsensingmodalities,therewardneedstobegeneratedfromtheimageframeitself.Fig.2.Depth-baseddynamicwindowingWeusethedepthmapofthestatetowardsthegenerationofthereward.Adepthmapofaframeisanimageofthesamedimensionwithpixelsintensitiescorrespondingtothedepthofthepixelintheinputimage.Inthelastfewyearsvariousoff-linelearningalgorithmshavebeenexploredtogeneratedepthmapsusingasingleimage[15]–[17].Duetoitssuperiortestaccuracy,weusetheapproachproposedin[16].Inorderfortherewardtobesimpleandmeaningful,weusepartsofthedepthmaptowardsrewardgeneration.Thedepthmapgeneratedisdividedintothreewindows.Thedepthinthewindowsisusedtogenerateanotionofthedistancetotheclosestobstacleineachofthethree(left,centerandright)di-rections.Thisdistanceiscalculatedbyaveragingthesmallestn%pixeldepthvalues.Thevalueofndependsonthenatureoftheenvironment.Iftheenvironmentisexpectedtohavenarrow(wide)obstacles,thevalueofnisrelativelysmaller(larger).Wenotethatchangingthewindowsizedynamicallywiththeglobaldepthinthesceneaidsrewardgenerationandimprovesaccuracy.Iftheglobaldepthoftheimageisgreater,thentheobjectsbeingseenintheframearefartherapart.Wechoosetherelationshipbetweentheglobaldepthandwindowsizeempiricallytobe[H,W]/(0.75×globaldepth+0.5)where[H,W]arethedimensionsoftheinputframefromcamera.ThisglobaldepthbaseddynamicwindowingcanbeseeninFigure2.ThethreelocaldistancestotheclosestobstacleincorrespondingdirectionsarethenputtousetowardsrewardgenerationaccordingtoAlgorithm1whereα∈[0,1]isaparametricweightandistakentobe1/3;dthreshisusedtomarkthecompletionofanepisodeasexplainedinthenextsection.B.AddressingsafetyissuesIfatanypoint,thecenterwindowshowsthedistancetothenearestobstacledctobebelowsomethresholdvaluedthresh,theagentstopsandconsiderstohave“virtuallycrashed”.Thisvirtualcrashingmarkstheendofanepisode.Thustheagentdoesnotphysicallycollidewithobstaclesandsigniﬁcantlyreducestheriskofdamagingitselfortheenvironment.Oncetheagentvirtuallycrashes,apenalizingrewardrcrashisprovidedtothestate-actionpairleadingtothecrash.Algorithm1Rewardgenerationusingthedepthmapfunctionfr(st,at,s0t)d(st)←depthmapofstd(s0t)←depthmapofs0tdl(st),dc(st),dr(st)=DepthValues(d(st))dl(s0t),dc(s0t),dr(s0t)=DepthValues(d(s0t))ifat=aFthenrt=dc(s0t)elseifat=aLthenrt=dc(s0t)+α(dl(st)−dr(st))elsert=dc(s0t)+α(dr(st)−dl(st))ifdc(s0t)<dthreshthenrt=rcrashreturnrtC.ResettingtheagenttoasuitableinitialpositionInourapproach,theagentdoesnotresettoitsinitialposition,ratheranewinitialpositionisselectedaftertheendofeveryepisode.Thenewinitialpositionischoseninanautonomouswaymakinguseoftheknowledgeofthe“virtualcrash”state-actionpair.Theactionthatledtothecollisionisun-done.Theagentaccomplishesthisbytakingtheoppositeactions(fore.g.iftheforwardactionledtovirtualcrash,theagentaftermarkingittheendofanepisode,movesbackward)untildcisatleastdrecover;athresholdsetforrecoveringfromthecrash.Anewepisodestartsfromtherecoveredstateandthepolicypreventstheagentfromselectingthe“virtualcrash”actionforthatinitialstate.D.Largeonlinedataa)ExpertDataDE:WeaddresstherequirementofalargetrainingdatasetbymakingtheuseofLearningfromDemonstration(LfD)[18].Attheonset,ahumanexpertnavigatestheagentacrossthearenaandcollectsalimitedsetofexpertdata-points.Theideaofcollectingexpertdata-pointsistohelptheagentthroughguidedexploration.Thisexpertdatasetisusedtowardslearninginthefollowingtwoways.•Pre-trainingphase:TheneuralnetworkistrainedforthissmallsetofexpertdataDEandtheweightslearnedθEareusedasinitialweightsfortheonlinelearningphase.Thispreservessomeknowledgeabouttheenvironmentandgivestheagentagoodstartingpointforexploration.•Expertdataasapartofexperiencereplay:TheexpertdataisalsousedasapartofthereplaymemoryDreplayfromwhichthebatchesofdata-pointsaresampledfortraining.Makingexpertdataapersistentpartoftheexperiencereplayhelpsavoidtheneuralnetworkfromforgettingwhatithadlearnedinthepre-trainingphase.b)KnowledgebasedDataaggregation:Thedataaggre-gationiscarriedoutinthefollowingtwoways:•Whentheagentvirtuallycrashes,goingforwardfromthatstatewillleadtoacrashtoo.Iftheagentwhichisinstatestmovestothenextstates0tbytakinganactionatandvirtuallycrashes,thenthedata-point(s0t,aF,s0t,rcrash)willbeaggregatedtothecurrentdatapoints.•Sinceoppositeactionsareselectedtorecoverfromcrashes,theintermediatestateswillleadtoacrashaswell.Forexample,theagentinstatestmovestonextstatest+1bytakinganactionatandvirtuallycrashes.Leta0tbetheoppositeactiontoat.Ifat∈{aR,aL}thenthedata-points(st+i,a0t,st+i−1,rcrash)and(st+i,aF,st+i,rcrash)fori={1,2,3,...,nrecover}isaggregatedtocurrentdata-pointswherenrecoveristhenumberofstepsrequiredtorecoverfromthecrash.SincegoingbackwarddoesnotbelongtoourdeﬁnedactionspaceA,thedata-pointsarenotaggregatedifat=aFE.ConvergenceofDeepRLalgorithmThebasicRLalgorithmoftensuffersfromlimitedconver-gence,whichmainlyemergesbecausetheBellmanequationtendstoover-estimatetheQ-valuesduetoitsnon-linearnature.Also,theaggregatingnatureoftheBellmanequationmightleadtodivergingQ-values.So,inordertoavoidtheseissuesthefollowingsolutionsareimplemented.1)Restrictingtherangeofrewards:Thedistancestothenearestobstacle{dl,dc,dr}∈R+istheestimateddistancesinmeters.Thesedistancesarescaleddowntohavevaluesbetween[0,1α+1]whereαistheweightconstantusedintherewardfunction.Whenscaleddown,therewardfunctiongeneratestherewardwithinthelimitedrangeof[−1,1]2)ClippingQvaluesinBellmanequation:ThisensuresthattheQ-valueupdatesdonotdiverge.LetQtargetθ(s,a)=r+γmaxaQ(s0,a;θ)bethenormalQ-valueupdatewhereθistheweightsoftheneuralnetwork,thentheclippedBellmanequationisˆQtargetθ(s,a)=clip(Qtargetθ(s,a),−1,1)(2)wherethefunctionclip(a,n1,n2)clipsthevalueton1orn2ifaislessthann1orgreaterthann2respectively.TheupdatedequationensuresthatQtargetθ(s,a)∈[−1,1]anddoesnotdiverge.3)UseofDoubleDQN:WeaddresstheoverestimationoftheQvaluebyusingaDoubleDeepQNetwork(DDQN)[19].InDDQNtwodifferentcopiesofneuralnetworkareused.Oneoftheneuralnetworks(thebehaviournetwork,θ)isusedfortraining,whiletheothernetwork(thetargetnetwork,θ0)isusedtowardstheBellmanequationupdate.Thetargetnetworkisupdatedwiththeweightsofthebehaviournetworkaftereveryntargetintervals.TheupdatedBellmanequationlookslikeQtargetθ0(s,a)=r+γmaxa0Q(s0,a0;θ0)(3)CombiningbothclippingandDDQN,theupdatedBellmanequationis:ˆQtargetθ0(s,a)=clip(r+γmaxaQ(s0,a;θ0),−1,1)(4)F.NetworkArchitectureWeuseamodiﬁedAlexNet[20]networktoestimatetheQvaluesforthestates.Theinputtothenetworkisthere-sizedcameraframest.Thenetworkconsistsof5convolutionallayersand3fully-connectedlayers.Algorithm2NAVREN-RLAlgorithmInput:Expertdata-points:DEInitialization:Behaviournetwork:Qθ(s)=N(s;θ),Targetnetwork:Qθ0(s)=N(s;θ0),m:Numberofpre-trainingup-dates,ntarget:Targetnetworkupdateinterval,bε:εannealingcoefﬁcient,nbatch:mini-batchsizefortrainingfori∈{1,2,3,...,m}doSampleamini-batchofsizenbatchfromDEEvaluatethelossJθ0(θ)PerformgradientdescenttominimizeJθ0(θ)w.r.tθInitializethereplaymemoryDreplay←DEfort∈{1,2,3,...}dost←Cameraimage,Q(st)←N(st;θ)Sampleanactionfrombehaviourpolicyat∼πbεQ(ε)Implementtheactionatontheagents0t←Cameraimage,Q(s0t)←N(s0t;θ)Generatetherewardrt←fr(st,at,s0t)Storethetuple(st,at,s0t,rt)inDreplayifvirtualcrashthenwhilenotrecoverfromcrashdoAggregatedata-pointstoDreplaySampleamini-batchofsizenbatchfromDreplayEvaluatethelossJθ0(θ)PerformgradientdescenttominimizeJθ0(θ)w.r.t.θiftmodntarget=0thenθ←θ0G.OnlineLearningBeforethelearningprocessbegins,anexpertusernavigatestheagentintheselectedenvironmentforacertainnumberofstepsnexpert.Thedatatuple(si,ai,s0i,ri)foreachofthestepsi∈{1,2,3,...,nexpert}isgeneratedandsavedinDE.Nextcomesthepre-trainingphasewhererandommini-batchesofsizenbatchareselectedfromtheexpertdataDEandaneuralnetworkQθ(s)=N(s;θ)istrainedminimizingJθ0(θ)=nbatch∑i=1J(si,ai,θ,θ0)+βJreg(θ)(5)whereJ(si,ai,θ,θ0)istheTDlossforithdata-pointdictatedbytheBellmanequationandJreg(θ)isregularizationlosstohelppreventover-ﬁttingthenetworkforthesmalleramountofexpertdata,andβisaregularizationweight.Theselossesaregivenby:J(si,ai,θ,θ0)=||ˆQtargetθ0(si,ai)−Q(si,ai;θ)||2(6)Jreg(θ)=||θ||2(7)whereˆQtargetθ0(st,at)isgivenbyequation4Afterthepre-trainingphase,theonlinetrainingphasebe-gins.Theagentisplacedintheenvironment,andfollowsaε-greedypolicyforactions.withbεastheannealingcoefﬁcient.εisvariedlinearlyfrom0.1to0.9asthenumberofdata-pointsvariesfrom1tobε.Ateverytimestept,thedronesavesTABLEILISTOFHYPERPARAMETERSUSEDFORTRAININGLearningrate1e-6ntarget200nbatch32β0.001dthresh0.02rcrash-1visivisiFig.3.Snapshotsandthelayoutsofthearenasused.Toprow:A1Hallway,Bottomrow:A2SC-roomthedatapoints(st,at,s0t,rt)inDreplay.Amini-batchofsizenbatchisrandomlysampledfromthereplaymemoryDreplayandusedtominimizethelossdeﬁnedinequation5throughgradientdescent.Algorithm2showsthecompletealgorithm,whiletableIliststhehyperparametersused.V.EXPERIMENTALRESULTSReal-timeexperimentationarecarriedouttovalidatetheproposedapproachfordronenavigation.A.HardwarespeciﬁcationsWeusealowcostParrotARdrone2.0whichdoesnothavethecomputationalpowertocarryouttherequiredprocessingon-board.Hence,thedronesendsthecameraframestoaworkstation/cloudequippedwithacorei7processorandGTX1080GPUs.Controlactionsarecommunicatedbacktothedrone.WeuseTensorﬂowtocarryouttheneuralnetworkcomputationontheworkstation.B.TestingenvironmentsWeusethefollowingtwoarenastocarryouttheexperi-mentationforsuccessfulnavigation.1)ArenaA1:OpenHallway:Thisisahallwayinanengi-neeringbuildingwithglasswalls.Thedronehastonavigatethroughthenarrowhallways(minimumwidthof≈1.5m).Therearenoextraobstaclesbetweenthehallwaypathexceptforwaterdispenser,benchesandtrashcans.2)ArenaA2:SCRoom:Thisarenaisaclutteredbreak-outroomwithcouches,chairs,tablesandbar-stoolswithnarrowpassagesinbetween(≈1m).ThelayoutandﬂoorplansofthesearenascanbeseenintheFigure3C.BaselineAlgorithmsforComparisonWecompareourmethodwiththefollowingbaselinealgo-rithms.0204060040008000DQN with no clipping010203040015003000DDQN with no clipping00.20.40.605001000DDQN with clippingRL lossNumber of iterationsFig.4.ConvergenceofRLwithandwithoutDDQNandclippingvisivisiA1 : HallwayA2 : SC-RoomSLLRSSSE2E-RLFig.5.TrajectoriesfollowedbythebaselinesandNAVREN-RLfor5differentinitiallocations1)Straight-linecontroller:Thiscontrolleralwayspredictstheforwardactionhencemovingtheagentinastraightlineinamannerdescribedin[14].Thiscontrollerprovidesagoodcomparisonofthecomplexityofthearena.2)Left-Right-Straight(LRS)controller:Thisbaselineisbasedontheworkin[13]whereasupervisedapproachisusedtoclassifyimageswithrespecttotheactionsrequiredtobetaken.Ahumanexpertroamsaroundthearenaandcollectstheimagesusingleft,rightandforwardfacingcameras.Imagescollectedfromleft(right)facingcameraarelabeledwiththetargetactionofright(left)whiletheonescollectedfromforwardfacingcameraarelabeledwiththetargetactionofforward.Theselabeledimagesarethenusedtotrainaneuralnetworkofﬂineinasupervisedmanner.3)Self-supervised(SS)controller:Thiscontrollerusestheworkproposedin[10]wherealargedata-setofcrashandsafeimagesarecollectedovervariousindoorenvironments.Theselabeledimagesarethenusedtotrainaneuralnetworktoclassifyeachimageaseithersafeorcrash.Intheinferencephase,theinputcameraframeisthendividedintothreewindowsandtheprobabilityofcrashineachofthesub-framesiscalculated.Basedontheseprobabilities,ahandcraftedcontrollerisdesigned,following[10]totakesuitableactions.D.PerformanceFigure4showsthecomparisonofRLconvergencewithandwithoutDDQNandclippingoftheBellmanequation.ItcanbeseenthattheDDQNwithclippingofBellmanequationshowsgoodconvergence.WeassesstheperformanceofNAVREN-RLbycomparingitagainstthebaselinesmentionedabove.3000(700expert+2300Fig.6.Safeﬂight(inmeters)comparisonacrossbaselinesTABLEIIARENASTATSArenaMethodTotalDi-stance(m)SafeFlight(m)Impro-vementHallwaySL80.716.14.45xLRS[13]162.932.62.21xSS[10]324.965.01.11xNAVREN-RL(ours)359.571.9−SC-roomSL6.31.34.55xLRS[13]10.92.22.65xSS[10]24.95.01.16xNAVREN-RL(ours)28.85.8−online)data-pointsarecollectedintheHallwayarena,while2000(600expert+1400online)data-pointsarecollectedintheSC-roomarena.Ineachofthearenas,allthe4techniquesareseparatelyusedtolearnaneuralnetwork.Theagentisinitializedbythelearnedneuralnetworkandtheperformanceisevaluatedbyplacingthedroneat5differentinitiallocations.Tokeepthecomparisonfair,theagentisplacedpreciselythesamewayacrossallthetechniques.Ineachofthecases,thetrajectoryfollowedbytheagentisrecordeduntiltheagentisnolongerabletonavigate.Thislossofnavigationisconsideredif•Theagentcollidesintoanobstacle•Theagentkeepshovering,beingstuckinarepetitivepatternofleft/rightactions,anddoesnotmoveforwardfor10iterationsThetrajectoriescanbeseeninFigure5.Thedistancecov-eredbytheagentbeforecrashistakentobetheperformancemetricandcanbeseeninthetableII.Thetotaldistancecoveredisthesumoftheindividualdistancescoveredbythedronefromeachoftheinitiallocations.Thesafeﬂightforanytechniqueistheaveragedistancecoveredbythedronefromthedifferentinitiallocations.InmostofthecasestheproposedNAVREN-RLmethodoutperformsthebaselines,i.ethesafeﬂight(m)fortheproposedRLmethodisthehighestamongthebaselines.ThiscanbeseeninFigure6.VI.CONCLUSIONSThispaperprovidesanend-to-endreinforcementlearningalgorithmforautonomousnavigationofdronesinindoorrealenvironmentsbyaddressingtheproblemsassociatedwiththeRLimplementation.Experimentationiscarriedoutindifferentarenasandtheperformanceiscomparedtootherbase-lines.Theresultsshowthattheagentisabletonavigateintheindoorarenawithlimitedsensingcapabilitiesanddata-pointswithcomparableperformance.ACKNOWLEDGMENTSThisworkwassupportedinpartbyC-BRIC,oneofsixcentersinJUMP,aSemiconductorResearchCorporation(SRC)programsponsoredbyDARPA.REFERENCES[1]Y.K.KwagandJ.W.Kang,“Obstacleawarenessandcollisionavoid-anceradarsensorsystemforlow-altitudeﬂyingsmartuav,”inDigitalAvionicsSystemsConference,2004.DASC04.The23rd,vol.2.IEEE,2004,pp.12–D.[2]A.S.L.Raimundoetal.,“Autonomousobstaclecollisionavoidancesystemforuavsinrescueoperations,”Ph.D.dissertation,2016.[3]D.Silver,A.Huang,C.J.Maddison,A.Guez,L.Sifre,G.VanDenDriessche,J.Schrittwieser,I.Antonoglou,V.Panneershelvam,M.Lanctotetal.,“Masteringthegameofgowithdeepneuralnetworksandtreesearch,”nature,vol.529,no.7587,p.484,2016.[4]V.Mnih,K.Kavukcuoglu,D.Silver,A.A.Rusu,J.Veness,M.G.Bellemare,A.Graves,M.Riedmiller,A.K.Fidjeland,G.Ostrovskietal.,“Human-levelcontrolthroughdeepreinforcementlearning,”Nature,vol.518,no.7540,p.529,2015.[5]D.O.Sales,P.Shinzato,G.Pessin,D.F.Wolf,andF.S.Osorio,“Vision-basedautonomousnavigationsystemusingannandfsmcontrol,”inRoboticsSymposiumandIntelligentRoboticMeeting(LARS),2010LatinAmerican.IEEE,2010,pp.85–90.[6]R.Huang,P.Tan,andB.M.Chen,“Monocularvision-basedautonomousnavigationsystemonatoyquadcopterinunknownenvironments,”inUnmannedAircraftSystems(ICUAS),2015InternationalConferenceon.IEEE,2015,pp.1260–1269.[7]C.D.Schuman,T.E.Potok,R.M.Patton,J.D.Birdwell,M.E.Dean,G.S.Rose,andJ.S.Plank,“Asurveyofneuromorphiccomputingandneuralnetworksinhardware,”arXivpreprintarXiv:1705.06963,2017.[8]C.RichterandN.Roy,“Safevisualnavigationviadeeplearningandnoveltydetection,”inProc.oftheRobotics:ScienceandSystemsConference,2017.[9]L.Tai,S.Li,andM.Liu,“Autonomousexplorationofmobilerobotsthroughdeepneuralnetworks,”InternationalJournalofAdvancedRoboticSystems,vol.14,no.4,p.1729881417703571,2017.[10]D.Gandhi,L.Pinto,andA.Gupta,“Learningtoﬂybycrashing,”arXivpreprintarXiv:1704.05588,2017.[11]D.K.KimandT.Chen,“Deepneuralnetworkforreal-timeautonomousindoornavigation,”arXivpreprintarXiv:1511.04668,2015.[12]A.Giusti,J.Guzzi,D.C.Cires¸an,F.-L.He,J.P.Rodr´ıguez,F.Fontana,M.Faessler,C.Forster,J.Schmidhuber,G.DiCaroetal.,“Amachinelearningapproachtovisualperceptionofforesttrailsformobilerobots,”IEEERoboticsandAutomationLetters,vol.1,no.2,pp.661–667,2016.[13]S.Ross,N.Melik-Barkhudarov,K.S.Shankar,A.Wendel,D.Dey,J.A.Bagnell,andM.Hebert,“Learningmonocularreactiveuavcontrolinclutterednaturalenvironments,”inRoboticsandAutomation(ICRA),2013IEEEInternationalConferenceon.IEEE,2013,pp.1765–1772.[14]F.SadeghiandS.Levine,“Cad2rl:Realsingle-imageﬂightwithoutasinglerealimage,”arXivpreprintarXiv:1611.04201,2016.[15]A.Saxena,S.H.Chung,andA.Y.Ng,“3-ddepthreconstructionfromasinglestillimage,”Internationaljournalofcomputervision,vol.76,no.1,pp.53–69,2008.[16]I.Laina,C.Rupprecht,V.Belagiannis,F.Tombari,andN.Navab,“Deeperdepthpredictionwithfullyconvolutionalresidualnetworks,”in3DVision(3DV),2016FourthInternationalConferenceon.IEEE,2016,pp.239–248.[17]C.Godard,O.MacAodha,andG.J.Brostow,“Unsupervisedmonoculardepthestimationwithleft-rightconsistency,”inCVPR,vol.2,no.6,2017,p.7.[18]T.Hester,M.Vecerik,O.Pietquin,M.Lanctot,T.Schaul,B.Piot,D.Horgan,J.Quan,A.Sendonaris,G.Dulac-Arnoldetal.,“Deepq-learningfromdemonstrations,”arXivpreprintarXiv:1704.03732,2017.[19]H.VanHasselt,A.Guez,andD.Silver,“Deepreinforcementlearningwithdoubleq-learning.”inAAAI,vol.2.Phoenix,AZ,2016,p.5.[20]A.Krizhevsky,I.Sutskever,andG.E.Hinton,“Imagenetclassiﬁcationwithdeepconvolutionalneuralnetworks,”inAdvancesinneuralinfor-mationprocessingsystems,2012,pp.1097–1105.