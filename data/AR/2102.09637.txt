1
2
0
2

b
e
F
8
1

]

R
P
.
h
t
a
m

[

1
v
7
3
6
9
0
.
2
0
1
2
:
v
i
X
r
a

Explicit Bivariate Rate Functions for Large Deviations in AR(1)
and MA(1) Processes with Gaussian Innovations

M.J. Karling, A.O. Lopes and S.R.C. Lopes ‡

Mathematics and Statistics Institute
Federal University of Rio Grande do Sul
Porto Alegre, RS, Brazil

February 22, 2021

Abstract

We investigate large deviations properties for centered stationary AR(1) and MA(1) processes with in-
dependent Gaussian innovations, by giving the explicit bivariate rate functions for the sequence of ran-
dom vectors (Sn)n∈N = (cid:0)n−1(P
k )(cid:1)n∈N. In the AR(1) case, we also give the explicit
rate function for the bivariate random sequence (W n)n>2 = (cid:0)n−1(P
k=1 X 2
k=2 XkXk+1)(cid:1)n>2.
Via Contraction Principle, we provide explicit rate functions for the sequences (n−1 P
n
k=1 Xk)n∈N,
(n−1
n
k=2 XkXk+1)n>2, as well. In the AR(1) case, we present a new proof
for an already known result on the explicit deviation function for the Yule-Walker estimator.

k )n>2 and (n−1

n
k=1 Xk, P

k=1 X 2

k=1 X 2

k , P

P

P

n

n

n

n

Keywords: Large Deviations; Empirical Autocovariance; Quadratic and Sample Means; Autore-
gressive Processes; Moving Average Processes; Yule-Walker Estimator

2010 Mathematics Subject Classiﬁcation: 60F10; 60G10; 60G15; 11E25; 62F12; 62M10

1

Introduction

Since the ﬁrst establishments on the Large Deviations theory, there has been a great expansion of the
number of surveys on Large Deviations Principles (LDP). Nowadays, we can ﬁnd a variety of examples
applied to the time series analysis and stochastic processes in general; for instance, LDPs for Stable laws
(see, e.g. Heyde [22], Rozovskii [31], Rozovskii [32] and Zaigraev [36]), stationary Gaussian processes
(see, e.g. Bercu et al. [3], Bercu et al. [4], Bryc and Dembo [9], Donsker and Varadhan [17] and Zani
[37]), autoregressive and moving average processes (see, e.g. Bercu [2], Bryc and Smolenski [10], Burton
and Dehling [12], Djellout and Guillin [16], Macci and Trapani [25], Mas and Menneteau [27], Miao [29]
and Wu [35]) and continuous processes (see, e.g. Bercu and Richou [5] and Bercu and Richou [6]).

When considering the Empirical Autocovariance function

˜γn(h) =

1
n

h

n

−

Xk=1

XkXk+h,

for 0 6 h 6 d and d

N,

∈

‡Corresponding author. E-mail: silviarc.lopes@gmail.com

 
 
 
 
 
 
2

Explicit Rate Functions for LDP

N, few results on LDP are known. Regarding Gaussian distributions, one of the ﬁrst
of a process (Xn)n
studies in the literature is the one from Bryc and Smolenski [10], concerning the LDP for the Quadratic
Mean

∈

˜γn(0) =

1
n

X 2
k.

n

Xk=1

h=0 is available when (Xn)n
∈

Bryc and Dembo [9] showed that an LDP for the vector (˜γn(h))d
N is an
(0, 1). It is well known that most of
independent and identically distributed (i.i.d.) process, with Xn ∼ N
the relevant stochastic processes are not independent and, as the authors have claimed, their approach
needs some adjustments when trying to show that a similar LDP works, for instance, when dealing with
the classical centered stationary Gaussian AR(1) process (see example 1 in Bryc and Dembo [9]). On the
other hand, Bercu et al. [4] proved an LDP for Toeplitz quadratic forms of centered stationary Gaussian
processes in an univariate setting. Their survey eliminated the need for the variables of (Xn)n
N to be
independent, extending the result in Bryc and Dembo [9] by including the AR(1) process. However, it is
not clear if the LDP was available even for the bivariate random vector (˜γn(0), ˜γn(1)), once the LDP has
only been proved for each one of the components separately. More precisely, the results in Bercu et al.
[4] only cover the LDP of the random variable

∈

Wn =

1
n

X (n)T

Mn X (n),

denoting the transpose of X (n), and where (Mn)n
∈

N is a sequence

where X (n) = (X1,
of n

n Hermitian matrices.

· · ·

, Xn), with X (n)T

×
In a more general setting, Carmona et al. [13] present a level-1 LDP for the empirical autocovariance
function of order h for any innovation processes, that encompasses the AR(d) process with Gaussian
innovations. In this paper, the authors used the level-2 LDP together with the Contraction Principle.
The process itself is obtained from iterations of a continuous uniquely ergodic transformation, preserving
the Lebesgue measure on the circle. In Carmona and Lopes [14], the authors considered a similar problem
where the dynamics are given by an expanding transformation on the circle. In the same line of research,
Wu [35] proved an LDP for (˜γn(h))d
n)) is ﬁnite, for λ > 0, where
(εn)n

N is the white noise of an AR(d) process, excluding in turn the Gaussian case.

h=0 under the assumption that E(exp(λε2

∈

In the present manuscript, we take into account the studies from Bercu et al. [4] and Bryc and Dembo

[9] to give a proof that the sequence (W n)n>2, given by

W n =

1
n

(˜γn(0), ˜γn(1)) =

1
n  

n

n

X 2
k,

Xk=1

Xk=2

XkXk

,

1

−

!

for n > 2,

does, in fact, satisfy an LDP when (Xn)n
N is a centered stationary Gaussian AR(1) process and we
present its explicit bivariate rate function. The asymptotical behavior of the sequence (W n)n>2 is well
known (see Brockwell and Davis [8]), that is

∈

−
By deﬁnition of almost sure convergence, as n

(cid:19)

−
, the sequence of probabilities

W n

n
→∞
−−−−→

1

(cid:18)

1

θ2 ,

1

θ

θ2

,

almost surely.

→ ∞
1

1

(cid:18)

−

θ2 ,

W n −

P

(cid:18) (cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

θ

θ2

1

−

> δ

,

(cid:19)

(cid:19)(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(1.1)

M.J. Karling, A.O. Lopes and S.R.C. Lopes

3

1
θ2 ,

1

−

θ
θ2

1

−

(cid:16)

(cid:17)

converges to zero, for all δ > 0. However, if the convergence of these probabilities is very slow, even for
large n, we have a certain reasonable chance of choosing a bad sample X1,
N, such
that W n is distant from the true value

, Xn from (Xn)n

· · ·

∈

.

The Large Deviations theory considers the asymptotic behavior of the probabilities presented in (1.1),
ensuring that they converge to zero approximately in exponential rate (see chapter 1 in Bucklew [11]).
Its usual deﬁnition is given as follows (see Dembo and Zeitouni [15]).

Deﬁnition 1.1. A sequence of random vectors (V n)n
∈
Principle (LDP) with speed n and rate function J(·), if J(·) : Rd
function such that,

N of Rd, for d

→

∈
[0,

N, satisﬁes a Large Deviation
] is a lower semi-continuous

∞

• Upper bound: for any closed set F

Rd,

⊂

1
n

lim sup
n

→∞

log P (V n ∈

F ) 6

J(x);

inf
x
F
∈

−

• Lower bound: for any open set G

Rd,

⊂

inf
x
G
∈

J(x) 6 lim inf
→∞

n

−

1
n

log P (V n ∈

F ) .

Moreover, J(·) is said to be a good rate function if its level sets J −

1([0, b]) are compact, for all b

R.

∈

Remark 1. In this work, we only deal with good rate functions, but for short, we sometimes write rate
function instead.

In general, it is not easy to prove that an arbitrary sequence of random vectors satisﬁes an LDP
(see, e.g. Bercu and Richou [6], Bryc and Dembo [9], Dembo and Zeitouni [15], Ellis [18], Macci and
Trapani [25] and Mas and Menneteau [27]). An elegant way of proving such property is to verify the
validity of the G¨artner-Ellis’ theorem conditions (see theorem 2.3.6 in Dembo and Zeitouni [15]), which
is a counterpart to the very well known Cram´er-Chernoﬀ’s theorem (see theorem 2.2.30 in Dembo and
Zeitouni [15]). It is worth mentioning that, within the conditions of the G¨artner-Ellis’ theorem, little use
of the dependency structure is made and the focus mainly rests in the behavior of the limiting cumulant
generating function, deﬁned by

where Ln(·) : R2

R

→

∪ {∞}

denotes the normalized cumulant generating function of W n,

L(λ) = lim
→∞

n

Ln(λ), for all λ

R2,

∈

Ln(λ) =

1
n

log E [exp (n

λ, W ni
h

)] .

We shall present an explicit expression for L(·) in the case λ = (λ1, λ2) depends on two variables. As a
result, we obtain the explicit rate function through the Fenchel-Legendre transform of L(·).

In the second part of our study, we shall analyze the LDP of the sequence of bivariate random vectors

(Sn)n
∈

N, where

Sn =

1
n  

n

n

Xk,

Xk=1

Xk=1

X 2
k

.

!

4

Explicit Rate Functions for LDP

We shall call Sn as the bivariate SQ-Mean, for short, since its ﬁrst and second components are, respec-
tively, the Sample Mean and the Quadratic Mean. We dedicate our study to the particular cases when
(Xn)n
N follows an AR(1) or an MA(1) process. This study is based on a particular result presented in
Bryc and Dembo [9] and which has a very interesting application when the Contraction Principle can be
applied.

∈

Our study is organized as follows. Section 2 is dedicated to the proof of the LDP and computations of
the explicit rate function for the random sequence (W n)n>2, under the assumption that (Xn)n
N follows
∈
an AR(1) process. In Section 3, we obtain the LDP for some particular cases, namely, the Quadratic
Mean and the ﬁrst order Empirical Autocovariance of a random sample X1,
, Xn from the AR(1)
process. Moreover, the LDP for the Yule-Walker estimator is provided likewise. As a direct application
of the studies in Section 2, we dedicate Section 4 to show that the LDP for the SQ-Mean (Sn)n
N of an
∈
AR(1) process is available. Next, we give the details of the LDP for the Quadratic Mean of an MA(1)
process and, as a consequence, the LDP for the SQ-Mean. Section 5 gives insights on future work and
concludes the manuscript.

· · ·

2 LDP and the centered stationary Gaussian AR(1) process

Consider the autoregressive process (Xn)n

∈

N deﬁned by the equation

Xn+1 = θXn + εn+1,

for

θ

|

|

< 1 and n

N,

∈

(2.1)

where (εn)n>2 is a sequence of i.i.d. random variables, with εn ∼ N
X1 is independent of (εn)n>2, with
N
Gaussian AR(1) process with (positive) spectral density function deﬁned as

θ2)) distribution. Then (Xn)n
∈

(0, 1/(1

−

(0, 1), for all n > 2. Assume that
N is a centered stationary

gθ(ω) =

1 + θ2

1
2θ cos(ω)

−

, ω

T = [

π, π).

−

∈

Throughout this section, we shall study the existence of an LDP for the random vector

W n =

1
n  

n

n

X 2
k ,

Xk=1

Xk=2

XkXk

.

1

−

!

(2.2)

(2.3)

Consider λ = (λ1, λ2)

R2. Let Ln(
·

,

∈
function associated to the sequence (W n)n>2, deﬁned by

→

·

) : R2

R represent the normalized cumulant generating

Ln(λ1, λ2) =

log E

en
h

(λ1,λ2),Wn

i

,

for n > 2,

(2.4)

1
n

(x1, y1), (x2, y2)
i
h

where
G¨artner-Ellis’ theorem, which requires the convergence of Ln(
·

), as n

,

·

.
→ ∞

(cid:17)
:= x1x2 + y1y2 denotes the usual inner product in R2. We want to apply the

(cid:16)

M.J. Karling, A.O. Lopes and S.R.C. Lopes

5

2.1 Analysis of the normalized cumulant generating function

We shall present below, the expression for the limiting function L(
·
,
))n>2. In particular, we use the function L(
functions (Ln(
·
·
in order to obtain the rate function of the sequence (W n)n>2.

·

,

·

,

), when n

, of the sequence of
) by applying the G¨artner Ellis’ theorem

→ ∞

·

, Xn) and X (n)T

· · ·

denoting the transpose of X (n), note that, one can rewrite

With X (n) = (X1,

(2.3) as

W n =

where ϕ1 : T

1

}

→ {

and ϕ2 : T

→

1
n
[

X (n)T

Tn(ϕ1)X (n), X (n)T

Tn(ϕ2)X (n)

,

(2.5)

(cid:16)
1, 1] are real valued functions, given respectively by
−

(cid:17)

ϕ1(ω) = 1

and

ϕ2(ω) = cos(ω).

The matrix Tn(f ) represents the Toeplitz matrix associated to the function f : T
by

→

R, which is deﬁned

Tn(f ) =

ei (j

k)ωf (ω) dω

−

.

(cid:21)16j,k6n

1
2π

(cid:20)

T

Z

Remark 2. A vast literature comprehending Toeplitz matrices has emerged in the last century and one of
the most famous and referenced works is given in Grenander and Szeg¨o [21]. A modern treatment about
this subject may be found in Gray [20] and in Nikolski [30].

Inserting (2.5) into (2.4), we obtain

Ln(λ1, λ2) =

1
n

and, by linearity of Toeplitz matrices, we get

log E

eX (n)T

(λ1Tn(ϕ1)+λ2Tn(ϕ2))X (n)

(cid:16)

(cid:17)

where ϕλ : T

→

R is deﬁned by

Ln(λ1, λ2) =

1
n

log E

eX (n)T

Tn(ϕλ)X (n)

(cid:16)

,

(cid:17)

ϕλ(ω) = λ1ϕ1(ω) + λ2ϕ2(ω) = λ1 + λ2 cos(ω).

Observe that ϕλ(
·

) depends on the choice of λ = (λ1, λ2) and that

(2.6)

(2.7)

Tn(ϕλ) =

2λ1

λ2

0

λ2

2λ1

λ2

0

0

0

0
...
0

λ2

0
. . .

· · ·

2λ1 λ2
. . .
. . .
0

λ2
. . .
0

1
2
















· · ·
. . .
. . .
. . .

0
...

0

0

2λ1
λ2

λ2
2λ1

.
















The fact that X (n) has multivariate Gaussian distribution gives us some advantage here. A standard
result from Probability theory (see section B.6 in Bickel and Doksum [7]) shows that there is always a
multivariate Gaussian vector Y (n) = (Yn,1,

, Yn,n) with independent components, such that

· · ·

X (n) = Tn(gθ)1/2 Y (n),

(2.8)

6

Explicit Rate Functions for LDP

) is given in (2.2) and Tn(gθ)1/2 is the square root matrix of Tn(gθ). We also note that
N. Therefore,
N is the sequence of autocovariance matrices associated to the process (Xn)n

where gθ(
·
(Tn(gθ))n
since Tn(gθ) is a positive deﬁnite matrix, the sequence of matrices (Tn(gθ)1/2)n
∈

∈
N is well deﬁned.

∈

From (2.8) we obtain

X (n)T

Tn(ϕλ) X (n) = Y (n)T

Tn(gθ)1/2 Tn(ϕλ) Tn(gθ)1/2 Y (n).

(2.9)

∈

Since Tn(gθ)1/2 Tn(ϕλ) Tn(gθ)1/2 is a real symmetric matrix, there exists a sequence of orthogonal matrices
(Pn)n

N such that

with Λn = Diag(αλ

n,1,

· · ·

Tn(gθ)1/2 Tn(ϕλ) Tn(gθ)1/2 = Pn Λn P T
n ,
n,k)n

n matrix, where (αλ

n,n) a diagonal n

, αλ

×

k=1 are the eigenvalues of

(2.10)

T 1/2
n (gθ)Tn(ϕλ)T 1/2

n (gθ).

Remark 3. It is interesting to note that (αλ

n,k)n

k=1 are also the eigenvalues of Tn(ϕλ) Tn(gθ).

From (2.9) and (2.10) we obtain

Y (n)T

Tn(gθ)1/2 Tn(ϕλ) Tn(gθ)1/2 Y (n) = Y (n)T

Pn Λn P T

n Y (n).

(2.11)

As Pn is orthogonal, the product P T

n Y (n) has a multivariate Gaussian distribution with independent

components. From (2.9) and (2.11), it is easy to conclude that

X (n)T

Tn(ϕλ) X (n) =

αλ
n,kZn,k,

n

Xk=1

(2.12)

where Z1,n,
ating function given by

· · ·

, Zn,n are i.i.d. random variables, each one having a χ2

1 distribution with moment gener-

MZn,k (t) = E(etZn,k ) = 


(1

−

1
2t)1/2 ,
,
∞

t < 1
2 ,

t > 1
2 ,

(2.13)

for k = 1,

, n.

· · ·



Returning to the analysis of (2.6) and considering (2.12), as Z1,n,

we conclude that

, Zn,n are mutually independent,

· · ·

Ln(λ1, λ2) =

1
n

log E

(cid:16)

n
k=1 α

λ
n,kZn,k

eP

=

1
n

log

(cid:17)

n

E

λ
n,kZn,k

eα

Yk=1

(cid:16)

.

!

(cid:17)

(2.14)

From (2.13), we observe that E
(2.14) is ﬁnite if

λ
n,kZn,k

eα

is only deﬁned if each one of the αλ

n,k < 1/2. In other words,

(cid:16)
0 < 1

(cid:17)
n,k,

2αλ

−

for all k such that 1 6 k 6 n.

(2.15)

The condition in (2.15) is equivalent to requiring that In −
(see Bercu et al. [4]). Since Tn(gθ) is a positive deﬁnite matrix and

2 Tn(ϕλ)Tn(gθ) must be positive deﬁnite

In −

2 Tn(ϕλ)Tn(gθ) = (T −

1
n (gθ)

2Tn(ϕλ)) Tn(gθ),

−

 
M.J. Karling, A.O. Lopes and S.R.C. Lopes

it is suﬃcient to show that

r1

q

Dn,λ = T −

1
n (gθ)

2Tn(ϕλ) =

−

0
. . .
. . .
. . .
0

θ



q

p
. . .
. . .

0
...
0










2λ1 and q =

· · ·

0
...



· · ·
. . .
. . .

p
q

0










λ2. The domain

q
r1

7

(2.16)

R2, where

D ⊆

is positive deﬁnite, where r1 = 1
Dn,λ (and so In −
Lemma 2.1. If the pair (λ1, λ2) belongs to the domain

2λ1, p = 1 + θ2

−
2Tn(ϕλ)Tn(gθ)) is positive deﬁnite, is given by the following lemma.

−

−

−

D

=

D1 ∪ D2, where
, 4(θ + λ2)2 < (1 + θ2

R2

λ1 6

θ2

1

−
2

(cid:12)
(cid:12)

1

R2

θ2

−
2

< λ1 <

1
2

, (θ + λ2)2 < θ2(1

−

2λ1)2

−

2λ1)

(cid:27)

(cid:27)

,

,

(2.17)

D1 =

D2 =

(λ1, λ2)

(cid:26)

(λ1, λ2)

(cid:26)

∈

∈

(cid:12)
(cid:12)

then, for n large enough, the tridiagonal matrix Dn,λ, given in (2.16), is positive deﬁnite.

Proof. The proof is given in Appendix A.

To illustrate the domains presented in Lemma 2.1, Figures 2.1 and 2.2 show the graphs of

when
D2 separately, while Figure 2.2 shows the union

D

θ = 0.9. In particular, Figure 2.1 shows the sets

=

D1 ∪ D2.

D

λ2

D1 and

1

2

2 1
(cid:14) θ
2
(cid:17)θ (cid:18) λ2)
1
2
2 1

(cid:22) θ

 (cid:15) λ1
2

(cid:19) θ
 (cid:23) λ1

(cid:20)

λ2

1

(cid:16)

1

(cid:21)

2
2 λ1)
1
∧
θ

2

(cid:24)

(cid:25)

(cid:26)

2
λ2)

θ2

1

(cid:27)

(cid:28)

(cid:29)

2 λ1)

λ1

2

0

2

(cid:13)

4

(cid:12)

1

λ1 ≤
4

2 1
(θ + λ2
1
2 1

λ1 ≤

)

2

2

- θ

< 1
2

(cid:2) θ

2

(cid:1)

2

2 λ1
2

(cid:3)θ (cid:4) λ2

(cid:5)

(cid:0) θ
 ∧ 4

(cid:6) 1

(cid:7) θ

2

2 λ1

(cid:8)

2

λ1

2

0

2

"

4

!

3

2

1

0

1

2

3

3

2

1

0

1

2

3

(cid:9)

(cid:10)

(cid:11)

(cid:30)

(cid:31)

Figure 2.1: Regions
θ = 0.9.

D1 (in the left) and

D2 (in the right) deﬁned in (2.17) in the particular case when

The knowledge of the domain where the matrix Dn,λ is positive deﬁnite, allows one to give continuity
. It is shown in Bryc and Dembo

) and its limiting function when n

to the computations of Ln(
·
[9] (see page 330), for the special case θ = 0, that

·

,

→ ∞

lim
n
→∞

Ln(λ1, λ2) = 


1
2 log

−

,
∞

(cid:18)



1

−

2λ1+√(1
−
2

2λ1)2

4λ2
2

−

,

if (λ1, λ2)

∈ Dϕ,

otherwise,

(cid:19)

 
8

Explicit Rate Functions for LDP

λ2

λ1

2

0

-2

-4

-3

-2

-1

0

1

2

3

Figure 2.2: Region

=

D1 ∪ D2, when θ = 0.9.

D

where

Dϕ =

{

(λ1, λ2)

R2

|

∈

sup
T
ω

∈

ϕλ(ω) < 1/2

.

}

Even though representing a particular degenerate case, it is important to note such result. If θ = 0, the
process (Xn)n
N in (2.1) reduces itself to an i.i.d. sequence of random variables with standard Gaussian
distribution. We shall generalize the result in Bryc and Dembo [9] on a bivariate setting, for the case
when θ

= 0.

∈

Lemma 2.2. Let Ln(
·

,

·

) denote the normalized cumulant generating function of (W n)n>2, then

Ln(λ1, λ2) = L(λ1, λ2),

lim
n
→∞

is deﬁned by

(2.18)

1 + θ2

2λ1 +

−

(1 + θ2
2

p

2λ1)2

−

−

4(θ + λ2)2

,

!

for (λ1, λ2)

,

∈ D

(2.19)

∪ {∞}
1
2

log

−

,
∞
given in Lemma 2.1.

otherwise,

where L : R2

R

→

L(λ1, λ2) = 


with the domain
D

n,k)n

Proof. Let (αλ
spectral density function, deﬁned in (2.2), and ϕλ(
·
2.1 guarantees that αλ
follows that

k=1 represent the sequence of eigenvalues of Tn(ϕλ) Tn(gθ), with gθ(
·

) denoting the
, Lemma
n,k < 1/2, for all 1 6 k 6 n and n large enough. Then, from (2.13) and (2.14) it

) the function given in (2.7). If (λ1, λ2)

∈ D

Ln(λ1, λ2) =

1
2n

−

n

Xk=1

log

1

−

2αλ
n,k

,

(cid:0)

(cid:1)

for (λ1, λ2)

∈ D

and n large enough.

(2.20)

6
 
M.J. Karling, A.O. Lopes and S.R.C. Lopes

9

and (αλ

n,n)n
∈

∈ D

Nonetheless,

if (λ1, λ2) /

N. In that case, we have MZnj ,nj

Tn(ϕλ) Tn(gθ), we can always ﬁnd a subsequence (αλ
j
Henceforth, we only need to take care when (λ1, λ2) belongs to
ﬁnite for n large enough and it is given by (2.20).

nj ,nj )j
∈
, for all j

nj ,nj ) =

(αλ

∞

∈

D

N of (αλ

N represents the sequence of maximum eigenvalues of
> 1/2, for all
.
∞
→∞
, because in this case, Ln(λ1, λ2) is

n,n)n
∈
N, implying that limn

N such that αλ

Ln(λ1, λ2) =

nj ,nj

∈

Consider in what follows the measure space L∞(T) := L∞(T,

measure acting on

B

(T), the Borel σ-algebra over T. If h

∈

(T), ν), were ν(
) is the Lebesgue
·
h

=

L∞(T), the usual norm

B

||

||∞

{
∈ B
shall be considered. Since ϕλ, gθ ∈

|

inf

h∗

S(N )

N

(T), ν(N ) = 0

, with h∗

}

S(N ) = sup

{|

h(x)
|

: x /
∈

N

,

}

L∞(T), it is straightforward to show that (see Avram [1])

αλ
n,k|

|

6

ϕλ

||

gθ||∞

,

||∞||

for all 1 6 k 6 n and n

N.

∈

(2.21)

Let mϕλ gθ and Mϕλ gθ denote, respectively, the essential inﬁmum and essential suppremum (see
R, belonging to L∞(T) and deﬁned

Grenander and Szeg¨o [21]) of the continuous mapping ϕλ gθ : T
by

→

(ϕλ gθ)(ω) = ϕλ(ω) gθ(ω) =

.

(2.22)

λ1 + λ2 cos(ω)

1 + θ2

2θ cos(ω)

−

The function (ϕλ gθ)(
·
in that interval. It follows that

) is continuous and bounded in [

−

π, π], hence it attains a maximum and a minimum

mϕλ gθ = min
T{
∈

ω

(ϕλ gθ)(ω)
}

and Mϕλ gθ = max
T {

ω

∈

(ϕλ gθ)(ω)
}

.

Since

d
dω

(ϕλ gθ)(ω) =

we notice that (ϕλ gθ)(ω) has two critical points at ω =

λ2 sin(ω)

2θ(λ1 + λ2 cos(ω)) sin(ω)

−

1 + θ2

2θ cos(ω) −

−

(1 + θ2

2θ cos(ω))2

−
π and ω = 0. Moreover,

,

−

• if λ2 <

• if λ2 >

• if λ2 =

−

−

−

2θλ1/(1 + θ2), then d2

dω2 (ϕλ gθ)(

2θλ1/(1 + θ2), then d2

dω2 (ϕλ gθ)(

π) < 0 and d2

dω2 (ϕλ gθ)(0) > 0;

π) > 0 and d2

dω2 (ϕλ gθ)(0) < 0;

−

−

2θλ1/(1 + θ2), then (ϕλ gθ)(ω) = λ1 is constant.

Therefore, since

we conclude that

(ϕλ gθ)(

−

π) =

λ2

λ1 −
1 + θ2 + 2θ

and (ϕλ gθ)(0) =

λ1 + λ2

1 + θ2

2θ

−

,

mϕλ gθ =

λ1+λ2

2θ ,
1+θ2
−
λ2
λ1−
1+θ2+2θ ,




if λ2 <
if λ2 >

2θλ1
1+θ2 ,
2θλ1
1+θ2 ,

−

−

and Mϕλ gθ =

Considering the case in which (λ1, λ2)

, it follows that



∈ D
θ + λ2|

2

|

< 1 + θ2

2λ1,

−

λ1−
λ2
1+θ2+2θ ,
λ1+λ2
2θ ,

1+θ2

−

if λ2 <
if λ2 >

2θλ1
1+θ2 ,
2θλ1
1+θ2 .

−

−






10

whence

Explicit Rate Functions for LDP

1 + θ2
−
2

2λ1

(cid:19)

−

(cid:18)

< θ + λ2 <

1 + θ2
−
2

2λ1

.

(2.23)

From the left-hand side of (2.23), we get

λ2

λ1 −
1 + θ2 + 2θ

<

1
2

,

while from the right-hand side of (2.23), we obtain

Hence, we conclude that Mϕλ gθ < 1/2. On the other hand, from

λ1 + λ2

1 + θ2

2θ

−

<

1
2

.

ϕλ

||∞||
we conclude that mϕλ gθ >

||

= max

Mϕλ gθ |
. Therefore, we just proved that

mϕλ gθ |}

{|

|

,

>

mϕλ gθ ,

−

gθ||∞
ϕλ

−||

>

||

ϕλ gθ||∞
gθ||∞
[mϕλ gθ , Mϕλ gθ ]

||∞||

ϕλ

[

−||

⊆

gθ||∞

||∞||

, 1/2).

(2.24)

The denominator in the left-hand side of (2.22) satisﬁes

inf
ω
∈

T |

1 + θ2

2θ cos(ω)
|

−

= min

{

1 + θ2

−

2θ, 1 + θ2 + 2θ

}

> 0,

for all θ

1, 1).

(
−

∈

Then, it follows from theorem 5.1 in Tyrtyshnikov [34] that, if F is any arbitrary continuous function
with bounded support (i.e., the set of those x

= 0 is bounded), we get

R for which F (x)

∈

1
n

lim
n
→∞

n

Xk=1

F (αλ

n,k) =

1
2π

(F

◦

T
Z

(ϕλ gθ))(ω) dω.

(2.25)

In particular, the latter convergence applies itself when considering the continuous function
F : [

R deﬁned by

, 1/2)

ϕλ

−||

||∞||

gθ||∞

→

F (x) =

log(1

2x)

.

−
2

−

Indeed, from (2.21) and (2.24), combined with the result of Lemma 2.1, we conclude that F (
·
bounded support and that F (αλ
2 (ϕλ gθ)(ω)] is ﬁnite, for every ω
(F
of (2.25) are well deﬁned and such convergence holds, giving

) has
n,k) are ﬁnite, for every 1 6 k 6 n and n large enough. Besides that,
T, due to (2.24). Therefore, the two sides

(ϕλ gθ))(ω) = log[1

−

∈

◦

Ln(λ1, λ2) = lim

lim
n
→∞

→∞ −
n
1
2π

−

T

Z

1
2

−

log

=

=

1
2n

n

Xk=1

log(1

−

(F

(ϕλ gθ))(ω) dω =

◦
1 + θ2

n

F (αn,k)

n

2αn,k) = lim
→∞
1
4π

−

1
n

T

Z

Xk=1
log(1

2λ1 +

−

(1 + θ2
2

p

−

2λ1)2

−

4(θ + λ2)2

,

!

2 (ϕλ gθ)(ω)) dω

−

where the last equality was achieved using equation 4.224(9) in Gradshteyn and Ryzhik [19].

6
 
M.J. Karling, A.O. Lopes and S.R.C. Lopes

11

2.2 LDP of the random sequence (W n)n>2

Here we use the lemmas of Subsection 2.1, combined with the G¨artner-Ellis’ theorem, to prove that the
sequence (W n)n>2 in (2.3) satisﬁes an LDP. There are two conditions that must be satisﬁed in order to
apply the G¨artner-Ellis’ theorem (see pages 43-44 in Dembo and Zeitouni [15]).

• Condition A: for each (λ1, λ2)

R2, the limiting cumulant generating function L(
·

,

·

∈

), deﬁned as

the limit in (2.18) and explicitly given by (2.19), exists as an extended real number. Moreover, if

DL =
,
denotes the eﬀective domain of L(
·

(λ1, λ2)

R2

|

∈

L(λ1, λ2) <

∞

(2.26)

(cid:8)
), the origin must belong to
·

(cid:9)
◦L (the interior of
D

DL).

• Condition B: L(
·

,

·

) is an essentially smooth function, that is,

◦L is non-empty;

) is diﬀerentiable throughout

·

◦L;

D

1.

D
2. L(
·
3. L(
·

,

,

·

) is steep, i.e., we get limn

L(λ1,n, λ2,n)
◦L converging to a boundary point of

→∞ ||∇

sequence in
the usual Euclidean norm in R2.

D

||
D

=
∞
◦L, where

, in the case (λ1,n, λ2,n)n
=

N is a
∈
x2 + y2 denotes

(x, y)

||

||

p

Note that, if Condition A above is satisﬁed, then Condition B.1 is redundant. In the following
proposition, we verify that both Conditions A and B are satisﬁed when considering the LDP for
(W n)n>2. The cornerstone of our proof stands on the observation that the eﬀective domain
DL, deﬁned
in (2.26), contains the domain

, given in Lemma 2.1.

D

Proposition 2.1. The sequence of random vectors (W n)n>2, deﬁned in (2.3), satisﬁes an LDP with
good rate function

J(x, y) =

1
2

x(1 + θ2)




h
,
∞

1

−

−

2yθ + log

x2

x

−

y2

(cid:16)

,

for 0 < x and

(cid:17)i

otherwise.

< x,

y

|

|

(2.27)

,
Proof. Let L(
·
,
domain of L(
·

) denote the function in (2.19) and
·
) is given by
·



the domain deﬁned in Lemma 2.1. The eﬀective

D

DL =

(λ1, λ2)

(cid:26)

R2

|

∈

λ1 <

1
2

, 4(θ + λ2)2 < (1 + θ2

−

2λ1)2

.

(cid:27)

Notice that

D

is a proper subset of

DL. Furthermore, if (λ1, λ2) = (0, 0), then
4θ2 < (1 + θ2)2.
θ2)2

2θ2 + θ4

0 < 1

0 < (1

−

⇒
R2 belongs to the interior of

−

⇒
DL, for any θ

Whence, the origin (0, 0)
A above is fulﬁlled. The proof that L(
·
proof given in section 3.6 of Bryc and Dembo [9], so that Condition B is also veriﬁed.

1, 1), proving that Condition
(
−
) is an essentially smooth function follows the same steps as the

∈

∈

·

,

Explicit Rate Functions for LDP

12

Let J : R2

→

R denote the Fenchel-Legendre dual of L(
·

,

), deﬁned by the suppremum

J(x, y) = sup
(λ1,λ2)

∈

sup
(λ1,λ2)

∈D (

xλ1 + yλ2 +

1 + θ2

1
2

log

R2

−

·
xλ1 + yλ2 −
(cid:8)

2λ1 +

(1 + θ2
2

p

L(λ1, λ2)

=

(cid:9)
2λ1)2

−

−

4(θ + λ2)2

.

!)

(2.28)

From the G¨artner-Ellis’ theorem, (W n)n>2 satisﬁes an LDP with good rate function J(
·
compute J(
·

), consider the auxiliary function K :

R, deﬁned by

D →

·

,

). To explicitly

,

·

K(λ1, λ2) = xλ1 + yλ2 −

L(λ1, λ2), with (x, y)

R2.

∈

The partial derivatives of K(
·

,

·

) are

Kλ1(λ1, λ2) = x

and

Kλ2(λ1, λ2) = y +

1 + θ2

(cid:18)

−

Provided that x > 0 and x >

y

|

|

−

1

−

4λ1 + 4λ2

1 −

p

1
8θλ2 −

4λ2

2 −

2(θ + λ2)

−

2λ1 +

(1 + θ2

4(θ + λ2)2

(1 + θ2

2λ1)2

−

−

q

(cid:19) q
, the solution to the system of equations

2θ2(2λ1 + 1) + θ4

.

2λ1)2

−

−

4(θ + λ2)2

Kλ1(λ1, λ2) = 0,

Kλ2(λ1, λ2) = 0,




is given by

λ∗1 =

1 + θ2


x2 + y2

2 −

2x(x2

y2)

−

and λ∗2 =

y

−

x2

θ.

y2 −

It is not diﬃcult to prove that (λ∗1, λ∗2) is the point where the suppremum in (2.28) is attained. Hence, it
follows that

J(x, y) = K(λ∗1, λ∗2) =

where 0 < x and

< x.

y

|

|

1
2

x(1 + θ2)
(cid:20)

1

−

−

2yθ + log

,

x2

(cid:18)

−

y2

(cid:19)(cid:21)

x

Note that, the restrictions 0 < x and

and Jim´enez [28]) and

n
k=2 XkXk

1|

−

|

y
|
<

|

< x are related to the inequalities 0 <
n
k=1 X 2
k .

P

n
k=1 X 2

k (see McLeod

If x 6 0 or

y

|

|

> x, we may deﬁne J(x, y) =

P

P

, since K(λ1, λ2) is unbounded. Indeed, if x < 0, then

∞

because the linear part xλ1 rules over the logarithmic part of K(λ1, λ2), while if x = 0, then

lim
λ1→−∞

K(λ1, λ2)

lim
λ1→−∞

≈

xλ1 =

,
∞

lim
λ1→−∞

K(λ1, λ2) = lim

λ1→−∞

yλ2 −

L(λ1, λ2) =

.
∞

 
M.J. Karling, A.O. Lopes and S.R.C. Lopes

13

If x > 0, but

y

|

|

> x, then we have two cases to consider: the ﬁrst one is when y 6

x, whereby

−

the second case is when y > x, for which it follows that

lim
λ2→−∞

K(λ1, λ2)

lim
λ2→−∞

≈

yλ2 =

;
∞

K(λ1, λ2)

lim
λ2→∞

≈

lim
λ2→∞

yλ2 =

.
∞

A graph of the function J(
·

,

), in (2.27), is shown in Figure 2.3, when θ = 0.3. Since L(
·

) is a convex

,

·

·

) must also be a convex function (see section VI.5 in Ellis [18]).

function, J(
·

,

·

Figure 2.3: Graph of the function J(x, y) given in (2.27) when θ = 0.3, x

(0, 3] and y

3, 3).

(
−

∈

∈

3 Particular cases

We dedicate this section to show three particular examples where the reasoning of the last section can be
used, via Contraction Principle, to get explicit rate functions for univariate random sequences. Two of
these examples were already known from Bercu et al. [4] and Bryc and Smolenski [10]. We shall obtain
them as a continuous transform of the random vector W n, deﬁned in (2.3). In Subsection 3.2, we present
a result which we believe is new in the literature.

The Contraction Principle will be of great importance for the computations of the rate functions.

Theorem 3.1 (Contraction Principle). If a sequence of random vectors (V n)n
∈
] and Un = f (V n), where f (·) : E
satisﬁes an LDP with good rate function J(·) : Rd
→
a continuous function, then the random sequence (Un)n
I(
·

Rd
⊆
R is
N also satisﬁes an LDP with good rate function

N with values in E

] given by

) : R

[0,

[0,

→

∞

→

∞

∈

I(c) = inf
E
∈

x

J(x)

|

(cid:8)

with x such that f (x) = c

,

for all c

R.

∈

(cid:9)

Proof. See section 4.2.1 in Dembo and Zeitouni [15].

14

Explicit Rate Functions for LDP

Since the sequence of random vectors (W n)n>2 satisﬁes an LDP with rate function J(
·

(2.27), the Contraction Principle ensures that any sequence of vectors (f (W n))n>2, for f : R2
continuous, satisﬁes an LDP with good rate function

·

,

), given in
R

→

I(c) = inf
(x,y)
∈

R2

J(x, y)

|

(cid:8)

with (x, y) such that f (x, y) = c

,

for all c

R.

∈

(3.1)

(cid:9)

There is a standard procedure involving Calculus techniques for computing the inﬁmum in (3.1),
). In the examples considered below,

namely, checking for the critical points of the derivatives from J(
·
the Wolfram Mathematica software (version 11.2.0.0) was used in the calculations.

·

,

Note that, f (W n) = f

n
is a continuous function involving only the
k=2 XkXk
components 1
1. Any statistic that can be written in terms of these com-
n
ponents, as a continuous transform of W n, is suitable for our method. In particular, in Sections 3.1-3.3
we shall consider f : R2

R as being respectively deﬁned by

n
k=1 X 2
k ,
k=2 XkXk
P
−

1
n
k and 1
n

n
k=1 X 2

(cid:0)P
P

P

(cid:1)(cid:1)

(cid:0)

−

1

→

1. f (x, y) = x;

2. f (x, y) = y;

3. f (x, y) = y

x , for x > 0.

Other continuous functions f : R2
restrict our attention to these three cases.

→

R could also be considered, however, in the present work, we shall

3.1 LDP for the quadratic mean

Consider the Quadratic Mean of a random sample X1,

, Xn which satisﬁes (2.1), given by

· · ·
n

˜γn(0) =

1
n

X 2
k.

Xk=1

Bryc and Smolenski [10] proved that the sequence (˜γn(0))n

N satisﬁes an LDP with rate function

∈

given by

I(c) =

1
2

c

1 + θ2




h
,
∞

(cid:0)

−

(cid:1)

√1 + 4θ2c2

log

−

(cid:16)

2c
1+√1+4θ2c2

,

if c > 0,

(cid:17)i

if c 6 0.

(3.2)

Here we obtain the result from Bryc and Smolenski [10] as a particular case, by using Proposition 2.1
and the Contraction Principle.



Note that ˜γn(0) may be obtained as the projection on the ﬁrst coordinate of the vector W n, given
R the continuous function given by f1(x, y) = x. Then f1(W n) = ˜γn(0)
), given in (2.27), the Contraction Principle
].

in (2.3). Consider f1 : R2
→
and, since (W n)n>2 satisﬁes an LDP with rate function J(
,
·
N satisﬁes an LDP with rate function, which we shall denote by I1 : R
ensures that (˜γn(0))n
Then I1(
·

) can be computed from (3.1) in the following way.

[0,

→

∞

∈

·

M.J. Karling, A.O. Lopes and S.R.C. Lopes

• By the Contraction Principle, if c > 0, then

I1(c) =

{

inf
y
|

0<x,

<x

J(x, y)

{
}
c(1 + θ2)

|
1
2

= inf
<c
y
|

|

(cid:26)
• The inﬁmum in (3.3) is attained at

(cid:20)

15

(3.3)

J(c, y)
}

f1(x, y) = c

}

|

= inf
y
|

|

<c{
c

1

−

−

2yθ + log

c2

(cid:18)

−

y2

(cid:19)(cid:21)(cid:27)

;

1 + √1 + 4c2θ2
2θ

,

yc = 


−

0,

< 1,

θ

if 0 <

|

|
if θ = 0;

• If θ = 0, then it immediately follows that



• If 0 <

θ

|

|

< 1, then, after some algebraic computations, we obtain

I1(c) =

1

c

−

−
2

log(c)

;

I1(c) =

=

1
2
1
2

c(1 + θ2)
(cid:20)
c(1 + θ2)
(cid:20)

1

−

−

p

2ycθ + log

−

(cid:18)
1 + 4c2θ2 + log

c2

c

−

y2
c (cid:19)(cid:21)

2cθ2

√1 + 4c2θ2

(cid:18)

1 (cid:19)(cid:21)

−

,

2cθ2

= log

√1 + 4c2θ2

1 (cid:19)

−
2cθ2(√1 + 4c2θ2 + 1)

1 + 4c2θ2

1

−

2cθ2(√1 + 4c2θ2 + 1)

(√1 + 4c2θ2

−

1)(√1 + 4c2θ2 + 1) !

= log

!

√1 + 4c2θ2 + 1
2c

log

=

−

!

(cid:18)

2c
√1 + 4c2θ2 + 1

,

(cid:19)

and since

log

(cid:18)

= log

we obtain

I1(c) =

1
2

c(1 + θ2)
(cid:20)
, for c 6 0, we conclude that I1(c) = I(c), for all c

2c
√1 + 4c2θ2 + 1 (cid:19)(cid:21)

1 + 4c2θ2

log

p

−

−

(cid:18)

;

R, with I(
·

∈

) deﬁned

• Considering that I1(c) =

in (3.2).

∞

Therefore, we get the same result as in expression (1.2) in Bryc and Smolenski [10]. The graphs of
) is symmetric with respect
) are illustrated in Figure 3.1 for four diﬀerent values of θ. Notice that I1(
·
(0, 1).
∈

I1(
·
to the values of θ, i.e., I1(
·

) is the same function for θ and

θ, given that θ

−

3.2 LDP for the ﬁrst order empirical autocovariance

Consider now the ﬁrst order Empirical Autocovariance of X1,

, Xn, deﬁned below as

· · ·

˜γn(1) =

1
n

n

Xk=2

XkXk+1.

 
 
 
16

Explicit Rate Functions for LDP

q  = 0
q  = 0.3   
q  = 0.6   
q  = 0.9   

)
c
(
1
I

0
3

.

5

.

2

0
2

.

5
1

.

0
1

.

5
0

.

0
0

.

0

2

4

6

8

10

c

Figure 3.1: Graphs of I1(
·

) for θ

∈ {

0, 0.3, 0.6, 0.9

and c

}

∈

(0, 10].

By the same reasoning as for the Quadratic Mean, we show here that the sequence (˜γn(1))n>2 satisﬁes
an LDP, under the assumption that (Xn)n
N follows an AR(1) process, as deﬁned in (2.1). We present
the explicit expression for the deviation function, a result which we believe has not yet been shown in
the literature.

∈

Consider the continuous function f2 : R2

R, with law f2(x, y) = y. Since f2(W n) = ˜γn(1), it
follows from Proposition 2.1 and the Contraction Principle that (˜γn(1))n>2 satisﬁes an LDP with rate
function I2 : R

), we proceed as follows.

[0,

→

]. To give an explicit expression for I2(
·

→

∞

• By the Contraction Principle,

I2(c) =

0<x,

inf
y
|

{

<x

|

{
}

J(x, y)

|

f2(x, y) = c

=

}

0<x,

inf
c
|

{

=

0<x,

inf
c
|

{

<x

|

} (cid:26)

1
2

(cid:20)

x(1 + θ2)

1

−

−

2cθ + log

• Denote by A(c, θ) =

J(x, c)
}

<x

|

{
}
x

x2

(cid:18)

−

c2

(cid:19)(cid:21)(cid:27)

(3.4)

;

3

1 + 18c2 (1 + θ2)2 + 3√3

s

r−
Then, the inﬁmum in (3.4) is attained at

c2 (1 + θ2)2

c4 (1 + θ2)4

(cid:16)

11c2 (1 + θ2)2

−

−

1

.

(cid:17)

xc =

provided that

1 + 3c2

1 + θ2

2

+ A(c, θ) + A (c, θ)2

3 (1 + θ2) A(c, θ)

(cid:1)

(cid:0)

c2 <

11 + 5√5
2(1 + θ2)2 .

,

(3.5)

(3.6)

M.J. Karling, A.O. Lopes and S.R.C. Lopes

17

The condition in (3.6) guarantees that A(c, θ) is real valued when c

• Inserting (3.5) into (3.4), we obtain

I2(c) =

1
2 (cid:20)

xc(1 − 2cθ + θ2) − 1 + log

1

(cid:18)

xc(1 − c2) (cid:19)(cid:21)

=

∈

−
h

q

11+5√5
2(1+θ2)2 ,

11+5√5
2(1+θ2)2

.

q

i

1 + 3c2(1 + θ2)2 +

=

−2 − 6cθ + 3 log















1+3c2(1+θ2)2+A(c,θ)+A(c,θ)2

(cid:16)1+3c2(1+θ2)2

+A(c,θ)+A(c,θ)2

9(1+θ2)2

A(c,θ)2

3(1+θ2)A(c,θ)

6A(c, θ)

2

(cid:17)

−c2
















A(c, θ) + A(c, θ)2

,

for c2 < 11+5√5
2(1+θ2)2 ;

• By setting I2(c) = +

function I2(
·

).

, if c2 > 11+5√5

2(1+θ2)2 , we concluded that (˜γn(1))n>2 satisﬁes an LDP with rate

∞

The graph of I2(
·

) is illustrated in Figure 3.2 for ﬁve diﬀerent values of θ.

q  = −0.99  
q  = −0.99   
q  = −0.6
q  = −0.6
q  =  0
q  =  0
q  =  0.6
q  =  0.6
q  =  0.99   
q  =  0.99  

)
c
(
2
I

5
.
3

0
.
3

5
.
2

0
.
2

5
.
1

0
.
1

5
.
0

0
.
0

−3

−2

−1

1

2

3

0

c

. The vertical lines c2 = (11 + 5√5)/2 (in
Figure 3.2: Graphs of I2(
0.6, 0, 0.6, 0.99
·
blue), c2 = (11 + 5√5)/3.699 (in red) and c2 = (11 + 5√5)/7.841 (in green) represent the values of c
where I2(
·

= 0.99, respectively.

, when θ = 0,

) changes to

= 0.6 and

) for θ

∈ {−

0.99,

∞

−

}

θ

θ

|

|

|

|

3.3 LDP for the Yule-Walker estimates

Consider the Yule-Walker estimator

˜θn =

n
k=2 XkXk
n
k=1 X 2
k

1

−

P

P

(3.7)

18

Explicit Rate Functions for LDP

of the parameter θ, for the AR(1) processes given in (2.1). The asymptotical behavior of such estimator
is well known (see Brockwell and Davis [8]), so that

and that (see Mann and Wald [26])

√n(˜θn −

θ)

(0, 1

θ2)

−

⇒ N

˜θn

n
→∞
−−−−→

θ,

almost surely.

In Bercu et al. [4] it was proved that the Yule-Walker estimator satisﬁes an LDP with rate function

given by

S(c) = 


1
2

log

,
∞

1 + θ2
1

−

(cid:18)

2θc

−
c2

,

(cid:19)

if

c

< 1,

|

|
otherwise.

Latter on, Bercu et al. [3] provided a Sharp Large Deviation Principle (SLDP) for Hermitian quadratic
forms of stationary Gaussian processes, obtaining the Yule-Walker’s SLDP as a particular case. In Bercu
[2], the study on LDP of the Yule-Walker estimator in AR(1) processes was extended to the unstable
(
|

= 1) and explosive (
|

> 1) cases.



θ

θ

|

|

Here we obtain the result from Bercu et al. [4] by using Proposition 2.1 and the Contraction Principle.
< 1 and n > 2,

Since the rate function can be related to the sequence of probabilities P
< 1, and inﬁnite when
it actually makes sense to get S(c) ﬁnite, for

˜θn|

, for

c

|

˜θn > c
> 1.
c
(cid:16)
|

|

(cid:17)

From (3.7), note that

˜θn =

n
k=2 XkXk
n
k=1 X 2
k

1

−

= f (W n),

where W n is the random vector given in (2.3) and f : R2

R is the continuous function deﬁned by

→

f (x, y) =

for 0 < x and

< x.

y

|

|

(3.8)

Since (W n)n>2 satisﬁes an LDP with rate function J(
·

,

is applicable and (˜θn)n>2 must satisfy an LDP with rate function, given by Iθ(
·
can be computed from (3.1) and (3.8) as follows.

), given in (2.27), the Contraction Principle
)

) : R

[0,

]. Then Iθ(
·

→

∞

·

• By the Contraction Principle,

Iθ(c) =

{

0<x,

J(x, y)

|

<x

|

{
}

f (x, y) = c

=

}

{

0<x,

inf
y
|
1

<x

|

{
}

J(x, y)

y

|

−

cx = 0

}

= inf
0<x

x(1

−

(cid:20)

2cθ + θ2)

1 + log

−

x(1

(cid:18)

−

c2)

(cid:19)(cid:21)(cid:27)

,

for

< 1;

c

|

|

inf
y
|
1
2

(cid:26)

• The inﬁmum in (3.9) is attained at

• Inserting (3.10) into (3.9), Iθ(c), for

c

|

|

xc =

1
2cθ + θ2 ;
< 1, reduces itself to

−

1

Iθ(c) =

1
2

(cid:20)

xc(1

−

2cθ + θ2)

1 + log

−

1

−

xc(1

(cid:18)

c2)

(cid:19)(cid:21)

=

1
2

log

1 + θ2
1

−

(cid:18)

2θc

−
c2

;

(cid:19)

(3.9)

(3.10)

|

|

P

P
y
x

,

M.J. Karling, A.O. Lopes and S.R.C. Lopes

19

q  = −0.5   
q  =  0   
q  =  0.5   

)
c
(

Iq

5
3

.

0

.

3

5

.

2

0
2

.

5
1

.

0

.

1

5
0

.

0
0

.

−1.0

−0.5

0.0

c

0.5

1.0

Figure 3.3: Graph of Iθ(
·

) for θ

∈ {−

0.5, 0, 0.5

and c

1, 1).

(
−

∈

}

• Considering that Iθ(c) =

, for

∞

c

|

|

> 1, we obtain Iθ(c) = S(c),

R.

c
∀

∈

Therefore, we get the same result as in expression (4.6) in Bercu et al. [4]. The graph of Iθ(
·

illustrated in Figure 3.3 for three diﬀerent values of θ.

) is

4 Large deviations for the bivariate SQ-Mean

After ﬁnding the rate function for the random sequence (n−
a simple variation of that approach leading to the LDP for the sequence of bivariate SQ-Mean (Sn)n
∈
where

k )n>2 in Section 3.1, there exists
N,

P

1

n
k=1 X 2

n

n

Sn =

1
n  

Xk,

Xk=1

Xk=1

X 2
k

.

!

(4.1)

We shall use a result proved in Bryc and Dembo [9], which we enunciate below for completeness.

Proposition 4.1. Let (Xn)n
∈
density f (
·
rate function

) is diﬀerentiable. Then, (Sn)n
∈

N be a real-valued centered stationary Gaussian process whose spectral
N, for Sn given in (4.1), satisﬁes an LDP (in R2) with good

K(x, y) = I(y

x2) +

−

x2
2f (0)

,

(4.2)

where 0/0 := 0 in (4.2) and I(
·

) is the rate function associated to (n−

1

n
k=1 X 2

k ).

P

Proof. See section 3.5 in Bryc and Dembo [9].

20

Explicit Rate Functions for LDP

We dedicate the next two subsections to the particular study of the LDP of the bivariate SQ-Mean
when (Xn)n
N is an AR(1) process (Subsection 4.1) and is an MA(1) process (Subsection 4.2). Since the
LDP for the Quadratic Mean is already available for the AR(1) process, it is easy to show such property
in this case. For the MA(1) process, however, we must ﬁrst derive the LDP of the Quadratic Mean in
order to apply Proposition 4.1 and to provide the LDP for the bivariate SQ-Mean, likewise.

∈

4.1 AR(1) process

Since the AR(1) process (Xn)n
∈
from Proposition 4.1 that (Sn)n
∈

N in (2.1) is a real-valued centered stationary Gaussian process, it follows

N satisﬁes an LDP with rate function

JS(x, y) = I(y

x2) +

−

x2
2gθ(0)

,

where I(
·
gθ(
·

) is deﬁned by (3.2) and gθ(
·

) is diﬀerentiable. The explicit rate function is given by

) denotes the spectral density function given in (2.2). Note that

JS(x, y) = 


1
2

y(1 + θ2)
(cid:20)
,
∞

2x2θ

−

−

1 + 4θ2(y

x2)2

log

−

−

p

(cid:18)

x2)
2(y
1+√1+4θ2(y

−

−

x2)2

(cid:19)(cid:21)

,

if y > x2,

if y 6 x2.



As a consequence, by an application of the Contraction Principle with the auxiliary continuous func-
n
k=1 Xk.

tion f1(x, y) = x, we are able to obtain the rate function for the AR(1) Sample Mean X n = n−
Following the same steps from Section 3.1, notice that the inﬁmum

1

P

IX (c) = inf

y>x2{

JS(x, y)

|

f1(x, y) = c

= inf
y>c2

}

JS(c, y)

= inf

y>c2 (

1
2 "

is attained at

y(1 + θ2)

2c2θ

−

−

1 + 4θ2(y

c2)2

log

−

−

c2)
2(y
−
1 + 4θ2(y

1 +

p

c2)2 !#)

−

p

yc =

1 + c2(1
1

−
θ2

θ2)

.

−

Hence, the sequence (X n)n

∈

N satisﬁes an LDP with rate function

IX (c) = JS(c, yc) =

c2(1

θ)2

−
2

,

for c

R.

∈

The graphs of IX (
·
shape of a parabola.

) are depicted in Figure 4.1 for three diﬀerent values of θ. Notice that, IX (
·

) has the

4.2 MA(1) process

Consider the MA(1) process, deﬁned by the equation

Yn = εn + φεn

−

1,

with

φ
|

|

< 1 and n

N.

∈

(4.3)

 
M.J. Karling, A.O. Lopes and S.R.C. Lopes

21

q  = −0.5   
q  =  0  
q  =  0.5   

)
c
(

X

I

0
5

0
4

0
3

0
2

0
1

0

−10

−5

0

c

5

10

Figure 4.1: Graph of IX (
·

) for θ

∈ {−

0.5, 0, 0.5

and c

10, 10].

[

−

∈

}

Here, we assume that the innovations (εn)n>0 are i.i.d., with εn ∼ N
each n

N, and the spectral density function associated to (Yn)n
∈
T = [

hφ(ω) = 1 + φ2 + 2φ cos(ω),

for ω

∈

∈

π, π).

−

(0, 1). Then, Yn ∼ N

N is given by

(0, 1 + φ2), for

The process (Yn)n
Nevertheless, the assumption
positive for all ω

T.

∈

∈

N is stationary for any φ

< 1 in (4.3) ensures that the process is also invertible and that hφ(
·

R (see deﬁnition 3.4 in Shumway and Stoﬀer [33]).
) is

∈

φ
|

|

Let us denote by

˜γn(1) =

1
n

n

Y 2
k

Xk=1

the Quadratic Mean of a random sample Y1,
Since the autocovariance function of (Yn)n

· · ·

N is equal to

∈

, Yn, following the MA(1) process described in (4.3).

it is known (see section 7.3 in Brockwell and Davis [8]) that

˜γn(0) n

→∞
−−−−→

γY (0) = 1 + φ2,

almost surely.

We shall prove that the sequence (˜γn(0))n

N satisﬁes an LDP. For this reason, consider the normalized

cumulant generating function

1 + φ2,

if k = 0,

γY (k) = 


φ,

0,

if

if

k

k

|

|

|

|

= 1,

> 1,



∈

Ln(λ) =

1
n

log E(eλ˜γn(0)).

 
 
22

Explicit Rate Functions for LDP

In this case, the asymptotic distribution of Ln(
·
immediately obtain the convergence

) is known (see Grenander and Szeg¨o [21]) and we

Ln(λ) = L(λ) =

lim
n
→∞

1
4π

−

,
∞




R

T log[1

−

2λhφ(ω)] dω,

where Mhφ denotes the essential suppremum of hφ(
·



), given by

Mhφ =




1
2(1+φ)2 ,
1
φ)2 ,

2(1

−

if φ > 0,

if φ < 0.

,
−∞

if λ

∈
otherwise,

(cid:16)

1
2Mhφ

,

(cid:17)

As presented in Bercu et al. [4] and corollary 1 in Bryc and Dembo [9], (˜γn(0))n

N satisﬁes an LDP

∈

whose good rate function is the Fenchel-Legendre dual of L(
·

), given by



Kφ(x) =

Since

supλ< 1

2Mhφ




,
∞



xλ + 1
4π

T log[1

−

(cid:8)

R

2λhφ(ω)] dω

,

for x > 0,

(cid:9)

for x 6 0.

(4.4)

log[1

−

T
Z

2λhφ(ω)] dω =

log[1

T
Z

= log

1

"

the suppremum in (4.4) is attained at

λφ(x) =

4λφ cos(ω)] dω

2λ(1 + φ2)

−
2λ(1 + φ2) +

−

−

(1

p

−
2

2λ(1 + φ2))2

16λ2φ2

−

,

#

Aφ(x) + Bφ(x)
12x2 (φ2

Cφ(x) + Cφ(x)
1)2

−

,

(4.5)

where

and

with

Aφ(x) = 4x

x

φ2 + 1

φ2

−

−

Bφ(x) = 4x2

x2

(cid:0)
φ4 + 14φ2 + 1

(cid:16)

(cid:1)
(cid:0)
φ2 + 1

+ 4x

2

1

,

(cid:1)
φ2

(cid:17)
1

−

(cid:0)

(cid:1) (cid:0)

(cid:1)

(cid:0)

2

+

φ2

−

4

1

,

(cid:17)

(cid:1)

(cid:16)

(cid:0)

Cφ(x) =

−

(1 + i√3)

−
h

x6

φ6

33φ4

−

(cid:0)
+6x4

φ2

1

−

(cid:1)

−

4

33φ2 + 1

6x5

φ2

−

−

2

1

φ4

−

10φ2 + 1

φ2 + 1

(cid:1)
+ x3

(cid:0)
φ2

1

−

6

(cid:0)
(cid:1)
+ 3√3

cφ(x)

1/3
(cid:1)

,

(cid:21)

q

(cid:0)

(cid:1)

(cid:0)

(cid:1)

(cid:0)

(cid:1)

cφ(x) =

x8

φ2

−

−

(cid:0)

4

1

4x4φ2 + 32x3

φ4 + φ2

+ x2

φ4 + 46φ2 + 1

φ2

(cid:1)

(cid:16)
+6x

φ2 + 1

(cid:0)
φ2

4
(cid:1)
+

1

−

φ2

1

(cid:0)
−

(cid:0)

(cid:1) (cid:0)

(cid:1)

(cid:0)

6

.

(cid:17)

(cid:1)

(cid:1) (cid:0)

−

2

1

(cid:1)

M.J. Karling, A.O. Lopes and S.R.C. Lopes

Remark 4. Although Cφ(
·
for any x > 0. In fact, λφ(x) in (4.5) is one of the solutions from the polynomial equation

) appears in a complex form, it can be proved that Bφ(x)/Cφ(x) + Cφ(x)

∈

23

R,

λ3

+ λ
(cid:0)

4x2φ4
x2

−
4xφ2

8x2φ2 + 4x2

+ λ2

4x2φ2

4x2 + 4xφ4

4x + φ4
(cid:1)

−
2φ2 + 1
(cid:0)

−
+ x

φ2

−
1 = 0,

8xφ2 + 4x

(cid:1)

−
which has three real roots if x > 0. Moreover, we have λφ(x) < 1

−

−

−

−

(cid:1)

(cid:0)

.

2Mhφ

We conclude that (˜γn(0))n

∈

N satisﬁes an LDP with rate function given by

Kφ(x) = xλφ(x) +

=

fφ(x)

12x (φ2

−

1)2 +

1
2

1
2

2λφ(x)(1 + φ2) +

1

−

log

"

(1

−

p

2λφ(x)(1 + φ2))2
2

−

16λφ(x)2φ2

#

log 

1
2 − (cid:0)

φ2 + 1
12x2 (φ2
(cid:1)

fφ(x)

1)2 +

−

1
2 −

(φ2 + 1) fφ(x)
1)2
12x2 (φ2

−

v
u
u
t

2

φ2fφ(x)2

!

−

36x4 (φ2

−

1)4 



(4.6)

,

for all x > 0 and Kφ(x) =

, for x 6 0, with




∞

fφ(x) = Aφ(x) +

Bφ(x)
Cφ(x)

+ Cφ(x).

The graph of Kφ(
·

) is illustrated in Figure 4.2 for four diﬀerent values of φ and x

(0, 5].

∈

f  = 0.2   
f  = 0.4   
f  = 0.6   
f  = 0.8   

)
x
(

K

0
.
1

8
.
0

6
.
0

4
.
0

2
.
0

0
0

.

0

1

2

3

4

5

x

Figure 4.2: Graphs of Kφ(
·

) for φ

∈ {

0.2, 0.4, 0.6, 0.8

and x

(0, 5].

∈

}

By Proposition 4.1 we may now conclude that (Sn)n
∈

N satisﬁes an LDP with rate function

KS(x, y) =

Kφ(y




,
∞



x2) + x2

2(1+φ)2 ,

−

if y > x2,

if y 6 x2.

 
f
24

Explicit Rate Functions for LDP

where Kφ(
·

) is given in (4.6).

Note that, by the Contraction Principle, the sequence (f1(Sn))n
∈

N =

1

n−

n
k=1 Yk

f1(x, y) = x and Sn = n−

1

n
k=1 Yk,

n
k=1 Y 2
k

, must satisfy an LDP with rate function

(cid:0)

P

(cid:0)P
IY (c) = inf

y>x2{

P
KS(x, y)

(cid:1)
f1(x, y) = c
|

}

= inf
y>c2

KS(c, y) = inf
y>c2

Kφ(y

−

x2) +

(cid:26)

x2
2(1 + φ)2

.

(cid:27)

N, where

n

∈

(cid:1)

(4.7)

However, when trying to compute the inﬁmum in (4.7), we face a non-trivial problem.

Fortunately, an LDP for the Sample Mean of moving average processes has already been given in

Burton and Dehling [12], where the authors considered the sequence

Xn =

ak+n εk,

for n

Z
Xk
∈

Z,

∈

∈

∈

∈

Z a sequence of i.i.d. random variables. They proved the LDP under the hypotheses that
with (εn)n
Z is an absolutely summable sequence and that the moment generating function E(etε1 ) is ﬁnite, for
(an)n
R. In Djellout and Guillin [16], a similar approach has been given. In this paper, the authors proved
all t
Z a2
.
an analogous result under the hypotheses that the sequence (εn)n
∞
If we set a2n = 1, a2n
1 is the MA(1)
1
P
\ {
process given in (4.3), as long as the same hypotheses for the distribution of (εn)n>0 are considered. Then
by theorem 2.1 in Burton and Dehling [12], the Sample Mean (Y n)n
N satisﬁes an
LDP with rate function

Z is bounded and that
∈
, then Xn = εn + φεn

1 = φ and ak = 0 for k

n
k=1 Yk

2n, 2n

k <

N =

n−

−

∈

Z

}

−

−

∈

∈

∈

n

k

1

(cid:0)

P

(cid:1)

IY (c) = sup
R
∈

λ

(cid:26)

c λ

1 + φ −

λ2
2

(cid:27)

=

c2
2(1 + φ)2 ,

for c

R.

∈

5 Conclusion

In this work, we showed that an LDP is available for the sequence (W n)n>2, given in (2.3). The same
technique to ﬁnd such LDP is not restricted to the AR(1) process. There may exist other classes of
processes that can be explored as well. If we take another process (Zn)n
N which still has a multivariate
Gaussian distribution, equipped with another spectral density function, other than the one given in (2.2),
the proposed technique may remain valid. The LDP is, however, not always guaranteed and in most
cases, the rate function is hard to compute. This diﬃculty mainly arises when trying to compute a closed
form for the Fenchel-Legendre transform. Besides that, to obtain a similar convergence result as given in
Lemma 2.2, for another class of Gaussian processes remains an intriguing problem. A remarkable class
of processes that requires a more sophisticated approach, is the class of MA(1) process, which was not
covered in this work when evaluating the LDP for the random vectors (W n)n>2.

∈

In Section 3, we presented three important particular examples using the previous reasoning from
Section 2, together with the Contraction Principle. Two of these examples were already known from
Bercu et al. [4] and Bryc and Smolenski [10] for univariate sequences. Here we obtained them as a

M.J. Karling, A.O. Lopes and S.R.C. Lopes

25

continuous transformation of the random vector W n, given in (2.3). In Subsection 3.2, we presented a
result which we believe is new in the literature. In Subsection 3.3, the LDP for the Yule-Walker estimator
was obtained, via the Contraction Principle, getting the same result as in Bercu et al. [4]. The approach
used here, ﬁrst proving an LDP for bivariate random vectors and then particularizing to univariate
random sequences via Contraction Principle has recently been used with continuous stochastic processes
by Bercu and Richou [5], where the authors investigated the LDP of the maximum likelihood estimates
for the Ornstein-Uhlenbeck process with shift. A similar approach was subsequently used by the same
authors in Bercu and Richou [6], allowing them to circumvent the classical diﬃculty of non-steepness.

In Section 4, we provided an LDP for the sequence of bivariate SQ-Mean, for both AR(1) and MA(1)
processes. For the AR(1) process, the computations were simple and the previous technique of proving
an LDP for the bivariate random vector W n was extremely helpful. Nevertheless, when dealing with
the MA(1) process, we found some issues due to the complexity of the computations involved. The same
technique explored here may perhaps be available for general AR(d) processes with Gaussian innovations.
This is an important issue to be explored in the future.

A Proof of Lemma 2.1

In this appendix, we give the details for the proof of Lemma 2.1, which was based on the techniques given
in page 270 in Jensen [24]. In summary, we use Sylvester’s Criterion (see theorem 7.2.5 in Horn [23]) to
check for the positive deﬁniteness of each principal minor of Dn,λ and resort to the use of an auxiliary
function with its corresponding iterates.

By Sylvester’s Criterion, Dn,λ is positive deﬁnite, if and only if, the principal minors of Dn,λ are

positive. Hence, we analyze each one of the principal minors of Dn,λ as follows:

• 1-st Step: since the ﬁrst principal minor of Dn,λ is r1 = 1

consequence, since p = r1 + θ2, we obtain

2λ1, we require that r1 > 0. As a

−

• 2-nd Step: the second principal minor of Dn,λ is deﬁned as the determinant

0 < r1 < p

0 < p.

⇒

Since we already restricted our analysis for r1 > 0, (A.1) requires in addition that r2 := p

= p r1 −

q2 =

p

−

(cid:18)

q2
r1 (cid:19)

r1.

r1
q

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

q
p (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

• 3-rd Step: the third principal minor of Dn,λ is the determinant

r1
q
0

q
p
q

0
q
p

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

= p2 r1 −

q2 r1 −

q2 p =

p

−

p

q2

−

p
q2
r1 ! (cid:18)

−

q2
r1 (cid:19)

r1.

(A.1)

q2
r1

−

> 0.

(A.2)

 
26

Explicit Rate Functions for LDP

Since we already restricted our analysis for r1 > 0 and p

p

(cid:18)

q2

−

p

−

q2
r1 (cid:19)

> 0.

q2
r1

−

> 0, (A.2) requires that r3 :=

• k-th Step: by induction, the k-th principal minor of Dn,λ, for 1 6 k 6 n

1, is the determinant

−

r1

q

0
...
0

q

p

q
. . .

· · ·

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
p

for r2 = p

q2
r1

−

G : (0,

)
∞

→

(0,

, r3 =

−
), given by

(cid:18)

∞

q2

p

−

q2
r1 (cid:19)

0
...

· · ·
. . .

0
. . .
. . .
. . .
0

q

0

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
and rk = Gk
−

q
r1

p
q

G(a) = p

q2
a

.

−

= rk rk

r2 r1,

1 · · ·

−

1(r1), where Gk denotes the k-th iterate of

N is arbitrary, we must require that Gk(r1) > 0, for all k

N. Without loss of generality,
= 0 (if q = 0, then Dn,λ is a diagonal matrix; this happens if and only if

∈

∈

Since n
we may assume that q
λ2 =

θ).

−
Notice that G(
·

) has the following two ﬁxed points

R =

1
2

p

p2

−

4q2

−

(cid:16)

p

(cid:17)

and

Q =

1
2

p +

p2

−

4q2

.

(cid:16)

p

(cid:17)

The point named Q is an attractor point and the point named R is a repulsor point, provided that
p2 > 4q2. If p2 = 4q2, then P = Q = p is neither an attractor, neither a repulsor point. Let us
consider henceforth p2 > 4q2.

Observe that G(
·
Gk(r1) > 0, for all k
R converges towards Q and since R > 0, it follows that

) is an increasing concave function. Therefore, the problem of knowing when
N, reduces to knowing where r1 > R. In one hand, every point greater than

∈

On the other hand,

Since r1 = 1

2λ1 = p

−

r1 > R

⇔

p

r1 >

−

−

Gk(r1) > R > 0,

for all k

N.

∈

x < R,

∀
θ2, we get

n0 ∈

∃

N; Gn0 (x) < 0.

4q2

−

p2
2

p

p2

−

4q2 > p

2r1 = p

2(p

−

−

−

⇔

θ2) = 2θ2

p.

−

(A.3)

p

If p > 2θ2, then obviously r1 > R, since the right-hand side of (A.3) is non-positive. But if p < 2θ2,
then

r1 > R

p2

−

⇔

4q2 >

2θ2

(cid:0)

−

2

p

(cid:1)

p2

−

⇔

4q2 > 4θ4

−

4θ2p + p2

θ2(p

−

⇔

θ2) > q2.

6
M.J. Karling, A.O. Lopes and S.R.C. Lopes

27

Therefore, we obtain the domain

=

D1 ∪ D2, where

D

D1 =
D2 =

r1 > 0, p2 > 4q2, p > 2θ2

{

and

}

r1 > 0, p2 > 4q2, p < 2θ2, q2 < θ2(p

{
Notice that r1 > 0 is equivalent to p > θ2. Moreover, from

θ2)
}

.

−

0 >

−

θ2

(cid:16)

we conclude that θ2(p
p, q belong to

−

−

2

=

p
2
−
(cid:17)
θ2) < p2

θ4 + 2θ2 p

2 −

p2
4

=

−

θ4 + θ2p

p2
4

−

= θ2(p

θ2)

−

−

p2
4

,

4 . Hence, if q2 < θ2(p

θ2), it follows that 4q2 < p2. Therefore, if

−

D1 =

{

p > 2θ2, p2 > 4q2

or

D2 =

{

}

θ2 < p < 2θ2, q2 < θ2(p

θ2)
}

,

−

then Gk(r1) > 0, for all k

N.

∈

• n-th Step: last but not least, the n-th principal minor (or determinant) of Dn,λ is

0
. . .
. . .
. . .
0

· · ·
. . .

q

p
q

r1

q

0
...
0

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
r1 −
(cid:12)
(cid:18)

q

p

q
. . .

· · ·
q2
rn

−

Dn,λ

=

|

|

=

0
...

0

q
r1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
2 · · ·

= r1(rn

1 rn

−

2 · · ·

r2 r1)

q2(rn
−

2 · · ·

−

r2 r1)

−

(rn

−

1 rn

−

1 (cid:19)

r2 r1) = (Gn
−

1(r1)

θ2)(rn
−

1 rn

−

2 · · ·

−

r2 r1).

It is not diﬃcult to see that, for n large enough, we eventually obtain Gn

−

1(r1) > θ2. Indeed

so that

p > θ2

⇒

Q > θ2

⇒

lim
n
→∞

Gn(r1) = Q > θ2,

n0 ∈

∃

N ; n > n0 ⇒

Gn(r1) > θ2.

The set

D1 ∪ D2 is therefore, the closed domain where all principal minors of Dn,λ are positive, and
D2 to the

consequently, where the matrix Dn,λ is positive deﬁnite. Converting the domains
(λ1, λ2) notation, we obtain the desired expressions given by (2.17).

D1 and

Acknowledgments

M.J. Karling was supported by Coordena¸c˜ao de Aperfei¸coamento de Pessoal de N´ıvel Superior (CAPES)-
Brazil and Conselho Nacional de Desenvolvimento Cient´ıﬁco e Tecnol´ogico (CNPq)-Brazil (170168/2018-
2). A.O. Lopes’ research was partially supported by CNPq-Brazil (304048/2016-0). S.R.C. Lopes’ re-
search was partially supported by CNPq-Brazil (303453/2018-4). The authors wish to express their
sincere thanks to Dr. Bernard Bercu for indicating valuable references from the Large Deviations theory.

28

References

Explicit Rate Functions for LDP

[1] Avram, F. (1988). On bilinear forms in Gaussian random variables and Toeplitz matrices. Probability Theory and

Related Fields 79(1), 37–45.

[2] Bercu, B. (2001). On large deviations in the Gaussian autoregressive process: stable, unstable and explosive case.

Bernoulli 7(2), 299–316.

[3] Bercu, B., Gamboa, F. and Lavielle, M. (2000). Sharp large deviations for Gaussian quadratic forms with appli-

cations. ESAIM: Probability and Statistics 4(1), 1–24.

[4] Bercu, B., Gamboa, F. and Rouault, A. (1997). Large deviations for quadratic forms of stationary Gaussian

processes. Stochastic Processes and their Applications 71(1), 75–90.

[5] Bercu, B. and Richou, A. (2015). Large deviations for the Ornstein-Uhlenbeck with shift. Advances in Applied

Probability 47(3), 880–901.

[6] Bercu, B. and Richou, A. (2017). Large deviations for the Ornstein-Uhlenbeck process without tears. Statistics &

Probability Letters 123, 45–55.

[7] Bickel, P. J. and Doksum, K. A. (2001). Mathematical Statistics: Basic ideas and selected topics, vol. 1, 2nd edn.

Prentice Hall, Upper Saddle River.

[8] Brockwell, P. J. and Davis, R. A. (1991). Time Series: Theory and methods, 2nd edn. Springer, New York.

[9] Bryc, W. and Dembo, A. (1997). Large deviations for quadratic functionals of Gaussian processes. Journal of

Theoretical Probability 10(2), 307–332.

[10] Bryc, W. and Smolenski, W. (1993). On the large deviation principle for a quadratic functional of the autoregressive

process. Statistics & Probability Letters 17(4), 281–285.

[11] Bucklew, J. A. (1990). Large Deviation Techniques in Decision, Simulation, and Estimation. John Wiley & Sons,

New York.

[12] Burton, R. M. and Dehling, H. (1990). Large deviations for some weakly dependent random processes. Statistics

& Probability Letters 9(5), 397–401.

[13] Carmona, S. C., Landim, C., Lopes, A. O. and Lopes, S. R. C. (1998). A level 1 large-deviation principle for the
autocovariances of uniquely ergodic transformations with additive noise. Journal of Statistical Physics 91, 395–421.

[14] Carmona, S. C. and Lopes, A. O. (2000). Large deviations for expanding transformations with additive white noise.

Journal of Statistical Physics 98, 1311–1333.

[15] Dembo, A. and Zeitouni, O. (2010). Large Deviations Techniques and Applications, 2nd edn. Springer-Verlag, New

York.

[16] Djellout, H. and Guillin, A. (2001). Large and moderate deviations for moving average processes. Annales de la

Facult´e des Sciences de Toulouse X(1), 23–31.

[17] Donsker, M. D. and Varadhan, S. R. S. (1985). Large deviations for stationary Gaussian processes. Communications

in Mathematical Physics 97, 187–210.

[18] Ellis, R. S. (1985). Entropy, Large Deviations, and Statistical Mechanics, 2nd edn. Springer-Verlag, New York.

[19] Gradshteyn, I. S. and Ryzhik, I. M. (2007). Table of Integrals, Series, and Products, 7th edn. Academic Press, San

Diego.

[20] Gray, R. M. (2006). Toeplitz and circulant matrices: a review. Foundations and Trends in Communications and

Information Theory 2(3), 155–239

M.J. Karling, A.O. Lopes and S.R.C. Lopes

29

[21] Grenander, U. and Szeg¨o, G. (1958). Toeplitz Forms and Their Applications, 2nd edn. Cambridge University Press,

Cambridge.

[22] Heyde, C. C. (1967). A contribution to the theory of large deviations for sums of independent random variables.

Zeitschrift f¨ur Wahrscheinlichkeitstheorie und Verwandte Gebiete 7(5), 303–308.

[23] Horn, R. A. (2013). Matrix Analysis, 2nd edn. Cambridge University Press, New York.

[24] Jensen, J. L. (1995). Saddlepoint Approximations. Oxford University Press, New York.

[25] Macci, M. and Trapani, S. (2013). Large deviations for posterior distributions on the parameter of a multivariate

AR(p) process. Annals of the Institute of Statistical Mathematics 65, 703–719.

[26] Mann, H. B. and Wald, A. (1943). On the statistical treatment of linear stochastic diﬀerence equations. Econometrica

11(3), 173–200.

[27] Mas, A. and Menneteau, L. (2003). Large and moderate deviations for inﬁnite-dimensional autoregressive processes.

Journal of Multivariate Analysis 87(2), 241–260.

[28] McLeod, A. I. and Jim´enez, C. (1984). Nonnegative deﬁniteness of the sample autocovariance function. The American

Statistician 38(4), 297–298.

[29] Miao, Y. (2009). Large deviation principles for moving average processes of real stationary sequences. Acta Applican-

dae Mathematicae 106, 177–184.

[30] Nikolski, N. (2020). Toeplitz Matrices and Operators. Cambridge University Press, Cambridge.

[31] Rozovskii, L. V. (1989). Probabilities of large deviations of sums of independent random variables with common
distribution function in the domain of attraction of the normal law. Theory of Probability & Its Applications 34(4),
625–644.

[32] Rozovskii, L. V. (1999). Large deviations of sums of independent random variables from the domain of attraction of

a stable law. Journal of Mathematical Sciences 93(3), 421–435.

[33] Shumway, R. H. and Stoffer, D. S. (2016). Time Series Analysis and its Applications: With R Examples, 4th edn.

Springer, New York.

[34] Tyrtyshnikov, E. E. (1994). Inﬂuence of matrix operations on the distribution of eigenvalues and singular values of

Toeplitz matrices. Linear Algebra and its Applications 207, 225–249.

[35] Wu, L. (2004). On large deviations for moving average processes. Probability, Finance and Insurance: Proceedings of

a Workshop, the University of Hong Kong , 15–49.

[36] Zaigraev, A. (1999). Multivariate large deviations with stable limit laws. Probability and Mathematical Statistics

19(2), 323–335.

[37] Zani, M. (2013). Sample path large deviations for squares of stationary Gaussian processes. Theory of Probability &

its Applications 57(2), 347–357.

