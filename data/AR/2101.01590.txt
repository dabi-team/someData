Mixing convergence of LSE for supercritical Gaussian
AR(2) processes using random scaling

M´aty´as Barczy∗,(cid:5), Fanni K. Ned´enyi∗, Gyula Pap

* MTA-SZTE Analysis and Stochastics Research Group, Bolyai Institute, University of Szeged,
Aradi v´ertan´uk tere 1, H–6720 Szeged, Hungary.

e–mails: barczy@math.u-szeged.hu (M. Barczy), nfanni@math.u-szeged.hu (F. K. Ned´enyi).

(cid:5) Corresponding author.

Abstract

We prove mixing convergence of the least squares estimator of autoregressive parame-
ters for supercritical Gaussian autoregressive processes of order 2 having real characteris-
tic roots with diﬀerent absolute values. We use an appropriate random scaling such that
the limit distribution is a two-dimensional normal distribution concentrated on a one-
dimensional ray determined by the characteristic root having the larger absolute value.

1

Introduction

Studying asymptotic behaviour of the Least Squares Estimator (LSE) of AutoRegressive (AR)
parameters of AR processes has a long history, it goes back at least to Mann and Wald [8]. Most
of the authors proved convergence in distribution of appropriately normalized versions of the
LSE in question, but one can rarely ﬁnd other types of convergence in the corresponding limit
theorems. For some AR processes of order p (AR(p) processes), Jeganathan [7, Theorems 9,
14 and 17] proved so-called strong convergence (see Jeganathan [7, Deﬁnition 3]) of the LSE
in question, and for AR(1) processes, H¨ausler and Luschgy [6, Chapter 9] proved so-called
stable and mixing convergence (see Appendix A) of the LSE. Below, we will recall the results of
H¨ausler and Luschgy [6] on the asymptotic behaviour of the LSE in question for supercritical
AR(1) processes with Gaussian innovations. Note that, since stable (mixing) convergence yields
convergence in distribution, the results of H¨ausler and Luschgy [6, Chapter 9] immediately
imply convergence in distribution of the LSE in question. According to our knowledge, results
on stable (mixing) convergence of the LSE of AR parameters of higher order AR processes are
not available in the literature. In the present paper we consider a supercritical Gaussian AR

2020 Mathematics Subject Classiﬁcations: 62F12, 62H12, 60G15, 60F05.
Key words and phrases: autoregressive processes, least squares estimator, stable convergence, mixing con-

vergence.

M´aty´as Barczy and Fanni K. Ned´enyi are supported by grant NKFIH-1279-2/2020 of the Ministry for

Innovation and Technology, Hungary.

1
2
0
2

c
e
D
0
1

]
T
S
.
h
t
a
m

[

2
v
0
9
5
1
0
.
1
0
1
2
:
v
i
X
r
a

1

 
 
 
 
 
 
process of order 2 having real characteristic roots with diﬀerent absolute values, and we prove
mixing convergence of the LSE of its AR parameters using some random normalization.

Limit theorems stating stable (mixing) convergence instead of convergence in distribution
are important not only from theoretical point of view. These types of limit theorems have such
statistical applications where limit theorems stating only convergence in distribution cannot be
directly used. Such a nice application for the description of the asymptotic behaviour of the
so-called Harris estimator of the oﬀspring mean of a supercritical Galton–Watson process is
explained on pages 3 and 4 in H¨ausler and Luschgy [6]. In the heart of this application there
is a generalization of Slutsky’s lemma in a way that convergence in distribution is replaced by
stable convergence and in return the limit of the stochastically convergent sequence can be also
random not only a deterministic constant.

Let Z+, N, R, R+ and R++ denote the set of non-negative integers, positive integers,
real numbers, non-negative real numbers and positive real numbers, respectively. Let (Zn)n∈N
be a sequence of independent and identically distributed (i.i.d.) random variables such that
Z1 is normally distributed with mean 0 and with variance σ2, where σ ∈ R++.

Now we recall the results of H¨ausler and Luschgy [6, Example 8.10] on mixing convergence
of the LSE of the AR parameter of a supercritical AR(1) process with Gaussian innovations.
Let Y0 be a random variable independent of (Zn)n∈N and suppose that Y0 has a ﬁnite second
moment. Let us consider a Gaussian AR(1) process (Yn)n∈Z+ deﬁned by Yn = ϑYn−1 + Zn,
n ∈ N, where ϑ ∈ R. The AR(1) process
is called subcritical, critical and
supercritical if
i.e., for
supercritical Gaussian AR(1) processes, H¨ausler and Luschgy [6, Example 8.10] proved mixing
2 ((cid:98)ϑ(n) − ϑ) towards a Gaussian distribution with mean 0 and

(Yn)n∈Z+
respectively. In case of

|ϑ| = 1 and |ϑ| > 1,

|ϑ| < 1,

|ϑ| > 1,

(cid:16)(cid:80)n

(cid:17) 1

convergence of
variance σ2 as n → ∞, where

j=1 Y 2
j−1

(cid:98)ϑn =

(cid:80)n
j=1 YjYj−1
(cid:80)n
j=1 Y 2

j−1

,

n ∈ N,

denotes the LSE of ϑ based on the observations Y0, Y1, . . . , Yn (provided that the denominator
above is not zero). For results on stable (mixing) convergence of (cid:98)ϑn in case of subcritical and
critical AR(1) processes, see H¨ausler and Luschgy [6, Theorems 9.1 and 9.3].
(X0, X−1)(cid:62) be a random vector with values in R2

independent of
(Zn)n∈N, and suppose that X0 and X−1 have ﬁnite second moments. Let us consider a
Gaussian AR(2) process (Xn)n(cid:62)−1 deﬁned by

In what follows, let

(1.1)

Xn = ϑ1Xn−1 + ϑ2Xn−2 + Zn,

n ∈ N,

where (ϑ1, ϑ2)(cid:62) ∈ R2. Note that (1.1) implies

(cid:34)

Xn
Xn−1

(cid:35)

(cid:34)

= ϑ

Xn−1
Xn−2

(cid:35)

(cid:34)

+

(cid:35)

,

Zn
0

n ∈ N,

2

and hence

(1.2)

(cid:35)

(cid:34)

Xn
Xn−1

= ϑk

(cid:34)

Xn−k
Xn−k−1

(cid:35)

+

n
(cid:88)

ϑn−j

j=n−k+1

(cid:34)

(cid:35)

,

Zj
0

n ∈ Z+,

k ∈ {0, 1, . . . , n},

where (cid:80)n

j=n+1 := 0 and

ϑ :=

(cid:34)

(cid:35)

.

ϑ1 ϑ2
0
1

By (cid:37)(ϑ), we denote the spectral radius of ϑ. The AR(2) process (Xn)n(cid:62)−1 given in (1.1) is
called subcritical, critical and supercritical if (cid:37)(ϑ) < 1, (cid:37)(ϑ) = 1 and (cid:37)(ϑ) > 1, respectively.
We have (cid:37)(ϑ) = max{|λ+|, |λ−|}, where λ+ and λ− denote the eigenvalues of ϑ given by

(1.3)

λ+ :=

ϑ1 + (cid:112)ϑ2
2

1 + 4ϑ2

∈ C,

λ− :=

ϑ1 − (cid:112)ϑ2
2

1 + 4ϑ2

∈ C.

Note that λ+ and λ− are the (characteristic) roots of the autoregressive (characteristic)
polynomial x2 − ϑ1x − ϑ2 of the AR(2) process
is
called explosive if its characteristic polynomial has at least one root outside the unit circle
|λ−| (cid:54)= 1,
(supercritical case), but has no roots on the unit circle, i.e., (cid:37)(ϑ) > 1 and |λ+| (cid:54)= 1,
see Jeganathan [7, Section 6].
If both roots λ+ and λ− are outside the unit circle, i.e.,
|λ+| > 1 and |λ−| > 1, then the process (Xn)n(cid:62)−1 is called purely explosive. An explosive,
but not purely explosive AR(2) process is sometimes called partially explosive.

(Xn)n(cid:62)−1. The process

(Xn)n(cid:62)−1

In this paper we will consider a supercritical AR(2) process supposing also that |λ+| (cid:54)= |λ−|.
1 + 4ϑ2 > 0, hence we have λ+ ∈ R and λ− ∈ R. Note that this case includes the
Then ϑ2
(purely and partially) explosive case, but it also includes for example the case |λ+| > 1 and
λ− = 1 (when ϑ1 > 2, ϑ2 = 1 − ϑ1, and λ+ = ϑ1 − 1),
i.e., when there is a so-called unit
root of the characteristic polynomial of (Xn)n(cid:62)−1.

We will study the asymptotic behaviour of the LSE of the parameters ϑ1 and ϑ2 based
on the observations X−1, X0, X1, . . . , Xn using an appropriate random normalization with
the aim to establish mixing convergence (see Appendix A) of the LSE in question. For each
n ∈ N, a least squares estimator ((cid:98)ϑ(n)
2 )(cid:62) of (ϑ1, ϑ2)(cid:62) based on the observations X−1,
X0, X1, . . . , Xn can be obtained by minimizing the sum of squares

1 , (cid:98)ϑ(n)

n
(cid:88)

(Xk − ϑ1Xk−1 − ϑ2Xk−2)2

k=1

with respect to (ϑ1, ϑ2)(cid:62) over R2.
LSE ((cid:98)ϑ(n)
2 )(cid:62) of
probability 1, and this LSE has the form given by

It is known that for each n ∈ N with n (cid:62) 3, a unique
(ϑ1, ϑ2)(cid:62) based on the observations X−1, X0, X1, . . . , Xn exists with

1 , (cid:98)ϑ(n)

(cid:34)

1

(cid:98)ϑ(n)
(cid:98)ϑ(n)

2

(cid:35)



=



(cid:34)

n
(cid:88)

k=1

Xk−1
Xk−2

(cid:35) (cid:34)

Xk−1
Xk−2

−1

(cid:35)(cid:62)


n
(cid:88)

k=1

Xk

(cid:35)

,

(cid:34)

Xk−1
Xk−2

n ∈ N,

3

on the event

see, e.g., Lemma 2.2.



det








(cid:34)

n
(cid:88)

k=1

Xk−1
Xk−2

(cid:35) (cid:34)

Xk−1
Xk−2

(cid:35)(cid:62)

 > 0

,






Venkataraman [18], [19], [20] and Narasimham [12] proved convergence in distribution of
((cid:98)ϑ(n)
1 , (cid:98)ϑ(n)
2 )(cid:62) for explosive AR(2) processes with not necessarily Gaussian innovations using non-
random normalizations. Datta [5, Theorem 2.2] showed convergence in distribution of the LSE of
AR parameters of a purely explosive AR process of order p ∈ N with (not necessarily Gaussian)
i.i.d. innovations satisfying a logarithmic moment condition using non-random normalization
(depending on the unknown parameters to be estimated). Datta [5, Corollary 5.1] also proved
convergence in distribution of the LSE in question for a partially explosive AR process of order
p ∈ N with (not necessarily Gaussian) i.i.d. innovations satisfying a second order moment
condition using non-random normalization (depending on the unknown parameters).

Touati [17, Th´eor`eme 1] proved convergence in distribution of the LSE of AR parameters
of a purely explosive or partially explosive AR process of order p ∈ N with (not necessarily
Gaussian) i.i.d. innovations having a ﬁnite second moment using nonrandom normalization
(depending on the unknown parameters) and using a kind of random normalization but which
also depends on the unknown parameters. The limit laws were identiﬁed as an inﬁnite mixture
of random matrices that are not Gaussian in general. In Section 4 we give a detailed comparison
of our forthcoming results with those of Touati [17]. Recently, Monsour [11] has provided a
uniﬁed approach for proving convergence in distribution of LSE of the AR parameters of an
AR(p) process of order p ∈ N with Gaussian innovations. Part (b) of Theorem 3 in Monsour
[11] gives a general result on the asymptotic behaviour of the LSE of AR parameters of a
supercritical AR process of order p ∈ N with Gaussian innovations using a non-random
normalization establishing weak convergence of the LSE in question. Part (c) of Theorem 3
in Monsour [11] describes the asymptotic behaviour of the LSE of AR parameters of a general
AR process of order p ∈ N with Gaussian innovations using a random normalization proving
weak convergence of the LSE in question. In Section 4 we give a detailed comparison of our
forthcoming results with those of Monsour [11].

Recently, Aknouche [1] has studied the asymptotic behaviour of the LSE for some explosive
strong periodic AR processes of order p ∈ N and in case of independent and periodically
distributed Gaussian innovations with zero mean, it has been shown that the LSE using an
appropriate random scaling converges in distribution towards a p-dimensional standard normal
distribution.

Results on asymptotic behaviour of the LSE of AR parameters of AR processes of order
p ∈ N with not necessarily independent innovations are also available in the literature. Chan
and Wei [4] proved convergence in distribution of the LSE in question for subcritical and critical
AR processes of order p ∈ N with innovations that are martingale diﬀerences with respect to
an increasing sequence of σ-algebras. Boutahar [3] considered AR processes of order p ∈ N
with innovations that form a stationary Gaussian process with a regularly varying spectral

4

density, and convergence in distribution of the LSE of AR parameters was established in the
subcritical, critical and supercritical cases.

In the present paper we consider a supercritical AR(2) process

(Xk)k(cid:62)−1 given in (1.1)

with real characteristic roots λ+, λ− ∈ R satisfying |λ+| (cid:54)= |λ−| and we show that
(cid:1)1/2 (cid:80)n
((cid:80)n
(cid:0)(cid:80)n

k=1 X 2

(cid:0)(cid:80)n

(1.4)

(cid:1)1/2







k−1





(cid:35)

(cid:34)

k=1 Xk−1Xk−2
k−1)1/2
k=1 X 2
k=1 X 2

k−2

(cid:98)ϑ(n)
1 − ϑ1
(cid:98)ϑ(n)
2 − ϑ2

(cid:80)n
((cid:80)n

k=1 Xk−1Xk−2
k−2)1/2
k=1 X 2

converges mixing to σN [1 sign(λ1)](cid:62) as n → ∞, where λ1 denotes the characteristic
root having the larger absolute value, N is a one-dimensional standard normally distributed
random variable, see Theorem 3.1. The limit distribution is in fact a two-dimensional normal
distribution concentrated on a one-dimensional ray determined by the characteristic root having
the larger absolute value. The random normalization above for [(cid:98)ϑ(n)
2 − ϑ2](cid:62) seems
to be new in the literature, e.g., compared to Touati [17, Th´eor`emes 1 et 2] and Monsour [11,
Theorem 3]. Our proof is based on a multidimensional stable limit theorem (see Theorem B.1,
proved in Barczy and Pap [2]) which is a multidimensional analogue of the corresponding one-
dimensional result in H¨ausler and Luschgy [6, Theorem 8.2]. Our proof technique is motivated
by that of Theorem 9.2 in H¨ausler and Luschgy [6], and it is completely diﬀerent from that of
Monsour [11]. We note that we tried to prove mixing (or stable-) convergence of ((cid:98)ϑ(n)
1 , (cid:98)ϑ(n)
2 )
as n → ∞ in the supercritical case using a non-random normalization, but our attempts have
not been successful so far (for more details, see the end of Section 4).

1 − ϑ1, (cid:98)ϑ(n)

In Section 5 we illustrate our main result Theorem 3.1 using generated sample paths of a
supercritical AR(2) process started from (X0, X−1) = (0, 0). We choose the AR parameters ϑ1
and ϑ2 in a way that the characteristic polynomial of the corresponding AR(2) process has two
diﬀerent, real roots and the AR(2) process in question is purely explosive (Case 1), partially
explosive (Case 2), supercritical with a characteristic root 1 (Case 3), and supercritical with
a characteristic root −1 (Case 4), respectively. In Table 1 we summarize these four cases, and
we give our choices of parameters. For every particular choice of (ϑ1, ϑ2) we generate 1000
independent 102-length trajectories of the AR(2) process, and for each of them we calculate
the corresponding LSE ((cid:98)ϑ(100)
) and the scaled error (1.4) of the LSE in question. For
every particular choice of (ϑ1, ϑ2) we plot the density histogram of the scaled errors of LSEs
of the parameters. We calculate the empirical mean of the LSEs ((cid:98)ϑ(100)
) based on our
1000 replications. We also calculate the empirical mean, variance, median, skewness, kurtosis
and interquartile range of the scaled errors. We calculate the empirical covariance between the
scaled errors of the LSEs (cid:98)ϑ(100)
as well. Using Kolmogorov-Smirnov, Pearson’s
chi-squared, Anderson-Darling and Jarque-Bera tests, we test whether the scaled errors follow
a normal distribution. Our simulation results are in accordance with our theoretical results in
Theorem 3.1, for a detailed description, see Section 5.

and (cid:98)ϑ(100)

, (cid:98)ϑ(100)

, (cid:98)ϑ(100)

2

1

1

2

2

1

The paper is structured as follows.
1 , (cid:98)ϑ(n)

In Section 2, we recall the derivation of the LSE
((cid:98)ϑ(n)
2 − ϑ2)(cid:62) as well,
which is used in the proof of Theorem 3.1. Section 3 contains the precise formulation of our

(ϑ1, ϑ2)(cid:62), and a useful decomposition of

1 − ϑ1, (cid:98)ϑ(n)

2 )(cid:62) of

((cid:98)ϑ(n)

5

main result Theorem 3.1 together with its two corollaries. In Remark 3.5 we discuss how one
can use Theorem 3.1 for constructing asymptotic conﬁdence regions for the AR parameters ϑ1
and ϑ2. Section 4 is devoted to give a detailed comparison of Theorem 3.1 with the existing
results in the literature, especially with those of Monsour [11] and Touati [17]. Section 5 con-
tains a summary of our simulation results, which conﬁrm our theoretical results. All the proofs
for Sections 2 and 3 can be found in Section 6. We close the paper with three appendices:
we recall the notions of stable and mixing convergence (see Appendix A), a multidimensional
analogue of a one-dimensional stable limit theorem in H¨ausler and Luschgy [6, Theorem 8.2]
which was proved in Barczy and Pap [2, Theorem 1.4] (see Appendix B), and the Lenglart’s
inequality (see Appendix C).

P

P-a.s.−→,

−→ and

for x ∈ R+. The Borel σ-algebra on Rd

In what follows, we collect the notations used in the paper and not deﬁned so far. Let
log+(x) := log(x)1{x(cid:62)1} + 0 · 1{0(cid:54)x<1}
is denoted
by B(Rd), where d ∈ N. Almost sure convergence with respect to a probability measure
P, convergence in a probability measure P and in distribution under a probability measure
D(P)
−→, respectively. For an event A with P(A) > 0,
P will be denoted by
let PA(·) := P(· | A) = P(· ∩ A)/P(A) denote the conditional probability measure given A.
Let EP denote the expectation under the probability measure P. Almost sure equality
P-a.s.= and D=,
under a probability measure P and equality in distribution will be denoted by
respectively. Every random variable will be deﬁned on a complete probability space (Ω, F, P)
(i.e., on a probability space (Ω, F, P) having the property that for all B ∈ F with P(B) = 0
and all subsets A ⊂ B, we have A ∈ F). For a random variable ξ : Ω → Rd,
its distribution
on (Rd, B(Rd)) under P is denoted by Pξ. By (cid:107)x(cid:107) and (cid:107)A(cid:107), we denote the Euclidean
norm of a vector x ∈ Rd and the induced matrix norm of a matrix A ∈ Rd×d, respectively.
By (cid:104)x, y(cid:105), we denote the Euclidean inner product of vectors x, y ∈ Rd. The null vector and
the null matrix will be denoted by 0. Moreover, I d ∈ Rd×d denotes the identity matrix, and
e1, . . . , ed denotes the natural bases in Rd. For a symmetric and positive semideﬁnite matrix
A ∈ Rd×d,
If
V ∈ Rd×d is symmetric and positive semideﬁnite, then Nd(0, V ) denotes the d-dimensional
normal distribution with mean vector 0 ∈ Rd and covariance matrix V .
In case of d = 1,
instead of N1 we simply write N .

its unique symmetric, positive semideﬁnite square root is denoted by A1/2.

2 Preliminaries on LSE

For each n ∈ N, a least squares estimator ((cid:98)ϑ(n)
X−1, X0, X1, . . . , Xn can be obtained by minimizing the sum of squares

1 , (cid:98)ϑ(n)

2 )(cid:62) of (ϑ1, ϑ2)(cid:62) based on the observations

n
(cid:88)

(Xk − ϑ1Xk−1 − ϑ2Xk−2)2

k=1

6

with respect to (ϑ1, ϑ2)(cid:62) over R2. For each n ∈ N, we deﬁne the function Qn : Rn+2×R2 → R
by

Qn(x−1, x0, x1, . . . , xn; ϑ1, ϑ2) :=

(xk − ϑ1xk−1 − ϑ2xk−2)2

n
(cid:88)

for all (x−1, x0, x1, . . . , xn)(cid:62) ∈ Rn+2 and (ϑ1, ϑ2)(cid:62) ∈ R2. By deﬁnition, for each n ∈ N, a
least squares estimator of (ϑ1, ϑ2)(cid:62) is a measurable function Fn : Rn+2 → R2 such that

k=1

Qn(x−1, x0, x1, . . . , xn; Fn(x−1, x0, x1, . . . , xn)) =

inf
(ϑ1,ϑ2)(cid:62)∈R2

Qn(x−1, x0, x1, . . . , xn; ϑ1, ϑ2)

for all (x−1, x0, x1, . . . , xn)(cid:62) ∈ Rn+2. Next, we give the solutions of this extremum problem.

2.1 Lemma. For each n ∈ N, any least squares estimator of
function Fn : Rn+2 → R2
for which

(ϑ1, ϑ2)(cid:62) is a measurable

(2.1)

Fn(x−1, x0, x1, . . . , xn) = Gn(x−1, x0, x1, . . . , xn)−1Hn(x−1, x0, x1, . . . , xn)

on the set

where

Dn := (cid:8)(x−1, x0, x1, . . . , xn)(cid:62) ∈ Rn+2 : det(Gn(x−1, x0, x1, . . . , xn)) > 0(cid:9),

Gn(x−1, x0, x1, . . . , xn) :=

(cid:34)

n
(cid:88)

k=1

xk−1
xk−2

(cid:35) (cid:34)

xk−1
xk−2

(cid:35)(cid:62)

,

Hn(x−1, x0, x1, . . . , xn) :=

n
(cid:88)

k=1

xk

(cid:34)

(cid:35)

.

xk−1
xk−2

The next result is about the unique existence of ((cid:98)ϑ(n)

1 , (cid:98)ϑ(n)

2 )(cid:62).

(Xk)k(cid:62)−1

be an AR(2) process given in (1.1) such that

2.2 Lemma. Let
sequence of independent and identically distributed random variables with Z1
where σ ∈ R++, and (X0, X−1)(cid:62) is a random vector with values in R2
(Zn)n∈N and with EP(X 2
have P(Ωn) = 1 for the event Ωn given by

is a
D= N (0, σ2),
independent of
−1) < ∞. Then for each n ∈ N with n (cid:62) 3, we

0 ) < ∞, EP(X 2

(Zn)n∈N

(2.2)

Ωn :=






det





n
(cid:88)

k=1

(cid:34)

Xk−1
Xk−2

(cid:35) (cid:34)

Xk−1
Xk−2

(cid:35)(cid:62)

 > 0

,






and hence a unique least squares estimator ((cid:98)ϑ(n)
2 )(cid:62) of (ϑ1, ϑ2)(cid:62) based on the observations
X−1, X0, X1, . . . , Xn exists with probability 1, and this least squares estimator has the form given
by

1 , (cid:98)ϑ(n)

(cid:34)

(2.3)

(cid:35)



=



(cid:34)

n
(cid:88)

k=1

Xk−1
Xk−2

(cid:35) (cid:34)

Xk−1
Xk−2

−1

(cid:35)(cid:62)


n
(cid:88)

k=1

Xk

(cid:35)

,

(cid:34)

Xk−1
Xk−2

n ∈ N,

1

(cid:98)ϑ(n)
(cid:98)ϑ(n)

2

on the event Ωn.

7

By Lemma 2.2, for each n ∈ N with n (cid:62) 3, on the event Ωn having probability 1, we

have

(cid:34)

(cid:98)ϑ(n)
1 − ϑ1
(cid:98)ϑ(n)
2 − ϑ2

(cid:35)



=



(cid:34)

n
(cid:88)

k=1

Xk−1
Xk−2

(cid:35) (cid:34)

Xk−1
Xk−2

−1

(cid:35)(cid:62)




=



(cid:34)

n
(cid:88)

k=1

Xk−1
Xk−2

(cid:35) (cid:34)

Xk−1
Xk−2

−1

(cid:35)(cid:62)


(cid:34)

(cid:34)

n
(cid:88)

k=1

n
(cid:88)

k=1

Xk

Xk

Xk−1
Xk−2

(cid:35)

(cid:34)

−

(cid:35)

ϑ1
ϑ2

(cid:35)

Xk−1
Xk−2



−



(cid:34)

n
(cid:88)

k=1

Xk−1
Xk−2

(cid:35) (cid:34)

Xk−1
Xk−2

(cid:35)(cid:62)


−1 



(cid:34)

n
(cid:88)

k=1

(cid:35) (cid:34)

Xk−1
Xk−2

(cid:34)

(cid:35)(cid:62)


(cid:35)

ϑ1
ϑ2

Xk−1
Xk−2



=



(cid:34)

n
(cid:88)

k=1

Xk−1
Xk−2

(cid:35) (cid:34)

Xk−1
Xk−2

−1

(cid:35)(cid:62)




=



(cid:34)

n
(cid:88)

k=1

Xk−1
Xk−2

(cid:35) (cid:34)

Xk−1
Xk−2

−1

(cid:35)(cid:62)


n
(cid:88)

k=1

n
(cid:88)

k=1

(Xk − ϑ1Xk−1 − ϑ2Xk−2)

(cid:35)

(cid:34)

Xk−1
Xk−2

(cid:34)

(cid:35)

.

Xk−1
Xk−2

Zk

Hence

(2.4)

where

(2.5)

(cid:35)

(cid:34)

(cid:98)ϑ(n)
1 − ϑ1
(cid:98)ϑ(n)
2 − ϑ2

= (cid:104)M (cid:105)−1

n M n,

M n := σ−2

n
(cid:88)

k=1

Zk

(cid:34)

(cid:35)

,

Xk−1
Xk−2

n ∈ N,

with M 0 := 0 is a square integrable martingale with respect to the ﬁltration (Fn)n∈Z+,
where Fn := σ(X−1, X0, X1, . . . , Xn) = σ(X−1, X0, Z1, . . . , Zn), n ∈ Z+, and ((cid:104)M (cid:105)n)n∈Z+ is
its quadratic characteristic process given by

(2.6)

(cid:104)M (cid:105)n = σ−2

(cid:34)

n
(cid:88)

k=1

Xk−1
Xk−2

(cid:35) (cid:34)

Xk−1
Xk−2

(cid:35)(cid:62)

,

n ∈ N,

with (cid:104)M (cid:105)0 := 0.

Indeed, using the deﬁnition of a quadratic characteristic process (see, e.g., H¨ausler and Luschgy
[6, page 193] or Corollary C.2), for each n ∈ N, we have

(cid:104)M (cid:105)n :=

n
(cid:88)

k=1

EP((M k − M k−1)(M k − M k−1)(cid:62) | Fk−1)

=

n
(cid:88)

k=1

EP

(cid:32)(cid:18)

σ−2Zk

(cid:34)

Xk−1
Xk−2

(cid:35)(cid:19)(cid:18)

σ−2Zk

(cid:34)

(cid:35)(cid:19)(cid:62) (cid:12)
(cid:12)
(cid:12)
(cid:12)

Xk−1
Xk−2

(cid:33)

Fk−1

8

= σ−4

n
(cid:88)

k=1

EP(Z 2
k)

(cid:35)(cid:62)

(cid:34)

Xk−1
Xk−2

(cid:35) (cid:34)

Xk−1
Xk−2

= σ−2

(cid:34)

n
(cid:88)

k=1

Xk−1
Xk−2

(cid:35) (cid:34)

Xk−1
Xk−2

(cid:35)(cid:62)

.

As a consequence of (2.6), we have Ωn = {det((cid:104)M (cid:105)n) > 0}, where Ωn is given in (2.2).

3 Mixing convergence of LSE using random scaling

Using an appropriate random scaling we prove mixing convergence of the LSE ((cid:98)ϑ(n)
2 )(cid:62)
given in (2.3) as n → ∞ in supercritical cases with real characteristic roots having diﬀerent
absolute values.

1 , (cid:98)ϑ(n)

Considering a supercritical AR(2) process given in (1.1) such that

|λ+| (cid:54)= |λ−|,

let us

introduce the notation

(λ1, λ2) :=

(cid:40)

(λ+, λ−)
(λ−, λ+)

if

if

|λ+| > |λ−|,
|λ−| > |λ+|,

where λ+ and λ− are given in (1.3), i.e., λ1 and λ2
larger and smaller absolute value, respectively.

is the characteristic root having the

(Xk)k(cid:62)−1

be an AR(2) process given in (1.1) such that

3.1 Theorem. Let
sequence of independent and identically distributed random variables with Z1
σ ∈ R++, and (X0, X−1)(cid:62) is a random vector with values in R2
and with EP(X 2
polynomial x2 − ϑ1x − ϑ2 of
|λ1| > 1. Then

(Zn)n∈N is a
D= N (0, σ2), where
(Zn)n∈N
−1) < ∞.
Suppose that the autoregressive (characteristic)
(Xk)k(cid:62)−1 has real roots λ1 and λ2 with |λ1| > |λ2| and

0 ) < ∞, EP(X 2

independent of

(3.1)






(cid:0)(cid:80)n

k=1 X 2

k−1

(cid:80)n
((cid:80)n

k=1 Xk−1Xk−2
k−2)1/2
k=1 X 2

(cid:1)1/2 (cid:80)n
((cid:80)n
(cid:0)(cid:80)n

k=1 Xk−1Xk−2
k−1)1/2
k=1 X 2
k=1 X 2

k−2

(cid:1)1/2






(cid:35)

(cid:34)
(cid:98)ϑ(n)
1 − ϑ1
(cid:98)ϑ(n)
2 − ϑ2

→ σN

(cid:34)

1

(cid:35)

sign(λ1)

F∞-mixing

as n → ∞, where F∞ := σ(cid:0)(cid:83)∞
is a standard normally distributed random variable being P-independent of F∞.

(cid:1) with Fn = σ(X−1, X0, X1, . . . , Xn), n ∈ Z+, and N

n=0 Fn

Note that the random scaling matrix in (3.1) is well-deﬁned with probability 1 for each
k−1 > 0) = 1 and

n ∈ N with n (cid:62) 3, since X1 is absolutely continuous yielding P((cid:80)n
P((cid:80)n

k=1 X 2
In the next remark we give another representation of the law of the limit random variable

k−2 > 0) = 1.

k=1 X 2

in (3.1) in Theorem 3.1.

3.2 Remark. By Step 6 of the proof of Theorem 3.1, the law of the limit random variable
in (3.1) can be represented as the law of the P-almost surely convergent series (cid:80)∞
1 N j,

j=0 λ−j

9

where (N j)j∈Z+ is a sequence of P-independent and identically distributed R2-valued random
vectors being P-independent of F∞ such that the law of N 0 under P is

(cid:32)

N2

0,

λ2
1 − 1
λ2
1

(cid:34)

1

sign(λ1)

(cid:35)(cid:33)

.

sign(λ1)
1

(cid:50)

Next, we formulate two corollaries of the proof of Theorem 3.1 about the asymptotic be-
haviour of M n (given in (2.5)) and its quadratic characteristic process (cid:104)M (cid:105)n (given in (2.6))
as n → ∞, proving stable convergence and P-almost sure convergence, respectively. These
two results can be interesting on their own rights as well.

3.3 Corollary. Under the conditions of Theorem 3.1, for the process (M n)n∈Z+ deﬁned in
(2.5), we have

λ−n
1 M n → η

∞
(cid:88)

j=0

λ−j
1 N j

D= ηN

(cid:34)

(cid:35)

1

sign(λ1)

F∞-stably as n → ∞,

where

• the random variable η is given by

η :=

|Y |
σ(cid:112)λ2

1 − 1

(cid:35)

(cid:34)

0

1
0 |λ1|−1

with

(3.2)

Y :=

λ1
λ1 − λ2

(X0 − λ2X−1) +

λ1
λ1 − λ2

∞
(cid:88)

j=1

λ−j
1 Zj,

where the series is absolute convergent P-almost surely,

• (N j)j∈Z+ is a sequence of P-independent and identically distributed R2-valued random

vectors being P-independent of F∞ such that the law of N 0 under P is

(cid:32)

N2

0,

λ2
1 − 1
λ2
1

(cid:34)

1

sign(λ1)

(cid:35)(cid:33)

sign(λ1)
1

,

• the series (cid:80)∞

j=0 λ−j

1 N j converges P-almost surely,

• N is P-independent of F∞ with N D= N (0, 1). Consequently, N and η are

P-independent, since η is F∞-measurable.

10

Note that the P-almost sure absolute convergence of the series (cid:80)∞

1 Zj in the deﬁnition
of Y in (3.2) follows, e.g., from Lemma 8.1 in H¨ausler and Luschgy [6], since EP(log+(|Z1|)) <
∞ and |λ1| > 1, λ1 ∈ R. Remark also that Y
is F∞-measurable, since the series
(cid:80)∞
j=1 λ−j
is F∞-measurable for all n ∈ N (due to
Fn = σ(X−1, X0, Z1, . . . , Zn), n ∈ N), and the underlying probability space
is
complete.

converges P-a.s., (cid:80)n

(Ω, F, P)

j=1 λ−j

j=1 λ−j

1 Zj

1 Zj

Next, we describe the asymptotic behaviour of (cid:104)M (cid:105)n (given in (2.6)) as n → ∞.

3.4 Corollary. Under the conditions of Theorem 3.1, for the process ((cid:104)M (cid:105)n)n∈Z+ given in
(2.6), we have

(3.3)

λ−2n
1

(cid:104)M (cid:105)n =

λ−2n
1
σ2

(cid:34) (cid:80)n
(cid:80)n

k=1 X 2
k=1 Xk−1Xk−2

k−1

(cid:80)n

k=1 Xk−1Xk−2
(cid:80)n
k=1 X 2

k−2

(cid:35)

P-a.s.−→

Y 2
1 − 1)σ2

(λ2

(cid:34)

1
λ−1
1

(cid:35)

λ−1
1
λ−2
1

as n → ∞, where Y is given in (3.2). Especially, λ−4n

1

det((cid:104)M (cid:105)n)

P-a.s.−→ 0 as n → ∞.

In the next remark we discuss how one can use Theorem 3.1 for constructing asymptotic

conﬁdence regions for the AR parameter [ϑ1, ϑ2](cid:62).

3.5 Remark. Let us suppose that the conditions of Theorem 3.1 hold. Then, using (3.1), the
fact that mixing convergence yields convergence in distribution and the continuous mapping
theorem, we get

(cid:34)

(cid:98)ϑ(n)
1 − ϑ1
(cid:98)ϑ(n)
2 − ϑ2

(cid:35)(cid:62) 



(cid:0)(cid:80)n

k=1 X 2

k−1

(cid:80)n
((cid:80)n

k=1 Xk−1Xk−2
k−2)1/2
k=1 X 2

(cid:1)1/2 (cid:80)n
((cid:80)n
(cid:0)(cid:80)n

k=1 Xk−1Xk−2
k−1)1/2
k=1 X 2
k=1 X 2

k−2

(cid:1)1/2

(cid:62)








(cid:0)(cid:80)n

k=1 X 2

k−1

×




(cid:80)n
((cid:80)n

k=1 Xk−1Xk−2
k−2)1/2
k=1 X 2

(cid:1)1/2 (cid:80)n
((cid:80)n
(cid:0)(cid:80)n

k=1 Xk−1Xk−2
k−1)1/2
k=1 X 2
k=1 X 2

k−2

(cid:1)1/2

(cid:34)






(cid:35)

(cid:98)ϑ(n)
1 − ϑ1
(cid:98)ϑ(n)
2 − ϑ2

D(P)
−→ σ2N 2

(cid:34)

1

(cid:35)(cid:62) (cid:34)

(cid:35)

1

sign(λ1)

sign(λ1)

as n → ∞, where N D= N (0, 1), yielding that N 2 has a chi-squared distribution with 2
degrees of freedom. Consequently,

(cid:34)
(cid:98)ϑ(n)
1 − ϑ1
(cid:98)ϑ(n)
2 − ϑ2

(cid:35)(cid:62) 



(cid:80)n

k=1 X 2

k−1 +

((cid:80)n

k=1 Xk−1Xk−2)2
(cid:80)n
k=1 X 2

k−2

2 (cid:80)n

k=1 Xk−1Xk−2

2 (cid:80)n

k=1 Xk−1Xk−2
((cid:80)n

k=1 Xk−1Xk−2)2
(cid:80)n
k=1 X 2

k−1

(cid:80)n

k=1 X 2

k−2 +

(cid:34)






(cid:35)

(cid:98)ϑ(n)
1 − ϑ1
(cid:98)ϑ(n)
2 − ϑ2

D(P)
−→ 2σ2N 2

as n → ∞.

1−α(2) denote the (1 − α)-quantile of the chi-squared distribution
Given α ∈ (0, 1),
with 2 degrees of freedom. Thus, under the conditions of Theorem 3.1, for large sample size

let χ2

11

n, the region

(cid:40) (cid:34)

(cid:35)

u

v

(cid:34)

∈ R2 :

(cid:35)(cid:62) 



(cid:98)ϑ(n)
1 − u
(cid:98)ϑ(n)
2 − v

(cid:80)n

k=1 X 2

k−1 +

((cid:80)n

k=1 Xk−1Xk−2)2
(cid:80)n
k=1 X 2

k−2

2 (cid:80)n

k=1 Xk−1Xk−2

(cid:80)n

k=1 X 2

k−2 +

2 (cid:80)n

k=1 Xk−1Xk−2
((cid:80)n

k=1 Xk−1Xk−2)2
(cid:80)n
k=1 X 2

k−1

×

(cid:34)

(cid:35)
(cid:98)ϑ(n)
1 − u
(cid:98)ϑ(n)
2 − v

(cid:41)

(cid:54) 2σ2χ2

1−α(2)

contains the (true) AR parameter [ϑ1, ϑ2](cid:62) with probability close to 1 − α.






(cid:50)

4 Discussion on related results

First, we give a detailed comparison of Theorem 3.1 (of the present paper) and part (c) of
Theorem 3 in Monsour [11] specialized to the two-dimensional case. We will distinguish two
supercritical cases, namely, without a unit root and with a unit root. Then we compare Theorem
3.1 (of the present paper) and Th´eor`eme 1 in Touati [17] specialized to the two-dimensional
case.

First of all, we note that we are not convinced that the proof of part (c) of Theorem 3
in Monsour [11] is complete/correct. Namely, with the notations of Monsour [11], we do not
D(P)
understand how the weak convergence O(cid:62)
−→ I p as n → ∞ yields that On converges
in distribution as n → ∞, where On := (V G (cid:80)n
n H 1/2, n ∈ N,
j−1G(cid:62)V (cid:62))−1/2V GR−1
and we cannot see why (On, H −1/2Rn
converges in distribution as n → ∞
(which is implicitly used in the proof in question).

j=1 (cid:101)Xj−1 (cid:101)X (cid:62)

j=1 (cid:101)Xj−1εj)

n On

(cid:80)n

Let us suppose that the conditions of Theorem 3.1 hold together with |λ2| (cid:54)= 1,
i.e.,
λ1, λ2 ∈ R and either |λ1| > |λ2| > 1 (purely explosive case) or |λ1| > 1 > |λ2| (partially
explosive case). For simplicity, let us suppose that σ = 1. Provided that the proof of part (c)
of Theorem 3 in Monsour [11] is complete, in our considered case it states that

(4.1)





(cid:80)n

k−1

k=1 X 2
k=1 Xk−1Xk−2

(cid:80)n

(cid:80)n

k=1 Xk−1Xk−2
(cid:80)n
k=1 X 2

k−2



1/2 (cid:34)



(cid:35)

(cid:98)ϑ(n)
1 − ϑ1
(cid:98)ϑ(n)
2 − ϑ2

D(P)
−→ N2(0, I 2)

as n → ∞.

We note that in part (c) of Theorem 3 in Monsour [11], the limit law is represented in a more
complicated form, but from its proof it turns out that in the special case of no characteristic
roots on the unit circle (as in our considered case) the limit law can be represented in the form
O (ζ1, ζ2)(cid:62), where O is some random 2×2 orthogonal matrix independent of (ζ1, ζ2)(cid:62) having
a 2-dimensional standard normal distribution (see page 305 in Monsour [11]). In this case,
the limit law in question is indeed N2(0, I 2), since the law of a (multidimensional) standard
normally distributed random variable is invariant under an orthogonal transformation, and
the random orthogonal matrix O and the standard normally distributed (two-dimensional)

12

random vector (ζ1, ζ2)(cid:62) are independent. In the purely explosive case, the weak convergence
in (4.1) was established in several other papers, see, e.g., Jeganathan [7, Theorem 14], Mikulski
and Monsour [9, Theorem 1] or Monsour and Mikulski [10, Theorem on page 146].

Since mixing convergence yields convergence in distribution, as a consequence of Theorem

3.1, we have

(4.2)






(cid:0)(cid:80)n

k=1 X 2

k−1

(cid:80)n
((cid:80)n

k=1 Xk−1Xk−2
k−2)1/2
k=1 X 2

(cid:1)1/2 (cid:80)n
((cid:80)n
(cid:0)(cid:80)n

k=1 Xk−1Xk−2
k−1)1/2
k=1 X 2
k=1 X 2

k−2

(cid:1)1/2

(cid:34)






(cid:35)

(cid:98)ϑ(n)
1 − ϑ1
(cid:98)ϑ(n)
2 − ϑ2

D(P)
−→ N

(cid:34)

1

(cid:35)

sign(λ1)

as n → ∞,

where N is a standard normally distributed random variable.

Next we check that (4.1) yields (4.2). Using (2.4) and (2.6), the weak convergence in (4.1)

can be rewritten in the form

(cid:104)M (cid:105)1/2

n (cid:104)M (cid:105)−1

n M n = (cid:104)M (cid:105)−1/2

n M n

D(P)
−→ N2(0, I 2)

as n → ∞.

By the decomposition in Step 1 of the proof of Theorem 3.1, the weak convergence in (4.2) can
be rewritten in the form

(4.3)

AnM n

D(P)
−→ N

(cid:34)

(cid:35)

1

sign(λ1)

as n → ∞,

where An, n ∈ N,
have AnM n = An(cid:104)M (cid:105)1/2
it is enough to verify that

is given in (6.2) with σ = 1. Since on the event Ωn (given in (2.2)), we
n M n), to check that (4.1) yields (4.2), by Slutsky’s lemma,

n ((cid:104)M (cid:105)−1/2

An(cid:104)M (cid:105)1/2

n

P-a.s.−→

1
(cid:112)1 + λ−2

1

(cid:34)

1

sign(λ1)

(cid:35)

λ−1
1
|λ1|−1

as n → ∞.

(4.4)

Indeed,

(cid:32)

1
(cid:112)1 + λ−2

1

(cid:34)

1

sign(λ1)

λ−1
1
|λ1|−1

(cid:35)(cid:33) (cid:32)

1
(cid:112)1 + λ−2

1

(cid:34)

1

sign(λ1)

λ−1
1
|λ1|−1

(cid:35)(cid:33)(cid:62)

=

1
1 + λ−2
1

(cid:34)

1 + λ−2
1
sign(λ1) + (λ1|λ1|)−1

sign(λ1) + (λ1|λ1|)−1
1 + λ−2
1

(cid:35)

(cid:34)

=

1

sign(λ1)

(cid:35)

,

sign(λ1)
1

which coincides with the covariance matrix of N [1, sign(λ1)](cid:62), as desired. Next, we prove
(4.4). Recall that if V = (vi,j)i,j=1,2 ∈ R2×2 is a symmetric and positive deﬁnite matrix, then

V 1/2 =

(cid:113)

1
v1,1 + v2,2 + 2(cid:112)det(V )

(cid:16)

V + (cid:112)det(V )I 2

(cid:17)

,

13

and hence on the event Ωn,

(cid:104)M (cid:105)1/2

n =

1

(cid:113)(cid:80)n

k=1 X 2

k−1 + (cid:80)n

k=1 X 2

k−2 + 2(cid:112)det((cid:104)M (cid:105)n)

(cid:16)
(cid:104)M (cid:105)n + (cid:112)det((cid:104)M (cid:105)n)I 2

(cid:17)

.

Consequently, by (6.7), (6.8), (6.9), (6.14), the absolute continuity of Y (see Step 2 of Theorem
3.1) and Corollary 3.4, we have

An(cid:104)M (cid:105)1/2

n =

1

(cid:113)(cid:80)n

k=1 X 2

k−1 + (cid:80)n

k=1 X 2

k−2 + 2(cid:112)det((cid:104)M (cid:105)n)





(cid:0)(cid:80)n

k=1 X 2

k−1

×







(cid:80)n
((cid:80)n

k=1 Xk−1Xk−2
k−2)1/2
k=1 X 2

(cid:1)1/2 (cid:80)n
((cid:80)n
(cid:0)(cid:80)n

k=1 Xk−1Xk−2
k−1)1/2
k=1 X 2
k=1 X 2

k−2

(cid:1)1/2


 + (cid:112)det((cid:104)M (cid:105)n)An







=

(cid:113)

λ−2n
1

(cid:80)n

k=1 X 2

k−1 + λ−2n

1

1

(cid:80)n

k=1 X 2

k−2 + 2(cid:112)λ−4n

1

det((cid:104)M (cid:105)n)






×

1


(cid:0)λ−2n



(cid:80)n

k=1 X 2

k−1

(cid:1)1/2

(cid:80)n

λ−2n
1
(λ−2n
1

k=1 Xk−1Xk−2
k−2)1/2
(cid:80)n
k=1 X 2

(cid:80)n

λ−2n
1
(λ−2n
1
(cid:0)λ−2n

1

k=1 Xk−1Xk−2
k−1)1/2
(cid:80)n
k=1 X 2
k=1 X 2

k−2

(cid:80)n

(cid:1)1/2




 +

(cid:113)

λ−4n
1

det((cid:104)M (cid:105)n)λn

1 An






P-a.s.−→

1
(cid:113) 1
1−1Y 2 +
λ2

1
1(λ2
λ2

1−1)Y 2






|Y |√
λ2
1−1
Y 2/(λ1(λ2
√
|Y |/(|λ1|

1−1))
λ2
1−1)

Y 2/(λ1(λ2
1−1))
λ2
1−1

|Y |/

√

|Y |
√

λ2
1−1

|λ1|






=

1
(cid:112)1 + λ−2

1

(cid:34)

1

sign(λ1)

(cid:35)

λ−1
1
|λ1|−1

as n → ∞,

yielding (4.4), as desired. Note that (4.4) holds in case of
previous argument we did not use that |λ2| (cid:54)= 1).

|λ2| = 1 as well (indeed, in the

Now let us suppose that the conditions of Theorem 3.1 hold together with |λ2| = 1,

i.e.,
λ1 ∈ R,
|λ1| > 1, and either λ2 = 1 or λ2 = −1. For simplicity, let us suppose again that
σ = 1. Provided that the proof of part (c) of Theorem 3 in Monsour [11] is complete, in our
considered case it states that

(4.5)





(cid:80)n

k−1

k=1 X 2
k=1 Xk−1Xk−2

(cid:80)n

(cid:80)n

k=1 Xk−1Xk−2
(cid:80)n
k=1 X 2

k−2



1/2 (cid:34)



(cid:35)

(cid:98)ϑ(n)
1 − ϑ1
(cid:98)ϑ(n)
2 − ϑ2

D(P)
−→ O(±)





ζ (±)
± (cid:82) 1
u dW (±)
0 W (±)
((cid:82) 1
0 (W (±)
u )2 du)1/2

u





as n → ∞, where the ± sign is according to λ2 = ±1, O(±) is some random 2×2 orthogonal
matrix, ζ (±) is a one-dimensional standard normally distributed random variable, (W (±)
)u∈[0,1]
is standard Wiener process such that ζ (±) and (cid:82) 1
are uncorrelated. Here O(±)
u /((cid:82) 1
and (ζ (±), (cid:82) 1
)2 du)1/2) are not necessarily independent. Using (4.4)

u dW (±)

u dW (±)

0 W (±)

u

u

0 (W (±)

u

0 W (±)

14

(which holds in case of
before, (4.5) should yield that

|λ2| = 1 as well) and Slutsky’s lemma, similarly as we have seen



(cid:0)(cid:80)n

k=1 X 2

k−1




(cid:80)n
((cid:80)n

k=1 Xk−1Xk−2
k−2)1/2
k=1 X 2

(cid:1)1/2 (cid:80)n
((cid:80)n
(cid:0)(cid:80)n

k=1 Xk−1Xk−2
k−1)1/2
k=1 X 2
k=1 X 2

k−2

(cid:1)1/2

(cid:34)






(cid:35)

(cid:98)ϑ(n)
1 − ϑ1
(cid:98)ϑ(n)
2 − ϑ2

= AnM n = An(cid:104)M (cid:105)

n (cid:104)M (cid:105)

1
2

− 1
n M n
2

D(P)
−→

D(P)
−→

1
(cid:112)1 + λ−2

1

(cid:34)

1

sign(λ1)

(cid:35)

λ−1
1
|λ1|−1





O(±)

ζ (±)
± (cid:82) 1
u dW (±)
0 W (±)
((cid:82) 1
0 (W (±)
u )2 du)1/2

u





as n → ∞.

Taking into account (3.1) and that mixing convergence yields convergence in distribution, under
the conditions of Theorem 3.1 together with |λ2| = 1 and σ = 1,

it should hold that

1
(cid:112)1 + λ−2

1

(cid:34)

1

sign(λ1)

(cid:35)

λ−1
1
|λ1|−1





O(±)

ζ (±)
± (cid:82) 1
u dW (±)
0 W (±)
((cid:82) 1
0 (W (±)
u )2 du)1/2

u





D= N

(cid:34)

1

sign(λ1)

(cid:35)

.

We were not able to check whether the previous equality in distribution holds or not (mainly
due to the lack of an explicit form for the random matrix O(±)), and in fact, we are not sure
that it is true, since, we detailed earlier, we are not convinced that the proof of part (c) of
Theorem 3 in Monsour [11] is complete/correct.

To ﬁnish the comparison of our results and part (c) of Theorem 3 in Monsour [11], we
emphasize that both in (4.1) and in (4.2) the type of convergence is convergence in distribution,
and in (3.1) we proved mixing convergence which is stronger than convergence in distribution,
so in general (4.1) (or (4.2)) would not yield (3.1) without any additional work. Furthermore,
our proof technique is completely diﬀerent from that of Monsour [11].

Now we turn to compare Theorem 3.1 (of the present paper) and Th´eor`eme 1 in Touati [17]
specialized to the two-dimensional case. Let us suppose that the conditions of Theorem 3.1
hold together with |λ1| > |λ2| > 1 (i.e., we consider the purely explosive case). For simplicity,
let us suppose that σ = 1. Th´eor`eme 1 in Touati [17] specialized to this case states that





(cid:80)n

k−1

k=1 X 2
k=1 Xk−1Xk−2

(cid:80)n

(4.6)

ϑ−n

where

(cid:80)n

k=1 Xk−1Xk−2
(cid:80)n
k=1 X 2

k−2

(cid:34)





(cid:98)ϑ(n)
1 − ϑ1
(cid:98)ϑ(n)
2 − ϑ2

(cid:35)

D(P)
−→

∞
(cid:88)

k=1

ϑ−k (cid:101)Y ζk

as n → ∞,

(cid:101)Y :=

(cid:35)

+

(cid:34)

X0
X−1

∞
(cid:88)

k=1

ϑ−k

(cid:34)

(cid:35)

,

Zk
0

and (ζk)k∈N is a sequence of independent and standard normally distributed random variables
(ζk)k∈N
such that
is independent of (cid:101)Y as well). Recall that, since mixing convergence yields convergence in
distribution, as a consequence of Theorem 3.1, we have (4.2).

(X0, X−1) and (Zn)n∈N (and consequently,

(ζk)k∈N is independent of

15

Next, we check that (4.6) yields (4.2) in the considered purely explosive case. Using (2.4)

and (2.6), the weak convergence in (4.6) can be rewritten in the form

(4.7)

ϑ−n(cid:104)M (cid:105)n((cid:104)M (cid:105)−1

n M n) = ϑ−nM n

D(P)
−→

∞
(cid:88)

k=1

ϑ−k (cid:101)Y ζk

as n → ∞.

We have already seen that the weak convergence in (4.2) can be rewritten in the form (4.3).
Let us consider the decomposition AnM n = Anϑn(ϑ−nM n), n ∈ N. By (6.4), (6.9) and
using that (cid:0) λ2
λ1

(cid:1)n → 0 as n → ∞ (due to |λ2| < |λ1|), we have

Anϑn = λn

1 An

1
λ1 − λ2

(cid:35)

(cid:34)

λ1 −λ1λ2
1 −λ2

(cid:19)n

+

(cid:18) λ2
λ1

λn
1 An

1
λ1 − λ2

(cid:35)

(cid:34)

−λ2 λ1λ2
λ1
−1

P-a.s.−→

(cid:112)λ2
1 − 1
|Y |

(cid:34)

1

(cid:35)

0

0 |λ1|

1
λ1 − λ2

(cid:34)

λ1 −λ1λ2
1 −λ2

(cid:35)

as n → ∞,

where Y is given in (3.2). Hence, by Slutsky’s lemma, (4.7) yields that

AnM n

D(P)
−→

(cid:112)λ2

1 − 1

(cid:34)
1

0

(cid:35) (cid:34)

|Y |(λ1 − λ2)

0 |λ1|

λ1 −λ1λ2
1 −λ2

(cid:35) ∞
(cid:88)

k=1

ϑ−k (cid:101)Y ζk

as n → ∞.

Using (6.4), for each k ∈ N, we have

ϑ−k =

(cid:34)

λ+ λ−
1
1

(cid:35) (cid:34)

λ−k
+
0

(cid:35)−1

(cid:35) (cid:34)

λ+ λ−
1
1

0
λ−k
−

=

λ−k
1
λ1 − λ2

(cid:34)

λ1 −λ1λ2
1 −λ2

(cid:35)

+

λ−k
2
λ1 − λ2

(cid:34)

−λ2 λ1λ2
λ1
−1

(cid:35)

.

Hence for each k ∈ N, we get

(cid:35)

(cid:34)

λ1 −λ1λ2
1 −λ2

ϑ−k =

λ−k
1
λ1 − λ2

(4.8)

=

λ−k
1
λ1 − λ2

(cid:34)

(cid:34)

Consequently,

λ1 −λ1λ2
1 −λ2

(cid:35)2

+

λ−k
2
λ1 − λ2

(cid:34)

λ1 −λ1λ2
1 −λ2

(cid:35) (cid:34)

−λ2 λ1λ2
λ1
−1

(cid:35)

(cid:35)2

.

λ1 −λ1λ2
1 −λ2

AnM n

D(P)
−→

(cid:112)λ2

1 − 1

(cid:34)

1

(cid:35) ∞
(cid:88)

0

|Y |(λ1 − λ2)

0 |λ1|

k=1

(cid:34)

λ−k
1
λ1 − λ2

λ1 −λ1λ2
1 −λ2

(cid:35)2

(cid:101)Y ζk

as n → ∞.

16

Here, using (4.8), we have

(cid:34)

λ1 −λ1λ2
1 −λ2

(cid:35)

(cid:101)Y =

=

=

(cid:34)

(cid:34)

(cid:34)

λ1 −λ1λ2
1 −λ2

(cid:35) (cid:32)(cid:34)

X0
X−1

(cid:35)

+

∞
(cid:88)

(cid:96)=1

ϑ−(cid:96)

(cid:35)(cid:33)

(cid:34)

Z(cid:96)
0

λ1 −λ1λ2
1 −λ2

(cid:35)

(cid:35) (cid:34)

X0
X−1

λ1 −λ1λ2
1 −λ2

(cid:35) (cid:32)(cid:34)

X0
X−1

+

(cid:35)

∞
(cid:88)

(cid:96)=1

λ−(cid:96)
1
λ1 − λ2

(cid:34)

λ1 −λ1λ2
1 −λ2

(cid:35)2 (cid:34)

(cid:35)

Z(cid:96)
0

+

1
λ1 − λ2

(cid:32) ∞
(cid:88)

(cid:96)=1

λ−(cid:96)
1 Z(cid:96)

(cid:35)(cid:33)

.

(cid:33) (cid:34)

λ1
1

This yields that

AnM n

D(P)
−→

as n → ∞. Here

(cid:112)λ2

1 − 1

(cid:34)

1

0

(cid:35) (cid:34)

|Y |(λ1 − λ2)

0 |λ1|

(cid:35)2

λ1 −λ1λ2
1 −λ2

(cid:35)

(cid:32)(cid:34)

X0
X−1

×

+

1
λ1 − λ2

(cid:32) ∞
(cid:88)

(cid:96)=1

λ−(cid:96)
1 Z(cid:96)

(cid:33) (cid:34)

λ1
1

(cid:35)(cid:33) ∞
(cid:88)

k=1

λ−k
1
λ1 − λ2

ζk

(cid:34)

1

0

(cid:35) (cid:34)

0 |λ1|

λ1 −λ1λ2
1 −λ2

(cid:35)2 (cid:32)(cid:34)

(cid:35)

X0
X−1

+

1
λ1 − λ2

(cid:32) ∞
(cid:88)

(cid:96)=1

λ−(cid:96)
1 Z(cid:96)

(cid:35)(cid:33)

(cid:33) (cid:34)

λ1
1

=

=

(cid:34)

1

0

(cid:35) (cid:34)

0 |λ1|

λ1 −λ1λ2
1 −λ2

(cid:35) (cid:32)(cid:34)

λ1(X0 − λ2X−1)
X0 − λ2X−1

(cid:35)

(cid:32) ∞
(cid:88)

+

(cid:96)=1

λ−(cid:96)
1 Z(cid:96)

(cid:35)(cid:33)

(cid:33) (cid:34)

λ1
1

(cid:34)

1

0

(cid:35) (cid:34)

0 |λ1|

λ1 −λ1λ2
1 −λ2

(cid:35)

(cid:35) (cid:34)

(λ1 − λ2)Y
λ1−λ2
λ1

Y

= (λ1 − λ2)

(cid:34)

1

0

(cid:35) (cid:34)

0 |λ1|

λ1 −λ1λ2
1 −λ2

(cid:35) (cid:34)

(cid:35)

Y.

1
1
λ1

Consequently,

AnM n

D(P)
−→

(cid:112)λ2
1 − 1
λ1 − λ2

Y
|Y |

(cid:34)

1

0

(cid:35) (cid:34)

0 |λ1|

λ1 −λ1λ2
1 −λ2

(cid:35) (cid:34)

(cid:35) ∞
(cid:88)

k=1

1
1
λ1

λ−k
1 ζk

as n → ∞.

(ζk)k∈N is independent of

(ζk)k∈N is
Recall that
independent of Y as well. Consequently, since
is normally
distributed with mean zero, and the law of a normally distributed random variable with mean

(X0, X−1) and (Zn)n∈N, and hence
|Y | = sign(Y ), (cid:80)∞
1 ζk

k=1 λ−k

Y

17

zero is invariant under an orthogonal transformation, we have

AnM n

D(P)
−→

(cid:112)λ2
1 − 1
λ1 − λ2

(cid:34)

1

0

(cid:35) (cid:34)

0 |λ1|

λ−k
1 ζk

(cid:35) ∞
(cid:88)

k=1

(cid:35)

λ1 − λ2
1 − λ2
λ1

(cid:34)

1

(cid:113)

λ2
1 − 1

=

∞
(cid:88)

k=1

λ−k
1 ζk

sign(λ1)

as n → ∞,

where (cid:112)λ2

1 − 1 (cid:80)∞

k=1 λ−k

1 ζk is normally distributed with mean zero and variance

(λ2

1 − 1)

∞
(cid:88)

k=1

1 = (λ2
λ−2k

1 − 1)

λ−2
1
1 − λ−2
1

= 1,

i.e., (cid:112)λ2

1 − 1 (cid:80)∞

k=1 λ−k

1 ζk is standard normally distributed. This yields (4.3), as desired.

Next we compare our proof technique with that of Th´eor`eme 1 in Touati [17]. The proofs
for the purely explosive case in Th´eor`eme 1 in Touati [17] are based on a limit theorem for a
triangular array which does not satisfy the asymptotically negligible condition stating stable
convergence of the row-sums of the triangular array in question, see Touati [17, Th´eor`eme A].
Our Theorem B.1 might be considered to be similar to Th´eor`eme A in Touati [17], but in fact
these two theorems are quite diﬀerent. For example, in Th´eor`eme A in Touati [17] there is
a condition on the conditional characteristic function of the row-sums of the triangular array,
while in our Theorem B.1 (speciﬁed to triangular arrays) one can ﬁnd a similar condition only
on the conditional charasteristic function of the summands.
In the proof of Th´eor`eme 1 in
Touati [17] the author eﬀectively used his Th´eor`eme A for proving convergence in distribution
of the appropriately normalized LSE of the AR parameters, but stable (mixing) convergence of
the LSE in question was not proved. We also note that our random normalization is diﬀerent
from the ones θ−n(cid:104)M (cid:105)n and θn in Touati [17, Th´eor`eme 1], which contain the unknown
parameters to be estimated, while our random normalization in Theorem 3.1 does not contain
the unknown parameters, and hence one can easily use Theorem 3.1 for constructing conﬁdence
regions, see Remark 3.5. In the partially explosive case, according to Touati [17, Section 3], an
AR(2) process can be decomposed into a supercritical and a subcritical AR(1) process, and the
results on the asymptotic behaviour of the LSE of the AR parameter for a supercritical and a
subcritical AR(1) process were combined in Touati [17, Section 3]. Hence our proof technique is
completely diﬀerent from that of Touati [17, Section 3] for partially explosive AR(2) processes.
Further, note that our Theorem 3.1 also covers supercritical AR(2) processes with a unit root,
while Touati [17] did not handle this case.

Finally, we note that we tried to prove mixing (or stable-) convergence of ((cid:98)ϑ(n)

2 ) as
n → ∞ in the supercritical case using a non-random normalization, but our attempts have not
been successful so far. It is mainly due to the fact that the almost sure limit of λ−2n
(cid:104)M (cid:105)n as
n → ∞ in Corollary 3.4 is a random R2×2-valued matrix having determinant zero P-almost
surely.

1 , (cid:98)ϑ(n)

1

18

5 Simulation results

In this section we illustrate Theorem 3.1 using generated sample paths of a supercritical AR(2)
process. More precisely, by simulations, we illustrate the weak convergence

(5.1)






(cid:0)(cid:80)n

k=1 X 2

k−1

(cid:80)n
((cid:80)n

k=1 Xk−1Xk−2
k−2)1/2
k=1 X 2

(cid:1)1/2 (cid:80)n
((cid:80)n
(cid:0)(cid:80)n

k=1 Xk−1Xk−2
k−1)1/2
k=1 X 2
k=1 X 2

k−2

(cid:1)1/2

(cid:34)






(cid:35)

(cid:98)ϑ(n)
1 − ϑ1
(cid:98)ϑ(n)
2 − ϑ2

D(P)
−→ N

(cid:34)

1

(cid:35)

sign(λ1)

as n → ∞,

where N is a standard normally distributed random variable. The weak convergence (5.1)
is a consequence of the mixing convergence (3.1) in Theorem 3.1 with σ = 1,
since mixing
convergence yields convergence in distribution. We consider a supercritical AR(2) process
is a sequence of independent and identically
(Zn)n∈N
(Xn)n(cid:62)−1 given in (1.1) such that
D= N (0, 1) and we suppose (X0, X−1) = (0, 0). We
distributed random variable with Z1
choose the AR parameters ϑ1 and ϑ2
in a way that the characteristic polynomial of the
corresponding AR(2) process has two diﬀerent, real roots and the AR(2) process in question is
purely explosive (Case 1), partially explosive (Case 2), supercritical with a characteristic root
1 (Case 3), and supercritical with a characteristic root −1 (Case 4), respectively. In Table 1
we summarize these four cases, and we give our particular choices of (ϑ1, ϑ2) in these cases.

Case 1:

|λ1| > 1

Case 2:

|λ1| > 1

|λ2| > 1

|λ2| < 1

Case 3:

|λ1| > 1

λ+ = 1 or λ− = 1

(ϑ1, ϑ2)

(1,3)

(1,1)

(-1,2)

Case 4:

|λ1| > 1 λ+ = −1 or λ− = −1

(-3,-2)

Table 1: The four cases with diﬀerent, real characteristic roots: purely explosive (Case 1), partially
explosive (Case 1), supercritical with a characteristic root 1 (Case 3), and supercritical with a
characteristic root −1 (Case 4). The fourth column contains our particular choices of (ϑ1, ϑ2) in
Cases 1–4.

1

, (cid:98)ϑ(100)

For every particular choice of (ϑ1, ϑ2) we generate 1000 independent 102-length trajectories
of (Xn)n(cid:62)−1, and for each of them we calculate the corresponding LSE ((cid:98)ϑ(100)
) and
the left hand side of (5.1) with n = 100, which we simply call the scaled error of the LSE in
question. We used the open software R for making the simulations. Since the AR(2) process in
question is supercritical, the generated value for X100 is quite large (for our particular choices
of parameters, its order is between 1020 and 1035), and, because of this, we used a precision
of 800 bits for the calculations. By making simulations, we realized that the two coordinates
of the scaled error in question are almost the same (in case of λ1 > 1) or are almost the
(−1)-time of each other (in case of λ1 < −1) despite the high precision. More precisely, for
the 1000 generated trajectories, the Euclidean distance between the two vectors consisting of
the ﬁrst and second coordinate of the scaled error in question, respectively, is of order between
10−51 and 10−28. As a consequence, the descriptive statistics and p-values of some tests for

2

19

2

2

2

1

1

1

2

1

.

, (cid:98)ϑ(100)

and that of the LSE (cid:98)ϑ(100)

the scaled error of the LSE (cid:98)ϑ(100)
based on our 1000 replications and those for the scaled error
of the LSE (cid:98)ϑ(100)
are basically the same when we use the built-in functions of the software R.
Because of this, in what follows, we do not distinguish between the scaled errors of the LSE
(cid:98)ϑ(100)
, and we will simply write ”scaled errors of the LSEs of the
parameters”. There will be one exception when we calculate the empirical covariance between
the scaled errors of the LSE (cid:98)ϑ(100)

and that of the LSE (cid:98)ϑ(100)
For every particular choice of (ϑ1, ϑ2), we plot the density histogram of the scaled errors of
the LSEs of the parameters based on the 1000 replications, and we also plot the density function
of a standard normally distributed random variable in red on the same ﬁgure. We calculate the
empirical mean of the LSEs ((cid:98)ϑ(100)
) based on our 1000 replications. We also calculate
the empirical mean, variance, median, skewness, kurtosis and interquartile range of the scaled
errors of the LSEs of the parameters based on the 1000 replications. These values are expected
to be around 0, 1, 0, 0, 3 and Φ−1(0.75) − Φ−1(0.25) ≈ 1.349 (where Φ denotes the
distribution function of a standard normally distributed random variable), since, due to (5.1),
the scaled errors of the LSEs of the parameters converge in distribution to a standard normal
distribution as n → ∞, and the (theoretical) mean, variance, median, skewness, kurtosis,
and interquartile range of a standard normal distribution are the above listed values. We also
and (cid:98)ϑ(100)
calculate the empirical covariance between the scaled errors of the LSEs (cid:98)ϑ(100)
,
which is expected to be around 1 if λ1 > 0 and around −1 if λ1 < 0 due to (5.1). Using
Kolmogorov-Smirnov, Pearson’s chi-squared, Anderson-Darling and Jarque-Bera tests, we test
whether the scaled errors of the LSEs of the parameters follow a normal distribution based on
the 1000 replications of the 102-length trajectories of the AR(2) process in question. Using
Kolmogorov-Smirnov test, we also test whether the scaled errors of the LSEs of the parameters
follow a standard normal distribution. We collect the p-values for these tests in a corresponding
table.

1

2

For each of our four particular choices of

(ϑ1, ϑ2)

corresponding values of λ1 and λ2, and the empirical mean of the LSEs ((cid:98)ϑ(100)
on our 1000 replications, see Table 2.

1

(given in Table 1), we calculate the
) based

, (cid:98)ϑ(100)

2

λ1

λ2

Empirical mean of the LSEs ( (cid:98)ϑ(100)

1

, (cid:98)ϑ(100)
2

)

(ϑ1, ϑ2) = (1, 3)

2.302776

-1.302776

(ϑ1, ϑ2) = (1, 1)

1.618034

-0.618034

(ϑ1, ϑ2) = (−1, 2)

(ϑ1, ϑ2) = (−3, −2)

-2

-2

1

-1

(0.999999, 3.000001)

(1.013616, 0.977967)

(-1.016926, 1.966146)

(-2.980015, -1.960030)

Table 2: Values of λ1 and λ2, and the empirical mean of the LSEs ( (cid:98)ϑ(100)
replications.

1

, (cid:98)ϑ(100)
2

) based on 1000

Figure 1 contains the density histograms for the scaled errors for our four particular choices
(ϑ1, ϑ2), and it supports the standard normality of the scaled errors of the LSEs of the

of
parameters for each of the four particular choices of (ϑ1, ϑ2).

20

(a) (ϑ1, ϑ2) = (1, 3)

(b) (ϑ1, ϑ2) = (1, 1)

(c) (ϑ1, ϑ2) = (−1, 2)

(d) (ϑ1, ϑ2) = (−3, −2)

Figure 1: Density histogram for the scaled errors.

Table 3 contains the empirical mean, variance, median, skewness, kurtosis, interquartile
range (IQR) and covariance for the scaled errors of the parameters for each of the four particular
choices of (ϑ1, ϑ2), and, in order to help the readers, in the second row of the table we present
the corresponding theoretical values as well. The simulation results in Table 3 support the
standard normality of the scaled errors of the LSEs of the considered four parameters.

Descriptive statistics Mean

Variance Median

Skewness Kurtosis

IQR

Covariance

Theoretical values

0

1

0

0

3

≈ 1.349

sign(λ1)

(ϑ1, ϑ2) = (1, 3)

(ϑ1, ϑ2) = (1, 1)

0.00753

0.94871

0.04073

-0.06399

3.03043

1.30492

0.94776

0.01016

0.95664

-0.01613

-0.03674

2.97045

1.30881

0.95568

(ϑ1, ϑ2) = (−1, 2)

-0.03357

0.98938

-0.02913

-0.03199

3.16162

1.30595

-0.98839

(ϑ1, ϑ2) = (−3, −2)

0.02392

0.94096

0.02936

-0.12024

3.00297

1.31009

-0.94003

Table 3: Empirical mean, variance, median, skewness, kurtosis, interquartile range and covariance for
the scaled errors. The second row contains the corresponding theoretical values.

Table 4 contains the p-values of the tests mentioned earlier for our four particular choices
(given in Table 1), and it shows that at any reasonable signiﬁcance level all the
(ϑ1, ϑ2)

of

21

Histogram of scaled errorsscaled errorsDensity−3−2−101230.00.10.20.30.4Histogram of scaled errorsscaled errorsDensity−4−3−2−101230.00.10.20.30.4Histogram of scaled errorsscaled errorsDensity−3−2−101230.00.10.20.30.4Histogram of scaled errorsscaled errorsDensity−4−3−2−101230.00.10.20.30.4considered four tests accept normality, and the Kolmogorov-Smirnov test accepts standard
normality of the scaled errors in question.

p-values of normality tests KS test KS test for N (0, 1) PCS test AD test

JB test

(ϑ1, ϑ2) = (1, 3)

(ϑ1, ϑ2) = (1, 1)

(ϑ1, ϑ2) = (−1, 2)

(ϑ1, ϑ2) = (−3, −2)

0.948

0.9304

0.8643

0.8454

0.6133

0.6079

0.5518

0.61

0.8376

0.4588

0.6415

0.6973

0.4344

0.8775

0.226

0.5238

0.5329

0.8005

0.5944

0.2997

Table 4: The p-values of Kolmogorov-Smirnov (KS), Pearson’s chi-squared (PCS), Anderson-Darling
(AD) and Jarque-Bera (JB) tests for testing normality of the scaled errors. The 3rd column contains
the p-values of Kolmogorov-Smirnov test for testing standard normality of the scaled errors.

All in all, our simulation results are in accordance with our theoretical results in Theorem

3.1.

6 Proofs

Proof of Lemma 2.1. For each n ∈ N, (ϑ1, ϑ2)(cid:62) ∈ R2 and (x−1, x0, x1, . . . , xn)(cid:62) ∈ Rn+2,
we have

Qn(x−1, x0, x1, . . . , xn; ϑ1, ϑ2) = ϑ2
1

n
(cid:88)

k=1

x2
k−1 + 2ϑ1ϑ2

n
(cid:88)

k=1

xk−1xk−2 + ϑ2
2

n
(cid:88)

k=1

x2
k−2

hence the function R2 (cid:51) (ϑ1, ϑ2)(cid:62) (cid:55)→ Qn(x−1, x0, x1, . . . , xn; ϑ1, ϑ2) is strictly convex if

− 2ϑ1

n
(cid:88)

k=1

xkxk−1 − 2ϑ2

n
(cid:88)

k=1

xkxk−2 +

n
(cid:88)

k=1

x2
k,

det

(cid:32)(cid:34) (cid:80)n
(cid:80)n

k−1

k=1 x2
k=1 xk−1xk−2

(cid:80)n

k=1 xk−1xk−2
(cid:80)n
k=1 x2

k−2

(cid:35)(cid:33)

= det(Gn(x−1, x0, x1, . . . , xn)) > 0.

Indeed, if det(Gn(x−1, x0, x1, . . . , xn)) > 0, then (cid:80)n

k=1 x2

Gn(x−1, x0, x1, . . . , xn) =

k−1 > 0 holds as well, so the matrix
(cid:35)

(cid:80)n

(cid:34) (cid:80)n
(cid:80)n

k−1

k=1 x2
k=1 xk−1xk−2

k=1 xk−1xk−2
(cid:80)n
k=1 x2

k−2

is positive deﬁnite yielding that the function R2 (cid:51) (ϑ1, ϑ2)(cid:62) (cid:55)→ Qn(x−1, x0, x1, . . . , xn; ϑ1, ϑ2)
is strictly convex. Hence, for each n ∈ N and for any least squares estimator Fn of (ϑ1, ϑ2)(cid:62),
we have if (x−1, x0, x1, . . . , xn)(cid:62) ∈ Dn, then Fn(x−1, x0, x1, . . . , xn) is the unique solution of
the linear system of equations

(cid:40)

ϑ1
ϑ1

(cid:80)n

(cid:80)n

(cid:80)n

k=1 x2
k−1 + ϑ2
k=1 xk−1xk−2 + ϑ2

k=1 xk−1xk−2 = (cid:80)n
k−2 = (cid:80)n

k=1 x2

(cid:80)n

k=1 xkxk−1,
k=1 xkxk−2.

22

Consequently, on the set Dn, any least squares estimator Fn of (ϑ1, ϑ2)(cid:62) has the form given
(cid:50)
in (2.1).

Proof of Lemma 2.2. It is enough to check that for each n ∈ N with n (cid:62) 3, we have





P(Ωn) = P

det



(cid:34)

n
(cid:88)

k=1

Xk−1
Xk−2

(cid:35) (cid:34)

Xk−1
Xk−2

(cid:35)(cid:62)



 > 0

 = 1,

since then, by Lemma 2.1,

Fn(X−1, X0, X1, . . . , Xn) =

(cid:34)

(cid:35)

on the event Ωn,

1

(cid:98)ϑ(n)
(cid:98)ϑ(n)

2

where Fn : Rn+2 → R2 is a measurable function satisfying (2.1) on the set Dn. Here for each
n ∈ N, we get



det



(cid:34)

n
(cid:88)

k=1

Xk−1
Xk−2

(cid:35) (cid:34)

Xk−1
Xk−2

(cid:35)(cid:62)

 = det

(cid:32)(cid:34) (cid:80)n
(cid:80)n

k=1 X 2
k=1 Xk−1Xk−2

k−1

(cid:35)(cid:33)

(cid:80)n

k=1 Xk−1Xk−2
(cid:80)n
k=1 X 2

k−2

=

n
(cid:88)

k=1

X 2

k−1

n
(cid:88)

k=1

X 2

k−2 −

(cid:32) n

(cid:88)

k=1

(cid:33)2

Xk−1Xk−2

.

By Cauchy-Schwartz’s inequality, we have

n
(cid:88)

k=1

X 2

k−1

n
(cid:88)

k=1

X 2

k−2 −

(cid:32) n

(cid:88)

k=1

(cid:33)2

Xk−1Xk−2

(cid:62) 0,

n ∈ N,

and equality holds if and only if the random vectors

(cid:104)

X0 X1

. . . Xn−1

(cid:105)(cid:62)

and

(cid:104)
X−1 X0

(cid:105)(cid:62)

. . . Xn−2

are linearly dependent, i.e., there exist K, L ∈ R (depending on ω ∈ Ω) such that K 2+L2 > 0
and

(6.1)

(cid:104)
X0 X1

K

. . . Xn−1

(cid:105)(cid:62)

+ L

(cid:104)
X−1 X0

. . . Xn−2

(cid:105)(cid:62)

(cid:104)

=

0 0 . . . 0

(cid:105)(cid:62)

.

In what follows let us suppose that n ∈ N and n (cid:62) 3.
If (6.1) holds with K = 0 (yielding
L (cid:54)= 0) or L = 0 (yielding K (cid:54)= 0), then X1 = 0, which can occur only with probability zero,
is absolutely continuous (following from the facts that X1 = ϑ1X0 + ϑ2X−1 + Z1
since X1
and Z1 is absolutely continuous). So if (6.1) holds, then, using that the underlying probability
space is complete, we have {K = 0} ∪ {L = 0} ∈ F and P({K = 0} ∪ {L = 0}) = 0. Further,
if (6.1) holds with X−1 = 0, then X1 = 0, since if L = 0, then K > 0 and X0 = X1 =
· · · = Xn−1 = 0; and if L > 0 and K = 0,
then X−1 = X0 = X1 = · · · = Xn−2 = 0;
and if L > 0 and K > 0, then X0 = 0, yielding X1 = 0. Similarly, if (6.1) holds with

23

X0 = 0, then X1 = 0. So, using again the absolute continuity of X1,
P({X−1 = 0} ∪ {X0 = 0}) = 0.

if (6.1) holds, then

Consequently,





P(Ωn) = P

det



(cid:34)

n
(cid:88)

k=1

Xk−1
Xk−2

(cid:35) (cid:34)

Xk−1
Xk−2

(cid:35)(cid:62)



 = 0












X0
X1
...
Xn−1










(cid:18)

=

−

(cid:19)

L
K










X−1
X0
...
Xn−2












, K (cid:54)= 0, L (cid:54)= 0, X−1 (cid:54)= 0, X0 (cid:54)= 0








(cid:18)

Xk−1 =

−

(cid:18)

Xk−1 =

−

(cid:19)k

(cid:19)k

L
K

L
K

X−1, k = 1, . . . , n; K (cid:54)= 0, L (cid:54)= 0, X−1 (cid:54)= 0, X0 (cid:54)= 0

(cid:33)

X−1, k = 1, . . . , n;

X0
X−1

= −

L
K

, K (cid:54)= 0, L (cid:54)= 0, X−1 (cid:54)= 0, X0 (cid:54)= 0

(cid:33)

(cid:54) P

= P

(cid:54) P

(cid:54) P










(cid:32)

(cid:32)

(cid:32)

(cid:32)

(cid:54) P

X1 =

(cid:32)

(cid:18)

Z1 =

= P

(cid:90)

=

R2\{(0,0)}

X 2
0
X−1
(cid:18)

P

Xk−1 =

(cid:19)k

(cid:18) X0
X−1

X−1, k = 1, . . . , n, X−1 (cid:54)= 0, X0 (cid:54)= 0

(cid:33)

(cid:19)2

(cid:18) X0
X−1

X−1, X−1 (cid:54)= 0, X0 (cid:54)= 0

(cid:33)

= P

ϑ1X0 + ϑ2X−1 + Z1 =

(cid:19)2

(cid:18) X0
X−1

X−1, X−1 (cid:54)= 0, X0 (cid:54)= 0

(cid:33)

− ϑ1X0 − ϑ2X−1, X−1 (cid:54)= 0, X0 (cid:54)= 0

(cid:19)

Z1 =

− ϑ1x0 − ϑ2x−1

FX−1,X0(dx−1, dx0) = 0,

(cid:19)

x2
0
x−1

since Z1 and (X−1, X0) are P-independent, and Z1 is absolutely continuous, where FX−1,X0
(cid:50)
denotes the distribution function of (X−1, X0).

Proof of Theorem 3.1. We divide the proof into six steps.

Step 1 (a decomposition of the left-hand side of (3.1)): For each n ∈ N with n (cid:62) 3, by

(2.4) and (2.6), we have



(cid:0)(cid:80)n

k=1 X 2

k−1




(cid:80)n
((cid:80)n

k=1 Xk−1Xk−2
k−2)1/2
k=1 X 2

(cid:1)1/2 (cid:80)n
((cid:80)n
(cid:0)(cid:80)n

k=1 Xk−1Xk−2
k−1)1/2
k=1 X 2
k=1 X 2

k−2

(cid:1)1/2

(cid:34)






(cid:35)

=

(cid:98)ϑ(n)
1 − ϑ1
(cid:98)ϑ(n)
2 − ϑ2

24

(cid:34)(cid:0)(cid:80)n

k=1 X 2
k−1
0

=

(cid:1)−1/2

0
k=1 X 2

k−2

(cid:0)(cid:80)n

(cid:35) (cid:34) (cid:80)n
(cid:80)n

k=1 X 2
k=1 Xk−1Xk−2

k−1

(cid:80)n

k=1 Xk−1Xk−2
(cid:80)n
k=1 X 2

k−2

(cid:35)

(cid:35) (cid:34)

(cid:98)ϑ(n)
1 − ϑ1
(cid:98)ϑ(n)
2 − ϑ2

(cid:1)−1/2

= (σ−1An)(σ2(cid:104)M (cid:105)n)((cid:104)M (cid:105)−1

n M n) = σAnM n,

where

(6.2)

An := σ

(cid:34)(cid:0)(cid:80)n

k=1 X 2
k−1
0

(cid:1)−1/2

0
k=1 X 2

k−2

(cid:0)(cid:80)n

(cid:35)

,

(cid:1)−1/2

n ∈ N,

((cid:98)ϑ(n)

on an event having probability one. Indeed, by Lemma 2.2, for each n ∈ N with n (cid:62) 3,
1 , (cid:98)ϑ(n)
the least squares estimator
(given in (2.2))
is absolutely continuous (following from the facts that
having probability one, and, since X1
k−1 > 0) =
X1 = ϑ1X0 + ϑ2X−1 + Z1 and Z1
P((cid:80)n
k−2 > 0) = 1 for each n ∈ N with n (cid:62) 3, yielding that An is well-deﬁned for
each n ∈ N with n (cid:62) 3 P-almost surely. Hence, by part (c) of Theorem 3.18 in H¨ausler and
Luschgy [6], in order to prove (3.1), it is enough to show that

is absolutely continuous), we have P((cid:80)n

2 )(cid:62) exists uniquely on the event Ωn

k=1 X 2

k=1 X 2

(6.3)

AnM n → N

(cid:34)

(cid:35)

1

sign(λ1)

F∞-mixing as n → ∞.

(Bn)n∈N :=
We are going to apply Theorem B.1 with d := 2,
(An)n∈N, (Qn)n∈N := (λ−n
1 I 2)n∈N, (Fn)n∈Z+ := (σ(X−1, X0, X1, . . . , Xn))n∈Z+, and G := Ω
(the random matrix η and the matrix P appearing in assumptions (i) and (iii) of Theorem
B.1, respectively, will be chosen later on in Step 3). Note that An is invertible for each n ∈ N
with n (cid:62) 3 P-almost surely.

(U n)n∈Z+ := (M n)n∈Z+,

Step 2 (asymptotic behaviour of An given in (6.2) as n → ∞): In order to check the
conditions (i)–(iv) of Theorem B.1 with the choices given at the end of Step 1, we need the
asymptotic behavior of An as n → ∞. The vectors

(cid:35)

(cid:34)

λ+
1

and

(cid:35)

(cid:34)

λ−
1

are right eigenvectors of the matrix ϑ corresponding to the eigenvalues λ+ and λ− (given in
(1.3)), respectively. Due to our assumption |λ1| > |λ2| and our notations, we have λ+ (cid:54)= λ−,
hence the matrix ϑ can be written in a Jordan canonical form

ϑ =

(cid:34)

λ+ λ−
1
1

(cid:35) (cid:34)

λ+
0

0

λ−

(cid:35) (cid:34)

λ+ λ−
1
1

(cid:35)−1

.

25

Consequently, for each n ∈ Z+, we have

(6.4)

ϑn =

(cid:34)

λ+ λ−
1
1

(cid:35) (cid:34)

λ+
0

0

λ−

(cid:35)n (cid:34)

λ+ λ−
1
1

(cid:35)−1

(cid:34)

(cid:34)

=

1
λ+ − λ−

=

λn
+
λ+ − λ−

λ+ λ−
1
1

(cid:35) (cid:34)

λn
+
0

0
λn
−

(cid:35) (cid:34)

1 −λ−
λ+
−1

(cid:35)

λ+ −λ−λ+
1

−λ−

(cid:34)

(cid:35)

+

λn
−
λ+ − λ−

−λ− λ+λ−
−1

λ+

(cid:35)

=

λn
1
λ1 − λ2

(cid:35)

(cid:34)

λ1 −λ1λ2
1 −λ2

+

λn
2
λ1 − λ2

(cid:34)

(cid:35)

.

−λ2 λ1λ2
λ1
−1

For each n ∈ N, by (1.2) with k = n, we obtain

(6.5)

Xn =

(cid:34)

(cid:35)(cid:62) (cid:34)
1

0

(cid:35)

=

Xn
Xn−1

(cid:35)(cid:62)
(cid:34)
1

0

ϑn

(cid:35)

+

(cid:34)

X0
X−1

n
(cid:88)

(cid:34)

(cid:35)(cid:62)
1

0

j=1

ϑn−j

(cid:34)

(cid:35)

.

Zj
0

Hence for each n ∈ Z+, by (6.4), we have

(cid:34)

(cid:35)(cid:62)
1

0

ϑn =

λn+1
1
λ1 − λ2

(cid:34)

1

(cid:35)(cid:62)

−λ2

+

λn+1
2
λ1 − λ2

(cid:34)

(cid:35)(cid:62)

−1

λ1

.

Thus, by (6.5), for each n ∈ N, we get

Xn =

λn+1
1
λ1 − λ2

(X0 − λ2X−1) +

λn+1
2
λ1 − λ2

(−X0 + λ1X−1)

+

λ1
λ1 − λ2

n
(cid:88)

j=1

λn−j
1 Zj +

λ2
λ1 − λ2

n
(cid:88)

j=1

λn−j
2

(−Zj).

Hence we obtain

λ−n
1 Xn =

λ1
λ1 − λ2

(X0 − λ2X−1) +

λ2
λ1 − λ2

(−X0 + λ1X−1)

(cid:19)n

(cid:18) λ2
λ1

(6.6)

+

λ1
λ1 − λ2

n
(cid:88)

j=1

λ−j
1 Zj −

λ2
λ1 − λ2

(cid:18) λ2
λ1

(cid:19)n n

(cid:88)

j=1

λ−j
2 Zj

P-a.s.−→

λ1
λ1 − λ2

(X0 − λ2X−1) +

λ1
λ1 − λ2

∞
(cid:88)

j=1

λ−j
1 Zj = Y

as n → ∞,

where the P-almost sure absolute convergence of the series (cid:80)∞
follows by Lemma 8.1
in H¨ausler and Luschgy [6]. Indeed, since EP(log+(|Z1|)) < ∞, λ1 ∈ R and |λ1| > 1, Lemma

j=1 λ−j

1 Zj

26

8.1 in H¨ausler and Luschgy [6] yields the P-almost sure absolute convergence of (cid:80)∞
and we check that (cid:0) λ2
λ1
(cid:16) λ2
(cid:80)∞
j=1 λ−j
λ1

1 Zj,
P-a.s.−→ 0 as n → ∞. For this it is enough to verify that
< ∞ for each ε ∈ R++. Using that Zj, j ∈ N, are i.i.d.

(cid:1)n (cid:80)n
j=1 λ−j
2 Zj
(cid:12)
(cid:17)
(cid:12)
(cid:12) > ε

2 Zj
random variables having zero mean and variance σ2,

j=1 λ−j

(cid:17)n (cid:80)n

(cid:16)(cid:12)
(cid:12)
(cid:12)

n=1

P

∞
(cid:88)

P

n=1

(cid:32)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:18) λ2
λ1

(cid:19)n n

(cid:88)

j=1

λ−j
2 Zj

(cid:33)

> ε

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:54) 1
ε2

=

1
ε2

∞
(cid:88)

n=1

∞
(cid:88)

n=1

for each ε ∈ R++, we have
2



EP



(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:18) λ2
λ1

(cid:19)n n

(cid:88)

j=1

λ−j
2 Zj



(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:18) λ2
λ1

(cid:19)2n n

(cid:88)

j=1

λ−2j
2 σ2 < ∞,

since

and if

|λ2| = 1, then

n
(cid:88)

j=1

λ−2j
2 =






n
λ−2
2

λ−2n
2 −1
λ−2
2 −1

if

if

|λ2| = 1,
|λ2| (cid:54)= 1,

lim sup
n→∞

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:17)2(n+1)

(cid:16) λ2
λ1

(n + 1)

(cid:17)2n

(cid:16) λ2
λ1

n

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

=

2

(cid:12)
(cid:12)
(cid:12)
(cid:12)

λ2
λ1

(cid:12)
(cid:12)
(cid:12)
(cid:12)

= |λ1|−2 < 1,

and if

|λ2| (cid:54)= 1, then |λ1| > |λ2|

implies that

lim sup
n→∞

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:17)2(n+1)

(cid:16) λ2
λ1

(cid:17)2n

(cid:16) λ2
λ1

λ−2(n+1)
2
λ−2
2 −1

−1

λ−2n
2 −1
λ−2
2 −1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

=

2

(cid:12)
(cid:12)
(cid:12)
(cid:12)

λ2
λ1

(cid:12)
(cid:12)
(cid:12)
(cid:12)

lim sup
n→∞

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

− 1

λ−2(n+1)
2
λ−2n
2 − 1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

=






(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

λ2
λ1

λ2
λ1

2

2

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

< 1

lim supn→∞

(cid:12)
(cid:12)
(cid:12)

λ−2
2 −λ2n
2
1−λ2n
2

(cid:12)
(cid:12)
(cid:12) =

(cid:12)
(cid:12)
(cid:12)

λ2
λ1

2

(cid:12)
(cid:12)
(cid:12)

if

|λ2| > 1,

|λ2|−2 = |λ1|−2 < 1 if

|λ2| < 1.

By D’Alambert’s criteria, this yields that the series (cid:80)∞
We note that if, in addition,
|λ1| > |λ2| > 1), then (cid:0) λ2
λ1
case, by Lemma 8.1 in H¨ausler and Luschgy [6], (cid:80)∞
and, since |λ2| < |λ1|, we have (cid:0) λ2

(cid:1)2n (cid:80)n
is convergent.
is purely explosive as well (i.e., λ1, λ2 ∈ R and
P-a.s.−→ 0 as n → ∞ follows more easily, since in this
is absolutely convergent P-a.s.,
j=1 λ−j

j=1 λ−j
2 Zj
P-a.s.−→ 0 · (cid:80)∞

2 Zj = 0 as n → ∞.

(Xk)k(cid:62)−1
j=1 λ−j
2 Zj

j=1 λ−2j

j=1 λ−j

(cid:1)n (cid:80)n

(cid:1)n (cid:80)n

(cid:0) λ2
λ1

2 Zj

n=1

2

λ1

The random variable Y given in (3.2) is absolutely continuous. Indeed,

Y =

λ1
λ1 − λ2

(X0 − λ2X−1 + λ−1

1 Z1) +

λ1
λ1 − λ2

∞
(cid:88)

j=2

λ−j
1 Zj,

where the absolute continuity of Z1 and the independence of Z1 and (X−1, X0) yield the
absolute continuity of X0 −λ2X−1 +λ−1
1 Zj and X0 −λ2X−1 +
λ−1
1 Z1 are independent, we have the absolute continuity of Y .

1 Z1. Hence, using that (cid:80)∞

j=2 λ−j

27

By (6.6), we get λ−2n

= ∞ (due to
|λ1| > 1, λ1 ∈ R), applying the Toeplitz lemma (see, e.g., H¨ausler and Luschgy [6, Lemma
6.28]), we get

P-a.s.−→ Y 2 as n → ∞, and, since (cid:80)∞

k=1 λ2(k−1)

1 X 2
n

1

Consequently, using (cid:80)n

k=1 λ2(k−1)

1

(6.7)

λ−2n
1

n
(cid:88)

k=1

X 2

k−1 = λ−2n

1

as n → ∞.

(cid:80)n

1

k−1

P-a.s.−→ Y 2

(cid:80)n
k=1 X 2
k=1 λ2(k−1)
= λ2n
1 −1
1−1 , we conclude
λ2
(cid:80)n
k=1 X 2
k=1 λ2(k−1)

λ2n
1 − 1
λ2
1 − 1

(cid:80)n

k−1

1

P-a.s.−→

1
λ2
1 − 1

Y 2

as n → ∞.

In a similar way, we have

(6.8)

λ−2n
1

n
(cid:88)

k=1

X 2

k−2

P-a.s.−→

1
1(λ2
λ2
1 − 1)

Y 2

as n → ∞.

Indeed, by (6.6) and (6.7), we have

λ−2n
1

n
(cid:88)

k=1

X 2

k−2 = λ−2n

1

(X 2

−1 − X 2

n−1) + λ−2n

1

n
(cid:88)

k=1

X 2

k−1

= λ−2n

1 X 2

−1 − λ−2

1 λ−2(n−1)

1

X 2

n−1 + λ−2n

1

n
(cid:88)

k=1

X 2

k−1

P-a.s.−→ 0 − λ−2

1 Y 2 + (λ2

1 − 1)−1Y 2 =

1
1(λ2
λ2
1 − 1)

Y 2

as n → ∞,

as desired.

The absolute continuity of Y implies P(Y = 0) = 0, hence, by (6.2), (6.7) and (6.8), we

obtain

(6.9)

(cid:34)(cid:0)λ−2n

1

λn
1 An = σ

(cid:80)n

k−1

k=1 X 2
0

(cid:1)−1/2

(cid:0)λ−2n

1

0
k=1 X 2

(cid:80)n

k−2

(cid:35)

(cid:1)−1/2






P-a.s.−→ σ

(cid:17)−1/2

(cid:16) Y 2
λ2
1−1

0



(cid:17)−1/2


 =

1 − 1

σ(cid:112)λ2
|Y |

(cid:34)

1

(cid:35)

0

0 |λ1|

0

(cid:16) Y 2
(λ2

1−1)λ2
1

as n → ∞.

Step 3 (checking conditions (i) and (iii) of Theorem B.1): Recall that at the end of Step 1
we gave our choices for (U n)n∈Z+, (Bn)n∈N, (Qn)n∈N, (Fn)n∈Z+ and G in Theorem B.1,
the random matrix η and the matrix P appearing in assumptions (i) and (iii) of Theorem
B.1, respectively, will be chosen below. Applying (6.9), we obtain

1 A−1
λ−n

n = (λn

1 An)−1 P-a.s.−→

(6.10)

(cid:32)

1 − 1

σ(cid:112)λ2
|Y |

(cid:34)

1

0

(cid:35)(cid:33)−1

0 |λ1|

=

|Y |
σ(cid:112)λ2

1 − 1

(cid:35)

(cid:34)

0

1
0 |λ1|−1

28

as n → ∞.

Hence, since almost sure convergence yields convergence in probability, we obtain that condition
(i) of Theorem B.1 holds with

η :=

|Y |
σ(cid:112)λ2

1 − 1

(cid:34)

(cid:35)

,

0

1
0 |λ1|−1

so P(∃ η−1) = P(Y (cid:54)= 0) = 1. Here η is
which is invertible if and only if {Y (cid:54)= 0},
F∞-measurable, since Y is F∞-mesaurable. Indeed, the series (cid:80)∞
j=1 λ−j
1 Zj converges P-a.s.
(see Step 2), (cid:80)n
is F∞-measurable for all n ∈ N (due to σ(X−1, X0, X1, . . . , Xn) =
σ(X−1, X0, Z1, . . . , Zn), n ∈ N), and the underlying probability space (Ω, F, P) is complete.
Moreover, by (6.10), for every r ∈ N, we have

j=1 λ−j

1 Zj

AnA−1

n−r = λ−r

1 (λn

1 An)(λ−(n−r)

1

A−1

n−r)

P-a.s.−→ λ−r

1 η−1η = λ−r

1 I 2

as n → ∞,

hence we obtain that condition (iii) of Theorem B.1 holds with P := λ−1
spectral radius is strictly less than 1, since |λ1| > 1).

1 I 2

(of which the

Step 4 (checking condition (ii) of Theorem B.1): Condition (ii) of Theorem B.1 with the
is stochastically

earlier given choices (see the end of Step 1) holds if and only if (cid:0)λ−n
bounded for each j ∈ {1, 2}, where (M (j)
Indeed,
P(G ∩ {∃ η−1}) = P(Y (cid:54)= 0) = 1 yielding that PG∩{∃ η−1} = P, and for each n ∈ Z+ and
K ∈ (0, ∞), we have

j M n)n∈Z+ for j ∈ {1, 2}.

n )n∈Z+ := (e(cid:62)

1 M (j)

n∈Z+

(cid:1)

n

P((cid:107)λ−n

1 M n(cid:107) > K) (cid:54) P

and

(cid:18)

|λ−n

1 M (1)

n | >

(cid:19)

K
√
2

+ P

(cid:18)

|λ−n

1 M (2)

n | >

(cid:19)

,

K
√
2

P(|λ−n

1 M (j)

n | > K) (cid:54) P((cid:107)λ−n

1 M n(cid:107) > K),

j = 1, 2.

By (2.6), the process (M (1)
(Fn)n∈Z+ and it has a quadratic characteristic process

n )n∈Z+ is a square integrable martingale with respect to the ﬁltration

(cid:104)M (1)(cid:105)n = σ−2

n
(cid:88)

k=1

X 2

k−1,

n ∈ N,

with (cid:104)M (1)(cid:105)0 = 0. For each n ∈ N and K ∈ (0, ∞), by Lenglart’s inequality (see Corollary
C.2), we get

P(|λ−n

1 M (1)

n | (cid:62) K) = P(|M (1)

n |2 (cid:62) K 2λ2n

1 ) (cid:54) 1
K

+ P((cid:104)M (1)(cid:105)n > Kλ2n

1 ),

so that for each K ∈ (0, ∞), we have

(6.11)

sup
n∈N

P(|λ−n

1 M (1)

n | (cid:62) K) (cid:54) 1
K

+ sup
n∈N

P(λ−2n
1

(cid:104)M (1)(cid:105)n > K).

29

By (6.7), we get

λ−2n
1

(cid:104)M (1)(cid:105)n = λ−2n

1 σ−2

n
(cid:88)

k=1

X 2

k−1

P-a.s.−→

Y 2
1 − 1)σ2

(λ2

as n → ∞,

hence (cid:0)λ−2n

1

(cid:104)M (1)(cid:105)n

(cid:1)
n∈Z+

is stochastically bounded, i.e.,

lim
K→∞

sup
n∈N

P(|λ−2n
1

(cid:104)M (1)(cid:105)n| > K) = 0.

Consequently, by (6.11),
is stochastically bounded. Using (6.8),
(cid:0)λ−n
1 M (2)
B.1 holds.

(cid:1)
n∈Z+

n

limK→∞ supn∈N P(|λ−n

1 M (1)

n | (cid:62) K) = 0,

n∈Z+
in a similar way, one can check that the process
is stochastically bounded, and we conclude that condition (ii) of Theorem

i.e.,

(cid:0)λ−n

1 M (1)

n

(cid:1)

Step 5 (checking condition (iv) of Theorem B.1): In order to check condition (iv) of Theorem
B.1, let us observe that the square integrable martingale (M n)n∈Z+ has conditional Gaussian
increments with respect to the ﬁltration (Fn)n∈Z+,
the conditional
Xn−1
Xn−2

distribution of ∆M n = M n − M n−1 = 1

since for each n ∈ N,

given Fn−1 is

σ2 Zn

(cid:35)

(cid:34)



N2

0,

1
σ4

EP(Z 2
n)

(cid:34)

Xn−1
Xn−2

(cid:35) (cid:34)

Xn−1
Xn−2

(cid:35)(cid:62)



 = N2

0,

1
σ2

(cid:34)

Xn−1
Xn−2

(cid:35) (cid:34)

Xn−1
Xn−2

(cid:35)(cid:62)

 = N2(0, ∆(cid:104)M (cid:105)n),

where the last equality follows by (2.6). More precisely, using the notations and results of
H¨ausler and Luschgy [6], for each n ∈ N, the conditional distribution P∆M n | Fn−1 of ∆M n
given Fn−1 can be calculated as follows:

P∆M n | Fn−1 = Pσ−2Zn[Xn−1, Xn−2](cid:62) | Fn−1 = Pg(Xn−2,Xn−1,Zn) | Fn−1 =

(cid:16)

P(Xn−2,Xn−1,Zn) | Fn−1

(cid:17)g

(cid:16)

=

δ(Xn−2,Xn−1) ⊗ PZn | Fn−1

(cid:17)g

= (cid:0)δ(Xn−2,Xn−1) ⊗ PZn(cid:1)g

,

g(x1, x2, z) := σ−2z[x2, x1](cid:62),
δ(Xn−2,Xn−1)

PZn denotes the
where g : R3 → R2,
distribution of Zn under P,
is the Dirac Markov kernel corresponding to
(Xn−2, Xn−1), and we used part (a) of Lemma A.5 in H¨ausler and Luschgy [6], parts (b) and
(c) of Lemma A.4 in H¨ausler and Luschgy [6], the independence of Zn and Fn−1, and the
Fn−1-measurability of Xn−1 and Xn−2. Hence for each n ∈ N, ω ∈ Ω, and B ∈ B(R2),
we have

(x1, x2, z) ∈ R3,

P∆M n | Fn−1(ω, B) = (δ(Xn−2,Xn−1) ⊗ PZn)(cid:0)ω, {(x1, x2, z)(cid:62) ∈ R3 : g(x1, x2, z) ∈ B}(cid:1)

= PZn(cid:0){z ∈ R : σ−2z[Xn−1(ω), Xn−2(ω)](cid:62) ∈ B}(cid:1)
= P(cid:0){(cid:101)ω ∈ Ω : σ−2Zn((cid:101)ω)[Xn−1(ω), Xn−2(ω)](cid:62) ∈ B}(cid:1)
= Pζn,ω (B),

30

where ζn,ω is an R2-valued random variable having distribution

(cid:34)


0, σ−2

N2

(cid:35) (cid:34)

Xn−1(ω)
Xn−2(ω)

Xn−1(ω)
Xn−2(ω)

(cid:35)(cid:62)

 = N2(0, ∆(cid:104)M (cid:105)n(ω)),

as desired. Consequently, for each θ ∈ R2 and n ∈ N, we obtain

(cid:18)
exp(cid:8)i (cid:104)θ, An∆M n(cid:105)(cid:9)

EP

(cid:19)

(cid:18)

exp

= EP

Fn−1

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:26)

i (cid:10)A(cid:62)

n θ, ∆M n

(cid:19)

Fn−1

(cid:11)

(cid:27) (cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:26)

= exp

−

(cid:68)

1
2

(∆(cid:104)M (cid:105)n)A(cid:62)

n θ, A(cid:62)
n θ

(cid:69)(cid:27)

(cid:26)

= exp

−

1
2

θ(cid:62)An(∆(cid:104)M (cid:105)n)Anθ

(cid:27)
,

where, at the last equality, we used that An is symmetric. Using (2.6) and (6.6), we get
(cid:35)

(cid:35) (cid:34)

(cid:35)(cid:62)

(cid:34)

(cid:34)

1 ∆(cid:104)M (cid:105)n = σ−2λ−2n
λ−2n

1

Xn−1
Xn−2

Xn−1
Xn−2

X 2

n−1
Xn−1Xn−2

Xn−1Xn−2
X 2

n−2

(6.12)

P-a.s.−→ σ−2Y 2

(cid:34)

λ−2
1
λ−3
1

(cid:35)

λ−3
1
λ−4
1

= σ−2λ−2n

1

as n → ∞.

Applying (6.9) and (6.12), since P(Y = 0) = 0, we get
1 ∆(cid:104)M (cid:105)n)(λn

An(∆(cid:104)M (cid:105)n)An = (λn

1 An)(λ−2n

P-a.s.−→

(λ2

1 − 1)σ2
Y 2

·

Y 2
σ2

1 An)
(cid:35) (cid:34)

0

(cid:34)
1

0 |λ1|

λ−2
1
λ−3
1

=

λ2
1 − 1
λ2
1

(cid:34)

1

sign(λ1)

(cid:35)

sign(λ1)
1

(cid:35) (cid:34)
1

(cid:35)

0

0 |λ1|

λ−3
1
λ−4
1

as n → ∞.

Hence, for each θ ∈ R2, we have
(cid:12)
(cid:12)
(cid:12)
(cid:12)

exp(cid:8)i (cid:104)θ, An∆M n(cid:105)(cid:9)

EP

(cid:18)

Fn−1

(cid:19) P-a.s.−→ exp

(cid:40)

−

λ2
1 − 1
2λ2
1

θ(cid:62)

(cid:34)

1

sign(λ1)

(cid:35)

(cid:41)

θ

sign(λ1)
1

(cid:90)

=

R2

ei(cid:104)θ,x(cid:105) Pζ(dx)

as n → ∞, where ζ is an R2-valued random variable having distribution

(cid:32)

N2

0,

λ2
1 − 1
λ2
1

(cid:34)

1

sign(λ1)

(cid:35)(cid:33)

.

sign(λ1)
1

Hence we obtain that condition (iv) of Theorem B.1 holds with µ := Pζ, since

(cid:90)

R2

log+((cid:107)x(cid:107)) µ(dx) =

(cid:90)

log((cid:107)x(cid:107)) µ(dx) (cid:54)

(cid:90)

(cid:107)x(cid:107) µ(dx)

{x∈R2:(cid:107)x(cid:107)(cid:62)1}

{x∈R2:(cid:107)x(cid:107)(cid:62)1}

(cid:90)

(cid:54)

{x=(x1,x2)(cid:62)∈R2:(cid:107)x(cid:107)(cid:62)1}

(|x1| + |x2|) µ(dx) < ∞

31

due to the fact that all the mixed moments of µ (being a 2-dimensional normal distribution)
are ﬁnite.

Step 6 (application of Theorem B.1): Using Steps 1–5, we can apply Theorem B.1 with our

choices given at the end of Step 1 and in Step 3 (for η and P ), and we obtain

(6.13)

AnM n →

(cid:34)

∞
(cid:88)

j=0

λ−j
1
0

(cid:35)

0
λ−j
1

N j

F∞-mixing as n → ∞,

where (N j)j∈Z+ is a sequence of P-independent and identically distributed R2-valued random
vectors being P-independent of F∞ with P(N 0 ∈ B) = µ(B) for all B ∈ B(R2), and the
series in (6.13) converges P-almost surely. The distribution of the limit random variable in
(6.13) can be written in the form

(cid:34)

∞
(cid:88)

j=0

λ−j
1
0

(cid:35)

0
λ−j
1

N j =

∞
(cid:88)

j=0

λ−j
1 N j

D=

(cid:34)

N

sign(λ1)N

(cid:35)

(cid:34)

= N

(cid:35)

1

,

sign(λ1)

where N is P-independent of F∞ and N D= N (0, 1), since the 2-dimensional random vector
(cid:80)∞

1 N j has a 2-dimensional normal distribution with covariance matrix
(cid:35)

(cid:33)

(cid:34)

(cid:35)

(cid:34)

j=0 λ−j

(cid:32) ∞
(cid:88)

j=0

λ−2j
1

λ2
1 − 1
λ2
1

1

sign(λ1)

sign(λ1)
1

=

1

sign(λ1)

sign(λ1)
1

.

Consequently, we conclude (6.3), hence, as it was explained, the convergence (3.1) follows from
(cid:50)
part (a) of Theorem 3.18 in H¨ausler and Luschgy [6].

Proof of Corollary 3.3. In the proof of Theorem 3.1, we showed that Theorem B.1 can be
applied with the choices d := 2, (U n)n∈Z+ := (M n)n∈Z+, (Bn)n∈N := (An)n∈N, where An,
n ∈ N,
1 I 2)n∈N, (Fn)n∈Z+ := (σ(X−1, X0, X1, . . . , Xn))n∈Z+,
G := Ω, and η and P are given in Step 3 in the proof of Theorem 3.1. So, by (B.2), we
(cid:50)
have the statement.

is given in (6.2), (Qn)n∈N := (λ−n

Proof of Corollary 3.4. First, we prove

(6.14)

By (6.6), we have

λ−2n
1

n
(cid:88)

k=1

Xk−1Xk−2

P-a.s.−→

1
1 − 1)λ1

(λ2

Y 2

as n → ∞.

λ−n
1 Xn

P-a.s.−→ Y

as n → ∞,

and

λ−(n−1)
1

Xn−1

P-a.s.−→ Y

as n → ∞,

so

1 (Xn + Xn−1) = λ−n
λ−n
Since (cid:80)∞
k=1 λ2(k−1)
H¨ausler and Luschgy [6, Lemma 6.28]), we get

1 Xn + λ−1

1 λ−(n−1)

1

1

= ∞ (due to |λ1| > 1, λ1 ∈ R), applying the Toeplitz lemma (see, e.g.,

Xn−1

P-a.s.−→ (cid:0)1 + λ−1

1

(cid:1) Y

as n → ∞.

(cid:80)n

k=1(Xk−1 + Xk−2)2
k=1 λ2(k−1)

(cid:80)n

1

P-a.s.−→ (1 + λ−1

1 )2Y 2

as n → ∞.

32

Since (cid:80)n

k=1 λ2(k−1)

1

= λ2n
1 −1
1−1 , we have
λ2

λ−2n
1

n
(cid:88)

k=1

(Xk−1 + Xk−2)2 P-a.s.−→

(λ−1
1 + 1)2
λ2
1 − 1

Y 2

as n → ∞,

and, by (6.7) and (6.8),

λ−2n
1

n
(cid:88)

k=1

Xk−1Xk−2 =

1
2

P-a.s.−→

as n → ∞, yielding (6.14).

(cid:34) n

(cid:88)

(Xk−1 + Xk−2)2 −

λ−2n
1

n
(cid:88)

k=1

X 2

k−1 −

(cid:35)

X 2

k−2

n
(cid:88)

k=1

k=1
(cid:20) (λ−1
1 + 1)2
λ2
1 − 1

1
2

−

1
λ2
1 − 1

−

1
1 − 1)λ2
1

(λ2

(cid:21)

Y 2 =

1
1 − 1)λ1

(λ2

Y 2

Finally, (2.6), (6.7), (6.8) and (6.14) yield (3.3).

(cid:50)

Appendices

A Stable convergence

We recall the notions of stable and mixing convergence.

A.1 Deﬁnition. Let
(X n)n∈N and X be Rd-valued random variables, where d ∈ N.

(Ω, F, P)

be a probability space and G ⊂ F be a sub-σ-ﬁeld. Let

(i) We say that X n converges G-stably to X as n → ∞,
if the conditional distribution
PX n | G of X n given G converges G-stably to the conditional distribution PX | G of X
given G as n → ∞, which equivalently means that

lim
n→∞

EP(ξ EP(h(X n) | G)) = EP(ξ EP(h(X) | G))

for all random variables ξ : Ω → R with EP(|ξ|) < ∞ and for all bounded and continuous
functions h : Rd → R.

(ii) We say that X n converges G-mixing to X as n → ∞,
if X n converges G-stably
to X as n → ∞, and PX | G = PX P-almost surely, where PX denotes the distribution of
X on (Rd, B(Rd)) under P. Equivalently, X n converges G-mixing to X as n → ∞,
if X n converges G-stably to X as n → ∞, and σ(X) and G are independent, which
equivalently means that

lim
n→∞

EP(ξ EP(h(X n) | G)) = EP(ξ) EP(h(X))

for all random variables ξ : Ω → R with EP(|ξ|) < ∞ and for all bounded and continuous
functions h : Rd → R.

33

In Deﬁnition A.1, PX n | G, n ∈ N, and PX | G are the P-almost surely unique G-measurable

Markov kernels from (Ω, F) to (Rd, B(Rd)) such that for each n ∈ N,

(cid:90)

G

(cid:90)

and

PX n | G(ω, B) P(dω) = P(X −1

n (B) ∩ G)

for every G ∈ G, B ∈ B(Rd).

PX | G(ω, B) P(dω) = P(X −1(B) ∩ G)

for every G ∈ G, B ∈ B(Rd),

G

respectively. For more details, see H¨ausler and Luschgy [6, Chapter 3 and Appendix A].

B A multidimensional stable limit theorem

log+(x) := log(x)1{x(cid:62)1} + 0 · 1{0(cid:54)x<1}

for x ∈ R+, and for an event A with
Recall that
P(A) > 0, PA denotes the conditional probability measure given A. For an Rd-valued
stochastic process (U n)n∈Z+, the increments ∆U n, n ∈ Z+, are deﬁned by ∆U 0 := 0 and
∆U n := U n − U n−1 for n ∈ N.

We recall a multidimensional analogue of Theorem 8.2 in H¨ausler and Luschgy [6] which

was proved in Barczy and Pap [2, Theorem 1.4].

B.1 Theorem. (Barczy and Pap [2, Theorem 1.4]) Let
be
Rd-valued and Rd×d-valued stochastic processes, respectively, adapted to a ﬁltration (Fn)n∈Z+,
where Bn is invertible for suﬃciently large n ∈ N. Let
(Qn)n∈N be a sequence in Rd×d
is invertible for suﬃciently large n ∈ N. Let
such that Qn → 0 as n → ∞ and Qn
G ∈ F∞ := σ((cid:83)∞
n=0 Fn) with P(G) > 0. Assume that the following conditions are satisﬁed:

and (Bn)n∈Z+

(U n)n∈Z+

(i) there exists an Rd×d-valued, F∞-measurable random matrix η : Ω → Rd×d

such that

P(G ∩ {∃ η−1}) > 0 and

QnB−1
n

PG−→ η

as n → ∞,

(ii) (QnU n)n∈N is stochastically bounded in PG∩{∃ η−1}-probability, i.e.,

lim
K→∞

sup
n∈N

PG∩{∃ η−1}((cid:107)QnU n(cid:107) > K) = 0.

(iii) there exists an invertible matrix P ∈ Rd×d with (cid:37)(P ) < 1 such that
PG−→ P r

as n → ∞ for every r ∈ N,

BnB−1
n−r

(iv) there exists a probability measure µ on (Rd, B(Rd)) with (cid:82)

Rd log+((cid:107)x(cid:107)) µ(dx) < ∞

such that

EP

(cid:0)ei(cid:104)θ,Bn∆U n(cid:105) | Fn−1

(cid:1) P

G∩{∃η−1}−→

(cid:90)

Rd

ei(cid:104)θ,x(cid:105) µ(dx)

as n → ∞

for every θ ∈ Rd.

34

Then

(B.1)

and

(B.2)

BnU n →

∞
(cid:88)

j=0

P jZ j

F∞-mixing under PG∩{∃ η−1} as n → ∞,

QnU n → η

∞
(cid:88)

j=0

P jZ j

F∞-stably under PG∩{∃ η−1} as n → ∞,

where (Z j)j∈Z+ denotes a sequence of P-independent and identically distributed Rd-valued
random vectors P-independent of F∞ with P(Z 0 ∈ B) = µ(B) for all B ∈ B(Rd).

The series (cid:80)∞

j=0 P jZ j

in (B.1) and in (B.2) is absolutely convergent P-almost surely,
since, by condition (iv) of Theorem B.1, EP(log+((cid:107)Z 0(cid:107))) < ∞ and one can apply Lemma
1.3 in Barczy and Pap [2]. Further, the random variable η and the sequence (Z j)j∈Z+ are
P-independent in Theorem B.1, since η is F∞-measurable and the sequence (Z j)j∈Z+ is
P-independent of F∞.

C Lenglart’s inequality

The following form of the Lenglart’s inequality can be found, e.g., in H¨ausler and Luschgy [6,
Theorem A.8].

C.1 Theorem. Let
(Fn)n∈Z+ and with compensator An := (cid:80)n
for each a, b ∈ R++ and n ∈ N,

(ξn)n∈Z+

k=1

be a nonnegative submartingale with respect to a ﬁltration
EP(ξk − ξk−1 | Fk−1), n ∈ N, A0 := 0. Then

(cid:16)

P

max
k∈{0,1,...,n}

ξk (cid:62) a

(cid:17)

(cid:54) b
a

+ P(ξ0 + An > b).

Applying Theorem C.1 for the square of a square integrable martingale, we obtain the

following corollary.

C.2 Corollary. Let
(Fn)n∈Z+
n ∈ N, (cid:104)η(cid:105)0 := 0. Then for each a, b ∈ R++ and n ∈ N,

and with quadratic characteristic process

be a square integrable martingale with respect to a ﬁltration
EP((ηk − ηk−1)2 | Fk−1),

(cid:104)η(cid:105)n := (cid:80)n

(ηn)n∈Z+

k=1

(cid:16)

P

max
k∈{0,1,...,n}

η2
k

(cid:62) a

(cid:17)

(cid:54) b
a

+ P(η2

0 + (cid:104)η(cid:105)n > b).

Note that under the conditions of Corollary C.2, the quadratic characteristic process ((cid:104)η(cid:105)n)n∈Z+
of (ηn)n∈Z+ coincides with the compensator of (η2

n)n∈Z+.

35

Acknowledgements

We are grateful to Michael Monsour for sending us his paper [9] and the paper of Venkataraman
[20] about the limiting distributions for the LSE of AR parameters of AR(2) processes.

References

[1] Aknouche, A. (2015). Explosive strong periodic autoregression with multiplicity one.

Journal of Statistical Planning and Inference 161 50–72.

[2] Barczy, M. and Pap, G. (2020). A multidimensional stable limit theorem. ArXiv

2012.04541 Available at https://arxiv.org/abs/2012.04541

[3] Boutahar, M. (2002). General autoregressive models with long-memory noise. Statistical

Inference for Stochastic Processes 5 321–333.

[4] Chan, N. H. and Wei, C. Z. (1988). Limiting distributions of least-squares estimates of

unstable autoregressive processes. The Annals of Statistics 16(1) 367–401.

[5] Datta, S. (1995). Limit theory and bootstrap for explosive and partially explosive au-

toregression. Stochastic Processes and their Applications 57 285–304.

[6] H¨ausler, E. and Luschgy, H. (2015). Stable Convergence and Stable Limit Theorems.

Springer, Cham.

[7] Jeganathan, P. (1988). On the strong approximation of the distributions of estimators
in linear stochastic models, I and II: Stationary and explosive AR models. The Annals of
Statistics 16(3) 1283–1314.

[8] Mann, H. B. and Wald, A. (1943). On the statistical treatment of linear stochastic

diﬀerence equations. Econometrica 11(3/4) 173–220.

[9] Mikulski, P. W. and Monsour, M. J. (1998). Limiting distributions in the second
order autoregressive process without a unit root. Technical Report TR-5/98 Dept. of
Math., American University of Beirut.

[10] Monsour, M. J. and Mikulski, P. W. (1998). On limiting distributions in explosive

autoregressive processes. Statistics and Probability Letters 37 141–147.

[11] Monsour, M. J. (2016). Decomposition of an autoregressive process into ﬁrst order

processes. Journal of Multivariate Analysis 147 295–314.

[12] Narasimham, G. V. L. (1969). Some properties of estimators occuring in the theory of
linear stochastic process. In: Economic models, estimation and risk programming: Essays
in honor of Gerhard Tintner, 375–389. Edited by K. A. Fox, J. K. Sengupta and G. V.

36

L. Narasimham. Lecture Notes in Operations Research and Mathematical Economics 15,
Springer-Verlag, Berlin-New York.

[13] R´enyi, A. (1950). Contributions to the theory of independent random variables. Acta

Mathematica Academiae Scientiarum Hungaricae 1 99–108.

[14] R´enyi, A. (1958). On mixing sequences of sets. Acta Mathematica Academiae Scientiarum

Hungaricae 9(1–2) 215–228.

[15] R´enyi, A. (1963). On stable sequences of events. Sankhy¯a. Series A 25 293–302.

[16] R´enyi, A. and R´ev´esz, P. (1958). On mixing sequences of random variables. Acta Math-

ematica Academiae Scientiarum Hungaricae 9(3–4) 389–393.

[17] Touati, A. (1996). Vitesse de convergence en loi de l’estimateur des moindres carr´es d’un
mod`ele autor´egressif (Cas mixte). Annales de l’Institut Henri Poincare (B) Probability and
Statistics 32(2) 211–230.

[18] Venkataraman, K. N. (1967). A note on the least squares estimators of the parameters
of a second order linear stochastic diﬀerence equation. Calcutta Statistical Association.
Bulletin 16 15–28.

[19] Venkataraman, K. N. (1968). Some limit theorems on a linear explosive stochastic
diﬀerence equation with a constant term, and their statistical applications. Sankhy¯a. Series
A 30(1) 51–74.

[20] Venkataraman, K. N. (1973). Some convergence theorems on a second order linear ex-
plosive stochastic diﬀerence equation with a constant term. Journal of the Indian Statistical
Association 11 47–69.

37

