Quasi-Periodic Parallel WaveGAN Vocoder: A Non-autoregressive Pitch-
dependent Dilated Convolution Model for Parametric Speech Generation

Yi-Chiao Wu1, Tomoki Hayashi1, Takuma Okamoto2, Hisashi Kawai2, and Tomoki Toda12

1Nagoya University, Japan
2National Institute of Infomation and Communications Technology, Japan
yichiao.wu@g.sp.m.is.nagoya-u.ac.jp, tomoki@icts.nagoya-u.ac.jp

0
2
0
2

g
u
A
7

]
S
A
.
s
s
e
e
[

2
v
4
5
6
8
0
.
5
0
0
2
:
v
i
X
r
a

Abstract
In this paper, we propose a parallel WaveGAN (PWG)-like
neural vocoder with a quasi-periodic (QP) architecture to im-
prove the pitch controllability of PWG. PWG is a compact non-
autoregressive (non-AR) speech generation model, whose gen-
erative speed is much faster than real time. While utilizing
PWG as a vocoder to generate speech on the basis of acoustic
features such as spectral and prosodic features, PWG generates
high-ﬁdelity speech. However, when the input acoustic features
include unseen pitches, the pitch accuracy of PWG-generated
speech degrades because of the ﬁxed and generic network of
PWG without prior knowledge of speech periodicity. The pro-
posed QPPWG adopts a pitch-dependent dilated convolution
network (PDCNN) module, which introduces the pitch informa-
tion into PWG via the dynamically changed network architec-
ture, to improve the pitch controllability and speech modeling
capability of vanilla PWG. Both objective and subjective eval-
uation results show the higher pitch accuracy and comparable
speech quality of QPPWG-generated speech when the QPPWG
model size is only 70 % of that of vanilla PWG.
Index Terms: neural vocoder, parallel WaveGAN, quasi- peri-
odic WaveNet, pitch-dependent dilated convolution

1. Introduction

Because of the high temporal resolution of speech signals,
speech waveform modeling is challenging. The general tech-
nique to tackle speech synthesis (SS) is called a vocoder [1, 2],
which encodes speech into low-dimensional acoustic features
and decodes speech on the basis of these acoustic features. The
conventional vocoders such as STRAIGHT [3] and WORLD
(WD) [4] are usually designed on the basis of a source-ﬁlter
model [5], which decomposes speech into spectral and prosodic
acoustic features. However, the ad hoc signal-processing mech-
anisms imposed on the conventional vocoders cause the loss of
phase information and temporal details, which results in marked
speech quality degradation.

To achieve high ﬁdelity SS, many neural network (NN)-
based autoregressive (AR) SS models such as SampleRNN [6]
and WaveNet (WN) [7] have been proposed to directly model
the probability distributions of speech waveforms without many
ad hoc assumptions of SS. The NN-based vocoders [8–11] are
also proposed on the basis of these AR SS models to replace the
synthesizers of the conventional vocoders for recovering the lost
phase information and temporal details and generating high-
quality speech. Furthermore, because of the extremely slow
generation of WN and SampleRNN, many AR models with
compact networks and speciﬁc knowledge [12–14] and non-AR
models such as ﬂow-based [15–19] and generative adversarial
network (GAN) [20]-based models [21–23] have been proposed
for real-time speech generation.

However, because of the data-driven nature and the lack of
prior speech knowledge, it is hard for these NN-based SS mod-
els to deal with unseen data. For instance, if the pitches of test-
ing acoustic features are scaled or outside the observed pitch
range of training data, the pitch accuracy and speech quality
of the WN-generated speech samples markedly degrade. Since
the pitch controllability is an essential feature for a vocoder,
NN-based SS models with carefully designed periodic and ape-
riodic inputs [24–26] greatly improve the pitch modeling capa-
bility. Furthermore, in our previous work, we proposed a quasi-
periodic WN vocoder (QPNet) [27, 28], which adopts pitch-
dependent dilated convolution networks (PDCNNs) to dynam-
ically change the network architecture according to the input
pitches, to improve the pitch controllability and speech model-
ing ability of WN.

To tackle the slow generation of AR WN/QPNet, we apply a
quasi-periodic (QP) structure to a compact non-AR model par-
allel WaveGAN (PWG) [21]. Since PWG transforms a noise
sequence sampled from a standard Gaussian distribution into
speech samples by taking conventional acoustic features as the
auxiliary feature, PWG is more ﬂexible than the models re-
quired speciﬁc periodic and aperiodic inputs. Moreover, the
non-AR fashion also makes the parallelized generation avali-
able for real-time generation. In this paper, we propose a fast
and ﬂexible QPPWG vocoder to improve the pitch controlla-
bility and speech modeling efﬁciency of PWG. Both objective
and subjective evaluations are conducted, and the experimental
results show the higher pitch accuracy, comparable speech qual-
ity, and smaller model size of the QPPWG vocoder than that of
the PWG vocoder.

2. Parallel WaveGAN

As shown in Fig. 1, PWG is composed of a discriminator (D), a
generator (G), and a multi-resolution short-time Fourier trans-
form (STFT) loss module. The discriminator is trained to detect
synthesized samples as fake speech and natural samples as real
speech. The training criterion of the discriminator is to mini-
mize the loss LD, which is formulated as

LD(G, D)
= Ex∈pdata

(cid:2)(1 − D(x))2(cid:3) + Ez∈N (0,I)

(cid:2)D(G(z))2(cid:3) , (1)

where x denotes the natural samples, pdata denotes the data
distribution of the natural samples, N (0, I) denotes a Gaussian
distribution with zero mean and standard deviation, and z de-
notes the input noise of the generator drawn from the Gaus-
sian distribution. Note that all auxiliary features of G are omit-
ted in this chapter for simplicity. The discriminator is a fully-
convolutional network, which consists of several dilated con-
volution network (DCNN) [29] layers with LeakyReLU [30]

 
 
 
 
 
 
Figure 1: Architecture of Parallel WaveGAN.

activation functions. The dilation size of each DCNN layer is
extending in an exponential growth manner with a base of two
and an exponent of its layer index. The generator is trained to
generate speech samples, which makes the discriminator difﬁ-
cult to distinguish between the synthesized and natural samples.
The training criterion of the generator is to minimize the gener-
ator loss (LG) formulated as

LG(G, D) = Lsp(G) + λadvLadv(G, D),

(2)

which is the weighted sum of an adversarial loss (Ladv) from
the GAN structure and a spectral loss (Lsp) from the multi-
resolution STFT loss module with a weight λadv. The Ladv
is formulated as

Ladv(G, D) = Ez∈N (0,I)

(cid:2)(1 − D(G(z)))2(cid:3) .

(3)

Unlike ﬂow-based models [15–19] adopting an invertible net-
work to transform the distribution of the input noise to the real
data distribution, PWG adopts a GAN structure to make the
generator learn the transformation via the feedback from the
discriminator. The generator adopts a WN-like network but
without the AR mechanism and causality, so the generation of
PWG markedly faster than that of WN. To ensure the stability
and efﬁciency of the GAN training, PWG adopts an extra Lsp
loss as a regularizer of the generator. Speciﬁcally, the Lsp is a
summation of a spectral convergence loss (Lsc) and a log STFT
magnitude loss (Lm). The Lsp is formulated as

Lsc(x, ˆx) =

(cid:107)|STFT(x)| − |STFT( ˆx)|(cid:107)F
(cid:107)STFT(x)|(cid:107)F

,

(4)

and the Lm is formulated as

Lm(x, ˆx) =

1
N

(cid:107) log |STFT(x)| − log |STFT( ˆx)|(cid:107)L1, (5)

where ˆx denotes the generated samples from the generator,
(cid:107)·(cid:107)F is Frobenius norm, (cid:107)·(cid:107)L1 is L1 norm, |STFT (·)| denotes
STFT magnitudes, and N is the number of the magnitude ele-
ments. Moreover, to avoid the generator overﬁtting to a speciﬁc
STFT resolution causing a suboptimal problem, the ﬁnal Lsp
is a summation of several Lsp values calculated on the basis of
STFT features with different analysis parameters such as FFT
size and frame length and shift.

Although PWG achieves high ﬁdelity speech generation,
the ﬁxed and generic network architecture of PWG is not appro-
priate. Speech is a quasi-periodic signal consisting of periodic
components with long-term correlations and aperiodic compo-
nents with short-term correlations, so the ﬁxed architecture of

Figure 2: Pitch-dependent dilated convolution.

PWG modeling all components is inefﬁcient. Speciﬁcally, a re-
ceptive ﬁeld is a region in the input space that the outputs are
affected by, and the receptive ﬁeld length is highly related to
the modeling capacity. However, modeling speech using ﬁxed-
length receptive ﬁelds may lead to the receptive ﬁelds includ-
ing many redundant samples from the oversampled periodic
components. To tackle this problem, we proposed a QPPWG
vocoder to respectively model periodic and aperiodic compo-
nents using cascaded ﬁxed and pitch-adaptive modules.

3. Quasi-Periodic parallel WaveGAN

Since the GAN architecture and the multi-resolution STFT loss
of PWG have shown the effectiveness for training a speech gen-
erator, the proposed QPPWG inherits the discriminator and the
Lsp regularizer from PWG and focuses on improving the gener-
ator using a QP structure. The main modules of the QP structure
are PDCNN components and a cascaded architecture, which are
inspired by the pitch ﬁltering and the cascaded recursive struc-
ture of the code-excited linear prediction (CELP) codec [31].

3.1. Pitch-dependent dilated convolution

As shown in Fig. 2, there are gaps between the inputs of a
DCNN kernel, and the length of each gap is a predeﬁned hy-
perparameter called a dilation size (rate). PDCNN is an ex-
tension of DCNN, and its dilation size is pitch-dependent and
dynamically changed according to the input pitch. Speciﬁcally,
a dilated convolution with a kernel size three is formulated as

t = W (c) ∗ y(i)
y(o)

t + W (p) ∗ y(i)

t−d + W (f) ∗ y(i)

t+d,

(6)

where y(i) and y(o) are the input and output of the DCNN layer.
W (c), W (p) and W (f) are the trainable 1×1 convolution ﬁlters
of current, previous, and following samples, respectively. ∗ is
the convolution operator, and the dilation size d of DCNN is a
time-invariant constant. By contrast, PDCNN adopts a pitch-
dependent dilated factor Et to dynamically change the dilation
size d(cid:48) in each time step t as

d(cid:48) = Et × d.

The dilated factor Et is derived from

Et = Fs/(F0,t × a),

(7)

(8)

where Fs is the sampling rate, F0 is the fundamental frequency,
and a is a hyperparameter called a dense factor, which indicates
the number of samples in one cycle taken as the PDCNN inputs
for each time step. The higher the dense factor, the lower the
sparsity of PDCNN. For simplicity, the d(cid:48) of each time step is
rounded. For the unvoiced segments, we ﬁnd that the models

Generator (G)Gaussian noiseAcoustic featuresDiscriminator (D)Generated speechNatural speechMulti-resolutionSTFT lossLDLadvLspLGλadv TrainingSynthesis<=66)<=66>*.*08’3*/?’*1-@??*.8’3*/+*.*08’3*/?’*1-A$80$8&:‘@\;=B’--*:&:‘A\;=C:0$8)’8.9/-*0*:-*:8/-’1%8*-/?%.8,+&/;=;4‘@222222A$80$8&‘@B’--*:&‘AC:0$8Table 1: Numbers of generator parameters.

PWG 30

PWG 20

QPPWGaf

QPPWGf a

Macro 0
Macro 1

Size (M)

BF30C3
-

BF20C2
-

1.16

0.78

BA10C2
BF10C1

0.79

BF10C1
BA10C2

0.79

Vocoder
Blocks

1 × F0
1/2 × F0
2 × F0
Average

1 × F0
1/2 × F0
2 × F0
Average

Table 2: Objective evaluation results.

WD
-

0.10
0.14
0.10
0.11

2.58
3.89
3.79
3.42

PWG

30

20

QPPWG

af

f a

RMSE of log F0
0.15
0.32
0.15
0.21

0.12
0.27
0.15
0.18

MCD (dB)
3.74
4.39
5.06
4.40

3.69
4.47
5.24
4.46

0.11
0.19
0.11
0.14

3.80
4.52
4.92
4.41

0.11
0.20
0.11
0.14

4.54
5.18
5.61
5.11

4.2. Experimental settings

All NN-based vocoders were trained in a multi-speaker manner.
The training corpus consisted of 2200 utterances of the “slt” and
“bdl” speakers of CMU-ARCTIC [33] and 852 utterances of all
speakers of Voice Conversion Challenge 2018 (VCC2018) [34].
The total data length was around 2.5 hours. The testing corpus
was the SPOKE set, which consisted of two male and two fe-
male speakers, of VCC2018 corpus, and the number of testing
utterances of each speaker was 35. All speech data were set to
a sampling rate of 22,050 Hz and a 16-bit resolution.

The auxiliary features of these speech generation models
consisted of one-dimensional continuous F0, one-dimensional
unvoiced/voiced binary code (U/V ), 35-dimensional mel-
cepstrum (mcep), and two-dimensional coded aperiodicity
(codeap). The WD vocoder was ﬁrst adopted to extract one-
dimensional F0 and 513-dimensional spectral feature (sp) and
aperiodicity (ap) with a frameshift of 5 ms. F0 was interpolated
to the continuous F0 and converted to the U/V , ap was coded
into the codeap, and sp was parameterized into the mcep. To
simulate unseen data, the continuous F0 was scaled by the ratios
of 1/2 and 2 while keeping other features the same. Moreover,
the dilated factor Et of QPPWG was calculated with the con-
tinuous F0 because of the higher speech quality, and the dense
factor of QPPWG was empirically set to four.

The models were trained with a RAdam optimizer [35]
((cid:15) = 1e−6) with 400 k iterations. For the stability, the gen-
erators of the models were trained with only multi-resolution
STFT losses, which were calculated on the basis of three dif-
ferent FFT sizes (1024 / 2048 / 512), frameshifts (120 / 240 /
50), and frame lengths (600 / 1200 / 240), for the ﬁrst 100 k it-
erations and then jointly trained with the discriminators for the
following 300 k iterations. The balance weight λadv of Ladv
was set to 4.0. The learning rates, which decayed 50 % every
200 k iterations, of the generators were 1e−4 and the discrim-
inators were 5e−5. The minibatch size was six and the batch
length was 25,520 samples.

4.3. Objective evaluations

Root mean square error (RMSE) of log F0 and mel-cepstral
distortion (MCD) were adopted to the objective evaluations.
Both measurements were calculated using the auxiliary fea-
tures and the features extracted from the generated speech.
As shown in Table 2, the proposed QPPWG vocoders achieve

Figure 3: Architecture of QPPWG generator.
with interpolated F0 outperform the models setting d(cid:48) to one,
so our implementation [32] adopts interpolated F0. In conclu-
sion, PDCNN introduces the pitch information to the network,
makes each sample have a pitch-dependent receptive ﬁeld size,
and efﬁciently enlarges receptive ﬁelds.

3.2. Generator of QPPWG

The architecture of the QPPWG generator is shown in Fig. 3,
and it is similar to the WN-like PWG generator. The main
difference is the hierarchical architecture of the stacked resid-
ual blocks. Speciﬁcally, QPPWG includes two cascaded mac-
roblocks with different types of residual blocks while PWG
only contains one type of residual blocks. The ﬁrst macroblock
of QPPWG consists of stacked adaptive blocks with PDCNN
layers to model the periodic components with long-term cor-
relations, and the second macroblock of QPPWG consists of
stacked ﬁxed blocks with DCNN layers to model the aperiodic
components with short-term correlations.

4. Experiments

4.1. Model descriptions

In this paper, the QPPWG models with two different orders of
macroblocks and the PWG models with two different numbers
of residual blocks were evaluated. Speciﬁcally, the QPPWG
model, whose ﬁrst macroblock included 10 adaptive blocks
with 2 cycles of the dilation size expansions (BA10C2) and the
following macroblock included 10 ﬁxed blocks with one cy-
cle (BF10C1), was denoted as QPPWGaf and the one with a
reverse macroblock order was QPPWGf a. The generator of
vanilla PWG (PWG 30) contained 30 ﬁxed blocks with 3 cy-
cles (BF30C3) and the compact PWG (PWG 20) contained 20
ﬁxed blocks with 2 cycles (BF20C2). The channel and ker-
nel sizes of the non-causal convolution of these generators were
64 and three. As shown in Table 1, the generator size of the
proposed QPPWG is around 70 % of that of the vanilla PWG
because of the less stacked residual blocks. However, because
the time-variant mechanism degrades the parallelism of CNNs,
the real-time factor (RTF) of QPPWG generation with a Titan
V GPU is around 0.020, which is higher than 0.016 of PWG 30
and 0.011 of PWG 20. Moreover, these four models adopted
the the same discriminator architecture including 10 non-causal
DCNN layers with 64 convolution channels, three kernels, and
LeakyReLU (α is 0.2) activation functions, and the number of
the discriminator parameters was around 0.1 M.

BC43+;*&&%;)4*&D9(3)4@%>:*;CD9(3)4@%>:*;CE42%9+>:*;CE42%9+>:*;C999D9(3)4@%+F+E42%9+GgL)#!2%&’#A%&’#8’D&).’(!#+%2)-2/$4I’WXLgLI’WXLgL5.’B!/+().’(!#+%2)-2/$4LgLA’"’.%&’#)(1’’$*DA24:4(’G+,%()A’%6BC43+;*&&%;)4*&A%+((!%")"/!(’D9(3)4@%+F+E42%9+’%649A(:+>:*;C999X1(%,12’H$/+(&!$)0’%&+.’(LgLLgLLgLDA24:4(’G+,%()A’%68(;’*>:*;C+H8(;’*>:*;C+7QPPWGaf

QPPWGf a

Intermediate cumulative outputs of 1–10 blocks

Figure 4: MOS results of speech quality with 95 % CI.

Cumulative outputs of 1–20 blocks

Figure 5: ABX results of pitch accuracy with 95 % CI.

markedly higher F0 accuracy than the PWG vocoders when
conditioned on the scaled F0, and it conﬁrms the effective-
ness of the proposed QP structure. The results also show that
QPPWGaf achieves a comparable spectral prediction accu-
racy as the vanilla and compact PWGs. Moreover, QPPWGaf
achieving lower MCDs than QPPWGf a implies that modeling
long-term correlations ﬁrst get a better overall spectral structure.
In conclusion, the proposed QP structure improves the accuracy
of pitch modeling of the PWG vocoder and efﬁciently extends
the receptive ﬁeld length. The QPPWG with an adaptive to ﬁxed
order outperforms the QPPWG with a reverse order.

4.4. Subjective evaluations

The subjective evaluation set consisted of 960 selected utter-
ances of four testing speakers, four vocoders (WD, PWG 30,
PWG 20, and QPPWGaf ), and three F0 scaled ratios (1, 1/2,
and 2). For each speaker, vocoder, and F0 ratio, we randomly
selected 20 utterances from the 35 testing utterances for both
mean opinion score (MOS) and ABX tests. Speciﬁcally, the
speech quality of each utterance was evaluated by listeners as-
signing MOSs (1–5). The higher the MOS, the better the speech
quality. For the ABX test, every time listeners compared two
testing utterances with one reference to pick up the utterance
whose pitch contour was more consistent with that of the ref-
erence. The WD-generated utterances were taken as the ref-
erences, and the pitch accuracies of the QPPWGaf -generated
utterances with 1/2 and 2 F0 inputs were compared with that
of the PWG 30-generated utterances. Eight listeners involved
in both tests and each utterance was evaluated by at least two
listeners. Most listeners were audio-related researchers.

As shown in Fig. 4, the QPPWGaf vocoder markedly out-
performs the same sized PWG 20 vocoder for all F0 inputs.
Even compared with PWG 30, QPPWGaf still achieves higher
speech qualities for the scaled F0 inputs and a comparable
speech quality for the unchanged F0 input. In addition, the re-
sults of Fig. 5 show the perceptible differences of the pitch ac-

Figure 6: Spectra of cumulative outputs.

curacies between the QPPWGaf and PWG 30 vocoders with
scaled F0 inputs. In conclusion, introducing the pitch informa-
tion to the PWG model by the QP structure markedly improves
the pitch and speech modeling capabilities of the PWG vocoder,
which results in compact model size and better pitch controlla-
bility of the QPPWG voder.

4.5. Discussion

Since the model capacity is highly related to the receptive ﬁeld
length [27, 28], and the length of PWG 30 is 6139 samples
(20 + · · · + 29 = 1023 with three cycles and two sides plus
one), QPPWG attains a longer effective receptive ﬁeld length
around 3,000–16,000 samples. Speciﬁcally, the size is 2047 of
the BF10C1 and 124×ET (20 + · · · + 24 = 31 with two cycles
and two sides) of the BA10C2, and the ET is around 11–110 of
the 500–50 Hz pitches when the dense factor is four.

Moreover, as the intermediate cumulative outputs shown
in Fig. 6, the ﬁrst ten adaptive blocks of QPPWGaf focus on
modeling the pitch and harmonic components, which have long-
term correlations, while the ﬁrst ten ﬁxed blocks of QPPWGf a
focus on modeling the non-harmonic components, which have
short-term correlations. The results conﬁrm our assumptions of
the QP structure, and the behavior, which is similar to the har-
monic plus noise model [36, 37], of QPPWG is more tractable
and interpretable than that of vanilla PWG. More details and
demo samples can be found on our website [38].

5. Conclusions

In this paper, we integrate a fast and compact PWG vocoder
with a QP structure to improve its pitch controllability. The
proposed QPPWG vocoder reduces the model size and achieves
higher speech quality and pitch accuracy than the PWG vocoder
when the input F0 sequence is scaled. In conclusion, the QP-
PWG vocoder is more in line with the deﬁnition of a vocoder,
which attains acoustic controllability.

6. Acknowledgments

This work was supported in part by JST CREST Grant Num-
ber JPMJCR19A3. The initial investigation in this study was
performed while Y.-C. Wu was interning at NICT.

3.52.22.94.32.92.53.82.72.24.13.22.7123451×𝐹₀½×𝐹₀2×𝐹₀MEANOPINIONSCOREWDPWG_30PWG_20QPPWG𝑎𝑓27.120.372.979.7020406080100½×𝐹₀2×𝐹₀PREFERENCESCORE(%)PWG_30QPPWG𝑎𝑓                7 L P H   V        ) U H T X H Q F \   N + ]                 7 L P H   V        ) U H T X H Q F \   N + ]                 7 L P H   V        ) U H T X H Q F \   N + ]                 7 L P H   V        ) U H T X H Q F \   N + ] 7. References
[1] H. Dudley, “The vocoder,” Bell Labs Record, vol. 18, no. 4, pp.

122–126, 1939.

[2] M. R. Schroeder, “Vocoders: Analysis and synthesis of speech,”

Proc. IEEE, vol. 54, no. 5, pp. 720–734, 1966.

[3] H. Kawahara, I. Masuda-Katsuse, and A. De Cheveigne, “Re-
structuring speech representations using a pitch-adaptive time–
frequency smoothing and an instantaneous-frequency-based f0
extraction: Possible role of a repetitive structure in sounds,”
Speech communication, vol. 27, no. 3-4, pp. 187–207, 1999.

[4] M. Morise, F. Yokomori, and K. Ozawa, “World: a vocoder-based
high-quality speech synthesis system for real-time applications,”
IEICE TRANSACTIONS on Information and Systems, vol. 99,
no. 7, pp. 1877–1884, 2016.

[5] R. McAulay and T. Quatieri, “Speech analysis/synthesis based
on a sinusoidal representation,” IEEE Transactions on Acoustics,
Speech, and Signal Processing, vol. 34, no. 4, pp. 744–754, 1986.

[6] S. Mehri, K. Kumar, I. Gulrajani, R. Kumar, S. Jain, J. Sotelo,
A. Courville, and Y. Bengio, “SampleRNN: An unconditional
end-to-end neural audio generation model,” in Proc. ICLR, Apr.
2017.

[7] A. van den Oord, S. Dieleman, H. Zen, K. Simonyan, O. Vinyals,
A. Graves, N. Kalchbrenner, A. Senior, and K. Kavukcuoglu,
“WaveNet: A generative model for raw audio,” in Proc. SSW9,
Sept. 2016, p. 125.

[8] A. Tamamori, T. Hayashi, K. Kobayashi, K. Takeda, and T. Toda,
“Speaker-dependent WaveNet vocoder,” in Proc. Interspeech,
Aug. 2017, pp. 1118–1122.

[9] T. Hayashi, A. Tamamori, K. Kobayashi, K. Takeda, and T. Toda,
“An investigation of multi-speaker training for WaveNet vocoder,”
in Proc. ASRU, Dec. 2017, pp. 712–718.

[10] K. Tachibana, T. Toda, Y. Shiga, and H. Kawai, “An investiga-
tion of noise shaping with perceptual weighting for wavenet-based
speech generation,” in 2018 IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP).
IEEE, 2018,
pp. 5664–5668.

[11] Y. Ai, H.-C. Wu, and Z.-H. Ling, “SampleRNN-based neu-
ral vocoder for statistical parametric speech synthesis,” in Proc.
ICASSP, Apr. 2018, pp. 5659–5663.

[12] Z. Jin, A. Finkelstein, G. J. Mysore, and J. Lu, “FFTNet: A real-
time speaker-dependent neural vocoder,” in Proc. ICASSP, Apr.
2018, pp. 2251–2255.

[13] N. Kalchbrenner, E. Elsen, K. Simonyan,

S. Noury,
N. Casagrande, E. Lockhart, F. Stimberg, A. van den Oord,
S. Dieleman, and K. Kavukcuoglu, “Efﬁcient neural audio
synthesis,” in Proc. ICML, July 2018, pp. 2415–2424.

[14] J.-M. Valin and J. Skoglund, “LPCNet: Improving neural speech
synthesis through linear prediction,” in Proc. ICASSP, May 2019,
pp. 5826–7830.

[15] A. van den Oord, Y. Li, I. Babuschkin, K. Simonyan, O. Vinyals,
K. Kavukcuoglu, G. van den Driessche, E. Lockhart, L. C. Cobo,
F. Stimberg, N. Casagrande, D. Grewe, S. Noury, S. Dieleman,
E. Elsen, N. Kalchbrenner, H. Zen, A. Graves, H. King, T. Wal-
ters, D. Belov, and D. Hassabis, “Parallel WaveNet: Fast high-
ﬁdelity speech synthesis,” in Proc. ICML, July 2018, pp. 3915–
3923.

[16] W. Ping, K. Peng, and J. Chen, “ClariNet: Parallel wave genera-
tion in end-to-end text-to-speech,” in Proc. ICLR, May 2019.

[17] R. Prenger, R. Valle, and B. Catanzaro, “WaveGlow: A ﬂow-
based generative network for speech synthesis,” in Proc. ICASSP,
May 2019, pp. 3617–3621.

[18] S. Kim, S.-G. Lee, J. Song, J. Kim, and S. Yoon, “FloWaveNet :
A generative ﬂow for raw audio,” in Proc. ICML, June 2019, pp.
3370–3378.

[19] N.-Q. Wu and Z.-H. Ling, “WaveFFJORD: FFJORD-based
vocoder for statistical parametric speech synthesis,” in Proc.
ICASSP, May 2020, pp. 7214–7218.

[20] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-
Farley, S. Ozair, A. Courville, and Y. Bengio, “Generative adver-
sarial nets,” in Proc. NIPS, Dec. 2014, pp. 2672–2680.

[21] R. Yamamoto, E. Song, and J.-M. Kim, “Parallel WaveGAN: A
fast waveform generation model based on generative adversarial
networks with multi-resolution spectrogram,” in Proc. ICASSP,
May 2020, pp. 6199–6203.

[22] K. Kumar, R. Kumar, T. de Boissiere, L. Gestin, W. Z. Teoh,
J. Sotelo, A. de Br´ebisson, Y. Bengio, and A. C. Courville, “Mel-
GAN: Generative adversarial networks for conditional waveform
synthesis,” in Proc. NeurIPS, Dec. 2019, pp. 14 910–14 921.

[23] M. Bi´nkowski, J. Donahue, S. Dieleman, A. Clark, E. Elsen,
N. Casagrande, L. C. Cobo, and K. Simonyan, “High ﬁdelity
speech synthesis with adversarial networks,” in Proc. ICLR, Apr.
2020.

[24] X. Wang, S. Takaki, and J. Yamagishi, “Neural source-ﬁlter-based
waveform model for statistical parametric speech synthesis,” in
Proc. ICASSP, May 2019, pp. 5916–5920.

[25] X. Wang, S. Takaki, and J. Yamagishi, “Neural source-ﬁlter
waveform models for statistical parametric speech synthesis,”
IEEE/ACM Transactions on Audio, Speech, and Language Pro-
cessing, vol. 28, pp. 402–415, 2020.

[26] K. Oura, K. Nakamura, K. Hashimoto, Y. Nankaku, and
K. Tokuda, “Deep neural network based real-time speech vocoder
with periodic and aperiodic inputs,” in Proc. SSW10, Sept. 2019,
pp. 13–18.

[27] Y.-C. Wu, T. Hayashi, P. L. Tobing, K. Kobayashi, and T. Toda,
“Quasi-periodic WaveNet vocoder: A pitch dependent dilated
convolution model for parametric speech generation,” in Proc. In-
terspeech, Sept. 2019, pp. 196–200.

[28] Y.-C. Wu, T. Hayashi, P. L. Tobing, K. Kobayashi, and T. Toda,
“Quasi-periodic WaveNet: An autoregressive raw waveform gen-
erative model with pitch-dependent dilated convolution neural
network,” IEEE/ACM Transactions on Audio, Speech, and Lan-
guage Processing, (submitted).

[29] F. Yu and K. Vladlen, “Multi-scale context aggregation by dilated

convolutions,” in Proc. ICLR, May 2016.

[30] A. L. Maas, A. Y. Hannun, and A. Y. Ng, “Rectiﬁer nonlinearities
improve neural network acoustic models,” in Proc. ICML, June
2013, pp. 3–11.

[31] M. Schroeder and B. Atal,

“Code-excited linear predic-
tion(CELP): High-quality speech at very low bit rates,” in Proc.
ICASSP, vol. 10, Apr. 1985, pp. 937–940.

[32] Y.-C. Wu, QPPWG repository, Accessed:
Available: https://github.com/bigpon/QPPWG

2020. [Online].

[33] J. Kominek and A. W. Black, “The CMU ARCTIC speech
databases for speech synthesis research,” in Tech. Rep. CMU-LTI-
03-177, 2003.

[34] J. Lorenzo-Trueba, J. Yamagishi, T. Toda, D. Saito, F. Villavicen-
cio, T. Kinnunen, and Z. Ling, “The voice conversion challenge
2018: Promoting development of parallel and nonparallel meth-
ods,” in Proc. Odyssey, June 2018, pp. 195–202.

[35] L. Liu, H. Jiang, P. He, W. Chen, X. Liu, J. Gao, and J. Han, “On
the variance of the adaptive learning rate and beyond,” in Proc.
ICLR, Apr. 2020.

[36] Y. Stylianou, O. Capp´e, and E. Moulines, “Continuous probabilis-
tic transform for voice conversion,” IEEE Trans. Speech Audio
Process., vol. 6, no. 2, pp. 131–142, 1998.

[37] Y. Stylianou, “Applying the harmonic plus noise model in con-
catenative speech synthesis,” IEEE Trans. Speech Audio Process.,
vol. 9, no. 1, pp. 21–29, 2001.

[38] Y.-C. Wu, QPPWG demo, Accessed: 2020. [Online]. Available:

https://bigpon.github.io/QuasiPeriodicParallelWaveGAN demo/

