Efﬁcient Heterogeneous Video Segmentation at the Edge

Jamie Menjay Lin Siargey Pisarchyk

Juhyun Lee David Tian Tingbo Hou

Karthik Raveendran Raman Sarokin George Sung Trent Tolley Matthias Grundmann
Google, Mountain View, CA, USA
{jmlin,siargey,impjdi,dctian,krav,tingbo,sorokin,gsung,trenttolley,grundman}@google.com

2
2
0
2

g
u
A
4
2

]

V
C
.
s
c
[

1
v
6
6
6
1
1
.
8
0
2
2
:
v
i
X
r
a

Abstract

We introduce an efﬁcient video segmentation system
for resource-limited edge devices leveraging heterogeneous
compute. Speciﬁcally, we design network models by search-
ing across multiple dimensions of speciﬁcations for the neu-
ral architectures and operations on top of already light-
weight backbones, targeting commercially available edge
inference engines. We further analyze and optimize the
heterogeneous data ﬂows in our systems across the CPU,
the GPU and the NPU. Our approach has empirically fac-
tored well into our real-time AR system, enabling remark-
ably higher accuracy with quadrupled effective resolutions,
yet at much shorter end-to-end latency, much higher frame
rate, and even lower power consumption on edge platforms.

1. Introduction

Video segmentation is a foundational technology pow-
ering various computer vision tasks in business and enter-
tainment use cases, such as video editing in augmented re-
ality (AR) and background blur and replacement in video
conferencing. Machine learning (ML) inference for high-
quality real-time video segmentation has been a challenging
problem particularly on resource-limited edge devices, e.g.
smartphones, with model accuracy, real-time latency, and
power consumption being the key areas for improvement.

We present practical

techniques for real-time high-
quality video segmentation at the edge (Figure 1). We de-
scribe i) our advances made to common light-weight net-
work architectures, e.g. MobileNetV3 [18] and Efﬁcient-
NetLite [9], and ii) our optimizations to the inference
pipelines at the edge, such as WebGL-based [4] browsers
and the NPU inference pipelines on mobile devices.

2. Related Work

In the course of neural network-based model develop-
ment for semantic segmentation, fully convolutional net-
works [30] were introduced as an earlier progress. Nu-
merous follow-up works continued to improve on accu-

Figure 1. Real-time video segmentation comparison on Google
Pixel 6 smartphone. Left column: Baseline ML inference at
89.5% mIoU [2] (segmentation quality metric) and 2.03 Watts.
Right column: Our ML inference at 95.1% mIoU and 1.84 Watts.

racy and efﬁciency, proposing various types of model ar-
chitectures for segmentation, such as DeepLab [15], Vortex
Pooling [17], PSPNet [36], and HRNet [31, 35]. Another
recent line of work targets the problem of image matting
for foreground-background separation, which typically re-
quires either the availability of a trimap [13, 23, 32] or a
pre-captured background image [13, 23, 29]. Dynamic rout-
ing [22] was introduced on a generalized view over various
architectures for segmentation. Neural architecture search
techniques have been another line of work in searching for
optimized segmentation model architectures [14, 24, 27], as
well as particular types of model improvements such as
transformer-based segmentation models [16, 26].

 
 
 
 
 
 
Figure 2. Left: The bottleneck block paralleled with a squeeze-
and-excitation operation, which increases GPU latency due in part
to the global pooling operations. Right: Our decoder with the MLP
operation, which does not involve global pooling operations.

3. Edge Segmentation Model Design

Neural networks running on edge devices are often tai-
lored to the available computational resources. As a base-
line for the segmentation task, we consider a commonly
used fully convolutional U-net architecture based on a Mo-
bileNetV3 backbone. Running CPU model inference with
an input resolution of 256×256 takes over 50ms on an
Intel Celeron N3060 as used in common EDU-targeted
Chromebooks, suggesting that even a low-quality segmen-
tation model is too expensive to run at real-time on older
hardware.

To allow for larger models while still supporting real-
time video, we targeted GPU and NPU that can deliver the
required FLOPS [1] or TOPS [3] for the task of high res-
olution video segmentation. We analyze the impact of our
design choices including the structure of the encoder and
decoder blocks, convolution types, input resolutions, and
width and depth multipliers. Table 1 summarizes the full
ablation study discussed in this section.

While the squeeze-and-excitation blocks [19] in Mo-
bileNetV3 are efﬁcient on the CPU, they increase the la-
tency on GPUs due to the global average pooling of large
tensors. We re-base our architecture on EfﬁcientNetLite
where each encoder block comes with an inverted residual
bottleneck and produces an output tensor that is 1/32 of the
input. The decoder then begins with a bottleneck block that
predicts a coarse segmentation mask as shown in Sec. 4. In
this instance, the global average pooling only needs to op-
erate on a much smaller tensor and the prediction accuracy
beneﬁts from such channel attention mechanism.

Next, we explore three variants for the decoder design:

1. Bilinear Upsampling: Upsamples bilinearly the lower
resolution tensor to match the resolution of the next
higher encoder block. This applies a non-linearity and
sum with the tensor from the encoder skip connection.

2. Channel Attention: Applies squeeze-and-excitation
blocks after upsampling, followed by a set of 1×1 con-
volutions and a 3×3 depth-wise convolution.

3. Multilayer Perceptron (MLP): Applies upsampling
followed by a sequence of 1×1 convolutions (MLPs).

Ablation Study

Parameter

Convolution Type

Width Multiplier

Decoder Type

Image Resolution

Depthwise Conv
Group Conv*
1.0*
1.5
2.0
Bilinear Upsampling
Channel Attention
Multilayer Perceptron*
256x256
384x384
512x512*
640x640

Model
Inf
Time
(ms)
4.7
4.7
5.7
8.2
11.4
6.5
8.2
6.9
1.6
3.1
6.9
9.5

Model
Size
(MB)

OPs
(109)

Segment
IOU
(%)

1.7
7.2
9.4
21.0
37.0
8.7
8.7
8.7
8.7
8.7
8.7
8.7

1.5
4.8
5.3
12.1
20.6
7.4
7.5
7.6
1.9
4.3
7.6
12.0

97.19
97.41
97.50
97.89
97.96
97.65
97.75
97.81
96.94
97.51
97.79
97.98

Pose
IOU
(%)

89.60
90.20
91.40
92.37
92.63
90.65
91.81
91.80
87.23
90.10
91.80
92.94

Table 1. Ablation study over edge segmentation model design pa-
rameters. “∗” denotes our ﬁnal choice in each ablation category.

We found that 1×1 convolutions make a signiﬁcant dif-
ference to the accuracy of the model when it comes to ﬁner
details, and that the MLP block serves as a reasonable per-
formance compromise compared to the more expensive at-
tention block. We also considered the effect of various con-
volution types on the network, ranging from standard con-
volutions to other types of convolutions. We found while
separable convolutions are a natural ﬁt for modern GPUs,
certain hardware such as NPUs can beneﬁt from group con-
volutions due to increased chip utilization, enhancing the
model expressivity without increasing the latency. Finally,
we performed a hyperparameter search for the Efﬁcient-
NetLite family and settled on a multiplier of 1.0.

Having settled on an architecture, we measured the
trade-offs between input resolution, latency, and accuracy.
In each of these cases, we trained the network using images
down-scaled to the speciﬁed input resolution and measured
the IoU by down-scaling the ground truth mask to the output
resolution. We found that resolution has the largest impact
on the quality of segmentation, but observed rapidly dimin-
ishing returns to increasing the resolution beyond 640×640.

4. E2E Heterogeneous Pipeline Optimization

One of the most important performance criteria of a
video segmentation system for AR applications is the E2E
frame rate, which is directly perceivable by the applica-
tion users. In this section, we discuss our particular con-
siderations for ML inference pipeline optimization on web
browsers and mobile devices leveraging the GPU and the
NPU, respectively.

GPU for the Web: In browsers, ML frameworks with
WebGL support [6, 10, 11] were our natural choices for
hardware-aided ML inference in the browser, as NPUs cur-
rently lack a web standard. Since these implementations
ran signiﬁcantly slower than native OpenGL [21], we wrote
our own WebGL inference engine that can run at near-
In our WebGL implemen-
native OpenGL performance.
tation, performance-critical operations, e.g. convolutions,
leverage a modern GPU feature called Multiple Render Tar-

gets (MRT) [34], which allows rendering multiple textures
at once, substantially reducing the overhead of multiple
draw calls. For efﬁcient MRT, we separate logical tensors
and physical GPU objects, which have a 1-to-1 correspon-
dence in other frameworks, and allow tensors to take ﬂexi-
ble layouts instead of one hard-coded layout.

NPU for Mobile Devices: Advanced mobile-optimized
video processing frameworks, such as MediaPipe [25], offer
streamlined pipelines primarily targeting image and video
processing along with downstream co-processor interac-
tions and GPU execution, typically for the tasks of image
acquisition, pre-/post-processing, and rendering. For NPU-
accelerated ML inference use cases, the system can ben-
eﬁt from a processing chain that is tightly coupled with
the NPU. A naive replacement of GPU inference with its
NPU counterpart can introduce major inefﬁciencies such
as indirect inter-processor data ﬂow and latency from inter-
processor synchronization. We streamline the former with
Native Hardware Buffers [5] supporting shared access by
both the GPU and the NPU, and reduce the latter with sync
fences in the Android Synchronization API [12].

5. Results

Our segmentation models are trained with a standard Jac-
card [20, 33] loss on annotated datasets totaling 250k im-
ages that cover a wide range of video scenarios, from video
conferencing to human activities. We evaluate our models
on proprietary datasets, e.g. “video meeting” (800 samples)
and “full-body poses” (947 samples), and report their mean
Intersection over Union (mIoU) [2], J Mean [28] for region
similarity, and F Mean [28] for contour accuracy.

GPU for the Web: For video segmentation in web
apps designed for desktop and laptop web browsers, we
run our EfﬁcientNetLite- and MobileNetV3-based segmen-
tation models with the baseline CPU (XNNPACK [7] with
WebAssembly SIMD [8]) and with the GPU (our WebGL
implementation). Tab. 3 presents the direction for ML in-
ference in the web browser: (a) to employ the GPU for Web
ML when possible, and (b) to prefer EfﬁcientNetLite over
MobileNetV3 for GPU-accelerated video segmentation on
the web. In fact, the latter observation led to us conﬁdently
pursuing EfﬁcientNetLite-based models for the NPU below.

NPU for Mobile Devices: With our model improvement
and pipeline optimization as discussed in Sections 3 and
4, we have achieved signiﬁcantly higher accuracy and en-
abled much shorter inference latency, while reducing power
consumption compared to the baseline. For end-to-end per-
formance evaluation, we run video segmentation tasks with
512x512 input image resolution and 7.6B FLOPS in various
architectures on a Google Pixel 6 smartphone. As shown
in Tab. 2, the Model System #M4 (Final) with conﬁgu-
rations of the NPU, shared data buffer, and asynchronous
GPU-NPU execution along with other choices indicated in
the ablation study of Sec. 3, outperforms all other variants
in both latency and power consumption. Remarkably, our
ﬁnal model #M4 achieves higher accuracy with a speed up
by 81% (at 42.7 inf/sec) while consuming only 74% power
of #M1 with the baseline ML model as shown in Tab. 2.

6. Conclusion

In this paper, we showcase a development for edge-
based video segmentation with our improvements and op-
timizations of ML model, inference acceleration and E2E
heterogeneous compute pipeline. We visit our delibera-
tions that cover the scope of model architecture design and
system pipeline analysis, targeting speciﬁcally resource-
limited edge ML. We present our design and optimiza-
tion methodology in the ML model architecture and in
the end-to-end ML pipeline with efﬁcient use of resources
and APIs to maximize the throughput for production-ready,
heterogeneous edge ML solutions. We demonstrate that
our methodology enables higher accuracy across various
datasets,
lower end-to-end inference latency, and lower
power consumption. We expect our methodology to be ben-
eﬁcial and extensible to a wider range of edge CV/ML sys-
tems and real-time AR/VR applications.

References

[1] FLOPS.

https : / / en . wikipedia . org / wiki /

FLOPS. 2

[2] Intersection over Union. https://en.wikipedia.

org/wiki/Jaccard_index. 1, 3

[3] TOPS. https://semiengineering.com/tops-
memory - throughput - and - inference -
efficiency/. 2

[4] WebGL. https://khronos.org/webgl, 2011. 1
[5] Native Hardware Buffer.

https : / / developer .
android . com / ndk / reference / group / a -
hardware-buffer, 2017. 3

[6] TensorFlow JS. https://tensorflow.org/js, 2018.

2

[7] XNNPACK.

https : / / github . com / google /

XNNPACK, 2018. 3

[8] WebAssembly Core Speciﬁcation. https://www.w3.

org/TR/wasm-core-1/, 2019. 3

[9] EfﬁcientNet-lite.

https : / / github . com /
tensorflow / tpu / tree / master / models /
official/efficientnet/lite, 2020. 1

[10] Paddle.js. https://github.com/PaddlePaddle/

Paddle.js, 2020. 2

[11] ONNX Runtime Web.

https : / / github . com /

[12] Android

microsoft/onnxruntime, 2021. 2
Synchronization

https :
/ / source . android . com / devices / graphics /
sync, 2022. 3

Framework.

[13] Yagiz Aksoy, Tunc¸ Ozan Aydin, and Marc Pollefeys. Design-
ing Effective Inter-Pixel Information Flow for Natural Image
Matting. In 2017 IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), pages 228–236, 2017. 1
[14] Liang-Chieh Chen, Maxwell D. Collins, Yukun Zhu, George
Papandreou, Barret Zoph, Florian Schroff, Hartwig Adam,
and Jonathon Shlens. Searching for Efﬁcient Multi-Scale Ar-
chitectures for Dense Image Prediction. In NeurIPS, pages
8713–8724, 2018. 1

[15] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos,
Kevin Murphy, and Alan L. Yuille. Deeplab: Semantic im-
age segmentation with deep convolutional nets, atrous con-
IEEE Transactions on
volution, and fully connected crfs.
Pattern Analysis and Machine Intelligence, 40(4):834–848,
2018. 1

[16] Mingyu Ding, Xiaochen Lian, Linjie Yang, Peng Wang,
Xiaojie Jin, Zhiwu Lu, and Ping Luo. HR-NAS: Search-
ing Efﬁcient High-Resolution Neural Architectures with
Lightweight Transformers. In 2021 IEEE/CVF Conference
on Computer Vision and Pattern Recognition (CVPR), pages
2981–2991, 2021. 1

[17] Jiansheng Dong, Jingling Yuan, Lin Li, Xian Zhong, and
Weiru Liu. An efﬁcient semantic segmentation method using
pyramid shufﬂenet V2 with vortex pooling. In ICTAI, pages
1214–1220. IEEE, 2019. 1

[18] Andrew Howard, Mark Sandler, Bo Chen, Weijun Wang,
Liang-Chieh Chen, Mingxing Tan, Grace Chu, Vijay Va-
sudevan, Yukun Zhu, Ruoming Pang, Hartwig Adam, and

Quoc Le. Searching for MobileNetV3. In 2019 IEEE/CVF
International Conference on Computer Vision (ICCV), pages
1314–1324, 2019. 1

[19] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-Excitation Net-
works. In 2018 IEEE/CVF Conference on Computer Vision
and Pattern Recognition, pages 7132–7141, 2018. 2

[20] Paul Jaccard. The Distribution of the Flora in the Alpine

Zone. New Phytologist, 11(2):37–50, 1912. 3

[21] Juhyun Lee, Nikolay Chirkov, Ekaterina Ignasheva, Yury
Pisarchyk, Mogan Shieh, Fabio Riccardi, Raman Sarokin,
Andrei Kulik, and Matthias Grundmann. On-Device Neural
Net Inference with Mobile GPUs. In CVPR Workshop ECV,
2019. 2

[22] Yanwei Li, Lin Song, Yukang Chen, Zeming Li, Xiangyu
Zhang, Xingang Wang, and Jian Sun. Learning Dynamic
In 2020 IEEE/CVF
Routing for Semantic Segmentation.
Conference on Computer Vision and Pattern Recognition
(CVPR), pages 8550–8559, 2020. 1

[23] Shanchuan Lin, Andrey Ryabtsev, Soumyadip Sen-
gupta, Brian Curless, Steve Seitz, and Ira Kemelmacher-
Real-Time High-Resolution Background
Shlizerman.
In 2021 IEEE/CVF Conference on Computer
Matting.
Vision and Pattern Recognition (CVPR), pages 8758–8767,
2021. 1

[24] Chenxi Liu, Liang-Chieh Chen, Florian Schroff, Hartwig
Adam, Wei Hua, Alan L. Yuille, and Li Fei-Fei. Auto-
DeepLab: Hierarchical Neural Architecture Search for Se-
mantic Image Segmentation. In 2019 IEEE/CVF Conference
on Computer Vision and Pattern Recognition (CVPR), pages
82–92, 2019. 1

[25] Camillo Lugaresi, Jiuqiang Tang, Hadon Nash, Chris Mc-
Clanahan, Esha Uboweja, Michael Hays, Fan Zhang, Chuo-
Ling Chang, Ming Guang Yong, Juhyun Lee, Wan-Teh
Chang, Wei Hua, Manfred Georg, and Matthias Grundmann.
MediaPipe: A Framework for Building Perception Pipelines.
In CVPR Workshop on Computer Vision for AR/VR, 2019. 3
[26] Eslam Mohamed and Ahmad El Sallab. Spatio-Temporal
Multi-Task Learning Transformer for Joint Moving Object
Detection and Segmentation. In 2021 IEEE International In-
telligent Transportation Systems Conference (ITSC), pages
1470–1475, 2021. 1

[27] Vladimir Nekrasov, Hao Chen, Chunhua Shen, and Ian Reid.
Fast Neural Architecture Search of Compact Semantic Seg-
mentation Models via Auxiliary Cells. In 2019 IEEE/CVF
Conference on Computer Vision and Pattern Recognition
(CVPR), pages 9118–9127, 2019. 1

[28] Federico Perazzi, Jordi Pont-Tuset, Brian McWilliams,
Luc Van Gool, Markus Gross, and Alexander Sorkine-
Hornung. A Benchmark Dataset and Evaluation Methodol-
ogy for Video Object Segmentation. In The IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), 2016.
3

[29] Soumyadip Sengupta, Vivek Jayaram, Brian Curless,
Steven M. Seitz, and Ira Kemelmacher-Shlizerman. Back-
In
ground Matting: The World Is Your Green Screen.
2020 IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition (CVPR), pages 2288–2297, 2020. 1

[30] Evan Shelhamer, Jonathan Long, and Trevor Darrell. Fully
Convolutional Networks for Semantic Segmentation. IEEE
Transactions on Pattern Analysis and Machine Intelligence,
39(4):640–651, 2017. 1

[31] Ke Sun, Bin Xiao, Dong Liu, and Jingdong Wang. Deep
high-resolution representation learning for human pose esti-
mation. In 2019 IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR), pages 5686–5696, 2019. 1
[32] Yanan Sun, Guanzhi Wang, Qiao Gu, Chi-Keung Tang, and
Yu-Wing Tai. Deep Video Matting via Spatio-Temporal
Alignment and Aggregation. In 2021 IEEE/CVF Conference
on Computer Vision and Pattern Recognition (CVPR), pages
6971–6980, 2021. 1

[33] Taffee T Tanimoto. Elementary mathematical theory of clas-

siﬁcation and prediction. 1958. 3

[34] Wikipedia

contributors.
the

targets
https :
— Wikipedia,
/ / en . wikipedia . org / w / index . php ? title =
Multiple_Render_Targets&oldid=574813172,
2013. [Online; accessed 6-April-2022]. 3

Multiple
encyclopedia.

render

free

[35] Changqian Yu, Bin Xiao, Changxin Gao, Lu Yuan, Lei
Zhang, Nong Sang, and Jingdong Wang. Lite-HRNet: A
Lightweight High-Resolution Network. In 2021 IEEE/CVF
Conference on Computer Vision and Pattern Recognition
(CVPR), pages 10435–10445, 2021. 1

[36] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang
Wang, and Jiaya Jia. Pyramid scene parsing network. In 2017
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), pages 6230–6239, 2017. 1

