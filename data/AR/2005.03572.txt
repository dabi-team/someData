IEEE TRANSACTIONS ON CYBERNETICS, VOL. XX, NO. XX, MONTH YEAR

1

Enhancing Geometric Factors in Model Learning
and Inference for Object Detection and Instance
Segmentation

Zhaohui Zheng, Ping Wang, Dongwei Ren, Wei Liu, Rongguang Ye, Qinghua Hu, Wangmeng Zuo

1
2
0
2

l
u
J

5

]

V
C
.
s
c
[

4
v
2
7
5
3
0
.
5
0
0
2
:
v
i
X
r
a

Abstract—Deep learning-based object detection and instance
segmentation have achieved unprecedented progress. In this
paper, we propose Complete-IoU (CIoU) loss and Cluster-NMS
for enhancing geometric factors in both bounding box regression
and Non-Maximum Suppression (NMS), leading to notable gains
of average precision (AP) and average recall (AR), without the
sacriﬁce of inference efﬁciency. In particular, we consider three
geometric factors, i.e., overlap area, normalized central point dis-
tance and aspect ratio, which are crucial for measuring bounding
box regression in object detection and instance segmentation. The
three geometric factors are then incorporated into CIoU loss for
better distinguishing difﬁcult regression cases. The training of
deep models using CIoU loss results in consistent AP and AR
improvements in comparison to widely adopted (cid:96)n-norm loss and
IoU-based loss. Furthermore, we propose Cluster-NMS, where
NMS during inference is done by implicitly clustering detected
boxes and usually requires less iterations. Cluster-NMS is very
efﬁcient due to its pure GPU implementation, and geometric
factors can be incorporated to improve both AP and AR. In
the experiments, CIoU loss and Cluster-NMS have been applied
to state-of-the-art instance segmentation (e.g., YOLACT and
BlendMask-RT), and object detection (e.g., YOLO v3, SSD and
Faster R-CNN) models. Taking YOLACT on MS COCO as an
example, our method achieves performance gains as +1.7 AP and
+6.2 AR100 for object detection, and +1.1 AP and +3.5 AR100
for instance segmentation, with 27.1 FPS on one NVIDIA GTX
1080Ti GPU. All the source code and trained models are available
at https://github.com/Zzh-tju/CIoU.

Index Terms—Instance segmentation, object detection, bound-

ing box regression, non-maximum suppression.

I. INTRODUCTION

O BJECT detection and instance segmentation have re-

ceived overwhelming research attention due to their
practical applications in video surveillance, visual tracking,
face detection and inverse synthetic aperture radar detection
[1]–[6]. Since Deformable Part Model [7], bounding box
regression has been widely adopted for localization in object
detection. Driven by the success of deep learning, prosperous
deep models based on bounding box regression have been

This work was supported by National Natural Science Foundation of China

under Grants (Nos. 61801326 and U19A2073).

Z. Zheng, P. Wang and R. Ye are with the School of Mathematics,
Tianjin University, Tianjin, 300350, China. (Email: zh zheng@tju.edu.cn,
wang ping@tju.edu.cn, ementon@tju.edu.cn)

D. Ren and W. Zuo are with the School of Computer Science and
Technology, Harbin Institute of Technology, Harbin, 150001, China. (Email:
rendongweihit@gmail.com, cswmzuo@gmail.com)

W. Liu and Q. Hu are with the Tianjin Key Laboratory of Machine Learning,
College of Intelligence and Computing, Tianjin University, Tianjin, 300350,
China. (Email: lewiswestbrook95@gmail.com, huqinghua@tju.edu.cn)

Corresponding author: Dongwei Ren

Fig. 1. Diversity of bounding box regression, where green box is the ground-
truth box. First, albeit different ways of overlaps, these regression cases
have the same (cid:96)1 loss and IoU loss. We propose CIoU loss by considering
three geometric factors to distinguish them. Second, albeit NMS is a simple
post-processing step, it is the bottleneck for suppressing redundant boxes in
terms of both accuracy and inference efﬁciency. We then propose Cluster-
NMS incorporating with geometric factors for improving AP and AR while
maintaining high inference efﬁciency.

studied, including one-stage [8]–[13], two-stage [14], [15],
and multi-stage detectors [16], [17]. Instance segmentation
is a more challenging task [18]–[20], where instance mask
is further required for accurate segmentation of individuals.
Recent state-of-the-art methods suggest to add an instance
mask branch to existing object detection models, e.g., Mask
R-CNN [21] based on Faster R-CNN [15] and YOLACT [20]
based on RetinaNet [12]. In object detection and instance seg-
mentation, dense boxes are usually regressed [10], [15], [20],
[21]. As shown in Fig. 1, existing loss functions are limited
in distinguishing difﬁcult regression cases during training, and
it takes tremendous cost to suppress redundant boxes during
inference. In this paper, we suggest to handle this issue by
enhancing geometric factors of bounding box regression into
the learning and inference of deep models for object detection
and instance segmentation.

In training phase, a bounding box B = [x, y, w, h]T is forced
to approach its ground-truth box Bgt = [xgt, ygt, wgt, hgt]T by
minimizing loss function L,
(cid:88)

L(B, Bgt|Θ),

(1)

min
Θ

Bgt∈Bgt

 
 
 
 
 
 
IEEE TRANSACTIONS ON CYBERNETICS, VOL. XX, NO. XX, MONTH YEAR

2

where Bgt is the set of ground-truth boxes, and Θ is the
parameter of deep model for regression. A typical form of L is
(cid:96)n-norm, e.g., Mean-Square Error (MSE) loss and Smooth-(cid:96)1
loss [22], which have been widely adopted in object detection
[23], [24], pedestrian detection [25], [26], text detection [27]–
[29], 3D detection [30], [31], pose estimation [32], [33],
and instance segmentation [18], [20]. However, recent works
suggest that (cid:96)n-norm based loss functions are not consistent
with the evaluation metric, i.e., Interaction over Union (IoU),
and instead propose IoU-based loss functions [34]–[36]. For
training state-of-the-art object detection models, e.g., YOLO
v3 and Faster R-CNN, Generalized IoU (GIoU) loss achieves
better precision than (cid:96)n-norm based losses. However, GIoU
loss only tries to maximize overlap area of two boxes, and still
performs limited due to only considering overlap areas (refer
to the simulation experiments in Sec. III-A). As shown in Fig.
2, GIoU loss tends to increase the size of predicted box, while
the predicted box moves towards the target box very slowly.
Consequently, GIoU loss empirically needs more iterations
to converge, especially for bounding boxes at horizontal and
vertical orientations (see Fig. 4).

In testing phase, the inference of deep model is often efﬁ-
cient to predict dense boxes, which are left to Non-Maximum
Suppression (NMS) for suppressing redundant boxes. NMS is
an essential post-processing step in many detectors [8]–[10],
[12], [14], [15], [20], [37], [38]. In original NMS, a box is
suppressed only if it has overlap exceeding a threshold with the
box having the highest classiﬁcation score, which is likely to
be not friendly to occlusion cases. Other NMS improvements,
e.g., Soft-NMS [39] and Weighted-NMS [40], can contribute
to better detection precision. However, these improved NMS
methods are time-consuming, severely limiting their real-time
inference. Some accelerated NMS methods [20], [41] have
been developed for real-time inference, e.g., Fast NMS [20].
Unfortunately, Fast NMS yields performance drop due to that
many boxes are likely to be over-suppressed.

In this paper, we propose to enhance geometric factors in
both training and testing phases, where Complete-IoU (CIoU)
loss aims to better distinguish difﬁcult regression cases and
Cluster-NMS can improve AP and AR without the sacriﬁce
of inference time. As for CIoU loss, three geometric factors,
i.e., overlap area, normalized central point distance and aspect
ratio, are formulated as invariant to regression scale. Beneﬁting
from complete geometric factors, CIoU loss can be deployed
to improve average precision (AP) and average recall (AR)
when training deep models in object detection and instance
segmentation. From Fig. 2, CIoU loss converges much faster
than GIoU loss, and the incorporation of geometric factors
leads to much better match of two boxes.

We further propose Cluster-NMS, by which NMS can be
done by implicitly clustering detected boxes and geometric
factors can be easily incorporated, while maintaining high
inference efﬁciency. First, in Cluster-NMS, redundant detected
boxes can be suppressed by grouping them implicitly into
clusters. Cluster-NMS usually requires less iterations, and
its suppression operations can be purely implemented on
GPU, beneﬁting from parallel acceleration. Cluster-NMS can
guarantee exactly the same result with original NMS, while

Fig. 2. Updating of predicted boxes after different iterations optimized by
GIoU loss (ﬁrst row) and CIoU loss (second row). Green and black denote
target box and anchor box, respectively. Blue and red denote predicted boxes
for GIoU loss and CIoU loss, respectively. GIoU loss only considers overlap
area, and tends to increase the GIoU by enlarging the size of predicted box.
Beneﬁting from all the three geometric factors, the minimization of normalized
central point distance in CIoU loss gives rise to fast convergence and the
consistency of overlap area and aspect ratio contributes to better match of
two boxes.

it is very efﬁcient. Then, geometric factors, such as overlap-
based score penalty, overlap-based weighted coordinates and
normalized central point distance, can be easily assembled
into Cluster-NMS. Beneﬁting from geometric factors, Cluster-
NMS achieves signiﬁcant gains in both AP and AR, while
maintaining high inference efﬁciency.

In the experiments, CIoU loss and Cluster-NMS have been
applied to several state-of-the-art instance segmentation (e.g.,
YOLACT [20] and BlendMask-RT [42]) and object detection
(e.g., YOLO v3 [9], SSD [10] and Faster R-CNN [15]) models.
Experimental results demonstrate that CIoU loss can lead to
consistent gains in AP and AR against (cid:96)n-norm based and IoU-
based losses for object detection and instance segmentation.
Cluster-NMS contributes to notable gains in AP and AR, and
guarantees real-time inference.

This paper is a substantial extension of our pioneer work
[43], comparing with which we have three main changes.
First, the new CIoU loss in this work is a hybrid version of
DIoU and CIoU losses in [43], and is given more analysis.
Second, a novel Cluster-NMS is proposed to accommodate
kinds of NMS methods with high inference efﬁciency, and
DIoU-NMS [43] can also be easily incorporated to boost their
performance. Third, besides object detection, CIoU loss and
Cluster-NMS are further applied to state-of-the-art instance
segmentation models, e.g., YOLACT [20] and BlendMask-RT
[42]. We summarize the contributions from three aspects:

• A Complete IoU loss, i.e., CIoU loss, is proposed by tak-
ing three geometric factors, i.e., overlap area, normalized
central point distance and aspect ratio, into account, and
results in consistent performance gains for training deep
models of bounding box regression.

• We propose Cluster-NMS, in which geometric factors can
be further exploited for improving AP and AR while
maintaining high inference efﬁciency.

• CIoU loss and Cluster-NMS have been applied to state-
of-the-art
instance segmentation (e.g., YOLACT and
BlendMask-RT) and object detection (e.g., YOLO v3,
SSD and Faster R-CNN) models. Experimental results
validate the effectiveness and efﬁciency of our methods.

IEEE TRANSACTIONS ON CYBERNETICS, VOL. XX, NO. XX, MONTH YEAR

3

The remainder is organized as follows: Sec. II brieﬂy
reviews related works, Sec. III proposes CIoU loss by taking
complete geometric factors into account, Sec. IV presents
Cluster-NMS along with its variants by incorporating geomet-
ric factors, Sec. V gives experimental results and Sec. VI ends
this paper with concluding remarks.

II. RELATED WORK

A. Object Detection and Instance Segmentation

For a long time bounding box regression has been adopted
as an essential component in many representative object de-
tection frameworks [7]. In deep models for object detection,
R-CNN series [15], [17], [21] adopt two or three bounding box
regression modules to obtain higher location accuracy, while
YOLO series [8], [9], [38] and SSD series [10], [11], [44]
adopt one for faster inference speed. Recently, in RepPoints
[45], a rectangular box is formed by predicting several points.
FCOS [13] locates an object by predicting the distances from
the sampling points to the top, bottom, left and right sides of
the ground-truth box. PolarMask [19] predicts the length of
n rays from the sampling point to the edge of the object in
n directions to segment an instance. There are other detectors
such as RRPN [27] and R2CNN [46] adding rotation angle
regression to detect arbitrary orientated objects for remote
sensing detection and scene text detection. For instance seg-
mentation, Mask R-CNN [21] adds an extra instance mask
branch on Faster R-CNN, while the recent state-of-the-art
YOLACT [20] does the same thing on RetinaNet [12]. To
sum up, bounding box regression is one key component of
state-of-the-art deep models for object detection and instance
segmentation.

B. Loss Function for Bounding Box Regression

Albeit the architectures of deep models have been well
studied, loss function for bounding box regression also plays a
critical role in object detection. While (cid:96)n-norm loss functions
are usually adopted in bounding box regression,
they are
sensitive to varying scales. In YOLO v1 [38], square roots
for w and h are adopted to mitigate this effect, while YOLO
v3 [9] uses 2 − wh. In Fast R-CNN, Huber loss is adopted to
obtain more robust training. Meyer [47] suggested to connect
Huber loss with the KL divergence of Laplace distributions,
and further proposed a new loss function to eliminate the
transition points between (cid:96)1-norm and (cid:96)2-norm in the Huber
loss. Libra R-CNN [48] studies the imbalance issues and pro-
poses Balanced-(cid:96)1 loss. In GHM [49], the authors proposed a
gradient harmonizing mechanism for bounding box regression
loss that rectiﬁes the gradient contributions of samples. IoU
loss is also used since Unitbox [34], which is invariant to the
scale. To ameliorate the training stability, Bounded-IoU loss
[35] introduces the upper bound of IoU. GIoU [36] loss is
proposed to tackle the issues of gradient vanishing for non-
overlapping cases, but still suffers from the problems of slow
convergence and inaccurate regression. Nonetheless, geometric
factors of bounding box regression are actually not fully
exploited in existing loss functions. Therefore, we propose
CIoU loss by taking three geometric factors into account for

better training deep models of object detection and instance
segmentation.

C. Non-Maximum Suppression

NMS is a simple post-processing step in the pipelines of
object detection and instance segmentation, but it is the key
bottleneck for detection accuracy and inference efﬁciency. As
for improving detection accuracy, Soft-NMS [39] penalizes
the detection score of neighbors by a continuous function
w.r.t. IoU, yielding softer and more robust suppression than
original NMS. IoU-Net [50] introduces a new network branch
to predict the localization conﬁdence to guide NMS. Weighted-
NMS [40] outputs weighted combination of the cluster based
on their scores and IoU. Recently, Adaptive NMS [51] and
Softer-NMS [52] are proposed to respectively study proper
threshold and weighted average strategies. As for improv-
ing inference efﬁciency, boolean matrix [41] is adopted to
represent IoU relationship of detected boxes, for facilitating
GPU acceleration. A CUDA implementation of original NMS
by Faster R-CNN [15] uses logic operations to check the
boolean matrix line by line. Recently, Fast NMS [20] is
proposed to improve inference efﬁciency, but it inevitably
brings a drop of performance due to the over-suppression of
boxes. In this work, we propose efﬁcient Cluster-NMS, and
geometric factors can be readily exploited to obtain signiﬁcant
improvements in both precision and recall.

III. COMPLETE-IOU LOSS
For training deep models in object detection, IoU-based
losses are suggested to be more consistent with IoU metric
than (cid:96)n-norm losses [34]–[36]. The original IoU loss can be
formulated as [36],

LIoU = 1 − IoU.

(2)

However, it fails in distinguishing the cases that two boxes do
not overlap. Then, GIoU [36] loss is proposed,
|C − B ∪ Bgt|
|C|

LGIoU = 1 − IoU +

(3)

,

where C is the smallest box covering B and Bgt, and |C| is
the area of box C. Due to the introduction of penalty term in
GIoU loss, the predicted box will move towards the target box
in non-overlapping cases. GIoU loss has been applied to train
state-of-the-art object detectors, e.g., YOLO v3 and Faster R-
CNN, and achieves better precision than MSE loss and IoU
loss.

A. Analysis to IoU and GIoU Losses

To begin with, we analyze the limitations of original IoU
loss and GIoU loss. However, it is very difﬁcult to analyze
the procedure of bounding box regression simply from the
detection results, where the regression cases in uncontrolled
benchmarks are often not comprehensive, e.g., different dis-
tances, different scales and different aspect ratios. Instead,
we suggest conducting simulation experiments, where the
regression cases should be comprehensively considered, and
then the issues of a given loss function can be easily analyzed.

IEEE TRANSACTIONS ON CYBERNETICS, VOL. XX, NO. XX, MONTH YEAR

4

Algorithm 1 Simulation Experiment
Input: Loss L is a continuous bounded function deﬁned on R4
+.

s=1}N

B = {{Bn,s}S
n=1 is the set of anchor boxes at N = 5, 000
uniformly scattered points within the circular region with center
(10, 10) and radius 3, and S = 7×7 covers 7 scales and 7 aspect
ratios of anchor boxes.
Bgt = {Bgt
i }7
with area 1, and have 7 aspect ratios.

i=1 is the set of target boxes that are ﬁxed at (10, 10)

Output: Regression error E ∈ RT ×N
1: Initialize E = 0 and maximum iteration T .
2: Do bounding box regression:
3: for n = 1 to N do
4:
5:
6:

for s = 1 to S do

for i = 1 to 7 do

for t = 1 to T do



η =

0.1
0.01
0.001 if

t <= 0.8T

if
if 0.8T < t <= 0.9T


t > 0.9T
n,s is gradient of L(Bt−1
∇Bt−1
Bt
n,s + η(2 − IoU t−1
n,s = Bt−1
E(t, n) = E(t, n) + |Bt

n,s , Bgt
n,s )∇Bt−1
n,s
n,s − Bgt
i |

i ) w.r.t. Bt−1

n,s

7:

8:
9:
10:
11:
12:
13:
14: end for
15: return E

end for

end for

end for

1) Simulation Experiment: In the simulation experiments,
we try to cover most of the relationships between bounding
boxes by considering geometric factors including distance,
scale and aspect ratio, as shown in Fig. 3(a). In particular,
we choose 7 unit boxes (i.e., the area of each box is 1) with
different aspect ratios (i.e., 1:4, 1:3, 1:2, 1:1, 2:1, 3:1 and 4:1)
as target boxes. Without loss of generality, the central points
of the 7 target boxes are ﬁxed at (10, 10). The anchor boxes
are uniformly scattered at 5,000 points. (i) Distance: In the
circular region centered at (10, 10) with radius 3, 5,000 points
are uniformly chosen to place anchor boxes with 7 scales and 7
aspect ratios. In these cases, overlapping and non-overlapping
boxes are included. (ii) Scale: For each point, the areas of
anchor boxes are set as 0.5, 0.67, 0.75, 1, 1.33, 1.5 and 2.
(iii) Aspect ratio: For a given point and scale, 7 aspect ratios
are adopted, i.e., following the same setting with target boxes
(i.e., 1:4, 1:3, 1:2, 1:1, 2:1, 3:1 and 4:1). All the 5, 000 × 7 × 7
anchor boxes should be ﬁtted to each target box. To sum up,
there are totally 1, 715, 000 = 7 × 7 × 7 × 5, 000 regression
cases.

Then given a loss function L, we can simulate the procedure
of bounding box regression for each case using stochastic
gradient descent algorithm. For predicted box Bi, the current
prediction can be obtained by

i

,

(4)

)∇Bt−1
i

i = Bt−1
Bt

i + η(2 − IoU t−1
i is the predicted box at iteration t, ∇Bt−1

where Bt
denotes the
i
gradient of loss L w.r.t. Bt−1
at iteration t − 1, and η is the
learning rate. It is worth noting that in our implementation,
the gradient is multiplied by 2 − IoU t−1
to accelerate the
convergence. The performance of bounding box regression is
evaluated using (cid:96)1-norm. For each loss function, the simulation
experiment is terminated when reaching iteration T = 200,

i

i

Fig. 3. Simulation experiments: (a) 1,715,000 regression cases are adopted by
considering different distances, scales and aspect ratios, (b) regression error
sum (i.e., (cid:80)

n E(t, n)) curves of different loss functions at iteration t.

and the error curves are shown in Fig. 3(b).

2) Limitations of IoU and GIoU Losses:

In Fig. 4, we
visualize the ﬁnal regression errors at iteration T for 5,000
scattered points. From Fig. 4(a), it is easy to see that IoU loss
only works for the cases of overlapping with target boxes. The
anchor boxes without overlap will not move due to that the
gradient is always 0.

By adding a penalty term as Eqn. (3), GIoU loss can better
relieve the issues of non-overlapping cases. From Fig. 4(b),
GIoU loss signiﬁcantly enlarges the basin, i.e., the area that
GIoU works. But the cases with extreme aspect ratios are
likely to still have large errors. This is because that the penalty
term in GIoU loss is used to minimize |C − A ∪ B|, but the
area of C − A ∪ B is often small or 0 (when two boxes have
inclusion relationships), and then GIoU almost degrades to IoU
loss. GIoU loss would converge to good solution as long as
running sufﬁcient iterations with proper learning rates, but the
convergence rate is indeed very slow. Geometrically speaking,
from the regression steps as shown in Fig. 2, one can see that
GIoU actually increases the predicted box size to overlap with
target box, and then the IoU term will make the predicted box
match with the target box, yielding a very slow convergence.
To sum up, IoU-based losses only aim to maximize the
overlap area of two boxes. Original IoU loss converges to
bad solutions for non-overlapping cases, while GIoU loss is
with slow convergence especially for the boxes with extreme
aspect ratios. And when incorporating into object detection
or instance segmentation pipeline, both IoU and GIoU losses
cannot guarantee the accuracy of regression. We in this paper
suggest that a good loss function for bounding box regression
should enhance more geometric factors besides overlap area.

B. CIoU Loss

Considering the geometric factors for modeling regression
relationships in Simulation Experiment, we suggest that a loss
function should take three geometric factors into account, i.e.,
overlap area, distance and aspect ratio. Generally, a complete
loss can be deﬁned as,

L = S(B, Bgt) + D(B, Bgt) + V (B, Bgt),

(5)

where S, D, V denote the overlap area, distance and aspect
ratio, respectively. One can see that IoU and GIoU losses only

IEEE TRANSACTIONS ON CYBERNETICS, VOL. XX, NO. XX, MONTH YEAR

5

(a) LIoU

(b) LGIoU

(c) LCIoU

Fig. 4. Visualization of regression errors of IoU, GIoU and CIoU losses at the ﬁnal iteration T , i.e., E(T, n) for every coordinate n. We note that the
basins in (a) and (b) correspond to good regression cases. One can see that IoU loss has large errors for non-overlapping cases, GIoU loss has large errors
for horizontal and vertical cases, and our CIoU loss leads to very small regression errors everywhere.

regression problem. (ii) Analogous to GIoU loss, CIoU loss
can provide moving directions for bounding boxes when non-
overlapping with target box. Furthermore, our CIoU loss has
two merits over IoU and GIoU losses, which can be evaluated
by simulation experiment. (i) As shown in Fig. 2 and Fig. 3,
CIoU loss can rapidly minimize the distance of two boxes, and
thus converges much faster than GIoU loss. (ii) For the cases
with inclusion of two boxes, or with extreme aspect ratios,
CIoU loss can make regression very fast, while GIoU loss has
almost degraded to IoU loss, i.e., |C − B ∪ Bgt| → 0.

Fig. 5. Normalized central point distance. c is the diagonal length of the
smallest enclosing box covering two boxes, and d = ρ(p, pgt) is the distance
of central points of two boxes.

consider the overlap area. In the complete loss, IoU is only a
good choice for S,

S = 1 − IoU.

(6)

IV. CLUSTER-NMS

Similar to IoU, we want to make both D and V be also in-
variant to regression scale. In particular, we adopt normalized
central point distance to measure the distance of two boxes,
D = ρ2(p,pgt)

(7)

,

c2

where p = [x, y]T and pgt = [xgt, ygt]T are the central points
of boxes B and Bgt, c is the diagonal length of box C, and
ρ is speciﬁed as Euclidean distance, as shown in Fig. 5. And
the consistency of aspect ratio is implemented as
π2 (arctan wgt
h )2.
Finally, we obtain the Complete-IoU (CIoU) loss,

hgt − arctan w

V = 4

(8)

LCIoU = 1 − IoU +

ρ2(p, pgt)
c2

+ αV.

(9)

It is easy to see that S, D and V are invariant to regression
scale and are normalized to [0, 1]. Here, we only introduce
one trade-off parameter α, which is deﬁned as

α =

(cid:26) 0,

V
(1−IoU )+V ,

if IoU < 0.5,
if IoU ≥ 0.5.

(10)

One can see that our CIoU loss will degrade to DIoU loss
in our pioneer work [43] when IoU < 0.5. It is reasonable
that when two boxes are not well matched, the consistency
of aspect ratio is less important. And when IoU ≥ 0.5, the
consistency of aspect ratio becomes necessary.

The proposed CIoU loss inherits some properties from IoU
and GIoU losses. (i) CIoU loss is still invariant to the scale of

Most state-of-the-art object detection [9], [10], [15] and
instance segmentation [20] adopt the strategy to place more
anchor boxes to detect difﬁcult and small objects for improving
detection accuracy. Moreover, NMS for suppressing redundant
boxes is facing tremendous pressure during inference.

Let B = [B1, B2, · · · , BN ]T 1 be an N × 4 matrix storing
N detected boxes. These boxes have been sorted according
to the non-ascending classiﬁcation scores, i.e., s1 ≥ s2 ≥
· · · ≥ sN . Original NMS is to suppress redundant boxes by
sequentially traversing N boxes. Speciﬁcally, for the box M
with the current highest score, original NMS can be formally
deﬁned as,

(cid:40)

sj =

sj, if IoU (M, Bj) < ε,
if IoU (M, Bj) ≥ ε,
0,

(11)

where ε is a threshold. Original NMS is very time-consuming.
And several improved NMS, e.g., Soft-NMS [39] and non-
maximum weighted (Weighted-NMS) [40], can further im-
prove the precision and recall, but
they are much more
inefﬁcient. We propose Cluster-NMS, where NMS can be
parallelly done on implicit clusters of detected boxes, usually
requiring less iterations. Besides, Cluster-NMS can be purely
implemented on GPU, and is much more efﬁcient than original
NMS. Then, we incorporate the geometric factors into Cluster-
NMS for further improving both precision and recall, while
maintaining high inference efﬁciency.

IEEE TRANSACTIONS ON CYBERNETICS, VOL. XX, NO. XX, MONTH YEAR

6

Algorithm 2 Cluster-NMS
Input: N detected boxes B = [B1, B2, · · · , BN ]T with non-
ascending sorting by classiﬁcation score, i.e., s1 ≥ · · · ≥ sN .
Output: b = {bi}1×N , bi ∈ {0, 1} encodes ﬁnal detection result,

where 1 denotes reservation and 0 denotes suppression.

1: Initialize T = N , t = 1, t∗ = T and b0 = 1
2: Compute IoU matrix X = {xij}N ×N with xij = IoU (Bi, Bj).
(cid:46) Upper triangular matrix with xii = 0, ∀i
3: X = triu(X)
4: while t ≤ T do
5:
6:
7:

At = diag(bt−1)
C t = At × X
g ← max

C t

(cid:46) Find maximum for each column j
(cid:26) bj = 1,
bj = 0,

if gj < ε
if gj ≥ ε

(cid:46)

j

8:

bt ← find(g < ε)

if bt == bt−1 then
t∗ = t, break

9:
10:
11:
12:
13: end while
14: return bt∗

end if
t = t + 1

A. Cluster-NMS

We ﬁrst compute the IoU matrix X = {xij}N ×N , where
xij = IoU (Bi, Bj). We note that X is a symmetric matrix,
and we only need the upper triangular matrix of X, i.e., X =
triu(X) with xii = 0, ∀i. In Fast NMS [20], the suppression
is directly performed on the matrix X, i.e., a box Bj would be
suppressed as long as ∃ xij > ε. Fast NMS is indeed efﬁcient,
but it suppresses too many boxes. Considering ∃Bi has been
suppressed, the box Bi should be excluded when making the
rule for suppressing Bj even if xij > ε. But in Fast NMS,
Bi is actually taken into account, thereby being likely to over-
suppress more boxes.

Let b = {bi}1×N , bi ∈ {0, 1} be a binary vector to indicate
the NMS result. We introduce an iterative strategy, where
current suppressed boxes with bi = 0 would not affect the
results. Speciﬁcally, for iteration t, we have the NMS result
bt−1, and introduce two matrices,

At = diag(bt−1),
Ct = At × X.

(12)

Then the suppression is performed on the matrix C. The
details can be found in Alg. 2. All these operations can be
implemented on GPU, and thus Cluster-NMS is very efﬁcient.
When T = 1, Cluster-NMS degrades to Fast NMS, and
when T = N , the result of Cluster-NMS is totally same with
original NMS. Due to the pure operation on matrix A and X,
the predicted boxes without overlaps are implicitly grouped
into different clusters, and the suppression is performed in
parallel between clusters. Cluster-NMS can guarantee the same
result with original NMS, and generally can stop with less
iterations, referring to Sec. IV-C for theoretical analysis. We
present an example in Fig. 6, where 10 detected boxes can be
divided into 3 clusters and the largest cluster contains 4 boxes.
Thus, the maximum iteration t∗ of Cluster-NMS in Alg. 2 is 4.
But in Fig. 6, b after only 2 iterations is exactly the ﬁnal result

of original NMS. In practical, Cluster-NMS usually requires
less iterations.

We also note that original NMS has been implemented as
CUDA NMS in Faster R-CNN [15], which is recently included
into TorchVision 0.3. TorchVision NMS is faster than Fast
NMS and our Cluster-NMS (see Table V), due to engineering
accelerations. Our Cluster-NMS requires less iterations and
can also be further accelerated by adopting these engineering
tricks, e.g., logic operations on binarized X as in Proposition
1. But this is beyond the scope of this paper. Moreover, the
main contribution of Cluster-NMS is that geometric factors can
be easily incorporated into Cluster-NMS, while maintaining
high efﬁciency.

B. Incorporating Geometric Factors into Cluster-NMS

Geometric factors for measuring bounding box regression
can be introduced into Cluster-NMS for improving precision
and recall.

1) Score Penalty Mechanism into Cluster-NMS: Instead of
the hard threshold in original NMS Eqn. (11), we introduce
a Score Penalty Mechanism based on the overlap areas into
Cluster-NMS, analogous to Soft-NMS. Speciﬁcally, in Cluster-
NMSS, we adopt the score penalty following ”Gaussian” Soft-
NMS [39], and the score sj is re-weighted as

sj = sj

(cid:89)

e−

(A×X)2
ij
σ

,

i

(13)

where σ = 0.2 in this work. It is worth noting that our Cluster-
NMSS is not completely same with Soft-NMS. In Cluster-
NMSS, sj is only penalized by those boxes with higher scores
than sj, since A × X is an upper triangular matrix.

2) Normalized Central Point Distance into Cluster-NMS:
As suggested in our pioneer work [43], normalized central
point distance can be included into NMS to beneﬁt the cases
with occlusions. By simply introducing the normalized central
point distance D in Eqn. (7) into IoU matrix X, Cluster-
NMSD is actually equivalent with DIoU-NMS in our pioneer
work [43]. Moreover, we can incorporate the normalized
central point distance into Cluster-NMSS, forming Cluster-
NMSS+D. Speciﬁcally, the score sj is penalized as

sj = sj

(cid:89)

i

min{e−

(A×X)2
ij
σ

+ Dβ, 1},

(14)

where β = 0.6 is a trade-off parameter for balancing the
precision and recall (see Fig. 8).

3) Weighted Coordinates into Cluster-NMS: Weighted-
NMS [40] is a variant of original NMS. Instead of deleting
redundant boxes, Weighted-NMS creates new box by merging
box coordinates according to the weighted combination of the
boxes based on their scores and overlap areas. The formulation
of Weighted-NMS is as follow,

B =

1
(cid:80)
wj
j

(cid:88)

Bj ∈Λ

wjBj,

(15)

1Actually, B is a tensor with size C × N × 4, which contains C classes.
Since different classes share the same NMS operation, we omit this channel
for simplicity.

where Λ = {Bj | xij ≥ ε, ∀i} is a set of boxes, weight
wj = sjIoU (B, Bj), and B is the newly created box. However,

IEEE TRANSACTIONS ON CYBERNETICS, VOL. XX, NO. XX, MONTH YEAR

7

Fig. 6. An example of Cluster-NMS, where 10 detected boxes are implicitly grouped into 3 clusters. The IoU matrix X has been binarized by threshold
ε = 0.5. When t = 1, Cluster-NMS is equivalent to Fast NMS [20], where the boxes are over-suppressed. Theoretically, Cluster-NMS will stop after at
most 4 iterations, since the largest cluster (red boxes) has 4 boxes. But b after only 2 iterations is exactly the ﬁnal result of original NMS, indicating that
Cluster-NMS usually requires less iterations.

Weighted-NMS is very inefﬁcient because of the sequential
operations on every box.

Analogously, such weighted strategy can be included into
our Cluster-NMS. In particular, given the matrix C, we
multiply its every column using the score vector s =
[s1, s2, · · · , sN ]T in the entry-by-entry manner, resulting in C(cid:48)
to contain both the classiﬁcation score and IoU. Then for the
N detected boxes B = [B1, B2, · · · , BN ]T, their coordinates
can be updated by

B =

C(cid:48) × B

Repmat4((cid:80)

i C(cid:48)(i, :))

,

(16)

where Repmat4 copies the N ×1 vector 4 times to form N ×4
matrix, making the entry-wise division feasible. The Cluster-
NMSW shares the same output form with Cluster-NMS, but
the coordinates of boxes have been updated. Moreover, the
normalized central point distance can be easily assembled
into Cluster-NMSW , resulting in Cluster-NMSW +D, where
the IoU matrix X is computed by considering both overlap
area and distance as DIoU-NMS [43]. Cluster-NMSW has the
same result with Weighted-NMS as well as 6.1 times faster
efﬁciency, and Cluster-NMSW +D contributes to consistent
improvements in both AP and AR (see Table V).

(i) When t = 1, b1

1 = [b1]T = [1]T is same to the result by
original NMS, since the ﬁrst box with the largest score is kept
deﬁnitely. When t = 2, we have

C = A × X =

(cid:18) A2

0

0 AN −2

(cid:19) (cid:18) X2 Xother
0 XN −2

(cid:19)

. (18)

Since the result b2
we only consider

2 is not affected by the last N − k boxes,

C2 = A2 × X2 =

(cid:18) 0
0

b1x1,2
0

(cid:19)

(cid:18) 0 x1,2
0
0

=

(cid:19)

,

(19)

where x1,2 is a binary value. And thus b2
2 = [b1, b2]T, where
b2 = ¬x1,2 is exactly the output of original NMS at iteration
t = 2.

(ii) Then we assume that when t = k, bk

k determined by Ck
is same with the result of original NMS after iteration t = k.
When t = k + 1, we have

C = A×X =

(cid:18) Ak+1
0

0
AN −k−1

(cid:19) (cid:18) Xk+1 Xother
XN −k−1

0

(cid:19)

.

(20)
Analogously, we
block matrices
AN −k−1, Xother and XN −k−1, since they do not affect
Ck+1,

these

care

not

do

C. Theoretical Analysis

Ck+1 = Ak+1 × Xk+1 =

In the following, we ﬁrst prove that Cluster-NMS with
T = N iterations can achieve the same suppression result
with original NMS, and then discuss that Cluster-NMS usually
requires less iterations.

Proposition 1. Let bT be the result of Cluster-NMS after T
iterations, bT is also the ﬁnal result of original NMS.

Proof. Let Xk be the square block matrix of X containing
the ﬁrst k × k partition, while let XN −k be the square block
matrix of X containing the last (N − k) × (N − k) partition.
The matrices A and C share the same deﬁnition. Let bk
k
be the subvector of b containing the ﬁrst k elements after
k iterations in Cluster-NMS. And it is straightforward that
Ak = diag(bk
k). Besides, we binarize the upper triangular
IoU matrix X = {xij}N ×N by threshold ε,
(cid:26) xij = 0,
xij = 1,

if xij < ε,
if xij ≥ ε,

(17)

which does not affect the result by Cluster-NMS in Alg. 2,
but makes the proof easier to understand.

(cid:19) (cid:18) Xk X:,k+1

(cid:18) Ak
0

0
b
(cid:18) Ck Ak × X:,k+1

0

(cid:19)

0

,

0

0

(cid:19)

,

=

(21)
where X:,k+1 is the k-th column of Xk by excluding the last
0. Then it is easy to see that bk+1 can be determined by

bk+1 = ¬ max(Ak × X:,k+1),

(22)

which is the output of original NMS at iteration t = k + 1.
And thus bk+1
k+1 = [bk
k; bk+1] is same with the result of original
NMS after iteration t = k + 1.

Combining (i) and (ii), it can be deduced that bT after T
iterations in Cluster-NMS is exactly the ﬁnal result of original
NMS.

Discussion: Cluster-NMS usually requires less iterations.
Let B∗ = {Bj1 , Bj2, · · · , BjM } be the largest cluster con-
taining M boxes, where a box Bj ∈ B∗ if and only if
∃i ∈ {j1, j2, · · · , jM }, IoU (Bi, Bj) ≥ ε, and IoU (Bj, Bu) <
ε, ∀j ∈ {j1, j2, · · · , jM } and ∀u /∈ {j1, j2, · · · , jM }. Thus for
the binarized IoU matrix X, xu,j = 0, ∀j ∈ {j1, j2, · · · , jM }

IEEE TRANSACTIONS ON CYBERNETICS, VOL. XX, NO. XX, MONTH YEAR

8

and ∀u /∈ {j1, j2, · · · , jM }. That is to say B∗ is actually
processed without considering boxes in other clusters, and
after M iterations, b deﬁnitely will not change. Different
clusters are processed in parallel by Cluster-NMS.

V. EXPERIMENTAL RESULTS
In this section, we evaluate our proposed CIoU loss
and Cluster-NMS for state-of-the-art instance segmentation
YOLACT [20] and BlendMask-RT [42], and object detection
YOLO v3 [9], SSD [10] and Faster R-CNN [15] on popular
benchmark datasets, e.g., PASCAL VOC [53] and MS COCO
[54]. CIoU loss is implemented in C/C++ and Pytorch, and
Cluster-NMS is implemented in Pytorch. For the threshold ε,
we adopt their default settings. The source code and trained
models have been made publicly available.

A. Instance Segmentation

1) YOLACT: YOLACT [20] is a real-time instance segmen-
tation method, in which Smooth-(cid:96)1 loss is adopted for training,
and Fast NMS is used for real-time inference. To make a
fair comparison, we train two YOLACT2 (ResNet-101-FPN)
models using Smooth-(cid:96)1 loss and CIoU loss, respectively. The
training is carried out on NVIDIA GTX 1080Ti GPU with
batchsize 4 per GPU. The training dataset is MS COCO 2017
train set, and the testing dataset is MS COCO 2017 validation
set. The evaluation metrics include AP, AR, inference time
(ms) and FPS. The details of different settings of AP and AR
can be found in [10].

The comparison results are reported in Table I. By adopting
the same NMS strategy, i.e., Fast NMS, one can see that
CIoU loss is superior to Smooth-(cid:96)1 loss in terms of most
AP and AR metrics. Then on the YOLACT model trained
using our CIoU loss, we evaluate the effectiveness of Cluster-
NMS by assembling different geometric factors. One can see
that: (i) In comparison to original NMS, Fast NMS is efﬁcient
but yields notable drops in AP and AR, while our Cluster-
NMS can guarantee exactly the same results with original
NMS and its efﬁciency is comparable with Fast NMS. (ii)
By enhancing score penalty based on overlap areas, Cluster-
NMSS achieves notable gains in both AP and AR. Especially,
for large objects, Cluster-NMSS performs much better in APL
and ARL. The hard threshold strategy in original NMS is
very likely to treat large objects with occlusion as redundancy,
while our Cluster-NMSS is friendly to large objects. (iii)
By further incorporating distance, Cluster-NMSS+D achieves
higher AR metrics, albeit its AP metrics are only on par
with Cluster-NMSS. From Fig. 8, one can see that distance
is a crucial factor for balancing precision and recall, and
we choose β = 0.6 in our experiments. (iv) Incorporating
geometric factors into Cluster-NMS takes only a little more
inference time, and ∼28 FPS on one GTX 1080Ti GPU can
guarantee real-time inference. To sum up, our Cluster-NMS
with geometric factors contributes to signiﬁcant performance
gains, while maintaining high inference efﬁciency.

One may notice that our re-trained YOLACT model using
Smooth-(cid:96)1 loss is a little inferior to the results reported in their

2https://github.com/dbolya/yolact

original paper [20]. This is because batchsize in [20] is set as
8, which causes out of memory on our GTX 1080Ti GPU.
Then, we also evaluate Cluster-NMS on their released models
trained using Smooth-(cid:96)1 loss. From Table II, one can draw
the consistent conclusion that Cluster-NMS with geometric
factors contributes to AP and AR improvements. Considering
Tables I and II, Cluster-NMSS+D is a better choice to balance
precision and recall. From Fig. 8, it is easy to see that Cluster-
NMSS+D can achieve higher precision by setting larger β, or
can achieve higher recall by setting smaller β. Finally, we
present the results of detection and segmentation in Fig. 7,
from which one can easily ﬁnd the more accurate detected
boxes and segmentation masks by our CIoU loss and Cluster-
NMSS+D.

2) BlendMask-RT: We then evaluate our CIoU loss and
Cluster-NMS on the most state-of-the-art instance segmenta-
tion method BlendMask-RT [42], which is based on YOLACT
by introducing attention mechanism. To make a fair compar-
ison, we train two BlendMask-RT3 (ResNet-50-FPN) models
using GIoU loss and CIoU loss, respectively. In experiments,
we adopt the same training and testing sets with YOLACT
and also the same evaluation measures. Table III reports the
quantitative results of BlendMask-RT, in which we can draw
the same conclusion that CIoU loss is effective for training,
and Cluster-NMS with geometric factors can further improve
the performance, while maintaining high inference efﬁciency.

B. Object Detection

For object detection, YOLO v3, SSD and Faster R-CNN are

adopted for evaluation.

1) YOLO v3 [9]: First, we evaluate CIoU loss in compar-
ison to MSE loss, IoU loss and GIoU loss on PASCAL VOC
2007 test set [53]. YOLO v3 is trained on PASCAL VOC
07+12 (the union of VOC 2007 trainval and VOC 2012 train-
val). The backbone network is Darknet608. We follow exactly
the GDarknet4 training protocol released from [36]. Original
NMS is adopted during inference. The performance for each
loss has been reported in Table IV. Besides AP metrics based
on IoU, we also report the evaluation results using AP metrics
based on GIoU. As shown in Table IV, GIoU as a generalized
version of IoU indeed achieves a certain degree of performance
improvement. DIoU loss only includes distance in our pioneer
work [43], and can improve the performance with gains of
3.29% AP and 6.02% AP75 using IoU as evaluation metric.
CIoU loss takes the three important geometric factors into
account, which brings an amazing performance gains, i.e.,
5.67% AP and 8.95% AP75. Also in terms of GIoU metric,
we can draw the same conclusion, validating the effectiveness
of CIoU loss.

Since YOLO v3 with GDarknet

is implemented using
C/C++, it is not suitable for evaluating Cluster-NMS. Then
we evaluate Cluster-NMS on a pre-trained YOLO v3 model
in Pytorch, where the model YOLOv3-spp-Ultralytics-6085

3https://github.com/aim-uofa/AdelaiDet
4https://github.com/generalized-iou/g-darknet
5https://github.com/ultralytics/yolov3

IEEE TRANSACTIONS ON CYBERNETICS, VOL. XX, NO. XX, MONTH YEAR

9

TABLE I
INSTANCE SEGMENTATION RESULTS OF YOLACT [20]. THE MODELS ARE RE-TRAINED USING SMOOTH-(cid:96)1 LOSS AND CIOU LOSS BY US, AND THE
RESULTS ARE REPORTED ON MS COCO 2017 VALIDATION SET [54].

Method

Loss
Smooth-(cid:96)1

YOLACT-550 [20]
R-101-FPN

LCIoU

NMS Strategy
Fast NMS
Fast NMS
Original NMS
Cluster-NMS
Cluster-NMSS

FPS Time AP AP50 AP75 APS APM APL AR1 AR10 AR100 ARS ARM ARL
30.6 32.7
18.8 44.7 59.8
30.6 32.7
18.0 44.0 60.8
18.7 45.8 62.8
11.5 86.6
18.8 45.8 62.8
28.8 34.7
19.7 47.7 65.9
28.6 35.0
19.5 47.8 66.4
Cluster-NMSS+D 27.1 36.9

27.5 39.2
27.6 39.3
27.5 40.1
27.5 40.1
27.7 41.4
27.6 41.3

32.0 48.5
32.0 49.7
32.2 49.8
32.2 49.7
33.0 50.8
32.8 50.7

29.1 47.4
29.6 48.1
29.7 48.3
29.7 48.3
30.3 49.1
30.2 48.9

40.3
40.3
41.7
41.7
43.6
43.8

30.5
30.9
31.0
31.0
31.7
31.7

9.4
9.4
9.4
9.4
9.7
9.6

TABLE II
INSTANCE SEGMENTATION RESULTS OF YOLACT [20]. THE MODEL IS BORROWED FROM THE ORIGINAL PAPER [20], AND THE RESULTS ARE REPORTED
ON MS COCO 2017 VALIDATION SET [54].

Method

Backbone

YOLACT-550 [20] R-101-FPN

NMS Strategy
Fast NMS
Original NMS
Cluster-NMS
Cluster-NMSS

FPS Time AP AP50 AP75 APS APM APL AR1 AR10 AR100 ARS ARM ARL
30.6 32.7
18.9 44.8 61.0
19.5 46.4 62.8
11.9 83.8
19.5 46.4 62.8
29.2 34.2
20.4 48.3 66.3
28.8 34.7
20.5 48.5 66.8
Cluster-NMSS+D 27.5 36.4

10.1 32.2 50.1
10.0 32.3 50.3
10.0 32.3 50.3
10.3 33.1 51.2
10.2 32.9 51.1

27.8 39.6
27.7 40.4
27.7 40.4
27.8 41.8
27.8 41.7

29.8 48.3
29.9 48.4
29.9 48.4
30.5 49.3
30.4 49.1

31.3
31.4
31.4
32.1
32.0

40.8
42.1
42.1
44.1
44.3

TABLE III
INSTANCE SEGMENTATION RESULTS OF BLENDMASK-RT [42]. THE MODELS ARE RE-TRAINED USING GIOU LOSS AND CIOU LOSS BY US, AND THE
RESULTS ARE REPORTED ON MS COCO 2017 VALIDATION SET.

Method

Loss
LGIoU

BlendMask-RT

R-50-FPN

LCIoU

NMS Strategy
Fast NMS
Fast NMS

FPS Time AP AP50 AP75 APS APM APL AR1 AR10 AR100 ARS ARM ARL
42.7 23.4 34.4 54.3 36.3
25.6 53.0 63.4
42.7 23.4 34.9 55.2 37.0
25.5 53.2 63.8
Original NMS (TorchVision) 42.7 23.4 35.1 55.4 37.2
26.4 55.0 65.9
40.7 24.6 35.1 55.4 37.2
26.4 55.0 65.9
39.7 25.2 35.2 55.4 37.3
26.4 55.2 66.2
26.5 55.3 66.4
38.5 26.0 35.2 55.3 37.4

14.4 38.3 51.9 29.3 45.2
14.1 38.9 51.9 29.6 45.3
14.2 39.1 52.2 29.6 46.3
14.2 39.1 52.2 29.6 46.3
14.3 39.2 52.5 29.7 46.4
14.3 39.2 52.5 29.7 46.4

Cluster-NMS
Cluster-NMSW
Cluster-NMSW +D

46.8
47.0
48.5
48.5
48.7
48.8

TABLE IV
QUANTITATIVE COMPARISON OF YOLOV3 [9] TRAINED USING
DIFFERENT LOSSES. THE RESULTS ARE REPORTED ON THE TEST SET OF
PASCAL VOC 2007.

Loss / Evaluation

MSE
LIoU
LGIoU
LDIoU
LCIoU

AP

AP75

IoU
46.1
46.6
47.7
48.1
49.2

GIoU
45.1
45.8
46.9
47.4
48.4

IoU
48.6
49.8
52.2
52.8
54.3

GIoU
46.7
48.8
51.1
51.9
52.9

achieves much better performance than the original paper of
YOLO v3 [9] on MS COCO 2014 validation set. Table V re-
ports the comparison of different NMS methods. We note that
original NMS (TorchVision) is the most efﬁcient due to CUDA
implementation and engineering acceleration in TorchVision.
Our Cluster-NMS is only a little slower than original NMS
(TorchVision). By merging coordinates based on overlap areas
and scores, Weighted-NMS achieves higher AP and AR than
original NMS, but it dramatically lowers the inference speed,
making it infeasible for real-time application. Our Cluster-
NMSW can guarantee the same results with Weighted-NMS,
but is much more efﬁcient. Our Cluster-NMSW +D contributes
to further improvements than Cluster-NMSW , especially in
terms of AR. Actually, Cluster-NMS can be further accelerated

by logic operations on binarized IoU matrix, but it makes
infeasible for incorporating other geometric factors. Also our
Cluster-NMS with geometric factors can still guarantee real-
time inference.

2) SSD [10]: We use another popular one-stage method
SSD to further conduct evaluation experiments. The latest
PyTorch implementation of SSD6 is adopted. Both the training
set and testing set share the same setting with YOLO v3 on
PASCAL VOC. The backbone network is ResNet-50-FPN.
And then we train the models using IoU, GIoU, DIoU and
CIoU losses. Table VI gives the quantitative comparison, in
which AP metrics based on IoU and evaluation of NMS
methods are reported. As for loss function, we can see the
consistent
improvements of CIoU loss against IoU, GIoU
and DIoU losses. As for NMS, Cluster-NMSW +D leads to
signiﬁcant improvements in AP metrics, and its efﬁciency is
still well maintained.

3) Faster R-CNN [15]: We also evaluate CIoU loss for
a two-stage object detection method Faster R-CNN7 on MS
COCO 2017 [54]. Following the same training protocol of
[36], we have trained the models using CIoU loss in compari-
son with IoU, GIoU and DIoU losses. The backbone network
is ResNet-50-FPN. Table VII reports the quantitative compar-

6https://github.com/JaryHuang/awesome SSD FPN GIoU
7https://github.com/generalized-iou/Detectron.pytorch

IEEE TRANSACTIONS ON CYBERNETICS, VOL. XX, NO. XX, MONTH YEAR

10

Fig. 7. Detection and segmentation results of YOLACT [20] on MS COCO 2017.

TABLE V
COMPARISON OF DIFFERENT NMS METHODS ON PRE-TRAINED PYTORCH-YOLO V3 MODEL. THE RESULTS ARE REPORTED ON MS COCO 2017
VALIDATION SET.

Method

YOLO v3

NMS Strategy
Fast NMS
Original NMS
Original NMS (TorchVision)
Weighted-NMS
Cluster-NMS
Cluster-NMSD
Cluster-NMSW
Cluster-NMSW +D

FPS Time
71.9 13.9
9.6 103.9
69.0 14.5
6.2 162.3
65.4 15.3
60.6 16.5
57.1 15.8
53.5 17.1

AP AP50 AP75 APS APM APL AR1 AR10 AR100 ARS ARM ARL
73.4
42.7 63.0
75.8
43.2 63.2
75.8
43.2 63.2
43.6 63.4
75.5
75.8
43.2 63.2
76.7
43.3 63.0
76.0
43.6 63.3
77.1
43.8 63.0

27.9 47.8
28.3 48.4
28.3 48.4
29.0 48.9
28.3 48.4
28.4 48.6
29.0 48.9
29.0 49.0

53.2
53.7
53.7
53.7
53.7
53.8
53.7
54.0

45.8
46.5
46.5
47.4
46.5
47.1
47.5
47.9

56.4
57.7
57.7
58.1
57.7
58.1
58.4
58.8

44.7
46.9
46.9
47.6
46.9
47.6
48.0
48.7

60.1
62.7
62.7
63.0
62.7
63.6
63.8
64.6

64.8
67.8
67.8
68.3
67.8
68.7
69.1
69.8

34.7
34.7
34.7
34.9
34.7
34.7
34.9
34.9

ison. The gains of CIoU loss in AP are not as signiﬁcant as
those in YOLO v3 and SSD. It is actually reasonable that
the initial boxes ﬁltrated by RPN are likely to have overlaps
with ground-truth box, and then all DIoU, GIoU and CIoU

losses can make good regression. But due to the complete
geometric factors in CIoU loss, the detected bounding boxes
will be matched more perfectly, resulting in moderate gains.
As for suppressing redundant boxes, Cluster-NMSW +D is

IEEE TRANSACTIONS ON CYBERNETICS, VOL. XX, NO. XX, MONTH YEAR

11

NMS against original NMS.

C. Discussion

As for CIoU loss, we have veriﬁed the effectiveness of three
geometric factors when training deep models of object detec-
tion and instance segmentation. One may notice that CIoU
loss yields lower metrics for small objects by Faster-RCNN
(Table VII), small&median objects by YOLACT (Table I) and
small objects by BlendMask-RT (Table III). The reason may be
attributed that for small or median objects, the central point
distance is more important than aspect ratio for regression,
and the aspect ratio may weaken the effect of normalized
distance between the two boxes. For large objects, both central
point distance and aspect ratio are crucial for bounding box
regression, thus resulting in consistent improvements for all
these models. For these models, the average precision and
recall on all the objects have validated the superiority of our
CIoU loss, although there is some leeway for studying aspect
ratio to remedy possible adverse effects for small or median
objects, which can be left in future work.

As for Cluster-NMS, from Tables I, III and V, one can
see that these state-of-the-art deep models of object detec-
tion and instance segmentation can achieve the highest AP
and AR metrics by collaborating with our Cluster-NMS, but
they may need different Cluster-NMS versions, e.g., Cluster-
NMSW +D for YOLO v3 and BlendMask-RT, and Cluster-
NMSS+D for YOLACT. For a given deep model of object
detection or instance segmentation, their detected boxes may
have speciﬁc property and distribution, thus requiring different
NMS methods to obtain the best performance. However, this
is not the drawback of our Cluster-NMS, since Cluster-NMS
is a general container to accommodate various NMS meth-
ods, while maintaining high inference efﬁciency. Meanwhile,
the central point distance in Cluster-NMSS+D and Cluster-
NMSW +D plays a crucial role, verifying the effectiveness of
geometric factors during inference.

VI. CONCLUSION

In this paper, we proposed to enhance geometric factors into
CIoU loss and Cluster-NMS for object detection and instance
segmentation. By simultaneously considering the three geo-
metric factors, CIoU loss is better for measuring bounding box
regression when training deep models of object detection and
instance segmentation. Cluster-NMS is purely implemented
on GPU by implicitly clustering detected boxes, and is much
more efﬁcient than original NMS. Geometric factors can then
be easily incorporated into Cluster-NMS, resulting in no-
table improvements in precision and recall, while maintaining
high inference efﬁciency. CIoU loss and Cluster-NMS have
been applied to the training and inference of state-of-the-
art deep object detection and instance segmentation models.
Comprehensive experiments have validated that the proposed
methods contribute to consistent improvements of AP and
AR, and the high efﬁciency of Cluster-NMS can guarantee
the real-time inference. CIoU loss and Cluster-NMS can be
widely extended to other deep models for object detection and
instance segmentation.

(a) Average Precision with different β values

(b) Average Recall with different β values

Fig. 8. Balancing precision and recall by different values of β in Cluster-
NMSS+D. The results are from YOLACT model on MS COCO 2017
validation set.

TABLE VI
QUANTITATIVE COMPARISON OF SSD [10] FOR EVALUATING DIFFERENT
LOSS FUNCTIONS AND NMS METHODS. THE RESULTS ARE REPORTED ON
THE TEST SET OF PASCAL VOC 2007.

Loss / Evaluation
LIoU
LGIoU
LDIoU
LCIoU

AP
51.0
51.1
51.3
51.5

AP75
54.7
55.4
55.7
56.4

(a) Comparison of different loss functions.

NMS Strategy
Fast NMS
Original NMS
Cluster-NMS
Cluster-NMSW
Cluster-NMSW +D

FPS
28.8
17.8
28.0
26.8
26.5

Time
34.7
56.1
35.7
37.3
37.8

AP
50.7
51.5
51.5
51.9
52.4

AP75
56.2
56.4
56.4
56.3
57.0

(b) Comparison of NMS methods on the model trained by LCIoU .

TABLE VII
QUANTITATIVE COMPARISON OF FASTER R-CNN [15] TRAINED USING
LIoU (BASELINE), LGIoU , LDIoU AND LCIoU . CLUSTER-NMSW +D IS
APPLIED ON THE MODEL TRAINED USING LCIoU , WHILE THE OTHER
RESULTS ARE PRODUCED BY ORIGINAL NMS. THE RESULTS ARE
REPORTED ON THE VALIDATION SET OF MS COCO 2017.

Loss / Evaluation

LIoU
LGIoU
LDIoU
LCIoU
Cluster-NMSW +D

AP

37.9
38.0
38.1
38.7
39.0

AP75

40.8
41.1
41.1
42.0
42.3

APS

21.6
21.5
21.7
21.3
21.7

APM

40.8
41.1
41.2
41.9
42.2

APL

50.1
50.2
50.3
51.5
52.1

applied on the model trained by CIoU loss, and it brings
further improvements for all the evaluation metrics, validating
the effectiveness of assembling geometric factors into Cluster-

IEEE TRANSACTIONS ON CYBERNETICS, VOL. XX, NO. XX, MONTH YEAR

12

REFERENCES

[1] X. Wang, M. Wang, and W. Li, “Scene-speciﬁc pedestrian detection for
static video surveillance,” IEEE Transactions on Pattern Analysis and
Machine Intelligence, vol. 36, no. 2, pp. 361–374, 2014. 1

[2] P. Voigtlaender, J. Luiten, P. H. Torr, and B. Leibe, “Siam r-cnn: Visual
tracking by re-detection,” in The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), 2020. 1

[3] J. Marłn, D. Vzquez, A. M. Lpez, J. Amores, and L. I. Kuncheva, “Oc-
clusion handling via random subspace classiﬁers for human detection,”
IEEE Transactions on Cybernetics, vol. 44, no. 3, pp. 342–354, 2014.
1

[4] W. Wu, Y. Yin, X. Wang, and D. Xu, “Face detection with different
scales based on faster r-cnn,” IEEE Transactions on Cybernetics, vol. 49,
no. 11, pp. 4017–4028, 2019. 1

[5] B. Xue and N. Tong, “Diod: Fast and efﬁcient weakly semi-supervised
deep complex isar object detection,” IEEE Transactions on Cybernetics,
vol. 49, no. 11, pp. 3991–4003, 2019. 1

[6] J. Han, D. Zhang, G. Cheng, N. Liu, and D. Xu, “Advanced deep-
learning techniques for salient and category-speciﬁc object detection: A
survey,” IEEE Signal Processing Magazine, vol. 35, no. 1, pp. 84–100,
2018. 1

[7] P. F. Felzenszwalb, R. B. Girshick, D. McAllester, and D. Ramanan,
“Object detection with discriminatively trained part-based models,”
IEEE Transactions on Pattern Analysis and Machine Intelligence,
vol. 32, no. 9, pp. 1627–1645, 2009. 1, 3

[8] J. Redmon and A. Farhadi, “Yolo9000: Better, faster, stronger,” in The
IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
2017, pp. 6517–6525. 1, 2, 3

[9] J. Redmon and F. Ali, “YOLOv3: An Incremental Improvement,”

arXiv:1804.02767, 2018. 1, 2, 3, 5, 8, 9

[10] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y. Fu, and A. C.
Berg, “Ssd: Single shot multibox detector,” in The European Conference
on Computer Vision (ECCV). Springer, 2016, pp. 21–37. 1, 2, 3, 5, 8,
9, 11

[11] C.-Y. Fu, W. Liu, A. Ranga, A. Tyagi, and A. C. Berg, “DSSD:

Deconvolutional single shot detector,” arXiv:1701.06659, 2017. 1, 3

[12] T. Lin, P. Goyal, R. Girshick, K. He, and P. Dollr, “Focal loss for dense
object detection,” IEEE Transactions on Pattern Analysis and Machine
Intelligence, vol. 42, no. 2, pp. 318–327, 2020. 1, 2, 3

[13] Z. Tian, C. Shen, H. Chen, and T. He, “FCOS: Fully convolutional
one-stage object detection,” in The IEEE International Conference on
Computer Vision (ICCV), 2019, pp. 9626–9635. 1, 3

[14] R. Girshick, “Fast r-cnn,” in The IEEE International Conference on

Computer Vision (ICCV), 2015, pp. 1440–1448. 1, 2

[15] S. Ren, K. He, R. Girshick, and J. Sun, “Faster r-cnn: Towards real-time
object detection with region proposal networks,” IEEE Transactions on
Pattern Analysis and Machine Intelligence, vol. 39, no. 6, pp. 1137–
1149, 2017. 1, 2, 3, 5, 6, 8, 9, 11

[16] S. Gidaris and N. Komodakis, “Object detection via a multi-region and
semantic segmentation-aware cnn model,” in The IEEE International
Conference on Computer Vision (ICCV), 2015, pp. 1134–1142. 1
[17] Z. Cai and N. Vasconcelos, “Cascade r-cnn: Delving into high quality
object detection,” in The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), 2018, pp. 6154–6162. 1, 3

[18] X. Chen, R. Girshick, K. He, and P. Doll´ar, “Tensormask: A foundation
for dense object segmentation,” in The IEEE International Conference
on Computer Vision (ICCV), 2019, pp. 2061–2069. 1, 2

[19] E. Xie, P. Sun, X. Song, W. Wang, X. Liu, D. Liang, C. Shen,
and P. Luo, “Polarmask: Single shot instance segmentation with polar
representation,” arXiv preprint arXiv:1909.13226, 2019. 1, 3

[20] D. Bolya, C. Zhou, F. Xiao, and Y. J. Lee, “Yolact: real-time instance
segmentation,” in The IEEE International Conference on Computer
Vision (ICCV), 2019, pp. 9157–9166. 1, 2, 3, 5, 6, 7, 8, 9, 10

[21] K. He, G. Gkioxari, P. Dollr, and R. Girshick, “Mask r-cnn,” IEEE
Transactions on Pattern Analysis and Machine Intelligence, vol. 42,
no. 2, pp. 386–397, 2020. 1, 3

[22] P. J. Huber et al., “Robust estimation of a location parameter,” The
Annals of Mathematical Statistics, vol. 35, no. 1, pp. 73–101, 1964. 2
[23] S.-H. Bae, “Object detection based on region decomposition and assem-
bly,” in The AAAI Conference on Artiﬁcial Intelligence, vol. 33, 2019,
pp. 8094–8101. 2

[24] G. Cheng, J. Han, P. Zhou, and D. Xu, “Learning rotation-invariant and
ﬁsher discriminative convolutional neural networks for object detection,”
IEEE Transactions on Image Processing, vol. 28, no. 1, pp. 265–278,
2019. 2

[25] G. Brazil, X. Yin, and X. Liu, “Illuminating pedestrians via simultaneous
detection & segmentation,” in The IEEE International Conference on
Computer Vision (ICCV), 2017, pp. 4960–4969. 2

[26] C. Zhou, M. Wu, and S.-K. Lam, “Ssa-cnn: Semantic self-attention cnn
for pedestrian detection,” arXiv preprint arXiv:1902.09080, 2019. 2
[27] J. Ma, W. Shao, H. Ye, L. Wang, H. Wang, Y. Zheng, and X. Xue,
“Arbitrary-oriented scene text detection via rotation proposals,” IEEE
Transactions on Multimedia, vol. 20, no. 11, pp. 3111–3122, 2018. 2,
3

[28] M. Liao, Z. Zhu, B. Shi, G.-s. Xia, and X. Bai, “Rotation-sensitive
regression for oriented scene text detection,” in The IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), 2018, pp. 5909–
5918. 2

[29] S. Qin, A. Bissacco, M. Raptis, Y. Fujii, and Y. Xiao, “Towards
unconstrained end-to-end text spotting,” in The IEEE International
Conference on Computer Vision (ICCV), 2019, pp. 4704–4714. 2
[30] Y. Zhou and O. Tuzel, “Voxelnet: End-to-end learning for point cloud
based 3d object detection,” in The IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), 2018, pp. 4490–4499. 2

[31] S. Shi, X. Wang, and H. Li, “Pointrcnn: 3d object proposal generation
and detection from point cloud,” in The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), 2019, pp. 770–779. 2

[32] K. Sun, B. Xiao, D. Liu, and J. Wang, “Deep high-resolution repre-
sentation learning for human pose estimation,” in The IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), 2019, pp. 5693–
5703. 2

[33] K. Iskakov, E. Burkov, V. Lempitsky, and Y. Malkov, “Learnable
triangulation of human pose,” in The IEEE International Conference
on Computer Vision (ICCV), 2019, pp. 7718–7727. 2

[34] J. Yu, Y. Jiang, Z. Wang, Z. Cao, and T. Huang, “Unitbox: An advanced
object detection network,” in Proceedings of the ACM International
Conference on Multimedia, 2016, pp. 516–520. 2, 3

[35] L. Tychsen-Smith and L. Petersson, “Improving object localization with
ﬁtness nms and bounded iou loss,” in The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), 2018, pp. 6877–6885. 2, 3
[36] H. Rezatoﬁghi, N. Tsoi, J. Gwak, A. Sadeghian, I. Reid, and S. Savarese,
“Generalized intersection over union: A metric and a loss for bounding
box regression,” in The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), 2019, pp. 658–666. 2, 3, 8, 9

[37] R. Girshick, J. Donahue, T. Darrell, and J. Malik, “Rich feature
hierarchies for accurate object detection and semantic segmentation,”
in The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), 2014, pp. 580–587. 2

[38] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, “You only look
once: Uniﬁed, real-time object detection,” in The IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), 2016, pp. 779–788.
2, 3

[39] N. Bodla, B. Singh, R. Chellappa, and L. S. Davis, “Soft-nms ł improv-
ing object detection with one line of code,” in The IEEE International
Conference on Computer Vision (ICCV), 2017, pp. 5562–5570. 2, 3, 5,
6

[40] H. Zhou, Z. Li, C. Ning, and J. Tang, “Cad: Scale invariant framework
for real-time object detection,” in The IEEE International Conference
on Computer Vision (ICCV Workshop), 10 2017, pp. 760–768. 2, 3, 5,
6

[41] D. Oro, C. Fern´andez, X. Martorell, and J. Hernando, “Work-efﬁcient
parallel non-maximum suppression for embedded gpu architectures,” in
The IEEE International Conference on Acoustics, Speech and Signal
Processing (ICASSP), 2016, pp. 1026–1030. 2, 3

[42] H. Chen, K. Sun, Z. Tian, C. Shen, Y. Huang, and Y. Yan, “BlendMask:
Top-down meets bottom-up for instance segmentation,” in The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), 2020.
2, 8, 9

[43] Z. Zheng, P. Wang, W. Liu, J. Li, R. Ye, and D. Ren, “Distance-IoU
Loss: Faster and better learning for bounding box regression,” in The
AAAI Conference on Artiﬁcial Intelligence, 2020. 2, 5, 6, 7, 8

[44] P. Zhou, B. Ni, C. Geng, J. Hu, and Y. Xu, “Scale-transferrable object
detection,” in The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), 2018, pp. 528–537. 3

[45] Z. Yang, S. Liu, H. Hu, L. Wang, and S. Lin, “Reppoints: Point
set representation for object detection,” in The IEEE International
Conference on Computer Vision (ICCV), 2019, pp. 9656–9665. 3
[46] Y. Jiang, X. Zhu, X. Wang, S. Yang, W. Li, H. Wang, P. Fu, and
Z. Luo, “R2cnn: Rotational region cnn for orientation robust scene text
detection,” arXiv preprint arXiv:1706.09579, 2017. 3

[47] G. P. Meyer, “An alternative probabilistic interpretation of the huber

loss,” arXiv preprint arXiv:1911.02088, 2019. 3

IEEE TRANSACTIONS ON CYBERNETICS, VOL. XX, NO. XX, MONTH YEAR

13

[48] J. Pang, K. Chen, J. Shi, H. Feng, W. Ouyang, and D. Lin, “Libra r-cnn:
Towards balanced learning for object detection,” in The IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), 2019, pp. 821–
830. 3

[49] B. Li, Y. Liu, and X. Wang, “Gradient harmonized single-stage detector,”
in The AAAI Conference on Artiﬁcial Intelligence, vol. 33, 2019, pp.
8577–8584. 3

[50] B. Jiang, R. Luo, J. Mao, T. Xiao, and Y. Jiang, “Acquisition of
localization conﬁdence for accurate object detection,” in The European
Conference on Computer Vision (ECCV), 2018, pp. 784–799. 3
[51] S. Liu, D. Huang, and Y. Wang, “Adaptive nms: Reﬁning pedestrian
detection in a crowd,” in The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), 2019, pp. 6452–6461. 3

[52] Y. He, C. Zhu, J. Wang, M. Savvides, and X. Zhang, “Bounding box
regression with uncertainty for accurate object detection,” in The IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), 2019,
pp. 2883–2892. 3

[53] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisser-
man, “The pascal visual object classes (voc) challenge,” International
Journal of Computer Vision, vol. 88, no. 2, pp. 303–338, 2010. 8
[54] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan,
P. Doll´ar, and C. L. Zitnick, “Microsoft coco: Common objects in
context,” in The European Conference on Computer Vision (ECCV).
Springer, 2014, pp. 740–755. 8, 9

Zhaohui Zheng received the M.S. degree in com-
putational mathematics from Tianjin University in
2021. He is currently a Ph.D. candidate with college
of computer science at Nankai University, Tianjin,
China. His research interests include object detec-
tion, instance segmentation and scene text detection.

Ping Wang received the B.S., M.S., and Ph.D.
degrees in computer science from Tianjin University,
Tianjin, China, in 1988, 1991, and 1998, respec-
tively. She is currently a Professor with the School of
Mathematics, Tianjin University. Her research inter-
ests include image processing and machine learning.

Dongwei Ren received two Ph.D. degrees in com-
puter application technology from Harbin Institute
of Technology and The Hong Kong Polytechnic
University in 2017 and 2018, respectively. From
2018 to 2021, he was an Assistant Professor with
the College of Intelligence and Computing, Tianjin
University. He is currently an Associate Professor
with the School of Computer Science and Technol-
ogy, Harbin Institute of Technology. His research
interests include computer vision and deep learning.

Wei Liu received the B.S and M.S. degrees in com-
puter science from the School of Computer Science
and Technology, Tianjin University, China, in 2017
and 2020. He is now working at NR Electric Co.,
Ltd, Nanjing, China. His research interests include
multimodal computing and computer vision.

Rongguang Ye received the B.S degree from the
School of Mathematics, Tianjin University, Tianjin,
China, in 2019. He is now pursuing a M.S degree
of computational mathematics in Tianjin University.
His research interests include object detection and
computer vision.

Qinghua Hu received the B.S., M.S., and Ph.D.
degrees from the Harbin Institute of Technology,
Harbin, China, in 1999, 2002, and 2008, respec-
tively. He was a Post-Doctoral Fellow with the
Department of Computing, Hong Kong Polytechnic
University, from 2009 to 2011. He is currently the
Dean of the School of Artiﬁcial Intelligence, the
Vice Chairman of the Tianjin Branch of China
Computer Federation, the Vice Director of the SIG
Granular Computing and Knowledge Discovery, and
the Chinese Association of Artiﬁcial Intelligence.
He is currently supported by the Key Program, National Natural Science
Foundation of China. He has published over 200 peer-reviewed papers. His
current research is focused on uncertainty modeling in big data, machine
learning with multi-modality data, intelligent unmanned systems. He is an
Associate Editor of the IEEE TRANSACTIONS ON FUZZY SYSTEMS,
Acta Automatica Sinica, and Energies.

Wangmeng Zuo (M’09-SM’14) received the Ph.D.
degree in computer application technology from the
Harbin Institute of Technology, Harbin, China, in
2007. He is currently a Professor in the School
of Computer Science and Technology, Harbin In-
stitute of Technology. His current research interests
include image enhancement and restoration, image
and face editing, object detection, visual tracking,
and image classiﬁcation. He has published over 100
papers in top tier academic journals and conferences.
According to the statistics by Google scholar, his
publications have been cited more than 20,000 times in literature. He has
served as an Associate Editor of the IEEE Transactions on Pattern Analysis
and Machine Intelligence and IEEE Transactions on Image Processing.

