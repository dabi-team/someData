MusicID: A Brainwave-based User Authentication
System for Internet of Things

Jinani Sooriyaarachchi, Suranga Seneviratne, Kanchana Thilakarathna, and Albert Y. Zomaya

1

0
2
0
2

n
u
J

2

]

R
C
.
s
c
[

1
v
1
5
7
1
0
.
6
0
0
2
:
v
i
X
r
a

Abstract—We propose MusicID, an authentication solution for
smart devices that uses music-induced brainwave patterns as
a behavioral biometric modality. We experimentally evaluate
MusicID using data collected from real users whilst they are
listening to two forms of music; a popular English song and
individual’s favorite song. We show that an accuracy over 98%
for user identiﬁcation and an accuracy over 97% for user
veriﬁcation can be achieved by using data collected from a 4-
electrode commodity brainwave headset. We further show that a
single electrode is able to provide an accuracy of approximately
85% and the use of two electrodes provides an accuracy of
approximately 95%. As already shown by commodity brain-
sensing headsets for meditation applications, we believe including
dry EEG electrodes in smart-headsets is feasible and MusicID
has the potential of providing an entry point and continuous
authentication framework for upcoming surge of smart-devices
mainly driven by Augmented Reality (AR)/Virtual Reality (VR)
applications.

Index Terms—Behavioural Biometrics, Smart Sensing, EEG,

Authentication, IoT

I. INTRODUCTION

S MART head-mounted devices of various kinds such as

smart glasses, mixed reality headsets, and smart earbuds,
are becoming increasingly popular in engaging and control-
ling smart Internet of Things (IoT) environments. This is in
addition to the commonly used Bluetooth headsets which are
the most sold wearable globally by a signiﬁcant margin [1].
Moreover,
the rapid growth of mixed reality in multiple
application domains such as gaming, education, and health
will also ensure that the usage of smart-headsets, especially
the ones that are capable of standalone operation will continue
to increase. Secure and robust user authentication on such
headsets is of paramount importance due to unprecedented
sensitivity of information that is sensed, stored, and transmit-
ted by these devices such as health-care data, ﬁnancial records,
user location, and surrounding environmental information. In
industrial and health-care settings, it is necessary to make sure
that these devices are worn by the authorized personnel only.
Also, in personal smart-home settings, authentication of the
user enables personalized service delivery.

Existing methods of authentication such as passwords and
PINs are not observation resistant and less secure due to
reuse. Attempts to increase the complexity of passwords by
adding more constraints on character, numeral, and symbol
combinations, reduce the usability. Other alternative of static

J. Sooriyaarachchi was with Data61-CSIRO, Australia.
S. Seneviratne, K. Thilakarathna and A. Y. Zomaya are with School of

Computer Science, The University Sydney, Australia.

biometrics such as ﬁngerprints and face IDs are prone to replay
attacks and spooﬁng [2]. As a result, behavioral biometrics are
emerging as an exciting new alternative. Behavioral biometrics
focus on behavior modalities that are potentially unique to in-
dividuals in the likes of typing patterns [3], touch patterns [4],
gait [5], and heart rate [6]. Behavioral biometrics are more
secure as they are difﬁcult to mimic, record, and synthesize and
can readily be used for implicit authentication [7]. The main
challenge in behavioral biometrics is ﬁnding modalities that
can be captured non-intrusively and are sufﬁciently consistent
over time so that they can be used for user re-identiﬁcation.
Incorporating behavioral biometrics to standalone Internet of
Things devices such as smart-headsets is further challenging
due to their limited user interaction capabilities.

In this paper, we propose MusicID, a novel behavioral bio-
metric for user authentication for Internet of Things environ-
ments, which measures the emotional behavior and reactions
of the user to music through EEG (Electroencephalogram)
signals. Recent advances in head-mounted wearables made
non-invasive EEG capture possible through soft dry electrodes
as demonstrated by commodity headsets such as Muse [8]
and Neurosky [9]. These sensors can be easily integrated with
smart-headsets that are increasingly becoming popular in both
smart-home and industrial Internet of Things. The possibility
of using brainwave patterns for authentication makes inroads
towards a unique and hard to spoof behavioral biometric
modality that can be useful not only for smart-headsets such
as Microsoft Holo Lens and Oculus Rift, but also in general
as a universal authentication solution.

We make following contributions in this paper.

• We show that the music stimulated EEG patterns are
potentially unique to individuals by collecting EEG data
from 20 users over one and half months using Muse brain
sensing headband. We measure brain reactions to each
individuals’ favorite song as well as a reference song
and show that our observations are statistically signiﬁcant
through an ANOVA test (p < 0.0001).

• We develop Random Forest classiﬁers for both user
identiﬁcation and user veriﬁcation tasks using features
extracted from decomposed EEG signals and show that
over 97% accuracy can be achieved.

• We investigate the trade-off between accuracy and the
number of electrodes and frequency bands. Our results
show that use of two front electrodes gives an accuracy
of 94%. Also, we show that all four brainwave frequency
bands are necessary to have a signiﬁcant accuracy.

The rest of the paper is organized as follows. In Section

This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be
accessible

 
 
 
 
 
 
TABLE I: Summary of different brainwaves

Type
Delta
Theta

Frequency
0.5-4.0 Hz
4.0-7.5 Hz

Alpha

7.5-12.0 Hz

Beta
Gamma

12.0-30.0 Hz
30.0-100.0 Hz

Activities
Deep sleep
Meditation, Light sleep, Vivid visualizations,
Creativity, Emotional connection, Relaxation
Deep relaxation with eyes closed, State of con-
centration, Memorizing, Learning, Visualization
Normal waking conditions, Daily consciousness
High level information processing in improved
memory conditions, Sudden insights

2

Fig. 2: Methodology

bands playing a major role in music processing [11],Gamma
brainwaves conﬁned to subjects with music training [12], and
high Theta brainwave power in frontal midline in contrast to
pleasant and unpleasant music [13]. Motivated by these back-
ground information, in this paper, we explore the feasibility
of using music-induced brainwaves as a behavioral biometric
modality.

III. METHODOLOGY

In this section we describe our experiment design and
data collection process followed by data pre-processing steps,
features used, and classiﬁer details. The overall summary of
our methodology is illustrated in Figure 2.

A. Data Collection

We recruited 20 volunteers (11 females and 9 males, aged
between 20 and 40) to participate in our experiment. The
participants were students and research staff associated with
Data61, CSIRO. The duration of the experiment spanned over
two months and there was a minimum one week gap between
two sessions of a single participant. Each user participated
in at least two sessions. Our experiment was approved by the
CSIRO Human Research Ethics Committee (Ref. No. 104/17).
During each session the participants performed two tasks;
i) listening to a popular English song1 and ii) listening
to individual’s favorite song whilst wearing Muse [8] brain
sensing headset. Muse headset is equipped with 4 electrodes
in the standard 4-channel conﬁguration (TP9, AF7, AF8, TP10
as shown in Figure 1). The duration of the each experiment
was 150 seconds and the participants closed their eyes during
this experiment. This was done due to two main reasons; i)
avoid distractions and head movements that may affect the
measurements, and ii) eye blinks are known to impact the EEG
readings [14] as a result of the large potentials evoked above
and below the eye during eye blinks and eye movements.

In the Muse Monitor application [15] running in a Google
Pixel phone, we selected a sampling rate of 220 Hz and
a recording interval of 0.5 seconds. The recording interval
deﬁnes when the data is recorded to the output .csv ﬁle. Thus,
at the end of each session with this recording interval, 300 data
samples were collected for the task of listening to the reference
song and another 300 data samples for the task of listening
to favorite songs. Each sample included 24 readings; absolute

1https://www.youtube.com/watch?v=JRfuAukYTKg

Fig. 1: Standard 10-20 system

II, we provide background information on EEG and brain
behaviors. Section III describes our methodology and Section
IV presents results. We discuss related work in Section V
followed by a discussion of implications,
limitations, and
future work in Section VI. Section VII concludes the paper.

II. BACKGROUND

EEG (Electroencephalogram) measures the electrical activ-
ities in human brain. EEG data is captured by measuring
voltage difference between two electrodes placed on the scalp.
More speciﬁcally EEG data consist of inhibitory and excitatory
post-synaptic potentials generated in pyramidal cells of the
brain cortex depending on the mental activity. Whenever the
brain receives stimuli from our senses such as visual, audio,
touch, pain, taste, and smell, EEG signals vary in magnitude
as well as in frequency. EEG brain signals can be decomposed
into ﬁve based on their frequencies; Alpha, Beta, Theta, Delta,
and Gamma. Each of these frequency bands has its own
characteristics depending on the brain activity and the state
of consciousness. Table I summarizes the frequencies of each
brainwave band and activities that generate them.

Brain structure of every human being is unique and highly
inﬂuenced by DNA [10]. Also, how a person’s brain responds
to a stimulus is highly dependent on that person’s previous
experiences. As such, brainwave patterns generated for some
activities can potentially be used as an authentication modality
as they can be unique to individuals and difﬁcult to spoof.

Brainwaves can be measured using invasive techniques in
which electrodes are placed under the scalp or non-invasive
techniques in which electrodes are placed above the scalp.
Scalp electrodes can detect faint electrical signals on the
surface of the head that represent the underlying neural ac-
tivity. Figure 1 shows the standard 10-20 electrode locations
commonly used to place electrodes on the scalp.

Various studies found changes in brainwaves induced by the
emotions and aesthetic pleasure of music. These include Beta

AF7AF8TP9TP10Fp1FpzFp23x	Reference	Sensors2x	Forehead	Sensors2x	Back-of-the	Ear	SensorsFp1FpzFp2AF7AF8TP9TP10NasionInionData	CollectionFavorite	SongSame	Song20 UsersPre-ProcessingFramingFeature	ExtractionClassificationUser	VerificationOne	vs	rest	classificationUser	IdentificationMulti-class	classificationIdentified	User	IDUser IDBrainwavesVerified	User	IDRejectedVerification	ModelIdentification	ModelModel	Development1Evaluation2Brainwaves +brainwave values of Alpha, Beta, Theta, Delta, Gamma, and
raw EEG (without separating into the sub bands) from each of
4-channels. Figure 3 shows the setup of the experiment and in
Figure 4 we show example brainwave data streams of a user
collected from AF7 electrode for two experiment scenarios.

3

Fig. 3: Experimental setup

(a) Listening to Favorite Song

Fig. 5: Framing 300 samples to 14 frames

it performed better compared to Support Vector Machines and
Logistic Regression, which is evaluated in Section IV-B.

For user identiﬁcation, we used Random Forest classiﬁers
in a multi-class setting. For user veriﬁcation we used Random
Forest classiﬁers in One-vs-Rest conﬁguration where a clas-
siﬁer is built on per user basis that predicts whether a given
frame belongs a particular user or not. To avoid over-ﬁtting
we performed 10-fold cross validation. We used 80% of the
samples from each session for training and the rest of the
20% for testing. We avoided two consecutive feature vectors
going into training and test sets since there is a 50% overlap
and allowing it would cause information transfer from the
training set to the test set, artiﬁcially increasing the classiﬁer
performance. Table II shows the number of samples for each
user in training and test sets. This training and test dataset
sizes are identical for both experiment scenarios.

(b) Listening to Same Song
Fig. 4: Example brainwave data streams from AF7

TABLE II: Summary of training and test datasets
Test set size
Users
15
User 1-5
12
User 6
9
User 7-11
6
User 12-20
186
Total

Training set size
55
44
33
22
682

No. of sessions
5
4
3
2

B. Data pre-processing and features

Since Delta brainwave frequencies are related to deep sleep,
and not signiﬁcant in our experimental tasks, we removed 4
absolute Delta brainwave readings from 4-channels and used
the remaining 20 readings.

We framed 300 samples from each session into 14 overlap-
ping frames of 50% overlap as shown in Figure 5. For each
frame we had 20 readings for Alpha, Beta, Theta, Gamma,
and raw EEG values coming from each of the four electrodes.
For each reading we calculated the mean, maximum, minimum,
and zero crossing rate (ZCR) giving us 80 features for each
frame. Thus, one listening task in a session gave us 14 samples
of data that are of size 1 × 80.

C. Classiﬁer

We built Random Forest classiﬁers for two authentication
settings; user identiﬁcation and user veriﬁcation. The choice
of Random Forest is empirically decided based on the fact that

IV. RESULTS

First, we present an analysis of our features using hypothesis
testing followed by the classiﬁer performance for user iden-
tiﬁcation and veriﬁcation tasks. Then, we further analyze the
data to identify the most important features that differentiate
users. Next, we provide results to show how the placement
of electrodes affect the accuracy and whether we can use
only a certain type of brainwaves for authentication. Finally,
we do cross song training and testing to check whether the
user responds to music in general or there are differences in
responses when listening to favorite music and other music.

A. Statistical hypothesis testing

To check whether the features we extracted from brainwaves
are speciﬁc to individual users, we conducted a statistical
hypothesis testing under the null hypothesis; “features ex-
tracted from brainwaves do not have a relationship to the
corresponding user” using ANOVA (Analysis of Variance).
The resulting F − test value was 22.462 which corresponds

FpzTP9TP10AF8AF7050100150200250300−1−0.500.511.52SamplesAbsolute Power (dB)  ThetaAlphaBetaGamma050100150200250300−1−0.500.511.52SamplesAbsolute Power (dB)  ThetaAlphaBetaGammaFrame3Frame20												20										40												60												80											120																									160																									200																									240											260										280										300																																																																			Frame13Frame141 x 80 Feature Vector 4x SensorsAF7AF8TP9TP10ThetaAlphaBetaGammaRaw	EEG5x Waveform types4x FeaturesMinMaxMeanZCRto a p value less than 0.0001, indicating that null hypothesis
can be omitted and the features indeed have a statistical
relationship to the individual users.

B. Classiﬁer performance

i) User identiﬁcation: In user identiﬁcation, given a brainwave
sample the classiﬁer predicts whose sample is that from a
closed set of users. We trained Random Forest classiﬁers at
different maximum depth levels using 10-fold cross validation
for the two experiment scenarios. That is, we used the training
dataset from listening to the same song and tested the models
on the test set built using the same experiment scenario.
Table III shows the performance of the classiﬁers.

TABLE III: User identiﬁcation: accuracy on test set

Maximum
Tree Depth
5
10
15

to

Listening
same song
86.56%
98.39%
98.39%

to

Listening
favorite song
86.02%
98.39%
99.46%

We obtained an accuracy of 98.39% for listening to the
same song and an accuracy of 99.46% for listening to the
favorite song with Random Forest classiﬁer with a maximum
tree depth of 15. This corresponds only to 3 (1.61%) and
1 (0.54%) incorrectly classiﬁed samples in the test set. The
accuracy does not increase further along with the tree depth.
ii) User veriﬁcation: In user veriﬁcation the classiﬁer is given
with a sample and an user ID and needs to make a decision
whether the given sample belongs to the said user or not. We
show the performance of the classiﬁer in Table IV. We were
able to achieve an accuracy of 98.92% for listening to the same
song and an accuracy of 97.31% for listening to the favorite
song with a maximum tree depth of 10 for the Random Forest
classiﬁer in One-vs-Rest conﬁguration. We highlight that the
classiﬁer performance does not increase after the maximum
depth of 10 and it is always good to select a classiﬁer with
lower depth given the choice to have a better generalization.

TABLE IV: User veriﬁcation: accuracy on test set

Maximum
Tree Depth
5
10
15

to

Listening
same song
98.39%
98.92%
98.92%

to

Listening
favorite song
96.77%
97.31%
97.31%

In Figure 6 we show some example confusion matrices.
Figure 6a shows the confusion matrix of the best classiﬁer
for user identiﬁcation using the favorite song that resulted an
accuracy of 99.46%. The only error it makes is, it identiﬁes one
sample from User-20 as User-16. Similarly, Figure 6b show
the best performing classiﬁer for user veriﬁcation using the
favorite song that gave an accuracy of 97.31%. Note that there
is no signiﬁcant difference in accuracy in the two experiment
scenarios; listening to same song and listening one’s favorite
song. We analyze this further in Section IV-F.

Finally, as mentioned in Section III-C we selected Random
Forest classiﬁer because it was the best performing model out
of the models we tested. For example, the SVM classiﬁer
achieved only 86.56% and 82.26% for user identiﬁcation and
veriﬁcation tasks respectively whilst listening to the favorite

4

(a) User identiﬁcation

(b) User veriﬁcation

Fig. 6: Confusion matrices of best classiﬁers (favorite song)

song. For the task of listening to same song we were able to
obtain an accuracy of 83.87% for user identiﬁcation and an
accuracy of 86.55% for user veriﬁcation.

C. Feature importance & characteristics

To further understand the classiﬁer performance we next do
a feature importance analysis. We used the gini impurity based
feature importance generated using Random Forest classiﬁers.

Fig. 7: Signiﬁcance of individual features

Figure 7 shows the distribution of feature signiﬁcance and
Table V lists the top-20 features for the favorite song scenario.
The same song scenario shows a similar behavior with only
2 changes in top 20 features. The mean values of Alpha
brainwaves obtained from all 4 electrodes show a high feature
importance because the experiment subjects are closing their
eyes and listening to the songs, making the Alpha brainwave
intensity higher as seen in Figure 4. Noticeably there are only
three features coming from Gamma waves which is potentially
due to their low intensities as previously shown in Figure 4.
We show box plots of two example features of highest
importance; mean of the Alpha waves measured at electrode
position TP9 in Figure 8a and mean of the Alpha waves
measured at electrode position AF7 in Figure 8b. As Figure 8a
shows, for many of the users the signal strength of Alpha
waves vary in a limited range in TP9 electrode with the
exception of User-14, User-18 and User-10. Figure 8b shows
that in AF7 electrode the Alpha wave signal strength varies
more in number of users such as User-6, User-8, and User-
14 compared to TP9 electrode. This is potentially due to the

02468101214161820Feature Index00.0050.010.0150.020.0250.030.0350.04Feature Importance0102030405060708000.010.020.030.04Top-20featuresbyimportanceIndexFeatureIndexFeature1MeanAlphaTP911MinBetaTP92MeanAlphaAF712MeanGammaAF73MeanAlphaTP1013MaxAlphaTP104MeanBetaTP914MinAlphaTP105MaxAlphaTP915MinRawTP106MeanThetaAF816MeanGammaTP107MeanThetaAF717MeanGammaAF88MeanAlphaAF818MeanBetaAF79MaxRawTP1019MeanThetaTP1010MeanBetaTP1020MeanBetaAF8[4]S.Nakamura,N.Sadato,T.Oohashi,E.Nishina,Y.Fuwamoto,andY.Yonekura,“Analysisofmusicâ˘A¸Sbraininteractionwithsimultaneousmeasurementofregionalcerebralbloodﬂowandelectroencephalogrambetarhythminhumansubjects,”NeuroscienceLetters,vol.275,no.3,pp.222–226,1999.[Online].Available:http://www.sciencedirect.com/science/article/pii/S0304394099007661[5]J.BhattacharyaandH.Petsche,“Musiciansandthegammaband:asecretaffair?”NeuroReport,vol.12,no.2,pp.371–374,2001.[6]D.Sammler,M.Grigutsch,T.Fritz,andS.Koelsch,“Musicandemotion:electrophysiologicalcorrelatesoftheprocessingofpleasantandunpleasantmusic,”Psychophysiology,vol.44,no.2,pp.293–304,2007.[7]M.Iwasaki,C.Kellinghaus,A.Alexopoulos,R.Burgess,A.Kumar,Y.Han,H.LÃijders,andR.Leigh,“Effectsofeyelidclosure,blinks,andeyemovementsontheelectroencephalogram,”ClinicalNeurophys-iology,vol.116,no.4,pp.878–885,42005.[8]D.Gafurov,K.Helkala,andT.Søndrol,“Biometricgaitauthenticationusingaccelerometersensor.”JCP,vol.1,no.7,pp.51–59,2006.[9]M.O.Derawi,P.Bours,andK.Holien,“Improvedcycledetectionforaccelerometerbasedgaitauthentication,”inIntelligentInformationHidingandMultimediaSignalProcessing(IIH-MSP),2010SixthInter-nationalConferenceon.IEEE,2010,pp.312–317.[10]N.Sae-Bae,K.Ahmed,K.Isbister,andN.Memon,“Biometric-richgestures:anovelapproachtoauthenticationonmulti-touchdevices,”inProceedingsoftheSIGCHIConferenceonHumanFactorsinComputingSystems.ACM,2012,pp.977–986.[11]M.Frank,R.Biedert,E.Ma,I.Martinovic,andD.Song,“Touchalytics:Ontheapplicabilityoftouchscreeninputasabehavioralbiometricforcontinuousauthentication,”IEEEtransactionsoninformationforensicsandsecurity,vol.8,no.1,pp.136–148,2013.[12]J.Chauhan,Y.Hu,S.Seneviratne,A.Misra,A.Seneviratne,andY.Lee,“Breathprint:Breathingacoustics-baseduserauthentication,”inProceedingsofthe15thAnnualInternationalConferenceonMobileSystems,Applications,andServices.ACM,2017,pp.278–291.[13]J.Chauhan,S.Seneviratne,Y.Hu,A.Misra,A.Seneviratne,andY.Lee,“Breathrnnet:Breathingbasedauthenticationonresource-constrainediotdevicesusingrnns,”arXivpreprintarXiv:1709.07626,2017.[14]C.X.Zhao,T.Wysocki,F.Agraﬁoti,andD.Hatzinakos,“Securinghandhelddevicesandﬁngerprintreaderswithecgbiometrics,”inBio-metrics:Theory,ApplicationsandSystems(BTAS),2012IEEEFifthInternationalConferenceon.IEEE,2012,pp.150–155.[15]M.Loulakis,G.Blatsios,C.Vrettou,andI.Kominis,“Quantumbio-metricswithretinalphotoncounting,”arXivpreprintarXiv:1704.04367,2017.[16]E.S.Finn,X.Shen,D.Scheinost,M.D.Rosenberg,J.Huang,M.M.Chun,X.Papademetris,andR.T.Constable,“Functionalconnectomeﬁngerprinting:identifyingindividualsusingpatternsofbrainconnectiv-ity,”Natureneuroscience,vol.18,no.11,pp.1664–1671,2015.[17]M.Phothisonothai,“Aninvestigationofusingssvepforeeg-baseduserauthenticationsystem,”inSignalandInformationProcessingAsso-ciationAnnualSummitandConference(APSIPA),2015Asia-Paciﬁc.IEEE,2015,pp.923–926.[18]M.V.Ruiz-Blondet,Z.Jin,andS.Laszlo,“Cerebre:Anovelmethodforveryhighaccuracyevent-relatedpotentialbiometricidentiﬁcation,”IEEETransactionsonInformationForensicsandSecurity,vol.11,no.7,pp.1618–1629,2016.[19]G.BajwaandR.Dantu,“Neurokey:Towardsanewparadigmofcan-celablebiometrics-basedkeygenerationusingelectroencephalograms,”Computers&Security,vol.62,pp.95–113,2016.[20]J.Chuang,H.Nguyen,C.Wang,andB.Johnson,“Ithink,thereforeiam:Usabilityandsecurityofauthenticationusingbrainwaves,”inInternationalConferenceonFinancialCryptographyandDataSecurity.Springer,2013,pp.1–16.[21]J.Sohankar,K.Sadeghi,A.Banerjee,andS.K.Gupta,“E-bias:Aperva-siveeeg-basedidentiﬁcationandauthenticationsystem,”inProceedingsofthe11thACMSymposiumonQoSandSecurityforWirelessandMobileNetworks.ACM,2015,pp.165–172.[22]B.Kaur,D.Singh,andP.P.Roy,“Anovelframeworkofeeg-baseduseridentiﬁcationbyanalyzingmusic-listeningbehavior,”MultimediaToolsandApplications,pp.1–22,2016.[23]S.Seneviratne,Y.Hu,T.Nguyen,G.Lan,S.Khalifa,K.Thilakarathna,M.Hassan,andA.Seneviratne,“Asurveyofwearabledevicesandchallenges,”IEEECommunicationsSurveysTutorials,vol.19,no.4,pp.2573–2620,2017.[24]S.Li,A.Ashok,Y.Zhang,C.Xu,J.Lindqvist,andM.Gruteser,“Whosemoveisitanyway?authenticatingsmartwearabledevicesusinguniqueheadmovementpatterns,”inPervasiveComputingandCommunications(PerCom),2016IEEEInternationalConferenceon.IEEE,2016,pp.1–9.[25]J.Chauhan,H.J.Asghar,A.Mahanti,andM.A.Kaafar,“Gesture-basedcontinuousauthenticationforwearabledevices:Thesmartglassesusecase,”inInternationalConferenceonAppliedCryptographyandNetworkSecurity.Springer,2016,pp.648–665.variation in regional activity of the brain stimulated by music
and the fact that TP9 electrode is relatively closer to the
auditory cortex than AF7 electrode.

The two least important features; zero crossing rate (ZCR)
of Alpha wave from TP9 and zero crossing rate (ZCR) of
raw EEG from TP9 consist of mainly zeros. We show the
distribution of the third least important feature in Figure 9. As
the ﬁgure shows, for many users mean raw EEG from TP10
behaves similar (i.e. less predictive power).

TABLE V: Top-20 features by importance

Index

Feature

Index

Feature

1
2
3
4
5
6
7
8
9
10

Mean Alpha TP9
Mean Alpha AF7
Mean Alpha TP10
Mean Beta TP9
Max Alpha TP9
Mean Theta AF8
Mean Theta AF7
Mean Alpha AF8
Max Raw TP10
Mean Beta TP10

11
12
13
14
15
16
17
18
19
20

Min Beta TP9
Mean Gamma AF7
Max Alpha TP10
Min Alpha TP10
Min Raw TP10
Mean Gamma TP10
Mean Gamma AF8
Mean Beta AF7
Mean Theta TP10
Mean Beta AF8

(a) Mean Alpha-TP9

(b) Mean Alpha-AF7

Fig. 8: User-wise distribution of example signiﬁcant features
D. Electrode placement

We next obtain the accuracy for using the data collected
from individual electrodes and electrode combination scenar-
ios of two front electrodes and two rear electrodes. We used
selected electrodes’ features from 80 feature vector of the
total dataset given in Table II. In Figure 10a and Figure 10b
we show the accuracies for the two experiment scenarios
for both user veriﬁcation and user identiﬁcation. As expected
when the number of electrodes decreases the accuracy drops
and results show that there are no signiﬁcant differences in
accuracy between individual electrodes. However, combining
two electrodes provides an accuracy of approximately 95%
which is only a 4% difference to the all-electrode accuracy.
This is a promising result as,
is easier to include two
electrodes in the front of many consumer headsets.

it

5

Fig. 9: User-wise distribution of a low signiﬁcant feature

(a) Same song

(b) Favourite song

Fig. 10: Accuracy using different electrodes

E. Signiﬁcance of different brainwave frequencies

We next analyse whether some brainwaves show more
differentiation between individual participants. We trained
classiﬁers only from data from different brainwave types and
some combinations of them and the accuracies are shown in
Figure 11 for the two experiment scenarios. As the ﬁgures
show none of the individual waves gives sufﬁcient accuracy.
Also, it is interesting to observe that despite not showing up
into overall top-20 features (c.f Section IV-C) Gamma waves
provide a performance in par with Alpha and Beta waves.
Figures 11a and 11b show that combining Alpha and Beta
waves provide a slightly higher performance compared to other
combinations. This can be expected as there were more Alpha
and Beta related features in top-20 signiﬁcant features.

(a) Same song

(b) Favourite song

Fig. 11: Accuracy using different brainwave frequencies

F. Cross song and combined data training and testing

Our classiﬁer results shown previously indicated that both
the experiments; listening to same song and favorite song pro-
vide equally good results. To check whether there is actually

00.20.40.60.811.21.41.61234567891011121314151617181920User IDMean Alpha TP9 (dB)−0.4−0.200.20.40.60.811.21234567891011121314151617181920User IDMean Alpha AF7 (dB)8008108208308408508608708808909001234567891011121314151617181920User IDMean Raw TP10 (µv)AllDataAF7AF8AF7 &AF8TP10TP9TP9 &TP10200406080100Electrode PairsSingle ElectrodeAccuracy (%)User IdentificationUser VerificationAllDataAF7AF8AF7 &AF8TP10TP9TP9 &TP10200406080100Electrode PairsSingle ElectrodeAccuracy (%)User IdentificationUser VerificationAllDataAlphaBetaAlpha &BetaThetaGammaGamma &ThetaBeta &Gamma200406080100Paired Frequency BandsSingle Frequency BandAccuracy (%)User IdentificationUser VerificationAllDataAlphaBetaAlpha &BetaThetaGammaGamma &ThetaBeta &Gamma200406080100Paired Frequency BandsSingle Frequency BandAccuracy (%)User IdentificationUser Verificationa difference in brainwave responses in two scenarios, we did
a cross experiment training and testing; i.e. training a model
using data from listening to same song and testing the model’s
performance on test data from listening to the favorite song
and vice versa. We show the results in Table VI. The results
show that there is approximately 20%-30% drop in accuracy in
cross song setting. It is interesting to note that models are still
achieve an accuracy in the range between 64%-79% indicating
there is some common response to music in the brain.

TABLE VI: Cross song training and testing

Tested on

User identiﬁcation
Trained on
Favorite Song
Same Song
User veriﬁcation
Favorite Song
Same Song

Favorite Song
99.46%
72.04%

Same Song
79.57%
98.39%

97.31%
64.52%

77.96%
98.92%

Finally, we trained a classiﬁer with the combined training
dataset and tested on different test datasets. For the combined
test dataset the classiﬁer gave an accuracy of 98.39% and indi-
vidual accuracies of 98.92% and 97.85% respectively on same
song and favorite song test datasets for user identiﬁcation.

G. Summary of results

In Table VII we provide a summarized view of all of our

results and we below list the main take away messages.

• Individuals can be authenticated accurately using brain-
waves extracted from audio stimulation of the brain and
we were able to achieve accuracies in the range of 97%-
99% for both user identiﬁcation and veriﬁcation tasks.
• The accuracies drop only by 2%-5% when only two
electrodes are used. This is promising as it is feasible to
include two electrodes in the front side of many consumer
headsets.

• Feature importance analysis and frequency band-wise
training indicated that features derived from Alpha and
Beta waveforms have more predictive capability com-
pared to other frequency bands.

TABLE VII: Summary of results

Scenario
All data
Electrode position
AF7
AF8
TP9
TP10
AF7 + AF8
TP9 + TP10
Brainwave type
Alpha
Beta
Gamma
Theta
Alpha + Beta
Beta + Gamma
Gamma + Theta

Favorite song
Identiﬁcation Veriﬁcation
99.46%

97.31%

Same song
Identiﬁcation Veriﬁcation
98.39%

98.92%

86.02%
85.48%
89.78%
89.78%
94.62%
94.62%

79.56%
82.80%
79.03%
72.58%
91.93%
90.86%
88.17%

86.02%
85.48%
89.24%
92.47%
94.62%
94.62%

85.48%
83.87%
81.72%
72.04%
93.54%
92.47%
91.93%

87.63%
80.10%
90.86%
86.55%
94.62%
94.62%

77.96%
80.11%
79.57%
76.88%
94.09%
91.40%
95.70%

86.02%
83.87%
92.01%
90.86%
93.54%
96.77%

82.26%
85.48%
82.26%
77.96%
96.77%
92.47%
93.01%

V. RELATED WORK
A. Novel behavioral biometric modalities

Limitations of the passwords/token-based systems as well
as static biometrics such as ﬁngerprints led to a large body

6

of works exploiting behavioral biometrics as a mean of user
authentication. Such biometrics include gait [5], touch pat-
terns [4], breathing acoustics [16], [17], and heart rate [6].
For example, Buriro et al. [18] proposed an authentication
mechanism for smartphones using the built-in smartphone
sensors to capture the behavioural signatures of the way a
user slides the lock button on the screen to unlock the phone,
and brings the phone towards ear. True Acceptance Rate of
99.35% was achieved using Random Forest classiﬁer with 85
users performing the unlocking actions, in sitting, standing,
and walking postures. Hadiyoso et al. [6] demonstrated a new
biometric modality using ECG signals from 11 participants.
Using empirical mode decomposition (EMD) and statistical
analysis for feature extraction authors achieved an accuracy
of 93.6% using discriminant analysis.

More recently another set of modalities that extend the con-
ventional “what you are” deﬁnition of behavioral biometrics
to “what you perceive”, are becoming popular as they tend
to be more resilient, reliable, and non-intrusive. Also, such
biometrics suit well for IoT devices that have limited user
interactivity.

For instance, Loulakis et al. [19] proposed a quantum
biometric method using single photon detection ability of the
human retina. Using photon counting principles of human rod
vision, authors formed light ﬂash patterns that only a speciﬁc
individual can identify and theoretically showed that false
positive and false negative rates are lower than 10−10 and
10−4. Finn et al. [20] demonstrated that functional connec-
tivity proﬁles of the brain observed by fMRI can potentially
be unique to individuals. Using data collected from 126
participants, authors showed that between 87%-99% accuracy
can be achieved under different brain activities.

Our work contributes to behavioral biometrics of “what you
perceive” category by exploring the feasibility of using music
induced brainwave patterns as a novel authentication modality.

B. Use of EEG for user authentication

Several work looked different forms of brain stimulations
that can be used to generate EEG data for user authentication
such as visual [21], audio-visual [22], mental tasks [23], [24]
as well as resting state [25].

Chiu et al. [21] used eight channel EEG headset (BR8) to
capture brainwaves from 30 subjects. Using visual stimulated
brainwave data from a set of 15 images (5 familiar, 5 dj vu, and
5 unfamiliar) authentication tokens were retrieved and authors
achieved an accuracy of 98.7% using an SVM classiﬁer. Huang
et al. [22] used 14 channel Emotive Epoc+ headset to focus
on brainwaves evoked by an audio-visual paradigm of subjects
own face image and a voice calling their own name. With
raw EEG collected from 30 adult subjects and a bagging-
based ensemble machine learning model authors achieved an
accuracy around 92%.

Chuang et al. [23] used a single channel EEG headset
(Neurosky) for capturing brainwaves from 15 subjects. Using 7
mental tasks in two repeated sessions, an accuracy of 99% was
achieved with a cosine distance based kNN classiﬁer. Sohankar
et al. [25] also used Neurosky headset, but focused on brain

behavior while the user is in resting state and achieved 95% ac-
curacy for user authentication and 80% for user identiﬁcation
with 10 different subjects. Similarly, MindID [26] exploited
Delta brainwaves for authentication (98.2% accuracy), while
users relax with closed eyes using Emotive Epoc+ with 8
subjects. In another study using the same device with 12
subjects, Jayarathne et al. [27] showed 96.67% accuracy can
be achieved using LDA for classiﬁcation. Zeynali et al. [28]
conducted a similar study using ﬁve mental activities and
achieved accuracies in the range of 97-98% using Neural
Network classiﬁer. Our work is different from these related
work as we introduce a new form of non-invasive music based
brain stimulation method for EEG based user authentication.
Similar to our work, Kaur et al. [29] explored the potential
of using music stimulations for authentication. An experiment
with 60 users listening to 4 genres of music, 97.5% and
93.83% of accuracies were reached using Hidden Markov
Models and SVM classiﬁers respectively. Huang et al. [30]
proposed a system targeting IoT applications. In contrast to
this work, we use a different form of audio stimulation and
our experiments span over 1.5 months with multiple sessions.

C. Authentication solutions for head-mounted devices

Head-mounted devices of different

types such as smart
glasses, smart ear-buds, and virtual/augmented reality headsets
are increasingly becoming standalone devices [31]. Due to
the lack of user input modalities, several work looked into
the possibility of coming up with password-less authentication
mechanisms for head-mounted devices. Head movement pat-
terns of individuals generated in response to an audio stimulus
was proposed to be used to authenticate them to smart glasses
by Li et al. [32]. With accelerometer data from 30 users and
simple distance-based threshold classiﬁers, the authors showed
Equal Error Rate of 4.43% can be achieved. Using the data
collected from 20 users, Rogers et al. [33] proposed blinks
and head-movements captured using infrared, accelerometer,
and gyroscope sensors can be used for user identiﬁcation with
an accuracy of 94%. Mustafa et al. [34] proposed security-
sensitive Virtual Reality (VR) using head, hand and (or) body
movement patterns exhibited by a user freely interacting with
a VR. With 23 users the authors achieved a mean equal
error rates of 7%. George et al. [35] investigated the third
dimension for authentication in immersive virtual reality and
in the real world. In the proposed system, users authenticate
by selecting a series of 3D objects in a room using a pointer
and investigated the inﬂuence of randomized user and object
positions, in a series of user studies with 48 subjects.

In contrast to above work, this paper propose EEG as a
feasible authentication modality for head-mounted devices and
to the best of our understanding this is the ﬁrst work that
demonstrated the feasibility of using a commodity brainwave
headset for user authentication.

VI. DISCUSSION & FUTURE WORK

We below discuss the implications of our results, potential

application scenarios, limitations, and potential future work.

7

Application scenarios. Our results showed that it is pos-
sible to authenticate users with over 97% accuracy by cap-
turing the brainwave patterns generated whilst a person is
listening to music. The results are promising and the fact
that we were able to do it using a commodity-headset allude
that brainwaves can be used as a potential entry point and
continuous authentication solution for smart-headsets. Also,
we showed that two electrodes are sufﬁcient to achieve around
95% accuracy. This is signiﬁcant as most of the commodity
head-mounted devices such as smart glasses, ear-buds, and VR
headsets can easily support two front electrodes. We believe
the innovative applications that are expected to rise along with
the IoT in education, remote support, tele-health, and many
other domains will demand for increased security mechanisms
for head-mounted devices. To this end, MusicID provides an
intrusion-free and secure solution.

Participant pool. We used a dataset collected from 20
voluntary participants spanning over 1.5 months. 55% of the
participants provided data in over more than three sessions and
45% provided data in only two sessions. While we achieved
over 97% accuracy, it is necessary to explore the feasibility
of MusicID in a much larger participant pool that includes
diverse demographics. However, we highlight that majority of
comparable studies (E.g. [23], [25], [26], [27], [32]) demon-
strated their accuracies with less than 20 participants.

Contextual changes. Our results showed consistency over
multiple sessions spanning 1.5 months and we were able to
re-identify users in new sessions based on previous session’s
data. Nonetheless further experiments on collecting data after
different physical and mental activities, different times of the
day can help to identify whether there are context-driven short
term variations that are dependent on physical activity, health
conditions [36], stress levels, fatigue [37], and mood. Also,
there can be long term variations in brainwave responses (e.g.
due to aging [38]) and a more robust model must be tuned
over time to capture such variations. For such settings learning
models based on transfer learning can be leveraged.

Attacks. Brainwave patterns provide a unique biometric that
is more secure compared to other biometrics. For example,
it is difﬁcult to record brainwave patterns for replay attacks
compared to other biometrics. For example, a person’s ﬁnger-
print can be copied while the person is asleep, however this
type of recording is not applicable for brainwaves as brain
activities are completely different whilst sleeping. Similarly,
an attacker can record a person’s voice from a distance and use
voice synthesis to bypass a voice based authentication system.
However, brainwaves can’t be recorded at a distance and also
has to be collected in the very speciﬁc moments where the user
is engaged in a related activity such as listening to music in
this case. One possible attack is an attacker starting with some
existing data and trying to synthesize the target’s brainwave
pattern, which appears to be highly difﬁcult.

A. Future work

During our study we found another potential biometric
modality related to the electrical activities in eye and has the
potential to be used as a standalone modality or in combination
with EEG. Due to the potential difference between the cornea

and retina of the eye, eye blinks evoke an artifact waveform in
EEG known as EOG (Electrooculogram) which can potentially
be unique to individuals. The usage of EOG as a biometric
system has been tested in [39] using electrodes placed on
the face to capture the potential difference evoked due to
eye blinks. During our study we found that Muse headset
is capable of capturing the EEG signals during eye blinks,
which can then be used to extract EOG using signal processing
techniques such as Discrete Wavelet Transformation.

VII. CONCLUSION

We proposed MusicID a behavioral biometric modality
suitable for smart-headsets enabled IoT environments induced
by the human brain’s response to music. Using a 4-electrode
commodity brainwave headset we collected brainwave data
samples from real users over multiple sessions while they were
listening two forms of music; i) a common English song, ii)
individual’s favorite song. We built Random Forest classiﬁers
and showed that an accuracy over 98% can be achieved for
user identiﬁcation and an accuracy over 97% can be achieved
for user veriﬁcation using both audio stimulations. Our feature
importance analysis showed that Alpha and Beta waves have
more predictive capabilities. By investigating the classiﬁer
performance at
individual electrodes and in combinations,
we showed that use of two electrodes instead of four, drops
accuracy only by 2%-5%. While further studies are required
with much larger set of users, the performance of MusicID
indicates the feasibility of a non-obtrusive, user friendly, and
secure solution to the problem of entry point and continuous
user authentication in smart-headsets that are expected to
become increasingly popular with the proliferation of IoT.

REFERENCES

[1] “Gartner, Inc.” https://www.gartner.com/newsroom/id/3790965, 2017.
[2] T. Matsumoto, H. Matsumoto, K. Yamada, and S. Hoshino, “Impact of
artiﬁcial gummy ﬁngers on ﬁngerprint systems,” in Optical Security and
Counterfeit Deterrence Techniques IV, vol. 4677, 2002, pp. 275–290.

[3] F. Monrose and A. D. Rubin, “Keystroke dynamics as a biometric for
authentication,” Future Generation computer systems, vol. 16, no. 4, pp.
351–359, 2000.

[4] M. Frank, R. Biedert, E. Ma, I. Martinovic, and D. Song, “Touchalytics:
On the applicability of touchscreen input as a behavioral biometric for
continuous authentication,” IEEE transactions on information forensics
and security, vol. 8, no. 1, pp. 136–148, 2013.

[5] D. Gafurov, K. Helkala, and T. Søndrol, “Biometric gait authentication
using accelerometer sensor.” JCP, vol. 1, no. 7, pp. 51–59, 2006.
[6] S. Hadiyoso, A. Rizal, and S. Aulia, “Ecg based person authentication
using empirical mode decomposition and discriminant analysis,” Journal
of Physics: Conference Series, vol. 1367, p. 012014, 11 2019.

[7] K. Saeed, New directions in behavioral biometrics. CRC Press, 2016.
[8] “Muse— meditation made easy,” http://www.choosemuse.com/, 2017.
[9] “Neurosky Store,” https://store.neurosky.com/pages/mindwaves, 2017.
[10] P. M. Thompson, T. D. Cannon, K. L. Narr, T. Van Erp, V.-P. Poutanen,
M. Huttunen, J. L¨onnqvist, C.-G. Standertskj¨old-Nordenstam, J. Kaprio,
M. Khaledy et al., “Genetic inﬂuences on brain structure,” Nature
neuroscience, vol. 4, no. 12, p. 1253, 2001.

[11] H. Petsche, P. Richter, A. V. Stein, S. C. Etlinger, and O. Filz, “Eeg
coherence and musical thinking,” Music Perception: An Interdisciplinary
Journal, vol. 11, no. 2, pp. 117–151, 1993.

[12] J. Bhattacharya and H. Petsche, “Musicians and the gamma band: A
secret affair?” NeuroReport, vol. 12, no. 2, pp. 371–374, 2001.
[13] D. Sammler, M. Grigutsch, T. Fritz, and S. Koelsch, “Music and
emotion: Electrophysiological correlates of the processing of pleasant
and unpleasant music,” Psychophysiology, pp. 293–304, 2007.

8

[14] M. Iwasaki, C. Kellinghaus, A. Alexopoulos, R. Burgess, A. Kumar,
Y. Han, H. L¨uders, and R. Leigh, “Effects of eyelid closure, blinks, and
eye movements on the electroencephalogram,” Clinical Neurophysiol-
ogy, vol. 116, no. 4, pp. 878–885, 4 2005.

[15] “Muse Monitor,” http://www.musemonitor.com, 2017.
[16] J. Chauhan, S. Seneviratne, Y. Hu, A. Misra, A. Seneviratne, and
Y. Lee, “Breathing-based authentication on resource-constrained iot
devices using recurrent neural networks,” Computer, May 2018.
[17] J. Chauhan, J. Rajasegaran, S. Seneviratne, A. Misra, A. Seneviratne,
and Y. Lee, “Performance characterization of deep learning models for
breathing-based authentication on resource-constrained devices,” Pro-
ceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous
Technologies, vol. 2, no. 4, pp. 1–24, 2018.

[18] A. Buriro, B. Crispo, and M. Conti, “Answerauth: A bimodal behavioral
biometric-based user authentication scheme for smartphones,” Journal
of Information Security and Applications, vol. 44, pp. 89 – 103, 2019.
[19] M. Loulakis, G. Blatsios, C. Vrettou, and I. Kominis, “Quantum bio-
metrics with retinal photon counting,” preprint arXiv:1704.04367, 2017.
[20] E. S. Finn, X. Shen, D. Scheinost, M. D. Rosenberg, J. Huang, M. M.
Chun, X. Papademetris, and R. T. Constable, “Functional connectome
ﬁngerprinting: Identifying individuals using patterns of brain connectiv-
ity,” Nature neuroscience, vol. 18, no. 11, pp. 1664–1671, 2015.
[21] W. Chiu, C. Su, C.-Y. Fan, C.-M. Chen, and K.-H. Yeh, “Authentication
with what you see and remember in the internet of things,” Symmetry,
vol. 10, no. 11, 2018.

[22] H. Huang, L. Hu, F. Xiao, A. Du, N. Ye, and F. He, “An eeg-
based identity authentication system with audiovisual paradigm in iot,”
Sensors, vol. 19, no. 7, 2019.

[23] J. Chuang, H. Nguyen, C. Wang, and B. Johnson, “I think, therefore
I am: Usability and security of authentication using brainwaves,” in
Financial Cryptography and Data Security, 2013, pp. 1–16.

[24] G. Bajwa and R. Dantu, “Neurokey: Towards a new paradigm of can-
celable biometrics-based key generation using electroencephalograms,”
Computers & Security, vol. 62, pp. 95–113, 2016.

[25] J. Sohankar, K. Sadeghi, A. Banerjee, and S. K. Gupta, “E-bias: A
pervasive EEG-based identiﬁcation and authentication system,” in Proc.
of ACM Symposium on QoS and Security for Wireless and Mobile
Networks, 2015, pp. 165–172.

[26] X. Zhang, L. Yao, S. S. Kanhere, Y. Liu, T. Gu, and K. Chen, “MindID:
Person identiﬁcation from brain waves through attention-based recurrent
neural network,” arXiv preprint arXiv:1711.06149, 2017.

[27] I. Jayarathne, M. Cohen, and S. Amarakeerthi, “BrainID: Development
of an EEG-based biometric authentication system,” in IEMCON, 2016.
[28] M. Zeynali and H. Seyedarabi, “Eeg-based single-channel authentication
systems with optimum electrode placement
for different mental
activities,” Biomedical Journal, vol. 42, no. 4, pp. 261 – 267,
2019. [Online]. Available: http://www.sciencedirect.com/science/article/
pii/S2319417018302439

[29] B. Kaur, D. Singh, and P. P. Roy, “A novel framework of EEG-based
user identiﬁcation by analyzing music-listening behavior,” Multimedia
Tools and Applications, pp. 1–22, 2016.

[30] H. Huang, L. Hu, F. Xiao, A. Du, N. Ye, and F. He, “An eeg-
based identity authentication system with audiovisual paradigm in iot,”
Sensors, vol. 19, no. 7, p. 1664, 2019.

[31] S. Seneviratne, Y. Hu, T. Nguyen, G. Lan, S. Khalifa, K. Thilakarathna,
M. Hassan, and A. Seneviratne, “A survey of wearable devices and
challenges,” IEEE Communications Surveys Tutorials, 2017.

[32] S. Li, A. Ashok, Y. Zhang, C. Xu, J. Lindqvist, and M. Gruteser, “Whose
move is it anyway? authenticating smart wearable devices using unique
head movement patterns,” in PerCom’16.

IEEE, 2016, pp. 1–9.

[33] C. Rogers, A. Witt, A. Solomon, and K. Venkatasubramanian, “An

approach for user identiﬁcation for head-mounted displays,” 09 2015.

[34] T. Mustafa, R. Matovu, A. Serwadda, and N. Muirhead, “Unsure how
to authenticate on your vr headset?: Come on, use your head!” 03 2018.
[35] C. George, M. Khamis, D. Buschek, and H. Hussmann, “Investigating
the third dimension for authentication in immersive virtual reality and
in the real world,” in 2019 IEEE Conference on Virtual Reality and 3D
User Interfaces (VR).

IEEE, 2019, pp. 277–285.

[36] S. Roizenblatt, H. Moldofsky, A. A. Benedito-Silva, and S. Tuﬁk, “Alpha
sleep characteristics in ﬁbromyalgia,” Arthritis & Rheumatology, 2001.
[37] A. Craig, Y. Tran, N. Wijesuriya, and H. Nguyen, “Regional brain wave
activity changes associated with fatigue,” Psychophysiology, 2012.
[38] E. C. Anyanwu, “Neurochemical changes in the aging process: Implica-
tions in medication in the elderly,” The Scientiﬁc World Journal, 2007.
[39] M. S. Hossain, K. Huda, S. M. S. Rahman, and M. Ahmad, “Implemen-
tation of an EOG based security system by analyzing eye movement
patterns,” in ICAEE’15, Dec 2015, pp. 149–152.

