Time Series (re)sampling using Generative Adversarial
Networks∗

Christian M. Dahl† Emil N. Sørensen‡

February 2, 2021

Abstract

We propose a novel bootstrap procedure for dependent data based on Generative Ad-

versarial networks (GANs). We show that the dynamics of common stationary time

series processes can be learned by GANs and demonstrate that GANs trained on a sin-

gle sample path can be used to generate additional samples from the process. We ﬁnd

that temporal convolutional neural networks provide a suitable design for the generator

and discriminator, and that convincing samples can be generated on the basis of a vec-

tor of iid normal noise. We demonstrate the ﬁnite sample properties of GAN sampling

and the suggested bootstrap using simulations where we compare the performance to

circular block bootstrapping in the case of resampling an AR(1) time series processes.

We ﬁnd that resampling using the GAN can outperform circular block bootstrapping

in terms of empirical coverage.

1
2
0
2

n
a
J

0
3

]

G
L
.
s
c
[

1
v
8
0
2
0
0
.
2
0
1
2
:
v
i
X
r
a

∗Acknowledgements: The authors gratefully acknowledge support from the Google Tensorﬂow Research
Cloud (TFRC). PyTorch code for this paper is available on request. We also thank Giovanni Mellace and
Peter Sandholt Jensen for useful comments.

†Department of Business and Economics, University of Southern Denmark, cmd@sam.sdu.dk
‡School of Economics, University of Bristol, e.sorensen@bristol.ac.uk

 
 
 
 
 
 
1. INTRODUCTION

Generative Adversarial Nets (GANs) were introduced by Goodfellow, Pouget-Abadie, et al.

(2014). Based on an initial training sample, GANs can learn to generate additional data that

looks similar. GANs are intensely researched in the deep learning literature (Radford et al.,

2015; Salimans et al., 2016; Gulrajani et al., 2017; Arjovsky et al., 2017) but have received

minor attention in time series analysis. There are examples of GANs being explored for

structural models (Kaji et al., 2018) and estimation of treatment eﬀects (Athey et al., 2019).

However, these are both in the cross-sectional iid setting. Hyland et al. (2017) suggest that

GANs can be used for what they call “medical” time series but they lack a clear deﬁnition of

the data generating process (DGP) and correspondingly measures of quality for the learned

model. Recently, Wiese et al. (2020) described a GAN for ﬁnancial time series that can

reproduce some of the stylised facts of such series using a temporal convolution architecture

related to the one suggested in this work. Smith and Smith (2020) also outlined a method

for training GANs on time series using spectrograms. Unlike Wiese et al. (2020) and Smith

and Smith (2020) our focus is on the general applicability of GANs as a bootstrap method

for dependent processes. The potential usefulness of GANs for such time series bootstraps

is brieﬂy mentioned in recent work by Haas and Richter (2020).

GANs have frequently been applied for image synthesis (Goodfellow, Pouget-Abadie,

et al., 2014; Radford et al., 2015; Arjovsky et al., 2017) and the generated samples are

often evaluated using measures such as Inception Score (Salimans et al., 2016) or Fr`echet

Inception Distance (Heusel et al., 2017). Both measures utilise a neural network trained for

image recognition to attempt to assess the visual quality of the generated samples. However,

these are heuristics and it is diﬃcult to construct a theoretically motivated notion of quality.

Contrary to Hyland et al. (2017), we argue that it is straightforward to assess the basic

properties of the generated samples in a time series context as the theoretical properties

of many time series are well understood – contingent on considering explicitly deﬁned data

generating processes. A similar point is made by Wiese et al. (2020).

1

We show that stationary autoregressive time series processes – exempliﬁed by the AR(1)

process – can be learned by GANs trained on a single sample path and ﬁnd that temporal

convolutional neural networks provide a suitable design for the generator and discriminator.

Bootstrapping dependent data – such as samples from a time series process – has received

long-standing attention in the literature, see e.g. the overview of block bootstrapping by

Lahiri (1999) and the newer contributions of Paparoditis and Politis (2001) and Shao (2010).

We suggest that GANs provide a novel approach to such bootstrapping which we call the

generative bootstrap (GB). The theoretical properties of GANs – and hence our suggested

generative bootstrap – is an active area of research, see (Biau, Cadre, et al., 2018; Biau,

Sangnier, et al., 2020; Haas and Richter, 2020) and we instead contribute by analysing the

ﬁnite sample properties of generative bootstrapping using simulations where we compare the

performance against Circular Block Bootstrapping (CBB) (Politis and Romano, 1992) for

the AR(1) process.

In particular, we recover the parameters of the true data generating

process using the simulated data and thereby show that the generative model has learned

at least a minimum of characteristics of the process. Further, we ﬁnd that resampling using

the generative model can outperform CBB for dependent data on the empirical coverage of

percentile conﬁdence intervals.

Our main contributions are: (1) we show that GANs can learn the dynamics of stationary

autoregressive time series processes, (2) we ﬁnd that temporal convolutions provide a working

architecture for the discriminator and generator networks, and (3) we show that GANs can

be used to resample from dependent data with suggestive ﬁnite-sample improvements in

empirical coverage over CBB. (1) and (2) are also discussed by Wiese et al. (2020). However,

Wiese et al. (2020) do not consider the more general applicability of GANs to time series

bootstrapping which we deﬁne and subsequently evaluate in our simulations.

In Section 2 we review two common GANs and discuss how they can be trained to

generate samples from a time series based on an initial sample path. Section 3 discusses an

algorithm for using the trained GAN to bootstrap dependent data. In Section 4 we provide

2

simulation evidence of the quality of the learned GAN and the performance when it is used

for bootstrapping. Section 5 concludes.

2. GENERATIVE ADVERSARIAL NETS

2.1. Basic GAN

The concept of GANs can be introduced intuitively as follows. Assume that a real sample

of data is drawn from the unknown distribution FX and assume that we have another arbi-

trary, but known, distribution FZ. The generator G is a function that transforms a sample

from FZ into a sample that looks like it is drawn from the real data distribution FX. The

discriminator D is a function that tries to determine if a given sample is drawn from the

real data distribution FX or not. The two models are set to play a game against each other.

The generator tries to fool the discriminator by generating fake samples that look as real

as possible, and the discriminator tries to detect the generator’s forgery by determining if it

got a real or fake sample.

Let G and D be speciﬁed up to the ﬁnite dimensional parameters θG and θD respectively.

Also, let xreal denote some generic real sample from distribution FX and xf ake = G(z; θG), z ∼

FZ a generated sample.

Goodfellow, Pouget-Abadie, et al. (2014) suggest solving the minimax problem

min
θG

max
θD

EFX log D(x; θD) + EFZ log(1 − D(G(z; θG); θD)).

(1)

In practice Goodfellow, Pouget-Abadie, et al. (2014) separate the minimisation and maximi-

sation steps into

max
θD

min
θG

EFX log D(x; θD) + EFZ log(1 − D(G(z; θG); θD))

EFZ log(1 − D(G(z; θG); θD))

3

and iterate between these to learn the discriminator and generator using batching and

stochastic gradient descent. We skip the details here, but describe the training in detail

for the Wasserstein GAN in the following section. Algorithm 1 provides an overview of the

training algorithm.

Biau, Cadre, et al. (2018) and Goodfellow, Pouget-Abadie, et al. (2014) argue that, under

a set of assumptions, the optimal discriminator in the minimax formulation in Equation 1 is

related to the Jensen-Shannon divergence between the distributions of the real and generated

data.

If FG is the distribution of the transformation G(Z; θG), Z ∼ FZ and the optimal

discriminator is in the class of functions {D(·, θD) : θD ∈ Θ} where Θ is some parameter

space then the solution to the maximisation problem in Equation 1 is the Jensen-Shannon

divergence JS between FX and FG (Biau, Cadre, et al., 2018; Goodfellow, Pouget-Abadie,

et al., 2014),

max
θD∈Θ

EFX log D(x; θD) + EFZ log(1 − D(G(z; θG); θD)) = 2JS(FX, FG) − log 4

(2)

and, heuristically, if we assume the discriminator is optimal then the generator is solving the

problem

min
θG

JS(FX, FG).

(3)

As noted by Biau, Cadre, et al. (2018), this seems to have motivated work on investigating

other divergences/distances in the context of GANs. One such distance is the Wasserstein

distance which we will consider in the following section.

2.2. Wasserstein GAN

Arjovsky et al. (2017) argue that an alternative distance measure in Equation 3 is the

order-1 Wasserstein (Earth-Mover) distance which results in the Wasserstein GAN. Consider

informally two probability measures P and Q deﬁned on a suitable common probability space

4

i log D(xreal; θD) + log (1 − D(xf ake; θD))

(cid:46) X is a given collection of samples from FX
(cid:46) Discriminator loss

Algorithm 1 GAN (Goodfellow, Pouget-Abadie, et al., 2014)

for i = 1, 2, ..., N do:
z ← Sample(FZ)
xf ake ← G(z; θG)
xreal ← Sample(X )
(cid:80)
L(b)
D ← 1
nb
θD ← θD − lrD∇θDL(b)

D

z ← Sample(FZ)
xf ake ← G(Z; θG)
(cid:80)
L(b)
G ← 1
nb
θG ← θG − lrG∇θGL(b)

G

i log(1 − D(xf ake; θD))

(cid:46) Discriminator update

(cid:46) Parameter update

(cid:46) Generator update

(cid:46) Generator loss

(cid:46) Parameter update

end for

(M, ·). The Wasserstein distance W1 between P and Q is deﬁned as

W1(P, Q) = inf
V∈Π

EV||x − y||

(4)

where, with abuse of notation, Π is the set of all joint probability measures V(x, y) with

marginal probabilities P(x) and Q(y), and || · || is the absolute value norm (Arjovsky et al.,

2017). Here EV denotes expectation under the probability measure V. Arjovsky et al. (2017)

argue that Equation 4 is equivalent to

W1(P, Q) = sup
D∈F

EP D(x) − EQ D(x)

(5)

where F is the set of real-valued Lipschitz functions on M with Lipschitz constant 1. In

Equation 5 we have conveniently denoted the function to be optimised over by D as we

can consider it to play the role of a discriminator. Given the discriminator, the generator

would like to minimise the distance between the generated data and real data, if the laws

of generated and real data are given by P and Q then the generator is solving the problem

inf G W1(P, Q). This is analogous to Equation 3 but the Jensen-Shannon divergence JS

has been replaced by the Wasserstein distance W1. A primary issue in operationalising

5

Equation 5 is enforcing the Lipschitz condition on D. For example, say we learn D using a

neural network, how do we constrain this network to only learn Lipschitz-1 functions? Let

G and D be speciﬁed up to the ﬁnite dimensional parameters θG and θD respectively. Also,

let xreal denote some generic real sample from distribution FX and xf ake = G(z; θG), z ∼ FZ

a generated sample. Now based on Equation 5 Arjovsky et al. (2017) suggest solving the

minimax problem

min
θG

max
θD

EFX D(x; θD) − EFZ D(G(z; θG); θD)

(6)

subject to D( · ; θD) ∈ F. A simple training algorithm for solving the problem (6) would be

splitting it into a min and max step, and iterate between them (Goodfellow, Pouget-Abadie,

et al., 2014)

max
θD

min
θG

EFX D(x; θD) − EFZ D(G(z; θG); θD)

−EFZ D(G(z; θG); θD).

(7)

(8)

Gulrajani et al. (2017) recognise that a function is Lipschitz-1 if and only if the norm

of the gradient is 1 or less everywhere, so they suggest a gradient penalty to enforce the

Lipschitz condition in the discriminator

P (θD) = EF ˜X

(||∇˜xD(˜x; θD)||2 − 1)2

where ||·||2 is the l2 norm. Note that ˜x = axreal +(1−a)xf ake is a convex combination of xreal

and xf ake with uniform random weight a ∼ U(0, 1), and we let F˜x denote the distribution

of these convex combinations. This procedure is motivated heuristically in Gulrajani et al.

(2017) and is a less computationally intensive way of enforcing the Lipschitz constraint across

6

all possible x. Under the gradient penalty the discriminator objective function is now

max
θD

EFX D(x; θD) − EFZ D(G(z; θG); θD) + λP (θD)

(9)

where the weight of the gradient penalty is adjusted by the hyper parameter λ.

Let {(zi, xi,real)}nb

i=1 constitute a (mini) batch of data where zi is noise sampled from FZ

and xi,real is a real sample. During training we minimise the batch discriminator loss

L(b)

D =

1
nb

nb(cid:88)

i=1

D(xi,f ake; θD) − D(xi,real; θD) + λ

1
nb

nb(cid:88)

i=1

(||∇˜xD(˜xi; θD)||2 − 1)2 ,

(D1)

˜xi = axi,real + (1 − a)xi,f ake

which is the empirical and batched equivalent of Equation 9, and recall that xi,f ake =
G(zi; θG). As per usual, the batch gradients ∇θDL(b)
(i.e. here LD is the loss over the entire training sample while L(b)

D serve as unbiased estimates of ∇θDLD

D is the loss in the batch only,

so for LD the sums run over (1, ..., n) instead of (1, ..., nb)) which allow us to do stochastic

gradient descent on the parameters (θD, θG). The ﬁrst sum in (D1) amounts to the discrim-

inator objective in Arjovsky et al. (2017) while the second sum corresponds to the gradient

penalty suggested by Gulrajani et al. (2017).

Similarly, for the generator we minimise the batch generator loss

L(b)

G =

1
nb

nb(cid:88)

i=1

−D(G(z; θG); θD)

(G1)

corresponding to Equation 8. By alternating between the objectives (D1) and (G1) we can

learn the parameters of G and D. The complete training algorithm of Gulrajani et al. (2017)

is given in Algorithm 2 in pseudo-code. Notice that during the discriminator updates the

gradient is with respect to θD and in the generator updates with respect to θG. Algorithm 2

uses Stochastic Gradient Descent (SGD) to update the parameters, but more sophisticated

optimisation methods could also be applied, e.g. ADAM (Kingma and Ba, 2014).

7

Algorithm 2 Wasserstein GAN with Gradient Penalty (Arjovsky et al., 2017; Gulrajani
et al., 2017).

1: for i = 1, 2, ..., N do:
2:

for j = 1, 2, ...Ndiscriminator do:

(cid:46) Discriminator updates.

z ← Sample(FZ)
xf ake ← G(z; θG)
xreal ← Sample(X )

(cid:46) X is a ﬁxed collection of samples from FX

˜x ← axreal + (1 − a)xf ake
(cid:80)nb
P (b) ← 1
nb
(cid:80)nb
L(b)
D ← 1
nb
θD ← θD − lrD ∇θDL(b)

D

b=1 (||∇˜xD(˜x; θD)||2 − 1)2
b=1 D(xf ake; θD) − D(xreal; θD) + λP (b)

(cid:46) a is drawn from the uniform distribution U(0, 1).
(cid:46) Gradient penalty.

(cid:46) Discriminator loss.

(cid:46) Parameter update, lrD is the learning rate.

(cid:46) Generator updates.

(cid:46) Parameter update, lrG is the learning rate.

(cid:46) Generator loss.

3:

4:

5:

6:

7:

8:

9:

10:

11:

12:

13:

14:

15:

16:

17:

18:

end for

for j = 1, 2, ...Ngenerator do

z ← Sample(FZ)
xf ake ← G(z; θG)

(cid:80)nb
L(b)
G ← 1
nb
θG ← θG − lrG ∇θGL(b)

G

b=1 −D(xf ake; θD)

end for

19:
20: end for

8

The GAN formulation above does not necessarily impose how we should parameterise the

discriminator D and generator G. However, in practice, they are commonly learned using

neural networks with exact parameterisations depending on the application.1 Hornik et al.

(1990) showed that neural networks with fully-connected layers enjoy universal approxima-

tion properties and hence are a natural choice. We do not give an introduction to neural

networks and their terminology but refer to the textbook treatment by Goodfellow, Bengio,

et al. (2016).

Consider a time series process Y = {Yt : t ∈ T } indexed by time t. A time series has the

deﬁning property that information ﬂow is unidirectional, so the state of the process at time

t, Yt, can only depend on past information (Yt−1, Yt−2, ...) while the future is unknown. This

constraint is useful when we choose the parameterisation of G and D.

The GAN in Hyland et al. (2017) relied on recurrent neural networks (RNNs) to model

time series. We pursue a diﬀerent approach and base the generator and discriminator on

stacked dilated temporal convolutions (DTC) used by Oord et al. (2016) for audio generation.

We will refer to this as the TC-architecture. The temporal convolutions are similar to

conventional convolutions – see (Goodfellow, Bengio, et al., 2016, Chp. 3) – but they enforce

the unidirectional ﬂow of information. They were applied to time series forecasting by

Borovykh et al. (2017) and Sen et al. (2019). In particular, Borovykh et al. (2017) showed

that DTC networks outperform RNNs in several forecasting problems and are easier to

train even for long-range dependence. Very recently, Wiese et al. (2020) similarly suggested

temporal convolutions based on (Oord et al., 2016) for ﬁnancial time series modelling with

GANs. The dilation of the temporal convolutions increases the receptive ﬁeld – in context

of time series this is the number of lags that the model can accommodate at once – while

limiting the number of parameters (Oord et al., 2016; Borovykh et al., 2017).

In practice, temporal convolutions can be implemented as conventional one-dimensional

convolutions with appropriate zero-padding applied to the input, see (Oord et al., 2016). If

1For example, Radford et al. (2015) use convolutions for images while (Athey et al., 2019) use fully-

connected layers in the context of treatment eﬀects.

9

Figure 1: Adapted from Figure 2 in (Oord et al., 2016). An illustration of the DTC network.
The output yt at time t is a function of the present and past noise terms (zt, zt−1, zt−2, zt−3).
We generate the output samples as we slide across the noise terms.

we stack d DTC layers with kernel size 2 where the dilation for layer i is 2i then the total

receptive ﬁeld size at the ﬁnal layer will be (Yu and Koltun, 2015)

p =

d
(cid:88)

i=1

2i = 2d+1 − 1.

(10)

For illustration, assume that our generator G consists of d DTC layers. To generate a time

series of length b we slide the DTC layers over a sequence of (b + p) iid noise terms

(z1, z2, ..., zp+b),

zt ∼ FZ

where FZ is some arbitrarily chosen distribution and p is the receptive ﬁeld size given in

Equation 10. During the GAN training, the parameters in the DTC layers learn to transform

this sequence of iid noise into observations from the time series. This is illustrated in Figure 1

for a generator with two DTC layers. On the other hand, the discriminator D considers

sequences of observations from the generated or real time series (y1, y2, ..., yT ) and learns

the parameters in the DTC layers to distinguish between real and generated samples. We

will detail this process in the context of bootstrapping in Section 3, and provide a complete

example of the architecture in Section 4.

10

tt-1tt-2t-3t-4t-5yzzzzzz3. GENERATIVE BOOTSTRAP

We propose the GAN with temporal convolution layers as a method to resample from a time

series process. This procedure is called the Generative Bootstrap (GB). The GB is composed

of two stages: (1) the GAN is trained on an initial sample from the true DGP using blocking

– the training stage. (2) samples are generated from the generator of the trained GAN – the

sampling stage.

The initial sample from the true DGP is re-sampled using a moving block scheme similar

to the Moving Block Bootstrap (MBB) (Kunsch, 1989; Liu, Singh, et al., 1992) and nb of

such blocks constitute a batch of data that are used to perform one iteration over the batch

losses in Equation D1-G1. Many iterations are performed until the GAN losses stabilise.

For sampling, the discriminator is discarded and we feed noise into the generator from an

arbitrary distribution FZ. The generator output is used as a sample to calculate one set of

bootstrap statistics. This procedure is repeated for an arbitrary number of samples and the

collection of statistics is used to form the GB estimates.

We will see that this diﬀers from conventional block bootstrapping in two respects.

Firstly, a conventional MBB would sample blocks of size b with replacement from the initial

sample and stack these into one sample path matching the length of the initial sample. This

stacked sample is then used to calculate bootstrap statistics. In the GB these blocks are not

stacked, but fed as individual samples to train the GAN. Secondly, sampling the GAN does

not have to use stacking and the GAN can provide a sample of any size once it has been

trained. The size of the sample is determined by the noise terms supplied to the generator.

We proceed to discuss the training and sampling stages below. Algorithm 3 provides an

overview.

Traning stage. The training stage mimics Algorithm 2 but uses moving blocks to re-

sample the initial sample, see Line 5 in Algorithm 2. Let y∗

i = (y∗

1,i, y∗

2,i, ..., y∗

T,i) be the initial

sample from the true DGP. We perform a blocking procedure identical to the moving block

bootstrap. Deﬁne each of the (T − b1 − 1) overlapping blocks by B∗

j = (y1j = y∗

1+j,i, ..., y2j =

11

Use Algorithm 2 but batches consist of resampled blocks with size b1.

(cid:46) Training stage

(cid:46) Sampling stage

Algorithm 3 Generative Bootstrap

1: for r = 1, 2, ..., R do:
2:
3: end for
4:
5: for s = 1, 2, ..., S do:
6:

x ← Sample(model, b2)
estimates[s] ← Statistic(x)

7:
8: end for

y∗
1+j+b1,i) where the block size is b1 < T . A batch of training data is given by randomly

sampling nb blocks from {B∗

1, B∗

2, ..., B∗

T −b1−1} without replacement, denote this batch of

true blocks by Y ∗.

Next, we sample the generator noise (see Line 3 of Algorithm 2) from a multivariate stan-

dard normal distribution with an identity variance-covariance matrix, but the distribution

FZ could be selected arbitrarily. To generate a sample path of length b1 we need (b1 + p)

noise terms where p is the receptive ﬁeld in the DTC layers – see Equation 10. The noise

terms are used to generate a block sample Bj = (G(z1, ..., zp; θG), G(z1+1, ..., zp+1; θG), ...,

G(zb1, ..., zb1+p; θG)), see Line 4 of Algorithm 2, and nb of such blocks are generated to form

a batch of fake blocks, denote them by Y. The true and fake samples are fed to the discrim-

inator and it tries to distinguish between them. This requires that the blocks in Y ∗ and Y

have the same size. This procedure of sampling true and fake blocks and feeding them to the

discriminator constitutes the training stage. In practice, we iterate over Equation D1-G1

until the losses stabilise. Equation D1 requires both a fake and true batch per iteration,

while Equation G1 needs only a fake batch.

Sampling stage. Let G( · ; ˆθG) be the learned generator from the training stage. Once

trained, the generator should produce samples mimicking the true DGP that generated the

initial sample y∗

i . To generate a sample of length b2, we ﬁrst sample a sequence of noise

vectors from FZ

zi = (z1,i, z2,i, ..., zb2+p,i),

zt,i ∼ FZ

12

where p is again given in Equation 10. As in the training stage, FZ is a multivariate standard

normal distribution with an identity variance-covariance matrix. Any distribution could be

used, the important point is that the training and sampling stages use the same distribution

for FZ. Next we obtain a generated sample path yi by passing the noise vectors through the

learned generator,

yi = (G(z1,i, ..., zp,i; ˆθG), G(z1+1,i, ..., zp+1,i; ˆθG), ..., G(zb,i, ..., zb2+p,i; ˆθG)).

A single sequence of innovation vectors zi = (z1,i, ..., zb2+p,i) generates one sample path yi of

length b2. We can repeat this process to obtain an arbitrary number of sample paths.

Under the proposed TC-architecture the generator can sample a block of any length from

the underlying process and hence we are not restricted to the block size on which the model

was trained, i.e. it is perfectly acceptable if b1 (cid:54)= b2. This does not necessarily hold for all

choices of architecture, e.g. a fully-connected network would not have this property. This

is a very attractive feature of the TC and GAN approach as it alleviates the need to stack

individual blocks in a way that might break the dependence structure of the time series. We

can simple choose the sampling block size to be equal to the size of the initial sample path,

so b2 = T .

When b2 < T we refer to it as blocked sampling, while b2 = T is called complete sampling.
Bootstrap statistics. Let G(·; ˆθG) be the learned generator from the training stage

that has been trained on a single initial sample y∗

i from the true DGP. We now discuss how

to calculate bootstrap statistics on the GAN samples. Assume that we are interested in

parameter φ which has a suitable estimator ˆφ. We use the sampling procedure from the

previous section to obtain m samples from the learned generator G( · ; ˆθG), denote these

samples by (y1, y2, ..., ym) where yi = (y1,i, ..., yT,i), i = 1, ..., m. Each yi is considered a

realisation of the DGP that produced the initial training sample for the GAN. We calculate

the bootstrap statistics ˆφi ≡ ˆφ(yi),

i = 1, ..., m resulting in m estimates ( ˆφ1, ˆφ2, ..., ˆφm).

13

Similar to conventional bootstrapping (Efron, 1981), the GB variance estimate of ˆφ is

ˆσGB, ˆφ =

1
m

(cid:88)

i

( ˆφi − ˆφGB)2.

(11)

The (1 − α) GB conﬁdence interval (CI) for φ is the (1 − α) percentile CI (Efron, 1981)

constructed using the empirical quantiles2 (α/2, 1 − α/2) of ( ˆφi)i,

ˆIφ,1−α =

(cid:104) ˆφ(α/2), ˆφ(1−α/2)

(cid:105)

.

(12)

4. SIMULATIONS

In this section we will illustrate the performance of the GB by using simulations and by mak-

ing comparisons to the established CBB approach for bootstrapping dependent processes.

For simplicity of exposition we base our illustrations on the AR(1) as the data generating

process.

4.1. AR(1) process

The simulation design is as follows. The true DGP is a zero mean and stable AR(1) process

yt = φ yt−1 + (cid:15)t,

(cid:15)t ∼ N(0, 1)

(13)

with φ = 0.5, 0.8, 0.9. For each replication, a sample path of length T = 1, 000 is generated

from Equation 13. This sample is used to train the GAN with a training block size of

b1 = 150 and batch size nb = 64. Once training is complete, we sample 10, 000 sample paths

from the GAN. These samples are used for two purposes:

(a) we compare the samples generated by the proposed GAN to the known properties of

the DGP under complete sampling b2 = 1, 000. We use the generated samples to estimate the

2A possible improvement on this conﬁdence interval would be the bias-correction techniques outlined in

(Efron, 1987).

14

autocorrelation (ACF) and partial autocorrelation (PACF) functions over 1, 000 replications.

(b) we compare GB and the CBB for conﬁdence interval estimation, i.e., empirical cov-

erage, of the least-squares estimator ˆφLS of φ. The GB is run for 1, 000 replications and

the CBB is run for 5, 000 replications. The CBB resamples from the same initial sample as

is used to train the GB. We consider CBB block sizes b1 = 50, 100, 150. The GB training

block size equals 150. The number of replications for GB is lower as the simulation time

is considerably higher than for CBB. A GB replication takes around 20-30 minutes while

it is less than a minute for CBB. It is important to note that, in both the CBB and GB,

we do not specify the dynamics of the true DGP. The GB assumes that the dynamics can

be approximated by some functions of the noise vectors but these functions are not fully

speciﬁed.

The following section details the hyper parameters and training of the GAN. The two

succeeding sections discuss the simulation results – ﬁrst the correlation structure of the

generated samples and secondly the higher-level statistics in a bootstrapping context.

GAN implementation details. We discuss the hyper parameters and network design

of the GAN.

The discriminator has 6 temporal convolution layers with common kernel size 2 and

dilations (1, 2, 4, 8, 16). The ﬁlters are (8, 16, 32, 32, 64, 64). The output from temporal layers

number 1, 2, and 6 are run through adaptive max pooling (AMP) with feature size 16 and

concatenated into a feature vector of size 48. This is followed by two fully connected layers

that regress into a single output unit. All layers use leaky ReLU activation (Maas et al.,

2013) except for the ﬁnal layer which has no activation function. The leaky ReLU avoids

the zero-gradient problem of conventional ReLUs during training (Maas et al., 2013). The

generator has 6 temporal convolution layers that directly outputs a sample path. The ﬁlters

are (128, 64, 32, 32, 16, 1). Except for the last layer, all layers use the Tan activation function

as it – unlike ReLU – is symmetric. The total number of (trainable) discriminator parameters

is 233, 609 while the generator has 89, 921 (trainable) parameters.

15

The generator noise is sampled from a multivariate standard normal distribution with an

identity variance-covariance matrix. To generate a sample path of length b we need (b + p)

noise terms where p is the receptive ﬁeld size in the DTC layers – see Equation 10 – and b

is either b1 or b2 corresponding to training or sampling stage. The dimension of the noise

term is a hyper parameter and can be chosen arbitrarily, in our simulations we use 256. If

we stack all the noise terms needed to produce a sample path of size b then we obtain a

(b + p) × 256 matrix with iid standard normal distributed entries.

The GAN is trained for 5,000 steps based on a single sample from the DGP. We do not

employ any (early) stopping criterion, so the GAN is always trained till the ﬁnal step. The

training involves iteratively minimising the batch losses, see Equation D1-G1. Instead of

stochastic gradient descent, we use the more complex Adam algorithm as it can accelerate

training, see (Kingma and Ba, 2014). Table 1 in the Appendix lists all hyper parameters for

the GAN in this paper.

(a) ACF and PACF properties of the GAN samples. We compare the samples

produced by the GAN and CBB against the theoretical properties of the AR(1) process.3

A common discussion is whether the GAN has learned to produce new samples or if it

simply reproduces the original samples perfectly. If the generative model learned to perfectly

replicate the original sample then the method would perform approximately on-par with

CBB. Favourable bootstrapping characteristics of the GAN relative to CBB could indicate

that the GAN has an advantage in capturing the dynamics of the DGP and that it does not

simply replicate blocks of the original sample.

The theoretical autocorrelation function (ACF) for an AR(1) process is given by γ(j) ≡

Cor(yt, yt−j) = φj for φ = 0.5, 0.8, 0.9. We estimate the ACF using generated samples under

the complete sampling scheme. The ACF estimates are averaged over 1, 000 replications.

Figure 2 plots the estimated ACF (full line) against the theoretical ACF (dashed line). In

Figure 2 we have also included the interquartile range (IQR) for the theoretical ACF and

3For the implementation of the CBB we have used the Python library by Sheppard (2020).

16

Figure 2: Theoretical (dashed line, blue ribbon) and sample autocorrelation functions for
CBB (green ribbon) and GAN (red ribbon) resamples when φ = 0.5, 0.8, 0.9. Block size for
CBB is 150 and training block size for GAN is 150. The ACF estimates and conﬁdence bands
are based on 1, 000 replications of the generative model with 10, 000 samples per replication.

17

0.000.250.500.751.00Averaged ACF−0.250.000.250.500.751.00Averaged ACF0.000.250.500.751.00Averaged ACF−0.250.000.250.500.751.00Averaged ACF0.000.250.500.751.00Averaged ACF0.000.250.500.751.00Averaged ACFf = 0.9f = 0.9f = 0.8f = 0.8f = 0.5f = 0.50102030405001020304050010203040500102030405001020304050LagMethodCBBTheoretical01020304050LagMethodGBTheoreticalfor the ACF estimated across the 1, 000 replications. If the GAN has learned the dynamics

of the AR(1) process, then the estimated and the theoretical ACF should be similar and

the theoretical and the estimated IQR should be overlapping. Clearly, for higher values of

the autoregressive parameter φ the persistency of the process is stronger and challenges the

GAN to learn longer range dependencies.

From Figure 2 the estimated ACFs are close to their theoretical counterparts for all lags

when φ = 0.5. For φ = 0.8 there is a small upwards bias in the estimated ACFs that is

larger for the CBB, particularly, for the intermediate range of lag lengths, i.e, lags 5-20. For

φ = 0.9 there is a small but noticeable bias in the estimated ACFs for all lags considered

for both CB and CBB. The bias is again uniformly larger for the CBB. Importantly, all

theoretical ACFs are well within the IQR of the estimated ACFs.

For φ = 0.5 the estimated IQR of the CBB (green ribbon) is almost identical to the

theoretical IRQ (blue ribbon). However, for φ = 0.8 and φ = 0.9 the upper limit of the

estimated IQRs for CBB seem to be considerably downward biased for all lags. Noticably,

the estimated IQSs for CB (red ribbon) appear to be much less sensitive to the value of φ

and the estimated IQSs for CB are only marginally wider than the theoretical IQRs. We

ﬁnd these results very encouraging.

Next we turn to the partial autocorrelation function (PACF). For an AR(p) process the

PACF is zero for lags larger than p. The AR(1) process is expected to have PACF equal

to φ at lag 1 and zero PACF for all following lags. Figure 3 depicts the estimated PACF

using the GAN samples and plots it against the theoretical PACF (horizontal dotted line).

The black horizontal marks denote the estimated IQRs. The estimated PACFs have the

expected behaviour for φ = 0.5, 0.8 with values close to 0.5 and 0.8 at lag 1 respectively and

with values very close to zero for all remaining higher order lags. For the highly persistent

case φ = 0.9 the estimated PACF is slightly more imprecise with a notable non-zero PACF

at lag 2. However, overall, the PACF based on the GAN sample clearly suggests that the

underlying time series under consideration is a highly persistent AR(1) process.

18

Figure 3: Top panel: Sample partial autocorrelation function (blue bars) for φ = 0.5, φ = 0.8
and φ = 0.9. The estimates are based on 1, 000 replications of the generative model with
10, 000 samples per replication. The black marks depict the interquartile range (IQR) of the
estimates across the 1, 000 replications.

(b) Bootstrapping the least-squares estimator. We apply the GAN for re-sampling

and examine if it recovers higher-level statistics, in particular, the sampling distribution of

the usual least-squares (LS) estimator ˆφLS of the autoregressive parameter φ.

The simulation design is identical to that initially described. We obtain 10, 000 sample

paths from the GAN across 1, 000 replications with φ = 0.5, 0.8, 0.9. For each replication,

the GB variance of ˆφLS and the GB conﬁdence intervals are constructed as in Section 3.
Using the usual asymptotic approximation, the LS estimator ˆφLS has asymptotic variance

(1 − φ2)−1.

Figure 4 contains the main simulation results for the GB using complete sampling (b2 =

1, 000). For values of φ in the range 0 − 0.75 the GB in general produces far better empirical

coverage than CBB for all nominal conﬁdence levels, but in particularly for the levels 0.99,

0.95, and 0.9. For the highly persistent processes none of the re-sampling methods considered

have good empirical coverage. In these cases the empirical coverage of the GB is at par with

the CBB with a block size equal to 100. Not surprisingly, the CBB with the largest block

19

f = 0.5f = 0.8f = 0.91234567891234567891234567890.000.250.500.75LagPACFFigure 4: Empirical coverage of percentile conﬁdence intervals – with nominal conﬁdence
levels (0.99, 0.95, 0.90, 0.80) – for the CBB and the GB (using complete sampling) under
diﬀerent choices of the autoregressive parameter φ. The horizontal lines depict the corre-
sponding desired nominal conﬁdence levels.

size (=150) here has the best coverage.

It is very likely that the performance of the GB for highly persistent processes could be

improved by increasing the number of layers in temporal convolution network. Recall, that

a temporal convolution network with d layers and ﬁxed kernel size 2 accounts for at-most

2d+1 − 1 lags (the receptive ﬁeld), hence an increase in d might rectify this problem . How to

select the optimal number of d layers in the GB procedure as a function of the persistence

20

lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllConfidence level 0.9Confidence level 0.8Confidence level 0.99Confidence level 0.950.000.250.500.751.000.000.250.500.751.000.60.70.80.90.600.650.700.750.800.60.70.80.91.00.60.70.80.9fEmpirical coverageMethodllCBB (100)CBB (150)CBB (50)GBof original process is ongoing work.

5. CONCLUDING REMARKS

GANs provide a promising approach for simulating time series data. Our results suggest that

GANs can accurately learn the dynamics of common autoregressive time series processes

using temporal convolutional networks.

In addition, it seems compelling that the GAN

appears to improve empirical coverage in bootstrapping of dependent data when compared

to the circular block bootstrap. This gives credibility to the use of GANs on data from time

series processes that are unknown.

It is important to note that the various dependent bootstraps have a theoretical justiﬁ-

cation and their properties have been theoretically derived, see e.g. the overview in (Lahiri,

1999). The GAN and GB currently do not have this theoretical reassurance.

The GAN relies on a large number of hyper parameters and design choices. We have

not investigated how sensitive our results are to these but research on this is ongoing. We

have used sensible defaults for batch sizes, learning rates, gradient penalty, update iteration

scheme, activation functions, optimiser but these are by no means optimal choices. This is

a general shortcoming in the GAN literature and little is known about how to optimally

choose these values.

Our simulations also rely on a simple and basic time series process. It would be fruitful

to consider the performance on more general stationary processes (ARMA) and in settings

where the error term has stochastic variance, e.g. GARCH.

21

REFERENCES
Arjovsky, Martin, Soumith Chintala, and L´eon Bottou (2017). “Wasserstein GAN”. In: arXiv

preprint 1701.07875. url: https://arxiv.org/abs/1701.07875.

Athey, Susan et al. (2019). “Using Wasserstein Generative Adversarial Networks for the
Design of Monte Carlo Simulations”. In: arXiv preprint 1909.02210. url: https : / /
arxiv.org/abs/1909.02210.

Biau, G., B. Cadre, et al. (2018). “Some Theoretical Properties of GANs”. In: arXiv preprint
1803.07819 (forthcoming in Annals of Statistics). url: https://arxiv.org/abs/1803.
07819.

Biau, G´erard, Maxime Sangnier, and Ugo Tanielian (2020). “Some Theoretical Insights into

Wasserstein GANs”. In: url: https://arxiv.org/abs/2006.02682.

Borovykh, Anastasia, Sander Bohte, and Cornelis W. Oosterlee (2017). “Conditional Time
Series Forecasting with Convolutional Neural Networks”. In: arXiv preprint 1703.04691.
url: https://arxiv.org/abs/1703.04691.

Efron, Bradley (1981). “Nonparametric Standard Errors and Conﬁdence Intervals”. In: The
Canadian Journal of Statistics / La Revue Canadienne de Statistique 9.2, pp. 139–158.
— (1987). “Better Bootstrap Conﬁdence Intervals”. In: Journal of the American Statistical

Association 82.397, pp. 171–185.

Goodfellow, Ian, Yoshua Bengio, and Aaron Courville (2016). Deep learning. MIT press.
Goodfellow, Ian, Jean Pouget-Abadie, et al. (2014). “Generative Adversarial Nets”. In: Ad-

vances in neural information processing systems, pp. 2672–2680.

Gulrajani, Ishaan et al. (2017). “Improved training of Wasserstein GANs”. In: Advances in

neural information processing systems, pp. 5767–5777.

Haas, Moritz and Stefan Richter (2020). “Statistical analysis of Wasserstein GANs with
applications to time series forecasting”. In: url: https://arxiv.org/abs/2011.03074.
Heusel, Martin et al. (2017). “GANs Trained by a Two Time-Scale Update Rule Converge
to a Nash Equilibrium”. In: CoRR abs/1706.08500. url: http://arxiv.org/abs/1706.
08500.

Hornik, Kurt, Maxwell Stinchcombe, and Halbert White (1990). “Universal approximation
of an unknown mapping and its derivatives using multilayer feedforward networks”. In:
Neural networks 3.5, pp. 551–560.

Hyland, Stephanie L, Crist´obal Esteban, and Gunnar R¨atsch (2017). “Real-valued (medical)
time series generation with recurrent conditional GANs”. In: arXiv preprint 1706.02633.
url: https://arxiv.org/abs/1706.02633.

Kaji, Tetsuya, Elena Manresa, and Guillaume Pouliot (2018). Deep Inference: Artiﬁcial In-
telligence for Structural Estimation. Tech. rep. url: https://events.barcelonagse.
eu/live/files/2773-elenamanresa66433pdf.

Kingma, Diederik P and Jimmy Ba (2014). “Adam: A method for stochastic optimization”.

In: arXiv preprint 1412.6980. url: https://arxiv.org/abs/1412.6980.

Kunsch, Hans R (1989). “The jackknife and the bootstrap for general stationary observa-

tions”. In: The annals of Statistics, pp. 1217–1241.

Lahiri, Soumendra N (1999). “Theoretical comparisons of block bootstrap methods”. In:

Annals of Statistics, pp. 386–404.

22

Liu, Regina Y, Kesar Singh, et al. (1992). “Moving blocks jackknife and bootstrap capture

weak dependence”. In: Exploring the limits of bootstrap 225, p. 248.

Maas, Andrew L, Awni Y Hannun, and Andrew Y Ng (2013). “Rectiﬁer nonlinearities im-

prove neural network acoustic models”. In: Proc. icml. Vol. 30. 1.

Oord, Aaron van den et al. (2016). “WaveNet: A Generative Model for Raw Audio”. In:

arXiv preprint 1609.03499. url: https://arxiv.org/abs/1609.03499.

Paparoditis, Efstathios and Dimitris N. Politis (2001). “Tapered Block Bootstrap”. In:

Biometrika 88.4, pp. 1105–1119.

Politis, Dimitris N and Joseph P Romano (1992). “A circular block-resampling procedure

for stationary data”. In: Exploring the limits of bootstrap 2635270.

Radford, Alec, Luke Metz, and Soumith Chintala (2015). “Unsupervised representation
learning with deep convolutional generative adversarial networks”. In: arXiv preprint
1511.06434. url: https://arxiv.org/abs/1511.06434.

Salimans, Tim et al. (2016). “Improved Techniques for Training GANs”. In: CoRR abs/1606.03498.

url: http://arxiv.org/abs/1606.03498.

Sen, Rajat, Hsiang-Fu Yu, and Inderjit Dhillon (2019). “Think Globally, Act Locally: A
Deep Neural Network Approach to High-Dimensional Time Series Forecasting”. In: arXiv
preprint 1905.03806. url: https://arxiv.org/abs/1905.03806.

Shao, Xiaofeng (Mar. 2010). “The Dependent Wild Bootstrap”. In: Journal of the American

Statistical Association 105.489, pp. 218–235.

Sheppard, Kevin (2020). “bashtage/arch: Release 4.15 (Version 4.15).” In: Zenodo. url:

https://doi.org/10.5281/zenodo.593254.

Smith, Kaleb E and Anthony O Smith (2020). “Conditional GAN for timeseries generation”.

In.

Wiese, Magnus et al. (Apr. 2020). “Quant GANs: deep generation of ﬁnancial time series”.

In: Quantitative Finance, pp. 1–22.

Yu, Fisher and Vladlen Koltun (2015). “Multi-Scale Context Aggregation by Dilated Con-
volutions”. In: arXiv preprint 1511.07122. url: https://arxiv.org/abs/1511.07122.

23

APPENDIX

Hyper parameter

Value

Discriminator learning rate, lrd
Generator learning rate, lrd
Gradient penalty, λ
Batch size, nb
Discriminator init updates, Ninit
Discriminator updates, Ndiscriminator
Generator updates, Ngenerator

0.00025
0.00025
20
64
50
5
1
Initial weight distribution N(0, 0.02)
10−8
0.5
0.9

Adam optimiser, (cid:15)
Adam optimiser, β1
Adam optimiser, β2

Table 1: Generative Bootstrap (GB) hyper paramaters.

24

