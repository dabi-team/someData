3D Shape Reconstruction from Free-Hand Sketches

Jiayun Wang, Jierui Lin, Qian Yu, Runtao Liu, Yubei Chen, and Stella X. Yu

1

2
2
0
2

n
a
J

9
1

]

V
C
.
s
c
[

2
v
4
9
6
9
0
.
6
0
0
2
:
v
i
X
r
a

Abstract—Sketches are the most abstract 2D representations
of real-world objects. Although a sketch usually has geometrical
distortion and lacks visual cues, humans can effortlessly envision
a 3D object from it. This suggests that sketches encode the
information necessary for reconstructing 3D shapes. Despite great
progress achieved in 3D reconstruction from distortion-free line
drawings, such as CAD and edge maps, little effort has been made
to reconstruct 3D shapes from free-hand sketches. We study this
task and aim to enhance the power of sketches in 3D-related
applications such as interactive design and VR/AR games.

Unlike previous works, which mostly study distortion-free
line drawings, our 3D shape reconstruction is based on free-
hand sketches. A major challenge for free-hand sketch 3D
reconstruction comes from the insufﬁcient training data and free-
hand sketch diversity, e.g. individualized sketching styles. We
thus propose data generation and standardization mechanisms.
Instead of distortion-free line drawings, synthesized sketches are
adopted as input training data. Additionally, we propose a sketch
standardization module to handle different sketch distortions and
styles. Extensive experiments demonstrate the effectiveness of
our model and its strong generalizability to various free-hand
sketches. Our code is publicly available.

Index Terms—Sketch, data insufﬁciency,

interactive design,

shape reconstruction, 3D reconstruction

I. INTRODUCTION

H UMAN free-hand sketches are the most abstract 2D

representations for 3D visual perception. Although a
sketch may consist of only a few colorless strokes and exhibit
various deformation and abstractions, humans can effortlessly
envision the corresponding real-world 3D object from it. It
is of interest to develop a computer vision model that can
replicate this ability. Although sketches and 3D representations
have drawn great interest from researchers in recent years,
these two modalities have been studied relatively indepen-
dently. We explore the plausibility of bridging the gap between
sketches and 3D, and build a computer vision model to recover
3D shapes from sketches. Such a model will unleash many
applications, like interactive CAD design and VR/AR games.
With the development of new devices and sensors, sketches
and 3D shapes, as representations of real-world objects beyond
natural images, become increasingly important. The popularity
of touch-screen devices makes sketching not a privilege to
professionals anymore and increasingly popular. Researchers
have applied sketch in tasks like image retrieval [4], [5] and
image synthesis [6], [7] to leverage its power in expression.
Furthermore, as depth sensors, such as structured light device,
LiDAR, and TOF cameras, become more ubiquitous, 3D data

the

authors

All
Berkeley
{peterwg,jerrylin0928,runtao liu,yubeic,stellayu}@berkeley.edu).
is with Beihang University, Beijing, China

Berkeley,

Yu
CA,

Q. Yu

except

ICSI,

USA

are

Q.

/

with

UC
(e-mail:

(e-mail:

qianyu@buaa.edu.cn) and UC Berkeley / ICSI, Berkeley, CA, USA.

become an emerging modality in computer vision. 3D recon-
struction, the process of capturing the shape and appearance
of real objects, is an essential topic in 3D computer vision.
3D reconstruction from multi-view images has been studied
for many years [8], [9], [10]. Recent works [11], [12], [13]
have further explored 3D reconstruction from a single image.
Despite these trends and progress, there are limited works
connecting 3D and sketches. We argue that sketches are
abstract 2D representations of 3D perception, and it is of great
signiﬁcance to study sketches in a 3D-aware perspective and
build connections between two modalities. Researchers have
explored the potential of distortion-free line drawings (e.g.
edge maps) for 3D modeling [14], [15], [16]. These works are
based on distortion-free line drawings and generalize poorly to
free-hand sketches (Fig.1(Left)). Furthermore, the role of line
drawings in such works is to provide geometrical information
for the subsequent 3D modeling. Some other works [1], [3]
employ neural networks to reconstruct 3D shapes directly from
line drawings. However, their decent reconstructions come
with two major limitations: 1) they use distortion-free line
drawings as training data, which makes such models hard to
generalize to free-hand sketches; 2) they usually require inputs
depicting the object from multi-views to achieve satisfactory
outcomes. Therefore, such methods cannot reconstruct the 3D
shape from a single-view free-hand sketch well, as we show
later in the experiment section. Other works such as [2],
[17] tackle 3D retrieval instead of 3D shape reconstruction
from sketches. Retrieved shapes come from the pre-collected
gallery set and may not resembles novel sketches well. Overall,
reconstructing a 3D shape from a single free-hand sketch is
still left not well explored.

In this work, we explore single-view free-hand sketch-
based 3D reconstruction (Fig.1(Middle)). A free-hand sketch
is deﬁned as a line drawing created without any additional
tool. As an abstract and concise representation, it is different
from distortion-free line drawings (e.g. edge maps) since it
commonly has some spatial distortions, but it can still reﬂect
the essential geometric shape. 3D reconstruction from sketch
is challenging due to the following reasons:

1) Data insufﬁciency. Paired sketch-3D datasets are rare
although there exist several large-scale sketch datasets
and 3D shape datasets, respectively. Furthermore, col-
lecting sketch-3D pairs can be very time-consuming and
expensive than collecting sketch-image pairs, as each 3D
shape could be sketched from various viewing angles.
2) There is a misalignment between two representations. A
sketch depicts an object from a certain view while a 3D
shape can be viewed from multiple angles due to the
encoded depth information.

3) Due to the nature of hand drawing, a sketch is usually
geometrically imprecise with a individual style com-

 
 
 
 
 
 
2

Fig. 1. We study 3D reconstruction from a single-view free-hand sketch, differing from previous works [1], [2], [3] which use multi-view distortion-free
line-drawings as training data. (Left). While previous works [1], [2] employ distortion-free line drawings (e.g. edge-maps) as proxies for sketches, we show
that a model trained on synthesized sketches can generalize better to free-hand sketches. We also show that the proposed sketch standardization module makes
the method generalizes well to free-hand sketches by dealing with different sketching styles as well as distortions (Middle). Our model reconstructs 3D shapes
from various free-hand sketches and may unleash many practical applications such as real-time 3D modeling with sketches (Right). A real-time sketching
demo is available at https://streamable.com/z76m47.

pared to the real object. Thus a sketch can only provide
suggestive shape and structural information. In contrast,
a 3D shape is faithful to its corresponding real-world
object with no geometric deformation.

To address these challenges, we propose a single-view
sketch-to-3D shape reconstruction framework. Speciﬁcally, it
takes a sketch from an arbitrary angle as input and re-
constructs a 3D point cloud. Our model cascades a sketch
standardization module U and a reconstruction module G.
U handles various sketching styles/distortions and transfers
inputs to standardized sketches while G takes a standardized
sketch to reconstruct the 3D shape (point cloud) regardless of
the object category. The key novelty lies in the mechanisms
we propose to tackle the data insufﬁciency issue. Speciﬁcally,
we ﬁrst train an photo-to-sketch model on unpaired large-
scale datasets. Based on the model, sketch-3D pairs can be
automatically generated from 2D renderings of 3D shapes.
Together with the standardization module U which uniﬁes
input sketch styles, the synthesized sketches provide sufﬁcient
information to train the reconstruction model G. We con-
duct extensive experiments on a composed sketch-3D dataset,
spanning 13 classes, where sketches are synthesized and 3D
objects come from the ShapeNet dataset [18]. Furthermore, we
collect an evaluation set, which consists of 390 real sketch-3D
pairs. Results demonstrate that our model can reconstruct 3D
shapes with certain geometric details from real sketches under
different styles, stroke line-widths, and object categories. Our
model also enables practical applications such as real-time 3D
modeling with sketches (Fig.1(Right)).

To summarize, our work makes the following contributions:

1) We pioneer to study the plausibility of reconstructing
3D shapes from single-view free-hand sketches. The
proposed model demonstrates its robust performance on
real sketches.

2) We propose a novel framework for this task and explore

various design choices to overcome challenges.

3) To handle the data insufﬁciency problem, we propose to
train on synthetic sketches generated by a GAN-based
model. Moreover, a sketch standardization module is
introduced to make the model generalize to free-hand
sketches better. The proposed sketch generation method
can also ease the generalization issue for many other
stroke-based applications, such as OCR.

II. RELATED WORKS

A. 3D Reconstruction from Images

SfM [10] and SLAM [8] achieve success in handling
multi-view 3D reconstructions in various real-world scenarios.
However, their reconstructions can be limited by insufﬁcient
input viewpoints and 3D scanning data. Deep-learning-based
methods have been proposed to further improve reconstruc-
tions by completing 3D shapes with occluded or hollowed-out
areas [19], [9], [20].

In general, recovering the 3D shape from a single-view
image is an ill-posed problem. Attempts to tackle the problem
include 3D shape reconstructions from silhouettes [12], shad-
ing [11], and texture [21]. However, these methods need strong
presumptions and expertise in natural images [22], limiting
their usage in real-world scenarios. Generative adversarial
networks (GANs) [23] and variational autoencoders (VAEs)
[24] have achieved success in image synthesis and enabled
[25] 3D shape reconstruction from a single-view image. Fan
et al. [13] further adopt point clouds as 3D representation,
enabling models to reconstruct certain geometric details from
an image. They may not directly work on sketches as many
visual cues are missing.

3D reconstruction networks are designed differently depend-
ing on the output 3D representation. 3D voxel reconstruction
networks [26], [27], [28] beneﬁt from many image processing
networks as convolutions are appropriate for voxels. They are
usually constrained to low resolution due to the computational
overhead. Mesh reconstruction networks [29], [30] are able to

Edge mapSketchstandardizationDistortion-free line drawings to 3D Free-hand sketch to 3D Pervious worksOurs3

Fig. 2. The pipeline of our model. The model consists of three major components: sketch generation (red box), sketch standardization (orange box), and
3D reconstruction (green box). To generate synthesized sketches for training the model, we ﬁrst render 2D images for a 3D shape (in the format of mesh)
from multiple viewpoints, and then employ an image-to-sketch translation model to generate sketches of corresponding views. The standardization module is
introduced to handle sketches with different styles and distortions. Deformation D2 is only used in training for augmentation such that the model is robust
to geometric distortions of sketches. During evaluation, free-hand sketches are dilated (D1) and reﬁned (R) so their style matches that of training sketches.
In the 3D reconstruction network, a view estimation module is adopted to align the output’s view and the corresponding ground-truth 3D shape.

directly learn from meshes, where they suffer from topology
issues and heavy computation [31]. We adopt point cloud
representation as it can capture certain 3D geometric details
with low computational overhead. Reconstructing 3D point
clouds from images has been shown to beneﬁt from well-
designed network architectures [13], [32], latent embedding
matching [33], additional image supervision [34], etc.

B. Sketch-Based 3D Retrievals and Reconstructions

Free-hand sketches are used for 3D shape retrieval [2], [17]
given their power in expression. However, retrieval methods
are signiﬁcantly constrained by the gallery dataset. Precise
sketching is also studied in the computer graphics community
for 3D shape modeling or procedural modeling [14], [35], [16].
These works are designed for professionals and require addi-
tional information for shape modeling, e.g., surface-normal,
procedural model parameters.

Delanoy et al. [1] ﬁrst employ neural networks to learn
3D voxels from line-drawings. While it achieves impressive
performance, this model has several limitations: 1) The model
uses distortion-free edge map as training data. While working
on some sketches with small distortions, it cannot generalize
to general free-hand sketches. 2) The model requires multiple
inputs from different viewpoints for a satisfactory result. These
limitations prevent the model from generalizing to real free-
hand sketches. Recent works also explore reconstructing 3D
models from sketches with direct shape optimization [36],
differential renderer [37], and unsupervised learning [38].
Unlike the existing works, the proposed method in this work
reconstructs the 3D point cloud based on a single-view free-
hand sketch. Our model may make 3D reconstruction and its
applications more accessible to the general public.

III. 3D RECONSTRUCTION FROM SKETCHES

The proposed framework is composed of three modules
(Fig.2). To deal with the data insufﬁciency issue, we ﬁrst
synthesize sketches as the training set (Sec. III-A). The
module U transfers an input sketch to a standardized sketch
(Sec. III-B). After that, the module G takes the standardized

sketch to reconstruct a 3D shape (point clouds) (Sec. III-C).
In Sec. IV-A, we present details of a new sketch-3D dataset,
which is collected for evaluating the proposed model.

A. Synthetic Sketch Generation

To the best of our knowledge, there exists no paired sketch-
3D dataset. While it is possible to resort to edge maps [1],
edge maps are different from sketches (as shown in the 3rd
and 4th rows of Fig.3). We show that the reconstruction model
trained on edge maps cannot generalize well to real free-hand
sketches in Sec. IV-D. Thus it is crucial to ﬁnd an efﬁcient
and reliable way to synthesize sketches for 3D shapes. Inspired
by [6], we employ a generative model to synthesize sketches
from rendered images of 3D shapes. Fig.2(Left) depicts the
procedure. Speciﬁcally, we ﬁrst render m images for each 3D
shape, where each image corresponds to a particular view of
a 3D shape. We then adopt the model introduced in [6] to
synthesize gray-scale sketches images, denoted as {Si|Si ∈
RW ×H }, as our training data. W, H refer to the width and
height of a sketch image.

B. Sketch Standardization

Free-hand sketches usually have strong individual styles and
geometric distortions. Due to the large gap between the free-
hand sketches and the synthesized sketches, directly using the
synthesized sketches as training data would not lead to a robust
model. The main issues are that the synthesized sketches have
a uniform style and they do not contain enough geometric
distortions. Rather, the synthesized sketches can be treated
as an intermediate representation if we can ﬁnd a way to
project a free-hand sketch to the synthesized sketch domain.
We propose a zero-shot domain translation technique,
the
sketch standardization module, to achieve this domain adaption
goal without using the free-hand sketches as the training
data. The training of the sketch standardization module only
involves synthesized sketches. The general idea is to project
a distorted synthesized sketch to the original synthesized
sketch. The training consists of two parts: 1) since the free-
hand sketches usually have geometric distortions, we apply

𝑈𝐺Dilation𝐷!Refinement 𝑅NNView estimation3D rotationDeformation𝐷"Image-to-sketch3DshapeRendered imagesSynthesizedsketchesRenderSketchGenerationTraining SketchStandardization3DReconstructionInput Sketch3D ReconstructionStandardizedSketchmultiple views4

Fig. 3. Synthesized sketches (2nd row) are visually more similar to free-hand sketches (4th row) than edge maps (3rd row) as they contain distortions and
emphasize perceptually signiﬁcant contours. Note that newly collected free-hand sketches (4th row) are used for evaluation only. Additionally, applying the
standardization module, the standardized free-hand sketches (last row) share a uniform style similar to that of training data.

predeﬁned distortion augmentation to the input synthesized
sketches ﬁrst. 2) A geometrically distorted synthesized sketch
still has a different style and line style compared to the
free-hand sketches. Thus, the ﬁrst stage of the standardiza-
tion is to apply a dilation operation. The dilation operation
would project distorted synthesized sketches and the free-hand
sketches to the same domain. Then, a reﬁnement network
follows to project the dilated sketch back to the synthesized
sketch domain.

In summary, as shown in Fig.2, the standardization module
U ﬁrst applies dilation operator D2 to the input sketch,
which is followed by a reﬁnement operator R to transfer to
the standardized synthesized-sketch style (or training-sketch
style) (cid:101)Si, i.e. U = R ◦ D2. R is implemented as an image
translation network. During training, a synthesized sketch (cid:101)Si
is ﬁrst augmented by the deformation operator D1 to mimic
the drawing distortion, and then U aims to project it back to
(cid:101)Si. Please note that D1 would not be used during the testing.
We illustrate the standardization process in Fig.2(Right) with
more details in the following.

Deformation. During training of U , each synthesized input
sketch is randomly deformed with moving least squares [39]
to make the distortion is local and rigid. Speciﬁcally, we
randomly sample a set of control points on sketch strokes
and denote them as p, and denote the corresponding deformed
point set as q. Following moving least squares, we solve for
the best afﬁne transformation lv(x) such that:

(cid:88)

min

wi|lv(pi) − qi|2

i

(1)

Style Translation. Adapting to unknown input free-hand
sketch style during inference can be considered as a zero-
shot domain translation problem, which is challenging. In-
spired by [40], we ﬁrst dilate the augmented training sketch
strokes with 4 pixels and then use image-to-image translation
network Pix2Pix [41] to translate the dilated sketches to the
un-distorted synthesized sketches. During inference, we also
dilate the free-hand sketches and apply the trained Pix2Pix
model such that the style of an input free-hand sketch could
be adapted to the synthesized sketch style during training.
The dilation step can be considered as introducing uncertainty
for the style adaption. Further, we show in Section IV-E
that the proposed style standardization module could be used
as a general zero-shot domain translation technique, which
generalizes to more applications such as sketch classiﬁcation
and zero-shot image-to-image translation.

A More General Message: Zero-Shot Domain Transla-
tion. We illustrate in Fig.4 a more general message of the
standardization module: it can be considered as a general
method for zero-shot domain translation. Consider the follow-
ing problem: we would like to build a model to transfer domain
X to domain Z but we do not have any training data from
domain X. We propose a general idea to solve this problem
is to build an intermediate domain Y as a bridge such that:
1) we can translate data from domain X to domain Y and 2)
we can further translate data from domain Y to domain Z.
We give two examples in the caption of Fig.4 and provided
experimental results in Section IV-E.

1
where pi and qi are row vectors and weights wi =
|pi−v|2α .
Afﬁne transformation can be written as lv(pi) = piM +T . We
add constraint M T M = I to make the deformation is rigid
to avoid too much distortion. Details of solving Eqn.1 can be
found in [39].

C. Sketch-Based 3D Reconstruction

Our 3D reconstruction network G (pipeline in Fig.2(Right))
consists of several components. Given a standardized sketch
(cid:101)Si, the view estimation module ﬁrst estimates its viewpoint.
(cid:101)Si is then fed to the sketch-to-3D module to generate a point

3DSynthesized sketchEdgemapStandardizedsketchFree-hand sketch5

Fig. 4. Sketch standardization module can be considered as a general zero-
shot domain translation method. Given a sample from a zero-shot (input)
domain X, we ﬁrst translate it to a universal intermediate domain Y , and
ﬁnally to the target domain Z. In the ﬁrst example (second row),
the
input domain is an unseen free-hand sketch. With sketch standardization, it
is translated to an intermediate domain: standardized sketch, which shares
similar style as synthesized sketch for training. With 3D reconstruction, the
standardized sketch can be translated to the target domain: 3D point clouds.
In the second example (last row), the input domain is an unseen nighttime
image. With edge extraction, it gets translated to an intermediate domain:
edge map. With the image-to-image translation model, the standardized edge
map can be translated to the target domain: daytime image.

cloud Pi,pre, whose pose aligns with the sketch viewpoint.
A 3D rotation corresponding to the viewpoint is then applied
to Pi,pre to output the canonically-posed point cloud Pi. The
objective of G is to minimize distances between reconstructed
point cloud Pi and the ground-truth point cloud Pi,gt.

View Estimation Module. The view estimation module
g1 aims to determine the three-dimensional pose from an
input sketch (cid:101)S. Similar to the input transformation module
of the PointNet [42], g1 estimates a 3D rotation matrix A
i.e., A = g1( (cid:101)S). A regularization loss
from a sketch (cid:101)S,
Lorth = (cid:107)I − AAT (cid:107)2
F is applied to ensure A is a rotation
(orthogonal) matrix. The rotation matrix A rotates a point
cloud from the viewpoint pose to a canonical pose, which
matches the ground truth.

3D Reconstruction Module. The reconstruction network
g2 learns to reconstruct a 3D point cloud Ppre from a sketch
(cid:101)S, i.e., Ppre = g2( (cid:101)S). Ppre is further transformed by the
corresponding rotation matrix A to P so that P aligns with the
ground-truth 3D point cloud Pgt’s canonical pose. Overall, we
have P = g1( (cid:101)S) · g2( (cid:101)S). To train G, we penalize the distance
between an output point cloud P and the ground-truth point
cloud Pgt. We employ the Chamfer distance (CD) between
two point clouds P, Pgt ⊂ R3:

dCD(P (cid:107)Pgt) =

(cid:88)

p∈P

min
q∈Pgt

(cid:107)p − q(cid:107)2

2 +

(cid:88)

q∈Pgt

min
p∈P

(cid:107)p − q(cid:107)2
2

(2)

The ﬁnal loss of the entire network is:

Fig. 5.
Performance on free-hand sketches with different design choices.
The design pool includes the model with a cascaded two-stage structure (2nd
column), the model trained on edge maps (3rd column), the model whose
3D output is represented by voxel (4th column), and the proposed model (5th
column). Overall, the proposed method achieves better performance and keeps
more ﬁne-grained details, e.g., the legs of chairs.

where λ is the weight of the orthogonal regularization loss
and (cid:101)Si = R ◦ D2 ◦ D1(Si) is the standardized sketch from
Si. Note that we employ CD rather than EMD (Section
IV-B) to penalize the difference between the reconstruction
and the ground-truth point clouds because CD emphasizes the
geometric outline of point clouds and leads to reconstructions
with better geometric details. EMD, however, emphasizes the
point cloud distribution and may not preserve the geometric
details well at locations with low point density.

IV. EXPERIMENTAL RESULTS

We ﬁrst present the datasets, training details, evaluation
metrics, and implementation details, followed by our model’s
qualitative and quantitative results. Then, we provide com-
parisons with some state-of-the-art methods. We also conduct
ablation studies to understand the beneﬁts of each module.

A. 3D Sketching Dataset

To evaluate the performance of our method, we collected
a real-world evaluation set containing paired sketch-3D data.
Speciﬁcally, we randomly choose ten 3D shapes from each
of the 13 categories of the ShapeNet dataset [18]. Then we
randomly render 3 images from different viewpoints for each
3D shape. Totally, there are 130 different 3D shapes and
390 rendered images. We recruited 11 volunteers to draw the
sketches for the rendered images. Final sketches are reviewed
for quality control. We present several examples in Fig.3.

L =

=

=

(cid:88)

i
(cid:88)

i
(cid:88)

i

dCD (G ◦ U (Si)(cid:107)Pi,gt) + λLorth

dCD (Ai · Pi,pre(cid:107)Pi,gt) + λLorth

(cid:16)

dCD

g1( (cid:101)Si) · g2( (cid:101)Si)(cid:107)Pi,gt

(cid:17)

+ λLorth

(3)

(4)

(5)

B. Training Details and Evaluation Metrics

Training. The proposed model is trained on a subset of
ShapeNet [18] dataset, following settings of [28]. The dataset
consists of 43,783 3D shapes spanning 13 categories, including
car, chair, table, etc. For each category, we randomly select
80% 3D shapes for training and the rest for evaluation.
As mentioned in Section III-A, corresponding sketches of

unseen free-hand sketchstandardized sketchreconstructed 3D shapesketchstandardization3D reconstructionZero-shot/input domainIntermediate domainTarget domainimage translationedge extraction & standardizationunseen night imageedge maptranslated daytime image𝑋𝑌𝑍SketchGTOursCascaded [5, 12]Edge-map [1]Voxel [1]rendered images from 24 viewpoints of each 3D shape of
ShapeNet are synthesized with our synthetic sketch generation
module (Section III-A).

Evaluation. To evaluate our method’s 3D reconstruction
performance on free-hand sketches, we use our proposed
sketch-3D datasets (Section IV-A). To evaluate the general-
izability of our model, we also evaluate on three additional
free-hand sketch datasets, including the Sketchy dataset [5],
the TU-Berlin dataset [43], and the QuickDraw dataset [44].
For these additional datasets, only sketches from categories
that overlap with the ShapeNet dataset are considered.

Following the previous works on point cloud generation
[13], [33], [45], we adopt two evaluation metrics to measure
the similarity between the reconstructed 3D point cloud P
and the ground-truth point cloud Pgt. The ﬁrst one is the
Chamfer Distance (Eqn.2), and another one is the Earth
Mover’s Distance (EMD):

dEM D(P, Pgt) = min

φ:P (cid:55)→Pgt

(cid:88)

x∈P

(cid:107)x − φ(x)(cid:107)

(6)

where P, Pgt has the same size |P | = |Pgt| and φ : P (cid:55)→ Pgt
is a bijection. CD and EMD evaluate the similarity between
two point clouds from two different perspectives (more details
can be found in [13]).

C. Implementation Details

Sketch Generation. We utilize an off-the-shelf sketch-
image translation model [6] to synthesize sketches for training.
Given the appropriate quality of the generated sketches on the
ShapeNet dataset (with some samples depicted in Fig.3), we
directly use the model without any ﬁne-tuning.

Data Augmentation. During training, to further improve
the model’s generalizability and robustness, we perform data
augmentation for synthetic sketches before feeding them to the
standardization module. Speciﬁcally, we apply image spatial
translation (up to ±10 pixels) and rotation (up to ±10◦) on
each input sketch.

Sketch Standardization. Each input sketch Si is ﬁrst ran-
domly deformed with moving least squares [39] both globally
and locally (D1), and then binarized and dilated ﬁve times
iteratively (D2) to obtain a rough sketch Sr. The rough sketch
Sr is then used to train a Pix2Pix model [41], R, to reconstruct
the input sketch Si. The network is trained for 100 epochs with
an initial learning rate of 2e-4. Adam optimizer [46] is used
for the parameter optimization. During evaluation, random
deformation D1 is discarded.

3D Reconstruction. The 3D reconstruction network is
based on [13]’s framework with hourglass network architec-
ture [47]. We compare several different network architectures
(simple encoder-decoder architecture, two-prediction-branch
architecture, etc.) and ﬁnd that hourglass network architecture
gives the best performance. This may be due to its ability
to extract key points from images [47], [48]. We train the
network for 260 epochs with an initial learning rate of 3e-5.
The weight λ of the orthogonal loss is 1e-3. To enhance the
performance on every category, all categories of 3D shapes are
trained together. The class-aware mini-batch sampling [49] is

6

adopted to ensure a balanced category-wise distribution for
each mini-batch. We choose Adam optimizer [46] for the
parameter optimization. 3D point clouds are visualized with
the rendering tool from [50].

D. Results and Comparisons

We ﬁrst present our model’s 3D shape reconstruction per-
formance, along with the comparisons with various baseline
methods. Then we present the results on sketches from differ-
ent viewpoints and of different categories, as well the results
on other free-hand sketch datasets. Note that unless speciﬁcally
mentioned, all evaluations are on the free-hand sketches rather
than synthesized sketches.

Baseline Methods. Our 3D reconstruction network is a one-
stage model where the input sketch is treated as an image, and
point clouds represent the output 3D shape. As conducting the
ﬁrst work for single-view sketch-based 3D reconstruction, we
explore different design options adopted by previous works
on distortion-free line drawings and/or 3D reconstruction,
including architectures, representation of sketches and 3D
shapes. We compare with different variants to demonstrate the
effectiveness of each choice of our model.

Model design: end-to-end vs. two-stage. Although the task
of reconstructing 3D shapes from free-hand sketches is new,
sketch-to-image synthesize and 3D shape reconstruction from
images have been studied before [6], [28], [13]. Is a straight
instead of an end-to-end
combination of the two models,
model, enough to perform well for the task? To compare these
two architectures’ performance, We implement a cascaded
model by composing a sketch-to-image model [52] and an
image-to-3D model [13] to reconstruct 3D shapes.

Sketch: point-based vs. image-based. Considering a sketch
is relatively sparse in pixel space and consists of colorless
strokes, we can employ 2D point clouds to represent a sketch.
Speciﬁcally, 512 points are randomly sampled from strokes
of each binarized sketch, and we use a point-to-point network
architecture (adapted from PointNet [42]) to reconstruct 3D
shapes from the 2D point clouds.

Sketch: Using edge maps as proxy. We compare with a
previous work [1]. Our proposed model uses synthetic sketch
for training. However, an alternative option is using edge maps
as a proxy of the free-hand sketch. As edge maps can be
generated automatically (we use the Canny edge detector in
implementation), the comparison helps us understand if our
proposed synthesizing method is necessary.

3D shape: voxel vs. point cloud. We compare with a
previous work [28]. In this variant, we follow their settings and
represent a 3D shape with voxels. As the voxel representation
is adopted from the previous method, the comparison helps
to understand if representing 3D shapes with point clouds has
beneﬁts.

3D shape: depth map vs. point cloud. In this variant, we
exactly follow a previous work [51] and represent the 3D shape
with multi-view depth maps.

Comparison and Results. Table I and Fig.5 present quanti-
tative and qualitative results of our method and different design
variants. Speciﬁcally for quantitative comparisons (Table I),

TABLE I
OUR APPROACH OUTPERFORMS BASELINE METHODS FOR 3D SHAPE RECONSTRUCTION. [1] USES EDGE-MAPS RATHER THAN SKETCHES AS INPUT. [28]
USES VOXELS RATHER THAN POINT CLOUDS AS OUTPUT. [51] REPRESENTS THE 3D SHAPEWITH MULTI-VIEW DEPTH MAPS. “CAS.” REFERS TO THE
TWO-STAGE CASCADED TRAINING FOLLOWING [6], [13]. CD AND EMD MEASURE DISTANCES BETWEEN RECONSTRUCTIONS AND GROUND-TRUTHS
FROM DIFFERENT PERSPECTIVES (SEE TEXT FOR DETAILS). THE LOWER, THE BETTER.

7

Chamfer Distance (×10−4)

Earth Mover’s Distance (×10−2)

retrieval[2] ours points edge [1] voxel [28] cas.

error

airplane
bench
cabinet
car
chair
display
lamp
speaker
riﬂe
sofa
table
telephone
watercraft
avg.
free-hand sketch

points edge [1] voxel [28]
11.4
29.2
61.7
20.8
41.8
68.6
63.3
88.2
17.0
32.8
55.2
30.7
32.9
42.6
87.1

35.1
202.8
59.1
173.2
108.6
33.1
107.0
203.2
170.1
141.2
134.7
26.9
129.1
117.2
162.5

7.8
16.7
50.4
13.3
36.4
48.3
59.4
79.7
12.1
20.9
49.4
27.3
26.0
34.4
89.0

[51]
cas.
71.7
8.0
414.1 16.8
354.5 51.5
114.2 14.1
237.1 36.1
340.2 49.3
214.0 60.2
406.4 81.2
15.4 12.3
482.4 22.3
469.5 50.5
259.8 27.1
53.8 26.0
264.1 35.0
334.2 91.8

11.2
14.5
45.3
14.2
33.0
38.2
63.5
72.3
14.2
20.3
49.1
27.4
27.3
33.1
89.2

6.1
13.0
39.2
10.4
26.9
37.7
46.3
62.1
10.1
16.3
40.7
21.3
20.3
26.9
86.1

8.5
11.1
17.6
8.9
15.1
15.5
21.3
19.4
11.2
11.1
19.1
13.4
12.5
14.2
18.6

7.3
8.7
17.8
20.0
15.6
15.1
22.6
19.2
13.8
8.5
17.7
13.6
11.1
14.7
16.4

10.8
22.0
17.0
25.2
19.4
13.1
21.2
23.8
23.7
18.6
18.5
15.1
23.1
19.3
22.9

[51]
12.7 8.5
25.8 10.0
29.6 18.4
20.0 21.6
22.8 16.1
27.9 16.4
24.9 22.3
28.0 21.8
15.4 15.2
25.4 9.1
26.5 18.2
27.2 15.1
17.8 12.2
23.4 15.8
26.1 17.0

retrieval[2] ours
6.5
7.8
16.0
18.0
13.0
14.4
20.4
17.9
12.4
7.7
17.3
12.3
10.6
13.4
16.0

11.9
8.6
17.2
21.2
15.3
14.6
22.6
20.0
17.6
8.6
18.2
15.3
12.7
15.7
16.8

Fig. 6. Reconstruction with different number of points. Increasing the number
of points improves the reconstruction quality.

we report 3D shape reconstruction performance on both syn-
thesized (evaluation set) and free-hand sketches. This is due to
that the collected free-hand sketch dataset is relatively small
and together they provide a more comprehensive evaluation.
We have the following observations:

1) Representing sketches as images outperforms represent-

ing them as 2D point clouds (points vs. ours).

2) The model trained on synthesized sketches performs bet-
ter on real free-hand sketches than the model trained on
edge maps (89.0 vs. 86.1 on CD, 16.4 vs. 16.0 on EMD).
Training with edge maps could reconstruct okay overall
coarse shape. However, the unsatisfactory performance
on geometric details reveals such methods are hard to
generalize to free-hand sketches with distortions. It also
shows the necessity of the proposed sketch generation
and standardization modules.

3) In terms of the model design, the end-to-end model
outperforms the two-stage model by a large margin (cas.
vs. ours).

4) In terms of the 3D shape representation, while the voxel
representation can reconstruct the general shape well, the
ﬁne-grained details are frequently missing due to its low
resolution (32 × 32 × 32). Thus, point clouds outperform
voxels. The proposed method also outperforms a previ-
ous work that uses depth maps as 3D shape representa-
tion [51] . Note that the resolution of voxels can hardly

Fig. 7. Ours (2nd row) versus nearest-neighbor retrieval results (last row)
of given sketches. Our model generalizes to unseen 3D shapes better and has
higher geometry ﬁdelity.

improve much due to the complexity and computational
overhead. However, we show that increasing the number
of points improves the reconstruction quality (Fig.6).
Retrieval Results. We show the generalizability of the
proposed method compared with nearest-neighbor retrievals,
following methods and settings of [2] (Fig.7). Our method is
able to generalize to unseen 3D shapes and reconstructs with
higher geometry ﬁdelity (e.g., stand of the lamp, square shape
of the table).

Reconstruction with Different Categories and Views.
Fig.8 shows 3D reconstruction results with sketches from
different object categories. Our model reconstructs 3D shapes
of multiple categories unconditionally. There are some failure
cases that the model may not handle well.

Fig.9 depicts 3D reconstructions with sketches from differ-
ent views. Our model can reconstruct 3D shapes from different
views even if certain parts are occluded (e.g. legs of the table).
Slight variations in details exist for different views.

Evaluation on Other Free-Hand Sketch Datasets. To
evaluate the generalizability of the proposed method, we also
evaluate it on three other free-hand sketch datasets [5], [43],
[44]. We only present some qualitative results (Fig.10) as

Sketch1024 pts2048 ptsSketch1024 pts2048 pts8

3D reconstructions on our newly-collected free-hand sketch evaluation dataset. Left: Examples of some good reconstruction results. Our model
Fig. 8.
reconstructs 3D shapes with ﬁne geometric ﬁdelity of multiple categories unconditionally. Right: Examples of failure cases. Our model may not handle
detailed structures well (e.g., watercraft), recognize the wrong category (e.g., display as a lamp) due to the ambiguity of the sketch, as well as not able to
generate 3D shape from very abstract sketches where few geometric information is available (e.g., riﬂe).

Fig. 9.
sketch’s viewpoint. The module transforms the pose of the output 3D shape to align with canonical pose, i.e. the pose of the ground-truth 3D shape.

3D reconstructions of sketches from different viewpoints. Before the view estimation module, the reconstructed 3D shape aligns with the input

Fig. 10. Our approach trained on ShapeNet can be directly applied to other unseen sketch datasets and it genealize well. Results on other sketch datasets.
Left: Sketchy dataset [5]; Middle: TU-Berlin dataset [43]; Right: QuickDraw dataset [44]. Our model is able to reconstruct 3D shapes from sketches with
different styles and line-widths, and even low-resolution data.

the ground-truth 3D shapes are not available. Our model can
reconstruct 3D shapes from sketches with different styles, line-
widths, and levels of distortions even at low resolution.

E. Sketch Standardization Module

Visualization. The standardization module can be consid-
ered as a domain translation module designed for sketches.

Sketch(input)GTReconstructionw/o standardizationSketchGTAfter view est.Before view est.SketchyTU-BerlinQuickDraw9

Fig. 11.
Standardized sketches converted from different individual styles (by different volunteers). For each rendered image of a 3D object, we show
free-hand sketches from two volunteers and the standardized sketches from these free-hand sketches. Contents are preserved after the standardization process,
and standardized sketches share the style similar to the synthesized ones.

TABLE II
3D SHAPE RECONSTRUCTION ERRORS OF ABLATION STUDIES OF
STANDARDIZATION AND VIEW ESTIMATION MODULE. HAVING BOTH
STANDARDIZATION AND VIEW ESTIMATION MODULE GIVES THE HIGHEST
PERFORMANCE. THE LOWER, THE BETTER.

error
CD (×10−4)
EMD (×10−2)

no standardization
92.6
18.2

no view est.
86.8
16.2

ours
86.1
16.0

TABLE III
RECONSTRUCTION ERROR WITH DIFFERENT COMPONENTS OF THE
STANDARDIZATION MODULE. HAVING BOTH PARTS GIVES THE HIGHEST
PERFORMANCE.

deformation
×
×
(cid:88)
(cid:88)

translation CD (×10−4) EMD (×10−2)

×
(cid:88)
×
(cid:88)

92.6
87.2
90.1
86.1

18.2
16.3
17.4
16.0

Fig.11 shows several free-hand sketches of the same objects
drawn by different volunteers. We show the standardized
sketches of these free-hand sketches and compare them to the
synthesized ones. Applied with the standardization module, the
sketches share a style similar to synthesized sketches which
are used as training data. Thus, the standardization module
helps diminish the domain gap of sketches with various styles
and enhances the generalizability of the proposed method.

Ablation Studies of the Entire Module. The sketch stan-
dardization module is introduced to handle various drawing
styles of humans. We thus verify this module’s effectiveness
on real sketches, both quantitatively (Table II) and qualitatively
(Fig. 8). As shown in Table II, the reconstruction performance
has a signiﬁcant drop when removing the standardization mod-
ule. Its effect is also proved in visualizations. In Fig. 8, we can
observe that our full model equipping with the standardization
module can produce 3D shapes with higher quality, being more
similar to GT shapes, e.g., the airplane and the lamp.

Ablation Studies of Different Components. The stan-
dardization module consists of two components: sketch de-
formation and style translation. We study each module’s
performance and report in Table III. We observe that the style
transformation part improves the reconstruction performance
better compared with the deformation part, while having both
parts gives the highest performance.

Additional Applications. We show the effectiveness of the
proposed sketch standardization with two more applications.
The ﬁrst applications is on cross-dataset sketch classiﬁcation.
We identify the common 98 categories of TU-Berlin sketch
dataset [43] and Sketchy dataset [5]. Then we train on TU-
Berlin and evaluate on Sketchy. As reported in Table IV,
adding additional sketch standardization module, the classi-
ﬁcation accuracy improves 3 percentage points.

The second application corresponds to the second example

TABLE IV
THE SKETCH STANDARDIZATION MODULE IMPROVES CROSS-DATASET
SKETCH CLASSIFICATION ACCURACY. A RESNET-50 MODEL IS TRAINED
ON TU-BERLIN [43] AND EVALUATED SKETCHY DATASET [5]. THE
SKETCH STANDARDIZATION MODULE GIVES 3 PERCENTAGE POINTS GAIN.

accuracy
w/o std.
w/ std.

Sketchy classiﬁcation
75.1%
78.1%

depicted in Fig.4. The target domain is CityScapes dataset
[53], where the training data comes from. We extract corre-
sponding edge maps with a deep learning approach [54] and
train an image-to-image translation model [41] to translate
edge maps to the corresponding RGB images. We evaluate
the zero-shot domain translation performance on three new
datasets: UNDD [55] (night images), Night-Time Driving [56]
(night images) and GTA [57] (synthetic images; screenshots
taken from simulated environment). The novel domains of
night and simulated images can be translated to the target
domain of daytime and real-world images. We visualize the
results in Fig.12.

F. View Estimation Module

In terms of quantitative results (Table II), removing the
view estimation module leads to a performance drop of CD
and EMD. This suggests that an explicit view estimation
helps reconstruct the 3D shape more faithfully. The qualitative
results of the view estimation module are in Fig.9. Before the
3D rotation, the reconstructed 3D shape has the pose aligned
with the input sketch. After the 3D rotation based on the
estimated viewpoint, the 3D shape is aligned to the ground
truth’s canonical pose.

3D objectSynthesized sketchStandardizedsketchFree-handsketch3D objectSynthesized sketchStandardizedsketchFree-hand sketch3D objectSynthesized sketchStandardizedsketchFree-hand sketch10

[3] Z. Lun, M. Gadelha, E. Kalogerakis, S. Maji, and R. Wang, “3d shape
reconstruction from sketches via multi-view convolutional networks,” in
2017 International Conference on 3D Vision (3DV).
IEEE, 2017, pp.
67–77. 1, 2

[4] Q. Yu, F. Liu, Y.-Z. Song, T. Xiang, T. M. Hospedales, and C.-C.
Loy, “Sketch me that shoe,” in Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, 2016. 1

[5] P. Sangkloy, N. Burnell, C. Ham, and J. Hays, “The sketchy database:
learning to retrieve badly drawn bunnies,” ACM Transactions on Graph-
ics, vol. 35, no. 4, pp. 1–12, 2016. 1, 6, 7, 8, 9

[6] R. Liu, Q. Yu, and S. Yu, “An unpaired sketch-to-photo translation

model,” arXiv preprint arXiv:1909.08313, 2019. 1, 3, 6, 7

[7] A. Ghosh, R. Zhang, P. K. Dokania, O. Wang, A. A. Efros, P. H. Torr,
and E. Shechtman, “Interactive sketch & ﬁll: Multiclass sketch-to-image
translation,” in Proceedings of the IEEE International Conference on
Computer Vision, 2019. 1

[8] J. Fuentes-Pacheco, J. Ruiz-Ascencio, and J. M. Rend´on-Mancha,
“Visual simultaneous localization and mapping: a survey,” Artiﬁcial
intelligence review, vol. 43, no. 1, pp. 55–81, 2015. 1, 2

[9] C. B. Choy, D. Xu, J. Gwak, K. Chen, and S. Savarese, “3d-r2n2: A
uniﬁed approach for single and multi-view 3d object reconstruction,” in
European conference on computer vision. Springer, 2016. 1, 2
[10] O. ¨Ozyes¸il, V. Voroninski, R. Basri, and A. Singer, “A survey of structure
from motion*.” Acta Numerica, vol. 26, pp. 305–364, 2017. 1, 2
[11] S. R. Richter and S. Roth, “Discriminative shape from shading in
uncalibrated illumination,” in Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, 2015. 1, 2

[12] E. Dibra, H. Jain, C. Oztireli, R. Ziegler, and M. Gross, “Human shape
from silhouettes using generative hks descriptors and cross-modal neural
networks,” in Proceedings of the IEEE conference on computer vision
and pattern recognition, 2017. 1, 2

[13] H. Fan, H. Su, and L. J. Guibas, “A point set generation network for 3d
object reconstruction from a single image,” in Proceedings of the IEEE
conference on computer vision and pattern recognition, 2017. 1, 2, 3,
6, 7

[14] C. Li, H. Pan, Y. Liu, X. Tong, A. Sheffer, and W. Wang, “Bendsketch:
modeling freeform surfaces through 2d sketching,” ACM Transactions
on Graphics, vol. 36, no. 4, pp. 1–14, 2017. 1, 3

[15] B. Xu, W. Chang, A. Sheffer, A. Bousseau, J. McCrae, and K. Singh,
“True2form: 3d curve networks from 2d sketches via selective regular-
ization,” Transactions on Graphics, vol. 33, no. 4, 2014. 1

[16] C. Li, H. Pan, Y. Liu, X. Tong, A. Sheffer, and W. Wang, “Robust ﬂow-
guided neural prediction for sketch-based freeform surface modeling,”
ACM Transactions on Graphics (TOG), vol. 37, no. 6, pp. 1–12, 2018.
1, 3

[17] X. He, Y. Zhou, Z. Zhou, S. Bai, and X. Bai, “Triplet-center loss for
multi-view 3d object retrieval,” in Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, 2018. 1, 3

[18] A. X. Chang, T. Funkhouser, L. Guibas, P. Hanrahan, Q. Huang, Z. Li,
S. Savarese, M. Savva, S. Song, H. Su et al., “Shapenet: An information-
rich 3d model repository,” arXiv preprint arXiv:1512.03012, 2015. 2,
5

[19] B. Yang, S. Rosa, A. Markham, N. Trigoni, and H. Wen, “Dense 3d
object reconstruction from a single depth view,” IEEE transactions on
pattern analysis and machine intelligence, vol. 41, no. 12, pp. 2820–
2834, 2018. 2

[20] A. Kar, C. H¨ane, and J. Malik, “Learning a multi-view stereo machine,”
in Advances in neural information processing systems, 2017. 2
[21] A. P. Witkin, “Recovering surface shape and orientation from texture,”

Artiﬁcial intelligence, vol. 17, no. 1-3, pp. 17–45, 1981. 2

[22] Y. Zhang, Z. Liu, T. Liu, B. Peng, and X. Li, “Realpoint3d: An efﬁcient
generation network for 3d object reconstruction from a single image,”
IEEE Access, vol. 7, pp. 57 539–57 549, 2019. 2

[23] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,
S. Ozair, A. Courville, and Y. Bengio, “Generative adversarial nets,” in
Advances in neural information processing systems, 2014. 2

[24] D. P. Kingma and M. Welling, “Auto-encoding variational bayes,” arXiv

preprint arXiv:1312.6114, 2013. 2

[25] J. Wu, C. Zhang, T. Xue, B. Freeman, and J. Tenenbaum, “Learning a
probabilistic latent space of object shapes via 3d generative-adversarial
modeling,” in Advances in neural information processing systems, 2016.
2

[26] M. Tatarchenko, A. Dosovitskiy, and T. Brox, “Octree generating
networks: Efﬁcient convolutional architectures for high-resolution 3d
the IEEE International Conference on
outputs,” in Proceedings of
Computer Vision, 2017. 2

Fig. 12. Zero-shot domain translation results. We aim to translate zero-shot
images (Left) to the training data domain (Right). Speciﬁcally, we evaluate
the proposed zero-shot domain translation performance on three new datasets:
UNDD [55], Night-Time Driving [56] and GTA [57]. The novel domains of
night-time and simulated images can be translated to the target domain of
daytime and real-world images by leveraging the synthetic edge map domain
as a bridge. The target domain is CityScapes dataset [53], from where the
training data comes. We extract corresponding edge maps and train an image-
to-image translation model [41] to translate edge maps to the corresponding
RGB images. 1st, 3rd, 5th, 7th rows of column 1 depict some sample training
RGB images and 2nd, 4th, 6th, 8th rows of column 1 depict the corresponding
edge maps respectively.

V. SUMMARY

We study a novel task, 3D shape reconstruction from a
single-view free-hand sketch. The major novelty is that we
use synthesized sketches as training data and introduce a
sketch standardization module, in order to tackle the data
insufﬁciency and sketch style variation issues. Extensive ex-
perimental results shows that the proposed method is able to
successfully reconstruct 3D shapes from single-view free-hand
sketches unconditioned on viewpoints and categories. The
work may unleash more potentials of the sketch in applications
such as sketch-based 3D design/games, making them more
accessible to the general public.

REFERENCES

[1] J. Delanoy, M. Aubry, P. Isola, A. A. Efros, and A. Bousseau, “3d
sketching using multi-view deep volumetric prediction,” Proceedings of
the ACM on Computer Graphics and Interactive Techniques, vol. 1,
no. 1, pp. 1–22, 2018. 1, 2, 3, 6, 7

[2] F. Wang, L. Kang, and Y. Li, “Sketch-based 3d shape retrieval using
convolutional neural networks,” in Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, 2015. 1, 2, 3, 7

Training dataTarget domainTest data: zero-shot domainUNDDNight-time DrivingGTA-dayGTA-nightTranslated imageInput edge mapInput image11

[49] L. Shen, Z. Lin, and Q. Huang, “Relay backpropagation for effective
learning of deep convolutional neural networks,” in European conference
on computer vision. Springer, 2016. 6

[50] K. Mo, P. Guerrero, L. Yi, H. Su, P. Wonka, N. Mitra, and L. J. Guibas,
“Structurenet: Hierarchical graph networks for 3d shape generation,”
ACM Transactions on Graphics, vol. 38, no. 6, 2019. 6

[51] N. Nozawa, H. P. Shum, E. S. Ho, and S. Morishima, “Single sketch
image based 3d car shape reconstruction with deep learning and lazy
learning.” in VISIGRAPP (1: GRAPP), 2020, pp. 179–190. 6, 7
[52] J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros, “Unpaired image-to-image
translation using cycle-consistent adversarial networks,” in Proceedings
of the IEEE international conference on computer vision, 2017. 6
[53] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benen-
son, U. Franke, S. Roth, and B. Schiele, “The cityscapes dataset for
semantic urban scene understanding,” in Proc. of the IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), 2016. 9, 10
[54] X. S. Poma, E. Riba, and A. Sappa, “Dense extreme inception network:
Towards a robust cnn model for edge detection,” in Proceedings of
the IEEE/CVF Winter Conference on Applications of Computer Vision,
2020, pp. 1923–1932. 9

[55] S. Nag, S. Adak, and S. Das, “What’s there in the dark,” in 2019 IEEE
IEEE, 2019, pp.

International Conference on Image Processing (ICIP).
2996–3000. 9, 10

[56] D. Dai and L. Van Gool, “Dark model adaptation: Semantic image
segmentation from daytime to nighttime,” in 2018 21st International
Conference on Intelligent Transportation Systems (ITSC).
IEEE, 2018,
pp. 3819–3824. 9, 10

[57] S. R. Richter, V. Vineet, S. Roth, and V. Koltun, “Playing for data:
Ground truth from computer games,” in European conference on com-
puter vision. Springer, 2016, pp. 102–118. 9, 10

[27] C. H¨ane, S. Tulsiani, and J. Malik, “Hierarchical surface prediction
for 3d object reconstruction,” in 2017 International Conference on 3D
Vision.

IEEE, 2017. 2

[28] H. Xie, H. Yao, X. Sun, S. Zhou, and S. Zhang, “Pix2vox: Context-aware
3d reconstruction from single and multi-view images,” in Proceedings
of the IEEE International Conference on Computer Vision, 2019. 2, 5,
6, 7

[29] N. Wang, Y. Zhang, Z. Li, Y. Fu, W. Liu, and Y.-G. Jiang, “Pixel2mesh:
Generating 3d mesh models from single rgb images,” in Proceedings of
the European Conference on Computer Vision, 2018. 2

[30] N. Kolotouros, G. Pavlakos, and K. Daniilidis, “Convolutional mesh
regression for single-image human shape reconstruction,” in Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition,
2019. 2

[31] J. Pan, X. Han, W. Chen, J. Tang, and K. Jia, “Deep mesh reconstruc-
tion from single rgb images via topology modiﬁcation networks,” in
Proceedings of the IEEE International Conference on Computer Vision,
2019. 3

[32] P. Mandikal and V. B. Radhakrishnan, “Dense 3d point cloud reconstruc-
tion using a deep pyramid network,” in 2019 IEEE Winter Conference
on Applications of Computer Vision.

IEEE, 2019. 3

[33] P. Mandikal, K. Navaneet, M. Agarwal, and R. V. Babu, “3d-lmnet:
Latent embedding matching for accurate and diverse 3d point cloud
reconstruction from a single image,” arXiv preprint arXiv:1807.07796,
2018. 3, 6

[34] K. Navaneet, P. Mandikal, M. Agarwal, and R. V. Babu, “Capnet:
Continuous approximation projection for 3d point cloud reconstruction
using 2d supervision,” in Proceedings of
the AAAI Conference on
Artiﬁcial Intelligence, vol. 33, 2019, pp. 8819–8826. 3

[35] H. Huang, E. Kalogerakis, E. Yumer, and R. Mech, “Shape synthesis
from sketches via procedural models and convolutional networks,” IEEE
transactions on visualization and computer graphics, vol. 23, no. 8, pp.
2003–2013, 2016. 3

[36] Z. Han, B. Ma, Y.-S. Liu, and M. Zwicker, “Reconstructing 3d shapes
from multiple sketches using direct shape optimization,” IEEE Transac-
tions on Image Processing, vol. 29, pp. 8721–8734, 2020. 3

[37] S.-H. Zhang, Y.-C. Guo, and Q.-W. Gu, “Sketch2model: View-aware
3d modeling from single free-hand sketches,” in Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition,
2021, pp. 6012–6021. 3

[38] L. Wang, C. Qian, J. Wang, and Y. Fang, “Unsupervised learning of
3d model reconstruction from hand-drawn sketches,” in Proceedings of
the 26th ACM international conference on Multimedia, 2018, pp. 1820–
1828. 3

[39] S. Schaefer, T. McPhail, and J. Warren, “Image deformation using
moving least squares,” in ACM SIGGRAPH 2006 Papers, 2006, pp. 533–
540. 4, 6

[40] S. Yang, Z. Wang, J. Liu, and Z. Guo, “Deep plastic surgery: Robust and
controllable image editing with human-drawn sketches,” arXiv preprint
arXiv:2001.02890, 2020. 4

[41] P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros, “Image-to-image translation
with conditional adversarial networks,” in Proceedings of the IEEE
conference on computer vision and pattern recognition, 2017. 4, 6,
9, 10

[42] C. R. Qi, H. Su, K. Mo, and L. J. Guibas, “Pointnet: Deep learning on
point sets for 3d classiﬁcation and segmentation,” in Proceedings of the
IEEE conference on computer vision and pattern recognition, 2017. 5,
6

[43] M. Eitz, J. Hays, and M. Alexa, “How do humans sketch objects?” ACM
Transactions on Graphics, vol. 31, no. 4, pp. 44:1–44:10, 2012. 6, 7,
8, 9

[44] Google, “The quick, draw! dataset,” 2017, https://quickdraw.withgoogle.

com/data. 6, 7, 8

[45] G. Yang, X. Huang, Z. Hao, M.-Y. Liu, S. Belongie, and B. Hariharan,
“Pointﬂow: 3d point cloud generation with continuous normalizing
ﬂows,” in Proceedings of the IEEE International Conference on Com-
puter Vision, 2019. 6

[46] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”

arXiv preprint arXiv:1412.6980, 2014. 6

[47] A. Newell, K. Yang, and J. Deng, “Stacked hourglass networks for
human pose estimation,” in European conference on computer vision.
Springer, 2016. 6

[48] Z. Cao, T. Simon, S.-E. Wei, and Y. Sheikh, “Realtime multi-person 2d
pose estimation using part afﬁnity ﬁelds,” in Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition, 2017. 6

