Nonfractional Memory:
Filtering, Antipersistence, and Forecasting

J. Eduardo Vera-Vald´es∗

Department of Mathematical Sciences, Aalborg University, and CREATES.

Abstract

The fractional diﬀerence operator remains to be the most popular mechanism to generate long
memory due to the existence of eﬃcient algorithms for their simulation and forecasting. Nonethe-
less, there is no theoretical argument linking the fractional diﬀerence operator with the presence
of long memory in real data. In this regard, one of the most predominant theoretical explanations
for the presence of long memory is cross-sectional aggregation of persistent micro units. Yet, the
type of processes obtained by cross-sectional aggregation diﬀers from the one due to fractional
diﬀerencing. Thus, this paper develops fast algorithms to generate and forecast long memory
by cross-sectional aggregation. Moreover, it is shown that the antipersistent phenomenon that
arises for negative degrees of memory in the fractional diﬀerence literature is not present for cross-
sectionally aggregated processes. Pointedly, while the autocorrelations for the fractional diﬀerence
operator are negative for negative degrees of memory by construction, this restriction does not ap-
ply to the cross-sectional aggregated scheme. We show that this has implications for long memory
tests in the frequency domain, which will be misspeciﬁed for cross-sectionally aggregated processes
with negative degrees of memory. Finally, we assess the forecast performance of high-order AR and
ARF IM A models when the long memory series are generated by cross-sectional aggregation. Our
results are of interest to practitioners developing forecasts of long memory variables like inﬂation,
volatility, and climate data, where aggregation may be the source of long memory.

Keywords: Nonfractional memory, long memory, fractional diﬀerence, antipersistence, forecasts.

JEL classiﬁcation: C15, C22, C53.

8
1
0
2

n
a
J

0
2

]
T
S
.
h
t
a
m

[

1
v
7
7
6
6
0
.
1
0
8
1
:
v
i
X
r
a

∗E-mail: eduardo@math.aau.dk Webpage: https://sites.google.com/view/veravaldes/

Address: Skjernvej 4A, 9220 Aalborg East, Denmark.

Preprint submitted to arXiv

January 23, 2018

 
 
 
 
 
 
1. Introduction

Long memory has been a topic of interest in econometrics since Granger’s (1966) study on
the shape of the spectrum of economic variables. Granger found that long-term ﬂuctuations in
economic variables if decomposed into frequency components are such that the amplitudes of the
components decrease smoothly with decreasing period. As shown by Adenstedt (1974), this type of
behaviour implies long lasting autocorrelations. The presence of long memory in the data can have
perverse eﬀects in estimation and forecasting methods if not included into the modelling scheme,
see Beran (1994), and Beran et al. (2013).

The autoregressive fractionally integrated moving average (ARF IM A) class of models has
become one of the most popular methods to model long memory in the time series literature.
They have the appeal of bridging the gap between the stationary autoregressive moving aver-
age (ARM A) models and the nonstationary autoregressive integrated moving average (ARIM A)
model. ARF IM A models rely on the fractional diﬀerence operator to introduce long memory
behaviour. Nonetheless, there are currently no economic or ﬁnancial reasonings implying the
fractional diﬀerence operator.

One of the reasons behind the reliance of the time series literature on generating long memory
by the fractional diﬀerence operator is the existence of eﬃcient algorithms for their simulation and
forecasting. In general, these type of algorithms are not available for other long memory generating
schemes. Thus, in this paper, we develop algorithms for memory generation and forecasting by
cross-sectional aggregation `a la Granger (1980).

We then contrast the properties of cross-sectionally aggregated processes to the ones obtained
by the fractional diﬀerence operator.
It is shown that cross-sectional aggregated processes are
more ﬂexible than fractionally diﬀerenced ones, in the sense that more short term dynamics may
be included.

Moreover, we show that the antipersistent property of negative autocorrelation for negative
degrees of memory does not apply to long memory generated by cross-sectional aggregation. We
show that this has repercussions for long memory estimators in the frequency domain.

Finally, this paper evaluates the forecasting power of the ARF IM A, and high-order AR models
when forecasting long memory generated by cross-sectional aggregation. It ﬁnds that high-order
AR models beat a pure fractional diﬀerence process, I(d), in terms of forecasting performance for
some cases. Nonetheless, allowing for short term dynamic in the form of an ARF IM A(1, d, 0)
model produces comparable forecast performance as high-order AR models while relying on fewer
parameters.

This paper proceeds as follows.

In Section 2, we present the fractional diﬀerence operator
commonly used to model long memory in the time series literature. Section 3 discusses cross-
sectional aggregation as the theoretical explanation behind the presence of long memory in the
data and develops a fast algorithm for its generation. Moreover, it contrasts properties of the
fractional diﬀerence operator against the cross-sectional aggregation scheme. Section 4 discusses
the antipersistent property in the context of cross-sectionally aggregated processes. Section 5
constructs minimum squared error forecasts, and studies the theoretical performance of high-
order AR, and ARF IM A models when forecasting long memory generated by cross-sectional
aggregation. Section 6 concludes.

2. The Fractional Diﬀerence Operator

The ARF IM A speciﬁcation due to Granger and Joyeux (1980), and Hosking (1981) has become
the standard model to study long memory in the time series literature. They extended the ARM A
model to include long memory dynamics by introducing the fractional diﬀerence operator

(1 − L)dxt = εt,

2

where εt is a white noise process, and d ∈ (−1/2, 1/2). Following the standard binomial expansion,
they decompose the fractional diﬀerence operator, (1 − L)d, to generate a series given by

xt =

∞
(cid:88)

j=0

πjεt−j,

(1)

with coeﬃcients πj = Γ(j + d)/(Γ(d)Γ(j + 1)) for j ∈ N. Using Stirling’s approximation, it can
be shown that these coeﬃcients decay at a hyperbolic rate, πj ≈ jd−1, which in turn translates to
slowly decaying autocorrelations. We write xt ∼ I(d) to denote a process generated by equation
(1), that is, an integrated process with parameter d.

The properties of the ARF IM A model have been well documented in, among others, Baillie
(1996), and Beran et al. (2013). Moreover, fast algorithms have been developed to generate series
using the fractional diﬀerence operator, see Jensen and Nielsen (2014). Thus, the ARF IM A
model has become the canonical construction for modelling and forecasting long memory in the
time series literature.

Even though the ARF IM A provides a good representation of long memory, and bridges the
gap between the stationary ARM A models and the non-stationary ARIM A model, to the best
of our knowledge, there is no theoretical reasoning linking the ARF IM A model with the memory
found in real data. That is, in contrast with the complete market hypothesis implying that stock
prices follow a random walk, or capital depreciation suggesting an autoregressive process, there
are no economic or ﬁnancial arguments for the fractional diﬀerence operator to occur in real data.
In this regard, the next section presents the most common theoretical motivation behind long
memory in the time series literature, cross-sectional aggregation.

3. Cross-Sectional Aggregation

Granger (1980), in line with the work of Robinson (1978) on autoregressive processes with
random coeﬃcients, showed that aggregating AR(1) processes with coeﬃcients sampled from a
Beta distribution can produce long memory.

Deﬁne a cross-sectional aggregated series as

xt =

1
√
N

N
(cid:88)

i=1

xi,t,

(2)

where the N individual series are generated as

xi,t = αixi,t−1 + (cid:15)i,t

i = 1, 2, · · · , N ;

i,t] = σ2
where εi,t is an i.i.d. process with E[(cid:15)2
with parameters a, b > 0, with density given by

(cid:15) . Moreover, α2

i is sampled from a Beta distribution

B(α; a, b) =

1
B(a, b)

αa−1(1 − α)b−1

for α ∈ (0, 1),

where B(·, ·) is the Beta function.

Granger showed that, as N → ∞, the autocorrelation function of xt decays at a hyperbolic

rate with parameter d = 1 − b/2. Thus, xt has long memory.

The cross-sectional aggregation result has been extended in several directions, including to
allow for general ARM A processes, as well as to other distributions. See for instance, Linden
(1999), Oppenheim and Viano (2004), and Zaﬀaroni (2004). As argued by Haldrup and Vera
Vald´es (2017), maintaining the Beta distribution allows us to have closed form representations.

3

Furthermore, Beran et al. (2010) proposed a method to estimate the parameters from the Beta
distribution from the individual series, xi,t above.

In applied work, cross-sectional aggregation has been cited as source of long memory for many
series. To name but a few, it has been proposed for inﬂation, output, and volatility; see Balcilar
(2004), Diebold and Rudebusch (1989), and Osterrieder et al. (2015).

Even though a cross-sectional aggregated process has long memory, Haldrup and Vera Vald´es
(2017) show that the resulting series does not belong to the ARF IM A class of processes. Pointedly,
they show that the fractionally diﬀerenced cross-sectional aggregated process does not follow an
ARM A speciﬁcation.

The next subsections expands on the cross-sectional aggregation literature by developing fast
algorithms for its generation, and contrasting its theoretical properties to the ones of the fractional
diﬀerence operator.

3.1. Nonfractional Memory Generation

As can be seen by its deﬁnition, Equation (2), generating cross-sectionally aggregated processes
is computationally demanding. For each cross-sectionally aggregated process, we need to simulate
a vast number of AR(1) processes, which are in turn computationally demanding. Haldrup and
Vera Vald´es (2017) suggest that to get a good approximation, the cross-sectional dimension should
increase as the sample size. The computational demands are thus particularly large for Monte Carlo
type of analysis on cross-sectionally aggregated processes. In what follows, we use the theoretical
autocorrelations of the cross-sectionally aggregated process to present an algorithm that makes
generation of long memory by cross-sectional aggregation comparable to fractional diﬀerencing in
terms of computational requirements.

Denote xt ∼ CSA(a, b) to a series generated by cross-sectional aggregation of autoregressive
parameters sampled from the Beta distribution B(a, b), Equation (2). The notation makes explicit
the origin of the memory by cross-sectional aggregation and its dependence on the two parame-
ters of the Beta distribution. Theorem 1 develops a fast algorithm to generate cross-sectionally
aggregated processes.

Theorem 1. Let xt ∼ CSA(a, b) deﬁned as in (2), then xt can be computed as the ﬁrst T elements
of the (2T − 1) × 1 vector

T −1F ( ¯F ˜φ (cid:12) ¯F ˜ε),
where ¯F is the discrete Fourier transform, T −1F is the inverse transform, and ’(cid:12)’ denotes multi-
plication element by element. Furthermore, ˜z is a (2T −1)×1 vector given by ˜z := [z0, z1, · · · , zT −1,
0, · · · , 0] for zt = φt, εt where εj ∼ i.i.d.N (0, σ2

(cid:15) ), and φj = (B(p + j, q)/B(p, q))1/2 , ∀j ∈ N.

Proof: See appendix A.

The theorem is an application of the circular convolution theorem for the coeﬃcients associ-
ated with the cross-sectionally aggregated process. In this sense, it is in line with the algorithm
of Jensen and Nielsen (2014) for the fractional diﬀerence operator and thus achieves equivalent
computational eﬃciency. The diﬀerence in computation times between the standard simulation
of a cross-sectional aggregated times series, Equation (2), and the fast implementation in Theo-
rem 1 are very large for all sizes. For small series (T ≈ 102) the gains are in the hundreds of
times faster, while in the thousands for medium sized series (T ≈ 104). This is of course not
surprising given that, for a sample of size T , the number of computations needed for the stan-
dard implementation is of order N T 2, with N the cross-sectional dimension, which, as argued
before, should increase as T does. Meanwhile, the computational requirements for the fast imple-
mentation are of order T log (T ). Codes implementing the algorithm for memory generation by
cross-sectional aggregation are available in Appendix B, and on the author’s github repository at
github.com/everval/Nonfractional.

4

To get a better understanding of the dynamics of the long memory by cross-sectional aggrega-
tion, the next sections compare it against the pure fractional noise, that is, a fractionally diﬀerenced
white noise. However, note that we can allow more short-term dynamic by adding AR and M A
ﬁlters to both speciﬁcations.

3.2. Nonfractional Memory and Fractional Diﬀerence Operator

One notable diﬀerence between a cross-sectionally aggregated process and a fractionally diﬀer-
enced one is the number of parameters needed for its generation. Fractionally diﬀerenced processes
rely on only one parameter to model the entire series, whilst a cross-sectionally aggregated process
uses two. As we will see below, this gives the cross-sectional aggregation procedure more ﬂexibility.
Let γI(d)(·), and γCSA(a,b)(·) be the autocorrelation functions of an I(d), and a CSA(a, b)

process, respectively. That is,

γI(d)(k) =

Γ(k + d)Γ(1 − d)
Γ(k − d + 1)Γ(d)

≈ k2d−1

as k → ∞

γCSA(a,b)(k) =

B(a + k/2, b − 1)
B(a, b − 1)

≈ k1−b

as k → ∞,

(3)

(4)

which show that both processes have hyperbolic decaying autocorrelations. As argued by Granger
(1980), the asymptotic behaviour of the autocorrelation function for a cross-sectionally aggregated
process, Equation (4), only depends on the second argument of the Beta function. In this context,
both processes show the same rate of decay for the autocorrelation function for b = 2(1 − d).

Moreover, the ﬁrst argument of the Beta function allows us to introduce short term dynamics.
Figure 1 shows the autocorrelation function of cross-sectional aggregated processes for diﬀerent
values of ‘a’, the ﬁrst parameter of the Beta distribution. The ﬁgure shows that as the ﬁrst
parameter gets larger, so does the autocorrelation function for the initial lags. Thus, ‘a’ acts as a
short memory modulator.

Figure 1: Autocorrelation function for an CSA(a, b) processes for diﬀerent values of ‘a’ while having the same
asymptotic behaviour.

Section 5 will look at the forecasting performance of ARF IM A models when working with
cross-sectionally aggregated processes, but for now suppose we are interested in looking in the
other direction. That is, assume we have a process generated by the fractional diﬀerence operator
and we want to approximate it with a cross-sectionally aggregated process. We show that we can
use the ﬁrst argument to generate such a series.

Consider the loss function

L(k, a, d) :=

k
(cid:88)

i=0

(cid:0)γI(d)(i) − γCSA(a,2(1−d))(i)(cid:1)2 ,

(5)

5

which measures the squared diﬀerence between autocorrelations at the ﬁrst k lags for cross-
sectionally aggregated and fractionally diﬀerenced processes with the same long memory dynamics,
hence we set b = 2(1 − d) for the cross-sectionally aggregated process.

Minimizing (5) with respect to ‘a’, allows us to ﬁnd the cross-sectional aggregated process
that best approximates an I(d) one up to lag k, while having the same long term dynamics.
Given the diﬀerent forms of the autocorrelation functions, (3), and (4), there is in general not a
value of ‘a’ that minimizes (5) for all values of k; for instance, mina L(2, a, 0.2) = 0.118, while
mina L(30, a, 0.2) = 0.121. Moreover, it will depend on d.

Nonetheless, selecting a medium sized k, say k ≈ 10, the approximation turns out to be quite
good in general. In Figure 2, we present a ﬁltered {εt}104
t=1 ∼ N (0, 1) vector using the fractional
diﬀerence operator with long memory parameter d = 0.2, and using the cross-sectional aggregated
algorithm with parameters a = 0.12, and b = 1.6, so that they show similar short and long memory
behaviours.

Figure 2: Filtered series and autocorrelation function for a fractional diﬀerenced process I(0.2), and a cross-sectional
aggregated one CSA(0.12, 1.6).

The ﬁgure shows that the ﬁltered series are quite similar. This behaviour can further be
seen on the autocorrelation functions showing similar dynamics. Thus, the ﬁgure shows that it is
possible to generate cross-sectionally aggregated processes that closely mimic ones due to fractional
diﬀerencing.

4. Nonfractional Memory and Antipersistent Processes

It is well known in the long memory literature that the fractional diﬀerence operator implies
that the autocorrelation function is negative for negative degrees of memory, d ∈ (−1/2, 0). This
can be seen in (3) where the sign of γI(d)(k) depends on Γ(d) in the denominator, which is negative
for d ∈ (−1/2, 0). Furthermore, the behaviour of the spectral density for a fractionally diﬀerenced
process near the origin is given by

fI(d)(λ) ≈ c0λ−2d

as λ → 0,

(6)

where c0 is a constant. Thus, fI(d)(λ) → 0 as λ → 0, that is, the fractional diﬀerence operator
for negative degrees of memory imply a spectral density collapsing to zero at the origin. These
properties, among other related ones, has been named antipersistence in the literature.

We argue that cross-sectionally aggregated processes do not share these features. First, Equa-
tion (4) shows that the autocorrelation function for the cross-sectionally aggregated process only
depends on the Beta function, which is always positive. Figure 3 shows the autocorrelation func-
tion for both fractionally diﬀerenced and cross-sectional aggregated processes for a negative degree

6

of memory, d = −0.2. The ﬁgure shows that, even though both processes show the same rate of
decay in their autocorrelation functions, they have opposite signs.

Figure 3: Autocorrelation functions for a fractional diﬀerenced process, I(−0.2), and a cross-sectional aggregated
one, CSA(0.09, 2, 4). The right plot shows a close-up for lags 70 to 110.

Then, given the positive sign for all autocorrelations, Theorem 2 shows that the spectral density
for the cross-sectionally aggregated process for negative degrees of memory converges to a positive
constant.

Theorem 2. Let xt ∼ CSA(a, b) deﬁned as in (2) with b ∈ (2, 3) so that the long memory
parameter is in the negative range, d ∈ (−1/2, 0), then, the spectral density of xt at the origin is
positive. That is,

fCSA(a,b)(0) = ca,b > 0,

(7)

where ca,b depends on the parameters of the Beta distribution.

Proof: See appendix A.

Figure 4 shows the periodogram, an estimate of the spectral density, for CSA(a, b) and I(d)
processes of size T = 104 averaged for 104 replications. The ﬁgure shows that the periodogram for
both processes show similar patterns for positive degrees of memory, both diverging to inﬁnity at
the same rate. Nonetheless, for negative degrees of memory, the periodogram collapses to zero as
the frequency goes to zero for the fractionally diﬀerenced process, while it converges to a constant
for the cross-sectionally aggregated process.

Moreover, the latter property has implications for estimation and inference. In particular, tests
for long memory in the frequency domain will be aﬀected. These tests are based on the rate to
which the periodogram goes to zero as an estimator for long memory using the log-periodogram
regression, see Geweke and Porter-Hudak (1983), and Robinson (1995).

The log-periodogram regression is given by

log( ˆfX (λj)) = a − 2d log(λj) + uj,

where ˆfX (·) is the periodogram, a is a constant, and uj is the error term. From (6), note that the
log-periodogram regression provides an estimate of the long memory parameter, d, for fractionally
diﬀerenced processes. Tests in the frequency domain use this expression to estimate the degree of
memory. Nonetheless, as Theorem 2, and Figure 4 show, these tests will be misspeciﬁed for long
memory by cross-sectional aggregation for negative degrees of memory.

To illustrate the misspeciﬁcation problem, Table 1 reports the degree of long memory estimated
by the method of Geweke and Porter-Hudak, 1983, GP H hereinafter, for several degrees of memory
for both fractionally diﬀerenced, and cross-sectionally aggregated processes.

7

Figure 4: Mean periodogram of a fractional diﬀerenced process, I(d), [top], and a cross-sectional aggregated process,
CSA(0.2, 2(1 − d)), [bottom]. A sample size of T = 10, 000 was used, with 10,000 replications, and d = 0.4, −0.4.

Table 1: Mean, and standard deviation in parentheses, of estimated long memory by the GP H method for
CSA(0.2, 2(1 − d)) and I(d) processes. We use the standard T 1/2 bandwidth, where the sample size is T = 104, and
with 104 replications.

d = 0.4

d = 0.2

d = −0.2

d = −0.4

CSA(a, b)
0.4062
(0.0701)

I(d)
0.4034
( 0.0697)

CSA(a, b)
0.2628
(0.0697)

I(d)
0.2011
(0.0696)

CSA(a, b)
0.1036
(0.0687)

I(d)
-0.1985
(0.0685)

CSA(a, b)
0.0653
(0.0695)

I(d)
-0.3927
(0.0717)

The table shows that the estimator is relatively close to the true memory for both processes
when the memory is positive, if slightly overshooting it for the cross-sectionally aggregated process,
as reported by Haldrup and Vera Vald´es (2017). This contrasts to the case of negative memory,
where the table shows that the estimator remains precise for the fractionally diﬀerenced series,
while it is incapable of detecting the long memory in the cross-sectionally aggregated processes,
with estimators not statistically diﬀerent from zero. This is of course not surprising in light of
Theorem 2.

In sum, the lack of the antipersistent property in cross-sectionally aggregated processes shows
that care must be taken when estimating long memory for negative degrees of memory if the
long memory generating mechanism is not the fractional diﬀerence operator. Further analysis of
negative degrees of memory and antipersistence is a line of inquiry open for further research.

5. Nonfractional Memory Forecasting

In the time series literature, some eﬀort has been directed to assess the performance of the
ARF IM A type of models when forecasting long memory processes. For instance, Ray (1993)
calculates the percentage increase in mean-squared error (M SE) from forecasting I(d) series with
AR models. She argues that the M SE may not increase signiﬁcantly, particularly when we do not
know the true long memory parameter. Crato and Ray (1996) compare the forecasting performance

8

of ARF IM A models against ARM A alternatives and ﬁnd that ARF IM A models are in general
outperformed by ARM A alternatives. Moreover, Man (2003) argues that an ARM A(2, 2) model
compares favourably to an I(d) for short-term forecasts of long memory time series with fractionally
diﬀerenced structure.

One thing that these forecasting comparison studies have in common is the underlying as-
sumption that long memory is generated by an ARF IM A process. In this context, a priori the
forecasting exercises assume that a fractionally diﬀerenced process is the correct speciﬁcation.
As previously discussed, even though cross-sectional aggregation does generate long memory, the
series does not follow an ARF IM A speciﬁcation. The question remains whether an ARF IM A
model serves as a good approximation for forecasting purposes.

The next subsection computes the minimum square error forecasts for cross-sectional aggre-
gated processes, CSA(a, b). With those as benchmark, the following subsections evaluate the
forecasts performance of high order AR, and ARF IM A models on CSA(a, b) processes.

5.1. Minimum Square Error Forecasts

Theorem 3 computes the minimum mean square error forecasts for a series generated by cross-

sectional aggregation.

Theorem 3. Let xt ∼ CSA(a, b) deﬁned as in (2), and let h ∈ N, then the minimum mean square
error forecast h periods ahead, ˆxt+h, can be computed as1

ˆxT +h =

T
(cid:88)

j=h

φjνT −j+h,

(8)

where νi is computed as

with φj as in Theorem 1.

Proof: See Appendix A.

νi = xi −

i
(cid:88)

j=1

φjνi−j

∀i ∈ 0, 1, · · · , T ,

The theorem relies on the M A(∞) representation to construct the forecasts. Algorithms for
computing the minimum mean square forecasts, (8), are presented in Appendix B, and are available
on the author’s github repository at github.com/everval/Nonfractional.

Theorem 3 allows for the construction of forecasts for the correct theoretical speciﬁcation. Note
that in the theorem we have assumed that we know the parameters of the the Beta distribution
for the cross-sectionally aggregated process; for empirical applications we can use Beran et al.’s
(2010) method to estimate them. In the following, we will use these computations to assess the
forecasting performance of high-order AR, and ARF IM A models when working on long memory
generated by cross-sectional aggregation.

5.2. Forecasts With AR(p) Models

This subsection computes the forecasting eﬃciency loss of an AR(p) model when working with
a CSA(a, b) process. Theorem 4 obtains the parameter estimates for an AR(p) model ﬁtted to a
CSA(a, b) process, and computes the eﬃciency loss for one-step ahead forecasts.

1The Theorem assumes a Type II process analogous to samples of fractionally diﬀerenced processes, see Davidson

(2009). In particular, it assumes νj = 0, ∀j < 0.

9

Theorem 4. Let xt ∼ CSA(a, b) deﬁned as in (2), and estimate an AR(p) given by (1 − α1L −
α2L2 − · · · − αpLp)xt = ut, then








α1
α2
...
αP















=

1
γCSA(a,b)(1)
...

γCSA(a,b)(1)
1
...

γCSA(a,b)(p − 1) γCSA(a,b)(p − 2)

· · · γCSA(a,b)(p − 1)
· · · γCSA(a,b)(p − 2)
. . .
· · ·

...
1








−1 






γCSA(a,b)(1)
γCSA(a,b)(2)
...
γCSA(a,b)(p)








,

with γCSA(a,b)(·) deﬁned as in 4.

Furthermore, the one-step ahead forecast error variance relative to the minimum square error

forecasts, ζAR(p), is given by

ζAR(p) =

(cid:18) B(a, b − 1)
B(a, b)



(cid:32)

(cid:19)



1 +

(cid:33)

α2
i

+ 2

p
(cid:88)

i=1

p
(cid:88)

i=1



γCSA(a,b)(i)

−αi +





αjαj+i



 .

p−i
(cid:88)

j=1

Proof: See Appendix A.

As an example, consider estimating an AR(1) model to forecast a cross-sectionally aggregated
process. From Theorem 4, the autoregressive parameter is α1 = γCSA(a,b)(1) = B(a + 1/2, b −
1)/B(a, b−1), while the one-step ahead forecast error variance is ζAR(1) = (B(a, b−1)/B(a, b))(1−
γ2
CSA(a,b)(1)), which shows that the eﬃciency loss is a nonlinear function of both parameters of the
CSA(a, b) process.

To get a better sense of the nonlinearity, Figure 5 presents both the autoregressive parameter,
α1, and the eﬃciency loss, ζAR(p), while varying the ﬁrst parameter of the cross-sectional aggregated
process, ‘a’, with ﬁxed b = 1.8. On the one hand, the ﬁgure shows that as ‘a’ increases, so does the
autoregressive parameter, this is in line with the discussion above regarding the ﬁrst parameter as
a short memory regulator. On the other hand, the one-step ahead forecast error variance shows a
maximum around a = 0.5, with a forecast error variance of almost 15%. That is, more than double
the reported by Man (2003) for an I(0.1) process. Thus, the ﬁgures suggest that we could expect
worse performance using an AR(1) process to forecast a CSA(a, b) process than a pure I(d) one.

Figure 5: Estimated autoregressive parameter and forecast error variance of ﬁtted AR(1) on CSA(a, b) processes
relative to the optimum forecasts while varying ‘a’, for diﬀerent degrees of memory.

Table 2 shows the one-step ahead forecast error variance of ﬁtted AR(1) models on CSA(a, b)
processes for diﬀerent values of a, b. The losses are in line with the ones computed by Man (2003)
for the I(d) case, increasing as the memory of the process increases. Moreover, as noted by Ray
(1993) and Man (2003), we can reduce the one-step ahead forecast error variance by allowing for
more lags in the AR speciﬁcation. To get a sense of the improvements we can achieve, Table 2 also

10

Table 2: One-step ahead forecast error variance of ﬁtted AR(1) and AR(20) models on CSA(a, b) processes relative
to the optimum forecasts shown in (8). The brackets show the associated long memory parameter d.

AR(1)

AR(20)

a\b [d] 1.8 [0.1] 1.6 [0.2] 1.4 [0.3] 1.2 [0.4] 1.1 [0.45] 1.8 [0.1] 1.6 [0.2] 1.4 [0.3] 1.2 [0.4] 1.1 [0.45]

0.1
0.5
0.9
1.3
1.7

1.085
1.145
1.129
1.110
1.095

1.110
1.172
1.146
1.122
1.104

1.154
1.211
1.170
1.137
1.114

1.257
1.273
1.202
1.156
1.126

1.387
1.320
1.223
1.168
1.133

1.071
1.111
1.099
1.086
1.075

1.085
1.123
1.107
1.091
1.079

1.104
1.137
1.115
1.096
1.082

1.129
1.153
1.124
1.101
1.085

1.144
1.161
1.128
1.103
1.086

presents the one-step ahead forecast error variance of AR(20) models ﬁtted to CSA(a, b) processes
for diﬀerent values of a, b. As the table shows, increasing the order of the AR model can greatly
reduce the one-step ahead forecast error variance, particularly for larger degrees of long memory.

5.3. Forecasts With Fractional Models

This section studies the forecast performance of fractional models when working on cross-
sectional aggregated processes. In particular, we compute the one-step ahead forecast error vari-
ance of the pure I(d) model and of an ARF IM A(1, d, 0) allowing for more short term dynamics.

Theorem 5. Let xt ∼ CSA(a, b) deﬁned as in (2), then the one-step ahead forecast error variance
of the d = 1 − b/2 fractional diﬀerence of the series, ζI(d), is given by

ζI(d) = γz(0).

Furthermore, estimate an ARF IM A(1, d, 0) by ﬁtting an AR(1) model to the d = 1 − b/2
fractional diﬀerence of the series, then the autoregressive parameter, αI , and the one-step ahead
forecast error variance, ζARF IM A(1,d,0), are given by

αI =

γz(1)
γz(0)

,

ζARF IM A(1,d,0) =

γz(0)2 − γz(1)2
γz(0)2

.

Where in both expressions γz(k) is given by

γz(k) =

(cid:20)

γ∗(k)
B(a, b)

B(a, b − 1) (F1(k) − 1) + B(a +

(cid:21)
, q − 1)F2(k)

,

1
2

with

and

γ∗(k) = σ2
ε

Γ(1 + 2d)
Γ(−d)Γ(1 + d)

Γ(−d − k)
Γ(1 + d − k)

,

(cid:20)(cid:26)

F1(k)

:= F

1, a,

(cid:20)(cid:26)

F

1, a,

1 − d + k
2
1 − d − k
2

,

,

−d + k
2
−d − k
2

(cid:27)

(cid:26)

,

,

(cid:26)

(cid:27)

a + b − 1,

a + b − 1,

2 + d + k
2
2 + d − k
2

,

,

1 + d + k
2
1 + d − k
2

(cid:27)

(cid:21)

, 1

+

(cid:27)

(cid:21)

, 1

,

F2(k)

:=

−d + k
1 + d + k

(cid:20)(cid:26)

∗

F

1, p +

1
2

,

1 − d + k
2

,

2 − d + k
2

(cid:27)

(cid:26)

,

a + b −

1
2

,

2 + d + k
2

,

3 + d + k
2

(cid:27)

(cid:21)

, 1

+

−d − k
1 + d − k

∗

11

(cid:20)(cid:26)

F

1, a +

1
2

,

1 − d − k
2

,

2 − d − k
2

(cid:27)

(cid:26)

,

a + b −

1
2

,

2 + d − k
2

,

3 + d − k
2

(cid:27)

(cid:21)

, 1

,

where d = 1 − b/2, and F [·] is the generalized hypergeometric function.

Proof: See Appendix A.

Table 3 presents the one-step ahead forecast error variance of ﬁtted ARF IM A(1, d, 0), and
I(d) models on CSA(a, b) processes for diﬀerent values of a, b. The table also shows the estimated
autoregressive parameter of the ﬁtted ARF IM A(1, d, 0) model.

As the table shows, the pure fractional diﬀerenced process can be quite bad at forecasting
CSA(a, b) processes, specially for higher values of ‘a’. Once again, relating the ﬁrst parameter
functioning as a short memory regulator. Once we allow for more short term dynamics in the form
of an ARF IM A(1, d, 0) model, the forecasting performance is much in line with the one from high
order AR models. Thus, it greatly helps to allow for some short term dynamics in the modelling
scheme.

Table 3: One-step ahead forecast error variance, and estimated autoregressive parameter in parenthesis, of ﬁtted I(d)
and ARF IM A(1, d, 0) models on CSA(a, b) processes. The brackets show the associated long memory parameter d.

a\b [d] 1.8 [0.1] 1.6 [0.2] 1.4 [0.3] 1.2 [0.4] 1.1 [0.45] 1.8 [0.1]

1.6 [0.2]

1.4 [0.3]

1.2 [0.4]

1.1 [0.45]

I(d)

ARF IM A(1, d, 0)

0.1

0.5

0.9

1.3

1.7

1.077

1.084

1.112

1.158

1.186

1.345

1.253

1.202

1.176

1.169

1.615

1.435

1.318

1.240

1.212

1.880

1.611

1.431

1.309

1.263

2.138

1.778

1.538

1.374

1.312

1.072

1.147

1.131

1.083

1.103
( 0.067 ) ( -0.019 ) ( -0.091 ) ( -0.153 ) ( -0.180 )
1.138
( 0.229 )
1.123
( 0.384 )
1.108
( 0.475 )
1.097
( 0.536 )

1.132
( 0.312 )
1.121
( 0.468 )
1.107
( 0.559 )
1.096
( 0.620 )

1.127
( 0.402 )
1.118
( 0.555 )
1.104
( 0.642 )
1.093
( 0.699 )

1.152
( 0.123 )
1.125
( 0.268 )
1.106
( 0.352 )
1.093
( 0.408 )

1.147
( 0.156 )
1.125
( 0.305 )
1.107
( 0.393 )
1.095
( 0.451 )

Comparing the results from Tables 2 and 3, we see that, as was the case for long memory
series generated by the fractional diﬀerence operator, a high order AR(p) can be a good model for
forecasting long memory at short horizons. Yet, the bias-variance trade-oﬀ has to be assessed given
the number of estimated parameters. As an alternative, the ARF IM A(1, d, 0) produces similar
results while relying in only two parameters.

6. Conclusions

Even though there is no theoretical argument linking the presence of long memory in the data
with the fractional diﬀerence operator, fractionally diﬀerenced processes remain the most popular
construction in the long memory time series literature. This may be due to the existence of
eﬃcient algorithms for their simulation and forecasting. Thus, this paper presents fast algorithms
to generate long memory by a theoretically based mechanism that do not rely on the fractional
diﬀerence operator, cross-sectional aggregation.

The cross-sectional aggregated process is then contrasted to the fractional diﬀerence operator.
In particular, the paper analyses the antipersistent phenomenon.
It is proven that, for nega-
tive degrees of memory, while the autocorrelations are negative by deﬁnition for the fractional
diﬀerence operator, this restriction does not apply to the cross-sectional aggregated scheme. Fur-
thermore, the paper shows that the lack of antipersistence for cross-sectional aggregated processes

12

has implications for long memory estimators in the frequency domain which will be misspeciﬁed
in general.

Moreover, this work evaluated the eﬃciency loss of using high-order AR and ARF IM A models
to forecast long memory series generated by cross-sectional aggregation. It ﬁnds that, at short hori-
zons, high-order AR models beat a pure fractional diﬀerence process, I(d), in terms of forecasting
performance. Nonetheless, allowing for short term dynamic in the form of an ARF IM A(1, d, 0)
model produces comparable forecast performance as high-order AR models while relying on less
parameters.

The results of this paper can be used in the context of Monte Carlo simulations of long memory
estimators and forecasts. Of particular interest is the analysis of long memory in inﬂation, one
of the primer examples of cross-sectional aggregation producing long memory due to the way the
data is computed. Moreover, the results allow for ﬁnancial econometrics and climate econometrics
models to incorporate long memory forecasts consistent with the theoretical argument posed by
cross-sectional aggregation.

Acknowledgements

I would like to thank Niels Haldrup for the careful reading of this article and all the insightful

comments.

References

Adenstedt, R. K. (1974). On Large-Sample Estimation for the Mean of a Stationary Random Sequence. The Annals

of Statistics, 2(6):1095–1107.

Baillie, R. T. (1996). Long memory processes and fractional integration in econometrics. Journal of Econometrics,

73(1):5–59.

Balcilar, M. (2004). Persistence in inﬂation: does aggregation cause long memory? Emerging Markets Finance and

Trade, 40(5):25–56.

Beran, J. (1994). Statistics for long-memory processes. Chapman & Hall.
Beran, J., Feng, Y., Ghosh, S., and Kulik, R. (2013). Long-Memory Processes: probabilistic theories and Statistical

Methods. Springer.

Beran, J., Schtzner, M., and Ghosh, S. (2010). From short to long memory: Aggregation and estimation. Compu-

tational Statistics and Data Analysis, 54(11):2432–2442.

Crato, N. and Ray, B. K. (1996). Model selection and forecasting for long-range dependent processes. Journal of

Forecasting, 15(2):107–125.

Davidson, J. (2009). When is a time series I(0)? In Castle, J. and Shepherd, N., editors, The Methodology and

Practice of Econometrics, chapter 13. Oxford University Press.

Diebold, F. X. and Rudebusch, G. D. (1989). Long Memory and Persistence in Agregate Output. Journal of

Monetary Economics, 24(2):189–209.

Geweke, J. and Porter-Hudak, S. (1983). The estimation and application of long memory time series models. Journal

of Time Series Analysis, 4(4):221–238.

Granger, C. W. (1966). The Typical Spectral Shape of an Economic Variable. Econometrica, 34(1):150–161.
Granger, C. W. (1980). Long memory relationships and the aggregation of dynamic models. Journal of Econometrics,

14(2):227–238.

Granger, C. W. and Joyeux, R. (1980). An Introduction to Long Memory Time Series Models and Fractional

Diﬀerencing. Journal of Time Series Analysis, 1(1):15–29.

Haldrup, N. and Vera Vald´es, J. E. (2017). Long memory, fractional integration, and cross-sectional aggregation.

Journal of Econometrics, 199(1):1–11.

Hosking, J. R. M. (1981). Fractional diﬀerencing. Biometrika, 68(1):165–176.
Jensen, A. N. and Nielsen, M. Ø. (2014). A Fast Fractional Diﬀerence Algorithm. Journal of Time Series Analysis,

35(5):428–436.

Linden, M. (1999). Time series properties of aggregated AR(1) processes with uniformly distributed coeﬃcients.

Economics Letters, 64(1):31–36.

Man, K. S. (2003). Long memory time series and short term forecasts.

International Journal of Forecasting,

19(3):477–491.

Oppenheim, G. and Viano, M. C. (2004). Aggregation of random parameters ornstein-uhlenbeck or ar processes:

Some convergence results. Journal of Time Series Analysis, 25(3):335–350.

13

Osterrieder, D., Ventosa-Santaul`aria, D., and Vera-Vald´es, J. E. (2015). Unbalanced Regressions and the Predictive

Equation. CREATES Research Paper, 9.

Ray, B. K. (1993). Modeling Long Memory Processes for Optimal Long Range Prediction. Journal of Time Series

Analysis, 14(5):511–525.

Robinson, P. M. (1978). Statistical Inference for a Random Coeﬃcient Autoregressive Model. Scandinavian Journal

of Statistics, 5(3):163–168.

Robinson, P. M. (1995). Log-Periodogram Regression of Time Series with Long Range Dependence. The Annals of

Statistics, 23(3):1048–1072.

Zaﬀaroni, P. (2004). Contemporaneous aggregation of linear dynamic models in large economies. Journal of Econo-

metrics, 120(1):75–102.

Appendix

A. Proofs

Proofs of Theorem 1, and Theorem 2

Lemma 1 of Haldrup and Vera Vald´es (2017), which is in turn an application of the Central
Limit Theorem, show that in the limit, the cross-sectionally aggregated process, Equation (2),
allows an M A(∞) representation. Using this representation, Theorem 1 is an application of the
circular convolution Theorem, see Jensen and Nielsen (2014).

Moreover, note that the spectral density for xt evaluated at the origin is given by

fX (0) =

σ2
ε
2π

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

∞
(cid:88)

j=0

(cid:12)
2
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

,

φj

where φj are the coeﬃcients of the M A(∞) representation given by φj = B(a+j,b)1/2
the Beta function. Thus,

B(a,b)1/2 , with B(·, ·),

fX (0) =

σ2
ε
2π

(cid:12)
(cid:12)
∞
(cid:88)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

j=0

φj

=

σ2
ε Γ(b)
2πB(a, b)

= ca,b < ∞,

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

2

=

σ2
ε
2πB(a, b)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

∞
(cid:88)

j=0

B(a + j, b)1/2

(cid:12)
2
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

=

σ2
ε
2πB(a, b)

(cid:12)
(cid:12)
∞
(cid:88)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

j=0

Γ(a + j)1/2Γ(b)1/2
Γ(a + j + b)1/2

(cid:12)
2
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

∞
(cid:88)

j=0

Γ(a + j)1/2
Γ(a + j + b)1/2

(cid:12)
2
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

≈

σ2
ε Γ(b)
2πB(a, b)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

∞
(cid:88)

j=0

j−b/2

(cid:12)
2
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

=

σ2
ε Γ(b)
2πB(a, b)

ζ(−b/2)2

where Γ(·), ζ(·) are the Gamma and EulerRiemann zeta function, respectively. Moreover, note
that all terms in the expression are positive and thus ca,b > 0.

Proof of Theorem 3

Assuming a Type II process, the Lemma shows that xt can be expressed as










xT
xT −1
xT −2
...
x0










=



1 φ1 φ2
1 φ1
0


1
0
0


...
...
...


0
0
0

φT

· · ·
· · · φT −1
· · · φT −2
. . .
· · ·

...
1



















νT
νT −1
νT −2
...
ν0










,

(9)

with φt, νt deﬁned in Theorem 1. Solving for νj, this represents a system with an upper triangular
matrix to which fast algorithms exist. Once we have solved it, the minimum mean square error
forecast h periods ahead can be easily obtained.

14

Proofs of Theorem 4, and Theorem 5

Let xt ∼ CSA(a, b) and denote Φ(z:,t) := (cid:80)∞

j=0 φjzt−j the function that applies the ﬁlter
associated to the M A(∞) representation of the CSA(a, b) process to z:,t = (zt, zt−1, · · · ). That is,

xt =

∞
(cid:88)

j=0

φjνt−j := Φ(ν:,t),

where νj ∼ N (0, σ2

ε ) and φj = (B(p + j, q)/B(p, q))1/2 , ∀j ∈ N.

Let ut be the residual of a ﬁtted AR(p) process to xt, i.e.,

ut = (1 − α1L − α2L2 − · · · − αpLp)xt,

where L is the lag operator and {α1, · · · , αp} are the ﬁtted parameters. Hence,

ut = (1 − α1L − α2L2 − · · · − αpLp)xt

= (1 − α1L − α2L2 − · · · − αpLp)Φ(ν:,t)
= Φ(ν:,t) − α1Φ(ν:,t−1) − α2Φ(ν:,t−2) − · · · − αpΦ(ν:,t−p)
= νtφ0 + νt−1(φ1 − α1φ0) + · · · + νt−s(φs − · · · − αpφs−p) + · · ·

=

∞
(cid:88)

j=0

˜φjνt−j,

˜φj :=






φ0
φ1 − α1φ0
φ1 − α1φ1 − α2φ0
...
φj − · · · − αpφj−p

j = 0
j = 1
j = 2
...
j ≥ p

where

Thus,

var(ut)σε =

∞
(cid:88)

j=0

2

˜φj

= φ2

0 + (φ1 − α1φ0)2 + · · · +

∞
(cid:88)

j=p

(φj − α1φj−1 − · · · − αpφj−p)2





∞
(cid:88)


 (1 + α2

1 + · · · + α2

p) + 2

φ2
j





∞
(cid:88)



φjφj−1

 (−α1 + α1α2 + · · · + αp−1αp)

j=0


+2



∞
(cid:88)



j=1



φjφj−2

 (−α2 + α1α3 + · · · + αp−2αp) + 2





φjφj−p

 (−αp)

∞
(cid:88)

j=p

(cid:2)(1 + α2

1 + · · · + α2

p) + 2γCSA(a,b)(1)(−α1 + α1α2 + · · · + αp−1αp)

+ · · · + 2γCSA(a,b)(p)(−αp)(cid:3) .

Which shows that minimizing with respect to {α1, · · · , αp} yields the Yule-Walker system of

equations, the result follows.

Finally, for Theorem 5, the ﬁrst part regarding the autocorrelation function of a fractionally
diﬀerenced cross-sectional aggregated process is Theorem 3 in Haldrup and Vera Vald´es (2017).
For the second part, the proof follows the same steps as for the pure AR(p) case above replacing the
autocorrelation function of the pure CSA(a, b) process for the one of the fractionally diﬀerenced
one.

15

=

=

j=2
(cid:18) B(a, b − 1)
B(a, b)

(cid:19)

B. Code Implementation

Nonfractional Memory Generation

R code

c s a d i f f = function ( x , p , d ) {
iT = length ( x )
n = nextn ( 2 ∗iT − 1 , 2 )
k = 0 : ( iT −1)
b = ( beta ( p+k , d ) /beta ( p , d ) ) ˆ ( 1 / 2 )
dx = f f t ( f f t ( c ( x , rep ( 0 , n − iT ) ) ) ∗
f f t ( c ( b , rep ( 0 , n − iT ) ) ) ,
return (Re( dx [ 1 : iT ] ) )
}

inverse = T) / n ;

Matlab code

function [ cx ] = c s a d i f f ( x , p , q )
T = s i z e ( x , 1 ) ;
np2 = 2 . ˆ nextpow2( 2 ∗T−1);
c o e f s = [ 1 , ( beta ( p +(1:T−1) , q )
) . ˆ ( 1 / 2 ) ] ;
beta ( p , q )
cx = i f f t ( f f t ( x , np2 ) . ∗ f f t ( c o e f s ’ , np2 ) ) ;
cx = cx ( 1 : T,
end

. / . . .

: ) ;

Nonfractional Memory Forecasts

R code

get e r r c s a = function ( x , p , q ) {
iT = length ( x )
k = 0 : ( iT −1)
b = ( beta ( p+k , q ) /beta ( p , q ) ) ˆ ( 1 / 2 )
zen = c ( 1 , rep ( 0 , iT −1))
ma = pracma : : T o e p l i t z ( b , zen )
v j s = solve (ma, x )

return ( v j s )
}

f o r e c a s t c s a = function ( x , p , q , h ) {
iT = length ( x )
k = seq ( 1 , ( iT ) , 1 )
b = ( beta ( p+k , q ) /beta ( p , q ) ) ˆ ( 1 / 2 )

e r r = get e r r c s a ( x , p , q )
e r r v = rev ( e r r )
f x = rep ( 0 , h )
fo r ( i i
f x [ i i ] = crossprod ( b [ i i : iT ] , e r r v [ 1 : ( iT− i i + 1 ) ] )
}
return ( f x )
}

i n 1 : h ) {

16

Matlab code

function [ v j s ] = g e t e r r c s a ( x , p , q )
T = s i z e ( x , 1 ) ;
c o e f s = [ 1 , ( beta ( p +(1:T−1) , q )
zen = [ 1 , zeros ( 1 ,T− 1 ) ] ;
ma = t o e p l i t z ( c o e f s , zen ) ;

. / beta ( p , q )

) . ˆ ( 1 / 2 ) ] ;

v j s = ma\x ;
end

function [ f x ] = f o r e c a s t c s a ( x , p , q , h )
T = s i z e ( x , 1 ) ;
c o e f s = [ 1 , ( beta ( p +(1:T−1) , q )
c o e f s = f l i p l r ( c o e f s ) ;

. / beta ( p , q )

) . ˆ ( 1 / 2 ) ] ;

e r r = g e t e r r c s a ( x , p , q ) ;
f x = zeros ( h , 1 ) ;
fo r i i = 1 : h
f x ( i i , 1 ) = c o e f s ( 1 , 1 : T− i i ) ∗ e r r ( i i +1:T , 1 ) ;
end
end

17

