1
2
0
2

c
e
D
6

]
P
A

.
t
a
t
s
[

3
v
9
0
4
1
1
.
5
0
1
2
:
v
i
X
r
a

A tutorial on reproducing a predeﬁned autocovariance
function through AR models: Application to stationary
homogeneous isotropic turbulence

December 7, 2021

Cristobal Gallego-Castillo1,∗, Alvaro Cuerva-Tejero1, Mohanad Elagamy1,
Oscar Lopez-Garcia1, Sergio Avila-Sanchez1

1DAVE (ETSIAE), Universidad Polit´ecnica de Madrid. Pza. Cardenal Cisneros, 3, 28040, Madrid

∗Corresponding author: Cristobal Gallego-Castillo.

Email: cristobaljose.gallego@upm.es.
ORCID:0000-0002-8249-5179

Abstract

Sequential methods for synthetic realisation of random processes have a number of ad-
vantages compared with spectral methods. In this article, the determination of optimal au-
toregressive (AR) models for reproducing a predeﬁned target autocovariance function of a
random process is addressed. To this end, a novel formulation of the problem is developed.
This formulation is linear and generalises the well-known Yule-Walker (Y-W) equations and
a recent approach based on restricted AR models (Krenk-Møller approach, K-M). Two main
features characterise the introduced formulation: (i) ﬂexibility in the choice for the autoco-
variance equations employed in the model determination, and (ii) ﬂexibility in the deﬁnition
of the AR model scheme. Both features were exploited by a genetic algorithm to obtain op-
timal AR models for the particular case of synthetic generation of homogeneous stationary
isotropic turbulence time series. The obtained models improved those obtained with the Y-W
and K-M approaches for the same model parsimony in terms of the global ﬁtting of the target
Implications for the reproduced spectra are also discussed. The
autocovariance function.
formulation for the multivariate case is also presented, highlighting the causes behind some
computational bottlenecks.

Keywords: random processes · numerical generation · autoregressive models · autoco-

variance function · turbulence

1

Introduction

The synthetic realisation of a random process (or, simply, the numerical generation of a random process)
refers to the computational generation of time/space series that simulate the random behaviour of a
dynamical system accurately.
In this context, accurately means that the obtained time/space series
reproduce suﬃciently well a number of statistical features deﬁned beforehand, like time/space covariance
and cross spectra.

Numerical generation has been employed in numerous scientiﬁc and engineering problems over decades,
such as the simulation of earthquake ground motions [Chang et al., 1981, Gersch and Kitagawa, 1985, De-
odatis and Shinozuka, 1988], ocean waves [Spanos and Hansen, 1981, Spanos, 1983, Samii and Vandiver,
1984], atmospheric variables (mainly wind velocity ﬂuctuations [Reed and Scanlan, 1983, Li and Kareem,
1990, Li and Kareem, 1993, Deodatis, 1996, Di Paola and Gullo, 2001, Di Paola and Zingales, 2008, Ka-
reem, 2008, Krenk, 2011, Krenk and Møller, 2019], but also pressure [Reed and Scanlan, 1983], temperature

1

 
 
 
 
 
 
and precipitation [Sparks et al., 2018], and variables for hydrological modelling like discharge and ﬂood
frequency [Beven, 2021]), random vibration systems in the context of structural system identiﬁcation
[Gersch and Luo, 1972, Gersch and Foutch, 1974, Gersch and Liu, 1976, Gersch and Yonemoto, 1977] and
spatial structures of several geological phenomena [Sharifzadehlari et al., 2018, Soltan Mohammadi et al.,
2020].

While many of the aforementioned real-life processes can only be rigourously represented through
a non-stationary and/or non-homogeneous random process, stationarity and homogeneity are usually
assumed in the modelling for convenience.
In some cases, the obtained algorithms have served as a
basis for the development of strategies oriented to non-stationary random processes, as in the case of
evolutionary spectra, see the seminal paper [Deodatis and Shinozuka, 1988].

There are diﬀerent types of numerical generation approaches. Although an agreed classiﬁcation based
on common names for the diﬀerent models lacks, see [Kleinhans et al., 2009, Liu et al., 2019], two fami-
lies, referred to as spectral and sequential methods, are usually identiﬁed. Spectral methods are based on
strategies like harmonic superposition and inverse Fast Fourier Transform (FFT) [Shinozuka and Deodatis,
1991, Shinozuka and Deodatis, 1996]. These methods require information regarding the spectral charac-
terisation of the random process as an input, for example, in the form of predeﬁned target cross power
spectral density (CPSD) functions, coherence functions, etc. Some limitations of spectral methods are
related to the fact that the process realisation needs to be synthesised in the whole time/space domain at
once, which translates into high computational requirements for long-duration/long-distance multivariate
and/or multi-dimensional processes [Kareem, 2008].

Sequential methods, also referred to as digital ﬁlters, are usually based on time series linear models,
such as autoregressive (AR) and Moving Average (MA) models, or a combination of both (ARMA),
and their multivariate versions (VAR, VMA and VARMA, respectively). Compared to spectral methods,
sequential methods are less intensive in computational requirements during the synthesis, as only the model
coeﬃcients need to be stored. In addition, synthesis is a sequential process that can be stopped at a desired
length of the time series and restarted later to lengthen the simulation. However, the determination of the
model coeﬃcients may demand high computational memory for multi-dimensional and/or multivariate
problems. Model coeﬃcients can be derived from a predeﬁned target autocovariance function deﬁned in
time and/or space1, though some approaches introduce spectral information as input as well, see [Spanos
and Hansen, 1981, Spanos, 1983].

Another advantage of sequential methods is that AR, MA, and ARMA models have theoretical expres-
sions of their autocovariance and PSD functions that can be computed directly from the model coeﬃcients.
This fact allows comparing directly the target autocovariance function with the model theoretical auto-
covariance function to assess the accuracy of the model in reproducing the desired statistical information,
while this comparison for the spectral methods is based on sample functions estimated from a ﬁnite num-
ber of ﬁnite-length realisations, subjected to smearing and leakage eﬀects [Stoica and Moses, 2005]. It is
remarked that statistical bias aﬀecting the sample autocovariance and PSD functions estimated from time
series has to be taken into account when generating synthetic time series, regardless the nature of the
method employed (spectral/sequential), in order to properly set the parameters of the simulations, like the
time series length. [Dimitriadis and Koutsoyiannis, 2015] provides expressions for the bias of estimators
for both the PSD and autocovariance function, and discusses the advantages of the climacogram as an
alternative statistical object to characterise random processes.

It is noted that, in the context of stationary random processes, both spectral and sequential methods
can be applied regardless the form of the input information, since the power spectrum and the autoco-
variance function are Fourier pairs, thus they are two forms of providing the same statistical information.
However, going from a PSD to an autocovariance function (and vice versa), except for particular cases
that admit a theoretical formulation, requires a numerical implementation of the corresponding Fourier
transform. For this reason, spectral methods are usually applied to problems where the input information
is provided in the frequency domain, while time domain descriptions of a random process represent natural
inputs for sequential methods.

This tutorial is focused on the determination of AR models to optimally reproduce a predeﬁned target
autocovariance function. Indeed, optimal is a notion that needs to be clearly deﬁned, as it will be discussed
in Section 5. To frame the problem, consider a one-dimensional univariate discrete random process,
{zt(α)}, where t ∈ N is a time index and α ∈ N is the index of the realisation. Thus, zti (α) is a random
variable associated to time index ti, and {zt(αj)} is a time series corresponding to the αj-th realisation of
the random process. To simplify notation, the realisation index α will be omitted, so that {zt} will be used
to refer a time series for a generic realisation α, and zt to refer the random variable associated to time t.

1For simplicity, we will refer only to univariate time series from here on, unless stated otherwise

2

The random process is assumed to be Gaussian, stationary, and zero-mean. In addition, the existence of
the integral time scale (i.e. the integral of the autocorrelation function) is assumed. Consequently, long-
term persistence processes (Hurst phenomenon), characterised by an inﬁnite integral scale, are excluded.
The reason for this hypothesis is that this research considers stationary AR models, whose integral time
scale always exists because the autocorrelation function decays exponentially. Long memory processes
can be handled through diﬀerent model types, like Fractional Autoregressive-Moving Average (FARMA)
models [Hip, 1994].

The general formulation of an AR model of order p, AR(p), is as follows:

zt =

p
(cid:88)

i=1

ϕi zt−i + σ εt,

(1)

where ϕi for i = 1, ..., p are the regression coeﬃcients of the AR model, and σ εt represents the random
term; εt is a sequence of independent and identically distributed (iid) random variables with zero mean
and unit variance, and σ, here referred to as the noise coeﬃcient, scales the variance of the random term,
given by Var[σ εt] = σ2. Thus, the AR(p) model has p + 1 parameters, comprising p regression coeﬃcients
and one noise coeﬃcient.

Figure 1 illustrates the diﬀerent elements involved in the synthetic generation of a random process
with an AR model (also valid for MA and ARMA models). γT
is the target autocovariance funcion,
l
which depends only on the time lag l under the assumption of stationarity (the formal deﬁnition of the
autocovariance function is provided in Section 2). In some applications, γT
is derived from theoretical
l
models and admits a mathematical expression, but in real-life problems it usually has to be estimated
from observations. Step (1) represents the determination of the AR coeﬃcients from γT
l ; this step requires
a methodology and a choice for the model order, p. γAR
is the theoretical autocovariance function, and
it can be computed directly from the AR model coeﬃcients, step (2) in the ﬁgure. Step (3) represents
the synthesis of the random process through the AR model. By using a sequence of random values as
inputs, {εt}, the AR model can be employed to generate realisations of the process in the form of time
series. γα
l denotes the sample autocovariance function computed for realisation α. Averaging N sample
autocovariance functions yields the ensemble autocovariance function, γE
when
N −→ ∞. Thus, the objective in step (1) is to deﬁne a methodology that yields an AR model with a
theoretical autocovariance function, γAR
l ; this can be veriﬁed
without the need for generating a large number of realisations N to compute γE
l .

, that optimally reproduces the target, γT

converges to γAR

l . γE
l

l

l

l

Figure 1: Scheme with the diﬀerent elements involved in the synthetic generation of a random
process with an AR model.

Regarding the methodologies to obtain the AR(p) model parameters, the vast majority of works make
use of the Yule-Walker (Y-W) equations [Spanos and Hansen, 1981, Spanos, 1983, Reed and Scanlan,
1983, Samaras et al., 1985], which establish relationships between the p + 1 AR parameters and the p + 1
ﬁrst autocovariance terms, from l = 0 to l = p. These relationships arise simply from applying the
deﬁnition of the autocovariance function for time lags from 0 to p. In the context of this work, these

3

  γlTγlARAR model(1)methodologyp(2){εt}α=1α=2α=N. . .. . .γlα=1γlα=2γlα=NγlE(3)expressions are referred to as autocovariance equations for time lags from 0 to p. This approach leads
to a perfect match of the ﬁrst p + 1 terms of the target autocovariance function. Consequently, under
this approach, there exists no means to improve the matching between the target and the theoretical AR
autocovariance functions for lags larger than the model order, p. This fact is problematic for processes with
high inertia (i.e., large integral time scale) or situations in which small sampling times of the time series are
employed, because large values of the model order p are required to guarantee the matching of the target
autocovariance function in a suﬃciently wide range of time lags. But large p values reduce model parsimony
and increase the computational cost to determine the model coeﬃcients [Spanos, 1983, Dimitriadis and
Koutsoyiannis, 2018]. Model parsimony refers to achieving a certain model performance with the lowest
number of model parameters, and is considered a key feature in time series modelling [Box et al., 2016].
A proposal to preserve model parsimony is to use ARMA models [Gersch and Liu, 1976, Samaras et al.,
1985]. Several methodologies have been developed, typically based on multistage approaches that build
the ARMA model by combining an AR(p) model previously deﬁned with a MA(q) component. However,
the potential of these approaches may be limited because MA processes show nonzero autocovariance only
in the ﬁrst q + 1 terms, see for example [Madsen, 2007]. Thus, the improvement of the matching between
target and the theoretical autocovariance functions for large time lags could be conditioned to considering
high q values. In this article, a proposal for overcoming this limitation is introduced in Section 2.2. It
consists in including autocovariance equations for lags larger than p in the procedure.

In a recent paper [Krenk and Møller, 2019], an interesting proposal based on AR models only with
regression coeﬃcients at certain time lags was introduced for the synthetic generation of turbulent wind
ﬁelds. The theoretical formulation is provided for a generic sequence of lags, and the simulations are
performed for AR models with an exponential scheme (regression coeﬃcients only for time lags 2k, k =
0, 1, 2, ...). In econometrics, such models are usually referred to as restricted AR models [Giannini and
Mosconi, 1987], because they can be seen as a particular case of an AR(p) model for which some of the
regression coeﬃcients are imposed to be zero.2 Thus, the actual number of regression coeﬃcients is lower
than the AR order, and the capacity of matching exactly the ﬁrst p + 1 terms of the target autocovariance
function is lost. However, the trade-oﬀ between model parsimony and target autocovariance function
reproducibility for a wide range of lags was improved. In our opinion, one limitation of that work is that
the exponential scheme of the model was assumed to be reasonably good for the considered application,
and no further discussion is provided concerning the impact of diﬀerent model schemes on the results.
In what follows, the methodology presented in [Krenk and Møller, 2019] will be referred to as the K-M
approach.

Within this context, this article introduces a general formulation to determine the parameters of a
restricted AR model from a predeﬁned target autocovariance function. Under this general framework, it
will be shown that both the Y-W approach and the K-M approach could be seen as particular cases of the
presented formulation. The described approach requires a reduced number of input parameters related
to the model scheme and the employed autocovariance equations. An optimisation procedure based on
genetic algorithms is applied to obtain AR models that reproduce the target autocovariance function more
accurately than the Y-W and K-M approaches for the same model parsimony. The main ideas contained
in this tutorial and its research contributions are as follows:

• The Yule-Walker approach to obtain an AR(p) model from a target autocovariance function is
described, emphasising the classical result consisting in the perfect matching of the ﬁrst p + 1 terms
of the target autocovariance function as a consequence of selecting a set of autocovariance equations
for time lags from 0 to p.

• We show that using autocovariance equations for time lags larger than p may improve the matching

of the target autocovariance function for lags beyond the model order.

• The potential of restricted AR models for improving the matching of a target autocovariance function

is revisited by considering the K-M approach introduced in [Krenk and Møller, 2019].

• We introduce a general formulation for the AR parameters determination from a target autocovari-
ance function. The formulation is general in the sense that it provides ﬂexibility in the choice for
the autocovariance equations and in the deﬁnition of the AR model scheme.

• The introduced formulation is exploited by a genetic algorithm to obtain optimal AR models without
a pre-deﬁned model scheme. The considered application is based on a stationary, homogeneous, and
isotropic (SHI) turbulence model. Results are compared to those obtained with the Y-W approach
and the K-M approach.

2In line with the literature, AR models refer to unrestricted AR models, unless stated otherwise.

4

• The introduced general formulation is extended to the multivariate case. This leads to some com-

putational bottlenecks that are highlighted.

The article is organised as follows. Section 2 describes the relationships between the parameters of an
AR model and its theoretical autocovariance function, emphasising the impact of the selected autocovari-
ance equations on the matching of the target autocovariance function. The case of restricted AR models
is addressed in Section 3, where the focus is placed on the role of the model scheme. The general formu-
lation for the determination of a restricted AR model from a predeﬁned target autocovariance function is
introduced in Section 4. Section 5 contains the optimisation exercise based on genetic algorithms. The
generalisation of the problem to the mulivariate case is brieﬂy described in Section 6. The paper ends with
the main conclusions gathered in Section 7. The article includes a number of examples and reﬂections to
facilitate comprehension.

2 The autocovariance equations of an AR model

In time series analysis, the covariance between two random variables zt1 and zt2 is usually denoted by
γt1,t2 . Under the assumption of stationarity, the autocovariance depends only on the time lag, l = t1 − t2,
and is referred to as the autocovariance function:

γl = Cov[ zt, zt−l ] = E[ zt zt−l ].

(2)

Note that the autocovariance function is symmetric, γ−l = γl. Note also that, since the random term
in (1) is independent, there is no dependency between the random term at time t, σ εt, and previous values
of the process, zt−l for l > 0. Indeed, the following expression can be demonstrated [Madsen, 2007]:

E[ (σ εt) zt−l ] =

(cid:40)

σ2 for l = 0
0 for l > 0

.

(3)

Given these considerations, Equation (2) together with (1) and (3) provide a means to generate
analytical expressions that relate the autocovariance function for diﬀerent time lags and the AR model
parameters. As mentioned above, these expressions are here referred to as autocovariance equations. The
autocovariance equation for lag l = 0 is:

γ0 = Cov[ zt, zt ] = E[ zt zt ] = E

(cid:34) (cid:32) p

(cid:88)

i=1

(cid:33)

(cid:35)

ϕi zt−i + σ εt

zt

=

p
(cid:88)

i=1

ϕi γ−i + σ2.

The autocovariance equation for a generic positive time lag l is:

γl = Cov[ zt, zt−l ] = E[ zt zt−l ] = E

(cid:34) (cid:32) p

(cid:88)

(cid:33)

(cid:35)

ϕi zt−i + σ εt

zt−l

=

p
(cid:88)

ϕi γl−i.

(4)

(5)

i=1
Note that Equation (4) is the only one among all autocovariance equations that includes the noise
coeﬃcient, σ. Actually, this equation deﬁnes the relationship between the variance of the AR process, γ0,
and the variance of the random term, σ2.

i=1

Equations (4) and (5) are the basis for computing the theoretical autocovariance function of a given
AR model, addressed in Section 2.1, and for obtaining the AR model parameters from a predeﬁned target
autocovariance function, see Section 2.2.

2.1 Computing the theoretical autocovariance function of an AR model

The objective of this section is to compute the ﬁrst n + 1 terms of the theoretical autocovariance function
of an AR(p) model, γAR
n , assuming that the AR parameters, ϕi for i = 1, ..., p and σ, are
known.

, ..., γAR

, γAR
1

0

Without loss of generality, the case of an AR(2) model is considered. The following expression repre-
sents the autocovariance equations for lags from l = 0 to l = n, see equations (4) and (5), in the form of
a matrix equation, where the autocovariance terms have been gathered into the independent vector.

5

l = 0 :
l = 1 :
l = 2 :
l = 3 :
...
l = n :












0
0
0
...
0

−ϕ2 −ϕ1

1

−ϕ2 −ϕ1

0
1

0
0
1

0
0
...
0

−ϕ2 −ϕ1

0
...
0

−ϕ2 −ϕ1
...
...
0
0

0
0
0
1
...
0

. . .
. . .
. . .
. . .
. . .
. . .












0
0
0
0
...
1
















=
















γ−2
γ−1
γ0
γ1
γ2
γ3
...
γn












σ2
0
0
0
...
0












.

(6)

Equation (6) represents a linear system of n+1 equations with n+3 unknowns.3 However, by applying
symmetry in the autocovariance function, γ−l = γl, it is possible to remove terms γ−1 and γ−2 from the
independent vector. This allows one expressing the system of equations (6) with as many equations as
unknowns:

l = 0 :
l = 1 :
l = 2 :
l = 3 :
...
l = n :












1
−ϕ1
−ϕ2
0
...
0

−ϕ1
1 − ϕ2
−ϕ1
−ϕ2
...
0

−ϕ2
0
1
−ϕ1
...
0

0
0
0
1
...
0

. . .
. . .
. . .
. . .
. . .
. . .


































γ0
γ1
γ2
γ3
...
γn

0
0
0
0
...
1

=























σ2
0
0
0
...
0

.

(7)

From that, the following expression for the theoretical autocovariance function of the AR(2) model is

readily obtained:












γAR
0
γAR
1
γAR
2
γAR
3
...
γAR
n












=












1
−ϕ1
−ϕ2
0
...
0

−ϕ1
1 − ϕ2
−ϕ1
−ϕ2
...
0

−ϕ2
0
1
−ϕ1
...
0

0
0
0
1
...
0

. . .
. . .
. . .
. . .
. . .
. . .



−1 



















0
0
0
0
...
1












σ2
0
0
0
...
0

.

(8)

Equation (8) can be employed to obtain an arbitrary number n of terms of γAR

. Increasing n comes
at the expense of increasing the dimension of the matrix to be inverted, thus, the computational memory
requirements. An alternative approach for computational alleviation consists in solving the subsystem
given by the p + 1 ﬁrst autocovariance equations in order to obtain γAR

, ..., γAR

,

l

0

p



 =





γAR
0
γAR
1
γAR
2





1
−ϕ1
−ϕ2

−ϕ1
1 − ϕ2
−ϕ1

−ϕ2
0
1



−1 







 ,

σ2
0
0

(9)

and then to compute recursively γAR
that the accumulation of rounding errors may lead to inaccurate estimations of γAR

for l > p through Equation (5). The counterpart of this approach is

for large l values.

l

l

for an AR(2) model given by ϕ1 = 1.2, ϕ2 = −0.3 and σ = 0.5,

As an example, Figure 2 shows γAR

l

computed for lags up to l = 20.

2.2 Computing the AR model parameters from a predeﬁned target au-

tocovariance function

The objective of this section is to compute the parameters of an AR model from a predeﬁned target
autocovariance function, γT
l , assuming that the target is available for any time lag l. It will be shown that
only a limited number of values of γT
l are required, depending on the employed autocovariance equations.
The p + 1 model parameters, ϕ1, ϕ2, ..., ϕp and σ, are computed from p + 1 autocovariance equations.
The traditional approach to this problem considers the autocovariance equations for lags l = 0, 1, ..., p,
which leads to the Yule-Walker (Y-W) equations. For this reason, this approach is here referred to as the
Y-W approach. For illustrative purposes, consider the case of an AR(3) model, note that symmetry in
the autocovariance function has already been applied:

3 In the general case of an AR(p) model, n + 1 equations with n + p + 1 unknowns.

6

Figure 2: Theoretical autocovariance function of a predeﬁned AR(2) model, ϕ1 = 1.2, ϕ2 = −0.3
and σ = 0.5.

l = 0 : γ0 = ϕ1 γ1 + ϕ2 γ2 + ϕ3 γ3 + σ2,
l = 1 : γ1 = ϕ1 γ0 + ϕ2 γ1 + ϕ3 γ2,
l = 2 : γ2 = ϕ1 γ1 + ϕ2 γ0 + ϕ3 γ1,
l = 3 : γ3 = ϕ1 γ2 + ϕ2 γ1 + ϕ3 γ0.

Equation (10) can be written in matrix form as:













=







1
0
0
0

γ1
γ0
γ1
γ2

γ2
γ1
γ0
γ1







γ3
γ2
γ1
γ0

γ0
γ1
γ2
γ3







.







σ2
ϕ1
ϕ2
ϕ3

(10)

(11)

By replacing in (11) the autocovariance terms γi by the corresponding target values, γT
0 , ..., γT
3 :

model parameters are obtained from γT

i , the four AR









−1 



σ2
ϕ1
ϕ2
ϕ3

γT
2
γT
1
γT
0
γT
1
A key consequence of employing the autocovariance equations for lags l = 0, ..., 3 is that number of
model parameters and the number of required γT
l values is the same, which leads to an AR(3) model with
a theoretical autocovariance function that matches exactly the employed target values. This conclusion
can be extended for an AR(p) and the ﬁrst p + 1 terms of the target autocovariance function.

γT
1
γT
0
γT
1
γT
2

γT
3
γT
2
γT
1
γT
0

γT
0
γT
1
γT
2
γT
3

1
0
0
0

























(12)

=

.

As an example, the following AR(3) model has been obtained for the target autocovariance function

described in Appendix B:

zt = 0.663 zt−1 + 0.099 zt−2 + 0.044 zt−3 + 0.636 εt.

(13)

The theoretical autocovariance function of the obtained AR model, γAR

, and the target autocovariance
function, γT
τ , are shown in Figure 3. The four target values employed during the model determination
have been highlighted. Note that the theoretical autocovariance function of the AR(3) model matches
exactly the employed target autocovariance values, but increasing diﬀerences with the target are observed
for larger lags.

τ

Note also the following comments:

(i) The determination of the AR model involves a matrix inversion. Care should be taken with issues
related to ill-conditioned matrices that may arise from some target autocovariance functions deﬁned

7

0246810121416182000.511.52Figure 3: Autocovariance function of an AR(3) model and target autocovariance function. The
AR model was obtained by considering the autocovariance equations for lags l = 0, 1, 2, 3. The
employed values of the target autocovariance function are highlighted.

arbitrarily. For example, target values γT
0 = 1, γT
matrix in (12), regardless the value of γT
3 .
(ii) While the autocovariance function of the obtained AR model reproduces exactly γT
l

for l = 0, ..., p,
no constraints have been imposed on the autocovariance function for larger time lags l > p. This
implies that there is no means to improve the matching between γAR
for time lags larger
than p.

2 = −0.5 lead to a non-invertible

1 = 0.5 and γT

and γT
l

l

is it possible to introduce information of γT
l

The last comment leads to an important question:

for
time lags larger than the AR model order in the determination of the model parameters? If so, that
would provide a means to obtain AR(p) models for which there exists some control on γAR
for l > p,
potentially improving the trade-oﬀ between model parsimony and matching between target and theoret-
ical autocovariance function, compared to the Y-W approach. To address this question, autocovariance
equations for time lags larger than p could be considered. Let us deﬁne vector l = [l1, l2, ..., lN ] with the
N positive lags corresponding to the autocovariance equations employed in the model determination. By
default, the autocovariance equation for l = 0 is always required to determine the noise parameter, σ. For
this reason, only positive lags are speciﬁed in vector l. Note also that N must be equal to the number
of model regression coeﬃcients, p. As an example, consider the set of autocovariance equations obtained
with l = [1, 2, 5] for an AR(3) model:

l

l = 0 : γ0 = ϕ1 γ1 + ϕ2 γ2 + ϕ3 γ3 + σ2
l = 1 : γ1 = ϕ1 γ0 + ϕ2 γ1 + ϕ3 γ2
l = 2 : γ2 = ϕ1 γ1 + ϕ2 γ0 + ϕ3 γ1
l = 5 : γ5 = ϕ1 γ4 + ϕ2 γ3 + ϕ3 γ2.

Now, the AR model parameters are given by:







σ2
ϕ1
ϕ2
ϕ3







=







1
0
0
0

γT
1
γT
0
γT
1
γT
4

γT
2
γT
1
γT
0
γT
3

γT
3
γT
2
γT
1
γT
2



−1 















.

γT
0
γT
1
γT
2
γT
5

(14)

(15)

Equation (15) reveals that, in this case, the four AR model parameters are computed from six terms
5 . For the particular case of the target autocovariance

of the target autocovariance function, from γT
function described in Appendix B, the obtained AR(3) model is:

0 to γT

zt = 0.657 zt−1 + 0.066 zt−2 + 0.092 zt−3 + 0.635 εt,

(16)

8

0246810121416182000.20.40.60.81which diﬀers notably from the model obtained with the Y-W approach, see (13). Figure 4 shows the target
and the theoretical autocovariance functions, γT
l and γAR
, respectively. The six target values employed
during the model determination have been highlighted. It can be seen that γAR
does not match exactly
γT
for any time lag. However, a visual comparison with Figure 3 reveals that the global matching is
l
improved.

l

l

Figure 4: Autocovariance function of an AR(3) model and target autocovariance function. The
AR model was obtained by considering the autocovariance equations for lags l = 0 and l = [1, 2, 5].
The employed values of the target autocovariance function are highlighted.

l

From this analysis, it can be concluded that, in the determination of the parameters of an AR(p)
model, using autocovariance equations for lags larger than p leads to a number of required target terms
higher than the number of model parameters. Since the autocovariance equations represent constraints
between the AR model parameters and certain terms of the autocovariance function, this inequality makes
that γAR
does not exactly match the target values employed in the model determination, Equation (15),
as it was the case for the Y-W approach; γAR
only fulﬁls the constraints determined by the selected
autocovariance equations. However, given a particular target γT
l and a considered AR model order p,
optimal decisions on the selected autocovariance equations (deﬁned in vector l) may lead to AR models
that reproduce γT
l globally better, as compared with the models obtained with the Y-W approach, as it
was shown in the previous example. Thus, considering vector l as an input parameter in the determination
of an AR(p) model introduces ﬂexibility as compared with the assumption l = [1, ..., p] that underlies the
Y-W approach, and represents a path for improvement in reproducing a predeﬁned target autocovariance
function. To the authors knowledge, this strategy has not been addressed previously in the literature.

l

3 The autocovariance equations of a restricted AR model

In a restricted AR model, not all regression coeﬃcients from ϕ1 to ϕp are considered. Let us deﬁne vector
j = [j1, j2, ..., jN ], with N < p, containing the lags of the regression terms included in the model. The AR
order is given by the regression term with the highest lag, p ≡ jN . Note that N represents the number
of regression coeﬃcients of the model. To distinguish between restricted and unrestricted AR models, the
following notation will be employed for restricted AR models:

A restricted AR(p, j) model can be seen as a particular case of an AR(p) model for which:

zt =

N
(cid:88)

i=1

aji zt−ji + b εt,

and

ϕh =

(cid:40)

ah , for h ∈ j
0 , for h /∈ j

, h = 1, ..., p.

9

(17)

(18)

0246810121416182000.20.40.60.81Note also that an AR(p) can be seen as a particular case of a restricted AR(p, j) model for which:

σ = b.

(19)

j = [ 1, 2, 3, ..., p ],

(20)

provided that the constraint N < p is relaxed.

The autocovariance equations of a restricted AR(p, j) process can be readily obtained by combining
the autocovariance equations of the corresponding unrestricted AR(p) model, see Section 2, with equations
(18) and (19):

γ0 =

N
(cid:88)

i=1

aji γ−ji + b2,

γl =

N
(cid:88)

i=1

aji γl−ji .

(21)

(22)

3.1 Computing the theoretical autocovariance function of a restricted

AR model
The problem of computing γAR
for a restricted AR(p, j) model can be readily addressed by applying the
procedure described in Section 2.1 together with eqs. (18) and (19). As an example, Figure 5 shows γAR
for an AR(5, [1, 2, 5]) model given by a1 = 1.2, a2 = −0.5, a5 = 0.1 and b = 0.5, computed for lags up to
l = 20.

l

l

Figure 5: Autocovariance function of a predeﬁned AR(5, j = [1, 2, 5]) model with a1 = 1.2, a2 =
−0.5, a5 = 0.1 and b = 0.5.

A peculiarity of restricted AR models is that speciﬁc model schemes (i.e., speciﬁc values of vector j
components) lead to theoretical autocovariance functions with alternating zero and nonzero values. For
example, let us consider the model AR(3,[3]):

The values of the autocovariance function γAR

l

up to lag l = 3 are provided by the following autoco-

zt = a3 zt−3 + b εt

(23)

variance equations:

10

0246810121416182000.20.40.60.81l = 0 : γ0 = a3 γ−3 + b2,
l = 1 : γ1 = a3 γ−2,
l = 2 : γ2 = a3 γ−1,
l = 3 : γ3 = a3 γ0.

(24)

By applying symmetry to the autocovariance function, the system of equations given in (24) can be

expressed as follows:







1
0
0
−a3

0
1
−a3
0

0
−a3
1
0







−a3
0
0
1







γ0
γ1
γ2
γ3







=







b2
0
0
0







.

(25)

Equation (25) can be divided into two independent subsystems. The ﬁrst one with equations for lags
l = 0 and l = 3, and the second one for lags for which there are no regression terms, l = 1 and l = 2.
Solving the ﬁrst subsystem yields:

b2
1 − a2
3
Concerning the second subsystem, the only possible solution is:

b2
1 − a2
3

, and γT

γT
0 =

3 =

a3.

(26)

1 = γT
γT

2 = 0.

By recursively applying Equation (22), one gets nonzero values for γAR

only at lags l = 3, 6, 9, ....
In a general case, a restricted AR(p) model with a single regression term at lag j = [p] has a theoretical
autocovariance function with non-zero values only at lags l = p and its multiples. This particular structure
of γAR
is also observed for restricted AR models with j vectors such that ji is multiple of j1 for i > 1.
l
For example, the theoretical autocovariance function of an AR(6,[2, 4, 6]) model will show nonzero values
only at even time lags. Such model schemes are likely to represent bad candidates for reproducing real-life
autocovariance functions that usually fade out to zero in a continuous fashion.

l

3.2 Computing the parameters of a restricted AR model from a prede-

ﬁned target autocovariance function

In Section 2.2 it was shown that the p + 1 parameters of an (unrestricted) AR(p) model can be com-
puted to reproduce the ﬁrst p + 1 values of a predeﬁned target autocovariance function through the Y-W
approach. It was also discussed how the exact matching up to lag p could be sacriﬁced in favour of im-
proving the global matching by employing autocovariance equations for lags larger than p. In this section,
restricted AR models are considered as an additional strategy to increase the control on the obtained
model autocovariance function for time lags larger than the number of regression coeﬃcients, N .

Without loss of generality, consider the problem of computing the parameters of a restricted AR model
given by j = [1, 2, 5]. Note that the model parsimony is the same than that of the AR(3) model employed
in Section 2.2, i.e. N = 3, but in this case the model order is p = 5. Let us start by considering the
autocovariance equations of the corresponding unrestricted AR(5) model, for lags l = 0, ..., 5:

l = 0 : γ0 = ϕ1 γ1 + ϕ2 γ2 + ϕ3 γ3 + ϕ4 γ4 + ϕ5 γ5 + σ2,
l = 1 : γ1 = ϕ1 γ0 + ϕ2 γ1 + ϕ3 γ2 + ϕ4 γ3 + ϕ5 γ4,
l = 2 : γ2 = ϕ1 γ1 + ϕ2 γ0 + ϕ3 γ1 + ϕ4 γ2 + ϕ5 γ3,
l = 3 : γ3 = ϕ1 γ2 + ϕ2 γ1 + ϕ3 γ0 + ϕ4 γ1 + ϕ5 γ2,
l = 4 : γ4 = ϕ1 γ3 + ϕ2 γ2 + ϕ3 γ1 + ϕ4 γ0 + ϕ5 γ1,
l = 5 : γ5 = ϕ1 γ4 + ϕ2 γ3 + ϕ3 γ2 + ϕ4 γ1 + ϕ5 γ0.

(27)

The system of equations (27) contains six equations, six model parameters (σ , ϕ1, ..., ϕ5) and six
autocovariance terms (γ0, γ1, ...γ5). Thus, according to the Y-W approach described in Section 2.2, by
introducing the six target autocovariance terms, the system of equations is linear and provides the six

11

parameters of an AR model whose theoretical autocovariance function matches exactly the imposed target
autocovariance values.

Now, consider the restricted model AR(5,[1, 2, 5]), obtained from an AR(5) by imposing ϕ3 = 0 and
ϕ4 = 0. Since these two model parameters are no longer unknowns in (27), two possible strategies can be
followed to obtain the model parameters (ϕ1, ϕ2, ϕ5 and σ) from (27):

1. To select two new unknowns from the set of six autocovariance terms. These two terms will not be
replaced by target autocovariance values. This yields a system of equations with six equations and
six unknowns (four model parameters and the two selected autocovariance terms).

2. To discard two autocovariance equations, in order to have a system of equations with four equations

and four unknowns (the four model parameters).

If the strategy deﬁned in 1 is considered, only four (and not six) values of the target autocovariance
function are required. Let us select, with no loss of generality, γ0, γ1, γ3 and γ5 to be replaced by the
corresponding target autocovariance values, and consider γ2 and γ4 as additional unknowns. In this case,
the system of equations (27) yields the six unknowns (σ, ϕ1, ϕ2, ϕ5, γ2 and γ4). The drawback of this
strategy is that the system of equations becomes non-linear, due to the products between unknowns such
as ϕ2 γ2 and ϕ5 γ4 in the ﬁrst and second equations, respectively. Note that the computational cost may
be dramatically increased, specially for AR models with large order p, as the system of equations to be
solved comprises p+1 equations, regardless the number of AR coeﬃcients assigned to zero. The advantage
of this strategy is that the obtained restricted AR model has a theoretical autocovariance function that
matches exactly the four provided target autocovariance values.

To illustrate this, the restricted AR(5,[1, 2, 5]) model was obtained for the target autocovariance func-

tion described in Appendix B:

zt = 0.649 zt−1 + 0.138 zt−2 + 0.026 zt−5 + 0.634 εt.

(28)

Figure 6 shows the corresponding γAR

and γT
functions, where the four target values employed during
l
the model determination have been highlighted. The ﬁgure shows an exact matching of γT
3 and
γT
5 , as well as an improved global matching of the target autocovariance function with respect to the Y-W
approach for the same model parsimony can be observed, see Figure 3.

0 , γT

1 , γT

l

Figure 6: Autocovariance function of an AR(5,j=[1,2,5]) model and target autocovariance function.
The restricted AR model was obtained with the non-linear approach, see text for details. The
employed values of the target autocovariance function are highlighted.

The second strategy deﬁned in 2 is actually equivalent to just selecting as many autocovariance equa-
tions as AR parameters, by deﬁning a vector l. For the considered example, and without loss of generality,
the autocovariance equations for lags l = 0 and l = [1, 4, 5] are selected. The resulting system of equations,
using the notation for restricted AR models, is:

12

0246810121416182000.20.40.60.81l = 0 : γ0 = a1 γ1 + a2 γ2 + a5 γ5 + b2,
l = 1 : γ1 = a1 γ0 + a2 γ1 + a5 γ4,
l = 4 : γ4 = a1 γ3 + a2 γ2 + a5 γ1,
l = 5 : γ5 = a1 γ4 + a2 γ3 + a5 γ0.

(29)

The system of equations (29) is linear, and comprises four equations, four unknowns (the model param-
eters b2, a1, a2 and a5), and requires six autocovariance terms. By introducing the target autocovariance
terms, the model parameters are given by:









−1 



=













1
0
0
0

b2
a1
a2
a5

γT
1
γT
0
γT
3
γT
4

γT
5
γT
4
γT
1
γT
0

γT
2
γT
1
γT
2
γT
3
This situation is similar to that explained in Section 2.1, in the sense that, since the number of
required target autocovariance terms is higher than the number of equations (i.e., the model parameters),
the autocovariance function of the obtained AR model will not exactly match any of the imposed target
autocovariance values, but it may show a reasonably good matching for a wide range of time lags. To
illustrate this idea, Figure 7 shows γAR
functions of the AR(5,[1,2,5]) model computed with
l = [1, 4, 5] for the target autocovariance function described in Appendix B. The six target values employed
during the model determination have been highlighted. The obtained model is:

γT
0
γT
1
γT
4
γT
5

and γT
l













(30)

.

l

zt = 0.611 zt−1 + 0.198 zt−2 + 0.009 zt−5 + 0.633 εt.

(31)

Figure 7: Autocovariance function of an AR(5,j=[1,2,5]) model and target autocovariance function.
The restricted AR model was obtained by considering the autocovariance equations for lags l = 0
and l = [1, 4, 5]. The employed values of the target autocovariance function are highlighted.

Although the obtained restricted AR model given in (31) performs slightly worst than the model given
in (16), it still improves the model obtained with the Y-W approach, Equation (13). The key idea of
the presented analysis is that selecting appropriate AR model schemes through vector j has the potential
to improve the global matching between γAR
l . To the authors knowledge, this strategy has been
considered only to a limited extent in a previous work [Krenk and Møller, 2019], since in that work an
exponential model scheme j = [1, 2, ..., 2N −1] was assumed for the presented simulations, which leaves the
possibility of optimising the AR scheme unexplored.

and γT

l

l − γAR

Finally, to gather the results obtained with the examples described in Sections 2.2 and 3.2, Figure 8
shows |el| = |γT
|, computed for the AR model obtained with the Y-W approach, Equation (13),
the AR model obtained by selecting autocovariance equations l = [1, 2, 5], see Equation (16), and the
restricted models AR(5,[1,2,5]) obtained with the linear formulation, Equation (31), and the non-linear
formulation, Equation (28).

l

13

0246810121416182000.20.40.60.81Figure 8: Absolute error between target and theoretical autocovariance function, |el|, for the
AR model obtained with the Y-W approach, Equation (13), the AR model obtained by selecting
autocovariance equations l = [1, 2, 5], see Equation (16), and the restricted models AR(5,j=[1,2,5])
obtained with the linear formulation, Equation (31), and the non-linear formulation, Equation
(28).

4 General formulation for the determination of a restricted
AR model from a predeﬁned target autocovariance func-
tion

This section provides a general formulation for the linear problems addressed in sections 2.2 and 3.2. The
non-linear problem described in Section 3.2 is not covered by the following formulation and will be further
analysed in future research. The objective here is to obtain the parameters of a restricted AR(p, j) model
given a target autocovariance function, from generic input vectors j = [j1, j2, ..., jN ] (regression coeﬃcients
considered in the model) and l = [l1, l2, ..., lN ] (autocovariance equations considered to obtain the model
parameters), by means of a linear system that can be computed with low computational resources.

The problem is divided into two steps:

(i) Determination of the regression coeﬃcients aji , for i = 1, ..., N , by means of the set of N autocovari-
ance equations for lags gathered in l. For convenience, the regression coeﬃcients are encapsulated
into a row vector a = [aj1 , aj2 , ..., ajN ].

(ii) Determination of the noise coeﬃcient, σ, by means of the autocovariance equation for lag l = 0.

Note that, while in sections 2.2 and 3.2 a single system of equations including (i) and (ii) was considered,
the division of the problem proposed in this section is always possible since the noise coeﬃcient appears
only in the autocovariance equation for lag l = 0. By doing so, a more handy general formulation is
obtained. Note also that the formulation is presented for the case of a restricted AR model, but the case
of an unrestricted AR model is included by simply considering Equation (20).

4.1 Determination of the model coeﬃcients: a

The autocovariance equations considered for the lags gathered in l and for the case of a restricted AR(p, j)
model can be written in matrix form as follows:

[ γl1 γl2 . . . γlN ] = [ aj1 aj2 . . . ajN ]

or, in a more compact way:

14








γl1−j1
γl1−j2
...
γl1−jN

γl2−j1
γl2−j2
...
γl2−jN

. . .
. . .
. . .
. . .








,

γlN −j1
γlN −j2
...
γlN −jN

(32)

0246810121416182000.010.020.030.040.050.06γl = a · γj,l,
where γl is a row vector with dimension N , and γj,l is a matrix N × N , both containing values of the
autocovariance function at speciﬁc time lags, according to vectors j and l. It is worth noting that, for
the particular case j = l = [1, 2, 3, ..., p], the system of equations given in (32) becomes the Yule-Walker
equations. If the autocovariance terms in (33) are replaced by the target values, the model coeﬃcients are
given by:

(33)

a = γl · γ−1
j,l .

(34)

4.2 Determination of the noise coeﬃcient: b

The autocovariance equation for time lag l = 0 for a restricted AR(p, j) model is as follows:

γ0 = [ aj1 aj2 . . . ajN ][γ−j1 γ−j2 . . . γ−jN ](cid:48) + b2,
where tilde means transposed. Operating and applying symmetry in the autocovariance function, the
noise coeﬃcient is given by:

(35)

or, in a more compact way:

b2 = γ0 − [ aj1 aj2 . . . ajN ] [γj1 γj2 . . . γjN ](cid:48),

(36)

b2 = γ0 − a · γ(cid:48)
j,
where γj is a row vector containing N values of the target autocovariance function at time lags gathered
in j.

(37)

Finally, note that equations (33) and (37) particularised for j = l = [1, 2, ..., 2N −1] represent the
univariate case of the formulation introduced in [Krenk and Møller, 2019]. The formulation here presented
is more ﬂexible, as it allows searching for optimal j and l vectors independently, that is, without assuming
the constraint j = l.

5 Optimal AR models for synthetic isotropic turbulence

generation

In this section, a methodological proposal is presented to obtain optimal AR models from a predeﬁned
target autocovariance function. The particular case of homogeneous stationary isotropic (SHI) turbulence
is considered. The methodology combines the general formulation described in Section 4 with the use of
genetic algorithms. The aim is to ﬁnd, for a given number of regression coeﬃcients N , optimal vectors j =
[ j1, j2, ..., jN ] and l = [ l1, l2, ..., lN ]. In this context, optimal means that the obtained AR model resulting
from expressions (34) and (37) provides minimum mean squared error, M SE, between its theoretical
, and the target autocovariance function, γT
autocovariance function, γAR

l . M SE is given by:

l

with

M SE =

1
M

M
(cid:88)

l=0

e2
l ,

(38)

γT
l

l − γAR
(39)
l
is deﬁned as the non-dimensional longitudinal autocovariance function of SHI turbulence, ˚Ru(˚r),
as described in Appendix B; ˚r = r/L is a non-dimensional spatial coordinate, and L is the length scale
parameter of the three-dimensional energy spectrum. In (38), M represents the maximum lag considered
in the computation of the M SE. This parameter has been ﬁxed to M = 41, which corresponds to a
maximum non-dimensional spatial distance of ˚rmax = 40 · ∆˚r = 4.98. It holds that:

el = γT

.

meaning that the selected M value accounts for the 99.5% of the integral length scale, Lx
u.

(cid:90) ˚rmax

0

˚Ru(˚r) d˚r ≈ 0.995Lx
u,

15

The genetic algorithm is designed to minimise the criterion given in (38). The following inequality

constraints were included in the algorithm:

1. 0 < j1 < j2 < ... < jN , with ji ∈ N, for i = 1, ..., N.
2. 0 < l1 < l2 < ... < lN , with li ∈ N, for i = 1, ..., N.
3. ji − ∆ ≤ li ≤ ji + ∆, with ∆ ∈ N , for i = 1, ..., N.

The ﬁrst and second constraints derive from the deﬁnition of vectors j and l. The third constraint
introduces the parameter ∆, that regulates the maximum diﬀerence between the regression lags included
in the AR model and the corresponding autocovariance equations included in the equation system that
provides the model parameters. It has been observed that large diﬀerences between elements ji and li may
derive into numerical instabilities during the optimisation process. Note that ∆ = 0 means that l = j.

The analysis includes a benchmark exercise for a number of models, which are compared for the same
model parsimony. The range N = 1, ..., 10 is considered in what follows. The benchmark comparison
includes the following models (italic letters are employed to denote the models):

• Y-W, an unrestricted AR(p) model obtained through the Yule-Walker approach, that is, p = N and

j = l = [ 1, 2, ..., p ].

• K-M, a restricted AR(p, j) with an exponential model scheme, j = [ 1, 2, ..., 2N −1 ]), and j = l, as

proposed in [Krenk and Møller, 2019].

• GA-0: obtained with a genetic algorithm with ∆ = 0. This model allows exploring the potential
improvement due to relaxing the exponential model scheme employed in K-M, while keeping the
constraint j = l.

• GA-10: obtained with a genetic algorithm with ∆ = 10. This model allows assessing the additional

improvement with respect to GA-0 due to relaxing the constraint j = l.

Figure 9 shows the components of vector j obtained for model GA-0, for diﬀerent N values. Figure 10
shows the components of vectors j (left) and l (right) obtained for model GA-10, for diﬀerent N values.
Y-W and K-M models are included in both ﬁgures for comparison.

Figure 9: Components ji of vector j obtained with GA-0 model. Y-W and K-M models are
included for comparison.

It is noted that, for the GA-10 model, the maximum diﬀerence found between i-th elements ji and li
was 5, meaning that the choice ∆ = 10 was ﬂexible enough to allow the search for optimal AR models
witout actually constraining the diﬀerence between corresponding elements in j and l vectors.

Results show that, for the most parsimonious models, N = 1, the single regression term considered in
the four models is the previous lag, j = [1]. However, for GA-10 the employed autocovariance equation
is for lag l = [2]. For the case N = 2, both GA-0 and GA-10 models provided optimal regression lags

16

123456789100246810121416182022Figure 10: Components ji of vector j (left) and li of vector l (right) obtained with GA-10 model.
Y-W and K-M models are included for comparison.

j = [1, 3], diﬀerent from Y-W and K-M models, for which j = [1, 2]. Concerning the model order p,
given by the last term of j, jN , results show that, while for models Y-W and K-M p is determined by N
(linearly and exponentially, respectively), the proposed methodology has the ability to reveal an optimal
model order for a given model parsimony. It is noted that the obtained optimal order models depend
on the speciﬁc target autocovariance function considered. Indeed, for increasing N values, the obtained
model order for the most ﬂexible model, GA-10, stagnates around p = 23. This value is coherent with
the fact that the target autocovariance function is already very close to zero for this lag, see Figure 18.
Thus, including additional regression coeﬃcients by reducing the model parsimony is better exploited by
the model by rearranging terms in vector j rather than increasing the model order beyond this value, as
it is the case for models Y-W and K-M.

Figure 11 shows the M SE obtained with all the models considered in the analysis, as a function of N .

Figure 11: M SE obtained with Y-W, K-M, GA-0 and GA-10 models, as a function of the number
of regression terms considered in the model, N . (Logarithmic scale).

As expected, the MSE obtained with the Y-W approach decreases with N , because the obtained models
match exactly the target autocovariance function for lags up to the model order. For the K-M approach,
the error decreases rapidly for low N values, reﬂecting the advantages of restricted AR models as compared
with the Y-W approach. However, the obtained error stagnates from N = 5 on. This can be explained
by the fact that including a regression term for lag 26−1 = 32 (i.e. N = 6) improves little the ﬁtting of a

17

12345678910051015202530123456789100510152025301234567891010-810-710-610-510-410-310-2target autocovariance function that becomes very close to zero for such lag values. This result clearly shows
that an exponential model scheme, as that of model K-M, has an intrinsic upper limit on the number of
regression terms that is worthy to consider, this limit being related to the range of lags for which γT
l takes
non-negligeable values. This range depends on the selected ∆˚r employed to obtain the discrete target
values from the continuous non-dimensional target autocovariance function of the underlying random
process, see Appendix B. The improvements provided by GA-0 with respect to K-M model show the
importance of searching for optimal model schemes rather than assuming a predeﬁned model structure.
The improvement becomes noticeable for N values larger than the aforementioned upper limit N =
5. Finally, the gap between the results for GA-0 and GA-10 clearly shows the additional improvement
associated with an optimal selection of the autocovariance equations employed in the determination of the
model parameters. Thus, relaxing the hypothesis l = j, that usually underlies in the literature, is actually
a clear path for improvement.

To delve more into this analysis, the distribution of M SE with the lag is analysed. Figure 12 shows

e2
l , see Equation (39), for case N = 3.

Figure 12: Squared ﬁtting errors e2(l) for the diﬀerent models. Case N = 3.

It can be seen that improving the global ﬁtting of the target autocovariance function by means of
restricted AR models may come at the expense of deteriorating the local ﬁtting in some of the ﬁrst lags,
as compared with the results of the unrestricted models employed under the Y-W approach. For example,
model GA-10 improves greatly the other models in terms of the global ﬁtting, see Figure 11, but at the
same time GA-10 shows the highest error for lag l = 2. This trade-oﬀ between global ﬁtting and local
ﬁtting of the target autocovariance function is relevant when using restricted AR models, and it probably
needs to be addressed with a broader concept of optimal model, which may vary from one problem to
another. In this regard, additional information from the frequency domain could be included to deﬁne
the notion of optimal model. This is particularly relevant for some engineering problems, for which some
constraints could be deﬁned in terms of frequency ranges, like those involving wind loads [Li and Kareem,
1990, Kareem, 2008]. For illustrative purposes, Table 1 shows the model parameters obtained for N = 3,
and Figure 13 gathers, on top, the non-dimensional one-sided target spectrum, ˚ST (˚k), together with
the corresponding theoretical spectra of the AR models obtained with the four approaches, and at the
bottom, the diﬀerences between the target spectrum and the AR model spectra. ˚k is the non-dimensional
wave number, ˚k = k L. Note that ˚ST (˚k) is aﬀected by aliasing due to the discretisation of the target
autocovariance function, see details in Appendix B. The non-dimensional one-sided spectrum of an AR
model can be obtained either from the model coeﬃcients,

˚SAR(˚k) =

1
˚kmax

b2/σ2
0
(cid:16)

(cid:12)
(cid:12)1 − (cid:80)N
(cid:12)

n=1 ajn exp

−i n π ˚k

˚kmax

2 ,

(cid:17)(cid:12)
(cid:12)
(cid:12)

(40)

or from its theoretical autocovariance function [Box et al., 2016]:

18

051015202530354000.20.40.60.811.21.410-3˚SAR(˚k) =

1
˚kmax

(cid:34)
γAR
0 + 2

∞
(cid:88)

l=1

γAR
l

cos

(cid:32)

πl˚k
˚kmax

(cid:33)(cid:35)

,

(41)

where ˚kmax = 1
employed to obtain the discrete version of the target autocovariance function, see Appendix B.

2∆˚r is the maximum non-dimensional wave number, and ∆˚r is the non-dimensional length

Table 1: AR models obtained for the diﬀerent approaches for N = 3.

Approach
Y-W
K-M
GA-0
GA-10

j
[1,2,3]
[1,2,4]
[1,2,5]
[1,2,7]

l
[1,2,3]
[1,2,4]
[1,2,5]
[1,6,12]

Model
zt = 0.663 zt−1 + 0.099zt−2 + 0.044 zt−3 + 0.636 εt
zt = 0.664 zt−1 + 0.109zt−2 + 0.035 zt−4 + 0.636 εt
zt = 0.665 zt−1 + 0.115zt−2 + 0.029 zt−5 + 0.636 εt
zt = 0.646 zt−1 + 0.147zt−2 + 0.025 zt−7 + 0.635 εt

Figure 13: Top: non-dimensional one-sided target spectrum, ˚ST (˚k), together with the correspond-
ing theoretical spectra of the AR models obtained with the four approaches; bottom: diﬀerences
between the target spectrum and the AR model spectra. Case N = 3.

Figure 13 illustrates that, in terms of ﬁtting the target spectrum, model GA-10 outperforms the rest
of the models for low frequencies (up to ˚k = 4), including the frequency at which the maximum of the
target spectrum is attained (˚k ≈ 1.245). For higher frequencies, all model spectra show similar oscillations
around the target spectrum. This result suggests that, a priory, reducing the local ﬁtting of the target
autocovariance function in the ﬁrst lags to improve the global ﬁtting does not have a clear negative impact
on the spectrum ﬁtting.

19

10-210-110010100.050.10.150.20.250.310-210-1100101-0.03-0.02-0.0100.010.020.036 Generalisation to multivariate processes

In this section, the analyses described in sections 2, 3 and 4 are extended for a one-dimensional multivariate
random process, {zt(α)}, with the column vector zt = [z1, z2, ..., zk](cid:48)
t being a k-variate random variable.
In particular, we describe the reasons why some bottlenecks in terms of computational cost may arise as
a consequence of the multivariate character of the formulation. Note that multi-dimensional multivariate
problems in some cases admit a one-dimensional multivariate formulation, as ﬁrstly described in [Mignolet
and Spanos, 1991].
In [Krenk and Møller, 2019], a three-dimensional three-variate description of SHI
turbulent wind ﬁeld was expressed in the form of a one-dimensional 3P -variate random process by stacking
the three velocity components at P points of the plane perpendicular to the mean wind into a random
variable.

The general formulation of a zero-mean k-variate VAR(p) model is:

zt =

p
(cid:88)

i=1

Φi zt−i + Σ εt.

(42)

where z = [z1, z2, ..., zk](cid:48) is a k-variate random variable, and εt is a sequence of independent and identically
distributed (iid) random vectors with zero mean, E[εt] = 0k×1, and unity covariance matrix, Var[εt] = Ik.
The p + 1 model matrix parameters are given by the regression matrices, Φi with i = 1, ..., p, and the
noise matrix, Σ; the dimension of all the matrix parameters is k × k. The covariance of the random term
is given by Var[Σ εt] = E[(Σ εt) (Σ εt)(cid:48)] = ΣΣ(cid:48).

Γt1,t2 is the covariance between random vectors zt1 and zt2 . As in the univariate case, the assumption
of stationary process implies that Γt1,t2 depends only on the time lag, l = t1 − t2, which is referred to as
the covariance matrix function:

Γl = Cov[ zt, zt−l ] = E[ zt z(cid:48)

t−l ] =






γz1,z1,l
...
γzk,z1,l

γz1,z2,l
...
γzk,z2,l

. . .
. . .
. . .




 .

γz1,zk,l
...
γzk,zk,l

The covariance equations that generalise equations (4) and (5) for the multivariate case are:

Γ0 =

p
(cid:88)

i=1

Φi Γ−i + ΣΣ(cid:48),

Γl =

p
(cid:88)

i=1

Φi Γl−i.

(43)

(44)

(45)

6.1 Computing the theoretical covariance matrix function of a VAR

model

l

As in the univariate case, equations (44) and (45) can be employed to compute the theoretical covariance
matrix function, ΓV AR
, from the model parameters, Φi, i = 1, ..., p and Σ. However, note that in the
multivariate case, Γ−l = Γ(cid:48)
l. This fact makes the linear formulation described in Section 2.1 not applicable
for the multivariate case, as it is not possible to replicate the step given between equations (6) and (7).
There are two approaches for obtaining the theoretical covariance matrix function of a VAR(p) model,
both involving an increasing computational cost, as compared with the univariate case:

1. To obtain ΓV AR

from the covariance matrix of the VAR(1) representation of the VAR(p) model.
This strategy provides the exact covariance matrix function of the VAR(p) model, but it is compu-
tationally very expensive, as the size of the involved matrices scales up to (pk)2 × (pk)2.

l

2. To obtain ΓV AR

l

from the covariance matrix function of the equivalent multivariate Vector Moving
Average (VMA) model. Since this equivalent VMA model contains inﬁnite terms, truncation to a
VMA(q) model is required, meaning that the obtained covariance matrix function is an approxima-
tion of the exact VAR covariance matrix function. In this case, the increase in the computational
cost comes from the fact that appropriate q values are related to the integral length scale of the
process, which typically leads to q (cid:29) p.

In what follows, both methodologies are brieﬂy described.

20

6.1.1 Covariance matrix function of a VAR(p) model through the VAR(1) represen-

tation

A k-variate VAR(p) model can be expressed in the form of an extended pk-dimensional VAR(1) model
by using an expanded model representation [Tsay, 2013]. For illustrative purposes, the case of a VAR(3)
model is analysed:

Let us deﬁne the expanded 3k-variate random variable as:

zt = Φ1 zt−1 + Φ2 zt−2 + Φ3 zt−3 + Σ εt.

The VAR(1) representation of the original VAR(3) model is given by:

z∗
t =



 .





zt
zt−1
zt−2

1 z∗
1 is called the companion matrix, deﬁned as:

t = Φ∗
z∗

t−1 + Σ∗ ε∗
t ,

where Φ∗

Φ∗

1 =





Φ1 Φ2 Φ3
0k
0k
Ik
0k
Ik
0k



 ,

and the random matrix is given by:

Σ∗ =





Σ 0k 0k
0k 0k 0k
0k 0k 0k


 , ε∗

t =







 .

εt
0k×1
0k×1

The covariance matrix function of the extended VAR(1) model of equation (48) is given by:

Γ∗

0 = Var[ z∗
= E[ (Φ∗
1 Γ∗
= Φ∗

t , z∗
1 z∗
0 Φ∗
1

t ] = E[ z∗
t−1 + Σ∗ ε∗
(cid:48) + Σ∗Σ∗(cid:48).

(cid:48) ]
t z∗
t
t ) (Φ∗

1 z∗

t−1 + Σ∗ ε∗

t )(cid:48) ]

The solution of Equation (51) is given by [Tsay, 2013]:

vec(Γ∗

0) = vec(Σ∗Σ∗(cid:48)) (I(pk)2 − Φ∗

1 ⊗ Φ∗

1)−1,

(46)

(47)

(48)

(49)

(50)

(51)

(52)

where vec(·) denotes the column-stacking vector of a matrix and ⊗ is the Kronecker product of two
matrices.

The covariance matrix function of the original VAR(3) model can be readily obtained from Γ∗

0, noting

that ΓV AR

0

, ΓV AR
1

and ΓV AR

2

are given by:

ΓV AR
0
ΓV AR
−1
ΓV AR
−2
and using recursively Equation (45) to obtain ΓV AR

0 = E[ z∗

(cid:48) ] =

t z∗
t

Γ∗





l

for lags l ≥ 3.

ΓV AR
1
ΓV AR
0
ΓV AR
−1



 ,

ΓV AR
2
ΓV AR
1
ΓV AR
0

(53)

6.1.2 Covariance matrix function of a VAR(p) model through VMA(q) representa-

tion

It can be demonstrated that any VAR(p) model admits a Vector Moving-Average (VMA) representation
with inﬁnite terms:

where matrices Ψi are known functions of the VAR regression matrices, Φi [Tsay, 2013]:

zt = Σ εt +

∞
(cid:88)

i=1

Ψi (Σ εt−i),

(54)

21

Ψi =

min(i,p)
(cid:88)

j=1

Φj Ψi−j , for i = 1, 2, ...

(55)

and Ψ0 = Ik.

The theoretical covariance matrix function of a VMA process is given by:

ΓV M A(∞)

l

= Cov[ zt, zt−l ] = E[ zt z(cid:48)

t−l ]

= E[ (Σ εt +

∞
(cid:88)

i=1

Ψi (Σ εt−i)) (Σ εt−l +

∞
(cid:88)

i=1

Ψi (Σ εt−l−i))(cid:48)]

=

∞
(cid:88)

i=l

Ψi ΣΣ(cid:48) Ψ(cid:48)

i−l.

(56)

ΓV M A(∞)

l

matches exactly the covariance matrix function of the related VAR(p) model. However,
for practical reasons, this approach requires truncation by considering only the ﬁrst q terms of the VMA
representation, that is, a VMA(q) model:

The covariance matrix function of a VMA(q) model is given by:

zt = Σ εt +

q
(cid:88)

i=1

Ψi (Σ εt−i).

ΓV M A(q)

l

= Cov[ zt, zt−l] =






q
(cid:80)
i=l
0k

ΨiΣΣ(cid:48)Ψ(cid:48)

i−l

, for 0 ≤ l ≤ q

, for l > q.

(57)

(58)

l

In summary, ΓV AR

can be approximated in a two step procedure: (i) computing a truncated VMA(q)
representation of the VAR(p) model, see Equation (57); the q coeﬃcient matrices of the VMA repre-
sentation can be computed through Equation (55); (ii) computing the covariance matrix function of the
VMA(q) model given by Equation (58). Concerning the choice for parameter q, note that ΓV M A(q)
= 0k
for l > q, thus q should be such that ΓV AR

l

≈ 0k.

q

To illustrate the two presented methodologies, ﬁgures 14 and 15 show the theoretical covariance matrix

function, ΓV AR

l

of the following 2-variate VAR(2) model:

zt =

(cid:19)

(cid:18)z1,t
z2,t

(cid:18) 1.1 −0.1
0.7

−0.2

(cid:19)

=

zt−1 +

(cid:18)−0.3
−0.1

(cid:19)

0.2
0.1

zt−2 +

(cid:18)0.3
0.1

(cid:19)

0.0
0.2

εt.

In particular, Figure 14 shows the exact covariance matrix function computed through the VAR(1) repre-
l can be appraised from γV AR
sentation, where the property Γ−l = Γ(cid:48)

z1,z2,l and γV AR

z2,z1,l.

Figure 15 illustrates γV AR

z1,z1,l together with three approximations obtained through the VMA(q) repre-

sentation. It can be noted that a reasonable approximation requires q (cid:29) p.

6.2 General formulation for the determination of a restricted VAR

model from a predeﬁned target covariance matrix function

A restricted VAR(p, j) model, with j = [j1, j2, ..., jN ] is given by:

zt =

N
(cid:88)

i=1

Aji zt−ji + B εt.

(59)

As in the univariate case, a restricted VAR(p) model deﬁned by vector j can be seen as a particular

case of a VAR(p) model for which:

(cid:40)

Φh =

Ah , for h ∈ j
0k , for h /∈ j

, h = 1, ..., p.

and

Σ = B.

22

Figure 14: Exact covariance matrix function of a VAR(2) process, obtained through VAR(1)
representation.

Figure 15: γV AR
VAR(1) representation, and three estimations obtained through the VMA(q) representation.

z1,z1,l from the covariance matrix function of a VAR(2) process obtained through

Note also that a VAR(p) model can be seen as a particular case of a restricted VAR(p) model for

which:

j = [ 1, 2, 3, ..., p ].

In the following, the formulation for obtaining the matrix parameters of a restricted VAR(p) model from
a target covariance matrix function is introduced for generic vectors j = [j1, j2, ..., jN ] and l = [l1, l2, ..., lN ].
As in the univariate case, the problem is divided into two steps, and the regression matrix coeﬃcients are
encapsulated into matrix A = [Aj1 Aj2 ... AjN ], which has dimension k × kN .

6.2.1 Determination of the model matrix coeﬃcients: A

The multivariate form of Equation (32) is given by:

23

-20-1001020-0.200.20.4-20-1001020-0.200.20.4-20-1001020-0.200.20.4-20-1001020-0.200.20.4-30-20-100102030-0.2-0.100.10.20.30.40.5[ Γl1 Γl2 . . . ΓlN ] = [ Aj1 Aj2 . . . AjN ]








Γl1−j1
Γl1−j2
...

Γl2−j1
Γl2−j2
...

Γl1−jN Γl2−jN

ΓlN −j1
ΓlN −j2
...

. . .
. . .
. . .
. . . ΓlN −jN








,

or in a more compact way:

Γl = A · Γj,l,

(60)

(61)

where Γl is a matrix k × kN and Γj,l a matrix kN × kN . By replacing the covariance terms by the target
values, the model matrix coeﬃcients are given by:

A = Γl · Γ−1
j,l .

6.2.2 Determination of the noise matrix: B

The multivariate version of Equation (35) is:

Γ0 = [ Aj1 Aj2 . . . AjN ][Γ−j1 Γ−j2 . . . Γ−jN ](cid:48) + BB(cid:48).

Operating and applying Γ−l = Γ(cid:48)

l, the covariance matrix of the random term, BB(cid:48), is given by:

or in a more compact way:

BB(cid:48) = Γ0 − [ Aj1 Aj2 . . . AjN ] [Γj1 Γj2 . . . ΓjN ](cid:48),

(62)

(63)

(64)

BB(cid:48) = Γ0 − A · Γ(cid:48)
j,
where Γj is a matrix k × kN . From (65), the noise matrix B can be obtained in several ways, for example,
through Cholesky decomposition. Finally, it is remarked that equations (61) and (65) particularised for j=l
represent the formulation introduced in [Krenk and Møller, 2019] (in particular Γ0 = Cuu, Γj = Γl = Cuw
and Γj,l = Cww).

(65)

As an example, two VAR models with three regression matrices have been computed to reproduce
the target covariance matrix function described in Appendix B, for the case of the longitudinal wind
component at two spatial locations with a non-dimensional lateral separation of ∆˚y = ∆y
L = λ = 0.747.
The ﬁrst model is a VAR(3) model, and it was computed with the Y-W approach, i.e. j = l = [1, 2, 3]:

zt =

(cid:18)0.659
0.022

(cid:19)

0.022
0.659

zt−1 +

(cid:18)0.096
0.011

(cid:19)

0.011
0.096

zt−2

(cid:18)0.039
0.015

(cid:19)

0.015
0.039

zt−3 +

(cid:18)0.634
0.013

(cid:19)

0.000
0.634

εt.

(66)

The second model is a restricted VAR(5,[1,2,5]) computed for l=[1, 2, 6]:

zt =

(cid:18)0.660
0.023

(cid:19)

0.023
0.660

zt−1 +

(cid:18)0.109
0.015

(cid:19)

0.015
0.109

zt−2

(cid:18)0.028
0.013

(cid:19)

0.013
0.028

zt−5 +

(cid:18)0.634
0.013

(cid:19)

0.000
0.634

εt.

(67)

Figures 16 and 17 show the resulting covariance matrix functions, computed through VAR(1) repre-
sentation (i.e. exact values). As expected, the model obtained with the Y-W approach provides exact
matching for lags from l = −3 to l = 3. However, an improved global ﬁtting is obtained with the restricted
VAR(5,[1,2,5]) model.

7 Conclusions

Sequential methods for synthetic realisation of random processes have a number of advantages compared
to spectral methods. For instance, they are characterised by a more handy synthesis process, as it can be
stopped and restarted at any time. In addition, the models obtained through the sequential approach (e.g.,
autoregressive models) have theoretical expressions of their autocovariance function and power spectrum
density (PSD) function, which allows an improved assessment of the accuracy of the models in reproducing
the predeﬁned statistical information.

In this article, a methodological proposal for the determination of optimal autoregressive (AR) models
from a predeﬁned target autocovariance function was introduced. To this end, a general formulation of

24

Figure 16: Target and VAR covariance matrix function of a VAR(3) model obtained with the Y-W
approach.

Figure 17: Target and VAR covariance matrix function of a VAR(5,[1,2,5]) model computed for
l=[1, 2, 6].

the problem was developed. Two main features characterise the introduced formulation: (i) ﬂexibility in
the choice for the autocovariance equations employed in the model determination, through the deﬁnition
of a lag vector l; and (ii) ﬂexibility in the deﬁnition of the AR model scheme through vector j, that deﬁnes
the regression terms considered in the model. The AR parameters are directly obtained as a solution
of a linear system that depends on j and l. The well-known Yule-Walker (Y-W) and a recent approach
based on restricted AR models (K-M) can be seen as particular cases of the introduced formulation, since
j = l = 1, ..., p for Y-W and j = l for K-M.

The introduced formulation was exploited by a genetic algorithm to obtain optimal AR models for

25

-20-100102000.20.40.60.81-20-100102000.050.10.150.20.25-20-100102000.050.10.150.20.25-20-100102000.20.40.60.81-20-100102000.20.40.60.81-20-100102000.050.10.150.20.25-20-100102000.050.10.150.20.25-20-100102000.20.40.60.81the synthetic generation of stationary homogeneous isotropic (SHI) turbulence time series. The resulting
models improved Y-W and K-M based models for the same model parsimony in terms of the global ﬁtting
of the target autocovariance function. This achievement was obtained at the expense of reducing the local
ﬁtting for some lags. The impact of this trade-oﬀ on the frequency domain was presented as a path for
extending the notion of optimal model to speciﬁc problems in which constraints in the frequency domain
may exist, as it is the case in some engineering problems.

The formulation for the one-dimensional multivariate case was also presented. The reasons behind

some computational bottlenecks associated with the multivariate formulation were highlighted.

Finally, a non-linear approach for the univariate case was described, for which preliminary results

suggest an improved ﬁtting of the target autocovariance function.

A Isotropic turbulence

The covariance tensor of a three-dimensional statistically stationary, homogeneous and isotropic velocity
ﬁeld between two spatial points separated by vector r is given by:

R(r) = Cov[ u(x + r) , u(x) ] = E[ u(x + r) u(x)T ] = σ2
0

(cid:18)

[f (r) − g(r)]

r rT
rT r

(cid:19)

+ g(r) I

,

(A.1)

where σ2
0 is the isotropic variance parameter, r = |r|, f (r) is the longitudinal correlation function and
g(r) is the transverse correlation function [de K´arm´an and Howarth, 1938]. Assuming incompressibility
provides the following relationship between g(r) and f (r):

For the particular case of a line along the wind direction, r = [r, 0, 0]T , the covariance tensor is given

g(r) = f (r) +

r
2

d
dr

f (r)

(A.2)

by:

R(r) =





Ru(r)
0
0

0
Rv(r)
0

0
0
Rw(0)


 = σ2

0





f (r)
0
0

0
g(r)
0



 .

0
0
g(r)

(A.3)

From the generalized von Karman spectral density functions, explicit expressions for f (r) and g(r)

can be obtained [Krenk and Møller, 2019]:

f (r) =

2
Γ(γ − 1/2)

(cid:16) r
2L

(cid:17)γ−1/2

Kγ−1/2

(cid:17)

,

(cid:16) r
L

g(r) = f (r) −

2
Γ(γ − 1/2)

(cid:16) r
2L

(cid:17)γ+1/2

Kγ−3/2

(cid:17)

,

(cid:16) r
L

(A.4)

(A.5)

where L is the length scale parameter of the three-dimensional von Karman energy spectrum, Γ(·) is the
gamma function (not to be confused with the covariance function), and Kn is the Bessel function of second
kind of order n. The integral length scale in the longitudinal direction, Lx

u, is deﬁned as:

and it holds that:

Lx

u =

(cid:90) ∞

0

f (r)dr,

Lx

u = λ · L,

(A.6)

(A.7)

with λ = Γ(1/2) Γ(γ)
confused with the autocovariance function).

Γ(γ−1/2) , where γ is a parameter with the value proposed by von Karman, γ = 5/6 (not to be

Note that Equation (A.6) implies that the integral length scale exists and is Lx

u < ∞. However,
there is evidence on the fact that geophysical time series, including turbulent velocity ﬂuctuations, may
show long-term persistence (Hurst phenomenon), meaning that the integral length scale does not exist
[Nordin et al., 1972, Helland and Atta, 1978]. According to [Dias et al., 2018], the assumption of a ﬁnite
integral scale in atmospheric turbulence is not well based on experimental evidence. There are at least
two reasons for that: on the one hand, the measurement periods are in the range of minutes-hours due to
the inherent non-stationarity of the atmospheric boundary layer. On the other hand, tools for spectrum

26

estimation from records usually include smoothing and low frequency loss of information, which leads
to a bad representation of the PSD for very low frequencies (the length scale is the value of the PSD at
frequency zero) [Dimitriadis et al., 2016]. In [Dimitriadis et al., 2016], a review on the most common three-
dimensional power-spectrum-based models of stationary and isotropic turbulence is performed, concluding
that Hurst-Kolmogorov (HK) behaviour is systematically excluded. To the author’s knowledge, the impact
of this fact is still unclear, and it probably depends on the application. For the case of synthetic generation
of turbulent wind velocity ﬁelds oriented to aeroelastic wind turbine simulation, the impact could be small
because the duration of the required simulations is in the order of minutes, and the focus is placed on
microturbulence, rather than low frequency ﬂuctuations.

We deﬁne the non-dimensional autocovariance function for the longitudinal wind velocity component

along the wind direction as a function of the non-dimensional separation ˚r = r/L as follows:

˚Ru(˚r) = σ−2

0 Ru(r = ˚rL) =

2
Γ(γ − 1/2)

(cid:19)γ−1/2

(cid:18)˚r
2

Kγ−1/2 (˚r) .

(A.8)

It holds that:

The target autocovariance function γT

l results from taking discrete values from ˚Ru(˚r) with a given ∆˚r:

(cid:90) ∞

0

˚Ru(˚r) d˚r =

Lx
u
L

= λ.

(A.9)

The numerical application considered in this work was obtained for:

l = ˚Ru(l · ∆˚r) ,
γT

l = 0, 1, ...

Figure 18 shows ˚Ru(˚r) (continuous line) together with γT

l (discrete values).

∆˚r =

λ
6

= 0.1245.

(A.10)

(A.11)

Figure 18: Non-dimensional autocovariance function for longitudinal wind component along wind
direction in isotropic turbulence (continuous line), and target autocovariance function for ∆˚r =
0.1245 (discrete values).

It is worth mentioning that the choice for parameter ∆˚r usually represents a trade-oﬀ between diﬀerent
criteria [Spanos, 1983]. On the one hand, small ∆˚r values involve target autocovariance functions with
non negligible values for a large number of lags, increasing the number of required AR model coeﬃcients to
reproduce γT
l reasonably well. On the other hand, large ∆˚r values may derive into problems in reproducing
the spectrum for high frequencies (aliasing), which is problematic for some engineering problems [Li and
Kareem, 1990]. Indeed, by discretising the continuous autocovariance function of a physical process to
build γT
l ) diﬀers from
l
the spectrum of the original continuous process in the high frequency range. To illustrate this, Figure 19
shows the non-dimensional one-sided spectrum of the employed von Karman turbulence model, ˚SV K (˚k),

(see above), the corresponding target spectrum (i.e., the Fourier Transform of γT

27

00.511.522.533.544.5500.20.40.60.810510152025303540together with the target spectrum resulting from three diﬀerent values of ∆˚r, referred to as ˚ST for a
speciﬁc ∆˚r. ˚SV K (,˚k) is given by:

˚SV K (˚k) =

SV K (k = ˚k/L)
σ2
0 L

=

√

Γ(γ)
πΓ(γ − 0.5)

2˚k
[1 + ˚k2]γ

, ˚k ∈ [0, ∞],

(A.12)

where ˚k is the non-dimensional wave number, ˚k = k L, and γ = 5/6, as stated above. Note that ˚ST are
obtained only up to a maximum value of ˚k derived from the Nyquist theorem, ˚kmax = 2π

2∆˚r .

Figure 19: Non-dimensional one-sided von Karman spectrum together with the non-dimensional
one-sided target spectrum obtained for diﬀerent ∆˚r values.

B Results for a target autocovariance function with very

low decay rate

As mentioned in Section 1, long-term persistence processes (Hurst phenomenon), characterised by an
inﬁnite integral scale, are not considered in this study. However, because many physical processes may
show very large integral time scales, it is interesting at least to highlight how the structure of the optimal
AR model changes according to the decay rate of the considered target autocovariance function. To
this end, the optimisation process performed in Section 5 has been repeated assuming a discretisation of
the non-dimensional continuous autocovariance function ten times ﬁner, i.e. ∆˚r = λ
60 = 0.01245. As a
consequence, the target autocovariance function decays ten times slower, and the maximum lag considered
in the MSE computation to account for the 99, 5% of the integral length scale becomes M = 401. Figure
20 shows the resulting target autocovariance function. A direct consequence of this modiﬁcation is an
increase of the computational burden associated with the optimisation procedure.

Table 2 shows the obtained AR model order (the last element of vector j) for the diﬀerent approaches
and model parsimony, N . While the model order for Y-W and K-M approaches depends solely on the
number of model parameters, N , diﬀerent model orders are obtained for the case of the optimal GA-0 and
GA-10 models, according to the diﬀerent decay rate of the target.

The comparison of the model performances in terms of the MSE is qualitatively similar to that shown
in Figure 11, the single diﬀerence being that the error of K-M approach stagnates from N = 8 on, as γT
l
takes values very close to zero at lag 28 = 256. The optimal models obtained for N = 3 are shown in
Table 3. A comparison with Table 1 illustrates the importance of not assuming a predeﬁned AR model
structure, as the optimal one clearly depends on the decay rate of the target.

Acknowledgements

This research has been undertaken as a part of the zEPHYR project. This project has received funding
from the European Union’s Horizon 2020 research and innovation programme under Grant Agreement No
EC grant 860101.

Conﬂict of interest

The authors declare that they have no conﬂict of interest.

28

10-210-110010110200.050.10.150.20.250.3Figure 20: Non-dimensional autocovariance function for longitudinal wind component along wind
direction in isotropic turbulence (continuous line), and target autocovariance function for ∆˚r =
0.01245 (discrete values).

Y-W K-M

Table 2: AR model order, p, for diﬀerent approaches, model parsimony, N , and decay rates of the
target (fast decay: ∆˚r = 0.1245; slow decay: ∆˚r = 0.01245)
Fast decay of γT
l
GA-10
GA-0
1
1
3
3
7
5
10
8
12
10
18
13
19
14
22
16
23
17
23
19

Slow decay of γT
l
GA-10
GA-0
1
1
10
11
42
23
101
37
115
49
132
64
138
77
151
86
146
94
162
105

N = 1
N = 2
N = 3
N = 4
N = 5
N = 6
N = 7
N = 8
N = 9
N = 10

1
2
4
8
16
32
64
128
256
564

1
2
3
4
5
6
7
8
9
10

Table 3: AR models obtained for the diﬀerent approaches for N = 3 and slow decay rate of the
target autocovariance function.

Approach
Y-W
K-M
GA-0
GA-10

j
[1,2,3]
[1,2,4]
[1,4,23]
[1,4,42]

l
[1,2,3]
[1,2,4]
[1,4,23]
[1,9,34]

Model
zt = 0.757 zt−1 + 0.126zt−2 + 0.080 zt−3 + 0.310 εt
zt = 0.757 zt−1 + 0.138zt−2 + 0.069 zt−4 + 0.309 εt
zt = 0.840 zt−1 + 0.109zt−4 + 0.018 zt−23 + 0.311 εt
zt = 0.791 zt−1 + 0.171zt−4 + 0.009 zt−42 + 0.310 εt

References

[Hip, 1994] (1994). Chapter 11 Fractional Autoregressive-Moving Average Models.

In Hipel, K. W.
and McLeod, A. I., editors, Time Series Modelling of Water Resources and Environmental Systems,
volume 45 of Developments in Water Science, pages 389–413. Elsevier.

[Beven, 2021] Beven, K. (2021). Issues in generating stochastic observables for hydrological models. Hy-

drological Processes, 35(6):e14203.

[Box et al., 2016] Box, G. E. P., Jenkins, G. M., Reinsel, G. C., and Ljung, G. M. (2016). Time series

29

00.511.522.533.544.5500.20.40.60.81050100150200250300350400analysis: forecasting and control. Wiley, 5th edition.

[Chang et al., 1981] Chang, M. K., Kwiatkowski, J. W., Nau, R. F., Oliver, R. M., and Pister, K. S.
(1981). ARMA models for earthquake ground motions. Seismic Safety Margins Research Program.
Technical Report NUREG/CR–1751, International Atomic Energy Agency (IAEA).

[de K´arm´an and Howarth, 1938] de K´arm´an, T. and Howarth, L. (1938). On the Statistical Theory of
Isotropic Turbulence. Proceedings of the Royal Society of London. Series A, Mathematical and Physical
Sciences, 164(917):192–215.

[Deodatis, 1996] Deodatis, G. (1996). Simulation of Ergodic Multivariate Stochastic Processes. Journal

of Engineering Mechanics, 122(8):778–787.

[Deodatis and Shinozuka, 1988] Deodatis, G. and Shinozuka, M. (1988). Auto-Regressive Model for Non-

stationary Stochastic Processes. Journal of Engineering Mechanics, 114(11):1995–2012.

[Di Paola and Gullo, 2001] Di Paola, M. and Gullo, I. (2001). Digital generation of multivariate wind

ﬁeld processes. Probabilistic Engineering Mechanics, 16(1):1–10.

[Di Paola and Zingales, 2008] Di Paola, M. and Zingales, M. (2008). Stochastic diﬀerential calculus for
wind-exposed structures with autoregressive continuous (ARC) ﬁlters. Journal of Wind Engineering
and Industrial Aerodynamics, 96(12):2403–2417.

[Dias et al., 2018] Dias, N. L., Crivellaro, B. L., and Chamecki, M. (2018). The Hurst Phenomenon in
Error Estimates Related to Atmospheric Turbulence. Boundary-Layer Meteorology, 168(3):387–416.

[Dimitriadis and Koutsoyiannis, 2015] Dimitriadis, P. and Koutsoyiannis, D. (2015). Climacogram ver-
sus autocovariance and power spectrum in stochastic modelling for Markovian and Hurst–Kolmogorov
processes. Stochastic Environmental Research and Risk Assessment, 29(6):1649–1669.

[Dimitriadis and Koutsoyiannis, 2018] Dimitriadis, P. and Koutsoyiannis, D. (2018). Stochastic synthesis
approximating any process dependence and distribution. Stochastic Environmental Research and Risk
Assessment, 32(6):1493–1515.

[Dimitriadis et al., 2016] Dimitriadis, P., Koutsoyiannis, D., and Papanicolaou, P. (2016). Stochastic sim-
ilarities between the microscale of turbulence and hydro-meteorological processes. Hydrological Sciences
Journal, 61(9):1623–1640.

[Gersch and Foutch, 1974] Gersch, W. and Foutch, D. (1974). Least squares estimates of structural system
parameters using covariance function data. IEEE Transactions on Automatic Control, 19(6):898–903.

[Gersch and Kitagawa, 1985] Gersch, W. and Kitagawa, G. (1985). A time varying AR coeﬃcient model
for modelling and simulating earthquake ground motion. Earthquake Engineering & Structural Dynam-
ics, 13(2):243–254.

[Gersch and Liu, 1976] Gersch, W. and Liu, R. S.-Z. (1976). Time Series Methods for the Synthesis of

Random Vibration Systems. Journal of Applied Mechanics, 43(1):159–165.

[Gersch and Luo, 1972] Gersch, W. and Luo, S. (1972). Discrete Time Series Synthesis of Randomly
Excited Structural System Response. The Journal of the Acoustical Society of America, 51(1B):402–
408.

[Gersch and Yonemoto, 1977] Gersch, W. and Yonemoto, J. (1977). Synthesis of multivariate random
vibration systems: A two-stage least squares AR-MA model approach. Journal of Sound and Vibration,
52(4):553–565.

[Giannini and Mosconi, 1987] Giannini, C. and Mosconi, R. (1987). Predictions from unrestricted and

restricted VAR models. Giornale degli Economisti e Annali di Economia, 46(5/6):291–316.

[Helland and Atta, 1978] Helland, K. and Atta, C. (1978). The “Hurst phenomenon” in grid turbulence.

Journal of Fluid Mechanics, 85(3):573–589.

[Kareem, 2008] Kareem, A. (2008). Numerical simulation of wind eﬀects: A probabilistic perspective.
Journal of Wind Engineering and Industrial Aerodynamics, 96(10):1472–1497. 4th International Sym-
posium on Computational Wind Engineering (CWE2006).

[Kleinhans et al., 2009] Kleinhans, D., Friedrich, R., Schaﬀarczyk, A. P., and Peinke, J. (2009). Synthetic
turbulence models for wind turbine applications. In Progress in Turbulence III, pages 111–114. Springer.

[Krenk, 2011] Krenk, S. (2011). Explicit calibration and simulation of stochastic ﬁelds by low-order
ARMA processes. In ECCOMAS thematic conference on computational methods instructural dynamics
and earthquake engineering, number 550, pages 1–10. ECCOMAS.

30

[Krenk and Møller, 2019] Krenk, S. and Møller, R. N. (2019). Turbulent wind ﬁeld representation and

conditional mean-ﬁeld simulation. Proceedings of the Royal Society A, 475(2223):20180887.

[Li and Kareem, 1990] Li, Y. and Kareem, A. (1990). ARMA systems in wind engineering. Probabilistic

Engineering Mechanics, 5(2):49–59.

[Li and Kareem, 1993] Li, Y. and Kareem, A. (1993). Simulation of Multivariate Random Processes:
Hybrid DFT and Digital Filtering Approach. Journal of Engineering Mechanics, 119(5):1078–1098.

[Liu et al., 2019] Liu, Y., Li, J., Sun, S., and Yu, B. (2019). Advances in Gaussian random ﬁeld generation:

a review. Computational Geosciences, 23(5):1011–1047.

[Madsen, 2007] Madsen, H. (2007). Time series analysis. Chapman & Hall, CRC (Boca Raton).

[Mignolet and Spanos, 1991] Mignolet, M. and Spanos, P. (1991). Simulation of homogeneous two-
dimensional random ﬁelds. Part I. AR and ARMA models.
In Anon, editor, American Society of
Mechanical Engineers (Paper), pages 1–10. Publ by ASME. ASME Winter Annual Meeting ; Confer-
ence date: 01-12-1991 Through 06-12-1991.

[Nordin et al., 1972] Nordin, C. F., McQuivey, R. S., and Mejia, J. M. (1972). Hurst phenomenon in

turbulence. Water Resources Research, 8(6):1480–1486.

[Reed and Scanlan, 1983] Reed, D. A. and Scanlan, R. H. (1983). Time Series Analysis of Cooling Tower

Wind Loading. Journal of Structural Engineering, 109(2):538–554.

[Samaras et al., 1985] Samaras, E., Shinzuka, M., and Tsurui, A. (1985). ARMA Representation of Ran-

dom Processes. Journal of Engineering Mechanics, 111(3):449–461.

[Samii and Vandiver, 1984] Samii, K. and Vandiver, J. K. (1984). A Numerically Eﬃcient Technique for
the Simulation of Random Wave Forces on Oﬀshore Structures. In Oﬀshore Technology Conference.

[Sharifzadehlari et al., 2018] Sharifzadehlari, M., Fathianpour, N., Renard, P., and Amirfattahi, R.
(2018). Random partitioning and adaptive ﬁlters for multiple-point stochastic simulation. Stochas-
tic Environmental Research and Risk Assessment, 32(5):1375–1396.

[Shinozuka and Deodatis, 1991] Shinozuka, M. and Deodatis, G. (1991). Simulation of Stochastic Pro-

cesses by Spectral Representation. Applied Mechanics Reviews, 44(4):191–204.

[Shinozuka and Deodatis, 1996] Shinozuka, M. and Deodatis, G. (1996). Simulation of Multi-Dimensional

Gaussian Stochastic Fields by Spectral Representation. Applied Mechanics Reviews, 49(1):29–53.

[Soltan Mohammadi et al., 2020] Soltan Mohammadi, H., Abdollahifard, M. J., and Doulati Ardejani,
F. (2020). CHDS: conﬂict handling in direct sampling for stochastic simulation of spatial variables.
Stochastic Environmental Research and Risk Assessment, 34(6):825–847.

[Spanos, 1983] Spanos, P. D. (1983). ARMA Algorithms for Ocean Wave Modeling. Journal of Energy

Resources Technology, 105(3):300–309.

[Spanos and Hansen, 1981] Spanos, P. D. and Hansen, J. E. (1981). Linear Prediction Theory for Digital

Simulation of Sea Waves. Journal of Energy Resources Technology, 103(3):243–249.

[Sparks et al., 2018] Sparks, N. J., Hardwick, S. R., Schmid, M., and Toumi, R. (2018).

IMAGE: a
multivariate multi-site stochastic weather generator for European weather and climate. Stochastic
Environmental Research and Risk Assessment, 32(3):771–784.

[Stoica and Moses, 2005] Stoica, P. and Moses, R. (2005). Spectral Analysis of Signals. Prentice Hall.

[Tsay, 2013] Tsay, R. S. (2013). Multivariate Time Series Analysis: With R and Financial Applications.

Wiley Series in Probability and Statistics. Wiley.

31

