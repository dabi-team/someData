9
1
0
2

l
u
J

5
1

]

R
P
.
h
t
a
m

[

1
v
2
8
7
6
0
.
7
0
9
1
:
v
i
X
r
a

AR(1) processes driven by second-chaos white noise: Berry-Esséen
bounds for quadratic variation and parameter estimation

Soukaina Douissi1, Khalifa Es-Sebaiy2, Fatimah Alshahrani3, Frederi G. Viens4.

1 Laboratory LIBMA, Faculty Semlalia, Cadi Ayyad University, 40000 Marrakech, Morocco.
Email: douissi.soukaina@gmail.com
2 Department of Mathematics, Faculty of Science, Kuwait University, Kuwait.
Email: khalifa.essebaiy@ku.edu.kw
3 Department of mathematical science, Princess Nourah bint Abdulrahman university, Riyadh.
3,4 Department of Statistics and Probability, Michigan State University, East Lansing, MI 48824.
Email: alshah10@msu.edu, viens@msu.edu

July 17, 2019

Abstract: In this paper, we study the asymptotic behavior of the quadratic variation for
the class of AR(1) processes driven by white noise in the second Wiener chaos. Using tools from the
analysis on Wiener space, we give an upper bound for the total-variation speed of convergence to
the normal law, which we apply to study the estimation of the model’s mean-reversion. Simulations
are performed to illustrate the theoretical results.

Key words: Central limit theorem; Berry-Esséen; Malliavin calculus; parameter estimation; time
series; Wiener chaos

2010 Mathematics Subject Classiﬁcation: 60F05; 60H07; 62F12; 62M10

1

Introduction

The topic of statistical inference for stochastic processes has a long history, addressing a number of
issues, though many diﬃcult questions remain. At the same time, a number of application ﬁelds are
anxious to see some practical progress in a selection of directions. Methodologies are sought which
are not just statistically sound, but stand a good chance of being computationally implementable,
if not nimble, to help practitioners make data-based decisions in stochastic problems with complex
time evolutions. In this paper, which is motivated by parameter estimation within the above context,

The ﬁrst author is supported by the Fulbright joint supervision program for PhD students for the academic year
2018-2019 between Cadi Ayyad University and Michigan State University. The fourth author is partially supported
by NSF awards DMS 1734183 and 1811779, and ONR award N00014-18-1-2192.

1

 
 
 
 
 
 
we propose a quantitatively sharp analysis in this direction, and we honor the scientiﬁc legacy of
Prof. Larry Shepp.

Prof. Shepp is widely known for his seminal work on stochastic control, optimal stopping,
and applications in areas such as investment ﬁnance. Often labeled as an applied probabilist, by
those working in that area, he had the merit, among many other qualities, of showing by example
that research activity in this area could beneﬁt from an appreciation for the mathematical aesthetics
of constructing stochastic objects for their own sake. His papers also showed that one’s work is only
as applied as one’s ability to calibrate a stochastic model to a realistic scenario. As obvious as this
view may seem, it is nonetheless in short supply in some current circles, where model sophistication
seems to replace all other imperatives. Instead, we believe some of the principles guiding applied
probability research should include (i) statistical parsimony and robustness, (ii) feature discovery,
and above all, (iii) real-world impact where mathematicians propose a real solution to a real problem.
We think that Prof. Shepp would not have been shy about agreeing that his seminal and highly
original works on optimal stopping and stochastic control [45], [?], including the invention [46] of the
widely used Russian ﬁnancial option, illustrate items (ii) and (iii) in this philosophy perfectly. This
leaves the question of how to estimate model parameters needed to implement applied solutions.
Prof. Shepp proved on many occasions that this concern was also high on his list of objectives
in applied work; he proposed methods aligning with our stated principle (i) above. The best
example is the work for which Shepp is most widely known outside of our stochastic circles: the
mathematical foundation of the Computational Tomography (CT) scanner, and in particular, the
basis [47] for its data analysis. Prof. Shepp is less well known for his direct interest in statistically
motivated stochastic modeling; the posthumous paper [17] is an instance of this, on asymptotics of
auto-regressive processes with normal noise (innovations).

Our paper honors this legacy by providing a detailed and mathematically rigorous stochastic
analysis of some building blocks needed in the data analysis of a simple class of stochastic processes.
Our paper’s originality is in working out detailed quantitative properties for auto-regressive processes
with innovations in the second Wiener chaos. Our framework is parsimonious in the sense of being
determined by a small number of parameters, while covering features of stationarity, mean-reversion,
and heavier-than-normal tail weight. We focus on establishing rates of convergence in the central
limit theorem for quadratic variations of these processes, which we are then able to transfer to similar
rates for the model’s moments-based parameter estimation. This precision would allow practitioners
to determine the validity and uncertainty quantiﬁcation of our estimates in the realistic setting of
moderate sample size. Careless use of a method of moments would ignore the potential for abusive
conclusions in this heavy-tailed time-series setting.

The remainder of this introduction begins with an overview of the landscape of parameter
estimation for stochastic processes related to ours. The few included references call for the reader
to ﬁnd additional references therein, for the sake of conciseness. We then introduce the speciﬁc
model class used in this paper. It represents a continuation of the current literature’s motivation
to calibrate stochastic models with features such as stochastic memory and path roughness.
It
constitutes a departure from the same literature’s focus on the framework of Gaussian noise.

2

1.1 Parameter estimation for stochastic processes: historical and recent context

Some of the early impetus in parameter estimation for stochastic processes was inspired by classical
ideas from frequentist statisitics, such as the theoretical and practical superiority of maximum
likelihood estimation (MLE), over other, less constrained methodologies, in many contexts. We will
not delve into the description of many such instances, citing only the seminal account [26], ﬁrst
published in Russian in 1974 (see references therein in Chapter 17, such as [34]). This was picked
up two decades later in the context of processes driven by fractional Brownian motion, where it was
shown that the martingale property used in earlier treatments was not a necessary ingredient to
establish the properties of such MLEs: see in particular the treatment of processes with fractional
noise in [24] and in [51]. It was also noticed that least-squares ideas, which led to MLEs in cases
of white-noise driven processes, did not share this property in the case of processes driven by
fractional noise: this was pointed out in the continuous-time based paper [22]. See also a more
detailed account of this direction of work in [16] and references therein, including a discussion of the
distinction between estimators based on continuous paths, and those using discrete sampling under
in-ﬁll and increasing-horizon asymptotics. These were applied particularly to various versions of the
Ornstein-Uhlenbeck process, as examples of processes with stationary increments and an ability to
choose other features such as path regularity and short or long memory.

The impracticality of computing MLEs for parameters of stochastic processes in these
feature-rich contexts, led the community to consider other methodologies, looking more closely
at least squares and beyond. A popular approach is to work with incarnations of the method of
moments. A full study in the case of general stationary Gaussian sequences, with application to
in-ﬁll asymtotics for the fractional Ornstein-Uhlenbeck process, is in [4]. This paper relates the
relatively long history of those works where estimation of a memory or Hölder-regularity parameter
uses moments-based objects, particularly quadratic variations. It also shows that the generalized
method of moments can, in principle, provide a number of options to access vectors of parameters
for discretely observed Gaussian processes in a practical way. This was also illustrated recently in
[18], where the Malliavin calculus and its connection to Stein’s method was used to establish speeds
of convergence in the central-limit theorems for quadratic-variations-based estimators for discretely
observed processes. The Stein-Malliavin technical methodology employed in [18] is that which was
introduced by Nourdin and Peccati in 2009, as described in their 2012 research monograph [32].

Other estimation methods are also proposed for general stationary time series, which we
mention here, though they fall out of the scope of our paper, and they do not lead to the same
precision as those based on the Stein-Malliavin method: see e.g. [53] and [54] for the Yule-Walker
method and extensions. While the paper [52] establishes that essentially every continuous-time
stationary process can be represented as the solution of a Langevin (Ornstein-Uhlenbeck-type)
equation with an appropriate noise distribution, the two aforementioned follow-up papers [53, 54],
which present an analog in discrete time, do not, however, connect the discrete and continuous
frameworks via any asymptotic theory.

Following an initial push in [51], most of the recent papers mentioned above, and recent
references therein, state an explicit eﬀort to work with discretely observed processes. At least
in the increasing-horizon case, the papers [18] and [13] had the merit of pointing out that many
of the discretization techniques used to pass from continuous-path to discrete-observation based

3

estimators, were ineﬃcient, and it is preferable to work directly from the statistics of the discretely
observed process. Our paper picks up this thread, and introduces a new direction of research
which, to our knowledge, has not been approached by any authors: can the asymptotic normality of
quadratic variations and related estimators, including very precise results on speeds of convergence,
be obtained when the driving noise is not Gaussian?

The main underlying theoretical result we draw on is the optimal estimation of total-
variation distances between chaos variables and the normal law, established in [31]. It was used for
quadratic variations of stationary sequences in the Gaussian case in [29]. But when the Gaussian
setting is abandonned, the result in [31] cannot be used directly. Instead, our paper makes a theo-
retical advance in the analysis on Wiener space, by drawing on a simple idea in the recent preprints
[37] and [33]; our main result provides an example of a sum of chaos variables whose distance to
the normal appears to be estimated optimally, whereas a standard use of the Schwartz inequality
would result in a much weaker result. The precise location of the technique leading to this improve-
ment is pointed out in the main body of our paper: see Theorem 8, particularly inequality (24)
in its proof and the following brief discussion there, and Remark 9. This allows us to prove our
Berry-Esséen-type speed of n−1/2, rather than what would have resulted in a speed of n−1/4.

1.2 A stationary process with second-chaos noise, and related literature

Given our intent to address the new issue of noise distribution, and knowing that Berry-Esséen-
type questions for models with mere Gaussian noise already present technical challenges, we choose
to minimize the number of technical issues to address in this paper by focusing on the simplest
possible stationary model class which does not restrict the marginal noise distribution within a
family which is tractable using Wiener chaos analysis and tools from the Malliavin calculus. This is
the auto-regressive model of order 1 (a.k.a. AR(1)) with independent noise terms, where the noise
distribution is in the second Wiener chaos, i.e.

Yn = a0 + a1Yn−1 + εn

(1)

where {εn; n ∈ Z} is an i.i.d. sequence in the second Wiener chaos, and a0 and a1 are constants.
The complete description and construction of this process and of the noise sequence is given in
Section 3, see (8).

As explained in Section 2, the second Wiener chaos is a linear space, and since the model (1)
is linear, its solution, if any, lies in the same chaos. This points to a simple theoretical motivation
and justiﬁcation for studying the increasing horizon problem as opposed to the in-ﬁll problem.
We also include a practical motivation for doing so, further below in this section, coming from an
environmental statistics question.

For the former motivation, note that the AR(1) speciﬁcation (1), with essentially any square-
integrable i.i.d. noise distribution, is known to converge weakly, after appropriate aggregation
and scaling, to the so-called Ornstein-Uhlenbeck process (also known occasionally as the Vasicek
process), which solves the stochastic diﬀerential equation

dXt = α(m − Xt)dt + σdW (t)

(2)

4

where W is a standard Wiener process (Gaussian Brownian motion), and the parameters α, m,
and σ are explicitly related to a0, a1 and V ar [εn]. See for instance [48, Chapter 2], which covers
the case of all square-integrable innovations; this paper assumes a piecewise linear interpolation
in the normalization, which could be eliminated by switching to convergence in the Skorohod J1
topology. A reference avoiding linear interpolation, with convergence in the Skorohod J1 topology,
is [10], where innovations are assumed to have four moments. In any case, this central limit theorem
constrains the modeling of stationary/ergodic processes via diﬀusive diﬀerential formulation: under
in-ﬁll asymptotics with weakly dependent noise, the AR(1) speciﬁcation cannot preserve any non-
normal noise distribution in the limit. It is of course possible to interpolate the above process Y in
a number of ways, to result in a continuous-time process whose discrete-time marginals are those
speciﬁed via (1).

However we believe it is diﬃcult or impossible to give a linear diﬀusion-type stochastic
diﬀerential equation, akin to (2), whose ﬁxed-time-step marginals are as in (1) for an arbitrary
noise distribution, while simultaneously describing what second-chaos process diﬀerential would
need to replace W in (2). The so-called Rosenblatt process (see [50]), the only known second-chaos
continuous-time process with a stochastic calculus similar to W ’s, gives an example of a viable
alternative to (2) living in the second chaos. But this process is known to have only a long-memory
version. Thus it cannot be a proxy for any continuous-time analogue of (1), since the noise there
has no memory. Similar issues would presumably exist for other non-Gaussian AR(1) and related
auto-regressive processes. A few have been studied recently in the literature. We mention [21, 55],
which cover various noise structures similar to second-chaos noises, and here again, no asymptotic or
interpolation theory is provided to relate to continuous time. There does exist a general treatment in
[17] of asymptotics for all AR(p) processes: the limit processes are the so-called Continuous-AR(p)
processes, which are Gaussian, and have p − 1-diﬀerentiable paths (a form of very long memory for
p > 1); that paper assumes normal innovations to keep technicalities to a minimum.

Another indication that ﬁnding such a proxy may fail comes in the speciﬁc case of the so-
called Gumbel distribution for εn. This law is a popular distribution for extreme-value modeling.
The fact that this law is in the second chaos is a classical result (as a weighted sum of exponentials,
see [44]), though it does not appear to be widely know in the extreme-value community. The stan-
dard (mean-zero) Gumbel law can be represented as (cid:80)∞
/2
is a standard exponential variable (chi-squared with two degrees of freedom, Nj and ¯Nj are iid
standard normals). The Gumbel law is known to give rise to a second-chaos version of an isonormal
Gaussian process, known as the Gumbel noise measure (or Gumbel process); that stochastic mea-
sure obeys the same laws as the white-noise measure (including independence of increments which
fails for the Rosenblatt noise), if one replaces the standard algebra of the reals by the max-plus
algebra. This is explained in detail in the preprint [27]; also see references therein. By virtue of this
change of algebra, stochastic diﬀerential speciﬁcations as in (2) cannot be deﬁned using the Gumbel
noise.

n=1 j−1 (Ej − 1) where Ej =

j + ¯N 2

N 2

(cid:17)

(cid:16)

j

However, the discrete version of the Gumbel noise, an i.i.d. sequence (εn)n with Gumbel
marginals, is a good example of a noise type which can be used in the AR(1) process (1). This
speciﬁc model, known as the AR(1) process with Gumbel noise (or innovations), presents a main
motivation for our work. Recent references on this process, and on the closely related process where

5

the marginals of Y are Gumbel-distributed, include [28] for a Bayesian study, [49] for applications to
maxima in greenhouse gas concentration data, and [3] for AR processs in the broader extreme-value
context. A survey on AR(1) models with diﬀerent types of innovations and marginals, while not
including the Gumbel, is in the unpublished manuscript [20]. The use of the Gumbel distribution
for describing environmental time series, mainly when looking at extremes, is fairly widespread, but
we do not cite this literature because it does not appear willing to acknowledge that time-series
models driven by Gumbel innovations should be used, rather than using tools for i.i.d. Gumbel
data. This literature, which is easy to ﬁnd, is also entirely unaware that the Gumbel distribution is
in the second Wiener chaos.

All these reasons give us ample cause to investigate the basic method-of-moments building
blocks for determining parameters in stationary time series with second-chaos innovations. For
the sake of concentrating on the core mathematical analysis towards this end, we focus on the
asymptotics of quadratic variations for models in the class (1). The methodology developed in
[18] can then be adapted to handle any method-of-moments-based estimators, at the cost of some
additional eﬀort. We provide examples of this in the latter sections of this paper. Our main result
is that, for any second-chaos innovations in (1), the quadratic variation of Y has explicit normal
asymptotics, with a speed of convergence in total variation which matches the classical Berry-Esseén
speed of n−1/2.

The remainder of this paper is structured as follows. Section 2 provides elements from the
analysis on Wiener space which will be used in the paper. Section 3 presents the details of the class of
AR(1) models we will analyze. Section 4 computes the asymptotic variance of the AR(1)’s quadratic
variation by looking separately at its 2nd-chaos and 4th-chaos components, whose asymptotics are
of the same order. Section 5 establishes our main result, the Berry-Esseén speed of convergence
in total-variation for the normal ﬂuctuations of the AR(1)’s quadratic variation. Finally, Section
6 deﬁnes a method-of-moments estimator for the mean-reversion rate of this AR(1) process, and
establishes its asymptotic properties; a numerical study is included to gauge the distance between
this renormalized estimator and the normal law.

2 Preliminaries

In this ﬁrst section, we recall some elements from stochastic analysis that we will need in the
paper. See [32], [39], and [40] for details. Any real, separable Hilbert space H gives rise to an
isonormal Gaussian process: a centered Gaussian family (G(ϕ), ϕ ∈ H) of random variables on
In this paper, it is enough to
a probability space (Ω, F, P) such that E(G(ϕ)G(ψ)) = (cid:104)ϕ, ψ(cid:105)H.
use the classical Wiener space, where H = L2([0, 1]), though any H will also work. In the case
H = L2([0, 1]), G can be identiﬁed with the stochastic diﬀerential of a Wiener process W and one
interprets G(ϕ) := (cid:82) 1

The Wiener chaos of order n is deﬁned as the closure in L2 (Ω) of the linear span of the
random variables Hn(G(ϕ)), where ϕ ∈ H, (cid:107)ϕ(cid:107)H = 1 and Hn is the Hermite polynomial of degree n.
The intuitive Riemann-sum-based notion of multiple Wiener stochastic integral In with respect to
G ≡ W , in the sense of limits in L2 (Ω), turns out to be an isometry between the Hilbert space

0 ϕ (s) dW (s).

6

H(cid:12)n (symmetric tensor product) equipped with the scaled norm 1√
(cid:107) · (cid:107)H⊗n and the Wiener chaos
n!
of order n under L2 (Ω)’s norm. In any case, we have the following fundamental decomposition of
L2 (Ω) as a direct sum of all Wiener chaos.
• The Wiener chaos expansion. For any F ∈ L2 (Ω), there exists a unique sequence of functions
fn ∈ H(cid:12)n such that

F = E[F ] +

In(fn),

∞
(cid:88)

where the terms are all mutually orthogonal in L2 (Ω) and
E (cid:2)In(fn)2(cid:3) = n!(cid:107)fn(cid:107)2

H⊗n.

n=1

(3)

• Product formula and contractions. Since L2 (Ω) is closed under multiplication, the special
case of the above expansion exists for calculating products of Wiener integrals, and is explicit using
contractions: for any p, q, and symmetric integrands f ∈ H(cid:12)p and g ∈ H(cid:12)q,

Ip(f )Iq(g) =

p∧q
(cid:88)

r=0

r!Cr

pCr

q Ip+q−2r(f ⊗r g);

(4)

see [39, Proposition 1.1.3] for instance; the contraction f ⊗r g is the element of H⊗(p+q−2r) deﬁned
by

(f ⊗r g)(s1, . . . , sp−r, t1, . . . , tq−r)

(cid:90)

:=

[0,1]p+q−2r

f (s1, . . . , sp−r, u1, . . . , ur)g(t1, . . . , tq−r, u1, . . . , ur) du1 · · · dur.

The special case for p = q = 1 is particularly handy, and can be written in its symmetrized form:

I1(f )I1(g) = 2−1I2 (f ⊗ g + g ⊗ f ) + (cid:104)f, g(cid:105)H.

• Hypercontractivity in Wiener chaos. For h ∈ H⊗q, the multiple Wiener integrals Iq(h),
which exhaust the set Hq, satisfy a hypercontractivity property (equivalence in Hq of all Lp norms
for all p (cid:62) 2), which implies that for any F ∈ ⊕q
l=1Hl (i.e. in a ﬁxed sum of Wiener chaoses), we
have

(cid:0)E(cid:2)|F |p(cid:3)(cid:1)1/p (cid:54) cp,q

(cid:0)E(cid:2)|F |2(cid:3)(cid:1)1/2

for any p (cid:62) 2.

(5)

It should be noted that the constants cp,q above are known with some precision when F ∈ Hq: by
Corollary 2.8.14 in [32], cp,q = (p − 1)q/2.
• Malliavin derivative and other operators on Wiener space. The Malliavin derivative
operator D, and other operators on Wiener space, are needed brieﬂy in this paper, to provide an
eﬃcient proof of the ﬁrst theorem in Section 5, and to interpret an observation of I. Nourdin and
G. Peccati, given below in (7), for a bound on the total variation distance of any chaos law to the
normal law. We do not provide any background on these operators, referring instead to Chapter 2
in [32], and brieﬂy mentioning here the facts we will use in the proof of Section 5, without spelling
out all assumptions. Strictly speaking, all the results in this paper can be obtained without the
following facts, but this would be exceedingly tedious and wholly nontransparent.

7

• The operator D maps Iq (f ) to t (cid:55)→ qIq−1 (f (., t)) and is consistent with the ordinary chain

rule. Its domain is denoted by D1,2,and includes all chaos variables.

• The operator L, known as the generator of the Orstein-Uhlenbeck semigroup on Wiener
space, maps Iq (f ) to −qIq (f ), and L−1 denotes its pseudo-inverse: L’s kernel is the con-
stants, all other chaos are its eigenspaces. Combining this with the previous point, we obtain
−DtL−1Iq (f ) = Iq−1 (f (., t)).

• This D has an adjoint δ in L2 (Ω), which by deﬁnition satisﬁes the duality relation E (cid:104)DF, u(cid:105)H =
E [F δ (u)], where u is any stochastic process for which the expressions are deﬁned. The do-
main of δ is a non-trivial object of study, but it is known to contain all square-integrable
W -adapted processes for the case of G = W , the wiener process, where H = L2 ([0, 1]).

• We have the relation

F = δ(−DL−1)F.

• Distances between random variables. The following is classical. If X, Y are two real-valued
random variables, then the total variation distance between the law of X and the law of Y is given
by

dT V (X, Y ) := sup

A∈B(R)

|P [X ∈ A] − P [Y ∈ A]|

where the supremum is over all Borel sets. The Kolmogorov distance dKol (X, Y ) is the same as
dT V except one take the sup over A of the form (−∞, z] for all real z. The Wasserstein distance
uses Lipschitz rather than indicator functions:

dW (X, Y ) := sup

|Ef (X) − Ef (Y )| ,

f ∈Lip(1)

Lip(1) being the set of all Lipschitz functions with Lipschitz constant (cid:54) 1.
• The observation of Nourdin and Peccati. Let N denote the standard normal law. The
following observation relates an integration-by-parts formula on Wiener space with a classical result
of Ch. Stein.

Let X ∈ D1,2 with E [X] = 0 and V ar [X] = 1. Then (see [31, Proposition 2.4], or [32, Theorem

5.1.3]), for f ∈ C1

b (R),

E [Xf (X)] = E (cid:2)f (cid:48) (X) (cid:10)DX, −DL−1X(cid:11)

(cid:3)

H

and by combining this with properties of solutions of Stein’s equations, one gets

dT V (X, N ) (cid:54) 2E (cid:12)

(cid:12)1 − (cid:10)DX, −DL−1X(cid:11)

(cid:12)
(cid:12) .

H

(6)

One notes in particular that when X ∈ Hq, since −L−1X = q−1X, we obtain conveniently

dT V (X, N ) (cid:54) 2E

(cid:12)
(cid:12)1 − q−1 (cid:107)DX(cid:107)2
(cid:12)

H

(cid:12)
(cid:12)
(cid:12) .

(7)

8

• A convenient lemma. The following result is a direct consequence of the Borel-Cantelli Lemma
[25]). It is convenient for establishing almost-sure convergences
(the proof is elementary; see e.g.
from Lp convergences.

Lemma 1 Let γ > 0. Let (Zn)n∈N be a sequence of random variables. If for every p (cid:62) 1 there
exists a constant cp > 0 such that for all n ∈ N,

(cid:107)Zn(cid:107)Lp(Ω) (cid:54) cp · n−γ,

then for all ε > 0 there exists a random variable ηε which is almost such that

for all n ∈ N. Moreover, E|ηε|p < ∞ for all p (cid:62) 1.

|Zn| (cid:54) ηε · n−γ+ε

almost surely

3 The model

3.1 Deﬁnition

We consider the following AR(1) model






Yn = a0 + a1Yn−1 + εn, n (cid:62) 1
∞
(cid:80)
εn =
δ=1
Y0 = y0 ∈ R.

n,δ − 1)

σδ(Z2

(8)

where a0, a1 and {σδ, δ (cid:62) 1} are real constants. The sequence of innovations {εn, n (cid:62) 1} is i.i.d.,
with distribution in the second Wiener chaos. It turns out that this sequence can be represented as
in the second line above in (8), where the family {Zn,δ, n (cid:62) 1, δ (cid:62) 1} are i.i.d. standard Gaussian
random variables deﬁned on (Ω, F, P), and {σδ; δ (cid:62) 1} is a sequence of reals satisfying

∞
(cid:88)

δ=1

σ2
δ < ∞.

(9)

This is explained in [32, Section 2.7.4]. We assume that the mean reversion parameter a1 is such
that |a1| < 1. Under this condition, (8) also admits a stationary ergodic solution. Both the version
above and the stationary version are linear functionals of elements of the form of εn, which are
elements of the second Wiener chaos. Since this chaos is a vector space, both versions of Y take
values in the second Wiener chaos.

By truncating the series in (8), one obtains a process which is a sum of chi-squared variables,
converging to Y in L2(Ω). Special cases where the sum is ﬁnite, can be considered. In the ﬁgures
below, we simulate 500 observations from such cases, to show the variety of behaviors, even with a
limited number of terms in the noise series.

• When σ1 = σ and σδ = 0, for all δ (cid:62) 2, corresponds to a scaled mean-zero chi-squared white
n,1 − 1) ∼ χ2(1).

noise with one degree of freedom: (Z2

9

(a) a0 = 2, a1 = 0.7, y0 = 3, σ = 2.

(b) a0 = 1.2, a1 = 0.4, y0 = 2, σ = 4.

(c) a0 = 0.8, a1 = 0.5, y0 = 1, σ1 = σ2 = 2.

(d) a0 = 1.2, a1 = 0.4, y0 = 0, σ1 = −σ2 = 1.

Figure 1: 500 various observations from (8) for diﬀerent values of a0, a1, y0 and σδ, δ = 1, 2.

• When σ1 = σ2 = σ and σδ = 0, ∀δ (cid:62) 3, an exponential white noise with rate parameter

1/(2σ). Indeed (Z2

n,1 − 1) + (Z2

n,2 − 1) ∼ E(1/2).

• When σ1 = −σ2, and σδ = 0, for all δ (cid:62) 2, which is a symmetric second chaos white noise,
standard normals, then

if N, N (cid:48) are two i.i.d.

ε’s law is equal to a product normal law:
n,1 − Z2
2N N (cid:48) ∼ (Z2

n,1 − 1) − (Z2

n,2 − 1) = Z2

n,2.

Remark 2 We can see from the ﬁgures above the asymmetry in ﬁgures (a), (b) and (c) due to the
asymmetric nature of the noise; ﬁgure (d) shows more symmetry because of the choice σ1 = −σ2.
We also notice that when the mean reversion is fairly strong and the noise is large the shape of the
observations is balanced (ﬁgure (a)), while when the noise is larger compared to the mean-reversion

10

parameter, the observations look like an Ornstein-Uhlenbeck process with a noise larger than the
drift (see ﬁgure (b)).

3.2 Quadratic variation

This paper’s main goal is to determine the asymptotic distribution of the quadratic variation of the
observations {Yn, n (cid:62) 1} using analysis on Wiener space.

This will be facilitated by the fact, mentioned above, that the sequence (Yn)n(cid:62)1 lives in the
second Wiener chaos with respect to the Wiener process W , by virtue of being the solution of a
linear equation with noise in the second chaos. To be more speciﬁc, observe that {Yn, n (cid:62) 1} in (8)
can be expressed recursively as follows :

where

Yi = di +

i
(cid:88)

k=1

ai−k
1

∞
(cid:88)

δ=1

σδ

(cid:0)Z2

k,δ − 1(cid:1) ,

i (cid:62) 1

di = ai

1y0 + a0

i
(cid:88)

k=1

ai−k
1

.

(10)

(11)

For the sake of ease of computation in Wiener chaos, it will be convenient throughout this pa-
per to refer to the Wiener integral representation of the noise terms Z2
k,δ. For this, there exists
{hk,δ, k (cid:62) 1, δ (cid:62) 1} an orthonormal family L2([0, 1]) for which Zk,δ = W (hk,δ) = I W
1 (hk,δ) =
(cid:82) 1
0 hk,δ (r) dW (r). Hence, using the fact, which comes from the most elementary application of
the product formula (4), that W 2 (ϕ) − 1 = I W
2

(cid:0)ϕ⊗2(cid:1), we have for i (cid:62) 1 :

Yi = di +

= di +

= di +

i
(cid:88)

k=1
i
(cid:88)

k=1
i
(cid:88)

ai−k
1

ai−k
1

ai−k
1

∞
(cid:88)

δ=1
∞
(cid:88)

δ=1
∞
(cid:88)

σδ

(cid:0)Z2

k,δ − 1(cid:1)

(cid:0)W 2(hk,δ) − 1(cid:1)

σδ

σδI W

2 (h⊗2
k,δ)

k=1
Therefore, using the linearity property of multiple integrals, we can write for i (cid:62) 1,

δ=1

Yi = di + ˜Yi,

where

˜Yi := I W

2 (fi)

and fi :=

i
(cid:88)

k=1

ai−k
1

∞
(cid:88)

δ=1

σδh⊗2
k,δ.

(12)

A straightforward computation shows that under Assumption (9), the kernel fi ∈ L2([0, 1]2) for all
i (cid:62) 1 : indeed

(cid:107)fi(cid:107)2

L2([0,1]2) =

∞
(cid:88)

δ=1

σ2
δ ×

(1 − a2i
1 )
(1 − a2
1)

(cid:54)

1
(1 − a2
1)

11

∞
(cid:88)

δ=1

σ2
δ < ∞.

Our main object of study in the next two sections is the asymptotics of the quadratic

variation deﬁned as follows :

Qn :=

1
n

n
(cid:88)

i=1

(Yi − di)2 =

1
n

n
(cid:88)

i=1

˜Y 2
i .

Using product formula (4), we get

Qn − E[Qn] =

=

1
n

1
n

n
(cid:88)

(cid:16)

2 (fi)2 − 2(cid:107)fi(cid:107)2
I W

L2([0,1]2)

(cid:17)

i=1
n
(cid:88)

i=1
(cid:32)

I W
4 (fi ⊗ fi) +

4
n

n
(cid:88)

i=1

(cid:33)

= I W
4

fi ⊗ fi

+ I W
2

n
(cid:88)

1
n

i=1
=: T4,n + T2,n.

I W
2 (fi ⊗1 fi)

(cid:32)

4
n

n
(cid:88)

i=1

(cid:33)

fi ⊗1 fi

(13)

(14)

In the next section, we show that the asymptotic variance of
n (Qn − E[Qn]) exits and we will
compute its speed of convergence. Then we establish a CLT for Qn, and compute its Berry-Esséen
speed of convergence in total variation.

√

4 Asymptotic variance of the quadratic variation

Using the orthogonality of multiple integrals living in diﬀerent chaos, to calculate the limiting
variance of
n(Qn − E[Qn]), we need only study separately the second moments of the terms T2,n
and T4,n given in (14).

√

4.1 Scale constant for T2,n

Proposition 3 Under Assumption (9), with T2,n as in (14), for large n,

(cid:104)(cid:0)√

nT2,n

(cid:1)2(cid:105)

−

E

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

32

∞
(cid:80)
δ=1
(1 − a2

σ4
δ

1)2

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:54) C1
n

,

where C1 := 32

(cid:18) ∞
(cid:80)
δ=1

σ4
δ

(cid:19) [1+a2
(1−a4

1)]
1(5+6a2
1) . In particular
1)2(1−a2

l1 := lim
n→∞

E

(cid:104)(cid:0)√

(cid:1)2(cid:105)

=

nT2,n

12

32

∞
(cid:80)
δ=1
(1 − a2

σ4
δ
1)2 .

(15)

(16)

we get

Proof. We have T2,n = I W

2 ( 4
n

n
(cid:80)
i=1

fi ⊗1 fi), by the isometry property (3) of multiple integrals,

E[T 2

2,n] = 2(cid:107)

4
n

n
(cid:88)

i=1

fi ⊗1 fi(cid:107)2

L2([0,1]2) =

32
n2

n
(cid:88)

i,j=1

(cid:104)fi ⊗1 fi, fj ⊗1 fj(cid:105)L2([0,1]2) .

(17)

Moreover, under Assumption (9), we have

(fi ⊗1 fi)(x, y) =

=

=

i
(cid:88)

k1,k2=1
i
(cid:88)

k1,k2=1

ai−k1
1

ai−k2
1

ai−k1
1

ai−k2
1

∞
(cid:88)

δ1,δ2=1
∞
(cid:88)

δ1,δ2=1

σδ1σδ2(h⊗2
k1,δ1

⊗1 h⊗2

k2,δ2

)(x, y)

σδ1σδ2(hk1,δ1 ⊗ hk2,δ2)(x, y)δδ1,δ2δk1,k2

i
(cid:88)

k=1

a2(i−k)
1

∞
(cid:88)

δ=1

δ (h⊗2
σ2

k,δ)(x, y).

Therefore, for i, j (cid:62) 1 such that j (cid:62) i, we get

(cid:104)fi ⊗1 fi, fj ⊗1 fj(cid:105)L2([0,1]2) =

=

=

i
(cid:88)

k1=1

i
(cid:88)

k1=1

i
(cid:88)

k1=1

a2(i−k1)
1

a2(i−k1)
1

a2(i−k1)
1

j
(cid:88)

k2=1
j
(cid:88)

k2=1
j
(cid:88)

k2=1

a2(j−k2)
1

a2(j−k2)
1

a2(j−k2)
1

∞
(cid:88)

δ1,δ2=1

∞
(cid:88)

δ1,δ2=1

∞
(cid:88)

δ1,δ2=1

δ1σ2
σ2
δ2

(cid:68)

h⊗2
k1,δ1

, h⊗2

k2,δ2

(cid:69)

L2([0,1]2)

(cid:16)

δ1σ2
σ2
δ2

(cid:104)hk1,δ1, hk2,δ2(cid:105)L2([0,1])

(cid:17)2

δ1σ2
σ2

δ2δδ1,δ2δk1,k2

= a2(j−i)
1

i
(cid:88)

k=1

a4(i−k)
1

×

∞
(cid:88)

σ4
δ

δ=1
(cid:18) 1 − a4i
1
1 − a4
1

(cid:19)

.

=

∞
(cid:88)

δ=1

δ × a2(j−i)
σ4

1

Therefore, by (17), we have

√

E (cid:2)(

nT2,n)2(cid:3) =

32
n

n
(cid:88)

i=1

(cid:107)fi ⊗1 fi(cid:107)2

L2([0,1]2) +

64
n

n−1
(cid:88)

n
(cid:88)

i=1

j=i+1

(cid:104)fi ⊗1 fi, fj ⊗1 fj(cid:105)L2([0,1]2) .

Moreover,

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

32
n

n
(cid:88)

i=1

(cid:107)fi ⊗1 fi(cid:107)2

L2([0,1]2) −

32

σ4
δ

∞
(cid:80)
δ=1
(1 − a4
1)

32

σ4
δ

∞
(cid:80)
δ=1
(1 − a4
1)

(cid:12)
(cid:12)
(cid:12)
−
(cid:12)
(cid:12)

1
n

n
(cid:88)

i=1

a4i
1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:54)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

=

13

32

∞
(cid:80)
δ=1
(1 − a4

σ4
δ

1)2

1
n

.

On the other hand

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

64
n

n−1
(cid:88)

n
(cid:88)

i=1

j=i+1

(cid:104)fi ⊗1 fi, fj ⊗1 fj(cid:105)L2([0,1]2) −

1
n

n−1
(cid:88)

(1 − a4i
1 )

n
(cid:88)

i=1

j=i+1

a2(j−i)
1

−

64a2
1

σ4
δ

∞
(cid:80)
δ=1
1)(1 − a2
1)

(1 − a4

64a2
1

σ4
δ

∞
(cid:80)
δ=1
1)(1 − a2
1)

(1 − a4

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

64

σ4
δ

∞
(cid:80)
δ=1
(1 − a4
1)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
n

n
(cid:88)

i=1

(cid:12)
(cid:16)
(cid:12)
(cid:12)

(1 − a4i

1 )(1 − a2(n−i)

1

) − 1

(cid:17)(cid:12)
(cid:12)
(cid:12)

(cid:32)

3
n

n
(cid:88)

i=0

(cid:33)

a2i
1

64a2
1

(1 − a4

64a2
1

(1 − a4

64a2
1

(1 − a4

σ4
δ

σ4
δ

∞
(cid:80)
δ=1
1)(1 − a2
1)
∞
(cid:80)
δ=1
1)(1 − a2
1)
∞
(cid:80)
δ=1
1)(1 − a2

σ4
δ

3
n

.

1)2

=

(cid:54)

(cid:54)

(cid:54)

Consequently

(cid:12)
(cid:12)
(cid:12)
(cid:12)
E
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:104)(cid:0)√

(cid:1)2(cid:105)

−

nT2,n

32

∞
(cid:80)
δ=1
(1 − a2

σ4
δ

1)2

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:54)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

32
n

n
(cid:88)

i=1

(cid:107)fi ⊗1 fi(cid:107)2

L2([0,1]2) −

32

σ4
δ

∞
(cid:80)
δ=1
(1 − a4
1)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

+

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

64
n

n−1
(cid:88)

n
(cid:88)

i=1

j=i+1

(cid:104)fi ⊗1 fi, fj ⊗1 fj(cid:105)L2([0,1]2) −

64a2
1

σ4
δ

∞
(cid:80)
δ=1
1)(1 − a2
1)

(1 − a4

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

σ4
δ

1)2

32

∞
(cid:80)
δ=1
(1 − a4
∞
(cid:88)

σ4
δ

σ4
δ

64a2
1

∞
(cid:80)
δ=1
1)(1 − a2
(1 − a4
1)(cid:3)
1(5 + 6a2
1
1)2(1 − a2
n
1)

.

+

1
n
(cid:2)1 + a2
(1 − a4

(cid:54)

(cid:54) 32

3
n

1)2

The desired is therefore obtained.

δ=1

14

4.2 Scale constant for T4,n

Proposition 4 Under Assumption (9), with T4,n as in (14), for large n,

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

√

E (cid:2)(

nT4,n)2(cid:3) −

4
(1 − a2

1)2





∞
(cid:88)

δ=1

σ4
δ +

(cid:18) 1 + a2
1
1 − a2
1

(cid:19) (cid:32) ∞
(cid:88)

δ=1

σ2
δ

(cid:12)
(cid:33)2
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:54) C2
n

,

where

C2 := C2,1 + C2,2, C2,1 := 4

(cid:33)2

(cid:32) ∞
(cid:88)

δ=1

σ2
δ

(3 + 17a2
1)
(1 − a2
1)4

, C2,2 := 4

(cid:32) ∞
(cid:88)

δ=1

σ4
δ

(cid:33) (cid:2)1 + a2
(1 − a4

1)(cid:3)
1(5 + 6a2
1)2(1 − a2
1)
(18)

.

In particular

l2 := lim
n→∞

√

E (cid:2)(

nT4,n)2(cid:3) =

4
(1 − a2

1)2





∞
(cid:88)

δ=1

σ4
δ +

(cid:18) 1 + a2
1
1 − a2
1

(cid:19) (cid:32) ∞
(cid:88)

(cid:33)2
 .

σ2
δ

δ=1

(19)

Proof. By deﬁnition of the term T4,n, we have

√

E[(

nT4,n)2] = 4!(cid:107)

1
√
n

n
(cid:88)

n
(cid:88)

i=1

fi ˜⊗fi(cid:107)2

L2([0,1]4)

(cid:10)fi ˜⊗fi, fj ˜⊗fj

(cid:11)
L2([0,1]4) ,

=

4!
n

i,j=1

where fi ˜⊗fi denotes the symmetrization of fi ⊗ fi, because the kernel (cid:80)n
no longer symmetric. We deal with symmetrization by using a combinatorial formula, obtaining
(cid:11)
L2([0,1]4) = (2!)2 (cid:104)fi ⊗ fi, fj ⊗ fj(cid:105)L2([0,1]4) + (2!)2 (cid:104)fi ⊗1 fj, fj ⊗1 fi(cid:105)L2([0,1]2) .

4! (cid:10)fi ˜⊗fi, fj ˜⊗fj

i=1 fi⊗fi ∈ L2([0, 1]4) is

Therefore

√

E[(

nT4,n)2] =

4
n

n
(cid:88)

i,j=1

(cid:104)fi ⊗ fi, fj ⊗ fj(cid:105)L2([0,1]4) +

4
n

n
(cid:88)

i,j=1

(cid:104)fi ⊗1 fj, fj ⊗1 fi(cid:105)L2([0,1]2)

=: T4,1,n + T4,2,n.

Moreover,

T4,1,n =

=

=

4
n

4
n

4
n

n
(cid:88)

(cid:104)fi ⊗ fi, fj ⊗ fj(cid:105)L2([0,1]4)

i,j=1
n
(cid:88)

(cid:16)

(cid:104)fi, fj(cid:105)L2([0,1]2)

(cid:17)2

i,j=1

n
(cid:88)

(cid:16)

i=1

(cid:104)fi, fi(cid:105)L2([0,1]2)

(cid:17)2

+

8
n

n−1
(cid:88)

n
(cid:88)

(cid:16)

i=1

j=i+1

(cid:104)fi, fj(cid:105)L2([0,1]2)

(cid:17)2

.

(20)

15

On the other hand, using (9), we have for j (cid:62) i

(cid:104)fi, fj(cid:105)L2([0,1]2) =

i
(cid:88)

j
(cid:88)

aj−k
1

ai−k1
1

∞
(cid:88)

k1=1

k2=1

δ1,δ2=1

σδ1σδ2δδ1,δ2δk1,k2

= a(j−i)
1

×

(cid:32) ∞
(cid:88)

(cid:33)

σ2
δ

×

δ=1

(cid:18) 1 − a2i
1
1 − a2
1

(cid:19)

.

Therefore by (20), we get

(cid:12)
(cid:12)
(cid:12)
T4,1,n −
(cid:12)
(cid:12)
(cid:12)

4(1 + a2
1)
(1 − a2
1)3

(cid:32) ∞
(cid:88)

δ=1

σ2
δ

(cid:33)2(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:54)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

4
n

n
(cid:88)

i=1

+

4
(1 − a2

1)2

(cid:32) ∞
(cid:88)

δ=1

σ2
δ

(cid:33)2(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:104)fi, fj(cid:105)L2([0,1]2)

(cid:17)2

−

8a2
1
(1 − a2

1)3

(cid:32) ∞
(cid:88)

δ=1

σ2
δ

(cid:33)2(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

n
(cid:88)

((1 − a2i

(cid:12)
(cid:12)
1 )2 − 1)
(cid:12)
(cid:12)
(cid:12)

i=1
(cid:33)2 (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
n

n
(cid:88)

(cid:16)

i=1

(1 − a2i

1 )2(1 − a2(n−i)

1

(cid:17)

) − 1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

+

80a2
1
(1 − a2
1)4

(cid:33)2

(cid:32) ∞
(cid:88)

δ=1

σ2
δ

1
n

(cid:107)fi(cid:107)4

L2([0,1]2) −

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

8
n

n−1
(cid:88)

n
(cid:88)

(cid:16)

j=i+1

i=1
(cid:32) ∞
(cid:88)

δ=1

σ2
δ

(cid:33)2 (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
n

(cid:32) ∞
(cid:88)

σ2
δ

δ=1
(cid:33)2

1
n

σ2
δ

(cid:54)

4
(1 − a2

1)2

+

8a2
1
(1 − a2

1)3
(cid:32) ∞
(cid:88)

δ=1

(cid:19)2

(cid:54)

=

12
(1 − a2

1)3

4

σ2
δ

(cid:18) ∞
(cid:80)
δ=1
(1 − a2

1)4

(3 + 17a2
1)
n

.

Now let us estimate

T4,2,n =

4
n

n
(cid:88)

i,j=1

(cid:104)fi ⊗1 fj, fj ⊗1 fi(cid:105)L2([0,1]2) .

For j (cid:62) i, x, y ∈ L2([0, 1]), we have

(fi ⊗1 fj)(x, y) = a(j−i)

1

i
(cid:88)

k=1

a2(i−k)
1

∞
(cid:88)

δ=1

δ (h⊗2
σ2

k,δ)(x, y).

(21)

So, for j (cid:62) i,

(cid:104)fi ⊗1 fj, fj ⊗1 fi(cid:105)L2([0,1]2) =

(cid:90)

[0,1]2

(fi ⊗1 fj)2(x, y)dxdy = a2(j−i)

1

(cid:32) ∞
(cid:88)

(cid:33)

σ4
δ

×

δ=1

(cid:18) 1 − a4i
1
1 − a4
1

(cid:19)

.

16

Therefore,
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

T4,2,n −

σ4
δ

4

∞
(cid:80)
δ=1
(1 − a2

1)2

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

=

(cid:54)

(cid:54)

=

σ4
δ

∞
(cid:80)
δ=1
(1 − a4
1)

1
n

n
(cid:88)

(1 − a4i

1 ) +

i=1

1
n

n
(cid:88)

i=1

a4i
1

4

σ4
δ

∞
(cid:80)
δ=1
(1 − a4
1)

4

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
−
(cid:12)
(cid:12)
(cid:12)
(cid:12)

σ4
δ

∞
(cid:80)
δ=1
(1 − a4
1)

8
n

n−1
(cid:88)

(1 − a4i
1 )

n
(cid:88)

i=1

j=i+1

a2(j−i)
1

−

σ4
δ

4

∞
(cid:80)
δ=1
(1 − a2

1)2

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

δ a2
σ4
1

∞
(cid:80)
δ=1
1)(1 − a2
(1 − a4
1)

1
n

n
(cid:88)

(cid:16)

i=1

(1 − a4i

1 )(1 − a2(n−i)

1

(cid:17)

) − 1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

8

+

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
∞
(cid:80)
δ=1
1)(1 − a2

σ4
δ

1
n

1)2

σ4
δ

4

∞
(cid:80)
δ=1
(1 − a4

24a2
1

(1 − a4

1
n

+

1)2
∞
(cid:80)
δ=1
1)2(1 − a2
1)

σ4
δ

4

(1 − a4

(cid:2)1 + a2

1(5 + 6a2

1)(cid:3)

n

,

which completes the proof.

To get a sense of how the two terms T2,n and T4,n compare to each other, we propose the
following example, which shows that, despite one’s best eﬀorts, one should not expect either of these
two terms to dominate the other.

Remark 5 In the AR(1) model (8) with chi-squared white noise, i.e. when σ1 = σ and σδ = 0 for
all δ (cid:62) 2, one can try to compare the two formulas for the asymptotic variances of T2,n and T4,n.
Avoiding the situation where |a1| is very close to 1, assuming for instance |a1| < 2−1/2, so that
1 − a2

1 > 1/2, when n is large, we have

V ar(T2,n) ∼

1
n

32σ4
(1 − a2

1)2 ∼ 4 × (1 − a2

1) × V ar(T4,n) > 4 ×

1
2

× V ar(T4,n) = 2 × V ar(T4,n).

Therefore the sequence T4,n can be made to have a variance which is signiﬁcantly smaller that the
one of T2,n in this case, but both of them converge to zero at the same speed n−1.

Using the orthogonality between T2,n and T4,n, Proposition 3 and Proposition 4, we conclude

the following.

Theorem 6 Under Assumption (9), with Qn as in (13), for large n,
|n V ar(Qn) − (l1 + l2)| (cid:54) (C1 + C2)

,

n

and in particular the asymptotic variance of Qn is

lim
n→∞

n V ar(Qn) = l1 + l2

=

36
(1 − a2

1)2

(cid:32) ∞
(cid:88)

(cid:33)

σ4
δ

+

δ=1

4(1 + a2
1)
(1 − a2
1)3

(cid:33)2

(cid:32) ∞
(cid:88)

δ=1

σ2
δ

17

where C1, l1, and C2, l2 are given respectively in (15), (16), (18) and (19).

Remark 7

• From the previous theorem, we notice that for n large, and ﬁxed values of the noise scale
nQn has high values when |a1| is close to 1,

√

parameter family {σδ, δ (cid:62) 1}, the variance of
and approaches

when |a1| is small.

δ=1

(cid:32) ∞
(cid:88)

(cid:33)

σ4
δ

+ 4

36

(cid:33)2

(cid:32) ∞
(cid:88)

δ=1

σ2
δ

• The previous theorem also shows that one can obtain other asymptotics depending on the
relation between a1 and the family {σδ, δ (cid:62) 1}. For instance, when |a1| is close to 1, which
is the limit of fast mean reversion, one can avoid an explosion of Qn’s asymptotic variance
by scaling the variance parameters appropriately, leading to a fast-mean reversion and small
noise regime. Letting 1/α := 1 − a2
1, where α is interpreted as a rate of mean reversion, one
δ = O (cid:0)α−3/2(cid:1). In the example where
δ = O (cid:0)α−2(cid:1) and (cid:80) σ2
would only need to ensure that (cid:80) σ4
there is a single non-zero value σ, for instance, we would obtain for large α,

n × V ar(Qn) ∼ 36α2σ4 + 8α3σ4;

here the second term dominates, and as α → ∞, assuming α3σ4 remains bounded, we would
get an asymptotic variance of 8 limα→∞ α3σ4 if the limit exists.

5 Berry-Esséen bound for the asymptotic normality of the quadratic-

variation

In this section, we prove that the quadratic variation deﬁned in (13) is asymptotically normal and
we estimate the speed of this convergence in total variation distance, showing it is of the Berry-
Esséen-type order n−1/2. For this aim, we will need the following theorem, which estimates the total
variation distance to the normal of the standardized sum of variables in the 2nd and 4th chaos.

Theorem 8 Let F = I2(f ) + I4(g) where f ∈ L2

s([0, 1]2) and g ∈ L2

s([0, 1]4). Then

dT V

(cid:18) F
√

EF 2

(cid:19)

, N (0, 1)

(cid:54)

(cid:104)√

√

4
EF 2

+36
√

+3

2 (cid:107)f ⊗1 f (cid:107)L2([0,1]2) + 2
√

2 (cid:107)g ⊗3 g(cid:107)L2([0,1]2) + 9
(cid:113)

√

6! (cid:107)g ⊗1 g(cid:107)L2([0,1]6) + 18
(cid:113)
2

(cid:104)f ⊗ f, g ⊗2 g(cid:105)L2([0,1]4)

4!

(cid:107)f ⊗1 f (cid:107)L2([0,1]2) (cid:107)g ⊗3 g(cid:107)L2([0,1]2)

(cid:105)

.

(22)

√

4! (cid:107)g ⊗2 g(cid:107)L2([0,1]4)

Moreover, letting RF be the bracketed term on the right-hand side of (22), for any constant σ > 0,
we have

dT V

, N (0, 1)

(cid:18) F
σ

(cid:19)

(cid:54) 4

σ2 RF + 2

1 −

(cid:12)
(cid:12)
(cid:12)
(cid:12)

EF 2
σ2

(cid:12)
(cid:12)
(cid:12)
(cid:12)

.

(23)

18

Proof. We have F = I2(f ) + I4(g). Then

−DtL−1F = I1(f (., t)) + I3(g(., t)) =: u(t).

Thus, using F = δ(−DL−1)F , we can write F = δ(u). Now we use the result of a simple calculation,
labeled as (9) in the preprint [33] (see also [37]), to obtain

(cid:19)

(cid:18) F
√

(cid:113)

(cid:54)

dT V

EF 2

, N (0, 1)

V ar (cid:0)(cid:104)DF, u(cid:105)L2([0,1])

2
EF 2
2
EF 2
where the last equality comes from the duality relation E(cid:104)DF, u(cid:105)L2([0,1]) = E(F δ(u)) = EF 2. The
prior inequality appears to be used in a more general context here than what is stated in [33,
Eq. (9)], but an immediate inspection of its proof therein shows that it applies to any situation
where F = δ(u), using only general results such as Stein’s lemma, the chain rule for the Malliavin
derivative D, and the duality between D and δ.

E (cid:0)(cid:104)DF, u(cid:105)L2([0,1]) − EF 2(cid:1)2,

(24)

(cid:113)

=

(cid:1)

On the other hand, using the product formula (4),

(cid:104)DF, u(cid:105)L2([0,1]) =

=

=

=

(cid:90) 1

0
(cid:90) 1

0
(cid:90) 1

0
(cid:90) 1

u(t)DtF dt

u(t)(2I1(f (., t)) + 4I3(g(., t)))dt

2 [I1(f (., t))]2 + 4 [I3(g(., t))]2 + 6I1(f (., t))I3(g(., t)) dt

(cid:104)
2I2(f (., t) ⊗ f (., t)) + 2(cid:107)f (., t)(cid:107)2

L2([0,1]) + 4I6(g(., t) ⊗ g(., t))

0
+36I4(g(., t) ⊗1 g(., t)) + 72I2(g(., t) ⊗2 g(., t)) + 24g(., t) ⊗3 g(., t)
+6I4(f (., t) ⊗ g(., t)) + 18I2(f (., t) ⊗1 g(., t))] dt.

Thus

where

(cid:104)DF, u(cid:105)L2([0,1]) − EF 2 =

(cid:90) 1

0

I2(h1(., t)) + I4(h2(., t)) + I6(h3(., t)) dt,

h1(.t) = 2f (., t) ⊗ f (., t) + 72g(., t) ⊗2 g(., t) + 18f (., t) ⊗1 g(., t)
h2(.t) = 36g(., t) ⊗1 g(., t) + 6f (., t) ⊗ g(., t)
h3(.t) = 4g(., t) ⊗ g(., t).

Therefore, using Minkowski inequality,

(cid:113)

E (cid:0)(cid:104)DF, u(cid:105)L2([0,1]) − EF 2(cid:1)2 (cid:54)

h1(., t) dt

(cid:13)
(cid:13)
(cid:13)
(cid:13)L2([0,1]2)

√

4!

+

(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:90) 1

0

h2(., t) dt

(cid:13)
(cid:13)
(cid:13)
(cid:13)L2([0,1]4)

√

(cid:90) 1

2

(cid:13)
(cid:13)
(cid:13)
(cid:13)
√

(cid:90) 1

0
(cid:13)
(cid:13)
(cid:13)
(cid:13)

0

19

+

6!

h3(., t) dt

(cid:13)
(cid:13)
(cid:13)
(cid:13)L2([0,1]6)

.

Furthermore,

(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:90) 1

0

h1(., t) dt

(cid:13)
(cid:13)
(cid:13)
(cid:13)L2([0,1]2)

(cid:54)

(cid:13)
(cid:13)
2
(cid:13)
(cid:13)

(cid:90) 1

0

f (., t) ⊗ f (., t) dt

+

(cid:13)
(cid:13)
(cid:13)
(cid:13)

18

(cid:90) 1

0

f (., t) ⊗1 g(., t) dt

+

(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)
(cid:13)L2([0,1]2)
(cid:13)
(cid:13)
(cid:13)
(cid:13)L2([0,1]2)

72

(cid:90) 1

0

g(., t) ⊗2 g(., t) dt

(cid:13)
(cid:13)
(cid:13)
(cid:13)L2([0,1]2)

= 2 (cid:107)f ⊗1 f (cid:107)L2([0,1]2) + 72 (cid:107)g ⊗3 g(cid:107)L2([0,1]2) + 18

(cid:113)

(cid:104)f ⊗ f, g ⊗2 g(cid:105)L2([0,1]4).

Also,

(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:90) 1

0

h2(., t) dt

(cid:13)
(cid:13)
(cid:13)
(cid:13)L2([0,1]4)

(cid:54)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

36

(cid:90) 1

0

g(., t) ⊗1 g(., t) dt

(cid:13)
(cid:13)
(cid:13)
(cid:13)L2([0,1]2)

+

(cid:13)
(cid:13)
6
(cid:13)
(cid:13)

(cid:90) 1

0

f (., t) ⊗ g(., t) dt

(cid:13)
(cid:13)
(cid:13)
(cid:13)L2([0,1]4)

= 36 (cid:107)g ⊗2 g(cid:107)L2([0,1]4) + 6

(cid:113)

(cid:104)f ⊗1 f, g ⊗3 g(cid:105)L2([0,1]2)

(cid:54) 36 (cid:107)g ⊗2 g(cid:107)L2([0,1]4) + 6

(cid:113)

(cid:107)f ⊗1 f (cid:107)L2([0,1]2) (cid:107)g ⊗3 g(cid:107)L2([0,1]2),

and

(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:90) 1

0

h3(., t) dt

(cid:13)
(cid:13)
(cid:13)
(cid:13)L2([0,1]6)

=

(cid:13)
(cid:13)
4
(cid:13)
(cid:13)

(cid:90) 1

0

g(., t) ⊗ g(., t) dt

(cid:13)
(cid:13)
(cid:13)
(cid:13)L2([0,1]6)

= 4 (cid:107)g ⊗1 g(cid:107)L2([0,1]6) .

As a consequence,

(cid:113)

E (cid:0)(cid:104)DF, u(cid:105)L2([0,1]) − EF 2(cid:1)2 (cid:54) 2

√

√

2 (cid:107)f ⊗1 f (cid:107)L2([0,1]2) + 72
√

(cid:113)

2 (cid:107)g ⊗3 g(cid:107)L2([0,1]2)
√

2

(cid:104)f ⊗ f, g ⊗2 g(cid:105)L2([0,1]4) + 36

4! (cid:107)g ⊗2 g(cid:107)L2([0,1]4)
√

+18
√

+6

(cid:113)

4!

(cid:107)f ⊗1 f (cid:107)L2([0,1]2) (cid:107)g ⊗3 g(cid:107)L2([0,1]2) + 4

6! (cid:107)g ⊗1 g(cid:107)L2([0,1]6) .

This, combined with (24), establishes inequality (22).

For (23), we have by (6)

dT V

(cid:18) F
σ

(cid:19)

, N (0, 1)

(cid:54) 2

σ2 E[|σ2 − (cid:104)DF, u(cid:105)L2([0,1]) |]

(cid:54) 2

σ2 E[|E[F 2] − (cid:104)DF, u(cid:105)L2([0,1]) |] + 2

(cid:12)
(cid:12)
(cid:12)
(cid:12)

1 −

E[F 2]
σ2

(cid:12)
(cid:12)
(cid:12)
(cid:12)

.

(25)

Inequality (23) follows using inequalities (25) and (22).

We will now use Theorem 8 to prove that the quadratic variation Qn satisﬁes the following

Berry-Esséen theorem.

20

Remark 9 It turns out that, when applying Theorem 8 to estimate the speed of convergence in
(cid:104)f ⊗ f, g ⊗2 g(cid:105)L2([0,1]4) cannot merely be bounded above via Schwarz’s
the CLT for Qn, the term
inequality. See Lemma 13 below and its proof. This is the key element which allows us to obtain the
Berry-Esséen speed n−1/2 in the next theorem.

(cid:113)

Theorem 10 With Qn deﬁned in (13), under Assumption (9), we have for all n (cid:62) 1

(cid:18) √

dT V

n(Qn − E[Qn])
l1 + l2

√

(cid:19)

, N (0, 1)

(cid:54) C0√
n

,

where

C0 :=

√

2
4
l1 + l2

(cid:18) 1
√
2

2

(C1 + C2) + C3 + 36C4,3 + 9C5 + 36

√

√

3C4,2 + 6

(26)

√

(cid:19)

10C4,1

,

3C1/2

3 C1/2

4,3 + 12

where l1, l2, are deﬁned in the previous section in (16), (19), and C1, C2, C3, C4,r, r = 1, 2, 3 and
C5 are given in the lemmas below, respectively in (15), (18), (29), (30) and (34).

In particular

√

n(Qn − E[Qn]) is asymptotically Gaussian, namely

√

lim
n→∞

n (Qn − E[Qn]) law−→ N (0, l1 + l2).

Proof. Based on the decomposition of (Qn − E(Qn)) given in (14), we have

√

n (Qn − E[Qn]) =

√

nT2,n +

√

nT4,n = I W

2 (g2,n) + I W

4 (g4,n),

where

g2,n :=

4
√
n

n
(cid:88)

i=1

fi ⊗1 fi and g4,n :=

1
√
n

n
(cid:88)

i=1

fi ⊗ fi.

(27)

√

n (Qn − E[Qn]), we get

Applying Theorem 8 to
(cid:18) √

dT V

n(Qn − E[Qn])
l1 + l2

√

(cid:19)

, N (0, 1)

2 (cid:107)g2,n ⊗1 g2,n(cid:107)L2([0,1]2) + 36

√

2 (cid:107)g4,n ⊗3 g4,n(cid:107)L2([0,1]2)

(cid:104)√

(cid:54) 4
l1 + l2
√

+ 9

(cid:113)

2
√

(cid:104)g2,n ⊗ g2,n, g4,n ⊗2 g4,n(cid:105)L2([0,1]4)
(cid:113)

√

4! (cid:107)g4,n ⊗2 g4,n(cid:107)L2([0,1]4) + 3

4!

+18

√

+2

6! (cid:107)g4,n ⊗1 g4,n(cid:107)L2([0,1]6)

(cid:105)

+ 2

(cid:12)
(cid:12)
(cid:12)
1 −
(cid:12)
(cid:12)
(cid:12)

(cid:107)g2,n ⊗1 g2,n(cid:107)L2([0,1]2) (cid:107)g4,n ⊗3 g4,n(cid:107)L2([0,1]2)
(cid:104)

√
(

n (Qn − E[Qn]))2(cid:105)

E

(l1 + l2)

.

(28)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

We study ﬁrst the contractions of the kernels g2,n and g4,n given in (27) and we prove that they
satisfy the following lemmas.

21

Lemma 11 If Assumption (9) holds, the kernel g2,n deﬁned in (27) satisﬁes

(cid:107)g2,n ⊗1 g2,n(cid:107)L2([0,1]2) (cid:54) C3√
n

,

(29)

where C3 :=

(cid:114)

4! 44
n

(cid:0)(cid:80)∞

δ=1 σ8
δ

(cid:1)

1
1−a8
1

(cid:16) 1

1−a2
1

(cid:17)3

.

Proof. We have

(cid:107)g2,n ⊗1 g2,n(cid:107)2

L2([0,1]2) =

44
n2 (cid:107)

n
(cid:88)

(fi ⊗1 fi) ⊗1 (fj ⊗1 fj)(cid:107)2

L2([0,1]2)

i,j=1
n
(cid:88)

(cid:104)(fi ⊗1 fi) ⊗1 (fj ⊗1 fj), (fk ⊗1 fk) ⊗1 (fl ⊗1 fl)(cid:105)L2([0,1]2) .

=

44
n2

i,j,k,l=1

Moreover, by above calculations and (9)

(fi ⊗1 fi)(x, y) =

i
(cid:88)

k=1

a2(i−k)
1

∞
(cid:88)

δ=1

δ h⊗2
σ2

k,δ(x, y).

Hence

(fi ⊗1 fi) ⊗1 (fj ⊗1 fj)(x, y)

(cid:90)

=

[0,1]

(fi ⊗1 fi)(x, t)(fj ⊗1 fj)(y, t)dt

=

=

=

i
(cid:88)

k1=1

i
(cid:88)

k1=1
i∧j
(cid:88)

k1=1

a2(i−k1)
1

a2(i−k1)
1

j
(cid:88)

k2=1
j
(cid:88)

k2=1

a2(j−k2)
1

∞
(cid:88)

δ1,δ2=1

δ1σ2
σ2
δ2

(cid:90)

[0,1]

hk1,δ1(x)hk1,δ1(t)hk2,δ2(y)hk2,δ2(t)dt

a2(j−k2)
1

∞
(cid:88)

δ=1

σ4
δ (hk1,δ ⊗ hk2,δ)(x, y)δk1,k2

a2(i+j−2k1)
1

(cid:32)+∞
(cid:88)

(cid:33)

σ4
δ

δ=1

(hk1,δ ⊗ hk1,δ)(x, y).

Similarly

(fk ⊗1 fk) ⊗1 (fl ⊗1 fl)(x, y) =

k∧l
(cid:88)

k2=1

a2(k+l−2k2)
1

(cid:32)+∞
(cid:88)

(cid:33)

σ4
δ

δ=1

(hk2,δ ⊗ hk2,δ)(x, y).

22

Therefore

(cid:104)(fi ⊗1 fi) ⊗1 (fj ⊗1 fj), (fk ⊗1 fk) ⊗1 (fl ⊗1 fl)(cid:105)L2([0,1]2)

(cid:90)

=

[0,1]2

((fi ⊗1 fi) ⊗1 (fj ⊗1 fj)) (x, y) ((fk ⊗1 fk) ⊗1 (fl ⊗1 fl)) (x, y)dxdy

i∧j∧k∧l
(cid:88)

=

m=1

a2(i+j+k+l−4m)
1

(cid:32) ∞
(cid:88)

(cid:33)

σ8
δ

.

δ=1

Consequently

(cid:107)g2,n ⊗1 g2,n(cid:107)2

L2([0,1]2) =

44
n2

n
(cid:88)

i∧j∧k∧l
(cid:88)

a2(i+j+k+l−4r)
1

∞
(cid:88)

δ=1

σ8
δ

i,j,k,l=1
(cid:32) ∞
(cid:88)

(cid:54) 4!

(cid:54) 4!

(cid:54) 4!

(cid:54) 4!

44
n2

44
n2

44
n2

44
n

δ=1
(cid:32) ∞
(cid:88)

δ=1
(cid:32) ∞
(cid:88)

δ=1
(cid:32) ∞
(cid:88)

δ=1

r=1
(cid:33)

σ8
δ

σ8
δ

σ8
δ

(cid:33)

(cid:33)

(cid:33)

σ8
δ

(cid:88)

i
(cid:88)

1(cid:54)i(cid:54)j(cid:54)k(cid:54)l(cid:54)n

r=1

a2(i+j+k+l−4r)
1

(cid:88)

a2(j+k+l−3i)
1

1(cid:54)i(cid:54)j(cid:54)k(cid:54)l(cid:54)n

1 − a8i
1
1 − a8
1

1
1 − a8
1

(cid:88)

a2(j+k+l−3i)
1

1(cid:54)i(cid:54)j(cid:54)k(cid:54)l(cid:54)n

1
1 − a8
1

n
(cid:88)

a2(k1+k2+k3)
1

= 4!

4
n

(cid:54) 4!

44
n

(cid:32) ∞
(cid:88)

(cid:33)

σ8
δ

1
1 − a8
1



δ=1
(cid:32) ∞
(cid:88)

δ=1

(cid:33)

σ8
δ

1
1 − a8
1

k1,k2,k3=0



3

n
(cid:88)

a2k1
1



k1=0
(cid:18) 1

1 − a2
1

(cid:19)3

,

where we used the change of variables k1 = j − i, k2 = k − i, k3 = l − i. The desired result therefore
follows.

Lemma 12 If Assumption (9) holds, for every r = 1, 2, 3, the kernel g4,n deﬁned in (27) satisﬁes

(cid:107)g4,n⊗rg4,n(cid:107)L2([0,1](8−2r))

(cid:54) C4,r√
n

,

(30)

23

where

C4,r :=






(cid:115)

4!

(cid:18) ∞
(cid:80)
δ=1

σ2
δ

(cid:19)2 (cid:18) ∞
(cid:80)
δ=1

σ4
δ

(cid:19) (cid:16)

(cid:17)3

(1−a2

1
1)(1−a4
1)

(cid:115)

4!

(cid:18) ∞
(cid:80)
δ=1

(cid:19)4 (cid:16) 1
1−a2
1

σ2
δ

(cid:17)7

if

r = 1,

if

r = 2,

(cid:115)

4!

(cid:18) ∞
(cid:80)
δ=1

σ2
δ

(cid:19)2 (cid:18) ∞
(cid:80)
δ=1

(cid:19)

σ4
δ

1
(1−a4

1)2

1
(1−a2

1)4

if

r = 3.

Proof. For r = 1, 2, 3, we have

(cid:107)g4,n⊗rg4,n(cid:107)2

L2([0,1]2(4−r)) =

1
n2 (cid:107)

(cid:54) 1

n2 (cid:107)

=

1
n2

For r = 1, we get

i,j,k,l=1

n
(cid:88)

(fi ⊗ fi) ˜⊗r(fj ⊗ fj)(cid:107)2

L2([0,1]2(4−r))

i,j=1
n
(cid:88)

(fi ⊗ fi)⊗r(fj ⊗ fj)(cid:107)2

L2([0,1]2(4−r))

i,j=1
n
(cid:88)

(cid:104)(fi ⊗ fi)⊗r(fj ⊗ fj), (fk ⊗ fk)⊗r(fl ⊗ fl)(cid:105)L2([0,1]2(4−r)) .

(cid:107)g4,n⊗1g4,n(cid:107)2

L2([0,1]6)

(cid:54) 1
n2

=

=

1
n2

4!
n2

n
(cid:88)

i,j,k,l=1
n
(cid:88)

(cid:104)(fi ⊗ fi)⊗1(fj ⊗ fj), (fk ⊗ fk)⊗1(fl ⊗ fl)(cid:105)L2([0,1]6)

(cid:104)fi, fk(cid:105)L2([0,1]2) (cid:104)fj, fl(cid:105)L2([0,1]2) (cid:104)fi ⊗1 fj, fk ⊗1 fl(cid:105)L2([0,1]2)

i,j,k,l=1
(cid:88)

1(cid:54)i(cid:54)j(cid:54)k(cid:54)l(cid:54)n

(cid:104)fi, fk(cid:105)L2([0,1]2) (cid:104)fj, fl(cid:105)L2([0,1]2) (cid:104)fi ⊗1 fj, fk ⊗1 fl(cid:105)L2([0,1]2) .

By (9), for all 1 (cid:54) i, k (cid:54) n,

(cid:104)fi, fk(cid:105)L2([0,1]2) =

Similarly, for all 1 (cid:54) j, l (cid:54) n,

(cid:104)fj, fl(cid:105)L2([0,1]2) =

i∧k
(cid:88)

m=1

i∧k
(cid:88)

m=1

ai+k−2m
1

×

(cid:32) ∞
(cid:88)

(cid:33)

σ2
δ

.

δ=1

aj+l−2m
1

×

(cid:32) ∞
(cid:88)

(cid:33)

σ2
δ

.

δ=1

24

(31)

(32)

On the other hand, for all 1 (cid:54) i, j (cid:54) n

(fi ⊗1 fj)(x, y) =

=

(cid:90) 1

0
i∧j
(cid:88)

Hence, by (9), for all 1 (cid:54) i, j, k, l (cid:54) n,

m=1

fi(x, t)fj(y, t)dt

ai+j−2m
1

∞
(cid:88)

δ=1

δ h⊗2
σ2

m,δ(x, y).

(33)

(cid:104)fi ⊗1 fj, fk ⊗1 fl(cid:105)L2([0,1]2) =

i∧j∧k∧l
(cid:88)

m=1

a(i+j+l+k−4m)
1

×

(cid:32) ∞
(cid:88)

(cid:33)

σ4
δ

.

δ=1

Therefore, from (31) and above calculations, we have

1
(1 − a4
1)

1
n2

(cid:88)

a2(k−i)
1

a2(l−i)
1

1(cid:54)i(cid:54)j(cid:54)k(cid:54)l(cid:54)n

(1 − a2i

1 )(1 − a4i

1 )(1 − a2j
1 )

(cid:107)g4,n⊗1g4,n(cid:107)2

(cid:54)

(cid:54)

4!
(1 − a2

1)2

4!
(1 − a2

1)2

=

4!
(1 − a2

1)2

(cid:32) ∞
(cid:88)

(cid:54) 4!

σ2
δ

L2([0,1]6)
(cid:32) ∞
(cid:88)

σ2
δ

δ=1
(cid:32) ∞
(cid:88)

σ2
δ

(cid:33)2 (cid:32) ∞
(cid:88)

δ=1
(cid:33)2 (cid:32) ∞
(cid:88)

(cid:33)

(cid:33)

σ4
δ

σ4
δ

δ=1

δ=1

(cid:32) ∞
(cid:88)

(cid:33)2 (cid:32) ∞
(cid:88)

σ2
δ

1
(1 − a4
1)
(cid:33)2 (cid:32) ∞
(cid:88)

δ=1
(cid:33) (cid:18)

σ4
δ

δ=1

δ=1

1
(1 − a4
1)
(cid:33)

σ4
δ

δ=1

1
n

1
n

n
(cid:88)

a2(2k1+2k2+k3)
1

k1,k2,k3=0
(cid:32) n
(cid:88)

a4r1
1

r1=0

(cid:33)2 (cid:32) n
(cid:88)

(cid:33)

a2r2
1

r2=0

1
1)(1 − a4
1)

(1 − a2

(cid:19)3

×

1
n

,

where we used the change of variables k1 = j − i, k2 = k − j, k3 = l − k.

For r = 2, we have

(cid:107)g4,n⊗2g4,n(cid:107)2

L2([0,1]4)

(cid:54) 1
n2

=

=

1
n2

4!
n2

n
(cid:88)

i,j,k,l=1
n
(cid:88)

(cid:104)(fi ⊗ fi)⊗2(fj ⊗ fj), (fk ⊗ fk)⊗2(fl ⊗ fl)(cid:105)L2([0,1]4)

(cid:104)fi, fj(cid:105)L2([0,1]2) (cid:104)fk, fl(cid:105)L2([0,1]2) (cid:104)fi, fk(cid:105)L2([0,1]2) (cid:104)fj, fl(cid:105)L2([0,1]2)

i,j,k,l=1
(cid:88)

1(cid:54)i(cid:54)j(cid:54)k(cid:54)l(cid:54)n

(cid:104)fi, fj(cid:105)L2([0,1]2) (cid:104)fk, fl(cid:105)L2([0,1]2) (cid:104)fi, fk(cid:105)L2([0,1]2) (cid:104)fj, fl(cid:105)L2([0,1]2) .

25

Hence, by (32) and (9), we get

(cid:107)g4,n⊗2g4,n(cid:107)2

L2([0,1]4)

(cid:54)

4!
(1 − a2

1)4

(cid:54)

4!
(1 − a2

1)4

=

4!
(1 − a2

1)4

(cid:19)4

(cid:18) ∞
(cid:80)
δ=1

σ2
δ

n2

(cid:19)4

(cid:19)4

σ2
δ

(cid:18) ∞
(cid:80)
δ=1
n

σ2
δ

(cid:18) ∞
(cid:80)
δ=1
n

(cid:88)

a2(l−i)
1

1(cid:54)i(cid:54)j(cid:54)k(cid:54)l(cid:54)n

(1 − a2i

1 )2(1 − a2k

1 )(1 − a2j
1 )

n
(cid:88)

k1,k2,k3=0

a2(k1+k2+k3)
1



3

a2k1
1







n
(cid:88)

k1=0

(cid:19)4

σ2
δ

(cid:18) ∞
(cid:80)
δ=1
n

,

(cid:54)

4!
(1 − a2

1)7
where we used the change of variables k1 = j − i, k2 = k − j, k3 = l − k.

For r = 3, we have

(cid:107)g4,n⊗3g4,n(cid:107)2

L2([0,1]2)

(cid:54) 1
n2

1
n2

4!
n2

=

=

(cid:54)

(cid:54)

n
(cid:88)

i,j,k,l=1
n
(cid:88)

(cid:104)(fi ⊗ fi)⊗3(fj ⊗ fj), (fk ⊗ fk)⊗3(fl ⊗ fl)(cid:105)L2([0,1]2)

(cid:104)fi, fj(cid:105)L2([0,1]2) (cid:104)fk, fl(cid:105)L2([0,1]2) (cid:104)fi ⊗1 fj, fk ⊗1 fl(cid:105)L2([0,1]2)

i,j,k,l=1
(cid:88)

(cid:104)fi, fj(cid:105)L2([0,1]2) (cid:104)fk, fl(cid:105)L2([0,1]2) (cid:104)fi ⊗1 fj, fk ⊗1 fl(cid:105)L2([0,1]2)

1(cid:54)i(cid:54)j(cid:54)k(cid:54)l(cid:54)n
(cid:32) ∞
(cid:88)

4!
(1 − a2

1)2

δ=1
(cid:32) ∞
(cid:88)

4!
(1 − a2

1)2

(cid:33)2 (cid:32) ∞
(cid:88)

δ=1
(cid:33)2 (cid:32) ∞
(cid:88)

(cid:33)

(cid:33)

σ4
δ

σ4
δ

σ2
δ

σ2
δ

δ=1

δ=1

(cid:32) ∞
(cid:88)

(cid:33)2 (cid:32) ∞
(cid:88)

σ2
δ

=

4!
(1 − a2

1)2

(cid:32) ∞
(cid:88)

(cid:54) 4!

σ2
δ

1
(1 − a4
1)
(cid:33)2 (cid:32) ∞
(cid:88)

δ=1

δ=1

δ=1
(cid:33)

σ4
δ

1
(1 − a4

1)2

1
(1 − a2

1)4 ×

1
n

1
(1 − a4
1)
(cid:33)

σ4
δ

δ=1

1
n

1
n

n
(cid:88)

a2(2k1+k2+k3)
1

k1,k2,k3=0
(cid:32) n
(cid:88)

a4r1
1

r1=0

(cid:33) (cid:32) n
(cid:88)

(cid:33)2

a2r2
1

r2=0

1
(1 − a4
1)

1
n2

(cid:88)

a2(j−i)
1

a2(l−i)
1

1(cid:54)i(cid:54)j(cid:54)k(cid:54)l(cid:54)n

(1 − a2i

1 )(1 − a4i

1 )(1 − a2k
1 )

where, we used (32) and the change of variables k1 = j − i, k2 = k − j, k3 = l − k, which ends the
proof.

26

Lemma 13 Suppose Assumption (9) holds. Consider the kernels g2,n and g4,n deﬁned in (27), then
we have

(cid:113)

(cid:104)g2,n ⊗ g2,n, g4,n ⊗2 g4,n(cid:105)L2([0,1]4)

,

(34)

(cid:54) C5√
n

where C5 :=

(cid:114)

4! 4
n

(cid:0)(cid:80)∞

δ=1 σ8
δ

(cid:1)

1
1−a8
1

(cid:16) 1

1−a2
1

(cid:17)3

.

Proof. We have

(cid:104)g2,n ⊗ g2,n, g4,n ⊗2 g4,n(cid:105)L2([0,1]4)
4
n2

n
(cid:88)

=

=

i,j,k,l=1
n
(cid:88)

(cid:90)

4
n2

i,j,k,l=1

[0,1]4

(cid:104)(fi ⊗1 fi) ⊗ (fj ⊗1 fj), (fk ⊗ fk) ⊗2 (fl ⊗ fl)(cid:105)L2([0,1]4)

(fi ⊗1 fk)(x1, x3)(fi ⊗1 fk)(x1, x4)(fj ⊗1 fl)(x2, x3)(fj ⊗1 fl)(x2, x4)dx1dx2dx3dx4.

Consequently, using (33), we get

(cid:104)g2,n ⊗ g2,n, g4,n ⊗2 g4,n(cid:105)L2([0,1]4)

(cid:54) 4
n2

n
(cid:88)

i∧j∧k∧l
(cid:88)

a2(i+j+k+l−4r)
1

∞
(cid:88)

δ=1

σ8
δ

i,j,k,l=1
(cid:32) ∞
(cid:88)

4
n2

4
n2

4
n2

δ=1
(cid:32) ∞
(cid:88)

δ=1
(cid:32) ∞
(cid:88)

δ=1

r=1
(cid:33)

σ8
δ

σ8
δ

σ8
δ

(cid:33)

(cid:33)

(cid:88)

i
(cid:88)

1(cid:54)i(cid:54)j(cid:54)k(cid:54)l(cid:54)n

r=1

a2(i+j+k+l−4r)
1

(cid:88)

a2(j+k+l−3i)
1

1(cid:54)i(cid:54)j(cid:54)k(cid:54)l(cid:54)n

1 − a8i
1
1 − a8
1

1
1 − a8
1

(cid:88)

a2(j+k+l−3i)
1

1(cid:54)i(cid:54)j(cid:54)k(cid:54)l(cid:54)n

n
(cid:88)

a2(k1+k2+k3)
1

4
n

4
n

4
n

(cid:33)

σ8
δ

(cid:33)

(cid:33)

σ8
δ

σ8
δ

(cid:32) ∞
(cid:88)

δ=1

(cid:32) ∞
(cid:88)

δ=1
(cid:32) ∞
(cid:88)

δ=1

1
1 − a8
1

1
1 − a8
1

1
1 − a8
1

k1,k2,k3=0


n
(cid:88)



a2k1
1




3

k1=0
(cid:18) 1

1 − a2
1

(cid:19)3

,

(cid:54) 4!

(cid:54) 4!

(cid:54) 4!

(cid:54) 4!

= 4!

(cid:54) 4!

where we used the change of variables k1 = j − i, k2 = k − i, k3 = l − i.

The bound (26) is then a direct consequence of inequality (28) and the estimates given

respectively in (29), (30), (34) and Theorem 6.

27

6 Application: estimation of the mean-reversion parameter

In this section, to illustrate the implications of Theorem 10 in parameter estimation in an easily
tractable case, we consider that we have observations Yn coming from a speciﬁc version of our
second-chaos AR(1) model (8), that which is driven by a chi-squared white noise with one degree
of freedom:




Yn = a0 + a1Yn−1 + σ(Z2

n − 1), n (cid:62) 1

(35)



Y0 = y0 ∈ R.

where a0, a1 and σ are real constants, and {Zn, n (cid:62) 1} are i.i.d. standard normal. This is model
(8) where all σδ’s are zero except for the ﬁrst one.

Proposition 14 The quadratic variation Qn deﬁned in (13) for model (35) satisﬁes, for all n (cid:62) 1,
2σ2
(1 − a2
1)

(cid:12)
(cid:12)
E[Qn] −
(cid:12)
(cid:12)

(cid:54) C6
n

(cid:12)
(cid:12)
(cid:12)
(cid:12)

,

where C6 := 2σ2
(1−a2

1)2 .

Proof. From the deﬁnition of Qn in (13), we have by the isometry property of multiple

integrals

(cid:12)
(cid:12)
E[Qn] −
(cid:12)
(cid:12)

2σ2
(1 − a2
1)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

=

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

2
n

n
(cid:88)

(cid:107)fi(cid:107)2

L2([0,1]2) −

i=1
2σ2
1 − a2
1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

=

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

2σ2
1 − a2
1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

2σ2
1 − a2
1
(cid:32)

1
n

n
(cid:88)

i=1

(1 − a2i

1 ) − 1

(cid:33)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

=

=

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

n
(cid:88)

i
(cid:88)

a2(i−k)
1

−

2σ2
n

i=1
2σ2
1 − a2
1

k=1

1
n

n
(cid:88)

i=1

a2i
1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

−

(cid:54)

2σ2
(1 − a2

1)2

1
n

.

Remark 15 Assuming that σ is known, Proposition (14) shows that the quadratic variation Qn is
an asymptotically unbiased estimator for 2σ2/(1 − a2
1), and thus, after a transformation, for |a1| as
well:

(cid:115)

lim
n→∞

1 −

2σ2
E[Qn]

= |a1| .

Therefore, using the fact that E[Qn] can be estimated via Qn, we suggest the following

moment estimator for the mean-reversion rate |a1|

where

ˆan := f (Qn),

(cid:114)

f (x) :=

1 −

x > 2σ2.

2σ2
x

,

28

(36)

(37)

6.1 Properties of the estimator ˆan

Proposition 16 The estimator ˆan of the mean reversion parameter |a1| deﬁned in (36) is strongly
consistent, namely almost surely

lim
n→∞

ˆan = |a1|.

Proof. We write Qn = Vn√

n + E[Qn], with Vn =

√

n(Qn − E[Qn]). According to Proposition

(6), we have E[V 2

n ] → l1 + l2, n → ∞. Hence, there exists a constant C > 0, such that n (cid:62) 1

(cid:32)

E

(cid:34)(cid:12)
(cid:12)
(cid:12)
(cid:12)

Vn√
n

2(cid:35)(cid:33)1/2
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:54) C
√
n

.

Hence, by Lemma 1 we have almost surely Vn√
, as n → ∞. Thus Qn → 2σ2
14, E[Qn] → 2σ2
1−a2
1−a2
1
1

n → 0, as n → ∞. On the other hand, by Proposition

almost surely as n → ∞, as announced.

Proposition 17 Under Assumption (9), the estimator ˆan deﬁned in (36) satsiﬁes

(cid:18)

dW

√
√

n
l1 + l2

f (cid:48)(µ)

(ˆan − |a1|) , N (0, 1)

(cid:19)

(cid:69) n−1/2.

where l1 and l2 are given in Propositions 3 and 4 respectively and µ = 2σ2
1) .
(1−a2

In particular ˆan is asymptotically Gaussian; more precisely we have as n → +∞

where

√

n (ˆan − |a1|) −→ N (cid:0)0, f (cid:48)(µ)2 × (l1 + l2)(cid:1) ,

f (cid:48)(µ)2 × (l1 + l2) =

(1 − a2

1)(5 − 4a2
1)
2a2
1

.

Proof. For x > 2σ2, the function f deﬁned in (37) has an inverse f −1(y) = 2σ2
1) > 2σ2, for all a1 ∈ (−1, 1). On the other hand, we can write

denote µ = f −1(|a1|) = 2σ2
(1−a2

1−y2 . Let

√

n
l1 + l2

√

(Qn − µ) =

√

√

n
l1 + l2

(Qn − E[Qn]) +

√

√

n
l1 + l2

(E[Qn] − µ)

Therefore, from the properties of the Wasserstein metric and denoting N ∼ N (0, 1), we get

(cid:18) √
√

n
l1 + l2

dW

(Qn − µ) , N

(cid:19)

(cid:54)

(cid:54)

√

n
l1 + l2

√

|E[Qn] − µ| + dW

(cid:18) √
√

n
l1 + l2

(Qn − E[Qn])) , N

(cid:19)

C6√

l1 + l2

n−1/2 + C0n−1/2

(cid:69) n−1/2.

(38)

29

where we used the bound (26) and Proposition 14 for the above bounds. On the other hand, since
the function f deﬁned in (37) is a diﬀeomorphism and since ˆan = f (Qn), then by the mean-value
theorem, there exists a random variable ξn ∈ [|Qn, µ|] such that
(ˆan − |a1|) = f (cid:48)(ξn) (Qn − µ) .

We have
(cid:18)

dW

√
√

n
l1 + l2

f (cid:48)(µ)

(ˆan − |a1|) , N (0, 1)

(cid:19)

(cid:18)

(cid:54) dW

f (cid:48)(µ)

√
√
(cid:18) √
√

n
l1 + l2
n
l1 + l2

(ˆan − |a1|) ,

√

(cid:19)

(Qn − µ)

√

n
l1 + l2
(cid:19)

+ dW

(Qn − µ) , N (0, 1)

According to (38), the last term is bounded by the speed n−1/2. Moreover, applying the mean-
value theorem again since f is twice continuously diﬀerentiable, there exists a random variable δn
∈ [|ξn, µ|] ⊂ [|Qn, µ|], such that

√

n
l1 + l2

√

(cid:19)

(Qn − µ)

|f (cid:48)(µ)|

(cid:112)

l1 + l2

√
√

(cid:18)

dW
√

√

√

√

f (cid:48)(µ)

(ˆan − |a1|) ,

n
l1 + l2
nE[|(Qn − µ)(f (cid:48)(ξn) − f (cid:48)(µ))|]
nE[(cid:12)
(cid:12)(Qn − µ)f (cid:48)(cid:48)(δn)(ξn − µ)(cid:12)
(cid:12)]
nE[(cid:0)Qn − µ)2(cid:1) |f (cid:48)(cid:48)(δn)|]
n (cid:0)E[(Qn − µ)2p](cid:1)1/p (cid:16)

(cid:54)

=
(cid:54)

(cid:54)

E[|f (cid:48)(cid:48)(δn)|p(cid:48)

(cid:17)1/p(cid:48)
]

,

(39)

where we used Hölder’s inequality with p, p(cid:48) are two reals greater than 1 such that 1
p(cid:48) = 1.
Moreover, by the hypercontractivity property for multiple integrals (5), there exists a constant C(p)
such that

p + 1

√

√

n (cid:0)E[(Qn − µ)2p](cid:1)1/p (cid:54) C(p)
(cid:54) 2C(p)
(cid:54) Cn−1/2 + Cn−3/2
(cid:69) n−1/2,

nE[(Qn − µ)2]
√
nE[(Qn − E[Qn])2] + 2C(p)

√

n(E[Qn] − µ)2

(40)

where we used the inequality (a + b)2 (cid:54) 2a2 + 2b2, for all a, b ∈ R and the bounds of Proposition
14 and Theorem 6 respectively. On the other hand, for all x > 2σ2,

(cid:48)(cid:48)

f

(x) = −

(cid:18)

1 −

2σ2
x3

3σ2
2x

(cid:19) (cid:18)

1 −

2σ2
x

(cid:19)−3/2

< 0.

Therefore using (39) and (40), to obtain a bound for the term dW
it remains to show that E[|f (cid:48)(cid:48)(δn)|p(cid:48)] is ﬁnite for some p(cid:48) > 1 but using the fact that δn ∈ [|Qn, µ|]
and the monotonicity of f (cid:48)(cid:48), it is actually suﬃcient to show that for some p(cid:48) > 1, we have

(ˆan − |a1|) ,

l1+l2

f (cid:48)(µ)

(cid:16)

√
n
√
l1+l2

√
n√

(Qn − µ)

(cid:17)

,

(cid:104)

|f (cid:48)(cid:48)(x)|p(cid:48)(cid:105)

E

sup
n(cid:62)1

= sup
n(cid:62)1

E

(cid:104)

|2σ2Qn − 3σ4|p(cid:48)

|Qn|−5p(cid:48)/2|Qn − 2σ2|−3p(cid:48)/2(cid:105)

< ∞.

30

The function |f (cid:48)(cid:48)| has two singularities in 0 and in 2σ2 and thus is not bounded. But, we can write

E (cid:2)|f (cid:48)(cid:48)(Qn)|(cid:3) = E

(cid:20)

|f (cid:48)(cid:48)(Qn)|1{|Qn−µ|(cid:62) 1√

n

(cid:21)

}

(cid:20)

+ E

|f (cid:48)(cid:48)(Qn)|1{|Qn−µ|< 1√

n

(cid:21)

.

}

(cid:20)

(cid:21)

n

}

For the term E

|f (cid:48)(cid:48)(Qn)|1{|Qn−µ|< 1√

and since µ > 2σ2, we can pick n such that 1√

n < σ2. Then
Qn > 2σ2 − σ2 > 0, therefore Qn is bounded away from 0 and the term |Qn|−5p(cid:48)/2 has no singularity
for any p(cid:48) > 1. For the term |Qn − 2σ2|−3p(cid:48)/2 , we put C := µ−2σ2
, the constant C (cid:54)= 0, because
µ (cid:54)= 2σ2 ⇔ a1 (cid:54)= 0, we can assume a1 (cid:54)= 0, because there is no AR(1) process with a1 = 0. Therefore,
we can pick n such that 1√
n + 2C > 2C − C > 0,
hence the term |Qn − 2σ2|−3p(cid:48)/2 has no singularities at 2σ2 for any p(cid:48) > 1. In conclusion, to avoid
the singularities at both 0 and 2σ2, it is suﬃcient to pick n such that
if |a1| (cid:62) 1√
2
if |a1| (cid:54) 1√
2

n < C. In this case Qn − 2σ2 = Qn − µ + 2C > − 1√

1√
n < σ2 ∧ C =

(cid:40)σ2
C

2

,

.

For the other term, by the asymptotic normality of

n(Qn − µ), we get as n ∼ +∞ and for

√

p(cid:48) > 1

(cid:20)

E

|f (cid:48)(cid:48)(Qn)|1{|Qn−µ|(cid:62) 1√

n

(cid:21)

}

∼ |2σ2|p(cid:48) (cid:90) +∞
(cid:54) |2σ2|p(cid:48) (cid:90) +∞

1

√

2π
2

(cid:54)

which gives the desired result.

6.2 Numerical Results

|

|

z
√
n
z
√
n

+ µ −

σ2|p(cid:48)

|

3
2

z
√
n

+ µ|−5p(cid:48)/2|

z
√
n

+ µ − 2σ2|−3p(cid:48)/2e−z2/2dz

+ µ − 2σ2|−7p(cid:48)/2e−z2/2dz

1

|2σ2|p(cid:48)

(µ − 2σ2)−7p(cid:48)/2.

The table below reports the mean and standard deviation of the proposed estimator ˆan deﬁned in
(36) of the true value of the mean-reversion parameter |a1|.

|a1| = 0.10

|a1| = 0.30

|a1| = 0.50

|a1| = 0.70

Mean

Std dev Mean

Std dev Mean

Std dev Mean

Std dev

n = 3000

0.2178

0.0901

0.2887

0.0969

0.4946

0.0520

0.6962

0.0234

n = 5000

0.1878

0.0866

0.2905

0.0616

0.4966

0.0413

0.6978

0.0270

n = 10000

0.1630

0.0692

0.2928

0.0852

0.4974

0.0315

0.6987

0.0215

We simulate the values of the estimator ˆan from the quadratic variation Qn for diﬀerent
sample sizes n and for ﬁxed σ2 chosen to be equal to 1. For each sample size n, the mean and the

31

standard deviation are obtained by 500 replications. The table above conﬁrms that the estimator
ˆan is strongly consistent even for small values of n and has small standard deviations for diﬀerent
true values of |a1|. Moreover, the estimator ˆan is more eﬃcient for values of |a1| greater than
0.5, this could be explained by the fact that the asymptotic variance of limiting law of |ˆan| is
(1−a2
, which is high for small values of |a1| and small for values of |a1| close to 1. Therefore,
ˆan is presumably more accurate as an estimator when |a1| is closer to 1, e.g. greater than 0.5 as
can be seen in the ﬁgure below.

1)(5−4a2
1)
2a2
1

Figure 2: Asymptotic variance for values of |a1| between 0.09 and 0.99

To investigate the asymptotic distribution of ˆan empirically, we need to compare the distri-

bution of the following statistic

φ(n, a1) :=

(cid:112)2a2
1
1)(5 − 4a2
1)

(cid:112)(1 − a2

√

n(ˆan − |a1|)

(41)

with the standard normal distribution N (0, 1). For this aim, for parameter choices |a1| = 0.5,
n = 3000, σ = 1, and based on 3000 replications, we obtained the following histogram:

32

Figure 3: Histogram of φ(n, a1) with n = 3000, |a1| = 0.5, σ = 1, 3000 replications.

This Figure 3 shows that the normal approximation of the distribution of the statistic
φ(n, a1) is reasonable even if the sampling size n is not very large. The table below compares
statistics of φ(n, a1) and N (0,1) based on 3000 replications, with n = 3000, and σ = 1. The
empirical mean, median and standard deviation of φ(n, a1) match those of N (0,1) very closely,
corroborating our theoretical results.

Statistics
N (0,1)
φ(n, a1)

Mean
0
0.0048515

Median
0
0.0020649

Standard Deviation
1
1.0004691

We can check more precisely how fast is the statistic φ(n, a1) converges in law to N (0,1).
We chose to compute the Kolmogorov distance between φ(n, a1) and N (0, 1). For this aim, we
approximate the cumulative distribution function using empirical cumulation distribution function
based on 500 replications of the computation of φ(n, a1) for n = 3000. The next ﬁgure shows the
empirical and standard normal cumulative distribution functions.

33

The Kolmogorov distance between the two laws, which equals the sup norm of the diﬀerence
of these cumulative distribution functions, computes to approx. 0.052. On the other hand, since
(See for example Theorem 3.3 of [9] for a proof)

dKol(φ(n, a1), N (0, 1)) (cid:54) 2(cid:112)dW (φ(n, a1), N (0, 1)),

(42)

the distance on the left-hand side should be bounded above by 2 × 3000−1/4 = 0.27 approx times
any constant coming from the upper bound in Proposition 17. This is ﬁve times larger than our
estimate of the actual Kolmogorov distance 0.052, a reassuring practical conﬁrmation of Proposition
17, and of our underlying results on normal asymptotics of 2nd-chaos AR(1) quadratic variations.
If that proposition’s upper bound with its rate n−1/2 applied directly to the Kolomogorov distance,
as is known to be the case for the Berry-Esséen theorem in the classical CLT, the value 0.052 should
be compared to 3000−1/2 = 0.018 approx., which is arguably in the same order of magnitude. This
is a motivation to investigate whether the so-called delta method which we used here to prove
Proposition 17 under the Wasserstein distance, could also apply to the total variation distance,
since it is known to be an upper bound on the Kolmogorov distance without the need for the square
root as in the comparison (42).

References

[1] Azmoodeh, E. and Morlanes, G. I. (2013). Drift parameter estimation for fractional Ornstein-

Uhlenbeck process of the second kind. Statistics. DOI: 10.1080/02331888.2013.863888.

34

[2] Azmoodeh, E. and Viitasaari, L. (2015). Parameter estimation based on discrete observations
of fractional Ornstein-Uhlenbeck process of the second kind. Statist. Infer. Stoch. Proc. 18, no.
3, 205-227.

[3] Balakrishna, N. and Shiji, K. (2014). Extreme Value Autoregressive Model and Its Applications,

Journal of Statistical Theory and Practice, 8 (3), 460–481.

[4] Barboza, L.A. and Viens, F. (2017). Parameter estimation of Gaussian stationary processes

using the generalized method of moments. Electron. J. Statist. 11 (1), 401-439.

[5] Belfadli, R., Es-Sebaiy, K. and Ouknine, Y. (2011). Parameter Estimation for Fractional
Ornstein-Uhlenbeck Processes: Non-Ergodic Case. Frontiers in Science and Engineering (An
International Journal Edited by Hassan II Academy of Science and Technology). 1, no. 1, 1-16.

[6] Biermé, H., Bonami, A., Nourdin, I. and Peccati, G. (2012). Optimal Berry-Esséen rates on

the Wiener space: the barrier of third and fourth cumulants. ALEA 9, no. 2, 473-500.

[7] Brouste, A. and Iacus, S. M. (2012). Parameter estimation for the discretely observed fractional
Ornstein-Uhlenbeck process and the Yuima R package. Comput. Stat. 28, no. 4, 1529-1547.

[8] Cheridito, P., Kawaguchi, H. and Maejima, M. (2003). Fractional Ornstein-Uhlenbeck pro-

cesses, Electr. J. Prob. 8, 1-14.

[9] L.H.Y. Chen, L. Goldstein and Q.-M. Shao (2011). Normal Approximation by Stein’s Method.

Berlin: Springer-Verlag.

[10] Cumberland, W.G. and Sykes, Z.M. (1982). Weak Convergence of an Autoregressive Process

Used in Modeling Population Growth. Journal of Applied Probability, 19 (2) 450-455.

[11] Davydov, Y.A. and Martynova, G. V. (1987). Limit behavior of multiple stochastic integral.

Statistics and control of random process. Preila, Nauka, Moscow, 55-57 (in Russian).

[12] Dobrushin, R. L. and Major, P. (1979). Non-central limit theorems for nonlinear functionals of

Gaussian ﬁelds. Z. Wahrsch. verw. Gebiete, 50, 27-52.

[13] Douissi, S., Es-Sebaiy, K., Viens, F. (2019). Berry-Esséen bounds for parameter estimation of
general Gaussian processes. Latin American journal of probability and mathematical statistics,
16, 633-664.

[14] El Machkouri, M., Es-Sebaiy, K. and Ouknine, Y. (2015). Least squares estimator for non-
ergodic OrnsteinUhlenbeck processes driven by Gaussian processes. Journal of the Korean Sta-
tistical Society, DOI: 10.1016/j.jkss.2015.12.001 (In press).

[15] El Onsy, B., Es-Sebaiy, K. and Tudor, C. (2014). Statistical analysis of the non-ergodic frac-

tional Ornstein-Uhlenbeck process of the second kind. Preprint.

[16] El Onsy, B., Es-Sebaiy, K. and Viens, F. (2017). Parameter Estimation for Ornstein-Uhlenbeck
driven by fractional Ornstein-Uhlenbeck processes. Stochastics, Vol. 89, No. 2, 431-468.

35

[17] Ernst, Ph., Brown, L., Shepp, L., Wolpert, R. (2017). Stationary Gaussian Markov processes as
limits of stationary autoregressive time series. Journal of Multivariate Analysis, 155, 180-186.

[18] Es-Sebaiy, K. and Viens, F. (2018). Optimal rates for parameter estimation of stationary Gaus-
sian processes. Stochastic Processes and their Applications, in press, 49 pages, available online
DOI: 10.1016/j.spa.2018.08.010.

[19] Fernique, X. Régularité des trajectoires des fonctions alé atoires gaussiennes. In Ecole d’Et´e
de Probabilités de Saint-Flour IV-1974, Lecture Notes in Math. 480, 1-96. Springer V., 1975.

[20] Grunwald, G.K., Hyndman, R.J., and Tedesco, L.M. (1996). A uniﬁed View of linear AR(1)

models, preprint.

[21] Hürlimann, W. (2012). “On Non-Gaussian AR(1) Inﬂation Modeling” Journal of Statistical and

Econometric Methods, 1 (1), 93-109.

[22] Hu, Y. and Nualart, D. (2010). Parameter estimation for fractional Ornstein-Uhlenbeck pro-

cesses. Statist. Probab. Lett. 80, 1030-1038.

[23] Hu, Y. and Song, J. (2013). Parameter estimation for fractional Ornstein-Uhlenbeck processes
with discrete observations. F. Viens et al (eds), Malliavin Calculus and Stochastic Analysis: A
Festschrift in Honor of David Nualart, 427-442, Springer.

[24] Kleptsyna, M. and Le Breton, A. (2002). Statistical analysis of the fractional Ornstein- Uhlen-

beck type process. Statist. Infer. Stoch. Proc. 5, 229-241.

[25] Kloeden, P. and Neuenkirch, A. (2007). The pathwise convergence of approximation schemes

for stochastic diﬀerential equations. LMS J. Comp. Math. 10, 235-253.

[26] Liptser, R. S. and Shiryaev, A. N. (2001). Statistics of Random Processes: II. Applications,

Springer-Verlag, New York.

[27] Maddison, C.J., Tarlow, D., Minka, T. (2015). A* Sampling, Preprint, available online at

https://arxiv.org/abs/1411.0030v2 .

[28] Nakajima, J., Kunihama, T., Omori, Y., and Frühwirth-Schnatter, S. (2012). Generalized
extreme value distribution with time-dependence using the AR and MA models in state space
form. Computational Statistics & Data Analysis, 56 (11), 3241-3259.

[29] Neufcourt, L. and Viens, F. (2014). A third-moment theorem and precise asymptotics
for variations of stationary Gaussian sequences. In press in ALEA; preprint available at
http://arxiv.org/abs/1603.00365

[30] Neuenkirch, A. and Tindel, S. (2014). A least square-type procedure for parameter estimation
in stochastic diﬀerential equations with additive fractional noise.Statist. Infer. Stoch. Proc. 17,
no. 1, 99-120.

36

[31] Nourdin, I. and Peccati, G. (2015). The optimal fourth moment theorem. Proc. Amer. Math.

Soc. 143, 3123-3133.

[32] Nourdin, I. and Peccati, G. (2012). Normal approximations with Malliavin calculus :

from
Stein’s method to universality. Cambridge Tracts in Mathematics 192. Cambridge University
Press, Cambridge.

[33] Nourdin, I., Peccati, G., and Yang, X. (2018). Berry-Esseen bounds in the Breuer-Major CLT

and Gebelein’s inequality. Preprint https://arxiv.org/abs/1812.08838.

[34] Novikov, A.A. (1971). Sequential estimation of the parameters of diﬀusion type processes. Teor.

Veroyatn. Primen., 16 (2), 394-396

[35] Nualart, D. (2006). The Malliavin calculus and related topics. Springer-Verlag, Berlin.

[36] Nualart, D. and Peccati, G. (2005). Central limit theorems for sequences of multiple stochastic

integrals. Ann. Probab. 33, no. 1, 177-193.

[37] D. Nualart and H. Zhou (2018): Total variation estimates in the Breuer-Major theorem.

Preprint arXiv:1807.09707.

[38] Viens, F.; Vizcarra, A. Supremum concentration inequality and modulus of continuity for sub-

nth chaos processes. J. Functional Analysis 248 (1) (2007), 1-26.

[39] Nualart, D. (2006). The Malliavin calculus and related topics. Springer-Verlag, Berlin.

[40] Üstünel, A. S. (1995). An Introduction to Analysis on Wiener Space. Lecture Notes in Math.

1610. Springer, Berlin.

[41] Nualart, D. and Ortiz-Latorre, S. (2008). Central limit theorems for multiple stochastic integrals

and Malliavin calculus. Stochastic Process. Appl. 118 614- 628.

[42] Nualart, D. and Peccati, G. (2005). Central limit theorems for sequences of multiple stochastic

integrals. Ann. Probab. 33 177-193.

[43] Peccati, G. and Tudor, C. A. (2004). Gaussian limits for vector-valued multiple stochastic inte-
grals. In Séminaire de Probabilités XXXVIII. Lecture Notes in Math. 1857 247-262. Springer,
Berlin.

[44] Schoenberg, I.J. (1951). On Polya frequency functions. Journal d’Analyse Mathématique, 1,

331-374.

[45] Shepp, L.A. (1969). Explicit solutions to some problems of optimal stopping. The Annals of

Mathematical Statistics 40, 993-1010.

[46] Shepp, L.A. and Shiryaev, A.N. (1993). The Russian option: reduced regret. The Annals of

Applied Probability 3, 631-640.

37

[47] Shepp, L.A. and Vardi, Y. (1982) Maximum likelihood reconstruction for emission tomography.

IEEE transactions on Medical Imaging, 1, 113-122.

[48] Tanaka, K. (2017). Time Series Analysis: Nonstationary and Noninvertible Distribution The-

ory, volume 16. Wiley New York.

[49] Gwladys Toulemonde, Armelle Guillou, Philippe Naveau, Mathieu Vrac and Frederic Cheval-
lier. (2010). Autoregressive models for maxima and their applications to CH4 and N2O. Envi-
ronmetrics, 21 (2), 189-207.

[50] Tudor, C.A. (2008). Analysis of the Rosenblatt process. ESAIM: Probability and Statistics, 12,

230–257.

[51] Tudor, C. and Viens, F. (2007). Statistical aspects of the fractional stochastic calculus. Ann.

Statist. 35, no. 3, 1183-1212.

[52] Viitasaari, L. Representation of Stationary and stationary increment processes via Langevin
equation and self-similar processes. (2016). Statistics and Probability Letters, 115, 45-53.

[53] Voutilainen, M., Viitasaari, L., and Ilmonen, P. (2017). On model ﬁtting and estimation of

strictly stationary processes. Modern Stochastics : Theory and Applications. 4 (4), 381–406.

[54] Voutilainen, M., Viitasaari, L., and Ilmonen, P. (2019). Note on AR(1) characterization of sta-
tionary processes and model ﬁtting. Modern Stochastics : Theory and Applications, to appear,
https://doi.org/10.15559/19-VMSTA132 .

[55] Yuan Yan, Genton, M.C. (2018). Non-Gaussian autoregressive processes with Tukey g-and-h

transformations. Environmetrics, e2503, https://doi.org/10.1002/env.2503 .

38

