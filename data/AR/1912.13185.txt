9
1
0
2

c
e
D
1
3

]
T
S
.
h
t
a
m

[

1
v
5
8
1
3
1
.
2
1
9
1
:
v
i
X
r
a

arXiv: arXiv:0000.0000

Model-free Bootstrap for a General Class
of Stationary Time Series

Yiren Wang, Dimitris N. Politis

Department of Mathematics
University of California, San Diego
La Jolla, CA 92093-0112, USA
email: yiw518@ucsd.edu; dpolitis@ucsd.edu

Abstract: A model-free bootstrap procedure for a general class of sta-
tionary time series is introduced. The theoretical framework is established,
showing asymptotic validity of bootstrap conﬁdence intervals for many
statistics of interest. In addition, asymptotic validity of one-step ahead boot-
strap prediction intervals is also demonstrated. Finite-sample experiments
are conducted to empirically conﬁrm the performance of the new method,
and to compare with popular methods such as the block bootstrap and the
autoregressive (AR)-sieve bootstrap.

1. Introduction

The bootstrap, since its introduction by Efron (1979), has been an invaluable
tool for statistical inference with independent data. Resampling for time series
has also been a ﬂourishing topic since the late 1980s. However, there is a plethora
of ways to resample a stationary time series. It is always important to validate
the correctness of such bootstrap procedures, i.e. show their asymptotic validity
and range of applicability with respect to common statistics. These problems
have been well studied for popular methods like the block bootstrap and the
autoregressive (AR)-sieve bootstrap. For a summary of the state-of-the-art, see
McElroy and Politis (2019) and Kreiss and Paparoditis (2020).

In a dependent setup, the main purpose of bootstrap is two-fold: one is to
obtain conﬁdence intervals for a parameter of interest and/or conduct a hy-
pothesis test. Another important aspect of time series analysis is forecasting. A
1, the goal is
standard setup is the following: given the time series data
1.
h-step ahead prediction, i.e., predicting Yt
`
An optimal h-step ahead point predictor

Yt
u
h for some integer h
Yt
`
h and itself, conditioned on the current data
p

loss between the true Yt
The most widely used loss functions are L1 and L2. The L2 loss, E

h should minimize the expected
, Yn

n
t
“
ě

Y1,

¨ ¨ ¨

t

`

t
Yt

Yt

h

u
h

is minimized by
“
Yt
is minimized by the conditional median med
p

Y1,
|

Yn
p

¨ ¨ ¨

Yn

Yt

E

`

`

h

h

. The L1 loss, E
q

h

`

Y1,
|

¨ ¨ ¨

p

`

Yt
|
, Yn
´
p

q

Besides point predictors, prediction intervals and joint prediction regions are
quite useful; since any point predictor will invariably incur an error, it is im-
portant to provide a range of values where the future point Yt
h will be found

`

ˆ´
Yt
p
h
h
´
`
instead.

`

´
Y1,

||

¨ ¨ ¨

`

.

2

Y1,
|
¯
, Yn

¯

, Yn

¨ ¨ ¨

˙

1

 
 
 
 
 
 
Y. Wang and D.N. Politis/Model-free Bootstrap for Stationary Series

2

with high probability. Prediction intervals can be constructed by approximating
the distribution of the so-called predictive root, i.e. Yt
h, and using the
respective quantiles to produce upper and lower bounds. Approximating this dis-
tribution typically requires one to ﬁt a speciﬁc model to the data, which enables
a model-based resampling for Yt
h separately; see Pan and Politis
(2016a) for a review.

h and

Yt

Yt

´

p

`

`

`

`

h

p

e1, . . . , en
p

Y1, . . . , Yn
p

1 to a new data vector
q

However, model-ﬁtting and prediction are two separate notions with very
diﬀerent objective functions. Cross-validation ideas that are currently popular
attempt to link the two notions, in choosing a model that is actually good for
predictive purposes. Nevertheless, it is possible for the practitioner to proceed
directly to prediction without the intermediary step of model-ﬁtting; this is the
essence of the model-free prediction principle of Politis (2013), Politis (2015). To
describe it, the goal is to ﬁnd an invertible transformation that transforms the
1 whose entries are
data vector
q
independent, identically distributed (i.i.d.). One can then employ the i.i.d. boot-
strap on the e1, . . . , em to generate e˚1 , . . . , e˚m, and use the inverse transform
to get bootstrap samples Y ˚1 , . . . , Y ˚m in the domain of the original data. Using
n is the standard framework for estimation and conﬁdence intervals; inter-
m
h allows us to equally address the problem of forecasting
estingly, using m
Yt

h with prediction intervals.
`
Under regularity conditions, such a transformation always exists but is not
unique; see Ch. 2.3.3 of Politis (2015). The challenge for the practitioner is to use
the structure of the data at hand in order to devise a transformation that works
in the given setting, having features that can be estimated from the data. In a
model-based approach, these steps are analogous to choosing a model, and then
ﬁtting the model using the data. Indeed, any model driven by i.i.d. errors can
be used to deﬁne a transformation of the data towards the i.i.d. target; however,
the power of the model-free approach is that it can work without restricting
oneself to a model equation.

`

“

“

n

u

To elaborate, if the data arise as a stretch of a strictly stationary time series
Yt
with (absolutely) continuous distributions, then the Rosenblatt transfor-
t
mation (Rosenblatt (1952)) can be used to transform Y1, . . . , Yn to a set of n
i.i.d. Uniform random variables. In general, this application of the Rosenblatt
transformation can not be implemented in practice because it involves n un-
known conditional distribution functions. However, if additional structure is
assumed, e.g., when
stationary Markov sequence, then this approach is
feasible; see Pan and Politis (2016b) and Ch. 8 of Politis (2015).

Yt

u

t

is a ‘white noise’. If

Y1, . . . , Yn
p
Wt
t

W1, . . . , Wn
1 to a data vector
p
q
Yt
u

To describe a diﬀerent approach, recall the Linear Process Bootstrap (LPB)
of McMurry and Politis (2010) which essentially transforms the the data vec-
1 that has uncorrelated entries,
tor
q
i.e.,
can fur-
ther be claimed to be i.i.d. (under some conditions). The LPB has parallels
with the AR-sieve bootstrap since both are applicable to nonlinear time se-
ries as long as the statistic of interest has a large-sample distribution that
only depends on the ﬁrst and second order moment structure of the data; see
Kreiss, Paparoditis and Politis (2011) and Jentsch and Politis (2015).

is a linear time series, then

Wt

t

u

t

u

Y. Wang and D.N. Politis/Model-free Bootstrap for Stationary Series

3

Nevertheless, in the search of a transformation that renders the data i.i.d.,
it may be helpful to ﬁrst devise a transformation into Gaussianity; see e.g., Ch.
2.3.2 of Politis (2015). For example, we can use a version of the Probability Inte-
gral Transform (PIT) in order to transform our time series data to Gaussian; the
latter can then be transformed to i.i.d. by a decorrelating/whitening operation
as in the LPB. This approach was ﬁrst suggested in Ch. 9 of Politis (2015), and
was practically implemented to the setting of a locally stationary time series by
Das and Politis (2020).

In the paper at hand, we focus on stationary time series data, with the goal of
establishing the realm of applicability of the above mentioned procedure which,
for lack of a better word, we will call the model-free bootstrap (MFB). We will
show asymptotic validity of the MFB for a general class of stationary processes,
and for many types of statistics of interest. We will also establish MFB’s validity
for the construction of one-step-ahead prediction intervals, i.e., to ﬁx ideas we
will focus on the case h

1 in the above.

The remaining of the paper is organized as follows. Section 2 restates the MFB
algorithm carefully. Section 3 introduces some necessary tools and assumptions
to be used, and summarizes some useful preliminary results for our proofs. Sec-
tion 4 proves MFB’s asymptotic validity for various estimation problems, while
Section 5 shows its validity for prediction intervals. Numerical experiments that
back up our asymptotic results are presented in Section 6. Technical proofs are
given in the Appendix.

“

2. Model-free bootstrap algorithm

2.1. The MFB algorithm

Here we describe the model-free bootstrap (MFB) algorithm for inference and
prediction as proposed in Chaper 9 of Politis (2015). Given a time series
Z
that is strictly stationary, let FY be the cumulative distribution function (CDF)
of Y0. The PIT deﬁned by

Yt

t
P

t

u

Ut

FY

Yt
p

q

“

1

implies that Ut is uniformly distributed on
, assuming FY is continuous.
See Angus (1994). Let Φ be the CDF of standard normal distribution, and
Φ´
is
N

p
p
q ě
0, 1
p
Let Σn denote the covariance matrix of Zn “ p

p
u
distributed. Also, stationarity is preserved for
Z1,

is the quantile function; then, Zt

“
Zt
u

q “
q

Φ´
.

t
¨ ¨ ¨

R : Φ

u
, Zn

Ut
p

x
p

and

inf

0, 1

Ut

x

P

t

t

q

s

r

1

Σ´
n

the lower triangular matrix from the Cholesky decomposition of Σ´
Σ´
ξ
n “
normal.

n Z n is a vector of i.i.d. N

entries, provided Z1,

0, 1
p

¨ ¨ ¨

q

1
2

, and denote by
q
1
n . Then,
, Zn are jointly

1
2

Suppose we use a resampling scheme to create the i.i.d. bootstrap sample
1, and Y ˚t “

n where ξ˚

, ξ˚nq

ξ˚1 , ξ˚2 ,

n ξ˚

¨ ¨ ¨

Σ

1
2

n “ p
is our bootstrapped sample.

ξ˚1 , ξ˚2 ,
1
Φ
F ´
Y p

, ξ˚n. Then, letting Z ˚n “
Y ˚t u
t

, then

¨ ¨ ¨
Z ˚t qq
p

Y. Wang and D.N. Politis/Model-free Bootstrap for Stationary Series

4

1

Moreover, ξ˚n
1 can also be generated through i.i.d. sampling, and Z ˚n
1 can
`
`
. Using the in-
Z n, Z ˚n
be generated through the relation
q
p
`
verse of the previously mentioned transforms, the next bootstrap value can be
generated by Y ˚n
. It can be shown that by using these theo-
“
`
retical transforms, Y ˚n
`

1
Y p
Y n has the same distribution as Yn
1
|

Nevertheless, to use the above steps for practical purposes, each transform
must also be estimated in a consistent manner from the data at hand. Further-
more, the validity of the bootstrap procedure has to be investigated, both for
estimation and prediction. Thus, several questions arise:

Z ˚n
p
`

Y n.
|

2
1
{
n
`

q “

, ξn

ξ
p

F ´

qq

Φ

Σ

`

`

n

1

1

1

1

1

• Under what circumstance are the entries of Z n jointly normal?
• What estimators for FY and Σn should we use so that the above steps

lead to validity of the bootstrap?

• How should we create the i.i.d. bootstrap values

?

ξ˚t u

t

t

u

p

p

ξt

n
t
“

1, with

The ﬁrst two points will be addressed in the following paragraphs. For the third
point, Politis (2015) has proposed two ways to do it. One way is sampling with
ξt calculated from Yt using estimated transform
replacement from
0, 1
functions. A second way is to generate ξ˚t as i.i.d. N
, which is presumably
q
p
ξt. The ﬁrst method is called model-free (MF), and
the limiting distribution of
the second is referred to as limit model-free (LMF) since the limit distribution
is used.

p
Frequently used notations include the following. Let

Σn denote general
estimators for FY and Σn respectively. The subscript Y is dropped from
F for
1 its quantile
simplicity. Φ is the CDF of a standard normal distribution with Φ´
function. Let ˜Φ be the CDF of a thresholded standard normal distribution:
0, 1
X
suppose X
N
| ą
p
|
„
is the sign function. Then ˜Φ denotes the CDF of Xc and its
c, where sgn
p¨q
1 the quantile function. We omit c in the notation for simpliﬁcation.
inverse ˜Φ´
1. The
Asymptotically we also require c
reason of this augmentation is provided in Section 2.4 and asymptotic details
are explained in Section 3.

1 converges to Φ´

such that ˜Φ´

c and Xc

c for
q

, Xc
q

p
X
|

F and

X for

Ñ 8

X
p

sgn

| ď

“

“

p

p

By using these practical transforms, we can calculate
1
, which are the estimations for the latent series
q

˜
Zt
“
respec-
p
Σn can not be directly calculated. Instead, we use

, and
q
Zt
t

Yt
F
p
and
p

is latent,

Ut
t
p

“
u

Zt

Ut

u

k
p

Σn which is the same estimator calculated based on

˜
Zt
.
u
Let σZ
σZ
k
EZ0Zk be the lag-k autocovariance of Zt,
p
˜
pp
1
p
k
σZ
E
Zt. Let k
p denote the
and
kp “
p
¨
p
norm of a random variable; k
p
kop denotes the operator norm of a matrix,
´
¨
n square matrix. Relative
i.e., kM kop “
pp
quantities in the bootstrap world will be denoted by a superscript

be its estimator
p
p| ¨ |

q “
be the estimator calculated from

1 kM xk2 where M is a n

Rn,kxk2“

supx

ˆ

p

p

t

q

q

q

P

.

˜Φ´
Ut
p
tively. Since
p

t

u

Given the above introduction, we can now describe the model-free bootstrap

˚

algorithm.

Y. Wang and D.N. Politis/Model-free Bootstrap for Stationary Series

5

2.2. MFB for conﬁdence intervals

n
t
“

Let θ0 be a population parameter of interest;
Yt
1, and
t
the real-world root r
“
equal-tailed conﬁdence interval (CI) for θ0 is

θ0

´

p

u

θn an estimator of θ0 from data
1. Deﬁne
100%
α
q

n
Y ˚t u
t
“
1
´
p

t

θ˚n the same estimator from bootstrapped data

p
θn; let R denote its CDF. Then, a

p
1
R´

θn
p
r

`
R : R

α
p
{
r
p

2

θn

,
q
x
p
u

1

R´

`

1
p

2

α
{

,
qq

´

1

where R´
distribution R could be approximated through bootstrap simulations.

denotes the quantile function of R. The

x
p

q ě

q “

p
P

inf

t

Algorithm 1 (Model-free bootstrap for parameter inference)

1. Given data tYtun

2. (MF) Let ξ˚

Ut “

t“1, let

˜
Zt “ ˜Φ´1p
F pYtq;
t be i.i.d. samples from ¯F
ξ, where ¯F
ξ is the empirical CDF of
p
p
p
F ´1pΦpZ ˚
. Let Y ˚
n ξ˚
p
n

pp
t qq. Calculate the bootstrap

˜
Z n.

t “

Σ´
n

Utq;

“

Σ

p

p

p

1
2

ξ

n

p

1
2

ξtun
t
root r˚ “

t“1, and Z ˚
θn ´

n “
θ˚
n .

p
p
3. Do the above step B times to form an empirical CDF ¯R based on the
B replicates of r˚. ¯R is used to approximate R; hence, an approximate
p1 ´ αq100% CI for θ0 is

p

p

θn ` ¯R´1
p

pα{2q,

θn ` ¯R´1

p1 ´ α{2qq.

pp

p

For limit model-free (LMF) bootstrap, replace step 2 with following:

p

2. (LMF) Let ξ˚
F ´1pΦpZ ˚

t qq. Calculate the bootstrap root r˚ “

t be i.i.d. samples from N p0, 1q, and Z˚

n “

Σ

1
2

n ξ˚
n

. Let Y ˚

t “

θn ´

θ˚
n .

p

p

pp

As an alternative, it may be easier to do:

p

Let Z˚

n “ N p0,

Σnq.

pp

in order to omit ﬁnding the Cholesky decomposition of the covariance matrix
Σn. (However in practice, ﬁnding the Cholesky decomposition of a covariance
matrix is required for generating multivariate normal samples.)
pp

Below are some simple examples of statistics of interest.

• The mean: θ0
Y0
E
p
• Autocovariance: θ0
• Autocorrelation: θ0

“

“
k
p
q
0
ř
“
q
p
Additional examples will be addressed in Section 4.

n
t“1 Yt
n

θn
;
q
γY
k
p
p
k
ρ
p

“ ř
θn
;
“
q
k
γ
;
qγ
p
0
q “
p
q
p

.
k
γ
q “
p
k
ρ
q “
p
p
p

1
n
γ
γ

p
p

¯Yn

Yt
p

´

Yt

¯Yn

.
q

k

´

`

qp

n
k
´
1
t
“
.

2.3. MFB for prediction intervals

For prediction problems, we want to use the bootstrap to simulate the distri-
bution of Yn
1. For this purpose, we use

1 conditional on past values

Yt

`

n
t
“

u

t

Y. Wang and D.N. Politis/Model-free Bootstrap for Stationary Series

6

1

`

Yn

1, where

bootstrap to approximate the conditional distribution of the predictive root
1 is a predictor chosen by the practitioner. Let G de-
Yn
note the conditional distribution of the predictive root deﬁned above. Then a
1
p

100% equal tailed prediction interval for Yn
α
p
q

1 is :

Yn

´

´

p

`

`

`

Yn
p

`

1

`

1

G´

α
{
p

2

,
q

Yn

`

1

`

1

G´

1
p

2

α
{

qq

´

Model-free bootstrap algorithm for one step ahead prediction is the following:
p

p

Algorithm 2 (Model-free bootstrap for 1-step ahead prediction)

1. Given data tYtun

t“1, let

Ut “

F pYtq,

˜
Zt “ ˜Φ´1p

Utq.

2. Denote

σpnq
p
...
pp
σp1q

p

p
, and

p
Σ12 “ »

pp

pp

ﬁ

Σn,

Σ22 “

Σ11 “

Σn`1 “

σp0q. Let

Let tξ˚
1
2
n ξ˚
n

Σ11
ΣT
12
pp
ﬃ
pp
pp
ﬃ
t un`1
ﬂ
ξtun
t“1, and Z ˚
pp
t“1 be drawn randomly with replacement from t
1
. Let Z ˚
ξn, ξ˚
Σ
n`1 be the pn ` 1qth element of the vector
2
n`1p
p

Σ12
Σ22ﬀ
pp
pp
n “
n`1q.
Denote the distribution of Z ˚
. This is also the estimated
pp
pp
conditional distribution of Zn`1|Y n. The form of this distribution is con-
F ´1pΦpZ ˚
ditional on our data Y n “ pY1, ¨ ¨ ¨ , Ynq. Let Y ˚

F pn`1q
Z

n`1 as

—
—
–

n`1qq.

n`1 “

Σ

pp

pp

pp

«

p

p

.

3. Choose a predictor

Yn`1 for Yn`1 based on Y n. For example, the L2
optimal predictor as mentioned in Section 1 is the expectation of Zn`1
conditioning on Y n that can be approximated by

p

p

Yn`1 “

F ´1

pΦpzqqd

F pn`1q
Z

pzq.

ż

p

The above integral can be evaluated through Monte-Carlo simulation. The
chosen predictor will be used as the center of our prediction interval, and
the bootstrap procedure will be used to capture the distribution of the pre-
dictive root in the next steps.

p

p

t “

t qq. Let p

F pn`1q
Z

F ´1pΦpZ ˚

in the above calculation, with bootstrapped data Z ˚
Y ˚
function for Z ˚
p

used
4. Re-estimate all the transforms, matrices and the distribution
n “ pZ ˚
n q and
q˚ denote the re-estimated distribution
n q. Let pZn`1|Ynq˚
F pn`1q
Z
Y ˚
n`1 denote the one step ahead predictor with re-estimated transforms
p
Y ˚
n`1 “

Let
based on the bootstrap pseudo data. In the L2-optimal setting,
F ˚q´1pΦpzqqdp
p

denote the random variable with estimated conditional distribution p

n`1 with bootstrap data Y ˚
p

1 , ¨ ¨ ¨ , Y ˚

n “ pY ˚

q˚pzq.

F pn`1q
Z
1 , ¨ ¨ ¨ , Z ˚
p

p

F pn`1q
Z

q˚.

p

5. The bootstrapped L2´ optimal predictive root is:

ş

p

p

Y ˚
n`1 ´
6. Denote the empirical CDF of bootstrapped predictive roots as ¯G. The ap-

Y ˚

n`1

p

proximate p1 ´ αq prediction interval for Yn`1 is

Yn`1 ` ¯G´1

pα{2q,

Yn`1 ` ¯G´1

p1 ´ α{2q

.

Algorithm 3 (Limit model-free bootstrap for 1 step ahead prediction)

p

´

p

¯

1. Given data tYtun

t“1, let

Ut “

F pYtq,

˜
Zt “ ˜Φ´1p

Utq.

p

p

p

p

Y. Wang and D.N. Politis/Model-free Bootstrap for Stationary Series

7

σpnq
...
pp
σp1q

2. (LMF) Denote

Σ11 “

Σn,

Σ12 “ »

ﬁ

. Let Z˚

n „ N p0,

Σnq; Z ˚

n`1 „

N p

ﬃ
ﬃ
ﬂ
Σ12q; Y ˚
n`1 “
pp
3. Choose a predictor for Yn`1 based on Y n. For example, the L2 optimal

pp
Σ22 ´

F ´1pΦpZ ˚

pp
Σ´1
11

pp
Σ21

n`1qq

˜
Zn,

Σ´1
11

Σ21

—
—
–

pp

pp

pp

predictor is

p

pp

pp

pp

pp

Yn`1 “ Ep
˜
Σ´1
Zn,
11
p

p
pΦpZn`1qq|Y nq.

F ´1

Σ22 ´

Σ21

Σ´1
11

Σ12q.

Where Zn`1|Y n „ N p
Σ21
p

p

pp

pp

n and Y ˚

F ´1pΦpZ ˚

4. Re-estimate all the transforms and matrices used in the above calculation,
pp
pp
with Z˚
t qq. The one step ahead predictor in the boot-
F ˚q´1pΦpZn`1qq|Y nq, where the expectation
strap world is
in the bootstrap world is calculated through the distribution of pZn`1|Y nq˚
that is N p
21p

pp
pp
t “
Y ˚
n`1 “ E˚pp

p
11q´1 ˜
p
Σ˚

12q distributed.

p
Σ˚
22 ´

11q´1

Σ˚
Z n,
5. The bootstrapped L2´ optimal predictive root is:
p

pp
pp
Y ˚
Y ˚
n`1 ´
6. Denote the empirical CDF of bootstraped predictive root as ¯G. The approx-

n`1

21p

Σ˚

Σ˚

Σ˚

pp

pp

pp

pp

p

imate p1 ´ αq prediction interval for Yn`1 is

Yn`1 ` ¯G´1

pα{2q,

Yn`1 ` ¯G´1

´

p1 ´ α{2q
¯

.

1

1

1

1

p

p

`

`

`

`

Φ

E

N

F ´

, Zn

Φ
p

Zn
p

Zn
p

F ´
p

Y n „
|

Y nq
qq|
Σ21Σ´
p

1
´
1 has the conditional distribution N

1
Y p
qq
; and in the expectation Zn
q

Here we provide an explanation to step 5 above. Bootstrap is supposed to cap-
, where
ture the distribution of the predictive root Yn
11 Z n, Σ22
for Yn
1
“
`
1
1
Σ21Σ´
11 Σ12
Σ´
Σ12
1
11
q
`
is estimated from data. Clearly, the randomness in the distribution of the predic-
tive root not only comes from the randomness of the series, but also randomness
in the estimation. Thus in the bootstrap world, we should replace theoretical
transforms with their data-dependent analogue, and also account for all the er-
Σn with
rors arising from estimation, i.e. replace FY with
Σ˚n, resulting in the formula in step 5. In this way, the bootstrap procedure can
capture all the randomness in the distribution of predictive root, which helps
pp
relieve potential undercoverage issue for ﬁnite data. See also Ch. 9 of Politis
(2015).

p
1
Σ´
Σ21
11
p

F ˚, and

F with

˜
Z n,

Σ21

Σ22

F ,

´

pp

pp

pp

pp

pp

pp

pp

p

p

p

p

´

2.4. Appropriate estimators for

F ,

˜Φ´

1 and

Σn

p

pp
1 might be needed. Firstly, it is necessary that

Now we discuss what should be the appropriate estimators
an augmented version of Φ´
1,
Σn should be consistent in certain forms for FY , F ´
F ´
Y
F , the ﬁrst idea is to use the empirical CDF ¯F
y
For
p
q “
pp
1
is the indicator function, and its inverse ¯F ´
p
p
where I
q “
p
p
p
u
and ¯F ´

Σn and also why
F ,
p
and Σn respectively.
p
y
,
u
t¨u
q ě
. Under moment and short-range dependence assumptions, consistency of ¯F
1 can be established by looking into the empirical process and quantile

n
t
“
y
P
t
ř

1 I
Yt
t
R : ¯F

1
n
inf

ď
y
p

F ,

pp

1

Y. Wang and D.N. Politis/Model-free Bootstrap for Stationary Series

8

process. Details are in later sections and will play an important role in our
proofs.

1

1

y

p

Fh

y
p

t

t

p

´

´

K

ř

Yt

Ut

¨ ¨ ¨

y
p

n
t
“

1 Kh

, 1
p
u

n , 2
n ,

U ˚t q
p

Yt
´
h q

¯F
Yt
p
q
n
Yt
t
u
“

, where Kh
q

“
only takes value in

only takes value in
1. But by using the kernel estimator

Another natural candidate is the kernel smoothed CDF estimator
Yt

q “
1
and K is a smooth CDF
y
n
p
q “
p
Fh is that it
function with additional assumptions. The obvious advantage of
is continuous, which is a property ¯F is lacking. An additional implication of
using ¯F is the resulting
and Y ˚t “
¯F ´
Fh and
its inverse, Y ˚t can take values that did not appear in the original series. If the
p
Fh is
data size n is large, the inﬂuence of this is minimal; whereas if n is small,
a better estimator because of its ability to interpolate unseen values compared
to the coarse behavior of ¯F . It is also worth mentioning that when sample size
p
is large, using ¯F and its inverse will save computational time comparing to
Fh.
will result in having a value
1
1
is not well deﬁned.
q
p
1, which is the inverse CDF of a thresh-
To this respect, we use an augmented ˜Φ´
p
1 is
olded standard normal Nc
p
, which relieves this problem. Asymptotically, thresholding also
bounded on
1 at endpoints 0 and 1, which helps
controls the fast diverging behavior of Φ´
Σn for both ¯F and
Fh
in analyzing convergence of the covariance estimator
1 to
scenarios. Nevertheless, under ﬁnite sample settings, the correction from Φ´
1 doesn’t require too much attention. For ¯F scenario, we can simply change
˜Φ´
p
1
the value 1 to n
and the eﬀect
n from
´
of doing this is negligible. And for

1
p
Fh, no correction is needed.

as deﬁned in Section 2.1. By doing this, ˜Φ´

1, and the following second step transform Φ´

Using ¯F for the ﬁrst step transform

1 to avoid the problem of Φ´

0, 1
p

Yt
p

n
t
“

n
t
“

0, 1

P t

Ut

Ut

Ut

¯F

“

pp

p

t

u

u

q

q

q

s

r

1

1

p

Consistent estimator

Σn of the autocovariance matrix Σn has been well stud-
ied. Wu and Pourahmadi (2009) established the ﬁrst result on consistency of a
banded matrix estimator. Here we shall use the more general ﬂat-top estimators
x
of McMurry and Politis (2010). Let κ
q
p

be the tapering weight function:

p

p

1

κ

x
|
ă
x
|
1. The most commonly used ﬂat-top kernel is deﬁned by

x
q “ $
p
’&

| ď
1
x
|
| ą

| ď
cκ

if
if
if

1
g
0

’%

cκ

|q

p|

x

where

g
|

x
p

q| ă

1
2
0

x
|

´ |

κ

x
q “ $
p
’&

x
if
| ď
|
if 1
ă |
x
if
| ą
|

1
x
| ď
2;

2

see Politis and Romano (1995). Let l be the bandwidth of choice and κl
κ

has value:

at entry

κ,l
qn

Σp

l

x
{
p

. The tapered estimator
q

i, j
p

q

“

’%
Σn

Σp

κ,l
qn

p
q “

i, j
p

κl

p
k
p

p

1
n

k

n

´

1
t
ÿ
“

q ˜

ZtZt

`

,

k

¸

(2.1)

(2.2)

x
p

q “

(2.3)

Y. Wang and D.N. Politis/Model-free Bootstrap for Stationary Series

9

where

j

i
|

´

| “

k.

Remark 1. It is worth noting that the above estimator is not guaranteed to
be positive deﬁnite (PD) for ﬁnite samples, but can be corrected towards PD
by looking into its Cholesky decomposition; for details see McMurry and Politis
(2010), McMurry and Politis (2015). Asymptotically, the estimator is PD with
probability tending to one, and the corrected estimator enjoys the same rate
of convergence as the original estimator. Either way, it is not a problem in
asymptotic studies.

not be used in the calculation of

While convergence result exist for

is latent and can
t
Σn. We can only use the estimator with the
˜
Zt deﬁned in Section 2.1 that are calculated from original data Yt,

Σn, the true series

estimated

Zt

p

u

resulting in an estimator
p

p
Σn, where

Σp

pp
κ,l
i, j
qn
p

q “

κl

k
p

q ˜

pp

1
n

k

n

´

1
t
ÿ
“

˜
Zt

˜
Zt

`

.

k

¸

p

p

(2.4)

Consistency of

Σn to Σn will be shown in the following.

3. Assumptions and preliminary results

pp

3.1. Acceptable forms of

Yt

t

u

The stationary series

Yt

t

u

in our setting will be assumed to have the form:

Yt

f

Wt
p

q

“

(3.1)

“

where f is some continuously diﬀerentiable function such that the CDF of Yt
FY is strictly increasing and continuously diﬀerentiable, and
is a strictly
stationary Gaussian process. Without loss of generality, we may assume that
0 since the mean of Wt can be incorporated in the function f .
EWt

Wt

u

t

Equation (3.1) is a common form of extension from Gaussian series to non-
Gaussian case. It has been used in the study of long range dependence, as well
as analyzing time series with heavy tails; see Samorodnitsky and Taqqu (1994).
This assumption also ﬁgures in a completely diﬀerent setting, namely that of
Bayesian machine learning; see Snelson, Ghahramani and Rasmussen (2004).

By the Wold decomposition (see Brockwell and Davis (1991)) coupled with

Gaussianity, Wt admits the following expansion

Wt

“

8

0
j
ÿ
“

ajǫt

´

j

`

Vt

(3.2)

where ǫt
. If
we assume Wt is purely nondeterministic, then Vt vanishes, and it is clear that

, and Vt is a deterministic process independent from
q

0, 1
p

N

ǫt

t

u

i.i.d.
„

Y. Wang and D.N. Politis/Model-free Bootstrap for Stationary Series

10

h

Yt is of the form Yt
for some function h. Such representation
naturally appears in many time series and dynamical system models, and is also
a common form used for developing short range dependence conditions; see Wu
(2005a) for details.

1, ǫt

p¨ ¨ ¨

, ǫt

“

´

q

The following lemma holds.

Lemma 3.1. Let Zt
˝
and only if Yt admits the representation in equation (3.1).

. Them Z1,
q

Yt
p

Φ´

FY

¨ ¨ ¨

“

1

, Zn are jointly normal if

1

1

f

˝

˝

˝

˝

Φ´

FY

“
FY

Wt
p

. Since both Wt and Zt are nor-
Proof. If (3.1) holds, Zt
q
mally distributed, Φ´
f is a normality preserving, continuously diﬀer-
entiable transform. Therefore the transform is linear by Corollary 2 of Mase
(1977). Therefore, each Zt is linearly transformed from Wt, t
.
u
Collecting the n instaneous linear transforms we obtain a transform of the vec-
are jointly
tor
normal and a linear transform preserves normality,
is multivariate
normal; see Lemma 3.1 of Das and Politis (2020).

W1,
p
Z1,
p

that is linear. Since

W1,
p

Z1,
p

¨ ¨ ¨
, Zn

, Wn

, Wn

, Zn

1, 2,

P t

¨ ¨ ¨

¨ ¨ ¨

¨ ¨ ¨

¨ ¨ ¨

, n

to

q

q

q

q

Conversely, given Zt

we have Yt

“

F ´

1
Y ˝

Φ

FY

Φ´
“
q
Zt
, which is of the form (3.1).
q
p

Yt
p

˝

1

is jointly normal and strict stationary,

The above lemma essentially clariﬁes the scope of time series models the
Φ is monotone, it is rea-
is equivalent
modulo aﬃne diﬀerences in their arguments. Thus we have the

model-free algorithm can be applied to. Since F ´
Y
sonable to believe that the function f is monotone. In fact, f
to F ´
following lemma:

1
Y ˝

p¨q

p¨q

Φ

˝

1

Lemma 3.2. Under the setup in equation (3.1), given FY is strictly increasing,
then f is a strictly monotone function.

Remark 2. One large subclass of strictly monotone, continuously diﬀerentiable
functions is an f with derivative bounded away from 0. As it turns out, this
subclass can simplify our short range dependence assumption to be introduced
next.

3.2. Short range dependence (SRD) assumptions on

Yt

t

u

The SRD assumption is necessary for both consistency of mentioned transforms
and central limit theorems. Finding the appropriate SRD assumption is a core
problem for proving bootstrap validity in our setting since direct calculation of
bootstrap variances is almost impossible. We must take into consideration the
special characteristics of both our bootstrap procedure and bootstrap pseudo
data and ﬁnd the SRD assumption that can be well incorporated in the proofs.
Thus the SRD condition should fulﬁll the requirement that with it assumed:

1. Consistency of proposed transforms can be established in proper mathe-

matical forms;

2. Certain central limit theorems can be established.
3. Consistency of bootstrap variance, as further explained in Section 4.

Y. Wang and D.N. Politis/Model-free Bootstrap for Stationary Series

11

One of the most widely used SRD assumptions for strictly stationary time
series is the strong mixing condition introduced by Rosenblatt (1956), and ex-
tensively studied since. Many useful results are available for series under certain
mixing rates. However such conditions are often hard to verify for general time
series models. More recently, Wu (2005a) introduced the physical dependence
Lp with
measure described in the following. Assume Yt
ǫt

´
Fǫ; then, the physical dependence measure is deﬁned by:

1, ǫt

p¨ ¨ ¨

q P

, ǫt

“

h

i.i.d.
„

where Y 1j “
´
Yt is called strongly p

p¨ ¨ ¨

, ǫ

h

1, ǫ10,

, ǫj
¨ ¨ ¨
stable if

´

with ǫ10

q

δp

j
p

q “

(cid:13)
(cid:13)Yj

Y 1j

(cid:13)
(cid:13)p

(3.3)

Fǫ independent of

. The series

ǫt

t

u

´

„

C0
p

q

∆p

“

8

δp

j
p

q ă 8

.

(3.4)

0
j
ÿ
“
The p-stable assumption was able to provide the ﬁrst results on the conver-

gence of banded and/or tapered autocovariance matrix estimators by Wu and Pourahmadi
(2009) and McMurry and Politis (2010). Yet results for functional central limit
theorem for the empirical process–which relates to uniform consistency of empiri-
cal CDF–can not be readily established with this assumption, and require further
conditions. To simplify assumptions, we hope for a condition that oﬀers more
ﬂexibility than what is mentioned above. One short range dependence measure
that is gaining interest is the m
approximation assumption developed in a se-
ries of papers in Berkes, Hörmann and Schauer (2009), Berkes, Hörmann and Schauer
(2011) and Hörmann and Kokoszka (2010).

´

The following related conditions are used for proving diﬀerent properties and

are very important in our proofs.
Deﬁnition 3.1. The following diﬀerent m
posed. Let
Y p
series
t

u
that

t
m
q

Yt

be a strictly stationary time series, there exists an m

approximation conditions are pro-
dependent

´

´

u

t
(C1) @t P Z, P p|Yt´Y pmq

t

(C2) both Yt and Y pmq
some A ą 0, and such that

t

(cid:13)
(cid:13)Yt ´ Y pmq
(cid:13)

t

| ą γmq ă δm, for some sequence γm Ñ 0 and δm Ñ 0.

are in Lp; Dδpmq : N Ñ R`, satisfying δpmq ! m´A for

(C3) both Yt and Y pmq

t

are in Lp;

8
m“0

Here, the notation "an

!

ř
bn" means lim sup

ď δpmq.

(cid:13)
(cid:13)
(cid:13)p
(cid:13)
(cid:13)Ym ´ Y pmq
(cid:13)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12) Ñ

an
bn

m

ă 8.

(cid:13)
(cid:13)
(cid:13)p

0.

Assumption (C1) was introduced in Berkes, Hörmann and Schauer (2009)
to establish asymptotic behavior of empirical process. (C2) was introduced in
Berkes, Hörmann and Schauer (2011) to establish invariance principle for par-
tial sums. Both are relevant in our setting. (C3) appeared in Hörmann and Kokoszka
(2010) in the context of functional time series. While it is not directly related
to the setup here, there is an important relation between (C3) and the physical

Y. Wang and D.N. Politis/Model-free Bootstrap for Stationary Series

12

dependence measure (C0), under which consistency of tapered autocovariance
matrix estimator can be established.

q

´

h

“

, ǫt

p¨ ¨ ¨

1, ǫt

There are various methods to construct the m-dependent sequence Y p
t

. Typ-
ically one can use truncation, substitution and coupling method on the repre-
sentation Yt
; for details refer to Berkes, Hörmann and Schauer
q
(2009). The coupling construction is most desirable: we replace ǫ by i.i.d. inde-
pendent copies of ǫ1 for times that are at least m steps away from current time t,
i.e. let Y p
m
t
´
dependent, and also has the advantage that it has the same distribution and
moments as Yt. This construction along with (C2) were also used in Wu (2005b)
where it was called the geometric-moment contracting property, assuming a
faster geometric decay rate. We will use it in what follows.

. The resulting Y p
t
q

m, ǫt

is m

1, ǫ1t

p¨ ¨ ¨

, ǫ1t

, ǫt

¨ ¨ ¨

“

1,

h

m

m

m

´

´

`

´

´

q

q

m

The above SRD conditions are related; the following lemma clariﬁes.

Lemma 3.3. Suppose that
then:

t

m

q

Y p
t

u

is constructed by coupling as deﬁned above,

1. Assume (C2) with A

holds with

C

p

“

1
θ q

for some C

0, θ

0, 1

P p

; then (C1)
q

1
p `
m

q

P

Yt

p|

´

Y p
t

m´

C

θ
{

q ă

| ą

ą
C .

m´

2. Assume (C2) with A

1; then (C3) holds.

ą

3. Assume (C3); then (C0) holds.

4. (C2) is preserved (with a new rate) under θ-Lipschitz transforms. To elab-
,

Lipschitz function, i.e., for some constant K and θ
δ

P p
A, then

with δ

0, 1

m

y

s

q

m´

orate, let g be a θ
K
g
g
|

q| ď

x
p

y
p

q ´

´
x
|

(cid:13)
(cid:13)
(cid:13)p ď

θ. If
|

(cid:13)
(cid:13)
(cid:13)Yt

m
p

q

m
p

q !

Y p
t

´

´
(cid:13)
(cid:13)
(cid:13)g

Yt
p

q ´

g

Y p
t
p

m

q

(cid:13)
(cid:13)
(cid:13)p ď
q

Kδ

m
p

θ
q

!

m´

θA.

Proof. See Appendix.

Based on the above lemma, it is convenient that we assume condition (C2)
with appropriate rate since other dependence measures can be derived from it.

We summarize our ﬁrst assumptions as follows.

Assumption 1.

f

“

Wt
p

(A1) Yt

R is a continuously diﬀerentiable func-
tion, and Wt is a (zero mean) stationary Gaussian process with spectral density
bounded and strictly bounded away from 0. Also assume Wt is purely nondeter-
ministic.

, where f : R
q

Ñ

Y. Wang and D.N. Politis/Model-free Bootstrap for Stationary Series

13

(A2) The CDF FY

is strictly increasing, θ-Lipschitz and continuously dif-

ferentiable with density function fY

p¨q

0.

p¨q ą

(A3) Yt
0,
f 1
|

ą

P
| ě

c

D

Lp satisﬁes (C2) with p
c

0.

ą

2, and A to be speciﬁed later. Also,

ą

Or:

(A4) Wt satisﬁes (C2) with p

serves the (C2) property.

ą

2, and A to be speciﬁed later. Also, f pre-

Remark 3. (a). It is possible to relax (A1) and (A2) when f is a possibly
discontinous but strictly monotone function, and FY is an absolutely continuous
function subject to some extra constraints. It can be shown that Lemma 3.1 still
holds under this assumption; see Corollary 3.1 of Das and Politis (2020). Our
simulations in Section 6 also conﬁrms this. However for the purpose of analysis
and to avoid edge results, we require a stronger assumption as in (A1) and (A2).
(b). Assumptions (A3) and (A4) have the same purpose: to establish SRD for
. It turns out with the derivative of the transfer function f strictly
Yt
t
bounded away from 0, SRD of Yt will deduce SRD for the other series; while
otherwise SRD assumption on both Wt and Yt is required. This is because it is
not clear yet what kind of functions f preserve the m
approximable property.
(c). Under assumptions (A1) and (A2), by Lemma 3.2, f is also a strictly

and

Zt

´

u

u

t

monotone function.

Lemma 3.4. Assume (A1)–(A3), then

Wt

t

u

and

Zt

t

u

satisfy (C2).

Proof. Under (A1), (A2) and by Lemma 3.2, f is continuously diﬀerentiable
is bounded,
and strictly monotone. Therefore it is invertible and
1 is a Lipschitz function. Since (C2) is preserved under Lipschitz
which means f ´
transform and Wt
as
q
Yt
also
t
satisﬁes (C2) with same rate δ

also satisﬁes (C2) with same rate δ
m
p
Zt
u

u
. By Lemma 3.1, the transform from Wt to Zt is linear; thus

1
f 1 |

Yt
p

| “ |

Wt

f ´

f ´

,
q

1
q

“

|p

u

t

t

1

1

m
p

.
q

3.3. Preliminary results

Now we quote the necessary theorems relative to our work.

s, t
p

Theorem 3.5. (Berkes, Hörmann, Schauer (2009))
s
I
Deﬁne R
np
p
qq
ď
θ, δm
tinuous, and (C1) holds with γm
{
there exists a two parameter Gaussian process K
t
p

, such that
q

s, s1
p

u ´
m´

FY
C

Γ
q

q “

t
ď

Yk

ř

“

^

t1

s

t

ď

q

1

. Assume FY is θ-Lipschitz con-
4. Then
ą
s1, t1
K
p
q

C and some C
s, t
K
with E
p
p

m´
“
s, t
p

qq “

R

sup
s,t |

s, t
p

q ´

K

s, t
p

q| “

2
n1
o
{
p

log n
p

´
q

α

, a.s.
q

(3.5)

Y. Wang and D.N. Politis/Model-free Bootstrap for Stationary Series

14

for some α

0. In addition,

ą
Γ

Z
k
ÿ
P
is absolutely convergent for all choices of s, s1.

s, s1
p

q “

E

I
p

Y0
p

s

ď

q ´

FY

s
p

I
qqp

Yk
p

ď

s1

q ´

FY

s1
p

qq

(3.6)

P

Remark 4. We are more interested in the case t
n such that the above
theorem reduces to a 1-dimensional centered Gaussian process K indexed by
R. It turns out that tightness of K plays an important role in the accuracy
s
Ut. Although Theorem 3.5 does not guarantee the limiting
of the estimated
Gaussian process to be tight, it is reasonable to believe so and also it can be
shown that for series of the form (3.1) tightness can be shown through continuity
and boundedness of the covariance structure.

“

p

Theorem 3.6. (Berkes, Hörmann, Schauer (2011))
2
p
2, η
1
Assume (C2) with p
, A
´
2η p
q
k
is absolutely convergent. Also,
p

Z γY

P p

0, 1

ą

ą

q

k

P

1

η
`
p q _

´

1. Then σ2

8 “

ř

n

1 p

k
ÿ
“

Yk

EY0

W1

s2
nq `
p

W2

t2
nq `
p

O

np
p

1

`

η

p

q{

, a.s.
q

q “

´

(3.7)

P p

0, 1

bn means limn

. Here an
q

where W1 and W2 are two Brownian motions and s2
b
γ

an
bn “
Theorem 3.7. (McMurry & Politis(2010))
For the tapered estimator deﬁned in equation (2.3) with tapering function (2.1),
assume (C0) with 2
then,

n, t2
n „
a, b
.
u

σ2
8
max
t

cnγ with

n „
“

, with l

q ă 8

n
o
p

4 for

1; a

k
p

and

p´2
p

Z σ

Zt

Ñ8

“

_

ď

„

ă

p

t

u

q

k

P

ř

tcκlu

k
ÿ
“
(cid:13)
(cid:13)
(cid:13)

σ
0 |

k
p

q ´

σ

k
p

q| ÑLp{2

0,

p
κ,l
qn ´

Σp

Σn

(cid:13)
(cid:13)
(cid:13)op ÑLp{2

0.

(3.8)

(3.9)

and

Furthermore, assuming Zt has spectral density fZ satisfying 0
p
c2

q ď
is positive deﬁnite with probability tending to 1, and

0, 2π

w
p

κ,l
qn

fZ

Σp

c1

ď

ă

w

,

,

ă 8

@

P r

s

p

(cid:13)
(cid:13)
(cid:13)p

Σp

κ,l
qn

1

´
q

´

1

Σ´
n

(cid:13)
(cid:13)
(cid:13)op “

op

1
p

q

(3.10)

p

We now proceed to the proof of bootstrap validity for parameter estimation

and prediction.

Y. Wang and D.N. Politis/Model-free Bootstrap for Stationary Series

15

4. Model-free bootstrap validity for estimation

For the following paragraphs, let P ˚ denote the probability in the bootstrap
world. To show that Algorithm 1 produces asymptotically correct bootstrap
conﬁdence interval, the following type of results will be shown in this section:

P ˚

sup
R |
x

P

τn
p

θ˚n ´
p
p

θn

x

q ´

P

τn
p

θn
p

´

θ0

q ď

q ď

x

q|

n
Ñ8
ÝÝÝÑ

0, in probability (4.1)

p

p
Moving forward all convergence is with respect to n
and this notation will
θn. In
be omitted for simplicity. τn is the rate of convergence for the estimator
consistent estimator for θ0; the
many circumstances, τn
´
most important such case is the mean where θ0
θn
1 Yt.
Moreover, the class of functions of linear statistics is most useful, namely when
the statistic can be written as

θn is a ?n

?n and

Y0
p

Ñ 8

n
p
t
“

and

ř

“

“

“

1
n

E

p

q

p

q

p

n

1
k

´

1

`

n

´

1

k

`

1
t
ÿ
“

g

Yt,
p

¨ ¨ ¨

, Yt

`

1

k

´

qq

(4.2)

R˜q and g : Rk
Rq are smooth functions. By the δ-method, the
where q : Rq
Ñ
statistic (4.2) inherits the ?n
consistency of the linear statistic. The class of
statistics of the type (4.2) includes a wide range of estimators in time series, e.g.
sample mean, sample autocovariance and sample autocorrelation, to name a few.
The class (4.2) was investigated by Kunsch (1989), Politis and Romano (1992),
Bühlmann (1997),Lahiri (2003) and Kreiss, Paparoditis and Politis (2011).

Ñ
´

4.1. Bootstrap validity for the mean

In this section, we focus on model-free bootstrap for the mean θ0
q
and extension to statistic of the form (4.2) will be addressed in later section.
Therefore we let τn
?n for later discussions. Typically, convergence of the
“
type (4.1) is proved in a two step procedure:

Y0
p

“

E

1. Asymptotic normality for the estimator and its bootstrapped analogue :

?n
θn
p

E˚
p

d

8p
?n
θ˚n ´
p
p

´
θ˚nqq
p
p

d

8p
,
8p¨

where d
sional) distributions deﬁned by:

¨q

θ0

, N
q

0, σ2
p

, N

0,
p

σ˚nq
p

qq Ñ
2

0, σ2

.

ă 8

(4.3)

0, in probability

(4.4)

qq Ñ
denotes the Kolmogorov metric between two (one dimen-

d

F, G

8p

q “

F

sup
R |
x

P

x
p

q ´

x
G
p

.
q|

and where σ2 and
σ˚nq
p
?n-scaled estimator ?n
θn
p
in the bootstrap world, respectively.

2 are the asymptotic variances of the centered and
θ˚nqq
p
p

θ˚n ´
in the real world and ?n
p
p

E˚

θ0

´

p

q

Y. Wang and D.N. Politis/Model-free Bootstrap for Stationary Series

16

2. Asymptotically equal variances:

σ˚nq
p
Remark 5. It is believed that asymptotic normality is often necessary for the
asymptotic validity of bootstrap for general inference problems, and thus τn
“
?n is also necessary. For a detailed explanation, see Ch. 6 of Dehling and Philipp
(2002).

(4.5)

σ2.

2 P
Ñ

Following the two step procedure, we have the following:

Lemma 4.1. Let
u
t
2
p
η
1 and
1
A
´
`
2η p
p q _
summable. Also,

Yt

ą

´

1

satisfy assumptions (A1)-(A4) with p
0. Then γY
1

p

2

η

`

q{

´

{

ă

ą
k
q
p

1
p

n

Yk

1 p

k
ÿ
“

EY0

d
Ñ

q

N

0, σ2
p

8q

´

0, 1

2, η
,
q
P p
is absolutely

(4.6)

1
?n

with σ2

Z γY

k

k
p

.
q

8 “

P
Proof. By Theorem 3.5, γY

ř

?n

¯Yn
p

´

EY0

q “

since nγ

´

1

Ñ

0 and np

η

`

q{

1
?n

W1

N

“
d
Ñ
1

´
s2
n
n{
p
0, σ2
p
p

q `
.
8q
2
1
{

´

Ñ

0.

is absolutely summable for k

k
p

q

Z. Then

P

W1

s2
nq `
p
W2

W2

t2
n{
p

t2
nq `
p
n

q `

Oa.s.

1

`

η

p

q{

Oa.s.

η

p

`

q{

´

np
p
np
p

1

q
¯
2
1
{

(4.7)

q

Lemma 4.2. Under the assumptions of Lemma 4.1, and the additional assump-
tion A
1
p `

4, we have

1
θ ą

with C

4, and

ą

P

Yt

p|

´

m

q

Y p
t

| ą

m´

C

θ
{

q ă

C

m´

sup
n
t
s
Pr

Ut
|

´

Ut

| “

Op

1
?n q

p

Proof. See Appendix.

p

(4.8)

(4.9)

“

Ut

FY

Yt
p

Remark 6. The above lemma argues that the uniform error for the ﬁrst step
transform
rate. This is important as consistency of
˜
Zt to Zt, which itself

is of Op

1
?n q

Σn to Σn depends on the consistency of the estimated
depends on the consistency of
pp

Ut to Ut.

To achieve the same result as in Lemma 4.2 with kernel estimators

p

p

q

p

assume the following:

p

Fh, we

p

Y. Wang and D.N. Politis/Model-free Bootstrap for Stationary Series

17

Assumption 2. (A5) FY is 2nd order continuously diﬀerentiable such that
is bounded and Hölder continuous.

x

2

q

d2FY
p
dx2

(A6) The kernel K is a distribution function with density k satisfying:

x
F p
qY p

q “

is bounded.

1. k
2. k
3.

x
q
p
x
k
x
.
q “
p
q
p´
pk
dx
x
x
ă 8
q
p
|
|
(A7) The class of functions FK

.

R

x
is Donsker with respect to
ş
q
´
under condition (C1). In other words, there exists a tight Gaussian process
u
, x
x
q
p

R, such that

x0
p

“ t

: x0

K

P

P

u

Yt
t
Gh

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
?n

sup
R
x

P

n

K

1 ˆ

k
ÿ
“

Yk

x

p

´
h

q ´

Y0

x

p

´
h

E

K

ˆ

q
˙˙

´

Gh

(cid:12)
(cid:12)
(cid:12)
x
(cid:12)
(cid:12) “
q
p

op

2
n1
{
p

.
q

(4.10)

In the above, the covariance of Gh

is given by

x
q
p

ΓGhp

x, x1

q “

E

K

Y0

x

p

´
h

E

K
p

p

q ´

Y0

x

´
h

K

qq

˙ ˆ

Yk

x1

p

´
h

E

K
p

p

q ´

Yk

x1

´
h

.

qq

˙

Z
k
ÿ
P
4
1
{

ˆ

.
q

(A8) h

“

n´
o
p

p

1
?n q

error rate for the ﬁrst transform. It is not yet clear if the m

Remark 7. The purpose of assumption (A7) is to help establish tightness
for the supremum of the empirical process, which helps establish the uniform
Op
approximable
condition can lead to (A7); however under similar dependence conditions, (A7)
is shown to be correct. For example Theorem 2.1 in Arcones and Yu (1994) es-
tablished such a result for β
mixing stationary series with FK a V-C subgraph
class of functions.

´

´

Lemma 4.3. Let

h

U p
qt “
p

Proof. See Appendix.

Fh

Yt
p

. With assumptions (A1)–(A8) holding,
q

Ut

| “

Op

1
.
?n q

p

p
sup
n
t
s
Pr

h

U p
qt ´
|
p

With assumptions (A1)-(A8) along with Lemma 4.2 and 4.3 holding, we can
prove MFB validity results using either the empirical CDF estimator ¯F or the
Fh. Therefore in the following results, we only consider a
kernel CDF estimator
F that can represent either ¯F or
general CDF estimator
p

Theorem 4.4. Assume the conditions of Lemma 4.2 or 4.3, with appropriate
rate for l

. Then

and c

Fh.

p

n
p

q Ñ 8

k
p

q ´

σ

k
p

q|

0

P
Ñ

(4.11)

p
n
p

q Ñ 8
tcκlu

σ
0 |

pp

k
ÿ
“

Y. Wang and D.N. Politis/Model-free Bootstrap for Stationary Series

18

which implies

and

(cid:13)
(cid:13)
(cid:13)
(cid:13)

Σn

(cid:13)
(cid:13)
(cid:13)
(cid:13)op

0

P
Ñ

Σn

´

pp
Σ´

(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
n ´

1

Σ´
n

(cid:13)
(cid:13)
(cid:13)
(cid:13)op

0.

P
Ñ

(4.12)

(4.13)

Proof. See Appendix.

pp

With the above results, we can show that the pseudo data generated by MF
and LMF bootstrap procedure converge in distribution to the true distribution,
in probability. Before proceeding, we state one more assumption:

Assumption 3. (A9)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

Σ

1
2
{
n

´

1
2
{
n

Σ

(cid:13)
(cid:13)
(cid:13)
(cid:13)op “

op

1
p

q

and

(cid:13)
(cid:13)
(cid:13)
(cid:13)

Σ´
n

2
1
{

´

2
1
{

Σ´
n

(cid:13)
(cid:13)
(cid:13)
(cid:13)op “

op

1
p

.
q

pp

Assumption (A9) holds if we have an additional mild convergence rate for
the estimator towards the true covariance matrix, which can be achieved by
choosing appropriate tapering parameter l
and normal thresholding
q Ñ 8
parameter c
. By Theorem 2.1 of Drmac, Omladic and Veselic (1994),
if

q Ñ 8

n
p

n
p

pp

log n
p

2
q

Σn

Σn

´

0

P
Ñ

(4.14)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)
(cid:13)op

then (A9) holds; see also Jentsch and Politis (2015) and the Corrigendum of
McMurry and Politis (2010)).

pp

Lemma 4.5. Assume (A1)-(A8). (1) For LMF bootstrap,

N,

d

@

P

Y ˚t1 , Y ˚t2 ,
p

¨ ¨ ¨

, Y ˚td q

d˚
Ñ p

Yt1 , Yt2 ,

¨ ¨ ¨

, Ytd q

, in probability.

(4.15)

(2) Further assume (A9). Then the model-free procedure is asympototically

equivalent to limit model-free procedure. i.e., the inﬁnite sequence

converges in distribution to

ξ˚1 , ξ˚2 ,
p
where
thermore, equation (4.15) holds for the model-free bootstrap.

is an inﬁnite sequence with entries being i.i.d. N

ζ1, ζ2,
p

ζ1, ζ2,
p

in probability

¨ ¨ ¨ q

¨ ¨ ¨ q

¨ ¨ ¨ q

(4.16)

0, 1
p

. Fur-
q

Proof. See Appendix.

k
Let γ˚Y p

long-run variance in the bootstrap world is then

and Γ˚n denote the bootstrap analogues of γY
k

q

σ˚
p

2
8q

“

and Γn. The
k
p
q
k
Z γ˚Y p
P

.
q

Theorem 4.6. Under (A1)-(A9). For both MF bootstrap and LMF bootstrap

ř

σ˚
p

2
8q

σ2
8

Ñ

, in probability.

(4.17)

and

sup
R
x

P

(cid:12)
(cid:12)P ˚

?n
p

¯Y ˚n ´

p

E˚

Y ˚t qq ď
p

x

q ´

P

¯Yn
?n
p
p

E

Y0
p

´

qq ď

(cid:12)
x
(cid:12)
q

P
Ñ

0.

(4.18)

Proof. See Appendix.

Y. Wang and D.N. Politis/Model-free Bootstrap for Stationary Series

19

4.2. Bootstrap validity for smooth functions of linear statistics

Now that the model-free bootstrap validity for the mean has been shown, we can
extend the result for statistics of the form (4.2). For convenience, denote Xt
1
g
Xt
Xt
Yt,
q
p
p
p
¨ ¨ ¨
and θ˚0
analogue
, respectively. In order
to extend previous results using same proof strategy, we need assumptions such
ř
that the key points in the proof are checked:

, Yt
θ˚n “
p

“
with bootstrap

and θ0
g
E˚
p

E
g
“
p
X ˚t qq
p

1
n
k
´
`
n
k
`
´
1
t
“

X ˚t q
g
ř
p

θn
1
k
p
`

n
k
´
1
t
“

,
q
n

“
1

“

qq

1
1

´

´

`

`

k

1

Assumption 4.

(A10) q
of θ0, and

x
q
p
q
i
“

: Rq
q

1

pB

Ñ
xi
{B

θ0xi does not vanish.

x
q|

“

R˜q has continuous partial derivatives in a neighborhood

ř
“ p

g1,

(A11) g
, gq
approximable in the sense of (C2) with p

m
and appropriate constant A such that Lemma 4.1 holds for all gi

2 and gi

is a continuous function and

i
@
m
X p
t
p

ď
q
q

¨ ¨ ¨

ą

´

q

q

is Lp

q, gi
Xt
p
its m
approximation,
´
Xt
.
q
p

´

Theorem 4.7. With (A1)- (A11) hold, for MF bootstrap and LMF bootstrap
with empirical CDF ¯F and kernel smoothed CDF

Fh,

P ˚

sup
x

R ˜q |

P

?n
q
p
p

θ˚nq ´
p
p

q

θ˚0
p

qq ď

x

q ´

P

?n
p

q
p

θn
p

p
q ´

q

θ0
p

qq ď

x

q|

P
Ñ

0.

(4.19)

Xt
p

The proof is similar to Theorem 4.6 by checking the key points listed there
for g
in combination with Cramér-Wold device and the δ-method, therefore
is omitted. The mean is now a special case of Theorem 4.7. Moreover, it can be
applied to more general statistics such as autocovariance and/or autocorrelation.
We will address the autocovariance case in the following.

q

p

4.2.1. Autocovariances

1,

“ p

Yt, Yt

1, g
For simplicity, let Xt
k
¨ ¨ ¨
`
`
and q is the identity map. Then 1
is a version of an autocovari-
k
n
´
0 .
ance estimator that estimates up to lag k autocovariances, assuming EY0
ř
Note that we can also design g and q more carefully such that we get the usual
autocovariance estimators. Then the following corollary holds:

Rk
`
Xt
p

q P
`
n
k
1 g
´
t
“

Xt
p

, YtYt

q “ p

YtYt

, Yt

¨ ¨ ¨

“

1,

`

q

k

q

Corollary 4.7.1. Assume Yt
4 such that (C2) is satisﬁed with appro-
2.
priate rate A, then g
Then by Theorem 4.7 the model-free bootstrap procedure is asymptotically correct
for bootstrapping vector of autocovariances.

as deﬁned above also satisﬁes (C2) with p1

p
2 ą

Xt
p

“

ą

P

q

Lp p

Y. Wang and D.N. Politis/Model-free Bootstrap for Stationary Series

20

Proof. Let Y p
t

m

q

be the approximation series generated by coupling. Then

(cid:13)
(cid:13)
(cid:13)YtYt

`

m

Y p
t

q

m
q
k

Y p
t
`

k

´

(cid:13)
(cid:13)
(cid:13)p1 “

ď

“

“

(cid:13)
(cid:13)
(cid:13)YtYt

kYt

`

2 kYtkp

k

`

´
(cid:13)
(cid:13)
(cid:13)Yt
kkp
(cid:13)
(cid:13)
(cid:13)Yt

Y p
t

m

q

Yt

`

k

`

q

(cid:13)
(cid:13)
(cid:13)p `
(cid:13)
(cid:13)
(cid:13)p

m

Y p
t

´

m

q

Y p
t

´

Yt

m

m

q

Y p
t
(cid:13)
(cid:13)
(cid:13)Y p

t

k

`
(cid:13)
(cid:13)
(cid:13)p

q

´
(cid:13)
(cid:13)
(cid:13)Yt

m

q

Y p
t

m
q
k

Y p
t
`

k

´

`

Y p
t
`

(cid:13)
(cid:13)
(cid:13)p1
(cid:13)
m
(cid:13)
q
(cid:13)p
k

A

m´
o
p

q

(4.20)

To complete proof, we apply Theorem 4.7.

Remark 8. (a). The usual assumption of ﬁnite sum of 4th order cumulants of
Yt that ensures ﬁniteness of asymptotic variance can be dropped since it can be
deduced by checking assumption (A11) along with Lemma 4.1.

(b). Notably, Kreiss, Paparoditis and Politis (2011) showed that the autore-
gressive(AR) sieve bootstrap procedure does not work in general for bootstrap-
ping autocovariances of strictly stationary series (even for linear series that
are not Gaussian) because of inconsistency of the limiting variance associated
the companion AR process. In essence, AR-sieve bootstrap (and the previously
mentioned Linear Process bootstrap) mimic correctly the ﬁrst and second order
moment structure of Yt; if the statistic of interest has a large-sample distribu-
tion that depends on higher order moments, the AR-sieve bootstrap (and the
LPB) may fail. However, bootstrap validity holds for the MFB due to the as-
sumption that Yt
where Wt is a Gaussian process whose covariance
structure can be consistently estimated (up to aﬃne transform). Since a Gaus-
sian process is fully described by its second order moment property, consistency
of higher moments can also be obtained by the bootstrap procedure. Therefore
the model-free bootstrap also works for higher order statistics.

Wt
p

“

f

q

4.3. Approximately linear statistics and sample quantiles

Now consider a statistic

ηn that can be expressed as

p
ηn

“

1
n

n

1
t
ÿ
“

g

Yt
p

q `

op

1
p

{

?n
.
q

(4.21)

p
1
n

“

θn

1 g

n
t
“

Under aforementioned conditions, the model-free bootstrap can be applied to
θn will be ?n-consistent (under the
Yt
the linear statistic
. Since
q
p
θn are asymptotically equivalent,
ηn and
required conditions), it follows that
p
p
θ0
i.e., ?n
has the same asymptotic distribution with ?n
where
´
p
θ0
Yt
E
. Therefore, since the model-free bootstrap works to approximate
qq
p
the distribution of ?n
, it will also work to approximate the distribution
p
q
ηn
of ?n
p

Focusing on sample quantiles, we have the following result:

ηn
p
g
p

θn
p

θn
p

.
q

ř

θ0

θ0

θ0

´

´

“

´

p

p

q

q

p

p

Y. Wang and D.N. Politis/Model-free Bootstrap for Stationary Series

21

Lemma 4.8. Assume (C2) for
¯F ´
; up
un,p
q
in Wu (2005b):

1
p
Y p

p
p

F ´

“

“

1

Yt

with appropriate rate A as well as (A2). Let
. Then for the Bahadur-Kiefer process mentioned
q

t

u

We have

p

@

0, 1

P p

, αn
q

“

αn

“

(cid:12)
(cid:12)fY

up
p
Op

rn
p

qp

q

up

p
q ´ p

un,p

´
where rn

¯F

´
2
1
n´
o
{
p

up
p

(cid:12)
(cid:12) .
qq

. As a result,
q

(4.22)

“

¯F
up
p
up
p
Under additional assumptions of (A5)-(A8), then the above equation also holds
for un,p

n´
p

(4.23)

un,p

´
fY

2
1
{

F ´

.
q

up

op

“

`

´

p

q

q

1
p
h p

, i.e.
q

“

p

p

up

Fh

un,p

up
p
up
p
p
Remark 9. As shown in Wu (2005b), for the empirical CDF estimator ¯F , a
stronger version of equation (4.23) holds provided a faster geometric convergence
rate for the m

approximation assumption (C2).

n´
p

(4.24)

´
fY

2
1
{

.
q

op

“

`

´

q

q

´

In equation (4.23) and (4.24), p
´
fY

F

up
p
up

q

q

is of the linear form in equation (4.21).

For equation (4.23),

p
p

p

´
fY

¯F
up
p
up
p

q

q

1
n

“

n

1
k
ÿ
“

FY

up
p

q ´
fY

Yk

I
t
up
p

q

up

u

;

ď

the same holds for equation (4.24) by the relation

p

´
fY

Fh

up
p
up
p
p

q

1
n

q

“

n

FY

up
p

1
k
ÿ
“

Yk

´
h

q

.

up

K
p
q ´
up
fY
q
p

by previous analysis, we have the following theorem.

Theorem 4.9. Under assumptions of Theorem 4.7 with respect to the linear part
p
and with Lemma 4.8 holding, both MF bootstrap and LMF bootstrap are
´
fY

up
p
up

F

q

asymptotically valid for sample quantiles.

p
p

q

A special case of interest is to let p

1
2 . Then, the above discussion shows

validity of the model-free bootstrap for the sample median.

“

4.4. Bootstrap validity for kernel smoothed spectral density

Another important parameter of interest is the spectral density evaluated at
ω
some frequency ω. For this subsection, let fsp.d
denote the spectral density
q
p
ikω. The kernel smoothed
k
Z γY
e´
of
p
q
in this setting. Let In
ω
estimator is a widely used estimator of fsp.d
q
p

which is deﬁned as fsp.d

ωj
p

ω
p

q “

q “

1
2π

Yt

u

t

k

P

ř

Y. Wang and D.N. Politis/Model-free Bootstrap for Stationary Series

22

(cid:12)
(cid:12)

2
ikωj (cid:12)
(cid:12)

Yke´

1
2πn
su
are the Fourier frequencies. The kernel smoothed spectral density estimator is
ř
deﬁned by:

denote the periodogram of

where ωj

2πj
n , j

P t

P r

Yt

n

t

u

fsp.d

ω
p

q “

˜κh

ω
p

ωj

In
q

ωj
p

.
q

´

(4.25)

j

p

n
ÿ
s
Pr
is a kernel function under certain assumptions and ˜κh

where ˜κ
h
ω
.
q
p
q
ω
f ˚sp.dp
be the kernel smoothed estimator based on the bootstrap samples
Let
n
Y ˚t u
1 generated through model-free bootstrap. We would like to show validity
t
t
“
for this procedure. Besides key assumptions for previous theorems to hold, we
p
also need the following:

p¨q “

h´

1˜κ

p¨{

q

Assumption 5. (A12)

Yt

t

u

satisﬁes (C2) with p

4; inf ω

π,π

s

Pr´

fsp.d

ω
p

q ą

0.

“

(A13) The kernel ˜κ
1 and
du
q

u
p

“

˜κ

p¨q

Also,

is a symmetric and bounded square integrable function.
u2˜κ

.

u
p

du
q

ă 8

ş
(A14) h

ş
0 and nh
Ñ 8
Then the following holds.

Ñ

.

Theorem 4.10. Under (A1)-(A9) and (A12)-(A14), for any ﬁxed ω

π, π

s

P r´

(4.26)

˜κ2

u
p

du
q

q

ş

In the above, σ2
π
for ω

p
ω
du for ω
sp.dp
q
Z. Also, for both model-free and limit model-free bootstrap

ω “

ω “

u
p

2f 2

π

R

{

E

fsp.d

ω
p

q ´

q
¯

N

d
0, σ2
.
ωq
Ñ
p
Z and σ2

?nh

fsp.d

´
f 2
p
ω
sp.dp
q

ω
p
˜κ2

ş

{

P

(cid:12)
(cid:12)
(cid:12)P ˚

?nh
p

sup
R
x

P

E˚

q ´

ω
f ˚sp.dp
p

´

ω
f ˚sp.dp
p

ď

q
¯

x

q ´

P

?nh
p

fsp.d

´

p

ω
p

q ´

E

fsp.d

p

ω
p

q
ď
¯
(4.27)

(cid:12)
(cid:12)
x
(cid:12)
q

P
Ñ

0.

Proof. See Appendix.

As is well-known, the bandwidth h governs the trade-oﬀ between bias and
. If we choose h in a way that undersmoothing occurs, i.e.,
ω
fsp.d
variance of
q
p
ω
fsp.d
is of smaller order of magnitude than its standard
when the bias of
q
p
fsp.d
deviation, then equation (4.27) holds true with E
,
q
i.e., we have

replaced by fsp.d

ω
p

ω
p

p

p

q

(cid:12)
(cid:12)
(cid:12)P ˚

sup
R
x

?nh
p

ω
f ˚sp.dp
p

E˚

ω
f ˚sp.dp
p

P

´

q ´

q
ď
¯
(4.28)
Equation (4.28) can then be used to construct model-free conﬁdence intervals for
fsp.d

based on the quantiles of the bootstrap distribution of ?nh

q
¯

q ´

q ´

ď

´

p

fsp.d

ω
p

fsp.d

ω
p

x

P

(cid:12)
(cid:12)
x
(cid:12)
q

p
?nh
p

ω
p

q

E˚

q ´

f ˚sp.dp
ω
p

´

0.

P
Ñ

f ˚sp.dp
ω
p

.

q
¯

Y. Wang and D.N. Politis/Model-free Bootstrap for Stationary Series

23

5. Model-free bootstrap for prediction

For simplicity, we focus on proving model-free bootstrap validity for one-step
ahead prediction; generalizing to h- step ahead case is also possible. First of all,
we list the following deﬁnitions relevant to predictive setting:

Deﬁnition 5.1. (Predictive distribution) Let Yn
of the time series dataset
is called the predictive distribution.

n
t
“

Yt

u

t

`

1 be the 1- step ahead value
Y n
|

1 of Yn

`

`

1

1. The conditional distribution Dn

Deﬁnition 5.2. (Predictive root) Let
n
Yn
t
`
“
Yt
u

Yn
`
1, i.e.,
p
1. The predictive root is deﬁned as

1 based on the original series
n
t
“

Yt

u

t

t

1 be the one step ahead predictor of
1 depends entirely on the data

Yn

`

p
1.

(5.1)

Yn

`

1

Yn

`

´

We wish to approximate the distribution of the predictive root by the boot-
p
2 quantile and right
2 quantile of the (conditional on Y n) distribution of the predictive

strap procedure. Let Lα
2 be the left lower α
2 and Rα
{
{
{
upper α
{
root. Then, an exact two-sided 1

α prediction interval for Yn

1 is:

`

in other words,

Yn
p

`

1

p

Yn

`

1

Rα
2
{

;
q

`

p

´
2,
Lα
{

`

P

Yn
p

`

1

`

Lα
2
{

Yn

`

1

Yn

`

1

Rα
2
{

Y nq “
|

1

`

´

α.

ď

ď

p

Deﬁnition 5.3. (Bootstrap validity for prediction interval) Let Y ˚n
1 be the one-
`
Y ˚n
step ahead value generated through bootstrap procedure. Also let
1 be the
`
1 conditioning on Y n with its formula estimated
one step ahead predictor of Yn
`
n
Y ˚t u
by the bootstrap samples
1. We say that boostrap validity for prediction
t
t
“
0, 1
intervals holds if for any α
P p

p

p

L˚α
2
Ñ
{
in probability. Here L˚α
2 denotes the relative α
2 and R˚α
{
{
{
to the conditional distribution of the bootstrap predictive root Y ˚n
`

Rα
2
{

Ñ

In other words, the two-sided approximate 1

2 quantiles with respect

1

´

1.

Y ˚n
`

α bootstrap prediction interval

p

´

:
q
2 and R˚α
Lα
2
{
{

for Yn

`

1 is:

`
while equation (5.4) would imply

`

Yn
p

1

p

2,
L˚α
{

Yn

`

1

R˚α
2
{

,
q

`

p

(5.2)

(5.3)

(5.4)

(5.5)

(5.6)

P

Yn
p

`

1

L˚α
2
{

`

Yn

`

1

Yn

`

1

R˚α
2
{

Y nq Ñ
|

1

`

´

α.

ď

ď

Given previous deﬁnitions, we state the bootstrap validity theorem for pre-
p

p
diction intervals.

Y. Wang and D.N. Politis/Model-free Bootstrap for Stationary Series

24

Theorem 5.1. (Model-free bootstrap validity for 1-step ahead prediction) As-
sume that the conditional distribution of the predictive root Yn
1 is con-
tinuous. With assumptions (A1) - (A9), Algorithms 2 and 3 are asymptotically
valid in the sense of Deﬁnition 5.3.

Yn

´

`

`

1

p

Proof. See Appendix.

Unlike the bootstrap for parameter inference, the conditional distribution of
the predictive root is not asymptotically degenerate. Hence, no central limit
theorems are required for parameter estimates since their associated variabilty
vanishes asymptotically.

Asymptotic validity of bootstrap prediction interval is of less importance than
that of conﬁdence intervals. A good ﬁnite-sample prediction interval should in-
corporate the variance of all estimated features, else it will result into undercov-
erage. However, as stated above, the property of asymptotic validity has nothing
to do with capturing ﬁnite-sample variability in estimation; see also Ch. 2.4.1 of
Politis (2015). In this respect, the performance of prediction intervals must be
quantiﬁed in ﬁnite-sample numerical experiments as in the following section.

6. Numerical results

We look into three data generating models and evaluate the coverage perfor-
mance of our proposed MFB methods with respect to both conﬁdence intervals
and prediction intervals. All three models satisfy equation (3.1) with

f

x
q “ #
p

?
´
´
2
1
x
q
p
`
10

x x

0

0

ă

ě

(6.1)

x

and Wt generated by an autoregressive moving average (ARMA) model, namely

Wt

φ1Wt

´

1

´

´ ¨ ¨ ¨ ´

φpWt

´

p

“

ǫt

θ1ǫt

´

1

`

` ¨ ¨ ¨ `

θqǫt

´

q

with ǫt
of the following ways:

0, 1
p

N

i.i.d.
„

. The three ARMA models considered are as follows: in one
q

1. MA model of order one, with θ1
2. AR model of order one, with φ1
3. MA model of order 30, with θ1
and all other parameters zero.

“ ´
“
“

0.5 and all other parameters zero.

0.5 and all other parameters zero.
10
2, θ2
k2 when 3

1, θk

k

“

“

ď

ď

30,

We analyze the performance of model-free and limit model-free bootstrap

with both empirical CDF estimator and kernel CDF estimator.

6.1. Performance for bootstrap conﬁdence intervals

For conﬁdence intervals, the parameter of interests are: the mean for all three
models; lag 1 autocovariance for model 1 and 2; lag 2 autocovariance for model

Y. Wang and D.N. Politis/Model-free Bootstrap for Stationary Series

25

3. As a comparison, we benchmark our methods with the two most popular
alternative methods, i.e. block bootstrap and AR-sieve bootstrap.

The mean is, of course, a linear statistic. The lag-k autocovariance can be

written in the form (4.2) with

g

Yt,
p

¨ ¨ ¨

, Yt

`

k

q “

YtYt
Yt

`

„

k



and

q

1

k

n

´

1

k

n

´

p

n

k

1
t
ÿ
“

1

k

n

´

Yt

2.

s

1
t
ÿ
“

g

Yt,
p

¨ ¨ ¨

, Yt

k

`

qq “

n

k

YtYt

k

`

´ r

n

k

1
t
ÿ
“

´

´

´
Although we can do block bootstrap on the Yt and recompute
γk on the boot-
strap data to construct conﬁdence intervals, this procedure will suﬀer from end
eﬀects resulting into an estimator that is biased towards 0. As a remedy, let
as we did in Section 4.2, then a block bootstrap on the Xt
Xt
data will relieve this problem and it is equivalent to the so-called blocks-of-blocks
bootstrap on the Yt data; see Politis and Romano (1992), as well as Paradigm
12.8.11 of the book by McElroy and Politis (2019). For our purposes, we used
5 that can be used to capture autocovariances
ﬁrst level of blocking with k
up to lag 4.

“ p

, Yt

Yt,

¨ ¨ ¨

“

p

`

q

k

const

For the block bootstrap on the Xt data, we can choose the block size b as
3 where const is selected according to Patton, Politis and White
n1
b
{
(2009). As for the AR-sieve bootstrap, the order p of the ﬁtted AR model is
selected through minimizing the AIC; see Bühlmann (1997).

“

˚

P t

100, 200, 500, 1000

The metric we use for comparison is the empirical coverage rate for the boot-
1000 be the number of replicated experi-
strap intervals. To elaborate, let N
250
ments, n
be the length of samples, α
“
the number of bootstrap replications. So for experiment i
, we gen-
erate a time series sample of length n, and use diﬀerent methods to construct a
1
based on the B bootstrap replicates.
Li, Ri
p
Let θ0 denote the true parameter of interest; then the empirical coverage rate
for the whole experiment is:

α bootstrap conﬁdence interval

0.05 and B
1, . . . , N

“
P t

“

´

u

u

q

CV Rθ0 “

1
1000

1000

1
i
ÿ
“

Iθ0

Li,Ri

q

Pp

(6.2)

The purpose of looking at CVR is two fold: ﬁrst, comparing CVR values in a
ﬁnite-sample size setting will tell which method has better performance. Second,
as the sample size n gets large, asymptotic validity can be observed by checking
whether the empirical CVR converges to the nominal 100
% percent, which
α
q
´
will provide numerical grounds to our previous conclusions.

1
p

The following tables contains CVR results for diﬀerent settings all with nom-
0.05 (95% conﬁdence interval). The acronyms in the “Method" column
inal α
of Table 1 have the following meaning. "MF" for model-free bootstrap; "LMF"
for limit model-free bootstrap; "emp" for empirical CDF estimator used in the

“

Y. Wang and D.N. Politis/Model-free Bootstrap for Stationary Series

26

Method

n=100

200

MF-ker
LMF-ker
MF-emp
LMF-emp
BB
AR-sieve

MF-ker
LMF-ker
MF-emp
LMF-emp
BB
AR-sieve

MF-ker
LMF-ker
MF-emp
LMF-emp
BB
AR-sieve

92.1
92.3
91.6
92.0
90.4
94.0

91.4
88.5
88.7
85.3
85.2
90.6

87.9
86.9
82.7
80.9
82.2
83.6

model 1

92.5
92.5
92.4
92.9
93.1
93.6

model 2

92.4
92.1
90.9
88.4
89.6
92.9

model 3

89.3
88.1
85.5
85.8
85.4
88.1

500

93.2
93.0
93.1
93.5
94.1
94.0

93.2
93.3
92.8
92.4
91.3
92.8

90.3
90.5
89.6
89.5
87.3
90.0

1000

2000

93.6
94.7
94.0
93.8
95.0
95.2

93.2
93.3
94.1
93.0
92.3
93.6

91.7
92.1
91.4
91.4
89.8
91.9

94.8
93.9
94.4
94.0
95.0
95.6

93.7
94.4
92.9
92.9
93.0
94.2

93.3
93.1
92.7
92.8
91.4
92.2

Table 1
Empirical CVR for the mean parameter across 3 models

bootstrap procedure; "ker" for kernel CDF estimator; "BB" for block bootstrap;
"AR-sieve" for the autoregressive sieve bootstrap.

For the mean parameter (see Table 1), all the methods being compared are
theoretically valid for all three models. We can observe that for each method, as
n increases the coverage rate approaches 95% albeit the speed of convergence can
be diﬀerent. The AR sieve holds an obvious advantage against other methods
100, whereas the proposed
for model 1 with coverage close to 95% even at n
model-free bootstrap works on par with the block bootstrap. However for the
other two models, MF and LMF bootstrap with kernel CDF estimator has a
noticeable advantage over AR-sieve and block bootstrap, especially for model 3
which has a more complex data generating process.

“

Interestingly, the behavior of model-free bootstrap using the empirical CDF
is almost the same with block bootstrap. However the advantage goes away for
large n which is fortunate since calculating the quantile inverse of a kernel CDF
(which is needed in the bootstrap algorithm)is computationally expensive for
large sample size.

Tables 2 and 3 present the empirical CVR for the autocovariance with dif-
ferent models. Both lag 1 and lag 2 autocovariances were considered; for con-
ciseness, we present lag 1 results from models 1 and 2 (see Table 2), and lag 2
results from model 3 (see Table 3). Recall that the AR-sieve bootstrap is not
asymptotically valid in general for the autocovariance. Comparing Tables 2 and
3, we see that the AR-sieve bootstrap works well for the autocovariance in mod-
els 1 and 2 but not with data from model 3; see Table 3 where the AR-sieve

Y. Wang and D.N. Politis/Model-free Bootstrap for Stationary Series

27

Method

n=100

200

MF-ker
LMF-ker
MF-emp
LMF-emp
BB
AR-sieve

82.0
89.6
80.0
89.2
86.0
94.3

model 1

88.2
94.3
88.3
93.5
88.8
94.8

model 2

500

90.5
94.0
90.1
94.0
91.5
94.6

1000

2000

92.2
94.8
91.6
95.0
91.1
96.3

91.1
94.1
92.0
93.3
93.0
95.0

MF-ker
LMF-ker
MF-emp
LMF-emp
BB
AR-sieve

86.5
94.2
82.9
82.5
86.2
91.8
Table 2
Empirical CVR for lag 1 autocovariance for the ﬁrst 2 models

75.0
92.2
68.6
76.9
82.3
88.3

94.6
94.0
91.9
91.6
90.5
95.5

94.2
95.4
90.2
89.0
89.0
95.1

94.6
93.4
94.2
94.4
91.5
95.5

CVR appears to converge to around 86% instead of the 95% nominal level.

We can also observe asymptotic validity of model-free bootstrap methods
manifesting from Table 2 and 3. Furthermore, model-free methods appear to
enjoy a faster convergence towards nominal compared to block bootstrap. Inter-
estingly, limit model-free with kernel CDF estimator works signiﬁcantly better
than other methods across all 3 models.

Method
MF-ker
LMF-ker
MF-emp
LMF-emp
BB
AR-sieve

n=100
76.9
77.7
68.2
65.1
64.0
69.4

200
87.8
86.0
80.9
77.5
73.9
76.4

500
91.7
91.3
86.8
85.8
83.2
84.9

1000
93.0
92.4
91.6
89.5
87.3
86.1

2000
94.1
93.8
93.5
93.5
89.4
86.1

Table 3
Empirical CVR for lag 2 autocovariance for model 3

6.2. Performance of bootstrap prediction intervals

We now move on to the prediction performance for the three models. The em-
pirical coverage rate is deﬁned similarly as

CV RYn`1 “

1000

1
i
ÿ
“

IY piq

n`1Pp

Y piq
n `

Li,

Y piq
n `

Ri

q

p

p

(6.3)

are sample quantiles of bootstrap predictive root generated by
1 value of the time series sample from the ith

1 is the n

Li, Ri
p

where
bootstrap and Y p
experiment.

q

i
qn
`

`

Y. Wang and D.N. Politis/Model-free Bootstrap for Stationary Series

28

300

500

n=100

Method

95.0
94.1
94.0
92.2
94.4

91.8
93.0
94.2
93.8
93.2

91.6
93.9
92.2
94.0
94.0

MF-ker
LMF-ker
MF-emp
LMF-emp
AR-sieve

200
model 1
94.6
96.6
94.2
93.9
94.6
model 2
94.0
95.3
93.4
90.7
93.2
model 3
94.4
96.7
96.2
94.8
94.8
Table 4
Empirical CVR for prediction intervals across the three models

MF-ker
LMF-ker
MF-emp
LMF-emp
AR-sieve

MF-ker
LMF-ker
MF-emp
LMF-emp
AR-sieve

95.4
95.6
96.4
96.4
93.2

93.8
91.8
92.2
92.8
91.4

94.8
93.6
95.2
93.2
93.0

93.6
90.2
94.1
89.2
94.0

93.2
92.8
94.2
93.6
94.4

92.2
96.4
95.0
95.0
94.0

Since block bootstrap is not a viable method for generating 1-step ahead
prediction value, we benchmark predictive performance of the model-free meth-
ods comparing them with the AR-sieve bootstrap. The bootstrap samples are
generated through a forward bootstrap manner as described in Algorithm 3.1
of Pan and Politis (2016a); see also Alonso, Peña and Romo (2002). Table 4
provides the empirical CVR with n
; the samples sizes are
u
smaller compared to our previous simulations because of the increasing computa-
tional cost for prediction. However, it is reassuring that all methods considered,
i.e., the 4 model-free variations as well as the AR-sieve bootstrap, produce pre-
diction intervals with CVR close to the nominal 95% even with n as low as 200.
It is diﬃcult to do a ﬁner comparison of these 5 bootstrap methods since, for
computational reasons, we had to choose a small number of bootstrap replica-
250). Ongoing work includes devising an analog of the ‘Warp-Speed’
tions (B
method of Giacomini, Politis and White (2013) that will speed up Monte Carlo
experiments involving bootstrap in the case of prediction intervals.

100, 200, 300, 500

P t

“

Acknowledgments. Many thanks are due to Jens-Peter Kreiss, Stathis Pa-
paroditis, and the participants of the August 2015 Workshop on Recent Devel-
opments in Statistics for Complex Dependent Data, Loccum (Germany), where
the seeds of this work were ﬁrst presented. This research was partially supported
by NSF grants DMS 16-13026 and DMS 19-14556.

References

Adler, R. J. (1990). An introduction to continuity, extrema, and related topics
for general Gaussian processes. Lecture Notes–Monograph Series Volume 12.

Y. Wang and D.N. Politis/Model-free Bootstrap for Stationary Series

29

Institute of Mathematical Statistics, Hayward, CA.

Alonso, A. M., Peña, D. and Romo, J. (2002). Forecasting time series with
sieve bootstrap. Journal of Statistical Planning and Inference 100 1 - 11.
Angus, J. E. (1994). The Probability Integral Transform and Related Results.

SIAM Review 36 652–654.

Arcones, M. A. and Yu, B. (1994). Central limit theorems for empirical and
U-processes of stationary mixing sequences. Journal of Theoretical Probability
7 47–71.

Berkes, I., Hörmann, S. and Schauer, J. (2009). Asymptotic results for
the empirical process of stationary sequences. Stochastic Processes and their
Applications 119 1298 - 1324.

Berkes, I., Hörmann, S. and Schauer, J. (2011). Split invariance principles

for stationary processes. Ann. Probab. 39 2441–2473.

Brockwell, P. J. and Davis, R. A. (1991). Time Series: Theory and Methods.

Springer New York, New York, NY.

Bühlmann, P. (1997). Sieve bootstrap for time series. Bernoulli 3 123–148.
Das, S. and Politis, D. N. (2020). Predictive inference for locally stationary
time series with an application to climate data. Journal of the American
Statistical Association. To appear.

Dehling, H. and Philipp, W. (2002). Empirical Process Techniques for De-

pendent Data. Birkhäuser Boston, Boston, MA.

Drmac, Z., Omladic, M. and Veselic, K. (1994). On the Perturbation of
the Cholesky Factorization. SIAM J. Matrix Anal. Appl. 15 1319–1332.

Efron, B. (1979). Bootstrap Methods: Another Look at the Jackknife. Ann.

Statist. 7 1–26.

Giacomini, R., Politis, D. N. and White, H. (2013). A Warp-speed Method
for Conducting Monte Carlo Experiments Involving Bootstrap Estimators.
Econometric Theory 29 567–589.

Hörmann, S. and Kokoszka, P. (2010). Weakly dependent functional data.

Ann. Statist. 38 1845–1884.

Jentsch, C. and Politis, D. N. (2015). Covariance matrix estimation and
linear process bootstrap for multivariate time series of possibly increasing
dimension. Ann. Statist. 43 1117–1140.

Kallenberg, O. (1997). Foundations of Modern Probability. Springer, New

York.

Kreiss, J.-P., Paparoditis, E. and Politis, D. N. (2011). On the range of
validity of the autoregressive sieve bootstrap. Ann. Statist. 39 2103–2130.
Kreiss, J.-P. and Paparoditis, E. (2020). Bootstrap for Time Series: Theory

and Applications. Springer New York.

Kunsch, H. R. (1989). The Jackknife and the Bootstrap for General Stationary

Observations. Ann. Statist. 17 1217–1241.

Lahiri, S. (2003). Resampling Methods for Dependent Data. Springer, New

York.

Li, Q. and Racine, J. S. (2006). Nonparametric Econometrics: Theory and

Practice. Princeton University Press.

Liu, W. and Wu, W. B. (2010). Asymptoticis of Spectral Density Estimates.

Y. Wang and D.N. Politis/Model-free Bootstrap for Stationary Series

30

Econometric Theory 26 1218–1245.

Mase, S. (1977). Some Theorems on Normality-Preserving Transformations.
Sankhy¯a: The Indian Journal of Statistics, Series A (1961-2002) 39 186–190.
McElroy, T. S. and Politis, D. N. (2019). Time Series: A First Course with

Bootstrap Starter. Chapman and Hall/CRC Press, Boca Raton.

McMurry, T. L. and Politis, D. N. (2010). Banded and tapered estimates
for autocovariance matrices and the linear process bootstrap. Journal of Time
Series Analysis 31 471-482.

McMurry, T. L. and Politis, D. N. (2015). High-dimensional autocovariance

matrices and optimal linear prediction. Electron. J. Statist. 9 753–788.

Pan, L. and Politis, D. N. (2016a). Bootstrap prediction intervals for linear,
nonlinear and nonparametric autoregressions. Journal of Statistical Planning
and Inference 177 1 - 27.

Pan, L. and Politis, D. N. (2016b). Bootstrap prediction intervals for Markov

processes. Computational Statistics and Data Analysis 100 467 - 494.

Patton, A., Politis, D. N. and White, H. (2009). Correction to Automatic
Block-Length Selection for the Dependent Bootstrap by D. Politis and H.
White. Econometric Reviews 28 372-375.

Politis, D. N. (2013). Model-free model-ﬁtting and predictive distributions.

Test 22 183-221.
Politis, D. N.

(2015). Model-Free Prediction and Regression: A

Transformation-Based Approach to Inference. Springer, New York.

Politis, D. N. and Romano, J. P. (1992). A General Resampling Scheme
for Triangular Arrays of α-Mixing Random Variables with Application to the
Problem of Spectral Density Estimation. Ann. Statist. 20 1985–2007.

Politis, D. N. and Romano, J. P. (1995). Bias-corrected Nonparametric

Spectral Estimation. Journal of Time Series Analysis 16 67-103.

Politis, D. N., Romano, J. P. and Wolf, M. (1999). Subsampling. Springer

New York, New York, NY.

Rosenblatt, M. (1952). Remarks on a Multivariate Transformation. Ann.

Math. Statist. 23 470–472.

Rosenblatt, M. (1956). A Central Limit Theorem and a Strong Mixing Con-

dition. Proceedings of the National Academy of Sciences 42 43–47.

Samorodnitsky, G. and Taqqu, M. S. (1994). Stable Non-Gaussian Random

Processes. Routledge, New York.

Snelson, E., Ghahramani, Z. and Rasmussen, C. E. (2004). Warped Gaus-
sian Processes. In Advances in Neural Information Processing Systems 16
337–344. MIT Press.

Van Der Vaart, A. W. (1998). Asymptotic Statistics. Cambridge University

Press.

Wu, W. B. (2005a). Nonlinear system theory: Another look at dependence.

Proceedings of the National Academy of Sciences 102 14150–14154.

Wu, W. B. (2005b). On the Bahadur representation of sample quantiles for

dependent sequences. Ann. Statist. 33 1934–1963.

Wu, W. B. and Pourahmadi, M. (2009). Banding Sample Autocovariance

Matrices of Stationary Processes. Statistica Sinica 19 1755–1768.

Y. Wang and D.N. Politis/Model-free Bootstrap for Stationary Series

31

7. Appendix

Proof of Lemma 3.3. 1. Let γm
ity,

“

m´

C

θ and δm
{

“

m´

C . By Markov’s inequal-

P

Yt

p|

´

m

q

Y p
t

| ą

m´

C

θ
{

P

Yt

p|

´

q “

m

Y p
t

q

p
|
q

p
|

m´

θ
Cp
{

q

ą

q

(7.1)

E

p|

m´
m´

ď

!

m

Y p
t
θ
Cp
{

Yt
´
m´
pA

θ “
Cp
{

C

m´

2.

8

(cid:13)
(cid:13)
(cid:13)Yt

´

m

q

Y p
t

(cid:13)
(cid:13)
(cid:13)p ď

8

0
m
ÿ
“

δ

m
p

.
q

0
m
ÿ
“
A with A

Since δ

m
p

q !

m´

1, δ

m
p

q

ą

is summabe. Thus (C3) holds.

3. Let ǫ10 be the new independent sample to be used in δp

¨ ¨ ¨
be an inﬁnite sequence of i.i.d. samples from Fǫ. By triangle inequality we have:

. Let ǫ20, ǫ2
q
´

t
p

1,

1, ǫ0,

, ǫ

´

¨ ¨ ¨

, ǫm

q ´

δp

m
p

q “

ď

`

“

p¨ ¨ ¨

p¨ ¨ ¨

(cid:13)
(cid:13)h
(cid:13)
(cid:13)h
(cid:13)
(cid:13)h
p¨ ¨ ¨
(cid:13)
(cid:13)
(cid:13)Ym

2

, ǫ

1, ǫ0,

¨ ¨ ¨
´
, ǫ2
1, ǫ20, ǫ1,
´
(cid:13)
(cid:13)
Y p
(cid:13)p

m
qm

´

, ǫm

1, ǫ10,

¨ ¨ ¨
1, ǫ20, ǫ1,

h

h

p¨ ¨ ¨

p¨ ¨ ¨
h

, ǫ

´
, ǫ2
´
, ǫ

1, ǫ10,

´

¨ ¨ ¨

¨ ¨ ¨

(cid:13)
(cid:13)p
q
, ǫm

, ǫm

(cid:13)
(cid:13)p
q
(cid:13)
(cid:13)p
q

, ǫm

q ´
, ǫm

¨ ¨ ¨

q ´

p¨ ¨ ¨

(7.2)

Thus (C3) implies (C0).

4. See page 2443 in Berkes, Hörmann and Schauer (2011).

Proof of Lemma 4.2. Equation (4.8) is deduced by result in Lemma 3.3. Then
by Theorem 3.5

sup
n
t
s
Pr

Ut
|

´

Ut

| ď

p

“

ď

n

sup
R |
s

P

sup
s |

I
1p
k
ÿ
“
R

s, n
p

1
n

1
n
1
?n

Yk
p

ď

s

q ´

FY

s
p

qq|

q|

(7.3)

K

sup
s |

s, 1
p

q| `

n´
o
p

2
1
{

log n
p

´
q

α

, a.s.
q

where K

s, 1
p

q

is a centered Gaussian process with covariances

Γ

s, s1
p

q “

Z
k
ÿ
P

E

I
p

Y0
p

ď

s

q ´

FY

s
p

I
qqp

Yk
p

ď

s1

q ´

FY

s1
p

qq

(7.4)

Y. Wang and D.N. Politis/Model-free Bootstrap for Stationary Series

32

q

P

Op

1
p

s, s1

q| “

s, 1
p

s, 1
p

Note that K

can be reparametrized as K 1

@
to be tight such that sups |
K

R. We also need the limiting Gaussian process
absolutely convergent
s, 1
K
, which usually is true but
q
p
not mentioned in Theorem 3.5. Another way to show this is to prove certain
continuity and boundedness condition for the covariance Γ

s, s1
.
q
p
s
FY
such that the
q
p
0, 1
centered gaussian process is now living in a bounded domain T
“ r
of the empirical process). Also, since FY is absolutely continuous and strictly in-
creasing, sups
t, t1
K 1
K
denote the covariance
R
q
p
|
P
1
Γ
. Obviously Γ1
kernel of K 1
t1
, F p´
u
q
Y
p
q
q
p
0. Thus the Gaussian process K 1
1, 1
Γ1
Γ1
u
p
p
q “
enpoints 0 and 1.
We claim that

, for which we have
qq
is pinned to 0 a.s. at

. Let Γ1
1
t
q
p

t
q|
p
F p´
Y
p

Pr
t, t1
p

0, 0
p

s, 1
p

, u
q

supt

q| “

u
p

q “

q “

0,1

“

s |

q

q

s

(reparametrization

• (1) Γ1
• (2) Γ1
• (3) K 1 satisﬁes the Kolmogorov-Chentsov condition:

is uniformly bounded for t, t1
is continuous in t and t1.

t, t1
p
t, t1
p

0, 1

P r

q
q

s

2.

that

α, β, C

D

ą

0, such

E

K 1
|

t
p

α

K 1

s
p

C

t
|

β

s

1
|

`

q|

ď

q ´

Z E

´
(2) is a consequence of (1). To see this, note that
Γ1
t2, t
k
ď
q| “ |
p
P
t
t2
t1
I
E
t
q ´ p
p
r
q ´
ř
orem, the expectation exists
@
As
Thus by dominated convergence theorem we have

t
q ´
q ´
Uk
I
Z
p
p

q ´
t2
´
t and t1

Uk
p
qs r
ă

U0
I
p
p
U0

I
qp
t1

t2
|

q| “

| Ñ

ď
k

t2,

t1
@
U0
p
t
q ´
t2, and uniformly bounded by 2 supt
ř

t1, t
q ´
p
ă
Uk
I
ď
ď
p
qp
. By previous the-
qs

t,
@
I
p
t

Γ1
|
t2

q ´

0,1
s |
0, the random variable inside the expectation converges to 0 a.s.

ď

ď

´

ď

t1

t2

t1

t1

E

Pr

t

P

Γ1
|

t1, t
p

q ´

Γ1

t2, t
p

q| Ñ

0.

Γ1

t, t
p

.
q|

For (1), by positive deﬁniteness of covariance kernel we have that

sup

Γ1

t,t1

p

qPr

2 |
0,1
s

t, t1
p

q| “

“

“

sup
0,1
s

t
Pr
sup
0,1
s

t
Pr

sup
0,1
s

t
Pr

|

|

Γ1
|

t, t
p

q| “

E

I

U0
p

“

k
ÿ

t
Pr

sup
0,1
s
t

ď

|

Z
k
ÿ
P
Uk
p

I
q

E

I
p

U0
p

t

q ´

ď

ď
t2

t

q ´

t

I
qp

Uk
p

ď

t

q ´

t

q|

|

k

f p
qW p

x, y

‰
fW

q ´

fW
x
q
p

y
p

dxdy
q

|

2
t
qs

p

,g´1

k żp´8
ÿ
k
f p
qW p

x, y

ď

R2
ż

k |
ÿ

fW

fW
x
q
p

y
p

q ´

dxdy

q|

¨ ¨ ¨ p˚q

(7.5)

“

FY

Where g
˝
normal density of
is bounded.
k
Rewrite f p
σ2
k
W p
“
varaince for Wt.

qW p
and bk

x, y

q

f is a monotone (increasing) function; f p
W0, Wk
p

; fW is the normal density of Wt. We show that
q

k
qW is the bivariate

p˚q

q
σW

ak, bk
as fxy
p
y2
x2
0
p

`

qp

q “
q ´

1
2π a´
k
2σW
k
p

2
1
{

exp

t´
xy, where σW
q

1
2 a´

1
k bk
,with ak
u
k
q
p

σ2
0
W p

“
q ´
is the lag-k autoco-

Y. Wang and D.N. Politis/Model-free Bootstrap for Stationary Series

33

∇f

“ «

f
B
ak
B
f
bk ﬀ “ «
B
B

3
2

1
2 a´

k p
´

1
k bk
a´
1
2 a´

´
k e´

3
2

1

2 a´1

k bk

1
e´
q
2 a´1
k bk

1

ﬀ

(7.6)

Then

fxy

R2 |
ż

ak, bk
p

q ´

fxy

σ2
0
W p
p

, σW
q

0
p

x2

qp

y2

dxdy

“

∇f T

R2 |
ż

σ2
0
W p
p

, σW
q

0
p

x2

qp

`

y2

qq

„

´

q
xy
q

dxdy
|

σW
o
p

k
p

`

q `



σ2
k
W p
qq
(7.7)

`

qq|
σ2
k
W p
´
2σW
k
p

and

p˚q “

R2

ż

fxy

k |
ÿ

R2

˜ż

∇f T

k |
ÿ

O

“

O

“

σW

k
p

q `

σ2
k
W p

q¸

k
ÿ

˜

k
ÿ

ak, bk
p

q ´

fxy

σ2
0
W p
p

, σW
q

0
p

x2

qp

`

y2

dxdy

qq|

σ2
0
W p
p

, σW
q

0
p

x2

qp

`

y2

σ2
k
W p
´
2σW
k
p

q
xy
q



qq

„

´

dxdy
|

¸

(7.8)

Wt
With
q
t
k σ2
k
, the above quantity is bounded.
W p
q

satisfying (C2),

k σW

k
p

u

ř

For (3), Since K 1 is a centered gaussian process, We have for s
ř

t

ă

is absolutely convergent, then so is

E

K 1
|

t
p

q ´

K 1

s
p

q|

α

Where σ2

s,t “

Z E

k

P

It suﬃce to show that

ř

2
π

żx
Pp
s, t

“ c
U0
I
p
p
α

D

ą

xα 1
σs,t

e´

0,

8q

2
2

x
2σ

s,t dx

C3σα
´
s,t

1

“

(7.9)

P p
1, β, C

t
s
Uk
I
qqp
´
sq´ p
p
0 s.t. σ2
s,t ď

C

ą

s, t

P p
t
p

´

s

q

s

t
sq´ p
qq
2p1`βq
α´1 . Note that

.
¨ ¨ ¨ p˚˚q

´

k

f p
qW p

x, y

fW

fW
x
q
p

y
p

q ´

dxdy

q|

(7.10)

p˚˚q ď

x,y

żp

qP

A2

s,t

k |
ÿ

t

ş

P

Ă

´

R is such that

As,t

I
p

As,t

qq `

C3E

W
p

Where As,t

dx
x
q
p
“
W 2I
p

s. With same Taylor ex-
fW
pansion arguments, there exists positive constants Ci such that the right hand
side of equation (7.10) is bounded by C1E
P
P
As,t
, where W is a normal random variable with den-
sity fW . Since in the calculation of the expectations the dominating term is
ǫ
o
t
the exponential decay,
, and so is
´
qq “
q
pp
ǫ0 . The Kolmogorov con-
E
s
q
´
tinuity condition is satisﬁed. By Borell’s inequality(Chapter 2.1, Adler (1990)),
the tail probability of the supremum is bounded by the tail probability of the
Gaussian distribution with variance supt
. Thus we have proved
the desired result.

W 2I
p
0, 1
,
q

W
p
P
p˚˚q ď

ǫ
@
. Thus

As,t
t
C
p

ą
ǫ0
D

t, t
p

q ă 8

W
p

W
p

W
p

0, E

C2E

qq `

As,t

As,t

0,1
s

1
q

I
|

I
|

P p

W

W

Γ1

´

qq

qq

p|

p|

P

Pr

s

Y. Wang and D.N. Politis/Model-free Bootstrap for Stationary Series

34

Proof of Lemma 4.3. By calculations in Theorem 1.2 in Li and Racine (2006),

where c2

“

ş

E

Fh
p

x
p

qq ´

F

x
p

q “

c2
2

h2F p

2
q

x
p

q `

h2
o
p

.
q

(7.11)

x2k

dx
x
q
p

p
ă 8

. Thus with F p

2
q

x
q
p

bounded and h

n´
o
p

“

4
1
{

,
q

E

sup
R |
x

P

Fh
p

x
p

qq ´

F

x
p

q| “

n´
o
p

2
1
{

.
q

(7.12)

Also by (A7) we have

p

?n

Fh
p

x
p

E

Fh
p

x
p

Gh

x
p

op

1
p

q ´

qqq “
is a tight centered Gaussian process with covariance ΓGh p
q “
x
n
x, x1
p
p
q
p
R, where Γ is given in equation (3.6). As shown in Lemma 4.2, the

Yk
´
h q ´

Y0
´
h q ´

Y0
´
h qq

x, x1
as h

p
K
p

K
p

p
K

q `

Ñ

E

E

Γ

p

q

p

p

x

x

x

q Ñ

(7.13)

Yk
´
h qq
˘

0,
ř
limit Gaussian process is tight. This is suﬃcient for

˘ `

P

x
q
p
K

and Gh
Z E
P
x, x1
`
@

k

(cid:12)
(cid:12)
Fh
(cid:12)

sup
R
x

P

x
p

q ´

E

Fh
p

(cid:12)
(cid:12)
(cid:12) “
qq

x
p

Op

1
.
?n q

p

(7.14)

Together with equation (7.12) we have supx
fore supt
Ut

Op

h

p

p

1
.
?n q

p

| “

n
s |

Pr

U p
qt ´
p

R

P

Fh
|

x
p

q ´

F

x
p

q| “

Op

p

1
?n q

. There-

p

Proof of Theorem 4.4. Note that

tcκlu

tcκlu

tcκlu

σ
0 |

k
p

q ´

σ

k
p

q| ď

σ
0 |

k
p

q ´

σ

k
p

q|

`

σ
0 |

k
p

q ´

σ

k
p

q|

(7.15)

k
ÿ
“

pp

k
ÿ
“

pp

p

1
q
p
looooooooomooooooooon
Zt
is (C2) with A
u
o
n
,
q
p
1
˜Φ´
Ut
Ut
p
q
p
1 is a function bounded by c and

First of all, for (2), by Lemma 3.2,
t
(C0). Then by Theorem3.7, with l
“
˜
For (1), let ˜Zt
Zt
Then ˜Φ´

P
Ñ

˜Φ´

and

p´2
p

“

“

q

1

ą
0.

1 and therefore satisﬁes

2
p
, where ˜Φ is deﬁned in Section 2.
q
c.

k
ÿ
“

p

2
q
p
looooooooomooooooooon

p

´
p

Y. Wang and D.N. Politis/Model-free Bootstrap for Stationary Series

35

Now consider:

tcκlu

0 |

k
ÿ
“

σZ

k
p

q ´

σZ

k
p

q| “

pp

p

“

tcκlu

0
k
ÿ
“
tcκlu

0
k
ÿ
“
tcκlu

k

n

´

´

1
t
ÿ
“
k
n
´

´

1
t
ÿ
“
k
n
´

1
n

|

1
n |

1
n

1
n

ZtZt

`

k

´

˜
Zt

˜
Zt

`

|

k

ZtZt

`

k

´

x
p
˜Zt ˜Zt
`

¯

˜Zt ˜Zt
`

k

´

k

`

˜
Zt

˜
Zt

`

ď

0 |
k
ÿ
“
tcκlu

`

0 |

k
ÿ
“

ZtZt

`

´
˜Zt ˜Zt
`

´

˜Zt ˜Zt
`

k

k

´

|

¯

˜
Zt

˜
Zt

`

k

´

x

p

k

|

¯

1
t
ÿ
“
k
n
´

1
t
ÿ
“

x

p

T1

p

q

T2

p

q

Let X

Y denote

À

For T2,

0, X

c

D

ą

ď

cY .

T2

tcκlu

“

0 |
k
ÿ
“
tcκlu

0 |
k
ÿ
“
tcκlu

1
n

1
n

k

n

´

1
t
ÿ
“
k
n
´

1
t
ÿ
“

˜Zt ˜Zt
`

k

˜Zt

˜
Zt

`

k

`

˜Zt

˜
Zt

`

k

´

˜
Zt

˜
Zt

`

k

|

´

˜Zt

k

˜Zt
`
´

p
˜
Zt

`

´

p

`

k

¯

p
˜
Zt

`

k

p

´

˜Zt

p
˜
Zt

x

´

x

|

¯

“

À

“

c
c
φ
p
lce
p

c

Ut
|

sup
n
t
q
Pr
s
2
2
1
2 n´
{

.
q

0
k
ÿ
“
Op

Ut

|

´

p

Choose c
n
p
Meanwhile,

q Ñ 8

such that lce

2
c
2

o
?n
p

q

“

makes T2

Ñ

0 in probability.

k

|

¯

(7.16)

(7.17)

ZtZt

`

˜Zt ˜Zt
`

k

|

k

´

k

n

´

1
t
ÿ
“
k

E

1
n

n

E

|

´
n

tcκlu

0
k
ÿ
“
tcκlu

0
k
ÿ
“
lE

ET1

ď

“

À

ď

q|

˜Zt
k
`
˜Zt
`

k

(cid:13)
(cid:13)
(cid:13)2

´

k

k

`

Zt
Zt
p
|
(cid:13)
(cid:13)
l kZtk2
(cid:13)Zt
c2

O

l
p

ce´
p

`
2
{

´
2
1
{
q

ZtZt
|

`

Zt ˜Zt
`

k

Zt ˜Zt
`

k

˜Zt ˜Zt
`

k

|

´

`

k

´

(7.18)

“

q
with speciﬁc calculations for E
choosing l, c
both T1

Zt
p
such that lcec2

Ñ 8
0 and T2

0.

P
Ñ

P
Ñ

k
`
2
{

2 using Zt
˜Zt
k
k
q
`
`
2e´
and lc1
?n
o
{
q
p

´
“

N
„
c2
4
{

0, 1
. Thus by
q
p
1
o
we have
q
p
“

Y. Wang and D.N. Politis/Model-free Bootstrap for Stationary Series

36

Equation (4.12) is a direct consequence of equation (4.11). Note that both

Σn and Σn are Toeplitz, then:

pp

(cid:13)
(cid:13)
(cid:13)
(cid:13)

Σn

pp

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)op ď d
(cid:13)

Σn

(cid:13)
(cid:13)
(cid:13)
(cid:13)1

(cid:13)
(cid:13)
(cid:13)
(cid:13)

Σn

Σn

´

Σn

´

Σn

´

(cid:13)
(cid:13)
(cid:13)
(cid:13)

8

“

“

pp
tcκlu

k
ÿ|
|ď

σ
|

k
p

q ´

pp
k
σ
p

q| `

tcκlu

k
ÿ|
|ą

pp

k
p

q ´

σ

k
p

q|

(7.19)

σ
Z |
k
ÿ
P
σ
|

pp
k
q|
p

k
p

is absolutely summable by (C2) condition on Zt, then with l

First term is shown to converge to 0 in probability. For second term, note that
σ
,
q Ñ 8
second term converge to 0. Since the spectral density of Wt is both bounded
and bounded away from 0, with Zt a linear transform from Wt, same holds for Zt.

n
p

q

Then by Theorem 2 of McMurry and Politis (2010),

(cid:13)
(cid:13)
(cid:13)
(cid:13)

Σ´

1
n ´

1

Σ´
n

(cid:13)
(cid:13)
(cid:13)
(cid:13)op

P
Ñ

0.

Proof of Lemma 4.5. Before proving the assertion, some preliminary lemmas
are required and listed below:

pp

Lemma 7.1. Given that
and strictly increasing, then with,

1
p
Y p
Proof. See Lemma 1.2.1 in Politis, Romano and Wolf (1999)

P
Ñ

p
p

F ´

0, 1

F ´

P p

,
q

@

p

p

q

1

.
q

F is uniformly consistent for FY and FY is continous

Lemma 7.2. Let U
F , FY
FY , With d

„

0, 1
U nif
p
supx |
F

. Then
q
FY
x
p

Y

x
q|
p
p
Proof. For the ﬁrst part, see Angus (1994). The second part is straightforward.

q “

q ´

8p

p

F and Y

F ´
Y in probability.

“

1
Y p

U

q „

1

p
F ´
“
P
0,
Ñ
p

q „

U
p
Y d
Ñ

p

p

p

By Lemma 7.2 and Theorem 4.4, also by continuous mapping theorem, (1)

in Lemma 4.5 is straightforward. We focus on proving (2).

Note that for both ecdf and kernel cdf with appropriate bandwidth choice,
0 with relative assumptions. Then by continuous map-
1

1

Yk
p

converges in probability to Zk

FY
p
Z d. Then with assumption (A9)
p

qq

Φ´

F
p

Yk
p

.
qq

“

supx |
x
F
q ´
p
ping theorem,
p
Speciﬁcally,

P
FY
x
q|
Ñ
p
N, ˜Φ´
k
P
˜
P
Zd
n,
Ñ

@
d

ď

@

p

ξ

d

ξ

P
Ñ

N

0, Id
p

q

d „

(7.20)

p

ξ1,
¨ ¨ ¨ q
“ p
i.i.d.
. Let ¯F
0, 1
N
q
p
„
n
k“1 I
x
ξk
and Ln,x
q
p
n

ξ
Thus by Lemma 3.4 of Kallenberg (1997), sequence
with repect to the metric ρ
ξ, ξ
p
p
ξ, ¯Fξ be the empirical CDF of
p
limn
, Lx
x
q
p
p
Ñ8
0. Since supx |

probability to ξ
where ξt
¯Fξ
¯Fξ
continous mapping theorem(Theorem 18.11, Van Der Vaart (1998)) on Ln,x
and Lx

converges in
1 2´
ξk
ξk
,
q “
|
|
p
ξ and ξ respectively, i.e.

, we have for x a.e., ¯F

. By using
q

ř
Ln,x

¨ ¨ ¨ q
8k
“

q “ ř

x
q
p

P
Ñ

q| Ñ

x
p

x
p

x
p

q “

q “

“ p

ξ
p

ξ
p

ξ
p

q ´

q ´

¯Fξ

¯Fξ

ξ1,

p¨q

p¨q

´

Φ

p

ď

k

p

x
ξp
p

Y. Wang and D.N. Politis/Model-free Bootstrap for Stationary Series

37

P
¯F
0, a.s., along with continuity of Φ and by Pólya’s theorem, supx |
x
ξp
Ñ
0. Since for any ﬁnite dimensional vector
ξ˚t1 ,
, each of the elements are
p
i.i.d. sampled from ¯F
p
, ξ˚td q
ξ˚t1 ,
converge in distribution(in bootstrap
p
world) to a d-vector with i.i.d. standard normals in probability. By Theorem
3.29 of Kallenberg (1997), assertion in (4.16) holds.The remaining proof goes
back to the setting in (1).

ξ, then

, ξ˚td q

x
p

q´

¨ ¨ ¨

¨ ¨ ¨

Φ

q|

p

Proof of Theorem 4.6. The proof is based on several key components that can
be checked easily:

1. Finite dimensional convergence in distribution shown in Lemma 4.5.
2. The bootstrap sample Y ˚t are tcκlu-dependent with respect to P ˚ due to
δ

0,
m approximable assumption of Yt(with p

i.i.d. sampling of the ξ˚t s and bandedness of
Op
plies uniform integrability of
for Y ˚t
in probability.

Y ˚t q
p
3. Central limit theorem holds by Lemma 4.1 for ¯Y ˚n in probability.

Y ˚t q
t, E˚
`
p
2) which im-
2 (with respect to n). Also (C2) holds

Σn. Also

from Lp

@
ą

1
p

ą

´

pp

D

δ

q

2

“

tcκlu
k

σ˚
p
Ñ 8

The consequences of 1 & 2 above are two fold. One, the long run variance of Y ˚t
2
k
tcκlu γ˚Y p
which is absolutely convergent in
can be written as
8q
probability as n
by Lemma 4.1. Second, by similar arguments to the proof
of Theorem 3.3 in Bühlmann (1997), we have
1
op
. Note that
q
p
tcκlu
imply
k

q| “
is also absolutely convergent. All together, they
ř
k
p

, and as a consequence,
q

Z γY
P
k
γ˚Y p
ř

q ´
The remaining arguments are standard.

k
γ˚Y p

2 P
Ñ

k
p
γY

σ˚
p

tcκlu |

σ2
8

q| “

M |

k
p

1
p

q´

M
k

8q

γY

M

op

ř

“´

“´

“´

0,

ą

“

@

q

q

k

.

ř

Proof of Theorem 4.10. The kernel smoothed spectral density estimator

fsp.d

ω
p

q “

˜κh

ω
p

ωj

In
q

ωj
p

q

´

n
j
ÿ
ď

(7.21)

p

can be rewritten as the lag-window spectral density estimator with asymptoti-
cally negligible error (even after ?nh- scaling), i.e.:

fsp.d

ω
p

q “

1
2π

Kh

k
p

γ
q

k
p

e´
q

iωk

O

`

1
n

.

˙

ˆ

(7.22)

Z
k
ÿ
P

1
n

p
n
t
“

¯Y

p
and Kh

¯Y

1

k

`

γ

qp

´

Yt

q “

k
p

Yt
p

k.
with
q
See Politis and Romano (1992). Under assumption (A12) and Lemma 3.3, also
ř
4; with
with previous results holding,
and
additional assumptions (A13) and (A14) holding, conditions in Theorem 2,
Liu and Wu (2010) are satisﬁed. Therefore asymptotic normality in the sense of
equation (4.26) holds for both
and

with limiting variance σ2

satisfy (C0) with p

Y ˚t u

ei
p
q

1 ˜κh

ω
p

k
p

q “

ωj

Yt

ř

´

´

“

ωj

p

fsp.d

t

u

t

´

“

ω

q

n
j

ω “

ω
p

q

p

ω
f ˚sp.dp
p

q

Y. Wang and D.N. Politis/Model-free Bootstrap for Stationary Series

38

2

ηω
p
`
Since
γ˚Y p
k
q|
f 2
ω
sp.dp
|

σ˚ωq
p
1
2π

f 2
˜κ2
1
du and
ω
u
q
p
q
p
q
ω
f ˚sp.dp
ω
fsp.d
p
q| ď
q ´
|
ş
P
0 and both fsp.d
Ñ
ω
f ˚sp.dp

ηω
“ p
`
k
γY
Z
p
|p
P
ω
and f ˚sp.dp
ř
q
0, which implies
σ˚ωq
p

2
qq

P
Ñ

q ´ p

ω
p

q

k

|

2 P
Ñ

σ2
ω.

1

2
ω
f ˚
p
qq
k
γ˚Y p
qq

du respectively.
u
q
p
qp
γY
q ´
|
q ´
| ď
are bounded and bounded a.s. ,

˜κ2
eiωk
ş

k
p

Z

k

P

ř

¯F . The case for the kernel estima-
F
Proof of Theorem 5.1. We prove it for
Fh is similar with relative assumptions assumed. We mainly show that the
tor
distribution of bootstrap predictive root converges in probability to the true dis-
p
tribution in a conditional sense. i.e., Y ˚n
`
conditioning on past values Y n.

1 in probability,

Y ˚n
`

d˚
Ñ

Yn

Yn

´

“

´

p

`

`

1

1

1

Firstly, conditioning on Yn, Y ˚n
`
Φ

Φ

1

Y ˚n
`

1

“

1
¯F ´
Y p

1
¯F ´
Y p

Z ˚n
p
`

qq “

`

1

d˚
Ñ
Z ˚n
p
`

p
1 in probability:

p

Yn

`

1

qq ´

F ´

1
Y p

Φ

Z ˚n
p
`

1

`

qq
˘

F ´

1
Y p

Φ

Z ˚n
1
p
qq
`
(7.23)

1

1

`

`

U

U

U

Φ

Φ

“

`

q “

Y
p

¯FY

q ÑP

Y
p
p

i
qq ´

1
Y p

i
qq
˘

To show F ´

1
´
n ă
FY

Y
i
p
qq ´
p
Y
i
qq ´
p
p

i
n ´
˘
“
Y
i
qq ´
p
p

is continuous, Y
p

We show that ﬁrst term converges to 0 in probability and the distribution of
second term converges to that of Yn
1, in probability. Let U
. Then
q
1
¯F ´
, where i
i
0.
U
n . It is equivalent to show Y
i
Y p
ď
q
p
¯FY
U
Y
Y
i
p
p
qq ´
p
p
by Lemma 4.1. For the second term, ¯FY
U
`
. Hence the second term goes to 0. Therefore FY

Z ˚n
p
“
1
`
F ´
i
Y p
q ´
. The ﬁrst term
U , where
0 in
U

Consider FY
is Op
U

1
?n q
p
1
n , i
i
´
n s
P p
probability. Since F ´
Y
Z ˚n
p
`
ditioning on Yn, Z ˚n
`
Y n, ξ˚n
Z ˚n
1 with its theoretical analogue L
1
p
`
`
The formula of Ln
,
¨q
p¨
,
values Y n; While L
¨q
p¨
the past values Y n and innovation ξn
`
in Lemma 4.5 and assumption (A9), Ln
Also the distribution of ξ˚n
`
continuous mapping theorem, Ln

`
1 in probability. For MF bootstrap, let n

:
¨q
1.
`
is estimated from Y n and therefore depends on the past
is the theoretical data-generating mechanism that links
1 so it does not depend on Y n. By results
in probability;
1 converges to Φ for MF bootstrap. As a result of

Y n, ξ˚n
p
`
For LMF bootstrap, we only need to show the normal parameters in step 2
of Algorithm 3 converge to their theoretical analogue, in probability. As an
1

, we need to show con-

F ´
1
Y p

converges to L

in probability.

Y n, ξn
p

Y n, ξn
p

q
Zn
p

1
Y p
Φ

d˚
Ñ
Zn

i
q ´
F ´

1
qq
d˚
Ñ

x, y
p

x, y
p

P
Ñ
1

,
p¨
Zn

d˚
Ñ

q Ñ

q Ñ

,
p¨

Yn

Ñ

0.

“

qq

U

L

¨q

`

`

`

`

q

q

q

q

1

1

1

1

1

:

example, we show

Σ21

1
Σ´
11

Σ21Σ´

11 Zn in probability. Let cn

Σ21Σ´

1
11 &

“

˜
Zn Ñ
p
n

Σ21

cn

“

1

Σ´

11 . we have

pp

pp

pp

pp

pp
cn
|

pp

˜
Zn ´
p

cnZn| ď

ď

cn,i

1 |
i
ÿ
“
n
pp
cn,i
1 |
i
ÿ
“
pp

˜
Zi

cn,iZi

|

´

Zi

´

| ` |

n

cn,i

1
i
ÿ
“

´

p
˜
Zi
||

p

´

|

cn,i

Zi

¯

pp

(7.24)

Y. Wang and D.N. Politis/Model-free Bootstrap for Stationary Series

39

˜
Zi

1

´

Zi

, which is op
|

The ﬁrst term is bounded by

n
supi |
cn,i
i
|
|
“
n
¯
´ř
cn,i
is bounded in probability for large
continuous mapping theorem and
1
i
pp
|
|
“
0,
Σn
cn
n. The second term has N
cn
cn
distribution, where the
q
p
´
p
p
2
(cid:13)
(cid:13)
pp
(cid:13)
(cid:13)
variance is bounded by λmax
cn
(cid:13)cn
. Thus if
(cid:13)
pp
2
term converge to 0 in probability.
pp
(cid:13)
(cid:13)
(cid:13)
(cid:13)
Σ21
(cid:13)
(cid:13)
(cid:13)
(cid:13)op

pp
(cid:13)
(cid:13)
Σ21
(cid:13)
(cid:13)2

p
T
cn
q
(cid:13)
(cid:13)
(cid:13)cn
pp

0, then second

(cid:13)
(cid:13)
(cid:13)2 ÑP

(cid:13)
(cid:13)
(cid:13)
(cid:13)2 `

1
n ´

(cid:13)
(cid:13)
(cid:13)cn

(cid:13)
(cid:13)Σ´
n

(7.25)

(cid:13)
(cid:13)op

Σ´
n

1
p

(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

Σ21

ř
´

cn

cn

by

Σn

´

´

´

´

q

q

1

1

Since

(cid:13)
(cid:13)
(cid:13)
(cid:13)

pp
1
n ´

Σ´

0, by previous results,

Σ21

0. Then

´
0. The proof for consistency of the second parameter follows a similar fashion.
pp

What remains to be shown is conditioning on Y n,

´

pp

pp

pp

pp
(cid:13)
(cid:13)
Σ21
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)
(cid:13)2 ÑP

(cid:13)
(cid:13)
(cid:13)cn

cn

(cid:13)
(cid:13)
(cid:13)2 ÑP

Σ´

(cid:13)
(cid:13)
(cid:13)2 ď
(cid:13)
(cid:13)
(cid:13)
(cid:13)op ÑP

Σ´
n

pp

1

Y ˚n
`

1

P ˚
Ñ

1.

Yn

`

(7.26)

1 is the self-chosen predictor. In L2 optimal sense,
. Therefore it is suﬃcient to show

1

E

(In probability), where
Y nq “
Yn
Yn
1
|
p
`
that:
¯F ˚
p

E˚

“

`

1

pp

´
q

Yn
`
1
F ´
E
Y p
p
p
1

Φ
p

Zn
p

p
Φ
Zn
p

1

`

qq|

p
Y nq
E

`

qq|
Y nq

P ˚
Ñ

Where the expectation is taken with respect to
as
deﬁned in Algorithm 2 and 3. Since directly calculating the expectation in our
setup is impossible, we use the same approach as previously used in the proof
of Theorem 4.6 by showing:

Zn
p

`

`

1

Φ

F ´
p

1
Y p
Zn
p

1

`

Zn
p
Y nq
1
|

Y nq
qq|
˚ and

(7.27)
Y nq
|

˚

`

Zn
p

Y nq
1.
1
|
2. Consistency of
3. Uniform integrability of

d˚
Zn
Y nq
.
|
Ñ p
1
`
1 to F ´
¯F ˚
Y .
´
q
p
1
¯F ˚
´
q
p

1

on Y n.

Φ
p

Zn
p

`

1

q

and F ´

1
Y p

Φ

Zn
p

`

1

qq

conditioning

Y n in prob-
The second First of all, we need to show that
|
ability. In LMF bootstrap setup as in Algorithm 3, this can be checked by
Σ˚n to Σn in
1 are m-dependent and have ﬁnite pth moment in prob-

showing convergence of the bootstraped autocovariance matrix
probability. Since
ability, Theorem 3.7 applies to

Y nq
|

Y ˚t u

Zn
p

n
t
“

Zn

t

`

`

˚

1

1

pp

d˚
Ñ

n
t
“

1 and thus

Y ˚t u
t
(cid:13)
(cid:13)
Σ˚n ´
(cid:13)
(cid:13)
pp
where Σ˚n is the autocovariance matrix of the bootstrap samples
t
k
σ˚
Lemma 4.5 and uniform integrability of
p
|
0. Therefore by triangle inequality,
This implies kΣ˚n ´

2, We have

(cid:13)
(cid:13)
(cid:13)
(cid:13)op

Y ˚t q
p

Σnkop

P ˚
Ñ

P
Ñ

Σ˚n

ř

0

Z

k

P

(7.28)

Y ˚t u
k
σ
p
q´

. Also by
P ˚
Ñ

q|

0.

Σn

(cid:13)
(cid:13)
(cid:13)
(cid:13)op

(cid:13)
(cid:13)
Σ˚n ´
(cid:13)
(cid:13)
pp

P ˚
Ñ

0.

(7.29)

Y. Wang and D.N. Politis/Model-free Bootstrap for Stationary Series

40

q

Σn

P ˚
Ñ
(cid:13)
(cid:13)
(cid:13)
(cid:13)op

P ˚
Ñ

R,

n

1
q

`

`

t

P

1
q

F p
Z

z
p

(cid:13)
(cid:13)
(cid:13)
(cid:13)

z
@

p
2
q

F p
Z
p

Y ˚t u

log n
p

FY
¯F ˚Y p
y

In MF bootstrap setup as in Algorithm 2, we need to show
n

z
˚
p
q
. This can also be shown through equation (4.15) in Lemma 4.5 and a
q
Σ˚n ´
pp

slightly stronger result than equation (7.29) above, i.e.: we assume
p
0.

for which the CDF F ˚Y satisﬁes
dependence of
0 almost surely with respect to P ˚. Combining
0 in probability. By Lemma 7.1 and
Zn
1
p

Since ¯F ˚Y is the empirical CDF of
P
supy
0 by Lemma 4.5, also along with m
y
y
F ˚Y p
R
q|
p
|
Ñ
q ´
P
, supy
y
F ˚Y p
Y ˚t u
R
t
q ´
|
P
these two we have supy
R
|
continous mapping theorem used in Lemma 4.5,
having the distribution of
with Zn
by ﬁnite pth moment of Yt we have uniform integrability of
probability) and F ´
1

q´
¯F ˚Y q
with Zn
qq
1
`
Zn
˚ converges in distribution to F ´
Y nq
1
1
Y p
qq
p
|
Y n, in probability. Also
1 following the conditional distribution of Zn
|
Φ
(in
p
. Thus equation (7.27) holds. For the L1 opti-
Yn

mal predictor mentioned in Section 1, proving
1 follows in a similar
fashion but may require additional assumptions; for example, continuity and
Y n is necessary to en-
strict monotonicity of the conditional distribution Yn
p
1
|
sure consistency of

1, which is the conditional sample median.

¯F ˚Y p
Zn
p

¯F ˚Y q

1
Y p

q| Ñ
y

Y ˚n
`

Zn
p

Zn
p

P ˚
Ñ

Φ
p

q| Ñ

y
p

FY

´

qq

qq

Φ

Φ

p

`

`

`

´

`

`

`

`

´

`

`

p

p

P

1

1

1

1

1

1

Yn

Summing up the previous results and by Slutsky’s theorem, the bootstrap
predictive root converges in distribution to the true predictive root(in probabil-
ity). Since the distribution of the true predictive root is continuous, we have
consistency of the bootstrap quantiles.

p

`

