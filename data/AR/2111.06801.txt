1
2
0
2

v
o
N
2
1

]

M
B
.
o
i
b
-
q
[

1
v
1
0
8
6
0
.
1
1
1
2
:
v
i
X
r
a

Benchmarking deep generative models for diverse
antibody sequence design

Igor Melnyk
IBM Research AI
igor.melnyk@ibm.com

Payel Das
IBM Research AI
daspa@us.ibm.com

Vijil Chenthamarakshan
IBM Research AI
ecvijil@us.ibm.com

Aurelie Lozano
IBM Research AI
aclozano@us.ibm.com

Abstract

Computational protein design, i.e. inferring novel and diverse protein sequences
consistent with a given structure, remains a major unsolved challenge. Recently,
deep generative models that learn from sequences alone or from sequences and
structures jointly have shown impressive performance on this task. However,
those models appear limited in terms of modeling structural constraints, capturing
enough sequence diversity, or both. Here we consider three recently proposed deep
generative frameworks for protein design: (AR) the sequence-based autoregressive
generative model, (GVP) the precise structure-based graph neural network, and
Fold2Seq that leverages a fuzzy and scale-free representation of a three-dimensional
fold, while enforcing structure-to-sequence (and vice versa) consistency. We
benchmark these models on the task of computational design of antibody sequences,
which demand designing sequences with high diversity for functional implication.
The Fold2Seq framework outperforms the two other baselines in terms of diversity
of the designed sequences, while maintaining the typical fold.

1

Introduction

Antibodies and their functional domains play key roles in research, diagnostics, and therapeutics.
Among them, an attractive class is comprised of nanobodies, which are functional antibody domains
with small size (
15 kDa) and high stability (Tm up to 90 C), and therefore are of increasing
therapeutic interest [3]. Designing functional sequences typically requires a combinatorial exploration
of sequence space. To address this issue, one can impose sequence and/or structural constraints to
narrow the search space.

∼

Functionally diverse antibodies correspond to the highly stable immunoglobulin fold, a universal
protein scaffold structure. Antigen binding speciﬁcity is largely determined by the sequence and
structural diversity of the complementarity-determining regions (CDRs) that are displayed on canoni-
cal frameworks. Among the CDRs, CDR3 contributes most of the sequence and length diversity. As
a result, a big focus in computational antibody design has been on sampling diverse CDR3s.

Along this direction, recently deep generative models have been used for sampling virtual sequences
that were then prioritized for experimental validation. Most of those models are sequence-based that
leveraged autoregressive neural nets such as LSTM. Such models have been successfully used for the
entire functional antibody sequence design [13], as well as for designing only the CDRs given rest
of the sequence as a context [14]. Nevertheless, such model does not explicitly account for the 3D

35th Conference on Neural Information Processing Systems (NeurIPS 2021).

 
 
 
 
 
 
Figure 1: Overview of our study for benchmarking models for antibody sequence design. Starting
with the preﬁx sequence (CDR1, CDR2 and the framework), structure or the fold of the PDB ID
3K3Q, we use three models i.e., AR [14], GVP [9] and Fold2Seq [4], respectively to generate the
protein’s CDR regions. For GVP and Fold2Seq, whose output is the entire protein sequence, we also
run ANARCI to extract the CDRs. The output from all the models is then processed and ﬁltered for
the ﬁnal property analysis.

structural constraints associated with the characteristic immunoglobulin fold and may be limited in
terms of capturing the broad landscape of CDRs.

In this paper, we benchmark three recently proposed state-of-the-art deep generative models –
sequence-based, 3D structure-based, and 3D fold-based – by comparing the generated sequences of
llama nanobodies. The structure-based and fold-based models generate full protein sequences by using
a single representative 3D structure of the llama nanobody as an input, whereas the autoregressive
sequence-based model designs CDR3 by conditioning on the preceding germline framework-CDR1-
CDR2 nanobody sequence. The structure-based model leverages the precise backbone coordinates,
whereas the fold-based model uses a fuzzy description of the secondary structures elements in 3D as
the input. Machine learning, physicochemical, bioinformatics, and structural metrics of the generated
sequences from a single llama nanobody sequence/structure and their comparison with the natural
llama nanobody repertoire suggest that generated sequences strongly vary in term of novelty, diversity,
consistency, and coverage.

2 Related Work

The protein/antibody design problem has been recently addressed using multiple machine learning
approaches. For example, the sequence-based methods of [2, 13, 1] formulated the problem as an
autoregressive process, leveraging deep recurrent networks such as an LSTM, to design the entire
antibody sequence. On the other hand, the work of [14] focused on the variable complementarity
determining regions (CDRs), which are complex and highly diverse parts of the sequence, determining
the speciﬁcity of the antibody. Instead of recurrent networks, they applied causal dilated convolutions
to model the autoregressive likelihood.

Alternatively, ML-physics hybrid methods have also been explored. For instance, [15], proposed a
system, that starts from a random protein sequence, and the search process is guided by the physics-
based constraints to satisfy the desired structural motifs. EVOdesign [11], on the other hand, is a
structure-based approach that uses the evolutionary proﬁles to guide the sequence search. It ﬁrst
identiﬁes the structural analogs from PDB and then constructs a structural sequence proﬁle using
MSA. The physics-based force ﬁelds are then used to search for the low free-energy sequence states.

Recently, a class of deep generative models that account for the 3D structural constraints, have been
proposed. For example, [7] used a generative model for protein sequences design given a target
structure, represented as a graph over the residues. The key challenge was to account for long-range
dependencies in the protein sequence that are usually short-range in the 3D structure space.

2

Fold2SeqforProteinDesignRescaleDiscretizeFeaturization(a)(b)(c)(d)Figure1:(a)ThestructureofT4lysozyme(PDBID107L).Thesecondarystructuresarecoloredas:helicesinred,betasheetsinyellow,loopsingreenandbend/turninblue.(b)Thestructureisrescaledtoﬁtthe40˚A⇥40˚A⇥40˚Acubicbox.(c)Theboxisdiscretizedintovoxels.(d)Featuresofeachvoxelareobtainedfromthestructurecontentofthevoxel.fulnessoflearningjointembeddingforsinglemodalclassi-ﬁcation(Ngiametal.,2011;Wangetal.,2018a;Toutanovaetal.,2015).Moreover,Chenetal.(2018)usedjointembed-dinglearningfortexttoshapegeneration.Jointsequence–labelembeddingisalsoexploredfororappliedtomolecularprediction/generation(Cao&Shen,2021;Dasetal.,2018).3.Methods3.1.BackgroundAproteinconsistsofalinearchainofaminoacids(residues)thatdeﬁnesits1Dsequence.ChemicalcompositionandinteractionswithneighboringresiduesdrivethefoldingofasequenceintodifferentsecondarystructureelementsorSSEs(helix,beta-sheet,loop,etc.,seeFig.1(a)),thateventuallyformsacompletenative3Dstructure.Aproteinfoldcapturesthestructuralconsensusofthe3Dtopologyandthecompositionofthosesecondarystructureelements.3.2.FoldRepresentationthrough3DvoxelsoftheSSEdensityIndenovoproteindesignthatwetarget,nobackbonestruc-tureisassumed.Instead,atopological“blueprint”(consis-tentwiththedesiredfold)isgiven.Andinitialbackbonestructurescanbegeneratedaccordinglyusingfragmentas-semblies(Huangetal.,2016).Inthisstudywefocusongen-eratingfoldrepresentationsoncethestructuresareavailableandadditionallyexplorethechallengesfromsuch“blueprint”inputstructuresthroughthreereal-worldchallenges.Weherebydescribehowwerepresentthe3Dstructuretoex-plicitlycapturethefoldinformation,asillustratedinFig.1.Theposition(3Dcoordinates)ofeachresidueisrepresentedbyits↵-carbon.ForagivenproteinoflengthN,weﬁrsttranslatethestructuretomatchitscenterofgeometry(↵-carbon)withtheoriginofthecoordinatesystem.Wethenrotatetheproteinaroundtheorigintolettheﬁrstresiduebeonthenegativesideofz-axis(principalcomponent-basedorientingwasalsoexploredasinTrainingandDecodingStrategy).Wedenotetheresultingresiduecoordinatesasc1,c2,...,cN.ThesecondarystructurelabeltoeachresidueisassignedbasedontheirSSEassignment(Kabsch&Sander,1983)inProteinDataBank(Bermanetal.,2000).Weconsider4typesofsecondarystructurelabels:helix,betastrand,loopandbend/turn.Inordertoconsiderthedistributionofdifferentsecondarystructurelabelsinthe3Dspace,wediscretizethe3Dspaceintovoxels.Duetothescale-freedeﬁnitionofaproteinfold,werescaletheoriginalstructure,sothatitﬁtsintoaﬁxed-sizecubicbox.Basedonthedistributionofsizesofsingle-chain,single-domainproteinsintheCATHdatabase(Sillitoeetal.,2019),wechoosea40˚A⇥40˚A⇥40˚Aboxwitheachvoxelofsize2˚A⇥2˚A⇥2˚A.Wedenotethescalingratioasr2R3.Forvoxeli,wedenotethecoordinatesofitscenterasvi.WeassumethatthecontributionofresiduesjtovoxelifollowsaGaussianform:yij=exp( ||cj r vi||22 2)·tj,(1)wheretj2{0,1}4istheone-hotencodingofthesecondarystructurelabelofaminoacidj.Thestandarddeviationischosentobe2˚A.Wesumupallresiduestogethertoobtaintheﬁnalfeaturesofthevoxeli:yi=PNj=1yij.Thefoldrepresentationy2R20⇥20⇥20⇥4isthe4Dtensorofyioverall20⇥20⇥20voxels.Thisfoldrepresentationusing3DSSEdensitiesbettercapturesscale-freeSSEtopologiesthatdeﬁnefolds,whileremovingfold-irrelevantstructuredetails.Itresultsinsequencegenerationthatexploresthesequencespaceavailabletoaspeciﬁcfoldmorewidely(asshowninexperiments).3.3.Fold2SeqwithJointSequence–FoldEmbeddingModelArchitecture.Inthetrainingstage,ourmodelcon-sistsofthreemajorcomponents:asequenceencoder:hs(·),afoldencoder:hf(·)andasequencedecoder:p(x|h(·)),asshowninFig.2(Left).(i)SequenceEncoder/Decoder.Bothsequenceencoderanddecoderareimplementedusingthevanillatransformermodelandavanillasequenceembeddingmodule(learnablelookuptable+sinusoidalpositionalencoding),asdescribedinVaswanietal.(2017).AlltrainingsequencesarepaddedtothemaximumlengthNsof200,as77%ofsingle-domainMVSLTQSGGGSVKVGEDVTLTCTV…ANARCICDR1CDR2CDR3CDR3MVSLTQSGGGSVKVInputOutput ProcessingCDR extractionAnalysisStructureFoldSequenceModelPDBID:3K3QARAutoregressive Causal Dilated ConvolutionsGVPEncoder-DecoderGNN Fold2SeqEncoder-Decoder Transformer•Novelty•Sequence recovery•Uniqueness•Input consistency•Coverage/Diversity•…Figure 2: Kernel density plot as a function of pairwise sequence similarity within the same ensemble
(f2s= Fold2Seq, gvp=GVP, AR=AR, nat=Natural) and with the natural ensemble (*-nat). Higher
score indicates better similarity.

Based on [7] the work of [9] then proposed Geometric Vector Perceptrons (GVPs) to allow for the
embedding of geometric information at nodes and edges without reducing such information to scalars
that may not fully capture complex geometry. The proposed graph network ensures that the vector
and scalar outputs are equivariant and invariant with respect to rotations and reﬂections.

Designing proteins based on a rigid backbone structure is known to restrict the diversity and novelty
of the sequences. For this reason, [4] proposed Fold2Seq, a transformer-based generative framework
for designing protein sequences conditioned on a generic target fold, rather than the speciﬁc high
resolution 3D structure. The fold here was deﬁned as the spatial arrangement of the local secondary
structure elements. The method also uses joint sequence–fold embedding to better capture the
relationship between the two modalities.

Finally, a recent work of [8] proposed a generative model to speciﬁcally design only the CDRs of
antibodies rather than the whole protein sequence. The approach is based on co-designing the CDR
sequence and 3D structure of CDR as graphs. The output is built autoregressively while iteratively
reﬁning its predicted global structure, which in turn guides the next residue choice.

3 Method

Figure 1 shows the overview of our benchmarking study.

Models We examine three different models. AR [14]: the autoregressive approach that uses the causal
dilated convolutions for the input preﬁx sequence to generate the CDR3 protein subsequence. GVP
[9]: the encode-decoder GNN model that uses Geometric Vector Perceptrons to encode the scalar and
vector structure information which is then decoded in the autoregressive manner to generate the entire
protein sequence. Fold2seq [4]: another encode-decoder model based on transformer architecture
that embeds the fuzzy 3D protein structure information (fold) in the joint sequence-fold embedding
space and then decodes it autoregressively into the corresponding protein sequence. For the GVP and
Fold2Seq outputs we run ANARCI [6] to extract the CDR regions.

3

Table 1: Sequence recovery rate (SRR) and NLL from the autoregressive sequence model [14] trained
on natural llama nanobody repertoire for the sequences generated by the comparison methods and the
sequences from the natural llama library, synthetic library, and next-generation sequencing library.

Model
Fold2Seq
GVP
AR
Natural
Synthetic
NGS

Seq Recovery Rate (%)
30.711
40.131
48.865
–
–
–

NLL
2.572
2.987
0.375
0.371
4.912
5.102

Sequence Design The sequence and structure corresponding to the Chain A of pdb id 3K3Q were
used as inputs for this study. It is worth noting that this structure is included in the training of both
GVP and Fold2Seq model, whereas a maximum of 58.94% sequence identity was found to be present
between the input sequence and the AR training set.

We compare the full sequences as well as the CDRs across the generated ensembles. For this
purpose, we extracted the CDRs from the generated sequences using the IMGT numbering scheme as
returned by ANARCI software [6]. For the extracted CDRs, we estimate the percentage of unique
sequences (uniqueness). Sequences that contain glycosylation sites, asparagine deamination motifs,
or sulfur-containing amino acids (cysteine and methionine) were removed. For the AR model, we
optionally considered an extra ﬁlter to exclude sequences that do not end with the ﬁnal beta-strand
of the nanobody template as in [14]. We denote the approach with ﬁnal beta-strand ﬁltering by AR
ﬁltered, while we call AR unﬁltered the version without ﬁnal beta-strand ﬁltering.

j

|

g

∈G

Evaluation Metrics We deﬁne the set of the generated sequences (structures) conditioned on se-
quence/structure j as
Gj. In structure-based design, Sequence Recovery rate is deﬁned as (SRR)
for yj as SRRstructure(j) = 1
j SIM (xg, xj). A global alignment scheme and BLAST62
|G
matrix, with a gap opening penalty of -10 and gap extending penalty of -1, were used for estimat-
ing pairwise sequence identity (SIM) and alignment score. Negative log likelihood (NLL) was
estimated by using the autoregressive (AR) generative model trained on 1.2 million natural llama
nanobody sequences [14], as following: N LL =
X<k)), which is sum of the
cross-entropy between the true residue at each position and the predicted distribution over possible
residues, conditioned on the preceding characters. Structural recovery of the three sequence design
models by predicting the 3D structure of the top 100 generated sequences using pretrained models
from Alphafold21.

k=1 log(p(xk|

(cid:80)

(cid:80)

−

K

4 Results

Table 2: Uniqueness and novelty of the CDR3, CDR2, and CDR1 regions of the sequences generated
by Fold2Seq, GVP, AR without ﬁnal beta-stand ﬁltering (AR unﬁltered), AR with ﬁnal beta-strand
ﬁltering (AR ﬁltered), and the sequences from the natural llama library. Note that the AR approach
has been trained to generate CDR3 only, given the preceding portion of a ground truth sequence.

Fold2Seq GVP AR unﬁltered AR ﬁltered Natural Llama

CDR3

CDR2

CDR1

Uniqueness
Novelty
Uniqueness
Novelty
Uniqueness
Novelty

100
43.36
100
58.70
92.49
60.75

88.33
52.71
9.15
9.15
56.20
51.99

87.57
11.92
–
–
–
–

13.85
8.97
–
–
–
–

100
52.64
100
83.83
100
83.37

Table 1 reports the average sequence recovery rate and average negative log likelihood from the trained
autoregressive model in [14], estimated using 10k generated sequences. The autoregressive sequence
model provides highest sequence recovery rate, followed by GVP and Fold2Seq. Both methods yield

1https://github.com/kalininalab/alphafold_non_docker

4

Figure 3: 2D kernel density plot as a function of isoelectric point and length of generated/natural
CDR3 sequences. Black dot indicates the groundtruth CDR3.

≥

sequences that share
30% identity on average with the sequence of the input structure, implying
fold-consistency of the generated sequences, as a 30% sequence identity threshold typically suggests
fold homology [12]. SRR of GVP is higher than Fold2Seq, consistent with earlier obeservation
that a structure-based model yields higher sequence recovery than a fold-based model [4]. The
negative log likelihood (NLL) from the trained autoregressive model was found to be consistent
with the experimentally reported thermostability of unseen llama nanobody sequences, which is an
important aspect of nanobody ﬁtness [14]. Therefore, we also estimate the NLL of the generated
sequences using Fold2Seq, GVP, AR, as well as a state-of-art synthetic library (Synthetic, constructed
combinatorically using the position-speciﬁc amino acid frequencies of nanobody sequences with
crystal structures in the PDB database) [10] and a large collection of nanobody sequences from
next-generation sequencing repositories (ngs) [5]. As expected, AR-generated sequences show a
NLL close to the natural nanobodies that were used for training of the model. Both synthetic and ngs
libraries show very high NLL, indicating the bias of the trained AR toward natural llama nanobody
repertoire. Among the two 3D-conditional generative models, interestingly, F2S sequences show
lower NLL than GVP, implying F2S sequences are closer to the broader llama nanobody sequences.

As reported in Table 2, we estimate uniqueness and novelty of all three generative models with
respect to the training set used for the AR model training. Fold2Seq outperforms both GVP and AR
by a signiﬁcant margin in term of uniqueness for all CDRs. Trend in novelty is not as clear though
between GVP and Fold2Seq: while GVP produces more novel CDR3s, Fold2Seq performs better
for CDR1s and CDR2s in term of novelty. For reference we also include in Table 2 uniqueness and
novelty for 10,000 sequences selected at random from the natural llama library of [14]. This result
implies that a fuzzy representation of a 3D fold allows generating sequences with novel and unique
CDRs, when compared to a sequence-based and a backbone structure-based model.

5

This result is conﬁrmed as we estimate the pairwise sequence similarity within the generated ensemble
from a particular model (Figure 2). When compared to the database comprised of natural llama
nanobody sequences, Fold2Seq sequences are more diverse (low similarity score within the ensemble)
than GVP and AR sequences. GVP sequences show an interesting characteristic – majority of
sequences are highly similar - indicating a mode collapse. In term of similarity with the natural
CDR3s, Fold2Seq sequences are more distant, followed by GVP and AR. The similarity distribution
corresponding to GVP seem narrower, consistent with lack of diversity in generated samples.

Further, we analyze the physicochemical properties, such as isoelectric point (pI) and length of
the CDR3 sequences. Figure 3 shows that the natural nanobody CDR3s populate broad ranges of
isoelectric point and sequence length, with a preference toward pI around 6 and length around 14 and
18. Fold2Seq produces a signiﬁcant coverage of the natural sequences, with a clustering around pI =
6 and length = 14, while still exhibiting non-zero density around the input. GVP produces sequences
that are very close to the one corresponding to the input pdb only, while AR tends to generate short
(around 6 amino acid) sequences with pI around 4.

≥

Finally, we compare the structural recovery of the three generative frameworks by predicting the 3D
structure of 100 random generated sequences using Alphafold2. Figure 4(a) shows that, almost all of
the sequences generated by the Fold2Seq and GVP share a sequence identity of
30% and TM-score
0.5 with the groundtruth, indicating both methods perform similarly in term of recovering
of
correct fold topology [16]. However, GVP sequences exhibit higher TM-score than Fold2Seq ones,
consistent with high consistency of the generated sequences with the input while lacking coverage
and diversity (see Figure 4(a)). The AR sequences show high sequences identity as well as high
TM-score, which is not surprising given that the model “grafts” a short (around 6 residue) sequences
within the input sequence. In contrast, though Fold2Seq generates diverse and dissimilar sequences,
corresponding structures turn out to be high-consistency, with a 1.8-3 Å backbone RMSD from the
input structure (see Figure 4(b)).

≥

In conclusion, this benchmarking study highlights key performance differences of three protein
sequence design models on the nanobody sequence generation task. The backbone based GVP
model provides high consistency with the input, while lacking the diversity present within the llama
nanobody sequences. A sequence-based model, on the other hand, captures a limited spectrum of the
llama nanobody repertoire. The sequence design model that employs a fuzzy representation of the

Figure 4: (a) Scatter plot of sequence identity and structure similarity (TM-score) for Fold2Seq,
GVP and AR generated sequences. The structure of the generated sequences are estimated using
AlphaFold and the TM-Score is calculated with the ground truth structure using TM-Align. Note that
since the sequence identity is computed over the entire protein chain, the AR method, generating only
the CDR region (as opposed to others, that generate the whole sequence), by construction has higher
similarity to the ground truth. (b) Alignment of the structure corresponding to a Fold2seq generated
sequence (green) with the ground truth structure for PDB Id 3K3Q chain A using TM-align. Arrow
indicates CDR3. TM-score is 0.75 and backbone RMSD is 1.86 Å.

6

0.20.30.40.50.60.70.80.9Sequence Identity0.60.70.80.91.0TM-Scoregvpfold2seqARcorresponding 3D fold as the input and generates sequences by leveraging representation learned by
join training on 3D folds and 1D sequences provides better diversity, while still providing sufﬁcient
consistency with the input sequence/structure and coverage of the llama nanobody repertoire.

Ultimately, the choice of method should be guided by the task at hand. If one wishes to generate
sequences very close to an input sequence/structure, GVP might be more appropriate. If one wishes
to graft speciﬁc regions such as a short CDR3 within an input sequence, then the AR approach would
be better suited. If one wishes to generate diverse sequences with broader coverage that “extrapolate”
to a greater extent from a given input sequence, while still maintaining consistency with the input,
then Fold2Seq would be a method of choice.

References

[1] Rahmad Akbar, Philippe A Robert, Cédric R Weber, Michael Widrich, Robert Frank, Milena
Pavlovi´c, Lonneke Scheffer, Maria Chernigovskaya, Igor Snapkov, and Andrei Slabodkin. In
silico proof of principle of machine learning-based antibody design at unconstrained scale.
BioRXiV, 2021.

[2] Ethan C Alley, Grigory Khimulya, Surojit Biswas, Mohammed AlQuraishi, and George M
Church. Uniﬁed rational protein engineering with sequence-based deep representation learning.
Nature methods, 16(12):1315–1322, 2019.

[3] Peter Bannas, Julia Hambach, and Friedrich Koch-Nolte. Nanobodies and nanobody-based
human heavy chain antibodies as antitumor therapeutics. Frontiers in immunology, 8:1603,
2017.

[4] Yue Cao, Payel Das, Vijil Chenthamarakshan, Pin-Yu Chen, Igor Melnyk, and Yang Shen.
Fold2seq: A joint sequence (1d)-fold (3d) embedding-based generative model for protein design.
In International Conference on Machine Learning, pages 1261–1271. PMLR, 2021.

[5] Piotr Deszynski, Jakub Mlokosiewicz, Adam Volanakis, Igor Jaszczyszyn, Natalie Castellana,
Indi-integrated nanobody

Stefano Bonissone, Rajkumar Ganesan, and Konrad Krawczyk.
database for immunoinformatics. medRxiv, 2021.

[6] James Dunbar and Charlotte M Deane. Anarci: antigen receptor numbering and receptor

classiﬁcation. Bioinformatics, 32(2):298–300, 2016.

[7] John Ingraham, Vikas Garg, Regina Barzilay, and Tommi Jaakkola. Generative models for
graph-based protein design. In Advances in Neural Information Processing Systems, volume 32.
Curran Associates, Inc., 2019.

[8] Wengong Jin, Jeremy Wohlwend, Regina Barzilay, and Tommi Jaakkola. Iterative reﬁnement

graph neural network for antibody sequence-structure co-design, 2021.

[9] Bowen Jing, Stephan Eismann, Patricia Suriana, Raphael JL Townshend, and Ron Dror. Learn-
ing from protein structure with geometric vector perceptrons. arXiv preprint arXiv:2009.01411,
2020.

[10] Conor McMahon, Alexander S Baier, Roberta Pascolutti, Marcin Wegrecki, Sanduo Zheng,
Janice X Ong, Sarah C Erlandson, Daniel Hilger, Søren GF Rasmussen, and Aaron M Ring.
Yeast surface display platform for rapid discovery of conformationally selective nanobodies.
Nature structural & molecular biology, 25(3):289–296, 2018.

[11] Robin Pearce, Xiaoqiang Huang, Dani Setiawan, and Yang Zhang. Evodesign: Designing
protein-protein binding interactions using evolutionary interface proﬁles in conjunction with an
optimized physical energy function. Journal of molecular biology, 431 13:2467–2476, 2019.

[12] William R Pearson. An introduction to sequence similarity (“homology”) searching. Current

protocols in bioinformatics, 42(1):3–1, 2013.

[13] Koichiro Saka, Taro Kakuzaki, Shoichi Metsugi, Daiki Kashiwagi, Kenji Yoshida, Manabu
Wada, Hiroyuki Tsunoda, and Reiji Teramoto. Antibody design using lstm based deep generative
model from phage display library for afﬁnity maturation. Scientiﬁc reports, 11(1):1–13, 2021.

[14] Jung-Eun Shin, Adam J Riesselman, Aaron W Kollasch, Conor McMahon, Elana Simon, Chris
Sander, Aashish Manglik, Andrew C Kruse, and Debora S Marks. Protein design and variant
prediction using autoregressive generative models. Nature Communications, 12, 2021.

7

[15] Doug Tischer, Sidney Lisanza, Jue Wang, Runze Dong, Ivan Anishchenko, Lukas F Milles,
Sergey Ovchinnikov, and David Baker. Design of proteins presenting discontinuous functional
sites using deep learning. bioRxiv, 2020.

[16] Jinrui Xu and Yang Zhang. How signiﬁcant is a protein structure similarity with tm-score= 0.5?

Bioinformatics, 26(7):889–895, 2010.

8

