A COMPARATIVE STUDY ON NON-AUTOREGRESSIVE MODELINGS FOR
SPEECH-TO-TEXT GENERATION

Yosuke Higuchi1, Nanxin Chen2, Yuya Fujita3, Hirofumi Inaguma4, Tatsuya Komatsu5,
Jaesong Lee6, Jumon Nozaki4,5, Tianzi Wang2, Shinji Watanabe7

1Waseda University, 2Johns Hopkins University, 3Yahoo Japan Corporation, 4Kyoto University,
5LINE Corporation, 6Naver Corporation, 7Carnegie Mellon University

1
2
0
2

t
c
O
1
1

]
S
A
.
s
s
e
e
[

1
v
9
4
2
5
0
.
0
1
1
2
:
v
i
X
r
a

ABSTRACT

Non-autoregressive (NAR) models simultaneously generate multi-
ple outputs in a sequence, which signiÔ¨Åcantly reduces the inference
speed at the cost of accuracy drop compared to autoregressive base-
lines. Showing great potential for real-time applications, an increas-
ing number of NAR models have been explored in different Ô¨Åelds
to mitigate the performance gap against AR models. In this work,
we conduct a comparative study of various NAR modeling methods
for end-to-end automatic speech recognition (ASR). Experiments
are performed in the state-of-the-art setting using ESPnet. The re-
sults on various tasks provide interesting Ô¨Åndings for developing an
understanding of NAR ASR, such as the accuracy-speed trade-off
and robustness against long-form utterances. We also show that the
techniques can be combined for further improvement and applied
to NAR end-to-end speech translation. All the implementations are
publicly available to encourage further research in NAR speech pro-
cessing.

Index Terms‚Äî Non-autoregressive sequence generation, end-

to-end speech recognition, end-to-end speech translation

1. INTRODUCTION

In the last decade, deep learning has brought remarkable success
in automatic speech recognition (ASR) [1, 2], which has become
a central user interface in various IoT applications. Much of the
recent research progress is attributed to improvement in the end-
to-end system [3‚Äì5], where an ASR model is trained to directly
optimize speech-to-text conversion. Owing to the well-established
sequence-to-sequence modeling techniques [6‚Äì8] and more sophisti-
cated neural network architectures [9‚Äì11], end-to-end ASR systems
have achieved comparable results with those of the conventional hy-
brid systems [12‚Äì14].

Current state-of-the-art end-to-end ASR systems are based on
autoregressive (AR) models [6, 8], where each token prediction is
conditioned on the previously generated tokens (Figure 1 left). Such
a generation process can lead to slow inference speed, requiring
L-step incremental calculations to generate an L-length sequence.
Non-autoregressive (NAR) models [15, 16], in contrast, permit gen-
erating multiple tokens in parallel, which signiÔ¨Åcantly speeds up the
decoding process (Figure 1 right). However, such simultaneous pre-
dictions often prevent the NAR models from learning the conditional
dependencies between output tokens, worsening the recognition ac-
curacy compared to AR models.

Fast inference is one of the crucial factors for deploying deep
learning models to real-world applications. ASR systems, in partic-
ular, are desired to be fast and light in numerous scenes, such as in

Fig. 1. Illustrations of autoregressive and non-autoregressive ASR.

spoken dialogue systems, where users prefer quick interactions with
a conversational agent. Accordingly, various attempts have been ac-
tively made to develop and improve NAR end-to-end ASR mod-
els [17‚Äì23], inspired by the great success of NAR modeling tech-
niques in neural machine translation [24‚Äì30]. However, while an
increasing number of NAR models have been proposed and shown
their effectiveness, the research community lacks a comprehensive
study for comparing different NAR methods in a fair experimental
setting. Hence, it remains unclear what the advantages and disad-
vantages each model has when compared to other models.

Our work aims to conduct a comparative study on NAR mod-
eling methods for end-to-end ASR. We have made an effort to
cover a wide variety of methods, including a standard connectionist
temporal classiÔ¨Åcation (CTC)-based model [3]; Mask-CTC [19]
and Improved Mask-CTC [31] based on masked language model-
ing [28, 32]; Align-Denoise [33] based on reÔ¨Ånement training [25];
Insertion Transformer and KERMIT [21] based on insertion-based
modeling [26, 34];
intermediate CTC [35] and self-conditioned
CTC [36] based on regularization techniques; and a continuous
integrate-and-Ô¨Åre (CIF)-based NAR model (CIF-NA) [37]. All
the models are fairly evaluated in the state-of-the-art setup using
ESPnet [38], adopting Conformer [11] for the network architecture.
The contributions of this work are summarized as follows:

‚Ä¢ We conduct comparative experiments on a variety of NAR
ASR models using various ASR tasks. In addition to compar-
ing the accuracy and speed, we further analyze the results to
help develop a deep understanding of NAR ASR.

‚Ä¢ We show that different NAR techniques can be combined to
improve the performance and applied to other NAR speech-
to-text tasks, e.g., end-to-end speech translation.

‚Ä¢ We provide reproducible implementations and recipes used
in our experiments, hoping to encourage further research in
NAR speech processing.

„Äàeos„ÄâNon-autoregressive ASRùë¶!ùë¶"ùë¶#ùë¶$„Äàsos„Äâ„Äàeos„ÄâAutoregressive ASRùë¶!ùë¶"ùë¶#ùë¶$ 
 
 
 
 
 
Table 1. Comparison of various NAR end-to-end ASR models.
Processing unit CTC
#iter
Model

3
8
1
1
10
5
5
1

A-CMLM [17]
Imputer [18]
LASO [20]
Spike-Triggered [22]
Mask-CTC [19]
Improved Mask-CTC [31]
Align-ReÔ¨Åne [23]
Align-Denoise [33]
Insertion Transformer [21] (cid:39) log2(L)
(cid:39) log2(L)
KERMIT [21]
1
Intermediate CTC [35]
1
Self-conditioned CTC [36]
1
CIF-NA [39]

token
frame
token
token
token
token
frame
frame
token
token
frame
frame
token

(cid:88)

(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)

(cid:88)
(cid:88)
(cid:88)
(cid:88)

2. RELATED WORKS

In Table 1, we list several NAR end-to-end ASR models to help com-
pare and understand them at a glance. Here, we include some im-
portant aspects for the comparison. The number of decoding itera-
tions (#iter) can be increased to improve an output sequence at the
expense of extra computations, which enables a model to generate
more valid tokens conditioned on previously generated tokens in a
semi-autoregressive manner [25, 28]. Processing unit is a unique
property in NAR ASR, which can be either frame-level or token-
level. Token-level processing reduces the speed and computational
cost during inference, which is especially important when a model
performs the iterative prediction. However, it needs additional ef-
forts to estimate or adjust the length of an output sequence [16].
Frame-level processing, on the other hand, is prone to slow infer-
ence with the requirement of computational resources, but it does
not require the length prediction. CTC indicates the usage of con-
nectionist temporal classiÔ¨Åcation (CTC) [15], which is a promising
NAR modeling method for ASR.

CTC is the very fundamental method for realizing NAR end-to-
end ASR. CTC makes a strong conditional independence assumption
between token frame predictions, enabling the model to perform fast
inference while limiting the recognition accuracy compared to other
end-to-end ASR models [40]. Inspired by the conditional masked
language model (CMLM) [28], Audio-CMLM (A-CMLM) [17]
effectively learns the conditional distribution of output tokens over
a partially observed sequence through the NAR mask prediction
task [32]. Imputer [18] and Mask-CTC [19, 31] combine CTC with
CMLM to improve frame-level or token-level CTC predictions, get-
ting rid of the cumbersome length prediction required in the previous
approach. While Imputer and Mask-CTC suffer from the mismatch
between training and testing conditions, Align-ReÔ¨Åne [23] and
Align-Denoise [33] introduce iterative reÔ¨Ånement [25] to optimize
the reÔ¨Ånement process of CTC predictions directly.

Some of the recent efforts in NAR end-to-end ASR focus on im-
proving the performance of the standard CTC-based model itself. In-
termediate CTC [35] and self-conditioned CTC [36] apply auxiliary
CTC losses to intermediate layers as in [41], which effectively en-
hances the intermediate representations and leads to improved CTC
performance. Convolution-based neural network architectures have
been shown to improve the CTC-based and the other end-to-end
ASR models in general [31, 42, 43]. When a large amount of speech
data is available for pre-training, powerful speech representations
learned by wav2vec 2.0 [44] can signiÔ¨Åcantly boost the performance
of CTC [42].

Another direction for NAR ASR is based on insertion-based

modeling, which permits the model for generating tokens in an arbi-
trary order without the left-to-right constraint in AR models. Show-
ing promising results in neural machine translation, Insertion Trans-
former [26] and Kontextuell Encoder Representations Made by In-
sertion Transformations (KERMIT) [34] are successfully adopted
for end-to-end ASR [21].

3. NON-AUTOREGRESSIVE ASR

This section reviews NAR modeling methods for end-to-end ASR
compared in our study, including CTC, Mask-CTC, Improved Mask-
CTC, Align-Denoise, Insertion Transformer, KERMIT, intermediate
CTC, self-conditioned CTC, and CIF-NA. We have made an effort
to cover a wide variety of methods, each of which has a unique ca-
pability as an NAR model, as described in Section 2.
Notations: We formulate end-to-end ASR as a sequence mapping
between a T -length input sequence X = (xt ‚àà RD|t = 1, . . . , T )
and an L-length output sequence Y = (yl ‚àà V|l = 1, . . . , L). Here,
xt is a D-dimensional acoustic feature at frame t, yl an output token
at position l, and V a vocabulary.

3.1. Connectionist temporal classiÔ¨Åcation (CTC)

CTC [15] predicts a frame-level alignment sequence Z = (zt ‚àà
V ‚à™ {(cid:15)}|t = 1, . . . , T ), which is obtained by introducing a special
blank token (cid:15) into the output sequence Y . Based on the conditional
independence assumption per token frame prediction, CTC models
the conditional probability P (Y |X) by marginalizing over all paths
(frame alignments) as:

Pctc(Y |X) =

(cid:88)

T
(cid:89)

P (zt|X),

(1)

Z‚ààB‚àí1(Y )
where B‚àí1(Y ) denotes all possible paths compatible with Y . The
CTC objective is deÔ¨Åned by the negative log-likelihood of Eq. (1):

t=1

Lctc = ‚àí log Pctc(Y |X).

(2)

During inference, we use the best path decoding [15] to generate an
output sequence, where the most probable tokens argmaxZ P (Z|X)
are selected at each frame, and an output sequence is obtained by
suppressing repeated tokens and removing blank symbols.

3.2. Mask-CTC

Mask-CTC [19] is built upon an encoder-decoder structure, where
the CTC loss (Eq. (1)) is applied to the encoder output, and the de-
coder adopts the conditional masked language model (CMLM) [17,
28]. The CMLM decoder is trained to predict output tokens Ymask ‚àà
Y , given a partially observed ground-truth sequence Yobs = Y \Ymask:

Pcmlm(Ymask|Yobs, X) =

(cid:89)

y‚ààYmask

P (y|Yobs, X),

(3)

where Ymask are obtained by randomly replacing ground-truth tokens
with a special mask token <MASK>. The objective of the CMLM
decoder is deÔ¨Åned as:

Lcmlm = ‚àí log Pcmlm(Ymask|Yobs, X).

(4)

The Ô¨Ånal loss of Mask-CTC is deÔ¨Åned as a weighted sum of the stan-
dard CTC loss Lctc in Eq. (2) and Lcmlm in Eq. (4). During inference,
an output sequence of CTC is Ô¨Årst obtained from the encoder, and
then the decoder reÔ¨Ånes the CTC output through the mask predic-
tion process based on Eq. (3). By masking low-conÔ¨Ådence tokens
in the CTC output and predicting the masked tokens based on the

other high-conÔ¨Ådence unmasked tokens, errors from the conditional
independence assumption are expected to be recovered.

3.3. Improved Mask-CTC

One limitation of Mask-CTC is that the length of an output se-
quence cannot be changed from that of the CTC output during
inference, making it difÔ¨Åcult to recover deletion and insertion errors.
To overcome this problem, Mask-CTC is enhanced by introduc-
ing a length prediction network in the CMLM decoder [31]. The
length prediction network is trained to predict the length of a par-
tial target sequence from a masked token. For example, given a
ground-truth sequence Y = [y1, y2, y3, y4] and its masked sequence
[y1, <MASK>, y4, <MASK>], symbols 2 and 0 are predicted from
each masked position, indicating the length of the corresponding
partial sequence in Y . With this length prediction network, during
inference, the number of <MASK> at each masked position is Ô¨Årst
modiÔ¨Åed based on the predicted length. Each mask is then pre-
dicted conventionally as in Eq. (3), allowing the model to change the
sequence length dynamically by deleting and inserting mask tokens.

3.4. Align-Denoise

Align-Denoise [33] is built on the prior work of Align-ReÔ¨Åne [23]
and Imputer [18] and adopts a similar encoder-decoder structure on
frames. CTC is applied to both the encoder output and decoder out-
put. Instead of taking multiple steps to get the intermediate results
as Align-ReÔ¨Åne, Align-Denoise generates noisy CTC alignment ÀúZ
during the training and requires a single iteration for decoding. The
training objective is based on the CTC loss in Eq. (2) and a ground-
truth alignment Zgt = argmaxZ P (Z|X):
Laligndenoise = E ÀúZ‚àºq( ÀúZ|X,Zgt)

log Pctc(Y |X, ÀúZ)

(5)

(cid:105)

(cid:104)

,

where q(¬∑) is the noisy function for generating the noisy alignment.
The idea is similar to the denoising autoencoder [45] (DAE), where
the input to the decoder is the ground truth alignment with a certain
level of noise.

3.5. Intermediate CTC

Intermediate CTC [35] extends the CTC-based modeling with a reg-
ularization loss. During training, a sequence of intermediate repre-
sentations Xinter is obtained from an intermediate layer of the en-
coder, and its intermediate CTC loss is computed as:

Linter = ‚àí log Pctc(Y |Xinter).

(6)

The Ô¨Ånal loss is a weighted sum of the original CTC loss in Eq. (2)
and intermediate loss. During inference, the intermediate loss is un-
used, and the model is treated as an ordinary CTC model.

3.6. Self-conditioned CTC

Self-conditioned CTC [36] extends intermediate CTC by exploiting
the intermediate representations Xinter for conditioning the sub-
sequent encoder layers. During both training and inference, each
token posterior distribution in the intermediate layers Ainter =
softmax(Xinter) is fed back to the input of the next layer, mak-
ing the subsequent encoder layers conditioned on the intermediate
predictions. The self-conditioned CTC loss is deÔ¨Åned as:

Lselfcond = ‚àí log Pctc(Y |X, Ainter).

(7)

The Ô¨Ånal loss is a weighted sum of the intermediate CTC loss in
Eq. (6) and the self-conditioned CTC loss.

3.7. Insertion Transformer

In the case of insertion-based NAR models, the conditional proba-
bility P (Y |X) is modeled by marginalizing over insertion order œÄ:

Pins(Y |X) =

(cid:88)

œÄ

P (Y, œÄ|X) =

(cid:88)

œÄ

P (Y œÄ|X)P (œÄ).

(8)

Insertion order œÄ represents the permutation of tokens in a sequence
Y , e.g., if L = 4 and œÄ = (3, 1, 2, 4), Y œÄ = (y3, y1, y2, y4).

During training, an upper bound of negative log-likelihood is

minimized under a prior distribution over insertion order œÄ:

Lins = ‚àí

(cid:88)

œÄ

P (œÄ) log P (Y œÄ|X) ‚â• ‚àí log Pins(Y |X).

(9)

Insertion Transformer [26] trains Transformer, with an encoder-
to predict a token and its position to be in-
decoder structure,
serted, which aims to model P (Y œÄ|X) in Eq. (8). When P (œÄ)
is deÔ¨Åned by the balanced binary tree (BBT)-based insertion or-
der [26], decoding Ô¨Ånishes empirically with log2(L) iterations.
The BBT order is to insert the centermost tokens of the cur-
For example, given an output sequence with
rent hypothesis.
L = 7,
the hypothesis grows based on the tree structure like
(y4) ‚Üí (y2, y4, y6) ‚Üí (y1, y2, y3, y4, y5, y6, y7).

3.8. KERMIT

KERMIT [34] is a variant of Insertion Transformer. Its basic formu-
lation is the same but only the Transformer encoder is used to predict
a token and its position to be inserted.

KERMIT can be trained with the CTC loss in a multi-task learn-
ing manner [21], which makes the CTC alignment prediction con-
ditioned on a partial hypothesis from the insertion-based decoding.
Given the posterior probability of a CTC alignment at k-th decoding
step as P (Z|Y œÄk
, X), the objective of KERMIT is to minimize the
following negative log-likelihood:

Lkermit = Lins ‚àí Œªkermit log

(cid:88)

P (Z|Y œÄk

, X),

(10)

Z‚ààB‚àí1(Y )

where Œªkermit is a tunable weight on the CTC loss. During inference,
the CTC decoding is performed using P (Z|Y œÄk
, X) in Eq. (10),
and it can be terminated at any number of iterations.

3.9. CIF-NA

Continuous integrate-and-Ô¨Åre (CIF) [37] provides a soft and mono-
tonic alignment in the encoder-decoder framework. CIF Ô¨Årst learns
information weights (Œ±1, ...Œ±T ) from the encoder output Xenc and
accumulates the weights from left to right to locate the acoustic
boundary. Then, the acoustic embedding C = (c1, .., cL) for each
target token is obtained by integrating the encoder sates based on
their estimated weights. In this paper, we investigated CIF with an
NAR decoder, noted as CIF-NA. Following the prior work [39], both
the encoder states and acoustic embeddings are fed into the decoder
to predict the probability of output tokens in parallel:

Lcif = ‚àílog

L
(cid:89)

l=1

P (yl|C, Xenc).

(11)

CIF also adopts a quantity loss to supervise the model to predict the
quantity of the integrated embeddings closer to the length of a target
sequence, deÔ¨Åned as Lqua = | (cid:80)T
t=1 Œ±t ‚àí L|. The Ô¨Ånal loss of CIF-
NA is a weighted sum of the CTC loss in Eq. (2), Lcif and Lqua.

Table 2. Word error rate (WER) or character error rate (CER) on LibriSpeech-100h (LS-100), TEDLIUM2 (TED2), and CSJ-APS. #iter
denotes the number of iterations required to generate each token in an output sequence. Real time factor (RTF) was used to measure the
inference speed and was evaluated on the LS-100 ‚Äútest-other‚Äù set using CPU.

Model

AR

CTC/attention

+ beam-search

Transducer

+ beam-search

#iter

L
> L
L
> L

Inference speed

LS-100 (WER)

TED2 (WER)

CSJ-APS (CER)

RTF

Speedup

dev

test

clean

other

clean

other

dev

test

eval1

eval2

eval3

0.341
3.419
0.069
0.234

1.00√ó
0.10√ó
4.94√ó
1.46√ó

6.8
6.3
7.3
6.4

18.8
18.2
19.9
18.8

7.4
6.8
7.6
6.8

19.0
18.5
19.9
18.9

11.6
10.4
9.6
8.6

8.7
8.4
9.2
8.2

5.4
5.1
6.3
5.2

4.0
3.8
4.5
4.1

9.8
9.0
10.6
10.0

NAR

1
CTC
10
Mask-CTC
5
Improved Mask-CTC
1
Align-Denoise
1
Intermediate CTC
1
Self-conditioned CTC
(cid:39) log2(L)
KERMIT
Insertion Transformer (cid:39) log2(L)
CIF-NA‚Ä†

7.4
7.2
7.0
8.0
6.9
6.6
7.1
16.0
15.4
‚Ä†According to discussions with the author of CIF [37], CIF-NA suffers from the degradation due to the difÔ¨Åculty of acoustic boundary decisions.

5.78√ó
5.41√ó
4.74√ó
4.67√ó
5.78√ó
5.78√ó
1.06√ó
4.11√ó
4.67√ó

0.059
0.063
0.072
0.073
0.059
0.059
0.361
0.083
0.073

20.5
20.3
19.8
22.3
19.7
19.4
19.7
27.3
34.0

20.8
20.6
20.2
22.5
20.2
19.7
20.2
27.4
34.6

7.8
7.5
7.3
8.4
7.1
6.9
7.4
16.2
15.7

4.0
4.0
4.0
3.7
4.1
3.7
3.7
‚Äì
‚Äì

5.4
5.6
5.5
5.4
5.6
5.3
5.4
‚Äì
‚Äì

8.6
8.5
8.3
8.7
8.3
8.0
8.2
‚Äì
‚Äì

8.9
8.9
8.8
9.0
8.5
8.7
9.1
‚Äì
‚Äì

1

9.6
9.6
9.5
9.1
9.8
9.1
9.5
‚Äì
‚Äì

4. EXPERIMENTS

ters (Latin alphabets) were used for tokenizing target texts.

Aiming to compare the NAR models in Section 3, we conducted
ASR experiments using ESPnet [38, 46].
In addition to the NAR
models, we evaluated autoregressive (AR) models, including the
attention-based sequence-to-sequence model with the CTC/attention
objectives [47, 48] and Conformer-Transducer [49]. The recognition
accuracy was evaluated based on word error rate (WER) or character
error rate (CER), depending on a task, and the inference speed was
measured using real time factor (RTF).

4.1. Experimental setup

Data: The main experiments were carried out using three datasets,
including LibriSpeech (LS) [50], TEDLIUM2 (TED2) [51], and
Corpus of Spontaneous Japanese (CSJ) [52]. LS consists of ut-
terances from read English audiobooks, and we used the 100-hour
subset (LS-100) for training. TED2 contains utterances from English
Ted Talks, and we used the 210-hour training data. CSJ includes
Japanese public speeches on different academic topics, and we used
the 271-hour subset of academic presentation speech (CSJ-APS) for
training. For LS-100 and TED2, we used the standard validation and
test sets for tuning hyper-parameters and evaluating performance,
respectively. SpeciÔ¨Åcally, for LS-100, the validation and test sets are
divided into ‚Äúclean‚Äù and ‚Äúother‚Äù based on the quality of the recorded
utterances. For CSJ-APS, we used a part of the training set as a
validation set and the ofÔ¨Åcial evaluation sets (‚Äúeval1‚Äù, ‚Äúeval2‚Äù, and
‚Äúeval3‚Äù) for testing. For LS-100 and TED2, we used 300 to 500
subwords for tokenizing output texts, which were constructed from
each training set using SentencePiece [53]. For CSJ-APS, we used
Japanese syllable characters (Kana) and Chinese characters (Kanji),
which resulted in 2753 distinct tokens.

We also evaluated several models under noisy conditions using
CHiME-4 [54]. CHiME-4 contains English recordings in everyday
noisy environments, including a cafe, a street junction, public trans-
port, and a pedestrian area. We combined the CHiME-4 and Wall
Street Journal (WSJ) [55] datasets to obtain a 190-hour training set.
We used the validation and test sets provided by CHiME4 for tuning
hyper-parameters and evaluating performance, respectively. Charac-

As input speech features, we extracted 80 mel-scale Ô¨Ålterbank
coefÔ¨Åcients with three-dimensional pitch features using Kaldi [56].
To avoid overÔ¨Åtting, we applied speed perturbation [57] and SpecAug-
ment [58] to the input speech from LS-100, TED2, and CSJ-APS,
and only SpecAugment to the input speech from CHiME4.
Network architecture: All the end-to-end ASR models were con-
structed based on the Conformer-based architecture [11, 49]. For
the self-attention module, the number of heads dh, the dimension of
a self-attention layer dmodel, and the dimension of a feed-forward
network dÔ¨Ä were set to 4, 256, and 1024, respectively. The ker-
nel size of the convolution module was set to 15. For the mod-
els with the encoder-decoder structure (i.e., CTC/attention, Mask-
CTC, Improved Mask-CTC, Align-Denoise, Insertion Transformer,
and CIF-NA), the encoder consisted of two convolutional neural net-
work (CNN)-based downsampling layers followed by a stack of 12
Conformer encoder blocks. The decoder consisted of 6 Transformer
decoder blocks, where the self-attention module had the same con-
Ô¨Åguration as the Conformer block, except dÔ¨Ä was increased to 2048
to match the number of parameters in the Conformer block. For
the models without the decoder (i.e., CTC, KERMIT, Intermediate
CTC, Self-conditioned CTC), we used two CNN-based downsam-
pling layers followed by a stack of 18 Conformer encoder blocks.
Conformer-Transducer consisted of the 18-layer encoder and a sin-
gle long short-term memory (LSTM) layer for the decoder. Note that
the number of parameters in all the ASR models was around 30M.
Training and decoding conÔ¨Ågurations: The ASR models were
trained using the Adam optimizer [59] with Œ≤1 = 0.9, Œ≤2 = 0.98,
(cid:15) = 10‚àí9, and the Noam learning rate scheduling [60]. Warmup
steps (e.g., 4k to 25k) and a learning rate factor (e.g., 1.0 to 10.0)
were tuned for each model. We followed the same setup as in [14,
49] for regularization hyperparameters (e.g., dropout rate and label-
smoothing weight). The models were trained up to 300 epochs until
convergence. For evaluation, a Ô¨Ånal model was obtained by averag-
ing model parameters over 5 to 30 checkpoints with the best valida-
tion performance. For decoding with the CTC-based NAR models,
we performed the best path decoding of CTC [15] to keep the infer-
ence process NAR. For evaluating the AR models, we applied beam
search decoding with beam sizes of 1 (i.e., greedy decoding) and

10. All of the decodings were done without using external language
models (LMs). Decoding hyper-parameters that are unique to each
model were tuned following the previous works. To evaluate the
inference speed, RTF was measured using Intel(R) Xeon(R) Silver
4114 CPU, 2.20GHz on the same machine environment. All of the
implementations are publicly available1.

4.2. Main results

Table 2 shows the results on LS-100, TED2, and CSJ-APS.
LibriSpeech-100h (LS-100): By comparing the results obtained
from the NAR models on LS-100, all of our NAR models, except
Insertion Transformer and CIF-NA, outperformed the standard CTC-
based model. Moreover, on the ‚Äúclean‚Äù sets, Improved Mask-CTC,
intermediate CTC, self-conditioned CTC, and KERMIT achieved
comparable performances with those of the AR models. On the
‚Äúother‚Äù sets, in contrast, the NAR models resulted in worse perfor-
mances than the CTC/attention model. Since the ‚Äúother‚Äù sets in-
clude utterances with lower quality than the ‚Äúclean‚Äù sets, a model
is required to capture dependencies between output tokens to com-
pensate for information loss in the low-quality speech. With the
language model mechanism included in the network structure, the
CTC/attention model resulted in better performances on ‚Äúother‚Äù sets,
effectively modeling dependencies between output tokens.
TEDLIUM2 (TED2): On TED2, all the NAR models, including the
standard CTC-based model, achieved results competitive to those
obtained from the AR models. Especially on the development set,
the NAR models outperformed the CTC/attention model by a large
margin. We further discuss this interesting outcome in Section 4.5.
CSJ-APS: All the NAR models achieved comparable performance
with the AR models on CSJ-APS, indicating the effectiveness in-
dependent of the language. Especially, the performances of Align-
Denoise and Self-conditioned CTC aligned with the searched results
obtained from the CTC/attention model.
Discussions on Insertion Transformer and CIF-NA: Insertion
Transformer resulted in the performance drop because of its two
unique characteristics. The Ô¨Årst is that it is difÔ¨Åcult to judge whether
more tokens to be inserted or not. It is controlled by a special token
that represents ‚Äúno more token to be inserted‚Äù. However, partial
hypothesis including recognition errors leads to misjudging of token
insertion. The second is that once a hypothesis encounters recog-
nition error, it can not be recovered during inference. The error
is propagated to later iterations and leads to more errors. CIF is a
novel alignment mechanism that can be adopted for various ASR
scenarios. However, different from the original AR version, CIF-
NA resulted in the performance degradation due to the difÔ¨Åculty
of estimating acoustic boundaries in the English speech dataset.
According to [37, 39], it could achieve competitive performance
in monosyllable languages with clear acoustic boundaries (e.g.,
Chinese Mandarin).

From the above results, it can be concluded that CTC is the
key technique to realizing effective NAR end-to-end ASR, as
all the well-performing models are based on CTC. Overall, self-
conditioned CTC resulted in the best performance among the NAR
models, Ô¨Ålling the gap against the AR performance.

4.3. Recognition accuracy vs. inference speed

Table 3. Error analysis on LS-100. WERs on Table 2 are split into
substition (sub), deletion (del), and insertion (ins) error rates.
test-other

test-clean

Model

R CTC/attention
A

+ beam-search

R
A
N

CTC
Improved Mask-CTC
Intermediate CTC
Self-conditioned CTC
KERMIT

sub

5.3
5.4

6.3
5.9
5.7
5.6
6.0

del

1.2
0.6

0.7
0.6
0.6
0.6
0.6

ins

0.9
0.8

0.8
0.8
0.8
0.7
0.8

sub

14.5
14.7

16.6
16.2
16.2
15.8
16.2

del

2.4
1.8

2.2
2.0
1.9
2.0
2.0

ins

2.1
2.1

1.9
2.0
2.1
1.9
1.9

the ‚Äútest-other‚Äù set of LS-100, all the NAR models except KERMIT
achieved fast inference speed compared to the AR models. This is
because of the non-autoregressive nature of NAR models, i.e., it can
generate multiple tokens at a single iteration step. The RTF improve-
ment is achieved at the expense of quality-drop in WER. Especially
on the ‚Äúother‚Äù sets of LS-100, NAR models did not reach the per-
formance obtained from the CTC/attention model. In contrast, on
the ‚Äúclean‚Äù sets, several NAR models (e.g., self-conditioned CTC)
achieved better WERs than the greedy results of the CTC/attention
model. Improving the recognition of difÔ¨Åcult utterances is an impor-
tant problem for NAR models to be solved.

4.4. Error analysis

In Table 3, we break down WERs on LS-100 reported in Table 2. By
comparing the results obtained from CTC and the other NAR mod-
els, it appeared that the major improvements on our NAR models
are attributed to reducing substitution errors. Deletion and insertion
error rates, in contrast, were kept relatively low and stayed almost
the same from the CTC results, which can often be reduced by using
wordpieces. The AR model handled substitution errors more effec-
tively than the NAR models. However, deletion and insertion errors
were higher than those of the NAR models, indicating that the NAR
models are more effective at generating a sequence with a correct
length. From these results, it can be suggested that the key for fur-
ther improvement of NAR models is to mitigate substitution errors.

4.5. Robustness against output sequence length

Figure 2 shows the correlation between output sequence length and
error rate for the LS-100 test-clean set. Here, we observed that
the performance of the AR model is prone to degradation when the
length of an output sequence is long (Figure 2 top). On the other
hand, the NAR models successfully recognized the long sequence
without having such a severe quality drop. To further investigate the
results, we compared the error components between the AR and self-
conditioned CTC models (Figure 2 bottom). While the substitution
and insertion errors were in the same range between the two mod-
els, the deletion error in the AR model got signiÔ¨Åcantly high as the
output sequence length increased, where we observed consecutive
tokens in the sequence are completely skipped as reported in [61].
This explains why the performance of the AR models on the TED2
development set fell behind those of the NAR models (Table 2), as
the set includes long-form utterances up to 40 seconds.

We study the trade-off between recognition accuracy and inference
speed. According to RTF results in Table 2, which are measured on

4.6. Evaluation under noisy condition

1Mask-CTC, Improved Mask-CTC, Align-Denoise, Intermediate CTC,

Self-conditioned CTC, Insertion Transformer, KERMIT, CIF-NA

Table 4 shows results under noisy conditions based on the CHiME4
task. Note that we only focused on the models achieving promis-

Table 5. Word error rate (WER) on LS-100 and CHiME4 test sets
for Mask-CTC and its extension with intermediate CTC.

Model

LS-100

CHiME4

clean

other

real

simu

Mask-CTC

+ Intermediate CTC

7.5
7.2

20.6
20.4

24.9
24.9

25.8
25.0

Table 6. BLEU (‚Üë) scores of speech translation models on Fisher-
CallHome Spanish. #iter is the number of iterations.

Model

Fisher

CallHome

dev

dev2

test

devtest

evltest

51.0
CTC
51.1
Mask-CTC
Intermediate CTC
51.3
Self-conditioned CTC 50.7
Orthros (#iter = 4)
50.1
Orthros (#iter = 10)
51.6

51.6
51.7
51.4
51.2
50.6
52.4

50.8
50.6
51.0
50.5
48.7
50.5

18.0
17.9
19.0
19.1
19.5
20.5

18.7
18.3
19.0
19.2
19.8
20.7

4.8. Application to end-to-end speech translation (E2E-ST)

We applied Mask-CTC, intermediate CTC, self-conditioned CTC,
and Orthros [62] to the NAR E2E-ST task on Fisher-CallHome
Spanish corpus [63]. Orthros is based on CMLM but has an addi-
tional AR decoder on the same encoder to select the most probable
translation among multiple length candidates. All models used the
Conformer encoder and 16k wordpieces as output units. The encoder
parameters were initialized with a pre-trained AR ASR encoder. We
followed the setting in [62] and applied sequence-level knowledge
distillation [64]. With the help of the NAR modeling methods, we
observed some gains over the CTC-based results (Table 6). While
self-conditioned CTC was the most effective method for ASR, it
had smaller impact on the E2E-ST task, and Orthros resulted in the
best performance. Since input-output alignments are not monotonic
in this task, token-level iterative reÔ¨Ånement is required to make the
sequence generation conditioned on a translated sequence, rather
than only depending on acoustic information from input speech.

5. CONCLUSIONS

This paper presented a comprehensive study of NAR modeling meth-
ods for end-to-end ASR. Various NAR models were compared based
on different ASR tasks. The results provided several interesting Ô¨Ånd-
ings, including the accuracy-speed trade-off and robustness against
long-form speech utterances. We also showed that different NAR
techniques could be combined to improve the performance and ap-
plied to end-to-end speech translation. We believe that the repro-
ducible implementations and recipes used in this paper will acceler-
ate further NAR research in speech processing.

6. ACKNOWLEDGEMENT

This work was partly supported by ASAPP. This work used the Ex-
treme Science and Engineering Discovery Environment (XSEDE)
[65], which is supported by National Science Foundation grant
number ACI-1548562. SpeciÔ¨Åcally, it used the Bridges system [66],
which is supported by NSF award number ACI-1445606, at the
Pittsburgh Supercomputing Center (PSC). The authors would like to
thank Linhao Dong and Florian Boyer for helpful discussions.

Fig. 2. Comparison of NAR and AR (CTC/attention) models eval-
uated on different output sequence length. WERs are compared
among the models (top), and the error components are compared
between AR and Self-conditioned CTC (bottom).

Table 4. Word error rate on CHiME4.

Model

dt05

et05

real

simu

real

simu

R CTC/attention
A

+ beam-search

14.0
13.9

R
A
N

16.5
CTC
14.9
Improved Mask-CTC
Intermediate CTC
15.2
Self-conditioned CTC 14.6

16.0
15.7

18.1
16.8
16.8
16.6

22.2
22.4

25.9
24.6
24.2
23.7

24.1
23.7

27.0
25.2
25.2
24.3

ing results on the main tasks. The evaluations were performed on
the 1-channel track, where the development (dt05) and test (et05)
sets included real and simulated (simu) utterances recorded from
one of a single microphones on the tablet device [54]. Comparing
the results obtained from the NAR models, all of our NAR models
outperformed the standard CTC-based model. However, the results
were not comparable with those obtained from the AR model. With
the nonstationary noises included in the input speech, it is crucial
for an ASR model to attend to dependency among output tokens for
generating an accurate sequence. As observed in the LS-100 task in
Section 4.2, the results suggest that the NAR models are likely to de-
pend more on acoustic information, having difÔ¨Åculty capturing the
token dependencies when the quality of an input speech is low.

4.7. Combination of different techniques

As intermediate CTC is a regularization method for CTC modeling
and does not require any architectural changes, it is possible to aug-
ment other CTC-based NAR modeling with intermediate CTC. We
combine Mask-CTC and intermediate CTC by adding an interme-
diate CTC loss to the encoder of Mask-CTC, as proposed in [35].
Table 5 shows the comparison of Mask-CTC and its intermediate
CTC extension on LS-100 and CHiME-4. In all cases, intermediate
CTC improved Mask-CTC, similar to experimental results reported
by [35], while requiring no extra computation during inference.

020406080100Output sequence length01020304050WER [%]ARCTCImproved Mask-CTCIntermediate CTCSelf-conditioned CTCKERMIT020406080100Output sequence length0102030Error rate [%]AR sub. err.AR ins. err.AR del. err.Self-cond. CTC sub. err.Self-cond. CTC ins. err.Self-cond. CTC del. err.7. REFERENCES

[1] Geoffrey Hinton, Li Deng, Dong Yu, George E Dahl, Abdel-
rahman Mohamed, Navdeep Jaitly, Andrew Senior, Vincent
Vanhoucke, Patrick Nguyen, Tara N Sainath, et al., ‚ÄúDeep neu-
ral networks for acoustic modeling in speech recognition: The
shared views of four research groups,‚Äù IEEE Signal Process.
Mag., vol. 29, no. 6, 2012.

[2] Alex Graves, Abdelrahman Mohamed, and Geoffrey Hinton,
‚ÄúSpeech recognition with deep recurrent neural networks,‚Äù in
Proc. ICASSP, 2013, pp. 6645‚Äì6649.

[3] Alex Graves and Navdeep Jaitly, ‚ÄúTowards end-to-end speech
recognition with recurrent neural networks,‚Äù in Proc. ICML,
2014, pp. 1764‚Äì1772.

[4] Jan K Chorowski, Dzmitry Bahdanau, Dmitriy Serdyuk,
Kyunghyun Cho, and Yoshua Bengio, ‚ÄúAttention-based mod-
els for speech recognition,‚Äù in Proc. NeurIPS, 2015, pp. 577‚Äì
585.

[5] William Chan, Navdeep Jaitly, Quoc Le, and Oriol Vinyals,
‚ÄúListen, attend and spell: A neural network for large vocab-
ulary conversational speech recognition,‚Äù in Proc. ICASSP,
2016, pp. 4960‚Äì4964.

[6] Alex Graves, ‚ÄúSequence transduction with recurrent neural

networks,‚Äù arXiv preprint arXiv:1211.3711, 2012.

[7] Ilya Sutskever, Oriol Vinyals, and Quoc V Le, ‚ÄúSequence to
sequence learning with neural networks,‚Äù in Proc. NeurIPS,
2014, pp. 3104‚Äì3112.

[8] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio,
‚ÄúNeural machine translation by jointly learning to align and
translate,‚Äù in Proc. ICLR, 2014.

[9] Linhao Dong, Shuang Xu, and Bo Xu, ‚ÄúSpeech-Transformer:
a no-recurrence sequence-to-sequence model for speech recog-
nition,‚Äù in Proc. ICASSP, 2018, pp. 5884‚Äì5888.

[10] Samuel Kriman, Stanislav Beliaev, Boris Ginsburg, Jocelyn
Huang, Oleksii Kuchaiev, Vitaly Lavrukhin, Ryan Leary, Ja-
son Li, and Yang Zhang, ‚ÄúQuartznet: Deep automatic speech
recognition with 1d time-channel separable convolutions,‚Äù in
Proc. ICASSP, 2020, pp. 6124‚Äì6128.

[11] Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Par-
mar, Yu Zhang, Jiahui Yu, Wei Han, Shibo Wang, Zheng-
dong Zhang, Yonghui Wu, and Ruoming Pang, ‚ÄúConformer:
Convolution-augmented Transformer for speech recognition,‚Äù
in Proc. Interspeech, 2020, pp. 5036‚Äì5040.

[12] Chung-Cheng Chiu, Tara N Sainath, Yonghui Wu, Rohit Prab-
havalkar, Patrick Nguyen, Zhifeng Chen, Anjuli Kannan, Ron J
Weiss, Kanishka Rao, Ekaterina Gonina, et al., ‚ÄúState-of-the-
art speech recognition with sequence-to-sequence models,‚Äù in
Proc. ICASSP, 2018, pp. 4774‚Äì4778.

[13] Christoph L¬®uscher, Eugen Beck, Kazuki Irie, Markus Kitza,
Wilfried Michel, Albert Zeyer, Ralf Schl¬®uter, and Hermann
Ney, ‚ÄúRWTH ASR systems for LibriSpeech: Hybrid vs at-
tention,‚Äù in Proc. Interspeech, 2019, pp. 231‚Äì235.

[14] Shigeki Karita, Nanxin Chen, Tomoki Hayashi, Takaaki Hori,
Hirofumi Inaguma, Ziyan Jiang, Masao Someki, Nelson En-
rique Yalta Soplin, Ryuichi Yamamoto, Xiaofei Wang, et al.,
‚ÄúA comparative study on Transformer vs RNN in speech appli-
cations,‚Äù in Proc. ASRU, 2019, pp. 449‚Äì456.

[15] Alex Graves, Santiago Fern¬¥andez, Faustino Gomez, and J¬®urgen
Schmidhuber,
‚ÄúConnectionist temporal classiÔ¨Åcation: La-
belling unsegmented sequence data with recurrent neural net-
works,‚Äù in Proc. ICML, 2006, pp. 369‚Äì376.

[16] Jiatao Gu, James Bradbury, Caiming Xiong, Victor OK Li, and
Richard Socher, ‚ÄúNon-autoregressive neural machine transla-
tion,‚Äù in Proc. ICLR, 2018.

[17] Nanxin Chen, Shinji Watanabe, Jesus Antonio Villalba, Piotr
Zelasko, and Najim Dehak, ‚ÄúNon-autoregressive transformer
for speech recognition,‚Äù IEEE Signal Process. Lett., 2020.
[18] William Chan, Chitwan Saharia, Geoffrey Hinton, Mohammad
Norouzi, and Navdeep Jaitly, ‚ÄúImputer: Sequence modelling
via imputation and dynamic programming,‚Äù in Proc. ICML,
2020, pp. 1403‚Äì1413.

[19] Yosuke Higuchi, Shinji Watanabe, Nanxin Chen, Tetsuji
Ogawa, and Tetsunori Kobayashi,
‚ÄúMask CTC: Non-
autoregressive end-to-end ASR with CTC and mask predict,‚Äù
in Proc. Interspeech, 2020, pp. 3655‚Äì3659.

[20] Ye Bai, Jiangyan Yi, Jianhua Tao, Zhengkun Tian, Zhengqi
Wen, and Shuai Zhang, ‚ÄúListen attentively, and spell once:
Whole sentence generation via a non-autoregressive architec-
ture for low-latency speech recognition,‚Äù in Proc. Interspeech,
2020, pp. 3381‚Äì3385.

[21] Yuya Fujita, Shinji Watanabe, Motoi Omachi, and Xuankai
Chang, ‚ÄúInsertion-based modeling for end-to-end automatic
speech recognition,‚Äù in Proc. Interspeech, 2020, pp. 3660‚Äì
3664.

[22] Zhengkun Tian, Jiangyan Yi, Jianhua Tao, Ye Bai, Shuai
Zhang, and Zhengqi Wen, ‚ÄúSpike-triggered non-autoregressive
Transformer for end-to-end speech recognition,‚Äù in Proc. In-
terspeech, 2020, pp. 5026‚Äì5030.

[23] Ethan A Chi, Julian Salazar, and Katrin Kirchhoff, ‚ÄúAlign-
ReÔ¨Åne: Non-autoregressive speech recognition via iterative re-
alignment,‚Äù in Proc. NAACL-HLT, 2021, pp. 1920‚Äì1927.

[24] JindÀárich Libovick¬¥y and JindÀárich Helcl,

‚ÄúEnd-to-end non-
autoregressive neural machine translation with connectionist
temporal classiÔ¨Åcation,‚Äù in Proc. EMNLP, 2018, pp. 3016‚Äì
3021.

[25] Jason Lee, Elman Mansimov, and Kyunghyun Cho, ‚ÄúDeter-
ministic non-autoregressive neural sequence modeling by iter-
ative reÔ¨Ånement,‚Äù in Proc. EMNLP, 2018, pp. 1173‚Äì1182.
[26] Mitchell Stern, William Chan, Jamie Kiros, and Jakob Uszko-
reit, ‚ÄúInsertion Transformer: Flexible sequence generation via
insertion operations,‚Äù in Proc. ICML, 2019, pp. 5976‚Äì5985.

[27] Jiatao Gu, Changhan Wang, and Junbo Zhao, ‚ÄúLevenshtein
Transformer,‚Äù in Proc. NeurIPS, 2019, pp. 11181‚Äì11191.
[28] Marjan Ghazvininejad, Omer Levy, Yinhan Liu, and Luke
Zettlemoyer, ‚ÄúMask-predict: Parallel decoding of conditional
masked language models,‚Äù in Proc. EMNLP-IJCNLP, 2019,
pp. 6114‚Äì6123.

[29] Chitwan Saharia, William Chan, Saurabh Saxena, and Moham-
mad Norouzi, ‚ÄúNon-autoregressive machine translation with
latent alignments,‚Äù in Proc. EMNLP, 2020, pp. 1098‚Äì1108.

[30] Xuezhe Ma, Chunting Zhou, Xian Li, Graham Neubig, and
Eduard Hovy, ‚ÄúFlowSeq: Non-autoregressive conditional se-
quence generation with generative Ô¨Çow,‚Äù in Proc. EMNLP-
IJCNLP, 2019, pp. 4273‚Äì4283.

[31] Yosuke Higuchi, Hirofumi Inaguma, Shinji Watanabe, Tetsuji
Ogawa, and Tetsunori Kobayashi, ‚ÄúImproved Mask-CTC for
non-autoregressive end-to-end ASR,‚Äù in Proc. ICASSP, 2021,
pp. 8363‚Äì8367.

[32] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
Toutanova, ‚ÄúBERT: Pre-training of deep bidirectional trans-
formers for language understanding,‚Äù in Proc. NAACL-HLT,
2019, pp. 4171‚Äì4186.

[33] Nanxin Chen, Piotr ÀôZelasko, Laureano Moro-Vel¬¥azquez, Jes¬¥us
Villalba, and Najim Dehak, ‚ÄúAlign-Denoise: Single-pass non-
autoregressive speech recognition,‚Äù in Proc. Interspeech, 2021.
[34] William Chan, Nikita Kitaev, Kelvin Guu, Mitchell Stern, and
Jakob Uszkoreit, ‚ÄúKERMIT: Generative insertion-based mod-
eling for sequences,‚Äù arXiv preprint arXiv:1906.01604, 2019.

[35] Jaesong Lee and Shinji Watanabe, ‚ÄúIntermediate loss regular-
ization for CTC-based speech recognition,‚Äù in Proc. ICASSP,
2021, pp. 6224‚Äì6228.

[36] Jumon Nozaki and Tatsuya Komatsu, ‚ÄúRelaxing the condi-
tional independence assumption of CTC-based ASR by con-
ditioning on intermediate predictions,‚Äù in Proc. Interspeech,
2021.

[37] Linhao Dong and Bo Xu, ‚ÄúCIF: Continuous integrate-and-Ô¨Åre
for end-to-end speech recognition,‚Äù in Proc. ICASSP, 2020,
pp. 6079‚Äì6083.

[38] Shinji Watanabe, Takaaki Hori, Shigeki Karita, Tomoki
Hayashi, Jiro Nishitoba, Yuya Unno, Nelson Enrique Yalta So-
plin, Jahn Heymann, Matthew Wiesner, Nanxin Chen, Adithya
Renduchintala, and Tsubasa Ochiai,
‚ÄúESPnet: End-to-end
in Proc. Interspeech, 2018, pp.
speech processing toolkit,‚Äù
2207‚Äì2211.

[39] Fan Yu, Haoneng Luo, Pengcheng Guo, Yuhao Liang,
Zhuoyuan Yao, Lei Xie, Yingying Gao, Leijing Hou, and
Shilei Zhang, ‚ÄúBoundary and context aware training for CIF-
based non-autoregressive end-to-end ASR,‚Äù arXiv preprint
arXiv:2104.04702, 2021.

[40] Eric Battenberg, Jitong Chen, Rewon Child, Adam Coates,
Yashesh Gaur Yi Li, Hairong Liu, Sanjeev Satheesh, Anuroop
Sriram, and Zhenyao Zhu, ‚ÄúExploring neural transducers for
end-to-end speech recognition,‚Äù in Proc. ASRU, 2017, pp. 206‚Äì
213.

[41] Andros Tjandra, Chunxi Liu, Frank Zhang, Xiaohui Zhang,
Yongqiang Wang, Gabriel Synnaeve, Satoshi Nakamura, and
Geoffrey Zweig, ‚ÄúDeja-vu: Double feature presentation and
iterated loss in deep transformer networks,‚Äù in Proc. ICASSP,
2020, pp. 6899‚Äì6903.

[42] Edwin G Ng, Chung-Cheng Chiu, Yu Zhang, and William
Chan, ‚ÄúPushing the limits of non-autoregressive speech recog-
nition,‚Äù in Proc. Interspeech, 2021.

[43] Somshubra Majumdar, Jagadeesh Balam, Oleksii Hrinchuk,
Vitaly Lavrukhin, Vahid Noroozi, and Boris Ginsburg, ‚ÄúCit-
rinet: Closing the gap between non-autoregressive and autore-
gressive end-to-end models for automatic speech recognition,‚Äù
arXiv preprint arXiv:2104.01721, 2021.

[44] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and
Michael Auli, ‚Äúwav2vec 2.0: A framework for self-supervised
learning of speech representations,‚Äù in Proc. NeurIPS, 2020.

[50] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev
Khudanpur, ‚ÄúLibrispeech: An ASR corpus based on public
domain audio books,‚Äù in Proc. ICASSP, 2015, pp. 5206‚Äì5210.
[51] Anthony Rousseau et al., ‚ÄúEnhancing the TED-LIUM corpus
with selected data for language modeling and more TED talks,‚Äù
in Porc. LREC, 2014, pp. 3935‚Äì3939.

[52] Kikuo Maekawa, Hanae Koiso, Sadaoki Furui, and Hitoshi Isa-
in Proc.
‚ÄúSpontaneous speech corpus of Japanese.,‚Äù

hara,
LREC, 2000, pp. 1‚Äì5.

[53] Taku Kudo, ‚ÄúSubword regularization: Improving neural net-
work translation models with multiple subword candidates,‚Äù in
Proc. ACL, 2018.

[54] Emmanuel Vincent, Shinji Watanabe, Aditya Arie Nugraha,
Jon Barker, and Ricard Marxer, ‚ÄúAn analysis of environment,
microphone and data simulation mismatches in robust speech
recognition,‚Äù Computer Speech & Language, vol. 46, pp. 535‚Äì
557, 2017.

[55] Douglas B Paul and Janet M Baker,

‚ÄúThe design for the
wall street journal-based CSR corpus,‚Äù in Proc. Workshop on
Speech and Natural Language, 1992, pp. 357‚Äì362.

[56] Daniel Povey, Arnab Ghoshal, Gilles Boulianne, Lukas Bur-
get, Ondrej Glembek, Nagendra Goel, Mirko Hannemann, Petr
Motlicek, Yanmin Qian, Petr Schwarz, et al.,
‚ÄúThe Kaldi
speech recognition toolkit,‚Äù in Proc. ASRU, 2011.

[57] Tom Ko, Vijayaditya Peddinti, Daniel Povey, and Sanjeev Khu-
danpur, ‚ÄúAudio augmentation for speech recognition,‚Äù in Proc.
Interspeech, 2015, pp. 3586‚Äì3589.

[58] Daniel S Park, William Chan, Yu Zhang, Chung-Cheng Chiu,
Barret Zoph, Ekin D Cubuk, and Quoc V Le, ‚ÄúSpecAugment:
A simple data augmentation method for automatic speech
recognition,‚Äù in Proc. Interspeech, 2019, pp. 2613‚Äì2617.
[59] Diederik P Kingma and Jimmy Ba, ‚ÄúAdam: A method for

stochastic optimization,‚Äù in Proc. ICLR, 2015.

[60] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polo-
sukhin, ‚ÄúAttention is all you need,‚Äù in Proc. NeurIPS, 2017,
pp. 5998‚Äì6008.

[61] Jan Chorowski and Navdeep Jaitly, ‚ÄúTowards better decoding
and language model integration in sequence to sequence mod-
els,‚Äù in Proc. Interspeech, 2017, pp. 523‚Äì527.

[45] Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-
Antoine Manzagol, ‚ÄúExtracting and composing robust features
with denoising autoencoders,‚Äù in Proc. ICML, 2008, pp. 1096‚Äì
1103.

[62] Hirofumi Inaguma, Yosuke Higuchi, Kevin Duh, Tatsuya
Kawahara, and Shinji Watanabe, ‚ÄúOrthros: Non-autoregressive
in Proc.
end-to-end speech translation with dual-decoder,‚Äù
ICASSP, 2021, pp. 7503‚Äì7507.

[46] Shinji Watanabe, Florian Boyer, Xuankai Chang, Pengcheng
Guo, Tomoki Hayashi, Yosuke Higuchi, Takaaki Hori, Wen-
Chin Huang, Hirofumi Inaguma, Naoyuki Kamo, et al., ‚ÄúThe
2020 ESPNet update: New features, broadened applications,
performance improvements, and future plans,‚Äù in 2021 IEEE
Data Science and Learning Workshop (DSLW), 2021, pp. 1‚Äì6.
[47] Shinji Watanabe, Takaaki Hori, Suyoun Kim, John R Hershey,
and Tomoki Hayashi, ‚ÄúHybrid CTC/attention architecture for
end-to-end speech recognition,‚Äù IEEE Journal of Selected Top-
ics in Signal Processing, vol. 11, no. 8, pp. 1240‚Äì1253, 2017.
[48] Shigeki Karita, Nelson Enrique Yalta Soplin, Shinji Watan-
abe, Marc Delcroix, Atsunori Ogawa, and Tomohiro Nakatani,
‚ÄúImproving Transformer-based end-to-end speech recognition
with connectionist temporal classiÔ¨Åcation and language model
integration,‚Äù in Proc. Interspeech, 2019, pp. 1408‚Äì1412.
[49] Pengcheng Guo, Florian Boyer, Xuankai Chang, Tomoki
Hayashi, Yosuke Higuchi, Hirofumi Inaguma, Naoyuki Kamo,
Chenda Li, Daniel Garcia-Romero, Jiatong Shi, et al., ‚ÄúRecent
developments on ESPnet toolkit boosted by Conformer,‚Äù in
Proc. ICASSP, 2021, pp. 5874‚Äì5878.

[63] Matt Post, Gaurav Kumar, Adam Lopez, Damianos Karakos,
‚ÄúImproved
Chris Callison-Burch, and Sanjeev Khudanpur,
speech-to-text
translation with the Fisher and Callhome
Spanish‚ÄìEnglish speech translation corpus,‚Äù in Proc. IWSLT,
2013.

[64] Yoon Kim and Alexander M. Rush, ‚ÄúSequence-level knowl-
edge distillation,‚Äù in Proc. EMNLP, 2016, pp. 1317‚Äì1327.
[65] John Towns, Timothy Cockerill, Maytal Dahan, Ian Foster,
Kelly Gaither, Andrew Grimshaw, Victor Hazlewood, Scott
Lathrop, Dave Lifka, Gregory D Peterson, et al., ‚ÄúXSEDE:
Accelerating scientiÔ¨Åc discovery,‚Äù Computing in Science &
Engineering, vol. 16, no. 5, pp. 62‚Äì74, 2014.

[66] Nicholas A Nystrom, Michael J Levine, Ralph Z Roskies, and
J Ray Scott,
‚ÄúBridges: a uniquely Ô¨Çexible HPC resource
for new communities and data analytics,‚Äù in Proc. the 2015
XSEDE Conference: ScientiÔ¨Åc Advancements Enabled by En-
hanced Cyberinfrastructure, 2015, pp. 1‚Äì8.

