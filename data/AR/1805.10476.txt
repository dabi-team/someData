L1-2D2PCANet: A Deep Learning Network for Face 
Recognition

Yun-Kun Li1, Xiao-Jun Wu1*, Josef Kittler2 
1 Jiangsu Provincial Engineering Laboratory of Pattern Recognition and Computational Intelligence,  
Jiangnan University, 214122, Wuxi, China 
2 Center for Vision, Speech and Signal Processing(CVSSP), University of Surry, GU2 7XH, Guildford, UK 
{m15979095336, xiaojun_wu_jnu}@163.com, j.kittler@surrey.ac.uk 

two-directional 

Abstractâ€”In  this  paper,  we  propose  a  novel  deep  learning 
network  L1-2D2PCANet  for  face  recognition,  which  is  based  on 
L1-norm-based 
two-dimensional  principal 
component analysis (L1-2D2PCA). In our network, the role of L1-
2D2PCA is to learn the filters of multiple convolution layers. After 
the convolution layers, we deploy binary hashing and block-wise 
histogram for pooling. We test our network on some benchmark 
facial datasets YALE, AR, Extended Yale B, LFW-a and FERET 
with CNN, PCANet, 2DPCANet and L1-PCANet as comparison. 
The  results  show  that  the  recognition  performance  of  L1-
2D2PCANet in all tests is better than baseline networks, especially 
when there are outliers in the test data. Owing to the L1-norm, L1-
2D2PCANet  is  robust  to  outliers  and  changes  of  the  training 
images. 

Keywordsâ€”face recognition, deep learning, L1-2D2PCA, outlier  

I.   INTRODUCTION 

In pattern recognition and computer vision, face recognition 
is  a  very  important  research  field  [1,2,3,4].  Due  to  the 
complexity of facial features and the difficulty of manual feature 
selection [1], it is commonly agreed that the best features can be 
obtained by using unsupervised feature extraction methods [3,4]. 

Recently,  with  Google  AlphaGo  Zero  defeating  many  Go 
masters, deep learning has received intensive attentions [5,6]. As 
a classical deep learning model, Convolution Neural Networks 
(CNNs)  with  convolution  and  pooling  layers  have  achieved 
astonishing results in many image recognition tasks, reaching an 
unprecedented  accuracy  [7,8].  However,  CNN  still  has  many 
shortcomings. During training a CNN model, researchers need 
to  obtain  a  huge  amount  of  parameters,  which  leads  to  large 
computational cost. 

To solve this problem, researchers  are committed to find a 
simple  CNN  model  which  requires  a  small  number  of 
parameters. Chan et al. proposed PCANet [9], which is a simple 
deep learning network based on unsupervised learning. PCANet 
uses PCA to learn the filters and deploys simple binary hashing 
and  block  histograms  for  indexing  and  pooling.  Unlike  other 
CNNs  that  learn  filters  by  back  propagation,  PCANet  learns 
filters  using  the  PCA  method.  Thus  PCANet  requires  less 

computational  cost, 
time  and  storage  space.  The 
experimental  results  show  the  astonishing  performance  of 
PCANet. 

less 

The PCA method used by PCANet is based on 1D vectors. 
Before deploying PCA, we need to convert 2D image matrices 
into 1D vectors which will cause two major problems: (1) Some 
spatial information of image is implied in the 2D structure of the 
image.  Obviously,  the  intrinsic  information  is discarded  when 
the image matrix is converted into 1D vector. (2) the long 1D 
vector leads to the requirement of large computational time and 
storage  space  in  computing  the  eigenvectors.  To  solve  these 
problems,  Yu  et  al.  proposed  two-dimensional  principal 
component analysis network (2DPCANet) [10], which replaces 
PCA with 2DPCA [11,12,13]. 

However,  Both  PCA  and  2DPCA  are  based  on  L2-norm 
method. It is well known that the methods based on L2-norm are 
sensitive to outliers so that data with outliers can totally ruin the 
results from the desired methods. To solve this problem, Nojun 
et al. proposed a novel PCA method based on L1-norm [14]. L1-
norm is widely considered to be more robust to outliers [15,16]. 
L1-PCA adopts the L1-norm for measuring the reconstruction 
error. On this basis, Li et al. proposed L1-norm-based 2DPCA 
[17]. 

In  this  paper,  L1-norm  was  introduced  into  PCANet  to        

get  L1-PCANet.  Then  we  generalize  L1-PCANet  to  L1-
2D2PCANet.  L1-2D2PCANet  and  2DPCANet  share  the  same 
structure  to  generate  the  feature  of  input  data  but  L1-
2D2PCANet  learns  filters  by  L1-2DPCA.  In  addition,  we  use 
Support  Vector Machine  (SVM) as  classifiers  for  the  features 
generated  by  the  networks.  To  test  the  performance  of  L1-
2D2PCANet, we compare it with other three networks (PCANet, 
2DPCANet and L1-PCANet) on Yale, AR [18], Extended Yale 
B [19], LFW-a [20] and FERET [21] face databases. 

The rest of paper is organized as follows. Section â…¡ reviews 
related work on L1-PCA and L1-2DPCA. L1-PCANet is given 
in  Section  â…¢  and  L1- 2D2 PCANet  is  given  in  Section  â…£. 
Section â…¤ reports the results and analysis of the experiments and 
Section â…¥ concludes this paper. 

 
 
 
Fig.1 The illustration of two-layer L1-PCANet 

Fig.2 The illustration of two-layer L1-2D2PCANet 

II.  RELATERD WORK 

A.  L1-norm-based PCA 
  The  proposed  L1-PCANet  is  based  on  L1-PCA  [14,15]. 
PCA-L1 is considered as the simplest and most efficient among 
many  models 
of  L1-norm-based  PCA.  Let  ğ‘‹ =
[ğ‘¥1, ğ‘¥2, â€¦ , ğ‘¥ğ‘] âˆˆ â„ğ·Ã—ğ‘ ,  with  ğ‘¥ğ‘– = ğ‘šğ‘ğ‘¡ğ·(ğ¼ğ‘–) âˆˆ â„ğ·Ã—1 ( ğ‘– =
1,2, â€¦ , ğ‘).  The ğ‘šğ‘ğ‘¡ğ·(ğ¼) is  a  function  that  maps  a  matrix ğ¼ âˆˆ
â„ğ‘šÃ—ğ‘›  to  a  vector  ğ‘£ âˆˆ â„ğ·Ã—1  and  ğ· = ğ‘š Ã— ğ‘› .  Suppose  w âˆˆ
â„ğ·Ã—1 be  the  principal  vector  to  be  obtained.  Here,  we  set  the 
number  of  principal  vectors  to  one  to  simplify  the  procedure. 
The objective of PCA-L1 is to maximize the L1-norm variance 
in the feature space as follows: 

ğ‘

ğ‘“(w) = â€–ğ‘¤ğ‘‡ğ‘‹â€–1 = âˆ‘|ğ‘¤ğ‘‡ğ‘¥ğ‘–|

ğ‘–=1

ğ‘ ğ‘¢ğ‘ğ‘—ğ‘’ğ‘ğ‘¡ ğ‘¡ğ‘œ â€–ğ‘¤â€–2 = 1,                            (1) 

where â€–âˆ™â€– denotes L2-norm and |âˆ™| denotes L1-norm. 

  To solve the computational problems posed by the symbol 
of  absolute  value,  we  introduce  a  polarity  parameter  ğ‘ğ‘–  in 
Equation (1)  

ğ‘ğ‘– = {

   1, ğ‘¤â„ğ‘’ğ‘› ğ‘¤ğ‘‡ğ‘¥ğ‘– â‰¥ 0
âˆ’1, ğ‘¤â„ğ‘’ğ‘› ğ‘¤ğ‘‡ğ‘¥ğ‘– < 0.

                           (2) 

  By introducing ğ‘ğ‘–, Equation 1 can be rewritten as: 

ğ‘“(w) = âˆ‘ ğ‘ğ‘–

ğ‘
ğ‘–=1 ğ‘¤ğ‘‡ğ‘¥ğ‘–.                              (3) 

  The process of maximization is achieved by Algorithm 1. 

  Here, ğ‘¡ denotes the number of iterations and w(t) and ğ‘ğ‘–(ğ‘¡) 
denote w and ğ‘ğ‘– during iteration ğ‘¡. 
  By  the  above  algorithm,  we  can  obtain  the  first  principal 
âˆ—(ğ‘˜ > 1) ,  we  have  to  update  the 
vector  w1
training data as: 

âˆ— .  To  compute  wğ‘˜

ğ‘˜ = ğ‘¥ğ‘–
ğ‘¥ğ‘–

ğ‘˜âˆ’1 âˆ’ ğ‘¥ğ‘–

ğ‘˜âˆ’1(wğ‘˜âˆ’1

âˆ—
âˆ— wğ‘˜âˆ’1

ğ‘‡).                   (4) 

 
 
 
 
 
Algorithm 1: L1-PCA method 

Input:   

ï¬ 

training set: ğ‘¿ = [ğ’™ğŸ, ğ’™ğŸ, â€¦ , ğ’™ğ‘µ] âˆˆ â„ğ‘«Ã—ğ‘µ 

Output:  

ï¬ 

filters ğ°âˆ— 

1: set ğ’˜(ğŸ) = ğŸ and ğ­ = ğŸ 

2: For all ğ¢ âˆˆ {ğŸ, ğŸ, â€¦ , ğ‘µ}, calculate ğ’‘ğ’Š(ğ’•) by using Equation (2) 
3:  Let  ğ­ = ğ­ + ğŸ  and  ğ°(ğ­) = âˆ‘ ğ’‘ğ’Š(ğ’• âˆ’ ğŸ)ğ’™ğ’Š
â€–ğ°(ğ­)â€–ğŸ 
4:  If ğ’˜(ğ’•) â‰  ğ’˜(ğ’• âˆ’ ğŸ),  go back to  Step 2.  Otherwise, set   ğ’˜âˆ— = ğ’˜(ğ’•) and 
stop.    

.Then  let  ğ°(ğ­) = ğ’˜(ğ’•)/

ğ‘µ
ğ’Š=ğŸ

B.  L1-norm-based 2DPCA 

In this section, we extend PCA-L1 to L1-2DPCA [17]. As 
mentioned above, 2DPCA compute eigenvectors with 2D input. 
Suppose ğ¼ğ‘–(i = 1,2, â€¦ , ğ‘)  denote ğ‘ input training images and 
ğ· = ğ‘š Ã— ğ‘› being the image size. Let w âˆˆ â„ğ‘¤Ã—1    be the first 
principal component to be learned. Let                        X =
[ğ‘¥1, ğ‘¥2, â€¦ , ğ‘¥ğ‘] âˆˆ â„ğ·Ã—ğ‘
ğ‘¥ğ‘– = [ğ‘¥ğ‘–1, ğ‘¥ğ‘–2, â€¦ , ğ‘¥ğ‘–â„]ğ‘‡ âˆˆ
,  with 
â„â„Ã—ğ‘¤(ğ‘– = 1,2, â€¦ , ğ‘). Note, ğ‘¥ğ‘–ğ‘— âˆˆ â„1Ã—ğ‘¤ The objective of PCA-
L1  is  to  maximize  the  L1-norm  variance  in  feature  space  as 
follows: 

ğ‘˜ = ğ‘¥ğ‘–ğ‘—
ğ‘¥ğ‘–ğ‘—

ğ‘˜âˆ’1 âˆ’ ğ‘¥ğ‘–ğ‘—

ğ‘˜âˆ’1(wğ‘˜âˆ’1

âˆ—
âˆ— wğ‘˜âˆ’1

ğ‘‡).                   (7) 

  At  this point,  we  can  find  that  the difference  between  L1-
PCA and L1-2DPCA is that L1-PCA converts the image matrix 
into  vectors,  and  L1-2DPCA  directly  uses  each  row  in  the 
original image matrix as a vector. 

III.  L1-PCANET 

In this section, we propose a new PCA-based deep learning 
network, L1-PCANet. To overcome the sensitivity to outliers in 
PCANet due to the use of L2-norm, we use the PCA-L1 rather 
than the PCA to learn the filters. L1-PCANet and PCANet [9] 
share the same network architecture which is shown in Fig.1. 

Suppose  there  are  ğ‘  training  images  ğ¼ğ‘–(ğ‘– = 1,2, â€¦ , ğ‘)  of 
size  ğ‘š Ã— ğ‘› ,  and  we  get  ğ· = ğ‘š Ã— ğ‘›  patches  of  size  k Ã— k 
around  each  pixel  in ğ¼ğ‘–.  Then  we  take  all  overlapping  patches 
and map them into vectors 

[ğ‘¥ğ‘–,1, ğ‘¥ğ‘–,2, â€¦ , ğ‘¥ğ‘–,ğ‘šğ‘›] âˆˆ â„ğ‘˜2Ã—ğ‘šğ‘›.                     (8) 

  And we remove the patch mean from each patch and get 

ğ‘‹Ì… = [ğ‘¥Ì…ğ‘–,1, ğ‘¥Ì…ğ‘–,2, â€¦ , ğ‘¥Ì…ğ‘–,ğ‘šğ‘›] âˆˆ â„ğ‘˜2Ã—ğ‘šğ‘›.               (9) 

For  all  input  images,  we  construct  the  same  matrix  and 

ğ‘

â„

combine them into one matrix to obtain 

ğ‘“(w) = â€–ğ‘¥ğ‘–ğ‘¤â€–1 = âˆ‘ âˆ‘|ğ‘¥ğ‘–ğ‘—ğ‘¤|

ğ‘–=1

ğ‘—=1

ğ‘ ğ‘¢ğ‘ğ‘—ğ‘’ğ‘ğ‘¡ ğ‘¡ğ‘œ â€–ğ‘¤â€–2 = 1,                             (5) 

  The polarity parameter ğ‘ğ‘–ğ‘— can be computed as: 

ğ‘ğ‘–ğ‘— = {

  1, ğ‘¤â„ğ‘’ğ‘› ğ‘¥ğ‘–ğ‘—ğ‘¤ â‰¥ 0
âˆ’1, ğ‘¤â„ğ‘’ğ‘› ğ‘¥ğ‘–ğ‘—ğ‘¤ < 0.

                       (6) 

  The process of maximization is achieved by Algorithm 2. 

Algorithm 2: L1-2DPCA method 

Input:   

ï¬ 

training set: ğ‘¿ = [ğ’™ğŸ, ğ’™ğŸ, â€¦ , ğ’™ğ‘µ] âˆˆ â„ğ‘«Ã—ğ‘µ 

Output:  

ï¬ 

filters ğ°âˆ— 

1: Set ğ’˜(ğŸ) = ğŸ and ğ­ = ğŸ 

2:  For  all ğ’Š âˆˆ {ğŸ, ğŸ, â€¦ , ğ‘µ}  and ğ’‹ âˆˆ {ğŸ, ğŸ, â€¦ , ğ’‰} ,  calculate ğ’‘ğ’Šğ’‹(ğ’•) by  using 
Equation (6). 

3:  Let  ğ­ = ğ­ + ğŸ  and  ğ°(ğ­) = âˆ‘ âˆ‘ ğ’‘ğ’Šğ’‹(ğ’• âˆ’ ğŸ)ğ’™ğ’Šğ’‹
ğ°(ğ­) = ğ’˜(ğ’•)/â€–ğ°(ğ­)â€–ğŸ 
4:  If ğ’˜(ğ’•) â‰  ğ’˜(ğ’• âˆ’ ğŸ),  go back to  Step 2.  Otherwise, set   ğ’˜âˆ— = ğ’˜(ğ’•) and 
stop.    

.Then  we  initialize 

ğ‘µ
ğ’Š=ğŸ

ğ’‰
ğ’‹=ğŸ

  To compute wğ‘˜
as: 

âˆ— (ğ‘˜ > 1), we have to update the training data 

 ğ‘‹ = [ğ‘‹Ì…1, ğ‘‹Ì…2, â€¦ , ğ‘‹Ì…ğ‘] âˆˆ â„ğ‘˜2Ã—ğ‘ğ‘šğ‘›.              (10)  

  Then, we use L1-PCA mentioned above to learn the filters in 
stage 1. The filter we want to find is w âˆˆ â„ğ‘˜2Ã—1. We take ğ‘‹ as 
the input data of L1-PCA. Assuming that the number of filters 
âˆ— } 
âˆ—, â€¦ , wğ¿1
in stage 1 is ğ¿1, we can obtain the first stage filters {w1
by repeatedly calling Algorithm 1. The L1-PCA filters of stage 
1 are expressed as: 

ğ‘Šğ‘

1 = ğ‘šğ‘ğ‘¡ğ‘˜,ğ‘˜(wğ‘

âˆ— ) âˆˆ â„ğ‘˜Ã—ğ‘˜,                      (11) 

where ğ‘ = 1,2, â€¦ , ğ¿1. 
  The output of stage 1 can expressed as: 

ğ‘ = ğ¼ğ‘– âˆ— ğ‘Šğ‘
ğ‘‚ğ‘–

1, ğ‘– = 1,2, â€¦ , ğ‘,                       (12) 

where âˆ— denote  2D  convolution.  We  set  the  boundary  of  the 
ğ‘ is of the same 
input image to zero-padding to make sure that ğ‘‚ğ‘–
size as ğ¼ğ‘–. We can get the filters of second and subsequent layers 
by  simply  repeating  the  process  of  the  first  layer  design.  The 
pooling layer of L1-PCANet is almost the same as the pooling 
layer of L1-2D2PCANet.  

In 

IV.  L1-2D2PCANET 
this  section,  we  generalize  L1-PCANet 
to  L1-
2D2PCANet.  L1-2D2PCANet  and  2DPCANet  [10]  share  the 
same network as shown in Fig.2. 

 
 
 
 
 
 
 
 
 
A.  The first L1-2ğ·2PCANet stage 
  Let all the assumptions be the same as in Section III. We get 
all the overlapping patches 

 ğ‘¥ğ‘–,ğ‘— âˆˆ â„ğ‘˜Ã—ğ‘˜, ğ‘— = 1,2, â€¦ , ğ‘šğ‘›,                      (13) 

and subtract the patch mean from each of them and we form 

a matrix 

ğ‘‹Ì…ğ‘¥,ğ‘– = [ğ‘¥Ì…ğ‘–,1, ğ‘¥Ì…ğ‘–,2, â€¦ , ğ‘¥Ì…ğ‘–,ğ‘šğ‘›] âˆˆ â„ğ‘˜Ã—ğ‘˜ğ‘šğ‘›.                (14) 

  And we use the transpose of ğ‘¥ğ‘–,ğ‘— to form matrix 

ğ‘‹Ì…ğ‘¦,ğ‘– = [ğ‘¥Ì…ğ‘–,1

ğ‘‡, ğ‘¥Ì…ğ‘–,2

ğ‘‡, â€¦ , ğ‘¥Ì…ğ‘–,ğ‘šğ‘›

ğ‘‡] âˆˆ â„ğ‘˜Ã—ğ‘˜ğ‘šğ‘›.             (15) 

For all  input  images,  we  construct  the  matrix  by  the  same 

way and put them into one matrix, we can obtain 

ğ‘Œğ‘¥ = [ğ‘Œğ‘¥

1, ğ‘Œğ‘¥

2, â€¦ , ğ‘Œğ‘¥

ğ¿1] âˆˆ â„ğ‘˜Ã—ğ¿1ğ‘ğ‘˜ğ‘šğ‘›.               (24) 

ğ‘Œğ‘¦ = [ğ‘Œğ‘¦

1, ğ‘Œğ‘¦

2, â€¦ , ğ‘Œğ‘¦

ğ¿1] âˆˆ â„ğ‘˜Ã—ğ¿1ğ‘ğ‘˜ğ‘šğ‘›.               (25) 

We  take ğ‘Œğ‘¥ and ğ‘Œğ‘¦ as  the  input  data  of  L1-2DPCA.  Assuming 
that the number of filters in stage 2 is ğ¿2, we design the  second 
stage filters {wx,1
} by repeatedly 
} and {wğ‘¦,1
calling  Algorithm  2.  The  L1-2DPCA  filters  of  Stage  2  are 
expressed as: 

âˆ— , â€¦ , wğ‘¦,ğ¿2

âˆ— , â€¦ , wğ‘¥,ğ¿2

âˆ—

âˆ—

ğ‘Šğ‘

2 = wğ‘¥,ğ‘

âˆ— Ã— wğ‘¦,ğ‘

âˆ— ğ‘‡ âˆˆ â„ğ‘˜Ã—ğ‘˜,                     (26) 

where ğ‘ = 1,2, â€¦ , ğ¿2. 

  Therefore, we have ğ¿2 outputs for each output ğ‘‚ğ‘–

ğ‘ of Stage 1 

ğ‘ = {ğ‘‚ğ‘–
ğµğ‘–

ğ‘ âˆ— ğ‘Šğ‘

2}, ğ‘™ = 1,2, â€¦ , ğ¿2.                   (27) 

ğ‘‹ğ‘¥ = [ğ‘‹Ì…ğ‘¥,1, ğ‘‹Ì…ğ‘¥,2, â€¦ , ğ‘‹Ì…ğ‘¥,ğ‘] âˆˆ â„ğ‘˜Ã—ğ‘ğ‘˜ğ‘šğ‘›,              (16) 

  Note, the number of outputs of Stage 2 is ğ¿1ğ¿2. 

ğ‘‹ğ‘¦ = [ğ‘‹Ì…ğ‘¦,1, ğ‘‹Ì…ğ‘¦,2, â€¦ , ğ‘‹Ì…ğ‘¦,ğ‘] âˆˆ â„ğ‘˜Ã—ğ‘ğ‘˜ğ‘šğ‘›.              (17) 

  Then,  we  use  L1-2DPCA  mentioned  above  to  learn  the 
âˆ— âˆˆ â„ğ‘˜Ã—1 and 
filters  in  Stage  1.  We  want  to  obtain  filters wğ‘¥,ğ‘
âˆ— âˆˆ â„ğ‘˜Ã—1 ,  where ğ‘ = 1,2, â€¦ , ğ¿1 . ğ‘‹ğ‘¥  and   ğ‘‹ğ‘¦  are  the  input 
wğ‘¦,ğ‘
data for 2DPCA-L1. Assuming that the number of filters in stage 
the 
}  and 
1 
filters  {wğ‘¥,1
is  ğ¿1 , 
âˆ—
âˆ—
} are obtained by repeatedly calling Algorithm 2. 
{wğ‘¦,1

âˆ— , â€¦ , wğ‘¥,ğ¿1

first  stage 

, â€¦ , wğ‘¦,ğ¿1

âˆ—

  The filters we need in Stage 1 can finally be expressed as: 

ğ‘Šğ‘

1 = wğ‘¥,ğ‘

âˆ— Ã— wğ‘¦,ğ‘

âˆ— ğ‘‡ âˆˆ â„ğ‘˜Ã—ğ‘˜.                    (18) 

  The output of Stage 1 will be 

ğ‘ = ğ¼ğ‘– âˆ— ğ‘Šğ‘
ğ‘‚ğ‘–

1, ğ‘– = 1,2, â€¦ , ğ‘.                      (19) 

B.    The second L1-2ğ·2PCANet stage 
  Like  in  the  first  stage,  we  can  start  with  the  overlapping 
ğ‘ and remove the patch mean from each patch. Then 
patches of ğ‘‚ğ‘–
we form 

C.  The pooling stage 

First, we use a Heaviside-like step function to binarize the 

output of Stage 2. The function ğ»(âˆ™) can be expressed as: 

ğ»(ğ‘¥) = {

                                 (28) 

0, ğ‘¥ < 0 
1, ğ‘¥ â‰¥ 0.

  Each pixel is encoded by the following function 

ğ¿2
ğ‘š = âˆ‘ 2ğ‘™âˆ’1ğ»(ğµğ‘–
ğ‘‡ğ‘–
ğ‘™

ğ‘)

,                            (29) 

where ğ‘‡ğ‘–

ğ‘š is an integer of range [0, 2ğ¿2âˆ’1].  

Second,  we  divide  ğ‘‡ğ‘–
histogram of all blocks of ğ‘‡ğ‘–
all the histogram of B blocks into one vector hist(ğ‘‡ğ‘–
way, we obtain ğ¿1 histograms and we put them into a vector 

ğ‘š  into  B  blocks.  Then  we  make  a 
ğ‘š with 2ğ¿2 values, and concatenate 
ğ‘š). In this 

ğ‘“ğ‘– = [hist(ğ‘‡ğ‘–

1), â€¦ , hist(ğ‘‡ğ‘–

ğ¿2)] âˆˆ â„2ğ¿2ğ¿1ğµÃ—1.      (30) 

  Using  the  2DPCA-L1  model  described  above,  we  can 
transform an input image into a feature vector as the output of 
L1-2D2PCANet. 

ğ‘ = [ğ‘¦Ì…ğ‘–,ğ‘,1, â€¦ , ğ‘¦Ì…ğ‘–,ğ‘,ğ‘šğ‘›] âˆˆ â„ğ‘˜Ã—ğ‘˜ğ‘šğ‘›.               (20) 
ğ‘Œğ‘¥,ğ‘–

V.  EXPERIMENTS AND ANALYSIS 

ğ‘ = [ğ‘¦Ì…ğ‘–,ğ‘,1
ğ‘Œğ‘¦,ğ‘–

ğ‘‡, â€¦ , ğ‘¦Ì…ğ‘–,ğ‘,ğ‘šğ‘›

ğ‘‡] âˆˆ â„ğ‘˜Ã—ğ‘˜ğ‘šğ‘›.             (21) 

Further,  we  define  the  matrix  that  collects  all  the  patches 
ğ‘˜ being removed as: 

without the patch mean of the kth output ğ‘‚ğ‘–

ğ‘ = [ğ‘Œx,1
ğ‘Œğ‘¥

ğ‘š , ğ‘Œx,2

ğ‘š , â€¦ , ğ‘Œğ‘¥,ğ‘

ğ‘š ] âˆˆ â„ğ‘˜Ã—ğ‘ğ‘˜ğ‘šğ‘›.               (22) 

ğ‘ = [ğ‘Œy,1
ğ‘Œğ‘¦

ğ‘ , ğ‘Œy,2

ğ‘ , â€¦ , ğ‘Œğ‘¦,ğ‘

ğ‘ ] âˆˆ â„ğ‘˜Ã—ğ‘ğ‘˜ğ‘šğ‘›.               (23) 

Finally  the  input  of  the  second  stage  is  obtained  by 
ğ‘ for all ğ¿1 filters 

ğ‘ and ğ‘Œğ‘¦

concatenating ğ‘Œğ‘¥

In this section, we evaluate the performance of L1-PCANet 
and L1-2D2PCANet with PCANet and 2DPCANet as baselines 
on YALE, AR, Extended Yale B and FRRET databases which 
are shown in Figure 3. The parameters are set as ğ‘˜ = 5, ğµ = 8, 
ğ¿1 = ğ¿2 = 4. SVM [22] implementation from the libsvm is used 
as  the  classifier  with  default  settings.  We  repeat  some 
experiments  ten  times  and  calculate  the  average  recognition 
accuracy and root mean square error(RMSE). 

A.  Extended Yale B 
  Extended Yale B consists of 2414 images of 38 individuals 
captured  with  different  lighting  conditions.  These  pictures  are 
pre-processed to have the same size 48 Ã— 42 and alignment.  

 
 
 
 
 
 
 
In  Experiment  1,  we  compare  L1-PCANet  and  L1-
2D2PCANet with PCANet and 2DPCANet. We randomly select 
i(=2,3,4,5,6,7) images per individual for training and use the rest 
for testing. We also test a two-layer CNN for comparison which 
is trained on the test images for 2000 epochs. The architecture 
of CNN is the same as [10]. The results are shown in TABLE I. 

In Experiment 2, to evaluate the robustness of L1-PCANet 
and  L1-2D2PCANet  to  outliers,  we  randomly  add  block-wise 
noise  to  the  test  images  to  generate  test  images  with  outliers. 
Within each block, the pixel value is randomly set to be 0 or 255. 
These blocks occupy 10%, 20%, 30%, and 50% of the images 
and  they  are  added  to  the  random  position  of  the  image, 
respectively which can be seen in see Figure 4. The results are 
shown in TABLE â…¡. 

In Experiment 3, we examine the impact of the block size B 
for L1-2D2PCANet. The block size changes from 2 Ã— 2 to 8 Ã—
8. The rest parameters are the same as Experiment 1. The results 
are shown in Figure 5. 

B.  AR 

AR face database contains 2600 color images corresponding 
to 100 people's faces (50 men and 50 women). It has two session 
data from two different days and each person in each session has 
13  images  including  7  images  with  only  illumination  and 
expression  change  and  3  images  wearing  sunglasses  and  3 
images wearing scarf. Images show frontal faces with different 
facial  expressions,  illumination  conditions,  and  occlusions 
(sunglasses and scarf). These pictures are pre-processed to have 
the same size 40 Ã— 30. 

In  Experiment  4,  in  order  to  investigate  the  impact  of  the 
choice of  training  images,  we  divide  the experiment  into  four 
groups. (1) In group 1, we randomly select 5 images with only 
illumination  and  expression  change  from  session  1  per 
individual as training images; (2) In group 2, we randomly select 
4 images  with  only  illumination  and  expression  change  and 1 
image  wearing  sunglasses  from  session  1  per  individual  as 
training  images;  (3) In  group 3,  we  randomly  select  4  images 
with  only  illumination  and  expression  change  and  1  image 
wearing scarf from session 1 per individual as training images.  
The  remaining  images  are  test  samples;  (4)  In  group  4,  we 
randomly select 3 images with only illumination and expression 
change, 1 image wearing sunglasses and 1image wearing scarf 
from session 1 per individual as training images. The remaining 
images in session 1 and all images in session 2 are used as test 
images. We manually select five images from session 1 as the 
gallery images and keep gallery images of each group the same. 
The results are shown in TABLE â…¢. 

In  order  to  investigate  the  impact  of  the  choice  of  gallery 
images, Experiment 5 is the same as Experiment 4 except that 
the gallery images and the training images are exchanged. We 
use the remaining images in session 1 and all images in session 
2 as test samples. The results are shown in TABLE â…£. 

C.  FERET 

This database contains a total of 11338 facial images. They 
were collected by photographing 994 subjects at various facial 
angles. We gathered a subset from FERET which is composed 
by  1400  images  recording  of  200  individuals,  with  each  7 

images exhibit large variations in facial expression, facial angle, 
and illumination. These pictures are pre-processed to have the 
same size 40 Ã— 40 and alignment.  

In  Experiment  6,  we  divide  the  experiment  into  7  groups. 
The training images of each group consist of 200 images from 
the  subset  with  different  facial  angle,  expression  and 
illumination. We use the remaining images in the subset as test 
images. The parameters are set as ğ‘˜ = 5, ğµ = 10, ğ¿1 = ğ¿2 = 4. 
The results are shown in TABLE â…¤. 

D.  YALE 

YALE  consists  of  15  individuals  and  11  images  for  each 
individual  which  shows  varying  facial  expressions  and 
configurations.  These  pictures  are  pre-processed  to  have  the 
same size 32 Ã— 32.  

In Experiment 7, we randomly select i(=2,3,4,5,6,7) images 
per  individual  for  training  and  use  the  rest  for  testing.  The 
parameters  are  set  as ğ‘˜ = 5, ğµ = 4, ğ¿1 = ğ¿2 = 4. The  results 
are shown in TABLE â…¥. 

E.  LFW-a 

LFW-a  is  a  version  of  LFW  after  alignment  with  deep 
funneling.  We  gathered  the  individuals  including  more  than  9 
images and then form a dataset with 158 individuals from LFW-
a. 

In Experiment 8, we randomly choose i(=3,4,5,6,7) images 
per  individual  for  gallery  images  and  keep  training  images  of 
each group the same. The results are shown in TABLE â…¦. 

F.  Results and Analysis 

Table I shows the result of Experiment 1 on Extended Yale 
B, Table â…¢ shows the result of Experiment 4 on AR, Table â…¤ 
shows the result of Experiment 6 on FERET, Table â…¥  shows 
the result on Yale, and Table â…¦ shows the result on LFW-a. In 
these experiments, we changed the training images by random 
selection. From the results, we can see that the L1-2D2PCANet 
outperforms PCANet, 2DPCANet and L1-PCANet in terms of 
recognition  accuracy  and  RMSE,  because  we  introduce  L1-
norm  into  the  network.  The  two  L1-norm-based  networks  we 
proposed  are  far  superior  to  the  traditional  L2-norm-based 
networks  in  terms  of  RMSE,  which  means  the  proposed 
networks are insensitive to changes in training images. That is, 
the accuracy of the traditional L2-norm-based networks largely 
depends  on  the  choice  of  training  images  while  the  L1-norm-
based  networks  we  proposed  can  achieve  better  and  stable 
accuracy under any training images. A possible explanation of 
this phenomenon is as follows. In fact, the expression, posture, 
illumination  condition  and  occlusion  in  the  images  can  be 
regarded as interference or noise in face recognition. This noise 
degrades L2-norm-based networks much more than it degrades 
L1-norm-based  networks.  Therefore,  the  proposed  networks 
exhibit  its  superiority  when  the  training  images  contain  some 
changes  in  expression,  posture,  illumination  condition  and 
occlusion. 

Table â…¡ shows the result of Experiment 2 on Extended Yale 
B. In this experiment, we randomly add block-wise noise to the 
test images. From the results, we can see that as the block-wise 
noise  increasing  from  10%  of  the  image  size  to  50%,  the 
performance  of  PCANet,  2DPCANet,  and  L1-PCANet  drops 

rapidly  while  L1- 2D2 PCANet  still  has  good  performance. 
Therefore, it can be considered that L1-2D2PCANet has better 
robustness against outlier and noise than other three networks. 

We  also  investigate  the  impact  of  the  choice  of  gallery 
images on AR; see Table â…£. From the horizontal comparison of 
Table â…¥, the more categories the gallery contains, the higher the 
accuracy is. 

Figure 5 shows the result of Experiment 3 on Extended Yale 
B.  When  the  block  is  small,  the  local  information  cannot  be 
contained perfectly, and it may get more noise when the block is 
too big. 

CNN 

PCANet 

2DPCANet 

L1 âˆ’ PCANet 
L1 âˆ’ 2D2PCANet 

2 

83.41 Â± 5.31 

97.48 Â± 1.03 

97.88 Â± 0.22 

99.67 Â± 0.09 

PCANet 

2DPCANet 

L1 âˆ’ PCANet 
L1 âˆ’ 2D2PCANet 

PCANet 

2DPCANet 

L1 âˆ’ PCANet 
L1 âˆ’ 2D2PCANet 

PCANet 

2DPCANet 

L1 âˆ’ PCANet 
L1 âˆ’ 2D2PCANet 

TABLE I.  Experiment 1 on Extended Yale B [19]. 
4 

3 

5 

Fig.5: Impact of the block size. 

6 

7 

80.56 

84.51 Â± 5.70 

97.34 Â± 1.81 

97.98 Â± 0.22 

99.71 Â± 0.07 

84.42 Â± 5.37 

97.01 Â± 1.64 

97.88 Â± 0.18 

99.73 Â± 0.09 

82.48 Â± 7.18 

96.71 Â± 2.48 

97.86 Â± 0.17 

99.73 Â± 0.06 

84.06 Â± 6.22 

95.16 Â± 2.93 

97.94 Â± 0.19 

99.75 Â± 0.06 

89.56 Â± 5.48 

97.22 Â± 2.02 

97.90 Â± 0.16 

99.77 Â± 0.07 

TABLE â…¡.  Experiment 2 on Extended Yale B [19]. 
20% 

10% 

30% 

92.68 Â± 0.42 

94.26 Â± 0.25 

94.34 Â± 0.40 

99.00 Â± 0.15 

No occlusion 

78.63 Â± 3.09 

82.94 Â± 4.31 

87.09 Â± 0.50 

89.26 Â± 0.37 

No occlusion 

66.71 Â± 0.87 

69.24 Â± 0.70 

68.56 Â± 0.65 

77.08 Â± 0.64 

88.51 Â± 0.40 

88.71 Â± 0.57 

91.50 Â± 0.51 

98.28 Â± 0.18 

TABLE â…¢.  Experiment 4 on AR [18]. 
Sunglass 

78.74 Â± 4.84 

83.85 Â± 4.48 

86.73 Â± 0.31 

88.59 Â± 0.27 

TABLE â…£.  Experiment 5 on AR [18]. 
Sunglass 

69.62 Â± 0.69 

74.78 Â± 0.70 

75.23 Â± 0.60 

81.10 Â± 0.37 

74.63 Â± 0.48 

79.54 Â± 0.89 

83.58 Â± 0.60 

95.73 Â± 0.20 

Scarf 

79.23 Â± 4.47 

82.21 Â± 2.97 

87.33 Â± 0.12 

88.85 Â± 0.28 

Scarf 

69.59 Â± 0.69 

72.14 Â± 0.99 

72.35 Â± 0.77 

78.34 Â± 0.61 

50% 

44.10 Â± 0.76 

55.34 Â± 0.70 

65.01 Â± 0.61 

84.01 Â± 0.74 

Sunglass and Scarf 

80.40 Â± 4.10 

83.44 Â± 4.27 

86.46 Â± 0.22 

88.52 Â± 0.19 

Sunglass and Scarf 

72.66 Â± 0.70 

75.51 Â± 0.61 

79.34 Â± 0.71 

84.17 Â± 0.75 

PCANet 

2DPCANet 

L1 âˆ’ PCANet 
L1 âˆ’ 2D2PCANet 

1 

75.83 

73.17 

82.83 

86.00 

2 

76.83 

76.17 

82.17 

84.83 

TABLE â…¤.  Experiment 6 on FERET [21]. 
3 

5 

4 

76.17 

76.17 

82.00 

85.50 

68.00 

73.67 

82.50 

86.50 

73.67 

78.33 

85.00 

87.33 

6 

69.83 

73.50 

82.50 

86.83 

7 

79.11 

74.00 

81.83 

86.83 

Avg. 

74.21 

75.00 

82.69 

86.26 

RMSE 

3.69 

1.78 

0.99 

0.81 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
PCANet 

2DPCANet 

L1 âˆ’ PCANet 
L1 âˆ’ 2D2PCANet 

2 

86.33 Â± 1.87 

91.33 Â± 2.80 

91.45 Â± 0.89 

94.03 Â± 0.32 

TABLE â…¥.  Experiment 7 on Yale [19]. 
4 

5 

3 

86.75 Â± 2.37 

91.78 Â± 1.94 

92.00 Â± 0.83 

95.10 Â± 0.41 

87.50 Â± 1.58 

90.44 Â± 2.59 

91.22 Â± 0.54 

94.95 Â± 0.33 

87.25 Â± 2.12 

90.67 Â± 2.34 

91.00 Â± 0.44 

95.25 Â± 0.32 

6 

87.25 Â± 2.14 

90.87 Â± 2.90 

91.89 Â± 0.51 

95.16 Â± 0.41 

7 

87.29 Â± 2.22 

91.93 Â± 2.13 

92.67 Â± 0.33 

95.66 Â± 0.40 

PCANet 

2DPCANet 

L1 âˆ’ PCANet 
L1 âˆ’ 2D2PCANet 

TABLE â…¦.  Experiment 8 on LFW-a [20]. 

3 

30.07 Â± 4.69 

33.00 Â± 3.52 

34.14 Â± 0.39 

39.35 Â± 0.29 

4 

31.86 Â± 5.35 

35.68 Â± 3.64 

36.27 Â± 0.29 

42.20 Â± 0.46 

5 

34.35 Â± 5.91 

39.02 Â± 3.74 

39.08 Â± 0.57 

45.91 Â± 0.34 

6 

35.71 Â± 6.34 

39.92 Â± 3.98 

40.25 Â± 0.77 

46.99 Â± 0.42 

7 

38.56 Â± 6.82 

43.15 Â± 4.12 

44.26 Â± 0.81 

50.12 Â± 0.47 

2D2PCANet is far less than those traditional CNNs which are 
based on back propagation. 

In the future work, we will work on  the improving of L1-
2DPCA  algorithm  to  solve  the  problem  of  the  computational 
cost of L1-2D2PCANet. 

REFERENCES 

[1]  Devijver P  A, Kittler J. Pattern Recognition: A  Statistical Approach[J]. 

Prentice/hall International, 1982. 

[2]  Ripley B D. Pattern Recognition and Neural Networks[J]. Technometrics, 

[3] 

1999, 39(2):233-234. 
Jain A K, et al. Statistical Pattern Recognition: A Review[J]. Optica Acta 
International Journal of Optics, 2000, 27(11):1502-1502. 

[4]  Bishop,  Christopher  M.  Pattern  Recognition  and  Machine  Learning 
(Information Science and Statistics)[C]// Springer-Verlag New York, Inc. 
2006:049901. 

[5]  Lecun  Y,  Bengio  Y,  Hinton  G.  Deep  learning[J].  Nature,  2015, 

521(7553):436. 

[6]  Silver D, Schrittwieser J, Simonyan K, et al. Mastering the game of Go 

without human knowledge.[J]. Nature, 2017, 550(7676):354. 

[7]  Lawrence S, Giles C L, Tsoi A C, et al. Face recognition: a convolutional 
neural-network  approach.[J].  IEEE  Transactions  on  Neural  Networks, 
1997, 8(1):98-113. 

[8]  Kalchbrenner  N,  Grefenstette  E,  Blunsom  P.  A  Convolutional  Neural 

Network for Modelling Sentences[J]. Eprint Arxiv, 2014, 1. 

[9]  Chan T H, Jia K, Gao S, et al. PCANet: A Simple Deep Learning Baseline 
for  Image  Classification?[J].  IEEE  Transactions  on  Image  Processing, 
2015, 24(12):5017. 

[10]  Yu D, Wu X J. 2DPCANet: a deep leaning network for face recognition[J]. 

Multimedia Tools & Applications, 2017(4):1-16. 

[11]  Yang  J,  et  al.  Two-dimensional  PCA:  a  new  approach  to  appearance-
based  face  representation  and  recognition.[J].  IEEE  Transactions  on 
Pattern Analysis & Machine Intelligence, 2004, 26(1):131. 

[12]  Jia Z, Han B, Gao X. 2DPCANet: Dayside Aurora Classification Based 
on  Deep  Learning[C]//  CCF  Chinese  Conference  on  Computer  Vision. 
Springer, Berlin, Heidelberg, 2015:323-334. 

[13]  Zhang Q R. Two-Dimensional Parameter Principal Component Analysis 
for  Face  Recognition[J].  Advanced  Materials  Research,  2014,  971-
973:1838-1842 

[14]  Kwak  N.  Principal  Component  Analysis  Based  on  L1-Norm 
Maximization[J].  IEEE  Transactions  on  Pattern  Analysis  &  Machine 
Intelligence, 2008, 30(9):1672-1680. 

[15]  Ding  C,  Zhou  D,  He  X,  et  al.  R1-PCA:rotational  invariant  L1-norm 
principal  component  analysis  for  robust  subspace  factorization[C]// 
International Conference on Machine Learning. ACM, 2006:281-288. 
[16]  Li X, Pang Y, Yuan Y. L1-norm-based 2DPCA.[J]. IEEE Transactions on 

Systems Man & Cybernetics Part B, 2010, 40(4):1170-1175. 

Fig.3: images in three datasets. Top line: Extended Yale B [19]. Middle line: 
AR [18]. Bottom line: FERET [21]. 

Fig.4: some generalized outlying face images of Extended Yale B [19]. 

VI.  CONCLUSION  

In  this  paper,  we  have  proposed  a  new  deep  learning 
network L1-2D2PCANet, which is a simple but robust method. 
We use the L1-norm-based 2DPCA [17] instead of L2-norm-
based  2DPCA  [11]  for  the  filter  learning  because  of  the 
advantages of L1-norm. It is more robust to outliers than L2-
norm. By introducing L1-norm into 2DPCANet [10], we hope 
the network will inherit such advantages. 

To verify the performance of L1-2D2PCANet, we evaluate 
them  on  the  facial  datasets  including  AR,  Extended  Yale  B, 
Yale  and  FRRET. The  results  show  that  L1-2D2PCANet  has 
three  distinct  advantages  over  traditional  L2-norm-based 
networks: (1) Statistically, the accuracy of  L1-2D2PCANet is 
higher than that of other networks on all test datasets. (2) L1-
2D2PCANet has better robustness to changes in training images 
compared with the other networks. (3) Compared with the other 
networks,  L1-2D2PCANet  has  better  robustness  to  noise  and 
outliers. Therefore,  L1-2D2PCANet is an efficient and robust 
network for face recognition. 

However,  L1-2DPCA  brings  more  computational  load  to 
the  network,  which  increases  the  computational  cost  of  L1-
2D2 PCANet.  Despite  this,  the  computational  cost  of  L1-

 
 
 
 
 
 
[17]  Xuelong  L,  Yanwei  P,  Yuan  Y.  L1-norm-based  2DPCA[J].  IEEE 
Transactions  on  Systems  Man  &  Cybernetics  Part  B  Cybernetics  A 
Publication  of  the  IEEE  Systems  Man  &  Cybernetics  Society,  2010, 
40(4):1170-5. 

[20]  Zhu  P,  Zhang  L,  Hu  Q,  et  al.  Multi-scale  Patch  Based  Collaborative 
Representation 
for  Face  Recognition  with  Margin  Distribution 
Optimization[C]//  European  Conference  on  Computer  Vision.  Springer 
Berlin Heidelberg, 2012:822-835. 

[18]  Martinez A M. The AR face database[J]. Cvc Technical Report, 1998, 24. 
[19]  Geo A S, et al. From Few to Many: Illumination Cone Models for Face 
Recognition  under  Variable  Lighting  and  Pose[J].  Pattern  Analysis  & 
Machine Intelligence IEEE Transactions on, 2001, 23(6):643-660. 

[21]  Phillips  P  J,  Moon  H,  Rauss  P,  et  al.  The  FERET  September  1996 
database and evaluation procedure[J]. Lecture Notes in Computer Science, 
1997, 1206:395-402. 

[22]  Burges  C.  A  Tutorial  on  Support  Vector  Machines  for  Pattern 

Recognision[J]. Data Mining & Knowledge Discovery, 1998, 2. 

 
