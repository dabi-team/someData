Improving Top-K Decoding for Non-Autoregressive Semantic Parsing via
Intent Conditioning

Geunseob (GS) Oh, Rahul Goel, Chris Hidey,
Shachi Paul, Aditya Gupta, Pararth Shah, Rushin Shah
Google
{ohgs,goelrahul,chrishidey,shachipaul,gaditya,pararth,rushinshah}@google.com

Abstract

Semantic parsing (SP) is a core component
of modern virtual assistants like Google As-
sistant and Amazon Alexa. While sequence-
to-sequence based auto-regressive (AR) ap-
proaches are common for conversational SP,
recent studies (Shrivastava et al., 2021) em-
ploy non-autoregressive (NAR) decoders and
reduce inference latency while maintaining
competitive parsing quality. However, a ma-
jor drawback of NAR decoders is the difﬁculty
of generating top-k (i.e., k-best) outputs with
approaches such as beam search. To address
this challenge, we propose a novel NAR se-
mantic parser that introduces intent condition-
ing on the decoder. Inspired by the traditional
intent and slot tagging parsers, we decouple
the top-level intent prediction from the rest of
a parse. As the top-level intent largely gov-
erns the syntax and semantics of a parse, the
intent conditioning allows the model to better
control beam search and improves the quality
and diversity of top-k outputs. We introduce a
hybrid teacher-forcing approach to avoid train-
ing and inference mismatch. We evaluate the
proposed NAR on conversational SP datasets,
TOP & TOPv2. Like the existing NAR mod-
els, we maintain the O(1) decoding time com-
plexity while generating more diverse outputs
and improving top-3 exact match (EM) by 2.4
points.
In comparison with AR models, our
model speeds up beam search inference by 6.7
times on CPU with competitive top-k EM.

1

Introduction

Neural sequence models are widely used for the
task of conversational semantic parsing, which
converts natural language utterances to machine-
understandable meaning representations. Recent
approaches (Chen et al., 2020; Rongali et al., 2020;
Lialin et al., 2020; Aghajanyan et al., 2020a; Shri-
vastava et al., 2021) combine Transformer-based se-
quence models (Vaswani et al., 2017; Devlin et al.,
2019) and Pointer Generator Networks (Vinyals

Query: what is going on this weekend?

Label:

[in:get_event [sl:date_time this weekend
] ]

(Failure 1: repeated tokens)

[in:get_event [sl:date_time this weekend
] [sl:date_time this weekend ] ]
(Parse with the repeated slot sl:data_time
and leaf nodes this and weekend)

(Failure 2: invalid syntax)

[in:get_event [sl:date_time
(Incomplete parse; missing ‘] ]’)

Table 1: Examples of possible failures from the limited
beam search of the existing NAR semantic parsers.

et al., 2015; See et al., 2017). A vast majority of
semantic parsing approaches (Gupta et al., 2018;
Chen et al., 2020; Rongali et al., 2020; Lialin et al.,
2020; Aghajanyan et al., 2020a; Yin et al., 2021)
employ autoregressive (AR) decoders to generate
structured output frames for the quality of output
parses. During the AR decoding, output tokens
are generated sequentially, conditioned on all pre-
viously generated tokens. As a result, the decoding
time (i.e. inference latency) increases linearly with
the decoding length. This not only limits the ca-
pacity to accommodate larger language models but
may also degrade the user experience of intelligent
conversational assistants (e.g. Google Assistant,
Amazon Alexa) due to the high inference latency.
On the contrary, non-autoregressive (NAR) de-
coders are capable of parallel decoding and thus
allow much faster inference. Compared to AR de-
coders, which perform O(n) decoding steps, the
NAR decoding is typically achieved in either O(1)
(Babu et al., 2021; Shrivastava et al., 2021) or
O(log(n)) (Zhu et al., 2020) steps. A recent study
(Babu et al., 2021) built NAR parsers and reduced
the latency up to 81% compared to AR parsers.
Another study by Shrivastava et al. (2021) pro-

2
2
0
2

r
p
A
4
1

]
L
C
.
s
c
[

1
v
8
4
7
6
0
.
4
0
2
2
:
v
i
X
r
a

 
 
 
 
 
 
posed a NAR semantic parser called Span Pointer
Network and cut the latency by 8.5-10x on CPU
while achieving comparable performance to their
AR benchmarks. All these studies suggest that the
NAR decoders have signiﬁcant latency beneﬁts.

Although the recent NAR semantic parsers have
decreased the performance gap from AR parsers
on the exact match metric, one of the main lim-
itations of NAR models still remain unsolved:
beam search and top-k outputs. AR models lever-
age beam search algorithms (Wiseman and Rush,
2016), which are especially effective at producing
top-k outputs (Wiseman and Rush, 2016; Gupta
et al., 2018). This is possible since the beam search
allows AR models to dynamically sort out less-
probable candidate parses and adjust their decod-
ing lengths. As a result, AR models are capable of
generating diverse high-quality output parses.

On the other hand, the existing NAR parsers
(Babu et al., 2021; Shrivastava et al., 2021) cannot
employ the AR beam search as the output tokens
of the NAR parsers are generated independently
from each other. Instead, the existing NAR parsers
perform a beam search by generating k candidate
frame lengths. For each frame length, a single parse
is produced, resulting in a total of k parses.

However, we ﬁnd that the existing NAR beam
search tends to produce duplicates of the most prob-
able output parse. As exempliﬁed in Table 1, top-k
beam outputs often include outputs with repeated
tokens and parses with invalid syntax (e.g., trun-
cated or extended versions of the most-probable
parse) rather than diverse output parses.

The ability to generate diverse top-k parses is
important for modern conversational assistants. Ta-
ble 2 presents an example that demonstrates how a
query may correspond to different parses depend-
ing on the context. The diverse semantic parses
can be leveraged in a downstream component that
has more contextual information. An additional
re-ranking module can also be employed to select
the most relevant semantic parse.

This work focuses on improving the quality of
top-k outputs of NAR conversational semantic pars-
ing. Our idea is to leverage the fact that the top-
level intent of a semantic parse mainly determines
the syntax and semantics of the parse. This is the
key driving point of the classic intent classiﬁca-
tion and slot tagging (Liu and Lane, 2016) based
parsers, which were widely used traditional seman-
tic parsers. We build our model upon the existing

Query: avoid bridges on my route

Parses from the Proposed NAR
(1a) [in:update_directions [sl:path_avoid [in:get
_location [sl:category_location bridges]]]]

(2a) [in:get_directions [sl:path_avoid [in:get

_location [sl:category_location bridges]]]]

Parses from the Baseline NAR
(1b) [in:update_directions [sl:path_avoid [in:get
_location [sl:category_location bridges]]]]
(2b) [in:update_directions [sl:path_avoid [in:get
_location [sl:category_location bridges

Table 2: Examples of beam outputs. The parses pro-
duced by the proposed NAR are both valid depending
on the context. If the query was made after the user al-
ready had set a path, parse 1a is more relevant, else 2a.
In contrast, the baseline NAR only produced one valid
parse. Parse 2b is an invalid duplicate of parse 1b.

NAR (i.e., frame length conditioned NAR), but de-
couple the prediction of the top-level intent from
the output. We leverage the conditional dependen-
cies of the frame length and parse on the top-level
intent for diverse top-k outputs.

We evaluate the proposed NAR model on two
datasets: TOP (Gupta et al., 2018) and TOPv2
(Chen et al., 2020) and demonstrate that we fur-
ther close the gap between AR and NAR parsers.
Most importantly, we show that the intent condi-
tioning improves the quality and diversity of the
top-k parses and the total inference time of our
NAR beam search is 6.7x times shorter than the
AR baseline on CPU.

2 Related Work

2.1 Non-autoregressive Semantic Parsing

NAR sequence models have been an active research
area across different ﬁelds of natural language pro-
cessing for their fast inference speeds. While vari-
ous designs of NAR decoders exist, recent works
in machine translation use models with iterative
reﬁnements (Lee et al., 2018; Ghazvininejad et al.,
2019, 2020), insertion-based (Stern et al., 2019),
and latent alignment methods (Libovický and Helcl,
2018; Chan et al., 2020; Saharia et al., 2020).

The task of semantic parsing (SP) is similar to
the machine translation task as they both trans-
late input sentences from one representation to an-
other. For this reason, recent studies in SP have
adopted modeling techniques from machine transla-

tion. Zhu et al. (2020) leveraged the insertion-based
seq2seq models for NAR semantic parsing and re-
duced the decoding time from O(n) to O(log(n))
while matching the performance of AR models.
Babu et al. (2021); Shrivastava et al. (2021) applied
an iterative reﬁnement method Mask-Predict to se-
mantic parsing and brought the time complexity fur-
ther down to O(1) while performing comparably to
the baseline AR models. As opposed to the original
Mask-Predict model that performs multiple itera-
tions of re-masking and prediction (Ghazvininejad
et al., 2019), they only perform a single iteration
of the masking and output prediction as they claim
that task-oriented SP does not beneﬁt much from
iterative reﬁnements (Babu et al., 2021; Shrivastava
et al., 2021). In addition to the recent sequence-to-
sequence NAR semantic parsers, traditional intent
and slot ﬁlling model (Mesnil et al., 2014) is an-
other approach with non-autoregressive decoding.

2.2 Baseline NAR

Span Pointer Network (Shrivastava et al., 2021),
which is one of the two recent works that leveraged
the mask-predict algorithm, showed the competi-
tive performance to the AR parsers while signiﬁ-
cantly reducing their latency. For this reason, we
utilize Span Pointer Network as a basis of our work
and accordingly refer to it as baseline NAR to dis-
tinguish the NAR semantic parser we propose. It
should be noted that we replicated Span Pointer
Network as it is not publicly available.

The baseline NAR is built with a pre-trained
encoder, a frame length module, and a Transformer
decoder. The encoder is a language model such as
BERT (Devlin et al., 2019) or RoBERTa (Liu et al.,
2019) that takes input queries x and outputs their
encoded representations e.

e1:l = Encoder(x1:l),

(1)

where l indicates the length of the source (i.e.,
input query). The frame length module takes the
encoded representations and predicts the length
of the frame (i.e., target parse) n followed by the
generation of n mask tokens.

n = FrameLengthModule(e1:l).
[MASK]1:n = MaskCreation(n).
The transformer decoder takes the N mask to-

(2)

kens as inputs and produce h for each token.

h1:n = Decoder([MASK]1:n; e1:l).

(3)

Figure 1: The proposed model architecture. By decou-
pling the top-level intent prediction from the output pre-
diction, we perform conditional generation of the frame
length and parse upon the top-level intent.

Lastly, the pointer-generator mapping layer is
used to convert h to the target frame y as follows.

y1:n = PTR(h1:n; e1:l),

(4)

where yk is either a token from the target vocab
that consists of parse symbols (e.g., intents & slots)
or copy of a source token. The output tokens are
generated in parallel as they are conditionally inde-
pendent given the source and frame length.

2.3 Beam search for AR vs baseline NAR

AR decoders produce multiple candidate output
parses per source by employing the beam search
algorithm (Wiseman and Rush, 2016). During the
beam search with width k, the k most probable
candidates are kept at each decoding step and less
probable candidates are sorted out. The decoding
process ﬁnishes when a special “[END]” token is
encountered. For a decoding length n, the score of
an AR parse is computed as follows.

SAR(y) = p(y|x) =

n
(cid:89)

j=1

p(yj|y1:j−1, x).

(5)

Since the AR decoders consider multiple can-
didate tokens yj at each decoding step, they can
generate a large number of unique parses.

In comparison, the baseline NAR only produces
a single output parse per frame length. The baseline
NAR mimics the beam search by generating top-
k frame lengths, which results in k outputs. The
output parse is scored using the joint probability of
the frame length and output parse as follows.

SN AR,baseline(y) = p(y, n|x) =

p(y|n, x) · p(n|x) =

p(yj|n, x) · p(n|x),

(6)

n
(cid:89)

j=1

where SN AR,baseline(y) denotes the score of the
baseline NAR output. p(y|n, x) is obtained using
the conditional independence of the output tokens
given the frame length n and source x.

3 Proposed Approach

To mitigate the aforementioned limitation of the
baseline NAR, we propose Intent-conditioned Non-
autoregressive Neural Semantic Parser. Our ap-
proach is motivated by traditional semantic parsers
such as Liu and Lane (2016), which performs an
intent classiﬁcation followed by slot tagging. An-
other motivation comes from an observation that
the top-level intent of the parse largely governs the
syntax and semantics of the rest of the parse.

Our idea is to decouple the top-level intent pre-
diction from the output prediction as shown in Fig-
ure 1. This builds explicit dependencies of the
frame length and the rest of the frame on the top-
level intent. As a result, the joint probability of
output and frame length is expressed as follows.

p(y, n|x) = p(y2:n|n, y1, x) · p(n|y1, x) · p(y1|x).
(7)
This facilitates effective conditional generation
of the length and output, while keeping the decod-
ing time complexity as O(1); the number of decod-
ing steps does not scale with the output length.

3.1

Intent Conditioning

The proposed NAR utilizes a pre-trained language
model to obtain encoded source representation e.
Then, it performs the top-level intent prediction,
which is formulated as a multiclass classiﬁcation
of the size of the dimensionality of intent vocab.

logits(y1) = IntentModule(e1:l),

(8)

where y1 refers to the top-level intent of the se-
mantic parse or the ﬁrst token of the parse. As
the top-level intent prediction is decoupled, the in-
tent module works with a smaller vocabulary size.
Compared to the output vocabulary that combines
a target and copy index vocabulary, the intent vo-
cabulary is approximately 4 times smaller with a
source length 32 on TOP dataset. We use a ran-
domly initialized Transformer as the intent module.
We use the logits of the top-level intent, as op-
posed to the discrete intent class variable, as the
inputs to all subsequent modules. This is because
the dense logits convey information about the un-
certainty of the intent prediction for each intent

class and help the subsequent modules to condition
on richer information. We found that this resulted
in better performance than the model conditioned
on the 1-dimensional discrete intent.

Subsequent to the top-level intent prediction, the
frame length module takes the logits of the intent
y1 and produces the decoding length followed by
the creation of mask tokens. After that, the initial
mask tokens together with the frame length pass
through a positional encoding layer to compute the
encoded mask tokens, [MASK]2:n.

n − 1 = FrameLengthModule(logits(y1); e1:l),
[MASK]2:n = PosEncoding(MaskCreation, n).
(9)
The rest of the frame is generated using parallel
decoding followed by the source token mapping
via the pointer-generator mapping layer.

h2:n = Decoder([MASK]2:n; logits(y1), e1:l),
y2:n = PTR(h2:n; e1:l), y1:n = cat(y1, y2:n).

(10)
Finally, the output parse is obtained by concate-

nating the top-level intent and rest of the frame.

3.2 Training Objective of the Proposed NAR

The loss is a weighted sum of top-level intent clas-
siﬁcation loss Lint, frame length classiﬁcation loss
Llen, and output loss Lout as follows.

L = Lout + λlen · Llen + λint · Lint.

(11)

We jointly optimize the three loss terms and em-
ploy label smoothing (LS) (Szegedy et al., 2016)
as a regularizer to penalize overconﬁdent predic-
tions by computing negative log-likelihood (NLL)
between the smoothed one-hot labels and predic-
tions. That is, Lint = NLL(LS(y1,label), y1,pred),
Llen = NLL(LS(nlabel), npred), and Lout =
NLL(LS(y2:n,label), y2:n,pred).

3.3 Beam Search with the Proposed NAR

The proposed intent-conditioned NAR produces
multiple output parses per source via the sequential
conditioning on the top-level intent and length. Pre-
cisely, the model ﬁrst computes top-k1 top-level
intents. For each top-level intent, the length model
outputs top-k2 frame lengths. As a result, k1 · k2
pairs of top-level intents and lengths are produced.
Finally, the decoder generates a single parse per
pair, yielding a total of k1 · k2 output parses.

The proposed NAR performs a single decoder
pass regardless of the beam size, k1 · k2. This is
possible as we cast the k1 · k2 pairs as the batch
dimension. As a result, the time complexity of the
beam search remains constant, to the extent of the
memory capacity. The memory increases linearly
to the beam size.

The intent conditioning provides explicit control
of the top-level intents via the selection of top-k1
unique intents. It also builds dependencies of the
frame length and output on the top-level intents.
We ﬁnd that these help the model to improve diver-
sity and quality of top-k output parses. To further
validate the hypothesis, we devise two experiments
to (1) identify the potential (i.e., upper limit of the
accuracy) of the proposed NAR beam search and
(2) examine the diversity and quality of the output
parses compared to the baseline NAR beam search.

3.4 Hybrid Teacher Forcing

During our initial attempts to inspect the proposed
model, we discovered that the inference perfor-
mance is unstable. This was due to a discrepancy
in the computation of the top-level intent during the
training and inference. Recall that the outputs of
the top-level intent module are logits (see Equation
8). In the inference, we use the logits to sample the
top-k1 top-level intents, which are sparse one-hot
vectors, and convert them back to the logits. Con-
versely, in the training, we do not sample top-level
intents and instead use the direct outputs (dense
logits) from the top-level intent module. We ﬁrst
attempted to resolve the problem with a teacher-
forcing of the top-level intents. While this stabi-
lized the inference, the training became unstable
due to the sparsity of the teacher logits.

We address this problem by leveraging a sam-
pling strategy depicted in Figure 2. The idea is to
uniformly sample logits from a pair of teacher and
model logits as follows.

logits(y1) ∼ U(logits(y1,label), logits(y1,model)).
(12)
We name this strategy as hybrid teacher forcing
to reﬂect that the model leverages both teacher and
model top-level intent logits. This sampling tech-
nique may be seen as a non-autoregressive variant
of the scheduled sampling (Bengio et al., 2015).

3.5 Scoring Beam Search Outputs

Figure 2: The computation of the top-level intent. (a)
During training via naive teacher forcing, label intent
logits are fed to the length module and decoder. (b) In
the inference, top-k intents are sampled from the model
logits and fed to the other modules. (c) Hybrid teacher-
forcing uses both label and model logits for training.

The ﬁrst method S1 uses the joint log probability of
the top-level intent and frame length as the score.

S1 = log(p(n, y1|x)) = log(p(n|y1, x) · p(y1|x)).
(13)
The second method S2 uses the joint log proba-

bility of the output, length, and top-level intent.

S2 = log(p(y1:n, n|x)) =
log(cid:0)p(y2:n|n, y1, x) · p(n|y1, x) · p(y1|x)(cid:1).

(14)

The last scoring method S3 combines the joint

probability p(y1:n, n|x) and a length penalty.

lp(y) = ((5 + l)/6)α,
S3 = S2(y)/lp(y),

(15)

where lp is the length penalty (Wu et al., 2016).
Depending on k1, k2, and scoring method, the
top-k outputs may consist of parses with multiple
distinct lengths and intents or parses with a single
intent with multiple lengths.

3.6 Semantic Parse Representation

We represent semantic parses using the decoupled
tree representation (Aghajanyan et al., 2020a) that
is an extension of the compositional tree representa-
tion (Gupta et al., 2018). An example is depicted in
Figure 3. Likewise, another query “What is happen-
ing in Boston on New Year’s Eve” is semantically
parsed into “[in:get_event [sl:location Boston ]
[sl:date_time on New Year’s Eve ] ]”.

A scoring method is used to select the top-k parses
from the k1 · k2 pairs. We tested three methods.

An example of the decoupled tree representation
is Index form that replaces the copy tokens with the

Figure 3: Decoupled representation of the semantic
parse for a query “How long is my drive to Houston?”.

indices to the corresponding source tokens. In this
sense, the above semantic parse is represented as
“[in:get_event [sl:location 4 ] [sl:date_time 5 6 7 8
] ]”. Together with the pointer network, this signif-
icantly reduces the size of output vocabulary (Ron-
gali et al., 2020). Another representation is Span
form introduced in (Shrivastava et al., 2021). As
opposed to specifying all indices of spans of copy
tokens, the span form uses the start and end indices
of the spans. That is, “[in:get_event [sl:location
4 4 ] [sl:date_time 5 8 ] ]”. The beneﬁts of the
span form include shorter target lengths and ﬁxed
number of leaf nodes that helps decoupling the syn-
tax from the semantics. In addition, the number of
valid frame length classes are halved as the frame
lengths of parses in the span form are always even.

4 Experiments

4.1 Datasets

To quantify the beneﬁts of our NAR parsers over
the baseline NAR, we utilize two conversational
semantic parsing datasets. The ﬁrst dataset is TOP
(Task Oriented Parsing) (Gupta et al., 2018), a col-
lection of human-generated queries in English and
the corresponding semantic parses that are repre-
sented as hierarchical trees. Another public dataset
we explore is TOPv2 (Chen et al., 2020), which is
an extension of TOP with six more domains.

4.2 Evaluation Metrics

We mainly utilize exact match (EM) to evaluate
quality of both greedy decoding and beam decoding
output. EM is deﬁned as the percentage of queries
whose label parses are correctly predicted.

Inference time is the metric we use to quantify
the latency beneﬁt of our model. We measure the
model inference time on 1 TPU (Jouppi et al., 2017)
from the Google Cloud TPUv2 and 1 CPU of the
Intel Cascade Lake CPU platform with 8GB RAM.
We used batch size 1 and computed latency on 1000
examples from the val set of the TOP dataset. We
report per-example average latencies of these runs.

In addition, diversity of the output parses is com-
puted to quantify the capability of the proposed
NAR approach at producing distinct top-k outputs.
We mainly compute two metrics: (1) number of
unique top-level intents in top-k output parses and
(2) the distinct n-grams (Li et al., 2016), which
are commonly used diversity metrics in natural lan-
guage processing (Vijayakumar et al., 2018; Xu
et al., 2018). Speciﬁcally, we compute percentage
of distinct n-grams by counting unique n-grams
in top-k outputs, dividing the counts by the total
number of output tokens, and scaling it by 100.

4.3

Implementation details

4.3.1 Encoder

We use pre-trained BERTBASE (L12/H768) as the
encoders for all AR, baseline NAR, and proposed
NAR for the experiments. We use uncased versions
of them along with lowercased datasets as they
generally performed better than the cased models
with cased datasets. We leave it to future work to
quantify the performance with other pre-trained lan-
guage models such as RoBERTa (Liu et al., 2019).

4.3.2 Decoder

We primarily compare the performances of two AR
baseline decoders, the baseline NAR decoder, and
the proposed NAR decoder. The two AR decoders
are identical except that one is trained on the label
parses represented in the index form and the other
in the span form. All NAR decoders were trained
on the label parses in the span form. We used the
same decoder architecture for all models across
all experiments. As discussed earlier, the baseline
NAR is a replication of the NAR model proposed
by Shrivastava et al. (2021). Speciﬁcally, we use
the vanilla version that does not utilize the R3F loss
(Aghajanyan et al., 2020b).

Unlike Shrivastava et al. (2021) and Babu et al.
(2021) that use an MLP or CNN length module,
our frame length module is a randomly initialized
Transformer decoder; we found that it performs
better. We used the same length module for both
baseline and proposed NAR models. Similarly, we
use a randomly initialized Transformer decoder for
the top-level intent prediction. Further details of the
model parameters are described in the appendix.

4.3.3 Hyperparameters

We observed that the ratio of the loss terms is impor-
tant. In general, higher top-level intent penalty and

Model

Latency,
TPU

Latency,
CPU

EM,
TOPv1

EM,
TOPv2

Models

Variants of the baseline NAR

Oracle EM

Base. NAR
Prop. NAR

AR, index
AR, span

1x
1.01x

2.23x
2.26x

10.26x
11.17x

48.70x
46.98x

82.56
83.11

83.43
83.40

84.86
85.22

85.40
85.56

Table 3: Greedy decoding results with BERT encoder.
Latency numbers are measured with TOPv1 dataset and
relative to the baseline NAR latency on TPU.

moderate frame length penalty resulted in better ac-
curacy. We use the hybrid teacher forcing to train
the proposed NAR for the experiments presented
in this paper. We trained the models using Adam
optimizer (Kingma and Ba, 2014) with exponen-
tial learning rate decay. Additionally, we use 1000
learning rate warmup steps. We also apply dropout
to the source embeddings and the Transformer de-
coder for better generalization. The details of the
parameters are described in the appendix.

5 Results

We show that the proposed intent-conditioned NAR
further closes the gap between AR and NAR SP.
Most importantly, our model greatly improves the
quality of top-k output parses while maintaining
the O(1) decoding time of the baseline NAR.

We ﬁrst present the greedy decoding results. We
then investigate the upper-bound beam search per-
formance of the baseline and proposed NAR. Next,
we share the beam search results that quantify the
quality and diversity of top-k output parses. Lastly,
we share the results of the comparison of various
beam scoring methods. We point out that we used
the identical encoder & decoder architectures and
model parameters for all AR & NAR models. In
addition, the identical frame length module is used
for both baseline and proposed NAR models.

5.1 Greedy Decoding

Table 3 presents the greedy decoding results. At
each decoding step of AR models, the most prob-
able token is selected. This repeats until an END
token is produced. In NAR greedy decoding, beam
width of 1 is used for both frame length and top-
level intent. No scoring method is used.

The results indicate that our model improves the
performance of baseline NAR by 0.4-0.5 EM while
matching the latency of the prior NAR approach.

Shallow decoder
Shallow decoder, Label smoothing
Deep decoder
Deep decoder, Label smoothing

Best of the baseline NAR

Variants of the proposed NAR

Shallow decoder
Shallow decoder, Label smoothing
Deep decoder
Deep decoder, Label smoothing

Best of the proposed NAR

83.34
82.57
83.73
83.03

83.73

86.22
86.40
86.89
85.66

86.89

Table 4: Empirical upper-bound of beam search with
NARs. To obtain the oracle EMs, we feed the gold
frame length for the baseline NARs in the inference.
Likewise, we use the gold top-level intent for the pro-
posed NARs.

Compared to AR baselines, our model cuts the
greedy decoding inference time (i.e., latency of
the semantic parser including both encoder and
decoder) by 4.2-4.4x on CPU and 2.2x on TPU.

5.2 Potential Impact of Intent Conditioning

for Beam Search

We designed an experiment to investigate the po-
tential beneﬁts of the proposed beam search. We
aimed to empirically quantify the upper bound of
the beam search performance of the baseline NAR
and our intent-conditioned NAR. In the experiment,
we ﬁrst trained the baseline and proposed NAR
parsers, then fed the oracle (i.e., gold) frame length
to the baseline NAR or the oracle top-level intent to
the proposed NAR during the inference. To ensure
validity of the results, we ran the experiments with
different model conﬁgurations.

As shown in Table 4, the proposed NAR con-
sistently scored higher oracle EM across various
model conﬁgurations. It should be noted that the
proposed NARs only use the gold top-level intent
(i.e., the gold length is not used for the inference
with the proposed NARs). Table 5 conﬁrms these
results and shows that our NAR performs more
effective beam-coding and achieves higher exact
match for the top-k output.

TPU Latency CPU Latency

top-1 EM top-3 EM top-1 IM top-3 IM

Baseline NAR
Proposed NAR

AR, index form
AR, span form

1x
1.12x

2.78x
2.84x

11.98x
14.93x

99.24x
99.81x

82.61
83.12

83.44
83.47

83.62
86.00

87.34
87.12

94.59
94.85

94.61
94.67

95.22
98.05

96.44
96.22

Table 5: Beam decoding results. The proposed NAR outperforms the baseline NAR in top-3 EM by 2.4 and top-3
IM by 2.8 points. Compared to AR models, our model cuts the beam search latency by 6.7 times on CPU.

5.3 Beam search

In table 5, we report the beam search results on
TOP dataset as top-k EM for k = 1, 2, 3. If the
label parse matches with any of the top-k output
parses, it counts towards the top-k EM. We also
report top-level intent match (IM) percents. The
top-k parses are selected from the k1 · k2 beam
search outputs using the scoring method 3 (Equa-
tion 15). We used the same k1 · k2 for the fair
comparison. Speciﬁcally, we used k2 = 25 for
the baseline NAR and k1 = 25, k2 = 1 for our
NAR as there exist 25 distinct length classes and
25 distinct intent classes in TOP dataset. For the
proposed NAR, we observed that a higher k2 has
minor impact on top-k EMs, compared to a higher
k1. We note that the hybrid teacher forcing (Equa-
tion 12) was used for the training and helped the
models achieve consistent test set accuracy.

The results demonstrate a large gap between AR
and the existing NAR models in top-k outputs. No-
tably, the top-3 EM from the existing NAR model
is outperformed by the baseline AR by 3.5 points.
Secondly, the proposed NAR outperforms baseline
NAR in top-3 EM by 2.4 and top-3 IM by 2.8 points.
Lastly, we show that the proposed NAR reduces the
performance gap from the autoregressive models.
The inference time of our NAR beam search is 6.7x
times shorter on CPU and 2.5x times shorter on
TPU. Our AR baselines manage parallel compute
loads better than NAR models and thus result in
smaller latency gaps on TPU.

5.4 Diversity

Table 6 elaborates on the diversity of the output
parses with the baseline and proposed NAR mod-
els. Speciﬁcally, we compute their average num-
bers of unique top-level intents and distinct n-gram
(Li et al., 2016) percentages in top-3 output parses.
We ﬁnd that top-k parses of the proposed NAR out-
performs the baseline NAR in all diversity metrics.
We observed that the baseline NAR beam search

Metric (higher is more diverse)

top-3 Exact Match

Baseline
NAR

Proposed
NAR

83.62

86.00

# of unique 1st intents in top-3

1.12

3.00

Distinct 1-gram, sentence-wise

69.84

79.92

Distinct 2-gram, sentence-wise

85.93

94.12

Distinct 1-gram, corpus-wise

25.77

40.79

Distinct 2-gram, corpus-wise

40.36

53.77

Table 6: We measure the diversities of the top-3 parses
of the baseline and proposed NAR models reported in
Table 5. We report sentence-wise (i.e., distinct n-grams
in each parse) and corpus-wise distinct n-grams (i.e.,
distinct n-grams in each collection of top-3 parses).

often duplicates the most probable parse with re-
peated tokens and/or invalid syntax. This suggests
that the frame length conditioning alone is not ef-
fective at producing top-k outputs (see Table 1 and
2 for examples). Table 6 quantitatively conﬁrms the
observation with lower sentence-wise and corpus-
wise distinct n-gram scores of the baseline NAR,
compared to the proposed NAR.

6 Conclusion

We present a novel non-autoregressive neural se-
mantic parser: the intent-conditioned NAR for im-
proved top-k decodings. The proposed model ad-
dresses the main limitation of the recent NAR mod-
els (i.e., the limited beam search capability) by
decoupling the prediction of the top-level intent
from output. This builds explicit dependencies of
frame lengths and output parses on top-level intents
and helps NAR semantic parsers to better control
beam search. The proposed NAR model further
closes the performance gap from AR models by im-
proving the quality and diversity of top-k outputs.
We highlight that the decoding time complexity is
O(1), regardless of output length or beam width.

References

Armen Aghajanyan, Jean Maillard, Akshat Shrivas-
tava, Keith Diedrick, Michael Haeger, Haoran Li,
Yashar Mehdad, Veselin Stoyanov, Anuj Kumar,
Mike Lewis, and Sonal Gupta. 2020a. Conversa-
tional semantic parsing. In Proceedings of the 2020
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 5026–5035, On-
line. Association for Computational Linguistics.

Armen Aghajanyan, Akshat Shrivastava, Anchit Gupta,
Naman Goyal, Luke Zettlemoyer, and Sonal Gupta.
2020b. Better ﬁne-tuning by reducing represen-
In International Conference on
tational collapse.
Learning Representations.

Arun Babu, Akshat Shrivastava, Armen Aghajanyan,
Ahmed Aly, Angela Fan, and Marjan Ghazvinine-
jad. 2021. Non-autoregressive semantic parsing for
compositional task-oriented dialog. In Proceedings
of the 2021 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies, pages 2969–2978,
Online. Association for Computational Linguistics.

Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and
Noam Shazeer. 2015. Scheduled sampling for se-
quence prediction with recurrent neural networks.
arXiv preprint arXiv:1506.03099.

William Chan, Chitwan Saharia, Geoffrey Hinton, Mo-
Im-
hammad Norouzi, and Navdeep Jaitly. 2020.
puter: Sequence modelling via imputation and dy-
In International Conference
namic programming.
on Machine Learning, pages 1403–1413. PMLR.

Xilun Chen, Asish Ghoshal, Yashar Mehdad, Luke
Zettlemoyer, and Sonal Gupta. 2020. Low-resource
domain adaptation for compositional task-oriented
semantic parsing. In Proceedings of the 2020 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 5090–5100, Online. As-
sociation for Computational Linguistics.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of
deep bidirectional transformers for language under-
In Proceedings of the 2019 Conference
standing.
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers),
pages 4171–4186, Minneapolis, Minnesota. Associ-
ation for Computational Linguistics.

Marjan Ghazvininejad, Omer Levy, Yinhan Liu, and
Luke Zettlemoyer. 2019. Mask-predict: Parallel de-
coding of conditional masked language models. In
Proceedings of the 2019 Conference on Empirical
Methods in Natural Language Processing and the
9th International Joint Conference on Natural Lan-
guage Processing (EMNLP-IJCNLP), pages 6112–
6121, Hong Kong, China. Association for Computa-
tional Linguistics.

Marjan Ghazvininejad, Omer Levy, and Luke Zettle-
Semi-autoregressive training im-
arXiv preprint

moyer. 2020.
proves mask-predict decoding.
arXiv:2001.08785.

Sonal Gupta, Rushin Shah, Mrinal Mohit, Anuj Ku-
mar, and Mike Lewis. 2018. Semantic parsing for
task oriented dialog using hierarchical representa-
In Proceedings of the 2018 Conference on
tions.
Empirical Methods in Natural Language Processing,
pages 2787–2792, Brussels, Belgium. Association
for Computational Linguistics.

Norman P Jouppi, Cliff Young, Nishant Patil, David
Patterson, Gaurav Agrawal, Raminder Bajwa, Sarah
Bates, Suresh Bhatia, Nan Boden, Al Borchers, et al.
2017. In-datacenter performance analysis of a ten-
sor processing unit. In Proceedings of the 44th an-
nual international symposium on computer architec-
ture, pages 1–12.

Diederik P Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980.

Jason Lee, Elman Mansimov, and Kyunghyun Cho.
2018. Deterministic non-autoregressive neural se-
In Pro-
quence modeling by iterative reﬁnement.
ceedings of the 2018 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1173–
1182, Brussels, Belgium. Association for Computa-
tional Linguistics.

Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao,
and Bill Dolan. 2016. A diversity-promoting ob-
jective function for neural conversation models. In
Proceedings of the 2016 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 110–119, San Diego, California. Association
for Computational Linguistics.

Vladislav Lialin, Rahul Goel, Andrey Simanovsky,
Anna Rumshisky, and Rushin Shah. 2020. Update
frequently, update fast: Retraining semantic pars-
ing systems in a fraction of time. arXiv preprint
arXiv:2010.07865.

Jindˇrich Libovický and Jindˇrich Helcl. 2018. End-to-
end non-autoregressive neural machine translation
In Pro-
with connectionist temporal classiﬁcation.
ceedings of the 2018 Conference on Empirical Meth-
ods in Natural Language Processing, pages 3016–
3021, Brussels, Belgium. Association for Computa-
tional Linguistics.

Bing Liu and Ian Lane. 2016. Attention-based recur-
rent neural network models for joint intent detection
and slot ﬁlling. Interspeech 2016, pages 685–689.

Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-
dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
Luke Zettlemoyer, and Veselin Stoyanov. 2019.
Roberta: A robustly optimized bert pretraining ap-
proach. arXiv preprint arXiv:1907.11692.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
In Advances in neural information pro-
you need.
cessing systems, pages 5998–6008.

Ashwin K Vijayakumar, Michael Cogswell, Ram-
prasaath R Selvaraju, Qing Sun, Stefan Lee, David
Crandall, and Dhruv Batra. 2018. Diverse beam
search for improved description of complex scenes.
In Thirty-Second AAAI Conference on Artiﬁcial In-
telligence.

Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly.
2015. Pointer networks. Advances in Neural Infor-
mation Processing Systems, 28:2692–2700.

Sam Wiseman and Alexander M. Rush. 2016.
Sequence-to-sequence learning as beam-search op-
timization. In Proceedings of the 2016 Conference
on Empirical Methods in Natural Language Process-
ing, pages 1296–1306, Austin, Texas. Association
for Computational Linguistics.

Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V
Le, Mohammad Norouzi, Wolfgang Macherey,
Maxim Krikun, Yuan Cao, Qin Gao, Klaus
Macherey, et al. 2016. Google’s neural machine
translation system: Bridging the gap between hu-
arXiv preprint
man and machine translation.
arXiv:1609.08144.

Jingjing Xu, Xuancheng Ren,

Junyang Lin, and
Xu Sun. 2018. Diversity-promoting GAN: A cross-
entropy based generative adversarial network for di-
versiﬁed text generation. In Proceedings of the 2018
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 3940–3949, Brussels, Bel-
gium. Association for Computational Linguistics.

Pengcheng Yin, Hao Fang, Graham Neubig, Adam
Pauls, Emmanouil Antonios Platanios, Yu Su, Sam
Thomson, and Jacob Andreas. 2021. Compositional
generalization for neural semantic parsing via span-
In Proceedings of the
level supervised attention.
2021 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 2810–2823, On-
line. Association for Computational Linguistics.

Qile Zhu, Haidar Khan, Saleh Soltan, Stephen Rawls,
and Wael Hamza. 2020. Don’t parse, insert: Multi-
lingual semantic parsing with insertion based decod-
ing. In Proceedings of the 24th Conference on Com-
putational Natural Language Learning, pages 496–
506, Online. Association for Computational Linguis-
tics.

Grégoire Mesnil, Yann Dauphin, Kaisheng Yao,
Yoshua Bengio, Li Deng, Dilek Hakkani-Tur, Xi-
aodong He, Larry Heck, Gokhan Tur, Dong Yu, et al.
2014. Using recurrent neural networks for slot ﬁll-
ing in spoken language understanding. IEEE/ACM
Transactions on Audio, Speech, and Language Pro-
cessing, 23(3):530–539.

Prafull Prakash, Saurabh Kumar Shashidhar, Wenlong
Zhao, Subendhu Rongali, Haidar Khan, and Michael
Kayser. 2020. Compressing transformer-based se-
mantic parsing models using compositional code em-
beddings. In Findings of the Association for Com-
putational Linguistics: EMNLP 2020, pages 4711–
4717, Online. Association for Computational Lin-
guistics.

Subendhu Rongali, Luca Soldaini, Emilio Monti, and
Wael Hamza. 2020. Don’t parse, generate! a se-
quence to sequence architecture for task-oriented se-
mantic parsing. In Proceedings of The Web Confer-
ence 2020, pages 2962–2968.

Chitwan Saharia, William Chan, Saurabh Saxena, and
Mohammad Norouzi. 2020. Non-autoregressive ma-
chine translation with latent alignments. In Proceed-
ings of the 2020 Conference on Empirical Methods
in Natural Language Processing (EMNLP), pages
1098–1108, Online. Association for Computational
Linguistics.

Abigail See, Peter J. Liu, and Christopher D. Manning.
2017. Get to the point: Summarization with pointer-
generator networks. In Proceedings of the 55th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 1073–
1083, Vancouver, Canada. Association for Computa-
tional Linguistics.

Bo Shao, Yeyun Gong, Weizhen Qi, Guihong Cao, Jian-
shu Ji, and Xiaola Lin. 2020. Graph-based trans-
former with cross-candidate veriﬁcation for seman-
tic parsing. In Proceedings of the AAAI Conference
on Artiﬁcial Intelligence.

Akshat Shrivastava, Pierce Chuang, Arun Babu, Shrey
Desai, Abhinav Arora, Alexander Zotov, and Ahmed
Span pointer networks for non-
Aly. 2021.
autoregressive task-oriented semantic parsing.
In
Findings of the Association for Computational Lin-
guistics: EMNLP 2021, pages 1873–1886, Punta
Cana, Dominican Republic. Association for Compu-
tational Linguistics.

Mitchell Stern, William Chan, Jamie Kiros, and Jakob
Uszkoreit. 2019. Insertion transformer: Flexible se-
In In-
quence generation via insertion operations.
ternational Conference on Machine Learning, pages
5976–5985. PMLR.

Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe,
Jon Shlens, and Zbigniew Wojna. 2016. Rethinking
the inception architecture for computer vision.
In
Proceedings of the IEEE conference on computer vi-
sion and pattern recognition, pages 2818–2826.

Scoring Method

S1 (Equation 13)

Exact Match

top-1

83.11

top-2

84.88

top-3

85.36

S2 (Equation 14)

83.11

84.95

85.49

S3 (Equation 15)
α = 1.0
α = 3.0
α = 5.0

83.13
83.12
83.15

85.21
85.15
85.08

85.81
86.00
85.93

Table 8: Performance of the three scoring methods.

C Model Architecture Details

Here, we elaborate on the details of the model ar-
chitecture and parameters used across different ex-
periments. Table 9 speciﬁes hyper-parameters we
used to build the AR and NAR models used across
different experiments. Compared to the AR mod-
els, the baseline NAR has roughly 12.5% more
model parameters as it includes the frame length
module in addition. The proposed NAR has ap-
proximately 4.4% more model parameters than the
baseline NAR due to the addition of the intent mod-
ule. Figure 4 depicts the detailed model architec-
ture for the proposed NAR. The ﬁgure includes
(sub)-modules, inputs, outputs, as well as the di-
mensionality of them.

A Comparisons with Other Benchmarks

We compare the AR and NAR models we used
with some of the benchmarks in the literature for
the TOP & TOPv2 datasets. While there are bench-
marks with other pre-trained encoders, we report
the prior models with pre-trained BERT encoders
to be consistent with the AR and NAR models used
in this work (Table 7).

Encoders

TOP TOPv2

Non-autoregressive Models

BERTBASE baseline (greedy)

82.56 84.86

BERTBASE proposed (greedy)

83.11 85.22

Autoregressive Models

BERTBASE (beam size = 4)
(Rongali et al., 2020)

BERTBASE (beam size = 5)
(Prakash et al., 2020)

Transformer
(Shao et al., 2020)

+ BERTBASE

83.13

85.01

82.51

-

-

-

BERTBASE (greedy-ours, index)

83.43 85.40

BERTBASE (greedy-ours, span)

83.40 85.56

Table 7: Reported performance of semantic parsers
with BERT encoders on TOP and TOPv2 datasets.
Greedy refers to greedy decoding (i.e., beam size 1).

We note that the two baseline AR semantic
parsers used in this work are based on the AR archi-
tecture presented in Rongali et al. (2020). Our AR
baselines perform comparably against the reported
numbers in the literature, as shown in Table 7.

B Scoring Beam Outputs

Table 8 depicts the performance of the three scoring
methods on the TOP dataset. Each scoring method
sorts the beam outputs of the proposed NAR differ-
ently (Equation 13-15).

Our study indicates that the scoring method
3 works the best for our NAR model, empiri-
cally demonstrating the effectiveness of the length
penalty (Wu et al., 2016) for NAR models. We
observed that high length penalties (e.g., α =
2.0 − 3.0) often corresponded to the highest EM.
For fair comparison, we also applied the length
penalty with α = 1.0 for the AR models for all
experiments reported in this work.

Number of parameters
Number of TPUs used for training
decoder (layer/hidden/head)
length module (layer/hidden/head)
intent module (layer/hidden/head)
nonlinearity
model dropout
source embeddings dropout

λlength
λintent

optimizer
learning rate
learning rate warmup steps
learning rate scheduler
batch size

AR

114.1M

baseline NAR

proposed NAR

128.4M
8 Google Cloud TPUv2
L4/H256/HD2

134.1M

N/A
N/A

N/A
N/A

L8/H256/HD4

L8/H256/HD4

10

100

N/A
Relu
0.0316
0.0022

N/A

Adam
0.00004
1000
Exponential Decay
256

Table 9: Hyper-parameters related to the models used across different experiments presented in this work.

Figure 4: The detailed architecture of the proposed NAR.

