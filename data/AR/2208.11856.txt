2
2
0
2

g
u
A
5
2

]

O
R
.
s
c
[

1
v
6
5
8
1
1
.
8
0
2
2
:
v
i
X
r
a

Design and Implementation of a Human-Robot Joint Action Framework using
Augmented Reality and Eye Gaze

WESLEY P. CHAN, Monash University, Australia
MORGAN CROUCH, Monash University, Australia
KHOA HOANG, Monash University, Australia
CHARLIE CHEN, Monash University, Australia
NICOLE ROBINSON, Monash University, Australia
ELIZABETH CROFT, University of Victoria, Australia

When humans work together to complete a joint task, each person builds an internal model of the situation and how it will evolve.

Efficient collaboration is dependent on how these individual models overlap to form a shared mental model among team members,

which is important for collaborative processes in human-robot teams. The development and maintenance of an accurate shared mental

model requires bidirectional communication of individual intent and the ability to interpret the intent of other team members. To

enable effective human-robot collaboration, this paper presents a design and implementation of a novel joint action framework in

human-robot team collaboration, utilizing augmented reality (AR) technology and user eye gaze to enable bidirectional communication

of intent. We tested our new framework through a user study with 37 participants, and found that our system improves task efficiency,

trust, as well as task fluency. Therefore, using AR and eye gaze to enable bidirectional communication is a promising mean to improve

core components that influence collaboration between humans and robots.

CCS Concepts: • Human-centered computing → Collaborative interaction; User studies; Collaborative interaction; Mixed
/ augmented reality; • Computer systems organization → Robotics; Robotics.

Additional Key Words and Phrases: Human-robot interaction, human-robot collaboration, joint action framework, augmented reality,

wearable interface, assistive robotics

ACM Reference Format:
Wesley P. Chan, Morgan Crouch, Khoa Hoang, Charlie Chen, Nicole Robinson, and Elizabeth Croft. 2022. Design and Implementation
of a Human-Robot Joint Action Framework using Augmented Reality and Eye Gaze. In Proceedings of Transactions on Human-Robot
Interaction (THRI). ACM, New York, NY, USA, 14 pages.

1 INTRODUCTION

A key focus of human-robot interaction (HRI) research is to create seamless collaboration between robots and humans.

Studies in psychological science indicate that a critical factor for collaboration between people, particularly when

completing joint activities, is the capacity to reason about perception and action plans, and to construct a shared mental

model of the collaborative task [3]. This construction of a shared mental model is called ‘Theory of Mind’ [2]. A shared

mental model can be defined as the overlapping information that collaborating agents (where an agent can be a human

Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not
made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components
of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to
redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.

© 2022 Association for Computing Machinery.
Manuscript submitted to ACM

1

 
 
 
 
 
 
THRI, Transactions on Human-Robot Interaction

Chan et al.

Fig. 1. User’s view when collaborating with the robot arm using our AR joint action framework. The block selected by the robot is
highlighted in red to convey to the user the robot’s intention of picking up the block.

or a robot) share about the task [20]. The model is developed by first understanding each other’s task representation

(mutual understanding) and then incorporating both task representations (mutual agreement) [12]. Once established,

each agent then formulates their action plan to contribute to the task, and together, the team can proceed with joint

actions to complete the task. The shared mental model allows for anticipatory information sharing, which can improve

task efficiency [31].

The establishment of an accurate shared mental model requires bidirectional communication between collaborating

agents. Enabling effective communication within human-robot teams is a current challenge for the field of HRI,

as humans and robots typically rely on different communicate channels to exchange information. In recent years,

augmented reality (AR) technology has grown as an interface technology, with a number of AR head-mounted displays

becoming commercially available [13, 21, 24]. Leveraging AR technology, existing research have demonstrated its

potential in proving intuitive communication between human and robot [4, 9, 25, 28] To enable effective human-robot

teaming, this paper presents a novel human-robot joint action framework design incorporating AR technology. Our

contributions are as follows:

(1) We have proposed and implemented a novel design of a human-robot joint action framework, that uses AR and

user eye gaze to enable bidirectional communication (Fig. 1).

(2) We conducted a user study with 37 participants to validate our novel system. Results show that our system

improves task error and user’s trust of the robot, without increasing the task load.

While there exist research on the use of AR or eye gaze alone for human-robot collaboration, the combined use for

implementing a joint action framework has not been proposed or examined.

2 RELATED WORK

2.1 Joint Action Frameworks

A joint action framework should facilitate the coordination of individual agents’ goals, intentions, plans, and actions

during execution, towards establishing a shared mental models. In developing this shared model, Clodic et al. suggests

that a framework must overcome motivational uncertainty (misaligned goals), instrumental uncertainty (shared plans

and goals), and common ground (public information) uncertainty [7]. This is particularly important in human-robot joint

2

Human-Robot Joint Action Framework using AR and Eye Gaze

THRI, Transactions on Human-Robot Interaction

action tasks as robots and humans (particularly non robot expert users) do not typically possess the same background

knowledge or share the same level of mutual understanding as humans (particularly those who have shared experience)

have with each other.

Devin et al. has developed a joint action framework by proposing a Theory of Mind manager, which models an agent

as a collection of sets. These sets include the high level actions that the agent is able to perform, the state of a goal, the

set of facts about the environment, and the state of the plan from an agent’s point of view. How the robot perceives the

evolution of these states will determine how the robot will utilise its capacities to execute the shared plan. [11]. Breazeal

et al. tried to reduce uncertainty and coordinate actions by replicating human-human collaborative discourse, such as

recreating human social gestures, collaborative dialogue, and facial expressions in humanoids [5]. These methods have

shown to improve team outcomes but require a robot to have a level of anthropomorphism. Lasota et al. showed that

the speed, distance, and positioning of a robot arm can increase predictability and user confidence when working in

close proximity [23]. Nikolaidis et al. have also explored the used of unsupervised learning for improving joint action

frameworks. Their system was able to learn a reward policy that can select a set of action sequences that outperform a

manually specified policy in both efficiency and fluency of a human-robot team, by considering anticipatory actions

[27].

2.2 Augmented Reality in HRI

Previous research on how AR can assist human-robot collaboration predominately focuses on using AR to display

workplace information, control feedback information or task information [9]. Examples of workspace information

display uses AR to highlight the space that the robot is currently using or plans to use and where it is safe for the

user. Such include using colour gradients projected onto a surface [25] or using virtual light barriers placed within the

environment [32] to warn the user of the robot’s current operation.

Examples of control feedback information display include applications in path generation or teleoperation. Yamamoto

et al. used AR 3D material property overlays and virtual fixtures to provide visual and haptic feedback in robotic surgery

[36]. Blankemeyer et al. showed that AR virtual path displays can provide a more intuitive control mechanism [4].

Quintero et al. have also created an AR robot teleoperation interface, and showed that it reduces physical workload

when compared with kinesthetic teaching, but at the expense of a higher mental workload [28].

Using Augmented Reality as a tool for displaying task and robot status information has been shown to improve HRI.

Weng et al. explored and measured the effectiveness of AR displays including 3D arrows, virtual leading lines, and

virtual text [35]. Walker et al. explored the use of AR for displaying an aerial robot’s future flight motion by visualizing

navigation points, arrows, robot’s gaze, and an environment map. [33, 34].

2.3 Eye Gaze in HRI

Eye gaze has been widely examined for HRI as it is a natural form of human communication that contains a wealth of

information about one’s mental state. Eye gaze is used to ascertain what a person is looking at and to cue people’s

attention [14, 22]. Aronson et al. examined the dynamics of control behaviour and eye gaze in human-robot shared

manipulation tasks. By studying gaze features like saccades, fixations, smooth pursuits, and scan paths, they determined

that there were specific gaze patterns for monitoring and planning glances [1]. These provide meaningful signals that a

robot could use to improve the quality of interaction. The use of these gaze patterns to predict intent was explored

by Huang et al., who quantified patterns of gaze cues to create a support vector machine (SVM) based model. Their

model had 76% accuracy at predicting intended user requests using eye gaze alone. [17]. They subsequently created an

3

THRI, Transactions on Human-Robot Interaction

Chan et al.

anticipatory control method for robots working in a team setting with humans. Using an intent prediction model for

eye gaze, the robotic assistant was able to reach target objects 2.5 seconds earlier than a comparable reactive method.

Additionally, user perception of the robot was found to be more positive when working with the robot incorporating

the eye gaze model [18]. They pointed out that the accuracy of theirs and other existing gaze prediction system [26] is

a limiting factor.

3 RESEARCH AIM

While there exists research that use human eye gaze to enable a robot to predict the user’s intent, and research that

uses AR to display robot status and plans to the user, prior work that utilizes both AR and user eye gaze in the context

of a joint action framework has not been reported. Our research aims to construct a joint action framework that utilizes

augmented reality and human eye gaze cues to enable bidirectional communication for human-robot collaboration, and

investigate the benefits this approach can bring.

We hypothesise that such a joint action framework for supporting human-robot collaborative tasks will improve

task efficiency and fluency, reduce the user’s cognitive load and increase the user’s trust in their robot partner.

4 JOINT ACTION FRAMEWORK

We propose a joint action framework implementation that can predict user intent, communicate robot intent, and

update the robot’s actions based on its cognitive model of the user (Fig. 2). Our system is comprised of three main

components: a) an eye gaze tracking and user intent prediction system, b) a traffic light warning system that conveys

the robot’s intent to the user through AR displays, and c) a robot motion planner that can update and adjust the robot’s

actions based on perceived user intent.

When working on a collaborative task, the user and the robot each formulates an action plan, and executes the

actions to contribute towards completing the task. Through the use of AR and eye gaze, our system enables the agents

to communicate and predict each other’s future actions. Using such information, each agent can then update their

cognitive model of the other agent, and adjust their own action plan accordingly.

To provide context for the development, implementation, and evaluation of our proposed framework, we consider a

collaboration scenario, where a human and a robot needs to move a set of items (blocks) from their starting locations to

their respective target locations (Fig. 3). Such a task is representative of many common tasks in the industry such as

packaging, sorting, and assembly.

We implemented the AR interface on a HoloLens 2 head-mounted AR display device1, and the robot motion planning
system using Robot Operating System (ROS) on a control PC. We used the Franka Emika Panda robot arm. The following

subsections explain the system modules.

4.1 AR Interface

Virtual Robot Model: We created a one-to-one scale virtual robot model co-located with the real robot. The virtual
model is connected to the real robot and mimics the real robot’s movements. An AR tag placed at a known location

relative to the real robot and the work table is used to calibrate the AR system at startup. The AR headset detects the

position of this marker using its outward facing cameras, and it uses its location to compute the relative transformation

between the virtual world and the physical world. Once this relative transformation is determined, the virtual robot

1Microsoft HoloLens 2|https://www.microsoft.com/en-us/hololens

4

Human-Robot Joint Action Framework using AR and Eye Gaze

THRI, Transactions on Human-Robot Interaction

Fig. 2. System Diagram

Fig. 3. The experiment setup. On the left are the blocks at their initial positions, ready to be picked up by either the robot and the
user. Behind the robot are two "block placement zones" which are highlighted with bright coloured (pink and red) papers.

model and other virtual objects can then be rendered at the appropriate locations. The virtual model allows the user to

confirm the calibration and connectivity between the virtual and real robot/system.

Traffic Light Warning System: A traffic light warning system is used to communicate the robot’s intent to the user
(Fig 4). The color red is used to indicate danger and yellow to indicate caution. As this association is well established to

most people due to the universal use in traffic lights, users can effortlessly interpret this information. In our system,

when the robot selects a target block to pick up and a target zone to drop off, it will highlight both the block it plans to

pick and the target placement zone in yellow. This signals to the human that the robot is intending to pick the selected

block, and place it in the selected zone. The robot will then move to a ready position, and plan it’s pick-and-place

5

THRI, Transactions on Human-Robot Interaction

Chan et al.

motion. After 3 seconds, the system will then highlight the selected block and drop off zone in red to signal to the user

that the robot is about to start moving, and the robot will execute the pick-and-place motion.

Eye Gaze Tracking & Prediction: We use the Hololens’ eye tracking function to track the user’s gaze and predict
their intention (Fig. 5) since people often gaze at the target object before they manipulate it. The Hololens’ eye tracking

function can determine where in the workspace the user is gazing at. If the user’s gaze continuously dwells on a block

for a certain period of time (𝑑 seconds), our system predicts the block as the next target block the user intends to

pick and move. Continuous dwell was used to filter out eye saccades that frequently occurs while the user scans the

workspace to plan their next moves. For our purposes, the dwell period 𝑑 was empirically set to 0.8 seconds to balance

between prediction speed and accuracy.

Block Placement Confirmation Menu: For the purpose of our user study, we implemented an interactive AR
window for the user to indicate to the robot which block they have picked (Fig. 6). The AR window has a set of buttons

with labels corresponding to each of the blocks. The user is asked to press the corresponding AR button each time after

they pick up a block. This is implemented as an easy and reliable way to enable to robot to keep track of which blocks

are still remaining in the workspace. In a real factory setting, this menu could be be replaced by a camera detection

systems.

4.2 Robot Motion Planning & Control System

The robot motion planner plans the robot actions. It takes into account the predicted user action, and adjusts the robot’s

action plan accordingly. During the collaboration task, at each time, the robot will randomly select its next target block
from the remaining blocks on the workspace. It then highlights the block in yellow using the Traffic Light Warning
System. While a block is highlighted yellow, if the Eye Gaze Tracking & Prediction module determines that the user
is also intending to pick the same block, the motion planner will decide to pick another block instead. However, once

the a target block is highlighted red, the robot is committed to that block, and will proceed to pick the block regardless

of what the predicted user target block is.

After the robot completes the picking action, it will check its gripper finger positions to determine if there is an

object between its fingers. If the gripper fingers are fully closed, the robot will infer that the user has taken the block

instead. The robot will then abort the placing motion, and plan to pick another block instead.

Fig. 4. Traffic Light System in action.

6

Human-Robot Joint Action Framework using AR and Eye Gaze

THRI, Transactions on Human-Robot Interaction

Fig. 5. Eye Gaze Tracking and Prediction system in action. The block with blue circle at the bottom are the one going to be picked by
the user from predicting their eye gaze.

Fig. 6. Block Placement Confirmation Menu. Red means the block has been taken by the user. Blue means the block has not been
picked up yet, or block that has been picked and placed by the robot.

The starting positions of the blocks and the placement zone positions are fixed and known to the robot, and we used

the MoveIt! Motion Planning Framework for planning the robot’s trajectories [8]

5 EXPERIMENT

5.1 Experimental Design

We conducted a user study to evaluate the effectiveness of our joint action framework implementation and examine the

benefits it provides to human-robot collaboration tasks. We tested four conditions in our user study, where participants

collaborated with the robot using 1) our full framework implementation with both eye gaze prediction and the AR

traffic light warning system, 2) only eye-gaze prediction, 3) only the AR traffic light warning system, and 4) and a

7

THRI, Transactions on Human-Robot Interaction

Chan et al.

Table 1. Subjective Fluency Metric Scales in post experiment survey

Individual Questions
Q1 The human-robot team worked fluently together.
Q2 The human-robot team’s fluency improved over time.
Q3 The robot contributed to the fluency of the collaboration.
Q4 The robot was committed to the success of the team.
Q5 The robot had an important contribution to the success of
the team.

baseline condition with no eye gaze prediction and no AR displays. A Balanced Latin Square design is used to mitigate

ordering effects

The experiment setup is shown in Fig. 3. There are 15 blocks placed on one side of the workspace, and the human and

the robot are required to collaboratively move all the blocks to the other side of the workspace. There are two placement

zones labelled “1" and“2" (red and pink areas in Fig. 3). Each block also has a label“1" or “2". Both the participant and the

robot can only move one block at a time, and they can work asynchronously. The participant is instructed to place the

block they pick into the placement zone with the same label. However, the robot can place each block it picks randomly

in either zone. This is to make the robot’s action appear not fully predictable to the participant, so that we can test

the benefits of our system. Participants were told that when a robot highlights a block/zone yellow, it means that the

robot has selected the block/zone. However, the participant may still override the robot’s decision by taking the block

selected. However, once the block/zone is highlighted red, it means that the robot has committed to it, and is going to

start moving and picking up the block, and the participant should not try to pick the highlighted block.

The rules for the participants are the following:

• The participant cannot enter a zone that the robot has committed to (red zone).
• The participant cannot grab a block after the robot has committed to it.
• The participant cannot be within 20cm of the robot or in a position that risks causing a collision with the robot.

These rules are designed to protect the participant from colliding with the robot. The breaching of any of these rules

will result in the E-stop button for the robot being pressed by the experimenter. Both the robot and the participant and

the robot are then reset, the blocks they are holding are placed back in their starting position, before the experiment

continued.

The experiment is completed when all the blocks have been moved to the placement zones. After the experiment,

participants are asked to complete a collection of surveys, which consists of the NASA Task Load Index [15], 14 items

from Trust Perception Scale-HRI [29] for perception of trust and a subset of questions from the Subjective Fluency

Metric scales [16], which are listed in Table 1, for task fluency. We also recorded the time taken to finish the whole task,

the number of block picking errors and block placing errors. A block picking error occurs when the participant picks

up the block that the robot has committed to pick up. A block placing error occurs when the participant places a block

into the same zone the robot is placing its block into.

5.2 Hypotheses

We hypothesize the following:

H1: Our joint action framework implementation using AR and gaze tracking together will increase task efficiency

when compared to using AR alone, using Gaze Tracking alone or using neither.

8

Human-Robot Joint Action Framework using AR and Eye Gaze

THRI, Transactions on Human-Robot Interaction

Table 2. Task Completion Time measured in seconds

AR and Gaze AR only Gaze only No AR No Gaze

178.3±24.4

180.7±24.5 180.9±25.8

179.1±29.6

t(s)

Table 3. Number of Block Picking and Placing errors in each trial

AR and Gaze AR only Gaze only No AR No Gaze

Placing Error
Picking Error

0.19±0.39
0.05±0.23

0.37±0.54 0.65±0.81
0.16±0.49 0.38±0.54

1.27±0.95
0.49±1.54

H2: Our joint action framework implementation using AR and gaze tracking will not increase the perceived taskload

compared to using AR alone, using Gaze Tracking alone or using neither.

H3: Our joint action framework implementation using AR and gaze tracking together will increase the perception of

trust toward the robot compared to using AR alone, using Gaze Tracking alone or using neither.

H4: Our joint action framework implementation using AR and gaze tracking together will increase the fluency of

the task compared to using AR alone, using Gaze Tracking alone or using neither.

6 DATA ANALYSIS & RESULTS

The data were collected from 37 participants (27 males, 7 females, 3 prefer not to say) with a mean age of 22.75 (SD =

2.68). Due to COVID-19 restrictions in place at the time of the experiment, only students from Monash University were

available to participate in the experiment. The user study was approved by the Monash University Human Research

Ethics Committee (Project ID: 28438). Following existing works [6, 10, 30], we analyzed our data using ANOVA, and

post-hoc analysis using t-test and Mann Whitney-Wilcoxon test. Examining our data using the Anderson-Darling

normality test showed that three out of thirty seven sets (8.1%) were normally distributed. A study by de Winter and

Doudou [10] showed that there is only a marginal difference in power between using paired t-test and Mann-Whitney-

Wilcoxon test when the data is not normally distributed. We analyzed our data with both methods and confirmed that

results were marginally different, and paired t-tests will be reported below. Results were treated with Bonferroni’s

p-value correction [19] and significance reported at 𝛼 = 0.05.

6.1 Task Completion Time

Task completion times are reported in Table 2. The completion times for all conditions were very similar, and ANOVA

results indicated that there are no significant differences among all conditions tested for task completion time.

6.2 Block Placing and Picking Errors

Number of block placing and picking errors are reported in Table 3. ANOVA results indicated that there are significant

differences in block placing errors among the four conditions (F(3, 144) = 16, p < 0.001), while there are no differences

in block picking errors among the four conditions. Post-hoc analysis showed that our joint action framework (AR and

Gaze Tracking) significantly reduced block placing errors when compared to No AR no Gaze, i.e. baseline, (t(36) =

-6.8938, p < 0.001) and when compared to Gaze Tracking only (t(36) = -3.0026, p = 0.0145).

6.3 Perceived Taskload

Results indicated that there were no significant differences among the four conditions for perceived taskload measured

by the NASA-TLX. Results of all four conditions are summarized in Table 4.

9

THRI, Transactions on Human-Robot Interaction

Chan et al.

Table 4. NASA-TLX [15] questionnaire results (7 point scale). For Performance, high score is better; for the rest, lower score is better

AR and Gaze AR only Gaze only No AR No Gaze

Mental
Physical
Temporal
Performance
Effort
Frustration

2.08±1.07
1.92±1.0
2.38±1.26
1.97±1.48
2.03±1.03
2.13±1.36

2.30±1.27 2.81±1.54
1.84±0.85 2.05±1.21
2.46±1.39 2.43±1.46
2.35±1.91 2.16±1.24
2.43±1.50 2.62±1.58
2.00±1.27 2.49±1.69

2.65±1.60
1.95±1.18
2.73±1.5
2.22±1.49
2.57±1.50
2.35±1.65

Table 5. Perception of Trust [29] survey results (10 point scale). Asterisks indicate significant differences found.

Dependable
Reliable
Unresponsive*
Predictable*
Consistent*
Successful
Malfunction
Have Error

AR & Gaze AR only Gaze only No AR/Gaze
7.92±2.10 7.11±2.73 7.08±2.61
8.19±1.83 6.81±2.80 7.08±2.57
1.81±2.43 1.51±1.80 2.84±3.10
7.16±2.85 6.38±3.22 5.22±3.11
8.08±2.19 7.43±2.76 6.51±2.77
8.30±1.71 8.24±1.95 7.59±2.33
0.97±1.85 1.97±2.90 1.89±2.67
1.41±2.03 1.78±2.21 1.89±2.24
Provide Feedback* 4.54±4.32 5.68±4.05 1.73±2.85
7.65±2.77 7.70±2.38 7.54±2.34
8.00±2.49 7.38±3.10 2.95±3.73
6.95±3.46 6.62±3.46 2.03±3.12
8.30±2.29 8.05±2.36 7.03±3.17
7.86±2.79 7.35±3.11 6.11±3.70

6.81±2.26
7.24±2.12
2.97±3.10
4.73±3.05
7.00±2.53
8.27±1.67
1.76±2.69
1.51±2.10
1.49±2.96
7.57±2.27
2.54±3.62
1.54±2.86
7.70±2.43
6.89±3.34

Meet Needs
Informative*
Communicative*
Do as Instructed
Follow Directions

6.4 Trust Perception

Survey results of trust perception are shown in Table 5. ANOVA results showed that there were significant differences

for several items - Unresponsive: (F(3, 144) = 2.71, p = 0.0472), Predictable: (F(3, 144) = 4.68, p = 0.0038, Provide Feedback:

(F(3, 144) = 11.96, p < 0.001), Informative: (F(3, 144) = 27.7. p < 0.001), Communicative: (F(3, 144) = 28.83, p < 0.001)) and

fluency (Fluently Improved: (F(3, 144) = 3.22, p = 0.025).

Post-hoc analysis revealed that our system combining AR and Gaze Tracking improved user’s trust toward the

robot in some aspects compared to the baseline. Our system received better ratings for the following items: Predictable

(t(36) = 4.0142, p < 0.001), Provide Feedback (t(36) = 3.3763, p = 0.0053), Informative (t(36) = 6.8456, p < 0.001) and

Communicative (t(36) = 6.6544, p < 0.001). Our system when compared to the Gaze-only system received better ratings

for the items Consistent (t(36) = 2.5558, p = 0.0449), Provide Feedback (t(36) = 4.8061, p < 0.001), Informative (t(36)

= 7.095, p < 0.001) and Communicative (t(36) = 5.715, p < 0.001). The combined system when compared to AR-only

system did not have any significant differences.

6.5 Fluency

Table 6 summarizes the results for the survey questions on fluency. Our system combining the use of AR and Gaze yielded

the best scores for all questions, except for fluency improvements, which had higher score only in the AR condition.

ANOVA results showed that there were significant differences in Q1 of the fluency survey - Fluently together (F(3, 144)

= 3.2238, p = 0.0245). Post-hoc analysis indicated that with the help of the AR and Gaze Tracking system combined,

10

Human-Robot Joint Action Framework using AR and Eye Gaze

THRI, Transactions on Human-Robot Interaction

Table 6. Subjective Fluency Metrics [16] results (7 point scale). Higher score is better

AR & Gaze AR only Gaze only No AR/Gaze

Fluently Together 5.73±1.31 5.43±1.33 4.67±1.8
Fluency Improved 5.41±1.55 5.43±1.62 4.92±1.75
5.62±1.60 5.21±1.74 4.65±1.98

Robot Fluency
Contribute

Robot Committed 5.41±1.75 5.05±1.86 4.78±1.9
5.51±1.67 5.49±1.69 5.08±1.85

Robot Success
Contribution

5.05±1.63
5.20±1.48
4.78±1.74

4.95±1.74
5.03±1.65

users felt that the human-robot team worked together more fluently (Q1) compared to using Gaze Tracking-only system

(t(36) = 2.8368, p = 0.0223). Other than that, there were no statistically significant differences.

7 DISCUSSION

7.1 Task Efficiency

Our hypothesis H1 stated that our joint action framework implementation using AR and gaze tracking will increase
task efficiency, and our results supported this claim. Experiment results showed that our system combining AR and Gaze

Tracking yielded fewer block placing errors when compared to the baseline or the Gaze Tracking only system. While in

our experiment, block placing errors did not result in any penalties or other consequences, in certain human-robot

collaborative tasks, making errors could potentially result in a costly recovery procedure, or even injuring a human. In

such cases, the benefits provided by our joint action framework by reducing task errors would be even greater.

7.2 Perceived Taskload

Results shows that our system combining AR and Gaze tracking did not increase the perceived taskload, and this
provided support to H2. It is an advantage that our system does not come with the cost of an increased perceived
taskload, as some existing studies have found that the use of AR can result in increased mental demand [28]. There are

a couple of differences between our study the the one in [28]. Our study involves a human-robot collaboration task, and

our system uses a traffic light warning system, which is highly familiar to most users, for AR display. The study in [28],

on the other hand, involved a robot trajectory programming task, and uses curves and spherical markers to display

robot trajectory, which is less familiar to users, in AR. As AR is still a relatively novel technology to users, it may be

important to choose more familiar concepts for implementing AR visual displays when possible, to avoid increasing the

users’ cognitive load.

7.3 Perception of Trust

Our results have provided support to H3, which stated that our joint action framework implementation using AR and
gaze tracking together will increase the perception of trust. As reported in the previous section, our system yield better

scores in almost all items in the trust perception survey, when compared to all other conditions. In a team collaboration

setting, bidirectional communication and establishing mutual understanding is important towards building trust. Our

joint action framework system aims to enable to establishment of mutual understanding by enabling bidirectional

communication between collaborating humans and robots. Through our study, we have demonstrated that our system

enables better establishment of trust between human and robot partners.

11

THRI, Transactions on Human-Robot Interaction

Chan et al.

7.4 Task Fluency

Our results showed that our system yielded best scores for almost all task fluency survey questions, although not
reaching significance. Thus we found weak support for H4. We suspect that this may be a result of the limited accuracy
of the gaze tracking system of the Hololens, as we found that the gaze prediction system only had an accuracy of around

50% when predicting which block the user actually picks in the experiment. Improving the gaze tracking system’s

performance can potentially further improve the task fluency when using our overall system, and this can be a direction

of future work.

8 CONCLUSION

In this paper, we have proposed the design of a novel joint action framework for facilitating efficient human-robot

collaboration. Our system enables bidirectional communication via the use of AR to communicate robot intent, and

eye gaze to predict user’s task intent. We have presented an implementation of the framework and a user study to

demonstrate the benefits of our proposed system. Results showed that our system increases task efficiency, trust, as well

as task fluency, while not imposing a higher perceived taskload, which is a critical outcome for creating more functional

human-robot collaborative systems without burdening the user. Potential future work may focus on improving the

performance of gaze tracking, as we believe this can further improve the performance of our joint action framework

system, or further applying and testing the benefits of our system for highly asynchronous tasks.

ACKNOWLEDGMENTS

This project was supported by the Australian Research Council Discovery Projects Grant, Project ID: DP200102858.

REFERENCES
[1] Reuben M. Aronson, Thiago Santini, Thomas C. Kübler, Enkelejda Kasneci, Siddhartha Srinivasa, and Henny Admoni. 2018. Eye-Hand Behavior in
Human-Robot Shared Manipulation. In Proceedings of the 2018 ACM/IEEE International Conference on Human-Robot Interaction. ACM, Chicago IL
USA, 4–13. https://doi.org/10.1145/3171221.3171287

[2] Simon Baron-Cohen, Alan M. Leslie, and Uta Frith. 1985. Does the autistic child have a “theory of mind” ? Cognition 21, 1 (Oct. 1985), 37–46.

https://doi.org/10.1016/0010-0277(85)90022-8

[3] Brigid Barron. 2000. Achieving Coordination in Collaborative Problem-Solving Groups. Journal of the Learning Sciences 9, 4 (Oct. 2000), 403–436.

https://doi.org/10.1207/S15327809JLS0904_2 Publisher: Routledge _eprint: https://doi.org/10.1207/S15327809JLS0904_2.

[4] Sebastian Blankemeyer, Rolf Wiemann, Lukas Posniak, Christoph Pregizer, and Annika Raatz. 2018. Intuitive Robot Programming Using Augmented

Reality. Procedia CIRP 76 (Jan. 2018), 155–160. https://doi.org/10.1016/j.procir.2018.02.028

[5] Cynthia Breazeal, Guy Hoffman, and A. Lockerd. 2004. Teaching and working with robots as a collaboration. Proceedings of the AAMAS 3, 1030–

1037. https://doi.org/10.1109/AAMAS.2004.242646

[6] Wesley P. Chan, Geoffrey Hanks, Maram Sakr, Haomiao Zhang, Tiger Zuo, H.F. Machiel Van der Loos, and Elizabeth Croft. 2022. Design and
Evaluation of an Augmented Reality Head-Mounted Display Interface for Human Robot Teams Collaborating in Physically Shared Manufacturing
Tasks. J. Hum.-Robot Interact. (feb 2022). https://doi.org/10.1145/3524082 Just Accepted.

[7] Aurélie Clodic, Elisabeth Pacherie, Rachid Alami, and Raja Chatila. 2017. Key Elements for Human-Robot Joint Action. In Sociality and Normativity
for Robots: Philosophical Inquiries into Human-Robot Interactions, Raul Hakli and Johanna Seibt (Eds.). Springer International Publishing, Cham,
159–177. https://doi.org/10.1007/978-3-319-53133-5_8

[8] David Coleman, Ioan Sucan, Sachin Chitta, and Nikolaus Correll. 2014. Reducing the barrier to entry of complex robotic software: a moveit! case

study. Journal of Software Engineering for Robotics 5(1) (5 2014), 3–16.

[9] Francesco De Pace, Federico Manuri, Andrea Sanna, and Claudio Fornaro. 2020. A systematic review of Augmented Reality interfaces for collaborative

industrial robots. Computers & Industrial Engineering 149 (Nov. 2020), 106806. https://doi.org/10.1016/j.cie.2020.106806

[10] JCF de Winter and D Dodou. 2010. Five-point Likert items: t test versus Mann-Whitney-Wilcoxon. Practical Assessment, Research and Evaluation 15

(2010), 1–12.

[11] Sandra Devin and Rachid Alami. 2016. An implemented theory of mind to improve human-robot shared plans execution. In 2016 11th ACM/IEEE
International Conference on Human-Robot Interaction (HRI). IEEE, Christchurch, New Zealand, 319–326. https://doi.org/10.1109/HRI.2016.7451768

12

Human-Robot Joint Action Framework using AR and Eye Gaze

THRI, Transactions on Human-Robot Interaction

[12] Pierre Dillenbourg and David Traum. 2006.

ing. Journal of the Learning Sciences 15, 1 (Jan. 2006), 121–151.
https://doi.org/10.1207/s15327809jls1501_9.

Sharing Solutions: Persistence and Grounding in Multimodal Collaborative Problem Solv-
https://doi.org/10.1207/s15327809jls1501_9 Publisher: Routledge _eprint:

[13] Epson Moverio 2018. Augmented Reality and Mixed Reality. https://epson.com/moverio-augmented-reality.
[14] Alexandra Frischen, Andrew P. Bayliss, and Steven P. Tipper. 2007. Gaze cueing of attention: Visual attention, social cognition, and individual
differences. Psychological Bulletin 133, 4 (2007), 694–724. https://doi.org/10.1037/0033-2909.133.4.694 Place: US Publisher: American Psychological
Association.

[15] Sandra G. Hart. 2006. Nasa-Task Load Index (NASA-TLX); 20 Years Later. Proceedings of the Human Factors and Ergonomics Society Annual Meeting

50, 9 (2006), 904–908. https://doi.org/10.1177/154193120605000909 arXiv:https://doi.org/10.1177/154193120605000909

[16] Guy Hoffman. 2019. Evaluating Fluency in Human–Robot Collaboration. IEEE Transactions on Human-Machine Systems 49, 3 (2019), 209–218.

https://doi.org/10.1109/THMS.2019.2904558

[17] Chien-Ming Huang, Sean Andrist, Allison Sauppé, and Bilge Mutlu. 2015. Using gaze patterns to predict task intent in collaboration. Frontiers in

Psychology 6 (2015). https://doi.org/10.3389/fpsyg.2015.01049 Publisher: Frontiers.

[18] Chien-Ming Huang and Bilge Mutlu. 2016. Anticipatory robot control for efficient human-robot collaboration. 83–90. https://doi.org/10.1109/HRI.

2016.7451737

[19] Mohieddin Jafari and Naser Ansari-Pour. 2019. Why, When and How to Adjust Your P Values? Cell journal 20 (05 2019), 604–607. https:

//doi.org/10.22074/cellj.2019.5992

[20] Richard Klimoski and Susan Mohammed. 1994. Team Mental Model: Construct or Metaphor? Journal of Management 20, 2 (1994), 403–437.

https://doi.org/10.1177/014920639402000206 arXiv:https://doi.org/10.1177/014920639402000206

[21] Bernard C Kress and William J Cummings. 2017. Towards the Ultimate Mixed Reality Experience: HoloLens Display Architecture Choices. In SID

Symposium Digest of Technical Papers, Vol. 48. 127–131.

[22] Gustav Kuhn, Valerie Benson, Sue Fletcher-Watson, Hanna Kovsho, Cristin A McCormick, Julie Kirkby, and Sue R Leekam. 2010. Eye movements

affirm: automatic overt gaze and arrow cueing for typical adults and adults with autism spectrum disorder. Exp Brain Res (2010), 11.

[23] Przemyslaw A. Lasota, Gregory F. Rossano, and Julie A. Shah. 2014. Toward safe close-proximity human-robot interaction with standard industrial
robots. In 2014 IEEE International Conference on Automation Science and Engineering (CASE). IEEE, Taipei, 339–344. https://doi.org/10.1109/CoASE.
2014.6899348

[24] Magic Leap 2018. Magic Leap. https://www.magicleap.com/.
[25] Sotiris Makris, Panagiotis Karagiannis, Spyridon Koukas, and Aleksandros-Stereos Matthaiakis. 2016. Augmented reality system for operator

support in human–robot collaborative assembly. CIRP Annals 65, 1 (Jan. 2016), 61–64. https://doi.org/10.1016/j.cirp.2016.04.038

[26] Bilge Mutlu, Fumitaka Yamaoka, Takayuki Kanda, Hiroshi Ishiguro, and Norihiro Hagita. 2009. Nonverbal leakage in robots: communication of
intentions through seemingly unintentional behavior. In Proceedings of the 4th ACM/IEEE international conference on Human robot interaction (HRI
’09). Association for Computing Machinery, New York, NY, USA, 69–76. https://doi.org/10.1145/1514095.1514110

[27] Stefanos Nikolaidis, Ramya Ramakrishnan, Keren Gu, and Julie Shah. 2015. Efficient Model Learning from Joint-Action Demonstrations for

Human-Robot Collaborative Tasks. In 2015 10th ACM/IEEE International Conference on Human-Robot Interaction (HRI). 189–196.

[28] Camilo Perez Quintero, Sarah Li, Matthew KXJ Pan, Wesley P. Chan, H.F. Machiel Van der Loos, and Elizabeth Croft. 2018. Robot Programming
Through Augmented Trajectories in Augmented Reality. In 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). 1838–1844.
https://doi.org/10.1109/IROS.2018.8593700 ISSN: 2153-0866.

[29] Kristin Schaefer. 2016. Measuring Trust in Human Robot Interactions: Development of the “Trust Perception Scale-HRI”. 191–218. https://doi.org/10.

1007/978-1-4899-7668-0_10

[30] S. Stadler, K. Kain, M. Giuliani, N. Mirnig, G. Stollnberger, and M. Tscheligi. 2016. Augmented reality for industrial robot programmers: Workload
analysis for task-based, augmented reality-supported robot control. In RO-MAN. 179–184. https://doi.org/10.1109/ROMAN.2016.7745108

[31] Piet Van den Bossche, Wim Gijselaers, Mien Segers, Geert Woltjer, and Paul Kirschner. 2011. Team learning: building shared mental models.

Instructional Science 39, 3 (May 2011), 283–301. https://doi.org/10.1007/s11251-010-9128-3

[32] Christian Vogel and Norbert Elkmann. 2017. Novel Safety Concept for Safeguarding and Supporting Humans in Human-Robot Shared Workplaces
with High-Payload Robots in Industrial Applications. In Proceedings of the Companion of the 2017 ACM/IEEE International Conference on Human-Robot
Interaction (HRI ’17). Association for Computing Machinery, New York, NY, USA, 315–316. https://doi.org/10.1145/3029798.3038314

[33] Michael Walker, Hooman Hedayati, Jennifer Lee, and Daniel Szafir. 2018. Communicating Robot Motion Intent with Augmented Reality. In
Proceedings of the 2018 ACM/IEEE International Conference on Human-Robot Interaction (HRI ’18). Association for Computing Machinery, New York,
NY, USA, 316–324. https://doi.org/10.1145/3171221.3171253

[34] Michael E. Walker, Hooman Hedayati, and Daniel Szafir. 2019. Robot Teleoperation with Augmented Reality Virtual Surrogates. In 2019 14th
ACM/IEEE International Conference on Human-Robot Interaction (HRI). 202–210. https://doi.org/10.1109/HRI.2019.8673306 ISSN: 2167-2148.
[35] Thomas Weng, Leah Perlmutter, Stefanos Nikolaidis, Siddhartha Srinivasa, and Maya Cakmak. 2019. Robot Object Referencing through Legible
Situated Projections. In 2019 International Conference on Robotics and Automation (ICRA). 8004–8010. https://doi.org/10.1109/ICRA.2019.8793638
ISSN: 2577-087X.

[36] Tomonori Yamamoto, Niki Abolhassani, Sung Jung, Allison M. Okamura, and Timothy N. Judkins. 2012. Augmented reality and haptic interfaces for
robot-assisted surgery. The International Journal of Medical Robotics and Computer Assisted Surgery 8, 1 (2012), 45–56. https://doi.org/10.1002/rcs.421

13

THRI, Transactions on Human-Robot Interaction

Chan et al.

_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/rcs.421.

14

