0
2
0
2

v
o
N
7
1

]
L
M

.
t
a
t
s
[

2
v
7
3
2
3
0
.
2
0
0
2
:
v
i
X
r
a

LEARNING CHARME MODELS WITH NEURAL NETWORKS

JOS´E G. G ´OMEZ GARC´IA∗, JALAL FADILI∗∗, AND CHRISTOPHE CHESNEAU∗

Abstract. In this paper, we consider a model called CHARME (Conditional Heteroscedas-
tic Autoregressive Mixture of Experts), a class of generalized mixture of nonlinear nonpara-
metric AR-ARCH time series. Under certain Lipschitz-type conditions on the autoregressive
and volatility functions, we prove that this model is stationary, ergodic and τ -weakly depen-
dent. These conditions are much weaker than those presented in the literature that treats
this model. Moreover, this result forms the theoretical basis for deriving an asymptotic
theory of the underlying (non)parametric estimation, which we present for this model. As
an application, from the universal approximation property of neural networks (NN), we de-
velop a learning theory for the NN-based autoregressive functions of the model, where the
strong consistency and asymptotic normality of the considered estimator of the NN weights
and biases are guaranteed under weak conditions.

1. Introduction

Statistical models such as AR, ARMA, ARCH, GARCH, ARMA-GARCH, etc. are still
popular today in time series analysis (see [46, Part III]). These time series are part of
the general class of models called conditional heteroscedastic autoregressive nonparametric
(CHARN) process, which takes the form

Xt = f (Xt−1, . . . , Xt−p, θ0) + g(Xt−1, . . . , Xt−p, λ0)(cid:15)t,

(1.1)

with unknown functions f , g and independent identically distributed zero-mean innovations
(cid:15)t. It provides a ﬂexible class of models for many applications such as in econometrics or
ﬁnance, see [25] and [22]. However, in practice, note that it is not always realistic to assume
that the observed process has the same trend function f and the same volatility function g at
each time point (this is for instance the case of EEG signals, see [38]). In particular, if those
functions change slowly over time, local stationarity can be assumed (see [9]), in which there
is already a good list of appropriate models. Anyway, estimation procedures for those models
are mainly based on applying estimators for stationary processes locally in time which do
not work well if the structure of the time series generating mechanism changes more or less
abruptly. In this paper, we consider a more general class of nonparametric models (called
CHARME), which adapt to situations where explosive phases may be included. The basics
of this new class are presented below.

Date: Submitted on November 16, 2020.
2000 Mathematics Subject Classiﬁcation. Primary 37A25, 49J53 ; Secondary 92B20, 37M10.
Key words and phrases. Nonparametric AR-ARCH; deep neural network; mixture models; Markov switch-

ing; τ -weak dependence; ergodicity; stationarity; identiﬁability; consistency.
The ﬁrst author was funded by the Normandy Region RIN program.
The second author was partly supported by Institut Universitaire de France.

1

 
 
 
 
 
 
1.1. The CHARME model. Let (E, (cid:107)·(cid:107)) be a Banach space, and E endowed with its Borel
σ−algebra E. The product Banach space Ep is naturally endowed with its product σ−algebra
E ⊗p. The conditional heteroscedastic p−autoregressive mixture of experts CHARME(p)
model, with values in E, is the random process deﬁned by

Xt =

K
(cid:88)

k=1

ξ(k)
t

where

(cid:0)fk(Xt−1, . . . , Xt−p, θ0

k) + gk(Xt−1, . . . , Xt−p, λ0

k)(cid:15)t

(cid:1) , t ∈ Z,

(1.2)

• for each k ∈ [K] := {1, 2, . . . , K}, fk : Ep × Θk −→ E and gk : Ep × Λk −→ R are
the so-called autoregressive and volatility functions, with Θk and Λk as their spaces
of parameters, which are, respectively, E ⊗p × B(Θk)– and E ⊗p × B(Λk)–measurable
functions, where B(Θk) is the Borel ﬁeld on Θk and similarly for Λk;

• ((cid:15)t)t are E−valued independent identically distributed (iid) zero-mean innovations;
• ξ(k)
t = I{Rt=k}, with IC the characteristic function of C (takes 1 on C and 0 otherwise),
where (Rt)t∈Z is an iid sequence with values in a ﬁnite set of states [K], which is
independent of the innovations ((cid:15)t)t∈Z. In the sequel, we will denote πk = P(R0 = k).
Model (1.2) can be extended to the case where p = ∞, called CHARME with inﬁnite
memory, denoted by CHARME(∞) for short. For the related setting, we will deﬁne the
subset of EN as

E∞ := (cid:8)(xk)k>0 ∈ EN : xk = 0 for k > N, for some N ∈ N∗(cid:9) ,

which will be considered with its product σ−algebra E ⊗N.

It is obvious that the model (1.2) contains the model (1.1) (corresponding to the case
K = 1 in (1.2)). On the other hand, applications of the CHARME model (1.2) have been
directly and indirectly seen in various areas, such as ﬁnancial analysis [54] (for asset man-
agement and risk analysis) and [58] (for predictions of daily probability distributions of
S&P returns), hydrology [31] (for the detection of structural changes in hydrological data),
electroencephalogram (EEG) signals [37] (for the analysis of EEG recordings from human
subjects during sleep), among others.

k, λ0

1.2. Contributions. The objective of this article is to build an estimation theory for the
CHARME and feedforward neural network (NN) based CHARME models. In this regard,
we ﬁrst approach the CHARME model in a general context, showing its τ -weak dependence,
ergodicity and stationarity under weak conditions. This consequence together with simple
conditions allow us to establish strong consistency for the estimators of the parameters
(θ0
k)k∈[K] of the model (1.2), which are the minimizers of a general loss function, not
necessarily diﬀerentiable. Addressing non-diﬀerentiable losses and non iid samples is rather
challenging and necessitate to invoke intricate arguments from the calculus of variations (in
particular on normal integrands and epi-convergence; see Section 4). Such arguments are not
that common in the statistical literature and allow us to investigate new cases that have not
been considered before. Additionally, under the same weak assumptions to obtain ergodicity
and stationarity together with usual regularity conditions on the autoregressive functions,
we prove the asymptotic normality of the conditional least-squares estimator of a simpler
CHARME model (i.e., (1.2) with gk ≡ 1).

2

For the NN-based CHARME(p) model (i.e., the CHARME(p) model with NN-based au-
toregressive functions), we specialize the above results that will ensure establish learning
consistency guarantees.

Our results are not limited to the case where p is ﬁnite.

Indeed, we will show that
the stationary solution of the CHARME(∞) model can be approximated by the stationary
solution of its associated CHARME(p) model (see Remark 3.1 and (3.4) in Section 3), when
p is large enough. Moreover, in Section 6.3, we will argue that CHARME(p) models can be
universally approximated by NN-based CHARME(p) models. Altogether, this will provide
us with a provably controlled way to learn inﬁnity memory CHARME models with neural
networks.

1.3. Relation to prior work. Stockis et al. [51] show geometric ergodicity of CHARME(p)
models, with p < ∞, under certain conditions, including regularity. Speciﬁcally, they de-
mand that the iid random variables (cid:15)t have a continuous density function, positive every-
where. In contrast, in this paper, the innovations are not supposed to be absolutely contin-
uous and our approach can also be applied, for example, to discrete state space processes.
Note also that [51] uses this regularity condition in order to obtain some mixing conditions
of ηt = (Xt, ξt)t∈Z for deriving asymptotic stability of the model through the results of [41].
However, observe that taking a simple model as the AR(1)-input, solution of the recursion

Xt =

1
2

(Xt−1 + (cid:15)t),

t ∈ Z,

(1.3)

with ((cid:15)t)t∈Z iid such that P((cid:15)0 = 0) = P((cid:15)0 = 1) = 1/2, we can see that the assumptions
in [51] are not satisﬁed. In fact, this model is not mixing, see [1]. On the other hand, this
model is τ -weakly dependent and satisﬁes all our assumptions, see [12].

1.4. Paper organization. The paper is organized as follows: In Section 2 we start with
the preliminaries such as the deﬁnition and most important properties of τ -weak dependence
which characterize our model, and a summary of neural networks. In Section 3 we study the
properties of ergodicity and stationarity of the CHARME model, which will be essential for
developing a theory of estimation of the model. In Section 4 we provide estimators of the pa-
rameters of the model (1.2) and we prove its strong consistency under very weak conditions.
Asymptotic normality of the conditional least-squares estimator is also established in Sec-
tion 5, but for a simpler CHARME model (the model (1.2) with gk ≡ 1) in order to simplify
the presentation. In Section 6 we discuss the previous results in the context of NN-based
CHARME models and examine the diﬀerence between approximation and exact modeling
by NNs. Numerical experiments are included in Section 7 and the proofs in Section 8.

2. Preliminaries
Let (E, (cid:107) · (cid:107)) be a Banach space and h : E −→ R. We deﬁne (cid:107)h(cid:107)∞ = supx∈E |h(x)| and

the Lipschitz constant/modulus of h as

Lip(h) = sup
x(cid:54)=y∈E

|h(x) − h(y)|
(cid:107)x − y(cid:107)

.

For an E−valued random variable X deﬁned on a probability space (Ω, A, P), and m ≥ 1,
we denote by (cid:107)·(cid:107)m the Lm-norm, i.e., (cid:107)X(cid:107)m = (E(cid:107)X(cid:107)m)1/m, where E denotes the expectation.
3

2.1. Weak dependence. The appropriate notion of weak dependence for the model (1.2)
was introduced in [12]. It is based on the concept of the coeﬃcient τ deﬁned below.
Deﬁnition 2.1 (τ -dependence). Let (Ω, A, P) be a probability space, M a σ-sub-algebra of
A and X a random variable with values in E such that (cid:107)X(cid:107)1 < ∞. The coeﬃcient τ is
deﬁned as

(cid:27)(cid:12)
(cid:12)
(cid:12)
(cid:12)

.

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:90)

(cid:26)(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:90)

(cid:12)
(cid:12)
h(x)PX(dx)
(cid:12)
(cid:12)

τ (M, X) = E

sup

h(x)PX|M(dx) −

: h such that Lip(h) ≤ 1

E
Note that if Y is any random variable with the same distribution as X and independent

E

of M, then

τ (M, X) ≤ (cid:107)X − Y (cid:107)1.
This is a coupling argument that allows us to easily bound the τ coeﬃcient. See the examples
in [12]. On the other hand, if the probability space (Ω, A, P) is rich enough (which we always
assume in the sequel), there exists X ∗ with the same distribution as X and independent of
M such that τ (M, X) = (cid:107)X − X ∗(cid:107)1.

Using the deﬁnition of this τ coeﬃcient with the σ-algebra Mp = σ(Xt, t ≤ p) and the
norm (cid:107)x − y(cid:107) = (cid:107)x1 − y1(cid:107) + · · · + (cid:107)xk − yk(cid:107) on Ek, we can assess the dependence between
the past of the sequence (Xt)t∈Z and its future k-tuples through the coeﬃcients

τk(r) = max
1≤l≤k

1
l

sup{τ (Mp, (Xj1, . . . , Xjl)) with

p + r ≤ j1 < · · · < jl}.

Finally, denoting τ (r) := τ∞(r) = supk>0 τk(r), the time series (Xt)t∈Z is called τ -weakly
dependent if its coeﬃcients τ (r) tend to 0 as r tends to inﬁnity.

2.2. Neural networks. Neural networks produce structured parametric families of func-
tions that have been studied and used for almost 70 years, going back to the late 1940’s
and the 1950’s [27, 47]. An often cited theoretical feature of neural networks, known since
the 1980’s, is their universal approximation capacity [30], i.e., given any continuous target
function f and a target accuracy (cid:15) > 0, neural networks with enough judiciously chosen
parameters give an approximation to the function within an error of size (cid:15).

It appears then natural to use this property when it comes to model the functions fk and

gk, k ∈ [K], of the process (1.2).
Deﬁnition 2.2. Let d, L ∈ N. A fully connected feedforward neural network with input
dimension d, L layers and activation map ϕ : R −→ R, is a collection of weight matrices
(cid:0)W (l)(cid:1)
l∈[L], where W (l) ∈ RNl×Nl−1 and b(l) ∈ RNl, with N0 = d,
and Nl ∈ N is the number of neurons for layer l ∈ [L]. Let’s gather these parameters in the
vector

l∈[L] and bias vectors (cid:0)b(l)(cid:1)

θ = (cid:0)(W (1), b(1)), (W (2), b(2)), . . . , (W (L), b(L))(cid:1) ∈

RNl×Nl−1 × RNl.

L×

l=1
Then, a neural network parametrized by1 θ produces a function

f : (x, θ) ∈ Rd ×

(cid:32) L×

l=1

RNl×Nl−1 × RNl

(cid:33)

(cid:55)→ f (x, θ) = x(L) ∈ RNL,

1We intentionally omit the explicit dependence on ϕ since the latter is chosen once for all.

4

where xL results from the following recursion:






x(0)
x(l)
x(L)

:= x,
:= ϕ(W (l)x(l−1) + b(l)),
:= W (L)x(L−1) + b(L),

for l = 1, . . . , L − 1,

where ϕ acts componentwise, that is, for y = (y1, . . . , yN ) ∈ RN , ϕ(y) = (ϕ(y1), . . . , ϕ(yN )).

The rectiﬁed linear unit (ReLU) is the activation map of preference in many applications,
but other examples of activation maps in the literature include the sigmoid, softplus, ramp
or other activations [50, Chapter 20.4].

Remark 2.1. Modern machine learning emphasizes the use of deep architectures (as opposed
to shallow networks popular in the 1980’s-1990’s). A few recent works have focused on the
advantages of deep versus shallow architectures in neural networks by showing that deep
networks can approximate many interesting functions more eﬃciently, per parameter, than
shallow networks (see [26, 56, 60, 61, 11] for a selection of rigorous results). In particular,
the work of [11] has shown that neural networks with suﬃcient depth and appropriate width,
possess greater expressivity and approximation power than traditional methods of nonlin-
ear approximation. They also exhibited large classes of functions which can be exactly or
eﬃciently captured by neural networks whereas classical nonlinear methods fall short of the
task.

3. Ergodicity and Stationarity of CHARME models

In this section we study the properties of ergodicity and stationarity of the model (1.2)
for the general case, i.e., for the case p = ∞, because the case p < ∞ is a straightforward
corollary. In turn, these properties will be instrumental in establishing statistical inference
guarantees.

Theorem 3.1. Consider the CHARME(∞) model, i.e., (1.2) with p = ∞. Assume that
there exist non-negative real sequences (a(k)
i )i≥1,k∈[K], such that, for any
x, y ∈ E∞, and ∀k ∈ [K],

i )i≥1,k∈[K] and (b(k)

(cid:107)fk(x, θ0

k) − fk(y, θ0

k)(cid:107) ≤

∞
(cid:88)

a(k)
i (cid:107)xi − yi(cid:107)

and

|gk(x, θ0

k) − gk(y, θ0

k)| ≤

i=1
∞
(cid:88)

b(k)
i (cid:107)xi − yi(cid:107).

i=1

(3.1)

Denote Ak = (cid:80)∞

i=1 a(k)

i

, Bk = (cid:80)∞

i=1 b(k)

i

and

C(m) = 2m−1

K
(cid:88)

k=1

πk (Am

k + Bm

k (cid:107)(cid:15)0(cid:107)m

m) .

Then, the following statements hold:

5

(i) if c := C(1) < 1, then there exists a τ −weakly dependent strictly stationary solution

(Xt)t∈Z of CHARME(∞) which belongs to L1, and such that

τ (r) ≤ 2

µ1
1 − c

inf
1≤s≤r

(cid:32)

cr/s +

1
1 − c

∞
(cid:88)

i=s+1

(cid:33)

ci

−→
r→∞

0,

(3.2)

where µ1 = (cid:80)K
i (cid:107)(cid:15)0(cid:107)1
(ii) if moreover C(m) < 1 for some m > 1, then the stationary solution belongs to Lm.

k)|(cid:107)(cid:15)0(cid:107)1) and ci = (cid:80)K

k=1 πk ((cid:107)fk(0, θ0

k)(cid:107) + |gk(0, θ0

i + b(k)
a(k)

k=1 πk

(cid:16)

(cid:17)

.

Corollary 3.1. Consider the CHARME(p) model (1.2) and suppose that the inequalities
(3.1) hold (in this case a(k)
i = 0 for all i > p and all k ∈ [K]). Under the notations of
Theorem 3.1, if c < 1, then there exists a τ −weakly dependent stationary solution (Xt)t∈Z of
CHARME(p) which belongs to L1 and such that τ (r) ≤ 2µ1(1−c)−1cr/p for r ≥ p. Moreover,
if C(m) < 1 for some m > 1, then this solution belongs to Lm.

i = b(k)

Remark 3.1.

t

t

, . . . , ξ(K)

k) and gk(·, θ0

(1) Consider the assumptions of Theorem 3.1. The Lipschitz-type assumption (3.1)
entails continuity of fk(·, θ0
k), whence we deduce continuity of F as
deﬁned in (8.2).
It then follows from [16, Lemma 5.5] and the completeness of
Lm, that there exits a measurable function H such that the CHARME(∞) process
can be written as Xt = H( ˜ξt, ˜ξt−1, . . .), where ˜ξt := ((cid:15)t, ξ(1)
) = ((cid:15)t, ξt) ∈
E × {e1, . . . , eK}, where e1, . . . , eK are the canonical basis vectors for RK. In other
words, the CHARME(∞) process can be represented as a causal Bernoulli shift.
Moreover, under these assumptions, (Xt)t∈Z is the unique causal Bernoulli shift solu-
tion to (1.2) with p = ∞. Therefore, the solution (Xt)t∈Z is automatically an ergodic
process. Finally, the ergodic theorem implies the SLLN for this process. This con-
sequence of Theorem 3.1 will be a key to establish strong consistency when it comes
to estimating the autoregressive and volatility functions of the CHARME(p) model.
(2) Using the arguments in [16], it can be shown that the stationary solution of CHARME
(∞) can be approximated by a stationary solution of the CHARME(p) model (1.2)
for some large value of p. In fact, the bounds of the weak dependence coeﬃcients
of [16, Theorem 3.1] come from an approximation with Markov chains of order p
along with its weak dependence and stationarity properties (see [16, Corollary 3.1]).
Indeed, let Xt be the stationary solution of the CHARME(∞) model and let Xp,t be
the stationary solution of its associated CHARME(p) model, i.e.,

Xp,t = F (Xp,t−1, . . . , Xp,t−p, 0, 0, . . . ; ˜ξt),
where F is deﬁned in (8.2). Then, [16, Lemma 5.5] gives

E(cid:107)Xt − Xp,t(cid:107) ≤

µ1
(1 − c)2

∞
(cid:88)

ci.

i=p+1

(3.3)

(3.4)

(3) In [51], the authors show that CHARME(p) is geometrically ergodic for p < ∞ consid-
ering the process (Rt)t∈Z as a ﬁrst-order irreducible and aperiodic strictly stationary
Markov chain, together with a list of conditions. In particular, they demand that the
iid random variables (cid:15)t have a continuous density function, positive everywhere. In
contrast, in this paper the innovations are not supposed to be absolutely continuous

6

and our approach can also be applied to discrete state space processes. We refer the
reader to [18, 19, 20, 21, 15, 14, 13].

Additionally, in [51], the geometric ergodicity of ηt = (Xt, ξt), t ∈ Z, has been
shown in order to obtain some mixing conditions of (ηt)t∈Z for deriving asymptotic
stability of the model and, therefore, for formalizing an asymptotic theory for non-
parametric estimation. However, note that, by taking the simple AR(1) model de-
ﬁned in (1.3), we can see that this does not satisfy some the assumptions in [51]. In
fact, the AR(1) process (1.3) is not mixing, see [1]. It turns out that the main re-
strictions of the mixing processes are the regularity conditions required for the noise
process ((cid:15)t)t∈Z. These regularity conditions, however, are not needed within the
framework of τ −dependence. For example, the process (1.3) is τ −weakly dependent
with τ (r) ≤ 2−r(cid:112)1/6; see [12, Application 1].

4. Estimation of CHARME parameters: Consistency
In the sequel, we will denote the space of parameters as the product spaces Θ :=×K

k=1 Θk

and Λ :=×K

k=1 Λk.

t

Let (Xt)−p+1≤t≤n

2 be n + p observations of a strictly stationary solution (Xt)t∈Z of the
model (1.2) (which exists by Theorem 3.1). We assume that the number of states K is
known, and that we have access to observations of the hidden iid variables (cid:0)Rt
(cid:1)
−p+1≤t≤n, or
(cid:1)
equivalently, the variables (cid:0)ξ(k)
−p+1≤t≤n,k∈[K].
Remark 4.1. One may wonder how strong these two assumptions are. In general, a careful
analysis of the model usually provides interpretation for the number of states K in terms
of physical signiﬁcance or economical meaning. As far as the assumption that (cid:0)Rt
−p+1≤t≤n
are observed is concerned, it is rather common in the literature, see, e.g., [54, 51] for special
cases of CHARME. If both K and p still happen to be unknown, one may appeal to BIC-
type model selection criteria to estimate them. Nevertheless, given the additional challenges
that this would be bring to the estimators, we leave it to a future work (including other
extensions of the model such as removing the iid assumption on (cid:0)Rt
(cid:1)
−p+1≤t≤n or considering
K increasing with the number of data).

(cid:1)

Our goal now is to design consistent estimators of the parameters
(θ0, λ0) := (θ0

K, λ0
of the CHARME(p) model (1.2) from observations (cid:0)Xt
(cid:0)ξ(k)

1, . . . , λ0
K)
(cid:1)
−p+1≤t≤n and

1, . . . , θ0

(cid:1)

t

−p+1≤t≤n,k∈[K]. This will be achieved through solving the minimization problem

((cid:98)θn, (cid:98)λn) ∈ Argmin(θ,λ)∈Θ×Λ Qn(θ, λ), where

Qn(θ, λ) :=

1
n

n
(cid:88)

K
(cid:88)

t=1

k=1

t (cid:96)(cid:0)Xt, fk(Xt−1, . . . , Xt−p, θk), gk(Xt−1, . . . , Xt−p, λk)(cid:1).
ξ(k)

(4.1)

Here, (cid:96) : E ×E ×R → R∪{+∞} is some loss function. Typically, (cid:96) would satisfy (cid:96)(u, u, τ ) =
0, ∀τ . Observe that we allow (cid:96) to be extended-real-valued (i.e., possibly taking value +∞).

2With a slight abuse of notations, we use the same symbol for the observations.

7

This will allow to deal equally well with non-classical (and challenging) situations as would
be the case if we wanted to include some information/constraints one might have about
certain parameters and the relationships between them in the estimation process. Handling
extended-real-valued functions when establishing consistency theorems is very challenging
which will necessitate more sophisticated arguments.

It will be convenient to deﬁne the processes

Yt = (Xt−p, Xt−p+1, . . . , Xt) and ξt = (ξ(1)

t

, . . . , ξ(K)

t

),

t ∈ Z.

Observations (Xt)−p+1≤t≤n yield observations (Yt)1≤t≤n. Denote {e1, . . . , eK} be the set of
canonical basis vectors for RK. Let (Ep+1 × {e1, . . . , eK}, E ⊗(p+1) ⊗ Ξ, P ) the (common)
probability space on which the random vectors Yt and ξt are deﬁned. We use the shorthand
notation

h(Yt, ξt, θ, λ) :=

K
(cid:88)

k=1

t (cid:96)(cid:0)Xt, fk(Xt−1, . . . , Xt−p, θk), gk(Xt−1, . . . , Xt−p, λk)(cid:1).
ξ(k)

(4.2)

Consistency will be established under the following assumptions. We will denote rang :=
(cid:83)

k∈[K] gk(Ep, Λk) ⊂ R; i.e., the union of the ranges of the functions gk.
(A.1) E ⊗(p+1) ⊗ Ξ is P -complete, namely, a subset of a null set in E ⊗(p+1) ⊗ Ξ also belongs

to E ⊗(p+1) ⊗ Ξ.

(A.2) For each k ∈ [K], Θk and Λk are Polish spaces, i.e., a complete, separable, metric

spaces.

(A.3) For any k ∈ [K], fk and gk are Carath´eodory mappings, i.e., fk(X1, . . . , Xp, θk) (resp.
gk(X1, . . . , Xp, λk)) is E ⊗p-measurable in (X1, . . . , Xp) for each ﬁxed θk (resp. λk) and
continuous in θk (resp. λk) for each ﬁxed (X1, . . . , Xp).

(A.4) (cid:96) is E ⊗B(E)⊗B(rang)-measurable, and for every u ∈ E, (v, τ ) ∈ E ×rang (cid:55)→ (cid:96)(u, v, τ )

is lower semicontinuous (lsc).

(A.5) inf((cid:96)) ≥ 0.
(A.6) For each k ∈ [K] and t ∈ [n], there exists ¯θk ∈ Θk such that

fk(Xt−1, . . . , Xt−p, ¯θk) = 0.

(A.7) There exist non-negative constants C and c, and γ > 0, such that for all k ∈ [K] and

t ∈ [n],

(cid:96)(cid:0)Xt, 0, gk(Xt−1, . . . , Xt−p, λk)(cid:1) ≤ C(cid:107)Xt(cid:107)γ + c.

inf
λk∈Λk

Before proceeding, some remarks on these assumptions are in order.

Remark 4.2.

1. The completeness assumption (A.1) is harmless and for technical convenience. Stan-

dard techniques can be used to eliminate it.

2. Functions verifying Assumption (A.4) are known as random lsc or normal integrands.
The concept of a random lsc function is due to [44], who introduced it in the con-
text of the calculus of variations under the name of normal integrand. Properties
of random lsc functions are studied [45, Chapter 14]. The proof of our consistency
theorem will rely on stability properties of the family of random lsc functions under

8

various operations, and on their powerful ergodic properties set forth in the series of
papers [35, 34, 33]. Unlike other works on the Law of Large Numbers for random lsc
functions [4, 2, 29], which postulate iid sampling, only stationarity is needed in our
context.

3. Lower-semicontinuity wrt the parameters is a much weaker assumption than those
found in the literature. In addition to allowing to handle constraints on the param-
eters easily (see the discussion after Theorem 4.1), it will also allow for non-smooth
activations maps in NN-based learning such as the very popular ReLU. In fact, even
continuity is not needed in our context whereas diﬀerentiability is an important as-
sumption in existing works; see, e.g., [51, 54].

4. Assumption (A.5) can be weakened to lower-boundedness by a negative combination
of powers (with appropriate exponents) of the norm. We leave the details to the
interested reader.

5. Assumption (A.6) is quite natural and is veriﬁed in most applications we have in

mind (e.g., neural networks).

6. Our proof technique does not really need p to be ﬁnite. Thus our result can be
extended equally well to the CHARME(∞) model by considering the process Yt as
valued in E∞ and assume E ⊗N-measurability in our assumptions.

Example 4.1. A prominent example in applications is where the loss function (cid:96) takes the
form

(cid:96)(u, v, τ ) =

,

γ > 0.

(cid:107)u − v(cid:107)γ
|τ |γ

In view of the role played by τ , it is natural to impose the following assumption on gk:

(Ag) ∃δ > 0 such that ∀k ∈ [K], inf x1,...,xp,λk |gk(x1, . . . , xp, λk)| ≥ δ.
Let us show that (cid:96) complies which assumptions (A.4), (A.5) and (A.7). First, (A.5) is

obviously veriﬁed. As for (A.7), we have from (Ag) that

(cid:96)(cid:0)Xt, 0, gk(Xt−1, . . . , Xt−p, λk)(cid:1) ≤ δ−γ(cid:107)Xt(cid:107)γ,
whence assumption (A.7) holds with C = δ−γ and c = 0. It remains to check (A.4). Since
(Ag) implies that 0 (cid:54)∈ rang = R\]−δ, δ[, continuity of the norm and (Ag) entails that (cid:96), which
is the ratio of continuous functions on Borel spaces, is continuous, hence a Borel function.

We are now in position to state our consistency theorem.

Theorem 4.1. Let (Xt)t∈Z be a strictly stationary ergodic solution of (1.2), which exists
under the assumptions of Theorem 3.1 with C(m) < 1 for some m ≥ 1. Let ((cid:98)θn, (cid:98)λn) the
estimator deﬁned by (4.1), and assume that (A.1)-(A.7) are veriﬁed with γ = m. Then,
the following statements hold:

(i) each cluster point of ((cid:98)θn, (cid:98)λn)n∈N belongs to Argmin(θ,λ)∈Θ×Λ
(ii) if moreover the sequence (Qn)n∈N is equi-coercive, and
Argmin(θ,λ)∈Θ×Λ

Eh(Y, ξ, θ, λ) = {θ0, λ0},

9

Eh(Y, ξ, θ, λ) a.s.

then

((cid:98)θn, (cid:98)λn) → (θ0, λ0) and Qn((cid:98)θn, (cid:98)λn) → Eh(Y, ξ, θ0, λ0) a.s.

Recall that a sequence of functions (φn)n∈N is equi-coercive if there exists a lsc coercive
function ψ such that φn ≥ ψ, ∀n ∈ N, see [10, Deﬁnition 7.6 and Proposition 7.7]. This
entails in particular that the sublevel sets of the functions φn are compact3 uniformly in n.
For instance, a suﬃcient condition to ensure equi-coerciveness in our context is that, for
each k ∈ [K], there exists a B(Θk) ⊗ B(Λk)-measurable compact subset Ck ⊂ Θk × Λk such
that4

dom (cid:0)(cid:96)(cid:0)Xt, fk(Xt−1, . . . , Xt−p, ·), gk(Xt−1, . . . , Xt−p, ·)(cid:1)(cid:1) ⊂ Ck,

∀t ∈ [n].

Indeed, it is immediate to see that such a condition implies that

which is then a compact set.

dom(Qn) ⊂

K×

k=1

Ck,

The sets Ck can be used to impose some prior constraints on the parameters (θk, λk) which
might follow from certain physical, economic or mathematical considerations. For instance,
these parameters can be constrained to comply with the strict stationarity assumption in
Theorem 3.1. Other constraints can be also used to promote some desirable properties
such robustness and generalization for the case of neural networks (see Section 6 for further
discussion). In general, to account for constraints, one sets
(cid:96)(cid:0)Xt, fk(Xt−1, . . . , Xt−p, θk), gk(Xt−1, . . . , Xt−p, λk)(cid:1) =

(cid:101)(cid:96)(cid:0)Xt, fk(Xt−1, . . . , Xt−p, θk), gk(Xt−1, . . . , Xt−p, λk)(cid:1) + ιCk(θk, λk),
where (cid:101)(cid:96) is a full-domain loss verifying (A.4), (A.5) and (A.7), and ιCk is the indicator
function of Ck, taking 0 on Ck and +∞ otherwise. By assumptions on Ck, ιCk is B(Θk) ⊗
B(Λk)-measurable and lsc, and thus (cid:96) inherits (A.4) from (cid:101)(cid:96).
(A.5) is trivially veriﬁed,
and for (A.6) to hold, it is necessary and suﬃcient that for each k ∈ [K] and t ∈ [n],
fk(Xt−1, . . . , Xt−p, ·)−1(0) × Λk ∩ Ck (cid:54)= ∅.

∀k ∈ [K],

We ﬁnally stress that the constraints above do not need to be separable, as soon as one

takes h(Yt, ξt, θ, λ) as

h(Yt, ξt, θ, λ) =

K
(cid:88)

k=1

t (cid:101)(cid:96)(cid:0)Xt, fk(Xt−1, . . . , Xt−p, θk), gk(Xt−1, . . . , Xt−p, λk)(cid:1) + ιC(θ, λ),
ξ(k)

where C ⊂ Θ × Λ is a B(Θ) ⊗ B(Λ)-measurable compact set. Thus, depending on the
application at hand, our reasoning above can be extended to more complicated situations.

3We here specialized [10, Deﬁnition 7.6] to metric spaces (see(A.2)) where compactness implies closeness

and countable compactness.

4Observe that accounting for this constraint does not compromise assumption (A.4) thanks to compact-

ness of Ck.

10

5. Estimation of CHARME parameters: Asymptotic normality

To establish asymptotic normality, we need to restrict ourselves to a ﬁnite-dimensional
framework where E = Rd and Θk = Rdk. Throughout this section, (cid:107) · (cid:107) denotes the stan-
dard Euclidean norm and the corresponding (Euclidean) space is to be understood from the
context.

In this section, we consider the following constant-volatility special case of the model in

(1.2):

Xt =

K
(cid:88)

k=1

ξ(k)
t

(cid:0)fk(Xt−1, . . . , Xt−p, θ0

k) + (cid:15)k,t

(cid:1) ,

t ∈ Z.

(5.1)

We then specialize the estimator in (4.1) to (5.1) and the quadratic loss, which now reads

(cid:40)

(cid:98)θn ∈ Argminθ∈Θ

Qn(θ) :=

1
n

n
(cid:88)

K
(cid:88)

t=1

k=1

ξ(k)
t (cid:107)Xt − fk(Xt−1, . . . , Xt−p, θk)(cid:107)2

.

(5.2)

(cid:41)

This corresponds to the conditional least-squares method. We focus on this simple loss
although our results hereafter can be extended easily, through tedious calculations, to any
loss (cid:96) which is three-times continuously diﬀerentiable wrt its second argument.

For a three-times continuously diﬀerentiable mapping h : ν ∈ Rdk (cid:55)→ h(ν) ∈ Rd, we
will denote ∂h/∂νi(µ) ∈ Rd the derivative of h wrt to the i-th entry of ν evaluated at
µ ∈ Rdk, and J[h](µ) = (∂h/∂ν1(µ) . . . ∂h/∂νdk(µ)) the Jacobian of h. Similarly the second
and third order (mixed) derivatives are denoted as ∂2h/(∂νi∂νj)(µ) and ∂3h/(∂νi∂νj∂νl)(µ),
respectively. For a diﬀerentiable scalar-valued function on an Euclidean space, ∇ will denote
its gradient operator (the vector of its partial derivatives).

From Example 4.1, Theorem 4.1 applies, hence showing consistency of the estimator (5.2).
On the other hand, to establish asymptotic normality of this estimator, we will invoke [55,
Theorem 3.2.23 or 3.2.24] (which are in turn due to [32]). This requires to impose the
following more stringent regularity assumptions:

(B.1) For each k ∈ [K], the function θk ∈ Θk (cid:55)→ fk(Xp, . . . , X1, θk) is three-times continu-
ously diﬀerentiable almost everywhere in an open neighborhood V of θ0 = (θ0
K).

1, . . . , θ0

(B.2) For all k ∈ [K] and all i, j ∈ [dk],

E

(cid:13)
(cid:13)
(cid:13)
(cid:13)

∂fk(Xp, . . . , X1, θ0
k)
∂θk,i

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)

< ∞ and E

(cid:13)
(cid:13)
(cid:13)
(cid:13)

∂2fk(Xp, . . . , X1, θ0
k)
∂θk,j∂θk,i

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)

< ∞.

(B.3) The vectors {∂fk(Xp, . . . , X1, θ0

k)/∂θk,i}i∈[dk],k∈[K], are linearly independent in the

sense that if (ak,i)i∈dk,k∈[K] are arbitrary real numbers such that

E

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

K
(cid:88)

dk(cid:88)

k=1

i=1

ak,i

∂fk(Xp, . . . , X1, θ0
k)
∂θk,i

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

= 0,

then ak,i = 0 for all i ∈ [dk] and all k ∈ [K].

11

(B.4) For k ∈ [K] and i, j, r ∈ [dk]

Gijr
k

:= E

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:18) ∂fk(Xp, . . . , X1, θ0
k)
∂θk,i

(cid:19)(cid:62) ∂2fk(Xp, . . . , X1, θ0
k)

∂θk,j∂θk,r

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

< ∞

and

H ijr
k

:= E

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:0)Xp+1 − fk(Xp, . . . , X1, θ0

k)(cid:1)(cid:62) ∂3fk(Xp, . . . , X1, θ0
k)

∂θk,i∂θk,j∂θk,r

(cid:12)
(cid:12)
(cid:12)
(cid:12)

< ∞.

(B.5) For all k ∈ [K] and all i, j ∈ [dk], |Wk,ij| < ∞, where

(cid:34)
Wk,ij = E

ξ(k)
t

(cid:0)Xp+1 − fk(Xp, . . . , X1, θ0

k)(cid:1)(cid:62) ∂fk(Xp, . . . , X1, θ0
k)

∂θk,i

· (cid:0)Xp+1 − fk(Xp, . . . , X1, θ0

k)(cid:1)(cid:62) ∂fk(Xp, . . . , X1, θ0
k)

∂θk,j

(cid:35)

.

Let us denote by W = (Wkl)1≤k,l≤K the block–diagonal matrix deﬁned by the sub-
matrices




Wkl =

0dk×dl

if k (cid:54)= l

(5.3)



(Wk,ij)1≤i,j,≤dk

if k = l.

We are now in shape to formalize our asymptotic normality result.
Theorem 5.1. Let (Xt)t∈Z be a strictly stationary ergodic solution of (5.1) with E(cid:107)Xt(cid:107)2 <
∞, which exists under the assumptions of Theorem 3.1 with C(m) < 1 for m = 2. Suppose
that (B.1)-(B.5) hold. Then there exists a sequence of estimators (cid:98)θn such that

(cid:98)θn → θ0

a.s.

and for any ε > 0, there exists N large enough and an event with probability at least 1 − ε on
which, for all n > N , ∇Qn((cid:98)θn) = 0, and Qn attains a relative minimum at (cid:98)θn. Furthermore,

√

(cid:16)

(cid:98)θn − θ0(cid:17) D−→ N (cid:0)0, V −1W V −1(cid:1) ,

n

as n → ∞, where V = (Vkl)1≤k,l≤K is the block-diagonal matrix deﬁned by the sub-matrices

Vkl =

(cid:40)0dk×dl
(cid:104)
πk E
(J[fk(Xp, . . . , X1, ·)](θ0

(cid:105)
k))(cid:62) J[fk(Xp, . . . , X1, ·)](θ0
k)

if k (cid:54)= l

if k = l.

(5.4)

Observe that the covariance matrix V −1W V −1 is also block-diagonal with diagonal blocks
kk WkkV −1
V −1
kk .

6. Learning CHARME models with Neural Networks
In this section, we apply our results to the case where E = Rd and each of the functions
fk and gk in the CHARME(p) model (1.2) is exactly modeled by a feedforward neural
network (see Section 2.2). More precisely, given an activation map ϕ, and for each k ∈ [K],
fk and gk are feedforward neural networks according to Deﬁnition 2.2, parameterized by
12

(cid:16)

k ), . . . , (W (Lk)

k

, b(Lk)
k

(cid:17)
)

and λk =

weights and biases given respectively by θk =
(cid:16)
( ¯W (1)

k , b(1)
. For each k ∈ [K], we have:

k , ¯b(1)
• for each layer l ∈ [Lk] of the k-th NN modeling fk, W (l)

k ), . . . , ( ¯W ( ¯Lk)

, ¯b( ¯Lk)

(W (1)

(cid:17)
)

k

k

k = (β(l)
b(l)

k,1, . . . , β(l)

k,Nk,l

k = (w(l)

k,ij)(i,j)∈[Nk,l]×[Nk,l−1] and
)(cid:62) are respectively the matrix of weights and vector of biases;
k,ij)(i,j)∈[ ¯Nk,l]×[ ¯Nk,l−1] and
)(cid:62) are respectively the matrix of weights and vector of biases;

k = ( ¯w(l)

• for each layer l ∈ [ ¯Lk] of the k-th NN modeling gk, ¯W (l)

k = ( ¯β(l)
¯b(l)

k,1, . . . , ¯β(l)

k, ¯Nk,l
• Nk,0 = ¯Nk,0 = d · p, Nk,L = d and ¯Nk,L = 1.

We throughout make the standard assumption that the activation map ϕ is Lipschitz

continuous5.

6.1. Ergodicity and stationarity. Considering the notations of Theorem 3.1, let x(cid:62) =
(x1, . . . , xdp) ∈ Rdp and y(cid:62) = (y1, . . . , ydp) ∈ Rdp. Split the matrix W (1)
into p column blocks
(cid:16)
W (1)

. It is easy to see that

k,i ∈ RNk,1×d such that W (1)

k,2 . . . W (1)

k,1 W (1)

k =

W (1)

k,p

(cid:17)

k

(cid:107)fk(x, θk) − fk(y, θk)(cid:107)
(W (Lk)
k

≤ Lip(ϕ)Lip

(cid:16)

· −b(Lk)
k

) ◦ · · · ◦ ϕ ◦ (W (2)

k

· −b(2)
k )

= Lip(ϕ)Lip

(cid:16)

(W (Lk)
k

· −b(Lk)
k

) ◦ · · · ◦ ϕ ◦ (W (2)

k

· −b(2)
k )

W (1)

k,i (xi − yi)

(cid:13)
(cid:13)
k (x − y)
(cid:13)

(cid:17)

(cid:17) (cid:13)
(cid:13)W (1)
(cid:13)
(cid:13)
p
(cid:13)
(cid:88)
(cid:13)
(cid:13)
(cid:13)
(cid:17) p
(cid:88)

i=1

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

≤ Lip(ϕ)Lip

(cid:16)

(W (Lk)
k

· −b(Lk)
k

) ◦ · · · ◦ ϕ ◦ (W (2)

k

· −b(2)
k )

(cid:12)
(cid:12)
(cid:12)
(cid:12)W (1)
(cid:12)
(cid:12)

k,i

(cid:12)
(cid:12)
(cid:12)
(cid:12)(cid:107)xi − yi(cid:107),
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12) · (cid:12)
(cid:12)
(cid:12)
where (cid:12)
(cid:12) stands for the spectral norm. Similarly, we have
(cid:12)
(cid:12)
(cid:12)
(cid:12)

i=1

|gk(x, λk) − gk(y, λk)|

≤ Lip(ϕ)Lip

(cid:16)

( ¯W ( ¯Lk)

k

· −¯b( ¯Lk)

k

) ◦ · · · ◦ ϕ ◦ ( ¯W (2)

k

· −¯b(2)
k )

(cid:17) p
(cid:88)

i=1

(cid:12)
(cid:12)
(cid:12)
(cid:12) ¯W (1)
(cid:12)
(cid:12)

k,i

(cid:12)
(cid:12)
(cid:12)
(cid:12)(cid:107)xi − yi(cid:107).
(cid:12)
(cid:12)

Identifying with (3.1), we may take the above bounds as estimates for Ak and Bk, i.e.,

Ak = Lip(ϕ)Lip

(cid:16)

(W (Lk)
k

· −b(Lk)
k

) ◦ · · · ◦ ϕ ◦ (W (2)

k

(cid:17) p
(cid:88)

· −b(2)
k )

(cid:12)
(cid:12)
(cid:12)
(cid:12)W (1)
(cid:12)
(cid:12)

k,i

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

i=1

and Bk = Lip(ϕ)Lip

(cid:16)

( ¯W ( ¯Lk)

k

· −¯b( ¯Lk)

k

) ◦ · · · ◦ ϕ ◦ ( ¯W (2)

k

(cid:17) p
(cid:88)

· −¯b(2)
k )

(cid:12)
(cid:12)
(cid:12)
(cid:12) ¯W (1)
(cid:12)
(cid:12)

k,i

(cid:12)
(cid:12)
(cid:12)
(cid:12).
(cid:12)
(cid:12)

(6.1)

i=1

Therefore, if C(m) = 2m−1 (cid:80)K
m) < 1 for some m ≥ 1, there exists a
stationary solution of the NN-based CHARME(p) model such that the coeﬃcient τ (r) ≤
M (C(1))r/p for r > p and some M > 0.

k=1 πk (Am

k + Bm

k (cid:107)(cid:15)0(cid:107)m

5Actually the Lipschitz constant is even 1 in general, e.g., ReLU, Leaky ReLU, SoftPlus, Tanh, Sigmoid,

ArcTan or Softsign.

13

Remark 6.1. The expression of C(m) and the corresponding condition C(m) < 1 is the crux
of the stability of our model. Thus, checking this condition in practice, as for the case of
neural networks with Ak and Bk given by (6.1), is key. This in turn relies on having a good
estimate of the Lipschitz constant of the neural network6 which is captured in the ﬁrst part
of these expressions. It is is known however that computing exactly this Lipschitz constant,
even for two layer neural networks, is a NP-hard problem [49, Theorem 2].

A simple upper-bound is given in [53], i.e.,

(cid:16)

Lip

(W (Lk)
k

· −b(Lk)
k

) ◦ · · · ◦ ϕ ◦ (W (2)

k

(cid:17)
· −b(2)
k )

≤ Lip(ϕ)Lk−1

Lk(cid:89)

l=2

(cid:12)
(cid:12)
(cid:12)
(cid:12)W (l)
(cid:12)
(cid:12)

k

(cid:12)
(cid:12)
(cid:12)
(cid:12),
(cid:12)
(cid:12)

(6.2)

and this bound can be computed eﬃciently with a forward pass on the computational graph.
However, the bound (6.2) depends exponentially on the numbers of layers, Lk, and can
provide very pessimistic estimates with a gap in the upper-bound that is in general oﬀ by
factors or orders of magnitude especially as Lk increases; see the discussion in [49]. In turn,
such a crude bound may harm the condition C(m) < 1 when Lk becomes large. This gap
can be explained by the fact that for diﬀerentiable activations, with the chain rule7, the
equality in (6.2) can only be attained if the activation Jacobian at each layer maps the left
singular vectors of W (l)
. But these Jacobians being
k
diagonal, this is unlikely to happen causing misaligned singular vectors. Starting from this
observation, and using Rademacher’s theorem together with the chain rule for diﬀerentiable
activation maps, a much better bound is proposed in [49, Theorem 3]. This computaional
burden to get this bound lies in computing the SVD of the weight matrices and solving a
maximization problem in each layer. The latter is itelf given an explicit estimate for large
number of neurones in [49, Lemma 2].

to the right singular vectors of W (l+1)

k

6.2. Learning guarantees.

6.2.1. Consistency. To invoke the consistency result of Theorem 4.1, we need to check that fk
and gk verify the corresponding assumptions. Obviously, the Euclidean spaces of parameters
Θk and Λk obey (A.2). As for (A.3), it is also fulﬁlled thanks to obvious continuity properties
of NN functions, deﬁned as composition of aﬃne and Lipschitz continuous mappings. (A.6)
is obviously veriﬁed, for instance, by zeroing both the weight matrix and bias vector at any
same layer (a fortiori, this is true for θk = 0). When the volatility functions gk are not
(non-zero) constant, we need to ensure that (Ag) is veriﬁed, which will in turn guarantee
that (A.7) holds when the loss is as in Example 4.1. For this, if ϕ is positive-valued (as for
the ReLU), then it would be suﬃcient to impose that for any k ∈ [K], the weights ¯W ( ¯Lk)
of
the last layer are non-negative and the bias ¯b( ¯Lk)

k ≥ δ for some δ > 0.

Thus, since there exists a stationary solution of the NN-based CHARME(p) under the
condition of the previous section, the statement of Theorem 4.1(i) applies to the estimator
(4.1) of the NN parameters.

k

To be able to apply Theorem 4.1(ii), we need some equi-coerciveness and uniqueness
of the true parameters (θ0, λ0). First, it is important to note that neural networks are

6Excluding the ﬁrst layer.
7The reasoning is only valid for diﬀerentiable activation maps unlike what is done in [49], and thus excludes

the ReLU; see [6] for a thorough justiﬁcation on the chain rule for neural networks.

14

often non-identiﬁable models, which means that diﬀerent parameters can represent the same
function, or equivalently, fk(·, θk) = fk(·, θ(cid:48)
k. In fact there are invariances in
the NN parametrization which induce ambiguities in the solutions of the estimation problem
(4.1). Clearly, this is a non-convex problem which may not have a global minimizer, not to
mention uniqueness of the latter, even with the population risk Eh(Y, ξ, ·, ·) if the weights
and biases are allowed to vary freely over the parameters space 8. Clearly, there is a need
to appropriately constraining the weights and biases to get the neessary compactness in our
case.

k) (cid:54)⇒ θk = θ(cid:48)

While there is empirical evidence that suggests that when the size of the network is large
enough and ReLU non-linearities are used all local minima could be global, there is currently
no complete rigorous theory that provides a precise mathematical explanation for these
observed phenomena. This is the subject of intense research activity which goes beyond the
scope of this paper; see the review paper [57]. A few suﬃcient deterministic conditions for the
existence of global minimizers of (4.1)9 can be found in [24, 63]. In [24], it is shown that for
certain network architectures with positively homogeneous activations and regularizations,
any sparse local minimizer is a global one. The work in [63] deals with general architectures
but with smooth activations but no regularization, and delivers conditions under which any
critical point is a global minimizer.

Regularizing a neural network by constraining its Lipschitz constant has been proven an
eﬀective and successful way to ensure good stability and generalization properties, see, e.g.,
[5, 8, 42, 43, 39, 59, 62].
In our context, from Section 6.1, this amounts to imposing a
constraint of the form
(W (Lk)
k

) ◦ · · · ◦ ϕ ◦ (W (2)

k ) ◦ ϕ ◦ (W (1)

(cid:17)
· −b(1)
k )

· −b(Lk)
k

· −b(2)

≤ L,

Lip

(cid:16)

k

k

where L > 0. As discussed in Remark 6.1, even computing this bound is hard not to mention
a constraint based on it. Many authors, e.g., [62, 42] and others, use the simplest strategy
(cid:12)
(cid:12)
(cid:12)
that consists in constraining each layer of the network to be Lipschitz, i.e., (cid:12)
(cid:12)
(cid:12)
(cid:12)W (l)
(cid:12) ≤ L1/Lk,
(cid:12)
(cid:12)
(cid:12)
(cid:12)
where we used the bound (6.2) and that the activation maps are also 1-Lipschitz. In [43],
the authors imposed an even cruder bound by constraining group norms of the weights. All
these bounds deﬁne a compact constraint, whose radius L can be chosen such that it satisﬁes
C(m) < 1 for m ≥ 1 known.

k

To summarize, if (4.1) is solved with (cid:96) and compact constraint sets Ck (with appropriate
diameter), or more generally any lsc coercive regularizers, see the discussion after Theo-
rem 4.1, then equi-coerciveness holds true. If uniqueness is assumed (see discussion above),
then Theorem 4.1(ii) yields that the estimator (4.1) of the NN parameters is (strongly)
consistent.

6.2.2. Asymptotic normality. We now turn to asymptotic normality of the estimator (5.2) for
the CHARME(p) model (5.1), where fk is neutral network. We need to check the assumptions
of Theorem 5.1. For this, we assume in this section that the activation map of the NN is

8This is the case for rescaling when the activation is positively homogeneous, in which case multiplying
one layer of a global minimizer by a positive constant and dividing another layer by the same constant
produces a pair of diﬀerent global minimizers

9More precisely, in all the works cited here, their framework amounts to considering gk as a constant and

(cid:96) as quadratic in our setting.

15

three-times continuously diﬀerentiable with bounded derivatives (this is the case for softplus,
smoothed ReLU, sigmoid, etc.). In turn, this will entail that ϕ is Lipschitz continuous, and
that, for all k ∈ [K], θk (cid:55)→ fk(Xp, . . . , X1, θk) is almost surely three-times continuously
diﬀerentiable at any θk ∈ Θk, i.e., (B.2) holds.

Let us now check our assumptions. In view of the derivatives of fk in (A.1) (see Section A),

boundedness of the derivatives of ϕ and stationarity, it is not diﬃcult to check that

E

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

E

E

∂fk(Xp, . . . , X1, θ0
k)
∂w(l)
k,ij

∂w(l)

∂2fk(Xp, . . . , X1, θ0
k)
k,ij∂w(l)
∂3fk(Xp, . . . , X1, θ0
k)
∂w(l)

k,i(cid:48)j(cid:48)∂w(l)

k,ij∂w(l)

k,i(cid:48)(cid:48)j(cid:48)(cid:48)

k,i(cid:48)j(cid:48)

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

= O(E (cid:107)Xt(cid:107)2),

= O( max
s∈{2,4}

(E (cid:107)Xt(cid:107)s)),

= O( max

s∈{2,4,6}

(E (cid:107)Xt(cid:107)s)).

Thus,

ϕ.
k=1 πkAm

The derivatives wrt biases β(l)
k,i as given in (A.2) are bounded in view of boundedness of the de-
with
Theorem
of
rivative
C(m) = 2m−1 (cid:80)K
k < 1, for m ∈ {2, 4, 6}, then maxs∈{2,4,6}(E (cid:107)Xt(cid:107)s) < ∞, whence
conditions (B.2), (B.4), and (B.5) hold. As far as assumption (B.3) is concerned, it cap-
tures the fact that θ0 is a strict local minimizer of (5.2), which is in turn closely related
to our discussion on uniqueness in the previous section. Assuming that it holds, we are in
position to invoke Theorem 5.1 to prove asymptotic normality of the estimator (5.2) of the
NN-parameters of the CHARME(p) model (5.1).

holds

3.1

if

6.3. Approximation vs exact modeling by neural networks. Until now, we have as-
sumed that the autoregressive and volatility functions fk are gk are exactly modeled by
feedforward NNs with ﬁnitely many neurons. A natural question we ask is: what are the
consequences if the NN architecture (depth and width) is such that it provides only ε-
approximations to fk and gk ?

To settle this question, let Xt be the CHARME process given in (1.2), and (cid:101)Xt be the
CHARME process deﬁned by the same innovations and hidden process (Rt)t∈Z but with
functions (cid:101)fk and (cid:101)gk, i.e.,

(cid:101)Xt =

(cid:16)

ξ(k)
t

K
(cid:88)

k=1

(cid:101)fk( (cid:101)Xt−1, . . . , (cid:101)Xt−p, (cid:101)θk) + (cid:101)gk( (cid:101)Xt−1, . . . , (cid:101)Xt−p, (cid:101)λk)(cid:15)t

(cid:17)

,

t ∈ Z.

(6.3)

The functions (cid:101)fk and (cid:101)gk are supposed to be two neural networks providing approximations
to fk and gk. Denote the approximation accuracy as

εk :=

sup
(x1,...,xp)∈Ep

(cid:16)

(cid:107) (cid:101)fk(x1, . . . , xp, (cid:101)θk) − fk(x1, . . . , xp, θ0

k)(cid:107),

|(cid:101)gk(x1, . . . , xp, (cid:101)λk) − gk(x1, . . . , xp, λ0
k)|

(cid:17)

.

(6.4)

16

To compare the two processes, it is natural to assume that the functions ( (cid:101)fk, (cid:101)gk)k∈N verify
the assumptions of Theorem 3.1 so that (cid:0)
(cid:1)
t∈Z is a strictly stationary solution of (6.3).
(cid:101)Xt
Thus, ∀t ∈ Z, we have
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:16)
(cid:101)gk( (cid:101)Xt−1, . . . , (cid:101)Xt−p, (cid:101)λk) − gk( (cid:101)Xt−1, . . . , (cid:101)Xt−p, θ0
k)
(cid:17)
k) − fk(Xt−1, . . . , Xt−p, θ0
fk( (cid:101)Xt−1, . . . , (cid:101)Xt−p, θ0
k)

(cid:101)fk( (cid:101)Xt−1, . . . , (cid:101)Xt−p, (cid:101)θk) − fk( (cid:101)Xt−1, . . . , (cid:101)Xt−p, θ0
k)

(cid:107) (cid:101)Xt − Xt(cid:107) =

K
(cid:88)

ξ(k)
t

k=1

(cid:32)

+

+

(cid:15)t

(cid:16)

(cid:16)

(cid:17)

(cid:17)

(cid:16)

+

gk( (cid:101)Xt−1, . . . , (cid:101)Xt−p, θ0

k) − gk(Xt−1, . . . , Xt−p, θ0
k)

(cid:17)

(cid:33)(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:15)t

≤

K
(cid:88)

k=1

ξ(k)
t

(cid:32)

εk(1 + (cid:107)(cid:15)t(cid:107)) +

p
(cid:88)

(cid:16)

i=1

i + b(k)
a(k)

i (cid:107)(cid:15)t(cid:107)

(cid:17)

(cid:33)

(cid:107) (cid:101)Xt−i − Xt−i(cid:107)

.

Taking expectations in both sides and thanks to stationarity of both processes, and by
assumptions on (cid:15)t and ξ(k)

, we get

t

E(cid:107) (cid:101)Xt − Xt(cid:107) ≤ (1 + E(cid:107)(cid:15)0(cid:107))

≤ (1 + E(cid:107)(cid:15)0(cid:107))

≤ (1 + E(cid:107)(cid:15)0(cid:107))

≤ (1 + E(cid:107)(cid:15)0(cid:107))

K
(cid:88)

k=1

K
(cid:88)

k=1

K
(cid:88)

k=1

K
(cid:88)

k=1

πkεk + E(cid:107) (cid:101)Xt − Xt(cid:107)

πkεk + E(cid:107) (cid:101)Xt − Xt(cid:107)

πkεk + E(cid:107) (cid:101)Xt − Xt(cid:107)

(cid:32) K
(cid:88)

k=1
(cid:32) K
(cid:88)

k=1
(cid:32) K
(cid:88)

k=1

πk (Ak + BkE(cid:107)(cid:15)0(cid:107))

(cid:33)

πk (Ak + BkE(cid:107)(cid:15)0(cid:107))m

(cid:33)1/m

(cid:33)1/m

πk2m−1 (Am

k + Bm

k (cid:107)(cid:15)0(cid:107)m
m)

πkεk + E(cid:107) (cid:101)Xt − Xt(cid:107)C(m)1/m,

where we have used that m ≥ 1 in the second line and Jensen’s inequality in the third. Since
by assumption C(m) < 1 for some m ≥ 1, see Theorem 3.1, we get that

E(cid:107) (cid:101)Xt − Xt(cid:107) ≤

(1 + E(cid:107)(cid:15)0(cid:107)) (cid:80)K

k=1 πkεk

1 − C(m)1/m

.

(6.5)

In a nutshell, this inequality highlights the fact that, as expected, the mean error between
(cid:101)Xt and the true process Xt is within a factor of the average approximation accuracy of fk
and gk. This bound also casts a new light on the role of C(m), and the smaller, the better.
Notice also that if ¯Xt is the stationary solution of the CHARME(∞) model ((1.2), for
p = ∞) and Xt is the stationary solution of its associated CHARME(p) model (deﬁned in
(3.3)), we can then approximate this solution by (cid:101)Xt, for some large integer value of p and εk
small enough for all k ∈ [K]. Precisely, we would get that

E(cid:107) ¯Xt − (cid:101)Xt(cid:107) ≤ E(cid:107) ¯Xt − Xt(cid:107) + E(cid:107)Xt − (cid:101)Xt(cid:107) ≤ (3.4) + (6.5) −→ 0,
17

as εk → 0 for all k ∈ [K] and p → ∞. This justiﬁes that one could learn inﬁnity memory
CHARME models with neural networks, by approximating them by a CHARME(p) for p
ﬁnite but suﬃciently large. Of course, strictly speaking, learning a CHARME(∞) would
necessitate inﬁnitely many observations.

7. Numerical experiments

In order to assess numerically the performance (consistency and asymptotic normality) of
our estimator and support our theoretical predictions, we here report some numerical ex-
periments. The CHARME(p) models in (5.1) were generated in two scenarios: (i) when the
autoregressive functions fk are generated by feedforward NNs, in which case the functions fk
are exacly modeled by neural networks; and (ii) when they are not, that is a neural network
may provide only an εk-approximations to each function fk. In all cases, we parametrize
the functions fk with feedforward NNs, and we train the NNs by minimizing (5.2) to esti-
mate the corresponding weights and biases θk. The estimation/training step is accomplished
using stochastic (sub)gradient descent (SGD). For smooth activation maps, the gradient is
computed via the chain rule through reverse mode automatic diﬀerentiation (i.e., backprop-
agation algorithm); see [23]. For non-smooth activations such as the ReLU, we invoke the
theory of conservative ﬁelds and deﬁnability proposed recently in [7] to justify our use of the
non-smooth chain rule and automatic diﬀerentiation.

All experiments were conducted under R with an interface to Keras 2.2.5 [17]. R notebooks
that allow to reproduce our experiments are publicly available for download at https://
github.com/jose3g/Learning_CHARME_models_with_DNN.git.

1) = (N1,0, . . . , N1,5) = (30, 50, 60, 40, 20, 1), #neu(θ0

Experiment 1 (Learning NNs from NN-based CHARME data). We simulate a NN-based
CHARME(p) model as in (5.1) with K = 3 and p = 30, where fk = fk(·, θ0
k), k = 1, 2, 3,
are neural networks with #neu(θ0
2) =
(N2,0, . . . , N2,3) = (30, 20, 5, 1) and #neu(θ0
3) = (N3,0, . . . , N3,3) = (30, 25, 30, 1), all with a
ReLU activation function. We have taken the weights w(l)
k,ij arbitrarily (randomly uniform
over a small interval [−δ, δ]) and (π1, π2, π3) = (0.1, 0.4, 0.5) such that C(1) < 1 (the ex-
plicit expression is provided in (6.1)) in order to guarantee the stationarity of the model.
k,1, . . . , β(l)
Precisely, C(1) = 0.8480806 for this model. The biases b(l)
)(cid:62) are also
taken arbitrarily but in R and we have set particularly (b(5)
3 ) = (1, 0, −1). Then,
from this model and with innovations (cid:15)t ∼ N (0, 1), we have generated a dataset of n = 105
observations.

k = (β(l)
2 , b(3)
1 , b(3)

k,Nk,l

Let us turn to the estimation/training step. For this, we consider the quadratic loss func-
tion deﬁned in (5.2) with the same conﬁgurations of the model that generates the data, that
is, with K = 3, (π1, π2, π3) = (0.1, 0.4, 0.5) and fk such that #neu(θ1) = (30, 50, 60, 40, 20, 1),
#neu(θ2) = (30, 20, 5, 1) and #neu(θ3) = (30, 25, 30, 1), and the ReLU activation function.
We run 103 iterations of the SGD algorithm with learning rate/step-size 0.001 which decays
at the rate of 0.5. Let (cid:98)θ∗
n,3) be the parameters obtained in the last iteration.

n = ((cid:98)θ∗

n,2, (cid:98)θ∗

n,1, (cid:98)θ∗

18

Figure 1. Histograms for the estimated errors (cid:98)(cid:15)t of Experiment 1 (left) and
Experiment 2 (right).

On the left side in Figure 1, we show the histogram of the errors (cid:98)(cid:15)t = Xt − (cid:98)Xt, where

(cid:98)Xt =

K
(cid:88)

k=1

ξ(k)
t fk(Xt−1, . . . , Xt−p, (cid:98)θ∗

n,k).

(7.1)

The Gaussian probability density function (pdf) with mean and variance equal to the em-
pirical mean and variance of (cid:98)(cid:15)t is also displayed in a blue solid line.
Experiment 2 (Learning NNs from non NN-based CHARME data). In this experiment we
simulate a CHARME(5) model as follows:
Xt = (cid:15)t+(Xt−1 + 3)I{Rt=1}

(cid:113)

0.2X 2

t−1 + 0.1X 2

+(
+(0.05Xt−1 + 0.2Xt−2 + 0.15Xt−3 + 0.03Xt−4 + 0.01Xt−5 + 0.1)I{Rt=3}

t−4 + 0.05X 2

t−2 + 0.25X 2

t−3 + 0.2X 2

t−5 − 3)I{Rt=2}

with (π1, π2, π3) = (0.15, 0.35, 0.5). Note that the ﬁrst autoregressive process X (1)
t = Xt−1 +
3 + (cid:15)t is not stationary, although the entire process is stationary (because C(1) < 1). By
taking (cid:15)t ∼ N (0, 1), we generate again a dataset of n = 105.

For the estimation/training procedure, we consider also the quadratic loss function (5.2)
with three NNs fk(·, θk), k = 1, 2, 3, such that #neu(θ1) = (5, 300, 400, 200, 1), #neu(θ2) =
(5, 500, 600, 400, 1) and #neu(θ3) = (5, 300, 400, 200, 1), all with a ReLU activation map.
We run 2000 iterations of the SGD algorithm with learning rate/ste-size 0.01 which decays
at the rate 10−6. Let (cid:98)θ∗
n,3) be the parameters obtained in the last iteration.

n = ((cid:98)θ∗

n,1, (cid:98)θ∗

n,2, (cid:98)θ∗

Similarly to Experiment 1, we show on the right side of Figure 1 the histogram of the
errors (cid:98)(cid:15)t = Xt − ˆXt, where (cid:98)Xt is as given in (7.1). The Gaussian pdf with mean and variance
equal to the empirical mean and variance of (cid:98)(cid:15)t is also displayed in a blue solid line.

19

Figure 2. Boxplots of 100 coordinates of ηn.

1) = (16, 32, 64, 32, 1), #neu(θ0

Experiment 3 (Asymptotic normality of trained NNs parameters). We set a CHARME(p)
model as in (5.1) with K = 3 and p = 16, where fk = fk(·, θ0
k), k = 1, 2, 3, are NNs with
#neu(θ0
3) = (16, 32, 64, 1), all with
sigmoid activation function (this is because for the CLT result of Theorem 5.1 to apply, the
activation function must be three-times continuously diﬀerentiable). Of course, the weights
generated satisfy the condition C(1) < 1. In particular, C(1) = 0.9743731 for the weights
generated in this model.

2) = (16, 64, 32, 1), #neu(θ0

We now perform the following steps N = 125 times:

(i) By taking normal standard innovations with the aforementioned model, we generate

a dataset of n = 2 · 104,

(ii) By considering the quadratic loss function (5.2) with the same conﬁgurations of the
model that generates the data and the sigmoid activation function, we run 2000
iterations of the SGD algorithm with learning rate/step-size 0.01 and decay rate
10−6, in order to obtain an approximation (cid:98)θ∗

n of (cid:98)θn.

n(t), t = 1, . . . , 125, be the estimates10 obtained in each step of the Monte Carlo
Let (cid:98)θ∗
simulation and let ηn(t) :=
, t = 1, . . . , 125. On can easily check that the
number of parameters to learn is 10691, i.e., θ0 ∈ R10691, and in turn each ηn(t) is a vector
in R10691.

n(t) − θ0(cid:17)
(cid:98)θ∗

√

(cid:16)

n

Figure 2 shows the box-plots of the coordinates of ηn. For the sake of readability, we only

show 100 arbitrarily selected coordinates.

10These are really the SGD-approximations of the conditional least-squares estimates.

20

147101316192225283134374043464952555861646770737679828588919497−200−1000100200100 arbitrarily chosen parametersTable 1. Multivariate normality test results.

Test
Mardia

Test Statistic

p-value

Skewness
Kurtosis
Henze-Zirkler
Royston

687.5626
-0.4461589
0.9931136
22.35297

0.4120106
0.6554825
0.7983517
0.0987472

Figure 3. Chi-Square Q-Q plot: empirical quantiles of squared Mahalanobis
distance from ηn|B to (cid:126)0 vs chi-square quantiles.

To test normality of ηn, as predicted by Theorem 5.1, we apply three multivariate normality
tests: Mardia, Henze-Zirkler and Royston test (for the details of these tests, see [28, 40, 48,
36]). Given that the dimension of ηn(t) is quite large (anyway larger than N = 125), to avoid
numerical instabilities due to matrix inversion, these tests were not applied to the entire
set of coordinates of ηn(t), but to an arbitrary subset of 15 parameters (i.e., 15 arbitrary
coordinates of ηn that we will call ηn|B, where B ⊂ [10691]), which yield the results shown
in Table 1.

We also report the Chi-Square Q-Q plot for Squared Mahalanobis Distance from ηn|B to
0 on Figure 3. We can see that the Q-Q plot is, in fact, almost along the straight line.
Therefore, observing this behavior and the p-values obtained in the three tests of normality
on Table 1, we can conclude that the vector ηn|B has indeed the predicted Gaussian behavior.

21

5101520253051015202530Chi−Square Q−Q PlotSquared Mahalanobis DistanceChi−Square Quantile8.1. Proof of Theorem 3.1.

8. Proofs

(i) Note that the CHARME(∞) model deﬁned in (1.2) with p = ∞, can be written as

a Markov process:

Xt = F (Xt−1, Xt−2, . . . ; ˜ξt),

t ∈ Z,

by taking the function

F (x; (ξ(0), . . . , ξ(K))) =

K
(cid:88)

k=1

ξ(k) (cid:0)fk(x, θ0

k) + gk(x, λ0

k)ξ(0)(cid:1)

(8.1)

(8.2)

with innovations ˜ξt := ((cid:15)t, ξ(1)
) = ((cid:15)t, ξt) ∈ E × {e1, . . . , eK}. Therefore, ver-
ifying [16, Conditions (3.1) and (3.3)], we will obtain the result by [16, Theorem 3.1].
Note that Condition (3.2) of that paper is already assumed.

, . . . , ξ(K)

t

t

Indeed, since the sequences ((cid:15)t)t∈Z and (Qt)t∈Z are independent and ξ0 ∈ {e1, . . . , eK},

denoting E(cid:15) the expectation with respect to the distribution of (cid:15), we obtain for
x = (x1, x2, . . .) and y = (y1, y2, . . .), that

(cid:107)F (x; ˜ξ0) − F (y; ˜ξ0)(cid:107)1 = E

(cid:104)

(cid:105)
(cid:107)F (x; ˜ξ0) − F (y; ˜ξ0)(cid:107)

=E

K
(cid:88)

(cid:34)(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:34) K
(cid:88)

k=1

=E(cid:15)

j=1
(cid:34) K
(cid:88)

k=1

=E(cid:15)

ξ(k)
0

(cid:0)fk(x, θ0

k) − fk(y, θ0

k) + (gk(x, λ0

k) − gk(y, λ0

k))(cid:15)0

(cid:35)

(cid:13)
(cid:13)
(cid:1)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

K
(cid:88)

k=1

e(k)
j

(cid:0)fk(x, θ0

k) − fk(y, θ0

k) + (gk(x, λ0

k) − gk(y, λ0

k))(cid:15)0

(cid:35)

P(ξ0 = ej)

(cid:1)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

πk

(cid:13)
(cid:0)fk(x, θ0
(cid:13)

k) − fk(y, θ0

k) + (gk(x, λ0

k) − gk(y, λ0

k))(cid:15)0

(cid:35)

(cid:1)(cid:13)
(cid:13)

K
(cid:88)

k=1

∞
(cid:88)

=

≤

πkE(cid:15)

(cid:13)
(cid:0)fk(x, θ0
(cid:13)

k) − fk(y, θ0

k) + (gk(x, λ0

k) − gk(y, λ0

k))(cid:15)0

(cid:1)(cid:13)
(cid:13)

(cid:32) K
(cid:88)

(cid:16)

πk

i + b(k)
a(k)

i (cid:107)(cid:15)0(cid:107)1

(cid:33)

(cid:17)

(cid:107)xi − yi(cid:107),

i=1

k=1

by the Minkowski inequality and the Lipschitz-type assumptions (3.1) on fk and gk.
So, this veriﬁes (3.1) of [16].

On the other hand, using the same arguments as above, we can establish that

˜µ1 = (cid:107)F (0; ˜ξ0)(cid:107)1 ≤

K
(cid:88)

k=1

πk

(cid:0)(cid:107)fk(0, θ0

k)(cid:107) + |gk(0, λ0

k)| (cid:107)(cid:15)0(cid:107)1

(cid:1) ,

which is ﬁnite because (cid:15)0 ∈ L1. The ﬁrst part of the theorem is proven.

(ii) Suppose now that C(m) < 1 for some m ∈ N ∩ (1, ∞). Let x = (x1, . . .) and rewrite
k). Then, from (3.1) and the Minkowski

k) = fk(x, θ0

k) − fk(0, θ0

k) + fk(0, θ0

fk(x, θ0

22

inequality, we have

(cid:107)fk(x, θ0

k)(cid:107) ≤

∞
(cid:88)

i=1

a(k)
i (cid:107)xi(cid:107) + ok = wk(x) + ok,

where ok = (cid:107)fk(0, θ0

k)(cid:107) and wk(x) = (cid:80)∞
i=1 a(k)
(cid:19)

i (cid:107)xi(cid:107). Thus,

(cid:107)fk(x, θ0

k)(cid:107)m ≤

k(x) om−j
wj

k + wm

k (x).

m−1
(cid:88)

j=0

(cid:18)m
j

(8.3)

(8.4)

Taking the probability weights λi = a(k)
apply Jensen’s inequality for any s ≥ 1 as follows:

i /Ak (recall that Ak = (cid:80)∞

i=1 a(k)

i ), we can

ws

k(x) = As
k

(cid:32) ∞
(cid:88)

i=1

a(k)
i
Ak

(cid:33)s

(cid:107)xi(cid:107)

≤ As−1
k

∞
(cid:88)

i=1

a(k)
i (cid:107)xi(cid:107)s.

(8.5)

Let us denote Yt−1 = (Xt−1, Xt−2, . . .). From the stationarity of (Xt)t∈Z, for s ≥ 1,
we obtain

E [ws

k(Yt−1)] ≤ As−1

k

∞
(cid:88)

i=1

a(k)
i

E(cid:107)Xt−i(cid:107)s = As
k

E(cid:107)X0(cid:107)s

and therefore

k)(cid:107)m ≤ Am
k
(cid:1)Aj
k os−j
k xj.
Similarly, with the same steps, we can prove that

E(cid:107)fk(Yt−1, θ0
(cid:0)s
j

where Rk,s(x) := (cid:80)s−1

j=0

E(cid:107)X0(cid:107)m + E [Rk,m((cid:107)X0(cid:107))] ,

E|gk(Yt−1, λ0
(cid:0)s
j

k)|m ≤ Bm
k
(cid:1)Bj
kOs−j

where ¯Rk,s(x) := (cid:80)s−1
j=0
, . . . , ξ(K)

Since (ξ(1)

k xj, with Bk = (cid:80)∞
) ∈ {e1, . . . , eK}, for m ∈ N∗,

t

t

E(cid:107)X0(cid:107)m + E (cid:2) ¯Rk,m((cid:107)X0(cid:107))(cid:3) ,

i=1 b(k)

i

and Ok = |gk(0, λ0

k)|.

(cid:107)Xt(cid:107)m =

K
(cid:88)

k=1

ξ(k)
t (cid:107)fk(Yt−1, θ0

k) + gk(Yt−1, λ0

k)(cid:15)t(cid:107)m

≤ 2m−1

K
(cid:88)

k=1

ξ(k)
t

(cid:0)(cid:107)fk(Yt−1, θ0

k)(cid:107)m + |gk(Yt−1, λ0

k)|m(cid:107)(cid:15)t(cid:107)m(cid:1) ,

(8.6)

(8.7)

(8.8)

(8.9)

where the last line is due to Jensen’s inequality. On the other hand, as Rt is inde-
pendent of the random vector ((cid:15)t, Yt−1) and (cid:15)t is independent of Yt−1, then, under the
invariant measure (the existence of this measure is from the stationarity of (Xt)t∈Z),
we obtain that

E(cid:107)X0(cid:107)m = E(cid:107)Xt(cid:107)m ≤ 2m−1

≤ 2m−1

K
(cid:88)

k=1

K
(cid:88)

k=1

πk

(cid:0)E(cid:107)fk(Yt−1, θ0

k)(cid:107)m + (cid:107)(cid:15)0(cid:107)m
m

E|gk(Yt−1, λ0

k)|m(cid:1)

πk(Am

k + Bm

k (cid:107)(cid:15)0(cid:107)m

m)E(cid:107)X0(cid:107)m + C,

(8.10)

23

where C = 2m−1 (cid:80)K
k=1 πk
recursion E(cid:107)X0(cid:107)m−1 < ∞. Therefore, by taking

(cid:0)E [Rk,m((cid:107)X0(cid:107))] + (cid:107)(cid:15)0(cid:107)m

m

E (cid:2) ¯Rk,m((cid:107)X0(cid:107))(cid:3)(cid:1) < ∞ since from

D =

K
(cid:88)

k=1

πk (Am

k + Bm

k (cid:107)(cid:15)0(cid:107)m

m) <

1
2m−1 ,

(8.11)

we conclude that

E(cid:107)X0(cid:107)m <

C
1 − 2m−1D

< ∞.

For the case m ∈ (1, ∞) \ N, we write m = n + δ, where n = (cid:98)m(cid:99) and δ ∈ (0, 1).

Then, by using the expression (8.3), we have that

(cid:107)fk(x, θ0

k)(cid:107)m = (cid:107)fk(x, θ0

k)(cid:107)δ (cid:107)fk(x, θ0

k)(cid:107)n ≤ (wk(x) + ok)δ

n
(cid:88)

j=0

(cid:18)n
j

(cid:19)

k(x) on−j
wj

k

≤

n
(cid:88)

j=0

(cid:18)n
j

(cid:19)

wj+δ
k

(x) on−j

k +

n
(cid:88)

(cid:19)

k(x) on+δ−j
wj

k

(cid:18)n
j

= wm

k (x) + oδ

kwn

k (x) +

n−1
(cid:88)

j=0

wj+δ
k

(x) on−j

k +

n−1
(cid:88)

j=0

(cid:18)n
j

(cid:19)

k(x) on+δ−j
wj

k

.

j=0
(cid:19)
(cid:18)n
j

As in the previous case, using (8.5) and (8.6), we get that

E(cid:107)fk(Yt−1, θ0

k)(cid:107)m ≤ Am
k

E(cid:107)X0(cid:107)m + oδ

kAn
k

E(cid:107)X0(cid:107)n + E (cid:2)R∗

k,m((cid:107)X0(cid:107))(cid:3) ,

where

R∗

k,s(x) :=

(cid:98)s(cid:99)−1
(cid:88)

j=0

(cid:18)(cid:98)s(cid:99)
j

(cid:19)

Aj+s−(cid:98)s(cid:99)

k

o(cid:98)s(cid:99)−j
k

xj+s−(cid:98)s(cid:99) +

(cid:98)s(cid:99)−1
(cid:88)

j=0

(cid:18)(cid:98)s(cid:99)
j

(cid:19)

Aj

kos−j

k xj.

Similarly, with the same steps, we can prove that

E|gk(Yt−1, λ0

k)|m ≤ Bm
k

E(cid:107)X0(cid:107)m + Oδ

kBn
k

E(cid:107)X0(cid:107)n + E (cid:2) ¯R∗

k,m((cid:107)X0(cid:107))(cid:3) ,

where

¯R∗

k,s(x) :=

(cid:98)s(cid:99)−1
(cid:88)

j=0

(cid:18)(cid:98)s(cid:99)
j

(cid:19)

Bj+s−(cid:98)s(cid:99)

k

O(cid:98)s(cid:99)−j
k

xj+s−(cid:98)s(cid:99) +

(cid:98)s(cid:99)−1
(cid:88)

j=0

(cid:19)

(cid:18)(cid:98)s(cid:99)
j

Bj

kOs−j

k xj,

with Bk = (cid:80)∞

i=1 b(k)

i

and Ok = |gk(0, λ0

k)|.

Using the same arguments to prove (8.10), we arrive at

E(cid:107)X0(cid:107)m = E(cid:107)Xt(cid:107)m ≤ 2m−1

≤ 2m−1

K
(cid:88)

k=1

K
(cid:88)

k=1

πk

(cid:0)E(cid:107)fk(Yt−1, θ0

k)(cid:107)m + (cid:107)(cid:15)0(cid:107)m
m

E|gk(Yt−1, λ0

k)|m(cid:1)

πk(Am

k + Bm

k (cid:107)(cid:15)0(cid:107)m

m)E(cid:107)X0(cid:107)m + C ∗,

24

where C ∗ = 2m−1

K
(cid:88)

k=1

πk

(cid:0)(oδ

kAn

k + Oδ

kBn

k (cid:107)(cid:15)0(cid:107)m

m)E(cid:107)X0(cid:107)n

+E (cid:2)R∗

k,m((cid:107)X0(cid:107)) + (cid:107)(cid:15)0(cid:107)m
m

k,m((cid:107)X0(cid:107))(cid:3)(cid:1) (8.12)
¯R∗

which is ﬁnite by recursion, because E(cid:107)X0(cid:107)m−1 < (E(cid:107)X0(cid:107)n)

m−1
n < ∞.

Therefore,

E(cid:107)X0(cid:107)m <

C ∗
1 − 2m−1D

< ∞,

which completes the proof of the theorem.

(cid:3)

8.2. Proof of Theorem 4.1. The proof consists in showing that all conditions of [33,
Theorem 1.1] are in force under our assumptions, and to combine this with epi-convergence
arguments; see [45, 3, 10] for more about epi-convergence theory and applications.

By virtue of (A.3) and (A.4),

it follows from the composition rule in [45, Proposi-

tion 14.45(a)] that

(Yt, (λk, θk)) (cid:55)→ (cid:96)(cid:0)Xt, fk(Xt−1, . . . , Xt−p, θk), gk(Xt−1, . . . , Xt−p, λk)(cid:1)

is random lsc. This entails that

t (cid:96)(cid:0)Xt, fk(Xt−1, . . . , Xt−p, θk), gk(Xt−1, . . . , Xt−p, λk)(cid:1)
ξ(k)
is also random lsc thanks to [45, Corollary 14.46]. In turn, h (see (4.2)), which is the sum of
such K random lsc, is also random lsc in view of [45, Proposition 14.44(c)].

It remains to show that inf Θ×Λ h(Yt, ξt, ·, ·) ∈ L1. We have

0 ≤

E

(A.5)

= E

(cid:20)

inf
θ,λ

(cid:34)

inf
θ,λ

(cid:21)
h(Yt, ξt, θ, λ)

(cid:35)
t (cid:96)(cid:0)Xt, fk(Xt−1, . . . , Xt−p, θk), gk(Xt−1, . . . , Xt−p, λk)(cid:1)
ξ(k)

K
(cid:88)

k=1

=
Separability

E

≤
Optimality

E

E

=
(A.6)

(cid:34) K
(cid:88)

k=1
(cid:34) K
(cid:88)

k=1
(cid:34) K
(cid:88)

k=1

(cid:32) K
(cid:88)

k=1

≤
(A.7)

(cid:35)
(cid:96)(cid:0)Xt, fk(Xt−1, . . . , Xt−p, θk), gk(Xt−1, . . . , Xt−p, λk)(cid:1)

ξ(k)
t

inf
θk,λk

(cid:96)(cid:0)Xt, fk(Xt−1, . . . , Xt−p, ¯θk), gk(Xt−1, . . . , Xt−p, λk)(cid:1)

(cid:35)

(cid:96)(cid:0)Xt, 0, gk(Xt−1, . . . , Xt−p, λk)(cid:1)

(cid:35)

ξ(k)
t

inf
λk

ξ(k)
t

inf
λk

(cid:33)

πk

(CE(cid:107)Xt(cid:107)γ + c) = CE(cid:107)Xt(cid:107)γ + c.

Using the fact that γ = m and E(cid:107)Xt(cid:107)m < +∞ by Theorem 3.1, we deduce that inf Θ×Λ h(Yt, ξt, ·, ·) ∈
L1.

25

Now, by (A.2), Θ × Λ, as a product space of Polish spaces is also Polish. Thus combining
this with (A.1), that h is random lsc, and the summability property we have just shown, as
well as the stationarity and ergodicity of Yt which are inherited from those of Xt, it follows
from [33, Theorem 1.1] that Qn epi-converges to Eh(Y, ξ, ·, ·) a.s. It remains now to invoke
standard epi-convergence arguments that entail the convergence of the minimizers of Qn to
those of Eh(Y, ξ, ·, ·).

(i) Apply [10, Corollary 7.20].
(ii) Apply [10, Corollary 7.24].

This completes the proof.

(cid:3)

8.3. Proof of Theorem 5.1. The proof consists in showing that the conditions (A1)-(A4)
of [55, Theorem 3.2.23] are fulﬁlled.
Indeed, let us denote Yt−1 = (Xt−1, . . . , Xt−p). Then, from strict stationarity and ergodicity,
the ergodic theorem and (B.2), it follows that

1
n

∂Qn(θ0)
∂θk,i

= −

2
n

n
(cid:88)

t=1

ξ(k)
t

(cid:0)Xt − fk(Yt−1, θ0

a.s.−→ − 2πkE

(cid:20)
(cid:0)Xp+1 − fk(Yp, θ0

k)(cid:1)(cid:62) ∂fk(Yt−1, θ0
k)
∂θk,i
(cid:21)
k)(cid:1)(cid:62) ∂fk(Yp, θ0
k)
∂θk,i

= 0,

for all k ∈ [K] and all i ∈ [dk]. Hence, condition (A1) of [55, Theorem 3.2.23] is satisﬁed.

Similarly, using again (B.2) and the ergodic theorem, we have that
(cid:34)(cid:18) ∂fk(Yt−1, θ0
k)

(cid:19)(cid:62) ∂fk(Yt−1, θ0
k)

n
(cid:88)

1
n

∂2Qn(θ0)
∂θl,j∂θk,i

=

2
n

ξ(k)
t

t=1

− (cid:0)Xt − fk(Yt−1, θ0

∂θk,j

∂θk,i
k)(cid:1)(cid:62) ∂2fk(Yt−1, θ0
k)
∂θk,j∂θk,i
(cid:19)(cid:62) ∂fk(Yp, θ0
k)

(cid:21)

∂θk,j

∂θk,i

I{l=k}

(cid:34)(cid:18)∂fk(Yp, θ0
k)

a.s.−→ 2πkE

(cid:35)

I{l=k} = 2(Vkl)ij ,

(8.13)

for all k, l ∈ [K] and all (i, j) ∈ [dk] × [dl], because

n−1

n
(cid:88)

t=1

ξ(k)
t

(cid:0)Xt − fk(Yt−1, θ0

k)(cid:1)(cid:62) ∂2fk(Yt−1, θ0
k)
∂θk,j∂θk,i

I{l=k}

a.s.−→ 0,

for all k, l ∈ [K] and all (i, j) ∈ [dk] × [dl]; see [52]. In the expression (8.13), (Vkl)ij denotes
the (i, j)-th entry of the matrix Vkl deﬁned in (5.4). From (B.3), the Gram matrix of each
Jacobian J[fk(Xp, . . . , X1, ·)](θ0
k) is invertible for any k ∈ [K], whence we deduce that V is
positive deﬁnite since it is block-diagonal whose diagonal blocks are those Gram matrices
(up to multiplication by πk > 0). Thus, assumption (A2) of [55, Theorem 3.2.23] is also
satisﬁed.

Now, let θ ∈ V, and δ > 0 such that the ball (cid:107)θ−θ0(cid:107) < δ is contained in V (δ can be chosen
arbitratily small for this to hold). Let the closed segment [θ0, θ] = {ρθ + (1 − ρ)θ0 : ρ ∈ [0, 1]}
26

and the open segment ]θ0, θ[= {ρθ + (1 − ρ)θ0 : ρ ∈]0, 1[}. Then, for ¯θ ∈ [θ0, θ], and any
k, l ∈ [K] and (i, j) ∈ [dk] × [dl], we have from the mean value theorem that

(cid:0)Tn(¯θ)(cid:1)

kl,ij :=




∂2Qn(¯θ)
∂θk,j∂θk,i

0

−

∂2Qn(θ0)
∂θk,j∂θk,i

if l = k

if l (cid:54)= k

= (¯θ − θ0)(cid:62)∇

(cid:33)

(cid:32)

∂2Qn(¯¯θ)
∂θk,j∂θk,i

I{l=k},

for some ¯¯θ ∈]θ0, ¯θ[.

Since by deﬁnition (cid:107)¯θ −θ0(cid:107) < δ, we have (cid:107)¯¯θ −θ0(cid:107) < δ and thus ¯¯θ ∈ V. Hence from continuity
of the norm and that of the derivatives of Qn up to third-order on V, we get, upon using
Cauchy-Scwartz inequality, that

1
nδ

(cid:12)
(cid:12)
(cid:12)

sup
δ→0

(cid:0)Tn(¯θ)(cid:1)

kl,ij

≤ lim inf

δ→0

(cid:12)
(cid:12)
(cid:12)

1
n

(cid:32)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

∇

≤ lim
δ→0

2
n

I{l=k}

(cid:33)

∂2Qn(¯¯θ)
∂θk,j∂θk,i

I{l=k}

dk(cid:88)

r=1

(cid:32)

t=1





ξ(k)
t

n
(cid:88)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
∂2fk(Yt−1, ¯¯θk)
∂θk,i∂θk,r

Xt − fk(Yt−1, ¯¯θk)

1
n

= lim
δ→0

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
∂fk(Yt−1, ¯¯θk)
∂θk,i

(cid:33)(cid:62)

(cid:33)

(cid:32)

∂2Qn(¯¯θ)
∂θk,j∂θk,i

I{l=k}

∇

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

∂2fk(Yt−1, ¯¯θk)
∂θk,j∂θk,r

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
∂2fk(Yt−1, ¯¯θk)
∂θk,i∂θk,j

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

+

(cid:32)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:33)(cid:62)

∂fk(Yt−1, ¯¯θk)
∂θk,r

(cid:17)(cid:62) ∂3fk(Yt−1, ¯¯θk)
∂θk,i∂θk,j∂θk,r

.

(cid:33)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:19)(cid:62) ∂2fk(Yt−1, θ0
k)
∂θk,j∂θk,r

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

n
(cid:88)

ξ(k)
t

(cid:32)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

r=1

t=1

(cid:19)(cid:62) ∂2fk(Yt−1, θ0
k)
∂θk,i∂θk,r

(cid:18) ∂fk(Yt−1, θ0
k)
∂θk,i
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

+

(cid:18) ∂fk(Yt−1, θ0
k)
∂θk,r

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:19)(cid:62) ∂2fk(Yt−1, θ0
k)
∂θk,i∂θk,j
k)(cid:1)(cid:62) ∂3fk(Yt−1, θ0
k)
∂θk,i∂θk,j∂θk,r

(cid:19)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

.

+

(cid:0)Xt − fk(Yt−1, θ0

(cid:12)
(cid:12)
(cid:12)
(cid:12)

+

(cid:32)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:33)(cid:62)

∂fk(Yt−1, ¯¯θk)
∂θk,j

(cid:16)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
dk(cid:88)

+

=

I{l=k}

2
n
(cid:18) ∂fk(Yt−1, θ0
k)
∂θk,j

+

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

From strict stationarity, ergodicity and the condition (B.4), by using the ergodic theorem
again, it follows that

lim
n→∞

sup
δ→0

1
nδ

(cid:12)
(cid:12)
(cid:12)

(cid:0)Tn(¯θ)(cid:1)

kl,ij

(cid:12)
(cid:12) ≤2πkI{l=k}
(cid:12)

dk(cid:88)

r=1

(cid:0)Gijr

k + Gjir

k + Grij

k + H ijr

k

(cid:1) < ∞.

With this we have shown that condition (A3) of [55, Theorem 3.2.23] holds.

27

Finally, by using [55, Theorem 1.3.3], the vector process (Zt)t∈Z deﬁned by
1))(cid:62) ∂f1(Yt−1, θ0
1)

ξ(1)
t (Xt − f1(Yt−1, θ0

Zt = −2

, . . .

(cid:18)

∂θ1,1

is strictly stationary and ergodic. Therefore, condition (A4) of [55, Theorem 3.2.23] follows
(cid:3)
by combining (B.5) and [55, Theorem A.2.14]. This completes the proof.

. . . , ξ(K)
t

(Xt − f1(Yt−1, θ0

1))(cid:62) ∂fK(Yt−1, θ0
K)
∂θK,dK

(cid:19)

Appendix A. Derivatives with respect to NN parameters
Let θ = (cid:0)(W (1), β(1)), . . . , (W (L), b(L))(cid:1) be an architecture of a NN f : (x, θ) ∈ Rd −→ RNL
and denote W (l) = (w(l)
)jl∈[Nl], with l ∈ [L]. We denote
D[f ]W (l)(x, θ) the Fr´echet derivatives of f wrt to W (l) evaluated at (x, θ). Recalling the
recursion in Deﬁnition 2.2, and by the standard chain rule, D[f ]W (l) acting in the direction
H (l) ∈ RNl×Nl−1 reads, for l ∈ [L],
(cid:32) l

)(jl,jl−1)∈[Nl]×[Nl−1] and β(l) = (β(l)
jl

jljl−1

(cid:33)

D[f ]W (l)(x, θ)(H (l)) =

(cid:89)

W (i+1)J[ϕ] (cid:0)W (i)x(i−1) + b(i)(cid:1)

H (l)x(l−1).

(A.1)

Similarly, we have

i=L−1

J[f ]b(l)(x, θ) =

l
(cid:89)

W (i+1)J[ϕ] (cid:0)W (i)x(i−1) + b(i)(cid:1) .

(A.2)

i=L−1
∂f (x, θ)
∂w(l)
ij

∂f (x, θ)
∂β(l)
i

As usual, the partial derivatives

(x, θ) (resp.

(θ)) is nothing but (A.1)

(resp. (A.2)) evaluated in the direction H (l) (resp. i-th standard basis vector of RLl) such
that H (l)

ij = 1 and 0 otherwise.

A similar calculation can be carried out to get the second- and third-order derivatives that

we leave to the reader.

References

[1] Andrews, D.W.K. (1984) Non strong mixing autoregressive processes. J. Appl. Prob.,
21:930–934.
[2] Artstein, Z. and Wets, R. J-B. (1995) Consistency of minimizers and the SLLN for
stochastic programs. J. Convex Anal., 2:1–17.
[3] Attouch, H. (1984) Variational convergence for functions and operators. Applicable math-
ematics series. Pitman Advanced Publishing Program.
[4] Attouch, H. and Wets, R. J-B. (1990) Epigraphical processes: laws of large numbers for
random lsc unctions. S´em. Anal. Convexe, Montpellier, 13:1–29.
[5] Bartlett, P. L., Foster D. J. and Telgarsky, M. J. (2017) Spectrally-normalized mar-
gin bounds for neural networks. In Advances in Neural Information Processing Systems
(NeurIPS), 30:6241–6250
[6] Bolte, J. and Pauwels, E. (2020) Conservative set valued ﬁelds, automatic diﬀerentiation,
stochastic gradient methods and deep learning. Mathematical Programming.

28

[7] Castera, C., Bolte, C., F´evotte, C. and Pauwels, E. (2019) An inertial newton algorithm
for deep learning. https://arxiv.org/pdf/1905.12278.pdf.
[8] M. Ciss´e and P. Bojanowski and E. Grave and Y. Dauphin and N. Usunier (2017) Parseval
networks: improving robustness to adversarial examples. ICML, 70:854–863
[9] Dahlhaus, R. (2000) A likelihood approximation for locally stationary processes. Annals
of Statistics, 28:1762–1794.
[10] Dal Maso, G. (2012) An introduction to Γ-convergence, volume 8. Springer Science &
Business Media.
[11] Daubechies, I., DeVore, R., Foucart, S., Hanin, B.and Petrova, G. (2019) Nonlinear
approximation and (deep) ReLU networks. arxiv preprint arxiv:1905.02199.
[12] Dedecker, J. and Prieur, C. (2004) Coupling for τ -dependent sequences and applications.
J. Theoret. Probab., 17(4):861–885.
[13] Douc, R. Fokianos, K. and Moulines, E. (2017) Asymptotic properties of quasi-maximum
likelihood estimators in observation-driven time series models. Electronic Journal of Statis-
tic, 11:2707–2740.
[14] Doukhan, P., Fokianos, K. and Li, X. (2012) On weak dependence conditions: The case
of discrete valued processes. Statistics and Probability Letters, 82:1941–1948.
[15] Doukhan, P., Fokianos, K. and Tjøstheim, D. (2012) On weak dependence conditions
for Poisson autoregressions. Statistics and Probability Letters, 82:942–948.
[16] Doukhan, P. and Wintenberger, O. (2008) Weakly dependent chains with inﬁnite mem-
ory. Stochastic Processes and their Applications, 118:1997–2013.
[17] Falbel, D., Allaire,
JJ., Chollet, F., RStudio, Google, Tang, Y., Van
Der Bijl, W., Studer, M. and Keydana, S. (2019) Package ‘keras’. https://cran.r-
project.org/web/packages/keras/keras.pdf.
[18] Ferland, R., Latour, A. and Oraichi, D. (2006) Integer-valued GARCH processes. Journal
of Time Series Analysis, 27:923–942.
[19] Fokianos, K. and Fied, R. (2010) Interventions in INGARCH processes. Journal of Time
Series Analysis, 31:210–225.
[20] Fokianos, K., Rahbek, A. and Tjøstheim, D. (2009) Poisson autoregression. Journal of
the American Statistical Association, 104:1430–1439.
[21] Fokianos, K. and Tjøstheim, D. (2012) Nonlinear poisson autoregression. Annals of the
Institute of Statistical Mathematics, 64:1205–1225.
[22] Franke, J., Hardle, W. and Hafner, C. (2019) Statistics of Financial Markets: An intro-
duction. Springer, 5 edition.
[23] Griewank, A. and Walther, A. (2008) Evaluating Derivatives: Principles and Techniques
of Algorithmic Diﬀerentiation, Second Edition. SIAM.
[24] Haeﬀele, B. and Vidal, R. (2017) Global optimality in neural network training. In CVPR.
[25] Hafner, C. (1998) Nonlinear Time Series Analysis with Applications to Foreign Exchange
Rate Volatility. Contributions to Economics. Springer-Verlag Berlin Heidelberg GmbH.
[26] Hanin, B. and Sellke, M. (2017) Approximating continuous functions by ReLU nets of
minimal width. arxiv preprint arXiv:1710.11278.
[27] Hebb, D. (1949) The organization of behavior: A neuropsychological theory. Wiley.
[28] Henze, N. and Zirkler, B. (1990) A class of invariant consistent tests for multivariate
normality. Commun Stat Theory Methods, 19(10):3595–3617.

29

[29] Hess, C. (1996) Epi-convergence of sequences of normal integrands and strong consis-
tency of the maximum likelihood estimator. Annals of Statistics, 24(3):1298–1315.
[30] Hornik, K., Stinchcombe, M. and White, H. (1989) Multilayer feedforward networks are
universal approximators. Neural networks, 2(5):359–366.
[31] Kirch, C. and Kamgaing, T. (2012) Testing for parameter stability in nonlinear autore-
gressive models. Journal of Time Series Analysis, 33(3):365–385.
[32] Klimko, L.A. and Nelson, P.I. (1978) On conditional least squares estimation for sto-
chastic processes. The Annals of Statistics, 6(3):629–642.
[33] Korf; L.A. and Wets, R.J.B. (2001) Random lsc functions: An ergodic theorem. Mathe-
matics of Operations Research, 26(2):421–445, May 2001.
[34] Korf, L.A. and Wets, R.J.B. (2000) An ergodic theorem for stochastic programming
problems. In V.H. Nguyen, J.J. Strodiot, and P. Tossings, editors, Proceedings of the 9th
Belgian-French-German Conference on Optimization, volume 481 of Lecture Notes in Eco-
nomics and Mathematical Sciences, pages, pages 203–217. Springer.
[35] Korf, L.A. and Wets, R.J.B. (2000) Random lsc functions: An ergodic theorem. In
Stochastic Programming E-print Series (SPEPS). Humboldt-Universit¨at.
[36] Korkmaz, S., Goksuluk, D. and Zarasiz, G. (2014) An R package for assessing multi-
variate normality. R Journal, 6(2):151–162.
[37] Liehr, S., Pawelzik, K., Kohlmorgen, J. and Moler, K.R. (1999) Hidden markov mixtures
of experts with an application to eeg recordings from sleep. Theory of Biosciences, 118:246–
260.
[38] Lo, M.T., Tsai, P.H., Lin, P.F., Lin, C. and Hsin, Y.L. (2009) The nonlinear and
nonstationary properties in eeg signals: probing the complex ﬂuctuations by hilbert-huang
transform. Advances in Adaptive Data Analysis, 1(3):461–482.
[39] Von Luxburg, U. and Bousquet, O. (2004) Distance-Based Classiﬁcation with Lipschitz
Functions. J. Mach. Learn. Res. 5:669–695.
[40] Mardia, K.V. (1970) Measures of multivariate skewness and kurtosis with applications.
Biometrika, 57(3):519–530.
[41] Meyn, S.P. and Tweedie; R.L. (1993) Markov Chain and Stochastic Stability. Springer-
Verlag.
[42] Miyato, T., Kataoka, T., Koyama, M. and Yoshida, Y. (2018) Spectral normalization for
generative adversarial networks. Proceedings of the International Conference on Learning
Representations (ICLR).
[43] Neyshabur, B., Wu, Y., Salakhutdinov, R. and Srebro, N. (2016) Path-normalized opti-
mization of recurrent neural networks with relu activations. In NIPS.
[44] Rockafellar, R.T. (1976) Integral functionals, normal integrands and measurable selec-
tions. In J. Gossez and L. Waelbroeck, editors, Nonlinear Operators nd the Calculus of
Variations, number 543 in Lecture Notes in Mathematics, pages 157–207. Springer.
[45] Rockafellar, R.T. and Wets, R.J.B. (1998) Variational Analysis. Springer.
[46] Rojas, I. Pomares, H. and Valenzuela, O. editors. (2017) Advances in Time Series Anal-
ysis and Forecasting: Selected Contributions from ITISE 2016. Contributions to Statistics.
Springer International Publishing.
[47] Rosenblatt, F. (1958) The perceptron: a probabilistic model for information storage and
organization in the brain. Psychological review, 65(6):386.

30

[48] Royston, P. (1992) Approximating the shapiro-will W test for non-normality. Statistics
and Computing, 2(3):117–119.
[49] Scaman, K. and Virmaux, A. (2018) Lipschitz regularity of deep neural networks: analy-
sis and eﬃcient estimation. Advances in Neural Information Processing Systems (NeurIPS),
31:3835–3844
[50] Shalev-Shwartz, S. and Ben-David, S. Understanding machine learning: from theory to
algorithms. Cambridge University Press, 2014.
[51] Stockis, J-P., Franke, J. and Tadjuidje Kamgaing, J. (2010) On geometric ergodicity of
charme models. Journal of Time Series Analysis, 31:141–152.
[52] Stout, W.F. (1974) Almost Sure Convergence. Academic Press, New York.
[53] Szegedy, C., Zaremba, W., Sutskever, I. Bruna, J., Erhan, D., Goodfellow, I., Fergus,
R. (2014) Intriguing properties of neural networks. ICLR.
[54] Tadjuidje-Kamgaing, J. (2005) Competing neural networks as model for nonstationary
ﬁnancial time series. PhD thesis, University of Kaiserslautern.
[55] Taniguchi, M and Kakizawa, Y. (2000) Asymptotic Theory of Statistical Inference for
Time Series. Springer-Verlag, New York.
[56] Telgarsky, M. (2015) Representation beneﬁts of deep feedforward networks. arXiv
preprint arXiv:1509.08101.
[57] Vidal, R., Bruna, J., Giryes, R. and Soatto, S. (2017) Mathematics of deep learning. In
IEEE CDC.
[58] Weigend, A.S. and Shi, S. (2000) Predicting daily probability distributions of s&p500
returns. Journal of Forecasting, 19(4):375–392.
[59] Xu, H. and Mannor, S. (2012) Robustness and generalization. Machine Learning 86,
3:391–423.
[60] Yarotsky, D. (2017) Error bounds for approximations with deep relu networks. Neural
Networks, 94:103–114, 2017.
[61] Yarotsky, D. (2017) Quantiﬁed advantage of discontinuous weight selection in approxi-
mations with deep neural networks. arXiv preprint arXiv:1705.01365.
[62] Yoshida, Y. and Miyato, T. (2017) Spectral norm regularization for improving the gen-
eralizability of deep learning. arXiv preprint arXiv:1705.10941
[63] Yun, C., Sra, C. and Jadbabaie, A. (2018) Global optimality conditions for deep neural
newtorks. In ICLR.

* Normandie Universit´e, UNICAEN, CNRS, LMNO, France.
Email address: jose3g@gmail.com; christophe.chesneau@unicaen.fr

∗∗ Normandie Universit´e, ENSICAEN, UNICAEN, CNRS, GREYC, France.
Email address: Jalal.Fadili@ensicaen.fr

31

