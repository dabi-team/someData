X-Vision: An augmented vision tool with real-time
sensing ability in tagged environments

Yongbin Sun* , Sai Nithin R. Kantareddy* , Rahul Bhattacharyya , and Sanjay E. Sarma

Auto-ID Labs, Department of Mechanical Engineering
Massachusetts Institute of Technology
Cambridge, USA
{yb sun, nithin, rahul b, sesarma} at mit.edu

8
1
0
2

v
o
N
5
2

]

C
H
.
s
c
[

2
v
7
6
5
0
0
.
6
0
8
1
:
v
i
X
r
a

Abstract—We present the concept of X-Vision, an enhanced
Augmented Reality (AR)-based visualization tool, with the real-
time sensing capability in a tagged environment. We envision
that
this type of a tool will enhance the user-environment
interaction and improve the productivity in factories, smart-
spaces, home & ofﬁce environments, maintenance/facility rooms
and operation theatres, etc. In this paper, we describe the design
of this visualization system built upon combining the object’s pose
information estimated by the depth camera and the object’s ID
& physical attributes captured by the RFID tags. We built a
physical prototype of the system demonstrating the projection of
3D holograms of the objects encoded with sensed information like
water-level and temperature of common ofﬁce/household objects.
The paper also discusses the quality metrics used to compare
the pose estimation algorithms for robust reconstruction of the
object’s 3D data.

I. INTRODUCTION

Superimposing information on to the real-world, the concept
commonly known to us as Augmented reality (AR), has been
rapidly evolving over the past few years due to the advances
in computer vision, connectivity and mobile computing. In
recent years, multiple AR-based applications have touched
everyday lives of many of us: few such examples are Google
translate’s augmented display [1] to improve productivity,
AR GPS navigation app for travel [2], CityViewAR tool for
tourism [3], etc.

All these applications require a method to implement a
link between the physical and digital worlds. Often this link
is either ID of the object or information about the physical
space, for instance, an image in Google translate app or
GPS location in AR navigation tool. This link can be easily
established in a informationally structured environments using
visual markers, 2D barcodes and RFID tags. Among the
three, RFID tags have an unique leverage with the ability to
wirelessly communicate within couple of meters of distance
without requiring line of sight access. In addition, RFID tags
can be easily attached to inventory and consumer products
in large numbers at extremely low per unit costs. Passive
RFID, in particular, has many applications in object tracking
[4], automatic inventory management [5], pervasive sensing
[6], etc. In a tagged environment, with RFID infrastructure
information of tagged object’s ID and physical
installed,

* Equal Contribution

Fig. 1. Left: A user wearing the system sees a cup with overlaid temperature
information. Right: System components: an Intel RealSense D415 RGB-D
camera is attached on a HoloLens via a custom mount.

attributes can be wirelessly retrieved and mapped to a digital
avatar.

In this paper, we have designed a visualization framework
called X-Vision, hoping to equip users with the ability to di-
rectly see the physical attributes of surrounding objects (Figure
1). One of the goals of this framework is to demonstrate
the advantages of tagged environments to enhance the user-
environment interaction with real-time sensing at low cost
for potential use cases in smart-spaces, home & ofﬁce envi-
ronments, maintenance/facility rooms and operation theatres,
etc. The rest of the paper is structured as follows: Section
II discusses the relevant work in fusion of other technologies
with RFID and AR; Section III provides the details of the
proposed framework and the visualization system; Section IV
discusses the evaluations of the test experiments, followed by
the conclusions in Section V.

II. RELATED WORK

A. AR-Based Smart Environment

AR brings digital components into a person’s perception
of the real world. Today, advanced AR technologies facili-
tates the interactive bidirectional communication and control
between a user and objects in the environment. Two main
branches exist for AR associated research. In one branch,
researchers attempt to design algorithms to achieve accurate
object recognition and 3D pose estimation for comprehensive
environment understanding. Related work in this direction can
be found in [7], [8] for object recognition, and in [9], [10],
[11] for 3D data processing. Research in this direction provides
theoretic supports for industry products. In the other branch,

 
 
 
 
 
 
Fig. 2. The pipeline of the proposed framework.

efforts have been devoted to applying existing computer vision
techniques to enhance user-environment interaction experience
for different purposes. Research work on this track beneﬁts
areas, such as education [12], tourism [13] and navigation
[14], by improving user experience. Our work follows this
trend by fusing object recognition and 3D pose estimation
techniques with RFID sensing capabilities, aiming to create a
smart environment.

B. Emerging RFID Applications

RFID is massively used as identiﬁcation technology to
support tracking in supply chain, and has so far been suc-
cessfully deployed in various industries. Recently industry’s
focus seems shifting towards generating higher value from
the existing RFID setups by tagging more & more items
and by developing new applications using tags that allow
for sensing, actuation & control [15] and even gaming [16].
Another such exciting application with industrial beneﬁt is
fusion with emerging computer vision and AR technologies.
Fusion of RFID and AR is an emerging ﬁeld and there are
recent studies combining these technologies for gaming and
education, yet we see lot of space to explore further especially
going beyond ID in RFID. One of the earlier papers [17]
studied the use of RFID to interact with physical objects
in playing a smartphone-based game which enhanced the
gaming experience. Another study [18] used a combination of
smart bookshelves equipped with RFID tags and mixed-reality
interfaces for information display in libraries. Another study
[19] explores the use of AR with tags to teach geometry to
students. These studies show a good interest in the community
to explore mixed reality applications using tags for object IDs.
In this paper, we use RFID for not just ID but also to wirelessly

sense the environment and object’s attributes to create a more
intimate and comprehensive interaction between the humans
and surrounding objects.

III. SYSTEM

Our system (hardware and visualization shown in Fig. 1)
contains two parallel branches (shown in Figure 2) to concur-
rently detect and sense RFID tag-sensor attached objects. On
one side, the system captures color and depth images using
the depth camera for in-view target object identiﬁcation and
pose estimation. On the other side, the system collects tag-
data reﬂecting the target object’s physical properties, such as
temperature, using an RFID interrogator\reader. Information
collected from both sources are uploaded to a shared central
server, where heterogeneous information is uniﬁed and deliv-
ered to the HoloLens for augmented visualization. Details are
given in the following subsections.

A. Object Identiﬁcation and Pose Estimation

Our system uses an Intel RealSense D415 depth camera
to capture color and depth information. It is attached to an
HoloLens via a custom mount provided by [20], and faces in
the same direction as the HoloLens (Figure 1). The captured
images are used to identify the in-view target object and
estimate its pose.
Object Identiﬁcation: Object recognition is a well-studied
problem, and we adopt the local feature based method [8]
in our system, since it is suitable for small-scale database.
Generally, to identify an in-view object from a given database,
the local feature based method ﬁrst extracts representative
local visual features for both the scene image and template
object images, and then matches scene features with those

of each template object. The target object in the view is
identiﬁed as the template object with the highest number of
matched local features. If the number of matched features
of all
template objects is not sufﬁciently large (below a
predetermined threshold), then the captured view is deemed to
not contain a target. Our system follows this scheme, and uses
SURF algorithm [8] to compute local features, since compared
to other local feature algorithms, such as SIFT [21], SURF is
fast and good at handling images with blurring and rotation.
Pose Estimation: After identifying the in-view object, our
system estimates its position and rotation, namely 3D pose,
in the space, thus augmented information can be rendered
properly. We achieve this by constructing point cloud of the
scene, and aligning the identiﬁed object’s template point cloud
with it. Many algorithms exist for point cloud alignment, and
we adapt widely-used Iterative Closest Point (ICP) algorithm
[22] in our system, since it usually ﬁnds a good alignment
in a quick manner. To obtain better pose estimation results,
especially for non-symmetric objects (i.e. mug), a template
object usually contains point clouds from multiple viewpoints.
Yet,
the performance of ICP relies on the quality of the
initialization. Our system ﬁnds a good initial pose by moving
a template object’s point cloud to the 3D position that is back-
projected from the centroid of matched local feature coordi-
nates in the scene image. The coordinates of correctly matched
local feature are the 2D projections of target object surface
points, thus back-projecting their centroid should return a 3D
point close to target object surface points. After initializing the
pose of template point cloud, our system reﬁnes its pose using
ICP. Finally, the estimated pose can be represented as a 4 × 4
matrix, Mpose = MiniMicp, where Mini is the transformation
matrix for pose initialization, and Micp is the transformation
matrix for pose reﬁnement using ICP. All the transformation
matrix are in the format of

M =

(cid:21)
(cid:20)R t
1
0

, where R is a 3×3 matrix representing rotation, and t is a 3×1
vector representing translation. Related details are illustrated
in [23].

B. RFID Sensing

An ofﬁce space already equipped with the RFID infrastruc-
ture is used as the tagged-environment for the experiments in
this study. The space is set up using the Impinj Speedway
Revolution RFID reader, connected to multiple circularly
polarized Laird Antennas with gain of 8.5 dB. The reader
system is broadcasting at the FCC maximum of 36 dBm EIRP.
For the tag-sensors, we make use of the Smartrac’s paper
RFID tags with Monza 5 IC as the backscattered-signal based
water level sensors and custom designed tags with EM 4325
IC as the temperature sensors. We use the Low Level Reader
Protocol (LLRP) implemented over Sllurp (Python library) to
interface with RFID readers and collect the tag-data.

Purely-passive or semi-passive tags can be designed to
sense multiple physical attributes and environmental condi-
tions. One approach is based on tag-antenna’s response to

Fig. 3. Tag-sensors for sensing object properties: Top row: Water level
sensing using paper tags; Bottom row: Temperature sensing using custom
tag with EM4325.

changed environments as a result of sensing event. Change
in signal power or response frequency of the RFID tag due
to this antenna’s impedance shift can be attributed to sensing
events like temperature rise [24], presence of gas concentration
[25], soil moisture [26] etc. Another approach is to use IC’s
on-board sensors or external sensors interfaced with GPIOs
[27]. In this study, we use both the antenna’s impedance
shift approach to detect water-level and the IC’s on-board
temperature sensor to detect the real-time temperature in a
coffee cup.
Water Level Sensing: Water-level sensor works on the con-
cept of relating the detuning of the tag’s antenna in the
presence of water in the neighborhood of the tag. In this study,
we used tags as the water-level sensors on common house-
hold/ofﬁce objects such as coffee cup made of paper, ceramic
mug and plastic bottle. In an empty state, the background
dielectric for the tags is air, therefore, the backscattered signal
strength from the tags is at the maximum. In the state where
the mug contains water, the antenna is signiﬁcantly detuned
due to the change in background dielectric, as a result the tag
becomes unresponsive. However, when the mug is emptied the
tag can be read again indicating empty cup. We build on this
concept to detect discrete levels of water in the container by
using three tags to deﬁne states as empty, low, mid, and high
(illustrated in Table I). Fig. 3(a) shows the level sensor labels
implemented on a standard ceramic coffee mug.

TABLE I
WATER LEVEL INDICATION

Status
Empty
Middle
Full

A
(cid:55)
(cid:55)
(cid:55)

B
C
(cid:51) (cid:51)
(cid:51)
(cid:55)
(cid:55)
(cid:55)

Temperature Sensing: Temperature sensor is implemented by
using EM Microelectronics’s EM 4325 with on-board tem-
perature as the RFID IC. Fig. 3(b) shows a T-match antenna
with EM 4325 IC and a button-cell battery implemented as

a temperature sensor on a standard coffee cup. Temperature
measurements from this IC can be made in both passive as
well as semi-passive mode. In the passive mode, the tag has
to be in range of a reader antenna. In the semi-passive mode,
the battery or external energy source keeps the IC on. The IC’s
temperature measurement is triggered by writing any random
information into the speciﬁc word of the user memory bank
(Memory bank:3 and Wordptr:256). The IC updates this word
with the measured temperature from the on-board temperature
sensor. By reading the word again current temperature can
be known. We have implemented this code using the Sllrup
library. Real-time temperature sensing is possible using this
IC within −64o to 64o Celsius.

C. Augmented Visualization

After obtaining the target object’s identity, pose and physical
properties, the system superimposes augmented information
(i.e. CAD model) onto the object. Since the object’s 3D pose
is estimated in the depth camera coordinate system, a series
of transformations are required to obtain the 3D pose in the
world coordinate system, which is required in the HoloLens
rendering system. Our system computes the transformation
using:

Mworld

pose = Tworld

HoloLensTHoloLens

dep cam Mdep cam

pose

pose

and Mworld
pose

, where Mdep cam
are the 3D poses in depth
camera and world coordinate system, respectively, THoloLens
dep cam
maps the pose from the depth camera coordinate system to the
HoloLens coordinate system, and Tworld
HoloLens maps the pose
from the HoloLens coordinate system to the world coordinate
system. All the transformation matrices are in the same format
as those described for pose estimation.

Fig. 4. Water level sensing results: (a) shows the HoloLens rendered results
before and after water is added into a tagged mug; (b) shows multiple objects
with different detected water levels.

IV. EVALUATIONS

A. Sensing Results Visualization

We ﬁrst test our system’s performance on both water level
sensing and temperature sensing. A user’s augmented views
are recorded and shown to demonstrate the effectiveness of
the proposed system.

We present water level sensing results for both single object
and multiple objects cases (Figure 4). The system projects 3D
CAD models of identiﬁed objects into the scene according to
their estimated poses. The color of projected 3D models is
changed at different heights to reﬂect different water levels.
As can be observed, our system properly aligns 3D models to
corresponding target objects, even for non-symmetric shapes
(i.e. mug). The detected discrete water levels (empty, low, mid,
high) also matches the actual water level in our experiments.
Temperature sensing results are shown in Figure 5. These
results are selected from a recorded video clip containing the
whole temperature changing process after hot water is poured
into a tagged cup. In the beginning, the tag-sensor reports
room temperature for the empty cup. After hot water is added,
the tag-sensor reports water temperature (and intermediate
temperatures). Temperatures are rendered using the color code

Fig. 5. A sequence of temperature sensing results after hot water is added.
The temperature change after adding hot water results from the temperature
sensor latency.
shown on the right of Figure 5. Our system shows visually
appealing results.

B. Pose Estimation Evaluation

We evaluate the pose estimation pipeline used in the system
by considering recognition accuracy, pose estimation quality
and running time. The Fast Point Feature Histograms (FPFH)
algorithm [28] with and without using local features are imple-
mented as competing methods. The original FPFH algorithm
was designed for point cloud alignment. As a baseline method

Fig. 6. Test objects.

in this experiment, it identiﬁes the in-view object and estimates
its pose by aligning each template point cloud to scene point
cloud, and the object with the best alignment is selected.
“Local Feature + FPFH” works in the same way as our pipeline
by ﬁrst identifying the target object using local features and
then estimating its pose using FPFH.

Three different objects (a water bottle, a coffee cup and a
mug) are tested. For each object, ﬁve images from different
angles at a range between 0.3-0.5 meter are collected for
evaluation (as show in Figure 6). Within this range, texture
details of objects can be captured and their point clouds can
be constructed with less noise.

First, the recognition accuracy of each method is evaluated
and reported in Table II. As can be noted, compared to
the FPFH baseline method, local visual features enhance the
recognition accuracy for all cases. This performance boosting
results from the rotational invariance of the detected SURF
features. Identifying in-view objects is important, thus correct
template point clouds can be used for pose estimation in the
following step.

TABLE II
RECOGNITION ACCURACY

FPFH
Local Feature + ICP
Local Feature + FPFH

Bottle
1/5
5/5
5/5

Cup Mug
3/5
3/5
5/5
5/5
5/5
5/5

Avg.
7/15
15/15
15/15

Second, the pose estimation quality is evaluated using point-

to-point residual error, which is deﬁned as:

E =

1
n

n
(cid:88)

i=1

(cid:107)ti − pi(cid:107)2

, where ti is the ith point in the target object point cloud
(green points in Figure 7), and pi is the closest point in the
transformed template object point cloud {p} (red points in
Figure 7) to ti, such that pi = arg minp (cid:107)ti − p(cid:107)2. Results
are reported in Table III, where the residual error is averaged
across all correctly identiﬁed target objects. Point clouds of
target objects in the scene are manually labeled (i.e. green
points in Figure 7) Due to good pose initialization from local
visual feature matching, two-phase pipelines achieves lower
point-to-point residual error. “Local Feature + FPFH” performs
slightly better than “Local Feature + ICP”, since ICP is prone
to get trapped into local minima. Examples of successfully
aligned point clouds are shown in Figure 7.

Third, we compare the running time of different methods,
and report the average time for each testing object in Table
IV. Despite “Local Feature + ICP” shows a little higher point-

Fig. 7. Point cloud pose estimation examples of different methods. Scene
points are marked as blue, target object points are marked as green, and
transformed template object points are marked as red.

to-point residual error than “Local Feature + FPFH”, it runs
signiﬁcantly faster and is suitable for real-time performance.
TABLE III
POINT-TO-POINT RESIDUAL ERROR

FPFH
Local Feature + ICP
Local Feature + FPFH

Bottle
0.0069
0.0088
0.0057

Cup
0.0059
0.0074
0.0055

Mug
0.0094
0.0070
0.0087

Avg.
0.0075
0.0077
0.0066

TABLE IV
POSE ESTIMATION TIME (sec.)

FPFH
Local Feature + ICP
Local Feature + FPFH

Bottle
4.603
0.055
2.719

Cup Mug
4.590
4.502
0.074
0.015
1.329
0.493

Avg.
4.565
0.048
1.514

C. Working Range Testing

Object recognition accuracy of our system is affected by
the camera-object separation. The minimum distance recom-
mended by the depth camera manufacturer is 30 cm. As the
separation increases, the quality of the depth data deteriorates
and beyond 1 m, texture details of target objects are hard to
capture. Similarly, RFID tag-reader communication is affected
by the tag-reader separation. If the separation is too large,
the power reaching the tag is too low to power the IC and
backscatter the signal to the reader. We deﬁne a score called
normalized RSSI for generalized comparison between different
material-range-signal strength experiments. Score of 1 denotes
a good backscattered signal strength of -20 dBm at the reader
and a score of 0 means signal strength is below the sensitivity
of the reader (-80 dBm).

Recognition accuracy and normalized RSSI scores are ob-
tained for different objects in this study by varying the camera-
object and reader-object separation distances (see Fig.8). From
our observations, to achieve a reliable sensing and good quality
visualization, we set an acceptable score of 0.5-1 for both the
metrics. We propose a 40-75 cm working range between the
camera & target object, and less than 100-150 cm working
range between the tagged objects & readers for good quality
and reliable visualization. One of our ultimate goals is to
package the camera and reader on to the head mount so that a
need for separate RFID infrastructure is eliminated. Therefore,
this data shows that the RFID range is suitable for this type
of application and the human-object distance is limited by the
camera.

[10] Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay E Sarma, Michael M
Bronstein, and Justin M Solomon. Dynamic graph cnn for learning
on point clouds. arXiv preprint arXiv:1801.07829, 2018.

[11] Jonathan Dyssel Stets, Yongbin Sun, Wiley Corning, and Scott W
Greenwald. Visualization and labeling of point clouds in virtual reality.
In SIGGRAPH Asia 2017 Posters, page 31. ACM, 2017.

[12] Juan Garz´on, Juan Pav´on, and Silvia Baldiris. Augmented reality
applications for education: Five directions for future research.
In
International Conference on Augmented Reality, Virtual Reality and
Computer Graphics, pages 402–414. Springer, 2017.

[13] Namho Chung, Heejeong Han, and Youhee Joun. Tourists intention to
visit a destination: The role of augmented reality (ar) application for a
heritage site. Computers in Human Behavior, 50:588–599, 2015.
[14] Junchen Wang, Hideyuki Suenaga, Kazuto Hoshi, Liangjing Yang,
Etsuko Kobayashi, Ichiro Sakuma, and Hongen Liao. Augmented
reality navigation with automatic marker-free image registration using
3-d image overlay for dental surgery. IEEE transactions on biomedical
engineering, 61(4):1295–1304, 2014.

[15] Manuel Ferdik, Georg Saxl, and Thomas Ussmueller. Battery-less
uhf rﬁd controlled transistor switch for internet of things applicationsa
In Wireless Sensors and Sensor Networks (WiSNet),
feasibility study.
2018 IEEE Topical Conference on, pages 96–98. IEEE, 2018.

[16] Ankur Agrawal, Glen J Anderson, Meng Shi, and Rebecca Chierichetti.
In Extended
Tangible play surface using passive rﬁd sensor array.
Abstracts of the 2018 CHI Conference on Human Factors in Computing
Systems, page D101. ACM, 2018.

[17] Omer Rashid, Will Bamford, Paul Coulton, Reuben Edwards, and
Jurgen Scheible. Pac-lan: mixed-reality gaming with rﬁd-enabled mobile
phones. Computers in Entertainment (CIE), 4(4):4, 2006.

[18] Ko-Chiu Wu, Chun-Ching Chen, Tzu-Heng Chiu, and I-Jen Chiang.
Transform children’s library into a mixed-reality learning environment:
Using smartwatch navigation and information visualization interfaces.
In Paciﬁc Neighborhood Consortium Annual Conference and Joint
Meetings (PNC), 2017, pages 1–8. IEEE, 2017.

[19] Andr´es Ayala, Graciela Guerrero, Juan Mateu, Laura Casades, and
Xavier Alam´an. Virtual touch ﬂystick and primbox: two case studies
In International Conference
of mixed reality for teaching geometry.
on Ubiquitous Computing and Ambient Intelligence, pages 309–320.
Springer, 2015.

[20] Mathieu Garon, Pierre-Olivier Boulet, Jean-Philippe Doironz, Luc
Beaulieu, and Jean-Franc¸ois Lalonde. Real-time high resolution 3d data
In Mixed and Augmented Reality (ISMAR-Adjunct),
on the hololens.
2016 IEEE International Symposium on, pages 189–191. IEEE, 2016.
[21] David G Lowe. Object recognition from local scale-invariant features.
In Computer vision, 1999. The proceedings of the seventh IEEE inter-
national conference on, volume 2, pages 1150–1157. Ieee, 1999.
[22] Paul J Besl and Neil D McKay. Method for registration of 3-d shapes.
In Sensor Fusion IV: Control Paradigms and Data Structures, volume
1611, pages 586–607. International Society for Optics and Photonics,
1992.

[23] James E Gentle. Matrix transformations and factorizations. Springer,

2007.

[24] Alexander Vaz, Aritz Ubarretxena, Ibon Zalbide, Daniel Pardo, H´ector
Solar, Andr´es Garcia-Alonso, and Roc Berenguer. Full passive uhf
tag with a temperature sensor suitable for human body temperature
IEEE Transactions on Circuits and Systems II: Express
monitoring.
Briefs, 57(2):95–99, 2010.

[25] Cecilia Occhiuzzi, Amin Rida, Gaetano Marrocco, and Manos Tentzeris.
Rﬁd passive gas sensor integrating carbon nanotubes. IEEE Transactions
on Microwave Theory and Techniques, 59(10):2674–2684, 2011.
[26] Azhar Hasan, Rahul Bhattacharyya, and Sanjay Sarma. Towards per-
vasive soil moisture sensing using rﬁd tag antenna-based sensors.
In
RFID Technology and Applications (RFID-TA), 2015 IEEE International
Conference on, pages 165–170. IEEE, 2015.

[27] Danilo De Donno, Luca Catarinucci, and Luciano Tarricone. A battery-
assisted sensor-enhanced rﬁd tag enabling heterogeneous wireless sensor
networks. IEEE Sensors Journal, 14(4):1048–1055, 2014.

[28] Radu Bogdan Rusu, Nico Blodow, and Michael Beetz. Fast point feature
histograms (fpfh) for 3d registration. In Robotics and Automation, 2009.
ICRA’09. IEEE International Conference on, pages 3212–3217. IEEE,
2009.

Fig. 8. Plot showing normalized detection parameters (normalized RSSI in
case of RFID and projection accuracy in case of augmented vision) from 1:
good to 0: poor; 0.5 is chosen as the safe limit for good quality detection and
rendering. Shaded regions show safe distances for object & RFID-reader and
object & depth-camera to achieve good quality results

V. CONCLUSION

We present the working of an enhanced augmented-vision
system named X-Vision which superimposes physical objects
with 3D holograms encoded with sensing information captured
from the tag-sensors attached to everyday objects. Two testing
cases, water level and temperature sensing, are demonstrated
in this paper. We also observe that the distance between the
depth camera and RFID reader with the objects is critical for
system performance. We propose a 40-75 cm range between
the camera & target object, and less than 100-150 cm between
the tagged objects & readers for good quality and reliable
visualization.

REFERENCES

[1] Otavio Good. Augmented reality language translation system and

method, April 21 2011. US Patent App. 12/907,672.

[2] Mona Singh and Munindar P Singh. Augmented reality interfaces. IEEE

Internet Computing, 17(6):66–70, 2013.

[3] Gun A Lee, Andreas D¨unser, Seungwon Kim, and Mark Billinghurst.
Cityviewar: A mobile outdoor ar application for city visualization. In
Mixed and Augmented Reality (ISMAR-AMH), 2012 IEEE International
Symposium on, pages 57–64. IEEE, 2012.

[4] SNR Kantareddy, R Bhattacharyya, and S Sarma. Towards low-cost
object tracking: Embedded rﬁd in golf balls using 3d printed masks. In
RFID (RFID), 2017 IEEE International Conference on, pages 137–143.
IEEE, 2017.

[5] SNR Kantareddy, R Bhattacharyya, and SE Sarma. Low-cost, automated
inventory control of sharps in operating theaters using passive rﬁd tag-
In RFID Technology & Application (RFID-TA), 2017 IEEE
sensors.
International Conference on, pages 16–21. IEEE, 2017.

[6] Rahul Bhattacharyya, Christian Floerkemeier, and Sanjay Sarma. Low-
cost, ubiquitous rﬁd-tag-antenna-based sensing. Proceedings of
the
IEEE, 98(9):1593–1600, 2010.

[7] Leonid Karlinsky, Joseph Shtok, Yochay Tzur, and Asaf Tzadok. Fine-
grained recognition of thousands of object categories with single-
example training. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pages 4113–4122, 2017.

[8] Herbert Bay, Tinne Tuytelaars, and Luc Van Gool. Surf: Speeded up
robust features. In European conference on computer vision, pages 404–
417. Springer, 2006.

[9] Yu Xiang, Tanner Schmidt, Venkatraman Narayanan, and Dieter Fox.
Posecnn: A convolutional neural network for 6d object pose estimation
in cluttered scenes. arXiv preprint arXiv:1711.00199, 2017.

