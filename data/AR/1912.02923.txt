Generating 3D People in Scenes without People

Yan Zhang*1,3, Mohamed Hassan2, Heiko Neumann3, Michael J. Black2, Siyu Tang*1
1 ETH Z¨urich, Switzerland
2Max Planck Institute for Intelligent Systems, T¨ubingen, Germany
3Institute of Neural Information Processing, Ulm University, Germany

0
2
0
2

r
p
A
9
1

]

V
C
.
s
c
[

3
v
3
2
9
2
0
.
2
1
9
1
:
v
i
X
r
a

Figure 1: 3D human bodies with various shapes and poses are automatically generated to interact with the scene. Appropriate
human-scene contact is encouraged, and human-scene surface interpenetration is discouraged.

Abstract

We present a fully automatic system that takes a 3D scene
and generates plausible 3D human bodies that are posed
naturally in that 3D scene. Given a 3D scene without peo-
ple, humans can easily imagine how people could interact
with the scene and the objects in it. However, this is a chal-
lenging task for a computer as solving it requires that (1)
the generated human bodies to be semantically plausible
within the 3D environment (e.g. people sitting on the sofa
or cooking near the stove), and (2) the generated human-
scene interaction to be physically feasible such that the hu-
man body and scene do not interpenetrate while, at the same
time, body-scene contact supports physical interactions. To
that end, we make use of the surface-based 3D human model
SMPL-X. We ﬁrst train a conditional variational autoen-
coder to predict semantically plausible 3D human poses

conditioned on latent scene representations, then we further
reﬁne the generated 3D bodies using scene constraints to
enforce feasible physical interaction. We show that our ap-
proach is able to synthesize realistic and expressive 3D hu-
man bodies that naturally interact with 3D environment. We
perform extensive experiments demonstrating that our gen-
erative framework compares favorably with existing meth-
ods, both qualitatively and quantitatively. We believe that
our scene-conditioned 3D human generation pipeline will
be useful for numerous applications; e.g. to generate train-
ing data for human pose estimation, in video games and in
VR/AR. Our project page for data and code can be seen at:
https://vlg.inf.ethz.ch/projects/PSI/.

∗ This work was performed when Y. Z. and S. T. were at MPI-IS and

University of T¨ubingen.

 
 
 
 
 
 
1. Introduction

In recent years, many high-quality datasets of 3D indoor
scenes have emerged such as Matterport3D [3], Replica
[42], and Gibson [47], which employ 3D scanning and
reconstruction technologies to create digital 3D environ-
ments. Also, virtual robotic agents exist inside of 3D en-
vironments such as Gibson [47] and the Habitat simulator
[32]. These are used to develop scene understanding meth-
ods from embodied views, thus providing platforms for in-
door robot navigation, AR/VR, computer games and many
other applications. Despite this progress, a signiﬁcant limi-
tation of these environments is that they do not contain peo-
ple. The reason such worlds contain no people is that there
are no automated tools to generate realistic people interact-
ing realistically with 3D scenes, and manually doing this re-
quires signiﬁcant artist effort. Consequently, our goal is to
automatically generate natural and realistic 3D human bod-
ies in the scene. The generated human bodies are expected
to be physically plausible (e.g. neither ﬂoating nor inter-
penetrating), diverse, and posed naturally within the scene.
This is a step towards equipping high-quality 3D scenes and
simulators (e.g. Matterport3D [3] and Habitat [32]) with se-
mantically and physically plausible 3D humans, and is es-
sential for numerous applications such as creating synthetic
datasets, VR/AR, computer games, etc.

Our solution is inspired by how humans infer plausi-
ble interactions with the environment. According to the
studies of [49], humans tend to propose interaction plans
depending on the structure and the semantics of objects.
Afterwards, to realize the interaction plan, physical rules
will apply to determine the detailed human-object conﬁgu-
ration, while guaranteeing that the human body can neither
ﬂoat in the air nor collide into the objects. Therefore, our
method has two steps: (1) We propose a generative model
of human-scene interaction using a conditional variational
autoencoder (CVAE) [39] framework. Given scene depth
and semantics, we can sample from the CVAE to obtain
various human bodies. (2) Next, we transform the gener-
ated 3D human body to the world coordinates and perform
scene geometry-aware ﬁtting, so as to reﬁne the human-
scene interaction and eliminate physically implausible con-
ﬁgurations (e.g. ﬂoating and collision).

We argue that realistically modeling human-scene inter-
actions requires a realistic model of the body. Previous stud-
ies on scene affordance inference and human body synthe-
sis in the literature, like [28, 46, 57], represent the body as
a 3D stick ﬁgure or coarse volume. This prevents detailed
reasoning about contact such as how the leg surface contacts
the sofa surface. Without a model of body shape, it is not
clear whether the estimated body poses correspond to plau-
sible human poses. To overcome these issues, we use the
SMPL-X model [36], which takes a set of low-dimensional
body pose and shape parameters and outputs a 3D body

mesh with important details like the ﬁngers. Since SMPL-
X is differentiable, it enables straightforward optimization
of human-scene contact and collision prevention [18]. In
addition, we incorporate the body shape variation in our ap-
proach, so that our generated human bodies have various
poses and shapes.

To train our method we exploit the PROX-Qualitative
dataset [18], which includes 3D people captured moving
in 3D scenes. We extend this by rendering images, scene
depth, and semantic segmentation of the scene from many
virtual cameras. We conduct extensive experiments to eval-
uate the performance of different models for scene-aware
3D body mesh generation. For testing, we extract 7 dif-
ferent rooms from the Matterport3D [3] dataset and use a
virtual agent in the Habitat Simulator [32] to capture scene
depth and semantics from different views. Based on prior
work, e.g. [28, 46], we propose three metrics to evaluate the
diversity, the physical plausibility, and the semantic plausi-
bility of our results. The experimental results show that our
solution effectively generates 3D body meshes in the scene,
and outperforms the modiﬁed version of a state-of-the-art
body generation method [28]. We will make our datasets
and evaluation metrics available to establish a benchmark.
Our trained model learns about the ways in which 3D
people interact with 3D scenes. We show how to leverage
this in the form of a scene-dependent body pose prior and
show how to use this to improve 3D body pose estimation
from RGB images. In summary, our contributions are as
follows: (1) We present a solution to generate 3D human
bodies in scenes, using a CVAE to generate a body mesh
with semantically plausible poses. We follow this with
scene-geometry-aware ﬁtting to reﬁne the human-scene in-
teraction. (2) We extend and modify two datasets, and pro-
pose three evaluation metrics for scene-aware human body
generation. We also modify the method of [28] to generate
body meshes as the baseline (see Sec. 4.1.2). The experi-
mental results show that our method outperforms the base-
line. (3) We show that our human-scene interaction prior is
able to improve 3D pose estimation from RGB images.

2. Related work

Multiple studies focus on placing objects in an image
so that they appear natural [11, 27, 29, 34]. For example,
[11, 43, 45] use contextual information to predict which ob-
jects are likely to appear at a given location in the image.
Lin et al. [29] apply homography transformations to 2D ob-
jects to approximate the perspectives of the object and back-
ground. Tan et al. [44] predict likely positions for people in
an input image and retrieve a person that semantically ﬁts
into the scene from a database. Ouyang et al. [34] use a
GAN framework to synthesize pedestrians in urban scenes.
Lee et al. [27] learn where to place objects or people in a
semantic map and then determine the pose and shape of the

respective object. However, all these methods are limited
to 2D image compositing or inpainting. Furthermore, the
methods that add synthetic humans do not take interactions
between the humans and world into account.

To model human-object or human-scene interactions it
is beneﬁcial to know which interactions are possible with
a given object. Such opportunities for interactions are re-
ferred to as affordances [14] and numerous works in com-
puter vision have made use of this concept [7, 8, 15, 17,
21, 25, 24, 28, 38, 46, 56, 57]. Object affordance is of-
ten represented by a human pose when interacting with a
given object [8, 15, 17, 21, 28, 38, 46, 56, 57]. For exam-
ple, [15, 17, 57] search for valid positions of human poses
in 3D scenes. Delataire et al. [8] learn associations between
objects and human poses in order to improve object recogni-
tion. Given a 3D model of an object Kim et al. [21] predict
human poses interacting with the given object. Given an
image of an object Zhu et al. [56] learn a knowledge base
to predict a likely human pose and a rough relative loca-
tion of the object with respect to the pose. Savva et al. [38]
learn a model connecting human poses and arrangement of
objects in a 3D scene that can generate snapshots of object
interaction given a corpus of 3D objects and a verb-noun
pair. Monszpart et al. [33] use captured human motion to
infer the objects in the scene and their arrangement. Sava
et al. [37] predict action heat maps that highlight the likeli-
hood of an action in the scene. Recently, Chen et al. [5] pro-
pose to tackle scene parsing and 3D pose estimation jointly
and to leverage their coupled nature to improve scene un-
derstanding. Chao et al. [4] propose to train multiple con-
trollers to imitate simple motions from mocap, and then use
hierarchical reinforcement learning (RL) to achieve higher-
level interactive tasks. The work of Zanﬁr et al. [50] ﬁrst
estimates the ground plane in the image, and requires a fore-
ground person image as input. The above methods do not
use a realistic body model to represent natural and detailed
human-environment interactions.

Recently, Wang et al. [46] published an affordance
dataset large enough to obtain reliable estimates of the prob-
abilities of poses and to train neural networks on affordance
prediction. The data is collected from multiple sitcoms and
contains images of scenes with and without humans. The
images with humans contain rich behavior of humans in-
teracting with various objects. Given an image and a lo-
cation as input, Wang et al. ﬁrst predict the most likely
pose from a set of 30 poses. This pose is deformed and
scaled by a second network to ﬁt it into the scene. Li et
al. [28] extend this work to automatically estimate where to
put people and to predict 3D poses. To acquire 3D training
data they map 2D poses to 3D poses and place them in 3D
scenes from the SUNCG dataset [40, 51]. This synthesized
dataset is cleaned by removal of all predictions intersect-
ing with the 3D scene or without sufﬁcient support for the

body. The methods of [28, 46] are limited in their gener-
alization, since they require a large amount of paired data
and manual cleaning of the pose detections. Such a large
amount of data might be hard to acquire for scenes that are
less frequently covered by sitcoms, or in the case of [28] in
3D scene datasets. Furthermore, both methods only predict
poses represented as stick ﬁgures. Such a representation is
hard to validate visually, lacks details, and can not directly
be used to generate realistic synthetic data of humans inter-
acting with an environment.

3. Methods

3.1. Preliminaries

3D scene representation. We represent the scene from the
view of an embodied agent, as in the Habitat simulator [32].
According to [52], which indicates that the depth and se-
mantic segmentation are the most valuable modalities for
scene understanding, we capture scene depth and semantics
as our scene representation. For each view, we denote the
stack of depth and semantics as xs, the camera perspective
projection from 3D to 2D as π(·), and its inverse opera-
tion as π−1(·) for 3D recovery. Our training data, xs, are
generated from Habitat and we resize this to 128 × 128 for
compatibility with our network; we retain the aspect ratio
and pad with zeros where needed. The 3D-2D projection
π(·) normalizes the 3D coordinates to the range of [−1, 1],
using the camera intrinsics and the maximal depth value.
Note that each individual xs is from a single camera view.
We do not use multi-view data in our work.

3D human body representation. We use SMPL-X [36] to
represent the 3D human body. SMPL-X can be regarded as
a function M(·), mapping a group of low-dimensional body
features to a 3D body mesh. The 3D body mesh has 10475
vertices and a ﬁxed topology. In our study, we use the body
shape feature β ∈ R10, the body pose feature θb ∈ R32,
and the hand pose feature θh ∈ R24. The body pose feature
θb is represented in the latent space of VPoser [36], which
is a variational autoencoder trained on a large-scale motion
capture dataset, AMASS [31]. The global rotation R, i.e.,
the rotation of the pelvis, is represented by a 6D continuous
rotation feature [55], which facilitates back-propagation in
our trials. The global translation t is represented by a 3D
vector in meters. The global rotation and translation is with
respect to the camera coordinates. Based on the camera ex-
trinsics, T w
c , one can transform the 3D body mesh to the
world coordinates.

We denote the joint body representation as xh :=
(t, R, β, θb, θh)T ∈ R75; i.e., the concatenation of individ-
ual body features. When processing the global and the local
features separately as in [28], we denote the global transla-
tion as xg

h, and the other body features as xl
h.

where the terms denote the reconstruction loss, the Kull-
backLeibler divergence, the VPoser loss, the human-scene
contact loss and the human-scene collision loss, respec-
tively. The set of α’s denotes the loss weights. For sim-
plicity, we denote αcontLcontact + αcollLcollision as LHS,
implying the loss for human-scene interaction.
Reconstruction loss Lrec: It is given by Lrec =

|xg

h − xg,rec

h

h) − π(xg,rec

h

| + |π(xg
2

)|

+ |xl

h − xl,rec

h

|, (2)

Figure 2: Network diagrams of our models. The trape-
zoids denote the scene encoders, which are ﬁne-tuned from
a pre-trained ResNet18 network. The blue rectangles de-
note fully-connected (fc) layers.
In the residual blocks,
Leaky-ReLU [30] is employed between fc layers. The or-
ange dashed arrows denote the sampling operation in the
VAE re-parameterization trick [23]. The blocks with “cat”
denote the feature concatenation operation.

3.2. Scene context-aware Human Body Generator

3.2.1 Network architecture

We employ a conditional variational autoencoder (CVAE)
[39] framework to model the probability p(xh|xs). When
inferring all body features jointly, we propose a one-stage
(S1) network. When inferring xg
h and xl
h successively, we
h, xs)p(xg
h|xg
factorize the probability as p(xl
h|xs) and use a
two-stage (S2) network. The network architectures are il-
lustrated in Fig. 2. Referring to [28], our scene encoder is
ﬁne-tuned from the ﬁrst 6 convolutional layers in ResNet18
[19], which is pre-trained on ImageNet [9]. The human fea-
ture xh is ﬁrst lifted to a high dimension (256 in our study)
via a fully-connected layer, and then concatenated with the
encoded scene feature.
In the two-stage model, the two
scene encoders are both ﬁne-tuned from ResNet18, but do
not share parameters. After the ﬁrst stage, the reconstructed
body global feature xg,rec
is further encoded, and is used in
the second stage to infer the body local features.

h

3.2.2 Training loss

The entire training loss can be formulated as

L = Lrec + αklLKL + αvpLVPoser
+ αcontLcontact + αcollLcollision,

(1)

in which the global translation, the projected and normal-
ized global translation, and the other body features are con-
sidered separately. We apply this reconstruction loss in both
our S1 model and our S2 model.
KL-Divergence LKL: Denoting our VAE encoder as
q(zh|xh), the KL-divergence loss is given by

LKL = DKL (q(zh|xh) || N (0, I)) .

(3)

Correspondingly, in our S2 model the KL-divergence loss
is given by LKL =

h|xl

h|xg

(cid:0)q(zl

DKL (q(zg

h)||N (0, I)) + DKL

h)||N (0, I)(cid:1) .
(4)
We use the re-parameterization trick in [23] so that the KL
divergence is closed form.
VPoser loss LV P oser: Since VPoser [36] attempts to en-
code natural poses with a normal distribution in its latent
space, like in [36] and [18], we employ the VPoser loss, i.e.

LV P oser = |θrec

b

|2,

(5)

to encourage the generated bodies to have natural poses.
Collision loss Lcollision: Based on the model output xrec
h ,
we generate the body mesh and transform it to world co-
ordinates. Then, we compute the negative signed-distance
values at the body mesh vertices given the negative signed
distance ﬁeld (SDF) Ψ−

s (·), and minimize

Lcoll = E (cid:2)|Ψ−

s (T w

c M(xrec

h )) |(cid:3) .

(6)

indicating the average of absolute values of negative SDFs
on the body.
Contact loss Lcontact: Following [18], we encourage con-
tact between the body mesh and the scene mesh. Hence, the
contact loss is written as

Lcontact =

(cid:88)

vc∈C(T w

c M(xrec

h ))

min
vs∈Ms

ρ(|vc − vs|),

(7)

in which C(·) denotes selecting the body mesh vertices for
contact according to the annotation in [18], Ms denotes
the scene mesh, and ρ(·) denotes the Geman-McClure er-
ror function [13] for down-weighting the inﬂuence of scene
vertices far away from the body mesh.

catcatx2x2catcatx2x2catcatx2x2one-stage (S1) networktwo-stage (S2) network3.3. Scene geometry-aware Fitting

We reﬁne the body meshes with an optimization step
similar to [18].
It encourages contact and helps to avoid
inter-penetration between the body and the scene surfaces,
while not deviating much from the generated pose. Let the
generated human body conﬁguration be x0
h. To reﬁne this,
we minimize a ﬁtting loss taking into account the scene ge-
ometry, i.e.

Lf (xh) = |xh − x0

h| + α1Lcontact + α2Lcollision

+ α3LV P oser,

(8)

in which the α’s denote the loss weights; the loss terms are
deﬁned above.

3.4. Implementation

Our implementation is based on PyTorch v1.2.0 [35].
For the Chamfer distance in the contact loss we use the
same implementation as [10, 16]. For training, we set
{αkl, αvp} = {0.1, 0.001} in Eq. 1 for both our S1
and S2 models, in which αkl increases linearly in an an-
nealing scheme [1]. When additionally using LHS, we
set {αcont, αcoll} = {0.001, 0.01}, and enable it after
75% training epochs to improve the interaction model-
ing. We use the Adam optimizer [22] with the learning
rate 3e−4, and terminate training after 30 epochs. For
the scene geometry-aware ﬁtting, we set {α1, α2, α3} =
{0.1, 0.5, 0.01} in all cases. Our data, code and models will
be available for research purposes.

4. Experiments 1

4.1. Scene-aware 3D Body Mesh Generation

4.1.1 Datasets

PROX-E: The PROX-E dataset (pronounced “proxy”) is
extended from the PROX-Qualitative (PROX-Q) dataset
[18], which records how people interact with various in-
door environments. In PROX-Q, 3D human body meshes
in individual frames are estimated by ﬁtting the SMPL-X
body model to the RGB-D data subject to scene constraints
[18]. We use this data as pseudo-ground truth in our study,
and extend PROX-Q in three ways: (1) We manually build
up virtual walls, ﬂoors and ceilings to enclose the original
open scans and simulate real indoor environments. (2) We
manually annotate the mesh semantics following the object
categorization of Matterport3D [3]. (3) We down-sample
the original recordings and extract frames every 0.5 sec-
onds. In each frame, we set up virtual cameras with various
poses to capture scene depth and semantics. The optical
axis of each virtual camera points towards the human body,

Figure 3:
Illustration of how we extend the PROX-
Qualitative dataset [18] to PROX-E. In the rows of PROX-
Qualitative, a video frame, a body-scene mesh and a depth
map are shown from left to right. In the rows of PROX-E,
the virtual camera setting, the mesh with semantics, and the
completed depth map are shown from left to right.

and then Gaussian noise is applied on the camera transla-
tion. To avoid severe occlusion, all virtual cameras are lo-
cated above half of the room height and below the virtual
ceiling. As a result, we obtain about 70K frames in total.
We use ‘MPH16’, ‘MPH1Library’, ‘N0SittingBooth’ and
‘N3OpenArea’ as test scenes, and use samples from other
scenes for training. See Fig. 3.
MP3D-R: This name denotes “rooms in Matterport3D [3]”.
From the architecture scans of Matterport3D, we extract 7
different rooms according to the annotated bounding boxes.
In addition, we create a virtual agent using the Habitat simu-
lator [32], and manipulate it to capture snapshots from var-
ious views in each room. We employ the RGB, the depth
and the semantics sensor on the agent. These sensors are of
height 1.8m from the ground, and look down at the scene;
these are in a similar range as the virtual cameras in PROX-
E. For each snapshot, we also record the extrinsic and in-
trinsic parameters of the sensors. As a result, we obtain
32 snapshots in all 7 rooms. Moreover, we follow the same
procedure as in PROX-Qualitative [18] to calculate the SDF
of the scene mesh. Our MP3D-R is illustrated in Fig. 4.

4.1.2 Baseline

To our knowledge, the most related work is Li et al. [28],
which proposes a generative model to put 3D body stick
ﬁgures into images2. For fair comparison, we modify their
method to use SMPL-X to generate body meshes in 3D

2The data and the pre-trained model in [28] are based on SUNCG [41],

1Please see appendix for more details.

and not publicly available.

PROX-QualitativePROX-EPROX-QualitativePROX-ETable 1: Comparison between models, in which “+LHS”
denotes the model is trained with that human-scene interac-
tion loss (Sec. 3.2.2). The best results are in boldface.

model

baseline [28]
S1
S1 + LHS
S2
S2 + LHS

rec. err.

−logP (x)

val

0.52
0.22
0.16
0.24
0.20

test

0.48
0.25
0.24
0.70
0.23

val

0.98
0.23
0.27
0.25
0.30

test

0.72
0.41
0.36
0.49
0.39

4.1.4 Evaluation: 3D body mesh generation

Given a 3D scene, our goal is to generate diverse, physically
and semantically plausible 3D human bodies. Based on [28,
46], we propose to quantitatively evaluate our method using
a diversity metric and a physical metric. Also, we perform
a user perceptual study to measure the semantic plausibility
of the generated human bodies.

The quantitative evaluation is based on the PROX-E and
the MP3D-R dataset. When testing on PROX-E, we train
our models using all samples in the training scenes, and
generate body meshes using the real camera snapshots in
the testing scenes. For each individual model and each test
scene, we randomly generate 1200 samples, and hence ob-
tain 4800 samples. When testing on MP3D-R, we use all
samples from PROX-E to train the models. For each snap-
shot and each individual model, we randomly generate 200
samples, and hence obtain 6400 samples.
(1) Diversity metric: This metric aims to evaluate how di-
verse the generated human bodies are. Speciﬁcally, we em-
pirically perform K-means to cluster the SMPL-X parame-
ters of all the generated human bodies to 20 clusters. Then,
we compute the entropy (a.k.a Shannon index, a type of di-
versity index) of the cluster ID histogram of all the samples.
We also compute the average size of all the clusters.

A higher value indicates that the generated human bod-
ies are more diverse in terms of their global positions, their
body shapes and poses. We argue that this metric is essen-
tial for evaluating the quality of the generated bodies and
should always be considered together with other metrics.
For instance, a posterior-collapsed VAE, which always gen-
erates an identical body mesh, could lead to a low diversity
score but superior performance according to the physical
metric and the semantic metric.

The results are shown in Tab. 2. Overall, our methods
consistently outperform the baseline. Notably, our methods
increase the average cluster size of the generated samples by
large margins, indicating that the generated human bodies
are much more diverse than those from the baseline.
(2) Physical metric: From the physical perspective, we
evaluate the collision and the contact between the body
mesh and the scene mesh. Given a scene SDF and a SMPL-

Figure 4: The left column shows two rooms in MP3D-R.
The right column shows snapshots captured by the Habitat
virtual agent [32] from different views, which contain RGB
images, depth maps and scene semantics.

scenes. Speciﬁcally, we make the following modiﬁcations:
(1) We change the scene representation from RGB (or RGB-
D) to depth and semantics like ours to improve generaliza-
tion. (2) During training, we perform K-means to cluster
the VPoser pose features of training samples to generate
the pose class. (3) The where module is used to infer the
global translation, and the what module infers other SMPL-
X parameters. (4) For training the geometry-aware discrim-
inator, we project the body mesh vertices, rather than the
stick ﬁgures, to the scene depth maps. We train the modiﬁed
baseline model using PROX-E with the default architecture
and loss weights in [28]. Moreover, we combine the modi-
ﬁed baseline method with our scene geometry-aware ﬁtting
in our experiments.

4.1.3 Evaluation: representation power

Here we use PROX-E to investigate how well the pro-
posed network architectures represent human-scene interac-
tion. We train all models using samples from virtual cam-
eras in training scenes, validate them using samples from
real cameras in training scenes, and test them using samples
from real cameras in test scenes. For quantitative evalua-
tion, we feed individual test samples to our models, and re-
port the mean of the reconstruction errors, and the negative
evidenced lower bound (ELBO), i.e. −logP (X), which is
the sum of the reconstruction error and the KL divergence.
For fair comparison, the reconstruction error of all models
is based on Lrec in Eq. 2. As shown in Tab. 1, our models
outperform the baseline model on both validation and test
set by large margins. The metrics on the validation and the
test sets are comparable, indicating that our virtual camera
approach is effective in preventing severe over-ﬁtting on the
seen environments.

Table 2: Comparison between different models according
to the diversity metric. The best results for each metric
are in boldface. “S1” and “S2” denote our stage-1 and
stage-2 architecture, respectively. “+ LHS” denotes that the
model is trained with the human-scene interaction loss (see
Sec. 3.2.2). “+Lf ” denotes the results are after the scene-
aware ﬁtting process (see Sec. 3.3).

cluster ID entropy

cluster size average

model

PROX-E MP3D-R

PROX-E MP3D-R

baseline [28]
S1
S1 + LHS
S2
S2 + LHS

baseline + Lf
S1 + Lf
S1 + LHS + Lf
S2 + Lf
S2 + LHS + Lf

2.89
2.96
2.93
2.97
2.96

2.93
2.97
2.94
2.94
2.91

2.93
2.99
2.99
2.91
2.89

2.92
2.98
2.96
2.87
2.90

1.49
2.51
2.40
2.46
2.22

1.52
2.53
2.43
2.48
2.26

1.84
2.81
2.73
2.85
2.90

1.94
2.86
2.79
2.91
2.95

X body mesh, we propose a non-collision score, which is
calculated as the number of body mesh vertices with pos-
itive SDF values divided by the number of all body mesh
vertices (10475 for SMPL-X). Simultaneously, if any body
mesh vertex has a non-positive SDF value, then the body
has contact with the scene. Then, for all generated body
meshes, the non-collision score is the ratio of all body ver-
tices in the free space, and the contact ratio is the calculated
as the number of body meshes with contact divided by all
generated body meshes. Therefore, due to the physical con-
straints, a higher non-collision score and contact ratio indi-
cate a better generation, in analogy with precision and recall
in an object detection task.

The results are presented in Tab. 3. First, one can see that
our proposed methods consistently outperform the baseline
for the physical metric. The inﬂuence of the LHS loss on
3D body generation is not as obvious as on the interaction
modeling task (see Tab. 1). Additionally, one can see that
the scene geometry-aware ﬁtting consistently improves the
physical metric, since the ﬁtting process aims to improve
the physical plausibility. Fig. 7 shows some generated ex-
amples before and after the ﬁtting.
(3) User study: In our study, we render our generated re-
sults as images, and upload them to Amazon Mechanical
Turk (AMT) for a user study. Due to the superior perfor-
mance of our S1 model without LHS, we compare it with
the baseline, as well as ground truth if it exists. For each
scene and each model, we generate 100 and 400 bodies in
PROX-E and MP3D-R, respectively, and ask Turkers to
give a score between 1 (strongly not natural) and 5 (strongly
natural) to each individual result. The user study details
are in the appendix. Also, for each scene in the PROX-
E dataset, we randomly select 100 frames from the ground

Table 3: Comparison between different models according
to the physical metric. The best results are in boldface.

non-collision score

contact score

model

PROX-E MP3D-R

PROX-E MP3D-R

baseline [28]
S1
S1 + LHS
S2
S2 + LHS

baseline + Lf
S1 + Lf
S1 + LHS + Lf
S2 + Lf
S2 + LHS + Lf

0.89
0.93
0.89
0.91
0.89

0.93
0.94
0.92
0.94
0.93

0.92
0.94
0.95
0.93
0.95

0.97
0.97
0.98
0.97
0.97

0.93
0.95
0.88
0.88
0.88

0.99
0.99
0.99
0.99
0.99

0.78
0.80
0.65
0.79
0.56

0.89
0.88
0.81
0.88
0.81

Table 4: Comparison between models in the user study
score (1-5). The best results for each metric are in boldface.

use study score w.r.t. mean±std

model

PROX-E

MP3D-R

baseline [28]
baseline + Lf
S1
S1 + Lf

3.31 ± 1.39
3.32 ± 1.35
3.29 ± 1.36
3.49 ± 1.26

3.14 ± 1.41
3.35 ± 1.38
3.15 ± 1.40
3.30 ± 1.30

ground truth

4.04 ± 1.03

n/a

truth [18], and ask Turkers to evaluate them as well.

The results are presented in Tab. 4. Not surprisingly, the
ground-truth samples achieve the best score from the user
study. We observe that the geometry-aware ﬁtting improves
the performance both for the baseline and our model, most
likely due to the improvement of the physical plausibility.
Note that, although the baseline and our model achieve sim-
ilar average scores, the diversity of our generated samples
is much higher (Tab. 2). This indicates that, compared to
the baseline, our method generates more diverse 3D human
bodies, while being equally good in terms of semantic plau-
sibility given a 3D scene.

Qualitative results are shown in Fig. 5, Fig. 6 and Fig. 8.

More results are in the appendix.

4.2. Scene-aware 3D Body Pose Estimation

Here we perform a down-stream application and show
our model
improves 3D human pose estimation from
monocular images. Given a RGB image of a scene with-
out people, we estimate the depth map using the pre-trained
model [26], and perform semantic segmentation using the
model of [6] pre-trained on the ADE20K [53] dataset. To
unify the semantics, we create a look-up table to convert the
object IDs from ADE20K to Matterport3D. Next, we feed
the estimated depth and semantics to our S1 model with
LHS and randomly generate 100 bodies. We compute the
average of the pose features in the VPoser latent space, and
denote it as θs
b .

Table 5: Results of 3D pose estimation from RGB frames
in PROX-Quantitative, in which “PJE”/“p.PJE” denote
the mean per-joint error without/with Procrustes alignment,
and “V2V”/“p.V2V” denote the mean vertex-to-vertex error
without/with Procrustes alignment, respectively.

Error (in millimeters)

method

PJE

V2V

p.PJE

p.V2V

Simplify-X [36]
PROX [18]
Ours

223.83
171.78
174.10

225.60
173.97
171.75

73.28
73.20
71.73

62.93
64.76
62.64

b |2. We eval-
VPoser term in [18, Eq. 7] from |θb|2 to |θb − θs
uate the performance using the PROX-Quantitative dataset
[18]. We derive the 2D keypoints from the frames via Al-
phaPose [12, 48], and obtain a θs
b from a background image
without people. Then, we use the same optimization meth-
ods and the evaluation metric in [18] for fair comparison.
The results are shown in Tab. 5. We ﬁnd that our method
improves 3D pose estimation on the PROX-Quantitative
dataset. This suggests that our model learns about the ways
in which 3D people interact with 3D scenes. Leveraging it
as a scene-dependent body pose prior can improve 3D body
pose estimation from RGB images.

5. Conclusion

In this work, we introduce a generative framework to
produce 3D human bodies that are posed naturally in the 3D
environment. Our method consists of two steps: (1) A scene
context-aware human body generator is proposed to learn a
distribution of 3D human pose and shape, conditioned on
the scene depth and semantics; (2) geometry-aware ﬁtting
is employed to impose physical plausibility of the human-
scene interaction. Our experiments demonstrate that the au-
tomatically synthesized 3D human bodies are realistic and
expressive, and interact with 3D environment in a semantic
and physical plausible way.
Acknowledgments. We sincerely acknowledge: Joachim
Tesch for all his work on graphics supports. Xueting Li
for implementation advice about the work [28]. David
Hoffmann, Jinlong Yang, Vasileios Choutas, Ahmed Os-
man, Nima Ghorbani and Dimitrios Tzionas for insight-
ful discussions. Daniel Scharstein and Cornelia K¨ohler for
proof reading. Benjamin Pellkofer and Mason Landry for
IT/hardware supports. Y. Z. and S. T. acknowledge funding
by Deutsche Forschungsgemeinschaft (DFG, German Re-
search Foundation) Projektnummer 276693517 SFB 1233.
Disclosure. MJB has received research gift funds from In-
tel, Nvidia, Adobe, Facebook, and Amazon. While MJB
is a part-time employee of Amazon, his research was per-
formed solely at MPI. He is also an investor in Meshcapde
GmbH.

Figure 5: Generated human bodies in two test scenes of
PROX-E. Results are visualized in two views.

Figure 6: Generated results in three scenes of MP3D-R.

Figure 7: Results before and after the scene geometry-aware
ﬁtting.

Figure 8: Two typical failure cases in our results.

When performing 3D pose estimation in the same scene,
we follow the optimization framework of SMPlify-X [36]
and PROX [18]. In contrast to these two methods, we use
our derived θs
b to initialize the optimization, and change the

beforeafterFailure cases1.The generated body poses are not always plausible, such as collision with the scene mesh, floating in the air, etc.2.In the post-processing phase, the contact loss pulls the body mesh according to the closest vertices in the scene, which is perhaps not meaningful. 3.The collision loss is dependent on the scene SDF, which is not always accurate and pulls the person to implausible configurations.failed generationfailed scene geometry-aware fittingReferences

[1] Samuel R Bowman, Luke Vilnis, Oriol Vinyals, Andrew M
Dai, Rafal Jozefowicz, and Samy Bengio. Generating sen-
tences from a continuous space. In Proceedings of the 20th
Conference on Computational Natural Language Learning
(SIGNLL), 2016. 5
[2] Christopher P Burgess,

Irina Higgins, Arka Pal, Loic
Matthey, Nick Watters, Guillaume Desjardins, and Alexan-
der Lerchner. Understanding disentangling in β -VAE. arXiv
preprint arXiv:1804.03599, 2018. 15

[3] Angel Chang, Angela Dai, Thomas Funkhouser, Maciej Hal-
ber, Matthias Niessner, Manolis Savva, Shuran Song, Andy
Zeng, and Yinda Zhang. Matterport3D: Learning from RGB-
D data in indoor environments. In Proceedings of the Inter-
national Conference on 3D Vision (3DV), 2017. 2, 5, 12,
13

[4] Yu-Wei Chao, Jimei Yang, Weifeng Chen, and Jia Deng.
Learning to sit: Synthesizing human-chair interactions via
hierarchical control. arXiv preprint arXiv:1908.07423, 2019.
3

[5] Yixin Chen, Siyuan Huang, Tao Yuan, Siyuan Qi, Yixin
Zhu, and Song-Chun Zhu. Holistic++ scene understanding:
Single-view 3D holistic scene parsing and human pose esti-
mation with human-object interaction and physical common-
sense. In Proceedings of the IEEE International Conference
on Computer Vision (ICCV), 2019. 3

[6] Franc¸ois Chollet. Xception: Deep learning with depthwise
separable convolutions. In Proceedings of the IEEE Confer-
ence on Computer Vision and Pattern Recognition (CVPR),
2017. 7

[7] Ching-Yao Chuang, Jiaman Li, Antonio Torralba, and Sanja
Fidler. Learning to act properly: Predicting and explain-
In Proceedings of the IEEE
ing affordances from images.
Conference on Computer Vision and Pattern Recognition
(CVPR), 2018. 3

[8] Vincent Delaitre, David F Fouhey, Ivan Laptev, Josef Sivic,
Abhinav Gupta, and Alexei A Efros. Scene semantics from
long-term observation of people. In Proceedings of the Eu-
ropean Conference on Computer Vision (ECCV), 2012. 3
[9] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
ImageNet: A large-scale hierarchical im-
and Li Fei-Fei.
In Proceedings of the IEEE Conference on
age database.
Computer Vision and Pattern Recognition (CVPR), 2009. 4
[10] Theo Deprelle, Thibault Groueix, Matthew Fisher,
Vladimir G Kim, Bryan C Russell, and Mathieu Aubry.
Learning elementary structures for 3D shape generation
In Proceedings of the Advances in Neural
and matching.
Information Processing Systems (NeurIPS), 2019. 5

[11] Nikita Dvornik, Julien Mairal, and Cordelia Schmid. On the
importance of visual context for data augmentation in scene
understanding. IEEE Transactions on Pattern Analysis and
Machine Intelligence (TPAMI), PP:1–1, 12 2019. 2

[12] Hao-Shu Fang, Shuqin Xie, Yu-Wing Tai, and Cewu Lu.
RMPE: Regional multi-person pose estimation. In Proceed-
ings of the IEEE International Conference on Computer Vi-
sion (ICCV), 2017. 8

[13] Stuart Geman and Donald E. McClure. Statistical methods
for tomographic image reconstruction. In Proceedings of the
46th Session of the International Statistical Institute, Bulletin
of the ISI, volume 52, 1987. 4

[14] James J Gibson. The ecological approach to visual percep-

tion: classic edition. Psychology Press, 2014. 3

[15] Helmut Grabner, Juergen Gall, and Luc Van Gool. What
makes a chair a chair? In Proceedings of the IEEE Confer-
ence on Computer Vision and Pattern Recognition (CVPR),
2011. 3

[16] Thibault Groueix, Matthew Fisher, Vladimir G. Kim, Bryan
Russell, and Mathieu Aubry. 3D-CODED: 3D correspon-
dences by deep deformation. In Proceedings of the European
Conference on Computer Vision (ECCV), 2018. 5

[17] Abhinav Gupta, Scott Satkin, Alexei A Efros, and Martial
Hebert. From 3D scene geometry to human workspace. In
Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), 2011. 3

[18] Mohamed Hassan, Vasileios Choutas, Dimitrios Tzionas,
and Michael J. Black. Resolving 3D human pose ambigu-
ities with 3D scene constraints. In Proceedings of the IEEE
International Conference on Computer Vision (ICCV), 2019.
2, 4, 5, 7, 8, 12, 13, 15, 16

[19] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Proceed-
ings of the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), 2016. 4

[20] Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess,
Xavier Glorot, Matthew Botvinick, Shakir Mohamed, and
Alexander Lerchner. β-VAE: Learning basic visual concepts
with a constrained variational framework. In Proceedings of
the International Conference on Learning Representations
(ICLR), 2017. 15

[21] Vladimir G Kim, Siddhartha Chaudhuri, Leonidas Guibas,
and Thomas Funkhouser. Shape2Pose: Human-centric shape
analysis. ACM Transactions on Graphics (TOG), 33(4):120,
2014. 3

[22] Diederick P Kingma and Jimmy Ba. Adam: A method for
stochastic optimization. In Proceedings of the International
Conference on Learning Representations (ICLR), 2015. 5

[23] Diederik P Kingma and Max Welling. Auto-encoding varia-
tional Bayes. In Proceedings of the International Conference
on Learning Representations (ICLR), 2014. 4

[24] Hema Swetha Koppula, Rudhir Gupta, and Ashutosh Sax-
ena. Learning human activities and object affordances from
RGBD videos. International Journal of Robotics Research,
32(8):951–970, 2013. 3

[25] Hema S Koppula and Ashutosh Saxena. Physically grounded
In Proceedings of the
spatio-temporal object affordances.
European Conference on Computer Vision (ECCV), 2014. 3
[26] Iro Laina, Christian Rupprecht, Vasileios Belagiannis, Fed-
erico Tombari, and Nassir Navab. Deeper depth prediction
with fully convolutional residual networks. In Proceedings
of the International Conference on 3D Vision (3DV), 2016. 7
[27] Donghoon Lee, Sifei Liu, Jinwei Gu, Ming-Yu Liu, Ming-
Hsuan Yang, and Jan Kautz. Context-aware synthesis and

In Proceedings of the Ad-
placement of object instances.
vances in Neural Information Processing Systems (NeurIPS),
2018. 2

[28] Xueting Li, Sifei Liu, Kihwan Kim, Xiaolong Wang, Ming-
Hsuan Yang, and Jan Kautz. Putting humans in a scene:
Learning affordance in 3D indoor environments. In Proceed-
ings of the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), 2019. 2, 3, 4, 5, 6, 7, 8, 14, 15, 19
[29] Chen-Hsuan Lin, Ersin Yumer, Oliver Wang, Eli Shechtman,
and Simon Lucey. ST-GAN: Spatial transformer generative
In Proceed-
adversarial networks for image compositing.
ings of the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), 2018. 2

[30] Andrew L Maas, Awni Y Hannun, and Andrew Y Ng. Recti-
ﬁer nonlinearities improve neural network acoustic models.
In Proceedings of the International Conference on Machine
Learning (ICML), 2013. 4

[31] Naureen Mahmood, Nima Ghorbani, Nikolaus F. Troje, Ger-
ard Pons-Moll, and Michael J. Black. AMASS: Archive of
motion capture as surface shapes. In Proceedings of the IEEE
International Conference on Computer Vision (ICCV), 2019.
3

[32] Manolis Savva, Abhishek Kadian, Oleksandr Maksymets,
Yili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub, Jia
Liu, Vladlen Koltun, Jitendra Malik, Devi Parikh, and Dhruv
Batra. Habitat: A Platform for Embodied AI Research. In
Proceedings of the IEEE International Conference on Com-
puter Vision (ICCV), 2019. 2, 3, 5, 6, 13

[33] Aron Monszpart, Paul Guerrero, Duygu Ceylan, Ersin
Yumer, and Niloy J. Mitra.
Interaction-guided
scene mapping from monocular videos. ACM Transactions
on Graphics (TOG), 38(4):15, 2019. 3

iMapper:

[34] Xi Ouyang, Yu Cheng, Yifan Jiang, Chun-Liang Li, and
Pan Zhou. Pedestrian-Synthesis-GAN: Generating pedes-
arXiv preprint
trian data in real scene and beyond.
arXiv:1804.02047, 2018. 2

[35] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,
James Bradbury, Gregory Chanan, Trevor Killeen, Zeming
Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison,
Andreas Kopf, Edward Yang, Zachary DeVito, Martin Rai-
son, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner,
Lu Fang, Junjie Bai, and Soumith Chintala. PyTorch: An
imperative style, high-performance deep learning library. In
Proceedings of the Advances in Neural Information Process-
ing Systems (NeurIPS). 2019. 5

[36] Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani,
Timo Bolkart, Ahmed A. A. Osman, Dimitrios Tzionas, and
Michael J. Black. Expressive body capture: 3D hands, face,
and body from a single image. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition
(CVPR), 2019. 2, 3, 4, 8, 14, 15

[37] Manolis Savva, Angel X Chang, Pat Hanrahan, Matthew
Fisher, and Matthias Nießner. SceneGrok: Inferring action
maps in 3D environments. ACM Transactions on Graphics
(TOG), 33(6):212, 2014. 3

[38] Manolis Savva, Angel X Chang, Pat Hanrahan, Matthew
Fisher, and Matthias Nießner. PiGraphs: Learning inter-

action snapshots from observations. ACM Transactions on
Graphics (TOG), 35(4):139, 2016. 3

[39] Kihyuk Sohn, Honglak Lee, and Xinchen Yan. Learning
structured output representation using deep conditional gen-
In Proceedings of the Advances in Neural
erative models.
Information Processing Systems (NeurIPS), 2015. 2, 4
[40] Shuran Song, Fisher Yu, Andy Zeng, Angel X Chang, Mano-
lis Savva, and Thomas Funkhouser. Semantic scene com-
In Proceedings of the
pletion from a single depth image.
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), 2017. 3

[41] Shuran Song, Fisher Yu, Andy Zeng, Angel X. Chang,
Manolis Savva, and Thomas Funkhouser. Semantic scene
completion from a single depth image. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), 2017. 5

[42] Julian Straub, Thomas Whelan, Lingni Ma, Yufan Chen, Erik
Wijmans, Simon Green, Jakob J. Engel, Raul Mur-Artal,
Carl Ren, Shobhit Verma, Anton Clarkson, Mingfei Yan,
Brian Budge, Yajie Yan, Xiaqing Pan, June Yon, Yuyang
Zou, Kimberly Leon, Nigel Carter, Jesus Briales, Tyler
Gillingham, Elias Mueggler, Luis Pesqueira, Manolis Savva,
Dhruv Batra, Hauke M. Strasdat, Renzo De Nardi, Michael
Goesele, Steven Lovegrove, and Richard Newcombe. The
Replica dataset: A digital replica of indoor spaces. arXiv
preprint arXiv:1906.05797, 2019. 2

[43] Jin Sun and David W Jacobs. Seeing what is not there:
Learning context to determine where objects are missing.
In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), 2017. 2

[44] Fuwen Tan, Crispin Bernier, Benjamin Cohen, Vicente Or-
donez, and Connelly Barnes. Where and who? Automatic
semantic-aware person composition. In Proceedings of the
IEEE Winter Conference on Applications of Computer Vision
(WACV), 2018. 2

[45] Antonio Torralba. Contextual priming for object detec-
International Journal of Computer Vision (IJCV),

tion.
53(2):169–191, 2003. 2

[46] Xiaolong Wang, Rohit Girdhar, and Abhinav Gupta. Binge
Watching: Scaling affordance learning from sitcoms. In Pro-
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), 2017. 2, 3, 6, 14, 15

[47] Fei Xia, Amir R. Zamir, Zhi-Yang He, Alexander Sax, Ji-
tendra Malik, and Silvio Savarese. Gibson Env: Real-world
perception for embodied agents. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition
(CVPR), 2018. 2

[48] Yuliang Xiu, Jiefeng Li, Haoyu Wang, Yinghong Fang, and
Cewu Lu. Pose Flow: Efﬁcient online pose tracking. In Pro-
ceedings of the British Machine Vision Conference (BMVC),
2018. 8

[49] Hsiao-chen You and Kuohsiang Chen. Applications of af-
fordance and semantics in product design. Design Studies,
28(1):23–38, 2007. 2

[50] Mihai Zanﬁr, Elisabeta Oneata, Alin-Ionut Popa, Andrei
Zanﬁr, and Cristian Sminchisescu. Human synthesis and
scene compositing. In Proceedings of the AAAI Conference
on Artiﬁcial Intelligence (AAAI), 2020. 3

[51] Yinda Zhang, Shuran Song, Ersin Yumer, Manolis Savva,
Joon-Young Lee, Hailin Jin, and Thomas Funkhouser.
Physically-based rendering for indoor scene understanding
using convolutional neural networks. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), 2017. 3

[52] Brady Zhou, Philipp Kr¨ahenb¨uhl, and Vladlen Koltun. Does
computer vision matter for action? Science Robotics, 4(30),
2019. 3, 15

[53] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela
Scene parsing through
Barriuso, and Antonio Torralba.
In Proceedings of the IEEE Conference
ADE20K dataset.
on Computer Vision and Pattern Recognition (CVPR), 2017.
7

[54] Qian-Yi Zhou, Jaesik Park, and Vladlen Koltun. Open3D: A
modern library for 3D data processing. arXiv:1801.09847,
2018. 12

[55] Yi Zhou, Connelly Barnes, Jingwan Lu, Jimei Yang, and Hao
Li. On the continuity of rotation representations in neural
networks. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR), 2019. 3
[56] Yuke Zhu, Alireza Fathi, and Li Fei-Fei. Reasoning about
object affordances in a knowledge base representation.
In
Proceedings of the European Conference on Computer Vi-
sion (ECCV), 2014. 3

[57] Yixin Zhu, Chenfanfu Jiang, Yibiao Zhao, Demetri Ter-
zopoulos, and Song-Chun Zhu. Inferring forces and learn-
ing human utilities from videos. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition
(CVPR), 2016. 2, 3

Appendix

A. Experiment Details

A.1. From PROX-Qualitative to PROX-E

The PROX-Qualitative (or PROX-Q for short) dataset comprises recordings of 20 subjects in 12 indoor scenes, including
3 bedrooms, 5 living rooms, 2 sitting booths and 2 ofﬁces. The 3D scenes were scanned with a commercial Structure Sensor
RGB-D camera and reconstructed by the accompanying 3D reconstruction solution Skanect. We refer to [18] for more details
of how PROX-Q was created. Note that the scene meshes of PROX-Q do not form valid rooms, i.e. there is no ceiling and
some walls are missing. Furthermore, the meshes are not semantically segmented.

To our knowledge, PROX-Q is the largest dataset capturing real human-scene interactions at the 3D mesh level. However,
due to the incomplete room scans and lack of mesh semantics, we extend PROX-Q as described below to serve our purposes
of human-scene interaction modeling and generation from the viewpoint of an embodied agent:

(1) Building up virtual walls, ﬂoors, ceilings. To achieve this goal, we import the scene meshes of PROX-Q into Blender,
which we use to enclose the original scene meshes to create rooms. With complete rooms, when we image the 3D scene
using a virtual camera, we can always obtain a completed depth map. The completed depth maps are illustrated in Fig. 3.

(2) Semantic annotation of the scene meshes. The mesh semantics follow the Matterport3D dataset [3], which incorpo-
rates 40 categories of common indoor objects3. Our annotation is performed manually, and the mesh vertex color denotes the
object labels. Our virtual images of the scene, therefore capture both the scene depth and semantics.

(3) Setting up virtual cameras. The original PROX-Q dataset only incorporates video recordings from a single view in
each scene; this gives only have 12 depth-semantics pairs to use for training. This is too limited to learn generalization to
new scenes. To overcome this, for each individual frame captured by the real camera, we create a set of virtual cameras
in the scene to capture the human behavior. The virtual cameras are posed according to the room structure and the human
body position. Speciﬁcally, we create a 3D grid according to the room size. The range of width and length is determined
by the size of the room. The range of height is between the pelvis of the human body and the ceiling height that we have
created. For each camera, the X-axis is parallel to the ground, and the Z-axis is towards the human body center. Next, we
add Gaussian noise on the camera translations, and discard views with no human bodies or strong body occlusions; i.e., we
keep views where the body part around the pelvis (± 10 pixels) is not occluded by any object in the scene. We argue that
such noise is essential. Otherwise the generated human bodies will always be located in the center of the depth-semantic
maps. Furthermore, we only keep the virtual cameras with the distance to the human body between 1.65m and 6.5m, so that
the projected body sizes to the virtual cameras are similar to the body sizes captured by real cameras. Fig. S1 shows a set of
virtual cameras before and after applying the Gaussian noise to the camera translations. Moreover, the resolution of depth
and semantics is set to 480×270, and the camera intrinsic parameters are

K =





233.826
0
0

0
233.826
0



 ,

239.5
134.5
1

(9)

which is a default setting in Open3D [54] after specifying the depth/semantics resolution.

A.2. Creating the MP3D-R dataset

Our MP3D-R dataset is extracted from the Matterport3D dataset [3]. We extract the 7 rooms by annotating bounding

boxes of regions, as shown in Tab. S1. These 7 rooms have room types that are similar to PROX-E.

When trimming the rooms according to the annotation, we expand the annotated bounding box size by 0.5 meters to
ensure that walls, ceilings and ﬂoors are incorporated. Note that, the Habitat simulator and the original Matterport3D dataset
have different gravity directions. The Habitat simulator assumes that the gravity direction is along −Y . Thus, after loading
the scene meshes from Matterport3D, we rotate the scene mesh by −90 degree w.r.t. the X-axis to match the bounding box
annotation from Habitat. Fig. S2 shows some retrieved room meshes with the world coordinate origins.

3One can see the object categorization via: https://github.com/niessner/Matterport/blob/master/metadata/mpcat40.tsv

Figure S1: Illustration of the virtual cameras before and after applying the Gaussian noise to the camera translation. The X,
Y and Z axes of each camera are denoted by red, green and blue, respectively.

Table S1: The seven rooms in MP3D-R, retrieved from the Matterport3D dataset [3].

scan ID

region ID

room type

17DRP5sb8fy
17DRP5sb8fy
17DRP5sb8fy
sKLMLpTHeUy
X7HyMhZNoso
zsNo4HB9uLZ
zsNo4HB9uLZ

0-0
0-8
0-7
0-1
0-16
0-0
0-13

bedroom
family room
living room
family room
living room
bedroom
living room

Figure S2: Three examples of the rooms in MP3D-R. One can see the world origins, and the gravity direction is along −Y .

Next, we use the Habitat simulator [32] to create a virtual agent in the room. In each scene, we ﬁrst put the agent in the
room center, and then manipulate that virtual agent to cruise around the room. According to ranges of virtual cameras in
PROX-E, we set the height of agent sensor to 1.8 meters from the ground. For each snapshot, we record the RGB image, the
scene depth, the scene semantics, as well as the camera extrinsic parameters. The frame resolution and the camera intrinsics
are identical to our settings in PROX-E.

Following the pipeline for creating PROX-Q, we also compute scene signed distance functions (SDFs) of the MP3D-R
scenes. For each room, we ﬁrst use Poisson surface reconstruction to convert the meshes to be watertight. Fig. S3 shows an
example of the reconstructed scene mesh. Next, similar to [18, Sec. 3.6] we compute the SDF in a uniform voxel grid of size
256 × 256 × 256 which spans a padded bounding box of the scene mesh.

Figure S3: From left to right: The original scene mesh, the mesh after Poisson surface reconstruction, and their overlay.

Figure S4: The user interface of our user study, in which the users are requested to rate how naturally the human is interacting
with the environment.

A.3. Details of the baseline method

To our knowledge, the most related work is Li et al. [28], which aims to put humans in a scene and infer the affordances
of 3D indoor environments. The authors ﬁrst propose an efﬁcient and fully-automatic 3D human pose synthesizer to generate
stick ﬁgures, using a pose prior learned from a large-scale 2D dataset [46] and the physical constraints from the target 3D
scenes. With this pose synthesizer, the authors create a dataset incorporating synthesized human-scene interactions. Next,
based on the synthesized dataset, the authors develop a generative model for 3D affordance prediction, which is able to
generate body stick ﬁgures based on the scene images.

Compared to the method of Li et al.

[28], our solution has the following key differences: (1) Our PROX-E dataset
contains real human-scene interactions rather than synthesized ones. This is highly beneﬁcial to model the distribution of
human-scene interactions in the real world. (2) Our solution is to generate body meshes rather than 3D body stick ﬁgures (See
Fig. 5 in [28]). Therefore, the results can be directly used in applications like VR, AR and others. (3) We use the SMPL-X
model [36] in our work, hence our methods can generate various body shapes and ﬁne-grained hand poses, beyond the body
global conﬁgurations and local poses. (4) SMPL-X can be regarded as a differentable function mapping from human body
features to human body meshes, so the physical constraints applied on the body mesh surfaces can be back-propagated to the

original scene meshwatertight scene meshoverlaybody features like in [18]. (5) We use scene depth and semantics to represent the scene, rather than using RGB (or RGBD)
images as in [28]. In our study, the RGB images are only available from the limited number of real camera views in PROX-E,
and hence using RGB images can increase the risk of overﬁtting. In addition, the beneﬁts of scene depth and semantics are
revealed in [52].

Therefore, in our work, we modify the method of Li at al. [28] as mentioned in Sec. 4.1.2, so that their model can
generate body meshes like our method, and a fair comparison can be conducted. We treat the modiﬁed version of [28] as our
baseline. We train the baseline model with the PROX-E dataset like training our models. After generating body meshes in
test scenes, we also apply our scene geometry-aware ﬁtting to reﬁne the results of the baseline model. The qualitative results
of the baseline with ﬁtting are shown in Fig. S6 and Fig. S8. We argue that our modiﬁcation is necessary and favorable to the
baseline to produce high quality 3D human bodies. For the quantitative comparison, please refer to Tab. 2, Tab. 3 and Tab. 4,
in the main paper.

A.4. Details of the user study

To evaluate how naturally the human body meshes are posed in the scenes, we perform a user study via Amazon Mechan-
ical Turk (AMT). For each generated body pose, we render images of the body-and-scene mesh from two different views.
Fig. S4 shows our AMT user interface. We pose the hypothesis that the human body interacts with the scene in a very natural
manner, and then ask subjects to judge the correctness of this hypothesis. Their judgements are recorded on a 5-point Likert
scale.

Unlike the user study in [28, 46], we do not show pairs of results from different methods in the user interface. Instead, we
have multiple methods to compare, and hence let the Turker evaluate each individual result in order to keep the user interface
clear. Also, we report the standard errors of the user study results in addition to the mean values, which indicate how reliable
the scores are. One can see in Tab. 4 that the scene geometry-aware ﬁtting can reduce the the standard error, indicating that
subjects tend to give more consistent judgements. The ground truth has the lowest standard error, which indicates that Turkers
are able to judge when the human-scene interaction is natural.

A.5. More discussions on model training

We discussed loss weights and training schemes in Sec. 3.4. The weights are determined empirically: First, the KL-
divergence weight is 0.1 for better representation power of the latent variables, as indicated in [2, 20]. The annealing scheme
effectively avoids a collapsed VAE posterior, which outputs a constant result no matter how the latent variable varies. Second,
the VPoser weight 0.001 is determined referring to [18, 36], to balance plausibility and variability of generated body poses.
A too small weight increases body pose variations but can lead to implausible body poses (e.g. twisted legs). Additionally,
in our trials the LHS weights are set to avoid overﬁtting. Also, enabling LHS earlier during training causes bad body
reconstruction, since the modiﬁed Chamfer loss in Eq. (7) can pull the preliminary reconstructed body mesh to the closest
scene mesh vertices. Moreover, the scene geometry-aware ﬁtting loss weights are larger than the training loss weights, so as
to reduce number of iterations in optimization while retaining the quality.

B. Generative Model Latent Space Analysis

We show how the body smoothly changes in Fig. S5. Note that the results are without scene geometry-aware ﬁtting. First,
Fig. S5 indicates that our model effectively learns natural human-scene interactions. It shows that the generated body tends
to stand when located on the ﬂoor, touches the desk (Fig. S5 (2)), and sits on the bed (Fig. S5 (4)) when located close to
the furniture. Second, the body conﬁgurations are disentangled in the latent space to some extent. For example, the body in
Fig. S5 (2) mainly moves along the X direction in the world coordinate system, while the body in Fig. S5 (3) mainly moves
along the Y direction. Third, the body pose is less plausible when its latent variable is far away from zero. This is similar to
VPoser [36]; Tab. 5 in the manuscript shows the beneﬁts of our model used as a scene-dependent pose prior.

C. More Qualitative Results and Failure Cases

Fig. S6 and Fig. S7 show qualitative results in the test scenes of PROX-E. Fig. S8 and Fig. S9 show qualitative results of

the generative models and the scene geometry-aware ﬁtting in MP3D-R.

We ﬁnd that failure cases can be categorized to two cases: First, the generative model is not always reliable in test scenes,
since samples from the model are not always plausible. Some results sampled from the generative model cannot match the
geometric structures in the test scenes, and hence the body ﬂoats in the air, or collides with the scene mesh. See Fig. S10 for
examples. Such failure cases can occur in both the baseline and our methods. Second, although the scene geometry-aware

Figure S5: Illustration of the 32D latent space in the one-stage model. We regularly sample points along a line ranging from
-3 to 3, and show the body meshes from two views. From top to bottom: (1) All dimensions of the line change. (2) The ﬁrst
16 dimensions change, and the rest are zero. (3) Only the last 16 dimensions change. (4) Only the middle 16 dimensions
change.

ﬁtting can effectively resolve ﬂoating and collision, its optimization process cannot simulate all real physics such as gravity
and elasticity. Therefore, it could hurt the human-scene interaction semantics of the results produced by the generative model.
Fig. S11 shows some examples of such failure cases, which contain abnormal body global conﬁgurations and human-scene
contact caused by our scene geometry-aware ﬁtting.

Moreover, we discover that the quality of generated bodies also depends on the test scene data quality. For example,
we have observed many low-quality generations in MP3D-R, which has more complex scene structures and noisy scans
(unknown surfaces ﬂoating in the air) than PROX-E. Noisy scans can lead to noisy depth and semantic segmentation, and
complex geometric structures can make geometry-aware ﬁtting fail.

D. Details of Scene-aware 3D Body Pose Estimation

In the experiment presented in Sec. 4.2, we follow the work of [18], and modify its Eq. (1) to incorporate our learned

scene-dependent pose prior. The equation (1) in [18] is given by

E(β, θ, ϕ, γ, Ms) = EJ + λDED + λθb Eθb + λθf Eθf + λθh Eθh

+ λαEα + λβEβ + λ(cid:15)E(cid:15) + λ(cid:15)E(cid:15)
+ λP EP + +λCEC,

(10)

where the notation corresponds to that in [18]. In our work, we only modify the VPoser regularizer, i.e., Eθb = (cid:107)θb(cid:107)2
leave the other terms unchanged. Speciﬁcally, we change it to

2, and

Eθb = (cid:107)θb − θs

b (cid:107)2
2,

(11)

in which θs
the initial pose feature is set to θs
performance.

b is our scene-dependent pose prior. We have demonstrated how to derive the θs

b in Sec. 4.2. During optimization,
b , instead of a zero vector as in [18]. In our trials, changing the weight to 1.5λθb yields better

Figure S6: Qualitative results of the baseline method in PROX-E. The results before and after the scene geometry-aware
ﬁtting are shown.

before fittingafter fittingbaselinebefore fittingafter fittingbaselineFigure S7: Qualitative results of S1 in PROX-E. The results before and after the scene geometry-aware ﬁtting are shown.

before fittingafter fittingoursbefore fittingafter fittingoursFigure S8: Qualitative results of the baseline with ﬁtting in MP3D-R. We argue that our modiﬁcations to [28] are necessary
and favorable to produce high quality 3D human bodies. For the quantitative comparison, please refer to Tab. 2, Tab. 3 and
Tab. 4

Figure S9: Qualitative results of S1 with ﬁtting in MP3D-R.

Figure S10: Failure cases of body mesh generation. One can see the body ﬂoating and colliding with the scene mesh, which
are implausible in the real world.

Figure S11: Failure cases of the scene geometry-aware ﬁtting, for which results before and after the ﬁtting are presented.
One can see abnormal body translation, rotation and body-scene contact in the real world.

Failure cases 1: wrong generationsFailure cases 2: wrong fittingsbeforeafter