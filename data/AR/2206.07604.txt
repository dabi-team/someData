ARES:LocallyAdaptiveReconstruction-basedAnomalyScoringAdamGoodge1,3,BryanHooi1,2,SeeKiongNg1,2,andWeeSiongNg31SchoolofComputing,NationalUniversityofSingapore2InstituteofDataScience,NationalUniversityofSingapore3InstituteforInfocommResearch,A*STAR,Singaporeadam.goodge@u.nus.edu,{dcsbhk,seekiong}@nus.edu.sg,wsng@i2r.a-star.edu.sgAbstract.Howcanwedetectanomalies:thatis,samplesthatsigniﬁ-cantlydiﬀerfromagivensetofhigh-dimensionaldata,suchasimagesorsensordata?Thisisapracticalproblemwithnumerousapplicationsandisalsorelevanttothegoalofmakinglearningalgorithmsmorero-busttounexpectedinputs.Autoencodersareapopularapproach,partlyduetotheirsimplicityandtheirabilitytoperformdimensionreduction.However,theanomalyscoringfunctionisnotadaptivetothenaturalvariationinreconstructionerroracrosstherangeofnormalsamples,whichhinderstheirabilitytodetectrealanomalies.Inthispaper,weempiricallydemonstratetheimportanceoflocaladaptivityforanomalyscoringinexperimentswithrealdata.WethenproposeournovelAdap-tiveReconstructionError-basedScoringapproach,whichadaptsitsscor-ingbasedonthelocalbehaviourofreconstructionerroroverthelatentspace.Weshowthatthisimprovesanomalydetectionperformanceoverrelevantbaselinesinawidevarietyofbenchmarkdatasets.Keywords:Anomalydetection·machinelearning·unsupervisedlearning.1IntroductionThedetectionofanomalousdataisimportantinawidevarietyofapplications,suchasdetectingfraudulentﬁnancialtransactionsormalignanttumours.Re-cently,deeplearningmethodshaveenabledsigniﬁcantimprovementsinperfor-manceoneverlargerandhigher-dimensionaldatasets.Despitethis,anomalydetectionremainsachallengingtask,mostnotablyduetothediﬃcultyofob-tainingaccuratelabelsofanomalousdata.Forthisreason,supervisedclassi-ﬁcationmethodsareunsuitableforanomalydetection.Instead,unsupervisedmethodsareusedtolearnthedistributionofanall-normaltrainingset.Anoma-liesarethendetectedamongstunseensamplesbymeasuringtheirclosenesstothenormal-datadistribution.Autoencodersareanextremelypopularapproachtolearnthebehaviourofnormaldata.Thereconstructionerrorofasampleisdirectlyusedasitsanomalyscore;anomaliesareassumedtohaveahigherreconstructionerrorthanarXiv:2206.07604v1  [cs.LG]  15 Jun 20222A.Goodgeetal.normalsamplesduetotheirdiﬀerenceindistribution.However,thisanomalyscorefailstoaccountforthefactthatreconstructionerrorcanvarygreatlyevenamongstdiﬀerenttypesofnormalsamples.Forexample,considerasensorsysteminafactorywithvariousactivitiesonweekdaysbutzeroactivityonweekends.Theweekdaysamplesarediverseandcomplex,thereforewecouldexpectthereconstructionerrorofthesesamplestovarygreatly.Meanwhile,evenasmallamountofactivityinaweekendsamplewouldbeanomalous,eveniftheeﬀectonthereconstructionerrorisminimal.Theanomalydetectorwouldlikelydetectfalsepositives(highreconstructionerror)amongweekdaysamplesandfalsenegativesamongweekendsamples.Althoughencodedwithintheinputdataattributes,thecontextofeachindividualsample(i.e.dayoftheweek)isneglectedatthedetectionstagebythestandardscoringapproach.Instead,allsamplesareassessedaccordingtothesameerrorthresholdorstandard.Thisinvokestheimplicitassumptionthatthereconstructionerrorsofallsamplesareidenticallydistributed,regardlessofanyindividualcharacteristicsandcontextwhichcouldpotentiallyinﬂuencethereconstructionerrorsigniﬁcantly.Inthiswork,weaimtoaddressthisproblembyproposingAdaptiveRe-constructionError-basedScoring(ARES).Ourlocally-adaptivescoringmethodisabletoautomaticallyaccountforanycontextualinformationwhichaﬀectsthereconstructionerror,resultinginmoreaccurateanomalydetection.Weuseaﬂexible,neighbourhood-basedapproachtodeﬁnethecontext,basedonthelocationofitslatentrepresentationlearntbythemodel.Ourscoringapproachisappliedattesttime,soitcanberetroﬁttedtopre-trainedmodelsofanysizeandarchitectureorusedtocomplementexistinganomalydetectiontechniques.Ourscoreissimpleandeﬃcienttocompute,requiringverylittleadditionalcomputationaltime,andthecodeisavailableonline4.Insummary,wemakethefollowingcontributions:1.Weempiricallyshowwithreal,multi-classdatathevariationinre-constructionerroramongdiﬀerentsamplesinthenormalsetandwhythisjustiﬁestheneedforlocaladaptivityinanomalyscoring.2.Weproposeanovelanomalyscoringmethodwhichadaptstothisvariationbyevaluatinganomalystatusbasedonthelocalcontextofagivensampleinthelatentspace.3.Weevaluateourmethodagainstawiderangeofbaselineswithvariousbench-markdatasets.Wealsostudytheeﬀectofdiﬀerentcomponentsandformu-lationsofourscoringmethodinanablationstudy.2Background2.1AutoencodersAutoencodersareneuralnetworksthatoutputareconstructionoftheinputdata[26].Theyarecomprisedoftwocomponents:anencoderanddecoder.The4https://github.com/agoodge/ARESARES:LocallyAdaptiveReconstruction-basedAnomalyScoring3encodercompressesdatafromtheinputlevelintolower-dimensionallatentrep-resentationsthroughitshiddenlayers.Theencoderoutputistypicallyknownasthebottleneck,fromwhichthereconstructionoftheoriginaldataisfoundthroughthedecoderhiddenlayers.Thenetworkistrainedtominimisethere-constructionerroroverthetrainingset:minθ,φkx−(fθ◦gφ)(x)k22,(1)wheregφistheencoder,fθthedecoder.Theassumptionisthatdataliesonalower-dimensionalmanifoldwithinthehigh-dimensionalinputspace.Theau-toencoderlearnstoreconstructdataonthismanifoldbyperformingdimensionreduction.Bytrainingthemodelononlynormaldata,thereconstructionerrorofanormalsampleshouldbelowasitclosetothelearntmanifoldonwhichithasbeenreconstructedbytheautoencoder,whereasanomaliesarefarawayandarereconstructedwithhighererror.2.2LocalOutlierFactorTheLocalOutlierFactor(LOF)methodisaneighbourhood-basedapproachtoanomalydetection;itmeasuresthedensityofagivensamplerelativetoitsknearestneighbours’[8].Anomaliesareassumedtobeinsparseregionsfarawayfromtheoneormorehigh-densityclustersofnormaldata.Alowerden-sitythereforesuggeststhesampleisanomalous.Theoriginalmethodusesthe‘reachabilitydistance’;deﬁnedforapointAfrompointBas:max{k-distance(B),d(A,B)}(2)whered(·,·)isachosendistancemetricandk-distance(B)isthedistanceofBtoitskthnearestneighbour.ThelocalreachabilitydensityandsubsequentlythelocaloutlierfactorofAbasedonitssetofneighboursNk(A),is:lrdk(A):= PB∈Nk(A)reachability-distancek(A,B)|Nk(A)|!−1.(3)LOFk(A)=PB∈Nk(A)lrdk(B)|Nk(A)|·lrdk(A),(4)3RelatedWorkAnomaliesareoftenassumedtooccupysparseregionsfarawayfromhigh-densityclustersofnormaldata.Assuch,agreatvarietyofmethodsdetectanomaliesbymeasuringthedistancestotheirnearestneighbours,mostnotablyKNN[35],whichdirectlyusesthedistanceofapointtoitskthnearestneighbourastheanomalyscore.LOF[8]measuresthedensityofapointrelativetothedensityofpointsinitslocalneighbourhood,asseeninSection2.Thisisbeneﬁcialasa4A.Goodgeetal.givendensitymaybeanomalousinoneregionbutnormalinanotherregion.Thislocalviewaccommodatesthenaturalvariationindensityandthereforeallowsforthedetectionofmoremeaningfulanomalies.Morerecently,deepmodelssuchasgraphneuralnetworkshavebeenusedtolearnanomalyscoresfromneighbour-distancesinadata-drivenway[15].Naturally,thisreliesonaneﬀectivedistancemetric.Thisisoftenitselfadiﬃcultproblem;eventhemostestablishedmetricshavebeenshowntolosesigniﬁcanceinhigh-dimensionalspaces[6]duetothe‘curseofdimensionality’.Reconstruction-basedmethodsrelyondeepmodels,suchasautoencoders,tolearntoreconstructsamplesfromthenormalsetaccurately.Samplesarethenﬂaggedasanomalousiftheirreconstructionerrorhigherthansomepre-deﬁnedthreshold,asitisassumedthatthemodelwillreconstructsamplesoutsideofthenormalsetwithahigherreconstructionerror[26,13,4,9,14].Morerecentworkshaveusedautoencodersorothermodelsaredeepfea-tureextractors,withthelearntfeaturesthenusedinanotheranomalydetectionmoduledownstream,suchasGaussianmixturemodels[7],DBSCAN[3],KNN[5]andauto-regressivemodels[1].[11]calibratesdiﬀerenttypesofautoencodersagainsttheeﬀectsofvaryinghardnesswithinnormalsamples.Othermethodsmeasurethedistanceofasampletoanormalset-enclosinghypersphere[30,25].oritslikelihoodunderalearntmodel[12,24,34].Generativeadversarialnetworkshavealsobeenproposedforanomalydetection,mostlyrelyingeitheronthediscriminatorscoreortheaccuracyofthegeneratorforagivensampletodetermineanomalousness[2,33].Inallofthesemethods,thenormalsetisoftenrestrictedtoasubsetoftheavailabledata;e.g.justoneclassfromamulti-classdatasetinexperiments.Inpractice,normaldatacouldbelongtomultipleclassesormodesofbehaviourwhichallneedtobemodelledadequately.Assuch,developinganomalydetectionmethodsthatcanmodeladiverserangeofnormalbehavioursandadapttheirscoringappropriatelyareimportant.4Methodology4.1ProblemDeﬁnitionWeassumetohavemnormaltrainingsamplesx(train)1,···,x(train)m∈Rdandntestingsamples,x(test)1,···,x(test)n∈Rd,eachofwhichmaybenormaloranomalous.Foreachtestsamplex,ouralgorithmshouldindicatehowanomalousitisthroughcomputingitsanomalyscores(x).Thegoalisforanomaliestobegivenhigheranomalyscoresthannormalpoints.Inthiswork,thefundamentalquestionis:Givenanautoencoderwithencodergφanddecoderfθ,howcanweusethelatentencodingz=gφ(x)andreconstructionˆx=(fθ◦gφ)(x)ofasamplextoscoreitsanomalousness?Ourapproachisnotlimitedtostandardautoencoders,andcanbeappliedtoothermodelsthatinvolvealatentencodingzandareconstructionˆx.ARES:LocallyAdaptiveReconstruction-basedAnomalyScoring5Inpractice,theanomalyscoreiscomparedwithauser-deﬁnedanomalythreshold;sampleswhichexceedthisthresholdareﬂaggedasanomalies.Dif-ferentapproachescanbeemployedtosetthisthreshold,suchasextremevaluetheory[28].Ourfocusisinsteadontheapproachtoanomalyscoringitself,whichallowsforanychoiceofthresholdingschemetobeusedalongsideit.4.2StatisticalInterpretationofReconstruction-basedAnomalyDetectionInthestandardapproach,withtheresidualasε:=x−ˆx,theanomalyscoreis:R(ε):=kεk22=kx−ˆxk22.(5)Thiscanbeseenasanegativelog-likelihood-basedscorethatassumesεfollowsaGaussiandistributionwithzeromeanandunitvariance:ε∼N(0,I):−logP(ε)=d2log(2π)+R(ε)2(6)Thenegativelog-likelihoodisanintuitiveanomalyscorebecauseanomaloussamplesshouldhavelowerlikelihood,thushighernegativelog-likelihood.Thekeyconclusion,ignoringtheconstantadditiveandmultiplicativefactors,isthatR(ε)canbeequivalentlyseenasanegativelog-likelihood-basedscore,basedonamodelwiththeimplicitassumptionofconstantmeanaswellasconstantvarianceresidualsacrossallsamples,knowninthestatisticalliteratureashomoscedasticity.Mostcrucially,thisassumptionappliestoallxregardlessofitsindividuallatentrepresentation,z,whichimpliesthatthereconstructionerrorofxdoesnotdependonthelocationofzinthelatentspace.Wequestionthevalidityofthisassumptioninrealdatasets.Inourearlierexampleofafactorysensorsystem,thevarianceofasamplexdependsgreatlyonitscharacteristics(i.e.dayoftheweek),withhighvarianceduringweekdaysandlowvarianceonweekends.Thisisinsteadaprimeexampleofheteroscedasticity.Furthermore,sincethelatentrepresentationztypicallyencodestheseimportantcharacteristicsofx,thereislikelytobeaclearde-pendencebetweenzandthereconstructionerrorR(ε),whichwillbeexaminedinSection4.3.Thisrelationshipcouldbeexploitedtoimprovethemodellingofreconstructionerrorsandthusdetectionaccuracybyadaptingtheanomalyscoretothisrelationshipatthesample-level.4.3Motivation&EmpiricalResultsInthissection,wetrainanautoencoderonall10classesofMNISTtoempiricallyexaminethevariationinthereconstructionerrorofrealdatatoscrutinisethehomoscedasticassumption.Indoingso,weexemplifytheshortcomingsofthestandardapproachwhichimplicitlyassumesallreconstructionerrorsfromaﬁxedGaussiandistribution.TheautoencoderarchitectureandtrainingprocedureisthesameasthosedescribedinSection5.Wemakethefollowingobservations:6A.Goodgeetal.Fig.1:t-SNEplotofthelatentencodingswithcolourdeterminedbyreconstruc-tionerroroftheassociatedsample.1.Inter-classvariation:InFigure1,thet-SNE[21]projectionsofthela-tentrepresentationsoftrainingpointsarecolouredaccordingtotheirrecon-structionerror.Weseethattheautoencoderlearnstoseparateeachclassapproximatelyintodistinctclusters.Inthiscontext,theclasslabelcanbeseenasavariablecharacteristicbetweendiﬀerentsampleswithinthenormalset(similartotheweekdayvs.weekendexample).Fig.2:Probabilitydensityfunctionofreconstructionerrorsassociatedwithdif-ferentclassesoftrainingsamples,estimatedviakerneldensityestimation.2.Intra-classvariationAlsoinFigure1,wecanseethatthereissigniﬁcantvariationevenwithinanysinglecluster.Inotherwords,therearenoticeablydistinctregionsofhigh(andlow)reconstructionerrorswithineachindividualcluster.ThisshowsthatthereareadditionalcharacteristicsthatinﬂuenceARES:LocallyAdaptiveReconstruction-basedAnomalyScoring7whetheragivennormalsamplehasahighorlowreconstructionerrorbesidesitsclasslabel.Weobservethatthereissigniﬁcantvariationinreconstructionerrorbetweenthediﬀerentclusters.Mostnotably,samplesintheleftmostcluster(corre-spondingtoclass1)hassigniﬁcantlylowerreconstructionerrorsthanmostothers.Indeed,asshowninFigure2,thedistributionofreconstructionerrorsassociatedwithclass1samplesisverydiﬀerenttothoseoftheotherclasses.Thisshowsthatthereissigniﬁcantvariationinreconstructionerrorsbetweenclasses,andthatitisinappropriatetoassumethatthereconstructionerrorsofallsamplescanbemodelledbyasingle,ﬁxedGaussiandistribution.Forexample,areconstructionerrorof0.06wouldbehighforaclass1orclass9sample,butlowforaclass2orclass8sample.Fig.3:Averagereconstructionerrorofthetrainingsetnearestneighboursofapointversusitsownreconstructionerrorfornormaltestsamples.3.Neighbourhoodcorrelation:InFigure3,weplotthereconstructioner-roroftestsamplesagainsttheaveragereconstructionerrorofitsnearesttrainingsetneighboursinthelatentspace(neighbourhooderror).Weseethatthereconstructionerrorofatestpointincreasesasitsneighbourhooderrorincreases.Furthermore,thevarianceintesterrorsincreasesforlargerneighbourhooderrors:aclearheteroscedasticrelationship.Thisinformationisusefulindetermininganomalousness:atesterrorof0.1wouldbeanoma-louslyhighifitsneighbourhooderroris0.02,butnormalifitis0.06.Giventheseobservations,weproposethatanomaliescouldbemoreaccuratelydetectedbyincorporatingcontextualinformationintotheanomalyscoringbe-yondreconstructionerroralone.Furthermore,analysingtheneighbourhoodofagivensampleprovidesthiscontextualinformationtohelpdetermineitsanoma-8A.Goodgeetal.lousness.Insection4.4,weproposeourAdaptiveReconstructionError-basedScoringmethodtoachievethis.4.4AdaptiveReconstructionError-basedScoringInSection4.2,wesawthatthestandardapproachassumesthatallresidualscomefromaﬁxedGaussianwithconstantmeanandunitvariance:ε∼N(0,I).InSection4.3,wesawthatthisassumptionisinappropriate.Inthissection,wedetailARES,anovelanomalyscoringmethodologywhichaimstoaddressthisﬂawbyadaptingthescoringforeachsampleslocalcontextinthelatentspace.Thenormallevelofreconstructionerrorvariesforsamplesindiﬀerentregionsofthelatentspace,meaningthelatentencodingofasampleholdsimportantinformationregardinganomalousness.Assuch,ARESisinspiredbythejoint-likelihoodofasamplesresidualεwithitslatentencodingz,deﬁnedasfollows:−logP(ε,z)=−logP(ε|z)−logP(z).(7)Theﬁrstterm,−logP(ε|z),istheconditional(negativelog-)likelihoodofthepointsresidualconditionedonitslatentencoding.Thesecondterm,−logP(z),isthelikelihoodofobservingthelatentencodingfromthenormalset.Wenowdetailourapproachtointerpretthesetermsintotractable,eﬃcientscoreswhichwenamethelocalreconstructionscoreandlocaldensityscorerespectively.ThesescoresarecombinedtogivetheoverallARESanomalyscore.4.5LocalReconstructionScoreThelocalreconstructionscoreisbasedonanestimateofhowlikelyagivenresid-ualε(andconsequentreconstructionerror)istocomefromthecorrespondingsamplex,basedonitslatentencodingz:r(x)=−logP(ε|z)(8)Thislikelihoodcannotbecalculateddirectlyforanyindividualz.Instead,weconsidertheknearestneighboursofzinthelatentspace,denotedNk(z),tobeasamplepopulationwhichzbelongsto.Thisisintuitiveasdatapointswithsimilarcharacteristicstoxintheinputspacearemorelikelytobeencodednearertozinthelatentspace.ThefulllocalreconstructionscorealgorithmisshowninAlgorithm1.Aftertrainingtheautoencoder,weﬁxthemodelweightsandstoreinmem-oryalltrainingsamples’reconstructionerrorsandlatentencodingstakenfromthebottlenecklayer.Foratestsamplex,weﬁnditsownlatentencodingzandreconstructionˆx.WethenﬁndNk(z),thesetofknearestneighbourstozamongstthetrainingsetencodings.Weonlyﬁndnearestneighboursamongthetrainingsamplesastheyareallassumedtobenormalandthereforeshouldhavereconstructionerrorswithinthenormalrange.Wewanttoobtainanestimateofthereconstructionerrorthatcouldbeexpectedofx,assumingitsanormalpoint,basedonthereconstructionerrorsofARES:LocallyAdaptiveReconstruction-basedAnomalyScoring9its(normal)neighbours.Wecanthencomparethisexpectedvalue,conditionedonitsuniquelocationinthelatentspace,withthepointstruereconstructionerrortodetermineitsanomalousness.Inafullyprobabilisticapproach,wecoulddothisbymeasuringthelikelihoodofthetestpointsreconstructionerrorunderaprobabilitydistribution,e.g.aGaussian,ﬁttotheneighbours’reconstructionerrors.However,itisunnecessar-ilyrestrictivetoassumeanyclosed-formprobabilitydistributiontoadequatelymodelthispopulationfortheuniqueneighbourhoodofeachandeverytestsam-ple.Instead,weoptforanon-parametricapproach;wemeasurethediﬀerencebetweenthetestpointsreconstructionerrorandthemedianreconstructionerrorofitsneighbouringsamples.Thelargerthediﬀerencebetweenthem,themoreoutlyingthetestpointisincomparisontoitslocalneighbours,thereforethemorelikelyitistobeanomalous.Inpractice,usingthemedianwasfoundtoperformbetterthanthemeanasitismorerobusttoextrema.Withthis,weobtainthelocalreconstructionscoreas:r(x)=kx−ˆxk22−mediann∈Nk(z)(kn−ˆnk22).(9)Nearestneighboursearchinthelatentspaceispreferableovertheinputspaceasdimensionreductionhelpstoalleviatesthecurseofdimensionality(seeSection3),resultinginmoresemantically-meaningfulneighbors.Secondly,fromapracticalperspective,theneighboursearchislesstime-consumingandcompu-tationallyintensiveinlowerdimensionalspaces.Algorithm1LocalReconstructionScoreInput:AutoencoderA(·)=(fθ◦gφ)(·),trainingsetXtrain,testsamplextestParameters:neighbourcountkOutput:Localreconstructionscorer(xtest)1:TrainautoencoderAontrainingsetaccordingto:minθ,φkXtrain−ˆXtraink22whereˆxitrain=(fθ◦gφ)(xitrain)forxitrain∈Xtrain.2:Findlatentencodingofxtest:ztest:=gφ(xtest)3:Findthesetofknearestneighbourstoztestamonglatentencodingsofthetrainingdata:Nk(ztest):={z1train,...,zktrain}wherezitrain=gφ(xitrain)∈fori={1,...,k}.4:returnr(xtest)=kxtest−ˆxtestk22−mediann∈Nk(z)(kxitrain−ˆxitraink22)4.6LocalDensityScoreThelocalreconstructionscorecorrespondstotheconditionaltermofthejointlikelihood.Wenowintroducethelocaldensityscore,whichcorrespondstothelikelihoodofobservingthegivenencodinginthelatentspace.Thisisadensityestimationtask,whichconcernstherelativedistanceofztoitsnearestneigh-bours,unlikethelocalreconstructionscorewhichfocusesonthereconstruction10A.Goodgeetal.errorofneighbours.Anomaliesareassumedtoexistinsparseregions,wherenormalsamplesareunlikelytobefoundinsigniﬁcantnumbers.Anymultivari-atedistributionP,withtrainableparametersΘ,couldbeusedtoestimatethisdensity:d(x):=−logP(z;Θ)(10)Notethatitiscommontoignoreconstantfactorshiftsintheanomalyscore.Thus,thedistributionPneednotbenormalized;evenunnormalizeddensityestimationtechniquescanbeusedasscoringfunctions.WenotethatLOFisanexampleofanunnormalizedscorewhichissimilarlylocallyadaptivelikethelocalreconstructionscore,soLOFisusedforthelocaldensityscoreinourmainexperiments.Othermethodsarealsotestedandtheirperformanceisshownintheablationstudy.Theoverallanomalyscoreforsamplexis:s(x):=r(x)+αd(x),(11)wherer(x)isitslocalreconstructionscoreandd(x)thelocaldensityscore.Thesetwoscoresareunnormalized,soweuseascalingfactorαtobalancetherelativemagnitudesofthetwoscores.Weheuristicallysetitequalto0.5foralldatasetsandsettingsforsimplicity,asthiswasfoundtobalancethetwoscoressuﬃcientlyfairlyinmostcases.Wechoosenottotreatαasahyper-parametertobetunedtooptimiseperformance,althoughdiﬀerentvaluescouldgivebetterperformancefordiﬀerentdatasets.Theeﬀectofchangingαisshowninthesupplementarymaterial.ComputationalRuntime:TheaverageruntimesofexperimentswiththeMNISTdatasetcanbefoundinthesupplementarymaterial.Weseethat,despitetakinglongerthanthestandardreconstructionerrorapproach,theadditionalcomputa-tionalruntimeofARESanomalyscoringisinsigniﬁcantinrelationtothemodeltrainingtime.AnomalyscoringwithARESisjust1.2%oftheoveralltimetaken(includingtraining)intheone-classcase(0.017minutesforscoringversus1.474minutesfortraining),and3.5%inthemulti-classcase.Theadditionalrun-timeisaresultoftheknearestneighboursearch.AnexactsearchalgorithmswouldbeO(nm)withntrainandmtestsamples,howeverapproximatemethodscanachievenear-exactaccuracymuchmoreeﬃciently.5ExperimentsInourexperiments,weaimtoanswerthefollowingresearchquestions:RQ1(Accuracy):DoesARESperformbetterthanexistinganomalydetectionmethods?RQ2(AblationStudy):HowdodiﬀerentcomponentsanddesignchoicesofAREScontributetoitsperformance?ARES:LocallyAdaptiveReconstruction-basedAnomalyScoring115.1DatasetsandExperimentalSetupDataset#Dim#Classes#SamplesDescriptionSNSR[31]48Multi-class58,509ElectriccurrentsignalsMNIST[19]784Multi-class70,0000-9digitimagesFMNIST[32]784Multi-class70,000FashionarticleimagesOTTO[22]93Multi-class61,878E-commercetypesMI-F[29]58Single-class25,286CNCmillingdefectsMI-V[29]58Single-class23,125CNCmillingdefectsEOPT[16]20Single-class90,515StoragesystemfailuresTable1:Nameanddescriptionsofthedatasetsusedinexperiments,includingthenumberofsamplesanddimensions.Table1showsthedatasetsweuseinexperiments.Thesingle-classdatasetsconsistofgroundtruthnormal-vs-anomalylabels,asopposedtothemultipleclasslabelsinmulti-classdatasets.Inthelatter,wedistinguishbetweenthe’one-classnormality’setup,inwhichoneclasslabelisusedasthenormalclassandallotherclassesareanomalous.Alternatively,inthe’multi-classnormality’setup,oneclassisanomalousandallotherclassesarenormal.ForadatasetwithNclasses,thereareNpossiblearrangementsofnormalandanomalyclasses.Wetrainseparatemodelsforeacharrangementandﬁndtheiraveragescorefortheﬁnalresult.WeusetheArea-Under-Curve(AUC)metrictomeasureperformanceasitdoesnotrequireananomalyscorethresholdtobeset.Asanomalyscoresarecalculatedforeachtestsampleindependentlyofeachother,theproportionofanomaliesinthetestsethasnoimpactontheanomalyscoreassignedtoanygivensample.Therefore,weareabletouseanor-mal:anomalyratioof50:50inourexperimentsforthesakeofsimplicityandanunbiasedAUCmetric.Besidesthenormalsampelsinthetestset,theremainingnormalsamplesaresplit80:20intotrainingandvalidationsetsforallmodels.Fullimplementationdetailscanbefoundinthesupplementarymaterial.5.2BaselinesWetesttheperformanceofARESagainstarangeofbaselines.Weusethesci-kitlearnimplementationsofLOF(intheinputspace)[8],IForest[20],PCA[27]andOC-SVM[10].PubliclyavailablecodesareusedforDAGMM[7],RAPP-SAPandRAPP-NAP[17],andweusePytorchtobuildtheautoen-coder(AEandARES)andvariationalautoencoder(VAE).AllexperimentswereconductedinWindowsOSusinganNvidiaGeForceRTX2080TiGPU.Wedonottunehyper-parametersrelatingtothemodelarchitecturesortrainingproceduresforanymethod.Theeﬀectofvariationinhyper-parametersisstudiedintheablationstudyinSection5.4andthesupplementarymaterial12A.Goodgeetal.instead.Wesetthenumberofneighboursk=10forboththelocaldensityandlocalreconstructionscoreinourmainexperiments.5.3RQ1(Accuracy):Table2showstheaverageAUCscoresasapercentage(i.e.multipliedby100).Intheone-classnormalitysetting,ARESsigniﬁcantlyimprovesperformanceoverthebaselinesinallmulti-classdatasetsbesidesFMNIST.Inthesingle-classdatasets,thisimprovementisevengreater,e.g.+8%liftforMI-FandEOPT.ComparedwithAE,weseethatlocaladaptivityhelpstodetecttrueanomaliesbycorrectingforthenaturalvariationinreconstructionerrorinthelatentspace.Thiseﬀectisevenmorepronouncedinthemulti-classnormalitysetting,whereARESgivesthebestperformanceonalldatasets.Here,thenormalsetismuchmorediverseandthereforelocaladaptivityisevenmoreimportant.Weshowthestandarddeviationsandadditionalsigniﬁcancetestscoresinthesupplementarymaterial,whichalsoshowstatisticallysigniﬁcant(p<0.01)improvement.DatasetLOFIForestPCAOC-SVMSAPNAPDAGMMVAEAEARESOne-classNormalitySNSR97.9889.1692.0195.8598.7998.7488.0889.4998.3098.83**MNIST96.8585.4495.6890.3595.3597.2589.6091.7396.9697.89**FMNIST91.3591.3990.1390.7489.6693.0887.9777.5992.3391.63OTTO84.7670.3480.0981.4381.6182.7768.0282.3985.2687.86**MI-F59.7981.5355.0776.6981.7880.6182.2376.9371.1989.52MI-V83.9784.3587.3283.5888.2489.3575.4589.0390.7593.94EOPT55.0161.6154.7259.6659.8761.6960.6368.0859.8568.43Multi-classNormalitySNSR60.7452.7052.9452.7957.5258.3254.7761.3657.2869.78**MNIST77.4056.4970.4158.5684.7686.3854.2484.1280.0493.25**FMNIST71.5064.7566.9260.2768.5072.0957.5671.1871.0372.49OTTO63.0154.1458.2762.9657.4763.4458.9661.8859.5963.54Table2:MeanAUCscoresforeachdatasetsandnormalitysetting.ThebestscoresarehighlightedinboldandwemarkthemostsigniﬁcantimprovementsoverAE(p<0.01)with**.Furthertestsareinthesupplementarymaterial.Theneighboursofanormalsamplewithhighreconstructionerrortendtobehavehighreconstructionerrorsthemselves.Bybasingtheanomalyscoringontheirrelativediﬀerence,ARESusesthistobetterdetecttrulyanomaloussam-ples.Furthermore,ARESalsousesthelocaldensityofthepoint,whichdependspurelyonitsdistancetothetrainingsamplesinthelatentspace.Thisisimpor-tantastheremaybesomeanomalieswithsuchlowreconstructionerrorthatcomparisonwithneighboursdoesnotaloneindicateanomalousness(forexampleARES:LocallyAdaptiveReconstruction-basedAnomalyScoring13theweekendsamplesmentionedearlier).Thesesamplescouldbeexpectedtobeoccupyverysparseregionsofthelatentspaceduetotheirsigniﬁcantdeviationfromthenormalset,whichmeanstheycanbebetterdetectedthroughthelocaldensityscore.Bycombiningthesetwoscores,weareabletodetectawiderrangeofanomalousdatathaneithercouldindividually.5.4RQ2(AblationStudy):DensityEstimationMethodTable3showstheperformanceofARESwithotherdensityestimationmethods.KNNisthedistancetothekthnearestneighbourinthelatentspacewithk=20.GDisthedistanceofapointtotheclosestofNGaussiandistributionsﬁttothelatentencodingsofsamplesforeachoftheclassesinthetrainingset(N=1intheone-classnormalitycase).NFisthelikelihoodunderaRealNVPnormalizingﬂow[23].DatasetLOFKNNGDNFOne-classNormalitySNSR98.8398.6695.6898.29MNIST97.8995.2487.2997.30FMNIST91.6392.9490.2692.14OTTO87.7683.1481.2786.74MI-F89.5276.1280.4184.47MI-V93.9491.6079.6192.89EOPT68.4367.6363.3561.07Multi-classNormalitySNSR69.7867.4261.8660.63MNIST93.2592.9291.1486.25FMNIST72.4972.7773.0270.48OTTO63.5461.7166.3063.67Table3:MeanAUCscoresforeachchoiceoflocaldensityscore.Thebestscoresarehighlightedinbold.LOFperformsbestoverall,closelyfollowedbyKNN.GDperformspoorlyinone-classexperiments,howeveritisbetterinmulti-classnormalityexper-imentsandeventhebestforFMNISTandOTTO.Thiscouldbeastheuseofmultipledistributionsprovidesmoreﬂexbility.NFisnoticeablyworse;previousstudieshavefoundthatnormalizingﬂowsarenotwell-suitedtodetectout-of-distributiondata[18].Inthesupplementarymaterial,wefurthertestthesedensityestimationmeth-odsbyvaryingtheirhyper-parametersandﬁndLOFtostillcomeoutbest.Furthersupplementaryexperimentsshowthatthelocaldensityscoregenerallyperformsbetterthanthelocalreconstructionscoreinthemulti-classnormality14A.Goodgeetal.caseandviceversaintheone-classcase.Combiningthem,asinARES,generallygivesthebestperformanceoverallacrossdiﬀerentlatentembeddingsizes.RobustnesstoTrainingContaminationInpractice,itislikelythatasmallpro-portionofanomalies‘contaminate’thetrainingset.Inthissection,westudytheeﬀectofdiﬀerentlevelsofcontaminationonARES,deﬁnedasn%ofthetotalnumberofsamplesinthetrainingset.Table4showsthatincreasingnworsensperformanceoverall.Withmoreanomaliesinthetrainingset,itismorelikelythatanomaliesarefoundinthenearestneighboursetofmoretestpoints,whichskewsboththeaverageneighbourhoodreconstructionerroraswellastheirdensityinthelatentspaceanddegradesperformance.NeighbourhoodSizen1050100200500One-classNormality097.8997.8597.8097.7397.560.595.7796.0896.0695.9595.72182.5094.4595.9795.8195.42290.6592.8893.2993.3392.99388.4191.3591.9292.0791.86584.9187.8589.2090.2690.651079.6981.5183.1684.8886.82Multi-classNormality093.2592.7192.0891.3189.680.568.3876.0778.3680.7581.42152.3858.4267.6265.7568.91259.5063.1365.7167.9869.68357.0959.0060.7862.8965.35550.8651.4852.4053.4755.781052.5050.8050.8751.1452.11Table4:MeanAUCscoresinMNISTwithtrainingsetanomalycontamination(n%)anddiﬀerentneighbourhoodsizes.Thebestscoresarehighlightedinbold.WeﬁndthatARESismorerobusttotrainingsetcontaminationwithahighersettingofthenumberofneighbours(k).Intheone-classsetup,weseethatARESperformsbetterwithhighervaluesofkastheproportionofanomaliesincreases.Byusingmoreneighbours,theeﬀectofanyindividualanomaliesontheoverallneighbourhooderrorisreduced,whichhelpstomaintainbetterperformance.Thiseﬀectisevenmorestrongerinthemulti-classnormalitysetup.Thehighestneighbourcountofk=500givesthebestperformanceinallcasesexceptforn=0%and10%.Asthemulti-classtrainingsetsaremuchlargerthantheirARES:LocallyAdaptiveReconstruction-basedAnomalyScoring15one-classnormalitycounterparts,n%correspondstoamuchlargernumberofanomalouscontaminants,whichexplainstheirgreatereﬀectforagivenk.6ConclusionAutoencodersareextremelypopulardeeplearningmodelsusedforanomalyde-tectionthroughtheirreconstructionerror.Wehaveshownthattheassumptionmadebythestandardreconstructionerrorscore,thatreconstructionerrorsareidenticallydistributedforallnormalsamples,isunsuitableforrealdatasets.Weempiricallyshowthatthereisaheteroscedasticrelationshipbetweenlatentspacecharacteristicsandreconstructionerror,whichdemonstrateswhyadap-tivitytolocallatentinformationisimportantforanomalyscoring.Assuch,wehavedevelopedanovelapproachtoanomalyscoringwhichadaptivelyevalu-atestheanomalousnessofasamplesreconstructionerror,aswellasitsdensityinthelatentspace,relativetothoseofitsnearestneighbours.Weshowthatourapproachresultsinsigniﬁcantperformanceimprovementsoverthestandardapproach,aswellasotherprominentbaselines,acrossarangeofrealdatasets.AcknowledgementsThisworkwassupportedinpartbyNUSODPRTGrantR252-000-A81-133.References1.Abati,D.,Porrello,A.,Calderara,S.,Cucchiara,R.:Latentspaceautoregressionfornoveltydetection.In:ICCV.pp.481–490(2019)2.Akcay,S.,Atapour-Abarghouei,A.,Breckon,T.P.:Ganomaly:Semi-supervisedanomalydetectionviaadversarialtraining.In:ACCV.pp.622–637.Springer(2018)3.Amarbayasgalan,T.,Jargalsaikhan,B.,Ryu,K.H.:Unsupervisednoveltydetectionusingdeepautoencoderswithdensitybasedclustering.AppliedSciences8(9),1468(2018)4.An,J.:VariationalAutoencoderbasedAnomalyDetectionusingReconstructionProbability.In:SNUDataMiningCenter2015-2SpecialLectureonIE(2015)5.Bergman,L.,Cohen,N.,Hoshen,Y.:Deepnearestneighboranomalydetection.arXivpreprintarXiv:2002.10445(2020)6.Beyer,K.,Goldstein,J.,Ramakrishnan,R.,Shaft,U.:Whenis“nearestneighbor”meaningful?In:Internationalconferenceondatabasetheory.pp.217–235.Springer(1999)7.Bo,Z.,Song,Q.,Chen,H.:Deepautoencodinggaussianmixturemodelforunsu-pervisedanomalydetection(2018)8.Breunig,M.M.,Kriegel,H.P.,Ng,R.T.,Sander,J.:LOF:identifyingdensity-basedlocaloutliers.In:SIGMOD.vol.29,pp.93–104.ACM(2000)9.Chen,J.,Sathe,S.,Aggarwal,C.,Turaga,D.:Outlierdetectionwithautoencoderensembles.In:SDM.pp.90–98.SIAM(2017)10.Chen,Y.,Zhou,X.S.,Huang,T.S.:One-classSVMforlearninginimageretrieval.In:ICIP.pp.34–37.Citeseer(2001)16A.Goodgeetal.11.Deng,A.,Goodge,A.,Lang,Y.A.,Hooi,B.:CADET:CalibratedAnomalyDetec-tionforMitigatingHardnessBias.IJCAI(2022)12.Dinh,L.,Sohl-Dickstein,J.,Bengio,S.:Densityestimationusingrealnvp.arXivpreprintarXiv:1605.08803(2016)13.Feng,W.,Han,C.:ANovelApproachforTrajectoryFeatureRepresentationandAnomalousTrajectoryDetection.ISIFpp.1093–1099(2015)14.Goodge,A.,Hooi,B.,Ng,S.K.,Ng,W.S.:RobustnessofAutoencodersforAnomalyDetectionUnderAdversarialImpact.IJCAI(2020)15.Goodge,A.,Hooi,B.,Ng,S.K.,Ng,W.S.:Lunar:Unifyinglocaloutlierdetectionmethodsviagraphneuralnetworks.In:ProceedingsoftheAAAIConferenceonArtiﬁcialIntelligence(2022)16.inIT:Toolweardetectionincncmill(2018),https://www.kaggle.com/init-owl/high-storage-system-data-for-energy-optimization17.Kim,K.H.,Shim,S.,Lim,Y.,Jeon,J.,Choi,J.,Kim,B.,Yoon,A.S.:RaPP:NoveltyDetectionwithReconstructionalongProjectionPathway.In:ICLR(2019)18.Kirichenko,P.,Izmailov,P.,Wilson,A.G.:Whynormalizingﬂowsfailtodetectout-of-distributiondata.arXivpreprintarXiv:2006.08545(2020)19.Lecun,Y.:Mnist(2012),http://yann.lecun.com/exdb/mnist/20.Liu,F.T.,Ting,K.M.,Zhou,Z.H.:Isolation-basedanomalydetection.ACMTrans-actionsonKnowledgeDiscoveryfromData(TKDD)6(1),1–39(2012)21.Maaten,L.v.d.,Hinton,G.:Visualizingdatausingt-sne.Journalofmachinelearn-ingresearch9(Nov),2579–2605(2008)22.Otto,G.:Ottogroupproductclassiﬁcationchallenge(2015),https://www.kaggle.com/c/otto-group-product-classification-challenge23.Papamakarios,G.,Pavlakou,T.,Murray,I.:Maskedautoregressiveﬂowfordensityestimation.In:NeurIPS.pp.2338–2347(2017)24.Rezende,D.J.,Mohamed,S.:Variationalinferencewithnormalizingﬂows.arXivpreprintarXiv:1505.05770(2015)25.Ruﬀ,L.,Vandermeulen,R.,Goernitz,N.,Deecke,L.,Siddiqui,S.A.,Binder,A.,M¨uller,E.,Kloft,M.:Deepone-classclassiﬁcation.In:ICML.pp.4393–4402(2018)26.Sakurada,M.,Yairi,T.:Anomalydetectionusingautoencoderswithnonlineardimensionalityreduction.In:MLSDA.p.4.ACM(2014)27.Shyu,M.L.,Chen,S.C.,Sarinnapakorn,K.,Chang,L.:Anovelanomalydetectionschemebasedonprincipalcomponentclassiﬁer.Tech.rep.,MiamiUnivCoralGablesFLDeptofElectricandComputerEngineering(2003)28.Siﬀer,A.,Fouque,P.A.,Termier,A.,Largouet,C.:Anomalydetectioninstreamswithextremevaluetheory.In:SIGKDD.pp.1067–1075(2017)29.SMART:Toolweardetectionincncmill(2018),https://www.kaggle.com/shasun/tool-wear-detection-in-cnc-mill30.Tax,D.M.,Duin,R.P.:Supportvectordatadescription.Machinelearning54(1),45–66(2004)31.UCI:Sensorlessdrivediagnosis(2015)32.Xiao,H.,Rasul,K.,Vollgraf,R.:Fashion-mnist:anovelimagedatasetforbench-markingmachinelearningalgorithms(2017)33.Zenati,H.,Foo,C.S.,Lecouat,B.,Manek,G.,Chandrasekha,V.R.:Eﬃcientgan-basedanomalydetection(2019)34.Zhai,S.,Cheng,Y.,Lu,W.,Zhang,Z.:Deepstructuredenergybasedmodelsforanomalydetection.ICML48,1100–1109(2016)35.Zimek,A.,Gaudet,M.,Campello,R.J.G.B.,Sander,J.:Subsamplingforeﬃcientandeﬀectiveunsupervisedoutlierdetectionensembles.In:SIGKDD.pp.428–436(2013)SupplementaryMaterial:AppendixExperimentsModelArchitectureandTrainingTheautoencodermodelsforthetwoimagedatasetshadsevenhiddenlayersinboththeencoderanddecoder,for14layerstotal,whilethetabulardatasetshadsixineach(12total).AlllinearlayerswerefollowedbyaLeakyReLUlayerandBatchNormalizationexceptfortheﬁnaloutputlayerinthedecoder.Fortheimagedatasets,thehiddenlayersizesfortheencoderwereasfollows:[784,600,500,400,300,200,100,20]andthesameinreverseorderforthede-coder.Fortabulardatasets,thebottlenecksizewassetequaltothenumberofPCAprincipalcomponentsneededtoexplain90%ofthedata,andthehiddenlayersizesdecrease(increase)linearlyintheencoder(decoder)accordingly.Allautoencodermodels(includingVAE)modelsfollowedthisstructureforthesakeofconsistency.Allmodelsweretrainedfor350epochswithearlystoppingacti-vatedifthelossfunction,meansquarederror,didnotachieveanewminimumfor20consecutiveepochs.Thebatchsizewassetat250samples.StandardDeviationinAUCScoreTable1showsthestandarddeviationsoftheAUCscoresovertheNtrialsforeachdatasetandsettings,whereNisthenumberofclassesinthedataset.Naturally,aseachsetuphasadiﬀerentsetofnormalandanomalousclasses,thescoresvarygreatly.Thereforeweshowtheiﬁcancevaluesforperformanceimprovementinthenextsection.One-sidedWilcoxonSigniﬁcanceTestInTable2,weshowthep-valuesoftheone-sidedWilcoxonsigned-ranktest[1].WetestourmethodagainstboththeAE,toseetheeﬀectoflocaladaptivity,aswellasRAPP-NAPperformance,asitwasgenerallythenextbest-performingmethod.Alowerp-valueindicatesmoreconﬁdencethatourmethodperformsbetteroverthecorrespondingbaselinemethod.Thebinary-classdatasetsarenotshownastheyonlyhadoneexperimentalsetup,i.e.oneAUCscoretomeasure.Thep-valuesfortheSNSR,MNISTandOTTOdatasetsinparticularareextremelylow.TheperformanceismorecomparablebetweenARESandthebaselinesforFMNIST,hencethelargerp-values.Themedianp-valueofARESagainstAEis0.0073andforRAPP-NAPitis0.016.arXiv:2206.07604v1  [cs.LG]  15 Jun 20222DatasetPCALOFOC-SVMAESAPNAPDAGMMARESMulti-ClassNormalitySNSR7.9118.4719.1617.2518.5220.1913.1516.73MNIST16.4814.9116.6522.6413.5712.5414.307.77FMNIST16.1217.0117.1017.4317.4815.2023.7916.23OTTO15.2422.7619.5817.6217.8820.3215.6418.09One-ClassNormalitySNSR5.411.122.541.320.660.676.380.94MNIST4.112.726.592.442.821.905.761.75FMNIST6.254.775.745.076.124.848.624.04OTTO8.646.467.176.157.637.278.444.98Table1:StandarddeviationofAUCscoresoverthediﬀerentruns.Table2:pvaluesfromtheone-sidedWilcoxonsigned-ranktest.Valueslowerthan0.05and0.01aremarkedwith*and**respectively.pvalueAERAPP-NAPOne-ClassNormalitySNSR0.0063**0.2969MNIST0.0083**0.0234*FMNIST0.93030.9937OTTO0.0038**0.0038**Multi-ClassNormalitySNSR0.0017**0.0029**MNIST0.0035**0.0035**FMNIST0.25380.6006OTTO0.06930.6165SupplementaryMaterial:Appendix3RuntimeTable3showstheruntimesoftheautoencodertraining,theregularAEanomalyscoringandourARESscoring.Weseethat,despitenoticeablylongercompu-tationaltimeforourmethodattesttime,itisveryinsigniﬁcantinrelationtotheoverallpipelineofthemodelincludingtrainingtime.TimeTrainingAEscoreARESscoreOne-Class1.474440.000140.01721Multi-Class24.291610.000370.87681Table3:Runtimes(minutes)oftrainingandtestingforAEandARESforcomparisonforeachnormalitysetting.RuntimesarecalculatedfortheMNISTdatasetandaveragedovereachofthetrialsforeachmodalitysetup.AnalysisTounderstandwhylocaladaptivityimprovesanomalydetectionperformance,westudyoneparticularproblemsetup:MNISTsamplesofclass8arenormalandallotherclassesareallanomalies.Figure1showsthereconstructionerrorofthetestsamplesofallclassesinthissetup.Weseethatreconstructionerrorsoftheclass8samplesaregenerallylowerthanthosefromotherclassesexceptforclass1.Thismeansthat,basedonreconstructionerroralone,manyanomaliesfromclass1willbefalselyclassiﬁedasnormal.Fig.1:Probabilitydensityfunctionsofreconstructionerrorsofeachclass(class8isnormalandothersareanomalous.4AREScomparesapointsreconstructionerroragainstitsnearestneighbourswhichprovideslocalcontextformoreaccuratedetection.AsshowninFigure3,theneighboursofapoorlyreconstructednormaltestpointtendtobethemorepoorlyreconstructedtrainingpoints,perhapsbecausethesetrainingpointsalsocontainedsomekindofabnormality.Withthis,theabnormalityofthesenormaltestpointsislessoutlyingwithinthecontextofthetrainingpointsaroundthem.Thiscontextisneglectedinthestandardscoringapproach,andsoislikelytodetectfalsepositivesforthesekindsofsamples,unlikeinourmethodwhichaccountsforthiscontext.Fig.2:t-SNEembeddingsofclass1testsamples(blue),class8testsamples(darkorange)andclass8trainingsamples(lightorange).Fig.3:Reconstructionerrorofnormal(8)testsamplesversustheaveragerecon-structionerroroftheirtrainingsetnearestneighbours.SupplementaryMaterial:Appendix5Furthermore,ARESalsousesthelocaldensityofthepoint,whichdependsnotonitsreconstructionerrorbutpurelyonitsdistancetothetrainingsamplesinthelatentspace.InFigure2,wecanseethatthevastmajorityofanomalousclass1samples(inblue)arefarawayfromtheirnearestneighboursinthetrainingsetthanthenormalclass8testsamples(indarkorange).Bycombiningthesetwoscores,weareabletodetectsuchanomaliesevenifthereconstructionerroralonecannot.ScalingfactorInTable4,weseetheaverageAUCscorefoundwhenusingdiﬀerentvaluesforthescalingfactorα.Alowerorhighervalueofthescalingfactorgivesmoreweighttothelocalreconstructionordensityscorerespectively.Generally,weseethatalowervalueofαisbetterintheone-classnormalitycase,whilehigherisbetterforthemulti-classnormalitycase,meaningthelocaldensityscoreismorebeneﬁcialwhenthediversityinthenormalsetislarger.ThiscorroboratestheresultsseeninTable5,asthelocalreconstructionscoreisrelativelybetterthanthelocaldensityscoreintheone-classnormalitycaseandviceversainthemulti-classnormalitycase.One-ClassNormalityMulti-ClassNormalityαMNISTFMNISTMNISTFMNIST0.197.5191.5288.0870.840.2597.7691.7091.5371.970.597.8991.6393.2572.49197.7291.1693.9172.49297.0090.2593.9072.17Table4:AverageAUCscore(%)ofARESwithdiﬀerentsettingsofscalingfactorαScoreComparisonTable5showstheperformanceofthelocaldensityandlocalreconstructionscoresseparately.Weseethatthelocalreconstructionscoretendstodobetterintheone-classnormalityexperimentswhilethelocaldensityscoreisbetterformulti-classnormalityexperiments.However,combiningbothofthemasinARESgivesthebestperformanceinmostcasesoverall.6Table5:AUCscoreofthetwocomponentsofARESinisolation.Datasetd(x)r(x)ARESOne-ClassNormalitySNSR96.2498.4998.83MNIST91.5997.2197.89FMNIST86.9791.1791.63OTTO77.1788.5787.86MI-F87.0868.6989.52MI-V87.9394.7393.94EOPT67.8860.1368.43Multi-ClassNormalitySNSR70.4662.6669.78MNIST93.2381.9693.25FMNIST71.0568.9772.49OTTO62.5460.8463.54DensityEstimationMethodTable6showstheperformanceofARESwithotherdensityestimationmethods.KNNisthedistancetothekthnearestneigh-bourinthelatentspace.GDisthedistanceofapointtoaGaussiandistributionﬁttothetrainingsetlatentencodingsortheclosestofNinthemultimodalcase.GDperformspoorlyinunimodalexperiments,howeveritisbetterinmultimodalexperimentsandeventhebestforFMNISTandOTTO.ThiscouldbeasthereareNseparatedistributionsusedforeachoftheNnormalclassesinthiscase.WithinGD,wetesttheuseoftheMahalanobisdistanceversustheEuclideandistance,andweseetheformeroutperformthelatterinmostcases.NFisthelikelihoodunderaRealNVPnormalizingﬂow[2].Otherﬂowsweretestedbutdidnotgivestableresults.Itisnoticeablyworsethantheothermethods;pre-viousstudieshavefoundthatnormalizingﬂowsarenotwell-suitedtodetectout-of-distributiondata.SupplementaryMaterial:Appendix7DatasetSNSRMNISTFMNISTOTTOMI-FMI-VEOPTOne-ClassNormalityLOFk=1098.8397.8991.6387.7689.5293.9468.43k=4098.7597.8592.5388.0479.3191.9962.28k=10098.5597.8093.0288.0869.6989.9860.80KNNk=598.6595.5392.4783.9977.2693.5579.45k=2098.6695.2492.9483.1476.1291.6067.63k=4098.5694.9392.9982.7674.2490.50864.91GDEuclidean97.2172.7479.4782.0880.8075.7761.11Mahalanobis95.6887.2990.2681.2780.4179.6163.35NF98.2997.3092.1486.7484.4792.8961.07Multi-ClassNormalityLOFk=1069.7893.2572.4963.54k=4067.6592.9072.7463.80k=10066.3692.0872.4864.73KNNk=568.6393.6873.5261.76k=2067.4292.9272.7761.74k=4066.5092.0672.2861.74GDEuclidean56.5574.4066.0059.38Mahalanobis61.8691.1473.0266.30NF60.6386.2570.4863.67Table6:MeanAUCscoresforeachchoiceoflocaldensityscore.Thebestscoresarehighlightedinbold.8EmbeddingSizeWealsostudytheeﬀectoflatentsize,orthedimensionalityoflatentencodings,ontheperformanceofourapproachaswellasthefollowingvariants:–AE:ThestandardAEmethodwithoutlocaladaptivity.–ARES-G:AEreconstructionscorecombinedwithlocaldensityscore.–L-R:Localreconstructionscoreonly.–L-D:Localdensityscoreonly.–ARES:Combinedlocalreconstructionanddensityscore.InFigure4,weseethiseﬀectfortheMNISTdatasetinparticular.Weseethatembeddingsizehaslittleeﬀectonreconstructionscoresintheone-classnormalitysettingbutreducesperformancessigniﬁcantlyinthemulti-classnor-malitysetting.Inbothsettings,however,thedensityscoresmostlyimprovewithincreasingembeddingsize.Overall,weseethatcombiningthelocalreconstruc-tionscorewiththedensityscoregivesthebestperformanceacrosstheboard;itismorerobusttovariationinhyper-parameterchoices.Fig.4:Performanceofvariouscomponentmethodsacrossarangeofembeddingsizes.References1.Demsar,J.:Statisticalcomparisonsofclassiﬁersovermultipledatasets.TheJournalofMachineLearningResearch7,1–30(2006)2.Papamakarios,G.,Pavlakou,T.,Murray,I.:Maskedautoregressiveﬂowfordensityestimation.In:NeurIPS.pp.2338–2347(2017)