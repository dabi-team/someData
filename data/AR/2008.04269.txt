1
2
0
2

v
o
N
8

]

M
E
.
n
o
c
e
[

2
v
9
6
2
4
0
.
8
0
0
2
:
v
i
X
r
a

Nonparametric prediction with spatial data

Abhimanyu Gupta ∗†

Javier Hidalgo ‡

November 9, 2021

Abstract

We describe a (nonparametric) prediction algorithm for spatial data, based on a canonical
factorization of the spectral density function. We provide theoretical results showing that the
predictor has desirable asymptotic properties. Finite sample performance is assessed in a Monte
Carlo study that also compares our algorithm to a rival nonparametric method based on the
inﬁnite AR representation of the dynamics of the data. Finally, we apply our methodology to
predict house prices in Los Angeles.

Keywords: Lattice data, unilateral models, canonical factorization, spectral density, nonpara-
metric prediction.

1

Introduction

Random models for spatial or spatio-temporal data play an important role in many disciplines of

economics, such as environmental, urban, development or agricultural economics as well as economic

geography, among others. When data is collected over time such models are termed ‘noncausal’

and have drawn interest in economics, see for instance Breidt et al. (2001) among others for some

early examples. Other studies may be found in the special volume by Baltagi et al. (2007) or

Cressie (1993). Classic treatments include the work by Mercer and Hall (1911) on wheat crop yield

data (see also Gao et al. (2006)) or Batchelor and Reed (1918) which was employed as an example

and analysed in the celebrated paper by Whittle (1954). Other illustrations are given in Cressie

and Huang (1999), see also Fernandez-Casal et al. (2003). With a view towards applications in

environmental and agricultural economics, Mitchell et al. (2005) employed a model of the type

studied in this paper to analyse the eﬀect of carbon dioxide on crops, whereas Genton and Koul

(2008) examine the yield of barley in UK. The latter manuscripts shed light on how these models

can be useful when there is evidence of spatial movement, such as that of pollutants, due to winds

or ocean currents.

∗Department of Economics, University of Essex, Wivenhoe Park, Colchester CO4 3SQ, U.K. Email:

a.gupta@essex.co.uk

†Research supported by ESRC grant ES/R006032/1.
‡Economics Department, London School of Economics, Houghton Street, London WC2A 2AE, U.K. Email:

f.j.hidalgo@lse.ac.uk

1

 
 
 
 
 
 
Doubtless one of the main aims when analysing data is to provide predicted values of realizations
of the process. More speciﬁcally, assume that we have a realization Xn = {xti}n
i=1 at locations
t1, ..., tn of a process {xt}t∈D, where D is a subset of Rd. We wish then to predict the value of xt
at some unobserved location t0, say xt0. For instance in a time series context, we wish to predict
the value xn+1 at the unobserved location (future time) n + 1 given a stretch of data x1, .., xn. It
is often the case that the predictor of xt0 is based on a weighted average of the data Xn, that is

(cid:98)xt0 =

n
(cid:88)

i=1

βixti,

(1.1)

where the weights β1, ..., βn are chosen to minimize the L2-risk function

(cid:32)

E

xt0 −

(cid:33)2

n
(cid:88)

i=1

bixti

with respect to b1, ..., bn. With spatial data, the solution in (1.1) is referred as the Kriging predictor,
see Stein (1999), which is also the best linear predictor for xt0. Notice that under Gaussianity or
our Condition C1 below, the best linear predictor is also the best predictor. It is important to

bear in mind that with spatial data prediction is also associated with both interpolation as well as

extrapolation.

The optimal weights {βi}n

i=1 in (1.1) depend on the covariogram (or variogram) structure of
{xt1, ..., xtn; xt0} =: {Xn; xt0}, see among others Stein (1999) or Cressie (1993). That is, denot-
(cid:1) =: C (ti, tj) and assuming stationarity, so that C (ti, tj) =:
ing the covariogram by Cov (cid:0)xti, xtj
C (|ti − tj|), we have that the best linear predictor (1.1) becomes

(cid:98)xt0 = γ(cid:48) (t0) CXn,

(1.2)

where

C = {C (|ti − tj|)}n

i,j=1 ;

γ(cid:48) (t0) = Cov (Xn; xt0) = E {xt0 (xt1, ..., xtn)} .

When the data is regularly observed, the unknown covariogram function C (h) is replaced by

its sample analogue

(cid:98)C (h) =

1
|n (h)|

(cid:88)

n(h)

xtixtj ,

where n (h) = {(ti, tj) : |ti − tj| = h} and |n (h)| denotes the cardinality of the set n (h). When
the data is not regularly spaced some modiﬁcations of (cid:98)C (h) have been suggested, see Cressie
(1993, p.70) for details. One problem with the above estimator (cid:98)C (h) is that it can only be employed
for lags h which are found in the data, and hence the Kriging predictor (1.2) cannot be computed if
|ti − t0| (cid:54)= h for any h such that n (h) is not an empty set. To avoid this problem a typical solution
is to assume some speciﬁc parametric function C (h) =: C (h; θ), so that one computes (1.2) with

(cid:16)

(cid:17)
h; (cid:98)θ

C

replacing C (h) therein, where (cid:98)θ is some estimator of θ.

2

In this paper, we shall consider the situation when the spatial data is collected regularly, that is

on a lattice. This may occur as a consequence of some planned experiment or due to a systematic

sampling scheme, or when we can regard the (possibly non-gridded) observations as the result of

aggregation over a set of covering regions rather than values at a particular site, see e.g. Conley

(1999), Conley and Molinari (2007), Bester et al. (2011), Wang et al. (2013), Nychka et al. (2015),

Bester et al. (2016). As a result of this ability to map locations to a regular grid, lattice data

are frequently studied in the econometrics literature, see e.g. Roknossadati and Zarepour (2010),

Robinson (2011) and Jenish (2016). Nonsystematic patterns may occur, although these might arise

as a consequence of missing observations, see Jenish and Prucha (2012) for a study that covers

irregular spatial data.

However contrary to the solution given in (1.2), our aim is to provide an estimator of (1.1)

without assuming any particular parameterization of the dynamic or covariogram structure of the

data a priori, for instance without assuming any particular functional form for the covariogram

C (h). The latter might be of interest as we avoid the risk that misspeciﬁcation might induce

on the predictor. In this sense, this paper may be seen as a spatial analog of contributions in a

standard time series context such as Bhansali (1974) and Hidalgo and Yajima (2002).

The remainder of the paper is organized as follows. In the next section, we describe the multi-

lateral and unilateral representation of the data and their links with a Wold-type decomposition.

We also describe the canonical factorization of the spectral density function, which plays an impor-

tant role in our prediction methodology described in Section 3, wherein we examine its statistical

properties. Section 4 describes a small Monte-Carlo experiment to gain some information regarding

the ﬁnite sample properties of the algorithm, and compares our frequency domain predictor to a

potential ‘space-domain’ competitor. Because land value and real-estate prices comprise classical

applications of spatial methods, see e.g. Iversen Jr (2001), Banerjee et al. (2004), Majumdar et al.

(2006), we apply the procedures to prediction of house prices in Los Angeles in Section 5. Fi-

nally, Section 6 gives a summary of the paper whereas the proofs are conﬁned to the mathematical

appendix.

2 Multilateral and unilateral representations

Before we describe how to predict the value of the process {xt}t∈Zd at unobserved locations, for
d ≥ 1, it is worth discussing what do we understand by multilateral and unilateral representations

of the process and, more importantly, the link with the Wold-type decomposition. Recall that in

the prediction theory of stationary time series, i.e. when d = 1, the Wold decomposition plays a
key role. For that purpose, and using the notation that for any a ∈ Zd, a = (a [1] , ..., a [d]), so that
t − j stands for (t [1] − j [1] , ...., t [d] − j [d]), we shall assume that the (spatial) process {xt}t∈Zd

3

admits a representation given by

xt =

(cid:88)

j∈Zd

ψjεt−j,

(cid:40) d

(cid:88)

(cid:88)

(cid:41)

j2 [(cid:96)]

(cid:12)
(cid:12)ψj

(cid:12)
(cid:12) < ∞,

j∈Zd

(cid:96)=1

(2.1)

where the εt are independent and identically distributed random variables with zero mean, unit
variance and ﬁnite fourth moments. The model in (2.1) denotes the dynamics of xt and it is known
as the multilateral representation of {xt}t∈Zd. It is worth pointing that a consequence of the latter
representation is that the sequence {εt}t∈Zd loses its interpretation as being the “prediction” error
of the model, and thus they can no longer be regarded as innovations, as was ﬁrst noticed by Whittle

(1954). When d = 1, this multilateral representation gives rise to so-called noncausal models or,

in Whittle’s terminology, linear transect models. These models can be regarded as forward looking

and have gained some consideration in economics, see for instance Lanne and Saikonnen (2011),

Davis et al. (2013), Lanne and Saikkonen (2013) or Cavaliere et al. (2020).

(cid:16)

It is worth remarking that, contrary to d = 1, it is not suﬃcient for the coeﬃcients ψj in
Indeed, suppose that
for any η > 0 as our next example illustrates.
(cid:110)(cid:80)d
(cid:16)

|j|−3−η(cid:17)
(2.1) to be O
ψj = (j [1] + j [2])−4 = O
not summable. That is, see for instance Limaye and Zeltser (2009),

. However it is known that the sequence

(cid:107)j(cid:107)−4(cid:17)

(cid:96)=1 j2 [(cid:96)]

(cid:12)
(cid:12) is

(cid:12)ψj

(cid:111) (cid:12)





c−1
N =

N
(cid:88)

(cid:40) d

(cid:88)

(cid:41)

j2 [(cid:96)]

−1


(cid:12)
(cid:12)


(cid:12)
(cid:12)ψj

j[1],j[2]=1

(cid:96)=1

→
N →∞

0.

(2.2)

One classical parameterization of (2.1) is the ARM A ﬁeld model

P (L) xt = Q (L) εt,

P (z) =

(cid:88)

j∈Zd
1

αjzj; α0 = 1; Q (z) =

βjzj;

β0 = 1,

(cid:88)

j∈Zd
2

1 and Zd

where Zd
00 = 1. As an example, we have the ARM A (−k1, k2; −(cid:96)1, (cid:96)2) ﬁeld

2 are ﬁnite subsets of Zd and henceforth zj = (cid:81)d

(cid:96)=1 z [(cid:96)]j[(cid:96)] with the convention that

k2(cid:88)

j=−k1

αjxt−j =

(cid:96)2(cid:88)

j=−(cid:96)1

βjεt−j,

α0 = β0 = 1.

(2.3)

As mentioned above, the Wold decomposition, and hence the concept of past and future, plays

a key role in the theory of prediction when d = 1. However, contrary to the situation when d = 1,

an intrinsic problem with spatial or lattice data is that we cannot assign a unique meaning to the

concept of “past” and/or “future”. One immediate consequence is then that diﬀerent deﬁnitions of

what might be considered as past (or future) will yield diﬀerent Wold-type decompositions. More
speciﬁcally, denote a “half-plane” of Z2 according to the lexicographical (dictionary) ordering “≺”

4

deﬁned as

j ≺ k ⇔ (j [1] < k [1]) or (j [1] = k [1] ∨ j [2] < k [2]) ,

(2.4)

where herewith we shall consider the case when d = 2, often encountered with real data. The half-

plane deﬁned by “≺” is illustrated in Figure 1. Following earlier work by Helson and Lowdenslager
(1958, 1961), there exists then a Wold-type representation of the (spatial) process {xt}t∈Z2 given
by

xt = ϑt +

ζ jϑt−j,

(cid:88)

(cid:88)

(cid:12)
(cid:12)ζ j

(cid:12)
(cid:12) < ∞,

(2.5)

0≺j

0≺j

where {ϑt}t∈Z2 is a zero mean white noise sequence with ﬁnite second moments σ2
ϑ. It is worth
recalling that ϑt once again has the interpretation of being the “one-step” prediction error. Often
(2.5) is called a unilateral representation of {xt}t∈Z2 as opposed to the multilateral representation
in (2.1). See also Whittle (1954) for some earlier work on multilateral versus unilateral represen-
tations. As an example, (2.3) becomes a unilateral or causal model when (cid:96)1 = k1 = 0.
(2.5)
might be regarded as a particular way to model the dependence of xt induced by the lexicographic
ordering in (2.4). Of course, the choice of the “half-plane” of Z2 according to the associated cho-
sen lexicographic ordering is not the only possible one. That is, a diﬀerent choice of “half-plane”
of Z2, induced by the lexicographic ordering, will yield a “similar” but diﬀerent representation
of xt to that given in (2.5). As it will become clear in the next section, the choice of a speciﬁc
lexicographic ordering, or its associated half-plane, will depend very much on practical purposes.
For instance, the choice of (2.4) will depend on the location where we wish to predict xt. Last
but not least it is worth, and important, mentioning that the sequences {εt}t∈Z2 and {ϑt}t∈Z2 are
not the same. Recall that a similar phenomenon occurs when d = 1 and the practitioner allows
for noncausal/bilateral representations of the sequence xt. When this is the case, the “bilateral or
noncausal” representation has errors which are independent and identically distributed, whereas for

its “unilateral or causal”representation, the corresponding errors are only a white noise sequence.

It is clear from the introduction that to provide accurate and valid (linear) predictions (or
interpolations), a key component is to obtain the covariogram function of the sequence {xt}t∈Z2,
that is C (h) = Cov (xt, xt+h), which is related to the spectral density function f (λ) via the
expression

(cid:90)

C (h) =

f (λ) e−ih·λdλ, h ∈ Z2,

where Π = (−π, π]. Henceforth the notation “h · λ” means the inner product of the vectors h and

λ. It is worth observing that we can factorize f (λ) as

Π2

f (λ) =

σ2
(2π)2 |Ψ (λ)|2 =:
ε

σ2
(2π)2 |Υ (λ)|2 ,
ϑ

λ ∈ Π2,

5

where σ2

ε = Eε2

t and σ2

ϑ = Eϑ2

t , and

Ψ (λ) =

(cid:88)

j∈Z2

ψje−ij·λ;

Υ (λ) = 1 +

ζ je−ij·λ.

(cid:88)

0≺j

The latter displayed expressions indicate that either Ψ (λ) or Υ (λ) summarize the covariogram
structure of {xt}t∈Z2.

When d = 1 and the sequence {xt}t∈Z is purely nondeterministic we know, see Whittle (1961, p.26)

or Brillinger (1981, Theorem 3.8.4), that the spectral density f (λ) admits a representation

f (λ) =:

exp (−α0)
2π

|A (λ)|2 =

1
2π

(cid:40)

exp

−α0 − 2

(cid:41)

αk cos (kλ)

,

∞
(cid:88)

k=1

k=1 αkeik·λ(cid:9). The latter expression is referred to as the
where by deﬁnition A (λ) =: exp (cid:8)− (cid:80)∞
canonical factorization of the spectral density function and is also known as Bloomﬁeld’s model.
One important consequence of the canonical factorization is that the sequence {xt}t∈Z can be
written as

∞
(cid:88)

xt +

ajxt−j = ϑt,

where ϑt is a zero mean white noise sequence with ﬁnite second moments and aj are the Fourier
coeﬃcients of A (λ), that is

j=1

aj =

1
2π

(cid:90) π

−π

A (λ) eij·λdλ;

0 < j,

with 2π exp (α0) = σ2

ϑ, i.e. the one-step prediction error. However, more importantly, denoting

B (λ) = A−1 (λ) = exp

(cid:41)

αkeik·λ

,

(cid:40) ∞
(cid:88)

k=1

we have that its Fourier coeﬃcients equal the coeﬃcients ζ j in (2.5).

Whittle (1954), Section 6, signalled that a similar argument can be used when d > 1. However

a formal and theoretical justiﬁcation for a canonical factorization of f (λ) when d > 1 was discussed

in Korezlioglu and Loubaton (1986), see also Solo (1986). More speciﬁcally, they show that the
spectral density function of {xt}t∈Z2 might be characterized using the representation

f (λ) =:

exp (−α0)
(2π)2

|A (λ)|2 =

1
(2π)2 exp

(cid:40)

−α0 − 2

(cid:88)

0≺k

(cid:41)

αk cos (k · λ)

,

(2.6)

where

(cid:40)

A (λ) =: exp

−

(cid:41)

αkeik·λ

,

(cid:88)

0≺k

(2.7)

6

Figure 1: Half-plane illustration for d = 2. Circles form the half plane ≺0 while solid dots form the half plane 0≺.
The large black solid dot marks the origin.

which is sometimes known as the Cepstrum model by Solo (1986), who notes that if 0 < f (λ) < M

then the representation of the spectral density in (2.6) or in (2.7) exists, see also McElroy and
Holan (2014). Note that the coeﬃcients αk in (2.7) are the Fourier coeﬃcients of log (f (λ)), that
is

log (f (λ)) cos (k · λ) dλ,

0 ≺ k and k = 0,

(2.8)

αk =

(cid:90)

1
2π2

(cid:101)Π2

where (cid:101)Π2 = [0, π] × Π, that is λ ∈ (cid:101)Π2 if λ [1] ∈ [0, π] and λ [2] ∈ Π.

As it is the case when d = 1, there is a relationship between the representation in (2.5) and
(2.6) / (2.7), i.e. between the coeﬃcients ζ j and αk. So, it will be convenient to discuss the
relationship between the representations of the sequence {xt}t∈Z2 in the “frequency” and “space”
domains. The link among these coeﬃcients turns out to play a crucial role in our prediction

algorithm. For that purpose, consider the lexicographic ordering given in (2.4). Then, denoting

the Fourier coeﬃcients of A (λ) by

aj =

1
4π2

(cid:90)

Π2

A (λ) eij·λdλ;

0 ≺ j,

and a0 = 1, the sequence {xt}t∈Z2 has a unilateral representation given by

xt +

(cid:88)

0≺j

ajxt−j = ϑt,

(2.9)

(2.10)

where {ϑt}t∈Z2 is the sequence given in (2.5). But also we have that the coeﬃcients ζ j in (2.5) are
the Fourier coeﬃcients of B (λ) =: A−1 (λ) = exp (cid:8)(cid:80)

0≺k αke−ik·λ(cid:9). That is,

ζ j =

1
(2π)2

(cid:90)

Π2

B (λ) eij·λdλ,

0 ≺ j;

ζ 0 = 1,

see Section 1.2 of Korezlioglu and Loubaton (1986). The latter might be considered as an extension

of the canonical factorization given in Brillinger (1981) to the case d > 1. However, one key aspect
is that there is a direct link between αk and the coeﬃcients of the Wold-type decomposition of
its autoregressive representation, that is aj/ζ j and αk. This observation will be important for our

7

prediction methodology in the next section.

3 Prediction algorithm

The purpose of the section is to present and examine a prediction algorithm, extending the method-

ology in Bhansali (1974) or Hidalgo and Yajima (2002), to the case when d = 2. Similar to the

aforementioned work, a key component of the methodology will be based on the canonical factor-

ization of the spectral density in (2.6). Due to the rather unusual notation in this paper, we have

decided to collate it at this stage for convenience. Given two vectors a and b, a ≥ (≤) b means that

a [(cid:96)] ≥ (≤) b [(cid:96)] for all (cid:96) = 1, 2. Denote

(cid:26)

Π2

n =

λk[(cid:96)] =

2πk [(cid:96)]
n [(cid:96)]

, k [(cid:96)] = 0, ±1, ..., ±˜n [(cid:96)] =:

n [(cid:96)]
2

,

(cid:27)

(cid:96) = 1, 2

,

where λk = (cid:0)λk[1], λk[2]
denote

(cid:1) are the Fourier frequencies and (cid:101)Π2

n = (cid:8)λk ∈ Π2

n : λk[1] > 0(cid:9). Finally, we

(cid:90) +

λ(cid:22)π

(cid:90)

=

=

(cid:90) π

(cid:90) π

λ[1]=0

(cid:90) b[1]

λ[2]=−π
(cid:90) b[2]

(cid:90) −

=

;

(cid:90) 0

(cid:90) π

;

λ(cid:22)π

λ[1]=−π

λ[2]=−π

(3.1)

.

a≤λ≤b

λ[1]=a[1]

λ[2]=b[2]

Similarly, we denote

(cid:88)

+

j(cid:22)J

(cid:88)

−

j(cid:22)J

cj =

J[2]
(cid:88)

c0,j[2] +

J[1]
(cid:88)

J[2]
(cid:88)

cj[1],j[2];

j[2]=1

j[1]=1

j[2]=1−J[2]

cj =

0
(cid:88)

c0,j[2] +

0
(cid:88)

J[2]
(cid:88)

j[2]=1−J[2]

j[1]=1−J[1]

j[2]=1−J[2]

(cid:88)

=

b[1]
(cid:88)

b[2]
(cid:88)

,

a≤t≤b

t[1]=a[1]

t[2]=a[2]

cj[1],j[2];

(3.2)

where we are using the convention that for any k ∈ Z2, we write dk as

dk = dk[1],k[2].

Observe that (cid:80)+

j(cid:22)J + (cid:80)−

j(cid:22)J = (cid:80)

−J<j≤J , and likewise (cid:82) +

λ(cid:22)π + (cid:82) −

λ(cid:22)π = (cid:82)

λ∈Π2.

Before we describe our prediction algorithm, we shall introduce our set of regularity conditions.

Condition C1 (a) {ϑt}t∈Z2 in (2.5) is a zero mean white noise sequence of random variables with

variance σ2

ϑ and ﬁnite 4th moments, with κ4,ϑ denoting the fourth cumulant of ϑt.

(b) The unilateral Moving Average representation of {xt}t∈Z2 in (2.5) can be written (or it

8

has a representation) as a unilateral Autoregressive model

xt +

(cid:88)

0≺j

ajxt−j = ϑt.

(3.3)

(c) The coeﬃcients in ζ j in (2.5) satisfy

(cid:40) 2

(cid:88)

(cid:88)

(cid:41)

j4 [(cid:96)]

0≺j

(cid:96)=1

(cid:12)
(cid:12)ζ j

(cid:12)
(cid:12) < ∞.

Condition C2 n = (n [1] , n [2]) satisﬁes that n [1] (cid:16) n [2] where “a (cid:16) b” means that K−1 ≤ a/b ≤

K for some ﬁnite positive constant K.

We now comment on Conditions C1 and C2. First, Condition C2 can be generalized to allow
for diﬀerent rates of convergence to zero of n−1 [(cid:96)], (cid:96) = 1, 2. However, for notational simplicity, we
prefer to keep it as it stands. Condition C1 could have been written in terms of the multilateral

representation in (2.1). However since the prediction employs the representation in (3.3) or (2.5),

we have opted to write C1 as it stands. Part (a) of Condition C1 seems to be a minimal condition

for our results below to hold true. Suﬃcient regularity conditions required for the validity of the

expansion in (3.3) is Υ (z) be nonzero for any z [(cid:96)], (cid:96) = 1, 2. The latter condition guarantees that
f (λ) > 0 for all λ ∈ (cid:101)Π2. Part (c) entails that the spectral density f (λ) is 4 times continuously
diﬀerentiable. This is needed if one wants to achieve a similar rate of approximation of sums by

their integrals when d = 1 and the function is twice continuously diﬀerentiable. Indeed whereas

when d = 1, we have that

1
n

n
(cid:88)

i=1

(cid:19)

(cid:18) i
n

g

−

(cid:90) 1

0

g (x) =

1
n

(g (0) − g (1)) + O (cid:0)n−2(cid:1) ,

with two continuous derivatives for g (x), to have a “similar” result when d = 2 one needs g(x) to

be 4 times continuously diﬀerentiable. See Lemma 6 in the appendix for some extra insight.

We now discuss the methodology to predict the value of xt at an unobserved location without
imposing any speciﬁc parametric model for f (λ). In addition, as a by-product, we provide a simple
estimator of the coeﬃcients ζ j or aj. First, A (λ) and expression (2.8) suggest that to compute an
estimator of the coeﬃcients αj and/or aj, it suﬃces to obtain an estimator of f (λ). To that end,
for a generic sequence {vt}n

t=1, we shall deﬁne the discrete Fourier transform, DF T , as

and the periodogram as

wv (λ) =

1
n1/2

(cid:88)

vte−it·λ,

1≤t≤n

Iv (λ) =

1

(2π)2 |wv (λ)|2 ;

λ ∈ (cid:101)Π2,

9

where, in what follows, we use the notation that for any g = (g [1] , g [2]),

g = g [1] g [2] .

(3.4)

In real applications, in order to make use of the fast Fourier transform, the periodogram will be
evaluated at the Fourier frequencies λk.

However as noted by Guyon (1982), due to non-negligible end eﬀects (the edge eﬀect), the bias

of the periodogram does not converge to zero fast enough when d > 1. We therefore proceed as in

Dahlhaus and K¨unsch (1987), and employ the tapered periodogram deﬁned as

I T
v (λj) =

1
(2π)2

(cid:12)
(cid:12)wT

v (λj)(cid:12)
2
(cid:12)

;

wT

v (λj) =

1

(cid:16)(cid:80)

1≤t≤n h2
t

(cid:17)1/2

(cid:88)

1≤t≤n

htvteit·λj ,

(3.5)

where wT
cosine-bell (or Hanning) function, which is deﬁned as

v (λj) denotes the taper discrete Fourier transform, DF T . One common taper is the

ht =

1
4

ht[1]ht[2];

ht[(cid:96)] =

1 − cos

(cid:18)

(cid:18) 2πt [(cid:96)]
n [(cid:96)]

(cid:19)(cid:19)

,

(3.6)

see Brillinger (1981). It is worth observing the cosine-bell taper DFT is related to wv (λ) by the
equality

wT

v (λj) =

1
6

2
(cid:89)

(cid:96)=1

(cid:2)−wv

(cid:0)λj[(cid:96)]−1

(cid:1) + 2wv

(cid:0)λj[(cid:96)]

(cid:1) − wv

(cid:0)λj[(cid:96)]+1

(cid:1)(cid:3) .

(3.7)

In this paper we shall explicitly consider the cosine-bell, although the same results follow employing

other taper functions such as Parzen or Kolmogorov tapers (Brillinger, 1981). This is formalized

in the next condition.

Condition C3 {ht}n

t=1 is the cosine-bell taper function in (3.6).

Using notation in (3.4), we shall estimate f (λ) by the average tapered periodogram

(cid:98)f (λ) =

1
4m

(cid:88)

I T
x (λ + λ(cid:96)) ,

−m<(cid:96)≤m

where m [(cid:96)] /n [(cid:96)] + m [(cid:96)]−1 = o (1), for (cid:96) = 1, 2. Next, we denote (cid:101)λk =
0, 1, ..., M [1] =: ˜n [1] /m [1] and k [2] = 0, ±1, ..., ±M [2] =: ˜n [2] /m [2], where

(cid:16)

(cid:101)λk[1], (cid:101)λk[2]

(3.8)

(cid:17)(cid:48)

, for k [1] =

(cid:101)λk[(cid:96)] =

πk [(cid:96)]
M [(cid:96)]

;

(cid:96) = 1, 2.

Bearing in mind (3.2), denoting M = {j :

(0 ≺ j; j = 0) ∧ (−M < j ≤ M )} and abbreviating

10

(cid:17)

(cid:16)

φ

(cid:101)λk

by φk for a generic function φ (λ), we estimate the coeﬃcients aj, j = 1, ..., M , as

(cid:98)aj =

1
4M

(cid:88)

−M <k≤M

(cid:98)Akeij·(cid:101)λk ,

j ∈ M;

(cid:98)Ak = (cid:98)A−k = exp




−

(cid:88)

+



j(cid:22)M

(cid:98)αje−ij·(cid:101)λk






,

k ∈ M∪ {0} ;

(cid:98)αj =

1
2M

(cid:88)

+

(cid:16)

cos

j · (cid:101)λk

(cid:17)

log (cid:98)fk,

j ∈ M.

k(cid:22)M

(3.9)

(3.10)

It is also worth deﬁning the quantities (3.9) and (3.10) when (cid:98)f (λ) is replaced by f (λ), that is

That is,

(cid:101)aj,n =

1
4M

(cid:101)f (λ) =

1
4m

(cid:88)

f (λ + λk) .

−m<k≤m

(cid:88)

(cid:101)Ak,neij·(cid:101)λk

j ∈ M

−M <k≤M



(cid:101)Ak,n = (cid:101)Ak,n = exp

−

(cid:88)

+



j(cid:22)M

(cid:101)αj,ne−ij·(cid:101)λk






k ∈ M∪ {0}

(3.11)

(cid:101)αj,n =

1
2M

(cid:88)

+

(cid:16)

cos

j · (cid:101)λk

(cid:17)

log (cid:101)fk

k(cid:22)M

j ∈ M∪ {0}

and also we denote

aj,n =

1
4M

(cid:88)

Ak,neij·(cid:101)λk

j ∈ M

−M <k≤M



Ak,n = Ak,n = exp

−

(cid:88)

+



j(cid:22)M

αj,ne−ij·(cid:101)λk






k ∈ M∪ {0}

(3.12)

αj,n =

1
2M

(cid:88)

+

(cid:16)

cos

j · (cid:101)λk

(cid:17)

log fk

k(cid:22)M

j ∈ M∪ {0} .

We shall now begin describing how we can predict a value xt at the location s = (s [1] , s [2])
such that 1 ≤ s [1] ≤ n [1] and 1 ≤ s [2] ≤ n [2]. For instance, we wish to predict the unobserved

11

value xs

•

•

•

•

•

•

•

•

•

•

•

•

•

•

•

•

•

•

•

•

•

•

•

•

•

•

•

•

•

•

•

•

(s [1] , s [2])?

•

•

•

•

•

•

•

or

•

•

•

•

•

•

•

•

•

•

•

•

•

•

•

•

•

•

•

•

•

•

•

•

•

•

•

(s [1] , s [2])?

•

•

Now, the location of s suggests that a convenient unilateral representation of xt appears to be

xt = −

∞
(cid:88)

k[2]=1

a0,k[2] xt[1],t[2]−k[2] −

∞
(cid:88)

∞
(cid:88)

k[1]=1

k[2]=−∞

ak xt−k + ϑt,

(3.13)

which comes from the lexicographic ordering in (2.4). Since we need to estimate the coeﬃcients
ak, the prediction will then become

(cid:98)xs[1],s[2] = −

M [2]
(cid:88)

k[2]=1

(cid:98)a0,k[2] xs[1],s[2]−k[2] −

M [1]
(cid:88)

M [2]
(cid:88)

k[1]=1

k[2]=1−M [2]

(cid:98)ak xs−k,

(3.14)

where (cid:98)ak xs−k =: (cid:98)ak[1],k[2] xs[1]−k[1],s[2]−k[2]. However, it may be very plausible that the value of M
is such that we may not observe the process at some of the locations employed to compute (3.14).
That is, consider the situation where we want to predict xs

•

•

•

•

•

•

•

•

•

•

•

•

•

•

•

•

•

•

•

•

•

•

•

•

•

•

•

•

•

•

(s [1] , s [2])?

•

•

•

•

•

•

•

•

•

In this case we observe that to compute (3.14), we ﬁrst need to obtain a predictor of values
of xs−k when say k [1] = 1 and k [2] < 0, since xs−k is not observed at those locations, which
in its computation needs predictors of the relevant values themselves. See (3.18) for more exact

details. However, in this case one can avoid this extra computational burden. Indeed, this is so as

the relative location (s [1] , s [2]) suggests that the practitioner might have employed the Wold-type

representation

xt = −

∞
(cid:88)

k[1]=1

ak[1],0 xt[1]−k[1],t[2] −

∞
(cid:88)

∞
(cid:88)

k[2]=1

k[1]=−∞

ak xt−k + ϑt

(3.15)

which can be regarded as induced by the lexicographic ordering

j ≺ k ⇔ (j [2] < k [2]) or (j [2] = k [2] ∨ j [1] < k [1]) .

(3.16)

12

Note that the lexicographic ordering (3.16) is as that in (2.4) but swapping j [2] for j [1]. From

here, we proceed as with (3.14) but with the “coordinates” [2] and [1] changing their roles.

Finally, consider the case where location we wish to predict xs is (n [1] + 1, s [2]). That is,

.

•

•

•

•

.

•

•

•

•

.

•

•

•

•

.

•

•

•

•

.

•

•

•

•

.

•

•

•

•

.

.

.

(n [1] + 1, s [2])

.

Now, the location of s =: (n [1] + 1, s [2]) suggests that the more convenient representation of
xs appears to be that in (3.13) which comes from the lexicographic ordering in (2.4), and hence our
prediction is given in (3.14). That is, since we need to estimate the coeﬃcients ak, the prediction
will then become

(cid:98)xn[1]+1,s[2] = −

M [2]
(cid:88)

k[2]=1

(cid:98)a0,k[2] xn[1]+1,s[2]−k[2] −

M [1]
(cid:88)

M [2]
(cid:88)

k[1]=1

k[2]=1−M [2]

(cid:98)ak xs−k.

(3.17)

However to compute the prediction we also need to replace the unobserved xs by its prediction.
As with “standard” time series when we wish to predict beyond 1 period ahead, this is done by
recursion, that is we make use of formula (3.14) starting say from the value xn[1]+1,s[2]−M [2]. Once
we have “predicted” the value for this observation, we then predict xn[1]+1,s[2]−M [2]+1 and so on.
For instance, for any r [1] = 0, ..., r and r [2] = 0, ..., r = min {n [2] /8; M [2]},

(cid:98)xt[1]−r[1],t[2]−r[2] = −

−

M [2]
(cid:88)

k[2]=1

M [1]
(cid:88)

(cid:98)a0,k[2] (cid:98)xt[1]−r[1],t[2]−r[2]−k[2]

(3.18)

M [2]
(cid:88)

(cid:98)ak[1],k[2] (cid:98)xt[1]−r[1]−k[1],t[2]−r[2]−k[2],

k[1]=1

k[2]=1−M [2]

where we take the convention that (cid:98)xs = xs if the location were observed and =: 0 when s [2] < −r or
{s [2] < 0 ∧ s [1] < n [1] − r}. Finally, if we were interested to predict xt at the unobserved location
(t [1] , n [2] + 1), then it suggests to employ the lexicographic ordering in (3.16) and hence the

representation given in (3.15), and then we would proceed as above but again with the “coordinates”

[2] and [1] changing their roles.

Before we examine the statistical properties of (cid:98)xt in (3.14) or (3.17), we shall look at those of

(cid:98)αj or (cid:98)Aj. For that purpose, denote

δj

: = 1 if j = 0 and := 0 otherwise

φk =

(cid:40)

i 1−cos(kπ)
kπ
1,

,

if k ∈ N+
k = 0.
if

(3.19)

13

Also, denote (cid:8)ξj

(cid:9)
j

the Fourier coeﬃcients of g (λ) given by

g (λ) =

1
6

(f11 (λ) + f22 (λ)) ;

λ ∈ Π2

(3.20)

f(cid:96)1(cid:96)2 (λ) =

∂2
∂λ[(cid:96)1]∂λ[(cid:96)2]

f (λ) ; (cid:96)1, (cid:96)2 = 1, 2.

Notice that Condition C1 implies that g (λ) is twice continuous diﬀerentiable, so that (cid:8)ξj
summable.

(cid:9)
j

is

We introduce one extra condition relating the rate of increase of m [(cid:96)] with respect of n [(cid:96)].

Condition C4 n [(cid:96)] , m [(cid:96)] → ∞, for (cid:96) = 1, 2, such that

n3 [(cid:96)]
m4 [(cid:96)]

+

m [(cid:96)]
n [(cid:96)]

→ 0

(cid:96) = 1, 2.

Theorem 1. Under C1 − C4, for any ﬁnite integer J, we have that

(a) n1/2 ((cid:98)αj − (cid:101)αj,n)J

j=1

d→ N (0, Ωα) ,

(b)

(cid:101)αj,n − αj,n = O (cid:0)M−1ξj + M2(cid:1) ,

j = 1, ..., J,

where Ωa is a diagonal matrix whose (j, j)-th element is 1 + (1 + κ4,ϑ) δj.

Remark Because σ2

ϑ = 2π exp (α0), we have that (cid:98)σ2

ϑ =: 2π exp ((cid:98)α0) is a consistent estimator of
σ2
ϑ. Indeed, by standard delta methods, the proof follows using Theorem 1 and that Lemma
6 implies that α0,n − α0 = O (cid:0)M−1/2(cid:1).

Theorem 2. Under C1 − C4, for any ﬁnite integer J, we have that

(a)

m1/2 (cid:16)

(cid:98)Aj − (cid:101)Aj,n

(cid:17)J

j=1

d→ N c (0, ΩA) ,

(b)

(cid:101)Aj,n − Aj,n =

gjAj,n + o

(cid:16)

m−1/2(cid:17)

,

1
M

j = 1, ..., J,

(cid:16)

(cid:17)

where gj = g
with the (j1, j2)-th element of ΩA given by

(cid:101)λj

is given in (3.20) and N c (0, ΩA) denotes a complex normal random variable

ΩA,j1j2 = 2

(cid:16)

δj1[1]−j2[1] + 2−1φj1[1]φj2[1] − iφj1[1]−j2[1]

(cid:17)

δj1[2]±j2[2]Aj1Aj2.

We shall now denote aυ = 0 if υ ≺ 0.

Theorem 3. Under C1 − C4, for any ﬁnite integer J, we have that

(a) n1/2 ((cid:98)aj − (cid:101)aj,n)J
(b) n1/2 ((cid:101)aj,n − aj,n) → 0.

j=1

d→ N (0, Ωa) ,

14

where a typical element (j1, j2) of Ωa, with j1 (cid:22) j2, is (cid:80)

0(cid:22)k akak+j2−j1.

Once we have obtained the asymptotic properties of the estimators of aj, for 0 ≺ j and j ≤ M ,
we are in a position to examine the asymptotic properties of the predictor (cid:98)xs in (3.14) or (3.17).
To that end, denote by {x∗
t }t∈Z2 a new independent replicate sequence with the same statistical
properties of the original sequence {xt}t∈Z2 not used in the estimation of the spectral density
function. Then let (cid:98)x∗

s be as in (3.14) but with (cid:98)xt replaced by x∗

t there, that is

(cid:98)xs[1],s[2] = −

M [2]
(cid:88)

k[2]=1

(cid:98)a0,k[2] x∗

s[1],s[2]−k[2] −

M [1]
(cid:88)

M [2]
(cid:88)

k[1]=1

k[2]=1−M [2]

(cid:98)akx∗

s−k,

or (3.17) but with (cid:98)xt being replaced by x∗

t there, that is

(cid:98)x∗
t[1]−r[1],t[2]−r[2] = −

M [2]
(cid:88)

k[2]=1

(cid:98)a0,k[2] (cid:98)x∗

t[1]−r[1],t[2]−r[2]−k[2] −

M [1]
(cid:88)

M [2]
(cid:88)

k[1]=1

k[2]=1−M [2]

(cid:98)akx∗

(s−r)−k.

Theorem 4. Under C1 − C4, we have that

(a)

(cid:16)

AE

s[1],s[2] − x∗
(cid:98)x∗

s[1],s[2]

(cid:16)

(b) AE

n[1]+1,t[2] − x∗
(cid:98)x∗

n[1]+1,t[2]

(cid:17)2

(cid:17)2

= σ2
ϑ,


=

1 +


 σ2
ϑ,

ζ 2
0,k[2]

∞
(cid:88)

k[2]=1

where AE denotes the “ Asymptotic Expectation”.

4 Monte Carlo experiment

We examine the ﬁnite-sample behaviour of our algorithm in a set of Monte Carlo simulations. As

in Robinson and Vidal Sanz (2006) and Robinson (2007) we used the model

xt = (cid:15)t + τ

1
(cid:88)

1
(cid:88)

(cid:15)t−s,

s1=−1

s2=−1

s(cid:54)=0

similar to one considered in Haining (1978). Then

f (λ) = (2π)−2 {1 + τ ν (λ)} ,

(4.1)

(4.2)

with ν (λ) = (cid:81)2
condition for invertibility of (4.1) is

j=1 (1 + 2 cos λj) − 1. Robinson and Vidal Sanz (2006) show that a suﬃcient

|τ | < 1/8.

(4.3)

We ﬁrst generated a 40 × 41 lattice using (4.1), with τ = 0.05, 0.075, 0.10 and the (cid:15)t drawn

15

independently from three diﬀerent distributions for each τ : U (−5, 5), N (0, 1) and χ2
9 − 9. The
aim of this section is to examine the performance of both prediction algorithms in predicting the

20, 20-th element of this lattice. We did this by assuming a situation in which the practitioner has

available data sets of various sizes, generated from (4.1). To permit a clear like-for-like comparison

of improvement in performance as sample size increases, we construct the prediction coeﬃcients

using the samples generated in each replication and then use these to construct predictions for the

20,20-th element of the 40 × 41 lattice.

We took n[1] = n∗ + 1 and n[2] = 2n∗ + 1, for some positive integer n∗, implying n =
(2n∗ + 1) (n∗ + 1), and generated iid (cid:15)t from each of the three distributions mentioned in the
previous paragraph. In each of the 1000 replications we experimented with τ = 0.05, 0.075, 0.10
and n∗ = 5, 10, 20 and 40. The choices of τ satisfy (4.3).

Given the diﬀerent sample sizes in each dimension, we can experiment with more values of

m[1], m[2] and p1, p2 as n∗ increases. We make the following choices:

m[1] = m[2] = 1; p∗ = p1 = p2 = 1, 2, when n∗ = 5,
m[1] = 1, 2; m[2] = 1, 2; p∗ = p1 = p2 = 1, 2, 3, when n∗ = 10,
m[1] = 1, 2, 3; m[2] = 1, 2, 3, 4, 5; p∗ = p1 = p2 = 1, 2, 3, when n∗ = 20,
m[1] = m[2] = 1, 2, 3, 4, 5; p∗ = p1 = p2 = 1, 2, 3, 4, 5 when n∗ = 40.

The ﬂexible exponential approach requires a nonparametric estimate of f (λ). Two such esti-

mates are available to use: the ﬁrst one based on the tapered periodogram described in (3.8), which
we denote ˆf (λ), and the second based on the autoregressive approach in Gupta (2018). The latter
also provides a rival prediction methodology based on a nonparametric algorithm using AR model

ﬁtting, extending well established results for d = 1, see Bhansali (1978) and Lewis and Reinsel

(1985). The idea is ﬁrst to obtain a least squares predictor based on a truncated autoregression
of order p = (pL1, pU1; pL2, pU2), for non-negative integers pL(cid:96), pU(cid:96), (cid:96) = 1, 2, with the truncation
allowed to diverge as N → ∞. That is, we approximate the inﬁnite unilateral representation in

(2.10) by one of increasing order.

In view of the half-plane representation we can a priori set, say, pL2 = 0 when considering (cid:52).
If we could observe the AR prediction coeﬃcients ak, say, a prediction of xs based on (cid:52) could be
constructed as

ˇxs =

(cid:88)

ak ˇxs−k,

(4.4)

k∈S[−pL,pU ]
where S [−pL, pU ] is the intersection of the set {t ∈ L : −pL(cid:96) ≤ t(cid:96) ≤ pU(cid:96), (cid:96) = 1, 2} with the predic-
tion half-plane. This is the spatial version of one-step prediction and again we follow the convention
that ˇxs = xs if xs is observed. However (4.4) is not feasible and needs to be replaced by an ap-
proximate version, as described below.

Writing p(cid:96) = pL(cid:96) + pU(cid:96), we assume throughout that n[(cid:96)] > p(cid:96) for (cid:96) = 1, 2, and denote np =
(cid:96)=1 (n[(cid:96)] − p(cid:96)), h(p) = pU2 + (p1 + 1) pU2 , i.e. the cardinality of S [−pL, pU ]. Suppose that the

(cid:81)2

16

data are observed on {(t1, t2) : nL1 ≤ t1 ≤ nU1, −nL2 ≤ t2 ≤ nU2}. Deﬁne a least squares predictor
of order h(p) by

ˇdp = arg minak,k∈S[−pL,pU ]n−1

p



xj −

(cid:88)

(cid:48)(cid:48)

j(p,n)

(cid:88)


2

akxk−j



,

k∈S[−pL,pU ]

(4.5)

where (cid:80)(cid:48)(cid:48)
the elements of ˇdp by ˇdp(k), k ∈ S [−pL, pU ], and the minimum value by ˇσ2
prediction based on a ﬁtted autoregression of order p is given by

j(p,n) runs over {(j1, j2) : p1 − nL1 < j1 ≤ nU1 + 1, p2 − nL2 < j2 ≤ nU2 + 1}. We denote
p. A feasible half-plane

ˇxp,s =

(cid:88)

ˇdp(k)ˇxs−k.

k∈S[−pL,pU ]

The autoregressive nonparametric spectrum estimate is deﬁned as

ˇf (λ) =

ˇσ2
p

(2π)2

(cid:12)
(cid:12)1 − (cid:80)
(cid:12)

k∈S[−pL,pU ]

ˇdp(k)eik(cid:48)λ

2 .

(cid:12)
(cid:12)
(cid:12)

(4.6)

(4.7)

A predictor of xs based on (3.14) using ˆf (λ) (respectively ˇf (λ)) is denoted ˆxs (respectively ˜xs),
while a predictor based on (4.6) is denoted ˇxs as mentioned above.

Let (cid:126)xr,s be a generic predictor of xs in replication r, r = 1, . . . , 1000. We report a statistic

called the root mean squared error (RMSE) of prediction, deﬁned as

RMSE ((cid:126)xs) =

(cid:40)

1
1000

1000
(cid:88)

r=1

(cid:41) 1

2

((cid:126)xr,s − xs)2

.

(4.8)

The results are reported in Tables 1-4. We observe an improvement in prediction performance as
n∗ increases, and also as the bandwidths ((m[1], m[2]) and p∗) increase as function of n∗. This is as
expected in the theory. Nevertheless, even for rather small sample sizes the RMSE is acceptable.
For example, for (cid:15)t ∼ U (−5, 5) and (cid:15)t ∼ N (0, 1) with n∗ = 5, we can obtain predictions with
RMSE that are not radically diﬀerent from the n∗ = 10 case, even though this change in n∗ entails
a sample that is nearly four times larger (231 against 66).
In comparison the RMSE with the
smaller sample size can be quite close to those obtained with more data in some cases, cf. ˇx20,20
for any error distribution.

For the smallest sample size ˇx20,20 can outperform ˆx20,20 and ˜x20,20, but with increasing n∗ the
latter two clearly begin to dominate. An inspection of Tables 1-4 reveals that the use of the ﬂexible

exponential algorithm proposed in this paper together with either the tapered periodogram or the

AR spectral estimator of Gupta (2018) outperforms autoregressive prediction in moderate to large

sample sizes. There is little to choose from between the two best performing algorithms, and a

practitioner might choose to use either one. However the AR prediction is clearly dominated by

our algorithm.

17

(cid:15)t ∼ U (−5, 5)
τ

(m[1], m[2])

(1,1)
(1,1)

(cid:15)t ∼ N (0, 1)
τ

(m[1], m[2])

(1,1)
(1,1)

(cid:15)t ∼ χ2
τ

9 − 9

(m[1], m[2])

(1,1)
(1,1)

p∗

1
2

p∗

1
2

p∗

1
2

0.05

0.075

ˆx20,20

0.10

0.05

0.075

˜x20,20

0.10

0.05

0.10

0.075

ˇx20,20

0.5273

0.5252

0.5269

0.5143
0.5141

0.5123
0.5123

0.5144
0.5144

0.4279
0.4806

0.4575
0.5051

0.4813
0.5261

0.05

0.075

ˆx20,20

0.10

0.05

0.075

˜x20,20

0.10

0.05

0.10

0.075

ˇx20,20

1.2916

1.1360

1.1072

1.2713
1.2712

1.1120
1.1120

1.0810
1.0811

1.0487
1.0829

1.0226
1.0589

0.9874
1.0313

0.05

0.075

ˆx20,20

0.10

0.05

0.075

˜x20,20

0.10

0.05

0.10

0.075

ˇx20,20

2.0651

1.9869

1.8656

2.0199
2.0197

1.9435
1.9435

1.8238
1.8239

2.2475
2.5720

2.1666
2.5355

2.0835
2.4353

Table 1: Monte Carlo RMSE of prediction with n∗ = 5, model (4.1)

(cid:15)t ∼ U (−5, 5)
τ

(m[1], m[2])

p∗

(1,1)
(2,2)
(1,2)

(cid:15)t ∼ N (0, 1)
τ

(m[1], m[2])

(1,1)
(2,2)
(1,2)

(cid:15)t ∼ χ2
τ

9 − 9

(m[1], m[2])

(1,1)
(2,2)
(1,2)

1
2
3

p∗

1
2
3

p∗

1
2
3

0.05

0.4212
0.5288
0.3859

0.05

1.2711
1.1526
1.2689

0.05

0.9720
2.0694
1.3581

0.075

ˆx20,20

0.4200
0.5308
0.3806

0.075

ˆx20,20

1.3066
1.1248
1.2123

0.075

ˆx20,20

0.8856
1.9571
1.2594

0.10

0.05

0.4158
0.5336
0.3776

0.3999
0.5161
0.3849

0.10

0.05

1.2479
1.0953
1.1755

1.2487
1.1318
1.2433

0.10

0.05

0.8149
1.8487
1.1650

0.9805
2.0256
1.5024

0.075

˜x20,20

0.3989
0.5184
0.3792

0.075

˜x20,20

1.2862
1.1016
1.1868

0.075

˜x20,20

0.8932
1.9150
1.4011

0.10

0.05

0.3946
0.5217
0.3757

0.4250
0.4325
0.4390

0.10

0.05

1.2283
1.0700
1.1501

1.0517
1.0575
1.0700

0.10

0.05

0.8217
1.8080
1.3045

2.0183
2.0018
2.1316

0.075

ˇx20,20

0.4554
0.4626
0.4630

0.075

ˇx20,20

1.0152
1.0252
1.0376

0.075

ˇx20,20

1.9234
1.8313
1.9801

0.10

0.4808
0.4896
0.4894

0.10

0.9788
0.9912
1.0006

0.10

1.8353
1.6798
1.8485

Table 2: Monte Carlo RMSE of prediction with n∗ = 10, model (4.1)

18

(cid:15)t ∼ U (−1, 1)
τ

(m[1], m[2])

p∗

(1,1)
(1,2)
(1,3)
(2,3)
(2,4)
(3,4)
(3,5)

1
2
2
2
2
4
3

(cid:15)t ∼ N (0, 1)
τ

(m[1], m[2])

p∗

(1,1)
(1,2)
(1,3)
(2,3)
(2,4)
(3,4)
(3,5)

1
2
2
2
2
4
3

(cid:15)t ∼ χ2
τ

9 − 9

(m[1], m[2])

p∗

(1,1)
(1,2)
(1,3)
(2,3)
(2,4)
(3,4)
(3,5)

1
2
2
2
2
4
3

0.05

0.2946
0.3917
0.4526
0.4273
0.3792
0.4274
0.4270

0.05

1.1985
1.2035
1.0145
0.9794
1.2033
1.1636
1.1634

0.05

1.0012
0.6229
0.9546
0.9984
1.3377
1.7939
1.7913

0.075

ˆx20,20

0.2865
0.3861
0.4427
0.4170
0.3763
0.4238
0.4238

0.075

ˆx20,20

1.2252
0.9851
0.8611
0.9397
1.1699
1.1374
1.1371

0.075

ˆx20,20

0.5964
0.5203
0.8202
0.8924
1.2419
1.6942
1.6904

0.10

0.05

0.2806
0.3817
0.4335
0.4084
0.3732
0.4216
0.4216

0.2989
0.3942
0.4448
0.4136
0.3783
0.4246
0.4243

0.10

0.05

1.1586
0.9384
0.8164
0.8998
1.1390
1.1112
1.1110

1.2006
1.1964
1.0234
0.9850
1.1771
1.1456
1.1454

0.10

0.05

0.5140
0.4430
0.6878
0.7884
1.1488
1.5952
1.5911

1.0421
0.7138
1.0768
1.1088
1.4835
1.7852
1.7825

0.075

˜x20,20

0.2905
0.3881
0.4350
0.4036
0.3749
0.4203
0.4203

0.075

˜x20,20

1.2274
0.9775
0.8709
0.9457
1.1439
1.1187
1.1185

0.075

˜x20,20

0.6479
0.6053
0.9393
0.9986
1.3859
1.6818
1.6780

0.10

0.05

0.10

0.075

ˇx20,20

0.2843
0.3832
0.4258
0.3953
0.3713
0.4174
0.4173

0.4245
0.4263

0.4549
0.4587

0.4806
0.4867

0.4326
0.4271

0.4617
0.4593

0.4889
0.4871

0.10

0.05

0.10

0.075

ˇx20,20

1.1612
0.9311
0.8262
0.9061
1.1132
1.0919
1.0917

1.0515
1.0532

1.0146
1.0145

0.9783
0.9800

1.0572
1.0594

1.0161
1.0170

0.9786
0.9795

0.10

0.05

0.10

0.075

ˇx20,20

0.5613
0.5215
0.8037
0.8906
1.2906
1.5790
1.5749

1.9713
1.9134

1.8688
1.7618

1.7806
1.5989

1.9971
1.9484

1.8303
1.7935

1.6848
1.6429

Table 3: Monte Carlo RMSE of prediction with n∗ = 20, model (4.1)

19

(cid:15)t ∼ U (−5, 5)
τ

(m[1], m[2])

p∗

(1,1)
(2,2)
(3,3)
(4,4)
(5,5)

1
2
3
4
5

(cid:15)t ∼ N (0, 1)
τ

(m[1], m[2])

p∗

(1,1)
(2,2)
(3,3)
(4,4)
(5,5)

1
2
3
4
5

(cid:15)t ∼ χ2
τ

9 − 9

(m[1], m[2])

p∗

(1,1)
(2,2)
(3,3)
(4,4)
(5,5)

1
2
3
4
5

0.05

0.2034
0.2880
0.3229
0.3987
0.3766

0.05

1.0378
1.0132
1.0971
1.1024
1.0987

0.05

1.0228
0.5742
0.7560
0.8226
0.8275

0.075

ˆx20,20

0.2049
0.2821
0.3133
0.3946
0.3712

0.075

ˆx20,20

1.0174
0.9739
1.0582
1.0610
1.0490

0.075

ˆx20,20

0.9107
0.4864
0.6947
0.7490
0.7496

0.10

0.05

0.2066
0.2763
0.3039
0.3907
0.3656

0.2138
0.2924
0.3100
0.3768
0.3734

0.10

0.05

0.9847
0.9349
1.0201
1.0191
0.9989

1.0430
1.0156
1.0716
1.0782
1.0538

0.10

0.05

0.7998
0.4025
0.6335
0.6752
0.6720

1.0813
0.6323
0.8305
0.8315
0.9196

0.075

˜x20,20

0.2151
0.2862
0.3008
0.3728
0.3678

0.075

˜x20,20

1.0226
0.9765
1.0331
1.0377
1.0054

0.075

˜x20,20

0.9668
0.5412
0.7655
0.7571
0.8378

0.10

0.05

0.2165
0.2801
0.2917
0.3691
0.3619

0.4228
0.4234
0.4234
0.4243
0.4253

0.10

0.05

0.9897
0.9379
0.9954
0.9967
0.9566

1.0516
1.0512
1.0508
1.0511
1.0517

0.10

0.05

0.8535
0.4533
0.7007
0.6825
0.7563

1.9533
1.8933
1.9050
1.9156
1.9327

0.075

ˇx20,20

0.4541
0.4561
0.4560
0.4567
0.4575

0.075

ˇx20,20

1.0145
1.0155
1.0134
1.0136
1.0141

0.075

ˇx20,20

1.8618
1.7385
1.7582
1.7701
1.7883

0.10

0.4800
0.4844
0.4843
0.4848
0.4852

0.10

0.9782
0.9809
0.9755
0.9758
0.9760

0.10

1.7730
1.5718
1.6037
1.6184
1.6380

Table 4: Monte Carlo RMSE of prediction with n∗ = 40, model (4.1)

5 An application to house price prediction in Los Angeles

In this section we show how the techniques established in the paper can be used to predict house

prices. This can be of interest in real estate and urban economics, as well as for property developers.

Indeed, spatial methods are frequently used in these ﬁelds, as studied for instance by Iversen Jr

(2001), Banerjee et al. (2004) and Majumdar et al. (2006). We use median house price data

for census blocks in California from the 1990 census from Pace and Barry (1997), available at

www.spatial-statistics.com. We conﬁne our analysis to the city of Los Angeles. The data is gridded
as follows: a 14 × 23 grid of square cells is superimposed on Los Angeles, from 33.75◦N to 34.17◦N
and 117.75◦W to 118.44◦W. The grid covers a total of 5259 observations. The average of the
median house values for each cell is calculated and the 322 such observations form our sample. The

gridding is shown in Figure 2, in which the 8 empty cells are ﬁlled and marked with a cross. We

wish to predict the house price for these cells. House price data is not a zero mean process, so we

subtract the sample mean using the whole sample from each cell.

We proceed in the following way: to obtain the coeﬃcients ˆa(cid:96), and ˇd((cid:96)) in (3.14) and (4.6) we

20

use the 14×19 sublattice formed of the ﬁrst 19 columns of cells. This sublattice contains no missing

observations. Once the coeﬃcients are obtained we construct predictions using the remaining 4×19

sublattice, in a step-by-step manner. The shaded-and-crossed cell (8,20) is predicted ﬁrst, followed

by (8,21) and (8,22). We then predict (4,21), followed by (7,22), (9,23), (6,23) and (1,23).

The predicted values are tabulated for various values of (m[1], m[2]) and (p1, p2) in Tables 5
and 6. The predicted values are quite stable across the choices (m[1], m[2]) = (2, 2), (2, 3) using

either the periodogram or AR spectral estimate. They most closely match those obtained when
(p1, p2) = (2, 2), (2, 3) in (4.6). In the latter case we compare in Table 7 the order selection criteria
proposed by Gupta (2018), which include the usual FPE and BIC (denoted with a (cid:98) ) as well as
corrected version that account for the spatial case (denoted with(cid:101)and¯). The FPE tends to favour
longer lag lengths no matter which version is used, as do (cid:100)BIC and BIC. However the latter as well
as (cid:93)FPE are not monotonically decreasing in lag length, unlike (cid:91)BIC, (cid:92)F P E and F P E. Thus the
latter three are likely to overﬁt and seem undesirable. If we impose a selection rule that picks the

desirable lag order as the ﬁrst instance when the selection criteria shows an increase with lag length,
then we get (p1, p2) = (2, 2) using (cid:94)F P E and BIC. (cid:93)BIC indicates a choice of (p1, p2) = (1, 2), on
the other hand. All considered, it seems that (p1, p2) = (2, 2) is a reasonable choice.

6 Conclusion

In this paper we have dealt with the problem of prediction when the data {xt}t∈Z2 is collected
on a lattice. To do so, we considered unilateral representations of {xt}t∈Z2 and in particular
the canonical factorization of the spectral density function, the latter being possible as observed

by Whittle (1954). Our approach does not need any parameterization of the model (i.e.

the

covariogram structure of the data), so we avoid the consequences that a wrong parameterization

can have in the predictor. We have also compared our methodology to one based on the space

domain by using a ﬁnite approximation of the unilateral autoregressive model in (3.3).

However, it might be interesting to examine how our proposed methodology compares with one
based on the conditional autoregressive (CAR) representation of Besag (1974). That is, let xt be

Figure 2: Gridded Los Angeles median house price data

21

given by

xt = µ + E [xt | xr; r (cid:54)= t] + ut

= µ +

(cid:88)

r(cid:54)=t

ζ |r−t|xr + ut.

(6.1)

Note that our deﬁnition in (6.1) implies that xt is, among other characteristics, homogeneous. The
representation of xt given in (6.1) suggests to predict a value xt at a location s = (s [1] , s [2]),
1 ≤ s [1] ≤ n [1] and 1 ≤ s [2] ≤ n [2], by

(cid:98)xs = (cid:98)µ +

(cid:88)

(cid:98)ζ |r−s|xr,

r(cid:54)=s;|r−s|<M

(6.2)

where (cid:98)µ and (cid:98)ζ |r−s| are respectively the least squares estimator of µ and ζ |r−t|, and with the con-
vention that xr = 0 if it were not observed. This is in the same spirit as we did with our pre-
dictor in (3.14). On the other hand, if we were interesting to predict a value xt at a location
s = (n [1] + 1, s [2]), we might then use

(cid:98)xn[1]+1,s[2] = (cid:98)µ +

(cid:88)

(cid:98)ζ |r−s|xr.

r(cid:54)=s;|r−s|<M

(6.3)

However to compute the prediction we would also need to replace the unobserved xr by its prediction
as in (3.17). The latter might be done in an iterative fashion similar to what we did in (3.18).

.

22

(m[1], m[2])

(8,20)

(8,21)

(8,22)

(4,21)

(7,22)

(9,23)

(6,23)

(1,23)

(1,1)

(1,2)

(2,1)

(2,2)

(2,3)

(1,3)

(cid:98) 0.5645
(cid:101) 0.5244
(cid:98) 0.8054
(cid:101) 0.7043
(cid:98) 1.0691
(cid:101) 0.9828
(cid:98) 2.0717
(cid:101) 1.8934
(cid:98) 1.9970
(cid:101) 1.7926
(cid:98) 0.8150
(cid:101) 0.7218

0.5540
0.5529
0.5312
0.4290
0.8761
0.8133
1.7439
1.4566
1.6205
1.3057
0.5414
0.4460

0.5384
0.5523
0.8791
0.7684
0.3926
0.4722
1.4680
1.1207
1.3149
0.9511
0.8899
0.7874

0.6976
0.6643
1.2740
1.1141
1.4462
1.3612
3.0421
2.7803
2.9325
2.6323
1.2892
1.1419

0.6671
0.6256
0.8797
0.7693
1.9503
1.7429
2.4033
2.1965
2.3167
2.0796
0.8902
0.7885

0.1962
0.1969
1.0049
0.8788
0.7717
0.6726
2.3360
2.1349
2.2518
2.0213
1.0169
0.9007

0.9337
0.8770
1.2029
1.0520
0.8519
0.8448
2.3242
2.1242
2.2404
2.0111
1.2173
1.0782

0.4518
0.4348
0.6338
0.5543
1.0789
1.0044
2.0935
1.9134
2.0181
1.8115
0.6414
0.5681

Table 5: Los Angeles house price predictions, in ‘00,000 US Dollars

(p1, p2)

(8,20)

(8,21)

(8,22)

(4,21)

(7,22)

(9,23)

(6,23)

(1,23)

(1,1)
(1,2)
(2,1)
(2,2)
(3,2)
(4,3)

1.6525
1.5373
1.8138
1.8703
1.7925
2.4465

1.2876
1.2516
1.6433
1.9858
1.9298
2.2325

1.0325
0.8473
1.1237
1.4089
1.4400
1.9075

2.4728
2.3741
2.4881
2.6079
2.4489
2.0319

1.6663
1.7944
1.8545
2.4100
2.6472
3.1207

2.1204
1.8452
2.3404
2.4992
2.5962
3.5439

1.6825
1.7626
1.7547
2.0862
2.4021
2.8234

1.7266
1.5659
2.3233
2.2252
1.9518
2.0841

Table 6: Los Angeles house price predictions using (4.6), in ‘00,000 US Dollars

(p1, p2) (cid:100)BIC (cid:103)BIC

BIC (cid:91)FPE (cid:93)FPE

(1,1)
(1,2)
(2,1)
(2,2)
(3,2)
(4,3)

0.5979
0.5973
0.5953
0.5946
0.5943
0.5824

0.6001
0.5979
0.6042
0.6008
0.6105
0.6081

0.5990
0.5976
0.5997
0.5977
0.6024
0.5953

0.5472
0.5144
0.4766
0.4351
0.4022
0.2129

0.5639
0.5183
0.5377
0.4728
0.5018
0.3058

FPE

0.5555
0.5164
0.5062
0.4535
0.4489
0.2543

Table 7: Los Angeles house price predictions using (4.6), BIC and FPE

23

Mathematical Appendix

A Proofs of Theorems

For the sake of notational simplicity, we shall assume that M [1] = M [2] and also that n [1] = n [2],
so that n = n2 [1] and m = m2 [1] say. Also to simplify the notation we shall write (cid:80)
j(cid:22)J instead
of (cid:80)+

j(cid:22)J given in (3.2). That is,

(cid:88)

k(cid:22)M

dk =

M [2]
(cid:88)

d0,k[2] +

M [1]
(cid:88)

M [2]
(cid:88)

dk[1],k[2].

k[2]=1

k[1]=1

k[2]=1−M [2]

(A.1)

A.1 Proof of Theorem 1

We shall examine part (a), since part (b) follows by Lemma 2 and standard arguments. By the

Cram´er-Wold device, it suﬃces to show that for a ﬁnite set of constants ϕj, j = 1, ..., J,

n1/2

J
(cid:88)

j=1



ϕj ((cid:98)αj − (cid:101)αj,n) d→ N

0,

J
(cid:88)

j=1

ϕ2

j (1 + (1 + κ4,ϑ) δj)



 .

First, by deﬁnition of (cid:98)αj and (cid:101)αj,n, we have that

(cid:98)αj − (cid:101)αj,n =

1
2M

(cid:88)

log

k(cid:22)M

(cid:33)

(cid:32)

(cid:98)fk
(cid:101)fk

(cid:16)

cos

(cid:17)

.

j · (cid:101)λk

Because standard inequalities and then Lemma 3 yield that

(A.2)

(A.3)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:98)fk − (cid:101)fk
(cid:101)fk

2

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:88)

≤

k(cid:22)M

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:98)fk − (cid:101)fk
(cid:101)fk

2

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

sup
k(cid:22)M

= Op

(cid:19)

(cid:18) M
m

= op (1) ,

(A.4)

the left side of (A.3) is, by Lemma 3,

1
2M

1
2M

+op

=

(cid:88)

k(cid:22)M

(cid:98)fk − (cid:101)fk
(cid:101)fk

(cid:88)

(cid:98)fk − (cid:101)fk
fk
n−1/2(cid:17)

k(cid:22)M
(cid:16)

,

(cid:16)

(cid:16)

cos

cos

(cid:17)

j · (cid:101)λk

+ Op

(cid:0)m−1(cid:1)

(cid:17)

+

j · (cid:101)λk

1
2M

(cid:88)

k(cid:22)M

(cid:33) (cid:32)

(cid:32)

(cid:98)fk − (cid:101)fk
fk

fk − (cid:101)fk
(cid:101)fk

(cid:33)

(cid:16)

cos

j · (cid:101)λk

(cid:17)

(A.5)

after using Taylor series expansion of log (z) around z = 1 and Condition C4. Now, the absolute

24

value of the second term on the right of the last displayed expression is bounded by

1
2M

(cid:88)

k(cid:22)M

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:98)fk − (cid:101)fk
fk

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

fk − (cid:101)fk
(cid:101)fk

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

= O

(cid:19)

(cid:18)

1
Mm1/2

(cid:16)

n−1/2(cid:17)

= o

(A.6)

by Lemmas 2 and 3. So, we conclude that

n1/2 ((cid:98)αj − (cid:101)αj,n) =

n1/2
2M

(cid:88)

k(cid:22)M

(cid:98)fk − (cid:101)fk
fk

(cid:16)

cos

j · (cid:101)λk

(cid:17)

+ op (1)

=

1
2n1/2

(cid:88)

k(cid:22)n

I T
x (λk) − f (λk)
f (λk)

hk,n (j) + op (1) ,

where hk,n (j) is a step function deﬁned as

hk,n (j) = f −1

p f (λk) cos

(cid:16)

j · (cid:101)λk

(cid:17)

when 2p [(cid:96)] − 1 < k[(cid:96)]
Lemma 1, we have that for all j,

m[(cid:96)] < 2p [(cid:96)] + 1 and 1 ≤ p [1] < M [1], 1 − M [2] < p [2] ≤ M [2]. Now, using

(cid:32)

(cid:88)

k(cid:22)n

I T
x (λk)
f (λk)

−

ϑ (λk)

(2π)2 I T
σ2
ϑ

(cid:33)

hk,n (j) = op

(cid:16)

n1/2(cid:17)

.

So, we conclude that the left side of (A.2) is

n1/2

J
(cid:88)

j=1

ϕj ((cid:98)αj − (cid:101)αj,n) =

=

J
(cid:88)

j=1

J
(cid:88)

j=1

ϕj

1
n1/2

ϕj

1
n1/2

(cid:88)

k(cid:22)n

(cid:88)

k(cid:22)n

(cid:32)

(cid:32)

ϑ (λk)

(2π)2 I T
σ2
ϑ

ϑ (λk)

(2π)2 I T
σ2
ϑ

(cid:33)

− 1

hk,n (j) + op (1)

(cid:33)

− 1

cos

(cid:16)

j · (cid:101)λk

(cid:17)

(1 + op (1))

after we observe that Condition C1 implies that

hk,n (j) = cos

(cid:16)

j · (cid:101)λk

(cid:17) (cid:18)

1 +

1
M1/2

(cid:18) ∂f (λk)
∂λ [1]

+

∂f (λk)
∂λ [2]

(cid:19)

+ O

(cid:19)(cid:19)

.

(cid:18) 1
M

Recall that M [1] = M [2]. From here the conclusion is standard proceeding as in the proof of

Theorems 1 and 2 of Hidalgo (2009), see also Robinson and Vidal-Sanz (2006), and so it is omitted.
(cid:4)

25

A.2 Proof of Theorem 2

Deﬁne (cid:98)dj = log (cid:98)Aj, (cid:101)dj,n = log (cid:101)Aj,n and dj,n = log Aj,n. We begin with part (b). First by deﬁnition,

(cid:101)dj,n − dj,n =:

(cid:88)

k(cid:22)M

((cid:101)αk,n − αk,n) e−ik·(cid:101)λj ,

(A.7)

which by Taylor expansion of log

(cid:16)

(cid:101)fr/fr

(cid:17)

, (B.1) in Lemma 2 and Condition C4, it is

1
2M

1
2M

=

(cid:88)

(cid:88)

k(cid:22)M

(cid:88)

r(cid:22)M



(cid:88)

k(cid:22)M



r(cid:22)M




(cid:32)



(cid:101)fr − fr
fr

(cid:32)

(cid:33)

+

1
2

(cid:101)fr − fr
fr

(cid:33)2




(cid:16)

cos

k · (cid:101)λr

(cid:17)

e−ik·(cid:101)λj + o

(cid:18) 1

(cid:19)

m1/2

(cid:40)(cid:32)

(cid:33)

(cid:101)fr − fr
fr

+

1
72M2 g2

r

(cid:41)

(cid:16)

cos

k · (cid:101)λr

(cid:17)

e−ik·(cid:101)λj






+ o

(cid:18) 1

m1/2

(cid:19)

.

Now using the inequality

(cid:88)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
k(cid:22)M

e−ik·(cid:101)λp

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤ K(cid:101)λ

−1
p[1](cid:101)λ

−1
p[2],

(A.8)

we have that

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
M3




(cid:88)

(cid:88)

k(cid:22)M



r(cid:22)M

g2
r cos

(cid:16)

k · (cid:101)λr

(cid:17)

e−ik·(cid:101)λj


(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:12)

≤

K
M3

(cid:88)

g2
r

r(cid:22)M
(cid:18) log2 M
M2

= O

(cid:16)

cos

k · (cid:101)λr

(cid:17)

e−ik·(cid:101)λj

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:88)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
k(cid:22)M
(cid:19)

.

Thus using that 2 cos x = eix + e−ix, the right side of (A.7) is




(cid:88)

(cid:32)

(cid:33)

(cid:101)fr − fr
fr

(cid:16)

cos

k · (cid:101)λr

(cid:17)

e−ik·(cid:101)λj






+ O

(cid:19)

(cid:18) log2 M
M2

(cid:101)fr − fr
fr

(cid:32)

(cid:33)

(cid:88)

k(cid:22)M

eik·(cid:101)λr−j + eik·(cid:101)λ−r−j
2

gr cos

(cid:16)

k · (cid:101)λr




(cid:17)



(cid:88)

r(cid:22)M
(cid:19)

e−ik·(cid:101)λj + O

(cid:33)




+ O

(cid:19)

(cid:18) log2 M
M2





1
M2

(cid:88)

r(cid:22)M

1
r ± j





1
2M

(cid:88)

k(cid:22)M

1
2M

(cid:88)

r(cid:22)M



r(cid:22)M
(cid:32)







1
12M

(cid:88)



k(cid:22)M

1
M

1
6M

gj + o

(cid:18) 1

m1/2

,

=

=

=

where in the second equality we have used (B.1) and (A.8) and then Condition C4 and for the

26

third equality that

(cid:16)

k · (cid:101)λr

(cid:90)

(cid:17)

=

gr cos

1
M

(cid:88)

r(cid:22)M

g (λ) cos (k · λ) dλ + O (cid:0)M−1(cid:1) =: ξk + O (cid:0)M−1(cid:1)

and then that (cid:80)
diﬀerentiable so that |ξk| = O
algebra.

k(cid:22)M ξke−ik·λ = g (λ) + O (cid:0)M−1(cid:1) since g (λ), given in (3.20), is twice continuously
. From here we conclude the proof of part (b) by standard

|k|−3(cid:17)

(cid:16)

Next, we show part (a). By Cram´er-Wold device, it suﬃces to examine that for any set of ﬁnite

constants ϕq1, ..., ϕq2, the behaviour of

m1/2

q2
(cid:88)

j=q1

(cid:16)

ϕj

(cid:98)Aj − Aj,n

(cid:17)

.

First, by deﬁnitions of (cid:98)Aj,n and (cid:101)Aj,n, we have that

m1/2 (cid:16)

(cid:98)dj − (cid:101)dj,n

(cid:17)

= −m1/2 (cid:88)

((cid:98)αk − (cid:101)αk,n) e−ik·(cid:101)λj

(A.9)

k(cid:22)M

= −m1/2 (cid:88)

k(cid:22)M

1
2M

(cid:88)

s(cid:22)M

(cid:33)

(cid:32)

(cid:98)fs − (cid:101)fs
(cid:101)fs

(cid:16)

cos

k · (cid:101)λs

(cid:17)

e−ik·(cid:101)λj + op (1)

=

(cid:88)

k(cid:22)M

m1/2
2n

(cid:88)

s(cid:22)n

ρshs,n (k) e−ik·(cid:101)λj + op (1)

proceeding as in the proof of Theorem 1, where ρs = (2π)2 σ−2
there. So, because n [(cid:96)] = 2M [(cid:96)] m [(cid:96)] for (cid:96) = 1, 2, denoting ψs,n (j) = (2M)−1/2 (cid:80)
we conclude that

ϑ I T

ϑ (λs) and hs,n (k) were deﬁned

k(cid:22)M hs,n (k) e−ik·(cid:101)λj ,

m1/2

J
(cid:88)

j=1

(cid:16)

ϕj

(cid:98)dj − (cid:101)dj,n

(cid:17)

=

1
2n1/2

J
(cid:88)

(cid:88)

ρs

ϕjψs,n (j) + op (1)

(A.10)

j=1
s(cid:22)n
d→ N (cid:0)0, ϕ(cid:48)V ϕ(cid:1)

proceeding as in the proof of Theorem 1, where ϕ(cid:48)= (ϕ1, ..., ϕJ ) and

V = lim
n→∞

= lim
n→∞

J
(cid:88)

j1,j2=1

J
(cid:88)

j1,j2=1

ϕj1ϕj2

1
n

(cid:88)

(cid:96)(cid:22)n

ψ(cid:96),n (j1) ψ(cid:96),n (j2)

ϕj1ϕj2

1
2M2

(cid:88)

(cid:88)

e−ik1(cid:101)λj1 +ik2(cid:101)λj2 cos

(cid:16)

k1 · (cid:101)λ(cid:96)

(cid:17)

cos

(cid:16)

k2 · (cid:101)λ(cid:96)

(cid:17)

(cid:96)(cid:22)M

k1,k2(cid:22)M

= 2−1

J
(cid:88)

(cid:16)

ϕj1ϕj2

j1,j2=1

δj1[1]−j2[2] + 2−1φj1[1]φj2[1] − iφj1[1]−j2[1]

(cid:17)

27

by Lemma 4. From here the conclusion of the theorem follows by standard delta arguments.

(cid:4)

A.3 Proof of Theorem 3

We begin with part (a). To that end, it suﬃces to show that

n1/2

q
(cid:88)

υ=p

ϕυ ((cid:98)aυ − (cid:101)aυ,n) d→ N

(cid:32)

0,

q
(cid:88)

υ1,υ2=p

(cid:33)

ϕυ1ϕυ2Ωa,υ1,υ2

.

(A.11)

By deﬁnition of (cid:98)aυ − (cid:101)aυ,n and Taylor expansion of (cid:98)Aj − (cid:101)Aj,n, a typical component on the left of
(A.11) is

n1/2
4M

(cid:88)

(cid:16)

(cid:17)

(cid:98)dj − (cid:101)dj,n

(cid:101)Aj,neiυ·(cid:101)λj +

−M <j≤M

n1/2
4M

(cid:88)

−M <j≤M

(cid:12)
(cid:12)
(cid:12) (cid:98)dj − (cid:101)dj,n

2 (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12) (cid:101)Aj,n
(cid:12)

(cid:12)
(cid:12)
(cid:12) (1 + op (1)) .

(A.12)

Now (A.10) implies that

m

(cid:12)
(cid:12)
(cid:12) (cid:98)dj − (cid:101)dj,n

(cid:12)
(cid:12)
(cid:12)

2

= C

1
n

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:88)

k(cid:22)n

ρkψk,n (j)

(cid:12)
2
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

+ op (1) = Op (1) ,

by Theorem A of Serﬂing (1980), p. 14 because Condition C1 implies that x4 (t) is uniformly inte-

grable and Theorem 2 and the continuous mapping theorem implies that
χ2. So, using (A.7) we conclude that (A.12) is

(cid:12)
(cid:12)n−1/2 (cid:80)
(cid:12)

(cid:12)
2
(cid:12)
k(cid:22)n ρkψk,n (j)
(cid:12)

→d

−

n1/2
4M

(cid:88)





(cid:88)

−M <j≤M

k(cid:22)M

((cid:98)αk − (cid:101)αk,n) e−ik·(cid:101)λj


 (cid:101)Aj,neiυ·(cid:101)λj + Op

(cid:32)

(cid:33)

n1/2
m

(cid:88)





(cid:32)

(cid:88)

(cid:33)

(cid:98)fr − (cid:101)fr
(cid:101)fr

(cid:16)

cos

k · (cid:101)λr

(cid:17)

e−ik·(cid:101)λj


 (cid:101)Aj,neiυ·(cid:101)λj + Op

(cid:32)

(cid:33)

.

n1/2
m

(cid:88)

(cid:16)

cos

k · (cid:101)λr

(cid:17)



aυ−k

 + Op

(cid:16)

M−1/2(cid:17)

(A.13)

= −

= −

n1/2
4M2

n1/2
4M

k,r(cid:22)M
(cid:33) 


−M <j≤M
(cid:32)

(cid:88)

r(cid:22)M

(cid:98)fr − (cid:101)fr
(cid:101)fr
(cid:16)

k(cid:22)M

M−1/2(cid:17)

= : n1/2(cid:37)n,υ−k + Op

,

28

where in the ﬁrst equality we use (A.9) and in the second equality that Lemma 6 implies that

n1/2
M

n1/2
M

(cid:88)

r(cid:22)M

(cid:88)

r(cid:22)M

(cid:33)

(cid:33)

(cid:32)

(cid:32)

(cid:98)fr − (cid:101)fr
(cid:101)fr

(cid:98)fr − (cid:101)fr
(cid:101)fr

(cid:16)

(cid:16)

cos

cos

(cid:88)

k(cid:22)M

(cid:88)

k(cid:22)M

k · (cid:101)λr

k · (cid:101)λr

=

= Op

(cid:16)

M−1/2(cid:17)


(cid:17) (cid:26) 1
M




(cid:17)

1
4M

(cid:88)

−M <j≤M

(cid:101)Aj,ne−i(k−υ)·(cid:101)λj − ak−υ






hk−υ + O

(cid:19)(cid:27)

(cid:18) 1
M2

because

(cid:16)

(cid:98)fr − (cid:101)fr

(cid:17)

/ (cid:101)fr = Op

(cid:0)m−1/2(cid:1) and {hk}k is a summable sequence.

So, we conclude that the left side of (A.11) is

−

q
(cid:88)

υ=p

ϕυn1/2(cid:37)n,υ−k + op (1) d→ N (0, V ) ,

where V = limM →∞ M−1 (cid:80)

r(cid:22)M

(cid:16)(cid:80)q

υ=p ϕυ

(cid:80)

k(cid:22)M cos (k · λr) ak−υ

(cid:17)2

.

We now conclude because using that 2 cos (x) = eix + e−ix, 2 cos (x) cos (y) = cos (x + y) +

cos (x − y), a typical component of V is ϕυ1ϕυ2 times

lim
M →∞

= lim
M →∞

= lim
M →∞

(cid:88)

k1,k2(cid:22)M
(cid:88)

k1,k2(cid:22)M

(cid:88)

k1,k2(cid:22)M

ak1−υ1ak2−υ2

1
M

(cid:88)

(cid:16)

cos

k1 · (cid:101)λr

(cid:17)

(cid:16)

cos

k2 · (cid:101)λr

(cid:17)

ak1−υ1ak2−υ2

(cid:16)

(cid:16)

cos

(k1 + k2) · (cid:101)λr

(cid:17)

(cid:16)

+ cos

(k1 − k2) · (cid:101)λr

(cid:17)(cid:17)

r(cid:22)M
(cid:88)

r(cid:22)M

1
2M

ak1−υ1ak2−υ2

1
2M [1]

M [1]
(cid:88)

(cid:110)

cos

(cid:16)

r[1]=1

(k1 [1] + k2 [1]) (cid:101)λr[1]

(cid:17)

δk2[2]+k1[2]

(cid:16)

+ cos

(k1 [1] − k2 [1]) (cid:101)λr[1]

(cid:17)

(cid:111)

δk2[2]−k1[2]

=

(cid:88)

0(cid:22)k

akak+υ2−υ1 − lim
M →∞

1
M [1]

(cid:88)

ak1−υ1ak2−υ2,

k1(cid:54)=k2(cid:22)M ;k1±k2[1]=1,3,...,[M/2]

where we have taken υ1 (cid:22) υ2. From here the conclusion is standard since aυ is summable.

Part (b) follows by similar arguments to those in (A.13) and Lemma 2, so it is omitted.

(cid:4)

29

A.4 Proof of Theorem 4

We begin with part (a). For that purpose, denote

˙x∗
s =

¨x∗
s =

∞
(cid:88)

a0,k[2] x∗

s[1],s[2]−k[2] +





M [1]
(cid:88)

M [2]
(cid:88)



⊥



akx∗

s−k

k[2]=M [2]+1

k[1]=1

k[2]=1−M [2]

M [2]
(cid:88)

k[2]=1

a0,k[2] x∗

s[1],s[2]−k[2] +

M [1]
(cid:88)

M [2]
(cid:88)

k[1]=1

k[2]=1−M [2]

akx∗

s−k

x∗
n,s = −

M [2]
(cid:88)

k[2]=1

an,(0,k[2]) x∗

s[1],s[2]−k[2] −

M [1]
(cid:88)

M [2]
(cid:88)

k[1]=1

k[2]=1−M [2]

an,kx∗

s−k,

where

(cid:16)(cid:80)M [1]
k[1]=1

(cid:80)M [2]

k[2]=1−M [2]

(cid:17)⊥

= (cid:80)∞

k[1]=1

(cid:80)∞

k[2]=−∞ − (cid:80)M [1]

k[1]=1

(cid:80)M [2]

k[2]=1−M [2]. Then,

s − (cid:98)x∗
x∗

s = ϑs − ˙x∗

s + (cid:0)x∗

n,s − (cid:98)x∗

s

(cid:1) − (cid:0)x∗

n,s + ¨x∗
s

(cid:1) .

(A.14)

The second moment of

(cid:12)
(cid:12) < K for any k [1] and
(cid:12)
(cid:12) < K for any k [2] and that M → ∞. Next, the second moment of the last term

s is clearly o (1) since (cid:80)∞
˙x∗

(cid:12)
(cid:12)ak[1],k[2]

k[2]=−∞

(cid:80)∞

k[1]=1

(cid:12)
(cid:12)ak[1],k[2]

on the right of (A.14) is bounded by



2E



M [2]
(cid:88)

k[2]=1

(cid:0)an,(0,k[2]) − a0,k[2]

(cid:1) x∗

s[1],s[2]−k[2]



+ 2E



2





M [1]
(cid:88)

M [2]
(cid:88)

k[1]=1

k[2]=1−M [2]



2

(an,k − ak) x∗

s−k



= o (1) ,

by Lemma 7 and that the covariance of x∗
of x∗

s on the right of (A.14), which is

n,s − (cid:98)x∗

s is summable. Thus, it remains to examine the behaviour

M [2]
(cid:88)

(cid:0)
(cid:98)a0,k[2] − (cid:101)an,(0,k[2])

(cid:1) x∗

s[1],s[2]−k[2] +

M [1]
(cid:88)

M [2]
(cid:88)

k[1]=1

k[2]=1−M [2]

((cid:98)ak − (cid:101)an,k) x∗

s−k

(A.15)

k[2]=1

M [2]
(cid:88)

+

k[2]=1

(cid:0)
(cid:101)an,(0,k[2]) − an,(0,k[2])

(cid:1) x∗

s[1],s[2]−k[2] +

M [1]
(cid:88)

M [2]
(cid:88)

k[1]=1

k[2]=1−M [2]

((cid:101)an,k − an,k) x∗

s−k.

Now Theorem 3 part (b) and summability of the covariance of x∗
s yields that the second moment
of the second term of (A.15) is o (1). So, to complete the proof of part (a), we need to look at the

30

ﬁrst term, which is

M [2]
(cid:88)

(cid:16)
(cid:98)a0,k[2] − (cid:101)an,(0,k[2]) − (cid:37)n,(0,k[2])

(cid:17)

x∗
s[1],s[2]−k[2] +

k[2]=1

M [2]
(cid:88)

+

k[2]=1

(cid:37)n,(0,k[2]) x∗

s[1],s[2]−k[2] +

M [1]
(cid:88)

M [2]
(cid:88)

k[1]=1

k[2]=1−M [2]

(cid:37)n,kx∗

s−k

M [1]
(cid:88)

M [2]
(cid:88)

k[1]=1

k[2]=1−M [2]

(cid:0)
(cid:98)ak − (cid:101)an,k − (cid:37)n,k

(cid:1) x∗

s−k

=

M [2]
(cid:88)

k[2]=1

(cid:37)n,(0,k[2]) x∗

s[1],s[2]−k[2] +

M [1]
(cid:88)

M [2]
(cid:88)

k[1]=1

k[2]=1−M [2]

(cid:37)n,kx∗

s−k + Op

(cid:16)

M1/2n−1/2(cid:17)

because using expression (A.13), |(cid:98)ak − ak,n − (cid:37)k| = Op

(cid:0)n−1/2M−1/2(cid:1). Finally

E

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

M [1]
(cid:88)

M [2]
(cid:88)

k[1]=1

k[2]=1−M [2]

(cid:37)n,kx∗

s−k

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤

M [1]
(cid:88)

M [2]
(cid:88)

k[1]=1

k[2]=1−M [2]

(cid:0)E(cid:37)2

n,k

(cid:1)1/2 (cid:0)Ex∗2
s−k

(cid:1)1/2

(cid:16)

Mn−1/2(cid:17)

= O

= o (1)

by triangle and Cauchy-Schwarz inequalities and then Condition C4. Similarly we have that

E

(cid:12)
(cid:12)
(cid:12)

(cid:80)M [2]

k[2]=1 (cid:37)n,0,k[2] x∗

s[1],s[2]−k[2]

(cid:12)
(cid:12)
(cid:12) = o (1), which concludes the proof of part (a).

We now show part (b), that is when s =: (n [1] + 1, s [2]). For that purpose, it is convenient to
n[1]+1,s[2] − (cid:98)xn[1]+1,s[2]

recall our representation in (2.5). The reason is because the prediction error x∗
can be written as

n[1]+1,s[2] − (cid:98)xn[1]+1,s[2] = ϑ∗
x∗

s +

ζ kϑ∗

s−k −

(cid:88)

0≺k

M [2]
(cid:88)

k[2]=1

(cid:98)ζ 0,k[2] ϑ∗

n[1]+1,s[2]−k[2] −

M [1]
(cid:88)

M [2]
(cid:88)

k[1]=1

k[2]=1−M [2]

(cid:98)ζ k ϑ∗

s−k,

where (cid:98)ζ k[1],k[2] is similar to (cid:98)ak[1],k[2] but where

(cid:98)ζ k =

1
4M

(cid:88)

(cid:98)A(cid:96)eik·(cid:101)λ(cid:96),

k ∈ M;

−M <(cid:96)≤M



(cid:98)A(cid:96) = (cid:98)A−(cid:96) = exp

(cid:88)

+



j(cid:22)M

(cid:98)αje−ij·(cid:101)λ(cid:96)






,

(cid:96) ∈ M∪ {0}

and (cid:98)αj as deﬁned in (3.10). Now, it is obvious that we have the same type of (statistical) results
for (cid:98)ζ k as those obtained for (cid:98)ak, and hence proceeding as in part (a), we conclude that

x∗
n[1]+1,s[2] − (cid:98)xn[1]+1,s[2] = ϑ∗

n[1]+1,s[2] +

∞
(cid:88)

k[2]=1

ζ 0,k[2] ϑ∗

n[1]+1,s[2]−k[2] + op (1)

and that

(cid:16)

AE

x∗
n[1]+1,s[2] − (cid:98)xn[1]+1,s[2]

(cid:17)



=

1 +

∞
(cid:88)

k[2]=1


 σ2
ϑ.

ζ 2
0,k[2]

31

This concludes the proof of the theorem.

B Technical Lemmas

To simplify the notation, we abbreviate (cid:80)

−m<j≤m by (cid:80)

j in what follows.

Lemma 1. Under Conditions C1 − C4 we have that

(cid:98)fk =

1
4m

(cid:88)

(cid:16)

f

λj + (cid:101)λk

j

(cid:16)

(cid:17) I T

ϑ

(cid:17)

λj + (cid:101)λk
σ2
ϑ

+ (cid:15)n,k,

where {(cid:15)k,n}k is a triangular array sequence of r.v.’s such that E supk |(cid:15)k,n|2 = o (cid:0)m−1(cid:1).

Proof. The proof follows easily from Lemma 4 of Hidalgo (2009), and so it is omitted.

Lemma 2. Assuming C1−C4, (a) (cid:101)αk,n −αk,n = M−1ξk +O (cid:0)M−2(cid:1) and (b) αk,n −αk = O (cid:0)M−1(cid:1).

Proof. We begin with part (a). By deﬁnition of (cid:101)fj and then Taylor series expansion of log (·), we
have that

(cid:101)αk,n − αk,n =

1
2M




(cid:32)

(cid:88)

j(cid:22)M



(cid:101)fj − fj
fj

(cid:32)

(cid:33)

+

1
2

(cid:101)fj − fj
fj

(cid:33)2




(1 + o (1))



(cid:16)

cos

k · (cid:101)λj

(cid:17)

.

So, it suﬃces to examine the behaviour of f −1

j

(cid:16)

(cid:101)fj − fj

(cid:17)

. By deﬁnition and (3.20),

{f (λk+mj) − f (λmj)}

(cid:101)fj − fj
fj

=

=

=

f −1
j
4m

f −1
j
4m

1
6M

(cid:88)

k

(cid:88)

k

(cid:26) k2 [1]
n2 [1]
(cid:18) 1
M2

gj + O

f11 (λmj) +

k2 [2]
n2 [2]

(cid:27)

f22 (λmj)

+ O

(cid:19)

(cid:18) 1
M2

(cid:19)

,

(B.1)

because f (λ) is a four times diﬀerentiable function and (cid:80)
k kc1 [1] kc2 [2] = 0 if c1 + c2 is an odd
integer. From here the conclusion follows by standard arguments, because g (λ) is a continuous

diﬀerentiable function, so that the Riemman sums converge to their integral counterpart.

Part (b) follows using Lemma 6.

Lemma 3. Assuming, C1 − C4, for all k = 1, 2, ...

(cid:16)

E

(cid:101)f −1
k

(cid:16)

(cid:98)fk − (cid:101)fk

(cid:17)(cid:17)2

= O (cid:0)m−1(cid:1) .

32

Proof. Because (cid:101)fk = (4m)−1 (cid:80)
multiplicative constants, bounded by

j f (λj+mk) > 0, the left side of the last displayed equality is, up to



E



1
m

(cid:88)

j

f (λj+mk)

(cid:18) I T

x (λj+mk)
f (λj+mk)

−

I T
ϑ (λj+mk)
σ2
ϑ


2

(cid:19)



+E





1
m

(cid:88)

j

f (λj+mk)

(cid:18) I T

ϑ (λj+mk)
σ2
ϑ


2

(cid:19)

− 1



.

The ﬁrst term of the last displayed expression is o (cid:0)m−1(cid:1) by Lemma 1, whereas the second term
follows by standard arguments, as ϑt is an iid sequence of r.v.’s with ﬁnite fourth moments.

Lemma 4.

(cid:88)





(cid:88)

(cid:16)

cos

k1 · (cid:101)λp

(cid:17)

e−ik1·(cid:101)λj1









(cid:88)

(cid:16)

cos

−k2 · (cid:101)λp

(cid:17)

eik2·(cid:101)λj2





(B.2)

1
M2

k1(cid:22)M

p(cid:22)M
δj1[1]−j2[1] + 2−1φj1[1]φj2[1] − iφj1[1]−j2[1]

k2(cid:22)M
(cid:17)

δj1[2]±j2[2] + O (cid:0)M−1(cid:1) .

(cid:16)

= 2

Proof. First,

e−ik·(cid:101)λp = :

(cid:88)

k(cid:22)M

M
(cid:88)

k[2]=1

e−ik[2] πp[2]

M +

M
(cid:88)

e−ik[1] πp[1]

M

M
(cid:88)

e−ik[2] πp[2]

M

k[1]=1

k[2]=1−M

=

M
(cid:88)

k[2]=1

e−ik[2] πp[2]

M + 2M

M
(cid:88)

k[1]=1

e−ik[1] πp[1]

M δp[2]

= : D (p [2]) + 2M D (p [1]) δp[2] = Ξ (p [1] , p [2]) ,

where for notational simplicity, we assume that M [1] = M [2] =: M .

Next, because 2 cos (x) = exp (ix) + exp (−ix), we have then that (B.2) is

1
4M2

1
4M2

=

(cid:88)





(cid:88)

(cid:16)

p(cid:22)M

k1(cid:22)M

e−ik1·(cid:101)λp+j1 + e−ik1·(cid:101)λj1−p

(cid:17)









(cid:88)

(cid:16)

k2(cid:22)M

eik2·(cid:101)λp+j2 + e−ik2·(cid:101)λp−j2

(cid:17)





(cid:88)

p(cid:22)M

{(Ξ (p [1] + j1 [1] , p [2] + j1 [2]) + Ξ (j1 [1] − p [1] , j1 [2] − p [2]))

(Ξ (−p [1] − j2 [1] , −p [2] − j2 [2]) + Ξ (p [1] − j2 [1] , p [2] − j2 [2]))} . (B.3)

Let’s examine a typical term on the right of (B.3), say

1
4M2

(cid:88)

p(cid:22)M

Ξ (p [1] + j1 [1] , p [2] + j1 [2]) Ξ (−p [1] − j2 [1] , −p [2] − j2 [2]) .

33

By deﬁnition, the last displayed expression is

1
4M2

(cid:88)

p(cid:22)M

{D (p [2] + j1 [2]) D (−p [2] − j2 [2])}

+

1
2M 3

+

1
2M 3

+

1
M 2

p(cid:22)M

(cid:88)

p(cid:22)M
(cid:88)

(cid:8)D (p [2] + j1 [2]) D (−p [1] − j2 [1]) δp[2]+j2[2]

(cid:9)

(cid:8)D (−p [2] − j2 [2]) D (p [1] + j1 [1]) δp[2]+j1[2]

(cid:9)

p(cid:22)M
(cid:88)

(cid:8)D (p [1] + j1 [1]) δp[2]+j1[2]D (−p [1] − j2 [1]) δp[2]+j2[2]

(cid:9)

Because (A.8), it easy to see that the ﬁrst term is O (cid:0)M −1(cid:1), whereas the second term is

1
2M 3

M
(cid:88)

p[1]=1

D (j1 [2] − j2 [2]) D (p [1] + j2 [1])

M
(cid:88)

D (p [1] + j2 [1]) ≤ K

≤ K

1
M 2

p[1]=1
(cid:19)

(cid:18) log M
M

,

= O

1
M

M
(cid:88)

p[1]=1

1
(p [1] + j2 [1])+

so is the third term by symmetry. Finally the fourth term is diﬀerent than zero if j1 [2] = j2 [2], in
which case becomes

1
M 2

M
(cid:88)

p[1]=1

D (p [1] + j1 [1]) D (−p [1] − j2 [1]) .

Then, proceeding similarly with the other three terms in (B.3), we can conclude, except negligible

terms, that it is

M
(cid:88)

p[1]=1

M
(cid:88)

p[1]=1

(cid:88)

1
M 2

1
M 2

2
M 2

=

k1,k2(cid:22)M

{D (p [1] + j1 [1]) D (−p [1] − j2 [1]) + D (j1 [1] − p [1]) D (p [1] − j2 [1])} δj1[2]−j2[2]

{D (j1 [1] − p [1]) D (−p [1] − j2 [1]) + D (p [1] + j1 [1]) D (p [1] − j2 [1])} δj1[2]+j2[2]


e−i(j1[1](cid:101)λk1[1]−j2[1](cid:101)λk2[1])

M
(cid:88)

(cid:16)

cos

p[1]=1

(k1 [1] − k2 [1]) (cid:101)λp[1]

(cid:17)


 δj1[2]−j2[2]

+

2
M 2

(cid:88)

k1,k2(cid:22)M


e−i(j1[1](cid:101)λk1[1]−j2[1](cid:101)λk2[1])

M
(cid:88)

(cid:16)

cos

p[1]=1

(k1 [1] + k2 [1]) (cid:101)λp[1]

(cid:17)


 δj1[2]+j2[2].

From here we conclude by Lemma 4 of Hidalgo and Yajima (2002).

34

Lemma 5. Under Condition C1, we have that

(cid:88)

αke−ik·(cid:101)λj = O (cid:0)M−2(cid:1) .

{M [1]≤k[1]}∨{M [2]≤k[2]}

Proof. The proof is standard because four times continuous diﬀerentiability of f (λ) implies that
αk[(cid:96)] = O

k [(cid:96)]−5(cid:17)

for (cid:96) = 1, 2.

(cid:16)

The next lemma is regarding the approximation of integrals by sums. Taking for simplicity that

ˇn =: n [1] = n [2] and recalling our notation, we have then that j/n =: (j [1] /ˇn, j [2] /ˇn). Also, use
the standard notation, |k| = k1 + k2, k! = k1!k1!, yk = yk1

2 and for a function Υ (x)

1 yk2

∂kΥ (x) =

∂|k|Υ (x)
∂x|k|

.

Lemma 6. Assume that Υ (·) is a function q times continuously diﬀerentiable in [0, 1]2. Then,

1
ˇn2

ˇn
(cid:88)

ˇn
(cid:88)

j[1]=1

j[2]=1

(cid:19)

(cid:18) j
n

Υ

−

(cid:90) 1

(cid:90) 1

0

0

Υ (x) dx =

(cid:88)

hn,k(cid:122)k + O

|k|≤q−1

(cid:19)

,

(cid:18) 1
ˇnq

(B.4)

where hn,k is a sequence such that hn,k = O (cid:0)ˇn−k(cid:1) and (cid:122)1, (cid:122)2,...,(cid:122)q−1 are ﬁnite constants.

Proof. The left side of (B.4) is

ˇn
(cid:88)

ˇn
(cid:88)

(cid:90) j[1]

ˇn

(cid:90) j[2]

ˇn

j[1]=1

j[2]=1

j[1]−1
ˇn

j[2]−1
ˇn

(cid:19)

(cid:18)

Υ

(cid:18) j
n

(cid:19)

− Υ (x)

dx

(B.5)

=

ˇn
(cid:88)

ˇn
(cid:88)

(cid:90) j[1]

ˇn

(cid:90) j[2]

ˇn




(cid:88)

∂kΥ

(cid:17)

(cid:16) j
n

j[1]=1

j[2]=1

j[1]−1
ˇn

j[2]−1
ˇn



|k|≤q−1

k!

(cid:18)

x −

(cid:19)k

j
n

(cid:88)

+

|k|=q

∂kΥ (x (j))
k!

(cid:18)

x −

(cid:19)k

j
n






dx,

by Taylor’s expansion and where x (j) denotes a point between (j − 1) /n and j/n. Now, the right

side of (B.5) is

(cid:88)

|k|≤q−1

1
ˇnk+2

ˇn
(cid:88)

ˇn
(cid:88)

j[1]=1

j[2]=1

∂kΥ

(cid:17)

(cid:16) j
n

k!

+ O

(cid:19)

.

(cid:18) 1
ˇnq

(B.6)

Denoting ∂kΥ (x) = ηk (x) and (cid:122)k = (cid:82) 1

0

(cid:82) 1
0 ηk (x) dx, k = 1, ..., 3, and proceeding as with (B.5),

we conclude that (B.6), and hence the left side of (B.4), is

(cid:88)

|k|≤q−1

K
ˇnk

(cid:122)k + O

(cid:19)

(cid:18) 1
ˇn4

after observing that ηk (x), k = 1, ..., q −1, are respectively q −k continuous diﬀerentiable functions.

35

Lemma 7. Under Condition C1, for all p,

ap,n − ap =

(cid:37)p
M

+ O (cid:0)M−2 log M(cid:1) ,

where (cid:8)(cid:12)

(cid:12)(cid:37)p

(cid:12)
(cid:9)
(cid:12)

p≥1

is a summable sequence.

Proof. By deﬁnition, ap,n − ap is

1
4M



+



(cid:88)

(cid:0)Aj,n − A∗

j

(cid:1) eip·(cid:101)λj +

−M <j≤M

1
4M

(cid:88)

(cid:0)A∗

j − Aj

(cid:1) eip·(cid:101)λj

−M <j≤M

1
4M

(cid:88)

Ajeip·(cid:101)λj −

−M <j≤M

1
4π2

(cid:90)

Π2

A (λ) eip·λdλ



 ,

(B.7)

(cid:16)

− (cid:80)
where A∗
third term of (B.7) is

j = exp

k(cid:22)M αke−ik·(cid:101)λj

(cid:17)

. Because A (λ) is four times continuous diﬀerentiable, the

M−1(cid:122)2,p + M−3/2(cid:122)3,p + O (cid:0)M−2(cid:1)

(B.8)

by Lemma 6. This is the case after we notice that (cid:122)(cid:96),p there is given by

(cid:122)(cid:96),p =:

(cid:90) π

(cid:90) π

−π

−π

∂(cid:96)A (λ) eip·λj dλ = O

(cid:16)

p(cid:96)ap

(cid:17)

(cid:16)

p(cid:96)−5(cid:17)

= O

which is clearly summable since (cid:96) ≤ 3, and because A (−π, λ [2]) = A (π, λ [2]) and A (λ [2] , −π) =

A (λ [2] , π) for every λ [1] , λ [2] ∈ Π implies that

(cid:122)1,p =:

(cid:90) π

(cid:90) π

−π

−π

(cid:18) ∂A (λ) eip·λj
∂λ [1]

+

∂A (λ) eip·λj
∂λ [2]

(cid:19)

dλ = 0.

Next, the second term of (B.7) is bounded in absolute value by

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
4M

(cid:88)

(cid:32)

(cid:40)

exp

(cid:88)

†

−M <j≤M

p

(cid:41)

(cid:33)

αke−ik·(cid:101)λj

− 1

Ajeip·(cid:101)λj

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤

K
4M

(cid:88)

|Aj|

−M <j≤M

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:88)

†

αke−ik·(cid:101)λj

k

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(1 + O (1)) = O (cid:0)M−2(cid:1) ,

(B.9)

by Lemma 5 and that (cid:80)

−M <j≤M |Aj| = O (M), where (cid:80)†

p denotes the summation in

S (p) =: {p : (0 ≺ p) ∧ {(M [1] < p [1]) ∨ (M [2] < p [2])}} .

36

Finally, by deﬁnition of Aj,n and A∗

j , the ﬁrst term of (B.7) is

1
2M

(cid:88)

−M <j≤M



exp




(cid:88)



k(cid:22)M

(αk,n − αk) e−ik·(cid:101)λj


 A∗

j eip·(cid:101)λj ,

− 1






(B.10)

where using the inequality in (A.8), we have that (cid:80)

k(cid:22)M (αk,n − αk) e−ik·(cid:101)λj is





1
4M

(cid:88)

−k(cid:22)M

(cid:88)

log (f(cid:96)) cos

(cid:16)

(cid:17)

−

k · (cid:101)λ(cid:96)

−M <(cid:96)≤M

1
4π2

(cid:90)

Π2

log (f (λ)) cos (k · λ) dλ


 e−ik·(cid:101)λj

=

(cid:18) 1
M

(cid:88)

k(cid:22)M

(cid:122)2,k +

(cid:19)

1
M3/2

(cid:122)3,k

e−ik·(cid:101)λj + O (cid:0)M−1j−1(cid:1) ,

(B.11)

by Lemma 6 and where now (cid:122)(cid:96),k = (cid:82)
Π2 log (f (λ)) cos (k · λ) dλ, (cid:96) = 2, 3. Next, because log (f (λ))
is four times continuously diﬀerentiable, it implies that (cid:122)(cid:96),p =: O (cid:0)p(cid:96)−5(cid:1) and thus by standard
arguments, the right side of (B.11) is

M−1ψ2
(cid:88)

ψ(cid:96) (λ) =

(cid:122)(cid:96),ke−ik·λ.

(cid:17)

(cid:16)

(cid:101)λj

+ M−3/2ψ3

(cid:17)

(cid:16)

(cid:101)λj

+ O (cid:0)M−1j−1(cid:1)

From here and using Taylor expansion of exp(x), we obtain that (B.10) is

k(cid:22)∞

1
2M

νp,2
M

=

(cid:88)

(cid:110)
M−1ψ2

(cid:16)

(cid:101)λj

(cid:17)

+ M−3/2ψ3

(cid:17)(cid:111)

(cid:16)

(cid:101)λj

j eip·(cid:101)λj + O (cid:0)M−2 log M(cid:1)
A∗

−M <j≤M
νp,3
M3/2

+

+ O (cid:0)M−2 log M(cid:1)

(B.12)

where {νp,(cid:96)}p≥1, (cid:96) = 2, 3, are the Fourier coeﬃcients of ψ(cid:96) (λ) A∗ (λ), which are summable because
ψ(cid:96) (λ) A∗ (λ) is 4 − (cid:96) times diﬀerentiable function. The conclusion of the lemma now follows by
gathering terms (B.8), (B.9) and (B.12).

37

References

Baltagi, B. H., H. H. Kelejian, and I. R. Prucha (2007). Analysis of spatially dependent data.

Journal of Econometrics: Annals Issue.

Banerjee, S., A. E. Gelfand, J. R. Knight, and C. F. Sirmans (2004). Spatial modeling of house

prices using normalized distance-weighted sums of stationary processes. Journal of Business &

Economic Statistics 22, 206–213.

Batchelor, L. and H. Reed (1918). Relation of the variability of yields of fruit trees to the accuracy

of ﬁeld trials. Journal of Agricultural Research XII, 245–283.

Besag, J. (1974). Spatial interaction and the statistical analysis of lattice systems. Journal of the

Royal Statistical Society Series B 36, 192–236.

Bester, C. A., T. G. Conley, and C. B. Hansen (2011). Inference with dependent data using cluster

covariance estimators. Journal of Econometrics 165, 137–151.

Bester, C. A., T. G. Conley, C. B. Hansen, and T. J. Vogelsang (2016). Fixed-b asymptotics for

spatially dependent robust nonparametric covariance matrix estimators. Econometric Theory 32,

154–186.

Bhansali, R. J. (1974). Asymptotic properties of the Wiener-Kolmogorov predictor. I. Journal of

the Royal Statistical Society. Series B 36, 61–73.

Bhansali, R. J. (1978). Linear prediction by autoregressive model ﬁtting in the time domain. Annals

of Statistics 60, 224–231.

Breidt, F. J., R. A. Davis, and A. A. Trindade (2001). Least absolute deviation estimation for

all-pass time series models. Annals of Statistics 29, 919–946.

Brillinger, D. R. (1981). Time Series: Data Analysis and Theory. Holden Day, San Francisco.

Cavaliere, G., H. B. Nielsen, and A. Rahbek (2020). Bootstrapping noncausal autoregressions: with

applications to explosive bubble modeling. Journal of Business & Economic Statistics 38, 55–67.

Conley, T. G. . (1999). GMM estimation with cross sectional dependence. Journal of Economet-

rics 92, 1–45.

Conley, T. G. and F. Molinari (2007). Spatial correlation robust inference with imperfect distance

information. Journal of Econometrics 140, 76–96.

Cressie, N. and H. Huang (1999). Classes of nonseparable, spatio-temporal stationary covariance

functions. Journal of the American Statistical Association 94, 1330–1340.

Cressie, N. A. (1993). Statistics for Spatial Data. Wiley Series in Probability and Mathematical

Statistics: Applied Probability and Statistics. John Wiley & Sons.

Dahlhaus, R. and H. K¨unsch (1987). Edge eﬀects and eﬃcient parameter estimation for stationary

random ﬁelds. Biometrika 74, 877–882.

Davis, R. A., C. Kl¨uppelberg, and C. Steinkohl (2013). Statistical inference for max-stable processes

in space and time. Journal of the Royal Statistical Society Series B 75, 791–819.

Fernandez-Casal, R., W. Gonzalez-Manteiga, and M. Febrero-Bande (2003). Flexible spatio-

temporal stationary variogram models. Statistics and Computing 13, 127–136.

38

Gao, J., Z. Lu, and D. Tjøstheim (2006). Estimation in semiparametric spatial regression. The

Annals of Statistics 34, 1395–1435.

Genton, M. G. and H. L. Koul (2008). Minimum distance inference in unilateral autoregressive

lattice processes. Statistica Sinica 18, 617–631.

Gupta, A. (2018). Autoregressive spatial spectral estimates. Journal of Econometrics 203, 80–95.

Guyon, X. (1982). Parameter estimation for a stationary process on a d-dimensional lattice.

Biometrika 69, 95–105.

Haining, R. P. (1978). The moving average model for spatial interaction. Transactions of the

Institute of British Geographers 3, 202–225.

Helson, H. and D. Lowdenslager (1958). Prediction theory and Fourier series in several variables.

Acta Mathematica 99, 165–202.

Helson, H. and D. Lowdenslager (1961). Prediction theory and Fourier series in several variables.

II. Acta Mathematica 106, 175–213.

Hidalgo, J. (2009). Goodness of ﬁt for lattice processes. Journal of Econometrics 151, 113–128.

Hidalgo, J. and Y. Yajima (2002). Prediction and signal extraction of strongly dependent processes

in the frequency domain. Econometric Theory 18, 584–624.

Iversen Jr, E. (2001). Spatially disaggregated real estate indices. Journal of Business & Economic

Statistics 19, 341–357.

Jenish, N. (2016). Spatial semiparametric model with endogenous regressors. Econometric The-

ory 32, 714–739.

Jenish, N. and I. R. Prucha (2012). On spatial processes and asymptotic inference under near-epoch

dependence. Journal of Econometrics 170, 178 – 190.

Korezlioglu, H. and P. Loubaton (1986). Spectral factorization of wide sense stationary processes

on Z2. Journal of Multivariate Analysis 19, 24–47.

Lanne, M. and P. Saikkonen (2013). Noncausal vector autoregression. Econometric Theory 29,

447–481.

Lanne, M. and P. Saikonnen (2011). Noncausal autoregressions for economic time series. Journal

of Time Series Econometrics 3, 1–30.

Lewis, R. and G. C. Reinsel (1985). Prediction of multivariate time series by autoregressive model

ﬁtting. Journal of Multivariate Analysis 16, 393–411.

Limaye, B. V. and M. Zeltser (2009). On the Pringsheim convergence of double series. Proceedings

of the Estonian Academy of Sciences 58, 108.

Majumdar, A., H. J. Munneke, A. E. Gelfand, S. Banerjee, and C. F. Sirmans (2006). Gradients in

spatial response surfaces with application to urban land values. Journal of Business & Economic

Statistics 24, 77–90.

McElroy, T. S. and S. H. Holan (2014). Asymptotic theory of cepstral random ﬁelds. The Annals

of Statistics 42, 64–86.

Mercer, W. B. and A. D. Hall (1911). The experimental errors of ﬁeld trials. Journal of Agricultural

Science IV, 107–132.

39

Mitchell, M. W., M. G. Genton, and M. L. Gumpertz (2005). Testing for separability of space-time

covariances. Environmetrics 16, 819–831.

Nychka, D., S. Bandyopadhyay, D. Hammerling, F. Lindgren, and S. Sain (2015). A multiresolution

Gaussian process model for the analysis of large spatial datasets. Journal of Computational and

Graphical Statistics 24, 579–599.

Pace, R. K. and R. Barry (1997). Sparse spatial autoregressions. Statistics & Probability Letters 33,

291 – 297.

Robinson, P. M. (2007). Nonparametric spectrum estimation for spatial data. Journal of Statistical

Planning and Inference 137, 1024–1034.

Robinson, P. M. (2011). Asymptotic theory for nonparametric regression with spatial data. Journal

of Econometrics 165, 5–19.

Robinson, P. M. and J. Vidal Sanz (2006). Modiﬁed Whittle estimation of multilateral models on

a lattice. Journal of Multivariate Analysis 97, 1090–1120.

Roknossadati, S. M. and M. Zarepour (2010). M -estimation for a spatial unilateral autoregressive

model with inﬁnite variance innovations. Econometric Theory 26, 1663–1682.

Serﬂing, R. (1980). Approximation Theorems of Mathematical Statistics. John Wiley & Sons.

Solo, V. (1986). Modeling of two-dimensional random ﬁelds by parametric Cepstrum.

IEEE,

Transactions on Information Theory 42, 743–750.

Stein, M. (1999). Interpolation of Spatial Data: Some Theory for Kriging. Spinger-Verlag, New

York.

Wang, H., E. M. Iglesias, and J. M. Wooldridge (2013). Partial maximum likelihood estimation of

spatial probit models. Journal of Econometrics 172, 77–89.

Whittle, P. (1954). On stationary processes in the plane. Biometrika 41, 434–449.

40

