2
2
0
2

n
a
J

5
1

]

V
C
.
s
c
[

1
v
6
1
8
5
0
.
1
0
2
2
:
v
i
X
r
a

A Critical Analysis of Image-based Camera Pose
Estimation Techniques

Meng Xua, Youchen Wangb, Bin Xuc, Jun Zhangc, Jian Renb, Stefan Poslada,
Pengfei Xuc,∗

aQueen Mary University of London, London
bBeihang University, Beijing
cDidi Chuxing, Beijing

Abstract

Camera, and associated with its objects within the ﬁeld of view, localization

could beneﬁt many computer vision ﬁelds, such as autonomous driving, robot

navigation, and augmented reality (AR). After decades of progress, camera lo-

calization, also called camera pose estimation could compute the 6DoF pose

of objects for a camera in a given image, with respect to diﬀerent images in a

sequence or formats. Structure-based localization methods have achieved great

success when integrated with image matching or with a coordinate regression

stage. Absolute and relative pose regression methods using transfer learning can

support end-to-end localisation to directly regress a camera pose but achieve a

less accurate performance. Despite the rapid development of multiple branches

in this area, a comprehensive, in-depth and comparative analysis is lacking to

summarise, classify and compare, structure-based and regression-based cam-

era localization methods. Existing surveys either focus on larger SLAM (Si-

multaneous Localization and Mapping) systems or on only part of the camera

localization method, lack detailed comparisons and descriptions of the meth-

ods or datasets used, neural network designs such as loss designs, and input

formats, etc. In this survey, we ﬁrst introduce speciﬁc application areas and

the evaluation metrics for camera localization pose according to diﬀerent sub-

∗Corresponding author
Email address: xupengfeipf@didiglobal.com (Pengfei Xu)

Preprint submitted to Journal of Elsevier

January 19, 2022

 
 
 
 
 
 
tasks (learning-based 2D-2D task, feature-based 2D-3D task, and 3D-3D task).

Then, we review common methods for structure-based camera pose estimation

approaches, absolute pose regression and relative pose regression approaches

by critically modelling the methods to inspire further improvements in their

algorithms such as loss functions, neural network structures. Furthermore, we

summarise what are the popular datasets used for camera localization and com-

pare the quantitative and qualitative results of these methods with detailed

performance metrics. Finally, we discuss future research possibilities and appli-

cations.

Keywords: camera pose regression, structure-based localization, absolute pose

regression, relative pose regression

1. Introduction

Camera pose is used to describe the position and orientation of a camera

in a world coordinate system, with respect to six degrees of freedom (6DoF),

using diﬀerent representations, e.g., a transformation matrix. The 6DoF can

be grouped into two categories, translations and rotations: translations are lin-

ear, horizontal straightness and vertical straightness; rotations are pitch, yaw

and roll. Camera pose also includes the estimation of objects’ poses in scenes

or scenarios for the camera. Camera pose estimation is useful for a range of

applications areas, such as augmented reality, robot navigations, autonomous

vehicles. These use the camera pose for further calculations, such as object

positions and scene perception. Compared with alternative location sensing de-

vices, such as Light detection and ranging (LiDAR), Global navigation satellite

system (GNSS). Camera pose estimation is easier to deploy and can be extended

to some downstream high-level tasks (e.g., robot grabbing, robot navigation),

where the camera determining the 6DoF can be fused with other sensors (e.g.,

Inertial Measurement Unit (IMU) sensors, Wi-Fi). The camera localisation task

estimates the 6DoF pose of the camera under a world coordinate system in re-

lation to objects from images or videos captured by cameras. With the recent

2

rapid progress of deep learning techniques applied to computer vision, camera

pose estimation methods developed from structure-based methods (recovering

the camera pose by establishing the correspondence between features in a query

image and a 3D structure feature in a scene model) to regression-based methods

(regressing the camera pose of a reference through a regressor by optimizing the

weights of a neural network). For both indoor and outdoor environments, im-

ages or videos captured by cameras could estimate the camera pose, speciﬁcally,

indoor environments require more accurate pose estimation as indoor spaces are

more cluttered.

This paper focuses on image-based camera pose estimation methods. The

system inputs are from camera images, which may include RGB and/or depth

images, a single image or image sequences, or videos, from moving or stationary

cameras. The ﬁnal output of the system is the 6DoF pose, but there may

also be some intermediate stage results (e.g., retrieval stage outputs, query

image related retrieval images, image matching stage outputs corresponding

from image pairs) from structure-based localization methods. We divide camera

pose methods into two main branches: structure-based localisation methods and

direct regression-based localisation methods. In addition to introducing these

methods, we also list a detailed artiﬁcial neural network analysis and internal

structure comparison. In addition, this survey reports the comparison of relative

datasets, quantitative and qualitative results, and gives some potential research

directions.

1.1. Application domains

To the best of our knowledge, there is no current in-depth survey of camera

pose applications. Augmented reality (AR) technology is currently a very pop-

ular image/video synthesis technology. It can superimpose three-dimensional

(3D) virtual objects outside the real environment onto images of the real envi-

ronment through projection to enhance the real-time images and support seam-

less integration of virtual and real images. It has a wide range of application

prospects, such as military training, education, games and entertainment. The

3

precise positioning of the camera is one of the core steps of the technology, which

is to obtain the 6 degrees of freedom (6DoF) of the camera. For AR-related tech-

nologies which are usually used indoors, a mobile machine-mounted camera is

normally used as it moves more smoothly than a human-moving mounted cam-

era for AR games because it avoids human body micromovements and limb

movement. Combining human movements and pose estimation of the camera

could improve the AR game experience and have more practical applications,

such as 3D reconstruction, etc.

Autonomous driving systems need a positioning module to sense the state

of the vehicle, usually input from sensors such as LiDAR and camera sensors.

The perception, navigation, planning and control systems of robots or cars need

copious and robust, knowledge of their location to decide the actions to do in

the next step. Devices on cars or robots use sensor data to measure the pose

relative to the initial pose (the camera pose when the camera starts to move)

to determine the current pose and then use a matching algorithm, navigation

beacons, etc.

for positioning and navigation. Outdoor localization accuracy

could be lower for navigation compared to those indoor environments because

objects may be larger, e.g., cars and objects are further apart, although, parking

and collision avoidance may need a higher accuracy. For autonomous driving,

real-time localization may be important.

Robot-related applications can use camera pose estimation for visual feed-

back, such as determining the position and direction of objects and helping

manual visual inspections for quality control and safety inspections. It can also

adaptively control the trajectory of walking robots. In addition, visual infor-

mation can be used to track paths, detect obstacles, and identify signs or the

environment to determine the location of the robot and avoid obstacles.

1.2. Survey organization

The remainder of this survey is organized as follows, section 2 introduces

the related survey papers and compares them with our survey’s scope and focus

(section 2.1), and critically analyzes the limitation of existing surveys (section

4

2.2). This survey mainly focuses on camera pose using detailed problem mod-

elling and gives a technical comparison. Section 3 models the camera pose

estimation problem with fundamental concepts of images, cameras, and pose

(sections 3.1 and 3.2) and some evaluation metrics (section 3.3) according to

diﬀerent computation models;

Section 4 analyses in more detail feature-based localization methods that

recover the camera pose by establishing the correspondence between features in

a query image and a 3D structure feature in a scene model. Matching based

localization methods explicitly establish 2D-3D correspondences via matching

descriptors (section 4.1) while scene coordinate regression-based localization

methods directly regress 3D scene coordinates from the query image (section

4.2);

Section 5 compares the network and loss functions for regression-based lo-

calization methods, which regresses the camera pose through the query im-

ages. These are classiﬁed into absolute camera pose (camera pose that uses

global coordinate system) regression method (APR, section 5.1) and relative

pose (camera relative pose between frames) regression method (RPR, section

5.2). Compared to APR methods, RPR methods ﬁrst regress the relative pose

using retrieval methods from a geo-referenced dataset or other methods and then

compute the absolute pose, instead of directly regressing the absolute pose.

Section 6 summarizes the datasets used in the previously proposed methods

(section 6.1) with the dataset size, environment, capture devices, application

area, etc., the benchmark results of published papers (section 6.2) and a real-

world application comparison (section 6.3). Section 7 ﬁnally summarizes the

previously published methods in the camera pose estimation area and proposes

future possible directions. Figure 1 illustrates the overview of sections 3-6.

5

Figure 1: The overview of survey sections 3-6

6

Section3ProblemformulationImagesandcamerasPoserepresentationEvaluationmetricsSection4 Structure-basedmethodsDirectmatchingmethods2D-2Dmatching3D-2DmatchingHierarchical matching methodsImageretrievalCo-visibility clustering Local feature matchingSparse-to-sparse Sparse-to-dense  Dense-to-dense Detect-then-describe Detect-and-describe Describe-to-detect Section5Regression-basedmethodsAbsolute camera pose regression Fixed Euclidean lossLearnable pose loss other loss methods Throughsingle image Throughimage sequences ThroughVideosRelative camera pose regression Throughexplicit retrieval Through implicit CNN Section6 Estimationcomparisons Comparison of datasetsComparison of Published Results on common benchmarksComparison of Real-World ApplicabilityPosition and rotation error Correctly localized queries Maximum reprojection difference Scene coordinate regression Matching-basedmethodsSection4-5Mainbranch2. Related work

2.1. Related surveys

Piasco et al.

[1] focus on the input data, which classiﬁes the heteroge-

neous input data into optical, geometric, semantic and cross-data (a fusion of

several types of data) information and emphasizes the application of diﬀerent

data features in direct and indirect vision-based localization methods, especially

the inﬂuence of features on positioning under appearance changes, e.g., light

change. Sattler et al.

[2] focus on theoretically modelling the absolute pose

regression system and compare structure-based and retrieval-based methods’

pipelines with experiments. This survey ﬁnally concludes that APR methods

could be improved to compete with other methods, e.g., RPR methods, al-

though a performance gap still exists between them. Wu et al.

[3] model the

camera localization problem as a SLAM system and presents the survey’s struc-

ture according to whether the environment is known or unknown, whether the

mapping process is real-time or oﬄine, etc.

However, the above surveys only pay attention to a narrow area (e.g., input

data of the system, APR system, SLAM system classiﬁcation), without oﬀering

any comprehensive guidance to camera pose localization methods’ overview. In

contrast to the above surveys, we propose a far wider analysis of camera local-

ization into structure-feature based methods (including matching-based local-

ization and scene coordinate-based localization) and regression-based methods

(including absolute camera pose regression-based localization and relative pose

regression-based localization).

Some additional surveys focus camera pose more on robotics. Debeunne et

al.

[4] introduce vision-based localization methods, LiDAR-based localization

methods and their fusion. Chen et al.

[5] discuss visual odometry, mapping,

localization and SLAM methods. Shavit et al. [6] focus on deep learning-based

absolute localization methods.

Instead of introducing all the SLAM system

research work or just explaining their absolute estimation methods, our survey

explains camera pose estimation methods using only image input, focusing on

7

analyzing and summarizing the problem deﬁnition, the algorithm pipeline, and

general approaches to improve the performance of such methods.

There is a lack of analysis and comparison of the two main branches of meth-

ods for camera pose estimation, structure-based methods and direct regression-

based methods, with respect to architecture, limitations and the beneﬁts of each

algorithm.

2.2. Analysis of limitations of existing surveys

Table 1: Comparison of existing camera pose surveys

Surv

Main classiﬁcation structure

Focus

Data

Struc

APR RPR

ey

[1]

Optical, geometric, semantic

Data structure (cid:88)

and cross-data information

sets

ture

[2]

Absolute regression methods,

APR

mod-

(cid:88)

(cid:88)

relative regression methods

elling

[3]

Known environment, unknown

SLAM classiﬁ-

environment

cation

[4]

Vision localization, LiDAR lo-

Sensor fusion

calization, fusion methods

[5]

SLAM,

Visual Odometry

SLAM system

(VO), Mapping, Localization

[6]

End-to-end localization, hy-

End-to-end

(cid:88)

(cid:88)

(cid:88)

brid localization

networks

Ours Structure-based

localization,

Camera pose

(cid:88)

(cid:88)

(cid:88)

(cid:88)

regression-based localization

estimation

Table 1 summarises the main structure and focus of each survey. A ‘tick’

indicates whether they contain a dataset comparison, deﬁne a structure-based

localization method versus an APR method or RPR method. Existing surveys

tend to focus on either larger SLAM systems or only on speciﬁc camera local-

8

ization methods. Existing camera pose surveys don’t tend to compare their

methods with respect to datasets, and to artiﬁcial neural networks (ANNs)

with respect to loss functions and input formats such as single image, image

sequences and videos, in detail. Moreover, because some pose estimation meth-

ods are built upon image retrieval or image matching, our review establishes

a model of a two-stage method. The ﬁrst stage is to retrieve the most similar

image of the reference image or to obtain the matching correspondences from

the input image pair. The second stage is to regress to the camera pose based on

the retrieval or matching results. This survey summarises and classiﬁes image

matching methods for camera pose estimation in the structure-based pose esti-

mation stage and seeks to address the lack of a description of such matching or

retrieval-based camera pose estimation problems in other surveys. Our survey

also reviews structure-based methods and regression-based methods (including

APR methods and RPR methods) concerning the analysis of ANNs including

loss functions. In addition, this survey also establishes the localization focus

formulation of diﬀerent models, e.g.,2D-2D localization, 2D-3D localisation and

3D-3D localisation, with diﬀerent multiple datasets formats (e.g., single image,

image sequence, videos) and environments.

3. Problem formulation

To understand camera pose, we ﬁrst introduce problem formulation with

images, cameras, pose representation and evaluation metrics. Given an image

IC, from a monocular or depth camera C, a series of methods is applied to the

image to get the 6DoF (degree of freedom) coordinates, which represents the

position and orientation of the image in 3D space. The visual localization task

obtains the pose within a known scene by matching a query object image in

a trained model from images of objects in a dataset. According to the types

of inquiry, we categorize camera pose methods into three types: (1) structure-

based localization to estimate the camera pose by matching 2D pixel features

to 3D point scene coordinates or to estimate the camera pose by matching

9

the 3D pixels with a 3D map; (2) absolute pose regression-based localization

to directly regress the pose from a 2D map built from an end-to-end neural

network; (3) relative pose regression-based localization to regress the relative

pose using retrieval methods from a geo-referenced dataset or other methods

and then compute the absolute pose.

3.1. Images and cameras

Generally, for a camera pose estimation task, cameras include monocular,

stereo, and RGB-D cameras that take the combination image of an RGB image

and its corresponding depth image. The monocular camera applies a pinhole

camera model which projects points in the real world, using an external pa-

rameter matrix from the world coordinate system to the camera coordinate

system, then using an internal parameter matrix from the camera coordinate

system to the pixel coordinate system. Through this reprojection process, the

points in the world would be represented in an image in the form of pixels while

monocular and stereo cameras generate RGB images, a RGB-D camera could

simultaneously return RGB images and depth images.

3.2. Pose representation

Each pose p includes a 3D camera position and orientation. There are many

formats to represent the change of position and orientation, i.e., translation

and rotation. For orientation, a 3×3 rotation matrix R, a 4-digit quaternion,

and Euler angles (yaw, pitch, roll) are each interconvertible to represent the

same rotation. Usually, the 3-digit coordinates x in 3D space and a 3-digit

normalized quaternion q are chosen separately as the position and orientation

representations. Thus, the ground truth pose vector p and estimation pose

vector ˆp could be deﬁned by the combination of translation and rotation:

(1)

p = (x, q)

ˆp = (ˆx, ˆq)

10

3.3. Evaluation metrics

The evaluation approaches for localization tasks change according to the

metric focus and localization methods. While evaluating the performance of

camera pose estimation methods, we need to compare the computed pose from

the estimation method with the ground truth pose to measure how close the

estimated result is to the ground truth. Since the camera pose is associated

with 3D model coordinates, the standard approach for obtaining ground truth

6DoF poses is to use structure-from-motion (SfM) tools, such as Bundler [7],

COLMAP [8], and VisualSFM [9] or to use such coordinated provided directly

by scanning devices such as Microsoft Kinect.

3.3.1. Position and rotation error (learning-based 2D-2D)

For the datasets that directly provide ground truth poses, the pose accuracy

of a method is measured by the deviation between the estimated and the ground

truth pose, two prominent error metrics for direct localization methods are the

absolute pose error (APE) and the relative pose error (RPE). The APE is well-

suited for measuring the performance of visual SLAM systems.

In contrast,

the RPE is well-suited for measuring the drift of a visual odometry system, for

example, the drift per second [10].

3.3.1.1. Absolute pose error (APE).

When the algorithm input is a single image, the absolute pose error is mea-

sured by the combination of absolute position error and orientation error, in

which the position error is measured as the Euclidean distance in m between

the estimated position ˆx and the ground truth position x.

tape = ||x − ˆx||2

(2)

The absolute orientation error |α|, measured as an angle in degrees, rep-

resents the minimum rotation angle α required to align the ground truth and

estimated orientations. The rotation error α could be calculated backwards
from the trace of the real and estimated rotation matrix, R and ˆR, which could

11

also be represented with the real and estimated quaternion, q and ˆq.

rotapeerr = α =

2cos(α) = trace(R−1 ˆR) − 1
1
2

arccos(trace(R−1 ˆR) − 1)

rotapeerr = α = 2arccos|q ˆq|

180◦
π

(3)

3.3.1.2. Relative pose error (RPE).

The algorithm is input as image pairs as a time-series from sequential images.

Similarly, from the absolute pose error, relative pose error is measured by the

combination of relative position error and orientation error, in which the position

error is measured as the Euclidean distance shift speed in m/s between the

estimated relative position ˆxrel and the ground truth relative position xrel. The

orientation error is measured as the minimum angle deviation rate in degree/s

between the estimated relative quaternion ˆqrel and the ground truth relative

orientation qrel using a quaternion representation.

rotrpeerr = αrel = 2arccos|qrel ˆqrel|

trpeerr = ||xrel − ˆxrel||2
180◦
π

(4)

The position and rotation errors are commonly reported using statistical

data metrics, e.g., median, mean and the standard deviation of a sequence of

images that may each have a position and rotation error.

3.3.2. Correctly localized queries (feature-based 2D-3D)

In contrast to directly comparing localization errors, some indirect methods

measure the correctly localized queries to represent the performance of the local-

ization algorithms. According to whether the query error thresholds are preset

ﬁxed or sampled, we introduce two diﬀerent error metrics, a ﬁxed thresholds

error and a sampled thresholds error.

3.3.2.1. Fixed thresholds error.

Another metric for indirect camera pose methods measures the percentage of

images registered within given error thresholds as the localization performance,

i.e., within X meters and Y degrees of their ground truth pose.

12

Sattler et al. [11] deﬁne a set of signiﬁcant margins under strict pose thresh-

olds. Their metrics report the percentage of dataset queries localized within

a given error bound on the estimated camera position and orientation, which

includes high-precision (0.25m, 2◦), medium-precision (0.5m, 5◦), and coarse-

precision (5m, 10◦). These thresholds highlight the overall accuracy as the

percentage of images whose localization poses are below the thresholds. The

higher the percentage, the better the performance the algorithm shows.

3.3.2.2. Sampled thresholds error.

Using the same error thresholds for all the images in a dataset will lead to

some limitations. For example, images closer to the camera will intuitively get

a smaller error range compared to further away images. Thus, sampled error

thresholds could set the error thresholds per image [12] using a set of sampling

k ratios, e.g., 50%, 30% and 10% respectively.

Using the ratio @k, or ratio @k%, one can measure the percentage of queries

that have a good match with the k or k% top-ranked images. Usually, k is set

to 10 or 1%. For example, if the ratio is set to @95% precision, this means the

algorithm is allowed to make a mistake in 5% of all cases on the percentage of

correctly localized images, and only the top 95% of database candidates need

to be considered.

3.3.3. Maximum reprojection diﬀerence (3D-3D task)

To measure pose accuracy based on reprojections means to measure the

diﬀerence between the reprojection of a set of 3D points for the ground truth

and estimated poses. Deﬁning certain thresholds around the reprojection of the

3D points could avoid the impact of perturbations on the camera that leads to a

change in the reprojected 2D locations of 3D points. The maximum reprojection
error between the ground truth pose T and estimated pose ˆT for the images
||π(pl, Ti) − π(pl, ˆTi||2, and the performance

i = max
l∈[1,N i
f ]
of this algorithm is measured by the percentages of images with r∞
i

could be described as r∞

that is lower

than the preset thresholds of the whole dataset.

13

4. Structure feature-based localization methods

The main branch of this camera pose estimation survey consists of structure

feature-based and regression-based pose estimation methods. In this section we

ﬁrst model structure feature-based methods. A structure-feature-based local-

ization pipeline refers to methods to recover the camera pose by establishing a

correspondence between features in a query image and a 3D structure feature

in a scene model, where the 3D point-cloud model is built using structure from

motion (SFM) or simultaneous localization and mapping (SLAM) that records

the structure of the whole scene. Compared with methods that regress camera

pose solely based on object features in images, a structure-feature-based pipeline

is more dependent on a priori information of the 3D scene model.

After establishing the correspondence between the 3D point cloud in the

scene model and query images, we can recover the camera pose through geo-

metric constraints which is a classic pipeline that applies a Perspective-n-Point

(PnP) to solve to compute the camera pose and uses a Random sample consen-

sus (RANSAC) [13] method to get rid of outliers.

According to methods that establish a correspondence between a queried

image of an object and a 3D sensed model of it, we can divide structure-feature-

based localization approaches into two categories: Matching Based Localiza-

tion and Scene Coordinate Regression-Based Localization. The ﬁrst method is

based on descriptor matching, and the other is based on a trainable localization

pipeline. These approaches are described in more detail as follows.

4.1. Matching based localization

Matching based localization methods use feature descriptors to establish a

correspondence between a query image of objects and existing images of object

scenes. Usually, the 3D scene model assigns one or several visible local descrip-

tors to each 3D point. Given a query image, we need to extract stable and

distinct features to build a correspondence between images and scene models.

Thus, a descriptor matching based localization task is converted to a feature

descriptor matching task. To match features from the scene model, usually,

14

we can compare the distance between descriptors. We can divide features into

two categories by the matching method used: direct matching and hierarchical

matching. In this paper, we deﬁne direct matching methods as direct matching

2D query image feature sets with a 3D scene feature point and hierarchical

matching methods as matching 2D query image feature sets with 2D scene

database image features to indirectly establish 2D-3D correspondence.

4.1.1. Direct matching methods

Previously, large-scale visual localization has been treated as a place recog-

nition problem [14, 15]. The location for the query image is determined by the

most similar image retrieved from the database. But the accuracy of retrieval

localization methods does not satisfy challenging applications which need to use

an accurate 6DoF pose. To achieve higher accuracy, the use of 3D scene models

to estimate pose has been increasingly proposed and used by researchers.

4.1.1.1. 2D-3D matching.

Figure 2: The processing pipeline for direct matching-based localization meth-

ods

It can be intuitive to directly match 2D feature points of query images with

3D feature points to build up a correspondence set, as shown in Figure 2. The

main challenge in direct matching methods is to eﬃciently and eﬀectively ﬁnd

a large enough number of high-quality correspondences to facilitate pose esti-

mation.

In the beginning, much work focused on improving matching methods. For

2D image features and 3D point matching, it’s unclear if any ordering of image

15

Query Image8419....+CNN=Global FeatureFeature MatchingDatabase8419....+=searchFeature Extraction=1  3  8  ·  ·  ·Local Feature+=Outlier Pre-filteringRANSACPose ComputationPose Estimationfeatures is better than any other. It’s time-consuming to consider all features

in the query image. To accelerate 2D-3D matching. Li et al.

[16] propose a

“Point-to-feature” matching method, a prioritized point matching algorithm,

which matches a subset of scene 3D points to features in the query image and

is ordered by a visibility graph. Furthermore, they accelerate search steps by

compressing the 3D scene model. Rather than delete images in the database,

they found the smallest point cloud subset covers each image at least K times.

But Sattler et al.

[17] believe that “Feature-to-point” matching methods also

reduce the long-term matching process time. They propose a priority matching

method termed Vocabulary-based Prioritized Search (VPS) to speed up descrip-

tor matching. They processed the features in ascending order of their matching

costs (starting with features whose activated visual words contain only a few

descriptors). Following this, they discussed the advantages and disadvantages of

2D-3D, 3D-2D matching [18]. Feature points in query images are several orders

of magnitude less than those in the model. 3D-2D matching is more eﬀective,

but the location accuracy can be lost. 2D-3D matching can ﬁlter out wrong

matches using a ratio test [19]. But for a crowded feature space in a large scene

model, it is hard to ﬁlter out fuzzy matches using a higher threshold of the ratio

test. Sattler et al. [20] proposed an Active Search method that combines 2D-3D

and 3D-2D matching. After ﬁnding a 2D-to-3D pair, they actively search for

3D-to-2D correspondences for the 3D points closest to the matched point. They

used coarse-level features in the vocabulary tree [21] to recover matches. Sattler

et al. [20] compressed the model by quantizing the point descriptors to achieve

run-time localization. By assigning diﬀerent labels to 3D points, they could use

a loose matching strategy to create a locally unique matching set. Similarly,

Feng et al. [22] use a binary descriptor combined with features from accelerated

segment test (FAST) [23] feature point detection to complete feature extraction

and proposed an improved binary descriptor retrieval method. They assigned

the label information of multiple features to the same 3D point and constructed

a supervised trained Random-Forest to complete the matching step. Based on

[17, 18], Sattler et al. proposed an eﬃcient and eﬀective pipeline. This pipeline

16

uses quantitative feature descriptors to accelerate 2D-3D matching, and 3D-2D

matching methods to recall the matching loss due to quantization. In a large-

scale environment, similar or repeated feature points always lead to location

determination failures. To build a more eﬃcient localization system, Liu et

al. proposed a method [24] that uses global context information to solve this

problem. This not only focuses on point-to-Feature 2D-3D matching but jointly

processes the feature set of the query graph and the corresponding candidate

matching set using a Random Walk with Restart (RWR) algorithm [25].

Except for improving matching methods, some work used additional infor-

mation or outlier ﬁlter methods to improve localization accuracy. Svarm et al.

[26] proposed a reliable and tractable outlier rejection scheme that can handle

massive amounts of outliers in data. Linus also proposed using vertical coordi-

nates to improve 3D localization [27]. Similar to [28], rather than to just improve

the matching precision, this takes a diﬀerent approach. This allows a matching

scheme to generate a large number of matches, whether correct or incorrect, to

ensure that no matching matches are missed. Then, they proposed a voting-

based geometric veriﬁcation process, which uses a priori information concerning

the direction of gravity and the height of the camera, to ﬁlter outliers.

4.1.1.2. 3D-3D matching.

For RGB-D images or stereo systems, we need to match 3D points with the

scene point cloud model. Without any already deﬁned descriptors, we usually

use the iterative closest point (ICP) method to match 3D points. Except di-

rectly using the spatial information, we can still extract features such as shape,

density, etc. By comparing descriptors, we can establish a 3D-3D correspon-

dence. Approaches such as [29, 30] are classic low-level hand-crafted geometric

3D feature descriptors. Choi et al. [31] use such descriptors for 3D reconstruc-

tion to achieve considerable results. But these descriptors can be unstable or

inconsistent when used in real-world partial surfaces from 3D scanning data and

are diﬃcult to adapt to new datasets. 3D-ShapeNet used 3D deep learning to

model point cloud shapes [32]. Similarly, [33, 34] focused on extracting features

17

from complete 3D object models at a global level. To provide a more robust

descriptor when dealing with partial data suﬀering from various occlusion pat-

terns and viewpoint diﬀerences, [35, 36] proposed approaches that compose only

local level features at a small range.

Compared with 2D-3D matching based, 3D-3D localization is relatively un-

derexplored.

4.1.2. Hierarchical matching methods

Figure 3: The pipeline of hierarchical matching based localization methods

The direct matching approaches introduced in section 4.1.1 mostly rely on

estimating correspondences between a 2D feature in the query and 3D points in

a sparse model using local descriptors. For direct matching methods, we need to

search each 3D point for a query feature, which is not eﬃcient. Although many

papers try diﬀerent ways to improve eﬃciency and accuracy, they still show

a fragile robustness for repetitive local features in the matching process. We

aim to substantially increase the robustness of the localization while retaining

tractable computational requirements, e.g., the calculation amount cannot ex-

ceed the model load and can deal with a certain amount of feature repeatability.

Therefore, a coarse-to-ﬁne hierarchical localization paradigm [37] is proposed to

solve this problem. as shown in Figure 3.

In 2009, Irschara et al.

[38] use a retrieval-based method [39] to search

for the smallest scene model subset. By only estimating correspondences in

this subset, they can achieve real-time localization in large-scale datasets. But

the robustness of retrieval-based methods is limited by the poor invariance of

18

Query Imagevocabulary tree2D-3D Search············Camera Pose Estimationfrom 2D-3D correspondence using RANSAChand-crafted local features. Recent features emerging from convolutional neural

networks (CNN) exhibit far better robustness at a lower computation cost. The

pipeline of a hierarchical approach is simple and eﬀective.

It follows image

retrieval, co-visibility (meaning that two images see common areas), clustering

and local feature matching.

4.1.2.1. Image retrieval.

We address the image retrieval problem as follows, given a query image, a

system should eﬃciently retrieve similar images from the database. Retrieval

systems have matured to incorporate spatial veriﬁcation [40, 41, 42] and query

expansion [43, 44]. Over more recent years, several image clustering methods

based on local features have been proposed, such as Bag-of-Words (BoW) [45],

Vector of Local Aggregated Descriptors (VLAD), etc. For local aggregation

retrieval methods that rely on local features but ignore global contextual infor-

mation, these tend to show a poor performance when repeated local features

appear in a large dataset. Since Krizhevsky et al.

[46] show the advantage

of learning-based features, further research followed that used CNN layer ac-

tivations as oﬀ-the-shelf image descriptors that appear as objective results in

retrieval tasks [47, 48]. Following classic retrieval approaches, such work uses

CNN to aggregate local features [49, 50]. Chum et al. [49] use a classiﬁcation

network followed by the use of a Maximum-Activations-of-Convolutions (MAC)

layer to extract feature vectors. This used a 3D model reconstructed by unla-

beled images to produce a training dataset. In addition, this proposed a search

strategy to ﬁnd hard positive and hard negative image pairs for training which

can impact the results. Arandjelovic et al. [50] proposed NetVLAD on the basis

of VLAD [51], as a trainable VLAD layer. They used Google street view1 to

produce a weakly supervised dataset. For a less accurate location ground truth

obtained by global navigation satellite systems (GNSS), triplet-loss is used to

ensure that the feature distance of all positive images should be smaller than

1See https://www.google.com/streetview/

19

the feature distance of all negative images. Similarly, with NetVLAD, Rade-

novi´c et al.

[52] also train their model using hard positive and hard negative

through triplet-loss. After extracting dense features through FCN, they used a

Generalized-mean pooling layer (GEM) to generate global features. Not only is

triplet loss used to learn spatial information, but this approach also exploits the

use of second-order spatial attention [53] in descriptor learning and combines

it with second-order descriptor loss to improve the learning global image repre-

sentation. All of the approaches mentioned above used triplet loss to train the

network by accurate location information obtained by the SfM model. Revaud

et al. [54] proposed a method to directly optimize the global retrieval ranking.

The author used listwise ranking loss to directly optimize mAP (mean average

precision). For the computational memory requirement, a multi-stage backprop-

agation method was proposed to train the network. In contrast to the above

method, DELF [55] used image classiﬁcation labels to supervise metric learning.

The main innovation is, after extracting dense local features through CNN, they

used a landmark classiﬁer to focus on extracting key points and descriptors. Fi-

nally, global features are generated through an FCN layer. Teichmann et al.

[56] proposed regionally aggregated match kernels to leverage selected image

regions and produce a discriminative image representation. Husain et al.

[57]

proposed an approach REMAP, to ensemble the multi-resolution region-based

features, which explicitly employs regions discriminative power, measured using

Kullback-Leibler (KL) divergence values, to control the aggregation process. All

these Global-Single-Pass methods, use global descriptors generated by a single

forward-pass through a CNN, have been designed to focus on global contextual

information for robust and discriminative global feature descriptors.

4.1.2.2. Local feature matching.

We address the local feature matching problem as given a query image and a

set of similar scene images, in contrast, to direct matching methods, we should

eﬃciently extract local features and match them based upon query-based sim-

ilar scene images, instead of 3D feature points in a scene model, according to

20

matching methods such as sparse-to-sparse matching, sparse-to-dense matching

and dense-to-dense matching.

(1) Sparse-to-sparse matching

Sparse-to-sparse matching is based on sparse feature extraction from im-

ages. This method can be further divided into three types: detect-then-describe,

detect-and-describe, and describe-to-detect, according to the role of the detector

and descriptor in the learning process.

• The Detect-then-describe approach is usually two-stage. First, a keypoint

detection is performed then the feature descriptor are extracted around

proposed key points. A good local feature typically should be robust and

invariant against scale transformation, rotation, and viewpoint, changes.

At the start, handcrafted keypoint detectors (such as SIFT, Harris or SU-

SAN) used gradient and other information to detect key points. This is

feasible but can be considered too computationally intensive for use in

real-time applications. To accelerate the detection step, Rosten et al. [58]

proposed FAST which was one of the ﬁrst attempts to use machine learn-

ing to derive a corner keypoint detector. Further work extended FAST

by adding a descriptor [59], or orientation estimation [60]. Verdie et al.

[61] proposed a new regression-based approach to extract feature points

that are repeatable under drastic illumination changes. Lenc et al.

[62]

introduce a novel learning formulation for covariant detectors. They pro-

posed to cast detection as a regression problem, then derived a covariance

constraint that can be used to automatically learn. Zhang et al. [63] ex-

tend the covariant constraint proposed by [62] by deﬁning the concepts

of standard patch and canonical features, which makes the learning pro-

cess more robust and less sensitive to the initialization setting. Since it

is often unclear what points are ”interesting”, human labelling cannot be

used to ﬁnd a truly unbiased solution. Savinov et al.

[64] cast detec-

tion as an unsupervised formulation. They trained a neural network that

maps an object point to a single real-valued response and then ranked

21

points according to this response. DeTone et al.

[65] presented a point

tracking system powered by two deep convolutional neural networks Mag-

icPoint and MagicWarp. After that, MagicPoint was extended in [66] to

Superpoint. After detecting the key point, the next step is to extract the

descriptor on a sparse set of key points. In the beginning, most work fo-

cused on learning descriptors from image patches. Zagoruyko et al.

[67]

use Siamese CNN networks to learn discriminant patch representations

from a large set of known pairs of corresponding and non-corresponding

patches. Han et al. [68] proposed MatchNet as a deep convolutional net-

work that extracted features from patches and a network of three fully

connected layers that computed the similarity between the extracted fea-

tures. They converted the descriptor regression task into a classiﬁcation

problem under a cross-entropy loss. Simo-Serra et al.

[69] proposed a

strategy of aggressive mining of hard positives and negatives on multi-

view stereo (MVS) datasets. They also used a Siamese network architec-

ture that employed two CNNs with identical parameters to compare pairs

of patches and to treat the CNN outputs as patch descriptors. Besides

patch correspondence-based learning, the descriptor is usually trained by

a metric loss, such as the triplet loss or a contrastive loss. Balntas et al.

[70] proposed to utilize triplets of training samples, together with in-triplet

mining of hard negatives. In addition, they discussed the loss functions

when learning with triplets or pairs and investigated their characteristics.

L2-Net [71] focused only on the relative distance which makes positive

pairs become the nearest to each other for L2 distance. Mishchuk et al.

[72] proposed HardNet to minimize the distance between the matching de-

scriptor and the closest non-matching descriptor. They proved that their

proposed loss is better than complex regularization methods. Tian et al.

[73] proposed SOSNet with a novel regularization term, named Second Or-

der Similarity Regularization (SOSR). This not only forces the distances

between matching descriptors to decrease or distances between nonmatch-

ing ones to increase, but it also forces the distances between nonmatching

22

descriptors’ distances respectively to be equal. Wang et al. [74] proposed

a weakly-supervised framework CAPS that can learn feature descriptors

solely from relative camera poses between images. They translate relative

camera poses into epipolar constraints between image pairs and enforce

the predicted matches to obey this constraint.

• Detect-and-describe. Recently, some work implemented an end-to-end fea-

ture detection and descriptor pipeline. As opposed to patch-based neural

networks, Detect-and-describe approaches operate on full-sized images and

jointly compute interest point locations and associated descriptors in one

forward pass. Yi et al. [75] proposed LIFT, a full-featured point handling

pipeline, including detection, orientation estimation, and feature descrip-

tion. LF-Net [76] is an entire feature extraction pipeline. To train the

network end-to-end, they design a two-branch network and optimize by

conﬁning it to one branch, while preserving diﬀerentiability in the other.

SuperPoint [66], created a large dataset of pseudo-ground truth interest

point locations in real images, supervised by the interest point detec-

tor itself. They jointly trained a network called SuperPoint for interest

point detection and description. Other than only learning key points, a

descriptor R2D2 [77] is used to train a predictor of the local descriptor

discriminator. They argued that salient but discriminative regions can

harm performance. ASLFeat [78] is based on D2-Net [79]. They improved

the ability to model the local shape for stronger geometric invariance, and

the ability to localize key points, more accurately.

• Describe-to-detect methods refer to extracting descriptors before detecting

key points. Dusmanu et al. [79] proposed a method D2-Net that detects

key points on a dense feature map generated by CNN. By postponing the

detection to a later stage, the obtained key points are more stable. Similar

to D2D [80], they proposed a relative and an absolute saliency measure of

local deep feature maps along the spatial and depth dimensions to deﬁne

key points. Benbihi et al. [81], proposed a detection method DELF, valid

23

for any trained CNN where key points are regarded as the local maxima

of a saliency map computed as the feature gradient for the input image.

(2) Sparse-to-dense matching

In this paper, sparse-to-dense is deﬁned as matching a sparse set of local

features with a dense feature map extracted from the image. Germain et al. [82]

proposed an approach for robust and accurate outdoor visual localization. After

getting the sparse feature points in the retrieved reference image, they search

for the corresponding 2D locations in the query image exhaustively. Inspired

by this paper, they proposed S2DNet [83], a sparse-to-dense matching pipeline,

where they designed and trained a network to predict correspondence for query

points.

(3) Dense-to-dense matching

Dense-to-dense matching approaches get rid of the detection stage altogether

by ﬁnding mutual nearest neighbors in dense feature maps. Most deep feature

dense-to-dense matching methods have focused on learning dense descriptors

over the image. NCNet [84] trains a CNN to search in the 4D space of all

possible correspondences, with the use of 4D convolutions. Melekhov et al. [85]

proposed a novel approach called DGC-Net. They leverage the advantages of

optical ﬂow approaches which have recently achieved signiﬁcant progress. By

extending optical ﬂow to the case of large transformations, they can provide

dense and subpixel accurate estimates in a complex environment. Wiles et al.

[86] proposed a new approach to determining correspondences between image

pairs under large changes. They designed a model to learn a conditioned feature

and distinctiveness score which is then used to choose the best matches by an

attention mechanism. In InLoc [87], they collected a new dataset with reference

6DoF poses for large-scale indoor localization, and dense feature extraction.

Their approach is based on matching on a sequence of progressively stricter

veriﬁcation steps.

24

4.1.3. Summary

These matching methods here attempted to eﬃciently generate an accurate

correspondence between query and scene. This is then used to calculate camera

pose by applying a Perspective-n-Point (PnP) solver inside a RANSAC loop.

Therefore, the precision of the matching module largely determines the accuracy

of positioning.

4.2. Scene coordinate regression-based localization

In contrast to matching based methods that explicitly establish 2D-3D cor-

respondences via matching descriptors, scene coordinate regression-based local-

ization methods directly regress 3D scene coordinates from the query image.

Namely, either a random forest or a neural network is trained to directly pre-

dict 3D scene coordinates for the pixels. In this way, correspondences between

2D points in the image and 3D points in the scene can be obtained densely

without feature detection and description, and explicit matching.

In 2017, Rosten et al. [88] proposed a diﬀerentiable RANSAC by soft argmax

and probabilistic selection, called DSAC. For end-to-end learning, they put

DSAC into the camera localization pipeline. Their trainable localization pipeline

exceeds the state-of-the-art results. Such a common pipeline was then improved

via reprojection loss [89, 90, 91]. Cai et al. [92] use multi-view geometric con-

straints to enable unsupervised learning. Brachmann et al.

[93] proposed a

joint classiﬁcation regression forest which is trained to predict scene identiﬁers

and scene coordinates. In [94], scene coordinate regression is formulated as two

separate tasks of object instance recognition and local coordinate regression.

Applied to diﬀerent scenes without any retraining or adaptation, SANet [95]

proposed a method to extract a scene representation from some reference scene

images and 3D points, instead of encoding speciﬁc scene information in network

parameters.

However, most existing scene coordinate regression methods can only be

adopted on small-scale scenes. They have not yet proven their capacity to be

as eﬀective in large-scale scenes.

25

5. Regression-based pose estimation methods

Apart from structure feature-based pose estimation methods, this section

focuses on regression-based pose estimation methods, in which we divide this

area into absolute camera pose regression and relative camera pose regression

according to whether the process is in an end-to-end direct or is a two-stage

that integrates image retrieval or CNN process to get a reference image’s pose

and then gets the camera pose.

5.1. Absolute camera pose regression

Absolute camera pose regression aims to predict a reference image’s 6DoF

pose through a CNN by optimizing the weights of the network, which directly

output the position and orientation information from an image to the regres-

sor. According to the principle of whether the input of the network is a single

image, image sequence or video, the absolute camera pose regression is intro-

duced in three parts.

i.e., absolute pose regression through single monocular

image, absolute pose regression with image sequences auxiliary, and absolute

pose regression through video.

Following on from single monocular image or auxiliary learning, existing re-

search work on APR problems show improvements mainly through 1) replacing

the encoder network or adding some modules; 2) modifying networks loss func-

tion; 3) enhancing image data by using more images or adding constraints on

time, space, etc.

5.1.1. Absolute pose regression through single monocular image

PoseNet is the ﬁrst work that could directly regress the 6DoF pose from

single images. Methods based on PoseNet have a similar fashion, which can be

expressed as ”encoder, localizer and regressor”, which is shown in Figure 4.

5.1.1.1. Problem modelling.

In this section, we introduce the pose regression method through a single

monocular image. The whole pipeline is input – network – output, which could

26

Figure 4: An overview of absolute pose regression methods architecture

directly estimate the pose relating to the camera of the capturing image. An

RGB image Ic captured by camera C is the input of the CNN, after extracting

features and regressors, the displacement between C and origin point can be

expressed by a position vector x ∈ R3, and the orientation vector q ∈ R4

in quaternion form, after the orientation normalization into 3 dimensions, the

pose emerges into a pose vector p ∈ R6, which is shown as p = [x, q].

5.1.1.2. Methods.

(1) Fixed Euclidean loss parameters

Fig 1 shows the typical architecture of deep absolute pose regression, which

uses a single image as the input. The output is a global pose result including

position and orientation. This kind of method extracts high dimensional features

through single images, then outputs these features with pose is expressed in a

linear fashion as a 6-dimensional vector.

As mentioned, PoseNet [96] is the ﬁrst work to regress camera pose from

single RGB images by training convolutional neural networks (CNNs), which

does not rely on separate mechanisms or cross-frames/key frames to estimate

pose. PoseNet shows robustness against the SIFT-based SfM (Structure from

motion) method, the latter fails sharply after decreasing the training samples

to a certain threshold.

To advance PoseNet, methods that use a single image as the input to im-

prove the localization performance and modify the network, or update the loss

27

ImageEncoderFeaturemapRegressorfunction, have been proposed. Methods that use a ﬁxed loss all share the same

strategy. They learn location and orientation simultaneously using a stochastic

gradient descent using the following objective loss function:

l = ||ˆx − x||2 + β||ˆq −

q
||q||

||2

(5)

β is a scaling factor to balance the value from any position error and ori-

entation error. Euclidean loss attempts to learn the position and normalized

quaternion diﬀerence.

To improve the localization performance and to understand the model uncer-

tainty, a Bayesian CNN [97] with Bernoulli distributions has been proposed. The

main contribution of the Bayesian CNN is in extending PoseNet to a Bayesian

model which can determine the uncertainty of localization. To implement this,

dropout layers were added after the sub-net and ﬁnal output layer to get the

stochastic pose samples The evaluation showed that there is a strong correla-

tion between the uncertainty estimation and location error, so uncertainty can

be used to predict the location error. This improves PoseNet’s relocalization

accuracy for indoor and outdoor scenes.

As mentioned, PoseNet has a 2048-dimensional fully connected (FC) layer,

thus, this enables a Long-Short-Term-Memory (LSTM) layer model to reduce

the feature dimensionality and to improve location accuracy [98, 99]. Walch et

al.

[98] suggested making use of Long-Short Term Memory (LSTM) units on

the PoseNet FC output, which performs a structured dimensionality reduction

and chooses the most useful feature correlations for the task of pose estimation.

Four LSTM units are used in the up, down, left and right directions respectively.

This method outperforms PoseNet by almost 30% for the positional error and

55% for the orientation error. LSTM is also applied to temporally improve

localization accuracy using image sequences, which will be introduced in the

next part.

To further improve the accuracy of localization, an hourglass network was

proposed to add another part to encode the rich and comprehensive information

from coarse object structures and a second part to recover the ﬁne-grained object

28

details.

Sharing the same thoughts of leveraging machine learning for camera local-

ization, SVS PoseNet [100] proposed a new network-based upon a classiﬁcation

network while using the same parameters rather than using hyperparameters

optimization for each training dataset, which achieves a better performance for

an outdoor dataset, e.g., Cambridge dataset [96]).

The orientation expression in PoseNet is not unique, while the training strat-

egy for orientation and translation is separately optimised. Furthermore, it’s a

higher time cost to compute sparse frames. To tackle these problems, Branch-

Net created a new two-branch network that simultaneously learns the orienta-

tion and translation representations to eﬀectively reduce the sparsity of sampled

poses.

The methods above, enhance the origin architecture using the following ex-

tensions. The Hourglass PoseNet [101] overall network consists of three com-

ponents named encoder, decoder and regressor. It uses a modiﬁed ResNet34

as encoder-decoder, which can be considered as the encoder part in the whole

pipeline (Fig 1). SVS PoseNet uses VGG16 with two additional FC layers for

independent orientation and position prediction. SVS PoseNet also proposed

data augmentation in 3D space through synthetic viewpoint generation. While

BranchNet [102] uses a diﬀerent fashion from PoseNet. That is, orientation

and translation vectors are predicted by two diﬀerent branches after the 5th

Inception module. All of this work uses the same loss function as PoseNet.

However, problems arise when setting the balancing factor β. Because the

loss function uses a joint loss, which needs careful tuning especially in a distinct

scene. Otherwise, the uncertainty of network output will increase greatly. To

address this problem, work that uses learnable pose loss function parameters

has emerged.

(2) Learnable pose loss parameters

To enhance the localization performance, Geometric PoseNet [103] proposed

learnable weights pose loss to balance the performance and improve the stability.

29

Thus, compared to PoseNet, this method can keep the scalability and robustness

while it doesn’t need to adjust the ﬁxed balance factor hyperparameters in the

loss function.

To learn an object’s position and orientation information from an image,

the ﬁxed Euclidean loss applies balanced hyperparameters, which independently

learns these two components, but it is costly to learn the weights of these. By

learning the estimate of the homoscedastic task uncertainty [97] during training

to represent uncertainty regularization terms and the residual regressions to

represent the regression performance, the loss could be mutually constrained.

lσ(I) = lx(I)ˆσ−2

x + logˆσ2 + lq(I)ˆσ−2

q + logˆσ2
q

Replacing ˆS := logˆσ2, the ﬁnal form of the learnable loss function is:

lσ(I) = lx(I)exp(−ˆsx) + ˆsx + lq(I)exp(−ˆsq) + ˆsq

(6)

(7)

Furthermore, diﬀerent methods can be used to apply a learnable geometric

loss [103] function to obtain geometry constraints while adding other modules

or functionality as follows. AtLoc [99] adds an attention module before deter-

mining the regression coordinates to force the network to concentrate on the

main part of the input images, which is a unique, static and stable area. In ad-

dition, AtLoc utilities ResNet34 as the encoder network which when pre-trained

on the ImageNet dataset, ﬁnally regresses the 2048-dimensional full connected

(FC) layer of PoseNet, like AtLoc, AdPR adds a discriminator network and

adversarial learning. This not only regresses the pose but could also reﬁne the

pose. When extracting features, AdPR [104] applies the ResNet-18 Network, as

it can achieve the best performance when compared with VGG16 and AlexNet.

APANet [105] also employs an adversarial network to generate related images to

the input image to better estimate the camera pose. PVL [106] adopted a prior-

guided dropout mask to avoid the inﬂuence of uncertainty of dynamic objects in

dynamic environments. A dropout module is added before the feature extrac-

tor encoder to output multiple uncertainty possibilities, which could improve

the pose robustness under challenging conditions, e.g., illimitation, viewpoint

30

changes. After extraction, the self-attention module is added to reweight the

feature map.

Furthermore, diﬀerent methods can be used to apply a learnable geometric

loss [103] function to obtain geometry constraints while adding other modules

or functionality as follows. AtLoc [99] adds an attention module before deter-

mining the regression coordinates to force the network to concentrate on the

main part of the input images, which is a unique, static and stable area. In ad-

dition, AtLoc utilities ResNet34 as the encoder network which when pre-trained

on the ImageNet dataset, ﬁnally regresses the 2048-dimensional full connected

(FC) layer of PoseNet, like AtLoc, AdPR adds a discriminator network and

adversarial learning. This not only regresses the pose but could also reﬁne the

pose. When extracting features, AdPR [104] applies the ResNet-18 Network, as

it can achieve the best performance when compared with VGG16 and AlexNet.

APANet [105] also employs an adversarial network to generate related images to

the input image to better estimate the camera pose. PVL [106] adopted a prior-

guided dropout mask to avoid the inﬂuence of uncertainty of dynamic objects in

dynamic environments. A dropout module is added before the feature extrac-

tor encoder to output multiple uncertainty possibilities, which could improve

the pose robustness under challenging conditions, e.g., illimitation, viewpoint

changes. After extraction, the self-attention module is added to reweight the

feature map.

Another method to improve the localization performance is to synthetically

generate training data, SPP-Net [107] shows a novel DNN architecture based

on Spatial Pyramid max-pooling units, which also share the same loss function

as geo.PoseNet [103].

(3) Other loss methods to enhance localization

Neither using ﬁxed-parameter loss nor using learnable loss functions, Geo-

PoseNet [108] and GPoseNet [103] consider other modules to enhance local-

ization, GeoPoseNet proposed the reprojection loss to learn the mean of all

residuals from points gi ∈ g(cid:48), which describes the reprojection error of scene

31

(cid:80)

geometry. lg(I) = 1
g(cid:48)
normalization operation, π maps a 3-D point g(cid:48) to 2-D image coordinates (cid:0)u
i.e., π(x, q, g) (cid:55)→ (cid:0)u

gi∈g(cid:48) ||π(x, q, gi) − π(ˆx, ˆq, ˆgi)||γ, where γ represents the
(cid:1),
(cid:1). The reprojection loss transfers the jointly learnable loss

v

v

to an image coordinates diﬀerence, which could vary the weighting between

position and orientation, according to the diﬀerent scenes during the model

training.

GPoseNet [108] builds a novel model by adding 2 Stochastic Variational

Inference Gaussian Process Regressions (SVI GPs) regressors after the fully

connected layer to learn the probability distribution of the output pose and

to reduce the hyperparameter usage. The loss function of GPoseNet, which

combines the SVI GPs loss using variational lower bound of two log marginal

likelihoods Lsvi and CNN loss with the hyperparameter βnt and βnq of PoseNet,

in which the hyperparameters βgt and βgq are set to be equal in the experiments

[108], is as follows:

l = βgtlsvi(st, St, Zt) + βgq lsvi(mq, Sq, Zq) + βnt||ˆt − t||2 + βnq ||ˆq − q||2

(8)

5.1.1.3. Critical thinking.

(1) How do the methods change the networks?

The absolute pose regression methods using a single image, improve the

algorithm performance by changing the network architecture of the traditional

encoder-decoder-localizer module or adding some external parts to ﬁlter away

information and add temporal information to better localize the camera pose.

Table 2 shows the network architectures of APR methods that use a single

image in our survey. From which we can see that the modiﬁed GoogLeNet,

ResNet34, ResNet-18 and VGG16 pre-trained with classiﬁcation task (such as

on ImageNet or Places datasets) are popular choices as the encoder of the APR

network. A dropout layer is added in some methods (Bayesian and PVL) to

help compute probability distribution or prior guided mask, while an LSTM

module is applied to oﬀer temporal information auxiliary. An attention module

is also widely utilized to force the network to focus on geometrically robust

32

Table 2: Network architecture comparison of APR methods through single im-

ages

Loss Type Method

Encoder

Localizer

Posenet [96]

GoogLeNet (3 softmax

1FC

layer + fc layers→3 re-

gressor layer)

Fixed

loss

Bayesian

PoseNet

GoogLeNet

(add

1FC+dropout

[97]

dropout after 9th Icp)

LSTM [101]

GoogLeNet

4LSTM+1FC

Hourglass PoseNet

ResNet34

Encoder-

3FC

[101]

Decoder

SVS PoseNet [100]

VGG16 (conv layers)

3FC

BranchNet [102]

GoogLeNet (truncated

2 x [GoogLeNet (6th-9th

after the 5th Icp)

Icp) + 1 FC]

Geo.PoseNet [103]

GoogLeNet

1FC+1normalisation

layer for orientation to

unit length

Learnable

Loss

AtLoc [99]

ResNet-34

AdPR [104]

ResNet-18

1FC

1FC

PVL [106]

Prior Guided Dropout

Composite Self-Attention

+ Resnet34

APANet [105]

ResNet-34

+ 1FC

1FC

SPPNet [107]

3× (4 layers of 1×1

3FC

convolutions)

+

Spatial Pyramid max-

pooling units

Other

Loss

Geo.PoseNet (repro-

GoogLeNet

1FC+1normalisation

jection error

loss)

layer for orientation to

[103]

unit length

GPoseNet [108]

GoogLeNet

1FC + 2

SVI GP

regressors

33

objects. The localizer part in all the methods has at least a regressor to output

the translation and orientation information, in which the 4-D quaternion is

normalized to a unit length.

(2) How do methods change the loss function?

Table 3: Loss function and publication information comparison of APR methods

thorough single image

Loss

Type

Fixed

Loss

Method

Year-Pub.-Cited

Loss function

PoseNet [96]

2015-ICCV-1146

Bayesian PoseNet [97]

2016-ICRA-330

LSTM PoseNet [101]

2017-ICCV-273

Hourglass

PoseNet

2017-ICCVW-80

[101]

SVS PoseNet [100]

2017-IROS-56

BranchNet [102]

2017-ICRA-62

Geo.PoseNet [103]

2017-CVPR-384

AtLoc [99]

2019-AAAI-6

l = ||ˆx − x||2 + β||ˆq − q

||q|| ||2

Learnable

AdPR [104]

2019-ICCVW-5

lσ(I) = lx(I)exp(−ˆsx) + ˆsx

Loss

PVL [106]

2019-ICCV-9

+lq(I)exp(−ˆsq) + ˆsq

Other

Loss

APANet [105]

2020-ECCVW-0

SPPNet [107]

2018-BMVC-6

Geo.PoseNet

(repro-

2017-CVPR-384

lg(I) = 1
g(cid:48)

(cid:80)

gi∈g(cid:48) ||π(x, q, gi) −

jection

error

loss)

[103]

GPoseNet [108]

2018-BMVC-18

π(ˆx, ˆq, ˆgi)||γ

=

l
βgtlsvi(st, St, Zt) +
βgq lsvi(mq, Sq, Zq) + βnt||ˆt −

t||2 + βnq ||ˆq − q||2

The research of applying the loss function tends to be automatic, hyperparameter-

free and more informative to reduce the use of empirical ﬁxed parameters. A

ﬁxed loss function computes the translation and orientation sum using a balance

34

factor to balance diﬀerent weighted items, which requires a long time to optimize

the loss of the training data. Later a learnable loss [103] was proposed by adding

homoscedastic uncertainty to automatically balance the translation and orien-

tation loss, which avoids using hyperparameters and surpasses the performance

of the ﬁxed loss methods. Apart from ﬁxed loss and learnable loss methods,

other methods proposed the use of reprojection error loss and GPoseNet loss

to add other information formats. e.g., a probability distribution of the output

pose, to improve the loss function. Table 3 compares the loss and publication

information of APR methods.

5.1.2. Absolute pose regression through image sequences auxiliary

Another method to regress absolute pose is for auxiliary learners to use image

sequences. Auxiliary learning refers to the combination of using absolute pose

regression and auxiliary task constraints (e.g., visual odometry). Loss functions

in auxiliary learning methods usually consist of APR loss and auxiliary tasks

loss. All the above-mentioned methods can be used to get the absolute camera

pose. They may even use relative pose regression loss in the pipeline.

5.1.2.1. Problem modelling.

Traditional structure-based methods still have advantages over deep neu-

ral networks. Therefore, absolute pose regression with images sequences using

auxiliary learning, has been proposed. Unlike methods using single images, aux-

iliary learning through image pairs, typically learn the absolute pose by ﬁrstly

estimating the relative pose with which auxiliary constraints. This could involve

globally consistent pose predictions to improve the localization performance, i.e.,

to reduce positioning error and to improve positioning robustness.

5.1.2.2. Methods.

MapNet [109] proposed to add an additional loss term from image pairs as a

geometric constraint, which could signiﬁcantly enhance the localization ability.

Other methods share the same intuition as MapNet using auxiliary learning,

which minimizes the combination of the per-image absolute pose loss and the

35

relative pose loss between image pairs with a weight coeﬃcient factor α. The

loss function is shown as:

l(Itotal) = l(Ii) + α

(cid:88)

i(cid:54)=j

loss(Iij)

(9)

Where loss(Iij) means the relative camera pose pi and pj between image

pairs Ii and Ij, which is computed by the learnable loss function (equation (7)).

In addition, MapNet also transforms the quaternion value to the logarithm

of the quaternion, which presents a 3DoF rotation with 3 dimensions that is

not over-parameterized. logq is deﬁned below, where u and v each represent the

real and imaginary part of a unit quaternion:




logq =

v

||v|| cos−1u,

if ||v|| (cid:54)= 0



0,

otherwise

(10)

Xue et al. [110] follow a similar notion to regress global camera pose through

spatial-temporal constraints, in which local features enhance global localization,

named Local Supports Global (LSG). Furthermore, LSG proposed the use of

a content-augmented valuation to estimate pose uncertainty and motion-based

reﬁnement, to optimize pose prediction via motion constraints. LSG employs a

global pose loss Lg from absolute regression, visual odometry loss Lvo, geometric

constraint and joint loss Ljoint motion constraint that together optimize the pose

regression as follows.

ltotal = lg + lvo + ljoint

(11)

Where the motion-based joint loss is coupled as the global and local pose

from the local window:

ljoint =

N
(cid:88)

i=1

D(Pi+1, P vo

i+1, Pi)

(12)

VlocNet [111] also simultaneously learns the visual odometry as an auxiliary

task to regress the global pose with two sub-networks. Geometric consistency

loss is adapted to minimize the pose error, which is deﬁned as:

l(Itotal) = (Iix + Iijx )exp(−ˆsx) + (Iiq + I q

ij)exp(−ˆsq) + ˆsq

(13)

36

VlocNet++ [112] introduces semantic knowledge to pose regression, which

fuses the geometric-temporal information with semantic features together. The

loss of VlocNet++ combines global pose regression, visual odometry loss and

the cross-entropy loss for semantic segmentation loss together, with three factors

ˆsloc, ˆsvo, ˆsseg to balance the three terms.

l(Itotal) = llocexp(−ˆsloc)+ ˆsloc +lvoexp(−ˆsvo)+ ˆsvo +lsegexp(−ˆsseg)+ ˆsseg (14)

As an extension of AtLoc, AtLocPlus [99] also incorporates temporal con-

straints to simultaneously learn the absolute pose loss and the relative pose loss,

which leads to a better performance than AtLoc using a single image input. At-

LocPlus shares the same loss function with MapNet.

DGRNet [113] proposed a novel architecture with relative pose regression

sub-network RCNN1 and global pose regression sub-network RCNN2 and fully

connected fusion layer FCFL to extract features through images. Cross trans-

formation constraints (CTC) and Mean square error (MSE) are applied to the

loss function to improve the regression performance. DGRNet jointly uses the
global and relative loss with the CTC functions ˆli and the ground truth ˆPi in

frame i as follows:

w = argmin

w

1
N

5.1.2.3. Critical thinking.

N

6
(cid:88)

i=1

k=0

(li

k) +

4
(cid:88)

j=0

||P ij − ˆP ij||2
2

(15)

(1) How methods add constraints multi-task?

With image sequences and adding geometric-aware temporal constraints or

other constraints, methods can not only obtain the localization result but also

obtain visual odometry information. Furthermore, DGRNet could also get the

semantic segmentation results through the network. Table 4 shows a general

comparison of what output and constraints image sequences auxiliary-based

APR methods apply.

(2) How do the methods improve the network and loss function?

37

Table 4: Constraints comparison with multi-tasks of APR methods thorough

image sequences

Methods

Output

Geometric

Other

Localization Visual

Semantic

odometry

segmen-

tation

constraints

-aware

temporal

constraints

MapNet [109]

LSG [110]

VlocNet [111]

VlocNet++

[112]

AtLocPlus [99]

DGRNet [113]

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

/

Motion-based

constraints

/

Semantic

constraints

/

/

Table 5: Loss function and publication information comparison of APR methods

thorough image sequences

Methods

Year-pub-cited

Encoder

Loss Function

MapNet [109]

2018-CVPR-155

ResNet34+global

average pooling

l(Itotal) = l(Ii)
+α (cid:80)

i(cid:54)=j loss(Iij)

LSG [110]

2019-ICCV-11

ResNet34+ResBlock

ltotal = lg + lvo + ljoint

VlocNet [111]

2018-ICRA-113

VlocNet++ [112]

2018- RA-L-105

ResNet50

(ReLUs→ELUs)

ResNet50

(ReLUs→ELUs)

+ global

average pooling

AtLocPlus [99]

2019-AAAI-6

ResNet-34

DGRNet [113]

2019- PRICAI-16

(Res1 to Res4)

ResNet-50

(BN+ ELUs)

38

l(Itotal) = (Iix + Iijx )exp(−ˆsx)
+(Iiq + I q

ij)exp(−ˆsq) + ˆsq

l(Itotal) = llocexp(−ˆsloc) + ˆsloc

+lvoexp(−ˆsvo) + ˆsvo

+lsegexp(−ˆsseg) + ˆsseg

l(Itotal) = l(Ii)
+α (cid:80)

i(cid:54)=j loss(Iij)

w = argmin

w

1
N

N
i=1

(cid:80)6

k=0(li
k)

+ (cid:80)4

j=0 ||P ij − ˆP ij||2

2

ResNet-34 and ResNet-50 with the modiﬁcation are widely used to extract

features in an image sequence regression network. MapNet, VlocNet and AtLoc-

Plus utilize the joint absolute and relative pose loss to improve the regression.

LSG applies a motion-based constrain to the loss function while VlocNet++

adds the semantic constraint into the loss function. DGRNet combines both

CTC and MSE in the loss computation. Table 5 generally lists the publica-

tion information, neural network encoder and loss function of image sequences

auxiliary-based APR methods.

5.1.3. Absolute pose regression through videos

Without using single images or image pairs to regress camera pose, video

clips could be used to add a temporal smoothness constraint to pose regression.

5.1.3.1. Problem modelling.

Videos and other sensor data can be easily accessed by mobile devices.

Videos can be synchronised using temporal information to other input data

such as visual odometry, Inertial Measurement Unit (IMU) sensors such as ac-

celerometer and gyroscope and GNSS data, by aligning timestamps. Sharing a

similar pipeline to single image-based and image sequence-based ARP methods,

video-based APR methods also regress the translation and orientation through

the CNN feature extractor and localizer regressor, which will also contain other

auxiliary information as with videos.

5.1.3.2. Methods.

VidLoc [114] proposed a CNN-RNN based model to regress camera pose

which could smooth the pose estimation from image or video input. The network

is formed by using GoogLeNet Inception [115] without using fully connected

layers to extract image features, and a bidirectional LSTM module to model

temporal information with memory cells and several gates. Adopting a LSTM

network with a bidirectional model could use two hidden states to process this,

forwards and backwards. This could also be concatenated with a single hidden

state to get the camera pose. The network loss of VidLoc is computed by a

39

weighted sum of translation and orientation error from the output of LSTM as

follows.

l =

T
(cid:88)

t=1

α1||xt − ˆxt|| + α2||qt − ˆqt||

(16)

where γt = [xt, qt] and ˆγt = [ˆxt, ˆqt] separately represent the ground truth

and prediction value for the camera pose translation and orientation values.

MapNet+ [109] and MapNet+PGO [109] share the same network architec-

ture with MapNet that extracts features through ResNet34 and uses a global

average pooling layer. Not only using the absolute pose loss, VidLoc, the visual

odometry loss is also computed to improve estimation quality in MapNet. The

method also integrates IMU and GNSS data to help improve pose regression.

This fuses the labeled data and unlabeled data from VO or sensors for self-

supervised learning and demonstrates a better performance under challenging

conditions, e.g., appearance changes.

l = llabelled data + lunlabelled data

(17)

Where the unlabeled data loss could be computed through combining relative

camera pose vij and the visual odometry ˆvij, or through other sensors, such as

IMU and GNSS.

MapNet+PGO [109] could further improve the performance whilst minimiz-

ing the computation cost by using pose graph optimization (PGO) to fuse the

pose from MapNet+ and the visual odometry.

lP GO(p0
i

T
i=1) =

T
(cid:88)

i=1

¯h(po

i , pi) +

T
(cid:88)

i,i=1,i(cid:54)=j

¯h(vo

ij, ˆvij)

(18)

5.1.3.3. Critical thinking.

VidLoc, MapNet and MapNet+PGO use videos as input, while some of these

fuse unlabeled data to help improve supervised learning. VidLoc adds a bidirec-

tional RNN to regress camera 6DoF pose whilst outputting the probabilistic of

pose estimation. MapNet+ and MapNet+PGO mainly utilize visual odometry

into the loss function to optimize the regression performance. Table 6 gives

a general comparison of video-based APR methods, including publication, loss

function, neural network main architectures, etc.

40

Table 6: Loss function and publication information comparison of APR methods

thorough video

Type

VidLoc [114]

MapNet+ [109] MapNet+PGO [109]

Publication

2017-CVPR

2018-CVPR

Cited

Input

Fusion ability

Loss function

163

Videos

/

155

unlabeled videos+vo+imu+GNSS

(cid:88)

(cid:88)

l = (cid:80)T

t=1 α1||xt − ˆxt||

+α2||qt − ˆqt||

l = llabelled data

+lunlabelled data

lP GO(p0
i
(cid:80)T
¯h(po

T
i=1) =
i , pi)

i=1
+ (cid:80)T
¯h(vo

i,i=1,i(cid:54)=j
ij, ˆvij)

Feature extraction

GoogLeNet Inception

ResNet34 + global average pooling

Regressor

Bidirectional RNN+1 FC

1FC

5.1.4. Summary

In this part, we discussed the work of 2D-to-2D absolute pose (consisting of

localization and orientation) regression using deep neural networks entirely and

using no image queries. Recent work shows that the APR methods can suﬀer

from less accuracy, and from overﬁtting, compared to structure-based methods.

These could be used in scene-speciﬁc environments, with the emergence of rela-

tive pose regression-based localization methods. The training process could be

generally used in multiple scenes [2].

5.2. Relative camera pose regression

A direct absolute camera pose regression model learns the mapping from

images of objects’ pixels to camera poses, which is decided by the coordinate

system that the speciﬁc scenes are in. Thus, cross-scene learning brings a coor-

dinates transfer that is bounded and that delivers learnable physical geometric

knowledge. In contrast to scene-speciﬁc absolute pose regression, relative cam-

41

era pose regression methods compute a reference image’s relative pose and are

trained on general multi-unseen scenes to increase the scalability in an end-to-

end manner.

5.2.1. Relative camera pose regression through explicit retrieval

Relative camera pose regression could be calculated through the prior image

retrieval process which computes the most similar image relative to the query

image in a database and then predicts the relative pose between them and ﬁnally

gets the absolute pose of the query image.

5.2.1.1. Problem modelling.

Given an image I a

c captured by camera c, its relative nearest similar image

I b
c could be estimated through the image retrieval method in a database. After
c and I b
getting the ground truth pose pb of I b
c ,
the absolute pose pa of I a
c could be deﬁned by a mathematical transformation.

c and relative pose pa→b between I a

5.2.1.2. Methods.

NNnet [116] ﬁrst proposed an image retrieval based relative pose regression

method. The input of the method is a query image and an image database

including ground truth poses. A set of image pairs is utilized to regress the

relative pose through a Siamese network with two modiﬁed ResNet34 branches

with a ﬁxed loss function. The nearest neighbor image to the query image could

be computed through a feature extractor formed by the network branch, then

the relative pose and neighbor’s ground truth pose could be fused to get the

absolute pose of the query image.

RelocNet [117] furtherly modiﬁes NNnet [116] with continuous metric learn-

ing to learn global image features with a camera frustum to improve the result,

while a geometric relative pose loss is also applied. Relative pose loss learns

the diﬀerential pose between two pose matrices using a representation of matrix

for rotation and translation. The training loss in which, frustum loss learns the

image pair overlaps is deﬁned as follows.

l = αlSE(3) + βlf rustum

(19)

42

To tackle the bottleneck in previous retrieval-based relative regression meth-

ods whose performance is limited because they use the same features for retrieval

and regression modules, CamNet [118] proposed a novel pipeline split into three

steps. Coarse-retrieval, ﬁne-retrieval, relative pose regression is used, which is

based on a Siamese architecture with three branches for each of the three steps.

This coarse-to-ﬁne framework improves regression accuracy and scalability. The

loss function of CamNet is based on RelocNet, which is shown as follows.

l = lf rustum + langle + ltriplet + lP F R + lP RP

(20)

Zhou et al.

[119] analyze the previous image retrieval based relative pose

regression method and propose a novel framework with essential matrices and

modiﬁed RANSAC for computing the absolute pose. A Siamese modiﬁed ResNet34

network with a ﬁxed matching layer (EssNet) and a Neighborhood Consensus

matching layer (NC-EssNet) is learned to produce a matching score map for a

further regression, essential matrix. The loss function optimizes the Euclidean

distance between the essential matrix with two 9D vectors (where a 3 × 3 matrix

becomes one 9D vector)

less(E∗, E) = ||e − e∗||2

(21)

5.2.2. Relative camera pose regression through implicit CNN

To avoid large collection for database and long test time consuming, some

methods try to regress relative camera pose through an implicit neural network.

Relative NN [120] proposed an end-to-end method to regress the relative pose

between two cameras with two images as input. A Siamese Hybrid-CNN with

a pre-trained AlexNet network consisting of two branches is used for regression

with the ﬁxed Euclidean loss, which has a good performance on the Technical

University of Denmark Robot Image Dataset (DTU dataset) [121].

AnchorNet [122] addresses the localization problem by deﬁning anchor points

as the visible landmark to learn the query image’s relative anchors and its oﬀset.

The multi-task model includes classifying the query image to which speciﬁc

anchor points. and ﬁnding the oﬀsets compared to the classiﬁed anchor point,

43

which forms the loss function. ˆC, X, and Y represent the classiﬁcation output

and the ground truth oﬀsets.

(cid:88)

l =

i

[(Xi − ˆXi)2 + (Yi − ˆYi)2] ˆC i

(22)

5.2.3. Summary

Table 7: Loss function and publication information comparison of RPR methods

Type

Method

Year-Pub-Cited

Loss function

NNnet [116]

2017-ICCVW-68

l = ||ˆx − x||2 + β||ˆq − q

||q|| ||2

Through

retrieval

RelocNet [117]

2018-ECCV-70

l = αlSE(3) + βlf rustum

Camnet [118]

2019-ICCV-22

l = lf rustum + langle +

To learn or not

to learn [119]

2020-ICRA-16

less(E∗, E) = ||e − e∗||2

ltriplet + lP F R + lP RP

Through

CNN

Relative NN [120]

2017-ACIVS-108

AnchorNet [122]

2018-BMVC-23

||q|| ||2
[(Xi − ˆXi)2 + (Yi −

l = ||ˆx − x||2 + β||ˆq − q
l = (cid:80)
i
ˆYi)2] ˆC i

To regress relative pose, retrieval-based methods utilize a multi-stage strat-

egy to ﬁnally get the absolute pose with the retrieval step as fundamental to the

process. CNN-based methods oﬀer another way to regress relative pose implic-

itly within the network. Table 7 summarizes the publication information and

loss function of relative camera pose regression-based methods.

6. Camera pose estimation comparisons

We reviewed structure feature-based and regression-based pose estimation

methods in section 4 and section 5, in this section, we systematically compare

the performance of the datasets that appear in these methods, the quantitative

and qualitative results, and the real-world applicability of these methods.

44

6.1. Comparison of datasets

Table 8: An overview of some popular camera localization datasets

Sc

en

es

Train

Test

ima

ges

ima

ges

Ar

ea

Scale

Imagery

RBG-D

Room

sensor

7

26000

17000

/

(Kinect)

Mobile

Street

phone

6

8380

4841

/

Room

cular

1

875

220

Street

camera

Stereo

&mon

ocular

camera

Mono

Small

town

Street

Small

town

Street

camera

Inter

net

images

VMX-

CS6

camera

system

Mobile

phone

camera

2 cam

eras

Panor

11

20862

11934

/

1

6044

800

/

28

7481

7518

/

2

3047

369

/

10

7159

75335

/

55

75

m2

18

5.8

m2

Room

amic

5

9972

356

images

Attri

butes

7Sce

nes

[96]

Camb

ridge

[96]

Oxfo

rd

Robot

Car

[123]

TUM

LSI

[98]

Dubr

ovnik

6K

[124]

Apo

llo

Scape

[125]

Aac

hen

[11]

CMU

[11]

Aﬃ.

Year

Cit

es

Plat

Publi

form

cation

Micr

osoft

Camb

ridge

Oxfo

rd

2015

1531 Hand

ICCV

2015

1531 Hand

ICCV

2016

620

Vehi

cle

IJRR

TUM 2017

296

NavV

is

M3

ICCV

Cor

nell

2010

455

Hand ECCV

Baidu

2018

236

Aach

en

2018

312

CMU 2018

312

Vehi

cle

Vehi

cle

Vehi

cle

CVPR

CVPR

CVPR

Tokyo

InLoc

Techn

[87]

ology

et al.

2018

136

Hand CVPR

Envi

ron

ment

Ind

oor

Hist

oric

city

Urb

an

Ind

oor

Hist

oric

city

Out

door

Hist

oric

city

Urb

an

Ind

oor

45

Table 9: Speciﬁc scenes information for 7Scenes and Cambridge datasets

Dataset

Scene

Spatial extent(m)

Area

Train

Test

or volume

frames

frames

All

chess

ﬁre

head

oﬃce

4x3m

3x2x1m

2.5x1x1m

2x0.5x1m

2.5x2x1.5m

pumpkin

2.5x2x1m

kitchen

4x3x1.5m

7Scenes

[96]

/

6m2

2.5m2

1m2

7.5m2

5m2

18m2

7.5m2

26000

17000

4000

2000

2000

2000

1000

1000

6000

4000

4000

2000

7000

5000

2000

1000

2.5x2x1.5m

stairs

All

100x500m

/

8380

4841

Cambridge

[96]

great court

/

k.college

140x40m

8000m3

5600m3

1532

1220

760

343

street

500x100m

50000m3

3015

2923

old hospital

40x40m

shop facade

35x25m

2000m3

875m3

895

231

st M.Church

80x60m

4800m3

1487

182

103

530

Large scale, multi-distribution, datasets that cover diﬀerent collection plat-

forms, environments, and imagery on challenging scenes, e.g., illimitation view-

point, or appearance changes, are critical for evaluating advanced camera local-

ization algorithms. Table 8 summarizes the common datasets used for camera

localization tasks, including 7Scenes, Cambridge, TUM LSI, etc. Table 9 mainly

introduces the two most important datasets of 7Scenes and Cambridge, which

is popular for use in indoor and outdoor environment camera pose tests respec-

tively.

46

6.2. Comparison of published results on common benchmarks

Table 10: A summary of published results of structure-based methods on the

7Scenes dataset

Method

Chess

Fire

Head

Oﬃce

Pumpkin Kitchen Stairs

ScoRe

0.03m,

0.05m,

0.06m,

0.04m,

0.04m,

0.04m,

0.32m,

Forest [96]

0.66°

1.50°

5.50°

0.78°

0.68°

0.76°

1.32°

[89]

[90]

[91]

0.02m,

0.02m,

0.01m,

0.03m,

0.04m,

0.04m,

0.09m,

0.5°

0.9°

0.8°

0.7°

1.1°

1.1°

2.6°

0.02m,

0.03m,

0.02m,

0.03m,

0.04m,

0.04m,

0.25m,

0.6°

1.0°

1.1°

0.8°

1.1°

1.2°

4.5°

0.19m,

0.19m,

0.11m,

0.26m,

0.42m,

0.30m,

0.41m,

1.11°

1.24°

1.82°

1.18°

1.41°

1.70°

1.42°

rgb+3d

0.18m,

0.19m,

0.22m,

0.25m,

0.39m,

0.38m,

0.29m,

model [91]

1.10°

1.24°

1.82°

1.15°

1.34°

1.68°

1.16°

rgb-d [91]

[92]

SANet [95]

0.10m,

0.11m,

0.10m,

0.12m,

0.20m,

0.21m,

0.26m,

1.03°

1.05°

1.88°

1.03°

1.17°

1.41°

1.15°

0.02m,

0.02m,

0.04m,

0.03m,

0.04m,

0.04m,

0.18m,

0.8°

1.0°

2.7°

0.8°

1.1°

1.1°

3.9°

0.03m,

0.03m,

0.02m,

0.03m,

0.05m,

0.04m,

0.16m,

0.88°

1.08°

1.48°

1.00°

1.32°

1.40°

4.59°

NetVlad+

0.03m,

0.03m,

0.02m,

0.03m,

0.05m,

0.04m,

0.09m,

DensePE [87]

NetVlad+

SparsePE [87]

1.05°

4m,

1.83°

1.06°

1m,

1.55°

1.06°

2m,

1.65°

1.05°

5m,

1.49°

1.55°

7m,

1.87°

1.31°

5m,

1.61°

2.47°

12m,

3.41°

Table 11: A summary of published results of regression-based methods on the

7Scenes dataset

Methods

Chess

Fire

Head

Oﬃce

Pump

Kit

kin

chen

Stairs

47

PoseNet

[96]

Dense

0.32m,

0.47m,

0.29m,

0.48m,

0.47m,

0.59m,

0.47m,

8.12◦

14.4◦

12.0◦

7.68◦

8.42◦

8.64◦

13.8◦

0.32m,

0.47m,

0.30m,

0.48m,

0.49m,

0.58m,

0.48m,

PoseNet [96]

6.60◦

14.0◦

12.2◦

7.24◦

8.12◦

8.34◦

13.1◦

Bayesian

0.37m,

0.43m,

0.31m,

0.48m,

0.61m,

0.58m,

0.48m,

PoseNet [97]

7.24◦

13.7◦

12.0◦

8.04◦

7.08◦

7.54◦

13.1◦

LSTM

0.24 m,

0.34 m,

0.21 m,

0.30 m,

0.33 m,

0.37 m,

0.40 m,

PoseNet [101]

5.77◦

11.9 ◦

13.7◦

8.08◦

7.00◦

8.83◦

13.7 ◦

Hourglass

0.15m,

0.27m,

0.19m,

0.21m,

0.25m,

0.27m,

0.29m,

PoseNet [101]

6.17◦

10.84◦

11.63◦

8.48◦

7.01◦

10.15◦

12.46◦

BranchNet

0.18m,

0.34m,

0.20m,

0.30m,

0.27m,

0.33m,

0.38m,

[102]

5.17◦

8.99◦

14.15◦

7.05◦

5.10◦

7.40◦

10.26◦

Geo.PoseNet

0.14m,

0.27m,

0.18m,

0.20m,

0.25m,

0.24m,

0.37m,

[103]

4.50◦

11.8◦

12.1◦

5.77◦

4.82◦

5.52◦

10.6◦

AtLoc [99]

AdPR [104]

0.10m,

0.25m,

0.16m,

0.17m,

0.21m,

0.23m,

0.26m,

4.07◦

11.4◦

11.8◦

5.34◦

4.37◦

5.42◦

10.5◦

0.12m,

0.27m,

0.16m,

0.19m,

0.21m,

0.25m,

0.28m,

4.8◦

11.6◦

12.4◦

6.8◦

5.2◦

6.0◦

8.4◦

APANet

N/A,

0.21m,

0.15m,

0.15m,

0.19m,

0.16m,

0.16m,

[105]

N/A

9.72◦

9.35◦

6.69◦

5.87◦

5.13◦

11.77◦

SPPNet

0.12m,

0.22m,

0.11m,

0.16m,

0.21m,

0.21m,

0.22m,

[107]

4.42◦

8.84◦

8.33◦

4.99◦

4.89◦

4.76◦

7.17◦

Geo.PoseNet

(reprojec

tion)[103]

0.13m,

0.27m,

0.17m,

0.19m,

0.26m,

0.23m,

0.35m,

4.48◦

11.3◦

13.0◦

5.55◦

4.75◦

5.35◦

12.4◦

GPoseNet

0.20m,

0.38m,

0.21m,

0.28m,

0.37m,

0.35m,

0.37m,

[108]

7.11◦

12.3◦

13.8◦

8.83◦

6.94◦

8.15◦

12.5◦

MapNet

0.08m,

0.27m,

0.18m,

0.17m,

0.22m,

0.23m,

0.30m,

[109]

3.25◦

11.7◦

13.3◦

5.15◦

4.02◦

4.93◦

12.1◦

48

LSG

[110]

0.09m,

0.26m,

0.17m,

0.18m,

0.20m,

0.23m,

0.23m,

3.28◦

10.92◦

12.70◦

5.45◦

3.69◦

4.92◦

11.3◦

VlocNet

0.036m,

0.039m,

0.046m,

0.039m,

0.037m,

0.039m,

0.097m,

[111]

1.71◦

5.34◦

6.64◦

1.95◦

2.28◦

2.20◦

6.48◦

VlocNet++

0.023m,

0.018m,

0.016m,

0.024m,

0.024m,

0.025m,

0.021m,

[112]

1.44◦

1.39◦

0.99◦

1.14◦

1.45◦

2.27◦

1.08◦

DGRNet

0.016m,

0.011m,

0.017m,

0.024m,

0.022m,

0.018m,

0.017m,

[113]

1.72◦

2.19◦

3.56◦

1.95◦

2.27◦

1.86◦

4.79◦

AtLocPlus

0.10m,

0.26m,

0.14m,

0.17m,

0.20m,

0.16m,

0.29m,

[99]

3.18◦

10.8◦

11.4◦

5.16◦

3.94◦

4.90◦

10.2◦

VidLoc

0.18m,

0.26m,

0.14m,

0.26m,

0.36m,

0.31m,

0.26m,

[114]

N/A

N/A

N/A

N/A

N/A

N/A

N/A

MapNet+

0.10m,

0.20m,

0.13m,

0.18m,

0.19m,

0.20m,

0.30m,

[109]

3.17◦

9.04◦

11.13◦

5.38◦

3.92◦

5.01◦

13.37◦

MapNet+

0.09m,

0.20m,

0.12m,

0.19m,

0.19m,

0.20m,

0.27m,

PGO [109]

3.24◦

9.29◦

8.45◦

5.42◦

3.96◦

4.94◦

10.57◦

NNnet [116]

0.13m,

0.26m,

0.14m,

0.21m,

0.24m,

0.24m,

0.27m,

6.46◦

12.72◦

12.34◦

7.35◦

6.35◦

8.03◦

11.82◦

RelocNet

0.12m,

0.26m,

0.14m,

0.18m,

0.26m,

0.23m,

0.28m,

[117]

4.14◦

10.4◦

10.5◦

5.32◦

4.17◦

5.08◦

7.53◦

CamNet

0.04m,

0.03m,

0.05m,

0.04m,

0.04m,

0.04m,

0.04m,

[118]

1.73◦

1.74◦

1.98◦

1.62◦

1.64◦

1.63◦

1.51◦

AnchorNet

0.08m,

0.16m,

0.09m,

0.11m,

0.14m,

0.13m,

0.21m,

[122]

4.12◦

11.1◦

11.2◦

5.38◦

3.55◦

5.29◦

11.9◦

Table 12: A summary of published results of regression-based methods on the

7Scenes dataset

49

PoseNet

[96]

Dense

N/A,

N/A

N/A,

1.66m,

2.96m,

2.62m,

1.41m,

2.45m,

4.86◦

6.00◦

4.90◦

7.18◦

7.96◦

1.92m,

N/A,

2.31m,

1.46m,

2.65m,

PoseNet [96]

N/A

5.40◦

N/A

5.38◦

8.08◦

8.46◦

Bayesian

N/A,

1.74m,

2.14m,

2.57m,

1.25m,

2.11m,

PoseNet [97]

N/A

4.06◦

4.96◦

5.14◦

7.54◦

8.38◦

LSTM

N/A,

0.99 m,

N/A,

1.51 m,

1.18m,

1.52m,

PoseNet [101]

N/A

Hourglass

N/A,

PoseNet [101]

N/A

3.65◦

N/A,

N/A

N/A

4.29◦

N/A,

N/A,

N/A

N/A

7.44◦

N/A,

N/A

6.68◦

N/A,

N/A

SVS

N/A,

1.06m,

N/A,

1.50m,

0.63m,

2.11m,

PoseNet [100]

N/A

2.81◦

N/A

4.03◦

5.73◦

9.11◦

Geo.PoseNet

7.00m,

0.99m,

20.7m,

2.17m,

1.05m,

1.49m,

[103]

3.65◦

1.06◦

25.7◦

2.94◦

3.97◦

3.43◦

Geo.PoseNet

(reprojec

tion) [103]

PVL [106]

APANet

[105]

SPPNet [107]

GPoseNet

[108]

VlocNet [111]

6.83m,

0.88m,

20.3m,

3.20m,

0.88m,

1.57m,

3.47◦

1.04◦

25.5◦

3.29◦

3.78◦

3.32◦

N/A,

N/A

N/A,

N/A

1.30m,

N/A,

N/A,

1.22m,

2.28m,

1.67◦

N/A,

N/A

N/A

N/A

6.17◦

4.80◦

N/A,

0.98m,

0.62m,

0.77m,

N/A

1.94◦

2.49◦

2.25◦

5.42m,

0.74m,

24.5m,

2.18m,

0.59m,

1.83m,

2.84◦

N/A,

N/A

N/A,

N/A

0.96◦

23.8◦

3.92◦

2.53◦

3.35◦

1.61m,

N/A,

2.62m,

1.14m,

2.93m,

2.29◦

N/A

3.89◦

5.73◦

6.46◦

0.836m,

N/A,

1.075m,

0.593m,

0.631m,

1.419◦

N/A

2.411◦

3.529◦

3.906◦

AnchorNet

5.89m,

0.79m,

11.8m,

2.11m,

0.77m,

1.22m,

[122]

3.53◦

0.95◦

24.3◦

3.05◦

3.25◦

3.02◦

50

DSAC++ [89]

0.40m,

0.18m,

N/A,

0.20m,

0.06m,

0.13m,

0.2◦

0.3◦

N/A

0.3◦

0.3◦

0.4◦

Scene coord

0.51m,

0.18m,

N/A,

0.19m,

0.07m,

0.25m,

inate [90]

0.3◦

0.3◦

N/A

0.4◦

0.3◦

0.7◦

RGB [91]

0.335m,

0.179m,

N/A,

0.212m,

0.52m,

0.151m,

0.21◦

0.31◦

N/A

0.38◦

0.25◦

0.50◦

rgb+3d

0.485m,

0.147m,

N/A,

0.210m,

0.46m,

0.134m,

model [91]

0.25◦

0.29◦

N/A

0.41◦

0.25◦

0.45◦

Multi-View

0.62m,

0.20m,

N/A,

0.19m,

0.07m,

0.20m,

[92]

0.4◦

0.3◦

N/A

0.4◦

0.3◦

0.6◦

SanNet [95]

3.28m,

0.32m,

8.74m,

0.32m,

0.10m,

0.16m,

1.95◦

0.54◦

12.64◦

0.54◦

0.47◦

0.57◦

Table 13: A summary of published results on the RobotCar dataset (mean)

Methods

LOOP1

LOOP2 FULL1

FULL2

average

PoseNet [96]

AtLoc [99]

28.81m,

25.29m,

125.6m,

131.06m,

77.85m,

19.62◦

17.45◦

27.1◦

26.05◦

22.56◦

8.61m,

8.86m,

29.6m,

48.2m,

23.8m,

4.58◦

4.67◦

12.4◦

11.1◦

8.19◦

MapNet [109]

8.76m,

9.84m,

41.4m,

59.3m,

29.8m,

3.46◦

3.96◦

12.5◦

14.8◦

8.68◦

LSG [110]

9.07m,

9.19m,

31.65m,

53.45m,

25.84m,

3.31◦

3.53◦

4.51◦

8.60◦

4.99◦

51

Table 14: A summary of published structure-based methods on the Aachen

dataset

Methods

Day

Night

All

Threshold

(0.25

(0.50

(5.0

(0.5

(1.0

(5.0

(0.5

(1.0

(5.0

Accuracy%

m,2◦)

m,5◦)

m,10◦)

m,2◦)

m,5◦)

m,10◦)

m,2◦)

m,5◦)

m,10◦)

/

/

/

/

/

/

/

/

/

/

/

/

/

/

/

/

/

/

/

/

/

/

/

/

/

/

/

38.8

62.2

85.7

42.8

57.1

75.5

44.9

64.3

88.8

42.8

57.1

75.5

46.9

65.3

88.8

42.9

64.3

85.7

44.9

68.4

88.8

NetVLAD [50]

0

0.2

18.9

0

2

12.2

HF-Net [37]

75.7

84.3

90.9

40.8

55.1

72.4

UR2KID [126]

79.9

88.6

93.6

45.9

64.3

83.7

Dense Seman

tic loc [127]

89.3

95.4

97.6

44.9

67.3

87.8

S2D [83]

84.3

90.9

95.9

46.9

69.4

86.7

CSL [27]

45.3

73.5

90.1

Active

search [128]

[129]

[11]

DELF [55]

Superpoint [66]

D2-net [79]

R2D2 [77]

ALSFeat[78]

Describe-to

-detect [80]

Dense-to

-Dense [85]

35.6

67.9

90.4

41.6

73.3

90.1

45.5

77

94.7

/

/

/

/

/

/

/

/

/

/

/

/

/

/

/

/

/

/

/

/

/

2.6

2.1

1.9

6.9

/

/

/

/

/

/

/

7.2

4.3

8.2

12.1

/

/

/

/

/

/

/

0.6

0.9

0.3

2.7

/

/

/

/

/

/

/

52

Table 15: A summary of part published structure based methods on RobotCar

Methods

Scene

RobotCar

dusk

sun

night

night-rain

NetVLAD [50]

7.4

29.7

92.9

5.7

16.5

86.7

0.2

1.8

15.5

0.5

2.7

16.4

HF-Net [37]

53.9

81.5

94.2

48.5

69.1

85.7

2.7

6.6

15.8

4.7

16.8

21.8

Scene

S2D [83]

CSL [27]

active search [128]

45.7

52.3

57.3

day

78

80

83.7

95.1

94.3

96.6

22.3

24.5

19.4

night

61.8

33.7

30.6

94.5

49

43.9

Table 16: A summary of part published structure-based methods on CMU. The

accuracy is measured at threshold (0.25m, 2◦), (0.5m, 5◦)and (5.0m, 10◦)

Methods

Scene

CMU

urban

suburban

Threshold

0.25m,2◦

0.50m,5◦

5.0m,10◦

0.25m,2◦

0.50m,5◦

5.0m,10◦

NetVLAD [50]

HF-Net [37]

17.4

90.4

40.3

93.1

93.2

96.1

7.7

71.8

21

78.2

80.5

87.1

Table 17: A summary of published results of APR and RPR methods on other

datasets

Methods

Scene

TUM-LSI

Dubrovnki 6K

road11

road12

generalized

ApolloScape

PoseNet [96]

/

LSTM PoseNet [98]

1.31m, 2.79◦

/

/

Geo.PoseNet [103]

MapNet [109]

NNnet [116]

CamNet [118]

/

/

/

/

9.88m, 4.73◦

/

/

/

53

13.85m, 3.49◦

11.24m, 3.55◦

/

/

/

/

8.30m, 2.77◦

6.83m, 2.72◦

/

/

/

/

6.90m, 3.28◦

6.34m, 3.33◦

16.60m, 3.49◦

5.24m, 2.57◦

5.19m, 2.70◦

8.63m, 2.97◦

Table 10-17 illustrates the published results for some common benchmarks,

e.g., 7Scenes [96], Cambridge [96], Oxford RobotCar [123], TUM LSI [98],

Dubrovnik 6K [124], ApolloScape [125], Aachen [11], CMU [11] and InLoc [87]

datasets. Through these results, we can see that structure-based localization

methods generally surpass camera pose regression methods. From the absolute

pose regression and relative camera pose regression methods results, we see that

improving the neural network architecture, optimizing the loss function through

adding more information such as providing sequential images or video, adding

geometric-aware temporal constraints, semantic constraints, can improve the

localization performance.

Table 18: Structure-based methods qualitative comparison

Paper

Input

Scene

Structure

Robus

Accu

Eﬃc

tness

racy

iency

Output

[127]

RGB-D image

3D model

H = (t, θ)

Random Forest

+RANSAC (PnP)

DSAC [130]

RGB image patches

3D model CNN + DSAC

DSAC++ [89]

RGB image patches

3D model

or not

CNN + DSAC++

ESAC [131]

RGB image patches

3D model CNN + ESAC

[132]

RGB image

3D model CNN + DSAC

NG-RANSAC

[133]

RGB image

3D model

CNN +

NG-RANSAC

HSC-NET [134] RGB image

3D model CNN + DSAC

HF-Net [37]

RGB image

3D model

InLoc [87]

RGB image

3D model

NetVLAD [50]

RGB image

3D model

CNN + NN

+ RANSAC

CNN + NN +

RANSAC (PnP)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

54

Table 19: Publication information, training model and ground truth label of

structure-based methods

Ground truth

label

Loc.

Image

Corres

pair

Task

Method

Year-Pub.-Cited

Training dataset

Keypoint

detection

Descriptor

learning

Matching

pipeline

QuadNet

[64]

Key.Net

[135]

HardNet

[72]

SOSNet

[73]

LIFT

[75]

SuperPoint

[66]

LF-Net

[76]

2017-CVPR-76

DTU robot image

2019-ICCV-41

ImageNet ILSVRC

2017-NIPS-222

2019-CVPR-57

2012

UBC/

Brown dataset

UBC/

Brown dataset

2016-ECCV-601

Piccadilly Circus

SFM

2018-CVPR-341 MS-COCO

Self

2018-NIPS-140

ScanNet,

25photo-tourism

D2-Net [79]

2019-CVPR-73

MegaDepth

R2D2

[77]

NetVLAD

Retrieval

[50]

2019-NIPS-67

Aachen

2016-CVPR-1249

Google street

wheel

GEM [52]

2018-TPAMI-318

SFM-120k

Stru

cture

-based

3D

model

Self

MVS

MVS

SFM

SFM

SFM/

Flow/

Style

T.M.

SFM

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

DELF [55]

2017-ICCV-304

Landmark dataset

Class

Multi-task

HF-Net

[37]

ContextDesc

[136]

2019-CVPR-114

2019-CVPR-45

Google landmark,

BDD

Photo-tourism,

aerial dataset

Teacher (cid:88)

SFM

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

6.3. Comparison of Real-World Applicability

Generally, structure-based methods achieve a higher accuracy compared to

end-to-end methods including APR. Some RPR methods keep pursuing the

55

accuracy of structure-based methods with a less complex pipeline. A retrieval-

based method has the worst eﬀect because it uses the retrieved image pose as

the calculated value.

Table 20: APR and RPR methods qualitative comparison

Methods

Robustness

Required

Time

Size

resources

(ms)

(Mb)

Impl

ement

ation

PoseNet

Lighting, motion blur,

[96]

diﬀerent camera intrinsics

Bayesian

Large viewpoint or

PoseNet [97]

appearance changes

LSTM

Motion blur and

Nvidia

Titan

black

Nvidia

Titan X

Nvidia

PoseNet [101]

illumination changes

Titan X

Hourglass

Continuous pose

PoseNet [101]

optimization

Nvidia

Titan X

Large translational

5-95

50

6

50

9.2

/

/

/

(cid:88)

(cid:88)

/

(cid:88)

SVS

deviations of the camera

Nvidia

PoseNet [100]

along with the depth

Titan X

12.5

10

/

of the scene

BranchNet

Ambiguities, motion-blur,

Nvidia

[102]

ﬂat surfaces, lighting

Titan X

6

46

/

Geo.PoseNet

(geo.loss)

/Geo.PoseNet

Lighting, motion blur,

Nvidia

(reprojection

unknown camera intrinsics

Titan X

5

/

(cid:88)

error loss)

[103]

56

AtLoc/

AtLocPlus

[99]

Dynamic objects,

illumination

Nvidia

Titan X

6.3

/

Motion blur, repeating

Nvidia

AdPR [104]

structures, texture-less

GeForce

50

surfaces

RTX 2080

PVL [106]

Dynamic environments

APANet

Lighting, viewpoint

/

/

[105]

SPPNet

[107]

MapNet

[109]

LSG [8.2]

VlocNet

[111]

Unevenly distributed

Nvidia

image features

Titan X

Nvidia

GeForce

GTX 1070

GPoseNet

Choice of hyperparame-

[108]

ters

Online, locally smooth

and drift-free

/

9.4

Pose uncertainties by

content augmentation

Nvidia

1080Ti

Environment, dynamic

Nvidia

objects, structure

Titan X

Noise, camera angle

VlocNet++

deviations, object scale,

[112]

frame-level distortions

Nvidia

Titan X

DGRNet

[113]

Camera parameters,

Nvidia

challenging environments

GTX 1080

VidLoc [114] Temporal smoothness

Titan X

Pascal

57

/

/

2

/

/

/

79

45-

65

18-

43

(cid:88)

/

(cid:88)

/

/

/

/

36.8 (cid:88)

/

/

/

/

/

/

/

/

/

/

/

/

/

(cid:88)

NNnet [116]

Pose ﬁltering

Nvidia

Titan X

RelocNet

Pose retrieval descriptors

/

[117]

CamNet

2D-3D matching

[118]

To learn or

not to learn

Outlier pairs

[119]

Repetitive structures,

Relative NN

textureless objects,

[120]

large viewpoint changes

Nvidia

Titan XP

Nvidia

Titan XP

Nvidia

Titan X

AnchorNet

Relative anchor point

/

[122]

/

/

/

/

/

/

/

/

/

/

/

/

(cid:88)

/

/

/

/

/

Structure feature-based localization methods rely on the correspondence be-

tween the 2D query feature and the 3D model. Matching based methods estab-

lish the correspondences between 3D point cloud in scene model and 2D query

image feature by matching feature descriptor. Camera localization algorithm

applications are quite mature. But they are still fragile for those repetitive lo-

cal features and there remains the issue that they have a high computational

requirement. Most of the work focuses on robust feature points or accurate

feature descriptors under extreme conditions in 2D images. We should consider

the constraints of 3D spatial geometry for the 2D feature in further complex

applications, e.g., most existing scene coordinate regression methods can only

be adopted on small-scale scenes. They have not yet proven their capacity in

large-scale scenes. For robustness or precision, accuracy or eﬃciency, we need

to think carefully about the trade-oﬀs. Table 19 shows a summary of some

58

published structure-based methods. We summarized their input format and

output result H, in which t and θ mean translation and orientation respec-

tively. According to their innovation, we divided those approaches into three

improved directions, robustness, accuracy and eﬃciency. Table 20 lists a general

summary of comparisons based on publication information, training model and

ground-truth label of structure-based methods.

Robustness and accuracy are the most important criteria for detecting posi-

tioning performance. The stronger the adaptability and robustness to changes in

the scene environment, the better the positioning performance. Table 21 shows

the qualitative comparison between APR and RPR methods, including robust-

ness, time to process each picture, computing resources required for positioning

tasks, scene model size, and whether there is public code implementation.

7. Summary and Concluding Remarks

7.1. Summary

Remarkable achievements have been made in multiple research branches of

image-based camera localization. This paper reviews the main branches and de-

scribes their methods, datasets, metrics, and gives detailed statistics for quan-

titative and qualitative comparisons of these methods.

7.2. Future Potential Research Directions

The future research directions for image-based camera localization are pro-

posed as follows:

7.2.1. Sensor fusion

Rather than just being limited to the image data output by a camera sensor,

other sensors that can obtain more extensive positioning information, such as

LiDAR, WiFi, IMU, Bluetooth, etc., could be used. An eﬀective complementary

combination of diﬀerent sensor information can help build a more accurate and

powerful positioning system. To achieve this, multiple sensor data fusion needs

to overcome the challenge of heterogeneous characteristics of multiple sensor

59

data. This kind of multi-modal task can merge diﬀerent features by learning

the joint contribution of every single task, or through learning the cross-coding

between diﬀerent tasks for more eﬃcient positioning.

7.2.2. Multi-features

Most of the structure-based work mentioned above is based on 3D point

features extracted from 2D scene images. It is limited under some challenging

conditions, such as weak texture, illumination, weather, etc. But we can also

use multi-features to improve localization. For example, some SLAM researchers

used line or plane features to estimate camera pose and get a very impressive

result. Maybe we can use CNN to improve the process of extracting line or

plane features, and then design an eﬃcient matching pipeline for all kinds of

features.

7.2.3. Semantic information auxiliaries

Another direction is to use the semantics of features. Semantic plays an

important role in real-world scenes [11]. With the help of semantic information,

we can easily ﬁlter out those features on dynamic objects that aﬀect the local-

ization result. And we can also verify the location by comparing the semantic

information between scene and query image which means we need to build a

semantic 3D scene model. Semantic information is an upper-level feature. Con-

structing a semantic three-dimensional model of the scene can not only assist

in localization but also be useful for the wider application level.

7.2.4. Multi-cameras

Compared with a single camera, a multi-camera system can cover a panoramic

360◦ ﬁeld of view and can signiﬁcantly improve performance in robotic applica-

tions such as camera positioning. However, multiple images output by multiple

cameras requires feature calculation during feature matching. One of the future

research and development directions may be to use deep learning to resolve the

features of multiple cameras end-to-end and speed up the feature calculation

time.

60

7.2.5. Challenging conditions

In many speciﬁc positioning tasks, how to eﬀectively improve positioning

performance in challenging scenarios is a key step in building an accurate po-

sitioning system. For example, environmental changes caused by illumination,

blur, and occlusion changes make the feature extraction of images less accurate.

Further, the position changes of dynamic objects between diﬀerent frames of im-

ages also cause interference to matching between images. In the future, we can

explore methods such as using the relationship between multiple frames of im-

ages and adding other auxiliary judgment information to solve these challenges

to improve the robustness of the positioning model in a variety of scenarios.

7.2.6. Integration in light-weight devices

In the future, applications that use camera positioning assistance will de-

velop towards being small and lightweight, which will be more quickly and con-

veniently applied to small portable devices, such as embedded devices or mobile

phones. In addition, algorithm applications on small ICT resource devices could

use methods such as model acceleration to optimize model calculation speed,

model size, and consumption of computing resources, to better integrate with

device functions to serve applications such as navigation, sports, teaching, enter-

tainment. In addition, the use of mobile phones and other devices with cameras

and other sensor fusions requires the development of more augmented reality

functions to serve smart navigation, guided services, and immersive games.

8. Acknowledgement

This research was funded in part by a PhD scholarship funded jointly by

the China Scholarship Council (CSC) and QMUL and partly funded under Didi

Chuxing and the Robotics and AI for Extreme Environments program’s NCNR

(National Centre for Nuclear Robotics) grant no. EP/R02572X/1.

61

References

[1] N. Piasco, D. Sidib´e, C. Demonceaux, V. Gouet-Brunet, A survey on

visual-based localization: On the beneﬁt of heterogeneous data, Pattern

Recognition 74 (2018) 90–109.

[2] T. Sattler, Q. Zhou, M. Pollefeys, L. Leal-Taixe, Understanding the lim-

itations of cnn-based absolute camera pose regression, in: Proceedings of

the IEEE/CVF conference on computer vision and pattern recognition,

2019, pp. 3302–3312.

[3] Y. Wu, F. Tang, H. Li, Image-based camera localization: an overview,

Visual Computing for Industry, Biomedicine, and Art 1 (1) (2018) 1–13.

[4] C. Debeunne, D. Vivet, A review of visual-lidar fusion based simultaneous

localization and mapping, Sensors 20 (7) (2020) 2068.

[5] C. Chen, B. Wang, C. X. Lu, N. Trigoni, A. Markham, A survey on deep

learning for localization and mapping: Towards the age of spatial machine

intelligence, arXiv preprint arXiv:2006.12567.

[6] Y. Shavit, R. Ferens, Introduction to camera pose estimation with deep

learning, arXiv preprint arXiv:1907.05272.

[7] N. Snavely, S. M. Seitz, R. Szeliski, Photo tourism: exploring photo col-

lections in 3d, ACM siggraph 2006 papers (2006) 835–846.

[8] J. L. Schonberger, J.-M. Frahm, Structure-from-motion revisited, in: Pro-

ceedings of the IEEE conference on computer vision and pattern recogni-

tion, 2016, pp. 4104–4113.

[9] W. Changchang, Visualsfm: A visual structure from motion system

(2011).

[10] J. Sturm, N. Engelhard, F. Endres, W. Burgard, D. Cremers, A bench-

mark for the evaluation of rgb-d slam systems, in: 2012 IEEE/RSJ in-

62

ternational conference on intelligent robots and systems, IEEE, 2012, pp.

573–580.

[11] T. Sattler, W. Maddern, C. Toft, A. Torii, L. Hammarstrand, E. Stenborg,

D. Safari, M. Okutomi, M. Pollefeys, J. Sivic, et al., Benchmarking 6dof

outdoor visual localization in changing conditions, in: Proceedings of the

IEEE Conference on Computer Vision and Pattern Recognition, 2018, pp.

8601–8610.

[12] Z. Zhang, T. Sattler, D. Scaramuzza, Reference pose generation for long-

term visual localization via learned features and view synthesis, Interna-

tional Journal of Computer Vision 129 (4) (2021) 821–844.

[13] M. A. Fischler, R. C. Bolles, Random sample consensus: a paradigm for

model ﬁtting with applications to image analysis and automated cartog-

raphy, Communications of the ACM 24 (6) (1981) 381–395.

[14] D. P. Robertson, R. Cipolla, An image-based system for urban navigation.,

in: Bmvc, Vol. 19, Citeseer, 2004, p. 165.

[15] W. Zhang, J. Kosecka, Image based localization in urban environments,

in: Third international symposium on 3D data processing, visualization,

and transmission (3DPVT’06), IEEE, 2006, pp. 33–40.

[16] Y. Li, N. Snavely, D. P. Huttenlocher, Location recognition using pri-

oritized feature matching, in: European conference on computer vision,

Springer, 2010, pp. 791–804.

[17] T. Sattler, B. Leibe, L. Kobbelt, Fast image-based localization using di-

rect 2d-to-3d matching, in: 2011 International Conference on Computer

Vision, IEEE, 2011, pp. 667–674.

[18] T. Sattler, B. Leibe, L. Kobbelt, Improving image-based localization by

active correspondence search, in: European conference on computer vi-

sion, Springer, 2012, pp. 752–765.

63

[19] D. G. Lowe, Distinctive image features from scale-invariant keypoints,

International journal of computer vision 60 (2) (2004) 91–110.

[20] T. Sattler, M. Havlena, F. Radenovic, K. Schindler, M. Pollefeys, Hy-

perpoints and ﬁne vocabularies for large-scale location recognition, in:

Proceedings of the IEEE International Conference on Computer Vision,

2015, pp. 2102–2110.

[21] D. G. Lowe, Distinctive image features from scale-invariant keypoints,

International journal of computer vision 60 (2) (2004) 91–110.

[22] Y. Feng, L. Fan, Y. Wu, Fast localization in large-scale environments

using supervised indexing of binary features, IEEE Transactions on Image

Processing 25 (1) (2015) 343–358.

[23] E. Rosten, T. Drummond, Machine learning for high-speed corner detec-

tion, in: European conference on computer vision, Springer, 2006, pp.

430–443.

[24] L. Liu, H. Li, Y. Dai, Eﬃcient global 2d-3d matching for camera localiza-

tion in a large-scale 3d map, in: Proceedings of the IEEE International

Conference on Computer Vision, 2017, pp. 2372–2381.

[25] H. Tong, C. Faloutsos, J.-Y. Pan, Fast random walk with restart and

its applications,

in: Sixth international conference on data mining

(ICDM’06), IEEE, 2006, pp. 613–622.

[26] L. Svarm, O. Enqvist, M. Oskarsson, F. Kahl, Accurate localization and

pose estimation for large 3d models, in: Proceedings of the IEEE Confer-

ence on Computer Vision and Pattern Recognition, 2014, pp. 532–539.

[27] L. Sv¨arm, O. Enqvist, F. Kahl, M. Oskarsson, City-scale localization for

cameras with known vertical direction, IEEE transactions on pattern anal-

ysis and machine intelligence 39 (7) (2016) 1455–1461.

64

[28] B. Zeisl, T. Sattler, M. Pollefeys, Camera pose voting for large-scale image-

based localization, in: Proceedings of the IEEE International Conference

on Computer Vision, 2015, pp. 2704–2712.

[29] A. E. Johnson, M. Hebert, Using spin images for eﬃcient object recog-

nition in cluttered 3d scenes, IEEE Transactions on pattern analysis and

machine intelligence 21 (5) (1999) 433–449.

[30] R. B. Rusu, N. Blodow, M. Beetz, Fast point feature histograms (fpfh) for

3d registration, in: 2009 IEEE international conference on robotics and

automation, IEEE, 2009, pp. 3212–3217.

[31] S. Choi, Q.-Y. Zhou, V. Koltun, Robust reconstruction of indoor scenes,

in: Proceedings of the IEEE Conference on Computer Vision and Pattern

Recognition, 2015, pp. 5556–5565.

[32] Z. Wu, S. Song, A. Khosla, F. Yu, L. Zhang, X. Tang, J. Xiao, 3d

shapenets: A deep representation for volumetric shapes, in: Proceedings

of the IEEE conference on computer vision and pattern recognition, 2015,

pp. 1912–1920.

[33] Y. Fang, J. Xie, G. Dai, M. Wang, F. Zhu, T. Xu, E. Wong, 3d deep shape

descriptor, in: Proceedings of the IEEE Conference on Computer Vision

and Pattern Recognition, 2015, pp. 2319–2328.

[34] S. Song, J. Xiao, Deep sliding shapes for amodal 3d object detection in

rgb-d images, in: Proceedings of the IEEE conference on computer vision

and pattern recognition, 2016, pp. 808–816.

[35] K. Guo, D. Zou, X. Chen, 3d mesh labeling via deep convolutional neural

networks, ACM Transactions on Graphics (TOG) 35 (1) (2015) 1–12.

[36] A. Zeng, S. Song, M. Nießner, M. Fisher, J. Xiao, T. Funkhouser, 3dmatch:

Learning local geometric descriptors from rgb-d reconstructions, in: Pro-

ceedings of the IEEE conference on computer vision and pattern recogni-

tion, 2017, pp. 1802–1811.

65

[37] P.-E. Sarlin, C. Cadena, R. Siegwart, M. Dymczyk, From coarse to ﬁne:

Robust hierarchical localization at large scale,

in: Proceedings of the

IEEE/CVF Conference on Computer Vision and Pattern Recognition,

2019, pp. 12716–12725.

[38] A. Irschara, C. Zach, J.-M. Frahm, H. Bischof, From structure-from-

motion point clouds to fast location recognition, in: 2009 IEEE Conference

on Computer Vision and Pattern Recognition, IEEE, 2009, pp. 2599–2606.

[39] A. Irschara, C. Zach, H. Bischof, Towards wiki-based dense city modeling,

in: 2007 ieee 11th international conference on computer vision, IEEE,

2007, pp. 1–8.

[40] J. Philbin, O. Chum, M. Isard, J. Sivic, A. Zisserman, Object retrieval

with large vocabularies and fast spatial matching, in: 2007 IEEE confer-

ence on computer vision and pattern recognition, IEEE, 2007, pp. 1–8.

[41] Y. Avrithis, Y. Kalantidis, Approximate gaussian mixtures for large scale

vocabularies, in: European Conference on Computer Vision, Springer,

2012, pp. 15–28.

[42] X. Shen, Z. Lin, J. Brandt, Y. Wu, Spatially-constrained similarity mea-

surefor large-scale object retrieval, IEEE transactions on pattern analysis

and machine intelligence 36 (6) (2013) 1229–1241.

[43] O. Chum, A. Mikulik, M. Perdoch, J. Matas, Total recall ii: Query ex-

pansion revisited, in: CVPR 2011, IEEE, 2011, pp. 889–896.

[44] G. Tolias, H. J´egou, Visual query expansion with or without geometry: re-

ﬁning local descriptors by feature aggregation, Pattern recognition 47 (10)

(2014) 3466–3476.

[45] K. Kesorn, S. Poslad, An enhanced bag-of-visual word vector space model

to represent visual content in athletics images, IEEE Transactions on Mul-

timedia 14 (1) (2011) 211–222. doi:10.1109/TMM.2011.2170665.

66

[46] A. Krizhevsky, I. Sutskever, G. E. Hinton, Imagenet classiﬁcation with

deep convolutional neural networks, Advances in neural information pro-

cessing systems 25 (2012) 1097–1105.

[47] J. Donahue, Y. Jia, O. Vinyals, J. Hoﬀman, N. Zhang, E. Tzeng, T. Dar-

rell, Decaf: A deep convolutional activation feature for generic visual

recognition, in:

International conference on machine learning, PMLR,

2014, pp. 647–655.

[48] A. Sharif Razavian, H. Azizpour, J. Sullivan, S. Carlsson, Cnn features oﬀ-

the-shelf: an astounding baseline for recognition, in: Proceedings of the

IEEE conference on computer vision and pattern recognition workshops,

2014, pp. 806–813.

[49] F. R. G. T. O. Chum, Cnn image retrieval learns from bow: Unsupervised

ﬁne-tuning with hard examples, IEEE Transaction on Image Processing.

[50] R. Arandjelovic, P. Gronat, A. Torii, T. Pajdla, J. Sivic, Netvlad: Cnn

architecture for weakly supervised place recognition, in: Proceedings of

the IEEE conference on computer vision and pattern recognition, 2016,

pp. 5297–5307.

[51] G. Amato, P. Bolettieri, F. Falchi, C. Gennaro, Large scale image retrieval

using vector of locally aggregated descriptors, in: International Conference

on Similarity Search and Applications, Springer, 2013, pp. 245–256.

[52] F. Radenovi´c, G. Tolias, O. Chum, Fine-tuning cnn image retrieval with

no human annotation, IEEE transactions on pattern analysis and machine

intelligence 41 (7) (2018) 1655–1668.

[53] T. Dai, J. Cai, Y. Zhang, S.-T. Xia, L. Zhang, Second-order atten-

tion network for single image super-resolution, in: Proceedings of the

IEEE/CVF Conference on Computer Vision and Pattern Recognition,

2019, pp. 11065–11074.

67

[54] J. Revaud, J. Almaz´an, R. S. Rezende, C. R. d. Souza, Learning with

average precision: Training image retrieval with a listwise loss, in: Pro-

ceedings of the IEEE/CVF International Conference on Computer Vision,

2019, pp. 5107–5116.

[55] H. Noh, A. Araujo, J. Sim, T. Weyand, B. Han, Large-scale image re-

trieval with attentive deep local features, in: Proceedings of the IEEE

international conference on computer vision, 2017, pp. 3456–3465.

[56] M. Teichmann, A. Araujo, M. Zhu, J. Sim, Detect-to-retrieve: Eﬃcient

regional aggregation for image search, in: Proceedings of the IEEE/CVF

Conference on Computer Vision and Pattern Recognition, 2019, pp. 5109–

5118.

[57] S. S. Husain, M. Bober, Remap: Multi-layer entropy-guided pooling of

dense cnn features for image retrieval, IEEE Transactions on Image Pro-

cessing 28 (10) (2019) 5201–5213.

[58] E. Rosten, T. Drummond, Machine learning for high-speed corner detec-

tion, in: European conference on computer vision, Springer, 2006, pp.

430–443.

[59] S. Leutenegger, M. Chli, R. Y. Siegwart, Brisk: Binary robust invariant

scalable keypoints, in: 2011 International conference on computer vision,

Ieee, 2011, pp. 2548–2555.

[60] E. Rublee, V. Rabaud, K. Konolige, G. Bradski, Orb: An eﬃcient alterna-

tive to sift or surf, in: 2011 International conference on computer vision,

Ieee, 2011, pp. 2564–2571.

[61] Y. Verdie, K. Yi, P. Fua, V. Lepetit, Tilde: A temporally invariant learned

detector, in: Proceedings of the IEEE conference on computer vision and

pattern recognition, 2015, pp. 5279–5288.

[62] K. Lenc, A. Vedaldi, Learning covariant feature detectors, in: European

conference on computer vision, Springer, 2016, pp. 100–117.

68

[63] X. Zhang, F. X. Yu, S. Karaman, S.-F. Chang, Learning discriminative

and transformation covariant local feature detectors, in: Proceedings of

the IEEE conference on computer vision and pattern recognition, 2017,

pp. 6818–6826.

[64] N. Savinov, A. Seki, L. Ladicky, T. Sattler, M. Pollefeys, Quad-networks:

unsupervised learning to rank for interest point detection, in: Proceedings

of the IEEE conference on computer vision and pattern recognition, 2017,

pp. 1822–1830.

[65] D. DeTone, T. Malisiewicz, A. Rabinovich, Toward geometric deep slam,

arXiv preprint arXiv:1707.07410.

[66] D. DeTone, T. Malisiewicz, A. Rabinovich, Superpoint: Self-supervised

interest point detection and description, in: Proceedings of the IEEE

conference on computer vision and pattern recognition workshops, 2018,

pp. 224–236.

[67] S. Zagoruyko, N. Komodakis, Learning to compare image patches via

convolutional neural networks, in: Proceedings of the IEEE conference on

computer vision and pattern recognition, 2015, pp. 4353–4361.

[68] X. Han, T. Leung, Y. Jia, R. Sukthankar, A. C. Berg, Matchnet: Unifying

feature and metric learning for patch-based matching, in: Proceedings of

the IEEE conference on computer vision and pattern recognition, 2015,

pp. 3279–3286.

[69] E. Simo-Serra, E. Trulls, L. Ferraz, I. Kokkinos, P. Fua, F. Moreno-

Noguer, Discriminative learning of deep convolutional feature point de-

scriptors, in: Proceedings of the IEEE international conference on com-

puter vision, 2015, pp. 118–126.

[70] V. Balntas, E. Riba, D. Ponsa, K. Mikolajczyk, Learning local feature

descriptors with triplets and shallow convolutional neural networks., in:

Bmvc, Vol. 1, 2016, p. 3.

69

[71] Y. Tian, B. Fan, F. Wu, L2-net: Deep learning of discriminative patch

descriptor in euclidean space, in: Proceedings of the IEEE conference on

computer vision and pattern recognition, 2017, pp. 661–669.

[72] A. Mishchuk, D. Mishkin, F. Radenovic, J. Matas, Working hard to know

your neighbor’s margins: Local descriptor learning loss, arXiv preprint

arXiv:1705.10872.

[73] Y. Tian, X. Yu, B. Fan, F. Wu, H. Heijnen, V. Balntas, Sosnet: Second or-

der similarity regularization for local descriptor learning, in: Proceedings

of the IEEE/CVF Conference on Computer Vision and Pattern Recogni-

tion, 2019, pp. 11016–11025.

[74] Q. Wang, X. Zhou, B. Hariharan, N. Snavely, Learning feature descriptors

using camera pose supervision, in: European Conference on Computer

Vision, Springer, 2020, pp. 757–774.

[75] K. M. Yi, E. Trulls, V. Lepetit, P. Fua, Lift: Learned invariant feature

transform, in: European conference on computer vision, Springer, 2016,

pp. 467–483.

[76] Y. Ono, E. Trulls, P. Fua, K. M. Yi, Lf-net: Learning local features from

images, arXiv preprint arXiv:1805.09662.

[77] J. Revaud, P. Weinzaepfel, C. De Souza, N. Pion, G. Csurka, Y. Cabon,

M. Humenberger, R2d2: repeatable and reliable detector and descriptor,

arXiv preprint arXiv:1906.06195.

[78] Z. Luo, L. Zhou, X. Bai, H. Chen, J. Zhang, Y. Yao, S. Li, T. Fang,

L. Quan, Aslfeat: Learning local features of accurate shape and localiza-

tion, in: Proceedings of the IEEE/CVF conference on computer vision

and pattern recognition, 2020, pp. 6589–6598.

[79] M. Dusmanu, I. Rocco, T. Pajdla, M. Pollefeys, J. Sivic, A. Torii, T. Sat-

tler, D2-net: A trainable cnn for joint description and detection of local

70

features, in: Proceedings of the ieee/cvf conference on computer vision

and pattern recognition, 2019, pp. 8092–8101.

[80] Y. Tian, V. Balntas, T. Ng, A. Barroso-Laguna, Y. Demiris, K. Miko-

lajczyk, D2d: Keypoint extraction with describe to detect approach, in:

Proceedings of the Asian Conference on Computer Vision, 2020.

[81] A. Benbihi, M. Geist, C. Pradalier, Elf: Embedded localisation of fea-

tures in pre-trained cnn, in: Proceedings of the IEEE/CVF International

Conference on Computer Vision, 2019, pp. 7940–7949.

[82] H. Germain, G. Bourmaud, V. Lepetit, Sparse-to-dense hypercolumn

matching for long-term visual localization, in: 2019 International Con-

ference on 3D Vision (3DV), IEEE, 2019, pp. 513–523.

[83] H. Germain, G. Bourmaud, V. Lepetit, S2dnet: Learning accurate

correspondences for sparse-to-dense feature matching, arXiv preprint

arXiv:2004.01673.

[84] I. Rocco, M. Cimpoi, R. Arandjelovi´c, A. Torii, T. Pajdla, J. Sivic, Neigh-

bourhood consensus networks, arXiv preprint arXiv:1810.10510.

[85] I. Melekhov, A. Tiulpin, T. Sattler, M. Pollefeys, E. Rahtu, J. Kannala,

Dgc-net: Dense geometric correspondence network, in: 2019 IEEE Winter

Conference on Applications of Computer Vision (WACV), IEEE, 2019, pp.

1034–1042.

[86] O. Wiles, S. Ehrhardt, A. Zisserman, D2d: Learning to ﬁnd good corre-

spondences for image matching and manipulation, arXiv e-prints (2020)

arXiv–2007.

[87] H. Taira, M. Okutomi, T. Sattler, M. Cimpoi, M. Pollefeys, J. Sivic,

T. Pajdla, A. Torii, Inloc: Indoor visual localization with dense matching

and view synthesis, in: Proceedings of the IEEE Conference on Computer

Vision and Pattern Recognition, 2018, pp. 7199–7209.

71

[88] E. Rosten, T. Drummond, Machine learning for high-speed corner detec-

tion, in: European conference on computer vision, Springer, 2006, pp.

430–443.

[89] E. Brachmann, C. Rother, Learning less is more-6d camera localization

via 3d surface regression, in: Proceedings of the IEEE Conference on

Computer Vision and Pattern Recognition, 2018, pp. 4654–4662.

[90] X. Li, J. Ylioinas, J. Verbeek, J. Kannala, Scene coordinate regression with

angle-based reprojection loss for camera relocalization, in: Proceedings of

the European Conference on Computer Vision (ECCV) Workshops, 2018,

pp. 0–0.

[91] E. Brachmann, C. Rother, Visual camera re-localization from rgb and rgb-

d images using dsac, IEEE Transactions on Pattern Analysis and Machine

Intelligence.

[92] M. Cai, H. Zhan, C. Saroj Weerasekera, K. Li, I. Reid, Camera relocaliza-

tion by exploiting multi-view constraints for scene coordinates regression,

in: Proceedings of the IEEE/CVF International Conference on Computer

Vision Workshops, 2019, pp. 0–0.

[93] E. Brachmann, F. Michel, A. Krull, M. Y. Yang, S. Gumhold, et al.,

Uncertainty-driven 6d pose estimation of objects and scenes from a single

rgb image, in: Proceedings of the IEEE conference on computer vision

and pattern recognition, 2016, pp. 3364–3372.

[94] I. Budvytis, M. Teichmann, T. Vojir, R. Cipolla, Large scale joint seman-

tic re-localisation and scene understanding via globally unique instance

coordinate regression, arXiv preprint arXiv:1909.10239.

[95] L. Yang, Z. Bai, C. Tang, H. Li, Y. Furukawa, P. Tan, Sanet: Scene ag-

nostic network for camera localization, in: Proceedings of the IEEE/CVF

International Conference on Computer Vision, 2019, pp. 42–51.

72

[96] A. Kendall, M. Grimes, R. Cipolla, Posenet: A convolutional network

for real-time 6-dof camera relocalization, in: Proceedings of the IEEE

international conference on computer vision, 2015, pp. 2938–2946.

[97] A. Kendall, R. Cipolla, Modelling uncertainty in deep learning for camera

relocalization, in: 2016 IEEE international conference on Robotics and

Automation (ICRA), IEEE, 2016, pp. 4762–4769.

[98] F. Walch, C. Hazirbas, L. Leal-Taixe, T. Sattler, S. Hilsenbeck, D. Cre-

mers, Image-based localization using lstms for structured feature correla-

tion, in: Proceedings of the IEEE International Conference on Computer

Vision, 2017, pp. 627–637.

[99] B. Wang, C. Chen, C. X. Lu, P. Zhao, N. Trigoni, A. Markham, Atloc:

Attention guided camera localization, in: Proceedings of the AAAI Con-

ference on Artiﬁcial Intelligence, Vol. 34, 2020, pp. 10393–10401.

[100] T. Naseer, W. Burgard, Deep regression for monocular camera-based 6-dof

global localization in outdoor environments, in: 2017 IEEE/RSJ Interna-

tional Conference on Intelligent Robots and Systems (IROS), IEEE, 2017,

pp. 1525–1530.

[101] I. Melekhov, J. Ylioinas, J. Kannala, E. Rahtu, Image-based localization

using hourglass networks, in: Proceedings of the IEEE international con-

ference on computer vision workshops, 2017, pp. 879–886.

[102] J. Wu, L. Ma, X. Hu, Delving deeper into convolutional neural networks

for camera relocalization,

in: 2017 IEEE International Conference on

Robotics and Automation (ICRA), IEEE, 2017, pp. 5644–5651.

[103] A. Kendall, R. Cipolla, Geometric loss functions for camera pose regres-

sion with deep learning, in: Proceedings of the IEEE conference on com-

puter vision and pattern recognition, 2017, pp. 5974–5983.

73

[104] M. Bui, C. Baur, N. Navab, S. Ilic, S. Albarqouni, Adversarial networks for

camera pose regression and reﬁnement, in: Proceedings of the IEEE/CVF

International Conference on Computer Vision Workshops, 2019, pp. 0–0.

[105] B. Chidlovskii, A. Sadek, Adversarial transfer of pose estimation regres-

sion, in: European Conference on Computer Vision, Springer, 2020, pp.

646–661.

[106] Z. Huang, Y. Xu, J. Shi, X. Zhou, H. Bao, G. Zhang, Prior guided dropout

for robust visual localization in dynamic environments, in: Proceedings of

the IEEE/CVF International Conference on Computer Vision, 2019, pp.

2791–2800.

[107] P. Purkait, C. Zhao, C. Zach, Synthetic view generation for absolute pose

regression and image synthesis., in: BMVC, 2018, p. 69.

[108] M. Cai, C. Shen, I. Reid, A hybrid probabilistic model for camera relocal-

ization, in: BMVC Press, 2019.

[109] S. Brahmbhatt, J. Gu, K. Kim, J. Hays, J. Kautz, Geometry-aware learn-

ing of maps for camera localization, in: Proceedings of the IEEE Confer-

ence on Computer Vision and Pattern Recognition, 2018, pp. 2616–2625.

[110] F. Xue, X. Wang, Z. Yan, Q. Wang, J. Wang, H. Zha, Local supports

global: Deep camera relocalization with sequence enhancement, in: Pro-

ceedings of the IEEE/CVF International Conference on Computer Vision,

2019, pp. 2841–2850.

[111] A. Valada, N. Radwan, W. Burgard, Deep auxiliary learning for visual

localization and odometry, in: 2018 IEEE international conference on

robotics and automation (ICRA), IEEE, 2018, pp. 6939–6946.

[112] N. Radwan, A. Valada, W. Burgard, Vlocnet++: Deep multitask learn-

ing for semantic visual localization and odometry, IEEE Robotics and

Automation Letters 3 (4) (2018) 4407–4414.

74

[113] Y. Lin, Z. Liu, J. Huang, C. Wang, G. Du, J. Bai, S. Lian, Deep global-

relative networks for end-to-end 6-dof visual localization and odometry, in:

Paciﬁc Rim International Conference on Artiﬁcial Intelligence, Springer,

2019, pp. 454–467.

[114] R. Clark, S. Wang, A. Markham, N. Trigoni, H. Wen, Vidloc: A deep

spatio-temporal model for 6-dof video-clip relocalization, in: Proceedings

of the IEEE Conference on Computer Vision and Pattern Recognition,

2017, pp. 6856–6864.

[115] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Er-

han, V. Vanhoucke, A. Rabinovich, Going deeper with convolutions, in:

Proceedings of the IEEE conference on computer vision and pattern recog-

nition, 2015, pp. 1–9.

[116] Z. Laskar, I. Melekhov, S. Kalia, J. Kannala, Camera relocalization by

computing pairwise relative poses using convolutional neural network, in:

Proceedings of the IEEE International Conference on Computer Vision

Workshops, 2017, pp. 929–938.

[117] V. Balntas, S. Li, V. Prisacariu, Relocnet: Continuous metric learning re-

localisation using neural nets, in: Proceedings of the European Conference

on Computer Vision (ECCV), 2018, pp. 751–767.

[118] M. Ding, Z. Wang, J. Sun, J. Shi, P. Luo, Camnet: Coarse-to-ﬁne retrieval

for camera re-localization, in: Proceedings of the IEEE/CVF International

Conference on Computer Vision, 2019, pp. 2871–2880.

[119] Q. Zhou, T. Sattler, M. Pollefeys, L. Leal-Taixe, To learn or not to learn:

Visual localization from essential matrices, in: 2020 IEEE International

Conference on Robotics and Automation (ICRA), IEEE, 2020, pp. 3319–

3326.

[120] I. Melekhov, J. Ylioinas, J. Kannala, E. Rahtu, Relative camera pose esti-

mation using convolutional neural networks, in: International Conference

75

on Advanced Concepts for Intelligent Vision Systems, Springer, 2017, pp.

675–687.

[121] H. Aanæs, R. R. Jensen, G. Vogiatzis, E. Tola, A. B. Dahl, Large-scale

data for multiple-view stereopsis, International Journal of Computer Vi-

sion 120 (2) (2016) 153–168.

[122] S. Saha, G. Varma, C. Jawahar, Improved visual relocalization by discov-

ering anchor points, arXiv preprint arXiv:1811.04370.

[123] W. Maddern, G. Pascoe, C. Linegar, P. Newman, 1 year, 1000 km: The

oxford robotcar dataset, The International Journal of Robotics Research

36 (1) (2017) 3–15.

[124] Y. Li, N. Snavely, D. Huttenlocher, P. Fua, Worldwide pose estimation us-

ing 3d point clouds, in: European conference on computer vision, Springer,

2012, pp. 15–29.

[125] X. Huang, P. Wang, X. Cheng, D. Zhou, Q. Geng, R. Yang, The apol-

loscape open dataset for autonomous driving and its application, IEEE

transactions on pattern analysis and machine intelligence 42 (10) (2019)

2702–2719.

[126] T.-Y. Yang, D.-K. Nguyen, H. Heijnen, V. Balntas, Ur2kid: Unifying

retrieval, keypoint detection, and keypoint description without local cor-

respondence supervision, arXiv preprint arXiv:2001.07252.

[127] T. Shi, H. Cui, Z. Song, S. Shen, Dense semantic 3d map based long-term

visual localization with hybrid features, arXiv preprint arXiv:2005.10766.

[128] T. Sattler, B. Leibe, L. Kobbelt, Eﬃcient & eﬀective prioritized match-

ing for large-scale image-based localization, IEEE transactions on pattern

analysis and machine intelligence 39 (9) (2016) 1744–1756.

[129] M. Geppert, P. Liu, Z. Cui, M. Pollefeys, T. Sattler, Eﬃcient 2d-3d match-

ing for multi-camera visual localization, in: 2019 International Conference

on Robotics and Automation (ICRA), IEEE, 2019, pp. 5972–5978.

76

[130] E. Brachmann, A. Krull, S. Nowozin, J. Shotton, F. Michel, S. Gumhold,

C. Rother, Dsac-diﬀerentiable ransac for camera localization, in: Proceed-

ings of the IEEE Conference on Computer Vision and Pattern Recogni-

tion, 2017, pp. 6684–6692.

[131] E. Brachmann, C. Rother, Expert sample consensus applied to camera re-

localization, in: Proceedings of the IEEE/CVF International Conference

on Computer Vision, 2019, pp. 7525–7534.

[132] X. Li, J. Ylioinas, J. Kannala, Full-frame scene coordinate regression for

image-based localization, arXiv preprint arXiv:1802.03237.

[133] E. Brachmann, C. Rother, Neural-guided ransac: Learning where to sam-

ple model hypotheses, in: Proceedings of the IEEE/CVF International

Conference on Computer Vision, 2019, pp. 4322–4331.

[134] X. Li, S. Wang, Y. Zhao, J. Verbeek, J. Kannala, Hierarchical scene coor-

dinate classiﬁcation and regression for visual localization, in: Proceedings

of the IEEE/CVF Conference on Computer Vision and Pattern Recogni-

tion, 2020, pp. 11983–11992.

[135] A. Barroso-Laguna, E. Riba, D. Ponsa, K. Mikolajczyk, Key. net: Key-

point detection by handcrafted and learned cnn ﬁlters, in: Proceedings of

the IEEE/CVF International Conference on Computer Vision, 2019, pp.

5836–5844.

[136] Z. Luo, T. Shen, L. Zhou, J. Zhang, Y. Yao, S. Li, T. Fang, L. Quan,

Contextdesc: Local descriptor augmentation with cross-modality context,

in: Proceedings of the IEEE/CVF Conference on Computer Vision and

Pattern Recognition, 2019, pp. 2527–2536.

77

