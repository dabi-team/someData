Aerial UAV-IoT Sensing for Ubiquitous Immersive
Communication and Virtual Human Teleportation

Jacob Chakareski
Electrical and Computer Engineering Department, The University of Alabama

7
1
0
2

l
u
J

1
2

]
I

N
.
s
c
[

2
v
2
9
1
4
0
.
3
0
7
1
:
v
i
X
r
a

Abstract—We consider UAV IoT aerial sensing that delivers
multiple VR/AR immersive communication sessions to remote
users. The UAV swarm is spatially distributed over a wide area
of interest, and each UAV captures a viewpoint of the scene
below it. The remote users are interested in visual immersive
navigation of speciﬁc subareas/scenes of interest, reconstructed
on their respective VR/AR devices from the captured data. The
reconstruction quality of the immersive scene representations at
the users will depend on the sampling/sensing rates associated
with each UAV. There is a limit on the aggregate amount of
data that the UAV swarm can sample and send towards the
users, stemming from physical/transmission capacity constraints.
Similarly, each VR/AR application has minimum reconstruction
quality requirements for its own session. We propose an optimiza-
tion framework that makes three contributions in this context.
First, we select the optimal sampling rates to be used by each
UAV, such that the system and application constraints are not
exceed, while the priority weighted reconstruction quality across
all VR/AR sessions is maximized. Then, we design an optimal
scalable source-channel signal representation that instills into the
captured data inherent rate adaptivity, unequal error protection,
and minimum required redundancy. Finally, the UAV transmis-
sion efﬁciency is enhanced by the use of small-form-factor multi-
beam directional antennas and optimal power/link scheduling
across the scalable signal representation layers. Our experiments
demonstrate competitive advantages over conventional methods
for visual sensing. This is a ﬁrst-of-its-kind study of an emerging
application of prospectively broad societal impact.

I. INTRODUCTION

Cyber-physical/human systems (CPS/CHS) are set to play
an increasingly prominent and important role in our lives and
society, advancing research and technology across different
disciplines at the same time [1–5]. Virtual/augmented real-
ity (VR/AR) and UAV swarms are two emerging CPS/CHS
technologies of prospectively broad societal impact. VR/AR
suspends our disbelief of being there (at a remote location),
akin to virtual human teleportation [6]. Its ongoing explosion
onto the market and into the mainstream1 is a forerunner of the
opportunities lying ahead. In brief, by equipping us with the
capacity to travel virtually and apply super-human-like vision,
VR/AR can help us achieve a broad range of technological and
societal advances2. UAVs can have a similar transformative

To appear @ INFOCOM 2017.
1As evidenced by the ﬂurry of related devices/services/platforms appearing
on the market, released broadly by electronics/device manufacturers and
Internet companies [7–17], and stunning investment/acquisition deals/growth
rates [18–21]. It is expected that AR/VR revenue will hit $150B by 2020,
disrupting the mobile market along the way [22].

2Seeing from multiple 3D perspectives, around/through obstacles, and
in ways not possible before will
lead to knowledge discovery that will
considerably enhance human/machine perception and decision making in
diverse tasks [23–34]. These gains will stimulate societal-scale applications
that will enhance energy conservation, quality of life, and the global economy.

impact on remote sensing applications, by lowering their
cost/extending their scope [35].

We envision a future where UAV-deployed VR/AR im-
mersive communication will break existing barriers in re-
mote sensing, monitoring, localization, navigation, etc. The
thereby achieved advances will beneﬁt diverse applications
spanning the environmental/weather sciences, public/national
safety, disaster relief, and transportation. By dispensing with
the need for our physical presence (transportation) there, they
will simultaneously make impact on climate change 3.

Fig. 1. VR/AR immersive communication via UAV IoT aerial sensing.

In this paper, we study UAV IoT aerial sensing that delivers
multiple VR/AR immersive communication sessions to remote
users, as illustrated in Figure 1. The UAV swarm is spatially
distributed over a wide area of interest, where each UAV i
records its own viewpoint Vi of the scene below it. Each
remote VR/AR user is interested in immersive navigation
of a subarea of speciﬁc interest to him/her. The aggregate
communication capacity of the sensing system is C and
needs to support the delivery of K 360◦-navigable 3D video
representations of the K subareas of interest, constructed from
the UAV-captured viewpoint data {V1, . . . , VN }.

The reconstruction quality of each subarea VR/AR visual
representation will depend on the speciﬁc set of captured
viewpoints and the sampling rate Ri at which each Vi has
been acquired. Our objective will be to ﬁnd the optimal
set of sampling rates for the UAV swarm such that
the
aggregate priority-weighted reconstruction quality across all
VR/AR sessions is maximized. At the same time, the minimum
reconstruction quality requirements for the VR/AR sessions

3According to the U.S. Environmental Protection Agency, human trans-
portation contributes to 1/3 of our carbon emission footprint [36]. Climate
and weather disasters attributed to climate change alone cost the American
economy more than $100B in 2012 [37].

 
 
 
 
 
 
should be met, as well, with the selected optimal sampling
rates. Furthermore, we design an optimal scalable source-
channel signal representation that instills into the captured
data inherent rate adaptivity, unequal error protection, and
minimum required redundancy. Finally, the UAV transmission
efﬁciency is enhanced by the use of small-form-factor multi-
beam directional antennas and optimal power/link scheduling
across the scalable signal representation layers.

Due to the complex interdependencies of these three system
blocks, in this preliminary study, we analyzed and optimized
each individually, while still keeping them coupled in their
design and operation through the selected sampling/encoding
rates that ﬂow through each. Motivated by our preliminary
results and advances, we plan to investigate a tighter integra-
tion of the three blocks into a unifying framework, as part
of our ongoing/future work efforts, and analytically quantify
the beneﬁts versus drawbacks of such an approach. Similarly,
we plan to derive further theoretical insights and analysis of
our optimization methods as part of a follow-up study, which
could not be included here due to limited space.

In the remainder, we ﬁrst provide an overview of related
work in Section II. Then, we describe our system modeling
in Section III, while in Section IV we provide the problem
formulation, subsequently. Analysis and optimization solution
of the problem are provided in Section V. Our scalable
source-channel signal representation is introduced in Sec-
tion VI, while our transmission power layered scheduling via
multi-beam directional antennas is presented in Section VII.
Comprehensive experimental analysis and benchmarking are
provided in Section VIII, and we conclude in Section IX.

II. RELATED WORK

VR/AR immersive communication via UAV IoT sensing is
a new topic that has not been studied before. Closely related
areas include ground-based multi-camera wireless sensing for
multi-view systems [38], immersive telecollaboration [25, 26,
39], multi-view video coding/communication [40–43], and
360◦ video streaming [44, 45]. Another related area is graph-
based signal processing for time-varying point clouds used in
AR applications [46, 47].

III. SYSTEM MODEL

Let V = {V1, . . . , VN } be the collection of aerial view-
points captured by the UAVs. There are K remote AR/VR
sessions delivered to users interested in immersive visual nav-
igation of different subareas (scenes) covered by the UAV ﬂeet.
Let Ck, for k = 1, . . . , K, denote the subset of UAVs covering
subarea k. For every UAV ν ∈ V (the association between
captured viewpoints and UAVs is unique), let Rν denote the
temporal (data) sampling rate used in capturing viewpoint ν.
The aggregate transmission capacity that the UAV swarm can
use to relay the captured data is C. The AR/VR application
constructs a 3D 360◦-navigable video representation of the
scene of interest for user k from the subset of captured
viewpoints Ck for that subarea. Let Rk = {Rν|ν ∈ Ck}
denote the set of sampling rates used by the UAVs from

cluster Ck to capture their respective viewpoints. Finally, let
R = (R1, . . . , RN ) denote the aggregate sampling rate vector.
We consider that each UAV will encode its captured view-
point data Vi into a scalable source-channel signal representa-
tion featuring L embedded data layers and an aggregate data
rate Ri. The description of this process and its optimization
are described later in Section VI. A UAV will then communi-
cate its scalable signal representation towards the destination
VR/AR user using its multi-beam directional antenna and
optimal power scheduling/link assignment across the scalable
signal representation layers. The description of this process is
postponed for Section VII.

IV. SPACE-TIME SIGNAL SAMPLING/SENSING

Let Dk denote the reconstruction error of scene k ob-
served by the remote VR/AR user. We deﬁne it as Dk =
wvDv(Rk), where Vk denotes the aggregate view space
Rv∈Vk
for the scene, Dv is the reconstruction error for an arbitrary
viewpoint v ∈ Vk, and wv is the popularity (relevance)
of viewpoint v for the user4. In words, Dk represents the
aggregate reconstruction error observed by the user across
Vk. Virtual (non-captured) views v ∈ Vk are synthesized
for the user from the captured data Ck via geometric signal
processing5 (DIBR [50]). Let D0
k represent an upper bound
(constraint) on the reconstruction error for session k imposed
by the VR/AR application requirements.

The problem of interest can then be formulated as

γkDk(Rk),

min
R X
k
Ri ≤ C and Dk ≤ D0

k, ∀k,

subject to: X
i

(1)

where γk represents the prospective mission/application pri-
ority placed on session k by the system. Note that R in
essence samples the signal sensing locations {Vi} across time
and space, simultaneously. In particular, Ri = 0 signiﬁes
that signal/location Vi is not sensed (at all). Note also that
Dk integrates the impact of the latency and interactivity
requirements of the VR/AR application.

V. ANALYSIS AND OPTIMIZATION

In our recent work [41, 51], we have shown that when
virtual view v ∈ Vk is interpolated via DIBR from the two
nearest sampled viewpoints Vi, Vj ∈ Vk, its reconstruction
error Dv(Rk) can be represented as a linear function of
DVi(Ri) and DVj (Rj), each multiplied by polynomial powers
of x, the relative position of v with respect to them.

Armed with this representation of Dv(Rk), for virtual views
v, we can reformulate the objective in (1) by grouping terms
associated with Di(Ri) = DVi(Ri), ∀i, to have

4This quantity stems from the navigation pattern of the user for the scene.
5This operation requires scene depth signals that can be captured via IR
sensors [48] or estimated from the captured viewpoints (color signals) [49].

min
R X
i

αiDi(Ri),

subject to: X
i

Ri ≤ C and Dk ≤ D0

k, ∀k,

(2)

Our embedded signal representation offers three advantages:
inherent adaptability to dynamic network links, minimum
required redundancy (to overcome transmission loss)6, and
inherent unequal error protection, all due to its design. The
optimization problem that each UAV i will consider here is

where the weights αi ≥ 0 represent the aggregated terms.

Note that (2) now represents a convex optimization problem
[52]. In particular, the weights αi can be normalized to add to
one, and the functions Di(Ri) can be accurately represented
as concave functions exp(−βiRi) [53]. Thus, (2) can be
efﬁciently solved using convex optimization methods [52].

VI. SCALABLE SOURCE-CHANNEL REPRESENTATION

To integrate the impact of unreliable/dynamic network links
into the analysis, we design a scalable source-channel repre-
sentation for the captured viewpoints {Vi}, as follows. First,
the sampled Vi data is encoded by the respective UAV into
a set of embedded layers, illustrated in Figure 2, featuring
incrementally increasing levels of ﬁdelity with the layer index l
[54]. This will allow for inherent adaptation of the transmitted
data to prospective bandwidth variations on the aerial network
links, as the former is communicated towards the remote user.

Fig. 2. Scalable sampled viewpoint Vi signal representation.

Second, to protect against prospective transmission errors
(packet loss) on the aerial links, each UAV applies efﬁcient
rateless random linear coding [55] to its encoded viewpoint
data, using the construction procedure from Figure 3.

In particular, L transmission windows are constructed,
where window l represents the aggregate collection of the ﬁrst
l layers of the encoded captured viewpoint. Coded symbols
are constructed from each window using random linear com-
binations of the source symbols. Finally, a coded symbol is
selected for transmission from window l with probability λl.

Fig. 3. Rateless random linear channel coding.

l

min
i},{λl}

{Rl

X
l

E[Di(

X
k=1

Rk

i ), {λl}],

(3)

subject to: X
l

Rl

i ≤ Ri and X
l

λl = 1, ∀k,

i is the data rate of layer l and E[Di(P

where Rl
i )|{λl}]
is the expected reconstruction error of viewpoint Vi, when
the ﬁrst l layers has been received/decoded by the destination
VR/AR user/session7.

l
k=1 Rk

Solving (3) is complex due to its nature. We design a
coordinate descent algorithm to solve it iteratively, at low
complexity. In particular, for ﬁxed {λl}, (3) transforms into

l

min
{Rl
i}

X
l

Di(

X
k=1

Rk

i ), subject to: X
l

Rl

i ≤ Ri,

(4)

which again is a convex optimization problem that can be
solved efﬁciently. The algorithm operates by adjusting {λl}
iteratively, in between every solution of (4), until convergence.
The adjustment is carried out such that the objective in (4) is
minimized, every time {λl} is updated. Formal description of
our algorithm is provided in Algorithm 1.

i} from (4)

λi = λi − ∆λ; λi+1 = λi+1 + ∆λ
Assign D = objective in (3)
if D < Dmax then
Dmax = D
FLAG1 = 1

Algorithm 1 Low-complexity coordinate descent
1: Initialize {λl} = {1, 0, . . . , 0}; Compute {Rl
2: Set ∆λ; Assign Dmax = objective in (3)
3: for i = 1 to L − 1 do
FLAG1 = 0
4:
repeat
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
end if
18:
19: end for

λi = λi + ∆λ; λi+1 = λi+1 − ∆λ
break

until λi ≤ ∆λ
if FLAG1 = 0 then

end if

break

else

Convergence of the algorithm is ensured, as the objective
in (3) is positive, bounded from below by zero, and monoton-
ically decreasing at every iteration.

6Our channel coding approach will allow UAV i to maintain its aggregate
source-channel data rate close to Ri, i.e., it will result in very little overhead.
7The probability of this event can be denoted as Pl({λl}, Ri) [55] and
i )|{λl}] = Pl({λl}, Ri)Di(Pl

thus E[Dl

k=1 Rk

k=1 Rk

i(Pl

i ).

VII. LAYERED DIRECTIONAL NETWORKING

We enhance the UAV transmission power/spectrum efﬁ-
ciency via smart multi-beam directional antennas [56, 57] and
layered scheduling. Simultaneously, this will enhance the inter-
activity and quality of experience of the VR/AR applications,
as it will reduce the transmission latency to them.

Let L denote the number of beams/network links that the
UAV antenna can establish towards its destination. Recall that
L also denotes the number of scalable signal representation
layers constructed in Section VI. Now,
let pl denote the
transmit power allocation to link l. Coded symbols from
transmission window l in Section VI are assigned to the
corresponding network link for transmission. UAV i assigns
its power budget pT according to the following optimization

min
{pl}

X
l

pl, subject to: X
l

pl ≤ pT and r(pl) = Rl

i, ∀l,

(5)

where r(pl) denotes the data rate on link l enabled by pl.
(5) also represents a convex optimization problem that can be
solved efﬁciently. The optimization is illustrated in Figure 4.

apply our optimization framework components via numerical
simulations. We set K = 6 equal priority (γk = 1) VR/AR
sessions delivered remotely. Their subareas/scenes of interests
are spatially distributed across the area under the UAV swarm
such that every Ck comprises at least three captured view-
points Vi covering it. The viewpoint popularity distribution wv
for scene Ck was selected to be Gaussian over Vk, matching
the ﬁeld of view of the VR/AR head-mounted displays.

We designed a reference system to compare to, as follows.
The UAV swarm samples the captured viewpoints at equal rate
Ri = C/N (uniform allocation). Conventional state-of-the-art
video encoding is employed to represent the captured data at
each UAV [60]. Conventional state-of-the-art (Reed-Solomon)
forward error correction is applied to the captured data at ﬁxed
rate (3%) [61]. The reference system is denoted as Baseline
and our optimization framework as Optimal, henceforth.

Scaling behavior. In Figure 5, we study how the optimiza-
tion (1) behaves as we scale C. In particular, let DS denote
the objective function in (1). We record its values for the
optimal sampling rate vector R, as we vary C. We plot the
resulting dependency DS versus |R| (where |R| ∼ C), param-
eterized by σz, the standard deviation of the surface f (x, y)
representing the 3D geometry of the area under the UAV
swarm (we can control this quantity in laboratory settings).
We can observe two distinct operating regimes (modes). For

Fig. 4. Optimal link assignment and transmit power scheduling.

Note that we consider that pT is much larger than the
minimum required power to transmit the aggregate sampling
rate Ri of UAV i. This is a reasonable assumption that
any UAV battery can easily meet, given its much higher
energy volume. In particular, the energy consumption for UAV
propulsion is a few orders of magnitude larger than that for
sensing/communication.

VIII. EXPERIMENTS

We carry out experiments to evaluate the effectiveness of
our framework. They involve aerial viewpoint data captured
in both, a controlled laboratory setting and an outdoor (in-
the-wild) environment [58]. For reproducibility and scientiﬁc
analysis, our experimentation methodology was carried as
follows. Viewpoint data was acquired using DJI Phantom 4
UAVs equipped with 4K cameras and 4G LTE dongles [59].
Since this is a proof-of-concept study of a novel application
and given the limited space here, we only considered a
one-hop aerial network in our experiments. That
is, each
UAV is directly connected to a ground-based base station to
which it sends its data. We collected measurements of the
wireless link to the base station for different UAV locations.
These traces and the UAV captured data were then used to

Fig. 5. Scaling behavior of the optimal sampling vector R.

2

z )|R|−c2(1/σ

smaller |R|, DS scales as c1(σ2
z ), where c1 and
c2 are two constants that depend on σz. We denote this mode
geometry coding, since the optimal sensing policy emphasizes
the ﬁdelity of the representation of f (x, y) here, due to its
impact on DS. This is accomplished by prioritizing sampling
more viewpoints ν ∈ V at the expense of the data/temporal
sampling rate assigned to each such ν. Note that the higher the
innovation rate (or entropy) of f (x, y) across the imaged area,
as embodied by σz, the higher DS is, as seen from Figure 5.
Once |R| is sufﬁciently large and f (x, y) can be represented
at sufﬁciently high ﬁdelity, we observe a transition to another
operating regime, where DS now scales as c3(σ2
c ),
and c3/c4 are again constants that depend on σc (the standard
deviation of the scene color values). We denote this mode

c )|R|−c4(1/σ

2

texture coding, as the optimization emphasizes here the color
value representation ﬁdelity for every triple (x, y, f (x, y)).
This is accomplished by emphasizing temporal sampling at
the expense of the number of sampled viewpoints. Still, the
impact of σc is much smaller here and all three DS versus
|R| graphs eventually converge8.

Sampling Efficiency: Reconstruction Quality Vs. Sampling Capacity

40

38

36

34

32

30

]

B
d
[

)

R
N
S
P
−
Y

(

y
t
i
l

a
u
q

n
o
i
t
c
u
r
t
s
n
o
c
e
r

e
n
e
c
S

28
4

6

8

Optimal
Baseline

16

18

20

10

12
Sampling capacity C (Mbps)

14

Fig. 6. Reconstruction quality versus sampling capacity (outdoors).

Sampling efﬁciency. In Figure 6, we examine the sampling
efﬁciency of our framework relative to the baseline reference.
In particular, we graph the achieved expected scene recon-
struction quality (the inverse of the objective in (1)) versus the
available sensing capacity, for the two methods. It can be seen
that the optimization is able to leverage the available resources
much more effectively, enabling considerable reconstruction
quality gains over the baseline system, for the same C.
Furthermore, we can see that Optimal is able to upscale its
performance at much higher rate as C is increased, relative
to Baseline. The equivalent results for the indoor data look
similar. To conserve space, we do not include them here.

Signal Representation Adaptivity: Reconstruction Quality Vs. Sampling Capacity Mismatch

0

−1

−2

−3

−4

−5

−6

−7

]

B
d
[

)

R
N
S
P
−
Y
∆
(
n
o
i
t
c
u
d
e
r

y
t
i
l

a
u
q

n
o
i
t
c
u
r
t
s
n
o
c
e
r

e
n
e
c
S

−8
0

1

2

Optimal
Baseline

8

9

10

3
7
5
Sampling capacity mismatch ∆C (%)

4

6

Fig. 7. Signal representation adaptivity versus capacity mismatch ∆C (%).

Signal representation adaptivity/reliability. We carry out
two experiments here to examine the adaptivity of our signal
representation design to sampling capacity mismatch and its
robustness to unreliable transmission, respectively. In partic-
ular, in the ﬁrst experiment, we consider that the captured

8σQ is the quantization step size used to encode the captured {Vi} [62].

viewpoint data {Vi} has been encoded for target capacity C0
(12 MBps here), however, the actual available capacity C at
which it can be relayed to the destination VR/AR sessions
is smaller. We measure the expected scene reconstruction
quality achieved thereby, for different values of the mismatch
∆C, expressed in percent of C0. These results are shown
in Figure 7. We can see the our approach offers a graceful
degradation as ∆C is increased. In fact, our signal design does
not exhibit any notable reduction in representation efﬁciency
had the captured data been encoded instead for target sampling
capacity C − ∆C, as the comparison with the corresponding
graph from Figure 6 shows. On the other hand, even a small
mismatch can degrade the end-of-the-end performance of the
baseline method considerably, as is evident from Figure 7,
due to the lack of adaptability of the conventional signal
representation it employs.

Signal Representation Reliability: Reconstruction Quality Vs. Packet Loss Rate

38

36

34

32

30

28

26

24

]

B
d
[

)

R
N
S
P
−
Y

(

y
t
i
l

a
u
q

n
o
i
t
c
u
r
t
s
n
o
c
e
r

e
n
e
c
S

22
1

2

3

4

Optimal
Baseline

7

8

9

10

5

6

Packet loss rate ε (%)

Fig. 8. Signal representation reliability versus packet loss rate ǫ (%).

The second experiment measures the achieved reconstruc-
tion quality across the VR/AR sessions as the average packet
loss rate ǫ on the aerial network links is increased. These
results are shown in Figure 8. We can see that our source-
channel signal representation offers robustness and intrinsic
adaptation to varying packet loss, without penalizing the end-
to-performance. On the other hand, the baseline reference
exhibits a cliff-effect, typical for conventional error protection
[63], beyond which the reconstruction quality dramatically
degrades, as the packet loss increases, as shown in Figure 8.
Transmission power/latency reduction. We measured in
our experiments that our framework consumes approximately
30% less transmission power relative to the baseline reference.
This observation is in line with results reported in prior
studies of directional networking [64]. Simultaneously, we also
observed that the delivery latency for the VR/AR applications
has been reduced by 25%. These advances make the practical
deployment of our framework more feasible.

IX. CONCLUSION

We explored UAV IoT aerial sensing for VR/AR immersive
communication to remote users in a ﬁrst-of-its kind study
on a novel topic of prospectively broad societal connotations.
The remote users are interested in visual immersive navigation

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
of speciﬁc subareas/scenes of interest, reconstructed on their
respective VR/AR devices from the aerial viewpoint data cap-
tured by the UAV swarm. We designed an optimization frame-
work that comprises three components that synergistically
operate to maximize the end-to-end performance across the
VR/AR sessions delivered to the users. In particular, we ﬁrst
derived the optimal sampling rates to be used by each UAV,
such that the related system and application constraints are
not exceed, while the priority weighted reconstruction quality
across all VR/AR sessions is maximized. Then, we formulated
an optimal scalable source-channel signal representation that
imbues into the captured data inherent rate adaptivity, unequal
error protection, and minimum required redundancy. Finally,
we enhanced the UAV transmission efﬁciency via small-form-
factor multi-beam directional antennas and optimal power/link
scheduling across the scalable signal representation layers.
Our experiments demonstrate competitive advantages over
conventional methods employed in practice for visual sensing.
The demonstrated advances are very promising and motivate
us to proceed with a follow-up study, in which we plan to
pursue closer integration of our optimization methods and
further analysis of their operation.

REFERENCES

[1] NSF CPS/CHS Programs. https://www.nsf.gov/
[2] NIST CPS Program. http://www.nist.gov/cps/
[3] Proceedings of the IEEE, Jan. 2012, Special Issue on CPS.
[4] ACM Transactions on Cyber-Physical Systems. http://tcps.acm.org/
[5] IEEE Transactions Human-Machine Systems. http://www.ieeesmc.org/
[6] J. Apostolopoulos, et al., “The road to immersive communication,” Proc.

of the IEEE, Apr. 2012.

[7] HTC Vive: This is REAL. Discover VR beyond imagination. [Online].

Available: https://www.htcvive.com/us/

[8] Sony PlayStation VR. https://www.playstation.com/
[9] Ricoh Theta: 360◦ degree experience. https://theta360.com/en/
[10] Samsung Gear 360. http://www.samsung.com/global/galaxy/gear-360/
[11] Microsoft HoloLens AR Holographic display.

[Online]. Available:

https://www.microsoft.com/microsoft-hololens/en-us

[12] Magic Leap Mixed Reality Lightﬁeld. https://www.magicleap.com
[13] Oculus Rift and Gear VR. https://www.oculus.com/
[14] Google Cardboard VR. https://vr.google.com/cardboard/
[15] Facebook 360: Interactive and Immersive. http://facebook360.fb.com
[16] Jump – Google VR platform. https://vr.google.com/jump/
[17] B. Darrow, “Project Tango is Google Maps on Steroids,” Fortune

Magazine, May 2016.

[18] P. Rubin, “Oculus Raises $75 Million for the VR Goggles of the

Future,” Wired Magazine, Dec. 2013.

[19] M. Isaac, “Magic Leap, an Augmented Reality Firm, Raises $793

Million,” The New York Times, Feb. 2016.

[20] “AR/VR M&A Timeline: Facebook, GoPro, HP, Apple Begin To Grab

Startups,” CB Insights, Jul. 2016. https://www.cbinsights.com/

[21] “Record $2 billion AR/VR investment in last 12 months,” Digi-Capital

Research, Jul. 2016. http://www.digi-capital.com/

[22] T. Merel, “Augmented and virtual reality to hit $150 billion, disrupting

mobile by 2020,” Tech Chrunch Online Magazine, Apr. 2015.

[23] A. Servetti and E. Masala, “Smartphone-based 3D real-time vision
system for teleoperation,” in Proc. IEEE ICME Workshops, Jul. 2013.
[24] E. Masala, “Sensor-based real-time adaptation of 3D video encoding

quality for remote control applications,” in Proc. IEEE MMSP 2013.

[25] R. Vasudevan, et al., “Real-time stereo-vision system for 3D teleimmer-

sive collaboration,” in Proc. IEEE ICME, Jul. 2010.

[26] O. Schreer, et al., “3DPresence - A system concept for multi-user and
multi-party immersive 3D videoconferencing,” in Proc. European Conf.
Visual Media Production, London, UK, Nov. 2008, pp. 1–8.

[27] J.-H. Kim, et al., “Modeling of driver’s collision avoidance maneuver
based on controller switching model,” IEEE TSMC, Part B, Dec. 2005.

[28] D. Weidlich, et al., “Machine design analysis using VR/AR visualiza-

tion,” IEEE Computer Graphics and Applications, Sep. 2008.

[29] A. Sourin, et al., “Virtual orthopedic surgery training,” IEEE Computer

Graphics and Applications, May 2000.

[30] N. Snavely, et al., “Scene reconstruction and visualization from com-

munity photo collections,” Proc. of the IEEE, Aug. 2010.

[31] Y. Wu, C. Liu, S. Lan, and M. Yang, “Real-time 3D road scene based
on virtual-real fusion method,” IEEE Sensors Journal, Feb. 2015.
[32] M. A. P. Sotelo, et al., “3-D model-based multiple-object video tracking

for treatment room supervision,” IEEE TBME, Feb. 2012.

[33] W.-K. Wong, et al., “A quasi-spherical

triangle-based approach for
efﬁcient 3-D soft-tissue motion tracking,” IEEE/ASME Transactions on
Mechatronics, vol. 18, no. 5, pp. 1472–1484, Oct. 2013.

[34] C. Yang, G. Cheung, and V. Stankovi´c, “Estimating heart rate via depth

video motion tracking,” in Proc. IEEE ICME, Jun. 2015.

[35] Agricultural Drones: MIT Technology Review: 10 Breakthrough

Technologies 2014.

[36] United States Environmental Protection Agency, “U.S. Greenhouse Gas

Inventory Report: 1990-2014,” Apr. 2016.

[37] The White House. Climate Change Action Plan. [Online]. Available:

http://www.whitehouse.gov/climate-change/

[38] J. Chakareski, “Uplink scheduling of visual sensors: When view popu-

larity matters,” IEEE Trans. Communications, Feb. 2015.

[39] M. Hosseini et al., “Coordinated bandwidth adaptations for distributed
3D tele-immersive systems,” in Proc. ACM MMVE Workshop 2015.
[40] G. Cheung, et al., “Interactive streaming of stored multiview video using

redundant frame structures,” IEEE TIP, Mar. 2011.

[41] J. Chakareski, et al., “User-action-driven view and rate scalable multi-

view video coding,” IEEE TIP, Sep. 2013.

[42] J. Chakareski, “Wireless streaming of interactive multi-view video via
network compression and path diversity,” IEEE TCOM, Apr. 2014.
[43] J. Chakareski, et al., “View-popularity-driven joint source and channel
coding of view and rate scalable multi-view video,” IEEE J. Selected
Topics in Signal Processing, Apr. 2015.

[44] F. Qian, et al., “Optimizing 360 video delivery over cellular networks,”

in Proc. ACM Workshop All Things Cellular, Oct. 2016.

[45] M. Hosseini and V. Swaminathan, “Adaptive 360 VR video streaming:
Divide and conquer!” in Proc. IEEE Int’l Symp. Multimedia 2016.
[46] H. Q. Nguyen, et al., “Compression of human body sequences using
graph wavelet ﬁlter banks,” in Proc. IEEE ICASSP, May 2014.
[47] D. Thanou, et al., “Graph-based compression of dynamic 3D point cloud

sequences,” IEEE Trans. Image Processing, Apr. 2016.

[48] S. Gokturk, et al., “A time-of-ﬂight depth sensor – system description,

issues and solutions,” in IEEE CVPR Workshop, Jun. 2004.

[49] M. Tanimoto, et al., “Multi-view depth map of Rena and Akko & Kayo,”

ISO/IEC JTC1/SC29/WG11 MPEG M14888, Oct. 2007.

[50] H.-Y. Shum, et al., Image-Based Rendering, Springer, Sep. 2006.
[51] V. Velisavljevi´c, J. Chakareski, and G. Cheung, “Bit allocation for
multiview image compression using cubic synthesized view distortion
model,” in Proc. IEEE Hot3D Workshop, Jul. 2011.
[52] S. Boyd and L. Vandenberghe, Convex Optimization.

Cambridge

University Press, 2004.

[53] M. Chen, et al., “Utility maximization in peer-to-peer systems,” in Proc.

ACM SIGMETRICS, Jun. 2008.

[54] H. Schwarz, et al., “Overview of the scalable video coding extension of

the H.264/AVC standard,” IEEE TCSVT, Sep. 2007.

[55] D. Vukobratovi´c and V. Stankovi´c, “Unequal error protection random
linear coding strategies for erasure channels,” IEEE TCOM, May 2012.
[56] S. Atmaca, et al., “A new QoS-aware TDMA/FDD MAC protocol with
multi-beam directional antennas,” Comp. Standards & Interfaces, 2009.

[57] Gigabit Multi-Beam Directional Antennas. https://www.cubic.com/
[58] “VIRAT Video Dataset.” [Online]. Available: http://www.viratdata.org/
[59] DJI Camera Drones/Quadcopters. http://www.dji.com/
[60] T. Wiegand, et al., “Overview of the H.264/AVC video coding standard,”

IEEE TCSVT, Jul. 2003.

[61] S. Lin and D. J. Costello, Jr., Error Control Coding: Fundamentals and

Applications. Englewood Cliffs, NJ: Prentice-Hall, 1984.

[62] A. Gersho and R. M. Gray, Vector quantization and signal compression,

1st ed. Springer, Nov. 1991.

[63] S. Rane, et al., “Systematic lossy error protection of video signals,”
IEEE Trans. Circuits and Systems for Video Technology, Oct. 2008.
[64] A. Spyropoulos and C. Raghavendra, “Energy efﬁcient communications
in ad hoc networks using directional antennas,” in Proc. IEEE INFO-
COM, Jun. 2002.

