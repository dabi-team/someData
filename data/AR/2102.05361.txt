1
2
0
2

b
e
F
0
1

]

C
H
.
s
c
[

1
v
1
6
3
5
0
.
2
0
1
2
:
v
i
X
r
a

Towards Tangible Cultural Heritage Experiences - Enriching
VR-Based Object Inspection with Haptic Feedback

STEFAN KRUMPEN, Institute of Computer Science II, University of Bonn, GERMANY
REINHARD KLEIN, Institute of Computer Science II, University of Bonn, GERMANY
MICHAEL WEINMANN, Institute of Computer Science II, University of Bonn, GERMANY

Fig. 1. Illustration of our VR-Based Object Inspection system with haptic feedback.

VR/AR technology is a key enabler for new ways of immersively experiencing cultural heritage artifacts based on their
virtual counterparts obtained from a digitization process. In this paper, we focus on enriching VR-based object inspection by
additional haptic feedback, thereby creating tangible cultural heritage experiences. For this purpose, we present an approach
for interactive and collaborative VR-based object inspection and annotation. Our system supports high-quality 3D models
with accurate reflectance characteristics while additionally providing haptic feedback regarding the object shape features
based on a 3D printed replica. The digital object model in terms of a printable representation of the geometry as well as
reflectance characteristics are stored in a compact and streamable representation on a central server, which streams the data to
remotely connected users/clients. The latter can jointly perform an interactive inspection of the object in VR with additional
haptic feedback through the 3D printed replica. Evaluations regarding system performance, visual quality of the considered
models as well as insights from a user study indicate an improved interaction, assessment and experience of the considered
objects.

CCS Concepts: • Computing methodologies → Virtual reality; Perception; Reflectance modeling.

Authors’ addresses: Stefan Krumpen, krumpen@cs.uni-bonn.de, Institute of Computer Science II, University of Bonn, Endenicher Allee 19a,
Bonn, GERMANY, 53115; Reinhard Klein, rk@cs.uni-bonn.de, Institute of Computer Science II, University of Bonn, Endenicher Allee 19a,
Bonn, GERMANY, 53115; Michael Weinmann, mw@cs.uni-bonn.de, Institute of Computer Science II, University of Bonn, Endenicher Allee
19a, Bonn, GERMANY, 53115.

Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that
copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first
page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy
otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from
permissions@acm.org.
© 2021 Association for Computing Machinery.
XXXX-XXXX/2021/0-ART0 $15.00
https://doi.org/XX.XXXX/XXXXXXX.XXXXXXX

ACM J. Comput. Cult. Herit., Vol. 0, No. 0, Article 0. Publication date: 2021.

 
 
 
 
 
 
0:2

• Krumpen et al.

Additional Key Words and Phrases: tangible cultural heritage, reflectance, bidirectional texture functions, haptic feedback, 3D
printing

ACM Reference Format:
Stefan Krumpen, Reinhard Klein, and Michael Weinmann. 2021. Towards Tangible Cultural Heritage Experiences - Enriching
VR-Based Object Inspection with Haptic Feedback. ACM J. Comput. Cult. Herit. 0, 0, Article 0 ( 2021), 19 pages. https:
//doi.org/XX.XXXX/XXXXXXX.XXXXXXX

1

INTRODUCTION

The potential of the rapidly emerging VR/AR technology lead to a new paradigm of presenting cultural heritage
contents to the public and opens new opportunities regarding their analysis by expert and non-expert users.
Instead of an inspection and analysis solely based on depictions in static images, presenting digitized CH artifacts
in terms of fully immersive and interactive experiences comes into reach. The ability of creating an immersive
experience of digitized artifacts is particularly relevant for the less stressful analysis/inspection of highly fragile
objects, that may easily take damage during inspection, analysis or transport. Furthermore, several artifacts are
susceptible to certain environment conditions such as temperature, humidity or illumination conditions. Hence,
it is difficult or even impossible to include them in public exhibitions and complicates their use for scientific
or educational purposes. Instead, using accurately digitized virtual counterparts allows avoiding the use of the
real artifact during inspection, analysis and transport and, additionally, opens new possibilities for simultaneous
artifact inspection such as required for virtual museum experiences or collaborative analysis by experts situated
at different physical locations. However, the realization of such a virtual, optionally collaborative, object analysis
and annotation depends on several crucial factors that have to be met to allow a pleasing experience while
preserving as much of the aura of CH contents [2] as possible. These challenges include the accurate object/scene
digitization and representation, where both geometric object details as well as characteristic details of material
appearance like reflectance behavior at scratches, engravings, etc. have to be captured, as well as intuitive
interaction metaphors. State-of-the-art reflectance representations based on memory-consuming bidirectional
texture functions (BTFs) [25, 43, 44, 49, 50] allow for an accurate virtual model of an object, but require powerful
hardware for real-time rendering, whereas analytical reflectance models come at less computational effort and
faster rendering. This comes at the cost of often not sufficiently capturing relevant reflectance characteristics
such as light exchange at fine structural details or local subsurface scattering. Furthermore, the presentation of
digitized contents on VR/AR devices imposes severe constraints regarding real-time rendering at framerates
beyond 60 Hz on the visualization devices, instant/high-speed data exchange and interaction mechanisms to
achieve a realistic impression for the joint analysis and interaction with the object or scene. Despite this progress
on representing and visualizing digitized objects, their visualization on screens or VR/AR glasses is impacted by
the interaction devices (i.e. mouse, keyboard, controller, etc.) that limit the experience regarding the intuitive
canonical interaction in terms of touching and rotating an object in your hands. In this paper, we focus on
enhancing the perceived aura of the digitized objects by augmenting the user experience with sensual information
beyond purely sight-related data and providing additional haptic feedback regarding the object shape. For this
purpose, we present a practical tool for collaborative remote object analysis and annotation involving both the
accurate depiction of the digitized artifacts at interactive framerates within VR devices and haptic feedback
based on a 3D printed replicate to improve the inspection experience. Using current VR devices, several experts
located at possibly different physical places access and interact with high-quality virtual counterparts of real
artifacts, where the reflectance data and the object geometry are stored on a central server and streamed to the
remote clients as needed. Our evaluation demonstrates that providing haptic feedback regarding the object shape
enhances VR-based interaction, assessment and experience of the considered objects. In summary, the main
contributions of this work are:

ACM J. Comput. Cult. Herit., Vol. 0, No. 0, Article 0. Publication date: 2021.

Towards Tangible Cultural Heritage Experiences

•

0:3

• We present a framework uniting the concepts of digital material appearance, VR technology and 3D printing
that is designed for the needs of enhanced remote collaborative inspection of cultural heritage contents.
• Our interactive multi-client inspection and annotation tool allows multiple users at different physical places
to collaboratively analyze objects based on various interaction metaphors for annotation and illuminating
the object.

• We demonstrate the potential of our approach in the scope of a user study.

2 RELATED WORK

Immersive Content Presentation. Facilitating the access to CH contents by digital representations has been
approached in various forms. As the most obvious form given by still images or videos limits the user experience
to pre-specified viewpoints and environment conditions (including the lighting conditions), the full aura of
CH content is not preserved. Instead, digitizing the 3D geometry based on laser scanning or structured light
scanning and additionally capturing a digital representation of the reflectance behavior allows the manipulation
of the environment and lighting conditions. Beyond interactive screen-based visualizations, a higher degree of
immersion is achieved based on interactive 3D experiences of the CH objects/scenes. The range from real-world
representations/environments to completely virtual representations/environments has been discussed under the
notion of reality-virtuality continuum [30]. For a detailed discussion of respective developments in the area of
cultural heritage, we refer to the survey of by Bekele et al. [1].

In VR settings, users are completely immersed into synthetic or pre-captured worlds that differ from the actual
surrounding [3, 57]. Among the major applications for experiencing, exploring and manipulating CH contents
are virtual museums and education. Augmented Virtuality aims at augmenting such completely virtual worlds
with live real-world contents. In contrast, Augmented Reality (AR) enriches the actual real-world surrounding
with synthetic contents as used for exhibition enhancement, education or exploration.

For all these approaches, interactions with the scene and its objects are realized based on the interplay between
visual depictions and the control devices of VR/AR equipment. However, these interfaces based on conventional
VR/AR controllers, gamepads or gloves do not transport tactile feedback when interacting with objects. Seminal
work on tangible interfaces has been presented in terms of the Tooteko framework [5] where facade reliefs were
also presented to users as 3D printed objects lying on a table to allow haptic experience. Touching different surface
parts was coupled to respective audio feedback. In contrast, our work extends VR-based 3D object inspection by
additional haptic feedback based on a 3D-printed replica.

Physical Interactions in VR. Early studies have demonstrated humans’ ability to gauge movements of their own
hands relative to their own bodies. This proprioceptive sense [31] has been explored in terms of body-relative
interaction types including the direct manipulation of objects as if these were in the user’s hands, physical
mnemonics (i.e. 3D body-relative widgets) and gesture commands. Further work focused on providing a feedback
to the user based on the shape of physical objects. In such passive haptics approaches [28], physical objects
are aligned with objects in virtual environments which has been shown to enhance user experience of virtual
environments [22]. In contrast to providing a passive haptic feedback for static phenomena in the scene like
ascending/descending stairs [35], the interaction with dynamic objects relies on carefully tracking the respective
real-world counterparts. This can be achieved based on special gloves [41] as well as attaching tracking modules
to physical objects [10, 15]. Our approach also exploits passive haptics in terms of a 3D printed virtual counterpart
of the object of interest, which is attached to a tracking module of typical VR equipment. In contrast to previous
work, our work focuses on both the accurate visual reproduction of objects within the virtual environments and
the haptic feedback regarding the object’s shapes.

ACM J. Comput. Cult. Herit., Vol. 0, No. 0, Article 0. Publication date: 2021.

0:4

• Krumpen et al.

Appearance Modeling. Object appearance, i.e. the observed colors and textures, results from the complex-
ity of light exchange at the surface depending on surface geometry, material reflectance characteristics and
the surrounding illumination conditions. As both the human visual system and acquisition devices are only
capable of observing material appearance depending on the coupling of these three modalities, appearance
modeling inevitably requires a decoupling of the respective modalities. Respective acquisition strategies have
been intensively discussed in literature [14, 53, 55]. While the geometric structure can be accurately recovered
for a wide range of materials (i.e. diffuse to moderately specular surfaces) using structured light techniques or
laser scanners [53], the separation of reflectance and illumination properties remains challenging. High-quality
object digitization of smaller figurines is therefore mostly carried out in lab environments/using setups with
controllable illumination such as gonioreflectometers [9, 20] or camera arrays [17–19, 23, 38, 39, 49, 50, 52].
Exploiting controllable illumination allows to accurately model reflectance in terms of parametric spatially
varying bidirectional reflectance distribution (SVBRDF) models [9, 18, 19, 21, 23, 26, 27, 36–40, 46] or data-driven
bidirectional texture functions [6, 17, 25, 49, 50] that both represent spatially varying surface reflectance depend-
ing on view and illumination conditions. SVBRDFs are parameterized over the exact 2D surface of the object
geometry and usually the local surface reflectance behavior is represented in terms of an analytical function.
This makes them particularly suitable for specular materials, where the highlights may be lost with a tabulated
representation. However, as the accuracy of the scanned geometry is still limited regarding fine structural details,
the light exchange may not be accurately reproduced in such cases. In contrast, BTFs are parameterized over an
approximate object surface. Hence, subtle effects of light exchange including interreflections, self-shadowing and
self-occlusions induced by finer structures not preserved in the reconstruction as well as local subsurface scatter-
ing are directly stored in terms of a data-driven image-based representation. The latter results in significantly
increasing computational and memory requirements in comparison to parametric representations. Whereas
standard BTFs are parameterized over 2D surface domains which suffers from distortions or seam artifacts, the
3D parameterization of OctreeBTFs [25] avoids distortions and significantly reduces the number of seam artifacts.
While our framework is not restricted to a particular representation, we employ the data-intensive OctreeBTF
representation [25] to demonstrate that even such expensive representations can be used for interactive object
inspection and annotation. In addition to a purely visual (in our case VR-based) experience, we provide additional
haptic feedback to enhance the user experience. Further digitization methods in the context of cultural heritage
are based on reflectance transformation imaging (RTI) [8, 11, 12, 29, 32, 33, 42]. However, these techniques usually
capture the lighting-dependent reflectance of an object only from a single view. Hence, RTI techniques are not
adequate for in-hand object inspection scenarios, where the view-dependent object appearance has to be taken
into account as well.

Rendering and Streaming of Appearance Data. Collaborative object inspection by users at different phyiscal
locations relies on efficient model representations, fast data streaming as well as rendering. Storing raw BTF
measurements comes at huge memory requirements of several tens to hundreds of gigabytes. These can be
compressed based on the combination of fitting a mixture of several SVBRDFs to the BTF data and residual
apparent BRDFs (ABRDFs) [56]. The latter are required to take non-local effects of the light transport in the
material into account as otherwise the quality of the reproduced reflectance behavior may be severely impacted.
As an alternative, several compression techniques rely on interpreting the BTF as a tensor for which a low-rank
approximation can be found via factorization. Respective techniques include Full Matrix Factorization (FMF)
[24], Decorrelated-FMF (DFMF) [48] and K-SVD based compression [45], that are capable of preserving details of
material appearance and allowing real-time rendering. However, the K-SVD approach outperforms the FMF-based
approaches by a compression factor of about 3 to 4 for comparable quality. Furthermore, investigations on
perceptually-inspired BTF compression leveraged the observation that lower downsampled levels of details
sufficiently approximate some of the factorized data, thereby reducing the memory requirements on the GPU

ACM J. Comput. Cult. Herit., Vol. 0, No. 0, Article 0. Publication date: 2021.

Towards Tangible Cultural Heritage Experiences

•

0:5

Fig. 2. Overview of our approach towards sharing tangible cultural heritage experiences. The initial digitization outputs the
object geometry, the reflectance characteristics in terms of an OctreeBTF [25]. Then, a pre-processing step (upper center)
computes a version of the geometry suitable for 3D printing and converts the BTF data into a streamable version which is
loaded by a server (bottom center). The server then streams the compressed OctreeBTF to remote clients that can interactively
inspect the object in VR, enhanced by haptic feedback provided by the 3D printed replica of the object.

wich led to compression factors of about 500. Further BTF compression schemes rely on the use of neural
networks [43, 44], where the compression and decompression are achieved based on an autoencoder approach, or
on statistical approaches [13], that allow extreme compression at the cost of loosing the capability to accurately
reproduce fine characteristics of reflectance behavior, as well as vector-quantization-based approaches [16]
or the use of multivariate radial basis functions [51], that both reach compression rates similar to FMF-based
compression. Aside from BTF compression to allow for real-time rendering, scenarios where BTFs have to be
streamed over the internet also should allow for a progressive refinement of the rendering. This can be achieved
by leveraging the fact that the used DFMF-compression orders the BTF data with respect to its importance to
stream a BTF over the internet and progressively refine the rendering when more data arrives [48]. Further
work has shown that the spatial data parts of a 2D-parameterized BTF can be split into smaller tiles that can be
streamed to the GPU independent of each other [47], which enables only loading the BTF data for the object
regions that are currently visible on the screen. In turn, this enables rendering scenes containing of multiple
BTFs in real-time, which would otherwise exceed the amount of VRAM available if each BTF would have to be
loaded completely without streaming. In our system, we combine these ideas [47, 48] to allow for a progressive
refinement of the rendering of OctreeBTFs [25], while streaming the most important data, i.e. the data that
contributes most to the final result first.

3 METHODOLOGY

Our approach for the joint immersive, tangible experience of CH contents by multiple users at possibly different
physical locations relies on a a client-server model as illustrated in Figure 2. Inputs in terms of the geometric
model as well as the reflectance characteristics (for the purpose of an accurate object representation, we use the
OctreeBTF representation [25]) of the considered models have to be obtained from a prior digitization process or a

ACM J. Comput. Cult. Herit., Vol. 0, No. 0, Article 0. Publication date: 2021.

Digitization + Client C1 + Client Cn + Client C2 Pre-processing Server Object Geometry Reflectance Characteristics … … Compression C1 C2 Cn … Streaming Progress … 3D Printable Geometry 0:6

• Krumpen et al.

model repository. In an initial step, we perform a pre-processing of these data by computing a 3D printable version
of the 3D shape and transform the OctreeBTF into a more compressed representation that allows for progressive
streaming over a network. The resulting object representation (i.e. in terms of geometry and reflectance) is stored
on the server and streamed to each remote client which renders the object on a HMD and allows for interactive
inspection enhanced with haptic feedback from a 3D printed replica. In the following sections, we discuss the
involved components in more detail. As we address the generation of highly immersive experiences of cultural
heritage objects, the objects need to be accurately captured so that characteristic details that determine the
object appearance remain preserved in the digital model. State-of-the-art capture techniques [14, 53, 55] rely on
an initial 3D scanning of the object geometry using laser scanners or structured light systems. Subsequently,
spatial variations of local surface appearance are captured under different viewing and lighting conditions.
Depending on the surface material characteristics, the dependence of the local reflectance behavior 𝑓 (x, 𝝎𝑣, 𝝎𝑙 )
at a surface point x, the viewing direction 𝝎𝑣 and the lighting direction 𝝎𝑙 can be represented using Spatially
Varying Bidirectional Reflectance Distribution Functions (SVBRDFs) [37] or Bidirectional Texture Functions
(BTFs) [6]. Our framework is not restricted to one particular representation. SVBRDFs as captured in several
other investigations [18, 19, 21, 23, 26, 27, 36, 38–40, 46] could also be integrated. However, to demonstrate the
efficiency of our approach, we conduct our experiments with the computationally more demanding OctreeBTF
representation [25] since it delivers the best visual quality of the representations mentioned above. The OctreeBTF
representation relies on parameterizing the reflectance representation in a volumetric manner instead of a 2D
parameterization over the surface which ameliorates seam artifacts and texture distortions. We use datasets
captured with a camera array [48, 50] for which surface geometry has been reconstructed using a structured
light system [54] and Octree-BTF representations have been provided by the authors of [25]. In addition to the
reflectance characteristics, the capturing process also outputs the detailed geometry of the object.

3.1 Pre-processing for Streaming

In a first step, the captured 3D geometry is converted to a version that is suited for 3D printing. In particular,
if the capturing process does not output a closed mesh, possibly occurring holes in the geometry have to be
filled. Afterwards, the model is scaled to a size that fits on the tracker-object (see Figure 3), that is part of the VR
system and used to track the printed object pose. Optionally, if the object does not have a flat bottom surface or
the surface does not cover the extends of the tracker, a socket can be added in order to mount the object on the
tracker, as can be seen in Figure 3. Furthermore, we have to consider the fact that the typical capturing process
outputs the OctreeBTF in a way which is well-suited for real-time rendering, but does not allow for (progressive)
streaming over a network due to data size and data layout. Hence, before we store the OctreeBTF on the server,
it has to be further compressed and converted to a representation that both allows for progressive streaming
and real-time rendering. Therefore, the BTF representation is split into parts that solely depend on the spatial
position x and the light- and view-directions (𝝎𝑣, 𝝎𝑙 ) respectively. These parts are further divided into chunks
that represent different levels of the octree corresponding to levels of detail of the reconstruction. The resulting
data chunks as well as the geometry are compressed by using Zstandard [4] and ordered in a scheme that allows
the client to start rendering when the first few chunks have been transmitted, and to progressively refine the
rendering as more chunks arrive. For a more detail on the BTF compression, how the data is divided into the
spatial and angular parts and how the chunks are ordered, we refer to the supplemental material.

3.2 Server Component

Initially, the server loads the compressed BTF data chunks and the geometry and then waits for clients to connect.
Each new client first receives general information about the object, followed by the geometry and the BTF data
chunks in the order discussed in the supplemental material. The server keeps track of the chunks transmitted to

ACM J. Comput. Cult. Herit., Vol. 0, No. 0, Article 0. Publication date: 2021.

Towards Tangible Cultural Heritage Experiences

•

0:7

each client, allowing multiple clients to connect at different times. Whenever a client places an annotation on the
object, the annotation is forwarded to all other clients, and is also stored on the server, so that clients connecting
at a later time also receive all annotations already present.

3.3 Remote Client Application

At the client side, we use an off-the-shelf VR-system consisting of a HMD, two controllers and an additional
tracker (see Figure 3 right), that can be attached to an object to include said object into the virtual world. To
allow insights on how the haptics-enriched VR experience improves on a purely screen-based experience, we also
created a Non-VR version of the application (in the following denoted as the "‘2D version"’), where the object
and the light are controlled by moving the mouse.

Tracking. The VR-System is set up in such way that we can use a simple tracker for the object, which is held
in one hand, and a controller for the light and menu interactions held in the other hand. The tracker features a
standard 3/4 inch mount used for cameras that we use to fix the printed object on the tracker, which has the
benefit of allowing to quickly exchange the objects. We utilize the trackpad of the controller to control the light
intensity and the object scale. Each rendered frame, the tracker, the controller and the HMD send their positions
and orientations to the application, which then adapts the scene to match the positions in the real world.

Rendering and Streaming. When the client has connected to the server, a placeholder geometry is rendered
until the real geometry has been received and decompressed. In general, decompression is performed in separate
thread on the CPU, and the uncompressed data is transferred to the GPU in an asynchronous manner, allowing
for seamless updates of geometry and reflectance information without stuttering, which is specifically important
when rendering on VR-headsets. When a chunk has been uploaded to the GPU, the render-thread is signaled to
update the rendering according to the now loaded data. Due to the iterative streaming of BTF data, the user can
already view the object and place annotations on it, while the rendering is refined as more data arrives.

3.4

3D Printing for Haptic Feedback

To provide the user with haptic feedback, we chose to print the model using an off-the-shelf 3D-printer. This
printer uses the Fused filament fabrication (FFF, also known as fused deposition modeling, FDM) technique,
where a thermoplastic material is heated to the point where it melts, and deposited by a moving printhead in a
per-layer fashion. The advantage of that technique is, that over the last years, printers have become more and
more affordable and easy to use, while the quality of the printed objects increased. As we want to attach the
objects to the tracker, we add a baseplate to the object with a hole for a screw. Printed objects are shown in
Figure 3. Furthermore, the objects have to be scaled down to fit the tracker, since a large object would occlude
the tracker too much, thereby decreasing the tracking accuracy.

4

INTERACTION TOOLS FOR REMOTE INSPECTION

The major challenge for the design and development of interaction metaphors are given by the specification of
desirable interaction possibilities with objects as well as their intuitive realization based on hardware equipment.
To allow an immersive object visualization, we leverage the potential of current VR devices and extend the
interaction possibilities by adding haptic object experience. In the following, we provide details regarding the
used control mechanisms for handling the object of interest as well as changing illumination conditions as well
as adding annotations. The respective controls can easily be adapted for right-handed and left-handed persons
(in the following, we will only describe the typical setting for right-handed persons).

Controls for Object Handling. As mentioned in Section 3.4, we create a haptic feedback during object exploration
in VR by 3D printing the respective object geometry and attaching it on a VR tracking device (we use the HTC

ACM J. Comput. Cult. Herit., Vol. 0, No. 0, Article 0. Publication date: 2021.

0:8

• Krumpen et al.

Fig. 3. Left and middle: Printed objects, right: Object mounted on the tracker.

Vive tracker, to which the printed object is attached). To allow a comparison regarding how haptic feedback from
the printed object enhances the user experience, we also implemented a second mode where the digital object is
(virtually) attached to one of the standard controllers, i.e. this interaction is solely determined by how the VR
controller is moved and does not provide any haptic feedback. The application is implemented in a way so that
we can switch between these two modes at any time directly within the application. In addition, we allow the
object to be scaled in the virtual environment based on the trackpad of the controller assigned to the right hand.

Light Controls. An important aspect for object exploration is given by the respective illumination conditions.
Besides showing how an object will look like within a certain environment, which we address by environment
lighting [7], our viewer also supports the direct lighting of an object via a user-controlled light source during its
inspection. When using the direct lighting mode, the controller in the right hand is used to control the main light
source that can be either a point light, or a spotlight to simulate a flashlight as commonly used to inspect objects.
The intensity of the light source can be changed via the controller. In case of environment lighting, we use a
spherical environment map to define the incoming light from every direction. However, as correct image-based
lighting is not possible for BTFs in real-time, we approximate the environment map by a series of eight directional
light sources.

Annotation Tools. During artifact inspection, users often need to annotate respective object parts. For this
purpose, our client application allows users to place annotations on the object by moving the second controller
(i.e. the one where the object is not attached to) over a spot on the object, and pressing a button on the controller.
Users can thereby annotate the object either by placing a simple marker highlighting a spot to direct other
(possibly remotely connected) users to inspect this in more detail, or, if more information is needed, a short and
descriptive text can be added to the annotation using a virtual keyboard. Annotated points on the object are
displayed as small spheres and the color of the spheres reflects the type of annotation, i.e. whether its purpose is
the highlighting of that particular location or region to other users or whether additional text information is
stored at the respective location. The latter texts are only visible to a user if she moves the controller over or
next to the sphere to avoid continuously occluding the object by the text. All added annotations are sent to the
server, which, in turn, forwards them to other connected clients. The display of annotations can be activated or
deactivated at any time.
As a further functionality, we allow the user to draw on the object by continuosly placing points on the object
surface while a certain controller button is pressed. This could be used for marking larger features on the object.

ACM J. Comput. Cult. Herit., Vol. 0, No. 0, Article 0. Publication date: 2021.

Towards Tangible Cultural Heritage Experiences

•

0:9

Fig. 4. View on a person operating the system.

5 EXPERIMENTAL RESULTS

To demonstrate the potential of our approach, we provide a perceptual evaluation obtained in terms of a user
study as well as evaluations regarding visual quality and performance. In addition, we compare our enhanced VR
system to a purely 2D screen-based application with the same features.

5.1 User Study

To verify whether the combination of VR-based object visualization as well as haptic feedback actually enriches
the experience of inspecting cultural artifacts, we performed a user study where the participants had to inspect
an object and perform certain tasks, such as annotating spots or describing materials or reflectance behavior
of the respectively considered object. All participants were naive to the goals of the experiment, provided
informed consent, reported normal or corrected-to normal visual and hearing acuity. During the experiment, the
participants had to perform the given tasks in three different modes:

• In the first mode 𝑀1, the 2D application was used where the user can manipulate the object displayed on a

standard screen based on mouse/keyboard interactions.

• In the second mode 𝑀2, users could interact with our novel VR-based application without haptic feedback

(i.e. the virtual model was attached to the second controller of the VR-system instead).

• In the third mode 𝑀3, users were able to interact with the objects based on our VR-application and the

printed model to allow for an additional haptic feedback.

To avoid biases, we varied the order of these modes for individual participants, i.e. the participants started
with different modes. For each condition, the participants were asked to provide ratings on a 7-point Likert
scale regarding aspects such as visual quality, adequacy for inspection, quality of object experience and ease
of interacting with the object, performing annotations or controlling view and light conditions. The results
shown in figure 5 demonstrate that the approaches 𝑀2 and 𝑀3 outperform the 2D application 𝑀1 in terms of the
ease of inspecting the object and controlling light conditions and, hence, result in a better overall experience

ACM J. Comput. Cult. Herit., Vol. 0, No. 0, Article 0. Publication date: 2021.

0:10

• Krumpen et al.

Fig. 5. Statistical results of our user study. Statistics regarding ratings obtained for mode 𝑀1 are colored in red, the ones
for mode 𝑀2 in green and the ones for mode 𝑀3 in blue. The illustration includes lower and upper fence (dashed line),
interquartile range (colored bar), median (marked with a vertical bar), outliers (marked with •) as well as the average value
(marked with ×). For most aspects, the VR-based modes 𝑀2 and 𝑀3 outperform the 2D application (𝑀1) with keyboard-mouse
controls. The additional use of haptic feedback of the object geometry further improves the quality of object experience and
assessment as well as view control and leads to the best overall ratings.

and intuitiveness regarding the interactions, which may be a result of the higher intuitiveness of hand-based
interactions with objects. The latter represent a kind of canonical approach for us humans to access real 3D objects
in our daily lifes. In comparison to the depiction of objects on a screen and keyboard/mouse based interactions
(𝑀1), users particularly acknowledged the easiness of controlling the viewpoint based on VR-based systems 𝑀2
and 𝑀3. Furthermore, users seemed to benefit from additionally using a printed replica (𝑀3) regarding the view
control. Mode 𝑀3 additionally also received the best ratings regarding the quality of object assessment and object
experience which were likely to also result in the best overall ease of assessment. However, the resolution of the
2D application 𝑀1 was rated higher than for the VR modes 𝑀2 and 𝑀3, which is a result of the limited resolution
of the used HMD. Finally, we expect the lower scores regarding the annotations observed for Modes 𝑀2 and 𝑀3
in comparison to 𝑀1 to be the result of the absence of a standard keyboard for entering text. Here, the use of a
virtual keyboard, where a laser-pointer is used to target each letter individually would have to be exchanged
by a respective speech recognition system to rapidly allow adequate text annotations. However, as this aspect
does not touch the main insights to be gained in the scope of this investigation – that the use of haptic feedback
within a VR-based object exploration improves the object experience – we leave it for future work.

5.2

Intuitiveness of Interaction

In a further experiment, we focused on the analysis whether the improved intuitiveness of interaction reported
in our previous study can also be seen in the times required for users to perform certain tasks, that involve an
interaction with the object such as rotating it. To further evaluate if providing the exact geometry for haptic
feedback benefits the interaction, we also provided a simple geometry in terms of a 3D printed cube which was

ACM J. Comput. Cult. Herit., Vol. 0, No. 0, Article 0. Publication date: 2021.

Towards Tangible Cultural Heritage Experiences

•

0:11

Fig. 6. Comparison of interaction times for marking a fixed series of spots on an object as well as drawing strokes along
characteristic object features. Interaction times for 2D application (𝑀1) are shown in red, times for 𝑀2 are given in green,
values for the VR mode with haptic feedback (𝑀3) are given in red the new mode 𝑀4 is printed in yellow.

mounted on the tracker (denoted as 𝑀4). In this experiment, the participants were first asked to highlight a fixed
series of spots on the object by clicking on the object and placing an annotation. To separate the impact of the
object interaction from the textual annotation, we only focused on the times to mark the respective surface points
and discarded the entering of text information for this experiment. Each participant performed this task in all
the modes 𝑀1, 𝑀2, 𝑀3 described in Section 5.1 as well as the new mode 𝑀4, starting with alternating modes to
eliminate biases. We constructed several series of points the users had to annotate, and alternated the series
between the modes and participants to eliminate learning effects. The series of spots were defined in such a way,
that the object has to be rotated in order to annotate the next spot, and when done, the participants were asked to
rotate the object back to its original position. In addition, we let the users draw strokes along characteristic object
features (neck, face and legs of the Buddha figurine) in all four modes. The respectively required interaction
times are displayed in Figure 6 and indicate that providing any form of immersive presentation is beneficial for
navigation when the object has to be rotated in order to perform a certain task. Further providing haptic feedback
regarding the object shape gives an additional small benefit, that can be seen in the variance of the interaction
times.

5.3 Visual Quality

As we considered a reflectance representation in terms of OctreeBTFs [25], there are no seams and distortions
that affect the visual quality in comparison to the use of conventional BTFs parameterized over the 2D object
surface. However, we had to turn off the spatial filtering of OctreeBTFs in the VR mode to meet the performance
requirements for interactive VR-based visualization. Despite this, the limited OctreeBTF resolution only becomes
visible when the object is held very close to the HMD. In Figure 7, we show a set of screenshots from the 2D
version of the application, where different objects are placed in different environment lighting and some surface
points have additionally been annotated by a user. Furthermore, the dependence of object appearance under
varying illumination conditions (in this example, the light source is moved) is shown in Figure 8.

5.4 Performance analysis

To analyze the performance of our approach, we measured the average frametimes over a period of about 30
seconds during a typical interaction scenario as seen in Figure 4, both using the 2D application and the VR version.
When rendering an OctreeBTF, or BTFs in general, the main bottleneck is the amount of data that has to be read
from the VRAM each frame, which means that rendering is mainly bandwidth limited. The amount of data mainly
depends on the number of screen pixels covered by the object, which determines how much of the spatial BTF
data has to be fetched from the VRAM, and the number of light sources used to approximate the illumination as
we need to sample the angular data for each light source. To get insights regarding how much the additional lights
used to represent the environment (in our experiments, we approximated environments using eight light sources)

ACM J. Comput. Cult. Herit., Vol. 0, No. 0, Article 0. Publication date: 2021.

0:12

• Krumpen et al.

Fig. 7. Screenshots of the 2D application where two objects are lit by different environments with the user-controlled light
being disabled. In the right images some object parts have been annotated by the user. Please note that the textsize has been
increased while capturing the screenshots for better readability. In the real application, the texts are much smaller.

Fig. 8. A series of screenshots of the 2D application where the environment lighting is disabled and the user-controlled light
is moved on an arc above the object, showing how the reflectance changes according to the light position.

ACM J. Comput. Cult. Herit., Vol. 0, No. 0, Article 0. Publication date: 2021.

Table 1. Average frametimes [ms] and frames per second [Hz] for both the 2D application and the VR applications. The
values were measured during a typical interaction scenario.

Towards Tangible Cultural Heritage Experiences

•

0:13

2D

VR

With environment
Without environment

18.18 / 55
14.28 / 70

28.50 / 35
18.18 / 55

affect the performance, we performed the measurement once without considering environment lighting and
once with the respective environment. When interacting with the objects, we additionally took typical distances
for close-range inspection into account to provide a realistic scenario, which means that a large fraction of the
screen pixels of the virtual camera of the 2D application or, in the VR case, the HMD are covered. As shown in
Table 1, frametimes are as expected higher when rendering in VR, even exceeding 28ms, leading to 35 frames
per second, i.e. a slight stuttering of the rendering, due to the increased resolution and the fact that we have to
render the scene twice to allow the stereo impression. However, interestingly, this aspect was not perceived to be
disturbing by the participants during the user study (see Section 5.1). We further noticed that after the BTF is
fully streamed, and the last octree level has been uploaded to the GPU, the frametimes increase significantly.
This is due to the fact that at this point there is an increase in the amount of data the GPU has to fetch from the
VRAM for each pixel. In addition, we observed that the frametimes decrease again when the object is really close
to the camera. This happens when one voxel of the octree covers more than one screen pixel, which means that
the overall number of visible voxels decreases as well as the amount of data the GPU needs to fetch from the
VRAM. Then, the data can be cached between shader invocations that process the same voxel. But for this effect
to occur, the object must be held really close to the camera, especially in VR, and the spatial resolution of the BTF
becomes visible.

5.5 Limitations

Despite the benefits of allowing an improved, tangible remote object experience with particularly better capabilities
regarding interactive object inspection and object assessment, our approach also faces limitations. One major
limitation is the resolution of current VR-HMDs, which is an issue that will hopefully be resolved in the near
future by the next generations of HMDs. On the other end, higher display resolutions also demand for higher
resolution of the used OctreeBTF, which, in turn, requires for more powerful GPUs with more video memory,
and better compression techniques for real-time streaming. Also, the used 3D printing process has some limiting
factors. The use of an FDM-printer, which prints objects layer by layer, causes these layers to be perceivable
when touching the object and probably impact the haptic feedback regarding fine geometrical features of the
object. A solution would be to use a SLA-printer, which can produce much thinner layers and thus can better
preserve small structures of the object. However, we only expect a small impact on the perception in the scenarios
shown in the scope of our evaluation, as the respective object features were easy to perceive, and that the overall
tendency of an improved tangible object experience will only be slightly improved. Furthermore, most 3D printers
are limited to one material which is usually plastic, but also wood like materials are possible. Some printers
have multiple extruders, which allow for multi-material prints, but these materials are all of the same kind, e.g.
one cannot mix plastic and wood-materials. We hope that in the future, these issues can be resolved by more
advanced printing technology, and, hence, do not consider them to be permanent limitations of our overall
approach towards creating tangible object experiences.

ACM J. Comput. Cult. Herit., Vol. 0, No. 0, Article 0. Publication date: 2021.

0:14

• Krumpen et al.

6 CONCLUSIONS

We have presented an approach towards tangible cultural heritage experiences. Our method allows interactive
and collaborative VR-based object inspection and annotation based on high-quality 3D models with accurate
reflectance characteristics, while additionally providing haptic feedback regarding the object shape features based
on a 3D printed replica. As demonstrated by our user study, the additional haptic feedback enriches VR-based
interaction, assessment and experience of the considered objects, which, in turn, indicates the potential of our
approach. While the navigation speed is on par with alternative VR-based interaction methods, providing haptic
feedback regarding the object shape enhances the overall experience. In future work, we would also see the
possibility to circumvent the use of a hardware tracking device, e.g. based on printing patterns used for tracking
into the geometry.

REFERENCES

[1] M. K. Bekele, R. Pierdicca, E. Frontoni, E. S. Malinverni, and J. Gain. 2018. A Survey of Augmented, Virtual, and Mixed Reality for

Cultural Heritage. J. Comput. Cult. Herit. 11, 2 (2018), 7:1–7:36.

[2] W. Benjamin. 1935. The Work of Art in the Age of Mechanical Reproduction (written 1935 as Das Kunstwerk im Zeitalter seiner

technischen Reproduzierbarkeit). (1935).

[3] J. Carmigniani, B. Furht, M. Anisetti, P. Ceravolo, E. Damiani, and M. Ivkovic. 2011. Augmented Reality Technologies, Systems and

Applications. Multimedia tools and applications 51, 1 (2011), 341–377.

[4] Y. Collet and C. Turner. 2016. Smaller and Faster Data Compression with Zstandard. https://code.fb.com/core-data/smaller-and-faster-

data-compression-with-zstandard/. Accessed: 2019-01-29.

[5] F. D’Agnano, C. Balletti, F. Guerra, and P. Vernier. 2015. Tooteko: A Case Study of Augmented Reality for an Accessible Cultural Heritage.
Digitization, 3D Printing and Sensors for an Audio-Tactile Experience. The International Archives of Photogrammetry, Remote Sensing
and Spatial Information Sciences 40, 5 (2015), 207.

[6] K. J. Dana, S. K. Nayar, B. van Ginneken, and J. J. Koenderink. 1997. Reflectance and Texture of Real-World Surfaces. In Proceedings of

the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 151–157.

[7] P. Debevec. 2006. Image-based Lighting. In ACM SIGGRAPH 2006 Courses.
[8] T. G. Dulecha, F. A. Fanni, F. Ponchio, F. Pellacini, and A. Giachetti. 2020. Neural reflectance transformation imaging. The Visual

Computer 36, 10 (2020), 2161–2174.

[9] J. Filip, R. Vavra, M. Haindl, P. Zid, M. Krupika, and V. Havran. 2013. BRDF slices: Accurate adaptive anisotropic appearance acquisition.

In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 1468–1473.

[10] A. Franzluebbers and K. Johnsen. 2018. Performance Benefits of High-Fidelity Passive Haptic Feedback in Virtual Reality Training. In

Proceedings of the Symposium on Spatial User Interaction. 16–24.

[11] A. Giachetti, I. M. Ciortan, C. Daffara, G. Marchioro, R. Pintus, and E. Gobbetti. 2018. A novel framework for highlight reflectance

transformation imaging. Computer Vision and Image Understanding 168 (2018), 118–131.

[12] E. Graeme, M. Kirk, and M. Tom. 2010. Archaeological applications of polynomial texture mapping: analysis, conservation and

representation. Journal of Archaeological Science 37, 8 (2010), 2040 – 2050.

[13] M. Haindl and J. Filip. 2007. Extreme Compression and Modeling of Bidirectional Texture Function. IEEE Transactions on Pattern Analysis

and Machine Intelligence 29, 10 (2007), 1859–1865.

[14] M. Haindl and J. Filip. 2013. Visual Texture: Accurate Material Appearance Measurement, Representation and Modeling. Springer.
[15] A. Hanus, M. Hoover, A. Lim, and J. Miller. 2019. A Collaborative Virtual Reality Escape Room with Passive Haptics. In 2019 IEEE

Conference on Virtual Reality and 3D User Interfaces (VR). 1413–1414.

[16] V. Havran, J. Filip, and K. Myszkowski. 2010. Bidirectional Texture Function Compression Based on Multi-Level Vector Quantization. In

Computer Graphics Forum, Vol. 29. 175–190.

[17] V. Havran, J. Hošek, Š. Němcová, J. Čáp, and J. Bittner. 2017. Lightdrum—Portable light stage for accurate btf measurement on site.

Sensors 17, 3 (2017), 423.

[18] T. Hawkins, J. Cohen, and P. Debevec. 2001. A Photometric Approach to Digitizing Cultural Artifacts. In Proceedings of the 2001 conference

on Virtual reality, archeology, and cultural heritage. 333–342.

[19] T. Hawkins, P. Einarsson, and P. E. Debevec. 2005. A Dual Light Stage. Rendering Techniques 5 (2005), 91–98.
[20] M. Holroyd, J. Lawrence, and T. Zickler. 2010. A Coaxial Optical Scanner for Synchronous Acquisition of 3D Geometry and Surface

Reflectance. ACM Transactions on Graphics 29, 4 (2010), 99:1–99:12.

[21] M. Holroyd, Ja. Lawrence, and T. Zickler. 2010. A Coaxial Optical Scanner for Synchronous Acquisition of 3D Geometry and Surface

Reflectance. ACM Transactions on Graphics (TOG) 29, 4 (2010), 1–12.

ACM J. Comput. Cult. Herit., Vol. 0, No. 0, Article 0. Publication date: 2021.

Towards Tangible Cultural Heritage Experiences

•

0:15

[22] R. D. Joyce and S. Robinson. 2017. Passive haptics to enhance virtual reality simulations. In AIAA Modeling and Simulation Technologies

Conference. 1313.

[23] J. Köhler, T. Nöll, G. Reis, and D. Stricker. 2013. A Full-Spherical Device for Simultaneous Geometry and Reflectance Acquisition. In 2013

IEEE Workshop on Applications of Computer Vision (WACV). 355–362.

[24] M. L Koudelka, S. Magda, P. N Belhumeur, and D. J. Kriegman. 2003. Acquisition, Compression, and Synthesis of Bidirectional Texture

Functions. In 3rd International Workshop on Texture Analysis and Synthesis (Texture 2003). 59–64.

[25] S. Krumpen, M. Weinmann, and R. Klein. 2017. OctreeBTFs - A Compact, Seamless and Distortion-free Reflectance Representation.

Computers & Graphics 68 (2017), 21–31.

[26] K. S. Ladefoged and C. B. Madsen. 2019. Spatially-Varying Diffuse Reflectance Capture Using Irradiance Map Rendering for Image-Based

Modeling Applications. In 2019 IEEE International Symposium on Mixed and Augmented Reality (ISMAR). 46–54.

[27] H. P. A. Lensch, J. Kautz, M. Goesele, W. Heidrich, and H.-P. Seidel. 2003. Image-based Reconstruction of Spatial Appearance and

Geometric Detail. ACM Transactions on Graphics 22 (2003), 234–257. Issue 2.

[28] R. W. Lindeman, J. L. Sibert, and J. K. Hahn. 1999. Hand-held windows: towards effective 2D interaction in immersive virtual environments.

In Proceedings IEEE Virtual Reality (Cat. No. 99CB36316). 205–212.

[29] T. Malzbender, D. Gelb, and H. Wolters. 2001. Polynomial Texture Maps. In Proceedings of the 28th Annual Conference on Computer

Graphics and Interactive Techniques. 519–528.

[30] P. Milgram and F. Kishino. 1994. A Taxonomy of Mixed Reality Visual Displays. IEICE TRANSACTIONS on Information and Systems 77,

12 (1994), 1321–1329.

[31] M. R. Mine, F. P.. Brooks, and C. H. Sequin. 1997. Moving Objects in Space: Exploiting Proprioception in Virtual-Environment Interaction.

In Proceedings of the 24th Annual Conference on Computer Graphics and Interactive Techniques. 19–26.

[32] M. Mudge, T. Malzbender, A. Chalmers, R. Scopigno, J. Davis, O. Wang, P. Gunawardane, M. Ashley, M. Doerr, A. Proenca, and J. Barbosa.
2008. Image-Based Empirical Information Acquisition, Scientific Reliability, and Long-Term Digital Preservation for the Natural Sciences
and Cultural Heritage. In Eurographics 2008 - Tutorials.

[33] M. Mudge, T. Malzbender, C. Schroer, and M. Lum. 2006. New Reflection Transformation Imaging Methods for Rock Art and Multiple-
viewpoint Display. In Proceedings of the 7th International Conference on Virtual Reality, Archaeology and Intelligent Cultural Heritage.
195–202.

[34] G. Müller. 2009. Data-Driven Methods for Compression and Editing of Spatially Varying Appearance. Dissertation. Universität Bonn.
[35] R. Nagao, K. Matsumoto, T. Narumi, T. Tanikawa, and M. Hirose. 2018. Ascending and Descending in Virtual Reality: Simple and Safe

System Using Passive Haptics. IEEE Transactions on Visualization and Computer Graphics 24, 4 (2018), 1584–1593.

[36] G. Nam, J. H. Lee, D. Gutierrez, and M. H Kim. 2018. Practical SVBRDF Acquisition of 3D Objects with Unstructured Flash Photography.

ACM Transactions on Graphics (TOG) 37, 6 (2018), 1–12.

[37] F. E. Nicodemus, J. C Richmond, J. J Hsia, I. W Ginsberg, T. Limperis, et al. 1977. Geometrical Considerations and Nomenclature for

Reflectance. Vol. 160.

[38] T. Nöll, J. Köhler, . Reis, and D. Stricker. 2013. Faithful, Compact and Complete Digitization of Cultural Heritage using a Full-spherical

Scanner. In 2013 Digital Heritage International Congress (DigitalHeritage), Vol. 1. 15–22.

[39] T. Nöll, J. Köhler, G. Reis, and D. Stricker. 2015. Fully Automatic, Omnidirectional Acquisition of Geometry and Appearance in the

Context of Cultural Heritage Preservation. Journal on Computing and Cultural Heritage (JOCCH) 8, 1 (2015), 1–28.

[40] G. Palma, M. Callieri, M. Dellepiane, and R. Scopigno. 2012. A Statistical Method for SVBRDF Approximation from Video Sequences
in General Lighting Conditions. Computer Graphics Forum (Proceedings of the Eurographics Symposium on Rendering) 31, 4 (2012),
1491–1500.

[41] J. S.. Pierce, B. C. Stearns, and R. Pausch. 1999. Voodoo Dolls: Seamless Interaction at Multiple Scales in Virtual Environments. In

Proceedings of the 1999 Symposium on Interactive 3D Graphics. 141–145.

[42] F. Ponchio, M. Corsini, and R. Scopigno. 2018. A Compact Representation of Relightable Images for the Web. In Proceedings of the 23rd

International ACM Conference on 3D Web Technology. 1:1–1:10.

[43] G. Rainer, A. Ghosh, W. Jakob, and T. Weyrich. 2020. Unified Neural Encoding of BTFs. Computer Graphics Forum 39, 2 (2020), 167–178.
[44] G. Rainer, W. Jakob, A. Ghosh, and T. Weyrich. 2019. Neural btf compression and interpolation. In Computer Graphics Forum, Vol. 38.

235–244.

[45] R. Ruiters and R. Klein. 2009. BTF Vompression via Sparse Tensor Decomposition. In Computer Graphics Forum, Vol. 28. 1181–1188.
[46] P. Santos, M. Ritz, R. Tausch, H. Schmedt, R. Monroy, A. De Stefano, O. Posniak, C. Fuhrmann, and D. W. Fellner. 2014. CultLab3D: On

the Verge of 3D Mass Digitization. In Proceedings of the Eurographics Workshop on Graphics and Cultural Heritage. 65–73.

[47] C. Schwartz, R. Ruiters, and R. Klein. 2013. Level-of-Detail Streaming and Rendering using Bidirectional Sparse Virtual Texture Functions.

Computer Graphics Forum (Proc. of Pacific Graphics) 32, 7 (2013), 345–354.

[48] C. Schwartz, R. Ruiters, M. Weinmann, and R. Klein. 2013. WebGL-based Streaming and Presentation of Objects with Bidirectional

Texture Functions. J. Comput. Cult. Herit. 6, 3 (2013), 11:1–11:21.

ACM J. Comput. Cult. Herit., Vol. 0, No. 0, Article 0. Publication date: 2021.

0:16

• Krumpen et al.

[49] C. Schwartz, R. Sarlette, M. Weinmann, M. Rump, and R. Klein. 2014. Design and Implementation of Practical Bidirectional Texture

Function Measurement Devices Focusing on the Developments at the University of Bonn. Sensors 14, 5 (2014), 7753–7819.

[50] C. Schwartz, M. Weinmann, R. Ruiters, and R. Klein. 2011. Integrated High-Quality Acquisition of Geometry and Appearance for
Cultural Heritage. In Proceedings of the International Symposium on Virtual Reality, Archaeology and Intelligent Cultural Heritage (VAST).
25–32.

[51] Y. Tsai, K. Fang, W. Lin, and Z. Shih. 2010. Modeling Bidirectional Texture Functions with Multivariate Spherical Radial Basis Functions.

IEEE transactions on pattern analysis and machine intelligence 33, 7 (2010), 1356–1369.

[52] B. Tunwattanapong, G. Fyffe, P. Graham, J. Busch, X. Yu, A. Ghosh, and P. Debevec. 2013. Acquiring Reflectance and Shape from

Continuous Spherical Harmonic Illumination. ACM Transactions on graphics (TOG) 32, 4 (2013), 1–12.

[53] M. Weinmann, F. Langguth, M. Goesele, and R. Klein. 2016. Advances in Geometry and Reflectance Acquisition. In Eurographics 2016

Tutorials.

[54] M. Weinmann, C. Schwartz, R. Ruiters, and R. Klein. 2011. A Multi-Camera, Multi-Projector Super-Resolution Framework for Structured
Light. In Proceedings of the International Conference on 3D Imaging, Modeling, Processing, Visualization and Transmission (3DIMPVT).
397–404.

[55] T. Weyrich, J. Lawrence, H. P. A. Lensch, S. Rusinkiewicz, and T. Zickler. 2009. Principles of Appearance Acquisition and Representation.

Foundations and Trends in Computer Graphics and Vision 4 (2009), 75–191. Issue 2.

[56] H. Wu, J.e Dorsey, and H. Rushmeier. 2011. A Sparse Parametric Mixture Model for BTF Compression, Editing and Rendering. In

Computer Graphics Forum, Vol. 30. 465–473.

[57] Q. Zhao. 2009. A Survey on Virtual Reality. Science in China Series F: Information Sciences 52, 3 (2009), 348–400.

ACM J. Comput. Cult. Herit., Vol. 0, No. 0, Article 0. Publication date: 2021.

A BTF COMPRESSION

Towards Tangible Cultural Heritage Experiences

•

0:17

BTF measurements can be stored in terms of huge matrices, columns contain the reflectance values for a particular
surface point x under various view-light configurations (𝜔𝑙, 𝜔𝑣). Depending on the resolution of the measurement
setup, even when stored with half-precision floating point values, this matrix can exceed hundreds of gigabytes,
and thus, needs to be compressed in order to meet the demands for real-time rendering and streaming. For this,
the Decorrelated Full Matrix Factorization (DFMF) [34], has been demonstrated to achieve good compression
ratios, while preserving visual quality. In a first step, the BTF color values are converted into the YUV colorspace,
which separates brightness from color information. Subsequently, the color channels are decorrelated by taking
the logarithm of the Y-channel, and dividing the U- and V-channel by the Y-channel. Then, for each of these
matrices, a SVD 𝐴 = U × Σ × V𝑇 is computed to separate spatial information stored in V from the light- and
view-dependent information in U. More precisely, a column 𝑢𝑐 (𝜔𝑙, 𝜔𝑣) of U stores the Eigen-ABRDF, whereas a
row 𝑣𝑐 (x) of V holds the Eigen-Texture of the BTF component 𝑐. As the SVD orders the rows V and the columns
of U with respect to their importance, we can compress the BTF by keeping the first 𝑘 rows of V and the first 𝑘
columns of U.

𝜌𝐵𝑇 𝐹 (x, 𝝎𝑙, 𝝎𝑣) ≈

𝑢𝑐 (𝝎𝑙, 𝝎𝑣) · 𝝈𝑐 · 𝑣𝑐 (x)

∑︁

0≤𝑐<𝑘

The resulting matrices (cid:101)U, (cid:101)Σ and (cid:101)V for each color channel are then stored, where (cid:101)Σ and (cid:101)V are pre-multiplied for
simplicity. Using this compression, the BTF data size is reduced to a few gigabytes, which fit into the memory
of current GPUs. Furthermore, the use of the YUV colorspace allows to compress the color channels U and V
even further by keeping less components, as most reflectance information is about changes in brightness and
the human eye is more sensitive to these than to changes in color. In our experiments, we observed the use of
eight BTF components for the U- and V-channel to be a good compromise between data size and visual quality
when rendering. While for the Y-channel a maximum of 100 components is sufficient for nearly all objects and
for objects with less details, one can also take less components. When rendering, four components are grouped
together so the number of components 𝑘 is chosen to be a multiple of four, an RGBA-array-texture with 𝑘/4
layers can be used for rendering. Applying the above, we end up with 𝑘𝑌 = 72 and 𝑘𝑈 𝑉 = 8. This algorithm can
be applied to both conventional, texture-based BTFs as well as OctreeBTFs. The difference is, that for OctreeBTFs,
the compression is done for the highest octree-level 𝑑, and the compressed BTF data for the lower levels is
computed as a post-processing step by averaging the BTF data over all child nodes in a top-down manner, starting
with level 𝑑 − 1 down to 𝑑𝑚𝑖𝑛 as described by Krumpen et al. [25].

B BTF DATA LAYOUT

Even when compressed, transmitting a BTF to a client over the network still takes some time, which makes it
feasible to split the BTF into smaller chunks of data and transmit these chunks independently to a remote client.
This enables the client to start rendering the BTF with a lower quality as soon as the first chunks are received and
to refine the rendering as more data-chunks arrive. An approach for streaming BTFs over the internet has been
presented yb Schwartz et al. [48], however, since the data-layout of the spatial matrices (cid:101)Σ × (cid:101)V of the OctreeBTF
differs from a conventional BTF, this approach cannot be applied to stream OctreesBTFs. The four-dimensional
angular data from (cid:101)U is stored as a nested parabolic map converted to an RGBA-array-texture for both BTF
representations and the layers of these textures can be streamed independently. The difference when streaming
lies in the handling of spatial data: For conventional, texture-based BTFs, the spatial data can be stored as an
RGBA-array texture for each color channel, where each layer of a texture stores the BTF components 𝑐𝑖 . . . 𝑐𝑖+3
for all surface points X. The individual texture-layers can then be streamed independent from each other, which
means that the client can start rendering as soon as the first texture-layer for all three color channels is received.
For OctreeBTFs however, the data is stored per voxel, where one voxel corresponds to one point x, i.e. first all

ACM J. Comput. Cult. Herit., Vol. 0, No. 0, Article 0. Publication date: 2021.

0:18

• Krumpen et al.

. . . 𝑐1

. . . 𝑐0

𝑘 for the first voxel are stored, then components 𝑐1
1

components 𝑐0
𝑘 for the second voxel, etc. This data
1
layout leads to a better cache-efficiency on the GPU during rendering, as described by Krumpen et al. [25], but
has the disadvantage that it is no longer possible (at least not without significant, costly re-arrangement of the
data) to stream one component for all voxels as one data-chunk. Thus, we have to treat all components for all
voxels for one octree levels as one data-chunk, and the client can only start rendering if the whole chunk (again
for all color channels) has been received. Finally, rendering the OctreeBTF needs information about the octree
itself and the normal and tangent vectors for each voxel, leading of an additional chunk of voxel-data per octree
level. We construct the OctreeBTF similar to Krumpen et al. [25], where the first octree levels are forced to be
complete, thus we have 𝑙 = 𝑑𝑚𝑎𝑥 − 𝑑𝑚𝑖𝑛 octree levels. Considering the number of texture-layers for the angular
data for all three color channels, we end up with

• 𝑙 chunks of voxel data, storing the octree structure.
• 𝑙 × 3 chunks of spatial BTF data, which cannot be split into components.
• 𝑘𝑌 /4 + 2 × 𝑘𝑈 𝑉 /4 chunks of angular BTF data, i.e. texture-layers, which are independent from the octree

structure.

All chunks are further compressed using the Zstandard library [4] and are stored in the main memory of the
server, together with the also compressed geometry.

C OCTREEBTF STREAMING

For streaming, we need to determine an order in which the chunks are transmitted, such that the client can start
rendering as early as possible. First, the geometry is transmitted, which is followed by the voxel and spatial data
for the octree level 𝑑𝑚𝑖𝑛 and the first four components of angular data (i.e. texture-layer) for each color channel.
The remaining angular texture layers are distributed evenly among the remaining octree-levels. The biggest
chunk is the last spatial chunk for the Y-channel due to the large number of voxels with 𝑘𝑌 components for each.
The complete algorithm is provided in Algorithm 1.

ACM J. Comput. Cult. Herit., Vol. 0, No. 0, Article 0. Publication date: 2021.

Towards Tangible Cultural Heritage Experiences

•

0:19

Algorithm 1 Build the load order for BTF data chunks

Add voxel data for level 𝑗0
Add spatial data for level 𝑗0 and channels YUV
Add first angular-layer for channels YUV
layers_per_lvl_Y = max(layers_total_Y / 𝑙, 1)
5: layers_per_lvl_UV = max(layers_total_UV / 𝑙, 1)

layers_added_Y = 1
layers_added_UV = 1
for 𝑖 = 1 to 𝑙 do

Add voxel data for level 𝑗𝑖
Add spatial data for level 𝑗𝑖 and channel Y
for 𝑔 = 1 to layers_per_lvl_Y do

if layers_added_Y < layers_total_Y then
Add angular-layer[layers_added_Y]

end if
layers_added_Y+=1

end for
Add spatial data for level 𝑗𝑖 and channel U
Add spatial data for level 𝑗𝑖 and channel V
for 𝑔 = 1 to layers_per_lvl_UV do

if layers_added_UV < layers_total_UV then
Add angular-layer[layers_added_UV]
Add angular-layer[layers_added_UV]

10:

15:

20:

end if
layers_added_UV+=1

25:

end for

end for

ACM J. Comput. Cult. Herit., Vol. 0, No. 0, Article 0. Publication date: 2021.

