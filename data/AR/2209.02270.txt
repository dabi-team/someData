2
2
0
2

p
e
S
6

]

G
L
.
s
c
[

1
v
0
7
2
2
0
.
9
0
2
2
:
v
i
X
r
a

An Indoor Localization Dataset and Data Collection Framework with High
Precision Position Annotation

F. Serhan Dani¸sa,b,∗, A. Teoman Naskalia, A. Taylan Cemgilb, Cem Ersoyb

aDept. of Computer Engineering, Galatasaray University, Istanbul, Turkey
bDept. of Computer Engineering, Bo˘gazi¸ci University, Istanbul, Turkey

Abstract

We introduce a novel technique and an associated high resolution dataset that aims to precisely

evaluate wireless signal based indoor positioning algorithms. The technique implements an aug-

mented reality (AR) based positioning system that is used to annotate the wireless signal parameter

data samples with high precision position data. We track the position of a practical and low cost

navigable setup of cameras and a Bluetooth Low Energy (BLE) beacon in an area decorated with

AR markers. We maximize the performance of the AR-based localization by using a redundant

number of markers. Video streams captured by the cameras are subjected to a series of marker

recognition, subset selection and ﬁltering operations to yield highly precise pose estimations. Our

results show that we can reduce the positional error of the AR localization system to a rate under

0.05 meters. The position data are then used to annotate the BLE data that are captured simul-

taneously by the sensors stationed in the environment, hence, constructing a wireless signal data

set with the ground truth, which allows a wireless signal based localization system to be evaluated

accurately.

Keywords: Data collection, indoor positioning, augmented reality, data annotation, localization

evaluation

1. Introduction

The majority of the world population has started to dwell in cities and urbanized areas, where

people spend most of their time indoors. As the buildings and structures become ever more complex,

the need to use navigation tools gains importance. However, outdoor navigation technologies like

GPS do not work indoors. Therefore, there is a need to use other means of localization indoors.

Wireless technologies are preferred for indoor localization as they are globally available and

∗Corresponding author: sdanis@gsu.edu.tr

Preprint submitted to Pervasive and Mobile Computing

September 7, 2022

 
 
 
 
 
 
aﬀordable. Whereas the localization algorithms have been improved with the ubiquitous Wi-Fi

technology for indoors [1], due to its low energy consumption, price and size, Bluetooth Low Energy

(BLE) is particularly suited to this domain [2]. BLE beacons or sensors are utilized to track objects

or people, and most hand held mobile devices can easily incorporate this technology. However, the

nature of the wireless signals render the localization task rather diﬃcult as the signals are easily

reﬂected or absorbed by obstacles. To calibrate and optimize wireless based localization systems,

ground truth data must be collected. More importantly, to measure the real performance of these

systems, we require trajectories labeled with ground truth positions alongside the wireless signal

data, rather than separately measured reference points. This ground truth collection process can

be rather tedious and usually labor intensive [3].

In the literature, there exist several valuable datasets formed by wireless signal parameters

and corresponding ground truth information, however, such data collections do not reﬂect the true

nature of the trajectories. Instead, the sets are formed by signal parameters collected with sparsely

located reference points [4], or by modifying the paths artiﬁcially making the carrier remaining

still on predeﬁned reference points [5]. Moreover, the ground truth notion may not even belong

to the positions.

In the work of Girolami et al. [6], the ground truth of social interactions are

reported with the BLE received signal strength indicator (RSSI) data. Likewise, in the work of

Sansano-Sansano et al. [7], the ground truth is in terms of the user gate speed.

High resolution ground truth for indoor positioning is rare because of the diﬃculties in collecting

accurate position data on every point the object of interest passes. This diﬃculty prevents the

researchers from setting a generalized benchmark for indoor positioning systems [8]. The positioning

community lacks a clear standard for evaluating positioning systems. Multiple attempts have been

made in order to ﬁll this gap to validate the accuracy of an indoor positioning system. De la Osa

et al. propose a method to deﬁne a ground truth for real life purposes [9]. The technique bases on

creating predeﬁned paths with checkpoints which are used as indicators when the tester passes over

them. The ground truth trajectory is then estimated by interpolated points between checkpoints.

Another one is the skeleton path technique in which the user follows a predetermined path with a

constant speed and the positions are sampled. Clearly the techniques based on walking are prone to

human errors, incorrectly assuming constant paces and true timings [10]. Moreover, the positions

are estimated using inaccurate dead-reckoning data based on step counting [11]. According to

Adler et al. [12], this predeﬁned path technique, or namely oﬃce walk, is the most preferred ground

truth collection method. This inaccurate method is also the cheapest and the most precise ground

2

truth data collection method so far used to validate indoor positioning systems.

Our ground truth data collection technique provides a set of timestamped wireless signal param-

eters. Whereas the technique can be easily modiﬁed to collect and annotate other wireless signal

based parameters, we work with the prototypical BLE RSSI data. The RSSI data are annotated

both in temporal and spatial domains accurately with the corresponding ground truth positions

using a vision-based localization system. We employ the markers designed for AR applications, to

annotate the RSSI data with ground truth positions.

AR is another technology that performs exceptionally well in indoor locations when based

on special markers. Whereas AR applications are initially developed for adding artiﬁcial ﬁgures

into real world views, a camera with proper conﬁgurations can detect the precise positions and

orientations of its special markers relative to the camera. This information is inversely used to

calculate the location of the camera if the marker is stationary, so they are preferred in indoor

navigation when we aim to track the viewer [13]. Although they function well, camera based

devices are usually more expensive than BLE devices and they require the active movement of

the camera to determine the location, which is not ideal if objects or people need to be passively

tracked. Accordingly, La Delfa et al. compare diﬀerent marker technologies for environmental

suitability [14]. Xavier et al. utilize markers for SLAM [15], whereas Ujkani et al. perform sensor

calibration with AR markers in a robotic cell achieving accuracies of 5-10 cm [16].

In the wireless positioning community, AR positioning techniques are used hybridly to enhance

the performance of the former systems as wireless signals are prone to yield high error rates [17, 18].

Amongst these studies, Byrne et al. propose a methodology for collecting RSSI information and

accelerometer data using markers placed on the ground and cameras mounted on people to collect

the ground truth [19].

We propose a framework that makes use of an AR-based localization system to collect precisely

annotated RSSI data in both time and space. The system estimates positions of a specially de-

signed camera-beacon bundle that captures video and RSSI data simultaneously, and is shown to

implement a very accurate positioning with an error rate under 0.05 m and it can function along-

side the BLE based localization without causing extra interference. With its low error rate, this

method provides ground truth positions to evaluate or verify a pure wireless signal based localiza-

tion system. After the postprocessing, we possess RSSI data points, each of which is annotated

with its precise 3D position. Moreover, the proposed technique has a potential to greatly speed up

the calibration procedure and decrease or fully automatize the workload of ﬁngerprinting and radio

3

frequency map generation.

As an end product, we also provide a publicly available dataset collected using the proposed

method. The dataset includes position annotated RSSI data of several trajectories alongside the

preprocessed probabilistic radio frequency map estimations in ﬁngerprint or grid formats 1.

In Section 2, the methods for data selection, ﬁltering and synchronization that make up the

position annotated wireless data collection framework are given. We describe our experimental

setup and dataset speciﬁcations in Section 3. The results supporting our proposed framework are

reported in Section 4 and we conclude with future insights in Section 5.

2. Data Annotation Methodology

Data annotation methodology comprises of four main consecutive sections as seen in Figure 1.

With the camera calibration matrices, premeasured poses (positions and orientations) of the AR

markers, timestamped RSSI readings and accompanying video frames in hand, we want to annotate

each RSSI reading on a trajectory with a precise position estimated using the videos and marker

information.

Before going through the data annotation procedure, we ﬁrst present the mathematical nota-

tions, deﬁnitions and necessary conversions in 3D space in Section 2.1. In Section 2.2 we employ

the ArUco marker detector to estimate marker poses relative to the camera(s) [20, 21]. With the

deﬁned mathematical foundations, estimated marker poses relative to the cameras are combined

with the marker poses relative to the world to obtain the pose of the camera-beacon bundle in the

world frame. In Section 2.3, we apply pruning techniques to eliminate the misleading estimations in

a batch of pose estimations, which we observe to be very noisy due to erroneous marker detections.

We apply a Kalman ﬁlter specially tailored for this problem to obtain the most relevant pose given

a batch of poses in Section 2.4. Finally, in Section 2.5 these smoothed precise camera poses are

synchronized in time with the RSSI readings.

2.1. Notations, deﬁnitions and conversions

Throughout this article, vectors are represented by bold lowercase letters (e.g. τ or θ), while

bold capital letters represent matrices (e.g. R). The variables, m, c and w denote the entities, a

1The dataset is publicly available with DOI: 10.34740/KAGGLE/DS/1662453, and the framework is public in

Github as “Position-Annotated-BLE-RSSI-Dataset”.

4

Figure 1: Position annotation on RSSI data: we ﬁrst (i) estimate the camera poses in the video frames using the

marker poses and the camera calibration matrices, and then (ii) ﬁlter the likely poses by eliminating highly irrelevant

estimations. The likely poses are (iii) smoothed using a properly designed Kalman ﬁlter to ﬁnd a ﬁnal pose for each

frame, which is ﬁnally (iv) synchronized with the simultaneous RSSI data in time to obtain a position annotated

RSSI dataset for each experiment in the area. The arrow weights denote the data load coarsely.

marker, the camera-beacon bundle and the world, respectively. We represent the position (trans-

lation) and the rotation (orientation) vectors with iτjk and iθj respectively. The superscript on

the left, i, is for the frame the vector is deﬁned in, and the subscript on the right, j, is for the

entity the vector belongs to. For the position vectors, a double subscript jk shows the start and

the end points. If the start position is the frame origin itself, we omit the ﬁrst entity, jτjk ≡ jτk.
For example, cθm deﬁnes the rotation vector of the marker m, in the camera frame, c, and wτcm,

speciﬁes a vector from the camera c to the marker m in the world frame w.

We use the rotation matrices to convert vectors from a predeﬁned coordinate frame to another.

A position vector deﬁned in the frame i is multiplied with the rotation matrix, k

i R, from the left to

be converted into the coordinate frame k. Note that with a proper multiplication, the coordinate

frames in the diagonals are eliminated, (i in (1)).

kτj = k

i R iτj

(1)

Similarly, if we have a rotation matrix from the frame i to j and another one from j to k, we

can obtain transitively the rotation matrix from i to k by multiplying them as in (2). Note the

elimination of the frame j.

i R = k
k

j R j

i R

5

(2)

MarkerposesCameramatricesCamera poseestimationSmoothing withKalman filterPoseeliminationRSSI datasynchronizationRaw posesLikely posesFinal poseRawvideosRSSIdataPositionannotatedRSSI data(i)(iii)(ii)(iv)Position Annotation on RSSI dataInputsOutputSetup andcalibrationData collectionduring navigationSince rotation matrices have a rank of three, a rotation vector is a convenient and compact

representation of a rotation matrix. Conversion from a rotation vector to a rotation matrix is

straightforward with the use of Rodrigues’ formula (3), as follows:

R = Rodrigues(θ)



= I cos(θ) + (1 − cos(θ))rrT + sin(θ)






0 −rz

ry

rz

0 −rz

−ry

rx

0








(3)

where θ = (cid:107)θ(cid:107), is the norm of the rotation vector, r = θ/θ, the normalized rotation vector, and

ri are the individual orientation components for the axes in 3D. If we have a rotation vector of the

entity i deﬁned in the coordinate frame j, we can obtain the associated rotation matrix from i to

j, using the Rodrigues’ rotation formula [22]:

j
i R = Rodrigues(jθi)

The inverse of the Rodrigues’ formula is also available, but with a more complicated setting:

θ = Rodrigues−1(R)

=





θ = 0,

θ = S1/2
θ = ρ

s θ,

(cid:17)

(cid:16) v

(cid:107)v(cid:107) π

,

if s = 0 and d = 1

if s = 0 and d = −1

if sin θ (cid:54)= 0

(4)

(5)

where A = R−RT

2

, ρ = [a32 a13 a21]T , s = (cid:107)ρ(cid:107), d = (tr(R − 1))/2, θ = arctan2(s, d), v is a

nonzero column of R + I and S1/2(θ) = −θ, if (cid:107)θ(cid:107) = π or S1/2(θ) = θ, otherwise. Both of these

formulas can be easily employed through the OpenCV library. If we have a rotation matrix from

the coordinate frame i to the frame j, we can obtain the associated rotation vector of the entity i

relative to the coordinate frame j, using the inverse of the Rodrigues’ rotation formula:

jθi = Rodrigues−1(j

i R)

(6)

We denote the positions of stationary markers as wτm, and their orientations as wθm. When

markers are detected and recognized in a video frame, the pose estimator supplied by the ArUco

suite estimates the position vectors pointing to the center of the marker, cτm, and rotation vector,
cθm, relative to the camera coordinate frame.

6

2.2. Camera pose estimation

We are interested in extracting the poses of the camera-beacon bundle, which we will denote

(wτc, wθc). The ArUco suite provides the marker detector routines. With properly calibrated

cameras, we can estimate the poses of the markers relative to the camera coordinate system,

denoted as (cτm, cθm). We relate the latter information to the former using the premeasured
marker poses relative to the world frame, (wτm, wθm) (see Section 3.2 for details on markers). For

the positions part, we do:

wτc = wτm − wτcm

= wτm − w

mR m

= wτm − w

mR (c

c R cτcm
mR)T cτm

(7)

where w

mR = Rodrigues(wθm) and c

mR = Rodrigues(cθm). The computation consists of a mere
redeﬁnition of the position vector from the camera to the marker in the world coordinate frame,

wτcm, and its subtraction from the marker position in the same coordinate frame, wτm as seen

Figure 2.

Figure 2: The position vectors and the coordinate frames: w, c and m; for the world, the camera and the marker

frames respectively. The axes, x, y and z are coded with red, green and blue arrows respectively. We are to compute

the cyan vector, given the orange vectors.

We can also easily obtain the rotation vector using the rotation matrix from the world frame

to the camera frame, w

c R. Whereas this matrix can be used to convert a vector from camera

coordinate system to the world coordinate system, it also deﬁnes how the camera is oriented within

the world frame. The rotation matrix was already computed as an intermediate step of the position

7

computation (7). Employing the inverse of the Rodrigues’ formula, we convert the rotation matrix

into the rotation vector of the camera in the world frame (8) as follows:

wθc = Rodrigues−1(w
= Rodrigues−1 (cid:16)w

c R)

mR (c

mR)T (cid:17)

(8)

The conversions and computations above are repeated for each detected marker in an image

frame. Since we construct our detection setup aiming to capture several markers at a time, we

expect to estimate multiple camera pose vectors at each frame, (wτc, wθc), namely the raw poses.

2.3. Pose elimination

The variances of the camera pose estimations are observed to be large due to noisy marker pose

detections caused by minor errors in manual measurements, faulty camera matrices and coeﬃcients,

motion blur or varying lighting conditions. The combination of these factors lead to inaccurate

rotation matrices, which in turn accumulate to varying positions for the camera. Before directly

applying a ﬁltering algorithm on the raw poses, we employ two elimination strategies (see Figure 3).

Figure 3: Position elimination strategies: close marker selection (left) and outlier elimination (right). Black points

are selected for the next stage, while the red points are eliminated.

At each video frame, we expect to recognize multiple markers. We use two cameras to expand

the ﬁeld of view, so if one camera misses the markers, the other one captures possibly more than

one marker. Moreover, to make sure that there exist multiple marker readings, we process F video

8

frames in one shot. The traversed distance of a batch, dF , for the camera-beacon bundle moving at

a constant speed of v can be computed with dF = vF/(2f ps), where 2f ps is the frame capture speed

of a two camera system. For example, with F = 15, f ps = 60, and navigating at v = 0.35 m/s,

each batch of frames corresponds to about dF = 0.044 meters.

We observe that camera poses from distant markers can be more misleading. This defect is due

to the number of pixels of the detected marker image. As the distance increases, the lack of pixels

lead to larger measurement errors. Therefore, as long as multiple poses are estimated in a time

window of F frames, the poses based on distant markers may be discarded. We set a closeness

parameter, C, that takes into account the closest C marker readings. This is a quick technique

that sorts the detected markers in a frame with respect to their distances to the camera and keeps

only the least C distant poses.

Another elimination is achieved by an outlier detection test. In a set of pose estimations, we

eliminate the outliers with respect to the distances to a consensus point [23]. The parameter U , is

deﬁned to determine the number of remaining poses after consecutive outlier detection iterations.

This technique is merely a slow backward subset selection method that is repeated in a greedy

manner.

Employing the elimination methods with proper values for the parameters, C and/or U , we

prune the raw poses into a smaller set of poses, namely the likely poses.

2.4. Kalman ﬁlter for smoothing

Even though some marker detections are eliminated, we still receive erroneous estimations.

Error ﬂuctuations can be controlled by employing appropriate ﬁltering mechanisms. The current

section is linked to the previous section through pose vector estimates of the camera-beacon bundle,

which will be our observations in the ﬁltering design: yt = (wτc, wθc)t. We model this new problem

as a state space model, in which the observed poses, yt, are assumed to be generated noisily from

the latent poses, xt. The latent pose vector of the tracked object, xt at time t, is a function of the

pose vector at a previous time, xt−1. This function is also called the motion model that stands for

the dynamics of the object.

The transition densities are assumed not to follow explicit motion models. Instead, we assume

that xt resides close to xt−1. Observation densities are designed similarly. yt should not be far away

from xt. With a prior distribution at the initial time, p(x0), we are to ﬁnd the ﬁltering distribution

p(xt|y1:t). These assumptions can be realized by forming transition and measurement densities

9

with Gaussians (9), whose noise covariance matrices will be denoted as Rt and Qt respectively.

x0 ∼ p(x0)

xt ∼ N (xt; xt−1, Rt)

yt ∼ N (yt; xt, Qt)

(9)

The dimensions of the latent space and the measurement space are assumed to be independent,

so we use diagonal matrices. The values of the covariance matrices are chosen to compensate for

unexpected high errors in measurements [24, 25]. The exact inference for the ﬁltering distribution

becomes tractable, and can be solved with the standard Kalman ﬁlter:

Predict:

ˆxt = Axt−1

ˆPt = APt−1AT + Rt

Update:

Kt = ˆPtHT (H ˆPtHT + Qt)−1

xt = ˆxt + Kt(yt − Hˆxt)

Pt = (Id − KtH) ˆPt

where transition and observation matrices, A and H, are chosen as identity matrices (Id), Pt are

the estimated covariance matrices, and Kt are the optimal Kalman gains. We obtain xt as the pose

estimations, namely the ﬁnal poses.

2.5. RSSI data synchronization

After the ﬁltering operation, timestamped pose data are obtained, but there still exist problems

in synchronizing the timestamped RSSI data to the timestamped position data.

The ﬁrst problem is that the cameras cannot be synchronized to a precise universal clock. We

overcome this problem by adding artiﬁcial disturbances that aﬀect both the camera views and the

RSSI data so that a synchronization may be possible. After various unsuccessful Faraday cage-like

attempts, we found that a pair of hands would be practical to give disturbances to electromagnetic

signals and visual data. We cover and release the camera-beacon bundle with two hands at the

beginning of each experiment. This hand movement can be clearly seen in the video frames as

shown in Figure 4a. Because body parts absorb electromagnetic signals, we observe that this

method creates a signiﬁcant reduction in the RSSI data. The artiﬁcial disturbance can easily be

10

(a) Detecting covering visually.

(b) Detecting covering in RSSI readings.

Figure 4: Synchronization of video frames with RSSI data in time. We apply artiﬁcial disturbances on the videos

and the RSSI data by our hands. We observe three stages: the bundle is uncovered (white); the hands are hold on

the bundle (gray) and the transition stage where the hands are moving to cover or release the bundle (light gray).

detected in RSSI readings as seen in Figure 4b. In the ﬁgures, we divide this hand movement into

four color-coded stages. We ﬁrst cover the camera-beacon bundle quickly (light gray). When the

bundle is covered with hands, we are in the gray region where the cameras’ views are largely blocked

and we observe that strong RSSI readings coarsely between -85 to -65 decibels are lost because of

the absorption due to the hands. Then we release (light gray) the bundle as quickly as possible to

reach the white region where the camera views are no longer blocked and the RSSI readings return

to their normal course. We select the timestamp of the second light gray region that points to the

change point between the white region and the gray region. We modify the data streams to start

from this selected timestamp, and we are left with synchronized video frames and RSSI data.

Whereas the data streams are set to begin at the same time, RSSI timestamps and video

timestamps do not match exactly. Secondly and obviously, we have to unify the two timestamps.

11

RSSI data is far less frequent than the pose data (2 Hz vs 10-30 Hz). To coincide the two data types

in time, we perform a linear interpolation of the video frame timestamps at the RSSI timestamps,

and we label each RSSI timestamp with the linearly interpolated pose data as its ground truth.

After the time synchronization stage, we have a dataset of RSSI data points annotated with the

position it is captured on. Two of the trajectories from our dataset are displayed in Figure 5.

Figure 5: RSSI data overlaid on the position data: time ﬂows from red bars to black bars.

3. Data Collection Setup

The aim of the data collection setup is to exploit as many sensors as possible and build a BLE

sensor network that covers the whole area at a minimal cost, and ﬁnally build a dataset with the

highest information within. We use a set of BLE beacons and adapters, low proﬁle computers,

motion cameras and visual markers installed in an indoor area of size 20.66 × 17.64 m2. Intended

originally as an oﬃce space for fellow graduate students, the area also includes columns, walls and

furniture that cause multipath interference on the wireless signals (see Figure 6).

Figure 6: Sample images from the area with markers, extension cables, BLE adapters, computers and other obstacles.

12

(a) Illustration of the sensor distribution: yel-

(b) The navigable platform (left); the beacon-camera bundle of a POI beacon

low rectangles are the computers, colored cir-

and two GoPro Hero 4 motion cameras (top middle); a Raspberry Pi 3 Model

cles the sensors and the gray lines are the USB

B+ computer (down middle); Logilink Bluetooth adapter (top right); and a

cables that link the sensors to the computers.

KLPRO KLLMZ60 laser distance meter (down right).

Figure 7: Hardware setup

3.1. Hardware setup

We consider that an eﬃcient conﬁguration of the sensor positions in terms of data quality can be

achieved by putting them on the walls surrounding the test area. However, there appear diﬃculties

in both powering the sensors and reading data from them with more than 10 meters of the extension

cables. Because of the cable length constraints, in our design, each two sensors are controlled by one

computer. To cover the whole area as equidistantly as possible, we install 8 Logilink Bluetooth 4.0

adapters on the walls, and use the 4 onboard Bluetooth embedded chips of the computers, making 12

Bluetooth sensors (Sensor-ij) in total as seen in Figure 7a. The computers are placed on the cabinets

to ensure that their embedded sensors have a standard elevation of one meter (see Figure 7b). The

USB cables are attached to the ceiling to minimize possible signal interferences. The adapters are

attached on the walls elevated about two meters from the ﬂoor. In this conﬁguration the extension

cables are tested to supply enough power to the adapters. The computers responsible for the data

capture system are four Raspberry Pi 3 Model B+ single board computers (RPi-i), which are based

on Broadcom BCM2837B0, Cortex-A53 (ARMv8) 64-bit quad-core processors. They run Raspbian

Stretch operating system, on which the Robot Operating System (ROS) Melodic middleware is

installed [26]. The computers scan the BLE packets emitted by the transmitters and publish them

into the local area network (see Figure 7b).

13

RPi-1RPi-2RPi-4RPi-3Sensor-1cSensor-1aSensor-1bSensor-2cSensor-3aSensor-2bSensor-2aSensor-4aSensor-4bSensor-3bSensor-4cSensor-3cWe use one sample of POI Beacons as the BLE signal transmitter, which is constructed upon

Nordic Semiconductor multi-protocol SoC and 32-bit Arm Cortex M0 processor with 256 kB ﬂash

and 16 kB on chip. POI Beacons use the Bluetooth 4.0 BLE protocol and iBeacon protocol, and

have a signal range of 70 meters. The beacons send two BLE identity packets per second and have

a running time up to 4 years with two standard AA batteries (see Figure 7b).

A 3D printed custom case, which we call “the camera-beacon bundle”, holds the BLE beacon

and two GoPro Hero 4 motion cameras together (see Figure 7b). The cameras are capable of

capturing images with a resolution of 1920 × 1080 at 60 fps. The special design of the custom case

lets the cameras stay in such a formation that they are placed back to back, and their lenses are

aligned facing opposite directions. One camera stays upside down with respect to the other one. The

design also minimizes the translations between camera coordinate frames and the beacon, provides

a large view angle in order to detect as many markers as possible at an instant, and maximizes

the functionality of the cameras by allowing reaching both of the buttons. The cameras and the

BLE beacon are deliberately put in the same place as we are to collect BLE data originating from

the beacon and simultaneous videos that will be used to implicitly estimate precise positions of the

mobile beacon. By this setup, we build a data collection infrastructure and then by processing the

videos captured by the cameras, we estimate the positions of the camera-beacon bundle, so that

the RSSI readings by the BLE beacon can be labeled with those positions. The labels will serve as

ground truth for the future pure BLE based localization purposes.

The bundle is attached onto a tripod which in turn is mounted on a wheeled cabinet that allows

smooth navigation in the test area (see Figure 7b). The beacon height is set to be 2 meters to obtain

a clear view of the test area in terms of both visual data and radio frequency data. The tripod

functionality also allows us to switch to diﬀerent alignments of the camera-beacon bundle, from

which we prefer horizontal alignment to maximize the view angle, hence, the number of detectable

markers, and to minimize the interference on the beacon due to the cameras.

The distances are rigorously measured with a KLPRO KLLMZ60 laser distance meter with a

working range of 60 m and an accuracy of ±0.002 meters (see Figure 7b).

3.2. Markers and camera calibration

We choose 30 square markers from the dictionary of 6 × 6 matrix markers of the ArUco marker

suite (Figure 8a), and print them on hard paper with edges of 0.3 meters. The markers are

attached onto the walls or columns where they can be viewed from almost any angle in the area

14

(a) ArUco marker samples (top) and the calibration

(b) The markers in the test area as black dots and arrows.

chart (bottom).

Figure 8: ArUco markers and their distribution in the test area.

(Figure 8b). We record the label and the pose (of the center) of each marker with respect to a

predetermined world coordinate frame (wτm, wθm). The measurement of the position vector, wτm,

is straightforward, that is, we manually measure the distances to the walls of reference and to the

ﬂoor with the help of the laser distance meter, but the computation of a rotation vector, wθm, may

be challenging. To ease this process, we install the markers on the walls or columns in such a way

that rotations between the marker coordinates and the world coordinates can be easily computed

by multiples of π/2. Speciﬁcally the y-axis of the markers are always aligned with the z-axis of the

world coordinate system. To compute a rotation vector of a marker in world coordinates, we ﬁrst

compute the required rotation around the x-axis, and then around its z-axis. For example, for the

marker m in Figure 2, we ﬁrst rotate it around its x-axis by π/2, and then around its z-axis by

π/2 again to coincide the marker coordinate frame with the world coordinate frame. The rotation

vector of the marker m in the world coordinate system is then recorded as wθm = [π/2, 0, π/2].

The ArUco suite provides calibration procedures as well as generating the marker images and

detecting them in still images. Using several views of the Charuco calibration chart [27] (Figure 8a),

the camera matrix and the corresponding distortion coeﬃcients are tuned. A highly accurate camera

matrix is mandatory for precise pose detection of the markers.

15

The current positioning of two cameras allows us to have an almost spherical world view with

the ﬁsh eye mode of the cameras, but we prefer the planar view mode of cameras to the ﬁsh eye

mode for three reasons: (i) the calibration procedure is challenging for the ﬁsh eye view, (ii) the ﬁsh

eye distortions have to be corrected before running the AR Marker detection algorithms as they

have to be fed with planar markers, and (iii) we do not need to see ceilings and with the planar

view mode, the cameras already have a large enough angle of view.

3.3. Handling setup errors

The errors in marker based localization are due to multiple factors that can be divided into two

sets: human errors and environmental errors. Human errors include mostly errors in measuring

the marker poses, reference points and unstable navigation. We can also put the camera calibra-

tion based errors into this set, which usually lack the necessary views for the calibration process.

Environmental errors are due mostly to ambient light ﬂuctuations.

The markers are installed with the following factors taken into consideration in order to minimize

the errors due to human factors: (i) From any position with any orientation in the area, at least one

marker should be detectable. Markers should be above the objects that can cause visual occlusion.

(ii) Markers are attached in the places illuminated uniformly and suﬃciently. (iii) Markers are

placed with the help of a jig to ensure that they are at a ﬁxed height and orientation to facilitate

the height and rotation measurements.

We minimize the errors due to camera calibration by using several views of the Charuco calibra-

tion chart [27] (Figure 8a), by which the camera matrix and the corresponding distortion coeﬃcients

are tuned properly. To evaluate the calibration and detection performance before running the main

set of experiments, we collect a preliminary set of still images. For this, the navigable platform is

put in the area in front of each marker at diﬀerent angles and distances, but the oﬃce has a raised

ﬂoor that is prone to ﬂexing while people are stationary or mobile in the proximity of the image

capture device. The tilting or oscillation of the image capture device can have a small eﬀect on the

captured position information. For calibration and veriﬁcation purposes, the authors waited until

the capture platform was stabilised and image capture was triggered wirelessly to obtain the best

results. Sample images can be seen in Figure 6. By adjusting the camera matrix and the parameters

to their best values, we achieve an empirical minimum mean squared distance error of 0.05 meters

for all markers installed in the area. As the recent studies on BLE based localization techniques

report localization errors no less than 1 meters for practical setups [28, 29], with our controlled

16

error rate at 0.05 meters, marker based localization is highly better than any other BLE or WiFi

based localization method. Thus this technique can be used as the ground truth to evaluate the

RF RSSI based localization performance.

The installation of the AR markers is a tedious and time consuming work, but this procedure can

be eased by considering the following points: (i) Instead of sticking the markers on the walls, ﬁxing

them on portable stands at predetermined elevations will reduce the installation time signiﬁcantly.

Polycarbonate stands should be preferred to minimize wireless signal attenuation and interference

due to these artiﬁcially introduced materials. The portable marker stands can be removed from the

area quickly after the data collection stage. (ii) Since markers can be recognized from any angle,

choosing common orientations like the multiples of π

2 will help greatly. With standard elevations
and easy orientation labeling, the experimenter is left with measuring the marker positions in 2D

coordinates. (iii) We use two motion cameras not to miss any marker at any time interval, but

this can be regarded as overkill. A single camera will also be suﬃcient to detect the markers,

but the focus of the camera should be static, as variable focal length will disrupt the marker

pose estimations. (iv) Instead of mobile beacons, one can use a mobile sensor (i.e. computer or

smartphone) to capture the BLE packets emitted by the beacons that furnish the area. Several

other sensors can be introduced into the setup, leading to more data that can improve the position

estimation accuracy.

3.4. Data collection software setup

For sharing data among multiple computers connected through Ethernet, we use the ROS

middleware. ROS middleware is a collection of open-source frameworks that are intended to be

used in robot software development. Besides the pure robotics services like low-level device control,

hardware abstraction, package management and common robotics functionality; the middleware

provides a message-passing service between processes running at diﬀerent devices. The messages of

sensor data, control, state, planning, actuator information are shared among processes, which are

called nodes. The nodes can publish or subscribe to the topics where the messages are streamed.

The middleware is developed to be platform and language independent.

In this work, the middleware is used for message-passing, visualizing and recording data. How-

ever, with its high and easy portability degree, the infrastructure allows plugging any node in

real-time. The proposed architecture is open to improvement in the domain of wireless sensor net-

works. Our ROS architecture, as seen in Figure 9, has three main node types: we have a master

17

Figure 9: Data collection and communication architecture: A ROS master computer (i) runs a master node that

orchestrates and takes the role of a system access point for the other nodes. Dedicated RPi devices (ii) run a publisher

node for each sensor to publish the sensor data into the system. Any other device (iii) can be used for live data

visualization, collection and higher level processing.

node (i) for orchestration and an entry point for other nodes. We prefer to dedicate another com-

puter, RPi-m, that runs the master node. This node is responsible for triggering the publishers in

the network. The network can easily be expanded with other nodes. A node that wants to enter

into the ROS network has to present itself to this master node. Each computer, RPi-i, runs three

publisher nodes, Publisher-ij, (ii), which are responsible for scanning the corresponding Bluetooth

devices, Sensor-ij, generate RSSI parameters from incoming Bluetooth packets, and publish these

parameters as ros messages. The published messages include a timestamp, MAC address of the

source beacon, MAC address of the sensor that captures the Bluetooth packet and the RSSI value

in decibels. Finally, we have a visualizer (iii) that subscribes to the RSSI topic messages and dis-

plays them in live windows with respect to time. The visualizer is also responsible for recording

the data in log ﬁles for each sensor. The master, publisher and visualizer nodes are running on

Python-3 scripts. The visualizers also have a graphical user interface based on PyQt5 wrappers

(see Figure 10).

To ease the synchronization of video and RSSI data, we develop and utilize a custom toolkit

written with PyQt5. A snapshot of the toolkit can be seen in Figure 11. The toolkit is used for

performing several tasks required for calibration, pose estimation and synchronization processes:

18

Local Area Network over EthernetRPi-1RPi-4RPi-2RPi-3Sensor-2aSensor-2cSensor-2bSensor-1aSensor-1cSensor-1bSensor-4aSensor-4bSensor-4cSensor-3aSensor-3cSensor-3bUSBUSBUSBUSBUSBUSBUSBUSBPublisher-1aPublisher-1cPublisher-1bPublisher-2bPublisher-2aPublisher-2cPublisher-3bPublisher-3aPublisher-3cPublisher-4aPublisher-4cPublisher-4bRPi-mAny device(i)(iii)(ii)UARTUARTUARTUARTVisualizerProcessorROS masterRecorderFigure 10: RSSI data ﬂow on ROS (left) and the live data visualizer (right).

Figure 11: AR localization toolkit for estimating, visualizing, synchronizing and logging data.

such as displaying video frames from two cameras and navigate through the frames freely; detecting

markers given the proper calibration parameters and displaying them in video frames; estimating

raw positions given the markers; applying diﬀerent elimination strategies; toggling Kalman ﬁlter

and tuning its noise parameters on the ﬂy; displaying raw, eliminated or smoothed poses on the

map with their related markers; taking snapshots from the map (most of the ﬁgures including maps

in this article are generated by this toolkit); marking the needed timestamps for synchronization;

displaying logged or predetermined trajectories; and logging the obtained poses into data ﬁles.

19

3.5. Data collection experiments

We collect two types of data simultaneously: timestamped RSSI data captured by BLE sensors,

and position estimations through the videos captured by the cameras (see Figure 12a).

(a) Detected markers in forward and backward camera views (left) and corre-

(b) Predetermined paths of the experiments.

sponding raw pose estimations (right).

Figure 12: Position data collection and experiments

We prepare ﬁve experimental predetermined paths for testing the performance of our method-

ology: three distinct straight paths, a rectangular path and a zigzagging one. The camera-beacon

bundle is driven along these paths trying to respect a linear and smooth motion, simulating human

movement. The predetermined paths are also given in Figure 12b. The data collection processes

are varied by changing the pace on the straight paths and adding rotation on the rectangular and

zigzagging paths, which makes a dataset of 15 trajectories.

The collected data are processed by varying parameters for the elimination strategies, the closest

selection, C, and the outlier elimination, U ; and with the measurement error matrix of Kalman

ﬁlter, Q = qId. We predeﬁne the transition noise matrix as Rt = 0.01Id, which favors the pose

estimations being closer to the previous ones. Some of the estimated paths are given in Figure 13.

There is a reason that we collect data from more complicated paths along with the pure straight

paths. The rectangular paths investigate an expected phenomenon whether the beginning and the

end of the trajectories point the same positions on the map, closing the loop. We observe that these

same positions (and even orientations) are successfully captured. Moreover, we witness that even a

point rotation may complicate the path on its own, because of the relative position of the camera-

beacon bundle with respect to the rotation axis. However, we see that the selected parameters

20

Figure 13: Sample paths of collected data: Diﬀerent straight paths, bendy paths with and without point rotations,

and paths at diﬀerent paces.

of the Kalman ﬁlter can compensate for this jumping behavior successfully. In our perspective, a

complicated path refers to diﬀerent things: this may be related to paths with turns and covering

more space in the map, but we think that the pace makes things more complicated than choosing

bendy paths, so we investigate this phenomenon and add the related results at the end of Section 4.

3.6. Position annotated dataset

The dataset provides the trajectory data ﬁles that include the position annotated RSSI data

collected while navigating the special platform in the test area. There are 15 diﬀerent trajectory

ﬁles with varying lengths, forms and paces. Each sample in a ﬁle comes with a timestamp, a

transmitter address, a sensor address, an RSSI reading, a ground truth position in 3D and its

associated rotation matrix. We also supply the map of the test area, the ﬁles for the conﬁguration

of the sensors and various parameters that are required to convert the measurements in meters

into pixels, and vice versa. For the completeness of an indoor localization project, we publish

auxiliary data of raw ﬁngerprints, their processed histograms, two sets of estimated probabilistic

radio frequency maps in the grid form and sets of occupancy maps [30].

21

4. Results

We hypothesize that raw poses of the camera-beacon bundle display two types of faults, by

visualizing raw poses given in Figure 12a: (i) they may be highly erroneous and positioned very far

from the true position, because of highly incorrect and inverted axes of occluded or very distant

markers (e.g. two points in the lower part of the map), or (ii) they may ﬂuctuate around a true point

because of pixel-wise problems with smaller errors (e.g. the ones around the dominant cluster). The

former types of errors can be controlled by discarding the estimations due to the farthest markers,

and then an outlier targeted pruning can be employed on the remaining poses for the latter cases.

Figure 14: Deviations from the predetermined linear paths of raw poses and isolated elimination strategies: C for

close marker selection and U for outlier elimination.

To evaluate the estimated trajectories, we report how the estimations deviate from the prede-

termined linear paths, knowing that data are collected by the experimenter that follows straight

segments as given in Figure 12b. We ﬁrst compare the isolated versions of the two elimination

strategies with the errors of raw poses (without elimination). According to Figure 14 and in ac-

cordance with our hypothesis, it is obvious that a pose elimination strategy is mandatory, and

between these isolated elimination strategies, the outlier elimination strategy with U = 15 per-

forms better than others, giving an error median of 0.045. Moreover, the error distributions for the

selection strategy display undesired skewed distributions with high numbers of outliers. However,

even though a pure close marker selection strategy performs signiﬁcantly worse than a pure outlier

elimination strategy, we still prefer to begin with a selection strategy because of its speed and

ability to discard probable systematic outliers due to misestimated rotation axes.

In Figure 15, we compare diﬀerent combinations of the pose elimination strategies with the

isolated selection strategy with U = 15. Whereas there is not a signiﬁcantly best pose elimination

combination, a combination of a selection strategy with C = 20 and a succeeding outlier elimination

22

Figure 15: Deviations from the predetermined linear paths of the elimination strategy combinations with diﬀerent

parameters: C for close marker selection and U for outlier elimination.

with U = 10 distinguishes among the experiments performed, with a median and mean deviations

of 0.043 m and 0.068 m, respectively. Usage of a close marker selection is also preferable because

a direct application of the greedy outlier elimination strategy is computationally intensive, so it

would be also intuitive to precede it with the quicker close marker selection strategy, since the far

markers tend to yield misleading estimations. Aligned with this preference, we select a combination

of the pose elimination strategies with C = 20, U = 10, which corresponds to quickly reducing the

number of poses from about 100-150 down to 20 with the close marker selection strategy, and then

applying the outlier elimination method on a set of 20 poses to obtain a ﬁnal set of 10 poses.

Figure 16: Deviations from the predetermined linear path after the Kalman ﬁlter with diﬀerent noise covariance

multipliers, preceded by a combination of selection and outlier detection strategies.

On the selected pose estimations after the elimination techniques, we employ a Kalman ﬁlter to

attain smoother trajectories. Figure 16 summarize the parameters for the noise covariance multi-

plier, q, and the deviations after employing the Kalman ﬁlter on the preprocessed pose estimations.

We decide that the deviation medians can be reduced down to 0.034 meters and the means under

0.05 meters with a covariance matrix of Q = 2.0Id.

23

Figure 17: Deviations from the predetermined linear path after the Kalman ﬁlter with diﬀerent covariance multipliers

and lag penalty application.

Smoothing with the Kalman ﬁlter misleads to inaccurate trajectories, because we discard time

up to this point. We measure and compare the deviations of the estimations from the predeﬁned

ideal paths. Smoother paths yield lower deviations, however, when the Kalman ﬁlter is applied on

the pose estimations, dragging is also possible if the noise covariance multiplier, q, of the Kalman

ﬁlter is chosen to be too high.

If this covariance is excessively high the estimated trajectory is

surely smooth, yielding very low deviances, but fails to estimate the real positions with respect to

time. This dragging can be evaluated by measuring the distance between a real known end point of

a segment and the estimated points at the same timestamp. To avoid high dragging situations, we

penalize the experiment by accumulating these drag distances as errors on the deviations, and we

obtain Figure 17. The parameters with q > 0.2 are highly penalized, meaning that estimated paths

do not lead to accurate localization, and with q < 0.1, less smoothing is applied. We conclude that

choosing q ∈ [0.1, 0.2] for these experiments results with the best performances.

(a)

(b)

(c)

(d)

Figure 18: Experiments with diﬀerent strategies with black dots denoting the estimated positions and green segments

the predeﬁned paths: (a) raw position estimations, (b) estimations after only the Kalman ﬁlter, (c) only elimination

strategies, (d) and the Kalman ﬁlter preceded by elimination strategies.

24

One may propose to apply a Kalman ﬁlter directly on the raw position estimations (Figure 18a)

without any prior elimination strategy (see Figure 18b), however, as the raw poses are highly

erroneous because of mainly misdetection of marker orientations, the Kalman ﬁlter is unable to

cope with raw poses unaidedly. An elimination strategy is mandatory. The positions after the

elimination strategies still draw ﬂuctuating trajectories as seen in Figure 18c, but the selected

position estimations are distributed around the true paths. These ﬂuctuations may be eliminated

with online ﬁltering mechanisms. Figure 18d is the result of ﬁrst applying the aforementioned

elimination strategies on the positions estimated by the ArUco marker detection algorithm, and then

smoothing the intermediate results using the Kalman ﬁlter with proper noise covariance parameters.

We see that the green predetermined path can also be visually matched with the ﬁltered positions.

We consider a ﬁnal situation where Kalman parameters are also aﬀected by the speed of the

navigation. We also test the same experiment at three diﬀerent paces. The results in Figure 19

show that choosing q = 0.2 and q = 0.1 for slow navigations (at about 0.3 m/s) and for quicker

navigations (at about 0.4 m/s) respectively will yield better performances.

Figure 19: Deviations from the predetermined linear path after employing the Kalman ﬁlter with diﬀerent noise

covariance multipliers and lag penalty application for the experiments at varying paces.

25

5. Conclusion

This work proposes techniques for collecting high precision position data using AR markers and

annotating BLE RSSI with precise positions in both time and space domain. We decorate a 364

square meter oﬃce area with AR markers and BLE sensing equipment to collect simultaneous video

and RSSI data using a low cost and practical custom setup. The videos are used to estimate the

noisy positions, which are then eliminated and ﬁltered into smoother trajectories, highly correlated

with the ideal paths. With the aforementioned processing and synchronization techniques, the

RSSI data can be annotated by the precise position estimations to obtain position annotated RSSI

datasets.

The results show that with AR markers we can achieve very high precision positioning (under

0.05 meters) in controlled indoor areas. This makes the proposed AR based positioning method

far more accurate than the state-of-art WiFi or BLE based positioning. Of course, the aim is not

to compare the positioning systems. Instead, the proposed setup and method can serve the indoor

positioning domain as a benchmark tool to measure and verify other yet less precise localization

performances in the indoor positioning domain. Moreover, we believe that there still exists some

area to make the AR based positioning system more precise, especially with better illumination

conditions, more rigorous measurements or larger and more markers.

In the literature, indoor positioning algorithms are evaluated mostly by manually measuring

the predeﬁned reference points. The lack of high frequency position annotations prevents the

community from standardizing the evaluation procedures. The position estimations of the tracking

algorithms are assessed using various distinct techniques. This need makes high frequency ground

truth data very valuable. Besides, we bring forth not only a method for ﬁne grained ground truth

data collection, but also a method for collecting data practically. Considering the lack of high

precision data sets in the indoor positioning community, we make the data publicly available for

the researchers who want to test their algorithms based on RSSI data.

We regard the AR based localization as a one-time prior assistive tool for the calibration of

a wireless indoor positioning system. It is intended for collecting position annotated data which

can later be used for parameter estimation of the tracking algorithms, ﬁngerprinting and more

importantly evaluation of the whole system in terms of position estimation accuracy.

Whereas, in this work, the data are shown to be processed and recorded by a single node, the

infrastructure and the dataset allow the researchers to approach the wireless based indoor tracking

problem through a distributed and decentralized manner. As an example, as a future work, it can

26

be considered to handle the problem in a pure decentralized way, where each computer or sensor

node is to estimate the position of an emitter and share its belief with the neighboring processors.

This approach will make the setup easily scalable to any area.

The main focus of this work is to propose a method for collecting position annotated wireless

data. Nevertheless the most prominent deﬁciency of the output is that the dataset is not constructed

to reﬂect the human body inﬂuence on the wireless signals, even though there is a person that drives

the navigable platform. As wireless based positioning systems are usually considered to be human

or living things centered, the lack of this inﬂuence in the dataset may be considered important.

Moreover, the dataset cannot be projected to the true nature of the crowded environments in terms

of the signal quality. However, for the purposes of tracking non-living things or living things in

sparse environments, the dataset still reﬂects the true nature and is valid to become a benchmark

testing of wireless based positioning and tracking systems. Besides, the method itself and the

system architecture are not prone to human body inﬂuence as they are already designed to be

redundant and scalable.

Collecting wireless data using the current setup and constructing a new dataset inﬂuenced by

the human body are left as future work highly due to the current pandemic situation. The future

data set should include varying versions of the same experiment where the object to be tracked is

held, pocketed or worn as an accessory. Moreover varying the movement and the number of people

is another issue that will present diﬀerent natural conditions.

As more future work, with proper tunings, this method can be used for automatic, online and

fast ﬁngerprinting for wireless signal based localization, which is substantially a challenging task

for the researchers working in the domain. With the markers installed in the area, the camera-

beacon bundle may be navigated to collect data from any point, and these data can be processed

to form the ﬁngerprints fast and easily. Moreover, a self navigating robot can be used for real

automatization of ﬁngerprinting. Furthermore, the technique also permits collecting data in three

dimensions, which will prove to be essential for indoor localization of robots, speciﬁcally drones.

Acknowledgment

This work is supported by the Turkish Directorate of Strategy and Budget under the TAM

Project number 2007K12-873. The authors would like to thank Sergio Bromberg for his contribution

of the back to back GoPro camera mount 3D model to Thingiverse and PoiLabs for supplying POI

Beacons for this work.

27

References

[1] I. Bisio, F. Lavagetto, M. Marchese, A. Sciarrone, Smart probabilistic ﬁngerprinting for wiﬁ-

based indoor positioning with mobile devices, Pervasive and Mobile Computing 31 (2016)

107–123.

[2] R. Faragher, R. Harle, Location ﬁngerprinting with bluetooth low energy beacons, IEEE Jour-

nal on Selected Areas in Communications 33 (11) (2015) 2418–2428.

[3] J. Torres-Sospedra,

´Oscar Belmonte-Fern´andez, G. M. Mendoza-Silva, R. Montoliu,

A. Puertas-Cabedo, L. E. Rodr´ıguez-Pupo, S. Trilles, A. Calia, M. Benedito-Bordonau,

J. Huerta, 3 - lessons learned in generating ground truth for indoor positioning systems based

on wi-ﬁ ﬁngerprinting, in: Geographical and Fingerprinting Data to Create Systems for In-

door Positioning and Indoor/Outdoor Navigation, Intelligent Data-Centric Systems, Academic

Press, 2019, pp. 45 – 67.

[4] G. M. Mendoza-Silva, M. Matey-Sanz, J. Torres-Sospedra, J. Huerta, Ble rss measurements

dataset for research on accurate indoor positioning, Data 4 (1) (2019) 12.

[5] P. Baronti, P. Barsocchi, S. Chessa, F. Mavilia, F. Palumbo, Indoor bluetooth low energy

dataset for localization, tracking, occupancy, and social interaction, Sensors 18 (12) (2018)

4462.

[6] M. Girolami, F. Mavilia, F. Delmastro, A bluetooth low energy dataset for the analysis of

social interactions with commercial devices, Data in Brief 32 (2020) 106102.

[7] E. Sansano-Sansano, F. J. Aranda, R. Montoliu, F. J. ´Alvarez, Ble-gspeed: A new ble-based

dataset to estimate user gait speed, Data 5 (4) (2020) 115.

[8] O. S. Eyobu, A. Poulose, D. S. Han, An accuracy generalization benchmark for wireless in-

door localization based on imu sensor data, in: 2018 IEEE 8th International Conference on

Consumer Electronics - Berlin (ICCE-Berlin), 2018, pp. 1–3.

[9] C. M. de la Osa, G. G. Anagnostopoulos, M. Togneri, M. Deriaz, D. Konstantas, Positioning

evaluation and ground truth deﬁnition for real life use cases, in: 2016 International Conference

on Indoor Positioning and Indoor Navigation (IPIN), 2016, pp. 1–7.

28

[10] A. Poulose, D. S. Han, Hybrid deep learning model based indoor positioning using wi-ﬁ rssi

heat maps for autonomous applications, Electronics 10 (1) (2021) 2.

[11] H. Ai, K. Tang, W. Huang, S. Zhang, T. Li, Fast ﬁngerprints construction via GPR of high

spatial-temporal resolution with sparse RSS sampling in indoor localization, Computing 102 (3)

(2020) 781–794.

[12] S. Adler, S. Schmitt, K. Wolter, M. Kyas, A survey of experimental evaluation in indoor

localization research, in: 2015 International Conference on Indoor Positioning and Indoor

Navigation (IPIN), 2015, pp. 1–10.

[13] J. Kim, H. Jun, Vision-based location positioning using augmented reality for indoor naviga-

tion, IEEE Transactions on Consumer Electronics 54 (3) (2008) 954–962.

[14] G. C. La Delfa, S. Monteleone, V. Catania, J. F. De Paz, J. Bajo, Performance analysis of

visualmarkers for indoor navigation systems, Frontiers of Information Technology & Electronic

Engineering 17 (8) (2016) 730–740.

[15] R. S. Xavier, B. M. F. da Silva, L. M. G. Goncalves, Accuracy analysis of augmented reality

markers for visual mapping and localization, in: 2017 Workshop of Computer Vision (WVC),

2017, pp. 73–77.

[16] E. Ujkani, J. Dybedal, A. Aalerud, K. B. Kaldestad, G. Hovland, Visual marker guided point

cloud registration in a large multi-sensor industrial robot cell, in: 2018 14th IEEE/ASME

International Conference on Mechatronic and Embedded Systems and Applications (MESA),

2018, pp. 1–6.

[17] A. Alnabhan, B. Tomaszewski, Insar: Indoor navigation system using augmented reality, in:

Proceedings of the Sixth ACM SIGSPATIAL International Workshop on Indoor Spatial Aware-

ness, ISA ’14, Association for Computing Machinery, New York, NY, USA, 2014, p. 36–43.

[18] I. A. Koc, T. Serif, S. G¨oren, G. Ghinea, Indoor mapping and positioning using augmented re-

ality, in: 2019 7th International Conference on Future Internet of Things and Cloud (FiCloud),

2019, pp. 335–342.

[19] D. Byrne, M. Kozlowski, R. Santos-Rodriguez, R. Piechocki, I. Craddock, Residential wearable

rssi and accelerometer measurements with detailed location annotations, Scientiﬁc Data 5 (1)

(2018) 180168.

29

[20] F. J. Romero-Ramirez, R. Mu˜noz-Salinas, R. Medina-Carnicer, Speeded up detection of

squared ﬁducial markers, Image and Vision Computing 76 (2018) 38–47.

[21] S. Garrido-Jurado, R. Munoz-Salinas, F. Madrid-Cuevas, R. Medina-Carnicer, Generation of

ﬁducial marker dictionaries using mixed integer linear programming, Pattern Recognition 51

(2016) 481–491.

[22] K. K. Liang, Eﬃcient conversion from rotating matrix to rotation axis and angle by extending

rodrigues’ formula (Oct. 2018). arXiv:1810.02999.

[23] M. A. Fischler, R. C. Bolles, Random sample consensus: A paradigm for model ﬁtting with

applications to image analysis and automated cartography, Commun. ACM 24 (6) (1981)

381–395.

[24] J. Wang, A. Hu, X. Li, Y. Wang, An improved pdr/magnetometer/ﬂoor map integration

algorithm for ubiquitous positioning using the adaptive unscented kalman ﬁlter, ISPRS Inter-

national Journal of Geo-Information 4 (4) (2015) 2638.

[25] Z. Chen, Q. Zhu, Y. C. Soh, Smartphone inertial sensor-based indoor localization and tracking

with ibeacon corrections, IEEE Transactions on Industrial Informatics 12 (4) (2016) 1540–1549.

[26] M. Quigley, B. Gerkey, K. Conley, J. Faust, T. Foote, J. Leibs, E. Berger, R. Wheeler, A. Ng,

Ros: an open-source robot operating system, in: Proc. of the IEEE Intl. Conf. on Robotics

and Automation (ICRA) Workshop on Open Source Robotics, Kobe, Japan, 2009.

[27] G. H. An, S. Lee, M.-W. Seo, K. Yun, W.-S. Cheong, S.-J. Kang, Charuco board-based

omnidirectional camera calibration method, Electronics 7 (12) (2018) 421.

[28] S. Tomaˇziˇc, I. ˇSkrjanc, An automated indoor localization system for online bluetooth signal

strength modeling using visual-inertial slam, Sensors 21 (2021) 2857.

[29] H. Obeidat, W. Shuaieb, O. Obeidat, R. Abd-Alhameed, A review of indoor localization

techniques and wireless technologies, Wireless Personal Communications 119 (1) (2021) 289–

327.

[30] F. S. Dani¸s, A. T. Cemgil, C. Ersoy, Adaptive sequential monte carlo ﬁlter for indoor posi-

tioning and tracking with bluetooth low energy beacons, IEEE Access 9 (2021) 37022–37038.

30

