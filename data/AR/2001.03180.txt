0
2
0
2

n
a
J

9

]

M

I
.
h
p
-
o
r
t
s
a
[

1
v
0
8
1
3
0
.
1
0
0
2
:
v
i
X
r
a

MNRAS 000, 1–20 (2019)

Preprint 13 January 2020

Compiled using MNRAS LATEX style ﬁle v3.0

MSTAR – a fast parallelised algorithmically regularised
integrator with minimum spanning tree coordinates

Antti Rantala1,2(cid:63), Pauli Pihajoki2, Matias Mannerkoski2, Peter H. Johansson2,
Thorsten Naab1
1Max-Planck-Institut f¨ur Astrophysik, Karl-Schwarzchild-Str. 1, D-85748, Garching, Germany
2Department of Physics, Gustaf H¨allstr¨omin katu 2, University of Helsinki, Finland

Accepted XXX. Received YYY; in original form ZZZ

ABSTRACT
We present the novel algorithmically regularised integration method MSTAR for high
accuracy (|∆E/E | (cid:38) 10−14) integrations of N-body systems using minimum spanning
tree coordinates. The two-fold parallelisation of the O(N 2
part) force loops and the substep
divisions of the extrapolation method allows for a parallel scaling up to NCPU = 0.2 ×
Npart. The eﬃcient parallel scaling of MSTAR makes the accurate integration of much
larger particle numbers possible compared to the traditional algorithmic regularisation
chain (AR-CHAIN) methods, e.g. Npart = 5000 particles on 400 CPUs for 1 Gyr in a few
weeks of wall-clock time. We present applications of MSTAR on few particle systems,
studying the Kozai mechanism and N-body systems like star clusters with up to Npart =
104 particles. Combined with a tree or a fast multipole based integrator the high
performance of MSTAR removes a major computational bottleneck in simulations with
regularised subsystems. It will enable the next generation galactic-scale simulations
with up to 109 stellar particles (e.g. m(cid:63) = 100M(cid:12) for a M(cid:63) = 1011 M(cid:12) galaxy) including
accurate collisional dynamics in the vicinity of nuclear supermassive black holes.

Key words: gravitation – methods: numerical – quasars: supermassive black holes
– galaxies: star clusters: general

1 INTRODUCTION

Galactic nuclei and their nuclear stellar clusters are among
the densest stellar systems in the entire Universe (Misgeld
& Hilker 2011). The nuclei of massive galaxies also host su-
permassive black holes (SMBHs) with typical masses in the
range of M = 106–1010 M(cid:12) (Kormendy & Richstone 1995;
Ferrarese & Ford 2005; Kormendy & Ho 2013), forming
a complex, collisionally evolving stellar-dynamical environ-
ment (e.g. Merritt 2013; Alexander 2017). In the ΛCDM
hierarchical picture of structure formation, galaxies grow
through mergers and gas accretion, resulting in situations
where the collisional evolution of a galactic nuclei is intermit-
tently interrupted and transformed by a merger (e.g. White
& Rees 1978; Begelman et al. 1980). For gas-poor mergers,
the more concentrated nucleus with a steeper stellar cusp,
will determine the structure of the remnant nucleus imme-
diately after the merger (Holley-Bockelmann & Richstone
1999; Boylan-Kolchin & Ma 2004).

If both of the merging galaxies host central SMBHs, the
SMBHs will merge through a three-stage process (Begelman

(cid:63) E-mail: anttiran@mpa-garching.mpg.de

© 2019 The Authors

et al. 1980). First on larger scales the SMBHs are brought
together through dynamical friction from stars and gas until
a hard binary is formed with a semi-major axis of a ∼ 10 pc.
In the second phase the hard binary will interact with the
stars in the centre of the merger remnant (Begelman et al.
1980; Milosavljevi´c & Merritt 2001, 2003; Khan et al. 2011),
scouring a low-density stellar core in the process (e.g. Mer-
ritt 2006; Lauer et al. 2007; Rantala et al. 2018, 2019). The
largest uncertainty in this process is the rate at which the
‘loss cone’ is depleted, but there is an emerging consensus
that the binary will avoid the so called ﬁnal-parsec problem
and eventually merge into a single SMBH, even in the col-
lisionless limit (e.g. Berczik et al. 2006; Vasiliev et al. 2015;
Gualandris et al. 2017; Ryu et al. 2018; Mannerkoski et al.
2019), with the ﬁnal coalescence driven by the emission of
gravitational waves (Peters & Mathews 1963).

If the galaxy merger is gas-rich, the evolution of the nu-
cleus of the merger remnant proceeds very diﬀerently. Dur-
ing galaxy mergers the resulting asymmetric potential eﬀec-
tively funnels gas inwards into the central regions causing
a central starburst which rapidly increases the nuclear stel-
lar density by several orders of magnitude (e.g. Sanders &
Mirabel 1996). In addition, the gas also plays an important

 
 
 
 
 
 
2

A. Rantala et al.

role by causing additional drag on the SMBHs (Beckmann
et al. 2018), as well as by forming circumbinary disks which
can have a signiﬁcant and complicated eﬀects on the angu-
lar momentum evolution of the binary (Tang et al. 2017;
Moody et al. 2019; Duﬀell et al. 2019). In general, SMBH
mergers are thought to occur very rapidly in dense gas-rich
environments (e.g. Khan et al. 2016) mainly due to the short
duration of the stellar interaction phase of the binary evo-
lution (Quinlan 1996).

The growth of SMBHs and the formation of binaries
in galaxy mergers have been extensively studied in recent
decades. A typical numerical approach is to use grid codes
(Kim et al. 2011; Dubois et al. 2013; Hayward et al. 2014),
smoothed particle hydrodynamics codes with tree gravity
(Springel et al. 2005; Mayer et al. 2007; Johansson et al.
2009) or direct summation codes (Berczik et al. 2006; Khan
et al. 2011). The drawback of grid codes and softened tree
codes is that they cannot properly treat collisional systems
as the grid cell size or the employed gravitational softening
length places a strict spatial resolution limit on the simula-
tion.

Direct summation codes on the other hand are very well
suited for studying collisional stellar systems with Npart (cid:46)
106 particles, such as globular clusters. However, the steep
O(N2
part) scaling of the direct summation algorithm limits the
applicability of this method to systems with much higher
particle numbers. In addition, the most widely used direct
summation codes are rarely coupled with a hydrodynamic
solver. One possibility is to use on-the-ﬂy code switching
(Milosavljevi´c & Merritt 2001; Khan et al. 2016), but this
type of procedure is typically cumbersome and may intro-
duce spurious numerical eﬀects into the simulation. We thus
argue that a self-consistent numerical framework for simu-
lating SMBH dynamics in a realistic collisional environment
with a high stellar density and including also a gas compo-
nent still remains to be developed. One of the major obsta-
cles for developing such a code has been the lack of available
fast accurate small-scale collisional post-Newtonian integra-
tors, which are also straightforward to couple to both large-
scale gravity solvers and hydrodynamical methods.

The most widely used method to treat binaries and close
encounters of simulation particles in the collisional regions of
simulations is the technique of regularisation. The key idea
of regularisation is to transform the equations of motion of a
dynamical system into a form without coordinate singular-
ities, which makes solving the dynamics signiﬁcantly easier
using standard numerical integration techniques (Aarseth
2003). The ﬁrst such method with practical numerical ap-
plications was the two-body Kustaanheimo-Stiefel (KS) reg-
ularisation method (Kustaanheimo & Stiefel 1965), which
transformed both the time and spatial coordinates of the
system. A major step forward for regularised dynamics was
the introduction of the chain concept. By arranging the sim-
ulation particles into a chain of inter-particle vectors, the
KS-CHAIN of Mikkola & Aarseth (1993) reduced the num-
ber of required KS transformations from Npart(Npart − 1)/2 to
Npart−1 yielding a much more eﬃcient regularisation method.
In the N-body codes of Aarseth (1999) the KS-CHAIN treats
the mutual encounters of not more than six particles simul-
taneously.

was discovered by Mikkola & Tanikawa (1999a) and Preto
& Tremaine (1999). This algorithmic regularisation (AR)
method is faster than the previous regularisation methods
and more accurate, especially in the case of large mass ratios
between particles in an N-body system. Many current direct
summation codes (Harfst et al. 2008; Aarseth 2012) and reg-
ularised tree codes (Rantala et al. 2017) use an implementa-
tion of the AR-CHAIN integrator (Mikkola & Merritt 2006,
2008) to resolve the small-scale dynamics around SMBHs.

Despite the many successes of regularisation methods,
their original design as few-body codes still limits their ap-
plicability to systems with a very large number of particles,
which is crucial for performing galactic-scale simulations at
high accuracy. The regularised tree code KETJU (Rantala
et al. 2017) is currently limited to a particle resolution of
Npart (cid:46) 107 stellar particles per galaxy, although the un-
derlying tree code GADGET-3 (Springel et al. 2005) could
easily run simulations with ∼ 10–100 times more stellar par-
ticles, up to Npart ∼ 109 collisionless particles in a single
galaxy. This is because the included AR-CHAIN integrator
becomes impractically ineﬃcient with more than ∼ 300–500
particles in the regularised chain region. We note that simi-
lar performance issues with the KS-CHAIN algorithm have
already been reported in the literature, see e.g. Milosavljevi´c
& Merritt (2001).

In this paper we present a new algorithmically regu-
larised integrator MSTAR developed and implemented with
the aim of addressing some of the shortcomings of the pre-
vious algorithms. Our new code contains two main improve-
ments compared to existing algorithmically regularised (AR)
integrators. Firstly, We use a minimum spanning tree (MST)
coordinate system instead of the chain structure, motivat-
ing the name of the code. We note that regularised integra-
tion algorithms using particular tree structures have been
implemented before: Jernigan & Porter (1989) developed a
KS-regularised binary tree code while Mikkola & Aarseth
(1989) proposed a ‘branching’ KS-chain structure for few-
body regularisation. However, neither of these methods are
widely used today. Secondly, a major speed-up compared to
the previous regularisation methods is achieved by apply-
ing a two-fold parallalisation strategy to the extrapolation
method, which is used in regularised integrators to ensure a
high numerical accuracy (Mikkola & Tanikawa 1999b).

The remainder of this paper is organised as follows: in
Section 2 we review our implementation of AR-CHAIN, as
the new code MSTAR was developed based on our earlier
experience with the AR-CHAIN integrator. The numerical
and parallelisation procedures of the MSTAR integrator and
the code implementation are discussed in Section 3. We de-
scribe and test the parallel extrapolation method in Section
4. In Section 5 we perform a number of few-body code tests
to validate the accuracy of the MSTAR code, whereas in
Section 6 we perform a number of scaling and timing tests.
Finally, we summarise our results and present our conclu-
sions in Section 7.

2 AR-CHAIN

2.1 Time transformation of equations of motion

A new regularisation method which does not require a
coordinate transformation but only a time transformation

The algorithmic regularisation chain (AR-CHAIN) integra-
tor is designed to perform extremely accurate orbital inte-

MNRAS 000, 1–20 (2019)

grations of gravitational few-body systems (Mikkola & Mer-
ritt 2006, 2008). The equations of motion of the system are
time-transformed by extending the phase space to include
the original time parameter as a coordinate together with
the corresponding conjugate momentum, equal to the bind-
ing energy of the system. A new independent variable is then
introduced through a Poincar´e time transformation. With a
speciﬁc choice of the time transformation function (Mikkola
& Tanikawa 1999a; Preto & Tremaine 1999), the new Hamil-
tonian and the equations of motion are separable so that the
system can be integrated using a leapfrog method. This sur-
prisingly yields an exact orbit for the Keplerian two-body
problem even for collision orbits. The only error is in the
time coordinate, or the phase of the Keplerian binary. How-
ever, this error can be removed by a further modiﬁcation
of the Hamiltonian (Mikkola et al. 2002), yielding an exact
solver, up to machine precision.

We start with the standard N-body Hamiltonian H de-

ﬁned as
H = T − U = (cid:213)
i

1
2

mi (cid:107)vi (cid:107)2 −

(cid:213)

(cid:213)

i

j>i

Gmimj
(cid:107)r j − r i (cid:107)

(1)

in which T is the kinetic energy and U is the force function,
equal to negative of the potential energy. This Hamiltonian
yields the familiar Newtonian equations of motion for the
particle positions r i and velocities vi:

dr i
dt
dvi
dt

= vi

= ai = G

(cid:213)

i(cid:44)j

mj

r j − r i
(cid:107) r j − r i (cid:107)3

(2)

in which we have introduced the Newtonian accelerations
ai. Possible additional acceleration terms, such as exter-
nal perturbations f i depending only on particle positions,
or velocity-dependent perturbations gi(v), such as post-
Newtonian corrections, can be directly added to the Newto-
nian accelerations, yielding ai → ai + f i

+ gi(v).

Next, we perform the time transformation (Mikkola &
Tanikawa 1999a; Preto & Tremaine 1999). A ﬁctitious time
s is introduced as a new independent variable. The original
independent variable, physical time t, is promoted to a co-
ordinate of the phase space of the system, while the binding
energy of the system B = −H becomes the corresponding
conjugate momentum. The old and new time variables are
related by the inﬁnitesimal time transformation

dt
ds

=

1
αU + βΩ + γ

(3)

in which the parameter triplet (α, β, γ) determines the type
of regularisation. Ω is an arbitrary real-valued function of
coordinates, such as the force function for the least massive
particles in the system (Mikkola & Aarseth 2002). With the
triplet (1, 0, 0) the method becomes the logarithmic Hamil-
tonian (LogH) method of Mikkola & Tanikawa (1999a) and
Preto & Tremaine (1999), while (0, 1, 0) corresponds to the
time-transformed leapfrog (TTL) introduced in Mikkola &
Aarseth (2002). The ordinary non-regularised leapfrog is ob-
tained by choosing the triplet (0, 0, 1). Of all the possible
choices, Mikkola & Merritt (2006) recommend using the log-
arithmic Hamiltonian option (1, 0, 0) for its superior numer-
ical performance.

MNRAS 000, 1–20 (2019)

A fast regularised integrator

3

Using the logarithmic Hamiltonian time transforma-

tion, the equations of motion for the system become

=

=

vi

1
T + B
1
T + B

dt
ds
dr i
ds
for the coordinates and
dvi
ds
dB
ds

(ai + f i
1
(cid:213)
U

mivi · ( f i

= 1
U

+ gi(v))

= −

i

+ gi(v))

(4)

(5)

for the velocities. In this discussion we omit the Ω function
and its velocity conjugate. For an unperturbed Newtonian
= gi(v) = 0 the derivatives of the coordinates
system i.e. f i
depend only on the velocities and vice versa, thus a leapfrog
algorithm can be constructed in a straightforward manner.
With non-zero external tidal perturbations f i the derivative
of the binding energy B depends on the particle velocities,
but the dependence is only linear and can thus be analyti-
cally integrated over the time-step (see e.g. the Appendix A
and B of Rantala et al. 2017).

An explicit leapfrog algorithm cannot be constructed
if the post-Newtonian accelerations gi(v) are non-zero. One
can in practise approach the problem by iterating the im-
plicit equations of motion, but this is very ineﬃcient. An
eﬃcient post-Newtonian leapfrog algorithm with velocity-
dependent accelerations can be implemented by extending
the phase space of the system with auxiliary velocities wi.
A single leapfrog velocity update (kick) is replaced by an
alternating combination of auxiliary and physical kicks, per-
formed in a standard leapfrog manner. For additional details
of the auxiliary velocity procedure see Hellstr¨om & Mikkola
(2010) and its generalisation by Pihajoki (2015).

Nevertheless, the LogH integrator on its own is not
accurate enough for high-precision solutions of general N-
body systems, even though the systems are regularised
against collision singularities. The LogH leapfrog must be
supplemented with additional numerical techniques such as
chained coordinates and extrapolation techniques. These
two methods are introduced in Section 2.2 and Section 2.3,
respectively.

2.2 Chained coordinate system

In AR-CHAIN the chained inter-particle coordinate system
does not play a role in the regularisation procedure itself,
unlike in the earlier KS-CHAIN regularisation. However,
numerical experiments (Mikkola & Tanikawa 1999a,b) have
shown that the chained coordinate system is very useful in
increasing the numerical accuracy of the method by signiﬁ-
cantly reducing the numerical ﬂoating point error.

When constructing the chained coordinates one ﬁrst
ﬁnds the shortest inter-particle coordinate vector of the N-
body system. These two particles become the initial tail and
head of the chain. Next, the particle closest to either the tail
or the head of the chain is found among the non-chained par-
ticles. This particle is added as the new tail or head of the
chain, depending which is end of the chain is closer. The
process is repeated until all particles are in the chain.

Labelling the particles starting from the tail of the

4

A. Rantala et al.

chain, the inter-particle position, velocity and various ac-
celeration vectors become
X k = r jk − r ik ≡ r k+1 − r k
V k = vjk − vik ≡ vk+1 − vk
Ak = ajk − aik ≡ ak+1 − ak
Fk = f jk
≡ f k+1 − f k
− f ik
Gk = gjk
≡ gk+1 − gk
− gik
in which the last expression on the right-hand side describes
the relabelling of the particle indexes along the chain. Note
that there are Npart − 1 inter-particle vectors for a system of
Npart bodies. The equations of motion for the chained coor-
dinates then become

(6)

dt
ds
dX i
ds

=

=

1
T + B
1
T + B

V i

while the velocity equations can be expressed as

dV i
ds
dB
ds

= 1
U

(Ai + Fi + Gi)

= −

1
U

(cid:213)

i

mivi · ( f i

+ gi).

(7)

(8)

It is worthwhile to note that the derivative of the binding
energy B is in fact easier to evaluate by using the original co-
ordinate system than the chained one. For this the chained
velocities need to be transformed back into the original co-
ordinate system during the integration.

Finally, the chained coordinate vectors are needed to
evaluate the accelerations ai and gi(v). We use the chained
coordinates for computing the separation vectors for Nd clos-
est particles in the chain structure while the original vectors
are used for more distant particles, i.e.

r j − r i =

r j − r i
max{i, j }−1
(cid:213)

sign(i − j)X k

if |i − j | > Nd

if |i − j | ≤ Nd.

k=min{i, j }





leapfrog fulﬁl this requirement. In this study we use only
leapfrog-type integrators with the GBS algorithm.

In general, when numerically solving initial value prob-
lems for diﬀerential equations the numerical solution will
converge towards the exact solution when the step-size h of
the numerical method is decreased. The numerical accuracy
of integrators with an extrapolation method is based on this
fact. The key idea of the GBS extrapolation is to successively
integrate the diﬀerential equation over an interval H using
an increasing number of substeps n. The integrations are
carried out in small steps of length h = H/n using a suitable
numerical method, and the results are then extrapolated to
h → 0. Diﬀerent substep division sequences nk have been
studied in the literature to achieve converged extrapolation
results with a minimum computational cost (Press et al.
2007). Popular options include the original GBS sequence
(Bulirsch & Stoer 1966) deﬁned as

{nk} = {2, 4, 6, 8, 12, 16, 24, 32, 48, 64, 96 . . . }
i.e. nk = 2nk−2, k > 2 and the so-called Deuﬂhard sequence
(Deuﬂhard 1983) of even numbers

(10)

{nk} = {2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, . . . }
with nk = 2k. In our MSTAR code and throughout this paper
we use the Deuﬂhard sequence.

(11)

The chained leapfrog sequence with n ≥ 1 substeps can

be written as
(cid:18) h
n

(cid:18) h
2n

(cid:19) (cid:20)

D

K

(cid:19) (cid:21) n−1

(cid:19)

D

(cid:18) h
n

K

(cid:19)

(cid:18) h
n

D

(cid:19)

,

(cid:18) h
2n

(12)

where the drift D(h) operators advance the coordinates and
the kick operators K(h) the velocity-like variables by the
time-step h. The GBS algorithm starts by computing the
ﬁrst few substep divisions nk with the leapfrog after which
a rational function or a polynomial extrapolation of the in-
tegration results to h → 0 is attempted. The error control is
enabled with the convergence criterion

(9)

(cid:107)∆Sk (cid:107)

(cid:107)S(s) + h

n S(cid:48)(s)(cid:107)

≤ ηGBS,

(13)

Typically Nd = 2 in the literature (Mikkola & Merritt 2008).
In general, selecting values Nd > 2 for the separation pa-
rameter undermines the usefulness of the chain construct as
the ﬂoating-point error begins to accumulate when summing
many inter-particle vectors in Eq. (9).

2.3 GBS extrapolation method

Even though the chained LogH leapfrog with the time-
transformed equations of motion yields regular particle or-
bits for all non-pathological initial conditions, the numeri-
cal integration accuracy is usually too low for high-precision
applications. Thus, the chained leapfrog integrator must be
accompanied by an extrapolation method to reach a high
numerical accuracy (Mikkola & Tanikawa 1999b). A widely
used method is the Gragg–Bulirsch–Stoer (Gragg 1965; Bu-
lirsch & Stoer 1966) or GBS extrapolation algorithm. The
GBS extrapolation method can only be used with integra-
tors that have an error scaling containing only even pow-
ers of the time-step, but fortunately many simple low-order
integrators such as the mid-point method and the chained

where S is any dynamical variable of the system, ∆Sk is the
extrapolation error estimate after the k’th substep sequence
and S(s) and S(cid:48)(s) are the value of the dynamical variable
and its time derivative obtained after the last complete time-
step H. The GBS tolerance ηGBS is a user-given free input
parameter. The extrapolation can typically be carried out
successfully even if ηGBS is set near the double-precision
ﬂoating-point precision ηGBS ∼ 10−16. In the literature, the
most typical values of the accuracy parameter are set in the
range of 10−12 ≤ ηGBS ≤ 10−6

If convergence is not reached after the ﬁrst few leapfrog
step divisions, the GBS algorithm proceeds to the next sub-
step division and tries the extrapolation again until conver-
gence, or until the maximum number of divisions nmax =
nkmax , is reached. In the case of no convergence after the
kmax’th substep sequence, the original step H is halved and
the process is started again from the beginning. After con-
vergence, the criterion for the next time-step Hi+1 after con-
vergence is (Press et al. 2007)

Hi+1 = aGBS

(cid:18) ηGBS
(cid:15)k

(cid:19)1/(2k−1)

Hi,

(14)

MNRAS 000, 1–20 (2019)

where (cid:15)i is the maximum error in the dependent variables
from the previous step, aGBS ∈ (0, 1] is a safety factor (Hairer
et al. 2008), and k is the substep sequence at which con-
vergence was achieved. The GBS algorithm also monitors
whether trying convergence at diﬀerent k, or equivalently,
changing the order 2k − 1 of the method, would lead to con-
vergence with a smaller work-load. The time-step H is then
adjusted accordingly, to bring the k where convergence is
expected to the optimal value (Press et al. 2007).

Finally, it should be noted that the extrapolation al-
gorithm used in AR-CHAIN described above is serial in
nature, even though the particle accelerations in the indi-
vidual leapfrog sequences can be computed in parallel as
in (Rantala et al. 2017). Thus, computing a large number
of subsequent subdivision counts or reaching the maximum
subdivision count kmax without convergence and restarting
with a smaller time-step H/2 can be very time-consuming.

2.4 Iteration to exact physical time

We next describe an improvement of the time iteration pro-
cedure in the new MSTAR integrator over our older AR-
CHAIN version. Consider the integration of the equations of
motion of an N-body system over a time interval ∆t. With
the standard leapfrog there is no problem in arriving at the
correct end time, but with the time transformed leapfrog
one has to be more careful (e.g. Mikkola 1997).

Integrating the time transformation of Eq. (3) over a
time interval H = ∆t with the parameter triplet (1, 0, 0) yields

∆s =

∫ ∆t

0

Udt = G

(cid:213)

(cid:213)

i

j>i

mimj

∫ ∆t

0

dt
(cid:107)r j − r i (cid:107)

.

(15)

One can in principle approach these Npart(Npart−1)/2 integrals
in the formula by using the Stumpﬀ-Weiss method which
assumes that all the particle motions during the time interval
are Keplerian (Stumpﬀ & Weiss 1968). However, a simple
approximation

∆s = U∆t

(16)

typically provides suﬃciently accurate results, especially
when the time interval ∆t is short.

In our AR-CHAIN implementation (Rantala et al. 2017)
we begin the regularised integration by estimating the
amount of ﬁctitious time ∆s based on the smallest Keple-
rian time-scale PKepler of the system. Using Eq. (16) we have
∆s = qUPKepler in which q is a safety factor 0 < q ≤ 1. Af-
ter the ﬁrst step we let the GBS algorithm decide the step
size until we exceed the output time ∆t. Then, we take ad-
ditional GBS iteration steps towards the correct exact time
until the relative diﬀerence between the integrated time and
∆t is small enough, typically ∼ 10−4–10−6. This requires 2 to
5 iteration steps in most cases.

We improve the time iteration procedure for our new
integrator as the GBS steps are expensive and one should
try to converge to the correct physical time with as few it-
erations as possible. For the ﬁrst step we already use Eq.
(16) multiplied by a safety factor instead of the Keplerian
criterion. Next we assume that after a GBS step has been
taken we have arrived at the physical time 0 < τ < ∆t. As
the GBS routine suggests a new time step ∆s we check that

MNRAS 000, 1–20 (2019)

A fast regularised integrator

5

the estimated time coordinate after the next step τ + ∆s/U
does not exceed ∆t. If it does, we set the next ﬁctitious time
step to ∆s = U(∆t − τ). If we still end up to time τ > ∆t
after integration we proceed as in the old method. This pro-
cedure typically requires a few expensive iteration steps less
than the approach we used in our previous AR-CHAIN im-
plementation. The speed-up gained by the updated iteration
procedure depends on ∆t and the N-body system, and the
maximum expected speed-up occurs in the integration of
very short time intervals.

3 MINIMUM SPANNING TREE

COORDINATES

3.1 Shortest Hamiltonian paths and minimum

spanning trees

The properties of chained inter-particle coordinate systems
can be conveniently expressed by using the language of graph
theory (e.g. Harary 1969). A graph G = (V, E) is an ordered
pair of vertices V and edges E. An edge is determined by
the vertices it connects, i.e. Ei j = (Vi, Vj). For our purposes
the N-body particles are the vertices of the graph while the
inter-particle vectors correspond to the edges of the graph.
A graph with N vertices and all possible N(N − 1)/2 edges is
called complete. Complete graphs are also necessarily con-
nected as any vertex can be reached from any other vertex
of the graph. Each edge E is weighted with a non-negative
real number w. We set the weights of the edges by calculat-
ing the Euclidean norm i.e. the length of the inter-particle
vector corresponding to each edge.

In graph theory a path is a sequence of distinct ver-
tices. A Hamiltonian path is a path which visits each vertex
of the graph exactly once. In a complete graph Hamiltonian
paths are guaranteed to exist. We note here that the chain
structures of AR-CHAIN are in fact Hamiltonian paths. The
problem of ﬁnding whether a Hamiltonian path exists in
a given graph is NP-complete i.e. in the practical point of
view meaning no solution in polynomial time O(N k ) with
k > 0 exists. Furthermore, it can be shown that there is
no polynomial time algorithm to ﬁnd the shortest Hamilto-
nian path of a complete graph (i.e. shortest chain) either.
The computational upper limit of a brute force approach
to constructing the shortest chain scales as O(N!). Thus,
strictly speaking our procedure in ﬁnding the chain of inter-
particle vectors in Section 2.2 corresponds to ﬁnding only
the approximately shortest Hamiltonian path of the system.
Consequently, there are usually more optimal chained co-
ordinate systems than the one we ﬁnd with our chain con-
struction algorithm but going through all the possible chain
conﬁgurations is not feasible.

A spanning tree T = (V, ET) of the graph G is a subgraph
of G connecting all the vertices of the original graph with a
minimum possible number of its edges. A spanning tree con-
necting N vertices has N −1 edges, the same as the number of
inter-particle vectors in the chain structure of AR-CHAIN.
In addition, T is a minimum spanning tree (MST) of G if the
sum of the edge weights w in T is the smallest among all the
possible spanning trees of G. It turns out that if the graph G
has unique edge weights w there is a unique minimum span-
ning tree T. This is usually the case in practical applications

6

A. Rantala et al.

of minimum spanning trees, such as our N-body systems.
Unlike for the shortest Hamiltonian path problem, there are
algorithms which ﬁnd the MST for a given complete graph
in a polynomial time.

There are two important graph properties which aid in
ﬁnding the MST or in ﬁltering out edges which are certainly
not in the MST of a graph. The ﬁrst is the cycle property. A
cycle C of graph G is a path which forms a loop. The cycle
property states that the edge Ei with the highest weight in
any cycle C in G cannot be in the MST of G. The second
property is the cut property. Divide the vertices Vi of G arbi-
trarily into two distinct groups. The essence of cut property
is that the edge Eij with the minimum weight connecting the
two vertex groups is necessarily in the MST of G.

3.2 Finding the MST: Prim’s algorithm

The problem of eﬃciently ﬁnding the minimum spanning
tree of a given graph has been extensively studied in the
literature, the classic solutions to the problem being found
by Bor˚uvka (1926), Kruskal (1956) and Prim (1957). We se-
lect the classic Prim’s algorithm due to its relative simplic-
ity and the fact that the algorithm somewhat resembles the
chain construction in our AR-CHAIN implementation with
the diﬀerence that the chain is now allowed to branch. In
addition, Prim’s algorithm makes labelling the edge vectors
easier for our purposes than the other two classic algorithms.
Prim’s algorithm proceeds as follows. First, one selects
a single vertex Vi of the graph G. For N-body systems we
suggest starting from the particle spatially closest to the
centre-of-mass of the system. Next, the edge Eij with a min-
imum weight w connected to the ﬁrst vertex is found and
added as the ﬁrst edge to the MST. Then the algorithm
proceeds by ﬁnding the successive minimum weight edges
among the edges in G connected to the MST and adds them
into the MST until all vertices are connected with the MST.
Our parallel implementation uses sorted adjacency lists on
diﬀerent tasks to eﬀectively ﬁnd the consecutive edges to
add to the MST. For complete graphs even the most sophis-
ticated MST ﬁnding algorithms typically scale as O(N2) as
the Prim’s algorithm does.

The crucial diﬀerence between the chain construction
and Prim’s algorithm manifests itself here. In the chain con-
struction it is allowed to add new inter-particle vectors only
to the tail and the head of the chain while in Prim’s algo-
rithm it is allowed to add new edges to any location in the
MST. This ensures that spatially close N-body particles are
always located near each other in the MST data structure,
which is necessarily not the case in the chain. The diﬀer-
ences between a chain and a MST built on a same group of
particles is illustrated in Fig. 1.

3.3 Finding the MST: divide-and-conquer method

It is possible to quickly ﬁnd spanning trees T with a total
weight very close to the total weight of the MST of the graph
(e.g Wang et al. 2009; Zhong et al. 2013). For simplicity we
also refer to these approximate minimum spanning trees as
MSTs throughout this study.

Our preferred method is the divide-and-conquer ap-
proach to the Prim’s algorithm. First one divides the original

Figure 1. A two-dimensional illustration highlighting the dif-
ference between a chained coordinate system and the minimum
spanning tree coordinates. Both the chain and the MST are con-
structed on a collection of 28 points corresponding to the locations
of the brightest stars in the familiar constellation of Orion. The
total length of the MST edges is smaller than the length of the
chain. The chain also occasionally suﬀers from the fact that spa-
tially close particles might be distant in the chain, which the MST
avoids by branching its tree structure.

√

N subgraphs G(cid:48). We use a simple octree-
graph G into ∼
based spatial partition to ﬁnd these subgraphs. A so-called
meta-graph G(cid:48)(cid:48) is formed as the contracted subgraphs as its
vertices and including all possible edges between the ver-
tices. The edge weights of the meta-graph are the minimum
edge weights between the subgraphs by the cut property.
Next we use Prim’s algorithm to construct the minimum
spanning trees of each G(cid:48) and the meta-graph G(cid:48)(cid:48). To speed
up the local MST construction we eliminate edges which
cannot be in the ﬁnal MST using the cycle property before
applying Prim’s algorithm. Now we have all the edges of the
total MST which are then ordered and labelled by perform-
ing a standard tree walk.

We ﬁnd that the spanning trees T found using the ap-
proximate algorithm are typically 5% longer than the true
MST of the graph. However, this diﬀerence is not a serious
problem as the spanning trees are locally true MSTs of the
subgraphs. Furthermore, our motivation to use MSTs is to
minimise numerical error originating mostly from computa-
tion operations involving spatially close particles by choos-
ing a clever local coordinate system. In addition, the divide-
and-conquer approach is faster than the original Prim’s al-
gorithm. Concerning the speed of the algorithms we ﬁnd
that in our code it is always proﬁtable to use the divide-
and-conquer method when the particle number is N (cid:38) 103.
With smaller particle numbers both approaches yield very
similar results as the wall-clock time elapsed in the MST
construction is negligible.

3.4 MST as a coordinate system

As both the chain and the MST consist of Npart − 1 inter-
particle vectors, the MST coordinate system can be con-

MNRAS 000, 1–20 (2019)

chainMSTA fast regularised integrator

7

state that if
|L(Vi) − L(VLCA)| + |L(Vj) − L(VLCA)| ≤ Nd
the two vertices Vi and Vj are close to each other in the MST.
Here L(Vi) again signiﬁes the level of the vertex Vi.

(17)

Finally, we write down the recipe for selecting which
vectors to use in the force calculation. If Vj and Vi are within
Nd edges of each other, we simply walk the MST from Vi to
Vj via VLCA and sum the traversed edge vectors to obtain the
MST separation vector X k, just as in Eq. (9) with the chain.
If the condition of Eq. (17) does not hold, we use the original
coordinate vectors to compute the separations r j − r i.

3.5 Numerical error from consecutive coordinate

transformations

During integration the MST (or chain) coordinate structure
is frequently built, deconstructed and rebuilt to keep the
coordinate structure up-to-date as the simulation particles
move. For N-body systems with a large number of particles
these coordinate transformations require a large number of
summation operations (e.g. Mikkola & Aarseth 1993), which
introduces a new source of numerical error in the integration.
This fact has received little attention in the literature thus
far, most probably due to the fact that for few-body systems
(Npart (cid:46) 10) the accumulated summation errors always re-
main small. The chain construction and deconstruction both
require summing on average (cid:104)Lchain(cid:105) = (Npart−1)/2 coordinate
vectors. For MST the corresponding number is the mean
level of the vertices in the MST, i.e. (cid:104)LMST(cid:105) = (cid:104)L(Vi)(cid:105). Now
the choice of the root vertex in Section 3.1 becomes impor-
tant. If the root vertex is spatially close to the centre-of-mass
of the system, (cid:104)LMST(cid:105) should always be smaller than (cid:104)Lchain(cid:105).
We demonstrate this fact in Fig. 3. Our results show that
(cid:104)Lchain(cid:105)/(cid:104)LMST(cid:105) ∼ 2–10 with particle numbers of Npart = 10–
1000.

This diﬀerence somewhat aﬀects the numerical perfor-
mance of the two algorithms. We perform an additional nu-
merical test in which we build, deconstruct and rebuild the
two coordinate systems consecutively Nrebuild = 105 times
while monitoring the accumulating numerical error. The N-
body system on which the coordinate systems are built con-
tains Npart = 379 particles with its full details presented in
Section 6.1. The results of the rebuild test are presented in
Fig. 4. The results indeed show that the MST coordinate
system accumulates less numerical error than the chained
coordinates, the diﬀerence being approximately an order of
magnitude in energy error. Apart from the diﬀerence of an
order of magnitude the cumulative error behaves very simi-
larly with the two coordinate systems. As the energy error
in the chain coordinate test reaches |∆E/E | ∼ 10−13 after
Nrebuild = 105 rebuilds we conclude that the MST coordinate
system is recommended for regularised N-body simulations
requiring extremely high numerical accuracy.

Finally, we note that advanced ﬂoating-point summa-
tion methods such as the Kahan summation (Kahan 1965) or
the Neumaier summation (Neumaier 1974) algorithm might
be used to further diminish the numerical error from the
coordinate transformation operations. However, the inclu-
sion of such more advanced summation algorithms is left
for future work, as our current MSTAR code is numerically
accurate enough for all current target applications.

Figure 2. A tree structure with 12 vertices, 11 edges and 5 levels.
The parent vertex of vertices V2 and V3 is the vertex V1. In ad-
dition, the vertex V1 is the 2nd ancestor of vertex V4. The lowest
common ancestor of vertices V9 and V12 (both in green) is the
vertex V7 (in orange).

structed with a recipe similar to the chained coordinates in
Eq. (6) with relatively small modiﬁcations to the procedure.
First, we need to index the MST edge vectors Eij as in
Eq. (6). In the chain construct setting the chained indices
is straightforward as one simply starts the indexing from
the tail of the chain and proceeds towards the head. In the
MST indexing is more complicated because of the branching
of the MST structure. However, we can take full advantage
of the fact that the MST is a tree structure. The ﬁrst chosen
vertex V0 is the root vertex of the MST and gets the index
and level zero in the MST i.e. L(V0) = 0. We index the subse-
quent vertices and edge vectors in the order they were added
to the MST. The vertex levels and the parents of the vertices
are assigned simultaneously as well. A simpliﬁed illustration
describing the indexing of our MST structure is presented
in Fig. 2. After the indexing the inter-particle vectors cor-
responding to the MST edges can be computed just as in
Eq. (6) with the additional rule that all the inter-particle
vectors point away from the root vertex, just as the chained
vectors are oriented away from the tail towards the head of
the chain.

Next, we generalise Eq. (9) to determine the rules when
two vertices are close to each other in the MST, i.e. within
Nd edges of each other, just as in the chain construct. For the
force calculation of the nearby vertices the MST edge vec-
tors are used while the original coordinate system is used
for the rest of the vertex pairs. The criterion of two vertices
Vi and Vj being within Nd MST edges of each other can be
conveniently expressed by using the lowest common ances-
tor (LCA) of the two vertices. The parent vertex is the 1st
ancestor of the vertex, the 2nd ancestor is the parent of the
parent vertex, and so on. The LCA is the vertex among the
common ancestors of both Vi and Vj which has the highest
level in the MST. We label this vertex VLCA. Note that Vi
or Vj itself may be the VLCA of the vertex pair. Now we can

MNRAS 000, 1–20 (2019)

012356789101112level 04level 1level 2level 3level 4012345678910118

A. Rantala et al.

Figure 3. Comparing the mean length (cid:104)Lchain (cid:105) of the a chain
structure (solid blue line) and the mean level (cid:104)LMST (cid:105) of particles
in a MST (red symbols). The particle distribution on which the
coordinate systems are build follows a ρ(r) ∝ r −1 density proﬁle.
We see that the mean level of particles in the inter-particle coor-
dinate structure is always lower in the MST by a factor of ∼ 2–10.
Changing the underlying density proﬁle has a very small eﬀect on
the result. The small mean particle level corresponds to a smaller
level of ﬂoating-point error when constructing and deconstructing
the coordinate systems as discussed in the text.

4 PARALLEL EXTRAPOLATION METHOD

4.1 Force loop parallelisation

The most straightforward way to begin to parallelise a se-
rial algorithmically regularised integrator is to parallelise
the force computation of the code. The MPI parallelisation
standard is adopted for this study. We use the basic par-
allelisation strategy in which the O(N2
part) iterations of the
force computing loop are divided evenly for Nforce MPI tasks,
speeding up the loop calculation. However, the inter-task
communication required to collect the ﬁnal results after the
force computations uses an increasing amount of CPU time
when the number of MPI tasks is increased.

In a serial direct summation integrator using a GBS
extrapolation method the total wall-clock time T elapsed
for the force calculation during the integration of a single
step H can be expressed as
kmax(cid:213)

kmax(cid:213)

T ≈

nktN2

part

= tN2

part

nk,

(18)

k=1

k=1

where kmax is the maximum number of GBS substep divi-
sions and t is a proportionality constant. Without the GBS
algorithm the sum expression would not appear in the for-
mula. Thus, the wall-clock time elapsed in the force calcula-
tion depends not only on the particle number Npart but also
on the sequence nk introduced in Eq. (10) and Eq. (11).

We deﬁne the speed-up factor of the parallelised force

computation as

Sforce(Nforce) =

Tserial
Tparallel(Nforce)

(19)

in which Tserial and Tparallel are the respective wall-clock times

Figure 4. The build, deconstruct and rebuild test of the two
coordinate systems on a simple N-body system with Npart = 379
particles. With the chained coordinates (blue line) the energy
error is approximately an order of magnitude larger than with the
MST coordinates (red line). The results are averages over 10 N-
body systems with diﬀering random seeds. The coloured regions
represent the scatter of one standard deviation.

elapsed during the force computation. Assuming ideal zero-
latency communication between the tasks, the speed-up fac-
tor Sforce of the parallelised force calculation scales initially
linearly with the number of MPI tasks, i.e. Sforce ∝ Nforce.
The linear scaling behaviour continues until the number of
tasks equals the particle number Nforce = Npart at which
point one MPI task handles one particle. After this the
speed-up factor Sforce remains constant. With realistic non-
instantaneous inter-task communication the ﬂat scaling be-
haviour is reached with Nforce well below the particle number
Npart due to the increasing communication costs.

We illustrate the results of a series of force computa-
tion scaling tests in Fig. 5. The N-body systems in the test
are selected from our sample of initial conditions with log-
arithmically spaced particle numbers in the range 101 ≤
Npart ≤ 104. The results are averages over three diﬀerent
random realisations of the systems. In the scaling test we
use 1 ≤ Nforce ≤ 400 MPI tasks. With a small number of
MPI tasks (Nforce (cid:46) 10) the speed-up factor increased lin-
early, as expected. After the linear phase Sforce(Nforce) ﬂat-
tens to a constant function at higher Nforce. Eventually the
speed-up factor actually begins to decrease as the communi-
cation costs start to dominate over the time elapsed in the
force computation. We deﬁne the maximum reasonable task
number Nmax
for N-body systems as the task number in
force
which ∼ 95% of the maximum speed-up has been achieved,
i.e. Sforce(Nmax
the ad-
force
dition of subsequent MPI tasks has only a negligible eﬀect
on the speed-up of the force computation. We ﬁnd in our
scaling experiments that Nmax
can be approximated with a
force
simple relation

. When Nforce (cid:38) Nmax
force

) = 0.95 × Smax
force

Nmax
force ≈ q × Npart
in which the constant factor is between 0.05 ≤ q ≤ 0.1. In
addition, the maximum force computation speed-up factor

(20)

MNRAS 000, 1–20 (2019)

101102103Npart100101102103L [inter-particle vectors]chainMST103104105Nrebuild10-1610-1510-1410-1310-12|E/E|chainMSTA fast regularised integrator

9

4.2 Substep division parallelisation

Solving initial value problems numerically for individual or-
dinary diﬀerential equations was long considered to be an in-
herently sequential process. However, numerical techniques
employing extrapolation methods are an important excep-
tion to this rule (e.g. Rauber & R¨unger 1997; Korch et al.
2011 and references therein). As the N-body problem is an
initial value problem for a coupled system of ordinary dif-
ferential equations, it is possible to introduce another layer
of parallelisation besides the parallel force loop computa-
tion into N-body codes which use extrapolation methods.
To our best knowledge the only work studying orbital dy-
namics with a code including a parallelised extrapolation
method is the study by Ito & Fukushima (1997). Unfortu-
nately, this pioneering study has not received wide attention
in the literature.

For the MSTAR code implementation we use the
Neville-Aitken algorithm (e.g. Press et al. 2007) for poly-
nomial extrapolation. Error control is implemented as in
Eq. (13) by studying the relative diﬀerence of consecutive
extrapolation results. We chose polynomials over rational
functions for extrapolation as we have observed that using
rational functions may occasionally lead to spurious extrap-
olation results even though the convergence criteria of Eq.
(13) are fulﬁlled. We do not use the sophisticated recipes
intended to optimise the amount of computational work per
unit step in serial extrapolation methods, present in some
GBS implementations (Press et al. 2007; Hairer et al. 2008).
Implementing another parallelisation layer into an al-
gorithmically regularised integrator begins with the obser-
vation that the computation of a single substep division in
the GBS method is independent of the other required sub-
step divisions. Thus, the diﬀerent substep divisions can be
integrated using diﬀerent CPU groups in parallel (Rauber
& R¨unger 1997), after which the results of subdivisions are
communicated and the actual extrapolation is performed.
The Neville-Aitken polynomial extrapolation for all the dy-
namical variables can be parallelised as well. In this work we
focus on the case of a single N-body system, but the method
described here can be extended to integrate multiple inde-
pendent N-body systems simultaneously. A simple example
with two CPU groups and six substep divisions is illustrated
in Fig. 6. We label the number of CPU groups Ndiv. As we
use a single MPI task per CPU, the total number of CPUs,
NCPU, the number of CPU groups Ndiv, and the number of
CPUs in force computation Nforce, are connected by the sim-
ple relation

NCPU = Ndiv × Nforce.

(23)

As stated in Section 2.3, the standard GBS method
computes the substep divisions in sequential manner, calcu-
lating subsequent substep divisions until the results converge
or the user-given maximum number of substep divisions kmax
is reached. The parallelisation of the substep divisions re-
quires that the number of substep divisions to be computed
must be set by user in advance in our implementation. We
call this ﬁxed number of substep divisions kﬁx. We note that
techniques to use a non-ﬁxed kmax exist even with paral-
lelisation (Ito & Fukushima 1997) but the simple approach
with a ﬁxed number of subdivisions has proven to be suﬃ-
cient for our purposes. The optimal value for kﬁx depends

Figure 5. The speed-up factor of the force computation Sforce as
a function of the number of MPI tasks Nforce for four diﬀerent N-
body particle numbers Npart. The symbols represent the measured
force computation speed-up factors while the solid lines are error
function ﬁts (Eq. 22) to the speed-up data as described in the text.
The speed-up factor initially grows linearly when Nforce (cid:28) Npart
and saturates to a roughly constant level after Nforce (cid:38) 0.1 × Npart.

Npart

264
546
1129
2336

b1

3.60
3.24
5.03
6.50

c1

5.10
5.33
9.08
12.35

b2

×
3.16
4.02
6.09

c2

×
4.97
31.87
79.49

Table 1. The force speed-up coeﬃcients bi and ci obtained by
ﬁtting the data from Fig. 5 using the function Eq. (22). For the
smallest particle number Npart = 264 ﬁts beyond a single term
were not proﬁtable.

Smax
force
log10(Smax

can be approximated with the formula

force) ≈ a1 log10(Npart) − a2.

(21)

with a1 ≈ 0.505 and a2 ≈ −0.58. For quantifying the be-
haviour of the speed-up factor Sforce in the intermediate
range of MPI tasks between the linearly increasing and the
constant speed-up factor a suitable ﬁtting function is re-
quired. A suitable choice is the error function erf which has
the correct asymptotic behaviour both with small and large
values of its argument. We use ﬁtting functions of the form

Sforce(Nforce) =

Ncoeﬀ(cid:213)

i

bi erf (ciNforce)

(22)

bi ≈ Smax
force

in which bi and ci are constant coeﬃcients. As expected,
(cid:205)Ncoeﬀ
by deﬁnition. We ﬁnd that using Ncoeﬀ ≈ 1–
i
2 terms yields good results. The ﬁt coeﬃcients are presented
in Table 1 and are later used in Section 4.3 to estimate the
optimal division of computational resources when additional
layers of parallelisation are implemented into the extrapola-
tion method of the integrator.

MNRAS 000, 1–20 (2019)

0100200300400Nforce [MPI tasks]020406080100Speed-up factor SforceForce onlyNpart=264Npart=1129Npart=546Npart=233610

A. Rantala et al.

Figure 7. The speed-up factor Sdiv for the parallelised substep di-
visions (symbols) and their interpolated continuous counterparts
(solid lines) as a function of the number of substep divisions kﬁx.
The interpolants are only shown to clarify the visualisation as
there are no groups with non-integer kmax. We see that Sdiv = Ndiv
until Ndiv = kﬁx/2 after which the speed-up factor attains a con-
stant value of Smax
div

= (kﬁx + 1)/2.

the code. Keeping Nforce ﬁxed we deﬁne the parallel substep
division speed-up Sdiv as

Sdiv(Ndiv) =

Tserial
Tparallel(Ndiv)

=

(cid:205)kﬁx

k=1 nk
maxi Ci

=

maxi

(cid:205)kﬁx
k=1 nk
(cid:16) (cid:104)(cid:205)Ni

j nk j

(cid:105)

(cid:17)

i

(24)

If Ndiv = 1 there is no speed-up in the force computation an
the running time is the same as in Eq. (18). When Ndiv > 1
there is a wall-clock time speed-up as the force computation
is divided into multiple CPU groups. With Ndiv = kﬁx we
have Sdiv = (kﬁx + 1)/2 assuming the Deuﬂhard sequence
from Eq. (11).

Now we can compute the speed-up factor Sdiv once kﬁx
and Ndiv are set. The computed results are presented in
Fig. 7. The speed-up factor follows the line Sdiv = Ndiv until
the point Ndiv = kﬁx/2 is reached, after which the Sdiv(Ndiv)
rapidly ﬂattens into the constant value of Sdiv = (kﬁx + 1)/2.
Thus, the maximum reasonable number of CPU groups is
= (cid:100)kﬁx/2(cid:101) in which we use the ceiling function (cid:100)·(cid:101).
Nmax
div

4.3 Speed-up with full parallelisation

Now we are ready to combine the force loop and the sub-
step division layers of parallelisation. The primary advan-
tage of using two layers of parallelisation compared to the
simple force loop computation parallelisation is that we can
eﬃciently use more MPI tasks to speed up the MSTAR in-
tegrator. Without the subdivision parallelisation it is not
reasonable to use more than Nforce ≈ 0.1 × Npart MPI tasks
as shown in Section 4.1. With the subdivision parallelisation
included, the maximum reasonable CPU (or task) number

MNRAS 000, 1–20 (2019)

Figure 6. The parallelisation strategy of the GBS extrapolation
method without substep parallelisation as in our previous AR-
CHAIN implementation (left) and including it in our new MSTAR
code (right). In this example the extrapolation method converges
after six substep divisions. The previous extrapolation method
computes the diﬀerent substep divisions, tries extrapolation and
check convergence in a sequential manner. The parallelised ex-
trapolation method computes the diﬀerent substep divisions in
parallel using diﬀerent CPU groups, communicates the results to
each node and then performs the extrapolation procedure.

on the GBS tolerance parameter ηGBS and the particular
N-body system in question. Thus, numerical experimenta-
tion is needed for determining kﬁx. If kﬁx is set to too low
a value, the extrapolation method must shorten the time-
step H in order to reach convergence, increasing the running
time of the code. On the other hand, if kﬁx is too high, extra
computational work is performed as the extrapolation would
have converged with fewer substep divisions. However, this
slow-down is somewhat compensated by the fact that longer
time-steps H can be taken with a higher kﬁx.

The number of CPU groups Ndiv can have values be-
tween 1 ≤ Ndiv ≤ kﬁx leaving Nforce = NCPU/Ndiv for paral-
lelisation of the force computation. The individual substep
divisions are divided into the Ndiv CPU groups with the
following recipe. Initially, each CPU group has the compu-
tational load Ci = 0. Starting from the substep division with
the highest number of substeps, i.e. max(nk) = 2kﬁx, we as-
sign the substep divisions one by one into the CPU group
which has the lowest computational load Ci at that moment
until no divisions remain. If there are several CPU groups
with the same computational load we select the ﬁrst one,
i.e. the CPU group with the lowest CPU group index in

   compute substepdivision 1compute substepdivision 2compute substepdivision 3compute substepdivision 4compute substepdivision 5compute substepdivision 6extrapolatecheck convergenceextrapolatecheck convergenceextrapolatecheck convergenceextrapolatecheck convergenceextrapolatecheck convergencewallclock time CPU group 1CPU group 2communicateresultsextrapolatecheck convergenceThis workcompute substepdivision 1compute substepdivision 2compute substepdivision 3compute substepdivision 6compute substepdivision 5compute substepdivision 4Rantala+17012345678910111213Ndiv012345678Speed-up factor Sdivkfix = 4Ndivkfix = 6Ndivkfix = 8Ndivkfix = 10Ndivkfix = 12Nforce fixedNdivA fast regularised integrator

11

Figure 8. An example of ﬁnding the optimal division of computational resources between the force and substep parallelisation. The
substep part of the speed-up factor can be computed analytically while the force parallelisation part is estimated by using simple
numerical tests as explained in the text. Starting from the top-left corner, the four panels have increasing CPU numbers of NCPU = 100,
NCPU = 250, NCPU = 750 and NCPU = 1000. Each panel shows the speed-up factor Stotal as a function of the number of CPU groups Ndiv
for four diﬀerent particle numbers of Npart = 264 (blue line), Npart = 546 (yellow), Npart = 1129 (green line) and Npart = 2336 (red line). The
total is typically found near Ndiv = kﬁx/2. The corresponding number of CPUs for the force computation is
maximum speed-up factor Smax
obtained by using the relation Nforce = NCPU/Ndiv.

becomes NCPU ≈ 0.05 × kﬁxNpart = 0.4 × Npart with the typi-
cal value of kﬁx = 8. This is the value of kﬁx we use in the
simulations of this study.

Next we estimate how the computational resources
should be divided to ensure the maximum total speed-up
factor Stotal if the number of CPUs, NCPU, and thus MPI
tasks, is ﬁxed. The values of Npart and kﬁx are assumed to
be ﬁxed as well. The optimal division of computational re-
sources corresponds to ﬁnding the maximum of the function

Stotal(Nforce, Ndiv) = Sforce(Nforce) × Sdiv(Ndiv)
= Sforce(NCPU/Ndiv) × Sdiv(Ndiv)
with the constraints Nforce, Ndiv ∈ N and NCPU = Nforce × Ndiv.
For arbitrary NCPU there are typically only a few solutions.
For the force computation speed-up Sforce we need to use the
approximate methods i.e. the ﬁtting function Eq. (22) and

(25)

MNRAS 000, 1–20 (2019)

its coeﬃcients bi and ci from Table 1. The substep division
speed-up factor Sdiv can be exactly estimated by using Eq.
(24).

In Fig. 8 we present the speed-up factor Stotal for four
diﬀerent particle numbers and four diﬀerent values for NCPU.
We set kmax = 8 for each of the 16 combinations of the par-
ticle number and the number of CPUs. For a ﬁxed parti-
cle number the total speed-up factor Stotal increases until
NCPU ∼ 0.4 × Npart. We ﬁnd that the maximum of Stotal is
typically located near (cid:100)Ndiv/2(cid:101), which corresponds to ﬁnd-
ing the optimal Nforce around NCPU/(2Ndiv).

However, we ﬁnd that the best strategy for ﬁnding
the optimal pair (Nforce, Ndiv) is to relax the requirement
of having a pre-set value for NCPU. One computes the val-
ues for Stotal for all the integer pairs (Nforce, Ndiv) satisfying
1 ≤ Nforce ≤ (cid:100)0.1× Npart(cid:101) and 1 ≤ Ndiv ≤ (cid:100)kﬁx/2(cid:101). The location

NCPU = 25012345678Ndiv=NCPU/Nforce101102103Speed-up factor StotalNCPU = 75012345678Ndiv=NCPU/NforceNCPU = 2000101102103Speed-up factor StotalNCPU = 100Full parallelizationkfix = 8Npart=264Npart=546Npart=1129Npart=233612

A. Rantala et al.

FREYA1 cluster of the Max Planck Computing and Data
Facility (MPCDF). Each computation node of FREYA con-
tains two Intel Xeon Gold 6138 CPUs totalling 40 cores per
computation node. However, in the context of this article we
refer to these core units as CPUs.

The Keplerian two-body problem is completely de-
scribed by its six integrals of motion. As the ﬁnal integral,
the periapsis time, can be arbitrarily chosen we only need
ﬁve integrals of motion to describe the orbit. The ﬁrst con-
served quantity is the energy E of the two-body system de-
ﬁned as
E = 1
2

GµM
(cid:107) r (cid:107)

µ(cid:107)v(cid:107)2 −

(26)

in which M = m1 + m2, µ = m1m2/M and r and v are the rel-
ative position and velocity vectors, respectively. The energy
of the two-body system uniquely deﬁnes its semi-major axis
a as

a = −

GµM
2E

(27)

Next, the conserved angular momentum vector L deﬁned as

L = µr × v.

(28)

Together E and L determine the orbital eccentricity e of the
two-body system as
1 + 2E L2
Gµ3 M2

(cid:19)1/2

(29)

e =

(cid:18)

.

Finally, we have the constant Laplace–Runge–Lenz vector

A = µv × L − GM ˆr

(30)

in which ˆr = r/(cid:107)r (cid:107). The Laplace–Runge–Lenz vector lies
in the orbital plane of the two-body system pointing to-
wards the periapsis. As we have now in total seven conserved
quantities and only ﬁve integrals are required the conserved
quantities cannot be independent. The ﬁrst relation is sim-
ply A · L = 0 while the non-trivial second relation reads
e = (cid:107) A(cid:107)/(GM), connecting both the energy E and the norm
of the angular momentum vector L to the norm of A.

It is convenient to study the accuracy of a numerical
integrator by observing the behaviour of E, L and A during
a simulation run. Symplectic integrators such as our chained
leapfrog typically conserve quadratic invariants such as the
angular momentum exactly and the energy approximately
but with no long-term secular error growth (Hairer et al.
2006). However, the Laplace–Runge–Lenz vector is a third
order invariant, and its conservation is not guaranteed. Thus
the orbit can precess in its orbital plane (e.g. Springel et al.
2005). This makes the rotation angle of the Laplace–Runge–
Lenz vector
θLRL = arctan(Ay/Ax)

(31)

a very suitable probe for testing the accuracy of an integra-
tor.

We perform a series of two-body simulations both with
our MSTAR integrator and our AR-CHAIN implementa-
tion. For the tests in this Section, serial code implementa-
tions are used. We initialise 360 equal-mass SMBH binaries

Figure 9. The strong scaling test of the force loop parallelised
(solid blue line) and the fully parallelised (solid red line) force
computation algorithms. The serial running time per GBS step
corresponds to Eq. (18). The dashed red line shows the ideal scal-
ing behaviour of the codes. The fully parallelised algorithm follows
the ideal scaling behaviour up to NCPU ∼ 0.5 × Npart while the loop
parallelised algorithm begins to deviate from the ideal scaling law
already with roughly ten times smaller CPU numbers.

of the maximum value of Stotal determines which values of
Nforce and Ndiv, and thus also NCPU, should be used. Addi-
tional constraints such as the number of CPUs per super-
computer node should also be taken into account i.e. NCPU
should be a multiple of this number.

Finally we present the results of a strong scaling test of
our force calculation algorithms in Fig. 9. In a strong scal-
ing test the problem size remains ﬁxed while the number of
CPUs is increased. We examine both the force loop paral-
lelised version and the code with full parallelisation. We see
that the force calculation algorithm with full parallelisation
follows the ideal scaling behaviour to higher CPU numbers
than the force loop parallelised version. With the fully paral-
lelised force computation one can use CPU numbers approx-
imately up to NCPU ∼ 0.5 × Npart before the scaling begins
to deviate from the ideal case. Including only the force loop
parallelisation the scaling behaviour becomes non-ideal with
roughly ten times smaller NCPU. We tested the force algo-
rithms up to NCPU = 400 and all fully parallelised tests with
particle numbers Npart (cid:38) 103 followed ideal scaling. The total
speed-up factors Stotal are consistent with our estimations in
this Section.

5 CODE ACCURACY: FEW-BODY TESTS

5.1 Eccentric Keplerian binary

Next we demonstrate the numerical accuracy of our MSTAR
integrator by studying the standard Keplerian two-body
problem by comparing the results to the analytical solu-
tion and to our AR-CHAIN code (Rantala et al. 2017). In
the following Sections we also runs tests with two additional
three-body setups.

All the simulation runs of this study are run on the

1 www.mpcdf.mpg.de/services/computing/linux/Astrophysics

MNRAS 000, 1–20 (2019)

100101102103Ncpu10-410-2100102Force time per GBS step [s]Npart = 264Npart = 546Npart = 1129Npart = 2336Npart = 4833Npart = 10000Force onlyForce loop parallelisationFull parallelisationIdeal scalingA fast regularised integrator

13

Figure 10. Results of the SMBH binary simulations with our AR-CHAIN implementation (blue) and MSTAR (red). Starting from
the top left panel, the four panels show the maximum extrapolation error (cid:15)GBS after an accepted step, the relative error in energy E
and angular momentum Lz and the rotation angle of the Laplace–Runge–Lenz vector θLRL. The error regions depict a single standard
deviation. The lower scatter in the maximum extrapolation error in MSTAR indicates that the code does not exceedingly increase the
step-size after a successful step which would lead to divergence and step split during the next step. Our the new code clearly performs
better than our AR-CHAIN implementation with the numerical errors being smaller throughout.

with M = 2 × 109 M(cid:12), a = 2 pc and e = 0.9. We orient the
initial binaries in a random orientation in space in order to
have a sample of binaries with initially the same integrals of
motion but with diﬀering numerical errors during the sim-
ulation. We run the binary simulations for T = 104 × P in
which P is the Keplerian orbital period of the binary. The
GBS tolerance is set to ηGBS = 10−12. We always use kﬁx = 8
substep divisions in the serial GBS procedure.

The results of the binary simulations are presented in
Fig. 10 and Fig. 11. The panels of Fig. 10 illustrate the
relative error of the energy and the norm of the angular
momentum vector as well as the absolute rotation angle of
the Laplace–Runge–Lenz vector. In addition we show the
maximum GBS error (cid:15)GBS after convergence in each step.
Fig. 11 in turn presents the elapsed wall-clock time, the GBS

MNRAS 000, 1–20 (2019)

step fail rate and the length of the ﬁctitious time-step during
the simulations.

The results of the binary simulations systematically
show that the new MSTAR implementation conserves the
orbital energy E, angular momentum L and the Laplace–
Runge–Lenz vector A ∼ 1–2 orders of magnitude better than
our old AR-CHAIN implementation. In addition, the new
code is faster than the old code by a factor of few. The diﬀer-
ence in the code speed originates from the fact that the GBS
recipe of Hairer et al. (2008) that we are using in our AR-
CHAIN implementation optimises the computational work
per unit step and also aggressively attempts longer steps H
after convergence. This leads to a large number of failed GBS
steps, slowing down the code in the test. However, the imple-
mentation is not very transparent and thus it is somewhat

10-1410-1210-1010-8Relative errorGBSGBSGBSGBSGBSGBSRantala+17This worktt2E/EE/EE/Ett2E/EE/EE/E101102103104t/P10-1410-1210-1010-8Relative errorLz/Lztt2Lz/LzLz/LzLz/Lztt2Lz/LzLz/Lz101102103104t/PLRLLRLtt2LRLLRLLRLtt2LRL14

A. Rantala et al.

Figure 12. The general overview of the orbits of the three bodies
in the Pythagorean three-body problem. Initially the three bodies
are gravitationally bound, but after a series of complicated inter-
actions the least massive body (black line) is ejected while the
other two bodies (red and blue) form a bound binary recoiling at
the opposite direction. By eye, there are no noticeable diﬀerences
in the results with the two integrators.

the orbital elements (a2,3,e2,3) of the formed binary and the
escape direction β1. If the initial triangle setup is oriented as
in Fig. 1 of Aarseth et al. (1994), the escape angle becomes

β1 = arctan(y1/x1)
in which the subscript refers to the least massive SMBH. The
system is extremely sensitive to the initial conditions and the
numerical accuracy of the used integrator, thus providing an
ideal test setup for demonstrating that our old AR-CHAIN
and the new MSTAR implementations yield the same ﬁnal
results.

(32)

We perform the integration with the same code param-
eters as the two-body tests. We show the orbits of the three
SMBHs in the Pythagorean three-body problem in Fig. 12
both with the old AR-CHAIN and the new MSTAR inte-
grator. The overall behaviour of the system is as expected
from the literature results: the least massive body becomes
unbound and the binary of the two remaining bodies recoils
in the opposite direction. At this level of scrutiny there are
no noticeable diﬀerences between the two integrator imple-
mentations.

The escape angle β1 as well as the orbital elements of
the pair of two most massive bodies are presented in Fig.
13. After a period of complicated gravitational dynamics the
values of β1, a2,3 and e2,3 settle to their ﬁnal values as the
motion of the system becomes ordered after the escape of the
least massive SMBH. Both the AR-CHAIN and the MSTAR
integrator implementations provide the results β1 ≈ 71.4◦,
a2,3 ≈ 5.5 pc and e2,3 ≈ 0.99 with a relative diﬀerence of only
∼ 10−4 in each value. Due to the extreme sensitivity of the
Pythagorean three-body problem to numerical errors during
integration we conclude that the two integrators produce the
same results within an accuracy suﬃcient for our purposes.
These ﬁnal results agree also very well with the literature
values (Szebehely & Peters 1967; Aarseth et al. 1994).

MNRAS 000, 1–20 (2019)

Figure 11. Additional results of the binary simulations with our
previous AR-CHAIN implementation (blue) and the new code
(red). The top panel presents the wall-clock times spent by the
integrators with the new implementation being 2–3 times faster
than the old one. The middle and the bottom panels show the
GBS step fail rate and the GBS step size H in ﬁctitious time ∆s.
These panels conﬁrm that the new AR-CHAIN implementation
is faster in two-body tests due to its factor of ∼ 2.5 smaller GBS
step fail rate.

diﬃcult to point to exactly where the speed and accuracy
diﬀerences originate compared to our own implementation
of the extrapolation algorithm in MSTAR.

5.2 Pythagorean three-body problem

The Pythagorean three-body problem (Burrau 1913; Szebe-
hely & Peters 1967) is a famous zero angular momentum
setup to test integrators and to study chaotic and orderly
motion in a relatively simple gravitating system (Aarseth
et al. 1994; Valtonen & Karttunen 2006). Three SMBHs
with masses of M1 = 3 × 108 M(cid:12), M2 = 4 × 108 M(cid:12) and
M3 = 5 × 108 M(cid:12) are placed at the corners of a right-angled
triangle with side lengths of r13 = 30 pc, r12 = 40 pc and
r23 = 50 pc i.e. in such a manner that the least massive
SMBH is opposite to the shortest side and so on. Initially all
velocities are set to zero. Once the simulation run is started,
the three bodies experience a series of complicated interac-
tions ﬁnally resulting in the ejection of the least massive M1
body while the remaining two SMBHs form a bound binary
recoiling to the opposite direction.

The ﬁnal outcome of the system can be parametrised by

0.00.51.01.52.02.53.0Wallclock time [s]Rantala+17This work00.10.20.30.4GBS step fail rate0200040006000800010000t/P00.20.40.60.8s [code units]-0.06-0.04-0.0200.020.040.06x [kpc]-0.06-0.04-0.0200.020.040.06y [kpc]Rantala+17This WorkA fast regularised integrator

15

ical value icrit deﬁned as

icrit = arccos

(cid:32)(cid:114)

(cid:33)

3
5

(34)

which is approximately icrit ≈ 39.2◦. The maximum achiev-
able eccentricity emax depends only on the initial inclination
i0 as

emax =

(cid:114)

1 −

5
3

cos 2i0.

(35)

We set up a hierarchical three-body system with masses
of M1 = M3 = 109 M(cid:12) and M2 = 103 M(cid:12) using the following
orbital parameters. The outer binary is circular (eouter = 0)
with a semi-major axis of aouter = 20 pc. The inner binary is
initially almost circular (einner = 10−3) and has a semi-major
axis of ainner = 2 pc. The orbital plane of the secondary is
initially inclined i0 = 80◦ with respect to the orbital plane
of the outer binary, exceeding the critical inclination icrit
so the system exhibits Lidov–Kozai oscillations. The test
particle approximation predicts the maximum eccentricity
of emax ≈ 0.975 for the system.

We simulate the evolution of the three-body system for
100 Myr using both the AR-CHAIN and MSTAR integra-
tors. The integrator accuracy parameters are identical to the
ones in the previous Section. The oscillations of eccentricity
and inclination of the secondary during the simulation are
presented in Fig. 14. The system experiences roughly ten
strong oscillations in 100 Myr reaching a maximum eccen-
tricity of emax = 0.983. The minimum inclination during the
oscillations is very close to the critical value of icrit ≈ 39.2◦.
The system evolves practically identically when run with the
old AR-CHAIN and the new MSTAR integrator. The rela-
tive diﬀerence of the value of emax with the two integrators
is only of the order of 10−8.

6 CODE SCALING AND TIMING: N-BODY

TESTS

6.1 N-body initial conditions

We construct gravitationally bound clusters of equal-mass
point particles in order to perform code timing tests. We use
20 diﬀerent particle numbers Npart, selected logarithmically
between Npart = 101 and Npart = 104 particles with three dif-
ferent random seeds for each run, totalling 60 cluster initial
conditions. The particle positions are drawn from the spher-
ically symmetric Hernquist sphere (Hernquist 1990) with a
density proﬁle of
ρ(r) = M
2π

aH
r(r + aH)3

(36)

,

where M is the total mass of the system and aH its scale
radius. We set M = 107 M(cid:12) and aH in such a manner that
= 10 pc. The
the half-mass radius of the system equals r1/2
particle velocities are sampled from the Hernquist density-
potential pair using the Eddington’s formula technique (e.g.
Binney & Tremaine 2008).

Even though we do not intentionally include primor-
dial binaries in our cluster construction setup, binaries may
still form when sampling particle positions and velocities. A

Figure 13. The three variables β1,a2,3 and e2,3 parameterising
the outcome of the Pythagorean three-body problem as described
in the main text. The results of the MSTAR and the AR-CHAIN
integrator always agree within a relative factor of 10−4.

5.3 Lidov–Kozai oscillations

The Lidov–Kozai mechanism (Lidov 1962; Kozai 1962) is a
widely studied dynamical phenomenon present in a family
of hierarchical three-body systems. The mechanism has a
large number of applications in dynamical astronomy reach-
ing from dynamics of artiﬁcial satellites to systems of super-
massive black holes (Naoz 2016). An inner binary consisting
of a primary and a secondary body is perturbed by a dis-
tant orbiting third body. The inner binary and the perturber
form the outer binary. The time-varying perturbation causes
the argument of pericenter of the secondary body to oscil-
late around a constant value. Consequently, the eccentricity
and the inclination of the inner binary with respect to the
orbital plane of the outer binary oscillate as well. The time-
scale of the oscillations exceeds by far the orbital periods of
the inner and outer binaries.

In the limit of the secondary body being a test particle

the quantity

(cid:113)

lz =

1 − e2

2 cos i2

(33)

is conserved. Here the subscripts of the orbital elements re-
fer to the secondary body with respect to the primary body.
The Lidov–Kozai oscillations are present in the three-body
system if the inclination i0 of the secondary exceeds the crit-

MNRAS 000, 1–20 (2019)

-200-10001002001 [°]Rantala+17This Work10-310-210-1100a2,3 [kpc]0.00.51.01.52.02.53.03.54.0Time [Myr]0.00.20.40.60.81.01.2e2,316

A. Rantala et al.

Figure 15. The results of the strong scaling test of our MSTAR
integrator (solid red line). The ideal scaling behaviour is indicated
by the dashed red line. The scaling behaviour of the integrator
begins to deviate from the ideal scaling law around NCPU ∼ 0.1 ×
Npart and its completely saturated around NCPU ∼ 0.2 × Npart. The
test setups with the two highest particle numbers tested retain
the ideal scaling behaviour up to the largest CPU number used
in these tests, NCPU = 400.

clusters in total. For the same reason we do not include a
single heavy point mass (SMBH) at the centre of the cluster
as the orbital period of the most bound light particle (star)
would then determine the running time of the simulation.

6.2 Strong scaling tests

We perform a series of strong scaling tests to study the over-
all scaling behaviour of our MSTAR integrator. The results
of the strong scaling test of the force calculation part of the
code were presented in Fig. 9. As before in a strong scal-
ing test the problem size remains ﬁxed while the number of
CPUs is increased. In our six tests we use six diﬀerent log-
arithmically spaced particle numbers with 264 ≤ Npart ≤ 104
as in Section 4.3. The strong scaling tests consists of in to-
tal 270 short N-body simulations with initial conditions de-
scribed in the previous Section. In the simulations each of
the point-mass clusters is propagated for T = 0.1 Myr which
is close to the crossing times of the point-mass clusters. The
GBS tolerance is set to ηGBS = 10−6 in these tests. We test
CPU numbers up to NCPU = 400. The CPU number NCPU
is always divided between the force loop tasks (Nforce) and
substep parallelisation (Ndiv) in a way which minimises the
simulation running time as explained in Section 4.3.

The results of the strong scaling tests are shown in
Fig. 15. The maximum speed-up factors with NCPU = 400
range from Stotal ≈ 15 with Npart = 264 to Stotal ≈ 145 when
Npart = 104. At this point the scaling of the setup with the
lower particle number Npart is completely saturated while
the setup with Npart = 104 would still beneﬁt from addi-
tional computational resources. However, we do not pursue
numerical experiments beyond NCPU = 400 in this study.
We ﬁnd that the integrator follows ideal scaling behaviour
roughly up to the CPU number of NCPU ∼ 0.1 × Npart and a

MNRAS 000, 1–20 (2019)

Figure 14. The Lidov–Kozai mechanism test with the AR-
CHAIN (blue line) and the MSTAR integrator (red line). The
hierarchical three-body system shows strong oscillations both in
the inclination (top panel) and the eccentricity (bottom panel) of
the secondary body. The results of the two integrators agree very
well with each other and analytical estimates as described in the
text.

binary is considered hard if its binding energy exceeds the
average kinetic energy of a particle in the cluster, i.e.

GµM
2a

(cid:38) 1
2

mσ2,

(37)

where m is the mass of a single particle and σ is the velocity
dispersion of the cluster. While our MSTAR integrator can
easily compute the dynamics of hard binaries, the possible
existence of such binaries is problematic for the N-body tim-
ing tests. This is because an integrator using the logarithmic
Hamiltonian (or equivalent) time transformation can propa-
gate physical time only for an amount ∆t per one step, where
∆t is of the order of the orbital period P of the hardest binary
in the cluster, deﬁned as

P = 2π

(cid:19)1/2

(cid:18) a3
GM

(38)

by the Kepler’s third law. Consequently, the total running
time of the simulation will scale as ∆t−1 ∝ a−3/2. This is
very inconvenient as the clusters with the same Npart but
a diﬀerent binary content may have a very large scatter in
their simulation times up to several orders of magnitude.
Thus, we exclude all clusters which contain even a single
hard binary and generate a new cluster until we have 60

020406080100Inclination [°]Rantala+17This work020406080100Time [Myr]00.20.40.60.81Eccentricity100101102103Ncpu10-410-2100102Total time per GBS step [s]Npart = 264Npart = 546Npart = 1129Npart = 2336Npart = 4833Npart = 10000MSTAR wall-clock timeIdeal scalingLabel

Integrator

Mode

Resources

R17-S-1
R17-P-24
R20-S-1
R20-P-max

serial
AR-CHAIN
AR-CHAIN parallel
serial
parallel

MSTAR
MSTAR

1 CPU
24 CPU
1 CPU
2-400 CPU

Table 2. The integrators and their serial/parallel conﬁgurations
studied in the running time test. In the setup R20-P-max we
selected the CPU number within 2 ≤ NCPU ≤ 400 for each particle
number which yielded the fastest simulation times.

ﬂat, saturated scaling occurs with NCPU (cid:38) 0.2 × Npart. The
scaling of the entire code starts to deviate from the ideal
scaling behaviour at a factor of a few smaller NCPU than the
scaling of only the force calculation part of the code. We
attribute this diﬀerence to Amdahl’s law (Amdahl 1967),
which states that maximum speed-up of a parallelised code
depends on the fraction of serial code or code which cannot
be eﬃciently parallelised. The force calculation part of the
code can be almost entirely parallelised except for the nec-
essary MPI communication between the CPUs. The entire
integrator contains additional parts which cannot be par-
allelised as eﬃciently as the force computation. The main
functions containing these serial code sections or functions
which are diﬃcult to parallelise eﬃciently are the MST con-
struction functions and the GBS extrapolation procedure.

6.3 Timing tests

We perform another set of N-body simulations to evaluate
how much faster the parallelised version of the MSTAR in-
tegrator is than our AR-CHAIN implementation in Rantala
et al. (2017). In the simulations each of the 60 point-mass
clusters is again propagated for T = 0.1 Myr with a GBS
tolerance of ηGBS = 10−6. The other parameters remain as
in the previous Sections.

We test four diﬀerent integrator conﬁgurations. The inte-
grator details are collected in Table 2. Our old AR-CHAIN
implementation is used both in a serial (R17-S-1) and a par-
allel mode (R17-P-24) with 24 CPUs as in Rantala et al.
(2017). We test the MSTAR integrator in serial and parallel
modes as well (R20-S-1 and R20-P-max). In the test setup
R20-P-max we experimented with CPU numbers within
2 ≤ NCPU ≤ 400 and chose the NCPU with gave the smallest
running time for each particle number. In general adding
more CPUs speeds up the computation until the scaling
stalls around NCPU ∼ 0.2 × Npart as already illustrated in
Fig. 15. This type of a test is not performed with our old
AR-CHAIN integrator as the scaling of the code becomes
poor beyond a few tens of CPUs (Rantala et al. 2017).

The wall-clock times elapsed in the timing tests of the
four integrator conﬁgurations are presented in Fig. 16 as a
function of the particle numbers of the simulated clusters.
The results are scaled into the units of wall-clock time in
seconds per simulated Gyr. We restrict our tests to simula-
tions which last less than 109 seconds of wall-clock time in
the scaled units. Studying the results with the AR-CHAIN
integrator ﬁrst, we see that the parallel implementation is
faster than the serial version in the particle number range

MNRAS 000, 1–20 (2019)

A fast regularised integrator

17

Figure 16. The timing test of our AR-CHAIN (blue) and
MSTAR (red) integrators. The parallel runs are indicated with
ﬁlled squares while the serial runs are labelled with open cir-
cles. Benchmarks of 1 day, 1 week and 1 month are also in-
cluded. The grey shaded area marks the region which we deem
too time-consuming for practical applications. We note that the
new MSTAR serial code is even faster than the old parallel AR-
CHAIN integrator and that the new parallel code is extremely
fast compared to the other implementations, especially for large
particle numbers.

50 (cid:46) Npart (cid:46) 700. Our simulations in previous studies
(Rantala et al. 2018, 2019) have used regularised particle
numbers in the lower half of this range. With lower parti-
cle numbers the communication time exceeds the speed-up
from parallelised force loops. The slow-down of the paral-
lelised old integrator at high particle numbers Npart (cid:38) 700
is attributed to the fact that the code is only partially par-
allelised as serial functions still remain in the parallelised
version.

Comparing the serial implementations of the MSTAR
and the AR-CHAIN integrator (R17-S-1 and R20-S-1) we
see that the new integrator is faster by a factor of ∼ 6 when
Npart (cid:46) 100. The speed diﬀerence of the two serial codes
reaches its minimum value of ∼ 2 around Npart = 103. After
this the speed diﬀerence of the codes begins to increase again
reaching a value of ∼ 8 at Npart = 5 × 103. We note that the
MSTAR serial implementation R20-S-1 is even faster than
the AR-CHAIN parallel version R17-P-24 in every simula-
tion we run for this study. Code run-time performance anal-
ysis tools reveal that the cache eﬃciency of our AR-CHAIN
code is poor compared to the new MSTAR code. This fact
explains the run-time speed diﬀerence of the two serial codes.
All the four integrator conﬁgurations fairly closely follow the
O(N2
part) scaling, being consistent with the theoretical scaling
of the regularisation algorithm of O(N2.13−2.20

).

Studying the results of the MSTAR integrator we can
see that the parallel setup R20-P-max is always faster than
the serial setup R20-S-1, even with small particle numbers.
In addition, the test runs with R20-P-max become increas-
ingly faster than the serial setup towards high particle num-
bers. Within 103 (cid:46) Npart (cid:46) 104 the speed-up factor is ∼ 55-

part

1 month1 week1 dayN2101102103104Npart102103104105106107108109Wall-clock time [s] per GyrR17-S-1R17-P-24R20-S-1R20-P-max18

A. Rantala et al.

145. Compared to the fastest old AR-CHAIN implementa-
tion the new parallel MSTAR code is faster by a very large
factor of ∼ 1100 in this range of particle numbers.

The adopted GBS accuracy parameter aﬀects the wall-
clock time elapsed in the simulations. We perform an addi-
tional brief series of timing tests using MSTAR with sim-
ulation parameters Npart = 1129, NCPU = 200 and 10−12 ≤
ηGBS ≤ 10−6. We ﬁnd that in our tests the elapsed wall-clock
time T scales well as a power-law as

(cid:19)−α

(cid:18) ηGBS
10−6

=

(39)

T
T(ηGBS = 10−6)
in which the power-law index α ≈ 0.05 when 10−10 (cid:46) ηGBS ≤
10−6 and α ≈ 1 when ηGBS (cid:46) 10−10. Due to the mild scaling
of the wall-clock time T as a function of the GBS accuracy
parameter ηGBS we conclude that the results in the Section
run with ηGBS = 10−6 apply in general for GBS tolerances
ηGBS (cid:38) 10−10. However, we stress that the timing results
of the codes depend on the N-body particle setups used,
with more centrally concentrated stellar systems requiring
in general more computational time.

Finally, we end this Section by discussing how large sim-
ulations can be run with our new MSTAR code within a rea-
sonable wall-clock time. We have marked the running times
of 1 day, 1 week and 1 month into Fig. 16. The grey area
in the illustration marks the running times beyond 1 month
per Gyr which we consider unacceptably time-consuming.
The parallel MSTAR code can perform simulations with
of the order of 10 times more N-body particles with simi-
lar wall-clock times as our old integrator implementations.
Simulations with 4000-7000 particles are expected to last a
few weeks with the parallel MSTAR code with NCPU = 400.
Running a simulation with Npart = 104 in a similar wall-clock
time would require NCPU ≈ 2000 cores.

7 CONCLUSIONS

We have developed and implemented the MSTAR integra-
tor, a new fast algorithmically regularised integrator. While
the time transformation scheme of the regularised integra-
tor remains the same as in the earlier AR-CHAIN integrator,
the coordinate system and the GBS extrapolation method
are signiﬁcantly improved. A brief summary of the main in-
gredients of our integrator code and a selection of related
integrators from the literature is collected in Table 3.

In our new MSTAR implementation the chained co-
ordinate system of AR-CHAIN is replaced by a minimum
spanning tree coordinate system, which can be viewed as a
branching chain structure. Due to its ability to branch, the
MST avoids the possible pathological chain conﬁgurations
in which spatially close particles can be found in very dif-
ferent parts of the chain. We ﬁnd that the numerical error
originating from building and deconstructing the coordinate
structures is approximately smaller by a factor of ∼ 10 for
the MST compared to the chain. The reason for this is that
the MST is a much shallower data structure than the chain
as the average number of inter-particle vectors to reach the
root vertex of the system is smaller. Thus, we recommend
using the MST coordinate system instead of the chained
coordinates even though the code implementation becomes
somewhat more complicated.

Our MSTAR integrator includes a simpliﬁed GBS ex-
trapolation method with two layers of MPI parallelisation.
First, the force loop computation is parallelised with Nforce
CPUs with one MPI task each. The second layer is included
in order to compute the kﬁx substep divisions in parallel us-
ing Ndiv CPU groups. We also provide a recipe for estimating
how to divide the computational resources to the diﬀerent
parallelisation layers and estimates for the maximum rea-
sonable number of CPUs for parallelisation before the code
scaling stalls.

We validate the numerical accuracy of our MSTAR in-
tegrator in a series of demanding two- and three-body test
simulations. The simulation setups include an eccentric Ke-
plerian binary, the classic Pythagorean three-body problem
and Lidov–Kozai oscillations in a hierarchical three-body
system. Overall the particle orbits in the test runs are practi-
cally identical with both the MSTAR and AR-CHAIN inte-
grators. In fact MSTAR conserves energy, angular momen-
tum and the direction of the Laplace–Runge–Lenz vector
somewhat better than our previous regularised integrator
AR-CHAIN.

We test the speed and scaling behaviour of the MSTAR
integrator in a series of N-body stellar cluster simulations
with up to Npart = 104 particles. We ﬁnd that the new inte-
grator is always faster than the AR-CHAIN. With full paral-
lelisation we can eﬃciently use ∼ 10 times more CPUs with
adequate code scaling behaviour compared to our integra-
tor implementation with force loop parallelisation only. The
speed-up gained by the new fully parallelised integrator is
substantial. The parallel MSTAR code is up to a factor of
∼ 145 faster than the serial MSTAR code and up to a fac-
tor of ∼ 1100 faster than the AR-CHAIN code when the
simulation particle number in the range 103 (cid:46) Npart (cid:46) 104.
The MSTAR integrator will be important when press-
ing towards the ultimate goal of running collisionless simula-
tions containing regularised regions with collisional stars and
SMBHs with up to Npart ∼ 5 × 108–109 simulation particles
in individual galaxies. We estimate that the MSTAR inte-
grator is able to run a Gyr-long simulation with Npart = 104
in approximately two weeks of wall-clock time using NCPU ≈
2000 CPUs. In our previous studies with the KETJU code
(Rantala et al. 2017) which couples the GADGET-3 tree
code to the AR-CHAIN integrator the total particle number
in galaxies was limited to Npart (cid:46) 107 particles (Rantala et al.
2018, 2019). This is due to the fact that our AR-CHAIN inte-
grator could eﬃciently handle only up to 200-300 particles in
the regularised regions. Based on these numbers we estimate
that MSTAR can treat ∼ 50 times more regularised particles
than our AR-CHAIN implementation in a reasonable wall-
clock time. Thus, galaxy simulations with accurate SMBH
dynamics using MSTAR in KETJU instead of AR-CHAIN
containing 5 × 108 (cid:46) Npart (cid:46) 109 simulation particles seem
achievable in the immediate future. These particle numbers
yield stellar mass resolutions down to m(cid:63) ≈ 100M(cid:12) even for
simulations of massive early-type galaxies.

Finally, the improved numerical scaling and perfor-
mance will be crucial when simulating the dynamics of
SMBHs in gas-rich galaxies, which are expected to have
steep central stellar concentrations due to elevated levels of
star formation in their nuclei. This is in particular impor-
tant as the upcoming LISA gravitational wave observatory
will be most sensitive for SMBHs with masses in the range

MNRAS 000, 1–20 (2019)

A fast regularised integrator

19

Ito & Fukushima (1997) Mikkola & Merritt (2008) Rantala et al. (2017) This work

Regularisation
Extrapolation method

Chained coordinates
MST coordinates

Serial code
Parallel force loops
Parallel GBS subdivisions

×
(cid:88)

×
×

(cid:88)
×
(cid:88)

(cid:88)
(cid:88)

(cid:88)
×

(cid:88)
×
×

(cid:88)
(cid:88)

(cid:88)
×

(cid:88)
(cid:88)
×

(cid:88)
(cid:88)

(cid:88)
(cid:88)

(cid:88)
(cid:88)
(cid:88)

Table 3. A brief summary of the main properties of the MSTAR integrator presented in this work alongside related integration methods
from the literature.

of MBH ∼ 106–107 M(cid:12) (Amaro-Seoane et al. 2007), which are
expected to reside at the centres of gas-rich late-type galax-
ies.

ACKNOWLEDGEMENTS

We would like to thank Seppo Mikkola, the referee of the
paper. The numerical simulations were performed on facili-
ties hosted by the CSC – IT Center for Science, Finland and
the Max Planck Computing and Data facility (MPCDF),
Germany. A.R., P.P., M.M. and P.H.J. acknowledge the sup-
port by the European Research Council via ERC Consolida-
tor Grant KETJU (no. 818930). TN acknowledges support
from the Deutsche Forschungsgemeinschaft (DFG, German
Research Foundation) under Germany’s Excellence Strategy
- EXC-2094 - 390783311 from the DFG Cluster of Excellence
”ORIGINS”.

REFERENCES

Aarseth S. J., 1999, Publications of the Astronomical Society of

the Paciﬁc, 111, 1333

Dubois Y., Gavazzi R., Peirani S., Silk J., 2013, MNRAS, 433,

3297

Duﬀell P. C., D’Orazio D., Derdzinski A., Haiman Z., Mac-
Fadyen A., Rosen A. L., Zrake J., 2019, arXiv e-prints, p.
arXiv:1911.05506

Ferrarese L., Ford H., 2005, Space Sci. Rev., 116, 523
Gragg W. B., 1965, SIAM Journal on Numerical Analysis, 2, 384
Gualandris A., Read J. I., Dehnen W., Bortolas E., 2017, MN-

RAS, 464, 2301

Hairer E., Lubich C., Wanner G., 2006, Geometric numerical in-
tegration: structure-preserving algorithms for ordinary diﬀer-
ential equations. Vol. 31, Springer-Verlag, Berlin Heidelberg
Hairer E., Nørsett S., Wanner G., 2008, Solving Ordinary Dif-
ferential Equations I: Nonstiﬀ Problems. Springer Series in
Computational Mathematics, Springer Berlin Heidelberg
Harary F., 1969, Graph theory. Reading: Addison-Wesley, 1969
Harfst S., Gualandris A., Merritt D., Mikkola S., 2008, MNRAS,

389, 2

Hayward C. C., Torrey P., Springel V., Hernquist L., Vogelsberger

M., 2014, MNRAS, 442, 1992

Hellstr¨om C., Mikkola S., 2010, Celestial Mechanics and Dynam-

ical Astronomy, 106, 143
Hernquist L., 1990, ApJ, 356, 359
Holley-Bockelmann K., Richstone D., 1999, ApJ, 517, 92
Ito T., Fukushima T., 1997, AJ, 114, 1260
Jernigan J. G., Porter D. H., 1989, The Astrophysical Journal

Aarseth S. J., 2003, Gravitational N-Body Simulations. Cam-

Supplement Series, 71, 871

bridge University Press

Aarseth S. J., 2012, MNRAS, 422, 841
Aarseth S. J., Anosova J. P., Orlov V. V., Szebehely V. G., 1994,

Celestial Mechanics and Dynamical Astronomy, 58, 1

Alexander T., 2017, ARA&A, 55, 17
Amaro-Seoane P., Gair J. R., Freitag M., Miller M. C., Mandel I.,
Cutler C. J., Babak S., 2007, Classical and Quantum Gravity,
24, R113

Amdahl G. M., 1967,

in Proceedings of

the April 18-
20, 1967, Spring Joint Computer Conference. AFIPS
’67 (Spring). ACM, New York, NY, USA, pp 483–485,
doi:10.1145/1465482.1465560

Beckmann R. S., Slyz A., Devriendt J., 2018, MNRAS, 478, 995
Begelman M. C., Blandford R. D., Rees M. J., 1980, Nature, 287,

Johansson P. H., Naab T., Burkert A., 2009, ApJ, 690, 802
Kahan W., 1965, Commun. ACM, 8, 40
Khan F. M., Just A., Merritt D., 2011, ApJ, 732, 89
Khan F. M., Fiacconi D., Mayer L., Berczik P., Just A., 2016,

ApJ, 828, 73

Kim J.-h., Wise J. H., Alvarez M. A., Abel T., 2011, ApJ, 738,

54

Korch M., Rauber T., Scholtes C., 2011, Concurrency and Com-

putation: Practice and Experience, 23, 1789
Kormendy J., Ho L. C., 2013, ARA&A, 51, 511
Kormendy J., Richstone D., 1995, ARA&A, 33, 581
Kozai Y., 1962, AJ, 67, 591
Kruskal J. B., 1956, Proceedings of the American Mathematical

Society, 7, 48

307

Kustaanheimo P., Stiefel E., 1965, J. Reine Angew. Math, 218,

Berczik P., Merritt D., Spurzem R., Bischof H.-P., 2006, ApJ, 642,

204

L21

Binney J., Tremaine S., 2008, Galactic Dynamics: Second Edition.

Princeton University Press

Lauer T. R., et al., 2007, ApJ, 664, 226
Lidov M. L., 1962, Planet. Space Sci., 9, 719
Mannerkoski M., Johansson P. H., Pihajoki P., Rantala A., Naab

Bor˚uvka O., 1926, Pr´ace Moravsk´e pˇr´ırodovˇedeck´e spoleˇcnosti,

T., 2019, The Astrophysical Journal, 887, 35

sv. III, 7, 37

Boylan-Kolchin M., Ma C.-P., 2004, MNRAS, 349, 1117
Bulirsch R., Stoer J., 1966, Numerische Mathematik, 8, 1
Burrau C., 1913, Astronomische Nachrichten, 195, 113
Deuﬂhard P., 1983, Numerische Mathematik, 41, 399ˆa ˘A¸S422

Mayer L., Kazantzidis S., Madau P., Colpi M., Quinn T., Wadsley

J., 2007, Science, 316, 1874
Merritt D., 2006, ApJ, 648, 976
Merritt D., 2013, Dynamics and Evolution of Galactic Nuclei.

Princeton University Press

MNRAS 000, 1–20 (2019)

20

A. Rantala et al.

Mikkola S., 1997, Celestial Mechanics and Dynamical Astronomy,

67, 145

Mikkola S., Aarseth S. J., 1989, Celestial Mechanics and Dynam-

ical Astronomy, 47, 375

Mikkola S., Aarseth S. J., 1993, Celestial Mechanics and Dynam-

ical Astronomy, 57, 439

Mikkola S., Aarseth S., 2002, Celestial Mechanics and Dynamical

Astronomy, 84, 343

Mikkola S., Merritt D., 2006, MNRAS, 372, 219
Mikkola S., Merritt D., 2008, AJ, 135, 2398
Mikkola S., Tanikawa K., 1999a, Celestial Mechanics and Dynam-

ical Astronomy, 74, 287

Mikkola S., Tanikawa K., 1999b, MNRAS, 310, 745
Mikkola S., Palmer P., Hashida Y., 2002, Celestial Mechanics and

Dynamical Astronomy, 82, 391

Milosavljevi´c M., Merritt D., 2001, ApJ, 563, 34
Milosavljevi´c M., Merritt D., 2003, ApJ, 596, 860
Misgeld I., Hilker M., 2011, MNRAS, 414, 3699
Moody M. S. L., Shi J.-M., Stone J. M., 2019, ApJ, 875, 66
Naoz S., 2016, ARA&A, 54, 441
Neumaier A., 1974, ZAMM - Journal of Applied Mathematics
and Mechanics / Zeitschrift f ˜Aijr Angewandte Mathematik
und Mechanik, 54, 39

Peters P. C., Mathews J., 1963, Physical Review, 131, 435
Pihajoki P., 2015, Celestial Mechanics and Dynamical Astron-

omy, 121, 211

Press W., Teukolsky S., Vetterling W., Flannery B., 2007, Nu-
merical Recipes 3rd Edition: The Art of Scientiﬁc Computing.
Cambridge University Press

Preto M., Tremaine S., 1999, AJ, 118, 2532
Prim R. C., 1957, Bell System Technical Journal, 36, 1389
Quinlan G. D., 1996, New Astron., 1, 35
Rantala A., Pihajoki P., Johansson P. H., Naab T., Lah´en N.,

Sawala T., 2017, ApJ, 840, 53

Rantala A., Johansson P. H., Naab T., Thomas J., Frigo M., 2018,

ApJ, 864, 113

Rantala A., Johansson P. H., Naab T., Thomas J., Frigo M., 2019,

ApJ, 872, L17

Rauber T., R¨unger G., 1997, Concurrency: Practice and Experi-

ence, 9, 181

Ryu T., Perna R., Haiman Z., Ostriker J. P., Stone N. C., 2018,

MNRAS, 473, 3410

Sanders D. B., Mirabel I. F., 1996, ARA&A, 34, 749
Springel V., Di Matteo T., Hernquist L., 2005, MNRAS, 361, 776
Stumpﬀ K., Weiss E. H., 1968, Journal of the Astronautical Sci-

ences, 15, 257

Szebehely V., Peters C. F., 1967, AJ, 72, 876
Tang Y., MacFadyen A., Haiman Z., 2017, MNRAS, 469, 4258
Valtonen M., Karttunen H., 2006, The Three-Body Problem.

Cambridge, UK: Cambridge University Press

Vasiliev E., Antonini F., Merritt D., 2015, ApJ, 810, 49
Wang X., Wang X., Mitchell Wilkes D., 2009, IEEE Transactions

on Knowledge and Data Engineering, 21, 945
White S. D. M., Rees M. J., 1978, MNRAS, 183, 341
Zhong C., Malinen M., Miao D., Fr¨anti P., 2013, in Wilson R.,
Hancock E., Bors A., Smith W., eds, Computer Analysis of
Images and Patterns. Springer Berlin Heidelberg, Berlin, Hei-
delberg, pp 262–269

This paper has been typeset from a TEX/LATEX ﬁle prepared by
the author.

MNRAS 000, 1–20 (2019)

