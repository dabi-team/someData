2
2
0
2

g
u
A
7
1

]

O
R
.
s
c
[

1
v
0
8
8
8
0
.
8
0
2
2
:
v
i
X
r
a

STTAR: Surgical Tool Tracking using off-the-shelf
Augmented Reality Head-Mounted Displays

1

Alejandro Martin-Gomez†, Haowei Li§, Tianyu Song, Sheng Yang, Guangzhi Wang,
Hui Ding, Nassir Navab, Fellow, IEEE, Zhe Zhao, Mehran Armand

Abstract—The use of Augmented Reality (AR) for navigation purposes has shown beneﬁcial in assisting physicians during the
performance of surgical procedures. These applications commonly require knowing the pose of surgical tools and patients to provide
visual information that surgeons can use during the performance of the task. Existing medical-grade tracking systems use infrared
cameras placed inside the Operating Room (OR) to identify retro-reﬂective markers attached to objects of interest and compute their
pose. Some commercially available AR Head-Mounted Displays (HMDs) use similar cameras for self-localization, hand tracking, and
estimating the objects’ depth. This work presents a framework that uses the built-in cameras of AR HMDs to enable accurate tracking
of retro-reﬂective markers, such as those used in surgical procedures, without the need to integrate any additional components. This
framework is also capable of simultaneously tracking multiple tools. Our results show that the tracking and detection of the markers can
be achieved with an accuracy of 0.09 ± 0.06 mm on lateral translation, 0.42 ± 0.32 mm on longitudinal translation and 0.80 ± 0.39◦ for
rotations around the vertical axis. Furthermore, to showcase the relevance of the proposed framework, we evaluate the system’s
performance in the context of surgical procedures. This use case was designed to replicate the scenarios of k-wire insertions in
orthopedic procedures. For evaluation, two surgeons and one biomedical researcher were provided with visual navigation, and each
performed 21 injections. Results from this use case provide comparable accuracy to those reported in the literature for AR-based
navigation procedures.

Index Terms—Augmented Reality, Computer-Assisted Medical Procedures, Navigation, Tracking.

(cid:70)

1 INTRODUCTION

I N recent years, an increasing number of Augmented

Reality (AR) applications have found their use in medical
domains, including educational, training, and surgical inter-
ventions [1], [2], [3], [4]. Among the multiple technologies
capable of delivering augmented content, using AR Head-
Mounted Displays (HMDs) has proven beneﬁcial to aid
physicians during surgical interventions [5], [6], [7], [8]. The
use of HMDs in the Operating Room (OR) allows for the
observation of relevant content in-situ, enables delivering
visual guidance and navigation, promotes the visualization
and understanding of diverse bi-dimensional and three-
dimensional medical
imaging modalities, and facilitates
training and communication [9], [10].

A fundamental requirement to provide visual guidance
and surgical navigation using AR is to know the three-
dimensional pose of the elements involved in the task (i.e.,
their position and orientation). Although multiple tech-
nologies can be used to estimate the pose of the objects

• A. Martin-Gomez† and H. Li§ have contributed equally to this work.
E-mail: †alejandro.martin@jhu.edu, §lihaowei19991202@gmail.com
• A. Martin-Gomez, N. Navab, and M. Armand are with the Laboratory
for Computational Sensing and Robotics, Whiting School of Engineering,
Johns Hopkins University, United States of America.

• M. Armand is also with the Department of Orthopaedic Surgery, Johns

Hopkins University School of Medicine, United States of America.

• H. Li, S. Yang, G. Wang, and H. Ding are with the Department of

Biomedical Engineering, Tsinghua University, China.

• Z. Zhao is with the Department of Orthopaedics, Beijing Tsinghua

Changgung Hospital. School of Clinical Medicine, Tsinghua University.

• T. Song and N. Navab are with the Chair for Computer Aided Medical
Procedures and Augmented Reality, Department of Informatics, Technical
University of Munich, Germany.

of interest, optical tracking is one of the most common
due to its computational simplicity, cost-effectiveness, and
the availability of cameras used in AR applications [11].
This tracking technology uses computer vision algorithms
to identify textural patterns or geometrical properties of
objects of interest, from which their dimensions are well
known, to estimate their pose.

In the medical context, providing surgical navigation
requires knowing the pose of the patients, surgical tools,
and, occasionally, the surgeons and the medical team’s head.
Commercially available medical-grade tracking systems es-
timate the pose of the objects of interest using infrared
cameras that detect the location of retro-reﬂective markers
rigidly attached to them. These retro-reﬂective markers are
arranged in unique conﬁgurations to distinguish between
the multiple objects observed in the scene. Commercially
available HMDs, use built-in infrared cameras for self-
localization, hand tracking, and estimation of the objects’
depth in the environment. Thus, these cameras can also be
used to locate the retro-reﬂective markers used in medical-
grade tracking systems1 [12], [13]. While combining the
accuracy provided by medical-grade tracking systems with
the visualization capabilities of AR HMDs could support
the integration of these devices into surgical scenarios, si-
multaneously maintaining the ﬁducial markers in the line
of sight of the HMD and the tracking system in a cluttered
workspace may hinder its use in the OR. As an alternative,
using the HMD for combined visualization and tracking of
these markers may mitigate such challenges. However, the

1. e.g., Polaris NDI, Northern Digital Incorporated. Ontario, Canada.

 
 
 
 
 
 
2

(a)

(b)

(c)

Fig. 1. (a) Calibration model used to align a set of virtual and real spheres. This model enables the registration of the observer’s view and the image
sensors. (b) Our tracking system enables the localization of surgical tools and anatomical models by attaching retro-reﬂective passive markers
without integrating any other external trackers or electronics. (c) A surgeon can then use this method to insert surgical k-wires assisted by virtual
trajectories displayed using an Augmented Reality Head-Mounted Display.

accuracy and feasibility of using cameras and sensors that
may not have the accuracy and ﬁeld of view of conventional
tracking systems require detailed investigation. Therefore, it
is essential to establish a framework capable of performing
this task and investigate the accuracy of combined tracking
and visualization when only using HMDs.

This work introduces a method that uses the built-
in Time-of-Flight (ToF) sensor of commercially available
HMDs2 to deﬁne, detect and localize passive retro-reﬂective
markers commonly used during navigation-assisted med-
ical procedures. Compared to existing works, our method
can track multiple objects in real-time and does not require
the integration of additional hardware such as infrared
LEDs to illuminate the retro-reﬂective markers. In addition,
we investigate the capabilities of the built-in cameras of the
HoloLens2 to provide valuable insights into their strengths
and limitations. Furthermore, we present a use case in the
context of medical applications to provide navigation capa-
bilities and aid physicians performing percutaneous surgical
procedures assisted by this type of HMD. We expect these
experiments can aid researchers in deciding if such devices
are suitable for speciﬁc applications.3

2 RELATED WORK

Using commercially available AR HMDs for medical ap-
plications has found application in several surgical ﬁelds.
In spine surgery, providing surgical navigation for pedi-
cle screw placement using AR HMDs has shown shorter
placement time when compared to traditional procedures
[14] and comparable accuracy to robotic-assisted computer-
navigated approaches [15]. In addition, it has supported
spinal pedicle guide placement for cannulation without
using ﬂuoroscopy [16] and enabled the registration between
the patient’s anatomy and three-dimensional models con-
taining surgical planning data [17]. This technology has also
been used to provide visual guidance during the perfor-
mance of osteotomies, contributing to an increase in the
accuracy achieved by inexperienced surgeons compared to

2. i.e., Microsoft HoloLens 2. Microsoft. Washington, USA.
3. The code for our work can be accessed through the following link:

https://github.com/lihaowei1999/IRToolTrackingWithHololens2.git

freehand procedures [18]. Additional studies have explored
the potential of using AR HMDs to support users during
the performance of hip arthroplasties, showing compara-
ble results to those achieved using commercial computer-
assisted orthopedic systems [19], [20]. Even more, using AR
HMDs reduces the events during which the surgeons devi-
ate their attention from the surgical scene or are exposed to
ﬂuoroscopy radiation due to unprotected parts when they
turn their bodies [21], [22].

Different works have used the built-in sensors of com-
mercially available HMDs to estimate the pose of objects
of interest in medical applications. These approaches avoid
incorporating external tracking systems and additional de-
vices into the already cluttered OR. On the one hand, identi-
fying markers in the visible spectrum using the built-in RGB
cameras has been used to investigate the feasibility of using
AR in surgical scenarios [17], [19]. The identiﬁcation of this
type of marker is not only limited to the visible spectrum
and has also been investigated using multimodal images
[23]. However, the low accuracy of the registration and
motion tracking may hinder their successful integration into
the surgical workﬂow [17]. On the other hand, the use of
infrared cameras to track retro-reﬂective markers has been
adopted by surgical grade tracking systems as they fulﬁll
the accuracy requirements to provide surgical navigation.
While commercially available HMDs frequently incorpo-
rate infrared cameras for self-localization, hand tracking, or
depth estimation, navigation accuracy analysis with these
cameras requires further investigation.

One of the initial works proposing the use of the built-in
cameras of AR HMDs for the detection and tracking of retro-
reﬂective markers was proposed by Kunz et al. [12]. This
work introduced two different approaches to detecting the
markers using the built-in sensors of commercially available
AR HMDs (i.e., Microsoft HoloLens 1). A ﬁrst approach
combined the reﬂectivity and depth information collected
using the near ﬁeld ToF sensor. A second approach used
the frontal environmental grayscale cameras as a stereo pair
to estimate the position of the objects of interest. This last
approach required the addition of external infrared LEDs
to illuminate the markers and increase their visibility in
the grayscale images. However, using the environmental

AlignCalibration ModelVirtual SpheresCalibrationModelTest ModelPlanned Trajectoriescameras provide a higher resolution than the depth camera
used as part of the ToF sensor (648x480 vs. 448x450 pixels,
respectively). Although an ofﬂine accuracy evaluation of
both approaches is presented in this work, the tracking
and implementation of the system are reported only for
the depth-based approach, showing a tracking accuracy of
0.76 mm when the markers are placed at distances ranging
between 40 to 60 cm. In addition, only translational and not
rotational experiments were reported in this work.

Additional work presented by Gsaxner et al. [13] used
the frontal environmental grayscale cameras of the Mi-
crosoft HoloLens 2 for similar purposes. This work in-
troduced two tracking pipelines that use stereo vision al-
gorithms and Kalman ﬁlters, respectively, to track retro-
reﬂective markers in the context of surgical navigation.
The proposed framework uses the stereo vision pipeline to
initialize the tracking of the markers and a recursive single-
constraint-at-a-time extended Kalman ﬁlter to keep tracking
of the object of interest. Whenever the tracking using the
Kalman ﬁlter is lost, the stereo vision pipeline is used to re-
initialize the tracking of the objects. This work showed that
the combination of these tracking pipelines enables real-time
tracking of the markers with six degrees of freedom and an
accuracy of 1.70 mm and 1.11◦. Unlike Kunz et al. [12], this
work investigated the tracking errors in the position and
orientation of the objects of interest. However, this system
also required the integration of additional infrared LEDs to
illuminate the retro-reﬂective markers.

3 METHODS

This work presents a framework that uses the built-in
cameras integrated into commercially available AR HMDs,
to enable the tracking of retro-reﬂective markers without
adding any external components. This section describes
the necessary steps to calibrate these cameras, enable the
detection of retro-reﬂective markers commonly attached to
surgical instruments and robotic devices, identify multiple
tools in the scene, and provide visual guidance to users or
AR applications. To exemplify the potential of our frame-
work, we present a use case in which the tool tracking
results serve to provide visual navigation to users during
the placement of pedicle screws in orthopedic surgery.

3.1 System Setup

The proposed framework was implemented using the Mi-
crosoft HoloLens 2 and a workstation equipped with an
Intel Xeon(R) E5-2623 v3 CPU, an NVIDIA Quadro K4200
GPU, and 104 Gigabytes of 2133 MHz ECC Memory. The
research mode provided for the HoloLens served to gain
access to the device’s built-in sensors and cameras [24].
To optimize the acquisition and processing of the images
collected using the HMD, we connected this device to the
workstation using a USB-C to ethernet adapter and a cable
that provided a 1000 Mbps transmit rate. A multi-threaded
sensor data transfer system, implemented in Python via TCP
socket connection, enabled the acquisition of the sensors’
data at a high frame rate and with low latency. The data
transfer system was tested using two different approaches.
A ﬁrst approach, using DirectX, allowed to transfer and

3

acquire the video from the cameras at an approximate
framerate of 31 fps. A second approach, implemented using
Unity 3D, reported an average speed of 21 fps. In addition, a
user interface was designed to collect, process, and analyze
the data collected from the HMD. This user interface also
provided visual guidance and assisted users while perform-
ing tasks requiring the precise alignment of virtual and real
objects in surgical scenarios.

3.2 Camera Calibration and Registration

Camera Calibration. To investigate the feasibility of utiliz-
ing and combining the multiple cameras integrated into the
HoloLens 2 for the tracking of retro-reﬂective markers, the
left front (LF), right front (RF), main (RGB), and Articulated
HAnd Tracking (AHAT) cameras need to be calibrated. This
calibration process enables extracting the spatial relation-
ship between the cameras and their intrinsic parameters and
representing them using a uniﬁed coordinate system.

We used a checkerboard with a 9 × 12 grid containing
individual squares of 2cm per side to calibrate the different
sensors. The checkerboard was attached to a robotic arm,
and the HoloLens was rigidly ﬁxed on a head model to
ensure steadiness during image capturing. A set of 120
pictures, acquired by placing the checkerboard at different
positions and orientations using the robotic arm, were col-
lected for each one of the cameras. The images for the RGB
camera were captured using the HoloLens device portal
with a resolution of 3904 × 2196 pixels, while the other
cameras’ images were retrieved via the HoloLens research
mode. For the AHAT camera, the calibration and registra-
tion procedure was completed using the reﬂectivity images.
After data collection, the images were randomly subdi-
vided into two subsets. The ﬁrst subset contained 90 pictures
used for camera calibration, and the remaining 30 images
served for testing. The Root Mean Square Error (RMSE)
of the detected and reprojected corner points was used to
evaluate the calibration and test image sets. The images
selection process was repeated twenty times to ensure the
repeatability of the results. The reprojection errors for the
calibration and test sets resulted in 0.165 ± 0.032 pixels and
0.152 ± 0.052 pixels for the AHAT, 0.069 ± 0.001 pixels and
0.070 ± 0.002 pixels for the LF, 0.080 ± 0.002 pixels and
0.080 ± 0.002 pixels for the RF, and 0.305 ± 0.020 pixels
and 0.308 ± 0.060 pixels for the RGB, respectively.

Camera Registration. For the registration of the different
cameras, a new set of 100 images were collected from the
different sensors while placing the checkerboard at different
poses. The checkerboard was kept steady before moving it
to a different position to ensure synchronization between
the multiple sensors. As an initial step, the RF camera was
registered to the LF camera using the stereo camera calibration
toolbox from Matlab. The following step registered the LF
and RGB cameras to the AHAT. For the registration of these
cameras, the three-dimensional positions of the checker-
board’s corner points were calculated using the respective
intrinsic camera parameters. The extrinsic parameters be-
tween two cameras were later calculated by solving a least-

squares optimization problem:

4

(RA

c , tA

c ) = argmin

R∈SO(3),t∈R3

n
(cid:88)

i=1

||(Rpi + t) − qi||2

(1)

where pi represents the three-dimensional corner points in
the RGB and LF cameras, and qi denotes the corner points
in AHAT space.

The registration error was evaluated using the RMSE
between the detected corner points of one camera and the
reprojected points of the other camera. The registration
between the RF-LF cameras produced a 0.190 pixels repro-
jection error. In comparison, the registration between the LF-
AHAT and RGB-AHAT cameras led to reprojection errors of
0.901 and 0.758 pixels, respectively.

In addition, the ﬁeld-of-view (FoV) of every camera was
estimated quantitatively in horizontal and vertical direc-
tions using the equations:

F oVx = max

0<i<j≤n

arccos

F oVy = max

0<i<j≤n

arccos

(cid:18)

(cid:18)

(xi, 0, 1) · (xj, 0, 1)
||(xi, 0, 1)|| × ||(xj, 0, 1)||

(0, yi, 1) · (0, yj, 1)
||(0, yi, 1)|| × ||(0, yj, 1)||

(cid:19)

(cid:19)

(2)

(3)

Once the registration between the cameras is complete,
the different sensors can be presented in a common coor-
dinate space using their corresponding camera parameters
(see Figure 2). This ﬁgure depicts the FoV of the visible
light cameras from the optical center to a unit plane using
frustums. As the AHAT sensor data is only effective within
a 1-meter distance, the FoV of this sensor is depicted by the
furthest detection surface.

During the calibration process presented in this work, a
12.76◦ angle down was observed for the AHAT compared
to the other cameras. The FoV of the AHAT camera resulted
in 127◦ × 127◦, which is about six times larger than the RGB
camera (40◦ × 65◦) and three times larger than the LF and
RF cameras (82◦ × 65◦), making it extremely useful for the
target detection within a meter distance, where most of the
actions in the personal space take place. In addition, the
relatively large FoV of the AHAT camera, combined with
the sensor information of environmental infrared reﬂectivity
and depth, makes this camera suitable for detecting retro-
reﬂective markers used in surgical procedures.

3.3 Tool Tracking

To explore the capabilities of the AHAT camera to track
retro-reﬂective markers used in surgical scenarios, we pro-
pose a framework for the deﬁnition, recognition, and lo-
calization of tools that use this speciﬁc type of marker.
As depicted in Figure 3, such a framework includes three
main stages: three-dimensional marker’s center detection,
tool deﬁnition, and multi-tool recognition and localization.

3.3.1 Marker Detection

As an initial step, the three-dimensional positions of the
individual retro-reﬂective markers are obtained using the
intrinsic camera parameters and the reﬂectivity and depth
images of the AHAT camera. This type of retro-reﬂective
marker depicts extremely high-intensity values when ob-
served in the reﬂectivity images of the AHAT camera.

Fig. 2. The spatial relationship between the built-in cameras of the
HoloLens 2. The camera spaces of the RGB, LF, and RF cameras are
displayed using frustums. The camera space of the AHAT camera is
presented using a one-meter radius sphere. The FoV of all the cameras
is depicted as height × width.

Fig. 3. The proposed framework comprises three main stages. i) The
three-dimensional position of all the retro-reﬂective markers observed
using the AHAT camera is extracted from the scene. ii) The detected
markers are grouped in subsets to identify particular arrangements
corresponding to speciﬁc tools. iii) The pose of the identiﬁed tools is
extracted from subsequent image frames.

Therefore, an intensity-based threshold criterion was used
to separate these objects of interest from the rest of the scene.
The AHAT intensity images are represented using a 16-bit
unsigned integer format. As depicted in Figure 4, most of
the environment presents a reﬂectivity intensity below 300.
At the same time, the retro-reﬂective markers report values
larger than 500, with a peak value over 2000 at the center of
the marker.

In addition, a connected component detection algorithm
was used to separate individual markers from each other.
Considering the environmental noise observed in the re-
ﬂectivity images, including large connected high-reﬂection
areas caused by ﬂat reﬂective objects like glass or moni-
tors or smaller areas resulting from random noise or sur-
rounding objects, an additional threshold on the connected
component area was applied to extract the individual retro-
reﬂective markers. To estimate the area in pixels Apx that a
retro-reﬂective marker with radius r at a distance d would
occupy in the image of the AHAT camera, we used the
following equation:

Apx ≈

π · r2
sx/fx · sy/fy · d2

(4)

where sx, sy, fx, and fy are the pixel size in mm/px and the

1mAHAT Reflectivity FrameMarker DetectionPassive Marker DetectionAHAT Depth Frame3D Marker CenterReconstruction𝑋𝑌𝑍Tool Definition2D MarkerCentersData CollectingTool DefinitionTool ConstructionTool TrackingPossible Tool RecognitionConflict SolutionRecognized ToolPose Estimation𝑇𝑖IntrinsicParametersKalman Filterfocal distance in mm of the AHAT intrinsic parameters.

After applying the intensity-based and connected com-
ponent detection algorithms, the three-dimensional posi-
tion of every retro-reﬂective marker is calculated using the
AHAT camera’s depth information di, its intrinsic parame-
ters, and the marker’s diameter. The central pixel of every
marker is ﬁrst back-projected into a unit plane (xi, yi) using
the intrinsic parameters. The depth information from the
AHAT camera represents the distance between a certain
point in the three-dimensional space and the camera’s op-
tical center [25]. Hence, the position (Xi, Yi, Zi) of this point
can be expressed as follows:

(Xi, Yi, Zi) =

di
||(xi, yi, 1)||2

(xi, yi, 1)

(5)

While the calculated position corresponds to the actual cen-
ter of the target when using ﬂat markers, the radius of the
target must be considered when using spherical markers.
In this particular case, the estimated depth information
provided by the AHAT camera corresponds to the central
point on the surface of the marker and not to the center
of the sphere. Therefore, when using spherical markers, the
central position of the sphere can be computed as:

(Xi, Yi, Zi)sphere =

di + r
di

(Xi, Yi, Zi)

(6)

where r represents the radius of the spherical marker.

3.3.2 Tool Deﬁnition

A common approach to tracking objects using retro-
reﬂective markers involves attaching a set of these compo-
nents to a rigid frame with a unique spatial distribution.
This spatial information contributes to estimating the pose
of a particular object and allows distinguishing between
different instances that can be observed simultaneously in
the scene. The procedure to deﬁne the properties of an object
using our framework, from now on referred to as a tool,
requires a series of steps that are described next.

First, a single image frame from the AHAT camera is
used to extract the three-dimensional distribution of the
retro-reﬂective markers that belong to a tool. The spatial
distribution of such markers provides the geometrical prop-
erties of the tool and enables the generation of an initial
conﬁguration of it. This step allows identifying potential
misdetection in subsequent frames by comparing the dis-
tance observed between the detected markers.

The following step uses an optimization method to
calculate the shape of the rigid tool using the N three-
dimensional positions of the markers extracted from every
frame. In this step, we use Ti : {Mi,j(x, y, z)} to represent
the set of markers T , in the ith frame of the total collection,
that deﬁnes the three-dimensional position of the individual
markers M with index j observed in the tool deﬁnition.

The RMSE between one tool in two frames, Tp and Tq, is

used to evaluate the difference as follows:

∆(Tp, Tq) =

(cid:118)
(cid:117)
(cid:117)
(cid:116)

1
N

(cid:88)

0≤j<N

||Mp,j − Mq,j||2
2

(7)

where || · ||2 is the Euclidean distance of the marker’s
position with index j observed in the frames p and q.

5

Fig. 4. The proposed framework combines the reﬂectivity and depth
images of the AHAT camera to detect and extract the three-dimensional
position of passive retro-reﬂective markers. The marker detection stage
facilitates identifying and deﬁning individual tools composed of multiple
spherical markers. This approach allows localizing and recognizing
multiple tools in the scene without modifying the AR-HMD.

Furthermore, to estimate the optimal position of the set
of markers that deﬁne a tool, the mean difference between
the optimized Topt and estimated Ti positions in every
frame are used as the minimization target as follows:

Topt = argmin
T :{Mj }

1
I

(cid:88)

0≤i<I

∆(T, Ti)

(8)

where I represents the total number of collected frames for
the tool deﬁnition procedure.

Lastly, the mean translation of the set of markers is

removed to generate the deﬁnition Tdef : {Mdef,j} using:

Mdef,j = Mopt,j −

1
N

(cid:88)

0≤k<N

Mopt,k

; 0 ≤ j < N (9)

Here, Tdef represents the ﬁnal deﬁnition of the tool with
origin at the geometric center of all the markers in the tool.

This procedure is depicted in Figure 4.

3.3.3 Tool Recognition and Localization
To identify multiple tools, each with shape T l : {M l
j}, from
a single AHAT frame where multiple markers {Pn} are

Detect MarkersReflectivityMarker DetectionReflectivityDepthNoise1781244ThresholdIntensity-based Threshold+3D Points𝑷𝟏𝑷𝟐𝑷𝑵𝑷𝟑Tool DefinitionTool TrackingOptimizeCenterReflectivity3D Points𝑷𝟏𝑷𝟐𝑷𝑵𝑷𝟑3D Points𝑻𝟏𝑻𝟐𝑻𝟑𝑻𝟒xc𝑻𝟏𝑻𝟐𝑻𝟑𝑻𝟒AHAT Coordinates𝑴𝟎𝟏𝑴𝟏𝟏𝑴𝟐𝟏𝑴𝟑𝟏Detect MarkersTool RecognitionLocalizationdetected, we estimate the Euclidean distance between the
markers and tools using the following equations:

LP (n, n + α) = ||Pn − Pn+α||2
for: 0 ≤ n < n + α < N
n − M l

LT l (n, n + α) = ||M l

n+α||2

(10)

(11)

for: 0 ≤ l < L ; 0 ≤ n < n + α < Nl
where n indicates the nth marker in the tool or a scene, and
l represents the lth tool to be detected in the environment.

All subsets {Pn}T l,k of detected markers P whose shape
ﬁts the deﬁnition of the tool T l are found using a depth-
ﬁrst graph searching algorithm; k refers to the kth possible
subset of markers that ﬁts the target tool. To quantitatively
depict the similarity between the possible solution and
target tool deﬁnition, we used a loss function that considers
the corresponding lengths of the tool:

L(T l

def , {Pn}T l,k) =

1
Nl(Nl − 1)

(cid:88)

βT l,k(i, j)

0≤i<j<Nl
(i, j) − L{Pn}T l,k

(12)

(i, j)||

βT l,k(i, j) = ||LT l

def

where Nl refers to the number of markers in the deﬁnition
of the lth tool.

During the searching process, we deﬁne two thresh-
olds to exclude incorrect matches. A ﬁrst threshold tside
identiﬁes mismatches between the corresponding sides of
the tool. A second threshold tshape discerns between the
obtained solution and the tool deﬁnition. These thresholds
are applied as follows:

tside > βT l,k(i, j)
tshape > L(T l

; 0 ≤ i < j < Nl

def , {Pn}T l,k)

(13)

(14)

To select a proper value for these thresholds, the uncertainty
of the three-dimensional position of the detected markers
Pn is considered. As shown in Figure 5c, the standard
error for depth detection, σp, changes as a function of the
depth at which the markers are observed. This value serves
to calculate the maximum standard error for a side using
σside =
2σp. When a 95% probability for detection is de-
sired, the thresholds for a single side can then be calculated
as tside = 2σside. Lastly, the error threshold for the whole
tool is computed using tshape = tside/(cid:112)Nl(Nl − 1).

√

After the possible solutions for every tool have been
extracted, the following step deals with any redundant
information found in the possible solutions. This redundant
information includes using the same retro-reﬂective marker
more than once or using the same set of markers to track
the same tool. When multiple solutions use the same set of
markers, ordered using different index values, to estimate
the pose of the same tool, the solution with the lowest error
L is preserved, and the remaining solutions are removed.
After removing these redundant solutions, the remaining
subsets are sorted according to their error L using their
respective tool deﬁnition T l. In this case, the solution with
the least error is considered the reference. Any remaining
solution that conﬂicts with the reference or aims at detect-
ing the same tool is removed. This procedure is repeated
until all the deﬁned tools are found or no other solution
exists. A ﬁnal step uses an SVD algorithm to calculate the
transformation matrices that map the tool coordinates into
the AHAT camera space.

6

3.3.4 Precision Enhancement

In addition, we proposed using Kalman ﬁlters to enhance
the precision that can be achieved with the proposed frame-
work. On the one hand, using these ﬁlters could contribute
to reducing the tremor observed by the cut-off error from the
AHAT depth data. This source of error is observed because
the AHAT provides an integer value corresponding to the
depth detected in millimeters, limiting the maximum depth
resolution to 1 mm and decreasing the tracking stability
of the passive markers and tools. On the other hand, the
implementation of Kalman ﬁlters could compensate for the
detection error observed when using the AHAT sensor.
However, to justify the use of Kalman ﬁlters, it is required
that the signals involved follow a gauss distribution. This
type of distribution allows the ﬁlters to behave as a best
linear unbiased estimator. Therefore, we conducted an ex-
periment to estimate if the AHAT’s detection error can be
modeled using a gaussian distribution.

Our experimental setup used a glass board with an
anodized aluminum surface and retro-reﬂective markers
located at the corners. A robotic arm was used to move
the glass board along the z-axis of the AHAT camera frame
while keeping the board steady during data collection
(Fig. 5a). The error distribution of the depth data was tested
using 48 different depths, assigned randomly and ranging
from 156 to 971 mm. These lower and upper boundaries
correspond to the limits where the board occupies the whole
image until the depth data is invalid. For every depth
distance, 300 continuous AHAT frames were collected. The
reﬂectivity information of the ﬁrst frame was used to extract
the target detection area. Every depth image contained in
the 300 frames was used to create individual point clouds
merged to produce a ﬁnal set. A plane ﬁtting algorithm used
the merged point cloud to estimate the board’s position.

Finally, the ﬁtting error was estimated using a point-
to-plane function distance. As depicted in Figure 5b, the
error presents a strong gauss-like distribution at different
depths. An Anderson-Darling test was used to test normal
distribution, where the p-value was found to be smaller than
0.0005 for all the 48 distances. In addition, the standard de-
viation increased as a function of the depth. Further details
regarding the error observed in the data at the different
depths are presented in Figure 5c. This relationship can be
modeled using a quadratic polynomial with a coefﬁcient of
determination equal to 0.9807.

These results support the use of a Kalman ﬁlter for the
proposed framework. Thus, we used several independent
ﬁlters for the individual markers for every tracked tool
based on their depth value. After the ﬁltered depth value
is calculated, the three-dimensional position and transform
matrix from the tool to AHAT space are adjusted. In addi-
tion, when the tool’s tracking is lost, every Kalman ﬁlter for
the speciﬁc tool is reinitialized.

4 EXPERIMENTS AND RESULTS
4.1 Localization Error

To assess the tracking accuracy of our algorithm, we con-
ducted a set of experiments in which we compared different
tracking technologies in a controlled environment. Among
these tracking technologies, we used optical cameras to

7

(a) Experimental Setup

(b) Point-to-plane Distance (mm)

(c) Depth (mm)

Fig. 5. (a) Experimental setup used to estimate the noise distribution provided by the built-in depth camera on the HMD. (b) The point-to-plane
distance error distribution at approximately 200mm, 500mm, and 700mm depths. (c) The standard error of depth detection for every pixel versus
depth. The vertical lines depict the mean depth at which the target was detected.

detect ArUco and ChArUco markers in the visible spec-
trum. Although existing works have used the environmental
cameras of the HoloLens for the tracking of these markers
[16], we used an MSIP-RM-PGR-CMU313Y3 camera4 cou-
pled with an M1614-MP2 lens5. This camera provided a
resolution of 1280 × 1024 pixels and enabled the acquisition
of images with better image quality than those acquired
using the built-in cameras of the HoloLens. In addition, we
compared the performance of our algorithm against an NDI
Polaris Spectra6. This system is commonly used in surgical
procedures to track passive retro-reﬂective markers in the
infrared spectrum.

Stability, Repeatability, and Tracking Accuracy

To compare the stability, repeatability, and accuracy of the
optical markers’ pose estimation using different tracking
technologies, we conducted an experiment in which we
translated and rotated the target markers using six dif-
ferent conﬁgurations. These experiments targeted multiple
distances in the personal space at reaching distances. To
precisely control the movements of the markers for the
different sensors, we used a 3-axis linear translation stage
and a rotation platform with an accuracy of 0.01mm and
0.1◦, respectively. The markers were left static for the ﬁrst
experiment, and 100 poses were collected. After data col-
lection, the markers were moved along the x-axis by 1mm,
and another set of 100 poses was recorded. The following
step brought back the markers to their original position and
a total number of 10,000 randomly selected distance values
were calculated using these two data sets. We performed
this procedure 20 times using different initial marker poses
and environmental light conditions to ensure repeatability.
A second experiment followed this procedure but used a
translation of 20mm along the x-axis. The markers were
moved along the z-axis using the same translations for
the third and fourth experiments. Lastly, the markers were
rotated 10 and 50◦ around the world-up vector for the ﬁfth
and sixth experiments.

To ensure that the tracking systems could provide accu-
rate values, we placed the markers at different distances
from the sensors depending on the tracking technology
used. For our proposed method, a tool composed of four
retro-reﬂective spheres with a diameter of 11.5mm was

placed at an approximate distance of 600mm from the
AHAT camera. The same tool was placed at approximately
2000mm for the NDI tracking system. In addition, for the
detection of markers in the visible spectrum, we used a 6×6
ArUco marker and a 3 × 3 ChArUco marker, each with a
side length of 80mm. These optical markers were placed at
800mm from the camera. The experimental setup used for
tracking these targets is shown in Figure 6, and the results
of this experiment are presented in Figure 7.

To evaluate the stability and tracking accuracy, we per-
formed a statistical analysis of the results collected. Con-
sidering the non-normal distribution of the data collected,
we used Kruskal-Wallis tests with α = 0.05 to compare
the results obtained for position and orientation. Posterior
Bonferroni tests were used to reveal signiﬁcant differences
between the tracking technologies. The median and inter-
quartile range (IQR) of all the experiments are summarized
in Table 1 and presented in Figure 8.

Overall, the lowest errors were reported by the NDI
tracking system, followed by the proposed method with
and without the addition of Kalman ﬁlters. When evaluating
the results for 1 mm translations in the x-axis, the Kruskal-
Wallis test revealed a signiﬁcant interaction between the
multiple tracking technologies (χ2(4) = 498650.94, p = 0).
A posterior Bonferroni test revealed statistical signiﬁcance
among all the tracking technologies (p = 0). The NDI
tracking system provided signiﬁcantly higher accuracy than
the other tracking technologies, followed by the proposed
method with and without Kalman ﬁlters and the ChArUco
and ArUco markers (see Figure 8a). Regarding the 20 mm
translations in the x-axis, the Kruskal-Wallis test revealed a
signiﬁcant interaction between the multiple tracking tech-
nologies (χ2(4) = 352721.55, p = 0). A posterior Bonferroni
test showed that the NDI tracking system and the ArUco
markers provided signiﬁcantly higher accuracy than the
ChArUco and the proposed method with and without the

4. FLIR Integrated Imaging Solutions, Inc.
5. CBC AMERICA LLC.
6. Northern Digital Incorporated

Fig. 6. Experimental setup for tracking accuracy using ArUco and
ChArUco (left) and retro-reﬂective (right) markers.

𝑍+𝑍−Target AreaProbabilityZ = 200 mm.Z = 500 mm.Z = 700 mm.Standard Error (mm)Rotation PlatformArUco MarkerRGBCameraRetro-reflectiveToolMoving PlatformHoloLens 2NDI TrackingSystem8

(a) 1 mm translation x-axis

(b) 1 mm translation z-axis

(c) 10◦ rotation

(d) 20 mm translation x-axis

(e) 20 mm translation z-axis

(f) 50◦ rotation

Fig. 7. Experiment results of tool tracking accuracy assessment of our infrared depth camera and Kalman ﬁlter-based tracking method, compared
with commercial IR and visible light marker tracking methods. (a),(d) Distribution of detected moving distances when moving optical platform 1 and
20 mm along x-axis over 20 repetitive tests. (b),(e) Distribution of detected moving distances when moving the optical platform 1 and 20 mm along
the z-axis. (c),(f) Distribution of detected rotation angle when rotating the optical platform 10 and 50 degrees.

addition of the Kalman ﬁlters (p = 0). However, the results
presented in Figure 8b show that the precision provided by
the ArUco markers is lower than for all the other tracking
technologies.

Regarding position accuracy on the z-axis, a Kruskal-
Wallis test revealed a signiﬁcant interaction between the
tracking technologies when the markers were translated 1
mm (χ2(4) = 60421.54, p = 0). A posterior Bonferroni
test revealed statistical signiﬁcance among all the tracking
technologies (p = 0). The NDI tracking system proved to
be more accurate than the other compared technologies.
The proposed method with and without Kalman ﬁlters
followed the NDI system, while the ChArUco and ArUco
reported the worst scores (Figure 8c). When translating
the markers 20 mm, our Kruskal-Wallis test revealed a
signiﬁcant interaction between the multiple technologies
(χ2(4) = 809777.32, p = 0). The posterior Bonferroni test
revealed the same behavior then the one observed for the 1
mm translations in this axis (see Figure 8d).

Furthermore, results from the Kruskal-Wallis tests for the
orientation errors revealed a signiﬁcant interaction between
the tracking technologies when rotating the target by 10
(χ2(4) = 193849.19, p = 0) and 50 (χ2(4) = 377705.61, p =
0) degrees. In contrast to the translation results, both ver-
sions of the proposed method reported lower median errors
than the NDI tracking system and the ArUco and ChArUco
markers. However, the precision reported by the NDI track-
ing system is higher than for the other tracking technologies
(see Figures 8e and 8f).

Workspace Deﬁnition

In addition to the tracking accuracy, a further experiment
investigated the role that the FoV plays over the proposed
method’s accuracy. To achieve more considerable displace-
ment capabilities than the one used in the previous ex-
periment, we moved our tracking tool along the x- and
z-axis using a different linear stage. For this portion of
the experiment, the NDI tracking system was used as the
ground truth to evaluate the tracking accuracy. The passive
tool was incrementally moved along the linear stage. After
the movement of the linear stage was completed, the data
corresponding to the tool’s pose was collected using the NDI
tracking system and the AHAT camera of the HoloLens. A
total number of 50 values were collected for every position.
This process was repeated from the nearest to the furthest
detection distances for the z-axis and from the center to the
lateral margins of the x-axis that could be detected using
the AHAT camera. The mean values of the data at every
position were considered the real pose values and used to
estimate the moving direction of the tool. The estimated
displacement along the moving platform can then be calcu-
lated by projecting the tracked tool’s position to the moving
direction and comparing it with its initial position.

The detection error observed during tool displacement
using our system is shown in Figure 9 as a function of
the observed depth. Such results show that our system can
steadily trace the passive tool within depths of 250 and
750mm (Figure 9a) and within radial distances of 509mm
when the tool is placed at a maximum distance of 510mm

TABLE 1
Median and IQR accuracy errors reported by the different tracking technologies.

9

Tracking
Technology

ArUco
ChArUco
NDI
Ours
Ours + Kalman

Translation (x-axis)

Translation (z-axis)

1 mm

20 mm

1 mm

20 mm

Rotation

10◦

50◦

Median

IQR

Median

IQR

Median

IQR

Median

IQR

Median

IQR

Median

IQR

1.0566
0.3047
-0.0007
0.1223
0.0764

2.6093
0.7078
0.0172
0.2226
0.1332

-0.020
0.047
-0.028
-0.089
-0.092

0.267
0.195
0.014
0.083
0.063

1.0544
0.4215
0.008
0.058
-0.010

3.0490
1.633
0.027
0.661
0.289

3.032
-0.022
-0.067
0.464
0.424

2.001
1.553
0.026
0.714
0.320

-0.461
0.386
0.294
-0.194
-0.231

1.717
11.941
0.151
0.917
0.583

-0.713
0.964
0.922
0.804
0.807

1.339
0.680
0.075
0.728
0.395

(a) 1 mm translation x-axis

(b) 20 mm translation x-axis

(c) 1 mm translation z-axis

(d) 20 mm translation z-axis

(e) 10◦ rotation

(f) 50◦ rotation

Fig. 8. Experiment results of tool tracking accuracy assessment of our
infrared depth camera and Kalman ﬁlter-based tracking method, com-
paring with commercial IR tracking method and visible light marker track-
ing methods. (a),(c) Detected moving distances when moving optical
platform 1 mm along x- and z-axis. (b),(d) Detected moving distances
when moving optical platform 20 mm along x- and z-axis. (e),(f) Detected
rotation angle when rotating the platform 10 and 50 degrees.

(Figure 9b). Using F oV = 2 · arctan(xmax/d), it can be
shown that these values are equivalent to an FoV of 89.9◦.
Interestingly, the detection error observed for the z-axis
depicts a different behavior before and after 400mm. In
addition, the absolute difference between the error observed
at 400 and 800mm results in a moving distance error of
2.5mm, or a distortion of 0.625%. For the radial direction, a
smaller moving distance error can be observed. The absolute

(a)

(b)

tracking accuracy of

Fig. 9. The global
the AHAT tracking method
compared to NDI Polaris Spectra. (a) Moving distance detection error
along the z-axis. (b) Moving distance detection error along the x-axis at
500mm depth.

difference observed between 0 and 500mm translations
depicts an error of ≈ 0.75mm, equivalent to a distortion
of 0.15%.

4.2 Runtime and Latency

To investigate the runtime capabilities of our system, we
placed different quantities of passive tools within the FoV
of the AHAT camera. The sensor data collected by the
AHAT was recorded and used for the ofﬂine runtime test.
In this way, the frame rate of the algorithm during the test
would not be limited by the sensor acquisition frequency.
In addition, we collected and evaluated the runtime for
detecting the ArUco and ChAruCo markers using an RGB
camera with a resolution of 1280 × 1024 pixels. We used the
average runtime of 10,000 frames on a Xeon(R) E5-2623 v3
CPU for this portion of the experiments.

This experiment revealed a runtime of 28.4 ms (35.2
Hz) for the detection of the ArUco and 30.14 ms (33.2 Hz)
for the ChArUco makers. The runtime results when using
the AHAT camera proved to be inﬂuenced by both the
number of retro-reﬂective markers detected in the scene
and the number of loaded tools for detection. These results
are depicted in Figure 10a. When only one passive tool
is expected to be detected, the runtime of our system is
5.80 ms (172.37 Hz). However, when the passive markers
corresponding to ﬁve different tools are detected and their
ﬁve corresponding deﬁnitions are loaded, the runtime is
19.48 ms (51.34 Hz).

The latency of our system was then compared using the
NDI tracking system as a reference. For this experiment, we
moved back and forth a passive tool attached to a linear
stage using a period Tmov of ≈ 15 seconds. The localization
data from the NDI and our system were collected simul-
taneously. The detected moving distance along the linear

10

Fig. 11. The spatial relationship between different components is re-
quired to provide surgical navigation using the proposed framework.

proposed framework, and the HMD’s self-localization data
T W

H as follows:

I = T W
T W

H · T H

A · T A
SP

· T SP
I

(17)

Likewise, the pose of the surgical drill in world coordinates
can be acquired using the tool pose T A

D = T W
T W

H · T H

A · T A
SD

SD utilizing:
· T SD
D

(18)

This procedure requires calculating the transformation
matrix from the AHAT camera to HoloLens view space
(T H
A ). For this purpose, we designed a 3D-printed structure
composed of a retro-reﬂective tool, SM , and four BBs to
register the AHAT and view spaces (see Figure 1a). To
calculate the position of the real BBs in the tracker space,
we performed a pivot calibration. The spatial information
extracted from the real BBs during the pivot calibration
served to generate a set of virtual replicas that were aligned
to their real counterparts. A computer keyboard enabled
the user to control the 6 DoF of the virtual objects in the
HoloLens world space (T W
M ). After proper alignment of the
real and virtual objects, the spatial relationship between the
AHAT camera and the view space (T H
A ) was computed as
follows:

A = (T W
T H

H )−1 · T W

M · (T A
SM

)−1

(19)

5.1 Surgical Navigation

To further evaluate the performance of our surgical naviga-
tion system, we designed a total of three phantom models
to replicate traditional k-wire placement procedures (Fig-
ure 1b). These models were composed of a 3D printed
base containing six cylindrical shapes with conical tips
covered using silica gel (see Figure 1b). After constructing
the phantom models, we collected CT volumes from them
to generate their virtual replicas. An experienced surgeon
later used the CT volumes to plan multiple trajectory paths
aiming at the tips of the individual cones. A total of two
different drilling paths were planned for every cone, leading
to a total of 12 trajectories per phantom model. These trajec-
tories deﬁned the optimal path in which a k-wire should
be inserted and were overlayed on the phantom using the
HoloLens. In addition to the trajectories, virtual indicators

(a)

(b)

Fig. 10. Frame rate and latency results of
the proposed algorithm
using the AHAT camera. (a) Tool detection frame rate under different
conditions. Every series depicts several passive tools containing four
retro-reﬂective markers detected using the AHAT camera. Different tools
are loaded to evaluate the detection frame rate when multiple markers
are observed. If more tools are deﬁned than the number of markers
observed, the system will
is not de-
tected. (b) Movement detection of synchronously acquired localization
data from the NDI tracking system and the proposed method. The target
is kept in motion back and forth using a linear stage.

indicate that the respective tool

stage for both systems can be expressed as N (t) and H(t)
with t ∈ [0, T ) for the NDI and HoloLens, respectively
(see Figure 10b). The moving signal of the HoloLens is ﬁrst
adjusted to match the amplitude of the NDI system using:

H (cid:48)(t) = H(t) + δ(N (t))−δ(H(t))

2

δ(x) = max(x) + min(x)

(15)

The following step computed the delay between our

tracking system and the NDI system using:

δH = argmin
0<δ<Tmov

(cid:90) T

δ

1
T − δ

(N (t) − H (cid:48)(t − δ))2dt

(16)

where δH depicts the time difference in seconds between the
signals H (cid:48)(t) and N (t).

This experiment showed that the proposed method
presents a time delay of 103.23 ms compared to the NDI
tracking system.

5 USE CASE

To highlight the relevance of the proposed framework for
surgical applications, we introduced a use case in which we
provided visual navigation using AR HMDs in orthopedic
surgical procedures for the placement of pedicle screws.
Two different tools containing retro-reﬂective markers were
attached to a phantom spine model (SP ) and a surgical drill
(SD) to enable visual navigation. We used an O-arm imaging
system and acquired a pre-operative computed tomography
(CT) scan from the spine model. This data was used to
plan the trajectories that the surgeons must follow during
the placement of the pedicle screws. In addition, we added
four metallic balls (BBs) to the phantom spine model that
were visible in the CT and by direct observation. This action
allowed us to determine the spatial relationship between the
tool attached to the spine model (SP ) and the image space
(I). The spatial relationship between the different coordinate
systems involved in this use case is depicted in Figure 11.

After successful tracking of the retro-reﬂective tools,
the pose of the phantom model in world coordinates can
be computed using the tool pose T A
SP , estimated with the

𝑇𝐼𝑆𝑃𝑇𝑆𝑃𝐴𝑇𝑆𝐷𝐴𝑇𝐴𝐻𝑇𝐻𝑊𝑇𝐼𝑊World𝑊Image𝐼Patient𝑃𝑇𝐷𝑆𝐷Drill𝐷Hololens𝐻AHAT𝐴𝑆𝐷𝑆𝑃in the form of concentric circles were presented over the
surface of the phantom. The virtual prompts indicated the
entry point that would lead to the optimal trajectory in
the physical model (Figure 1c). A total number of three
participants,
including two experienced surgeons and a
biomedical researcher with experience in mixed reality, took
part in this portion of the study. Among the surgeons,
one reported comprehensive experience using mixed reality,
while the other was unfamiliar with this technology.

Every participant performed an eye-tracking calibration
procedure before the start of the experiment. After calibra-
tion, participants were asked to complete the registration
procedure described in Equation 19 and depicted in Fig-
ure 1a. Once the virtual and real models were registered and
the spatial relationship between the AHAT camera and the
view space was computed, the pre-planned trajectories were
presented to the participants using the AR HMD. Before
collecting data for evaluation, every participant was allowed
to perform multiple k-wire insertions in one of the three
models to get familiar with the system. After this step, the
remaining models containing 24 drilling paths were used
for formal testing. Among these trajectories, three were not
considered for evaluation because the construction of the
model and the retro-reﬂective markers’ placement interfered
with the planned trajectory’s achievement. Therefore, we
considered 21 drilling trajectories per participant for eval-
uation. To measure the accuracy achieved by the study
participants, a registration step between the pre- and post-
operative imaging was performed. This step involved the
acquisition of CT volumes from the models and allowed
comparing the differences between the planned and real
trajectories. The translation and angular errors between the
optimal and real trajectories were used as metrics for the
performance evaluation.

To further evaluate the results obtained in this portion of
the study, we performed a statistical analysis using the data
collected for the translation and angular errors. The results
obtained for position and orientation were compared using
ANOVA tests with α = 0.05 . Posterior Tukey-Kramer tests
revealed signiﬁcant differences between the participants’
performance.

Results from this experiment, summarized in Figure 12,
showed a signiﬁcant interaction between the accuracy in
position achieved by the users. Our ANOVA test revealed
signiﬁcant interaction between the participants for align-
ment (F (2) = 4.65, p = 0.0133). The biomedical researcher
achieved better alignment scores (M n = 2.32mm, SD =
1.40, p = 0.0119) when compared to the experienced sur-
geon (M n = 3.64mm, SD = 1.47). However, no signiﬁcant
differences were found between the biomedical researcher
and the surgeon unfamiliar with AR-based navigation sys-
tems (M n = 3.25mm, SD = 1.45) or between the surgeons.
In addition, an ANOVA test for the rotation scores did
not reveal a signiﬁcant interaction between the participants
(F (2) = 2.96, p = 0.0593). However, as depicted in Fig-
ure 12b, the experienced surgeon achieved better align-
ment (M n = 3.23◦, SD = 1.95) than the other surgeon
(M n = 4.95◦, SD = 2.66) and the biomedical researcher
(M n = 4.36◦, SD = 2.27).

These results show no signiﬁcant differences in transla-
tion or orientation between the surgeons when using the

11

(a)

(b)

Fig. 12. (a) Translation and (b) orientation errors for k-wire insertion
using AR visual guidance and the proposed algorithm for tool tracking.
P1: Surgeon with reported experience using AR-guidance systems. P2:
Surgeon without reported experience using AR. P3: Researcher with
reported experience using AR.

system. This is particularly interesting as the surgeon with
experience using AR systems also reported an approximate
experience of 10 years working with navigation systems for
k-wire injection procedures, while the surgeon that did not
report experience with AR-based navigation systems also
reported an approximate experience of 1 year in the ﬁeld.
The scores reported by the participants of the study are
comparable to the accuracy values reported by other AR-
based navigation systems for pedicle screw placement [17],
[26], [27].

Although this experiment shows that

the proposed
methods provide comparable tracking accuracy and stabil-
ity to successfully performing the insertion of the k-wires,
the latency observed between the data processing using the
workstation and the visualization of the optimal trajectories
using the HMD seems to contribute to the observation of
uncertainty during the alignment and localization of the
targets. Furthermore, the elastic and smooth surface of
the silicone gel used to create the test models replicates
the challenges brought by percutaneous surgeries where
few landmarks are visible, and the working surface is soft
and deformable. These properties increase the difﬁculty
of performing the drilling task and the accurate insertion
of the k-wire. However, the scores reported by the users
demonstrate the performance of the proposed system when
these challenges exist.

6 DISCUSSION
This work introduces a framework to track retro-reﬂective
markers using the built-in cameras of commercially avail-
able AR HMDs. Results regarding the error distribution of
depth detection demonstrate that the error of the AHAT
depth data can be modeled using a normal distribution.
This normal distribution increases its standard error as a
function of the detected depth (see Figure 5). Results from
the test of tracking accuracy demonstrate that the integra-
tion of Kalman ﬁlters contributes to the reduction of the
IQR, as depicted in Table 1. Therefore, they contribute to
improve the precision of the methods proposed. As a result
of the integration of the Kalman ﬁlters, this framework
showed to be capable of achieving a tracking accuracy of
0.09 ± 0.06 mm on lateral translation, 0.42 ± 0.32 mm on

Translation error (mm)Orientation error (°)longitudinal translation, and 0.80±0.39◦ on rotation around
the vertical axis (see Figure 7 and Table 1). These results are
more accurate and precise than those achievable using tradi-
tional feature-based tracking algorithms such as the ArUco
and ChArUco markers. Although it can be expected that
feature-based tracking algorithms provide lower accuracy
and precision, they are frequently used to enable the optical
tracking of tools in AR applications. Therefore, they were
considered during our study. Of note, our statistical analysis
did not reveal statistical signiﬁcance when comparing the
tracking results of the NDI tracking system and the ArUco
markers for 20 mm movements on the x-axis. However,
when observing the distribution of the tracking results of
the ArUco markers, the precision provided by this type
of marker is worse than the one observed when using the
NDI tracking system and the two variants of the proposed
framework.

In terms of applicability, the use case for the placement
of k-wires during orthopedic interventions demonstrates its
potential use in medical environments during the perfor-
mance of surgical procedures [17], [26], [27]. Although the
biomedical researcher reported signiﬁcantly more accurate
scores for translation during the placement of the k-wires,
no signiﬁcant differences in translation and orientation were
found between the surgeons of the use case. These results
may suggest that using this technology can help novice
users reach similar results to those achieved by experienced
surgeons, even with little time to get familiar with the sys-
tem. However, further studies would need to be conducted
in this regard.

In addition, the proposed framework was demonstrated
to be capable of tracking individual tools containing up to
4 retro-reﬂective markers at a frame rate of 172 Hz. This
tracking frame rate remains over 50 Hz even when ﬁve dif-
ferent tools containing multiple retro-reﬂective markers are
detected simultaneously. These reported speeds are faster
than that of traditional feature-based tracking techniques
such as ArUco (35 Hz) and ChArUco (33.2 Hz) markers. This
frame rate is even higher than the frame rate of the AHAT
camera (45 Hz). Therefore, allowing for tracking multiple
targets before having a new sensor frame available.

In contrast to existing works, the proposed framework
only uses the ToF camera of the HoloLens 2 without the ad-
dition of external components. Moreover, when compared
to methods that use the grayscale environmental cameras
of the headset for tool tracking, several beneﬁts support
using the depth sensor when hand distance tracking is
needed. In this case, the tracking distance from 250 mm
to 750 mm provided by the AHAT sensor nicely satisﬁes
the speciﬁc application’s needs. At the same time, the LF
and RF cameras have short baseline distances and focal
lengths, restricting their performance at near distances [13].
In addition, using the AHAT camera provides an FoV of
≈ 90◦, while the overlap of the LF and RF cameras is smaller
than 60◦ in width. Moreover, the proposed algorithm does
not require the addition of external infrared light to enhance
the visibility of the passive markers. Therefore, mitigating
the possibility of interfering with the functionalities of the
HMD that rely on the use of this type of light, including
environmental reconstruction and hand tracking.

12

Limitations

The utility of the HMD’s depth camera for tracking and
detecting retro-reﬂective markers presented in this work
led to promising results in terms of precision and stability.
However, certain limitations associated with the proposed
framework exist. In this regard, compared to commercially
available tracking systems, the accumulated error observed
using our method increases as a function of the depth in
which the tools are tracked. However, this accumulated
error remains less than 1 mm at distances between 300
to 600 mm. In the future, this limitation caused by the
precision achieved by the built-in depth sensor of the HMD
could be addressed by modeling the error and designing
an error compensation algorithm. Another limitation of the
proposed methods is the latency observed when presenting
the virtual content to the user after the achievement of tool
tracking. This issue currently represents the most signiﬁcant
challenge in providing visual information that could enable
stable and reliable navigation. As the proposed framework
relies on the quality of the network to transfer the data
collected, the incorrect synchronization between the tool
tracking result T A
SP and the HMD’s self-localization data
T W
H could lead to the observation of inconsistencies in
the content displayed. These differences would be more
noticeable when using wireless networks, where the sensor
data transfer delay is longer and the frame rate is lower
(< 12f ps). More importantly, the results presented in this
work model the properties of the AR HMD used during
the experiments. Additional studies must be conducted to
investigate if the results are consistent for multiple devices
of the same type.

7 CONCLUSION
This paper proposes a framework that uses the built-in
cameras of commercially available AR headsets to enable
the accurate tracking of passive retro-reﬂective markers.
Such a framework enables tracking these markers without
integrating any additional electronics into the headset and
is capable of simultaneous tracking of multiple tools. The
proposed method showed a tracking accuracy of approx-
imately 0.1 mm for translations on the lateral axis and
approximately 0.5 mm for translations on the depth axis.
The results also show that the proposed method can track
retro-reﬂective markers with an accuracy of less than 1◦ for
rotations. Finally, we demonstrated the early feasibility of
the proposed framework for k-wire insertion as performed
in orthopedic procedures.

REFERENCES

[1] C. Kamphuis, E. Barsom, M. Schijven, and N. Christoph, “Aug-
mented reality in medical education?” Perspectives on medical
education, vol. 3, no. 4, pp. 300–311, 2014.

[2] T. M. Peters, C. A. Linte, Z. Yaniv, and J. Williams, Mixed and

augmented reality in medicine. CRC Press, 2018.

[3] C. A. Campisi, E. H. Li, D. E. Jimenez, and R. L. Milanaik, “Aug-
mented reality in medical education and training: from physicians
to patients,” in Augmented Reality in Education. Springer, 2020,
pp. 111–138.

[4] A. Mehrfard,

J. Fotouhi, G. Taylor, T. Forster, M. Armand,
N. Navab, and B. Fuerst, “Virtual reality technologies for clin-
ical education: evaluation metrics and comparative analysis,”
Computer Methods in Biomechanics and Biomedical Engineering:
Imaging & Visualization, vol. 9, no. 3, pp. 233–242, 2021.

[5] B. Fida, F. Cutolo, G. di Franco, M. Ferrari, and V. Ferrari, “Aug-
mented reality in open surgery,” Updates in surgery, vol. 70, no. 3,
pp. 389–400, 2018.

[6] G. Deib, A. Johnson, M. Unberath, K. Yu, S. Andress, L. Qian,
G. Osgood, N. Navab, F. Hui, and P. Gailloud, “Image guided
percutaneous spine procedures using an optical see-through head
mounted display: proof of concept and rationale,” Journal of
neurointerventional surgery, vol. 10, no. 12, pp. 1187–1191, 2018.

[7] L. Jud, J. Fotouhi, O. Andronic, A. Aichmair, G. Osgood, N. Navab,
and M. Farshad, “Applicability of augmented reality in orthopedic
surgery–a systematic review,” BMC musculoskeletal disorders,
vol. 21, no. 1, pp. 1–13, 2020.

[8] F. A. Casari, N. Navab, L. A. Hruby, P. Kriechling, R. Naka-
mura, R. Tori, F. de Lourdes dos Santos Nunes, M. C. Queiroz,
P. F ¨urnstahl, and M. Farshad, “Augmented reality in orthopedic
surgery is emerging from proof of concept towards clinical studies:
a literature review explaining the technology and current state of
the art,” Current Reviews in Musculoskeletal Medicine, vol. 14,
no. 2, pp. 192–203, 2021.

[9] R. Rahman, M. E. Wood, L. Qian, C. L. Price, A. A. Johnson,
and G. M. Osgood, “Head-mounted display use in surgery: a
systematic review,” Surgical innovation, vol. 27, no. 1, pp. 88–100,
2020.

[10] J. Fotouhi, A. Mehrfard, T. Song, A. Johnson, G. Osgood, M. Un-
berath, M. Armand, and N. Navab, “Development and pre-clinical
analysis of spatiotemporal-aware augmented reality in orthopedic
interventions,” IEEE transactions on medical imaging, vol. 40,
no. 2, pp. 765–778, 2020.

[11] G. A. Koulieris, K. Aks¸it, M. Stengel, R. K. Mantiuk, K. Mania,
and C. Richardt, “Near-eye display and tracking technologies for
virtual and augmented reality,” in Computer Graphics Forum,
vol. 38, no. 2. Wiley Online Library, 2019, pp. 493–519.

[12] C. Kunz, P. Maurer, F. Kees, P. Henrich, C. Marzi, M. Hlav´aˇc,
M. Schneider, and F. Mathis-Ullrich, “Infrared marker track-
ing with the hololens for neurosurgical interventions,” Current
Directions in Biomedical Engineering, vol. 6, no. 1, 2020.

[13] C. Gsaxner, J. Li, A. Pepe, D. Schmalstieg, and J. Egger, “Inside-out
instrument tracking for surgical navigation in augmented reality,”
in Proceedings of the 27th ACM Symposium on Virtual Reality
Software and Technology, 2021, pp. 1–11.

[14] J. W. Yoon, R. E. Chen, P. K. Han, P. Si, W. D. Freeman, and
S. M. Pirris, “Technical feasibility and safety of an intraopera-
tive head-up display device during spine instrumentation,” The
International Journal of Medical Robotics and Computer Assisted
Surgery, vol. 13, no. 3, p. e1770, 2017.

[15] C. A. Molina, N. Theodore, A. K. Ahmed, E. M. Westbroek,
Y. Mirovsky, R. Harel, M. Khan, T. Witham, D. M. Sciubba et al.,
“Augmented reality–assisted pedicle screw insertion: a cadaveric
proof-of-concept study,” Journal of Neurosurgery: Spine, vol. 31,
no. 1, pp. 139–146, 2019.

[16] J. T. Gibby, S. A. Swenson, S. Cvetko, R. Rao, and R. Javan,
“Head-mounted display augmented reality to guide pedicle screw
placement utilizing computed tomography,” International journal
of computer assisted radiology and surgery, vol. 14, no. 3, pp.
525–535, 2019.

[17] F. Liebmann, S. Roner, M. von Atzigen, D. Scaramuzza, R. Sut-
ter, J. Snedeker, M. Farshad, and P. F ¨urnstahl, “Pedicle screw
navigation using surface digitization on the microsoft hololens,”
International journal of computer assisted radiology and surgery,
vol. 14, no. 7, pp. 1157–1165, 2019.

[18] A. F. Vieh ¨ofer, S. H. Wirth, S. M. Zimmermann, L. Jaberg,
C. Dennler, P. F ¨urnstahl, and M. Farshad, “Augmented re-
ality guided osteotomy in hallux valgus correction,” BMC
musculoskeletal disorders, vol. 21, no. 1, pp. 1–6, 2020.

[19] H. Liu, E. Auvinet, J. Giles, and F. Rodriguez y Baena, “Aug-
mented reality based navigation for computer assisted hip
resurfacing: a proof of concept study,” Annals of biomedical
engineering, vol. 46, no. 10, pp. 1595–1605, 2018.

[20] J. Fotouhi, C. P. Alexander, M. Unberath, G. Taylor, S. C. Lee,
B. Fuerst, A. Johnson, G. M. Osgood, R. H. Taylor, H. Khanuja
et al., “Plan in 2-d, execute in 3-d: an augmented reality solution
for cup placement in total hip arthroplasty,” Journal of Medical
Imaging, vol. 5, no. 2, p. 021205, 2018.

[21] G. Ortega, A. Wolff, M. Baumgaertner, and D. Kendoff, “Useful-
ness of a head mounted monitor device for viewing intraoper-
ative ﬂuoroscopy during orthopaedic procedures,” Archives of

13

orthopaedic and trauma surgery, vol. 128, no. 10, pp. 1123–1126,
2008.

[22] J. Fotouhi, M. Unberath, T. Song, W. Gu, A. Johnson, G. Osgood,
M. Armand, and N. Navab, “Interactive ﬂying frustums (iffs):
spatially aware surgical data visualization,” International journal
of computer assisted radiology and surgery, vol. 14, no. 6, pp.
913–922, 2019.

[23] S. Andress, A. Johnson, M. Unberath, A. F. Winkler, K. Yu,
J. Fotouhi, S. Weidert, G. M. Osgood, and N. Navab, “On-the-
ﬂy augmented reality for orthopedic surgery using a multimodal
ﬁducial,” Journal of Medical Imaging, vol. 5, no. 2, p. 021209, 2018.
[24] D. Ungureanu, F. Bogo, S. Galliani, P. Sama, C. Meekhof,
J. St ¨uhmer, T. J. Cashman, B. Tekin, J. L. Sch ¨onberger, P. Olszta
et al., “Hololens 2 research mode as a tool for computer vision
research,” arXiv preprint arXiv:2008.11239, 2020.

[25] M. S. Labini, C. Gsaxner, A. Pepe,

J. Egger,
and V. Bevilacqua, “Depth-awareness in a system for mixed-
reality aided surgical procedures,” in International conference on
intelligent computing. Springer, 2019, pp. 716–726.

J. Wallner,

[26] F. M ¨uller, S. Roner, F. Liebmann, J. M. Spirig, P. F ¨urnstahl, and
M. Farshad, “Augmented reality navigation for spinal pedicle
screw instrumentation using intraoperative 3d imaging,” The
Spine Journal, vol. 20, no. 4, pp. 621–628, 2020.

[27] J. M. Spirig, S. Roner, F. Liebmann, P. F ¨urnstahl, and M. Farshad,
“Augmented reality-navigated pedicle screw placement: a cadav-
eric pilot study,” European Spine Journal, vol. 30, no. 12, pp. 3731–
3737, 2021.

Alejandro Martin-Gomez is a Postdoctoral Fel-
low at the Laboratory for Computational Sensing
and Robotics, Johns Hopkins University. Prior to
his Ph.D. at the Chair for Computer Aided Medi-
cal Procedures and Augmented Reality (CAMP),
he earned his M.Sc. degree in Electronic Engi-
neering from the Autonomous University of San
Luis Potosi, Mexico, and B.Sc. degree in Elec-
tronic Engineering from the Technical Institute
of Aguascalientes, Mexico. His main research
interests include the improvement of visual per-
ception for augmented reality and its applications in interventional
medicine.

is an undergraduate student

in
Haowei Li
the Department of Biomedical Engineering, Ts-
inghua University. He has been a member of
Prof. Guangzhi Wang’s lab since junior years.
interests include computer aided
His current
surgery and augmented reality.

Tianyu Song is a Ph.D. candidate of Computer
Science at the Chair for Computer Aided Medical
Procedures and Augmented Reality (CAMP) of
the Technical University of Munich. He received
the B.Sc. degree in Theoretical and Applied Me-
chanics from Sun Yat-sen University, China and
B.Sc. degree in Mechanical Engineering from
Purdue University, USA in 2017. He earned his
M.Sc. in Robotics at Johns Hopkins University,
USA in 2019. After graduation, he worked at the
Applied Research team at Verb Surgical/ John-
son&Johnson, USA for a year. His current interests include augmented
reality and computer aided surgery.

Sheng Yang is a PhD candidate at Department
of Biomedical Engineering, Tsinghua University.
Also a member of Guangzhi Wang’s Lab, for-
tunately advised by Prof. Guangzhi Wang, with
current research focus on Medical Robot, espe-
cially on Compute-Aided Image Guided Naviga-
tion System. He received the B.Sc degree and
the M.Sc degree in Biomedical Engineering from
Southeast University in 2016 and in 2019 re-
spectively. His current interests include medical
robot and medical image processing.

Guangzhi Wang , PhD, received his B.E. and
M.S. Degrees in mechanical engineering in 1984
and 1987 respectively, and received his PhD
Degree in Biomedical Engineering in 2003, all
from Tsinghua University, Beijing, China. Since
1987, he has been with the Biomedical Engi-
neering division at Tsinghua University. Since
2004, He is a tenured full Professor at Depart-
ment of Biomedical Engineering, Tsinghua Uni-
versity. He is the author of hundreds of peer-
reviewed scientiﬁc papers and is the inventor of
30 granted Chinese and PCT patents. Since 2015 Professor Guangzhi
Wang was elected as the vice president of Chinese Society of Biomed-
ical Engineering, and since 2018, he was elected as vice president
of Chinese Association of Medical Imaging Technology. His current
research interest includes biomedical image processing, image based
surgical planning and computer aided surgery.

Hui Ding is a senior engineer and laboratory
director in the Department of Biomedical Engi-
neering, Tsinghua University. She was awarded
the M.Sc degree in computer science from
the University of science and technology of
China. She has been engaged in medical image
processing, computer aided minimally invasive
treatment planning, human motion modeling and
analysis for rehabilitation. At present, her re-
search work includes multimode image guided
minimally invasive orthopedic and neurosurgery,
ultrasonic fusion imaging. She is the inventor of more than 10 Chinese
and PCT patents.

14

Nassir Navab is a Full Professor and Director
of the Laboratory for Computer-Aided Medical
Procedures (CAMP) at the Technical University
of Munich (TUM) and Adjunct Professor at Lab-
oratory for Computational Sensing and Robotics
(LCSR) at Johns Hopkins University (JHU). He
is a fellow of MICCAI society and received its
prestigious Enduring Impact Award in 2021. He
was also the recepient of 10 years lasting impact
award of IEEE ISMAR in 2015, SMIT Society
Technology award in 2010 and Siemens Inventor
of the year award in 2001. He completed his PhD at INRIA and Uni-
versity of Paris XI, France, in January 1993, before enjoying two years
of post-doctoral fellowship at MIT Media Laboratory and nine years of
expeirence at Siemens Corporate Research in Princeton, USA. He has
acted as a member of the board of directors of the MICCAI Society,
2007-2012 and 2014-2017, and serves on the Steering committee of
the IEEE Symposium on Mixed and Augmented Reality (ISMAR) and
Information Processing in Computer-Assisted Interventions (IPCAI). He
is the author of hundreds of peer-reviewed scientiﬁc papers, with more
than 52,000 citations and an h-index of 101 as of April, 2022. He is the
author of more than thirty awarded papers including 11 at MICCAI, 5 at
IPCAI, 2 at IPMI and 3 at IEEE ISMAR. He is the inventor of 51 granted
US patents and more than 60 International ones. His current research
interests include medical augmented reality, computer-assisted surgery,
medical robotics, computer vision and machine learning.

Zhe Zhao is an attending surgeon of the Depart-
ment of Orthopeadics, Beijing Tsinghua Chang-
gung Hospital. Associate professor of School of
Clinical Medicine, Tsinghua University. His re-
search interest covers orthopaedic trauma, com-
puter assisted surgery and orthopeadic implant
development.

Mehran Armand received the Ph.D. degree in
mechanical engineering and kinesiology from
the University of Waterloo, Waterloo, ON,
Canada,
in 1998. He is currently a Profes-
sor of Orthopaedic Surgery, Mechanical En-
gineering, and Computer Science with Johns
Hopkins University (JHU), Baltimore, MD, USA,
and a Principal Scientist with the JHU Applied
Physics Laboratory (JHU/APL). Prior to joining
JHU/APL in 2000, he completed postdoctoral
fellowships with the JHU Orthopaedic Surgery
and Otolaryngology-Head and Neck Surgery. He currently directs the
Laboratory for Biomechanical- and Image-Guided Surgical Systems,
JHU Whiting School of Engineering. He also directs the AVICENNA
Laboratory for advancing surgical technologies, Johns Hopkins Bayview
Medical Center. His laboratory encompasses research in continuum
manipulators, biomechanics, medical image analysis, and augmented
reality for translation to clinical applications of integrated surgical sys-
tems in the areas of orthopaedic, ENT, and craniofacial reconstructive
surgery.

