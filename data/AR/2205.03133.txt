BDIS: Bayesian Dense Inverse Searching Method
for Real-Time Stereo Surgical Image Matching

Jingwei Song, Qiuchen Zhu, Jianyu Lin, and Maani Ghaffari

1

2
2
0
2

y
a
M
6

]

V
C
.
s
c
[

1
v
3
3
1
3
0
.
5
0
2
2
:
v
i
X
r
a

Abstract—In stereoscope-based Minimally Invasive Surgeries
(MIS), dense stereo matching plays an indispensable role in
3D shape recovery, AR, VR, and navigation tasks. Although
numerous Deep Neural Network (DNN) approaches are proposed,
the conventional prior-free approaches are still popular in the
industry because of the lack of open-source annotated data
set and the limitation of the task-speciﬁc pre-trained DNNs.
Among the prior-free stereo matching algorithms, there is no
successful real-time algorithm in none GPU environment for
MIS. This paper proposes the ﬁrst CPU-level real-time prior-
free stereo matching algorithm for general MIS tasks. We
achieve an average 17 Hz on 640 × 480 images with a single-
core CPU (i5-9400) for surgical images. Meanwhile, it achieves
slightly better accuracy than the popular ELAS. The patch-
based fast disparity searching algorithm is adopted for the
rectiﬁed stereo images. A coarse-to-ﬁne Bayesian probability and
a spatial Gaussian mixed model were proposed to evaluate the
patch probability at different scales. An optional probability
density function estimation algorithm was adopted to quantify
the prediction variance. Extensive experiments demonstrated the
proposed method’s capability to handle ambiguities introduced
by the textureless surfaces and the photometric inconsistency
from the non-Lambertian reﬂectance and dark illumination. The
estimated probability managed to balance the conﬁdences of the
patches for stereo images at different scales. It has similar or
higher accuracy and fewer outliers than the baseline ELAS in
MIS, while it is 4-5 times faster. The code and the synthetic data
sets are available at https://github.com/JingweiSong/BDIS-v2.

Index Terms—Stereo matching, Bayesian theory, Posterior

probability inference

I. INTRODUCTION

M INIMALLY Invasive Surgery (MIS)

technique is
widely adopted in modern surgery since it mitigates
postoperative infections and enables faster recovery of the
patients. However, the surgeons suffer from the small ﬁeld
of view because the procedures are performed in a narrow
space with elongated tools and without direct 3D vision.
Hence, MIS poses more difﬁculties to surgeons than open
surgeries [1]. To overcome this challenge, stereoscopes are
integrated into the operational imaging system to provide 3D
stereo instead of single 2D images. Moreover, the recovered
3D shape can be further applied in applications including
dense Simultaneous Localization and Mapping (SLAM) [2, 3],
AR [4, 5] and diseases diagnosis [6, 7]. Among the many off-
the-shelf stereo matching methods, real-time implementation

J. Song and M. Ghaffari are with the University of Michigan, Ann Arbor,

MI 48109, USA. {jingweso,maanigj}@umich.edu

Q. Zhu is with School of Electrical and Data Engineering, University
of Technology, Sydney, P.O. Box 123, Broadway, NSW 2007, Australia.
Qiuchen.Zhu@uts.edu.au

J. Lin is with Hamlyn Centre for Robotic Surgery, Imperial College London,

London SW7 2AZ, UK. xjtuljy@gmail.com

[8] is essential for tasks like surgeon-centered AR, reduction
of error, decision making, or autonomous surgery’s safety
boundary. We would like to emphasize that the term “real-
time” is task-dependent and has no clear deﬁnition of frame
rate in the robotic community. This research deﬁnes “real-
time” as over 10 Hz, which is enough to serve most computer-
aided tasks in MIS or robotic surgery, such as visualization and
surgical navigation..

The state-of-the-art stereo intra-operative tissue shape re-
construction techniques strictly follow the pin-hole camera
projection model [9] and bridge the 3D shape and 2D stereo
image. The major difference exists in the workﬂow for esti-
mating the disparities from the left-right image pairs. Stereo
matching algorithms can be classiﬁed as prior-free and prior-
based. Prior-free approaches refer to aligning left and right
images pixel-wisely using explicit handcrafted features for
corner points matching (feature-metric), photometric consis-
tency presumption for direct dense pixel searching (photo-
metric) [10], or the combination of both methods [11–14].
Differently, incorporated with the annotated disparity, prior-
based methods, mostly Deep Neural Network (DNN) based
approaches, directly learn the complex “images to disparity”
process with the black box model from the training data
set [15–21].

The prior-free and prior-based categories do not contradict.
Although DNN-based methods [15, 16] are reported to be
more efﬁcient,
it comes with several disadvantages. First,
predictions from DNN-based methods may be invalidated with
changing parameters such as focal length and baseline or a
signiﬁcant difference in the texture between the training and
testing data [22, 23]. Moreover, the computational resources
are insufﬁcient for DNN-based methods when lacking high-
end GPU or saving GPU for other tasks such as SLAM [3],
detection, segmentation, or disease diagnosis [7]. Computa-
tional resources deﬁciency in the operating room is a common
problem for medical devices in the operating room. Integrating
more computational devices is difﬁcult due to hardware design
constraints in thermal and physical dimensions. Thus, the lack
of a powerful GPU severely limits the performance of DNN
in practice. More importantly, in many cases, access to high-
quality training data is unavailable because of lacking the nec-
essary hardware or other ethnic issues. This drawback could
even lead to a failure in training and predicting. Therefore,
prior-free methods are still widely applied in the industry for
their robustness and are free of the labeled training data.

Among the prior-free methods, ELAS [13] is the most
widely used prior-free algorithm in MIS. Both industry [24,
25] and academy [2, 3, 26, 27] routinely adopt ELAS for

 
 
 
 
 
 
depth recovery due to its satisfying accuracy and robustness.
However, CPU-based ELAS can only process stereo matching
in near real-time. ELAS is adopted to serve real-time stereo
requirements on GPU, which is not always available in the
operating rooms. Therefore, this article aims at developing a
real-time (more than 10 Hz on 640 × 480 on modern CPU)
and more accurate general stereo matching algorithm for CPU-
level real-time stereo matching, which can substitute ELAS.
The work Dense Inverse Searching (DIS) [10], which was
proposed to conduct real-time optical ﬂow, provided valuable
insights on fast disparity searching. DIS demonstrated that
the Lucas-Kanade (LK) optical ﬂow algorithm [28] could be
modiﬁed to a fast disparity searching manner. Meanwhile,
it also showed that
the accuracy could be improved sig-
niﬁcantly by implementing the fast LK using the coarse-
to-ﬁne patch registration. Therefore, coarse-to-ﬁne fast LK
has the potential to be applied for estimating the disparities
from images recording the continuous surfaces, such as the
MIS scenario. However, the basic assumption of fast LK,
the left-right image photometric consistency, cannot be fully
satisﬁed in MIS. Three factors contribute jointly to the
inconsistency: textureless surface, dark region, and non-
Lambertian reﬂectance. Unlike the indoor/outdoor scenario
with abundant texture, surgical scenes contain textureless soft-
tissue surfaces. Furthermore, the point source illumination of
the scope leads to some surface regions in the dark, which
in turn exacerbates the textureless issue. The non-Lambertian
reﬂectance brings uneven brightness to the surfaces, and it
cannot be eliminated by just enforcing the patch normaliza-
tion [29]. Previous researches [30, 31] demonstrated that the
non-Lambertian reﬂectance could be modeled with an afﬁne
lighting model on the observed images. Consequently, the
accuracy of the navigation system can be notably improved in
the indoor scenario. Nevertheless, the afﬁne modeling strategy
cannot be directly applied to Computer Assisted Surgery
(CAS). Unlike the indoor/outdoor cases, Shimasaki et al. [29]
pointed out that the photometric consistency in MIS is severely
contaminated, and the relationship is much more complicated
than the afﬁne modeling.

This article extends our preliminary work [32] and pro-
poses Bayesian Dense Inverse Searching (BDIS),
the ﬁrst
prior-free CPU-level real-time stereo matching algorithm, to
achieve comparable accuracy as the state-of-the-art near real-
time ELAS algorithm. The deterministic fast LK algorithm
from [10] is adopted and deeply integrated with our patch-wise
Bayesian posterior probability, which is based on the Condi-
tional Random Fields (CRFs). Moreover, a spatial simpliﬁed
Gaussian Mixture Model (sGMM) is adopted to quantify pixel-
wise conﬁdence within the patch. A probability propagation
strategy is designed to allow the probability to cover all
scales. The probabilities quantify the conﬁdence of multiple
overlapping disparities of DIS patch suffering from the tex-
tureless surface, dark region, and non-Lambertian reﬂectance
in MIS. To overcome the 3 major factors mentioned above,
the probability measuring module is employed for both local
overlapping patch estimation fusion and outlier ﬁltering. In
general, this work has the following contributions:

• To our knowledge, BDIS is the ﬁrst single-core CPU

2

stereo matching approach that achieves similar perfor-
mance to the near real-time method ELAS.

• A computationally-economic probabilistic model is de-
veloped to quantify the posterior probability of the patch
based on a simpliﬁed Bayesian assumption.

• An uncertainty-aware estimation of the disparity using
spatial sGMM is proposed to quantify the pixels’ conﬁ-
dence within the patch.

• A coarse-to-ﬁne probability propagation algorithm is
proposed to quantify the patch from different scales’
perspectives.

• A Maximum A Posteriori (MAP) based Probabilistic
Density Function (PDF) estimation algorithm is explored,
which can correctly estimate the pixel-wise variance if the
assumption of Gaussian distribution is valid and fastLK
converges close to global minima.

• An open-source C++ implementation is released along

with the synthesized stereo images and depths.

This article is an extension of our preliminary work [32]
and contributes additionally in the following ways. Unlike
the separate probability estimation in each level, this article
proposes a coarse-to-ﬁne strategy to allow the ﬁner prediction
to encode the probability of its coarser parents. Moreover,
a dynamic variance selection strategy is proposed for CRFs-
based probability estimation. Based on the probability within
the window, a MAP-based PDF estimation is proposed to
quantify the pixel-wise variance. Finally, an extensive amount
of in-vivo/ex-vivo real-world experiments with reference depth
1 and several ablation studies are conducted to validate the
performance of the proposed method.

The remainder of this article is organized as follows. Sec-
tion II provides an overview of the related work. Section III
covers the methodology with all the technical details. Sec-
tion IV conducts experiments to validate the proposed method
thoroughly. These include the qualitative and quantitative tests
on the synthetic, in-vivo data set, and ex-vivo data set. An
ablation study is conducted to illustrate the contributions of
different modules. Lastly, Section V concludes this article.

II. RELATED WORKS

Stereo matching is one of the most heavily investigated
topics in computer vision [34]. Trying to mimic human vision,
the stereo system comprises two cameras and a computing
device to triangulate the 3D shape by estimating the parallax
between the left and right images. Efﬁcient and accurate stereo
matching algorithms are essential or helpful for many tasks in
MIS, such as scope navigation, AR/VR, and disease diagnosis.
Regarding the theory and requirement, stereo matching algo-
rithms can be categorized into two groups, i.e., prior-free and
prior-based methods.

Prior-free methods. The traditional prior-free methods
directly estimate the parallax by addressing the similarity
between the left and right image. The similarity refers to

1In the robotic community, “true value” or “reference value” obtained
from hardware/software with higher precision is often termed as “ground
truth”. Following [33], we name it as “reference” considering its precision
inaccuracies. The rest content, ﬁgures, and tables use “reference” instead of
“ground truth”.

the hand-crafted features,
illumination invariance, or other
specially designed metrics such as zero-mean normalized cross
correlation [35]. The global methods [11, 36–38] minimize a
cost function that contains a similarity data term and a smooth-
ness term. The similarity data term enforces the illumination
invariance or some similarity metrics, while the smoothness
term regularizes the disparities between neighboring pixels.
To save the computational resources, local approaches [39–
42] simplify the optimization by aggregating neighboring
matching costs of the pixels. Among these approaches, the
Semi-Global Block Matching (SGBM) [11, 38] is one of the
most widely applied methods in the academy and industry.
SGBM constructs the matching cost volume with the range
of the predeﬁned depth. Then, it aggregates the cost with the
winner-takes-all strategy and computes the disparity directly
without iterative optimizations in the global methods. The re-
covered depth is further reﬁned with various strategies, such as
eliminating small patches and ﬁltering low contrast pixels. The
aggregation and winner-takes-all strategies ensure SGBM ob-
tains high-quality depth without heavy computational burdens
in the optimization step. ELAS [13], which deviates from these
cost-volume based depth searching techniques, has also been
widely used in industry [24, 25] and academy [2, 3, 26, 27].
Its procedure consists of sparse and dense steps. In the sparse
matching, ELAS uses Sobel masks to conduct sparse corner
matching as the supporting points set. The aligned sparse sup-
porting points are used for Delaunay triangulation to initialize
pixel-wise disparity. After the initialization, a densiﬁcation
step is carried out by maximizing the posterior probability
deﬁned by the photometric consistency. ELAS requires around
0.25 − 1 second with a single-core modern CPU core. Thus,
the real-time version needs to be implemented on the GPU end
(the code on GPU is publicly available). Aiming at real-time
disparity estimation, DIS [10] proposed a fast LK searching
technique for stereo matching. The fast LK, combined with
the coarse-to-ﬁne strategy, estimates the parallax in real-time.
Nevertheless, the deterministic fast LK was not applied in the
MIS community because it is vulnerable to imperfect image
pairs contaminated by textureless surface, non-Lambertian re-
ﬂectance, and dark illumination. Thus, this research adopts the
deterministic fast LK algorithm and proposes a probabilistic
formulation that is robust to these obstacles.

Prior-based methods. With the presence of high-quality
training data set, prior-based end-to-end stereo matching meth-
ods also serve the stereo matching tasks. These DNN-based
methods have an advantage over the traditional prior-based
methods in their ability to learn more complex end-to-end
searching. The delicately designed hand-crafted similarity term
in traditional prior-based methods cannot fully handle the
complex image to parallax process. The DNN-based methods,
however, directly learn the non-linear relationships from the
annotated training data set. The ﬁrst widely used DNN-based
method GCNet [43] follows the conventional stereo matching
algorithm such as SGBM by building a 3D cost volume
based on the left and right feature map. The disparity is
obtained by searching the cost volume. PSMNet [44] further
improves GCNet by introducing pyramid spatial pooling and
more convolution layers for cost aggregation. It is reported

3

to have better accuracy over GCNet. Later, GwcNet [45]
modiﬁes the structure of the 3D hourglass and introduces
group-wise correlation to form a group-based 3D cost volume.
In CAS domain, [15] is one of the earliest researches which
adopt DNN-based stereo matching techniques in the CAS
domain. PSMNet [44] and GwcNet [45] also attract the at-
tention from the CAS community [23]. Other stereo matching
techniques [17] and [20] were also tested and recommended
by the CAS community, which are capable of accomplishing
the stereo matching tasks. The latest work [21] proposed a
transformer-based stereoscopic depth perception algorithm.

The research of the prior-free CPU-level real-time stereo
matching techniques still signiﬁcantly beneﬁts the MIS com-
munity. Despite their efﬁciency,
the DNN-based methods
require a decent amount of annotated high-quality data set for
training. First, the data set is extremely difﬁcult to obtain in the
medical domain due to ethnic and hardware limitations. For
example, the publicly available data set with the reference [23]
adopted a structured light alone with the endoscope to collect
the image and shape from the porcine. The image and depth
are aligned based on the kinematic information of both sen-
sors. Moreover, to solve the synchronization problem of the
two sensors, the data is collected in the porcine, which re-
mains still during the entire procedure. Their complicated data
collection procedure in porcine implies the greater difﬁculty
in conducting the same process in the human body. Second,
the limited (high-end) GPU resource also limits the real-time
prediction of these approaches. Take PSMNet as an example,
it costs about 4G memory and around 400ms to predict a
KITTI stereo pair even on high-end GPUs. Besides, other
tasks also involve the computation on GPU, which makes it
more difﬁcult to maintain the GPU requirement of DNN-based
methods. Therefore, the prior-based and prior-free methods
complement each other in different scenarios (with or without
qualiﬁed training data set). Thus, we aim at proposing the
ﬁrst CPU-level real-time stereo shape recovery algorithm for
the MIS community.

.

III. METHODOLOGY
Fig. 1 demonstrates the workﬂow of the proposed BDIS.
The left to right parallax estimation starts from the coarsest
level. A fast LK algorithm is applied to estimate the initial
disparity of the patch. Then, our sGMM and CRFs-based prob-
ability propagation module evaluate the appropriate pixel-wise
probabilities based on the predictions from the fast LK. Next,
the predictions of all overlapping patches are fused to yield
the optimal probability, which is then used as an initialization
for the next ﬁner-scale processing. The fusion module strictly
addresses the pixel-wise probability. The matching and fusion
process is iteratively implemented until it reaches the ﬁnest
level. In the probability estimation procedure, the probability
in the coarser level is propagated to the ﬁner level to account
for multi-scale conﬁdences.

A. The fast LK algorithm

The fast LK was proposed by [10, 46]. For completeness,
we brieﬂy introduce the fast LK algorithm. In the authentic

4

Fig. 1: Illustrated is the framework of the proposed BDIS. It uses three scale levels as an example. The parallax of the patch is initialized with the prediction
from the last level. The fast LK algorithm is applied to estimate the disparity. Then, our sGMM and CRFs algorithms are used to yield the probability of
the patch in the current scale as well as the probability propagated from the last level. Finally, the predictions from the batches are fused by addressing the
predicted probabilities.

LK [28], the optimal parallax searching is realized by updating
the parallax u(n)
(kth patch on scale level (n)) by iteratively
searching the optimal parallax ∆u(cid:48). This is achieved by
minimizing the following objective function, which is

k

ˆu(n)

x =

(cid:88)

k∈Ω

1/(cid:107)I (n)
l
k∈Ω 1/(cid:107)I (n)

l

(cid:80)

(x + u(n)

k ) − I (n)

r

(x)(cid:107)2

(x + u(n)

k ) − I (n)

r

(x)(cid:107)2

u(n)

k , (2)

∆u(cid:48) = argmin∆u(cid:48)

(cid:88)

(cid:104)
I (n)
r

(x + u + ∆u(cid:48)) − I (n)

l

(cid:105)2

(x)

,

x

(1)
where x is the processed center position of the left patch, u
is the estimated parallax at one searching stage, and it keeps
being updated in each loop, I (n)
are the left image
patch and entire right image.

and I (n)

r

l

r

regarding I (n)

Since the discrete image introduces nonlinear behavior
to the system (1), the linearization of (1) is implemented
on ∆u(cid:48)
(·) at position u. Therefore, u is
updated in each iteration, and the Jacobian and Hessian
matrix need to be re-evaluated in each iteration corespond-
ingly. Kroeger et al. [10], Baker and Matthews [46] proposed
the inverse of the roles between left image patch I (n)
and
right image I (n)
. That is switching the roles and optimizing
(cid:80)
I (n)
r
x

(x + ∆u(cid:48))
. In the new formula-
tion, the linearization is carried out on the left image patch
Il, which can be predeﬁned. Therefore, the linearization of
the left image patch can be predetermined, and the Jacobian
and Hessian only need to be calculated once in the entire
optimization process.

(x + u) − I (n)

(cid:105)2

(cid:104)

r

l

l

After the patch-wise disparity searching, the predictions of
the overlapping patches are fused. In DIS [10], the optimal
disparity at the location x was fused by the overlapping patch
predictions based on the deterministic weights. The fusion
weights were determined by the normalized inverse of the left-
to-right photometric residuals. The optimal disparity is

where Ω is the set of patches covering the position x. The
pixel-wise disparity ˆu(n)
is the weighted average of the
x
estimated disparities from all patches, wherein the weight is
the inverse residual of brightness.

Fig. 2: Presented is the patch-wise probability density function of the
textureless region. It shows different probabilities (response) regarding the
value of disparity on the textured and textureless tissue surface.

B. The CRFs based patch-wise posterior probability

The weighting strategy (2) in DIS [10] cannot handle the
fusion process well in MIS stereo images. The deterministic
weights in (2) do not correctly depict
the conﬁdences of
MIS predictions due to the textureless surface, non-Lambertian
reﬂectance, and dark illumination. First, the textureless surface
and dark illumination in MIS lower the contrast and increase
the blurriness of the obtained stereo images. As fast LK
is based on the photometric consistency presumption,
the
parallax searching in (1) has a higher risk of falling into a
local minimum. Consequently, a small residual is insufﬁcient
to measure the performance of the disparity searching. The

residuals in Fig. 2 are both small, but the stereo matching on
the textureless surface (the right one) is much less reliable.
Furthermore, the photometric consistency presumption is also
seriously violated on the surface affected heavily by the non-
Lambertian reﬂectance. Ambiguities arise when the photo-
consistency assumption is violated, e.g., at intensive reﬂection
or very dark pixels. Even the Lambertian reﬂectance also
pollutes the photometric consistency because the reﬂected
intensity is related to the incident angle [29]. In the large
scale environment, afﬁne lighting formulation [30, 31] was
enforced by the SLAM systems and reported to handle the
photometric inconsistency well. The modeling enforces extra
afﬁne modeling of the illumination of the target image. Nev-
ertheless, the afﬁne modeling cannot fully tackle the complex
and severe non-Lambertian reﬂectance in MIS. Therefore, the
weights from (2) are misleading. This article seeks the CRFs
to depict the conﬁdence of the patch’s prediction.

CRFs is a sequential modeling technique that presents
transitional probabilities between ﬁnite states based on a
well-deﬁned distribution over observations. CRFs formulates
the joint probability of the states using a single exponential
formulation instead of per-state models for simpliﬁcation [47].
Given state s and target t, CRFs is expressed as a Boltzmann
distribution [48], which is a function to measure the probability
of the state as “state’s energy”. The probability is represented
in an exponential form [49] as

p(t | s, θ) =

exp(Ψ(t, s, h; θ))
t,h exp(Ψ (t, s, h; θ))

,

(cid:80)

(3)

where Ψ(·) is a potential function parameterized by θ, while
h is a set of hidden variables that can not be directly
observed [50]. Speciﬁcally, the distant observations from the
target employ low potential energy in our scenario. Hence, a
proper potential function follows the condition that its output
is inversely relevant to the distance metric.

Since there is no prior knowledge of the uncertainty distri-
bution of the left image patch and right image, it is difﬁcult
to infer the posterior probability in terms of disparity directly.
Inspired by CRFs, we choose to implicitly infer the probabil-
ity with Bayesian modeling using CRFs [51]. The posterior
probability of the patch-wise disparity u(n)

on level n is

k

5

p(I (n)
r

, u(n)
k )

|I (n)
l
P(cid:48) + P(cid:48)(cid:48)

∝

(5)

) ∝

r(n),

, I (n)
r

p(I (n)
r

p(I (n)
r

p(u(n)

, u(n)
k )

, u(n)
k )

P(cid:48)
P(cid:48) + P(cid:48)(cid:48) ∝
ki ∈P (cid:48) p(I (n)
, u(n)

k |I (n)
l
|I (n)
|I (n)
l
l
P(cid:48)
P(cid:48)
, u(n)
ki ) and P(cid:48)(cid:48) =
where P(cid:48) = Σu(n)
ki ∈P (cid:48)(cid:48) p(I (n)
|I (n)
ki ). P (cid:48) is the small window adopted
Σu(n)
l
and P (cid:48)(cid:48) is the rest window (P = P (cid:48) ∪ P (cid:48)(cid:48)). (5) simpliﬁes
P to a small window P (cid:48) considering the rest candidates are
numerically trivial. The compensation ratio r(n) (cid:44) P(cid:48)
P(cid:48)+P(cid:48)(cid:48)
and is close to 1. Since probability is assumed to follow the
Gaussian distribution, the marginal patches in P (cid:48)(cid:48) are small
numerically. Our experiment shows that r ranges from 0.99−1
and thus can be regarded as constant.

|I (n)
l

r

r

Equation (5) reveals that the posterior probability of the
disparity prediction can be retrieved by traversing the prior
probability on all possible positions u(n)
in the window
ki
P(cid:48) with size s. In addition to CRFs, we present another
straightforward explanation. The direct posterior probability
measurement, such as the inverse residual (2), is unavoid-
able to suffer from the textureless surface, non-Lambertian
reﬂectance, and dark illumination. In contrast, the posterior
probability implicitly inferred from the prior probability is
robust to these factors because the impact of these issues
is consistent on all patches and can be compensated on the
prior probabilities within the small window. Therefore, we
model the prior probability p(I (n)
ki , x) based on the
r
Boltzmann distribution, which is

|I (n)
l

, u(n)

p(I (n)
r

|I (n)
l

, u(n)

ki ) = exp

−

(cid:32)

(cid:107)I (n)
l

(u(n)

ki ) − I (n)
2σ(n)
s2
r

r
2

(u(n)

ki )(cid:107)2

(cid:33)

,

r

(6)
where scale σ(n)
is the hyper-parameter. Different from our
preliminary work [32] which uses an arbitrary value, σ(n)
is set
r
as the standard deviation of (cid:107)I (n)
k )(cid:107)2
F. This
l
dynamic parameter ensures the scale is reasonable within the
window. Results show it helps improving the accuracy and is
absence of parameter tuning. Therefore, the relative posterior
probability can be obtained with (5) and (6).

k ) − I (n)

(u(n)

(u(n)

r

p(u(n)

k |I (n)

l

, I (n)
r

) ∝

∝

p(I (n)
r
p(I (n)
r

, u(n)
|I (n)
k )
l
, u(n)
, I (n)
k )
l
p(I (n)
|I (n)
r
l
ki ∈P p(I (n)

r

Σu(n)

, u(n)
k )
|I (n)
l

, u(n)
ki )

(4)

,

where P is the window of all possible choice of u(n)
ki . In
general, naturally picking a u(n)
ki among P can be regarded as
an event of equal probability. Hence, the chance of each u(n)
ki
to be chosen is a ﬁxed constant. Denote p(u(n)
) as
the probability calculated in level n ∈ N where n is the pixel
size on level n. Speciﬁcally, one pixel on level n represents a
2n × 2n pixel patch on the original image. The computation
on (4) is time-consuming, and we approximate it with a small
window as

k |I (n)

, I (n)
r

l

Fig. 2 elaborates on the beneﬁt of the implicit inference of
the posterior probability. For the well-textured and well-lighted
image pairs, contrary to the deterministic inverse residual
weight (2), the residual has strong responses (large inverse
residual) at the correct parallax while the small response in
another parallax. For the textureless/dark/highlighted image
pairs, however, the response curve has smaller contrast, and the
weight from (2) is always large in the residual. Therefore, (2)
assigns similar high conﬁdence to both situations. (5) solves
this problem with the implicit inference and concludes that the
former situation has much higher conﬁdence than the latter.
Moreover, CRFs also tests the local convergence to ﬁlter the
Saddle points. The predictions identiﬁed as the Saddle points
are discarded. It should be noticed that, theoretically, this
principle does not apply to small patches in highlight (a few
pixels) because both large-scale and small-scale disturbances

identify the small highlighted patch as well-textured. The small
indistinguishable highlighted patch is mixed with texture. This
issue cannot be handled unless some handcrafted threshold or
prior-based method is applied. Nevertheless, we can hardly
identify this issue in the experiments.

C. The sGMM-based spatial Gaussian probability

On top of the posterior probability quantiﬁcation for the
entire patch in Section III-B, we propose a probabilistic
patch kernel using spatial sGMM [52] to estimate pixel-wise
probability within the patch.

Theoretically, any complex distribution can be decomposed
and ﬁt with a set of simple Gaussian distributions. The target
distribution can be represented as a Gaussian mixture, which
is the superposition of several Gaussian kernels. Speciﬁcally,
the GMM is a probability density function in the weighted
sum of Gaussian processes [52] as

p(x | λ) =

(cid:88)

i

wig (x | µi, σi) ,

(7)

where wi is the weight of the kernel g (x | µi, σi). For a
balanced observation, all the wi can be set as the same value.
GMM consists of various Gaussian kernels with diverse pa-
rameters. A uniﬁed form of each Gaussian density kernel [53]
can be expressed as the Gaussian kernel

g (x | µi, σi) =

1
√

(cid:18)

exp

−

(x − µi)2
2σi

(cid:19)

,

(8)

2π
where µi and σi are the mean and variance in the kernel.

σi

The authentic GMM is simpliﬁed as sGMM, which directly
uses the prior knowledge of the parameters. Jiang and Leung
[54] simpliﬁed GMM by assigning σi with a constant and only
estimated the non-exponential coefﬁcient of Gaussian kernels.
Differently, Ge and Fan [55] simpliﬁed GMM by ﬁxing the
weights and only estimate a uniﬁed σ to all σi. We push
them further by treating σi as hyperparameter while setting
all weights as 1. Thus, to avoid misunderstanding, we term it
sGMM.

In MIS, the images collected are natural and ﬁt the pre-
sumption in sGMM. Hence, a multivariate Gaussian distribu-
tion is adopted to measure the conﬁdence of the pixel-wise
probability using a Gaussian mask similar to (7). Considering
that the conﬁdence is measured patch-wisely, the patch’s center
accumulates a higher probability than the marginal pixels since
the multivariate Gaussian distribution is enforced on the patch.
The central pixels preserve more information. Assuming all
pixels in the patch are i.i.d., we have

p(u(n)

l

k |I (n)
k |I (n)

l

p(u(n)

, I (n)
r

, x) ∝

, I (n)
r

)

(cid:88)

xi∈ξ

(cid:18)

exp

−

(cid:107)x − xi(cid:107)2
F
2σ2
s

(cid:19)

,

(9)

where ξ(k)(x) is the set of all pixel positions within the patch
k in the image coordinate. σs is the 2D spatial variance of
the probability in (9). Note that (9) is independent of the
patch and can therefore be pre-computed before the process.

Combining (5), (6) and (9),
probability distribution on level n can be represented as

the ﬁnal pixel-wise posterior

6

(cid:18)

exp

−

(cid:19)

(cid:107)x − xi(cid:107)2
F
2σ2
s

(u(n)

k )(cid:107)2

F

(cid:19)

(10)

p(u(n)

k |I (n)

l

(cid:18)

exp

(cid:80)

ki ∈P exp
u(n)

, I (n)
r

, x) ∝

(cid:88)

xi∈ξ

k )−I (n)
2σ(n)
s2
r

r
2

l

(u(n)

− (cid:107)I (n)
(cid:18)
− (cid:107)I (n)

l

(cid:19) .

(u(n)

ki )(cid:107)2
F

(u(n)

ki )−I (n)
2σ(n)
s2
r

r
2

To avoid misunderstanding, we should emphasize that (6)
and (9) are not the cost functions but probability functions for
each patch or pixel. The probability quantiﬁes the weight for
the fusion process. Our probabilistic Bayesian method does
not require heavy computational cost in the optimization.

D. Probability propagation from coarse-to-ﬁne

In addition to the Bayesian probability in (5) at one level,
the measured probabilities on the coarser levels should also be
propagated to this level since the probability should describe
multi-scale observations. The probability on the ﬁnest level
should count the probability of all scales. However, as the
estimated disparity from the coarser level is only used as
initialization for the ﬁner level inverse searching, the exact
form of probability propagation cannot be accurately obtained.
We approximate it by treating the disparity estimation on each
level as independent processes with different weights. This
is reasonable because the inverse searching on each level is
bounded, and the coarsest prediction does not deviate much
from the ﬁnal prediction. Therefore, all scales contribute to
the searching of the ﬁnest disparity unevenly.

k |I (n)

We approximate it with an empirical formulation. Deﬁne
Γ(p(u(n)
)) as the probability contributed from level
n. The probability which considers coarser levels can be
obtained as

, I (n)
r

l

p(u(n)

k |I (n)

l

, I (n)
r

, x) ∝

(cid:88)

n∈N

Γ(p(u(n)

k |I (n)

l

, I (n)
r

, x)),

(11)

set of

the
, I (n)
r

where N is
In this paper,
levels.
Γ(p(u(n)
k |I (n)
)) is approximated as the following be-
cause the ﬁner scale is 2 times larger in one dimension, which
is

all

l

Γ(p(u(n)

k |I (n)

l

, I (n)
r

, x)) =

2n
n∈N 2n p(u(n)

k |I (n)

l

(cid:80)

, I (n)
r

, x).

(12)
It
is worth noting that any successful observation at an
arbitrary level can lead to an effective observation from global
perspective. Hence, the formulated probability is in the form
of superposition to represent such logical “or” relationship.

The pixel-wise weighting in (2) is substituted with

ˆu(n)

x =

(cid:88)

k∈Ω

Γ(p(u(n)

k |I (n)

l

, I (n)
r

, x))u(n)
k .

(13)

Based on the fused disparity in the ﬁnest scale ˆu(f )
x ((f )
is the ﬁnest scale), the depth can be obtained following the
routine stereo vision process, which is

dx =

fb
ˆu(f )
x

,

(14)

where f and b are the focal length and baseline.

E. PDF estimation of the depth

This section provides an optional inaccurate depth variance
estimation method in addition to the workﬂow shown in Fig. 1.
Although (5) obtains the patch’s posterior probability uk, the
parameter of the PDF, speciﬁcally the variance, is unknown.
With the Gaussian noise assumption, conﬁdence quantiﬁcation
plays an important role in 3D geometry and sensor fusion.
Assuming the errors are predominated by Gaussian distribu-
tion, we push (5) toward the estimation of the PDF. PDF
estimation technique [56] is employed and modiﬁed. Denote
the parameter set as θk = {uk, σk}, where σk is the standard
deviation of the patch k. Following the authentic deﬁnition of
the MAP algorithm, θk can be obtained as

θk = arg max

θk

p(θk | P(cid:48)) = arg max
θk

p(P(cid:48) | θk)p(θk)
p(P(cid:48))

= arg max

θk

= arg max

θk

p(P(cid:48) | θk)p(θk)

(cid:88)

(cid:16)

ln p

(cid:17)

u(n)
ki

| θk

+ ln p(θk),

i∈P(cid:48)

(15)

7

massive amount of Monte-Carlo experiments were conducted.
The experiment indicates (17) has a local minimum if σk is
set much larger than the reference. Thus, the initial σk is set as
√
0.1, which is enough because the variance in practice should
0.1. Meanwhile, the convergence of (17)
be much larger than
with GN is very fast. Even though 5 iterations are enough in
our experiments, we ﬁx the optimization iteration to 10. The
extremely small size of the state vector and Jacobian matrix
makes the computation small.

√

k

Additionally, (17) needs to be handled with two exceptions.
the residual of (17) is too large. The threshold for
First,
residual in (17) is set as 0.1. Second, the unconvergence in
the minimization process. For example, all probabilities are
almost equal, or the probability at u(f )
is not the largest. Both
situations hint the inconsistency of the Gaussian distribution
assumption. Therefore, we compromise this paradox by man-
ually setting the detected patch with a large standard deviation
which alleviates the issue. Finally, it is worth clarifying that
σx is assigned as the patch searching threshold in the fast
LK process. This is reasonable because the patch’s probability
(and weight) is small in both situations. Besides, it should
be emphasized that although some PDFs are inconsistent
with the Gaussian distribution presumption, the proportional
probability (10) is still valid.

The patch-wise variance σ2

k is propagated to pixel-wise
depth σ2
x (at position x) following weights deﬁned in (10) and
the disparity to depth process deﬁned in (13). The propagation
function is

(cid:17)

(cid:16)

is equivalent

the prior distribution of

the parame-
to the distribution of

where p(θk)
is
u(f )
ter θk. p
| θk
ki
|I (f )
, u(f )
p(I (f )
ki ) on condition of the PDF’s parameter θk.
r
l
The MAP formulation (15) can be applied in BDIS after three
modiﬁcations. First, the prior distribution of uk is ﬁxed at the
fast LK’s estimation u(f )
k . Second, the prior knowledge of the
distribution of σk is unknown and we do not enforce additional
prior knowledge term. Lastly, (5) indicates an approximate
ratio r(f ), an extra state ck helps compensating the impact
of inaccuracy resulted from the small processing window P(cid:48).
Applying the three modiﬁcations in MAP process, we have

min
σk,ck

(cid:88)

ck
√

exp

i∈P(cid:48)
− p(I (f )

r

σk
|I (f )
l

2π
, u(f )

ki ).

(cid:32)

−

(u(f )

k )2

ki − u(f )
2σ2
k

(cid:33)

(16)

To enable efﬁcient optimization of (16), natural logarithm

function ln(·) is applied.

min
σk,ck

(cid:88)

i∈P(cid:48)

ln(ck) −

1
2

ln (cid:0)σ2

k2π(cid:1) −

− ln(p(Ir|Il, u(f )

ki )).

(u(f )

k )2

ki − u(f )
2σ2
k

(17)

Gauss-Newton (GN) algorithm is adopted to solve (17).
The linearized system is solved with Cholesky decomposition
for fast solving. The initial input of σk and ck are set as
√
0.1 and 1. To reveal the reliability of GN in solving (17),

σ2
x =

(fb)2
4
ˆu(f )
x

(cid:32)

(cid:88)

k∈Ω

Γ(p(u(f )

k |I (f )

l

(cid:33)

, I (f )
r

, x))2σ2
k

.

(18)

It should be emphasized that the PDF estimation is partial
and inaccurate for two reasons. The proportional probabil-
ity (5) strictly follows the Bayesian theory and is independent
of the type of distribution. The PDF estimation, however,
requires the consistency of the Gaussian distribution presump-
tion of the error. There is no guarantee for consistency. In
many data sets, we do notice a large amount of probabil-
ity (6) within the window perfectly follows the Gaussian
curve. Furthermore, multi-scale fast LK suffers from the local
minima or bad convergence, which can be categorized as
epistemic uncertainty. Thus, our uncertainty estimation method
is optional and for reference only.

F. Technical details

This section covers important technical details in the imple-
mentation. First, in the fast LK (1) process of some patches,
the algorithm stops before the convergence due to reaching
the maximum iteration limit. Consequently, the unconverged
parallax prediction is not the minimum in the CRFs pro-
cess (5). Once detected,
the unconverged predictions are
discarded. Second, some image patch has invalid pixels caused
by image padding, undistortion, or rectiﬁcation. We follow
ELAS [13] and use threshold γ to ﬁlter small patches without
enough valid predicted pixels in the ﬁnal step. Third, the exact

8

Fig. 3: The ﬁgure shows sample synthetic left images of both diffuse lighting and non-Lambertian reﬂectance.

Fig. 4: Sample left images from the Hamlyn stereo data set.

Fig. 5: The ﬁgure shows sample calibrated left images provided by the SCARED data set. Surgical instruments are presented in two of the images.

Fig. 6: The ﬁgure shows sample calibrated left images provided by the SERV-CT stereo data set. From left to right are data set 1, 5, 9 and 15.

exponential computation consumes massive computational re-
sources. Schraudolph [57] adopted the practical approximation
method, which takes advantage of the manipulation of a
standard ﬂoating-point representation. Our experiment shows
the approximated exponential computation only takes less than
1/5 computational time of the original exponential function in
the standard C++ library. Schraudolph [57] reported that the
error is bounded for less than 4% which is acceptable in our
case.

Finally, we would like to clarify that the Bayesian prob-
ability quantiﬁcation method in Section III-B, III-C, III-D is

independent of the choice of the base method or even DNN. It
is coupled with fastLK because we aim at CPU-level real-time
stereo matching algorithm while preserving high accuracy.
It has the potential to be applied to other stereo matching
algorithms, including the DNN-based methods.

IV. RESULTS AND DISCUSSION

The efﬁciency of BDIS was validated from the perspectives
of accuracy and time consumption. This section ﬁrstly intro-
duces the data sets. Next, the accuracy comparisons on both
the synthetic data set and in-vivo data set are presented. The

following content summarizes the time consumption of all the
baseline methods. Lastly, an ablation study was conducted to
discuss the contributions of different strategies and modules.

A. Baseline algorithms, data sets, and metrics

Prior-free methods DIS [10], SGBM [11] and ELAS [13]
were adopted as the baseline methods to be compared with
BDIS in the in-vivo and synthetic data sets experiments 2.
Following the recommendations from [23], the DNN-based
methods PSMNet [44] and GwcNet [45] were also employed
for comparison. The computation platform was a commercial
desktop with CPU i5-9400 and GPU GTX 1080ti. DIS and
SGBM are open-sourced and provided from OpenCV (C++
version). The code of ELAS is also publicly available. We
implemented BDIS in C++, which was based on the code
from [10]. All the prior-free methods were compiled and run
on the CPU end (single). The two open-sourced DNN-based
methods were implemented on PyTorch [58] and run on the
GPU end.

A virtual surgical data set was synthesized to control
factors, including the intrinsic parameters, reference depth, and
calibration. The synthesized data set ensures absolute accuracy
in camera parameter calibration, reference depth retrieval, and
image rectiﬁcation and undistortion. Our synthetic data set
was generated from an off-the-shelf virtual phantom of a
male’s digestive system. The stereoscope was placed inside
the colon, and we implemented a module to enable it moving
automatically in the colon. The 3D game engine Unity3D3
was adopted to generate the sequential RGB stereo and depth
images, which strictly follows the pin-hole camera projection
model. The size of all images is 640 × 480. The number
of stereo image pairs collected from the colon is 310 (300
was collected for training DNN-based methods only). For
better comparison, both diffuse lighting and non-Lambertian
reﬂectance were simulated. It should be noticed that the dark
region near the edges is in low illumination, not pure black.
Sample images are presented in Fig. 3. Details of the training
data (DNN-based methods only) set will be described in each
experiment section.

Moreover,

two in-vivo and one ex-vivo data sets were
adopted for comparison. The ﬁrst in-vivo comes from the
public Hamlyn in-vivo stereo videos [59]. We adopted 200
image pairs with size 640 × 480 and 200 image pairs with
size 288 × 360. The Hamlyn data set does not contain the
reference depth. Sample images can be found in Fig. 4. The
second in-vivo data set (we name it SCARED) is from stereo
correspondence and reconstruction of endoscopic data sub-
challenge, which was organized during the Endovis challenge
at the conference MICCAI 2019 [23] (samples images are
shown in Fig. 5). This data set consists of the stereo images
with the annotated depth captured using a da Vinci Xi surgical
robot. The size of the images is 1280 × 720. It has 7 training
data sets and 2 testing data sets (5 keyframes for each data
set). Only 5 training data sets and 2 testing data sets were

2Readers are encouraged to watch the attached video. The code also

provides the visualization.

3https://unity.com/

9

used because the rest 2 are associated with incorrect calibration
parameters. Keyframe 4 in data set 1 is also not correct. All the
in-vivo stereo images (34 overall) were rectiﬁed, undistorted,
calibrated, and vertically aligned with the provided intrinsic
and extrinsic parameters. The ex-vivo data set SERV-CT [33]
was also obtained from the classic da Vinci Surgical System.
SERV-CT contains 16 stereo endoscopic image pairs with
reference anatomical segmentation derived from CT. Two dif-
ferent ex-vivo porcine samples were imaged using the straight
and 30◦ endoscopes. A CT scan provided the reference from
the O-arm system. The scan contains both the anatomy and
the endoscope, facilitating constrained manual alignment to
provide the pose of the viewed anatomical surface relative
to the endoscope. As sample calibrated left images in Fig. 6
indicate, the retrieved images in SERV-CT are less textured
than SCARED data set.

We used the following parameter setting: Coarsest scale
was 25, and ﬁnest scale was 21. min/max iterations 12, early
stopping parameters 0.05 0.95 0.10, patch size 10, patch
overlap 0.55, and no left
to right consistency check was
enforced; Patch mean normalization was enforced to reduce
the impact of illumination; No M-estimator was used. γ was
set to 0.75 for 640 × 480 and 0.25 for 288 × 360 data to
discard the patch without enough valid pixels. The sampling
within one Bayesian window was 5; the disturbance from
the convergence was 0.5 and 1 pixel; The minimal ratio
of the valid patch was set to 0.75;σs was set to 4. Pixel-
wise threshold 0.15 is enforced to ﬁlter unreliable predictions.
Regarding the preliminary work [32], most parameters were
the same except patch overlap set 0.55 instead of 0.6, allowing
almost 2 times faster speed.

For each pixel x with estimated depth dx, deﬁne the

absolute depth error as

ex = (cid:107)dx − dx(cid:107),

where dx is the reference depth at pixel x. For each image,
we count the mean and median errors in the entire image. For
the data set with multiple images, the metrics “average mean
error” and “average median error” refer to the average of mean
and median errors in the data set.

The numbers of predicted pixels were counted as “validity”
or “valid pixels” in the experiments. For robustness, some
predicted pixels’ disparity are ignored by ELAS, SGBM, and
BDIS. It is reasonable to discard relatively small regions that
is believed to be not conﬁdent.

B. Comparisons on the synthetic data set

The proposed BDIS was ﬁrst compared with the prior-free
methods ELAS, SGBM, DIS, and our preliminary work [32]
on the synthesized data set. Moreover, DNN-based methods
PSMNet [44] and GwcNet [45] were also tested following
the recommendation of Allan et al. [23]. They were ﬁrst
trained on the 300 additionally synthesized training image
pairs in the virtual colon. The 300 labeled pairs were employed
to ﬁne-tune the pre-trained network, which was obtained by
training on the city-scape Kitti2015 data sets [60, 61]. In the

TABLE I: The table is the average mean and median absolute depth error comparisons on the synthetic colon data with diffused light and non-Lambertian
reﬂectance shown in Fig. 3. Results of the prior-free methods ELAS, SGBM, DIS, and the proposed BDIS are presented. Prior-based methods GwcNet and
PSMNet are also shown. “Median” means the median error. “Mean” refers to mean error. “Validity” is the number of valid predicted pixels. The error is
presented in mm. The number of valid pixels is 1000. BDIS(cid:52) refers to our preliminary work [32]. “Time” refers to time consumption measured in second.

Diffuse light

Non-Lambertian reﬂectance

ELAS SGBM DIS GwcNet PSMNet BDIS(cid:52) BDIS ELAS SGBM DIS GwcNet PSMNet BDIS(cid:52) BDIS

10

0.542
Median 0.178 0.512 0.251
0.809
0.220 1.113 0.753
Mean
Validity 166.77 103.92 288.41 100.00
0.286
0.256 0.113 0.036
Time

0.161
0.320

0.248
0.417
0.641
0.475
301.42 208.44 167.07 153.00 115.19 274.91 143.20
0.281
0.375

0.161 0.157 1.211 0.338
0.202 0.295 1.660 0.702

0.066 0.221 0.094 0.032

0.086

0.126
0.142
0.528
0.221
0.850
0.316
303.28 226.03 142.05
0.061
0.082
0.366

Fig. 7: The ﬁgure shows the sample reconstructions in diffuse lighting and non-Lambertian reﬂectance scenarios. BDIS(cid:52) refers to our preliminary work [32].
These correspond to the diffused lighting images shown in Fig. 3. Readers are encouraged to watch the attached video for more restuls.

ﬁne-tuning process, 260 and 40 were used for training and
validating, respectively, in the colon data set. Both approaches
were trained with the Adaptive movement estimation (Adam)
optimizer [62]. The ﬁne-tuned training was conducted with
300 epochs of training. The rest parameter settings of the
DNN-based methods strictly followed the settings in their
original works [44, 45]. The quantitative accuracy comparisons
of the colon data set are presented in Table I in the average
mean and median absolute depth error. The synthetic data set is
free of inaccurate stereo image calibration and reference depth.
Fig. 7 presents the sample reconstructions of the baseline
approaches. Table I suggests DNN-based methods do not
perform well on the synthetic data sets.

All

the results indicate that predictions from BDIS are
similar to or slightly better than the baseline ELAS. BDIS
is more efﬁcient than other baseline methods in handling the

disparity prediction in the non-Lambertian scenario. Regarding
the median error, BDIS is 9.55% and 24.24% better than
ELAS in the diffuse lighting and non-Lambertian reﬂectance
illumination. Meanwhile, regarding the mean error, BDIS is
8.18% and 6.38% better than ELAS in the diffuse lighting and
non-Lambertian reﬂectance illumination. Furthermore, BDIS
has more valid predictions than ELAS, and its accuracy can
be higher by ﬁltering low probability predictions. Fig. 7 also
qualitatively shows that BDIS achieves fewer ambiguities than
ELAS. In general, BDIS is more robust and accurate than
ELAS in both diffuse lighting and non-Lambertian reﬂectance
illumination.

All results reveal

the better performance of BDIS over
DIS. Fig. 7 and Table I reveal that the bad average error
comparison is caused by the small group of far-out points
on the dark regions/edges. The ﬁgures and the number of

11

Fig. 8: The ﬁgure shows sample reconstructions of the SCARED stereo data set. DXKY means the keyframe Y in data set X. The error map (in mm) of the
last column is the error of BDIS. Readers are encouraged to refer to the attached video to appreciate the recovered shapes.

valid predictions suggest BDIS produces more predictions but
suffers from inaccurate dark region predictions than DIS. The
small differences cannot be easily observed visually, but the
quantitative results provide evidence on its side. In this article,
we do not enforce any prior smoothness constraint in the
fusion process. The smoothness prior needs to be treated with
caution because it impairs the accuracy in the abrupt changes
on the recovered shape.

The performance of DNN-based methods is much inferior
in the synthetic data set test (Table I) than in the following in-
vivo data set. The DNN’s performances in Table I deviate from
the conclusions in [23]. It also contradicts to the following
in-vivo test (Section IV-C). The failure should be credited
to insufﬁcient parameter tuning. Speciﬁcally, there is a large
texture gap between the synthetic training data set and the
data set adopted in training the pre-trained DNN model. The
pre-trained model is obtained from the city-scape data set
Kitti2015 [60, 61]. The texture of the synthetic data set was
simulated from the game engine and bereft of many imaging
details and cues. That is to say, the synthetic data set is not

the nature image as the Kitti2015. Previous researches [63–
65] suggest that the performance of the convolution neural
network is heavily dependent on the texture of the images.
This phenomenon also explains the massive effort devoted
to bridging the domain gap between synthetic data and real-
world data in the autonomous driving community. In our case,
the pre-trained weights of kernels contribute poorly to the
convergence of the model. Thus, instead of ﬁne-tuning the
pre-trained model, the DNN-based models should be trained
on the synthetic data set from the beginning. Unfortunately, we
could not successfully train the DNN based on the generated
data sets for the two DNN-based methods. The quantitative
real-world in-vivo/ex-vivo data sets with reference are tested
in Section IV-C for further validation.

C. Quantitative Comparisons on the in-vivo/ex-vivo data set

On top of the qualitative comparison, we validated the
performance of BDIS on the real-world SCARED (in-vivo)
and SERV-CT (ex-vivo) stereo data sets. For the SCARED
data set, two separate experiments were conducted. In the ﬁrst

12

Fig. 9: The ﬁgure shows sample reconstructions of the SERV-CT stereo data set. The rows show data set 1, 5, 7, 9, 12 and 15 which differs signiﬁcantly in
texture. The error map (in mm) in the last column is the error of BDIS. Readers are encouraged to refer to the attached video for more results.

TABLE II: The table presents the mean absolute depth error comparisons on the SCARED stereo data set. DXKY means the keyframe Y in data set X. The
average of the mean and the average median errors of these approaches are also listed. data set 8 and 9 are designated as the testing data set in [23]. ·∗ means
the method requires extra training data. Errors are in mm. The number of valid depth pixels is in 1000. BDIS(cid:52) refers to our preliminary work [32].

ELAS

SGBM

DIS

GwcNet∗

PSMNet∗

BDIS(cid:52)

D8K1
D8K2
D8K3
D8K4
D8K5
D9K1
D9K2
D9K3
D9K4
D9K5

6.452
3.813
1.642
2.008
2.927
3.197
0.683
0.834
0.806
0.385

Average mean error
Average median error
Valid depth pixel

2.275
1.825
674.71

6.522
3.785
3.258
1.912
3.187
2.539
1.452
1.360
19.094
30.608

7.372
3.223
560.42

6.847
4.036
1.904
2.434
2.625
3.399
0.995
1.862
7.441
13.456

4.500
3.012
817.37

7.726
3.642
1.513
1.824
1.599
2.670
0.826
0.859
1.032
0.509

2.220
1.556
826.25

14.759
7.151
3.210
3.691
3.220
6.577
1.833
2.064
1.220
0.888

4.461
3.215
826.46

6.593
3.947
1.782
2.106
1.906
2.630
0.766
0.968
0.694
0.358

2.175
1.820
739.06

BDIS

6.386
3.913
1.754
2.105
1.880
2.607
0.704
0.940
0.681
0.343

2.131
1.817
713.58

TABLE III: Table shows the mean absolute depth error comparisons on the
SCARED stereo data set. DXKY is the keyframe indexed as Y in data set
X. Results of the prior-free methods ELAS, SGBM, DIS, and the proposed
BDIS are presented. The cells are the average absolute mean error (in mm).
BDIS(cid:52) refers to our preliminary work [32].

ELAS

SGBM

DIS

BDIS(cid:52)

BDIS

TABLE IV: Table shows the mean absolute depth error comparisons on the
SERV-CT stereo data set. Left column is the keyframe indexed. Results of the
prior-free methods ELAS, SGBM, DIS, GwcNet, PSMNet and the proposed
BDIS are presented. The cells are the average absolute median error (in mm).

ELAS SGBM DIS GwcNet PSMNet BDIS

13

D1K1
D1K2
D1K3
D1K5
D2K1
D2K2
D2K3
D2K4
D2K5
D3K1
D3K2
D3K3
D3K4
D3K5
D6K1
D6K2
D6K3
D6K4
D6K5
D7K1
D7K2
D7K3
D7K4
D7K5
D8K1
D8K2
D8K3
D8K4
D8K5
D9K1
D9K2
D9K3
D9K4
D9K5

0.673
0.981
0.905
0.794
0.718
1.422
1.071
0.367
1.624
2.147
1.932
2.113
2.394
2.006
4.084
2.613
2.575
4.719
8.842
3.685
4.194
1.256
2.171
3.296
6.452
3.813
1.642
2.008
2.927
3.197
0.683
0.834
0.806
0.385

Mean
Median
Validity

1.6462
2.333
674.73

0.959
1.162
0.844
2.822
8.080
1.112
4.115
43.066
1.888
1.522
4.728
1.911
5.404
1.558
5.216
2.967
3.293
4.703
6.261
6.153
4.203
11.685
4.914
3.555
6.522
3.785
3.258
1.912
3.187
2.539
1.452
1.360
19.094
30.608

4.196
6.054
549.61

1.350
2.042
1.617
1.735
2.590
3.281
8.105
4.734
2.648
2.477
1.513
2.739
2.272
4.101
7.343
4.186
4.264
6.807
7.878
4.748
5.931
2.709
2.831
3.954
6.847
4.036
1.904
2.434
2.625
3.399
0.995
1.862
7.441
13.456

1.913
4.025
803.85

0.754
1.099
0.958
0.822
0.702
1.142
1.099
0.352
1.496
1.688
1.050
1.568
1.040
1.539
5.231
2.879
3.096
4.562
6.503
4.609
3.296
1.406
2.664
3.592
6.498
3.902
1.756
2.186
1.969
2.998
0.680
0.968
0.689
0.349

0.749
1.075
0.945
0.812
0.690
1.143
1.086
0.345
1.483
1.673
1.033
1.549
1.017
1.517
5.238
2.877
3.065
4.554
6.162
4.583
3.285
1.345
2.551
3.301
6.386
3.913
1.754
2.105
1.880
2.607
0.704
0.940
0.681
0.343

1.807
2.219
739.00

1.778
2.158
717.25

experiment, following the recommendation of Allan et al. [23],
only the last ten image pairs (images in data sets 8 and 9)
were tested since the DNN-based methods require the rest
images (24 stereo image pairs) for ﬁne-tuning the network.
Among the 24 stereo image pairs, 18 were adopted to ﬁne-
tune PSMNet and GwcNet, while the validation was done
on the other 6 stereo image pairs. The ﬁne-tuned networks
were further applied to predict the rest 10 testing data sets.
Fig. 8 shows sample reconstructions of the baseline methods.
In the second experiment for prior-free methods only, since the
prior-free approaches are free of training data, all available (34
totally) stereo image pairs were used for more comprehensive

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15

1.027
2.976
1.641
2.485
5.344
2.091
2.004
1.633
4.242
2.262
1.596
1.595
6.718
9.307
3.589

3.442
2.622
4.261
4.011
3.226
4.390
2.854
5.202
5.115
5.674
3.616
3.883
5.806
6.177
3.424
3.491
2.593
1.986
2.092
1.870
2.068
1.681
1.476
1.399
3.772
2.781
16.437 3.367
1.697
1.768

3.227
4.629
3.343
3.967
5.693
3.471
6.238
6.981
5.539
4.057
4.467
2.763
3.672
10.848
6.546

5.295
6.919
4.655
4.149
9.653
5.550
8.481
6.097
3.165
3.516
2.516
2.405
2.687
2.437
2.480

3.315
4.024
2.977
2.538
5.122
3.554
5.608
3.295
1.632
1.640
1.677
1.235
1.771
0.595
1.590

12.750
5.478
Mean
5.665
Median 3.233
5.029
4.291
Validity 120.00 184.83 398.30 379.56

7.602
3.187

8.585
4.667
412.02

3.750
2.705
311.48

comparisons.

The frame-wise mean absolute depth errors between the
state-of-the-art approaches and the proposed BDIS on the
SCARED stereo data set are demonstrated in Table II. The
last 10 stereo image pairs of the testing keyframes desig-
nated by Allan et al. [23] are individually presented. It also
summarizes the median error, average error, and the number
of valid depth pixel predictions of these approaches. The
results indicate that BDIS is 6.33% more accurate than ELAS
regarding the average median error. Compared with ELAS,
the proposed BDIS improves the performance on the average
median error by 0.44%. Nevertheless, BDIS has 5.67% more
valid predictions over ELAS. Moreover, Table III indicates
that SGBM can predict similar depth as BDIS and ELAS,
but
is
comparable to BDIS and ELAS. The results of GwcNet vali-
date our assumption (Section IV-B) and conclusions from the
researches [63–65] that the performance of DNN is dependent
on the texture similarity between the training and testing
images. The images in the SCARED stereo data set are natural
images, and their textures are much more abundant than the
synthetic data set (results in Section IV-B). However, it should
be emphasized that the accuracy of the DNN-based method is
heavily dependent on the quality of the training data set. The
comparisons of the prior-free and the DNN-based methods are
for reference only. In general, it conﬁrms our claim that BDIS
and ELAS have similar accuracy and predicts the same amount
of valid depth.

is not robust. It shows that

the trained GwcNet

it

Table III shows the comparisons of the prior-free methods
ELAS, SGBM, DIS, and BDIS. The frame-wise mean and
median errors of all keyframes are summarized. In general,
the accuracy of ELAS is 7.42% higher in average mean error

14

TABLE V: The table presents the quantitative comparisons between the DIS, SGBM and BDIS regarding the shape of ELAS as the reference. The results
are presented as the average absolute depth error (mm).

Median error

Average error

Standard deviation

Data 1
Data 2
Data 3
Data 4
Data 5

DIS

1.81
1.06
0.46
0.49
0.52

SGBM

BDIS

1.29
1.78
0.44
0.41
0.55

1.64
0.91
0.41
0.39
0.42

DIS

2.69
1.80
0.87
1.38
1.02

SGBM

BDIS

1.96
2.79
0.67
0.76
0.93

2.30
1.37
0.65
1.06
0.80

DIS

2.83
2.45
1.34
2.24
1.62

SGBM

BDIS

2.15
3.03
0.90
1.02
1.28

2.25
1.67
0.89
1.90
1.28

over BDIS, while BDIS is 7.55% better regarding the average
median error. Moreover, the valid depth pixels of BDIS are
5.93% more than ELAS. This result is consistent with other
quantitative experiments in this article.

ELAS, SGBM, DIS, PSMNet, GwcNet, and BDIS were also
tested on the SERV-CT data set. Since only 16 image pairs
are provided in SERV-CT, the trained DNNs of PSMNet and
GwcNet on SCARED data sets were adopted directly. Thus,
the results of PSMNet and GwcNet are cross the textural
domains. Sample qualitative comparisons are presented in
Fig. 9. Quantitative comparison is shown in Table IV. Please
note that only ﬁrst 15 data sets were tested because ELAS did
not yield a valid prediction of the 16th image pair.

Fig. 9 and Table IV indicate that ELAS has much smaller
valid predictions compared to BDIS, which contradicts the
results in SCARED experiments. The reason is that the region
of valid pixels is within the Delaunay polygon deﬁned by
the sparse supporting corner points. Since the textural and
illuminational quality of images in SERV-CT (Fig. 6) data
are worse than images in SCARED (Fig. 5), the sparse corner
points matching module in ELAS does not provide satisfying
point pairs. Thus, the accuracy of ELAS with small valid
regions is for reference only. In general, BDIS achieves the
best mean and median absolute error performance than the
other three prior-free methods.

Table IV also shows that the two DNN methods are not
satisfying. The major reason is that both DNN methods
predicted many outliers on the edges, which suffer from the
darkness. Thus, their average median errors are much better
than the mean error. Another reason (minor) may be the cross-
domain application because the image textures of SERV-CT
are different from SCARED. Moreover, although the table
shows that DNNs’ results are not satisfying, it should be
emphasized that Fig. 9 shows that the central region of the
two DNN methods are acceptable.

D. Qualitative comparison with ELAS on in-vivo data set

The proposed BDIS was compared with ELAS, DIS, and
SGBM on the in-vivo Hamlyn data sets. Only qualitative
comparisons of the prior-free methods can be given on this
data set because no reference labels are provided for training
DNNs. Like the SCARED data set, the Hamlyn data set was
collected from the stomach of the porcine, which provides
textured stereo images. This data set was
more abundant
employed to show the qualitative comparisons of the prior-

free approach and the quantitative difference between the
predictions of ELAS and BDIS. The experiment validates our
claim that the proposed BDIS has similar performance to the
baseline ELAS in the scenario of abundant textures.

The Hamlyn data set is categorized into 5 groups by the
camera-to-surface distance since the distances in this data
set vary signiﬁcantly. Fig. 10 shows the sample comparisons
between baseline prior-free methods and BDIS. Table V shows
the average accuracy of all frames within each category. It
indicates that the proposed method achieves around 10% (me-
dian error) and 15% accuracy over the original DIS algorithm.
Although no reference is provided, this comparison shows
BDIS achieves an average 0.4 − 1.66mm (median error) and
0.65 − 2.32mm (mean error) difference from ELAS’s results.
Fig. 10 shows BDIS can achieve similarly or even slightly
better robustness in handling the photometric inconsistency in
the stereo matching process. The photometric inconsistency
poses great difﬁculty to the surgical stereo matching process
since the stereo images are vulnerable to non-Lambertian
reﬂectance, dark region, or textureless surfaces. Fig. 8 and
Fig. 10 indicate BDIS has fewer outliers at the image edges.
BDIS overcomes this issue mainly due to three strategies:
probabilistic inverse residual-based patch fusion, initialization
strategy, and coarse-to-ﬁne probability propagation. The prob-
abilistic inverse residual-based patch fusion uses the predic-
tions from multiple patches to mitigate the invalid dispar-
ity pixels (ambiguous predictions). These invalid predictions
mainly result from the edges of the rectiﬁed image after the
image undistortion, occlusions from objects to the left or
right cameras, or unsuccessful predictions due to insufﬁcient
information. The dubious and unsuccessful predictions in
coarse level pass incorrect initialization to the ﬁner scale.
Our probabilistic inverse residual-based patch fusion quantiﬁes
the posterior probability; the initialization strategy discards
the patch that does not converge; coarse-to-ﬁne probability
propagation provides an important indicator to help lower
the patches’ probabilities with invalid pixels. Since there are
enough overlaps between the neighboring patches, the low
conﬁdence initialization may be compensated by its neighbors.
In the worst case, following ELAS, the predictions with very
low probability are removed.

On top of the initialization issue, the ambiguous local min-
ima pose noticeable difﬁculty in the stereo matching process.
The local minima issue has a heavy impact on the performance
of fast LK since the corresponding cost function under the

15

Fig. 10: The ﬁgure shows sample recovered models of the 5 classes. BDIS(cid:52) refers to the BDIS proposed in [32].

Fig. 11: The ﬁgure illustrates the qualitative comparisons of ELAS [13], DIS [10] and the proposed method. The marked regions are the wrongly reconstructed.

photometric consistency assumption is non-convex. The toy
example in Fig. 2 shows that our estimated coarse-to-ﬁne
strategy measures the probability well, and the fusion module
alleviates the ambiguity of the disparity with the neighboring
patch predictions. This article’s sample ﬁgures demonstrate
that BDIS has fewer local minima than the baseline prior-free
approaches. This is also one reason for the high accuracy of
BDIS in the qualitative comparisons. However, it should be
addressed that we do not enforce any prior smoothness con-
straint in the optimization process. The smoothness prior needs
to be treated with caution because it impairs the accuracy of
the abrupt changes and edges on the recovered shape.

In addition to testing the well-illuminated soft tissue shown
in Fig. 2, we further presented the scenario of bad illumination,
which corresponds to the non-Lambertian data set (Fig. 7). In
these cases, the light intensity is roughly (not accurate) pro-
portional to cos(α) where α is the angle between the surface’s
normal and the viewing direction. The texture vanishes when
α is close to zero (sharply lighted patch) or when α is large
(dark region). Both bad-textured regions are obstacles to the
cost function. As demonstrated in Fig. 11, the center of the soft
tissue is exposed to intense lighting while the marginal region
is dark. Note that the marginal region is dark but not invalid.
As depicted in Fig. 11, our proposed BDIS outperforms ELAS
in estimating or ﬁltering dark pixels.

E. Comments on the comparison with DNN-based methods

We provide our comments on the comparisons with DNN-
based methods in Section IV-B and IV-C. It should be empha-
sized that the comparison with the DNN-based methods is for
the completeness of all the comparisons, and our presented
conclusions regarding DNN-based methods are for reference
only. First, comparing prior-free methods with DNN-based
methods is not fair. DNN-based methods’ performance heavily
relies on the size and quality of the training data, which varies
greatly in CAS. Up to now, learning-based methods cannot
fully substitute prior-free methods due to prior-free methods’
robustness to parameter tuning and being free of training data.
Thus, the conclusion that DNN-based methods work in one
case cannot be generalized to other cases. Moreover, DNN
consumes heavy computational resources of GPU, while all
the prior-free methods use only one CPU core. Additionally,
BDIS and DNN methods can be complementary. Since DNN
is on GPU and dependent on the training data set, CPU-based
prior-free BDIS can assist in data fusion or cross-validation.

16

TABLE VI: The table shows the coverage rate of the estimated pixel-wise
variance (18). It summarizes the ratio of pixel-wise errors that falls within the
bound ex falls within the range of [−1.96ˆσx, 1.96ˆσx]. The ideal coverage
rate is 95%. A and a are the results on the synthetic data sets colon in diffuse
lighting and non-Lambertian reﬂectance. SX are the results on SERV-CT data
set. The results of the in-vivo data sets are also tested. It is marked as DXKY
where Y is the keyframe index and is X is data set index. All values are in
percentage.

a9

a1

a8

a2

a4

a3

S9

S1

S7

S2

S6

S8

A2

A4

A6

A3

A5

A8

A7

a10

S10

S14

A10

a5
90.9
S3

a6
90.75
S4

A1
93.87 92.35 87.37 64.01 90.13 65.76 82.35 89.02
A9
88.48 92.28 93.09 92.11 86.72 56.9
a7
85.33 85.66 73.26 93.52 46.27 42.16 34.56 57.30
S5
67.30 49.25 64.04 64.32 93.56 93.68 94.00 92.71
D1K1 D1K2 D1K3 D1K5 D2K1
S15
S13
90.25 93.35 94.46 93.04 92.3
92.81 91.29 87.01
D2K2 D2K3 D2K4 D2K5 D3K1 D3K2 D3K3 D3K4
97.06 84.53 87.43 90.94 93.82 89.21 89.79 90.35
D3K5 D6K1 D6K2 D6K3 D6K4 D6K5 D7K1 D7K2
90.49 66.38 45.02 57.98 57.8
50.64 50.87 63.51
D7K3 D7K4 D7K5 D8K1 D8K2 D8K3 D8K4 D8K5
47.97 38.66 35.01 50.35 33.12 52.61 43.92 61.84
D9K1 D9K2 D9K3 D9K4 D9K5
80.75 88.36 85.68 74.13 83.77

S11

S12

BDIS achieves similar/better performance over ELAS and runs
4-5 times faster. Table VIII shows BDIS achieves the second-
fastest speed (except DIS) over the rest algorithms with 17Hz
and 14Hz respectively on both datasets. The frame rate meets
the requirement for real-time SLAM [2, 3] and AR [4, 5]
tasks in CAS. The time consumption comparison validates our
claim that BDIS is the only real-time single-core CPU-level
stereo matching in CAS. BDIS can substitute near real-time
ELAS because it is much faster and similar or slightly better
regarding accuracy.

Moreover, readers may notice that the proposed BDIS is
almost two times faster than our preliminary work [32]. The
reason is that we reduce the patch ratio from 0.6 to 0.55. A
smaller patch ratio means much fewer patches to be processed.
Our new strategies, coarse-to-ﬁne and dynamic sigma σ(n)
,
contribute to better accuracy, and thus we can sacriﬁce small
accuracy in exchange for faster speed. The new strategies
consume very limited extra time in the implementation. Con-
sidering this, we balanced the time speed and performance by
reducing 0.6 to 0.55.

r

F. Time consumption

Table VIII presents the time consumption comparisons in
predicting the disparity of the prior-based methods ELAS,
DIS, SGBM, BDIS, and the prior-based methods PSMNet
and GwcNet. The experiments of the prior-free methods are
all carried out on a single core of the CPU (i5-9400). The
experiments of prior-based methods are implemented on GPU
(GTX 1080ti). The processing time of BDIS is double as DIS
owing to the extra time spent on patch-wise window traversing.
Since the size of the window P (cid:48) is chosen as 5, BDIS needs
to spend quintuple time on the patch convolution. In general,

G. Variance estimation

This section validates the optional variance estimation tech-
nique described in Section III-E. Ideally, 95% of absolute
depth error ex falls within the range of [−1.96ˆσx, 1.96ˆσx].

Table VI shows the frame-wise coverage rate. The experi-
ments indicate that the variance estimation algorithm performs
unstably. It indicates that results with higher accuracy have
more accurate variance estimation. As exempliﬁed in the
accuracy of in-vivo experiments (Table III), the average mean
error less than 2 mm (data set 1, 2, 3, and 9) has a much better

17

TABLE VII: The table shows the average mean and the median absolute depth error comparisons on the SCARED stereo data set and the synthetic data set
colon (in diffuse lighting and non-Lambertian reﬂectance). The full BDIS, BDIS without GMM, BDIS without a coarse-to-ﬁne strategy, and BDIS without
dynamic sigma σ(n)
is ﬁxed to 8 in our preliminary research [32]) are presented. For the beneﬁts of analyzing, the normalized results are also provided.

(σ(n)
r

r

SCARED

Synthetic
(diffuse lighting)

Synthetic
(non-Lambertian)

Median
Mean
Median (Normalize)
Mean (Normalize)

Median
Mean
Median (Normalize)
Mean (Normalize)

Median
Mean
Median (Normalize)
Mean (Normalize)

BDIS

1.778
2.158
1.000
1.000

0.161
0.202
1.000
1.000

0.138
0.224
1.000
1.000

BDIS
(no sGMM)

BDIS
(no Coarse-to-ﬁne)

BDIS
(no dynamic sigma)

1.792
2.204
1.008
1.021

0.160
0.210
0.994
1.040

0.140
0.247
1.020
1.102

1.799
2.197
1.012
1.018

0.163
0.207
1.012
1.020

0.138
0.240
1.000
1.071

1.784
2.213
1.036
1.025

0.163
0.207
1.012
1.020

0.139
0.236
1.007
1.053

TABLE VIII: llustrated is the average of 10 times of time consumption of
algorithms PSMNet, GwcNet, ELAS, DIS, SGBM, our preliminary work [32]
and the proposed method. The two prior-based methods are marked as
∗ because the computation is conducted on GPU. BDIS(cid:52) refers to our
preliminary work [32]. BDIS(VAR) refers to enabling the optional variance
estimation function. The time consumption is measured in seconds.

Synthetic
640 × 480

SCARED
1280 × 720

SERV-CT
720 × 576

PSMNet∗
GwcNet∗
ELAS
SGBM
DIS
BDIS(cid:52)
BDIS(VAR)
BDIS

0.327
0.284
0.246
0.124
0.038
0.097
0.072
0.064

0.566
0.430
0.291
0.346
0.093
0.131
0.104
0.072

0.431
0.391
0.264
0.219
0.054
0.114
0.087
0.068

coverage rate. The trend in the test of the synthetic data set
is not that apparent as in the in-vivo case. One explanation is
that the disparity largely deviates from the reference, and the
local ﬁne-scale variance fails to describe the uncertainty. The
large errors in the coverage rate of some frames conﬁrm our
claim that the variance estimation is partial and inaccurate. The
aleatoric uncertainty can be heavily affected by the epistemic
uncertainty in the fast LK process. Moreover, a large number
of pixels do not obey the Gaussian probability presumption.
Finally, table VIII shows an extra 12% to 40% percent of
processing time of in variance quantiﬁcation.

H. Ablation study

To investigate the contributions of the modules in the
proposed BDIS, several ablation studies were conducted. We
tested the performance of the sGMM (Section III-C), coarse-
to-ﬁne probability propagation (Section III-D) and the dy-
namic choice of σ(n)
(Section III-B) on the SCARED stereo
data set and the synthetic data set (in diffuse lighting and non-

r

Lambertian reﬂectance). CRFs (Section III-A) was not tested
in the ablation study because it is the major framework.

Table VII summarizes the accuracy (average mean and
average median) comparisons. The sGMM module was ﬁrst
disabled by setting the pixels in the patch in equal probabil-
ities. We only observe that BDIS is inferior in the average
median error in the synthetic diffuse lighting data set. Next,
the coarse-to-ﬁne strategy was disabled, and the obtained
probability only indicates the processing level. All results
suggest the accuracy is improved from 1 − 3%. The coarse-to-
ﬁne strategy is the major innovation regarding the preliminary
work [32]. Table I II III all validate its performance. Lastly,
the dynamic sigma σ(n)
enables tuning-free parameters and
slightly better accuracy. Finally, our experiments show that all
the new strategies in this paper have negligible impact on time
consumption.

r

V. CONCLUSION

This article proposes BDIS as the ﬁrst CPU-level real-
time prior-free stereo matching in the surgical scenario. It
is both novel and practical in many real-world CAS appli-
cations, e.g., lacking (high-end) GPUs, saving GPUs for other
tasks, lacking annotated surgical data set for training DNN
model, etc. The proposed BDIS inherits the speed of the
fast LK algorithm and overcomes major obstacles in surgical
image stereo matching, i.e., textureless/dark/non-Lambertian
reﬂectance tissue surfaces, by adopting three strategies to the
deterministic fast LK: sGMM, CRFs, and a Bayesian coarse-
to-ﬁne probability propagation techniques. The proposed BDIS
correctly describes the relative conﬁdence of the pixel-wise
disparity. A variance estimation algorithm is also introduced
based on the estimated probability. In this way, the pixels with
low conﬁdence are ﬁltered out.

Experiments show that

the prior-free BDIS achieves an
average 17 Hz on 640 × 480 image with a single core of
the CPU (i5-9400) for surgical images, which satisﬁes most
SLAM, AR, and VR requirements. Its accuracy is also slightly

better than the popular near real-time ELAS. The proposed
method correctly quantiﬁes the pixel-wise relative probability,
which beneﬁts outlier ﬁltering steps. The C++ code is open-
sourced for the beneﬁt of the community. It
is supposed
to assist tasks like surgeon-centered AR, reduction of error,
decision making, or safety boundaries for autonomous surgery.
We plan to implement BDIS on GPU end for faster per-
formance. Theoretically, BDIS should be faster than ELAS
on GPU. Future work may also focus on the adaption of
the proposed Bayesian strategy to the DNN-based methods.
It is also interesting to investigate the fusion of BDIS and
DNN-based methods because the prior-free CPU-based BDIS
and the DNN-based GPU-based methods are complementary.
When DNN is carried out on GPU and dependent on the
training data set, CPU-based prior-free BDIS can assist in data
fusion or cross-validation.

ACKNOWLEDGMENT

Toyota Research Institute provided funds to support this
work. Funding for M. Ghaffari was in part provided by NSF
Award No. 2118818. We would also like to thank Mr. Adam
Smidt from University of British Columbia for his helpful
suggestion.

REFERENCES
[1] P. Mountney and G.-Z. Yang, “Dynamic view expansion for
minimally invasive surgery using simultaneous localization and
mapping.”

IEEE, 2009, pp. 1184–1187.

[2] J. Song, J. Wang, L. Zhao, S. Huang, and G. Dissanayake,
“Dynamic reconstruction of deformable soft-tissue with stereo
scope in minimal invasive surgery,” IEEE Robot. and Autom.
Lett., vol. 3, no. 1, pp. 155–162, 2017.

[3] J. Song, J. Wang, L. Zhao, S. Huang, and G. Dissanayake,
“MIS-SLAM: Real-time large-scale dense deformable SLAM
system in minimal invasive surgery based on heterogeneous
computing,” IEEE Robot. and Autom. Lett., vol. 3, no. 4, pp.
4068–4075, 2018.

[4] N. Haouchine, J. Dequidt, I. Peterlik, E. Kerrien, M.-O. Berger,
and S. Cotin, “Image-guided simulation of heterogeneous tissue
deformation for augmented reality during hepatic surgery,”
in IEEE Intern. Symposium on Mixed and Augment. Reality.
IEEE, 2013, pp. 199–208.

[5] A. R. Widya, Y. Monno, K. Imahori, M. Okutomi, S. Suzuki,
T. Gotoda, and K. Miki, “3D reconstruction of whole stomach
from endoscope video using structure-from-motion.”
IEEE,
2019, pp. 3900–3904.

[6] F. Mahmood, Z. Yang, R. Chen, D. Borders, W. Xu, and N. J.
Durr, “Polyp segmentation and classiﬁcation using predicted
depth from monocular endoscopy,” in Medical Imaging 2019:
Computer-Aided Diagnosis, vol. 10950.
International Society
for Optics and Photonics, 2019, p. 1095011.

[7] X. Jia, X. Mai, Y. Cui, Y. Yuan, X. Xing, H. Seo, L. Xing, and
M. Q.-H. Meng, “Automatic polyp recognition in colonoscopy
images using deep learning and two-stage pyramidal feature
prediction,” IEEE Trans. Autom. Sci. Eng., vol. 17, no. 3, pp.
1570–1584, 2020.

[8] P. Edwards, M. Chand, M. Birlo, D. Stoyanov et al., “The
challenge of augmented reality in surgery,” Digital Surgery, pp.
121–135, 2021.

[9] A. M. Andrew, “Multiple view geometry in computer vision,”

Kybernetes, 2001.

[10] T. Kroeger, R. Timofte, D. Dai, and L. Van Gool, “Fast optical
ﬂow using dense inverse search,” in Proc. European Conf.
Comput. Vis. Springer, 2016, pp. 471–488.

18

[11] H. Hirschmuller, “Accurate and efﬁcient stereo processing by
semi-global matching and mutual information,” in Proc. IEEE
Conf. Comput. Vis. Pattern Recog., vol. 2.
IEEE, 2005, pp.
807–814.

[12] D. Stoyanov, M. V. Scarzanella, P. Pratt, and G.-Z. Yang, “Real-
time stereo reconstruction in robotically assisted minimally
invasive surgery,” in Int. Conf. on Med. Image Comput. and
Comput. Assist. Interv. Springer, 2010, pp. 275–282.

[13] A. Geiger, M. Roser, and R. Urtasun, “Efﬁcient large-scale
stereo matching,” in Proc. Asian Conf. Comput. Vis. Springer,
2010, pp. 25–38.

[14] J. K. Rappel, “Surgical stereo vision systems and methods for

microsurgery,” May 3 2016, US Patent 9,330,477.

[15] M. Ye, E. Johns, A. Handa, L. Zhang, P. Pratt, and G.-
Z. Yang, “Self-supervised siamese learning on stereo image
pairs for depth estimation in robotic surgery,” arXiv preprint
arXiv:1705.08260, 2017.

[16] M. Turan, Y. Almalioglu, H. Araujo, E. Konukoglu, and M. Sitti,
“Deep ENDOVO: A recurrent convolutional neural network
(RCNN) based visual odometry approach for endoscopic
capsule robots,” Neurocomputing, vol. 275, pp. 1861 –
1870, 2018. [Online]. Available: http://www.sciencedirect.com/
science/article/pii/S092523121731665X

[17] G. Yang, J. Manela, M. Happold, and D. Ramanan, “Hierarchi-
cal deep stereo matching on high-resolution images,” in Proc.
IEEE Conf. Comput. Vis. Pattern Recog., 2019, pp. 5515–5524.
[18] A. Tonioni, F. Tosi, M. Poggi, S. Mattoccia, and L. D. Stefano,
“Real-time self-adaptive deep stereo,” in Proc. IEEE Conf.
Comput. Vis. Pattern Recog., 2019, pp. 195–204.

[19] H. Xu and J. Zhang, “Aanet: Adaptive aggregation network for
efﬁcient stereo matching,” in Proc. IEEE Conf. Comput. Vis.
Pattern Recog., 2020, pp. 1959–1968.

[20] P. Brandao, D. Psychogyios, E. Mazomenos, D. Stoyanov,
and M. Janatka, “HAPNet: hierarchically aggregated pyramid
network for real-time stereo matching,” Computer Methods in
Biomechanics and Biomedical Engineering: Imaging & Visual-
ization, pp. 1–6, 2020.

[21] Y. Long, Z. Li, C. H. Yee, C. F. Ng, R. H. Taylor, M. Unberath,
and Q. Dou, “E-DSSR: Efﬁcient dynamic surgical scene recon-
struction with transformer-based stereoscopic depth perception,”
arXiv preprint arXiv:2107.00229, 2021.

[22] P. Pratt, C. Bergeles, A. Darzi, and G.-Z. Yang, “Practical
intraoperative stereo camera calibration,” in Int. Conf. on Med.
Image Comput. and Comput. Assist. Interv.
Springer, 2014,
pp. 667–675.

[23] M. Allan, J. Mcleod, C. C. Wang, J. C. Rosenthal, K. X.
Fu, T. Zefﬁro, W. Xia, Z. Zhanshi, H. Luo, X. Zhang et al.,
“Stereo correspondence and reconstruction of endoscopic data
challenge,” arXiv preprint arXiv:2101.01133, 2021.

[24] G. Zampokas, K. Tsiolis, G. Peleka, I. Mariolis, S. Malasiotis,
and D. Tzovaras, “Real-time 3D reconstruction in minimally
invasive surgery with quasi-dense matching,” in IEEE Int. Conf.
on Imaging Sys. and Tech.

IEEE, 2018, pp. 1–6.

[25] J. Cartucho, S. Tukra, Y. Li, D. S. Elson, and S. Giannarou,
“VisionBlender: a tool to efﬁciently generate computer vision
datasets for robotic surgery,” Computer Methods in Biomechan-
ics and Biomedical Engineering: Imaging & Visualization, pp.
1–8, 2020.

[26] L. Zhang, M. Ye, P. Giataganas, M. Hughes, and G.-Z. Yang,
“Autonomous scanning for endomicroscopic mosaicing and 3D
fusion,” in Proc. IEEE Int. Conf. Robot. and Automation. IEEE,
2017, pp. 3587–3593.

[27] J. Zhan, J. Cartucho, and S. Giannarou, “Autonomous tissue
scanning under free-form motion for intraoperative tissue char-
acterisation,” in Proc. IEEE Int. Conf. Robot. and Automation.
IEEE, 2020, pp. 11 147–11 154.

[28] B. D. Lucas, T. Kanade et al., “An iterative image registration
technique with an application to stereo vision.” Vancouver,
British Columbia, 1981.

[29] Y. Shimasaki, Y. Iwahori, D. R. Neog, R. J. Woodham, and
M. Bhuyan, “Generating Lambertian image with uniform re-
ﬂectance for endoscope image,” International Workshop on
Advanced Image Technology.

[30] J. Engel, J. Stückler, and D. Cremers, “Large-scale direct SLAM
with stereo cameras,” in Proc. IEEE/RSJ Int. Conf. Intell. Robots
and Syst.

IEEE, 2015, pp. 1935–1942.

[31] J. Engel, V. Koltun, and D. Cremers, “Direct sparse odometry,”
IEEE Trans. Pattern Anal. Mach. Intell., vol. 40, no. 3, pp.
611–625, 2017.

[32] J. Song, Q. Zhu, J. Lin, and M. Ghaffari, “Bayesian dense
inverse searching algorithm for real-time stereo matching in
minimally invasive surgery,” arXiv preprint arXiv:2106.07136,
2021.

[33] P. E. Edwards, D. Psychogyios, S. Speidel, L. Maier-
Hein, and D. Stoyanov, “SERV-CT: A disparity dataset
endoscopic 3D
from cone-beam CT for validation of
reconstruction,” Medical
p.
102302, 2022. [Online]. Available: https://www.sciencedirect.
com/science/article/pii/S1361841521003479

Image Analysis,

vol.

76,

[34] D. Scharstein and R. Szeliski, “A taxonomy and evaluation
of dense two-frame stereo correspondence algorithms,” Int. J.
Comput. Vis., vol. 47, no. 1, pp. 7–42, 2002.

[35] C. Lin, Y. Li, G. Xu, and Y. Cao, “Optimizing ZNCC calcu-
lation in binocular stereo matching,” Signal Processing: Image
Communication, vol. 52, pp. 64–73, 2017.

[36] J. Sun, N.-N. Zheng, and H.-Y. Shum, “Stereo matching using
belief propagation,” IEEE Trans. Pattern Anal. Mach. Intell.,
vol. 25, no. 7, pp. 787–800, 2003.

[37] A. Klaus, M. Sormann, and K. Karner, “Segment-based stereo
matching using belief propagation and a self-adapting dissimi-
larity measure,” in Proc. Int. Conf. Pattern Recog., vol. 3. IEEE,
2006, pp. 15–18.

[38] H. Hirschmuller, “Stereo processing by semiglobal matching
information,” IEEE Trans. Pattern Anal. Mach.

and mutual
Intell., vol. 30, no. 2, pp. 328–341, 2007.

[39] K.-J. Yoon and I. S. Kweon, “Adaptive support-weight approach
for correspondence search,” IEEE Trans. Pattern Anal. Mach.
Intell., vol. 28, no. 4, pp. 650–656, 2006.

[40] D. Min, J. Lu, and M. N. Do, “A revisit to cost aggregation
in stereo matching: How far can we reduce its computational
redundancy?” in Proc. IEEE Int. Conf. Comput. Vis.
IEEE,
2011, pp. 1567–1574.

[41] K. Zhang, J. Lu, and G. Lafruit, “Cross-based local stereo
matching using orthogonal integral images,” IEEE Trans. Cir-
cuits Syst. Video Technol., vol. 19, no. 7, pp. 1073–1079, 2009.
[42] Q. Yang, “A non-local cost aggregation method for stereo
matching,” in Proc. IEEE Conf. Comput. Vis. Pattern Recog.
IEEE, 2012, pp. 1402–1409.

[43] A. Kendall, H. Martirosyan, S. Dasgupta, P. Henry, R. Kennedy,
A. Bachrach, and A. Bry, “End-to-end learning of geometry and
context for deep stereo regression,” in Proc. IEEE Int. Conf.
Comput. Vis., 2017, pp. 66–75.

[44] J.-R. Chang and Y.-S. Chen, “Pyramid stereo matching net-
work,” in Proc. IEEE Conf. Comput. Vis. Pattern Recog., 2018,
pp. 5410–5418.

[45] X. Guo, K. Yang, W. Yang, X. Wang, and H. Li, “Group-wise
correlation stereo network,” in Proc. IEEE Conf. Comput. Vis.
Pattern Recog., 2019, pp. 3273–3282.

[46] S. Baker and I. Matthews, “Lucas-Kanade 20 years on: A
unifying framework,” Int. J. Comput. Vis., vol. 56, no. 3, pp.

19

221–255, 2004.

[47] J. Lafferty, A. McCallum, and F. C. Pereira, “Conditional
random ﬁelds: Probabilistic models for segmenting and labeling
sequence data,” 2001.

[48] H. Larochelle and Y. Bengio, “Classiﬁcation using discrimi-
native restricted boltzmann machines,” in Int. Conf. on Mach.
Learning, 2008, pp. 536–543.

[49] A. Quattoni, M. Collins, and T. Darrell, “Conditional random
ﬁelds for object recognition,” Proc. Advances Neural Inform.
Process. Syst. Conf., vol. 17, pp. 1097–1104, 2004.

[50] A. Quattoni, S. Wang, L.-P. Morency, M. Collins, and T. Darrell,
“Hidden conditional random ﬁelds,” IEEE Trans. Pattern Anal.
Mach. Intell., vol. 29, no. 10, pp. 1848–1852, 2007.

[51] M. G. Uzunbas, C. Chen, and D. Metaxas, “An efﬁcient
conditional random ﬁeld approach for automatic and interactive
neuron segmentation,” Medical image analysis, vol. 27, pp. 31–
44, 2016.

[52] D. A. Reynolds, “Gaussian mixture models.” Encyclopedia of

biometrics, vol. 741, pp. 659–663, 2009.

[53] C. E. Rasmussen et al., “The inﬁnite gaussian mixture model.”
in Proc. Advances Neural Inform. Process. Syst. Conf., vol. 12,
1999, pp. 554–560.

[54] Y. Jiang and H. F. Leung, “Gaussian mixture model and gaus-
sian supervector for image classiﬁcation,” in IEEE Int. Conf. on
Digital Signal Process.

IEEE, 2018, pp. 1–5.

[55] S. Ge and G. Fan, “Non-rigid articulated point set registration
with local structure preservation,” in Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition Work-
shops, 2015, pp. 126–133.

[56] C. M. Bishop, Pattern Recognition and Machine Learning (In-
formation Science and Statistics). Berlin, Heidelberg: Springer-
Verlag, 2006.

[57] N. N. Schraudolph, “A fast, compact approximation of the
exponential function,” Neural Computation, vol. 11, no. 4, pp.
853–862, 1999.

[58] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan,
T. Killeen, Z. Lin, N. Gimelshein, L. Antiga et al., “Pytorch:
An imperative style, high-performance deep learning library,”
Proc. Advances Neural Inform. Process. Syst. Conf., vol. 32,
pp. 8026–8037, 2019.

[59] S. Giannarou, M. Visentini-Scarzanella, and G.-Z. Yang, “Prob-
abilistic tracking of afﬁne-invariant anisotropic regions,” IEEE
Trans. Pattern Anal. Mach. Intell., vol. 35, no. 1, pp. 130–143,
2013.

[60] A. Geiger, P. Lenz, C. Stiller, and R. Urtasun, “Vision meets
robotics: The kitti dataset,” Int. J. Robot. Res., vol. 32, no. 11,
pp. 1231–1237, 2013.

[61] M. Menze, C. Heipke, and A. Geiger, “Object scene ﬂow,”
Journal of Photogrammetry and Remote Sensing, 2018.
[62] D. P. Kingma and J. Ba, “Adam: A method for stochastic

optimization,” arXiv preprint arXiv:1412.6980, 2014.

[63] X. Chen, Y. Wang, X. Chen, and W. Zeng, “S2R-DepthNet:
Learning a generalizable depth-speciﬁc structural representa-
tion,” in Proc. IEEE Conf. Comput. Vis. Pattern Recog., 2021,
pp. 3034–3043.

[64] A. Atapour-Abarghouei and T. P. Breckon, “Real-time monocu-
lar depth estimation using synthetic data with domain adaptation
via image style transfer,” in Proc. IEEE Conf. Comput. Vis.
Pattern Recog., 2018, pp. 2800–2810.

[65] C. Zheng, T.-J. Cham, and J. Cai, “T2net: Synthetic-to-realistic
translation for solving single-image depth estimation tasks,” in
Proc. European Conf. Comput. Vis., 2018, pp. 767–783.

