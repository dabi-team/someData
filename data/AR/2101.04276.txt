1
2
0
2

n
a
J

2
1

]
E
M

.
t
a
t
s
[

1
v
6
7
2
4
0
.
1
0
1
2
:
v
i
X
r
a

High-Dimensional Low-Rank Tensor Autoregressive

Time Series Modeling

Di Wang†, Yao Zheng‡ and Guodong Li†

†University of Hong Kong and ‡University of Connecticut

January 13, 2021

Abstract

Modern technological advances have enabled an unprecedented amount of struc-

tured data with complex temporal dependence, urging the need for new methods to

eﬃciently model and forecast high-dimensional tensor-valued time series. This paper

provides the ﬁrst practical tool to accomplish this task via autoregression (AR). By

considering a low-rank Tucker decomposition for the transition tensor, the proposed

tensor autoregression can ﬂexibly capture the underlying low-dimensional tensor dy-

namics, providing both substantial dimension reduction and meaningful dynamic factor

interpretation. For this model, we introduce both low-dimensional rank-constrained es-

timator and high-dimensional regularized estimators, and derive their asymptotic and

non-asymptotic properties.

In particular, by leveraging the special balanced struc-

ture of the AR transition tensor, a novel convex regularization approach, based on

the sum of nuclear norms of square matricizations, is proposed to eﬃciently encourage

low-rankness of the coeﬃcient tensor. A truncation method is further introduced to

consistently select the Tucker ranks. Simulation experiments and real data analysis

demonstrate the advantages of the proposed approach over various competing ones.

Keywords: dimension reduction; high-dimensional time series; nuclear norm; tensor de-

composition; tensor-valued data

1

 
 
 
 
 
 
1

Introduction

Modern technological development has made available enormous quantities of data, many

of which are structured and collected over time. Tensor-valued time series data, namely ob-

servations on a set of variables structured in a tensor form collected over time, have become

increasingly common in a wide variety of areas. Examples include multiple macroeconomic

indices time series for multiple countries (Chen et al., 2020c), dynamic inter-regional trans-

port ﬂow data (Chen and Chen, 2019), and sequential image and video processing (Walden

and Serroukh, 2002), among many others.

To model temporal dependencies of tensor-valued time series data, naturally one might

resort to vectorization of the tensor-valued observations so that conventional vector-valued

time series models can be directly applied. For instance, let Yt ∈ Rp1×···×pd be the tensor-

valued observation at time t, for t = 1, . . . , T , where T is the sample size. Then an obvious

ﬁrst step is to conduct the following vector autoregression (VAR) for its vectorization:

vec(Yt) = Avec(Yt−1) + vec(Et),

(1)

where A ∈ Rp×p is the unknown transition matrix, with p = (cid:81)d

i=1 pi, and Et ∈ Rp1×···×pd
is the tensor-valued random error at time t. Although model (1) is potent in that it takes

into account the linear association between every variable in Yt and that in Yt−1, it clearly

suﬀers from two major drawbacks:

• The vectorization will destroy the important multidimensional structural information

inherently embedded in the tensor-valued observations, resulting in a lack of inter-

pretability;

• The number of unknown parameters, p2 = ((cid:81)d

i=1 pi)2, can be formidable even for small

d and pi; e.g., even when d = 3 and p1 = p2 = p3 = 3, the number of unknown

parameters will be as large as (3 × 3 × 3)2 = 729, while the sample size T often has a

similar magnitude in practice; see, e.g., the real data examples in Section 7.

In this work, motivated by the idea of Tucker decomposition (Tucker, 1966), we propose

2

the Low-Rank Tensor Autoregressive (LRTAR) model through folding the p × p transition

matrix A in (1) into the 2d-th-order transition tensor A ∈ Rp1×···×pd×p1×···×pd which is as-

sumed to have low multilinear ranks (r1, . . . , r2d). That is, ri can be much smaller than

pi, where pd+i = pi for i = 1, . . . , d. As a result, the aforementioned drawbacks of the

vectorization approach can be overcome by the proposed model:

• As we will show in Section 3, the proposed model can ﬂexibly retain the distinct

structural information across all modes of observed tensor process, encapsulating an

interpretable low-dimensional tensor dynamic relationship between Yt and Yt−1.

• As the transition tensor A is assumed to have low multilinear ranks, the parame-

ter space is simultaneously constrained in 2d directions, reducing its dimension from
((cid:81)d

i=1 pi)2 drastically to (cid:81)2d

i=1 ri(pi − ri) + (cid:80)d

i=1 rd+i(pi − rd+i).

i=1 ri + (cid:80)d

Recently there has been a rapidly emerging interest in high-dimensional matrix- and

tensor-valued time series analysis, particularly through factor models, e.g., the matrix factor

model proposed in Wang et al. (2019), the constrained matrix factor model in Chen et al.

(2020a), and the tensor factor model in Chen et al. (2020c). Factor models are powerful tools

for dimension reduction with great interpretability. However, unlike autoregression, factor

models do not seek to explicitly model the temporal dependency and thus by themselves

cannot be directly used for forecasting. On the other hand, despite the extensive literature

on high-dimensional VAR models in recent years (e.g. Negahban and Wainwright, 2011;

Basu and Michailidis, 2015; Han et al., 2015a; Wang et al., 2020; Zheng and Cheng, 2020),

counterparts able to meet the particular needs of matrix- and tensor-valued time series

modeling tasks have been rarely explored. The most relevant existing work in this direction

so far might be the matrix autoregressive (MAR) model in Chen et al. (2020b), where the

focus is on low-dimensional modeling; see also Hoﬀ (2015). As we will discuss in Section 3,

the proposed model includes the MAR model as a special case, but enjoys greater ﬂexibility

and a more drastic reduction of the dimensionality; see also Figure 1 for an illustration.

The estimation of the proposed model is studied under both low- and high-dimensional

3

scaling. When the sample size T is suﬃciently large and the transition tensor A is exactly

low-rank, we consider the rank-constrained estimation method and prove the asymptotic

normality for the proposed low-Tucker-rank (LTR) estimator. Under the high-dimensional

setup, we consider a more general and natural setting where A can be well approximated

by a low-Tucker-rank tensor, and develop regularized estimation methods based on nuclear-

norm-type penalties. We ﬁrst study the Sum of Nuclear (SN) norm regularizer, deﬁned as

the sum of nuclear norms of all one-mode matricizations of A, and derive the non-asymptotic

estimation error bound for the corresponding estimator. The SN norm regularizer has been

widely used in the literature for various low-rank tensor problems (Gandy et al., 2011;

Tomioka et al., 2011; Liu et al., 2013; Raskutti et al., 2019). Its major strength lies in the

fact that the summation of nuclear norms allows enforcing the low-rankness simultaneously

across all modes of the tensor. In contrast, if only a single nuclear norm is used, the low-

rankness of only one mode will be accounted for, obviously leading to a much less eﬃcient

estimator.

However, although penalizing multiple one-mode matricizations of A simultaneously is

far more eﬃcient than penalizing only one of them, the SN regularized estimator suﬀers

from serious eﬃciency loss due to the fat-and-short shape of the one-mode matricizations.

Note that the low-rankness in fact can also be encouraged by penalizing nuclear norms

of multi-mode matricizations. For instance, the conventional Matrix Nuclear (MN) norm

regularized estimator (Negahban and Wainwright, 2011) simply penalizes the nuclear norm

of the transition matrix A in representation (1), which under the proposed LRTAR model

actually is a square-shaped multi-mode matricization, namely square matricization, of the

transition tensor A. As we will show in Theorem 3 and simulations, even though the MN

regularizer incorporates only one single square matricization, it still clearly beats the SN

regularizer, since the former avoids the eﬃciency bottleneck caused by the imbalance of the

one-mode matricization.

Indeed, due to the autoregressive form of the proposed model, the transition tensor A

has a special balanced structure in the sense that its ﬁrst d modes have exactly the same

4

dimensions as its other d modes. As a result, actually many diﬀerent square matricizations

of A can be formed by appropriately pairing up its modes; see Section 5.2 for details. This

naturally motivates us to propose a new regularizer that combines the strengths of both

SN and MN norms. Speciﬁcally, for the proposed tensor autoregression, we introduce a

novel Sum of Square-matrix Nuclear (SSN) norm regularizer, deﬁned as the sum of nuclear

norms of all the p × p square matricizations of the transition tensor A. The SSN regularizer

is expected to be superior to the MN, since it simultaneously encourages the low-rankness

across all possible square matricizations of A rather than only one of them. Moreover, thanks

to the use of square matricizations, the SSN regularized estimator is provably more eﬃcient

than the SN regularized one; see Theorem 4 and the last simulation experiment in Section

6. Note that the adoption of a more balanced (square) matricization to improve estimation

performance was proposed in Mu et al. (2014) for low-rank tensor completion problems,

where only a single square matricization was penalized, similarly to the MN regularizer.

This work is also related to the literature of matrix-variate regression and tensor regres-

sion for independent data. The matrix-variate regression model in Ding and Cook (2018) has

the same basic bilinear form as that of the MAR model mentioned earlier, while an envelope

method was introduced to further reduce the dimension. Raskutti et al. (2019) proposed

a multi-response tensor regression model, where they mainly studied the third-order coef-

ﬁcient tensor and the SN regularization which is known to be statistically sub-optimal for

higher-order tensor estimation.

In contrast, we study the model for general higher-order

tensor-valued time series. Moreover, our SSN regularized estimator has a much faster sta-

tistical convergence rate than the SN estimator. Recently, Chen et al. (2019) and Han

et al. (2020) studied non-convex projected gradient descent methods for tensor regression.

While their non-convex approaches require exact low-rankness with known Tucker ranks,

our methods can handle both exact and approximate low-rankness and select the unknown

ranks consistently in the exactly low-rank case. In addition, existing literature on tensor

regression has only considered independent data and Gaussian time series data, whereas we

allow sub-Gaussianity of the time series. This is a non-trivial relaxation, since unlike the

5

Gaussian case, sub-Gaussian time series cannot be linearly transformed into independent

samples.

We summarize the most important contributions of this paper as follows:

(i) This paper provides the ﬁrst practical tool to model and forecast general structured,

high-dimensional data with complex temporal dependence via tensor autoregression.

By ﬂexibly and eﬃciently capturing the underlying low-dimensional tensor dynamics,

the proposed model delivers signiﬁcant dimension reduction, meaningful structural

interpretation and favorable forecast performance.

(ii) By exploiting the special balanced structure of the transition tensor A, a novel SSN

regularization approach is introduced to simultaneously and eﬃciently encourage low-

rankness across all square matricizations of A, outperforming both the SN and MN

methods under both exact and approximate low-rankness. For exactly low-rank A, a

truncated estimator is further introduced for consistent rank selection.

(iii) On the technical front, by establishing a novel martingale-based concentration bound,

this paper relaxes the conventional Gaussian assumption in the literature on high-

dimensional time series to sub-Gaussianity. This technique is generally applicable to

the non-asymptotic estimation theory for high-dimensional time series models with a

VAR representation and hence is of independent interest.

The rest of the paper is organized as follows. Section 2 introduces basic notation and

tensor algebra. Section 3 presents the proposed LRTAR model. Section 4 studies the low-

dimensional least squares estimator and its asymptotic properties. The high-dimensional

regularized estimation is covered in Section 5, where we develop the non-asymptotic theory

for three regularized estimators and rank selection consistency for the truncated estimator.

Sections 6 and 7 present simulation studies and real data analysis, respectively. Section 8

concludes with a brief discussion. All technical proofs are relegated to the Appendix.

6

2 Preliminaries: Notation and Tensor Algebra

Tensors, also known as multidimensional arrays, are natural higher-order extensions of ma-

trices. The order of a tensor is known as the dimension, way or mode, so a multidimensional

array X ∈ Rp1×···×pd is called a d-th-order tensor. We refer readers to Kolda and Bader

(2009) for a detailed review of basic tensor algebra.

Throughout this paper, we denote vectors by boldface small letters, e.g. x, y, matrices

by boldface capital letters, e.g. X, Y , and tensors by boldface Euler capital letters, e.g. X,

Y. For any two real-valued sequences xk and yk, we write xk (cid:38) yk if there exists a constant

c > 0 such that xk ≥ cyk for all k, and write xk (cid:29) yk if limk→∞ yk/xk = 0. In addition, write

xk (cid:16) yk if xk (cid:38) yk and yk (cid:38) xk. We use C to denote a generic positive constant, which is

independent of the dimensions and the sample size.

For a generic matrix X, we let X (cid:62), (cid:107)X(cid:107)F, (cid:107)X(cid:107)op, (cid:107)X(cid:107)∗, vec(X), and σj(X) denote

its transpose, Frobenius norm, operator norm, nuclear norm, vectorization, and j-th largest

singular value, respectively. For any matrix X ∈ Rm×n, recall that the nuclear norm and its

dual norm, the operator norm, are deﬁned as

(cid:107)X(cid:107)∗ =

min(m,n)
(cid:88)

j=1

σj(X) and (cid:107)X(cid:107)op = σ1(X).

For any square matrix X, we let λmin(X) and λmax(X) denote its minimum and maximum

eigenvalues. For any real symmetric matrices X and Y , we write X ≤ Y if Y − X is a

positive semideﬁnite matrix.

Matricization, also known as unfolding, is the process of reordering the elements of a

third- or higher-order tensor into a matrix. The most commonly used matricization is the

one-mode matricization deﬁned as follows. For any d-th-order tensor X ∈ Rp1×···×pd, its
mode-s matricization X(s) ∈ Rps×p−s, with p−s = (cid:81)d

i=1,i(cid:54)=s pi, is the matrix obtained by

setting the s-th tensor mode as its rows and collapsing all the others into its columns, for

s = 1, . . . , d. Speciﬁcally, the (i1, . . . , id)-th element of X is mapped to the (is, j)-th element

7

of X(s), where

j = 1 +

d
(cid:88)

k=1
k(cid:54)=s

(ik − 1)Jk with Jk =

p(cid:96).

k−1
(cid:89)

(cid:96)=1
(cid:96)(cid:54)=s

The above one-mode matricization can be extended to the multi-mode matricization by

combining multiple modes to rows and combining the rest to columns of a matrix. For any
i∈S pi-by-(cid:81)
index subset S ⊂ {1, . . . , d}, the multi-mode matricization X[S] is the (cid:81)
matrix whose (i, j)-th element is mapped from the (i1, . . . , id)-th element of X, where

i /∈S pi

i = 1 +

(cid:88)

k∈S

(ik − 1)Ik and j = 1 +

(cid:88)

(ik − 1)Jk, with Ik =

k /∈S

p(cid:96) and Jk =

(cid:89)

(cid:96)∈S
(cid:96)<k

p(cid:96).

(cid:89)

(cid:96) /∈S
(cid:96)<k

Note that the modes in the multi-mode matricization are collapsed following their original
[S(cid:123)], where S(cid:123) = {1, . . . , d} \ S is the complement

order 1, . . . , d. Moreover, it holds X[S] = X(cid:62)

of S. In addition, the one-mode matricization X(s) deﬁned above is simply X[{s}].

We next review the concepts of tensor-matrix multiplication, tensor generalized inner

product and norm. For any d-th-order tensor X ∈ Rp1×···×pd and matrix Y ∈ Rqk×pk

with 1 ≤ k ≤ d, the mode-k multiplication X ×k Y produces a d-th-order tensor in

Rp1×···×pk−1×qk×pk+1×···×pd deﬁned by

(X ×k Y )i1···ik−1jik+1...id

=

pk(cid:88)

ik=1

Xi1···idY jik.

For any two tensors X ∈ Rp1×p2×···×pn and Y ∈ Rp1×p2×···pm with n ≥ m, their generalized

inner product (cid:104)X, Y(cid:105) is the (n − m)-th-order tensor in Rpm+1×···×pn deﬁned by
p2
(cid:88)

pm
(cid:88)

p1
(cid:88)

(cid:104)X, Y(cid:105)im+1...in =

· · ·

Xi1i2...imim+1...in

Yi1i2...im,

(2)

i1=1

i2=1

im=1

where 1 ≤ im+1 ≤ pm+1, . . . , 1 ≤ in ≤ pn. In particular, when n = m, it reduces to the

conventional real-valued inner product. In addition, the Frobenius norm of any tensor X is
deﬁned as (cid:107)X(cid:107)F = (cid:112)(cid:104)X, X(cid:105).

Some basic properties of the tensor generalized inner product are as follows. Let X ∈

Rp1×p2×···×pn, Y ∈ Rp1×p2×···pm and Z ∈ Rp1×···×pk−1×qk×pk+1···×pm be tensors with n ≥ m ≥

k ≥ 1. If Y ∈ Rqk×pk, then

(cid:104)X ×k Y , Z(cid:105) = (cid:104)X, Z ×k Y (cid:62)(cid:105).

(3)

8

If Z ∈ Rqm+j ×pm+j with 1 ≤ j ≤ n − m, then

Moreover,

(cid:104)X, Y(cid:105) ×j Z = (cid:104)X ×m+j Z, Y(cid:105).

vec((cid:104)X, Y(cid:105)) = X[S]vec(Y),

(4)

(5)

where S = {m + 1, . . . , n}, and when m = n, X[∅] = vec(X)(cid:62).

Finally, we summarize some concepts and useful results of the Tucker decomposition

(Tucker, 1966; De Lathauwer et al., 2000). For any tensor X ∈ Rp1×···×pd, its multilinear

ranks (r1, . . . , rd) are deﬁned as the matrix ranks of its one-mode matricizations, namely

ri = rank(X(i)), for i = 1, . . . , d. Note that ri’s are analogous to the row and column ranks

of a matrix, but are not necessarily equal for third- and higher-order tensors. Suppose that

X has multilinear ranks (r1, . . . , rd). Then X has the following Tucker decomposition:

X = Y ×1 Y 1 ×2 Y 2 · · · ×d Y d = Y ×d

i=1 Y i,

(6)

where Y i ∈ Rpi×ri for i = 1, . . . , d are the factor matrices and Y ∈ Rr1×···×rd is the core

tensor. Hence, the multilinear ranks are also called Tucker ranks.

If X has the Tucker decomposition in (6), then we have the following results for its one-

and multi-mode matricizations:

X(s) = Y sY(s)(Y d ⊗ · · · ⊗ Y s+1 ⊗ Y s−1 · · · ⊗ Y 1)(cid:62) = Y sY(s)(⊗i(cid:54)=sY i)(cid:62),

s = 1, . . . , d,

and

X[S] = (⊗i∈SY i)Y[S](⊗i /∈SY i)(cid:62), S ⊂ {1, . . . , d},

(7)

where ⊗i(cid:54)=s, ⊗i∈S and ⊗i /∈S are matrix Kronecker products operating in the reverse order

within the corresponding index sets.

Note that for any nonsingular matrices Oi ∈ Rri×r1 for i = 1, . . . , d, it holds

Y ×d

i=1 Y i = (Y ×d

i=1 Oi) ×d

i=1 (Y iO−1

i ).

This indicates that the Tucker decomposition in (6) is not unique unless appropriate iden-

tiﬁability constraints are imposed.

In this paper, to ﬁx ideas, we will focus on a special

9

Tucker decomposition called the higher-order singular value decomposition (HOSVD). In

the HOSVD, the factor matrix Y i in (6) is deﬁned as the tall orthonormal matrix con-

sisting of the top ri left singular vectors of X(i), for i = 1, . . . , d. Consequently, the core

tensor Y = X ×d

i=1 Y (cid:62)

i has the all-orthogonal property that Y(i)Y(cid:62)

(i) is a diagonal matrix for

i = 1, . . . , d.

3 Low-Rank Tensor Autoregression

For the tensor-valued time series {Yt}T

t=1, we propose the following Low-Rank Tensor Au-

toregressive (LRTAR) model:

Yt = (cid:104)A, Yt−1(cid:105) + Et,

(8)

where A ∈ Rp1×···×pd×p1×···×pd is the 2d-th-order transition tensor which is assumed to have

multilinear ranks (r1, . . . , r2d), with ri = rank(A(i)), (cid:104)·, ·(cid:105) is the generalized tensor inner

product deﬁned in (2), and Et ∈ Rp1×···×pd is the mean-zero random error at time t with

possible dependencies among its contemporaneous elements. Denote S1 = {1, 2, . . . , d} and

S2 = {d + 1, d + 2, . . . , 2d}. Note that by (5), model (8) can be written into the VAR form
in (1) with transition matrix A = A[S2].

Then, we have the higher-order singular value decomposition (HOSVD) of A,

A = G ×2d

i=1 U i,

(9)

where G ∈ Rr1×···×r2d is the core tensor, and each U i ∈ Rpi×ri is the orthonormal factor

matrix deﬁned as the top ri left singular vectors of A(i), for 1 ≤ i ≤ 2d. Thus, by (7), the

VAR representation of model (8) can be written as

vec(Yt)
(cid:124) (cid:123)(cid:122) (cid:125)yt

= (⊗i∈S2U i)G[S2](⊗i∈S1U i)(cid:62)
(cid:123)(cid:122)
(cid:125)
A[S2]

(cid:124)

vec(Yt−1)
(cid:125)
(cid:123)(cid:122)
(cid:124)
yt−1

+ vec(Et)
(cid:124) (cid:123)(cid:122) (cid:125)et

,

(10)

where yt = vec(Yt) and et = vec(Et).

In contrast to the conventional VAR model in (1) which has p2 unknown parameters,

where p = (cid:81)d

i=1 pi, the dimension of the parameter space for model (8) is reduced substan-

10

tially to

2d
(cid:89)

i=1

ri +

d
(cid:88)

i=1

ri(pi − ri) +

d
(cid:88)

i=1

rd+i(pi − rd+i),

(11)

which is computed by subtracting the number of constraints due to the orthonormality of

U i’s and the all-orthogonal property of G from the total number of parameters.

By the VAR representation in (10), we immediately have the necessary and suﬃcient

condition for the existence of a unique strictly stationary solution to model (8) as follows.

Assumption 1. The spectral radius of A[S2] is strictly less than one.

The proposed model implies an interesting low-dimensional tensor dynamical structure.

To be speciﬁc, by (3), (4) and the orthonormality of U i, it can be shown that (8) together

with (9) implies

Yt ×d
i=1 U (cid:62)
d+i
(cid:124)
(cid:125)
(cid:123)(cid:122)
rd+1×rd+2×···×r2d

= (cid:10)G, Yt−1 ×d
i=1 U (cid:62)
i
(cid:125)
(cid:123)(cid:122)
(cid:124)
r1×r2×···×rd

(cid:11) + Et ×d

i=1 U (cid:62)

d+i.

(12)

Note that in (12), Yt and Et are both projected onto a low-dimensional space via the U d+i’s,

while Yt−1 is projected onto another low-dimensional space via the U i’s, with 1 ≤ i ≤ d.

Hence, (12) is essentially a low-dimensional tensor regression deﬁned on the projections of

Yt and Yt−1. Element-wisely, the low-dimensional tensor Yt ×d
(cid:81)d

i=1 rd+i multilinear response factors, Yt−1 ×d

i=1 U (cid:62)

i as (cid:81)d
d+i as multilinear error factors. For this reason, we call U d+1, . . . , U 2d the

i=1 ri multilinear predictor factors,

i=1 U (cid:62)

d+i can be interpreted as

and Et ×d

i=1 U (cid:62)

response factor matrices, and U 1, . . . , U d the predictor factor matrices.

We discuss some special cases of the proposed model below.

Example 1. For simplicity, we ﬁrst consider the case with d = 2, so Yt ≡ Y t, Et ≡ Et ∈

Rp1×p2 are matrices. Then the VAR representation in (10) becomes

vec(Y t) = (U 4 ⊗ U 3)G[{3,4}](U (cid:62)

2 ⊗ U (cid:62)

1 )vec(Y t−1) + vec(Et),

(13)

and the low-dimensional representation in (12) becomes

U (cid:62)

3 Y tU 4 = (cid:10)G, U (cid:62)

1 Y t−1U 2

(cid:11) + U (cid:62)

3 EtU 4,

11

where G ∈ Rr1×···×r4. It is interesting to compare this model with the matrix autoregressive

(MAR) model in Chen et al. (2020b) and Hoﬀ (2015), which is deﬁned by

Y t = B1Y t−1B(cid:62)

2 + Et,

where B1 ∈ Rp1×p1 and B2 ∈ Rp2×p2, whose vector form is

vec(Y t) = (B2 ⊗ B1)vec(Y t−1) + vec(Et).

(14)

(15)

It can be easily seen that if r1 = r3 = p1, r2 = r4 = p2, U 3 = I p1, U 4 = I p2, and
G[{3,4}] = (B2 ⊗ B1)(U 2 ⊗ U 1), then (13) becomes exactly (15). Thus, the MAR model in

(14) can be viewed as a special case of the proposed model without reducing dimensions pi’s to

ri’s and without transforming Y t; see Figure 1 for an illustration. The above comparison also

applies to the general case with d ≥ 3. The tensor version of the MAR model is considered

in Hoﬀ (2015) and is deﬁned as

Yt = Yt−1 ×d

i=1 Bi + Et,

(16)

where Bi ∈ Rpi×pi for i = 1, . . . , d. We call (16) the multilinear tensor autoregressive

(MTAR) model. Note that its vector form is

vec(Yt) = (Bd ⊗ · · · ⊗ B1)vec(Yt−1) + vec(Et).

(17)

Similarly, (17) is a special case of (10) with ri = rd+i = pi, U d+i = I pi, for i = 1, . . . , d, and
G[S2] = (⊗i∈S1Bi)(⊗i∈S1U i). Obviously, the number of unknown parameters in the MTAR
model, (cid:80)d

i , is much larger than that of the proposed model as shown in (11). Also note

i=1 p2

that Chen et al. (2020b) focuses on the low-dimensional estimation and its asymptotic theory,

while Hoﬀ (2015) considers a Bayesian estimation method.

Example 2. In the special case where U d+i = U i and rd+i = ri for i = 1, . . . , d, the

proposed model may be understood from the perspective of dynamic factor modeling (Stock

and Watson, 2011; Bai and Wang, 2016) for tensor-valued time series. Speciﬁcally, consider

the following model:

Yt = Ft ×d

i=1 U i, Ft = (cid:104)G, Ft−1(cid:105) + Ht,

(18)

12

Figure 1: Illustration of the MAR model and the proposed LRTAR model in the case of

d = 2.

where Yt ∈ Rp1×···×pd is the observed tensor-valued time series, Ft ∈ Rr1×···×rd represents
(cid:81)d
i=1 ri factors, and U i ∈ Rpi×ri are orthonormal matrices for i = 1, . . . , d. Here Ft follows
the tensor autoregression (TAR) with transition tensor G ∈ Rr1×···×rd×r1×···×rd and random

error Ht. Note that (18) can be rewritten as

Yt = (cid:10)G ×d

i=1 U i ×2d

i=d+1 U i, Yt−1

(cid:11) + Ht ×d

i=1 U i.

Thus, model (18) is a special case of the proposed model with U d+i = U i and rd+i = ri

for i = 1, . . . , d, and Et = Ht ×d

model in the form of Yt = Ft ×d

i=1 U i. Chen et al. (2020c) introduces the tensor factor
i=1 U i + Et without an explicit modeling of the latent factors
Ft. Hence, model (18) may be regarded as a special tensor factor model with autoregressive

dynamic factors, but without any random error in the model equation of Yt.

4 Low-Dimensional Least Squares Estimation

We consider the parameter estimation for the low-dimensional case where the sample size T

is suﬃciently large such that the dimension of the parameter space can be assumed ﬁxed.

Throughout this section, we assume that the data are generated from the proposed model

in Section 3, where the true transition tensor A is exactly low-Tucker rank with multilinear

ranks (r1, . . . , r2d). This assumption will be relaxed under the high-dimensional setup in the

13

<𝓖𝓖,>𝑝𝑝1𝑝𝑝2𝑝𝑝2𝑝𝑝1𝑟𝑟3𝑝𝑝1𝑝𝑝2𝑝𝑝1𝑟𝑟4𝑝𝑝1𝑝𝑝2𝑟𝑟2𝑝𝑝2𝑝𝑝1𝑝𝑝2𝑝𝑝1𝑟𝑟4𝑼𝑼3′𝒀𝒀𝑡𝑡𝑼𝑼4=𝒀𝒀𝑡𝑡=𝑩𝑩1𝒀𝒀𝑡𝑡−1𝑩𝑩2′+𝑬𝑬𝑡𝑡𝑝𝑝1𝑝𝑝2𝑟𝑟1𝑟𝑟3𝑝𝑝2𝑝𝑝2𝑝𝑝1𝑝𝑝2𝑝𝑝1𝑼𝑼3′𝑼𝑼1′𝒀𝒀𝑡𝑡−1𝑼𝑼2+𝑬𝑬𝑡𝑡𝑼𝑼4MAR model:LRTAR model:next section.

Suppose that the true ranks (r1, . . . , r2d) of the exactly low-rank tensor A are known; we

relegate the rank selection to Section 5.3. Then the parameters can be estimated by

(cid:98)ALTR = (cid:98)G ×2d

i=1 (cid:98)U i = arg min

G,U i

T
(cid:88)

t=1

(cid:13)
(cid:13)Yt − (cid:104)G ×2d

i=1 U i, Yt−1(cid:105)(cid:13)
2
F .
(cid:13)

(19)

We call (cid:98)ALTR the low-Tucker-rank (LTR) estimator. Note that the minimization in (19)
is unconstrained, so the Tucker decomposition of (cid:98)ALTR is not unique.
more than one solution of (cid:98)G and (cid:98)U i’s corresponding to the same (cid:98)ALTR. Due to the lack of

Indeed, there are

identiﬁability of the Tucker decompositions, standard asymptotics of the maximum likelihood

estimation cannot apply directly. Nevertheless, we can still derive the asymptotic distribution

of (cid:98)ALTR using the asymptotic theory for overparameterized models in Shapiro (1986).

Recall that S1 = {1, 2, . . . , d} S2 = {d + 1, d + 2, . . . , 2d}, yt = vec(Yt), and et = vec(Et).
Let θ = (vec(G[S2])(cid:62), vec(U 1)(cid:62), · · · , vec(U 2d)(cid:62))(cid:62) be the parameter vector, and let h =
h(θ) = vec(A[S2]) = vec((⊗i∈S2U i)G[S2](⊗i∈S1U i)(cid:62)) be the vectorization of the transition
matrix. Denote Σy = var(yt), Σe = var(et), and J = Σ−1
1, . . . , 2d, let P (i)

[S2] be the p2 × p2 permutation matrix such that vec(A[S2]) = P (i)

In addition, for i =
[S2]vec(A(i)).

e ⊗ Σy.

Then, it can be shown that the Jacobian matrix H := ∂h(θ)/∂θ is given by

H = (cid:0)(⊗i∈S1U i) ⊗ (⊗i∈S2U i), P (1)

[S2]

(cid:8)(cid:2)(⊗2d

i=1,i(cid:54)=1U i)G(cid:62)

(1)

(cid:3) ⊗ I p1

(cid:9) ,

P (2)
[S2]

(cid:8)(cid:2)(⊗2d

i=1,i(cid:54)=2U i)G(cid:62)

(2)

(cid:3) ⊗ I p2

(cid:9) , . . . , P (2d)
[S2]

(cid:8)(cid:2)(⊗2d

i=1,i(cid:54)=2dU i)G(cid:62)

(2d)

(20)

(cid:3) ⊗ I p2d

(cid:9) (cid:1),

where pd+i = pi for i = 1, . . . , d.

Theorem 1. Suppose that the time series {Yt} is generated by model (8) with E(cid:107)et(cid:107)4

2 < ∞,

and Assumption 1 holds. Then,

√

T vec(( (cid:98)ALTR − A)[S2]) → N (0, ΣLTR),

in distribution as T → ∞, where ΣLTR = H(H (cid:62)J H)†H (cid:62), and † is the Moore-Penrose

inverse.

Since the asymptotic theory for overparameterized models in Shapiro (1986) allows for

unidentiﬁability of the components G and U i’s in the decomposition of A, Theorem 1 does

14

not require that the Tucker decomposition of A is unique; see the proof of Theorem 1 in

Appendix A for more details.

Next we compare the result in Theorem 1 to those of two other consistent estimators for

the proposed model in the low-dimensional setup. Note that the rank of A[S2] in (10) is at
most s1 := min((cid:81)d
i=d+1 ri). Thus, A[S2] can be estimated by both the reduced-rank

i=1 ri, (cid:81)2d

regression (RRR) and ordinary least squares (OLS) methods,

(cid:98)ARRR = arg min

rank(B[S2])≤s1

1
T

T
(cid:88)

t=1

(cid:107)Yt − (cid:104)B, Yt−1(cid:105)(cid:107)2
F

(21)

and

(cid:98)AOLS = arg min

B

1
T

T
(cid:88)

(cid:107)Yt − (cid:104)B, Yt−1(cid:105)(cid:107)2
F.

t=1
Naturally, under model (8), (cid:98)ALTR is asymptotically more eﬃcient than (cid:98)ARRR and (cid:98)AOLS:

Corollary 1. If the conditions of Theorem 1 hold, then

T vec(( (cid:98)AOLS−A)[S2]) → N (0, ΣOLS)
T vec(( (cid:98)ARRR − A)[S2]) → N (0, ΣRRR) in distribution as T → ∞. Moreover, it holds

and

√

√

that ΣLTR ≤ ΣRRR ≤ ΣOLS.

To solve the minimization problem in (19), we propose an alternating least squares (ALS)

method. Speciﬁcally, by the vector representation of the proposed model in (10), the loss

function in (19) can be rewritten as

L(G, U 1, . . . , U 2d) =

T
(cid:88)

t=1

(cid:13)
(cid:13)yt − (⊗i∈S2U i)G[S2](⊗i∈S1U i)(cid:62)yt−1
(cid:13)

(cid:13)
2
(cid:13)
(cid:13)
2

=

T
(cid:88)

t=1

(cid:13)
(cid:13)
2
(cid:13)
(cid:13)
(cid:13)yt − m(θ)
(cid:13)
2

,

where m(θ) = (⊗i∈S2U i)G[S2](⊗i∈S1U i)(cid:62)yt−1 is the conditional mean of yt = vec(Yt) and
2d)(cid:62), with θ0 = vec(G[S2]) and
is a function of the parameter vector θ = (θ(cid:62)

1 , . . . , θ(cid:62)

0 , θ(cid:62)

θi = vec(U i) for i = 1, . . . , 2d. Note that m(θ) is linear in each θi while keeping the other

components θj with 0 ≤ j (cid:54)= i ≤ 2d ﬁxed. This is indeed because

m(θ) = (⊗i∈S2U i)G[S2]P k,1

(cid:8)(cid:2)(⊗i∈S1,i(cid:54)=kU i)(cid:62)(Yt−1)(cid:62)

(cid:9) P k,3vec(U k)

= P k,2
= (cid:8)(cid:2)y(cid:62)

(cid:8)(cid:2)(⊗i∈S2,i(cid:54)=d+kU i)(Mt−1)(cid:62)
t−1(⊗i∈S1U i)(cid:3) ⊗ (⊗i∈S2U i)(cid:9) vec(G[S2]),

(cid:3) ⊗ I pk

(k)

(k)

(cid:3) ⊗ I rk
(cid:9) vec(U d+k)

15

Algorithm 1 ALS algorithm for LTR estimator
Initialize: A(0) = (cid:98)ARRR or (cid:98)AOLS
HOSVD: A(0) ≈ G(0) ×2d

i=1 U (0)

i

, with multilinear ranks (r1, . . . , r2d).

repeat s = 0, 1, 2, . . .

for k = 1, . . . , d

U (s+1)

k ← arg min

U k

end for

for k = 1, . . . , d

(cid:80)T

t=1

i )G[S2]P k,1

(cid:13)
(cid:13)yt − (⊗i∈S2U (s)
(cid:13)
(cid:13)
2
(cid:13)
(cid:13)
2

vec(U k)

(cid:110)(cid:104)

(⊗i∈S1,i(cid:54)=kU (s)

i )(cid:62)(Yt−1)(cid:62)

(k)

(cid:105)

(cid:111)

⊗ I rk

U (s+1)

d+k ← arg min

U d+k

(cid:80)T

t=1

(cid:13)
(cid:13)
(cid:13)yt − P k,2

(cid:8) (cid:104)

(⊗i∈S2,i(cid:54)=d+kU (s)

i )(M(s+1)

t−1 )(cid:62)
(k)

(cid:105)

⊗ I pk

(cid:9)vec(U d+k)

(cid:13)
2
(cid:13)
(cid:13)
2

end for

G(s+1) ← arg min

G

A(s+1) ← G(s+1) ×2d

(cid:80)T

t=1

(cid:13)
(cid:13)
(cid:13)yt −
i=1 U (s+1)

i

(cid:110)(cid:104)

t−1(⊗i∈S1U (s+1)
y(cid:62)

i

(cid:105)
)

⊗ (⊗i∈S2U (s+1)

i

(cid:111)
)

vec(G[S2])

(cid:13)
2
(cid:13)
(cid:13)
2

until convergence

i=1 ri×(cid:81)d

i=1 ri, P k,2 ∈ Rp×p and P k,3 ∈ Rrkpk×rkpk are

for k = 1, . . . , d, where P k,1 ∈ R(cid:81)d
permutation matrices deﬁned such that P k,1vec(T(k)) = vec(T) for any T ∈ Rr1×···×rd,
k ), and Mt−1 ∈
P k,2vec(T(k)) = vec(T) for any T ∈ Rp1×···×pd, and P k,3vec(U k) = vec(U (cid:62)
Rrd+1×···×r2d is deﬁned such that vec(Mt) = G[S2](⊗i∈S1U i)(cid:62)yt−1. Therefore, we can update
each component parameter vector θi, and hence the corresponding G and U i’s, iteratively

by the least squares method. The resulting ALS algorithm is shown in Algorithm 1.

As mentioned, the minimization in (19) is unconstrained. Accordingly, to obtain (cid:98)ALTR,
no orthogonality constraint of G and U i’s is needed in Algorithm 1. Instead, we compute the
ﬁnal unique estimates of (cid:98)G and (cid:98)U i’s by the HOSVD operation based on the unconstrained
solution (cid:98)ALTR obtained from Algorithm 1. Speciﬁcally, we compute each (cid:98)U i uniquely as
the top ri left singular vectors of ( (cid:98)ALTR)(i) such that the ﬁrst element in each column of
(cid:98)U i is positive, and set (cid:98)G = [[ (cid:98)ALTR; (cid:98)U

(cid:62)
2d]]. Similar alternating algorithms without

(cid:62)
1 , · · · , (cid:98)U

imposing identiﬁcation constraints can be found in the literature of tensor decomposition;

16

see, e.g. Zhou et al. (2013) and Li et al. (2018).

5 High-Dimensional Regularized Estimation

5.1 Regularization via One-Mode Matricization

The methods in the previous section rely on the assumptions that the dimension is ﬁxed and

that the true transition tensor is exactly low-rank with known Tucker ranks. We next relax

both assumptions. Speciﬁcally, under the high-dimensional setup, we consider regularized

estimation of model (8) via diﬀerent nuclear-norm-type penalties and develop the corre-

sponding non-asymptotic theory under only an approximately low-Tucker-rank assumption

on the underlying true transition tensor.

In model (8), the exactly low-rank transition tensor A ∈ Rp1×···×p2d is subject to the

constraints ri = rank(A(i)), for i = 1, . . . , 2d. A commonly used convex relaxation of such

multilinear rank constraints is the regularization via the sum of nuclear (SN) norms of all

the one-mode matricizations,

(cid:107)A(cid:107)SN =

2d
(cid:88)

i=1

(cid:107)A(i)(cid:107)∗.

(22)

The SN norm has been widely used in the literature (Gandy et al., 2011; Tomioka et al.,

2011; Liu et al., 2013; Raskutti et al., 2019) to simultaneously encourage the low-rankness

for all modes of a tensor. This leads us to the SN norm regularized estimator

(cid:98)ASN = arg min

A

(cid:40)

1
T

T
(cid:88)

t=1

(cid:107)Yt − (cid:104)A, Yt−1(cid:105)(cid:107)2

F + λSN(cid:107)A(cid:107)SN

,

(cid:41)

where λSN is the tuning parameter. Note that if instead of (cid:107)A(cid:107)SN, only one single nuclear

norm, say (cid:107)A(1)(cid:107)∗, is penalized, then the resulting estimator will only enforce the low-

rankness for the ﬁrst mode of A, while failing to do so for all the other 2d − 1 modes, and

hence will be less eﬃcient than the above SN estimator.

To derive the estimation error bound for (cid:98)ASN, we make the following assumption on the

random error et = vec(Et).

17

Assumption 2. Let et = Σ1/2
e ξt, where {ξt} is a sequence of i.i.d. random vectors, with
E(ξt) = 0 and var(ξt) = I p, and Σe = var(et) is a positive deﬁnite matrix. In addition, the
entries (ξit)1≤i≤p of ξt are mutually independent and κ2-sub-Gaussian, i.e., E(eµξit) ≤ eκ2µ2/2,
for any µ ∈ R and i = 1, . . . , p.

The sub-Gaussianity condition in Assumption 2 is milder than the commonly used nor-

mality assumption in the literature on high-dimensional stationary vector autoregressive

models (Basu and Michailidis, 2015; Raskutti et al., 2019). This relaxation is made possi-

ble through establishing a novel martingale-based concentration bound in the proof of the

deviation bound; see Lemma B.5 in Appendix B.4. The covariance matrix Σe captures the

contemporaneous dependency in Et, and the constant κ controls the tail heaviness of the

marginal distributions.

For any z ∈ C, let A(z) = I p − A[S2]z be a matrix polynomial, where C is the set of com-

plex numbers. Let µmin(A) = min
|z|=1

λmin(A∗(z)A(z)) and µmax(A) = max
|z|=1

λmax(A∗(z)A(z)),

where A∗(z) is the conjugate transpose of A(z); see Basu and Michailidis (2015) for more

discussions on the connection between the spectral density of the VAR process and the two

quantities. In addition, let

αRSC =

λmin(Σe)
µmax(A)

, M1 =

λmax(Σe)
µ1/2
min(A)

,

and M2 =

λmin(Σe)µmax(A)
λmax(Σe)µmin(A)

.

The exactly low-rankness of the true transition tensor A assumed in Section 4 could

be too stringent in real-world applications.

In what follows, we relax it to the following

approximately low-rank assumption: We assume that all one-mode matricizations of the

underlying true transition tensor A belong to the set of approximately low-rank matrices,

namely A(i) ∈ Bq(r(i)

q ; pi, p−ip) for some q ∈ [0, 1), where

Bq(r; d1, d2) :=






M ∈ Rd1×d2 :

min(d1,d2)
(cid:88)

i=1

σi(M )q ≤ r






,

and p−i = p/pi = (cid:81)d
j=1,j(cid:54)=i pj. For the convenience of notation, here we require that 00 = 0.
Note that when q = 0, B0(r; d1, d2) is the set of d1-by-d2 rank-r matrices. For q > 0, the

18

restriction on (cid:80)min(d1,d2)

i=1

σi(M )q ≤ r requires that the singular values decay fast, and it is

more general and natural than the exactly low-rank assumption.

Theorem 2. Under Assumptions 1 and 2, if T (cid:38) max1≤i≤d p−ip + max(κ2, κ4)M −2
κ2M1d−2 (cid:80)d
then with probability at least 1 − 2 (cid:80)d

(cid:112)p−ip/T , and A(i) ∈ Bq(r(i)

i=1 exp(−Cp−ip) − exp[−C min(κ−2, κ−4)M 2

q ; pi, p−ip) for some q ∈ [0, 1) and all i = 1, . . . , 2d,

2 p, λSN (cid:38)

2 p],

i=1

(cid:107) (cid:98)ASN − A(cid:107)F (cid:46) √

rq

(cid:19)1−q/2

(cid:18)2d · λSN
αRSC

where rq = (2d)−1 (cid:80)2d

i=1 r(i)
q .

By Theorem 2, when λSN (cid:16) κ2M1d−2 (cid:80)d
√

(cid:112)p−ip/T , the estimation error bound scales
rq(max1≤i≤d p−ip/T )1/2−q/4; note that the factor d in the error bounds is canceled by

i=1

as

the d−2 in the rate of λSN. When q = 0, namely A is exactly low-rank with Tucker ranks
), the error bound reduces to (cid:112)r0 max1≤i≤d p−ip/T and it is comparable to
(r(1)

0 , . . . , r(2d)

0

that in Tomioka et al. (2011) for i.i.d. tensor regression.

However, recent research (e.g., Mu et al., 2014; Raskutti et al., 2019) has shown that

the SN norm regularization approach is suboptimal, mainly because A(i) is an unbalanced

fat-and-short matricization of a higher-order tensor. Technically, in the proof of Theorem 2,

an essential intermediate step is to establish the deviation bound, where we need to upper

bound the operator norm of a sub-Gaussian random matrix with the same dimensions as

A(i); see Lemma B.5 in Appendix B.3 for details. Undesirably, the order of this operator

norm will be dominated by the larger of the row and column dimensions of the matrix

A(i) ∈ Rpi×p−ip, and hence by the column dimension p−ip, which eventually appears in the

error bound. This indicates that the imbalance of the matricization leads to the eﬃciency

bottleneck of the SN estimator.

On the other hand, similarly to (21), since the the reduced-rank VAR model can be re-

garded as an overparameterization of the proposed LRTAR model, alternatively one may

focus on the approximately low-rank structure of the transition matrix A[S2] in the VAR

representation in (10), and adopt the matrix nuclear (MN) estimator (Negahban and Wain-

19

wright, 2011) to estimate A,

(cid:98)AMN = arg min

A

(cid:40)

1
T

T
(cid:88)

t=1

(cid:107)Yt − (cid:104)A, Yt−1(cid:105)(cid:107)2

F + λMN(cid:107)A[S1](cid:107)∗

.

(23)

(cid:41)

Note that the multi-mode matricization A[S2] = A(cid:62)

[S1] is a p × p square matrix. Thus, the

loss of eﬃciency due to the unbalanced matricization can be avoided, which is conﬁrmed by

the following theorem.

Theorem 3. Under Assumptions 1 and 2, if T (cid:38) [1+max(κ2, κ4)M −2
and A[S1] ∈ Bq(s(1)
exp[−C min(κ−2, κ−4)M 2

2 p],

q ; p, p) for some q = [0, 1), then with probability at least 1 − exp(−Cp) −

2 ]p, λMN (cid:38) κ2M1

(cid:112)p/T ,

(cid:107) (cid:98)AMN − A(cid:107)F (cid:46)

(cid:113)

s(1)
q

(cid:18) λMN
αRSC

(cid:19)1−q/2

.

Theorem 3 shows that, with λMN (cid:16) κ2M1

(cid:112)p/T , the estimation error bound for (cid:98)AMN
s(1)
q (p/T )1/2−q/4. This result is comparable to that in Negahban and Wainwright

(cid:113)

scales as

(2011) for reduced-rank VAR models, yet we relax both the constraint (cid:107)A(cid:107)op < 1 on the

transition matrix A and the normality assumption on the random error in their paper. This

estimation error bound is clearly smaller than that in Theorem 2, as (max1≤i≤d p−ip/T )1/2−q/4
in general can be much larger than (p/T )1/2−q/4 when s(1)

q (cid:16) rq. Therefore, adopting square

matricization can indeed improve the estimation performance.

The idea of using square matricization to improve eﬃciency was adopted by Mu et al.

(2014) in low-rank tensor completion problems. Their proposed method, called the square

deal, is to ﬁrst unfold a general higher-order tensor into a matrix with similar numbers of

rows and columns, and then use the MN norm as the regularizer. However, for our estimation

problem, despite the advantage of (cid:98)AMN over (cid:98)ASN, Theorem 3 reveals another drawback of
(cid:98)AMN. That is, the error bounds for (cid:98)AMN depend on the (cid:96)q radius s(1)
of A[S1], suggesting that (cid:98)AMN may perform badly when s(1)

q of the singular values

is relatively large.

In other

q

words, unless we have prior knowledge that the (cid:96)q “norm” of singular values of particular
matricization A[S1] is truly small, (cid:98)AMN may not be desirable in practice.

20

On the other hand, although the SN regularizer in (22) suﬀers from ineﬃciency due

to the imbalance of one-mode matricizations, it has the attractive feature of simultaneously

enforcing the low-rankness across all modes of the tensor A, and thus is more eﬃcient than its

counterpart which considers only one single one-mode matricization, say, (cid:107)A(1)(cid:107)∗. Similarly,

if we can enforce the approximate low-rankness across all possible square matricizations of

A, the estimation performance may be further improved upon (cid:98)AMN. This motivates us to

propose a new regularization approach in the next subsection.

5.2 Regularization via Square Matricization

In this subsection, we propose a novel convex regularizer which improves upon both the

SN and MN regularizers in (22) and (23), respectively, by simultaneously encouraging the

low-rankness across all possible square matricizations of the transition tensor A.

For any 2d-th-order tensor A ∈ Rp1×···×pd×p1×···×pd, its multi-mode matricization A[I] will

be a p × p square matrix, with p = (cid:81)d

i=1 pi, if the index set is chosen as

I = {(cid:96)1, . . . , (cid:96)d},

where each index (cid:96)i

is set to either i or d + i, for i = 1, . . . , d. For instance, A[S1]

is the square matricization formed by setting (cid:96)i = i for all i = 1, . . . , d. Moreover, if

A has multilinear ranks (r1, . . . , r2d), then the rank of the matricization A[I] is at most
min((cid:81)2d

i=1,i /∈I ri). Therefore, if we penalize the sum of nuclear norms of all such

i=1,i∈I ri, (cid:81)2d

squares matricizations, which we call the sum of square-matrix nuclear (SSN) norms for

simplicity, then the resulting estimator would enjoy the eﬃciency gain from both the use of

square matricizations and simultaneous incorporation of many rank constraints.

Obviously, there are 2d possible choices of the index set I that corresponds to a square

matricization A[I]. However, since A[I] = A(cid:62)
[I (cid:123)], when deﬁning the SSN norm, we only
need to include one of I and its complement I (cid:123). A simple way to do so is to choose only

sets containing the index one. That is, ﬁx (cid:96)1 = 1 and choose (cid:96)i = i or d + i for i =

2, . . . , d. This results in totally 2d−1 chosen index sets, denoted by I1, I2, . . . , I2d−1. Note

21

that I1 = S1 = {1, . . . , d}. For example, when d = 3, we have four chosen index sets,

I1 = {1, 2, 3}, I2 = {1, 5, 3}, I3 = {1, 2, 6} and I4 = {1, 5, 6}.

Based on the above choice of the 2d−1 index sets, we introduce the following SSN norm,

(cid:107)A(cid:107)SSN =

2d−1
(cid:88)

k=1

(cid:13)
(cid:13)A[Ik]

(cid:13)
(cid:13)∗ .

The corresponding estimator is deﬁned as

(cid:98)ASSN = arg min

A

(cid:40)

1
T

T
(cid:88)

t=1

(cid:107)Yt − (cid:104)A, Yt−1(cid:105)(cid:107)2

F + λSSN(cid:107)A(cid:107)SSN

,

(24)

(cid:41)

where λSSN is the tuning parameter.

If the rank of one-mode matricizations rank(A(i)) = ri, each square matricization A[Ik]
ri). Similarly, if all A(i)s are

is also low-rank with rank(A[Ik]) ≤ min((cid:81)2d
approximately low-rank, the square matricizations are approximately low-rank as well. In

ri, (cid:81)2d

i=1,i /∈Ik

i=1,i∈Ik

contrast to the SN norm in (22) which directly matches the multilinear ranks rank(A(i)) for

i = 1, . . . , d, the SSN norm encourages the multilinear low-rank structure of A by simul-

taneously enforcing the low-rankness of all the square matricizations A[Ik]. The following
theorem gives the theoretical results for (cid:98)ASSN.

Theorem 4. Under Assumptions 1 and 2, if T (cid:38) [1+max(κ2, κ4)M −2
and A[Ik] ∈ B(s(k)
1 − exp[−C(p − d)] − exp[−C min(κ−2, κ−4)M 2

q

; p, p) for some q ∈ [0, 1) and all k = 1, . . . , 2d−1, with probability at least

2 ]p, λSSN (cid:38) κ2M121−d(cid:112)p/T ,

where sq = 21−d (cid:80)2d−1

k=1 s(k)

q

(cid:107) (cid:98)ASSN − A(cid:107)F (cid:46) √

.

2 p],
(cid:18) 2d−1λSSN
αRSC

sq

(cid:19)1−q/2

√

By Theorem 4, when λSSN (cid:16) κ2M121−d(cid:112)p/T , the estimation error bound scales as
sq(p/T )1/2−q/4 and reduces to (cid:112)s0p/T in the exactly low-rank setting for q = 0. For
a clearer comparison among the three estimators (cid:98)ASN, (cid:98)AMN and (cid:98)ASSN, we summarize the
main results of Theorems 2–4 in Table 1. First, both (cid:98)ASSN and (cid:98)AMN have much smaller error
bounds and less stringent sample size requirements than (cid:98)ASN, due to the diverging dimension

p−i in the results of the latter. This reaﬃrms the advantage of the square matricizations.

22

SN

MN

SSN

Sample size

Estimation error

T (cid:38) (max1≤i≤d p−i + M −2
√

rq(max1≤i≤d p−ip/T )1/2−q/4

2 )p

T (cid:38) (1 + M −2
(cid:113)

2 )p

q (p/T )1/2−q/4 √
s(1)

T (cid:38) (1 + M −2

2 )p

sq(p/T )1/2−q/4

Table 1: Summary of the sample size conditions and error upper bounds in Theorems 2–4,
where p−i = (cid:81)d

q , and sq = 21−d (cid:80)2d−1

j=1,j(cid:54)=i pj, rq = (2d)−1 (cid:80)2d

k=1 s(k)

i=1 r(i)

.

q

q

Secondly, comparing (cid:98)ASSN to (cid:98)AMN, since the factor sq in the error bounds of (cid:98)ASSN is the
for k = 1, . . . , 2d−1, (cid:98)ASSN can protect us from the bad scenarios where the

average of all s(k)
(cid:96)q “norm” of the singular values of A[S1] is relatively large. If all the s(k)
order, then the error upper bounds for (cid:98)ASSN and (cid:98)AMN in Table 1 will be similar. However,
our simulation results in Section 6 show that (cid:98)ASSN clearly outperforms (cid:98)AMN under various
settings, even when s(1)
. Indeed, the error bounds for (cid:98)ASSN in Theorem 4 is

q = · · · = s(2d)

’s are of the same

q

q

likely to be loose, which is believed to be caused by taking the upper bounds on the dual

norm of the SSN norm in the proof of Lemma B.3; see Appendix B.3 for details. By contrast,

the error bounds for (cid:98)AMN are minimax-optimal (Han et al., 2015b). Therefore, although

our theoretical results are not sharp enough to distinguish clearly between the error rates of

(cid:98)ASSN and (cid:98)AMN, we conjecture that the actual rate of the former is generally smaller than
that of the latter. Methodologically, this is also easy to understand because, unlike (cid:98)AMN,
(cid:98)ASSN simultaneously enforces the low-rankness across all square matricizations of A rather
than just on A[S1].

Remark 1. While our SSN regularization is proposed in the time series context, the idea

of imposing joint penalties on all (close to) square matricizations of the coeﬃcient tensor

can potentially be extended to general higher-order tensor estimation problems. Moreover,

it can be reﬁned to accommodate particular structures of the data. For example, if some of

the d modes of the tensor-value time series Yt, namely p1, . . . , pd, are equal, then even a

greater number of possible square matricizations of A can be formed, resulting in improved

estimation eﬃciency.

23

5.3 Truncated Regularized Estimation

While the proposed regularized estimation methods do not require exact low-rankness of

the true transition tensor A, sometimes imposing exact low-rankness may be more desirable

if one wants to interpret the underlying low-dimensional tensor dynamics. As discussed in

Section 3, the Tucker ranks can be interpreted as the numbers of dynamic factors in each

mode. In this section, we propose a truncation method to consistently estimate the true

multilinear ranks (r1, . . . , r2d) under the exact low-rank assumption.

Let γ be a threshold parameter to be chosen properly. Given the estimator (cid:98)ASSN, for

each i = 1, . . . , 2d, we calculate the singular value decomposition (SVD) of the mode-i matri-

cization ( (cid:98)ASSN)(i) with the singular values arranged in descending order. Next we truncate

the SVD by retaining only singular values greater than γ, and take their corresponding left

singular vectors to deﬁne the matrix (cid:101)U i. Then, the truncated core tensor is deﬁned as

(cid:101)G = (cid:98)ASSN ×2d

i=1 (cid:101)U

(cid:62)
i ,

based on which we propose the truncated sum of square-matrix nuclear (TSSN) estimator

(cid:98)ATSSN = (cid:101)G ×2d

i=1 (cid:101)U i.

To derive the theoretical results on rank selection, we make the following assumption on

the exact Tucker ranks and the magnitude of the singular values.

Assumption 3. For all i = 1, . . . , 2d, σr(A(i)) = 0 for all r > ri, and there exists a constant
(cid:1) ≥ Cγ. As T → ∞, the threshold parameter satisﬁes

(cid:0)A(i)

C > 1 such that min1≤i≤2d σri
γ (cid:29) (κ2M1/αRSC)(cid:112)s0p/T , where s0 = 21−d (cid:80)2d−1

k=1 rank(A[Ik]).

Assumption 3 requires that A has exact Tucker ranks (r1, . . . , r2d) which do not diverge

too fast. The smallest positive singular value for each A(i) is assumed to be bounded away

from the threshold γ when the sample size is suﬃciently large. Since Assumption 3 involves

unknown quantities, it cannot be used directly for determining γ in practice. Instead, we

recommend using the data-driven threshold parameter γ described at the end of the next

subsection.

24

The rank selection consistency of the truncation method and the asymptotic estimation

error rate of (cid:98)ATSSN are given by the following theorem.

Theorem 5. Under the conditions of Theorem 4 and Assumption 3, if the tuning parameter
λSSN (cid:16) κ2M121−d(cid:112)p/T , then

(cid:110)

rank

(cid:16)

P

( (cid:98)ATSSN)(i)

(cid:17)

= rank(A(i)), for i = 1, . . . , 2d

(cid:111)

→ 1,

as T → ∞, and for any ﬁxed d,

(cid:107) (cid:98)ATSSN − A(cid:107)F = Op

(cid:16)(cid:112)s0p/T

(cid:17)

,

where s0 is deﬁned as in Assumption 3.

5.4 ADMM Algorithm

This subsection presents the algorithm for the proposed (T)SSN regularized estimator. The

algorithm for (cid:98)ASN can be developed analogously, while (cid:98)AMN can be obtained easily as in

Negahban and Wainwright (2011).

The objective function for the estimator (cid:98)ASSN in (24) can be rewritten as

LT (A) + λSSN(cid:107)A(cid:107)SSN = LT (A) + λSSN

2d−1
(cid:88)

k=1

(cid:107)A[Ik](cid:107)∗,

(25)

where LT (A) = T −1 (cid:80)T
regularizer (cid:107)A(cid:107)SSN involves 2d−1 nuclear norms (cid:107)A[Ik](cid:107)∗, which are challenging to handle

F is the quadratic loss function.

t=1 (cid:107)Yt − (cid:104)A, Yt−1(cid:105)(cid:107)2

In (25), the

at the same time. A similar diﬃculty also occurs in low-rank tensor completion, for which

Gandy et al. (2011) applied the alternating direction method of multipliers (ADMM) algo-

rithm (Boyd et al., 2011) to eﬃciently separate the diﬀerent nuclear norms. Borrowing the

idea of Gandy et al. (2011), we develop an ADMM algorithm for the miminization of (25).

To separate the 2d−1 nuclear norms in (cid:107)A(cid:107)SSN, for each A[Ik], we introduce a diﬀerent
dummy variable Wk as a surrogate for A, where k = 1, . . . , 2d−1. Then the augmented

Lagrangian is

L(A, W, C) = LT (A) +

2d−1
(cid:88)

k=1

(cid:104)
λSSN(cid:107)(Wk)[Ik](cid:107)∗ + 2ρ(cid:104)Ck, A − Wk(cid:105) + ρ(cid:107)A − Wk(cid:107)2

F

(cid:105)
,

25

Algorithm 2 ADMM algorithm for (T)SSN estimator
Initialize: C(0)

k = A(0) = (cid:98)AMN, for k = 1, . . . , 2d−1, threshold parameter γ

k , W(0)

for j ∈ {0, 1, . . . , J − 1} do

A(j+1) ← arg min

(cid:110)
LT (A) + (cid:80)2d−1

k=1 ρ(cid:107)A − W(j)

k + C(j)

k (cid:107)2
F

(cid:111)

for k ∈ {1, 2, . . . , 2d−1} do
(cid:110)

W(j+1)

k ← arg min
k ← C(j)
C(j+1)

k + A(j+1) − W(j+1)

k

ρ(cid:107)A(j+1) − Wk + C(j)

k (cid:107)2

F + λSSN(cid:107)(Wk)[Ik](cid:107)∗

(cid:111)

end for

end for

(cid:98)ASSN ← A(J)

for i ∈ {1, 2, . . . , 2d} do

(cid:101)U i ← Truncated SVD(( (cid:98)ASSN)(i), γ)

end for

(cid:101)G ← (cid:98)ASSN ×2d
(cid:98)ATSSN ← (cid:101)G ×2d

i=1 (cid:101)U

i=1 (cid:101)U i

(cid:62)
i

where Ck are the Lagrangian multipliers, for k = 1, . . . , 2d−1, and ρ is the regularization

parameter. Then we can iteratively update A, Wk and Ck by the ADMM, as shown in

Algorithm 2.

In Algorithm 2, the A-update step is an (cid:96)2-regularized least squares problem. Similarly

to Gandy et al. (2011), the Wk-update step can be solved by applying the explicit soft-
thresholding operator to the singular values of (A + Ck)[Ik]. Both subproblems have close-

form solutions. Thus, the miminization of (25) can be solved eﬃciently.

For the tuning parameter selection, since the cross-validation method is unsuitable for

time series or intrinsically ordered data, we apply the Bayesian information criterion (BIC)

to select the optimal λSSN from a sequence of tuning parameter values, where the number of
degrees of freedom is deﬁned as 2−(d−1) (cid:80)2d−1

k=1 sk(2p − sk). For the threshold parameter γ of
the truncated estimator, we recommend γ = 2d−1λSSN/4 to practitioners, where λSSN is the

optimal tuning parameter selected by the BIC.

26

6 Simulation Studies

We conduct three simulation experiments to examine the ﬁnite-sample performance of the

proposed low- and high-dimensional estimation methods for the LRTAR model in previous
sections. Throughout, we generate the data from model (8) with vec(Et) i.i.d.∼ N (0, I p). The
entries of the core tensor G are generated independently from N (0, 1) and rescaled such that

(cid:107)G(cid:107)F = 5. The factor matrices U i’s are generated by extracting the leading singular vectors

from Gaussian random matrices while ensuring the stationarity condition in Assumption 1.

The ﬁrst experiment focuses on the proposed low-dimensional estimation method consid-

ered in Section 4. We consider four cases of data generating processes. In both cases (1a)

and (1b), we consider d = 2 and multilinear ranks (r1, r2, r3, r4) = (1, 1, 1, 1), (2, 2, 2, 2),

or (2, 3, 2, 3).

In both cases (2a) and (2b), we consider d = 3 and multilinear ranks

(r1, r2, r3, r4, r5, r6) = (1, 1, 1, 1, 1, 1), (2, 2, 2, 1, 1, 1), or (2, 2, 2, 2, 2, 2). Both pairs of cases

diﬀer in the setting of the dimensions pi’s: (1a) p1 = p2 = 5; (1b) p1 = p2 = 10; (2a)

p1 = p2 = p3 = 5; and (2b) p1 = p2 = p3 = 7. We repeat each data generating process

300 times, and conduct the estimation using true multilinear ranks. Figure 2 displays the

average estimation error against T ∈ [1000, 2000]. The LTR estimator clearly outperforms

both the RRR and OLS estimators throughout all cases, conﬁrming the theoretical eﬃciency

comparison in Corollary 1.

The second experiment aims to verify the estimation error bound for the proposed SSN es-

timator in high dimensions. By Theorem 4, when the true transition tensor A is exactly low-
F = Op(s0p/T ), where s0 = 21−d (cid:80)2d−1

k=1 s(k)
rank, we have (cid:107) (cid:98)ASSN −A(cid:107)2
Note that this rate is dependent on the overall dimension, p = (cid:81)d

0 = rank(A[Ik]).

0 with s(k)

i=1 pi, but independent of

the individual dimensions pi. To examine the relationship between the estimation error and

the overal dimension p, sample size T , multilinear ranks ri and individual dimensions pi, we
generate the data under the eight settings listed in Table 2, and plot (cid:107) (cid:98)ASSN − A(cid:107)2

F against

the varying parameter in each case in Figure 3, including p, 1/T, s0 and diﬀerent settings

of pi’s under a ﬁxed overall dimension p. The ﬁrst three columns of Figure 3 show that
(cid:107) (cid:98)ASSN − A(cid:107)2

F roughly scales linearly in p, T and s0, while the last column suggests that the

27

estimation error is invariant under diﬀerent settings of individual pi as long as p and the

other parameters are ﬁxed. This lends support to the theoretical error bound for the SSN

estimator.

In the third experiment, we compare the performance of all the four high-dimensional

estimators discussed in Section 5, namely the SN, MN, SSN and TSSN estimators. We

consider four cases of data generating processes. In cases (3a) and (3b), we consider d = 2

and multilinear ranks (r1, r2, r3, r4) = (1, 1, 1, 1), (2, 2, 1, 1), or (2, 2, 2, 2).

In cases (4a)

and (4b), we consider d = 3 and multilinear ranks (r1, r2, r3, r4, r5, r6) = (1, 1, 1, 1, 1, 1),

(2, 2, 2, 1, 1, 1), or (2, 2, 2, 2, 2, 2). Similarly to the ﬁrst experiment, both pairs of cases diﬀer

in the setting for pi’s: (3a) p1 = p2 = 5; (3b) p1 = p2 = 10; (4a) p1 = p2 = p3 = 5; and (4b)

p1 = p2 = p3 = 7. For each setting, we repeat 300 times, and conduct the estimation using

SN, MN, SSN and TSSN. The tuning parameter and the truncation threshold parameter are

selected by the method described in Section 5.4. In Figure 4, the average estimation error

is plotted against T ∈ [400, 1000] for cases (3a) and (3b), and T ∈ [600, 1200] for cases (4a)

and (4b). First, it can be seen that the SN estimator is much inferior to the other three

estimators, which is due to its use of the unbalanced one-mode matricizations. Secondly,

the SSN and TSSN estimators outperform or are at least as good as the MN estimator in

all cases, and their advantage is remarkably clear even when r1 = · · · = r2d. In addition,

the TSSN estimator generally performs better than the SSN, probably because the former

yields a more parsimonious model which further improves the estimation eﬃciency.

7 Real Data Analysis

7.1 Matrix-Valued Time Series

We ﬁrst consider the modeling of a matrix-valued time series. The data is the monthly

market-adjusted return series of Fama–French 10 × 10 portfolios from January 1979 to De-

cember 2019, obtained from French (2020). The portfolios are constructed as the intersec-

tions of 10 portfolios formed by the book-to-market (B/M) ratio and 10 portfolios formed

28

by the size (market value of equity). Hence, d = 2, p1 = p2 = 10 and T = 492 months.

We remove the market eﬀect for each portfolio by subtracting its average return from the

original return series.

Let Y t ∈ R10×10 be the observed matrix-valued time series, with its rows and columns

corresponding to diﬀerent B/M ratios (sorted from lowest to highest) and sizes (sorted from

smallest to largest), respectively, and denote yt = vec(Y t). For comparison, we consider the

following ﬁve candidate models:

• Vector autoregression (VAR): yt = Ayt−1 + et, where A ∈ R100×100. The model is

estimated by the least squares method.

• Vector factor model (VFM): yt = Λf t + et, where f t is the low-dimensional vector-

valued latent factor, and Λ is the loading matrix. The model is estimated by the

method in Lam et al. (2012), and for prediction, the estimated factors (cid:98)f t are then

ﬁtted by a VAR(1) model.

• Matrix autoregression (MAR): Y t = B1Y t−1B(cid:62)

2 + Et, where B1, B2 ∈ R10×10 are

coeﬃcient matrices; see Example 1. The model is estimated by the iterated least

squares method in Chen et al. (2020b).

• Matrix factor model (MFM): Y t = RF tC (cid:62) + Et, where F t is the low-dimensional

matrix-valued latent factor, and R and C are the loading matrices. The model is

estimated by the method in Wang et al. (2019), and for prediction, the estimated

factors (cid:98)F t are then ﬁtted by a VAR(1) model.

• The proposed LRTAR model: Y t = (cid:104)A, Y t−1(cid:105) + Et, with A = G ×4

i=1 U i. The model

is estimated using the SSN and TSSN regularized estimation methods in Section 5.

We ﬁrst focus on the results for proposed LRTAR model. By the proposed truncation

method, we obtain the estimated multilinear ranks ((cid:98)r1, (cid:98)r2, (cid:98)r3, (cid:98)r4) = (8, 8, 2, 2). As shown
(cid:11) +
in Example 1, the model can be written equivalently as U (cid:62)

3 Y tU 4 = (cid:10)G, U (cid:62)

1 Y t−1U 2

U (cid:62)

3 EtU 4. Thus, U 3 and U 1 can be viewed as the factor loadings across diﬀerent B/M ratios,

29

while U 4 and U 2 represent those across diﬀerent sizes. By the factor interpretation below

(12), this result indicates that the information in Y t can be eﬀectively summarized into the

2 × 2 response factors U (cid:62)

3 Y tU 4, while the low-rank structure associated with the predictor

Y t−1 is not strong. Moreover, the estimated multilinear ranks suggest that the MAR model

might be ineﬃcient for the data, since the MAR imposes that r1 = r2 = r3 = r4 = 10 and

U 3 = U 4 = I 10; see Example 1.

In addition, by an argument similar to the discussion

in Example 2, the MFM can be regarded as a special case of the proposed model with

(U 1, U 2) = (U 3, U 4) and (r1, r2) = (r3, r4). Thus, the fact that (cid:98)r3 and (cid:98)r4 are much smaller
than (cid:98)r1 and (cid:98)r2 suggests that the MFM may be too restrictive for the data.

The TSSN estimates of the factor matrices are shown in Figure 5. The patterns of the

estimated response factor matrices (cid:101)U 3, (cid:101)U 4 ∈ R10×2 are particularly interesting. First, for

both (cid:98)U 3 and (cid:98)U 4, the uniform pattern of the ﬁrst column indicate that portfolios across

diﬀerent B/M ratios (or sizes) contribute approximately equally to the ﬁrst B/M (or size)

response factor. Thus, this factor represents the component of the market performance which

is invariant to the size and B/M ratio of the portfolios. The signiﬁcance of this factor may

be partially because we ﬁt the model to market-adjusted returns, where the average return

is subtracted for all portfolios. Meanwhile, for both response factor matrices, the second

column has a smoothly increasing pattern, suggesting that part of the return variation in

the market has a monotonic relationship with the size and B/M ratio. Moreover, the above

interpretations are consistent with the famous Fama-French three-factor model, where the

return of a portfolio is expected to be aﬀected by the market premium, outperformance of

small versus big companies, and outperformance of high B/M versus small B/M companies.

The performance of the ﬁve models are compared through both average in-sample and

out-of-sample forecasting errors. The average in-sample forecasting error is calculated based

on the ﬁtted models for the entire data, while the average out-of-sample forecasting error is

calculated based on the rolling forecast procedure as follows. From January 2016 (t = 445)

to December 2019 (t = 492), we ﬁt the models using all the available data until time t−1 and

obtain the one-step-ahead forecast (cid:98)Y t. Then, we obtain the average of the rolling forecasting

30

errors for this period.

The average forecasting errors in (cid:96)2 and (cid:96)0 norms are presented in Table 3. Firstly, the

VAR model has the smallest in-sample forecasting error among all models, which is as ex-

pected because the VAR model is highly overparametrized. The bad in-sample performance

of the MFM agrees with our discussion about its restrictiveness for the data due to the

mismatch of the multilinear ranks. It is worth noting that the proposed LRTAR model has

competitive in-sample forecasting performance among all models.

The out-of-sample forecasting results provides a fuller picture of the eﬃciency of diﬀerent

methods. It can be seen that the VAR and VFM models perform worst among all, as they

both completely ignore the matrix structure of the data. Notably the proposed LRTAR

model, ﬁtted by either the SSN or the TSSN methods, have the smallest out-of-sample

forecasting errors. This suggests that the proposed LRTAR model can indeed eﬃciently

capture the dynamic structural information in the data.

7.2 Three-Way Tensor-Valued Time Series

Fama and French (2015) extended the classical Fama-French three-factor model to a ﬁve-

factor model which further incorporates the eﬀect of operating proﬁtability (OP) and invest-

ment (Inv). This motivates tensor-valued stock returns data formed according to the size

(small and big), B/M ratio (four groups sorted from lowest to highest), OP (four groups

sorted from lowest to highest), and Inv (four groups sorted from lowest to highest). We

consider two datasets retrieved from French (2020). The ﬁrst dataset consists of monthly

market-adjusted returns series of 4 × 4 × 2 portfolios from July 1963 to December 2019,

formed by the OP, B/M ratio and size. The second dataset consists of those of 4 × 4 × 2

portfolios formed by the Inv, B/M ratio and size. Hence, both are tensor-valued time series

with d = 3, p1 = p2 = 4, p3 = 2 and T = 678 months.

Similar to the analysis in the previous subsection, ﬁve candidate models are considered:

• Vector autoregression (VAR): yt = Ayt−1 + et, where A ∈ R32×32.

31

• Vector factor model (VFM): yt = Λf t + et, where f t is the low-dimensional vector-

valued latent factor, and Λ is the loading matrix.

• Multilinear tensor autoregression (MTAR): Yt = Yt−1 ×3

i=1 Bi + Et, where B1, B2 ∈

R4×4 and B3 ∈ R2×2 are coeﬃcient matrices; see Example 1.

• Tensor factor model (TFM): Yt = Ft ×3

i=1 U i + Et, where Ft is the low-dimensional

tensor-valued latent factor, and U i’s are the loading matrices.

• The proposed LRTAR model: Yt = (cid:104)A, Yt−1(cid:105) + Et, with A = G ×6

i=1 U i.

The TFM is estimated by the method in Chen et al. (2020c), and for prediction, the

estimated factors (cid:98)Ft are ﬁtted by a VAR(1) model. The other four models are ﬁtted by

the same methods as in the previous subsection. The multilinear ranks selected by the

truncation method are ((cid:98)r1, (cid:98)r2, (cid:98)r3, (cid:98)r4, (cid:98)r5, (cid:98)r6) = (3, 3, 2, 1, 1, 1) and (2, 2, 2, 1, 1, 1) for the OP-
BM-Size portfolio return series and the Inv-BM-Size portfolio return series, respectively.

Note that similar to the BM-Size 10 × 10 series in the previous subsection, the low-rank

structure for the response in these two tensor-valued datasets is more evident than that for

the predictor. Figure 6 shows the TSSN estimates of the factor matrices. We ﬁnd that the

estimated response factors have a uniform pattern similar to that in Figure 5 for the matrix-

valued data. The fact that only one response factor is extracted in each direction suggests

that there might not be substantial eﬀect of OP, Inv, B/M ratio or size on the returns for

these datasets.

Finally, using the same methods as in the previous subsection, we calculate the average

in-sample and out-of-sample forecasting errors for both datasets ﬁtted with the ﬁve models.

As shown in Table 4, the comparison results for the two datasets are quite similar. It can be

clearly observed that the VAR model always has the smallest in-sample forecasting error, yet

the largest out-of-sample forecasting error. On the contrary, the proposed LRTAR model,

ﬁtted by either the SSN or the TSSN methods, has the smallest out-of-sample forecasting

error. Moreover, the in-sample forecasting performance of the LRTAR model is competi-

tive even compared to the VAR model. Similarly to the MFM in the previous subsection,

32

the TFM model has poor in-sample performance, possibly due to the discrepancy between

(r1, r2, r3) and (r4, r5, r6), as reﬂected by the estimated multilinear ranks. In sum, the re-

sults support the eﬃciency and ﬂexibility of the proposed model and estimation methods for

tensor-valued time series data.

8 Conclusion and Discussion

Eﬃcient modeling and forecasting of general structured (tensor-valued), high-dimensional

time series data is an important research topic which however has been rarely explored in

the literature so far. This paper makes the ﬁrst thorough attempt to address this prob-

lem by introducing the low-rank tensor autoregressive model. By assuming the exactly or

approximately low-Tucker-rank structure of the transition tensor, the model exploits the

low-dimensional tensor dynamic structure of the high-dimensional time series data, and

summarizes the complex temporal dependencies into interpretable dynamic factors.

Asymptotic and non-asymptotic properties are derived for the proposed low- and high-

dimensional estimators, respectively. For the latter, we relax the conventional Gaussian

assumption in the high-dimensional time series literature to sub-Gaussianity via a new

martingale-based concentration technique. Moreover, based on the special structure of the

transition tensor, a novel convex regularizer, the SSN, is proposed, gaining eﬃciencies from

both the square matricization and simultaneous penalization across modes. A truncation

method, the TSSN, is further introduced to consistently select the multilinear ranks and

enhance model interpretability.

We discuss several directions for future research. First, the proposed estimators cannot

adapt to the contemporaneous correlation among elements of the random error Et, which

may lead to eﬃciency loss. This issue can be addressed by considering the generalized least
squares loss function, LT (A; Σe) = T −1 (cid:80)T
e vec(Yt − (cid:104)A, Yt−1(cid:105)),
where Σe = var(vec(Et)). Then A may be estimated jointly with Σe or by a two-step

t=1 vec(Yt − (cid:104)A, Yt−1(cid:105))(cid:62)Σ−1

approach based on a consistent estimator (cid:98)Σe (Basu and Michailidis, 2015; Davis et al.,

33

2016).

Secondly, the proposed methods can be generalized to the LRTAR model of ﬁnite lag

order L, deﬁned as Yt = (cid:104)A1, Yt−1(cid:105) + · · · + (cid:104)AL, Yt−L(cid:105) + Et, where A1, . . . , AL are 2d-th-

order multilinear low-rank coeﬃcient tensors. Then, one may consider the SSN regularized
estimation by minimizing T −1 (cid:80)T

j=1 λj(cid:107)Aj(cid:107)SSN. Alterna-
tively, the Aj’s can be combined into a (2d + 1)-th-order tensor A ∈ Rp1×···pd×p1×···×pd×L

j=1(cid:104)Aj, Yt−j(cid:105)(cid:107)2

t=1 (cid:107)Yt − (cid:80)L

F + (cid:80)L

whose mode-(2d + 1) matricization is A(2d+1) = (A1, . . . , AL); see Wang et al. (2020) for

a similar idea. Even though A may not have exactly square matricizations for L > 1, the

proposed SSN can still be adapted by employing approximately square matricizations. For

instance, consider the pL × p multi-mode matricizations A[Ik∪{2d+1}], where the index sets
Ik for k = 1, . . . , 2d−1 are deﬁned as in this paper. Then, a generalized SSN norm can be
constructed as (cid:107)A(cid:107)GSSN = (cid:80)2d−1
k=1 (cid:107)A[Ik∪{2d+1}](cid:107)∗, which will reduce to (cid:107)A(cid:107)SSN when L = 1.
Thirdly, the LRTAR model can be readily extended to incorporate exogenous tensor-

valued predictors, giving rise to the LRTAR-X model, Yt = (cid:104)A, Yt−1(cid:105) + (cid:104)B, Xt(cid:105) + Et, where

Xt is an m-th-order tensor of exogenous variables, and B is a (d + m)-th-order coeﬃcient

tensor. When the dimension of Xt is high, a low-dimensional structure, such as sparsity,

group sparsity or low-rankness, can be imposed on B to improve the estimation eﬃciency.

Moreover, an open question for matrix- or tensor-valued time series models is how to

incorporate additional structures or constraints; for instance, transport or trade ﬂow data

have unspeciﬁed diagonal entries (Chen and Chen, 2019), and realized covariance matrix

data are subject to positive deﬁnite constraints. Beyond the time series context, it is also

worth investigating the generalization of the SSN regularization method in higher-order

tensor estimation and completion applications, such as neuroimaging analysis (Li et al.,

2018), recommender sytem (Bi et al., 2018) and natural language processing (Frandsen and

Ge, 2019).

34

Appendix A: Proofs for Low-Dimensional Estimation

Below we give the proofs of Theorem 1 and Corollary 1 in Section 4, which generally follow

from Proposition 4.1 in Shapiro (1986) for overparameterized models.

Proof of Theorem 1. The proposed model in (10) can be written in the matrix form





y(cid:62)
0

y(cid:62)
1
...










=












y(cid:62)
1

y(cid:62)
2
...










y(cid:62)
T
(cid:124) (cid:123)(cid:122) (cid:125)
Y

y(cid:62)
T −1
(cid:124) (cid:123)(cid:122) (cid:125)
X












A(cid:62)

[S2] +

.

e(cid:62)
1

e(cid:62)
2
...























e(cid:62)
T
(cid:124) (cid:123)(cid:122) (cid:125)
E

Let (cid:98)hOLS = vec(( (cid:98)AOLS)[S2]). Under Assumption 1, by the classical asymptotic theory for

stationary VAR models, as T → ∞, we have

√

T ((cid:98)hOLS − h) d→ N (0, ΣOLS),

where ΣOLS = Σe ⊗ Σ−1

y , with Σe = var(et) and Σy = var(yt).

Following Shapiro (1986), consider the discrepancy function

F ((cid:98)hOLS, h) = (cid:107)vec(Y ) − (I p ⊗ X)h(cid:107)2

2 − (cid:107)vec(Y ) − (I p ⊗ X)(cid:98)hOLS(cid:107)2
2.

Note that F ((cid:98)hOLS, h) is nonnegative and twice continuously diﬀerentiable, and equals zero

if and only if (cid:98)hOLS = h.

The Jacobian matrix H = ∂h(θ)/∂θ in (20) can be veriﬁed by noting that

h = vec(A[S2]) = vec((⊗i∈S2U i)G[S2](⊗i∈S1U i)(cid:62)) = ((⊗i∈S1U i) ⊗ (⊗i∈S2U i)) vec(G[S2])

and that for any 1 ≤ i ≤ 2d,

h = vec(A[S2]) = P (i)

[S2]vec (cid:0)U iG(i)(⊗2d

j=1,j(cid:54)=iU (cid:62)

[S2]vec(A(i)) = P (i)
= P (i)
[S2]

(cid:8)(cid:2)(⊗2d

j=1,j(cid:54)=iU i)G(cid:62)

(i)

(cid:9) vec(U i).

j )(cid:1)
(cid:3) ⊗ I pi

Then, by Proposition 4.1 in Shapiro (1986), we obtain that the minimizer of F ((cid:98)hOLS, ·),

namely the LTR estimator, has the asymptotic normality,

√

T ((cid:98)hLTR − h) d→ N (0, ΣLTR)

35

and ΣLTR = P ΣOLSP (cid:62), where P = H(H (cid:62)J H)†H (cid:62)J is the projection matrix, J =

Σ−1

e ⊗ Σy is the Fisher information matrix of h, and † denotes the Moore-Penrose inverse.

Since ΣOLS = J −1, we can obtain that ΣLTR = H(H (cid:62)J H)†H (cid:62).

Proof of Corollary 1. As discussed in the proof of Theorem 1, ΣOLS = J −1, and observe that

ΣLTR = H(H (cid:62)J H)†H (cid:62) = J −1/2QJ 1/2HJ −1/2,

(A.1)

where QJ 1/2H is the projection matrix onto the orthogonal compliment of span(J 1/2H).

On the other hand, under the proposed model, the transition matrix can be decom-

posed as A[S2] = V 1V (cid:62)
s1 = min((cid:81)d

2 , where V 1 = ⊗i∈S1U i, V 2 = (⊗i∈S2U i)G[S2], and rank(A[S2]) ≤
i=d+1 ri). Thus, we can write h = vec(A[S2]) = h(φ), where φ =
(vec(V 1)(cid:62), vec(V 2)(cid:62))(cid:62) is the parameter vector for the RRR estimator. Then, similarly

i=1 ri, (cid:81)2d

to the proof of Theorem 1, we denote the Jacobian matrix by R = ∂h/∂φ. By similar

arguments, we have

√

T ((cid:98)hRRR − h) d→ N (0, ΣRRR), where (cid:98)hRRR = vec(( (cid:98)ARRR)[S2]), and

ΣRRR = R(R(cid:62)J R)†R(cid:62) = J −1/2QJ 1/2RJ −1/2,

(A.2)

where QJ 1/2R is the projection matrix onto the orthogonal compliment of span(J 1/2R).
Hence, it is clear that ΣRRR ≤ J −1 = ΣOLS.

Moreover, since the Tucker decomposition can be viewed as a further decomposition of

the low-rank decomposition V U (cid:62) for A[S2], we have H = ∂h/∂θ = R · ∂φ/∂θ. By (A.1)
and (A.2), we have ΣLTR ≤ ΣRRR, since span(J 1/2H) ⊂ span(J 1/2R).

Appendix B: Proofs for High-Dimensional Estimation

In this appendix, we provide the proofs of Theorems 2–5 in Section 5. We start with a

preliminary analysis in Appendix B.1 which lays out the common technical framework for

proving the estimation and prediction error bounds of the SN, MN and SSN regularized

estimators, and four lemmas, Lemmas B.1–B.4, are introduced herein. Then in Appendix

B.2 we give the proofs of Theorems 2–5. The proofs of Lemmas B.1–B.4 are provided in

Appendix B.3, and three auxiliary lemmas are collected in Appendix B.4

36

B.1 Preliminary Analysis

The technical framework for proving the error bounds in Theorem 2–4 consists of two main

steps, a deterministic analysis and a stochastic analysis, given in Sections B.1.1 and B.1.2,

respectively. The goal of the ﬁrst one is to derive the error bounds given the deterministic

realization of the time series, assuming that the parameters satisfy certain regularity con-

ditions. The goal of the second one is to verify that under stochasticity these regularity

conditions are satisﬁed with high probability.

B.1.1 Deterministic Analysis

Throughout the appendix, we adopt the following notations. We use C to denote a generic

positive constant, which is independent of the dimensions and the sample size. For any

matrix M and a compatible subspace S, we denote by M S the projection of M onto S.

In addition, let col(M ) be the column space of M , and let S ⊥ be the complement of the

subspace S. For a generic tensor W ∈ Rp1×···×p2d, the dual norms of its SSN norm and SN

norm, denoted by (cid:107)W(cid:107)SSN∗ and (cid:107)W(cid:107)SN∗, respectively, are deﬁned as

(cid:107)W(cid:107)SSN∗ =

sup
T∈Rp1×···×p2d ,(cid:107)T(cid:107)SSN≤1

(cid:104)W, T(cid:105), and (cid:107)W(cid:107)SN∗ =

sup
T∈Rp1×···×p2d ,(cid:107)T(cid:107)SN≤1

(cid:104)W, T(cid:105).

Moreover, for any two tensors X ∈ Rp1×···×pm and Y ∈ Rpm+1×···×pmn , their tensor outer

product is deﬁned as (X ◦ Y) ∈ Rp1×···×pm×pm+1×···×pm+n where

(X ◦ Y)i1...imim+1...im+n = Xi1...im

Yim+1...im+n,

for any 1 ≤ i1 ≤ p1, . . . , 1 ≤ im+n ≤ pm+n.

For the theory of regularized M -estimators, restricted error sets and restricted strong

convexity are essential deﬁnitions. To deﬁne the former, we need to ﬁrst introduce the

following restricted model subspaces.

For i = 1, . . . , 2d, denote by (cid:101)Ui and (cid:101)Vi the spaces spanned by the ﬁrst ri left and right

singular vectors in the SVD of A(i), respectively. Deﬁne the collections of subspaces

N = (N1, . . . , N2d) and N

⊥

= (N

⊥
1 , . . . , N

⊥
2d),

37

where

Ni = {M ∈ Rpi×p−ip|col(M ) ⊂ (cid:101)Ui, col(M (cid:62)) ⊂ (cid:101)Vi},

N

⊥

i = {M ∈ Rpi×p−ip|col(M ) ⊥ (cid:101)Ui, col(M (cid:62)) ⊥ (cid:101)Vi},

(B.1)

for i = 1, . . . , 2d. Note that Ni ⊂ N i.

Furthermore, for k = 1, . . . , 2d−1, denote by Uk and Vk the spaces spanned by the ﬁrst
k left and right singular vectors in the SVD of the square matricization A[Ik], respectively,
s∗
where s∗

k = rank(A)[Ik]. Similarly, deﬁne the collections of subspaces

M := (M1, . . . , M2d−1) and M

⊥

= (M

⊥
1 , . . . , M

⊥
2d−1),

where

Mk = {M ∈ Rp×p|col(M ) ⊂ Uk, col(M (cid:62)) ⊂ Vk},

M

⊥

k = {M ∈ Rp×p|col(M ) ⊥ Uk, col(M (cid:62)) ⊥ Vk},

(B.2)

for k = 1, . . . , 2d−1. In particular, as described in Section 5.2, I1 = S1 = {1, . . . , d}. Thus,
1 are the subspaces associated with the square matricization A[S1].

M1 and M

⊥

Then, for simplicity, for any W ∈ Rp1×···×p2d, we denote

W(i)

N = (W(i))Ni, W(i)

N ⊥ = (W(i))N ⊥

i

W(k)

M = (W[Ik])Mk, W(k)

M⊥ = (W[Ik])M⊥

, W(i)
N
, W(k)
M

k

= (W(i))N i

⊥ = (W(i))
N

⊥
i

, W(i)
N
, W(k)
M

⊥ = (W[Ik])

,

⊥
M
k

= (W[Ik])Mk

where i = 1, . . . , 2d and k = 1, . . . , 2d−1. Based on the subspaces deﬁned in (B.1) and (B.2),

we can deﬁne the restricted error sets corresponding to the three regularized estimators as

follows.

Deﬁnition 1. The restricted error set corresponding to M is deﬁned as

CSSN(M) :=




∆ ∈ Rp1×···×p2d :



2d−1
(cid:88)

k=1

(cid:107)∆(k)
M

⊥(cid:107)∗ ≤ 3

2d−1
(cid:88)

k=1

(cid:107)∆(k)
M

(cid:107)∗ + 4

2d−1
(cid:88)

k=1

(cid:107)A(k)

M⊥(cid:107)∗






.

The restricted error set corresponding to N is deﬁned as

(cid:40)

CSN(N ) :=

∆ ∈ Rp1×···×p2d :

2d
(cid:88)

i=1

(cid:107)∆(i)
N

⊥(cid:107)∗ ≤ 3

2d
(cid:88)

i=1

(cid:107)∆(i)
N

(cid:107)∗ + 4

(cid:41)

(cid:107)A(i)

N ⊥(cid:107)∗

.

2d
(cid:88)

i=1

38

The restricted error set corresponding to M1 is deﬁned as

CMN(M1) :=

(cid:110)

∆ ∈ Rp1×···×p2d : (cid:107)∆(1)
M

⊥(cid:107)∗ ≤ 3(cid:107)∆(1)
M

(cid:107)∗ + 4(cid:107)A(1)

M⊥(cid:107)∗

(cid:111)

.

The ﬁrst lemma shows that if the tuning parameter is well chosen for each regularized

estimator, the estimation error belongs to the corresponding restricted error set.

Lemma B.1. For the SSN estimator, if the regularization parameter λSSN ≥ 4(cid:107)T −1 (cid:80)T
Et(cid:107)SSN∗, the error ∆SSN = (cid:98)ASSN − A belongs to the set CSSN(M).

t=1

Yt−1◦

For the SN estimator, if the regularization parameter λSN ≥ 4(cid:107)T −1 (cid:80)T

t=1

Yt−1 ◦ Et(cid:107)SN∗,

the error ∆SN = (cid:98)ASN − A belongs to the set CSN(N ).

For the MN estimator, if the regularization parameter λMN ≥ 4(cid:107)T −1 (cid:80)T

t=1 vec(Yt−1)vec(Et)(cid:62)(cid:107)∗,

the error ∆MN = (cid:98)AMN − A belongs to the set CMN(M(1)).

Following Negahban and Wainwright (2012) and Negahban et al. (2012), a restricted

strong convexity (RSC) condition for the square loss function can be deﬁned as follows.

Deﬁnition 2. The loss function satisﬁes the RSC condition with curvature αRSC > 0 and

restricted error set C, if

1
T

T
(cid:88)

t=1

(cid:107)(cid:104)∆, Yt−1(cid:105)(cid:107)2

F ≥ αRSC(cid:107)∆(cid:107)2
F,

∀∆ ∈ C.

Based on the restricted error sets and RSC conditions, the estimation errors have the

following deterministic upper bounds.

Lemma B.2. Suppose that λSSN ≥ 4(cid:107)T −1 (cid:80)T
the parameter αRSC and restricted error set CSSN(M), and A[Ik] ∈ Bq(s(k)
q ∈ [0, 1) and all k = 1, . . . , 2d−1,

t=1

q

Yt−1 ◦ Et(cid:107)SSN∗, the RSC condition holds with

; p, p) for some

where sq = 21−d (cid:80)2d−1

k=1 s(k)

q

(cid:107)∆SSN(cid:107)F (cid:46) √

sq

(cid:18) 2d−1λSSN
αRSC

(cid:19)1−q/2

,

.

39

Suppose that λSN ≥ 4(cid:107)T −1 (cid:80)T

t=1

Yt−1◦Et(cid:107)SN∗, the RSC condition holds with the parameter

αRSC and restricted error set CSN(N ), and A(i) ∈ Bq(r(i)

q ; pi, p−ip) for some q ∈ [0, 1) and

all i = 1, . . . , 2d,

where rq = (2d)−1 (cid:80)2d

i=1 r(i)
q .

(cid:107)∆SN(cid:107)F (cid:46) √

rq

(cid:18) 2d · λSN
αRSC

(cid:19)1−q/2

,

Suppose that λMN ≥ 4(cid:107)T −1 (cid:80)T

parameter αRSC and restricted error set CMN(M1), and A[S1] ∈ Bq(s(1)

t=1 vec(Yt−1)vec(Et)(cid:107)∗, the RSC condition holds with the
q ; p, p) for some q ∈

[0, 1),

(cid:107)∆MN(cid:107)F (cid:46)

(cid:113)

s(1)
q

(cid:18) λMN
αRSC

(cid:19)1−q/2

.

Note that Lemma B.2 is deterministic and the radius sq, rq, and s(1)
q

can also diverge to

inﬁnity.

B.1.2 Stochastic Analysis

We continue with the stochastic analysis to show that the deviation bound and the RSC

condition hold simultaneously with high probability.

Lemma B.3 (Deviation bound). Suppose that Assumptions 1 and 2 hold. If T (cid:38) p and
λSSN (cid:38) κ2M121−d(cid:112)p/T , with probability at least 1 − exp[−C(p − d)],

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
T

T
(cid:88)

t=1

Yt−1 ◦ Et

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)SSN∗

≤

λSSN
4

where M1 = λmax(Σe)/µ1/2

min(A).
If T (cid:38) max1≤i≤d p−ip and λSN (cid:38) κ2M1d−2 (cid:80)d

i=1

(cid:112)p−ip/T , with probability at least 1 −

2 (cid:80)d

i=1 exp(−Cp−ip),

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
T

T
(cid:88)

t=1

Yt−1 ◦ Et

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)SN∗

≤

λSN
4

.

Moreover, if T (cid:38) p and λMN (cid:38) κ2M1

(cid:112)p/T , with probability at least 1 − exp(−Cp),

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
T

T
(cid:88)

t=1

vec(Yt−1)vec(Et)(cid:62)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)∗

≤

λMN
4

.

40

Next, we prove the restricted strong convexity for regularized estimators. According to

Lemma B.3, we need the sample size T (cid:38) p for all three estimators. In this case, we can

establish the strong convexity condition that is stronger than the RSC condition.

Lemma B.4 (Strong convexity). Under Assumptions 1 and 2, for T (cid:38) max(κ2, κ4)M −2

2 p,

with probability at least 1 − exp[−C min(κ−2, κ−4)M 2

2 p],

1
T

T
(cid:88)

t=1

(cid:107)(cid:104)∆, Yt−1(cid:105)(cid:107)2

F ≥ αRSC(cid:107)∆(cid:107)2
F,

where M2 = [λmin(Σe)µmax(A)]/[λmax(Σe)µmin(A)] and αRSC = λmin(Σe)/(2µmax(A)).

B.2 Proofs of Theorems 2–5

Proof of Theorems 2 and 3. Theorems 2 and 3 can be proved based on Lemmas B.2–B.4

following the same line of the proof of Theorem 4 given below. Therefore, we omit the

details here.

Proof of Theorem 4. The proof of Theorem 4 has been split into Lemmas B.2–B.4. By

Lemma B.2, for deterministic realization with sample size T of a tensor autoregressive pro-
cess, if we choose λSSN ≥ 4(cid:107)T −1 (cid:80)T

Yt−1 ◦ Et(cid:107)SSN∗ and RSC condition holds for the square

t=1

loss with the parameter αRSC, the following error upper bound can be established

(cid:107)∆(cid:107)F (cid:46) √
sq

(cid:18) 2d−1λSSN
αRSC

(cid:19)1−q/2

.

Denote the events E1(β) = {β ≥ 4(cid:107)T −1 (cid:80)T
Yt−1◦Et(cid:107)SSN∗} and E2(α) = {λmin(XX (cid:62)/T ) ≥
α}. If we take λSSN (cid:38) κ2M121−d(cid:112)p/T , it suﬃces to show that E1(Cκ2M121−d(cid:112)p/T ) and

t=1

E2(αRSC/2) occur simultaneously with high probability.

By Lemma B.3, when T (cid:38) p,

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
T

T
(cid:88)

t=1

Yt−1 ◦ Et

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)SSN∗

(cid:46) κ2M121−d

(cid:114) p
T

with probability at least 1 − exp[−C(p − d)].

41

By Lemma B.4, when T (cid:38) max(κ2, κ4)M −2

2 p, for any ∆ ∈ Rp1×···×p2d,

1
T

T
(cid:88)

t=1

(cid:107)(cid:104)∆, Yt−1(cid:105)(cid:107)2

F ≥

λmin(Σe)
2µmax(A)

(cid:107)∆(cid:107)2
F

with probability at least 1 − exp[−C min(κ−2, κ−4)M 2

2 p].

Hence, when T (cid:38) [1 + max(κ2, κ4)M −2

2 ]p and λ (cid:38) κ2M121−d(cid:112)p/T , with probability at
2 p], the condition λ ≥ 4(cid:107)T −1 (cid:80)T
Yt−1 ◦
t=1
Et(cid:107)SSN∗ and the RSC condition with the parameter αRSC = λmin(Σe)/µmax(A) hold.

least 1 − exp[−C(p − d)] − exp[−C min(κ−2, κ−4)M 2

Proof of Theorem 5. Theorem 4 gives the Frobenius estimation error bound. For simplicity,

we write (cid:98)A = (cid:98)ASSN and (cid:101)A = (cid:98)ATSSN in this proof. By deﬁnition, for any tensor A ∈
Rp1×···×p2d,

(cid:107)A(cid:107)2

F = (cid:107)A(i)(cid:107)2

F =

j (A(i)),
σ2

i = 1, 2, . . . , 2d.

pi(cid:88)

j=1

In other words, the Frobenius norm of the error tensor is equivalent to the (cid:96)2 norm of singular

values of the one-mode matricization. By Mirsky’s singular value inequality (Mirsky, 1960),

pi(cid:88)

[σj( (cid:98)A(i)) − σj(A(i))]2 ≤

j=1

pi(cid:88)

j=1

j ( (cid:98)A(i) − A(i)) = (cid:107) (cid:98)A − A(cid:107)2
σ2
F,

i = 1, 2, . . . , 2d.

(B.3)

Obviously, the (cid:96)∞ error bound is smaller than the (cid:96)2 error bound, so it follows the same
upper bound. By Theorem 4, when λSSN (cid:16) κ2M121−d(cid:112)p/T , with probability approaching

one,

max
1≤i≤2d

max
1≤j≤pi

|σj( (cid:98)A(i)) − σj(A(i))| ≤ max
1≤i≤2d

(cid:40) pi(cid:88)

[σj( (cid:98)A(i)) − σj(A(i))]2

(cid:41)1/2

j=1

≤(cid:107) (cid:98)A − A(cid:107)F (cid:46) κ2M1
αRSC

(cid:114) s0p
T

.

(B.4)

Therefore, by Assumption 3, as T → ∞,

γ (cid:29) max
1≤i≤2d

max
1≤j≤pi

|σj( (cid:98)A(i)) − σj(A(i))|.

Then, for any j > ri, since σj(A(i)) = 0, we have γ (cid:29) σj( (cid:98)A(i)). Thus, for all i = 1, . . . , 2d,
σj( (cid:98)A(i)) will be truncated for all j > ri. Meanwhile, by Assumption 3 and (B.4), we have

42

σri( (cid:98)A(i)) > γ for T suﬃciently large, for all i = 1, . . . , 2d. Therefore, the rank selection
consistency of the truncated estimator (cid:101)A can be established.

Denote the event E = {rank( (cid:101)A(i)) = ri, for i = 1, . . . , 2d}. For a generic tensor T ∈
Rp1×···×p2d, denote the sub-tensor Tik=j, a p1 × · · · × pk−1 × 1 × pk+1 × · · · × p2d tensor such

that

(Tik=j)i1...ik−11ik+1...i2d = Ti1...ik−1jik+1...i2d,

and sub-tensor Tik>j, a p1 × · · · × pk−1 × (pk − j) × pk+1 × · · · × p2d tensor such that

(Tik>j)i1...ik−1(cid:96)ik+1...i2d = Ti1...ik−1((cid:96)+j)ik+1...i2d.

Let the HOSVD of (cid:98)A be (cid:98)G ×2d

i=1 (cid:98)U i. By deﬁnition, (cid:98)G is a p1 × · · · × p2d all-orthogonal

and sorted tensor such that

(cid:107)(cid:98)Gik=1(cid:107)F ≥ (cid:107)(cid:98)Gik=2(cid:107)F ≥ · · · ≥ (cid:107)(cid:98)Gik=pk(cid:107)F,

for k = 1, . . . , 2d. On E, the truncation procedure is equivalent to truncating all the sub-

tensors (cid:98)Gik>rk to zeros. Thus, (cid:107) (cid:98)A − (cid:101)A(cid:107)F = (cid:107)(cid:98)G − (cid:101)G(cid:107)2

k=1 (cid:107)(cid:98)Gik>rk(cid:107)2
F.
By the deﬁnition of HOSVD, (cid:107)(cid:98)Gik=j(cid:107)F = σj((cid:98)G(k)) = σj( (cid:98)A(k)), and then

F ≤ (cid:80)2d

(cid:107)(cid:98)Gik>rk(cid:107)2

F =

pk(cid:88)

i=rk+1

i ( (cid:98)A(k)) =
σ2

pk(cid:88)

[σi( (cid:98)A(k)) − σi(A(k))]2

i=rk+1
pk(cid:88)

[σi( (cid:98)A(k)) − σi(A(k))]2 ≤ (cid:107) (cid:98)A − A(cid:107)2
F,

≤

i=1

where the last inequality follows from (B.3).

Finally, on the event E, (cid:107) (cid:101)A−A(cid:107)F ≤ (cid:107) (cid:101)A− (cid:98)A(cid:107)F +(cid:107) (cid:98)A−A(cid:107)F ≤ (1+

2d)(cid:107) (cid:98)A−A(cid:107)F, where
d is ﬁxed. Note that Theorem 4 implies the asymptotic rate (cid:107) (cid:98)A − A(cid:107)F = Op((cid:112)s0p/T ) and
the ﬁrst part of this proof shows that P(E) → 1, as T → ∞. The proof is complete.

√

B.3 Proofs of Lemmas B.1–B.4

Proof of Lemma B.1. In this part, we focus on (cid:98)ASSN and simplify it to (cid:98)A. The tuning
parameter λSSN is simpliﬁed to λ. The proof can be readily extended to (cid:98)ASN and (cid:98)AMN.

43

Note that the quadratic loss function can be rewritten as LT (A) = T −1 (cid:80)T

t=1 (cid:107)Yt −
2, where yt = vec(Yt). By the optimality of the

(cid:104)A, Yt−1(cid:105)(cid:107)2

F = T −1 (cid:80)T

t=1 (cid:107)yt − A[S2]yt−1(cid:107)2

SSN estimator,

1
T

1
T

1
T

1
T

T
(cid:88)

t=1
T
(cid:88)

t=1
T
(cid:88)

t=1
T
(cid:88)

t=1

⇒

⇒

⇒

(cid:107)yt − (cid:98)A[S2]yt−1(cid:107)2

2 + λ(cid:107) (cid:98)A(cid:107)SSN ≤

1
T

T
(cid:88)

t=1

(cid:107)yt − A[S2]yt−1(cid:107)2

2 + λ(cid:107)A(cid:107)SSN

(cid:107)∆[S2]yt−1(cid:107)2

2 ≤

2
T

T
(cid:88)

(cid:104)et, ∆[S2]yt−1(cid:105) + λ((cid:107)A(cid:107)SSN − (cid:107) (cid:98)A(cid:107)SSN)

(cid:107)∆[S2]yt−1(cid:107)2

2 ≤ 2

T −1

T
(cid:88)

(cid:43)

Yt−1 ◦ Et, ∆

+ λ((cid:107)A(cid:107)SSN − (cid:107) (cid:98)A(cid:107)SSN)

t=1

(cid:42)

(cid:107)∆[S2]yt−1(cid:107)2

2 ≤ 2(cid:107)∆(cid:107)SSN

t=1
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

T −1

T
(cid:88)

t=1

Yt−1 ◦ Et

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)SSN∗

+ λ((cid:107)A(cid:107)SSN − (cid:107) (cid:98)A(cid:107)SSN),

where (cid:107) · (cid:107)SSN∗ refers to the dual norm of the SSN norm.

By triangle inequality and decomposability, we have

(cid:107) (cid:98)A(cid:107)SSN − (cid:107)A(cid:107)SSN = (cid:107)A + ∆(cid:107)SSN − (cid:107)A(cid:107)SSN =

2d−1
(cid:88)

k=1

(cid:107)A[Ik] + ∆[Ik](cid:107)∗ −

2d−1
(cid:88)

k=1

(cid:107)A[Ik](cid:107)∗

=

≥

≥

2d−1
(cid:88)

k=1
2d−1
(cid:88)

k=1
2d−1
(cid:88)

k=1

(cid:107)A(k)

M + A(k)

M⊥ + ∆(k)

M

+ ∆(k)
M

⊥(cid:107)∗ −

2d−1
(cid:88)

k=1

(cid:107)A[Ik](cid:107)∗

(cid:104)
(cid:107)A(k)

M + ∆(k)

⊥(cid:107)∗ − (cid:107)A(k)

M⊥ + ∆(k)

M

M

(cid:107)∗ − (cid:107)A(k)

M⊥(cid:107)∗ − (cid:107)A(k)

M (cid:107)∗

(cid:105)

(cid:104)
(cid:107)∆(k)
M

⊥(cid:107)∗ − 2(cid:107)A(k)

M⊥(cid:107)∗ − (cid:107)∆(k)

M

(cid:105)

.

(cid:107)∗

If λ ≥ 4(cid:107)T −1 (cid:80)T

t=1

Yt−1 ◦ Et(cid:107)SSN∗, we have

0 ≤

≤

=

1
T

λ
2

λ
2

t=1
2d−1
(cid:88)

(cid:104)

(cid:107)∆(k)
M

k=1
2d−1
(cid:88)

(cid:104)

k=1

T
(cid:88)

(cid:107)∆[S2]yt−1(cid:107)2

2 ≤

λ
2

(cid:107)∆(cid:107)SSN − λ((cid:107) (cid:98)A(cid:107)SSN − (cid:107)A(cid:107)SSN)

(cid:107)∗ + (cid:107)∆(k)
M

⊥(cid:107)∗ − 2(cid:107)∆(k)
M

⊥(cid:107)∗ + 4(cid:107)A(k)

M⊥(cid:107)∗ + 2(cid:107)∆(k)

M

(cid:105)

(cid:107)∗

3(cid:107)∆(k)
M

(cid:107)∗ + 4(cid:107)A(k)

M⊥(cid:107)∗ − (cid:107)∆(k)

⊥(cid:107)∗

M

(cid:105)

.

Hence, the error ∆ lies in the restricted error set CSSN(M).

44

Proof of Lemma B.2. Similar to Lemma B.1, we focus on the SSN estimator, and the results

for SN and MN estimators can be extended in a similar way.

Note that T −1 (cid:80)T

t=1 (cid:107)(cid:104)∆, Yt−1(cid:105)(cid:107)2

F = T −1 (cid:80)T

t=1 (cid:107)∆[S2]yt−1(cid:107)2

2. Following the proof of Lemma

B.1, ∆ ∈ CSSN(M) and

1
T

T
(cid:88)

t=1

(cid:107)(cid:104)∆, Yt−1(cid:105)(cid:107)2

F ≤

λ
2

(cid:107)∆(cid:107)SSN + λ((cid:107)A(cid:107)SSN − (cid:107) (cid:98)A(cid:107)SSN) ≤

3λ
2

(cid:107)∆(cid:107)SSN

=

3λ
2

≤ 6λ

≤ 6λ

2d−1
(cid:88)

k=1
2d−1
(cid:88)

k=1
2d−1
(cid:88)

k=1

(cid:107)∆[Ik](cid:107)∗ ≤

3λ
2

2d−1
(cid:88)

(cid:16)

k=1

(cid:107)∆(k)
M

(cid:107)∗ + (cid:107)∆(k)
M

⊥(cid:107)∗

(cid:17)

(cid:107)∆(k)
M

(cid:107)∗ + 6λ

2d−1
(cid:88)

k=1

(cid:107)A(k)

M⊥(cid:107)∗

√

2sk(cid:107)∆(k)
M

(cid:107)F + 6λ

2d−1
(cid:88)

k=1

(cid:107)A(k)

M⊥(cid:107)∗

2d−1
(cid:88)

√

(cid:46) λ

k=1

2sk(cid:107)∆(cid:107)F + 6λ

2d−1
(cid:88)

k=1

(cid:107)A(k)

M⊥(cid:107)∗

where the last inequality stems from the fact that ∆(k)
M

has a matrix rank at most 2sk, similar

to Lemma 1 in Negahban and Wainwright (2011).

As the RSC condition holds with the parameter αRSC and restricted error set CSSN(M),

αRSC(cid:107)∆(cid:107)2

F ≤

1
T

T
(cid:88)

t=1

(cid:107)(cid:104)∆, Yt−1(cid:105)(cid:107)2
F

(cid:46) λ

2d−1
(cid:88)

√

k=1

sk(cid:107)∆(cid:107)F + λ

2d−1
(cid:88)

k=1

(cid:107)A(k)

M⊥(cid:107)∗.

Thus, by Cauchy inequality,

(cid:107)∆(cid:107)2
F

(cid:46)

λ2((cid:80)2d−1
k=1
α2

RSC

√

sk)2

+

λ (cid:80)2d−1

k=1 (cid:107)A(k)
αRSC

M⊥(cid:107)∗

(cid:46) λ22d−1 (cid:80)2d−1
α2

k=1 sk

RSC

+

λ (cid:80)2d−1

k=1 (cid:107)A(k)
αRSC

M⊥(cid:107)∗

.

Consider any threhold τk ≥ 0 and deﬁne the thresholded subspace M(k) corresponding
to the column and row spaces spanned by the ﬁrst r(k) singular vectors of A[Ik] where
σ1(A[Ik]) ≥ · · · ≥ σr(k)(A[Ik]) > τk ≥ σr(k)+1(A[Ik]). By the deﬁnition of Bq(s(k)
have s(k)
· τ −q
k .

q ≥ r(k) · τ q

; p, p), we

k and thus r(k) ≤ s(k)
Then, the approximation error can be bounded by

q

q

(cid:107)A(k)

M⊥(cid:107)∗ =

p
(cid:88)

σr(A[Ik]) =

p
(cid:88)

r=r(k)+1

r=r(k)+1

r(A[Ik]) · σ1−q
σq

r

(A[Ik]) ≤ s(k)

q

· τ 1−q
k

.

45

The estimation error can be bounded by
(cid:46) λ22d−1 (cid:80)2d−1
k=1 s(k)
α2

(cid:107)∆(cid:107)2
F

q

RSC

· τ −q
k

+

λ (cid:80)2d−1

k=1 s(k)
q
αRSC

· τ 1−q
k

.

Setting each τk (cid:16) α−1

RSC(q/(1 − q))2d−1λ, the upper bound can be minimized to

(cid:107)∆(cid:107)2
F

(cid:46) 21−d

2d−1
(cid:88)

k=1

s(k)
q

(cid:18) λ · 2d−1
αRSC

(cid:19)2−q

.

The proof is complete.

Proof of Lemma B.3. First, we derive an upper bound of the dual norm of the SSN norm.

By deﬁnition, for any tensor A and collection of index sets I = {I1, . . . , I2d−1}, the SSN norm

is

(cid:107)A(cid:107)SSN =

2d−1
(cid:88)

(cid:107)A[Ik](cid:107)∗,

k=1
and its dual norm is (cid:107)A(cid:107)SSN∗ := sup(cid:104)W, A(cid:105) such that (cid:107)W(cid:107)SSN ≤ 1. By a method similar

to that in Tomioka et al. (2011), it can be shown that

(cid:107)A(cid:107)SSN∗ =

inf

(cid:80)2d−1
k=1

Xk=A

max
k=1,...,2d−1

(cid:107)(Xk)[Ik](cid:107)op.

Then, we can take Xk = ((cid:80)2d−1

k=1 1/ck)−1(A/ck), where ck = (cid:107)A[Ik](cid:107)op, and apply Jensen’s

inequality so that we have

(cid:107)A(cid:107)SSN∗ ≤ 2−2(d−1)

2d−1
(cid:88)

k=1

(cid:107)A[Ik](cid:107)op.

Hence, we have

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
T

T
(cid:88)

t=1

Yt−1 ◦ Et

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)SSN∗

≤

1
22(d−1)

2d−1
(cid:88)

k=1

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
T

T
(cid:88)

t=1

(Yt−1 ◦ Et)[Ik]

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)op

.

In other words, the dual norm of the SSN norm can be upper bounded by the sum of the
scaled matrix operator norms of diﬀerent matricizations of the tensor T −1 (cid:80)T

Yt−1 ◦ Et.

t=1

All of the square matricizations based on Ik lead to a square p-by-p matrix. Therefore,

by the deviation bound in Lemma B.5, we can take a union bound such that

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
T

T
(cid:88)

t=1

Yt−1 ◦ Et

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)SSN∗

46

≤

Cκ2M1
2d−1

(cid:114) p
T

with probability at least 1 − exp[−C(p − d)].

Next, for the SN estimator, we can obtain a similar upper bound of the dual norm of the

SN norm. The SN norm is deﬁned as

(cid:107)A(cid:107)SN =

2d
(cid:88)

i=1

(cid:107)A(i)(cid:107)∗,

and its dual norm has the equivalent form

(cid:107)A(cid:107)SN∗ =

inf

(cid:80)2d

i=1

Yi=A

max
i=1,...,2d

(cid:107)(Yi)(i)(cid:107)op.

Then, we can obtain an upper bound,

(cid:107)A(cid:107)SN∗ ≤

1
(2d)2

2d
(cid:88)

i=1

(cid:107)A(i)(cid:107)op.

Then, for each one-mode matricization, we have the deviation bound. Then, we can take a

union bound such that

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
T

T
(cid:88)

t=1

Yt−1 ◦ Et

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)SN∗

≤

Cκ2M1
(2d)2

2d
(cid:88)

i=1

(cid:114) p−ip
T

,

with probability at least 1 − 2d exp[−Cp].

Finally, the MN estimator uses a special case of square matricization, and the upper

bound for the MN estimator can be obtained by Lemma B.5.

Proof of Lemma B.4. For any M ∈ Rm×p, denote RT (M ) = (cid:80)T −1
2. Note that
RT (∆[S2]) ≥ ERT (∆[S2]) − sup∆ |RT (∆[S2]) − ERT (∆[S2])|. Following the proof of Lemma
B.5, ERT (∆[S2]) = (cid:107)(I T ⊗ ∆[S2])P D(cid:107)2

F · λmin(Σe)λmin(P P (cid:62)).

t=0 (cid:107)M yt(cid:107)2

F ≥ T (cid:107)∆(cid:107)2

Similar to Lemma B.6, for any v ∈ Sp−1 and any t > 0,

P[|RT (v(cid:62)) − ERT (v(cid:62))| ≥ t]
(cid:18)
t2

(cid:18)

≤2 exp

− min

max(P P (cid:62))
Considering an (cid:15)-covering net of Sp−1, by Lemma B.7, we can easily construct the union

max(Σe)λ2

κ4T λ2

,

t
κ2λmax(Σe)λmax(P P (cid:62))

(cid:19)(cid:19)

.

bound for T (cid:38) p,

P

(cid:20)

sup
v∈Sp−1
(cid:18)

≤C exp

p − min

(cid:21)
|RT (v(cid:62)) − ERT (v(cid:62))| ≥ t

(cid:18)

t2

κ4T λ2

max(Σe)λ2

max(P P (cid:62))

,

t
κ2λmax(Σe)λmax(P P (cid:62))

(cid:19)(cid:19)

,

47

Letting t = λmin(Σe)λmin(P P (cid:62))/2, for T (cid:38) M −2

2 max(κ4, κ2)p, we have

P[|RT (v(cid:62)) − ERT (v(cid:62))| ≥ λmin(Σe)λmin(P P (cid:62))/2] ≤ 2 exp(−CM 2

2 min(κ−4, κ−2)T ),

where M2 = [λmin(Σe)λmin(P P (cid:62))]/[λmax(Σe)λmax(P P (cid:62))].

Therefore, with probability at least 1 − 2 exp(−CM 2

2 min(κ−4, κ−2)T ),

RT (∆[S2]) ≥

1
2

λmin(Σe)λmin(P P (cid:62))(cid:107)∆(cid:107)2
F.

Finally, since P is related to the VMA(∞) process, by the spectral measure of ARMA pro-

cess discussed in Basu and Michailidis (2015), we may replace λmax(P P (cid:62)) and λmin(P P (cid:62))

with 1/µmin(A) and 1/µmax(A), respectively.

B.4 Three Auxiliary Lemmas

Three auxiliary lemmas used in the proofs of Lemmas B.3 and B.4 are presented below.

Lemma B.5 (Deviation bound on diﬀerent matricizations). For any index set I ⊂ {1, 2, . . . , 2d},
denote q = (cid:81)2d

If T (cid:38) (q + q(cid:48)), with probability at least

i=1,i∈I pi and q(cid:48) = (cid:81)2d

i=1,i /∈I pi.

1 − exp[−C(q + q(cid:48))],

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
T

T
(cid:88)

(Yt−1 ◦ Et)[I]

t=1

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)op

< Cκ2M1

(cid:112)(q + q(cid:48))/T .

where M1 = λmax(Σe)/µ1/2

min(A).

Proof. For any index set I ⊂ {1, 2, . . . , 2d} and 2dth-mode tensor T, denote the inverse

operation of the multi-mode matricization T = T[I] by T [I] = T. Denote W(r; q, q(cid:48)) =

{W ∈ Rq×q(cid:48) : rank(W ) = r, (cid:107)W (cid:107)F = 1}.

By deﬁnition, (cid:107)T −1 (cid:80)T

t=1(Yt−1 ◦ Et)[I](cid:107)op = supW ∈W(1;q,q(cid:48))(cid:104)T −1 (cid:80)T

t=1(Yt−1 ◦ Et)[I], W (cid:105) =

supW ∈W(1;q,q(cid:48))(cid:104)T −1 (cid:80)T

t=1 vec(Et)vec(Yt−1)(cid:62), (W [I])(cid:62)

[S1](cid:105).

For an arbitrary matrix W ∈ Rq×q(cid:48) such that (cid:107)W (cid:107)F = 1, denote M = (W [I])(cid:62)

[S1]. Then,

one can easily check that (cid:104)(Yt−1 ◦ Et)[I], W (cid:105) = (cid:104)et, M yt−1(cid:105).

48

For a ﬁxed M , denote St(M ) = (cid:80)t

s=1(cid:104)es, M ys−1(cid:105) and Rt(M ) = (cid:80)t−1

s=0 (cid:107)M ys(cid:107)2

2, for

1 ≤ t ≤ T . By the standard Chernoﬀ argument, for any α > 0, β > 0 and c > 0,

P[{ST (M ) ≥ α} ∩ {RT (M ) ≤ β}]

= inf
m>0

= inf
m>0

≤ inf
m>0

= inf
m>0

≤ inf
m>0

P[{exp(mST (M )) ≥ exp(mα)} ∩ {RT (M ) ≤ β}]

P[exp(mST (M ))I(RT (M ) ≤ β) ≥ exp(mα)]

exp(−mα)E[exp(mST (M ))I(RT (M ) ≤ β)]

exp(−mα + cm2β)E[exp(mST (M ) − cm2β)I(RT (M ) ≤ β)]

exp(−mα + cm2β)E[exp(mST (M ) − cm2RT (M ))].

By the tower rule, we have

E[exp(mST (M ) − cm2RT (M ))]

=E[E[exp(mST (M ) − cm2RT (M ))]|FT −1]

=E[exp(mST −1(M ) − cm2RT −1(M ))E[exp(m(cid:104)eT , M yT −1(cid:105) − cm2(cid:107)M yT (cid:107)2

2)|FT −1]].

Since (cid:104)eT , M yT −1(cid:105) = (cid:104)ξT , Σ1/2

e M yT −1(cid:105), one can easily check that (cid:104)eT , M yT −1(cid:105) is a

κ2λmax(Σe)(cid:107)M yT −1(cid:107)2

2-sub-Gaussian random variable. In other words, E[exp(m(cid:104)eT , M yT −1(cid:105))] ≤

exp(m2κ2λmax(Σe)(cid:107)M yT −1(cid:107)2

2/2). Thus, letting c = κλmax(Σe)/2, we have

E[exp(mST (M ) − m2κ2λmax(Σe)RT (M )/2)]

≤E[exp(mST −1(M ) − m2κ2λmax(Σe)RT −1(M )/2)]

≤ · · · ≤ E[exp(mS1(M ) − m2κ2λmax(Σe)R1(M )/2)] ≤ 1.

Hence, we have that, for any α > 0 and β > 0,

P[{ST (M ) ≥ α} ∩ {RT (M ) ≤ β}]

≤ inf
m>0

exp(−mα + m2κ2λmax(Σe)β/2)
(cid:18)

(cid:19)

= exp

−

α2
2κ2λmax(Σe)β

.

t

κ2λ2

max(Σe)λ2

max(P P (cid:62))

,

(B.5)

(cid:19)(cid:19)

.

By Lemma B.6, we have that for any t > 0,

P[|RT (M ) − ERT (M )| ≥ t]
(cid:18)
t2

(cid:18)

≤2 exp

− min

κ4T λ2

max(Σe)λ2

max(P P (cid:62))

49

In addition, ERT (M ) = tr(ΣM ) = (cid:107)(I T ⊗ M )P D(cid:107)2

F ≤ T · λmax(Σe)λmax(P P (cid:62)). Letting

t = Cκ2T λmax(Σe)λmax(P P (cid:62)), we have

P[RT (M ) ≥ Cκ2T λmax(Σe)λmax(P P (cid:62))] ≤ 2 exp(−CT ).

Next, consider a (cid:15)-net W(1; q, q(cid:48)) for W(1; q, q(cid:48)). For any matrix W ∈ W(1; q, q(cid:48)), there

exist a matrix W ∈ W(1; q, q(cid:48)) such that (cid:107)W − W (cid:107)F ≤ (cid:15). Since the rank of ∆ = W − W

is at most 2, we can split the SVD of ∆ into 2 parts, such that ∆ = ∆1 + ∆2, where

rank(∆1) = rank(∆2) = 1 and (cid:104)∆1, ∆2(cid:105) = 0. Then, for any matrix N ∈ Rq×q(cid:48), we have

(cid:104)N , W (cid:105) = (cid:104)N , W (cid:105) + (cid:104)N , ∆(cid:105) = (cid:104)N , W (cid:105) +

2
(cid:88)

(cid:104)N , ∆i/(cid:107)∆i(cid:107)F(cid:105)(cid:107)∆i(cid:107)F,

where ∆i/(cid:107)∆i(cid:107)F ∈ W(1; q, q(cid:48)). Since (cid:107)∆(cid:107)2

(cid:107)∆1(cid:107)F + (cid:107)∆2(cid:107)F ≤

√

2(cid:107)∆(cid:107)F =

√

2(cid:15). Hence, we have

i=1
F + (cid:107)∆2(cid:107)2
F = (cid:107)∆1(cid:107)2

F, by Cauchy inequality,

γ :=

In other words,

sup
W ∈W(1;q,q(cid:48))

(cid:104)N , W (cid:105) ≤ max

(cid:104)N , W (cid:105) +

W ∈W(1;q,q(cid:48))

√

2γ(cid:15).

sup
W ∈W(1;q,q(cid:48))

(cid:104)N , W (cid:105) ≤ (1 −

√

2(cid:15))−1 max

(cid:104)N , W (cid:105).

W ∈W(1;q,q(cid:48))

Therefore, we have that, for any x > 0,
(cid:34)

(cid:42)

(cid:43)

(cid:35)

P

sup
W ∈W(1;q,q(cid:48))

(cid:34)

≤P

max
W ∈W(1;q,q(cid:48))

(Yt−1 ◦ Et)[I], W

≥ x

(cid:43)

(Yt−1 ◦ Et)[I], W

≥ (1 −

(cid:35)

√

2(cid:15))x

(B.6)

T
(cid:88)

(Yt−1 ◦ Et)[I], W

(cid:43)

√

2(cid:15))x

(cid:35)

.

≥ (1 −

≤|W(1; q, q(cid:48))| · P

Note that by (B.5), for any x > 0,

1
T

(cid:42)

1
T
(cid:34)(cid:42)

T
(cid:88)

t=1
T
(cid:88)

t=1

1
T

t=1

(cid:43)

(cid:34)(cid:42)

P

1
T

T
(cid:88)

t=1

(Yt−1 ◦ Et)[I], W

≥ (1 −

(cid:35)

√

2(cid:15))x

≤P[{ST (M ) ≥ T (1 −

√

2(cid:15))x} ∩ {RT (M ) ≤ Cκ2T λmax(Σe)λmax(P P (cid:62))}]

+P[RT (M ) > Cκ2T λmax(Σe)λmax(P P (cid:62))]
CT x2

(cid:20)

(cid:21)

≤ exp

−

κ4λ2

max(Σe)λmax(P P (cid:62))

+ 2 exp(−CT ).

50

By Lemma 3.1 in Candes and Plan (2011), for a (cid:15)-net for W(1; q, q(cid:48)), the covering number

|W(1; q, q(cid:48))| ≤ (9/(cid:15))q+q(cid:48). Combining (B.6), we have that, when T (cid:38) q + q(cid:48), for any x > 0,

(cid:34)

P

sup
W ∈W(1;q,q(cid:48))
(cid:20)

T
(cid:88)

(Yt−1 ◦ Et)[I], W

(cid:43)

(cid:35)

≥ x

(cid:42)

1
T

t=1

≤ exp

(q + q(cid:48)) log(9/(cid:15)) −

CT x2

(cid:21)

κ4λ2

max(Σe)λmax(P P (cid:62))

+ 2 exp[(q + q(cid:48)) log(9/(cid:15)) − CT ].

Taking (cid:15) = 0.1 and x = Cκ2λmax(Σe)λ1/2

max(P P (cid:62)) · (cid:112)(q + q(cid:48))/T , we have

(cid:34)

P

sup
W ∈W(1;q,q(cid:48))

(cid:42)

1
T

T
(cid:88)

t=1

(cid:43)

(Yt−1 ◦ Et)[I], W

≤ exp[−C(q + q(cid:48))].

≥ Cκ2λmax(Σe)λ1/2

max(P P (cid:62))

(cid:114) q + q(cid:48)
T

(cid:35)

Finally, since P is related to the VMA(∞) process, by the spectral measure of ARMA process

discussed in Basu and Michailidis (2015), we may replace λmax(P P (cid:62)) with 1/µmin(A).

Lemma B.6. For any M ∈ Rp×p such that (cid:107)M (cid:107)F = 1, denote RT (M ) = (cid:80)T −1

t=0 (cid:107)M yt(cid:107)2
2.

Then, for any t > 0,

P[|RT (M ) − ERT (M )| ≥ t]
(cid:18)
t2

(cid:18)

≤2 exp

− min

κ4T λ2

max(Σe)λ2

max(P P (cid:62))

,

t

κ2λ2

max(Σe)λ2

max(P P (cid:62))

where P is deﬁned as

P =












I p A A2 A3

. . . AT −1

O I p A A2
...
...
...

...

. . . AT −2
. . .

...

O O O O . . .

I p












. . .

. . .

. . .

. . .

.

(cid:19)(cid:19)

,

(B.7)

Proof. Denote by y = (y(cid:62)

T −1, y(cid:62)

T −2, . . . , y(cid:62)

0 )(cid:62), e = (e(cid:62)

T −1, e(cid:62)

T −2, . . . , e(cid:62)

0 , . . . )(cid:62), and ξ =

(ξ(cid:62)

T −1, ξ(cid:62)

T −2, . . . , ξ(cid:62)

0 , . . . )(cid:62). Based on the moving average representation of VAR(1), we can
rewrite yt to a VMA(∞), yt = et + Aet−1 + A2et−2 + A3et−2 + · · · . Note that RT (M ) =
y(cid:62)(I T ⊗ M (cid:62)M )y = e(cid:62)P (cid:62)(I T ⊗ M (cid:62)M )P e = ξ(cid:62)DP (cid:62)(I T ⊗ M (cid:62)M )P Dξ := ξ(cid:62)ΣM ξ,

51

where P is deﬁned in (B.7) and

D =












Σ1/2
e

O

O . . .

O Σ1/2

e

O . . .

O
...

e

O Σ1/2
...
...

. . .
. . .












.

By Hanson-Wright inequality, for any t > 0,

P[|RT (M ) − ERT (M )| ≥ t] ≤ 2 exp

(cid:18)

(cid:18)

− min

t2
κ4(cid:107)ΣM (cid:107)2
F

,

t
κ2(cid:107)ΣM (cid:107)op

(cid:19)(cid:19)

.

As (cid:107)M (cid:107)F = 1, by the submultiplicative property of the Frobenius norm and operator

norm, we have (cid:107)ΣM (cid:107)2

F ≤ T · λ2

max(Σe)λ2

max(P P (cid:62)) and (cid:107)ΣM (cid:107)op ≤ λmax(Σe)λmax(P P (cid:62)).

These imply that, for any t > 0,

P[|RT (M ) − ERT (M )| ≥ t]
(cid:18)
t2

(cid:18)

≤2 exp

− min

κ4T λ2

max(Σe)λ2

max(P P (cid:62))

,

t
κ2λmax(Σe)λmax(P P (cid:62))

(cid:19)(cid:19)

.

The proof of this lemma is accomplished.

Lemma B.7. (Covering number of unit sphere) Let N be an ε-net of the unit sphere Sp−1,

where ε ∈ (0, 1]. Then,

|N | ≤

(cid:19)p

.

(cid:18) 3
ε

Proof. This lemma follows directly from Corollary 4.2.13 of Vershynin (2018).

52

References

Bai, J. and Wang, P. (2016). Econometric analysis of large factor models. Annual Review

of Economics, 8:53–80.

Basu, S. and Michailidis, G. (2015). Regularized estimation in sparse high-dimensional time

series models. Annals of Statistics, 43:1535–1567.

Bi, X., Qu, A., and Shen, X. (2018). Multilayer tensor factorization with applications to

recommender systems. Annals of Statistics, 46:3308–3333.

Boyd, S., Parikh, N., Chu, E., Peleato, B., Eckstein, J., et al. (2011). Distributed op-

timization and statistical learning via the alternating direction method of multipliers.

Foundations and Trends® in Machine learning, 3:1–122.

Candes, E. J. and Plan, Y. (2011). Tight oracle inequalities for low-rank matrix recovery from

a minimal number of noisy random measurements. IEEE Transactions on Information

Theory, 57:2342–2359.

Chen, E. Y. and Chen, R. (2019). Modeling dynamic transport network with matrix factor

models: with an application to international trade ﬂow. arXiv:1901.00769 [econ.EM].

Chen, E. Y., Tsay, R. S., and Chen, R. (2020a). Constrained factor models for high-

dimensional matrix-variate time series. Journal of the American Statitical Association,

115:775–793.

Chen, H., Raskutti, G., and Yuan, M. (2019). Non-convex projected gradient descent for gen-

eralized low-rank tensor regression. The Journal of Machine Learning Research, 20(1):172–

208.

Chen, R., Xiao, H., and Yang, D. (2020b). Autoregressive models for matrix-valued time

series. Journal of Econometrics. To appear.

53

Chen, R., Yang, D., and Zhang, C.-H. (2020c). Factor models for high-dimensional tensor

time series. arXiv:1905.07530v2 [stat.ME].

Davis, R. A., Zang, P., and Zheng, T. (2016). Sparse vector autoregressive modeling. Journal

of Computational and Graphical Statistics, 25:1077–1096.

De Lathauwer, L., De Moor, B., and Vandewalle, J. (2000). A multilinear singular value

decomposition. SIAM Journal on Matrix Analysis and Applications, 21:1253–1278.

Ding, S. and Cook, R. D. (2018). Matrix variate regressions and envelope models. Journal

of the Royal Statistical Society: Series B, 80:387–408.

Fama, E. F. and French, K. R. (2015). A ﬁve-factor asset pricing model. Journal of Financial

Economics, 116:1–22.

Frandsen, A. and Ge, R. (2019). Understanding composition of word embeddings via tensor

decomposition. In International Conference on Learning Representations.

French, K. R. (2020). Data library: U.S. research returns data. Available at http://mba.

tuck.dartmouth.edu/pages/faculty/ken.french/data_library.html.

Gandy, S., Recht, B., and Yamada, I. (2011). Tensor completion and low-n-rank tensor

recovery via convex optimization. Inverse Problems, 27:025010.

Han, F., Lu, H., and Liu, H. (2015a). A direct estimation of high dimensional stationary

vector autoregressions. Journal of Machine Learning Research, 16:3115–3150.

Han, F., Xu, S., and Liu, H. (2015b). Rate-optimal estimation of a high-dimensional semi-

parametric time series model. Preprint.

Han, R., Willett, R., and Zhang, A. (2020). An optimal statistical and computational

framework for generalized tensor estimation. arXiv preprint arXiv:2002.11255.

Hoﬀ, P. D. (2015). Multilinear tensor regression for longitudinal relational data. Annals of

Applied Statistics, 9:1169–1193.

54

Kolda, T. G. and Bader, B. W. (2009). Tensor decompositions and applications. SIAM

Review, 51:455–500.

Lam, C., Yao, Q., et al. (2012). Factor modeling for high-dimensional time series: inference

for the number of factors. Annals of Statistics, 40:694–726.

Li, X., Xu, D., Zhou, H., and Li, L. (2018). Tucker tensor regression and neuroimaging

analysis. Statistics in Biosciences, 10:520–545.

Liu, J., Musialski, P., Wonka, P., and Ye, J. (2013). Tensor completion for estimating missing

values in visual data. IEEE Transactions on Pattern Analysis and Machine Intelligence,

35:208–220.

Mirsky, L. (1960). Symmetric gauge functions and unitarily invariant norms. Quarterly

Journal of Mathematics, 11:50–59.

Mu, C., Huang, B., Wright, J., and Goldfarb, D. (2014). Square deal: Lower bounds and im-

proved relaxations for tensor recovery. In International Conference on Machine Learning,

pages 73–81.

Negahban, S. and Wainwright, M. J. (2011). Estimation of (near) low-rank matrices with

noise and high-dimensional scaling. Annals of Statistics, 39:1069–1097.

Negahban, S. and Wainwright, M. J. (2012). Restricted strong convexity and weighted

matrix completion: Optimal bounds with noise. Journal of Machine Learning Research,

13:1665–1697.

Negahban, S. N., Ravikumar, P., Wainwright, M. J., Yu, B., et al. (2012). A uniﬁed frame-

work for high-dimensional analysis of M -estimators with decomposable regularizers. Sta-

tistical Science, 27:538–557.

Raskutti, G., Yuan, M., and Chen, H. (2019). Convex regularization for high-dimensional

multi-response tensor regression. Annals of Statistics, 47:1554–1584.

55

Shapiro, A. (1986). Asymptotic theory of overparameterized structural models. Journal of

the American Statistical Association, 81:142–149.

Stock, J. H. and Watson, M. W. (2011). Dynamic factor models.

In Clements, M. P.

and Hendry, D. F., editors, Oxford Handbook of Economic Forecasting. Oxford University

Press.

Tomioka, R., Suzuki, T., Hayashi, K., and Kashima, H. (2011). Statistical performance

of convex tensor decomposition. In Advances in Neural Information Processing Systems

(NIPS), pages 972–980.

Tucker, L. R. (1966). Some mathematical notes on three-mode factor analysis. Psychome-

trika, 31:279–311.

Vershynin, R. (2018). High-Dimensional Probability: An Introduction with Applications in

Data Science. Cambridge University Press, Cambridge.

Walden, A. and Serroukh, A. (2002). Wavelet analysis of matrix–valued time–series. Proceed-

ings of the Royal Society of London. Series A: Mathematical, Physical and Engineering

Sciences, 458:157–179.

Wang, D., Liu, X., and Chen, R. (2019). Factor models for matrix-valued high-dimensional

time series. Journal of Econometrics, 208:231–248.

Wang, D., Zheng, Yao Lian, H., , and Li, G. (2020). High-dimensional vector autoregres-

sive time series modeling via tensor decomposition. Journal of the American Statistical

Association. To appear.

Zheng, Y. and Cheng, G. (2020). Finite time analysis of vector autoregressive models under

linear restrictions. Biometrika. To appear.

Zhou, H., Li, L., and Zhu, H. (2013). Tensor regression with applications in neuroimaging

data analysis. Journal of the American Statistical Association, 108:540–552.

56

Figure 2: Average estimation error of LTR, OLS and RRR estimators for data generated

with diﬀerent d, pi’s and multilinear ranks.

57

Case (1a)Case (1b)Case (2a)Case (2b)0.00.20.40.60.810001250150017502000r = (1,1,1,1)012310001250150017502000r = (1,1,1,1)0123410001250150017502000r = (1,1,1,1,1,1)051010001250150017502000r = (1,1,1,1,1,1)0.00.20.40.60.810001250150017502000r = (2,2,2,2)012310001250150017502000r = (2,2,2,2)0123410001250150017502000r = (2,2,2,1,1,1)051010001250150017502000r = (2,2,2,1,1,1)0.00.20.40.60.810001250150017502000r = (2,3,2,3)012310001250150017502000r = (2,3,2,3)0123410001250150017502000r = (2,2,2,2,2,2)051010001250150017502000r = (2,2,2,2,2,2)EstimatorLTRRRROLSCase d

Fixed Parameter

Varying Parameter

(a)

(b)

(c)

(d)

(e)

(f)

(g)

2

2

2

2

3

3

3

T = 500, r = (2, 2, 2, 2)

p1 = p2 = 5, 7, 9, 10, 11

p1 = p2 = 8, r = (2, 2, 2, 2)

T = 200, 400, 600, 800, 1000

p1 = p2 = 8, T = 500

r = (1, 1, 1, 1), (1, 2, 1, 2),

(2, 2, 2, 2, 2), (2, 3, 2, 3), (3, 3, 3, 3)

p = 144, T = 1000, r = (1, 1, 1, 1)

p1 = 3, 4, 6, 8, 12

T = 1000, r = (2, 2, 2, 2, 2, 2)

p = (4, 4, 4), (4, 4, 5), (4, 5, 5), (5, 5, 5), (5, 5, 6)

p = (5, 5, 5), r = (2, 2, 2, 2, 2, 2)

T = 600, 800, 1000, 1200, 1400

p = (5, 5, 5), T = 1000

r = (1, 1, 1, 1, 1, 1), (1, 1, 2, 1, 1, 2)

(1, 2, 2, 1, 2, 2), (2, 2, 2, 2, 2, 2), (2, 2, 3, 2, 2, 3)

(h)

3

p = 144, T = 1000,

r = (1, 1, 1, 1, 1, 1)

p = (2, 2, 36), (3, 3, 16),

(4, 4, 9), (3, 4, 12), (4, 6, 6)

Table 2: Parameter setting for eight cases with diﬀerent varying parameters, where p =

(p1, . . . , pd) and r = (r1, . . . , r2d).

Figure 3: Average squared estimation error for the SSN estimator for eight cases with dif-

ferent varying parameters. The error bars in each panel represent ± one standard deviation.

58

0.51.01.52.0255075100125pCase (a)120.0010.0020.0030.0040.0051TCase (b)0.51.01.52.02.55.07.5sCase (c)0.00.20.40.62.55.07.510.012.5p1Case (d)0.81.21.62.06080100120140pCase (e)1.52.02.53.00.00090.00120.00151TCase (f)1232.55.07.510.012.5sCase (g)0.00.20.40.612345Five cases for piCase (h)Figure 4: Average estimation error for TSSN, SSN, MN, and SN estimators for data gener-

ated with diﬀerent d, pi’s and multilinear ranks.

59

Case (3a)Case (3b)Case (4a)Case (4b)0.000.250.500.751.004006008001000r = (1,1,1,1)01234006008001000r = (1,1,1,1)0.00.51.01.52.02.560080010001200r = (1,1,1,1,1,1)024660080010001200r = (1,1,1,1,1,1)0.000.250.500.751.004006008001000r = (2,2,1,1)01234006008001000r = (2,2,1,1)0.00.51.01.52.02.560080010001200r = (2,2,2,1,1,1)024660080010001200r = (2,2,2,1,1,1)0.000.250.500.751.004006008001000r = (2,2,2,2)01234006008001000r = (2,2,2,2)0.00.51.01.52.02.560080010001200r = (2,2,2,2,2,2)024660080010001200r = (2,2,2,2,2,2)EstimatorTSSNSSNMNSNFigure 5: TSSN estimates of predictor and response factor matrices for 10×10 matrix-valued

portfolio return series. From left to right: (cid:101)U 1, (cid:101)U 2, (cid:101)U 3 and (cid:101)U 4.

Model

VAR VFM MAR MFM

LRTAR

SSN TSSN

Best Worst

In-sample

Out-of-sample

(cid:96)2 norm 31.61 35.85

34.12

35.86

33.18

33.38

VAR MFM

(cid:96)0 norm

8.09

9.23

9.12

9.24

8.84

8.89

VAR MFM

(cid:96)2 norm 39.84

39.11

35.26

38.53

32.60 31.47

TSSN VAR

(cid:96)∞ norm 11.98

12.30

11.47

11.00

9.53

9.33

TSSN VFM

Table 3: Average in-sample forecasting error and out-of-sample rolling forecasting error for

10 × 10 matrix-valued portfolio return series. The best cases are marked in bold.

60

0.60.40.20-0.2-0.4-0.6B/M Response Factor LoadingB/M Predictor Factor LoadingSize Predictor Factor LoadingSize Response Factor LoadingLegendFigure 6: TSSN estimates of predictor and response factor matrices for 4×4×2 tensor-valued

portfolio return series. From left to right: (cid:101)U 1, (cid:101)U 2, (cid:101)U 3, (cid:101)U 4, (cid:101)U 5 and (cid:101)U 6.

Model

VAR VFM MTAR TFM

LRTAR

SSN TSSN

Best Worst

OP-BM-Size 4 × 4 × 2 series

In-sample

Out-of-sample

In-sample

Out-of-sample

(cid:96)2 norm 19.53 20.08

19.89

20.09

19.69

19.70

VAR TFM

(cid:96)0 norm

7.67

7.91

7.85

7.92

7.76

7.77

VAR TFM

(cid:96)2 norm 22.27

20.17

20.50

20.11

20.32 19.95

TSSN VAR

(cid:96)∞ norm 10.38

10.04

9.86

10.03

9.29

9.35

SSN

VAR

Inv-BM-Size 4 × 4 × 2 series

(cid:96)2 norm 16.80 17.10

17.05

17.11

16.86

16.88

VAR TFM

(cid:96)0 norm

6.25

6.40

6.38

6.41

6.31

6.32

VAR TFM

(cid:96)2 norm 18.70

17.70

16.89

17.67

16.11

16.29

SSN

VAR

(cid:96)∞ norm

7.42

7.37

6.79

7.33

6.62

6.43

TSSN VAR

Table 4: Average in-sample forecasting error and out-of-sample rolling forecasting error for

4 × 4 × 2 tensor-valued portfolio return series. The best cases are marked in bold.

61

0.80.60.40.20-0.2-0.4-0.6-0.8Inv Response Factor LoadingB/M Response Factor LoadingSize Response Factor LoadingLegendOP Predictor Factor LoadingB/M Predictor Factor LoadingSize Predictor Factor LoadingOP Response Factor LoadingB/M Response Factor LoadingSize Response Factor LoadingInv-BM-SizeInv Predictor Factor LoadingB/M Predictor Factor LoadingSize Predictor Factor LoadingOP-BM-Size