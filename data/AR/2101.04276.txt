1
2
0
2

n
a
J

2
1

]
E
M

.
t
a
t
s
[

1
v
6
7
2
4
0
.
1
0
1
2
:
v
i
X
r
a

High-Dimensional Low-Rank Tensor Autoregressive

Time Series Modeling

Di Wangâ€ , Yao Zhengâ€¡ and Guodong Liâ€ 

â€ University of Hong Kong and â€¡University of Connecticut

January 13, 2021

Abstract

Modern technological advances have enabled an unprecedented amount of struc-

tured data with complex temporal dependence, urging the need for new methods to

eï¬ƒciently model and forecast high-dimensional tensor-valued time series. This paper

provides the ï¬rst practical tool to accomplish this task via autoregression (AR). By

considering a low-rank Tucker decomposition for the transition tensor, the proposed

tensor autoregression can ï¬‚exibly capture the underlying low-dimensional tensor dy-

namics, providing both substantial dimension reduction and meaningful dynamic factor

interpretation. For this model, we introduce both low-dimensional rank-constrained es-

timator and high-dimensional regularized estimators, and derive their asymptotic and

non-asymptotic properties.

In particular, by leveraging the special balanced struc-

ture of the AR transition tensor, a novel convex regularization approach, based on

the sum of nuclear norms of square matricizations, is proposed to eï¬ƒciently encourage

low-rankness of the coeï¬ƒcient tensor. A truncation method is further introduced to

consistently select the Tucker ranks. Simulation experiments and real data analysis

demonstrate the advantages of the proposed approach over various competing ones.

Keywords: dimension reduction; high-dimensional time series; nuclear norm; tensor de-

composition; tensor-valued data

1

 
 
 
 
 
 
1

Introduction

Modern technological development has made available enormous quantities of data, many

of which are structured and collected over time. Tensor-valued time series data, namely ob-

servations on a set of variables structured in a tensor form collected over time, have become

increasingly common in a wide variety of areas. Examples include multiple macroeconomic

indices time series for multiple countries (Chen et al., 2020c), dynamic inter-regional trans-

port ï¬‚ow data (Chen and Chen, 2019), and sequential image and video processing (Walden

and Serroukh, 2002), among many others.

To model temporal dependencies of tensor-valued time series data, naturally one might

resort to vectorization of the tensor-valued observations so that conventional vector-valued

time series models can be directly applied. For instance, let Yt âˆˆ Rp1Ã—Â·Â·Â·Ã—pd be the tensor-

valued observation at time t, for t = 1, . . . , T , where T is the sample size. Then an obvious

ï¬rst step is to conduct the following vector autoregression (VAR) for its vectorization:

vec(Yt) = Avec(Ytâˆ’1) + vec(Et),

(1)

where A âˆˆ RpÃ—p is the unknown transition matrix, with p = (cid:81)d

i=1 pi, and Et âˆˆ Rp1Ã—Â·Â·Â·Ã—pd
is the tensor-valued random error at time t. Although model (1) is potent in that it takes

into account the linear association between every variable in Yt and that in Ytâˆ’1, it clearly

suï¬€ers from two major drawbacks:

â€¢ The vectorization will destroy the important multidimensional structural information

inherently embedded in the tensor-valued observations, resulting in a lack of inter-

pretability;

â€¢ The number of unknown parameters, p2 = ((cid:81)d

i=1 pi)2, can be formidable even for small

d and pi; e.g., even when d = 3 and p1 = p2 = p3 = 3, the number of unknown

parameters will be as large as (3 Ã— 3 Ã— 3)2 = 729, while the sample size T often has a

similar magnitude in practice; see, e.g., the real data examples in Section 7.

In this work, motivated by the idea of Tucker decomposition (Tucker, 1966), we propose

2

the Low-Rank Tensor Autoregressive (LRTAR) model through folding the p Ã— p transition

matrix A in (1) into the 2d-th-order transition tensor A âˆˆ Rp1Ã—Â·Â·Â·Ã—pdÃ—p1Ã—Â·Â·Â·Ã—pd which is as-

sumed to have low multilinear ranks (r1, . . . , r2d). That is, ri can be much smaller than

pi, where pd+i = pi for i = 1, . . . , d. As a result, the aforementioned drawbacks of the

vectorization approach can be overcome by the proposed model:

â€¢ As we will show in Section 3, the proposed model can ï¬‚exibly retain the distinct

structural information across all modes of observed tensor process, encapsulating an

interpretable low-dimensional tensor dynamic relationship between Yt and Ytâˆ’1.

â€¢ As the transition tensor A is assumed to have low multilinear ranks, the parame-

ter space is simultaneously constrained in 2d directions, reducing its dimension from
((cid:81)d

i=1 pi)2 drastically to (cid:81)2d

i=1 ri(pi âˆ’ ri) + (cid:80)d

i=1 rd+i(pi âˆ’ rd+i).

i=1 ri + (cid:80)d

Recently there has been a rapidly emerging interest in high-dimensional matrix- and

tensor-valued time series analysis, particularly through factor models, e.g., the matrix factor

model proposed in Wang et al. (2019), the constrained matrix factor model in Chen et al.

(2020a), and the tensor factor model in Chen et al. (2020c). Factor models are powerful tools

for dimension reduction with great interpretability. However, unlike autoregression, factor

models do not seek to explicitly model the temporal dependency and thus by themselves

cannot be directly used for forecasting. On the other hand, despite the extensive literature

on high-dimensional VAR models in recent years (e.g. Negahban and Wainwright, 2011;

Basu and Michailidis, 2015; Han et al., 2015a; Wang et al., 2020; Zheng and Cheng, 2020),

counterparts able to meet the particular needs of matrix- and tensor-valued time series

modeling tasks have been rarely explored. The most relevant existing work in this direction

so far might be the matrix autoregressive (MAR) model in Chen et al. (2020b), where the

focus is on low-dimensional modeling; see also Hoï¬€ (2015). As we will discuss in Section 3,

the proposed model includes the MAR model as a special case, but enjoys greater ï¬‚exibility

and a more drastic reduction of the dimensionality; see also Figure 1 for an illustration.

The estimation of the proposed model is studied under both low- and high-dimensional

3

scaling. When the sample size T is suï¬ƒciently large and the transition tensor A is exactly

low-rank, we consider the rank-constrained estimation method and prove the asymptotic

normality for the proposed low-Tucker-rank (LTR) estimator. Under the high-dimensional

setup, we consider a more general and natural setting where A can be well approximated

by a low-Tucker-rank tensor, and develop regularized estimation methods based on nuclear-

norm-type penalties. We ï¬rst study the Sum of Nuclear (SN) norm regularizer, deï¬ned as

the sum of nuclear norms of all one-mode matricizations of A, and derive the non-asymptotic

estimation error bound for the corresponding estimator. The SN norm regularizer has been

widely used in the literature for various low-rank tensor problems (Gandy et al., 2011;

Tomioka et al., 2011; Liu et al., 2013; Raskutti et al., 2019). Its major strength lies in the

fact that the summation of nuclear norms allows enforcing the low-rankness simultaneously

across all modes of the tensor. In contrast, if only a single nuclear norm is used, the low-

rankness of only one mode will be accounted for, obviously leading to a much less eï¬ƒcient

estimator.

However, although penalizing multiple one-mode matricizations of A simultaneously is

far more eï¬ƒcient than penalizing only one of them, the SN regularized estimator suï¬€ers

from serious eï¬ƒciency loss due to the fat-and-short shape of the one-mode matricizations.

Note that the low-rankness in fact can also be encouraged by penalizing nuclear norms

of multi-mode matricizations. For instance, the conventional Matrix Nuclear (MN) norm

regularized estimator (Negahban and Wainwright, 2011) simply penalizes the nuclear norm

of the transition matrix A in representation (1), which under the proposed LRTAR model

actually is a square-shaped multi-mode matricization, namely square matricization, of the

transition tensor A. As we will show in Theorem 3 and simulations, even though the MN

regularizer incorporates only one single square matricization, it still clearly beats the SN

regularizer, since the former avoids the eï¬ƒciency bottleneck caused by the imbalance of the

one-mode matricization.

Indeed, due to the autoregressive form of the proposed model, the transition tensor A

has a special balanced structure in the sense that its ï¬rst d modes have exactly the same

4

dimensions as its other d modes. As a result, actually many diï¬€erent square matricizations

of A can be formed by appropriately pairing up its modes; see Section 5.2 for details. This

naturally motivates us to propose a new regularizer that combines the strengths of both

SN and MN norms. Speciï¬cally, for the proposed tensor autoregression, we introduce a

novel Sum of Square-matrix Nuclear (SSN) norm regularizer, deï¬ned as the sum of nuclear

norms of all the p Ã— p square matricizations of the transition tensor A. The SSN regularizer

is expected to be superior to the MN, since it simultaneously encourages the low-rankness

across all possible square matricizations of A rather than only one of them. Moreover, thanks

to the use of square matricizations, the SSN regularized estimator is provably more eï¬ƒcient

than the SN regularized one; see Theorem 4 and the last simulation experiment in Section

6. Note that the adoption of a more balanced (square) matricization to improve estimation

performance was proposed in Mu et al. (2014) for low-rank tensor completion problems,

where only a single square matricization was penalized, similarly to the MN regularizer.

This work is also related to the literature of matrix-variate regression and tensor regres-

sion for independent data. The matrix-variate regression model in Ding and Cook (2018) has

the same basic bilinear form as that of the MAR model mentioned earlier, while an envelope

method was introduced to further reduce the dimension. Raskutti et al. (2019) proposed

a multi-response tensor regression model, where they mainly studied the third-order coef-

ï¬cient tensor and the SN regularization which is known to be statistically sub-optimal for

higher-order tensor estimation.

In contrast, we study the model for general higher-order

tensor-valued time series. Moreover, our SSN regularized estimator has a much faster sta-

tistical convergence rate than the SN estimator. Recently, Chen et al. (2019) and Han

et al. (2020) studied non-convex projected gradient descent methods for tensor regression.

While their non-convex approaches require exact low-rankness with known Tucker ranks,

our methods can handle both exact and approximate low-rankness and select the unknown

ranks consistently in the exactly low-rank case. In addition, existing literature on tensor

regression has only considered independent data and Gaussian time series data, whereas we

allow sub-Gaussianity of the time series. This is a non-trivial relaxation, since unlike the

5

Gaussian case, sub-Gaussian time series cannot be linearly transformed into independent

samples.

We summarize the most important contributions of this paper as follows:

(i) This paper provides the ï¬rst practical tool to model and forecast general structured,

high-dimensional data with complex temporal dependence via tensor autoregression.

By ï¬‚exibly and eï¬ƒciently capturing the underlying low-dimensional tensor dynamics,

the proposed model delivers signiï¬cant dimension reduction, meaningful structural

interpretation and favorable forecast performance.

(ii) By exploiting the special balanced structure of the transition tensor A, a novel SSN

regularization approach is introduced to simultaneously and eï¬ƒciently encourage low-

rankness across all square matricizations of A, outperforming both the SN and MN

methods under both exact and approximate low-rankness. For exactly low-rank A, a

truncated estimator is further introduced for consistent rank selection.

(iii) On the technical front, by establishing a novel martingale-based concentration bound,

this paper relaxes the conventional Gaussian assumption in the literature on high-

dimensional time series to sub-Gaussianity. This technique is generally applicable to

the non-asymptotic estimation theory for high-dimensional time series models with a

VAR representation and hence is of independent interest.

The rest of the paper is organized as follows. Section 2 introduces basic notation and

tensor algebra. Section 3 presents the proposed LRTAR model. Section 4 studies the low-

dimensional least squares estimator and its asymptotic properties. The high-dimensional

regularized estimation is covered in Section 5, where we develop the non-asymptotic theory

for three regularized estimators and rank selection consistency for the truncated estimator.

Sections 6 and 7 present simulation studies and real data analysis, respectively. Section 8

concludes with a brief discussion. All technical proofs are relegated to the Appendix.

6

2 Preliminaries: Notation and Tensor Algebra

Tensors, also known as multidimensional arrays, are natural higher-order extensions of ma-

trices. The order of a tensor is known as the dimension, way or mode, so a multidimensional

array X âˆˆ Rp1Ã—Â·Â·Â·Ã—pd is called a d-th-order tensor. We refer readers to Kolda and Bader

(2009) for a detailed review of basic tensor algebra.

Throughout this paper, we denote vectors by boldface small letters, e.g. x, y, matrices

by boldface capital letters, e.g. X, Y , and tensors by boldface Euler capital letters, e.g. X,

Y. For any two real-valued sequences xk and yk, we write xk (cid:38) yk if there exists a constant

c > 0 such that xk â‰¥ cyk for all k, and write xk (cid:29) yk if limkâ†’âˆ yk/xk = 0. In addition, write

xk (cid:16) yk if xk (cid:38) yk and yk (cid:38) xk. We use C to denote a generic positive constant, which is

independent of the dimensions and the sample size.

For a generic matrix X, we let X (cid:62), (cid:107)X(cid:107)F, (cid:107)X(cid:107)op, (cid:107)X(cid:107)âˆ—, vec(X), and Ïƒj(X) denote

its transpose, Frobenius norm, operator norm, nuclear norm, vectorization, and j-th largest

singular value, respectively. For any matrix X âˆˆ RmÃ—n, recall that the nuclear norm and its

dual norm, the operator norm, are deï¬ned as

(cid:107)X(cid:107)âˆ— =

min(m,n)
(cid:88)

j=1

Ïƒj(X) and (cid:107)X(cid:107)op = Ïƒ1(X).

For any square matrix X, we let Î»min(X) and Î»max(X) denote its minimum and maximum

eigenvalues. For any real symmetric matrices X and Y , we write X â‰¤ Y if Y âˆ’ X is a

positive semideï¬nite matrix.

Matricization, also known as unfolding, is the process of reordering the elements of a

third- or higher-order tensor into a matrix. The most commonly used matricization is the

one-mode matricization deï¬ned as follows. For any d-th-order tensor X âˆˆ Rp1Ã—Â·Â·Â·Ã—pd, its
mode-s matricization X(s) âˆˆ RpsÃ—pâˆ’s, with pâˆ’s = (cid:81)d

i=1,i(cid:54)=s pi, is the matrix obtained by

setting the s-th tensor mode as its rows and collapsing all the others into its columns, for

s = 1, . . . , d. Speciï¬cally, the (i1, . . . , id)-th element of X is mapped to the (is, j)-th element

7

of X(s), where

j = 1 +

d
(cid:88)

k=1
k(cid:54)=s

(ik âˆ’ 1)Jk with Jk =

p(cid:96).

kâˆ’1
(cid:89)

(cid:96)=1
(cid:96)(cid:54)=s

The above one-mode matricization can be extended to the multi-mode matricization by

combining multiple modes to rows and combining the rest to columns of a matrix. For any
iâˆˆS pi-by-(cid:81)
index subset S âŠ‚ {1, . . . , d}, the multi-mode matricization X[S] is the (cid:81)
matrix whose (i, j)-th element is mapped from the (i1, . . . , id)-th element of X, where

i /âˆˆS pi

i = 1 +

(cid:88)

kâˆˆS

(ik âˆ’ 1)Ik and j = 1 +

(cid:88)

(ik âˆ’ 1)Jk, with Ik =

k /âˆˆS

p(cid:96) and Jk =

(cid:89)

(cid:96)âˆˆS
(cid:96)<k

p(cid:96).

(cid:89)

(cid:96) /âˆˆS
(cid:96)<k

Note that the modes in the multi-mode matricization are collapsed following their original
[S(cid:123)], where S(cid:123) = {1, . . . , d} \ S is the complement

order 1, . . . , d. Moreover, it holds X[S] = X(cid:62)

of S. In addition, the one-mode matricization X(s) deï¬ned above is simply X[{s}].

We next review the concepts of tensor-matrix multiplication, tensor generalized inner

product and norm. For any d-th-order tensor X âˆˆ Rp1Ã—Â·Â·Â·Ã—pd and matrix Y âˆˆ RqkÃ—pk

with 1 â‰¤ k â‰¤ d, the mode-k multiplication X Ã—k Y produces a d-th-order tensor in

Rp1Ã—Â·Â·Â·Ã—pkâˆ’1Ã—qkÃ—pk+1Ã—Â·Â·Â·Ã—pd deï¬ned by

(X Ã—k Y )i1Â·Â·Â·ikâˆ’1jik+1...id

=

pk(cid:88)

ik=1

Xi1Â·Â·Â·idY jik.

For any two tensors X âˆˆ Rp1Ã—p2Ã—Â·Â·Â·Ã—pn and Y âˆˆ Rp1Ã—p2Ã—Â·Â·Â·pm with n â‰¥ m, their generalized

inner product (cid:104)X, Y(cid:105) is the (n âˆ’ m)-th-order tensor in Rpm+1Ã—Â·Â·Â·Ã—pn deï¬ned by
p2
(cid:88)

pm
(cid:88)

p1
(cid:88)

(cid:104)X, Y(cid:105)im+1...in =

Â· Â· Â·

Xi1i2...imim+1...in

Yi1i2...im,

(2)

i1=1

i2=1

im=1

where 1 â‰¤ im+1 â‰¤ pm+1, . . . , 1 â‰¤ in â‰¤ pn. In particular, when n = m, it reduces to the

conventional real-valued inner product. In addition, the Frobenius norm of any tensor X is
deï¬ned as (cid:107)X(cid:107)F = (cid:112)(cid:104)X, X(cid:105).

Some basic properties of the tensor generalized inner product are as follows. Let X âˆˆ

Rp1Ã—p2Ã—Â·Â·Â·Ã—pn, Y âˆˆ Rp1Ã—p2Ã—Â·Â·Â·pm and Z âˆˆ Rp1Ã—Â·Â·Â·Ã—pkâˆ’1Ã—qkÃ—pk+1Â·Â·Â·Ã—pm be tensors with n â‰¥ m â‰¥

k â‰¥ 1. If Y âˆˆ RqkÃ—pk, then

(cid:104)X Ã—k Y , Z(cid:105) = (cid:104)X, Z Ã—k Y (cid:62)(cid:105).

(3)

8

If Z âˆˆ Rqm+j Ã—pm+j with 1 â‰¤ j â‰¤ n âˆ’ m, then

Moreover,

(cid:104)X, Y(cid:105) Ã—j Z = (cid:104)X Ã—m+j Z, Y(cid:105).

vec((cid:104)X, Y(cid:105)) = X[S]vec(Y),

(4)

(5)

where S = {m + 1, . . . , n}, and when m = n, X[âˆ…] = vec(X)(cid:62).

Finally, we summarize some concepts and useful results of the Tucker decomposition

(Tucker, 1966; De Lathauwer et al., 2000). For any tensor X âˆˆ Rp1Ã—Â·Â·Â·Ã—pd, its multilinear

ranks (r1, . . . , rd) are deï¬ned as the matrix ranks of its one-mode matricizations, namely

ri = rank(X(i)), for i = 1, . . . , d. Note that riâ€™s are analogous to the row and column ranks

of a matrix, but are not necessarily equal for third- and higher-order tensors. Suppose that

X has multilinear ranks (r1, . . . , rd). Then X has the following Tucker decomposition:

X = Y Ã—1 Y 1 Ã—2 Y 2 Â· Â· Â· Ã—d Y d = Y Ã—d

i=1 Y i,

(6)

where Y i âˆˆ RpiÃ—ri for i = 1, . . . , d are the factor matrices and Y âˆˆ Rr1Ã—Â·Â·Â·Ã—rd is the core

tensor. Hence, the multilinear ranks are also called Tucker ranks.

If X has the Tucker decomposition in (6), then we have the following results for its one-

and multi-mode matricizations:

X(s) = Y sY(s)(Y d âŠ— Â· Â· Â· âŠ— Y s+1 âŠ— Y sâˆ’1 Â· Â· Â· âŠ— Y 1)(cid:62) = Y sY(s)(âŠ—i(cid:54)=sY i)(cid:62),

s = 1, . . . , d,

and

X[S] = (âŠ—iâˆˆSY i)Y[S](âŠ—i /âˆˆSY i)(cid:62), S âŠ‚ {1, . . . , d},

(7)

where âŠ—i(cid:54)=s, âŠ—iâˆˆS and âŠ—i /âˆˆS are matrix Kronecker products operating in the reverse order

within the corresponding index sets.

Note that for any nonsingular matrices Oi âˆˆ RriÃ—r1 for i = 1, . . . , d, it holds

Y Ã—d

i=1 Y i = (Y Ã—d

i=1 Oi) Ã—d

i=1 (Y iOâˆ’1

i ).

This indicates that the Tucker decomposition in (6) is not unique unless appropriate iden-

tiï¬ability constraints are imposed.

In this paper, to ï¬x ideas, we will focus on a special

9

Tucker decomposition called the higher-order singular value decomposition (HOSVD). In

the HOSVD, the factor matrix Y i in (6) is deï¬ned as the tall orthonormal matrix con-

sisting of the top ri left singular vectors of X(i), for i = 1, . . . , d. Consequently, the core

tensor Y = X Ã—d

i=1 Y (cid:62)

i has the all-orthogonal property that Y(i)Y(cid:62)

(i) is a diagonal matrix for

i = 1, . . . , d.

3 Low-Rank Tensor Autoregression

For the tensor-valued time series {Yt}T

t=1, we propose the following Low-Rank Tensor Au-

toregressive (LRTAR) model:

Yt = (cid:104)A, Ytâˆ’1(cid:105) + Et,

(8)

where A âˆˆ Rp1Ã—Â·Â·Â·Ã—pdÃ—p1Ã—Â·Â·Â·Ã—pd is the 2d-th-order transition tensor which is assumed to have

multilinear ranks (r1, . . . , r2d), with ri = rank(A(i)), (cid:104)Â·, Â·(cid:105) is the generalized tensor inner

product deï¬ned in (2), and Et âˆˆ Rp1Ã—Â·Â·Â·Ã—pd is the mean-zero random error at time t with

possible dependencies among its contemporaneous elements. Denote S1 = {1, 2, . . . , d} and

S2 = {d + 1, d + 2, . . . , 2d}. Note that by (5), model (8) can be written into the VAR form
in (1) with transition matrix A = A[S2].

Then, we have the higher-order singular value decomposition (HOSVD) of A,

A = G Ã—2d

i=1 U i,

(9)

where G âˆˆ Rr1Ã—Â·Â·Â·Ã—r2d is the core tensor, and each U i âˆˆ RpiÃ—ri is the orthonormal factor

matrix deï¬ned as the top ri left singular vectors of A(i), for 1 â‰¤ i â‰¤ 2d. Thus, by (7), the

VAR representation of model (8) can be written as

vec(Yt)
(cid:124) (cid:123)(cid:122) (cid:125)yt

= (âŠ—iâˆˆS2U i)G[S2](âŠ—iâˆˆS1U i)(cid:62)
(cid:123)(cid:122)
(cid:125)
A[S2]

(cid:124)

vec(Ytâˆ’1)
(cid:125)
(cid:123)(cid:122)
(cid:124)
ytâˆ’1

+ vec(Et)
(cid:124) (cid:123)(cid:122) (cid:125)et

,

(10)

where yt = vec(Yt) and et = vec(Et).

In contrast to the conventional VAR model in (1) which has p2 unknown parameters,

where p = (cid:81)d

i=1 pi, the dimension of the parameter space for model (8) is reduced substan-

10

tially to

2d
(cid:89)

i=1

ri +

d
(cid:88)

i=1

ri(pi âˆ’ ri) +

d
(cid:88)

i=1

rd+i(pi âˆ’ rd+i),

(11)

which is computed by subtracting the number of constraints due to the orthonormality of

U iâ€™s and the all-orthogonal property of G from the total number of parameters.

By the VAR representation in (10), we immediately have the necessary and suï¬ƒcient

condition for the existence of a unique strictly stationary solution to model (8) as follows.

Assumption 1. The spectral radius of A[S2] is strictly less than one.

The proposed model implies an interesting low-dimensional tensor dynamical structure.

To be speciï¬c, by (3), (4) and the orthonormality of U i, it can be shown that (8) together

with (9) implies

Yt Ã—d
i=1 U (cid:62)
d+i
(cid:124)
(cid:125)
(cid:123)(cid:122)
rd+1Ã—rd+2Ã—Â·Â·Â·Ã—r2d

= (cid:10)G, Ytâˆ’1 Ã—d
i=1 U (cid:62)
i
(cid:125)
(cid:123)(cid:122)
(cid:124)
r1Ã—r2Ã—Â·Â·Â·Ã—rd

(cid:11) + Et Ã—d

i=1 U (cid:62)

d+i.

(12)

Note that in (12), Yt and Et are both projected onto a low-dimensional space via the U d+iâ€™s,

while Ytâˆ’1 is projected onto another low-dimensional space via the U iâ€™s, with 1 â‰¤ i â‰¤ d.

Hence, (12) is essentially a low-dimensional tensor regression deï¬ned on the projections of

Yt and Ytâˆ’1. Element-wisely, the low-dimensional tensor Yt Ã—d
(cid:81)d

i=1 rd+i multilinear response factors, Ytâˆ’1 Ã—d

i=1 U (cid:62)

i as (cid:81)d
d+i as multilinear error factors. For this reason, we call U d+1, . . . , U 2d the

i=1 ri multilinear predictor factors,

i=1 U (cid:62)

d+i can be interpreted as

and Et Ã—d

i=1 U (cid:62)

response factor matrices, and U 1, . . . , U d the predictor factor matrices.

We discuss some special cases of the proposed model below.

Example 1. For simplicity, we ï¬rst consider the case with d = 2, so Yt â‰¡ Y t, Et â‰¡ Et âˆˆ

Rp1Ã—p2 are matrices. Then the VAR representation in (10) becomes

vec(Y t) = (U 4 âŠ— U 3)G[{3,4}](U (cid:62)

2 âŠ— U (cid:62)

1 )vec(Y tâˆ’1) + vec(Et),

(13)

and the low-dimensional representation in (12) becomes

U (cid:62)

3 Y tU 4 = (cid:10)G, U (cid:62)

1 Y tâˆ’1U 2

(cid:11) + U (cid:62)

3 EtU 4,

11

where G âˆˆ Rr1Ã—Â·Â·Â·Ã—r4. It is interesting to compare this model with the matrix autoregressive

(MAR) model in Chen et al. (2020b) and Hoï¬€ (2015), which is deï¬ned by

Y t = B1Y tâˆ’1B(cid:62)

2 + Et,

where B1 âˆˆ Rp1Ã—p1 and B2 âˆˆ Rp2Ã—p2, whose vector form is

vec(Y t) = (B2 âŠ— B1)vec(Y tâˆ’1) + vec(Et).

(14)

(15)

It can be easily seen that if r1 = r3 = p1, r2 = r4 = p2, U 3 = I p1, U 4 = I p2, and
G[{3,4}] = (B2 âŠ— B1)(U 2 âŠ— U 1), then (13) becomes exactly (15). Thus, the MAR model in

(14) can be viewed as a special case of the proposed model without reducing dimensions piâ€™s to

riâ€™s and without transforming Y t; see Figure 1 for an illustration. The above comparison also

applies to the general case with d â‰¥ 3. The tensor version of the MAR model is considered

in Hoï¬€ (2015) and is deï¬ned as

Yt = Ytâˆ’1 Ã—d

i=1 Bi + Et,

(16)

where Bi âˆˆ RpiÃ—pi for i = 1, . . . , d. We call (16) the multilinear tensor autoregressive

(MTAR) model. Note that its vector form is

vec(Yt) = (Bd âŠ— Â· Â· Â· âŠ— B1)vec(Ytâˆ’1) + vec(Et).

(17)

Similarly, (17) is a special case of (10) with ri = rd+i = pi, U d+i = I pi, for i = 1, . . . , d, and
G[S2] = (âŠ—iâˆˆS1Bi)(âŠ—iâˆˆS1U i). Obviously, the number of unknown parameters in the MTAR
model, (cid:80)d

i , is much larger than that of the proposed model as shown in (11). Also note

i=1 p2

that Chen et al. (2020b) focuses on the low-dimensional estimation and its asymptotic theory,

while Hoï¬€ (2015) considers a Bayesian estimation method.

Example 2. In the special case where U d+i = U i and rd+i = ri for i = 1, . . . , d, the

proposed model may be understood from the perspective of dynamic factor modeling (Stock

and Watson, 2011; Bai and Wang, 2016) for tensor-valued time series. Speciï¬cally, consider

the following model:

Yt = Ft Ã—d

i=1 U i, Ft = (cid:104)G, Ftâˆ’1(cid:105) + Ht,

(18)

12

Figure 1: Illustration of the MAR model and the proposed LRTAR model in the case of

d = 2.

where Yt âˆˆ Rp1Ã—Â·Â·Â·Ã—pd is the observed tensor-valued time series, Ft âˆˆ Rr1Ã—Â·Â·Â·Ã—rd represents
(cid:81)d
i=1 ri factors, and U i âˆˆ RpiÃ—ri are orthonormal matrices for i = 1, . . . , d. Here Ft follows
the tensor autoregression (TAR) with transition tensor G âˆˆ Rr1Ã—Â·Â·Â·Ã—rdÃ—r1Ã—Â·Â·Â·Ã—rd and random

error Ht. Note that (18) can be rewritten as

Yt = (cid:10)G Ã—d

i=1 U i Ã—2d

i=d+1 U i, Ytâˆ’1

(cid:11) + Ht Ã—d

i=1 U i.

Thus, model (18) is a special case of the proposed model with U d+i = U i and rd+i = ri

for i = 1, . . . , d, and Et = Ht Ã—d

model in the form of Yt = Ft Ã—d

i=1 U i. Chen et al. (2020c) introduces the tensor factor
i=1 U i + Et without an explicit modeling of the latent factors
Ft. Hence, model (18) may be regarded as a special tensor factor model with autoregressive

dynamic factors, but without any random error in the model equation of Yt.

4 Low-Dimensional Least Squares Estimation

We consider the parameter estimation for the low-dimensional case where the sample size T

is suï¬ƒciently large such that the dimension of the parameter space can be assumed ï¬xed.

Throughout this section, we assume that the data are generated from the proposed model

in Section 3, where the true transition tensor A is exactly low-Tucker rank with multilinear

ranks (r1, . . . , r2d). This assumption will be relaxed under the high-dimensional setup in the

13

<ğ“–ğ“–,>ğ‘ğ‘1ğ‘ğ‘2ğ‘ğ‘2ğ‘ğ‘1ğ‘Ÿğ‘Ÿ3ğ‘ğ‘1ğ‘ğ‘2ğ‘ğ‘1ğ‘Ÿğ‘Ÿ4ğ‘ğ‘1ğ‘ğ‘2ğ‘Ÿğ‘Ÿ2ğ‘ğ‘2ğ‘ğ‘1ğ‘ğ‘2ğ‘ğ‘1ğ‘Ÿğ‘Ÿ4ğ‘¼ğ‘¼3â€²ğ’€ğ’€ğ‘¡ğ‘¡ğ‘¼ğ‘¼4=ğ’€ğ’€ğ‘¡ğ‘¡=ğ‘©ğ‘©1ğ’€ğ’€ğ‘¡ğ‘¡âˆ’1ğ‘©ğ‘©2â€²+ğ‘¬ğ‘¬ğ‘¡ğ‘¡ğ‘ğ‘1ğ‘ğ‘2ğ‘Ÿğ‘Ÿ1ğ‘Ÿğ‘Ÿ3ğ‘ğ‘2ğ‘ğ‘2ğ‘ğ‘1ğ‘ğ‘2ğ‘ğ‘1ğ‘¼ğ‘¼3â€²ğ‘¼ğ‘¼1â€²ğ’€ğ’€ğ‘¡ğ‘¡âˆ’1ğ‘¼ğ‘¼2+ğ‘¬ğ‘¬ğ‘¡ğ‘¡ğ‘¼ğ‘¼4MAR model:LRTAR model:next section.

Suppose that the true ranks (r1, . . . , r2d) of the exactly low-rank tensor A are known; we

relegate the rank selection to Section 5.3. Then the parameters can be estimated by

(cid:98)ALTR = (cid:98)G Ã—2d

i=1 (cid:98)U i = arg min

G,U i

T
(cid:88)

t=1

(cid:13)
(cid:13)Yt âˆ’ (cid:104)G Ã—2d

i=1 U i, Ytâˆ’1(cid:105)(cid:13)
2
F .
(cid:13)

(19)

We call (cid:98)ALTR the low-Tucker-rank (LTR) estimator. Note that the minimization in (19)
is unconstrained, so the Tucker decomposition of (cid:98)ALTR is not unique.
more than one solution of (cid:98)G and (cid:98)U iâ€™s corresponding to the same (cid:98)ALTR. Due to the lack of

Indeed, there are

identiï¬ability of the Tucker decompositions, standard asymptotics of the maximum likelihood

estimation cannot apply directly. Nevertheless, we can still derive the asymptotic distribution

of (cid:98)ALTR using the asymptotic theory for overparameterized models in Shapiro (1986).

Recall that S1 = {1, 2, . . . , d} S2 = {d + 1, d + 2, . . . , 2d}, yt = vec(Yt), and et = vec(Et).
Let Î¸ = (vec(G[S2])(cid:62), vec(U 1)(cid:62), Â· Â· Â· , vec(U 2d)(cid:62))(cid:62) be the parameter vector, and let h =
h(Î¸) = vec(A[S2]) = vec((âŠ—iâˆˆS2U i)G[S2](âŠ—iâˆˆS1U i)(cid:62)) be the vectorization of the transition
matrix. Denote Î£y = var(yt), Î£e = var(et), and J = Î£âˆ’1
1, . . . , 2d, let P (i)

[S2] be the p2 Ã— p2 permutation matrix such that vec(A[S2]) = P (i)

In addition, for i =
[S2]vec(A(i)).

e âŠ— Î£y.

Then, it can be shown that the Jacobian matrix H := âˆ‚h(Î¸)/âˆ‚Î¸ is given by

H = (cid:0)(âŠ—iâˆˆS1U i) âŠ— (âŠ—iâˆˆS2U i), P (1)

[S2]

(cid:8)(cid:2)(âŠ—2d

i=1,i(cid:54)=1U i)G(cid:62)

(1)

(cid:3) âŠ— I p1

(cid:9) ,

P (2)
[S2]

(cid:8)(cid:2)(âŠ—2d

i=1,i(cid:54)=2U i)G(cid:62)

(2)

(cid:3) âŠ— I p2

(cid:9) , . . . , P (2d)
[S2]

(cid:8)(cid:2)(âŠ—2d

i=1,i(cid:54)=2dU i)G(cid:62)

(2d)

(20)

(cid:3) âŠ— I p2d

(cid:9) (cid:1),

where pd+i = pi for i = 1, . . . , d.

Theorem 1. Suppose that the time series {Yt} is generated by model (8) with E(cid:107)et(cid:107)4

2 < âˆ,

and Assumption 1 holds. Then,

âˆš

T vec(( (cid:98)ALTR âˆ’ A)[S2]) â†’ N (0, Î£LTR),

in distribution as T â†’ âˆ, where Î£LTR = H(H (cid:62)J H)â€ H (cid:62), and â€  is the Moore-Penrose

inverse.

Since the asymptotic theory for overparameterized models in Shapiro (1986) allows for

unidentiï¬ability of the components G and U iâ€™s in the decomposition of A, Theorem 1 does

14

not require that the Tucker decomposition of A is unique; see the proof of Theorem 1 in

Appendix A for more details.

Next we compare the result in Theorem 1 to those of two other consistent estimators for

the proposed model in the low-dimensional setup. Note that the rank of A[S2] in (10) is at
most s1 := min((cid:81)d
i=d+1 ri). Thus, A[S2] can be estimated by both the reduced-rank

i=1 ri, (cid:81)2d

regression (RRR) and ordinary least squares (OLS) methods,

(cid:98)ARRR = arg min

rank(B[S2])â‰¤s1

1
T

T
(cid:88)

t=1

(cid:107)Yt âˆ’ (cid:104)B, Ytâˆ’1(cid:105)(cid:107)2
F

(21)

and

(cid:98)AOLS = arg min

B

1
T

T
(cid:88)

(cid:107)Yt âˆ’ (cid:104)B, Ytâˆ’1(cid:105)(cid:107)2
F.

t=1
Naturally, under model (8), (cid:98)ALTR is asymptotically more eï¬ƒcient than (cid:98)ARRR and (cid:98)AOLS:

Corollary 1. If the conditions of Theorem 1 hold, then

T vec(( (cid:98)AOLSâˆ’A)[S2]) â†’ N (0, Î£OLS)
T vec(( (cid:98)ARRR âˆ’ A)[S2]) â†’ N (0, Î£RRR) in distribution as T â†’ âˆ. Moreover, it holds

and

âˆš

âˆš

that Î£LTR â‰¤ Î£RRR â‰¤ Î£OLS.

To solve the minimization problem in (19), we propose an alternating least squares (ALS)

method. Speciï¬cally, by the vector representation of the proposed model in (10), the loss

function in (19) can be rewritten as

L(G, U 1, . . . , U 2d) =

T
(cid:88)

t=1

(cid:13)
(cid:13)yt âˆ’ (âŠ—iâˆˆS2U i)G[S2](âŠ—iâˆˆS1U i)(cid:62)ytâˆ’1
(cid:13)

(cid:13)
2
(cid:13)
(cid:13)
2

=

T
(cid:88)

t=1

(cid:13)
(cid:13)
2
(cid:13)
(cid:13)
(cid:13)yt âˆ’ m(Î¸)
(cid:13)
2

,

where m(Î¸) = (âŠ—iâˆˆS2U i)G[S2](âŠ—iâˆˆS1U i)(cid:62)ytâˆ’1 is the conditional mean of yt = vec(Yt) and
2d)(cid:62), with Î¸0 = vec(G[S2]) and
is a function of the parameter vector Î¸ = (Î¸(cid:62)

1 , . . . , Î¸(cid:62)

0 , Î¸(cid:62)

Î¸i = vec(U i) for i = 1, . . . , 2d. Note that m(Î¸) is linear in each Î¸i while keeping the other

components Î¸j with 0 â‰¤ j (cid:54)= i â‰¤ 2d ï¬xed. This is indeed because

m(Î¸) = (âŠ—iâˆˆS2U i)G[S2]P k,1

(cid:8)(cid:2)(âŠ—iâˆˆS1,i(cid:54)=kU i)(cid:62)(Ytâˆ’1)(cid:62)

(cid:9) P k,3vec(U k)

= P k,2
= (cid:8)(cid:2)y(cid:62)

(cid:8)(cid:2)(âŠ—iâˆˆS2,i(cid:54)=d+kU i)(Mtâˆ’1)(cid:62)
tâˆ’1(âŠ—iâˆˆS1U i)(cid:3) âŠ— (âŠ—iâˆˆS2U i)(cid:9) vec(G[S2]),

(cid:3) âŠ— I pk

(k)

(k)

(cid:3) âŠ— I rk
(cid:9) vec(U d+k)

15

Algorithm 1 ALS algorithm for LTR estimator
Initialize: A(0) = (cid:98)ARRR or (cid:98)AOLS
HOSVD: A(0) â‰ˆ G(0) Ã—2d

i=1 U (0)

i

, with multilinear ranks (r1, . . . , r2d).

repeat s = 0, 1, 2, . . .

for k = 1, . . . , d

U (s+1)

k â† arg min

U k

end for

for k = 1, . . . , d

(cid:80)T

t=1

i )G[S2]P k,1

(cid:13)
(cid:13)yt âˆ’ (âŠ—iâˆˆS2U (s)
(cid:13)
(cid:13)
2
(cid:13)
(cid:13)
2

vec(U k)

(cid:110)(cid:104)

(âŠ—iâˆˆS1,i(cid:54)=kU (s)

i )(cid:62)(Ytâˆ’1)(cid:62)

(k)

(cid:105)

(cid:111)

âŠ— I rk

U (s+1)

d+k â† arg min

U d+k

(cid:80)T

t=1

(cid:13)
(cid:13)
(cid:13)yt âˆ’ P k,2

(cid:8) (cid:104)

(âŠ—iâˆˆS2,i(cid:54)=d+kU (s)

i )(M(s+1)

tâˆ’1 )(cid:62)
(k)

(cid:105)

âŠ— I pk

(cid:9)vec(U d+k)

(cid:13)
2
(cid:13)
(cid:13)
2

end for

G(s+1) â† arg min

G

A(s+1) â† G(s+1) Ã—2d

(cid:80)T

t=1

(cid:13)
(cid:13)
(cid:13)yt âˆ’
i=1 U (s+1)

i

(cid:110)(cid:104)

tâˆ’1(âŠ—iâˆˆS1U (s+1)
y(cid:62)

i

(cid:105)
)

âŠ— (âŠ—iâˆˆS2U (s+1)

i

(cid:111)
)

vec(G[S2])

(cid:13)
2
(cid:13)
(cid:13)
2

until convergence

i=1 riÃ—(cid:81)d

i=1 ri, P k,2 âˆˆ RpÃ—p and P k,3 âˆˆ RrkpkÃ—rkpk are

for k = 1, . . . , d, where P k,1 âˆˆ R(cid:81)d
permutation matrices deï¬ned such that P k,1vec(T(k)) = vec(T) for any T âˆˆ Rr1Ã—Â·Â·Â·Ã—rd,
k ), and Mtâˆ’1 âˆˆ
P k,2vec(T(k)) = vec(T) for any T âˆˆ Rp1Ã—Â·Â·Â·Ã—pd, and P k,3vec(U k) = vec(U (cid:62)
Rrd+1Ã—Â·Â·Â·Ã—r2d is deï¬ned such that vec(Mt) = G[S2](âŠ—iâˆˆS1U i)(cid:62)ytâˆ’1. Therefore, we can update
each component parameter vector Î¸i, and hence the corresponding G and U iâ€™s, iteratively

by the least squares method. The resulting ALS algorithm is shown in Algorithm 1.

As mentioned, the minimization in (19) is unconstrained. Accordingly, to obtain (cid:98)ALTR,
no orthogonality constraint of G and U iâ€™s is needed in Algorithm 1. Instead, we compute the
ï¬nal unique estimates of (cid:98)G and (cid:98)U iâ€™s by the HOSVD operation based on the unconstrained
solution (cid:98)ALTR obtained from Algorithm 1. Speciï¬cally, we compute each (cid:98)U i uniquely as
the top ri left singular vectors of ( (cid:98)ALTR)(i) such that the ï¬rst element in each column of
(cid:98)U i is positive, and set (cid:98)G = [[ (cid:98)ALTR; (cid:98)U

(cid:62)
2d]]. Similar alternating algorithms without

(cid:62)
1 , Â· Â· Â· , (cid:98)U

imposing identiï¬cation constraints can be found in the literature of tensor decomposition;

16

see, e.g. Zhou et al. (2013) and Li et al. (2018).

5 High-Dimensional Regularized Estimation

5.1 Regularization via One-Mode Matricization

The methods in the previous section rely on the assumptions that the dimension is ï¬xed and

that the true transition tensor is exactly low-rank with known Tucker ranks. We next relax

both assumptions. Speciï¬cally, under the high-dimensional setup, we consider regularized

estimation of model (8) via diï¬€erent nuclear-norm-type penalties and develop the corre-

sponding non-asymptotic theory under only an approximately low-Tucker-rank assumption

on the underlying true transition tensor.

In model (8), the exactly low-rank transition tensor A âˆˆ Rp1Ã—Â·Â·Â·Ã—p2d is subject to the

constraints ri = rank(A(i)), for i = 1, . . . , 2d. A commonly used convex relaxation of such

multilinear rank constraints is the regularization via the sum of nuclear (SN) norms of all

the one-mode matricizations,

(cid:107)A(cid:107)SN =

2d
(cid:88)

i=1

(cid:107)A(i)(cid:107)âˆ—.

(22)

The SN norm has been widely used in the literature (Gandy et al., 2011; Tomioka et al.,

2011; Liu et al., 2013; Raskutti et al., 2019) to simultaneously encourage the low-rankness

for all modes of a tensor. This leads us to the SN norm regularized estimator

(cid:98)ASN = arg min

A

(cid:40)

1
T

T
(cid:88)

t=1

(cid:107)Yt âˆ’ (cid:104)A, Ytâˆ’1(cid:105)(cid:107)2

F + Î»SN(cid:107)A(cid:107)SN

,

(cid:41)

where Î»SN is the tuning parameter. Note that if instead of (cid:107)A(cid:107)SN, only one single nuclear

norm, say (cid:107)A(1)(cid:107)âˆ—, is penalized, then the resulting estimator will only enforce the low-

rankness for the ï¬rst mode of A, while failing to do so for all the other 2d âˆ’ 1 modes, and

hence will be less eï¬ƒcient than the above SN estimator.

To derive the estimation error bound for (cid:98)ASN, we make the following assumption on the

random error et = vec(Et).

17

Assumption 2. Let et = Î£1/2
e Î¾t, where {Î¾t} is a sequence of i.i.d. random vectors, with
E(Î¾t) = 0 and var(Î¾t) = I p, and Î£e = var(et) is a positive deï¬nite matrix. In addition, the
entries (Î¾it)1â‰¤iâ‰¤p of Î¾t are mutually independent and Îº2-sub-Gaussian, i.e., E(eÂµÎ¾it) â‰¤ eÎº2Âµ2/2,
for any Âµ âˆˆ R and i = 1, . . . , p.

The sub-Gaussianity condition in Assumption 2 is milder than the commonly used nor-

mality assumption in the literature on high-dimensional stationary vector autoregressive

models (Basu and Michailidis, 2015; Raskutti et al., 2019). This relaxation is made possi-

ble through establishing a novel martingale-based concentration bound in the proof of the

deviation bound; see Lemma B.5 in Appendix B.4. The covariance matrix Î£e captures the

contemporaneous dependency in Et, and the constant Îº controls the tail heaviness of the

marginal distributions.

For any z âˆˆ C, let A(z) = I p âˆ’ A[S2]z be a matrix polynomial, where C is the set of com-

plex numbers. Let Âµmin(A) = min
|z|=1

Î»min(Aâˆ—(z)A(z)) and Âµmax(A) = max
|z|=1

Î»max(Aâˆ—(z)A(z)),

where Aâˆ—(z) is the conjugate transpose of A(z); see Basu and Michailidis (2015) for more

discussions on the connection between the spectral density of the VAR process and the two

quantities. In addition, let

Î±RSC =

Î»min(Î£e)
Âµmax(A)

, M1 =

Î»max(Î£e)
Âµ1/2
min(A)

,

and M2 =

Î»min(Î£e)Âµmax(A)
Î»max(Î£e)Âµmin(A)

.

The exactly low-rankness of the true transition tensor A assumed in Section 4 could

be too stringent in real-world applications.

In what follows, we relax it to the following

approximately low-rank assumption: We assume that all one-mode matricizations of the

underlying true transition tensor A belong to the set of approximately low-rank matrices,

namely A(i) âˆˆ Bq(r(i)

q ; pi, pâˆ’ip) for some q âˆˆ [0, 1), where

Bq(r; d1, d2) :=

ï£±
ï£²

ï£³

M âˆˆ Rd1Ã—d2 :

min(d1,d2)
(cid:88)

i=1

Ïƒi(M )q â‰¤ r

ï£¼
ï£½

ï£¾

,

and pâˆ’i = p/pi = (cid:81)d
j=1,j(cid:54)=i pj. For the convenience of notation, here we require that 00 = 0.
Note that when q = 0, B0(r; d1, d2) is the set of d1-by-d2 rank-r matrices. For q > 0, the

18

restriction on (cid:80)min(d1,d2)

i=1

Ïƒi(M )q â‰¤ r requires that the singular values decay fast, and it is

more general and natural than the exactly low-rank assumption.

Theorem 2. Under Assumptions 1 and 2, if T (cid:38) max1â‰¤iâ‰¤d pâˆ’ip + max(Îº2, Îº4)M âˆ’2
Îº2M1dâˆ’2 (cid:80)d
then with probability at least 1 âˆ’ 2 (cid:80)d

(cid:112)pâˆ’ip/T , and A(i) âˆˆ Bq(r(i)

i=1 exp(âˆ’Cpâˆ’ip) âˆ’ exp[âˆ’C min(Îºâˆ’2, Îºâˆ’4)M 2

q ; pi, pâˆ’ip) for some q âˆˆ [0, 1) and all i = 1, . . . , 2d,

2 p, Î»SN (cid:38)

2 p],

i=1

(cid:107) (cid:98)ASN âˆ’ A(cid:107)F (cid:46) âˆš

rq

(cid:19)1âˆ’q/2

(cid:18)2d Â· Î»SN
Î±RSC

where rq = (2d)âˆ’1 (cid:80)2d

i=1 r(i)
q .

By Theorem 2, when Î»SN (cid:16) Îº2M1dâˆ’2 (cid:80)d
âˆš

(cid:112)pâˆ’ip/T , the estimation error bound scales
rq(max1â‰¤iâ‰¤d pâˆ’ip/T )1/2âˆ’q/4; note that the factor d in the error bounds is canceled by

i=1

as

the dâˆ’2 in the rate of Î»SN. When q = 0, namely A is exactly low-rank with Tucker ranks
), the error bound reduces to (cid:112)r0 max1â‰¤iâ‰¤d pâˆ’ip/T and it is comparable to
(r(1)

0 , . . . , r(2d)

0

that in Tomioka et al. (2011) for i.i.d. tensor regression.

However, recent research (e.g., Mu et al., 2014; Raskutti et al., 2019) has shown that

the SN norm regularization approach is suboptimal, mainly because A(i) is an unbalanced

fat-and-short matricization of a higher-order tensor. Technically, in the proof of Theorem 2,

an essential intermediate step is to establish the deviation bound, where we need to upper

bound the operator norm of a sub-Gaussian random matrix with the same dimensions as

A(i); see Lemma B.5 in Appendix B.3 for details. Undesirably, the order of this operator

norm will be dominated by the larger of the row and column dimensions of the matrix

A(i) âˆˆ RpiÃ—pâˆ’ip, and hence by the column dimension pâˆ’ip, which eventually appears in the

error bound. This indicates that the imbalance of the matricization leads to the eï¬ƒciency

bottleneck of the SN estimator.

On the other hand, similarly to (21), since the the reduced-rank VAR model can be re-

garded as an overparameterization of the proposed LRTAR model, alternatively one may

focus on the approximately low-rank structure of the transition matrix A[S2] in the VAR

representation in (10), and adopt the matrix nuclear (MN) estimator (Negahban and Wain-

19

wright, 2011) to estimate A,

(cid:98)AMN = arg min

A

(cid:40)

1
T

T
(cid:88)

t=1

(cid:107)Yt âˆ’ (cid:104)A, Ytâˆ’1(cid:105)(cid:107)2

F + Î»MN(cid:107)A[S1](cid:107)âˆ—

.

(23)

(cid:41)

Note that the multi-mode matricization A[S2] = A(cid:62)

[S1] is a p Ã— p square matrix. Thus, the

loss of eï¬ƒciency due to the unbalanced matricization can be avoided, which is conï¬rmed by

the following theorem.

Theorem 3. Under Assumptions 1 and 2, if T (cid:38) [1+max(Îº2, Îº4)M âˆ’2
and A[S1] âˆˆ Bq(s(1)
exp[âˆ’C min(Îºâˆ’2, Îºâˆ’4)M 2

2 p],

q ; p, p) for some q = [0, 1), then with probability at least 1 âˆ’ exp(âˆ’Cp) âˆ’

2 ]p, Î»MN (cid:38) Îº2M1

(cid:112)p/T ,

(cid:107) (cid:98)AMN âˆ’ A(cid:107)F (cid:46)

(cid:113)

s(1)
q

(cid:18) Î»MN
Î±RSC

(cid:19)1âˆ’q/2

.

Theorem 3 shows that, with Î»MN (cid:16) Îº2M1

(cid:112)p/T , the estimation error bound for (cid:98)AMN
s(1)
q (p/T )1/2âˆ’q/4. This result is comparable to that in Negahban and Wainwright

(cid:113)

scales as

(2011) for reduced-rank VAR models, yet we relax both the constraint (cid:107)A(cid:107)op < 1 on the

transition matrix A and the normality assumption on the random error in their paper. This

estimation error bound is clearly smaller than that in Theorem 2, as (max1â‰¤iâ‰¤d pâˆ’ip/T )1/2âˆ’q/4
in general can be much larger than (p/T )1/2âˆ’q/4 when s(1)

q (cid:16) rq. Therefore, adopting square

matricization can indeed improve the estimation performance.

The idea of using square matricization to improve eï¬ƒciency was adopted by Mu et al.

(2014) in low-rank tensor completion problems. Their proposed method, called the square

deal, is to ï¬rst unfold a general higher-order tensor into a matrix with similar numbers of

rows and columns, and then use the MN norm as the regularizer. However, for our estimation

problem, despite the advantage of (cid:98)AMN over (cid:98)ASN, Theorem 3 reveals another drawback of
(cid:98)AMN. That is, the error bounds for (cid:98)AMN depend on the (cid:96)q radius s(1)
of A[S1], suggesting that (cid:98)AMN may perform badly when s(1)

q of the singular values

is relatively large.

In other

q

words, unless we have prior knowledge that the (cid:96)q â€œnormâ€ of singular values of particular
matricization A[S1] is truly small, (cid:98)AMN may not be desirable in practice.

20

On the other hand, although the SN regularizer in (22) suï¬€ers from ineï¬ƒciency due

to the imbalance of one-mode matricizations, it has the attractive feature of simultaneously

enforcing the low-rankness across all modes of the tensor A, and thus is more eï¬ƒcient than its

counterpart which considers only one single one-mode matricization, say, (cid:107)A(1)(cid:107)âˆ—. Similarly,

if we can enforce the approximate low-rankness across all possible square matricizations of

A, the estimation performance may be further improved upon (cid:98)AMN. This motivates us to

propose a new regularization approach in the next subsection.

5.2 Regularization via Square Matricization

In this subsection, we propose a novel convex regularizer which improves upon both the

SN and MN regularizers in (22) and (23), respectively, by simultaneously encouraging the

low-rankness across all possible square matricizations of the transition tensor A.

For any 2d-th-order tensor A âˆˆ Rp1Ã—Â·Â·Â·Ã—pdÃ—p1Ã—Â·Â·Â·Ã—pd, its multi-mode matricization A[I] will

be a p Ã— p square matrix, with p = (cid:81)d

i=1 pi, if the index set is chosen as

I = {(cid:96)1, . . . , (cid:96)d},

where each index (cid:96)i

is set to either i or d + i, for i = 1, . . . , d. For instance, A[S1]

is the square matricization formed by setting (cid:96)i = i for all i = 1, . . . , d. Moreover, if

A has multilinear ranks (r1, . . . , r2d), then the rank of the matricization A[I] is at most
min((cid:81)2d

i=1,i /âˆˆI ri). Therefore, if we penalize the sum of nuclear norms of all such

i=1,iâˆˆI ri, (cid:81)2d

squares matricizations, which we call the sum of square-matrix nuclear (SSN) norms for

simplicity, then the resulting estimator would enjoy the eï¬ƒciency gain from both the use of

square matricizations and simultaneous incorporation of many rank constraints.

Obviously, there are 2d possible choices of the index set I that corresponds to a square

matricization A[I]. However, since A[I] = A(cid:62)
[I (cid:123)], when deï¬ning the SSN norm, we only
need to include one of I and its complement I (cid:123). A simple way to do so is to choose only

sets containing the index one. That is, ï¬x (cid:96)1 = 1 and choose (cid:96)i = i or d + i for i =

2, . . . , d. This results in totally 2dâˆ’1 chosen index sets, denoted by I1, I2, . . . , I2dâˆ’1. Note

21

that I1 = S1 = {1, . . . , d}. For example, when d = 3, we have four chosen index sets,

I1 = {1, 2, 3}, I2 = {1, 5, 3}, I3 = {1, 2, 6} and I4 = {1, 5, 6}.

Based on the above choice of the 2dâˆ’1 index sets, we introduce the following SSN norm,

(cid:107)A(cid:107)SSN =

2dâˆ’1
(cid:88)

k=1

(cid:13)
(cid:13)A[Ik]

(cid:13)
(cid:13)âˆ— .

The corresponding estimator is deï¬ned as

(cid:98)ASSN = arg min

A

(cid:40)

1
T

T
(cid:88)

t=1

(cid:107)Yt âˆ’ (cid:104)A, Ytâˆ’1(cid:105)(cid:107)2

F + Î»SSN(cid:107)A(cid:107)SSN

,

(24)

(cid:41)

where Î»SSN is the tuning parameter.

If the rank of one-mode matricizations rank(A(i)) = ri, each square matricization A[Ik]
ri). Similarly, if all A(i)s are

is also low-rank with rank(A[Ik]) â‰¤ min((cid:81)2d
approximately low-rank, the square matricizations are approximately low-rank as well. In

ri, (cid:81)2d

i=1,i /âˆˆIk

i=1,iâˆˆIk

contrast to the SN norm in (22) which directly matches the multilinear ranks rank(A(i)) for

i = 1, . . . , d, the SSN norm encourages the multilinear low-rank structure of A by simul-

taneously enforcing the low-rankness of all the square matricizations A[Ik]. The following
theorem gives the theoretical results for (cid:98)ASSN.

Theorem 4. Under Assumptions 1 and 2, if T (cid:38) [1+max(Îº2, Îº4)M âˆ’2
and A[Ik] âˆˆ B(s(k)
1 âˆ’ exp[âˆ’C(p âˆ’ d)] âˆ’ exp[âˆ’C min(Îºâˆ’2, Îºâˆ’4)M 2

q

; p, p) for some q âˆˆ [0, 1) and all k = 1, . . . , 2dâˆ’1, with probability at least

2 ]p, Î»SSN (cid:38) Îº2M121âˆ’d(cid:112)p/T ,

where sq = 21âˆ’d (cid:80)2dâˆ’1

k=1 s(k)

q

(cid:107) (cid:98)ASSN âˆ’ A(cid:107)F (cid:46) âˆš

.

2 p],
(cid:18) 2dâˆ’1Î»SSN
Î±RSC

sq

(cid:19)1âˆ’q/2

âˆš

By Theorem 4, when Î»SSN (cid:16) Îº2M121âˆ’d(cid:112)p/T , the estimation error bound scales as
sq(p/T )1/2âˆ’q/4 and reduces to (cid:112)s0p/T in the exactly low-rank setting for q = 0. For
a clearer comparison among the three estimators (cid:98)ASN, (cid:98)AMN and (cid:98)ASSN, we summarize the
main results of Theorems 2â€“4 in Table 1. First, both (cid:98)ASSN and (cid:98)AMN have much smaller error
bounds and less stringent sample size requirements than (cid:98)ASN, due to the diverging dimension

pâˆ’i in the results of the latter. This reaï¬ƒrms the advantage of the square matricizations.

22

SN

MN

SSN

Sample size

Estimation error

T (cid:38) (max1â‰¤iâ‰¤d pâˆ’i + M âˆ’2
âˆš

rq(max1â‰¤iâ‰¤d pâˆ’ip/T )1/2âˆ’q/4

2 )p

T (cid:38) (1 + M âˆ’2
(cid:113)

2 )p

q (p/T )1/2âˆ’q/4 âˆš
s(1)

T (cid:38) (1 + M âˆ’2

2 )p

sq(p/T )1/2âˆ’q/4

Table 1: Summary of the sample size conditions and error upper bounds in Theorems 2â€“4,
where pâˆ’i = (cid:81)d

q , and sq = 21âˆ’d (cid:80)2dâˆ’1

j=1,j(cid:54)=i pj, rq = (2d)âˆ’1 (cid:80)2d

k=1 s(k)

i=1 r(i)

.

q

q

Secondly, comparing (cid:98)ASSN to (cid:98)AMN, since the factor sq in the error bounds of (cid:98)ASSN is the
for k = 1, . . . , 2dâˆ’1, (cid:98)ASSN can protect us from the bad scenarios where the

average of all s(k)
(cid:96)q â€œnormâ€ of the singular values of A[S1] is relatively large. If all the s(k)
order, then the error upper bounds for (cid:98)ASSN and (cid:98)AMN in Table 1 will be similar. However,
our simulation results in Section 6 show that (cid:98)ASSN clearly outperforms (cid:98)AMN under various
settings, even when s(1)
. Indeed, the error bounds for (cid:98)ASSN in Theorem 4 is

q = Â· Â· Â· = s(2d)

â€™s are of the same

q

q

likely to be loose, which is believed to be caused by taking the upper bounds on the dual

norm of the SSN norm in the proof of Lemma B.3; see Appendix B.3 for details. By contrast,

the error bounds for (cid:98)AMN are minimax-optimal (Han et al., 2015b). Therefore, although

our theoretical results are not sharp enough to distinguish clearly between the error rates of

(cid:98)ASSN and (cid:98)AMN, we conjecture that the actual rate of the former is generally smaller than
that of the latter. Methodologically, this is also easy to understand because, unlike (cid:98)AMN,
(cid:98)ASSN simultaneously enforces the low-rankness across all square matricizations of A rather
than just on A[S1].

Remark 1. While our SSN regularization is proposed in the time series context, the idea

of imposing joint penalties on all (close to) square matricizations of the coeï¬ƒcient tensor

can potentially be extended to general higher-order tensor estimation problems. Moreover,

it can be reï¬ned to accommodate particular structures of the data. For example, if some of

the d modes of the tensor-value time series Yt, namely p1, . . . , pd, are equal, then even a

greater number of possible square matricizations of A can be formed, resulting in improved

estimation eï¬ƒciency.

23

5.3 Truncated Regularized Estimation

While the proposed regularized estimation methods do not require exact low-rankness of

the true transition tensor A, sometimes imposing exact low-rankness may be more desirable

if one wants to interpret the underlying low-dimensional tensor dynamics. As discussed in

Section 3, the Tucker ranks can be interpreted as the numbers of dynamic factors in each

mode. In this section, we propose a truncation method to consistently estimate the true

multilinear ranks (r1, . . . , r2d) under the exact low-rank assumption.

Let Î³ be a threshold parameter to be chosen properly. Given the estimator (cid:98)ASSN, for

each i = 1, . . . , 2d, we calculate the singular value decomposition (SVD) of the mode-i matri-

cization ( (cid:98)ASSN)(i) with the singular values arranged in descending order. Next we truncate

the SVD by retaining only singular values greater than Î³, and take their corresponding left

singular vectors to deï¬ne the matrix (cid:101)U i. Then, the truncated core tensor is deï¬ned as

(cid:101)G = (cid:98)ASSN Ã—2d

i=1 (cid:101)U

(cid:62)
i ,

based on which we propose the truncated sum of square-matrix nuclear (TSSN) estimator

(cid:98)ATSSN = (cid:101)G Ã—2d

i=1 (cid:101)U i.

To derive the theoretical results on rank selection, we make the following assumption on

the exact Tucker ranks and the magnitude of the singular values.

Assumption 3. For all i = 1, . . . , 2d, Ïƒr(A(i)) = 0 for all r > ri, and there exists a constant
(cid:1) â‰¥ CÎ³. As T â†’ âˆ, the threshold parameter satisï¬es

(cid:0)A(i)

C > 1 such that min1â‰¤iâ‰¤2d Ïƒri
Î³ (cid:29) (Îº2M1/Î±RSC)(cid:112)s0p/T , where s0 = 21âˆ’d (cid:80)2dâˆ’1

k=1 rank(A[Ik]).

Assumption 3 requires that A has exact Tucker ranks (r1, . . . , r2d) which do not diverge

too fast. The smallest positive singular value for each A(i) is assumed to be bounded away

from the threshold Î³ when the sample size is suï¬ƒciently large. Since Assumption 3 involves

unknown quantities, it cannot be used directly for determining Î³ in practice. Instead, we

recommend using the data-driven threshold parameter Î³ described at the end of the next

subsection.

24

The rank selection consistency of the truncation method and the asymptotic estimation

error rate of (cid:98)ATSSN are given by the following theorem.

Theorem 5. Under the conditions of Theorem 4 and Assumption 3, if the tuning parameter
Î»SSN (cid:16) Îº2M121âˆ’d(cid:112)p/T , then

(cid:110)

rank

(cid:16)

P

( (cid:98)ATSSN)(i)

(cid:17)

= rank(A(i)), for i = 1, . . . , 2d

(cid:111)

â†’ 1,

as T â†’ âˆ, and for any ï¬xed d,

(cid:107) (cid:98)ATSSN âˆ’ A(cid:107)F = Op

(cid:16)(cid:112)s0p/T

(cid:17)

,

where s0 is deï¬ned as in Assumption 3.

5.4 ADMM Algorithm

This subsection presents the algorithm for the proposed (T)SSN regularized estimator. The

algorithm for (cid:98)ASN can be developed analogously, while (cid:98)AMN can be obtained easily as in

Negahban and Wainwright (2011).

The objective function for the estimator (cid:98)ASSN in (24) can be rewritten as

LT (A) + Î»SSN(cid:107)A(cid:107)SSN = LT (A) + Î»SSN

2dâˆ’1
(cid:88)

k=1

(cid:107)A[Ik](cid:107)âˆ—,

(25)

where LT (A) = T âˆ’1 (cid:80)T
regularizer (cid:107)A(cid:107)SSN involves 2dâˆ’1 nuclear norms (cid:107)A[Ik](cid:107)âˆ—, which are challenging to handle

F is the quadratic loss function.

t=1 (cid:107)Yt âˆ’ (cid:104)A, Ytâˆ’1(cid:105)(cid:107)2

In (25), the

at the same time. A similar diï¬ƒculty also occurs in low-rank tensor completion, for which

Gandy et al. (2011) applied the alternating direction method of multipliers (ADMM) algo-

rithm (Boyd et al., 2011) to eï¬ƒciently separate the diï¬€erent nuclear norms. Borrowing the

idea of Gandy et al. (2011), we develop an ADMM algorithm for the miminization of (25).

To separate the 2dâˆ’1 nuclear norms in (cid:107)A(cid:107)SSN, for each A[Ik], we introduce a diï¬€erent
dummy variable Wk as a surrogate for A, where k = 1, . . . , 2dâˆ’1. Then the augmented

Lagrangian is

L(A, W, C) = LT (A) +

2dâˆ’1
(cid:88)

k=1

(cid:104)
Î»SSN(cid:107)(Wk)[Ik](cid:107)âˆ— + 2Ï(cid:104)Ck, A âˆ’ Wk(cid:105) + Ï(cid:107)A âˆ’ Wk(cid:107)2

F

(cid:105)
,

25

Algorithm 2 ADMM algorithm for (T)SSN estimator
Initialize: C(0)

k = A(0) = (cid:98)AMN, for k = 1, . . . , 2dâˆ’1, threshold parameter Î³

k , W(0)

for j âˆˆ {0, 1, . . . , J âˆ’ 1} do

A(j+1) â† arg min

(cid:110)
LT (A) + (cid:80)2dâˆ’1

k=1 Ï(cid:107)A âˆ’ W(j)

k + C(j)

k (cid:107)2
F

(cid:111)

for k âˆˆ {1, 2, . . . , 2dâˆ’1} do
(cid:110)

W(j+1)

k â† arg min
k â† C(j)
C(j+1)

k + A(j+1) âˆ’ W(j+1)

k

Ï(cid:107)A(j+1) âˆ’ Wk + C(j)

k (cid:107)2

F + Î»SSN(cid:107)(Wk)[Ik](cid:107)âˆ—

(cid:111)

end for

end for

(cid:98)ASSN â† A(J)

for i âˆˆ {1, 2, . . . , 2d} do

(cid:101)U i â† Truncated SVD(( (cid:98)ASSN)(i), Î³)

end for

(cid:101)G â† (cid:98)ASSN Ã—2d
(cid:98)ATSSN â† (cid:101)G Ã—2d

i=1 (cid:101)U

i=1 (cid:101)U i

(cid:62)
i

where Ck are the Lagrangian multipliers, for k = 1, . . . , 2dâˆ’1, and Ï is the regularization

parameter. Then we can iteratively update A, Wk and Ck by the ADMM, as shown in

Algorithm 2.

In Algorithm 2, the A-update step is an (cid:96)2-regularized least squares problem. Similarly

to Gandy et al. (2011), the Wk-update step can be solved by applying the explicit soft-
thresholding operator to the singular values of (A + Ck)[Ik]. Both subproblems have close-

form solutions. Thus, the miminization of (25) can be solved eï¬ƒciently.

For the tuning parameter selection, since the cross-validation method is unsuitable for

time series or intrinsically ordered data, we apply the Bayesian information criterion (BIC)

to select the optimal Î»SSN from a sequence of tuning parameter values, where the number of
degrees of freedom is deï¬ned as 2âˆ’(dâˆ’1) (cid:80)2dâˆ’1

k=1 sk(2p âˆ’ sk). For the threshold parameter Î³ of
the truncated estimator, we recommend Î³ = 2dâˆ’1Î»SSN/4 to practitioners, where Î»SSN is the

optimal tuning parameter selected by the BIC.

26

6 Simulation Studies

We conduct three simulation experiments to examine the ï¬nite-sample performance of the

proposed low- and high-dimensional estimation methods for the LRTAR model in previous
sections. Throughout, we generate the data from model (8) with vec(Et) i.i.d.âˆ¼ N (0, I p). The
entries of the core tensor G are generated independently from N (0, 1) and rescaled such that

(cid:107)G(cid:107)F = 5. The factor matrices U iâ€™s are generated by extracting the leading singular vectors

from Gaussian random matrices while ensuring the stationarity condition in Assumption 1.

The ï¬rst experiment focuses on the proposed low-dimensional estimation method consid-

ered in Section 4. We consider four cases of data generating processes. In both cases (1a)

and (1b), we consider d = 2 and multilinear ranks (r1, r2, r3, r4) = (1, 1, 1, 1), (2, 2, 2, 2),

or (2, 3, 2, 3).

In both cases (2a) and (2b), we consider d = 3 and multilinear ranks

(r1, r2, r3, r4, r5, r6) = (1, 1, 1, 1, 1, 1), (2, 2, 2, 1, 1, 1), or (2, 2, 2, 2, 2, 2). Both pairs of cases

diï¬€er in the setting of the dimensions piâ€™s: (1a) p1 = p2 = 5; (1b) p1 = p2 = 10; (2a)

p1 = p2 = p3 = 5; and (2b) p1 = p2 = p3 = 7. We repeat each data generating process

300 times, and conduct the estimation using true multilinear ranks. Figure 2 displays the

average estimation error against T âˆˆ [1000, 2000]. The LTR estimator clearly outperforms

both the RRR and OLS estimators throughout all cases, conï¬rming the theoretical eï¬ƒciency

comparison in Corollary 1.

The second experiment aims to verify the estimation error bound for the proposed SSN es-

timator in high dimensions. By Theorem 4, when the true transition tensor A is exactly low-
F = Op(s0p/T ), where s0 = 21âˆ’d (cid:80)2dâˆ’1

k=1 s(k)
rank, we have (cid:107) (cid:98)ASSN âˆ’A(cid:107)2
Note that this rate is dependent on the overall dimension, p = (cid:81)d

0 = rank(A[Ik]).

0 with s(k)

i=1 pi, but independent of

the individual dimensions pi. To examine the relationship between the estimation error and

the overal dimension p, sample size T , multilinear ranks ri and individual dimensions pi, we
generate the data under the eight settings listed in Table 2, and plot (cid:107) (cid:98)ASSN âˆ’ A(cid:107)2

F against

the varying parameter in each case in Figure 3, including p, 1/T, s0 and diï¬€erent settings

of piâ€™s under a ï¬xed overall dimension p. The ï¬rst three columns of Figure 3 show that
(cid:107) (cid:98)ASSN âˆ’ A(cid:107)2

F roughly scales linearly in p, T and s0, while the last column suggests that the

27

estimation error is invariant under diï¬€erent settings of individual pi as long as p and the

other parameters are ï¬xed. This lends support to the theoretical error bound for the SSN

estimator.

In the third experiment, we compare the performance of all the four high-dimensional

estimators discussed in Section 5, namely the SN, MN, SSN and TSSN estimators. We

consider four cases of data generating processes. In cases (3a) and (3b), we consider d = 2

and multilinear ranks (r1, r2, r3, r4) = (1, 1, 1, 1), (2, 2, 1, 1), or (2, 2, 2, 2).

In cases (4a)

and (4b), we consider d = 3 and multilinear ranks (r1, r2, r3, r4, r5, r6) = (1, 1, 1, 1, 1, 1),

(2, 2, 2, 1, 1, 1), or (2, 2, 2, 2, 2, 2). Similarly to the ï¬rst experiment, both pairs of cases diï¬€er

in the setting for piâ€™s: (3a) p1 = p2 = 5; (3b) p1 = p2 = 10; (4a) p1 = p2 = p3 = 5; and (4b)

p1 = p2 = p3 = 7. For each setting, we repeat 300 times, and conduct the estimation using

SN, MN, SSN and TSSN. The tuning parameter and the truncation threshold parameter are

selected by the method described in Section 5.4. In Figure 4, the average estimation error

is plotted against T âˆˆ [400, 1000] for cases (3a) and (3b), and T âˆˆ [600, 1200] for cases (4a)

and (4b). First, it can be seen that the SN estimator is much inferior to the other three

estimators, which is due to its use of the unbalanced one-mode matricizations. Secondly,

the SSN and TSSN estimators outperform or are at least as good as the MN estimator in

all cases, and their advantage is remarkably clear even when r1 = Â· Â· Â· = r2d. In addition,

the TSSN estimator generally performs better than the SSN, probably because the former

yields a more parsimonious model which further improves the estimation eï¬ƒciency.

7 Real Data Analysis

7.1 Matrix-Valued Time Series

We ï¬rst consider the modeling of a matrix-valued time series. The data is the monthly

market-adjusted return series of Famaâ€“French 10 Ã— 10 portfolios from January 1979 to De-

cember 2019, obtained from French (2020). The portfolios are constructed as the intersec-

tions of 10 portfolios formed by the book-to-market (B/M) ratio and 10 portfolios formed

28

by the size (market value of equity). Hence, d = 2, p1 = p2 = 10 and T = 492 months.

We remove the market eï¬€ect for each portfolio by subtracting its average return from the

original return series.

Let Y t âˆˆ R10Ã—10 be the observed matrix-valued time series, with its rows and columns

corresponding to diï¬€erent B/M ratios (sorted from lowest to highest) and sizes (sorted from

smallest to largest), respectively, and denote yt = vec(Y t). For comparison, we consider the

following ï¬ve candidate models:

â€¢ Vector autoregression (VAR): yt = Aytâˆ’1 + et, where A âˆˆ R100Ã—100. The model is

estimated by the least squares method.

â€¢ Vector factor model (VFM): yt = Î›f t + et, where f t is the low-dimensional vector-

valued latent factor, and Î› is the loading matrix. The model is estimated by the

method in Lam et al. (2012), and for prediction, the estimated factors (cid:98)f t are then

ï¬tted by a VAR(1) model.

â€¢ Matrix autoregression (MAR): Y t = B1Y tâˆ’1B(cid:62)

2 + Et, where B1, B2 âˆˆ R10Ã—10 are

coeï¬ƒcient matrices; see Example 1. The model is estimated by the iterated least

squares method in Chen et al. (2020b).

â€¢ Matrix factor model (MFM): Y t = RF tC (cid:62) + Et, where F t is the low-dimensional

matrix-valued latent factor, and R and C are the loading matrices. The model is

estimated by the method in Wang et al. (2019), and for prediction, the estimated

factors (cid:98)F t are then ï¬tted by a VAR(1) model.

â€¢ The proposed LRTAR model: Y t = (cid:104)A, Y tâˆ’1(cid:105) + Et, with A = G Ã—4

i=1 U i. The model

is estimated using the SSN and TSSN regularized estimation methods in Section 5.

We ï¬rst focus on the results for proposed LRTAR model. By the proposed truncation

method, we obtain the estimated multilinear ranks ((cid:98)r1, (cid:98)r2, (cid:98)r3, (cid:98)r4) = (8, 8, 2, 2). As shown
(cid:11) +
in Example 1, the model can be written equivalently as U (cid:62)

3 Y tU 4 = (cid:10)G, U (cid:62)

1 Y tâˆ’1U 2

U (cid:62)

3 EtU 4. Thus, U 3 and U 1 can be viewed as the factor loadings across diï¬€erent B/M ratios,

29

while U 4 and U 2 represent those across diï¬€erent sizes. By the factor interpretation below

(12), this result indicates that the information in Y t can be eï¬€ectively summarized into the

2 Ã— 2 response factors U (cid:62)

3 Y tU 4, while the low-rank structure associated with the predictor

Y tâˆ’1 is not strong. Moreover, the estimated multilinear ranks suggest that the MAR model

might be ineï¬ƒcient for the data, since the MAR imposes that r1 = r2 = r3 = r4 = 10 and

U 3 = U 4 = I 10; see Example 1.

In addition, by an argument similar to the discussion

in Example 2, the MFM can be regarded as a special case of the proposed model with

(U 1, U 2) = (U 3, U 4) and (r1, r2) = (r3, r4). Thus, the fact that (cid:98)r3 and (cid:98)r4 are much smaller
than (cid:98)r1 and (cid:98)r2 suggests that the MFM may be too restrictive for the data.

The TSSN estimates of the factor matrices are shown in Figure 5. The patterns of the

estimated response factor matrices (cid:101)U 3, (cid:101)U 4 âˆˆ R10Ã—2 are particularly interesting. First, for

both (cid:98)U 3 and (cid:98)U 4, the uniform pattern of the ï¬rst column indicate that portfolios across

diï¬€erent B/M ratios (or sizes) contribute approximately equally to the ï¬rst B/M (or size)

response factor. Thus, this factor represents the component of the market performance which

is invariant to the size and B/M ratio of the portfolios. The signiï¬cance of this factor may

be partially because we ï¬t the model to market-adjusted returns, where the average return

is subtracted for all portfolios. Meanwhile, for both response factor matrices, the second

column has a smoothly increasing pattern, suggesting that part of the return variation in

the market has a monotonic relationship with the size and B/M ratio. Moreover, the above

interpretations are consistent with the famous Fama-French three-factor model, where the

return of a portfolio is expected to be aï¬€ected by the market premium, outperformance of

small versus big companies, and outperformance of high B/M versus small B/M companies.

The performance of the ï¬ve models are compared through both average in-sample and

out-of-sample forecasting errors. The average in-sample forecasting error is calculated based

on the ï¬tted models for the entire data, while the average out-of-sample forecasting error is

calculated based on the rolling forecast procedure as follows. From January 2016 (t = 445)

to December 2019 (t = 492), we ï¬t the models using all the available data until time tâˆ’1 and

obtain the one-step-ahead forecast (cid:98)Y t. Then, we obtain the average of the rolling forecasting

30

errors for this period.

The average forecasting errors in (cid:96)2 and (cid:96)0 norms are presented in Table 3. Firstly, the

VAR model has the smallest in-sample forecasting error among all models, which is as ex-

pected because the VAR model is highly overparametrized. The bad in-sample performance

of the MFM agrees with our discussion about its restrictiveness for the data due to the

mismatch of the multilinear ranks. It is worth noting that the proposed LRTAR model has

competitive in-sample forecasting performance among all models.

The out-of-sample forecasting results provides a fuller picture of the eï¬ƒciency of diï¬€erent

methods. It can be seen that the VAR and VFM models perform worst among all, as they

both completely ignore the matrix structure of the data. Notably the proposed LRTAR

model, ï¬tted by either the SSN or the TSSN methods, have the smallest out-of-sample

forecasting errors. This suggests that the proposed LRTAR model can indeed eï¬ƒciently

capture the dynamic structural information in the data.

7.2 Three-Way Tensor-Valued Time Series

Fama and French (2015) extended the classical Fama-French three-factor model to a ï¬ve-

factor model which further incorporates the eï¬€ect of operating proï¬tability (OP) and invest-

ment (Inv). This motivates tensor-valued stock returns data formed according to the size

(small and big), B/M ratio (four groups sorted from lowest to highest), OP (four groups

sorted from lowest to highest), and Inv (four groups sorted from lowest to highest). We

consider two datasets retrieved from French (2020). The ï¬rst dataset consists of monthly

market-adjusted returns series of 4 Ã— 4 Ã— 2 portfolios from July 1963 to December 2019,

formed by the OP, B/M ratio and size. The second dataset consists of those of 4 Ã— 4 Ã— 2

portfolios formed by the Inv, B/M ratio and size. Hence, both are tensor-valued time series

with d = 3, p1 = p2 = 4, p3 = 2 and T = 678 months.

Similar to the analysis in the previous subsection, ï¬ve candidate models are considered:

â€¢ Vector autoregression (VAR): yt = Aytâˆ’1 + et, where A âˆˆ R32Ã—32.

31

â€¢ Vector factor model (VFM): yt = Î›f t + et, where f t is the low-dimensional vector-

valued latent factor, and Î› is the loading matrix.

â€¢ Multilinear tensor autoregression (MTAR): Yt = Ytâˆ’1 Ã—3

i=1 Bi + Et, where B1, B2 âˆˆ

R4Ã—4 and B3 âˆˆ R2Ã—2 are coeï¬ƒcient matrices; see Example 1.

â€¢ Tensor factor model (TFM): Yt = Ft Ã—3

i=1 U i + Et, where Ft is the low-dimensional

tensor-valued latent factor, and U iâ€™s are the loading matrices.

â€¢ The proposed LRTAR model: Yt = (cid:104)A, Ytâˆ’1(cid:105) + Et, with A = G Ã—6

i=1 U i.

The TFM is estimated by the method in Chen et al. (2020c), and for prediction, the

estimated factors (cid:98)Ft are ï¬tted by a VAR(1) model. The other four models are ï¬tted by

the same methods as in the previous subsection. The multilinear ranks selected by the

truncation method are ((cid:98)r1, (cid:98)r2, (cid:98)r3, (cid:98)r4, (cid:98)r5, (cid:98)r6) = (3, 3, 2, 1, 1, 1) and (2, 2, 2, 1, 1, 1) for the OP-
BM-Size portfolio return series and the Inv-BM-Size portfolio return series, respectively.

Note that similar to the BM-Size 10 Ã— 10 series in the previous subsection, the low-rank

structure for the response in these two tensor-valued datasets is more evident than that for

the predictor. Figure 6 shows the TSSN estimates of the factor matrices. We ï¬nd that the

estimated response factors have a uniform pattern similar to that in Figure 5 for the matrix-

valued data. The fact that only one response factor is extracted in each direction suggests

that there might not be substantial eï¬€ect of OP, Inv, B/M ratio or size on the returns for

these datasets.

Finally, using the same methods as in the previous subsection, we calculate the average

in-sample and out-of-sample forecasting errors for both datasets ï¬tted with the ï¬ve models.

As shown in Table 4, the comparison results for the two datasets are quite similar. It can be

clearly observed that the VAR model always has the smallest in-sample forecasting error, yet

the largest out-of-sample forecasting error. On the contrary, the proposed LRTAR model,

ï¬tted by either the SSN or the TSSN methods, has the smallest out-of-sample forecasting

error. Moreover, the in-sample forecasting performance of the LRTAR model is competi-

tive even compared to the VAR model. Similarly to the MFM in the previous subsection,

32

the TFM model has poor in-sample performance, possibly due to the discrepancy between

(r1, r2, r3) and (r4, r5, r6), as reï¬‚ected by the estimated multilinear ranks. In sum, the re-

sults support the eï¬ƒciency and ï¬‚exibility of the proposed model and estimation methods for

tensor-valued time series data.

8 Conclusion and Discussion

Eï¬ƒcient modeling and forecasting of general structured (tensor-valued), high-dimensional

time series data is an important research topic which however has been rarely explored in

the literature so far. This paper makes the ï¬rst thorough attempt to address this prob-

lem by introducing the low-rank tensor autoregressive model. By assuming the exactly or

approximately low-Tucker-rank structure of the transition tensor, the model exploits the

low-dimensional tensor dynamic structure of the high-dimensional time series data, and

summarizes the complex temporal dependencies into interpretable dynamic factors.

Asymptotic and non-asymptotic properties are derived for the proposed low- and high-

dimensional estimators, respectively. For the latter, we relax the conventional Gaussian

assumption in the high-dimensional time series literature to sub-Gaussianity via a new

martingale-based concentration technique. Moreover, based on the special structure of the

transition tensor, a novel convex regularizer, the SSN, is proposed, gaining eï¬ƒciencies from

both the square matricization and simultaneous penalization across modes. A truncation

method, the TSSN, is further introduced to consistently select the multilinear ranks and

enhance model interpretability.

We discuss several directions for future research. First, the proposed estimators cannot

adapt to the contemporaneous correlation among elements of the random error Et, which

may lead to eï¬ƒciency loss. This issue can be addressed by considering the generalized least
squares loss function, LT (A; Î£e) = T âˆ’1 (cid:80)T
e vec(Yt âˆ’ (cid:104)A, Ytâˆ’1(cid:105)),
where Î£e = var(vec(Et)). Then A may be estimated jointly with Î£e or by a two-step

t=1 vec(Yt âˆ’ (cid:104)A, Ytâˆ’1(cid:105))(cid:62)Î£âˆ’1

approach based on a consistent estimator (cid:98)Î£e (Basu and Michailidis, 2015; Davis et al.,

33

2016).

Secondly, the proposed methods can be generalized to the LRTAR model of ï¬nite lag

order L, deï¬ned as Yt = (cid:104)A1, Ytâˆ’1(cid:105) + Â· Â· Â· + (cid:104)AL, Ytâˆ’L(cid:105) + Et, where A1, . . . , AL are 2d-th-

order multilinear low-rank coeï¬ƒcient tensors. Then, one may consider the SSN regularized
estimation by minimizing T âˆ’1 (cid:80)T

j=1 Î»j(cid:107)Aj(cid:107)SSN. Alterna-
tively, the Ajâ€™s can be combined into a (2d + 1)-th-order tensor A âˆˆ Rp1Ã—Â·Â·Â·pdÃ—p1Ã—Â·Â·Â·Ã—pdÃ—L

j=1(cid:104)Aj, Ytâˆ’j(cid:105)(cid:107)2

t=1 (cid:107)Yt âˆ’ (cid:80)L

F + (cid:80)L

whose mode-(2d + 1) matricization is A(2d+1) = (A1, . . . , AL); see Wang et al. (2020) for

a similar idea. Even though A may not have exactly square matricizations for L > 1, the

proposed SSN can still be adapted by employing approximately square matricizations. For

instance, consider the pL Ã— p multi-mode matricizations A[Ikâˆª{2d+1}], where the index sets
Ik for k = 1, . . . , 2dâˆ’1 are deï¬ned as in this paper. Then, a generalized SSN norm can be
constructed as (cid:107)A(cid:107)GSSN = (cid:80)2dâˆ’1
k=1 (cid:107)A[Ikâˆª{2d+1}](cid:107)âˆ—, which will reduce to (cid:107)A(cid:107)SSN when L = 1.
Thirdly, the LRTAR model can be readily extended to incorporate exogenous tensor-

valued predictors, giving rise to the LRTAR-X model, Yt = (cid:104)A, Ytâˆ’1(cid:105) + (cid:104)B, Xt(cid:105) + Et, where

Xt is an m-th-order tensor of exogenous variables, and B is a (d + m)-th-order coeï¬ƒcient

tensor. When the dimension of Xt is high, a low-dimensional structure, such as sparsity,

group sparsity or low-rankness, can be imposed on B to improve the estimation eï¬ƒciency.

Moreover, an open question for matrix- or tensor-valued time series models is how to

incorporate additional structures or constraints; for instance, transport or trade ï¬‚ow data

have unspeciï¬ed diagonal entries (Chen and Chen, 2019), and realized covariance matrix

data are subject to positive deï¬nite constraints. Beyond the time series context, it is also

worth investigating the generalization of the SSN regularization method in higher-order

tensor estimation and completion applications, such as neuroimaging analysis (Li et al.,

2018), recommender sytem (Bi et al., 2018) and natural language processing (Frandsen and

Ge, 2019).

34

Appendix A: Proofs for Low-Dimensional Estimation

Below we give the proofs of Theorem 1 and Corollary 1 in Section 4, which generally follow

from Proposition 4.1 in Shapiro (1986) for overparameterized models.

Proof of Theorem 1. The proposed model in (10) can be written in the matrix form

ï£¹

ï£®

y(cid:62)
0

y(cid:62)
1
...

ï£¯
ï£¯
ï£¯
ï£¯
ï£¯
ï£¯
ï£¯
ï£°

=

ï£®

ï£¯
ï£¯
ï£¯
ï£¯
ï£¯
ï£¯
ï£¯
ï£°

y(cid:62)
1

y(cid:62)
2
...

ï£º
ï£º
ï£º
ï£º
ï£º
ï£º
ï£º
ï£»

y(cid:62)
T
(cid:124) (cid:123)(cid:122) (cid:125)
Y

y(cid:62)
T âˆ’1
(cid:124) (cid:123)(cid:122) (cid:125)
X

ï£¹

ï£º
ï£º
ï£º
ï£º
ï£º
ï£º
ï£º
ï£»

A(cid:62)

[S2] +

.

e(cid:62)
1

e(cid:62)
2
...

ï£®

ï£¯
ï£¯
ï£¯
ï£¯
ï£¯
ï£¯
ï£¯
ï£°

ï£¹

ï£º
ï£º
ï£º
ï£º
ï£º
ï£º
ï£º
ï£»

e(cid:62)
T
(cid:124) (cid:123)(cid:122) (cid:125)
E

Let (cid:98)hOLS = vec(( (cid:98)AOLS)[S2]). Under Assumption 1, by the classical asymptotic theory for

stationary VAR models, as T â†’ âˆ, we have

âˆš

T ((cid:98)hOLS âˆ’ h) dâ†’ N (0, Î£OLS),

where Î£OLS = Î£e âŠ— Î£âˆ’1

y , with Î£e = var(et) and Î£y = var(yt).

Following Shapiro (1986), consider the discrepancy function

F ((cid:98)hOLS, h) = (cid:107)vec(Y ) âˆ’ (I p âŠ— X)h(cid:107)2

2 âˆ’ (cid:107)vec(Y ) âˆ’ (I p âŠ— X)(cid:98)hOLS(cid:107)2
2.

Note that F ((cid:98)hOLS, h) is nonnegative and twice continuously diï¬€erentiable, and equals zero

if and only if (cid:98)hOLS = h.

The Jacobian matrix H = âˆ‚h(Î¸)/âˆ‚Î¸ in (20) can be veriï¬ed by noting that

h = vec(A[S2]) = vec((âŠ—iâˆˆS2U i)G[S2](âŠ—iâˆˆS1U i)(cid:62)) = ((âŠ—iâˆˆS1U i) âŠ— (âŠ—iâˆˆS2U i)) vec(G[S2])

and that for any 1 â‰¤ i â‰¤ 2d,

h = vec(A[S2]) = P (i)

[S2]vec (cid:0)U iG(i)(âŠ—2d

j=1,j(cid:54)=iU (cid:62)

[S2]vec(A(i)) = P (i)
= P (i)
[S2]

(cid:8)(cid:2)(âŠ—2d

j=1,j(cid:54)=iU i)G(cid:62)

(i)

(cid:9) vec(U i).

j )(cid:1)
(cid:3) âŠ— I pi

Then, by Proposition 4.1 in Shapiro (1986), we obtain that the minimizer of F ((cid:98)hOLS, Â·),

namely the LTR estimator, has the asymptotic normality,

âˆš

T ((cid:98)hLTR âˆ’ h) dâ†’ N (0, Î£LTR)

35

and Î£LTR = P Î£OLSP (cid:62), where P = H(H (cid:62)J H)â€ H (cid:62)J is the projection matrix, J =

Î£âˆ’1

e âŠ— Î£y is the Fisher information matrix of h, and â€  denotes the Moore-Penrose inverse.

Since Î£OLS = J âˆ’1, we can obtain that Î£LTR = H(H (cid:62)J H)â€ H (cid:62).

Proof of Corollary 1. As discussed in the proof of Theorem 1, Î£OLS = J âˆ’1, and observe that

Î£LTR = H(H (cid:62)J H)â€ H (cid:62) = J âˆ’1/2QJ 1/2HJ âˆ’1/2,

(A.1)

where QJ 1/2H is the projection matrix onto the orthogonal compliment of span(J 1/2H).

On the other hand, under the proposed model, the transition matrix can be decom-

posed as A[S2] = V 1V (cid:62)
s1 = min((cid:81)d

2 , where V 1 = âŠ—iâˆˆS1U i, V 2 = (âŠ—iâˆˆS2U i)G[S2], and rank(A[S2]) â‰¤
i=d+1 ri). Thus, we can write h = vec(A[S2]) = h(Ï†), where Ï† =
(vec(V 1)(cid:62), vec(V 2)(cid:62))(cid:62) is the parameter vector for the RRR estimator. Then, similarly

i=1 ri, (cid:81)2d

to the proof of Theorem 1, we denote the Jacobian matrix by R = âˆ‚h/âˆ‚Ï†. By similar

arguments, we have

âˆš

T ((cid:98)hRRR âˆ’ h) dâ†’ N (0, Î£RRR), where (cid:98)hRRR = vec(( (cid:98)ARRR)[S2]), and

Î£RRR = R(R(cid:62)J R)â€ R(cid:62) = J âˆ’1/2QJ 1/2RJ âˆ’1/2,

(A.2)

where QJ 1/2R is the projection matrix onto the orthogonal compliment of span(J 1/2R).
Hence, it is clear that Î£RRR â‰¤ J âˆ’1 = Î£OLS.

Moreover, since the Tucker decomposition can be viewed as a further decomposition of

the low-rank decomposition V U (cid:62) for A[S2], we have H = âˆ‚h/âˆ‚Î¸ = R Â· âˆ‚Ï†/âˆ‚Î¸. By (A.1)
and (A.2), we have Î£LTR â‰¤ Î£RRR, since span(J 1/2H) âŠ‚ span(J 1/2R).

Appendix B: Proofs for High-Dimensional Estimation

In this appendix, we provide the proofs of Theorems 2â€“5 in Section 5. We start with a

preliminary analysis in Appendix B.1 which lays out the common technical framework for

proving the estimation and prediction error bounds of the SN, MN and SSN regularized

estimators, and four lemmas, Lemmas B.1â€“B.4, are introduced herein. Then in Appendix

B.2 we give the proofs of Theorems 2â€“5. The proofs of Lemmas B.1â€“B.4 are provided in

Appendix B.3, and three auxiliary lemmas are collected in Appendix B.4

36

B.1 Preliminary Analysis

The technical framework for proving the error bounds in Theorem 2â€“4 consists of two main

steps, a deterministic analysis and a stochastic analysis, given in Sections B.1.1 and B.1.2,

respectively. The goal of the ï¬rst one is to derive the error bounds given the deterministic

realization of the time series, assuming that the parameters satisfy certain regularity con-

ditions. The goal of the second one is to verify that under stochasticity these regularity

conditions are satisï¬ed with high probability.

B.1.1 Deterministic Analysis

Throughout the appendix, we adopt the following notations. We use C to denote a generic

positive constant, which is independent of the dimensions and the sample size. For any

matrix M and a compatible subspace S, we denote by M S the projection of M onto S.

In addition, let col(M ) be the column space of M , and let S âŠ¥ be the complement of the

subspace S. For a generic tensor W âˆˆ Rp1Ã—Â·Â·Â·Ã—p2d, the dual norms of its SSN norm and SN

norm, denoted by (cid:107)W(cid:107)SSNâˆ— and (cid:107)W(cid:107)SNâˆ—, respectively, are deï¬ned as

(cid:107)W(cid:107)SSNâˆ— =

sup
TâˆˆRp1Ã—Â·Â·Â·Ã—p2d ,(cid:107)T(cid:107)SSNâ‰¤1

(cid:104)W, T(cid:105), and (cid:107)W(cid:107)SNâˆ— =

sup
TâˆˆRp1Ã—Â·Â·Â·Ã—p2d ,(cid:107)T(cid:107)SNâ‰¤1

(cid:104)W, T(cid:105).

Moreover, for any two tensors X âˆˆ Rp1Ã—Â·Â·Â·Ã—pm and Y âˆˆ Rpm+1Ã—Â·Â·Â·Ã—pmn , their tensor outer

product is deï¬ned as (X â—¦ Y) âˆˆ Rp1Ã—Â·Â·Â·Ã—pmÃ—pm+1Ã—Â·Â·Â·Ã—pm+n where

(X â—¦ Y)i1...imim+1...im+n = Xi1...im

Yim+1...im+n,

for any 1 â‰¤ i1 â‰¤ p1, . . . , 1 â‰¤ im+n â‰¤ pm+n.

For the theory of regularized M -estimators, restricted error sets and restricted strong

convexity are essential deï¬nitions. To deï¬ne the former, we need to ï¬rst introduce the

following restricted model subspaces.

For i = 1, . . . , 2d, denote by (cid:101)Ui and (cid:101)Vi the spaces spanned by the ï¬rst ri left and right

singular vectors in the SVD of A(i), respectively. Deï¬ne the collections of subspaces

N = (N1, . . . , N2d) and N

âŠ¥

= (N

âŠ¥
1 , . . . , N

âŠ¥
2d),

37

where

Ni = {M âˆˆ RpiÃ—pâˆ’ip|col(M ) âŠ‚ (cid:101)Ui, col(M (cid:62)) âŠ‚ (cid:101)Vi},

N

âŠ¥

i = {M âˆˆ RpiÃ—pâˆ’ip|col(M ) âŠ¥ (cid:101)Ui, col(M (cid:62)) âŠ¥ (cid:101)Vi},

(B.1)

for i = 1, . . . , 2d. Note that Ni âŠ‚ N i.

Furthermore, for k = 1, . . . , 2dâˆ’1, denote by Uk and Vk the spaces spanned by the ï¬rst
k left and right singular vectors in the SVD of the square matricization A[Ik], respectively,
sâˆ—
where sâˆ—

k = rank(A)[Ik]. Similarly, deï¬ne the collections of subspaces

M := (M1, . . . , M2dâˆ’1) and M

âŠ¥

= (M

âŠ¥
1 , . . . , M

âŠ¥
2dâˆ’1),

where

Mk = {M âˆˆ RpÃ—p|col(M ) âŠ‚ Uk, col(M (cid:62)) âŠ‚ Vk},

M

âŠ¥

k = {M âˆˆ RpÃ—p|col(M ) âŠ¥ Uk, col(M (cid:62)) âŠ¥ Vk},

(B.2)

for k = 1, . . . , 2dâˆ’1. In particular, as described in Section 5.2, I1 = S1 = {1, . . . , d}. Thus,
1 are the subspaces associated with the square matricization A[S1].

M1 and M

âŠ¥

Then, for simplicity, for any W âˆˆ Rp1Ã—Â·Â·Â·Ã—p2d, we denote

W(i)

N = (W(i))Ni, W(i)

N âŠ¥ = (W(i))N âŠ¥

i

W(k)

M = (W[Ik])Mk, W(k)

MâŠ¥ = (W[Ik])MâŠ¥

, W(i)
N
, W(k)
M

k

= (W(i))N i

âŠ¥ = (W(i))
N

âŠ¥
i

, W(i)
N
, W(k)
M

âŠ¥ = (W[Ik])

,

âŠ¥
M
k

= (W[Ik])Mk

where i = 1, . . . , 2d and k = 1, . . . , 2dâˆ’1. Based on the subspaces deï¬ned in (B.1) and (B.2),

we can deï¬ne the restricted error sets corresponding to the three regularized estimators as

follows.

Deï¬nition 1. The restricted error set corresponding to M is deï¬ned as

CSSN(M) :=

ï£±
ï£²

âˆ† âˆˆ Rp1Ã—Â·Â·Â·Ã—p2d :

ï£³

2dâˆ’1
(cid:88)

k=1

(cid:107)âˆ†(k)
M

âŠ¥(cid:107)âˆ— â‰¤ 3

2dâˆ’1
(cid:88)

k=1

(cid:107)âˆ†(k)
M

(cid:107)âˆ— + 4

2dâˆ’1
(cid:88)

k=1

(cid:107)A(k)

MâŠ¥(cid:107)âˆ—

ï£¼
ï£½

ï£¾

.

The restricted error set corresponding to N is deï¬ned as

(cid:40)

CSN(N ) :=

âˆ† âˆˆ Rp1Ã—Â·Â·Â·Ã—p2d :

2d
(cid:88)

i=1

(cid:107)âˆ†(i)
N

âŠ¥(cid:107)âˆ— â‰¤ 3

2d
(cid:88)

i=1

(cid:107)âˆ†(i)
N

(cid:107)âˆ— + 4

(cid:41)

(cid:107)A(i)

N âŠ¥(cid:107)âˆ—

.

2d
(cid:88)

i=1

38

The restricted error set corresponding to M1 is deï¬ned as

CMN(M1) :=

(cid:110)

âˆ† âˆˆ Rp1Ã—Â·Â·Â·Ã—p2d : (cid:107)âˆ†(1)
M

âŠ¥(cid:107)âˆ— â‰¤ 3(cid:107)âˆ†(1)
M

(cid:107)âˆ— + 4(cid:107)A(1)

MâŠ¥(cid:107)âˆ—

(cid:111)

.

The ï¬rst lemma shows that if the tuning parameter is well chosen for each regularized

estimator, the estimation error belongs to the corresponding restricted error set.

Lemma B.1. For the SSN estimator, if the regularization parameter Î»SSN â‰¥ 4(cid:107)T âˆ’1 (cid:80)T
Et(cid:107)SSNâˆ—, the error âˆ†SSN = (cid:98)ASSN âˆ’ A belongs to the set CSSN(M).

t=1

Ytâˆ’1â—¦

For the SN estimator, if the regularization parameter Î»SN â‰¥ 4(cid:107)T âˆ’1 (cid:80)T

t=1

Ytâˆ’1 â—¦ Et(cid:107)SNâˆ—,

the error âˆ†SN = (cid:98)ASN âˆ’ A belongs to the set CSN(N ).

For the MN estimator, if the regularization parameter Î»MN â‰¥ 4(cid:107)T âˆ’1 (cid:80)T

t=1 vec(Ytâˆ’1)vec(Et)(cid:62)(cid:107)âˆ—,

the error âˆ†MN = (cid:98)AMN âˆ’ A belongs to the set CMN(M(1)).

Following Negahban and Wainwright (2012) and Negahban et al. (2012), a restricted

strong convexity (RSC) condition for the square loss function can be deï¬ned as follows.

Deï¬nition 2. The loss function satisï¬es the RSC condition with curvature Î±RSC > 0 and

restricted error set C, if

1
T

T
(cid:88)

t=1

(cid:107)(cid:104)âˆ†, Ytâˆ’1(cid:105)(cid:107)2

F â‰¥ Î±RSC(cid:107)âˆ†(cid:107)2
F,

âˆ€âˆ† âˆˆ C.

Based on the restricted error sets and RSC conditions, the estimation errors have the

following deterministic upper bounds.

Lemma B.2. Suppose that Î»SSN â‰¥ 4(cid:107)T âˆ’1 (cid:80)T
the parameter Î±RSC and restricted error set CSSN(M), and A[Ik] âˆˆ Bq(s(k)
q âˆˆ [0, 1) and all k = 1, . . . , 2dâˆ’1,

t=1

q

Ytâˆ’1 â—¦ Et(cid:107)SSNâˆ—, the RSC condition holds with

; p, p) for some

where sq = 21âˆ’d (cid:80)2dâˆ’1

k=1 s(k)

q

(cid:107)âˆ†SSN(cid:107)F (cid:46) âˆš

sq

(cid:18) 2dâˆ’1Î»SSN
Î±RSC

(cid:19)1âˆ’q/2

,

.

39

Suppose that Î»SN â‰¥ 4(cid:107)T âˆ’1 (cid:80)T

t=1

Ytâˆ’1â—¦Et(cid:107)SNâˆ—, the RSC condition holds with the parameter

Î±RSC and restricted error set CSN(N ), and A(i) âˆˆ Bq(r(i)

q ; pi, pâˆ’ip) for some q âˆˆ [0, 1) and

all i = 1, . . . , 2d,

where rq = (2d)âˆ’1 (cid:80)2d

i=1 r(i)
q .

(cid:107)âˆ†SN(cid:107)F (cid:46) âˆš

rq

(cid:18) 2d Â· Î»SN
Î±RSC

(cid:19)1âˆ’q/2

,

Suppose that Î»MN â‰¥ 4(cid:107)T âˆ’1 (cid:80)T

parameter Î±RSC and restricted error set CMN(M1), and A[S1] âˆˆ Bq(s(1)

t=1 vec(Ytâˆ’1)vec(Et)(cid:107)âˆ—, the RSC condition holds with the
q ; p, p) for some q âˆˆ

[0, 1),

(cid:107)âˆ†MN(cid:107)F (cid:46)

(cid:113)

s(1)
q

(cid:18) Î»MN
Î±RSC

(cid:19)1âˆ’q/2

.

Note that Lemma B.2 is deterministic and the radius sq, rq, and s(1)
q

can also diverge to

inï¬nity.

B.1.2 Stochastic Analysis

We continue with the stochastic analysis to show that the deviation bound and the RSC

condition hold simultaneously with high probability.

Lemma B.3 (Deviation bound). Suppose that Assumptions 1 and 2 hold. If T (cid:38) p and
Î»SSN (cid:38) Îº2M121âˆ’d(cid:112)p/T , with probability at least 1 âˆ’ exp[âˆ’C(p âˆ’ d)],

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
T

T
(cid:88)

t=1

Ytâˆ’1 â—¦ Et

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)SSNâˆ—

â‰¤

Î»SSN
4

where M1 = Î»max(Î£e)/Âµ1/2

min(A).
If T (cid:38) max1â‰¤iâ‰¤d pâˆ’ip and Î»SN (cid:38) Îº2M1dâˆ’2 (cid:80)d

i=1

(cid:112)pâˆ’ip/T , with probability at least 1 âˆ’

2 (cid:80)d

i=1 exp(âˆ’Cpâˆ’ip),

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
T

T
(cid:88)

t=1

Ytâˆ’1 â—¦ Et

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)SNâˆ—

â‰¤

Î»SN
4

.

Moreover, if T (cid:38) p and Î»MN (cid:38) Îº2M1

(cid:112)p/T , with probability at least 1 âˆ’ exp(âˆ’Cp),

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
T

T
(cid:88)

t=1

vec(Ytâˆ’1)vec(Et)(cid:62)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)âˆ—

â‰¤

Î»MN
4

.

40

Next, we prove the restricted strong convexity for regularized estimators. According to

Lemma B.3, we need the sample size T (cid:38) p for all three estimators. In this case, we can

establish the strong convexity condition that is stronger than the RSC condition.

Lemma B.4 (Strong convexity). Under Assumptions 1 and 2, for T (cid:38) max(Îº2, Îº4)M âˆ’2

2 p,

with probability at least 1 âˆ’ exp[âˆ’C min(Îºâˆ’2, Îºâˆ’4)M 2

2 p],

1
T

T
(cid:88)

t=1

(cid:107)(cid:104)âˆ†, Ytâˆ’1(cid:105)(cid:107)2

F â‰¥ Î±RSC(cid:107)âˆ†(cid:107)2
F,

where M2 = [Î»min(Î£e)Âµmax(A)]/[Î»max(Î£e)Âµmin(A)] and Î±RSC = Î»min(Î£e)/(2Âµmax(A)).

B.2 Proofs of Theorems 2â€“5

Proof of Theorems 2 and 3. Theorems 2 and 3 can be proved based on Lemmas B.2â€“B.4

following the same line of the proof of Theorem 4 given below. Therefore, we omit the

details here.

Proof of Theorem 4. The proof of Theorem 4 has been split into Lemmas B.2â€“B.4. By

Lemma B.2, for deterministic realization with sample size T of a tensor autoregressive pro-
cess, if we choose Î»SSN â‰¥ 4(cid:107)T âˆ’1 (cid:80)T

Ytâˆ’1 â—¦ Et(cid:107)SSNâˆ— and RSC condition holds for the square

t=1

loss with the parameter Î±RSC, the following error upper bound can be established

(cid:107)âˆ†(cid:107)F (cid:46) âˆš
sq

(cid:18) 2dâˆ’1Î»SSN
Î±RSC

(cid:19)1âˆ’q/2

.

Denote the events E1(Î²) = {Î² â‰¥ 4(cid:107)T âˆ’1 (cid:80)T
Ytâˆ’1â—¦Et(cid:107)SSNâˆ—} and E2(Î±) = {Î»min(XX (cid:62)/T ) â‰¥
Î±}. If we take Î»SSN (cid:38) Îº2M121âˆ’d(cid:112)p/T , it suï¬ƒces to show that E1(CÎº2M121âˆ’d(cid:112)p/T ) and

t=1

E2(Î±RSC/2) occur simultaneously with high probability.

By Lemma B.3, when T (cid:38) p,

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
T

T
(cid:88)

t=1

Ytâˆ’1 â—¦ Et

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)SSNâˆ—

(cid:46) Îº2M121âˆ’d

(cid:114) p
T

with probability at least 1 âˆ’ exp[âˆ’C(p âˆ’ d)].

41

By Lemma B.4, when T (cid:38) max(Îº2, Îº4)M âˆ’2

2 p, for any âˆ† âˆˆ Rp1Ã—Â·Â·Â·Ã—p2d,

1
T

T
(cid:88)

t=1

(cid:107)(cid:104)âˆ†, Ytâˆ’1(cid:105)(cid:107)2

F â‰¥

Î»min(Î£e)
2Âµmax(A)

(cid:107)âˆ†(cid:107)2
F

with probability at least 1 âˆ’ exp[âˆ’C min(Îºâˆ’2, Îºâˆ’4)M 2

2 p].

Hence, when T (cid:38) [1 + max(Îº2, Îº4)M âˆ’2

2 ]p and Î» (cid:38) Îº2M121âˆ’d(cid:112)p/T , with probability at
2 p], the condition Î» â‰¥ 4(cid:107)T âˆ’1 (cid:80)T
Ytâˆ’1 â—¦
t=1
Et(cid:107)SSNâˆ— and the RSC condition with the parameter Î±RSC = Î»min(Î£e)/Âµmax(A) hold.

least 1 âˆ’ exp[âˆ’C(p âˆ’ d)] âˆ’ exp[âˆ’C min(Îºâˆ’2, Îºâˆ’4)M 2

Proof of Theorem 5. Theorem 4 gives the Frobenius estimation error bound. For simplicity,

we write (cid:98)A = (cid:98)ASSN and (cid:101)A = (cid:98)ATSSN in this proof. By deï¬nition, for any tensor A âˆˆ
Rp1Ã—Â·Â·Â·Ã—p2d,

(cid:107)A(cid:107)2

F = (cid:107)A(i)(cid:107)2

F =

j (A(i)),
Ïƒ2

i = 1, 2, . . . , 2d.

pi(cid:88)

j=1

In other words, the Frobenius norm of the error tensor is equivalent to the (cid:96)2 norm of singular

values of the one-mode matricization. By Mirskyâ€™s singular value inequality (Mirsky, 1960),

pi(cid:88)

[Ïƒj( (cid:98)A(i)) âˆ’ Ïƒj(A(i))]2 â‰¤

j=1

pi(cid:88)

j=1

j ( (cid:98)A(i) âˆ’ A(i)) = (cid:107) (cid:98)A âˆ’ A(cid:107)2
Ïƒ2
F,

i = 1, 2, . . . , 2d.

(B.3)

Obviously, the (cid:96)âˆ error bound is smaller than the (cid:96)2 error bound, so it follows the same
upper bound. By Theorem 4, when Î»SSN (cid:16) Îº2M121âˆ’d(cid:112)p/T , with probability approaching

one,

max
1â‰¤iâ‰¤2d

max
1â‰¤jâ‰¤pi

|Ïƒj( (cid:98)A(i)) âˆ’ Ïƒj(A(i))| â‰¤ max
1â‰¤iâ‰¤2d

(cid:40) pi(cid:88)

[Ïƒj( (cid:98)A(i)) âˆ’ Ïƒj(A(i))]2

(cid:41)1/2

j=1

â‰¤(cid:107) (cid:98)A âˆ’ A(cid:107)F (cid:46) Îº2M1
Î±RSC

(cid:114) s0p
T

.

(B.4)

Therefore, by Assumption 3, as T â†’ âˆ,

Î³ (cid:29) max
1â‰¤iâ‰¤2d

max
1â‰¤jâ‰¤pi

|Ïƒj( (cid:98)A(i)) âˆ’ Ïƒj(A(i))|.

Then, for any j > ri, since Ïƒj(A(i)) = 0, we have Î³ (cid:29) Ïƒj( (cid:98)A(i)). Thus, for all i = 1, . . . , 2d,
Ïƒj( (cid:98)A(i)) will be truncated for all j > ri. Meanwhile, by Assumption 3 and (B.4), we have

42

Ïƒri( (cid:98)A(i)) > Î³ for T suï¬ƒciently large, for all i = 1, . . . , 2d. Therefore, the rank selection
consistency of the truncated estimator (cid:101)A can be established.

Denote the event E = {rank( (cid:101)A(i)) = ri, for i = 1, . . . , 2d}. For a generic tensor T âˆˆ
Rp1Ã—Â·Â·Â·Ã—p2d, denote the sub-tensor Tik=j, a p1 Ã— Â· Â· Â· Ã— pkâˆ’1 Ã— 1 Ã— pk+1 Ã— Â· Â· Â· Ã— p2d tensor such

that

(Tik=j)i1...ikâˆ’11ik+1...i2d = Ti1...ikâˆ’1jik+1...i2d,

and sub-tensor Tik>j, a p1 Ã— Â· Â· Â· Ã— pkâˆ’1 Ã— (pk âˆ’ j) Ã— pk+1 Ã— Â· Â· Â· Ã— p2d tensor such that

(Tik>j)i1...ikâˆ’1(cid:96)ik+1...i2d = Ti1...ikâˆ’1((cid:96)+j)ik+1...i2d.

Let the HOSVD of (cid:98)A be (cid:98)G Ã—2d

i=1 (cid:98)U i. By deï¬nition, (cid:98)G is a p1 Ã— Â· Â· Â· Ã— p2d all-orthogonal

and sorted tensor such that

(cid:107)(cid:98)Gik=1(cid:107)F â‰¥ (cid:107)(cid:98)Gik=2(cid:107)F â‰¥ Â· Â· Â· â‰¥ (cid:107)(cid:98)Gik=pk(cid:107)F,

for k = 1, . . . , 2d. On E, the truncation procedure is equivalent to truncating all the sub-

tensors (cid:98)Gik>rk to zeros. Thus, (cid:107) (cid:98)A âˆ’ (cid:101)A(cid:107)F = (cid:107)(cid:98)G âˆ’ (cid:101)G(cid:107)2

k=1 (cid:107)(cid:98)Gik>rk(cid:107)2
F.
By the deï¬nition of HOSVD, (cid:107)(cid:98)Gik=j(cid:107)F = Ïƒj((cid:98)G(k)) = Ïƒj( (cid:98)A(k)), and then

F â‰¤ (cid:80)2d

(cid:107)(cid:98)Gik>rk(cid:107)2

F =

pk(cid:88)

i=rk+1

i ( (cid:98)A(k)) =
Ïƒ2

pk(cid:88)

[Ïƒi( (cid:98)A(k)) âˆ’ Ïƒi(A(k))]2

i=rk+1
pk(cid:88)

[Ïƒi( (cid:98)A(k)) âˆ’ Ïƒi(A(k))]2 â‰¤ (cid:107) (cid:98)A âˆ’ A(cid:107)2
F,

â‰¤

i=1

where the last inequality follows from (B.3).

Finally, on the event E, (cid:107) (cid:101)Aâˆ’A(cid:107)F â‰¤ (cid:107) (cid:101)Aâˆ’ (cid:98)A(cid:107)F +(cid:107) (cid:98)Aâˆ’A(cid:107)F â‰¤ (1+

2d)(cid:107) (cid:98)Aâˆ’A(cid:107)F, where
d is ï¬xed. Note that Theorem 4 implies the asymptotic rate (cid:107) (cid:98)A âˆ’ A(cid:107)F = Op((cid:112)s0p/T ) and
the ï¬rst part of this proof shows that P(E) â†’ 1, as T â†’ âˆ. The proof is complete.

âˆš

B.3 Proofs of Lemmas B.1â€“B.4

Proof of Lemma B.1. In this part, we focus on (cid:98)ASSN and simplify it to (cid:98)A. The tuning
parameter Î»SSN is simpliï¬ed to Î». The proof can be readily extended to (cid:98)ASN and (cid:98)AMN.

43

Note that the quadratic loss function can be rewritten as LT (A) = T âˆ’1 (cid:80)T

t=1 (cid:107)Yt âˆ’
2, where yt = vec(Yt). By the optimality of the

(cid:104)A, Ytâˆ’1(cid:105)(cid:107)2

F = T âˆ’1 (cid:80)T

t=1 (cid:107)yt âˆ’ A[S2]ytâˆ’1(cid:107)2

SSN estimator,

1
T

1
T

1
T

1
T

T
(cid:88)

t=1
T
(cid:88)

t=1
T
(cid:88)

t=1
T
(cid:88)

t=1

â‡’

â‡’

â‡’

(cid:107)yt âˆ’ (cid:98)A[S2]ytâˆ’1(cid:107)2

2 + Î»(cid:107) (cid:98)A(cid:107)SSN â‰¤

1
T

T
(cid:88)

t=1

(cid:107)yt âˆ’ A[S2]ytâˆ’1(cid:107)2

2 + Î»(cid:107)A(cid:107)SSN

(cid:107)âˆ†[S2]ytâˆ’1(cid:107)2

2 â‰¤

2
T

T
(cid:88)

(cid:104)et, âˆ†[S2]ytâˆ’1(cid:105) + Î»((cid:107)A(cid:107)SSN âˆ’ (cid:107) (cid:98)A(cid:107)SSN)

(cid:107)âˆ†[S2]ytâˆ’1(cid:107)2

2 â‰¤ 2

T âˆ’1

T
(cid:88)

(cid:43)

Ytâˆ’1 â—¦ Et, âˆ†

+ Î»((cid:107)A(cid:107)SSN âˆ’ (cid:107) (cid:98)A(cid:107)SSN)

t=1

(cid:42)

(cid:107)âˆ†[S2]ytâˆ’1(cid:107)2

2 â‰¤ 2(cid:107)âˆ†(cid:107)SSN

t=1
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

T âˆ’1

T
(cid:88)

t=1

Ytâˆ’1 â—¦ Et

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)SSNâˆ—

+ Î»((cid:107)A(cid:107)SSN âˆ’ (cid:107) (cid:98)A(cid:107)SSN),

where (cid:107) Â· (cid:107)SSNâˆ— refers to the dual norm of the SSN norm.

By triangle inequality and decomposability, we have

(cid:107) (cid:98)A(cid:107)SSN âˆ’ (cid:107)A(cid:107)SSN = (cid:107)A + âˆ†(cid:107)SSN âˆ’ (cid:107)A(cid:107)SSN =

2dâˆ’1
(cid:88)

k=1

(cid:107)A[Ik] + âˆ†[Ik](cid:107)âˆ— âˆ’

2dâˆ’1
(cid:88)

k=1

(cid:107)A[Ik](cid:107)âˆ—

=

â‰¥

â‰¥

2dâˆ’1
(cid:88)

k=1
2dâˆ’1
(cid:88)

k=1
2dâˆ’1
(cid:88)

k=1

(cid:107)A(k)

M + A(k)

MâŠ¥ + âˆ†(k)

M

+ âˆ†(k)
M

âŠ¥(cid:107)âˆ— âˆ’

2dâˆ’1
(cid:88)

k=1

(cid:107)A[Ik](cid:107)âˆ—

(cid:104)
(cid:107)A(k)

M + âˆ†(k)

âŠ¥(cid:107)âˆ— âˆ’ (cid:107)A(k)

MâŠ¥ + âˆ†(k)

M

M

(cid:107)âˆ— âˆ’ (cid:107)A(k)

MâŠ¥(cid:107)âˆ— âˆ’ (cid:107)A(k)

M (cid:107)âˆ—

(cid:105)

(cid:104)
(cid:107)âˆ†(k)
M

âŠ¥(cid:107)âˆ— âˆ’ 2(cid:107)A(k)

MâŠ¥(cid:107)âˆ— âˆ’ (cid:107)âˆ†(k)

M

(cid:105)

.

(cid:107)âˆ—

If Î» â‰¥ 4(cid:107)T âˆ’1 (cid:80)T

t=1

Ytâˆ’1 â—¦ Et(cid:107)SSNâˆ—, we have

0 â‰¤

â‰¤

=

1
T

Î»
2

Î»
2

t=1
2dâˆ’1
(cid:88)

(cid:104)

(cid:107)âˆ†(k)
M

k=1
2dâˆ’1
(cid:88)

(cid:104)

k=1

T
(cid:88)

(cid:107)âˆ†[S2]ytâˆ’1(cid:107)2

2 â‰¤

Î»
2

(cid:107)âˆ†(cid:107)SSN âˆ’ Î»((cid:107) (cid:98)A(cid:107)SSN âˆ’ (cid:107)A(cid:107)SSN)

(cid:107)âˆ— + (cid:107)âˆ†(k)
M

âŠ¥(cid:107)âˆ— âˆ’ 2(cid:107)âˆ†(k)
M

âŠ¥(cid:107)âˆ— + 4(cid:107)A(k)

MâŠ¥(cid:107)âˆ— + 2(cid:107)âˆ†(k)

M

(cid:105)

(cid:107)âˆ—

3(cid:107)âˆ†(k)
M

(cid:107)âˆ— + 4(cid:107)A(k)

MâŠ¥(cid:107)âˆ— âˆ’ (cid:107)âˆ†(k)

âŠ¥(cid:107)âˆ—

M

(cid:105)

.

Hence, the error âˆ† lies in the restricted error set CSSN(M).

44

Proof of Lemma B.2. Similar to Lemma B.1, we focus on the SSN estimator, and the results

for SN and MN estimators can be extended in a similar way.

Note that T âˆ’1 (cid:80)T

t=1 (cid:107)(cid:104)âˆ†, Ytâˆ’1(cid:105)(cid:107)2

F = T âˆ’1 (cid:80)T

t=1 (cid:107)âˆ†[S2]ytâˆ’1(cid:107)2

2. Following the proof of Lemma

B.1, âˆ† âˆˆ CSSN(M) and

1
T

T
(cid:88)

t=1

(cid:107)(cid:104)âˆ†, Ytâˆ’1(cid:105)(cid:107)2

F â‰¤

Î»
2

(cid:107)âˆ†(cid:107)SSN + Î»((cid:107)A(cid:107)SSN âˆ’ (cid:107) (cid:98)A(cid:107)SSN) â‰¤

3Î»
2

(cid:107)âˆ†(cid:107)SSN

=

3Î»
2

â‰¤ 6Î»

â‰¤ 6Î»

2dâˆ’1
(cid:88)

k=1
2dâˆ’1
(cid:88)

k=1
2dâˆ’1
(cid:88)

k=1

(cid:107)âˆ†[Ik](cid:107)âˆ— â‰¤

3Î»
2

2dâˆ’1
(cid:88)

(cid:16)

k=1

(cid:107)âˆ†(k)
M

(cid:107)âˆ— + (cid:107)âˆ†(k)
M

âŠ¥(cid:107)âˆ—

(cid:17)

(cid:107)âˆ†(k)
M

(cid:107)âˆ— + 6Î»

2dâˆ’1
(cid:88)

k=1

(cid:107)A(k)

MâŠ¥(cid:107)âˆ—

âˆš

2sk(cid:107)âˆ†(k)
M

(cid:107)F + 6Î»

2dâˆ’1
(cid:88)

k=1

(cid:107)A(k)

MâŠ¥(cid:107)âˆ—

2dâˆ’1
(cid:88)

âˆš

(cid:46) Î»

k=1

2sk(cid:107)âˆ†(cid:107)F + 6Î»

2dâˆ’1
(cid:88)

k=1

(cid:107)A(k)

MâŠ¥(cid:107)âˆ—

where the last inequality stems from the fact that âˆ†(k)
M

has a matrix rank at most 2sk, similar

to Lemma 1 in Negahban and Wainwright (2011).

As the RSC condition holds with the parameter Î±RSC and restricted error set CSSN(M),

Î±RSC(cid:107)âˆ†(cid:107)2

F â‰¤

1
T

T
(cid:88)

t=1

(cid:107)(cid:104)âˆ†, Ytâˆ’1(cid:105)(cid:107)2
F

(cid:46) Î»

2dâˆ’1
(cid:88)

âˆš

k=1

sk(cid:107)âˆ†(cid:107)F + Î»

2dâˆ’1
(cid:88)

k=1

(cid:107)A(k)

MâŠ¥(cid:107)âˆ—.

Thus, by Cauchy inequality,

(cid:107)âˆ†(cid:107)2
F

(cid:46)

Î»2((cid:80)2dâˆ’1
k=1
Î±2

RSC

âˆš

sk)2

+

Î» (cid:80)2dâˆ’1

k=1 (cid:107)A(k)
Î±RSC

MâŠ¥(cid:107)âˆ—

(cid:46) Î»22dâˆ’1 (cid:80)2dâˆ’1
Î±2

k=1 sk

RSC

+

Î» (cid:80)2dâˆ’1

k=1 (cid:107)A(k)
Î±RSC

MâŠ¥(cid:107)âˆ—

.

Consider any threhold Ï„k â‰¥ 0 and deï¬ne the thresholded subspace M(k) corresponding
to the column and row spaces spanned by the ï¬rst r(k) singular vectors of A[Ik] where
Ïƒ1(A[Ik]) â‰¥ Â· Â· Â· â‰¥ Ïƒr(k)(A[Ik]) > Ï„k â‰¥ Ïƒr(k)+1(A[Ik]). By the deï¬nition of Bq(s(k)
have s(k)
Â· Ï„ âˆ’q
k .

q â‰¥ r(k) Â· Ï„ q

; p, p), we

k and thus r(k) â‰¤ s(k)
Then, the approximation error can be bounded by

q

q

(cid:107)A(k)

MâŠ¥(cid:107)âˆ— =

p
(cid:88)

Ïƒr(A[Ik]) =

p
(cid:88)

r=r(k)+1

r=r(k)+1

r(A[Ik]) Â· Ïƒ1âˆ’q
Ïƒq

r

(A[Ik]) â‰¤ s(k)

q

Â· Ï„ 1âˆ’q
k

.

45

The estimation error can be bounded by
(cid:46) Î»22dâˆ’1 (cid:80)2dâˆ’1
k=1 s(k)
Î±2

(cid:107)âˆ†(cid:107)2
F

q

RSC

Â· Ï„ âˆ’q
k

+

Î» (cid:80)2dâˆ’1

k=1 s(k)
q
Î±RSC

Â· Ï„ 1âˆ’q
k

.

Setting each Ï„k (cid:16) Î±âˆ’1

RSC(q/(1 âˆ’ q))2dâˆ’1Î», the upper bound can be minimized to

(cid:107)âˆ†(cid:107)2
F

(cid:46) 21âˆ’d

2dâˆ’1
(cid:88)

k=1

s(k)
q

(cid:18) Î» Â· 2dâˆ’1
Î±RSC

(cid:19)2âˆ’q

.

The proof is complete.

Proof of Lemma B.3. First, we derive an upper bound of the dual norm of the SSN norm.

By deï¬nition, for any tensor A and collection of index sets I = {I1, . . . , I2dâˆ’1}, the SSN norm

is

(cid:107)A(cid:107)SSN =

2dâˆ’1
(cid:88)

(cid:107)A[Ik](cid:107)âˆ—,

k=1
and its dual norm is (cid:107)A(cid:107)SSNâˆ— := sup(cid:104)W, A(cid:105) such that (cid:107)W(cid:107)SSN â‰¤ 1. By a method similar

to that in Tomioka et al. (2011), it can be shown that

(cid:107)A(cid:107)SSNâˆ— =

inf

(cid:80)2dâˆ’1
k=1

Xk=A

max
k=1,...,2dâˆ’1

(cid:107)(Xk)[Ik](cid:107)op.

Then, we can take Xk = ((cid:80)2dâˆ’1

k=1 1/ck)âˆ’1(A/ck), where ck = (cid:107)A[Ik](cid:107)op, and apply Jensenâ€™s

inequality so that we have

(cid:107)A(cid:107)SSNâˆ— â‰¤ 2âˆ’2(dâˆ’1)

2dâˆ’1
(cid:88)

k=1

(cid:107)A[Ik](cid:107)op.

Hence, we have

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
T

T
(cid:88)

t=1

Ytâˆ’1 â—¦ Et

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)SSNâˆ—

â‰¤

1
22(dâˆ’1)

2dâˆ’1
(cid:88)

k=1

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
T

T
(cid:88)

t=1

(Ytâˆ’1 â—¦ Et)[Ik]

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)op

.

In other words, the dual norm of the SSN norm can be upper bounded by the sum of the
scaled matrix operator norms of diï¬€erent matricizations of the tensor T âˆ’1 (cid:80)T

Ytâˆ’1 â—¦ Et.

t=1

All of the square matricizations based on Ik lead to a square p-by-p matrix. Therefore,

by the deviation bound in Lemma B.5, we can take a union bound such that

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
T

T
(cid:88)

t=1

Ytâˆ’1 â—¦ Et

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)SSNâˆ—

46

â‰¤

CÎº2M1
2dâˆ’1

(cid:114) p
T

with probability at least 1 âˆ’ exp[âˆ’C(p âˆ’ d)].

Next, for the SN estimator, we can obtain a similar upper bound of the dual norm of the

SN norm. The SN norm is deï¬ned as

(cid:107)A(cid:107)SN =

2d
(cid:88)

i=1

(cid:107)A(i)(cid:107)âˆ—,

and its dual norm has the equivalent form

(cid:107)A(cid:107)SNâˆ— =

inf

(cid:80)2d

i=1

Yi=A

max
i=1,...,2d

(cid:107)(Yi)(i)(cid:107)op.

Then, we can obtain an upper bound,

(cid:107)A(cid:107)SNâˆ— â‰¤

1
(2d)2

2d
(cid:88)

i=1

(cid:107)A(i)(cid:107)op.

Then, for each one-mode matricization, we have the deviation bound. Then, we can take a

union bound such that

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
T

T
(cid:88)

t=1

Ytâˆ’1 â—¦ Et

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)SNâˆ—

â‰¤

CÎº2M1
(2d)2

2d
(cid:88)

i=1

(cid:114) pâˆ’ip
T

,

with probability at least 1 âˆ’ 2d exp[âˆ’Cp].

Finally, the MN estimator uses a special case of square matricization, and the upper

bound for the MN estimator can be obtained by Lemma B.5.

Proof of Lemma B.4. For any M âˆˆ RmÃ—p, denote RT (M ) = (cid:80)T âˆ’1
2. Note that
RT (âˆ†[S2]) â‰¥ ERT (âˆ†[S2]) âˆ’ supâˆ† |RT (âˆ†[S2]) âˆ’ ERT (âˆ†[S2])|. Following the proof of Lemma
B.5, ERT (âˆ†[S2]) = (cid:107)(I T âŠ— âˆ†[S2])P D(cid:107)2

F Â· Î»min(Î£e)Î»min(P P (cid:62)).

t=0 (cid:107)M yt(cid:107)2

F â‰¥ T (cid:107)âˆ†(cid:107)2

Similar to Lemma B.6, for any v âˆˆ Spâˆ’1 and any t > 0,

P[|RT (v(cid:62)) âˆ’ ERT (v(cid:62))| â‰¥ t]
(cid:18)
t2

(cid:18)

â‰¤2 exp

âˆ’ min

max(P P (cid:62))
Considering an (cid:15)-covering net of Spâˆ’1, by Lemma B.7, we can easily construct the union

max(Î£e)Î»2

Îº4T Î»2

,

t
Îº2Î»max(Î£e)Î»max(P P (cid:62))

(cid:19)(cid:19)

.

bound for T (cid:38) p,

P

(cid:20)

sup
vâˆˆSpâˆ’1
(cid:18)

â‰¤C exp

p âˆ’ min

(cid:21)
|RT (v(cid:62)) âˆ’ ERT (v(cid:62))| â‰¥ t

(cid:18)

t2

Îº4T Î»2

max(Î£e)Î»2

max(P P (cid:62))

,

t
Îº2Î»max(Î£e)Î»max(P P (cid:62))

(cid:19)(cid:19)

,

47

Letting t = Î»min(Î£e)Î»min(P P (cid:62))/2, for T (cid:38) M âˆ’2

2 max(Îº4, Îº2)p, we have

P[|RT (v(cid:62)) âˆ’ ERT (v(cid:62))| â‰¥ Î»min(Î£e)Î»min(P P (cid:62))/2] â‰¤ 2 exp(âˆ’CM 2

2 min(Îºâˆ’4, Îºâˆ’2)T ),

where M2 = [Î»min(Î£e)Î»min(P P (cid:62))]/[Î»max(Î£e)Î»max(P P (cid:62))].

Therefore, with probability at least 1 âˆ’ 2 exp(âˆ’CM 2

2 min(Îºâˆ’4, Îºâˆ’2)T ),

RT (âˆ†[S2]) â‰¥

1
2

Î»min(Î£e)Î»min(P P (cid:62))(cid:107)âˆ†(cid:107)2
F.

Finally, since P is related to the VMA(âˆ) process, by the spectral measure of ARMA pro-

cess discussed in Basu and Michailidis (2015), we may replace Î»max(P P (cid:62)) and Î»min(P P (cid:62))

with 1/Âµmin(A) and 1/Âµmax(A), respectively.

B.4 Three Auxiliary Lemmas

Three auxiliary lemmas used in the proofs of Lemmas B.3 and B.4 are presented below.

Lemma B.5 (Deviation bound on diï¬€erent matricizations). For any index set I âŠ‚ {1, 2, . . . , 2d},
denote q = (cid:81)2d

If T (cid:38) (q + q(cid:48)), with probability at least

i=1,iâˆˆI pi and q(cid:48) = (cid:81)2d

i=1,i /âˆˆI pi.

1 âˆ’ exp[âˆ’C(q + q(cid:48))],

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
T

T
(cid:88)

(Ytâˆ’1 â—¦ Et)[I]

t=1

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)op

< CÎº2M1

(cid:112)(q + q(cid:48))/T .

where M1 = Î»max(Î£e)/Âµ1/2

min(A).

Proof. For any index set I âŠ‚ {1, 2, . . . , 2d} and 2dth-mode tensor T, denote the inverse

operation of the multi-mode matricization T = T[I] by T [I] = T. Denote W(r; q, q(cid:48)) =

{W âˆˆ RqÃ—q(cid:48) : rank(W ) = r, (cid:107)W (cid:107)F = 1}.

By deï¬nition, (cid:107)T âˆ’1 (cid:80)T

t=1(Ytâˆ’1 â—¦ Et)[I](cid:107)op = supW âˆˆW(1;q,q(cid:48))(cid:104)T âˆ’1 (cid:80)T

t=1(Ytâˆ’1 â—¦ Et)[I], W (cid:105) =

supW âˆˆW(1;q,q(cid:48))(cid:104)T âˆ’1 (cid:80)T

t=1 vec(Et)vec(Ytâˆ’1)(cid:62), (W [I])(cid:62)

[S1](cid:105).

For an arbitrary matrix W âˆˆ RqÃ—q(cid:48) such that (cid:107)W (cid:107)F = 1, denote M = (W [I])(cid:62)

[S1]. Then,

one can easily check that (cid:104)(Ytâˆ’1 â—¦ Et)[I], W (cid:105) = (cid:104)et, M ytâˆ’1(cid:105).

48

For a ï¬xed M , denote St(M ) = (cid:80)t

s=1(cid:104)es, M ysâˆ’1(cid:105) and Rt(M ) = (cid:80)tâˆ’1

s=0 (cid:107)M ys(cid:107)2

2, for

1 â‰¤ t â‰¤ T . By the standard Chernoï¬€ argument, for any Î± > 0, Î² > 0 and c > 0,

P[{ST (M ) â‰¥ Î±} âˆ© {RT (M ) â‰¤ Î²}]

= inf
m>0

= inf
m>0

â‰¤ inf
m>0

= inf
m>0

â‰¤ inf
m>0

P[{exp(mST (M )) â‰¥ exp(mÎ±)} âˆ© {RT (M ) â‰¤ Î²}]

P[exp(mST (M ))I(RT (M ) â‰¤ Î²) â‰¥ exp(mÎ±)]

exp(âˆ’mÎ±)E[exp(mST (M ))I(RT (M ) â‰¤ Î²)]

exp(âˆ’mÎ± + cm2Î²)E[exp(mST (M ) âˆ’ cm2Î²)I(RT (M ) â‰¤ Î²)]

exp(âˆ’mÎ± + cm2Î²)E[exp(mST (M ) âˆ’ cm2RT (M ))].

By the tower rule, we have

E[exp(mST (M ) âˆ’ cm2RT (M ))]

=E[E[exp(mST (M ) âˆ’ cm2RT (M ))]|FT âˆ’1]

=E[exp(mST âˆ’1(M ) âˆ’ cm2RT âˆ’1(M ))E[exp(m(cid:104)eT , M yT âˆ’1(cid:105) âˆ’ cm2(cid:107)M yT (cid:107)2

2)|FT âˆ’1]].

Since (cid:104)eT , M yT âˆ’1(cid:105) = (cid:104)Î¾T , Î£1/2

e M yT âˆ’1(cid:105), one can easily check that (cid:104)eT , M yT âˆ’1(cid:105) is a

Îº2Î»max(Î£e)(cid:107)M yT âˆ’1(cid:107)2

2-sub-Gaussian random variable. In other words, E[exp(m(cid:104)eT , M yT âˆ’1(cid:105))] â‰¤

exp(m2Îº2Î»max(Î£e)(cid:107)M yT âˆ’1(cid:107)2

2/2). Thus, letting c = ÎºÎ»max(Î£e)/2, we have

E[exp(mST (M ) âˆ’ m2Îº2Î»max(Î£e)RT (M )/2)]

â‰¤E[exp(mST âˆ’1(M ) âˆ’ m2Îº2Î»max(Î£e)RT âˆ’1(M )/2)]

â‰¤ Â· Â· Â· â‰¤ E[exp(mS1(M ) âˆ’ m2Îº2Î»max(Î£e)R1(M )/2)] â‰¤ 1.

Hence, we have that, for any Î± > 0 and Î² > 0,

P[{ST (M ) â‰¥ Î±} âˆ© {RT (M ) â‰¤ Î²}]

â‰¤ inf
m>0

exp(âˆ’mÎ± + m2Îº2Î»max(Î£e)Î²/2)
(cid:18)

(cid:19)

= exp

âˆ’

Î±2
2Îº2Î»max(Î£e)Î²

.

t

Îº2Î»2

max(Î£e)Î»2

max(P P (cid:62))

,

(B.5)

(cid:19)(cid:19)

.

By Lemma B.6, we have that for any t > 0,

P[|RT (M ) âˆ’ ERT (M )| â‰¥ t]
(cid:18)
t2

(cid:18)

â‰¤2 exp

âˆ’ min

Îº4T Î»2

max(Î£e)Î»2

max(P P (cid:62))

49

In addition, ERT (M ) = tr(Î£M ) = (cid:107)(I T âŠ— M )P D(cid:107)2

F â‰¤ T Â· Î»max(Î£e)Î»max(P P (cid:62)). Letting

t = CÎº2T Î»max(Î£e)Î»max(P P (cid:62)), we have

P[RT (M ) â‰¥ CÎº2T Î»max(Î£e)Î»max(P P (cid:62))] â‰¤ 2 exp(âˆ’CT ).

Next, consider a (cid:15)-net W(1; q, q(cid:48)) for W(1; q, q(cid:48)). For any matrix W âˆˆ W(1; q, q(cid:48)), there

exist a matrix W âˆˆ W(1; q, q(cid:48)) such that (cid:107)W âˆ’ W (cid:107)F â‰¤ (cid:15). Since the rank of âˆ† = W âˆ’ W

is at most 2, we can split the SVD of âˆ† into 2 parts, such that âˆ† = âˆ†1 + âˆ†2, where

rank(âˆ†1) = rank(âˆ†2) = 1 and (cid:104)âˆ†1, âˆ†2(cid:105) = 0. Then, for any matrix N âˆˆ RqÃ—q(cid:48), we have

(cid:104)N , W (cid:105) = (cid:104)N , W (cid:105) + (cid:104)N , âˆ†(cid:105) = (cid:104)N , W (cid:105) +

2
(cid:88)

(cid:104)N , âˆ†i/(cid:107)âˆ†i(cid:107)F(cid:105)(cid:107)âˆ†i(cid:107)F,

where âˆ†i/(cid:107)âˆ†i(cid:107)F âˆˆ W(1; q, q(cid:48)). Since (cid:107)âˆ†(cid:107)2

(cid:107)âˆ†1(cid:107)F + (cid:107)âˆ†2(cid:107)F â‰¤

âˆš

2(cid:107)âˆ†(cid:107)F =

âˆš

2(cid:15). Hence, we have

i=1
F + (cid:107)âˆ†2(cid:107)2
F = (cid:107)âˆ†1(cid:107)2

F, by Cauchy inequality,

Î³ :=

In other words,

sup
W âˆˆW(1;q,q(cid:48))

(cid:104)N , W (cid:105) â‰¤ max

(cid:104)N , W (cid:105) +

W âˆˆW(1;q,q(cid:48))

âˆš

2Î³(cid:15).

sup
W âˆˆW(1;q,q(cid:48))

(cid:104)N , W (cid:105) â‰¤ (1 âˆ’

âˆš

2(cid:15))âˆ’1 max

(cid:104)N , W (cid:105).

W âˆˆW(1;q,q(cid:48))

Therefore, we have that, for any x > 0,
(cid:34)

(cid:42)

(cid:43)

(cid:35)

P

sup
W âˆˆW(1;q,q(cid:48))

(cid:34)

â‰¤P

max
W âˆˆW(1;q,q(cid:48))

(Ytâˆ’1 â—¦ Et)[I], W

â‰¥ x

(cid:43)

(Ytâˆ’1 â—¦ Et)[I], W

â‰¥ (1 âˆ’

(cid:35)

âˆš

2(cid:15))x

(B.6)

T
(cid:88)

(Ytâˆ’1 â—¦ Et)[I], W

(cid:43)

âˆš

2(cid:15))x

(cid:35)

.

â‰¥ (1 âˆ’

â‰¤|W(1; q, q(cid:48))| Â· P

Note that by (B.5), for any x > 0,

1
T

(cid:42)

1
T
(cid:34)(cid:42)

T
(cid:88)

t=1
T
(cid:88)

t=1

1
T

t=1

(cid:43)

(cid:34)(cid:42)

P

1
T

T
(cid:88)

t=1

(Ytâˆ’1 â—¦ Et)[I], W

â‰¥ (1 âˆ’

(cid:35)

âˆš

2(cid:15))x

â‰¤P[{ST (M ) â‰¥ T (1 âˆ’

âˆš

2(cid:15))x} âˆ© {RT (M ) â‰¤ CÎº2T Î»max(Î£e)Î»max(P P (cid:62))}]

+P[RT (M ) > CÎº2T Î»max(Î£e)Î»max(P P (cid:62))]
CT x2

(cid:20)

(cid:21)

â‰¤ exp

âˆ’

Îº4Î»2

max(Î£e)Î»max(P P (cid:62))

+ 2 exp(âˆ’CT ).

50

By Lemma 3.1 in Candes and Plan (2011), for a (cid:15)-net for W(1; q, q(cid:48)), the covering number

|W(1; q, q(cid:48))| â‰¤ (9/(cid:15))q+q(cid:48). Combining (B.6), we have that, when T (cid:38) q + q(cid:48), for any x > 0,

(cid:34)

P

sup
W âˆˆW(1;q,q(cid:48))
(cid:20)

T
(cid:88)

(Ytâˆ’1 â—¦ Et)[I], W

(cid:43)

(cid:35)

â‰¥ x

(cid:42)

1
T

t=1

â‰¤ exp

(q + q(cid:48)) log(9/(cid:15)) âˆ’

CT x2

(cid:21)

Îº4Î»2

max(Î£e)Î»max(P P (cid:62))

+ 2 exp[(q + q(cid:48)) log(9/(cid:15)) âˆ’ CT ].

Taking (cid:15) = 0.1 and x = CÎº2Î»max(Î£e)Î»1/2

max(P P (cid:62)) Â· (cid:112)(q + q(cid:48))/T , we have

(cid:34)

P

sup
W âˆˆW(1;q,q(cid:48))

(cid:42)

1
T

T
(cid:88)

t=1

(cid:43)

(Ytâˆ’1 â—¦ Et)[I], W

â‰¤ exp[âˆ’C(q + q(cid:48))].

â‰¥ CÎº2Î»max(Î£e)Î»1/2

max(P P (cid:62))

(cid:114) q + q(cid:48)
T

(cid:35)

Finally, since P is related to the VMA(âˆ) process, by the spectral measure of ARMA process

discussed in Basu and Michailidis (2015), we may replace Î»max(P P (cid:62)) with 1/Âµmin(A).

Lemma B.6. For any M âˆˆ RpÃ—p such that (cid:107)M (cid:107)F = 1, denote RT (M ) = (cid:80)T âˆ’1

t=0 (cid:107)M yt(cid:107)2
2.

Then, for any t > 0,

P[|RT (M ) âˆ’ ERT (M )| â‰¥ t]
(cid:18)
t2

(cid:18)

â‰¤2 exp

âˆ’ min

Îº4T Î»2

max(Î£e)Î»2

max(P P (cid:62))

,

t

Îº2Î»2

max(Î£e)Î»2

max(P P (cid:62))

where P is deï¬ned as

P =

ï£®

ï£¯
ï£¯
ï£¯
ï£¯
ï£¯
ï£¯
ï£¯
ï£°

I p A A2 A3

. . . AT âˆ’1

O I p A A2
...
...
...

...

. . . AT âˆ’2
. . .

...

O O O O . . .

I p

ï£¹

ï£º
ï£º
ï£º
ï£º
ï£º
ï£º
ï£º
ï£»

. . .

. . .

. . .

. . .

.

(cid:19)(cid:19)

,

(B.7)

Proof. Denote by y = (y(cid:62)

T âˆ’1, y(cid:62)

T âˆ’2, . . . , y(cid:62)

0 )(cid:62), e = (e(cid:62)

T âˆ’1, e(cid:62)

T âˆ’2, . . . , e(cid:62)

0 , . . . )(cid:62), and Î¾ =

(Î¾(cid:62)

T âˆ’1, Î¾(cid:62)

T âˆ’2, . . . , Î¾(cid:62)

0 , . . . )(cid:62). Based on the moving average representation of VAR(1), we can
rewrite yt to a VMA(âˆ), yt = et + Aetâˆ’1 + A2etâˆ’2 + A3etâˆ’2 + Â· Â· Â· . Note that RT (M ) =
y(cid:62)(I T âŠ— M (cid:62)M )y = e(cid:62)P (cid:62)(I T âŠ— M (cid:62)M )P e = Î¾(cid:62)DP (cid:62)(I T âŠ— M (cid:62)M )P DÎ¾ := Î¾(cid:62)Î£M Î¾,

51

where P is deï¬ned in (B.7) and

D =

ï£®

ï£¯
ï£¯
ï£¯
ï£¯
ï£¯
ï£¯
ï£¯
ï£°

Î£1/2
e

O

O . . .

O Î£1/2

e

O . . .

O
...

e

O Î£1/2
...
...

. . .
. . .

ï£¹

ï£º
ï£º
ï£º
ï£º
ï£º
ï£º
ï£º
ï£»

.

By Hanson-Wright inequality, for any t > 0,

P[|RT (M ) âˆ’ ERT (M )| â‰¥ t] â‰¤ 2 exp

(cid:18)

(cid:18)

âˆ’ min

t2
Îº4(cid:107)Î£M (cid:107)2
F

,

t
Îº2(cid:107)Î£M (cid:107)op

(cid:19)(cid:19)

.

As (cid:107)M (cid:107)F = 1, by the submultiplicative property of the Frobenius norm and operator

norm, we have (cid:107)Î£M (cid:107)2

F â‰¤ T Â· Î»2

max(Î£e)Î»2

max(P P (cid:62)) and (cid:107)Î£M (cid:107)op â‰¤ Î»max(Î£e)Î»max(P P (cid:62)).

These imply that, for any t > 0,

P[|RT (M ) âˆ’ ERT (M )| â‰¥ t]
(cid:18)
t2

(cid:18)

â‰¤2 exp

âˆ’ min

Îº4T Î»2

max(Î£e)Î»2

max(P P (cid:62))

,

t
Îº2Î»max(Î£e)Î»max(P P (cid:62))

(cid:19)(cid:19)

.

The proof of this lemma is accomplished.

Lemma B.7. (Covering number of unit sphere) Let N be an Îµ-net of the unit sphere Spâˆ’1,

where Îµ âˆˆ (0, 1]. Then,

|N | â‰¤

(cid:19)p

.

(cid:18) 3
Îµ

Proof. This lemma follows directly from Corollary 4.2.13 of Vershynin (2018).

52

References

Bai, J. and Wang, P. (2016). Econometric analysis of large factor models. Annual Review

of Economics, 8:53â€“80.

Basu, S. and Michailidis, G. (2015). Regularized estimation in sparse high-dimensional time

series models. Annals of Statistics, 43:1535â€“1567.

Bi, X., Qu, A., and Shen, X. (2018). Multilayer tensor factorization with applications to

recommender systems. Annals of Statistics, 46:3308â€“3333.

Boyd, S., Parikh, N., Chu, E., Peleato, B., Eckstein, J., et al. (2011). Distributed op-

timization and statistical learning via the alternating direction method of multipliers.

Foundations and TrendsÂ® in Machine learning, 3:1â€“122.

Candes, E. J. and Plan, Y. (2011). Tight oracle inequalities for low-rank matrix recovery from

a minimal number of noisy random measurements. IEEE Transactions on Information

Theory, 57:2342â€“2359.

Chen, E. Y. and Chen, R. (2019). Modeling dynamic transport network with matrix factor

models: with an application to international trade ï¬‚ow. arXiv:1901.00769 [econ.EM].

Chen, E. Y., Tsay, R. S., and Chen, R. (2020a). Constrained factor models for high-

dimensional matrix-variate time series. Journal of the American Statitical Association,

115:775â€“793.

Chen, H., Raskutti, G., and Yuan, M. (2019). Non-convex projected gradient descent for gen-

eralized low-rank tensor regression. The Journal of Machine Learning Research, 20(1):172â€“

208.

Chen, R., Xiao, H., and Yang, D. (2020b). Autoregressive models for matrix-valued time

series. Journal of Econometrics. To appear.

53

Chen, R., Yang, D., and Zhang, C.-H. (2020c). Factor models for high-dimensional tensor

time series. arXiv:1905.07530v2 [stat.ME].

Davis, R. A., Zang, P., and Zheng, T. (2016). Sparse vector autoregressive modeling. Journal

of Computational and Graphical Statistics, 25:1077â€“1096.

De Lathauwer, L., De Moor, B., and Vandewalle, J. (2000). A multilinear singular value

decomposition. SIAM Journal on Matrix Analysis and Applications, 21:1253â€“1278.

Ding, S. and Cook, R. D. (2018). Matrix variate regressions and envelope models. Journal

of the Royal Statistical Society: Series B, 80:387â€“408.

Fama, E. F. and French, K. R. (2015). A ï¬ve-factor asset pricing model. Journal of Financial

Economics, 116:1â€“22.

Frandsen, A. and Ge, R. (2019). Understanding composition of word embeddings via tensor

decomposition. In International Conference on Learning Representations.

French, K. R. (2020). Data library: U.S. research returns data. Available at http://mba.

tuck.dartmouth.edu/pages/faculty/ken.french/data_library.html.

Gandy, S., Recht, B., and Yamada, I. (2011). Tensor completion and low-n-rank tensor

recovery via convex optimization. Inverse Problems, 27:025010.

Han, F., Lu, H., and Liu, H. (2015a). A direct estimation of high dimensional stationary

vector autoregressions. Journal of Machine Learning Research, 16:3115â€“3150.

Han, F., Xu, S., and Liu, H. (2015b). Rate-optimal estimation of a high-dimensional semi-

parametric time series model. Preprint.

Han, R., Willett, R., and Zhang, A. (2020). An optimal statistical and computational

framework for generalized tensor estimation. arXiv preprint arXiv:2002.11255.

Hoï¬€, P. D. (2015). Multilinear tensor regression for longitudinal relational data. Annals of

Applied Statistics, 9:1169â€“1193.

54

Kolda, T. G. and Bader, B. W. (2009). Tensor decompositions and applications. SIAM

Review, 51:455â€“500.

Lam, C., Yao, Q., et al. (2012). Factor modeling for high-dimensional time series: inference

for the number of factors. Annals of Statistics, 40:694â€“726.

Li, X., Xu, D., Zhou, H., and Li, L. (2018). Tucker tensor regression and neuroimaging

analysis. Statistics in Biosciences, 10:520â€“545.

Liu, J., Musialski, P., Wonka, P., and Ye, J. (2013). Tensor completion for estimating missing

values in visual data. IEEE Transactions on Pattern Analysis and Machine Intelligence,

35:208â€“220.

Mirsky, L. (1960). Symmetric gauge functions and unitarily invariant norms. Quarterly

Journal of Mathematics, 11:50â€“59.

Mu, C., Huang, B., Wright, J., and Goldfarb, D. (2014). Square deal: Lower bounds and im-

proved relaxations for tensor recovery. In International Conference on Machine Learning,

pages 73â€“81.

Negahban, S. and Wainwright, M. J. (2011). Estimation of (near) low-rank matrices with

noise and high-dimensional scaling. Annals of Statistics, 39:1069â€“1097.

Negahban, S. and Wainwright, M. J. (2012). Restricted strong convexity and weighted

matrix completion: Optimal bounds with noise. Journal of Machine Learning Research,

13:1665â€“1697.

Negahban, S. N., Ravikumar, P., Wainwright, M. J., Yu, B., et al. (2012). A uniï¬ed frame-

work for high-dimensional analysis of M -estimators with decomposable regularizers. Sta-

tistical Science, 27:538â€“557.

Raskutti, G., Yuan, M., and Chen, H. (2019). Convex regularization for high-dimensional

multi-response tensor regression. Annals of Statistics, 47:1554â€“1584.

55

Shapiro, A. (1986). Asymptotic theory of overparameterized structural models. Journal of

the American Statistical Association, 81:142â€“149.

Stock, J. H. and Watson, M. W. (2011). Dynamic factor models.

In Clements, M. P.

and Hendry, D. F., editors, Oxford Handbook of Economic Forecasting. Oxford University

Press.

Tomioka, R., Suzuki, T., Hayashi, K., and Kashima, H. (2011). Statistical performance

of convex tensor decomposition. In Advances in Neural Information Processing Systems

(NIPS), pages 972â€“980.

Tucker, L. R. (1966). Some mathematical notes on three-mode factor analysis. Psychome-

trika, 31:279â€“311.

Vershynin, R. (2018). High-Dimensional Probability: An Introduction with Applications in

Data Science. Cambridge University Press, Cambridge.

Walden, A. and Serroukh, A. (2002). Wavelet analysis of matrixâ€“valued timeâ€“series. Proceed-

ings of the Royal Society of London. Series A: Mathematical, Physical and Engineering

Sciences, 458:157â€“179.

Wang, D., Liu, X., and Chen, R. (2019). Factor models for matrix-valued high-dimensional

time series. Journal of Econometrics, 208:231â€“248.

Wang, D., Zheng, Yao Lian, H., , and Li, G. (2020). High-dimensional vector autoregres-

sive time series modeling via tensor decomposition. Journal of the American Statistical

Association. To appear.

Zheng, Y. and Cheng, G. (2020). Finite time analysis of vector autoregressive models under

linear restrictions. Biometrika. To appear.

Zhou, H., Li, L., and Zhu, H. (2013). Tensor regression with applications in neuroimaging

data analysis. Journal of the American Statistical Association, 108:540â€“552.

56

Figure 2: Average estimation error of LTR, OLS and RRR estimators for data generated

with diï¬€erent d, piâ€™s and multilinear ranks.

57

Case (1a)Case (1b)Case (2a)Case (2b)0.00.20.40.60.810001250150017502000r = (1,1,1,1)012310001250150017502000r = (1,1,1,1)0123410001250150017502000r = (1,1,1,1,1,1)051010001250150017502000r = (1,1,1,1,1,1)0.00.20.40.60.810001250150017502000r = (2,2,2,2)012310001250150017502000r = (2,2,2,2)0123410001250150017502000r = (2,2,2,1,1,1)051010001250150017502000r = (2,2,2,1,1,1)0.00.20.40.60.810001250150017502000r = (2,3,2,3)012310001250150017502000r = (2,3,2,3)0123410001250150017502000r = (2,2,2,2,2,2)051010001250150017502000r = (2,2,2,2,2,2)EstimatorLTRRRROLSCase d

Fixed Parameter

Varying Parameter

(a)

(b)

(c)

(d)

(e)

(f)

(g)

2

2

2

2

3

3

3

T = 500, r = (2, 2, 2, 2)

p1 = p2 = 5, 7, 9, 10, 11

p1 = p2 = 8, r = (2, 2, 2, 2)

T = 200, 400, 600, 800, 1000

p1 = p2 = 8, T = 500

r = (1, 1, 1, 1), (1, 2, 1, 2),

(2, 2, 2, 2, 2), (2, 3, 2, 3), (3, 3, 3, 3)

p = 144, T = 1000, r = (1, 1, 1, 1)

p1 = 3, 4, 6, 8, 12

T = 1000, r = (2, 2, 2, 2, 2, 2)

p = (4, 4, 4), (4, 4, 5), (4, 5, 5), (5, 5, 5), (5, 5, 6)

p = (5, 5, 5), r = (2, 2, 2, 2, 2, 2)

T = 600, 800, 1000, 1200, 1400

p = (5, 5, 5), T = 1000

r = (1, 1, 1, 1, 1, 1), (1, 1, 2, 1, 1, 2)

(1, 2, 2, 1, 2, 2), (2, 2, 2, 2, 2, 2), (2, 2, 3, 2, 2, 3)

(h)

3

p = 144, T = 1000,

r = (1, 1, 1, 1, 1, 1)

p = (2, 2, 36), (3, 3, 16),

(4, 4, 9), (3, 4, 12), (4, 6, 6)

Table 2: Parameter setting for eight cases with diï¬€erent varying parameters, where p =

(p1, . . . , pd) and r = (r1, . . . , r2d).

Figure 3: Average squared estimation error for the SSN estimator for eight cases with dif-

ferent varying parameters. The error bars in each panel represent Â± one standard deviation.

58

0.51.01.52.0255075100125pCase (a)120.0010.0020.0030.0040.0051TCase (b)0.51.01.52.02.55.07.5sCase (c)0.00.20.40.62.55.07.510.012.5p1Case (d)0.81.21.62.06080100120140pCase (e)1.52.02.53.00.00090.00120.00151TCase (f)1232.55.07.510.012.5sCase (g)0.00.20.40.612345Five cases for piCase (h)Figure 4: Average estimation error for TSSN, SSN, MN, and SN estimators for data gener-

ated with diï¬€erent d, piâ€™s and multilinear ranks.

59

Case (3a)Case (3b)Case (4a)Case (4b)0.000.250.500.751.004006008001000r = (1,1,1,1)01234006008001000r = (1,1,1,1)0.00.51.01.52.02.560080010001200r = (1,1,1,1,1,1)024660080010001200r = (1,1,1,1,1,1)0.000.250.500.751.004006008001000r = (2,2,1,1)01234006008001000r = (2,2,1,1)0.00.51.01.52.02.560080010001200r = (2,2,2,1,1,1)024660080010001200r = (2,2,2,1,1,1)0.000.250.500.751.004006008001000r = (2,2,2,2)01234006008001000r = (2,2,2,2)0.00.51.01.52.02.560080010001200r = (2,2,2,2,2,2)024660080010001200r = (2,2,2,2,2,2)EstimatorTSSNSSNMNSNFigure 5: TSSN estimates of predictor and response factor matrices for 10Ã—10 matrix-valued

portfolio return series. From left to right: (cid:101)U 1, (cid:101)U 2, (cid:101)U 3 and (cid:101)U 4.

Model

VAR VFM MAR MFM

LRTAR

SSN TSSN

Best Worst

In-sample

Out-of-sample

(cid:96)2 norm 31.61 35.85

34.12

35.86

33.18

33.38

VAR MFM

(cid:96)0 norm

8.09

9.23

9.12

9.24

8.84

8.89

VAR MFM

(cid:96)2 norm 39.84

39.11

35.26

38.53

32.60 31.47

TSSN VAR

(cid:96)âˆ norm 11.98

12.30

11.47

11.00

9.53

9.33

TSSN VFM

Table 3: Average in-sample forecasting error and out-of-sample rolling forecasting error for

10 Ã— 10 matrix-valued portfolio return series. The best cases are marked in bold.

60

0.60.40.20-0.2-0.4-0.6B/M Response Factor LoadingB/M Predictor Factor LoadingSize Predictor Factor LoadingSize Response Factor LoadingLegendFigure 6: TSSN estimates of predictor and response factor matrices for 4Ã—4Ã—2 tensor-valued

portfolio return series. From left to right: (cid:101)U 1, (cid:101)U 2, (cid:101)U 3, (cid:101)U 4, (cid:101)U 5 and (cid:101)U 6.

Model

VAR VFM MTAR TFM

LRTAR

SSN TSSN

Best Worst

OP-BM-Size 4 Ã— 4 Ã— 2 series

In-sample

Out-of-sample

In-sample

Out-of-sample

(cid:96)2 norm 19.53 20.08

19.89

20.09

19.69

19.70

VAR TFM

(cid:96)0 norm

7.67

7.91

7.85

7.92

7.76

7.77

VAR TFM

(cid:96)2 norm 22.27

20.17

20.50

20.11

20.32 19.95

TSSN VAR

(cid:96)âˆ norm 10.38

10.04

9.86

10.03

9.29

9.35

SSN

VAR

Inv-BM-Size 4 Ã— 4 Ã— 2 series

(cid:96)2 norm 16.80 17.10

17.05

17.11

16.86

16.88

VAR TFM

(cid:96)0 norm

6.25

6.40

6.38

6.41

6.31

6.32

VAR TFM

(cid:96)2 norm 18.70

17.70

16.89

17.67

16.11

16.29

SSN

VAR

(cid:96)âˆ norm

7.42

7.37

6.79

7.33

6.62

6.43

TSSN VAR

Table 4: Average in-sample forecasting error and out-of-sample rolling forecasting error for

4 Ã— 4 Ã— 2 tensor-valued portfolio return series. The best cases are marked in bold.

61

0.80.60.40.20-0.2-0.4-0.6-0.8Inv Response Factor LoadingB/M Response Factor LoadingSize Response Factor LoadingLegendOP Predictor Factor LoadingB/M Predictor Factor LoadingSize Predictor Factor LoadingOP Response Factor LoadingB/M Response Factor LoadingSize Response Factor LoadingInv-BM-SizeInv Predictor Factor LoadingB/M Predictor Factor LoadingSize Predictor Factor LoadingOP-BM-Size