MilliSonic: Pushing the Limits of Acoustic Motion Tracking

Anran Wang and Shyamnath Gollakota
University of Washington
{anranw,gshyam}@cs.washington.edu

9
1
0
2

n
a
J

0
2

]

C
H
.
s
c
[

1
v
1
0
6
6
0
.
1
0
9
1
:
v
i
X
r
a

ABSTRACT
Recent years have seen interest in device tracking and lo-
calization using acoustic signals. State-of-the-art acoustic
motion tracking systems however do not achieve millimeter
accuracy and require large separation between microphones
and speakers, and as a result, do not meet the requirements
for many VR/AR applications. Further, tracking multiple
concurrent acoustic transmissions from VR devices today
requires sacrificing accuracy or frame rate. We present Mil-
liSonic, a novel system that pushes the limits of acoustic
based motion tracking. Our core contribution is a novel local-
ization algorithm that can provably achieve sub-millimeter
1D tracking accuracy in the presence of multipath, while
using only a single beacon with a small 4-microphone array.
Further, MilliSonic enables concurrent tracking of upto four
smartphones without reducing frame rate or accuracy. Our
evaluation shows that MilliSonic achieves 0.7mm median 1D
accuracy and a 2.6mm median 3D accuracy for smartphones,
which is 5x more accurate than state-of-the-art systems. Mil-
liSonic enables two previously infeasible interaction applica-
tions: a) 3D tracking of VR headsets using the smartphone
as a beacon and b) fine-grained 3D tracking for the Google
Cardboard VR system using a small microphone array.

CCS CONCEPTS
• Human-centered computing → Pointing; Ubiquitous
and mobile computing systems and tools;

ACM Reference Format:
Anran Wang and Shyamnath Gollakota. 2019. MilliSonic: Pushing
the Limits of Acoustic Motion Tracking. In CHI Conference on Hu-
man Factors in Computing Systems Proceedings (CHI 2019), May 4–9,
2019, Glasgow, Scotland Uk. ACM, New York, NY, USA, 11 pages.
https://doi.org/10.1145/3290605.3300248

1 INTRODUCTION
Device localization and motion tracking has been a long-
standing challenge in the research community. It is a key
component in Virtual Reality and Augmented/Mixed Reality
applications and enables novel human-computer interactions
including gesture and skeletal tracking. Traditionally, spe-
cialized optical methods such as lasers and infrared beacons
have been used to localize VR headsets and controllers. This

CHI 2019, May 4–9, 2019, Glasgow, Scotland Uk
2019. ACM ISBN 978-1-4503-5970-2/19/05. . . $15.00
https://doi.org/10.1145/3290605.3300248

1

includes commercial systems like the HTC Vive VR, Ocu-
lus Rift and Sony PlayStation VR [3, 9, 14]. These optical
tracking solutions, however, require separate expensive bea-
cons to emit infrared signals and transceivers to receive and
process data. Existing devices like smartphones lack these
transceivers and hence are unsuitable for such techniques.
Acoustic-based localization and tracking methods have
recently emerged as an attractive alternative to optical sys-
tems [26, 35]. Speakers and microphones, used for emitting
and receiving acoustic signals, are cheap and easy to con-
figure. Furthermore, commodity smartphones and smart
watches already have built-in speakers and microphones,
which makes acoustic tracking an excellent fit for such de-
vices. As shown in Fig. 1(a), a simple microphone array could
act as a beacon to enable 3D location tracking for the Google
cardboard VR system. Conversely, instead of carrying around
additional devices (e.g., HTC IR beacons) to enable tracking
for VR headsets, one could reuse existing smartphones as
beacons to enable 3D localization and motion tracking.

State-of-the-art acoustic motion tracking systems [21, 34]
however do not adequately meet the requirements of VR/AR
applications for three main reasons.
• Tracking accuracy. Acoustic signals suffer from multi-path
where the signal reflects off nearby surfaces before arriving
at the receiver. Thus, existing 1D acoustic tracking accuracy
is 5-10 mm [21], which is much worse than optical systems
and may cause motion sickness with prolonged use [15].
• Microphone/speaker separation. 3D tracking requires trian-
gulation from multiple microphones/speakers, which when
placed close to each other limits accuracy. Prior work that
tracks smartphones uses multiple speakers separated by
90 cm [21], making them difficult to integrate into VR/AR
headsets. Conversely, using a 90 cm beacon for Google card-
board VR is unwieldy and limits portability.
• Concurrency. Tracking multiple headsets remains a chal-
lenge with existing designs. A naïve approach is to time mul-
tiplex the acoustic signals from each device. This however
reduces the frame rate linearly with the number of devices.

We present MilliSonic, a novel system that pushes the
limits of acoustic based motion tracking. Our core contri-
bution is a novel localization algorithm that can achieve
sub-millimeter 1D tracking accuracy in the presence of mul-
tipath, while using only a single beacon with a 4-microphone
array. To achieve this, like prior designs [16, 21], MilliSonic

 
 
 
 
 
 
CHI 2019, May 4–9, 2019, Glasgow, Scotland Uk

Anran Wang and Shyamnath Gollakota

Figure 1: Application scenarios: a) tracking a Google cardboard VR using a small microphone array; b) Tracking the 3D position
of VR/AR headsets using a smartphone as a beacon. Using the transmissions from the smartwatch to then track it w.r.t. the
headset; c) Concurrently tracking multiple devices with a single microphone array at a high per-device frame rate.

uses FMCW (frequency modulated continuous wave) acous-
tic transmissions where the frequency linearly increases
with time. Prior designs use FMCW to separate reflections
arriving at different times by mapping time differences to
frequency shifts. However, given the limited inaudible band-
width on smartphones, the ability to differentiate between
close-by paths using frequency shifts is limited, thus, limiting
accuracy. Our algorithm instead leverages the phase of the
FMCW reflections to perform tracking. We prove that this
allows us to achieve sub-millimeter 1D tracking. These high
1D accuracies allow us to reduce the separation between mi-
crophones at the beacon and achieve millimeter-resolution
3D tracking and localization. Finally, we show that by have
devices intentionally introduce different time delays to their
FMCW signals, we can support concurrent acoustic transmis-
sions from multiple devices, without reducing the accuracy
or frame rate for each device.

We implement our design using speakers on Android
smartphones including Samsung Galaxy S6, S7 and S9. We
design 15cm × 15cm and 6cm × 5.35cm 4-microphone arrays
using commercial microphones and implement our real-time
tracking algorithms on a Raspberry Pi 3 Model B+ [12].

This paper makes the following contributions.

• We show for the first time how to achieve sub-mm 1D
tracking and localization accuracies using acoustic signals
on smartphones, in the presence of multipath. To achieve
this, we introduce algorithms that use the phase of FMCW
signals to disambiguate between multiple paths.
• We enable multiple smartphones to transmit concurrently
using time-shifted FMCW acoustic signals and enable con-
current tracking without sacrificing accuracy or frame rate.
• We present experimental results that show that MilliSonic
can achieve a median 1D accuracy of 0.7 mm up to distances
of 1 m from the smartphone. The median 1D accuracy is
1.7 mm for distances between 1 and 2 m. MilliSonic’s median
3d accuracy is around 2.6 mm. Further, we can concurrently

track up to four smartphones at a per-device frame rate of
40 frames/sec without sacrificing accuracy.
• Finally, we describe the limitations of our system and
outline additional work required to more comprehensively
evaluate the system in various use case scenarios.

2 APPLICATION SCENARIOS
MilliSonic enables three key application scenarios.
• Current smartphone-based VR headsets (e.g., Google Card-
board) do not have 6DoF motion tracking capability. This is
because of the lack of optical transceivers, which limits their
usage. MilliSonic enables 6DoF motion tracking capability
for smartphone-based VR headsets using only a cheap and
small microphone array as a beacon, without requiring any
hardware modifications at the smartphone.
• Millisonic can transform the smartphone into a portable
beacon for VR tracking. Specifically, instead of requiring the
user to carry optical beacons for VR headsets to enable use
in different environments, a smartphone can be used as a
portable beacon. To do this, manufacturer can integrate a
cheap microphone array into the VR/AR headset. Using this
microphone array, the VR headset can also track the motion
of other acoustic-enabled devices such as smart watches.
• MilliSonic can support concurrent tracking of an unlim-
ited number of microphone arrays (i.e., VR headsets) in the
vicinity of a single speaker (i.e., a smartphone). Furthermore,
it can also support up to four speakers (i.e., smartphone
VR headsets) in the vicinity of a microphone array without
sacrificing accuracy or frame rate.

3 MILLISONIC DESIGN
We first present background on existing FMCW tracking sys-
tems and show why they have a limited accuracy for acoustic
tracking. We then present our algorithm that uses the FMCW
phase to achieve sub-mm 1D tracking. We then describe how

2

a)MicSpeakerb)c)MilliSonic: Pushing the Limits of Acoustic Motion Tracking

CHI 2019, May 4–9, 2019, Glasgow, Scotland Uk

to perform 3D tracking using the 1D locations using multiple
microphones. Finally, we address various practical issues.

FMCW Background
Acoustic tracking is traditionally achieved by computing the
time-of-arrival of the transmitted signals at the microphones.
At its simplest, the transmitted signal is a sine wave, x(t) =
exp(−j2π f t) where f is the wave frequency. A microphone
at a distance of d at the transmitter, has a time-of-arrival of
d = td × c where c is the speed of sound. The received signal
at this distance can now be written as, y(t) = exp(−j2πt(t −
td ). Dividing by x(t), we get ˆy(t) = exp(j2π f td ). Thus, the
phase of the received signal can be used to compute the time-
of-arrival, td . In practice, however, multipath significantly
distorts the received phase limiting accuracy.

To combat multipath, prior work [21, 23] uses Frequency
Modulated Continuous Wave (FMCW) chirps where as shown
in Fig. 2 the frequency of the signal changes linearly with
time. FMCW has good autocorrelation properties that al-
low the receiver to differentiate between multiple paths that
each have a different time-of-arrival. Further compared to
OFDM [24] and other waveforms [30], FMCW has high spec-
tral efficiency and is ease of demodulate. Mathematically,
2T t)t) =
the FMCW signal in Fig. 2 is, x(t) = exp(−j2π (f0 + B
2T t 2)), where f0, B and T are the initial fre-
exp(−j2π (f0t + B
quency, bandwidth and duration of the FMCW chirp respec-
tively. In the presence of multipath, the received signal can
2T (t 2 +
be written as, y(t) = (cid:205)M
i − 2tti ))), where Ai and ti = di (t )
t 2
are the attenuation and
time-of-flight of the ith path. Dividing this by x(t) we get,

i=1 Aiexp(−j2π (f0(t − ti ) + B

c

ˆy(t) =

M
(cid:213)

i=1

Aiexp(−j2π (

B
T

tit + f0ti −

B
2T

2
i ))

t

(1)

B

The above equation shows that multipath with different
times-of-arrival fall into different frequencies. The receiver
uses Discrete Fourier Transform (DFT) to find the first peak
frequency bin, fpeak , that corresponds to the line-of-sight
path to the transmitter. It then computes the distance to the
receiver as, d(t) = cfpe ak

.

While this conventional FMCW processing is effective in
disambiguating multiple paths that is separated by large dis-
tances, it has a limited accuracy when the multiple paths are
close to each other. Specifically, the minimum distance reso-
lution for FMCW is in the order of c
when the separation
B
between frequencies is 1 Hz. Given that smartphones have
a limited inaudible bandwidth of 7 kHz between 17-24 kHz,
prior work cannot distinguish between paths that are close
to the direct line-of-sight path and hence have a limited ac-
curacy [21, 26]. Further, since DFT operations are performed
over a whole chirp duration, it limits the frame rate of the
system to 1
T

, where T is the FMCW chirp duration.

Figure 2: FMCW signal structure.

Sub-mm 1D tracking using FMCW phase
We use phase of the FMCW signals to compute distance.
Thus, instead of using the first peak frequency of the FMCW
signal in the frequency domain to estimate the time-of-arrival,
our algorithms has two key steps: 1) we apply a dynamic
narrow band-pass filter in the time-domain to filter out most
multipath that has a distant time-of-arrival from the direct
path. This leaves us only a small portion of residual indirect
paths around the direct path. 2) We then extract the distance
information from the instantaneous FMCW phase.

Intuition. We provide the intuition for why FMCW phase
provides a better accuracy than existing FMCW approaches.
Traditional FMCW approaches. Let us first understand the
error in traditional peak estimation method for FMCW sig-
nals. Tracking error occurs when we have two paths that
are without a single frequency bin. Let us denote the time-
of-arrival of the direct path as t1 and its frequency in the
demodulated FMCW signal as ft1. An indirect path with
a time-of-arrival of t2 lies at frequency ft2 in the demod-
ulated signal. When |ft1 − ft2 | < 1, the two peaks merge
together in the frequency domain resulting in a single peak
at approximately (A2ft2 + A1ft1)/(A2 + A1), where A1 and
A2 are the amplitude of the direct path and the total am-
plitude of the residual indirect paths. Hence, the frequency
error is (A2ft2 + A1ft1)/(A2 + A1) − ft1 which is equivalent
=
to a distance error given by, d
(ft2 − ft1)/(1 + A1
. This error increases linearly with ft2 − ft1
) c
B
A2
and proportionally increases with A2
A1 .

= ( A2ft2 +A1ft1
A2+A1

Our method. In contrast, the error in the phase of the
FMCW signal is significantly smaller. To see this, let us as-
sume that the amplitude of the residual indirect paths after
filters have a lower amplitude than the direct path. As shown
in Fig. 3, the complex representation of the direct path is
represented by the blue vector while that of the sum of indi-
rect paths is represented by the red vector. The sum of the
two vectors is the resulting signal at the receiver which is
represented by the green vector. The maximum phase error
occurs when the red vector is perpendicular to the green
vector and this corresponds to a phase error of sin−1( A2
). The
A1
key observation is that this error does not depend on ft2 − ft1
and increases much slower at sin−1( A2
A1

Fig. 4 shows that distance error as a function of A2/A1 and
|ft2 − ft1 | for both traditional peak estimation techniques as

− ft1) c
B

(peak )
e

).

3

frequencytimef0B+f0TCHI 2019, May 4–9, 2019, Glasgow, Scotland Uk

Anran Wang and Shyamnath Gollakota

Figure 3: Phase error when
one indirect path (red vector)
is combined with the direct
path (blue vector).

Figure 4: Error comparison
of FMCW peak versus our
FMCW phase method.

well as our FMCW phase method. The plots show that the
distance errors using peak estimation is severely affected
by the time-of-flight of the indirect paths. In contrast, the
distance error using our FMCW phase technique is around
10x lower. We describe our algorithm and its theoretical
analysis in more detail in the Appendix.

3D tracking from 1D locations
The above FMCW phase technique allows us to achieve sub-
mm resolution in estimating 1D distances. To achieve 3D
tracking we use information from multiple microphones to
perform triangulation. We use multiple microphones instead
of speakers to reduce the power consumption as well as to
eliminate the complexity of multiplexing the multiple speak-
ers. Since our 1D resolution is high we can also reduce the
separation between the microphones while achieving a good
3D accuracy. Specifically, we place four microphones at four
corners of a rectangle. We have two pairs of microphones in
the vertical position and the other two pairs in the horizon-
tal position. Thus, by computing the intersection of all the
resulting 1D positions, we can compute the 3D location.

We note that the accuracy of triangulation is dependent
on the distance from the microphone array as well as the
separation between the microphones. Specifically, as the dis-
tance from the microphone array increases, the resulting 3D
accuracy become worse. Similarly, as the separation between
microphones increase the 3D accuracy improves, which is
why prior work uses a microphone separation of 90 cm [21].
In our solution, since we already achieve sub-mm 1D resolu-
tion, we can reduce the separation between microphones to
fit the form-factor of VR headset and still achieve good 3D
tracking accuracies upto 2 m. To improve the accuracy and
reduce jitters at larger distances, we average the 3D distance
measurements within each 10ms duration. Incorporating the
15ms latency of the band-pass filter, we get one distance
value every 25ms or a frame rate of 40 frames per second.

Addressing practical issues
We describe the practical issues in designing MilliSonic.

1) Phase ambiguity. Any phase tracking algorithm has to
address the problem of phase ambiguity. Specifically, we can

Figure 5: Supporting concurrent transmissions using virtual
time-of-arrival offsets at each VR headset.
only extract the phase modulo 2π from the demodulated
chirp ( ˆϕ(t) = ϕ(t) mod 2π ). This leads to two problems: a)
how to detect any modulo 2π shifts during a single chirp; and
b) how to estimate the initial 2N π phase offset, i.e., ϕ(0) =
2N π + ˆϕ(0) at the beginning of each chirp.

Because of the band-pass filter, adjacent samples does not
have a phase difference of more than π . The phase error
caused by residual indirect paths is bounded to (−π /2, π /2).
Thus, when the phase modulo 2π sees a sudden jump of
more than π /−π between adjacent samples at t and t − δt,
there is a modulo 2π jump at that time, which we can correct
by adding or subtracting 2π to the computed phase.

st ar t

= d (i)
end

and speed v(i)
end

To compute the initial 2N π phase offset at the beginning of
each chirp, we use the estimated distance and speed from the
end of the previous chirp. Instantaneous speed is computed
by performing least square linear regression (which is a linear
algorithm in 1D domain) over the distance values in a 10 ms
window to reduce the effects of noise and residual multipath.
Specifically, for the i + 1th received chirp, given the dis-
tance d (i)
estimated from the end of the
end
ith chirp, we can infer the distance of the beginning of the
current chirp ˆd (i+1)
+ v(i)
endδT where δT is the gap
between two chirps. We then find the 2N π offset in addition
to the ambiguous initial phase ˆϕ(0) that minimize the differ-
ence |d (i+1)(0, ˆϕ(0) + 2N π ) − ˆd (i+1)
st ar t |. Note that this relaxes
the constraints on speed imposed by prior work [21] and
instead has a constraint on acceleration. Specifically, prior
work [21] had a constraint on the maximum speed of 1 m/s.
In our algorithm, since each 2π difference of the phase corre-
sponds to around 2 cm distance difference, any error smaller
than 1cm will not cause an erroneous 2N π estimate. The
gap between two adjacent chirp is 5 ms and the delay of the
band-pass filter is 15 ms. Hence, as long as the acceleration
= 25m/s2, our algorithm does
does not exceed
not introduce erroneous phase offsets.

1cm
20ms×20ms

2) Clock synchronization. Clock differences exist in prac-
tice which we need to calibrate to achieve tracking. Specif-
ically, we need to calibrate for the initial phase as well as
any drift due to clock differences. To achieve this, at the
beginning of the session, the user touches the smartphone
speaker with a microphone at the receiver. The receiver
meanwhile starts recording the chirp for five seconds and
runs the above tracking algorithm. Using this setup, we use

4

QIA1eφA2eφ'maxerror 0 5 10 15 20 0 0.2 0.4 0.6 0.8 1Error bound (mm)A’/AOur method|ft2-ft1|=0.3Hz|ft2-ft1|=0.6Hz|ft2-ft1|=0.9HzftftdBTransmitted signalsDemodulated signalsTransmittersReceiverFilter 1:Filter 2:fMilliSonic: Pushing the Limits of Acoustic Motion Tracking

CHI 2019, May 4–9, 2019, Glasgow, Scotland Uk

autocorrelation to determine a starting time for the chirp at
the receiver. Because in this setup, at zero distance, the signal
has a high SNR, there is no motion and little indirect path,
the estimate of the distance from the peak of the FFT result,
denoted by D, is accurate. As a result, we can find the initial
2N π phase offset from D. Specifically, we find the best 2N π
which minimizes the differences of the two measurements
|D − d(0, ˆϕ(0) + 2N π )|. Finally, to address the clock drift, the
receiver detects a constant drift in the distance measurement
within the five seconds which is linear to the clock difference.
We then compensate the clock difference by removing this
drift for the following measurements at the receiver side.

3) Failure detection and recovery. Our algorithms relies on
continuous tracking. When tracking failure occurs, the sub-
sequent measurements are also prone to errors. In practice,
failures occur due to occlusions and noise.

While acoustic signal can penetrate some occlusions like
fabrics, for other occlusions like wood and human limbs,
refraction between different transmission mediums causes a
dense multipath around the direct path which is also greatly
attenuated. Therefore, the above algorithm fails because it
doesn’t satisfy the premise that the direct path dominates
the filtered demodulated signal. When such error happens
during a chirp, it will cause fluctuations in phase. Thus, there
would be multiple 2π phase tracking error during the chirp,
leading to a larger than 2cm distance error at the end of the
chirp. When the error happens between two chirps, it will
lead to wrong 2N π estimate that causes larger than 2cm
error for the subsequent chirps. Similarly at longer ranges the
signal attenuates which results in noisy phase measurements
which can also lead to wrong 2N π estimates.

To detect these failures, we utilize the redundancy across
the microphones. It is unlikely that all the four microphone
encounter the same extra phase error at the same time be-
cause of their different locations. Hence, if the measurements
from some of the four microphones are outliers with at least
2cm measurement errors from the others, it indicates an er-
ror. When such a failure is detected, if the anomaly is only
in one microphone, the receiver compensates the 2π offset
until it is in the similar range of the other three microphones.
If sustained failures occurs (which rarely happens), our algo-
rithms fall back to the traditional peak estimation method
for FMCW signals and notify the user.

4 TRACKING MULTIPLE DEVICES
The algorithm described above is unidirectional in that sig-
nals can only propagate from the speaker to the microphones.
Because of this, MilliSonic can support tracking of an un-
limited number of microphone arrays in the vicinity using a
one single smartphone speaker. Thus a single speaker can
be used as a beacon to support tracking for multiple VR
headsets that integrate our microphone array.

5

(a) 6cm × 5.35cm

(b) 15cm × 15cm

Figure 6: Prototypes of MilliSonic microphone arrays.

On the other hand, tracking multiple smartphone-based
VR headsets like the Google cardboard using a single micro-
phone array is challenging since it involves transmissions
from multiple smartphones. Traditionally, wireless systems
support multiple transmissions using either time-division
multiplexing or frequency-division multiplexing. In time-
division multiplexing, since each smartphone speaker is only
allowed to use a fraction of the time, it translates to a lower
refresh rate that is inversely proportional to the number
of smartphones. Using frequency-division multiplexing is
challenging given the limited inaudible bandwidth on smart-
phones and since the accuracy depends on the bandwidth.

To achieve concurrent transmissions from all the smart-
phone speakers, we note that from Eq. 1, any two received
FMCW paths with a time-of-arrival difference of δt, would
lie in a different FFT bin. This indicates that two devices that
have significantly different time-of-arrivals are at distant
FFT bins and hence can be concurrently decoded.

d

2N − t (i)

We utilize this to support concurrent transmissions from
multiple speakers. The challenge is that two devices can
have similar time-of-arrivals. To address this issue, we in-
troduce virtual time-of-arrival offsets at each device. Specifi-
cally, at the beginning, the N smartphones transmit FMCW
chirps using time division. The receiver computes their time-
of-arrivals using our algorithm, denoted by t (i)
for the ith
d
smartphone and sends back iT
to each transmitter i,
which is the virtual offset for transmitter i, using a Wi-Fi
connection. The transmitter i then intentionally delays its
transmission by its virtual offset. The receiver picks these
offsets to ensure that they are equally separated across all
the FFT bins. This allows concurrent speaker transmissions.
Now at the receiver, there exist N separate peaks evenly
distributed in the frequency domain, which corresponds to
N evenly distributed time-of-arrivals, where the ith time-of-
arrival is from the ith transmitter. The receiver can regard
transmissions from other transmitters as multipath. Because
of the orthogonality, they are filtered out by the band-pass
filter at the first step. It can then track the phase of each
of them using five different band-pass filters without losing
accuracy nor frame rate. After calculating the time-of-arrival

CHI 2019, May 4–9, 2019, Glasgow, Scotland Uk

Anran Wang and Shyamnath Gollakota

(a) 0-1m

(b) 1-2m

Figure 7: 1D accuracy compared with CAT and SoundTrak.

of the signal from each speaker, it subtracts the virtual offset
from it and obtains the final distance computation.

We note that because of motion, over time, the time-of-
arrivals for multiple speakers can merge together. This would
prevent the receiver from tracking all the devices concur-
rently. To prevent this, the receiver sends back a new set of
virtual delays using Wi-Fi whenever the peaks between any
two devices get close to each other in the FFT domain. When
the virtual delays get updated, which happens infrequently,
there is an additional delay of at most one chirp duration (45
ms), divided by the number of transmitters.

5 IMPLEMENTATION
We implement MilliSonic using Android smartphones. We
build an app that emits 45 ms 17.5-23.5 kHz FMCW acoustic
chirps through the smartphone speaker. We tested it us-
ing Samsung Galaxy S6, Samsung Galaxy S9 and Samsung
Galaxy S7 smartphones. We build our microphone array us-
ing off-the-shelf electronic elements shown in Fig. 5. We
use an Arduino Due connected to four MAX9814 Electret
Microphone Amplifiers [1]. We attach the elements to a
20cm × 20cm × 3cm cardboard and place the four microphone
on four corners of a 15cm × 15cm square on one side of the
cardboard. We also create a smaller 6 cm × 5.35 cm × 3cm
microphone array. We connect the Arduino to a Raspberry
Pi 3 Model B+ [12] to process the recorded samples. The
software is implemented in the Scala programming language
so that it can run on both a Raspberry Pi and a laptop with-
out modification. It utilize multithreading to improve the
performance. In our test, it requires 40ms and 9ms to process
a single 45ms chirp on the Raspberry Pi and PC, respectively.
Hence, it support real-time tracking on both platforms.

6 EVALUATION
We first evaluate the 1D and 3D tracking accuracy in a con-
trolled lab environment. We then recruited ten participants
to evaluate the real-world performance of MilliSonic.

1D Localization Accuracy. To get an accurate ground
truth, we use a linear actuator with a PhidgetStepper Bipo-
lar Stepper Motor Controller [10] which has an movement
resolution of 0.4µm to precisely control the location of the

Figure 8: Impact of cloth as
an occlusion.

Figure 9: 3D localization ac-
curacy.

(a) Motion and noise

(b) Long-term drift

Figure 10: Effect of environmental motion, noise, and drift.
platform. We place a Galaxy S6 smartphone on the platform
and place our microphone array on one end of the linear
actuator. At each distance location, we repeat the algorithm
ten times and record the measured distances. We also imple-
ment CAT [21] and SoundTrak [34]. CAT combines FMCW
with Doppler effect that is estimated using an additional car-
rier wave and SoundTrak uses phase tracking. To achieve a
fair comparison, we implement CAT using the same 6kHz
bandwidth for FMCW and an additional 16.5kHz carrier. We
implement SoundTrak using a 20kHz carrier wave. We do
not use IMU data for all three systems.

Fig 7(a) and (b) plot the CDF of the 1D errors for two
different distance ranges. We show the results for MilliSonic,
CAT as well as SoundTrak. The plots show that our system
achieves a median accuracy of 0.7 mm up to distances of 1 m.
In comparison, the median accuracy was 4 and 4.8 for CAT
and SoundTrak respectively. When the distance between the
smartphone and the microphone array is between 1–2 m,
the median accuracy was 1.74 mm, 6.89 mm and 5.68 mm for
MilliSonic, CAT and SoundTrak respectively. This decrease
in accuracy is expected since with increased distance the SNR
of the acoustic signals reduces. We also note that at closer
distances, the error is dominated by multipath which our
algorithm is designed to disambiguate multipath accurately.
Effect of environmental motion and noise. We place
the smartphone at 40cm on the linear actuator. We invite a
participant to randomly move their body at a distance of 0.2m
away from linear actuator. We also introduce acoustic noise
by randomly pressing a keyboard and playing pop music
using another smartphone that is around 1m away from the
linear actuator. Fig 10a shows the error. We can see that

6

 0 0.2 0.4 0.6 0.8 1 0 2 4 6 8 10CDF1D location error (mm)MilliSonicSoundTrakCAT 0 0.2 0.4 0.6 0.8 1 0 2 4 6 8 10 12 14 16 18CDF1D location error (mm)MilliSonicSoundTrakCAT 0 0.2 0.4 0.6 0.8 1 0 0.5 1 1.5 2 2.5 3 3.5 4CDF1D location error (mm)w/o clothw/ cloth 0 0.2 0.4 0.6 0.8 1 0 2 4 6 8 10 12 14 16CDF3D location error (mm)MilliSonicCATMilliSonic Mini 0 5 10 15 0 1 2 3 4 5 6 7 8 9 101D location error (mm)Time (min)MilliSonicCATSoundTrakMilliSonic: Pushing the Limits of Acoustic Motion Tracking

CHI 2019, May 4–9, 2019, Glasgow, Scotland Uk

MilliSonic is resilient to random motion in the environment
because of multipath resilience properties. Further, since
we filter out the audible frequencies, music playing in the
vicinity of our devices, does not affect its accuracy.

Distance drift over time. Tracking algorithms typically
can have a drift in the computed distance over time. We next,
measure the drift in the location as measured by our system
as a function of time. We also repeat the experiment for both
CAT and SoundTrak. Specifically, We place the smartphone
at 40cm on the linear actuator for 10 minutes. We place the
microphone array at the end of the actuator. We measure
the distance as measured by each of these techniques over a
duration of 10 minutes which we plot in Fig. 10b. SoundTrak
and MilliSonic uses phase to precisely obtain the clock dif-
ference of the two devices, while CAT relies on detecting the
drift of peak frequencies, which results in a larger drift. With
a few millimeter drift at 10 minutes, MilliSonic has better
stability than state-of-the-art acoustic tracking systems.

Effect of Environments. To verify the robustness to dif-
ferent environments, we additionally evaluate the 1D accu-
racy in a) an anechoic chamber; b) a 200m2 lobby; and c)
an outdoor open balcony; the median error was 0.75mm,
1.11mm and 0.94mm, respectively, at a distance of 0.6m.

Tracking through occlusions. Unlike optical signals,
acoustic signals can traverse through occlusions like cloth.
To evaluate this, we place the smartphone on a linear actuator
and change its location between 0 to 1 m away from the
microphone array. We place a cloth on the smartphone that
occludes it from the microphone array. We then run our
algorithm and compute the distance at each of the distance
values. We repeat the experiments without the cloth covering
the smartphone speaker. Fig. 8 plots the CDF of the distance
error across all the tested locations both in the presence and
absence of the cloth. The plots show that the median accuracy
is 0.74 mm and 0.95 mm in the two scenarios, showing that
MilliSonic can track devices through cloth. This is beneficial
when the phone is in the pocket and the microphone array
is tracking its location through the fabric.

3D Localization Accuracy Next, we measure the 3D lo-
calization accuracy of MilliSonic. To do this we create a
working area of 0.6m × 0.6m × 0.4m. We then print a grid of
fixed points onto a 0.6m × 0.6m wood substrate. We place
the receiver on one side of the substrate, and place the smart-
phone’s speaker at each of the points on the substrate. We
also change the height of the substrate across the working
area to test the accuracy along the axis perpendicular to the
substrate. To compare with prior designs, we run the same
implementation of CAT as in our 1D experiments. Note that
while CAT [21] uses a separation of 90 cm, we still use 15cm
microphone separation for CAT. This allows us to perform a
head-to-head comparison as well as evaluate the feasibility
of using a small microphone array.

7

Fig 9 shows the CDF of 3D location errors for MilliSonic
and CAT in a working area across all the tested locations in
our working area. The plots show that MilliSonic achieves a
median 3D accuracy of 2.6 mm while CAT has a 3D accuracy
of 10.6 mm. The larger errors for CAT is expected since it is
designed for microphone/speaker separations of 90 cm.

To understand the limits of the microphone separation, we
further reduce the microphone separation to 5.35cm using
a breadboard hardware prototype as shown in Fig 5. This
reduces the dimensions of the microphone array to approxi-
mately 6cm × 6cm × 3cm. We show the 3D error results in
Fig. 9 labelled as MilliSonic Mini. We can see that there is
little accuracy degradation. This shows that MilliSonic can
enable a portable beacon design that uses microphone arrays
to track smartphone based Google cardboard VR systems.
Similarly, given these dimensions, the microphone array can
be integrated into a VR headset which can then be tracked
in 3D using a commodity smartphone as a beacon.

Free Motion Tracking with Participants. We build a
simple draw-over-the-air interface based on MilliSonic. We
put our microphone array hardware on the table to act as the
beacon. We implement a software app on Android platforms
where participants can move the smartphone and touch the
screen to draw 2D images on the y − z plane over the air.
Meanwhile, the strokes are rendered on an external screen
in real-time. We use a Samsung S6 smartphone for this study.
We compare MilliSonic to a HTC Vive Controller which is
tracked using the HTC Lighthouse positioning system [3].
Specifically we put two Lighthouse base stations on two
tables with a distance of 2.5m. We attach the HTC Vive
controller to the smartphone using tape and use the HTC
Lighthouse positioning system to track its motion. Since the
Lighthouse positioning system has an accuracy of around
1mm [25], we still use it as the ground truth.

We recruit ten participants (2 female and 8 male) between
the ages of 22-29 to draw on the air using MilliSonic. None of
them were provided any monetary benefits. The participants
were free to draw whatever they like and see the motion on
the screen in real-time. We added a draw button on the screen,
so that when a user pushes the button, the app uses TCP to
send the action to another server which records the traces
and renders them on the screen in real-time. Each participant
had to draw at least one figure of their choosing but could
draw multiple figures if they wanted. The participants in
total drew 14 images. Fig. 11 shows five samples and the
corresponding ground truth captured by a HTC Lighthouse.
We compare MilliSonic’s accuracy with the ground truth
from the HTC Lighthouse system. Because of frame rate
differences, we linearly interpolate the ground truth result,
find the point at the ground truth that is nearest to each point
in our tracking result, and compute their difference. We show
the CDFs of 3D accuracy in Fig. 12 for each of the 14 drawings

CHI 2019, May 4–9, 2019, Glasgow, Scotland Uk

Anran Wang and Shyamnath Gollakota

(a) Infinity

(b) Pac-Man

(c) Heart

(d) Peppa Pig

Figure 11: Sample drawings by participants. Green and red traces are captured by HTC Lighthouse and MilliSonic respectively.

7 RELATED WORK
Prior work can be categorized as follows.

Tracking using IMUs. Inertial measurement units (IMUs)
are a frequently used hardware to enable device tracking.
IMUs sense 3D linear acceleration, rotational rate and head-
ing reference which can all be fused together [18]. Gaming
controllers [6, 7] as well as many low-end VR systems [2,
8, 13] use IMUs to support motion tracking. However IMUs
do not accurately provide absolute positioning information.
This is because position requires double integration of accel-
eration, which introduces a large drift error [17].

Tracking using IR/visible light. The HTC Vive VR [3] sys-
tem uses a laser Lighthouse beacon emitting coherent IR
signal to localize the headset as well as the controllers. Here,
a laser emitter sweeps coherent IR light spatially and the 3D
location is computed using the time it takes for the IR signals
to hit the photo-diodes on the receivers. Incoherent, infrared
and visible light from LEDs can also be used for localiza-
tion by cameras using specific colors. The Sony PlayStation
VR (PSVR) [14] system uses special visible colors that are
tracked by a standalone camera located in a fixed position.
Oculus Rift[9] VR system employs a separate IR camera. The
headset and controller are marked with IR LED markers cap-
tured by the IR camera. Despite being accurate enough for
VR/AR applications, these techniques work poorly in bright
environments [11]. More importantly, they require a ded-
icated beacon hardware. In contrast, our design can use a
smartphone as a basestation for tracking the VR headset.

Tracking using cameras. Unlike previous methods, Simul-
taneous Localization and Mapping (SLAM) techniques have
also been used to enable tracking without relying on any
beacon infrastructure. Using SLAM, devices can locate them-
selves solely based on the environment captured by its cam-
era. AR systems such as Microsoft Hololens[5] and Magic
Leap One[4] headsets use SLAM to achieve such tracking
capabilities. SLAM performance however highly depends on
the environment including light conditions and variety of vi-
sual features [22, 28]. Hence, it is not as robust as outside-in

Figure 12: The CDF of absolute 3D error across participants.
The black curve corresponds to the Infinity in Fig. 11.

which show accurate tracking capabilities using acoustic
signals. The outlier orange curve corresponds to the infinite
drawing in Fig. 11 which shows that the practical accuracies
are high. There were a few instances when a wrong 2N π
phase offset was estimated in the phase ambiguity removal
algorithm on one of the microphones. This was however
detected and successfully recovered by our failure recovery
mechanism and did not affect the following chirp.

We also measure the free motion speed distribution, ac-
celeration distribution and distance distribution across the
participants, which we plot in a Fig. 13. We see a range of
speeds and distances during this user study. We also note
that the maximum acceleration was 21 m/s2 with only 1
occurrence which was below our 25m/s2 limit.

Enabling concurrent transmissions. Finally, to evalu-
ate concurrent transmissions with MilliSonic, we use five
smartphones (3 Galaxy S6, 1 Galaxy S7, 1 Galaxy S9) as trans-
mitters and one single microphone array to track all of them.
We use the same experimental setup as the 1D tracking, but
place all five smartphones on the linear actuator platform.
We repeat experiments with different number of concurrent
smartphones ranging from one to five. Fig. 14 shows the 1D
tracking error of each of the smartphones in the range of
0-1m with different number of concurrent smartphones. We
see that our system can support up to 4 concurrent transmis-
sions without affecting the accuracy. With five concurrent
smartphones, nearby peaks start to interfere with each other,
resulting a slightly worse accuracy.

8

 0 0.2 0.4 0.6 0.8 1 0 2 4 6 8 10 12CDFerror (mm)MilliSonic: Pushing the Limits of Acoustic Motion Tracking

CHI 2019, May 4–9, 2019, Glasgow, Scotland Uk

Figure 13: Speed, acceleration and distance distribution during the user study.

tracking methods. SLAM is also a computational intensive
algorithm that often requires specialized hardware acceler-
ators to support real-time tracking. As a result, SLAM is
unlikely to be appropriate for tracking tiny controllers.

Device tracking using acoustic signals. Table. 1 shows recent
work on acoustic localization and tracking. BeepBeep [26]
and Swordfight [35] track 1D distances between phones
but do not achieve 3D localization. Sonoloc[16] realizes dis-
tributed localization and requires 10+ devices to achieve rea-
sonable accuracies. Prior work [33] also achieves 2D tracking
by assuming that there is no significant multipath. ALPS [20]
and Tracko [19] achieve centimeter-level accuracy using a
combination of Bluetooth and ultrasonic. The closest to our
work are CAT [21] and SoundTrak [34]. CAT achieves a me-
dian 3D error of 9mm using a combination of IMU sensor
data and FMCW localization to address multipath. It requires
a separation of 0.9m and 0.7m between its horizontal and ver-
tical speaker pairs respectively. As a result, we cannot have
a smartphone track the position of a VR headset, since both
the devices have much smaller dimensions. SoundTrak[34]
achieves an average 3D error of 1.3cm between a smart watch
and a customized finger ring using phase tracking where the
area of movement is limited to a 20cm × 16cm × 11cm space.
Our work builds on this foundational work but is the first to
1) achieve sub-millimeter 1D resolution, 2) do so without re-
quiring large separation between microphones/speakers and
3) enable for the first time concurrent transmissions where all
the acoustic devices transmit at the same time; thus allowing
for high refresh rate in the presence of multiple trackers.

Device-free tracking using acoustic signals. VSkin [27] tracks
gestures on the surface of mobile devices with a 2D accuracy
of 3mm. Strata [32], LLAP [29] and FingerIO [24] track mov-
ing fingers in the proximity of a mobile device with a 2D
accuracy of 1cm, 1.9cm and 1.2cm respectively. Toffee [31]

Figure 14: Tracking error with concurrent smartphones.

9

localizes the direction of a touch around a mobile device of
an angular error of 4.3°. While device-free finger tracking
is challenging because of noisy measurements, it benefits
from lack of synchronization issues and relies on more strict
multipath assumptions. This line of work however is com-
plimentary to our work on acoustic device tracking.

8 CONCLUSION AND DISCUSSION
We present MilliSonic, a novel system that pushes the lim-
its of acoustic based motion tracking and localization. We
show for the first time how to achieve sub-mm 1D tracking
and localization accuracies using acoustic signals on smart-
phones, in the presence of multipath. To achieve this, we
introduce algorithms that use the phase of FMCW signals to
disambiguate between multiple paths. We also enable multi-
ple smartphones to transmit concurrently using time-shifted
FMCW acoustic signals and enable concurrent tracking with-
out sacrificing accuracy or frame rate.

While this paper presents multiple benchmarks, user stud-
ies and evaluation in indoor and outdoor environments, more
extensive evaluation is required to understand its behavior in
various edge cases as well as in rooms with significant mul-
tipath that can adversely affect accuracy. Here, we discuss
the limitations of our current system design.

First, we support simple occlusions such as fabric and
paper, but do not support human limbs or the device itself.
Additional algorithmic development is required to support
these practical occlusion scenarios. Second, while our design
has better drift characteristics than prior work on acoustic
tracking, further work is required to make it comparable to
optical based systems. One approach is to perform sensor
fusion with IMU data and achieve better accuracy, lower
latency and more resilience to clock drifts. This could also
enable VR headset tracking while using a mobile beacon (i.e.,
smartphone) in the hand instead of placing it on a table.

Our current range is limited to 2 m. This is because the
microphones in our array prototype are not optimized for
performance and are not designed to have optimal response
in the 17.5–23.5 kHz frequencies. Finally, we support upto
4–5 concurrent smartphone acoustic transmissions without
affecting the frame rate per device. One way to increase
the number of concurrent devices is to use longer chirps so
as to support more time-shifted FMCW chirps that can be

 0 20 40 60 80 100 0 0.5 1 1.5 2 2.5 3 3.5 4 4.5 5No. of occurrencesspeed (m/s) 0 100 200 300 400 500 0 5 10 15 20 25No. of occurrencesacceleration (m/(s*s)) 0 5000 10000 15000 20000 0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6No. of occurrencesdistances (m) 0 1 2 3 4 5123451D location error (mm)# of transmittersS6#1S6#2S6#3S7S9CHI 2019, May 4–9, 2019, Glasgow, Scotland Uk

Anran Wang and Shyamnath Gollakota

Need
IMU

System

Setup

BeepBeep
Phone-Phone
Swordfight Phone-Phone
CAT
SoundTrak
Sonoloc

Ranging
technique
Autocorrelation N
Autocorrelation N
Y
FMCW
N
Autocorrelation N
FMCW+
Phase tracking

N

Speaker-Phone
Speaker-Watch Phase tracking
Phone-Phone
Microphone-
Phone

MilliSonic

Audible Dimension Accuracy Latency

Y
Y
N
Y
Y

N

1D
1D
3D
3D
2D
3D
1D

2cm
2cm
9mm
13mm
6cm
2.6mm
0.6mm

50ms
46ms
40ms
12ms
3.2-48s
25-40ms
15-30ms

Refresh
rate
20Hz
12Hz
25Hz
86Hz
-

Range

Concurrent
transmission
12m
N
3m
N
N
7m
20cm N
N
17m

≥40Hz

3m

Y

Mic/speaker
Separation
-
-
90cm
4cm
-

6-15cm

Table 1: Prior works on acoustic device tracking.

allocated to different smartphones. This however comes at
the expense of the frame rate per device.

9 APPENDIX: 1D TRACKING DETAILS
Our 1D tracking algorithm has two main components.

1) Adaptive band-pass filter to remove distant multipath.
For the first FMCW chirp, we extract the first peak of the
demodulated signal in the frequency domain using an DFT
similar to prior designs from Eq. 1. We then apply a Finite
Impulse Response (FIR) filter that only leaves a narrow range
of frequency bands around the peak. We adaptively set the
delay of the FIR filter from the SNR of the acoustic signals.
Specifically, when SN R > 10dB, we use a 15ms delay; other-
wise, we double the delay to 30ms.

For subsequent FMCW chirps we no longer use the DFT
to extract the peak frequency. Instead, for the i + 1th FMCW
chirp, we infer the new peak from the distance and speed
estimated at the end of the ith chirp. We then apply the
FIR filter around this new peak. Given the distance d (i)
end
and speed v(i)
estimated from the end of the ith chirp, we
end
can infer the distance of the beginning of the current chirp
ˆd (i+1)
endδT where δT is the gap between two
st ar t
chirps. We do this for two key reasons: a) our distance esti-
mates are far more accurate than the peak of the DFT result;
and b) unlike a DFT that is performed over a whole FMCW
chirp, we do not require receiving a full FMCW chirp before
processing, thus reducing the frame rate.

= d (i)
end

+ v(i)

Finally, Doppler effects can blur the peak in the frequency
domain. So, we adaptively increase the width of the pass band
in the FIR filter when the speed estimate at the end of the
previous chirp exceeds a given threshold. In our algorithm,
we set the pass band width to 1Hz when the speed does not
exceed 1m/s; otherwise, we set the pass band width to 2Hz.
2) Extracting distance from FMCW phase. The above pro-
cess eliminated all multipaths that have a much larger time-
of-flight than the direct path. This leaves us with residual
indirect paths around the direct path. Thus, when there is no
occlusion, the sum of the residual indirect paths has a lower
amplitude than the direct path (confirmed empirically).

To extract the distance from the phase value, we approx-
imate the effect of residual multipath after filtering. From

Eq. 1, we approximate the phase as,

ϕ(t) ≈ −2π (

B
T

ttd + f0td −

B
2T

2
d )

t

(2)

Where td is the time of arrival of the direct path. The approx-
imation assumes that we have already applied the dynamic
filter to remove most multipath that has a much larger time-
of-arrival distance than the direct path. The above quadratic
equation in td (t, ϕ(t)) can be uniquely solved; the equation
has two solutions but only one is in the range of the FMCW
chirp, [0,T ]. The distance d(t, ϕ(t)) can then be computed as,
ctd (t, ϕ(t)). We note the following about the distance error.

Lemma 9.1. Given the phase error bound of sin−1( A2
A1

) from
Fig. 4, the error in our distance estimate, d(t) is upper bounded
by sin−1(A2/A1)c
2π (f0−B/2)

, where f0 and B are the FMCW parameters.

dtd

= 2π B

= −2π ( B

Proof. First we show that the function ϕ(t, td ) in Eq. 2 is
convex with respect to td , which is the time-of-arrival. This is
because the first derivative is given by, dϕ(t,td )
T t +
T td ) < 0, when td < T . Further its second derivative
f0 − B
d 2ϕ(t,td )
T > 0 resulting in a convex function.
dt 2
d
Suppose an phase error ϕe would introduce a time-of-
arrival error of te because of multipath. Without loss of gen-
erality, we assume ϕe > 0. we know that for any t for a
convex function, ϕ(t, td ) > ϕ(t, td + te ) − dϕ(t,td +te )
te . Thus
the error in the phase ϕe = ϕ(t, td ) − ϕ(t, td + te ) can we
written as, ϕe > − dϕ(t,td +te )
te . This can be rewritten as,
ϕe
. The upper bound for
te <
T t +f0− B
this equation occurs when t = 0 and td +te is maximum. Since
the maximum delay permitted by our FMCW signal is half
its duration, this occurs when td + te = T
2 . First from Lemma
2.1, we know that ϕe < sin−1( A2
). Thus the above equation is
A1
sin−1( A2
sin−1( A2
)c
)
.
A1
A1
2π (f0− B
2π (f0− B
2 )
2 )
□

upper bounded as: te <

ϕe
− d ϕ(t, td
d td

. Thus, d

(phase)
e

T (td +te ))

2π ( B

+te )

dtd

dtd

=

<

REFERENCES
[1] Electret microphone amplifier - max9814 with auto gain control. https:

//www.adafruit.com/product/1713.

[2] Google daydream. https://vr.google.com/daydream/.

10

MilliSonic: Pushing the Limits of Acoustic Motion Tracking

CHI 2019, May 4–9, 2019, Glasgow, Scotland Uk

[24] Nandakumar, R., Iyer, V., Tan, D., and Gollakota, S. Fingerio: Using
active sonar for fine-grained finger tracking. In Proceedings of the 2016
CHI Conference on Human Factors in Computing Systems (2016), ACM,
pp. 1515–1525.

[25] Niehorster, D. C., Li, L., and Lappe, M. The accuracy and precision of
position and orientation tracking in the htc vive virtual reality system
for scientific research. i-Perception 8, 3 (2017), 2041669517708205.
[26] Peng, C., Shen, G., Zhang, Y., Li, Y., and Tan, K. Beepbeep: a high
accuracy acoustic ranging system using cots mobile devices. In Pro-
ceedings of the 5th international conference on Embedded networked
sensor systems (2007), ACM, pp. 1–14.

[27] Sun, K., Zhao, T., Wang, W., and Xie, L. Vskin: Sensing touch gestures
on surfaces of mobile devices using acoustic signals. In Proceedings
of the 24th Annual International Conference on Mobile Computing and
Networking (2018), ACM, pp. 591–605.

[28] Taketomi, T., Uchiyama, H., and Ikeda, S. Visual slam algorithms: a
survey from 2010 to 2016. IPSJ Transactions on Computer Vision and
Applications 9, 1 (2017), 16.

[29] Wang, W., Liu, A. X., and Sun, K. Device-free gesture tracking using
acoustic signals. In Proceedings of the 22nd Annual International Con-
ference on Mobile Computing and Networking (2016), ACM, pp. 82–94.
[30] Wen, Y., Huang, W., and Zhang, Z. Cazac sequence and its application
in lte random access. In Information Theory Workshop, 2006. ITW’06
Chengdu. IEEE (2006), IEEE, pp. 544–547.

[31] Xiao, R., Lew, G., Marsanico, J., Hariharan, D., Hudson, S., and
Harrison, C. Toffee: enabling ad hoc, around-device interaction
with acoustic time-of-arrival correlation. In Proceedings of the 16th
international conference on Human-computer interaction with mobile
devices & services (2014), ACM, pp. 67–76.

[32] Yun, S., Chen, Y.-C., Zheng, H., Qiu, L., and Mao, W. Strata: Fine-
grained acoustic-based device-free tracking. In Proceedings of the 15th
Annual International Conference on Mobile Systems, Applications, and
Services (2017), ACM, pp. 15–28.

[33] Yunting Zhang, Jiliang Wang, W. W. Z. W. Y. L. Vernier: Accurate
and fast acoustic motion tracking using mobile devices. In INFOCOM
(2018), IEEE.

[34] Zhang, C., Xue, Q., Waghmare, A., Jain, S., Pu, Y., Hersek, S., Lyons,
K., Cunefare, K. A., Inan, O. T., and Abowd, G. D. Soundtrak: Con-
tinuous 3d tracking of a finger using active acoustics. Proceedings of
the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies
1, 2 (2017), 30.

[35] Zhang, Z., Chu, D., Chen, X., and Moscibroda, T. Swordfight: En-
abling a new class of phone-to-phone action games on commodity
phones. In Proceedings of the 10th international conference on Mobile
systems, applications, and services (2012), ACM, pp. 1–14.

[3] Htc

vive

vr
vive-virtual-reality-system/.

system.

https://www.vive.com/us/product/

[4] Magic leap one. https://www.magicleap.com/magic-leap-one.
[5] Microsoft hololens. https://www.microsoft.com/en-us/hololens.
[6] Nintendo switch. https://www.nintendo.com/switch/.
[7] Nintendo wii. http://wii.com/.
[8] Oculus go. https://www.oculus.com/go/.
[9] Oculus rift. https://www.oculus.com/rift/.
[10] Phidgetstepper bipolar hc. https://www.phidgets.com/?tier=3&catid=

23&pcid=20&prodid=1029.

[11] Playstation vr: The ultimate faq. https://blog.us.playstation.com/2017/

10/02/playstation-vr-the-ultimate-faq/.

[12] Raspberry pi 3 model b+. https://www.raspberrypi.org/products/

raspberry-pi-3-model-b-plus/.

[13] Samsung gear vr. http://www.samsung.com/global/galaxy/gear-vr/.
[14] Sony playstation move controller. https://www.playstation.com/en-us/

explore/accessories/vr-accessories/playstation-move/.

[15] Akiduki, H., Nishiike, S., Watanabe, H., Matsuoka, K., Kubo, T.,
and Takeda, N. Visual-vestibular conflict induced by virtual reality
in humans. Neuroscience letters 340 3 (2003), 197–200.

[16] Erdélyi, V., Le, T.-K., Bhattacharjee, B., Druschel, P., and Ono, N.

Sonoloc: Scalable positioning of commodity mobile devices.

[17] Feliz Alonso, R., Zalama Casanova, E., and Gómez García-

Bermejo, J. Pedestrian tracking using inertial sensors.

[18] Fourati, H. Heterogeneous data fusion algorithm for pedestrian
navigation via foot-mounted inertial measurement unit and comple-
mentary filter. IEEE Transactions on Instrumentation and Measurement
64, 1 (2015), 221–229.

[19] Jin, H., Holz, C., and Hornbæk, K. Tracko: Ad-hoc mobile 3d tracking
using bluetooth low energy and inaudible signals for cross-device
interaction. In Proceedings of the 28th Annual ACM Symposium on User
Interface Software & Technology (2015), ACM, pp. 147–156.

[20] Lazik, P., Rajagopal, N., Shih, O., Sinopoli, B., and Rowe, A. Alps:
A bluetooth and ultrasound platform for mapping and localization. In
Proceedings of the 13th ACM conference on embedded networked sensor
systems (2015), ACM, pp. 73–84.

[21] Mao, W., He, J., and Qiu, L. Cat: high-precision acoustic motion
tracking. In Proceedings of the 22nd Annual International Conference
on Mobile Computing and Networking (2016), ACM, pp. 69–81.
[22] Montemerlo, M., Thrun, S., Koller, D., Wegbreit, B., et al. Fast-
slam: A factored solution to the simultaneous localization and mapping
problem. Aaai/iaai 593598 (2002).

[23] Nandakumar, R., Gollakota, S., and Watson, N. Contactless sleep
apnea detection on smartphones. In Proceedings of the 13th Annual
International Conference on Mobile Systems, Applications, and Services
(2015), ACM, pp. 45–57.

11

