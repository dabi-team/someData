1
2
0
2

t
c
O
4

]

M
E
.
n
o
c
e
[

3
v
5
4
4
2
1
.
4
0
0
2
:
v
i
X
r
a

Inference with Many Weak Instruments

Anna Mikusheva1 and Liyang Sun2

Abstract

We develop a concept of weak identiﬁcation in linear IV models in which the number of instru-

ments can grow at the same rate or slower than the sample size. We propose a jackknifed version

of the classical weak identiﬁcation-robust Anderson-Rubin (AR) test statistic. Large-sample in-

ference based on the jackknifed AR is valid under heteroscedasticity and weak identiﬁcation. The

feasible version of this statistic uses a novel variance estimator. The test has uniformly correct

size and good power properties. We also develop a pre-test for weak identiﬁcation that is related

to the size property of a Wald test based on the Jackknife Instrumental Variable Estimator. This

new pre-test is valid under heteroscedasticity and with many instruments.

Key words: instrumental variables, weak identiﬁcation, dimensionality asymptotics.

JEL classification codes: C12, C36, C55.

1 Introduction

Recent empirical applications of instrumental variables (IV) estimation often involve many

instruments that together may or may not be strongly relevant. For example, in a promi-

nent paper by Angrist and Krueger (1991) that started the weak IV literature, the authors

construct 180 instruments by interacting dummies for the quarter of birth with state and

year of birth, and use these instruments to study the eﬀect of schooling on wage. Other ex-

amples include papers that employ an empirical strategy known as “judge design” (Maestas

et al., 2013; Sampat and Williams, 2015; Dobbie et al., 2018). Fueled by rich administra-

tive data, these papers use the exogenous assignment of cases to judges as instruments for

treatment. Since each judge can only process a certain number of cases out of the total

1Department of Economics, M.I.T. Address: 77 Massachusetts Avenue, E52-526, Cambridge, MA,
02139. Email: amikushe@mit.edu. National Science Foundation support under grant number 1757199 is
gratefully acknowledged. We are grateful to Josh Angrist, Kirill Evdokimov, Whitney Newey and Mikkel
Sølvsten for advice, to Brigham Frandsen for sharing code for simulations, and to Ben Deaner and Sylvia
Klosin for research assistance.

2UC Berkeley and CEMFI. Email: lsun20@berkeley.edu. Support from the Jerry A. Hausman Gradu-
ate Dissertation Fellowship and Institute of Education Sciences, U.S. Department of Education, through
Grant R305D200010, is gratefully acknowledged.

1

 
 
 
 
 
 
court cases, the number of judges (the number of instruments) is usually proportional

to the sample size. Another example is the famous Fama-MacBeth procedure in Asset

Pricing (Fama and MacBeth, 1973; Shanken, 1992), which is equivalent to IV estimation

procedure with the number of instruments proportional to the number of assets.

This paper answers three questions in an environment with many instruments: how

to deﬁne weak identiﬁcation, what to do if identiﬁcation is weak, and how to pre-test for

weak instruments. We model many-instrument asymptotics by allowing the number of

instruments to grow at most proportionally with the sample size. Firstly, we deﬁne weak

identiﬁcation for linear IV models with many instruments by providing necessary and

suﬃcient conditions for the existence of a consistent test. Secondly, we introduce a test

that works when there are many instruments, but is also robust to weak identiﬁcation and

heteroscedasticity. Finally, we propose a pre-test for weak identiﬁcation. This pre-test

forms the basis for a two-step procedure that is analogous to that of Stock and Yogo

(2005). The two-step test controls size distortion under many-instrument asymptotics,

regardless of the strength of identiﬁcation or the presence of heteroscedasticity.

We deﬁne weak identiﬁcation as a situation where an analog of the concentration

parameter divided by the square root of the number of instruments stays bounded in

large samples. We prove that even in a homoscedastic model with known covariance, an

asymptotically consistent test does not exist if the ratio of the concentration parameter

over the square root of the number of instruments stays bounded in large samples. Thus,

a necessary condition for a consistent test to exist is that the concentration parameter

grows faster than the square root of the number of instruments. Later, we show that this

is also a suﬃcient condition by constructing a robust test that becomes consistent when

this condition is satisﬁed.

We propose a new jackknifed version of the Anderson-Rubin (AR) test which is robust

to both weak identiﬁcation and heteroscedasticity in a model with many instruments. The

new test uses an asymptotic approximation based on a Central Limit Theorem (CLT) for

quadratic forms. The new AR test has the correct size regardless of identiﬁcation strength

and becomes consistent as soon as the concentration parameter grows faster than the

square root of the number of instruments.

2

As an important technical contribution, we introduce a novel variance estimator for the

quadratic form CLT in the absence of a consistent estimator for the structural parameter.

The target variance is a quadratic form of the individual (heteroscedastic) variances of

errors. We apply cross-ﬁtting (Newey and Robins, 2018; Kline et al., 2020) to produce

unbiased proxies for the individual variances of errors. We adjust the quadratic form to

remove the bias due to correlations between proxies. We prove the consistency of the

new estimator under the null and local alternatives under a wide range of identiﬁcation

scenarios.

Finally, we propose a new pre-test for weak identiﬁcation which is easy to use and is

consistent with our deﬁnition of weak identiﬁcation. An empirical researcher can use our

pre-test to decide between employing our jackknife AR test if the pre-test suggests that

the identiﬁcation is weak or a Wald test based on the Jackknife Instrumental Variable

Estimator (JIVE, Angrist et al., 1999) if the pre-test suggests that the identiﬁcation is

strong. We guarantee the size of this two-step procedure. Chao et al.

(2012) prove

that JIVE is consistent in a heteroscedastic model when the concentration parameter

grows faster than the square root of the number of instruments. Chao et al.

(2012)

also derive a consistent estimator of the JIVE standard error. The two-step procedure is

appealing because when identiﬁcation is strong, the JIVE-Wald is more eﬃcient and easy

to implement and report.

Our pre-test is in the spirit of Stock and Yogo (2005), but it diﬀers from theirs in

two important ways. Firstly, our pre-test allows for a general form of heteroscedasticity,

while the pre-test proposed in Stock and Yogo (2005) works only under conditionally

homoscedastic errors. Secondly, the Stock and Yogo (2005) pre-test is designed for a small

number of instruments and is based on the Two-Stage Least Squares (TSLS) estimator.

With many instruments TSLS is consistent only when the concentration parameter grows

faster than the number of instruments, which makes the Stock and Yogo (2005) pre-test

not very informative.

We apply our pre-test to Angrist and Krueger (1991) and ﬁnd that their identiﬁcation

is strong. Consequently the JIVE conﬁdence set is reliable (has coverage within 5%

tolerance level of the declared coverage). Our weak identiﬁcation-robust jackknife AR

3

conﬁdence set is somewhat wider than the JIVE conﬁdence set but is still informative.

Relation to the Literature. Our paper contributes to both the literature on weak

IV and the literature on many instruments. The weak IV literature relates identiﬁcation

strength to the size of the concentration parameter and proposes robust tests that work

only when there are a small number of instruments. Generalizations to many weak in-

struments either strongly restrict the number of instruments (Andrews and Stock, 2007)

or work only under homoscedasticity (Anatolyev and Gospodinov, 2011). Crudu et al.

(2020) recently proposed a jackknife-type AR test that is robust towards weak identiﬁca-

tion and heteroscedasticity. While employing a similar statistic, their test uses a variance

estimator diﬀerent from ours, that can be shown to lead to a power loss at distant alter-

natives and inconsistency of the test in some settings where our test is consistent.

The many weak instruments literature started with a prominent paper by Bekker

(1994). It mostly establishes conditions for consistency and asymptotic gaussianity for

particular estimators. For example, Chao and Swanson (2005) show that in a homoscedas-

tic model limited information maximum likelihood (LIML) and bias-corrected TSLS

(BTSLS) are consistent when the concentration parameter grows faster than the square

root of the number of instruments.

In a heteroscedastic model, consistency of LIML

and BTSLS requires that the concentration parameter grows faster than the number of

instruments. By contrast, JIVE remains consistent when the concentration parameter

grows faster than the square root of the number of instruments (Chao et al., 2012). Our

paper shows that the condition in Chao et al. (2012) is necessary for consistency and if

it is violated it is impossible to consistently distinguish between any two values of the

structural parameter.

The remainder of this paper is organized as follows. Section 2 summarizes our proposal

for empirical researchers. In Section 3 we introduce our deﬁnition of weak identiﬁcation in

an environment with many instruments. In Section 4 we construct the jackknife AR test

and establish its power properties. In Section 5 we present the pre-test and prove that it

controls size. Section 6 conducts a simulation exercise inspired by Angrist and Frandsen

(2019), and Section 7 concludes. Some proofs and additional results may be found in the

Supplementary Appendix.

4

2 Many Weak Instruments: Empirical Practice

In empirical applications using instrumental variables, concerns about weak identiﬁcation

are widespread. The current consensus practice is to report the ﬁrst stage F statistic

and as long as it is above 10, researchers are allowed to rely on standard t-statistics

inferences. This practice has foundations in Stock and Yogo (2005) which showed that

the concentration parameter fully characterizes the size distortion of the TSLS-Wald test,

and empirically the concentration parameter can be judged based on the ﬁrst stage F

statistics. This result has been obtained under the assumptions of homoscedasticity and

for a ﬁxed number of instruments.

While the ﬁrst stage F pre-test provides reasonable classiﬁcation for homoscedastic

IV models with a small number of instruments, it is inadequate for settings with many

instruments. Hansen et al (2008) argue that the TSLS estimator should not be used in

applications with many instruments as it becomes very biased. They also argue that a

low ﬁrst stage F statistic is not always indicative of a weak identiﬁcation issue and t-

statistics inferences based on more appropriate estimators other than TSLS, along with

corrected standard errors, may still be reliable. Estimators with known good properties in

heteroscedastic settings with many instruments include JIVE (Chao and Swanson, 2005)

and heteroscedasticity-robust Fuller (Hausman et al, 2012).

While theoretical econometrics literature provides recommendations on the choice of

estimator, it is largely silent on how to determine whether the concerns of weak identi-

ﬁcation are valid in a given data set with many instruments. This prompted empirical

researchers to formulate econometric arguments and perform simulation studies to sup-

port their usage of t-statistics. For example, Bhuller et al (2020), recently published in the

Journal of Political Economy, used judges design instruments to study the eﬀects of incar-

ceration on recidivism and employment. Concerned about potentially having many weak

instruments, Bhuller et al (2020) included a 10-page-long Appendix D with a simulation

study to support their usage of the JIVE t-statistic.

Our paper proposes a new recipe for empirical researchers to gauge weak identiﬁcation

in applications with many instruments. Speciﬁcally, we argue that theoretically, the

strength of identiﬁcation is measured by the concentration parameter divided by the square

5

root of the number of instruments. This is in contrast to the ﬁrst stage F statistics from

Stock and Yogo (2005), which implicitly divide the concentration parameter by the number

of instruments. We suggest applied researchers calculate a new pre-test

F (see equation

(6)) and compare it to a cutoﬀ of 4.14. If

F is above the cutoﬀ then the researcher can rely

e

on the JIVE t-statistic with the caveats analogous to Stock and Yogo (2005): Namely, the

e

size distortions of the JIVE t-statistic are within 5% tolerance level of the nominal size. If

F is below the cutoﬀ we suggest researchers report a conﬁdence set obtained by inversion

of our newly proposed weak-identiﬁcation robust jackknife AR test (see Equation (2)).
e
As discussed in Section 5, applied researchers may also choose other cutoﬀs depending

on their tolerance level of size distortions. Here we illustrate this recipe with an example

from Angrist and Krueger (1991) (hereafter referred to as AK91).

AK91 provided a motivating example for the weak identiﬁcation literature, starting

with the seminal work by Bound et al. (1995). Staiger and Stock (1997) suggested that

the relatively low value of the ﬁrst stage F statistic can be seen as a sign of potentially

weak instruments in the AK91 application. Hansen et al.

(2008) argued that many

instruments may be a more relevant description of the identiﬁcation issue encountered in

AK91. They suggested that estimators other than the TSLS may restore the reliability

of standard inferences. We resolve the controversy of whether the instruments are weak

in this example utilizing a formal pre-test.

The original AK91 application estimated the eﬀect of schooling (Xi) on log weekly

wage (Yi) using quarter of birth as instruments in a sample from the 1980 census of

329,509 men born in 1930-39. There are multiple speciﬁcations in the original AK91

study. We focus on the speciﬁcation with 180 instruments and also on an extension of

this speciﬁcation using 1,530 instruments. The 180 instruments include 30 quarter and

year of birth interactions (QOB-YOB) and 150 quarter and state of birth interactions

(QOB-POB). For the second speciﬁcation with 1,530 instruments, we also include full

interactions among QOB-YOB-POB. Table 1 reports the ﬁrst stage F statistics (FF), our

proposed pre-test statistics

F , 5% and 2% conﬁdence sets based on the JIVE t-statistic

and the jackknife AR statistic proposed in this paper.

e

While the ﬁrst stage F statistic is below 10 and the current empirical practice would

6

point towards weak identiﬁcation for both speciﬁcations, the instruments turn out to be

strong in both speciﬁcations based on our pre-test. According to the results of this paper

discussed in Section 5, both of the reported conﬁdence sets based on a nominal 5% JIVE

t-test are reliable with the same caveats as in Stock and Yogo (2005), namely, the actual

rejection rate under the null hypothesis does not exceed 10%. This pre-test is based on the

statistic

F and rejects whenever

F > 4.14. Based on the pre-test, the empirical researcher

may report the JIVE conﬁdence set only, and not the identiﬁcation-robust AR conﬁdence

e

e

set.

This two-step procedure is similar to that popularized by Stock and Yogo’s (2005).

Choosing between JIVE and AR conﬁdence set to report based on the pretest in the

ﬁrst step guarantees that the reported conﬁdent set from this two-step procedure has an

overall size of 15%. If the applied researcher prefers that the two-step procedure has an

overall size of 5%, results in Section 5 suggest using a higher cutoﬀ of 9.98 for

F and

smaller nominal sizes to construct the conﬁdence sets. Speciﬁcally, the applied researcher

e

should choose between a nominal 98%-level JIVE conﬁdence set and a nominal 98%-level

AR conﬁdence set. In this case, for the speciﬁcation with 180 instruments, the applied

researcher can still report the JIVE conﬁdence set as the corresponding

F is greater than

9.98. However, for the speciﬁcation with 1530 instruments, the applied researcher needs

e

to report the identiﬁcation-robust AR conﬁdence set instead as the corresponding

F is

less than 9.98.

e

An alternative to pre-test is to always report a robust conﬁdence set, which would be

the 5% jackknife AR conﬁdence set in this case. We do see that the AR conﬁdence sets

are wider, yet still informative.

3 Weak Identiﬁcation with Many Instruments

We study the linear IV regression with a scalar outcome Yi, a potentially endogenous

scalar regressor Xi and a K

×

1 vector of instrumental variables Zi:

Yi = βXi + ei,

Xi = Πi + vi,

7






(1)

FF

F

180 instruments
1530 instruments

2.43
1.27

e
13.42
6.17

JIVE-t
(5%)
[0.066,0.13]
[0.024,0.12]

Jackknife AR
(5%)
[0.008,0.20]
[-0.047, 0.20]

JIVE-t
(2%)
[0.059,0.14]
[0.015,0.13]

Jackknife AR
(2%)
[0.0003, 0.21]
[ -0.066,0.22]

Table 1: AK91 Pre-test Results
Notes: Results on pre-tests for weak identiﬁcation and conﬁdence sets for the IV speciﬁcation underlying
Table VII Column (6) of Angrist and Krueger (1991) using the original data. FF is the ﬁrst stage F
F is the statistic introduced in (6). The jackknife AR conﬁdence set
statistic of Stock and Yogo (2005),
is based on analytical test inversion. The conﬁdence sets reported by the two-step procedure with Stock
and Yogo’s guarantee are in bold. The conﬁdence sets reported by the two-step procedure with overall
size of 5% is in italic.

e

for i = 1, ..., N. We denote Y to be the N

1 vector of outcome and X to be the N

1

×

×

vector of endogenous regressors. We collect the transpose of Zi in each row of Z, a N
matrix of instruments. We denote Πi = E[Xi|
endogenous regressor in a non-linear way. All results in this paper hold conditionally on a

Zi] and allow the instruments to aﬀect the

K

×

realization of the instruments. Thus, we treat the instruments as ﬁxed (non-random) and

Πi as some constants. We collect Πi in Π, a N

1 vector. The mean-zero errors (ei, vi)

×

are independent across i but not identically distributed and may be heteroscedastic. We

assume without loss of generality that there are no controls included in our model as they

may be partialled out.

Weak identiﬁcation under small K is studied extensively in the weak IV literature.

For Gaussian homoscedastic errors (ei, vi) and linear ﬁrst stage (Πi = π′Zi), the strength

of the instruments corresponds directly to the concentration parameter, π′Z ′Zπ

σ2
v

where

σ2
v = V ar(vi). The concentration parameter equals the signal-to-noise ratio in the ﬁrst-

stage regression and is related to the bias of the TSLS estimator and the quality of

Gaussian approximation for the TSLS t-statistic. For the general case with homoscedastic

errors, Staiger and Stock (1997) introduced weak instrument-asymptotics in which one

considers a sequence of models so that the concentration parameter converges to a constant

as N

→ ∞

. Under this asymptotic embedding, neither a consistent estimator of β nor a

consistent test of the null hypothesis that β equals some scalar exists, and the test based

on the TSLS t-statistic severely over-rejects.

The magnitude of the concentration parameter is not a good indicator of identiﬁcation

strength when the number of instruments is large. Inspired by Bekker (1994), we model

8

large K by considering K

as N

→ ∞

→ ∞

, with the only restriction that K is at most a

fraction of N. Under this many instrument-asymptotics, Theorem 1 below shows that the

re-scaled concentration parameter π′Z ′Zπ
σ2
v√K
in terms of the consistency of tests.

provides a characterization of weak identiﬁcation

Theorem 1 Assume we have a sample from model (1) with linear ﬁrst stage Πi = π′Zi.
Consider the reduced-form errors (ui, vi) where ui = Yi −
form errors are independently drawn from a Gaussian distribution

βπ′Zi. Assume the reduced-

(0, Ω) with a known

N

nonsingular covariance matrix Ω. Assume that the K

K matrix Z ′Z has rank K and

×
. For any sample of size N let ΨN be the class of all tests of size

K

→ ∞

as N

→ ∞

α for testing the hypothesis H0 : β = β0, that is, any ψ

ΨN is a measurable function

to the interval [0, 1] such that Eβ0,πψ

∈

α for any value of

≤

(Yi, Xi, Zi), i = 1, ..., N

{
RK. Then for any β∗

}

= β0 we have

from

π

∈

lim sup
N

→∞

max
ΨN 
ψ
∈

min
π: π′Z′Zπ
2
v √K
σ

=C

Eβ∗,πψ



< 1.





The setting considered in Theorem 1 is quite favorable: the ﬁrst stage is linear, errors

are Gaussian and homoscedastic with known covariance matrix. So the only unknown

parameters are β and π. Theorem 1 states that even in this favorable setting there exists

no test that consistently diﬀerentiates any β∗ from β0 if the ratio of the concentration

parameter to the square root of the number of instruments is bounded.
Indeed, for
any test ψ we can ﬁnd its guaranteed power Eβ∗,πψ by minimizing over the alternatives
(β∗, π) with bounded ratio of the concentration parameter over √K. We show that

even in this favorable setting the test that achieves the maximum guaranteed power

has guaranteed power strictly less than one asymptotically. With heteroscedasticity of

unknown form, suﬃcient statistics of low dimensions are not known, making the setting

even less favorable. Later we show that in a more general heteroscedastic model we can

construct a robust test that becomes consistent when Π′Π

√K → ∞

.

Theorem 1 can also be used to characterize weak identiﬁcation in terms of consistent

estimation since it implies there exists no consistent estimator for β when the ratio of the

concentration parameter to √K is bounded. Our result complements the literature on esti-

9

6
mation with many instruments. Chao and Swanson (2005) show that with homoscedastic

errors, when K grows proportionally to the sample size the TSLS estimator is consis-

tent only if the concentration parameter grows faster than the number of instruments

K, while LIML and BTSLS estimators are consistent when the concentration parameter

grows faster than √K. However, under heteroscedasticity, even when π′Z ′Zπ

√K → ∞

, LIML

and BTSLS become inconsistent, but JIVE is still consistent, according to Chao et al.

(2012).

The proof of Theorem 1 builds on several classical papers. Following the approach

of Andrews et al. (2006), we ﬁrst reduce the class of tests to those based on a suﬃcient

statistic. Among these tests, the minimal power is achieved by a test invariant to rotations

of the instruments. This observation allows us to further reduce our attention to invariant

tests, which depend on the data only through its maximal invariant under rotations. Then

we derive a limit experiment for K

→ ∞

similar to that derived in Andrews and Stock

(2007). In this limit experiment the minimax power is less than one. Finally we use the

argument of Müeller (2011) to bound the desired asymptotic minimax power using the

minimax power obtained in the limit experiment.

4 Jackknife AR

The goal of this section is to introduce a test robust to weak identiﬁcation in the het-

eroscedastic IV model when the number of instruments, K, is large.

The existing weak IV literature proposes several weak identiﬁcation-robust tests of

the null hypothesis H0 : β = β0, when K is small. These tests have correct size when

the identiﬁcation is weak and become consistent when the identiﬁcation is strong. One

example is the AR test. Speciﬁcally, the IV model (1) implies that under a given null
hypothesis H0 : β = β0, the exogeneity assumption holds E[Z ′e(β0)] = 0 for the im-

−

plied error e(β0) = Y

β0X. Then under mild assumptions, the scaled sample analog

N(0, Σ) satisﬁes a K-dimensional CLT. The AR statistic is deﬁned as

Z ′e(β0)

1
√N
1
N e(β0)′Z
rejects the null hypothesis when the AR statistic exceeds the (1

Σ is a consistent estimator of V ar

⇒
1Z ′e(β0), where

Σ−

b

b

Z ′e

. The AR test

1
√N
α) quantile of the χ2
K

(cid:17)

(cid:16)
−

10

N

1

√K

Φ

i=1
X

=i
Xj

p

b

distribution. The AR test has asymptotically correct size regardless of the value of the ﬁrst

stage coeﬃcients Πi and is asymptotically consistent when an analog of the concentration

parameter grows to inﬁnity.

Generalizing the AR statistic to the large-K setting is challenging for multiple reasons.

Firstly, the covariance matrix Σ has dimension K

K.

Its consistent estimation is

×

problematic if not impossible under general heteroscedasticity. Secondly, the AR statistic

under the null has an improperly centered limit distribution because χ2

K has a very large

mean. Thirdly, the K-dimensional CLT provides a poor approximation to the AR statistic

when K is large.

We propose an analog of the AR test that is heteroscedasticity-robust and weak

identiﬁcation-robust in the presence of a large number of instruments. Denote the projec-

tion matrix P = Z(Z ′Z)−

1Z ′. Our test rejects the null of H0 : β = β0 when the jackknife

AR statistic

AR(β0) =

Pijei(β0)ej(β0)

(2)

exceeds the (1

α) quantile of the standard normal distribution. We defer the discussion

−
of the estimator of the variance

Φ to the next subsection.

To address the challenges with the existing AR statistic, the AR statistic we propose

b

uses the default homoscedasticity-inspired weighting (Z ′Z)−

1 in place of

Σ−

1. With the

(Z ′Z)−

1 weighting, the existing AR statistic has a quadratic form e(β0)′P e(β0). However,

this quadratic form is not centered at zero as it contains the term

b
N
i=1 Piie2

i , and each

summand has positive mean. We thus remove this term from the quadratic form. This

P

re-centering can be referred to as leave-one-out or jackknife. In the context of consistent

estimation under many instruments, this leave-one-out idea was introduced by Angrist et

al. (1999) and fruitfully exploited in a number of papers including Hausman et al. (2012)

and Chao et al. (2012). Recently, this idea has been used in Chao et al. (2014) and

Crudu et al. (2020). In order to create a test of correct size based on our AR statistic,

we use a CLT for quadratic forms proved in Chao et al. (2012) that is restated below.

Assumption 1 Assume P is an N
and there exists a constant δ such that Pii ≤

×

δ < 1.

N projection matrix of rank K, K

as N

→ ∞

→ ∞

11

6
Lemma 1 (Chao et al., 2012) Let Assumption 1 hold for matrix P . Assume the errors
ηi are independent, Eηi = 0, and there exists a constant C such that maxi Eη4

i < C, then

1
√K√Φ

N

i=1
X

=i
Xj

Pijηiηj ⇒ N

(0, 1),

where Φ = 2
K

N
i=1

=i P 2

ijV ar(ηi)V ar(ηj).

j

P
The assumption Pii ≤

P

δ < 1 implies that K

N = 1

N

N

i=1 Pii ≤

δ < 1. This assumption

is often referred to as a balanced design assumption.
P

In the case of group-dummies

instruments, Pii is equal to the ratio of the size of the group that observation i belongs

to over N. Assumption 1 can be checked for any speciﬁc design.

While Lemma 1 requires K

→ ∞

, the Gaussian approximation may work well for

smaller K as well. For example, if K is ﬁxed and errors are homoscedastic, then

1
√K√Φ

N

i=1
X

=i
Xj

Pijηiηj ⇒

χ2

K

K −
√2K

as N

.

→ ∞

We prove this statement in the Supplementary Appendix S4. While the limit here is

not Gaussian it is very well approximated by a standard normal distribution even for
relatively small K. The random variable χ2

K
K −
√2K
normal distribution at most 7% of the time for all K, and at most 6% of the time for

exceeds the 95% quantile of the standard

K > 40.

4.1 Variance estimation

In order to conduct asymptotically valid inference based on the normal approximation in

Lemma 1, we need an estimator for the scale parameter Φ, which is consistent under the

null. One ‘naive’ estimator that achieves this is

Φ1 = 2
K

N
i=1

j

=i P 2

ije2

i (β0)e2

j (β0), which

uses the square of the implied error as an estimator for the i-th error variance. Under

P

P

b

the null when ei(β0) = ei, the estimator

Φ1 is consistent under relatively mild conditions.

However, using

Φ1 in a test would result in poor power. To see this, note that under an

b

alternative value of the parameter β = β0 + ∆, we can plug in the ﬁrst stage and write the

b

12

6
6
6
6
implied error ei(β0) = Yi −
random term ηi = ei + ∆vi:

β0Xi as the sum of a non-trivial mean ∆Πi and a mean-zero

ei(β0) = ∆Πi + ηi.

(3)

The AR statistics of a form similar to (2) with

Φ1 has been recently and independently

proposed by Crudu et al. (2020). The aforementioned paper establishes robustness of

b

their proposed test towards weak identiﬁcation and heteroscedasticity in terms of size.

While squaring ei(β0) makes it an unbiased estimator for V ar(ei) under the null, it is

biased under the alternative when ∆

= 0. The bias in

Φ1 grows at the same order as the

fourth power of ∆, which brings down the power of the test against distant alternatives.

b

In Section 4.2, we discuss the power implications of the ‘naive’ estimator in more detail.

In order to remove the bias in e2

i (β0) under the alternatives, one may residualize the

implied error before squaring. However, this introduces a bias under the null. Denote

M = I
error is biased E(Mie)2

−

P and let Mi be the ith row of M. Even under the null, the squared residualized

= V ar(ei). This is because the squared residual contains not only

the squared error ei but also the square of regression estimation mistake. The latter can

be large when the number of regressors K is large.

This bias can be removed successfully using the cross-ﬁt variance estimator suggested

in Kline et al. (2020) and Newey and Robins (2018). Namely, they show that a product

of the implied error and residual achieves both goals: it removes the linearly predictable

part of the implied error and remains an unbiased estimator of the variance

E

eiMie
Mii (cid:21)

(cid:20)

= V ar(ei).

Our challenge is that the scale parameter Φ deﬁned in Lemma 1 is a quadratic form

with a double summation. Residuals Mie(β0) and Mje(β0) are correlated since they

contain the same estimation mistake. One can show that

E [eiMieejMje] = (MiiMjj + M 2

ij)V ar(ei)V ar(ej).

13

6
6
Our proposed estimator of the scale parameter Φ re-weights each term in the summation

to remove the bias described above:

Φ =

2
K

b

N

i=1
X

=i
Xj

P 2
ij
MiiMjj + M 2
ij

[ei(β0)Mie(β0)] [ej(β0)Mje(β0)] .

(4)

We establish the consistency of

Φ under the null and extend this result to local alternatives.

Assumption 2 Errors ǫi, i = 1, ..., N are independent with Eǫi = 0, maxi E
k

b

ǫik

6 <

,

∞

and for some constants c∗ and C ∗ that do not depend on N

c∗

≤

min
i

min
x

x′V ar(ǫi)x
x′x

max
i

max
x

≤

x′V ar(ǫi)x
x′x

C ∗.

≤

Theorem 2 Let Assumption 1 hold for matrix P and Assumption 2 hold for errors ei,

then for β = β0, we have

b
Φ
Φ →

p 1 as N

.

→ ∞

Theorem 2 combined with Lemma 1 implies that under the null H0 : β = β0 our proposed

AR statistic has an asymptotically standard normal distribution. Since no assumption

about identiﬁcation is made, the resulting AR test has asymptotically correct size regard-

less of the strength of identiﬁcation.

Theorem 3 Let Assumption 1 hold for matrix P and Assumption 2 hold for errors ǫi =

(ei, vi)′, and Π′MΠ

≤
0, we have

C
K Π′Π. Then for β = β0 + ∆, where ∆ may depend on N such that
b
Φ
Φ →

p 1 as N

→ ∞

.

∆2

Π′Π
K →

·

Theorem 3 establishes the consistency of the variance estimator when the null hy-

pothesis does not hold. We use Theorem 3 to derive local power curves of the AR test

discussed in the next section. The variance estimator (4) residualizes the implied errors

Mie(β0) to remove non-trivial mean of e(β0) under the alternative. The residualization

is complete if the ﬁrst stage is linear Πi = π′Zi. We do not impose such an assumption

in Theorem 3. Instead we require that the approximation of Πi by a linear combination

of instruments improves with the number of instruments as measured by the norm of

the approximation mistake, Π′MΠ. In their Assumption 4, Chao et al. (2012) impose

that Π′M Π

N →

0, which may be weaker or stronger than our assumption Π′MΠ

C
K Π′Π

≤

14

6
depending on the identiﬁcation strength. The variance estimation in Chao et al. (2012)

is valid only under strong identiﬁcation as it relies on the consistency of the JIVE esti-

mator. The residuals from structural equation, with the JIVE estimate for β plugged in,

approximate the structural errors well. In contrast, our variance estimator remains valid

under weak identiﬁcation when no consistent estimator for β exists. This is why we need

stricter assumptions on the linear approximation to produce reliable residuals under weak

identiﬁcation.

4.2 Power of the Jackknife AR test

Let us introduce a jackknife measure of the information contained in the instruments:

N

µ2 =

PijΠiΠj.

i=1
X

=i
Xj

For the linear ﬁrst stage Πi = π′Zi, we have µ2 = π′Z ′Zπ

−
π′Z ′Zπ. Thus, the two measures µ2
√K

P

N
i=1 Pii(π′Zi)2. Assumption 1
are

and π′Z ′Zπ

√K

guarantees that (1

δ)π′Z ′Zπ

−

µ2

≤

≤

of the same order and increase to inﬁnity or not simultaneously. In the general case where

the instruments may aﬀect the endogenous regressor in an arbitrarily non-linear way, the

linear IV regression only uses the projection of Π onto the linear space of the instruments.

Thus the projection matrix appears naturally in our measure of identiﬁcation strength.

The parameter µ2 can be considered as a jackknife generalization of the parameter π′Z ′Zπ

to non-linear case.

Theorem 4 Let Pβ be a probability measure describing the distribution of AR(β0) deﬁned

in (2) and (4) under model (1) with parameter β = β0 + ∆. Assume that the sequence of

ﬁrst stage parameters Π satisﬁes the following assumptions: Π′MΠ

≤

C

K Π′Π and Π′Π

K →

0

as N

→ ∞

. If Assumption 1 holds and the errors ǫi = (ei, vi)′ satisfy Assumption 2, then

for any positive constant c we have:

lim
N
→∞

|

sup
∆

2

sup
z

c

|

≤

AR(β0) < z

} −

F

z
(cid:18)

−

= 0,

(5)

Pβ{
(cid:12)
(cid:12)
(cid:12)
(cid:12)

15

∆2µ2
√KΦ (cid:19)(cid:12)
(cid:12)
(cid:12)
(cid:12)

where F (

·

) is the standard normal cdf. If the sequence of ﬁrst stage parameters additionally

6
satisﬁes the condition µ2

√KΦ → ∞

asymptotically consistent:

, then for any ﬁxed ∆

= 0 the jackknife AR test is

lim
N
→∞

Pβ{

AR(β0)

z1

−

α}

≥

= 1

where z1

−

α is the (1

−

α) quantile of the standard normal distribution.

Equation (5) of Theorem 4 characterizes the local power curves of the jackknife AR test.

The power under the alternative β = β0 + ∆ is a function of the distance ∆ between the

alternative β and the null β0, the number of instruments K, a measure of identiﬁcation

strength µ2 and the degree of uncertainty √Φ. Our jackknife AR statistic can be negative,

unlike the AR statistic from the small-K case which is always non-negative. We reject

the null when AR(β0) exceeds the (1

−

α) quantile of the standard normal distribution.

Under the alternative β = β0 + ∆, the AR statistics has a positive drift and produces

non-trivial power for both positive and negative ∆. The second statement of Theorem 4

µ2

√K√Φ → ∞

.

shows that the AR test consistently distinguishes β from β0 as long as

Theorem 4 implies that µ2

√K → ∞

is a suﬃcient condition for the consistency of the

jackknife AR test in a model with a linear ﬁrst stage . This complements Theorem 1 which

implies that π′Z ′Zπ

√K → ∞

is necessary for the consistency of any test. This condition has

appeared before in Chao et al.

(2012) as a suﬃcient condition for the consistency of

the JIVE estimator and asymptotic validity and consistency of the JIVE t-test. The

important diﬀerence between the proposed jackknife AR test and the JIVE t-test is that

even under weak identiﬁcation ( π′Z ′Zπ

), the former maintains asymptotically valid

√K 6→ ∞

size, while the latter does not. It is worth noticing that the condition Π′Π

0 imposed

K →

by Theorem 4 is quite weak as it covers both weakly and strongly identiﬁed cases.

Power implications of variance estimation. While the leave-one-out AR test with

our proposed cross-ﬁt variance estimator is consistent against ﬁxed alternatives when

identiﬁcation is strong, the same test with a ‘naive’ variance estimator

Φ1 is in general not

consistent. The diﬀerence between the implied error ei(β0) and ηi as deﬁned in equation

b

(3) results in that the diﬀerence between

Φ1 and Φ is a fourth degree polynomial of ∆.

This makes the stochastic shift for the AR statistic with the naive variance estimator to

b

16

6
stabilize at the ﬁnite level when ∆

:

→ ±∞

∆2µ2
√K√Φr

≈

Φ

c∆4 + Φ →

C

±

as ∆

,

→ ±∞

∆2µ2

√K

Φ1

q

while it increases unboundedly for the statistic with the cross-ﬁt variance estimator. Here

b

c = 2
K

N
i=1

j

=i P 2

ijΠ2

i Π2
j .

Theoretical inconsistency of a test may or may not result in power diﬀerences of

P

P

empirical relevance for commonly used signiﬁcance levels. This depends partially on

whether the level at which the stochastic shift stabilizes is above the typically used critical
values. In very strongly identiﬁed cases where µ2
√K

is large, we may detect no signiﬁcant

diﬀerence between two statistics for alternatives with relatively small ∆, implying a small

power diﬀerence. While there is an increasingly large diﬀerence in realized values of

statistics for large ∆, such a diﬀerence might not translate to a power diﬀerence either since

both tests would reject. For example, in the AK91 example discussed in Section 2, using

the ‘naive’ variance estimator

Φ1 would yield nearly identical jackknife AR conﬁdence

sets. For the speciﬁcation that uses 1530 instruments, jackknife AR conﬁdence sets based

b

on the ‘naive’ variance estimator are [

0.048, 0.202] (5%) and [

0.662, 0.224] (2%), which

−

−

are very close to the ones based on

Φ as reported in Table 1.

We ﬁnd larger power diﬀerences for moderately weak instruments under a sparse ﬁrst

b

stage. The divergence between two statistics depends positively on parameter c. While
large values of the ﬁrst stage coeﬃcients Πi tend to produce large values of both µ2
√K
and c, the relation between the last two is not proportional. A more sparse ﬁrst stage

tends to produce higher values of c (and larger power diﬀerences) for the same level of
the identiﬁcation strength µ2
√K

, and therefore more stark power loss from using the ‘naive’

variance estimator. Based on a simple simulation design, Figure 1 plots the power curves

for the leave-one-out AR test with diﬀerent variance estimators under a sparse ﬁrst stage

(a) and a dense ﬁrst stage (b). We include additional power comparisons in Section 6 and

in the Supplementary Appendix.

17

6
0.7

0
=

0.6

:

0

H

,
t
s
e
T
%
5

f
o

y
t
i
l
i

b
a
b
o
r
P
n
o
i
t
c
e
e
R

j

0.5

0.4

0.3

0.2

0.1

0

-5

0.7

0
=

0.6

:

0

H

,
t
s
e
T
%
5

f
o

y
t
i
l
i

b
a
b
o
r
P
n
o
i
t
c
e
e
R

j

0.5

0.4

0.3

0.2

0.1

0
-5

-4

-3

-2

-1

0

1

2

3

4

5

-4

-3

-2

-1

0

1

2

3

4

5

cross-fit

(a) sparse, µ2
√K

naive
= 2.5

cross-fit

(b) dense, µ2
√K

naive
= 2.5

Figure 1: Power curves for leave-one-out AR tests with cross-ﬁt (blue line) and naive (red dash) variance
estimators under sparse vs. dense ﬁrst stage. The instruments are K = 40 balanced group indicators.
Sample size is N = 200. Number of simulation draws is 1,000. Details of the simulation design can be
found in the Appendix.

5 Pre-test for Weak Identiﬁcation

In a prominent paper, Stock and Yogo (2005) introduced a pre-test for weak identiﬁcation

that has gained enormous popularity in applied work. In homoscedastic IV models with

small K, the concentration parameter fully characterizes the worst bias of the TSLS as a

fraction of the OLS bias and the worst rejection rate of TSLS-Wald test. Stock and Yogo

(2005) suggest a set of cut-oﬀs for the ﬁrst stage F statistic, above which a researcher

can guarantee with high (prespeciﬁed) probability that the bias of TSLS is not larger

than 10% of the OLS bias, or that the TSLS-Wald statistic does not over-reject by more

than 5%. The cut-oﬀs depend on the goal (bias or size) and the number of instruments.

However, these details seem to be mostly disregarded in empirical practice that uses a

cut-oﬀ of 10, regardless of the goal or the number of instruments.

As with any procedure of such generality, the Stock-Yogo pre-test suﬀers from multi-

ple drawbacks. First, the pre-test is valid only if the model is homoscedastic. Andrews

(2018) shows that in models calibrated to commonly-used data sets with heteroscedas-

ticity one may ﬁnd cases with the ﬁrst stage F statistics exceeding 1000, that have large

over-rejections of the TSLS-Wald test. Second, the TSLS estimator is less robust to weak

identiﬁcation when K is large. In a homoscedastic model when K is growing proportion-

18

 
 
 
 
 
 
 
 
 
 
ally to the sample size, the TSLS estimator is consistent only if π′Z ′Zπ

, while LIML

K → ∞

and BTSLS estimators are consistent when π′Z ′Zπ

as shown in Chao and Swanson

√K → ∞

(2005). In this case, the pre-test becomes too conservative. Indeed, if π′Z ′Zπ

but

√K → ∞

, then the pre-test most likely declares weak identiﬁcation as the expectation

π′Z ′Zπ
K

9

∞

of the ﬁrst stage F equals to π′Z ′Zπ
Kσ2
v

+ 1, even though there exist consistent estimators and

a reasonable Wald-test can be constructed.

We propose a new pre-test for weak identiﬁcation, that allows us to assess the reliability

of the JIVE-Wald test. Our pre-test uses statistic

N

1

√K

Υ

i=1
X

=i
Xj

F =

e

p

b

PijXiXj,

(6)

here

Υ = 2
K

i

j

=i

P 2
ij
MiiMjj +M 2
ij

XiMiXXjMjX is an estimate of the variance Υ deﬁned

in (13). The JIVE-Wald test uses the JIV2 estimator introduced in Angrist et al. (1999):

P

P

b

βJIV E =

b

N
i=1
N
P
i=1

=i PijYiXj
j
=i PijXiXj

P
j

.

P

P

We use the following estimator of the JIVE variance, that is a cross-ﬁt version of the

estimator derived in Chao et al. (2012):

N
i=1

=i PijXj

j

2 beiMibe
Mii

+

N
i=1

P 2

ijMiX

eiMjX

ej

j

=i

P

(cid:16)P

(cid:17)

N
i=1

P

P
=i PijXiXj

j

,

2

e

(cid:17)

b

b

V =

b

P

(cid:16)P
P 2
ij
MiiMjj+M 2
ij

Xi

βJIV E and

P 2

ij =

. The Wald statistic is deﬁned as W ald(β0) =

where
ei = Yi−
( b
β0)
βJ IV E−
b
b
V

2

. Our choice of JIVE is based on two considerations. First, according to Haus-

b

e

man et al. (2012), in a heteroscedastic IV model, when π′Z ′Zπ

√K → ∞

, LIML and BTSLS

become inconsistent, but JIVE is consistent. Second, the JIVE estimator is a ratio of

two quadratic forms similar to the jackknife AR statistic, which motivates the following

characterization.

Theorem 5 Let Assumption 1 hold for matrix P and Assumption 2 hold for errors ǫi =

19

6
6
6
6
6
6
6
(ei, vi)′. Assume that Π′MΠ

K and Π′Π
CΠ′Π
K 2/3

≤

0 as N

→

→ ∞

. Then for β = β0,

P

W ald(β0)

x,

F

y

≤

≤

P

−

1

(

o

−

ξ2
ν + ξ2
2̺ ξ

ν2 ≤

x, ν

y

≤

e
where ξ and ν are two normal random variables with means 0 and

sup
x,y (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

n

0,

(7)

→

)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
, unit variances

µ2
√K√Υ

and correlation coeﬃcient ̺ deﬁned in equation (13).

Theorem 5 shows that the distribution of the JIVE-Wald statistics can be quite dif-
µ2
√K√Υ

ferent from its conventional χ2

is large, then most

1 limit when

is small. If

µ2
√K√Υ

realizations of the random variable ν are large as well and the limit of the JIVE-Wald is

close to the distribution of ξ2, which is χ2

1. This suggests that

µ2
√K√Υ

is a good measure

for identiﬁcation strength. The assumption Π′Π
K 2/3

→

both weakly and strongly identiﬁed cases.

0 is somewhat restrictive but covers

Using Theorem 5 we create a pre-test for one deﬁnition of weak identiﬁcation following

Stock and Yogo (2005), which stipulates whether the actual size of the conventional 5%

JIVE Wald test could exceed 10%. First, we calculate the worst asymptotic rejection rate
of the JIVE-Wald test for a given theoretical strength of identiﬁcation S = µ2

:

√K√Υ

Rmax
α

(S) = max
1,1]
̺

[
∈

−

PS,̺

1

(

−

ξ2
ν + ξ2
2̺ ξ

ν2 ≥

χ2
1,1

−

,

α

)

where PS,̺ is the probability distribution of (ξ, ν) as described in Theorem 5. The quantity
Rmax
α
at ̺ = 1). Speciﬁcally S = µ2

can be straightforwardly obtained from simulations (the maximum rejection occurs

> 2.5 implies Rmax

5% (S) < 10%.

√K√Υ

The strength of identiﬁcation parameter as measured by S = µ2

√K√Υ

is unknown in

practice. Theorem 5 also allows us to construct a 5%-test for the null hypothesis that the
unknown strength of identiﬁcation parameter S = µ2

is lower than 2.5. This test is

√K√Υ

based on the statistic

F and rejects whenever

F > 4.14. This test is therefore the analog

to Stock and Yogo (2005) ﬁrst stage F pre-test, which tests whether the actual size of the

e

e

conventional 5% JIVE Wald test could exceed 10%.

An advantage of the new pre-test based on

F for weak identiﬁcation is that when it

is combined with any weak identiﬁcation robust test, such as our jackknife AR test, to

e

20

be used when

F is below the cut-oﬀ, we can guarantee that the size of such two-step

procedure is within a tolerance bound of 10% from the declared nominal size.

e

Corollary 1 Let all assumptions of Theorem 5 hold. Then a two-step test for the null

hypothesis H0 : β = β0 that accepts the null if

F > 4.14 and W ald(β0) < χ2

1,0.95 or if

F

≤

4.14 and AR(β0) < z0.95, has an asymptotic size smaller than 15%.

e

e
The attraction of the two-step procedure is that conﬁdence sets based on the JIVE-Wald

test are relatively easy to construct and are well understood by the practitioners. As

we illustrate in simulations, the jackknife AR conﬁdence sets tend to be wider than the

JIVE-Wald conﬁdence sets when identiﬁcation is strong. Simulations also suggest the

Bonferroni bounds derived in Corollary 1 tend to be conservative, as the actual size of

the two-step test does not exceed 7%.

The 5% Wald conﬁdence set with 10% tolerance described in Corollary 1 is the leading

case considered by Stock and Yogo (2005). However, Theorem 5 also allows us to create

a two-step procedure with the overall size of 5% or 10% by adjusting the cut-oﬀ for

F

and using Wald and the jackknife AR conﬁdence sets with smaller nominal sizes (and
e
correspondingly larger critical values). Table 2 tabulates a few combinations of valid cut-

oﬀs and critical values. As an example of a 5% two-step procedure, the researcher may

compare the

F statistic with 9.98. If

F exceeds the cut-oﬀ, the researcher reports a JIVE-

Wald conﬁdence set that uses the 98% quantile of the χ2

1 as the critical value. Otherwise,

e

e

the researcher reports a jackknife AR conﬁdence set that uses the 98% quantile of the

standard normal distribution as the critical value. We apply this procedure to the AK91

example discussed in Section 2 and report the results in Table 1 in italic.

6 Return to Education: Monte Carlo Simulations

In this section we conduct Monte Carlo simulations to show that the jackknife AR and

the pre-test we develop are robust to many weak instruments unlike canonical IV estima-

tors. To maintain the practical relevance, we attempt to preserve the structure of AK91

as described in Section 2. Speciﬁcally, we adopt the simulation design by Angrist and

Frandsen (2019). There is very little endogeneity in the original AK91, which makes it

21

Cut-oﬀ for

F Wald-JIVE Jackknife AR Overall Size

e

7.15
9.98
12.86
5.01
7.65

5.41 (2%)
5.41 (2%)
5.41 (2%)
3.84 (5%)
3.84 (5%)

2.32 (1%)
2.05 (2%)
1.96 (2.5%)
2.05 (2%)
1.75 (4%)

5%
5%
5%
10%
10%

Table 2: Critical Values for Two-step Procedure
Notes: The two-step procedure switches between the Wald-JIVE test and the jackknife AR test based
F
on the cut-oﬀ for
is less than the cut-oﬀ, the jackknife AR test is conducted. In the parentheses we list the nominal size
e
associated with the critical values. The last column reports the overall size for the two-step procedure.

F is greater than the cut-oﬀ, the Wald-JIVE test is conducted. When

F . When

e

e

N

K Avg.

F

4,923
3,209
1,599

154
135
111

e

4.99
3.35
1.77

µ2
√K√Υ

4.91
3.29
1.74

OLS 2SLS
bias
bias
0.17
0.26
0.19
0.26
0.21
0.26

2SLS LIML LIML JIVE JIVE
size
bias
size
size
5%
0.6% -0.03
5.2%
2.7% -0.06
3.6%
14.5% 1.22

bias
96.6% -0.001
95.7% -0.05
92.3% -0.89

Table 3: AK91 Simulation Results: Bias of Diﬀerent Estimators and Size of Non-robust Tests

hard to study the biases of diﬀerent estimators. Thus, we follow Angrist and Frandsen

(2019) to introduce additional omitted variable bias to the simulated data. The simu-

lated data has a nonlinear ﬁrst stage and is heteroscedastic. We deviate from Angrist

and Frandsen (2019) in two respects. First, we vary the sample size N of the simulated

data to be 1.5%, 1% and 0.5% of the original sample size. This is to vary the identiﬁca-

tion strength. We report the identiﬁcation strength by

µ2
√K√Υ

as well as the average

F

across simulations. Simulations with sample size equal to 1.5% of the original sample size
e
produce strong identiﬁcation in our deﬁnition, 1% still produce strong identiﬁcation but

close to the weak identiﬁcation region, while 0.5% produce weak identiﬁcation.When we

reduce the sample size we also need to exclude the instruments of the groups that are no

longer populated. Second, both in data simulation and in estimation we do not include

controls in order to isolate the implications of many instruments. The Appendix provides

more details on our simulation design.

We evaluate the performance of common estimators and tests based on 1000 simulation

draws. In Table 3, we report the bias and size of Wald tests based on OLS, 2SLS, LIML

and JIVE estimators. For the Wald test based on the LIML estimator, we calculate the

22

N

K Avg. FF Avg.

F

4,923
3,209
1,599

154
135
111

1.63
1.44
1.24

e

4.99
3.35
1.77

µ2
√K√Υ
4.91
3.29
1.74

jackknife AR pre-test

two-step test

5.1%
5.6%
6.3%

70.5%
26.7%
4.5 %

5.8%
6.6%
7.2%

Table 4: AK91 Simulation Results: Size of Robust Tests

standard errors as in Hansen et al.

(2008). While Hansen et al.

(2008) correct the

canonical standard error estimator to be robust to many instruments, this test is not

robust to heteroscedasticity as LIML itself is inconsistent under heteroscedasticity. For

the Wald test based on the JIVE estimator, we calculate the heteroscedasticity-robust

standard errors as described in Section 5.

We ﬁnd that due to many instruments 2SLS has large bias even under strong identiﬁ-

cation. While Hausman et al. (2012) show LIML is inconsistent under many instruments

and heteroscedasticity, LIML is not too biased in our simulated data, as long as identiﬁ-

cation is not weak. We ﬁnd that JIVE has low bias when identiﬁcation is strong, but its

bias increases when identiﬁcation is weak. The Wald test based on either LIML or JIVE

is not robust to many weak instruments, and we ﬁnd substantial size distortion for LIML

under weak identiﬁcation. Surprisingly we do not ﬁnd large size distortion for JIVE.

In Table 4 we report the rejection frequency of the robust test we developed in this

paper based on the jackknife AR test statistic. We ﬁnd that the jackknife AR controls

size even under weak identiﬁcation. Our proposed pre-test also controls size and is able to

switch to the JIVE-Wald test when identiﬁcation is strong. In contrast, the ﬁrst stage F

statistics of Stock and Yogo (2005) (FF) are very small even under strong identiﬁcation,

which makes it not very informative.

In Table 5 we compare the length of conﬁdence intervals formed by inverting various

tests. In particular, when identiﬁcation is strong, jackknife AR conﬁdence sets are longer

(less eﬃcient) but are not unreasonably long compared to the Wald tests based on LIML

and JIVE. In this case, a pre-test can improve the eﬃciency by switching to the Wald

test based on JIVE. As with the canonical AR test, the jackknife AR test can result in

conﬁdence intervals with inﬁnite length. We report the probability of inﬁnite length in

the last column of Table 5, and note that such probability increases as identiﬁcation gets

23

N

K Avg.

F

4,923
3,209
1,599

154
135
111

e

4.99
3.35
1.77

µ2
√K√Υ
4.91
3.29
1.74

2SLS LIML JIVE jackknife AR inﬁnite jackknife AR

0.18
0.20
0.24

1.14
1.23
1.46

0.81
1.41
5244

1.66
2.77
6.90

1%
11%
49.6%

Table 5: AK91 Simulation Results, Length of Conﬁdence Interval

(a) “cross-ﬁt” variance estimator

N

K Avg.

F

4,923
3,209
1,599
796

154
135
111
77

e

4.99
3.35
1.77
0.92

N

K Avg.

F

4,923
3,209
1,599
796

154
135
111
77

e

4.99
3.35
1.77
0.92

µ2
√K√Υ
4.91
3.29
1.74
0.91

µ2
√K√Υ
4.91
3.29
1.74
0.91

jackknife AR size

two-step test CI length inﬁnite CI

5.1%
5.6%
6.3%
6.5%

5.8%
6.6%
7.2%
6.5%

1.66
2.77
6.90
10.26

1%
11%
49.6%
74.4%

(b) “naive” variance estimator

jackknife AR size

two-step test CI length inﬁnite CI

4.9%
5.4%
5.9%
5.4%

5.7%
6.5%
6.8%
5.4%

1.81
2.99
6.95
8.86

1%
11.1%
51.1%
77.3%

Table 6: Angrist and Krueger (1991) Simulation Results, Comparison of Variance Estimation

weaker.

To complement the discussion in Section 4.2, we compare the performance of the jack-

knife AR test based on our proposed “cross-ﬁt” variance estimator with that based on the

“naive” variance estimator. Since power loss does not show up with strong identiﬁcation,

we further reduce the sample size to be 0.25% of the original size. In Table 6 we conﬁrm

that the size is not aﬀected by the choice of variance estimator. Figure 2 demonstrates

the diﬀerence in power for the jackknife AR tests with the cross-ﬁt and the naive variance

estimators. The “cross-ﬁt” variance estimator performs slightly better in terms of power

when identiﬁcation is weak. As shown in the last two columns of Table 6, the power

diﬀerence is also reﬂected in fewer unbounded conﬁdence intervals based on the jackknife

AR test, and shorter conﬁdence intervals when the bounded using the “cross-ﬁt" variance

estimator.

24

.

1
0
=

:

0

H

,
t
s
e
T
%
5

f

o

y
t
i
l
i

b
a
b
o
r
P
n
o

i
t
c
e
e
R

j

0.6

0.5

0.4

0.3

0.2

0.1

0

.

1
0
=

:

0

H

,
t
s
e
T
%
5

f

o

y
t
i
l
i

b
a
b
o
r
P
n
o

i
t
c
e
e
R

j

0.35

0.3

0.25

0.2

0.15

0.1

0.05

0

-2

-1

0

1

2

3

-2

-1

0

1

2

3

cross-fit

naive

(a) N = 1599, K = 111

cross-fit

naive

(b) N = 796, K = 77

Figure 2: Power curves with varying identiﬁcation strength, H0 : β = 0.1

7 Conclusion

In this paper, we focus on identiﬁcation for linear IV models with many instruments. In

this environment, we characterize weak identiﬁcation as a situation where an analog of

the concentration parameter stays bounded relative to the square root of the number of

instruments in large samples. We introduce a jackknifed version of the AR test that is

robust to our deﬁnition of weak identiﬁcation and heteroscedasticity. We also propose a

pre-test for weak identiﬁcation and correspondingly a two-step testing procedure in the

spirit of Stock and Yogo (2005). Unlike the pre-test proposed by Stock and Yogo (2005),

our two-step test controls size distortion even under heteroscedasticity and with many

instruments. As an empirical example, our pre-test rejects weak identiﬁcation in Angrist

and Krueger (1991) where up to 1,530 instruments are used.

Data Availability Statement

The data underlying this article are available in “Replication package for Inference with

Many Weak Instruments", at https://doi.org/10.5281/zenodo.5546157.

References

Anatolyev, S., and Gospodinov, N. (2011). “Speciﬁcation Testing in Models with Many

Instruments." Econometric Theory 27, 427–441.

25

 
 
 
 
 
 
 
 
 
 
Andrews, I. (2018). “Valid Two-Step Identiﬁcation-Robust Conﬁdence Sets for GMM."

The Review of Economics and Statistics, 100, 337–348. Supplementary Appendix

Andrews, D.W.K., and Stock, J.H. (2007). “Testing with many weak instruments.” Jour-

nal of Econometrics 138, 24–46.

Andrews D, Moreira M, Stock J. (2006).

“Optimal two-sided invariant similar tests of

instrumental variables regression." Econometrica 74:715–752

Angrist, J.D., and Frandsen B. (2019). “Machine Labor" NBER working paper 26584.

Angrist, J.D., Imbens, G.W., and Krueger, A.B. (1999). “Jackknife instrumental variables

estimation.” Journal of Applied Econometrics 14, 57–67.

Angrist, J.D., and Krueger, A.B. (1991).

“Does Compulsory School Attendance Aﬀect

Schooling and Earnings?" The Quarterly Journal of Economics 106, 979–1014.

Bekker, P.A. (1994). “Alternative Approximations to the Distributions of Instrumental

Variable Estimators.” Econometrica 62, 657–681.

Bhuller, M., Dahl, G.B., Loken, K.V. and M. Mogstad (2020): “Incarceration, Recidivism,

and Employment,” Journal of Political Economy, 128(4), 1269–1324.

Bound, J., Jaeger, D.A. and Baker, R.M. (1995) “Problems with Instrumental Variables

Estimation when the Correlation between the Instruments and the Endogenous Explana-

tory Variable is Weak,” Journal of the American Statistical Association, 90:430, 443–450.

Chao, J.C., Hausman, J.A., Newey, W.K., Swanson, N.R., and Woutersen, T. (2014).

“Testing overidentifying restrictions with many instruments and heteroskedasticity.” Jour-

nal of Econometrics 178, 15–21.

Chao, J.C., and Swanson, N.R. (2005). “Consistent Estimation with a Large Number of

Weak Instruments." Econometrica 73, 1673–1692.

Chao, J.C., Swanson, N.R., Hausman, J.A., Newey, W.K., and Woutersen, T. (2012).

“Asymptotic Distribution of JIV in a heteroscedastic IV Regression with Many Instru-

ments.” Econometric Theory 28, 42–86.

Crudu, F., Mellace, G., and Sandor, Z. (2020). “Inference in Instrumental Variables Mod-

els with Heteroskedasticity and Many Instruments.” Econometric Theory, forthcoming.

Dobbie, W., Goldin, J., and Yang, C.S. (2018).

“The Eﬀects of Pretrial Detention on

Conviction, Future Crime, and Employment: Evidence from Randomly Assigned Judges."

26

American Economic Review 108, 201–240.

Fama, E.F., and MacBeth, J.D. (1973). “Risk, Return, and Equilibrium: Empirical Tests."

Journal of Political Economy 81, 607–636.

Hansen, C., Hausman, J., and Newey, W. (2008). “Estimation With Many Instrumental

Variables." Journal of Business & Economic Statistics 26, 398–422.

Hausman, J.A., Newey, W.K., Woutersen, T., Chao, J.C., and Swanson, N.R. (2012).

“Instrumental variable estimation with heteroscedasticity and many instruments.” Quan-

titative Economics 3, 211–255.

Kleibergen F. (2002). “Pivotal statistics for testing structural parameters in instrumental

variables regression.” Econometrica 70:1781–1803.

Kline, P., Saggio, R., and Sølvsten, M. (2020). “Leave-out estimation of variance compo-

nents.” Econometrica 88, 1859–1898.

Maestas, N., Mullen, K.J., and Strand, A. (2013).

“Does Disability Insurance Receipt

Discourage Work? Using Examiner Assignment to Estimate Causal Eﬀects of SSDI Re-

ceipt.” American Economic Review 103, 1797–1829.

Müller, U. K. (2011). “Eﬃcient Tests Under a Weak Convergence Assumption.” Econo-

metrica 79 (2): 395–435.

Newey, W. (2004). “Many Instrument Asymptotics.”

Newey, W.K., and Robins, J.R. (2018).

“Cross-Fitting and Fast Remainder Rates for

Semiparametric Estimation.”

Newey, W.K., and Windmeijer, F. (2009). “Generalized Method of Moments With Many

Weak Moment Conditions.” Econometrica 77, 687–719.

Sampat, B., and Williams, H.L. (2019). “How Do Patents Aﬀect Follow-On Innovation?

Evidence from the Human Genome.” American Economic Review 109, 203–236.

Shanken, J. (1992). “On the Estimation of Beta-Pricing Models." The Review of Finan-

cial Studies 5, 1–33.

Staiger, D., and Stock, J.H. (1997). “Instrumental Variables Regression with Weak In-

struments.” Econometrica 65 (3): 557–86.

Stock, J.H., and Yogo, M. (2005).

“Testing for weak instruments in Linear IV regres-

sion. In Identiﬁcation and Inference for Econometric Models: Essays in Honor of Thomas

27

Rothenberg," pp. 80–108.

8 Appendix with Proofs

Let C be a universal constant (that may be diﬀerent in diﬀerent lines but does not depend

on N or K).

Proof of Theorem 1. Denote A to be an upper-triangular matrix, such that AΩA′ =

I2. The suﬃcient statistic in model (1) is

ξ1

ξ2









= (A

IK)

⊗

(Z ′Z)−

(Z ′Z)−

· 



1/2Z ′Y
1/2Z ′X 


N

∼

βΠ
Π 
e


, I2K










(8)

where

β = (1, 0)A(β, 1)′ is a (known) linear one-to-one transformation of β. Denote the

corresponding null and alternative as

e

β0 and

β∗. We denote also Π = (Z ′Z)1/2π

σv

, which

is one-to-one transformation of π.

It is enough to restrict attention to the tests that

e

e

depend on the data through suﬃcient statistics only. Indeed, for any test ψ
construct a test ψS = E(ψ

ξ1, ξ2) which depends on the data only through the suﬃcient

ΨN we may

∈

statistics. Due to the law of iterated expectations the size and the power of ψS is the

|

same as the initial ψ.

Let

U

be the group of rotations on RK, that is U

are such U ′U = IK. No-

∈ U

tice that the model is invariant to group

, namely if (ξ1, ξ2) satisfy model (8) with

U

parameters (
β, Π) then (Uξ1, Uξ2) satisfy model (8) with parameters (
Π′Π = (UΠ)′(UΠ). This implies that for any function f we have E
β,Π)f (Uξ1, Uξ2) =
e
(
e
e
E
β,U Π)f (ξ1, ξ2).
e
(
We call a test ψ = ψ(ξ1, ξ2) invariant to rotations iﬀ for any U

β, UΠ). Note that

we have

∈ U

ψ(Uξ1, Uξ2) = ψ(ξ1, ξ2) for all realizations of (ξ1, ξ2). The maximum in Theorem 1 is

achieved at an invariant test.

Indeed, take any test ψ

ΨN that has size α, that is,

∈

E
β0,Π)ψ(ξ1, ξ2)
e
(

ψ(Uξ1, Uξ2)dU,
where the integral is taken uniformly over the unit sphere in RK. By construction, ψ∗ is

α for all Π. Let us consider a new test ψ∗(ξ1, ξ2) =

≤

∈U

U

R

28

an invariant test as for any

U

, we have U

U

for all U

so that

∈ U

∈ U

∈ U

ψ∗(

U ξ1,

Uξ2) =

e
ψ(U

U ξ1, U

e
U ξ2)dU =

ψ(Uξ1, Uξ2)dU.

E
β0,Π)ψ∗(ξ1, ξ2) =
e
(

e

e

ZU

∈U

ZU

∈U n

E
e
e
β0,Π)ψ(Uξ1, Uξ2)
e
(

ZU
∈U
dU =

o

ZU

E
β0,U Π)ψ(ξ1, ξ2)
e
(

∈U n

dU

α.

≤

o

So, it has correct size. Now we check that the minimal power of ψ∗ achieved over alter-

natives (

β∗, Π) with Π such that Π′Π
√K

= C is not smaller than that of ψ. Assume that the

minimum of power for test ψ is achieved at the alternative Π∗: min Π

e

E
(

e

β∗,Π∗)ψ(ξ1, ξ2). Then, similarly to above:

E
(

e

β∗,Π)ψ(ξ1, ξ2) =

=C

Π
′
√K

E
(

min
=C

Π
Π
′
√K

e

β∗,Π)ψ∗(ξ1, ξ2) = min

≥

ZU

∈U

min
=C

Π
Π
′
√K

Π
Π
′
√K

=C ZU

∈U n
β∗,U Π)ψ(ξ1, ξ2)

e

E
(

n

o

E
(

e

β∗,U Π)ψ(ξ1, ξ2)

dU

≥

o
β∗,Π∗)ψ(ξ1, ξ2).

e

dU = E
(

All invariant tests depend on the data only through maximal invariant. Thus, we should

only consider tests that depend on the data through statistics Q = (Q1, Q2, Q3) =
(ξ′1ξ1, ξ′1ξ2, ξ′2ξ2). If Π′Π/√K

C then Q converges to the following distribution:

→

ξ′1ξ1

K

−
√2K
ξ′1ξ2
√K

ξ′2ξ2

K

−
√2K








⇒








N 












β2 C
√2
βC
e

C
√2
e








, I3





= 





Q

∞

Q

∞

Q

∞

,1

,2

,3








= Q

∞

.

(9)

According to Theorem 1 of Müeller (2011) the limit of the maximal power of tests in

experiment based on Q is bounded above by the maximal power achieved in the limit

experiment described on Q
that the maximal achievable power E e

∞

as deﬁned in the right hand side of equation (9). Notice

β∗,Cψ∗(Q

∞

) is strictly less than 1 for any ﬁxed β∗

and ﬁxed C. Indeed, the best achievable power in the limit experiment (9) is no more

than the best achievable power in the experiment when C is known. If C is known, the

optimal test follows from the Neyman-Pearson lemma, and its power is less than 1.

29

Proof of Theorem 2. Assumptions 1 and 2 imply

1
K

1

≥

P 2

ij =

1
K

P 2

ij −

1
K

i
X

P 2

ii ≥

1

−

δ

1
K

Pii = 1

δ.

−

i
X

i
X

j
X

i
X

=i
Xj

δ)(c∗)2 < Φ < (C ∗)2 and it is suﬃcient to prove that

Thus, (1
statement holds due to Lemma 2 applied to ξi = (ei, ei, ei)′. (cid:3)

−

Φ

−

→

p 0. The last

Φ

b

Lemma 2 Let Assumption 1 hold. Assume the errors ξi = (ξ(1)
dent mean zero random vectors with maxi E
6 < C. Then as N
k

ξik

i

, ξ(2)
i

, ξ(3)

i )′ are indepen-

, we have:

→ ∞

1
K

i
X

=i (cid:26)
Xj

P 2
ij
MiiMjj + M 2
ij

ξ(1)
i Miξ(2)
h

ξ(1)
j Mjξ(3)

i h

E

P 2
ij

i ξ(2)
ξ(1)
h

i

i

E

j ξ(3)
ξ(1)
h

j

i(cid:27)

p 0.

→

−

i

Proof of Lemma 2. Here we use the following notation

P 2

ij =

P 2
ij
MiiMjj +M 2
ij

. Notice that

1
K

i
Xj
=i
X
1
+
K

E

P 2
ij

e

i Miξ(2)ξ(1)
ξ(1)
h

ijM 2
P 2
ij

E

1
K

j Mjξ(3)

=

i
j ξ(3)
j ξ(1)

i

i ξ(2)
ξ(1)

i
X

=i
Xj

e

h

i

e
ijMiiMjjE
P 2

i
=i
Xj
X
1
=
K

e

E

P 2
ij

i
X

=i
Xj

i ξ(1)

j ξ(3)

j

+

i ξ(2)
ξ(1)
h
i ξ(2)
ξ(1)
h

i

i

E

i
j ξ(3)
ξ(1)
h

j

i

Deﬁne ξij = ξ(1)
1
K

=i

j

i

j Mjξ(3)
p 0. Since 1
K

i Miξ(2)ξ(1)
P 2
ijξij →
e

e
summation over distinct indexes (i, i′, j, j′)):

E

−

i

i Miξ(2)ξ(1)
ξ(1)
h
j

P 2

=i

j Mjξ(3)

, then we need to prove that

ijξij has zero mean, it is suﬃcient to show that

i

the variance of each term in expression (10) deﬁned below converges to zero (here I4 is a

P

P

P

P

P 4
ij

Eξ2

ij+

Xj
=i
1
K 2

e

XI4

P 2
ij

P 2
i′j′

Eξijξi′j′.

(10)

e

e

E

1
K

+

1
K 2

i
X

=i
Xj

2

=

1
K 2

i
X
Eξijξii′ +

P 2

ijξij

!

e

P 2
ij

P 2
ii′

i
X

=i Xi′6
Xj
i,j
=
{

}

e

e

30

6
6
6
6
6
6
6
6
 
6
6
6
First, we prove that maxi,j Eξ2

ij < C. We expand ξij = A1,ij + A2,ij + A3,ij, where:

A1,ij =MiiMjj

A2,ij =ξ(1)

i ξ(1)

j

i ξ(2)
ξ(1)
(cid:16)

i ξ(1)

j ξ(3)

j −
MiiMji′ξ(2)

E[ξ(1)

j

]

i ξ(2)

+ M 2
ij

i ξ(1)

j ξ(3)

i ξ(3)
ξ(1)
(cid:16)
(cid:17)
i + MjjMii′ξ(2)
+ Mii′Mijξ(2)
ξ(3)
i′
i′

i ξ(1)

j ξ(2)

E[ξ(1)

i ξ(3)

i ξ(1)

j ξ(2)

j

]

,

j −
j + Mji′Mijξ(2)
ξ(3)

j ξ(3)
i′

i ξ(3)
i′

(cid:17)

,

(cid:17)

A3,ij =ξ(1)

i ξ(1)

j

i,j
=
Xi′6
{

} (cid:16)

i,j
=
Xi′6
{

i,j
=
} Xj′6
{

}

Mii′Mjj′ξ(2)
i′

ξ(3)
j′

.

It is suﬃcient to show that maxi,j EA2
dition implies EA2

MiiMjj + M 2
ij

C

2

s,ij is bounded for all s = 1, 2, 3. The moment con-

≤
between summands in As,ij imply that some indexes must coincide. We also use Lemma

(cid:1)

(cid:0)

1,ij ≤

C. Below we use that non-zero correlations

S1.1 from the Supplementary Appendix:

EA2

2,ij ≤

Xi′
3,ij ≤

EA2

C

(MiiMji′ + Mii′Mij + MjjMii′ + Mji′Mij)2

C

i,j
=
Xi′6
{

i,j
=
} Xj′6
{

} (cid:0)

P 2
ii′

P 2
jj′

+

Pii′Pjj′Pij′Pji′|

|

(cid:1)

C,

C.

≤

≤

Next notice that

P 2

ij =

P 2
ij
MiiMjj + M 2

ij ≤

P 2
ij
Pii)(1

(1

−

Pjj) ≤

(1

−

1

−

δ)2 P 2
ij.

(11)

=i P 4

ij ≤

j

K and

i

j

=i

=j P 2

ijP 2

ij′ ≤

=i,j′6

ij < C and by Cauchy-Schwarz inequality
P

P

P

j′6
P

e

Lemma B1 in Chao et al (2012) gives that

i
K. Thus, given the bound on maxi,j Eξ2
P
maxi,j,k |

Eξijξik|

< C, the ﬁrst two terms in expression (10) converge to zero.
For the last term in (10), since i, i′, j, j′ are all distinct, we have EA1,ijAs,i′j′ = 0 for

s = 2, 3, and EA2,ijA3,i′j′ = 0. The non-zero terms in Eξijξi′j′ are

C

EA2,ijA2,i′j′| ≤
|
+C
EA3,ijA3,i′j′| ≤
|

(MiiMjj′ + MijMij′)(Mi′i′Mjj′ + Mi′jMi′j′)

(MjjMii′ + Mji′Mij)(Mj′j′Mii′ + Mj′i′Mij′)

+

.

|

|

|

|

C(Pii′Pjj′ + Pij′Pi′j)2.

Given inequality (11) and the symmetry of summation, and statements (a)-(e) proved

31

6
6
in Lemma S1.2 in the Supplementary Appendix, we obtain that the last two terms in
equation (10) converge to zero. (cid:3)

Proof of Theorem 3. Denote λi = MiΠ, then

Φ =

2
K

b

P 2

ij (ηi + ∆Πi) (Miη + ∆λi) (ηj + ∆Πj) (Mjη + ∆λj) .

i
X

=i
Xj

e

Let us deﬁne

Φ0 = 1
K

i

j

=i

P 2

vi is uniformly bounded. Lemma 2 with ξi = (ηi, ηi, ηi)′ gives
b
·

P

P

e

ijηiMiηηjMjη. Assumption 2 guarantees that the variance
p 0

of ηi = ei +∆

uniformly over bounded ∆. Lemma 3 with ξi = (ηi, ηi, ηi, ηi)′ implies

Φ

−

Φ
→
(cid:12)
p 0. (cid:3)
(cid:12)
(cid:12)

Φ0 −
(cid:12)
Φ0 →
(cid:12)
(cid:12)b
b

1 random vectors,

, ξ(2)
i

, ξ(3)
i

, ξ(4)

i )′ be independent mean zero 4
C

b
×

4 < C. Let Assumption 1 hold. Assume that λ′λ

K Π′Π and ∆2

Π′Π
K →

·

0

≤

Lemma 3 Let ξi = (ξ(1)
such that E
ξik
k
. Then

as N

i

→ ∞

1
K

i
X

=i
Xj

ξ(1)
i + ∆Πi
(cid:16)

(cid:17) (cid:0)

P 2
ij

e

Miξ(2) + ∆λi

ξ(3)
j + ∆Πj

Mjξ(4) + ∆λj

1
K

−

(cid:1) (cid:16)

(cid:17) (cid:0)
i Miξ(2)ξ(3)

ijξ(1)
P 2

j Mjξ(4)

i
X

=i
Xj

e

→

−

(cid:1)
p 0.

Proof of Lemma 3. We write the main expression of interest as a polynomial of fourth
power in ∆: ∆4A4 +∆3A3 +∆2A2 +∆A1 and prove that all terms are negligible ∆lAl →
by showing that their means and variances converge to zero. Notice that for expressions

p 0

with identical structure but diﬀerent components of ξi, the proof of their negligibility is

exactly the same. Thus for simplicity we abuse the notation and drop the superscripts to

ξi when we can consolidate these expressions. For example, we write the expression for one
ijΠiλiλjξ(1)
P 2

ijΠiλiλjξj, which collects both 1
P 2
K

of the terms in A3 as 1
K

=i

=i

j

j

j

i

i

P

P

e

P

P

e

32

6
6
6
6
6
6
and 1
K

i

j

=i

ijΠiλiλjξ(3)
P 2

j

. We also treat ξi in all expressions below as scalar.

P

P

e
A4 =

A3 =

A2 =

+

A1 =

1
K

1
K

1
K

1
K

1
K

i
X

=i
Xj

i
X

=i
Xj

i
X

=i
Xj

i
X

=i
Xj

P 2

ijΠiλiΠjλj;

e
P 2
ijΠiλiλjξj +

1
K

e
P 2
ijλiλjξiξj +

1
K

e
P 2
ijλiΠiξjMjξ +

e
P 2

ijλiξiMjξξj +

i
X

=i
Xj

e

P 2

ijΠiλiΠjMjξ;

i
X

=i
Xj

e
P 2
ijλiξiΠjMjξ+

=i
Xj

e

P 2

ijΠiΠjMiξMjξ;

i
X

=i
Xj

e
P 2
ijΠiMiξξjMjξ.

i
X
1
K

1
K

i
X

=i
Xj

e

Term A4 is deterministic. We use bound (11) and Lemma S1.3 (d):

∆4

A4| ≤

|

C∆4Π′Πλ′λ
K

≤

C∆4(Π′Π)2
K 2

0.

→

Term A3 is mean zero. Using the inequality V ar(X + Y )

2V ar(X) + 2V ar(Y ) we have:

≤

∆6V ar(A3)

C∆6
K 2 

≤

2

P 2
ij|

Πi||

λi|!

λ2
j +

j  

X

i
X

(λ′λ)2Π′Π +



C∆6
K 2

≤

Xk  
P 2

i
X

=i
Xj

e
Πi′λi′Πj′|

i′j′|

P 2
ij|

ΠiλiΠj|

Xi,i′,j,j′

C∆6
K 2

≤

(λ′λ)2Π′Π + (Π′Π)2λ′λ

(cid:0)

≤

(cid:1)

P 2

ijΠiλiΠjMjk

2



≤

!



MjkMj′k|! ≤

|

Xk
C∆6(Π′Π)3
K 3

0.

→

For the ﬁrst inequality, we apply Assumption 2 and bound (11). Then we use Cauchy-

Schwarz inequality for the ﬁrst summand:

i P 2
ij|

Πi||

λi|

2

≤

Π′Πλ′λ. For the second

summand, we apply Lemma S1.1 (ii) and Lemma S1.3 (c). Finally, we apply Lemma S2.1
p 0. (cid:3)
and S2.2 to get ∆2A2 →

p 0 and ∆A1 →

(cid:0)P

(cid:1)

33

6
6
6
6
6
6
6
6
6
6
6
 
Proof of Theorem 4. The infeasible version of AR statistics under β = β0 + ∆ is:

1
√K√Φ

=

∆2
√K√Φ

i
X

=i
Xj

i
X

=i
Xj

Pijei(β0)ej(β0)

PijΠiΠj +

2∆
√K√Φ

PijΠj

ηi +

!

1
√K√Φ

i  

X

=i
Xj

i
X

=i
Xj

Pijηiηj. (12)

The ﬁrst term in (12) is deterministic and equals to ∆2

µ2
√K√Φ

. The second term has mean

zero and variance

∆2
KΦ

i  

X

=i
Xj

PijΠj

2

!

V ar(ηi)

Cc2
KΦ

≤

i
X

w2

i ≤

CΠ′Π

K →

0.

Here we used that variance of ηi is bounded by Assumption 2,

=i PijΠi = wi, and

j

the ﬁnal bound is proven in Lemma S1.4. Thus, the second term converges to zero in

P

probability uniformly over

2

∆
|

|

≤

c. The third term in (12) is asymptotically standard

normal due to Lemma 1. Finally, we notice that

AR(β0) =

Φ

s

Φ

1
√K√Φ

i
X

=i
Xj

Pijei(β0)ej(β0),

and apply Theorem 3. This ﬁnishes the proof of statement (5).

b

Now consider the case when

µ2

√K√Φ → ∞

and ∆

= 0 is ﬁxed. Above we proved that

1
√K√Φ

i
X

=i
Xj

Pijei(β0)ej(β0) =

µ2
√K√Φ

∆2 + op(1) + Op(1).

Finally, Theorem 3 implies that

p 1. As a result, we have AR(β0)

p

when

= 0 is ﬁxed. This lead to rejection probability converging to 1. (cid:3)

→

∞

bΦ
Φ →

µ2

√K√Φ → ∞

and ∆

Proof of Theorem 5. Denote

Q = (Qee, QXe, QXX )′ =

Pij (eiej, Xiej, XiXj)′ .

1
√K

N

i=1
X

=i
Xj

34

6
6
6
6
6
6
6
6
6
6
6
Lemma A2 in Chao et al.
Qee, QXe, QXX −
(cid:16)
implies that

(a′Σa)−

1/2

(2012) states that for any ﬁxed 3
µ2
√K

a

N(0, 1). According to Cramér-Wold theorem, this

1 vector a we have

×

⇒

(cid:17)

1/2

Σ−

(cid:18)

Qee, QXe, QXX −

µ2
√K (cid:19)

⇒

N(0, I3)

where Σ is the asymptotic covariance matrix of Q, with some of its elements written

below:

Ψ =

Υ =

τ =

1
K

2
K

2
K

N

i=1
X
N

=i
Xj

i=1
X
N

=i
Xj

i=1
X

=i
Xj

P 2

ijγiγj +

ijς 2
P 2

i ς 2

j +

ijς 2
P 2

i γj +

1
K

4
K

2
K

N

i=1
X
N

i=1
X
N

=i
Xj
ς 2
i (

=i
Xj

ijσ2
P 2

i ς 2

j +

1
K

N

(

i=1
X

=i
Xj

PijΠj)2σ2

i = AV ar(QXe),

PijΠj)2 = AV ar(QXX ),

(13)

γi(

PijΠj)2 = ACov(QXe, QXX ),

̺ =

i=1
X

=i
Xj

τ
√Ψ√Υ

.

where σ2
ei −

Xi(

β) and (

i = V ar(ei), ς 2
βJIV E −
b
W ald(β0) =

i = V ar(vi), γi = cov(ei, vi). Note that

ei = Yi −

Xi

βJIV E =

βJIV E −
b

β0) = QXe/QXX . Thus,

b

Q2

Xe

N
i=1

=i PijXj

j

2 beiMibe
Mii

+

N
i=1

P 2

ijMiX

eiMjX

ej

j

=i

b

,

where the denominator expands to

P

(cid:16)P

(cid:17)

P

P

e

b

b

N

e

+

P 2

ijMiX

eiMjX

ej =

b
eiMie
Mii

PijXj

2

!

(cid:18)

i=1
=i
Xj
X
N

e

+

=i
Xj

i=1
X
eiMiX
Mii

+

b

b

P 2
ijMiXeiMjXej

e
XiMie
Mii (cid:19)

+ 2



N

−

i=1
X

=i
Xj

PijXj

2

!

N

i=1  
X
N

=i
Xj

eiMi
Mii
b
2

PijXj

!

=i
Xj
N

i=1  
X
N

=i
Xj

i=1  
X

=i
Xj

=

−

+




i=1  
X
QXe

QXX 


Q2
Q2


Xe
XX 




PijXj

2

!

XiMiX
Mii

+

N

i=1
X

=i
Xj

35

P 2
ijMiXeiMjXXj

e

+



.

P 2
ijMiXXiMjXXj

e



6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
Applying Lemma S3.1 from the Supplementary Appendix to the expanded expression of

the denominator, we show the terms appearing in the braces converge to Ψ, 2τ and Υ

respectively. Then

W ald(β0) =

Ψ

−

Xe

Q2
2 QXe
QXX

τ + Q2
Q2

Xe

XX

(1 + op(1)) =

Υ

1

−

Xe/Ψ

Q2
2 QXe/√Ψ
QXX /√Υ

̺ + Q2
Q2

Xe

XX

(1 + op(1)).

Υ
Ψ

Lemmas 2 and 3 applied to

Υ with ξi = (vi, vi, vi, vi)′ and ∆ = 1 give

op(1)). Thus, the statement of Theorem 5 holds where we denote
the Gaussian limit of ( QXe
√Ψ

b
, QXX

µ2
√K√Υ

). (cid:3)

√Υ −

ξ, ν

(cid:16)

F = QXX
√Υ
µ2
√K√Υ
e

−

(1 +

to be

(cid:17)

Proof of Corollary 1. Denote S = µ2

√K√Υ

. If S > 2.5 then due to Theorem 5:

PS{

F > 4.14 and W ald(β0)

χ2

1,0.95} ≤

PS{

≥

W ald(β0)

χ2

1,0.95} ≤

≥

0.10.

e

2.5 then due to the asymptotic gaussianity of

F :

If S

≤

PS{

F > 4.14 and W ald(β0)

χ2

1,0.95} ≤

≥

e
PS{

F > 4.14

0.05.

} ≤

Finally, for any S > 0:

e

e

H0 is rejected

PS {
4.14 and AR(β0)

= P

}
{
z0.95} ≤
e

≥

F > 4.14 and W ald(β0)

0.10 + P

AR(β0)

{

≥

χ2

1,0.95}

+

0.15.

≥
z0.95} ≤

+P

F

{

≤

e

8.1 Simulation Details

For results reported in Section 6. To create many instruments, we interact QOB

dummies with dummies for year of birth (YOB) and place (state) of birth (POB). Inter-

acting three QOB dummies with nine YOB and 50 POB dummies generates 180 excluded

instruments. The excluded instruments are

Zi = ((1

{

Qi = q, Ci = c

)′q

}

2,3,4

,c

31,...,39

∈{

}

∈{

, 1

{

Qi = q, Pi = p

)′q

}

}

2,3,4

,p

}

∈{

50 states

∈{

)′,

}

36

where Qi, Ci, Pi are i’s QOB, YOB and POB respectively. Note, that Zi are not group

instruments in the strict sense as they are not mutually exclusive. We exclude instruments

with

N
i=1 Zij < 5 to satisfy the balanced instruments assumption (Assumption 1).

To increase the amount of omitted variable bias, we follow Angrist and Frandsen

P

(2019) by taking the LIML model as the ground truth, where the outcome variable is Yi

(income), the endogenous variable Xi (highest grade completed) is instrumented by Zi

and the control variables are a full set of POB-by-YOB interactions. Speciﬁcally, starting

with the full 1980 census sample, we compute the average Xi in each QOB-YOB-POB cell

¯s(q, c, p) . We then estimate LIML and retain ˆy(c, p), the second-stage ﬁtted value after
subtracting ˆβLIM LXi where ˆβLIM L is the LIML estimate of the returns to schooling. We

also retain the variance of LIML residuals ω(Qi, Ci, Pi) to mimic the heteroskedasticity.

The simulation model we consider is then

˜yi = ¯y + 0.1˜si + ω(Qi, Ci, Pi)(νi + κ2ǫi)

˜si ∼

P oisson(µi),

for independent standard normal νi and ǫi. Here ¯y = 1
N
γ′ZZi + κ1νi}
We set κ1 = 1.7 and κ2 = 0.1 following Angrist and Frandsen (2019). The ﬁrst stage is

where γ0 + γ′ZZi is the projection of ¯s(Qi, Ci, Pi) onto a constant and Zi.

i ˆy(Ci, Pi) and µi = max
{

1, γ0 +

P

therefore nonlinear non-linear in Zi as µi is a censored normal random variable. The ﬁrst

stage error is heteroskedastic and the theoretical variance can be derived analytically.

For results reported in Section 4.2. The DGP is given by a homoscedastic linear

IV model (1) with a linear ﬁrst stage Πi = Π′Zi. The instruments are K = 40 group

indicators, where the sample is divided into equal groups. The sample size is N = 200.

The error terms are generated i.i.d. as

ei

vi





∼ N 



,



0
0 


1 ρ
ρ 1 




with ρ = 0.2.

We simulate a sparse ﬁrst stage by setting one large coeﬃcient πK = 2 and πk = 0.001 for












all k < K. The dense ﬁrst stage has homogeneous ﬁrst stage coeﬃcients πk = 0.316 for
all k = 1, . . . , K. Identiﬁcation strength is held the same at µ2
√K

= 2.5 for both settings.

The results are reported in Figure 1.

37

