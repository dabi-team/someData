0
2
0
2

n
u
J

9
2

]

G
L
.
s
c
[

1
v
8
7
3
6
1
.
6
0
0
2
:
v
i
X
r
a

An EM Approach to Non-autoregressive Conditional Sequence Generation

Zhiqing Sun 1 Yiming Yang 1

Abstract

conditional probability of y given x is factorized as:

Autoregressive (AR) models have been the domi-
nating approach to conditional sequence genera-
tion, but are suffering from the issue of high infer-
ence latency. Non-autoregressive (NAR) models
have been recently proposed to reduce the latency
by generating all output tokens in parallel but
could only achieve inferior accuracy compared to
their autoregressive counterparts, primarily due to
a difﬁculty in dealing with the multi-modality in
sequence generation. This paper proposes a new
approach that jointly optimizes both AR and NAR
models in a uniﬁed Expectation-Maximization
(EM) framework. In the E-step, an AR model
learns to approximate the regularized posterior of
the NAR model. In the M-step, the NAR model
is updated on the new posterior and selects the
training examples for the next AR model. This
iterative process can effectively guide the system
to remove the multi-modality in the output se-
quences. To our knowledge, this is the ﬁrst EM
approach to NAR sequence generation. We evalu-
ate our method on the task of machine translation.
Experimental results on benchmark data sets show
that the proposed approach achieves competitive,
if not better, performance with existing NAR mod-
els and signiﬁcantly reduces the inference latency.

1. Introduction

State-of-the-art conditional sequence generation mod-
els (Bahdanau et al., 2014; Gehring et al., 2017; Vaswani
et al., 2017) typically rely on an AutoRegressive (AR) factor-
ization scheme to produce the output sequences. Denoting
by x = (x1, . . . , xT ) an input sequence of length T , and
by y = (y1, . . . , yT (cid:48)) a target sequence of length T (cid:48), the

1Carnegie Mellon University, Pittsburgh, PA 15213 USA. Cor-

respondence to: Zhiqing Sun <zhiqings@cs.cmu.edu>.

Proceedings of the 37 th International Conference on Machine
Learning, Vienna, Austria, PMLR 119, 2020. Copyright 2020 by
the author(s).

pAR(y|x) =

T (cid:48)
(cid:89)

i=1

p(yi|x, y1, y2, . . . , yi−1).

(1)

As such a sequential factorization cannot take the full advan-
tage of parallel computing, it yields high inference latency
as a limitation.

Recently, Non-AutoRegressive (NAR) sequence models (Gu
et al., 2017; Lee et al., 2018) are proposed to tackle the
problem of inference latency, by removing the sequential
dependencies among the output tokens as:

pN AR(y|x) = p(T (cid:48)|x)

T (cid:48)
(cid:89)

i=1

p(yi|x, T (cid:48)).

(2)

This formulation allows each token to be decoded in parallel
and hence brings a signiﬁcant reduction of the inference
latency. However, NAR models also suffer from the condi-
tional independence assumption among the output tokens,
and usually do not perform as well as their AR counterparts.
Such a performance gap is particularly evident when the out-
put distributions exhibit a multi-modality phenomenon (Gu
et al., 2017). which means that the input sequence can
be mapped to multiple correct output sequences. Such a
multi-modal output distribution cannot be represented as the
product of conditionally independent distributions for each
position in NAR models (See 3.2 for a detailed discussion).

How to overcome the multi-modality issue has been a cen-
tral focus in recent efforts for improving NAR models. A
standard approach is to use sequence-level knowledge dis-
tillation (Hinton et al., 2015; Kim & Rush, 2016), which
means to replace the target part y of each training instance
(x, y) with the system-predicted ˆy from a pre-trained AR
model (a.k.a. the ”teacher model”). Such a replacement
strategy removes the one-to-many mappings from the origi-
nal dataset. The justiﬁcation for doing so is that in practice
we do not really need the sequence generation models to
mimic a diverse output distribution for sequence generation
tasks such as machine translation1 and text summarization.
Such a knowledge distillation strategy has shown to be ef-
fective for improving the performance of NAR models in

1For example, Google Translate only provide one translation

for the input text.

 
 
 
 
 
 
An EM Approach to Non-autoregressive Conditional Sequence Generation

2. Related Work

Related work can be divided into two groups, i.e., the non-
autoregressive methods for conditional sequence generation,
and the various approaches to knowledge distillation in non-
autoregressive models.

Recent work on non-autoregressive sequence generation
has developed ways to address the multi-modality problem.
Several work try to design better training objectives (Shao
et al., 2019; Wei et al., 2019) or regularization terms (Li
et al., 2019; Wang et al., 2019; Guo et al., 2018). Other
methods focus on direct modeling of multi-modal target dis-
tributions via hidden variables (Gu et al., 2017; Kaiser et al.,
2018; Ran et al., 2019; Ma et al., 2019) or sophisticated out-
put structures (Libovick`y & Helcl, 2018; Sun et al., 2019).
There are also a few recent work (Lee et al., 2018; Stern
et al., 2019; Ghazvininejad et al., 2019; Gu et al., 2019)
focusing on a multiple-pass iterative-reﬁnement process to
generate the ﬁnal outputs, where the ﬁrst pass produce an
initial output sequence and the following passes reﬁne the
sequence iteratively in the inference phase.

As for knowledge distillation in NAR models, Gu et al.
(2017) is the ﬁrst effort to use knowledge distillation (Hin-
ton et al., 2015; Kim & Rush, 2016). Recently, Zhou et al.
(2019) analyzed why knowledge distillation would reduce
the complexity of datasets and hence be helpful in the train-
ing of NAR models. They also used Born-Again networks
(BANs) (Furlanello et al., 2018) to produce simpliﬁed train-
ing data for NAR models.

All the above methods take a pre-trained AR model as the
teacher model for knowledge distillation; none of them
iteratively updates the teacher model based on the feedback
from (or the measured performance of) the NAR model.
This is the fundamental difference between existing work
and our proposed EM approach in this paper.

3. Problem Deﬁnition

3.1. Conditional Sequence Generation

Let us describe the problem of conditional sequence genera-
tion in the context of machine translation and use the terms
of “sequence” and “sentence”, “source” and “input”, “target”
and “output” interchangeably. We use x and y to denote the
source and target sentences, xi to indicate the i-th token in
x, and X = {x1, x2, . . . , xN } and Y = {y1, y2, . . . , yN }
to be a parallel dataset of N sentence pairs in the source and
target languages, respectively.

The training of both AutoRegressive (AR) and Non-
AutoRegressive (NAR) sequence generation models is per-
formed via likelihood maximization over parallel data

Figure 1. The proposed framework: The AR and NAR models are
jointly optimized by alternating between a E-step and a M-step.
See Sec. 5.1 for the detailed explanation.

multiple studies (Gu et al., 2017; Kaiser et al., 2018; Li
et al., 2019; Ma et al., 2019; Sun et al., 2019; Ghazvininejad
et al., 2019; Gu et al., 2019).

We want to point out that in all the NAR methods with
knowledge distillation, the teacher AR model is pre-trained
only once on ground-truth data and then is used to generate
the output training examples for the NAR model. We argue
that such a single-pass knowledge distillation process may
not be sufﬁcient for optimizing the NAR model as sequence
ˆy predicted by the AR model cannot be perfect. More im-
portantly, it is not necessarily the best choice for alleviating
the multi-modality problem in the NAR model. In other
words, without knowing how the choice of ˆy by the AR
model would effect the training process in the NAR model,
the current knowledge distillation approach is unavoidably
sub-optimal.

To address this fundamental limitation, we propose a novel
Expectation-Maximization (EM) approach to the training of
NAR models, where both the teacher (an AR model) and the
student (a NAR model) are helping each other in a closed
loop, and the iterative updates of the models are guaranteed
to converge to a local optimum. This approach gives an
extra power to knowledge distillation between AR and NAR
models, and is the ﬁrst EM approach to NAR models, to
our knowledge. Fig. 1 illustrates our new framework. In
addition, we develop a principled plug-and-play decoding
module for effective removal of word duplication in the
model’s output. Our experiments on three machine transla-
tion benchmark datasets show that the proposed approach
achieves competitive performance as that of the best NAR
models in terms of prediction accuracy, and reduces the
inference latency signiﬁcantly.

Autoregressive ModelNon-autoregressive ModelE-step: Update the teacher modelM-step: Update the student modelAn EM Approach to Non-autoregressive Conditional Sequence Generation

(X , Y) as:

4.1. Theoretical expressiveness of NAR models

φ∗ = arg max

φ

θ∗ = arg max

θ

E(x,y)∼(X ,Y) log pAR(y|x; φ),

E(x,y)∼(X ,Y) log pN AR(y|x; θ),

(3)

(4)

where pAR and pN AR are deﬁned in Eq. 1 and Eq. 2; φ and θ
are the parameters of the AR and NAR models, respectively.

3.2. Instance-level & Corpus-level Multi-modality

There are two different but not mutually exclusive deﬁni-
tions for the concept of multi-modality (a.k.a., translation
uncertainty in machine translation terminology).

Gu et al. (2017) deﬁne the multi-modality problem as the ex-
istence of one-to-many mappings in the parallel data, which
we refer to as the instance-level multi-modality. Formally,
given the parallel data (X , Y), if there exist sentences i
and j such that xi = xj but yi (cid:54)= yj, we say that (X , Y)
contains instance-level multi-modality.

In contrast, Zhou et al. (2019) use the conditional entropy
to quantify the translation uncertainty, which we refer as the
corpus-level multi-modality. It requires the use of an addi-
tional alignment model in calculating this quantity. In this
paper, we want to avoid the requirement for external align-
ment tools. Thus, we directly use training-set likelihood of
the NAR model as a measure of corpus-level multi-modality.
Formally, the Corpus-level Multi-modality (CM) of parallel
data (X , Y) is deﬁned as:

CMX (Y) = E(x,y)∼(X ,Y)
θ∗ = arg max

θ

E(x,y)∼(X ,Y) log pN AR(y|x; θ).

(cid:2)− log pN AR(y|x; θ∗)(cid:3) ,

(5)

(6)

To make this metric comparable across different data
sets, we further deﬁne the Normalized Corpus-level Multi-
modality (NCM) as:

NCMX (Y) =

E(x,y)∼(X ,Y)

M (y|x; θ∗)(cid:3)

(cid:2)− log pN AR
Ey∼Y [|y|]

(7)

where |y| denotes the length of output sequence y.

4. Properties of NAR Models

How powerful are NAR models? Are they as expressive
as AR models? Our answer is both yes and no. On one
hand, it is easy to see that NAR models can only capture
distributions that can be factorized into conditional inde-
pendent parts. On the other hand, we will show in the next
that if the instance-level multi-modality can be removed
from the training data (e.g., via sequence-level knowledge
distillation), then NAR models can be as powerful just as
AR models.

Let us focus on the expressive power of NAR models when
the instance-level multi-modality is removed, that is, when
there are only one-to-one and many-to-one mappings in
training examples. More speciﬁcally, we consider the ability
of the vanilla Non-Autoregressive Transformer (NAT) mod-
els (Gu et al., 2017; Vaswani et al., 2017) in approximating
arbitrary continuous Rd×n → Rd×m sequence-to-sequence
single-valued functions, where n and m are the input and
output sequence length, and d is the model dimension.

Given the deﬁnition of a distance between two functions
f1, f2 : Rd×n → Rd×m as:

(cid:18)(cid:90)

dp(f1, f2) =

(cid:107)f1(X) − f2(X)(cid:107)p

pdX

(cid:19)1/p

.

(8)

where p ∈ [1, ∞). We can make the following statement:

Theorem 4.1. Let 1 ≤ p < ∞ and (cid:15) > 0, then for
any given continuous sequence-to-sequence function f :
Rd×n → Rd×m, there exists a non-autoregressive Trans-
former network g such that dp(f, g) ≤ (cid:15).

This theorem is a corollary of the Theorem 2 in Yun et al.
(2020). For completeness, we provide the formal theorem
with proof in the appendix.

4.2. What limits the success of NAR models in

practice?

Theorem 4.1 shows that for any sequence-to-sequence
dataset containing no instance-level multi-modality, we can
always ﬁnd a good NAT model to ﬁt this dataset. However,
in reality, it is still a big challenge for NAT models to ﬁt the
distilled deterministic training data very well.

The gap between theory and practice is due to the fact that in
theory we may use as many Transformer layers as needed,
but in reality, there are only a few layers (e.g., 6 layers)
in the Transformer model. This would greatly restrict the
model capacity of real NAR models.

To further understand the limitation let us examine the fol-
lowing two hypotheses:

• The NAT model intrinsically cannot accurately produce
very long output sequences when it has only a few
Transformer layers.

• The corpus-level multi-modality in data makes it hard
for NAT models to deal with (i.e., to memorize the
“mode” in the output for each input).

These hypotheses focus on two different reasons that might
cause the poor performance of NAR models. In order to

An EM Approach to Non-autoregressive Conditional Sequence Generation

Table 1. Toy examples illustrating the two types of synthetic exper-
iments.

Experiment I

Experiment II

target
2 2 1 4 4 4 4 3 3 3
2 2 2 2 3 3 3
2 2 1 5 5 5 5

source
2 1 4 3 →
2 2 3 →
2 1 5 →
2 1 4 3 → 0 2 2 1 4 4 4 4 3 3 3 0 0 0
2 2 3 → 0 0 0 2 2 2 2 3 3 3 0
2 1 5 → 2 2 1 5 5 5 5 5 0 0 0 0

Table 2. The accuracy in whole-sentence matching of the AR and
NAR models over 1000 synthetic examples.

Models

Accuracy(%) 99.9

Experiment I Experiment II
NAR
AR NAR AR
00.0
99.8
95.7

verifying which one is true, we design two types of synthetic
data and experiments.

In Experiment I, a synthetic translation dataset is constructed
as follows: The source and target sides share the same vocab-
ulary of {1, 2, 3, 4, 5}. 1, 2, 3, and 4, and 5 are translated
into 1, 2 2, 3 3 3, 4 4 4 4, and 5 5 5 5 5, respectively. The
translation is deterministic, i.e., no multi-modality in the
resulted parallel dataset. In Experiment II, we randomly
insert four 0 in the front or the back of each target sentence
in the 1st dataset. In other words, the source-to-target trans-
lation in the 2nd dataset is non-deterministic and hence is
with corpus-level multi-modality. In addition, we ﬁlter the
source data in Experiment II to make sure that there is no
instance-level multi-modality in this dataset. The toy ex-
amples illustrating the two types of datasets can be found
in Tab. 1. Following such rules we randomly generated
2,000,000 sentences for training and 1000 sentences for test-
ing; both the training and testing source sentences have the
length of 30.

We trained both the AR transformer and the NAR Trans-
former on these synthetic datasets, each of which consists of
a 3-layer encoder and a 3-layer decoder. The detailed model
settings can be found in the appendix. In our evaluation we
used the ground-truth lengths for the decoding of both the
AR and NAR Transformers; the whole-sentence matching
accuracy of those models are listed in Tab. 2.

The results in Experiment I show that both the autoregressive
Transformer and the non-autoregressive Transformer can
achieve an high accuracy of 99.9% and 95.7%, respective,
when the training data do not have the multi-modality. In
contrast, the results of Experiment II show that the non-
autoregressive Transformer model failed completely on the
synthetic dataset with corpus-level multi-modality. The
sharp contrast in these synthetic experiments indicates that
the real problem with NAR models is indeed due to the

corpus-level multi-modality issue.

5. Proposed Method

Let us formally introduce our EM approach to addressing
the multi-modality issue in NAR models, followed by a
principled decoding module for effective removal of word
duplication in the predicted output.

5.1. The EM Framework

With the deﬁnition of the corpus-level multi-modality (i.e.,
CM in Eq. 5), we consider how to reduce this quantity for
the better training of NAR models. Formally, given source
data X , we want to ﬁnd target data Y ∗ that satisﬁes the
following property:

Y ∗ = arg min

Y

CMX (Y)

= arg min
Y

E(x,y)∼(X ,Y)

(cid:2)− log pN AR(y|x; θ∗)(cid:3) .

However, there can be many trivial solutions for Y ∗. For
example, we can simply construct a dataset with no variation
in the output to achieve zero corpus-level multi-modality.
To avoid triviality, we may further apply a constraint to Y ∗.
This leads us to the posterior regularization framework.

Posterior Regularization (Ganchev et al., 2010) is a proba-
bilistic framework for structured, weakly supervised learn-
ing. In this framework, we can re-write our objective as
following:

LQ(θ) = min
q∈Q

KL(q(Y)(cid:107)pN AR(Y|X ; θ)),

where q is the posterior distribution of Y and Q is a con-
straint posterior set that controls the quality of the parallel
data given by:

Q = {q(Y) : EY∼q [QX (Y)] ≥ b},

where Q is a metric for quality mapping from (X , Y) to RN
in the training set and b is a bound vector.

For sequence generation tasks, there are many corpus-level
quality metrics, such as BLEU (Papineni et al., 2002) and
ROUGE (Lin & Hovy, 2003). However, they are known to
be inaccurate for measuring the quality of single sentence
pairs. Thus, we use the likelihood score of a pre-trained AR
model as a more reliable quality metric:

[QX (Y)]i = Qxi(yi) = log pAR(yi|xi; φ1),

where φ1 denotes that the AR model trained on the original
ground-truth dataset.

Given the posterior regularization likelihood LQ(θ), we use
the EM algorithm (McLachlan & Krishnan, 2007; Ganchev

An EM Approach to Non-autoregressive Conditional Sequence Generation

et al., 2010) to optimize it. In the E-step (a.k.a,the inference
procedure), the goal is to ﬁx pN AR and update the posterior
distribution:

qt+1 = arg min

KL(q(Y)(cid:107)pN AR(Y|X ; θt)),

q∈Q

In the M-step (a.k.a., learning procedure), we will ﬁx q(Y)
and update θ to maximize the expected log-likelihood:

θt+1 = arg max

θ

Eqt+1[log pN AR(Y|X ; θ)],

Next, we introduce the details of the E-step and the M-step
in our framework.

Inference Procedure The E-step aims to compute the
posterior distribution q(Y) that minimizes the KL diver-
gence between q(Y) and pN AR(Y|X ). Ganchev et al.
(2010) show that for graphical models, q(Y) can be efﬁ-
ciently solved in its dual form. Speciﬁcally, the primal
solution q∗ is given in terms of the dual solution λ∗ by:

q∗(Y) ∝ pN AR(Y|X ; θt) exp{λ∗ · QX (Y)}

∝

N
(cid:89)

i=1

(cid:104)

i (cid:105)
pN AR(yi|xi; θt) (cid:0)pAR(yi|xi; φ0)(cid:1)λ∗

,

However, a problem here, as pointed out by Zhang et al.
(2018), is that it is hard to specify the hyper-parameter b to
effectively bound the expectation of the features for neural
models. Besides, even when b is given, calculating λ∗ is
still intractable for neural models. Therefore, in this paper,
we introduce another way to compute q(Y).

We ﬁrst factorize q(Y) as the product of {q(yi)}, and then
follow the idea of amortized inference (Gershman & Good-
man, 2014) to parameterize q(yi) with an AR sequence
generation model:

q(Y) =

N
(cid:89)

i=1

pAR(yi|xi; φ).

The E-step can thus be re-written as follows:

This can be intuitively viewed as to construct a weighted
pseudo training dataset (X t+1, Y t+1), where the training
examples are sampled at random from pAR(y|x; φ) and
weighted by log pN AR(y|x;θt)
pAR(y|x;φ) .

In practice, we ﬁnd that there are two problems in im-
plementing this algorithm: One is that sampling from
pAR(y|x; φ) is very inefﬁcient; the other is that the con-
straint φ ∈ Q(cid:48) cannot be guaranteed. Therefore, we instead
use a heuristic way when constructing the pseudo training
dataset (X t+1, Y t+1): We ﬁrst follow Wu et al. (2018) to
replace the inefﬁcient sampling process with beam search
(Sutskever et al., 2014) on pAR(y|x; φt), and then ﬁlter out
the candidates that doesn’t satisfy the following condition:

Qx(y) ≥ ˆbi,

where ˆbi is a newly introduced pseudo bound that can be
empirically set by early stopping. In this way, we control
the quality of pAR(y|x; φt+1) by manipulating the quality
of its training data. Finally, we choose the ones with the
highest pAR(y|x; φt) log pN AR(y|x;θt)
pAR(y|x;φt) score as the training
examples in Y t+1, and X t+1 is merely a copy of X .

In each E-step, in principle, we should let φt+1 converge un-
der the current NAR model. Although this can be achieved
by constructing the pseudo datasets and train the AR model
for multiple times, it is practically prohibitive due to the
expensive training cost. We, therefore, use only a single
update iteration of the AR model in the inner loop of each
E-step.

Learning Procedure
In the M-step, we seek to learn the
parameters θt+1 with the parameterized posterior distribu-
tion pAR(Y|X ; φt+1). However, directly sampling training
examples from the AR model will cause the instance-level
multi-modality problem. Therefore, we apply sequence-
level knowledge distillation (Kim & Rush, 2016) to solve
this problem, that is, we only use the targets with maximum
likelihood in the AR model to train the NAR model:

φt+1 = arg min

φ∈Q(cid:48)

Ex∼X KL(pAR(y|x; φ)(cid:107)pN AR(y|x; θt)).

θt+1 = arg max

θ

E
(x,y)∼(X , ˆY t+1) log pN AR(y|x; θ).

where the new constraint posterior set Q(cid:48) is deﬁned as

(cid:8)φ : EpAR(Y|X ;φ) [CQX (Y)] ≥ b(cid:9) .

We further apply the REINFORCE algorithm (Williams,
1992) to estimate the gradient of LQ w.r.t. φ ∈ Q(cid:48):

∇φLQ = Ex∼X Ey∼pAR(y|x;φ)
(cid:18)
pN AR(y|x; θt)
pAR(y|x; φ)

− log

∇φ log pAR(y|x; φ)

.

(cid:19)

where ˆY t+1 denotes the training examples produced by the
AR model pAR(Y|X ; φt+1).

Joint Optimization We ﬁrst pre-train an AR teacher
model on the ground-truth parallel data as pAR(y|x; φ1).
Then we alternatively optimize pN AR and pAR until con-
vergence. We summarize the optimization algorithm in Alg.
1. In our EM method, the AR and NAR models are jointly
optimized to reduce the corpus-level multi-modality.

An EM Approach to Non-autoregressive Conditional Sequence Generation

Algorithm 1 An EM approach to NAR models

Input: Parallel training dataset (X , Y)
t = 0
Pre-train pAR(y|x; φ1) on (X , Y).
while not converged do

t = t + 1
(cid:0) M-Step: Learning Procedure
Construct the distillation dataset ˆY t with pAR(y|x; φt).
Train pN AR(y|x; θt) on (X , ˆY t).
(cid:0) E-Step: Inference Procedure
Construct the pseudo dataset (X t+1, Y t+1).
Train pAR(y|x; φt+1) on (X t+1, Y t+1).

end while
Output: A NAR model pN AR(y|x; θt).

5.2. The Optimal De-duplicated Decoding Module

Word duplication is a well-known problem in NAR models
caused by the multi-modality issue. To improve the per-
formance of NAR models, some previous work (Lee et al.,
2018; Li et al., 2019) remove any duplication in the model
prediction by collapsing multiple consecutive occurrences
of a token. Such an empirical approach is not technically
sound. After collapsing, the length of the target sequence
is changed. This will cause a discrepancy between the pre-
dicted target length and the actual sequence length and thus
make the ﬁnal output sub-optimal.

We aim to solve the word duplication problem in NAR mod-
els, while preserving the original sequence length. Similar
to Sun et al. (2019), we use the Conditional Random Fields
(CRF) model (Lafferty et al., 2001) for the decoding of
NAR models. The CRF model is manually constructed as
follows. It treats the tokens to be decoded as the predicted
labels. The unitary scores of the labels in each position are
set to be NAR models’ output distribution and the transition
matrix is set to −∞ · I, where I is an identity matrix.

Our model is able to ﬁnd the optimal decoding when con-
sidering only the top-3 candidates w.r.t. the unitary scores
in each position:

Proposition 5.1. In a CRF with a transition matrix of −∞ ·
I, only top-3 likely labels for each position are possible to
appear in the optimal (most likely) label sequence.

We can thus crop the transition matrix accordingly by only
keeping a 3 × 3 transition sub-matrix between each pair of
adjacent positions. The forward-backward algorithm (Laf-
ferty et al., 2001) is then applied on the top-3 likely labels
and the 3 × 3 transition sub-matrices to ﬁnd the optimal
decoding with the linear time complexity of O(|y|).

The proposed decoding module is a lightweight plug-and-
play module that can be used for any NAR models. Since
this principled decoding method is guaranteed to ﬁnd the

optimal prediction that has no word duplication, we refer it
as optimal de-duplicated (ODD) decoding method2.

6. Experiments

6.1. Experimental Settings

We use several benchmark tasks to evaluate the effec-
tiveness of the proposed method, including IWSLT143
German-to-English translation (IWSLT14 De-En) and
WMT144 English-to-German/German-to-English transla-
tion (WMT14 En-De/De-En). For the WMT14 dataset, we
use Newstest2014 as test data and Newstest2013 as valida-
tion data. For IWSLT14/WMT14 datasets, we split words
into BPE tokens (Sennrich et al., 2015), forming a 10k/32k
vocabulary shared by source and target languages.

We use the Transformer (Vaswani et al., 2017) model as
the AR teacher, and the vanilla Non-Autoregressive Trans-
former (NAT) (Gu et al., 2017) model with sequence-level
knowledge distillation (Kim & Rush, 2016) as the NAR
baseline. For both AR and NAR models, we use the orig-
inal base setting for the WMT14 dataset, and a small
setting for the IWSLT14 dataset. To investigate the inﬂu-
ence of the model size on our method, we also evaluate
large/base NAT models on WMT14/IWSLT14 datasets
as a larger model setting. These larger NAT models are not
used in the EM iterations. They are merely trained with
the ﬁnal AR teacher from the EM iterations of the original
model (base/small for WMT14/IWSLT14). The detailed
settings of the model architectures can be founded in the
appendix.

We use Adam optimizer (Kingma & Ba, 2014) and employ
a label smoothing (Szegedy et al., 2016) of 0.1 in all experi-
ments. The base and large models are trained for 125k
steps on 8 TPUv3 nodes in each iteration, while the small
models are trained for 20k steps. We use a beam size of
20/5 for the AR model in the M/E-step of our EM training
algorithm. The pseudo bounds {ˆbi} is set by early stopping
with the accuracy on the validation set.

6.2. Inference

During decoding, the target length l = |y| is predicted by
an additional classiﬁer conditional on the source sentence:
l = arg maxT (cid:48) p(T (cid:48)|x). We can also try different target

2Although this method does not allow any word duplication in
the output sequence, it is still able to produce any sequence. To
solve the problem that multiple consecutive occurrences of a token
cannot be captured by our decoding method, we can introduce a
special “(cid:104)concat(cid:105)” symbol. For example, “very very good” can be
represented by “very (cid:104)concat(cid:105) very good”.

3https://wit3.fbk.eu/
4http://statmt.org/wmt14/translation-task.

html

An EM Approach to Non-autoregressive Conditional Sequence Generation

Table 3. Performance of BLEU score on WMT14 En-De/De-En
and IWSLT14 De-En tasks for single-pass NAR models. ”/” de-
notes that the results are not reported in the original paper. Trans-
former (Vaswani et al., 2017) results are based on our own repro-
duction.

Table 4. Performance of BLEU score on WMT14 En-De/De-En
and IWSLT14 De-En tasks for NAR models with rescoring or
iterative reﬁnement. ”/” denotes that the results are not reported
in the original paper. Transformer (Vaswani et al., 2017) results
are based on our own reproduction.

Models

Transformer

w/ beam size 5

WMT14

IWSLT14

En-De De-En De-En
Autoregressive teacher model

Latency Speedup

27.84 32.14

34.69

393ms

1.00×

Non-autoregressive models

/

/

NAT-FT
LT
ENAT
NAT-BAN
NAT-REG
NAT-HINT
NAT-CTC
FlowSeq-base
ReorderNAT
NAT-CRF

17.69 21.47
19.80
20.65 23.02
21.47
20.65 24.77
21.11 25.24
17.68 19.80
21.45 26.16
22.79 27.28
23.44 27.22
Ours
19.55 23.44
+ EM training
23.27 26.73
+ ODD decoding 24.54 27.93

NAT baseline

/
/
24.13
/
23.89
25.55
/
27.55
/
27.44

22.35
29.38
30.69

15.6×†
39ms†
105ms†
/
25.3×†
24ms†
/
/
27.6×†
22ms†
26ms†
30.2×†
350ms† 3.42×†

/
/
37ms†

/
16.1×†
10.4×†

22ms
22ms
24ms

17.9×
17.9×
16.4×

lengths ranging from (l − b) to (l + b) and obtain multiple
translations with different lengths, where b is the half-width,
and then use the AR model pAR(y|x; φ1) as the rescorer
to select the best translation. Such a decoding and rescor-
ing process can be conducted in parallel and is referred as
parallel length decoding. To make a fair comparison with
previous work, we set b to 4 and use 9 candidate translations
for each sentence. For each dataset, we evaluate the model
performance with the BLEU (Papineni et al., 2002) score.
5 We evaluate the average per-sentence decoding latency6
on WMT14 En-De test sets with batch size 1 on a single
NVIDIA GeForce RTX 2080 Ti GPU by averaging 5 runs.

6.3. Main Results

We compare our model with the Transformer (Vaswani et al.,
2017) teacher model and several NAR baselines, includ-
ing NAT-FT (Gu et al., 2017), LT (Kaiser et al., 2018),
ENAT (Guo et al., 2018), NAT-BAN (Zhou et al., 2019),
NAT-REG (Wang et al., 2019), NAT-HINT (Li et al., 2018),
NAT-CTC (Libovick`y & Helcl, 2018), Flowseq (Ma et al.,
2019), ReorderNAT (Ran et al., 2019), NAT-CRF (Sun et al.,
2019), NAT-IR (Lee et al., 2018), CMLM (Ghazvininejad
et al., 2019), and LevT (Gu et al., 2019).

5We follow common practice in previous works to make a fair
comparison. Speciﬁcally, we use tokenized case-sensitive BLEU
for WMT datasets and case-insensitive BLEU for IWSLT datasets.
6† in Tab. 3 and Tab. 4 indicates that the latency and the speedup
may be affected by hardware settings and are thus not fair for direct
comparison.

Models

Transformer

w/ beam size 5

WMT14

IWSLT14

En-De De-En De-En
Autoregressive teacher model

Latency Speedup

27.84 32.14

34.69

393ms

1.00×

FlowSeq

NAT-FT (rescore 10)
LT (rescore 10)
ENAT (rescore 9)
NAT-REG (rescore 9)
NAT-HINT (rescore 9)

/

Non-autoregressive models
/
18.66 22.41
/
21.00
27.30
24.28 26.10
28.04
24.61 28.90
28.80
25.20 29.52
/
base (rescore 15) 23.08 28.07
/
large (rescore 15) 25.03 30.48
/
24.74 29.11
29.99
26.07 29.68
/
21.61 25.48
/
25.94 29.90
27.03 30.53
/
27.27
/

/

ReorderNAT (rescore 7)
NAT-CRF (rescore 9)
NAT-IR (10 iterations)
(4 iterations)
(10 iterations)

CMLM

LevT (7+ iterations)

7.68×†
79ms†
/
/
12.4×†
49ms†
15.1×†
40ms†
17.8×†
44ms†
1.04×†
/
0.96×†
/
7.40×†
/
63ms†
6.14×†
222ms† 1.88×†
3.05×†
1.30×†
3.55×†

/
/
/

NAT baseline (rescore 9)

+ EM training
+ ODD decoding
+ larger model

Ours
22.24 26.21
25.03 28.78
25.75 29.29
26.21 29.81

29.64
31.74
32.66
33.25

41ms
41ms
43ms
65ms

9.59×
9.59×
9.14×
6.05×

Tab. 3 provides the performance of our method with maxi-
mum likely target length l, together with other NAR base-
lines that generate output sequences in a single pass. From
the table, we can see that the EM training contributes most
to the improvement of the performance. The optimal de-
duplicated (ODD) decoding also signiﬁcantly improves the
model performance. Compared with other models, our
method signiﬁcantly outperforms all of them, with nearly
no additional overhead compared with the vanilla NAT.

Tab. 4 illustrates the performance of our method equipped
with rescoring and other baselines equipped with rescor-
ing or iterative reﬁnement. Since our method has nearly
no additional overhead compared with the vanilla NAT, to
make a fair comparison with previous work (Kaiser et al.,
2018; Lee et al., 2018; Ma et al., 2019; Sun et al., 2019;
Gu et al., 2019), we also show the results of our method
with a larger model setting. From the table, we can still
ﬁnd that the EM training signiﬁcantly improves the perfor-
mance of the vanilla NAT model, but the effect of the OOD
decoding is not as signiﬁcant as in the single-pass setting.
This shows that the rescoring process can mitigate the word
duplication problem to some extent. Surprisingly, we also
ﬁnd that using the larger model also does not bring much
gain. A potential explanation for this is that since our EM
algorithm signiﬁcantly simpliﬁes the training dataset and
the NAT model can be over-parameterized, there is no much
gain in further increasing the model size. Compared with

An EM Approach to Non-autoregressive Conditional Sequence Generation

other baselines, our method signiﬁcantly outperforms these
rescored single-pass NAR methods and achieves competi-
tive performance to iterative-reﬁnement models with a much
better speedup. Note that these iterative-reﬁnement mod-
els (Lee et al., 2018; Ghazvininejad et al., 2019; Gu et al.,
2019) still rely on sequence-level knowledge distillation in
training, which indicates that it is still a hard problem for
these approaches to capture multi-modality in the real data.
Our EM algorithm may further improve their performance.
We leave combining the two techniques for future work.

6.4. Analysis of the Convergence

We analyze the convergence of the EM algorithm. We show
the dynamics of the performance of the NAR model (test
BLEU), the performance of the AR model (test BLEU),
and the Normalized Corpus-level Multi-modality (NCM,
deﬁned by Eq. 7) on the WMT14 En-De dataset. The results
are shown in Tab. 5.

We describe the detailed optimization process here to clarify
how our EM algorithm works with early stopping in this
example. In the ﬁrst 5 iterations, as we have no idea how to
set {ˆbi} precisely, we simply set them to zeros. But after the
5th iteration, we ﬁnd an accuracy drop in the validation set.
So we will re-use the quality metrics at the 4th iteration to
set {ˆbi} and continue the EM algorithm until convergence.

We can see that our EM algorithm takes only a few iterations
to convergence, which is very efﬁcient. With the EM algo-
rithm continues, the NCM metric, which can be regarded
as the optimization objective, decreases monotonically. The
performance of the NAR model and the performance of the
AR model also converge after 5 iterations.

Table 5. Analysis of the convergence for the EM algorithm on
WMT14 En-De test BLEU. NCM represents the Normalized
Corpus-level Multi-modality. All the models are evaluated without
ODD decoding and rescoring. Iteration t = 1, 2, 3, 4, 5∗ are per-
formed without quality constraints. Iteration t = 5, 6 are re-started
from t = 4 with quality constraints.

t = 1
t = 2
t = 3
t = 4
t = 5∗
t = 5
t = 6

NAR Model AR Model NCM
2.88
2.33
2.24
2.16
2.04
2.12
2.11

27.84
27.50
27.13
26.78
26.11
26.72
26.75

19.55
22.27
22.85
23.27
22.86
23.18
23.16

6.5. Analysis of the Amortized Inference

In our EM method, we employ amortized inference and
parametrize the posterior distribution of the target sequences
by an AR model. In this section, we investigate the impor-
tance of amortized inference. Speciﬁcally, we try to directly

train the NAR model on (X t+1, Y t+1) in the M-step. The
results are shown in Tab. 6. We can see that parameterizing
the posterior distribution by a uniﬁed AR model always
improves the performance of the NAR model.

Table 6. Analysis of the amortized inference for iteration t =
2, 3, 4 on WMT En-De test BLEU. All the models are evaluated
without ODD decoding and rescoring. We show the results of the
NAR models using different training data in the M-step.

t = 2
t = 3
t = 4

amortized
22.27
22.85
23.27

non-amortized
21.78
22.44
22.98

6.6. Analysis of the Optimal De-duplicated Decoding

Finally, we analyze the effect of the proposed optimal de-
duplicated (ODD) decoding approach. We compare it with
another plug-and-play de-duplicated decoding approach,
that is, “removing any repetition by collapsing multiple
consecutive occurrences of a token” (Lee et al., 2018), which
we refer to as post-de-duplication. The results are shown in
Tab. 7. We can see that the proposed ODD decoding method
consistently outperforms this empirical method. This shows
that our proposed method can overcome the sub-optimal
problem of the post-de-duplication method.

Table 7. Analysis of the ODD decoding on WMT En-De test
BLEU. All the models are trained with our EM algorithm.

WMT En-De WMT De-En IWSLT
24.96
32.03
30.69
32.66

26.93
28.92
27.93
29.29

23.67
25.56
24.54
25.75

post-de-duplication
+ rescoring 9
ODD decoding
+ rescoring 9

7. Conclusion

This paper proposes a novel EM approach to non-
autoregressive conditional sequence generation, which ef-
fectively addresses the multi-modality issue in NAR training
by iterative optimizing both the teacher AR model and the
student NAR model. We also developed a principled plug-
and-play decoding method for efﬁciently removing word
duplication in the model’s output. Experimental results on
three tasks prove the effectiveness of our approach. For
future work, we plan to examine the effectiveness of our
method in a broader range of applications, such as text sum-
marization.

Acknowledgements

We thank the reviewers for their helpful comments. This
work is supported in part by the National Science Founda-

An EM Approach to Non-autoregressive Conditional Sequence Generation

tion (NSF) under grant IIS-1546329.

References

Ba, J. L., Kiros, J. R., and Hinton, G. E. Layer normalization.

arXiv preprint arXiv:1607.06450, 2016.

Bahdanau, D., Cho, K., and Bengio, Y. Neural machine
translation by jointly learning to align and translate. arXiv
preprint arXiv:1409.0473, 2014.

Kaiser, Ł., Roy, A., Vaswani, A., Parmar, N., Bengio, S.,
Uszkoreit, J., and Shazeer, N. Fast decoding in sequence
models using discrete latent variables. arXiv preprint
arXiv:1803.03382, 2018.

Kim, Y. and Rush, A. M. Sequence-level knowledge distil-

lation. arXiv preprint arXiv:1606.07947, 2016.

Kingma, D. P. and Ba, J. Adam: A method for stochastic
optimization. arXiv preprint arXiv:1412.6980, 2014.

Furlanello, T., Lipton, Z. C., Tschannen, M., Itti, L., and
Anandkumar, A. Born again neural networks. arXiv
preprint arXiv:1805.04770, 2018.

Lafferty, J., McCallum, A., and Pereira, F. C. Conditional
random ﬁelds: Probabilistic models for segmenting and
labeling sequence data. 2001.

Ganchev, K., Gillenwater, J., Taskar, B., et al. Posterior
regularization for structured latent variable models. Jour-
nal of Machine Learning Research, 11(Jul):2001–2049,
2010.

Gehring, J., Auli, M., Grangier, D., Yarats, D., and Dauphin,
Y. N. Convolutional sequence to sequence learning. In
Proceedings of the 34th International Conference on Ma-
chine Learning-Volume 70, pp. 1243–1252. JMLR. org,
2017.

Lee, J., Mansimov, E., and Cho, K. Deterministic non-
autoregressive neural sequence modeling by iterative re-
ﬁnement. arXiv preprint arXiv:1802.06901, 2018.

Li, Z., He, D., Tian, F., Qin, T., Wang, L., and Liu, T.-
Y. Hint-based training for non-autoregressive translation.
2018.

Li, Z., Lin, Z., He, D., Tian, F., Qin, T., Wang, L., and Liu,
T.-Y. Hint-based training for non-autoregressive machine
translation. arXiv preprint arXiv:1909.06708, 2019.

Gershman, S. and Goodman, N. Amortized inference in
probabilistic reasoning. In Proceedings of the annual
meeting of the cognitive science society, volume 36, 2014.

Libovick`y, J. and Helcl, J. End-to-end non-autoregressive
neural machine translation with connectionist temporal
classiﬁcation. arXiv preprint arXiv:1811.04719, 2018.

Ghazvininejad, M., Levy, O., Liu, Y., and Zettlemoyer, L.
Mask-predict: Parallel decoding of conditional masked
language models. In Proceedings of the 2019 Conference
on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural
Language Processing (EMNLP-IJCNLP), pp. 6114–6123,
2019.

Gu, J., Bradbury, J., Xiong, C., Li, V. O., and Socher, R.
Non-autoregressive neural machine translation. arXiv
preprint arXiv:1711.02281, 2017.

Gu, J., Wang, C., and Zhao, J. Levenshtein transformer.

arXiv preprint arXiv:1905.11006, 2019.

Guo, J., Tan, X., He, D., Qin, T., Xu, L., and Liu, T.-Y. Non-
autoregressive neural machine translation with enhanced
decoder input. arXiv preprint arXiv:1812.09664, 2018.

Lin, C.-Y. and Hovy, E. Automatic evaluation of summaries
using n-gram co-occurrence statistics. In Proceedings
of the 2003 Human Language Technology Conference
of the North American Chapter of the Association for
Computational Linguistics, pp. 150–157, 2003.

Ma, X., Zhou, C., Li, X., Neubig, G., and Hovy, E. Flowseq:
Non-autoregressive conditional sequence generation with
generative ﬂow. arXiv preprint arXiv:1909.02480, 2019.

McLachlan, G. J. and Krishnan, T. The EM algorithm and
extensions, volume 382. John Wiley & Sons, 2007.

Papineni, K., Roukos, S., Ward, T., and Zhu, W.-J. Bleu: a
method for automatic evaluation of machine translation.
In Proceedings of the 40th annual meeting on association
for computational linguistics, pp. 311–318. Association
for Computational Linguistics, 2002.

He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learn-
ing for image recognition. In Proceedings of the IEEE
conference on computer vision and pattern recognition,
pp. 770–778, 2016.

Ran, Q., Lin, Y., Li, P., and Zhou, J. Guiding non-
autoregressive neural machine translation decoding with
reordering information. arXiv preprint arXiv:1911.02215,
2019.

Hinton, G., Vinyals, O., and Dean, J.
the knowledge in a neural network.
arXiv:1503.02531, 2015.

Distilling
arXiv preprint

Sennrich, R., Haddow, B., and Birch, A. Neural machine
translation of rare words with subword units. arXiv
preprint arXiv:1508.07909, 2015.

An EM Approach to Non-autoregressive Conditional Sequence Generation

Shao, C., Zhang, J., Feng, Y., Meng, F., and Zhou,
J. Minimizing the bag-of-ngrams difference for non-
autoregressive neural machine translation. arXiv preprint
arXiv:1911.09320, 2019.

Stern, M., Chan, W., Kiros, J., and Uszkoreit, J. Insertion
transformer: Flexible sequence generation via insertion
operations. arXiv preprint arXiv:1902.03249, 2019.

Sun, Z., Li, Z., Wang, H., He, D., Lin, Z., and Deng, Z. Fast
structured decoding for sequence models. In Advances in
Neural Information Processing Systems, pp. 3011–3020,
2019.

Sutskever, I., Vinyals, O., and Le, Q. V. Sequence to se-
quence learning with neural networks. In Advances in
neural information processing systems, pp. 3104–3112,
2014.

Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., and Wojna,
Z. Rethinking the inception architecture for computer vi-
sion. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pp. 2818–2826, 2016.

Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,
L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Atten-
tion is all you need. In Advances in neural information
processing systems, pp. 5998–6008, 2017.

Wang, Y., Tian, F., He, D., Qin, T., Zhai, C., and Liu, T.-Y.
Non-autoregressive machine translation with auxiliary
regularization. arXiv preprint arXiv:1902.10245, 2019.

Wei, B., Wang, M., Zhou, H., Lin, J., and Sun, X.

Im-
itation learning for non-autoregressive neural machine
translation. arXiv preprint arXiv:1906.02041, 2019.

Williams, R. J. Simple statistical gradient-following algo-
rithms for connectionist reinforcement learning. Machine
learning, 8(3-4):229–256, 1992.

Wu, L., Tian, F., Qin, T., Lai, J., and Liu, T.-Y. A study
of reinforcement learning for neural machine translation.
arXiv preprint arXiv:1808.08866, 2018.

Yun, C., Bhojanapalli, S., Rawat, A. S., Reddi, S., and
Kumar, S. Are transformers universal approximators of
sequence-to-sequence functions? In International Confer-
ence on Learning Representations, 2020. URL https:
//openreview.net/forum?id=ByxRM0Ntvr.

Zhang, J., Liu, Y., Luan, H., Xu, J., and Sun, M. Prior knowl-
edge integration for neural machine translation using pos-
terior regularization. arXiv preprint arXiv:1811.01100,
2018.

Zhou, C., Neubig, G., and Gu, J. Understanding knowl-
edge distillation in non-autoregressive machine transla-
tion. arXiv preprint arXiv:1911.02727, 2019.

An EM Approach to Non-autoregressive Conditional Sequence Generation

A. Non-autoregressive Transformers are Universal Approximators of Sequence-to-Sequence

Functions

A.1. Problem Deﬁnition

A non-autoregressive Transformer (Vaswani et al., 2017) a Transformer encoder and a non-autoregressive Transformer
decoder. More concretely, both encoder and decoder consist of two types of layers: the multi-head attention layer Attn and
the token-wise feed-forward layer FF, with both layers having a skip connection7 (He et al., 2016). The encoder block in
the non-autoregressive Transformer tenc maps an input X ∈ Rd×n consisting of d-dimensional embeddings of n tokens
to an output tenc(X) ∈ Rd×m. It consists of a self-attention layer and a feed-forward layer. The decoder block in the
non-autoregressive Transformer tdec maps an input Y ∈ Rd×m consisting of d-dimensional embeddings of m tokens and a
context X ∈ Rd×n consisting of d-dimensional embeddings of n tokens to an output tdec(X, Y ) ∈ Rd×m. It consists of a
self-attention layer, a encoder-decoder attention layer, and a feed-forward layer:

Attn(X, Y ) = Y +

h
(cid:88)

i=1

W i

OW i

V X · σ (cid:2)(W i

QX)T (W i

KY )(cid:3) ,

FF(Y ) = Y + W2 · ReLU(W1 · Y ),
tenc(X) = FF(Attnenc−self (X, X)),

tdec(X, Y ) = FF(Attnenc−dec(X, Attndec−self (Y , Y ))),

(9)

(10)

(11)

(12)

O ∈ Rd×k, W i

Q ∈ Rk×d, W2 ∈ Rd×r, and W1 ∈ Rr×d are learnable parameters. σ is the softmax
where W i
function. Following Yun et al. (2020), we also do not use layer normalization (Ba et al., 2016) in the setup of our analysis.
The family of the Transformer encoders is Rd×n → Rd×n functions and can be deﬁned as:

K, W i

V , W i

T h,k,r
enc

:=




h : Rd×n → Rd×n



(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

X 0 = X
X i = th,k,r
h(X) = X M






enc (X i−1)

,

(13)

enc

denotes a Transformer encoder block deﬁned by an attention layer with h heads of size k each, and a

where th,k,r
feed-forward layer with r hidden nodes. M is the number of stacked blocks.
Similarly, the family of the non-autoregressive Transformer decoders is Rd×(n+m) → Rd×m functions and can be deﬁned
as:

T h,k,r
dec

:=

h : Rd×(n+m) → Rd×m

dec (X, Y i−1)

,

(14)






(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Y 0 = Y
Y i = th,k,r
h(X, Y ) = Y N






dec denotes a Transformer decoder block deﬁned by attention layers with h heads of size k each and a feed-forward

where th,k,r
layer with r hidden nodes. N is the number of stacked blocks.
Finally, the family of non-autoregressive Transformers is Rd×n → Rd×m functions and can be deﬁned as:

T h,k,r :=

(cid:110)

g(X) = h2(h1(X + E1), E2)

(cid:12)
(cid:12) h1 ∈ T h,k,r
(cid:12)

enc

and h2 ∈ T h,k,r

dec

(cid:111)

,

(15)

where E1 ∈ Rd×n and E2 ∈ Rd×m are the trainable positional embeddings.

A.2. Transformer Encoders are Universal Approximators of Sequence-to-Sequence Functions (Yun et al., 2020)

Recently, Yun et al. (2020) show that the Transformer encoders equipped with positional embeddings are universal
approximators of all continuous Rd×n → Rd×n functions that map a compact domain in Rd×n to Rd×n.

We ﬁrst describe the results in Yun et al. (2020). Let us start by deﬁning the target function class Fenc, which consists of
all continuous sequence-to-sequence functions with compact support that map a compact domain in Rd×n to Rd×n. Here

7The bias b is omitted for all matrix multiplication operations for brevity.

An EM Approach to Non-autoregressive Conditional Sequence Generation

continuity is deﬁned with respect to any entry-wise (cid:96)p norm, 1 ≤ p < ∞. Given two functions f1, f2 : Rd×n → Rd×n, for
1 ≤ p < ∞, we deﬁne a distance between them as

(cid:18)(cid:90)

dp(f1, f2) :=

(cid:107)f1(X) − f2(X)(cid:107)p

pdX

(cid:19)1/p

.

The Transformer encoders with positional embeddings is deﬁned as:

P−enc := (cid:8)hPX = h(X + E)|h ∈ T h,k,r
T h,k,r

enc

and E ∈ Rd×n(cid:9) ,

(16)

(17)

where E is learnable positional embeddings. The following result shows that a Transformer encoder with positional
embeddings with a constant number of heads h, head size k, and hidden layer of size r can approximate any function in
Fenc:
Theorem A.1. Let 1 ≤ p < ∞ and (cid:15) > 0, then for any given f ∈ Fenc, there exists a Transformer encoder h ∈ T 2,1,4
P−enc
such that we have dp(f, h) ≤ (cid:15).

We provide the sketch of the proof in Yun et al. (2020) here. Without loss of generality, we can assume that the compact
support of f is contained in [0, 1]d×n. The proof of Theorem A.1 can be achieved in the following three steps:

Step 1: Approximate Fenc with piece-wise constant functions. We ﬁrst use (a variant of) the classical result that any
continuous function can be approximated up to arbitrary accuracy by piece-wise constant functions. For δ > 0, we deﬁne
the following class of piece-wise constant functions:

F enc(δ) :=

(cid:110)

f : X (cid:55)→

(cid:88)

(cid:12)
AL1{X ∈ GL}
(cid:12)

(cid:12) AL ∈ Rd×n(cid:111)

,

L∈Gδ

(18)

where Gδ := {0, δ, . . . , 1 − δ}d×n and, for a grid point L ∈ Gδ, SL := (cid:81)d
the associated cube of width δ. Let f ∈ F enc(δ) be such that dp(f, f ) ≤ (cid:15)/3.

j=1

(cid:81)n

k=1[Lj,k, Lj,k + δ) ⊂ [0, 1]d×n denotes

Step 2: Approximate F enc(δ) with modiﬁed Transformer encoders. We then consider a slightly modiﬁed architecture
for Transformer networks, where the softmax operator σ[·] and ReLU(·) are replaced by the hardmax operator σH[·] and an
activation function φ ∈ Φ, respectively. Here, the set of allowed activations Φ consists of all piece-wise linear functions
h,k,r
with at most three pieces, where at least one piece is constant. Let T
enc denote the function class corresponding to the
sequence-to-sequence functions deﬁned by the modiﬁed Transformer encoders. The following result establishes that the
2,1,1
enc can closely approximate functions in F enc(δ).
modiﬁed Transformer encoders in T

Proposition A.1. For each f ∈ F enc(δ) and 1 ≤ p < ∞, ∃ g ∈ T

2,1,1
enc

such that dp(f , g) = O(δd/p).

Step 3: Approximate modiﬁed Transformer encoders with (original) Transformer encoders. Finally, we show that
g ∈ T

can be approximated by T 2,1,4. Let g ∈ T 2,1,4 be such that dp(g, g) ≤ (cid:15)/3.

2,1,1

Theorem A.1 now follows from these three steps, because we have

dp(f, g) ≤ dp(f, f ) + dp(f , g) + dp(g, g) ≤ 2(cid:15)/3 + O(δd/p).

(19)

Choosing δ small enough ensures that dp(f, g) ≤ (cid:15).

A.3. Proof Sketch of Proposition A.1 (Yun et al., 2020)

Especially, the proof of Proposition A.1 is decomposed into three steps:

Sub-step 1: Quantization by feed-forward layers Given an input X ∈ Rd×n, a series of feed-forward layers in the
modiﬁed Transformer encoder can quantize X to an element L on the extended grid G+
δ := {−δ−nd, 0, δ, . . . , 1 − δ}d×n.

Sub-step 2: Contextual mapping by self-attention layers Next, a series of self-attention layers in the modiﬁed Trans-
former encoder can take the input L and implement a contextual mapping q : Gδ → Rn such that, for L and L(cid:48) that are not
permutation of each other, all the elements in q(L) and q(L(cid:48)) are distinct.

An EM Approach to Non-autoregressive Conditional Sequence Generation

Sub-step 3: Function value mapping by feed-forward layers Finally, a series of feed-forward layers in the modiﬁed
Transformer encoder can map elements of the contextual embedding q(L) to the desired output value of f ∈ F enc at the
input X.

A.4. Non-autoregressive Transformers are Universal Approximators of Sequence-to-Sequence Functions

In this paper, we take a further step and show that the non-autoregressive Transformers are universal approximators of all
continuous Rd×n → Rd×m functions that map a compact domain in Rd×n to Rd×m, where n and m can be different.

We start with describing the formal form of Theorem 4.1 in the main text. In the non-autoregressive conditional sequence
generation problem, the target function class Fs2s becomes the set of all continuous sequence-to-sequence functions with
compact support that map a compact domain in Rd×n to Rd×m, where n and m can be different. Given two functions
f1, f2 : Rd×n → Rd×m, for 1 ≤ p < ∞, similar to Eq. 16, we deﬁne a distance between them as

(cid:18)(cid:90)

dp(f1, f2) :=

(cid:107)f1(X) − f2(X)(cid:107)p

pdX

(cid:19)1/p

.

(20)

With the deﬁnition of non-autoregressive Transformers in Eq. 15, we have the following result:

Theorem A.2. Let 1 ≤ p < ∞ and (cid:15) > 0, then for any given f ∈ Fs2s, there exists a non-autoregressive Transformer
g ∈ T 2,1,4 such that we have dp(f, g) ≤ (cid:15).

The proof of Theorem A.2 can be done in a similar way as Theorem A.1. Especially, the step 1 and step 3 in the proof of
Theorem A.1 can be seamlessly used here. We refer the readers to Yun et al. (2020) for the detailed proof of these two steps.

The step 2 in the proof Theorem A.2 is a bit different. Basically, we need to prove the following result:

Proposition A.2. For each f ∈ F s2s(δ) and 1 ≤ p < ∞, ∃ g ∈ T

2,1,1

such that dp(f , g) = O(δd/p).

where F s2s(δ) and T
are deﬁned in a similar way as F enc(δ) and T
A.1, the proof of Proposition A.2 can be decomposed into three steps:

2,1,1

2,1,1
enc , respectively. Similar to the proof of Proposition

Sub-step 1∗: Quantization by feed-forward layers in the encoder Given an input X ∈ Rd×n, a series of feed-forward
layers in the encoder of the modiﬁed non-autoregressive Transformer can quantize X to an element L on the extended grid
G+

δ := {−δ−nd, 0, δ, . . . , 1 − δ}d×n.

Sub-step 2∗: Contextual mapping by attention layers in the encoder and the decoder Next, a series of attention
layers in the encoder and decoder of the modiﬁed non-autoregressive Transformer can take the input L and implement a
contextual mapping q : Gδ → Rm such that, for L and L(cid:48) that are not permutation of each other, all the elements in q(L)
and q(L(cid:48)) are distinct.

Sub-step 3∗: Function value mapping by feed-forward layers in the decoder Finally, a series of feed-forward layers
in the decoder of the modiﬁed non-autoregressive Transformer can map elements of the contextual embedding q(L) to the
desired output value of f ∈ F s2s at the input X.

Since Sub-step 1∗ and Sub-step 3∗ are exactly the same as Sub-step 1 and Sub-step 3 in the proof of Proposition A.1, we
only provide the proof of Sub-step 2∗. We refer the readers to Yun et al. (2020) for the detailed proof of these two sub-steps.

A.5. Proof of Sub-step 2∗ in Proposition A.2

Without loss of generality, we can assume that the compact support of f is contained in [0, 1]d×n. Following Yun et al.
(2020), we choose

E1 =


0
0


...


0

1
1
...
1

2
2
...
2

· · · n − 1
· · · n − 1

...

· · · n − 1








.

An EM Approach to Non-autoregressive Conditional Sequence Generation

and

E2 =


0
0


...


0

1
1
...
1

2
2
...
2

· · · m − 1
· · · m − 1








.

...

· · · m − 1

By Sub-step 1∗, we quantize any input X + E1 to its quantized version with the feed-forward layers in the Transformer
encoder. We call this quantized version L:

L ∈ [0 : δ : 1 − δ]d × [1 : δ : 2 − δ]d × · · · × [n − 1 : δ : n − δ]d.

We do not need to quantize E2 in our Sub-step 1∗ with the feed-forward layers in the Transformer decoder because E2 is
already quantized.

As done in Lemma 6 in Yun et al. (2020), we deﬁne u := (1, δ−1, . . . , δ−d+1) and lj := uT L:,j, for all j ∈ [n]. Next,
following the construction in Appendix C.2 in Yun et al. (2020), with n(1/δ)d self-attention layers in the Transformer
encoder, we can get (cid:101)l1 < (cid:101)l2 < · · · < (cid:101)ln such that the map from L to (cid:101)ln is one-to-one. In addition, (cid:101)ln is bounded by
nδ−(n+1)d.

Finally, in a similar way as Appendix B.5.1 in Yun et al. (2020), we add an extra encoder-decoder attention layer with
attention part nδ−(n+1)d−1Ψ(·; 0). This layer shifts all the layers in the Transformer decoder by nδ−(n+1)d−1(cid:101)ln. We deﬁne
the output of this layer as gc(L). In this way, we ensure that different contexts L are mapped to distinct numbers in uT gc(L),
thus implementing a contextual mapping.

B. Proof of Proposition 5.1

We prove this proposition by contradiction. Assuming that the k-th likely label in position i is chosen by the CRF algorithm
and k > 3, we consider the following two cases:

Case 1: i = 0 is the ﬁrst position or i = n − 1 is the last position. Without loss of generality, we can assume i = 0.
The ﬁrst and second labels in the current decoding is denoted by l∗
1. We also denote the top 2 labels in position 0 as
l0,1 and l0,2. If l0,1 = l∗
0 to be l0,2, which construct a label sequence with higher likelihood in the CRF model.
Otherwise, we can set l∗

1, we can set l∗
0 to be l0,1. In both cases, k > 3 is not the optimal solution.

0 and l∗

Case 2: i is the neither the ﬁrst position nor the last position. We denote the labels on the position before and after i
as l∗
i+1. We denote the j-th likely label on the position i as li,j. In this case, we will always ﬁnd such a j ≤ 3 that
li,j (cid:54)= l∗

i+1. Therefore, k > 3 is still not the optimal solution.

i−1 and li,j (cid:54)= l∗

i−1 and l∗

C. Illustration of Different Decoding Approaches

Fig. 2 shows how the proposed optimal de-duplicated decoding method solves the sub-optimal decoding problem of the post
de-duplication method.

D. Model Settings

The Non-Autoregressive Transformer model (NAT) is developed using the general encoder-decoder framework which is the
same as the Autoregressive Transformer (AT) model. Fig. 3 shows the architectures of NAT and AT. We use a simpliﬁed
version of the NAT model in Gu et al. (2017), that is, we do not copy the source embeddings as the input of the Transformer
decoder and do not use the positional attention proposed in Gu et al. (2017). The input to our Transformer decoder is simply
the padding symbols. More details about the description about the architectures can be found in Vaswani et al. (2017); Gu
et al. (2017).

We use four model settings in our experiments, including toy, small, base, and large. The detailed conﬁgurations of
these four model settings can be found in Tab. 8.

An EM Approach to Non-autoregressive Conditional Sequence Generation

Figure 2. Illustration of different decoding methods. Darker boxes represent higher likelihood. The argmax decoding method produces
“An approach appraoch” as the result, which contains word duplication. The empirical post de-duplication method can solve the word
duplication problem, but after collapsing, the length of the target sequence is changed. This will cause a discrepancy between the predicted
target length and the actual sequence length and thus make the ﬁnal output sub-optimal. The proposed Optimal De-Duplicated (ODD)
decoding can produce the optimal prediction in the CRF framework. Note that OOD decoding only needs to consider the top-3 labels for
each position in the forward-backward algorithm, which is very efﬁcient.

E. Analysis of Training Examples for the NAR Model

In Tab. 9, we present randomly picked examples from the training data for the NAR model on the WMT14 De-En task. We
can ﬁnd that the proposed EM algorithm constantly changes the training examples for the NAR model.

F. Analysis of Translation Results

In Tab. 10, we present randomly picked translation outputs from the test set of WMT14 De-En. We have the following
observations:

• The proposed OOD decoding method preserves the original predicted length of tokens, which avoid the sub-optimal

problem of the post de-duplication method.

• The proposed EM algorithm can effectively jointly optimize both the AR model and the NAR model. During EM
iterations, the multi-modality in the AR models is reduced, while the translation quality of the NAR models is improved.

Table 8. Transformer model settings

toy
small
base
large

encoder-layer
3
5
6
6

decoder-layer
3
5
6
6

hidden-size
256
256
512
1024

ﬁlter-size
1024
1024
2048
4096

num-heads
4
4
8
16

Optimal De-Deduplicated Decoding (CRF)AnAEMapproachmethodAnAEMapproachmethodAnAEMapproachmethodAmethodAmethodAapproachmethodAnAnEMEMAnapproachapproachEMAn EM approachAnapproachapproachPost De-DuplicationAn approachAn EM Approach to Non-autoregressive Conditional Sequence Generation

Figure 3. The architectures of Autoregressive Transformer and Non-autoregressive Transformer used in this paper.

An EM Approach to Non-autoregressive Conditional Sequence Generation

Source
Ground Truth
Iteration 0
Iteration 1
Iteration 2

Source
Ground Truth

Iteration 0
Iteration 1

Iteration 2

Source
Ground Truth
Iteration 0
Iteration 1
Iteration 2

Source
Ground Truth
Iteration 0
Iteration 1
Iteration 2

Source

Ground Truth

Iteration 0

Iteration 1

Iteration 2

Source
Ground Truth
Iteration 0

Iteration 1
Iteration 2

Source

Ground Truth

Iteration 0

Iteration 1

Iteration 2

, um den Korb zu verkleinern ( bis 75 % ) und in die Ecke zu schieben .
to resize the fragment ( by 75 % ) and move it to the lower right corner .
to reduce the basket ( up to 75 % ) and move it to the corner .
to reduce the basket ( up to 75 % ) and push it into the corner .
to reduce the basket ( up to 75 % ) and put it into the corner .

In den Interviews betonten viele Mnner , dass ihre Erwerbsabweichung ihre Karriere behindere .
In the interviews , many men emphasized that their employment deviation has hindered their
careers .
In the interviews , many men emphasized that their divorce in employment hinders their careers .
In the interviews , many men stressed that their deviation from employment hindered their
careers .
In the interviews , many men stressed that their deviation in employment hinders their careers .

Um einen Pferd gesund und munter zu halten , mssen Sie seine physischen Bedrfnisse beachten .
To keep your horse well , healthy and content you must satisfy its physical needs .
In order to keep a horse healthy and healthy , you must take into account its physical needs .
In order to keep a horse healthy and cheerful , you must take into account its physical needs .
In order to keep a horse healthy and cheerful , you must take into account your physical needs .

Der Effekt von &apos; Eiskltefalle &apos; wird nun bei erlittenem Schaden abgebrochen .
Freezing Trap now breaks on damage .
Ice Cat Trap effect will now be discarded if damage is dealt .
The effect of Ice Cage Trap will now be aborted in case of damage suffered .
The effect of ice cold trap is now aborted in the event of damage suffered .

Wir haben in der Europischen Union Mglichkeiten , wirksam gegen die Arbeitslosigkeit vorzuge-
hen und zwar so , da man da , wo es am ntigsten ist , auch etwas davon sprt .
We have the opportunity , in the EU , to do something that will have a positive effect on
unemployment , characterised by taking action where there is the greatest need .
We in the European Union have the means to combat unemployment effectively , in such a way
that we feel something of it where it is most necessary .
We in the European Union have opportunities to take effective action against unemployment , in
such a way that we can feel something about it where it is most necessary .
We in the European Union have opportunities to take effective action against unemployment , in
such a way that we also feel something of it where it is most necessary .

In der Altstadt sind die Gassen so eng und verwinkelt , dass ein Auto nur mhsam vorankommt .
Kaneo Settlement - Start the walk to Kaneo from St. Sophia church .
In the old town , the streets are so narrow and winding that a car can only progress with difﬁculty
.
In the old town , the streets are so narrow and winding that a car is progressing hard .
In the old town , the streets are so narrow and winding that a car is only progressing laboriously .

Sie mssen sich nur einmal die weltweit steigende Anzahl und Huﬁgkeit von Naturkatastrophen
ansehen , um die Folgen der Klimavernderung zu erkennen .
They only need to look at the increasing number and frequency of natural disasters worldwide to
see its impact .
You only have to look at the increasing number and frequency of natural disasters worldwide to
see the consequences of climate change .
They only have to look at the increasing number and frequency of natural disasters around the
world to see the consequences of climate change .
They only have to look at the increasing number and frequency of natural disasters around the
world in order to identify the consequences of climate change .

Table 9. Examples in the training data for the NAR model on the WMT14 De-En task.

An EM Approach to Non-autoregressive Conditional Sequence Generation

Source

Ground Truth

AR model - Iter 0

AR model - Iter 2

NAR model - Iter 0

NAR model - Iter 0 w/ post de-duplication

NAR model - Iter 2

NAR model - Iter 2 w/ post de-duplication

NAR model - Iter 2 w/ ODD decoding

Source

Ground Truth

AR model - Iter 0

AR model - Iter 2

NAR model - Iter 0

NAR model - Iter 0 w/ post de-duplication

NAR model - Iter 2

NAR model - Iter 2 w/ post de-duplication

NAR model - Iter 2 w/ ODD decoding

Sie ist die Tochter von Peter Tunks , einem ehemaligen Spieler der australischen
Rubgy @-@ Liga , der sich an das Auenministerium in Canberra mit der Bitte
um Hilfe fr seine Tochter gewandt hat .
She is the daughter of former Australian league player Peter Tunks , who has
appealed to the Department of Foreign Affairs in Canberra to assist his daughter
.
She is the daughter of Peter Tunks , a former player of the Australian Rubgy
League , who has addressed to the Ministry of Foreign Affairs in Canberra asking
for help for his daughter .
She is the daughter of Peter Tunks , a former player of the Australian Rubgy
League who addressed the Ministry of Foreign Affairs in Canberra asking for
help for his daughter .
She is the daughter of Peter Tunks , a former player of the Australian Rubgy
League , who addressed the Ministry of Foreign Affairs Canberberra asking help
help his daughter .
She is the daughter of Peter Tunks , a former player of the Australian Rubgy
League , who addressed the Ministry of Foreign Affairs Canberra asking help
his daughter .
She is the daughter of Peter Tunks , a former player of the Australian Rubgy
League ague who addressed the Ministry of Foreign Affairs in Canberra ra asking
for help to his daughter .
She is the daughter of Peter Tunks , a former player of the Australian Rubgy
League ague who addressed the Ministry of Foreign Affairs in Canberra asking
for help to his daughter .
She is the daughter of Peter Tunks , a former player of the Australian Rubgy
League ague who addressed the Ministry of Foreign Affairs in Canberra for
asking for help to his daughter .

In australischen Berichten war zu lesen , dass sie in der Zwischenzeit im Ferienort
Krabi in Sdthailand Urlaub macht .
Reports in Australia said that in the meantime , she was holidaying at the resort
area of Krabi in Southern Thailand .
In Australian reports it was read that in the meantime it is making a holiday in
the holiday resort of Krabi in South thailand .
Australian reports read that , in the meantime , it is a holiday in the resort of
Krabi in southern Thailand .
Australian reports have that , in the meantime , they is a holiday holiday southern
southern in the Krabi of Krabi .
Australian reports have that , in the meantime , they is a holiday southern in the
Krabi of Krabi .
Australian reports read that , in the meantime , it is a holiday holiday the resort
of Kraresort in southern Thailand .
Australian reports read that , in the meantime , it is a holiday the resort of
Kraresort in southern Thailand .
Australian reports read that , in the meantime , it is a holiday in the resort of
Kraresort in southern Thailand .

Table 10. Examples of translation outputs on the WMT14 De-En task. We do not apply rescoring to the NAR model’s outputs.

