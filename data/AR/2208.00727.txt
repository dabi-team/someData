2
2
0
2

g
u
A
1

]
T
S
.
h
t
a
m

[

1
v
7
2
7
0
0
.
8
0
2
2
:
v
i
X
r
a

On the impact of serial dependence on penalized regression methods

Simone Tonini ∗1, Francesca Chiaromonte2, 3, and Alessandro Giovannelli4

1Institute of Economics, Sant’Anna School of Advanced Studies, Pisa, Italy.
2Institute of Economics & EMbeDS, Sant’Anna School of Advanced Studies, Pisa, Italy.

3Department of Statistics, Penn State University, USA.

4University of L’Aquila, Department of Information Engineering, Computer Science and Mathematics, L’Aquila, Italy.

Abstract. This paper characterizes the impact of serial dependence on the non-asymptotic estimation

error bound of penalized regressions (PRs). Focusing on the direct relationship between the degree

of cross-correlation of covariates and the estimation error bound of PRs, we show that orthogonal or

weakly cross-correlated stationary AR processes can exhibit high spurious cross-correlations caused

by serial dependence. In this respect, we study analytically the density of sample cross-correlations

in the simplest case of two orthogonal Gaussian AR(1) processes. Simulations show that our results

can be extended to the general case of weakly cross-correlated non Gaussian AR processes of any

autoregressive order. To improve the estimation performance of PRs in a time series regime, we

propose an approach based on applying PRs to the residuals of ARMA models ﬁt on the observed

time series. We show that under mild assumptions the proposed approach allows us both to reduce

the estimation error and to develop an eﬀective forecasting strategy. The estimation accuracy of our

proposal is numerically evaluated through simulations. To assess the eﬀectiveness of the forecasting

strategy, we provide the results of an empirical application to monthly macroeconomic data relative

to the Euro Area economy.

Keywords: serial dependence, spurious correlation, minimum eigenvalue, penalized regressions.

JEL Classiﬁcation: C13, C18, C22, C31, C46.

∗Corresponding author: simone.tonini@santannapisa.it

Acknowledgments: The authors wish to thank Marco Lippi for suggesting to develop the theoretical part of this

work and for the precious technical support. We are also grateful to Sebastiano Michele Zema and Luca Insolia for their

helpful comments and for the stimulating dialogues.

1

 
 
 
 
 
 
1

Introduction

Much contemporary statistical literature is devoted to the problem of extracting information from

large datasets, which are ubiquitous in many ﬁelds of science (Fan et al., 2013). In the context of high

dimensional regression problems, with a number of predictors comparable to or larger than the sample

size, coeﬃcient estimates produced by ordinary least squares (OLS) are aﬀected by unacceptable

variance, or even numerically unstable or undetermined. Among the approaches proposed to tackle

this issue are penalized regression methods (PRs) which reduce (or shrink) standard OLS regression

coeﬃcients towards zero. This reduces their variance at the price of introducing some bias, but if

the penalty is properly tuned, overall, estimation error improves. In particular, we refer to the most

commonly used PRs, namely, those based on (cid:96)1-penalty (Tibshirani, 1996; Zou, 2006), (cid:96)2-penalty

(Hoerl and Kennard, 1970) and their combinations (Zou and Hastie, 2005). Depending on the form

of the penalty, PRs can produce dense solutions, where coeﬃcients may have small yet non-zero

estimates, or sparse solutions, where less relevant predictors have coeﬃcient estimates equal to zero.

Thus, PRs that provide sparse solutions (sparse PRs) in eﬀect select a subset of predictors relevant

for the regression.

Zhao and Yu (2006); Bickel et al. (2009); Lounici et al. (2009); Negahban et al. (2009, 2012);

Hastie et al. (2015); Zou and Zhang (2009); Xin et al. (2017) studied the estimation properties of

PRs and showed that their non-asymptotic estimation error bound depends critically on the degree

of cross-correlation between covariates. In particular, PRs perform best with orthogonal or weakly

cross-correlated covariates, since the bound is inversely proportional to the minimum eigenvalue of the

sample cross-correlation matrix of the covariates themselves. In this respect we can distinguish two

diﬀerent situations; that of multicollinear covariates, where cross-correlations exists at the population

level, and that of spurious correlations, where covariates may be orthogonal or nearly orthogonal

at the population level but other mechanisms in the way observations are collected generate cross-

correlations in the sample. Unfortunately, spurious correlations become more prevalent the higher the

dimension of the data (Fan and Zhou, 2016; Fan et al., 2018) and can lead to false scientiﬁc discoveries

and wrong statistical inferences (Fan et al., 2013).

In the context of time series, and especially of economic and ﬁnancial time series, the case of

multicollinear covariates has been extensively studied. In particular, Stock and Watson (2002a,b);

Forni et al. (2000, 2003, 2016, 2018) proposed approximated dynamic factor models (DFM) as a means

2

to produce eﬀective forecasts with highly cross-correlated covariates. De Mol et al. (2008); Giannone

et al. (2018) showed that multicollinearity induced by latent factors leads to an approximation of

the factor space with sparse PRs, producing forecasts similar to those of DFMs, but noted that

the selection of covariates is not stable over time. Fan et al. (2020) proposed a consistent strategy

for sparse PRs when covariates can be eﬀectively de-correlated via a few pervasive latent factors.

Medeiros and Mendes (2012) studied the covariate selection consistency of sparse PRs in the presence

of multicollinear time series. Finally, in an exhaustive simulation analysis, Smeekes and Wijler (2018)

showed that sparse PRs improve forecasts over traditional approaches when applied to non-stationary

and co-integrated data despite, again, poor performance in terms of covariate selection.

In this paper we focus on the issue of spurious correlations, as already described in Fan et al.

(2013); Fan and Zhou (2016); Fan et al. (2018), extending its treatment to the time series framework.

The econometric literature has dealt with the issue of autocorrelated errors, referred to as spurious

regression, with seminal papers by Box and Newbold (1971) and Granger and Newbold (1974). Box

and Newbold (1971) pointed out the risk of obtaining a spurious model if suﬃcient care is not placed

on an appropriate formulation for the autocorrelation structure of the errors in the regression equation.

Using simulations, Granger and Newbold (1974) studied the case of two independent drift-free random

walks showing that, as a consequence of autocorrelated regression errors, the usual signiﬁcance tests

on the regression coeﬃcients are invalid. In particular, they concluded that if the residuals are strongly

autocorrelated (low Durbin-Watson value), then the regression equation is misspeciﬁed, whatever the

observed coeﬃcient of determination (R2).

In time series spurious correlations are well known to occur with pairs of independent unit root

processes, not least is the case of two stationary processes. Bartlett (1935) observed that the variance of

the sample correlation between two orthogonal Gaussian AR(1) processes depends on both the sample

size and the degree of serial dependence. In this regard, McGregor (1962) provided the approximate

null distribution of the sample correlation when the series are stationary Markov type. Granger et al.

(2001) explored the possible existence of spurious regression between a pair of independent stationary

series through both theoretical and simulation results, and provided the limiting distribution of the t-

statistic for the linear regression coeﬃcient between two stationary Gaussian AR(1) processes, showing

that the variance of such limiting distribution increases with the degree of serial dependence.

With most of the existing literature focused on the eﬀects of serially dependent errors for the

OLS estimator, our aim is to broaden the picture by showing that the serial dependence of covariates

3

also entails problems for estimating regression coeﬃcients. Our main theoretical contribution is to

show the eﬀect of such serial dependence on the minimum eigenvalue of the sample correlation matrix

of the covariates, which is one of the major determinants of the non-asymptotic estimation error

bound of PRs. Speciﬁcally, we demonstrate how the probability of spurious cross-correlation between

stationary Gaussian AR processes depends not only on the sample size, but also on the degree of serial

dependence. To achieve this result we derived the density of the sample cross-correlation between

orthogonal stationary Gaussian AR(1) processes, adapting the treatment in Anderson (2003) to time

series. Since the theoretical upper bound of the minimum eigenvalue of the sample correlation matrix

of the covariates decreases as the maximum absolute value of the diagonal grows, this ﬁnding for a

single generic sample correlation can be leveraged in settings with more than two covariates.

Beyond our theoretical results, we use Monte Carlo simulations to study the impact of serial

dependence when covariates are not Gaussian, when they do show cross-correlations at the population

level, and when they are generated by more complex models than AR(1). We also consider the case

where covariates are generated by an approximate factor model with an autocorrelated idiosyncratic

component.

Our main results can be summarized as follows: (i) through our theoretical density, we show that,

whenever the autocorrelation coeﬃcients of the AR(1) processes have the same sign, an increase in the

degree of serial dependence induces an increase in the probability of large spurious cross-correlation;

(ii) through simulations, we show that the association between serial dependence and the probability

of large spurious correlation holds much more generally, e.g., in cases where the processes are not

Gaussian, weakly correlated or generated by models diﬀerent than AR(1). These results highlight

that a small minimum eigenvalue is more likely in ﬁnite realizations of serially dependent weakly

correlated (or orthogonal) processes, compared to the case of independent samples – and thus that

serially dependent covariates can cause major problems for the estimation accuracy of PRs.

To mitigate the adverse eﬀects of covariates serial dependence, we propose to apply PRs on the

residuals of ARMA models ﬁtted on the observed time series. We show that, under mild assumptions,

this produces better estimates of regression coeﬃcients, as well as an improved selection of the relevant

ones in sparse regimes. However, our procedure does involves a cost; namely, it hinders our ability

to estimate regression parameters relative to past values of the target variable. To illustrate our

rationale we provide simulations where we apply the standard OLS estimator to residuals of the ARMA

processes, and compare it with some of the best known methods for addressing serial dependence in

4

regression. We then apply LASSO (that is the most representative (cid:96)1-penalty based PR) to ARMA

residuals, evaluating our proposal both on simulated data and on monthly macroeconomic data relative

to the Euro Area economy.

The reminder of the paper is organized as follows. In Section 2 we describe the problem setup and

our contribution. In Section 3 we present our theoretical result on the impact of serial dependence on

sample cross-correlation. In Section 4 we provide simulation studies to corroborate and extend the

theoretical results of Section 3. In Section 5 we introduce and evaluate our proposal for mitigating the

adverse eﬀects of serial dependence, i.e. the application of PRs to ARMA residuals, using simulated

and actual econometric data. In Section 6 we provide some ﬁnal remarks.

The following notations will be used throughout the paper. For any dimension p, bold letters

denote vectors and the corresponding regular letters their elements, for example aaa = (a1, a2, . . . , ap)(cid:48).

Supp(aaa) denotes the support of a vector; that is, {j ∈ {1, 2, . . . , p} : aj (cid:54)= 0}, and |Supp(aaa)| the

support cardinality. The (cid:96)q norm of a vector is ||aaa||q :=

(cid:16)(cid:80)p

j=1 |aj|q(cid:17)1/q

for 0 < q < ∞, with

||aaa||k

q :=

(cid:16)(cid:80)p

j=1 |aj|q(cid:17)k/q

, and with the usual extension ||aaa||0 := |Supp(aaa)|. Bold capital letters denote

matrices, for example AAA. Moreover, 000p denotes a p-length vector of zeros, while III p denotes a p × p

identity matrix. Finally, Sign(r) indicates the sign of a real number r.

2 Problem Setup and Our Contribution

2.1 State-of-the-Art

Let X = {xt}T
y = {yt}T
stationary and absolutely regular processes (cid:8)(yt, x(cid:48)

t=1 denote an n × T rectangular array of observations concerning n covariates, and
t=1 a 1 × T response vector. Assume that y and X are realizations of strictly Gaussian
t) ∈ R1+n, t ∈ Z, n ∈ N(cid:9) deﬁned on the probability
space (Ω, F, P ). Further assume that yt has ﬁnite mean and that xt has mean vector 0 and ﬁnite

second order moments. Let CCCx

the covariates, and (cid:99)CCCx
ij and eigenvalues (cid:98)ψx
(cid:98)cx
1
t xit = 0 and 1
T −1
T −1

0 = 1
max ≥ . . . ≥ (cid:98)ψx
t x2

(cid:80)

(cid:80)

k = E[xtx(cid:48)

t−k], k ≥ 0 indicate the generic lag k covariance matrix of
T −1 XX(cid:48) an estimate of the cross-covariance matrix, with generic element
min. Finally, assume that each xi has been standardized so that

it = 1. We consider the following data generating process (DGP) for the

response variable

y = X(cid:48)ααα + εεε,

(1)

5

where ααα is the n × 1 unknown s-sparse vector of regression coeﬃcients, i.e.

||ααα||0 = s < n, and
εεε ∈ RT is a random noise vector. Note that in equation (1), for simplicity of exposition, at each time

yt depends only on xt (in standard time series literature yt is allowed to depend on lagged vector(s)

xt−l, l = 1, . . . ). If n is comparable to or larger than T , ααα is estimated solving a convex optimization

problem where a quadratic loss function is combined with a regularization penalty:

(cid:98)ααα = argmin
ααα∈Rn

(cid:26) 1
2T

||y − X(cid:48)ααα||2

2 + λ(cid:96)(ααα)

(cid:27)

.

(2)

Here λ > 0 represents the weight of the penalty, and (cid:96) : Rn → R+ is a norm. We note that this setup

allows for misspeciﬁed models as well. The following deﬁnitions will be used in our developments.

Deﬁnition 1 (Strong Convexity): Given a diﬀerentiable function L : Rn → R and the vector diﬀer-

ential operator (cid:53), we say that L is strongly convex with parameter γ > 0 at aaa ∈ Rn if the inequality

L(bbb) − L(aaa) ≥ (cid:53)L(aaa)(cid:48)(bbb − aaa) +

γ
2

||bbb − aaa||2
2

holds for all bbb ∈ Rn.

Strong Convexity (see Negahban et al. 2009, 2012) guarantees a small coeﬃcient estimation error. In

particular, when L (the loss function) is “sharply curved” around its optimum (cid:98)ααα, a small |L(ααα)−L((cid:98)ααα)|
guarantees that ||ααα − (cid:98)ααα||2 is also small. The parameter γ governs the strength of convexity; when L is
twice diﬀerentiable, strong convexity requires the minimum eigenvalue of the Hessian (cid:53)2L(ααα) to be
at least γ for all ααα in a neighborhood of (cid:98)ααα. Thus, since its Hessian is (cid:53)2L(ααα) = (cid:99)CCCx
L(ααα) = 1

2 is strongly convex with parameter γ if and only if (cid:98)ψmin ≥ γ (see Hastie et al.
2015, p. 293). Consequently, in this case ||(cid:98)ααα − ααα||2 depends on (cid:98)ψmin. It is relevant to note that when
n > T the quadratic loss cannot be strongly convex since (cid:99)CCCx
0 is singular and thus (cid:98)ψmin = 0. In this

0, the quadratic loss

2T ||y − X(cid:48)ααα||2

case Bickel et al. (2009) proposed a Restricted Eigenvalue Condition, which is essentially a restriction

on the eigenvalues of (cid:99)CCCx

0 as a function of the degree of sparsity, s. The Restricted Eigenvalue Condition

allows for strong convexity (Deﬁnition 1) to hold in the singular case, and we refer to this as Restricted

Strong Convexity (see Negahban et al. 2012; we provide more details in Supplement A).

Deﬁnition 2 (Dual Norm and Subspace Compatibility Constant): Given a norm (cid:96) and the inner

product (cid:104)·, ·(cid:105), we deﬁne the dual norm of (cid:96) as

(cid:96)∗(vvv) := sup

u∈Rn\{0}

(cid:104)u, v(cid:105)
(cid:96)(u)

.

6

For any subspace A of Rn that captures the constraints underlying (2), we deﬁne the subspace com-

patibility constant with respect to the pair ((cid:96), || · ||2) as

Ψ(A) := sup

u∈A:u(cid:54)=0

(cid:96)(u)
||u||2

.

The following Proposition is derived from Corollary 1 of Negahban et al. (2012) and provides the

non-asymptotic coeﬃcient estimation error bound for PRs.

Proposition 1 Consider the convex optimization problem in (2). If the penalty parameter λ is strictly
T X(cid:48)εεε(cid:1), and strong convexity holds with parameter γ > 0. Then, any optimal
positive and ≥ 2(cid:96)∗ (cid:0) 1

solution (cid:98)α(cid:98)α(cid:98)α satisﬁes the bound

||(cid:98)ααα − ααα||2 ≤ 3

λ
γ

Ψ(A).

Proof: See Corollary 1 in Negahban et al. (2012).

(3)

(cid:4)

The coeﬃcient estimation error bound in Proposition 1 increases with the penalty parameter λ,
T X(cid:48)εεε(cid:1); increases with the subspace
which must be strictly positive and satisfy the lower bound 2(cid:96)∗ (cid:0) 1
compatibility constant Ψ(A), which in turn increases with the size of the model subspace A; and

decreases with the convexity parameter γ. Negahban et al. (2009, 2012) derive the bound for PRs in

the case of independent observations (no serial dependence). To this end, the authors compute the
T X(cid:48)εεε(cid:1) under the assumption that the entries of X and εεε are sub-Gaussian,
probability that λ ≥ 2(cid:96)∗ (cid:0) 1
and assume that strong convexity (or restricted strong convexity) holds with parameter γ, i.e. that

(cid:98)ψx

min ≥ γ (see Corollary 2 in Negahban et al. 2012 for an example of sparse PR estimation error

bound, and Corollary 6 in Negahban et al. 2009 for an example of dense PR estimation error bound).

This analysis shows the role of covariates cross-correlation in determining the estimation accuracy

of PRs. In particular, Proposition 1 shows that PRs perform better if covariates are orthogonal or

weakly correlated in the sample, since high sample cross-correlations correspond to small (cid:98)ψx

min. As

mentioned in the Introduction, high sample cross-correlations may be due to the presence of true,

population-level multicollinearities. In this case Fan et al. (2020) focus on the fact that time series

multicollinearities can be captured with Factor Models, and propose to apply PRs on the estimated

idiosyncratic components obtained by ﬁltering the observed time series through estimated factors (see

Supplement B for more details). However, high sample cross-correlation may also be spurious; this is

the case we wish to tackle in the particular context of time series.

7

2.2 Our Contribution

We argue that spurious correlations are one of the causes that potentially limits the use of PRs in time

series. In particular, we focus on the implications of serial dependence on (cid:98)ψx

min, which determines the

“strength” of strong convexity (see Deﬁnition 1), that is one of the main components of the PRs error

bound presented in Proposition 1. In this respect, we relax the assumption that strong convexity (or

restricted strong convexity) holds with parameter γ > 0, and show that the probability of getting

(cid:98)ψx
min ≤ γ increases as the degree of serial dependence grows. Note that in order to focus on (cid:98)ψx
we are assuming that (cid:99)CCCx
treatment. If n > T and the matrix (cid:99)CCCx

0 is positive deﬁnite. This choice is motivated by the wish to simplify our

0 is singular, we can replicate our arguments considering the

min

probability that the restricted eigenvalue is ≤ γ (see Bickel et al. 2009).

The logical structure behind our theoretical contribution is as follows. Given γ = 1 − τ , τ ∈ [0, 1),

and the upper bound (cid:98)ψx
of the matrix (cid:99)CCCx

min ≤ 1 − maxi(cid:54)=j |(cid:98)cx

ij|, we emphasize the role of a generic oﬀ-diagonal element

0 in determining the probability that (cid:98)ψx

min ≤ γ through the inequalities

(cid:110)

(cid:98)ψx
min ≤ 1 − τ

Pr

(cid:26)

(cid:111)

≥ Pr

1 − max
i(cid:54)=j

|(cid:98)cx
ij| ≤ 1 − τ

(cid:27)

≥ Pr (cid:8)1 − |(cid:98)cx

i(cid:54)=j| ≤ 1 − τ (cid:9) = Pr (cid:8)|(cid:98)cx

i(cid:54)=j| ≥ τ (cid:9) .

(4)

i(cid:54)=j| ≥ τ } plays a role in determining the probability of dealing with a small (cid:98)ψx

Thus, Pr{|(cid:98)cx
consequently, through strong convexity, on the PRs estimation error bound presented in Proposition 1.
It follows that any impact of the degree of serial dependence on Pr{|(cid:98)cx
on such bound.

i(cid:54)=j| ≥ τ } results in an impact

min and

To better illustrate our reasoning, we introduce a toy example where we show numerically the

impact of serial dependence on maxi(cid:54)=j |(cid:98)cx
xt = Dφxt−1 + ut, t = 1, . . . , 100, where Dφ is a 10 × 10 diagonal matrix with the same autocorrelation

min. We generate 10 processes from the model

ij| and (cid:98)ψx

coeﬃcient φ in all positions along the main diagonal, and ut ∼ N (00010, III 10). Note that for these

AR(1) processes the the degree of serial dependence is determined by |φ| and, since the processes

are orthogonal, the minimum eigenvalue of the population cross-correlation matrix CCCx

0 is ψx

min = 1.

We consider ﬁve values for φ, namely 0.0, 0.3, 0.6, 0.9, 0.95, and for each we calculate the average and
the standard deviation of both maxi(cid:54)=j |(cid:98)cx
reported in Figure 1. We see that the stronger the persistence of the process (φ closer to 1) the higher

min on 5000 Monte Carlo replications. Results are

ij| and (cid:98)ψx

is the probability of a large spurious sample correlation (orange circle), which in turn leads to a small

minimum eigenvalue of the sample cross-correlation matrix (blue triangle), as a consequence of (4).

In light of these results, our next task is to derive the ﬁnite sample density of (cid:98)cx

ij for the purpose

8

of formalizing the impact of serial dependence on Pr{|(cid:98)cx
i(cid:54)=j| ≥ τ }. It is noteworthy that when the
covariates have a Factor-based DGP (as in Supplement B) high spurious cross-correlation in the

sample may aﬀect the idiosyncratic components if they are serially dependent, reducing the accuracy

of the procedure proposed by Fan et al. (2020).

Figure 1: Results from a numerical toy example. The orange circles and bars represent means and standard deviations of
maxi(cid:54)=j |(cid:98)cx
and bars represent means and standard deviations of (cid:98)ψx

ij | for various values of the autocorrelation φ, as obtained from 5000 Monte Carlo replications. Similarly, blue triangles

min.

3 Distribution of the Sample Correlation between two Orthogonal

AR(1) Gaussian Processes

In this section we present our main theoretical contribution concerning the impact of serial dependence

on the non-asymptotic estimation error bound of PRs. We show formally that the probability of

incurring in spurious correlation increases with serial dependence.

Consider a ﬁrst order bivariate autoregressive process xt = φφφxt−1 + ut, t = 1, . . . , T , where φφφ is

a 2 × 2 diagonal matrix with main diagonal elements φ1, φ2 < 1. We make the following assumption

about the bivariate vector of autoregressive residuals:

Assumption 1 ut ∼ N (0002, III 2).

Therefore xxxt ∼ N (0002, CCCx

0) with (CCCx

0)ii = 1
1−φ2
i

, and (CCCx

0)12 = cx

12 = 0. In this setting we focus on the

density of the sample correlation coeﬃcient deﬁned as

(cid:98)cx
12 =

a12√
√
a11

a22

,

9

(5)

where ai,j = (cid:80)T
approach of Anderson (2003) to our time series context. In particular, when cu

t=1 xitxjt, since xi = 0, i, j = 1, 2. We generalize the
12 = 0, b = a21/a11 and

t=1(xit − xi)(xjt − xj) = (cid:80)T

v = a22 − a2

21/a11, Anderson (2003, p. 119) shows that

√

a11 b
(cid:112)v/(T − 2)

√

=

T − 2

a12/
(cid:112)1 − a2

√

a11a22

12/(a11a22)

√

=

T − 2

12

(cid:98)cx
(cid:112)1 − ((cid:98)cx

12)2

.

(6)

Note that b is the least squares regression coeﬃcient of x2t on x1t, and v is the sum of the square
of residuals of such regression. Thus, to derive the ﬁnite sample density of (cid:98)cx
densities of b and v.

12 we need the sample

Remark 1 In contrast to asymptotic statements, our theoretical analysis is intended to derive distri-

butions and densities of estimators that hold for T < ∞. Hence we will not employ the usual concepts

of convergence in probability and in distribution; rather we will use a notion of approximation, whose

“precision” needs to be evaluated. The precision of our approximations will be extensively tested under

several ﬁnite T scenarios in both the simulation study provided in Section 4 and in the Supplement.

Sample Distribution of b. We start by deriving the sample distribution of b, the OLS regression

coeﬃcient for x2 on x1. This is an arbitrary choice, in fact regressing x1 on x2 would not change the

distribution we will eventually obtain for the sample correlation coeﬃcient.

Proposition 2 Under Assumption 1 the sample distribution of b is approximately

(cid:18)

N

0,

1φ2
(1 − φ2
(T − 1)(1 − φ2

2)(1 − φ2
1)
2)(1 − φ1φ2)2

(cid:19)

.

Proof: We ﬁrst focus on the distribution of the sample covariance between x1 and x2, which is

(cid:100)Cov(x1, x2) =

a12
(T − 1)

=

(cid:32)T −1
(cid:88)

l=1

φl
1 (cid:100)Cov(u1[−l], u2) +

T −1
(cid:88)

l=1

φl
2 (cid:100)Cov(u2[−l], u1) + (cid:100)Cov(u1, u2)

(1−φ1φ2)−1,

(cid:33)

where (cid:100)Cov(ui[−l], uj) = (cid:80)T
and uj = 1

t=l+1(uit−l−ui)(ujt−uj)/(T −l−1), for i (cid:54)= j = 1, 2, ui = 1

t=l+1 uit−l
t=l+1 ujt. Since u1 and u2 are standard Normal, we have (see Glen et al. 2004 and

T −l−1

(cid:80)T

(cid:80)T

T −l−1

Supplement C)

Moreover, the quantity

(cid:100)Cov(u1, u2) ≈ N




0 ,




 .

1

(T − 1)

η12 =

T −1
(cid:88)

l=1

φl
1 (cid:100)Cov(u1[−l], u2) +

T −1
(cid:88)

l=1

φl
2 (cid:100)Cov(u2[−l], u1),

10

is a linear combination of the sample covariances between the residual of a time series at time t and
(cid:17)

the lagged residuals of the other time series. Note that η12 is a linear combination of N

0,

(cid:16)

,

φ2l
i
T −l−1

i = 1, 2. However, because |φi| < 1, we can approximate η12 as a linear combination of centered

Normals with variance

1
T −1 , so that

(cid:18)

η12 ≈ N

0,

1 + φ2
φ2
(T − 1)(1 − φ2

2 − 2φ2

1φ2
2
1)(1 − φ2
2)

(cid:19)

and

1φ2
1 − φ2
2
1)(1 − φ2
2)(1 − φ1φ2)2
Since a11 is T − 1 times the sample variance of x1, a11 ≈ T −1
1−φ2
1

(cid:100)Cov(x1, x2) ≈ N

(T − 1)(1 − φ2

0 ,

(cid:18)

(cid:19)

.

. Therefore, b = a21
a11

is Normally

distributed and, based on the approximation of mean and variance of a ratio (see Stuart and Ord

1998), we have E[b] = 0 and

V ar[b] = (T − 1)2 V ar[a12]

(cid:104)

(cid:100)Cov(x1, x2)

(cid:19)2

(cid:105) (cid:18) 1 − φ2
1
T − 1

E[a11]2 ≈ (T − 1)2V ar
(1 − φ2
1φ2
(T − 1)(1 − φ2

=

2)(1 − φ2
1)
2)(1 − φ1φ2)2

.

(cid:4)

Remark 2 For T < ∞ the quantity η12 has a variance that increases with the degree of serial depen-

dence. This quantity strongly aﬀects the impact of the degree of serial dependence on the variance of

a12 and, as a consequence, on the variance of both (cid:100)Cov(x1, x2) and b.

Proposition 2 shows that the OLS estimate b is normally distributed with a variance that strongly

depends on the degrees of serial dependence. In this context, it is common to adjust the standard

error of the OLS to achieve consistency in the presence of heteroskedasticity and/or serial dependence;

this leads, for instance, to the Heteroskedasticity and Autocorrelation Consistent (HAC) estimator

of Newey and West (1987) (NW). However, it has been recently shown that NW estimates can be

highly sub-optimal (or ineﬃcient) in the presence of strong serial dependence (Baillie et al., 2022). In

Supplement D we provide a simulation study to corroborate the result in Proposition 2.

It is important to note that if x1 and x2 are generated by independent MA(q) processes then serial

dependence increases the variance of b as the order q increases; see Granger et al. (2001). This is

due to the fact that any MA(∞) can be represented as an AR(1). Thus, increasing q, we are faced

with the same spurious component η12 that impacts the sample covariance between orthogonal AR(1)

processes.

11

Sample Distribution of v. To obtain the sample distribution of v we adapt Theorem 3.3.1 in

Anderson (2003, p. 75) to the case of AR(1) processes.

Proposition 3 Under Assumption 1 the sample distribution of v is approximately

(cid:18) T − 2
2

,

Γ

(cid:19)

.

2
1 − φ2
2

Proof: Consider a (T − 1) × (T − 1) orthogonal matrix M with ﬁrst row x(cid:48)
(cid:80)T −1

t=1 dthx2h, t = 1, . . . , T , h = 1, . . . , T . We have

1/

√

a11 and let st =

b =

(cid:80)T −1

t=1 x1tx2t
(cid:80)T −1
t=1 x2
1t

=

(cid:80)T −1
t=1 d1tx2t
√
a11

=

s1√
a11

.

Then, from Lemma 3.3.1 in Anderson (2003, p. 76), we have

v =

T −1
(cid:88)

t=1

2t − b2
x2

T −1
(cid:88)

t=1

x2
1t =

T −1
(cid:88)

t=1

t − s2
s2

1 =

T −1
(cid:88)

t=2

s2
t

.

Thus, v approximates the sum of T − 2 Normal variables with variance 1/(1 − φ2

2). Now, let zt be the

variable obtained by standardizing x2t. We have

v =

T −1
(cid:88)

t=2

s2
t ≈

T −1
(cid:88)

t=2

z2
t
1 − φ2
2

.

The right side of (7) is a Gamma distribution with shape parameter T −2

2 and rate parameter

(7)

2
1−φ2
2

. (cid:4)

Sample Density of (cid:98)cx
equation (6) we can now derive the density associated to the sample distribution of (cid:98)cx
12.

12. Note that b and v are independent. Using Propositions 2 and 3 and

Theorem 1 Let xt be a stationary bivariate Gaussian AR(1) process with autoregressive residuals

distributed according to N (0002, III 2). Further, let φ12 = φ1φ2 where φi, i = 1, 2, are the autoregressive
coeﬃcients. Then, the sample density of (cid:98)cx
(cid:1) (1 − φ12)
(cid:1) √
π

12 is approximated by

2 (cid:0)1 − φ2
12

(1 − ((cid:98)cx

1 − φ2

12)2)

(cid:19) T −1

(cid:1) T −2

(8)

(cid:98)cx

=

D

T −4

(cid:18)

Γ (cid:0) T −1
2
Γ (cid:0) T −2
2

12

.

2

2

1
12 + 2((cid:98)cx
12)2φ12(φ12 − 1)
1−φ2
1φ2
2
2)(1−φ1φ2)2

. Let δ2 =

(cid:17)

(1−φ2

1−φ2
1φ2
2)(1−φ1φ2)2 ,
2

(1−φ2

√

a11b is approximately N

(cid:16)

0,

. In the reminder of the proof, we consider the distributions of b and v in

Proof: Because of Proposition 2,

θ2 = 1

1−φ2
2

and t =

√
√

a11 b
v/(T −2)

Propositions 2 and 3 as exact, not approximate. Thus, we have the densities

√

g (

a11b) =

e− a11b2

2δ2

,

1
√
2π

δ

h(v) =

1
2 Γ (cid:0) T −2
T −2
2

v

(cid:1)

(2θ2)

12

T −2

2 −1e− v

2θ2 .

(9)

(10)

We focus on

f (t) =

=

=

=

(cid:18)(cid:114) v

T − 2

(cid:19)

t

h(v)dv

(cid:90) (cid:114) v

T − 2
(cid:114) v

(cid:90) ∞

g

0

T − 2

δ

1
√
2π

e

1

− vt2

(T −2)2δ2

v

T −2

2 −1e− v

2θ2
2 Γ (cid:0) T −2
2

T −2

dv

(cid:1)

(2θ2)
(cid:90) ∞

(cid:112)2π(T − 2)δ(2θ2)

T −2

2 Γ (cid:0) T −2
2

(cid:1)

1

0
(cid:90) ∞

(cid:112)2π(T − 2)δ(2θ2)

T −2

2 Γ (cid:0) T −2
2

(cid:1)

0

1
2 v

T −2

2 −1e

v

− vt2

(T −2)2δ2 e− v

2θ2 dv

T −3

2 e

v

−

(cid:16) 1
θ2

t2
(T −2)δ2

(cid:17) v

2 dv .

Now deﬁne Υ =

√

1

2π(T −2)δ(2θ2)

T −2

2 Γ( T −2
2 )

and x =

(cid:16) 1
θ2 + t2

(T −2)δ2

(cid:17) v

2 . Then

f (t) = Υ

(cid:90) ∞

(cid:32)

2x

0

(cid:18) 1

θ2 +

t2
(T − 2)δ2

(cid:19)−1(cid:33) T −3

2

e−xdx

= Υ 2

T −1
2

(cid:18) 1

θ2 +

t2
(T − 2)δ2

(cid:19)− T −1

2 (cid:90) ∞

0

T −1

2 −1e−xdx .

x

The integral on the right hand side can be represented by using the gamma function

Thus we obtain

Γ(α) =

(cid:90) ∞

0

xα−1e−xdx .

f (t) = Υ 2

T −1
2

Γ (cid:0) T −1
2

(cid:18) (T − 2)δ2 + t2θ2
θ2(T − 2)δ2
(cid:1) 2
(cid:112)2π(T − 2)δ(2θ2)
Γ (cid:0) T −1
2
(cid:112)π(T − 2)δΓ (cid:0) T −2
2

T −1
2

(cid:1) θ

T −2

(cid:1)

=

=

(cid:19)− T −1

2

Γ

(cid:19)

(cid:18) T − 1
2
(cid:18) (T − 2)δ2 + t2θ2
θ2(T − 2)δ2

(cid:1)

2 Γ (cid:0) T −2
2
(cid:18) (T − 2)δ2 + t2θ2
(T − 2)δ2

(cid:19)− T −1

2

.

(cid:19)− T −1

2

Substituting δ2 with

, we obtain the density

1−φ2

1φ2
2

Γ (cid:0) T −1
2

(1−φ2

1
1−φ2
2

2)(1−φ1φ2)2 and θ2 with
(cid:1) (1 − φ1φ2)(cid:112)(1 − φ2
2)
2)(1 − φ2
2)
(cid:18)

(cid:1) (cid:112)π(T − 2)(1 − φ2
(cid:1) (1 − φ1φ2)
Γ (cid:0) T −1
2
(cid:1) (cid:112)π(T − 2)(1 − φ2

1φ2

1 +

1φ2
2)

Γ (cid:0) T −2
2

Γ (cid:0) T −2
2

f (t) =

=

(cid:18)

1 +

t2(1 − φ1φ2)2(1 − φ2
2)
2)(1 − φ2
2)

(T − 2)(1 − φ2

(cid:19)− T −1

2

1φ2
(cid:19)− T −1

2

.

t2(1 − φ1φ2)2
1φ2
(T − 2)(1 − φ2
2)

The density of w = (cid:98)cx

12(1 − ((cid:98)cx

2 is thus

12)2)− 1
Γ (cid:0) T −1
2
Γ (cid:0) T −2
2

f (w) =

(cid:1) (1 − φ1φ2)
(cid:1) (cid:112)π(1 − φ2
1φ2
2)

(cid:18)

1 +

w2(1 − φ1φ2)2

(1 − φ2

1φ2
2)

(cid:19)− T −1

2

.

13

Next, deﬁne κ((cid:98)cx
Θ = (cid:0)Γ (cid:0) T −1
2

(cid:1) (1 − φ12)(cid:1) /

12) = w = (cid:98)cx

12(1 − ((cid:98)cx
Γ (cid:0) T −2
2

12)2)− 1
(cid:1) (cid:112)π(1 − φ2

12)

2 , from which κ(cid:48)((cid:98)cx

12) = (1 − ((cid:98)cx
. We can use these quantities to write

12)2)− 3

(cid:17)

(cid:16)

2 , φ12 = φ1φ2 and

= fw(κ((cid:98)cx

12))κ(cid:48)((cid:98)cx

12) = Θ

(cid:18)

1 +

D

(cid:98)cx

12

12)2)− 1

2

(cid:19)− T −1

2

(cid:17)2 (1 − φ12)2
(1 − φ2
12)

(1 − ((cid:98)cx

12)2)− 3

2

(cid:16)
12(1 − ((cid:98)cx
(cid:98)cx
(cid:19)− T −1

2

(1 − ((cid:98)cx

12)2)− 3

2

(cid:18)

= Θ

1 +

(cid:18) (1 − ((cid:98)cx

((cid:98)cx
(1 − ((cid:98)cx

12)2(1 − φ12)2
12)2)(1 − φ2

12)2)(1 − φ2
(1 − ((cid:98)cx

12)
12) + ((cid:98)cx
12)2)(1 − φ2
12)
12)2φ12(φ12 − 1)

(cid:18) 1 − φ2

12 + 2((cid:98)cx
(1 − ((cid:98)cx

12)2)(1 − φ2

12)

= Θ

= Θ

= Θ(1 − ((cid:98)cx

12)2)

(cid:18)

T −4
2

12)2(1 − φ12)2

(cid:19)− T −1

2

(1 − ((cid:98)cx

12)2)− 3

2

(cid:19)− T −1

2

(1 − ((cid:98)cx

12)2)− 3

2

(1 − φ2

12)

(cid:19) T −1

2

.

Thus, the (ﬁnite) sample density of (cid:98)cx

1 − φ2

12)2φ12(φ12 − 1)

12 + 2((cid:98)cx
12, taking the densities in (9) and (10) as exact, is

D

(cid:98)cx

12

=

Γ (cid:0) T −1
2
Γ (cid:0) T −2
2

(cid:1) (1 − φ12)
(cid:1) √
π

(1 − ((cid:98)cx

12)2)

T −4

2 (cid:0)1 − φ2
12

(cid:18)

(cid:1) T −2

2

1
12 + 2((cid:98)cx
12)2φ12(φ12 − 1)

1 − φ2

(cid:19) T −1

2

.

(cid:4)

Remark 3 D

(cid:98)cx

12

is the density of the sample correlation coeﬃcient (5) based on a ﬁnite T , with

serial dependence expressed by φ1 and φ2, and under the assumption of orthogonal Gaussian AR(1)

processes.

Remark 4 From (8) we see that φ12 determines the density of (cid:98)cx
More precisely, when Sign(φ1) = Sign(φ2), the probability in the tails increases as |φ12| grows. On

12 trough both its magnitude and sign.

the other hand, when Sign(φ1) (cid:54)= Sign(φ2), an increase in |φ12| leads to a density more concentrated

around the origin. This peculiarity on the eﬀect of Sign(φ12) will be numerically explored and validated

in Section 4.

Theorem 1 shows that, in a ﬁnite T context, the probability of observing sizeable spurious cross-

correlation between orthogonal Gaussian Autoregressive processes heavily depends on the degree of

serial dependence. This has important consequences on the non-asymptotic performance of PRs for
the reasons that we pointed out at the beginning of Section 2.2, related to the role of Pr {|(cid:98)cx
(see inequality (4), Deﬁnition 1 and Proposition 1). The implication of Theorem 1 for such probability

12| ≥ τ }

can be summarized in the following remark.

14

Remark 5 Because of Theorem 1

Pr {|(cid:98)cx

12| ≥ τ } ≈

(cid:90) −τ

−1

D

(cid:98)cx

12

d(cid:98)cx

12 +

(cid:90) 1

τ

D

(cid:98)cx

12

d(cid:98)cx

12

(11)

depends on the degrees of serial dependence of the processes.

4 Monte Carlo Experiments

12 described in the previous Section. Next, we expand the theoretical results in more generic

Here we ﬁrst conduct Monte Carlo experiments to assess numerically the approximation of the density
of (cid:98)cx
contexts, relaxing the assumption that the covariates are orthogonal Gaussian AR(1) processes. To
simplify matters, henceforth we indicate the density of (cid:98)cx

12 obtained by simulations as ds((cid:98)cx

12).

4.1 Numerical Approximation of ds((cid:98)cx

12) to D

(cid:98)cx

12

We generate data from the bivariate process xt = Dφxt−1 + ut for t = 1, . . . , T , where Dφ is a 2 × 2

diagonal matrix with same autocorrelation coeﬃcient φ in both position along the diagonal, and

ut ∼ N (0002, III 2) We consider T = 50, 100, 250 and φ = 0.3, 0.6, 0.9, 0.95 – thus, the parameter φ12 in

, here equal to φ2, takes values 0.09, 0.36, 0.81, 0.90. The left panels of Figures 2-4 show, for

12

D
(cid:98)cx
various values of T and φ12, the density ds((cid:98)cx
right panels of Figures 2-4 (b) show the corresponding D

12) generated through 5000 Monte Carlo replications. The

. These were plotted using 5000 values of

(cid:98)cx

12

the argument starting at -1 and increasing by steps of size 0.0004 until 1. Unsurprisingly, we observe
that the approximation of ds((cid:98)cx
in Figure 2, where T = 50, D
(cid:98)cx

increases as T grows and/or φ12 decreases. In particular,

12) for a low-to-intermediate degree of serial

approximates well ds((cid:98)cx

12) to D

(cid:98)cx

12

12

dependence (φ12 ≤ 0.36, i.e. φ ≤ 0.6). In contrast, in cases with high degree of serial dependence
has larger tails compared to ds((cid:98)cx
probability of large spurious correlations. However, it is noteworthy that the diﬀerence between the

12); that is, it over-estimates the

(φ12 ≥ 0.81, i.e. φ ≥ 0.9), D

(cid:98)cx

12

two densities is negligible for T ≥ 100 (Figures 3 and 4), also with high degree of serial dependence

(φ12 = 0.90, i.e. φ = 0.95). These numerical experiments corroborate the fact that the sample cross-

correlation between orthogonal Gaussian AR(1) processes is aﬀected by the degree of serial dependence

in a way that is well approximated by D
Pr {|(cid:98)cx

12| ≥ τ }, τ > 0, increases with φ12 in a similar way for ds((cid:98)cx

(cid:98)cx

12

12) and D

.

(cid:98)cx

12

. In fact, for a suﬃciently large ﬁnite T , we observe that

15

(a) ds((cid:98)cx

12)

(b) D

(cid:98)cx
12

Figure 2: Densities of (cid:98)cx

12 (a), and corresponding D

(cid:98)cx
12

(b), for T =50 and various values of φ12.

(a) ds((cid:98)cx

12)

(b) D

(cid:98)cx
12

Figure 3: Densities of (cid:98)cx

12 (a), and corresponding D

(cid:98)cx
12

(b), for T =100 and various values of φ12.

(a) ds((cid:98)cx

12)

(b) D

(cid:98)cx
12

Figure 4: Densities of (cid:98)cx

12 (a), and corresponding D

(cid:98)cx
12

(b), for T =250 and various values of φ12.

16

The Impact of Sign(φ12)

In Remark 4 we pointed out that the impact of φ12 on D
when −1 < φ12 < 0 an increment on |φ12| makes the density of (cid:98)cx
order to validate this result numerically, we run simulations with T = 100 and diﬀerent values for the

12 more concentrated around 0. In

depends on Sign(φ12). In particular,

(cid:98)cx

12

second element of the diagonal of Dφ; namely, −0.3, −0.6, −0.9, −0.95. Results are shown in Figure 5.
Indeed, we see that when Sign(φ1) (cid:54)= Sign(φ2) and |φ12| increases, ds((cid:98)cx
around 0 in a way that is, again, well approximated by D

12) increases its concentration

.

(cid:98)cx

12

(a) ds((cid:98)cx

12)

(b) D

(cid:98)cx
12

Figure 5: Densities of (cid:98)cx

12 (a), and corresponding D

(cid:98)cx
12

(b), for T =100 and various (negative) values of φ12.

4.2 General Case

To generalize our ﬁndings to the case of non-Gaussian weakly correlated ARMA and AR processes,

we generate covariates according to the following DGPs:

x1t = (φ + 0.1)x1t−1 + (φ + 0.1)x1t−2 − 0.2x1t−3 + u1t

x2t = φx2t−1 + φx2t−2 + u2t + 0.8u2t−1

(12)

(13)

where t = 1, . . . , 100 and φ = 0.15, 0.3, 0.45, 0.475. Moreover, we generate u1t and u2t from a bivariate

Laplace distribution with means 0, variances 1, and cu
not know an approximate theoretical density for (cid:98)cu
show the eﬀect of serial dependence on Pr {|(cid:98)cx
Monte Carlo replications for the diﬀerent values of φ. In short, also in the more general cases where

12. Therefore, we rely entirely on simulations to

12 = 0.2. In these more general cases, we do

12| ≥ τ }. Figure 6 shows ds((cid:98)cx

12) obtained from 5000

covariates are non Gaussian, weakly correlated AR(3) and ARMA(2,1) processes, the probability of

getting large sample cross-correlations depends on the degree of serial dependence. More simulation

17

results are provided in Supplement E.

Figure 6: ds((cid:98)cx

12) in the case of non Gaussian weakly correlated AR(3) and ARMA(2,1) processes, for various values of φ.

5 A Remedy for Serial Dependence-Induced Spurious Correlation

In this section we propose a solution to the issues caused by serial dependence-induced spurious

correlations for the performance of PRs. Our proposal consists of a two-step procedure. In the ﬁrst

step, we estimate a univariate model on each covariate time series (for example, an ARMA model);

in the second step, we run PRs using the residuals of the models ﬁtted at the ﬁrst step instead of the

original covariates. More precisely, let xit (the i-th time series at time t) be generated by the model

xit =

pi
(cid:88)

l=1

φilxit−l +

qi
(cid:88)

k=1

θikuit−k + uit

(14)

where i = 1, . . . , n, t = 1 . . . , T . This describes an ARMA(pi,qi) process where pi is the order of

autocorrelation, which determines the order of the weighted moving average over past values of the

covariate, and qi is the order of the weighted moving average over past errors. Note that the AR

(i.e. pi ≥ 1, qi = 0) and MA (i.e. pi = 0, qi ≥ 1) models are special cases of (14). For notational
simplicity let xit|t−1 = (cid:80)pi
to run PRs using the estimated residuals (cid:98)uit = xit − (cid:98)xit|t−1.

k=1 uit−k and let (cid:98)xit|t−1 be an estimate of xit|t−1. We propose

l=1 φilxit−l + (cid:80)qi

18

5.1 The Working Model on ARMA residuals

Assume that response variable and predictors are generated by the following DGPs:

yt = α1x1t−1 + α2x2t−1 + εt

(15)

xit = φixit−1 + uit

εt = φεεt−1 + ωt ,

where i = 1, 2, t = 1, . . . , T , |φi| < 1 |φε| < 1, and uit and ωt are the i.i.d. random errors of the

processes. The following two assumptions are crucial for our proposal:

Assumption 2 uit ⊥ ujt−l for any i, j, t and l (cid:54)= 0;

Assumption 3 uit−l ⊥ ωt for any i, t and l.

In this context we can apply a standard OLS estimator to estimate α1 and α2. However, we remark

that the problems caused by serial dependence for the non-asymptotic estimation error bound of the

PRs also apply to the OLS estimator (see Corollary 6 in Negahban et al. 2009). For the sake of the

argument, temporarily assume that the uit−1’s are observable, so that we do not need to estimate

them through the (xit−1 − (cid:98)xit−1|t−2)’s. If we could observe the errors, our proposal would consist of
estimating the following working model:

yt = α1u1t−1 + α2u2t−1 + φyyt−1 + ωt .

(16)

We refer to OLS applied to this as u-OLS, and we illustrate it in Examples 1– 4.

Example 1 (Equal degrees of serial dependence). Suppose φ1 = φ2 = φε = φ. Then, model (15) can

be rewritten as

yt = α1(φx1t−2 + u1t−1) + α2(φx2t−2 + u2t−1) + φεt−1 + ωit

= α1u1t−1 + α2u2t−1 + φyt−1 + ωt .

Thus, in an “ideal regime” in terms of degree of serial dependence (also known as “common factor

restriction”), the working model (16) is equivalent to the true model (15) because of the decomposition

of the AR(1) processes x1t−1, x2t−1 and εt.

19

Example 2 (Diﬀerent degrees of serial dependence). Suppose φ1 (cid:54)= φ2 (cid:54)= φε. Then, with some simple

steps, model (15) can be rewritten as

yt = α1(φ1x1t−2 + u1t−1) + α2(φ2x2t−2 + u2t−1) + φεεt−1 + ωit

= α1u1t−1 + α2u2t−1 + α1φ1x1t−2 + α2φ2x2t−2 + φεεt−1 + ωt .

Thus, in this perhaps more realistic regime, the working model (16) is not equivalent to the true model

(15) since the predictors and the error do not have the same degree of serial dependence, and therefore

the use of yt−1 does not allow us to summarize the serial dependence of yt.

Example 3 (Equal degrees of serial dependence and diﬀerent models for the predictors). Consider

x1t and x2t generated as

x1t = φx1t−1 + φx1t−2 + u1t

x2t = φx2t−1 + θu2t−1 + u2t

where 2|φ| < 1. Model (15) can be rewritten as

yt = α1(φx1t−2 + φx1t−3 + u1t−1) + α2(φx2t−2 + θu2t−2 + u2t−1) + φεεt−1 + ωit

= α1u1t−1 + α2u2t−1 + φyt−1 + φα1x1t−3 + α2θu2t−2 + ωt .

Thus, if we have an “ideal regime” in terms of degree of serial dependence, but diﬀerent models for

the predictors, the working model (16) is not equivalent to the true model (15). Here, the diﬀerence

between true and working model is due to the diﬀerences between the mechanisms generating x1t|t−1

and x2t|t−1. Again, this makes yt−1 not suitable for summarizing the serial dependence of yt.

Example 4 (Equals degrees of serial dependence and diﬀerent model for the error). Consider now

the case where

with 2|φ| < 1. Model (15) can be rewritten as

εt = φεt−1 + φεt−2 + ωt

yt = α1(φx1t−2 + u1t−1) + α2(φx2t−2 + u2t−1) + φεt−1 + φεt−2 + ωit

= α1u1t−1 + α2u2t−1 + φyt−1 + φεt−2 + ωt .

Thus, if we have an “ideal regime” in terms of degree of serial dependence, but a diﬀerent model

for the error, the working model (16) is not equivalent to the true model (15). Here, the diﬀerence

between true and working model is due to the diﬀerences between the mechanism generating εt|t−1 and

the mechanism generating the predictors. In this case, the residual of the working model would have

an autoregressive component.

20

It is crucial to note that in Examples 1-4 the working model allows us to estimate the true α1

and α2 through u1t−1 and u2t−1, regardless of the possible issues in estimating the serial dependence

of yt. This is possible because uit ⊥ xit|t−1 for any speciﬁcation of xit|t−1, which is a consequence of

Assumptions 2 and 3. For this reason, even if we omit the autoregressive and/or the moving average

component(s) from the working model, this does not lead to an omitted-variables bias of the estimated

α’s. However, we get a reduction in the explained variance of y, which is mitigated by including the

lags of yt among the regressors of the working model.

Thus, even when true and working models do not match, as in Examples 2–4, u-OLS moves

us from estimating coeﬃcients in a context characterized by high spurious cross-correlation, to one

characterized by very weak (or absent) spurious cross-correlations. Of course the parameters we

estimate about the past of yt change, but we can still formulate an eﬀective forecasting strategy.
Speciﬁcally, when εt = (cid:80)pε

j=1 φεjεt−j + ωt, our conjecture is that (even in cases such as Examples 2–4),
the variability introduced by a misspeciﬁcation of the serial dependence of yt through the estimation

of the working model is less than that introduced by estimating the model directly on the x’s.

Regarding the error term, since it is not correlated with the regressors included in the model, serial

dependence does not violate the assumption of exogeneity and the OLS estimator remains unbiased

and consistent. However, one strategy to prevent autocorrelation in standard errors is to increase

the number of lags of yt considered in the working model, as to come as close as possible to a “white

noise” residual. Therefore, considering lagged dependent variables can help us cope with the existence

of autocorrelation in the model.

Of course, in practice, the uit−1’s are not observable and need to be replaced by estimated residuals

of ARMA, AR or MA processes. When ﬁtting the working models with such residuals we refer to our

proposal as (cid:98)u-OLS or, in the case of PRs, as (cid:98)u-PRs. In the following Sections we show its potential.
First, we present simulation experiments where we compare (cid:98)u-OLS with methods customarily applied
to correct for serial dependence. Then, we demonstrate the estimation and forecasting performance

of (cid:98)u-LASSO (LASSO applied on ARMA residuals) through both simulations and an empirical appli-
cation. We note here that an in-depth study of the advantages in forecasting and variable selection

oﬀered by sparse (cid:98)u-PRs will be the topic of future studies.

21

5.2

(cid:98)u-OLS: Coeﬃcients Estimation and Prediction Accuracy in Low Dimension

We simulate the data using (15); that is

yt = α1x1t−1 + α2x2t−1 + εt

xit = φixit−1 + uit

εt = φεεt−1 + ωt

for i = 1, 2, t = 1, . . . , T . Here we take the i.i.d. errors uit and ωt to be standard Normal random

variables for which Assumptions 2 and 3 hold, and we consider three diﬀerent scenarios:

1. Equal degrees of serial dependence, with φ1 = φ2 = φε = 0.7 (as in Example 1). This is the ideal

regime in terms of degree of serial dependence, where the working model estimated through

(cid:98)u-OLS is equivalent to the true model. Moreover, in this common factor restriction regime, the
true model is those estimated through the CO and DynReg methods; see below (McGuirk and

Spanos, 2002; Baillie et al., 2022).

2. Diﬀerent degrees of serial dependence, with φ1 = 0.75, φ2 = 0.6, and φε = 0.9 (as in Example

2). Here the common factor restriction does not hold.

3. Diﬀerent models for predictors and error, with

x1t = 0.6x1t−1 + u1t + 0.5u1t−1

x2t = 0.75x2t−1 + u2t

εt = 0.6εt−1 + 0.3εt−2 + ωt .

Here x1t, x2t and εt are ARMA(1,1), AR(1) and AR(2) processes, respectively.

We compare coeﬃcients estimation and forecasting performance of the following methods:

• NW: the Heteroskedasticity and Autocorrelation Consistent (HAC) Newey-West estimator (Newey

and West, 1987), which accommodates autocorrelation and heteroskedasticity of the error terms

in model (15). The forecasting equation, in terms of the projection of yt on the hyperplane
spanned by the covariates, is y(x)

t = Proj(yt|yt−1, x1t−1, x2t−1).

• CO: the Cochrane-Orcutt generalized least squares (GLS) estimator (Cochrane and Orcutt,

1949), which adjusts a linear model for serial correlation in the error terms iterating two steps,

22

one to estimate the ﬁrst order autocorrelation on OLS residuals, and one to transform the vari-

ables to eliminate serial dependence in the errors, until a certain criterion is satisﬁed (e.g., the

estimated autocorrelation has converged); transformations are applied from the second obser-
vation onward, i.e. for t = 2, . . . , T . The forecasting equation is y∗(x)

= Proj(y∗

t |x∗

1t−1, x∗

2t−1),

t

where y∗

t = yt − (cid:98)φ∗

εyt−1, x∗

it−1 = xit−1 − (cid:98)φ∗

εxit−2, and (cid:98)φ∗

ε is the CO estimate of φε.

• DynReg: the dynamic regression method (Baillie et al., 2022), which includes lags of the vari-

ables as predictors; if the error is AR(p) with p known, one adds to the model p lagged values of yt
and xit−1, i = 1, . . . , n. The forecasting equation is y(x)
in Scenarios 1 and 2; and y(x)

t = Proj(yt|yt−1, yt−2, x1t−1, x1t−2,x1t−3, x2t−1, x2t−2, x2t−3) in Sce-

t = Proj(yt|yt−1, x1t−1, x1t−2, x2t−1, x2t−2)

nario 3.

•

(cid:98)u-OLS: our proposal, which applies OLS using as predictors (cid:98)uit−1 = xit − (cid:98)xit|t−1, i = 1, . . . , n.
The forecasting equation is y((cid:98)u)

t = Proj(yt|yt−1, (cid:98)u1t−1, (cid:98)u2t−1).

Table 1 reports, for each method, the average and standard deviation of the coeﬃcient estimation error
||(cid:98)ααα − ααα||2 and of the coeﬃcient of determination (R2) over 1000 Monte Carlo replications, considering
T = 100 (panel (a)) and T = 1000 (panel (b)) (more simulation results are provided in Supplement

F). Unsurprisingly, NW has the largest coeﬃcient estimation error (it retains OLS estimates and only

adjusts standard errors). CO, DynReg and (cid:98)u-OLS have smaller and similar coeﬃcient estimation
errors. However, in terms of R2, CO is outperformed by DynReg and (cid:98)u-OLS, which both include
yt−1 as predictor in their forecasting equation. We note that, while DynReg and (cid:98)u-OLS have similar
estimation and prediction performance, DynReg requires the estimation of more parameters. In fact,

to express yt through n covariates and an AR(p) error, DynReg estimates p + n + n × p parameters.

In contrast, (cid:98)u-OLS always estimates p + n parameters (where p refers to the number of lags of yt).
This fact highlights the advantage of using our proposal when n is comparable to or larger than T ,

and we turn to PRs. In Supplement G we also provide an analysis of the t-statistics associated with

these methods in the case of spurious regression between uncorrelated autoregressive processes.

23

DGP Metric

Stat.

(a)

(b)

NW CO DynReg

(cid:98)u-OLS NW CO DynReg

(cid:98)u-OLS

1

2

3

||(cid:98)ααα − ααα||2

ave.

0.317

0.128

0.129

0.129

0.341

0.040

0.040

s.d.

0.132

0.069

0.069

0.068

0.046

0.020

0.020

R2

ave.

0.747

0.682

0.824

0.817

0.747

0.668

0.829

s.d.

0.066

0.056

0.045

0.046

0.021

0.017

0.014

||(cid:98)ααα − ααα||2

ave.

0.351

0.124

0.132

0.134

0.379

0.037

0.039

s.d.

0.227

0.066

0.069

0.071

0.238

0.020

0.020

R2

ave.

0.761

0.704

0.836

0.814

0.768

0.695

0.845

s.d.

0.066

0.057

0.059

0.065

0.022

0.019

0.046

||(cid:98)ααα − ααα||2

ave.

0.474

0.126

0.134

0.148

0.579

0.038

0.040

s.d.

0.184

0.066

0.070

0.078

0.072

0.020

0.021

R2

ave.

0.789

0.701

0.888

0.846

0.791

0.684

0.900

s.d.

0.072

0.054

0.039

0.051

0.034

0.016

0.017

0.040

0.020

0.829

0.014

0.040

0.021

0.828

0.049

0.044

0.024

0.868

0.020

Table 1: Coeﬃcient estimation error and coeﬃcient of determination (R2) of Newey-West-style HAC estimator (NW), Cochrane-

Orcutt GLS estimator (CO), Dynamic Regression (DynReg) and OLS applied on (cid:98)u’s ((cid:98)u-OLS) across the three simulation scenarios
(DGPs). Panel (a) T = 100, panel (b) T = 1000. Results are obtained on 1000 Monte Carlo replications.

5.3

(cid:98)u-LASSO

5.3.1 Coeﬃcient Estimation Error Bound

Here, we present Monte Carlo experiments to assess the eﬀectiveness of (cid:98)u-LASSO in reducing the
coeﬃcient estimation error. We generate the response as

yt =

n
(cid:88)

i=1

αixit−1 + εt,

where εt = φεt−1 + ωt, and ωt ∼ N (0, σ2

ω). The coeﬃcient vector ααα = (α1, . . . , αn)(cid:48) is sparse with
||ααα||0 = 10. The active covariates are the ﬁrst 10, followed by n − 10 inactive ones, and α1 = · · · =

α10 = 1. We generate the n covariates as xit = φxit−1 + uit, i = 1, . . . , n, t = 1, . . . , 100, where
uit ∼ N (0, 1) and cu
ij = 0.3|i−j|. We consider n = 50, 150 and φ = 0.3, 0.6, 0.9, 0.95. Panels (a) and
(b) of Figure 7 display mean and standard deviation of the ratio between (cid:98)ψ(cid:98)u
min obtained

min and (cid:98)ψx

from 1000 Monte Carlo simulations run with n = 50 and 150, respectively; for n = 150 we consider the

minimum eigenvalue of the correlation matrix restricted to the 10 relevant variables. As expected, the

24

correlation matrix of the (cid:98)u’s does not suﬀer from spurious correlation induced by serial dependence,
and this leads to an increment of (cid:98)ψ (cid:98)u
as φ increases. To observe how this result translates into
(cid:98)ψx

min

min

coeﬃcients estimation accuracy, we compare the coeﬃcient estimation error of LASSO (||(cid:98)αααx − ααα||2)
(cid:98)u − ααα||2), where the tuning parameter λ is selected by BIC. Figure 8 shows
with that of (cid:98)u-LASSO (||(cid:98)ααα
how the mean and standard deviation of ||(cid:98)ααα
vary as a function of φ. Also here, as expected, the

(cid:98)u−ααα||2
||(cid:98)αααx−ααα||2

application of LASSO on serially uncorrelated data reduces the coeﬃcient estimation error, with a

gain in estimation accuracy that increases with φ. To summarize, results shown in Figures 7 and 8

corroborate the theoretical analysis according to which an increase in the degree of serial dependence

leads to an increase in the probability of large spurious correlations, which in turn increases the

probability of a small minimum eigenvalue for the sample correlation matrix. This negatively aﬀects

the estimation accuracy of PRs (see Proposition 1).

(a) n

T = 0.5

(b) n

T = 1.5

Figure 7: (a) Average and (b) standard deviation of (cid:98)ψ (cid:98)u
(cid:98)ψx
dependence (φ). The blue horizontal line marks a ratio value of 1.

min

min

across 1000 Monte Carlo replications, for several degrees of serial

(a) n

T = 0.5

(b) n

T = 1.5

Figure 8: (a) Average and (b) standard deviation of ||(cid:98)ααα

(cid:98)u−ααα||2
||(cid:98)αααx−ααα||2

across 1000 Monte Carlo replications, for several degree of serial

dependence (φ). The blue horizontal line marks a ratio value of 1.

25

5.3.2 Empirical Application

We consider Euro Area data obtained from Proietti and Giovannelli (2021), composed by 309 monthly

macroeconomic time series spanning the period between January 1997 and December 2018. The series

were all transformed to achieve stationarity by taking ﬁrst or second diﬀerences, logarithms or ﬁrst

or second diﬀerences of logarithms (full detail on the transformations applied to each series is given

in Supplement H). However, no treatment for outliers was applied.

The target variable is the Consumer Price Index (CPI), which is transformed as I(2), i.e. integrated

of order 2, following Stock and Watson (2002b):

yt+h = (1200/h)log(CP It+h/CP It) − 1200log(CP It/CP It−1) ,

where yt = 1200log(CP It/CP It−1) − 1200log(CP It−1/CP It−2), and h is the forecasting horizon. We

compute forecasts of yt+h at horizon h = 24 using a rolling ω-year window [t − ω, t + 1]; the models

are re-estimated at each t, adding one observation on the right of the window and removing one

observation on the left.

In particular, we ﬁx ω = 130, using data from Feb:1997 to Dec:2007 for

the ﬁrst estimation. The last available date is Dec:2018. The methods employed for our empirical

exercises are:

• Univariate AR(p): the autoregressive forecasting model based on p lagged values of the target

variable, i.e. (cid:98)yt+h = (cid:98)α0 + (cid:80)p

i=1 (cid:98)φiyt−i+1.

• LASSO: the classical LASSO (Tibshirani, 1996). Forecasts are obtained from the equation

t+h = (cid:98)α0+(cid:98)ααα(cid:48)
(cid:98)yx
estimated by the LASSO on the original time series.

xxt+(cid:80)p

i=1 (cid:98)φiyt−i+1, where (cid:98)αααx is the sparse vector of penalized regression coeﬃcients

•

(cid:98)u-LASSO: our proposal, where LASSO is applied to the residuals of the estimated serial-
dependent processes. Forecasts are obtained from the equation (cid:98)y (cid:98)u
i=1 (cid:98)φiyt−i+1,
where (cid:98)ααα
(cid:98)u is the sparse vector of penalized regression coeﬃcients estimated by the LASSO on
the estimated residuals.

(cid:98)u(cid:98)ut+(cid:80)p

t+h = (cid:98)α (cid:98)u

0 +(cid:98)ααα(cid:48)

For the AR(p) benchmark, coeﬃcients are estimated using the the R package lm and the lag order p is

selected by BIC within 0 ≤ p ≤ 12. For the (cid:98)u-LASSO, estimated residuals are obtained ﬁltering each
time series with an ARMA(pi, qi) using the R package auto.arima, and pi and qi are selected by BIC

within 0 ≤ pi ≤ 12, 0 ≤ qi ≤ 12, i = 1, . . . , n. The shrinkage parameter λ of LASSO and (cid:98)u-LASSO is
selected with BIC by using the R package HDeconometrics.

26

Forecasting accuracy for all three methods is evaluated using the root mean square forecast error

(RMSFE), deﬁned as

RM SF E =

(cid:118)
(cid:117)
(cid:117)
(cid:116)

1
T1 − T0

T1(cid:88)

(cid:16)

τ =T0

(cid:17)2

(cid:98)yτ − yτ

where T0 and T1 are the ﬁrst and last point in time used for the out of sample evaluation. For LASSO

and (cid:98)u − LASSO we also consider the number of selected variables.

Table 2 reports ratios of RMSFE’s between pairs of methods, as well as signiﬁcance of the corre-

sponding Diebold-Mariano test (Diebold and Mariano, 1995) for the alternative hypothesis that the

second method is less accurate. There are two relevant ﬁndings. First, (cid:98)u-LASSO produces signiﬁ-
cantly better forecasts than both the classical LASSO and the AR(p). Second, (cid:98)u-LASSO provides
a parsimonious model with respect to the LASSO. In fact, the ratio between the average number of

selected variable with (cid:98)u-LASSO over the average number of selected variable with LASSO is 0.133,
and the ratio between the standard deviations is 0.289 (note that average and standard deviation of

the selected variables by LASSO are 61 and 20, respectively). This is, in principle, consistent with

the theoretical analysis we provided earlier. The sparser (cid:98)u-LASSO output may be due to fewer false
positives, as compared to the LASSO – the latter suﬀers from the eﬀects of spurious correlations

induced by serial dependence. However, since in this real data application we do not know the true

DGP, any comments regarding accuracy in variable selection is necessarily speculative.

Method 1 Method 2 RMSFE (ratio)

(cid:98)u-LASSO
(cid:98)u-LASSO
LASSO

LASSO

AR(p)

AR(p)

0.82**

0.88**

1.08

Table 2: Ratio of the RMSFE of the employed methods. We report the p-value of the Diebold-Mariano test for the alternative

hypothesis that the second method is less accurate in forecasting. In particular, the p-value are indicated as follows: 0 ’***’ 0.001

’**’ 0.01 ’*’ 0.05 ’•’ 0.1”.

6 Concluding Remarks

In this paper we demonstrated that the probability of spurious cross-correlations between stationary

orthogonal or weakly cross-correlated processes depends not only on the sample size, but also on the

degree of serial dependence. Through this result, we pointed out that serial dependence negatively

27

aﬀects the behavior of the sample cross-correlation matrix, leading to a large probability of getting a

small minimum eigenvalue. Considering the role of the minimum eigenvalue on the non-asymptotic

estimation error bounds of PRs, our ﬁndings highlight the limitations of these methods in a time

series context. In order to tackle such limitations, we proposed a two-step procedure based on the

application of PRs on the residuals obtained by ﬁltering each time series with an ARMA process.

We showed that in a low dimensional scenario, where the degree of serial dependence is the same

for covariates and error, our procedure estimates a working model equivalent to the real one. We

also showed that beyond such simple scenario, our procedure still provides a valid estimation and

forecasting strategy. We assessed the performance of our proposal through Monte Carlo simulations

and an empirical analysis of Euro Area macroeconomic time series. Through simulations we observed

that (cid:98)u-LASSO, i.e. the LASSO applied on ARMA residuals, reduces the probability of large spurious
cross-correlation, performing better than classical LASSO in coeﬃcients estimation. Through the

empirical analysis we observed that (cid:98)u-LASSO improves the forecasting performance of LASSO, and
produces more parsimonious models. These ﬁndings encourage us to further investigate the potential

of (cid:98)u-PRs – and especially sparse (cid:98)u-PRs.

28

References

Anderson, T. W. (2003). An Introduction to Multivariate Statistical Analysis (3rd ed.). New York.

Baillie, R. T., F. X. Diebold, G. Kapetanios, and K. H. Kim (2022). On robust inference in time

series regression.

Bartlett, M. S. (1935). Some aspects of the time-correlation problem in regard to tests of signiﬁcance.

98 (3), 536–543.

Bickel, P. J., Y. Ritov, and A. B. Tsybakov (2009, Aug). Simultaneous analysis of lasso and dantzig

selector. The Annals of Statistics 37 (4), 1705–1732.

Box, G. and P. Newbold (1971). Some comments on a paper of coen, gomme and kendall. Journal of

the Royal Statistical Society, Series A (General) 134 (2), 229–240.

Cochrane, D. and G. H. Orcutt (1949). Application of least squares regression to relationships contain-

ing auto- correlated error terms. Journal of the American Statistical Association 44 (245), 32–61.

De Mol, C., D. Giannone, and L. Reichlin (2008). Forecasting using a large number of predictors: Is

bayesian shrinkage a valid alternative to principal components? Journal of Econometrics 146 (2),

318–328.

Diebold, F. X. and R. S. Mariano (1995, July). Comparing Predictive Accuracy. Journal of Business

& Economic Statistics 13 (3), 253–263.

Fan, J., F. Han, and H. Liu (2013). Challenges of big data analysis. National Science Review 1.

Fan, J., Y. Ke, and K. Wang (2020). Factor-adjusted regularized model selection. Journal of Econo-

metrics 216 (1), 71 – 85. Annals Issue in honor of George Tiao: Statistical Learning for Dependent

Data.

Fan, J., Q.-M. Shao, and W.-X. Zhou (2018, Jun). Are discoveries spurious? distributions of maximum

spurious correlations and their applications. The Annals of Statistics 46 (3).

Fan, J. and W.-X. Zhou (2016). Guarding against spurious discoveries in high dimensions. Journal

of Machine Learning Research 17 (203), 1–34.

29

Forni, M., A. Giovannelli, M. Lippi, and S. Soccorsi (2018). Dynamic factor model with inﬁnite-

dimensional factor space: Forecasting. Journal of Applied Econometrics 33 (5), 625–642.

Forni, M., M. Hallin, M. Lippi, and L. Reichlin (2000). The generalized dynamic-factor model:

Identiﬁcation and estimation. The Review of Economics and Statistics 82 (4), 540–554.

Forni, M., M. Hallin, M. Lippi, and L. Reichlin (2003). The generalized dynamic factor model.

one-sided estimation and forecasting.

Forni, M., M. Hallin, M. Lippi, and P. Zaﬀaroni (2016). Dynamic Factor Models with Inﬁnite-

Dimensional Factor Space. Asymptotic Analysis. Technical report.

Giannone, D., M. Lenza, and G. E. Primiceri (2018, August). Economic Predictions with Big Data:

The Illusion Of Sparsity. CEPR Discussion Papers 12256, C.E.P.R. Discussion Papers.

Glen, A. G., L. M. Leemis, and J. H. Drew (2004). Computing the distribution of the product of two

continuous random variables. Computational Statistics & Data Analysis 44 (3), 451–464.

Granger, C., N. Hyung, and Y. Jeon (2001). Spurious regressions with stationary series. Applied

Economics 33 (7), 899–904.

Granger, C. and P. Newbold (1974). Spurious regressions in econometrics. Journal of Economet-

rics 2 (2), 111 – 120.

Hastie, T., R. Tibshirani, and M. Wainwright (2015). Statistical learning with sparsity : the lasso

and generalizations. pp. 291 – 294.

Hoerl, A. and W. Kennard (1970). Ridge regression: Biased estimation for nonorthogonal problems.

pp. 55–67.

Lounici, K., M. Pontil, A. B. Tsybakov, and S. van de Geer (2009). Taking advantage of sparsity in

multi-task learning.

McGregor, J. R. (1962, 12). The approximate distribution of the correlation between two stationary

linear Markov series. Biometrika 49 (3-4), 379–388.

McGuirk, A. M. and A. Spanos (2002). The Linear Regression Model With Autocorrelated Errors:

Just Say No To Error Autocorrelation. Technical report.

30

Medeiros, M. C. and E. F. Mendes (2012). Estimating high-dimensional time series models. Texto

para discuss

textasciitildeao 602, Rio de Janeiro.

Negahban, S., B. Yu, M. J. Wainwright, and P. Ravikumar (2009). A uniﬁed framework for high-

dimensional analysis of m-estimators with decomposable regularizers. 22.

Negahban, S. N., P. Ravikumar, M. J. Wainwright, and B. Yu (2012, Nov). A uniﬁed framework for

high-dimensional analysis of m-estimators with decomposable regularizers. Statistical Science 27 (4).

Newey, W. K. and K. D. West (1987). A simple, positive semi-deﬁnite, heteroskedasticity and auto-

correlation consistent covariance matrix. Econometrica 55 (3), 703–708.

Proietti, T. and A. Giovannelli (2021). Nowcasting monthly gdp with big data: A model averaging

approach. Journal of the Royal Statistical Society Series A 184 (2), 683–706.

Smeekes, S. and E. Wijler (2018). Macroeconomic Forecasting Using Penalized Regression Methods.

Stock, J. H. and M. W. Watson (2002a). Forecasting using principal components from a large number

of predictors. Journal of the American Statistical Association 97 (460), 1167–1179.

Stock, J. H. and M. W. Watson (2002b). Macroeconomic forecasting using diﬀusion indexes. Journal

of Business & Economic Statistics 20 (2), 147–162.

Stock, J. H. and M. W. Watson (2008).

Introduction to econometrics / James H. Stock, Mark

W. Watson. (Brief ed. ed.). The Addison-Wesley series in economics. Boston, Mass. ; London:

Pearson/Addison-Wesley.

Stuart, A. and K. Ord (1998). Kendall’s advanced theory of statistics (Sixth ed.), Volume 1, Classical

Inference and Relationship.

Tibshirani, R. (1996). Regression shrinkage and selection via the lasso. Journal of the Royal Statistical

Society Series B 58, 267–288.

Xin, X., H. Jianhua, and L. Liangyuan (2017). On the oracle property of a generalized adaptive

elastic-net for multivariate linear regression with a diverging number of parameters. Journal of

Multivariate AnalysisVolume 162, 16 – 31.

31

Zhao, P. and B. Yu (2006, December). On model selection consistency of lasso. J. Mach. Learn.

Res. 7, 2541–2563.

Zou, H. (2006). The adaptive lasso and its oracle properties. Journal of the American Statistical

Association 101, 1418–1429.

Zou, H. and T. Hastie (2005). Regularization and variable selection via the elastic net. Journal of the

Royal Statistical Society, Series B 67, 301–320.

Zou, H. and H. H. Zhang (2009). On the adaptive elastic-net with a diverging number of parameters.

The Annals of Statistics 37 (4), 1733 – 1751.

32

Supplement

A Restricted Eigenvalue

In the speciﬁc case of n > T the loss function L(ααα) cannot be strongly convex since XX(cid:48)/T is not

positive deﬁnite.
In this speciﬁc case Bickel et al. (2009) proposed a solution based on a kind of
strong convexity for some subset C ⊂ Rn of possible perturbation vectors ∆ = |(cid:98)ααα − ααα| ∈ Rn, named
Restricted Eigenvalue Condition. In particular, for any subset S ⊆ {1, 2, . . . , n} with cardinality s,
let ∆S ∈ RS and ∆Sc ∈ RSc. The restricted eigenvalue condition requires that there exists a positive

number ν such that

min
∆∈Rn:∆(cid:54)=0

√

||X∆||2
T ||∆S||2

≥ ν.

Such condition is essentially a restriction on the eigenvalues of XX(cid:48)/T as a function of sparsity, which

allows for the strong convexity (Deﬁnition 1) to hold whit parameter γ = ν, which characterizes how

strong the covariates depend on each other. According to Bickel et al. (2009), the restricted eigenvalue

condition restricts the LASSO error to a set of the form:

C (S) :=

(cid:110)
(cid:98)∆ ∈ Rn : || (cid:98)∆Sc||1 ≤ 3|| (cid:98)∆S||1

(cid:111)

.

B On the Population Cross-Correlation in Time Series

We consider the case where the covariates in model (1) are generated as follows

xit = λiFt + uit,

(17)

with i = 1, . . . , n, t = 1, . . . , T . Ft represents a common factor that introduce population cross-

correlation between covariates, λi is the factor loading relative to xi, and uit is the idiosyncratic

component relative to xi at time t.

In this case, Fan et al. (2020) propose a method to reduce the cross-correlation between covariates

in order to improve the estimation performance of sparse PRs.

It consists in using the principal

component analysis to obtain (cid:98)λi, (cid:98)Ft and (cid:98)uit = xit − (cid:98)λi (cid:98)Ft, i.e. estimates of λi, Ft and ut. Then the
vector (cid:98)ααα is obtained by replacing the design matrix X in equation (2) with the T ×(1+n) design matrix
containing (cid:98)F, (cid:98)u1, . . . , (cid:98)un. Note that the penalty is applied on the estimated residuals only. Hence,
when covariates are generated by model (17), the procedure proposed by Fan et al. (2020) allows us

33

to deal with the problem from PRs estimation with highly cross-correlated covariates x1, . . . , xn to

PRs estimation with weakly or orthogonal covariates (cid:98)u1, . . . , (cid:98)un.

However, we want to stress that if the idiosyncratic components are orthogonal Gaussian AR(1)

processes with equal signs of the autoregressive coeﬃcients, then the density of their sample cross-

correlation coincides with that observed in Figure 2-4, and therefore the methodology proposed by

Fan et al. (2020) would not solve the problem of the high spurious cross-correlation caused by serial

dependence.

C Distribution of (cid:100)Cov(u1, u2)

In Figure 9 we report the density of (cid:100)Cov(u1, u2) when u1 and u2 are standard Normal in the cases of
(cid:16)

(cid:17)

T = 10 and 100. Red line shows the density of N

. Observations are obtained on 5000 Monte

0,

1
T −1

Carlo replications. We observe that the approximation of (cid:100)Cov(u1, u2) to N (0,

1
T −1 ) holds also when
T is small (see Figure 9 (a) relative to T =10). This analysis corroborate numerically the results in

Glen et al. (2004), which show that if x and y are N (0, 1), then the probability density function of
xy is K0(|xy|)

, where K0(|xy|) is the Bessel function of the second kind.

pi

(a) T = 10

(b) T = 100

Figure 9: Density of (cid:100)Cov(u1, u2) between two uncorrelated standard Normal variables for T = 10 (a) and T = 100 (b).

34

D Distribution of b

Consider two orthogonal Gaussian AR(1) processes generated according to the model xit = φixit−1 +

uit, where uit ∼ N (0, 1), i = 1, 2, t = 1, . . . , 100 and φ1 = φ2 = φ. In this simulation exercise we run

the model

x2t = βx1t + et,

(18)

where et ∼ N (0, σ2

e ), and study the distribution of the OLS estimator b of β in the following four

cases in terms of degrees of serial dependence: φ = 0.3, 0.6, 0.9, 0.95. Figure 10 reports the density of

b across the φ values obtained on 5000 Monte Carlo replications. We compare this density with that

of three zero-mean Normal variables where the variances are respectively:

• S2

1 =

(cid:98)σ2
(cid:98)e

(cid:80)T

t=1(x1t−x1)2 , where (cid:98)σ2

(cid:98)e is the sample variance of the estimated residual (cid:98)et = x2t − bx1t.

This is the OLS estimator for the variance of β.

(cid:80)T

(cid:80)T

1
T −2
[ 1
T

• S2

2 = 1
T

t

(cid:98)e2

t=1(x1t−x1)2
t=1(x1t−x1)2]2 (cid:98)ft, is the Newey-West (NW) HAC estimator (Newey and West, 1987;
is the correction factor that

(cid:16)

(cid:17)

(cid:17)

1 + 2 (cid:80)m−1
j=1

(cid:16) m−j
m

(cid:98)ρj

Stock and Watson, 2008), where (cid:98)ft =

adjusts for serially correlated errors and involves estimates of m−1 autocorrelation coeﬃcients (cid:98)ρj,
, with (cid:98)vt = (x1t − x1)(cid:98)et. A rule of thumb for choosing m is m = [0.75T 1/3].
and (cid:98)ρj =

(cid:80)T

t=j+1 (cid:98)vt(cid:98)vt−j
(cid:80)T
t=1 (cid:98)v2

t

• S2

3 =

(1−φ2
1φ2
(T −1)(1−φ2

2)(1−φ2
1)
2)(1−φ1φ2)2 , is the theoretical variance of b obtained in Proposition 2.

From Figure 10 we observe that the variance of b increases with the degree of serial dependence (φ)

in a way that is well approximated by the distribution derived in Proposition 2 (see dotted red line).

On the contrary, OLS (solid line) and NW (dashed blue line), are highly sub-optimal in the presence

of strong serial dependence, underestimating the variability of b as the serial dependence increases.

35

(a) φ = 0.3, T = 100

(b) φ = 0.6, T = 100

(c) φ = 0.9, T = 100

(d) φ = 0.95, T = 100

Figure 10: Density of b between uncorrelated AR(1) Gaussian processes. Black line indicates the approximated density obtained

by using the classical OLS estimator, dashed blue line indicates the approximated density obtained by using the NW estimator,

and, ﬁnally, dotted red line shows the theoretical approximated density obtained in Proposition 2.

E More General Cases

We study the density of (cid:98)cx
correlated processes; and ARMA processes with diﬀerent order. Note that for the ﬁrst two cases

12 in three diﬀerent cases: non-Gaussian processes; weakly and high cross-

the variables are AR(1) processes with T = 100 and autocorrelation coeﬃcient φ = 0.3, 0.6, 0.9, 0.95.

Since we do not have D
replications, i.e. ds((cid:98)cx

(cid:98)cx

12

12), to show the eﬀect of serial dependence on Pr {|(cid:98)cx

12| ≥ τ }.

for these cases, we rely on the densities obtained on 5000 Monte Carlo

36

The Impact of non-Gaussianity

The theoretical contribution reported in Section 3 requires the Gaussianity of u1 and u2. With the
following simulation experiments we show that the impact of φ12 on the density of (cid:98)cx
also when u1t and u2t are not Gaussian random variables. To this end, we generate u1t and u2t from

12 is relevant

the following distributions: Laplace with mean 0 and variance 1 (case (a)); Cauchy with location

parameter 0 and scale parameter 1 (case (b)); and from a mixed case where u1 is generated by a

t-student with 1 degree of freedom and u2 by a Uniform in the interval (-4,4) (case (C)). Figure 11

reports the results of the simulation experiment. All densities in Figure 11 gave very similar result,

with the exception of the Cauchy (Figure 11 (b)) where the eﬀect of φ12 declines. However, we can

state that regardless the distribution of the processes, whenever Sign(φ1) = Sign(φ2), the probability
of large values of (cid:98)cx

12 increases with φ12.

(a) Laplace

(b) Cauchy

(c) Mix t-Student and Uniform

Figure 11: Simulated density of (cid:98)cx

12 in various scenarios in terms of φ12 for not Gaussian processes.

The Impact of Population Cross-Correlation

37

Since orthogonality is an unrealistic assumption for most economic applications, here we admit popu-
lation cross-correlation. In Figure 12 we report ds((cid:98)cx
with cu

12) when the processes are weakly cross-correlated

12 = 0.2, and when the processes are multicollinear with cu

12 = 0.8 (usually we refer to multi-

collinearity when cu

12 ≥ 0.7). We observe that the impact of φ12 on ds((cid:98)cx

12) depends on the degree of

(population) cross-correlation ass follows. In the case of weakly correlated processes, an increase in

φ12 yields a high probability of observing large sample correlations in absolute value (the obtained

density is similar to that in Figure 3 (a) with the obvious diﬀerence that here it is not centered at

zero). In the case of multicollinear processes, on the other hand, an increase in φ12 leads to a high

probability of underestimating the true population cross-correlation.

(a) cu

12 = 0.2

(b) cu

12 = 0.8

Figure 12: ds((cid:98)cx

12) obtained through simulations in the case of cx

12 = 0.2 (a) and cx

12 = 0.8 (b).

Density of (cid:98)cx
To show the eﬀect of serial dependence on a more general case, we generate x1 and x2 through the

12 in the case of ARMA(pi, qi) processes

following ARMA processes

x1t = φx1t−1 + φx1t−2 − φx1t−3u1t + 0.5u1t−1,

x2t = φx2t−1 + φx2t−2 + u2t + 0.7u2t−1 − 0.4u3t−2,

where t = 1, . . . , 100 and ui ∼ N (0, 1).
T = 100 and φ = 0.1, 0.2, 0.3, 0.33. With no loss of generality we can observe that ds((cid:98)cx
as φ increases, that is Pr {|(cid:98)cx

In Figure 13 we report the density of (cid:98)cx

12| ≥ τ } increases with |φ|.

12 in the case of

12) gets larger

38

Figure 13: Densities of ds((cid:98)cx

12) between two uncorrelated ARMA Gaussian processes.

39

F Extended Results for (cid:98)u-OLS

Metric

Stat.

(a)

(b)

NW CO DynReg

(cid:98)u-OLS NW CO DynReg

(cid:98)u-OLS

|(cid:98)α1 − α1|

ave.

s.d.

0.241

0.104

0.105

0.104

0.244

0.031

0.031

0.244

0.124

0.124

0.123

0.145

0.038

0.038

p-val. ave.

0.000

0.000

0.000

0.000

0.000

0.000

0.000

p-val. s.d.

0.000

0.000

0.000

0.000

0.000

0.000

0.000

|(cid:98)α2 − α2|

ave.

s.d.

0.244

0.101

0.103

0.102

0.243

0.032

0.032

0.248

0.122

0.123

0.121

0.143

0.038

0.038

p-val. ave.

0.000

0.000

0.000

0.000

0.000

0.000

0.000

p-val. s.d.

0.000

0.000

0.000

0.000

0.000

0.000

0.000

(cid:98)φy

R2

ave.

s.d.

0.322

0.231

p-val. ave.

0.002

p-val. s.d.

0.013

–

–

–

–

0.658

0.320

0.000

0.000

0.685

0.344

0.280

0.134

0.000

0.000

0.000

0.000

–

–

–

–

0.697

0.176

0.000

0.000

ave.

s.d.

0.747

0.682

0.824

0.817

0.747

0.668

0.829

0.066

0.056

0.045

0.046

0.021

0.017

0.014

0.032

0.038

0.000

0.000

0.032

0.038

0.000

0.000

0.698

0.155

0.000

0.000

0.829

0.014

Table 3: Estimation and in sample prediction performance of the Newey-West-style HAC estimators (NW), Cochrane-Orcutt GLS

estimator (CO), Dynamic Regression (DynReg) and the ordinary least squares applied on the estimated AR residuals ((cid:98)u-OLS) in
the case of constant degree of serial correlation: φ1 = φ2 = φε = 0.7. Panel (a) T = 100, panel (b) T = 1000. Results are obtained

on 1000 Monte Carlo replications.

40

Metric

Stat.

(a)

(b)

NW CO DynReg

(cid:98)u-OLS NW CO DynReg

(cid:98)u-OLS

|(cid:98)α1 − α1|

ave.

s.d.

0.430

0.101

0.105

0.110

0.481

0.030

0.031

0.376

0.123

0.127

0.131

0.228

0.037

0.038

p-val. ave.

0.003

0.000

0.000

0.000

0.000

0.000

0.000

p-val. s.d.

0.026

0.000

0.000

0.000

0.000

0.000

0.000

|(cid:98)α2 − α2|

ave.

s.d.

0.357

0.097

0.104

0.105

0.383

0.029

0.031

0.338

0.116

0.124

0.124

0.205

0.035

0.037

p-val. ave.

0.002

0.000

0.000

0.000

0.000

0.000

0.000

p-val. s.d.

0.008

0.000

0.000

0.000

0.000

0.000

0.000

(cid:98)φy

R2

ave.

s.d.

0.573

0.355

p-val. ave.

0.000

p-val. s.d.

0.000

–

–

–

–

0.854

0.313

0.000

0.000

0.782

0.642

0.303

0.216

0.000

0.000

0.000

0.000

–

–

–

–

0.895

0.159

0.000

0.000

ave.

s.d.

0.749

0.694

0.874

0.858

0.759

0.688

0.888

0.072

0.053

0.040

0.044

0.022

0.017

0.011

0.033

0.040

0.000

0.000

0.032

0.038

0.000

0.000

0.807

0.169

0.000

0.000

0.875

0.013

Table 4: Estimation and in sample prediction performance of the Newey-West-style HAC estimators (NW), Cochrane-Orcutt GLS

estimator (CO), Dynamic Regression (DynReg) and the ordinary least squares applied on the estimated AR residuals ((cid:98)u-OLS) in
the case of diﬀerent degree of serial correlation: φ1 = 0.75, φ2 = 0.6, and φε = 0.9. Panel (a) T = 100, panel (b) T = 1000. Results

are obtained on 1000 Monte Carlo replications.

41

Metric

Stat.

(a)

(b)

NW CO DynReg

(cid:98)u-OLS NW CO DynReg

(cid:98)u-OLS

|(cid:98)α1 − α1|

ave.

s.d.

0.358

0.090

0.104

0.111

0.407

0.027

0.032

0.335

0.109

0.127

0.137

0.209

0.032

0.037

p-val. ave.

0.000

0.000

0.000

0.000

0.000

0.000

0.000

p-val. s.d.

0.003

0.000

0.000

0.000

0.000

0.000

0.000

|(cid:98)α2 − α2|

ave.

s.d.

0.342

0.110

0.111

0.121

0.375

0.033

0.032

0.342

0.129

0.130

0.143

0.210

0.040

0.038

p-val. ave.

0.002

0.000

0.000

0.000

0.000

0.000

0.000

p-val. s.d.

0.016

0.000

0.000

0.000

0.000

0.000

0.000

(cid:98)φy

R2

ave.

s.d.

0.403

0.312

p-val. ave.

0.004

p-val. s.d.

0.021

–

–

–

–

0.572

0.352

0.000

0.002

0.777

0.470

0.333

0.200

0.000

0.000

0.000

0.000

–

–

–

–

0.597

0.197

0.000

0.000

ave.

s.d.

0.762

0.698

0.874

0.831

0.765

0.683

0.887

0.067

0.052

0.038

0.050

0.022

0.016

0.012

0.034

0.042

0.000

0.000

0.036

0.043

0.000

0.000

0.798

0.192

0.000

0.000

0.853

0.015

Table 5: Estimation and in sample prediction performance of the Newey-West-style HAC estimators (NW), Cochrane-Orcutt GLS

estimator (CO), Dynamic Regression (DynReg) and the ordinary least squares applied on the estimated AR residuals ((cid:98)u-OLS) in
the case of diﬀerent DPs: x1t = 0.6x1t−1 + u1t + 0.5u1t−1, x2t = 0.75x2t−1 + u2t, εt = 0.6εt−1 + 0.3εt−2 + ωt. Panel (a) T = 100,

panel (b) T = 1000. Results are obtained on 1000 Monte Carlo replications.

G Detecting Spurious Regression

Here we generate data as in Supplement D and compare the t-statistics of the methods analyzed in

the Section 5.2 to evaluate their ability in avoiding spurious regressions. In Table 6 we report the

percentage of times that the t-statistics are greater than 1.96 in absolute value. Note that according

to statistical theory |tb| > 1.96 will occur approximately 5% of the time. The main results from this

analysis are: (i) OLS estimator (Table 6 panel (a)) suﬀers of spurious regressions for any φ > 0,

which occurs about %50 when φ = 0.9. (ii) NW estimator (Table 6 panel (b)) reduces the problem,

but for large value of φ spurious regression occurs frequently. Note that these two results are in line

with those in Granger et al. (2001). (iii) CO, DynReg and (cid:98)u-OLS (Table 6 panel (c)-(e)) solve the

42

problem of spurious regression due to serial dependence, by making the variance of b independent of

φ. However, (cid:98)u-OLS keeps the advantage already mentioned with respect to CO and DynReg, that are
a better prediction accuracy and the estimation of less parameters (see Section 5.2).

(a) |tols

b

| > 1.96

(b) |tnw

b

| > 1.96

(c) |tco

b | > 1.96

(d) |tdr

b | > 1.96

(e) |t(cid:98)u−ols
b

| > 1.96

T

50

100

250

1000

10000

50

100

250

1000

10000

50

100

250

1000

10000

50

100

250

1000

10000

50

100

250

1000

10000

φ = 0.0 φ = 0.3 φ = 0.6 φ = 0.9 φ = 0.95

5.96

5.38

6.06

4.94

5.12

7.32

6.96

6.96

5.00

5.32

6.58

5.78

6.16

5.00

5.22

5.88

5.36

5.86

4.86

5.22

6.08

5.36

6.02

4.94

5.12

8.00

7.44

7.20

7.28

7.08

9.36

7.58

6.08

5.24

4.68

6.76

5.60

4.76

5.06

5.14

5.94

5.16

4.62

5.02

5.10

6.48

5.40

4.66

5.00

5.16

17.16

18.00

18.26

17.72

19.00

16.48

12.48

9.34

7.36

6.08

7.16

5.92

5.42

4.74

4.80

6.16

5.30

5.14

4.84

4.82

5.58

5.34

5.10

4.78

4.86

47.58

50.54

51.16

51.82

53.62

41.82

36.36

29.00

18.88

9.48

8.24

5.86

4.52

5.02

4.86

6.12

5.04

4.56

4.88

4.86

6.08

4.84

4.52

4.96

4.84

56.12

60.36

64.90

66.48

65.76

50.58

47.72

43.62

31.72

17.00

8.42

6.28

5.04

5.20

5.56

5.52

5.52

4.86

5.12

5.58

5.06

5.26

4.70

5.16

5.54

Table 6: Percentage of t-statistics over 1.96 in absolute value obtained on 1000 Monte Carlo replications.

As a further analysis, the following Proposition shows that the variability of the limiting distribution

of tols

b depends only on the degree of serial dependence of the processes.

Proposition 4 Let S2

ols =

(cid:98)σ2
(cid:98)e

(cid:80)T

t=1(x1t−x1)2 , where (cid:98)σ2

(cid:98)e is the estimated variance of the residual of model

43

(18). Then

b
Sols

d−→ N

(cid:18)

0,

1φ2
1 − φ2
2
(1 − φ1φ2)2

(cid:19)

.

Proof: From Proposition 1 we know that b ≈ N

1−φ2
1
(T −1)(1−φ2

2) , we have

b
Sols

(cid:16)

d−→ N

0, 1−φ2

1φ2
2
(1−φ1φ2)2

(cid:17)

.

(cid:16)

0,

1φ2
(1−φ2
(T −1)(1−φ2

2)(1−φ2
1)
2)(1−φ1φ2)2

(cid:17)

. Then, considering S2

ols ≈
(cid:4)

Note that the result in Proposition 4 has been also derived in Granger et al. (2001). This result

show that the misspeciﬁcation of tols

b

is only due to the degree of serial dependence. To conﬁrm this,

look at the columns of Table 6 and consider that the value of |tols

b

| increases with the degree of serial

dependence φ, but stay quite constant regardless of the sample size T .

H List of Time Series in the Euro Area Data

We report the list of series for the Euro Area dataset adopted in the forecasting exercise. As for

the FRED data, the column tcode denotes the data transformation for a given series xt: (1) no
transformation; (2) ∆xt; (3)∆2xt; (4) log(xt); (5) ∆log(xt); (6) ∆2log(xt). (7) ∆(xt/xt− − 1.0).

The acronyms for the sectors refer to:

• ICS: Industry & Construction Survey

• CCI: Consumer Conﬁdence Indicators

• M&IR: Money & Interest Rates

• IP: Industrial Production

• HCPI: Harm. Consumer Price Index

• PPI: Producer Price Index

• TO: Turnover & Retail Sale

• HUR: Harm. Unemployment rate

• SI: Service Svy.

44

ID

Description

Area

Sector Tcode

Table 7: A sample long table.

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

Ind Svy: Employment Expectations

Ind Svy: Export Order-Book Levels

Ind Svy: Order-Book Levels

Ind Svy: Mfg - Selling Price Expectations

Ind Svy: Production Expectations

Ind Svy: Production Trend

Ind Svy: Mfg - Stocks Of Finished Products

Constr. Svy: Price Expectations

Ind Svy: Export Order Book Position

Ind Svy: Production Trends In Recent Mth.

Ind Svy: Selling Prc. Expect. Mth. Ahead

Ret. Svy: Employment

Ret. Svy: Orders Placed With Suppliers

Constr. Svy: Synthetic Bus. Indicator

Bus. Svy: Constr. Sector - Capacity Utilisation Rate

Constr. Svy: Activity Expectations

Constr. Svy: Price Expectations

Constr. Svy: Unable To Increase Capacity

Constr. Svy: Workforce Changes

Constr. Svy: Workforce Forecast Changes

Svy: Mfg Output - Order Book & Demand

Svy: Mfg Output - Order Book & Foreign Demand

Svy: Mfg Output - Personal Outlook

Svy: Auto Ind - Order Book & Demand

Svy: Auto Ind - Personal Outlook

Svy: Basic & Fab Met Pdt Ex Mach & Eq - Personal Outlook

Svy: Ele & Elec Eq, Mach Eq - Order Book & Demand

Svy: Ele & Elec Eq, Mach Eq - Order Book & Foreign Demand

Svy: Ele & Elec Eq, Mach Eq - Personal Outlook

Svy: Mfg Output - Price Outlook

Svy: Mfg Of Chemicals & Chemical Pdt - Order Book & Demand

Svy: Mfg Of Chemicals & Chemical Pdt - Personal Outlook

Svy: Mfg Of Food Pr & Beverages - Order Book & Demand

Svy: Mfg Of Food Pr & Beverages - Order Book & Foreign Demand

Svy: Mfg Of Trsp Eq - Finished Goods Inventories

EA

EA

EA

EA

EA

EA

EA

EA

EA

EA

EA

EA

EA

FR

FR

FR

FR

FR

FR

FR

FR

FR

FR

FR

FR

FR

FR

FR

FR

FR

FR

FR

FR

FR

FR

ICS

ICS

ICS

ICS

ICS

ICS

ICS

ICS

ICS

ICS

ICS

ICS

ICS

ICS

ICS

ICS

ICS

ICS

ICS

ICS

ICS

ICS

ICS

ICS

ICS

ICS

ICS

ICS

ICS

ICS

ICS

ICS

ICS

ICS

ICS

1

1

1

1

1

1

1

1

1

1

1

1

1

1

1

1

1

1

1

1

1

1

1

1

1

1

1

1

1

1

1

1

1

1

1

Continued on next page

45

Table 7 – continued from previous page

Description

Area

Sector Tcode

ID

36

37

38

39

40

41

42

43

44

45

46

47

48

49

50

51

52

53

54

55

Svy: Mfg Of Trsp Eq - Order Book & Demand

Svy: Mfg Of Trsp Eq - Order Book & Foreign Demand

Svy: Mfg Of Trsp Eq - Personal Outlook

Svy: Oth Mfg, Mach & Eq Rpr & Instal - Ord Book & Demand

FR

FR

FR

FR

Svy: Oth Mfg, Mach & Eq Rpr & Instal - Ord Book & Fgn Demand FR

Svy: Oth Mfg, Mach & Eq Rpr & Instal - Personal Outlook

Svy: Other Mfg - Order Book & Demand

Svy: Rubber, Plastic & Non Met Pdt - Order Book & Demand

Svy: Rubber, Plastic & Non Met Pdt - Order Book & Fgn Demand

Svy: Rubber, Plastic & Non Met Pdt - Personal Outlook

Svy: Total Ind - Order Book & Demand

Svy: Total Ind - Order Book & Foreign Demand

Svy: Total Ind - Personal Outlook

Svy: Total Ind - Price Outlook

Svy: Wood & Paper, Print & Media - Ord Book & Fgn Demand

Trd. & Ind: Bus Sit

Trd. & Ind: Bus Expect In 6Mo

Trd. & Ind: Bus Sit

Trd. & Ind: Bus Climate

Cnstr Ind: Bus Climate

ICS

ICS

ICS

ICS

ICS

ICS

ICS

ICS

ICS

ICS

ICS

ICS

ICS

ICS

ICS

ICS

ICS

ICS

ICS

ICS

ICS

ICS

ICS

ICS

ICS

ICS

ICS

ICS

ICS

ICS

ICS

ICS

ICS

ICS

ICS

ICS

1

1

1

1

1

1

1

1

1

1

1

1

1

1

1

1

1

1

1

1

1

1

1

1

1

1

1

1

1

1

1

1

1

1

1

1

FR

FR

FR

FR

FR

FR

FR

FR

FR

FR

DE

DE

DE

DE

DE

DE

DE

DE

DE

DE

DE

DE

DE

DE

DE

DE

DE

DE

DE

DE

DE

56 Mfg: Bus Climate

57 Mfg: Bus Climate

58 Mfg Cons Gds: Bus Climate

59 Mfg (Excl Fbt): Bus Climate

60 Whsle (Incl Mv): Bus Climate

61 Mfg: Bus Sit

62 Mfg: Bus Sit

63 Mfg (Excl Fbt): Bus Sit

64 Mfg (Excl Fbt): Bus Sit

65

66

Cnstr Ind: Bus Expect In 6Mo

Cnstr Ind: Bus Expect In 6Mo

67 Mfg: Bus Expect In 6Mo

68 Mfg: Bus Expect In 6Mo

69 Mfg Cons Gds: Bus Expect In 6Mo

70 Mfg (Excl Fbt): Bus Expect In 6Mo

71 Mfg (Excl Fbt): Bus Expect In 6Mo

Continued on next page

46

Table 7 – continued from previous page

Description

Area

Sector Tcode

ID

72

Rt (Incl Mv): Bus Expect In 6Mo

73 Whsle (Incl Mv): Bus Expect In 6Mo

74

Bus. Conf. Indicator

75 Order Book Level: Ind

76 Order Book Level: Foreign - Ind

77 Order Book Level: Investment Goods

78 Order Book Level: Int. Goods

79

80

81

82

83

84

85

86

Production Level - Ind

Cons. Conﬁdence Indicator

Cons. Svy: Economic Situation Last 12 Mth. - Emu 11/12

Cons. Svy: Possible Savings Opinion

Cons. Svy: Future Financial Situation

Svy - Households, Economic Situation Next 12M

Cons. Conﬁdence Indicator - DE

Cons. Conﬁdence Index

87 Gfk Cons. Climate Svy - Bus. Cycle Expectations

88

89

90

91

92

93

94

95

96

97

98

99

Cons.S Conﬁdence Index

Cons. Conﬁdence Climate (Balance)

Cons. Svy: Economic Climate Index (N.West It)

Cons. Svy: Economic Climate Index (Southern It)

Cons. Svy: General Economic Situation (Balance)

Cons. Svy: Prices In Next 12 Mths. - Lower

Cons. Svy: Unemployment Expectations (Balance)

Cons. Svy: Unemployment Expectations - Approx. Same

Cons. Svy: Unemployment Expectations - Large Increase

Cons. Svy: Unemployment Expectations - Small Increase

Cons. Svy: General Economic Situation (Balance)

Cons. Svy: Household Budget - Deposits To/Withdrawals

100 Cons. Svy: Household Economy (Cpy) - Much Worse

101 Cons. Svy: Italian Econ.In Next 12 Mths.- Much Worse

102 Cons. Svy: Major Purchase Intentions - Balance

103 Cons. Svy: Major Purchase Intentions - Much Less

104 Cons. Svy: Households Fin Situation - Balance

105

106

107

Indl. Prod. - Excluding Constr.

Indl. Prod. - Cap. Goods

Indl. Prod. - Cons. Non-Durables

DE

DE

IT

ES

ES

ES

ES

ES

EA

EA

FR

FR

FR

DE

DE

DE

DE

DE

IT

IT

IT

IT

IT

IT

IT

IT

IT

ES

FR

FR

FR

FR

FR

EA

EA

EA

ICS

ICS

ICS

ICS

ICS

ICS

ICS

ICS

CCI

CCI

CCI

CCI

CCI

CCI

CCI

CCI

CCI

CCI

CCI

CCI

CCI

CCI

CCI

CCI

CCI

CCI

CCI

CCI

CCI

CCI

CCI

CCI

CCI

IP

IP

IP

1

1

1

1

1

1

1

1

1

1

1

1

1

1

5

1

5

1

5

5

1

5

1

5

5

5

1

5

5

5

1

5

1

5

5

5

Continued on next page

47

Table 7 – continued from previous page

Description

Area

Sector Tcode

Indl. Prod. - Cons. Durables

Indl. Prod. - Cons. Goods

Indl. Prod.

Indl. Prod. - Mfg

Indl. Prod. - Mfg (2010=100)

Indl. Prod. - Manuf. Of Motor Vehicles, Trailers, Semitrailers

Indl. Prod. - Int. Goods

Indl. Prod. - Indl. Prod. - Constr.

Indl. Prod. - Manuf. Of Wood And Paper Products

Indl. Prod. - Manuf. Of Computer, Electronic And Optical Prod

Indl. Prod. - Manuf. Of Electrical Equipment

Indl. Prod. - Manuf. Of Machinery And Equipment

Indl. Prod. - Manuf. Of Transport Equipment

Indl. Prod. - Other Mfg

Indl. Prod. - Manuf. Of Chemicals And Chemical Products

Indl. Prod. - Manuf. Of Rubber And Plastics Products

Indl. Prod. - Investment Goods

Indl. Prod.

Indl. Prod.

Indl. Prod. - Cons. Goods - Durable

Indl. Prod. - Investment Goods

Indl. Prod. - Int. Goods

Indl. Prod. - Chemical Products & Synthetic Fibres

Indl. Prod. - Machines & Mechanical Apparatus

Indl. Prod. - Means Of Transport

Indl. Prod. - Metal & Metal Products

Indl. Prod. - Rubber Items & Plastic Materials

Indl. Prod. - Wood & Wood Products

Indl. Prod.

Indl. Prod. - Computer, Electronic And Optical Products

Indl. Prod. - Basic Pharmaceutical Products

Indl. Prod. - Constr.

Indl. Prod. - Ind Incl Cnstr

Indl. Prod. - Mfg

Indl. Prod. - Rebased To 1975=100

Indl. Prod. - Chems & Chem Prds

EA

EA

FR

FR

FR

FR

FR

FR

FR

FR

FR

FR

FR

FR

FR

FR

IT

IT

IT

IT

IT

IT

IT

IT

IT

IT

IT

IT

IT

IT

IT

DE

DE

DE

DE

DE

IP

IP

IP

IP

IP

IP

IP

IP

IP

IP

IP

IP

IP

IP

IP

IP

IP

IP

IP

IP

IP

IP

IP

IP

IP

IP

IP

IP

IP

IP

IP

IP

IP

IP

IP

IP

5

5

5

5

5

5

5

5

5

5

5

5

5

5

5

5

5

5

5

5

5

5

5

5

5

5

5

5

5

5

5

5

5

5

5

5

ID

108

109

110

111

112

113

114

115

116

117

118

119

120

121

122

123

124

125

126

127

128

129

130

131

132

133

134

135

136

137

138

139

140

141

142

143

Continued on next page

48

Table 7 – continued from previous page

Description

Area

Sector Tcode

Indl. Prod. - Ind Excl Cnstr

Indl. Prod. - Ind Excl Energy & Cnstr

Indl. Prod. - Mining & Quar

Indl. Prod. - Cmptr, Eleccl & Opt Prds, Elecl Eqp

Indl. Prod. - Interm Goods

Indl. Prod. - Cap. Goods

Indl. Prod. - Durable Cons Goods

Indl. Prod. - Tex & Wearing Apparel

Indl. Prod. - Pulp, Paper&Prds, Pubshg&Print

Indl. Prod. - Chem Prds

Indl. Prod. - Rub&Plast Prds

Indl. Prod. - Basic Mtls

Indl. Prod. - Cmptr, Eleccl & Opt Prds, Elecl Eqp

Indl. Prod. - Motor Vehicles, Trailers&Semi Trail

Indl. Prod. - Tex & Wearing Apparel

Indl. Prod. - Paper & Prds, Print, Reprod Of Recrd Media

Indl. Prod. - Chems & Chem Prds

Indl. Prod. - Basic Mtls, Fab Mtl Prds, Excl Mach&Eqp

Indl. Prod. - Repair & Install Of Mach & Eqp

Indl. Prod. - Mfg Excl Cnstr & Fbt

Indl. Prod. - Mining & Ind Excl Fbt

Indl. Prod. - Ind Excl Fbt

Indl. Prod. - Interm & Cap. Goods

Indl. Prod. - Fab Mtl Prds Excl Mach & Eqp

Indl. Prod.

Indl. Prod. - Cons. Goods

Indl. Prod. - Cap. Goods

Indl. Prod. - Int. Goods

Indl. Prod. - Energy

Indl. Prod. - Cons. Goods, Non-Durables

Indl. Prod. - Mining

Indl. Prod. - Mfg Ind

Indl. Prod. - Other Mining & Quarrying

Indl. Prod. - Textile

Indl. Prod. - Chemicals & Chemical Products

Indl. Prod. - Plastic & Rubber Products

DE

DE

DE

DE

DE

DE

DE

DE

DE

DE

DE

DE

DE

DE

DE

DE

DE

DE

DE

DE

DE

DE

DE

ES

ES

ES

ES

ES

ES

ES

ES

ES

ES

ES

ES

ES

IP

IP

IP

IP

IP

IP

IP

IP

IP

IP

IP

IP

IP

IP

IP

IP

IP

IP

IP

IP

IP

IP

IP

IP

IP

IP

IP

IP

IP

IP

IP

IP

IP

IP

IP

IP

5

5

5

5

5

5

5

5

5

5

5

5

5

5

5

5

5

5

5

5

5

5

5

5

5

5

5

5

5

5

5

5

5

5

5

5

ID

144

145

146

147

148

149

150

151

152

153

154

155

156

157

158

159

160

161

162

163

164

165

166

167

168

169

170

171

172

173

174

175

176

177

178

179

Continued on next page

49

Table 7 – continued from previous page

Description

Area

Sector Tcode

ID

180

181

182

183

184

Indl. Prod. - Other Non-Metal Mineral Products

Indl. Prod. - Metal Processing Ind

Indl. Prod. - Metal Products Excl. Machinery

Indl. Prod. - Electrical Equipment

Indl. Prod. - Automobile

185 Euro Interbank Oﬀered Rate - 3-Month (Mean)

186 Money Supply: Loans To Other Ea Residents Excl. Govt.

187 Money Supply: M3

188 Euro Short Term Repo Rate

189 Datastream Euro Share Price Index (Mth. Avg.)

190 Euribor: 3-Month (Mth. Avg.)

191 Mﬁ Loans To Resident Private Sector

192 Money Supply - M1

193 Money Supply - M3

194

Share Price Index - Sbf 250

195 Fibor - 3 Month (Mth.Avg.)

196 Money Supply - M3

197 Money Supply - M2

198 Bank Prime Lending Rate / Ecb Marginal Lending Facility

199 Dax Share Price Index, Ep

200

Interbank Deposit Rate-Average On 3-Months Deposits

201 Oﬃcial Reserve Assets

202 Money Supply: M3 - Spanish

203 Madrid S.E - General Index

204 Hicp - Overall Index

205 Hicp - All-Items Excluding Energy, Index

206 Hicp - Food Incl. Alcohol And Tobacco, Index

207 Hicp - Processed Food Incl. Alcohol And Tobacco, Index

208 Hicp - Unprocessed Food, Index

209 Hicp - Goods, Index

210 Hicp - Industrial Goods, Index

211 Hicp - Industrial Goods Excluding Energy, Index

212 Hicp - Services, Index

213 Hicp - All-Items Excluding Tobacco, Index

214 Hicp - All-Items Excluding Energy And Food, Index

215 Hicp - All-Items Excluding Energy And Unprocessed Food, Index

ES

ES

ES

ES

ES

EA

EA

EA

FR

FR

FR

FR

FR

FR

DE

DE

DE

DE

DE

IT

IT

ES

ES

ES

EA

EA

EA

EA

EA

EA

EA

EA

EA

EA

EA

EA

IP

IP

IP

IP

IP

M&IR

M&IR

M&IR

M&IR

M&IR

M&IR

M&IR

M&IR

M&IR

M&IR

M&IR

M&IR

M&IR

M&IR

M&IR

M&IR

M&IR

M&IR

M&IR

HCPI

HCPI

HCPI

HCPI

HCPI

HCPI

HCPI

HCPI

HCPI

HCPI

HCPI

HCPI

5

5

5

5

5

5

5

5

5

1

5

5

5

5

1

5

5

5

5

1

5

5

5

5

6

6

6

6

6

6

6

6

6

6

6

6

Continued on next page

50

ID

Description

Area

Sector Tcode

Table 7 – continued from previous page

216 All-Items Hicp

217 All-Items Hicp

218 All-Items Hicp

219 All-Items Hicp

220 Goods (Overall Index Excluding Services)

221 Goods (Overall Index Excluding Services)

222 Processed Food Including Alcohol And Tobacco

223 Processed Food Including Alcohol And Tobacco

224 Processed Food Including Alcohol And Tobacco

225 Processed Food Including Alcohol And Tobacco

226 Unprocessed Food

227 Unprocessed Food

228 Unprocessed Food

229 Unprocessed Food

230 Non-Energy Industrial Goods

231 Non-Energy Industrial Goods

232

233

Services (Overall Index Excluding Goods)

Services (Overall Index Excluding Goods)

234 Overall Index Excluding Tobacco

235 Overall Index Excluding Tobacco

236 Overall Index Excluding Energy

237 Overall Index Excluding Energy

238 Overall Index Excluding Energy And Unprocessed Food

239 Overall Index Excluding Energy And Unprocessed Food

240 Ppi: Ind Excluding Constr. & Energy

241 Ppi: Cap. Goods

242 Ppi: Non-Durable Cons. Goods

243 Ppi: Int. Goods

244 Ppi: Non Dom. - Mining, Mfg & Quarrying

245 Ppi: Non Dom. Mfg

246 Ppi: Int. Goods Excluding Energy

247 Ppi: Cap. Goods

248 Ppi: Cons. Goods

249 Ppi: Fuel

250 Ppi: Indl. Products (Excl. Energy)

251 Ppi: Machinery

DE

ES

FR

IT

DE

FR

DE

ES

FR

IT

DE

ES

FR

IT

DE

FR

DE

FR

DE

FR

DE

FR

DE

FR

EA

EA

EA

EA

EA

DE

DE

DE

DE

DE

DE

DE

HCPI

HCPI

HCPI

HCPI

HCPI

HCPI

HCPI

HCPI

HCPI

HCPI

HCPI

HCPI

HCPI

HCPI

HCPI

HCPI

HCPI

HCPI

HCPI

HCPI

HCPI

HCPI

HCPI

HCPI

PPI

PPI

PPI

PPI

PPI

PPI

PPI

PPI

PPI

PPI

PPI

PPI

6

6

6

6

6

6

6

6

6

6

6

6

6

6

6

6

6

6

6

6

6

6

6

6

6

6

6

6

6

6

6

6

6

6

6

6

Continued on next page

51

ID

Description

Area

Sector Tcode

Table 7 – continued from previous page

252 Deﬂated T/O: Ret. Sale In Non-Spcld Str With Food, Bev & Tob

253 Deﬂated T/O: Oth Ret. Sale In Non-Spcld Str

254 Deﬂated T/O: Sale Of Motor Vehicle Pts & Acces

255 Deﬂated T/O: Wholesale Of Agl Raw Matls & Live Animals

256 Deﬂated T/O: Wholesale Of Household Goods

257 T/O: Ret. Trd, Exc Of Mv , Motorcyles & Fuel

258 T/O: Ret. Sale Of Clth & Leath Gds In Spcld Str

259 T/O: Ret. Sale Of Non-Food Prds (Exc Fuel)

260 T/O: Ret. Sale Of Info, Househld & Rec Eqp In Spcld Str

261 Ek Unemployment: All

262 Ek Unemployment: Persons Over 25 Years Old

263 Ek Unemployment: Women Under 25 Years Old

264 Ek Unemployment: Women Over 25 Years Old

265 Ek Unemployment: Men Over 25 Years Old

266 Fr Hur All Persons (All Ages)

267 Fr Hur Femmes (Ages 15-24)

268 Fr Hur Femmes (All Ages)

269 Fr Hur Hommes (Ages 15-24)

270 Fr Hur Hommes (All Ages)

271 Fr Hur All Persons (Ages 15-24)

272 Fr Hurall Persons(Ages 25 And Over)

273 Fr Hur Females (Ages 25 And Over)

274 Fr Hur Males (Ages 25 And Over)

275 Bd Hur All Persons (All Ages)

276 Bd Hur Femmes (Ages 15-24)

277 Bd Hur Femmes (All Ages)

278 Bd Hur Hommes (Ages 15-24)

279 Bd Hur Hommes (All Ages)

280 Bd Hur All Persons (Ages 15-24)

281 Bd Hurall Persons(Ages 25 And Over)

282 Bd Hur Females (Ages 25 And Over)

283 Bd Hur Males (Ages 25 And Over)

284

285

286

287

It Hur All Persons (All Ages)

It Hur Femmes (All Ages)

It Hur Hommes (All Ages)

It Hur All Persons (Ages 15-24)

DE

DE

DE

DE

IT

ES

ES

ES

ES

EA

EA

EA

EA

EA

FR

FR

FR

FR

FR

FR

FR

FR

FR

DE

DE

DE

DE

DE

DE

DE

DE

DE

IT

IT

IT

IT

T/O

T/O

T/O

T/O

T/O

T/O

T/O

T/O

T/O

HUR

HUR

HUR

HUR

HUR

HUR

HUR

HUR

HUR

HUR

HUR

HUR

HUR

HUR

HUR

HUR

HUR

HUR

HUR

HUR

HUR

HUR

HUR

HUR

HUR

HUR

HUR

5

5

5

5

5

5

5

5

5

5

5

5

5

5

5

5

5

5

5

5

5

5

5

5

5

5

5

5

5

5

5

5

5

5

5

5

Continued on next page

52

ID

Description

Area

Sector Tcode

Table 7 – continued from previous page

288

It Hurall Persons(Ages 25 And Over)

289 Es Hur All Persons (All Ages)

290 Es Hur Femmes (Ages 16-24)

291 Es Hur Femmes (All Ages)

292 Es Hur Hommes (Ages 16-24)

293 Es Hur Hommes (All Ages)

294 Es Hur All Persons (Ages 16-24)

295 Es Hurall Persons(Ages 25 And Over)

296 Es Hur Females (Ages 25 And Over)

297 Es Hur Males (Ages 25 And Over)

298 De - Service Conﬁdence Indicator

299 De Services - Buss. Dev. Past 3 Months

300 De Services - Evol. Demand Past 3 Months

301 De Services - Exp. Demand Next 3 Months

302 De Services - Evol. Employ. Past 3 Months

303 Fr - Service Conﬁdence Indicator

304 Fr Services - Buss. Dev. Past 3 Months

305 Fr Services - Evol. Demand Past 3 Months

306 Fr Services - Exp. Demand Next 3 Months

307 Fr Services - Evol. Employ. Past 3 Months

308 Fr Services - Exp. Employ. Next 3 Months

309 Fr Services - Exp. Prices Next 3 Months

IT

ES

ES

ES

ES

ES

ES

ES

ES

ES

DE

DE

DE

DE

DE

FR

FR

FR

FR

FR

FR

FR

HUR

HUR

HUR

HUR

HUR

HUR

HUR

HUR

HUR

HUR

SI

SI

SI

SI

SI

SI

SI

SI

SI

SI

SI

SI

5

5

5

5

5

5

5

5

5

5

1

1

1

1

1

1

1

1

1

1

1

1

53

