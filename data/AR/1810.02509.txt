An Edge-Computing Based Architecture for Mobile
Augmented Reality

Jinke Ren, Yinghui He, Guan Huang, Guanding Yu, Yunlong Cai, Zhaoyang Zhang
Department of Information Science and Electronic Engineering, Zhejiang University, Hangzhou 310027, China

8
1
0
2

t
c
O
5
1

]
T
I
.
s
c
[

2
v
9
0
5
2
0
.
0
1
8
1
:
v
i
X
r
a

Abstract—In order to mitigate the long processing delay and
high energy consumption of mobile augmented reality (AR)
applications, mobile edge computing (MEC) has been recently
proposed and is envisioned as a promising means to deliver better
quality of experience (QoE) for AR consumers. In this article, we
ﬁrst present a comprehensive AR overview, including the indis-
pensable components of general AR applications, fashionable AR
devices, and several existing techniques for overcoming the thorny
latency and energy consumption problems. Then, we propose a
novel hierarchical computation architecture by inserting an edge
layer between the conventional user layer and cloud layer. Based
on the proposed architecture, we further develop an innovated
operation mechanism to improve the performance of mobile AR
applications. Three key technologies are also discussed to further
assist the proposed AR architecture. Simulation results are ﬁnally
provided to verify that our proposals can signiﬁcantly improve
the latency and energy performance as compared against existing
baseline schemes.

Index Terms—Augmented reality, mobile edge computing,

hierarchical computing, energy efﬁciency, latency.

I. INTRODUCTION

As a revolutionary innovation, mobile augmented reality
(AR) has become a miraculous technology in the past few
years. By combining the computer-generated and sensor-
extracted elements with the real objects and enabling real-time
3D interaction between mobile users and physical surround-
ings at ﬁngertips, mobile AR is envisioned as a new paradigm
to submerge mobile subscribers in a fabulous mixed-reality
world [1], [2]. In view of the great business opportunities
involved in mobile AR, many leading companies have continu-
ously scrambled to design and promote their own mobile AR
applications and products, such as Google Glass, Microsoft
HoloLens, and Recon Jet.

However, despite the fully immersive user experience,
mobile AR still faces with many technical challenges that
hinder its widespread commercial use. Due to the hardware
limitation, running sophisticated AR algorithms at mobile de-
vices generally causes long processing delay and high energy
consumption, which are the main hurdles for implementation.
To handle these issues, many cutting-edge technologies, such
as advanced hardware structure, acceptable approximate com-
puting, and partial video frame updating have been developed
to improve the mobile AR performance. Nevertheless, these
methods still cannot catch up with the rapid growth of user
demand while pointlessly increasing the cost of mobile de-
vices. Hence, novel techniques are urgently expected to tackle
the above challenges.

To overcome the computational resource shortage of mobile
devices, another major innovation in the past decade is mo-
bile cloud computing (MCC), which allows users to ofﬂoad
computation-intensive tasks to a number of powerful cloud
servers deployed at the remote cloud platform for processing
[3]. However, with the strict delay requirement of mobile
AR applications, MCC suffers from extra propagation delay
due to the long physical distance between mobile devices
and cloud servers [4]. To this end, an improved technique,
which is referred to as mobile edge computing (MEC), has
been recently proposed by the European Telecommunication
Standards Institute (ETSI) and is also considered by the
Third Generation Partnership Project (3GPP) in their future
standards. By distributing the conventional centralized cloud
computing resources to the edge of mobile networks, MEC of-
fers an adjacent computing environment for mobile subscribers
and provides a variety of beneﬁts, including ultra-low latency,
real-time access, and location-aware services, etc. [5]–[7].

On the other hand, by taking advantages of the proximity to
mobile users in MEC and the abundant computation capacity
in MCC, effective collaboration between cloud and edge com-
puting can further improve the system performance. Several
hierarchical edge-to-cloud architectures have been proposed to
collaborate the computation capacity of edge servers and cloud
servers [8], [9]. However, they are not speciﬁcally designed for
AR applications. Motivated by this, in this work, we propose
a novel hierarchical computation architecture to deal with
the long processing delay and high energy consumption of
mobile AR applications. The newly-designed architecture is
composed of three layers: the user layer, the edge layer, and the
cloud layer. By integrating the communication, computation,
and control modules in the edge layer while seamlessly
collaborating the edge and cloud computing capacities, several
critical components of AR applications can be intelligently
ofﬂoaded to both edge and cloud servers for further processing.
In accordance with this architecture, an advanced operation
mechanism is developed by embedding the MEC functions
into the AR process. Moreover, to support the proposed AR
framework, we further discuss three key technologies, i.e., the
joint communication and computation resource allocation, the
collaborative cloud and edge computing, and the content-based
image retrieval from the viewpoints of energy efﬁciency and
delay optimization. The main merit of the proposed MEC-
based AR framework is that it merges the advantages of cloud
computing and edge computing to enormously improve the
quality of experience (QoE) for AR applications.

 
 
 
 
 
 
In what follows, we ﬁrst provide a comprehensive AR
overview, including the indispensable components of general
AR applications, fashionable AR devices, and existing tech-
nologies for alleviating the inherent long processing delay and
high energy consumption problems. To improve the system
performance, our key contributions are the novel hierarchi-
cal computation architecture and the related AR operation
mechanism as well as the three key technologies, which can
signiﬁcantly facilitate the implementation of the MEC-based
AR framework. Finally, simulation results demonstrate that the
proposed AR framework can evidently improve the energy
and delay performance, as compared against some baseline
schemes. Our design in this article has the great potential to
provide an essential reference for the future development of
mobile AR technology.

II. OVERVIEW OF AUGMENTED REALITY

In this section, we present an overview of AR, including
the speciﬁc components of general AR applications, the com-
pelling mobile AR devices, and the incumbent AR technolo-
gies, from the viewpoints of energy efﬁciency and latency
optimization.

A. AR Application Components

To combine colorful computer-generated and sensor-
extracted data with the physical reality, different AR applica-
tions possess diverse modules for speciﬁc processing purposes.
However, ﬁve computation components, i.e., the video source,
tracker, mapper, object recognizer, and renderer, are indis-
pensable for almost all AR applications, and they play the
critical roles in general AR architectures [10], [11].

Resolution Image Format

Video Source

Renderer

Object
Recognizer

Tracker

Mapper

Number of Features

Fig. 1. Main components of general AR applications.

As illustrated in Fig. 1, the aforementioned ﬁve components
collaborate closely to accomplish an integrated AR process.
First, the video source fetches the raw videos from mobile
cameras and clips these videos into frames with speciﬁc image
format, such as JPEG and PNG. Next, the video frames are
delivered to the tracker to determine the user’s position with
respect to the physical surroundings. Given the tracking re-
sults, virtual coordinate of the environment can be established
by the mapper. Then, the internal objects in video frames
are identiﬁed by the object recognizer with robust features.

Afterwards, the augmented information of the identiﬁed object
can be accurately retrieved from local memory (or cloud
database) and properly mixed with the original videos by the
renderer. The results are ﬁnally displayed on the screens of
AR devices so that the subscribers are able to enjoy a magic
interactive experience with the physical reality at the ﬁngertips.

B. Fashionable AR Devices

A variety of AR devices with different operation modes
have been recently designed with multifarious functions. Table
I summarizes several well-known AR devices developed by
major companies. Among them, Google Glass, Microsoft
HoloLens, and Osterhout Design Group (ODG) R-9 operate
independently, whereas Meta 2 and Recon Jet need to rely
on external equipments such as personal computers (PCs) or
smart phones.

The Google Glass is the ﬁrst-appeared AR glass, which
has a stylish appearance and provides users with requested
information, such as calendar, weather, and message via
natural language voice commands. Comparatively, Microsoft
HoloLens enables users to engage with digital content and
interact with holograms of the surroundings for supplying
subscribers with a mixed-reality experience. This function is
extended in ODG R-9 by combining with some advanced
modules, such as Bluetooth 5.0, built-in GPS, and six degrees
of freedom tracking. However, these devices all suffer from
high energy consumption and cannot work for a long time.
On the other hand, by connecting powerful computers, Meta 2
allows users to intuitively touch, grab, and move the computer-
generated digital objects as real ones, which inevitably leads
to its non-portability for outdoor use. Particularly, Recon Jet is
extensively used in sport scenes and can exhibit the real-time
motion information of mobile users.

Although the above AR devices can provide fully immersive
experience for mobile subscribers, they generally suffer from
long processing delay and high energy consumption, which
are the most-critical performance metrics for AR applications.
To overcome these drawbacks, several existing technologies
have been recently developed, which will be discussed in the
following.

C. Existing Technologies

Presently, there are mainly three kinds of technologies to
reduce the processing delay and energy consumption of AR
applications, as summarized in the following [1].

• Advanced hardware structure: It is hard for traditional
battery to sustainably support the complex operations
of AR applications. As a result, advanced hardware
structures such as multi-core CPU with low frequency
and voltage can be utilized to replace the single-core ones.
Furthermore, the dynamic voltage and frequency scaling
(DVFS) technology can be adopted to save energy with
respect to speciﬁc requirements.

• Acceptable approximate computing: Consistently per-
forming accurate calculation in AR applications usually
consumes much energy and results in long processing

TABLE I
MAJOR AR DEVICES

CPU

GPU

Latency

Battery Autonomy

Weight

Product

Google
Glass
Microsoft
HoloLens
ODG R-9

Operation
Mode
Independent

1 GHz

300 MHz

Independent

1.04 GHz

Independent

2.45 GHz

HoloLens
Graphics
710 MHz

Meta 2

Dependent

3.4 GHz

1127 MHz

Recon Jet

Dependent

1 GHz

PowerVR
SGX540

delay. Therefore, approximate computing for AR tasks
can be adopted according to detailed computational ac-
curacy requirements. For example, low-precision tasks,
such as location sensing, can be approximately processed
to balance computational accuracy and energy (delay)
efﬁciency.

• Partial video frame updating: The frequent movement
of mobile AR devices would produce a large volume
of video data, which is adverse for real-time processing.
Therefore, partial video frame updating can be exploited
to avoid redundant computing by processing the newly-
generated data only.

Based on the above mentioned technologies, the end-to-end
latency and energy consumption of AR applications can be
improved to some extent. However, since the computation-
intensive tasks, e.g., tracking, mapping, and object recogni-
tion, are still remained for local computing, the end-to-end
latency and energy consumption cannot be further reduced
and the anticipated user demand is still far from reaching. To
overcome the resource shortage of mobile devices, MEC has
been recently proposed and is regarded as a new paradigm
to deliver better experience for mobile users by ofﬂoading
computation-intensive tasks to the servers deployed at the edge
of mobile networks [12]. Motivated by this, we present a
novel computation framework for embedding MEC into AR
applications in the following section.

III. MEC-BASED AR FRAMEWORK

In this section, we ﬁrst present a novel hierarchical computa-
tion architecture for multi-user AR systems and then introduce
a detailed mechanism to support AR applications.

A. Hierarchical Computation Architecture

Considering a multi-user AR scenario where several AR
applications need to be executed simultaneously, we propose a
hierarchical architecture composed of three layers, namely, the
user layer, the edge layer, and the cloud layer. As illustrated
in Fig. 2, multiple AR devices are located in the user layer
and are connected to the edge layer through wireless links.
The cloud layer mainly consists of an enormous database for
data storage and abundant computational resources for data
processing. These two layers are quite similar to the existing

700 ms for user interface
(UI) response
Few seconds for gesture
recognition
Not available

80 ms for gesture recogni-
tion
40 s for loading navigation
map

< 1 h when recording
videos
2-3 h for typical use

24 h for browsing and
casual use
Powered by PC

4 h for typical use

42 g

579 g

184 g

500 g

85 g

AR architectures and thus we do not devote special attention
henceforth.

Cloud Layer

Edge Layer

Operation Platform

Virtualized Controller

Database

Network 
Controller

AR Computing Platform

Computing 
Module

Graphics
Module

Computing 
Controller

System
Controller

Edge Cache

Storage 
Controller

Decoder

Communication Unit

Radio Remote Unit 
(RRU)

Encoder

 User Layer

Fig. 2. Hierarchical Computation Architecture.

The uniqueness of our proposed architecture is that an edge
layer is inserted between the conventional cloud layer and user
layer. The edge layer can be equipped at the base stations
(BSs) of cellular networks or the access points (APs) of WiFi
networks. The merit of this design is that MEC can be utilized
to improve the end-to-end latency and energy consumption
performance of AR applications. Furthermore, cloud comput-
ing and edge computing can be potentially collaborated for
further performance enhancement.

In the proposed hierarchical architecture, one major is-
sue is how to design the function modules in the edge
layer to seamlessly collaborate with the other two layers.
To tackle this problem, we introduce three functionalized

components, namely, communication unit, operation platform,
and virtualized controller in the edge layer on the basis of
the software deﬁned network (SDN) technology. Note that the
combination of the three modules also leads to the convergence
of communication, computation, and control. In what follows,
we will present a detailed description on these three modules.

B. MEC-based AR Operation Mechanism

Thus far, our discussions have focused on the physical
modules of the proposed hierarchical architecture for AR
applications. In the following, we further present some detailed
operations to facilitate the AR implementation.

• Communication Unit: The communication unit can be
regarded as a “bridge” that enables real-time data trans-
mission between the edge layer and the other two layers.
On one aspect, when multiple AR devices ofﬂoad the
computational tasks, such as video steams, to the BS, the
inner radio remote unit (RRU) has the responsibility to
successfully receive these data. Furthermore, these data
are required to be delivered to the operation platform for
further processing. On the other aspect, the computation
results from operation platform and cloud layer should
be multicasted to the corresponding users through the
communication unit.

• Operation Platform: The operation platform is the core
of this architecture, which processes the ofﬂoaded AR
tasks from mobile users. The original data from RRU
is ﬁrstly stored in the edge cache, and will be delivered
to the computing platform for further processing. The
critical AR computing platform is composed of a comput-
ing module (resembling computer’s CPU) and a graphics
module (resembling computer’s GPU). The former is
utilized to process the computing-related tasks, such as
tracking and mapping, while the latter is utilized to
process the graphics-related tasks like object recognition.
Since AR tasks usually require additional data, such as
3D-models and annotations of the recognized objects, we
establish a small database at the edge layer for storing the
object information that is frequently accessed. With this
design, we do not have to invariably fetch the requested
information from the remote cloud database, which can
signiﬁcantly reduce the end-to-end latency.

• Virtualized Controller: The virtualized controller serves
as the “centralized coordinator” of the whole edge layer,
which is divided into four speciﬁc components:
the
network controller, the computing controller, the storage
controller, and the system controller. The network con-
troller manages all network activities among three layers,
such as network establishment and data transmission.
Accordingly, the computing controller supervises the en-
tire process in the operation platform while it optimally
allocates the available computational resource to each AR
task with speciﬁc requirements. Moreover, the inherent
executive priorities and collaborative properties of AR
tasks from different subscribers are also evaluated in the
computing controller. In addition, the storage controller
aims at properly managing the memory mechanism of
the edge database for fast data searching and updating.
Finally, the system controller monitors the behaviours of
the above three controllers and coordinates them in a
more efﬁcient way.

Cloud Tier

Cloud
Database

Feature
Matching

No

Edge Tier

Searching 
Results

Core Network

Annotation
Storage

Yes

Find Result?

Image Database

Tracker-
Mapper

Feature 
Matching

Feature 
Extraction

Computing
Data
Storage

Edge Storage

Video
Stream
Cache

User 
Instruction
Cache

Edge Cache

Wireless Communication

Video 
Clipping

Classification

Downlink Cache

Uplink Cache

Local Cache

Renderer

User Tier

Camera & Sensor

Display

Fig. 3. MEC-based AR operation mechanism.

As depicted in Fig. 3, the proposed MEC-based AR oper-
ation mechanism can be divided into three tiers, which are
consistent with the hierarchical architecture, i.e., the user tier,
the edge tier, and the cloud tier. The main idea of this mecha-
nism is that the video source and renderer of AR applications
must be processed locally whereas the computation-intensive
components, i.e., the tracker, mapper, and object recognition
should be ofﬂoaded to the edge and cloud servers. The detailed
operations in each tier are generalized into speciﬁc blocks and
will be elaborated in the following.

The user tier is responsible for executing the local opera-
tions. Multiple AR devices simultaneously start with sensing
real environment, producing raw videos, and capturing users’
gestures via their cameras and sensors. Through identiﬁcation
analysis, this original information will be transformed into two
categories: the video streams which contain the raw video data
and the operation instructions which carry the speciﬁc require-
ments of mobile subscribers, such as object identiﬁcation and

sentiment analysis. Thereafter, this information will be further
duplicated into two copies with one stored in the uplink cache
for data transmission and the other stored in the local cache
for subsequent processing. Consequently, the transceivers of
mobile AR devices will transmit the data in the uplink cache
to the edge layer through wireless channels.

The edge tier plays a critical role in computing AR ap-
plications. Upon receiving the ofﬂoaded data from AR de-
vices,
the BS will ﬁrst classify them into two kinds: the
raw video streams and the user’s operation instructions, and
then deliver them to the edge cache for separate storage.
Thereafter, these two kinds of data are delivered to the tracker-
mapper module and the video clipping module, respectively.
The tracker-mapper module then tracks users’ locations and
builds virtual coordinates to coincide with the real world
via some algorithms, such as simultaneous localization and
mapping (SLAM) and parallel tracking and mapping (PTAM).
Meanwhile, the video clipping module slices one representa-
tive frame (or image) from each raw video for subsequent
processing. It should be noted that some users may observe
the same object and require the same information of this
object. With this regard, classiﬁers can be leveraged to assort
all frames from different videos into several categories based
on the inherent information, such that each category contains
those frames of the same object. This function can be imple-
mented via some well-known machine learning algorithms,
such as convolutional neural networks (CNN) and support
vector machines (SVM). Afterwards, one typical image of
each category is picked out and utilized to match with the
standard images pre-stored in the edge database by some
image retrieval methods. By this means, the inherent collabo-
rative properties of AR applications can be fully utilized and
redundant calculation can be effectively avoided. Then, if the
matched standard image is found in the edge database, the
corresponding annotation information will be fetched from the
adjacent edge storage. Otherwise, the related frame will be
ofﬂoaded to the cloud server for further searching.

The cloud tier contains a large cloud database for storing
the additional data that are not cached in the edge tier due
to its limited memory size. Once the retrieval operation in
the edge tier is not realized, the corresponding image will be
ofﬂoaded to the cloud tier for further searching. Owing to the
adequate computational resource at the cloud server and the
sufﬁcient capacity of the cloud storage, the image retrieval will
be generally achieved. Thereafter, the requested information
will be transmitted back to the edge tier and combined with
the virtual map that is reconstructed by the tracker-mapper
module. Also, the total computation results are multicasted to
the corresponding devices. Finally, these data are exhibited to
the subscribers after mixing with the original videos via the
local renderers.

IV. KEY IMPLEMENT TECHNOLOGIES

In this section, we will develop three key technologies, i.e.,
the joint communication and computation resource allocation,

the collaborative cloud and edge computing, and the content-
based image retrieval to further improve the energy efﬁciency
and reduce the processing latency.

A. Joint Communication and Computation Resource Alloca-
tion

In the proposed framework, the computational tasks of
an AR application can be jointly processed by the local
device, the edge server, and the cloud server. Meanwhile,
the communication resource of the wireless network and the
computational resources of the edge/cloud servers can be
shared by all AR devices. Therefore, joint communication
and computation resource allocation is essential to further
improve the delay and energy performance [13]. Generally,
two schemes in current AR scenarios should be taken into
account, i.e., the centralized and distributed resource allocation
schemes.

In centralized AR systems, both user priority and channel
state information can be acquired in the edge layer. There-
the network controller and the computing controller
fore,
can collaboratively design the resource allocation policy by
optimizing a speciﬁc objective function, such as minimizing
the end-to-end latency under a prescribed resource utilization
constraint and maximizing the mobile energy efﬁciency under
an ofﬂoading latency constraint. Moreover, the network con-
troller and the computing controller can monitor the speciﬁc
requirements of different AR tasks in real-time and adaptively
adjust the resource allocation to meet the diverse user de-
mands.

However, in distributed AR systems, the aforementioned
information can no longer be obtained by the edge layer,
making it difﬁcult to centrally control the allocation of avail-
able communication and computation resources. To tackle this
problem, game-theoretical techniques can be exploited to de-
velop distributed algorithms based on past network and chan-
nel information, whereas the obtained result is demonstrated
to achieve the Nash equilibrium and can deliver satisfactory
latency and energy consumption experiences for mobile AR
subscribers.

B. Collaborative Cloud and Edge Computing

On the one hand, traditional cloud servers are typically
deployed at the remote cloud platform, suffering from long
propagation delay when transmitting onerous AR tasks in the
core network. On the other hand, although the adjacent MEC
servers can be implemented at the nearby BS, their compu-
tation and storage capacities are usually limited. Therefore,
by combining the advantages of the powerful computation
capacity of cloud computing and the proximity to mobile
subscribers of edge computing, collaborative cloud and edge
computing is envisioned as a promising paradigm to achieve
better AR performance.

In the proposed framework, due to the limited edge storage
capacity, the additional information such as 3D models and
annotations of those unpopular objects should be stored in
the
the cloud database to alleviate the storage burden at

edge database. On the other hand, the edge database only
need to store those frequently accessed information of the
popular objects. This can be realized by storing the historical
access record of each object’s information in the edge layer
and setting an appropriate threshold to judge which object is
popular. By this means, massive object recognition operations
can be avoided at the edge layer and the AR processing
latency can be effectively reduced. Moreover, when several
edge servers are simultaneously served by the same cloud
server, it is important to properly allocate the available cloud
computational resource to each edge server based on their
workload and local computation capacities. The detailed re-
source allocation policy can be derived by solving a speciﬁc
optimization problem through some convex and non-convex
optimization tools. For instance, more cloud computational
resource need to be allocated to assist those edge servers with
less computation capacities and heavier workload to balance
the uneven resource and workload distribution over different
edge servers.

C. Content-based Image Retrieval

Object recognizer is a critical component of AR applica-
tions, which usually consumes a large processing delay, espe-
cially in the image retrieval procedure. Therefore, advanced
content-based image retrieval technology should be developed
to accelerate the searching speed. Motivated by the work in
[14], we present a typical pipeline for image retrieval within
the proposed mechanism, which consists of three steps: feature
extraction, feature matching, and geometric veriﬁcation.

1) Feature extraction: Once an image is input into the
graphics module of the edge layer,
the feature extraction
algorithm will immediately search its inherent salient interest
points, which are used to estimate the similarity between
this image and the standard images pre-stored in the edge
and cloud databases. Various robust feature descriptors can
be applied to obtain typical features, such as scale invariant
feature transform and speeded up robust features [15].

2) Feature matching: Current pairwise feature matching
algorithm generally consumes a lot of time since it directly
matches the input image with all standard images. To cope
with this issue, we pre-construct a data structure to store the
features of all standard images with particular indices. Then,
by comparing the extracted features with those in the data
structure, a shortlist of candidate images can be efﬁciently
ﬁltered. Thereafter, the slow pairwise feature matching method
can be applied between the input image and the candidate
images only, and a best-matched image can be eventually
picked out from the shortlist. In this way, the image retrieval
delay can be substantially reduced.

3) Geometric veriﬁcation: After obtaining the best-
matched standard image, further examination is required to
conﬁrm whether the matching result is correct. Geometric
veriﬁcation is commonly utilized to test whether the input
image and the best-matched standard image are similar with
only geometric and photometric distortions. If the veriﬁcation
result is correct, the matching relation will be established.

Otherwise, the input image and its features will be ofﬂoaded
to the cloud server for further searching.

the
It shall be noted that with the above three steps,
computation and storage resources of the cloud server and
edge server can be jointly utilized to improve the accuracy of
object recognition. Consequently, the end-to-end latency and
energy consumption can be effectively reduced.

V. PERFORMANCE EVALUATION

In this section, we present simulation results to manifest the
latency and energy performance enhancement of our proposals
against two benchmark schemes. We consider a scenario where
multiple mobile devices in the coverage of the same BS
execute AR applications simultaneously. The edge layer is
implemented at the central BS with a radius of 200 m. The
cloud layer is deployed in a remote cloud platform. The BS
connects the cloud platform through a backhaul link, whose
uplink and downlink transmission capacities are both set to
be 200 Mbps. The edge and cloud computing capacities are
set to be 5 × 1010 CPU cycle/s and 3 × 1011 CPU cycle/s,
respectively [12]. Each AR device is randomly distributed
according to Poisson distribution within the BS coverage,
adopting the TDMA channel access with a bandwidth of 15
MHz. The local computation capacity of each device follows
the uniform distribution between [5×108, 2×109] CPU cycle/s.
The channel gains between mobile devices and the BS are
generated according to independent and identically distributed
(i.i.d.) Rayleigh random variables with unit variance. Since the
cloud computing components are ﬁrst ofﬂoaded to the edge
layer through wireless links and then being transmitted to the
cloud server through the backhaul link equipped with high
transmission bandwidth, the distance between mobile devices
and the cloud server could be ignored in this paper. Other
major parameters of AR applications can be found in [16].

In our test, we compare the performance of our proposals
with two benchmark schemes: the local computing scheme
where the tracker and mapper modules are remained for local
processing while the object recognizer is ofﬂoaded for cloud
computing, and the cloud computing scheme where the tracker,
mapper, and object recognizer are all ofﬂoaded to the remote
cloud platform. Meanwhile, we name our proposed scheme as
the edge computing scheme.

Fig. 4(a) presents the comparative results of the end-to-end
latency of each device in the three schemes, where the trans-
mission power of each mobile device is bounded by 24 dBm.
In this simulation, since both communication and computation
latency increase almost linearly with the number of devices,
the end-to-end latency of each scheme has an approximately
linear trend. From the ﬁgure, the local computing scheme
always delivers the worst latency performance among three
schemes because of the insufﬁcient computational resource of
mobile devices. Moreover, due to the limited communication
resource, the end-to-end latency increases with the number of
devices in all schemes. However, the proposed edge computing
scheme always achieves the best latency performance since
it can collaborate the cloud computing and edge computing.

600

500

400

300

200

100

)
s
m

(

y
c
n
e

t

a

l

d
n
e
-
o

t
-
d
n
E

0

4

0.2

0.15

0.1

0.05

)
J
(
n
o

i
t

p
m
u
s
n
o
c

y
g
r
e
n
e

e

l
i

b
o
m

l

a
t
o
T

0

4

Local computing scheme
Cloud computing scheme
Edge computing scheme

12

20
AR user number

28

36

(a) End-to-end latency of three schemes.

Local computing scheme
Cloud computing scheme
Edge computing scheme

12

20
AR user number

28

36

(b) Energy consumption of three schemes.

(c) Total mobile energy consumption under different delay
tolerances.

Fig. 4. Simulation results.

Speciﬁcally, for the case with 36 devices, the edge computing
scheme can reduce about 41.44% and 12.85% latency as
compared with local computing and cloud computing schemes,
respectively. Note that in the simulation, we set the compu-
tation capacity of the cloud server almost 10 times of the
edge server as a typical example. With the increase of the
cloud computation capacity, the performance gap between the
cloud computing scheme and the edge computing scheme will
become smaller.

Fig. 4(b) depicts the comparative results of the total energy
consumption of all devices with the number of devices, where
the maximum delay tolerance of each AR application is 450

local devices. Nevertheless,

ms. From this ﬁgure, the cloud computing scheme can always
achieve a better energy efﬁciency than the local computing
scheme because of the extra energy consumption for tracking
and mapping at
the proposed
edge computing scheme always achieves the highest energy
efﬁciency among all schemes. In the scenario with 36 devices,
our proposed scheme can save about 73.71% and 65.34%
energy consumption as compared with local computing and
cloud computing schemes, respectively. Speciﬁcally, the over-
all energy consumption of each scheme would grow rapidly
when the number of devices becomes large. The reason is
that the communication energy becomes dominant with large
number of mobile devices, which increases exponentially with
the number of devices.

To further show the performance advantage on energy
saving of the proposed edge computing scheme, we depict the
total energy consumption of all mobile devices with different
delay tolerances in Fig. 4(c), where the number of devices is
set to be 30. From the ﬁgure, the mobile energy consumption
of each scheme decreases with the maximum delay tolerance.
The reason is that, more computational resource should be
utilized to assist the task processing to meet the stricter delay
requirement, leading to the higher mobile energy consumption.
Once again, the superiority of the edge computing scheme is
demonstrated by this test.

Overall, our tests demonstrate that

the MEC-based AR
framework is more efﬁcient
than other two conventional
schemes as a result of the collaboration between cloud com-
puting and edge computing. Both the processing latency and
the energy consumption can be signiﬁcantly reduced, leading
to better QoE, such as high image resolution, long battery
autonomy, convenient portability, and real-time performance
for AR subscribers.

VI. CONCLUSION

This article presents a novel MEC-based computation
framework for current AR applications to optimize the energy
efﬁciency and processing delay. A hierarchical computation
architecture is ﬁrst developed, which is composed of three
layers: the user layer, the edge layer, and the cloud layer. By
seamlessly integrating the communication, computation, and
control functions at the edge layer and taking full advantages
of both edge computing and cloud computing, computation-
intensive tasks of AR applications can be intelligently of-
ﬂoaded to both edge and cloud servers for collaborative
computation. In accordance with this architecture, we then
develop a novel mechanism to simultaneously support multiple
AR applications of different mobile subscribers. Three key
technologies are also discussed, which can cooperate to further
reduce the processing latency and energy consumption of
mobile AR devices. Our proposed architecture and mechanism
are ﬁnally tested by simulation results, which demonstrate the
substantial performance improvement of our proposals over
the existing schemes.

 
 
 
 
 
 
REFERENCES

[1] D. Chatzopoulos et. al., “Mobile augmented reality survey: From where
we are to where we go,” IEEE Access, vol. 5, pp. 6917-6950, Apr. 2017.
[2] M. Satyanarayanan et. al., “The case for VM-based cloudlets in mobile
computing,” IEEE Pervasive Comput., vol. 8, no. 4, pp. 14-23, Oct.-Dec.
2009.

[3] F. Liu et. al., “Gearing resource-poor mobile devices with powerful
clouds: Architectures, challenges, and applications,” IEEE Wireless
Commun., vol. 20, no. 3, pp. 14-22, Jun. 2013.

[4] W. Li et. al., “Mechanisms and challenges on mobility-augmented
service provisioning for mobile cloud computing,” IEEE Commun. Mag.,
vol. 53, no. 3, pp. 89-97, Mar. 2015.

[5] T. X. Tran et. al., “Collaborative mobile edge computing in 5G networks:
New paradigms, scenarios, and challenges,” IEEE Commun. Mag., vol.
55, no. 4, pp. 54-61, Apr. 2017.

[6] Y. Mao et. al., “A survey on mobile edge computing: The communication
perspective,” IEEE Commun. Surv. Tut., vol. 19, no. 4, pp. 2322-2358,
Aug. 2017.

[7] T. Taleb et. al., “Mobile edge computing potential in making cities

smarter,” IEEE Commun. Mag., vol. 55, no. 3, pp. 38-43, Mar. 2017.

[8] X. Masip-Bruin et. al., “Foggy clouds and cloudy fogs: A real need
for coordinated management of fog-to-cloud computing systems,” IEEE
Wireless Commun., vol. 23, no. 5, pp. 120-128, Oct. 2016.

[9] L. Tong et. al., “A hierarchical edge cloud architecture for mobile
computing,” in Proc. IEEE Int. Conf. Comput. Commun. (INFOCOM),
San Francisco, California, Apr. 2016, pp. 1-9.

[10] T. Verbelen et. al., “Leveraging cloudlets for immersive collaborative
applications,” IEEE Pervasive Computing, vol. 12, no. 4, pp. 30-38,
Oct.-Dec. 2013.

[11] S. Bohez et. al., “Mobile, collaborative augmented reality using

cloudlets,” in Proc. Mobilware, Bologna, Italy, Nov. 2013, pp. 45-54.

[12] C. You, et. al., “Energy-efﬁcient resource allocation for mobile-edge
computation ofﬂoading,” IEEE Trans. Wireless Commun., vol. 16, no.
3, pp. 1397-1411, Mar. 2017.

[13] S. Sardellitti et. al., “Joint optimization of radio and computational
resources for multicell mobile-edge computing,” IEEE Trans. Signal Inf.
Process. Netw., vol. 1, no. 2, pp. 89-103, Jun. 2015.

[14] B. Girod et. al., “Mobile visual search: Architectures, technologies, and
the emerging MPEG standard,” IEEE Multimedia, vol. 18, no. 3, pp.
86-94, Jul.-Sep. 2011.

[15] E. Salahat and M. Qasaimeh, “Recent advances in features extraction
and description algorithms: A comprehensive survey,” in IEEE ICIT,
Toronto, Canada, Mar. 2017, pp. 1059-1063.

[16] A. Al-Shuwaili and O. Simeone, “Energy-efﬁcient resource allocation
for mobile edge computing-based augmented reality applications,” IEEE
Wireless Commun. Lett., vol. 6, no. 3, pp. 398-401, Jun. 2017.

