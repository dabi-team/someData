A CONCATENATING FRAMEWORK OF SHORTCUT 

CONVOLUTIONAL NEURAL NETWORKS 

Yujian Li (liyujian@bjut.edu.cn), Ting Zhang, Zhaoying Liu, Haihe Hu 

ABSTRACT 

It  is  well  accepted  that  convolutional  neural  networks  play  an  important  role  in  learning 
excellent features for image classification and recognition. However, in tradition they only allow 
adjacent layers connected, limiting integration of multi-scale information. To further improve 
their  performance,  we  present  a  concatenating  framework  of  shortcut  convolutional  neural 
networks. This framework can concatenate multi-scale features by shortcut connections to the 
fully-connected layer that is directly fed to the output layer. We do a large number of experiments 
to investigate performance of the shortcut convolutional neural networks on many benchmark 
visual  datasets  for  different  tasks.  The  datasets  include  AR,  FERET,  FaceScrub,  CelebA  for 
gender  classification,  CUReT  for  texture  classification,  MNIST  for  digit  recognition,  and 
CIFAR-10  for  object  recognition.  Experimental  results  show  that  the  shortcut  convolutional 
neural networks can achieve better results than the traditional ones on these tasks, with more 
stability  in  different  settings  of  pooling  schemes,  activation  functions,  optimizations, 
initializations, kernel numbers and kernel sizes. 

1 INTRODUCTION 

Convolutional neural networks (CNNs) are hierarchical feed-forward architectures that compute 
progressively  in  invariant  representations  of  the  input  images.  As  an  excellent  method  for 
extracting image features, they have been widely applied to a variety of domains, such as face 
recognition  (Taigman et al, 2014) (Lopes et al, 2017) (Sun et al, 2016), bounding box object 
detection  (Girshick  et  al,  2014)  (Zhang  et  al,  2016),  key  point  prediction  (Sun  et  al,  2013) 
(Jonathan et al, 2014), and large-scale image classification task (Simonyan and Zisserman, 2014) 
(Deng et al, 2009) (Szegedy et al, 2015) (Fukushima. 1979), etc. 

The  first  implemented  CNN  is  considered  to  be  the  model  of  neocognitron  developed  by 
Fukushima with the insight of receptive field (Fukushima. 1979). In 1998, LeCun et al. combined 
convolutional  layers  with  pooling  layers  to  make  an  early  version  of  modern  CNNs  (LeNet) 
(LeCun et al, 1998). In 2012, Krizhevsky et al. (2012) proposed a breakthrough architecture, the 
AlexNet,  for  ImageNet  Large  Scale  Visual  Recognition  Competition  (ILSVRC).  In  2013, 
Simonyan and Zisserman presented the VGG network, using very small convolution filters to 
push the depth of weight layers (Simonyan and Zisserman, 2014). In 2014, by integrating with 
“inception modules”, Szegedy et al. designed the GoogLeNet (Szegedy et al, 2015). It is worth 
mentioning  that  the  AlexNet,  the  VGG  network,  and  the  GoogLeNet  won  the  first  place  in 
ILSVRC 2012, 2013, and 2014, respectively. 

Traditionally, a standard CNN is composed of convolutional layers (CLs), pooling layers (PLs), 

 
and fully-connected layers (FCLs), as illustrated in Fig.1. It can be seen that CLs and PLs are 
generally arranged in an alternating fashion to extract features from different scales. One or more 
FCLs together with the output layer are exploited to work as a classifier. For convenience, we 
refer to a CL or a PL as a CPL. In fact, a CNN comprises a number of CPLs, followed by several 
FCLs. Note that a CL/PL may consist of many convolutional/pooling feature maps. One major 
advantage of CNNs is the use of shared weights in CLs, which means that the same convolutional 
kernel is used for each pixel in the layer. This not only greatly reduces the number of parameters 
involved in a CNN, but also improves its performance (Jin et al, 2016). 

Strictly speaking, a standard CNN has no shortcut connections cross layers, where the topmost 
CPL  separates  the  lower  CPLs  from  the  first  fully-connected  layer  (FFCL).  Thus,  given  the 
features extracted from the topmost CPL, the final output of the FCLs is independent of the lower 
CPLs. Although such a CNN can work very well in many situations, it is limited to integrate 
multi-scale information from an image. 

Fig.1 An example of standard CNNs. There is only one FCL in this architecture. 

To make use of discriminative information from non-topmost CPLs, we propose a concatenating 
framework of shortcut convolutional neural networks (S-CNNs), to integrate multi-scale features 
through  shortcut  connections  in  a  CNN.  This  framework  can  select  some  different  levels  of 
powerful  features  to  be  concatenated  for  final  decision  of  classification  and  recognition.  As 
displayed in Fig. 2, an S-CNN can integrate multi-scale features through shortcut connections 
from a number of CPLs to the FFCL. It is well admitted that human vision is a multi-scale process 
(Donoho and Huo, 2001). Therefore, it would be reasonable to integrate different levels of image 
features for robust classification, where low-level features are finer and high-level features are 
more invariant, with their combination probably producing good representations in leverage of 
concrete and abstraction (Sermanet et al, 2013). 

Fig. 2. An architecture of S-CNNs, where some different CPLs are concatenated to form the FFCL through 

shortcut connections. 

InputCLPLCLCLPLPLFCLOutputMulti-level features10000 
 
Recently,  shortcut  connections  have  attracted  great  interest  since  the  success  of  residual 
networks  on  the  tasks  of  ILSVRC  2015  classification,  ImageNet  detection,  ImageNet 
localization, COCO detection, and COCO segmentation (He et al, 2016). The related work of S-
CNNs could be roughly divided into two modes: trainable and fixed.   

Trainable-mode  S-CNNs  refer  to  the  CNNs  that  have  trainable  shortcut  connections.  For 
example, Sermanet and LeCun applied a multi-scale CNN to the task of traffic sign classification 
(Sermanet and LeCun, 2011), and they got the first place in the German Traffic Sign Recognition 
Benchmark (GTSRB) competition. In their network, both the first pooling layer and the second 
pooling layer are directly fed to the fully-connected layer through trainable shortcut connections. 
Sun et al. proposed a DeepId network for face verification (Sun et al, 2014), which only allows 
the last CPL but one to have trainable shortcut connections. Srivastava et al. (2015) designed a 
highway network for digit classification, allowing earlier representations to flow unimpededly 
to later layers through parameterized shortcut connections known as “information highways”. 
The parameters of shortcut connections are learned for controlling the amount of information 
allowed on these “highways”. 

The fixed mode S-CNNs refer to the CNNs that have fixed shortcut connections. For example, 
the deep residual networks allow shortcut connections to cross two or three convolutional layers 
(He  et  al,  2016).  Huang  et  al.  presented  densely  connected  CNNs  (DenseNets),  which  allow 
connections from each layer to every other layer in a feed-forward fashion (Huang et al, 2017). 
Vincent et al. proposed a texture and shape CNN for texture classification, which only allows 
the  shortcut  connections  to  cross  three  CPLs  (Andrearczyk  and  Whelan,  2016).  Shen  et  al. 
presented a  multi-crop CNN for lung nodule malignancy suspiciousness classification, which 
concatenates three multi-crop pooling layers (Shen et al, 2017). Liu et al. introduced a single 
shot multibox detector for detecting objects in images using a single deep neural network, which 
concatenates multiple convolutional features (Liu et al, 2016). These shortcut connections are 
always alive and the gradients can easily back propagate through them, which results in faster 
training. 

Currently, both the trainable-mode and fixed-mode S-CNNs are discussed as a specific structure. 
Apart from them, we present a concatenating framework of multi-scales features instead of a 
specific structure. In contrast to the trainable-mode S-CNNs, our framework has the fixed value 
of  1  for  all  weights  of  shortcut  connections.  And  compared  with  the  fixed-mode  CNNs,  our 
framework can bypass more than three hidden layers. Overall, the motivation of our work is to 
integrate multi-scale features through a variety of shortcut connections with fixed weights. 

In this paper, we propose a concatenating framework of S-CNNs by adding shortcut connections 
to standard CNNs, together with a shortcut backpropagation  algorithm. Using an indicator of 
binary string (called shortcut indicator), we can conveniently choose a shortcut style from the 
framework to integrate different levels of multi-scale features. Based on this convenience, we 
conduct  a  large  number  of  experiments  to  compare  S-CNNs  with  standard  CNNs  on  seven 
datasets for classification of gender and texture as well as for recognition of digit and object. 
Moreover,  we compare their performance in different settings of pooling schemes, activation 
functions,  initializations,  optimizations,  and  convolutional  kernels’  numbers  and  sizes. 
Additionally, experimental results show that S-CNNs can generally achieve better performance 
than standard CNNs with more stability. Finally, we summarize the whole paper in conclusions. 

Fig.  3.  A  concatenating  framework  of  S-CNNs.  In  this  framework,  features  from  different  CPLs  are 

concatenated  to  form  the  FCL  which  is  directly  fed  to  the  output  layer.  Here,  the  shortcut  indicator  is 
, indicating the shortcut style of all lower 2r-1 CPLs having shortcut connections to the 

FCL. 

2 FRAMEWORK DESCRIPTION 

Based on the architecture of CNNs in Fig.1, we present a concatenating framework of S-CNNs 
by adding shortcut connections. As displayed in Fig. 3, this framework is an alternating structure 
of r CLs and r PLs, followed by a FCL and an output layer. The FCL is a concatenation of these 
CLs and PLs through a style of shortcut connections, which is represented by a binary string, 
called shortcut indicator (SI). Accordingly, we can give a description of the framework as follows. 

The input 
and n is the channel dimension, with 

  is a 3-dimensional array of size 

, where h and w are spatial dimensions, 

  for color images and 

  for grayscale images. 

Using “ ” to stand for convolutional operator and “
of a convolutional layer can be expressed as   

” for activation function, the computation 

,                      (1) 

where 
and the j-th feature map in the (2k-1)-th hidden layer, 
the (2k-1)-th hidden layer. 

  is the weight matrix between the i-th feature map in the (2k-2)-th hidden layer 
  is the bias of the j-th feature map in 
  and 
  denoting the i-th feature map in the (2k-2)-th hidden layer and the j-th feature map in the 
  can be sigmoid (Ni et al, 2013) or 

  stands for the j-th feature map in the k-th CL, with 

(2k-1)-th hidden layer for the l-th sample, respectively. 
rectified linear unit (ReLU) (Krizhevsky, et al, 2012). Here, we let 

. 

In  each  pooling  layer,  we  use  a  fixed  stride  for  all  feature  maps.  The  pooling  function  is 
formulated as: 

,                                (2) 

  can be average pooling or max-pooling. 

where 
  indicate the j-th 
feature map in the (2k-1)-th hidden layer and the j-th feature map in the 2k-th hidden layer for 
the l-th sample, respectively. 

  stands for the j-th feature map in the k-th PL. 

  and 

InputCLPLCLCLPLPLFCLOutputxh2r+1oh1(c1)h2(s1)h2k-1(ck)h2k(sk)h2r-1(cr)h2r(sr)111111xhwn3n1nf212121,,21,22,,1llllkkkjkjkjkiijjiffkrhcuhWb21kijW21kjb,lkjc22,lkih21,lkjhf0llhx2,,21,,1lllkjkjkjpoolingkrhshpooling21,lkjh2,lkjh,lkjs 
The  FCL  is  the  concatenation  of  two  or  more  CPL  activations  through  shortcut  connections, 
forming the entire discriminative vector of multi-scale features. In fact, the FCL takes the form: 

,                                        (3) 

  (

  and 

where 
sample,  respectively.  Let 
indicates the shortcut style. For example, 
1  associated  CPLs  have  shortcut  connections  to  the  FCL. 
style that only the first CPL has shortcut connections to the FCL. 
style that has no shortcut connections at all, meaning the standard CNN. 

)  denote  the  (2k-1)-th  and  the  2k-th  hidden  layer  for  the  l-th 
  be  a  binary  string  called  shortcut  indicator,  which 
  indicates the shortcut style that all the 2r-
  represents  the  shortcut 
denotes the shortcut 

The  actual  output  is  a  C-way  softmax  predicting  the  probability distribution  over  C  different 
classes, expressed as: 

,                                    (4) 

where 

  and 

  stand  for  the  weight  and  bias  of  the  output  layer,  with 

.   

3 LEARNING ALGORITHM 

For the l-th sample, the S-CNNs compute the activations of all CPLs, the FCL and the actual 
output as follows: 

,                            (5) 

Let 

  be the desired output and 

  the actual output. Taking 

the objective function of cross entropy loss, namely, 

we can first compute the sensitivities 
  of the output layer as follows: 

,                                                (6) 

  of each hidden layer and the sensitivity 

,                    (7) 

where 

  and
layer, respectively. 
FCL) that corresponds to the (2k-1)-th or 2k-th hidden layer. Additionally, 

  stands  for  the  sensitivity  (or  backpropagation  error)  of  the  output  layer, 
  represent  the  sensitivities  of  the  (2k-1)-th  and  2k-th  hidden 
  is the part of the (2r+1)-th hidden layer (i.e. the 
  is the 

  or 

2111222121222,,,,,,llllllrkkkkraaaahhhhhh21lkh2lkh1kr12321rAaaaa1111A1000A0000A222221llrlrrsoftmaxsoftmaxouWhb22rW22rb()expexpiijjsoftmaxxxx212121,21,22,2,21,2111222121222222221,1,1,,,,,,lllkkkjkjkiijjillkjkjllllllrkkkkrllrlrrffkrpoolingkraaaasoftmaxsoftmaxhuhWbhhhhhhhhouWhb12,,...,TllllCyyyy12,,...,TllllCoooo11,logNCllllNcclcLyoyo121lkkrδlδ2221221,FC2,FC21,FC2,FC2,FC2122,FC21,,2,2121,FC,212,21,22,FC,'',1180,1lllTlrllrrllllllkkrrllrrllllkjkjkjkkjllklkjkjijkkjsoftmaxfuppoolingakrrotaδoyWuδδδδδδδδδuδδδδWδkrlδ211lkkrδ21lkkrδ21,FClkδ2,FC1lkkrδuppoolingupsampling  function  for  the  pooling  function  defined  by  (2). 
derivative  of  the  softmax  function, 
vertically, and the symbol “ ” denotes Hadamard product. 

  stands  for  the 
  indicates  flipping  a  matrix  horizontally  and 

Using (6) and (7), we can compute the derivatives with respect to each parameter (i.e., weights 
and biases) as follows. 

,                                  (8) 

Based on (5)-(8), we design a training algorithm of gradient descent for the S-CNNs as shown 
in  Algorithm  1,  i.e.  shortcut  BP  for  S-CNNs.  Note  that  maxepoch  stands  for  the  number  of 
training iterations. 

Input: Training set

, network architecture, maxepoch 

Output: network parametets 

Randomly initialize the weights and biases of the S-CNNs; 
for epoch=1 to maxepoch do 
  for l=1 to N do 
    Compute the hidden activations and the actual outputs by (5); 
    Compute the sensitivities of each layer by (7); 
    Compute the derivatives by (8); 
    Update all the weights and biases with gradient descent; 
  end 
end 

Algorithm 1: Shortcut BP for S-CNNs 

4 EXPERIMENTS 

In  this  section,  we  evaluate  S-CNNs  for  gender  classification,  texture  classification,  digit 
recognition, and object recognition. We implemented a stochastic version of Algorithm 1 by the 
GPU-accelerated  ConvNet  library  Caffe  (Jia.  2013),  initializing  the  weights  by  the  “Xavier” 
method  (Glorot  and  Bengio,  2010)  to  train  the  S-CNNs.  The  experimental  environment  is  a 
desktop PC equipped with E5-2643 V3 CPU, 64GB memory and a NVIDIA Tesla K40c. 

In all experiments, the momentum is set to 0.9 and the mini-batch size is set to 100. The weight 
decay is set to 0.004 for gender classification, texture classification and object recognition, and 
to 0.005 for digit recognition. The fixed learning rate is set to 0.001 for weights and double for 
biases in all the four tasks. 

4.1 GENDER CLASSIFICATION 

In this subsection, we use four datasets, namely, AR (Maetinez. 2001), FERET (Phillips, et al, 
1998),  FaceScrub  (Ng  and  Winkle,  2014),  and  CelebA  (Yang  etal,  2015),  to  compare  the 
performance of the standard CNN and S-CNNs on gender classification. The training iterations 
are 5000, 5000, 50000, and 60000 for them, respectively. With their examples shown in Fig.4, 
we describe some more detailed information as follows. 

(1)  The AR dataset consists of over 4000 frontal images for 126 subjects, including different 

'softmax180rot2122221121,22,21,212111,,,,1δhδWbδhδWbNNTlllNNrrrllNNlllNNkjkikjkkllijjLLLLkr,,1llSlNxy 
facial expressions, illumination conditions and disguises. Only a subset of 50 male subjects and 
50 female subjects were used in the experiments, 26 images per subject. From the subset, 40 
females and 40 males were selected for training, and the rest for testing.   

(2)  The  FERET  dataset  contains  14038  images  of  1196  different  individuals  with  at  least  5 
images each, including different lighting conditions and non-neural expressions. In the dataset, 
there are 5195 female images and 8843 male images, respectively. All the images were used in 
the experiments, where 4414 female images and 4417 male images were selected for training, 
and the rest for testing.   

(a) 

(b) 

(c) 

(d) 

Fig. 4. Image examples from AR dataset (a), FERET dataset (b), FaceScrub dataset (c) and CeleA 

dataset (d). 

(3)  The FaceScrub dataset comprises a total of 107081 images of 530 celebrities, about 200 
images each. These images were retrieved from the Internet or taken under real-world situation, 
with the duplicate and degenerate images removed. From the dataset, 34250 female images and 
35718 male images were used in the experiments, where 30169 female images and 31648 male 
images were selected for training, and the rest for testing.   

(4)  The  CelebA  dataset  is  composed  of  118162  female  images  and  84437  male  images, 
covering large pose variations and background clutter. From the dataset, 80000 female images 
and 80000 male images were selected for training. From the rest, 4000 male images and 4000 
female images were chosen for testing.   

It should be noted that for each of these four datasets, all images for any single subject are either 
in the training set or the testing set, but not both. We describe the standard CNN in Table 1 and 
report the results in Tables 2-5, bolding the highest accuracies. Note that SI=00000 means the 
standard CNN. 

 
 
 
 
In  Table  2,  all  S-CNNs  outperform  the  CNN  (92.30%)  in  terms  of  accuracy.  With  only  one 
shortcut  CPL,  i.e.  the  shortcut  styles  of  10000,  01000,  00100,  00010  and  00001,  S-CNNs 
gradually have slightly worse performance. With two shortcut CPLs, the highest accuracy of S-
CNNs is 95.23% obtained by the 01010 shortcut style, and the lowest is 93.65% by the 00011 
shortcut style. With three shortcut CPLs, the highest accuracy is 94.83% obtained by the 01101 
shortcut style, and the lowest is 93.84% by the 11010 shortcut style. With four shortcut CPLs, 
the highest accuracy is 94.64% obtained by the 01111 shortcut style, and the lowest is 93.85% 
by the 11011 shortcut style. 

Table 1 Description of the standard CNN used for gender classification. 

layer 
x 
h1 
h2 
h3 
h4 
h5 
h6 
h7 
o 

type 
Input 
CL 
PL 
CL 
PL 
CL 
PL 
FCL 
Output 

activation function  patch size  stride  output size 

ReLU 
max-pooling 
ReLU 
max-pooling 
ReLU 
max-pooling 

softmax 

Table 2 Test accuracies (%) on the AR dataset. 

SI 
00000 
10000 
01000 
00100 
00010 
00001 
11000 
10100 

Accuracy 
92.30 
94.26 
94.25 
94.21 
93.70 
93.47 
93.68 
93.86 

SI 
10010 
10001 
01100 
01010 
01001 
00110 
00101 
00011 

Accuracy 
93.85 
93.84 
94.46 
95.23 
95.22 
94.44 
93.86 
93.65 

SI 
11100 
11010 
11001 
10110 
10101 
10011 
01110 
01101 

Accuracy 
94.05 
93.84 
94.06 
93.85 
94.06 
94.05 
94.83 
94.42 

SI 
01011 
00111 
11110 
11101 
11011 
10111 
01111 
11111 

Accuracy 
94.45 
94.24 
94.62 
94.42 
93.85 
94.61 
94.64 
94.44 

In Table 3, all S-CNNs but the three shortcut styles of 11100, 11110 and 11111 perform better 
than the CNN (85.29%). With one shortcut  CPL, the highest accuracy of S-CNNs is 89.07% 
achieved by the shortcut style of 01000, and the lowest is 86.75% by 00001. With two shortcut 
CPLs, the highest accuracy is 88.55% achieved by the shortcut style of 01001, and the lowest is 
85.36% by 00011. With three shortcut CPLs, the highest accuracy is 88.51% achieved by the 
shortcut style of 10110, and the lowest is 85.17% by 11100. With four shortcut CPLs, the highest 
accuracy is 89.81% achieved by the shortcut style of 10111, and the lowest is 83.73% by 11110. 

Table 3 Test accuracies (%) on the FERET dataset. 

SI 
00000 
10000 
01000 
00100 
00010 
00001 
11000 
10100 

Accuracy 
85.29 
89.05 
89.07 
88.74 
88.29 
86.75 
88.26 
87.50 

SI 
10010 
10001 
01100 
01010 
01001 
00110 
00101 
00011 

Accuracy 
87.35 
86.44 
87.01 
87.15 
88.55 
86.95 
86.61 
85.36 

SI 
11100 
11010 
11001 
10110 
10101 
10011 
01110 
01101 

Accuracy 
85.17 
85.72 
88.25 
88.51 
86.47 
86.16 
86.28 
86.31 

SI 
01011 
00111 
11110 
11101 
11011 
10111 
01111 
11111 

Accuracy 
86.96 
85.50 
83.73 
88.80 
89.68 
89.81 
87.92 
82.62 

323215512828622214146551101012222551222144162222216642 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
In Table 4, all S-CNNs have higher accuracies than the CNN (78.57%). With one shortcut CPL, 
the highest accuracy of S-CNNs is 80.98% reached by the style of 00100, and the lowest is 79.98% 
by 10000. With two shortcut CPLs, the highest accuracy is 82.14% reached by the style of 01001, 
and the lowest is 80.01% by 11000. With three shortcut CPLs, the highest accuracy is 81.37% 
reached by 11010, and the lowest is 80.52% by 10011 and 00111. With four shortcut CPLs, the 
highest accuracy is 81.58% reached by 10111, and the lowest is 79.79% by 11011. 

Table 4 Test accuracies (%) on the FaceScrub dataset. 

SI 
00000 
10000 
01000 
00100 
00010 
00001 
11000 
10100 

SI 
00000 
10000 
01000 
00100 
00010 
00001 
11000 
10100 

Accuracy 
78.57 
79.98 
80.68 
80.98 
80.6 
80.97 
80.01 
80.37 

SI 
10010 
10001 
01100 
01010 
01001 
00110 
00101 
00011 

Accuracy 
80.04 
80.53 
80.96 
81.10 
82.14 
80.85 
80.99 
80.93 

SI 
11100 
11010 
11001 
10110 
10101 
10011 
01110 
01101 

Accuracy 
80.55 
81.37 
80.79 
81.07 
80.57 
80.52 
81.21 
80.74 

Table 5 Test accuracies (%) on CelebA dataset. 

Accuracy 
84.21 
86.30 
85.95 
85.92 
85.81 
85.72 
86.18 
86.89 

SI 
10010 
10001 
01100 
01010 
01001 
00110 
00101 
00011 

Accuracy 
86.62 
86.20 
86.62 
86.57 
86.29 
86.26 
86.17 
86.54 

SI 
11100 
11010 
11001 
10110 
10101 
10011 
01110 
01101 

Accuracy 
86.73 
87.19 
86.15 
87.06 
86.74 
86.63 
86.54 
86.40 

SI 
01011 
00111 
11110 
11101 
11011 
10111 
01111 
11111 

SI 
01011 
00111 
11110 
11101 
11011 
10111 
01111 
11111 

Accuracy 
80.67 
80.52 
80.72 
80.56 
79.79 
81.58 
80.80 
80.59 

Accuracy 
86.39 
86.19 
86.73 
86.67 
86.64 
86.62 
86.75 
87.00 

In Table 5, all S-CNNs have accuracies exceeding the CNN (84.21%). With one shortcut CPL, 
S-CNNs  perform  slightly  worse  gradually  for  the  styles  of  10000,  01000,  00100,  00010  and 
00001. With two shortcut CPLs, the highest accuracy of S-CNNs is 86.89% attained by 10100, 
and the lowest is 86.17% by 00101. With three shortcut CPLs, the highest accuracy is 87.19% 
attained by 11010, and the lowest is 86.15% by 11001. With four shortcut CPLs, the highest 
accuracy is 86.75% attained by 01111, and the lowest is 86.62% by 10111. 

Overall, the S-CNNs get the highest accuracies of 95.23% on AR, 89.81% on FERET, 82.14% 
on FaceScrub, and 87.19% on CeleA. Compared to the CNN, these accuracies gain a relative 
increase of 3.17%, 5.30%, 4.54%, and 3.54%, respectively. This is probably because the S-CNNs 
can integrate multi-scale features from many CPLs, leading to a more suitable model. It should 
be noted that the best shortcut styles are generally data-dependent, varying on different datasets. 
Furthermore, in this experiment the shortcut style of 11111 is never the best one for S-CNNs, 
probably with too many parameters to get well-trained. 

4.2 TEXTURE CLASSIFICATION 

In this subsection, using the same CNN architecture except with the output size of 61 given in 
Table 1, we investigate performance of the CNN and S-CNNs on CUReT dataset (Dana et al, 
1999) for texture classification. The CUReT dataset has 12505 images in 61 texture classes, 205 
images per class, with different pose and illumination conditions, specularities, shadowing, and 
surface normal variations. From the dataset, 185 images per class were selected for training, and 

the rest for testing. Examples of this dataset are shown in Fig. 5. For 60000 training iterations, 
the results of the CNN and S-CNNs are reported in Table 6, with the highest accuracy bolded. 

In Table 6, all S-CNNs have higher accuracies than the CNN (66.17%). For 1-4 shortcut CPLs, 
the highest accuracies of S-CNNs are 75.12%, 78.86%, 79.00% and 75.81% with the lowest of 
66.97%,  64.87%,  70.39%  and  73.00%,  respectively.  The  highest  ones  are  achieved  by  the 
shortcut styles of 00001, 00011, 01101 and 11101, and the lowest by 10000, 11000, 10110 and 
01111. 

Fig. 5. Image examples from the CUReT dataset. 

SI 
00000 
10000 
01000 
00100 
00010 
00001 
11000 
10100 

Accuracy 
66.17 
66.97 
71.70 
73.42 
74.91 
75.12 
64.87 
72.13 

Table 6 Test accuracies (%) on the CUReT dataset. 
SI 
01011 
00111 
11110 
11101 
11011 
10111 
01111 
11111 

Accuracy 
69.95 
72.44 
76.86 
74.38 
70.24 
75.97 
77.34 
78.86 

Accuracy 
73.42 
73.58 
70.39 
71.87 
74.69 
75.49 
76.13 
79.00 

SI 
10010 
10001 
01100 
01010 
01001 
00110 
00101 
00011 

SI 
11100 
11010 
11001 
10110 
10101 
10011 
01110 
01101 

Accuracy 
77.35 
77.24 
74.00 
75.81 
75.14 
75.38 
73.00 
74.72 

Overall, the highest accuracy of S-CNNs on CUReT is 79.00%, relatively increased by 19.39% 
in comparison with the CNN. This means that the S-CNNs could be more suitable than the CNN 
for  texture  classification  by  integration  of  multi-scale  features.  However,  in  general  the  best 
shortcut style is not 11111, by which the S-CNN gets the accuracy of 74.72%. 

4.3 DIGIT RECOGNITION 

Taking the LeNet as the CNN (LeCun et al, 1998), we now move forward to test the CNN and 
S-CNNs on MNIST dataset for digit recognition. The MNIST dataset consists of 
  pixel 
grayscale images of hand-written digits (from 0 to 9) (Yu et al, 2013). There are 60000 training 
images and 10000 testing images in total, but noting that the number of images per digit is not 
uniformly distributed. We use the standard split for training and testing here. 

The LeNet contains two convolutional layers and two pooling layers with the fully-connected 
layer being the vectorization of the last pooling layer. For 10000 training iterations, the results 
of the CNN and S-CNNs are reported in Table 7. 

Table 7 Test accuracies (%) on the MNIST dataset. 
SI  Accuracy  SI  Accuracy 
000 
100 
010 
001 

99.04 
99.06 
99.20 
99.16 

99.04 
99.08 
99.17 
99.13 

110 
101 
011 
111 

From Table 7, we can see that the S-CNNs outperform the CNN (99.04%) overall, in consistency 

2828 
 
with that for gender and texture classification. The highest accuracy is 99.20% by the shortcut 
style of 010 for one shortcut CLP, and 99.17% by 110 for two shortcut CPLs, with the lowest 
accuracies of 99.06% and 99.04% by 100 and 110, respectively.   

Overall,  the  S-CNNs  get  the  highest  accuracy  of  99.20%  on  the  MNIST  dataset,  gaining  a 
relative 0.16% increase compared with the CNN. Note that the best performance is obtained by 
the style of 010, rather than by 111.   

activation function  patch size  stride  output size 

Table 8 The CNN architecture used for the CIFAR-10 dataset. 
type 
Input 
CL 
PL 
CL 
PL 
CL 
PL 
FCL 
Output 

ReLU 
max-pooling 
ReLU 
max-pooling 
ReLU 
max-pooling 

1 
2 
1 
2 
1 
2 

Softmax 

layer 
x 
h1 
h2 
h3 
h4 
h5 
h6 
h7 
o 

Table 9 Test accuracies (%) on the CIFAR-10 dataset. 

SI 
00000 
10000 
01000 
00100 

Accuracy 
73.10 
23.02 
40.64 
74.61 

SI 
00001 
00101 
00011 
00111 

Accuracy 
77.57 
70.00 
74.37 
69.64 

4.4 OBJECT RECOGNITION 

Using the same architecture described in Table 8 except with each pooling layer modified by the 
local response normalization (LRN) function (Krizhevsky et al, 2012), we evaluate performance 
of the CNN and S-CNNs on the CIFAR-10 dataset (Krizhevsky. 2012) for object recognition. 
  pixels. It contains 60000 images of 10 
The CIFAR-10 dataset is a set of color images of 
commonly seen object categories (e.g., animals and vehicles), varying significantly not only in 
object position and object scale within each class but also in colors and textures among classes. 
There are 50000 images used for training and the rest 10000 for testing, and all 10 categories 
have equal number of training and test images. We use the standard split for training and testing. 

For 60000 training iterations, we report the results of the CNN and seven S-CNNs in Table 9. 
We do not give the results of all short-cut styles because they generally deteriorate in case of 
concatenating too many CPLs. Even so, the best S-CNN can achieve the highest accuracy of 
77.57% on the CIFAR-10 dataset, gaining a relative increase of 6.11% compared with the CNN. 
Thus, a proper shortcut style is important to make better performance of S-CNNs. 

4.5 DIFFERENT SETTINGS 

Using the same CNN architecture described in Table 1, we further compare the CNN and S-
CNNs  with  different  settings  of  pooling  schemes,  activation  functions,  initializations,  and 
optimizations  on  the  AR  dataset.  The  results  are  reported  in  Tables  10-13,  with  the  best 
accuracies bolded. 

32323553232323316163255161632338832228832334416256103232 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
In Table 10, we show performance of the CNN and S-CNNs with average pooling (LeCun et al, 
1990) instead of max-pooling. Compared to 92.30% in Table 2, the CNN has a relative reduction 
of 4.12% in accuracy, whereas to 95.23%, the highest accuracy of S-CNNs is 94.85%, relatively 
reduced by 0.40%. 

Table 10 Test accuracies (%) with average pooling on the AR dataset. 
Accuracy 
88.50 
93.66 
93.48 
92.90 
92.88 
92.12 
93.08 
93.27 

Accuracy 
93.66 
93.67 
93.13 
94.42 
94.45 
94.46 
93.48 
92.13 

Accuracy 
93.27 
93.44 
93.48 
93.67 
93.67 
94.45 
94.85 
93.31 

SI 
10010 
10001 
01100 
01010 
01001 
00110 
00101 
00011 

SI 
11100 
11010 
11001 
10110 
10101 
10011 
01110 
01101 

SI 
01011 
00111 
11110 
11101 
11011 
10111 
01111 
11111 

Accuracy 
93.68 
94.45 
93.86 
93.85 
92.73 
93.28 
94.24 
94.05 

SI 
00000 
10000 
01000 
00100 
00010 
00001 
11000 
10100 

In  Table  11,  we  describe  their  performance  with  ReLU  replaced  by  sigmoid.  With  a  relative 
reduction of 26.68%, the accuracy of the CNN decreases from 92.30% to 67.67%, whereas the 
highest accuracy of S-CNNs drops from 95.23% to 94.85%, relatively reduced by 0.40%. 

Table 11 Test accuracies (%) with sigmoid function on the AR dataset. 
Accuracy 
67.67 
93.68 
93.00 
88.10 
67.93 
67.67 
94.26 
93.70 

Accuracy 
94.45 
94.41 
94.22 
93.67 
93.70 
93.70 
94.00 
91.78 

Accuracy 
93.65 
93.64 
94.06 
94.00 
92.00 
85.60 
90.23 
69.29 

SI 
10010 
10001 
01100 
01010 
01001 
00110 
00101 
00011 

SI 
11100 
11010 
11001 
10110 
10101 
10011 
01110 
01101 

SI 
01011 
00111 
11110 
11101 
11011 
10111 
01111 
11111 

Accuracy 
91.34 
85.05 
94.85 
94.84 
94.27 
94.25 
90.29 
94.06 

SI 
00000 
10000 
01000 
00100 
00010 
00001 
11000 
10100 

In Table 12, we depict their performance with a different initialization of Msra (He et al, 2015) 
from Xavier. It can be seen that the CNN has a relative 0.88% reduction, from 92.30% to 91.49%. 
However, S-CNNs has a relative 0.64% reduction in terms of the highest accuracy, from 95.23% 
to 94.62%. 

Table 12 Test accuracies (%) with Msra initialization on the AR dataset. 
Accuracy 
91.49 
93.47 
94.04 
92.53 
91.97 
91.90 
93.67 
94.62 

Accuracy 
94.35 
94.24 
94.24 
94.24 
92.70 
92.52 
93.69 
93.49 

Accuracy 
94.25 
94.21 
93.86 
93.73 
93.66 
93.56 
94.02 
92.91 

SI 
10010 
10001 
01100 
01010 
01001 
00110 
00101 
00011 

SI 
11100 
11010 
11001 
10110 
10101 
10011 
01110 
01101 

SI 
01011 
00111 
11110 
11101 
11011 
10111 
01111 
11111 

Accuracy 
92.32 
93.49 
94.06 
94.05 
93.86 
92.90 
92.13 
93.85 

SI 
00000 
10000 
01000 
00100 
00010 
00001 
11000 
10100 

In Table 13, we delineate their performance with the Adam (Kingma and Ba, 2015) optimization. 
We can clearly see that, the CNN achieves the accuracy of 91.35%, and the best S-CNN 94.84%. 
They have a relative reduction of 1.03% and 0.41% in comparison with 92.30% and 95.23%, 
respectively. 

Table 13 Test accuracies (%) with Adam optimization on the AR dataset 
Accuracy 
91.35 
93.37 
93.88 
93.47 
93.45 
93.27 
93.85 
94.23 

Accuracy 
94.23 
93.66 
93.47 
93.09 
92.34 
94.06 
93.86 
91.53 

Accuracy 
94.04 
93.65 
93.09 
93.09 
93.03 
92.89 
94.07 
93.86 

SI 
10010 
10001 
01100 
01010 
01001 
00110 
00101 
00011 

SI 
11100 
11010 
11001 
10110 
10101 
10011 
01110 
01101 

SI 
01011 
00111 
11110 
11101 
11011 
10111 
01111 
11111 

Accuracy 
94.84 
92.72 
94.80 
94.43 
94.04 
92.70 
92.33 
92.52 

SI 
00000 
10000 
01000 
00100 
00010 
00001 
11000 
10100 

Overall, on the AR dataset the S-CNNs have a smaller accuracy reduction than the CNN with 
different settings, especially of the sigmoid function. This indicates that the S-CNNs have more 
stable performance than the CNN in general. 

Table 14 Test accuracies (%) with different number of convolutional kernels on the CUReT dataset. 

SI 
00000 
10000 
01000 
00100 
00010 
00001 
11000 
10100 

Accuracy 
61.71 
62.52 
73.69 
73.70 
73.72 
68.11 
62.37 
67.94 

SI 
10010 
10001 
01100 
01010 
01001 
00110 
00101 
00011 

Accuracy 
68.02 
68.65 
75.22 
71.99 
70.22 
75.05 
74.61 
74.69 

SI 
11100 
11010 
11001 
10110 
10101 
10011 
01110 
01101 

Accuracy 
72.33 
66.81 
68.32 
73.87 
75.07 
74.13 
74.94 
78.73 

SI 
01011 
00111 
11110 
11101 
11011 
10111 
01111 
11111 

Accuracy 
77.89 
72.34 
72.35 
72.92 
74.88 
74.88 
74.95 
72.90 

4.6 DIFFERENT NUMBERS AND SIZES OF CONVOLUTONAL KERNELS 

Finally, we examine performance of the CNN and S-CNNs with different numbers and sizes of 
convolutional kernels on the CUReT dataset. 

In Table 14, we report the results using the same architecture except with 10 kernels for each of 
the  three  convolutional  layers  in  Table  1.  It  can  be  seen  that,  the  CNN  gets  the  accuracy  of 
61.71%, and the best S-CNN 78.73%. They have a relative accuracy reduction of 6.74% and 
0.34%, respectively compared to 66.17% and 79.00% in Table 6. 

Table 15 Test accuracies (%) of the CNN and S-CNNs with different size of   

convolutional kernels on the CUReT dataset 

SI 
00000 
10000 
01000 
00100 
00010 
00001 
11000 
10100 

Accuracy 
64.97 
67.02 
71.92 
71.34 
69.89 
70.52 
70.78 
72.20 

SI 
10010 
10001 
01100 
01010 
01001 
00110 
00101 
00011 

Accuracy 
72.34 
72.76 
74.28 
78.05 
75.25 
76.41 
77.71 
76.80 

SI 
11100 
11010 
11001 
10110 
10101 
10011 
01110 
01101 

Accuracy 
72.44 
68.37 
71.11 
71.47 
73.59 
75.96 
73.02 
77.08 

SI 
01011 
00111 
11110 
11101 
11011 
10111 
01111 
11111 

Accuracy 
76.06 
78.77 
72.00 
71.90 
69.53 
73.24 
76.54 
75.08 

In Table 15, we report the results using the same architecture except with kernel sizes of 

, 

77 and 

 for the three convolutional layers in Table 1. It can be seen that, the CNN gets 
the accuracy of 64.97%, and the best S-CNN 78.77%. They have a relative accuracy reduction 
of 1.81% and 0.29%, respectively. 

Therefore, with different number and size of convolutional kernels, the S-CNNs have a relatively 
smaller  reduction  of  performance  than  the  CNN  on  the  CUReT  dataset,  indicating  their  less 
insensitiveness. 

5 CONCLUSIONS 

In this paper, we have presented a concatenating framework of S-CNNs, which can integrate 
multi-scale features through shortcut connections in a CNN. Also, we compare performance of 
the  CNN  and  S-CNNs  on  four  different  tasks,  including  gender  classification,  texture 
classification,  digit  recognition,  and  object  recognition.  Based  on  extensive  experiments,  we 
show that the S-CNNs can produce higher accuracies than the CNN in these tasks, especially in 
texture  classification  and  gender  classification.  Moreover,  the  S-CNNs  have  more  stable 
performance  than  the  CNN  with  different  settings  of  pooling  schemes,  activation  functions, 
initializations,  and  optimizations.  Additionally,  the  S-CNNs  are  less  insensitive  to  kernel 
numbers and kernel sizes. Therefore, we conclude that the S-CNNs can improve performance of 
the CNN by integrating multi-scale features in the proposed concatenating framework, although 
the  S-CNNs  may  have  a  performance  reduction  in  case  of  concatenating  too  many  features, 
which are likely to contain much redundant information and even to make training very difficult. 

It should be noted that the best shortcut style is dataset-dependent. Different datasets may have 
different best shortcut styles. The shortcut style that all hidden layers are concatenated to the 
fully-connected layer cannot always guarantee the best performance, which was advocated in 
DenseNets (Huang et al, 2017). As future work, we will study the problem of how to dynamically 
determine the best shortcut style for a dataset in theory and practice. 

ACKNOELWDGMENTS 

This work was supported in part by the National Natural Science Foundation of China under 
Grant  61175004,  the  Natural  Science  Foundation  of  Beijing  under  grant  4112009,  the 
Specialized Research Fund for the Doctoral Program of Higher Education of China under Grant 
20121102110029,  the  China  Postdoctoral  Science  Foundation  funded  project  under  Grant 
2015M580952, and the project supported by Beijing Postdoctoral Research Foundation under 
Grant 2016ZZ-24. 

REFERENCES 

Taigman, Y., Yang, M., Ranzato, M. A., and Wolf, L. (2014) Deepface: closing the gap to 
human-level performance in face verification. In the International Conference on Computer 
Vision and Pattern Recognition, 1701-1708. 

Lopes,  A.  T.,  Agular,  E.,  Souze  A.  F.  D.,  and  Santos  T.  O.  (2017)  Facial  expression 
recognition  with  convolutional  neural  networks:  coping  with  few  data  and  the  training 

4422sample order. Pattern Recognition, 61(1):610-628. 

Sun, Y., Wang, X., and Tang, X. (2016) Hybrid deep learning for face verification. IEEE 
Transactions of Pattern Analysis and Machine Intelligence, 38(10): 1997-2009. 

Girshick,  R.,  Donahue,  J.,  Darrell,  T.,  and  Malik,  J.  (2014)  Rich  feature  hierarchies  for 
accurate  object  detection  and  semantic  segmentation.  In  the  International  Conference  on 
Computer Vision and Pattern Recognition, 580-587. 

Zhang,  X.,  Zou,  J.,  He,  K.,  and  Sun,  J.  (2016)  Accelerating  very  deep  convolutional 
networks  for  classification  and  detection.  IEEE  Transactions  of  Pattern  Analysis  and 
Machine Intelligence, 38(10): 1943-1955. 

Sun, Y., Wang, X., Tang, X. (2013) Deep Convolutional Network Cascade for Facial Point 
Detection. In the International Conference on Computer Vision and Pattern Recognition, 
3476-3483. 

Jonathan, L. L., Zhang, N., and Darrell, T. (2014) Do convnets learn correspondence? In 
the International Conference on Neural Information Processing Systems, 1601-1609. 

Simonyan, K. and Zisserman, A. (2014) Very deep convolutional networks for large-scale 
image recognition. arXiv preprint arXiv:1409.1556. 

Deng, J., Dong, W., Socher, R., Li, L. J., Li, K., and Li, F. F. (2009) Imagenet: A large-
scale hierarchical image database. In the International Conference on Computer Vision and 
Pattern Recognition, 248–255. 

Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Vanhoucke, 
V.,  and  Rabinovich,  A.  (2015)  Going  deeper  with  convolutions.  In  the  International 
Conference on Computer Vision and Pattern Recognition, 1-9. 

Fukushima,  K.  (1979)  Neural  network  model  for  a  mechanism  of  pattern  recognition 
unaffected by shift in position—Neocognitron. Transactions of the IECE, J62-A (10): 658–
665. 

LeCun, Y., Bottou, L., Bengio, Y., and Haffiner, P. (1998) Gradient based learning applied 
to document recognition. (1998) Proceedings of IEEE, 86(11) 2278-2324. 

Krizhevsky, A., Sutskever, I., and Hinton, G. E. (2012) Imagenet classification with deep 
convolutional neural networks, in Proceedings of the International Conference on  Neural 
Information Processing Systems, 1097-1105. 

Simonyan,  K.,  Zisserman,  A.  (2014)  Very  deep  convolutional  networks  for  large-scale 
image recognition. In the International Conference on Learning Representation. 

Jin, X., Yuan, X., Feng, J., and Yan, S. (2016) Training skinny deep neural networks with 
iterative hard thresholding methods. arXiv: 1607.05423. 

Donoho,  D.  L.,  Huo  X.  (2001)  Beamlets  and  Multiscale  image  analysis.  Multiscale  and 
Multiresolution Methods. Springer Berlin Heidelberg, 149-196. 

Sermanet, P., Kavukcuoglu, K., Chintala, S., LeCun, Y. (2013) Pedestrian detection with 
unsupervised  multi-stage  feature  learning.  In  the  International  Conference  on  Pattern 

Recognition, 3626-3633. 

He, K., Zhang, X., Ren, S. (2016) Deep Residual Learning for Image Recognition. In the 
International Conference on Computer Vision and Pattern Recognition, 3626-3633. 

Huang, G., Liu, Z., Weinberger, K. Q. (2017) Densely connected convolutional networks. 
In the International Conference on Computer Vision and Pattern Recognition, 4700-4708. 

Sermanet,  P.,  LeCun,  Y.  (2011)  Traffic  sign  recognition  with  multi-scale  convolutional 
networks. In the International Joint Conference on Neural Networks, 2809-2813. 

Sun, Y., Wang, X., and Tang, X. (2014) Deep learning face representation from predicting 
10000 classes. In the International Conference on Computer Vision Pattern Recognition, 
1891-1898. 

Srivastava, R. K., Greff, K., Schmidhuber, J. (2015) Highway networks. In the International 
Conference on Machine Learning. 

Andrearczyk,  V.,  and  Whelan,  P.  F.  (2016)  Using  filter  banks  in  convolutional  neural 
networks for texture classification. Pattern Recognition Letters, 84(1): 63-69. 

Shen, W., Zhou, M., Yang, F., Yu, D., Dong, D., Yang, C., Zang, Y., and Tian J. (2017) 
Multi-crop  convolutional  neural  networks  for  lung  nodule  malignancy  suspiciousness 
classification. Pattern Recognition, 61(1): 663-673. 

Liu, W., Anguelov, D., Erhan, D., Szegedy, C., Reed, S., Fu, C., and Berg, A. C. (2016) 
SSD: single shot multibox detector. arXiv: 1512. 02325. 

Ni,  Z.,  He,  H.,  Wen,  J.,  and  Xu,  X.  (2013)  Goal  representation  heuristic  dynamic 
programming on maze navigation. IEEE Transactions on Neural Networks and Learning 
Systems, 24(12): 2038-2050. 

Jia, Y. (2013) Caffe: an open source convolutional architecture for fast feature embedding. 
http:// caffe. Berkeleyvision.org. 

Glorot, X., Bengio, Y. (2010) Understanding the difficulity of training deep feedforward 
neural networks. In  the International Conference on Artificial Intelligence and Statistics, 
249-256. 

Maetinez,  A.  M.  (2001)  PCA  versus  LDA.  IEEE  Transactions  on  Pattern  Analysis  and 
Machine Intelligence, 23(2): 228-233. 

Phillips, P. J., Wechsler, H., Huang, J., and Rauss P. J. (1998) The FERET database and 
evaluation procedure for face-recognition algorithms. Imag. Visi. Comp., 16 (5): 295-306. 

Ng, H. W., Winkle, S. (2014) A data-driven approach to cleaning large face datasets. In the 
International Conference on Image Processing, 27-30. 

Yang,  S.,  Luo,  P.,  Loy,  C.  C.,  and  Tang,  X.  (2015)  From  facial  part  responses  to  face 
detection: a deep learning approach. In the International Conference on Computer Vision, 
3676-3684. 

Dana, K. J., Ginneken, C. B., Nayar, S. K., and Koenderink J. J. (1999) Reflectance and 

texture of real world surfaces. ACM Transactions on Graphics, 18(1): 1-34. 

LeCun, Y., Bottou, L., Bengio, Y., and Haffiner, P. (1998) Gradient based learning applied 
to document recognition. Proceedings of IEEE, 86(11): 2278-2324. 

Yu, Q., Tang, H., Tan, K. C., and Li H. (2013) Rapid feedforward computation by temporal 
encoding and learning with spiking neurons. IEEE Transactions on Neural Networks and 
Learning System, 24(10): 1539-1552. 

Krizhevsky, A. (2012) Learinng multiple layers of features from tiny images. 

LeCun, Y., Boser, B., Denker, J. S., Henderson, D., Howard, R. E., Hubbard, W., and Jackel, 
L.  D.  (1990)  Handwritten  digit  recognition  with  a  back-propagation  network.  In  the 
International Conference on Neural Information Processing System. 

He,  K.,  Zhang,  X.,  Ren,  S.,  and  Sun,  J.  (2015)  Delving  deep  into  rectifiers:  surpassing 
human-level performance on ImageNet classification. In the International Conference on 
Computer Vision, 1026-1034. 

Kingma,  D.  P.,  and  Ba  J.  L.  (2015)  Adam:  a  method  for  stochastic  optimization.  In  the 
International Conference on Learning Representation. 

