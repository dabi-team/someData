0
2
0
2

n
a
J

8
1

]

A
N
.
h
t
a
m

[

1
v
3
2
6
6
0
.
1
0
0
2
:
v
i
X
r
a

VERIFIED COMPUTATION OF MATRIX GAMMA FUNCTION ∗

SHINYA MIYAJIMA†

Abstract. Two numerical algorithms are proposed for computing an interval matrix containing
the matrix gamma function. In 2014, the author presented algorithms for enclosing all the eigenvalues
and basis of invariant subspaces of A ∈ Cn×n. As byproducts of these algorithms, we can obtain
interval matrices containing small matrices whose spectrums are included in that of A. In this paper,
we interpret the interval matrices containing the basis and small matrices as a result of veriﬁed
block diagonalization (VBD), and establish a new framework for enclosing matrix functions using
the VBD. To achieve enclosure for the gamma function of the small matrices, we derive computable
perturbation bounds. We can apply these bounds if input matrices satisfy conditions. We incorporate
matrix argument reductions (ARs) to force the input matrices to satisfy the conditions, and develop
theories for accelerating the ARs. The ﬁrst algorithm uses the VBD based on a numerical spectral
decomposition, and involves only cubic complexity under an assumption. The second algorithm
adopts the VBD based on a numerical Jordan decomposition, and is applicable even for defective
matrices. Numerical results show eﬃciency and robustness of the algorithms.

Key words. matrix gamma function, veriﬁed block diagonalization, veriﬁed numerical compu-

tation

AMS subject classiﬁcations. 15A16, 65F60, 65G20

1. Introduction. For z

deﬁned by

∈

C with positive real part, the gamma function is

Γ(z) :=

∞

0
Z

e−ttz−1dt,

and otherwise by analytic continuation. It is well known that Γ(z) is analytic every-
where in C, with the exception of non-positive integer numbers Z−. Therefore, the
general theory of primary matrix function [4] ensures that the matrix gamma function
Cn×n having no eigenvalues on Z−. If all eigenvalues of
Γ(A) is well deﬁned for A
A have positive real parts, then we have the representation

∈

(1.1)

Γ(A) =

∞

0
Z

e−ttA−In dt,

×

where tA−In := e(A−In) log(t) and In denotes the n

n identity matrix.

The function Γ(A) has connections with other special functions, which play an
important role in solving certain matrix diﬀerential equations [2]. Two of these special
functions are the matrix beta and Bessel functions. In [2], mathematical properties
of Γ(A) are elegantly clariﬁed, and fast and accurate algorithms for computing Γ(A)
are proposed.

The work presented in this paper addresses the problem of veriﬁed computations
for Γ(A), speciﬁcally, numerically computing interval matrices which are guaranteed
to contain Γ(A). To the author’s best knowledge, a veriﬁcation algorithm designed
speciﬁcally for Γ(A) does not yet appear in the literature. A possible method is to
use the VERSOFT [11] routine vermatfun. This routine is applicable not only to the
matrix gamma function but also to other matrix functions, and computes the interval

∗This work was partially supported by JSPS KAKENHI Grant Number JP16K05270.
†Faculty
020-8551,
Engineering,

University,

Iwate

and

of

Science
(miyajima@iwate-u.ac.jp).

Japan

1

 
 
 
 
 
 
2

SHINYA MIYAJIMA

O

matrices by enclosing all the eigenvalues and eigenvectors of A via the INTLAB [12]
routine verifyeig. This routine fails when A is defective or close to defective, and
requires

(n4) operations.

The purpose of this paper is to propose two veriﬁcation algorithms for Γ(A). In
[6], algorithms for enclosing all the eigenvalues and basis of invariant subspaces of A
are presented. As byproducts of these algorithms, we can obtain interval matrices
containing small matrices whose spectrums are included in that of A. In this paper,
we interpret the interval matrices containing the basis and small matrices as a result
of veriﬁed block diagonalization (VBD), and establish a new framework for enclosing
matrix functions using the VBD. To achieve enclosure for the gamma function of the
small matrices, we derive computable perturbation bounds. Here, the word “com-
putable” means that we can numerically obtain a rigorous upper bound which takes
rounding and truncation errors into account. We can ﬁnd a perturbation bound for
Γ(A) also in [2]. On the other hand, the bound in [2] is not a computable one. We can
apply the derived perturbation bounds if input matrices satisfy conditions. We incor-
porate matrix argument reductions (ARs) to force the input matrices to satisfy the
conditions, and develop theories for accelerating the ARs. The ﬁrst algorithm uses the
(n3)
VBD based on a numerical spectral decomposition (NSD), and involves only
operations under an assumption. The second algorithm adopts the VBD based on a
numerical Jordan decomposition (NJD), and is applicable even when A is defective.
We present a theory for verifying that A has no eigenvalues on Z−. By the aid of this
theory, these algorithms do not assume but prove that A has no eigenvalues on Z−.
The ﬁrst and second algorithms require intervals containing Γ(0)(z)/0!, . . . , Γ(ℓ)(z)/ℓ!,
C. To the author’s best knowledge, an al-
where ℓ is a non-negative integer and z
gorithm for computing such intervals is not available in literature, whereas there are
well-established algorithms [5, 13, 15] for computing intervals containing real scalar
gamma functions. We thus present a way for computing such intervals, which is based
on the Spouge approximation [14]. Although this way may be a slight modiﬁcation of
the Spouge method, the proposed algorithms are the ﬁrst ones which apply the VBD
to computation of an interval containing a matrix function. One may consider that
the VBD is a direct application of the algorithms in [6]. However, the established
framework enables us to enclose not only Γ(A) but also other matrix functions (see
Section 7). Moreover, utilizing the VBD as a means to enclose a matrix function, veri-
fying that A has no eigenvalues on Z−, deriving the computable perturbation bounds,
and accelerating of the ARs are the ﬁrst attempts and not obvious.

O

∈

The author has been proposed many veriﬁcation algorithms for matrix functions
(e.g., [7, 8, 9, 10]). However, the idea in this paper does not overlap with those in the
previous papers. This is because most of the previous algorithms are based on matrix
equations, whereas the algorithms in this paper are not. Although the algorithms
in [8] are not based on matrix equations and also utilize the NSD or NJD, these
algorithms do not use the VBD, which is the key idea in this paper.

This paper is organized as follows: Section 2 introduces notation and theories
used in this paper. Section 3 presents a way for computing the intervals containing
Γ(0)(z)/0!, . . . , Γ(ℓ)(z)/ℓ!. Sections 4 and 5 propose the ﬁrst and second algorithms,
respectively. Section 6 reports numerical results. Section 7 ﬁnally summarizes the
results in this paper and highlights possible extension and future work.

2. Preliminaries. For M

element, j-th column, spectral radius and spectrum of M , respectively, and
Mij|
(
|

Cn, denote the i-th element of v by vi. For M, N

Cn×n, let Mij, M:j, ρ(M ) and µ(M ) be the (i, j)
:=
M
|
Rm×n, the

). For v

∈

∈

∈

|

VERIFIED COMPUTATION OF MATRIX GAMMA FUNCTION

3

z

0

}

∈
∈

Z : z

≥
×

1, Z+ :=

Nij,
z
{
+ :=

n matrices, respectively. For C

≤
Z : z > 0
C : Re(z) > 0

−
, R+ := [0,
∞
, and Rm×n
0

N means Mij ≤
, Z− :=
}
, Rn

i, j. Let i := √
∀
Z : z
∈
v
{

inequality M
Z++ :=
C++ :=
M
0
Rm×n
m
+
matrix whose midpoint and radius are C and R, respectively, by
matrices contained in M
∈
M −1 : M
matrix including
∈
interval arithmetic. Let A, B
∈
use the following property of interval arithmetic (see [1], e.g.):

,
≥
}
), R++ := (0,
),
z
∞
≤
{
Rm×n :
Rn : v
M
:=
z
{
. Let also IC and ICm×n be the sets of all complex interval scalars and
}
Cm×n and R
, denote the interval
∈
. Suppose any
i
ICn×n is nonsingular. Then, M −1 denotes an interval
. Expressions containing intervals mean results of
+ . In Sections 4 and 5, we will

M
Cn×n and R, S

C, R
h

Rn×n

}
≥

∈

∈

∈

∈

∈

{

}

0

{

}

{

+

(2.1)

A, R
h

B, S

ih

AB,

i ⊆ h

S + R

A
|

|

B

|

|

+ RS

.
i

For α
Sections 4 and 5, we will use the incomplete gamma function

R, let

α
⌉

α
⌋

and

denote the ceiling and ﬂoor functions, respectively. In

∈

⌈

⌊

γ(α) :=

1

0
Z

e−ttα−1dt, where α

R++.

∈

For z

∈

C, let log(z) be the principal branch of the logarithm. Deﬁne

1
...
1

∈






1lv
n := 




Rn, 1lM

n := 




1
...
1

· · ·

· · ·

1
...
1

∈






Rn×n and Nj :=

0

1

0

. . .
. . .

















1
0

Rnj ×nj .

∈

In Section 3, we will use the Spouge approximation [14] and its error bound, which

are summarized in Lemma 2.1.

Lemma 2.1 (Spouge [14]). Let a

R++ and z, w

∈

∈

C. Deﬁne c0 := 1,

ck :=

1
√2π

H(z) := c0 +

(
−
(k

1)k−1
1)!

−
⌈a⌉−1

k + a)k−1/2e−k+a,

k = 1, 2, . . . ,

(
−

a

⌈

⌉ −

1,

ck
1 + k

z

, K(z) := √2π(z

√2π(

G(w) :=

ǫ(z) :=

i
2π

Xk=1
w
−
−
(
−

∞

−
a)−w−1/2ew+a
w
−
G(
−
−
e2π(v−ia)

iv)
1

1)!
a

+

,

0 (cid:18)
Z

−

G(
−
e2π(v+ia)

a + iv)
1

−

1 + a)z−1/2e−(z−1+a),

−

dv

a + iv

−

(cid:19)

−

.

z + 1

3 and Re(z

Assume a
(a) Γ(z) = K(z)(H(z) + ǫ(z));
(b) for m

≥

−

∈

1 + a) > 0. Then,

Z+, the m-th derivative of the error term ǫ(z) is bounded by

ǫ(m)(z)

| ≤

|

(Re(z

m!Ca
1 + a))m+1 , where Ca :=

−

(c) Ca <

ae/π(2π)−(a+1/2).

p

2/π
1)!

(a
p

−

∞

va−1/2

0

Z

e2πv

|

−

e2πia

|

dv;

4

SHINYA MIYAJIMA

Remark 2.2. We can obtain Lemma 2.1 (c) from [14, Proof of Theorem 1.3.1].

From Lemma 2.1 (b) and (c), we immediately obtain Corollary 2.3.

Corollary 2.3. Let m, a, z and ǫ(z) be as in Lemma 2.1.

If a

Re(z

−

1 + a) > 0, then

ǫ(m)(z)
|

|

< ξm(z), where

3 and

≥

ξm(z) :=

(Re(z

−

ae/π

m!
1 + a))m+1(2π)a+1/2 .

p

In Sections 4 and 5, we will use the following properties of matrix functions:
Lemma 2.4 (e.g., Higham [4]). Let A, X, Y

Cn×n and ϕ be deﬁned on the

spectrum of A. Then,
(a) if X is nonsingular, then ϕ(XAX −1) = Xϕ(A)X −1;
(b) if A = diag(A1, . . . , Ap) is block diagonal, then ϕ(A) = diag(ϕ(A1), . . . , ϕ(Ap));
(c) if XY = Y X, then eX+Y = eXeY = eY eX ;
Y
(d)

eX
k
We cite Lemma 2.5 as a theoretical basis for the ARs in Sections 4.4 and 5.4.
Lemma 2.5 (Cardoso and Sadeghi [2]). Let A

emax(kXk,kY k) for any consistent norm.

Cn×n have no eigenvalues on

k ≤ k

eY

X

−

−

k

∈

∈

Z−. Then, Γ(A + In) = AΓ(A).
R++.

Let α

γ(1)(α). To this end, we present Lemma 2.6.

∈

In Sections 4.3 and 5.3, we will estimate an upper bound for

Lemma 2.6. Let α

R++ and ω(α) :=

∈

2α + 1
α2(α + 1)2 +

cosh(1)

−
(α + 2)2

1

. Then,

γ(1)(α) < ω(α).

−

−

Proof. From γ(1)(α) =

1

0 e−ttα−1 log(t)dt and integration by parts, we obtain
R
∞

t=1

∞

1

1)itα+i
(
−
i!(α + i) #

+

t=0

0  

Z

i=0
X

1)itα+i−1
(
−
i!(α + i) !

dt

γ(1)(α) =

−

=

=

(2.2)

− "
∞

log(t)

i=0
X
1)i

(
−
i!(α + i)2

i=0
X
1
α2 −

1
(α + 1)2 +

∞

i=1 (cid:18)
X

1

(2i)!(α + 2i)2 −

1
(2i + 1)!(α + 2i + 1)2

.

(cid:19)

For i = 1, 2, . . . , it follows that

1

(2i)!(α + 2i)2 −

1
(2i + 1)!(α + 2i + 1)2 =

2i
(2i + 1)!(α + 2i + 1)2 +

2(α + 2i) + 1
(2i)!(α + 2i)2(α + 2i + 1)2

<

=

1
(2i)!(α + 3)2 +
1
(2i)!(α + 2)2 .

2(α + 2) + 1
(2i)!(α + 2)2(α + 3)2

This and (2.2) prove the inequality.

3. Enclosing Γ(0)(z)/0!, . . . , Γ(ℓ)(z)/ℓ!. As mentioned in Section 1, we need
Z+. To

to compute intervals containing Γ(0)(z)/0!, . . . , Γ(ℓ)(z)/ℓ! for z
this end, we use Lemma 2.1 and Corollary 2.3.

C and ℓ

∈

∈

Remark 3.1. There are many other methods for computing an approximation
of Γ(z) (see [2], e.g.). By exploiting these methods, computing an interval containing

VERIFIED COMPUTATION OF MATRIX GAMMA FUNCTION

5

Γ(z) seems to be possible. On the other hand, error bounds regarding to the derivatives
of Γ(z) are explicitly written in [14]. Therefore, the Spouge method is useful for our
purpose.

Let a, ck, H(z), K(z) and ǫ(z) be as in Lemma 2.1, and ξm(z) be as in Corol-
1 + a) > 0. From Lemma 2.1, Corollary 2.3, and

3 and Re(z

lary 2.3. Suppose a
≥
the Leibniz rule, for m = 0, . . . , ℓ, we have

−

Γ(m)(z)
m!

=

∈

⊆

m

Xk=0
m

Xk=0
m

Xk=0

K (k)(z)
k!

K (k)(z)
k!

K (k)(z)
k!

(cid:18)

(cid:28)

(cid:28)

(3.1)

H (m−k)(z)
k)!
(m

−
H (m−k)(z)
k)!
(m

−
H (m−k)(z)
k)!
(m

−

+

, |

ǫ(m−k)(z)
k)!
(m

−
ǫ(m−k)(z)
|
k)!
(m

−
ξm−k(z)
k)!
(m

,

−

(cid:29)

(cid:19)

(cid:29)

.

We thus enclose H (k)(z)/k! and K (k)(z)/k! for k = 0, . . . , ℓ. For large k, on the other
hand, explicit representations for K (k)(z)/k! seems to be complicated. For enclosing
K (k)(z)/k! without using the explicit representations, we propose the following way:
1 + a). Then, K (1)(z) = K(z)P (z), so that
Let P (z) := log(z

1/2)/(z

1 + a)

(a

−

−

−

−

(3.2)

K (k+1)(z)
(k + 1)!

=

(K(z)P (z))(k)
(k + 1)!

=

1
k + 1

k

j=0
X

K (j)(z)
j!

P (k−j)(z)
j)!
(k

−

,

k = 0, . . . , ℓ

1.

−

Hence, we can enclose K (k+1)(z)/(k + 1)! if enclosures for K (0)(z)/0!, . . . , K (k)(z)/k!
have already been obtained. Observe that we can easily write down H (j)(z)/j! and
P (j)(z)/j! explicitly. For j = 1, . . . , ℓ, in fact,

(3.3)

(3.4)

H (j)(z)
j!

P (j)(z)
j!

=

=

⌈a⌉−1

1)jck
(
1 + k)j+1 ,
−

(z

−
1)j−1
1 + a)j +

Xk=1
(
−
j(z
−

1)j−1(a
(
−
(z

−
1 + a)j+1

1/2)

−

.

We summarize our approach in Algorithm 3.2.

Algorithm 3.2. Let a

Z+. Assume Re(z
This algorithm computes intervals containing Γ(0)(z)/0!, . . . , Γ(ℓ)(z)/ℓ!.
Step 1. Enclose H (j)(z)/j! and P (j)(z)/j! for j = 0, . . . , ℓ based on (3.3) and (3.4),

3 be given and ℓ

1 + a) > 0.

−

≥

∈

respectively.

Step 2. Compute intervals including K (j)(z)/j! for j = 0, . . . , ℓ based on (3.2).
Step 3. Enclose Γ(j)(z)/j! for j = 0, . . . , ℓ based on (3.1).

Step 1 involves

ℓ) operations. Steps 2 and 3 require

Therefore, Algorithm 3.2 involves

ℓ + ℓ2) operations.

a

(
⌈

⌉

O

(ℓ2) operations.

O

For executing Algorithm 3.2, we need to determine a. From the assumption in
3. If we take a too small, then ξ0(z) does not
Corollary 2.3, we focus on the case a
become small. If we take a too large, on the other hand, many interval arithmetics are
required for computing an interval containing H(z), which causes enlargement of the
Z+ is not too
radius of the interval. If we take a in the form of a = b + 1/2, where b
large, then rounding errors do not occur in the ﬂoating point computations of a + 1/2
1/2. Based on these observations, we propose incrementing a by one from
and a

≥

∈

a

(
⌈

⌉

O

−

6

SHINYA MIYAJIMA

7/2, and terminating the increment when the radius exceeds ξ0(z). We summarize
this strategy in Algorithm 3.3.

Algorithm 3.3. Assume Re(z) >

5/2. This algorithm determines a in Algo-

rithm 3.2.
Step 1. Initialize a as a = 7/2.
Step 2. Compute intervals containing c1, . . . , c⌈a⌉−1 and H(z).
Step 3. If the radius of the interval containing H(z) exceeds ξ0(z), then output the

current a and terminate. Otherwise, go to Step 4.

Step 4. Update a such that a = a + 1 and go back to Step 2.

Note that c1, . . . , c⌈a⌉−1 are computed whenever a is incremented. Algorithm 3.3

−

thus requires

) operations per iteration.

a

(
⌈

⌉

O

By slightly modifying Algorithms 3.2 and 3.3, we can compute intervals containing
5/2.

Γ(j)(z)/j! : z
{
To be speciﬁc, by replacing z and ξm(z) in these algorithms by z and

for j = 0, . . . , ℓ, where z =:

IC satisﬁes Re(c)

c, r
h

r >

i ∈

−

−

∈

z

}

(Re(c)

r

−

ae/π

m!
1 + a)m+1(2π)a+1/2 ,
p

−

respectively, we can obtain such intervals.

4. Algorithm based on the NSD. We develop our algorithm in some steps.
Section 4.1 introduces the VBD based on the NSD, and framework using the VBD.
Section 4.2 develops the theory for verifying µ(A)
. Section 4.3 establishes
the computable perturbation bound for enclosing the gamma function of a diagonal
block. Section 4.4 explains the ARs, and presents the theory for its acceleration.
Section 4.5 proposes the overall algorithm.

Z− =

∩

∅

1

∈

≈

· · ·

1 , . . . , i(1)
< i(q)

p1 , . . . , i(q)
pq = n and p1 +

Cn×n with Λ = diag(λ1, . . . , λn) such that AX

λi(j)
, . . . , λi(j)
pj }
Z++ satisfy 1 = i(1)

4.1. The VBD based on the NSD. Assume as a result of the NSD of A, we
XΛ. By executing
, j = 1, . . . , q be sets of clusters,

have Λ, X
column permutation if necessary, let
1 , . . . , i(q)
where i(1)
i(q)
1 <
from the others is included in the case pj = 1. Let also Xj := [X:i(j)
Cn×pj and Pj ∈
for j = 1, . . . , q, and Wj ∈
µ(Pj )
⊆
Xj, ∆ji ∋
h
1, . . . , q. As byproducts of this algorithm, actually, we can obtain Rj ∈
λj Ipj , Rji ∋
that
h
X, ∆
and W :=
i ∈
h
b

{
<
pq ∈
+ pq = n. Note that the case where λj is isolated
, . . . , X:i(j)
]
pj
Cpj ×pj satisfy AWj = Wj Pj. Observe
IC such that
and
λj , ̺ji ∈
h
, . . . , λi(j)
being inputs, for j =
b

Rpj ×pj
+
Cn×n, ∆ := [∆1, . . . , ∆q]

Rn×pj
+
µ(Pj ) with Xj and λi(j)

µ(A). Then, [6, Algorithm 1] gives ∆j ∈

Pj. Let W := [W1, . . . , Wq]
ICn×n. Then, W
W and

λj, ̺ji ⊇
h
b

such
Rn×n
+

Wj and

< i(1)

1 <

p1 <

· · ·

· · ·

· · ·

∈

∈

∈

pj

1

1

AW = [AW1, . . . , AWq] = [W1P1, . . . , WqPq] = W diag(P1, . . . , Pq).

We can verify nonsingularity of any matrix contained in W by executing a known
algorithm (e.g., the INTLAB routine verifylss). If the veriﬁcation is succeeded,
then W is also nonsingular, so that A = W diag(P1, . . . , Pq)W −1. Thus, W and
) can be regarded as the result of the VBD. We es-
λ1Ip1 , R1i
diag(
h
tablish the new framework for enclosing matrix functions based on the VBD. Al-
though this paper treats Γ(A) only, this framework enables us to enclose other ma-
b
trix functions (see Section 7). From Lemma 2.4 (a) and (b), we have Γ(A) =

λqIpq , Rqi
h
b

, . . . ,

VERIFIED COMPUTATION OF MATRIX GAMMA FUNCTION

7

W diag(Γ(P1), . . . , Γ(Pq))W −1, so that the problem of enclosing Γ(A) can be reduced
to that of enclosing Γ(P1), . . . , Γ(Pq).

4.2. Veriﬁcation of µ(A) ∩ Z− = ∅. As another result of [6, Algorithm 1],
. We formulate and prove

+ such that µ(A)

Rn

we can obtain r
Theorem 4.1 for verifying µ(A)
Theorem 4.1. Let λ
Cn by

∈

∈

f

∈

Z− =
∩
∅
Cn and r

n
λi, rii
i=1h
⊆
using λi and r.
S
Rn
+ satisfy µ(A)

∈

n
i=1h

λi, rii

. Deﬁne

⊆

S

fi := max(Re(λi),

Re(λi)

⌋ −

⌊

Re(λi), Re(λi)

Re(λi)
⌉

− ⌈

) + Im(λi)i,

i = 1, . . . , n.

.

∅

∅

∩
∅

n
i=1h

fi| −
If mini(
|
Proof. If

Z− =
ri) > 0, then µ(A)
Z− =
, then µ(A)
λi, rii∩
∩
for each i by considering the cases of Re(λi)
≥
Consider ﬁrst the case where Re(λi)
0. Then, minc∈Z− |
λi| −
Re(λi)
⌊
⌋ −
∅
0, we have fi = λi. Therefore,
fi| −
ri > 0.
if
λi, rii ∩
h
Consider next the case where Re(λi) < 0. Then,

S
λi, rii ∩
that
h
Re(λi)
Re(λi)
⌉ ≤
− ⌈
ri > 0. Hence,
λi| −

λi, rii∩
. We thus prove
h
0 and Re(λi) < 0 separately.
, so
c
λi|
=
λi −
0 and
Re(λi)
ri > 0 is equivalent to

ri > 0. Since

follows if

fi| −

Z− =

Z− =

Z− =

Z− =

|
≤

≥

∅

∅

|

|

|

|

|

min
c∈Z− |

λi −

c

|

= min(

|⌊

Re(λi)

⌋ −

,

λi|

λi − ⌈

|

Re(λi)

⌉|

) =: gi,

Z− =
so that
λi, rii ∩
h
gives Re(λi) = Re(λi)
Re(λi)
(
−
and Re(λi) < Re(λi)

∈

∅
− ⌈

1, 0] and Re(λi)

ri > 0. If Re(λi)

follows if gi −
. If Re(λi)
Re(λi)
⌉
Re(λi)
1, 0] yield Re(λi) <
− ⌈
. Therefore, fi can be written as
Re(λi)
⌉

(
⌈
−
1, on the other hand, then
⌊
Re(λi)
⌋ −

1, 0), then

≤ −
(
−

⌉ ∈

∈

⌊

Re(λi)
⌉
Re(λi)

− ⌈

⌋−
Re(λi)

= 0

Re(λi)
⌉

) + Im(λi)i.

fi = max(
⌊

Re(λi)

Re(λi), Re(λi)

)/2

⌋ −

Re(λi)
⌉
. Hence,

Re(λi)
+
⌋
⌈
Re(λi)
⌉

− ⌈
Re(λi), then gi =
λi −⌈
Re(λi)
Re(λi)
Re(λi)
|
⌊
⌋−
+ Im(λi)i
Re(λi)
Re(λi)
fi|
Re(λi)
= gi. If (
⌉
⌋
⌊
and
)/2 > Re(λi), on the other hand, then gi =
Re(λi)
λi|
Re(λi)
⌋ −
⌊
Re(λi)+Im(λi)i
= gi. Therefore,
. Thus,
|

Re(λi)
|⌊
λi, rii ∩
fi| −
h
Remark 4.2. Theorem 4.1 enables us to treat all the cases considered in the

If (
⌊
Re(λi)
− ⌈
Re(λi)
⌉
⌈
Re(λi) > Re(λi)
Re(λi)
=
−⌈
⌉
ri > 0, so that
ri > 0, then gi −
if

⌋−
Z− =

|
⌋ −

≤
|

fi|

and

− ⌈

≤
+

=

⌉|

|⌊

∅

|

|

|

.

proof uniformly.

If λ and r are given, then the computation of f requires
(n) operations.

veriﬁcation thus require

O

(n) operations. The

O

4.3. Computable perturbation bound. As mentioned in Section 4.1, the
problem of enclosing Γ(A) is reduced to that of enclosing Γ(P1), . . . , Γ(Pq). For j =
Cpj ×pj satisﬁes
λjIpj + Qj, where Qj ∈
1, . . . , q, moreover, Pj can be written as Pj =
Rj. If pj = 1, then we can enclose Γ(Pj ) by executing the interval variants of
Qj| ≤
|
b
Algorithms 3.2 and 3.3 with
being the input. Otherwise, this approach is not
λj, Rji
h
2, we formulate and prove Theorem 4.3,
possible. In order to enclose Γ(Pj) when pj ≥
b
Z++ ∪ {∞}
kp, where p
.
which gives an upper bound for
Z++ ∪ {∞}
C, Qj ∈
,
Qj| ≤
|
Rjkp) + ω(Re(

Theorem 4.3. Let ω(α) be as in Lemma 2.6, p
b
Rjkp > 0 and
λj ) +

∈
λj ∈
Rj, and deﬁne
b

− k
b
Rjkp) + ω(Re(

Cpj ×pj and Rj ∈

Rjkp(Γ(1)(Re(

. Suppose Re(

λj Ipj + Qj)

Rjkp)).

Rpj ×pj
+

λj Ipj )

λj) +

δp :=

− k

λj )

λj )

Γ(

Γ(

−

∈

b

k

k

k

k

b

b

b

8

SHINYA MIYAJIMA

Then,

Γ(

λjIpj + Qj)

kp < δp.
Remark 4.4. We can compute a rigorous upper bound for Γ(1)(Re(

λjIpj )

Γ(

−

k

λj) +

Rjkp)

k

(0,

). It follows from Lemma 2.4 (b) and (c) that

b

by slightly modifying Algorithms 3.2 and 3.3.

b

b
Proof. Let t

∞

∈
tbλj Ipj −Ipj = elog(t)((bλj −1)Ipj +Qj )

−
= elog(t)(bλj −1)Ipj (elog(t)Qj

−
Rj and Lemma 2.4 (d), moreover, we have

tbλj Ipj +Qj −Ipj

−

(4.1)

From

Qj| ≤

|

elog(t)(bλj −1)Ipj

Ipj ) = tbλj −1(elog(t)Qj

e0).

−

The inequality Re(
λj )
− k
Rj yields
Qj| ≤
assumption
b
λj Ipj + Qj)

µ(

|

(4.2)

elog(t)Qj

k

e0

−

kp ≤ |

log(t)

Qjkpe| log(t)|kQj kp

log(t)

|k

≤ |

|k
Rjkp > 0 gives Re(

λj ) > 0, so that µ(

Rj kpe| log(t)|kRj kp .
λj Ipj ) ( C++. The

λj , ρ(Qj)

⊆ h
i ⊆ h
Rjkp > 0 give µ(

λj , ρ(Rj)

)
i ⊆ h

λj, ρ(
|

.
Rjkpi
λj Ipj + Qj) ( C++. The relations µ(
b

i ⊆ h

λj ,

b

k

λj Ipj ) (

This and Re(
C++, µ(

b
b
λj Ipj + Qj) ( C++, (1.1), (4.1) and (4.2) show

− k

λj )

b

b
Qj|

b

b

Γ(

b
λjIpj + Qj)

k

−

Γ(

λjIpj )

kp =

b

b

=

≤

b

∞

∞

0
Z

0
Z
∞

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
0
Z
Rjkp

|

(4.3)

tbλj Ipj −Ipj )dt

e0)dt

b

p

(cid:13)
(cid:13)
(cid:13)
(cid:13)

−

−

p

(cid:13)
(cid:13)
(cid:13)
kpdt
(cid:13)
e| log(t)|kRj kp dt.

e−t(tbλj Ipj +Qj −Ipj

e−ttbλj −1(elog(t)Qj

e−t

tbλj −1

elog(t)Qj

e0

−

|k

||
∞

e−ttRe(bλj )−1

log(t)
|

|

≤ k

0

Z
e| log(t)|kRj kp dt and

1

I0 :=

log(t)
Let
|
e| log(t)|kRjkp dt. Lemma 2.6 yields

0 e−ttRe(bλj )−1
R

|

(4.4)

From Γ(1)(α) =

(4.5)

∞

−

λj )

− k

γ(1)(Re(

I0 =
0 e−ttα−1 log(t)dt for α
R
I∞ = Γ(1)(Re(
< Γ(1)(Re(

λj ) +

b

k

λj ) +
b

k

∈
Rjkp)
−
Rjkp) + ω(Re(

∞

I∞ :=

1 e−ttRe(bλj )−1
R
Rjkp).
R++, we moreover have

− k

λj )

b

Rjkp) < ω(Re(

log(t)
|

|

γ(1)(Re(

λj) +

Rjkp)
k
Rjkp).

λj ) +
b

k

b

The relations (4.3) to (4.5) prove the inequality.

b
In [2], the estimations

Remark 4.5.

log(t)

t−1 and

log(t)

t for t

|

∈

∞

[1,

| ≤
∈
| ≤
(0, 1] and t
), respectively, are used. By using the derivatives instead of these
t−1, moreover,
estimations, Theorem 4.3 gives a smaller bound. If we use
then the obtained bound will contain an upper bound for γ(Re(
1), and
Rjkp −
the condition Re(
Rjkp > 1 will be required for computing the bound. Therefore,
the use of the derivatives enables us to weaken the condition. On the other hand,
using these estimations in [2] is reasonable. This is because the purpose of using these
estimations in [2] is to clarify not quantitative but qualitative properties of Γ(A).

log(t)
λj )

| ≤
− k

λj )

− k

b

b

|

|

VERIFIED COMPUTATION OF MATRIX GAMMA FUNCTION

9

From Theorem 4.3, we immediately obtain Corollary 4.6.
Corollary 4.6. Let

λj , Qj and δp be as in Theorem 4.3, and δ := min(δ1, δ∞).

Under the assumptions in Theorem 4.3, Γ(

b
Proof. Theorem 4.3 and

Γ(
λj Ipj + Qj)
b

λj Ipj +Qj)
Γ(
Γ(
−
|
Rjkp > 0, then the assumption Re(

λjIpj + Qj)
λj Ipj )
Γ(
−
b
< δ1lM
λj)Ipj |
b
b

Γ(
λj Ipj +Qj)

λj )Ipj , δ1lM
.
pj i
Γ(
−
b
pj , proving the result.
b
−

∈ h
Γ(

λj )Ipj via Algorithms 3.2 and 3.3.

λj ) >

| ≤ k

b

|

b

satisﬁed, so that we can enclose Γ(

for p = 1,

give

If Re(

∞
λj )

− k

b

λjIpj )

kp1lM

pj

5/2 in Algorithm 3.3 is

b

4.4. ARs of diagonal blocks. If Re(

Rjkp > 0 cannot be veriﬁed, then
Theorem 4.3 is not applicable. To overcome this issue, we apply the matrix AR based
.
λj Ipj +Qj)
on Lemma 2.5. If the assumption in Theorem 4.1 is true, then µ(
∩
∅
Z− =
, where Pj is as in
This is because µ(
b
Section 4.1. In this case, for mj ∈
b

∩
Z++, Lemma 2.5 implies

λj Ipj + Qj) = µ(Pj )

µ(A) and µ(A)

Z− =

− k

λj )

⊆

b

∅

b

−1

Γ(

λj Ipj + Qj) =

b

(4.6)

∈  

mj −1

i=0
Y
mj −1

i=0
Y

((

λj + i)Ipj + Qj)

b
λj + i)Ipj , Rji!
(
h
b

!
−1

Γ((

λj + mj)Ipj + Qj)

b

Γ((

λj + mj)Ipj + Qj),

b
λj + i)Ipj , Rji
is nonsingular. If we
provided that any matrix contained in
(
h
Rjkp > 0 can be veriﬁed, so that
λj) + mj − k
appropriately choose mj, then Re(
Q
b
Theorem 4.3 becomes applicable. We can verify nonsingularity of the any matrix,
and enclose (4.6) by executing a known veriﬁcation algorithm.
b

mj −1
i=0

If Re(

λj ) +

Rjkp ≫

k

1, then the term Γ(1)(Re(

λj ) +

large. In order not to use the large term, we can again execute the AR

Rjkp) becomes extremely

k

b

mj

b

mj)Ipj + Qj)

b

!

Γ(

Γ((

Γ((

λj )

(4.7)

i=1
Y
mj

∈  

If Re(

i)Ipj + Qj)

λjIpj + Qj) =

i)Ipj , Rji!

λj −
((
b
λj −
(
h
i=1
Y
b
[1, 2], then

λj −
b
λj −
b
Rjkp)
mj +
|
λj + i)Ipj , Rji
(
h
(mjp3
compute this product, then
b
large when mj and pj are large. For enclosing this product with only
ations, we present Theorem 4.7.
Theorem 4.7. Let mj ∈
Rpj ×pj
+

Γ(1)(Re(
In (4.6), we need to compute the product

Rpj ×pj
+
by R0 := Rj and

Z++, Rj ∈

Rjkp ∈

−
mj −1
i=0

mj +

Deﬁne R0, . . . , Rmj−1 ∈

λj )

O

−

b

k

k

|

. If we directly
j ) operations are required, which is prohibitively
j ) oper-

(mjp2

b
Q

O

mj)Ipj + Qj).

is not large.

and rr := [maxi(Rj )i1, . . . , maxi(Rj )ipj ].

k

Rk +

Rk+1 :=

Rj + 1lv

pj rrRk,

|

|

mj −1
i=0

λj + k + 1

λj + i)
(
(cid:12)
(cid:12)
i=0
(cid:12)
(cid:12)
Y
(cid:12)
(cid:12)
b
(cid:12)
(cid:12)
λj + i))Ipj , Rmj−1i
.
(
λj + i)Ipj , Rji ⊆ h
(
(
(cid:12)
(cid:12)
h
Proof. We prove Theorem 4.7 by induction. The result is obvious when mj = 1.
Q
ℓ
b
b
λj + i))Ipj , Rℓi
for ℓ > 1. Then, (2.1) and
(
λj + i)Ipj , Rji ⊆ h
(
i=0h
b
b

k = 0, . . . , mj −

b
mj −1
i=0

ℓ
i=0(

Q

Q

2.

Then,

Q
Suppose

 
 
10

1lv
pj rr ≥

Rj give

ℓ+1

SHINYA MIYAJIMA

ℓ

(
λj + i)Ipj , Rji ⊆ h
(
h
b

i=0
Y

λj + ℓ + 1)Ipj , Rji * 

λj + i)
(
i=0
Y

!

b

Rℓ +

|

Ipj , Rℓ

+

ℓ

λj + i)
(
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

b

i=0
Y

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Rj + RjRℓ

+

Ipj ,

|

λj + ℓ + 1

b
Ipj , Rℓ+1

.

+

!

!

b

ℓ+1
λj + i)
(

i=0
Y
b
ℓ+1
λj + i)
(

⊆ * 

⊆ * 

i=0
Y

b

The computation of Rk+1 involves
mj −1
putation of
i=0

(p2
λj + i))Ipj , Rmj−1i
(
b
by S1 := Rj and

Rpj ×pj
+

(
h

O

The reduction (4.7) can be accelerated completely analogously.
Corollary 4.8. Let mj, Rj and rr be as in Theorem 4.7. Deﬁne S1, . . . , Smj ∈

Q

j ) operations for each k. Therefore, the com-

requires only

(mjp2

j ) operations.

O

k

Sk +

Sk+1 :=

Rj + 1lv

pj rrSk,

|

|

1

k

−

1.

Q

Then,

i))Ipj , Smj i
.

k = 1, . . . , mj −

λj −
(
i=1
Y
b
mj
i=1(

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(
i)Ipj , Rji ⊆ h

λj −
b
mj
λj −
(
i=1h
b

i)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
λj −
In practical execution, we need to choose mj. We ﬁrst consider choosing mj in
b
λj ) + mj − k
Rjkp > 0. If mj is too
Rjkp is larger than,
λj ) + mj − k
b
1. Based on these observations,
≫
b
λj ) + mj −
Rjk∞⌋
[1, 2). We can analogously choose mj in (4.7). Speciﬁcally, we choose
b
[1, 2).
λj )

(4.6). As mentioned above, mj must satisfy Re(
large, then Γ(1)(Re(
but close to 0, then ω(Re(
we propose determining mj = 1
Rjk∞ ∈
k
Re(
mj =
⌊

Rjkp)
1. If Re(
≫
k
Rjk∞)
λj ) + mj − k
λj )
Re(
− k
− ⌊
b

There exists the case where the AR is required even when pj = 1. To be speciﬁc,
5/2 can not be veriﬁed. In this

1, which assures Re(

, which assures Re(

Rjk∞⌋ −

Rjk∞ ∈

λj ) + mj +

mj − k

− k

λj )

Q

−

b

b

b
Rj >

λj )

b
we can not execute Algorithm 3.3 if Re(
case, we execute the AR

−

−

Γ(

λj + Qj) =

b

mj −1
λj + i + Qj)
(

−1

b

i=0
Y
mj −1

∈  

b
λj + i, Rji!
h
b
Rj + mj larger than
λj)

i=0
Y

λj)
−
, which assures Re(
Rj ⌋
−
b

b

in order to make Re(
mj =

Re(

λj )

2

−

− ⌊

for enclosing Γ(A).

b

!
−1

Γ(

λj + mj + Qj)

b

Γ(

λj + mj + Qj)

b

5/2. We determine mj such that
Rj + mj ∈

1).

2,

−

−

[

−
−

4.5. Overall algorithm. Based on Sections 4.1 to 4.4, we propose an algorithm

ICpj ×pj and
Algorithm 4.9. Let Pj and W be as in Section 4.1, and P j ∈
ICpj ×pj contain Pj and Γ(Pj), respectively, for j = 1, . . . , q. This algorithm
Γ(A). If the algorithm successfully terminated,

ICn×n such that Γ

Γj ∈
computes Γ
then µ(A)

∈
Z− =

∅

∩

is moreover proved.

∋

 
VERIFIED COMPUTATION OF MATRIX GAMMA FUNCTION

11

Step 1. Compute W and P j, j = 1, . . . , q by executing [6, Algorithm 1]. Note that

r in Section 4.2 is also obtained in this process.

Step 2. Let f be as in Section 4.2. If mini(
|
Z− =
with failure. Otherwise, µ(A)

ri) > 0 cannot be veriﬁed, terminate
is proved.
ICpj ×pj for all j by repeatedly executing Algorithm 4.10.

Step 3. Compute Γj ∈
Step 4. Compute Γ by Γ = W diag(Γ1, . . . , Γq)W −1. Terminate.

fi|−
∅

∩

Algorithm 4.10. This algorithms computes Γj in Algorithm 4.9.

Step 1. If pj = 1, then go to Step 2. Otherwise, go to Step 3.
Step 2. Compute Γj by executing the interval valiants of Algorithms 3.2 and 3.3,

and AR if necessary. Terminate.

Step 3. If Re(

λj)

− k
Otherwise, go to Step 5.

Rjkp > 0 for p = 1,

cannot be veriﬁed, then go to Step 4.

∞

Step 4. Compute Γj with the AR (4.6). Terminate.
Step 5. Compute Γj with the AR (4.7). Terminate.

b

O

O

O

(p3

O
(n3 +

Step 1 in Algorithm 4.9 involves

in Algorithm 4.9 also involves
rithm 4.10 is
requires
negligible. Algorithm 4.9 thus involves only

(n3) operations (see [6, Section 3.4]). Step 4
(n3) operations. The computational cost of Algo-
q
j ). From this and
j=1 pj = n, Step 3 in Algorithm 4.9
j ) operations. Costs of other parts in Algorithm 4.9 are
(n3).

P
(n3) operations if

j + mjp2
q
j=1 mjp2

O
5. Algorithm based on the NJD. Let Nj and nj be as in Section 2. When A
is defective or close to defective, the matrix X in Section 4.1 becomes singular or ill-
conditioned, which causes failure of [6, Algorithm 1]. Even in such situations, we can
Cn×n, Z is nonsingular, J = diag(J1, . . . , Jp),
utilize the NJD AZ
∈
p
j=1 nj = n. We proceed similarly to Section 4.
Jj = λjInj + Nj, j = 1, . . . , p, and

ZJ, where Z, J

q
j=1 mjp2

j is

P

P

O

≈

P

p1 , . . . , i(q)

1 , . . . , i(1)

5.1. The VBD based on the NJD. Let q, i(1)

pq , Wj,
Pj and W be as in Section 4.1. We can execute [6, Algorithm 3] utilizing the NJD
instead of the numerical block diagonalization in [6, Section 4]. Then, we can obtain
C, r
λj ∈
λj , rji ⊇
h
µ(A). As byproducts, this algorithm also gives Rj, Mj ∈
µ(Pj ) and
b
b
Rpj ×pj
), where
+
sj
ℓ=1 nk(j)

Rq
+, Xj ∈
q
λj , rji ⊇
j=1h
λj Ipj + Mj, Rji ∋
such that
S
h
b
= pj. If veriﬁcation for nonsingularity of any matrix contained in W is
b

Pj and Mj = diag(Nk(j)

Cn×pj and ∆j ∈

Xj, ∆ji ∋
h

1 , . . . , i(q)

, . . . , Nk(j)

such that

Rn×pj
+

Wj,

∈

sj

1

ℓ

λ1Ip1 + M1, R1i
succeeded, then W and diag(
P
h
as the result of the VBD.
b

, . . . ,

λqIpq + Mq, Rqi
h
b

5.2. Veriﬁcation of µ(A) ∩ Z− = ∅. Similarly to Section 4.2, we have
Corollary 5.1. Let
λi, rii
Cq by
b
i = 1, . . . , q.

+ satisfy µ(A)

∈
fi := max(Re(

Cq and r

S
λi)i,

λi), Re(

q
i=1h

) + Im(

λi),

Re(

Re(

Re(

λi)

Rq

⊆

∈

∈

λ

. Deﬁne

f

) can be regarded

b
λi)
⌋ −

⌊

− ⌈

λi)
⌉

∅

b

b

b

∩

Z− =
b

fi| −

If mini(
|

ri) > 0, then µ(A)

λjIpj + Mj + Qj, where Qj ∈
as Pj =
λjIpj + Mj + Qj)
upper bound for
−
b
Theorem 5.2. Let ω(α) be as in Lemma 2.6, p

.
b
5.3. Computable perturbation bound. The diagonal block Pj can be written
Qj| ≤
Rj. We can derive an
|
kp analogously to Section 4.3.
Z++ ∪ {∞}
, and
λj, Qj, Mj
Rj, and deﬁne
Qj| ≤
|
b
Mj + Rjkp)
λj) +

− k
Mj + Rjkp) + ω(Re(

Cpj ×pj satisﬁes
λjIpj + Mj)

and Rj be as above. Suppose Re(

Mj + Rjkp > 0 and

Rjkp(Γ(1)(Re(

λj ) +
b

δp :=

λj )

Γ(

Γ(

∈

b

b

b

k

k

k

k

b

b

12

Then,

SHINYA MIYAJIMA

+ω(Re(

λj)

− k

Γ(

λjIpj + Mj + Qj)

b

k

Mj + Rjkp)).
Γ(

λjIpj + Mj)

−

kp < δp.

Theorem 5.2 immediately gives Corollary 5.3.
Corollary 5.3. Let

b

b

min(δ1, δ∞). Under the assumptions in Theorem 5.2, it holds that Γ(
Qj)

Γ(

b

λj , Qj, Mj and δp be as in Theorem 5.2, and δ :=
λjIpj + Mj +

λj Ipj + Mj) by executing Algorithms 3.2 and 3.3, because

b

λj Ipj + Mj), δ1lM
.
pj i

∈ h
We can enclose Γ(
λj Ipj + Mj) is equal to
b

b

Γ(

b





diag

λj)

Γ(0)(
0!

b

· · ·
. . .

(
λj)
1)!
b

λj)

Γ(0)(
0!



b



, . . . ,

· · ·
. . .

−1)

(n

(j)
1

k

Γ
(nk(j)
1 −
...
Γ(0)(
0!

λj )

























5.4. ARs of diagonal blocks. Suppose the assumption in Corollary 5.1 is true.
= 1, then the ARs in Section 4.4 are possible.
Mj + Rjkp > 0

If Mj = 0, i.e., nk(j)
· · ·
Otherwise, we execute the ARs as follows: Let mj ∈
can not be veriﬁed, then we execute the AR

Z+. If Re(

= nk(j)



























λj )

λj )

− k

=

b

b

sj

1

(
λj )
1)!
b





.

−1)

(n

k

(j)
sj

Γ
(nk(j)
sj −
...
Γ(0)(
0!

mj −1

b

−1

Γ(

λj Ipj + Mj + Qj) =

((

λj + i)Ipj + Mj + Qj)

i=0
Y
mj −1

∈  

i=0
Y

b
λj + i)Ipj + Mj, Rji!
(
h
b

!
−1

Γ((

λj + mj)Ipj + Mj + Qj)

b

Γ((

λj + mj)Ipj + Mj + Qj),

provided that any matrix contained in
Mj + Rjkp ≫
If Re(

λj ) +

mj −1
i=0
1, alternatively, then we execute

Q

k

b
λj + i)Ipj + Mj, Rji
(
h
b

is nonsingular.

b

Γ(

λj Ipj + Mj + Qj) =

mj

i=1
Y
mj

λj −
((
b
λj −
(
h
b

∈  

i=1
Y

i)Ipj + Mj + Qj)

!

Γ((

mj)Ipj + Mj + Qj)

i)Ipj + Mj, Rji!

Γ((

mj)Ipj + Mj + Qj).

λj −
b
λj −
b

The theories for verifying µ(A)

λj Ipj + Mj + Qj) seems
to be analogues of those in Section 4. However, theories for accelerating the ARs are
diﬀerent.

and enclosing Γ(

Z− =

∩

∅

b

Theorem 5.4. Let

λj, Rj, Mj and nk(j)

ℓ

in Theorem 4.7, mj ∈
0 , . . . , β(k)
β(k)
n(j)
max−1 ∈

Z++, and n(j)

max := maxℓ nk(j)

b

C by β(0)

0

:=

λj, β(0)
1

:= 1, β(0)

, ℓ = 1, . . . , sj be as above, rr be as
. For k = 0, . . . , mj −
:= 0, ℓ = 2, . . . , n(j)
1,
max
−

2, deﬁne

ℓ

ℓ

β(k+1)
0

:= (

λj + k + 1)β(k)
0 ,

b
β(k+1)
ℓ

:= (

λj + k + 1)β(k)

ℓ + β(k)
ℓ−1,

ℓ = 1, . . . , n(j)

max −

1.

b

b

b
(5.1)

b
(5.2)

 
 
VERIFIED COMPUTATION OF MATRIX GAMMA FUNCTION

13

For ℓ = 1, . . . , sj and k = 0, . . . , mj −

2, let

R(ℓ)
j

:=

r(j,ℓ)
c

:=

(cid:20)
max
i

(Rj):Pℓ−1

i=1 n
k

(j)
i

+1, . . . , (Rj):Pℓ

(R(ℓ)

j )1i, . . . , max

(R(ℓ)

j )pj i

i

∈

i=1 n
(j)
k
i (cid:21)
Rpj
+ ,

T

∈

pj ×n

R

+

k

(j)
ℓ

,

h
w(k,ℓ) := 
|


c w(k,1), . . . , r(j,sj )
r(j,1)

β(k)
0 |

β(k)
1 |

β(k)
0 |

Qk :=

+

|

,

|

c

, . . . ,

i
−1

n

k

(j)
ℓ

i=0
X

w(k,sj )

∈

h
max−1

n(j)
i=0

β(k)
i M i

i
j for k = 0, . . . , mj −

1×n

k

(j)
ℓ

,

R

+

∈

β(k)
i

|

|


Rpj ×pj
.
+

1. Deﬁne R0, . . . , Rmj −1 ∈

Rpj ×pj
+

Let also Ck :=
by R0 := Rj and
P
Rk+1 := Qk +

λj + k + 1

|

|

Rk + MjRk + 1lv

pj rrRk,

k = 0, . . . , mj −

2.

Then,

mj −1
i=0

Q
Suppose

b

Cmj −1, Rmj −1i
.
Proof. We prove Theorem 5.4 by induction. The result is obvious when mj = 1.
Cℓ, Rℓi

λj + i)Ipj + Mj, Rji ⊆ h
(
h
ℓ
b
λj + i)Ipj + Mj, Rji ⊆ h
(
i=0h
b

for ℓ > 1. Then, (2.1) gives

ℓ+1

i=0
Y

Q
λj + i)Ipj + Mj, Rji
(
h
b
Cℓ, Rℓi
λj + ℓ + 1)Ipj + Mj, Rjih
(
⊆ h
λj + ℓ + 1)Ipj + Mj|
(
λj + ℓ + 1)Ipj + Mj)Cℓ,
((
|
b
Cℓ+1,
h
b
b
Since M n(j)
b
max

Rℓ+1i
.
b

= 0, it follows that

⊆ h
=:

j

Rℓ + Rj|

Cℓ|

+ RjRℓi

Cℓ+1 = ((

λj + ℓ + 1)Ipj + Mj)



β(ℓ)
i M i

j 

=

n(j)

max−1

i=0
X

n(j)

max−1

i=0
X

β(ℓ+1)
i M i

j = Cℓ+1.



Rℓ+1 can be estimated as follows:

in

b
The term Rj|

b
Cℓ|
n(j)

Rj|

Cℓ|

= Rj

β(ℓ)
i

M i
j

|

|

max−1
b

i=0
X
n
k

(j)
1

−1

−1

n

k

(j)
sj

β(ℓ)
i

|

N i

k(j)
1

|

, . . . , R(sj )

j

= 

R(1)
j




i=0
X

−1

n

k

(j)
1

i=0
X

β(ℓ)
i

|

|

N i

sj 
k(j)



−1

n

k

(j)
sj

)T

r(j,1)
c

(1lv
n

k

(j)
1

≤ 
i=0
X


c w(ℓ,1), . . . , r(j,sj )
r(j,1)
h

=

c

β(ℓ)
i

|

N i

k(j)
1

|

, . . . , r(j,sj )
c

w(ℓ,sj )

= Qℓ.

i

(1lv
n

k

(j)
sj

)T

i=0
X

β(ℓ)
i

|

|

N i

sj 
k(j)



14

SHINYA MIYAJIMA

From this and 1lv

Qℓ +

Rj, we obtain

pj rr ≥
Rℓ+1 ≤
ℓ+1
b
λj + i)Ipj + Mj, Rji ⊆ h
(
i=0 h
b

λj + ℓ + 1

b

|

Hence,

Rℓ + MjRℓ + 1lv

pj rrRℓ = Rℓ+1.

|
Cℓ+1, Rℓ+1i
.

It is obvious that we do not need to execute the matrix multiplications M i
j and
MjRk in Cmj −1 and Rk+1, respectively, via ﬂoating point arithmetic. In fact, Cmj −1
and MjRk can be written as follows:

Q

· · ·
. . .

k

(j)
1

β(mj −1)
−1
n
...
β(mj −1)
0

T

)T

, where

β(mj −1)
0

· · ·
. . .








, . . . , 





k

(j)
sj

β(mj −1)
−1
n
...
β(mj −1)
0





,











β(mj −1)
0








Cmj −1 = diag 




k )T , . . . , (R(sj )
(R(1)
h
(Rk)Pℓ−1
i=1 n
k
...

MjRk =

(j)
i



k

R(ℓ)
k

:=

i

. . .

+2 1

. . .
. . .

(Rk)Pℓ−1
i=1 n
k
...

(j)
i

+2 pj



,

ℓ = 1, . . . , sj.

1

(Rk)Pℓ

(Rk)Pℓ

pj

(j)
i

(j)
i

· · ·








i=1 n
k
0

i=1 n
k
0







Hence, the computations of Cmj −1 and Rmj−1 require only
mj
i)Ipj + Mj, Rji
(
λj −
i=1h
λj, Rj, Mj, mj, n(j)
Corollary 5.5. Let
Q
b
1, deﬁne β(k)
0 , . . . , β(k)
n(j)
max−1 ∈
b
1,

For k = 1, . . . , mj −
β(1)
ℓ

:= 0, ℓ = 2, . . . , n(j)
max

The enclosure of

0

O

(mj p2
j ) operations.
in (5.2) can also be accelerated.
max, rr and Qk be as in Theorem 5.4.
:= 1,

C by β(1)

1, β(1)
1

:=

λj −
b

−

1)β(k)
0 ,

−

max−1

β(k)
i M i

β(k+1)
0

:= (

Let Dk :=
Rj and

k

λj −
n(j)
b
i=0

P

β(k+1)
ℓ

:= (

1)β(k)

ℓ + β(k)
ℓ−1,

−
j for k = 1, . . . , mj. Deﬁne S1, . . . , Smj ∈

k

λj −
b

ℓ = 1, . . . , n(j)

1.

max −

Rpj ×pj
+

by S1 :=

λj −
b

Sk+1 := Qk +

|

k

1

|

−

Sk + MjSk + 1lv

pj rrSk,

k = 1, . . . , mj −

1.

Then,

mj
λj −
(
i=1h
b
k

Q
is replaced by

Mj + Rjk∞.

i)Ipj + Mj, Rji ⊆ h

Dmj , Smj i
.

We can determine mj in (5.1) and (5.2) analogously to Section 4.4, where

Rjk∞

k

5.5. Overall algorithm. Based on Sections 5.1 to 5.4, we propose Algorithm 5.6.
Algorithm 5.6. Let Pj, W , P j, Γj and Γ be as in Algorithm 4.9. This
Z− =

algorithm computes Γ. Moreover, µ(A)
Step 1. Compute W and P j, j = 1, . . . , q by executing the Jordan valiant of [6,

is proved if successful.

∩

∅

Algorithm 3]. Note that r in Section 5.1 is also obtained.

Step 2. Analogous to that in Algorithm 4.9, where f in Section 5.2 is used instead.
Step 3. Compute Γj for all j via Algorithm 5.7.
Step 4. Similar to that in Algorithm 4.9.

Algorithm 5.7. This algorithms computes Γj in Algorithm 5.6.

VERIFIED COMPUTATION OF MATRIX GAMMA FUNCTION

15

Steps 1 and 2. Similar to those in Algorithm 4.10.
Step 3. Analogous to that in Algorithm 4.10, where
Step 4. Compute Γj with the AR (4.6) if Mj = 0, (5.1) otherwise. Terminate.
Step 5. Compute Γj with the AR (4.7) if Mj = 0, (5.2) otherwise. Terminate.

Rjkp is replaced by

k

k

Mj +Rjkp.

The NJD involves
q
j=1 mjp2
j is

tions if

O

(n4) operations. Algorithm 5.6 thus involves
O
(n4).

O

(n4) opera-

∩

P

∅
∅

∩
∩

Z− =

Z− =
Z− =

is not veriﬁed.

is veriﬁed,
is veriﬁed, and

6. Numerical results. We used a computer with an Intel Core 1.51 GHz CPU,
16.0 GB RAM, and MATLAB R2012a with the Intel Math Kernel Library and IEEE
754 double precision. We denote the compared algorithms as follows:
Gs: Algorithm 4.9, where µ(A)
Gj: Algorithm 5.6, where µ(A)
V: VERSOFT routine VERMATFUN, where µ(A)
In Gs and Gj, we perform the NSD and NJD by MATLAB and NAClab [16] routines
eig and NumericalJordanForm, respectively. The routine NumericalJordanForm
Jp),
generally returns not Z and J but
≈
Jk are not necessarily one. However, we can compute Z
and superdiagonal entries of
b
b
J (see [8, Section 5]). In Gs and Gj, we computed products of an
and J from
interval matrix and an interval matrix containing inverse matrices via verifylss. In
V, we called vermatfun(’gamma(z)’,A) when A is Hermitian, invoking the INTLAB
routine gamma. When A is not Hermitian, we called vermatfun(’verGamma(z)’,A),
where verGamma is a routine which computes an interval containing Γ(z) for z
C
based on Section 3. See http://web.cc.iwate-u.ac.jp/~miyajima/MGF.zip for
details of the implementations, where INTLAB codes for Gs, Gj, V, and verGamma
(denoted by Gs.m, Gj.m, V.m, and verGamma.m) are uploaded.

J such that

J = diag(

J1, . . . ,

Z and

Z and

J,

A

Z

Z

∈

b

b

b

b

b

b

b

b

b

b

∅

˜Γ, R
h
k∞/
R
k

Γ(A). To assess quality of enclosure, deﬁne the relative radius RR by
Let
i ∋
˜Γ
k∞. For some problems, Gs or V failed. The reason for the failure of
RR :=
k
Gs is that [6, Algorithm 1] failed because the nonsingularity of X cannot be veriﬁed.
That of V is enclosing all the eigenvalues and eigenvectors of A failed.

Example 1. We applied the algorithms to four classes of matrices, “frank”,
“gcdmat”, “minij”, and “poisson”, available from the MATLAB gallery function,
and chose matrices of various n for each of the classes. For the “gcdmat” and “minij”
matrices, we divided the generated matrix by n in order to avoid overﬂow. Tables 6.1
to 6.4 display the RR and CPU times (sec) of the algorithms. We see that Gs and Gj
were faster than V in many cases.

Table 6.1
The RR (left half ) and CPU times (sec) (right half ) for the “frank” matrix.

n
5
7
9
11

Gs
5.9e–12
2.4e–9
2.7e–6
1.4e+0

Gj
3.8e–12
4.4e–11
2.4e–6
1.8e–2

V
3.6e–12
2.4e–11
7.3e–8
6.0e–4

Gs
6.8e–1
9.5e–1
9.1e–1
9.5e–1

Gj
9.0e–1
1.0e+0
1.1e+0
1.2e+0

V
7.6e–1
1.0e+0
1.3e+0
1.5e+0

Example 2. We consider the case where A comes close to being defective. We

applied the algorithms to the problem in [3, Experiment 1], in which

A =

(cid:20)

1
0

1
1 + ε

(cid:21)

, where

ε

0,

≥

16

SHINYA MIYAJIMA

Table 6.2
The RR (left half ) and CPU times (sec) (right half ) for the “gcdmat” matrix divided by n.

n
100
200
300
400

Gs
7.5e–12
3.2e–11
1.1e–10
2.2e–10

Gj
3.6e–11
2.6e–11
3.3e–11
1.3e–10

V
5.5e–11
1.1e–10
3.2e–10
7.4e–10

Gs
6.4e–1
2.3e+0
8.0e+0
3.8e+1

Gj
1.7e+0
6.5e+0
2.5e+1
1.1e+2

V
8.7e+0
3.3e+1
7.4e+1
1.6e+2

Table 6.3
The RR (left half ) and CPU times (sec) (right half ) for the “minij” matrix divided by n.

n
100
200
300
400

Gs
1.0e–9
8.1e–9
3.7e–8
8.6e–8

Gj
9.5e–10
1.9e–7
5.0e–8
1.4e–7

V
1.2e–8
2.2e–7
1.2e–6
4.2e–6

Gs
7.0e–1
2.6e+0
9.5e+0
3.8e+1

Gj
1.7e+0
7.8e+0
3.0e+1
1.1e+2

V
1.0e+1
3.8e+1
8.8e+1
1.5e+2

whose eigenvector matrix becomes increasingly ill-conditioned as ε
0. Table 6.5
reports quantities similar to those in Tables 6.1 to 6.4 with ε varying from 20 to 2−52.
This table shows that the RR by Gj stayed about the same, whereas those by Gs and
V increased as ε decreased.

→

Example 3. We consider the case where A is defective. Let

A0 := 



, whose Jordan canonical form is 

2
0
1
1

−

1 0
2
1
1 1
1 0 0
−
1 1
1









1 1
0 1
0 0
0 0

0 0
1 0
1 1
0 1



.









We set A = σA0 for a parameter σ
∈
in Table 6.5 for various σ, showing that Gj succeeded for all the problems.

R. Table 6.6 reports quantities similar to those

Example 4. Consider the case where A is derogatory. Let v(j) := (I8):j, j =
1, . . . , 8, and P := [v(7), v(5), v(3), v(1), v(8), v(6), v(4), v(2)]
R8×8. Then, P is orthogo-
nal. Using A0 and σ in Example 3, we set A = σP diag(A0, A0)P T . Table 6.7 displays
quantities similar to those in Table 6.6, which also shows the robustness of Gj.

∈

7. Concluding remarks. We have established the new framework for enclosing
matrix functions based on the VBD, proposed Algorithms 4.9 and 5.6, and reported
the numerical results. As mentioned in Section 1, these algorithms are ﬁrst ones which
encloses a matrix function based on this framework. Let ϕ : C
C be deﬁned on
µ(A). Essentially, we can enclose ϕ(A) based on the VBD framework if the followings
are possible:

→

•
•

λj Ipj + Mj + Qj)

enclosing ϕ(0)(z)/0!, . . . , ϕ(ℓ)(z)/ℓ! for z
computing rigorous upper bounds for
λj Ipj + Mj)

∈
ϕ(
k
kp for p = 1,
b
λjIpj + Mj have simple structures, the derivations of the bounds are
Since
easier than those for general matrices. For example, enclosing eA, sin A and cos A will
be possible based on this framework. Our future work will be to develop algorithms
for enclosing the matrix beta and Bessel functions.

C and ℓ
λjIpj + Qj)
.
∞

ϕ(
λjIpj and

Z+, and
ϕ(

kp and/or

λj Ipj )

ϕ(

−

−

∈

b

b

b

b

b

k

VERIFIED COMPUTATION OF MATRIX GAMMA FUNCTION

17

Table 6.4
The RR (left half ) and CPU times (sec) (right half ) for the “poisson” matrix.

n
9
36
81
144

Gs
2.5e–14
3.7e–13
1.9e–12
8.6e–12

Gj
2.8e–14
5.0e–13
2.0e–12
1.4e–11

V
1.0e–1
failed
failed
failed

Gs
1.7e+0
8.3e+0
2.3e+1
4.3e+1

Gj
2.5e+0
1.2e+1
2.9e+1
5.3e+1

V
1.1e–1
failed
failed
failed

Table 6.5
The RR (left half ) and CPU times (sec) (right half ) in Example 2.

ε
20
2−26
2−39
2−48
2−52

Gs
2.6e–13
4.2e–6
3.4e–2
1.5e+1
1.3e+3

Gj
2.6e–13
3.2e–13
2.9e–13
3.0e–13
9.7e–13

V
2.6e–13
4.2e–6
3.4e–2
1.6e+1
failed

Gs
3.4e–1
3.4e–1
3.4e–1
3.8e–1
6.5e–1

Gj
3.2e–1
6.1e–1
6.0e–1
6.1e–1
6.7e–1

V
3.3e–1
3.8e–1
3.8e–1
3.4e–1
failed

REFERENCES

[1] H. Arndt, On the interval systems [x] = [A][x] + [b] and the powers of interval matrices in

complex interval arithmetics, Reliab. Comput., 13 (2007), pp. 245–259.

[2] J.R. Cardoso and A. Sadeghi, Computation of matrix gamma function, BIT, 59 (2019), pp.

343–370.

[3] M. Fasi, N.J. Higham, and B. Iannazzo, An algorithm for the matrix Lambert W function,

SIAM J. Matrix Anal. Appl., 36 (2015), pp. 669–685.

[4] N.J. Higham, Functions of Matrices: Theory and Computation, SIAM Publications, Philadel-

phia, 2008.

[5] W. Kr¨amer, Computation of the gamma function Γ(x) for real point and interval arguments,

Z. Angew. Math. Mech., 70(6) (1990), pp. 581–584.

[6] S. Miyajima, Fast enclosure for all eigenvalues and invariant subspaces in generalized eigen-

[7]

[8]

[9]

value problems, SIAM J. Matrix Anal. Appl., 35 (2014), pp. 1205–1225.

, Fast veriﬁed computation for the matrix principal pth root, J. Comput. Appl. Math.,

330 (2018), pp. 276–288.

, Veriﬁed computation of the matrix exponential, Adv. Comput. Math., 45 (2019), pp.

137–152.

, Veriﬁed computation for the matrix principal logarithm, Linear Algebra Appl., 569

(2019), pp. 38–61.

[10]

, Veriﬁed computation for the matrix Lambert W function, Appl. Math. Comput., 362

(2019), 124555.

[11] J.

Rohn,

VERSOFT:
http://uivtx.cs.cas.cz/~rohn/matlab

Veriﬁcation

Software

in

MATLAB/INTLAB,

[12] S.M. Rump, INTLAB - INTerval LABoratory, in Developments in Reliable Computing, T.

Csendes, ed., Kluwer Academic Publishers, Dordrecht, 1999, pp. 77–107.

[13]

, Veriﬁed sharp bounds for the real gamma function over the entire ﬂoating-point range,

NOLTA, IEICE, 5(3) (2014), pp. 339–348.

[14] J. Spouge, Computation of the gamma, digamma, and trigamma functions, SIAM J. Numer.

Anal., 31(3) (1994), pp. 931–944.

[15] N. Yamanaka, T. Okayama, and S. Oishi, Veriﬁed error bounds for the real gamma function
using double exponential formula over semi-inﬁnite interval, Lect. Notes Comput. Sci.,
9582 (2016), pp. 224–230.

[16] Z. Zeng and T.-Y. Li, NAClab: A Matlab toolbox for numerical algebraic computation, ACM

Commun. Comput. Algebra, 47 (2013), pp. 170–173.

18

SHINYA MIYAJIMA

Table 6.6
The RR (left half ) and CPU times (sec) (right half ) in Example 3.

σ
2−1
20
21
22
23

Gs
failed
failed
failed
failed
failed

Gj
4.0e–12
1.0e–11
1.3e–12
1.2e–12
1.1e–12

V
failed
failed
failed
failed
failed

Gs
failed
failed
failed
failed
failed

Gj
6.4e–1
6.2e–1
6.3e–1
6.4e–1
6.5e–1

V
failed
failed
failed
failed
failed

Table 6.7
The RR (left half ) and CPU times (sec) (right half ) in Example 4.

σ
2−1
20
21
22
23

Gs
failed
failed
failed
failed
failed

Gj
1.7e–11
2.1e–11
2.6e–12
7.0e–12
9.5e–12

V
failed
failed
failed
failed
failed

Gs
failed
failed
failed
failed
failed

Gj
1.1e+0
1.0e+0
8.9e–1
9.0e–1
8.9e–1

V
failed
failed
failed
failed
failed

