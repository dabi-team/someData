2
2
0
2

n
a
J

7
2

]

Y
S
.
s
s
e
e
[

1
v
5
2
8
1
1
.
1
0
2
2
:
v
i
X
r
a

Adam-based Augmented Random Search for Control Policies for Distributed
Energy Resource Cyber Attack Mitigation

Daniel Arnold†,(cid:63), Sy-Toan Ngo†,(cid:63), Ciaran Roberts†, Yize Chen†, Anna Scaglione◦, Sean Peisert†
†Lawrence Berkeley National Laboratory {dbarnold,sytoanngo,cmroberts,ychen,sppeisert}@lbl.gov
◦Cornell Tech as337@cornell.edu
(cid:63)These authors contributed equally to this effort

Abstract— Volt-VAR and Volt-Watt control

functions are
mechanisms that are included in distributed energy resource
(DER) power electronic inverters to mitigate excessively high
or low voltages in distribution systems. In the event that a
subset of DER have had their Volt-VAR and Volt-Watt settings
compromised as part of a cyber-attack, we propose a mecha-
nism to control the remaining set of non-compromised DER to
ameliorate large oscillations in system voltages and large voltage
imbalances in real time. To do so, we construct control policies
for individual non-compromised DER, directly searching the
policy space using an Adam-based augmented random search
(ARS). In this paper we show that, compared to previous
efforts aimed at training policies for DER cybersecurity using
deep reinforcement learning (DRL), the proposed approach is
able to learn optimal (and sometimes linear) policies an order
of magnitude faster than conventional DRL techniques (e.g.,
Proximal Policy Optimization).

I. INTRODUCTION

Distributed Energy Resources

(DER), most notably
rooftop solar photovoltaic (PV) systems, are envisioned to
be key components of realizing local, state, and federal
renewable energy goals in the next several decades. On the
federal level, discussions of a national clean energy standard
[1] and efforts to further bring down the cost of solar [2]
will undoubtedly serve to accelerate already high levels of
adoption of rooftop solar PV systems. Faced with a future
grid where a large amount of power is generated from DER,
stakeholders have convened over the past two decades to
codify standards that seek to govern the behavior of DER to
ensure resiliency and reliability of the power system.

A standard that has garnered signiﬁcant attention in recent
years is IEEE 1547 [3]. Among other things, this standard
proposes mechanisms that allow DER smart
inverters to
adjust their active and reactive power outputs in response to
changes in local system voltages. In so doing, the standard
seeks to ensure that DER will collectively act to minimize
occurrences of over- or under-voltages.

Two mechanisms proposed within IEEE 1547 aimed at
providing inverter-based voltage regulation are known as
Volt-VAR (VV) and Volt-Watt (VW) control functions. These
control functions, depicted in Figs. 2 - 3, alter DER reactive

This research was supported in part by the Director, Cybersecurity,
Energy Security, and Emergency Response, Cybersecurity for Energy De-
livery Systems program, of the U.S. Department of Energy, under contract
DE-AC02-05CH11231. Any opinions, ﬁndings, conclusions, or recommen-
dations expressed in this material are those of the authors and do not
necessarily reﬂect those of the sponsors of this work.

and active power injection according to piecewise linear non-
increasing functions of the locally sensed voltage. In addition
to prescribing the general structure of the control functions,
IEEE 1547 also speciﬁes different parameterizations of the
piecewise linear curves, which are realized via adjusting
the parameters η = [η1, η2, η3, η4, η5]. The ability to adjust
the parameters of the VV/VW curves, in theory, allows the
dynamic response of DER to changing system voltages to
be tailored to speciﬁc networks, or regions within a given
network.

While the ﬂexibility offered by IEEE 1547 ensures that
DER dynamic voltage response can be adjusted to better
serve different types of networks (e.g., radial vs. meshed
systems), the ability to adjust DER dynamics coupled with
Internet, cellular, or power line carrier connectivity of the
inverters themselves, potentially allows remote updating of
VV/VW parameters by an entity with malicious intention [4].
An excellent example of the extent to which aggregations of
smart inverters can be remotely updated was illustrated in
Hawaii, where local utilities worked with a smart inverter
vendor to remotely update the autonomous control functions
of 800,000 inverters in a single day [5].

In the event that a portion of the DER smart inverter func-
tions in a given network have had their settings adjusted as
part of a cyber-attack, our previous works (Roberts et. al. [6],
[7]) explored the use of Deep Reinforcement Learning (DRL)
to determine optimal control policies that alter the behavior
of the remaining population of non-compromised DER to
attempt to mitigate the effect of the cyber-attack in real time.
In our ﬁrst paper, we utilized Proximal Policy Optimization
(PPO) to train optimal polices that adjust DER smart inverter
VV and VW functions to mitigate attacks aimed at creating
large oscillations in system voltages [6]. In a subsequent
paper, we extended this framework using a different reward
function to develop optimal policies for mitigating attacks
designed to create large voltage imbalances in multi-phase
systems [7].

The PPO-based policies developed in our previous efforts
proved successful in mitigating large oscillations and voltage
imbalances even when almost half of the PV smart inverter
control functions in a given network were compromised
during the cyber-attack. The main drawback of the DRL ar-
chitecture used to train policies to manage non-compromised
DER was lengthy training time. Training on relatively small

 
 
 
 
 
 
networks, such as the IEEE 37-node test feeder using modest
resources (Intel® Core™ i7-8850H CPU @ 2.60GHz, 16GB
RAM) took several hours. Training control policies on the
IEEE 8500 node test feeder would often take tens of hours.
While proper parallelization and HPC/cloud resources could
be leveraged to bring down the overall training time and en-
able these solutions to scale to larger networks, many utilities
(rural electric cooperatives in the U.S., for example) lack the
expertise and ﬁnancial resources to properly engage these
assets for training unique policies for different networks. In
order to democratize the training of network-speciﬁc control
policies that optimally manage DER to ensure grid stability
in the face of cyber attacks, additional effort is needed to
develop alternative optimal control strategies which can train
equivalent policies in substantially less time.

In contrast to many popular reinforcement learning tech-
niques, evolutionary strategies (ES) do not attempt to ap-
proximate both the value function and the optimal policy of a
Markov decision process (MDP). ES, rather, directly search
for optimal policies through perturbing the present policy
and evaluating the improvement in the reward by conducting
“rollouts” of the MDP under the perturbed policy. Variants
of this “perturb and observe” paradigm include genetic
algorithms (GAs) from stochastic optimization [8] as well as
extremum seeking techniques from nonlinear control theory
[9]. In a compelling piece, Salimans et. al. [10] demonstrated
that random search (an extremely simple type of evolutionary
strategy) can be leveraged to learn competitive and highly
scalable policies compared to popular RL techniques for
several MDPs. Extending this work, Mania et. al. [11]
proposed the augmented random search (ARS) algorithm
that was used to train linear policies for several challenging
MDP learning tasks with excellent sample efﬁciency.

In this paper, motivated by the success of ARS in learning
optimal policies for complex MDPs with improved sample
efﬁciency, we consider an extension of ARS with an adaptive
learning rate applied to learn optimal policies for DER cyber-
resiliency. By incorporating the Adam optimization method
[12] into the gradient update step of ARS, we are able
to train linear policies to mitigate voltage oscillations and
neural-network based policies to mitigate voltage imbalances
caused by cyber-attacked DER. Our experiments show that
the Adam-based ARS approach (henceforth referred to as
Adam-ARS) enables training of competitive policies an order
of magnitude faster than PPO and with less variance than
ARS. As such,
this work stands to enable efﬁcient and
scalable learning of optimal policies for controlling DER
smart inverters on a network-speciﬁc basis.

This paper is organized as follows. Dynamic models of the
DER smart inverter VV and VW control functions, as well
as descriptions of the Adam solver and ARS are discussed
in Section II. Section III presents the Adam-ARS algorithm
and discusses the framework for training optimal policies.
Results showing the effectiveness of the trained policies in
mitigating voltage oscillations and imbalances introduced
by cyber-attacked DER are provided in Section IV. Finally,
concluding remarks are provided in Section V.

II. PRELIMINARIES

A. Inverter Dynamic Modeling and Agent Interaction

The goal of this work is to train an intelligent agent
capable of adjusting the parameters of VV/VW control
functions in non-compromised DER to mitigate large voltage
oscillations and imbalances introduced by subsets of DER
which have been cyber-attacked. A depiction of the agent
interacting with a dynamic model of a DER smart inverter
is depicted in Fig. 1.

Fig. 1: Block diagram of VV and VW control logic of an inverter.

As is shown in the ﬁgure, the smart inverter dynamic
model maps grid voltages vt into active pt and reactive qt
power injections. The grid voltage is ﬁrst passed through a
low-pass ﬁlter HM (z) to generate the measured grid voltage
ˆvt which is fed into the nonlinear Volt-VAR and Volt-Watt
control functions, represented by f q(ˆv) and f p(ˆv). These
functions compute reactive and active power setpoints uq
t
and up
t which are passed through low pass ﬁlters Ho(z)
to create the actual powers injected into the grid, qt and
pt. The VW/VV control functions are shown in Figs. 2 - 3
and consist of piecewise linear non-increasing functions of
grid voltage parameterized by a set of voltage values η =
[η1, η2, η3, η4, η5]. The VV/VW control structure depicted
in Fig. 1 is known as Volt-Watt precedence [13] where
priority is given to the VW controller to compute the active
power setpoint up
t , any
additional inverter capacity can be used for reactive power
generation, qavail, which is input into the VV controller to
determine the setpoint uq
t . We note that this dynamic model
of smart inverter behavior is consistent with IEEE 1547 and
supporting documents [3], [13].

t . Following the computation of up

The intelligent agent interacts directly with the VV/VW
time t (at). The
controllers through an action taken at
subsequent reward rt due to the application of at is then
input into the agent, along with a set of observations ot from
the grid as well as past actions at−1.

B. Observations of Voltage Oscillations and Imbalances

During a cyber-attack, we assume that an adversary has
the capability to alter the parameter vector η in a portion of
DER thereby adjusting the shape of their VV/VW curves

% available VARs

100%

η1

η2

η3

η4

V

Voltage Imbalances (VI): The metric used to compute

voltage imbalance at node i at time t is given by:

vii,t =

max(|¯vi,t − va

i,t|, |¯vi,t − vc

i,t|)

i,t|, |¯vi,t − vb
¯vi,t

(1)

where ¯vi denotes the mean measured voltage magnitude at
bus i, and va
i,t are the measured voltage magnitudes
on phase a, b, and c respectively.

i,t, vc

i,t, vb

-100%

C. Adam

Fig. 2: Inverter Volt-VAR curve. Positive values denote VAR
injection.

% max. watt output

100%

η4

η5

V

Fig. 3: Inverter Volt-Watt curve. Positive values denote watt injec-
tion.

to generate large voltage oscillations [6] or large voltage
imbalances [7]. We then assume that the remaining non-
compromised DER in the system are equipped with an
intelligent agent depicted in Fig. 1 which will re-adjust the
parameter vector η to mitigate the cyber-attack in real time.
Critical observations needed by the agent for cyber-attack
mitigation are measurements of the intensity of the voltage
oscillations and imbalances computed from grid telemetry.
We refer the reader to our previous works [6], [7] for detailed
discussions of how these observations are computed in real
time. The procedure is brieﬂy summarized here:

Voltage Oscillations (VO): We utilize an intuitive ﬁltering
process to extract the “energy” associated with observed volt-
age oscillations. The ﬁlter consists of the series connection
of a high-pass ﬁlter HHP , a signal square element (with
positive gain c), and a low-pass ﬁlter HLP , shown in Fig. 4.
The output of the ﬁlter voi,t is a non-negative value which
becomes larger as the amplitude of the oscillations in node
i voltage (vi,t) increase. For proper operation, the high and
low-pass ﬁlter critical frequencies should be chosen as to not
attenuate oscillations resulting from cyber-attacked inverters.

Adam is a ﬁrst-order gradient-based stochastic optimiza-
tion method that which utilizes adaptive estimates of lower-
order moments [14]. Similar to AdaGrad [12] and RMSProp
the method features an adaptive learning rate and
[15],
has been shown to work well with sparse gradients and in
non-stationary settings. We present the Adam algorithm for
completeness in Algorithm 1.

D. Random Search and Augmented Random Search

The goal of reinforcement learning is to determine a policy
that governs a dynamic system to maximize a reward asso-
ciated with a speciﬁc task. This problem can be formulated
as [11]:

max
θ∈Rn

Eξ[r(πθ, ξ)],

(2)

where πθ is the policy parameterized by θ ∈ Rn, ξ represents
the randomness of the environment, r(πθ, ξ) is the cumula-
tive reward that the policy πθ generates in one trajectory (i.e.,
a ”rollout”) of the system.

In situations where the gradient of the objective function is
not directly available to aid in the search for for the optimal
parameter vector θ∗, one can utilize measurements of the
reward to approximate the gradient of the reward under the
present policy g(θ). A ﬁnite-difference approximation of g(θ)
is given by [8]:

g(θ) =

r(πθ+cδ, ξ1) − r(πθ−cδ, ξ2)
2c

,

(3)

where c is a positive scalar and δ is a zero-mean and Gaus-
sian vector. Random Search (RS) [8], [10], [11] incorporates
single or mini-batches of gradient approximations of (3) into
simple gradient ascent to search for θ∗.

Mania et. al. [11] suggest several improvements to RS,
yielding the Augmented Random Search (ARS) algorithm.
The modiﬁcations are:

• Normalization of the states
• Scaling the gradient by the standard deviation of return
• Using top performing directions in mini-batch updates

vi,t

∆vi,t

HHP (z)

c · ()2

HLP (z)

Fig. 4: Block diagram of illustrating the ﬁltering process used to
compute a measurement of the intensity of voltage oscillations.

voi,t

A. Adam-based Augmented Random Search

III. METHODOLOGY

We propose an extension of ARS based on the replace-
ment of vanilla gradient ascent with the Adam optimization
algorithm. The method is speciﬁed in Algorithm 2.

Algorithm 1: Adam Optimization Algorithm 1-step forward
Hyperparameters: Gradient gt, stepsize α, exponential decay rate β0, β1 for moment estimates, tolerance parameter

λADAM > 0 for numerical stability. m0, v0 ← [0, 0, 0]

3

1 Function ADAM(θj, gj, α, β0, β1):
mj ← β1 · mj−1 + (1 − β1) · gj
2
vj ← β2 · vj−1 + (1 − β1) · g2
j
ˆmj ← mj/(1 − βj
1)
ˆvj ← vj/(1 − βj
2)
θj+1 ← θj − α · ˆmj/((cid:112) ˆvj + λADAM )
return θt

7

6

4

5

Algorithm 2: ADAM-based Augmented Random Search
Hyperparameters: number of directions sampled per iteration N , exploration noise ν, number of top-performing

directions b (b ≤ N )

Initialize: µ0 = 0 ∈ Rn, Σ0 = In ∈ Rn×n, j = 0
(cid:40)

πθ =

0 ∈ Rp×n if using linear policy
NN(0) if using non-linear policy

1 while ending condition not satisﬁed do
2

Sample δ1, δ2, ..., δN of appropriate dimension with i.i.d. standard normal entries.
Collect 2N rollouts of horizon H and their corresponding rewards using the 2N policies.

πθj ,k,+(˜x) = πθj +νδk (˜x)

and πθj ,k,−(˜x) = πθj −νδk (˜x)

where ˜x = diag(Σj)

−1

2 (x − µj)

for k ∈ {1, 2, 3, ...N }.
Sort the direction δk by max{r(πθj ,k,+), r(πθj ,k,−)}, denote by δ(k) the k-th largest direction, and by πθj ,(k),+
and πθj ,(k),− the corresponding policies.
Policy update step:

gj =

α
bσR

b
(cid:88)

[r(πθj ,(k),+) − r(πθj ,(k),−)]δ(k)

k=1

θj+1 = ADAM(θj, gj, α, β0, β1)

where σR is the standard deviation of the 2b directions used to update step.
Set µj+1, Σj+1 to be mean and the covariance of the 2N H(j + 1) states encountered from the start of training.
j ← j + 1

3

4

5

6

7
8 end

B. Training Framework

Training:

We seek to train a policy to mitigate large voltage oscilla-
tions (VO) and voltage imbalances (VI) stemming from DER
smart inverter VV/VW control functions with maliciously
chosen setpoints. We represent a single distribution feeder as
a graph G = (N , L), where N and L denote the set of nodes
and lines, respectively. For simplicity of presentation, it is
assumed that a DER equipped with VV/VW functionality is
located at every node in G. We now partition the set of invert-
ers into two groups H and U representing ”compromised”
and ”non-compromised” sets of DER, where H (cid:83) U = N .
We also make the assumption that the set U is nonempty
and contains sufﬁcient amounts of DER to mitigate the effect
of the cyber-attack. We adopt the following framework for
training optimal policies:

1) We deﬁne a single agent whose input observation
vector is from a single node in the network at time
t with worst case VU and VO. The agent has a
multi-head output action, ai
t∀i ∈ {a, b, c}, which is
a deviation/offset, ∆η, from default VV/VW control
curves in Figs. 2 - 3 that is applied across all single-
phase inverters. An example of an action is shown in
Fig. 5. The action space is a continuous offset between
-0.1 pu to 0.1 pu around the default inverter VV/VW
curve.

2) In order to preserve the Markov property, new param-
eterizations of VV/VW functions occurs on a slower
timescale than the ﬁlter dynamics in Fig. 1.

RL Agent Action
∆η

% available VARs

100%

η1

η2

η3

η4

V

-100%

Fig. 5: Action example.

Observation: The observation vectors oi,t, where i ∈ U,

d
r
a
w
e
r

d
r
a
w
e
r

0
9
−

0
1
1
−

0
4
1
−
0
9
−

0
0
3
−

0
0
5
−

0

20

40

60

80

Adam ARS
ARS

Adam ARS
ARS
PPO

0

50

100

150

200

Epochs

at each controller consist of:
1) vit: deﬁned in (1).
2) vot: deﬁned in Fig. 4.
3) qavail, nom
t
active power curtailment.
t−1, ac
t−1, ab
agent across each phase.
t, vb

4) aa

: the available reactive power capacity without

t−1: the previous action taken by the

t , vc

t: voltage phase measurements at time t

5) va
During the training of the agent, vot and vut are the worst-
case oscillations and imbalances in the feeder, and qavail, nom
is the average across all non-compromised inverters. When
the agent is deployed to manage individual DER, vot and
vut will be collected from the nearest three-phase node.

t

Reward: At a time t, the reward function, rt(at, ot) for a

single agent in training is:

(cid:32)

rt = −

σu||vit||∞ + σu||vot||∞ +

(cid:88)

σa1ai

t(cid:54)=ai

t−1

(cid:88)

σ0(cid:107)ai

t(cid:107)2 +

i∈{a,b,c}

1
|U|

|U |
(cid:88)

j=1

i∈{a,b,c}
(cid:32)

σp

1 −

(cid:33)2 (cid:33)
.

pj,t
pmax
j,t

+

(4)

The ﬁrst term seeks to minimize the maximum imbalance,
||vit||∞ and the second term seeks to minimize the maximum
oscillation, ||vut||∞ over all nodes in the network. For details
on the remaining components of the reward, we refer the
reader to our previous work [7].

Fig. 6: Average training reward. The shaded area represents the
standard deviation over 10 runs.

launched where the attacker controls between 10% and 40%
of inverter capacity in the system.

B. Training performance

Fig. 6 shows the training performance of Adam-ARS,
ARS, and PPO when these agents are learning to mitigate
a voltage imbalance attack. All three algorithms utilize the
same neural network architecture. As is shown in the ﬁgures,
ARS and ADAM-based ARS take approximately 80 epochs
to converge to the optimal policy while PPO takes more than
3 times the number of epochs to converge. Interestingly,
due to the nature of training a deep neural network with
backpropagation [16], the weights of neural network in PPO
requires ﬁne-tuned initialization. This initialization results in
extremely poor performance of PPO at the start of training,
taking over 100 epochs to achieve rewards similar to both
ARS variants. In contrast, since ARS/Adam-ARS randomly
perturbs the parameters of the neural network, neural net-
works for these algorithms can be initialized with weights
equal to 0.

The top subplot of Figure 6 shows Adam-ARS learns the
optimal policy faster than ARS and shows much less variance
in the associated rewards.

IV. RESULTS

C. Oscillation Attacks

A. Experimental Setup

Experiments were conducted on the IEEE-37 node test
feeder where DER with VV/VW capability were placed
load buses with peak active power generation of
at all
100% of nominal load. Additionally, the inverter capacity
associated with all DER was oversized by 10% to allow
for some reactive power compensation without active power
curtailment at all simulation timesteps. Agent training occurs
using 700 second rollouts with 1 second timestep simulations
using OpenDSS. Load, solar generation, percentage of DER
resource which are compromised, and the phase of the
voltage regulator are all randomized for each rollout. At
t = 200 seconds in the simulation, a simulated attack is

An attack on 30% of DER VV/VW controllers that creates
voltage oscillations is depicted in Fig. 7. This baseline case
demonstrates the effect of the attack in the system without
utilizing the policy trained by Adam-ARS to control the
remaining DER smart inverters in the system. Fig. 8 shows
the effect of a linear policy trained by Adam-ARS. Clearly,
the trained policy is effective in adjusting DER smart inverter
VV/VW controllers to minimize voltage oscillations in the
network shortly after they ﬁrst manifest. Hyperparameters
for this experiment are shown in Table I.

D. Imbalance Attacks

While a linear policy is sufﬁcient to mitigate oscillation
attacks, mitigation of imbalance attacks requires a more

2
0
.
1

7
9
.
0

3
9
.
0

4
.
0

2
.
0

0

1
.
0

0

1
.
0
−

1

0

2
0
.
1

7
9
.
0

3
9
.
0

4
.
0

2
.
0

0

1
.
0

0

1
.
0
−

1

0

)
.

.

u
p
(

e
g
a
t
l
o
v

O
V

η
∆

r
a
V
M

)
.
u
.
p
(

e
g
a
t
l
o
v

O
V

η
∆

r
a
V
M

attack

Phase A
Phase B
Phase C

Largest Network Voltage Oscillation

remaining DER smart inverters in the system. Fig. 10 shows
the effect of a neural network-based policy trained by Adam-
ARS. As is shown in the ﬁgure, the policy signiﬁcantly
reduces the largest voltage imbalance from 3% to less than
1%. Hyperparameters for this experiment are shown in Table
II.

Agent action

Offset phase A

Offset phase B

Offset phase C

Total DER Reactive Power Injection

0

100

200

Phase A

Phase B

Phase C

300
400
Time (s)

500

600

Fig. 7: 30% DER oscillation attack with no defense

attack

Phase A
Phase B
Phase C

)
.
u

.

p
(

e
g
a
t
l
o
v

)

%

(

U
V

η
∆

r
a
V
M

4
0
.
1

8
9
.
0

3
9
.
0

6

4

2

0

1
.
0

0

1
.
0
−

1

0

1
−

attack

Phase A
Phase B
Phase C

Largest Network Voltage Imbalance

Agent action

Offset phase A

Offset phase B

Offset phase C

Total DER Reactive Power Injection

0

100

200

Phase A

Phase B

Phase C

300
400
Time (s)

500

600

Largest Network Voltage Oscillation

Fig. 9: 30% DER imbalance attack with no defense

Agent action

Offset phase A

Offset phase B

Offset phase C

Total DER Reactive Power Injection

0

100

200

Phase A

Phase B

Phase C

300
400
Time (s)

500

600

Fig. 8: 30% DER oscillation attack with ADAM-based ARS

complex policy structure (i.e., a neural network). An attack
on 30% of DER VV/VW controllers aimed at creating large
voltage imbalances is depicted in Fig. 9. This baseline case
demonstrates the effect of the attack in the system without
utilizing the policy trained by Adam-ARS to control the

V. CONCLUSIONS

This paper explored the use of an Adam-based Aug-
mented Random Search (ARS) algorithm to directly search
for optimal policies that manage DER smart inverter Volt-
VAR/Volt-Watt control functions to mitigate the effects of
cyber attacks. In the event that a malicious entity gains
the ability to manipulate the VV/VW settings in a portion
of DER in a given system, the trained optimal policy was
shown to be effective in reconﬁguring the remaining non-
compromised DER VV/VW controllers in the system to
mitigate large oscillations and large imbalances in system
voltages. Compared to previous attempts, we demonstrated
that
the Adam-ARS approach can learn optimal policies
signiﬁcantly faster than Proximal Policy Optimization (PPO),
faster than ARS, and with less variance in the reward com-
pared to ARS. Additionally, the Adam-ARS algorithm is able
to learn a linear policy for defense against voltage oscillation
training optimal
attacks. The results herein indicate that
control policies for DER cybersecurity can be performed
with fewer computational resources than previously believed,
thereby allowing grid-managing entities with less expertise
and ﬁnancial resources the ability to use this technology for
their systems.

)
.

.

u
p
(

e
g
a
t
l
o
v

)

%

(

U
V

η
∆

r
a
V
M

4
0
.
1

8
9
.
0

3
9
.
0

6

4

2

0

1
.
0

0

1
.
0
−

1

0

1
−

attack

Phase A
Phase B
Phase C

Largest Network Voltage Imbalance

Agent action

Offset phase A

Offset phase B

Offset phase C

Total DER Reactive Power Injection

0

100

200

Phase A

Phase B

Phase C

300
400
Time (s)

500

600

Fig. 10: 30% DER imbalance attack with ADAM-based ARS

While in this work we have demonstrated the effectiveness
in adapting the learning rate during the training process,
future work will explore adapting the variance of the ex-
ploration noise used for ﬁnite-difference approximation of
the reward gradient. We believe that this feature may further
reduce training time and possibly yield superior rewards
compared to the proposed algorithm.

REFERENCES

[1] “Proposed Clean Energy Standard Could End Power Plant Greenhouse
Gas Emissions By 2035,” https://www.npr.org/2021/08/11/102683106
7/proposed-clean-energy-standard-could-end-power-plant-greenhous
e-gas-emissions-by, [Online; published Aug. 11, 2021].

[2] “DOE Announces Goal to Cut Solar Costs by More than Half by
2030,” https://www.energy.gov/articles/doe-announces-goal-cut-solar
-costs-more-half-2030, [Online; published Mar. 25, 2021].

[3] IEEE Standards Coordinating Committee 21, “IEEE Standard for
Interconnection and Interoperability of Distributed Energy Resources
with Associated Electric Power Systems Interfaces,” IEEE Std 1547-
2018 (Revision of IEEE Std 1547-2003), pp. 1–138, April 2018.
[4] S. Sahoo, T. Dragiˇcevi´c, and F. Blaabjerg, “Cyber Security in Control
of Grid-Tied Power Electronic Converters–Challenges and Vulnera-
bilities,” IEEE Journal of Emerging and Selected Topics in Power
Electronics, 2019.

[5] “800,000 Microinverters Remotely Retroﬁtted on Oahu in One Day,”
https://spectrum.ieee.org/energywise/green-tech/solar/in-one-day-800
000-microinverters-remotely-retrofitted-on-oahu, [Online; accessed
June-2019].

[6] C. Roberts, S.-T. Ngo, A. Milesi, S. Peisert, D. Arnold, S. Saha,
A. Scaglione, N. Johnson, A. Kocheturov, and D. Fradkin, “Deep
Reinforcement Learning for DER Cyber-Attack Mitigation,” in 2020
IEEE International Conference on Communications, Control, and
Computing Technologies for Smart Grids (SmartGridComm).
IEEE,
2020, pp. 1–7.

[7] C. Roberts, S.-T. Ngo, A. Milesi, A. Scaglione, S. Peisert, and
D. Arnold, “Deep Reinforcement Learning for Mitigating Cyber-
Physical DER Voltage Unbalance Attacks,” in 2021 American Control
Conference (ACC).
IEEE, 2021, pp. 2861–2867.

[8] J. C. Spall, Introduction to Stochastic Search and Optimization.

John

Wiley & Sons, Ltd, 2003.

[9] M. Krstic, Real-Time Optimization by Extremum-Seeking Control.

John Wiley & Sons, Ltd, 2003.

[10] T. Salimans, J. Ho, X. Chen, S. Sidor, and I. Sutskever, “Evolution
Strategies as a Scalable Alternative to Reinforcement Learning,” https:
//arxiv.org/abs/1703.03864, 2017.

[11] H. Mania, A. Guy, and B. Recht, “Simple random search provides a
competitive approach to reinforcement learning,” https://arxiv.org/ab
s/1803.07055, 2018.

[12] J. Duchi, E. Hazan, and Y. Singer, “Adaptive subgradient methods
for online learning and stochastic optimization,” Journal of Machine
Learning Research, vol. 12, no. 61, pp. 2121–2159, 2011.

[13] B. Seal, “Common Functions for Smart Inverters, 4th Ed.” Electric

Power Research Institute, Tech. Rep. 3002008217, 2017.

[14] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimiza-

tion,” https://arxiv.org/abs/1412.6980, 2017.

[15] G. Hinton, N. Srivastava, and K. Swersky, “Lecture notes on Neural

Networks for Machine Learning - Lecture 6a.”

[16] X. Glorot and Y. Bengio, “Understanding the difﬁculty of training
deep feedforward neural networks,” in Proceedings of the Thirteenth
International Conference on Artiﬁcial Intelligence and Statistics, ser.
Proceedings of Machine Learning Research, Y. W. Teh and M. Tit-
terington, Eds., vol. 9. Chia Laguna Resort, Sardinia, Italy: PMLR,
13–15 May 2010, pp. 249–256.

APPENDIX

Hyperparameter
α
µ
β0
β1
λADAM
γ
λP P O
(cid:15)
episodes
activation function
hidden layers
σy
σu
σa
σ0
σp

ARS
1 × 10−2
3 × 10−2
0.9
0.999
1 × 10−8
–
–
–
16
tanh
dense(16,16)
300
300
0.5
1
1

ADAM-based ARS
5 × 10−2
3 × 10−2
0.9
0.999
1 × 10−8
–
–
–
16
tanh
dense(16,16)
300
300
0.5
1
1

PPO
1 × 10−3
-
-
-
-
0.5
0.95
0.1
8
tanh
dense(16,16)
300
300
0.5
1
1

TABLE I: Hyperparameters of the network, training and reward for
unbalance attack

Hyperparameter
α
µ
β0
β1
λADAM
γ
λP P O
(cid:15)
episodes
activation function
hidden layers
σy
σu
σa
σ0
σp

ARS
3 × 10−3
1 × 10−2
0.9
0.999
1 × 10−8
–
–
–
8
–
linear
300
300
0.5
1
1

ADAM-based ARS
3 × 10−3
1 × 10−2
0.9
0.999
1 × 10−8
–
–
–
8
–
linear
300
300
0.5
1
1

PPO
1 × 10−3
-
-
-
-
0.5
0.95
0.1
8
tanh
dense (16, 16)
300
300
0.5
1
1

TABLE II: Hyperparameters of the network, training and reward
for oscillation attack

