1
2
0
2

y
a
M
1
3

]

R
P
.
h
t
a
m

[

1
v
7
8
0
0
0
.
6
0
1
2
:
v
i
X
r
a

Lecture Notes on Stationary Gamma Processes

Robert L. Wolpert

June 2, 2021

Abstract

7→

For each λ > 0 and every square-integrable inﬁnitely-divisible (ID) distribution there
exists at least one stationary stochastic process t
Xt with the speciﬁed distribution
for X1 and with ﬁrst-order autoregressive (AR(1)) structure in the sense that the au-
tocorrelation of Xs and Xt is exp(
) for all indices s, t. For the special case of
t
|
the standard Normal distribution, the process Xt is unique— namely, the ﬁrst-order
autoregressive Ornstein-Uhlenbeck velocity process. The process Xt is also uniquely
determined if X1 is accorded the unit rate Poisson distribution.

For the Gamma distribution, however, Xt is not determined uniquely.

In these
lecture notes we describe six distinct processes with the same univariate marginal
distributions and AR(1) autocorrelation function. We explore a few of their properties
and describe methods of simulating their sample paths.

s
|

−

−

λ

1

Introduction

T

T

T

For ﬁxed α, β > 0 these notes present six diﬀerent stationary time series, each with Gamma
Ga(α, β) univariate marginal distributions and autocorrelation function ρ|s−t| for
Xt ∼
Xs, Xt. Each will be deﬁned on some time index set

= Z or

, either

= R.

Five of the six constructions can be applied to other Inﬁnitely Divisible (ID) distribu-
tions as well, both continuous ones (normal, α-stable, etc.) and discrete (Poisson, negative
binomial, etc.). For speciﬁcally the Poisson and Gaussian distributions, all but one of them
(the Markov change-point construction) coincide— essentially, there is just one “AR(1)-like”
Gaussian process (namely, the AR(1) process in discrete time, or the Ornstein-Uhlenbeck
process in continuous time), and there is just one AR(1)-like Poisson process. For other ID
distributions, however, and in particular for the Gamma, each of these constructions yields
a process with the same univariate marginal distributions and the same autocorrelation but
with diﬀerent joint distributions at three or more times.

First, by “Ga(α, β)” we mean the Gamma distribution with mean α/β and variance α/β2,

1

 
 
 
 
 
 
i.e., with shape parameter α and rate parameter β. The pdf and chf are given by:

f (x

χ(ω

|

|

α, β) =

βα
Γ(α)

xα−1e−βx1{x>0}

iω/β)−α

α, β) = (1

−
= exp

eiωu αe−βu u−1 du

.

(1)

(cid:27)

R+

Z

−

(cid:26)

ξj of independent random variables ξj

The sum ξ+ :=
parameter also has a Gamma distribution, ξ+ ∼
with sum α+ := Σαj <
∞
ν(du) = αe−βu u−11{u>0}du.

Ga(αj, β) with the same rate
R+ are summable
. Eqn (1) shows that the “L´evy measure” for this distribution is

Ga(α+, β), if

αj} ⊂

P

{

ind
∼

From the Poisson representation of ID distributions

(du)

for
depend only on the L´evy measure ν(du): for large u

we can show that the extremal properties of ID random variables
R+,

ν(du)

N

∼

Po

(cid:0)

(cid:1)

X =

(du)

u

N

R
Z

P[X > u]

P

≈
= 1

(u,

)

∈
=

]

N

exp
(cid:2)
(cid:0)
−
(u,

∞

−

∅
(u,
ν
)
(cid:1)
∞
(α/βu)e−βu,
(cid:0)

)
(cid:0)
∞
from which one might mount a study of the multivariate extreme properties of the six
processes presented below.

(2)

≈

≈

(cid:0)

(cid:1)

(cid:1)

ν

2 Six Stationary Gamma Processes

2.1 The Gamma AR(1) Process

Fix α, β > 0 and 0

ρ < 1. Let X0 ∼

≤

Ga(α, β) and for t

∈

N deﬁne Xt recursively by

Xt := ρXt−1 + ζt

(3)

{

ζt}

with chf Eeiωζt = (1
iω/β)−α(1
for iid
positive-deﬁnite, with L´evy measure νζ(du) = α
of generating
The process
t

−α (easily seen to be
β−iω
β−iρω
u−11{u>0}du). A simple way
−
with this distribution from [Walker, 2000] is presented in the Appendix.
Ga(α, β) for every
has Gamma univariate marginal distribution Xt ∼

R+ and, at consecutive times 0, 1, joint chf

iρω/β)α =
e−βu/ρ

ζt}
{
Xt}
{

−
e−βu

(cid:2)
(cid:3)

−

(cid:3)

(cid:2)

∈

χ(s, t) = E exp(isX0 + itX1)

= E exp(i(s + ρt)X0 + itζ1)

(1

−

=

h

i(s + ρt)/β) (1

itρ/β

1

−

it/β)

−α

.

−

i

2

(4)

6
Since this is asymmetric in s, t, the process is not time-reversible; this is also evident from
the observation that

P[Xt ≥

0] = 1 > P[ζt ≤

ρXt−1] = P[ζt ≥

ρ2)/ρ] = P[Xt−1 ≥
Ga(α, β) at all times t, and has autocorrelation
This process has marginal distribution Xt ∼
Corr
Xs, Xt) = ρ|s−t| (easily found from either (3) or (4)). It is clearly Markov (from (3)),
and can be shown to have inﬁnitely-divisible (ID) multivariate marginal distributions of all
orders, since the

Xt−1(1

are ID.

ρXt].

−

(cid:0)

ζt}

{

2.2 Thinned Gamma Process

Ga(α1, β) and Y

If X
∼
U := X/Z
change of variables x = uz, y = (1

Ga(α+, β) and
Be(α1, α2) are also independent (where α+ := α1 + α2), because under the
u)z with Jacobian J(u, z) = z,

Ga(α2, β) are independent, then Z := X + Y

∼

∼

∼

−

f (u, z) =

(cid:26)

(cid:26)

=

=

βα1
Γ(α1)

xα1−1e−βx

βα2
Γ(α2)

βα+
Γ(α1)Γ(α2)
Γ(α+)
Γ(α1)Γ(α2)

(cid:27) (cid:26)
uα1−1(1

(cid:27)
uα1−1(1

yα2−1e−βy

J(u, z)

(cid:27)

u)α2−1zα+−2e−βzz

−

u)α2−1

−

(cid:27) (cid:26)

βα+
Γ(α+)

zα+−1e−βz

.

(cid:27)

Thus if Z
∼
and Y = (1

(cid:26)
Ga(α1 + α2, β) and U
U)Z

−

∼

Ga(α2, β) are independent too. Let

∼

Be(α1, α2) are independent then X = UZ

Ga(α1, β)

∼

and, for t

N (or t

∈

where

Ga(α, β)

X0 ∼
N, resp.) set

∈ −

Xt := ξt + ζt

Xt−1,
Ga(α ¯ρ, β)

ξt := Bt ·
ζt ∼

Be(αρ, α ¯ρ),

Bt ∼

(or ξt = Bt ·

Xt+1, resp.), where ¯ρ := (1

ρ) and all the

−

Bt}

{

and

ζt}

{

are independent.

Then, by induction, ξt ∼
Ga(α, β). Thus

Ga(α ¯ρ, β) are independent, with sum Xt ∼
is a Markov process with Gamma univariate marginal distribution

Ga(αρ, β) and ζt ∼

Xt}

{

3

Ga(α, β), now with symmetric joint chf

Xt ∼

χ(s, t) = E exp(isX0 + itX1)

= E exp
= (1

is(X0 −
{
is/β)−α¯ρ(1

−

ξ1) + i(s + t)ξ1 + itζ1}
i(s + t)/β)−αρ(1
−
−

it/β)−α¯ρ

(5)

and autocorrelation Corr
Xs, Xt) = ρ|s−t|. The process of passing from Xt−1 to ξt = Xt−1Bt
is called thinning, so Xt is called the thinned gamma process. A similar construction is
available for any ID marginal distribution.

(cid:0)

2.3 Random Measure Gamma Process

(dx dy) be a countably additive random measure that assigns independent random
Let
G
, β) to disjoint Borel sets Ai ∈ B
variables
(this is
possible by the Kolmogorov consistency conditions, and is illustrated in the Appendix) and,
for λ :=

log ρ, consider the collection of sets:

(R2) of ﬁnite area

Ga(α

Ai|

Ai|

(Ai)

∼

G

|

|

−

shown in Figure (1) whose intersections have area

(cid:8)

Gt :=

(x, y) : x

R, 0

∈

y < λe−2λ|t−x|

≤
Gs ∩

|

= e−λ|s−t|. For t

(cid:9)

Gt|

= R, set

∈ T

6
.
0

4
.
0

2
.
0

0
.
0

G1

G2

G3

−2

−1

0

1

2

3

4

5

Figure 1: Random measure construction of process Xt =

(Gt)

G

Time t

Xt :=

(Gt).

(6)

partition R2 into n(n + 1)/2 sets of
For any n times t1 < t2 <
Gti)c), so each Xti can be written as the sum of
ﬁnite area (and one with inﬁnite area, (
some subset of n(n + 1)/2 independent Gamma random variables. In particular, any n = 2
variables Xs and Xt can be written as

Gti}

· · ·

∪

{

G
< tn the sets

Xs =

(Gs\

G

Gt) +

(Gs ∩

G

Gt),

Xt =

(Gt\

G

Gs) +

(Gs ∩

G

Gt)

4

just as in the thinning approach of Section (2.2), so both 1-dimensional and 2-dimensional
marginal distributions for the random measure process coincide with those for the thinning
process. Again the joint chf is

χ(s, t) = E exp(isX0 + itX1)

= (1

is/β)−α¯ρ(1

i(s + t)/β)−αρ(1

it/β)−α¯ρ

(7)

−
and the autocorrelation is Corr
or, for integer times, ρ|s−t| for
λ
ρ := exp(
λ). The distribution for consecutive triplets diﬀers from those of the Thinned
Gamma Process, however, an illustration that the thinning process is Markov but the random
measure is not. The Random Measure process does feature inﬁnitely-divisible (ID) marginal
distributions of all orders, while the thinned process does not.

Xs, Xt) = exp

t
|

−

−

−

−

−

s

(cid:0)

(cid:1)

(cid:0)

|

2.4 The Markov change-point Gamma Process

{

ζn : n

Let
∈
Poisson process indexed by t
t <

Z
}

iid
∼

Ga(α, β) be iid Gamma random variables and let Nt be a standard
< s <

s) for all

Po(t

Ns)

R (so N0 = 0 and (Nt −

−∞

−

∼

∈

, with independent increments), and set

∞

Then each Xt ∼
∈
ρ|s−t|) or independent— reminiscent of a Metropolis MCMC chain. The chf is

R, Xs and Xt are either identical (with probability

Ga(α, β) and, for s, t

Xt := ζn,

n = Nλt.

χ(s, t) = E exp(isX0 + itX1)

= ρ

1

−

i(s + t)/β

−α + ¯ρ(1

is/β)−α(1

it/β)−α

(8)

−

−

Ga(α, β) and the autocorrelation function

and once again the marginal distribution is Xt ∼
is Corr

Xs, Xt) = ρ|s−t|.

(cid:1)

(cid:0)

(cid:0)

2.5 The Squared O-U Gamma Diﬀusion

Let
Gaussian processes with covariance Cov

Zi}

{

iid
∼

Zi(s), Zj(t)

= exp

OU(λ/2, 1) be independent Ornstein-Uhlenbeck velocity processes, mean-zero

s

λ
2 |

t
|

−

−

(cid:0)

(cid:1)

δij, and set

N and β
for n
EZi(s)2Zi(t)2 = 1 + 2 exp(

∈

∈

No(0, 1), so EZi(t)2 = 1 and EZi(t)4 = 3; it follows that
Ga(α, β) for α = n/2, with EXs = α/β and

(cid:0)
Xt :=

1
2β

n

(cid:1)
Zi(t)2

i=1
X

R+. Note Zi(t)
∼
). Then Xt ∼
t
|
n
1 + 2 exp(

|
−
EXsXt =

λ

s

−
1
4β2
(cid:8)
α2 + α exp

(cid:0)

=

λ

s

−
s

−

|
t
|

(cid:1)

−

β2
(cid:0)

λ

|

5

)

t
|

−

+ n(n

1)

−

(cid:1)

(cid:9)

so the autocovariance is Cov
is Corr

Xs, Xt) = ρ|s−t| for ρ := exp(

Xs, Xt) = α

β2 e−λ|s−t| and the autocorrelation at integer times

λ). The chf at consecutive integer times is

(cid:0)

(cid:0)

−
χ(s, t) = E exp(isX0 + itX1)

−
distinct from (4), (5)=(7), and (8), so this process is new. Itˆo’s formula is used in [Wolpert and Brown,
2021] to show that Xt has stochastic diﬀerential equation (SDE) representation

−

−

(cid:1)

(cid:0)

=

1

i(s + t)/β

st(1

ρ)/β2

−α

,

(9)

(10)

(11)

Xt = X0 −
and hence has generator Aφ(x) = (∂/∂ǫ)E[φ(Xt+ǫ)

Xs −
(cid:0)

α/β

0
Z

2λ

(cid:1)

ds + 2

t

t

λ/β

Xs dWs

p

Xt = x]

|

0
Z
ǫ=0 given by

p

Aφ(x) =

2λ(x

−

−

α/β)φ′(x) + (2λ/β)xφ′′(x),

(cid:12)
(cid:12)

which we will use to distinguish this process from that of Section (2.6). While the construc-
tion above required half-integer values for α, (9) is positive-deﬁnite and the SDE (10) has a
unique strong solution for all α > 0, so a time-reversible stationary Markov diﬀusion process
exists with this distribution. Cox et al. [1985, Eqn (17)], building on [Feller, 1951], found
the transition kernel for this process:

f (y, t

|

x, s) = ce−u−v

v
u

(α−1)/2

I(α−1)

2√uv

,

where

c := β/

1

e−2λ|t−s|

,

−

(cid:17)

(cid:16)
(cid:0)
u := cye−2λ|t−s|,

(cid:1)

v := cx.

where Iq(x) denotes the modiﬁed Bessel function of the ﬁrst kind of order q. With this
one can construct likelihood functions and generate posterior samples, MLEs, etc. for the
parameters of this process. Cox et al. show that the process Xt is strictly positive at all
times if α

1, but occasionally reaches zero for α < 1.

(cid:3)

(cid:2)

≥

2.6 Continuously Thinned Gamma Process

Pick a large integer n and set ǫ := 1/n, q := exp(
X0 ∼

Ga(α, β) and, for integers i, j

−

∈
Ga(αp, β)

N, draw independently
bj ∼

Be(αp, αq).

Set:

ζi ∼

λǫ), and p := 1

q = λǫ + o(ǫ). Draw

−

X0 = X0
Xǫ = X0(1
X2ǫ = Xǫ (1
= X0(1
X3ǫ = X0(1

b1)
b2)
b1)(1
b1)(1

−
−
−
−

+ ζ1

b2)
b2)(1

−
−

−

+ ζ1(1
b3) + ζ1(1

b2)
b2)(1

−
−

−

+ ζ2
+ ζ2
b3) + ζ2(1

b3) + ζ3

−

6

and, in general,

k

Xkǫ = X0

(1

j=1
Y

bj) +

−

k

ζi

i=1 (
X

Yi<j≤k

(1

bj)

.

)

−

(12a)

In the limit as n
t the products converge to the product integral of the beta
process introduced by Hjort [1980, §3] [and described lucidly by Thibaux and Jordan, 2007,
§2] and the sum to an ordinary gamma stochastic integral,

and kǫ

→ ∞

→

Xt = X0

dB(s)] +

[1

−

[1

−

0 
Z


Ys∈(r,t]

dB(s)]




Ys∈(0,t]

ζ(dr),

(12b)

t

Ga

∼

αλ dr, β

where ζ(dr)
process, i.e., an SII L´evy process with L´evy measure
(cid:1)
νB(du) = λα u−1(1

(cid:0)

is a Gamma random measure and B(s)



u)α−1 1{0<u<1} du

−



∼

BP

α, λ ds

is a Beta

(cid:0)

(cid:1)

with constant “concentration function” α(s)
λ(ds) = λds. The product integral can be written as a ratio

≡

α and translation-invariant “base measure”

[1

dB(s)] =

−

Ys∈(r,t]
dB(s)] satisﬁes

F (t)
F (r)

1
1

−
−

where F (t) =

s∈(t,∞)[1

−

Q

dF (t)

F (t)

1

−

= dB(t),

Bt =

dF (s)

Z(0,t]

1

−

F (s)

.

2.6.1 Generator

The Gamma process of Eqn (12b) is stationary and Markov, with generator

∞

Aφ(x) =

0
Z

x

+

0
Z

(cid:2)

φ(x + u)

−

φ(x)

αλu−1e−βu du

(cid:2)
φ(x

u)

−

−

φ(x)

(cid:3)
αλu−1(1

u/x)α−1 du

−

(cid:3)

(13)

(14)

Because this diﬀers from (11) (and in particular because it is a non-local operator, showing
Xt has jumps), this process is new. Once again the one-dimensional marginal distributions
are Xt ∼

Ga(α, β) and the autocorrelation is Corr(Xs, Xt) = exp

t
|

−

−

λ

s

|

.

(cid:0)

(cid:1)

7

3 Discussion

≥

We have now constructed six distinct processes that share the same univariate marginal
distribution and autocorrelation function, but which all diﬀer in their n-variate marginal
distributions for n
3. Some are Markov, some not; some are time-reversible, some not;
some have ID marginal distributions of all orders, some don’t. Similar methods can be used
to construct AR(1)-like processes with any ID marginal distribution, such as those listed
on p. 10; only in the two cases of Gaussian and Poisson do all these constructions coincide.
Many of these are useful for modeling time-dependent phenomena whose dependence falls
oﬀ over time, but for which traditional Gaussian methods are unsuitable because of heavy
tails, or integer values, or positivity, or for other reasons. I know of very little work (yet!)
exploring inference for processes like these; a beginning appears in [Wang, 2013, §3]. Code
in R to generate samples from each of these six processes is available on request.

The author would like to thank Lawrence D. Brown for many fruitful discussions about ID
processes, and Victor Pe˜na for helping develop methods of distinguishing these six processes
from observations.

8

Appendix

Proposition 1 (Walker [2000]). The innovations ζt in Eqn (3) can be constructed succes-
sively as follows:

Proof. For ω

λt ∼
R,

∈

Ga(α, 1),

Nt |

λt ∼

Po

1−ρ
ρ λt

,

ζt |

Nt ∼

Ga

Nt, β
ρ

.

(cid:0)

(cid:1)

(cid:0)

(cid:1)

iωρ/β)−n

1−ρ
ρ λt

n exp

−

1−ρ
ρ λt

/n!

(cid:1)

−

(cid:0)

(cid:1)

Eeiζtω

Eeiζtω

Eeiζtω

iωρ/β)−Nt

Nt = (1
∞

|

−

λt =

(1

|

n=0
X
= exp

(cid:16)
i

−

=

1

h

i

ρ)ω
iρω
ρ)ω
iρω

(1
β
(1
β

−
−
−
−

(cid:0)

λt

(cid:17)
−α

i

=

β
β

iω
iρω

−
−

h

−α

.

i

Poisson and Gamma SII Processes

The chf for a Poisson random variable X

∼
∞

Po(ν) is

χX (θ) = E[eiθX ] =

eiθk

Xk=0

νk
k!

(cid:26)

e−ν

= e(eiθ−1)ν

(cid:27)

so for any u

∈

R the re-scaled random variable Y := uX has chf

χuX(θ) = E[eiθuX ] = χX (uθ)
= e(eiθu−1)ν

and a linear combination Y :=

ujXj of independent Xj ∼

Po(νj) has chf

P
χY (θ) =

e(eiθuj −1)νj

j n
Y

= exp

(eiθuj

(

j
X

o

−

1)νj

)

= exp

(eiθu

−

1)ν(du)

(cid:27)

R
(cid:26)Z

ν(du) =

νjδuj (du)

X
9

(15a)

(15b)

for the discrete measure

(16a)

(16b)

that assigns mass νj to each point uj, provided the sum in (15a) converges. Of course the
sum converges if it has only ﬁnitely-many terms, or even if there are inﬁnitely-many with
2), but that condition isn’t actually necessary. Since also

eiθu

1

, the random variable Y will be well-deﬁned and ﬁnite provided that

νj <
1

−

eiθu
|
P

∞
| ≤ |

(because
θu

|

|

−

| ≤

or, in integral form,

1

uj|

∧ |

νj <

∞

(cid:1)

j
X

(cid:0)

1

u

|

∧ |

ν(du) <

.

∞

R
Z

(cid:1)
A random variable with chf of form (15a) for a sequence satisfying (16a) has a “compound
Poisson” distribution; one with the more general chf of form (15b) for a measure satisfying
(16b) is called Inﬁnitely Divisible, or ID.

(cid:0)

ID Distributions

For any σ-ﬁnite measure satisfying (16b) it’s easy to make a stochastic process with sta-
tionary independent increments of the more general form of (15b), beginning with a Poisson
random measure

R+ with intensity measure E

(du ds) = ν(du) ds:

(du ds) on R

N

×

N

Xt :=

(du ds).

u

N

R×(0,t]

Z Z

(17)

This is a right-continuous independent-increment nondecreasing process that begins at X0 =
E at rate ν(E) for
0 with jumps ∆t = [Xt −
R. The Poisson process itself is the special case where ν(E) = ν1{1∈E}, with
any Borel E
⊂
jumps of magnitude ∆t = 1 at rate ν

limsրt Xs] of magnitudes ∆t ∈
R+.

[Xt −

Xt−]

≡

Khinchine and L´evy [1936] showed that a random variable Y has a chf of the form (15b)
if and only1 if, for every n
ζn as the sum of n iid random
variables ζj. This property is called “Inﬁnite Divisibility” (abbreviated ID), and the processes
we have constructed whose increments have this property are called “SII” processes for their
stationary independent increments. Examples of ID distributions (or SII processes) and their
L´evy measures include:

N, one can write Y = ζ1 +

· · ·

∈

∈

Poisson
Negative Binomial
Gamma
α-Stable
Symmetric α-Stable SαS(α, γ)
Cauchy

Po(λ)
NB(α, p)
Ga(α, β)
St0(α, β, γ, δ)

Ca(δ, γ)

k δk(du),

ν(du) = λδ1(du)
k∈N α qk
ν(du) =
ν(du) = αu−1e−βu1{u>0} du
P
ν(du) = αγ
π Γ(α) sin πα
2 |
ν(du) = αγ
π Γ(α) sin πα
2 |
ν(du) = γ
−2 du.
u
π |

u
u

|
|

|

q := (1

p)

−

−α−1(1 + β sgn u) du
−α−1 du

1For nonnegative random variables this is true as stated, but a slightly more general form is necessary
for real-valued ID random variables, with a condition on ν(du) somewhat weaker than (16b) (see (18c))—
if you get interested, ask me about “compensation”. This is needed for the Ca(δ, γ) example and, for α
1,
the St0(α, β, γ, δ) and SαS(α, γ) examples below.

≥

10

∈

for each n

N there must exist iid random variables

The deﬁning condition for a random variable Y to be “Inﬁnitely Divisible” (ID) is that
such that Y and
n
j=1 ζj have the same distribution. This is clearly equivalent to the condition that every
power χα(ω) of the characteristic function χ(θ) := E exp(iθY ) must also be a characteristic
P
function (i.e., must be positive-deﬁnite) for each inverse integer α = 1/n, because we can
to be iid with chf χ1/n(ω) and verify that their sum has chf χ(ω). Less obvious
just take
but also true is that Y is ID if and only if χα(ω) is positive-deﬁnite for all real α > 0, and
even less obvious is the Khinchine and L´evy theorem that χ must take the speciﬁc form

ζj : 1

ζj}

n
}

≤

≤

{

{

j

χ(θ) = exp

iθδ

(cid:26)

θ2σ2/2 +

−

eiθu

−

1

ν(du)

(cid:3)

(cid:27)

R

Z

(cid:2)

(18a)

0, and Borel measure ν satisfying ν(

) = 0 and (16b) or, a little

0

{

}

for some δ
∈
more generally, the form

≥

R, σ2

χ(θ) = exp

iθδ

(cid:26)

θ2σ2/2 +

−

R

Z

eiθu

1

−

−

iθh(u)

ν(du)

(cid:2)

(cid:3)

for any bounded function h that satisﬁes h(u) = u + O(u2) near u
u1{|u|<1} or u/(1 + u2)) and a Borel measure ν satisfying the weaker restriction

≈

(18b)

(cid:27)
0 (like arctan u or

Some properties of ID distributions beyond the scope of these notes, but covered in [Steutel and van Harn,
2004] (see also [Bose et al., 2002]), include:

u2

ν(du) <

1

∧

.

∞

(18c)

R
Z

(cid:0)

(cid:1)

Theorem 1 (S&vH, Thm 2.13). Let χ(θ) be the chf of an Inﬁnitely Divisible distribution.
C, then
θ
= 0
Then (
∀
. Thus, ID chfs do not vanish on R or anywhere in C that χ(θ) is
θ
Ω)
(
∀
∈
analytic.

. Also, if χ(θ) is analytic in some open domain Ω

R)
∈
χ(θ)
{

χ(θ)
{
= 0

⊂

}

}

Theorem 2 (S&vH, Thm 9.8). Let X be an inﬁnitely divisible random variable that is not
normal or degenerate. Then the two-sided tail of X satisﬁes

lim
x→∞

−

log P[
X
|
|
x log x

> x]

= c

for a number 0
not degenerate has a normal distribution if and only if the same limit is c =

given by c−1 = max[ν(R+), ν(R−)]. An ID random variable that is

c <

∞

≤

.

∞

Theorem 3 (S&vH, Prop 2.3). No non-degenerate bounded random variable is ID

This one’s easy enough to prove. If
n
j=1 ζj for iid

k∞ = B <
ζjk∞ = B/n and σ2 := V(X) = nV(ζj)

ζj}

then

∞

X

k

and X has the same distribution as
B2/n, so σ2 = 0

nEζ 2

j ≤

≤

k
and X must be degenerate.
P

{

11

6
6
Gamma Variables & Processes

The Gamma distribution X

Ga(α, β) with mean α/β and variance α/β2 has chf

∼
χX(θ) = E[eiθX]

∞

eiθx

=

(cid:26)
iθ/β

0
Z
1
=
−
= exp
(cid:0)
{−

= exp

−

(cid:26)

0

Z

βα
Γ(α)
−α

xα−1e−βx

dx

(cid:27)

α log(1
(cid:1)
∞

iθ/β)

−
eiθu

1

−

(cid:0)

(cid:1)

}
αu−1e−βu du

,

(cid:27)

exactly of the form of (15b) with L´evy measure

ν(du) = αu−1e−βu1{u>0} du.

This measure, while inﬁnite, does satisfy condition (16b):

1

u

|

∧ |

ν(du) =

(cid:1)

R

Z

(cid:0)

0
Z

≤

0

Z

1

u

|

|

αu−1e−βu du +

1

αu−1e−βu du

∞

∞
(cid:0)
αe−βu du = α/β <

(cid:1)

(cid:0)

(cid:1)

1

Z

.

∞

Thus gamma-distributed random variables are ID and the SII gamma process

Xt =

(du ds)

u

N

R×(0,t]

Z Z

has inﬁnitely-many non-negative “jumps” ∆t = Xt −
b
Their sum
}
≤

∆t : a < t

≤
Xa is ﬁnite, however, with probability distribution

Xt− in any time interval a < t

b.

{
P

= Xb −
Xb −
(dx dy)

Xa ∼

Ga

α(b

a), β

.

−

The gamma random measure
struction,

G

Ga(α dx dy, β) of Section (2.3) has a similar con-

(cid:0)

(cid:1)

∼

(A) =

G

R×A

Z Z Z

(du dx dy)

u

N

as the sum of the heights uj of a Poisson cloud of points (uj, xj, yj) for which (xj, yj)
A.
Wolpert and Ickstadt [1998] show how to simulate such random measures very eﬃciently,
drawing the jumps

in monotone decreasing order.

∈

uj}

{

12

References

Arup Bose, Anirban DasGupta, and Herman Rubin. A contemporary review and bibliog-
raphy of inﬁnitely divisible distributions and processes. Sankhy¯a, Ser. A, 64(3):763–819,
2002. doi: 10.2307/25051430. Special issue in memory of D. Basu.

John C. Cox, Jonathan E. Ingersoll, Jr., and Stephen A. Ross. A theory of the term structure

of interest rates. Econometrica, 53(2):385–408, 1985. doi: 10.2307/1911242.

William Feller. Two singular diﬀusion problems. Annals of Mathematics, 54(1):173–182,

1951. doi: 10.2307/1969318.

Nils Lid Hjort. Nonparametric Bayes estimators based on beta processes in models for life

history data. Ann. Stat., 18(3):1259–1294, 1980. doi: 10.1214/aos/1176347749.

Alexander Ya. Khinchine and Paul L´evy. Sur les lois stables. Comptes rendus hebdomadaires
des seances de l’Acad´emie des sciences. Acad´emie des science (France). Serie A. Paris,
202:374–376, 1936.

Frederik W. Steutel and Klaas van Harn. Inﬁnite Divisibility of Probability Distributions on
the Real Line, volume 259 of Monographs and Textbooks in Pure and Applied Mathematics.
Marcel Dekker, New York, NY, 2004. ISBN 0-8247-0724-9. doi: 10.1201/9780203014127.

Romain Thibaux and Michael I. Jordan. Hierarchical beta processes and the Indian
buﬀet process.
In Marina Meila and Xiaotong Shen, editors, Proceedings of the
Eleventh International Conference on Artiﬁcial Intelligence and Statistics (AISTATS
2007), March 21–24, San Juan, Puerto Rico, volume 2, pages 564–571. PMLR, 2007.
URL http://proceedings.mlr.press/v2/thibaux07a.html.

Stephen G. Walker. A note on the innovation distribution of a gamma distributed autore-
gressive process. Scand. J. Stat., 27(4):575–576, 2000. doi: 10.1111/1467-9469.00208.

Jianyu Wang. Bayesian Modeling and Adaptive Monte Carlo with Geophysics Applications.

PhD thesis, Duke University Statistical Science Department, 2013.

Robert L. Wolpert and Lawrence D. Brown. Markov inﬁnitely-divisible stationary time-

reversible integer-valued processess. On arXiv.

Robert L. Wolpert and Katja Ickstadt. Poisson/gamma random ﬁeld models for spatial

statistics. Biometrika, 85(2):251–267, 1998. doi: 10.1093/biomet/85.2.251.

13

