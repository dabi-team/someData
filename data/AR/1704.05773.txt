7
1
0
2

r
p
A
9
1

]

R
C
.
s
c
[

1
v
3
7
7
5
0
.
4
0
7
1
:
v
i
X
r
a

Derivation of the Asymptotic Eigenvalue Distribution
for Causal 2D-AR Models under Upscaling

David V´azquez-Pad´ın∗, Fernando P´erez-Gonz´alez∗, and Pedro Comesa˜na-Alfaro∗

Abstract

This technical report complements the work in [1], describing the derivation of the asymptotic
eigenvalue distribution for causal 2D-AR models under an upscaling scenario. In particular, Sec-
tion 1 tackles the analytical derivation of the asymptotic eigenvalue distribution of the sample
autocorrelation matrix corresponding to genuine and upscaled images. The pseudocode of the de-
rived approaches for resampling detection and resampling factor estimation in [1] is included in
Section 2. Please refer to [1] for a complete description of the resampling process and the notation
used throughout this document.

1 Asymptotic Eigenvalue Distribution of the Sample Autocorre-

lation Matrix of an Image

The empirical distribution of the eigenvalues of the sample autocorrelation matrix corresponding to
a genuine image (or its singular values, thereof), can be approximated by using a two-dimensional
autoregressive (2D-AR) random ﬁeld as underlying model of the image. For the sake of simplic-
ity, we assume that the covariance function of a natural image is separable and we model linear
dependencies of the image through a causal 2D-AR random ﬁeld of order one. Note that for real
images this model is lossy, but it is suﬃciently accurate to follow their singular value distribution.
N genuine image X (which is free of any quantization noise), the
Therefore, considering an N
proposed 2D-AR model predicts each entry Xu,v as a function of past samples, such that

×

Xu,v = ρrXu−1,v + ρcXu,v−1 −

ρrρcXu−1,v−1 + Su,v,

N random matrix S with i.i.d.

where ρr and ρc represent the one-step correlation coeﬃcients for rows and columns, respectively,
and Su,v is an element of an N
S ) entries. Consequently, the
generated 2D-AR random ﬁeld (denoted by X) as a model of the genuine image can be expressed
in matrix form as X = UrSUc, where Ur and Uc synthesize the ﬁrst-order 2D-AR model as a
function of ρr and ρc. Given that for many image classes both correlation coeﬃcients are similar,
we simplify the model by setting ρr = ρc = ρ.1 Accordingly, the random ﬁeld can be characterized
as

(0, σ2

N

×

X = USUT ,

(1)

where U is a Toeplitz matrix of size N
1), with Q denoting the length of a truncated
i], where sequence uQ[n] , ρQ−1−n,
AR ﬁlter. Hence, matrix U is fully described as Ui,j = uQ[j
for n = 0, . . . , Q
1, and is zero elsewhere. Although this deﬁnition entails a truncation of the
inﬁnite impulse response inherent to an AR model, Q can be made arbitrarily large, a fact that we
will later use.

(N + Q

×

−

−

−

∗

Signal Theory and Communications Department, University of Vigo, Vigo, Spain (e-mail: dvazquez@gts.uvigo.es;

fperez@gts.uvigo.es; pcomesan@gts.uvigo.es)

1This hypothesis has been validated computing both correlation coeﬃcients for a set of 1317 images coming from

the Dresden Image Database [2], obtaining the averaged values ρr = 0.9713 and ρc = 0.9736.

1

 
 
 
 
 
 
0
10

i

λ

−5

10

−10

10

Genuine Image, λ
(Σ
)
X
i
Random Matrix with i.i.d. Gaussian entries, λ
(Σ
)
S
i
2D−AR Random Field, λ
(Σ
)
X
i

0

100

200

300

400

500
i

600

700

800

900

1000

(b) Scree plot comparison

(a) Natural image (1000

1000)

×

Figure 1: Comparison of the scree plot obtained from the renormalized sample autocorrelation
matrix corresponding to the image in (a) with the ones from a Gaussian matrix and a 2D-AR
model with ρ = 0.945 (Q = N = 1000).

A practical example is shown next to visually examine how well the eigenvalues of the sample
autocorrelation matrix can be ﬁt according to this model. Figure 1 shows the scree plot of the
renormalized sample autocorrelation matrix Σ ˜X when ˜X is the real image block depicted in Fig-
ure 1(a). For comparison, we plot the eigenvalues of ΣX corresponding to the above 2D-AR model
with ρ = 0.945. To highlight the diﬀerences with a model where no linear dependencies are taken
into account (i.e., when ρ = 0 in the 2D-AR random ﬁeld), we also plot the eigenvalues of ΣS, when
N i.i.d. Gaussian entries. For a better comparison, we have subtracted the mean to ˜Xi,j,
S has N
Xi,j, and Si,j, and normalized the resulting values by their standard deviations, before computing
the eigenvalues of Σ ˜X, ΣX, and ΣS, respectively. As it can be checked, the 2D-AR model is more
accurate than the Gaussian one.

×

As stated above, the asymptotic distribution of the eigenvalues of ΣS follows MPL [3]. However,
as it is apparent in Figure 1, the distribution of the eigenvalues of ΣS does not match that of Σ ˜X.
In fact, MPL does not apply to ﬁltered white noise processes. Remarkably, the study of the
sample eigenvalues of ﬁltered processes has awaken a great level of interest in the last years (cf.
[4, Sect. 3]). Seminal works analyzing the eigenvalue distribution of covariance matrices under
dependence conditions started tackling matrices of the form B = A1/2S where S has i.i.d. entries
and A is a nonnegative deﬁnite matrix. In this direction, Bai and Zhou ﬁrst derived in [5] explicit
formulas for the asymptotic distribution of K −1BBT when the columns of B follow a causal ﬁrst-
order AR process. Later on, Pfaﬀel and Schlemm extended this result to general linear processes
in [6].

Unfortunately, given the form of our random matrix X in (1), neither of these results can be
applied directly to our case. We will therefore resort to a procedure proposed by Tulino and Verd´u
in [7, Theorem 2.43] to analytically derive the asymptotic eigenvalue distribution of ΣX. This
theorem tackles the calculation of the so-called η-transform [7, Sect. 2.2.2] of an unnormalized
sample autocorrelation matrix BBT , where B has the form B = CSA, which coincides with the
matrix form of X. Similarly, matrix S has i.i.d. entries, and matrices C and A induce the linear
dependencies. Once the η-transform of ΣX is characterized, we further apply a simple relationship
[7, Eq. (2.48)] to get the Stieltjes transform, whose inversion formula [7, Eq. (2.45)] ﬁnally provides
the pdf of the eigenvalues of ΣX.

In the following subsections, we particularize the calculation of the asymptotic eigenvalue dis-
tribution for genuine images (Section 1.1); unquantized upscaled images (Section 1.2), and ﬁnally
images that have been upscaled and later quantized (Section 1.3). For the sake of simplicity, and
without loss of generality, in Sections 1.1 and 1.2 we assume that the entries of S are i.i.d. with
variance σ2
S = 1 as in [7]. More generally, in Section 1.3, we assume an arbitrary variance σ2
S
to further evaluate the eﬀect of the signal-to-quantization-noise ratio. Notice that the eigenvalue
distribution for σ2
S 6

= 1 is directly obtained by multiplying by σ2

S the eigenvalues for σ2

S = 1.

2

1.1 Asymptotic Distribution for Genuine Images

K where the matrix aspect ratio converges to a constant K

Tulino and Verd´u’s theorem generalizes the calculation of the η-transform for rectangular matrices
of size N
. In our
×
case, as K
N in (1), we construct a
submatrix XK by taking K consecutive columns (without loss of generality, we will assume that
we retain the ﬁrst K columns), such that

1. Starting with matrix X of size N

N , we have β

N →
×

β as N

→ ∞

≤

≤

XK = UN SUT
K ,

(2)

(0, 1) entries.

where UK is constructed from U by keeping the ﬁrst K rows, and UN = U is so written to stress
the fact that it has N rows. In (2) S is an (N + Q
1) random matrix with i.i.d.

(N + Q

1)

−

×

−

N
For the calculation of the η-transform of ΣXK , we ﬁrst need to derive the asymptotic spectra of
the matrices that induce the linear dependencies, i.e., D = UN UT
K UK . It is easy to
show that both matrices are full-rank and, for a suﬃciently large value of K and N , have identical
asymptotic spectra. Therefore, we focus on obtaining the asymptotic spectrum of D, which can be
seen as an unnormalized version of the autocorrelation matrix of UN , itself described by uQ[n].

N and T = UT

≥

ρ2(Q−|i−j|))

If we assume that Q

N , we can straightforwardly show that D is a symmetric Toeplitz matrix,
, this simpliﬁes
whose (i, j)-th element is given by (1
−
to Di,j = ρ|i−j|/(1
ρ2). Now, in order to calculate the asymptotic eigenvalue spectrum of D,
we can invoke Szeg¨o’s fundamental theorem of eigenvalue distribution [8], which establishes that
] , Di,j), where d[n] is
for a Toeplitz symmetric matrix D deﬁned by sequence d[n] (i.e., d[
i
|
|
absolutely summable,2 its asymptotic eigenvalue spectrum tends to the Fourier transform of d[n],
, the eigenvalues of D and (by extension) T will asymptotically
i.e., d(ω). Therefore, when N
converge to

ρ2). If we let Q

ρ|i−j|/(1

→ ∞

→ ∞

−

−

−

j

·

d(ω) =

1 + ρ2

1
2ρ cos(ω)

−

, ω

∈

[0, 2π).

(3)

The η-transform of ΣXK depends on two independent random variables D and T which are dis-
tributed as the asymptotic spectra of D and T. Both random variables can be seen as the result
[0, 2π), such that D = d(Ω)
of applying the transformation d(ω) in (3) to a random variable Ω
and T = d(Ω). Therefore, as described in Appendix A, once d(ω) is available, the η-transform of
ΣXK can be numerically calculated. Then, as previously indicated, ηΣXK (γ) must be written in
terms of the Stieltjes transform to ﬁnally obtain, through the Stieltjes inversion formula, the pdf
of the eigenvalues of ΣXK . Both transforms ηΣXK (γ) and
ΣXK (z) are linked by the following
S
1/γ) = γηΣXK (γ). For completing the pdf of the eigenvalues of ΣXK we need to
relation
calculate its asymptotic fraction of zero eigenvalues (in the sequel, AFZE), which is

ΣXK (
S

∼ U

−

min

1

−

{

βP(T

= 0), P(D

= 0)
}

.

(4)

Given that D and T have full rank and their asymptotic spectra through (3) satisfy d(ω) > 0, we
know that P(T
β). So
ﬁnally, using the inversion formula, we obtain the asymptotic pdf of the eigenvalues of ΣXK , i.e.,

1, the AFZE of ΣXK equals (1

= 0) = 1. Therefore, since β

= 0) = P(D

−

≤

fΣXK (λ) = (1

β)δ(λ) + lim
ν→0+

−

1
π

Im

ΣXK (λ + jν)
i

hS

,

) denotes the Dirac delta function.
where δ(
·

In Figure 2, we depict the analytically derived pdf of the eigenvalues of ΣXK for diﬀerent values
of β and ρ = 0.97. We can observe that if we build XK by extracting the ﬁrst K columns from
X, the nonzero eigenvalues of ΣXK tend to compress towards the variance of the population when
β = K/N decreases, as predicted by MPL for random matrices with i.i.d. entries. This ﬁnding is
very important because it generalizes the intuition behind this law to stochastic representations of
genuine images. In the following subsection, we will show that this property still holds after an
interpolation.

2There is an additional technical condition that applies in the cases considered in this paper, namely, that the set

{ω : d(ω) = x} has measure zero for all x ∈ R.

3

6
6
6
6
)
λ
(

Σ

f

X

1.2

1

0.8

0.6

0.4

0.2

0

0

β = 0.125
β = 0.25
β = 0.5
β = 1

0.5

1

1.5
λ

2

2.5

3

Figure 2: Partial representation of the calculated pdf of the eigenvalues of ΣXK , i.e., fΣXK (λ), for
diﬀerent values of β and ρ = 0.97. Note that the mass points at λ = 0 (present for β < 1) are not
shown.

1.2 Asymptotic Distribution for Unquantized Upscaled Images

N matrix Y generated as in [1, Eq. 2] to represent
As introduced in [1, Sect. II-A], we use an N
an upscaled image. In this case, we are interested in computing the eigenvalue distribution of ΣYK ,
where YK is a submatrix of size N
N consecutive columns from
matrix Y. Modeling ˆX in [1, Eq. 2] as in (1), we can write

K that is made up of K

≤

×

×

YK = HN USUT HT
K,

where U is now a Toeplitz matrix of size R
1) described by sequence uQ[n]. In the above
×
equation, matrices HN and HK are constructed as in [1, Eq. 3], with respective sizes N
R and
R, containing both shifted copies of the L diﬀerent polyphase components of h(iM/L + ϕ),
K
×
Z. Notice that [1, Eq. 3] is such that the ﬁrst row in HN (or HK ) corresponds to the zeroth
i
∈
polyphase component; however, as we will see, this arbitrary assignment has no eﬀect on our
analytical derivations.

(R + Q

−

×

As in the previous section, in order to compute the η-transform of ΣYK , we need to characterize
the asymptotic spectra of matrices D = CCT = HN UUT HT
KHKU. For
convenience, we start focusing on the nonzero eigenvalues of D = CCT , which are the same as
those of matrix D′ , CT C. Let us consider its inner matrix R , HT

N and T = AAT = UT HT

N HN , with entries given by

Ri,j =

R−1

Xl=0

h

l M
L + ϕ

(cid:0)

l M
L + ϕ

−

h

i
(cid:1)

(cid:0)

−

j

.

(cid:1)

(5)

Although this matrix is not Toeplitz, it can be seen to contain in its rows the diﬀerent components
of a polyphase decomposition of the kernel autocorrelation function. Since these rows are roughly
similar, it makes sense to convert R into Toeplitz by averaging those components. Let ¯R be such
matrix, with

¯Ri,j =

=

1
M

1
M

M −1

R−1

Xl=0

Xk=0
R·M −1

h

l M
L + k

L + ϕ

(cid:0)

L + k
l M

L + ϕ

−

h

i
(cid:1)

(cid:0)

j

−

(cid:1)

h

k
L + ϕ

(cid:0)

−

h

i
(cid:1)

(cid:0)

k
L + ϕ

−

j

,

(cid:1)

Xk=0

(6)

which, as it can be readily checked is symmetric Toeplitz, so ¯Ri,j only depends on
. Then,
|
¯R is completely characterized by the sequence rhh[
] , ¯Ri,j. Now, since U is also Toeplitz,
i
|
|
by expressing products of Toeplitz matrices as convolutions of their corresponding representative
sequences, it is possible to see that D′ = UT HT
∗
and D′ is symmetric Toeplitz;
uQ[
therefore, we can resort again to Szeg¨o’s theorem [8] to approximate the asymptotic eigenvalue
distribution of D′ when both Q, N

n]. This sequence is absolutely summable even for Q

N HN U is described by sequence uQ[n]

, as follows

rhh[n]

→ ∞

i
|

−

−

−

∗

j

j

d′(ω) =

(cid:18)

1 + ρ2

→ ∞
1
2ρ cos(ω) (cid:19)

−

kw−1

Xn=−(kw−1)

rhh[n] cos(nω),

(7)

4

 
 
)

D

(
λ

i

4
10

3
10

2
10

1
10

0
10

−1

10

−2

10

Linear
Catmull−Rom
B−Spline
Lanczos
Approximation through d’(ω)

100

200

300

400
i

500

600

700

(a) ρ = 0.97, ξ = 4
3

)

D

(
λ

i

4
10

3
10

2
10

1
10

0
10

−1

10

−2

10

Linear
Catmull−Rom
B−Spline
Lanczos
Approximation through d’(ω)

100

200

300
i

400

500

600

(b) ρ = 0.97, ξ = 8
5

)

D

(
λ

i

4
10

3
10

2
10

1
10

0
10

−1

10

−2

10

Linear
Catmull−Rom
B−Spline
Lanczos
Approximation through d’(ω)

100

200

300

400

500

i

(c) ρ = 0.97, ξ = 2

Figure 3: Evolution of the nonzero eigenvalues of D for diﬀerent values of ξ and the interpolation
kernels in [1, Table I]. Solid lines represent the ordered eigenvalues λi(D), while dashed lines
correspond to the approximation d′(ω) in (7) sorted in descending order, with ω = 2π i−1
1, . . . , N ξ−1
{

, (N = 1024).
}

N ξ−1 , i

∈

where ω
approximated by d′(ω).

∈

[0, 2π). This discussion extends to the non-null eigenvalues of T, which can also be

A crucial diﬀerence with respect to the case of genuine images (Section 1.1) is that now both D
and T will have null eigenvalues, given that matrices HN and HK do not have full rank. Actually,
the ranks of both matrices depend on the applied resampling factor ξ, and it is easy to check
ξ−1. From the rank properties
that limN→∞ rank(HN )/N
for real matrices, we have that rank(D) = rank(C) = rank(HN ) because U has full rank, so
ξ−1. We can conclude that the AFZE of D
rank(D)/N
and T is given by (1

ξ−1 and, by extension, rank(T)/K

ξ−1, and limK→∞ rank(HK )/K

ξ−1).

→

→

→

→

Figure 3 depicts the eigenvalues of matrix D for diﬀerent values of ξ and for the interpolation
kernels in [1, Table I], and the approximation in (7). Although the derived approximation is
generally accurate, we observe that for certain pairs of ξ and kernel, it is not able to follow the
existing discontinuities. The most evident examples show up with the Linear kernel for ξ = 4
3
5 . This diﬀerence comes from approximating R by ¯R. However, since the range of
and ξ = 8
the eigenvalues is well matched and their evolution is tracked up to a good degree, we adopt this
approximation.

−

From the plots in Figure 3, it is interesting to observe that the largest eigenvalues of the four
kernels converge to the same value, while the smallest ones diﬀer noticeably. Although for Catmull-
Rom and Lanczos kernels the smallest eigenvalues are almost identical, the Linear kernel provides
smaller eigenvalues as ξ gets closer to 1, and the B-spline kernel produces the smallest eigenvalues
at almost one order of magnitude below.

The procedure to compute the pdf of the eigenvalues of ΣYK is exactly the same as the one
followed with genuine images by taking d′(ω) as d(ω); thus, the numeric calculation of the η-
transform can be carried out following the same steps as in Appendix A. However, note that when
computing E1 and E2 as in (11) and ηΣYK as in (12), we must take into account that now the
ξ−1), and a
random variable Ω is of mixed type with a probability mass at ω = 0 of size (1
continuous pdf fΩ(ω) = (2πξ)−1 in (0, 2π), i.e., the pdf of a uniform random variable, but scaled
by ξ−1 so that the total probability adds up to 1.

−

Once the η-transform has been computed, the procedure to obtain the pdf fΣYK (λ) is exactly
the same as for the non-resampled case, where the AFZE of ΣYK is given by (4). In this case, we
know that P(T
1. Finally, we
have

= 0) = ξ−1 and so the AFZE equals (1

= 0) = P(D

βξ−1) for β

−

≤

fΣYK (λ) =

1
(cid:0)

βξ−1

−

δ(λ) + lim
ν→0+

(cid:1)

1
π

Im

ΣYK (λ + jν)

hS

.

i

Figure 4 shows the calculated pdfs of the eigenvalues of ΣYK under diﬀerent settings. In general,
the shape of the diﬀerent distributions for a particular ratio β = K/N resembles the one from
genuine images (see Figure 2), which conﬁrms that the compression property of the eigenvalues as

5

 
 
 
 
 
 
6
6
1

0.8

)
λ
(

Σ

f

K

Y

0.6

0.4

0.2

0

0

β = 0.125
β = 0.25
β = 0.5
β = 1

0.5

1

1.5

λ

1

0.8

)
λ
(

Σ

f

K

Y

0.6

0.4

0.2

0

0

β = 0.125
β = 0.25
β = 0.5
β = 1

0.5

1

1.5

λ

1

0.8

)
λ
(

Σ

f

K

Y

0.6

0.4

0.2

0

0

β = 0.125
β = 0.25
β = 0.5
β = 1

0.5

1

1.5

λ

1

0.8

)
λ
(

Σ

f

K

Y

0.6

0.4

0.2

0

0

β = 0.125
β = 0.25
β = 0.5
β = 1

0.5

1

1.5

λ

(a) Linear ﬁlter

(b) Catmull-Rom ﬁlter

(c) B-spline ﬁlter

(d) Lanczos ﬁlter

Figure 4: Partial representation of the calculated pdf of the eigenvalues of ΣYK for diﬀerent in-
terpolation kernels and values of β (ﬁxing ρ = 0.97). Solid lines depict the pdfs for ξ = 1.5 and
dashed lines correspond to ξ = 2. Note that the mass points at λ = 0 (when β < 1) are not shown.

K/N decreases still holds for the extracted submatrices YK from upscaled images. This property
will prove to be important in the following section for distinguishing the signal subspace from the
background noise. Moreover, the particular eﬀect of each interpolation kernel on the distribution
of the eigenvalues of ΣYK becomes apparent. For instance, the B-spline kernel is the one that
has its eigenvalues more concentrated towards zero, as it was expected from the results shown in
Figure 3. This diﬀerence with respect to the other three interpolation kernels, will unavoidably
result in diﬀerent performance when tackling resampling detection and estimation, as we will see
next.

1.3 Asymptotic Distribution for Upscaled & Quantized Images

The eﬀect of further quantizing the pixels of an upscaled image is examined next. Adhering to the
model in [1, Eq. 4], we analyze the eigenvalues of ΣZK , where the submatrix ZK of size N
K
is constructed by extracting K < N consecutive columns from matrix Z. Drawing on MPL and
Tulino and Verd´u’s theorem, we will analytically support the empirical ﬁndings discussed in [1,
Sect. II-A].

×

The analysis in Section 1.2 makes clear that for a suﬃciently large value of K, the submatrix
ξ−1). Therefore, in ΣZK , there
P corresponding to
K will be zero). From Weyl’s inequality applied to the
n; then

YK has rank P strictly smaller than K (i.e., rank(YK )/K
will be P leading eigenvalues corresponding to the signal subspace and K
the background noise (the remaining N
singular values [9, Exercise 1.3.22], we know that if A and B are m
σi+j−1(A + B)
1
λj(ΣWK ), for all j = 1, . . . , N

n matrices, m
m. Hence λP +j(ΣZK )

P . In particular, we can write

σi(A) + σj(B) for all i, j

1 such that i + j

×
≤

→

−

≤

−

≥

−

≤

≤

−

λP +1(ΣZK )

λ1(ΣWK )

≤

≤

λ+(ΣWK )

→

σ2
W (1 +

β)2,

(8)

p

where λ+(ΣWK ) denotes the limiting upper bound on the eigenvalues of ΣWK given by MPL. Now
we turn our attention to λP (ΣZK ). Resorting again to Weyl’s inequality, we can derive a lower
bound for λP (ΣZK ), which is of interest for us to model the transition between the signal subspace
and the noise space. To this end, we write σi+j−1(A)
σi(A + B), leading us to the
following lower bound for λP (ΣZK ):

σj(

B)

−

−

≤

λP (ΣZK )

λP (ΣYK )

−

≥

λ1(ΣWK )

≥

σ2
Sλ−(ΣYK )

λ+(ΣWK ),

−

(9)

where λ−(ΣYK ) represents the smallest nonzero eigenvalue of ΣYK . Remember that λ−(ΣYK ),
which can be obtained through the calculation of fΣYK (λ) as in Section 1.2, is scaled by σ2
S,
because the derived pdf fΣYK (λ) assumes σ2

S = 1.

Combining (8) and (9) we conﬁrm the existence of an asymptotic gap between λP (ΣZK ) and

λP +1(ΣZK ) marking the transition between the signal subspace and the noise, i.e.,

λP (ΣZK )
λP +1(ΣZK ) ≥

σ2
Sλ−(ΣYK )

λ+(ΣWK )

−
λ+(ΣWK )

σ2
S
σ2
W (cid:19)

λ−(ΣYK )
(1 + √β)2 −

1.

→ (cid:18)

(10)

6

 
 
 
 
 
 
 
 
)

K
Y

Σ
(

−

λ

0.45

0.4

0.35

0.3

0.25

0.2

0.15

0.1

0.05

0

0

Linear
Catmull−Rom
B−spline
Lanczos

0.25

0.2

0.15

)

Linear
Catmull−Rom
B−spline
Lanczos

K
Y

Σ
(

−

λ

0.2

0.4

0.6

0.8

1

ρ

(a) ξ = 1.5

0.1

0.05

0

1.1

1.2

1.3

1.4

1.6

1.7

1.8

1.9

1.5
ξ

(b) ρ = 0.95

Figure 5: Evolution of the smallest nonzero eigenvalue of ΣYK as a function of the correlation
coeﬃcient in (a) and modifying the resampling factor in (b), for diﬀerent interpolation kernels. In
both cases, β = 0.125 and N = 128.

The limiting bound in (10) monotonically increases with the signal-to-noise ratio

, which is
0, since λ−(ΣY ) and (1+√β)2 respectively
intuitively appealing. Moreover, it also increases as β
increases and decreases as β
0. Finally, for a ﬁxed value of β, the gap becomes smaller as both
the resampling factor ξ and the correlation coeﬃcient ρ get closer to 1, as can be seen in Figure 5.
Analyzing the gap in view of the interpolation kernel from Figure 5, it is clear that the smallest
gap will be obtained through the B-spline kernel, while the largest bound will be achieved by the
Lanczos kernel. A signiﬁcant gap is also expected for the Catmull-Rom kernel since it keeps a
similar behavior to the Lanczos ﬁlter. Finally, the Linear kernel is halfway between Catmull-Rom
and B-spline.

→

→

(cid:16)

σ2
S
σ2
W (cid:17)

7

2 Resampling Detection and Estimation

The pseudocode of the resampling detector detailed in [1, Sect.IV-A], is shown in Algorithm 1.

Algorithm 1 Proposed Resampling Detector
Inputs: Z, N, K, ∆
Output: κ

K
N

β
←
∆2
σ2
W ←
12
2(N
V
←
for v = 0 to V

−

K + 1)

1 do

−

if v is even then

ZK ←
ZK ←

else // v is odd

crop

ZT , [1, (v

crop (Z, [1, v/2 + 1, N, K])

−

1)/2 + 1, N, K]
(cid:1)

end if
(cid:0)
Σ(v)
1
N ZK ZT
K
ZK ←
Σ(v)
Λv ←
ZK
(cid:16)
if Λv < σ2
W (1

λK

// K-th eigenvalue of Σ(v)
ZK
(cid:17)
√β)2 then
−
add element v to the set

S

end if
Λ(0)
v
←
end for
if

←
else if 1

= 0 then
min (Λv) , v

|S|
κ

0, . . . , V

1
}

−

∈ {
< V then

≤ |S|

κ
else

median (Λv) , v

←
minv∈{0,...,V −1}
λ0 ←
κ
λ0
←
end if

0, . . . , V

∈ {

1
} \ S

−

Λ(0)
v
(cid:16)

(cid:17)

smallest λi

Σ(v)
ZK
(cid:16)

(cid:17)

, i

0, . . . , K

∈ {

, larger than σ2
}

W (1

−

√β)2

function crop (Z, [r, c, N, K])
return N
end function

×

K matrix from Z starting at row r, column c

Interestingly, from the asymptotic analysis in Section 1, we can determine a theoretical threshold
to drive the decision of our resampling detector based on the test statistic κ (cf. [1, Eq. 11]). Directly
Sλ−(ΣXK ). On the other hand,
from (8), we know that λK(ΣZK ) under
W (1 + √β)2. Then, provided that
the upper bound for λK(ΣZK ) under
σ2
W (which is typically the case for real images), the condition σ2
σ2
W (1 + √β)2
S ≫
will be satisﬁed depending on the value of λ−(ΣXK ), which increases as β
0 (see Figure 2).
Assuming that such condition is commonly satisﬁed in most practical cases, the proposed detector
W (1 + √β)2. Accordingly, the detector labels an observed
can operate with a ﬁxed threshold at σ2
image block Z as upscaled whenever κ < σ2

H0 is lower bounded by σ2

H1 is given by MPL at σ2

W (1 + √β)2, and genuine otherwise.

Sλ−(ΣXK ) > σ2

→

To illustrate the good behavior of our detector using the described theoretical threshold, we test
it over a total of 1317 uncompressed images captured by diﬀerent Nikon cameras belonging to the
Dresden Image Database [2]. Given its extensive use, we employ the image processing tool convert
from ImageMagick’s software to perform each full-frame resampling operation. As interpolation
i.e., those described in [1, Table I]. We
kernels, we select the ones most commonly available,
constrain the set of resampling factors to the interval [1.05, 2] uniformly sampled with step 0.05.

The analysis of resampling traces is conducted over the green channel of each image under study

8

C
U
A

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

1.1

1.2

1.3

1.4

1.5

ξ

1.6

1.7

1.8

1.9

2

Linear

Catmull−Rom

B−spline

Lanczos

%
9
4
1
2
.
1
≤
R
A
F

t
a

e
t
a
r

n
o
i
t
c
e
t
e
D

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

Linear

Catmull−Rom

B−spline

Lanczos

1.1

1.2

1.3

1.4

1.5

ξ

1.6

1.7

1.8

1.9

2

(a) AUC (without demosaicing traces)

(b) Detection rate at FAR

1.2149%

≤

Figure 6: Evaluation of our detector (solid lines) against the SVD-based [10] (dashed lines) and
the LP-based [11] (dotted lines) detectors in terms of AUC and detection rate for 32
32 blocks.
The employed genuine images do not contain demosaicing traces.

×

×

by processing its central square block Z of size 32
32, so as to test our detector in a realistic
scenario, where the tampered regions might be small. We apply the described testbench on genuine
images that do not contain demosaicing traces (i.e., they are constructed by getting access to the
output of the camera sensor using the tool dcraw and picking always the same green pixel position
from each 2

2 Bayer pattern)

×

The performance of the proposed detector is measured in terms of the Area Under the Curve
(AUC) corresponding to the Receiver Operating Characteristic (ROC), and the detection rate at a
ﬁxed False Alarm Rate (FAR). For comparison, the same tests are applied to two state-of-the-art
detectors: the “SVD-based detector” derived in [10], and the “LP-based detector” proposed in [11]
(where LP stands for Linear Predictor). We conﬁgure our detector to work with submatrices of
small aspect ratio β = 0.2812 (i.e., K = 9, yielding V = 48), because it makes the separation
between the smallest eigenvalue under each hypothesis more evident. For [10] we take ξmin = 1.05
and for [11] we ﬁx a neighborhood of 3 rows/columns.

For calculating the AUC in each of these cases, the three detectors are applied on both the
genuine and the upscaled images. Given that the proposed detector works with a ﬁxed theoretical
threshold, the detection rates and the corresponding FARs are also computed from the application
of our detector to both the genuine and the upscaled images from the whole dataset. However, for
deriving the empirical thresholds and computing the detection rates for the other two methods,
the database is randomly split in two disjoint sets, where 1/3 of the images are used for training
(i.e., a total of 439) and the remaining (i.e., a total of 878) are used for testing. The FAR obtained
with the application of our method is ﬁxed as a reference for comparing the detection rates of
the three detectors. This means that for the detectors in [10] and [11], the training set is used to
empirically determine the thresholds that give the same FAR. Finally, using the derived thresholds,
these detectors are applied on the test set of upscaled images to compute the detection rates.

ξ

≤

Figure 6 shows the obtained results when testing non-demosaiced images. Since the SVD-based
detector relies on the same principle of subspace decomposition than our detector, the performance
of the two approaches is similar, achieving better results for B-spline and Linear interpolation
kernels than for Catmull-Rom and Lanczos. Both methods outperform the LP-based detector,
which shows diﬃculties when dealing with small block sizes. In Figure 6(b) it can be seen that
the proposed technique clearly improves the detection performance of the other two methods when
1.05
1.2, which is particularly interesting in practice, because typically only slight trans-
formations are applied when performing a credible forgery. In fact, this range was known to be
especially challenging for existing resampling detectors when dealing with small image blocks. For
our detector, the eigenvalue compression for K < N elicits this notable improvement, making it
possible to achieve a FAR
1.2149% with the theoretically ﬁxed threshold. Moreover, this value
of FAR turns out to be very attractive in practice, when facing real forensic scenarios. Related to
this, the fast convergence towards zero of the eigenvalues when using the B-spline kernel (check
Figures 3 and 4(c)), improves the separability between upscaled and genuine images, thus providing
always the best performance.

≤

≤

9

 
 
 
 
 
 
 
 
 
On the other hand, regarding the estimation strategy proposed in [1, Sect. IV-B], the pseu-

docode to obtain the interval

K , ˆξ(u)
ˆξ(l)

K

h

(cid:17)

for a particular K is summarized in Algorithm 2.

Algorithm 2 Proposed Estimation Strategy
Inputs: Z, N, K, ∆, kw, ξmax, Tµ
K , ˆξ(u)
ˆξ(l)
Output:
K
N

// interval for ξ

(cid:17)

K

h

β
←
∆2
σ2
W ←
12
2(N
V
−
←
K/ξmax⌋
I ← {⌊
for v = 0 to V
−
if v is even then

K + 1)

, . . . , K
1 do

1
}

−

crop (Z, [1, v/2 + 1, N, K]) // cf. Alg. 1

−

1)/2 + 1, N, K]
(cid:1)

ZK ←
ZK ←

else // v is odd

crop

ZT , [1, (v

ZK ←

end if
(cid:0)
Σ(v)
1
N ZK ZT
K
for i = 1 to K
−
Σ(v)
ZK
(cid:16)
< σ2

Ψv[i]
end for
if λK

←
Σ(v)
ZK
(cid:16)

λi

(cid:17)

1 do

(cid:17) (cid:14)
W (1

add element v to the set

λi+1

Σ(v)
ZK
(cid:16)

(cid:17)

√β)2 then

−

S

Σ(v)
ZK

(cid:16)

(cid:17) −

arg min

λi
i∈{1,...,K} (cid:12)
(cid:12)
(cid:12)

end if
iv ←
end for
1
µ
V −|S|
for v = 0 to V

←

W (1 + √β)2
σ2

(cid:12)
(cid:12)
(cid:12)
Ψv[i]

v∈{0,...,V −1}\S max
i∈I

Ψv[i]/ median

i∈I

P
−
and µ
arg max
i∈I

1 do

≥
Ψv[i]

Tµ then

and µ < Tµ then

∈ S

if v /
∈ S
pv ←
else if v /
iv
pv ←
pv ←
end if
end for
ˆP

else

0

arg max

←

i∈{0,...,K}
if ˆP is zero then
K , ˆξ(u)
ˆξ(l)

K

(cid:17) ←

h
else

if µ < Tµ then
K , ˆξ(u)
ˆξ(l)

K

h
else

K , ˆξ(u)
ˆξ(l)
h
end if

K

end if

h(i, [p0, . . . , pV −1])

[1, ξmax)

1,

K−1

( ˆP −kw)−1 (cid:17)

(cid:17) ← h

(cid:17) ← h

K−1
( ˆP −kw)+1

, K−1
( ˆP −kw)−1 (cid:17)

10

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

C
U
A

0

0

1

2

3

4
2/σ
SNR [dB]: 10log(σ
S

2 )
W

5

6

7

Figure 7: Evaluation of the detector performance in terms of AUC as a function of the signal-to-
σ2
S
. 2D-AR model: ρ = 0.97, Q = N = 512. Quantization step: ∆ = 1. 1000 Monte
noise ratio
σ2
W (cid:17)
Carlo realizations are considered.

(cid:16)

2.1 Detector Performance Under Diﬀerent Signal-to-noise Ratio

From the theoretical analysis in Section 1, we know that the performance of our detector depends
on the signal-to-noise ratio
. Using synthetic 2D-AR random ﬁelds (generated as in (1)), we
analyze the detection performance of our detector discerning that genuine 2D-AR random ﬁelds
against their upscaled version by ξ = 3
2 (employing the Linear kernel). Fig. 7 reports the values of
. As expected, the larger

AUC achieved with our detector for diﬀerent signal-to-noise ratios
the signal-to-noise ratio, the better the detection performance.

σ2
S
σ2
W (cid:17)

σ2
S
σ2
W (cid:17)

(cid:16)

(cid:16)

11

A Appendix: η-transform Calculation

The numerical calculation of the η-transform of a renormalized autocorrelation matrix ΣB ,
N −1BBT (where B = CSA has size N
K and S is a random matrix with zero mean and
σ2
S = 1) is performed according to [7, Theorem 2.43], i.e., by employing the asymptotic spectra of
D = CCT and T = AAT (which in our case are modeled by d(ω)) as the transformation function
of the respective random variables D and T. Both random variables come from the deﬁnition of
the η-transform, i.e.,

×

ηΣB (γ) , E [ΓΣB (D, γ)] ,

where

ΓΣB (d, γ) ,

1 + γβdE

(cid:18)

T

1 + γTE [DΓΣB (D, γ)] (cid:21)(cid:19)

(cid:20)

−1

.

Since a closed-form solution to this ﬁxed-point equation does not exist, we use a simple method to
obtain the values of ΓΣB (d, γ) by computing iteratively the expectations

E1(d, γ) , E

D
1+γβDE2(d,γ)

h

i

and E2(d, γ) , E

T
1+γTE1(d,γ)

,

i

h

until we reach a value of E2(d, γ) whose absolute diﬀerence with respect to the same value in the
previous iteration is smaller than a predeﬁned tolerance threshold,3 i.e., ǫ < 10−6. Given that we
know that both random variables D and T correspond to d(Ω) for random variable Ω, we can solve
the above expectations using the derived function d(ω) and the corresponding probability density
function fΩ(ω) as follows

E1(d, γ) =

E2(d, γ) =

2π

2π

Z
0

Z
0

d(ω)
1 + γβd(ω)E2(d, γ)
d(ω)
1 + γd(ω)E1(d, γ)

fΩ(ω)dω,

fΩ(ω)dω.

(11)

Representing the result after convergence by E⋆

2 (d, γ), the η-transform is ﬁnally given by

ηΣB (γ) = E [ΓΣB (D, γ)] =

2π

d(ω)

Z
0

1 + γβd(ω)E⋆

2 (d, γ)

fΩ(ω)dω.

(12)

References

[1] David V´azquez-Pad´ın, Fernando P´erez-Gonz´alez, and Pedro Comesa˜na Alfaro. A random
matrix approach to the forensic analysis of upscaled images. IEEE Transactions on Information
Forensics and Security, (accepted) 2017.

[2] Thomas Gloe and Rainer B¨ohme. The Dresden Image Database for benchmarking digital
image forensics. In ACM Symposium on Applied Computing (SAC), pages 1584–1590, Mar.
2010.

[3] Vladimir Alexandrovich Marˇcenko and Leonid Andreevich Pastur. Distribution of eigenvalues
for some sets of random matrices. Mathematics of the USSR-Sbornik, 1(4):457–483, Apr. 1967.

[4] Debashis Paul and Alexander Aue. Random matrix theory in statistics: A review. Journal of

Statistical Planning and Inference, 150:1–29, Jul. 2014.

[5] Zhidong Bai and Wang Zhou. Large sample covariance matrices without independence struc-

tures in columns. Statistica Sinica, 18(2):425–442, Apr. 2008.

[6] Oliver Pfaﬀel and Eckhard Schlemm. Eigenvalue distribution of large sample covariance ma-

trices of linear processes. Probability and Mathematical Statistics, 31(2):313–329, 2011.

3As a ﬁrst step in the iterative process, the value of E1(d, γ) is initialized to a nearby point (if available), or to 1.

12

[7] Antonia Maria Tulino and Sergio Verd´u. Random matrix theory and wireless communications.
Foundations and Trends in Communications and Information Theory, 1(1):1–182, Jun. 2004.

[8] Ulf Grenander and Gabor Szeg¨o. Toeplitz Forms and Their Applications. University of Cali-

fornia Press, 1958.

[9] Terence Tao. Topics in Random Matrix Theory. American Math. Soc., 2012.

[10] David V´azquez-Pad´ın, Pedro Comesa˜na, and Fernando P´erez-Gonz´alez. An SVD approach to
forensic image resampling detection. In European Signal Processing Conference (EUSIPCO),
pages 2067–2071, Sep. 2015.

[11] Matthias Kirchner. Linear row and column predictors for the analysis of resized images. In

ACM Workshop on Multimedia and Security (MM&Sec), pages 13–18, Sep. 2010.

13

