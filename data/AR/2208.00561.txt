AvatarGen: a 3D Generative Model
for Animatable Human Avatars

Jianfeng Zhang1∗, Zihang Jiang1∗, Dingdong Yang2, Hongyi Xu2, Yichun Shi2,
Guoxian Song2, Zhongcong Xu1, Xinchao Wang1, Jiashi Feng2

1National University of Singapore

2ByteDance

2
2
0
2

g
u
A
1

]

V
C
.
s
c
[

1
v
1
6
5
0
0
.
8
0
2
2
:
v
i
X
r
a

Abstract

Unsupervised generation of clothed virtual humans with various appearance and
animatable poses is important for creating 3D human avatars and other AR/VR
applications. Existing methods are either limited to rigid object modeling, or not
generative and thus unable to synthesize high-quality virtual humans and animate
them. In this work, we propose AvatarGen, the ﬁrst method that enables not only
non-rigid human generation with diverse appearance but also full control over
poses and viewpoints, while only requiring 2D images for training. Speciﬁcally,
it extends the recent 3D GANs to clothed human generation by utilizing a coarse
human body model as a proxy to warp the observation space into a standard avatar
under a canonical space. To model non-rigid dynamics, it introduces a deformation
network to learn pose-dependent deformations in the canonical space. To improve
geometry quality of the generated human avatars, it leverages signed distance
ﬁeld as geometric representation, which allows more direct regularization from
the body model on the geometry learning. Beneﬁting from these designs, our
method can generate animatable human avatars with high-quality appearance and
geometry modeling, signiﬁcantly outperforming previous 3D GANs. Furthermore,
it is competent for many applications, e.g., single-view reconstruction, reanimation,
and text-guided synthesis. Code and pre-trained model will be available.

1

Introduction

Generating diverse and high-quality virtual humans (avatars) with full control over their pose and
viewpoint is a fundamental but extremely challenging task. Solving this task will beneﬁt many
applications like immersive photography visualization [69], virtual try-on [32], VR/AR [62, 22] and
creative image editing [68, 19].

Conventional solutions rely on classical graphics modeling and rendering techniques [9, 7, 12, 56] to
create avatars. Though offering high-quality, they typically require pre-captured templates, multi-
camera systems, controlled studios, and long-term works of artists. In this work, we aim to make
virtual human avatars widely accessible at low cost. Towards this goal, we propose the ﬁrst 3D-
aware avatar generative model that can generate 1) high-quality virtual humans with 2) various
appearance styles, arbitrary poses and viewpoints, 3) and be trainable from only 2D images, thus
largely alleviating the effort to create virtual human.

The 3D-aware generative models have recently seen rapid progress, fueled by introducing implicit
neural representation (INR) methods [6, 43, 36, 37] into generative adversarial networks [3, 40, 42, 16,
2]. However, these models are limited to relatively simple and rigid objects, such as human faces and
cars, and mostly fail to generate clothed human avatars whose appearance is highly sundry because
of their articulated poses and great variability of clothing. Besides, they have limited control over the

∗Equal contribution.

Preprint. Under review.

 
 
 
 
 
 
Figure 1: Our AvatarGen model can generate clothed avatars with diverse appearance from arbitrary
poses and viewpoints (top), animate the avatars with speciﬁc pose signals (middle) and inverse 2D
human images into animatable 3D avatars (bottom).

generation process and thus cannot animate the generated objects, i.e., driving the objects to move by
following certain instructions. Another line of works leverage INRs [37] to learn articulated human
avatars for reconstructing a single subject from one’s multi-view images or videos [47, 41, 63, 5, 48].
While being able to animate the avatars, these methods are not generative and cannot synthesize
novel identities and appearances.

Aiming at generative modeling of animatable human avatars, we propose AvatarGen, the ﬁrst model
that can generate novel human avatars with full control over their poses and appearances. Our model
is built upon EG3D [2], a recent method that can generate high quality 3D-aware human faces via
introducing a new tri-plane representation method. However, EG3D is not directly applicable for
clothed avatar generation because it cannot handle the challenges in modeling complex garments,
texture, and the articulated body structure with various poses. Moreover, EG3D has limited control
capability and thus it hardly animates the generated objects.

To address these challenges, we propose to decompose the clothed avatar generation into pose-guided
canonical mapping and canonical avatar generation. Guided by a parametric human body model
(e.g., SMPL [34]), our method warps each point in the observation space with a speciﬁed pose
to a standard avatar with a ﬁxed pre-deﬁned pose in a canonical space via an inverse-skinning
transformation [20]. To accommodate the non-rigid dynamics between the observation and canonical
spaces (like clothes deformation), our method further trains a deformation module to predict the
proper residual deformation. As such, our method can generate arbitrary avatars in the observation
space by deforming the canonical one which is much easier to generate and shareable across different
instances, thus largely alleviating the learning difﬁculties and achieving better appearance and
geometry modeling. Meanwhile, this formulation by design enables disentanglement between the
pose and appearance, offering independent control over them.

Although the aforementioned method can generate 3D human avatars with reasonable geometry, we
ﬁnd it tends to produce noisy surfaces due to the lack of constraints on the learned geometry (density
ﬁeld). Inspired by recent works on neural implicit surface [59, 66, 42, 48], we propose to use a signed
distanced ﬁeld (SDF) to impose stronger geometry-aware guidance for the model training. Compared
with the density ﬁeld, SDF gives a better-deﬁned surface representation, which facilitates more direct
regularization on learning the avatar geometries. Moreover, the model can leverage the coarse body
model from SMPL to infer reasonable signed distance values, which greatly improves quality of the
clothed avatar generation and animation. The SDF-based volume rendering techniques [59, 66, 42]
are used to render the low resolution feature maps, which are further decoded to high-resolution
images with the StyleGAN generator [26, 2].

As shown in Fig. 1, trained from 2D images without using any multi-view or temporal information
and 3D geometry annotations, AvatarGen can generate a large variety of clothed human with diverse

2

GenerationAnimationInversionSourceappearances under arbitrary poses and viewpoints. We evaluate it quantitatively, qualitatively, and
through a perceptual study; it strongly outperforms previous state-of-the-art methods. Moreover, we
demonstrate it on several applications, like single-view 3D reconstruction and text-guided synthesis.

Our contributions are threefold. 1) To our best knowledge, AvatarGen is the ﬁrst model able to
generate a large variety of animatable clothed human avatars without requiring multi-view, temporal or
3D annotated data. 2) We propose a human generation pipeline that achieves accurate appearance and
geometry modeling, with full control over the pose and appearance. 3) We demonstrate state-of-the-art
3D-aware human image synthesis on several benchmarks along with high-quality geometry.

2 Related Works

Generative 3D-aware image synthesis. Generative adversarial networks (GANs) [15] have recently
achieved photo-realistic image quality for 2D image synthesis [23, 25, 26, 24]. Extending these
capabilities to 3D settings has started to gain attention. Early methods combine GANs with voxel [61,
38, 39], mesh [57, 30] or point cloud [1, 29] representations for 3D-aware image synthesis. Recently,
several methods represent 3D objects by learning an implicit neural representation (INR) [54, 3, 40, 2,
42, 16, 10]. Among them, some methods use INR-based model as generator [54, 3, 10], while some
others combine INR generator with 2D decoder for higher-resolution image generation [40, 16, 65].
Follow-up works like EG3D [2] proposes an efﬁcient tri-plane representation to model 3D objects,
StyleSDF [42] replaces density ﬁeld with SDF for better geometry modeling and Disentangled3D [58]
represents objects with a canonical volume along with deformations to disentangle geometry and
appearance modeling. However, such methods are typically not easily extended to non-rigid clothed
humans due to the complex pose and texture variations. Moreover, they have limited control over
the generation process, making the generated objects hardly be animated. Differently, we study the
problem of 3D implicit generative modeling of clothed human, allowing free control over the poses
and appearances.

3D human reconstruction and animation. Traditional human reconstruction methods require
complicated hardware that is expensive for daily use, such as depth sensors [7, 12, 56] or dense camera
arrays [9, 17]. To reduce the requirement on the capture device, some methods train networks to
reconstruct human models from RGB images with differentiable renderers [64, 14]. Recently, neural
radiance ﬁelds [37] employ the volume rendering to learn density and color ﬁelds from dense camera
views. Some methods augment neural radiance ﬁelds with human shape priors to enable 3D human
reconstruction from sparse multi-view data [49, 5, 63, 55]. Follow-up improvements [47, 4, 31, 48, 60]
are made by combining implicit representation with the SMPL model and exploiting the linear blend
skinning techniques to learn animatable 3D human modeling from temporal data. However, these
methods are not generative, i.e., they cannot synthesize novel identities and appearances. In this work,
we learn fully generative modeling of human avatars from only 2D images, largely alleviating the
cost to create virtual humans.

3 Method

Our goal is to build a generative model for diverse clothed 3D human avatar generation with varying
appearances in arbitrary poses. The model is trained from 2D images without using multi-view or
temporal information and 3D scan annotations. Its framework is summarized in Fig. 2.

3.1 Overview

Problem formulation. We aim to train a 3D generative model G for geometry-aware human
synthesis. Following EG3D [2], we associate each training image with a set of camera parameters
c and pose parameters p (in SMPL format [34]), which are obtained from an off-the-shelf pose
estimator [27]. Given a random latent code z sampled from Gaussian distribution, and a new
camera c and pose p as conditions, the generator G can synthesize a corresponding human image
I = G(z|c, p). We optimize G with a discriminator D via adversarial training.

Framework. Fig. 2 illustrates the framework of our proposed generative model. It takes camera
parameters and SMPL parameters (specifying the generated pose) as inputs and generates 3D human
avatar and its 2D images accordingly. Our model jointly processes the random canonical code

3

Figure 2: Pipeline of AvatarGen. Taking the canonical code and camera parameters as input, the
encoder generates tri-plane based features of a canonical posed human avatar. The geometry code is
applied to modulate the deformation ﬁeld module which deforms the sampled points in the observation
space to the canonical space under the guidance of the pose condition (SMPL parameters). The
deformed spatial positions are used to sample features on the tri-plane, which are then rendered as
low-resolution features and images using the SDF based neural renderer. Finally, the decoder decodes
the feature images to high resolution images. The generator with is optimized a camera and pose
conditioned discriminator via adversarial training.

and camera parameters by a canonical mapping module (implemented by an MLP) to generate the
intermediate latent code that modulates the convolution kernels of the encoder. This encoder then
generates a tri-plane based features [2] corresponding to a canonical pose representation. Regarding
the pose control, our model ﬁrst jointly processes a random geometry code and the input pose
condition via an MLP-based geometry mapping module, and outputs latent features. Then, given a
spatial point x in the observation space, the output latent features are processed by a deformation
ﬁled module, generating non-rigid residual deformation ∆x(cid:48) over the inverse skinned point x(cid:48) in the
canonical space. We sample the features from the tri-plane according to the deformed spatial position
x(cid:48) + ∆x(cid:48), which are then transformed into appearance prediction (i.e., color features) and geometry
prediction (i.e., SDF-based features) for volume rendering. We will explain these steps in details in
the following sections.

3.2 Representations of 3D Avatars

It is important to choose an efﬁcient approach to represent 3D human avatars. The recent EG3D [2]
introduces a memory-efﬁcient tri-plane representation for 3D face modeling. It explicitly stores
features on three axis-aligned orthogonal planes (called tri-planes), each of which corresponds to
a dimension within the 3D space. Thus, the intermediate features of any point of a 3D face can be
obtained via simple lookup over the tri-planes, making the feature extraction much more efﬁcient
than NeRF that needs to forward all the sampled 3D points through MLPs [37]. Besides, the tri-plane
representation effectively decouples the feature generation from volume rendering, and can be directly
generated from more efﬁcient CNNs instead of MLPs.

Considering these beneﬁts, we also choose the tri-plane representation for 3D avatar modeling.
However, we found directly adopting it for clothed human avatars generation results in poor quality.
Due to the much higher degrees of freedom of human bodies than faces, it is very challenging for the
naive tri-plane representation model to learn pose-dependent appearance and geometry from only 2D
images. We thus propose the following new approaches to address the difﬁculties.

3.3 Generative 3D Human Modeling

There are two main challenges for 3D human generation. The ﬁrst is how to effectively integrate
pose condition into the tri-plane representations, making the generated human pose fully controllable
and animatable. One naive way is to combine the pose condition p with the latent codes z and c
directly, and feed them to the encoder to generate tri-plane features. However, such naive design
cannot achieve high-quality synthesis and animation due to limited pose diversity and insufﬁcient
geometry supervision. Besides, the pose and appearance are tightly entangled, making independent

4

Canonical MappingGeometry MappingCamera ParamsSMPL ParamsEncoderCanonical CodeGeometryCodeSample Feat.SDF-based Neural RendererDecoderMod.Mod.Disc.RealFakeConditionDeform.FieldsInverse SkinningCamera Paramscontrol impossible. The second challenge is that learning pose-dependent clothed human appearance
and geometry from 2D images only is highly under-constrained, making the model training difﬁcult
and generation quality poor.

To tackle these challenges, our AvatarGen decomposes the avatar generation into two steps: pose-
guided canonical mapping and canonical avatar generation. Speciﬁcally, AvatarGen uses SMPL
model [34] to parameterize the underlying 3D human body. With a pose parameterization, AvatarGen
can easily deform a 3D point x within the observation space with pose po to a canonical pose pc (an
“X”-pose as shown in Fig. 2) via Linear Blend Skinning [21]. Then, AvatarGen learns to generate
the appearance and geometry of human avatar in the canonical space. The canonical space is shared
across different instances with a ﬁxed template pose pc while its appearance and geometric details
can be varied according to the latent code z, leading to generative human modeling.

Such a task factorization scheme facilitates learning of a generative canonical human avatars and
effectively helps the model generalize to unseen poses, achieving animatable clothed human avatars
generation. Moreover, it by design disentangles pose and appearance information, making indepen-
dent control over them feasible. We now elaborate on the model design for these two steps.

Pose-guided canonical mapping. We deﬁne the human 2D image with SMPL pose po as the
observation space. To relieve learning difﬁculties, our model attempts to deform the observation
space to a canonical space with a predeﬁned template pose pc that is shared across different identities.
The deformation function T : R3 (cid:55)→ R3 thus maps spatial points xi sampled in the observation space
to x(cid:48)

i in the canonical space.

Learning such a deformation function has been proved effective for dynamic scene modeling [45, 50].
However, learning to deform in such an implicit manner cannot handle large articulation of humans
and thus hardly generalizes to novel poses. To overcome this limitation, we use the SMPL model
to explicitly guide the deformation [31, 47, 4]. SMPL deﬁnes a skinned vertex-based human model
(V, W), where V = {v} ∈ RN ×3 is the set of N vertices and W = {w} ∈ RN ×K is the set of the
skinning weights assigned for the vertex w.r.t. K joints, with (cid:80)

j wj = 1, wj ≥ 0 for every joint.

We use the inverse-skinning (IS) transformation to map the SMPL mesh in the observation space with
pose p into the canonical space [20]:

TIS(v, w, p) =

(cid:88)

wj · (Rjv + tj),

(1)

j
where Rj and tj are the rotation and translation at each joint j derived from SMPL with pose p.

Such formulation can be easily extended to any spatial points in the observation space by simply
adopting the same transformation from the nearest point on the surface of SMPL mesh. Formally,
for each spatial points xi, we ﬁrst ﬁnd its nearest point v∗ on the SMPL mesh surface as v∗ =
arg minv∈V ||xi − v||2. Then, we use the corresponding skinning weights w∗ to deform xi to x(cid:48)
i in
the canonical space as:

i = To(xi|p) = TIS(xi, w∗, p).
x(cid:48)

(2)

Although the SMPL-guided inverse-skinning transformation can help align the rigid skeleton with
the template pose, it lacks the ability to model the pose-dependent deformation, like cloth wrinkles.
Besides, different identities may have different SMPL shape parameters b, which likely leads to
inaccurate transformation.

To alleviate these issues, AvatarGen further trains a deformation network to model the residual
deformation to complete the ﬁne-grained geometric deformation and to compensate the inaccurate
inverse-skinning transformation by

∆x(cid:48)

i = T∆(x(cid:48)

i|w, p, b) = MLPs(Concat[Embed(x(cid:48)
(3)
where w is the canonical style code mapped from the input latent code z. We concatenate it with the
embedded x(cid:48)
i and SMPL pose p and shape b parameters and feed them to MLPs to yield the residual
deformation. The ﬁnal pose-guided deformation To→c from the observation to canonical spaces can
be formulated as

i), w, p, b]),

To→c(xi) = x(cid:48)

i + ∆x(cid:48)

i = To(xi|p) + T∆(To(xi|p)|w, p, b).

(4)

Canonical avatar generation. After deforming 3D points sampled in the observation space to the
canonical space, we apply AvatarGen with the tri-plane representation for canonical avatar generation.

5

Figure 3: Our proposed geometry-aware human modeling module. It ﬁrst predicts color and SDF
values using the sampled tri-plane features and corresponding point location in the canonical space.
Then it feeds them to volume renderer module to generate the raw image and features. The color
for each sampled point is directly predicted from the tri-plane feature with MLPs. We use the
SMPL model to guide the prediction of SDF and obtain a coarse signed distance value do which is
concatenated with the input tri-plane feature for predicting the residual distance ∆d. The ﬁnal SDF is
do + ∆d. See 3.4 for more details.

More concretely, it ﬁrst generates tri-plane via a StyleGAN generator by taking the latent code
z and camera parameters c as inputs. Then, for each point deformed via SMPL parameters p in
the canonical space, the model queries tri-plane to obtain the intermediate feature and maps it to
color-based feature c and density σ for volume rendering. As such, it generates clothed human
appearance and geometry in the canonical space with a predeﬁned canonical pose, which alleviates
the optimization difﬁculties and substantially helps our learning of high-quality avatar generation
with disentangled pose and appearance control.

3.4 Geometry-aware Human Modeling

To improve geometry modeling of AvatarGen, inspired by recent neural implicit surface works [59,
66, 42, 48], we adopt signed distance ﬁeld (SDF) instead of density ﬁeld as our geometry proxy,
because it introduces more direct geometry regularization and guidance. To achieve this, our model
learns to predict signed distance value rather than density in tri-plane for volume rendering.

SMPL-guided geometry learning. Although SDF has a well-deﬁned surface representation and
introduces several regularization for geometry learning, how to use it for generative human modeling
is still non-trivial due to the complicated body articulation and pose-dependent deformation. We
therefore leverage the SMPL model as a guidance for the geometry-aware generation and combine it
with a residual SDF network (as shown in Fig. 3), that models the surface details (including hair and
clothing) not represented by SMPL.

Speciﬁcally, given the input SMPL pose po and shape bo, we generate a SMPL mesh M =
TSMPL(po, bo), where TSMPL is the SMPL transformation function. For each 3D point x in the
observation space, we ﬁrst obtain its coarse signed distance value do by querying the SMPL mesh M .
Then, we feed do alone with the features from tri-plane to a light-weight MLP to predict the residual
SDF ∆d. The signed distance value of each point is computed as d = do + ∆d. Predicting SDF with
the coarse SMPL as guidance improves geometry learning of the model, thus achieving better human
generation and animation, as demonstrated in our experiments. We also introduce a SMPL-guided
regularization for SDF learning as elaborated in Sec. 3.5.

SDF-based volume rendering. Following [42], we adopt SDF-based volume rendering to obtain
the ﬁnal output images. For any point x on the sampled rays, we ﬁrst deform it to ¯x = To→c(x) by
pose-guided canonical mapping. We query feature vector F (¯x) for position ¯x from the canonical
tri-plane and then feed it into two MLP layers to predict the color feature c = MLPc(F (¯x)) and
the signed distance d = do + ∆d = do + MLPd(F (¯x), do). We then convert the signed distance
value di of each point xi alone a ray r to density value σi as σi = 1
α ), where α > 0
is a learnable parameter that controls the tightness of the density around the surface boundary. By

α · Sigmoid( −di

6

MLPsTriplane Feat.ConcatMLPsColorSDFVolume Rendererintegration along the ray r we can get the corresponding pixel feature as

I(r) =

N
(cid:88)





i−1
(cid:89)

i=1

j=1


 · (cid:0)1 − e−σi·δi(cid:1) · ci,

e−σj ·δj

(5)

where δi = ||xi − xi−1||. By aggregating all rays, we can get the entire image feature which is then
feed into a StyleGAN decoder [26] to generate the ﬁnal high-resolutions synthesized image.

3.5 Training

We use the non-saturating GAN loss LGAN [26] with R1 regularization LReg [15, 35] to train our
model end-to-end. We also adopt the dual-discriminator proposed by EG3D [2]. It feeds both the
rendered raw image and the decoded high-resolution image into the discriminator for improving
consistency of the generated multi-view images. To obtain better controllability, we feed both SMPL
pose parameters p and camera parameters c as conditions to the discriminator for adversary training.
To regularize the learned SDF, we apply eikonal loss to the sampled points as:

LEik =

(cid:88)

xi

(||∇di|| − 1)2,

(6)

where xi and di denote the sampled point and predicted signed distance value, respectively. Follow-
ing [42], we adopt a minimal surface loss to encourage the model to represent human geometry with
minimal volume of zero-crossings. It penalizes the SDF value close to zero:

LMinsurf =

(cid:88)

xi

exp(−100di).

(7)

To make sure the generated surface is consistent with the input SMPL model, we incorporate the
SMPL mesh as geometric prior and guide the generated surface to be close to the body surface.
Speciﬁcally, we sample vertices v ∈ V on the SMPL body surface and then use it as query to deform
to canonical space and sample features from the generated tri-plane and minimize the signed distance.

LSMPL =

(cid:88)

v∈V

||MLPd(F (To→c(v)))||.

The overall loss is ﬁnally formulated as

Ltotal = LGAN + λRegLReg + λEikLEik + λMinsurfLMinsurf + λSMPLLSMPL,

(8)

(9)

where λ∗ are the corresponding loss weights.

4 Experiments

We study the following four questions in our experiments. 1) Is AvatarGen able to generate 3D
human avatars with realistic appearance and geometry? 2) Is AvatarGen effective at controlling
human avatars poses? 3) How does each component of our AvatarGen model take effect? 4) Does
AvatarGen enable downstream applications, like single-view 3D reconstruction and text-guided
synthesis? To answer these questions, we conduct extensive experiments on several 2D human
fashion datasets [11, 33, 67].

Datasets. We evaluate methods of 3D-aware clothed human generation on three real-world fashion
datasets: MPV [11], DeepFashion [33] and UBCFashion [67]. They contain single clothed people in
each image. We align and crop images according to the 2D human body keypoints, following [13].
Since we focus on human avatar generation, we use a segmentation model [8] to remove irrelevant
backgrounds. We adopt an off-the-shelf pose estimator [27] to obtain approximate camera and SMPL
parameters. We ﬁlter out images with partial observations and those with poor SMPL estimations,
and get nearly 15K, 14K and 31K full-body images for each dataset, respectively. Horizontal-ﬂip
augmentation is used during training. We note these datasets are primarily composed of front-view
images—few images captured from side or back views. To compensate this, we sample more side- and
back-view images to re-balance viewpoint distributions following [2]. We will release pre-processed
scripts and datasets. For more details, please refer to the appendix.

7

Table 1: Quantitative evaluation in terms of FID, depth, pose and warp accuracy on three datasets.
Our AvatarGen outperforms all the baselines signiﬁcantly.

DeepFashion
FID↓ Depth↓ Pose↓ Warp↓ FID↓ Depth↓ Pose↓ Warp↓ FID↓ Depth↓ Pose↓ Warp↓

UBCFashion

MPV

GIRAFFE-HD
StyleNeRF
StyleSDF
EG3D

26.3
10.7
29.5
18.6

AvatarGen (Ours)

6.5

2.12
1.46
1.74
1.52

0.83

.099
.069
.648
.077

.050

31.4
26.2
19.8
20.3

4.7

25.3
20.6
41.0
16.2

9.6

1.94
1.44
1.69
1.70

0.86

.092
.067
.613
.065

.052

34.3
22.8
20.4
14.8

6.9

27.0
15.9
35.9
17.7

8.7

2.03
1.43
1.76
1.66

0.94

.094
.065
.611
.070

.059

35.2
20.5
13.0
23.9

6.0

4.1 Comparisons

Baselines. We compare our AvatarGen against four state-of-the-art methods for 3D-aware image
synthesis: EG3D [2], StyleSDF [42], StyleNeRF [16] and GIRAFFE-HD [65]. All these methods
combine volume renderer with 2D decoder for 3D-aware image synthesis. EG3D and StyleNeRF
adopt progressive training to improve performance. StyleSDF uses SDF as geometry representation
for regularized geometry modeling.

Quantitative evaluations. Tab. 1 provides quantitative comparisons between our AvatarGen and
the baselines. We measure image quality with Fréchet Inception Distance (FID) [18] between 50k
generated images and all of the available real images. We evaluate geometry quality by calculating
Mean Squared Error (MSE) against pseudo groundtruth (GT) depth-maps (Depth) and poses (Pose)
that are estimated from the generated images by [53, 27]. We also introduce an image warp metric
(Warp) that warps side-view image with depth map to frontal view and computes MSE against the
generated frontal-view image to further evaluate the geometry quality and multi-view consistency of
the model. For additional evaluation details, please refer to the appendix. From Tab. 1, we observe
our model outperforms all the baselines w.r.t. all the metrics and datasets. Notably, it outperforms
baseline models by signiﬁcant margins (69.5%, 63.1%, 64.0% in FID) on three datasets. These
results clearly demonstrate its superiority in clothed human avatar synthesis. Moreover, it maintains
state-of-the-art geometry quality, pose accuracy and multi-view consistency.

Qualitative results. We show a qualitative comparison against baselines in the left of Fig. 4. It can
be observed that compared with our method, StyleSDF [42] generate 3D avatar with over-smoothed
geometry and poor multi-view consistency. In addition, the noise and holes can be observed around
the generated avatar and the geometry details like face and clothes are missing. EG3D [2] struggles to
learn 3D human geometry from 2D images and suffers degenerated qualities. Compared with them,
our AvatarGen generates 3D avatars with high-quality appearance with better view-consistency and
geometric details.

4.2 Ablation studies

We conduct ablation studies on the Deepfashion dataset as its samples have diverse poses and
appearances. We investigate effects of varying the following designs of AvatarGen.

Geometry proxy. Our AvatarGen uses signed distance ﬁeld (SDF) as geometry proxy to regularize
the geometry learning. To investigate its effectiveness, we also evaluate our model with density ﬁeld
as the proxy. As shown in Tab. 2a, if replacing SDF with density ﬁeld, the quality of the generated
avatars drops signiﬁcantly—11.1% increase in FID, 38.6% and 66.8% increases in Depth and Warp
metrics. This indicates SDF is important for the model to more precisely represent clothed human
geometry. Without it, the model will produce noisy surface, and suffer performance drop.

Deformation schemes. Our model uses a pose-guided deformation to transform spatial points from
the observation space to the canonical space. We also evaluate other two deformation schemes in
Tab. 2b: 1) residual deformation [45, 44] only (RD), 2) inverse-skinning deformation [20] only (IS).
When using RD only, the model training does not converge, indicating that learning deformation
implicitly cannot handle large articulation of humans and lead to implausible results. While using
IS only, the model achieves a reasonable result (FID: 10.7, Depth: 0.93, Warp: 7.7), verifying the
importance of the explicitly pose-guided deformation. Further combining IS and RD (our model)
boosts the performance sharply—10.3%, 7.5% and 10.4% decrease in FID, Depth and Warp metrics,

8

Table 2: Ablations on Deepfashion. In (e), w/o denotes without using SMPL body SDF prior, Can. or
Obs. means using SDF prior from canonical or observation spaces.

Geo. FID Depth Warp

Deform. FID Depth Warp

KNN FID Depth Warp

Density 10.8 1.40 20.8
6.9

SDF

0.86

9.6

(a) The effect of different geom-
etry proxies.

Ray Steps FID Depth Warp

12
24
36
48

12.4 1.04
10.7 0.92
10.0 0.89
0.86
9.6

7.7
7.5
7.2
6.9

RD
IS

-

-

10.7 0.93
0.86

-
7.7
6.9

IS+RD 9.6

(b) Deformation schemes. IS and RD are
inverse skinning and residual deformation.

SDF Prior FID Depth Warp

w/o
Can.
Obs.

14.3 1.12
10.4 0.89
0.86
9.6

8.3
7.5
6.9

1
2
3
4

9.6
0.86
6.9
7.4
10.4 0.90
13.1 1.08 10.2
16.3 1.14 15.3

(c) Different number of KNN in
inverse skinning deformation.

SDF Scheme FID Depth Warp

Raw
Residual

10.8 0.94
0.86
9.6

7.9
6.9

(d) Number of ray steps.

(e) The effect of SMPL body SDF priors.

(f) SDF prediction schemes.

respectively. Introducing the residual deformation to collaborate with the posed-guided inverse-
skinning transformation indeed better represents non-rigid clothed human body deformation and thus
our AvatarGen achieves better appearance and geometry modeling.

Number of KNN neighbors in inverse skinning deformation. For any spatial points, we use
Nearest Neighbor to ﬁnd the corresponding skinning weights for inverse skinning transformation
(Eqn. (2)). More nearest neighbors can be used for obtaining skinning weights [31]. Thus, we
study how the number of KNN affects model performance in Tab. 2c. We observe using more KNN
neighbors gives worse performance. This is likely caused by 1) noisy skinning weights introduced by
using more neighbors for calculation and 2) inaccurate SMPL estimation in data pre-processing step.

Number of ray steps. Tab. 2d shows the effect of the number of points sampled per ray for volume
rendering. With only 12 sampled points for each ray, AvatarGen already achieves acceptable results,
i.e., 12.4, 1.04 and 7.7 in FID, Depth and Warp losses. With more sampling points, the performance
monotonically increases, demonstrating the capacity of AvatarGen in 3D-aware avatars generation.

SMPL body SDF priors. AvatarGen adopts a SMPL-guided geometry learning scheme, i.e., gen-
erating clothed human body SDFs on top of the coarse SMPL body mesh. As shown in Table 2e,
if removing SMPL body guidance, the performance drops signiﬁcantly, i.e., 32.9%, 23.2%, 16.9%
increase in FID, Depth and Warp losses. This indicates the coarse SMPL body information is im-
portant for guiding AvatarGen to better generate clothed human geometry. We also evaluate the
performance difference between SMPL body SDFs queried from observation (Obs.) or canonical
(Can.) spaces. We see the model guided by body SDFs queried from observation space obtains better
performance as they are more accurate than the ones queried from the canonical space. Moreover, we
study the effect of SMPL SDF regularization loss in Eqn. (8). If removing the regularization loss,
the performance in all metrics drops (FID: 10.8 vs. 9.6, Depth: 0.96 vs. 0.86, Warp: 7.8 vs. 6.9),
verifying the effectiveness of the proposed loss for regularized geometry learning.

SDF prediction schemes. Table 2f shows the effect of two SDF prediction schemes—predicting raw
SDFs directly or SDF residuals on top of the coarse SMPL body SDFs. Compared with predicting
the raw SDFs directly, the residual prediction scheme delivers better results, since it alleviates the
geometry learning difﬁculties.

4.3 Applications

Single-view 3D reconstruction and re-pose. The right panel of Fig. 4 shows the application of our
learned latent space for single-view 3D reconstruction. Following [2], we use pivotal tuning inversion
(PTI) [52] to ﬁt the target images (top) and recover both the appearance and the geometry (middle).
With the recovered 3D representation and latent code, we can further use novel SMPL parameters
(bottom) to re-pose/animate the human in the source images.

Text-guided synthesis. Recent works [46, 28] have shown that one could use a text-image embedding,
such as CLIP [51], to guide StyleGAN2 for controlled synthesis. We also visualize text-guided
clothed human synthesis in Fig. 5. Speciﬁcally, we use StyleCLIP [46] to manipulate a synthesized
image with a sequence of text prompts. The optimization based StyleCLIP is used as it is ﬂexible for
any input text. From the ﬁgure, our AvatarGen is able to synthesis different style human images given

9

Figure 4: (Left) Qualitative comparison of multi-view rendering and geometry quality against
(Right) Single-view 3D reconstruction and
baselines including EG3D [2] and StyleSDF [42].
reanimation result of AvatarGen. Given source image, we reconstruct both color and geometry of the
human in the image. The re-pose step further takes novel SMPL parameters as input and animates
the reconstructed avatar.

Figure 5: Text-guided (left) synthesis results of AvatarGen with multi-view rendering (right).

different text prompts. This clearly indicates that AvatarGen can be an effective tool for text-guided
portrait synthesis where detailed descriptions are provided.

5 Conclusion

This work introduced the ﬁrst 3D-aware human avatar generative model, AvatarGen. By factorizing
the generative process into the canonical avatar generation and deformation stages, AvatarGen
can leverage the geometry prior and effective tri-plane representation to address the challenges in
animatable human avatar generation. We demonstrated AvatarGen can generate clothed human avatars
with arbitrary poses and viewpoints. Besides, it can also generate avatars from multi-modality input
conditions, like natural language description and 2D images (for inverting). This work substantially
extends the 3D generative models from objects of simple structures (e.g., human faces, rigid objects)
to articulated and complex objects. We believe this model will make the creation of human avatars
more accessible to ordinary users, assist designers and reduce the manual cost.

10

SourceInversionRe-poseStyleSDFEG3DOurs“A lady is wearing a blue shirt and black jeans. She has black hair.”“The woman wears striped clothes, long white trousers and white shoes.”“This woman is wearing a red shirt and gray pants with graphic patterns.”References

[1] Panos Achlioptas, Olga Diamanti, Ioannis Mitliagkas, and Leonidas Guibas. Learning representations and

generative models for 3d point clouds. In ICML, 2018.

[2] Eric R Chan, Connor Z Lin, Matthew A Chan, Koki Nagano, Boxiao Pan, Shalini De Mello, Orazio
Gallo, Leonidas Guibas, Jonathan Tremblay, Sameh Khamis, et al. Efﬁcient geometry-aware 3d generative
adversarial networks. CVPR, 2022.

[3] Eric R Chan, Marco Monteiro, Petr Kellnhofer, Jiajun Wu, and Gordon Wetzstein. pi-gan: Periodic implicit

generative adversarial networks for 3d-aware image synthesis. In CVPR, 2021.

[4] Jianchuan Chen, Ying Zhang, Di Kang, Xuefei Zhe, Linchao Bao, Xu Jia, and Huchuan Lu. Animatable

neural radiance ﬁelds from monocular rgb videos. arXiv, 2021.

[5] Mingfei Chen, Jianfeng Zhang, Xiangyu Xu, Lijuan Liu, Yujun Cai, Jiashi Feng, and Shuicheng Yan.
Geometry-guided progressive nerf for generalizable and efﬁcient neural human rendering. arXiv, 2021.

[6] Zhiqin Chen and Hao Zhang. Learning implicit ﬁelds for generative shape modeling. In CVPR, 2019.

[7] Alvaro Collet, Ming Chuang, Pat Sweeney, Don Gillett, Dennis Evseev, David Calabrese, Hugues Hoppe,
Adam Kirk, and Steve Sullivan. High-quality streamable free-viewpoint video. In ACM Trans. on Graphics,
2015.

[8] PaddlePaddle Contributors. Paddleseg, end-to-end image segmentation kit based on paddlepaddle. https:

//github.com/PaddlePaddle/PaddleSeg, 2019.

[9] Paul Debevec, Tim Hawkins, Chris Tchou, Haarm-Pieter Duiker, Westley Sarokin, and Mark Sagar.
Acquiring the reﬂectance ﬁeld of a human face. In Proceedings of the 27th annual conference on Computer
graphics and interactive techniques, 2000.

[10] Yu Deng, Jiaolong Yang, Jianfeng Xiang, and Xin Tong. Gram: Generative radiance manifolds for

3d-aware image generation. CVPR, 2022.

[11] Haoye Dong, Xiaodan Liang, Xiaohui Shen, Bochao Wang, Hanjiang Lai, Jia Zhu, Zhiting Hu, and Jian

Yin. Towards multi-pose guided virtual try-on network. In ICCV, 2019.

[12] Mingsong Dou, Sameh Khamis, Yury Degtyarev, Philip Davidson, Sean Ryan Fanello, Adarsh Kowdle,
Sergio Orts Escolano, Christoph Rhemann, David Kim, Jonathan Taylor, et al. Fusion4d: Real-time
performance capture of challenging scenes. In ACM Trans. on Graphics, 2016.

[13] Jianglin Fu, Shikai Li, Yuming Jiang, Kwan-Yee Lin, Chen Qian, Chen-Change Loy, Wayne Wu, and

Ziwei Liu. Stylegan-human: A data-centric odyssey of human generation. arXiv, 2022.

[14] Thiago L. Gomes, Thiago M. Coutinho, Rafael Azevedo, Renato Martins, and Erickson R. Nascimento.

Creating and reenacting controllable 3d humans with differentiable rendering. In WACV, 2022.

[15] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron

Courville, and Yoshua Bengio. Generative adversarial nets. In NeurIPS, 2014.

[16] Jiatao Gu, Lingjie Liu, Peng Wang, and Christian Theobalt. Stylenerf: A style-based 3d-aware generator

for high-resolution image synthesis. CVPR, 2022.

[17] Kaiwen Guo, Peter Lincoln, Philip Davidson, Jay Busch, Xueming Yu, Matt Whalen, Geoff Harvey, Sergio
Orts-Escolano, Rohit Pandey, Jason Dourgarian, et al. The relightables: Volumetric performance capture
of humans with realistic relighting. In ACM Trans. on Graphics, 2019.

[18] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans

trained by a two time-scale update rule converge to a local nash equilibrium. NeurIPS, 2017.

[19] Fangzhou Hong, Mingyuan Zhang, Liang Pan, Zhongang Cai, Lei Yang, and Ziwei Liu. Avatarclip:

Zero-shot text-driven generation and animation of 3d avatars. ACM Trans. on Graphics, 2022.

[20] Zeng Huang, Yuanlu Xu, Christoph Lassner, Hao Li, and Tony Tung. Arch: Animatable reconstruction of

clothed humans. In CVPR, 2020.

[21] Alec Jacobson, Ilya Baran, Ladislav Kavan, Jovan Popovi´c, and Olga Sorkine. Fast automatic skinning

transformations. ACM Trans. on Graphics, 2012.

11

[22] Boyi Jiang, Yang Hong, Hujun Bao, and Juyong Zhang. Selfrecon: Self reconstruction your digital avatar

from monocular video. In CVPR, 2022.

[23] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of GANs for improved

quality, stability, and variation. In ICCV, 2018.

[24] Tero Karras, Miika Aittala, Samuli Laine, Erik Härkönen, Janne Hellsten, Jaakko Lehtinen, and Timo Aila.

Alias-free generative adversarial networks. In NeurIPS, 2021.

[25] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial

networks. In CVPR, 2019.

[26] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and

improving the image quality of StyleGAN. In CVPR, 2020.

[27] Nikos Kolotouros, Georgios Pavlakos, Michael J Black, and Kostas Daniilidis. Learning to reconstruct 3d

human pose and shape via model-ﬁtting in the loop. In ICCV, 2019.

[28] Gihyun Kwon and Jong Chul Ye. Clipstyler: Image style transfer with a single text condition.

In
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18062–
18071, 2022.

[29] Ruihui Li, Xianzhi Li, Chi-Wing Fu, Daniel Cohen-Or, and Pheng-Ann Heng. Pu-gan: a point cloud

upsampling adversarial network. In ICCV, 2019.

[30] Yiyi Liao, Katja Schwarz, Lars Mescheder, and Andreas Geiger. Towards unsupervised learning of

generative models for 3D controllable image synthesis. In CVPR, 2020.

[31] Lingjie Liu, Marc Habermann, Viktor Rudnev, Kripasindhu Sarkar, Jiatao Gu, and Christian Theobalt.
Neural actor: Neural free-view synthesis of human actors with pose control. ACM Trans. on Graphics,
2021.

[32] Ting Liu, Jianfeng Zhang, Xuecheng Nie, Yunchao Wei, Shikui Wei, Yao Zhao, and Jiashi Feng. Spatial-
aware texture transformer for high-ﬁdelity garment transfer. In IEEE Trans. on Image Processing, 2021.

[33] Ziwei Liu, Ping Luo, Shi Qiu, Xiaogang Wang, and Xiaoou Tang. Deepfashion: Powering robust clothes

recognition and retrieval with rich annotations. In CVPR, 2016.

[34] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll, and Michael J Black. Smpl: A

skinned multi-person linear model. ACM Trans. on Graphics, 2015.

[35] Lars Mescheder, Andreas Geiger, and Sebastian Nowozin. Which training methods for gans do actually

converge? In International conference on machine learning, pages 3481–3490. PMLR, 2018.

[36] Lars Mescheder, Michael Oechsle, Michael Niemeyer, Sebastian Nowozin, and Andreas Geiger. Occupancy

networks: Learning 3d reconstruction in function space. In CVPR, 2019.

[37] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng.

Nerf: Representing scenes as neural radiance ﬁelds for view synthesis. In ECCV, 2020.

[38] Thu Nguyen-Phuoc, Chuan Li, Lucas Theis, Christian Richardt, and Yong-Liang Yang. HoloGAN:

Unsupervised learning of 3D representations from natural images. In ICCV, 2019.

[39] Thu Nguyen-Phuoc, Christian Richardt, Long Mai, Yong-Liang Yang, and Niloy Mitra. BlockGAN:

Learning 3D object-aware scene representations from unlabelled images. In NeurIPS, 2020.

[40] Michael Niemeyer and Andreas Geiger. Giraffe: Representing scenes as compositional generative neural

feature ﬁelds. In CVPR, 2021.

[41] Atsuhiro Noguchi, Xiao Sun, Stephen Lin, and Tatsuya Harada. Neural articulated radiance ﬁeld. In ICCV,

2021.

[42] Roy Or-El, Xuan Luo, Mengyi Shan, Eli Shechtman, Jeong Joon Park, and Ira Kemelmacher-Shlizerman.

Stylesdf: High-resolution 3d-consistent image and geometry generation. CVPR, 2022.

[43] Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, and Steven Lovegrove. Deepsdf:

Learning continuous signed distance functions for shape representation. In CVPR, 2019.

[44] Keunhong Park, Utkarsh Sinha, Jonathan T Barron, Soﬁen Bouaziz, Dan B Goldman, Steven M Seitz, and

Ricardo Martin-Brualla. Deformable neural radiance ﬁelds. arXiv, 2020.

12

[45] Keunhong Park, Utkarsh Sinha, Jonathan T Barron, Soﬁen Bouaziz, Dan B Goldman, Steven M Seitz, and

Ricardo Martin-Brualla. Nerﬁes: Deformable neural radiance ﬁelds. In ICCV, 2021.

[46] Or Patashnik, Zongze Wu, Eli Shechtman, Daniel Cohen-Or, and Dani Lischinski. Styleclip: Text-driven

manipulation of stylegan imagery. In ICCV, 2021.

[47] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Hujun Bao, and Xiaowei Zhou.

Animatable neural radiance ﬁelds for human body modeling. ICCV, 2021.

[48] Sida Peng, Shangzhan Zhang, Zhen Xu, Chen Geng, Boyi Jiang, Hujun Bao, and Xiaowei Zhou. Animatable

neural implicit surfaces for creating avatars from videos. arXiv, 2022.

[49] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou.
Neural body: Implicit neural representations with structured latent codes for novel view synthesis of
dynamic humans. In CVPR, 2021.

[50] Albert Pumarola, Enric Corona, Gerard Pons-Moll, and Francesc Moreno-Noguer. D-nerf: Neural radiance

ﬁelds for dynamic scenes. In CVPR, 2021.

[51] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish
Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from
natural language supervision. In ICML, 2021.

[52] Daniel Roich, Ron Mokady, Amit H Bermano, and Daniel Cohen-Or. Pivotal tuning for latent-based

editing of real images. ACM Trans. on Graphics, 2021.

[53] Shunsuke Saito, Tomas Simon, Jason Saragih, and Hanbyul Joo. Pifuhd: Multi-level pixel-aligned implicit

function for high-resolution 3d human digitization. In CVPR, 2020.

[54] Katja Schwarz, Yiyi Liao, Michael Niemeyer, and Andreas Geiger. Graf: Generative radiance ﬁelds for

3d-aware image synthesis. NeurIPS, 2020.

[55] Shih-Yang Su, Frank Yu, Michael Zollhöfer, and Helge Rhodin. A-nerf: Articulated neural radiance ﬁelds

for learning human shape, appearance, and pose. In NeurIPS, 2021.

[56] Zhuo Su, Lan Xu, Zerong Zheng, Tao Yu, Yebin Liu, and Lu Fang. Robustfusion: Human volumetric

capture with data-driven visual cues using a rgbd camera. In ECCV, 2020.

[57] Attila Szabó, Givi Meishvili, and Paolo Favaro. Unsupervised generative 3D shape learning from natural

images. arXiv, 2019.

[58] Ayush Tewari, MalliKarjun B R, Xingang Pan, Ohad Fried, Maneesh Agrawala, and Christian Theobalt.
Disentangled3d: Learning a 3d generative model with disentangled geometry and appearance from
monocular images. In CVPR, 2022.

[59] Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku Komura, and Wenping Wang. Neus: Learning

neural implicit surfaces by volume rendering for multi-view reconstruction. NeurIPS, 2021.

[60] Chung-Yi Weng, Brian Curless, Pratul P. Srinivasan, Jonathan T. Barron, and Ira Kemelmacher-Shlizerman.

HumanNeRF: Free-viewpoint rendering of moving people from monocular video. CVPR, 2022.

[61] Jiajun Wu, Chengkai Zhang, Tianfan Xue, William T. Freeman, and Joshua B. Tenenbaum. Learning a
probabilistic latent space of object shapes via 3D generative-adversarial modeling. In NeurIPS, 2016.

[62] Donglai Xiang, Fabian Prada, Timur Bagautdinov, Weipeng Xu, Yuan Dong, He Wen, Jessica Hodgins,
and Chenglei Wu. Modeling clothing as a separate layer for an animatable human avatar. ACM Trans. on
Graphics, 2021.

[63] Hongyi Xu, Thiemo Alldieck, and Cristian Sminchisescu. H-nerf: Neural radiance ﬁelds for rendering and

temporal reconstruction of humans in motion. NeurIPS, 2021.

[64] Xiangyu Xu and Chen Change Loy. 3D human texture estimation from a single image with transformers.

In ICCV, 2021.

[65] Yang Xue, Yuheng Li, Krishna Kumar Singh, and Yong Jae Lee. Giraffe hd: A high-resolution 3d-aware

generative model. In CVPR, 2022.

[66] Lior Yariv, Jiatao Gu, Yoni Kasten, and Yaron Lipman. Volume rendering of neural implicit surfaces. In

NeurIPS, 2021.

13

[67] Polina Zablotskaia, Aliaksandr Siarohin, Bo Zhao, and Leonid Sigal. Dwnet: Dense warp-based network

for pose-guided human video generation. BMVC, 2019.

[68] Jiakai Zhang, Xinhang Liu, Xinyi Ye, Fuqiang Zhao, Yanshun Zhang, Minye Wu, Yingliang Zhang, Lan
Xu, and Jingyi Yu. Editable free-viewpoint video using a layered neural representation. ACM Trans. on
Graphics, 2021.

[69] Jiakai Zhang, Liao Wang, Xinhang Liu, Fuqiang Zhao, Minzhang Li, Haizhao Dai, Boyuan Zhang, Wei
Yang, Lan Xu, and Jingyi Yu. Neuvv: Neural volumetric videos with immersive rendering and editing.
ACM Trans. on Graphics, 2022.

14

