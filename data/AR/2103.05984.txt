1
2
0
2

r
a

M
0
1

]

C
H
.
s
c
[

1
v
4
8
9
5
0
.
3
0
1
2
:
v
i
X
r
a

Mixed Reality Interaction Techniques

JENS GRUBERT

This chapter gives an overview of interaction techniques for mixed reality including augmented and virtual
reality (AR/VR). Various modalities for input and output are discussed. Specifically, techniques for tangible
and surface-based interaction, gesture-based, pen-based, gaze-based, keyboard and mouse-based, as well as
haptic interaction are discussed. Furthermore, the combination of multiple modalities in multisensory and
multimodal interaction as well as interaction using multiple physical or virtual displays are presented. Finally,
interaction with intelligent virtual agents is considered.

1 INTRODUCTION
This chapter gives an overview of interaction techniques for mixed reality (MR), including aug-
mented and also virtual reality (AR/VR). Early research in the field of MR interaction techniques
focused on the use of surface-based, tangible as well as gesture-based interaction, which will
be presented at the beginning of this chapter. Further modalities, such as pen-based, gaze-based
or haptic interaction have seen recent attention and are presented next. Further, with the move
towards productivity-oriented use cases, interaction with established input devices such as key-
board and mice has seen interest in the research community. Finally, inspired by the popularity of
conversational agents, interaction with intelligent virtual agents are discussed. The development of
interaction techniques is closely related to the advancements in input devices. Hence, the reader is
invited to study the according book chapter as well. While this chapter follows the above mentioned
structure, further possibilities to structure interaction techniques include organizing according to
interaction tasks [41] such as object selection [4, 80, 302] and object manipulation [219], naviga-
tion [163], symbolic input [89] or system control [79]. Further, interaction techniques for specific
application domains have been discussed such as music [27] games [273] or immersive analytics
[51, 98]. Interested readers are also referred to further surveys on 3D interaction techniques [145]
or interaction with smart glasses [196].

2 TANGIBLE AND SURFACE-BASED INTERACTION
This section presents the concepts of Tangible user interfaces (TUIs) and their applicability in AR.
It covers the effects of output media, spatial registration approaches for TUIs, tangible magic lenses,
augmenting large surfaces like walls and whole rooms, the combination of AR with shape-changing
displays as well as the role of TUIs for VR-based interaction. Figure 1 depicts an overview about
output and input devices typically found in TUI-based interaction for MR.

TUIs are concerned with using physical objects as medium for interaction with computers [325]
and have seen substantial interest in human-computer interaction [295]. Early prototypes utilized
tabletops, on which physical objects were placed to change properties of digital media. For example
Underkoffler and Ishii introduced a simulation of an optical workbench using tangible objects on a
tabletop [326] as well as an application for architectural planning [327].

In AR, this concept was introduced by Kato et al. [170] as Tangible Augmented Reality (TAR).
They used a paddle as prop, equipped with a fiducial, to place furniture inside a house model. Fjeld

Author’s address: Jens Grubert, jg@jensgrubert.de.

Author Version.

 
 
 
 
 
 
2

Grubert

Fig. 1. Classification of input and output devices used in tangible user interfaces for AR and VR. OST: Optical
See-Through. VST: Video See-Through. HMDs: Head-Mounted Displays.

et al. [108] introduced further tangibles such as a booklet and a cube for interacting within an
educational application for chemistry.

TAR is typically used for visualizing digital information on physical objects while concurrently
using those physical objects as interaction devices. Billinghurst et al. [33] state TAR characteristics
as realizing spatial registration between virtual and physical objects and the ability for users to
interact with those virtual objects by manipulating the physical ones. For example, Regenbrecht et
al. [264] utilized a rotary plate to allow multiple co-located users to manipulate the orientation of a
shared virtual object. This way, the gap between digital output (e.g., on a flat screen) and physical
input (e.g., using a rotary knob) can be reduced as the digital information is directly overlaid over
the physical content. Lee et al. [194] described common interaction themes in TAR applications
such as static and dynamic mappings between physical and digital objects. They describe a space-
multiplexed approach, where each physical tool is mapped to a single virtual tool or function as

Mixed Reality Interaction Techniques

3

well as a time-multiplexed approach in which the physical object is mapped to different digital
tools dependent on the context-of use. However, the effect of this overlay depends also on the
output medium. For example, when using projection-based systems [34] or video see-through (VST)
head-mounted displays (HMDs) (c.f. chapter 10 in [143]), the distance between the observer to the
physical and virtual objects is the same. In contrast, when using commodity optical see-through
(OST) HMDs with a fixed focal plane, there can be a substantial cost of perceiving virtual and
physical objects at the same time. Specifically, Eiberger et al. [94] showed that when processing
visual information jointly from objects within arms reach (in this case a handheld display) and
information presented on a OST HMD at a different distance, task completion times increased by
approximately 50% and error rate increased by approximately 100% compared to processing this
visual information solely on the OST HMD.

For spatially registering physical and virtual objects, early works on TAR often relied on fiducial
markers, such as provided by ARToolKit [169] or ARUCO [279]. While, easy to prototype (i.e.
fiducials have to be simply printed out and attached to objects), these markers can inhibit interaction
due to their susceptibility to occlusions (typically through hand and finger interaction). Hence, it is
advised to use modern approaches for hand-based interaction [16, 203] with spatially tracked rigid
and non-rigid objects [141, 259, 346]. Also, when using OST HMDs, the calibration between the
HMD and the users’ eyes can impact the interaction [120, 134].

Evolving from the magic lens [30] and tangible interaction concepts [325] tangible magic lenses
allow to access and manipulate otherwise hidden data in interactive spatial environments. A wide
variety of interaction concepts for interactive magic lenses have been proposed within the scope of
information visualization (c.f. [319, 320]).

Within AR, various rigid shapes have been explored. Examples include rectangular lenses for
tabletop interaction [305] or circular lenses [306]. Flexible shapes [308] have been utilized as well as
multiple sheets of paper [156]. In their pioneering work, Szalavári and Gervautz [314] introduced
the personal-interaction-panel in AR. The two-handed and pen-operated tablet allowed for selection
and manipulation of virtual objects as well as for system control. Additionally, transparent props
have been explored (e.g., a piece of plexiglass) both for tabletop AR [44, 242, 270] as well as VR
[283]. Purely virtual tangible lenses have been proposed as well [207]. Brown et al. [45] introduced
a cubic shape which could either perspectively correct render and manipulate 3D objects or text.
This idea was later revisited by Issartel et al. [161] in a mobile setting.

Often, projection-based AR has been used to realize tangible magic lenses, in which a ceiling-
mounted projector illuminates a prop such as a piece of cardboard or other reflective materials
[61, 305] and (typically RGB or depth) cameras process user input.

Mobile devices such as smartphones and tablets are also commonly used as a tangible magic
lens [131, 199], see Figure 2, and can be used in conjunction with posters [131], books [178], digital
screens [199] or maps [226, 267].

When using the tangible magic lens metaphor in public space, one should be aware about the
social acceptability, specifically due to the visibility of spatial gestures and postures [263, 272].
For example, in a series of studies across gaming and touristic use cases, Grubert et al. [127, 132]
explored benefits and drawbacks of smartphone-based tangible lens interfaces in public settings
and compared them to traditional static peephole interaction, commonly used in mobile map
applications. They found that user acceptance is largely dependent on the social and physical
setting. In a public bus stop inside a large open space used at transit area, participants favored
the magic lens over a static peephole interface despite tracking errors, fatigue and potentially
conspicuous gestures. Also, most passersby did not pay attention to the participants and vice
versa. However, when deploying the same experience in a different public transportation stop with
other spatial and social context (waiting area, less space to avoid physical proximity to others),

4

Grubert

Fig. 2. Two magic lens applications in handheld AR. from left to right: Smartphone-based magic lens interface
reveals virtual huts and prices on a physical ski map [131]. Tourist searches for prices in front of the map.
Find and select game using a handheld magic lens interface. User playing the game in front of a physical
poster. [127]

Fig. 3. In device-perspective rendering (left) the scene is rendered from the point of view of the camera. In
user perspective rendering (right), the scene is rendered from the point of view of the user’s head [225].

participants used and preferred the magic lens interface significantly less compared to a static
peephole interface. In the context of of evaluations of magic lens metaphors in handheld AR, the
impact of tracking technology also has to be considered [228].

Further, when using smartphones or tablets as magic lenses, the default user’s view is based on
the position of the physical camera attached to the handheld device, see Figure 3, left and Figure 4,
left. However, this can potentially negatively affect the user experience [74, 75]. Hence, it can be
advisable to incorporate user-perspective rendering to render the scene from the point of view of
the user’s head, see Figure 3, right and Figure 4, right. For example, Hill et al. [153], introduced
user-perspective rendering as virtual transparency for VST AR. Baričević et al [15] compared user-
vs. device-perspective rendering in a VR simulation. Tomioka et al. [321] presented approximated
user-perspective rendering using homographies. Grubert et al. [133] proposed a framework for
enabling user-perspective rendering for augmenting public displays. Čopič et al. [74, 75], quantified
the performance differences between device- and user perspective rendering in map-related tasks
and Mohr et al. [225], developed techniques for efficient computation of head-tracking techniques
needed for user-perspective rendering.

Beyond handheld solutions, whole surfaces such as tables, walls or body parts can be augmented
and interacted with. Often projector-camera systems are used for processing input and creating
output on surfaces. Early works included augmenting desks using projectors to support office
work of single users [211, 338, 339] or in collaborative settings [269]. Later, the Microsoft Kinect
and further commodity depth sensors gave rise to a series of explorations with projector-camera
systems. For example, Xiao et al. [348] introduced WorldKit, to allow users to sketch and operate
user interface elements on everyday surfaces. Corsten et al. [76] proposed a pipeline for repurposing

Mixed Reality Interaction Techniques

5

Fig. 4. Effects of device-perspective rendering (left) and user perspective rendering (right) [225].

everyday objects as input devices. Henderson and Feiner also proposed to utilize passive haptic
feedback from everyday objects to interact with virtual control elements such as virtual buttons
[151]. Mistry and Maes [224] utilized a necklace-mounted projector-camera system to sense finger
interactions and project content on hands or the environment. Following suit, Harrison et al., [147]
introduced OmniTouch, a wearable projector-depth-camera system that allowed to project user
interface elements on body parts, such as the hand (e.g., a virtual dial pad) or to augment paper
using touch.

Beyond handheld solutions, whole surfaces such as tables, walls or body parts can be augmented
and interacted with. Often projector-camera systems are used for processing input and creating
output on surfaces. Early works included augmenting desks using projectors to support office
work of single users [211, 338, 339] or in collaborative settings [269]. Later the Microsoft Kinect
and further commodity depth sensors gave rise to a series of explorations with projector-camera
systems.

For example, Xiao et al. [348] introduced WorldKit, to allow users to sketch and operate user
interface elements on everyday surfaces. Corsten et al. [76] proposed a pipeline for repurposing
everyday objects as input devices. Henderson and Feiner also proposed to utilize passive haptic
feedback from everyday objects to interact with virtual control elements such as virtual buttons
[151].

Mistry and Maes [224] utilized a necklace-mounted projector-camera system to sense finger
interactions and project content on hands or the environment. Following suite, Harrison et al.,
[147] introduced OmniTouch, a wearable projector-depth-camera system that allowed to project
user interface elements on body parts, such as the hand (e.g., a virtual dial pad) or to augment
paper using touch.

Further, the idea of interacting with augmented surfaces was later expanded to cover bend
surfaces [24], walls [167], complete living rooms [166] or even urban facades [40, 107]. For example,
in IllumiRoom [167], the area around a television was augmented using a projector, after initially
scanning it with a depth camera, see Figure 5. Possible augmentations included extending the
field of view of on-screen content, selectively rendering scene elements of a game or changing
the appearance of the whole environment using non-photorealistic rendering (e.g., cartoon style
or a wobble effect). In RoomAlive, multiple projector-depth camera units were used to create a
3D scan of a living room as well as to spatially track the user’s movement within that room, see
Figure 6. Users were able to interact with digital elements projected in the room using touch and
in-air gestures. Apart from entertainment purposes, this idea was also investigated in productivity
scenarios such as collaborative content sharing in meetings [105]. Finally, the augmentation of

6

Grubert

Fig. 5. The area around a TV is augmented using a projector in IllumiRoom [167]. Left: The unmodified scene.
Middle: the The field of view of a game is extended beyond the TV boundaries. Right: the appearance of the
scene is changed using a cartoon style rendering. Image courtesy of Microsoft Research.

Fig. 6. Using multiple projector-camera systems a whole room is turned into an immersive experience in
RoomAlive [166]. Two different visualizations of the same physical room can be seen. Image courtesy of
Microsoft Research.

shape changing interfaces was also explored [109, 200, 261]. For example, in Sublimate [200] an
actuated pin display was combined with a stereoscopic see-through screen to achieve a close
coupling between physical and virtual object properties, e.g., for visualizing height fields or NURBS
surface modelling. InForm [109] expanded this idea to allow both for user input on its pins (e.g.,
utilizing them as buttons or handles) as well as manipulation of external objects (such as moving a
ball across its surface).

In VR, tangible interaction has been explored using various props. The benefit of using tangibles
in VR is that a single physical object can be used to represent multiple virtual objects [3], even if
they show a certain extend of discrepancy. Simeone et al. [301], presented a model of potential
substitutions based on physical objects such as mugs, bottles, umbrellas or a torch. Hettiarachchi et
al. [152] transferred this idea to AR. Harley et al. [146], proposed a system for authoring narrative
experiences in VR using tangible objects.

Mixed Reality Interaction Techniques

7

3 GESTURE-BASED INTERACTION
Touch and in-air gestures and postures make up a large part of interpersonal communication and
have also been explored in depth in Mixed Reality. A driver for gesture-based interaction was the
desire for "natural" user interaction, i.e. interaction without the need to explicitly handle artificial
control devices, but to rely on easy to learn interaction with (to the user) invisible input devices.
While many gesture sets have been explored by researchers or users [252], it can be debated how
"natural" those gesture-based interfaces really are [236], e.g., due to the poor affordances.

Still, the prevalence of small sensors such as RGB and depth cameras, inertial measurement units,
radars or magnetic sensors in mobile devices, AR and VR HMDs, as well as continuing advances
in hand [16, 203], head [229] and body pose estimation [54, 63, 67, 81, 117, 205, 281] gave rise to a
wide variety of gesture-based interaction techniques being explored for Mixed Reality.

For mobile devices researchers, began investigating options for interaction next to [241], above [110,

186], behind [82, 342], across [65, 123, 258, 286], or around [349, 361] the device.

The additional modalities are either substituting or complementing the devices’ capabilities. These
approaches typically relied on modifying existing devices using a variety of sensing techniques,
which can limit their deployment to mass audiences. Hence, researchers started to investigate the
use of unmodified devices. Nandakumar et al. [231] proposed to use the internal microphones of
mobiles to determine the location of finger movements on surfaces, but could not support mid-air
interaction. Song et. al [303] enabled in-air gestures using the front and back facing cameras of
unmodified mobile devices. With Surround See, Yang et al. [353] modified the front-facing camera
of a mobile phone with an omnidirectional lens, extending its field of view to 360◦ horizontally.
They showcased different application areas, including peripheral environment, object and activity
detection, including hand gestures and pointing, but did not comment on the recognition accuracy.
In GlassHands, it was demonstrated how the input space around a device can be extended, by using
a built-in front-facing camera of an unmodified handheld device and some reflective glasses, like
sunglasses, ski goggles or visors [128]. This work was later extended to investigate the feasibility
of utilizing eye reflections [288, 289].

While being explored since the mid 90’s in tabletop-based AR [46, 78, 87], for handheld AR,
vision-based finger and hand tracking became popular since the mid 2000’s [158, 197, 198, 296].
Yusof et al. [356] provide a survey on the various flavors of gesture-based interaction in handheld
AR, including marker-based and marker-less tracking of fingers or whole hands.

An early example for in-air interaction using AR HMDs is presented by Kolsch et al. [183],
who demonstrated finger tracking with a head-mounted camera. Xiao et al. [350] showed how
to incorporate touch gestures on everyday surfaces in to the Microsoft HoloLens. Beyond hand
and finger tracking, full-body tracking using head-mounted cameras was also explored [60]. Also,
reconstruction of facial gestures, e.g., for reenactment purposes, when wearing HMDs has seen
increased interest [64, 95, 318, 365]. Further solutions for freehand interaction were also proposed
including a wrist-worn gloveless sensor [174], swept frequency capacitive sensing [282], an optical
mouse sensor attached to a finger [352], or radar-based sensing [332].

Many AR and VR in-air interaction techniques rely on using arms not being supported by a
surface (e.g. an elbow resting on a table). Hence, to facilitate reliable selection, targets are designed
to be sufficiently large and spaced apart [304]. Also, while the addition of hand tracking to modern
AR and VR HMDs allows for easy access to in-air gestures, the accuracy of those spatial tracking
solutions still is significantly lower than dedicated lab-based external tracking systems [291].

Besides interaction with handheld or head-worn devices, also whole environments such as
rooms can be equipped with sensors to facilitate gesture-based interaction [43, 230, 360]. In VR,
off-the-shelf controllers were also appropriated to reconstruct human poses in real-time [55, 165].

8

Grubert

Fig. 7. Sketching in VR (top) supported by pen-based input in mid-air (bottom left) as well as using a tablet
as supporting 2D surface (bottom right) [88]. Image courtesy of Tobias Drey.

4 PEN-BASED INTERACTION
In-air interactions in AR and VR typically make use of unsupported hands or controllers designed
for gaming. In addition, pens (often in combination with tablets as supporting surface) have also
been explored as input device. Szalavári and Gervautz [314], and, similarly, Billinghurst et al. [31]
utilized pens for input on physical tablets in AR respectively VR. Watsen et al. [336] used a handheld
Personal Digital Assistant (PDA) for operating menus in VR. In the Studierstube framework, pens
were used to control 2D user interface elements on a PDA in AR. Poupyrev et al. [255] used a pen
for notetaking in VR. Gesslein et al. [116] used a pen for supporting spreadsheet interaction in
Mobile VR.

Researches also investigated the use of pens for drawing and modelling, see Figure 7. Sachs et al.
[277] presented an early system of 3D CAD modeling using a pen. Deering [83] used a pen for in-air
sketching in a fishtank VR environment. Keeve et al. [173] utilized a brush for expressive painting
in a Cave Automatic Virtual Environment (CAVE) environment. Encarnacao [97] used a pen and
pad for sketching in VR on top of an interactive table. Fiorentino et al. [106] explored the use of
pens in mid-air for CAD applications in VR. Xin et al. [351] enabled the creation of 3D sketches
using pen and tablet interaction in handheld AR. Yee et al. [354] used a pen-line device along a
VST HMD for in-situ sketching in AR. Gasquez et al. [114, 115], Arora et al. [5], as well as Drey et
al. [88] noted the benefits of supporting both free-form in-air sketching as well as sketching on a
supporting 2D surface in AR and VR. Suzuki et al. [313] expanded previous sketching applications
for AR with dynamic and responsive graphics, e.g. to support physical simulations.

The performance of pen-based input was also investigated in VR. Bowman and Wingrave [42]
compared pen and tablet input for menu selection against floating menus and a pinch-based

Mixed Reality Interaction Techniques

9

menu system and found that pen and tablet interaction was significantly faster. Teather and
Stuerzlinger [317] compared pen-based in put to mouse input for target selection in a fishtank VR
environment and found that 3D pointing was inferior to 2D pointing when targets where rendered
stereoscopically. Arora et al. [6] compared pen-based mid-air painting to surface-supported painting
and found supporting evidence that accuracy improved using a physical drawing surface. Pham
et al. [250] indicated that pens significantly outperform controllers for input in AR and VR and is
comparable to mouse-based input for target selection. Batmaz et al. explored different pen grip
styles for target selection in VR [17].

5 GAZE-BASED INTERACTION
Besides utilizing touch input, in-air gestures or physical input devices, gaze has also been explored
as input modality in Mixed Reality. Duchowski [91] presents a review of 30 years of gaze-based
interaction, in which gaze-based interaction is categorized within a taxonomy that splits interaction
into four forms, namely diagnostic (off-line measurement), active (selection, look to shoot), passive
(foveated rendering or gaze-contingent displays), and expressive (gaze synthesis).

For VR, Mine [222] proposed to use gaze-directed steering and look-at menus, as early as 1995.
Tanriverdi and Jacob [315] highlighted that VR can benefit from gaze tracking. They stated that
physical effort can by minimized through gaze and that user’s natural eye movement can be
employed to perform interactions in VR (e.g., with distant objects). They also indicated that a
proposed heuristic gaze selection technique outperformed virtual hand-based interaction in terms
of task-completion time. Cournia et al. [77] found that dwell-time based selection was slower
than manual ray-pointing. Duchowski et al. [92] presented software techniques for binocular eye
tracking within VR as well as their application to aircraft inspection training. Specifically, they
presented means for integrating eye trackers into a VR framework, novel 3D calibration techniques
and techniques for eye-movement analysis in 3D space. In 2020, Burova et al. [49] also utilized eye-
gaze analysis in industrial tasks. They used VR to develop AR solutions for maintenance tasks and
collected gaze data to elicit comments from industry experts on the usefulness of the AR simulation.
Zeleznik et al. [358] investigated gaze interaction for 3D pointing, movement, menu selection and
navigation (orbiting and flying) in VR. They introduced Lazy interactions that minimize hand
movements, Helping Hand techniques in which gaze augments hand-based techniques as well as
Hands Down techniques, in which the hand can operate a separate input device. Piumsomboon et al.
[253] presented three novel eye-gaze-based interaction techniques for VR: Duo-Reticles, an eye-gaze
selection techniques based on eye-gaze and inertial reticles, Radial Pursuit, a smooth pursuit-based
technique for cluttered object and Nod and Roll, a head-gesture-based interaction based on the
vestibulo-ocular reflex.

6 HAPTIC INTERACTION
Auditory and visual channels are widely addressed sensory channels in AR and VR systems. Still,
human experiences can be enriched greatly through touch and physical motion. Haptic devices
enable the interaction between humans and computers by rendering mechanical signals to stimulate
human touch and kinesthetic channels. Research in haptics has a long tradition and incorporates
expertise from various fields such as robotics, psychology, biology and computer science. Haptics
also play a role in diverse application domains such as gaming [85], industry [347], education [223]
or medicine [73, 144, 341]. Haptic interactions are based on cutaneous/tactile (i.e. skin-related) and
kinesthetic/proprioceptive (i.e. related to the body pose) sensations. Various devices have been
proposed for both sensory channels, varying in form factor, weight, mobility, comfort as well as
the fidelity, duration and intensity of haptic feedback. For recent surveys, please see [26, 245].

10

Grubert

Fig. 8. A haptic controller for VR (NormalTouch, [22]) is handheld by a user (middle), allowing to render the
surface height and normal when touching a virtual object (right). Image courtesy of Microsoft Research.

Also, in VR, using haptic feedback has a long tradition [218]. A commonly used active haptic
device for stationary VR environment with a limited movement range of the users hands, is the
PHANToM , which is a grounded system (or manipulandum) offering a high fidelity but low
portability. Hence, over time substantial research efforts have been made in creating mobile haptic
devices for VR [245], see Figure, 8.

In AR, a challenge for using haptics is that the display typically occludes real objects the user
might want to interact with. Also, in OST displays, the haptic device is still visible behind virtual
objects rendered on the display. When using VST displays, the haptic device might be removed by
inpainting [280].

Besides active haptic systems, researchers have also investigated the use of low-fidelity physical
objects to augment virtual environments in passive haptics. An early example of this type of haptic
feedback is presented by Insko [160], who showed that passive haptics can improve both sense of
presence and spatial knowledge training transfer in a virtual environment.

A challenge when using passive haptic feedback, besides a mismatch in surface fidality, is that
the objects used for feedback are typically static. To mitigate this problem two strategies can be
employed. First, the objects themselves can be moved during interaction by mounting them on
robotic platforms such as robots [312, 334] or by human operators [68, 70]. Second, the movements
of the user themselves can be redirected to a certain extend by decoupling the physical motion of a
user from the perceived visual motion. This can be done with individual body parts such as hands
[8, 69], see Figure 9 or the whole body using redirected walking techniques [181, 234].

7 MULTIMODAL INTERACTION
While, often, AR and VR system offer single input channels along with audio-visual output, rich
interaction opportunities arise when considering the combination of further input and output
modalities. Complementing the strengths of multiple channels can lead to enriched user experiences.
While multimodal (or multisensory) output is typically concerned with increasing the immersion
and sense of presence in a scene, multimodal input typically tries to increase the efficiency of user
interaction with a AR or VR system. For overviews about mutlimodal interaction beyond AR and
VR, please see works by Jaimes and Sebe [162] or Turk [324]. Nizam et al. also provide a recent
overview about multimodal interaction for specifically for AR [235].

The use of multisensory output such as the combination of audiovisual output with smell and
touch has been shown to increase presence and perceived realism in VR [57, 155] and has been
employed as early as in the 1960s [150]. Gallace et al. discussed both benefits and challenges when

Mixed Reality Interaction Techniques

11

Fig. 9. A user has the illusion of touching three different virtual cubes (top row), but is solely touching a single
physical cube, to which his arm movements are redirected (bottom row) [8]. Image courtesy of Microsoft
Research.

utilizing multiple output modes in VR [113]. Extrasensory experiences, [90, 201] (such as making
temperature visible through infrared cameras) has also been explored [179].

In AR, Narumi et al. [232] showed that increasing the perceived size of a real cookie using
AR also increased the feeling of satiety. Narumi et al. [233] also created a multisensory eating
expeirence in AR by changing the apparent look and smell of cookies. Koizumi et al. [182] were able
to modulate the perceived food texture using a bone-conducting speaker. Ban et al. [11], showed
that it is possible to influence fatigue while handling physical objects by affecting the perceived
weight of those objects through modulating their size using AR.

Regarding multimodal input in VR, the combination of speech and gestures is a commonly used
input combination. In 1980, Bolt [37] introduced put-that-there. Users could immerse themselves in
a Media Room to place objects within that environment through a combination of gestures and
speech. In 1989, Hauptmann [148] showed that users preferred a combination of speech and and
gestures for the spatial manipulation of 3D objects. Cohen et al. [72] used a handheld computer
along with speech and gesture for supporting map-based tasks on a virtual workbench. LaViola
[191], used hand-based interaction (sensed through a data glove) along with speech for interior
design in VR. Ciger et al. [71] combined speech with pointing of a magic wand on an immersive wall
to create "magical" experiences. Burdea et al. [48], present an early survey on VR input and output
devices as well as an overview about studies that quantify the potentials of several modalities on
simulation realism and immersion. Prange et al. [256], studied the use of speech and pen-based
interaction in a medical setting.

In AR, Olwal et al. [243], combined speech and gestures for object selection. Kaiser et al. [168]
extended that work by introducing mutual disambiguation to improve selection robustness. Simi-
larly, Heidemann et al. [149], presented an AR system for online acquisition of visual knowledge
and retrieval of memorized objects using speech and deictic (pointing) gestures. Kolsch et al. [183],
combined speech input with gestures in an outdoor AR environment. Piumsomboon [251], studied

12

Grubert

Fig. 10. Combined in-air and touch interaction for multiscale map navigation [220]. Left: View on a virtual
map at a small scale. In a study, users were asked to select an active target (blue dot), with inactive targets
visualized as red dots. The pink sphere indicates the fingertip position, the white disc below the pivot point
for zooming. The touch screen area of the smartphone is shown as semi-transparent blue rectangle with red
border. Right: View on the VR scene at 1:1 scale. The circular targets are complemented with 3D flag symbols
to indicate that targets can be selected. The two fingertips used for zooming are indicated by yellow and pink
spheres. The pivot point indicated by the white disc is visualized half-way between both fingertips.

the use of gestures and speech vs gestures only for object manipulation in AR. They found, that
the multimodal was not substantially better than gesture-only based interaction for most tasks (but
object scaling). This indicates, that multimodality per se is not always beneficial for interaction,
but needs to be carefully designed to suit the task at hand. Rosa et al. [274], discussed different
notions of AR and Mixed Reality as well as the role of multimodality. Wilson et al. [343] used a
projector-camera system mounted on a pan-tilt platform for multimodal inteaction in a physical
room using a combination of speech and gestures.

The combination of touch and 3D movements has also been explored in VR and AR. Tsang
et al. [322], introduced the Boom Chameleon, touch display mounted on a tracked mechanical
boom and used joint gesture, speech and viewpoint input in a 3D annotation application. Benko
et al. [23] combined on surface and in-air gestures for content transfer between a 2D screen and
3D space. Mossel et al. [227] as well as Marzo et al. [216], combined touch input and handheld
device movement for 3D object manipulations in mobile AR. Polvi et al. [254] utilized touch and
the pose of a handheld touchscreen for remided object positioning in mobile AR. Grandi et al. [118],
studied the use of touch and the orientation of a smartphone for collaborative object manipulation
in VR. Surale et al. [311] explored the use of touch input on a spatially tracked tablet for object
manipulations in VR. In VR, Menzner et al. [220] utilized combined in-air and touch movements
on and above smartphones for efficient navigation of multiscale information spaces, see Figure
10. Several authors combined pen input both in mid-air as well as on touch surfaces to enhance
sketching in VR [88] and AR [5, 114, 115].

Also, the combination of eye-gaze with other modalities such as mid-air gestures and head-
movements has seen recent interest for interaction in AR and VR. For example, Pfeuffer et al. [248]
investigated the combination of gaze and gestures in VR, see Figure 11. They described Gaze +
Pinch, which integrates eye gaze to select 3D objects, and indirect freehand gestures to manipulate
those objects. They explored this technique for object selection, manipulation, scene navigation,
menu interaction, and image zooming. Similarly, Ryu et al. [276] introduced a combined grasp
eye-pointing technique for 3D object selection. Kyto et al. [190] combined head and eye gaze for
improving target selection in AR. Sidenmark and Gellersen [299, 300], studied different techniques
combining eye and head pointing in VR. Gesslein et al. [116] combined pen-based input with gaze
tracking for efficient interaction across multiple spreadsheets, see Figure 12. Biener et al. [29]
utilized gaze and touch interaction for navigating virtual multi-display environments, see Figure
13.

Mixed Reality Interaction Techniques

13

Fig. 11. Multimodal target selection in VR using a combination of gaze and gestures [248]. Image courtesy of
Ken Pfeuffer.

Fig. 12. Interacting with multiple sheets using a combination of pen-based and gaze-based interaction [116].
Initially, solely icons indicating the existent of additionally accessible sheets are visible (a). Neighboring sheets
are expanded and each sheet the user gazes at is highlighted with a red frame (b). The user taps with his
non-dominant hand on the tablet bezel, causing the selected sheet to slide towards the tablet (c), where the
user can edit it using the tablet’s touchscreen (d).

Fig. 13. Left: a virtual multi-display environment [29]. Right: Users can navigate across displays by gaze (left,
right, up, down) and touch (depth).

8 MULTI-DISPLAY INTERACTION
Traditionally, output of interactive systems is often limited to a single display. However, multi-
display environments from the desktop to gigapixel displays are also increasingly common for
knowledge work and complex tasks such as financial trading or factory management as well as for
social applications such as second screen TV experiences [125]. Surveys about multi-display systems
and distributed user interfaces have been presented by Elmqvist [96], Grubert et al. [124, 125, 257]
and Brudy et al. [47].

14

Grubert

Fig. 14. Mobile displays can be spatially registered using head-tracking for augmented planar surfaces (left)
or creating a fishtank VR display (right) [121].

Augmented Reality has the potential to enhance interaction with both small and large displays
by adding an unlimited virtual screen space or other complementing characteristics like mobility.
However, this typically comes at the cost of a lower display fidelity compared to a physical panel
display (such as lower resolution, lower contrast, or a smaller physical field of view in OST HMDs).
In 1991, Feiner et al. [104], proposed a hybrid display combining a traditional desktop monitor
with an OST HMD and explored a window manager application. Butz et al. [53], combined multiple
physical displays ranging from handheld to wall-sized with OST HMDs in a multi-user collaborative
environment. Baudisch et al. [19] used a projector to facilitate focus and context interaction on a
desktop computer. MacWilliams et al. [213] proposed a multi-user game in which players could
interact with a tabletop, laptop as well as handheld displays. Serrano et al. [294] propose to use an
OST HMD to facilitate content transfer between multiple physical displays on a desktop. Boring et
al. [39] used a smartphone to facilitate content transfer between multiple stationary displays. They
later extended the work to manipulate screen content on stationary displays [20] and interactive
facades [38] using smartphones. Raedle et al. [258] supported interaction across multiple mobile
displays through a top-mounted depth-camera. Grubert et al. [121, 123] used face tracking to allow
user interaction across multiple mobile devices, which could be dynamically re-positioned, see
Figure 14, left. They also proposed to utilize face tracking [121, 122] for creating a cubic VR display
with user perspective rendering, see Figure 14, right. Butscher et al. [52] explored the combination
of VST HMDs with a tabletop displays for information visualization. Reipschläger et al. [265, 266]
combined a high resolution horizontal desktop display with an OST HMD for design activities.
Gugenheimer et al. [137] introduced face touch, which allows interacting with display-fixed user
interfaces (using direct touch) and world-fixed content (using raycasting). This work was later
extended to utilize three touch displays around the user’s head [139], see Figure 15. Gugenheimer
et al. also introduced ShareVR [138], which enabled multi-user and multi-display interaction across
users inside and outside of VR, see Figure16.

A number of systems also concentrated on the combination of HMDs and handheld as well
body-worn displays such as smartwatches, smartphones and tablets in mobile contexts. Here,
typically the head-mounted display extends the field of view of the handheld display to provide
a larger virtual field of view. In MultiFi [119], an OST HMD provides contextual information
for higher resolution touch-enabled displays (smartwatch and smartphone), see Figure 17. The
authors explored different spatial reference systems such as body-aligned, device-aligned, and
side-by-side modes. Similar explorations have followed suit using video-see-through HMDs [237],
an extended set of interaction techniques [364], using smartwatches [209, 340, 345], or with a focus
on understanding smartphone-driven window management techniques for HMDs [271].

Mixed Reality Interaction Techniques

15

Fig. 15. Multiple Displays around a users head for enhanced interaction in VR [139]. Image courtesy of Jan
Gugenheimer.

Fig. 16. Left: A controller mounted dipslay to allow users outside of VR to interact with a user wearing an
immersive head-mounted display in ShareVR [138]. Right: The ShareVR environment resembling a living
room. Image courtesy of Jan Gugenheimer.

Purely virtual multi-display environments have also been explored in AR and VR. In 1993, Feiner
et al. [103] introduced head-surrounding and world reference frames for positioning 3D windows
in VR. In 1998, Billinghurst et al. [32] introduced the spatial display metaphor, in which information
windows are arranged on a virtual cylinder around the user. Since then, virtual information
displays have been explored in various reference systems, such as world-, object-, head-, body- or
device-referenced systems [192]. Specifically, interacting with windows in body-centered reference
systems [330] has attracted attention, for instance to allow fast access to virtual items [66, 202],
mobile multi-tasking [99, 101] and visual analytics [100]. Lee et al. [195] investigated positioning a
window in 3D space using a continuous hand gesture. Petford et al. [247] compared the selection
performance of mouse and raycast pointing in full coverage displays (not in VR). Jetter et al. [164]
proposed to interactively design a space with various display form factors in VR.

9 INTERACTION USING KEYBOARD AND MOUSE
Being the de facto standard for human-computer interaction in personal computing environ-
ments for decades, standard input peripherals such as keyboard and mouse, while initially used in

16

Grubert

Fig. 17. Top: the field of view of a smartphone is extended by an optical see-through display, in MultiFi [119].
Bottom: A smartwatch is used to select individual icons using touch.

projection-based CAVE environments, were soon replaced by special purpose input devices and
associated interaction techniques for AR and VR (see previous sections) . This was partly due to the
constraints of those input devices making them challenging to use for spatial input with six degrees
of freedom. Physical keyboards typically support solely symbolic input. Standard computer mice
are restricted to two-dimensional pointing (along with button clicks, and a scroll-wheel). However,
with modern knowledge workers still relying on the efficiency of those physical input devices,
researchers revisited how to use them within AR and VR.

With increasing interest in supporting knowledge work using AR and VR HMDs [130, 140, 204,

275], keyboard and mouse interaction drew attention by several researchers.

The keyboard was designed for rapid entrance of symbolic information, and although it may not
be the best mechanism developed for the task, its familiarity that enabled good performance by
users without considerable learning efforts kept it almost unchanged for many years. However,
when interacting with spatial data, they are perceived as falling short of providing efficient input
capabilities [28], even though they are successfully used in many 3D environments (such as CAD
or gaming [309]), can be modified to to allow 3D interaction [246, 335] or can outperform 3D input
devices in specific tasks such as 3D object placement [25, 310]. Also for 3D object manipulation in
AR and VR they found to be not significantly slower than a dedicated 3D input device [187].

In VR, a number of works investigated the costs of using physical keyboards for standard text
entry tasks, see Figure 18. Grubert et al. [135, 136], Knierim et al. [180] as well as McGill et al.
[217] found physical keyboards to be mostly usable for text entry in immersive head-mounted
display-based VR but varied in their observations about the performance loss when transferring
text entry from the physical to the virtual world. Pham et al. [249] deployed a physical keyboard on
a tray to facilitate mobile text entry. Apart from standard QWERTY keyboards a variety of further
text entry input devices and techniques have been proposed for VR, see [89].

Besides using unmodified physical keyboards, there have been several approaches in extending
the basic input capabilities of physical keyboard beyond individual button presses. Specifically,
input on, above and around the keyboard surface have been proposed using acoustic [171, 189],
pressure [86, 208, 357], proximity [316], capacitive sensors [35, 102, 142, 268, 298, 323], cameras
[175, 260, 344], body-worn orientation sensors [50] or even unmodified physical keyboards [193,

Mixed Reality Interaction Techniques

17

Fig. 18. Text entry in VR using standard physical keyboards using different hand reprensentations [135].
From left to right: no representation, inverse-kinematic model, finger tip representation using spheres, video
pass-through of the user’s hands.

359]. Besides sensing, actuation of keys has also been explored [9]. Embedding capacitive sensing
into keyboards has been studied by various researchers. It lends itself to detect finger events on
and slightly above keys and can be integrated into mass-manufacturing processes. Rekimoto et
al. [268] investigated capacitive sensing on a keypad, but not a full keyboard. Habib et al. [142]
and Tung et al. [323] proposed to use capacitive sensing embedded into a full physical keyboard
to allow touchpad operation on the keyboard surface. Tung et al. [323] developed a classifier
to automatically distinguish between text entry and touchpad mode on the keyboard. Shi et al.
developed microgestures on capacitive sensing keys [297, 298]. Similarly, Zheng et al. [362, 363]
explored various interaction mappings for finger and hand postures. Sekimoro et al. focused on
exploring gestural interactions on the space bar [293]. Extending the idea of LCD-programmable
keyboards [159], Block et al. extended the output capabilities of touch-sensitive, capacitive sensing
keyboard by using a top-mounted projector [35]. Several commercial products have also augmented
physical keyboards with additional, partly interactive, displays (e.g., Apple Touch Bar [2], Logitech
G19 [206], Razer Death-Stalker Ultimate [262]).

Maiti et al. [215] explored the use of randomized keyboard layouts on physical keyboards using
an OST display. Wang et al. [333] explored the use of an Augmented Reality extension to a desktop-
based analytics environment. Specifically, they added a stereoscopic data view using a HoloLens to
a traditional 2D desktop environment and interacted with keyboard and mouse across both the
HoloLens and the desktop.

Schneider et al. [290] explored the design space of using physical keyboards in VR beyond
text entry, see Figure 19. Specifically, they proposed three different input mappings: 1 key to
1 action (standard mode of interaction using keyboards), multiple keys to a single action (e.g.,
mapping a large virtual button to several physical buttons), as well as mapping a physical key to
a coordinate in a two-dimensional input space. Similarly, they proposed three different output
mappings: augmenting individual keys (e.g., showing an emoji on a key), augmenting on and
around the keyboard (e.g., adding additional user interface elements on top of the keyboard such as
virtual sliders) as well as transforming the keyboard geometry itself (e.g., only displaying single
buttons, or replacing the keyboard by other visuals). Those ideas were later also considered in the
domain of immersive analytics [129].

Mouse-based pointing has been studied in depth outside of AR and VR for pointing on single
monitors [56] as well as multi-display environments [7, 18, 21]. However, it has been found that
standard 2D mice do not adapt well to multi-display interaction [331], an issue which is also relevant
for AR and VR. Consequently, standard mice have been modified in various ways to add additional
degrees of freedom. For example, Villar et al. [328] explored multiple form factors for multi-touch
enabled mice. Other researchers have added additional mouse sensors to support yawing [212, 244],
pressure sensors for discrete selection [59, 177] to allow for three instead of two degrees of freedom.

18

Grubert

Fig. 19. Input-output dimensions of reconfiguring physical keyboards in VR with mapped example applica-
tions, [290]. The 𝑥-axis shows input mappings and the 𝑦-axis shows output mappings. FL: Foreign Languages;
EM: Emojis; SC: Special Characters; PW: Secure Password Entry; BS: Browser Shortcuts; WM = Word Macros.

Three-dimensional interaction was enabled using the Rockin’Mouse [10] and the VideoMouse [154].
Both works added a dome below the device to facilitate 3D interaction. Steed and Slater [307]
proposed to add a dome on top of the mouse rather than below. Further form factors have also
been proposed to facilitate pointing based interaction in 3D [111, 112]. Recently, researchers also
worked on unifying efficient input both in 2D and 3D [246, 278].

Standard mice using a scroll wheel can also be efficiently used for 3D object selection when
being combined with gaze-tracking in virtual multi-display environments [29]. For example, in
the Windows Mixed Reality Toolkit [221], the x and y- movement of the mouse can be mapped to
the x and y movements on a proxy shape such as a cylinder (or any object on that cylinder, like a
window). The scroll wheel is used for changing the pointer depth (in discrete steps). The x- and y-
movements can be limited to the current field of view of the user to allow for acceptable control to
display ratios. The user gaze can then be used to change the view on different regions of the proxy
shape.

10 VIRTUAL AGENTS
Virtual agents can be considered as "intelligent" software programs performing tasks on behalf
of users’ questions or commands. While it can be argued, what "intelligent" really means in this
context, a widely accepted characteristic of this "intelligence" is context-aware behaviour [126, 329].
This allows an agent to interact with the user and environment through sensing and acting in
an independent and dynamic way. The behaviour is typically well-defined and allows to trigger
actions based on a set of conditions [240]. The rise of voice assistants (or conversational agents)
[157], which interact with users through natural language, has brought media attention and a
prevalence in various areas such as home automation, in-car operation, automation of call centers,
education and training [239].

Mixed Reality Interaction Techniques

19

Fig. 20. An intelligent virtual agent in Virtual Reality [287]. Image courtesy of Gerd Bruder.

In AR and VR, virtual agents often use more than a single modality for input and output.
Complementary to voice in- an output, virtual agents in AR and VR can typically react to body
gestures or postures or even facial expressions of the users. Due to their graphical representations
those agents are embodied in the virtual world. The level of embodiment of a virtual agent has been
studied extensively [84, 355]. For example it has been shown, that the effect of adding a face was
larger than effect of visual realism (both photo-realism and behavioural realism of the avatar) [355].
In VR, the level of visual realism of the virtual agent is typically matched to the visual realism of
the environment, see Figure 20. In contrast, in AR, there is often a noticeable difference between
the agent representation and the physical scene and those effects are still underexplored [176], see
Figure 21. Hantono et al. review the use of virtual agents in AR in educational settings. Norouzi et
al. provide review of the convergence between AR and virtual agents [238].

Specifically for AR, Maes et al. [214] introduced a magic mirror AR system, in which humans
could interact with a dog through both voice and gestures. Similarly, Cavazza et al. [58] allowed
participants to interact with virtual agents in an interactive storytelling environment. MacIntyre
et al. [210] used pre-recorded videos of phyiscal actors to let users interact with them using
OST HMDs. Anabuki et al. [1] highlight that having virtual agents and users share the same
physical environment is the most distinguishing aspect of virtual agents in AR. They introduced
Welbo, an animated virtual agent, which is is aware of its physical environment and can avoid
standing in the user’s way. Barakony et al. [12] presented AR Puppet, a system that explored the
context-aware animated agents within AR. The authors investigated aspects like visualization,
appearance or behaviors. They also studied AR-specific aspects such as the ability of the agent to
avoid physical obstacles or its ability to interact with physical objects. Based on this initial research,
the authors explored various applications [13, 14]. Chekhlov et al. [62] presented a system based
on Simultanteous Localization and Mapping (SLAM) [93], in which a virtual agent had to move in
a physical environment. Blum et al. [36] introduced an outdoor AR game which included virtual
agents. Kotranza et al. [184, 185] used a tangible physical representation of a human that could be
touched, along with a virtual visual representation in a medical education context. They called this
dual representaiton mixed reality humans and argued that affording touch between a human and a
virtual agent enables interpersonal scenarios.

20

Grubert

Fig. 21. An intelligent virtual agent in Augmented Reality [176]. Top: The user requests to turn of the light.
Bottom: the agent walks towards the physical light and turns the wirelessly connected light bulb off. Image
courtesy of Gerd Bruder.

11 SUMMARY AND OUTLOOK
This chapter served as an overview of a wide variety of interaction techniques MR - covering
both device- and prop-based input such as tangible interaction, pen and keyboard input as well as
utilizing human effector-based input such as spatial gestures, gaze or speech.

The historical development of the presented techniques was closely coupled to the available
sensing capabilities. For example, in order to recognize props such as paddles [170], they had to be
large enough in order to let fiducials be recognized by low-resolution cameras. With the advance-
ment of computer vision-based sensing, fiducials could be come smaller, change their appearance
to natural looking images or be omitted altogether (e.g., for hand and finger tracking). Further,
the combination of more than one modality became possible through increasing computational
capabilities of MR systems.

In the future, we expect an ongoing trend of both minimizing the size and price of sensors, as well
as the ubiquitous availability of those sensors, in dedicated computing devices, in everyday objects
[337], on [284] or even in the human body itself [292]. Hence, MR interaction techniques will play
a central role on shaping the future of both pervasive computing [126] as well as augmenting
humans with (potentially) super human capabilities (e.g., motor capabilities [172, 188], cognitive
and perceptual capabilities [285]). Besides technological and interaction challenges along the way,
the field of MR interaction will greatly benefit from including both social and ethical implications
when designing future interfaces.

Mixed Reality Interaction Techniques

21

REFERENCES

[1] Mahoro Anabuki, Hiroyuki Kakuta, Hiroyuki Yamamoto, and Hideyuki Tamura. 2000. Welbo: An embodied con-
versational agent living in mixed reality space. In CHI’00 extended abstracts on Human factors in computing systems.
10–11.

[2] Apple. [n.d.]. Apple Touch Bar. https://developer.apple.com/macos/touch-bar/. Last accessed 27.11.2018.
[3] Bruno Araujo, Ricardo Jota, Varun Perumal, Jia Xian Yao, Karan Singh, and Daniel Wigdor. 2016. Snake Charmer:
Physically enabling virtual objects. In Proceedings of the TEI’16: Tenth International Conference on Tangible, Embedded,
and Embodied Interaction. 218–226.

[4] Ferran Argelaguet and Carlos Andujar. 2013. A survey of 3D object selection techniques for virtual environments.

Computers & Graphics 37, 3 (2013), 121–136.

[5] Rahul Arora, Rubaiat Habib Kazi, Tovi Grossman, George Fitzmaurice, and Karan Singh. 2018. Symbiosissketch:
Combining 2d & 3d sketching for designing detailed 3d objects in situ. In Proceedings of the 2018 CHI Conference on
Human Factors in Computing Systems. ACM, 185.

[6] Rahul Arora, Rubaiat Habib Kazi, Fraser Anderson, Tovi Grossman, Karan Singh, and George W Fitzmaurice. 2017.

Experimental Evaluation of Sketching on Surfaces in VR.. In CHI, Vol. 17. 5643–5654.

[7] Mark Ashdown, Kenji Oka, and Yoichi Sato. 2005. Combining head tracking and mouse input for a GUI on multiple

monitors. In CHI’05 extended abstracts on Human factors in computing systems. 1188–1191.

[8] Mahdi Azmandian, Mark Hancock, Hrvoje Benko, Eyal Ofek, and Andrew D Wilson. 2016. Haptic retargeting:
Dynamic repurposing of passive haptics for enhanced virtual reality experiences. In Proceedings of the 2016 chi
conference on human factors in computing systems. 1968–1979.

[9] Gilles Bailly, Thomas Pietrzak, Jonathan Deber, and Daniel J Wigdor. 2013. Métamorphe: augmenting hotkey usage
with actuated keys. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. ACM, 563–572.
[10] Ravin Balakrishnan, Thomas Baudel, Gordon Kurtenbach, and George Fitzmaurice. 1997. The Rockin’Mouse: integral
3D manipulation on a plane. In Proceedings of the ACM SIGCHI Conference on Human factors in computing systems.
311–318.

[11] Yuki Ban, Takuji Narumi, Tatsuya Fujii, Sho Sakurai, Jun Imura, Tomohiro Tanikawa, and Michitaka Hirose. 2013.
Augmented endurance: controlling fatigue while handling objects by affecting weight perception using augmented
reality. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. 69–78.

[12] Istvan Barakonyi, Thomas Psik, and Dieter Schmalstieg. 2004. Agents that talk and hit back: Animated agents in
augmented reality. In Third IEEE and ACM International Symposium on Mixed and Augmented Reality. IEEE, 141–150.
[13] István Barakonyi and Dieter Schmalstieg. 2005. Augmented reality agents in the development pipeline of computer

entertainment. In International Conference on Entertainment Computing. Springer, 345–356.

[14] Istvan Barakonyi and Dieter Schmalstieg. 2006. Ubiquitous animated agents for augmented reality. In 2006 IEEE/ACM

International Symposium on Mixed and Augmented Reality. IEEE, 145–154.

[15] Domagoj Baričević, Cha Lee, Matthew Turk, Tobias Höllerer, and Doug A Bowman. 2012. A hand-held AR magic lens
with user-perspective rendering. In 2012 IEEE International Symposium on Mixed and Augmented Reality (ISMAR).
IEEE, 197–206.

[16] Emad Barsoum. 2016. Articulated hand pose estimation review. arXiv preprint arXiv:1604.06195 (2016).
[17] Anil Ufuk Batmaz, Aunnoy K Mutasim, and Wolfgang Stuerzlinger. [n.d.]. Precision vs. Power Grip: A Comparison of

Pen Grip Styles for Selection in Virtual Reality. ([n. d.]).

[18] Patrick Baudisch, Edward Cutrell, Ken Hinckley, and Robert Gruen. 2004. Mouse ether: accelerating the acquisition of
targets across multi-monitor displays. In CHI’04 extended abstracts on Human factors in computing systems. 1379–1382.
[19] Patrick Baudisch, Nathaniel Good, and Paul Stewart. 2001. Focus plus context screens: combining display technology
with visualization techniques. In Proceedings of the 14th annual ACM symposium on User interface software and
technology. 31–40.

[20] Dominikus Baur, Sebastian Boring, and Steven Feiner. 2012. Virtual projection: exploring optical projection as a
metaphor for multi-device interaction. In Proceedings of the SIGCHI Conference on Human Factors in Computing
Systems. 1693–1702.

[21] Hrvoje Benko and Steven Feiner. 2005. Multi-monitor mouse. In CHI’05 extended abstracts on Human factors in

computing systems. 1208–1211.

[22] Hrvoje Benko, Christian Holz, Mike Sinclair, and Eyal Ofek. 2016. Normaltouch and texturetouch: High-fidelity 3d
haptic shape rendering on handheld virtual reality controllers. In Proceedings of the 29th Annual Symposium on User
Interface Software and Technology. 717–728.

[23] Hrvoje Benko, Edward W Ishak, and Steven Feiner. 2005. Cross-dimensional gestural interaction techniques for

hybrid immersive environments. In IEEE Proceedings. VR 2005. Virtual Reality, 2005. IE EE, 209–216.

[24] Hrvoje Benko, Ricardo Jota, and Andrew Wilson. 2012. MirageTable: freehand interaction on a projected augmented

reality tabletop. In Proceedings of the SIGCHI conference on human factors in computing systems. 199–208.

22

Grubert

[25] François Bérard, Jessica Ip, Mitchel Benovoy, Dalia El-Shimy, Jeffrey R Blum, and Jeremy R Cooperstock. 2009. Did
“Minority Report” get it wrong? Superiority of the mouse over 3D input devices in a 3D placement task. In IFIP
Conference on Human-Computer Interaction. Springer, 400–414.

[26] Carlos Bermejo and Pan Hui. 2017. A survey on haptic technologies for mobile augmented reality. arXiv preprint

arXiv:1709.00698 (2017).

[27] Florent Berthaut. 2019. 3D interaction techniques for musical expression. Journal of New Music Research (2019), 1–13.
[28] Lonni Besançon, Paul Issartel, Mehdi Ammi, and Tobias Isenberg. 2017. Mouse, tactile, and tangible input for 3D
manipulation. In Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems. 4727–4740.
[29] Verena Biener, Daniel Schneider, Travis Gesslein, A. Otte, B. Kuth, Per Ola Kristensson, Eyal Ofek, Michel Pahud,
and Jens Grubert. 2020. Breaking the Screen: Interaction Across Touchscreen Boundaries in Virtual Reality for
Mobile Knowledge Workers. IEEE Transactions on Visualization and Computer Graphics 01 (oct 2020), 1–1. https:
//doi.org/10.1109/TVCG.2020.3023567

[30] Eric A Bier, Maureen C Stone, Ken Pier, William Buxton, and Tony D DeRose. 1993. Toolglass and magic lenses: the
see-through interface. In Proceedings of the 20th annual conference on Computer graphics and interactive techniques.
ACM, 73–80.

[31] Mark Billinghurst, Sisinio Baldis, Lydia Matheson, and Mark Philips. 1997. 3D palette: a virtual reality content creation

tool. In Proceedings of the ACM symposium on Virtual reality software and technology. 155–156.

[32] Mark Billinghurst, Jerry Bowskill, Mark Jessop, and Jason Morphett. 1998. A wearable spatial conferencing space. In

Digest of Papers. Second International Symposium on Wearable Computers (Cat. No. 98EX215). IEEE, 76–83.

[33] Mark Billinghurst, Raphael Grasset, and Julian Looser. 2005. Designing augmented reality interfaces. ACM Siggraph

Computer Graphics 39, 1 (2005), 17–22.

[34] Oliver Bimber and Ramesh Raskar. 2005. Spatial augmented reality: merging real and virtual worlds. CRC press.
[35] Florian Block, Hans Gellersen, and Nicolas Villar. 2010. Touch-display keyboards: transforming keyboards into
interactive surfaces. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. ACM, 1145–1154.
[36] Lisa Blum, Richard Wetzel, Rod McCall, Leif Oppermann, and Wolfgang Broll. 2012. The final TimeWarp: using form
and content to support player experience and presence when designing location-aware mobile augmented reality
games. In Proceedings of the designing interactive systems conference. 711–720.

[37] Richard A Bolt. 1980. “Put-that-there” Voice and gesture at the graphics interface. In Proceedings of the 7th annual

conference on Computer graphics and interactive techniques. 262–270.

[38] Sebastian Boring and Dominikus Baur. 2012. Making public displays interactive everywhere. IEEE computer graphics

and applications 33, 2 (2012), 28–36.

[39] Sebastian Boring, Dominikus Baur, Andreas Butz, Sean Gustafson, and Patrick Baudisch. 2010. Touch projector:
mobile interaction through video. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems.
2287–2296.

[40] Sebastian Boring, Sven Gehring, Alexander Wiethoff, Anna Magdalena Blöckner, Johannes Schöning, and Andreas
Butz. 2011. Multi-user interaction on media facades through live video on mobile devices. In Proceedings of the SIGCHI
Conference on Human Factors in Computing Systems. 2721–2724.

[41] Doug Bowman, Ernst Kruijff, Joseph J LaViola Jr, and Ivan P Poupyrev. 2004. 3D User interfaces: theory and practice,

CourseSmart eTextbook. Addison-Wesley.

[42] Doug A Bowman and Chadwick A Wingrave. 2001. Design and evaluation of menu systems for immersive virtual

environments. In Proceedings IEEE Virtual Reality 2001. IEEE, 149–156.

[43] Alan Bränzel, Christian Holz, Daniel Hoffmann, Dominik Schmidt, Marius Knaust, Patrick Lühne, René Meusel,
Stephan Richter, and Patrick Baudisch. 2013. GravitySpace: tracking users and their poses in a smart room using a
pressure-sensing floor. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. 725–734.

[44] Leonard D Brown and Hong Hua. 2006. Magic lenses for augmented virtual environments. IEEE Computer Graphics

and Applications 26, 4 (2006), 64–73.

[45] Leonard D Brown, Hong Hua, and Chunyu Gao. 2003. A widget framework for augmented interaction in SCAPE. In

Proceedings of the 16th annual ACM symposium on User interface software and technology. 1–10.

[46] Thomas Brown and Richard C Thomas. 2000. Finger tracking for the digital desk. In Proceedings First Australasian

User Interface Conference. AUIC 2000 (Cat. No. PR00515). IEEE, 11–16.

[47] Frederik Brudy, Christian Holz, Roman Rädle, Chi-Jui Wu, Steven Houben, Clemens Nylandsted Klokmose, and
Nicolai Marquardt. 2019. Cross-device taxonomy: survey, opportunities and challenges of interactions spanning
across multiple devices. In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems. 1–28.

[48] Grigore Burdea, Paul Richard, and Philippe Coiffet. 1996. Multimodal virtual reality: Input-output devices, system

integration, and human factors. International Journal of Human-Computer Interaction 8, 1 (1996), 5–24.

[49] Alisa Burova, John Mäkelä, Jaakko Hakulinen, Tuuli Keskinen, Hanna Heinonen, Sanni Siltanen, and Markku Turunen.
2020. Utilizing VR and Gaze Tracking to Develop AR Solutions for Industrial Maintenance. In Proceedings of the 2020

Mixed Reality Interaction Techniques

23

CHI Conference on Human Factors in Computing Systems. 1–13.

[50] Daniel Buschek, Bianka Roppelt, and Florian Alt. 2018. Extending Keyboard Shortcuts with Arm and Wrist Rotation

Gestures. In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems. ACM, 21.

[51] Wolfgang Büschel, Jian Chen, Raimund Dachselt, Steven Drucker, Tim Dwyer, Carsten Görg, Tobias Isenberg, Andreas
Kerren, Chris North, and Wolfgang Stuerzlinger. 2018. Interaction for immersive analytics. In Immersive Analytics.
Springer, 95–138.

[52] Simon Butscher, Sebastian Hubenschmid, Jens Müller, Johannes Fuchs, and Harald Reiterer. 2018. Clusters, trends, and
outliers: How immersive technologies can facilitate the collaborative analysis of multidimensional data. In Proceedings
of the 2018 CHI Conference on Human Factors in Computing Systems. 1–12.

[53] Andreas Butz, Tobias Hollerer, Steven Feiner, Blair MacIntyre, and Clifford Beshers. 1999. Enveloping users and
computers in a collaborative 3D augmented reality. In Proceedings 2nd IEEE and ACM International Workshop on
Augmented Reality (IWAR’99). IEEE, 35–44.

[54] Polona Caserman, Augusto Garcia-Agundez, and Stefan Goebel. 2019. A Survey of Full-Body Motion Reconstruction

in Immersive Virtual Reality Applications. IEEE transactions on visualization and computer graphics (2019).

[55] Polona Caserman, Augusto Garcia-Agundez, Robert Konrad, Stefan Göbel, and Ralf Steinmetz. 2019. Real-time body

tracking in virtual reality using a Vive tracker. Virtual Reality 23, 2 (2019), 155–168.

[56] Géry Casiez, Daniel Vogel, Ravin Balakrishnan, and Andy Cockburn. 2008. The impact of control-display gain on

user performance in pointing tasks. Human–computer interaction 23, 3 (2008), 215–250.

[57] John P Cater. 1994. Smell/taste: odors in reality. In Proceedings of IEEE International Conference on Systems, Man and

Cybernetics, Vol. 2. IEEE, 1781–vol.

[58] Marc Cavazza, Olivier Martin, Fred Charles, Steven J Mead, and Xavier Marichal. 2003. Interacting with virtual agents
in mixed reality interactive storytelling. In International Workshop on Intelligent Virtual Agents. Springer, 231–235.
[59] Jared Cechanowicz, Pourang Irani, and Sriram Subramanian. 2007. Augmenting the mouse with pressure sensitive

input. In Proceedings of the SIGCHI conference on Human factors in computing systems. 1385–1394.

[60] Young-Woon Cha, True Price, Zhen Wei, Xinran Lu, Nicholas Rewkowski, Rohan Chabra, Zihe Qin, Hyounghun Kim,
Zhaoqi Su, Yebin Liu, et al. 2018. Towards fully mobile 3D face, body, and environment capture using only head-worn
cameras. IEEE transactions on visualization and computer graphics 24, 11 (2018), 2993–3004.

[61] Leith KY Chan and Henry YK Lau. 2012. MagicPad: the projection based 3D user interface. International Journal on

Interactive Design and Manufacturing (IJIDeM) 6, 2 (2012), 75–81.

[62] Denis Chekhlov, Andrew P Gee, Andrew Calway, and Walterio Mayol-Cuevas. 2007. Ninja on a plane: Automatic
discovery of physical planes for augmented reality using visual slam. In 2007 6th IEEE and ACM International
Symposium on Mixed and Augmented Reality. IEEE, 153–156.

[63] Lulu Chen, Hong Wei, and James Ferryman. 2013. A survey of human motion analysis using depth imagery. Pattern

Recognition Letters 34, 15 (2013), 1995–2006.

[64] Shu-Yu Chen, Lin Gao, Yu-Kun Lai, Paul L Rosin, and Shihong Xia. 2018. Real-time 3d face reconstruction and gaze
tracking for virtual reality. In 2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR). IEEE, 525–526.
[65] Xiang’Anthony’ Chen, Tovi Grossman, Daniel J Wigdor, and George Fitzmaurice. 2014. Duet: exploring joint

interactions on a smart phone and a smart watch. In Proc. CHI ’14. ACM, 159–168.

[66] Xiang Anthony Chen, Nicolai Marquardt, Anthony Tang, Sebastian Boring, and Saul Greenberg. 2012. Extending a
mobile devices interaction space through body-centric interaction. In Proceedings of the 14th international conference
on Human-computer interaction with mobile devices and services. ACM, 151–160.

[67] Yucheng Chen, Yingli Tian, and Mingyi He. 2020. Monocular human pose estimation: A survey of deep learning-based

methods. Computer Vision and Image Understanding (2020), 102897.

[68] Lung-Pan Cheng, Patrick Lühne, Pedro Lopes, Christoph Sterz, and Patrick Baudisch. 2014. Haptic turk: a motion
platform based on people. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. 3463–3472.
[69] Lung-Pan Cheng, Eyal Ofek, Christian Holz, Hrvoje Benko, and Andrew D Wilson. 2017. Sparse haptic proxy: Touch
feedback in virtual environments using a general passive prop. In Proceedings of the 2017 CHI Conference on Human
Factors in Computing Systems. 3718–3728.

[70] Lung-Pan Cheng, Thijs Roumen, Hannes Rantzsch, Sven Köhler, Patrick Schmidt, Robert Kovacs, Johannes Jasper,
Jonas Kemper, and Patrick Baudisch. 2015. Turkdeck: Physical virtual reality based on people. In Proceedings of the
28th Annual ACM Symposium on User Interface Software & Technology. 417–426.

[71] Jan Ciger, Mario Gutierrez, Frederic Vexo, and Daniel Thalmann. 2003. The magic wand. In Proceedings of the 19th

spring conference on Computer graphics. 119–124.

[72] Philip Cohen, David McGee, Sharon Oviatt, Lizhong Wu, Josh Clow, Rob King, Simon Julier, and Lawrence Rosenblum.
1999. Multimodal interaction for 2D and 3D environments [virtual reality]. IEEE Computer Graphics and Applications
19, 4 (1999), 10–13.

24

Grubert

[73] Timothy R Coles, Dwight Meglan, and Nigel W John. 2010. The role of haptics in medical training simulators: a

survey of the state of the art. IEEE Transactions on haptics 4, 1 (2010), 51–66.

[74] Klen Čopič Pucihar, Paul Coulton, and Jason Alexander. 2013. Evaluating dual-view perceptual issues in handheld
augmented reality: device vs. user perspective rendering. In Proceedings of the 15th ACM on International conference
on multimodal interaction. 381–388.

[75] Klen Čopič Pucihar, Paul Coulton, and Jason Alexander. 2014. The use of surrounding visual context in handheld
AR: device vs. user perspective rendering. In Proceedings of the SIGCHI Conference on Human Factors in Computing
Systems. 197–206.

[76] Christian Corsten, Ignacio Avellino, Max Möllers, and Jan Borchers. 2013.

Instant user interfaces: repurposing
everyday objects as input devices. In Proceedings of the 2013 ACM international conference on Interactive tabletops and
surfaces. 71–80.

[77] Nathan Cournia, John D Smith, and Andrew T Duchowski. 2003. Gaze-vs. hand-based pointing in virtual environments.

In CHI’03 extended abstracts on Human factors in computing systems. 772–773.

[78] James Crowley, François Berard, Joelle Coutaz, et al. 1995. Finger tracking as an input device for augmented reality.

In International Workshop on Gesture and Face Recognition. 195–200.

[79] Raimund Dachselt and Anett Hübner. 2007. Three-dimensional menus: A survey and taxonomy. Computers & Graphics

31, 1 (2007), 53–65.

[80] Nguyen-Thong Dang. 2007. A survey and classification of 3D pointing techniques. In 2007 IEEE international conference

on research, innovation and vision for the future. IEEE, 71–80.

[81] Qi Dang, Jianqin Yin, Bin Wang, and Wenqing Zheng. 2019. Deep learning based 2d human pose estimation: A survey.

Tsinghua Science and Technology 24, 6 (2019), 663–676.

[82] Alexander De Luca, Emanuel von Zezschwitz, Ngo Dieu Huong Nguyen, Max-Emanuel Maurer, Elisa Rubegni,
Marcello Paolo Scipioni, and Marc Langheinrich. 2013. Back-of-device Authentication on Smartphones. In Proc. CHI
’13 (Paris, France). ACM, 2389–2398. https://doi.org/10.1145/2470654.2481330

[83] Michael F Deering. 1995. HoloSketch: a virtual reality sketching/animation tool. ACM Transactions on Computer-

Human Interaction (TOCHI) 2, 3 (1995), 220–238.

[84] Doris M Dehn and Susanne Van Mulken. 2000. The impact of animated interface agents: a review of empirical

research. International journal of human-computer studies 52, 1 (2000), 1–22.

[85] Shujie Deng, Jian Chang, and Jian J Zhang. 2013. A survey of haptics in serious gaming. In International Conference

on Games and Learning Alliance. Springer, 130–144.

[86] Paul H Dietz, Benjamin Eidelson, Jonathan Westhues, and Steven Bathiche. 2009. A practical pressure sensitive
computer keyboard. In Proceedings of the 22nd annual ACM symposium on User interface software and technology.
ACM, 55–58.

[87] Klaus Dorfmuller-Ulhaas and Dieter Schmalstieg. 2001. Finger tracking for interaction in augmented environments.

In Proceedings IEEE and ACM international symposium on augmented reality. IEEE, 55–64.

[88] Tobias Drey, Jan Gugenheimer, Julian Karlbauer, Maximilian Milo, and Enrico Rukzio. 2020. VRSketchIn: Exploring
the Design Space of Pen and Tablet Interaction for 3D Sketching in Virtual Reality. In Proceedings of the 2020 CHI
Conference on Human Factors in Computing Systems. ACM.

[89] Tafadzwa Joseph Dube and Ahmed Sabbir Arif. 2019. Text entry in virtual reality: A comprehensive review of the

literature. In International Conference on Human-Computer Interaction. Springer, 419–437.

[90] Gershon Dublon and Joseph A Paradiso. 2014. Extra sensory perception. Scientific american 311, 1 (2014), 36–41.
[91] Andrew T Duchowski. 2018. Gaze-based interaction: A 30 year retrospective. Computers & Graphics 73 (2018), 59–69.
[92] Andrew T Duchowski, Eric Medlin, Anand Gramopadhye, Brian Melloy, and Santosh Nair. 2001. Binocular eye
tracking in VR for visual inspection training. In Proceedings of the ACM symposium on Virtual reality software and
technology. 1–8.

[93] Hugh Durrant-Whyte and Tim Bailey. 2006. Simultaneous localization and mapping: part I. IEEE robotics & automation

magazine 13, 2 (2006), 99–110.

[94] Anna Eiberger, Per Ola Kristensson, Susanne Mayr, Matthias Kranz, and Jens Grubert. 2019. Effects of Depth Layer
Switching between an Optical See-Through Head-Mounted Display and a Body-Proximate Display. In Symposium on
Spatial User Interaction. 1–9.

[95] Mohamed Elgharib, Mallikarjun BR, Ayush Tewari, Hyeongwoo Kim, Wentao Liu, Hans-Peter Seidel, and Christian
Theobalt. 2019. EgoFace: Egocentric Face Performance Capture and Videorealistic Reenactment. arXiv preprint
arXiv:1905.10822 (2019).

[96] Niklas Elmqvist. 2011. Distributed user interfaces: State of the art. In Distributed User Interfaces. Springer, 1–12.
[97] LM Encarnaˆcão, Oliver Bimber, Dieter Schmalstieg, and SD Chandler. 1999. A Translucent Sketchpad for the Virtual
Table Exploring Motion-based Gesture Recognition. In Computer Graphics Forum, Vol. 18. Wiley Online Library,
277–286.

Mixed Reality Interaction Techniques

25

[98] Barret Ens, Benjamin Bach, Maxime Cordeil, Engelke Ulrich, Marcos Serrano, Wesley Willet, Arnaud Prouzeau,
Christoph Anthes, Wolfgang Büschel, Cody Dunne, Tim Dwyer, Jens Grubert, Haga Jason, Nurit Kirhsenbaum, Dylan
Kobayashi, Tica Lin, Monsurat Olaosebikan, Fabian Pointecker, David Saffo, Nazmus Saquib, Dieter Schmalstieg,
Danielle Albers Szafir, Matt Whitlock, and Yalong Yang. 2021. Grand Challenges in Immersive Analytics. In Proceedings
of the 2021 CHI Conference on Human Factors in Computing Systems.

[99] Barrett Ens, Juan David Hincapié-Ramos, and Pourang Irani. 2014. Ethereal planes: a design framework for 2D
information space in 3D mixed reality environments. In Proceedings of the 2nd ACM symposium on Spatial user
interaction. ACM, 2–12.

[100] Barrett Ens and Pourang Irani. 2016. Spatial analytic interfaces: Spatial user interfaces for in situ visual analytics.

IEEE computer graphics and applications 37, 2 (2016), 66–79.

[101] Barrett M Ens, Rory Finnegan, and Pourang P Irani. 2014. The personal cockpit: a spatial interface for effective task
switching on head-worn displays. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems.
ACM, 3171–3180.

[102] Wolfgang Fallot-Burghardt, Morten Fjeld, C Speirs, S Ziegenspeck, Helmut Krueger, and Thomas Läubli. 2006.
Touch&Type: a novel pointing device for notebook computers. In Proceedings of the 4th Nordic conference on Human-
computer interaction: changing roles. ACM, 465–468.

[103] Steven Feiner, Blair MacIntyre, Marcus Haupt, and Eliot Solomon. 1993. Windows on the world: 2 D windows for 3 D

augmented reality. In ACM Symposium on User Interface Software and Technology. 145–155.

[104] Steven Feiner and Ari Shamash. 1991. Hybrid user interfaces: Breeding virtually bigger interfaces for physically
smaller computers. In Proceedings of the 4th annual ACM symposium on User interface software and technology. 9–17.
[105] Andreas Rene Fender, Hrvoje Benko, and Andy Wilson. 2017. Meetalive: Room-scale omni-directional display system
for multi-user content and control sharing. In Proceedings of the 2017 ACM international conference on interactive
surfaces and spaces. 106–115.

[106] Michele Fiorentino, Antonio E Uva, and Giuseppe Monno. 2005. The Senstylus: a novel rumble-feedback pen device

for CAD application in Virtual Reality. (2005).

[107] Patrick Tobias Fischer and Eva Hornecker. 2012. Urban HCI: spatial aspects in the design of shared encounters for

media facades. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. 307–316.

[108] Morten Fjeld and Benedikt M Voegtli. 2002. Augmented chemistry: An interactive educational workbench. In

Proceedings. International Symposium on Mixed and Augmented Reality. IEEE, 259–321.

[109] Sean Follmer, Daniel Leithinger, Alex Olwal, Akimitsu Hogge, and Hiroshi Ishii. 2013. inFORM: dynamic physical

affordances and constraints through shape and object actuation.. In Uist, Vol. 13. 2501988–2502032.

[110] Euan Freeman, Stephen Brewster, and Vuokko Lantz. 2014. Towards Usable and Acceptable Above-device Interactions.

In Proc. MobileHCI ’14 (Toronto, ON, Canada). ACM, 459–464. https://doi.org/10.1145/2628363.2634215

[111] Bernd Froehlich, Jan Hochstrate, Verena Skuk, and Anke Huckauf. 2006. The globefish and the globemouse: two
new six degree of freedom input devices for graphics applications. In Proceedings of the SIGCHI conference on Human
Factors in computing systems. 191–199.

[112] Bernd Fröhlich and John Plate. 2000. The cubic mouse: a new device for three-dimensional input. In Proceedings of

the SIGCHI conference on Human Factors in Computing Systems. 526–531.

[113] Alberto Gallace, Mary K Ngo, John Sulaitis, and Charles Spence. 2012. Multisensory presence in virtual reality:
possibilities & limitations. In Multiple sensorial media advances and applications: New developments in MulSeMedia.
IGI Global, 1–38.

[114] Danilo Gasques, Janet G Johnson, Tommy Sharkey, and Nadir Weibel. 2019. PintAR: Sketching Spatial Experiences in
Augmented Reality. In Companion Publication of the 2019 on Designing Interactive Systems Conference 2019 Companion.
ACM, 17–20.

[115] Danilo Gasques, Janet G Johnson, Tommy Sharkey, and Nadir Weibel. 2019. What you sketch is what you get: Quick
and easy augmented reality prototyping with pintar. In Extended Abstracts of the 2019 CHI Conference on Human
Factors in Computing Systems. 1–6.

[116] Travis Gesslein, Verena Biener, Phillip Gagel, Daniel Schneider, Eyal Ofek, Michel Pahud, Per Ola Kristensson, and
Jens Grubert. 2020. Pen-based Interaction with Spreadsheets in Mobile Virtual Reality. In 2020 IEEE International
Symposium on Mixed and Augmented Reality (ISMAR). IEEE.

[117] Wenjuan Gong, Xuena Zhang, Jordi Gonzàlez, Andrews Sobral, Thierry Bouwmans, Changhe Tu, and El-hadi Zahzah.
2016. Human pose estimation from monocular images: A comprehensive survey. Sensors 16, 12 (2016), 1966.
[118] Jerônimo Gustavo Grandi, Henrique Galvan Debarba, Luciana Nedel, and Anderson Maciel. 2017. Design and
evaluation of a handheld-based 3D user interface for collaborative object manipulation. In Proceedings of the 2017 CHI
Conference on Human Factors in Computing Systems. ACM, 5881–5891.

[119] Jens Grubert, Matthias Heinisch, Aaron Quigley, and Dieter Schmalstieg. 2015. Multifi: Multi fidelity interaction with
displays on and around the body. In Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing

26

Systems. 3933–3942.

Grubert

[120] Jens Grubert, Yuta Itoh, Kenneth Moser, and J Edward Swan. 2017. A survey of calibration methods for optical
see-through head-mounted displays. IEEE transactions on visualization and computer graphics 24, 9 (2017), 2649–2662.
[121] Jens Grubert and Matthias Kranz. 2017. Headphones: Ad hoc mobile multi-display environments through head

tracking. In Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems. 3966–3971.

[122] Jens Grubert and Matthias Kranz. 2017. mpCubee: Towards a mobile perspective cubic display using mobile phones.

In 2017 IEEE Virtual Reality (VR). IEEE, 459–460.

[123] Jens Grubert and Matthias Kränz. 2017. Towards ad hoc mobile multi-display environments on commodity mobile

devices. In 2017 IEEE Virtual Reality (VR). IEEE, 461–462.

[124] Jens Grubert, Matthias Kranz, and Aaron Quigley. 2015. Design and technology challenges for body proximate display
ecosystems. In Proceedings of the 17th international conference on human-computer interaction with mobile devices and
services adjunct. 951–954.

[125] Jens Grubert, Matthias Kranz, and Aaron Quigley. 2016. Challenges in mobile multi-device ecosystems. mUX: The

Journal of Mobile User Experience 5, 1 (2016), 1–22.

[126] Jens Grubert, Tobias Langlotz, Stefanie Zollmann, and Holger Regenbrecht. 2016. Towards pervasive augmented
reality: Context-awareness in augmented reality. IEEE transactions on visualization and computer graphics 23, 6 (2016),
1706–1724.

[127] Jens Grubert, Ann Morrison, Helmut Munz, and Gerhard Reitmayr. 2012. Playing it real: magic lens and static
peephole interfaces for games in a public space. In Proceedings of the 14th international conference on Human-computer
interaction with mobile devices and services. 231–240.

[128] Jens Grubert, Eyal Ofek, Michel Pahud, Matthias Kranz, and Dieter Schmalstieg. 2016. Glasshands: Interaction around
unmodified mobile devices using sunglasses. In Proceedings of the 2016 ACM International Conference on Interactive
Surfaces and Spaces. 215–224.

[129] Jens Grubert, Eyal Ofek, Michel Pahud, and booktitle=Workshop on Envisioning Future Productivity for Immersive
Analytics at ACM CHI 2020 year=2020 organization=ACM Kristensson, Per Ola. [n.d.]. Back to the Future: Revisiting
Mouse and Keyboard Interaction for HMD-based Immersive Analytics.

[130] Jens Grubert, Eyal Ofek, Michel Pahud, Per Ola Kristensson, Frank Steinicke, and Christian Sandor. 2018. The office

of the future: Virtual, portable, and global. IEEE computer graphics and applications 38, 6 (2018), 125–133.

[131] Jens Grubert, Michel Pahud, Raphael Grasset, Dieter Schmalstieg, and Hartmut Seichter. 2015. The utility of magic
lens interfaces on handheld devices for touristic map navigation. Pervasive and Mobile Computing 18 (2015), 88–103.
[132] Jens Grubert and Dieter Schmalstieg. 2013. Playing it real again: a repeated evaluation of magic lens and static
peephole interfaces in public space. In Proceedings of the 15th international conference on Human-computer interaction
with mobile devices and services. 99–102.

[133] Jens Grubert, Hartmut Seichter, and Dieter Schmalstieg. 2014. Towards user perspective augmented reality for public

displays. In 2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR). IEEE, 339–340.

[134] Jens Grubert, Johannes Tuemle, Ruediger Mecke, and Michael Schenk. 2010. Comparative User Study of two

See-through Calibration Methods. VR 10, 269-270 (2010), 16.

[135] Jens Grubert, Lukas Witzani, Eyal Ofek, Michel Pahud, Matthias Kranz, and Per Ola Kristensson. 2018. Effects of
hand representations for typing in virtual reality. In 2018 IEEE Conference on Virtual Reality and 3D User Interfaces
(VR). IEEE, 151–158.

[136] Jens Grubert, Lukas Witzani, Eyal Ofek, Michel Pahud, Matthias Kranz, and Per Ola Kristensson. 2018. Text entry in
immersive head-mounted display-based virtual reality using standard keyboards. In 2018 IEEE Conference on Virtual
Reality and 3D User Interfaces (VR). IEEE, 159–166.

[137] Jan Gugenheimer, David Dobbelstein, Christian Winkler, Gabriel Haas, and Enrico Rukzio. 2016. Facetouch: Enabling
touch interaction in display fixed uis for mobile virtual reality. In Proceedings of the 29th Annual Symposium on User
Interface Software and Technology. 49–60.

[138] Jan Gugenheimer, Evgeny Stemasov, Julian Frommel, and Enrico Rukzio. 2017. Sharevr: Enabling co-located experi-
ences for virtual reality between hmd and non-hmd users. In Proceedings of the 2017 CHI Conference on Human Factors
in Computing Systems. 4021–4033.

[139] Jan Gugenheimer, Evgeny Stemasov, Harpreet Sareen, and Enrico Rukzio. 2017. FaceDisplay: enabling multi-user
interaction for mobile virtual reality. In Proceedings of the 2017 CHI Conference Extended Abstracts on Human Factors
in Computing Systems. 369–372.

[140] Jie Guo, Dongdong Weng, Zhenliang Zhang, Haiyan Jiang, Yue Liu, Yongtian Wang, and Henry Been-Lirn Duh. 2019.
Mixed Reality Office System Based on Maslow’s Hierarchy of Needs: Towards the Long-Term Immersion in Virtual
Environments. In 2019 IEEE International Symposium on Mixed and Augmented Reality (ISMAR). IEEE, 224–235.
[141] Yulan Guo, Hanyun Wang, Qingyong Hu, Hao Liu, Li Liu, and Mohammed Bennamoun. 2019. Deep Learning for 3D

Point Clouds: A Survey. arXiv preprint arXiv:1912.12033 (2019).

Mixed Reality Interaction Techniques

27

[142] Iman Habib, Niklas Berggren, Erik Rehn, Gustav Josefsson, Andreas Kunz, and Morten Fjeld. 2009. Dgts: Integrated

typing and pointing. In IFIP Conference on Human-Computer Interaction. Springer, 232–235.
[143] Rolf R Hainich and Oliver Bimber. 2016. Displays: fundamentals & applications. CRC press.
[144] Felix G Hamza-Lup, Crenguta M Bogdan, Dorin M Popovici, and Ovidiu D Costea. 2019. A survey of visuo-haptic

simulation in surgical training. arXiv preprint arXiv:1903.03272 (2019).

[145] Chris Hand. 1997. A survey of 3D interaction techniques. In Computer graphics forum, Vol. 16. Wiley Online Library,

269–281.

[146] Daniel Harley, Aneesh P Tarun, Daniel Germinario, and Ali Mazalek. 2017. Tangible vr: Diegetic tangible objects for
virtual reality narratives. In Proceedings of the 2017 Conference on Designing Interactive Systems. 1253–1263.
[147] Chris Harrison, Hrvoje Benko, and Andrew D Wilson. 2011. OmniTouch: wearable multitouch interaction everywhere.

In Proceedings of the 24th annual ACM symposium on User interface software and technology. 441–450.

[148] Alexander G Hauptmann. 1989. Speech and gestures for graphic image manipulation. In Proceedings of the SIGCHI

Conference on Human Factors in Computing Systems. 241–245.

[149] Gunther Heidemann, Ingo Bax, and Holger Bekel. 2004. Multimodal interaction in an augmented reality scenario. In

Proceedings of the 6th international conference on Multimodal interfaces. 53–60.

[150] Morton L Heilig. 1962. Sensorama simulator. US Patent 3,050,870.
[151] Steven Henderson and Steven Feiner. 2010. Opportunistic tangible user interfaces for augmented reality.

IEEE

Transactions on Visualization and Computer Graphics 16, 1 (2010), 4–16.

[152] Anuruddha Hettiarachchi and Daniel Wigdor. 2016. Annexing reality: Enabling opportunistic use of everyday objects
as tangible proxies in augmented reality. In Proceedings of the 2016 CHI Conference on Human Factors in Computing
Systems. 1957–1967.

[153] Alex Hill, Jacob Schiefer, Jeff Wilson, Brian Davidson, Maribeth Gandy, and Blair MacIntyre. 2011. Virtual transparency:
Introducing parallax view into video see-through AR. In 2011 10th IEEE International Symposium on Mixed and
Augmented Reality. IEEE, 239–240.

[154] Ken Hinckley, Mike Sinclair, Erik Hanson, Richard Szeliski, and Matt Conway. 1999. The videomouse: a camera-based
multi-degree-of-freedom input device. In Proceedings of the 12th annual ACM symposium on User interface software
and technology. 103–112.

[155] Hunter G Hoffman, Ari Hollander, Konrad Schroder, Scott Rousseau, and Tom Furness. 1998. Physically touching and

tasting virtual objects enhances the realism of virtual experiences. Virtual Reality 3, 4 (1998), 226–234.

[156] David Holman, Roel Vertegaal, Mark Altosaar, Nikolaus Troje, and Derek Johns. 2005. Paper windows: interaction
techniques for digital paper. In Proceedings of the SIGCHI conference on Human factors in computing systems. 591–599.
[157] Matthew B Hoy. 2018. Alexa, Siri, Cortana, and more: an introduction to voice assistants. Medical reference services

quarterly 37, 1 (2018), 81–88.

[158] Wolfgang Hürst and Casper Van Wezel. 2013. Gesture-based interaction via finger tracking for mobile augmented

reality. Multimedia Tools and Applications 62, 1 (2013), 233–258.

[159] I/O Universal Technologies Inc. [n.d.]. A Brief History of the LCD Key Technology. http://www.lcd-keys.com/english/

history.htm. Last accessed 27.11.2018.

[160] Brent Edward Insko, M Meehan, M Whitton, and F Brooks. 2001. Passive haptics significantly enhances virtual

environments. Ph.D. Dissertation. University of North Carolina at Chapel Hill.

[161] Paul Issartel, Lonni Besançon, Tobias Isenberg, and Mehdi Ammi. 2016. A tangible volume for portable 3d interaction.

In 2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct). IEEE, 215–220.

[162] Alejandro Jaimes and Nicu Sebe. 2007. Multimodal human–computer interaction: A survey. Computer vision and

image understanding 108, 1-2 (2007), 116–134.

[163] Jacek Jankowski and Martin Hachet. 2013. A survey of interaction techniques for interactive 3D environments.
[164] Hans-Christian Jetter, Roman Rädle, Tiare Feuchtner, Christoph Anthes, Judith Friedl, and Clemens Nylandsted
Klokmose. 2020. " In VR, everything is possible!": Sketching and Simulating Spatially-Aware Interactive Spaces in
Virtual Reality. In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems. 1–16.

[165] Fan Jiang, Xubo Yang, and Lele Feng. 2016. Real-time full-body motion reconstruction and recognition for off-the-shelf
VR devices. In Proceedings of the 15th ACM SIGGRAPH Conference on Virtual-Reality Continuum and Its Applications in
Industry-Volume 1. 309–318.

[166] Brett Jones, Rajinder Sodhi, Michael Murdock, Ravish Mehra, Hrvoje Benko, Andrew Wilson, Eyal Ofek, Blair
MacIntyre, Nikunj Raghuvanshi, and Lior Shapira. 2014. RoomAlive: magical experiences enabled by scalable,
adaptive projector-camera units. In Proceedings of the 27th annual ACM symposium on User interface software and
technology. 637–644.

[167] Brett R Jones, Hrvoje Benko, Eyal Ofek, and Andrew D Wilson. 2013. IllumiRoom: peripheral projected illusions for
interactive experiences. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. 869–878.

28

Grubert

[168] Ed Kaiser, Alex Olwal, David McGee, Hrvoje Benko, Andrea Corradini, Xiaoguang Li, Phil Cohen, and Steven Feiner.
2003. Mutual disambiguation of 3D multimodal interaction in augmented and virtual reality. In Proceedings of the 5th
international conference on Multimodal interfaces. 12–19.

[169] Hirokazu Kato. 2007. Inside ARToolKit. In 1st IEEE International Workshop on Augmented Reality Toolkit.
[170] Hirokazu Kato, Mark Billinghurst, Ivan Poupyrev, Kenji Imamoto, and Keihachiro Tachibana. 2000. Virtual object
manipulation on a table-top AR environment. In Proceedings IEEE and ACM International Symposium on Augmented
Reality (ISAR 2000). Ieee, 111–119.

[171] Jun Kato, Daisuke Sakamoto, and Takeo Igarashi. 2010. Surfboard: keyboard with microphone as a low-cost interactive
surface. In Adjunct proceedings of the 23nd annual ACM symposium on User interface software and technology. ACM,
387–388.

[172] H Kazerooni. 2008. A review of the exoskeleton and human augmentation technology. In Dynamic Systems and

Control Conference, Vol. 43352. 1539–1547.

[173] Daniel F Keefe, Daniel Acevedo Feliz, Tomer Moscovich, David H Laidlaw, and Joseph J LaViola Jr. 2001. CavePainting:
a fully immersive 3D artistic medium and interactive experience. In Proceedings of the 2001 symposium on Interactive
3D graphics. 85–93.

[174] David Kim, Otmar Hilliges, Shahram Izadi, Alex D Butler, Jiawen Chen, Iason Oikonomidis, and Patrick Olivier. 2012.
Digits: freehand 3D interactions anywhere using a wrist-worn gloveless sensor. In Proceedings of the 25th annual
ACM symposium on User interface software and technology. 167–176.

[175] David Kim, Shahram Izadi, Jakub Dostal, Christoph Rhemann, Cem Keskin, Christopher Zach, Jamie Shotton, Timothy
Large, Steven Bathiche, Matthias Nießner, et al. 2014. RetroDepth: 3D silhouette sensing for high-precision input
on and above physical surfaces. In Proceedings of the 32nd annual ACM conference on Human factors in computing
systems. ACM, 1377–1386.

[176] Kangsoo Kim, Luke Boelling, Steffen Haesler, Jeremy Bailenson, Gerd Bruder, and Greg F Welch. 2018. Does a digital
assistant need a body? The influence of visual embodiment and social behavior on the perception of intelligent virtual
agents in AR. In 2018 IEEE International Symposium on Mixed and Augmented Reality (ISMAR). IEEE, 105–114.
[177] Seoktae Kim, Hyunjung Kim, Boram Lee, Tek-Jin Nam, and Woohun Lee. 2008. Inflatable mouse: volume-adjustable
mouse with air-pressure-sensitive input and haptic feedback. In Proceedings of the SIGCHI Conference on Human
Factors in Computing Systems. 211–224.

[178] Matjaž Kljun, Klen Čopič Pucihar, Jason Alexander, Maheshya Weerasinghe, Cuauhtli Campos, Julie Ducasse, Barbara
Kopacin, Jens Grubert, Paul Coulton, and Miha Čelar. 2019. Augmentation not duplication: considerations for the
design of digitally-augmented comic books. In Proceedings of the 2019 CHI Conference on Human Factors in Computing
Systems. 1–12.

[179] Pascal Knierim, Francisco Kiss, and Albrecht Schmidt. 2018. Look inside: understanding thermal flux through
augmented reality. In 2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct).
IEEE, 170–171.

[180] Pascal Knierim, Valentin Schwind, Anna Maria Feit, Florian Nieuwenhuizen, and Niels Henze. 2018. Physical
keyboards in virtual reality: Analysis of typing performance and effects of avatar hands. In Proceedings of the 2018
CHI Conference on Human Factors in Computing Systems. 1–9.

[181] Luv Kohli, Eric Burns, Dorian Miller, and Henry Fuchs. 2005. Combining passive haptics with redirected walking. In

Proceedings of the 2005 international conference on Augmented tele-existence. 253–254.

[182] Naoya Koizumi, Hidekazu Tanaka, Yuji Uema, and Masahiko Inami. 2011. Chewing jockey: augmented food texture
by using sound based on the cross-modal effect. In Proceedings of the 8th International Conference on Advances in
Computer Entertainment Technology. 1–4.

[183] Mathias Kolsch, Matthew Turk, and Tobias Hollerer. 2004. Vision-based interfaces for mobility. In The First Annual
International Conference on Mobile and Ubiquitous Systems: Networking and Services, 2004. MOBIQUITOUS 2004. IEEE,
86–94.

[184] Aaron Kotranza and Benjamin Lok. 2008. Virtual human+ tangible interface= mixed reality human an initial

exploration with a virtual breast exam patient. In 2008 IEEE Virtual Reality Conference. IEEE, 99–106.

[185] Aaron Kotranza, Benjamin Lok, Adeline Deladisma, Carla M Pugh, and D Scott Lind. 2009. Mixed reality humans:
Evaluating behavior, usability, and acceptability. IEEE Transactions on Visualization and Computer Graphics 15, 3
(2009), 369–382.

[186] Sven Kratz and Michael Rohs. 2009. Hoverflow: exploring around-device interaction with IR distance sensors. In Proc.

MobileHCI ’09. ACM, 42.

[187] Max Krichenbauer, Goshiro Yamamoto, Takafumi Taketom, Christian Sandor, and Hirokazu Kato. 2017. Augmented
reality versus virtual reality for 3d object manipulation. IEEE transactions on visualization and computer graphics 24, 2
(2017), 1038–1048.

Mixed Reality Interaction Techniques

29

[188] Kai Kunze, Kouta Minamizawa, Stephan Lukosch, Masahiko Inami, and Jun Rekimoto. 2017. Superhuman sports:

Applying human augmentation to physical exercise. IEEE Pervasive Computing 16, 2 (2017), 14–17.

[189] Toshifumi Kurosawa, Buntarou Shizuki, and Jiro Tanaka. 2013. Keyboard Clawing: input method by clawing key tops.

In International Conference on Human-Computer Interaction. Springer, 272–280.

[190] Mikko Kytö, Barrett Ens, Thammathip Piumsomboon, Gun A Lee, and Mark Billinghurst. 2018. Pinpointing: Precise
Head-and Eye-Based Target Selection for Augmented Reality. In Proceedings of the 2018 CHI Conference on Human
Factors in Computing Systems. ACM, 81.

[191] J LaViola. 1999. Whole-hand and speech input in virtual environments. Unpublished master’s thesis, Department of

Computer Science, Brown University, CS-99-15 (1999).

[192] Joseph J LaViola Jr, Ernst Kruijff, Ryan P McMahan, Doug Bowman, and Ivan P Poupyrev. 2017. 3D user interfaces:

theory and practice. Addison-Wesley Professional.

[193] Byungjoo Lee, Haesun Park, and Hyunwoo Bang. 2013. Multidirectional Pointing Input Using a Hardware Keyboard.

ETRI Journal 35, 6 (2013), 1160–1163.

[194] Gun A Lee, Gerard J Kim, and Mark Billinghurst. 2007. Interaction design for tangible augmented reality applications.

In Emerging technologies of augmented reality: Interfaces and design. IGI Global, 261–282.

[195] Joon Hyub Lee, Sang-Gyun An, Yongkwan Kim, and Seok-Hyung Bae. 2018. Projective Windows: Bringing Windows
in Space to the Fingertip. In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems. ACM,
218.

[196] Lik-Hang Lee and Pan Hui. 2018. Interaction methods for smart glasses: A survey. IEEE Access 6 (2018), 28712–28732.
[197] Minkyung Lee, Richard Green, and Mark Billinghurst. 2008. 3D natural hand interaction for AR applications. In 2008

23rd International Conference Image and Vision Computing New Zealand. IEEE, 1–6.

[198] Taehee Lee and Tobias Hollerer. 2007. Handy AR: Markerless inspection of augmented reality objects using fingertip

tracking. In 2007 11th IEEE International Symposium on Wearable Computers. IEEE, 83–90.

[199] Sang-won Leigh, Philipp Schoessler, Felix Heibeck, Pattie Maes, and Hiroshi Ishii. 2015. THAW: tangible interaction
with see-through augmentation for smartphones on computer screens. In Proceedings of the Ninth International
Conference on Tangible, Embedded, and Embodied Interaction. ACM, 89–96.

[200] Daniel Leithinger, Sean Follmer, Alex Olwal, Samuel Luescher, Akimitsu Hogge, Jinha Lee, and Hiroshi Ishii. 2013.
Sublimate: state-changing virtual and physical rendering to augment interaction with shape displays. In Proceedings
of the SIGCHI conference on human factors in computing systems. 1441–1450.

[201] Vernon W Lemmon. 1937. Extra-sensory perception. The Journal of Psychology 4, 1 (1937), 227–238.
[202] Frank Chun Yat Li, David Dearman, and Khai N Truong. 2009. Virtual shelves: interactions with orientation aware
devices. In Proceedings of the 22nd annual ACM symposium on User interface software and technology. ACM, 125–128.
[203] Rui Li, Zhenyu Liu, and Jianrong Tan. 2019. A survey on 3D hand pose estimation: Cameras, methods, and datasets.

Pattern Recognition 93 (2019), 251–272.

[204] Zhen Li, Michelle Annett, Ken Hinckley, Karan Singh, and Daniel Wigdor. 2019. HoloDoc: Enabling Mixed Reality
Workspaces that Harness Physical and Digital Content. In Proceedings of the 2019 CHI Conference on Human Factors in
Computing Systems. ACM, 687.

[205] Zhao Liu, Jianke Zhu, Jiajun Bu, and Chun Chen. 2015. A survey of human pose estimation: the body parts parsing

based methods. Journal of Visual Communication and Image Representation 32 (2015), 10–19.

[206] Logitech. [n.d.]. Logitech G19 Keyboard for Gaming. https://support.logitech.com/en_us/product/g19-keyboard-for-

gaming. Last accessed 27.11.2018.

[207] Julian Looser, Raphael Grasset, and Mark Billinghurst. 2007. A 3D flexible and tangible magic lens in augmented

reality. In 2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality. IEEE, 51–54.

[208] Chen Change Loy, W Lai, and C Lim. 2005. Development of a pressure-based typing biometrics user authentication

system. ASEAN Virtual Instrumentation Applications Contest Submission (2005).

[209] Zhixiong Lu, Yongtao Hu, and Jingwen Dai. [n.d.]. WatchAR: 6-DoF Tracked Watch for AR Interaction. In IEEE

ISMAR 2019 Demonstrations.

[210] Blair MacIntyre, Jay David Bolter, Emmanuel Moreno, and Brendan Hannigan. 2001. Augmented reality as a new
media experience. In Proceedings IEEE and ACM International Symposium on Augmented Reality. IEEE, 197–206.
[211] Wendy E Mackay and Anne-Laure Fayard. 1999. Designing interactive paper: lessons from three augmented reality
projects. In Proceedings of the international workshop on Augmented reality: placing artificial objects in real scenes:
placing artificial objects in real scenes. AK Peters, Ltd., 81–90.

[212] I Scott MacKenzie, R William Soukoreff, and Chris Pal. 1997. A two-ball mouse affords three degrees of freedom. In

CHI’97 Extended Abstracts on Human Factors in Computing Systems. 303–304.

[213] A MacWilliams, C Sandor, M Wagner, M Bauer, G Klinker, and B Bruegge. 2003. Herding Sheep: Live System
Development for Distributed Augmented Reality. ISMAR’03: Proceedings of the 2nd IEEE. In ACM International
Symposium on Mixed and Augmented Reality. 123.

30

Grubert

[214] Pattie Maes, Trevor Darrell, Bruce Blumberg, and Alex Pentland. 1997. The ALIVE system: Wireless, full-body

interaction with autonomous agents. Multimedia systems 5, 2 (1997), 105–112.

[215] Anindya Maiti, Murtuza Jadliwala, and Chase Weber. 2017. Preventing shoulder surfing using randomized augmented
reality keyboards. In 2017 IEEE International Conference on Pervasive Computing and Communications Workshops
(PerCom Workshops). IEEE, 630–635.

[216] Asier Marzo, Benoît Bossavit, and Martin Hachet. 2014. Combining multi-touch input and device movement for 3D
manipulations in mobile augmented reality environments. In Proceedings of the 2nd ACM symposium on Spatial user
interaction. ACM, 13–16.

[217] Mark McGill, Daniel Boland, Roderick Murray-Smith, and Stephen Brewster. 2015. A dose of reality: Overcoming
usability challenges in vr head-mounted displays. In Proceedings of the 33rd Annual ACM Conference on Human Factors
in Computing Systems. 2143–2152.

[218] William A McNeely. 1993. Robotic graphics: a new approach to force feedback for virtual reality. In Proceedings of

IEEE Virtual Reality Annual International Symposium. IEEE, 336–341.

[219] Daniel Mendes, Fabio Marco Caputo, Andrea Giachetti, Alfredo Ferreira, and J Jorge. 2019. A survey on 3D virtual
object manipulation: From the desktop to immersive virtual environments. In Computer graphics forum, Vol. 38. Wiley
Online Library, 21–45.

[220] Tim Menzner, Travis Gesslein, Alexander Otte, and Jens Grubert. 2020. Above Surface Interaction for Multiscale
Navigation in Mobile Virtual Reality. In 2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR). IEEE,
372–381.
[221] Microsoft. 2019.

Getting started with the Windows Mixed Reality Toolkit.

https://microsoft.github.io/

MixedRealityToolkit-Unity/Documentation/GettingStartedWithTheMRTK.html. [Online; accessed 31-May-2020].

[222] Mark R Mine. 1995. Virtual environment interaction techniques. UNC Chapel Hill CS Dept (1995).
[223] James Minogue and M Gail Jones. 2006. Haptics in education: Exploring an untapped sensory modality. Review of

Educational Research 76, 3 (2006), 317–348.

[224] Pranav Mistry and Pattie Maes. 2009. SixthSense: a wearable gestural interface. In ACM SIGGRAPH ASIA 2009 Art

Gallery & Emerging Technologies: Adaptation. 85–85.

[225] Peter Mohr, Markus Tatzgern, Jens Grubert, Dieter Schmalstieg, and Denis Kalkofen. 2017. Adaptive user perspective

rendering for handheld augmented reality. In 2017 IEEE Symposium on 3D User Interfaces (3DUI). IEEE, 176–181.

[226] Ann Morrison, Antti Oulasvirta, Peter Peltonen, Saija Lemmela, Giulio Jacucci, Gerhard Reitmayr, Jaana Näsänen,
and Antti Juustila. 2009. Like bees around the hive: a comparative study of a mobile augmented reality map. In
Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. 1889–1898.

[227] Annette Mossel, Benjamin Venditti, and Hannes Kaufmann. 2013. 3DTouch and HOMER-S: intuitive manipulation
techniques for one-handed handheld augmented reality. In Proceedings of the Virtual Reality International Conference:
Laval Virtual. ACM, 12.

[228] Alessandro Mulloni, Jens Grubert, Hartmut Seichter, Tobias Langlotz, Raphael Grasset, Gerhard Reitmayr, and Dieter
Schmalstieg. 2012. Experiences with the impact of tracking technology in mobile augmented reality evaluations. In
MobileHCI 2012 Workshop MobiVis, Vol. 2. Citeseer.

[229] Erik Murphy-Chutorian and Mohan Manubhai Trivedi. 2008. Head pose estimation in computer vision: A survey.

IEEE transactions on pattern analysis and machine intelligence 31, 4 (2008), 607–626.

[230] Sara Nabil, Thomas Plötz, and David S Kirk. 2017. Interactive architecture: Exploring and unwrapping the potentials
of organic user interfaces. In Proceedings of the Eleventh International Conference on Tangible, Embedded, and Embodied
Interaction. 89–100.

[231] Rajalakshmi Nandakumar, Vikram Iyer, Desney Tan, and Shyamnath Gollakota. 2016. Fingerio: Using active sonar for
fine-grained finger tracking. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems. ACM,
1515–1525.

[232] Takuji Narumi, Yuki Ban, Takashi Kajinami, Tomohiro Tanikawa, and Michitaka Hirose. 2012. Augmented perception
of satiety: controlling food consumption by changing apparent size of food with augmented reality. In Proceedings of
the SIGCHI Conference on Human Factors in Computing Systems. 109–118.

[233] Takuji Narumi, Shinya Nishizaka, Takashi Kajinami, Tomohiro Tanikawa, and Michitaka Hirose. 2011. Augmented
reality flavors: gustatory display based on edible marker and cross-modal interaction. In Proceedings of the SIGCHI
conference on human factors in computing systems. 93–102.

[234] Niels Christian Nilsson, Tabitha Peck, Gerd Bruder, Eri Hodgson, Stefania Serafin, Mary Whitton, Frank Steinicke,
and Evan Suma Rosenberg. 2018. 15 years of research on redirected walking in immersive virtual environments. IEEE
computer graphics and applications 38, 2 (2018), 44–56.

[235] Siti Soleha Muhammad Nizam, Rimaniza Zainal Abidin, Nurhazarifah Che Hashim, Meng Chun Lam, Haslina Arshad,
and Nazatul Aini Abd Majid. 2018. A review of multimodal interaction technique in augmented reality environment.
Int. J. Adv. Sci. Eng. Inf. Technol 8, 4-2 (2018), 8–4.

Mixed Reality Interaction Techniques

31

[236] Donald A Norman. 2010. Natural user interfaces are not natural. interactions 17, 3 (2010), 6–10.
[237] Erwan Normand and Michael J McGuffin. 2018. Enlarging a smartphone with AR to create a handheld VESAD
(Virtually Extended Screen-Aligned Display). In 2018 IEEE International Symposium on Mixed and Augmented Reality
(ISMAR). IEEE, 123–133.

[238] Nahal Norouzi, Gerd Bruder, Brandon Belna, Stefanie Mutter, Damla Turgut, and Greg Welch. 2019. A systematic
review of the convergence of augmented reality, intelligent virtual agents, and the internet of things. In Artificial
Intelligence in IoT. Springer, 1–24.

[239] Nahal Norouzi, Kangsoo Kim, Jason Hochreiter, Myungho Lee, Salam Daher, Gerd Bruder, and Greg Welch. 2018. A
systematic survey of 15 years of user studies published in the intelligent virtual agents conference. In Proceedings of
the 18th international conference on intelligent virtual agents. 17–22.

[240] Hyacinth S Nwana. 1996. Software agents: An overview. The knowledge engineering review 11, 3 (1996), 205–244.
[241] Ian Oakley and Doyoung Lee. 2014. Interaction on the Edge: Offset Sensing for Small Devices. In Proceedings of the
SIGCHI Conference on Human Factors in Computing Systems (Toronto, Ontario, Canada) (CHI ’14). ACM, New York,
NY, USA, 169–178. https://doi.org/10.1145/2556288.2557138

[242] Ji-young Oh and Hong Hua. 2006. User evaluations on form factors of tangible magic lenses. In 2006 IEEE/ACM

International Symposium on Mixed and Augmented Reality. IEEE, 23–32.

[243] Alex Olwal, Hrvoje Benko, and Steven Feiner. 2003. Senseshapes: Using statistical geometry for object selection in
a multimodal augmented reality. In The Second IEEE and ACM International Symposium on Mixed and Augmented
Reality, 2003. Proceedings. IEEE, 300–301.

[244] Alex Olwal and Steven Feiner. 2004. Unit: modular development of distributed interaction techniques for highly
interactive user interfaces. In Proceedings of the 2nd international conference on Computer graphics and interactive
techniques in Australasia and South East Asia. 131–138.

[245] Claudio Pacchierotti, Stephen Sinclair, Massimiliano Solazzi, Antonio Frisoli, Vincent Hayward, and Domenico
Prattichizzo. 2017. Wearable haptic systems for the fingertip and the hand: taxonomy, review, and perspectives. IEEE
transactions on haptics 10, 4 (2017), 580–600.

[246] Gary Perelman, Marcos Serrano, Mathieu Raynal, Celia Picard, Mustapha Derras, and Emmanuel Dubois. 2015. The
roly-poly mouse: Designing a rolling input device unifying 2d and 3d interaction. In Proceedings of the 33rd Annual
ACM Conference on Human Factors in Computing Systems. 327–336.

[247] Julian Petford, Miguel A Nacenta, and Carl Gutwin. 2018. Pointing all around you: selection performance of mouse and
ray-cast pointing in full-coverage displays. In Proceedings of the 2018 CHI Conference on Human Factors in Computing
Systems. ACM, 533.

[248] Ken Pfeuffer, Benedikt Mayer, Diako Mardanbegi, and Hans Gellersen. 2017. Gaze+ pinch interaction in virtual reality.

In Proceedings of the 5th Symposium on Spatial User Interaction. ACM, 99–108.

[249] Duc-Minh Pham and Wolfgang Stuerzlinger. 2019. HawKEY: Efficient and Versatile Text Entry for Virtual Reality. In

25th ACM Symposium on Virtual Reality Software and Technology. 1–11.

[250] Duc-Minh Pham and Wolfgang Stuerzlinger. 2019. Is the Pen Mightier than the Controller? A Comparison of Input
Devices for Selection in Virtual and Augmented Reality. In 25th ACM Symposium on Virtual Reality Software and
Technology. 1–11.

[251] Thammathip Piumsomboon, David Altimira, Hyungon Kim, Adrian Clark, Gun Lee, and Mark Billinghurst. 2014.
Grasp-Shell vs gesture-speech: A comparison of direct and indirect natural interaction techniques in augmented
reality. In 2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR). IEEE, 73–82.

[252] Thammathip Piumsomboon, Adrian Clark, Mark Billinghurst, and Andy Cockburn. 2013. User-defined gestures for

augmented reality. In IFIP Conference on Human-Computer Interaction. Springer, 282–299.

[253] Thammathip Piumsomboon, Gun Lee, Robert W Lindeman, and Mark Billinghurst. 2017. Exploring natural eye-
gaze-based interaction for immersive virtual reality. In 2017 IEEE Symposium on 3D User Interfaces (3DUI). IEEE,
36–39.

[254] Jarkko Polvi, Takafumi Taketomi, Goshiro Yamamoto, Arindam Dey, Christian Sandor, and Hirokazu Kato. 2016.
SlidAR: A 3D positioning method for SLAM-based handheld augmented reality. Computers & Graphics 55 (2016),
33–43.

[255] Ivan Poupyrev, Numada Tomokazu, and Suzanne Weghorst. 1998. Virtual Notepad: handwriting in immersive VR. In
Proceedings. IEEE 1998 Virtual Reality Annual International Symposium (Cat. No. 98CB36180). IEEE, 126–132.
[256] Alexander Prange, Michael Barz, and Daniel Sonntag. 2018. Medical 3d images in multimodal virtual reality. In

Proceedings of the 23rd International Conference on Intelligent User Interfaces Companion. 1–2.

[257] Aaron Quigley and Jens Grubert. 2015. Perceptual and social challenges in body proximate display ecosystems. In
Proceedings of the 17th International Conference on Human-Computer Interaction with Mobile Devices and Services
Adjunct. 1168–1174.

32

Grubert

[258] Roman Rädle, Hans-Christian Jetter, Nicolai Marquardt, Harald Reiterer, and Yvonne Rogers. 2014. Huddlelamp:
Spatially-aware mobile displays for ad-hoc around-the-table collaboration. In Proceedings of the Ninth ACM Interna-
tional Conference on Interactive Tabletops and Surfaces. 45–54.

[259] Mohammad Muntasir Rahman, Yanhao Tan, Jian Xue, and Ke Lu. 2019. Recent advances in 3D object detection in the

era of deep neural networks: A survey. IEEE Transactions on Image Processing (2019).

[260] Julian Ramos, Zhen Li, Johana Rosas, Nikola Banovic, Jennifer Mankoff, and Anind Dey. 2016. Keyboard Surface

Interaction: Making the keyboard into a pointing device. arXiv preprint arXiv:1601.04029 (2016).

[261] Majken K Rasmussen, Esben W Pedersen, Marianne G Petersen, and Kasper Hornbæk. 2012. Shape-changing
interfaces: a review of the design space and open research questions. In Proceedings of the SIGCHI Conference on
Human Factors in Computing Systems. 735–744.

[262] Razor. [n.d.]. Razor Deathstalker Ultimate Keyboard. https://support.razer.com/gaming-keyboards/razer-deathstalker-

ultimate. Last accessed 27.11.2018.

[263] Stuart Reeves, Steve Benford, Claire O’Malley, and Mike Fraser. 2005. Designing the spectator experience. In

Proceedings of the SIGCHI conference on Human factors in computing systems. 741–750.

[264] Holger T Regenbrecht, Michael Wagner, and Gregory Baratoff. 2002. Magicmeeting: A collaborative tangible

augmented reality system. Virtual Reality 6, 3 (2002), 151–166.

[265] Patrick Reipschläger and Raimund Dachselt. 2019. DesignAR: Immersive 3D-Modeling Combining Augmented Reality
with Interactive Displays. In Proceedings of the 2019 ACM International Conference on Interactive Surfaces and Spaces.
29–41.

[266] Patrick Reipschläger, Severin Engert, and Raimund Dachselt. 2020. Augmented Displays: Seamlessly Extending
Interactive Surfaces With Head-Mounted Augmented Reality. In Extended Abstracts of the 2020 CHI Conference on
Human Factors in Computing Systems. 1–4.

[267] Gerhard Reitmayr, Ethan Eade, and Tom Drummond. 2005. Localisation and interaction for augmented maps. In
Fourth IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR’05). IEEE, 120–129.
[268] Jun Rekimoto, Takaaki Ishizawa, Carsten Schwesig, and Haruo Oba. 2003. PreSense: interaction techniques for finger
sensing input devices. In Proceedings of the 16th annual ACM symposium on User interface software and technology.
ACM, 203–212.

[269] Jun Rekimoto and Masanori Saitoh. 1999. Augmented surfaces: a spatially continuous work space for hybrid computing

environments. In Proceedings of the SIGCHI conference on Human Factors in Computing Systems. 378–385.

[270] Jun Rekimoto, Brygg Ullmer, and Haruo Oba. 2001. DataTiles: a modular platform for mixed physical and graphical

interactions. In Proceedings of the SIGCHI conference on Human factors in computing systems. 269–276.

[271] Jie Ren, Yueting Weng, Chengchi Zhou, Chun Yu, and Yuanchun Shi. 2020. Understanding Window Management
Interactions in AR Headset+ Smartphone Interface. In Extended Abstracts of the 2020 CHI Conference on Human Factors
in Computing Systems Extended Abstracts. 1–8.

[272] Julie Rico and Stephen Brewster. 2010. Usable gestures for mobile interfaces: evaluating social acceptability. In

Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. 887–896.

[273] Bernhard E Riecke, Joseph J LaViola Jr, and Ernst Kruijff. 2018. 3D user interfaces for virtual reality and games: 3D

selection, manipulation, and spatial navigation. In ACM SIGGRAPH 2018 Courses. 1–94.

[274] Nina Rosa, Peter Werkhoven, and Wolfgang Hürst. 2016. (Re-) examination of multimodal augmented reality. In

Proceedings of the 2016 workshop on Multimodal Virtual and Augmented Reality. 1–5.

[275] A. Ruvimova, J. Kim, T. Fritz, M. Hancock, and D. C. Shepherd. 2020. "Transport Me Away": Fostering Flow in Open
Offices through Virtual Reality.. In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems.
ACM.

[276] Kunhee Ryu, Joong-Jae Lee, and Jung-Min Park. 2019. GG Interaction: a gaze–grasp pose interaction for 3D virtual

object selection. Journal on Multimodal User Interfaces (2019), 1–11.

[277] Emanuel Sachs, Andrew Roberts, and David Stoops. 1991. 3-Draw: A tool for designing 3D shapes. IEEE Computer

Graphics and Applications 6 (1991), 18–26.

[278] Houssem Saidi, Marcos Serrano, Pourang Irani, and Emmanuel Dubois. 2017. TDome: a touch-enabled 6DOF
interactive device for multi-display environments. In Proceedings of the 2017 CHI Conference on Human Factors in
Computing Systems. 5892–5904.

[279] R Munoz Salinas. 2012. ArUco: A minimal library for Augmented Reality applications based on OpenCV.
[280] Christian Sandor, Shinji Uchiyama, and Hiroyuki Yamamoto. 2007. Visuo-haptic systems: Half-mirrors considered
harmful. In Second joint eurohaptics conference and symposium on haptic interfaces for virtual environment and
teleoperator systems (WHC’07). IEEE, 292–297.

[281] Nikolaos Sarafianos, Bogdan Boteanu, Bogdan Ionescu, and Ioannis A Kakadiaris. 2016. 3d human pose estimation: A
review of the literature and analysis of covariates. Computer Vision and Image Understanding 152 (2016), 1–20.

Mixed Reality Interaction Techniques

33

[282] Munehiko Sato, Ivan Poupyrev, and Chris Harrison. 2012. Touché: enhancing touch interaction on humans, screens,
liquids, and everyday objects. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems.
483–492.

[283] Dieter Schmalstieg, L Miguel Encarnação, and Zsolt Szalavári. 1999. Using transparent props for interaction with the

virtual table. SI3D 99 (1999), 147–153.

[284] Albrecht Schmidt. 2015. Biosignals in human-computer interaction. interactions 23, 1 (2015), 76–79.
[285] Albrecht Schmidt. 2017. Augmenting human intellect and amplifying perception and cognition. IEEE Pervasive

Computing 16, 1 (2017), 6–10.

[286] Dominik Schmidt, Julian Seifert, Enrico Rukzio, and Hans Gellersen. 2012. A Cross-device Interaction Style for

Mobiles and Surfaces. In Proc. DIS ’12.

[287] Susanne Schmidt, Gerd Bruder, and Frank Steinicke. 2019. Effects of virtual agent and object representation on

experiencing exhibited artifacts. Computers & Graphics 83 (2019), 1–10.

[288] Daniel Schneider and Jens Grubert. 2017. [POSTER] Feasibility of Corneal Imaging for Handheld Augmented Reality.

In 2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct). IEEE, 44–45.

[289] Daniel Schneider and Jens Grubert. 2017. Towards Around-Device Interaction using Corneal Imaging. In Proceedings

of the 2017 ACM International Conference on Interactive Surfaces and Spaces. 287–293.

[290] Daniel Schneider, Alexander Otte, Travis Gesslein, Philipp Gagel, Bastian Kuth, Mohamad Shahm Damlakhi, Oliver
Dietz, Eyal Ofek, Michel Pahud, Per Ola Kristensson, et al. 2019. ReconViguRation: Reconfiguring Physical Keyboards
in Virtual Reality. IEEE transactions on visualization and computer graphics (2019).

[291] Daniel Schneider, Alexander Otte, Axel Simon Kublin, Per Ola Kristensson, Eyal Ofek, Michel Pahud, Alexander
Martschenko, and Jens Grubert. 2020. Accuracy of Commodity Finger Tracking Systems for Virtual Reality Head-
Mounted Displays. In IEEE VR 2020. IEEE, IEEE. https://www.microsoft.com/en-us/research/publication/accuracy-of-
commodity-finger-tracking-systems-for-virtual-reality-head-mounted-displays/

[292] mc schraefel. 2019. in5: a Model for Inbodied Interaction. In Extended Abstracts of the 2019 CHI Conference on Human

Factors in Computing Systems. 1–6.

[293] Kodai Sekimori, Yusuke Yamasaki, Yuki Takagi, Kazuma Murata, Buntarou Shizuki, and Shin Takahashi. 2018. Ex-
space: Expanded space key by sliding thumb on home position. In International Conference on Human-Computer
Interaction. Springer, 68–78.

[294] Marcos Serrano, Barrett Ens, Xing-Dong Yang, and Pourang Irani. 2015. Gluey: Developing a head-worn display
interface to unify the interaction experience in distributed display environments. In Proceedings of the 17th International
Conference on Human-Computer Interaction with Mobile Devices and Services. 161–171.

[295] Orit Shaer and Eva Hornecker. 2010. Tangible user interfaces: past, present, and future directions. Foundations and

trends in human-computer interaction 3, 1–2 (2010), 1–137.

[296] Yan Shen, Soh-Khim Ong, and Andrew YC Nee. 2011. Vision-based hand interaction in augmented reality environment.

Intl. Journal of Human–Computer Interaction 27, 6 (2011), 523–544.

[297] Yilei Shi, Tomás Vega Gálvez, Haimo Zhang, and Suranga Nanayakkara. 2017. Gestakey: Get more done with
just-a-key on a keyboard. In Adjunct Publication of the 30th Annual ACM Symposium on User Interface Software and
Technology. 73–75.

[298] Yilei Shi, Haimo Zhang, Hasitha Rajapakse, Nuwan Tharaka Perera, Tomás Vega Gálvez, and Suranga Nanayakkara.
2018. GestAKey: Touch Interaction on Individual Keycaps. In Proceedings of the 2018 CHI Conference on Human Factors
in Computing Systems. ACM, 596.

[299] Ludwig Sidenmark and Hans Gellersen. 2019. Eye&Head: Synergetic Eye and Head Movement for Gaze Pointing and
Selection. In Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology. 1161–1174.
[300] Ludwig Sidenmark, Diako Mardanbegi, Argenis Ramirez Gomez, Christopher Clarke, and Hans Gellersen. 2020.
BimodalGaze: Seamlessly Refined Pointing with Gaze and Filtered Gestural Head Movement. In Proceedings of Eye
Tracking Research and Applications.

[301] Adalberto L Simeone, Eduardo Velloso, and Hans Gellersen. 2015. Substitutional reality: Using the physical environ-
ment to design virtual reality experiences. In Proceedings of the 33rd Annual ACM Conference on Human Factors in
Computing Systems. 3307–3316.

[302] Hari Singh and Jaswinder Singh. 2019. Object Acquisition and Selection in Human Computer Interaction Systems: A

Review. International Journal of Intelligent Systems and Applications in Engineering 7, 1 (2019), 19–29.

[303] Jie Song, Gábor Sörös, Fabrizio Pece, Sean Ryan Fanello, Shahram Izadi, Cem Keskin, and Otmar Hilliges. 2014. In-air
gestures around unmodified mobile devices. In Proceedings of the 27th annual ACM symposium on User interface
software and technology. ACM, 319–329.

[304] Marco Speicher, Anna Maria Feit, Pascal Ziegler, and Antonio Krüger. 2018. Selection-Based Text Entry in Virtual
Reality. In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems (Montreal QC, Canada)
(CHI ’18). Association for Computing Machinery, New York, NY, USA, 1–13. https://doi.org/10.1145/3173574.3174221

34

Grubert

[305] Martin Spindler and Raimund Dachselt. 2009. PaperLens: advanced magic lens interaction above the tabletop. In

Proceedings of the ACM International Conference on Interactive Tabletops and Surfaces. ACM, 7.

[306] Martin Spindler, Christian Tominski, Heidrun Schumann, and Raimund Dachselt. 2010. Tangible views for information

visualization. In ACM International Conference on Interactive Tabletops and Surfaces. ACM, 157–166.

[307] Anthony Steed and Mel Slater. 1995. 3d interaction with the desktop bat. In Computer Graphics Forum, Vol. 14. Wiley

Online Library, 97–104.

[308] Jürgen Steimle, Andreas Jordt, and Pattie Maes. 2013. Flexpad: highly flexible bending interactions for projected
handheld displays. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. ACM, 237–246.
[309] Wolfgang Stuerzlinger and Chadwick A Wingrave. 2011. The value of constraints for 3D user interfaces. In Virtual

Realities. Springer, 203–223.

[310] Junwei Sun, Wolfgang Stuerzlinger, and Bernhard E Riecke. 2018. Comparing input methods and cursors for 3D

positioning with head-mounted displays. In Proceedings of the 15th ACM Symposium on Applied Perception. 1–8.

[311] Hemant Bhaskar Surale, Aakar Gupta, Mark Hancock, and Daniel Vogel. 2019. TabletInVR: Exploring the Design
Space for Using a Multi-Touch Tablet in Virtual Reality. In Proceedings of the 2019 CHI Conference on Human Factors
in Computing Systems. ACM, 13.

[312] Ryo Suzuki, Hooman Hedayati, Clement Zheng, James L Bohn, Daniel Szafir, Ellen Yi-Luen Do, Mark D Gross, and
Daniel Leithinger. 2020. RoomShift: Room-scale Dynamic Haptics for VR with Furniture-moving Swarm Robots. In
Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems. 1–11.

[313] Ryo Suzuki, Rubaiat Habib Kazi, Li-Yi Wei, Stephen DiVerdi, Wilmot Li, and Daniel Leithinger. 2020. RealitySketch:
Embedding Responsive Graphics and Visualizations in AR with Dynamic Sketching. In Adjunct Publication of the
33rd Annual ACM Symposium on User Interface Software and Technology. 135–138.

[314] Zsolt Szalavári and Michael Gervautz. 1997. The personal interaction Panel–a Two-Handed interface for augmented

reality. In Computer graphics forum, Vol. 16. Wiley Online Library, C335–C346.

[315] Vildan Tanriverdi and Robert JK Jacob. 2000. Interacting with eye movements in virtual environments. In Proceedings

of the SIGCHI conference on Human Factors in Computing Systems. 265–272.

[316] Stuart Taylor, Cem Keskin, Otmar Hilliges, Shahram Izadi, and John Helmes. 2014. Type-hover-swipe in 96 bytes: a
motion sensing mechanical keyboard. In Proceedings of the 32nd annual ACM conference on Human factors in computing
systems. ACM, 1695–1704.

[317] Robert J Teather and Wolfgang Stuerzlinger. 2011. Pointing at 3D targets in a stereo head-tracked virtual environment.

In 2011 IEEE Symposium on 3D User Interfaces (3DUI). IEEE, 87–94.

[318] Justus Thies, Michael Zollhöfer, Marc Stamminger, Christian Theobalt, and Matthias Nießner. 2018. FaceVR: Real-time

gaze-aware facial reenactment in virtual reality. ACM Transactions on Graphics (TOG) 37, 2 (2018), 1–15.

[319] Christian Tominski, Stefan Gladisch, Ulrike Kister, Raimund Dachselt, and Heidrun Schumann. 2014. A Survey on

Interactive Lenses in Visualization.. In EuroVis (STARs). Citeseer.

[320] Christian Tominski, Stefan Gladisch, Ulrike Kister, Raimund Dachselt, and Heidrun Schumann. 2017. Interactive
lenses for visualization: An extended survey. In Computer Graphics Forum, Vol. 36. Wiley Online Library, 173–200.
[321] Makoto Tomioka, Sei Ikeda, and Kosuke Sato. 2013. Approximated user-perspective rendering in tablet-based

augmented reality. In 2013 IEEE International Symposium on Mixed and Augmented Reality (ISMAR). IEEE, 21–28.

[322] Michael Tsang, George W Fitzmaurice, Gordon Kurtenbach, Azam Khan, and Bill Buxton. 2002. Boom chameleon:
simultaneous capture of 3D viewpoint, voice and gesture annotations on a spatially-aware display. In Proceedings of
the 15th annual ACM symposium on User interface software and technology. 111–120.

[323] Ying-Chao Tung, Ta Yang Cheng, Neng-Hao Yu, Chiuan Wang, and Mike Y Chen. 2015. FlickBoard: Enabling trackpad
interaction with automatic mode switching on a capacitive-sensing keyboard. In Proceedings of the 33rd Annual ACM
Conference on Human Factors in Computing Systems. ACM, 1847–1850.

[324] Matthew Turk. 2014. Multimodal interaction: A review. Pattern Recognition Letters 36 (2014), 189–195.
[325] Brygg Ullmer and Hiroshi Ishii. 1997. The metaDESK: models and prototypes for tangible user interfaces. In Proceedings

of Symposium on User Interface Software and Technology (UIST 97), ACM.

[326] John Underkoffler and Hiroshi Ishii. 1998. Illuminating light: an optical design tool with a luminous-tangible interface.

In Proceedings of the SIGCHI conference on Human factors in computing systems. 542–549.

[327] John Underkoffler and Hiroshi Ishii. 1999. Urp: a luminous-tangible workbench for urban planning and design. In

Proceedings of the SIGCHI conference on Human Factors in Computing Systems. 386–393.

[328] Nicolas Villar, Shahram Izadi, Dan Rosenfeld, Hrvoje Benko, John Helmes, Jonathan Westhues, Steve Hodges, Eyal
Ofek, Alex Butler, Xiang Cao, et al. 2009. Mouse 2.0: multi-touch meets the mouse. In Proceedings of the 22nd annual
ACM symposium on User interface software and technology. 33–42.

[329] Sarah Theres Völkel, Christina Schneegass, Malin Eiband, and Daniel Buschek. 2020. What is" intelligent" in intelligent
user interfaces? a meta-analysis of 25 years of IUI. In Proceedings of the 25th International Conference on Intelligent
User Interfaces. 477–487.

Mixed Reality Interaction Techniques

35

[330] Julie Wagner, Mathieu Nancel, Sean G Gustafson, Stephane Huot, and Wendy E Mackay. 2013. Body-centric design
space for multi-surface interaction. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems.
ACM, 1299–1308.

[331] Manuela Waldner, Ernst Kruijff, and Dieter Schmalstieg. 2010. Bridging gaps with pointer warping in multi-display
environments. In Proceedings of the 6th Nordic Conference on Human-Computer Interaction: Extending Boundaries.
813–816.

[332] Saiwen Wang, Jie Song, Jaime Lien, Ivan Poupyrev, and Otmar Hilliges. 2016. Interacting with soli: Exploring fine-
grained dynamic gesture recognition in the radio-frequency spectrum. In Proceedings of the 29th Annual Symposium
on User Interface Software and Technology. 851–860.

[333] Xiyao Wang, Lonni Besançon, David Rousseau, Mickael Sereno, Mehdi Ammi, and Tobias Isenberg. 2020. Towards an
Understanding of Augmented Reality Extensions for Existing 3D Data Analysis Tools. In ACM Conference on Human
Factors in Computing Systems.

[334] Yuntao Wang, Zichao Chen, Hanchuan Li, Zhengyi Cao, Huiyi Luo, Tengxiang Zhang, Ke Ou, John Raiti, Chun Yu,
Shwetak Patel, et al. 2020. MoveVR: Enabling Multiform Force Feedback in Virtual Reality using Household Cleaning
Robot. In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems. 1–12.

[335] Colin Ware and Kathy Lowther. 1997. Selection using a one-eyed cursor in a fish tank VR environment. ACM

Transactions on Computer-Human Interaction (TOCHI) 4, 4 (1997), 309–322.

[336] Kent Watsen, Rudolph Darken, and Michael Capps. 1999. A handheld computer as an interaction device to a virtual

environment. In Proceedings of the third immersive projection technology workshop.

[337] Mark Weiser. 1999. The computer for the 21st century. ACM SIGMOBILE mobile computing and communications

review 3, 3 (1999), 3–11.

[338] Pierre Wellner. 1991. The DigitalDesk calculator: tangible manipulation on a desk top display. In Proceedings of the

4th annual ACM symposium on User interface software and technology. 27–33.

[339] Pierre Wellner. 1993. Interacting with paper on the DigitalDesk. Commun. ACM 36, 7 (1993), 87–96.
[340] Dirk Wenig, Johannes Schöning, Alex Olwal, Mathias Oben, and Rainer Malaka. 2017. WatchThru: Expanding
Smartwatch Displays with Mid-air Visuals and Wrist-worn Augmented Reality. In Proceedings of the 2017 CHI
Conference on Human Factors in Computing Systems. 716–721.

[341] Eleanora P Westebring-van der Putten, Richard HM Goossens, Jack J Jakimowicz, and Jenny Dankelman. 2008. Haptics
in minimally invasive surgery–a review. Minimally Invasive Therapy & Allied Technologies 17, 1 (2008), 3–16.
[342] Daniel Wigdor, Clifton Forlines, Patrick Baudisch, John Barnwell, and Chia Shen. 2007. Lucid Touch: A See-through
Mobile Device. In Proc. UIST ’07 (Newport, Rhode Island, USA) (UIST ’07). ACM, New York, NY, USA, 269–278.
https://doi.org/10.1145/1294211.1294259

[343] Andrew Wilson, Hrvoje Benko, Shahram Izadi, and Otmar Hilliges. 2012. Steerable augmented reality with the

beamatron. In Proceedings of the 25th annual ACM symposium on User interface software and technology. 413–422.

[344] Andrew D Wilson. 2006. Robust computer vision-based detection of pinching for one and two-handed gesture input.
In Proceedings of the 19th annual ACM symposium on User interface software and technology. ACM, 255–258.
[345] Dennis Wolf, John J Dudley, and Per Ola Kristensson. 2018. Performance envelopes of in-air direct and smartwatch
indirect control for head-mounted augmented reality. In 2018 IEEE Conference on Virtual Reality and 3D User Interfaces
(VR). IEEE, 347–354.

[346] Yi-Chin Wu, Liwei Chan, and Wen-Chieh Lin. 2019. Tangible and Visible 3D Object Reconstruction in Augmented

Reality. In 2019 IEEE International Symposium on Mixed and Augmented Reality (ISMAR). IEEE, 26–36.

[347] PingJun Xia, António M Lopes, and Maria Teresa Restivo. 2013. A review of virtual reality and haptics for product

assembly (part 1): rigid parts. Assembly Automation (2013).

[348] Robert Xiao, Chris Harrison, and Scott E Hudson. 2013. WorldKit: rapid and easy creation of ad-hoc interactive
applications on everyday surfaces. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems.
879–888.

[349] Robert Xiao, Greg Lew, James Marsanico, Divya Hariharan, Scott Hudson, and Chris Harrison. 2014. Toffee: enabling

ad hoc, around-device interaction with acoustic time-of-arrival correlation. In Proc. MobileHCI ’14. ACM, 67–76.

[350] Robert Xiao, Julia Schwarz, Nick Throm, Andrew D Wilson, and Hrvoje Benko. 2018. MRTouch: adding touch input

to head-mounted mixed reality. IEEE transactions on visualization and computer graphics 24, 4 (2018), 1653–1660.

[351] Min Xin, Ehud Sharlin, and Mario Costa Sousa. 2008. Napkin sketch: handheld mixed reality 3D sketching. In

Proceedings of the 2008 ACM symposium on Virtual reality software and technology. 223–226.

[352] Xing-Dong Yang, Tovi Grossman, Daniel Wigdor, and George Fitzmaurice. 2012. Magic finger: always-available input
through finger instrumentation. In Proceedings of the 25th annual ACM symposium on User interface software and
technology. 147–156.

[353] Xing-Dong Yang, Khalad Hasan, Neil Bruce, and Pourang Irani. 2013. Surround-see: enabling peripheral vision on
smartphones during active use. In Proceedings of the 26th annual ACM symposium on User interface software and

36

Grubert

technology. ACM, 291–300.

[354] Brandon Yee, Yuan Ning, and Hod Lipson. 2009. Augmented reality in-situ 3D sketching of physical objects. In

Intelligent UI workshop on sketch recognition, Vol. 1. Citeseer.

[355] Nick Yee, Jeremy N Bailenson, and Kathryn Rickertsen. 2007. A meta-analysis of the impact of the inclusion and
realism of human-like faces on user experiences in interfaces. In Proceedings of the SIGCHI conference on Human
factors in computing systems. 1–10.

[356] Cik Suhaimi Yusof, Huidong Bai, Mark Billinghurst, and Mohd Shahrizal Sunar. 2016. A review of 3D gesture

interaction for handheld augmented reality. Jurnal Teknologi 78, 2-2 (2016).

[357] Wolfgang L Zagler, Christian Beck, and Gottfried Seisenbacher. 2003. FASTY-faster and easier text generation for

disabled people. na.

[358] Robert C Zeleznik, Andrew S Forsberg, and Jürgen P Schulze. 2005. Look-that-there: Exploiting gaze in virtual reality

interactions. Technical report, Technical Report CS-05 (2005).

[359] Haimo Zhang and Yang Li. 2014. GestKeyboard: enabling gesture-based interaction on ordinary physical keyboard.

In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. ACM, 1675–1684.

[360] Yang Zhang, Chouchang Yang, Scott E Hudson, Chris Harrison, and Alanson Sample. 2018. Wall++ Room-Scale
Interactive and Context-Aware Sensing. In Proceedings of the 2018 CHI Conference on Human Factors in Computing
Systems. 1–15.

[361] Chen Zhao, Ke-Yu Chen, Md Tanvir Islam Aumi, Shwetak Patel, and Matthew S. Reynolds. 2014. SideSwipe: Detecting
In-air Gestures Around Mobile Devices Using Actual GSM Signal. In Proc. UIST ’14 (Honolulu, Hawaii, USA). ACM,
New York, NY, USA, 527–534. https://doi.org/10.1145/2642918.2647380

[362] Jingjie Zheng, Blaine Lewis, Jeff Avery, and Daniel Vogel. 2018. Fingerarc and fingerchord: Supporting novice to
expert transitions with guided finger-aware shortcuts. In Proceedings of the 31st Annual ACM Symposium on User
Interface Software and Technology. 347–363.

[363] Jingjie Zheng and Daniel Vogel. 2016. Finger-aware shortcuts. In Proceedings of the 2016 CHI Conference on Human

Factors in Computing Systems. 4274–4285.

[364] Fengyuan Zhu and Tovi Grossman. 2020. BISHARE: Exploring Bidirectional Interactions Between Smartphones and
Head-Mounted Augmented Reality. In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems.
ACM.

[365] Michael Zollhöfer, Justus Thies, Pablo Garrido, Derek Bradley, Thabo Beeler, Patrick Pérez, Marc Stamminger, Matthias
Nießner, and Christian Theobalt. 2018. State of the art on monocular 3D face reconstruction, tracking, and applications.
In Computer Graphics Forum, Vol. 37. Wiley Online Library, 523–550.

