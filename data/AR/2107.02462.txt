FloorLevel-Net: Recognizing Floor-Level Lines
with Height-Attention-Guided Multi-task Learning

Mengyang Wu, Wei Zeng, Member, IEEE and Chi-Wing Fu, Member, IEEE

1

1
2
0
2

l
u
J

6

]

V
C
.
s
c
[

1
v
2
6
4
2
0
.
7
0
1
2
:
v
i
X
r
a

Abstract—The ability to recognize the position and order of the
ﬂoor-level lines that divide adjacent building ﬂoors can beneﬁt
many applications, for example, urban augmented reality (AR).
This work tackles the problem of locating ﬂoor-level lines in
street-view images, using a supervised deep learning approach.
Unfortunately, very little data is available for training such a
network − current street-view datasets contain either semantic
annotations that lack geometric attributes, or rectiﬁed facades
without perspective priors. To address this issue, we ﬁrst compile
a new dataset and develop a new data augmentation scheme to
synthesize training samples by harassing (i) the rich semantics of
existing rectiﬁed facades and (ii) perspective priors of buildings
in diverse street views. Next, we design FloorLevel-Net, a multi-
task learning network that associates explicit features of building
facades and implicit ﬂoor-level lines, along with a height-attention
mechanism to help enforce a vertical ordering of ﬂoor-level lines.
The generated segmentations are then passed to a second-stage
geometry post-processing to exploit self-constrained geometric
priors for plausible and consistent reconstruction of ﬂoor-level
lines. Quantitative and qualitative evaluations conducted on
assorted facades in existing datasets and street views from Google
demonstrate the effectiveness of our approach. Also, we present
context-aware image overlay results and show the potentials
of our approach in enriching AR-related applications. Project
website: https://wumengyangok.github.io/Project/FloorLevelNet.

Index Terms—Multi-task learning, attention mechanism, se-

mantic segmentation, street view, augmented reality

I. INTRODUCTION

F LOOR-LEVEL lines are line segments that separate ad-

jacent ﬂoors on a building facade; see Fig. 1 (middle).
Being able to recognize them in city-wide street views can
beneﬁt various applications, e.g., urban 3D reconstruction [29],
building topology analysis [42], and urban vitality study [48].
Intrinsically, ﬂoor-level lines for the same building are parallel,
through which we can reconstruct the homography of the
facade in a perspective view [10] and support ﬂoor-aware
augmented reality (AR) applications; see Fig. 1 (right).

This work considers the problem of inferring ﬂoor-level
lines in street-view images, which requires the recovery of not
only geometric priors (e.g., positions and vanishing directions)
in the image view, but also semantic information (e.g., ﬂoor
orders) relevant to the ﬂoor-level lines. The task is relatively

M. Wu, and C.-W. Fu are with the Chinese University of Hong Kong.

e-mail: {mywu, cwfu}@cse.cuhk.edu.hk.

W. Zeng is with Shenzhen Institute of Advanced Technology, Chinese

Academy of Sciences. e-mail: wei.zeng@siat.ac.cn.

W. Zeng and C.-W. Fu are the corresponding authors.
This paper has

supplementary downloadable material available at
http://ieeexplore.ieee.org., provided by the author. The material includes ad-
ditional results and comparisons. Contact mywu@cse.cuhk.edu.hk for further
questions about this work.

Fig. 1. Left column shows two example street-view images in London (top)
and Hong Kong (bottom), where the camera views are side- and front-facing
relative to the building, respectively. Note the occlusions introduced by the
advertisement billboard and light post circled in red on bottom left. Middle
column shows ﬂoor-level lines recognized by our method with geometric
positions and semantic order labels. Right column shows potential ﬂoor-aware
image-overlay results to aid shopping (top) and navigation (bottom).

intuitive for humans but very challenging for computers. So
far, we are not aware of any work that can robustly detect and
recognize ﬂoor-level lines.

Related prior works on 3D reconstruction (e.g., [4], [15]),
and facade parsing (e.g., [27], [44]), typically rely on various
assumptions about the structural regularity on building fa-
cades, e.g., repetitive windows and balconies (e.g., [31], [41]).
Though these extra constraints help disambiguate the problem
and the results might help infer ﬂoor-level lines, they still have
various limitations, such as being error-prone when handling
perspective-oriented facades (Fig. 1 top-left) and scenes with
occlusions (Fig. 1 bottom-left). Alternatively, we may infer
ﬂoor-level lines by locating line segments roughly in the same
direction. However, existing line detectors (e.g., [14], [34])
can easily generate huge amount of irrelevant line segments
in cluttered street views, and offer no semantic information for
inferring the ﬂoor order. Recently, Lee et al. [22] proposed the
concept of semantic lines that characterize the spatial scene
structure in images. The method ﬁrst ﬁnds candidate lines
using line features at multi-scale pooling layers, then ﬁlters out
semantic lines using local line features. However, the ﬁltering
process (semantic or not) is a binary classiﬁcation problem,
whilst this work requires to recover not only a multi-class label
per line but also a plausible ﬂoor order for lines in the same
facade, i.e., order 1, order 2, etc.

Recent attempts of using deep neural networks for semantic
scene segmentation (e.g., [8]) and planar surface recognition
(e.g., [45], [49]) exploit the possibility of jointly learning
the semantic and geometric attributes in street views. The
methods achieve superior performances over previous deep

11332242314 
 
 
 
 
 
learning methods that use solely the semantic features. This
work also leverages a supervised deep learning approach that
can jointly infer geometric priors and semantic information of
ﬂoor-level lines. This is nevertheless a challenging task. First,
a key requirement for network training is the availability of
large, labeled street-view images with annotations of ﬂoor-
lines. However, existing street-view datasets contain
level
either semantic annotations that lack geometric attributes, or
rectiﬁed facades without perspective priors. To address this
issue, we devise a new data augmentation scheme and compile
a new dataset by effectively combining the rich context of
ﬂoor-level lines marked on an existing facade dataset, with
perspective priors easily-extracted on street-view images. In
this way, we can largely reduce the manual workload in the
dataset construction, while promoting the generalizability of
our network model.

Second, we are not aware of any network architecture
that can fulﬁll the requirement of recognizing and locating
ﬂoor-level lines in street-view images. To ﬁll the gap, we
design FloorLevel-Net − a new deep learning framework
with two main components inspired by the characteristics
of our ﬂoor-level dataset. (i) FloorLevel-Net leverages multi-
task learning that associates geometric properties of facades
orientation to camera, with semantic features of facade ap-
pearance, including windows, doors, and balconies, etc. (ii)
FloorLevel-Net incorporates a height-attention mechanism to
enforce vertical orderings of ﬂoor-level lines, as ﬂoor order
naturally increases from bottom to top in image space. We
further exploit various geometric properties,
including the
vanishing point (VP) and ﬂoor order constraints, to enforce
a consistent reconstruction of ﬂoor-level lines from piecewise
segmentations by FloorLevel-Net.

We evaluate the performance of our approach on both build-
ing images from existing facade datasets and self-collected
street-view images from Google Street View (GSV) [3]. Both
qualitative and quantitative analysis demonstrate the effective-
ness of the individual components in our approach. The main
contributions of this work include:

• We propose a data augmentation scheme to integrate
rich semantics of rectiﬁed facades and geometric priors
of buildings in diverse street views, and compile a new
dataset for recognizing ﬂoor-level lines (Sec. IV).

• We design FloorLevel-Net−a multi-task CNN architec-
ture with height attention to simultaneously predict a
facade segmentation map and a ﬂoor-level distribution
map for an input street-view image (Sec. V-A to V-C).
We further develop a post-processing framework to ex-
tract ﬂoor-level parameters (including the endpoints and
vanishing points), and reﬁne the parameters with self-
constrained geometric priors (Sec. V-D).

• We evaluate the effectiveness of our approach in recog-
nizing ﬂoor-level lines, and demonstrate the applicability
in enriching context-aware urban AR (Sec. VI).

II. RELATED WORK

The recognition of geometric structures in urban scenes,
e.g., surface layout [18] and driving lanes [50], has been

2

then split

gaining attention in image processing and vision research. This
work targets at ﬂoor-level lines that separate adjacent ﬂoors
on building facades. A closely-related topic is to reconstruct
3D building models from a monocular image. Conventional
approaches typically adopt a two-stage approach: ﬁrst parse
a facade into piecewise regions like windows, balconies,
the regions into regular layouts
etc. [27], [44],
for subsequent modeling, e.g., by shape grammars [28], [35].
Existing methods, however, rely heavily on detecting repeti-
tion [41], [31], symmetric or rectangular structures on facade
layout [36], [39], thus exhibiting signiﬁcant challenges for
parsing general urban scenes in the wild. Particular obstacles
include perspective-oriented facades (e.g., see Fig. 1 (top) and
the red inset in Fig. 2) and scene occlusions by billboards
and trafﬁc lights (e.g., see Fig. 1 (bottom) and the blue
inset
in Fig. 2), resulting in failures in matching repeti-
tion/symmetry/rectangular constraints. It is hard to identify
common geometric assumptions that can well ﬁt diverse styles
of building facades in different cities.

Recent studies on urban scene recognition focus on deep-
learning-based approaches, beneﬁting from new datasets of
urban scenes and advancements in neural network architec-
ture, e.g., [26], [51], [6]. Speciﬁcally, DeepFacade [24], [23]
parses facades using a fully convolutional network to produce
pixel-wise semantics. Some work also attempts to formulate
geometric understanding of urban scenes as an image segmen-
tation problem. For instance, Haines and Calway [16] infer
surface orientations using spatiograms of gradients and colors.
However, the results are piecewise segments that are typically
discontinuous with coarse and irregular boundaries, hindering
the recognition of geometric priors like the endpoints of ﬂoor-
level lines. To further recover geometric priors, a second-stage
post-processing framework can be employed. Zeng et al. [49]
employ a vanishing-point-constrained optimization to enhance
piecewise segmentations of planar building facades.

Similarly, we adopt a twofold process to recognize ﬂoor-
level lines on building facades. First, we predict a segmen-
tation mask of pixel-wise line labels (ﬂoor-level distribution)
using a multi-task CNN architecture. Second, we reﬁne the
piecewise ﬂoor-level distribution into line parameters using
self-constrained geometric priors. Nevertheless, the work faces
severe challenges of being lack of training dataset and suit-
able network architecture. We tackle the challenges from the
following perspectives:

• Data augmentation is a vital technique to improve the
diversity of the training data and the network generaliz-
ability. There are several publicly available datasets for
urban scene (e.g., KITTI [13] and Cityscapes [9]) and
facade (e.g., CMP [37] and LabelMeFacade [12]). Yet,
we cannot use them directly in this work. In contrast
to explicit building structures such as windows and bal-
conies that have obvious visual appearance, ﬂoor-level
lines are rather implicit, requiring contextual informa-
tion for the recognition. To ﬁll the gap, we propose a
new data augmentation scheme that leverages detailed
facade semantics of front-facing facades in CMP [37]
and diverse building perspectives in GSV images. In this

way, the synthesized dataset efﬁciently captures both the
rich context of ﬂoor-level lines and geometric priors of
building facades in reality.

• Multi-task learning helps to boost the performance of
many deep-learning-based vision tasks, by learning fea-
tures from relevant knowledge domains. In the literature,
various efforts have been devoted to exploit multi-task
learning for geometric understanding of urban scenes.
For example, Liu et al. [25] tackle scene recognition
and reconstruction tasks in a tightly-coupled framework;
besides, there are assorted works for indoor scene un-
derstanding (e.g., [38], [30], [11]) by fusing features of
depth, surface normals, and objects. Human experience
on locating ﬂoor-level lines typically relies on recogniz-
ing the facades and its enclosing regions, e.g., doors and
windows. Therefore, we divide our task into subtasks of
recognizing facades and ﬂoor-level lines, and design a
multi-task learning network to simultaneously address the
two subtasks together.

• Channel-wise attention exploits the inter-channel rela-
tionship of features and scales the feature map according
to the importance of each channel. The mechanism is ﬁrst
proposed in SENet [19] and adopted widely in image
classiﬁcation (e.g., [40]) and segmentation (e.g., [46]).
Recent attention approaches take more advantages of
contextual information inside the image domains, e.g.,
the criss-cross attention module by Huang et al. [20]
and recursive context routing (ReCoR) mechanism by
Chen et al. [7]. Choi et al. [8] utilize the unbalanced
class distribution at varying vertical locations in urban
scene images, and design a height-attention module to
emphasize the unbalance. Floor-level lines in our dataset
exhibit a similar property, since ﬂoor order on the same
facade always increases from bottom to top in the image
space. This drives us to adopt a similar conﬁguration in
the recognition of ﬂoor-level lines.

III. OVERVIEW

In this work, our goal is

to recognize and locate the ﬂoor-level lines on each
building facade (that is close to the camera) in the
image space of street-view images.

e, yi

The input to our method is a single RGB image I ∈ RW ×H×3
of width W and height H. Given I, we aim to predict a set
of line segments L = {L1, ..., Ln} per facade that marks the
separations between adjacent ﬂoors; see Fig. 1 (middle) for
two examples. In the following, we refer line segments as lines
for simplicity. Each line Li contains a pair of endpoints (xi
s, yi
s)
and (xi
e) in the image space of I. Also, we aim to recognize
a ﬂoor order (denoted as li ∈ {1, 2, 3, ...}) per line Li, such that
the line that separates the ground ﬂoor and the ﬁrst ﬂoor has
a ﬂoor order value of one, the next line above has a ﬂoor
order value of two, etc. So, each Li is speciﬁed as a 5-tuple
{xi
e, li}. As a visualization, we use the same coloring
scheme (orange for order 1, green for order 2, etc.) to reveal
the ﬂoor orders in all our results.

e, yi

s, xi

s, yi

3

There are three main parts in our approach.
(i) We compile a set of facade images from the CMP
dataset [37], and employ them to augment building
facades in GSV images based on the geometric prior
of facade perspective (Sec. IV). By our new data aug-
mentation scheme, we can efﬁciently generate a large
amount of image samples to train our network.

(ii) We develop a multi-task learning network−FloorLevel-
Net (Sec. V-A to V-C), to segment the input image into
piecewise regions of facades and to detect candidate
lines; see
pixels possibly associated with ﬂoor-level
the multi-task learning module in Fig. 2. Particularly,
FloorLevel-Net encapsulates semantic (e.g., windows,
doors, shops, etc.) and geometric (e.g., ﬂoor orders,
facade orientation, etc.) information related to ﬂoor-
level lines by articulating a fused loss function that is
differentiable by means of pixel-wise convolutions.
(iii) We infer per-line 5-tuple parameters from the piecewise
segmentations in the geometry post-processing stage
(Sec. V-D). Here, we locate facade regions, group ﬂoor-
level lines per facade, then regress a polyline per ﬂoor-
level line; see the geometry processing module in Fig. 2.
Further, we reﬁne the polylines based on self-constrained
geometric priors, i.e., the facade boundary, ﬂoor order,
and vanishing points.

So far, we are not aware of any work on recognizing
ﬂoor-level lines in street-view images. Existing related works
focus on recognizing building facades as a whole or explicit
objects, such as windows, doors, and balconies. Hence, we
show the effectiveness of our approach through comparisons
with ablation techniques (Sec. VI-C) and DeepFacade, which
is a closely-related work on facade parsing (Sec. VI-D), and
demonstrate the potential of our work to support and enrich
urban AR applications (Sec. VI-F).

IV. DATASET PREPARATION & ANALYSIS

An immediate challenge to recognizing ﬂoor-level lines is
the lack of properly-annotated street-view images. On the one
hand, existing city-wide datasets such as Cityscapes [9] pro-
vide mainly urban scene segmentations, e.g., roads, buildings,
vehicles, etc., while lacking details of facades, not to mention
ﬂoor-level lines. On the other hand, building facade datasets
such as CMP [37] provide detailed semantics of facades, e.g.,
windows, doors, balcony, etc. However, the images exhibit
mostly rectiﬁed views of front-facing facades, which cannot
reﬂect real-world facades with perspective orientations.

A. Data Augmentation

To relax the enormous workload for labeling a new dataset,
we propose a data augmentation scheme (Fig. 3) to take
the best advantages of the facade semantics in the CMP
dataset [37] together with the perspective-oriented building
facades extracted from GSV images.

To start, we ﬁrst analyze the facade images (Fig. 3 (a1))
with their semantics (Fig. 3 (a2)) provided by the CMP
dataset. Here, we make use of the simpliﬁed semantics (Fig. 3
(a3)), i.e., window, shop, and door, which offer contextual

4

Fig. 2. Our two-stage approach: (i) FloorLevel-Net is a multi-task learning network that segments the input image into building-facade-wise semantic regions
(top) and ﬂoor-level distributions (bottom); and (ii) our method further ﬁts and reﬁnes the pixel-wise network outputs into polylines with geometric parameters.
Further, we can take the reconstructed ﬂoor-level lines to support and enrich urban AR applications with ﬂoor-aware image overlay.

Fig. 3.
Illustration of our data augmentation scheme. We take the advantages of the rich context of facades in the CMP dataset [37] (a1 & a2) and the
perspective-oriented building facades readily extracted from the GSV (Google Street View) images (b1). From them, we can efﬁciently obtain simpliﬁed
semantics (a3) and annotate ﬂoor-level lines (a4), and further generate a very large amount of augmented image samples, i.e., an augmented image (b2) with
its associated semantic image (b3) and ﬂoor-level-lines image (b4), by pairing up different geometric priors with different CMP facades.

information for ﬂoor-level lines, i.e., windows usually appear
on every ﬂoor, whereas shops and doors usually appear on the
ground ﬂoor. There may not be obvious lines between adjacent
ﬂoors, yet humans can infer them based on the associated
contexts, e.g., windows. Thanks to the regular front-facing
structures in these inputs, manually labeling ﬂoor-level lines
(Fig. 3 (a4)) is very fast, compared with labeling on general
street-view images that are perspective. We extract a total of
150 rectiﬁed facade images from the CMP dataset.

Next, we employ the GSV API [1] to collect street-view
photos. Here, we randomize the camera headings to obtain
perspective-oriented facades, and set the pitch parameter in the
API to zero, so the camera view directions are always horizon-
tal, i.e., parallel to the ground. Also, for each GSV photo, we
identify nearby planar facades that are feasible for overlaying
CMP facade images, i.e., with an aspect ratio similar to those

in the CMP dataset. Then, we manually annotate a quad to
mark the region of each feasible planar facades (Fig. 3 (b1))
and label also its orientation {le f t, right, f ront} relative to the
camera view direction. The facade priors serve as reference
locations for overlaying facade images and semantics. For
instance, see Fig. 3 (b1), the blue quad marks the facade region
of the left building with orientation categorized as front (i.e.,
front facing the camera), whereas the green quad marks the
facade region of the right building with right orientation. By
doing so, we can derive an afﬁne transformation matrix for
each facade, and use it to generate an augmented street-view
image (Fig. 3 (b2)) with its associated semantic image (Fig. 3
(b3)) and labeled ﬂoor-level-lines image (Fig. 3 (b4)).

To enrich the data sample diversity and to improve the
network generalizability, we collect 200 GSV images in Hong
Kong and London, with both high-rise and relatively low-rise

(a1) Raw CMP image(b1) Geometric priors from GSV(b2) Synthetic image(a2) Raw CMP semantics(a3) Simplified semantics(a4) Floor-level lines(b3) Synthetic semantics(b4) Synthetic floor-level linesGeometric priors (b1)Geometric priors (b1)5

V. FLOORLEVEL-NET APPROACH

In this

section, we ﬁrst present

the framework of
FloorLevel-Net (see Fig. 5), which is a multi-task learning
network (Sec. V-A) that fully utilizes the two-stream seman-
tics of facades and ﬂoor-level lines, with a height-attention
mechanism (Sec. V-B) to enforce the vertical ordering of ﬂoor-
level lines. Then, we present the implementation details of
FloorLevel-Net (Sec. V-C), followed by the add-on geometry
post-processing module to generate the ﬁnal parameters for
each ﬂoor-level line (Sec. V-D).

Fig. 4. Characteristics of the data samples in our dataset: distributions of (a)
the facade orientation, (b) the highest ﬂoor order, and (c) average number of
pixels of each ﬂoor order in a single image.

buildings, respectively. Together with the facade images from
the CMP dataset, we compile a new dataset with 3,000 pairs
of augmented street-view images, each associated with facade
semantics and ﬂoor-level line labels. We train our FloorLevel-
Net using the dataset, and compare it with an ablation model
trained on the CMP dataset of only rectiﬁed facades. The
results show a signiﬁcant boost by the data augmentation
scheme (Sec. VI-C).

B. Dataset Characterization

We conduct a preliminary analysis of our augmented dataset
to show its strong generalizability, and later take it to derive
feasible components in our network. By observing the follow-
ing characteristics of the dataset, some hyper parameters, e.g.,
maximum ﬂoor order, are determined.

• Facade orientation: Fig. 4 (a) reports the distribution of
facade orientations. As mentioned in the previous subsec-
tion, three orientations {le f t, right, f ront} are considered
based on the facade orientation relative to the camera.
The three orientations share similar proportions, without
biasing towards any of the three.

• Highest ﬂoor order: Fig. 4 (b) plots the distribution of
the highest ﬂoor order in the data samples. Most facades
have a ﬂoor order of ﬁve. Also,
there is no single-
ﬂoor building, since we omit single-ﬂoor buildings when
compiling the data. There are few high-rise buildings
(< 5%) with the highest ﬂoor order above 10.

• Average pixel number per ﬂoor order: Fig. 4 (c) presents
the average number of pixels belonging to each ﬂoor
order in a single image of size 480×360. Since the street-
view images are taken from the ground, low ﬂoor orders
have more pixels, and the number of pixels decreases as
ﬂoor order increases. Floor-level lines of orders above 10
are too small to overlay AR images.

In consideration of the distribution of highest ﬂoor order and
average pixel number per ﬂoor order in the augmented dataset,
we set the maximum ﬂoor order to be 10 in our ﬂoor-level line
detection module (Sec. V-A).

A. Multi-task Learning

Based on the establishment of our augmented dataset, each
street-view image I ∈ RW ×H×3 for training is coupled with
two label classes: (i) facade semantics Ψ f a and (ii) ﬂoor-level
distributions Ψ f l. Ψ f a consists of {window, door, shop, left,
right, front, other}, where the ﬁrst three labels are the context,
the subsequent three indicate the facade orientation, and other
marks the non-facade pixels. Ψ f l consists of {l1, · · · , ln, other},
where n denotes the highest ﬂoor order (i.e., 10) as suggested
by the data characteristics (Sec. IV-B), and other marks the
non-ﬂoor-level-lines pixels. The two-stream semantics com-
plement each other for the goal: Ψ f a provides rich context
for separating ﬂoor-level lines in each facade, whereas Ψ f l
helps to estimate the order and position of each ﬂoor-level
line. This observation on our training data inspires us to apply
a multi-task learning process to beneﬁt each task.

As illustrated in Fig. 5, FloorLevel-Net ﬁrst employs a
shared encoder to learn a feature map, then it applies two-
branch decoders to gradually upsample the feature map via
deconvolutional
layers. The facade feature decoder in the
upper branch outputs segmentation mask ¯M f a ∈ RW ×H , such
¯M f a(ψ f a|x) indicates the probability of pixel x having
that
label ψ f a ∈ Ψ f a.

¯M f a is computed by

¯M f a(ψ f a|x) = so f tmax(Wf aF(x)),

(1)

where F(x) is the feature vector of pixel x and Wf a denotes the
parameters learned by the network. We use the loss function
L f a = Ce( ¯M f a,M f a) to supervise the network training, where
M f a is the ground truth map of facades and Ce(·) is a standard
softmax cross entropy function.

On the other hand, the line feature decoder in the lower
branch of FloorLevel-Net outputs ﬂoor-level distribution map
¯M f l ∈ RW ×H for ﬂoor-level line detection. Besides the soft-
max cross entropy Ce( ¯M f l,M f l), where M f l denotes the
associated ground truth, we also fuse L f a here in the loss
function for supervising the network training:

L f l = Ce( ¯M f a,M f a) + Ce( ¯M f l,M f l).

(2)

Incorporating Ce( ¯M f a,M f a) in L f l contributes to the ﬂoor-
level lines predictions, since facade context such as windows
can help infer the ﬂoor levels. Experimental results also
conﬁrm that the fused loss helps to predict more continuous
segmentations and more accurate ﬂoor orders (Sec. VI-C).

10%20%30%LeftCentralRight2K4K6K8K10K12K(a) Distribution of facade orientation123456789>1010(b) Distribution of highest floor order12345678910>10(c) Average number of pixels of each floor order in a single image6

Fig. 5. Overview of our FloorLevel-Net framework. We consume each input street-view image by a multi-task learning network to jointly segment the
facades and detect ﬂoor-level lines. In the module of our line feature decoder, we incorporate height-attention layers (see bottom left) along with the original
convolutional layers, to help enforce the vertical ordering of the ﬂoor-level lines, where “multi” denotes element-wise multiplication.

TABLE I
PROBABILITY DISTRIBUTIONS OF PIXELS (IN PERCENTAGE) OF THE
LOWEST SIX FLOOR ORDERS IN THE WHOLE IMAGE AND IN EACH
VERTICAL BOUND.

Given

Probabilities of the lowest six ﬂoor orders
p6
p1

p5

p4

p3

p2

Class
Entropy

Fig. 6. Height-related feature: Pixel distribution of ﬂoor-level lines in vertical
bounds of the image space.

B. Height-Attention Mechanism

Image

9.75

11.8

9.56

4.90

2.83

1.17

0.436

Low
Mid-low
Mid-high
High

27.7
6.75
2.79
1.80

10.1
20.1
10.3
6.66

1.71
8.72
14.8
13.0

0.35
2.97
7.07
9.20

0.12
1.09
3.86
6.22

0.03
0.32
1.20
3.09

0.298
0.386
0.429
0.442

0.389
(avg)

Observing that “cars can’t ﬂy in the sky”, i.e., pixels of
car are generally below those of the sky, Choi et al. [8]
suggest a height-attention mechanism based on the analysis
of the CityScape [9] dataset, which indicates that the class
distribution signiﬁcantly depends on a vertical position in
urban scene. For instance, a lower part of an image is mainly
composed of road, while the upper part, buildings and sky
are principal objects. We hypothesis a similar pattern for pixel
vertical distribution of ﬂoor-level lines in our case, i.e., pixels
of order i ﬂoor-level lines are generally below those of order
j ﬂoor-level lines, given i < j. Here we simplify the analysis
by dividing the image space (from bottom to top) into four
equal-sized vertical bounds, i.e., low, mid-low, mid-high, and
high; see the inset in Fig. 6. For each ﬂoor-level line in our
data samples, we count its pixels in each vertical bound, and
compute its distribution ratios in the four bounds. Then, we
sum and normalize the distribution ratios for ﬂoor-level lines
of same order in the whole data, and obtain the distribution
ratios per ﬂoor-level line order. From Fig. 6, we can see that
pixels of low ﬂoor-level lines appear more in the lower vertical
bound (e.g., 73% of ﬂoor order 1 pixels are in the low bound),
whilst pixels of high ﬂoor-level lines have a higher chance of
locating in the upper vertical bound (e.g., 53% of ﬂoor order
6 pixels are in the high bound).

Next, we analyze the probabilities of ﬂoor-level line pixels
in the whole image and in each vertical bound. Table I presents
the probability distributions of the lowest six dominant ﬂoors
orders, which take over 97% of all ﬂoor orders (see Fig. 4
(c)). Here pi denotes the probability that an arbitrary pixel is
assigned to the i-th ﬂoor order. The results further conﬁrm the
hypothesis of the pixel vertical distribution. For example, the
probability of ﬂoor order 1 p1 is 9.75% in the whole image,
and it drops from 27.7% to 1.8% in vertical bounds from low
to high. On the contrary, p6 increases from 0.03% to 3.09%,
which matches with the distribution in Fig. 6. We further
measure the uncertainty of pixel distribution probabilities in
separate bound by calculating the entropies as − ∑ pilogpi. The
overall entropy of the entire images is 0.436, and the low
bound has the smallest entropy of 0.298 due to the dominant
probability of ﬂoor order 1. The result indicates that if a pixel
falls in the low bound, it shall probably be predicted as ﬂoor
order 1 but not the other ﬂoor orders.

These observations suggest that vertical position in image
space can serve as a good indicator of ﬂoor order. Hence, we
are motivated to leverage height-attention layers in the design
of FloorLevel-Net. In detail, we include an HA layer between
adjacent convolutional layers in the line feature decoder; see
Fig. 5 (bottom left) for the illustration.

Street-viewimage 𝐼BuildingfacadeRes(x)Sharedencoder+Res(x)1x1 conv𝐶)×𝐻)×𝑊)pool1d-conv2d-convmulti1x1 convFacadefeaturedecoder𝐶)×𝐻)𝐶-×𝐻-𝐶-×𝐻-×𝑊-interpolateLine featuredecoderHeight-attentionlayerℳ/0))ℳ10)-𝒜0)ℳ/0)-ℳ03Floor-levellinesℳ0)LowMid-lowMid-highHighOrder 1Order 2Order 3Order 4Order 5Order 610%20%30%70%60%50%40%7

for the ﬁnal prediction layers, where softmax is applied. We
implement FloorLevel-Net using PyTorch and train it from
scratch on a single NVidia GeForce GTX 1080 Ti GPU card.
During the training, we adopt the momentum optimizer with
learning rate 1e-3 and a small batch size of four. Convergence
is reached at about 100K iterations.

D. Geometry Post-Processing

FloorLevel-Net predicts two piecewise segmentation masks
¯M f a and ¯M f l, from which we further extract line parameters
by considering various geometric constraints:
(i) Facade constraint: Floor-level lines are attached to fa-
cades, so the detected lines are valid only if they lie inside
a facade region. Since ¯M f a may contain multiple facades,
we process ﬂoor-level line segmentations and generate
only one line with a speciﬁc ﬂoor order per facade.
(ii) Vanishing point (VP) constraint: Floor-level lines of the
in 3D, so they
same facade are intrinsically parallel
should meet at a common VP, which is at a ﬁnite location
for perspective-oriented facades.

(iii) Order constraint: Assuming that the up direction in input
image roughly matches the up direction in real world,
orders of ﬂoor-level lines for the same facade should
strictly increase from bottom to top.

To enforce these constraints, we formulate a second-stage
geometry post-processing to generate ﬂoor-level line parame-
ters with the following steps (as illustrated in Fig. 8):
(i) Line grouping (Fig. 8 (a)): First, we group relevant labels
¯M f a to form a super-segment per facade. Then we
in
group ﬂoor-level lines segmentation in ¯M f l, and identify
a group of lines per facade.

(ii) Polyﬁt (Fig. 8 (b)): We ﬁt a polyline (in the form of βi
βi
j)}n
1x) for pixels of each ﬂoor-level line (say {(xi
for i-th line) using a least squares regression:

j, yi

0 +
j=1

{

arg min
0,βi
βi
1

n
∑
j=1

(yi

j − (βi

0 + βi

1xi

j))2}.

(3)

Note that on the same facade, there could be disjoint
predicted regions of the same ﬂoor order; we have to
merge these regions before the regression.

(iii) Reﬁnement (Fig. 8 (c)): Polylines derived for the same
facade may not perfectly meet at a common VP, so we
regress location (xc, yc) for the common V Pc and reﬁne
each polyline as βi(x − xc) + yc by taking V Pc as an
anchor point. To do so, we formulate a gradient-based
optimization to update parameters {βi}m
i=1, xc, yc with a
global least squares:

arg min
{βi},xc,yc

{

m
∑
i=1

n
∑
j=1

(yi

j − βi(x − xc) − yc)2}.

(4)

(iv) Final result (Fig. 8 (d)): Last, we take the horizontal
and vertical ranges of each facade, to derive the two
endpoints (xi
e) for each polyline. Together
with the ﬂoor order li, we can then obtain the ﬁve-tuple
e, li} for each ﬂoor-level line.
{xi

s) and (xi

e, yi

e, yi

s, xi

s, yi

s, yi

Fig. 7. Left is an example input image with annotated ﬂoor orders. Middle
plots the ﬂoor-level line pixel distributions in the input image, whereas right
plots the attention weights in the last height-attention layer.

¯Ml

Speciﬁcally, the HA layer bridges a lower-level feature map
f l ∈ RCl ×Wl ×Hl and a higher-level feature
of ﬂoor-level lines
map ¯Mh
f l ∈ RCh×Wh×Hh (note: subscripts l and h denotes lower-
level and higher-level, respectively) in the following ways:
(i) adopts a width-wise pooling to extract vertical features
from ¯Ml
f l; (ii) employs a 1-D convolutional layer with a
bilinear interpolation to generate attention map A f l ∈ RCh×Hh,
¯Mh
f l; and
which shares the same channel and height size as
(iii) generates a reﬁned higher-level feature map ¯¯Mh
f l by an
element-wise multiplication of A f l and ¯Mh
f l. In doing so, A f l
enriches the per-channel scaling factors for each individual
row of vertical positions, and the reﬁned higher-level feature
map ¯¯Mh
f l embodies the height-wise contextual information for
locating and also ordering the ﬂoor-level lines.

To study the effectiveness of the HA mechanism, we extract
attention weights learned by the last HA layer and compare
them with ﬂoor-level distributions in street-view images. Fig. 7
shows a typical comparison example using the input image in
the left. Here, we plot two heatmaps with ﬂoor orders as the
horizontal axis and image-space height as the vertical axis. The
heatmaps are constructed in the following way. Horizontal axes
denote ﬂoor order from 1 to 10, whereas vertical axes denote
vertical position (height) sub-divided into 10 vertical bounds
for visualization. For each cell (m, n), we count the pixels
in ﬂoor-level m (middle) or extract the attention weight of
the m-th channel (right), inside the n-th vertical bound. Also,
we normalize the intensities (in red) in each map. Comparing
the two heatmaps, we can see that they exhibit very similar
patterns. Particularly, if we mark the most frequent height(s)
per ﬂoor order (see the blue boxes) in both plots, we can see
that the attention weights reveal the heights of the pixels in
ﬂoor-level lines of different orders. The comparison together
with the ablation analysis presented in Sec. VI-C demonstrate
the effectiveness of the HA mechanism.

C. Implementation of FloorLevel-Net

FloorLevel-Net adopts an encoder-decoder structure based
on DeepLabV3+ [6] with ResNet101 [17] as the backbone.
Speciﬁcally, we employ four ResNet stages and one atrous
spatial pyramid pooling (ASPP) layer to generate the shared
features, and use ﬁve convolutional layers in each of the de-
coders for facade segmentation and ﬂoor-level lines detection.
In the line feature decoder, we arrange one height-attention
layer between each pair of adjacent convolutional layer, and
append a residual layer at the end to fuse the features from
the facade feature decoder. We use ReLU for all layers, except

12345678910last height-attention layerheight12345678910street-view image8

Fig. 8. Geometry-constrained post-processing for retrieving line parameters: grouping ﬂoor-level lines based on facade segmentation (a), ﬁtting polylines
according to line predictions (b), reﬁning vanishing point using geometry constraints (c), and the ﬁnal output (d).

VI. EXPERIMENT
To the best of our knowledge, this work is the ﬁrst attempt
at recognizing ﬂoor-level lines in street-view images. Hence,
there are no benchmark datasets, evaluation metrics, and
existing methods for the task. We ﬁll the gaps by preparing
a new dataset (Sec. VI-A), proposing quantitative metrics for
evaluation (Sec. VI-B), and comparing with ablated techniques
(Sec. VI-C) and some closely-related techniques (Sec. VI-D
and Sec. VI-E). Further, we present some AR-related applica-
tions to demonstrate the applicability of this work (Sec. VI-F).

A. Testing Dataset

Besides the new training dataset prepared by using our data
augmentation scheme (see Sec. IV),nwe prepare a new testing
dataset for evaluating the effectiveness and generalizability
of our approach. Here, we choose street-view images by
considering the followings: (i) the image should contain at
least one (nearby) facade, which is not fully occluded; (ii)
the facades should contain at least one ﬂoor-level line; (iii)
the facades can be perspective-oriented in view but cannot
be curved in the physical world; and (iv) the facades should
contain some semantic elements, e.g., windows or balconies.
We collect 600 street-view images for the testing dataset from
the following four different sources:

• We randomly select 150 street views from the eTrims [21]
and arcDataset [43]. Each image includes only one single
facade, and the buildings are mostly low-rise.

• We randomly select 150 more images from the LabelMe-
Facade [12], TSG-20 [2] and ZuBuD+ [32] datasets.
These images have more high-rise buildings, and some
have multiple facades.

• We randomly download 200 GSV images in London
and Glasgow (UK), which feature typical European-style
buildings, similar to those in the CMP dataset.

• Lastly, we randomly download another 100 GSV images
in Hong Kong (HK). These images feature high-rise
buildings, and many facades are partially occluded by
billboards, advertisements, etc.

On each test image, we manually label the position and
order of each ﬂoor-level line on the facades in the form of a
quadrilateral region, e.g., see the “GT” row in Fig. 9. It takes
around 2 minutes to label one image and around 20 hours in
total for all the 600 images. The datasets are available on
the project website: https://wumengyangok.github.io/Project/
FloorLevelNet/.

B. Evaluation Metrics

As a two-stage method, our results include (i) predicted
pixel-wise segmentations from FloorLevel-Net, and (ii) re-
gressed endpoints (xi
e) and order li of each ﬂoor-
e, yi
level line from the geometry post-processing. We employ the
following metrics to evaluate the two results separately:

s) & (xi

s, yi

• Pixel-wise accuracy. For each street-view image, we
denote the set of pixels in ground-truth and in predicted
line regions as Pgt and Ppred, respectively. We employ
the F1 score for quantitative comparisons, which can
be measured upon the number of true positives (TP =
|Pgt ∩ Ppred|), false positives (FP = |Ppred − Pgt ∩ Ppred|),
and false negatives (FN = |Pgt − Pgt ∩ Ppred|):

F1 = (2T P)/(2T P + FP + FN).

(5)

• Line-wise accuracy. We join the endpoints of each ﬂoor-
level line Li
to form a three-pixel wide straight line,
yielding a bag of pixels Pi. Then, we can compute the
conﬁdence of Li as being correctly recognized as

CI(Li) =

∑p∈Pi 1(M f l(p) = li)
|Pi|

,

(6)

where 1(·) is an indicator function for counting the
number of pixels in Pi that appears on the ground truth
image M f l with label li. If CI(Li) > 0.5, we regard the
line prediction as a true positive. For each street-view
image, we again count the number of ﬂoor-level lines as
being correctly detected (TP), missed (FN), and wrongly
recognized (FP), then employ the F1 score (like Eq. (5))
to compute the line-wise accuracy.

C. Ablation Analysis

We carry out an ablation analysis to evaluate the major com-
ponents in our approach, including (i) the data augmentation
scheme, (ii) the multi-task learning, (iii) the height-attention
mechanism in FloorLevel-Net, and (iv) the geometry-post-
processing. In the analysis, we employ the testing dataset, in
which the images do not appear in the training data.
Ablated techniques. To evaluate components in FloorLevel-
Net, we take a stepwise testing strategy by adding the com-
ponents one by one to the baseline DeepLabV3+ [6] model
until we have our full architecture. The difference between
consecutive tests indicates the performance enhancement made

(b) Polyfit(c) Refinement(d) Final result(a) Line groupinghorizontalrangeVPc1239

Fig. 9. Qualitative comparison results in the ablation analysis on the unseen testing dataset. The results illustrate the effects of (i) our data augmentation scheme
by comparing DeeplabV3+ models trained on CMP and on our training data (green background), (ii) multi-task learning with and without the height-attention
mechanism (red background), and (iii) our full method further with geometry post-processing (yellow background), vs. the ground truths (GT).

by the added component. To start, we evaluate our data
augmentation scheme by the following pair of experiments:

• Baseline+CMP: We train a DeepLabV3+ model (base-
line) using the facade dataset CMP [37] with rectiﬁed
facades (Fig. 3 (a1)) and our labels of ﬂoor-level lines
(Fig. 3 (a4)).

• Baseline+DataAugm: We train another DeepLabV3+
model on the training dataset of 3,000 pairs of augmented
street-view images (Fig. 3 (b2)) and labels of ﬂoor-level
lines (Fig. 3 (b4)), which are produced by our data
augmentation scheme, as described in Sec. IV-A.

Next, we evaluate the multi-task learning framework and
height-attention mechanism in FloorLevel-Net by adding these
components to Baseline+DataAugm, as follows:

• Multi-task: This refers to the multi-task learning network
that we build to simultaneously predict facades and ﬂoor-
level lines, as described in Sec. V-A.

• Multi-task+HA: We further incorporate a set of height-
attention layers, as described in Sec. V-B, to form the
full architecture of FloorLevel-Net.

Last, we arrive at Our Full Method by including the second-

stage geometry-post-processing to regress ﬂoor-level lines.
Qualitative analysis. Fig. 9 shows the qualitative comparison
results. Here, we select one or two street-view images from
each constituting component in our testing dataset: eTrims
(1st column), arcDataset (2nd column), TSG-20 (3rd column),
LabelMeFacade (4th column), ZuBuD+ (5th column), GSVs
in UK (6th & 7th columns), and GSVs in Hong Kong (8th &
9th columns). The top row shows the input images, the bottom

row shows the ground-truth images, whereas each of the other
rows from the top show the prediction results obtained by the
ablated methods, i.e., Baseline+CMP, Baseline+DataAugm,
Multi-task, Multi-task+HA, and Our Full Method.

Overall, Baseline+DataAugm generates better predictions
than Baseline+CMP (comparing 2nd & 3rd rows in Fig. 9),
especially for street views with multiple facades and for
facades that are perspective-oriented, e.g., the GSV images
in UK & Hong Kong. The results indicate the effectiveness of
our data augmentation scheme in generating augmented street-
view images for network training.

Next, comparing 3rd & 4th rows Fig. 9, we can see that
Multi-task is able to produce more continuous and distinctive
segmentations of ﬂoor-level lines than Baseline+DataAugm.
This shows that exploring the rich semantics in facades indeed
facilitates the recognition of ﬂoor-level lines. Further, we can
see that the full architecture of FloorLevel-Net with the height-
attention mechanism (5th row in Fig. 9) can produce even
better results, especially in the prediction of ﬂoor orders.
Particularly, the full architecture of FloorLevel-Net success-
fully predicts the ﬂoor orders, even for challenging street-
view images shown in the 4th to 9th columns, whilst the
method fails in the absence of the height-attention mecha-
nism. Better predictions help simplify the line generation and
reﬁnement in the subsequent geometry post-processing stage.
Beneﬁting from the ﬁne predictions by FloorLevel-Net, the
post-processing stage (6th row in Fig. 9) is able to deduce
satisfactory ﬂoor-level line parameters, subject to the various
geometric constraints we discussed earlier.

ZuBuD+Multi-taskeTRIMSInputGTBaseline+ DataAugmLabelMeFacadeHK1UK1Our FullMethodMulti-taks+ HeightAttentionBaseline+ CMParcDatasetUK2HK2TSG-2010

TABLE II
QUANTITATIVE COMPARISON WITH THE ABLATED TECHNIQUES IN TERMS OF PIXEL-WISE ACCURACY ON NETWORK PREDICTION RESULTS, AND
LINE-WISE ACCURACY ON GEOMETRY POST-PROCESSING RESULTS. RESULTS COLORED IN BLUE ARE ASSOCIATED WITH OUR FULL METHOD.

Facades &
buildings

GSV in
UK & HK

Baseline + CMP
Baseline + DataAugm
Multi-task
Multi-task + HA
Baseline CMP
Baseline + DataAugm
Multi-task
Multi-task + HA

Lower ﬂoors

Upper ﬂoors

Overall

Pixel-wise
Accuracy
0.374
0.599
0.631
0.676
0.152
0.530
0.555
0.636

Line-wise
Accuracy
0.725
0.887
0.912
0.926
0.592
0.781
0.819
0.854

Pixel-wise
Accuracy
0.136
0.479
0.498
0.586
0.114
0.449
0.491
0.531

Line-wise
Accuracy
0.676
0.828
0.845
0.860
0.486
0.708
0.734
0.749

Pixel-wise
Accuracy
0.289
0.560
0.586
0.644
0.133
0.505
0.535
0.605

Line-wise
Accuracy
0.702
0.859
0.880
0.894
0.524
0.742
0.774
0.798

for those in HK, resulting in less accurate predictions. Second,
predictions for lower ﬂoors have better accuracy than those
for higher ﬂoor. This is probably because of relatively weaker
supervisions for upper ﬂoor-level lines, as upper ﬂoor-level
lines are typically further away from the camera than the
lower ones and there are relatively less contextual information.
Third, the data augmentation scheme, multi-task learning, and
height-attention layers gradually contribute to the performance
enhancement, yet there are still much room for improvement
in terms of pixel-wise accuracy. The result
indicates that
ﬂoor-level line recognition is more challenging compared with
general urban scene segmentation tasks. Nevertheless, our
predictions provide enough information for the geometry post-
processing, which can correctly recognize and locate most
ﬂoor-level lines in terms of line-wise accuracy.

D. Comparison with a Variant of DeepFacade

As there are no previous works on locating ﬂoor-level
lines, we consider those on facade parsing, i.e., segmenting a
building facade into windows, doors, etc., which can then serve
as references for locating ﬂoor-level lines, as competitors.
Here, we compare with DeepFacade [24], which is a state-
of-the-art deep-learning-based method for facade parsing. In
detail, we extract ﬂoor-level lines from their results as follows:
(i) use RANSAC [5] to rectify a street-view image and record
the associated homographic matrix; (ii) apply DeepFacade [24]
on the rectiﬁed image to produce a facade segmentation map,
and extract only the window, door, and shop regions as positive
regions in the map; (iii) mark a ﬂoor-level line in-between each
pair of adjacent (piecewise) positive regions; and (iv) apply
the inverse of the homographic matrix to revert the original
positions of the detected lines to obtain the ﬁnal results. From
now, we refer the above steps as DeepFacade-Variant.

This

experiment

compares Our Full Method with
DeepFacade-Variant. However, we ﬁnd that DeepFacade-
Variant can hardly produce satisfactory results for GSVs in
UK and HK (see supplemental material for examples) for
two reasons: (i) RANSAC rectiﬁcation is constrained to street
views with only a single facade, but street views in UK
and HK often contain multiple facades, leading to multiple
vanishing points detected and unstable rectiﬁcation results;
(ii) buildings in modern cities may contain facades without
obvious line segments, thereby making it hard to deduce the
vanishing points and perform the rectiﬁcation. Hence, we
report the results only on 300 testing images from the eTrims,

Fig. 10. Comparison between our approach and DeepFacade-Variant on
orthogonal (left two columns) and perspective (right two columns) facades.
Both methods produce satisfactory results for facades that are orthogonal to
camera view, whilst our approach is more robust to facades in perspective
view, and multiple facades in one image (last row).

Quantitative analysis. Further, we conduct a quantitative
evaluation for each ablated technique. Here, we categorize the
testing dataset into two groups: images from datasets focusing
on buildings or facades (eTrims, LabelmeFacade, etc.) are
relatively easier for recognizing ﬂoor-level lines as the images
are prepared for facade parsing or building classiﬁcation task
with regular facade elements, whereas GSVs in UK & HK with
complex environments and dynamic attributes are harder for
recognition. We would also like to evaluate the performance
of our approach for different ﬂoor-level lines, as many AR
applications need only ﬂoor information close to the camera.
To do so, we categorize ﬂoors 1-3 as lower ﬂoors since most
of these ﬂoor pixels are in the low bound of image space, and
the remaining ﬂoors as upper ﬂoors.

We measure the pixel-wise accuracy on the ablation network
outputs, and line-wise accuracy on the line parameters after ge-
ometry post-processing. The quantitative results are presented
in Table II, in which our full method, Multi-task + HA with
geometry post-processing, is marked in blue. Based on the
results, we can derive some interesting observations. First, the
overall accuracy drops when scene complexity increases from
simple street views in facade datasets to GSVs in UK and HK.
There are more facades and higher ﬂoors in GSVs, especially

DeepFacade-VariantDeepFacade-VariantOursOursOrthogonal facadesPerspective-oriented facades11

Fig. 11. The potential of our approach to support and enrich various AR scenarios, e.g., navigation, advertisement, etc.

TABLE III
QUANTITATIVE COMPARISON BETWEEN OUR APPROACH AND
DeepFacade-Variant IN TERMS OF ACCURACY AND TIME EFFICIENCY.

TABLE IV
QUANTITATIVE COMPARISON BETWEEN OUR METHOD AND IMAGE
SEGMENTATION BASELINES IN TERMS OF PIXEL-WISE ACCURACY.

DeepFacade-Variant
Ours

Line-wise
accuracy
0.721
0.876

Recti-
ﬁcation
1.08s
-

Network
inference
0.06s
0.05s

Post-
processing
0.04s
0.14s

SUM

1.18s
0.19s

arcDataset, LabelMeFacade, TSG-20, and ZuBuD+ datasets,
which contain only one building facade in most of the images.
Fig. 10 shows typical results produced by our approach
and DeepFacade-Variant. From the results shown on the left
(1st & 2nd columns), we can see that both methods produce
good results for facades that are orthogonal to camera view.
Yet, DeepFacade-Variant is prone to fail when the improper
homographic matrix inferred by RANSAC (see row 2 for an
example), whilst our approach overcomes the limitation with
the reﬁnement made by geometry constraints. More failure
cases for DeepFacade-Variant are presented in 3rd & 4th
columns, where the facades are in perspective view. Input
image of row 1 is not rectiﬁed properly, hence the interpolated
lines point to wrong directions. In rows 2, windows on up
ﬂoors have little saliency due to distortion, thus the results
miss ﬂoor-level lines on high levels. The building in row 3
has partial irregular facade textures, such as lintel and stairs,
and the method wrongly recognizes adjacent ﬂoors. In the last
row, there are two facades, whilst the rectiﬁcation cannot be
managed by one single homographic matrix. In all these cases,
our approach produces good predictions.

Table III quantitatively compares the results of DeepFacade-
Variant and Our Full Method. From the line-wise accuracies,
we can see an obvious advancement of our approach over
the competitor. Also, our approach outperforms DeepFacade-
Variant in terms of execution time, in which the total running
time is only 0.19 seconds vs. 1.18 seconds. In DeepFacade-
Variant, a major bottleneck is the heavy time cost of RANSAC
rectiﬁcation, whilst our approach directly predicts pixelwise
segmentation without requiring rectiﬁcation.

Datasets

Facades &
buildings

GSV in
UK & HK

Segmentation methods
PSPNet + CMP
PSPNet + DataAugm
OCR + CMP
OCR + DataAugm
Ours
PSPNet + CMP
PSPNet + DataAugm
OCR + CMP
OCR + DataAugm
Ours

Lower
0.258
0.649
0.379
0.610
0.676
0.081
0.369
0.083
0.626
0.636

Upper
0.059
0.528
0.254
0.492
0.586
0.001
0.205
0.016
0.474
0.531

Overall
0.197
0.609
0.330
0.570
0.644
0.067
0.321
0.062
0.581
0.605

E. Comparison with Semantic Segmentation

As the pixel-wise ﬂoor segmentation can be formulated
as an image segmentation task, we also present comparison
with semantic segmentation methods on the pixel accuracy
of the intermediate results. Here we choose PSPNet [52]
and OCR [47] as baselines, since PSPNet is widely used as
baseline for semantic segmentation task, and OCR is one of the
state-of-the-art methods with highest pixel accuracy in some
urban semantic segmentation datasets. For each method, we
train the model with both CMP dataset and our augmented
dataset, and test with 600 real images following the same
settings as the ablation study in Sec. VI-C.

Table IV presents quantitative results in terms of pixel-
wise accuracy. Similar to the results of ablation analysis,
the low overall pixel accuracy by the comparison methods
trained on CMP dataset indicates the difﬁculty of inferring
ﬂoor-level lines, especially without proper dataset. Our data
augmentation signiﬁcantly improves the prediction accuracy to
an acceptable level for both segmentation methods, showing
well generalizability of the baseline network architectures.
Moreover, our methods with multi-task learning and height-
attention mechanism achieves the best performance, for both
the facades & buildings datasets and GSVs in UK & HK.

Our resultNavigationAdvertisementTourismHealthy LifeEntertainment12

Discussion. This work targets at inferring ﬂoor-level lines
in street-view images. We proposed the data augmentation
scheme and FloorLevel-Net to overcome the challenges of
being lack of training dataset and suitable network archi-
tecture. Though the methods are dedicated to the speciﬁc
task, we believe the ideas of augmenting dataset and tuning
network based on data characteristics can be feasibly extended
to some other applications, where labeling is expensive and
data have certain patterns. For instance, in autonomous driving,
a key requirement is to detect driving lanes and pedestrians
simultaneously. In such scenarios, we can possibly improve the
network performance by carefully modeling the relationship
between driving lane and pedestrian.
Limitations. First, our method could be disturbed by obstacles
at critical regions, e.g., a cab that occludes the ground ﬂoor
of a building (Fig. 12 (left)) may cause a wrongly-recognized
ground ﬂoor. Second, our method cannot handle facades that
are reﬂective (Fig. 12 (middle)). Third, when the camera is
too close to the facades (Fig. 12 (right)), our method cannot
get a wide view of the facades, so it may wrongly recognize
the upper and lower bounds of a ﬂoor-level line region as two
successive ﬂoor-lever lines.
Future works. We plan to focus on improving the prediction
precision and efﬁciency of our approach. First, a possible
direction is to work on short video clips instead of on single
images, so that we can obtain temporal information, e.g., struc-
ture from motion (SfM) features, to improve the prediction
precision. Second, we plan to develop strategies to estimate the
distance from camera to facades to address the last limitation
mentioned above. Third, our current pipeline takes around 0.19
seconds (see Table III), which is not slow compared with
DeepFacade-Variant but insufﬁcient to run in real-time. At
present, the main bottleneck is the post-processing module,
which can be improved through multiprocessing implemen-
tations, since we can simultaneously process line grouping
and polyﬁt. Also, we plan to explore lightweight networks
to further improve the pipeline efﬁciency.

ACKNOWLEDGMENT

We thank the street-view images from the Google Street
View service. This work is supported partially by the Research
Grants Council of the Hong Kong Special Administrative
Region (Project no. CUHK 14206320) and Guangdong Basic
and Applied Basic Research Foundation (2021A1515011700).

REFERENCES

[1] Google’s street view static API. https://developers.google.com/maps/

documentation/streetview. Accessed: 2020-10-01. 4

[2] Tourist sights graz image database. http://dib.joanneum.at/cape/TSG-20/.

Accessed: 2021-04-12. 8

[3] D. Anguelov, C. Dulong, D. Filip, C. Frueh, S. Lafon, R. Lyon, A. Ogale,
L. Vincent, and J. Weaver. Google street view: Capturing the world at
street level. Computer, 43(6):32–38, 2010. 2

[4] O. Barinova, V. Konushin, A. Yakubenko, K. Lee, H. Lim, and
A. Konushin. Fast automatic single-view 3-D reconstruction of urban
scenes. In Proceedings of the European Conference on Computer Vision
(ECCV), pages 100–113, 2008. 1

[5] K. Chaudhury, S. DiVerdi, and S. Ioffe. Auto-rectiﬁcation of user
photos. In Proceedings of the IEEE International Conference on Image
Processing (ICIP), pages 3479–3483, 2014. 10

Fig. 12. Limitations of our current approach: obstacle interference (left), glass
reﬂection (middle), and facades too close to camera (right).

F. Applicability

Fig. 11 shows some example usage scenarios that demon-
strates the potential of our method in enriching AR-related
applications. Enabled by the recognized ﬂoor-level lines, we
can augment real-world scenes with virtual contents in a
ﬂoor-aware manner. First, from the ﬂoor-level line results,
we can identify the bound of each facade, i.e., the lowest
and highest ﬂoor-level lines (and their locations), and overlay
virtual contents that cover the whole facade, while skipping the
real contents on the ground level (Fig. 11 (top)). By this means,
we may deliver AR contents, without obscuring necessary real
contents around the user on the street.

More importantly, with the ﬂoor-level lines, we can place
context-aware information on speciﬁc ﬂoors. For example,
in the bottom-row result on the 2nd column of Fig. 11, we
speciﬁcally mark the location of the target room for convenient
navigation; and in the middle-row result on the 3rd column
of Fig. 11, we put shopping directory information over the
corresponding ﬂoors to aid users to ﬁnd the user’s target
items in the department store. Besides, we may mark speciﬁc
apartments in a building that are for sales or for rent to aid
the property agents. On the other hand, we may also employ
our approach to present ﬂoor-related data (e.g., ﬂoor size,
occupancy information, etc.) on buildings and extend it to
estimate the number of ﬂoor levels and infer the building
heights, which could be helpful for land usage analysis and
urban planning [33], [42], [48].

VII. CONCLUSION, DISCUSSION, AND FUTURE WORK

We presented a new approach for recognizing ﬂoor-level
lines in street-view images. As a ﬁrst attempt to this challeng-
ing task, we contribute to (i) devising a new data augmentation
scheme that leverages an existing facade dataset to efﬁciently
generate data samples for network training; and (ii) formulat-
ing FloorLevel-Net, a new multi-task learning framework that
associates explicit features of facades and implicit ﬂoor-level
lines and incorporates a height-attention mechanism to enforce
a vertical ordering of ﬂoor-level lines. The pixelwise semantic
segmentations by FloorLevel-Net are further reﬁned through a
geometry post-processing module for plausible and consistent
reconstruction of ﬂoor-level lines. Quantitative and qualitative
comparisons with existing methods demonstrate the effective-
ness of our proposed method. Further, we demonstrate the
potential of our approach in supporting various AR scenarios.

[6] L.-C. Chen, Y. Zhu, G. Papandreou, F. Schroff, and H. Adam. Encoder-
decoder with atrous separable convolution for semantic image segmen-
tation. In Proceedings of the European Conference on Computer Vision
(ECCV), pages 801–818, 2018. 2, 7, 8

[7] Z. Chen, J. Zhang, and D. Tao. Recursive context routing for object
detection. International Journal of Computer Vision, 129(1):142–160,
2021. 3

[8] S. Choi, J. T. Kim, and J. Choo. Cars can’t ﬂy up in the sky: Improving
In
urban-scene segmentation via height-driven attention networks.
Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), pages 9373–9383, 2020. 1, 3, 6

[9] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Be-
nenson, U. Franke, S. Roth, and B. Schiele. The Cityscapes dataset
In Proceedings of the IEEE
for semantic urban scene understanding.
Conference on Computer Vision and Pattern Recognition (CVPR), pages
3213–3223, 2016. 2, 3, 6

[10] A. Criminisi, I. Reid, and A. Zisserman.

Single view metrology.

International Journal of Computer Vision, 40(2):123–148, 2000. 1

[11] D. Eigen and R. Fergus.

Predicting depth, surface normals and
semantic labels with a common multi-scale convolutional architecture. In
Proceedings of the IEEE International Conference on Computer Vision
(ICCV), pages 2650–2658, 2015. 3

[12] B. Fröhlich, E. Rodner, and J. Denzler. A fast approach for pixelwise la-
beling of facade images. In Proceedings of the International Conference
on Pattern Recognition (ICPR), pages 3029–3032, 2010. 2, 8

[13] A. Geiger, P. Lenz, and R. Urtasun. Are we ready for autonomous
In Proceedings of the
driving? The KITTI vision benchmark suite.
IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
pages 3354–3361, 2012. 2

[14] R. Grompone von Gioi, J. Jakubowicz, J.-M. Morel, and G. Randall.
LSD: a Line Segment Detector. Image Processing On Line, 2:35–55,
2012. 1

[15] A. Gupta, A. A. Efros, and M. Hebert. Blocks world revisited: Image
understanding using qualitative geometry and mechanics. In Proceedings
of the European Conference on Computer Vision (ECCV), pages 482–
496, 2010. 1

[16] O. Haines and A. Calway. Recognising planes in a single image. IEEE
Transactions on Pattern Analysis and Machine Intelligence, 37(9):1849–
1861, 2015. 2

[17] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), pages 770–778, 2016. 7

[18] D. Hoiem, A. A. Efros, and M. Hebert. Recovering surface layout from
International Journal of Computer Vision, 75(1):151–172,

an image.
2007. 2

[19] J. Hu, L. Shen, and G. Sun. Squeeze-and-excitation networks.

In
Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), pages 7132–7141, 2018. 3

[20] Z. Huang, X. Wang, L. Huang, C. Huang, Y. Wei, and W. Liu. CCNet:
Criss-cross attention for semantic segmentation. In Proceedings of the
IEEE International Conference on Computer Vision (ICCV), pages 603–
612, 2019. 3

[21] F. Korˇc and W. Förstner.

eTRIMS Image Database for interpreting
images of man-made scenes. Technical Report TR-IGG-P-2009-01,
April 2009. 8

[22] J.-T. Lee, H.-U. Kim, C. Lee, and C.-S. Kim. Semantic line detection and
its applications. In Proceedings of the IEEE International Conference
on Computer Vision (ICCV), pages 3229–3237, 2017. 1

[23] H. Liu, Y. Xu, J. Zhang, J. Zhu, Y. Li, and C. H. S. Hoi. DeepFacade:
A deep learning approach to facade parsing with symmetric loss. IEEE
Transactions on Multimedia, pages 1–1, 2020. 2

[24] H. Liu, J. Zhang, J. Zhu, and S. C. H. Hoi. DeepFacade: A deep learning
approach to facade parsing. In Proceedings of the International Joint
Conference on Artiﬁcial Intelligence (IJCAI), pages 2301–2307, 2017.
2, 10

[25] X. Liu, Y. Zhao, and S. Zhu. Single-view 3D scene reconstruction and
parsing by attribute grammar. IEEE Transactions on Pattern Analysis
and Machine Intelligence, 40(3):710–725, 2018. 3

[26] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional networks
for semantic segmentation. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), pages 3431–3440,
2015. 2

[27] A. Martinovi´c, M. Mathias, and L. Van Gool. ATLAS: A three-layered
approach to facade parsing. International Journal of Computer Vision,
118(1):22–48, 2016. 1, 2

13

[28] P. Mueller, G. Zeng, P. Wonka, and L. Van Gool. Image-based procedural
modeling of facades. ACM Trans. Graph. (SIGGRAPH), 26(3):85:1–
85:10, 2007. 2

[29] P. Musialski, P. Wonka, D. G. Aliaga, M. Wimmer, L. Van Gool, and
W. Purgathofer. A survey of urban reconstruction. Computer Graphics
Forum (Eurographics), 32(6):146–177, 2013. 1

[30] Z. Ren and Y. J. Lee. Cross-domain self-supervised multi-task feature
learning using synthetic imagery. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), pages 762–771,
2018. 3

[31] G. Schindler, P. Krishnamurthy, R. Lublinerman, Y. Liu, and F. Dellaert.
Detecting and matching repeated patterns for automatic geo-tagging
In Proceedings of the IEEE Conference on
in urban environments.
Computer Vision and Pattern Recognition (CVPR), pages 1–7, 2008.
1, 2

[32] H. Shao, T. Svoboda, and L. Van Gool. Zubud-zurich buildings database
for image based recognition. Computer Vision Lab, Swiss Federal
Institute of Technology, Switzerland, Tech. Rep, 260(20):6, 2003. 8
[33] Q. Shen, W. Zeng, Y. Ye, S. Mueller Arisona, S. Schubiger, R. Burkhard,
and H. Qu. StreetVizor: Visual exploration of human-scale urban forms
based on street views. IEEE Transactions on Visualization and Computer
Graphics, 24(1):1004 – 1013, 2018. 12

[34] G. Simon, A. Fond, and M.-O. Berger. A-contrario horizon-ﬁrst vanish-
ing point detection using second-order grouping laws. In Proceedings of
the European Conference on Computer Vision (ECCV), pages 323–338,
2018. 1

[35] O. Teboul, I. Kokkinos, L. Simon, P. Koutsourakis, and N. Paragios.
In Proceedings
Shape grammar parsing via reinforcement learning.
of the IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), pages 2273–2280, 2011. 2

[36] O. Teboul, L. Simon, P. Koutsourakis, and N. Paragios. Segmentation
of building facades using procedural shape priors. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
pages 3105–3112, 2010. 2

[37] R. Tyleˇcek and R. Šára. Spatial pattern templates for recognition of
In Proceedings of German Conference

objects with regular structure.
on Pattern Recognition (GCPR), pages 364–374, 2013. 2, 3, 4, 9
[38] X. Wang, D. Fouhey, and A. Gupta. Designing deep networks for surface
normal estimation. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), pages 539–547, 2015. 3
[39] J. Weissenberg, H. Riemenschneider, M. Prasad, and L. Van Gool. Is
there a procedural logic to architecture? In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), pages
185–192, 2013. 2

[40] S. Woo, J. Park, J.-Y. Lee, and I. S. Kweon. CBAM: Convolutional
block attention module. In Proceedings of the European Conference on
Computer Vision (ECCV), pages 3–19, 2018. 3

[41] C. Wu, J.-M. Frahm, and M. Pollefeys. Repetition-based dense single-
the IEEE Conference on
In Proceedings of
view reconstruction.
Computer Vision and Pattern Recognition (CVPR), pages 3113–3120,
2011. 1, 2

[42] Y. Wu, L. S. Blunden, and A. S. Bahaj. City-wide building height
determination using light detection and ranging data. Environment and
Planning B: Urban Analytics and City Science, 46(9):1741–1755, 2018.
1, 12

[43] Z. Xu, D. Tao, Y. Zhang, J. Wu, and A. C. Tsoi. Architectural
style classiﬁcation using multinomial
In
Proceedings of the European Conference on Computer Vision (ECCV),
pages 600–615, 2014. 8

logistic regression.

latent

[44] C. Yang, T. Han, L. Quan, and C. Tai. Parsing façade with rank-one
In Proceedings of the IEEE Conference on Computer
approximation.
Vision and Pattern Recognition (CVPR), pages 1720–1727, 2012. 1, 2
[45] F. Yang and Z. Zhou. Recovering 3D planes from a single image
In Proceedings of the European

via convolutional neural networks.
Conference on Computer Vision (ECCV), pages 87–103, 2018. 1
[46] C. Yu, J. Wang, C. Peng, C. Gao, G. Yu, and N. Sang. Learning a dis-
criminative feature network for semantic segmentation. In Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), pages 1857–1866, 2018. 3

[47] Y. Yuan, X. Chen, and J. Wang. Object-contextual representations for
semantic segmentation. In Proceedings of the European Conference on
Computer Vision (ECCV), pages 173–190, 2020. 11

[48] W. Zeng and Y. Ye. VitalVizor: A visual analytics system for studying
IEEE Computer Graphics and Applications, 38:38–53,

urban vitality.
09 2018. 1, 12

14

[49] Z. Zeng, M. Wu, W. Zeng, and C.-W. Fu. Deep recognition of
vanishing-point-constrained building planes in urban street views. IEEE
Transactions on Image Processing, 29:5912–5923, 2020. 1, 2

[50] J. Zhang, Y. Xu, B. Ni, and Z. Duan. Geometric constrained joint
In Proceedings of
lane segmentation and lane boundary detection.
the European Conference on Computer Vision (ECCV), pages 502–518,
2018. 2

[51] H. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia. Pyramid scene parsing
network. In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), pages 2881–2890, 2017. 2

[52] H. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia. Pyramid scene parsing
network. In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), 2017. 11

Mengyang Wu received the B.S. degree from Uni-
versity College London. He is currently a PhD
student in Computer Science and Engineering of
Chinese University of Hong Kong. His recent re-
search interests include deep learning for 3D vision,
scene understanding, and outdoor augmented reality.

Wei Zeng is currently an associate professor at
Shenzhen Institute of Advanced Technology, Chi-
nese Academy of Sciences. He received his bach-
elor’s degree (2011) and Ph.D. (2015), both in
computer science, from Nanyang Technological Uni-
versity. He served as co-chairs for PaciﬁcVis Poster,
and program committee members for various re-
search conferences, including IEEE Visualization,
ChinaVis, IVAPP. His recent research interests in-
clude data visualization and AR/VR, and their ap-
plications in smart city.

Chi-Wing Fu is currently an associate professor in
the Chinese University of Hong Kong. He served
as the co-chair of SIGGRAPH ASIA 2016’s Tech-
nical Brief and Poster program, associate editor of
IEEE Computer Graphics & Applications and Com-
puter Graphics Forum, panel member in SIGGRAPH
2019 Doctoral Consortium, and program committee
members in various research conferences, including
SIGGRAPH Asia Technical Brief, SIGGRAPH Asia
Emerging tech., IEEE visualization, CVPR, IEEE
VR, VRST, Paciﬁc Graphics, GMP, etc. His recent
research interests include computation fabrication, point cloud processing, 3D
computer vision, user interaction, and data visualization.

