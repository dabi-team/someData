FloorLevel-Net: Recognizing Floor-Level Lines
with Height-Attention-Guided Multi-task Learning

Mengyang Wu, Wei Zeng, Member, IEEE and Chi-Wing Fu, Member, IEEE

1

1
2
0
2

l
u
J

6

]

V
C
.
s
c
[

1
v
2
6
4
2
0
.
7
0
1
2
:
v
i
X
r
a

Abstract‚ÄîThe ability to recognize the position and order of the
Ô¨Çoor-level lines that divide adjacent building Ô¨Çoors can beneÔ¨Åt
many applications, for example, urban augmented reality (AR).
This work tackles the problem of locating Ô¨Çoor-level lines in
street-view images, using a supervised deep learning approach.
Unfortunately, very little data is available for training such a
network ‚àí current street-view datasets contain either semantic
annotations that lack geometric attributes, or rectiÔ¨Åed facades
without perspective priors. To address this issue, we Ô¨Årst compile
a new dataset and develop a new data augmentation scheme to
synthesize training samples by harassing (i) the rich semantics of
existing rectiÔ¨Åed facades and (ii) perspective priors of buildings
in diverse street views. Next, we design FloorLevel-Net, a multi-
task learning network that associates explicit features of building
facades and implicit Ô¨Çoor-level lines, along with a height-attention
mechanism to help enforce a vertical ordering of Ô¨Çoor-level lines.
The generated segmentations are then passed to a second-stage
geometry post-processing to exploit self-constrained geometric
priors for plausible and consistent reconstruction of Ô¨Çoor-level
lines. Quantitative and qualitative evaluations conducted on
assorted facades in existing datasets and street views from Google
demonstrate the effectiveness of our approach. Also, we present
context-aware image overlay results and show the potentials
of our approach in enriching AR-related applications. Project
website: https://wumengyangok.github.io/Project/FloorLevelNet.

Index Terms‚ÄîMulti-task learning, attention mechanism, se-

mantic segmentation, street view, augmented reality

I. INTRODUCTION

F LOOR-LEVEL lines are line segments that separate ad-

jacent Ô¨Çoors on a building facade; see Fig. 1 (middle).
Being able to recognize them in city-wide street views can
beneÔ¨Åt various applications, e.g., urban 3D reconstruction [29],
building topology analysis [42], and urban vitality study [48].
Intrinsically, Ô¨Çoor-level lines for the same building are parallel,
through which we can reconstruct the homography of the
facade in a perspective view [10] and support Ô¨Çoor-aware
augmented reality (AR) applications; see Fig. 1 (right).

This work considers the problem of inferring Ô¨Çoor-level
lines in street-view images, which requires the recovery of not
only geometric priors (e.g., positions and vanishing directions)
in the image view, but also semantic information (e.g., Ô¨Çoor
orders) relevant to the Ô¨Çoor-level lines. The task is relatively

M. Wu, and C.-W. Fu are with the Chinese University of Hong Kong.

e-mail: {mywu, cwfu}@cse.cuhk.edu.hk.

W. Zeng is with Shenzhen Institute of Advanced Technology, Chinese

Academy of Sciences. e-mail: wei.zeng@siat.ac.cn.

W. Zeng and C.-W. Fu are the corresponding authors.
This paper has

supplementary downloadable material available at
http://ieeexplore.ieee.org., provided by the author. The material includes ad-
ditional results and comparisons. Contact mywu@cse.cuhk.edu.hk for further
questions about this work.

Fig. 1. Left column shows two example street-view images in London (top)
and Hong Kong (bottom), where the camera views are side- and front-facing
relative to the building, respectively. Note the occlusions introduced by the
advertisement billboard and light post circled in red on bottom left. Middle
column shows Ô¨Çoor-level lines recognized by our method with geometric
positions and semantic order labels. Right column shows potential Ô¨Çoor-aware
image-overlay results to aid shopping (top) and navigation (bottom).

intuitive for humans but very challenging for computers. So
far, we are not aware of any work that can robustly detect and
recognize Ô¨Çoor-level lines.

Related prior works on 3D reconstruction (e.g., [4], [15]),
and facade parsing (e.g., [27], [44]), typically rely on various
assumptions about the structural regularity on building fa-
cades, e.g., repetitive windows and balconies (e.g., [31], [41]).
Though these extra constraints help disambiguate the problem
and the results might help infer Ô¨Çoor-level lines, they still have
various limitations, such as being error-prone when handling
perspective-oriented facades (Fig. 1 top-left) and scenes with
occlusions (Fig. 1 bottom-left). Alternatively, we may infer
Ô¨Çoor-level lines by locating line segments roughly in the same
direction. However, existing line detectors (e.g., [14], [34])
can easily generate huge amount of irrelevant line segments
in cluttered street views, and offer no semantic information for
inferring the Ô¨Çoor order. Recently, Lee et al. [22] proposed the
concept of semantic lines that characterize the spatial scene
structure in images. The method Ô¨Årst Ô¨Ånds candidate lines
using line features at multi-scale pooling layers, then Ô¨Ålters out
semantic lines using local line features. However, the Ô¨Åltering
process (semantic or not) is a binary classiÔ¨Åcation problem,
whilst this work requires to recover not only a multi-class label
per line but also a plausible Ô¨Çoor order for lines in the same
facade, i.e., order 1, order 2, etc.

Recent attempts of using deep neural networks for semantic
scene segmentation (e.g., [8]) and planar surface recognition
(e.g., [45], [49]) exploit the possibility of jointly learning
the semantic and geometric attributes in street views. The
methods achieve superior performances over previous deep

11332242314 
 
 
 
 
 
learning methods that use solely the semantic features. This
work also leverages a supervised deep learning approach that
can jointly infer geometric priors and semantic information of
Ô¨Çoor-level lines. This is nevertheless a challenging task. First,
a key requirement for network training is the availability of
large, labeled street-view images with annotations of Ô¨Çoor-
lines. However, existing street-view datasets contain
level
either semantic annotations that lack geometric attributes, or
rectiÔ¨Åed facades without perspective priors. To address this
issue, we devise a new data augmentation scheme and compile
a new dataset by effectively combining the rich context of
Ô¨Çoor-level lines marked on an existing facade dataset, with
perspective priors easily-extracted on street-view images. In
this way, we can largely reduce the manual workload in the
dataset construction, while promoting the generalizability of
our network model.

Second, we are not aware of any network architecture
that can fulÔ¨Åll the requirement of recognizing and locating
Ô¨Çoor-level lines in street-view images. To Ô¨Åll the gap, we
design FloorLevel-Net ‚àí a new deep learning framework
with two main components inspired by the characteristics
of our Ô¨Çoor-level dataset. (i) FloorLevel-Net leverages multi-
task learning that associates geometric properties of facades
orientation to camera, with semantic features of facade ap-
pearance, including windows, doors, and balconies, etc. (ii)
FloorLevel-Net incorporates a height-attention mechanism to
enforce vertical orderings of Ô¨Çoor-level lines, as Ô¨Çoor order
naturally increases from bottom to top in image space. We
further exploit various geometric properties,
including the
vanishing point (VP) and Ô¨Çoor order constraints, to enforce
a consistent reconstruction of Ô¨Çoor-level lines from piecewise
segmentations by FloorLevel-Net.

We evaluate the performance of our approach on both build-
ing images from existing facade datasets and self-collected
street-view images from Google Street View (GSV) [3]. Both
qualitative and quantitative analysis demonstrate the effective-
ness of the individual components in our approach. The main
contributions of this work include:

‚Ä¢ We propose a data augmentation scheme to integrate
rich semantics of rectiÔ¨Åed facades and geometric priors
of buildings in diverse street views, and compile a new
dataset for recognizing Ô¨Çoor-level lines (Sec. IV).

‚Ä¢ We design FloorLevel-Net‚àía multi-task CNN architec-
ture with height attention to simultaneously predict a
facade segmentation map and a Ô¨Çoor-level distribution
map for an input street-view image (Sec. V-A to V-C).
We further develop a post-processing framework to ex-
tract Ô¨Çoor-level parameters (including the endpoints and
vanishing points), and reÔ¨Åne the parameters with self-
constrained geometric priors (Sec. V-D).

‚Ä¢ We evaluate the effectiveness of our approach in recog-
nizing Ô¨Çoor-level lines, and demonstrate the applicability
in enriching context-aware urban AR (Sec. VI).

II. RELATED WORK

The recognition of geometric structures in urban scenes,
e.g., surface layout [18] and driving lanes [50], has been

2

then split

gaining attention in image processing and vision research. This
work targets at Ô¨Çoor-level lines that separate adjacent Ô¨Çoors
on building facades. A closely-related topic is to reconstruct
3D building models from a monocular image. Conventional
approaches typically adopt a two-stage approach: Ô¨Årst parse
a facade into piecewise regions like windows, balconies,
the regions into regular layouts
etc. [27], [44],
for subsequent modeling, e.g., by shape grammars [28], [35].
Existing methods, however, rely heavily on detecting repeti-
tion [41], [31], symmetric or rectangular structures on facade
layout [36], [39], thus exhibiting signiÔ¨Åcant challenges for
parsing general urban scenes in the wild. Particular obstacles
include perspective-oriented facades (e.g., see Fig. 1 (top) and
the red inset in Fig. 2) and scene occlusions by billboards
and trafÔ¨Åc lights (e.g., see Fig. 1 (bottom) and the blue
inset
in Fig. 2), resulting in failures in matching repeti-
tion/symmetry/rectangular constraints. It is hard to identify
common geometric assumptions that can well Ô¨Åt diverse styles
of building facades in different cities.

Recent studies on urban scene recognition focus on deep-
learning-based approaches, beneÔ¨Åting from new datasets of
urban scenes and advancements in neural network architec-
ture, e.g., [26], [51], [6]. SpeciÔ¨Åcally, DeepFacade [24], [23]
parses facades using a fully convolutional network to produce
pixel-wise semantics. Some work also attempts to formulate
geometric understanding of urban scenes as an image segmen-
tation problem. For instance, Haines and Calway [16] infer
surface orientations using spatiograms of gradients and colors.
However, the results are piecewise segments that are typically
discontinuous with coarse and irregular boundaries, hindering
the recognition of geometric priors like the endpoints of Ô¨Çoor-
level lines. To further recover geometric priors, a second-stage
post-processing framework can be employed. Zeng et al. [49]
employ a vanishing-point-constrained optimization to enhance
piecewise segmentations of planar building facades.

Similarly, we adopt a twofold process to recognize Ô¨Çoor-
level lines on building facades. First, we predict a segmen-
tation mask of pixel-wise line labels (Ô¨Çoor-level distribution)
using a multi-task CNN architecture. Second, we reÔ¨Åne the
piecewise Ô¨Çoor-level distribution into line parameters using
self-constrained geometric priors. Nevertheless, the work faces
severe challenges of being lack of training dataset and suit-
able network architecture. We tackle the challenges from the
following perspectives:

‚Ä¢ Data augmentation is a vital technique to improve the
diversity of the training data and the network generaliz-
ability. There are several publicly available datasets for
urban scene (e.g., KITTI [13] and Cityscapes [9]) and
facade (e.g., CMP [37] and LabelMeFacade [12]). Yet,
we cannot use them directly in this work. In contrast
to explicit building structures such as windows and bal-
conies that have obvious visual appearance, Ô¨Çoor-level
lines are rather implicit, requiring contextual informa-
tion for the recognition. To Ô¨Åll the gap, we propose a
new data augmentation scheme that leverages detailed
facade semantics of front-facing facades in CMP [37]
and diverse building perspectives in GSV images. In this

way, the synthesized dataset efÔ¨Åciently captures both the
rich context of Ô¨Çoor-level lines and geometric priors of
building facades in reality.

‚Ä¢ Multi-task learning helps to boost the performance of
many deep-learning-based vision tasks, by learning fea-
tures from relevant knowledge domains. In the literature,
various efforts have been devoted to exploit multi-task
learning for geometric understanding of urban scenes.
For example, Liu et al. [25] tackle scene recognition
and reconstruction tasks in a tightly-coupled framework;
besides, there are assorted works for indoor scene un-
derstanding (e.g., [38], [30], [11]) by fusing features of
depth, surface normals, and objects. Human experience
on locating Ô¨Çoor-level lines typically relies on recogniz-
ing the facades and its enclosing regions, e.g., doors and
windows. Therefore, we divide our task into subtasks of
recognizing facades and Ô¨Çoor-level lines, and design a
multi-task learning network to simultaneously address the
two subtasks together.

‚Ä¢ Channel-wise attention exploits the inter-channel rela-
tionship of features and scales the feature map according
to the importance of each channel. The mechanism is Ô¨Årst
proposed in SENet [19] and adopted widely in image
classiÔ¨Åcation (e.g., [40]) and segmentation (e.g., [46]).
Recent attention approaches take more advantages of
contextual information inside the image domains, e.g.,
the criss-cross attention module by Huang et al. [20]
and recursive context routing (ReCoR) mechanism by
Chen et al. [7]. Choi et al. [8] utilize the unbalanced
class distribution at varying vertical locations in urban
scene images, and design a height-attention module to
emphasize the unbalance. Floor-level lines in our dataset
exhibit a similar property, since Ô¨Çoor order on the same
facade always increases from bottom to top in the image
space. This drives us to adopt a similar conÔ¨Åguration in
the recognition of Ô¨Çoor-level lines.

III. OVERVIEW

In this work, our goal is

to recognize and locate the Ô¨Çoor-level lines on each
building facade (that is close to the camera) in the
image space of street-view images.

e, yi

The input to our method is a single RGB image I ‚àà RW √óH√ó3
of width W and height H. Given I, we aim to predict a set
of line segments L = {L1, ..., Ln} per facade that marks the
separations between adjacent Ô¨Çoors; see Fig. 1 (middle) for
two examples. In the following, we refer line segments as lines
for simplicity. Each line Li contains a pair of endpoints (xi
s, yi
s)
and (xi
e) in the image space of I. Also, we aim to recognize
a Ô¨Çoor order (denoted as li ‚àà {1, 2, 3, ...}) per line Li, such that
the line that separates the ground Ô¨Çoor and the Ô¨Årst Ô¨Çoor has
a Ô¨Çoor order value of one, the next line above has a Ô¨Çoor
order value of two, etc. So, each Li is speciÔ¨Åed as a 5-tuple
{xi
e, li}. As a visualization, we use the same coloring
scheme (orange for order 1, green for order 2, etc.) to reveal
the Ô¨Çoor orders in all our results.

e, yi

s, xi

s, yi

3

There are three main parts in our approach.
(i) We compile a set of facade images from the CMP
dataset [37], and employ them to augment building
facades in GSV images based on the geometric prior
of facade perspective (Sec. IV). By our new data aug-
mentation scheme, we can efÔ¨Åciently generate a large
amount of image samples to train our network.

(ii) We develop a multi-task learning network‚àíFloorLevel-
Net (Sec. V-A to V-C), to segment the input image into
piecewise regions of facades and to detect candidate
lines; see
pixels possibly associated with Ô¨Çoor-level
the multi-task learning module in Fig. 2. Particularly,
FloorLevel-Net encapsulates semantic (e.g., windows,
doors, shops, etc.) and geometric (e.g., Ô¨Çoor orders,
facade orientation, etc.) information related to Ô¨Çoor-
level lines by articulating a fused loss function that is
differentiable by means of pixel-wise convolutions.
(iii) We infer per-line 5-tuple parameters from the piecewise
segmentations in the geometry post-processing stage
(Sec. V-D). Here, we locate facade regions, group Ô¨Çoor-
level lines per facade, then regress a polyline per Ô¨Çoor-
level line; see the geometry processing module in Fig. 2.
Further, we reÔ¨Åne the polylines based on self-constrained
geometric priors, i.e., the facade boundary, Ô¨Çoor order,
and vanishing points.

So far, we are not aware of any work on recognizing
Ô¨Çoor-level lines in street-view images. Existing related works
focus on recognizing building facades as a whole or explicit
objects, such as windows, doors, and balconies. Hence, we
show the effectiveness of our approach through comparisons
with ablation techniques (Sec. VI-C) and DeepFacade, which
is a closely-related work on facade parsing (Sec. VI-D), and
demonstrate the potential of our work to support and enrich
urban AR applications (Sec. VI-F).

IV. DATASET PREPARATION & ANALYSIS

An immediate challenge to recognizing Ô¨Çoor-level lines is
the lack of properly-annotated street-view images. On the one
hand, existing city-wide datasets such as Cityscapes [9] pro-
vide mainly urban scene segmentations, e.g., roads, buildings,
vehicles, etc., while lacking details of facades, not to mention
Ô¨Çoor-level lines. On the other hand, building facade datasets
such as CMP [37] provide detailed semantics of facades, e.g.,
windows, doors, balcony, etc. However, the images exhibit
mostly rectiÔ¨Åed views of front-facing facades, which cannot
reÔ¨Çect real-world facades with perspective orientations.

A. Data Augmentation

To relax the enormous workload for labeling a new dataset,
we propose a data augmentation scheme (Fig. 3) to take
the best advantages of the facade semantics in the CMP
dataset [37] together with the perspective-oriented building
facades extracted from GSV images.

To start, we Ô¨Årst analyze the facade images (Fig. 3 (a1))
with their semantics (Fig. 3 (a2)) provided by the CMP
dataset. Here, we make use of the simpliÔ¨Åed semantics (Fig. 3
(a3)), i.e., window, shop, and door, which offer contextual

4

Fig. 2. Our two-stage approach: (i) FloorLevel-Net is a multi-task learning network that segments the input image into building-facade-wise semantic regions
(top) and Ô¨Çoor-level distributions (bottom); and (ii) our method further Ô¨Åts and reÔ¨Ånes the pixel-wise network outputs into polylines with geometric parameters.
Further, we can take the reconstructed Ô¨Çoor-level lines to support and enrich urban AR applications with Ô¨Çoor-aware image overlay.

Fig. 3.
Illustration of our data augmentation scheme. We take the advantages of the rich context of facades in the CMP dataset [37] (a1 & a2) and the
perspective-oriented building facades readily extracted from the GSV (Google Street View) images (b1). From them, we can efÔ¨Åciently obtain simpliÔ¨Åed
semantics (a3) and annotate Ô¨Çoor-level lines (a4), and further generate a very large amount of augmented image samples, i.e., an augmented image (b2) with
its associated semantic image (b3) and Ô¨Çoor-level-lines image (b4), by pairing up different geometric priors with different CMP facades.

information for Ô¨Çoor-level lines, i.e., windows usually appear
on every Ô¨Çoor, whereas shops and doors usually appear on the
ground Ô¨Çoor. There may not be obvious lines between adjacent
Ô¨Çoors, yet humans can infer them based on the associated
contexts, e.g., windows. Thanks to the regular front-facing
structures in these inputs, manually labeling Ô¨Çoor-level lines
(Fig. 3 (a4)) is very fast, compared with labeling on general
street-view images that are perspective. We extract a total of
150 rectiÔ¨Åed facade images from the CMP dataset.

Next, we employ the GSV API [1] to collect street-view
photos. Here, we randomize the camera headings to obtain
perspective-oriented facades, and set the pitch parameter in the
API to zero, so the camera view directions are always horizon-
tal, i.e., parallel to the ground. Also, for each GSV photo, we
identify nearby planar facades that are feasible for overlaying
CMP facade images, i.e., with an aspect ratio similar to those

in the CMP dataset. Then, we manually annotate a quad to
mark the region of each feasible planar facades (Fig. 3 (b1))
and label also its orientation {le f t, right, f ront} relative to the
camera view direction. The facade priors serve as reference
locations for overlaying facade images and semantics. For
instance, see Fig. 3 (b1), the blue quad marks the facade region
of the left building with orientation categorized as front (i.e.,
front facing the camera), whereas the green quad marks the
facade region of the right building with right orientation. By
doing so, we can derive an afÔ¨Åne transformation matrix for
each facade, and use it to generate an augmented street-view
image (Fig. 3 (b2)) with its associated semantic image (Fig. 3
(b3)) and labeled Ô¨Çoor-level-lines image (Fig. 3 (b4)).

To enrich the data sample diversity and to improve the
network generalizability, we collect 200 GSV images in Hong
Kong and London, with both high-rise and relatively low-rise

(a1) Raw CMP image(b1) Geometric priors from GSV(b2) Synthetic image(a2) Raw CMP semantics(a3) Simplified semantics(a4) Floor-level lines(b3) Synthetic semantics(b4) Synthetic floor-level linesGeometric priors (b1)Geometric priors (b1)5

V. FLOORLEVEL-NET APPROACH

In this

section, we Ô¨Årst present

the framework of
FloorLevel-Net (see Fig. 5), which is a multi-task learning
network (Sec. V-A) that fully utilizes the two-stream seman-
tics of facades and Ô¨Çoor-level lines, with a height-attention
mechanism (Sec. V-B) to enforce the vertical ordering of Ô¨Çoor-
level lines. Then, we present the implementation details of
FloorLevel-Net (Sec. V-C), followed by the add-on geometry
post-processing module to generate the Ô¨Ånal parameters for
each Ô¨Çoor-level line (Sec. V-D).

Fig. 4. Characteristics of the data samples in our dataset: distributions of (a)
the facade orientation, (b) the highest Ô¨Çoor order, and (c) average number of
pixels of each Ô¨Çoor order in a single image.

buildings, respectively. Together with the facade images from
the CMP dataset, we compile a new dataset with 3,000 pairs
of augmented street-view images, each associated with facade
semantics and Ô¨Çoor-level line labels. We train our FloorLevel-
Net using the dataset, and compare it with an ablation model
trained on the CMP dataset of only rectiÔ¨Åed facades. The
results show a signiÔ¨Åcant boost by the data augmentation
scheme (Sec. VI-C).

B. Dataset Characterization

We conduct a preliminary analysis of our augmented dataset
to show its strong generalizability, and later take it to derive
feasible components in our network. By observing the follow-
ing characteristics of the dataset, some hyper parameters, e.g.,
maximum Ô¨Çoor order, are determined.

‚Ä¢ Facade orientation: Fig. 4 (a) reports the distribution of
facade orientations. As mentioned in the previous subsec-
tion, three orientations {le f t, right, f ront} are considered
based on the facade orientation relative to the camera.
The three orientations share similar proportions, without
biasing towards any of the three.

‚Ä¢ Highest Ô¨Çoor order: Fig. 4 (b) plots the distribution of
the highest Ô¨Çoor order in the data samples. Most facades
have a Ô¨Çoor order of Ô¨Åve. Also,
there is no single-
Ô¨Çoor building, since we omit single-Ô¨Çoor buildings when
compiling the data. There are few high-rise buildings
(< 5%) with the highest Ô¨Çoor order above 10.

‚Ä¢ Average pixel number per Ô¨Çoor order: Fig. 4 (c) presents
the average number of pixels belonging to each Ô¨Çoor
order in a single image of size 480√ó360. Since the street-
view images are taken from the ground, low Ô¨Çoor orders
have more pixels, and the number of pixels decreases as
Ô¨Çoor order increases. Floor-level lines of orders above 10
are too small to overlay AR images.

In consideration of the distribution of highest Ô¨Çoor order and
average pixel number per Ô¨Çoor order in the augmented dataset,
we set the maximum Ô¨Çoor order to be 10 in our Ô¨Çoor-level line
detection module (Sec. V-A).

A. Multi-task Learning

Based on the establishment of our augmented dataset, each
street-view image I ‚àà RW √óH√ó3 for training is coupled with
two label classes: (i) facade semantics Œ® f a and (ii) Ô¨Çoor-level
distributions Œ® f l. Œ® f a consists of {window, door, shop, left,
right, front, other}, where the Ô¨Årst three labels are the context,
the subsequent three indicate the facade orientation, and other
marks the non-facade pixels. Œ® f l consists of {l1, ¬∑ ¬∑ ¬∑ , ln, other},
where n denotes the highest Ô¨Çoor order (i.e., 10) as suggested
by the data characteristics (Sec. IV-B), and other marks the
non-Ô¨Çoor-level-lines pixels. The two-stream semantics com-
plement each other for the goal: Œ® f a provides rich context
for separating Ô¨Çoor-level lines in each facade, whereas Œ® f l
helps to estimate the order and position of each Ô¨Çoor-level
line. This observation on our training data inspires us to apply
a multi-task learning process to beneÔ¨Åt each task.

As illustrated in Fig. 5, FloorLevel-Net Ô¨Årst employs a
shared encoder to learn a feature map, then it applies two-
branch decoders to gradually upsample the feature map via
deconvolutional
layers. The facade feature decoder in the
upper branch outputs segmentation mask ¬ØM f a ‚àà RW √óH , such
¬ØM f a(œà f a|x) indicates the probability of pixel x having
that
label œà f a ‚àà Œ® f a.

¬ØM f a is computed by

¬ØM f a(œà f a|x) = so f tmax(Wf aF(x)),

(1)

where F(x) is the feature vector of pixel x and Wf a denotes the
parameters learned by the network. We use the loss function
L f a = Ce( ¬ØM f a,M f a) to supervise the network training, where
M f a is the ground truth map of facades and Ce(¬∑) is a standard
softmax cross entropy function.

On the other hand, the line feature decoder in the lower
branch of FloorLevel-Net outputs Ô¨Çoor-level distribution map
¬ØM f l ‚àà RW √óH for Ô¨Çoor-level line detection. Besides the soft-
max cross entropy Ce( ¬ØM f l,M f l), where M f l denotes the
associated ground truth, we also fuse L f a here in the loss
function for supervising the network training:

L f l = Ce( ¬ØM f a,M f a) + Ce( ¬ØM f l,M f l).

(2)

Incorporating Ce( ¬ØM f a,M f a) in L f l contributes to the Ô¨Çoor-
level lines predictions, since facade context such as windows
can help infer the Ô¨Çoor levels. Experimental results also
conÔ¨Årm that the fused loss helps to predict more continuous
segmentations and more accurate Ô¨Çoor orders (Sec. VI-C).

10%20%30%LeftCentralRight2K4K6K8K10K12K(a) Distribution of facade orientation123456789>1010(b) Distribution of highest floor order12345678910>10(c) Average number of pixels of each floor order in a single image6

Fig. 5. Overview of our FloorLevel-Net framework. We consume each input street-view image by a multi-task learning network to jointly segment the
facades and detect Ô¨Çoor-level lines. In the module of our line feature decoder, we incorporate height-attention layers (see bottom left) along with the original
convolutional layers, to help enforce the vertical ordering of the Ô¨Çoor-level lines, where ‚Äúmulti‚Äù denotes element-wise multiplication.

TABLE I
PROBABILITY DISTRIBUTIONS OF PIXELS (IN PERCENTAGE) OF THE
LOWEST SIX FLOOR ORDERS IN THE WHOLE IMAGE AND IN EACH
VERTICAL BOUND.

Given

Probabilities of the lowest six Ô¨Çoor orders
p6
p1

p5

p4

p3

p2

Class
Entropy

Fig. 6. Height-related feature: Pixel distribution of Ô¨Çoor-level lines in vertical
bounds of the image space.

B. Height-Attention Mechanism

Image

9.75

11.8

9.56

4.90

2.83

1.17

0.436

Low
Mid-low
Mid-high
High

27.7
6.75
2.79
1.80

10.1
20.1
10.3
6.66

1.71
8.72
14.8
13.0

0.35
2.97
7.07
9.20

0.12
1.09
3.86
6.22

0.03
0.32
1.20
3.09

0.298
0.386
0.429
0.442

0.389
(avg)

Observing that ‚Äúcars can‚Äôt Ô¨Çy in the sky‚Äù, i.e., pixels of
car are generally below those of the sky, Choi et al. [8]
suggest a height-attention mechanism based on the analysis
of the CityScape [9] dataset, which indicates that the class
distribution signiÔ¨Åcantly depends on a vertical position in
urban scene. For instance, a lower part of an image is mainly
composed of road, while the upper part, buildings and sky
are principal objects. We hypothesis a similar pattern for pixel
vertical distribution of Ô¨Çoor-level lines in our case, i.e., pixels
of order i Ô¨Çoor-level lines are generally below those of order
j Ô¨Çoor-level lines, given i < j. Here we simplify the analysis
by dividing the image space (from bottom to top) into four
equal-sized vertical bounds, i.e., low, mid-low, mid-high, and
high; see the inset in Fig. 6. For each Ô¨Çoor-level line in our
data samples, we count its pixels in each vertical bound, and
compute its distribution ratios in the four bounds. Then, we
sum and normalize the distribution ratios for Ô¨Çoor-level lines
of same order in the whole data, and obtain the distribution
ratios per Ô¨Çoor-level line order. From Fig. 6, we can see that
pixels of low Ô¨Çoor-level lines appear more in the lower vertical
bound (e.g., 73% of Ô¨Çoor order 1 pixels are in the low bound),
whilst pixels of high Ô¨Çoor-level lines have a higher chance of
locating in the upper vertical bound (e.g., 53% of Ô¨Çoor order
6 pixels are in the high bound).

Next, we analyze the probabilities of Ô¨Çoor-level line pixels
in the whole image and in each vertical bound. Table I presents
the probability distributions of the lowest six dominant Ô¨Çoors
orders, which take over 97% of all Ô¨Çoor orders (see Fig. 4
(c)). Here pi denotes the probability that an arbitrary pixel is
assigned to the i-th Ô¨Çoor order. The results further conÔ¨Årm the
hypothesis of the pixel vertical distribution. For example, the
probability of Ô¨Çoor order 1 p1 is 9.75% in the whole image,
and it drops from 27.7% to 1.8% in vertical bounds from low
to high. On the contrary, p6 increases from 0.03% to 3.09%,
which matches with the distribution in Fig. 6. We further
measure the uncertainty of pixel distribution probabilities in
separate bound by calculating the entropies as ‚àí ‚àë pilogpi. The
overall entropy of the entire images is 0.436, and the low
bound has the smallest entropy of 0.298 due to the dominant
probability of Ô¨Çoor order 1. The result indicates that if a pixel
falls in the low bound, it shall probably be predicted as Ô¨Çoor
order 1 but not the other Ô¨Çoor orders.

These observations suggest that vertical position in image
space can serve as a good indicator of Ô¨Çoor order. Hence, we
are motivated to leverage height-attention layers in the design
of FloorLevel-Net. In detail, we include an HA layer between
adjacent convolutional layers in the line feature decoder; see
Fig. 5 (bottom left) for the illustration.

Street-viewimage ùêºBuildingfacadeRes(x)Sharedencoder+Res(x)1x1 convùê∂)√óùêª)√óùëä)pool1d-conv2d-convmulti1x1 convFacadefeaturedecoderùê∂)√óùêª)ùê∂-√óùêª-ùê∂-√óùêª-√óùëä-interpolateLine featuredecoderHeight-attentionlayer‚Ñ≥/0))‚Ñ≥10)-ùíú0)‚Ñ≥/0)-‚Ñ≥03Floor-levellines‚Ñ≥0)LowMid-lowMid-highHighOrder 1Order 2Order 3Order 4Order 5Order 610%20%30%70%60%50%40%7

for the Ô¨Ånal prediction layers, where softmax is applied. We
implement FloorLevel-Net using PyTorch and train it from
scratch on a single NVidia GeForce GTX 1080 Ti GPU card.
During the training, we adopt the momentum optimizer with
learning rate 1e-3 and a small batch size of four. Convergence
is reached at about 100K iterations.

D. Geometry Post-Processing

FloorLevel-Net predicts two piecewise segmentation masks
¬ØM f a and ¬ØM f l, from which we further extract line parameters
by considering various geometric constraints:
(i) Facade constraint: Floor-level lines are attached to fa-
cades, so the detected lines are valid only if they lie inside
a facade region. Since ¬ØM f a may contain multiple facades,
we process Ô¨Çoor-level line segmentations and generate
only one line with a speciÔ¨Åc Ô¨Çoor order per facade.
(ii) Vanishing point (VP) constraint: Floor-level lines of the
in 3D, so they
same facade are intrinsically parallel
should meet at a common VP, which is at a Ô¨Ånite location
for perspective-oriented facades.

(iii) Order constraint: Assuming that the up direction in input
image roughly matches the up direction in real world,
orders of Ô¨Çoor-level lines for the same facade should
strictly increase from bottom to top.

To enforce these constraints, we formulate a second-stage
geometry post-processing to generate Ô¨Çoor-level line parame-
ters with the following steps (as illustrated in Fig. 8):
(i) Line grouping (Fig. 8 (a)): First, we group relevant labels
¬ØM f a to form a super-segment per facade. Then we
in
group Ô¨Çoor-level lines segmentation in ¬ØM f l, and identify
a group of lines per facade.

(ii) PolyÔ¨Åt (Fig. 8 (b)): We Ô¨Åt a polyline (in the form of Œ≤i
Œ≤i
j)}n
1x) for pixels of each Ô¨Çoor-level line (say {(xi
for i-th line) using a least squares regression:

j, yi

0 +
j=1

{

arg min
0,Œ≤i
Œ≤i
1

n
‚àë
j=1

(yi

j ‚àí (Œ≤i

0 + Œ≤i

1xi

j))2}.

(3)

Note that on the same facade, there could be disjoint
predicted regions of the same Ô¨Çoor order; we have to
merge these regions before the regression.

(iii) ReÔ¨Ånement (Fig. 8 (c)): Polylines derived for the same
facade may not perfectly meet at a common VP, so we
regress location (xc, yc) for the common V Pc and reÔ¨Åne
each polyline as Œ≤i(x ‚àí xc) + yc by taking V Pc as an
anchor point. To do so, we formulate a gradient-based
optimization to update parameters {Œ≤i}m
i=1, xc, yc with a
global least squares:

arg min
{Œ≤i},xc,yc

{

m
‚àë
i=1

n
‚àë
j=1

(yi

j ‚àí Œ≤i(x ‚àí xc) ‚àí yc)2}.

(4)

(iv) Final result (Fig. 8 (d)): Last, we take the horizontal
and vertical ranges of each facade, to derive the two
endpoints (xi
e) for each polyline. Together
with the Ô¨Çoor order li, we can then obtain the Ô¨Åve-tuple
e, li} for each Ô¨Çoor-level line.
{xi

s) and (xi

e, yi

e, yi

s, xi

s, yi

s, yi

Fig. 7. Left is an example input image with annotated Ô¨Çoor orders. Middle
plots the Ô¨Çoor-level line pixel distributions in the input image, whereas right
plots the attention weights in the last height-attention layer.

¬ØMl

SpeciÔ¨Åcally, the HA layer bridges a lower-level feature map
f l ‚àà RCl √óWl √óHl and a higher-level feature
of Ô¨Çoor-level lines
map ¬ØMh
f l ‚àà RCh√óWh√óHh (note: subscripts l and h denotes lower-
level and higher-level, respectively) in the following ways:
(i) adopts a width-wise pooling to extract vertical features
from ¬ØMl
f l; (ii) employs a 1-D convolutional layer with a
bilinear interpolation to generate attention map A f l ‚àà RCh√óHh,
¬ØMh
f l; and
which shares the same channel and height size as
(iii) generates a reÔ¨Åned higher-level feature map ¬Ø¬ØMh
f l by an
element-wise multiplication of A f l and ¬ØMh
f l. In doing so, A f l
enriches the per-channel scaling factors for each individual
row of vertical positions, and the reÔ¨Åned higher-level feature
map ¬Ø¬ØMh
f l embodies the height-wise contextual information for
locating and also ordering the Ô¨Çoor-level lines.

To study the effectiveness of the HA mechanism, we extract
attention weights learned by the last HA layer and compare
them with Ô¨Çoor-level distributions in street-view images. Fig. 7
shows a typical comparison example using the input image in
the left. Here, we plot two heatmaps with Ô¨Çoor orders as the
horizontal axis and image-space height as the vertical axis. The
heatmaps are constructed in the following way. Horizontal axes
denote Ô¨Çoor order from 1 to 10, whereas vertical axes denote
vertical position (height) sub-divided into 10 vertical bounds
for visualization. For each cell (m, n), we count the pixels
in Ô¨Çoor-level m (middle) or extract the attention weight of
the m-th channel (right), inside the n-th vertical bound. Also,
we normalize the intensities (in red) in each map. Comparing
the two heatmaps, we can see that they exhibit very similar
patterns. Particularly, if we mark the most frequent height(s)
per Ô¨Çoor order (see the blue boxes) in both plots, we can see
that the attention weights reveal the heights of the pixels in
Ô¨Çoor-level lines of different orders. The comparison together
with the ablation analysis presented in Sec. VI-C demonstrate
the effectiveness of the HA mechanism.

C. Implementation of FloorLevel-Net

FloorLevel-Net adopts an encoder-decoder structure based
on DeepLabV3+ [6] with ResNet101 [17] as the backbone.
SpeciÔ¨Åcally, we employ four ResNet stages and one atrous
spatial pyramid pooling (ASPP) layer to generate the shared
features, and use Ô¨Åve convolutional layers in each of the de-
coders for facade segmentation and Ô¨Çoor-level lines detection.
In the line feature decoder, we arrange one height-attention
layer between each pair of adjacent convolutional layer, and
append a residual layer at the end to fuse the features from
the facade feature decoder. We use ReLU for all layers, except

12345678910last height-attention layerheight12345678910street-view image8

Fig. 8. Geometry-constrained post-processing for retrieving line parameters: grouping Ô¨Çoor-level lines based on facade segmentation (a), Ô¨Åtting polylines
according to line predictions (b), reÔ¨Åning vanishing point using geometry constraints (c), and the Ô¨Ånal output (d).

VI. EXPERIMENT
To the best of our knowledge, this work is the Ô¨Årst attempt
at recognizing Ô¨Çoor-level lines in street-view images. Hence,
there are no benchmark datasets, evaluation metrics, and
existing methods for the task. We Ô¨Åll the gaps by preparing
a new dataset (Sec. VI-A), proposing quantitative metrics for
evaluation (Sec. VI-B), and comparing with ablated techniques
(Sec. VI-C) and some closely-related techniques (Sec. VI-D
and Sec. VI-E). Further, we present some AR-related applica-
tions to demonstrate the applicability of this work (Sec. VI-F).

A. Testing Dataset

Besides the new training dataset prepared by using our data
augmentation scheme (see Sec. IV),nwe prepare a new testing
dataset for evaluating the effectiveness and generalizability
of our approach. Here, we choose street-view images by
considering the followings: (i) the image should contain at
least one (nearby) facade, which is not fully occluded; (ii)
the facades should contain at least one Ô¨Çoor-level line; (iii)
the facades can be perspective-oriented in view but cannot
be curved in the physical world; and (iv) the facades should
contain some semantic elements, e.g., windows or balconies.
We collect 600 street-view images for the testing dataset from
the following four different sources:

‚Ä¢ We randomly select 150 street views from the eTrims [21]
and arcDataset [43]. Each image includes only one single
facade, and the buildings are mostly low-rise.

‚Ä¢ We randomly select 150 more images from the LabelMe-
Facade [12], TSG-20 [2] and ZuBuD+ [32] datasets.
These images have more high-rise buildings, and some
have multiple facades.

‚Ä¢ We randomly download 200 GSV images in London
and Glasgow (UK), which feature typical European-style
buildings, similar to those in the CMP dataset.

‚Ä¢ Lastly, we randomly download another 100 GSV images
in Hong Kong (HK). These images feature high-rise
buildings, and many facades are partially occluded by
billboards, advertisements, etc.

On each test image, we manually label the position and
order of each Ô¨Çoor-level line on the facades in the form of a
quadrilateral region, e.g., see the ‚ÄúGT‚Äù row in Fig. 9. It takes
around 2 minutes to label one image and around 20 hours in
total for all the 600 images. The datasets are available on
the project website: https://wumengyangok.github.io/Project/
FloorLevelNet/.

B. Evaluation Metrics

As a two-stage method, our results include (i) predicted
pixel-wise segmentations from FloorLevel-Net, and (ii) re-
gressed endpoints (xi
e) and order li of each Ô¨Çoor-
e, yi
level line from the geometry post-processing. We employ the
following metrics to evaluate the two results separately:

s) & (xi

s, yi

‚Ä¢ Pixel-wise accuracy. For each street-view image, we
denote the set of pixels in ground-truth and in predicted
line regions as Pgt and Ppred, respectively. We employ
the F1 score for quantitative comparisons, which can
be measured upon the number of true positives (TP =
|Pgt ‚à© Ppred|), false positives (FP = |Ppred ‚àí Pgt ‚à© Ppred|),
and false negatives (FN = |Pgt ‚àí Pgt ‚à© Ppred|):

F1 = (2T P)/(2T P + FP + FN).

(5)

‚Ä¢ Line-wise accuracy. We join the endpoints of each Ô¨Çoor-
level line Li
to form a three-pixel wide straight line,
yielding a bag of pixels Pi. Then, we can compute the
conÔ¨Ådence of Li as being correctly recognized as

CI(Li) =

‚àëp‚ààPi 1(M f l(p) = li)
|Pi|

,

(6)

where 1(¬∑) is an indicator function for counting the
number of pixels in Pi that appears on the ground truth
image M f l with label li. If CI(Li) > 0.5, we regard the
line prediction as a true positive. For each street-view
image, we again count the number of Ô¨Çoor-level lines as
being correctly detected (TP), missed (FN), and wrongly
recognized (FP), then employ the F1 score (like Eq. (5))
to compute the line-wise accuracy.

C. Ablation Analysis

We carry out an ablation analysis to evaluate the major com-
ponents in our approach, including (i) the data augmentation
scheme, (ii) the multi-task learning, (iii) the height-attention
mechanism in FloorLevel-Net, and (iv) the geometry-post-
processing. In the analysis, we employ the testing dataset, in
which the images do not appear in the training data.
Ablated techniques. To evaluate components in FloorLevel-
Net, we take a stepwise testing strategy by adding the com-
ponents one by one to the baseline DeepLabV3+ [6] model
until we have our full architecture. The difference between
consecutive tests indicates the performance enhancement made

(b) Polyfit(c) Refinement(d) Final result(a) Line groupinghorizontalrangeVPc1239

Fig. 9. Qualitative comparison results in the ablation analysis on the unseen testing dataset. The results illustrate the effects of (i) our data augmentation scheme
by comparing DeeplabV3+ models trained on CMP and on our training data (green background), (ii) multi-task learning with and without the height-attention
mechanism (red background), and (iii) our full method further with geometry post-processing (yellow background), vs. the ground truths (GT).

by the added component. To start, we evaluate our data
augmentation scheme by the following pair of experiments:

‚Ä¢ Baseline+CMP: We train a DeepLabV3+ model (base-
line) using the facade dataset CMP [37] with rectiÔ¨Åed
facades (Fig. 3 (a1)) and our labels of Ô¨Çoor-level lines
(Fig. 3 (a4)).

‚Ä¢ Baseline+DataAugm: We train another DeepLabV3+
model on the training dataset of 3,000 pairs of augmented
street-view images (Fig. 3 (b2)) and labels of Ô¨Çoor-level
lines (Fig. 3 (b4)), which are produced by our data
augmentation scheme, as described in Sec. IV-A.

Next, we evaluate the multi-task learning framework and
height-attention mechanism in FloorLevel-Net by adding these
components to Baseline+DataAugm, as follows:

‚Ä¢ Multi-task: This refers to the multi-task learning network
that we build to simultaneously predict facades and Ô¨Çoor-
level lines, as described in Sec. V-A.

‚Ä¢ Multi-task+HA: We further incorporate a set of height-
attention layers, as described in Sec. V-B, to form the
full architecture of FloorLevel-Net.

Last, we arrive at Our Full Method by including the second-

stage geometry-post-processing to regress Ô¨Çoor-level lines.
Qualitative analysis. Fig. 9 shows the qualitative comparison
results. Here, we select one or two street-view images from
each constituting component in our testing dataset: eTrims
(1st column), arcDataset (2nd column), TSG-20 (3rd column),
LabelMeFacade (4th column), ZuBuD+ (5th column), GSVs
in UK (6th & 7th columns), and GSVs in Hong Kong (8th &
9th columns). The top row shows the input images, the bottom

row shows the ground-truth images, whereas each of the other
rows from the top show the prediction results obtained by the
ablated methods, i.e., Baseline+CMP, Baseline+DataAugm,
Multi-task, Multi-task+HA, and Our Full Method.

Overall, Baseline+DataAugm generates better predictions
than Baseline+CMP (comparing 2nd & 3rd rows in Fig. 9),
especially for street views with multiple facades and for
facades that are perspective-oriented, e.g., the GSV images
in UK & Hong Kong. The results indicate the effectiveness of
our data augmentation scheme in generating augmented street-
view images for network training.

Next, comparing 3rd & 4th rows Fig. 9, we can see that
Multi-task is able to produce more continuous and distinctive
segmentations of Ô¨Çoor-level lines than Baseline+DataAugm.
This shows that exploring the rich semantics in facades indeed
facilitates the recognition of Ô¨Çoor-level lines. Further, we can
see that the full architecture of FloorLevel-Net with the height-
attention mechanism (5th row in Fig. 9) can produce even
better results, especially in the prediction of Ô¨Çoor orders.
Particularly, the full architecture of FloorLevel-Net success-
fully predicts the Ô¨Çoor orders, even for challenging street-
view images shown in the 4th to 9th columns, whilst the
method fails in the absence of the height-attention mecha-
nism. Better predictions help simplify the line generation and
reÔ¨Ånement in the subsequent geometry post-processing stage.
BeneÔ¨Åting from the Ô¨Åne predictions by FloorLevel-Net, the
post-processing stage (6th row in Fig. 9) is able to deduce
satisfactory Ô¨Çoor-level line parameters, subject to the various
geometric constraints we discussed earlier.

ZuBuD+Multi-taskeTRIMSInputGTBaseline+ DataAugmLabelMeFacadeHK1UK1Our FullMethodMulti-taks+ HeightAttentionBaseline+ CMParcDatasetUK2HK2TSG-2010

TABLE II
QUANTITATIVE COMPARISON WITH THE ABLATED TECHNIQUES IN TERMS OF PIXEL-WISE ACCURACY ON NETWORK PREDICTION RESULTS, AND
LINE-WISE ACCURACY ON GEOMETRY POST-PROCESSING RESULTS. RESULTS COLORED IN BLUE ARE ASSOCIATED WITH OUR FULL METHOD.

Facades &
buildings

GSV in
UK & HK

Baseline + CMP
Baseline + DataAugm
Multi-task
Multi-task + HA
Baseline CMP
Baseline + DataAugm
Multi-task
Multi-task + HA

Lower Ô¨Çoors

Upper Ô¨Çoors

Overall

Pixel-wise
Accuracy
0.374
0.599
0.631
0.676
0.152
0.530
0.555
0.636

Line-wise
Accuracy
0.725
0.887
0.912
0.926
0.592
0.781
0.819
0.854

Pixel-wise
Accuracy
0.136
0.479
0.498
0.586
0.114
0.449
0.491
0.531

Line-wise
Accuracy
0.676
0.828
0.845
0.860
0.486
0.708
0.734
0.749

Pixel-wise
Accuracy
0.289
0.560
0.586
0.644
0.133
0.505
0.535
0.605

Line-wise
Accuracy
0.702
0.859
0.880
0.894
0.524
0.742
0.774
0.798

for those in HK, resulting in less accurate predictions. Second,
predictions for lower Ô¨Çoors have better accuracy than those
for higher Ô¨Çoor. This is probably because of relatively weaker
supervisions for upper Ô¨Çoor-level lines, as upper Ô¨Çoor-level
lines are typically further away from the camera than the
lower ones and there are relatively less contextual information.
Third, the data augmentation scheme, multi-task learning, and
height-attention layers gradually contribute to the performance
enhancement, yet there are still much room for improvement
in terms of pixel-wise accuracy. The result
indicates that
Ô¨Çoor-level line recognition is more challenging compared with
general urban scene segmentation tasks. Nevertheless, our
predictions provide enough information for the geometry post-
processing, which can correctly recognize and locate most
Ô¨Çoor-level lines in terms of line-wise accuracy.

D. Comparison with a Variant of DeepFacade

As there are no previous works on locating Ô¨Çoor-level
lines, we consider those on facade parsing, i.e., segmenting a
building facade into windows, doors, etc., which can then serve
as references for locating Ô¨Çoor-level lines, as competitors.
Here, we compare with DeepFacade [24], which is a state-
of-the-art deep-learning-based method for facade parsing. In
detail, we extract Ô¨Çoor-level lines from their results as follows:
(i) use RANSAC [5] to rectify a street-view image and record
the associated homographic matrix; (ii) apply DeepFacade [24]
on the rectiÔ¨Åed image to produce a facade segmentation map,
and extract only the window, door, and shop regions as positive
regions in the map; (iii) mark a Ô¨Çoor-level line in-between each
pair of adjacent (piecewise) positive regions; and (iv) apply
the inverse of the homographic matrix to revert the original
positions of the detected lines to obtain the Ô¨Ånal results. From
now, we refer the above steps as DeepFacade-Variant.

This

experiment

compares Our Full Method with
DeepFacade-Variant. However, we Ô¨Ånd that DeepFacade-
Variant can hardly produce satisfactory results for GSVs in
UK and HK (see supplemental material for examples) for
two reasons: (i) RANSAC rectiÔ¨Åcation is constrained to street
views with only a single facade, but street views in UK
and HK often contain multiple facades, leading to multiple
vanishing points detected and unstable rectiÔ¨Åcation results;
(ii) buildings in modern cities may contain facades without
obvious line segments, thereby making it hard to deduce the
vanishing points and perform the rectiÔ¨Åcation. Hence, we
report the results only on 300 testing images from the eTrims,

Fig. 10. Comparison between our approach and DeepFacade-Variant on
orthogonal (left two columns) and perspective (right two columns) facades.
Both methods produce satisfactory results for facades that are orthogonal to
camera view, whilst our approach is more robust to facades in perspective
view, and multiple facades in one image (last row).

Quantitative analysis. Further, we conduct a quantitative
evaluation for each ablated technique. Here, we categorize the
testing dataset into two groups: images from datasets focusing
on buildings or facades (eTrims, LabelmeFacade, etc.) are
relatively easier for recognizing Ô¨Çoor-level lines as the images
are prepared for facade parsing or building classiÔ¨Åcation task
with regular facade elements, whereas GSVs in UK & HK with
complex environments and dynamic attributes are harder for
recognition. We would also like to evaluate the performance
of our approach for different Ô¨Çoor-level lines, as many AR
applications need only Ô¨Çoor information close to the camera.
To do so, we categorize Ô¨Çoors 1-3 as lower Ô¨Çoors since most
of these Ô¨Çoor pixels are in the low bound of image space, and
the remaining Ô¨Çoors as upper Ô¨Çoors.

We measure the pixel-wise accuracy on the ablation network
outputs, and line-wise accuracy on the line parameters after ge-
ometry post-processing. The quantitative results are presented
in Table II, in which our full method, Multi-task + HA with
geometry post-processing, is marked in blue. Based on the
results, we can derive some interesting observations. First, the
overall accuracy drops when scene complexity increases from
simple street views in facade datasets to GSVs in UK and HK.
There are more facades and higher Ô¨Çoors in GSVs, especially

DeepFacade-VariantDeepFacade-VariantOursOursOrthogonal facadesPerspective-oriented facades11

Fig. 11. The potential of our approach to support and enrich various AR scenarios, e.g., navigation, advertisement, etc.

TABLE III
QUANTITATIVE COMPARISON BETWEEN OUR APPROACH AND
DeepFacade-Variant IN TERMS OF ACCURACY AND TIME EFFICIENCY.

TABLE IV
QUANTITATIVE COMPARISON BETWEEN OUR METHOD AND IMAGE
SEGMENTATION BASELINES IN TERMS OF PIXEL-WISE ACCURACY.

DeepFacade-Variant
Ours

Line-wise
accuracy
0.721
0.876

Recti-
Ô¨Åcation
1.08s
-

Network
inference
0.06s
0.05s

Post-
processing
0.04s
0.14s

SUM

1.18s
0.19s

arcDataset, LabelMeFacade, TSG-20, and ZuBuD+ datasets,
which contain only one building facade in most of the images.
Fig. 10 shows typical results produced by our approach
and DeepFacade-Variant. From the results shown on the left
(1st & 2nd columns), we can see that both methods produce
good results for facades that are orthogonal to camera view.
Yet, DeepFacade-Variant is prone to fail when the improper
homographic matrix inferred by RANSAC (see row 2 for an
example), whilst our approach overcomes the limitation with
the reÔ¨Ånement made by geometry constraints. More failure
cases for DeepFacade-Variant are presented in 3rd & 4th
columns, where the facades are in perspective view. Input
image of row 1 is not rectiÔ¨Åed properly, hence the interpolated
lines point to wrong directions. In rows 2, windows on up
Ô¨Çoors have little saliency due to distortion, thus the results
miss Ô¨Çoor-level lines on high levels. The building in row 3
has partial irregular facade textures, such as lintel and stairs,
and the method wrongly recognizes adjacent Ô¨Çoors. In the last
row, there are two facades, whilst the rectiÔ¨Åcation cannot be
managed by one single homographic matrix. In all these cases,
our approach produces good predictions.

Table III quantitatively compares the results of DeepFacade-
Variant and Our Full Method. From the line-wise accuracies,
we can see an obvious advancement of our approach over
the competitor. Also, our approach outperforms DeepFacade-
Variant in terms of execution time, in which the total running
time is only 0.19 seconds vs. 1.18 seconds. In DeepFacade-
Variant, a major bottleneck is the heavy time cost of RANSAC
rectiÔ¨Åcation, whilst our approach directly predicts pixelwise
segmentation without requiring rectiÔ¨Åcation.

Datasets

Facades &
buildings

GSV in
UK & HK

Segmentation methods
PSPNet + CMP
PSPNet + DataAugm
OCR + CMP
OCR + DataAugm
Ours
PSPNet + CMP
PSPNet + DataAugm
OCR + CMP
OCR + DataAugm
Ours

Lower
0.258
0.649
0.379
0.610
0.676
0.081
0.369
0.083
0.626
0.636

Upper
0.059
0.528
0.254
0.492
0.586
0.001
0.205
0.016
0.474
0.531

Overall
0.197
0.609
0.330
0.570
0.644
0.067
0.321
0.062
0.581
0.605

E. Comparison with Semantic Segmentation

As the pixel-wise Ô¨Çoor segmentation can be formulated
as an image segmentation task, we also present comparison
with semantic segmentation methods on the pixel accuracy
of the intermediate results. Here we choose PSPNet [52]
and OCR [47] as baselines, since PSPNet is widely used as
baseline for semantic segmentation task, and OCR is one of the
state-of-the-art methods with highest pixel accuracy in some
urban semantic segmentation datasets. For each method, we
train the model with both CMP dataset and our augmented
dataset, and test with 600 real images following the same
settings as the ablation study in Sec. VI-C.

Table IV presents quantitative results in terms of pixel-
wise accuracy. Similar to the results of ablation analysis,
the low overall pixel accuracy by the comparison methods
trained on CMP dataset indicates the difÔ¨Åculty of inferring
Ô¨Çoor-level lines, especially without proper dataset. Our data
augmentation signiÔ¨Åcantly improves the prediction accuracy to
an acceptable level for both segmentation methods, showing
well generalizability of the baseline network architectures.
Moreover, our methods with multi-task learning and height-
attention mechanism achieves the best performance, for both
the facades & buildings datasets and GSVs in UK & HK.

Our resultNavigationAdvertisementTourismHealthy LifeEntertainment12

Discussion. This work targets at inferring Ô¨Çoor-level lines
in street-view images. We proposed the data augmentation
scheme and FloorLevel-Net to overcome the challenges of
being lack of training dataset and suitable network archi-
tecture. Though the methods are dedicated to the speciÔ¨Åc
task, we believe the ideas of augmenting dataset and tuning
network based on data characteristics can be feasibly extended
to some other applications, where labeling is expensive and
data have certain patterns. For instance, in autonomous driving,
a key requirement is to detect driving lanes and pedestrians
simultaneously. In such scenarios, we can possibly improve the
network performance by carefully modeling the relationship
between driving lane and pedestrian.
Limitations. First, our method could be disturbed by obstacles
at critical regions, e.g., a cab that occludes the ground Ô¨Çoor
of a building (Fig. 12 (left)) may cause a wrongly-recognized
ground Ô¨Çoor. Second, our method cannot handle facades that
are reÔ¨Çective (Fig. 12 (middle)). Third, when the camera is
too close to the facades (Fig. 12 (right)), our method cannot
get a wide view of the facades, so it may wrongly recognize
the upper and lower bounds of a Ô¨Çoor-level line region as two
successive Ô¨Çoor-lever lines.
Future works. We plan to focus on improving the prediction
precision and efÔ¨Åciency of our approach. First, a possible
direction is to work on short video clips instead of on single
images, so that we can obtain temporal information, e.g., struc-
ture from motion (SfM) features, to improve the prediction
precision. Second, we plan to develop strategies to estimate the
distance from camera to facades to address the last limitation
mentioned above. Third, our current pipeline takes around 0.19
seconds (see Table III), which is not slow compared with
DeepFacade-Variant but insufÔ¨Åcient to run in real-time. At
present, the main bottleneck is the post-processing module,
which can be improved through multiprocessing implemen-
tations, since we can simultaneously process line grouping
and polyÔ¨Åt. Also, we plan to explore lightweight networks
to further improve the pipeline efÔ¨Åciency.

ACKNOWLEDGMENT

We thank the street-view images from the Google Street
View service. This work is supported partially by the Research
Grants Council of the Hong Kong Special Administrative
Region (Project no. CUHK 14206320) and Guangdong Basic
and Applied Basic Research Foundation (2021A1515011700).

REFERENCES

[1] Google‚Äôs street view static API. https://developers.google.com/maps/

documentation/streetview. Accessed: 2020-10-01. 4

[2] Tourist sights graz image database. http://dib.joanneum.at/cape/TSG-20/.

Accessed: 2021-04-12. 8

[3] D. Anguelov, C. Dulong, D. Filip, C. Frueh, S. Lafon, R. Lyon, A. Ogale,
L. Vincent, and J. Weaver. Google street view: Capturing the world at
street level. Computer, 43(6):32‚Äì38, 2010. 2

[4] O. Barinova, V. Konushin, A. Yakubenko, K. Lee, H. Lim, and
A. Konushin. Fast automatic single-view 3-D reconstruction of urban
scenes. In Proceedings of the European Conference on Computer Vision
(ECCV), pages 100‚Äì113, 2008. 1

[5] K. Chaudhury, S. DiVerdi, and S. Ioffe. Auto-rectiÔ¨Åcation of user
photos. In Proceedings of the IEEE International Conference on Image
Processing (ICIP), pages 3479‚Äì3483, 2014. 10

Fig. 12. Limitations of our current approach: obstacle interference (left), glass
reÔ¨Çection (middle), and facades too close to camera (right).

F. Applicability

Fig. 11 shows some example usage scenarios that demon-
strates the potential of our method in enriching AR-related
applications. Enabled by the recognized Ô¨Çoor-level lines, we
can augment real-world scenes with virtual contents in a
Ô¨Çoor-aware manner. First, from the Ô¨Çoor-level line results,
we can identify the bound of each facade, i.e., the lowest
and highest Ô¨Çoor-level lines (and their locations), and overlay
virtual contents that cover the whole facade, while skipping the
real contents on the ground level (Fig. 11 (top)). By this means,
we may deliver AR contents, without obscuring necessary real
contents around the user on the street.

More importantly, with the Ô¨Çoor-level lines, we can place
context-aware information on speciÔ¨Åc Ô¨Çoors. For example,
in the bottom-row result on the 2nd column of Fig. 11, we
speciÔ¨Åcally mark the location of the target room for convenient
navigation; and in the middle-row result on the 3rd column
of Fig. 11, we put shopping directory information over the
corresponding Ô¨Çoors to aid users to Ô¨Ånd the user‚Äôs target
items in the department store. Besides, we may mark speciÔ¨Åc
apartments in a building that are for sales or for rent to aid
the property agents. On the other hand, we may also employ
our approach to present Ô¨Çoor-related data (e.g., Ô¨Çoor size,
occupancy information, etc.) on buildings and extend it to
estimate the number of Ô¨Çoor levels and infer the building
heights, which could be helpful for land usage analysis and
urban planning [33], [42], [48].

VII. CONCLUSION, DISCUSSION, AND FUTURE WORK

We presented a new approach for recognizing Ô¨Çoor-level
lines in street-view images. As a Ô¨Årst attempt to this challeng-
ing task, we contribute to (i) devising a new data augmentation
scheme that leverages an existing facade dataset to efÔ¨Åciently
generate data samples for network training; and (ii) formulat-
ing FloorLevel-Net, a new multi-task learning framework that
associates explicit features of facades and implicit Ô¨Çoor-level
lines and incorporates a height-attention mechanism to enforce
a vertical ordering of Ô¨Çoor-level lines. The pixelwise semantic
segmentations by FloorLevel-Net are further reÔ¨Åned through a
geometry post-processing module for plausible and consistent
reconstruction of Ô¨Çoor-level lines. Quantitative and qualitative
comparisons with existing methods demonstrate the effective-
ness of our proposed method. Further, we demonstrate the
potential of our approach in supporting various AR scenarios.

[6] L.-C. Chen, Y. Zhu, G. Papandreou, F. Schroff, and H. Adam. Encoder-
decoder with atrous separable convolution for semantic image segmen-
tation. In Proceedings of the European Conference on Computer Vision
(ECCV), pages 801‚Äì818, 2018. 2, 7, 8

[7] Z. Chen, J. Zhang, and D. Tao. Recursive context routing for object
detection. International Journal of Computer Vision, 129(1):142‚Äì160,
2021. 3

[8] S. Choi, J. T. Kim, and J. Choo. Cars can‚Äôt Ô¨Çy up in the sky: Improving
In
urban-scene segmentation via height-driven attention networks.
Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), pages 9373‚Äì9383, 2020. 1, 3, 6

[9] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Be-
nenson, U. Franke, S. Roth, and B. Schiele. The Cityscapes dataset
In Proceedings of the IEEE
for semantic urban scene understanding.
Conference on Computer Vision and Pattern Recognition (CVPR), pages
3213‚Äì3223, 2016. 2, 3, 6

[10] A. Criminisi, I. Reid, and A. Zisserman.

Single view metrology.

International Journal of Computer Vision, 40(2):123‚Äì148, 2000. 1

[11] D. Eigen and R. Fergus.

Predicting depth, surface normals and
semantic labels with a common multi-scale convolutional architecture. In
Proceedings of the IEEE International Conference on Computer Vision
(ICCV), pages 2650‚Äì2658, 2015. 3

[12] B. Fr√∂hlich, E. Rodner, and J. Denzler. A fast approach for pixelwise la-
beling of facade images. In Proceedings of the International Conference
on Pattern Recognition (ICPR), pages 3029‚Äì3032, 2010. 2, 8

[13] A. Geiger, P. Lenz, and R. Urtasun. Are we ready for autonomous
In Proceedings of the
driving? The KITTI vision benchmark suite.
IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
pages 3354‚Äì3361, 2012. 2

[14] R. Grompone von Gioi, J. Jakubowicz, J.-M. Morel, and G. Randall.
LSD: a Line Segment Detector. Image Processing On Line, 2:35‚Äì55,
2012. 1

[15] A. Gupta, A. A. Efros, and M. Hebert. Blocks world revisited: Image
understanding using qualitative geometry and mechanics. In Proceedings
of the European Conference on Computer Vision (ECCV), pages 482‚Äì
496, 2010. 1

[16] O. Haines and A. Calway. Recognising planes in a single image. IEEE
Transactions on Pattern Analysis and Machine Intelligence, 37(9):1849‚Äì
1861, 2015. 2

[17] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), pages 770‚Äì778, 2016. 7

[18] D. Hoiem, A. A. Efros, and M. Hebert. Recovering surface layout from
International Journal of Computer Vision, 75(1):151‚Äì172,

an image.
2007. 2

[19] J. Hu, L. Shen, and G. Sun. Squeeze-and-excitation networks.

In
Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), pages 7132‚Äì7141, 2018. 3

[20] Z. Huang, X. Wang, L. Huang, C. Huang, Y. Wei, and W. Liu. CCNet:
Criss-cross attention for semantic segmentation. In Proceedings of the
IEEE International Conference on Computer Vision (ICCV), pages 603‚Äì
612, 2019. 3

[21] F. KorÀác and W. F√∂rstner.

eTRIMS Image Database for interpreting
images of man-made scenes. Technical Report TR-IGG-P-2009-01,
April 2009. 8

[22] J.-T. Lee, H.-U. Kim, C. Lee, and C.-S. Kim. Semantic line detection and
its applications. In Proceedings of the IEEE International Conference
on Computer Vision (ICCV), pages 3229‚Äì3237, 2017. 1

[23] H. Liu, Y. Xu, J. Zhang, J. Zhu, Y. Li, and C. H. S. Hoi. DeepFacade:
A deep learning approach to facade parsing with symmetric loss. IEEE
Transactions on Multimedia, pages 1‚Äì1, 2020. 2

[24] H. Liu, J. Zhang, J. Zhu, and S. C. H. Hoi. DeepFacade: A deep learning
approach to facade parsing. In Proceedings of the International Joint
Conference on ArtiÔ¨Åcial Intelligence (IJCAI), pages 2301‚Äì2307, 2017.
2, 10

[25] X. Liu, Y. Zhao, and S. Zhu. Single-view 3D scene reconstruction and
parsing by attribute grammar. IEEE Transactions on Pattern Analysis
and Machine Intelligence, 40(3):710‚Äì725, 2018. 3

[26] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional networks
for semantic segmentation. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), pages 3431‚Äì3440,
2015. 2

[27] A. Martinovi¬¥c, M. Mathias, and L. Van Gool. ATLAS: A three-layered
approach to facade parsing. International Journal of Computer Vision,
118(1):22‚Äì48, 2016. 1, 2

13

[28] P. Mueller, G. Zeng, P. Wonka, and L. Van Gool. Image-based procedural
modeling of facades. ACM Trans. Graph. (SIGGRAPH), 26(3):85:1‚Äì
85:10, 2007. 2

[29] P. Musialski, P. Wonka, D. G. Aliaga, M. Wimmer, L. Van Gool, and
W. Purgathofer. A survey of urban reconstruction. Computer Graphics
Forum (Eurographics), 32(6):146‚Äì177, 2013. 1

[30] Z. Ren and Y. J. Lee. Cross-domain self-supervised multi-task feature
learning using synthetic imagery. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), pages 762‚Äì771,
2018. 3

[31] G. Schindler, P. Krishnamurthy, R. Lublinerman, Y. Liu, and F. Dellaert.
Detecting and matching repeated patterns for automatic geo-tagging
In Proceedings of the IEEE Conference on
in urban environments.
Computer Vision and Pattern Recognition (CVPR), pages 1‚Äì7, 2008.
1, 2

[32] H. Shao, T. Svoboda, and L. Van Gool. Zubud-zurich buildings database
for image based recognition. Computer Vision Lab, Swiss Federal
Institute of Technology, Switzerland, Tech. Rep, 260(20):6, 2003. 8
[33] Q. Shen, W. Zeng, Y. Ye, S. Mueller Arisona, S. Schubiger, R. Burkhard,
and H. Qu. StreetVizor: Visual exploration of human-scale urban forms
based on street views. IEEE Transactions on Visualization and Computer
Graphics, 24(1):1004 ‚Äì 1013, 2018. 12

[34] G. Simon, A. Fond, and M.-O. Berger. A-contrario horizon-Ô¨Årst vanish-
ing point detection using second-order grouping laws. In Proceedings of
the European Conference on Computer Vision (ECCV), pages 323‚Äì338,
2018. 1

[35] O. Teboul, I. Kokkinos, L. Simon, P. Koutsourakis, and N. Paragios.
In Proceedings
Shape grammar parsing via reinforcement learning.
of the IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), pages 2273‚Äì2280, 2011. 2

[36] O. Teboul, L. Simon, P. Koutsourakis, and N. Paragios. Segmentation
of building facades using procedural shape priors. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
pages 3105‚Äì3112, 2010. 2

[37] R. TyleÀácek and R. ≈†√°ra. Spatial pattern templates for recognition of
In Proceedings of German Conference

objects with regular structure.
on Pattern Recognition (GCPR), pages 364‚Äì374, 2013. 2, 3, 4, 9
[38] X. Wang, D. Fouhey, and A. Gupta. Designing deep networks for surface
normal estimation. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), pages 539‚Äì547, 2015. 3
[39] J. Weissenberg, H. Riemenschneider, M. Prasad, and L. Van Gool. Is
there a procedural logic to architecture? In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), pages
185‚Äì192, 2013. 2

[40] S. Woo, J. Park, J.-Y. Lee, and I. S. Kweon. CBAM: Convolutional
block attention module. In Proceedings of the European Conference on
Computer Vision (ECCV), pages 3‚Äì19, 2018. 3

[41] C. Wu, J.-M. Frahm, and M. Pollefeys. Repetition-based dense single-
the IEEE Conference on
In Proceedings of
view reconstruction.
Computer Vision and Pattern Recognition (CVPR), pages 3113‚Äì3120,
2011. 1, 2

[42] Y. Wu, L. S. Blunden, and A. S. Bahaj. City-wide building height
determination using light detection and ranging data. Environment and
Planning B: Urban Analytics and City Science, 46(9):1741‚Äì1755, 2018.
1, 12

[43] Z. Xu, D. Tao, Y. Zhang, J. Wu, and A. C. Tsoi. Architectural
style classiÔ¨Åcation using multinomial
In
Proceedings of the European Conference on Computer Vision (ECCV),
pages 600‚Äì615, 2014. 8

logistic regression.

latent

[44] C. Yang, T. Han, L. Quan, and C. Tai. Parsing fa√ßade with rank-one
In Proceedings of the IEEE Conference on Computer
approximation.
Vision and Pattern Recognition (CVPR), pages 1720‚Äì1727, 2012. 1, 2
[45] F. Yang and Z. Zhou. Recovering 3D planes from a single image
In Proceedings of the European

via convolutional neural networks.
Conference on Computer Vision (ECCV), pages 87‚Äì103, 2018. 1
[46] C. Yu, J. Wang, C. Peng, C. Gao, G. Yu, and N. Sang. Learning a dis-
criminative feature network for semantic segmentation. In Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), pages 1857‚Äì1866, 2018. 3

[47] Y. Yuan, X. Chen, and J. Wang. Object-contextual representations for
semantic segmentation. In Proceedings of the European Conference on
Computer Vision (ECCV), pages 173‚Äì190, 2020. 11

[48] W. Zeng and Y. Ye. VitalVizor: A visual analytics system for studying
IEEE Computer Graphics and Applications, 38:38‚Äì53,

urban vitality.
09 2018. 1, 12

14

[49] Z. Zeng, M. Wu, W. Zeng, and C.-W. Fu. Deep recognition of
vanishing-point-constrained building planes in urban street views. IEEE
Transactions on Image Processing, 29:5912‚Äì5923, 2020. 1, 2

[50] J. Zhang, Y. Xu, B. Ni, and Z. Duan. Geometric constrained joint
In Proceedings of
lane segmentation and lane boundary detection.
the European Conference on Computer Vision (ECCV), pages 502‚Äì518,
2018. 2

[51] H. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia. Pyramid scene parsing
network. In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), pages 2881‚Äì2890, 2017. 2

[52] H. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia. Pyramid scene parsing
network. In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), 2017. 11

Mengyang Wu received the B.S. degree from Uni-
versity College London. He is currently a PhD
student in Computer Science and Engineering of
Chinese University of Hong Kong. His recent re-
search interests include deep learning for 3D vision,
scene understanding, and outdoor augmented reality.

Wei Zeng is currently an associate professor at
Shenzhen Institute of Advanced Technology, Chi-
nese Academy of Sciences. He received his bach-
elor‚Äôs degree (2011) and Ph.D. (2015), both in
computer science, from Nanyang Technological Uni-
versity. He served as co-chairs for PaciÔ¨ÅcVis Poster,
and program committee members for various re-
search conferences, including IEEE Visualization,
ChinaVis, IVAPP. His recent research interests in-
clude data visualization and AR/VR, and their ap-
plications in smart city.

Chi-Wing Fu is currently an associate professor in
the Chinese University of Hong Kong. He served
as the co-chair of SIGGRAPH ASIA 2016‚Äôs Tech-
nical Brief and Poster program, associate editor of
IEEE Computer Graphics & Applications and Com-
puter Graphics Forum, panel member in SIGGRAPH
2019 Doctoral Consortium, and program committee
members in various research conferences, including
SIGGRAPH Asia Technical Brief, SIGGRAPH Asia
Emerging tech., IEEE visualization, CVPR, IEEE
VR, VRST, PaciÔ¨Åc Graphics, GMP, etc. His recent
research interests include computation fabrication, point cloud processing, 3D
computer vision, user interaction, and data visualization.

