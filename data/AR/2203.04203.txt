2
2
0
2

l
u
J

0
2

]

V
C
.
s
c
[

5
v
3
0
2
4
0
.
3
0
2
2
:
v
i
X
r
a

AssistQ: Affordance-centric Question-driven
Task Completion for Egocentric Assistant

Benita Wong⋆, Joya Chen∗, You Wu∗, Stan Weixian Lei,
Dongxing Mao, Difei Gao, and Mike Zheng Shou†

Show Lab, National University of Singapore
benitawong@u.nus.edu {joyachen97,mike.zheng.shou}@gmail.com

Abstract. A long-standing goal of intelligent assistants such as AR
glasses/robots has been to assist users in affordance-centric real-world
scenarios, such as “how can I run the microwave for 1 minute?”. How-
ever, there is still no clear task definition and suitable benchmarks. In
this paper, we define a new task called Affordance-centric Question-
driven Task Completion, where the AI assistant should learn from in-
structional videos to provide step-by-step help in the user’s view. To
support the task, we constructed AssistQ, a new dataset comprising 531
question-answer samples from 100 newly filmed instructional videos. We
also developed a novel Question-to-Actions (Q2A) model to address the
AQTC task and validate it on the AssistQ dataset. The results show
that our model significantly outperforms several VQA-related baselines
while still having large room for improvement. We expect our task and
dataset to advance Egocentric AI Assistant’s development. Our project
page is available at: https://showlab.github.io/assistq/.

Keywords: affordance-centric, egocentric AI, question-answering

1

Introduction

People often require assistance when dealing with new events. Consider the ex-
ample in Figure 1: the user comes across an unfamiliar washing machine and
wants to start a cotton wash, but he does not know how to operate it. He may
search for the device’s instructional video, and experiment with buttons on the
machine. These actions are time-consuming and may not address the user’s ques-
tion effectively. This example highlights the need for an intelligent assistant to
help us with affordance-centric queries. The intelligent assistant should: (1) un-
derstand the user’s query and view, (2) learn from instructional video/manual,
(3) guide the user to achieve his goal.

However, few works support the development of such an assistant. First,
AI assistants should deal with affordance-centric queries, while current visual
question answering (VQA) benchmarks [3,17,19,23,26,36] focus on fact-based
questions and answers. Second, most ground-truth answers in VQA benchmarks

⋆ Equal Contribution. † Corresponding Author.

 
 
 
 
 
 
2

Affordance-centric Question-driven Task Completion

Fig. 1. An illustration of an AI assistant for affordance-centric questions. The AI as-
sistant on AR glass can guide the user to complete the intended task.

are single-step and textual, which fails to express detailed instructions to the
user’s complex question. Even though visual dialog [1,12] (VDialog) introduces
sequence question answering, the answer of each dialog round is still single-step
and textual. Finally, AI assistants should solve questions in users’ egocentric per-
spective [20,29,32,40,43], which is more natural and meaningful for understand-
ing users’ situations. However, existing VQA and VDialog studies mainly com-
prise third-person web videos. Although embodied QA [9,44] (EQA) attempts
to develop egocentric agents, they are mostly focused on virtual environments,
which may be difficult to generalize to real-world scenarios, and they are also
incapable of dealing with complex, affordance-centric user queries.

Hence, we propose a novel Affordance-centric Question-driven Task
Completion (AQTC) task. It presents a unique set-up: the user asks a ques-
tion in his/her view, and then the AI assistant answers in a sequence of actions.
Fiugre 2 shows the differences between our task and VQA, VDialog, and EQA.
Unlike VQA and VDialog, where the question is fact-based and the answer is
a short text, our question is task-oriented and our answer is multi-modal and
multi-step, which is more challenging and closer to the intelligent assistant set-
ting. Furthermore, our task makes use of new content – real-world, egocentric
videos – for question answering, which is more suitable for real-world applica-
tions compared to virtual environments in the EQA.

Affordance-centric Question-driven Task Completion

3

Fig. 2. The differences between VQA, VDialog, EQA, and our AQTC task.

To support the AQTC task, we collected the AssistQ dataset containing
531 multiple-choice QA samples on 100 newly filmed, with an average duration
of 115s egocentric videos. Participants recorded instructional videos by them-
selves (e.g., operating microwaves, washing machine), and provided scripts of
their narration. These instructional videos were used to create affordance-centric,
multi-step QA samples. We also developed a Question-to-Actions (Q2A) model
to address the AQTC task. It uses a context grounding module to infer from
multi-modal cues (video, script, user view), and introduces a steps network to
generate step-by-step answers. Experimental results show that our Q2A model
is especially suitable for the AQTC task. In summary, our contributions are:

• We proposed a new task, namely Affordance-centric Question-driven Task
Completion (AQTC), whereby AI assistants should learn from instructional
videos to guide users step-by-step in the users’ view.

• We constructed a pilot dataset for the task, comprising 531 question-answer

samples derived from 100 newly filmed instructional videos.

• We developed a Question-to-Actions (Q2A) model and performed extensive

experiments to show its superiority in the AQTC task.

2 Related Work

Visual Question Answering. The goal of VQA is to answer questions based on
visual cues. VQA v1 and v2 [3,19] are standard benchmarks for image QA. TGIF-
QA [23] is one of the earlier large-scale video QA datasets and was built from
short animated videos (GIFs) taken from Tumblr. In this task, the main goal is to
recognize actions and their repetitions. Similar video QA datasets [17,26,27,36]
mainly illustrate daily life or story. In contrast, our task focuses on instructional
videos more suitable for affordance-centric tasks.
Visual Dialog. VDialog [12] is a task to generate natural responses based on the
given image/video and the dialog context. An example is Audio Visual Scene-
Aware Dialog (AVSD) [1], where the agent is shown a video of indoor activity and

AssistQ (Ours)Turn clockwise to 2minPress here to startQ: How can I start heating for 2min?A:Visual Question AnsweringQ: What room was Wilson breaking into when House found him?A: The kitchenQ1: how long is the video?A1: It is 33 seconds long.Q2:How many people are in the video?A2: Just one person, a woman.Visual DialogQ: What room is the vase located in?A: [Go straight] -> LobbyEmbodied Question AnsweringFactoid text questionFactoid text questionsAffordance-centricFactoid text questionVirtual environmentDaily activitiesDaily activitiesInstructional, real-worldSequence of actionsMulti-modal actionsSequence of actionsSingle-step answerSingle-step answer to each question4

Affordance-centric Question-driven Task Completion

answers multiple questions about actions and events in the video. Compared to
models developed for single-step VQA tasks, VDialog models should utilize the
dialog history. Some models [1,12,28,33] simply splice historical dialogs or model
using RNN, while others [13,25] introduce reinforcement learning methods. Like
VDialog, historical actions in our AQTC task may affect the current answer step.
But their differences are apparent: Visual Dialog is more like a single-step QA
problem in each round, whereas AQTC requires a sequence of actions to answer.
We believe the latter is more appropriate for affordance-centric questions.
Embodied Question Answering. If we consider the instructional video in
our AQTC task as the environment, then our model should explore the envi-
ronment to answer the question. This is similar to EQA [9], where the agent
needs to explore the scene and answer the question. Recent works try to solve
or extend EQA by improving the logical reasoning [11,44], applying multi-agent
cooperation [10,22,44], or creating more challenging environments [18]. However,
the current setting of EQA focuses on virtual environments, while our proposed
AQTC task revolves around real-world images and videos. This allows AQTC
to be more readily adopted for practical applications, such as intelligent agents
built in portable devices (e.g., mobile phones, AR glasses).

3 Affordance-centric Question-driven Task Completion

In this section, we propose Affordance-centric Question-driven Task Completion
(AQTC), a novel task aimed at driving the evolution of egocentric AI assistants.
We begin by outlining the abilities required of an egocentric, user-helping AI
assistant. The AQTC work is then formalized in accordance with these abilities.
Finally, we talk about how to generalize and apply the task.

3.1 Required Abilities of Egocentric AI Assistants

Handling multi-modal inputs. In affordance-centric settings, the user often
asks questions in both visual (e.g., his/her current view) and linguistic (e.g.,
”how to start the microwave?”). As a result, the egocentric AI assistant is ex-
pected to handle multimodal inputs.
Learning from instructional sources. It is difficult for an AI assistant to
answer questions without learning anything. An acceptable way is for the AI
assistant to read the instructional manual first and then teach users. We no-
tice that there are various instructional videos available on the internet, and it
would be preferable if AI assistants could utilize these data, i.e., learning from
instructional videos and then guide users in their situation.
Grounding between modalities. The different input modalities are often not
grounded with respect to each other. For example, a video guide might point to
a button, whereas the corresponding textural guide may say “Press this”. The
egocentric AI assistant should know how to link visual and language concepts.
Multi-step interaction with users. Interaction with the user is necessary for
an egocentric AI assistant. For example, if a user inquires about ”how to start

Affordance-centric Question-driven Task Completion

5

Tasks
Property
Video QA
Factoid
Factoid
Video Dialog
Embodied QA Factoid

Model Output
Single textual answer
Multi textual answers
Multi actions, Single textual answer
Affordance Reference Video, User Situation Multi <action, textual answer> pairs

Model Input
Reference Video
Reference Video
User Situation

AQTC

Table 1. Comparisons with VQA-related tasks.

the microwave,” the assistant should tell him/her the location of the power
switch. When a user’s inquiry is complex, the interaction between the user and
the assistant may be held for several steps.

3.2 Task Definition of AQTC

Now we design a task for the abilities discussed in Section 3.1. To begin, the
task should include multi-modal inputs, such as the user’s egocentric view and
textural question. Then, there should be a reference instructional video from
which AI assistants can learn to address problems. Furthermore, rather than
using different modalities independently, the grounding between them should be
necessary to handle the task. Finally, the task should reflect multi-step interac-
tions between users and assistants, such as a sequence of actions taught to users
by the assistant.

With these considerations, we propose affordance-centric question-driven task
completion (AQTC) task. Given an instructional video (V ), the corresponding
video’s textural script (SV ), the user’s view (U ), and the question (QU ) asked
from the view, the model should answer the question by a sequence of answers
(A1, A2, ..., AI ) in I (I >= 1) steps. Note that the question QU is proposed un-
der the user view U rather than the instructional video V , so the model should
ground between U and V (with SV ) to answer QU . Currently, we set candidate
answers (Ai
j denotes the j-th
potential answer in the i-th step.

j) to lower the difficulties of answering, where Ai

Comparing with existing VQA-related tasks (e.g., VQA [3,26,36], VDia-
log [1,12], EQA [9]), our proposed AQTC task is more suitable in supporting the
affordance-centric requirements of AI assistants. We list the reasons in Table 1:
(1) Property: existing VQA-related datasets are mostly factoid, but some

applications go beyond just visual facts and focus on affordance.

(2) Model Input: our work is the first to consider both reference instructional

video and the video of user’s current situation.

(3) Model Output: to complete the user’s task, our model needs to operate
across multiple steps, necessitating the model to perform an action described by
textual answer at each step.

3.3 Task Application of AQTC

Our AQTC task actually proposes a typical and general AI process to assisting
humans, i.e., model solves a user’s query depicted in text and user view image

6

Affordance-centric Question-driven Task Completion

Fig. 3. Data Collection Pipeline for our AssistQ benchmark.

by seeking answers from the instructional video. Therefore, the task can support
a wide range of applications:

(1) Life assistant: supporting AR glass to guide users in daily life, like oper-

ating home appliances, cooking dishes.

(2) Skills training: reducing the labor costs, e.g.,, replacing human instruc-

tors to teach bicycle repair/PC assembly.

(3) Working partner: improving people’s working efficiency, such as recent
DARPA’s Virtual Partners program. In research, our task is also not limited in
a specific scope. It can be a typical task in egocentric AI, which is a new trend
in computer vision (e.g., Ego4D dataset).

4 AssistQ Benchmark

We now describe our new benchmark for the AQTC task. The data collection
and annotation pipeline is shown in Figure 3. Participants would record videos
of themselves operating home appliances, and then annotators would create QA
pairs after watching the instructional video.

4.1 Video Collection

Videos. The recruited participants were asked to record instructional videos for
home appliances. In each video, the participant demonstrated how to operate a
device, such as a washing machine or oven. Participants were asked to propose
the functions that they were to record to ensure that the video was sufficiently
informative for meaningful question-answering. For example, filming the steps
needed to change the temperature of a microwave would result in an information-
rich video of interest to AssistQ, whereas filming a video about merely turning
the device on would be inadequate. Participants were also encouraged to per-
form at least 3-5 functions in each video and were given video samples on the
recommended placement of their camera to ensure that the videos recorded are
of a baseline quality. In addition, participants were required to narrate their
actions in the video similar to the audio instructions that accompany product
videos which were then submitted.

Affordance-centric Question-driven Task Completion

7

Fig. 4. Sample Device (vacuum cleaner) with new buttons in the video frame.

4.2 Query Collection

User images and Button Annotation. For visual query, participants took
front-facing pictures of their appliances. In most cases, one image was sufficient
to capture all device buttons. But when there is huge change of the device state
(Figure 4), participants were asked to provide pictures of the other states to
ensure that the agent has sufficient visual information about available choices
at each query stage. Annotators were tasked to place bounding boxes over all
visible buttons in the image(s), using Makesense.AI annotation tool.
QA pairs. Annotators were asked to create at least 3 questions that were an-
swerable from the video. For greater language diversity in the question set, anno-
tators were encouraged to vary the question stems (“How do I”,“How to”,“What
to do to”), action words (“cook”,“bake”, “turn”) and question type. Questions
were categories as either specific functionality or pure functionality. Specific func-
tionality questions include specific end-stage values, whereas pure functionality
type questions only reference the intended functionality. For example, queries
such as “How to change timer on rice cooker to 3 min” are specific functionality
in nature as they contain a desired end-stage value (3 min), whereas queries such
as “How do I access the timer on the rice cooker” are pure functionality.

Annotators were asked to write MCQ options grounded to labelled buttons on
user image(s), creating a vision-language option space. There were no restrictions
on the number of MCQ options, though there should be at least as many MCQ
options as there are buttons, with each button referenced in at least one option.
The structure of the MCQ options varied by question type. MCQ options for
pure functionality questions were typically written as action <button>, such as
Press <button1>. For specific functionality questions, relevant function details
were appended to the option space i.e., Press <button1> to select Defrost. This
evaluated the agent’s ability to reason about the user’s desired outcome. To
push the model to understand the underlying semantics of QA pairs, at least
one wrong MCQ option had similar instructions to the correct one.

In summary, 18 participants were engaged in video collection; over 350 man-
hours were involved in recruitment, filming and liaison over a span of 6 months.

8

Affordance-centric Question-driven Task Completion

(a)

(b)

Fig. 5. Statistics of Videos in AssistQ. (a) Distribution of devices by appliance types.
(b) Distribution of instructional videos by length.

(a)

(b)

Fig. 6. Statistics of Questions in AssistQ. (a) Distribution of steps required to answer
generated queries. (b) Analysis of question structure by stem-action-target.

4 annotators were then engaged over another 300 man-hours to label images and
create multiple-choice, multi-step QA pairs.

4.3 Dataset Statistics

100 videos with duration averaging 115 seconds were collected. Figure 5(b) shows
the distribution of the video duration, with instructional videos spanning 1-2
min having the highest frequency in AssistQ. The videos spanned 25 common
household appliances, with microwave, washing machine and oven having the
highest frequencies due to their ubiquity in households (Figure 5(a)). The devices
came from a diverse pool of 53 distinct appliance brands such as Panasonic (5%),
Cornell (4%), Sharp (4%), LG (4%) and Bosch (4%). Instructional videos of more
complex devices such as printers (240s) and gym equipment (216s) have longer
average duration to capture their larger variety of functions.

Affordance-centric Question-driven Task Completion

9

Fig. 7. Architecture of our question-to-answer (Q2A) model. It can be viewed as an
encoder-decoder architecture, where the input encoders obtain feature representations
for different inputs (video, script, question, answer), and the decoder modules (context
grounding, steps network) estimate the answer from encodings. Best viewed in colors.

From the 100 instructional videos, we collected 531 multiple-choice QA pairs
(Figure 6) Of which, 251 are pure functionality questions and 278 are specific
functionality questions. 70% of QA pairs require more than 2 steps in the answer
sequence (Figure 6(a)), rendering a majority of our questions multi-step. Fig-
ure 6(b) shows the alluvial chart of question stem-action-target words obtained
from the generated queries. Only the top 10 most common action words were
included in the alluvial chart for greater clarity. From the chart, we can see that
while distributed, questions beginning with “How to” and “How do I”, actions
words set and change as well as target words [function] and [item] having the
highest occurrence in our dataset. Apart from diversity, this dataset also poses
a large number of interesting challenges. See supplementary material for details.

5 Question-to-Actions (Q2A) Model

In this section, we propose a question-to-actions (Q2A) model for the proposed
AQTC task. As shown in Figure 7, our Q2A model is an encoder-decoder archi-
tecture including: (1) Input Encoder extracts feature from video, script, question,
and answers; (2) Context Grounding Module generates context-aware feature for
each question-answer pair; (3) Steps Network answers the question in the current
step. Next, we describe them in detail.

5.1 Input Encoders

Video. Video encoding aims to obtain the visual location and of buttons. This
not only requires the video embedding [16,37,38], but also requires some ob-
ject information in video embedding [39,41,42]. For simplicity, we just use a
pretrained vision transformer (ViT) [15] to encode frames instead of a video

VisionTransformerBERTBERTAnswerEncoderFeature V(f ×dv)Feature S(e ×ds)Feature Q (dq)Feature {Aji}(jmax×da)Steps NetworkContextGroundingModuleFeature C1i (dc)Feature C2i (dc)Feature C3i (dc)Feature C4i (dc)Feature C5i (dc)Hidden H1i (dc)Hidden H2i (dc)Hidden H3i (dc)Hidden H4i (dc)Hidden H5i (dc)Ground TruthHidden Hi-1 (dc)Hidden Hi (dc)MLPwith Softmax0.10.20.10.50.1How to set alarm for 1 minute?The button left is the mode selection … long press the top left adjust … (script)videoscriptquestion1.Long press [button 1]2.Short press [button 2]3.Long press [button 3]4.Short press [button 4]5.Short press [button 4]Answers & User Image in i-th step10

Affordance-centric Question-driven Task Completion

Fig. 8. The architecture of the answer encoder. This figure presents the case of visual
button encoding with both mask and reverse-mask images. Note that ViT and BERT
used here are the same networks shown in Figure 7. Best viewed in colors.

transformer [4,6], but use a special button encoding method for user image (de-
scribed latter). We encode one frame per second for the video. Suppose the
video has f frames and the encoding dimension is dv, we can obtain the feature
representation V of shape f × dv.
Script. The video script describes the button’s operation and function, which are
the crucial cues to answer the question. We use BERT [14] for text embedding.
Because the answer in a step usually corresponds to a specific sentence in the
script, we transform the script into e sentences, and use the pooled outputs from
BERT as sentence embeddings S (e × ds), where ds is the feature dimension.
Question. Following the embedding way of the script, we also use the pooled
output from BERT [14] as the question feature Q1, with the dimension dq.
To keep the consistency, we leverage the same BERT used in the script (i.e.,
dq = ds). By this way, the question feature and the script feature may have
similarities, which can help answer the question. To distinguish the question
from the script, we add the prefix “Question:” to each question before encoding.
The j-th answer of the i-th step. An answer contains both text and visual
button (bounding-box in the user image), making it difficult to encode. For
answer text, we still use BERT to extract the feature. Like the prefix in question,
we also prefix each answer with “Answer:”. The challenging part is the button
representation. As shown in Figure 8, we encode the user image U with the
masked referenced button. Meanwhile, we also encode U with all buttons masked
except the referenced button. The visual button feature Bk, and text feature T i
j
are concatenated to the answer’s feature Ai
j. The masked button image allows
our model to utilize the appearance cues when buttons differ in their appearance,
and another reverse-masked image allows our model to focus on the mutual cues
(like the relative location) when buttons’ appearances are similar.
Context Grounding Module. In video QA models [7,26] there are usually
modules that utilize context for better question and answer representation. Fol-

1 QU in Section 3.2 denotes the question under user view. To simplify, we use Q here.

ViTLong press [button 1]user imageAnswer: Long press buttonAnswerreferencemask and reverse-maskFeature Bk(2 ×dv)BERTFeature Tji(dt)Feature Aji(2 × dv + dt)concatLong press [button 1]user imageAnswerGCN1342buttongraph1342updatedbuttongraphAnswer: Long press buttonBERTFeature Tji(dt)FeatureAji(dg+ dt)concatAffordance-centric Question-driven Task Completion

11

lowing this idea, we also design a context grounding module. Consider the input
feature representations V , S, Q, Ai
j as the button
encoding), we generate context-aware question-answer pair feature C i
j as:

j as the text encoding, Bi

j (T i

C i

j = M LP ([T i

j , Bi

j, AtQA→S(T i

j , S, S), AtQA(cid:57)(cid:57)(cid:75)S(cid:57)(cid:57)(cid:75)V (T i

j , S, V )])

(1)

where At(query, key, value) denotes the attention operation [5]. M LP denotes a
2-layer MLP, of which dimension is carefully adjusted to satisfy the calculation
requirements. [·] is the concatenate operation. QA → S denotes the attention
from question-answer (query) to script (key), and produces the QA-aware script
feature. We also introduce a “transfer attention” (QA (cid:57)(cid:57)(cid:75) S (cid:57)(cid:57)(cid:75) V ) here to
simulate the process of finding visual cues based on text:

M askQA(cid:57)(cid:57)(cid:75)S(cid:57)(cid:57)(cid:75)V (T i

j , S, V ) = M askQA→S(T i

j , S, S) × M askS→V (S, V, V ), (2)

where M ask denotes the attention mask. With the attention mask, we can obtain
the QA-aware video feature by AtQA(cid:57)(cid:57)(cid:75)S(cid:57)(cid:57)(cid:75)V = M askQA(cid:57)(cid:57)(cid:75)S(cid:57)(cid:57)(cid:75)V × V .
Steps Network. Our Q2A leverages the historical steps via a steps network,
which is a 2-layer MLP or a GRU. In i-th step, the steps network would use the
state from i − 1-th step to produce the state H i
j for j-th answer. Then, the state
of the ground-truth answer would be reused as the hidden state of the i + 1 step.
The initial state is a random-initialized vector with standard normal distribution
if the step just begins.
Prediction Head. In each step, the hidden states generated by the steps indi-
cate the answers’ feature representations. We use a two-layer MLP followed by
softmax activation to predict a score for each answer.
Training and Inference. Following common practice in Visual QA/Dialog [1,2],
BERT and ViT in the input encoder are frozen since they have a large number of
parameters. Other parts are trainable. We use cross-entropy (CE) with softmax
activation to calculate the loss. During inference, our Q2A model chooses the
state that can yield the highest predicted score after the prediction head. In this
way, the model can finish the whole multi-step answering procedure by itself.

6 Experiments

6.1 Experimental Setting

Data Splits. We randomly split 100 instructional videos of AssistQ into the
training set and the validation set with a ratio of 8:2. In statistics, the training
set has 80 instructional videos with 425 QA instances, and the testing set has
20 instructional videos with 106 QA instances. About 25% device types in the
testing set are not present in the training set, making our AssistQ benchmark
more challenging.
Evaluation Metrics. As illustrated in Section 4, the “multi-step” characteristic
is similar to “multi-round” characteristic in visual dialog [1,12]. Therefore, it is
natural to use their evaluation metrics for the proposed AssistQ task:

12

Affordance-centric Question-driven Task Completion

(a) Ablation on button encoding
Mask Reverse-mask R@1 ↑ R@3 ↑ MR ↓ MRR ↑
17.9
19.4
30.2
21.4

54.8
53.6
62.3
59.1

3.8
3.7
3.2
3.7

2.5
2.5
3.2
2.6

×
✓
×
✓

×
×
✓
✓

(b) Ablation on input modalities
Video Script R@1 ↑ R@3 ↑ MR ↓ MRR ↑

×
✓
×
✓

23.4
×
29.4
×
✓
24.6
✓ 30.2

57.9
59.5
58.7
62.3

3.6
3.3
3.6
3.2

2.8
3.1
2.9
3.2

(c) Ablation on context grounding
QA→S S→V QA(cid:57)(cid:57)(cid:75)S(cid:57)(cid:57)(cid:75)V R@1 ↑ R@3 ↑ MR ↓ MRR ↑

×
✓
×
✓
✓

×
×
✓
✓
✓

×
×
×
×
✓

19.8
28.6
22.6
29.0
30.2

52.8
69.8
56.0
62.7
62.3

3.8
3.1
3.6
3.2
3.2

2.5
3.2
2.6
3.2
3.2

(d) Ablation on historical steps

Network History R@1 ↑ R@3 ↑ MR ↓ MRR ↑

MLP

GRU

×
✓
×
✓

21.4
21.8
25.0
30.2

60.7
56.8
62.3
62.3

3.5
3.7
3.4
3.2

2.7
2.6
2.9
3.2

Table 2. Ablation studies of our proposed Q2A model on the AssistQ benchmark. (a)
and (b) are for the Q2A encoder, where (a) explores the necessary input modalities
and (b) studies the ways of button encoding. (c) and (d) are for the Q2A decoder,
where (c) illustrates the importance of using historical steps and (d) shows different
combinations for context grounding. Note → and (cid:57)(cid:57)(cid:75) denote attention methods we
described in Section 5.1.

• Recall@k measures how often the ground-truth answer selected in top k

choices. We report Recall@1 and Recall@3 in our experiments.

• Mean rank (MR) refers to mean value of the predicted ranking position

of the correct answer. The model should pursue lower MR.

• Mean reciprocal rank (MRR) is the mean value of the reciprocal pre-

dicted ranking position of the correct answer. Higher MRR is better.
Implementation Details. We use PyTorch [31] to perform experiments. For a
fair comparison, all models use the same optimizer and learning rate scheduler.
Specifically, we use a momentum SGD optimizer with a batch size of 16, and
a cosine annealing scheduler [30] with the learning rate 2 × 10−3, 1 epoch for
warmup, and maximum 6 training epochs.

6.2 Ablation Study

As illustrated in Section 5, we design an encoder-decoder architecture Q2A to
solve the AQTC task. In its encoder, we encode the video, script, question, and
candidate answers, where a special visual button encoding method is designed for
the answer encoding. Meanwhile, in the Q2A decoder, we also specifically develop
the context grounding module and utilize historical steps for better prediction.
A default setting of our model is (a) encode both video and script, (b) double-
mask button encoding, (c) QA → S → V attention, (d) MLP for historical steps.
Next, we perform ablation studies on the points mentioned above.
Visual Button Encoding. See Table 2(a), we analyze the encoding ways of
visual button encoding. Interestingly, the model achieves the best performance
when only using reverse-mask image encoding (mask × and reverse-mask ✓),
with a clear margin compared to other configurations. These results demonstrate
that it is better to mask the other buttons when encoding a visual button. In

Affordance-centric Question-driven Task Completion

13

our opinion, both mask and reverse-mask schemes can help the model to infer
the button location, but the reverse-mask scheme can help the model use the
button appearance.
Input Modalities. As shown in Table 2(b), when the model ignores some
contexts (video × or script ×), it achieves much lower performance (23.4 v.s.
30.2) than the model with the full contexts (video ✓ and script ✓). We also
found that the independent video (video ✓ or script ×) can lead to much better
results (29.4 v.s. 24.6) than the independent script (video × or script ✓). This
is because when using only script w/o video, some information in scripts might
be misleading. For example, say we have two candidate answers: (a) “press here
[refer to <button1> in video, but not accessible when using only script] on the
microwave”; (b) “press here [refer to <button2>] to heat it up”. If the script has
a language context (e.g., “press here to heat it up”) which is similar to (b), it
will mislead the model, making it more inclined to choose (b); but in fact, (a) is
the ground truth option, which we can clearly tell by watching the instructional
video. These results suggest that the model should fully utilize both video and
script to answer questions, rather than only one of them.
Context Grounding.2 According to Table 2(c), all parts in our proposed
grounding schemes (see Section 5.1) can improve the performances. Among them,
the model benefits less from S → V but obtains a significant gain from QA → S.
This is because S → V is unrelated to the question, suggesting the importance
of finding the corresponding context according to the question. Moreover, based
on QA → S and S → V , the model would benefit from the “transfer attention”
QA (cid:57)(cid:57)(cid:75) S (cid:57)(cid:57)(cid:75) V . We believe the “transfer attention” could link QA to V , which
simulates finding the related script based on the question, and then locating the
related video moment by the related script. Compared with direct QA → V , the
calculation cost of our “transfer attention” is much smaller since the script is
shared for all candidate QA pairs.
Steps Network. Table 2(d) presents the ablation on whether to use cues from
historical steps (✓ or ×) and how to use them (MLP or GRU [8]). We can
find that historical information can stably improve the performances of GRU
on all metrics, especially on Recall@1 (25.0 to 30.2). But for MLP with histor-
ical information, only a few gains (21.4 to 21.8) can be observed on Recall@1,
while other metrics even declined. Compared to MLP, GRU has advantages in
temporal modeling, which is more suitable as the steps network.

6.3 Comparison

As shown in Table 3, we compare our Q2A model with baselines in other domains
on the AssistQ benchmark. Since the answers encoded in other methods are plain
text (without placeholders for buttons), we simply introduce our double-mask
answer encoding scheme. For all models, we use the same backbone network in
the encoder for a fair comparison. We also provide the results of random guess
for improvement comparison.

2 We use the same notation of Section 5. V : video, S: script, Q: question, A: answer.

14

Affordance-centric Question-driven Task Completion

Baseline Method
Random Guess

Recall@1 ↑
18.3

MR ↓
3.9

MRR ↑
2.3

Multi-stream [26] (Video QA) 22.8 (+4.5)
LateFusion [1] (Video Dialog)
19.9 (+1.6)
PACMAN [9] (EQA)
24.6 (+6.3)
Q2A (Ours)

3.7 (-0.2) 3.0 (+0.7)
4.1 (+0.2) 2.6 (+0.3)
3.9 (-0.0) 2.9 (+0.6)
30.2 (+11.9) 62.3 (+11.9) 3.2 (-0.7) 3.2 (+0.9)
Table 3. Baseline comparison on the AssistQ benchmark. Benefiting from the specific
design for the benchmark, Q2A achieves the highest performance on all metrics.

Recall@3 ↑
50.4
54.8 (+4.4)
49.3 (-1.1)
54.0 (+3.6)

Video QA. Multi-stream is proposed as a baseline model for the TVQA [26]
task. Like our model, it has modules similar to context grounding. However, It
cannot take advantage of cues from historical steps (as video QA is a single-step
task), limiting its performance on the AssistQ benchmark.
Video Dialog. [1] proposes LateFusion to solve the multi-round video dialog,
which simply concatenates the historical dialog and introduces an LSTM [21]
to model them. However, its performance is even worse than Multi-stream. We
believe that simple late fusion leads to inferior performance as there is no module
similar to context grounding. Also, its LSTM models a very long sequence of the
entire script and historical dialogue, which may lose useful information.
EQA. PACMAN [9] uses an LSTM-based planner to update the agent state,
with an MLP-based control planner to predict answers. This procedure is similar
to our GRU-based decoder that utilizes historical steps, but PACMAN only
considers the visual information for navigation. As shown in Table 3, PACMAN
achieves better performance than Multi-stream and LateFusion, but still lags
behind our Q2A model.

7 Conclusion

In this paper, we proposed Affordance-centric Question-driven Task Completion
(AQTC) task, which enables AI assistants to guide users by learning from in-
structional videos. To support the task, we collected the AssistQ benchmark,
consisting of 531 multiple-step QA samples derived from 100 newly filmed in-
structional videos, with efforts underway to continue scaling this dataset. We
also present a new baseline called Question-to-Actions (Q2A) for our AQTC
task, and experimental results demonstrate the effectiveness of the Q2A model
on the AQTC task. We hope that our proposed AQTC and AssistQ can advance
the development of AI assistants that see the world through our eyes, assisting
humans in daily, real-world scenarios.
Acknowledgements. This project is supported by the National Research Foun-
dation, Singapore under its NRFF Award NRF-NRFF13-2021-0008, and Mike
Zheng Shou’s Start-Up Grant from NUS. The computational work for this article
was partially performed on resources of the National Supercomputing Centre,
Singapore.

Affordance-centric Question-driven Task Completion

15

Appendix

A Dataset Quality Control

To ensure the quality of videos and scripts, we worked with participants closely
(e.g. dissemination of comprehensive submission guidelines, checking of device
before filming, hands-on recording guidance, review of transcripts). Researchers
also conduct a quality check on the recorded videos before acceptance. In fact,
2-3% of videos have been rejected due to undesired quality (e.g. unclear video
instructions). To ensure the quality of QA pairs and annotation, AssistQ also
adopted all 4 quality control measures referenced in TGIF-QA [23]: gold standard
annotation to help annotators to understand requirements, rejection and reviews
(each video has been audited by at least 1 more person). The high quality of
our data can be further confirmed by the high human performances at 95.8%
recall@1.

B Dataset Collection Source

We noticed that many virtual world simulators could help to produce a clean
dataset, but we hold on collecting data in the real world. The main reason is that
ultimately we want to deploy such AI assistant models in real environments such
as smart glasses. To this end, at least we need to have testing data in real envi-
ronment. Regarding training data, we agree it is a promising research direction
to leverage a virtual environment. However, due to the significant domain gap
between the existing virtual environments (e.g., AI2Thor [24], AI Habitat [35])
and the real env, real-world training data at the scale provided by us is needed to
adapt model trained in virtual to real. In this paper’s scope, we focus on models
trained only on the real environment; we consider it as future work to pre-train
in the virtual environment first and then fine-tuning it on real-world data.

C Some Dataset Challenges

There are some interesting challenges in our dataset. These challenges are sum-
marized in Figure 9. Some challenges include (a) changing views within the same
instructional video; (b) occlusion of buttons due to hand placement, narrow views
or hidden buttons; (c) faint buttons from lighting; (d) motion blurs from ego-
motion or out-of-focus cameras; as well as; (e) user images taken under different
angles and lighting from the video, reiterating the richness and complexity of
AssistQ.

D Dataset Comparison

Table 4 compares AssistQ to related datasets. As we can see, AssistQ is unique
compared to others: (1) the video content is sourced from real-world, everyday

16

Affordance-centric Question-driven Task Completion

Fig. 9. Some challenging examples in our AssistQ datasets.

situations in the egocentric perspective, (2) the question are affordance-centric
i.e. how to use and execute functions of appliances, and multi-modal to more
closely reflect the inputs that an AI assistant (e.g. AR glass) receives from the
user. (3) Answers are a sequence of actions for the user to execute. AssistQ also
has longer videos on average compared to most datasets.

Dataset

Video Source Video Type Questions

Question Modality
Text
✓
MovieQA [36]
✓
TVQA [26]
✓
AVSD [1]
✓
Embodied QA [9] Simulated env. Egocentric
AssistQ (ours) Crowdsourced Egocentric Affordance ✓

Third-person Factoid
Movie
Third-person Factoid
TV show
Crowdsourced Third-person Factoid
Factoid

Visual
-
-
-
-
✓

Answers

#Clips #QA Ave Dur (s)

408

Single-step
14,944
Single-step 21,793 152,545
Multi-round 11,816 118,160
Multi-step
Multi-step

5,281
531

-
100

202.7
76.2
30
-
115

Table 4. Comparison of AssistQ with related datasets.

MovieQA [36]. The goal of MovieQA is to understand story plots in movies.
408 subtitled movies were collected together with their Wikipedia synopsis and
imsdb/Described Video Service (DVS) scripts where available. In the first round
of annotation, the annotators were shown the plot synopsis only and asked to
create any number of QA pairs that can be localized to a set of sentences in the

Affordance-centric Question-driven Task Completion

17

synopsis. Naturally, this led to questions that were plot-focused and less reliant
on visual information. In the second round of annotation, annotators were asked
to create 5 multiple-choice answers (1 right, 4 wrong) based on the synopsis and
questions. Finally, each sentence of the synopsis was aligned to time-stamps on
the video clips (∼ 200s in length); the video clip and aligned QA pairs then
formed the benchmark (Figure 10).

Fig. 10. Examples from the MovieQA dataset. 18% of questions were about who, fol-
lowed by 12.4% about why and 9.7% about what. As seen in the examples, questions
typically focused on plot developments with little reference to visual signals.

TVQA [26]. Similar to MovieQA, TVQA was created to understand human-
centric plots in videos. 6 TV shows were segmented into 60/90-second clips, ac-
companied by subtitles and aligned transcripts. Annotators were shown the video
clip and aligned subtitles, and encouraged to create questions in a 2-part format:
[What/How/Where/Why/. . . ]
. The
second part served to localize the question to the relevant moment in the clip
and ground the question in visual signals, such as What was House saying be-
fore he leaned over the bed?. Annotators provided 5 multiple-choice answers (1
right, 4 wrong) and annotated time-stamps of the exact video portion required
to answer the question (Figure 11).

[when/before/after]

Fig. 11. Examples from the TVQA dataset. The question had to be written with a
[when/before/after ] clause so that it is localized to a specific moment in the clip. 54%
of questions were about what, followed by 15% about who.

AVSD [1]. In Audio Visual Scene-Aware Dialog (AVSD), an agent is given
an input video, a dialog history and a follow-up question, and its goal is to

18

Affordance-centric Question-driven Task Completion

generate a correct response (Figure 12). 11,816 videos of everyday human activ-
ities were taken from the Charades human-activity dataset [34]. Each video was
handed to a pair of annotators. The “Questioner” was tasked to ask questions
about activities and events in the video clip, given only 3 video frames. The “An-
swerer” has access to the video and script, and answers the “Questioner” over a
sequence of 10 questions. Once the conversation is complete, the “Questioner”
is tasked to summarize the video.

Fig. 12. Examples from the AVSD dataset. The “Questioner” asks a series of questions
about the input video and the “Answerer” gives an answer at each round.

Embodied QA [9]. In Embodied QA, an agent is spawned at a random
location in a simulated home environment and asked a question about the
colour/location of an object. The objective of the agent is to navigate the en-
vironment through atomic actions (move forward, turn, etc.) and gather visual
information to answer the question (Figure 13). The dataset is built on a subset
of House3D environments, and the questions were written in specific formats
to ensure they were answerable and unambiguous. For example, location ques-
tions were written as What room is the <OBJ> located in?, where <OBJ> is an
unambiguous object that is query-able.

E Data Sample Visualisation

We attach an annotated example and an animated video to showcase the the
intended use-case of AssistQ in AI assistants. The video shows 2 examples with
egocentric instructional videos, and an example with a third-person instructional
video.

Annotated example. We provide 1 annotated example from AssistQ to
illustrate the format and information encapsulated in our annotations. The ex-
ample contains the instructional video (video.mp4), question-answering pairs
(qa.json), bounding box coordinates (buttons.csv), narration transcript (script.txt)
as well as the image folder containing front-view image(s) of the device.

Affordance-centric Question-driven Task Completion

19

Fig. 13. Example from the EQA dataset. The agent is spawned at a random location
in a simulated home environment and navigates the environment to answer a question
about the location/colour of an unambiguous object.

Examples with egocentric videos. We showcase 2 examples from our As-
sistQ dataset, namely the EF stove and Bosch washing machine, in an animated
video (Figure 14). Through these examples, we demonstrate the use of AssistQ
in our proposed Affordance-centric Question-driven Task Completion (AQTC)
task. The AI assistant learns offline from an egocentric instructional video so
that when the user asks a question (e.g. stove: How do I switch on the left stove
at maximum heat), the AI assistant provides the user with a series of steps to
perform (e.g. stove: 1. Press [here], 2. Press [here] and 3. Select [here] mode
9). Each step is grounded to bounding boxes in the user’s image, i.e. [here]
refers to a labelled button/knob on the device. The bounding box is shown on
the user’s view of the device (e.g. through AR glasses) so the user knows the
exact action to perform.

Example with third-person video. In the animated video, we also include
a Shark vacuum example that had its instructional video recorded in third-person
(Figure 15). Compared to the egocentric examples, the third-person example is
more challenging as it requires the AI assistant to resolve visual information
from the third-person video with that of the user’s egocentric visual query. The
example demonstrates that the AQTC task need not be confined to egocen-
tric videos. We intend to extend the AssistQ dataset to include more examples
with third-person videos, as it is more common for product videos/demos to be
recorded in third-person. This will allow further research and development in AI
assistants to benefit from our AQTC task and AssistQ dataset.

20

Affordance-centric Question-driven Task Completion

Fig. 14. Illustration of AQTC task with an example from AssistQ. The AI assistant
learns offline from a narrated instructional video. The user wonders how to use the
appliance and shows the AI assistant their view. The AI assistant predicts the sequence
of actions that the user should perform to accomplish the task, and guides the user
through each step with bounding boxes over their view of the appliance.

References

1. AlAmri, H., Cartillier, V., Das, A., Wang, J., Cherian, A., Essa, I., Batra, D.,
Marks, T.K., Hori, C., Anderson, P., Lee, S., Parikh, D.: Audio visual scene-aware
dialog. In: CVPR. pp. 7558–7567 (2019)

2. Anderson, P., He, X., Buehler, C., Teney, D., Johnson, M., Gould, S., Zhang,
L.: Bottom-up and top-down attention for image captioning and visual question
answering. In: CVPR. pp. 6077–6086 (2018)

3. Antol, S., Agrawal, A., Lu, J., Mitchell, M., Batra, D., Zitnick, C.L., Parikh, D.:

VQA: visual question answering. In: ICCV. pp. 2425–2433 (2015)

4. Arnab, A., Dehghani, M., Heigold, G., Sun, C., Luˇci´c, M., Schmid, C.: Vivit: A

video vision transformer. In: ICCV. pp. 6836–6846 (2021)

5. Bahdanau, D., Cho, K., Bengio, Y.: Neural machine translation by jointly learning

to align and translate. In: ICLR (2015)

Affordance-centric Question-driven Task Completion

21

Fig. 15. Illustration of AQTC task with instructional video recorded from the third-
person perspective. The task remains largely unchanged, except that the model has
to resolve visual signals from the third-person to the first-person image from the user.
This presents new challenges for the AQTC task and we are expanding the AssistQ
dataset to include more of such scenarios.

6. Bertasius, G., Wang, H., Torresani, L.: Is space-time attention all you need for

video understanding? In: ICML (2021)
7. Chadha, A., Arora, G., Kaloty, N.:

iperceive: Applying common-sense rea-
soning to multi-modal dense video captioning and video question answering.
arXiv:2011.07735 (2020)

8. Chung, J., G¨ul¸cehre, C¸ ., Cho, K., Bengio, Y.: Empirical evaluation of gated recur-

rent neural networks on sequence modeling. arXiv:1412.3555 (2014)

9. Das, A., Datta, S., Gkioxari, G., Lee, S., Parikh, D., Batra, D.: Embodied question

answering. In: CVPR. pp. 1–10 (2018)

10. Das, A., Gervet, T., Romoff, J., Batra, D., Parikh, D., Rabbat, M., Pineau, J.:
Tarmac: Targeted multi-agent communication. In: ICML. pp. 1538–1546 (2019)
11. Das, A., Gkioxari, G., Lee, S., Parikh, D., Batra, D.: Neural modular control for

embodied question answering. In: CoRL. pp. 53–62 (2018)

22

Affordance-centric Question-driven Task Completion

12. Das, A., Kottur, S., Gupta, K., Singh, A., Yadav, D., Moura, J.M.F., Parikh, D.,

Batra, D.: Visual dialog. In: CVPR. pp. 1080–1089 (2017)

13. Das, A., Kottur, S., Moura, J.M.F., Lee, S., Batra, D.: Learning cooperative visual

dialog agents with deep reinforcement learning. In: ICCV. pp. 2970–2979 (2017)

14. Devlin, J., Chang, M., Lee, K., Toutanova, K.: BERT: pre-training of deep bidirec-
tional transformers for language understanding. In: NAACL. pp. 4171–4186 (2019)
15. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner,
T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., Houlsby, N.:
An image is worth 16x16 words: Transformers for image recognition at scale. In:
ICLR (2021)

16. Feichtenhofer, C., Fan, H., Xiong, B., Girshick, R.B., He, K.: A large-scale study

on unsupervised spatiotemporal representation learning. In: CVPR (2021)

17. Gao, D., Wang, R., Bai, Z., Chen, X.: Env-qa: A video question answering bench-
mark for comprehensive understanding of dynamic environments. In: CVPR. pp.
1675–1685 (2021)

18. Gordon, D., Kembhavi, A., Rastegari, M., Redmon, J., Fox, D., Farhadi, A.: IQA:
visual question answering in interactive environments. In: CVPR. pp. 4089–4098
(2018)

19. Goyal, Y., Khot, T., Agrawal, A., Summers-Stay, D., Batra, D., Parikh, D.: Making
the V in VQA matter: Elevating the role of image understanding in visual question
answering. International Journal of Computer Vision 127(4), 398–414 (2019)
20. Grauman, K., Westbury, A., Byrne, E., Chavis, Z., Furnari, A., Girdhar, R., Ham-
burger, J., Jiang, H., Liu, M., Liu, X., Martin, M., Nagarajan, T., Radosavovic, I.,
Ramakrishnan, S.K., Ryan, F., Sharma, J., Wray, M., Xu, M., Xu, E.Z., Zhao, C.,
Bansal, S., Batra, D., Cartillier, V., Crane, S., Do, T., Doulaty, M., Erapalli, A.,
Feichtenhofer, C., Fragomeni, A., Fu, Q., Fuegen, C., Gebreselasie, A., Gonzalez,
C., Hillis, J., Huang, X., Huang, Y., Jia, W., Khoo, W., Kolar, J., Kottur, S., Ku-
mar, A., Landini, F., Li, C., Li, Y., Li, Z., Mangalam, K., Modhugu, R., Munro,
J., Murrell, T., Nishiyasu, T., Price, W., Puentes, P.R., Ramazanova, M., Sari,
L., Somasundaram, K., Southerland, A., Sugano, Y., Tao, R., Vo, M., Wang, Y.,
Wu, X., Yagi, T., Zhu, Y., Arbelaez, P., Crandall, D., Damen, D., Farinella, G.M.,
Ghanem, B., Ithapu, V.K., Jawahar, C.V., Joo, H., Kitani, K., Li, H., Newcombe,
R., Oliva, A., Park, H.S., Rehg, J.M., Sato, Y., Shi, J., Shou, M.Z., Torralba, A.,
Torresani, L., Yan, M., Malik, J.: Ego4d: Around the world in 3, 000 hours of
egocentric video. arXiv:2110.07058 (2021)

21. Hochreiter, S., Schmidhuber, J.: Long short-term memory. Neural Computation

9(8), 1735–1780 (1997)

22. Jain, U., Weihs, L., Kolve, E., Rastegari, M., Lazebnik, S., Farhadi, A., Schwing,
A.G., Kembhavi, A.: Two body problem: Collaborative visual task completion. In:
CVPR. pp. 6689–6699 (2019)

23. Jang, Y., Song, Y., Yu, Y., Kim, Y., Kim, G.: TGIF-QA: toward spatio-temporal

reasoning in visual question answering. In: CVPR. pp. 1359–1367 (2017)

24. Kolve, E., Mottaghi, R., Han, W., VanderBilt, E., Weihs, L., Herrasti, A., Gordon,
D., Zhu, Y., Gupta, A., Farhadi, A.: AI2-THOR: An Interactive 3D Environment
for Visual AI. arXiv:1712.05474 (2017)

25. Kottur, S., Moura, J.M.F., Parikh, D., Batra, D., Rohrbach, M.: Visual coreference
resolution in visual dialog using neural module networks. In: ECCV. pp. 160–178
(2018)

26. Lei, J., Yu, L., Bansal, M., Berg, T.L.: TVQA: localized, compositional video ques-

tion answering. In: EMNLP. pp. 1369–1379 (2018)

Affordance-centric Question-driven Task Completion

23

27. Lei, J., Yu, L., Berg, T.L., Bansal, M.: TVQA+: spatio-temporal grounding for
video question answering. In: Jurafsky, D., Chai, J., Schluter, N., Tetreault, J.R.
(eds.) ACL. pp. 8211–8225 (2020)

28. Li, Z., Li, Z., Zhang, J., Feng, Y., Zhou, J.: Bridging text and video: A universal
multimodal transformer for audio-visual scene-aware dialog. IEEE ACM Trans.
Audio Speech Lang. Process. 29, 2476–2483 (2021)

29. Lin, K.Q., Wang, A.J., Soldan, M., Wray, M., Yan, R., Xu, E.Z., Gao, D., Tu, R.,
Zhao, W., Kong, W., Cai, C., Wang, H., Damen, D., Ghanem, B., Liu, W., Shou,
M.Z.: Egocentric video-language pretraining. arXiv:2206.01670 (2022)

30. Loshchilov, I., Hutter, F.: SGDR: stochastic gradient descent with warm restarts.

In: ICLR (2017)

31. Paszke, A., Gross, S., Massa, F.e.a.: Pytorch: An imperative style, high-

performance deep learning library. In: NeurIPS. pp. 8026–8037 (2019)

32. Sax, A., Zhang, J.O., Emi, B., Zamir, A.R., Savarese, S., Guibas, L.J., Malik, J.:
Learning to navigate using mid-level visual priors. In: CoRL. pp. 791–812 (2019)
33. Schwartz, I., Schwing, A.G., Hazan, T.: A simple baseline for audio-visual scene-

aware dialog. In: CVPR. pp. 12548–12558 (2019)

34. Sigurdsson, G.A., Varol, G., Wang, X., Farhadi, A., Laptev, I., Gupta, A.: Hol-
lywood in homes: Crowdsourcing data collection for activity understanding. In:
ECCV. pp. 510–526 (2016)

35. Szot, A., Clegg, A., Undersander, E., Wijmans, E., Zhao, Y., Turner, J., Maestre,
N., Mukadam, M., Chaplot, D., Maksymets, O., Gokaslan, A., Vondrus, V., Dharur,
S., Meier, F., Galuba, W., Chang, A., Kira, Z., Koltun, V., Malik, J., Savva, M.,
Batra, D.: Habitat 2.0: Training home assistants to rearrange their habitat. In:
NeurIPS. pp. 251–266 (2021)

36. Tapaswi, M., Zhu, Y., Stiefelhagen, R., Torralba, A., Urtasun, R., Fidler, S.:
Movieqa: Understanding stories in movies through question-answering. In: CVPR.
pp. 4631–4640 (2016)

37. Tong, Z., Song, Y., Wang, J., Wang, L.: Videomae: Masked autoencoders are data-
efficient learners for self-supervised video pre-training. arXiv:2203.12602 (2022)
38. Wang, A.J., Ge, Y., Yan, R., Yuying, G., Lin, X., Cai, G., Wu, J., Shan, Y.,
Qie, X., Shou, M.Z.: All in one: Exploring unified video-language pre-training.
arXiv:2203.07303 (2022)

39. Wang, J., Ge, Y., Cai, G., Yan, R., Lin, X., Shan, Y., Qie, X., Shou, M.Z.: Object-
aware video-language pre-training for retrieval. In: CVPR. pp. 3313–3322 (2022)
40. Wortsman, M., Ehsani, K., Rastegari, M., Farhadi, A., Mottaghi, R.: Learning to
learn how to learn: Self-adaptive visual navigation using meta-learning. In: CVPR.
pp. 6750–6759 (2019)

41. Yan, R., Shou, M.Z., Ge, Y., Wang, A.J., Lin, X., Cai, G., Tang, J.: Video-text

pre-training with learned regions. arXiv:2112.01194 (2021)

42. Yan, R., Xie, L., Tang, J., Shu, X., Tian, Q.: Higcin: Hierarchical graph-based cross
inference network for group activity recognition. IEEE Transactions on Pattern
Analysis and Machine Intelligence pp. 1–1 (2020)

43. Yang, W., Wang, X., Farhadi, A., Gupta, A., Mottaghi, R.: Visual semantic navi-

gation using scene priors. In: ICLR (2019)

44. Yu, L., Chen, X., Gkioxari, G., Bansal, M., Berg, T.L., Batra, D.: Multi-target

embodied question answering. In: CVPR. pp. 6309–6318 (2019)

