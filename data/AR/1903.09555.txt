Evaluation of a deep learning system for the joint 
automated detection of diabetic retinopathy and 
age-related macular degeneration 

Cristina González-Gonzalo1,2,3,4*, MSc; Verónica Sánchez-Gutiérrez5, MD; 
Paula Hernández-Martínez5, MD; Inés Contreras5,6, MD, PhD; Yara T. 
Lechanteur4, MD; Artin Domanian4, MD; Bram van Ginneken2, PhD; Clara 
I. Sánchez1,2,3,4, PhD 

1

A-eye Research Group, Radboud University Medical Center, Nijmegen, The 
Netherlands 

2

3 

Diagnostic Image Analysis Group, Radboud University Medical Center, Nijmegen, 
The Netherlands 

Donders Institute for Brain, Cognition and Behaviour, Radboud University Medical 
Center, Nijmegen, The Netherlands 

4 

Department of Ophthalmology, Radboud University Medical Center, Nijmegen, The 
Netherlands 

5 

Department of Ophthalmology, University Hospital Ramón y Cajal, Ramón y Cajal 
Health Research Institute (IRYCIS), Madrid, Spain

6 

Clínica Rementería, Madrid, Spain 

Word-count abstract: 250; body: 3369 

* Corresponding author.   

Address: Diagnostic Image Analysis Group, Department of Radiology and Nuclear 
Medicine, Radboud University Medical Center, Geert Grooteplein 10 
6525 GA Nijmegen, The Netherlands.  
E-mail address: Cristina.GonzalezGonzalo@radboudumc.nl 

1 

 
 
 
 
 
 
 
 
 
 
Abstract 

Purpose: To validate the performance of a commercially-available, CE-certified deep learning 

(DL)  system,  RetCAD  v.1.3.0  (Thirona,  Nijmegen,  The  Netherlands),  for  the  joint  automatic 

detection  of  diabetic  retinopathy  (DR)  and  age-related  macular  degeneration  (AMD)  in  color 

fundus (CF) images on a dataset with mixed presence of eye diseases. 

Methods: Evaluation of joint detection of referable DR and AMD was performed on a DR-AMD 

dataset with 600 images acquired during routine clinical practice, containing referable and non-

referable cases of both diseases. Each image was graded for DR and AMD by an experienced 

ophthalmologist to establish the reference standard (RS), and by four independent observers for 

comparison  with  human  performance.  Validation  was  furtherly  assessed  on  Messidor  (1200 

images) for individual identification of referable DR, and the Age-Related Eye Disease Study 

(AREDS) dataset (133821 images) for referable AMD, against the corresponding RS. 

Results: Regarding joint validation on the DR-AMD dataset, the system achieved an area under 

the  ROC  curve  (AUC)  of  95.1%  for  detection  of  referable  DR  (SE=90.1%,  SP=90.6%).  For 

referable AMD, the AUC was 94.9% (SE=91.8%, SP=87.5%). Average human performance for 

DR was SE=61.5% and SP=97.8%; for AMD, SE=76.5% and SP=96.1%. Regarding detection of 

referable  DR  in  Messidor,  AUC  was  97.5%  (SE=92.0%,  SP=92.1%);  for  referable  AMD  in 

AREDS, AUC was 92.7% (SE=85.8%, SP=86.0%). 

Conclusions:  The  validated  system  performs  comparably  to  human  experts  at  simultaneous 

detection of DR and AMD. This shows that DL systems can facilitate access to joint screening of 

eye diseases and become a quick and reliable support for ophthalmological experts. 

2 

 
 
 
Introduction 

Screening for eye diseases has become a high-priority healthcare service to prevent vision loss.1,2 

Due to its proven efficiency, screening programs based on periodical examinations of the retina 

have been increasingly implemented worldwide.3-5 Established protocols rely on manual readings 

by  highly-specialized  workforce,6  failing  to  meet  the  requirements  of  large-scale  screening  in 

high-  and  low-resource  countries.7-11  Furthermore,  cost-effectiveness  remains  to  be  the  main 

burden for establishing screening programs,12-14 and different protocols are followed for different 

diseases,15-16 which translates to a larger burden to health systems and to the patient, that needs to 

undergo several of them. Nevertheless, exploiting the fact that examination protocols of retinal 

diseases rely mostly on the same principles and actions, it becomes more efficient to integrate 

them in one workflow.17,18 

Diabetic retinopathy (DR) has become a leading cause of preventable blindness worldwide with 

an overall prevalence of 35% among people with diabetes, which affects 1 in every 11 adults.19-

21 Age-related macular degeneration (AMD) is the most common cause of blindness in developed 

countries, being 9% its worldwide prevalence.22 Up to 80% of blindness cases caused by these 

diseases  are  avoidable  if  detected  early  enough  to  undergo  treatment.23,24  Nevertheless,  their 

incidence is expected to increase within the following decades, due to population ageing and the 

increasing prevalence of diabetes.20,22 Screening protocols for DR have been established in several 

countries.25,26  Regarding  AMD,  there  is  no  established  screening  protocol  but  it  will  soon  be 

required,27,28 since treatment options are still limited, although under development.29-30 

Automated screening solutions aim to provide a scalable, sustainable and high-quality approach 

to meet the increasing demand, while reducing the burden on highly-trained professionals and the 

associated costs. The introduction of deep learning (DL) has constituted a revolution in medical 

imaging  analysis.32,33  Previous  solutions for  the  automatic  analysis  of  retinal images34,35    have 

been outperformed by DL approaches.36 Several DL systems for the automatic detection of DR38-

41  and  AMD42-44 have  showed  performance  close  or  even  superior  to  that  achieved  by  human 

3 

 
graders. However, these systems perform independent analysis of each disease, although these 

diseases can co-exist and a solution for joint detection would be beneficial.17,18,45  

In this study, we present the validation of  a commercially-available, CE-certified DL software 

package, RetCAD v.1.3.0 (Thirona, Nijmegen, The Netherlands), that allows for joint detection 

of referable DR and AMD in color fundus (CF) images. The aim is to analyze the capability of a 

DL system to simultaneously identify both diseases and compare it with human experts and the 

current state-of-the-art methods, in order to determine the potential for automated joint screening 

of eye diseases. 

Methods 

Evaluation data 

The  validation  of  the  DL  system  was  first  performed  on  a  DR-AMD  dataset,  which  contains 

referable  and  non-referable  cases  of  DR  and  AMD,  for  the  joint  detection  of  both  diseases. 

Additional validation of individual detection of DR and AMD was assessed on Messidor and the 

Age-Related Eye Disease Study (AREDS) dataset, respectively.  

The DR-AMD dataset was extracted from a set of images collected in three different European 

medical centers (Sweden, Denmark, Spain). In total, 8871 images from more than 2000 patients 

were acquired during routine clinical practice between August 2011 and October 2016,  with a 

CR-2PlusAF fundus camera (Canon, Tokyo, Japan), at 45-degree field of  view with an image 

resolution  between  2376×1584  and  5184×3456  pixels.  No  mydriasis  was  applied.  Informed 

written consent was obtained from all patients at the medical centers and images were anonymized 

prior to transfer and use in this study, following the tenets set forth in the Declaration of Helsinki. 

The  8871  images  went  through  a  human  quality  check,  regarding  contrast,  clarity  and  focus, 

where  1785  images  were  excluded.  The  remaining  7086  images  went  through  a  preliminary 

grading, performed by a person with over six years of experience reading CF images.  Images 

were classified as referable AMD (1232 images), referable DR (381 images) or control (5519 

4 

 
images), which indicates non-referability for both DR and AMD, although other diseases might 

be present. Lastly, a random selection of 600 images  was performed, containing 150 referable 

AMD cases, 150 referable DR cases, and 300 controls, in order to ensure an enriched set. These 

images belong to 288 different patients, with an average of 2.11 images and 1.18 visits per patient. 

The 600 images define the DR-AMD set used for validation of joint detection of DR and AMD. 

The diagram in Supplementary Figure S1 summarizes the extraction of the dataset.   

Messidor is a publicly-available collection of macula-centered  CF images commonly used for 

performance comparison between automated DR detection systems. This dataset consists of 1200 

images  acquired  by  three  different  ophthalmologic  departments  using  a  3CCD  camera  on  a 

Topcon TRC NW6 non-mydriatic retinography with a 45-degree field of view, with an image 

resolution of 1440×960, 2240×1488 or 2304×1536 pixels. 800 images were acquired with pupil 

dilation and 400 without dilation.46 

AREDS dataset is currently the largest available set for AMD, previously used for the validation 

of automated AMD detection. AREDS was designed as a long-term prospective study of AMD 

development  and  cataract  in  which  patients  were  regularly  examined  and  followed  up  to  12 

years.47 Institutional review board approvals were obtained from each clinical center involved in 

the study, and written informed consent was obtained from each participant. The AREDS dbGaP 

set includes digitalized CF images. In 2014, over 134,000 macula-centered CF images from 4613 

participants were added to the set. We excluded images containing a lesion which disqualified an 

eye from the study, images considered as not gradable, and those which belong to eyes that were 

not included in the study, as mentioned in the AREDS dbGaP guidelines.48 In total, 133821 were 

used in this study. 

Grading 

To establish the reference standard (RS) in the DR-AMD dataset, the 600 images were scored by 

stage of disease severity for both DR and AMD by a certified ophthalmologist with more than 

twelve  years  of  experience  (IC).  In  the  case  of  DR,  the  grading  is  based  on  the  International 

5 

 
Clinical  Diabetic  Retinopathy  (ICDR)  severity  scale,  with  stages  0  (no  DR),  1  (mild  non-

proliferative  DR),  2  (moderate  non-proliferative  DR),  3  (severe  non-proliferative  DR),  and  4 

(proliferative DR).49 For AMD, the grading protocol is based on the AREDS system, with stages 

1 (no AMD), 2 (early AMD), 3 (intermediate AMD), and 4 (advanced AMD; with presence of 

foveal geographic atrophy or choroidal neovascularization).50 The measuring grid often used as 

part of the AREDS protocol was not applied for grading the DR-AMD dataset, taking into account 

lesions in the whole image and not only those located within the grid area.   

For comparison with human performance at joint detection of DR and AMD, four independent 

observers  also  provided  a  score  for  each  disease.  Two  of  the  graders  were  certified 

ophthalmologists with between one and three years of experience  (VS, PH)  and the other two 

graders were ophthalmology residents in their last year of residency (YL, AD). 

The gradings from the RS and the independent observers were then adjusted for the adaptation of 

the detection of both diseases into two separate binary classifications. In the case of DR: non-

referable DR (stage 0 or 1) and referable DR (stage 2, 3, or 4); for AMD: non-referable AMD 

(stage 1 or 2) and referable AMD (stage 3 or 4). Cases without both referable DR and referable 

AMD are referred to as controls from now on. Note that this implies non-referability for both DR 

and AMD, but other eye diseases might be present. 

The reference standard for Messidor was made publicly available when the dataset was originally 

published, with the subsequent correction of the published errata until the realization of this study 

in  2018.46  Medical  experts  provided  the  retinopathy  grade  for  each  image,  consisting  of  four 

distinct categories, from 0 to 3, ranging from normal to increasing severity of DR. In order to 

translate this RS into referable/non-referable classification, images assigned with DR stage 0 or 

1  were  considered  non-referable  cases;  those  with  DR  stage  2  or  3,  referable.  For  human 

performance  comparison,  manual  annotations  were  performed  by  two  independent  graders,  a 

general ophthalmologist and a retinal specialist, with 4 and 20 years of DR screening experience, 

respectively, following the same protocol as the RS.51 

6 

 
The reference standard for the AREDS dataset corresponds to the publicly-available grading in 

AREDS dbGaP, which is based on the AREDS severity scale for AMD described previously.48,50 

These scores were assigned to the images by experts at US grading centers, being consistent with 

the original AREDS AMD categorization without considering visual acuity.52 This RS was then 

adapted following the mentioned procedure into referable and non-referable cases for performing 

binary classification. 

Table 1 summarizes distribution of disease severity for DR and AMD in the validation datasets 

regarding the corresponding reference standard. 

Automated grading approach 

The DL system under validation uses convolutional neural networks (CNN) for the classification 

task of grading.53-57 CNNs are organized in multiple layers with artificial neurons, which learn 

representations of the input data at increasing levels of abstraction.36 Convolution operations act 

as feature detectors with adjustable parameters called weights. During training, the network is 

presented with a large set of annotated images. For each image, an output class label is produced 

in  a  forward  pass  through  the  network  and  a  loss  function  is  computed  to  measure  the  error 

between  the  output  and  the  actual  label.  With  the  aim  of  reducing  the  error,  the  weights  are 

adjusted by means of backpropagation.32 This process is repeated with several passes over the 

training data until the loss converges. 

RetCAD v.1.3.0 consists of two ensembles of state-of-the-art CNN architectures. Both ensembles, 

correspondingly, allow for the detection of referable DR and AMD. Firstly, each input image goes 

through a preprocessing stage, followed by an assessment of image quality. Then, joint image-

level detection of DR and AMD is applied. Each ensemble provides one score between 0 and 100 

which  is  monotonically  related  to  the  likelihood  of  presence  of  referable  DR  and  AMD, 

respectively. The final score for each disease is obtained by averaging the scores generated by the 

networks in each ensemble. 

7 

 
None of the images included in the datasets used in this validation study were used for training 

the system. 

Evaluation design 

To evaluate the performance of the system at automated joint detection of referable DR and AMD, 

we performed several validation experiments on the DR-AMD dataset. For detection of referable 

DR, binary classification was assessed between DR cases and the joint set of controls and AMD 

cases (DR vs. AMD + controls). A second binary classification for DR was performed between 

referable  and  only  control  cases,  in  order  to  analyze  the  influence  of  joint  AMD  cases  in  the 

performance of the system (DR vs. controls). The same procedure was applied for detection of 

referable AMD, assessing first a binary classification between AMD cases and the joint set of 

controls and DR cases (AMD vs. DR + controls), and a subsequent binary classification between 

AMD and only control cases (AMD vs. controls). 

Regarding validation of individual detection of referable DR and AMD, binary classification was 

performed between referable and non-referable DR cases in Messidor, and between referable and 

non-referable AMD cases in the AREDS dataset.  

The performance metrics used for validation were sensitivity (SE) and specificity (SP), defined 

as  the  proportions  of  cases  considered  referable  and  non-referable,  respectively,  by  both  the 

system and the reference standard. The tradeoff between both metrics was furtherly observed by 

means  of receiver  operating  characteristic  (ROC)  analysis. The optimal  operating  point  of  the 

system was considered to be the best tradeoff between SE and SP, i.e., the point closest to the 

upper left corner of the graph. For an overall interpretation of the system’s ability to discriminate 

between referable and non-referable cases, the area under the ROC curve (AUC) was computed. 

Human  performance  was  also  evaluated  by  computing  sensitivity  and  specificity  from  the 

gradings of each observer and then included in the corresponding ROC curve as operating points.  

8 

 
Data bootstrapping was used to assess statistical significance of the obtained evaluation metrics.58 

Samples  were  bootstrapped  1000  times  to  generate  a  distribution  of  each  evaluation  metric, 

obtaining the 2.5 and 97.5 percentiles as 95% confidence intervals (CI). 

Additionally,  in the validation datasets where gradings by independent observers were available, 

i.e.,  DR-AMD  dataset  and  Messidor,  intergrader  variability  was  measured  by  means  of  the 

quadratic Cohen’s weighted kappa coefficient (κ), between gradings per disease stage and the 

corresponding reference standard.59 

Results 

For the 600 images in the DR-AMD dataset, the ROC analysis corresponding to DR vs. AMD + 

controls is shown in Figure 1A. The optimal operating point of RetCAD v.1.3.0 corresponds to 

SE of 90.1% (95% CI, 84.2%-96.6%) and SP of 90.6% (95% CI, 85.9%-97.0%), with AUC of 

95.1%  (95%  CI,  90.8%-98.2%).  Average  observer  SE  and  SP  were  61.5%  and  97.8%, 

respectively. Figure 1B shows the ROC curve and optimal operating point by the DL  system 

regarding AMD vs. DR + controls. SE was 91.8% (95% CI, 84.4%-97.6%), SP was 87.5% (95% 

CI, 83.5%-97.9%), and AUC was 94.9% (95% CI, 90.9%-97.9%). Average observer SE and SP 

were 76.5% and 96.1%, respectively. Table 2 summarizes diagnostic performance of the system 

and the human observers for both validation experiments. 

Regarding validation of DR vs. controls on the DR-AMD dataset,  AUC was  95.6% (95% CI, 

91.8%-98.6%),  SE  was  91.7%  (95%  CI,  85.3%-98.0%)  and  SP  was  90.9%  (95%  CI,  86.7%-

96.7%). As for AMD vs. controls, AUC was 95.2% (95% CI, 91.0%-98.1%), SE was 88.6% (95% 

CI, 83.8%-100.0%) and SP was 92.1% (95% CI, 84.3%-95.2%). The corresponding ROC analysis 

and distribution of both classification results of RetCAD v.1.3.0 and the observers can be found 

in Supplementary Figure S2 and Supplementary Table S1. 

Intergrader disagreement in the DR-AMD dataset is shown in Figure 2, which includes interrater 

heatmaps with quadratic-weighted κ scores among the four observers and the reference standard, 

for DR and AMD. 

9 

 
Regarding  the  performance  validation  of  the  system  and  external  observers  at  detection  of 

referable DR in Messidor, the obtained results can be found in Figure 3A. The AUC was 97.5% 

(95% CI, 96.3%-98.5%), SE was 92.0% (95% CI, 89.3%-97.2%) and SP was 92.1% (95% CI, 

88.6%-95.2%). Diagnostic performance by the system and the two observers is summarized in 

Supplementary Table S2, while Supplementary Figure S3 shows the intergrader discrepancy 

among observers and the reference standard. 

The results of the ROC analysis for automated detection of referable AMD in the AREDS dataset 

are shown in Figure 3B. For the 133821 images, the DL system reached 85.8% (95% CI, 84.6%-

86.2%) for SE and 86.0% (95% CI, 85.7%-87.4%) for SP. AUC was 92.7% (95% CI, 92.5%-

92.9%).  The  classification  results  regarding  the  reference  standard  can  be  found  in 

Supplementary Table S3. 

Discussion 

In this study we validated the performance  for joint detection of referable DR and AMD of a 

commercially-available,  CE-certified  DL  system,  RetCAD  v.1.3.0  (Thirona,  Nijmegen,  The 

Netherlands) and compared it with independent human observers. The results in the DR-AMD 

dataset show the system is able to differentiate between the two diseases, which is one of the main 

aspects in joint detection. When identifying referable DR, false positive detections can be divided 

in cases graded as control or referable AMD in the reference standard, being the latter the 17.4% 

of the cases wrongly classified as referable DR. Regarding  false positive cases at detection of 

referable AMD, 24.2% were graded as referable DR in the reference standard. Furthermore, the 

performance  of  the  system  is  not  significantly  altered  when  individual  disease  detection  is 

assessed on the same dataset. 

The outcome of the joint validation also demonstrates the DL system performs comparably to 

human experts. RetCAD v.1.3.0 reaches lower specificity levels than human average, but higher 

sensitivity for both DR and AMD. This is particularly important at automated screening settings, 

10 

 
where fewer referable cases must be missed when the system is used for either initial assessment 

or grading support. 

Regarding  intergrader  variability,  greater  disagreement  was  observed  for  AMD,  which  might 

show the necessity of establishing AMD screening protocols as the ones already used for DR. For 

DR,  we  observed  relatively  low  sensitivity  scores  for  the  observers  regarding  the  reference 

standard, since many of the cases classified as stage 2 in the reference were graded by observers 

as stage 1. However, interobserver scores are still relatively high. This indicates graded stages are 

close, but intermediate DR levels become problematic for referable/non-referable classification.  

DL-based automated joint detection was also assessed by Ting et al.,45 reaching lower AUC values 

at  detection  of  referable  DR  and  AMD,  although  larger  datasets  were  used  for  validation  and 

detection of glaucoma was also evaluated. However, fewer external observers were included and 

different validation sets were used for identification of DR and AMD, which leaves the influence 

of each disease at joint screening unclear. 

Validation of individual detection of referable DR in Messidor shows exceptional performance 

by  RetCAD  v.1.3.0,  also  comparable  to  human  experts.  Intermediate  DR  stages  are  generally 

more difficult to identify (97.6% of false negatives belong to stage 2 and 92.7% of false positives 

belong  to  stage  1),  as  noted  previously  with  the  human  observers  in  the  DR-AMD  dataset. 

Nevertheless, detection errors are kept remarkably low. 

Previous  DL  approaches  for  DR  detection  have  been  reported  in  Messidor-260  with  optimal 

performances.  Since  there  is  no  publicly-available  image-based  reference  standard  for  this 

extension of Messidor, we reported on the original set to allow for further comparison. Gulshan 

et  al.38  used  their  own  reference  standard  for  Messidor-2,  whereas  patient-based  reference 

standard was made available and applied by Abramoff et al.34,40,61 We used this RS for additional 

validation in Messidor-2 (see Supplementary  Results Appendix, Supplementary  Figure S4 

and Supplementary Table S4). 

11 

 
The results of individual detection of referable AMD in the AREDS dataset show that, as with 

DR, misclassifications shift towards intermediate stages (86.0% of false positives belong to cases 

graded as AMD stage 2 in the reference standard, whereas 67.1% of false negatives belong to 

stage 3). RetCAD v.1.3.0 performs at a good level, considering the images in this set are digitized 

analog photographs. Burlina et al.42 also reported on DL-based referable AMD detection in the 

whole AREDS dataset, using the set also for training, which might explain better performance.  

Limitations and future work 

Although the output score of the validated DL system for DR and AMD is related to the presence 

of each disease, there is no clear cutoff for disease staging, which could be especially beneficial 

for easier identification of intermediate stages, since they tend to be more ambiguous to diagnose. 

Integrating  other  imaging  modalities  such  as  optical  coherence  tomography  could  provide 

valuable information for diagnosis. However, due to cost-effectiveness and easier adaptation in 

telemedicine,62  CF  imaging  facilitates  screening  of  eye  diseases,  especially  in  developing 

countries. 

This  validation  shows  the  capacity  of  a  commercially-available  DL  system  to  assess  joint 

detection  of  DR  and  AMD.  However,  future  integration  of  automated  detection  of  other  eye 

diseases that might co-exist, such as glaucoma and cataracts, might increase usability and support 

at screening settings. 

With  respect  to  this  study,  the  human  observers  were  professional  ophthalmologists  or 

ophthalmologists  in  training,  who  are  used  to  clinical  working  settings  and  tasks,  where  the 

prevalence  of  disease  and  manual  grading  tasks  differ  from  those  of  real  screening  settings. 

Besides, for evaluation of joint detection performance, one DR-AMD set of 600 images from 288 

patients was used. In this dataset, patients might contain images from different visits, and in some 

cases,  several  images  from  the  same  visit.  Future  studies  on  automated joint  screening  would 

benefit from more and larger validation datasets, with more subjects and increased inter-subject 

variability, which would allow to analyse the effect of higher patient and imaging diversity on the 

12 

 
performance  of  automated  approaches.  Additionally,  these  datasets  would  be  even  more 

beneficial by including graded cases with different severity levels for DR, AMD and additional 

eye diseases. 

In  conclusion,  this  validation  study  shows  the  capability  of  a  commercially-available,  CE-

certified  DL  system  to  assess  simultaneous  detection  of  DR  and  AMD  with  performance 

comparable to human experts. This demonstrates that an automated solution for joint detection 

would  be  beneficial  at  screening  settings,  since  eye  diseases  can  co-exist  and  examination 

protocols rely on the same principles and actions, while reducing subjectivity due to interobserver 

disagreement. This also shows that DL systems can facilitate access to screening of eye diseases, 

both  in  high-  and  low-resource  areas,  and  become  a  quick  and  reliable  support  for 

ophthalmological experts. 

13 

 
 
References 

1.  Rowe, S., MacLean, C. H., Shekelle, P. G. (2004). Preventing visual loss from chronic 

eye disease in primary care: scientific review. Jama, 291(12), 1487-1495. 

2.  Cunha‐Vaz, J. (1998). Lowering the risk of visual impairment and blindness. Diabetic 

medicine, 15(S4 4), S47-S50. 

3.  Jones, S., & Edwards, R. T. (2010). Diabetic retinopathy screening: a systematic review 

of the economic evidence. Diabetic medicine, 27(3), 249-256. 

4.  James,  M.,  Turner,  D.  A.,  Broadbent,  D.  M.,  Vora,  J.,  Harding,  S.  P.  (2000).  Cost 
eye 

threatening 

screening 

diabetic 

sight 

for 

of 

effectiveness 
disease. Bmj, 320(7250), 1627-1631. 

analysis 

5.  Arun, C. S., Ngugi, N., Lovelock, L., Taylor, R.  (2003). Effectiveness of screening in 
preventing blindness due to diabetic retinopathy. Diabetic medicine, 20(3), 186-190. 

6.  Piñero, D. P. (2013). Screening for eye disease: Our role, responsibility and opportunity 

of research. Journal of optometry, 6(2), 67-68. 

7.  Shaw, J. E., Sicree, R. A., Zimmet, P. Z. (2010). Global estimates of the prevalence of 

diabetes for 2010 and 2030. Diabetes research and clinical practice, 87(1), 4-14. 

8.  United  Nations  Department  of  Economic  and  Social Affairs.  World  Population Aging 
http://www.un.org/en/development/desa/population/publications/.  

2017. 

Report 
Accessed March 4, 2019. 

9.  Harmon, D., Merritt, J. (2009). Demand for Ophthalmic Services and Ophthalmologists–

A Resource Assessment. A Study Prepared by Market Scope, 200, 1-2. 

10.  Guariguata, L., Whiting, D. R., Hambleton, I., Beagley, J., Linnenkamp, U., Shaw, J. E. 
(2014).  Global  estimates  of  diabetes  prevalence  for  2013  and  projections  for 
2035. Diabetes research and clinical practice, 103(2), 137-149. 

11.  Wong,  W.  L.,  Su,  X.,  Li,  X.,  et  al.  (2014).  Global  prevalence  of  age-related  macular 
degeneration and disease burden projection for 2020 and 2040: a systematic review and 
meta-analysis. The Lancet Global Health, 2(2), e106-e116. 

12.  Wormald, R. (1999).  Epidemiology in practice: Screening for eye disease. Community 

eye health, 12(30), 29. 

13.  Karnon,  J.,  Czoski-Murray,  C.,  Smith,  K.,  et  al.  (2008).  A  preliminary  model-based 
assessment of the cost–utility of a screening programme for early age-related macular 
degeneration. 
In NIHR  Health  Technology  Assessment  programme:  Executive 
Summaries. NIHR Journals Library. 

14.  Hernández, R. A., Burr, J. M., Vale, L. D. (2008). Economic evaluation of screening for 
open-angle  glaucoma. International  journal  of  technology  assessment  in  health 
care, 24(2), 203-211. 

15.  AAO  PPP  Retina/Vitreous  Panel,  Hoskins  Center  for  Quality  Eye  Care.  Preferred 
practice pattern: diabetic retinopathy - Updated 2017.  https://www.aao.org/preferred-
practice-pattern/diabetic-retinopathy-ppp-updated-2017.  Accessed March 4, 2019. 

16.  AAO  PPP  Retina/Vitreous  Panel,  Hoskins  Center  for  Quality  Eye  Care.  Preferred 
Updated 
https://www.aao.org/preferred-practice-pattern/age-related-macular-

practice 
2015.  
degeneration-ppp-2015.  Accessed March 4, 2019. 

degeneration 

age-related 

macular 

pattern: 

- 

14 

 
17.  Chew, E. Y., Schachat, A. P. (2015). Should we add screening of age-related macular 
diabetic 

programs 

current 

degeneration 
retinopathy?. Ophthalmology, 122(11), 2155-2156. 

screening 

for 

to 

18.  Chan,  C.  K.,  Gangwani,  R.  A.,  McGhee,  S.  M.,  Lian,  J.,  Wong,  D.  S.  (2015).  Cost-
effectiveness  of  screening  for  intermediate  age-related  macular  degeneration  during 
diabetic retinopathy screening. Ophthalmology, 122(11), 2278-2285. 

19.  International  Diabetes  Federation  (IDF).  IDF  Diabetes  Atlas  8th  Edition  2017. 

http://www.diabetesatlas.org. Accessed March 4, 2019. 

20.  International  Agency  for  the  Prevention  of  Blindness  (IAPB).  Diabetic  Retinopathy. 
https://www.iapb.org/knowledge/what-is-avoidable-blindness/diabetic-retinopathy. 
Accessed March 4, 2019. 

21.  Yau, J. W., Rogers, S. L., Kawasaki, R., et al. (2012). Global prevalence and major risk 

factors of diabetic retinopathy. Diabetes care, 35(3), 556-564. 

22.  Wong,  W.  L.,  Su,  X.,  Li,  X.,  et  al.  (2014).  Global  prevalence  of  age-related  macular 
degeneration and disease burden projection for 2020 and 2040: a systematic review and 
meta-analysis. The Lancet Global Health, 2(2), e106-e116. 

23.  Pascolini, D., Mariotti, S. P. (2012). Global estimates of visual impairment: 2010. British 

Journal of Ophthalmology, 96(5), 614-618. 

24.  World  Health  Organization,  Global  Action  Plan  for  the  Prevention  of  Avoidable 
Blindness  and  Visual  Impairment  2014-2019  -  Towards  Universal  Eye  Health. 
https://www.iapb.org/advocacy/global-action-plan-2014-2019. Accessed March 4, 2019. 

25.  NHS  Diabetic  Eye  Screening  Programme. https://www.gov.uk/topic/population-

screening-programmes/diabetic-eye. Accessed March 4, 2019. 

26.  Nederlands  Oogheelkundig  Gezelschap,  Diabetic 

guideline, 
https://www.oogheelkunde.org/richtlijn/diabetische-retinopathie-multidisciplinaire-
richtlijn-geautoriseerd-november-2017. Accessed March 4, 2019. 

authorized 

November 

retinopathy,  multidisciplinary 
2017. 

27.  Jain,  S.,  Hamada,  S.,  Membrey,  W.  L.,  Chong,  V.  (2006).  Screening  for  age-related 
macular degeneration using nonstereo digital fundus photographs. Eye, 20(4), 471. 

28.  Ouyang, Y., Heussen, F. M., Keane, P. A., Sadda, S. R., Walsh, A. C. (2013). The Retinal 
Disease Screening Study: Prospective Comparison of Nonmydriatic Fundus Photography 
and Optical Coherence Tomography for Detection of Retinal Irregularities Nonmydriatic 
FP versus OCT. Investigative ophthalmology & visual science, 54(2), 1460-1468. 

29.  Comer, G. M., Ciulla, T. A., Criswell, M. H., Tolentino, M. (2004). Current and future 
age-related  macular 

nonexudative 

exudative 

options 

and 

for 
treatment 
degeneration. Drugs & aging, 21(15), 967-992. 

30.  Zarbin, M. A., Rosenfeld, P. J. (2010). Pathway-based therapies for age-related macular 
degeneration:  an  integrated  survey  of  emerging  treatment  alternatives. Retina, 30(9), 
1350-1367. 

31.  Gehrs,  K.  M.,  Jackson,  J.  R.,  Brown,  E.  N.,  Allikmets,  R.,  Hageman,  G.  S.  (2010). 
Complement,  age-related macular  degeneration  and a  vision of the future. Archives  of 
ophthalmology, 128(3), 349-358. 

32.  LeCun, Y., Bengio, Y., Hinton, G. (2015). Deep learning. Nature, 521(7553), 436. 

33.  Litjens, G., Kooi, T., Bejnordi, B. E., et al. (2017). A survey on deep learning in medical 

image analysis. Medical image analysis, 42, 60-88. 

15 

 
34.  Abràmoff, M.  D.,  Folk, J. C.,  Han,  D. P.,  et al.  (2013).  Automated analysis  of retinal 
images  for  detection  of  referable  diabetic  retinopathy. JAMA  ophthalmology, 131(3), 
351-357. 

35.  Burlina, P., Freund, D. E., Dupas, B., Bressler, N. (2011, August). Automatic screening 
of  age-related  macular  degeneration  and  retinal  abnormalities.  In Engineering  in 
Medicine  and  Biology  Society,  EMBC,  2011  Annual  International  Conference  of  the 
IEEE (pp. 3962-3966). IEEE. 

36.  Schmidt-Erfurth, U., Sadeghipour, A., Gerendas, B. S., Waldstein, S. M., Bogunović, H. 

(2018). Artificial intelligence in retina. Progress in retinal and eye research. 

37.  Raman, R., Srinivasan, S., Virmani, S., Sivaprasad, S., Rao, C., Rajalakshmi, R. (2018). 
in  detecting  diabetic 
learning 

algorithms 

Fundus  photograph-based  deep 
retinopathy. Eye, 1. 

38.  Gulshan, V., Peng, L., Coram, M., et al. (2016). Development and validation of a deep 
fundus 

for  detection  of  diabetic 

retinopathy 

retinal 

in 

learning  algorithm 
photographs. JAMA, 316(22), 2402-2410. 

39.  Gargeya, R., Leng, T. (2017). Automated identification of diabetic retinopathy using deep 

learning. Ophthalmology, 124(7), 962-969. 

40.  Abràmoff, M. D., Lou, Y., Erginay, A., Clarida, W., Amelon, R., Folk, J. C., Niemeijer, 
M. (2016). Improved automated detection of diabetic retinopathy on a publicly available 
dataset  through  integration  of  deep  learning. Investigative  ophthalmology  &  visual 
science, 57(13), 5200-5206. 

41.  Abràmoff, M. D., Lavin, P. T., Birch, M., Shah, N., Folk, J. C. (2018). Pivotal trial of an 
autonomous AI-based diagnostic system for detection of diabetic retinopathy in primary 
care offices. npj Digital Medicine, 1(1), 39. 

42.  Burlina,  P.  M.,  Joshi,  N.,  Pekala,  M.,  Pacheco,  K.  D.,  Freund,  D.  E.,  Bressler,  N.  M. 
(2017). Automated Grading of Age-Related Macular Degeneration From Color Fundus 
Images  Using  Deep  Convolutional  Neural  Networks. JAMA  ophthalmology, 135(11), 
1170-1176. 

43.  Grassmann, F., Mengelkamp, J., Brandl, C., et al. (2018). A Deep Learning Algorithm 
for Prediction of Age-Related Eye Disease Study Severity Scale for Age-Related Macular 
Degeneration from Color Fundus Photography. Ophthalmology, 125(9), 1410-1420. 

44.  Peng, Y., Dharssi, S., Chen, Q., et al. (2018). DeepSeeNet: A deep learning model for 
automated classification of patient-based age-related macular degeneration severity from 
color fundus photographs. Ophthalmology. 

45.  Ting, D. S. W., Cheung, C. Y. L., Lim, G., et al. (2017). Development and validation of 
a  deep  learning  system  for  diabetic  retinopathy  and  related  eye  diseases  using  retinal 
images from multiethnic populations with diabetes. JAMA, 318(22), 2211-2223. 

46.  ADCIS  (Image  Processing  Algorithm  Evaluation  and  data  Management).  Messidor 
dataset. (Methods to evaluate segmentation and indexing techniques in the field of retinal 
ophthalmology). 
http://www.adcis.net/en/DownloadThirdParty/Messidor.html. 
Accessed March 4, 2019. 

47.  National Eye Institute (NEI) Age-Related Eye Disease Study (AREDS). dbGaP Study 
https://www.ncbi.nlm.nih.gov/projects/gap/cgi-

Accession. 
bin/study.cgi?study_id=phs000001.v3.p1. Accessed March 4, 2019. 

48.  National Eye Institute (NEI). AREDS dbGaP Data Tables: A User’s Guide. Version 1.0. 

https://www.ncbi.nlm.nih.gov/projects/gap/cgi-bin/GetPdf.cgi?id=phd001552.1. 
Accessed March 4, 2019. 

16 

 
49.  Wilkinson, C. P., Ferris, F. L., Klein, R. E., et al. (2003). Proposed international clinical 
severity 

diabetic  macular 

retinopathy 

disease 

edema 

and 

diabetic 
scales. Ophthalmology, 110(9), 1677-1682. 

50.  Age-Related Eye Disease Study Research Group. (2001). The Age-Related Eye Disease 
Study system for classifying age-related macular degeneration from stereoscopic color 
fundus  photographs: the  Age-Related  Eye  Disease  Study  Report  Number  6. American 
journal of ophthalmology, 132(5), 668-681. 

51.  Sánchez, C. I., Niemeijer, M., Dumitrescu, A. V., Suttorp-Schulten, M. S., Abramoff, M. 
D.,  van  Ginneken,  B.  (2011).  Evaluation  of  a  computer-aided  diagnosis  system  for 
diabetic  retinopathy  screening  on  public  data. Investigative  ophthalmology  &  visual 
science, 52(7), 4866-4871. 

52.  National  Eye  Institute  (NEI).  AREDS  Manual  of  Operations,  Study  Design,  Section 
3.1.2.8. https://www.ncbi.nlm.nih.gov/projects/gap/cgi-bin/GetPdf.cgi?id=phd000003.2. 
Accessed March 4, 2019. 

53.  LeCun, Y., Bottou, L., Bengio, Y., Haffner, P. (1998). Gradient-based learning applied 

to document recognition. Proceedings of the IEEE, 86(11), 2278-2324. 

54.  Krizhevsky, A., Sutskever, I., Hinton, G. E. (2012). Imagenet classification with deep 
information  processing 

in  neural 

convolutional  neural  networks.  In Advances 
systems (pp. 1097-1105). 

55.  Simonyan, K., Zisserman, A. (2014). Very deep convolutional networks for large-scale 

image recognition. arXiv preprint arXiv:1409.1556. 

56.  Szegedy,  C.,  Liu,  W.,  Jia,  Y.,  et  al.  (2015).  Going  deeper  with  convolutions. 
In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 
1-9). 

57.  Szegedy,  C.,  Ioffe,  S.,  Vanhoucke,  V.,  Alemi,  A.  A.  (2017).  Inception-v4,  inception-
resnet and the impact of residual connections on learning. In AAAI (Vol. 4, p. 12). 

58.  Efron,  B.,  Tibshirani,  R.  J.  An  introduction  to  the  bootstrap.  (1993).  Boca  Raton. FL 

Chapman & Hall/CRC, 202-236. 

59.  Hripcsak,  G.,  Heitjan,  D.  F.  (2002).  Measuring  agreement  in  medical  informatics 

reliability studies. Journal of biomedical informatics, 35(2), 99-110. 

60.  Laboratoire  de  Traitement  de  l’Information  Médicale  (LaTIM  INSERM  U1101). 
Messidor-2  dataset  (Methods  to  evaluate  segmentation  and  indexing  techniques in the 
field  of 
(2011).  http://latim.univ-brest.fr/indexfce0.html. 
Accessed  November 21, 2018. 

retinal  ophthalmology) 

61.  University  of  Iowa  Health  Care,  Department  of  Ophthalmology  and  Visual  Sciences, 
Michael D. Abramoff, MD, PhD. Available at: https://medicine.uiowa.edu/eye/abramoff. 
Accessed March 4, 2019. 

62.  Cuadros,  J.,  Bresnick,  G.  (2009).  EyePACS:  an  adaptable  telemedicine  system  for 
diabetic  retinopathy  screening. Journal  of  diabetes  science  and  technology, 3(3),  509-
516. 

17 

 
 
Table 1. Disease severity distribution for DR and AMD in the validation datasets.  

Disease 
stage 

DR-AMD*,† 

Messidor‡ 

AREDS† 

DR 

AMD 

NR 

R 

NR 

R 

0 

1 

2 

3 

4 

1 

2 

3 

4 

489 
(81.5) 

111 
(18.5) 

527 
(87.8) 

73 
(12.2) 

487 
(81.2) 

2 (0.3) 

82 
(13.6) 

4 (0.7) 

25 
(4.2) 
483 
(80.5) 
44 
(7.3) 
54 
(9.0) 
19 
(4.2) 

700 
(58.3) 

500 
(41.7) 

547 
(45.6) 
153 
(12.7) 
247 
(20.6) 
253 
(21.1) 

- 

- 

- 

- 

- 

- 

- 

- 

- 

- 

- 

- 

- 

- 

74401 
(55.6) 

59420 
(44.4) 

41409 
(30.9) 
32992 
(24.7) 
41495 
(31.0) 
17925 
(13.4) 

Total images with available 
grading, No. (%) 

600 (100) 

1200 (100) 

133821 (100) 

* Reference standard for DR grading is ICDR (stages from 0 to 4) 

† Reference standard for AMD grading is AREDS (stages from 1 to 4). 

‡ Reference standard for DR grading is Messidor (stages from 0 to 3). 

Abbreviations: DR, diabetic retinopathy; AMD, age-related macular degeneration; R, referable; 

NR, non-referable. 

18 

 
 
 
 
 
Figure 1. Receiver operating characteristic curves for joint detection of referable DR (A) 

and AMD (B) in the DR-AMD dataset (600 images). 

Performance of RetCAD v.1.3.0 corresponds to the blue curves (95% CI within gray area); the 

colored circles, to the human observers. The black circle indicates the  SE and SP of RetCAD 

v.1.3.0 at its optimal operating point. For DR vs. AMD + controls (A), AUC was 95.1% (95% CI, 

90.8%-98.2%),  SE  was  90.1%  (95%  CI,  85.2%-96.8%)  and  SP  was  90.6%  (95%  CI,  85.5%-

96.7%).  For  AMD  vs.  DR  +  controls  (B),  AUC  was  94.9% (95%  CI,  90.9%-97.9%),  SE  was 

91.8% (95% CI, 84.6%-97.8%) and SP was 87.5% (95% CI, 83.5%-93.9%). 

Abbreviations:  DR,  diabetic  retinopathy;  AMD,  age-related  macular  degeneration;  AUC,  area 

under the receiver operating characteristic curve.; SE, sensitivity; SP, specificity; CI, confidence 

interval. 

19 

 
 
 
 
Table 2. Diagnostic performance for joint detection of referable DR and AMD of RetCAD 

v.1.3.0  and  observers  compared  with  reference  standard  in  the  DR-AMD  dataset  (600 

images). 

RetCAD 

Obs. 1 

Obs. 2 

Obs. 3 

Obs. 4 

DR 

RS 

R 

NR 

R 

99 

46 

(AMD, C) 

(8, 38) 

NR 

12 

443 

R 

60 

7 

(1, 6) 

NR 

51 

482 

R 

65 

14 

(4, 10) 

NR 

46 

475 

R 

82 

14 

(1, 13) 

NR 

29 

475 

R 

66 

9 

(1, 8) 

NR 

45 

480 

SE (%) 

90.1 

54.1 

58.6 

73.9 

59.5 

(95% CI)  

(85.2-96.8) 

(40.1-67.3) 

(46.3-70.9) 

(61.4-85.2) 

(46.7-72.0) 

SP (%) 

90.6 

98.6 

97.1 

97.1 

98.2 

(95% CI) 

(85.5-96.7) 

(97.0-99.6) 

(94.8-99.2) 

(94.7-99.2) 

(96.2-99.6) 

AMD 

RS 

R 

NR 

R 

66 

66 

(DR, C) 

(16, 50) 

NR 

7 

461 

R 

60 

26 

(8, 18) 

NR 

13 

501 

R 

41 

8 

(3, 5) 

NR 

32 

519 

R 

62 

22 

(7, 15) 

NR 

11 

505 

R 

60 

26 

(5, 21) 

NR 

13 

501 

SE (%) 

91.8 

82.1 

56.2 

84.9 

82.1 

(95% CI) 

(84.6-97.8) 

(69.0-93.8) 

(39.5-71.8) 

(72.5-96.0) 

(69.7-93.5) 

SP (%) 

87.5 

95.1 

98.5 

95.8 

95.1 

(95% CI) 

(83.5-93.9) 

(92.2-97.5) 

(96.9-99.6) 

(93.1-98.1) 

(92.2-97.5) 

Abbreviations: DR, diabetic retinopathy; AMD, age-related macular degeneration; RS, reference 

standard;  Obs.,  observer;  R,  referable;  NR,  non-referable;  C,  control;  SE,  sensitivity;  SP, 

specificity; CI, confidence interval. 

20 

 
 
 
 
 
 
 
 
 
Figure  2.  Intergrader  disagreement  in  DR  (A)  and  AMD  (B)  grading  stages  among 

independent human observers and reference standard in the DR-AMD dataset (600 images). 

Interrater  heatmaps  with  quadratic  Cohen’s  weighted  kappa  coefficients  comparing  disease 

staging for DR (A) and AMD (B) among the 4 independent human observers and the reference 

standard in the DR-AMD dataset (600 images with referable DR, AMD and control cases). 

Abbreviations:  DR,  diabetic  retinopathy;  AMD,  age-related  macular  degeneration;  Obs., 

observer; RS, reference standard. 

21 

 
 
 
Figure 3. Receiver operating characteristic curves for individual detection of referable DR 

in Messidor (1200 images) (A) and referable AMD in the AREDS dataset (133821 images) 

(B).  

Performance of RetCAD v.1.3.0 corresponds to the blue curves (95% CI within gray area); the 

colored circles, to the  human observers. The black circle indicates the  SE and SP of RetCAD 

v.1.3.0 at its optimal operating point.  For individual detection of referable DR (A), AUC was 

97.5% (95% CI, 96.3%-98.5%), SE was 92.0% (95% CI, 89.3%-97.2%) and SP was 92.1% (95% 

CI, 88.6%-95.2%). For individual detection of referable AMD (B), AUC was 92.7% (95% CI, 

92.5%-92.9%),  SE  was  85.8%  (95%  CI,  84.6%-86.2%)  and  SP  was  86.0%  (95%  CI,  85.7%-

87.4%). 

Abbreviations: DR, Diabetic retinopathy; AMD, Age-related macular degeneration; AUC, Area 

under the receiver operating characteristic curve; SE, sensitivity; SP, specificity; CI, confidence 

interval. 

22 

 
 
Supplementary Online Content 

Evaluation of a deep learning system for the joint automated  detection of 
diabetic retinopathy and age-related macular degeneration 

Figure S1. Extraction of the DR-AMD dataset. 

Figure S2. Receiver operating characteristic curves for individual detection of referable DR 
and AMD in the DR-AMD dataset. 

Table  S1.  Diagnostic  performance  for  individual  detection  of  referable  DR  and  AMD  of 
RetCAD v.1.3.0 and observers compared with reference standard in the DR-AMD dataset. 

Table S2. Diagnostic performance for individual detection of referable DR of RetCAD v.1.3.0 
and observers compared with reference standard in Messidor (1200 images). 

Figure S3. Interrater disagreement in DR grading stages among independent human observers 
and reference standard in Messidor (1200 images). 

Table  S3.  Diagnostic  performance  for  individual  detection  of  referable  AMD  of  RetCAD 
v.1.3.0 compared with reference standard in the AREDS dataset (133821 images). 

Results Appendix. Validation of individual detection of referable DR in Messidor-2. 

Figure S4. Receiver operating characteristic curve for individual detection of referable DR in 
Messidor-2 (874 subjects). 

Table S4. Diagnostic performance for individual detection of referable DR of RetCAD v.1.3.0 
compared with reference standard in Messidor-2 (874 subjects). 

 
 
 
 
 
 
 
 
 
Figure S1. Extraction of the DR-AMD dataset. 

The DR-AMD dataset was extracted from a set of 8871 images from more than 2000 patients collected in three 
different  European  medical  centers  (Sweden,  Denmark,  Spain)  during  routine  clinical  practice.  Images  went 
through a  human quality check, regarding contrast, clarity  and focus,  where 1785 images  were excluded. The 
remaining  7086  images  went  through  a  preliminary  grading,  performed  by  a  person  with  over  six  years  of 
experience  reading  CF  images.  Images  were  classified  as  referable  AMD  (1232  images),  referable  DR  (381 
images) or control (5519 images), which indicates non-referability for both DR and AMD, although other diseases 
might  be  present;  46  images  were  graded  as  having  both  referable  AMD  and  DR  present.  Lastly,  a  random 
selection of 600 images was performed, containing 150 referable AMD cases, 150 referable DR cases, and 300 
controls, in order to ensure an enriched set. These images belong to 288 different patients, with an average of 2.11 
images and 1.18 visits per patient. The 600 images define the DR-AMD set used for validation of joint detection 
of DR and AMD.  

Abbreviations: CF, Color fundus; DR, Diabetic retinopathy; AMD, Age-related macular degeneration. 

 
 
 
Figure S2. Receiver operating characteristic curves for individual detection of referable DR 
and AMD in the DR-AMD dataset. 

A.  DR vs. controls (527 images): AUC, 95.6% ; 95% 

B. AMD vs. controls (489 images):  AUC, 95.2%; 

CI, 91.8%-98.6% 

95% CI, 91.0%-98.1% 

Performance of RetCAD v.1.3.0 (blue curves; 95% CI within gray area) and human observers (colored circles) in 
the DR-AMD dataset for individual detection of referable DR (527 images with referable DR and control cases) 
(A) and individual detection of referable AMD (489 images with referable AMD and control cases) (B). The black 
circle indicates the SE and SP of RetCAD v.1.3.0 at its optimal operating point. In A, SE was 91.7% (95% CI, 
85.3%-98.0%)  and SP was 90.9% (95% CI, 86.7%-96.7%). In B, SE was 88.6% (95% CI, 83.8%-100.0%) and 
SP was 92.1% (95% CI, 84.3%-95.2%). 

Abbreviations: DR, Diabetic retinopathy; AMD, Age-related macular degeneration; AUC, Area under the receiver 
operating characteristic curve; SE, sensitivity; SP, specificity; CI, confidence interval. 

 
 
 
 
 
 
 
Table  S1.  Diagnostic  performance  for  individual  detection  of  referable  DR  and  AMD  of 
RetCAD v.1.3.0 and observers compared with reference standard in the DR-AMD dataset. 

RetCAD 

Obs. 1 

Obs. 2 

Obs. 3 

Obs. 4 

DR (527 images) 

RS 

R 
NR 
SE (%) 
(95% CI)  
SP (%) 
(95% CI) 

AMD (489 images) 

RS 

R 
NR 
SE (%) 
(95% CI)  
SP (%) 
(95% CI) 

R 

98 
38 

NR 

10 
381 

91.7 
(85.3-98.0) 
90.9 
(86.7-96.7) 

R 

61 
33 

NR 

9 
386 

88.6 
(83.8-100.0) 
92.1 
(84.3-95.2) 

R 

59 
6 

NR 

49 
413 

54.6 
(40.4-67.3) 
98.6 
(96.7-100.0) 

R 

57 
18 

NR 

13 
401 

81.4 
(66.7-94.3) 
95.7 
(92.9-98.1) 

R 

64 
10 

NR 

44 
409 

59.3 
(45.6-72.1) 
97.6 
(95.4-99.5) 

R 

39 
5 

NR 

31 
414 

55.7 
(39.0-72.4) 
98.8 
(97.1-100.0) 

R 

81 
13 

NR 

27 
406 

R 

65 
8 

NR 

43 
411 

75.0 
(63.2-86.4) 
96.9 
(94.4-99.0) 

60.0 
(47.1-72.6) 
98.1 
(96.1-99.5) 

R 

60 
15 

NR 

10 
404 

R 

58 
21 

NR 

12 
398 

85.7 
(73.3-96.8) 
96.4 
(93.7-98.6) 

82.9 
(70.0-93.9) 
95.0 
(91.9-97.6) 

For individual detection of referable DR: 527 images with referable DR and control cases. For individual detection 
of referable AMD: 489 images with referable AMD and control cases. 

Abbreviations:  DR,  Diabetic  retinopathy;  AMD,  Age-related  macular  degeneration;  RS,  Reference  standard; 
Obs., Observer; R, Referable; NR, Non-referable; C, Control; SE, Sensitivity; SP,  Specificity; CI, Confidence 
interval. 

 
 
 
 
 
 
Table S2. Diagnostic performance for individual detection of referable DR of RetCAD v.1.3.0 
and observers compared with reference standard in Messidor (1200 images). 

RS 

NR 

R 

DR stage 
0 
1 
2 
3 
SE (%) 
(95% CI)  
SP (%) 
(95% CI) 

RetCAD 
R 
NR 
543 
4 
51 
102 
207 
40 
252 
1 

92.0 
(89.1-95.9) 
92.1 
(88.7-95.2) 

Obs. 5 

Obs. 6 

NR 
542 
142 
92 
100 

R 
5 
11 
155 
243 

NR 
545 
149 
121 
34 

R 
2 
4 
126 
219 

79.6 
(74.8-84.8) 
97.7 
(96.0-99.2) 

69.0 
(62.9-74.7) 
99.1 
(97.9-100.0) 

Abbreviations:  RS,  Reference  Standard;  DR,  Diabetic  retinopathy;  R,  Referable;  NR,  Non-referable;  Obs., 
Observer; SE, Sensitivity; SP, Specificity; CI, Confidence interval. 

 
 
 
 
 
 
 
 
Figure S3. Interrater disagreement in DR grading stages among independent human observers 
and reference standard in Messidor (1200 images). 

Interrater heatmap with quadratic Cohen’s weighted kappa coefficients comparing disease staging for DR among 
the 2 independent human observers and the reference standard in Messidor (1200 images).  

Abbreviations:  DR,  Diabetic  retinopathy;  AMD,  Age-related  macular  degeneration;  Obs.,  Observer;  RS, 
Reference Standard. 

 
 
 
 
 
Table  S3.  Diagnostic  performance  for  individual  detection  of  referable  AMD  of  RetCAD 
v.1.3.0 compared with reference standard in the AREDS dataset (133821 images). 

RS 

NR 

R 

AMD stage 
1 
2 
3 
4 
SE (%) 
(95% CI)  
SP (%) 
(95% CI) 

RetCAD 

NR 
39954 
24018 
7263 
1146 

R 
1455 
8974 
34232 
16779 

85.8 
(84.6-86.2) 
86.0 
(85.7-87.4) 

Abbreviations:  RS,  Reference  standard;  AMD,  Age-related  macular  degeneration;  R,  Referable;  NR,  Non-
referable; SE, Sensitivity; SP, Specificity; CI, Confidence interval.  

 
 
 
 
 
 
 
 
Results Appendix. Validation of individual detection of referable DR in Messidor-2. 

As previous step to generate this dataset, diabetic patients from Brest University Hospital were recruited 
and  new  macula-centered  paired  color  fundus  (CF)  images  were  obtained  without  pharmacological 
dilation, using a Topcon TRC NW6 non-mydriatic fundus camera with a 45-degree field of view. This 
set of images is known as Messidor-Extension1 and contains 690 images from 345 examinations, which 
were combined with the images from the original Messidor set2 that came in pairs (one image per eye), 
that is, 1058 images from 529 examinations. The final set is known as Messidor-21 and includes in total 
1748 images from 874 examinations. 

We used as reference standard (RS) the gradings made publicly available by Abramoff et al.3,4 Three 
US-board retinal specialists graded each image assigning a level from the International Clinical Diabetic 
Retinopathy (ICDR) severity scale,5 obtaining then a consensus between the specialists. The gradings 
were  provided  at  participant  level  and  regarding  referability  or  non-referability,  i.e.,  a  person  was 
classified as a non-referable case if both eyes were classified as level 0 or 1, whereas a participant was 
considered to have referable DR if one or both eyes belonged to higher stages. In total, 684 subjects 
(78%) were graded as referable DR and 190 (22%) were graded as non-referable DR. 

For the validation of detection of referable DR in Messidor-2 using the mentioned RS, the participant-
based score provided by RetCAD v.1.3.0 was computed as the maximum score of both eyes’ image-
based scores. The DL system achieved an area under the receiver operating characteristic (ROC) curve 
of 98.0% (95% CI, 96.8%-99.0%), sensitivity was 92.6% (95% CI, 88.4%-97.4%) and specificity was 
93.4% (95% CI, 89.9%-97.2%). The corresponding ROC analysis and diagnostic performance of the 
DL  framework  can  be  found  in  FigureS3  and  FigureS4  (also  available  at www.aaojournal.org), 
respectively. 

References 

1.  Laboratoire  de  Traitement  de  l’Information  Médicale  (LaTIM  INSERM  U1101).  Messidor-2 
dataset  (Methods  to  evaluate  segmentation  and  indexing  techniques  in  the  field  of  retinal 
ophthalmology) (2011). http://latim.univ-brest.fr/indexfce0.html. Accessed  November 21, 2018. 
2.  ADCIS  (Image  Processing  Algorithm  Evaluation  and  data  Management).  Messidor  dataset. 
to  evaluate  segmentation  and  indexing  techniques  in  the  field  of  retinal 
Accessed 

http://www.adcis.net/en/DownloadThirdParty/Messidor.html. 

(Methods 
ophthalmology). 
November 21, 2018. 

3.  University of Iowa Health Care, Department of Ophthalmology and Visual Sciences, Michael D. 
Abramoff,  MD,  PhD.  Available  at:  https://medicine.uiowa.edu/eye/abramoff.  Accessed  
November 21, 2018. 

4.  Abràmoff, M. D., Folk, J. C., Han, D. P., et al. (2013). Automated analysis of retinal images for 

detection of referable diabetic retinopathy. JAMA ophthalmology, 131(3), 351-357. 

5.  Wilkinson, C. P., Ferris, F. L., Klein, R. E., et al. (2003). Proposed international clinical diabetic 
retinopathy and diabetic macular edema disease severity scales. Ophthalmology, 110(9), 1677-
1682. 

 
 
 
 
Figure S4. Receiver operating characteristic curve for individual detection of referable DR in 
Messidor-2 (874 subjects). 

Performance of RetCAD v.1.3.0 (blue curve; 95% CI within gray area) in Messidor-2 (874 subjects) for individual 
detection of referable DR. The black circle indicates the sensitivity (SE) and specificity (SP) of RetCAD v.1.3.0 
at its optimal operating point. SE was 92.6% (95% CI, 88.4%-97.4%) and SP was 93.4% (95% CI, 89.9%-97.2%). 
AUC was 98.0% % (95% CI, 96.8%-99.0%). 

Abbreviations:  DR,  Diabetic  retinopathy;  AUC,  Area  under  the  receiver  operating  characteristic  curve;  SE, 
Sensitivity; SP, Specificity; CI, Confidence interval. 

 
 
 
 
 
Table S4. Diagnostic performance for individual detection of referable DR of RetCAD v.1.3.0 
compared with reference standard in Messidor-2 (874 subjects). 

RS 

DR label 

NR 

R 

SE (%) 
(95% CI)  
SP (%) 
(95% CI) 

RetCAD 

NR 

639 

15 

R 

45 

175 

92.6 
(88.4-97.4) 
93.4 
(89.9-97.2) 

Abbreviations:  RS,  Reference  standard;  DR,  Diabetic  retinopathy;  R,  Referable;  NR,  Non-referable;  SE, 
Sensitivity; SP, Specificity; CI, Confidence interval. 

 
 
 
 
 
 
 
