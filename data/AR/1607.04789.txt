Sieving for closest lattice vectors
(with preprocessing)

Thijs Laarhoven

IBM Research
R¨uschlikon, Switzerland
mail@thijs.com

Abstract. Lattice-based cryptography has recently emerged as a prime candidate for eﬃ-
cient and secure post-quantum cryptography. The two main hard problems underlying its
security are the shortest vector problem (SVP) and the closest vector problem (CVP). Var-
ious algorithms have been studied for solving these problems, and for SVP, lattice sieving
currently dominates in terms of the asymptotic time complexity: one can heuristically solve
SVP in time 20.292d+o(d) in high dimensions d [Becker–Ducas–Gama–Laarhoven, SODA’16].
Although several SVP algorithms can also be used to solve CVP, it is not clear whether
this also holds for heuristic lattice sieving methods. The best time complexity for CVP is
currently 20.377d+o(d) [Becker–Gama–Joux, ANTS’14].

In this paper we revisit sieving algorithms for solving SVP, and study how these algo-
rithms can be modiﬁed to solve CVP and its variants as well. Our ﬁrst method is aimed at
solving one problem instance and minimizes the overall time complexity for a single CVP
instance with a time complexity of 20.292d+o(d). Our second method minimizes the amortized
time complexity for several instances on the same lattice, at the cost of a larger preprocessing
cost. Using nearest neighbor searching with a balanced space-time tradeoﬀ, with this method
we can solve the closest vector problem with preprocessing (CVPP) with 20.636d+o(d) space
and preprocessing, in 20.136d+o(d) time, while the query complexity can be further reduced to
20.059d+o(d) at the cost of 2d+o(d) space and preprocessing, or even to 2εd+o(d) for arbitrary
ε > 0, at the cost of preprocessing time and memory complexities of (1/ε)O(d).

For easier variants of CVP, such as approximate CVP and bounded distance decoding
(BDD), we further show how the preprocessing method achieves even better complexities.
For instance, we can solve approximate CVPP with large approximation factors κ with
polynomial-sized advice in polynomial time if κ = Ω((cid:112)d/ log d). This heuristically closes
the gap between the decision-CVPP result of [Aharonov–Regev, FOCS’04] (with equivalent
κ) and the search-CVPP result of [Dadush–Regev–Stephens-Davidowitz, CCC’14] (which
required larger κ).

Keywords: lattices, sieving algorithms, approximate nearest neighbors, shortest vector
problem (SVP), closest vector problem (CVP), bounded distance decoding (BDD)

1 Introduction

(B) =

b1, . . . , bd} ⊂
Z

{
i=1 λibi : λi ∈

Hard lattice problems. Lattices are discrete subgroups of Rd. More concretely, given
(B) generated by B is deﬁned as
a basis B =
d
, the Shortest Vector Problem
under the Euclidean norm, i.e., a non-
. Given a basis of a lattice

L
(SVP) asks to ﬁnd a shortest non-zero vector in
(cid:111)
zero lattice vector s of norm
and a target vector t
closest to t under the Euclidean distance, i.e. such that

∈L\{
Rd, the Closest Vector Problem (CVP) asks to ﬁnd a vector s

Rd, the lattice
. Given a basis of a lattice

) := minv

= λ1(

(cid:110)(cid:80)

∈ L

= minv

} (cid:107)

=

L

L

L

L

L

∈

v

v

s

s

(cid:107)

(cid:107)

(cid:107)

0

.

(cid:107)

−

t
(cid:107)

∈L (cid:107)

−

t
(cid:107)

These two hard problems are fundamental in the study of lattice-based cryptography,
as the security of these schemes is directly related to the hardness of SVP and CVP
in high dimensions. Various other hard lattice problems, such as Learning With Errors
(LWE) and the Shortest Integer Solution (SIS) problem are closely related to SVP and
CVP, and many reductions between these and other hard lattice problems are known; see

6
1
0
2

l
u
J

6
1

]

R
C
.
s
c
[

1
v
9
8
7
4
0
.
7
0
6
1
:
v
i
X
r
a

 
 
 
 
 
 
2

Thijs Laarhoven

e.g. [LvdPdW12, Figure 3.1] or [Ste16] for an overview. These reductions show that being
able to solve CVP eﬃciently implies that almost all other lattice problems can also be
solved eﬃciently in the same dimension, which makes the study of the hardness of CVP
even more important for choosing parameters in lattice-based cryptography.

Algorithms for SVP and CVP. Although SVP and CVP are both central in the
study of lattice-based cryptography, algorithms for SVP have received somewhat more
attention, including a benchmarking website to compare diﬀerent algorithms [SG15]. Var-
ious SVP methods have been studied which can solve CVP as well, such as enumeration
(see e.g. [Kan83, FP85, GNR10, MW15]), discrete Gaussian sampling [ADRS15, ADS15],
constructing the Voronoi cell of the lattice [AEVZ02,MV10a], and using a tower of sublat-
tices [BGJ14]. On the other hand, for the asymptotically fastest method in high dimensions
for SVP1, lattice sieving, it is not known how to solve CVP with similar costs as SVP.

After a series of theoretical works on constructing eﬃcient heuristic sieving algo-
rithms [NV08, MV10b, WLTB11, ZPH13, Laa15a, LdW15, BGJ15, BL16, BDGL16] as well
as practical papers studying how to speed up these algorithms even further [MS11, Sch11,
Sch13, BNvdP14, FBB+14, IKMT14, MTB14, MODB14, MLB15, MB16, MLB16], the best
time complexity for solving SVP currently stands at 20.292d+o(d) [BDGL16, MLB16]. Al-
though for various other methods the complexities for solving SVP and CVP are simi-
lar [GNR10,MV10a,ADS15], one can only guess whether the same holds for lattice sieving
methods. To date, the best heuristic time complexity for solving CVP in high dimensions
stands at 20.377d+o(d), due to Becker–Gama–Joux [BGJ14].

1.1 Contributions

In this paper we revisit heuristic lattice sieving algorithms, as well as the recent trend to
speed up these algorithms using nearest neighbor searching, and we investigate how these
algorithms can be modiﬁed to solve CVP and its generalizations. We present two diﬀerent
approaches for solving CVP with sieving, each of which we argue has its own merits.

Adaptive sieving. In adaptive sieving, we adapt the entire sieving algorithm to the prob-
lem instance, including the target vector. As the resulting algorithm is tailored speciﬁcally
to the given CVP instance, this leads to the best asymptotic complexity for solving a single
CVP instance out of our two proposed methods: 20.292d+o(d) time and space. This method
is very similar to solving SVP with lattice sieving, and leads to equivalent asymptotics
on the space and time complexities as for SVP. The corresponding space-time tradeoﬀ is
illustrated in Figure 1, and equals that of [BDGL16] for solving SVP.

Non-adaptive sieving. Our main contribution, non-adaptive sieving, takes a diﬀerent
approach, focusing on cases where several CVP instances are to be solved on the same
lattice. The goal here is to minimize the costs of computations depending on the target
vector, and spend more time on preprocessing the lattice, so that the amortized time
complexity per instance is smaller when solving many CVP instances on the same lattice.
This is very closely related to the Closest Vector Problem with Preprocessing (CVPP),
where the diﬀerence is that we allow for exponential-size preprocessed space. Using nearest

1 To obtain provable guarantees, sieving algorithms are commonly modiﬁed to facilitate a somewhat arti-
ﬁcial proof technique, which drastically increases the time complexity beyond e.g. the discrete Gaussian
sampler and the Voronoi cell algorithm [AKS01, NV08, PS09, MV10b]. On the other hand, if some nat-
ural heuristic assumptions are made to enable analyzing the algorithm’s behavior, then sieving clearly
outperforms these methods. We focus on heuristic sieving in this paper.

Sieving for closest lattice vectors (with preprocessing)

3

Fig. 1. Heuristic complexities for solving the Closest Vector Problem (CVP), the Closest Vector Problem
with Preprocessing (CVPP), Bounded Distance Decoding with Preprocessing (δ-BDDP), and the Approx-
imate Closest Vector Problem with Preprocessing (κ-CVPP). The red curve shows CVP complexities of
Becker–Gama–Joux [BGJ14]. The left blue curve denotes CVP complexities of adaptive sieving. The right
blue curve shows exact CVPP complexities using non-adaptive sieving. Purple curves denote relaxations
of CVPP corresponding to diﬀerent parameters δ (BDD radius) and κ (approximation factor). Note that
exact CVPP corresponds to δ-BDDP with δ = 1 and to κ-CVPP with κ = 1.

neighbor techniques with a balanced space-time tradeoﬀ, we show how to solve CVPP with
20.636d+o(d) space and preprocessing, in 20.136d+o(d) time. A continuous tradeoﬀ between
the two complexities can be obtained, where in the limit we can solve CVPP with (1/ε)O(d)
space and preprocessing, in 2εd+o(d) time. This tradeoﬀ is depicted in Figure 1.

A potential application of non-adaptive sieving is as a subroutine within enumeration
methods. As described in e.g. [GNR10], at any given level in the enumeration tree, one is
attempting to solve a CVP instance in a lower-dimensional sublattice of
, where the target
vector is determined by the path chosen from the root to the current node in the tree. That
means that if we can preprocess this sublattice such that the amortized time complexity
of solving CVPP is small, then this could speed up processing the bottom part of the
enumeration tree. This in turn might help speed up the lattice basis reduction algorithm
BKZ [Sch87, SE94, CN11], which commonly uses enumeration as its SVP subroutine, and
is key in assessing the security of lattice-based schemes. As the preprocessing needs to be
performed once, CVPP algorithms with impractically large preprocessing costs may not
be useful, but we show that with sieving the preprocessing costs can be quite small.

L

Outline. The remainder of the paper is organized as follows. In Section 2 we describe
some preliminaries, such as sieving algorithms and a useful result on nearest neighbor
searching. Section 3 describes adaptive sieving and its analysis for solving CVP with-
out preprocessing. Section 4 describes the preprocessing approach to solving CVP, with
complexity analyses for exact CVP and some of its relaxations.

20d20.1d20.2d20.3d20.4d20.5d20.6d20.7d20.8d20.9d21d20d20.1d20.2d20.3d20.4d20.5dCVPBGJ014CVPPδ-BDDPκ-CVPPtime=spaceu=√1−α−2u=1δ=12δ=0κ=2−→spacecomplexity−→(query)timecomplexity4

Thijs Laarhoven

2 Preliminaries

2.1 Lattice sieving for solving SVP

w

, then their sum/diﬀerence v

Heuristic lattice sieving algorithms for solving the shortest vector problem all use the
following basic property of lattices: if v, w
is a
∈ L
lattice vector as well. Therefore, if we have a long list L of lattice vectors stored in memory,
we can consider combinations of these vectors to obtain new, shorter lattice vectors. To
make sure the algorithm makes progress in ﬁnding shorter lattice vectors, L needs to
contain a lot of lattice vectors; for vectors v, w
w is
shorter than v, w iﬀ the angle between v, w is smaller than π/3, which for random vectors
v, w occurs with probability (3/4)d/2+o(d). The expected space complexity of heuristic
sieving algorithms follows directly from this observation: if we draw (4/3)d/2+o(d) random
vectors from the unit sphere, we expect a large number of pairs of vectors to have angle less
than π/3, leading to many short diﬀerence vectors. This is exactly the heuristic assumption
used in analyzing these sieving algorithms: when normalized, vectors in L follow the same
distribution as vectors sampled uniformly at random from the unit sphere.

of similar norm, the vector v

∈ L

∈ L

−

±

Heuristic 1 When normalized, the list vectors w
tributed random vectors from the unit sphere

d

∈
1 :=
{

−

S

L behave as i.i.d. uniformly dis-
.
x

Rd :

x
(cid:107)

(cid:107)

= 1
}

∈

Therefore, if we start by sampling a list L of (4/3)d/2+o(d) long lattice vectors, and it-
eratively consider combinations of vectors in L to ﬁnd shorter vectors, we expect to
keep making progress. Note that naively, combining pairs of vectors in a list of size
(4/3)d/2+o(d)

20.208d+o(d) takes time (4/3)d+o(d)

20.415d+o(d).

≈

≈

The Nguyen-Vidick sieve. The heuristic sieve algorithm of Nguyen and Vidick [NV08]
starts by sampling a list L of (4/3)d/2+o(d) long lattice vectors, and uses a sieve to map L,
with maximum norm R := maxv
, to a new list L(cid:48), with maximum norm at most γR
L (cid:107)
for γ < 1 close to 1. By repeatedly applying this sieve, after poly(d) iterations we expect
to ﬁnd a long list of lattice vectors of norm at most γpoly(d)R = O(λ1(
)). The ﬁnal list
is then expected to contain a shortest vector of the lattice. Algorithm 3 in Appendix A
2 time.
L
describes a sieve equivalent to Nguyen-Vidick’s original sieve, to map L to L(cid:48) in
|

L

v

(cid:107)

∈

|

,

(cid:107)

w2(cid:107)

, and each time a new vector v

Micciancio and Voulgaris’ GaussSieve. Micciancio and Voulgaris used a slightly
diﬀerent approach in the GaussSieve [MV10b]. This algorithm reduces the memory usage
by immediately reducing all pairs of lattice vectors that are sampled. The algorithm uses
w2(cid:107) ≥
a single list L, which is always kept in a state where for all w1, w2 ∈
is sampled, its norm is reduced with
w1(cid:107)
(cid:107)
vectors in L. After the norm can no longer be reduced, the vectors in L are reduced with
v. Modiﬁed list vectors are added to a stack to be processed later (to maintain the pairwise
reduction-property of L), and new vectors which are pairwise reduced with L are added to
L. Immediately reducing all pairs of vectors means that the algorithm uses less time and
memory in practice, but at the same time Nguyen and Vidick’s heuristic proof technique
does not apply here. However, it is commonly believed that the same bounds (4/3)d/2+o(d)
and (4/3)d+o(d) on the space and time complexities hold for the GaussSieve. Pseudocode
of the GaussSieve is given in Algorithm 4 in Appendix A.

w1 ±
(cid:107)

∈ L

L,

2.2 Nearest neighbor searching

Given a data set L
⊂
when given a query t
∈

Rd, the nearest neighbor problem asks to preprocess L such that,
Rd, one can quickly return a nearest neighbor s
L with distance

∈

Sieving for closest lattice vectors (with preprocessing)

5

∈

s

−

−

w

t
(cid:107)

t
(cid:107)

L (cid:107)

= minw

(cid:107)
a ﬁnite set of unstructured points, rather than the inﬁnite set of all points in a lattice

. This problem is essentially identical to CVP, except that L is
.

L
Locality-Sensitive Hashing/Filtering (LSH/LSF). A celebrated technique for ﬁnd-
ing nearest neighbors in high dimensions is Locality-Sensitive Hashing (LSH) [IM98,
WSSJ14], where the idea is to construct many random partitions of the space, and store
the list L in hash tables with buckets corresponding to regions. Preprocessing then consists
of constructing these hash tables, while a query t is answered by doing a lookup in each
of the hash tables, and searching for a nearest neighbor in these buckets. More details on
LSH in combination with sieving can be found in e.g. [Laa15a, LdW15, BGJ15, BL16].

Similar to LSH, Locality-Sensitive Filtering (LSF) [BDGL16,Laa15b] divides the space
into regions, with the added relaxation that these regions do not have to form a partition;
regions may overlap, and part of the space may not be covered by any region. This leads to
improved results compared to LSH when L has size exponential in d [BDGL16, Laa15b].
Below we restate one of the main results of [Laa15b] for our applications. The speciﬁc
1 sampled uniformly at random, and
problem considered here is: given a data set L
⊂ S
d
a random query t
L such that the angle between w and t is
at most θ. The following result further assumes that the list L contains n = (1/ sin θ)d+o(d)
vectors.

1, return a vector w

∈ S

∈

−

−

d

Lemma 1.

[Laa15b, Corollary 1] Let θ

d

−

1 be a list of n = (1/ sin θ)d+o(d) vectors sampled uniformly at random from

⊂
1. Then,
S
using spherical LSF with parameters αq = u cos θ and αu = cos θ, one can preprocess L in
time n1+ρu+o(1), using n1+ρu+o(1) space, and with high probability answer a random query
t

1 correctly in time nρq+o(1), where:

∈

∈

S

−

d

d

(0, 1

2 π), and let u

[cos θ, 1/ cos θ]. Let L

−

∈ S

nρq =

sin2 θ (u cos θ + 1)

d/2

u cos θ

cos 2θ

−

(cid:19)

(cid:18)

, nρu =

sin2 θ

d/2

1

(cid:18)

−

cot2 θ (u2

−

2u cos θ + 1)

(cid:19)

.

(1)

Applying this result to sieving for solving SVP, where n = sin( π

d+o(d) and we are
looking for pairs of vectors at angle at most π
3 to perform reductions, this leads to a space
and preprocessing complexity of n0.292d+o(d), and a query complexity of 20.084d+o(d). As the
20.208d+o(d)
preprocessing in sieving is only performed once, and queries are performed n
times, this leads to a reduction of the complexities of sieving (for SVP) from 20.208d+o(d)
space and 20.415d+o(d) time, to 20.292d+o(d) space and time [BDGL16].

3 )−

≈

3 Adaptive sieving for CVP

We present two methods for solving CVP using sieving, the ﬁrst of which we call adaptive
sieving – we adapt the entire sieving algorithm to the particular CVP instance, to obtain
the best overall time complexity for solving one instance. When solving several CVP
instances, the costs roughly scale linearly with the number of instances.

Using one list. The main idea behind this method is to translate the SVP algorithm by
the target vector t; instead of generating a long list of lattice vectors reasonably close to
0, we generate a list of lattice vectors close to t, and combine lattice vectors to ﬁnd lattice
vectors even closer vectors to t. The ﬁnal list then hopefully contains a closest vector to t.
One quickly sees that this does not work, as the fundamental property of lattices does
not hold for the lattice coset t +
. In other
words, two lattice vectors close to t can only be combined to form lattice vectors close to

: if w1, w2 ∈
L

, then w1 ±

w2 /
∈

t +

t +

L

L

6

Thijs Laarhoven

0, L(cid:48)

0, L(cid:48)
t

t ⊂ L contain (4/3)d/2+o(d) vectors at distance ≤ γR from 0, t

Algorithm 1 The adaptive Nguyen-Vidick sieve for ﬁnding closest vectors
Require: Lists L0, Lt ⊂ L containing (4/3)d/2+o(d) vectors at distance ≤ R from 0, t
Ensure: Lists L(cid:48)
1: Initialize empty lists L(cid:48)
2: for each (w1, w2) ∈ L0 × L0 do
3:
if (cid:107)w1 − w2(cid:107) ≤ γR then
Add w1 − w2 to the list L(cid:48)
4:
0
5: for each (w1, w2) ∈ Lt × L0 do
6:
7:
8: return (L(cid:48)

if (cid:107)(w1 − w2) − t(cid:107) ≤ γR then
Add w1 − w2 to the list L(cid:48)
t

0, L(cid:48)
t)

0 or 2t. So if we start with a list of vectors close to t, and combine vectors in this list as
in the Nguyen-Vidick sieve, then after one iteration we will end up with a list L(cid:48) of lattice
vectors close to 0.

Using two lists. To make the idea of translating the whole problem by t work for the
Nguyen-Vidick sieve, we make the following modiﬁcation: we keep track of two lists L = L0
and Lt of lattice vectors close to 0 and t, and construct a sieve which maps two input
lists L0, Lt to two output lists L(cid:48)0, L(cid:48)t of lattice vectors slightly closer to 0 and t. Similar
to the original Nguyen-Vidick sieve, we then apply this sieve several times to two initial
lists (L0, Lt) with a large radius R, to end up with two lists L0 and Lt of lattice vectors
) from 0 and t2. The argumentation that
at distance at most approximately
λ1(
this algorithm works is almost identical to that for solving SVP, where we now make the
following slightly diﬀerent heuristic assumption.

4/3

(cid:112)

L

·

Heuristic 2 When normalized, the list vectors L0 and Lt in the modiﬁed Nguyen-Vidick
sieve both behave as i.i.d. uniformly distributed random vectors from the unit sphere.

The resulting algorithm, based on the Nguyen-Vidick sieve, is presented in Algorithm 1.

Main result. As the (heuristic) correctness of this algorithm follows directly from the
correctness of the original NV-sieve, and nearest neighbor techniques can be applied to this
algorithm in similar fashion as well, we immediately obtain the following result. Note that
space-time tradeoﬀs for SVP, such as the one illustrated in [BDGL16, Figure 1], similarly
carry over to solving CVP, and the best tradeoﬀ for SVP (and therefore CVP) is depicted
in Figure 1.

Theorem 1. Assuming Heuristic 2 holds, the adaptive Nguyen-Vidick sieve with spherical
LSF solves CVP in time T and space S, with

S = (4/3)d/2+o(d)

≈

20.208d+o(d), T = (3/2)d/2+o(d)

20.292d+o(d).

≈

(2)

An important open question is whether these techniques can also be applied to the
faster GaussSieve algorithm to solve CVP. The GaussSieve seems to make even more use
of the property that the sum/diﬀerence of two lattice vectors is also in the lattice, and
cannot as easily be mimicked for the coset t+
operations in the GaussSieve in
. Solving
CVP with the GaussSieve with similar complexities is left as an open problem.

L

L

2 Observe that by the Gaussian heuristic, there are (4/3)d/2+o(d) vectors in L within any ball of radius
(cid:112)4/3 · λ1(L). So the list size of the NV-sieve will surely decrease below (4/3)d/2 when R < (cid:112)4/3 · λ1(L).

Sieving for closest lattice vectors (with preprocessing)

7

Algorithm 2 Non-adaptive sieving (Phase 2) for ﬁnding closest vectors
Require: A list L ⊂ L of αd/2+o(d) vectors of norm at most α · λ1(L), and t ∈ Rd
Ensure: The output vector s is the closest lattice vector to t (w.h.p.)
1: Initialize t(cid:48) ← t
2: for each w ∈ L do
3:
4:
5: return s = t − t(cid:48)

Replace t(cid:48) ← t(cid:48) − w and restart the for-loop

if (cid:107)t(cid:48) − w(cid:107) ≤ (cid:107)t(cid:48)(cid:107) then

4 Non-adaptive sieving for CVPP

Our second method for ﬁnding closest vectors with heuristic lattice sieving follows a slightly
diﬀerent approach. Instead of focusing only on the total time complexity for one problem
instance, we split the algorithm into two phases:

– Phase 1: Preprocess the lattice
L
– Phase 2: Process the query t and output a closest lattice vector s

, without knowledge of the target t;

to t.

∈ L

Intuitively it may be more important to keep the costs of Phase 2 small, as the pre-
processed data can potentially be reused later for other instances on the same lattice.
This approach is essentially equivalent to the Closest Vector Problem with Preprocessing
such that when given a target vector t later, one can quickly return
(CVPP): preprocess
a closest vector s
to t. For CVPP however the preprocessed space is usually restricted
to be of polynomial size, and the time used for preprocessing the lattice is often not taken
into account. Here we will keep track of the preprocessing costs as well, and we do not
restrict the output from the preprocessing phase to be of size poly(d).

L
∈ L

Algorithm description. To minimize the costs of answering a query, and to do the pre-
processing independently of the target vector, we ﬁrst run a standard SVP sieve, resulting
in a large list L of almost all short lattice vectors. Then, after we are given the target
can no
vector t, we use L to reduce the target. Finally, once the resulting vector t(cid:48) ∈
longer be reduced with our list, we hope that this reduced vector t(cid:48) is the shortest vector
, so that 0 is the closest lattice vector to t(cid:48) and s = t
in the coset t +
t(cid:48) is the closest
L
lattice vector to t.

t +

−

L

The ﬁrst phase of this algorithm consists in running a sieve and storing the resulting
list in memory (potentially in a nearest neighbor data structure for faster lookups). For
this phase either the Nguyen-Vidick sieve or the GaussSieve can be used. The second phase
is the same for either method, and is described in Algorithm 2 for the general case of an
input list essentially consisting of the αd+o(d) shortest vectors in the lattice. Note that a
standard SVP sieve would produce a list of size (4/3)d/2+o(d) corresponding to α =
4/3.

(cid:112)

List size. We ﬁrst study how large L must be to guarantee that the algorithm succeeds.
4/3 immediately in Algorithm 2. To see why
One might wonder why we do not ﬁx α =
which is no longer
t +
this choice of α does not suﬃce, suppose we have a vector t(cid:48) ∈
), similar to
reducible with L. This implies that t(cid:48) has norm approximately
what happens in the GaussSieve. Now, unfortunately the fact that t(cid:48) cannot be reduced
with L anymore, does not imply that the closest lattice point to t(cid:48) is 0. In fact, it is more
likely that there exists an s
) which is
closer to t(cid:48), but which is not used for reductions.

of norm slightly more than

L
4/3

λ1(

λ1(

4/3

t +

(cid:112)

(cid:112)

L

L

L

∈

·

·

By the Gaussian heuristic, we expect the distance from t and t(cid:48) to the lattice to be
). So to guarantee that 0 is the closest lattice vector to the reduced vector t(cid:48), we need

(cid:112)

λ1(

L

8

Thijs Laarhoven

t(cid:48) to have norm at most λ1(
). To analyze and prove correctness of this algorithm, we will
therefore prove that, under the assumption that the input is a list of the αd+o(d) shortest
lattice vectors of norm at most α
) for a particular choice of α, w.h.p. the algorithm
reduces t to a vector t(cid:48) ∈

To study how to set α, we start with the following elementary lemma regarding the

of norm at most λ1(

λ1(

t +

L

L

L

L

).

·

probability of reduction between two uniformly random vectors with given norms.

Lemma 2. Let v, w > 0 and let v = v

ev and w = w

ew. Then:

·

·

P

ev,ew

d−1

∼S

v
(cid:107)

−

w

2 <
(cid:107)

v

2
(cid:107)

(cid:107)

1

−

∼

2

w
2v

d/2+o(d)

.

(3)

Proof. Expanding
2 <
v
(cid:107)

v
(cid:107)

w

−

(cid:107)

(cid:107)

(cid:16)
2 = v2 + w2
w
v
(cid:107)
(cid:107)
2 equals w
ev, ew(cid:105)
2v <

−

(cid:104)

(cid:17)
2vw

(cid:104)
ev, ew(cid:105)
(cid:104)

(cid:105)

(cid:0)
(cid:1)
and

(cid:107)
. The result follows from [BDGL16, Lemma 2.1].

−

v

2 = v2, the condition
(cid:107)

Under Heuristic 1, we then obtain a relation between the choice of α for the input list

and the expected norm of the reduced vector t(cid:48) as follows.

Lemma 3. Let L
d
and let v
∈
exists a w
∈

d

α

1 be a list of αd+o(d) uniformly random vectors of norm α > 1,
⊂
1 be sampled uniformly at random. Then, for high dimensions d, there
β
−
· S
L such that

if and only if

· S

w

<

−

v
(cid:107)

−

(cid:107)

v
(cid:107)

(cid:107)

α4

−

4β2α2 + 4β2 < 0.

(4)

Proof. By Lemma 2 we can reduce v with w
−
α2
4β2 ]d/2+o(d). Since we have n = αd+o(d) such vectors w, the probability that none of them
can reduce v is (1
p,
(cid:29)
we obtain the given equation (4), where α4

1/p and 1
4β2α2 + 4β2 < 0 implies n

1/p. Expanding n
1/p.

L with probability similar to p = [1

p)n, which is o(1) if n

o(1) if n

(cid:28)

−

−

∈

·

−

(cid:29)

·

L

0
). To obtain a reduced vector t(cid:48) of norm β

Note that in our applications, we do not just have a list of αd+o(d) lattice vectors of
[1, α] we expect L to contain αd+o(d)
lattice vectors of norm
); for any α0 ∈
norm α
λ1(
L
), we therefore obtain
λ1(
λ1(
at most α0 ·
L
0 + 4β2
[1, α], it must hold that α4
the condition that for some value α0 ∈
0 −
4β2α2 + 4β2 has two roots r1 < 2 < r2 for
From (4) it follows that p(α2) = α4
α2, which lie close to 2 for β
α is
equivalent to α > r1, which for β = 1 + o(1) implies that α2
2 + o(1). This means that
asymptotically we must set α = √2, and use n = 2d/2+o(d) input vectors, to guarantee
that w.h.p. the algorithm succeeds. A sketch of the situation is also given in Figure 2a.

0) < 0 for some α0 ≤
≥

1. The condition that p(α2

0 < 0.

4β2α2

≈

−

·

Modifying the ﬁrst phase. As we will need a larger list of size 2d/2+o(d) to make sure
we can solve CVP exactly, we need to adjust Phase 1 of the algorithm as well. Recall
that with standard sieving, we reduce vectors iﬀ their angle is at most θ = π
3 , resulting
d+o(d). As we now need the output list of the ﬁrst phase to consist
in a list of size (sin θ)−
d+o(d) vectors for θ(cid:48) = π
of 2d/2+o(d) = (sin θ(cid:48))−
4 , we make the following adjustment: only
reduce v and w if their common angle is less than π
4 . For unit length vectors, this condition
2. This further accelerates
2
(2
w
is equivalent to reducing v with w iﬀ
(cid:107)
nearest neighbor techniques due to the smaller angle θ. Pseudocode for the modiﬁed ﬁrst
phase is given in Appendix B

√2)

· (cid:107)

−

≤

−

v

v

(cid:107)

(cid:107)

Sieving for closest lattice vectors (with preprocessing)

9

(a) For solving exact CVP, we must reduce
the vector t to a vector t(cid:48) ∈ t + L of norm at
most λ1(L). The nearest lattice point to t(cid:48) lies
in a ball of radius approximately λ1(L) around
t(cid:48) (blue), and almost all the mass of this ball is
contained in the (black) ball around 0 of radius
√
2 · λ1(L). So if s ∈ L \ {0} had lain closer to t(cid:48)
than 0, we would have reduced t(cid:48) with s, since
s ∈ L.

(b) For variants of CVP, a choice α for the list
size implies a norm β · λ1(L) of t(cid:48). The nearest
lattice vector s to t(cid:48) lies within δ·λ1(L) of t(cid:48) (δ = 1
for approx-CVP), so with high probability s has
norm approximately ((cid:112)β2 + δ2) · λ1(L). For δ-
BDD, if (cid:112)β2 + δ2 ≤ α then we expect the nearest
point s to be in the list L. For κ-CVP, if β ≤ κ,
then the lattice vector t − t(cid:48) has norm at most
κ · λ1(L).

Fig. 2. Comparison of the list size complexity analysis for CVP (left) and BDD/approximate CVP (right).
The point t represents the target vector, and after a series of reductions with Algorithm 2, we obtain
t(cid:48) ∈ t + L. Blue balls around t(cid:48) depict regions in which we expect the closest lattice point to t(cid:48) to lie, where
the blue shaded area indicates a negligible fraction of this ball [BDGL16, Lemma 2].

Main result. With the algorithm in place, let us now analyze its complexity for solving
CVP. The ﬁrst phase of the algorithm generates a list of size 2d/2+o(d) by combining pairs
of vectors, and naively this can be done in time T1 = 2d+o(d) and space S = 2d/2+o(d), with
query complexity T2 = 2d/2+o(d). Using nearest neighbor searching (Lemma 1), the query
and preprocessing complexities can be further reduced, leading to the following result.

Theorem 2. Let u
preprocessing time T1, space complexity S, and query time complexity T2 as follows:

√2, √2). Using non-adaptive sieving, we can solve CVP with

∈

( 1
2

S = T1 =

1
u(√2

(cid:18)

d/2+o(d)

u)

(cid:19)

−

,

T2 =

√2 + u

d/2+o(d)

(cid:32)

2u (cid:33)

.

(5)

Proof. These complexities follow from Lemma 1 with θ = π
4 , noting that the ﬁrst phase can
be performed in time and space T1 = S = n1+ρu, and the second phase in time T2 = nρq.

To illustrate the time and space complexities of Theorem 2, we highlight three special

cases u as follows. The full tradeoﬀ curve for u

( 1
2

∈

√2, √2) is depicted in Figure 1.

– Setting u = 1
2

√2, we obtain S = T1 = 2d/2+o(d) and T2 ≈

20.2925d+o(d).

√2·λ1(L)λ1(L)0tt0λ1(L)s√2·λ1(L)α·λ1(L)β·λ1(L)0tt0δ·λ1(L)spβ2+δ2·λ1(L)10

Thijs Laarhoven

– Setting u = 1, we obtain S = T1 ≈
– Setting u = 1

20.6358d+o(d) and T2 ≈
2 (√2 + 1), we get S = T1 = 2d+o(d) and T2 ≈

20.1358d+o(d).
20.0594d+o(d).

The ﬁrst result shows that the query complexity of non-adaptive sieving is never worse
than for adaptive sieving; only the space and preprocessing complexities are worse. The
second and third results show that CVP can be solved in signiﬁcantly less time, even with
preprocessing and space complexities bounded by 2d+o(d).

√2, the query complexity keeps decreasing
Minimizing the query complexity. As u
→
while the memory and preprocessing costs increase. For arbitrary ε > 0, we can set u =
√2 as a function of ε, resulting in asymptotic complexities S = T1 = (1/ε)O(d) and
uε ≈
T2 = 2εd+o(d). This shows that it is possible to obtain a slightly subexponential query
complexity, at the cost of superexponential space, by taking ε = o(1) as a function of d.

Corollary 1. For arbitrary ε > 0, using non-adaptive sieving we can solve CVPP with
preprocessing time and space complexities (1/ε)O(d), in time 2εd+o(d). In particular, we can
solve CVPP in 2o(d) time, using 2ω(d) space and preprocessing.

Being able to solve CVPP in subexponential time with superexponential preprocessing
and memory is neither trivial nor quite surprising. A naive approach to the problem, with
this much memory, could for instance be to index the entire fundamental domain of
in a
hash table. One could partition this domain into small regions, solve CVP for the centers
of each of these regions, and store all the solutions in memory. Then, given a query, one
looks up which region t is in, and returns the answer corresponding to that vector. With
a suﬃciently ﬁne-grained partitioning of the fundamental domain, the answers given by
the look-ups are accurate, and this algorithm probably also runs in subexponential time.
Although it may not be surprising that it is possible to solve CVPP in subexponen-
tial time with (super)exponential space, it is not clear what the complexities of other
methods would be. Our method presents a clear tradeoﬀ between the complexities, where
the constants in the preprocessing exponent are quite small; for instance, we can solve
CVPP in time 20.06d+o(d) with less than 2d+o(d) memory, which is the same amount of
memory/preprocessing of the best provable SVP and CVP algorithms [ADRS15, ADS15].
Indexing the fundamental domain may well require much more memory than this.

L

4.1 Bounded Distance Decoding with Preprocessing

We ﬁnally take a look at speciﬁc instances of CVP which are easier than the general prob-
lem, such as when the target t lies unusually close to the lattice. This problem naturally
appears in practice, when a private key consists of a good basis of a lattice with short basis
vectors, and the public key is a bad basis of the same lattice. An encryption of a message
could then consist of the message being mapped to a lattice point v
, and a small error
vector e being added to v (t = v + e) to hide v. If the noise e is small enough, then with
a good basis one can decode t to the closest lattice vector v, while someone with the bad
basis cannot decode correctly. As decoding for arbitrary t (solving CVP) is known to be
hard even with knowledge of a good basis [Mic01, FM02, Reg04, AKKV05], e needs to be
very short, and t must lie unusually close to the lattice.

∈ L

So instead of assuming target vectors t

Rd are sampled at random, suppose that t
∈
lies at distance at most δ
(0, 1). For adaptive sieving, recall that
, for δ
the list size (4/3)d/2+o(d) is the minimum initial list size one can hope to use to obtain a

) from

λ1(

L

L

∈

·

Sieving for closest lattice vectors (with preprocessing)

11

list of short lattice vectors; with fewer vectors, one would not be able to solve SVP.3 For
non-adaptive sieving however, it may be possible to reduce the list size below 2d/2+o(d).

λ1(

List size. Let us again assume that the preprocessed list L contains almost all αd+o(d)
lattice vectors of norm at most α
)
L
of the reduced vector t(cid:48), as described in Lemma 3. The nearest lattice vector s
to t(cid:48)
t(cid:48) is approximately orthogonal to t(cid:48); see
lies within radius δ
Figure 2b, where the shaded area is asymptotically negligible. Therefore w.h.p. s has norm
α, then we expect the nearest vector to
at most (
be contained in L, so that ultimately 0 is nearest to t(cid:48). Substituting α4
4β2α2 + 4β2 = 0
(cid:112)
and β2 + δ2

). The choice of α implies a maximum norm βα·
L
∈ L

α2, and solving for α, this leads to the following condition on α.

) of t(cid:48), and w.h.p. s

α + δ2)
β2

α + δ2
β2

). Now if

λ1(

λ1(

λ1(

(cid:112)

−

−

≤

L

L

·

·

·

≤

α2

≥

2

3 (1 + δ2) + 2

3

(1 + δ2)2

3δ2 .

−

(6)

(cid:112)

√2 as expected,
Taking δ = 1, corresponding to exact CVP, leads to the condition α
≥
while in the limiting case of δ
4/3. This matches
experimental observations using the GaussSieve, where after ﬁnding the shortest vector,
newly sampled vectors often cause collisions (i.e. being reduced to the 0-vector). In other
words, Algorithm 2 often reduces target vectors t which essentially lie on the lattice
0) to the 0-vector when the list has size (4/3)d/2+o(d). This explains why collisions
(δ
in the GaussSieve are common when the list size grows to size (4/3)d/2+o(d).

0 we obtain the condition α

→

→

(cid:112)

≥

Main result. To solve BDD with a target t at distance δ
) from the lattice, we
need the preprocessing to produce a list of almost all αd+o(d) vectors of norm at most
), with α satisfying (6). Similar to the analysis for CVP, we can produce such a
α
list by only doing reductions between two vectors if their angle is less than θ, where now
θ = arcsin(1/α). Combining this with Lemma 2, we obtain the following result.

λ1(

λ1(

L

L

·

·

Theorem 3. Let α satisfy (6) and let u
−
we can heuristically solve BDD for targets t at distance δ
preprocessing time T1, space complexity S, and query time complexity T2 as follows:

1 ). Using non-adaptive sieving,
) from the lattice, with

λ1(

(cid:113)

(cid:113)

α2

L

∈

(

·

α2
1
α2 ,
−

α2

S =

1

(cid:32)

−

(α2

−

1)(u2

1

−

2u
α

√α2

−

1 + 1) (cid:33)

d/2+o(d)

,

T1 = max

S, (3/2)d/2+o(d)

,

T2 =

(cid:110)

(cid:111)

2α

(cid:32)

α + u√α2
1
α3 + α2u√α2

−

−

(7)

.

(8)

d/2+o(d)

1 (cid:33)

−

Proof. These complexities directly follow from applying Lemma 1 with θ = arcsin(1/α),
and again observing that Phase 1 can be performed in time T1 = n1+ρu and space S =
n1+ρu, while Phase 2 takes time T2 = nρq. Note that we cannot combine vectors whose
angles are larger than π
3 in Phase 1, which leads to a lower bound on the preprocessing
time complexity T1 based on the costs of solving SVP.

Theorem 3 is a generalization of Theorem 2, as the latter can be derived from the former
by substituting δ = 1 above. To illustrate the results, Figure 1 considers two special cases:

3 The recent paper [BLS16] discusses how to use less memory in sieving, by using triple- or tuple-wise
reductions, instead of the standard pairwise reductions. These techniques may also be applied to adaptive
sieving to solve CVP with less memory, at the cost of an increase in the time complexity.

12

Thijs Laarhoven

≈

α2

(cid:112)

– For δ = 1

2 , we ﬁnd α

1.1976, leading to S

minimizing the space complexity.

20.2602d+o(d) and T2 = 20.1908d+o(d) when

≈

– For δ

0, we have α

4/3
S = (4/3)d/2+o(d), with query complexity T2 = 20.1610d+o(d).

→

→

≈

1.1547. The minimum space complexity is therefore

In the limit of u
→
(cid:113)
and a subexponential query time T2 →

α2

−

1 we need superexponential space/preprocessing S, T1 →

2ω(d)

2o(d) for all δ > 0.

4.2 Approximate Closest Vector Problem with Preprocessing

and a target vector t

Given a lattice
κ asks to ﬁnd a vector s
distance from t to
a lattice vector counts as a solution iﬀ it lies at distance at most κ

Rd, approximate CVP with approximation factor
is at most a factor κ larger than the real
s
(cid:107)
. For random instances t, by the Gaussian heuristic this means that

∈
such that

) from t.

∈ L

−

L

L

(cid:107)

t

λ1(

L

·

λ1(
λ1(

List size. Instead of reducing t to a vector t(cid:48) of norm at most λ1(
) as is needed for
solving exact CVP, we now want to make sure that the reduced vector t(cid:48) has norm at most
). If this is the case, then the vector t
κ
t(cid:48) is a lattice vector lying at distance at most
·
L
), which w.h.p. qualiﬁes as a solution. This means that instead of substituting β = 1
κ
·
L
in Lemma 3, we now substitute β = κ. This leads to the condition that α4
0+4β2 < 0
α. By a similar analysis α2 must therefore be larger than the smallest root
for some α0 ≤
1) of this quadratic polynomial in α2. This immediately leads to the
r1 = 2κ(κ
−
following condition on α:

4κ2α2

√κ2

0−

−

−

L

α2

≥

2κ

κ

−

κ2

−

1

.

(9)

(cid:16)
A sanity check shows that κ = 1, corresponding to exact CVP, indeed results in α
while in the limit of κ
most κ
λ1(
approximation factors, a preprocessed list of size (1 + ε)d+o(d) suﬃces.

√2,
1 suﬃces to obtain a vector t(cid:48) of norm at
). In other words, to solve approximate CVP with very large (constant)

a value α

→ ∞

(cid:112)

≥

≈

L

(cid:17)

·

Main result. Similar to the analysis of CVPP, we now take θ = arcsin(1/α) as the angle
with which to reduce vectors in Phase 1, so that the output of Phase 1 is a list of almost
all αd+o(d) shortest lattice vectors of norm at most α
). Using a smaller angle θ for
reductions again means that nearest neighbor searching can speed up the reductions in
both Phase 1 and Phase 2 even further. The exact complexities follow from Lemma 1.

λ1(

L

·

Theorem 4. Using non-adaptive sieving with spherical LSF, we can heuristically solve
κ-CVP with similar complexities as in Theorem 3, where now α must satisfy (9).

Note that only the dependence of α on κ is diﬀerent, compared to the dependence of α
on δ for bounded distance decoding. The complexities for κ-CVP arguably decrease faster
than for δ-BDD: for instance, for κ
1.0882 we obtain the same complexities as for BDD
with δ = 1
1.1547 leads to the same complexities as for BDD with
2 , while κ =
δ

≈
0. Two further examples are illustrated in Figure 1:

4/3

≈

→
– For κ = 2, we have α

(cid:112)

– For κ

≈

1.1976, which for u

0.5503 leads to S = T1 = 20.2602d+o(d) and
≈
T2 = 20.1908d+o(d), and for u = 1 leads to S = T1 = 20.3573d+o(d) and T2 = 20.0971d+o(d).
1, i.e. the required preprocessed list size approaches 2o(d) as
κ grows. For suﬃciently large κ, we can solve κ-CVP with a preprocessed list of size
2εd+o(d) in at most 2εd+o(d) time. The preprocessing time is given by 20.2925d+o(d).

, we have α

→ ∞

→

Sieving for closest lattice vectors (with preprocessing)

13

The latter result shows that for any superconstant approximation factor κ = ω(1),
we can solve the corresponding approximate closest vector problem with preprocessing in
subexponential time, with an exponential preprocessing time complexity 20.292d+o(d) for
solving SVP and generating a list of short lattice vectors, and a subexponential space
complexity required for Phase 2. In other words, even without superexponential prepro-
cessing/memory we can solve CVPP with large approximation factors in subexponential
time.

To compare this result with previous work, note that the lower bound on α from (9)
tends to 1+1/(8κ2)+O(κ−
4) as κ grows. The query space and time complexities are further
both proportional to αΘ(d). To obtain a polynomial query complexity and polynomial
storage after the preprocessing phase, we can solve for κ, leading to the following result.

Corollary 2. With non-adaptive sieving we can heuristically solve approximate CVPP
with approximation factor κ in polynomial time with polynomial-sized advice iﬀ κ =
Ω(

d/ log d).

(cid:112)

Proof. The query time and space complexities are given by αΘ(d), where α = 1 + Θ(1/κ2).
To obtain polynomial complexities in d, we must have αΘ(d) = dO(1), or equivalently:

1 + Θ

= α = dO(1/d) = exp O

1
κ2

log d
d

= 1 + O

log d
d

.

(cid:19)

(cid:18)

(10)

(cid:19)
Solving for κ leads to the given relation between κ and d.

(cid:19)

(cid:18)

(cid:18)

Apart from the heuristic assumptions, this approximation factor is equivalent to Aharonov

(cid:112)

and Regev [AR04], who showed that the decision version of CVPP with approximation
factor κ = Ω(
d/ log d) can provably be solved in polynomial time. This further (heuris-
tically) improves upon results of [LLS90,DRS14], who are able to solve search-CVPP with
polynomial time and space complexities for κ = O(d3/2) and κ = Ω(d/√log d) respectively.
Assuming the heuristic assumptions are valid, Corollary 2 closes the gap between these
previous results for decision-CVPP and search-CVPP with a rather simple algorithm: (1)
preprocess the lattice by storing all dO(1) shortest vectors of the lattice in a list; and (2)
apply Algorithm 2 to this list and the target vector to ﬁnd an approximate closest vector.
Note that nearest neighbor techniques only aﬀect leading constants; even without nearest
neighbor searching this would heuristically result in a polynomial time and space algo-
rithm for κ-CVPP with κ = Ω(
d/ log d). An interesting open problem would be to see if
this result can be made provable for arbitrary lattices, without any heuristic assumptions.

(cid:112)

Acknowledgments

The author is indebted to L´eo Ducas, whose initial ideas and suggestions on this topic
motivated work on this paper. The author further thanks Vadim Lyubashevsky and Oded
Regev for their comments on the relevance of a subexponential time CVPP algorithm
requiring (super)exponential space. The author is supported by the SNSF ERC Transfer
Grant CRETP2-166734 FELICITY.

References

ADRS15.

Divesh Aggarwal, Daniel Dadush, Oded Regev, and Noah Stephens-Davidowitz. Solving the
shortest vector problem in 2n time via discrete Gaussian sampling. In STOC, pages 733–742,
2015.

14

Thijs Laarhoven

ADS15.

AEVZ02.

Divesh Aggarwal, Daniel Dadush, and Noah Stephens-Davidowitz. Solving the closest vector
problem in 2n time – the discrete gaussian strikes again! In FOCS, 2015.
Erik Agrell, Thomas Eriksson, Alexander Vardy, and Kenneth Zeger. Closest point search in
lattices. IEEE Transactions on Information Theory, 48(8):2201–2214, Aug 2002.
AKKV05. Misha Alekhnovich, Subhash Khot, Guy Kindler, and Nisheeth Vishnoi. Hardness of approx-

AKS01.

AR04.

BDGL16.

BGJ14.

BGJ15.

BL16.

BLS16.
BNvdP14.

CN11.

DRS14.

FBB+14.

FM02.

FP85.

GNR10.

IKMT14.

IM98.

Kan83.

Laa15a.

Laa15b.
LdW15.

LLS90.

imating the closest vector problem with pre-processing. In FOCS, pages 216–225, 2005.
Mikl´os Ajtai, Ravi Kumar, and Dandapani Sivakumar. A sieve algorithm for the shortest
lattice vector problem. In STOC, pages 601–610, 2001.
Dorit Aharonov and Oded Regev. Lattice problems in NP ∩ coNP. In FOCS, pages 362–371,
2004.
Anja Becker, L´eo Ducas, Nicolas Gama, and Thijs Laarhoven. New directions in nearest
neighbor searching with applications to lattice sieving. In SODA, pages 10–24, 2016.
Anja Becker, Nicolas Gama, and Antoine Joux. A sieve algorithm based on overlattices. In
ANTS, pages 49–70, 2014.
Anja Becker, Nicolas Gama, and Antoine Joux. Speeding-up lattice sieving without increasing
the memory, using sub-quadratic nearest neighbor search. Cryptology ePrint Archive, Report
2015/522, pages 1–14, 2015.
Anja Becker and Thijs Laarhoven. Eﬃcient (ideal) lattice sieving using cross-polytope LSH.
In AFRICACRYPT, pages 3–23, 2016.
Shi Bai, Thijs Laarhoven, and Damien Stehl´e. Tuple lattice sieving. In ANTS, 2016.
Joppe W. Bos, Michael Naehrig, and Joop van de Pol. Sieving for shortest vectors in ideal
lattices: a practical perspective. Cryptology ePrint Archive, Report 2014/880, pages 1–23,
2014.
Yuanmi Chen and Phong Q. Nguyˆen. BKZ 2.0: Better lattice security estimates. In ASI-
ACRYPT, pages 1–20, 2011.
Daniel Dadush, Oded Regev, and Noah Stephens-Davidowitz. On the closest vector problem
with a distance guarantee. In CCC, pages 98–109, 2014.
Robert Fitzpatrick, Christian Bischof, Johannes Buchmann, ¨Ozg¨ur Dagdelen, Florian
G¨opfert, Artur Mariano, and Bo-Yin Yang. Tuning GaussSieve for speed. In LATINCRYPT,
pages 288–305, 2014.
Ulrich Feige and Daniele Micciancio. The inapproximability of lattice and coding problems
with preprocessing. In CCC, pages 32–40, 2002.
Ulrich Fincke and Michael Pohst. Improved methods for calculating vectors of short length
in a lattice. Mathematics of Computation, 44(170):463–471, 1985.
Nicolas Gama, Phong Q. Nguyˆen, and Oded Regev. Lattice enumeration using extreme
pruning. In EUROCRYPT, pages 257–278, 2010.
Tsukasa Ishiguro, Shinsaku Kiyomoto, Yutaka Miyake, and Tsuyoshi Takagi. Parallel Gauss
Sieve algorithm: Solving the SVP challenge over a 128-dimensional ideal lattice. In PKC,
pages 411–428, 2014.
Piotr Indyk and Rajeev Motwani. Approximate nearest neighbors: Towards removing the
curse of dimensionality. In STOC, pages 604–613, 1998.
Ravi Kannan. Improved algorithms for integer programming and related lattice problems. In
STOC, pages 193–206, 1983.
Thijs Laarhoven. Sieving for shortest vectors in lattices using angular locality-sensitive hash-
ing. In CRYPTO, pages 3–22, 2015.
Thijs Laarhoven. Tradeoﬀs for nearest neighbors on the sphere. 2015.
Thijs Laarhoven and Benne de Weger. Faster sieving for shortest lattice vectors using spherical
locality-sensitive hashing. In LATINCRYPT, pages 101–118, 2015.
Jeﬀrey C. Lagarias, Hendrik W. Lenstra, and Claus-Peter Schnorr. Korkin-zolotarev bases
and successive minima of a lattice and its reciprocal lattice. Combinatorica, 10(4):333–348,
1990.

MB16.

LvdPdW12. Thijs Laarhoven, Joop van de Pol, and Benne de Weger. Solving hard lattice problems and
the security of lattice-based cryptosystems. Cryptology ePrint Archive, Report 2012/533,
pages 1–43, 2012.
Artur Mariano and Christian Bischof. Enhancing the scalability and memory usage of Hash-
Sieve on multi-core CPUs. In PDP, 2016.
Daniele Micciancio. The hardness of the closest vector problem with preprocessing. IEEE
Transactions on Information Theory, 47(3):1212–1215, 2001.
Artur Mariano, Thijs Laarhoven, and Christian Bischof. Parallel (probable) lock-free Hash-
Sieve: a practical sieving algorithm for the SVP. In ICPP, pages 590–599, 2015.
Artur Mariano, Thijs Laarhoven, and Christian Bischof. A parallel variant of LDSieve for
the SVP on lattices. 2016.

MLB15.

MLB16.

Mic01.

Sieving for closest lattice vectors (with preprocessing)

15

PS09.

MS11.

NV08.

Reg04.

MW15.

MV10a.

MV10b.

MTB14.

MODB14. Artur Mariano, ¨Ozg¨ur Dagdelen, and Christian Bischof. A comprehensive empirical compar-
ison of parallel ListSieve and GaussSieve. In Euro-Par 2014, pages 48–59, 2014.
Benjamin Milde and Michael Schneider. A parallel implementation of GaussSieve for the
shortest vector problem in lattices. In PACT, pages 452–458, 2011.
Artur Mariano, Shahar Timnat, and Christian Bischof. Lock-free GaussSieve for linear
speedups in parallel high performance SVP calculation. In SBAC-PAD, pages 278–285, 2014.
Daniele Micciancio and Panagiotis Voulgaris. A deterministic single exponential time algo-
rithm for most lattice problems based on Voronoi cell computations. In STOC, pages 351–358,
2010.
Daniele Micciancio and Panagiotis Voulgaris. Faster exponential time algorithms for the
shortest vector problem. In SODA, pages 1468–1480, 2010.
Daniele Micciancio and Michael Walter. Fast lattice point enumeration with minimal over-
head. In SODA, pages 276–294, 2015.
Phong Q. Nguyˆen and Thomas Vidick. Sieve algorithms for the shortest vector problem are
practical. Journal of Mathematical Cryptology, 2(2):181–207, 2008.
Xavier Pujol and Damien Stehl´e. Solving the shortest lattice vector problem in time 22.465n.
Cryptology ePrint Archive, Report 2009/605, pages 1–7, 2009.
Oded Regev. Improved inapproximability of lattice and coding problems with preprocessing.
IEEE Transactions on Information Theory, 50(9):2031–2037, 2004.
Claus-Peter Schnorr. A hierarchy of polynomial time lattice basis reduction algorithms.
Theoretical Computer Science, 53(2–3):201–224, 1987.
Michael Schneider. Analysis of Gauss-Sieve for solving the shortest vector problem in lattices.
In WALCOM, pages 89–97, 2011.
Michael Schneider. Sieving for short vectors in ideal lattices.
375–391, 2013.
Claus-Peter Schnorr and Martin Euchner. Lattice basis reduction: Improved practical al-
gorithms and solving subset sum problems. Mathematical Programming, 66(2–3):181–199,
1994.
Michael Schneider and Nicolas Gama. SVP challenge, 2015.
Noah Stephens-Davidowitz. Dimension-preserving reductions between lattice problems.
Available at http://noahsd.com/latticeproblems.pdf., 2016.
Xiaoyun Wang, Mingjie Liu, Chengliang Tian, and Jingguo Bi.
heuristic sieve algorithm for shortest vector problem. In ASIACCS, pages 1–9, 2011.
Jingdong Wang, Heng Tao Shen, Jingkuan Song, and Jianqiu Ji. Hashing for similarity search:
A survey. arXiv:1408.2927 [cs.DS], pages 1–29, 2014.
Feng Zhang, Yanbin Pan, and Gengran Hu. A three-level sieve algorithm for the shortest
vector problem. In SAC, pages 29–47, 2013.

In AFRICACRYPT, pages

Improved Nguyen-Vidick

SG15.
Ste16.

WLTB11.

WSSJ14.

ZPH13.

Sch87.

Sch13.

Sch11.

SE94.

A Pseudocode of SVP algorithms

Algorithms 3 and 4 present pseudo-code for the (sieve part of the) original Nguyen-Vidick
sieve and the GaussSieve, respectively, as described in Section 2. For the Nguyen-Vidick
sieve, the presented algorithm is a more intuitive but equivalent version of the original
sieve; see [Laa15a, Appendix B] for details on this equivalence.

Algorithm 3 The quadratic Nguyen-Vidick sieve for ﬁnding shortest vectors
Require: An input list L ⊂ L of (4/3)d/2+o(d) vectors of norm at most R
Ensure: The output list L(cid:48) ⊂ L has (4/3)d/2+o(d) vectors of norm at most γ · R
1: Initialize an empty list L(cid:48)
2: for each w1, w2 ∈ L do
3:
4:
5: return L(cid:48)

Add w1 − w2 to the list L(cid:48)

if (cid:107)w1 − w2(cid:107) ≤ γR then

16

Thijs Laarhoven

Get a vector v from the stack (or sample a new one if S = ∅)
for each w ∈ L do

Algorithm 4 The GaussSieve algorithm for ﬁnding shortest vectors
Require: A basis B of a lattice L(B)
Ensure: The algorithm returns a shortest lattice vector
1: Initialize an empty list L and an empty stack S
2: repeat
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14: until v is a shortest vector
15: return v

if (cid:107)v − w(cid:107) ≤ (cid:107)v(cid:107) then
Replace v ← v − w
if (cid:107)w − v(cid:107) ≤ (cid:107)w(cid:107) then
Replace w ← w − v
Move w from the list L to the stack S (unless w = 0)

Add v to the stack S (unless v = 0)

Add v to the list L (unless v = 0)

if v has changed then

else

Get a vector v from the stack (or sample a new one if S = ∅)
for each w ∈ L do

Algorithm 5 The non-adaptive GaussSieve (Phase 1) for ﬁnding closest vectors
Require: A basis B of a lattice L(B), a parameter α > 1
Ensure: The output list L contains αd+o(d) vectors of norm at most α · λ1(L)
1: Initialize an empty list L and an empty stack S
2: Let α0 = max{α, (cid:112)4/3}
3: repeat
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15: until v is a shortest vector
16: return L

Replace w ← w − v
Move w from the list L to the stack S (unless w = 0)

if (cid:107)v − w(cid:107)2 ≤ (2 − 2
α0
Replace v ← v − w
if (cid:107)w − v(cid:107)2 ≤ (2 − 2
α0

Add v to the stack S (unless v = 0)

Add v to the list L (unless v = 0)

0 − 1) · (cid:107)w(cid:107)2 then

0 − 1) · (cid:107)v(cid:107)2 then

if v has changed then

(cid:112)α2

(cid:112)α2

else

B Pseudocode of Phase 1 for non-adaptive sieving

To generate a list of the αd+o(d) shortest lattice vectors with the GaussSieve, rather than
the (4/3)d/2+o(d) lattice vectors one would get with standard sieving, we relax the reduc-
tions: reducing if
corresponds to an angle π/3 between v and w, leading
−
to a list size (1/ sin( π
3 ))d+o(d) = (4/3)d/2+o(d). To obtain a list of size αd+o(d), we reduce
vectors if their angle is less than θ = arcsin(1/α), which for vectors v, w of similar norm
corresponds to the following condition:

v
(cid:107)

w

<

v

(cid:107)

(cid:107)

(cid:107)

v
(cid:107)

w

(cid:107)

−

<

2(1

−

cos θ)

(cid:112)

v

(cid:107)

· (cid:107)

=

2
(cid:114)

−

2
α

(cid:112)

α2

1

v

.

(cid:107)

· (cid:107)

−

(11)

This leads to the modiﬁed GaussSieve described in Algorithm 5.

