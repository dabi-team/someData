2
2
0
2

y
a
M
4

]
L
C
.
s
c
[

1
v
6
6
9
1
0
.
5
0
2
2
:
v
i
X
r
a

Non-Autoregressive Machine Translation: It’s Not as Fast as it Seems

Jindˇrich Helcl1,2 and Barry Haddow1 and Alexandra Birch1
1School of Informatics, University of Edinburgh
2Faculty of Mathematics and Physics, Charles University
{jhelcl,bhaddow,a.birch}@ed.ac.uk

Abstract

Efﬁcient machine translation models are com-
mercially important as they can increase in-
ference speeds, and reduce costs and carbon
there has been much
emissions. Recently,
interest in non-autoregressive (NAR) models,
which promise faster translation. In parallel to
the research on NAR models, there have been
successful attempts to create optimized autore-
gressive models as part of the WMT shared
task on efﬁcient translation. In this paper, we
point out ﬂaws in the evaluation methodology
present in the literature on NAR models and
we provide a fair comparison between a state-
of-the-art NAR model and the autoregressive
submissions to the shared task. We make the
case for consistent evaluation of NAR mod-
els, and also for the importance of comparing
NAR models with other widely used methods
for improving efﬁciency. We run experiments
with a connectionist-temporal-classiﬁcation-
based (CTC) NAR model implemented in C++
and compare it with AR models using wall
clock times. Our results show that, although
NAR models are faster on GPUs, with small
batch sizes, they are almost always slower un-
der more realistic usage conditions. We call
for more realistic and extensive evaluation of
NAR models in future work.

1

Introduction

Non-autoregressive neural machine translation
(NAR NMT, or NAT; Gu et al., 2018; Lee et al.,
2018) is an emerging subﬁeld of NMT which fo-
cuses on increasing the translation speed by chang-
ing the model architecture.

The deﬁning feature of non-autoregressive mod-
els is the conditional independence assumption on
the output probability distributions; this is in con-
trast to autoregressive models, where the output
distributions are conditioned on the previous out-
puts. This conditional independence allows one
to decode the target tokens in parallel. This can

substantially reduce the decoding time, especially
for longer target sentences.

The decoding speed is assessed by translating a
test set and measuring the overall time the process
takes. This may sound simple, but there are various
aspects to be considered that can affect decoding
speed, such as batching, number of hypotheses in
beam search or hardware used (i.e., using CPU or
GPU). Decoding speed evaluation is a challeng-
ing task, especially when it comes to comparabil-
ity across different approaches. Unlike translation
quality, decoding speed can be measured exactly.
However, also unlike translation quality, different
results are obtained from the same system under
different evaluation environments. The WMT Ef-
ﬁcient Translation Shared Task aims to evaluate
efﬁciency research and encourages the reporting
of a range of speed and translation quality values
to better understand the trade-off across different
model conﬁgurations (Heaﬁeld et al., 2021). In this
paper, we follow the emerging best practices de-
veloped in the Efﬁciency Shared Task and directly
compare with the submitted systems.

In the development of NAR models, modeling
error and its subsequent negative effect on transla-
tion quality remains the biggest issue. Therefore,
the goal of contemporary research is to close the
performance gap between the AR models and their
NAR counterparts, while maintaining high decod-
ing speed. Considering these stated research goals,
the evaluation should comprise of assessing trans-
lation quality as well as decoding speed.

Translation quality is usually evaluated by scor-
ing translations of an unseen test set either us-
ing automatic metrics, such as BLEU (Papineni
et al., 2002), ChrF (Popovi´c, 2015) or COMET
(Rei et al., 2020), or using human evaluation. To
prevent methods from eventually overﬁtting to a
single test set, new test sets are published each
year as part of the WMT News Translation Shared
Task. In contrast, translation quality evaluation in

 
 
 
 
 
 
NAR research is measured almost exclusively on
the WMT 14 English-German test set, using only
BLEU scores. Automatic evaluation of transla-
tion quality remains an open research problem, but
current research advises against relying on a sin-
gle metric, and especially against relying on only
BLEU (Mathur et al., 2020; Kocmi et al., 2021).
In our experiments, we follow the recent best prac-
tices by using multiple metrics and recent test sets.

In this paper, we examine the evaluation method-
ology generally accepted in literature on NAR
methods, and we identify a number of ﬂaws. First,
the results are reported on different hardware ar-
chitectures, which makes them incomparable, even
when comparing only relative speedups. Second,
most of the methods only report latency (decoding
with a single sentence per batch) using a GPU; we
show that this is the only setup favors NAR models.
Third, the reported baseline performance is usually
questionable, both in terms of speed and transla-
tion quality. Finally, despite the fact that the main
motivation for using NAR models is the lower time
complexity, the ﬁndings of the efﬁciency task are
ignored in most of the NAR papers.

We try to connect the separate worlds of NAR
and efﬁcient translation research. We train non-
autoregressive models based on connectionist tem-
poral classiﬁcation (CTC), an approach previously
shown to be effective (Libovický and Helcl, 2018;
Ghazvininejad et al., 2020; Gu and Kong, 2021).
We employ a number of techniques for improving
the translation quality, including data cleaning and
sequence-level knowledge distillation (Kim and
Rush, 2016). We evaluate our models following a
uniﬁed evaluation methodology: In order to com-
pare the translation quality with the rest of the NAR
literature, we report BLEU scores measured on the
WMT 14 test set, on which we achieve state-of-
the-art performance among (both single-step and
iterative) NAR methods; we also evaluate the trans-
lation quality and decoding speed of our models in
the same conditions as the efﬁciency task.

We ﬁnd that despite achieving very good results
among the NAT models on the WMT 14 test set,
our models fall behind in translation quality when
measured on the recent WMT 21 test set using
three different automatic evaluation metrics. More-
over, we show that GPU decoding latency is the
only scenario in which non-autoregressive models
outperform autoregressive models.

This paper contributes to the research commu-

nity in the following aspects: First, we point out
weaknesses in standard evaluation methodology of
non-autoregressive models. Second, we link the
worlds of non-autoregressive translation and op-
timization of autoregressive models to provide a
better understanding of the results achieved in the
related work.

2 Non-Autoregressive NMT

The current state-of-the-art NMT models are au-
toregressive – the output distributions are condi-
tioned on the previously generated tokens (Bah-
danau et al., 2016; Vaswani et al., 2017). The de-
coding process is sequential in its nature, limiting
the opportunities for parallelization.

Non-autoregressive models use output distribu-
tions which are conditionally independent of each
other, which opens up the possibility of paralleliza-
tion. Formally, the probability of a sequence y
given the input x in a non-autoregressive model
with parameters θ is modeled as

pθ(y|x) =

(cid:89)

yi∈y

p(yi|x, θ).

(1)

Unsurprisingly, the independence assumption in
NAR models has a negative impact on the trans-
lation quality. The culprit for this behavior is the
multimodality problem – the inability of the model
to differentiate between different modes of the joint
probability distribution over output sequences in-
side the distributions corresponding to individual
time steps. A classic example of this issue is the
sentence “Thank you” with its two equally proba-
ble German translations “Danke schön” and “Vie-
len Dank” (Gu et al., 2018). Because of the inde-
pendence assumption, a non-autoregressive model
cannot assign high probabilities to these two trans-
lations without also allowing for the incorrect sen-
tences “Vielen schön” and “Danke Dank”.

Knowledge distillation (Kim and Rush, 2016)
has been successfully employed to reduce the nega-
tive inﬂuence of the multimodality problem in NAR
models (Gu et al., 2018; Saharia et al., 2020). Syn-
thetic data tends to be less diverse than authentic
texts, therefore the number of equally likely trans-
lation candidates gets smaller (Zhou et al., 2020).
A number of techniques have been proposed for
training NAR models, including iterative methods
(Lee et al., 2018; Ghazvininejad et al., 2019), aux-
iliary training objectives (Wang et al., 2019; Qian
et al., 2021), or latent variables (Gu et al., 2018;

Lee et al., 2018; Kaiser et al., 2018). In some form,
all of the aforementioned approaches use explicit
target length estimation, and rely on one-to-one
correspondence between the output distributions
and the reference sentence.

A group of methods that relax the requirement of
the strict one-to-one alignment between the model
outputs and the ground-truth target sequence in-
clude aligned cross-entropy (Ghazvininejad et al.,
2020) and connectionist temporal classiﬁcation (Li-
bovický and Helcl, 2018).

The schema of the CTC-based model, as pro-
posed by Libovický and Helcl (2018), is shown
in Figure 1. The model extends the Transformer
architecture (Vaswani et al., 2017).
It consists
of an encoder, a state-splitting layer, and a non-
autoregressive decoder. The encoder has the same
architecture as in the Transformer model. The
state-splitting layer, applied on the encoder out-
put, linearly projects and splits each state into
k states with the same dimension. The decoder
consists of a stack of Transformer layers. Unlike
the Transformer model, the self-attention in the
non-autoregressive decoder does not use the causal
mask, so the model is not prevented from attending
to future states. Since the output length is ﬁxed
to k-times the length of the source sentence, the
model is permitted to output blank tokens. Dif-
ferent positions of the blank tokens in the output
sequence represent different alignments between
the outputs and the ground-truth sequence. Connec-
tionist temporal classiﬁcation (Graves et al., 2006)
is a dynamic algorithm that efﬁciently computes
the standard cross-entropy loss summed over all
possible alignments.

We choose the CTC-based architecture for our
models because it has been previously shown to be
effective for NAR NMT (Gu and Kong, 2021; Sa-
haria et al., 2020) and performs well in the context
of non-autoregressive research. It is also one of the
fastest NAR architectures since it is not iterative.

3 Evaluation Methodology

The research goal of the non-autoregressive meth-
ods is to improve translation quality while main-
taining the speedup brought by the conditional in-
dependence assumption. This means that careful
thought should be given to both quantifying the
speed gains and the translation quality evaluation.
The speed-vs-quality trade-off can be characterized
by the Pareto frontier. In this section we discuss

Input token embeddings

Encoder

h

Wsplh

s

Decoder

Connectionist Temporal Classiﬁcation

w1 w2 w3 ∅ w4 ∅ w5 w6 ∅ ∅ ∅ w7 w8 ∅ w9 ∅
Output tokens / null symbols

The schema of

Figure 1:
the CTC-based non-
autoregressive architecture. We show the original im-
age from Libovický and Helcl (2018).

the evaluation from both perspectives.

Translation Quality.
In the world of non-
autoregressive NMT, the experimental settings
are not very diverse. The primary language pair
for translation experiments is English-German,
sometimes accompanied by English-Romanian to
simulate the low-resource scenario. These lan-
guage pairs, along with the widely used test sets
– WMT 14 (Bojar et al., 2014) for En-De and
WMT 16 (Bojar et al., 2016) for En-Ro – became
the de facto standard benchmark for NAR model
evaluation.

A common weakness seen in the literature is the
use of weak baseline models. The base variant of
the Transformer model is used almost exclusively
(Gu et al., 2018; Gu and Kong, 2021; Lee et al.,
2018; Ghazvininejad et al., 2020; Qian et al., 2021).
We argue that using weaker baselines might lead to
overrating the positive effects brought by proposed
improvements. Since the baseline autoregressive
models are used to generate the synthetic parallel
data for knowledge distillation, the weakness is
potentially further ampliﬁed in this step.

Evaluation is normally with automatic metrics
only, and often only BLEU is reported. In light of
recent research casting further doubt on the relia-
bility of BLEU as a measure of translation quality
(Kocmi et al., 2021), we argue that this is insufﬁ-
cient.

Decoding Speed. The current standard in eval-
uation of NAR models is to measure translation
latency using a GPU, i.e., the average time to trans-
late a single sentence without batching. Since the
time depends on the hardware, relative speedup is
usually reported along with latency.

This is a reasonable approach but we need to
keep in mind the associated difﬁculties. First, the
results achieved on different hardware architectures
are not easily comparable even when considering
the relative speedups. We also note that the relative
speedup values should always be accompanied by
the corresponding decoding times in absolute num-
bers. Sometimes, this information is missing from
the published results (Qian et al., 2021).

We argue that measuring only GPU latency dis-
regards other use-cases. In the WMT Efﬁciency
Shared Task, the decoding speed is measured in
ﬁve scenarios. The speed is reported using a GPU
with and without batching, using all 36 CPU cores
(also, with and without batching), and using a sin-
gle CPU core without batching. In batched decod-
ing, the shared task participants could choose the
optimal batch size. Our results in Section 5 show
that measuring latency is the only one that favors
NAR models, and as the batch size increases, AR
models quickly reach higher translation speeds.

4 Experiments

We experiment with non-autoregressive models for
English-German translation. We used the data pro-
vided by the WMT 21 News Translation Shared
Task organizers (Akhbardeh et al., 2021).

As our baseline model, we use the CTC-based
NAR model as described by Libovický and Helcl
(2018). We use stack of 6 encoder and 6 decoder
layers, separated by the state splitting layer which
extends the state sequence 3 times.

We implement our models1 in the Marian toolkit
(Junczys-Dowmunt et al., 2018). For the CTC loss
computation, we use the warp-ctc library (Amodei
et al., 2016).

4.1 Teacher Models

For training our baseline autoregressive models, we
closely follow the approach of Chen et al. (2021).
The preparation of the baseline models consists of
three phases – data cleaning, backtranslation, and
the training of the ﬁnal models.

1Our code is publicly available at https://github.

com/jindrahelcl/marian-dev

Data

Raw size Cleaned size

Parallel – clean
Parallel – noisy

Monolingual – En
Monolingual – De

3.9
92.0

93.1
149.9

3.1
84.6

91.0
146.2

Table 1: The sizes of the parallel and monolingual train-
ing datasets (in millions of examples).

We train the teacher models on cleaned parallel
corpora and backtranslated monolingual data. For
the parallel data, we used Europarl (Koehn, 2005),
the RAPID corpus (Rozis and Skadin, š, 2017), and
the News Commentary corpus from OPUS (Tiede-
mann, 2012). We consider these three parallel
datasets clean. We also use noisier parallel datasets,
namely Paracrawl (Bañón et al., 2020), Common
Crawl2, WikiMatrix (Schwenk et al., 2019), and
Wikititles3. For backtranslation, we used the mono-
lingual datasets from the News Crawl from the
years 2018-2020, in both English and German.

We clean the parallel corpus (i.e. both clean and
noisy portions) using rule-based cleaning4. Addi-
tionally, we exclude sentence pairs with non-latin
characters. and we apply dual cross-entropy ﬁlter-
ing on the noisy part of the parallel data (Junczys-
Dowmunt, 2018). We train Transformer base mod-
els in both directions on the clean portion of the
parallel data. Then, we select the best-scoring 75%
of sentence pairs for the ﬁnal parallel portion of the
training dataset.

For backtranslation (Sennrich et al., 2016), we
train four Transformer big models on the cleaned
parallel data in both directions. We then use them
in an ensemble to create the synthetic source side
for the monolingual corpora. We add a special
symbol to the generated sentences to help the mod-
els differentiate between synthetic and authentic
source language data (Caswell et al., 2019).

We use hyperparameters of the Transformer big
model, i.e. model dimension 1,024, feed-forward
hidden dimension of 4,096, and 16 attention heads.
For training, we use the Adam optimizer (Kingma
and Ba, 2014) with β1, β2 and (cid:15) set to 0.9, 0.998
and 10-9 respectively. We used the inverted square-
root learning rate decay with 8,000 steps of linear

2https://commoncrawl.org/
3https://linguatools.org/
4https://github.com/browsermt/
students/blob/master/train-student/
clean/clean-corpus.sh

warm-up and initial learning rate of 10-4.

The teacher models follow the same hyperpa-
rameter settings as the models for backtranslation,
but are trained with the tagged backtranslations in-
cluded in the data. As in the previous case, we train
four teacher models with different random seeds
for ensembling.

Similar to creating the backtranslations, we use
the four teacher models in an ensemble to create
the knowledge-distilled data (Kim and Rush, 2016).
We translate the source side of the parallel data, as
well as the source-language monolingual data. We
do not translate back-translated data. Thus, the
source side data for the student models is authen-
tic, and the target side is synthetic, created by the
teacher models.

4.2 Student Models

We train ﬁve variants of the student models with dif-
ferent hyperparameter settings. The “Large” model
is our baseline model – the same number of layers
as the teacher models, 6 in the encoder, followed by
the state splitting layer, and another 6 layers in the
decoder. The “Base” model has the same number
of layers with reduced dimension of the embed-
dings and the feed-forward Transformer sublayer,
to match the Transformer base settings. We also
try reducing the numbers of encoder and decoder
layers. We shrink the base model to 3-3 (“Small”),
2-2 (“Micro”), and 1-1 (“Tiny”) architectures.

We run the training of each model for three

weeks on four Nvidia Pascal P100 GPUs.

5 Results

In this section, we try to view the results of the
NAR and efﬁciency research in a shared perspec-
tive. We evaluate our models and present results
in terms of translation quality and decoding speed.
We compare the results to the related work on both
non-autoregressive translation and model optimiza-
tion.

Translation Quality. The research on non-
autoregressive models uses the BLEU score (Pap-
ineni et al., 2002) measured on the WMT 14 test
set (Bojar et al., 2014) as a standard benchmark for
evaluating translation quality. We use Sacrebleu
(Post, 2018) as the implementation of the BLEU
score metric. Using a single test set for the whole
volume of research on this topic may however pro-
duce misleading results. To bring the evaluation

Saharia et al. (2020)
Gu and Kong (2021)
Qian et al. (2021)

Large
Base
Small
Micro
Tiny

En → De De → En

28.2
27.2
26.6

28.4
23.7
23.6
25.0
20.3

31.8
31.3
31.0

31.3
30.3
29.1
27.5
21.7

Table 2: The BLEU scores of the NAR models on the
WMT 14 test set

up to date with the current state-of-the-art transla-
tion systems, we also evaluate our models using
COMET (Rei et al., 2020)5 and BLEU6 scores on
the recent WMT 21 test set. The same test set was
used in the WMT 21 Efﬁciency Task.

Table 2 shows the BLEU scores of our NAR
models on the WMT 14 test set. We show the re-
sults of the ﬁve variants of the NAR models and
we include three of the best-performing NAR ap-
proaches from the related work. We see from the
table that using BLEU, the “Large” model scores
among the best NAR models on the WMT 14 test
set. As the NAR model size decreases, so does
the translation quality, with the notable exception
of the En→De “Micro” model, which outperforms
the “Base” model consistently on different test sets.
In Table 3, we report the automatic evaluation
results of our AR and NAR models on the multi-
reference WMT 21 test set (Akhbardeh et al., 2021).
We compare our NAR models to the AR large
teacher models from Section 4.1, an AR base model
trained on the original clean data, and an AR base
student model trained on the distilled data. Follow-
ing Heaﬁeld et al. (2021), we use references A, C,
and D for English-German translation.

We see that there is a considerable difference in
the translation quality between the NAR models
and the AR large teacher model. This difference
grows with beam search and ensembling applied
on the AR decoding, techniques not usually used
with NAR models because of the speed cost. We

5We use the COMET model wmt20-comet-da from

version dd2298 (1.0.0.rc9).

6Signature: nrefs:3|bs:1000|seed:12345|
case:mixed|eff:no|tok:13a|smooth:exp|
version:2.0.0. For WMT 21 De → En, only 2 references
were used. For WMT 14, we used the signature with the
exception of having only a single reference.

En → De

COMET

BLEU

Model

Latency (ms)

AR – Large
+ beam
+ ensemble

AR – Base
+ beam

0.4110
0.4053
0.4332

0.3881
0.3873

50.5 ±1.3
50.8 ±1.3
52.2 ±1.3

47.9 ±1.3
48.0 ±1.3

Student AR – Base

0.4550

51.6 ±1.2

NAR models
Large
Base
Small
Micro
Tiny

0.1485
-0.0521
-0.0752
-0.0083
-0.3333

47.8 ±1.2
41.8 ±1.1
41.9 ±1.1
43.5 ±1.1
34.7 ±1.0

Table 3: Results of quantitative evaluation of English-
German translation quality using automatic metrics on
the multi-reference WMT 21 test set. The conﬁdence
intervals were computed using Sacrebleu.

also note that when we train an AR base model
on the distilled data, it outperforms the NAR large
model by a considerable margin.

Another thing we notice is the enormous differ-
ence in the COMET scores between the AR and
NAR models. The AR base models achieve compa-
rable BLEU scores to the NAR large models, but
differ substantially in the COMET score. From
a look at the system outputs, we hypothesize that
the NAR systems produce unusual errors which
BLEU does not penalise as heavily as COMET.
This might suggest that NAR models would rank
poorly in human evaluation relative to their autore-
gressive counterparts, despite the reasonable BLEU
score values. Another reason might be that the dif-
ferent errors of NAR models are causing a domain
mismatch between the COMET training data and
the data being evaluated.

Decoding speed. We follow the decoding time
evaluation methodology of the WMT 21 Efﬁcient
Translation Shared Task (Heaﬁeld et al., 2021). We
recreate the hardware conditions that were used
in the task. For the GPU decoding measurements,
we use a single Nvidia Ampere A100 GPU. The
CPU evaluation was performed on a 36-core CPU
Intel Xeon Gold 6354 server from Oracle cloud. To
amortize the various computation overheads, the
models submitted to the shared task are evaluated
on a million sentence benchmark dataset.

We measure the overall wall time to translate the

Gu et al. (2018)
Wang et al. (2019)
Sun et al. (2019)

Ours – Large

39
22
37

14

Table 4: The comparison of the decoding time of var-
ious NAR models for a single sentence in a batch on
a P100 GPU. Note that this table should serve merely
as an illustration, since the results were measured on
different datasets.

shared task dataset with different batching settings
on both the GPU and the 36-core CPU. The decod-
ing times are shown in Figures 2 and 4 for the GPU
and CPU times, respectively. We do not report the
single-core CPU latencies as the decoding speed
of the NAR models falls far behind the efﬁcient
AR models in this setup and the translation of the
dataset takes too long.

We can see that in case of GPU decoding that
all models beneﬁt from having larger batch sizes.
However, the non-autoregressive models are much
faster when the batch size is small. We also ran the
evaluation on an Nvidia Pascal P100 GPU, which
showed that when the batch size is large enough,
autoregressive models eventually match the speed
of non-autoregressive models. We show the decod-
ing times on the Pascal GPU in Figure 3. In Table
4, we compare the latencies measured on the Pas-
cal GPU to some of the related NAR approaches
that report results on this GPU type. Due to imple-
mentation reasons, the maximum batch size for our
NAR models is around 220 sentences.

Comparison with Efﬁcient AR Models.
In Ta-
ble 5, we present a comparison on the million sen-
tence test set with “Edinburgh base”, one of the
leading submissions in the WMT 21 efﬁciency task
(Behnke et al., 2021), which uses the deep encoder
– shallow decoder architecture (Kasai et al., 2021).
First, we see that using three different evaluation
metrics (ChrF, COMET, and BLEU), our models
lag behind the Edinburgh base model. In line with
our previous observation, we see a considerable
drop in the COMET score values.
In terms of
decoding speed, the only scenario in which the
non-autoregressive model is better is on GPU with
batch size 1. This is in line with our intuition that
the parallelization potential brought by the GPU is
utilized more efﬁciently by the NAR model. On

14,000

12,000

10,000

8,000

6,000

4,000

2,000

)
s
d
n
o
c
e
s
(

e
m

i
t

g
n
i
d
o
c
e
D

0

1

AR Large
AR Base
Large
Base
Small
Micro
Tiny

2

4

8

16

32

64

128

Batch size (sentences)

Figure 2: The decoding times to translate the efﬁciency task test set using various batch size settings, computed on
a single Nvidia Ampere A100 GPU, i.e. the GPU type used for evaluation in the efﬁciency task.

30,000

25,000

20,000

15,000

10,000

5,000

)
s
d
n
o
c
e
s
(

e
m

i
t

g
n
i
d
o
c
e
D

0

1

AR Large
AR Base
Large
Base
Small
Micro
Tiny

2

4

8

16

32

64

128

Batch size (sentences)

Figure 3: The decoding times to translate the efﬁciency task test set using various batch size settings, computed on
a single Nvidia Pascal P100 GPU.

14,000

12,000

10,000

8,000

6,000

4,000

2,000

)
s
d
n
o
c
e
s
(

e
m

i
t

g
n
i
d
o
c
e
D

0

1

AR Big
AR Base
Large
Base
Small
Micro
Tiny

2

4

8

16

32

64

128

Batch size (sentences)

Figure 4: The decoding times to translate the efﬁciency task test set using various batch size settings, computed on
36 CPU cores.

Translation quality

Decoding time (seconds)

ChrF COMET BLEU GPU, b>1 GPU, b=1

CPU, b>1

Edinburgh base (Behnke et al., 2021)

61.5

AR – Large (teacher)
AR – Base (student)

NAR – Large
NAR – Micro

59.2
59.5

58.6
57.3

0.527

0.411
0.455

0.149
-0.008

55.3

50.5
51.6

47.8
43.5

140

16,851

1,918
1,465

782
311

> 24h
> 24h

7,020
2,322

500

9,090
2,587

7,434
897

Table 5: A comparison of our AR and NAR models with one of the submissions to the WMT 21 efﬁciency task.
We show the results of automatic translation quality evaluation using three different metrics, and the decoding time
to translate the test set using a GPU and 36-core CPU with either latency (b=1) or batched (b>1) decoding.

one hand, larger batches open up the paralleliza-
tion possibilities to AR models. On the other hand,
limited parallelization potential (in form of CPU
decoding) reduces the differences between AR and
NAR models. The batch size of the Edinburgh base
model was 1,280 in the batched decoding setup.

6 Conclusions

In this paper, we challenge the evaluation methodol-
ogy adopted by the research on non-autoregressive
models for NMT.

We argue that in terms of translation quality,
the evaluation should include newer test sets and
metrics other than BLEU (particularly COMET
and ChrF). This will provide more insight and put
the results into the context of the recent research.
From the decoding speed perspective, we should
bear in mind various use-cases for the model
deployment, such as the hardware environment
or batching conditions. Preferably, the research
should evaluate the speed gains across a range of
scenarios. Finally, given that the latency condi-
tion – translation of one sentence at a time on a
GPU – already translates too fast to be perceived
by human users of MT, there is currently no com-
pelling scenario that warrants the deployment of
NAR models.

Acknowledgments

This project received funding from the Euro-
pean Union’s Horizon 2020 research and innova-
tion programmes under grant agreements 825299
and 825303 (GoURMET, Bergamot), and from
the Czech Science Foundation grant 19-26934X
(NEUREM3) of the Czech Science Foundation.
Our work has been using data provided by the
LINDAT/CLARIAH-CZ Research Infrastructure,

supported by the Ministry of Education, Youth
and Sports of the Czech Republic (Project No.
LM2018101).

References

Farhad Akhbardeh, Arkady Arkhangorodsky, Mag-
dalena Biesialska, Ondˇrej Bojar, Rajen Chatter-
jee, Vishrav Chaudhary, Marta R. Costa-jussa,
Cristina España-Bonet, Angela Fan, Christian Fe-
dermann, Markus Freitag, Yvette Graham, Ro-
man Grundkiewicz, Barry Haddow, Leonie Harter,
Kenneth Heaﬁeld, Christopher Homan, Matthias
Huck, Kwabena Amponsah-Kaakyire, Jungo Ka-
sai, Daniel Khashabi, Kevin Knight, Tom Kocmi,
Philipp Koehn, Nicholas Lourie, Christof Monz,
Makoto Morishita, Masaaki Nagata, Ajay Nagesh,
Toshiaki Nakazawa, Matteo Negri, Santanu Pal, Al-
lahsera Auguste Tapo, Marco Turchi, Valentin Vy-
drin, and Marcos Zampieri. 2021. Findings of the
2021 conference on machine translation (WMT21).
In Proceedings of the Sixth Conference on Machine
Translation, pages 1–93, Online. Association for
Computational Linguistics.

Dario Amodei, Sundaram Ananthanarayanan, Rishita
Anubhai, Jingliang Bai, Eric Battenberg, Carl Case,
Jared Casper, Bryan Catanzaro, Qiang Cheng, Guo-
liang Chen, Jie Chen, Jingdong Chen, Zhijie Chen,
Mike Chrzanowski, Adam Coates, Greg Diamos,
Ke Ding, Niandong Du, Erich Elsen, Jesse En-
gel, Weiwei Fang, Linxi Fan, Christopher Fougner,
Liang Gao, Caixia Gong, Awni Hannun, Tony Han,
Lappi Johannes, Bing Jiang, Cai Ju, Billy Jun,
Patrick LeGresley, Libby Lin, Junjie Liu, Yang
Liu, Weigao Li, Xiangang Li, Dongpeng Ma, Sha-
ran Narang, Andrew Ng, Sherjil Ozair, Yiping
Peng, Ryan Prenger, Sheng Qian, Zongfeng Quan,
Jonathan Raiman, Vinay Rao, Sanjeev Satheesh,
David Seetapun, Shubho Sengupta, Kavya Srinet,
Anuroop Sriram, Haiyuan Tang, Liliang Tang,
Chong Wang, Jidong Wang, Kaifu Wang, Yi Wang,
Zhijian Wang, Zhiqian Wang, Shuang Wu, Likai
Wei, Bo Xiao, Wen Xie, Yan Xie, Dani Yogatama,
Bin Yuan, Jun Zhan, and Zhenyao Zhu. 2016. Deep

speech 2 : End-to-end speech recognition in en-
glish and mandarin. In Proceedings of The 33rd In-
ternational Conference on Machine Learning, vol-
ume 48 of Proceedings of Machine Learning Re-
search, pages 173–182, New York, New York, USA.
PMLR.

Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2016. Neural machine translation by jointly
learning to align and translate.

Marta Bañón, Pinzhen Chen, Barry Haddow, Ken-
neth Heaﬁeld, Hieu Hoang, Miquel Esplà-Gomis,
Mikel L. Forcada, Amir Kamran, Faheem Kirefu,
Philipp Koehn, Sergio Ortiz Rojas, Leopoldo
Pla Sempere, Gema Ramírez-Sánchez, Elsa Sar-
rías, Marek Strelec, Brian Thompson, William
Waites, Dion Wiggins, and Jaume Zaragoza. 2020.
ParaCrawl: Web-scale acquisition of parallel cor-
In Proceedings of the 58th Annual Meeting
pora.
of the Association for Computational Linguistics,
pages 4555–4567, Online. Association for Compu-
tational Linguistics.

Maximiliana Behnke, Nikolay Bogoychev, Al-
ham Fikri Aji, Kenneth Heaﬁeld, Graeme Nail,
Qianqian Zhu, Svetlana Tchistiakova,
Jelmer
van der Linde, Pinzhen Chen, Sidharth Kashyap,
and Roman Grundkiewicz. 2021. Efﬁcient machine
translation with model pruning and quantization.
In Proceedings of
the Conference on Machine
Translation at the 2021 Conference on Empirical
Methods in Natural Language Processing, Punta
Cana, Dominican Republic.

Ondˇrej Bojar, Christian Buck, Christian Federmann,
Barry Haddow, Philipp Koehn, Johannes Leveling,
Christof Monz, Pavel Pecina, Matt Post, Herve
Saint-Amand, Radu Soricut, Lucia Specia, and Aleš
Tamchyna. 2014. Findings of the 2014 workshop on
statistical machine translation. In Proceedings of the
Ninth Workshop on Statistical Machine Translation,
pages 12–58, Baltimore, Maryland, USA. Associa-
tion for Computational Linguistics.

Ondˇrej Bojar, Rajen Chatterjee, Christian Federmann,
Yvette Graham, Barry Haddow, Matthias Huck, An-
tonio Jimeno Yepes, Philipp Koehn, Varvara Lo-
gacheva, Christof Monz, Matteo Negri, Aurélie
Névéol, Mariana Neves, Martin Popel, Matt Post,
Raphael Rubino, Carolina Scarton, Lucia Spe-
cia, Marco Turchi, Karin Verspoor, and Marcos
Zampieri. 2016. Findings of the 2016 conference
the
on machine translation.
First Conference on Machine Translation: Volume
2, Shared Task Papers, pages 131–198, Berlin, Ger-
many. Association for Computational Linguistics.

In Proceedings of

Isaac Caswell, Ciprian Chelba, and David Grangier.
In Proceedings of
2019. Tagged back-translation.
the Fourth Conference on Machine Translation (Vol-
ume 1: Research Papers), pages 53–63, Florence,
Italy. Association for Computational Linguistics.

Pinzhen Chen, Jindˇrich Helcl, Ulrich Germann, Lau-
rie Burchell, Nikolay Bogoychev, Antonio Valerio
Miceli Barone, Jonas Waldendorf, Alexandra Birch,
and Kenneth Heaﬁeld. 2021. The University of Ed-
inburgh’s English-German and English-Hausa sub-
In
missions to the WMT21 news translation task.
Proceedings of the Conference on Machine Transla-
tion at the 2021 Conference on Empirical Methods
in Natural Language Processing, Punta Cana, Do-
minican Republic.

Marjan Ghazvininejad, Vladimir Karpukhin, Luke
Zettlemoyer, and Omer Levy. 2020. Aligned cross
entropy for non-autoregressive machine translation.
In Proceedings of the 37th International Conference
on Machine Learning, volume 119 of Proceedings
of Machine Learning Research, pages 3515–3523.
PMLR.

Marjan Ghazvininejad, Omer Levy, Yinhan Liu, and
Luke Zettlemoyer. 2019. Mask-predict: Parallel de-
coding of conditional masked language models. In
Proceedings of the 2019 Conference on Empirical
Methods in Natural Language Processing and the
9th International Joint Conference on Natural Lan-
guage Processing (EMNLP-IJCNLP), pages 6112–
6121, Hong Kong, China. Association for Computa-
tional Linguistics.

Alex Graves, Santiago Fernández, Faustino Gomez,
and Jürgen Schmidhuber. 2006.
Connectionist
temporal classiﬁcation: Labelling unsegmented se-
quence data with recurrent neural networks. In Pro-
ceedings of the 23rd International Conference on
Machine Learning, pages 369–376, Pittsburgh, PA,
USA. JMLR.org.

Jiatao Gu, James Bradbury, Caiming Xiong, Vic-
tor O. K. Li, and Richard Socher. 2018. Non-
In 6th
autoregressive neural machine translation.
International Conference on Learning Representa-
tions, ICLR 2018, Vancouver, BC, Canada.

Jiatao Gu and Xiang Kong. 2021.

Fully non-
autoregressive neural machine translation: Tricks of
the trade. In Findings of the Association for Compu-
tational Linguistics: ACL-IJCNLP 2021, pages 120–
133, Online. Association for Computational Linguis-
tics.

Kenneth Heaﬁeld, Qianqian Zhu, and Roman Grund-
kiewicz. 2021. Findings of the WMT 2021 shared
task on efﬁcient translation. In Proceedings of the
Conference on Machine Translation at the 2021 Con-
ference on Empirical Methods in Natural Language
Processing, Punta Cana, Dominican Republic.

Marcin Junczys-Dowmunt. 2018. Dual conditional
cross-entropy ﬁltering of noisy parallel corpora. In
Proceedings of the Third Conference on Machine
Translation: Shared Task Papers, pages 888–895,
Belgium, Brussels. Association for Computational
Linguistics.

Marcin Junczys-Dowmunt, Roman Grundkiewicz,
Tomasz Dwojak, Hieu Hoang, Kenneth Heaﬁeld,

Tom Neckermann, Frank Seide, Ulrich Germann,
Alham Fikri Aji, Nikolay Bogoychev, André F. T.
Martins, and Alexandra Birch. 2018. Marian: Fast
neural machine translation in C++. In Proceedings
of ACL 2018, System Demonstrations, pages 116–
121, Melbourne, Australia. Association for Compu-
tational Linguistics.

Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
In Proceedings of
uation of machine translation.
the 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311–318, Philadelphia,
Pennsylvania, USA. Association for Computational
Linguistics.

Lukasz Kaiser, Samy Bengio, Aurko Roy, Ashish
Vaswani, Niki Parmar, Jakob Uszkoreit, and Noam
Shazeer. 2018. Fast decoding in sequence mod-
els using discrete latent variables. In International
Conference on Machine Learning, pages 2390–2399.
PMLR.

Jungo Kasai, Nikolaos Pappas, Hao Peng, James Cross,
and Noah Smith. 2021. Deep encoder, shallow
decoder: Reevaluating non-autoregressive machine
In International Conference on Learn-
translation.
ing Representations.

Yoon Kim and Alexander M. Rush. 2016. Sequence-
level knowledge distillation. In Proceedings of the
2016 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1317–1327, Austin,
Texas. Association for Computational Linguistics.

Diederik P Kingma and Jimmy Ba. 2014. Adam:
CoRR,

A method for stochastic optimization.
abs/1412.6980.

Tom Kocmi, Christian Federmann, Roman Grund-
kiewicz, Marcin Junczys-Dowmunt, Hitokazu Mat-
To ship or
sushita, and Arul Menezes. 2021.
not to ship: An extensive evaluation of automatic
arXiv preprint
metrics for machine translation.
arXiv:2107.10821.

Philipp Koehn. 2005. Europarl: A parallel corpus for
In Proceedings of
statistical machine translation.
Machine Translation Summit X: Papers, pages 79–
86, Phuket, Thailand.

Jason Lee, Elman Mansimov, and Kyunghyun Cho.
2018. Deterministic non-autoregressive neural se-
In Pro-
quence modeling by iterative reﬁnement.
ceedings of the 2018 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1173–
1182, Brussels, Belgium. Association for Computa-
tional Linguistics.

Jindˇrich Libovický and Jindˇrich Helcl. 2018. End-to-
end non-autoregressive neural machine translation
In Pro-
with connectionist temporal classiﬁcation.
ceedings of the 2018 Conference on Empirical Meth-
ods in Natural Language Processing, pages 3016–
3021, Brussels, Belgium. Association for Computa-
tional Linguistics.

Maja Popovi´c. 2015. chrF: character n-gram F-score
for automatic MT evaluation. In Proceedings of the
Tenth Workshop on Statistical Machine Translation,
pages 392–395, Lisbon, Portugal. Association for
Computational Linguistics.

Matt Post. 2018. A call for clarity in reporting BLEU
scores. In Proceedings of the Third Conference on
Machine Translation: Research Papers, pages 186–
191, Brussels, Belgium. Association for Computa-
tional Linguistics.

Lihua Qian, Hao Zhou, Yu Bao, Mingxuan Wang, Lin
Qiu, Weinan Zhang, Yong Yu, and Lei Li. 2021.
Glancing transformer for non-autoregressive neural
machine translation. In Proceedings of the 59th An-
nual Meeting of the Association for Computational
Linguistics and the 11th International Joint Confer-
ence on Natural Language Processing (Volume 1:
Long Papers), pages 1993–2003, Online. Associa-
tion for Computational Linguistics.

Ricardo Rei, Craig Stewart, Ana C Farinha, and Alon
Lavie. 2020. COMET: A neural framework for MT
evaluation. In Proceedings of the 2020 Conference
on Empirical Methods in Natural Language Process-
ing (EMNLP), pages 2685–2702, Online. Associa-
tion for Computational Linguistics.

Roberts Rozis and Raivis Skadin, š. 2017.

Tilde
MODEL - multilingual open data for EU languages.
In Proceedings of the 21st Nordic Conference on
Computational Linguistics, pages 263–265, Gothen-
burg, Sweden. Association for Computational Lin-
guistics.

Chitwan Saharia, William Chan, Saurabh Saxena, and
Mohammad Norouzi. 2020. Non-autoregressive ma-
chine translation with latent alignments. In Proceed-
ings of the 2020 Conference on Empirical Methods
in Natural Language Processing (EMNLP), pages
1098–1108, Online. Association for Computational
Linguistics.

Holger Schwenk, Vishrav Chaudhary, Shuo Sun,
Hongyu Gong, and Francisco Guzmán. 2019. Wiki-
matrix: Mining 135M parallel sentences in 1620
arXiv preprint
language pairs from wikipedia.
arXiv:1907.05791.

Nitika Mathur, Timothy Baldwin, and Trevor Cohn.
2020. Tangled up in BLEU: Reevaluating the eval-
uation of automatic machine translation evaluation
In Proceedings of the 58th Annual Meet-
metrics.
ing of the Association for Computational Linguistics,
pages 4984–4997, Online. Association for Computa-
tional Linguistics.

Rico Sennrich, Barry Haddow, and Alexandra Birch.
Improving neural machine translation mod-
2016.
In Proceedings of the
els with monolingual data.
54th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
86–96, Berlin, Germany. Association for Computa-
tional Linguistics.

Zhiqing Sun, Zhuohan Li, Haoqing Wang, Di He,
Zi Lin, and Zhihong Deng. 2019. Fast structured de-
coding for sequence models. In Advances in Neural
Information Processing Systems, volume 32, pages
3016–3026. Curran Associates, Inc.

Jörg Tiedemann. 2012. Parallel data, tools and inter-
In Proceedings of the Eighth In-
faces in OPUS.
ternational Conference on Language Resources and
Evaluation (LREC’12), pages 2214–2218, Istanbul,
Turkey. European Language Resources Association
(ELRA).

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems, volume 30. Curran Associates, Inc.

Yiren Wang, Fei Tian, Di He, Tao Qin, ChengXiang
Zhai, and Tie-Yan Liu. 2019. Non-autoregressive
machine translation with auxiliary regularization. In
Proceedings of the AAAI Conference on Artiﬁcial In-
telligence, volume 33, pages 5377–5384.

Chunting Zhou,

Jiatao Gu, and Graham Neubig.
2020. Understanding knowledge distillation in non-
In 8th Inter-
autoregressive machine translation.
national Conference on Learning Representations,
ICLR 2020, Addis Ababa, Ethiopia, April 26-30,
2020. OpenReview.net.

