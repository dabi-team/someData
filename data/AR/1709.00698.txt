A survey on haptic technologies for mobile
augmented reality

Carlos Bermejo, Pan Hui
Hong Kong University of Science and Technology

1

7
1
0
2

p
e
S
2
1

]

C
H
.
s
c
[

3
v
8
9
6
0
0
.
9
0
7
1
:
v
i
X
r
a

Abstract

Augmented Reality (AR) and Mobile Augmented Reality (MAR) applications have gained much research and industry attention
these days. The mobile nature of MAR applications limits users’ interaction capabilities such as inputs, and haptic feedbacks.
This survey reviews current research issues in the area of human computer interaction for MAR and haptic devices. The survey
ﬁrst presents human sensing capabilities and their applicability in AR applications. We classify haptic devices into two groups
according to the triggered sense: cutaneous/tactile: touch, active surfaces, and mid-air; kinesthetic: manipulandum, grasp, and
exoskeleton. Due to the mobile capabilities of MAR applications, we mainly focus our study on wearable haptic devices for
each category and their AR possibilities. To conclude, we discuss the future paths that haptic feedbacks should follow for MAR
applications and their challenges.

Haptic Feedbacks, Wearables, Kinesthetic, Cutaneous, User Experience.

Index Terms

I. INTRODUCTION

AR and MAR have attracted interest from both industry and academia in the last decade. MAR enhances the real world
of a mobile user with computer-generated virtual content. AR applications combine real and virtual objects in a physical
environment, are interactive in real time and display an augmented view. The advances in mobile computing, computer vision,
and networking have enabled the AR ecosystem. Due to the mobile nature of MAR applications, they tend to run on mobile or
wearable devices such as smartphones, tablets, smart-glasses. These device provide user’s mobility, but at the cost of constrained
resources such as computing-power, and energy. The computational constrains of these mobile devices limit the performance
and design of AR applications in the mobile environment. Therefore, cloud infrastructures ([1], [2]), computing-ofﬂoading
solutions ([3], [4]), service providers, cloudlets ([5]) and Fog computing ([6]) continue to deploy innovative services to provide
a real time AR experience ([7]). Microsoft1, Facebook2 and Apple3 have shown their interest in AR applications and they
believe in the viability of this technology. The current trend of mobile AR applications has affected the mobile market. Some
well-known commercial MAR applications such as Pokemon GO4 are location-based AR mobile games.

On the other side of the reality spectrum, Virtual Reality (VR) relates to a completely generated virtual world where the

user moves, and interacts with virtual objects. VR applications will play an important role in the
future of gaming and leisure services (e.g., Facebook VR5). These virtual scenarios bring another user experience, but the same
interaction methods. Both AR and VR share similar inputs and outputs methods between the augmented/virtual reality and
the user. The input methods used range from external devices such as hand controllers, wearables (e.g., Myo armband6), to
mid-air interactions such as gaze controller or hand gesture recognition (e.g., Microsoft HoloLens). User eXperience (UX) is
a key factor in these AR/VR ecosystems. An object’s virtual nature needs to be supported by usable and intuitive interactions.
In the following we can deﬁne input and output interactions. The former, corresponds to user input interactions with the
environment such as hand gestures, or using an external device (e.g., hand controller, and wearables). The latter, is related to the
feedbacks that the environment should provide to user actions or virtual object interactions such as sound, vibration, or visual.
Although input interactions have improved during the past few years owing to the advances in computer vision, tracking, and
image capturing devices such as cameras and infrared devices, the feedback provided by such environments are still primitive.
Feedback appears for example as images (visual feedback), sounds (sound feedback) and/or vibration (vibrotactile feedbacks).
They are aimed to provide a better UX and give the user a sense of agency (SA). The latter, SA, is related to initiating, executing
and controlling a user action. As an example, users can move virtual objects with their bare hands or using a controller, but
the physical characteristics of these virtual objects such as texture, size, and weight cannot be perceived without increasing
the complexity of the ecosystem. There exists devices to simulate textures, and virtual objects’ sizes. However, these systems
require external and complicated devices to be worn by the user.

1https://www.microsoft.com/en-us/hololens
2https://developers.facebook.com/products/camera-effects/ar-studio/
3https://developer.apple.com/arkit/
4http://www.pokemongo.com/
5https://www.wired.com/2017/04/facebook-spaces-vr-for-your-friends/
6https://www.myo.com/

 
 
 
 
 
 
TABLE I: Haptic Device Classiﬁcation

Group

Type

Characteristics

Cutaneous/tactile

Cutaneous
Active surfaces
Mid-air

Haptic on the user’s skin (i.e., ﬁngertip)
Communication large-scale forces and shapes and tactile information
Tactile feedback without contact (i.e., air, ultrasound devices)

Kinesthetic

Manipulandum
Grasp
Exoskeleton

Grounded devices with 3 to 6 Degrees of Freedom (DOF)
Simulates grasping interactions at the user’s hand or ﬁngers
Grounded on the body, provide forces on natural DOF of the body

MAR applications limit the size of haptic devices that can be used. The mobile characteristics of these environments are
the focus of our related work on wearable devices. Although, there exists a myriad of developments regarding AR without
size/weight limitations such active surfaces (Section III) or manipulandum devices (Section IV).

In this survey, in section II we present the human sensing capabilities and their applicability to AR applications. After
classifying the main groups of haptic devices regarding the involved sense, in section III we present the tactile wearable
devices. In Section III, we also describe the three main approaches: cutaneous, active surfaces, and mid-air; to provide a
tactile experience. After that, we focus our study on kinesthetic feedback and the different approaches to designing the devices
(Section IV). In this Section IV we describe the grounded devices, called manipulandum, grasping devices, exoskeleton, and
other devices that do not ﬁt within these three categories. In Section VI, we mention the current commercial haptic feedback
devices on the market. Finally, we present the challenges and future directions for MAR haptic devices (Section VII) before
we conclude the survey (Section VIII).

II. HAPTICS

Haptic devices enables human-computer-interaction (HCI) through touch, and external forces. Unlike traditional interfaces
such as displays and sound devices, haptic devices render mechanical signals (i.e., external force) which stimulate human touch
and kinesthetic channels. This ﬁeld borrows from many areas, including robotics, experimental psychology, biology, computer
science, system and control, and others. Due to the recent popularity of VR and AR systems, haptic devices have received
great attention within the research community and entertainment industry (i.e, ﬁlm, gaming, and mobile industry). The haptic
ecosystem will continue its development to yield the interaction complexity needed for real-time information transmission.
Visual and auditory channels are not enough to provide a perfect UX in AR/VR ecosystems. There is a need to feel (i.e., touch
and move) objects in the virtual world analogously to the physical world. Haptic devices enable a bidirectional communication
channel between human-computer, it creates a strong sensation of immediacy.

Haptic devices appear in multiple application scenarios [8]:
• Feedback reinforcement of GUIs, such as buttons, pull-down menus
• Games
• multi-media publishing, new immersive media using VR and AR mobile platforms
• science and data analysis, for example in geology, data mining, multi-dimensional maps
• arts and creation, similar to music and painting equivalents of audio/visual communication channels
• Sound and images edition, help during the editing process with new user experiences
• Vehicle industry, better interfaces for controlling the vehicle without loosing the visual sense
• Manufacturing, to reduce the need of prototyping
• Telerobotics and Teleoperation, high quality manual controllers such as Da Vinci surgical system7
• Education and training, simulated training, and innovative passive learning methods
• Rehabilitation, for example the improvement of living conditions for visually impaired people
• Scientiﬁc study of touch
The principle operation of haptics is based on cutaneous and kinesthetic sensations. Cutaneous/tactile, is related to the
skin; Kinesthesia/proprioception/force, is a sense mediated by the end sensory organs located in muscles, tendons and joints
(Table I. It is related to the capabilities of sensing the relative position of the body’s parts and strength. The tactile receptors
vary tremendously with the parts of the body they cover. “Proprioceptive, or kinesthetic perception refers to the awareness of
one’s body state and includes the position, velocity and force supplied by the muscles” [9]. Together, kinesthetic and cutaneous
sensations are “fundamental to manipulation and locomotion” [10] of virtual objects in VR applications.

One of the main issues with tactile feedback is the necessary high rate refresh requirements in order to provide a satisfying
experience (1kHz or more); if we compare it with video (60Hz or more) the difﬁculties during the design and development of
cutaneous haptic devices are clear. However, the adaptability of humans also include the tactile sense and even in situations with
low haptic rendering the user ignores small imperfections and gaps in the stimulation. Besides, the addition of other feedback

7http://www.davincisurgery.com/

2

channels, such as visual or audio, improves the overall haptic feedback experience. In situations where the imperfections are
too obvious the realism breaks down in a similar way to videos with a lower frame rate.

A complete haptic interface can include one or several actuators such as vibroactuators, manipulandum, and sensors to
measure and react to user’s interactions. Besides, the feedback simulation of virtual objects is a key topic to render realistic
haptic sensations. For example, the rendering of a virtual texture such as roughness for the user’s cutaneous sense, or the weight
of a virtual object for the kinesthetic sense. Besides, the combination of haptic feedbacks can improve the overall experience
as it achieves the most realistic scenario.

Just noticeable difference (JND) known as the Weber-Fechner Law “is a measure of the minimum difference between two
stimuli which are necessary in order for the difference to be distinguishable”. The JND measure is widely used in the haptic
feedback ecosystem, as the cutaneous and force (kinesthetic) perceptions can differ between users and it is not easy to quantify
the differences. For example in [11], the authors use JNDs to locate thresholds of force sensitivity in virtual environments.
They use a PHANToM device for their experimental study to quantify the JND for force feedback.

Furthermore, the developments in wearable haptic devices open new challenges and innovative ideas for VR, AR and
the forthcoming tactile Internet [12]. The high requirements regarding latency, create new topics and research paths in the
networking, and sensor (i.e., electronics) ﬁelds, see more in Section VII.

Wearable ﬁnger-based cutaneous haptic devices are increasing in popularity as the most suitable approach for MAR/VR
applications. However, there is still the need to study the detection thresholds of these devices. The insights of these studies
can improve the resolution of the haptic feedbacks and reduce the network transmission of unnecessary feedback information
(i.e., levels not perceived by users).

In [13], the authors aim to quantitatively answer the JND in ﬁnger-tracking systems and the visual proprioceptive conﬂicts
that can arise in these scenarios. In ﬁnger-tracking systems, the user interacts with the virtual environment using their ﬁngers.
The authors focus their experimental study on cutaneous ﬁnger haptic devices and a virtual environment which is rendered
in a HMD (i.e., Oculus Rift8). The results show a JND value of 5.236cm, so users can distinguish between their own ﬁnger
and a false (i.e., rendered) one with error lager than previous JND value. Regarding the haptic feedback, the experiment
presents that in small error, proprioceptive and visual situations the participant’s rely on the haptic cues. In situations where
the visual proprioceptive error is high, the haptic cues lose their role in the tracking system, as the users will not rely on them.
The mentioned error threshold is around 3.64cm. This paper demonstrates the improvements in ﬁnger-tracking systems using
cutaneous haptic feedbacks with visual proprioceptive low error.

As within visual [14] and auditive feedback [15], tactile sensations produces different emotional responses. In [16], the authors
study the emotional responses of tactile icons. They evaluate the different responses according to four physical parameters of
tactile feedback: amplitude, frequency, duration, and envelope. The authors use the valence-arousal (V-A) space to quantify
the emotions [17]. The paper presents the response to tactile feedback using a vibrotactile actuator attached to the back of a
smartphone. The results show that amplitude (i.e., perceived intensity) and duration have similar effects on emotional responses.
Besides, the carrier frequency is strongly related to positive valence. For example more amplitude or duration corresponds with
lower valence, and the greater the carrier frequency and amplitude the higher the valence and vice-versa. These insights show
the relations between vibrotactile physical parameters and emotional responses. Therefore, tactile feedback needs to consider
the emotional responses that can arise. In [18], Papetti et al. study the effect of pressing the ﬁngertip while vibrating. Many
works have focused on vibrotactile thresholds without considering the effects of applied forces in the tactile sensation. One
key insight from the experiment results is that vibrotactile sensitivity depends on by applying pressure with the ﬁngertips.
Therefore, the active forces should be taken into consideration when designing vibrotactile feedback approaches. In [19],
the authors illustrate with several experiments how the dynamics of vibrotactile actuators change as a function of the body
attachment. The authors focus on two commonly used vibrotactile displays for their experiments. The experiment results
demonstrate the relation between different load forces and the vibration feedback. As with many wearable tactile displays, the
tactors attached to the skin can vary the user’s perception (i.e., attenuation) depending on their location.

A. Other Non-Haptic Feedback

Although this paper focuses on haptic feedback and their wearability for MAR ecosystems, we need to mention other
important non-haptic feedbacks such as visual and audio. The use of these non-haptics to enhance the UX is demonstrated in
several papers [20] [21], and they form the baseline case for feedback communication in several systems. For example, we can
see their implementation in touchscreen virtual keyboards where the key is zoomed (visual) and there is a click-sound (audio)
when the user presses a key. In the last decade, mobile phones provide cutaneous feedback in the form of vibration, but the
visual and sound still remain as feedbacks. Moreover, in the VR and AR applications the addition of haptic feedback does not
mean the suppression of any other audio-visual feedback channels, on the contrary the best approach is to combine the three
sense channels. However, due to the different nature of sensory receptors (i.e., audio, visual, touch, and proprioception) the
combination of these feedback complicate the design, feasibility and implementation of these realistic types of feedbacks.

8https://www.oculus.com/rift/

3

Visual feedback and user actions have been strongly related since the ﬁrst GUI computer system. The design of visual
feedback to react to user’s interactions is primordial to give an appropriate response to those actions. In [20], [21] the authors
study the user’s reaction time to visual feedback (190-260 ms). Furthermore, the visual improvements in current years not only
focus on simpler and more intuitive methods but in faster and more ﬂuent visual feedback. For example the visual latency of
a touchscreen can hinder the UX, and scrolling speed can affect the user satisfaction while using the device.

We perceive actions that produce sound in our daily lives. For example, beeps resulting from a key press, or clang while
dropping an object on the ﬂoor. In [22], the authors study the egocentric nature of the action-sound associations (i.e., gesture-
sound association). This question is very important in VR, and AR environments. The egocentric action-sound nature shows
that users are able to learn that certain gestures create different types of sounds. Besides audio feedback can modify user’s
perception of kinesthetic force in VR, AR environments (i.e., change the virtual arm length, or movement). In [23], the authors
present different audio manipulation techniques for haptic media generation. They use the physical nature of sound waves to
generate pleasant haptic feedback on a mobile phone. Although audio is a strong sensory feedback, the authors implement
additional vibration feedback to improve the UX.

The addition of more feedback channels such audio, sound and tactility can improve the interaction performance in many
situations. However, due to our strong reliance on our visual and auditory senses, there can be situations when the addition of
a haptic feedback does not provide any improvement [24]. In this paper, the authors compare different feedback conditions:
normal (no feedback), auditory, tactile and combined in a target selection of task experiments. Although, the results show
that there is no signiﬁcant difference in response times, error rates or bandwidth; there is a signiﬁcant difference in the ﬁnal
positioning times (i.e., mouse cursor entering the target to selecting the target). Regarding the ﬁnal positioning, the cutaneous
feedback was the fastest. The combined feedback (visual, auditory and tactile) is also very similar to the tactile feedback
condition. Therefore, there is not any increasing effect in combining sensory information for this particular task. However,
the addition of other feedback channels is expected to yield to performance improvements, such as audio-visual together with
haptic in the VR, and AR ecosystems.

B. Wearable Haptic Devices

The portable and mobile nature of MAR applications impose several design limitations on the haptic feedback we can
implement [25]. These wearable devices are required to be light weight, small and comfortable. Furthermore, the device should
not limit the user’s movements, and their body degrees-of-freedom (DOFs). For example, in ﬁngertip actuators we need to
consider that this design restrains the free movement of the user’s ﬁnger and limits the use of the ﬁnger for other tasks such
as typing on touchscreen devices. Wearable devices need to consider the body DOF in its design to avoid any restrictions on
user’s movement.

A wearable haptic device should follow the design principles in Table II. As a haptic device, they are required to follow the
haptic device guidelines such as feedback intensity, ﬁdelity factor, comfort, and mobility. The addition of portability includes
more constrains to the wearable device design such as form factor, comfort, mobility, and autonomy.

Moreover, the wearability characteristics limit the design of actuators and their energy consumption. The devices have to

include small actuators that can be portable and require small amounts of energy in order to provide good autonomy.

In the following Sections we describe the different devices according to their haptic feedback features. We mention not only
the wearable devices due to their size, weight and design, but also state of the art and novel devices that have had an impact
on the haptic feedback ﬁeld. As previously mentioned, the portable design of cutaneous devices make them one of the ﬁrst
approaches that a MAR designer needs to consider to provide haptic feedback to their AR ecosystem. Although, some novel
approaches of mid-air and exoskeleton to reduce their size also make them also feasible wearable devices to consider for MAR
haptic feedbacks design.

III. TACTILE, CUTANEOUS FEEDBACK

In this Section we enumerate the most novel and important haptic devices that use the cutaneous sensory system to provide
haptic feedback. We group the devices by user contact interaction (Table I), as we think it makes easier for the MAR haptic
feedback designer to ﬁnd the most appropriate wearable device according to the feedback they want to provide. For example
in virtual texture rendering scenarios, active surfaces III-B and mid-air devices III-C feature high resolution and accurate
characteristic, but the design portability hinders in some of the proposed systems (i.e., ﬁrst pin arrays devices).

A. Cutaneous devices

The cutaneous/tactile approach is currently one of the most used haptic feedback devices. The vibration on our smartphones or
games controllers enables cutaneous perception of user’s hand when we type, or crash a car in a video-game. The miniaturization
and simple design of vibration motors make them a cost effective and feasible implementation haptic technique. However,
vibration patterns are difﬁcult to distinguish in many situations such as walking, and offer limited information (i.e., duration,
strength and vibration pattern). Therefore, in [26], the authors propose a haptic display feedback with a piezoceramic ﬁlm

4

TABLE II: Design Principles Wearable Devices

Wearable Devices

Form factor

Device size and shape according to the body part it is to be attached to.
Depends on the actuators and feedback type (i.e., kinesthetic, tactile, mid-air)

Weight

Mobility

Comfort

The weight of the device is relative to the part of the body where it is worn.
For example, devices worn on the leg can be heavier than on the arm.
It depends on the muscular-skeletal strength

The mobility of the user needs to be considered in the placement on the body of the device.
The wearable cannot limit the mobility of any user’s muscles or body parts.
The device needs to feel natural.

The device needs to be for long periods without discomfort.
The device has to be adaptable and the components (i.e., shapes, edges, bands)
ergonomically ﬂexible.

Autonomy

Wearable devices require enough autonomy for MAR application use.

Haptic Devices

Feedback

Intensity

The feedback needs to be precise and accurate.
It should display coherently the force, and cutaneous haptics involved in the interaction

The intensity of the feedback has to be appropriate to the situation,
and user force and interactions (i.e., if the user is pushing the object).

Fidelity

Realistically rendered haptic feedback (i.e., realistic virtual object weight).

Duration

Duration according to the scenario, enough for the user to notice that a feedback has been triggered.

TABLE III: Most distinctive cutaneous haptic devices

Type

Device

Author/Reference

Characteristics

Vibration

Vibration Wristband
Smartphone vibration device

Carcedo et al. [29]
Hoggan et al. [30]

Three vibration motors on a wristband device
Vibroactuator on the smartphone

3 RRS
Hapthimble

Fingertip

Chinello et al. [31]
Kim et al. [32]

Haptic Thimble

Gabardi et al. [33]

Skin wristband
Skin displacement

Chinello et al. [34]
Culbertson et al. [35]

3DOFs ﬁngertip surface render device
Fingertip haptic thimble device for
pushing virtual buttons
Fingertip voicecoil actuator

Wristband skin stretch device
Skin displacement device to render
pull sensations

Fingersight
BrushTouch

Hovarth et al. [36]
Strasnick et al. [37]

Finger camera-haptic vibration device
Wristband skin brushing device

Skin

Other

actuator to enhance the vibration haptic feedback. The device can be implemented on any button, display or inside a smartphone,
and features very sharp and distinctive feedback. The authors proposed a better feedback response in comparison to vibrotactile
actuators in 2003. The authors demonstrate via experiments the UI design beneﬁts together with the tactile feedback in widgets’
drag and down actions. Haptic feedback such as vibration can beneﬁt the user interaction in the MAR environment as it adds
another information channel to transmit sensory data such as touching an icon, and moving it. In [27], the authors present an
eye-free text input approach based on auditory and tactile stimuli (i.e., vibration) from the smartphone. Their technique uses
Grafﬁti [28] as ﬁnger input text. Besides the feedback, an eyes-free mode is provided by auditory and tactile stimuli. The input
approach uses vibration for unrecognized strokes and sound such as characters spoken for recognized strokes. The participants
during the evaluation copy a text in their phones using the Grafﬁti input technique. The experiments show better performance
for the eyes-free mode against eyes-on mode. The results show that there is no signiﬁcant ranked (by participants) difference
between audio, tactile, and audio-tactile feedback. For speciﬁc feedback types, the design parameters are important for the
users (participants). To conclude, the duration of the feedback does not play an important role for most type of information
during the experiment; it just needs to be long enough to be perceived by the participants. The study demonstrates that vibration
feedback does not have better performance results than just audio scenarios.

Hoggan et al. [30] report their experimental insights into the relationship and beneﬁts of audio and tactile icons to optimize
information transmission. Previous research has focused on the use of different senses to provide the same information. They
also claim that there are speciﬁc parameters in audio and tactile feedback to give the user warning notiﬁcations. The authors
choose ﬁve different parameters in the design of the crossmodal audio and tactile design feedback. These design parameters
are: texture (i.e., different audio waveforms); spatial location (i.e., two vibration tactors on the back of the device); duration
(i.e., sound duration); and tempo/rate (i.e., for both sound and vibration). The experiment setup consists of a smartphone device

5

(a) Skin stretch device, four
actuators [34] .

(b) Asymmetric skin dis-
placement [35].

(c) hRing [41].

(d) FingerSight, ﬁngertip
camera [36].

Fig. 1: Wearable ﬁngertip haptic devices. In 1b, the two voicecoil actuators simulate the asymmetric vibration on user’s ﬁngers.

that displays different information types and feedback designs such as conﬁrmations (i.e., SMS received), errors, and progress
updates (i.e., downloads). This study demonstrates that the dimensions of the vibration patterns [29](i.e., tempo/rate, duration,
strength) enable more resolution for the haptic information transmission. Although, as the previous study [27], it depends on
the scenario and the user’s environment.

The dominance of visual sense can sometimes restrain our perception of the relationship between haptic and other non-
haptic feedback. For example, in many studies and commercial applications such as video-games. The visual feedback and
vibrotactile feedback are strongly related, if you touch a wall the haptic device vibrates. However, we can link together other
non-haptic feedback such as sound to enhance the experience in non visual MAR applications. In [38], the authors propose a
real-time audio-level algorithm for vibrotactile sensory translation. Author’s implementation improves auditory-tactile feedback,
and hence, enhances users immersion. The algorithm extracts loudness and roughness from audio signals and translates them
to vibrotactile perceptual metrics: intensity and roughness. The experiments conﬁrm that the proposed translation algorithm
improves sound effects corresponding to appropriate vibrotactile feedback. The proposed algorithm can be implemented in
mobile devices to provide an enhanced audio-tactile user experience. The designers of haptic feedback for MAR applications
have to consider the limitations of simple vibration designs and their capabilities. The small and light weight design of these
actuators make them the ﬁrst device to consider and almost include in any MAR application to provide a haptic communication
channel between a virtual objects and a user The information transmission limitations of vibrotactile actuators has been studied
extensively. The nature of these cutaneous haptic devices make the transmission of high dimensionally data difﬁcult as, they
are limited to patterns such as intensity, duration and frequency. One-dimensional vibration transmission has been studied
since the phantom sensations in [39]. In this paper, the authors study the information transmission on the skin along through
the location of vibration sensors. The results of the experiments show that the phantom sensation can display information
with a low learning curve. In [40], the authors extend the dimension of the vibroactuator on smartphones using four actuators
to provide two-dimensional vibrotactile capabilities. The smartphone displays the vibrotactile ﬂows to enable the dynamic
phantom sensations. The 2D vibrotactile ﬂows can display sensations in both 2D and circles in both directions. In [29], the
authors present an assistive multi-vibrotactile wristband that provides color information using vibration encoding for users
who are colorblind. The authors study different vibration motor displacement and the best conﬁguration based on user’s
perception accuracy. Besides, they analyze the best encoding vibration pattern to be easily and quickly recognizable. The three
motor designs offer high accuracy, although it depends on the user’s wrist size. Duration and sequence or number of pulses
and sequential vibrations offer the best encoding methods. The vibration pattern dimensionality can enable better and higher
bandwidth of information transmission.

FingerSight [36] is a novel ﬁngertip device for acquiring visual information through haptic channels. The visual environment
information is translated into haptic stimulations. The authors’ device aims to provide assistive technologies for the visually
impaired. The device consists of a camera capturing device, and two speaker-based vibrators, sub-ﬁgure 1d. For the purpose
of the experimental testing, the authors develop a software to detect changes in the background image color (color boundaries)
so the system generates vibrations. The proposed device permits the user to scan the environment using a ﬁnger to locate
speciﬁc targets (i.e., vibrates when the targeted object is found). However, the feedback interaction can be limited in 3D
surroundings environments as it can not provide enough information. This wearable device can provide ﬁnger interactions with
haptic feedback in MAR applications without camera-based tracking systems. Therefore, we can interact with virtual objects
without the tracking technique limitations.

Currently, most communication systems provide only visual and auditive interfaces. However, gestures such as stroking,
patting, and squeezing can enhance the emotional communication of current audio-visual systems. In [42], the authors conduct
a user study to provide insights about the user’s preferences for touch gestures in audio-tactile communications. The experiment
setup consists of a device that can recognize user’s gestures (i.e., squeezing, patting) and vibrotactile stimulation devices. Some
of the participants touch the devices while others will feel the touches created on the devices (by the mentioned participants)
simultaneously. Furthermore, the authors aim to map the touch gestures and their suitability during communications with a
questionnaire. For example, two participants ﬁnd the squeezing touch gesture as holding hands analogy. The main ﬁndings are
that squeezing in audio-tactile communications and the beneﬁt of using multiple vibrotactile actuators when the communication

6

is only tactile is preferred. The tactile communication channel could be very useful as the future SMS in the tactile Internet
(see more in Section VII). Kangas et al. [43] combine gaze gesture with vibrotactile feedback to provide conﬁrmation of
interactions. In gaze gestures approaches, visual feedback can be tricky to perceive during eye movement interactions. Therefore,
vibrotactile feedback can be beneﬁcial in these situations. Authors use a gaze tracker and smartphone to enable the vibrotactile
feedback. The experimental results show that haptic feedback improves efﬁciency of the overall gaze-experience. Sword of
Elements (SoEs) [44] is an attachable augmented reality vibrotactile feedback device. It is an attachable solution to enhance
player experience in VR environments. The device is attached to the HTC VIVE controller9 and features a motor module, an
electronic fan (i.e., VR wind) and a thermal module.

Roudaut et al. [45] propose a foil overlaid touchscreen to enable spatial gesture outputs. The transparent foil device can
provide up to a 1 cm motion range on a smartphone. The gesture output is non-visual and non-auditory, only tactile as it
moves (motors for X-Y coordinates) the transparent foil along the user’s ﬁnger. Results demonstrate that these 2-D gestures
outputs are easy to learn by transfer. The novel approach of moving the touch (i.e., touchscreen) surface on the user’s ﬁngertips
enables surface rendering capabilities on a portable device. We can think of different speed movements to render the moving
of virtual objects in a MAR application.

HACHIStack [46] is a novel system which can estimate the contact time in the touchscreen of approaching objects towards
a touchscreen. This sensing method not only provide estimation of the approaching velocity, it also reduces touch latency
experienced with current touch screens. In summary, HACHIStack can provide high-speed, velocity of the approaching object,
and contact time prediction on a touch screen using two-layer photosensors. The authors have tried to solve the latency problem
of touchscreen with a photosensor layer and a prediction algorithm, and hence render haptic feedback in real-time, without
delays.

In [47] the authors introduce a passive back-of-device tactile landmarks to estimate ﬁnger location without seeing the
screen’s device. The authors propose several landmark designs and study the performance with the base case (no landmarks).
The experiment results show that the back-of-device tactile landmarks outperform the base case. Hudin et al. [48] present a
system that renders independent tactile stimuli to multiple ﬁngers without tracking their positions on a transparent surface. The
tactile rendering approach is based on wave time-reversal focusing, which “enables the spatial and temporal mechanical waves
rendering using multiple transducers to create an impulse response”. The system can provide multiple focus simultaneously
and therefore multitouch tactile simulation.

Finger wearable devices have been relatively well studied in several works, as they provide a small device design and uses
the user’s ﬁngertip to transmit the haptic data accurately. In [49], the authors design a 2DOF force feedback ﬁngertip device,
(sub-ﬁgure 2b). The device can represent contact conditions with a realistic directional force vector feedback. Jang et al. [50]
explore the feedback possibilities for haptic cutaneous ﬁngertip devices. Pseudo-haptic feedbacks relies on “the visual feedback
and its dominance over the tactile to present the haptic sensations”. Through experimental analysis the authors demonstrate
that pseudo-haptics can render virtual stiffness by modulating visual cues. Pseudo-haptics can expand the range of perceived
virtual stiffness, and if there is contradictory information between visual and haptic cues the results can be confusing for the
user. Chinello et al. [31] present a novel 3DOF wearable cutaneous device for the ﬁngertip (Figure 2a). In comparison with
other devices, the proposed in this paper enables 3DOF due to its three-leg articulated design. This design will be followed in
other papers and studies as it offers good surface rendering performance in a small package. Pacchierotti et al. [51] present an
innovative approach to remote cutaneous interaction methods. This approach works with any haptic ﬁngertip device. The one
proposed algorithm maps the remote sensed data to the motor or actuators inside the haptic device. The model can sense, and
remotely render the haptic sensations in 3DOF scenarios. For the analysis of the model, the authors collect recorded interactions
with a sensor (BioTac10) to sense the haptic interactions and also the cutaneous feedback. BioTac mimics the sensory capabilities
of a human ﬁnger. However, the algorithm has some limitations such as the vibration perception against user’s applied forces,
as it only renders the remote sensed data, not the current situation on the user’s side. HapThimble [32] is a wearable devices
which provides vibrotactile, “pseudo-force ﬁnger sensing to mimic the physical buttons based on force-penetration for virtual
screens”. HapThimble can display haptic feedback for mid-air interaction with virtual touchscreen devices. Haptic Thimble [33]
offers tactile ﬁngertip cues in a 3DOF wearable ﬁnger device. The ﬁngertip voicecoil actuator can rotate around a user’s ﬁnger
to provide a more accurate surface rendering. eShiver is a haptic force feedback device which renders shear forces on the
ﬁngertip. eShiver operation is very similar to ShiverPAD [52], both uses a type of electroadhesion as a friction switch. The
authors use an artiﬁcial ﬁnger to measure the shear forces in a big device setup. The simulation of friction forces in AR/MAR
scenarios can provide a better understanding of the virtual object structures. However, the wearability of the proposed device
is lacking. The haptic feedback not only needs to focus on the stimuli, but in the correct measure of force for different actions,
and hence be able to provide accurate estimations and haptic responses. In [53], the authors propose a novel method to estimate
ﬁngertip tactile force in actions involving grasping objects. They use a custom sensing glove to estimate the contact force on
the ﬁngertip. The approach to estimating the contact force relies on magnetic, angular rate, and gravity sensors. These sensors

9https://www.vive.com/us/accessory/controller/
10https://www.syntouchinc.com/sensor-technology/

7

(a) 3-RSS [34].

(b) LinkTouch [49].

(c) hRing [41].

Fig. 2: Wearable ﬁngertip haptic devices.

are located along the user’ phalanges. To study the performance of the estimation method, the authors simulate the forces are
rendered them with an Omega 3 device as ground truth.

Skin displacement and stretch stimuli are another important source of cutaneous haptic devices. Furthermore, the location of
these device, sometimes on the user’s wrist, unlocks the ﬁngertip mobility limitations of the ﬁnger-wearable devices. Therefore,
depending on the stimuli to be rendered and MAR application scenario, these approaches can offer a better design. Chinello et
al. [34] present a novel cutaneous device capable of rendering skin stretch stimuli, sub-ﬁgure 1a. The device consists of four
cylindrical rotating end-effectors that enable four movements on the user’s wrist/arm: clockwise rotation, counter-clockwise
rotation, vertical motion, and horizontal motion. The experiment demonstrates that providing skin stretch feedback beneﬁts task
completion times and errors. Furthermore, the participants ﬁnd the device very useful for navigation cues. This proposed device
features different skin sensations without reducing user mobility as in case of ﬁngertip devices. In [35], the authors design an
asymmetric ungrounded vibration device to simulate pulling sensations through asymmetric skin displacement, sub-ﬁgure 1b.
There have been previous studies that study asymmetric vibration to provide ungrounded pulling sensations. However, they
have not studied the perceptual mechanism behind. The experiments show that the delay between the actuators can considerably
affect the skin displacement perception. There are many studies on vibrotactile actuators, but this paper offers another skin
stimuli using voicecoil actuators that can be useful in scenarios such as sliders UI, and surface texture simulations. Pacchierotti
et al [41] claim the lack of haptic feedback approaches for wearable devices (i.e., vibrotactile sensations). This paper presents
a novel wearable haptic device which consists of two servo motors and a fabric belt in contact with the user’s ﬁnger skin,
Figure 1c. The presented 2DOF cutaneous device provides normal and sheer stimuli to the proximal phalanx of the user’s
ﬁnger. Besides, the placement of the device helps to free the user’s ﬁngertip, and the ability to interact with other hand-tracking
devices such as Leap Motion11. Bianchi et al. [54] present a fabric-based wearable tactile display stretching the state of a
fabric that covers user’s ﬁnger. The device is placed over the user’s ﬁnger and attached with an elastic clip. The device mimics
different stiffness levels (i.e., passive mode) and softness (i.e., active mode).

In [55], the authors evaluate two wearable cutaneous devices for the ﬁngers in three different AR tasks such as hand writing,
picking and moving a virtual object, and ball and cardboard (i.e., balance a virtual ball on a piece of cardboard). The ﬁrst
wearable device (i.e., “3-RRS ﬁngertip”), presented by Chinello et al [34] uses servo motors to move a rigid platform under the
ﬁngertip. The second device, presented by Pacchierotti et al. [51] is based on a fabric belt that is controlled by two servomotors,
which can stimuli friction stimuli (i.e., ﬁnger sliding over a surface) and skin stretch (i.e., both motors spin opposite directions),
Figure 2. The experiment setup uses an Oculus Rift HMD, camera mounted on the Oculus, and visual markers to create the
AR scenario. In all of the experiments, there is a vibration feedback added to the tactile feedback provided by both devices.
In the ﬁrst experiment (written on a virtual board) no signiﬁcant difference is seen between device performance. In the second
one, grab and move virtual object, there is also no difference between wither devices. In the third experiment, balance virtual
ball on cardboard, the 3-RSS device outperform the hRing. However, the hRing, due to its construction is preferred by the
users as it leaves the ﬁngerprints free for touch interactions in the physical world.

In [37], the authors propose an innovative wearable tactile stimulation feedback device through brushing. The device consists
of multiple wrist-worn haptic interfaces that brush the user’s skin instead of conventional vibrotactile wristband devices. The
proposed device requires a greater degree of calibration in comparison with vibrotactile devices. Their experimental study
shows that certain cues using brushing are better recognized than vibration.

Rekik et al. [56] study the importance of surface haptic techniques to render real textures. They focus on two major
approaches: Surface Haptic Object (SHO), based on the ﬁnger position; and Surface Haptic Texture (SHT), based on ﬁnger
velocity. Then, they propose a new rendering technique called Localized Haptic Texture (LHT), based on the elementary
tactile information that is rendered on the display, taxel. The device to render the texture’s friction is a tactile tablet which

11https://www.leapmotion.com/

8

TABLE IV: Most distinctive active surfaces haptic devices

Type

Device

Author/Reference

Characteristics

Pin Array

Pin Array
Smartphone pin array

Velazquez et al. [61]
Jang et al. [62]

Portable pin array device to render surfaces
Pin array to deploy on smartphone sides

Multicell Array Multi array cell

Stanley et al. [63]

Flat deformable silicone arraycell

Finger-based

Normal-/TextureTouch

Benko et al. [64]

Fingertip device to render surfaces and textures

(a) FEELEX [59].

(b) Pin array [60].

(c) SMA device [61].

Fig. 3: Wearable ﬁngertip haptic devices.

uses ultrasonic vibration to regulate the friction between the user’s ﬁnger and the touchscreen. The extensive experiments
demonstrate that LHT improves the quality of the tactile rendering over SHO and SHT.

Doucette et al [57] study group interaction on digital tabletops when there is tactile feedback to people crossing their arms
so as to avoid touching other people’s arms. People are more aware of the management of shared public space because they try
to avoid touching other people’s arms. Using external devices on tabletops reduce the touching other people’s arm awareness,
in this paper they study the addition of vibrotactile feedback to these devices to simulate the previous scenario of avoiding
touching arms. Results demonstrate that the tactile feedback improve awareness and hence the management of tabletops on
public spaces. These feedback additions can be useful in MAR/AR collaborative environments. In [58], authors study the
communication of emotions through ultrasonic haptic feedback. The tactile sense can relate to emotions such as “anger, joy,
fear, or love”. The experiment insights show that mid-air emotion communication is not arbitrary and the participants can
express and recognize different tactile stimulations and associate them to a particular emotion.

B. Active surfaces

Active surfaces feature the best performance for rendering surfaces, with great resolution and accuracy. However, many
of the aforementioned devices lack portability due to the haptic actuators design (i.e., vacuum air-based, big sized pin array
actuators).

Iwata et al. [59] present FEELEX, a novel active surface haptic device that consists of an array of actuators, see sub-ﬁgure 3a.
The proposed device uses a matrix of linear actuators to interact with a user’s palms, there is also a projector to display the
GUI on the deformable screen. Although the device implementation is difﬁcult, there are several advantages as the device
provides a natural interaction with user’s bare hands. Leithinger et al [60] follow a similar approach for their table top active
surface design. The device consists of an array of 120 motorized pin actuators, sub-ﬁgure 3b. Each of the pins reacts to user
input such as pulling and pushing. The authors use electric slide potentiometers as linear actuators due to their precise and
fast actuation features. One of the examples of this prototype is a geospatial exploration application, whose active surface can
render the landscape texture.

The previous devices although they are the ﬁrst array of actuators prototypes, their size and weight make them unusable
in mobile environments. Velazquez et al. [61] presents a new low-cost, compact and light weight portable tactile display. The
device consists of an array of 8 × 8 pins based on shape memory alloy (SMA). The shape memory alloy materials are capable
of recovering a predetermined shape upon heating. The device use this property on a spring to create a linear actuator, see
sub-ﬁgure 3c. However, owing to its nature, this material does not offer accurate control and the frequency response is affected.
To heat and cool down the SMA springs the device uses electric input and a fan respectively. This wearable approach [61]
can be used to render surface and texture information of virtual objects on MAR applications.

Stanley et al. [63] [65] design a ﬂat deformable layer of silicone cells regulated by air, see Figure 4. When the air is
suctioned out of the individual cells the surface changes its structure. The main difference with other active surfaces lays on

9

(a) Particle jamming structure [63].

(b) Particle jamming active surface [65].

Fig. 4: Particle jamming active surface.

(a) NormalTouch.

(b) TextureTouch.

Fig. 5: Wearable ﬁngertip active surface devices [64].

the ﬂexibility and softness of the device’s surface nature, which can be very useful in medicinal training scenarios (i.e., organ
rendering), or fast model prototyping.

In [64], Benko et al. propose two devices for texture rendering. The ﬁrst device (NormalTouch) consists of a handhold
controller and active tiltable and extrudable platform similar to the cutaneous devices. The other proposed device (TextureTouch),
is a hand-held controller which renders 3D surfaces using a 4 × 4 array of actuate pins, see Figure 5. NormalToch renders
the surface of the virtual object, it tilts its platform to the relative 3D (i.e., virtual object, surface) orientation and extrudes its
platform according to user’s controller movement. TextureTouch works similar to the previous device. Although, it can display
ﬁne-grained surface structures. Both devices are tracked and the hand is rendered in the virtual environment. The experiment
results show that both devices provide good accuracy and low error in the tracing paths and ﬁdelity assessment tasks. The
proposed devices are ﬁnger-based which imply wearability and make them another useful surface/texture of virtual objects
rendering haptic devices for the MAR ecosystem.

Jang et al. [62] propose a novel haptic edged display based on a linear touch pin array around a smartphone screen. The
device not only provides an innovative haptic interaction, but can be used as a haptic notiﬁcation method (i.e., changing the pin
array placement). This approach can be used as feasible haptic feedback implementation for MAR applications on smartphones.
Meyer at al. [66] present a technique to render textures on a tactile display. The authors measure a series of ﬁngertip swipe
movements across different textures and store the data as spatial friction maps. The method does not measure the velocity as its
model is designed to render the texture in the spatial dimension. Due to the randomness inherent in swipes of a single texture,
the authors parametrize the textures using three different distributions: Rayleigh, Rice, and Weibull. Where Weibull emerges as
the most suitable distribution for ﬁtting and categorization, the proposed method parametrizes the stochastic friction patterns
in a 202-parameter model. The texture rendering is another important aspect that active surface designs need to be considered
to provide a realistic UX.

In [67], authors proposed PinPad, a pin array device capable of fast and high-resolution output using a 40 × 25 array of
actuated pins, Figure 6. PinPad offers better spatial and temporal resolution in comparison to the state-of-art pin array devices.
The experiment results show that the tactile feedback provided by PinPad enables high-resolution stimuli on the ﬁngers.
Although the prototype has some glitches in the pin movement design (i.e., stuck pin) and the noise of the device that need
to be ﬁxed in future versions. Overall, PinPad can be seen as a state-of-art pin array devices due to their high resolution and
real-time speed. These pin-pad devices are used for Braille reading devices. Besides, their portability, resolution and accuracy
can be included as texture/surface render devices for the MAR applications.

10

Fig. 6: Pinpad, pin array device [67].

C. Mid-air devices

The main limitation of the previous devices corresponds to user’s freewill movement that some devices apply. For example,
ﬁnger-based devices constrain the use of the ﬁngertip for other interactions such as touching a smartphone display, or as in
MAR scenarios can hinder ﬁnger tracking accuracy. Mid-air devices appear as a good solution for touchless interactions, but
the size and weight of many current devices do not make them a good wearable solution for MAR applications.

Ultrasonic/ultrasound haptic devices are the most distinguishable mid-air devices and they have been studied in many related
works. Moreover, due to the actuators size it is possible to wear some of these mid-air devices on the user’s body. Therefore,
they are a good candidate if we want to provide touchless interactions for MAR applications. Hoshi et al. [68] present an
innovative hologram device with tactile sensations provided by the mid-air ultrasonic array of devices (i.e., similar devices to
the commercial Ultrahaptics, sub-ﬁgure 7a). They use a mid-air display to project a ﬂoating image, a four ultrasound transducer
array for the tactile display and a vision-based hand tracker system to track the user’s hand. The experiment results show that
the device does not provide great range and the participants have to be close to the tactile display system (i.e., transducer array).
Future work includes the 3D focal point to enable tactile surface sensations moving the focal point on the user’s hand. This
is one of the ﬁrst work with mid-air interfaces and virtual object interaction using ultrasound devices. UltraHaptics [69] is a
well known and innovative mid-air ultrasonic haptic feedback. It provides multi-point haptic feedback on the user’s skin. With
a speciﬁc phase, amplitude, and frequency conﬁguration the device can render different focal points and generates surfaces on
the user’s skin. This device will push forward many other related works on mid-air surfaces, and will stand up as one of the
main mid-air haptic devices. HaptoMime [70] enables interaction with ﬂoating images using ultrasonic tactile feedback. The
device redirects the acoustic ultrasonic radiation by reﬂection, and hence, the ultrasonic device’s beam does not interact directly
with the user’s skin (i.e., hand and/or ﬁnger). The device consists of a IR touch sensor frame, ultrasonic haptics, an LCD,
and the Aerial Imaging Plate (AIP). The experiment results show that the tactile feedback improves user’s input as it provides
cues to guide the motion in mid-air (i.e., mid-air virtual keyboard). Yoshino et al. [71] propose a contactless touch interface
with blind support using tactile interaction. The authors use a visual projector and airborne ultrasonic phased array for the
tactile feedback. The audio-tactile cues help the visually impaired interact with the graphical user interface (GUI). Althought
the device is not exactly wearable, the rapid advances in screen projection and ultrasonic arrays can provide future holograms
or help the visually impaired to interact with AR content. SkinHaptics [72] is a wearable ultrasound hand-focused device. The
device consists in an ultrasound array that is attached to the user’s hand to provide a tactile feedback in and through the hand.
The experiment setup comprises a three by four ultrasound array matrix to simulate a numpad. One of the limitations of this
feedback technique is the sensations perceived by participants on the skin and deeper inside the hand that can spread from
the focus point. Makino et al. [73] propose HaptoClone, a new interactive system that enables real-time physical interactions
with haptic feedbacks using ultrasonic acoustic ﬁelds and volumetric images (i.e., images displayed using micro-mirror array
plates). The prototype provides limited feedback force control, and no collision detection of the occluded sides.

Air-based mid-air interfaces push air to the user’s hand to render the haptic feedback sensation. Usually, these interfaces
render virtual surfaces roughly as they lack good resolution and accuracy. Sodhi et al. [74] present a novel device for mid-air
tactile interaction based on air-jet approaches. Their device uses compressed air pressure to stimulate user’s skin and simulate
the touch sensation, sub-ﬁgure 7b. The device is able to track the user’s hand (3D depth-camera) and actuate accurately on
the user’s skin. The device provides long distance range and easy implementation and deployment. However, due to the vortex
nature of the feedback it can not provide high resolution tactile sensations. VacuumTouc [75] consists of a touch screen surface
which sucks the air between its surface and the area where the user’s ﬁnger makes contact. The authors propose several
designs such as “suction button’, “suction slider”, and “suction dial” that can be implemented. This paper introduces a novel
haptic interface based on attractive force sensation using previously sucked air on the user’s ﬁnger. In [76], the authors discuss

11

TABLE V: Most distinctive mid-air haptic devices

Type

Device

Author/Reference

Characteristics

Ultrasound

SkinHaptics
Ultrahaptics

Spelmezan et al. [72]
Carter et al. [69]

Ultrasound hand haptic device
Ultrasound haptic device

Air-jet

Aireal

Sodhi et al. [74]

Vortex-based

Laser-based

LaserStroke

Lee et al. [77]
Ochiai et al. [78]

Laser device to render surface on user’s palm
Laser+Ultrasound device for better
accuracy and haptic perception

Other

Electric

Spelmezan et al. [79]

Electric arcs for the haptic feedback

(a) Ultrahaptics, ultrasound haptic [69].

(b) Aireal vortex haptic device [74].

(c) Electric arc array device [79].

Fig. 7: Wearable mid-air haptic devices.

the current work on mid-air haptic feedback. Two tactile feedback methods are described in the paper: air-jet, and acoustic
radiation pressure. The former uses either direct compressed air methods and vortex-based methods [74] to simulate the tactile
sensation. The latter produces tactile sensation using ultrasound [69]. The advantages of air-jet against ultrasound are its easy
implementation and coverage. However, air-jet implementations have several disadvantages such as size, low spatial resolution
(i.e., big focal point) and slower transfer. Both methods offer advantages but not a complete solution to interact freely with
AR applications.

Laser approaches are noted owing to their accuracy and precision of the deployed systems. They are usually combined with
other mid-air solutions such as ultrasound to provide the best mid-air approach. However, due to the nature of laser devices,
they are dangerous to use in mobile environments, and the presented work illustrates the combination of several mid-air devices.
LaserStroke [77] stimulates the user’s tactile sense using a laser that irradiates to the user’s palm, which is covered with elastic
material (latex glove). The authors demonstrate the capabilities and usefulness of laser as a tactile stimulator. The thermal
changes on the user’s palm provide a sequence of moving tactile stimulations. LaserStroke is an interactive mid-air system
that tracks the user’s hand with Leap Motion and irradiates laser beams on user’s palm to provide tactile stimulations. [78]
presents a new method of rendering aerial “haptic images” combining femtosecond-laser light ﬁelds and ultrasonic acoustic
actuators. The former provides an accurate tactile perception. The latter produces a continuous and less ﬁne-grained contact
between the laser tactile perceptions. The novel combination of both mid-air ﬁelds offers the advantages of both approaches
and hence, a better performance than ultrasonic or laser stimulation separately.

Spelmezan et al. [79] demonstrate a novel mid-air method using touchable electric arcs on ﬁnger hover, sub-ﬁgure 7c. To
stimulate ﬁnger sensing, the device uses high-voltage arcs (safe to touch) that discharge when the ﬁnger is near the device’s
surface. The authors consider also the dangers and the security measures that the prototype should satisfy. The proposed device
can be extended to multiple keys in the future.

Mid-air haptic devices feature the main advantage of non-covering the user’s skin. Therefore, they enable many possibilities

for mobility, free movement and touch experiences in the real world.

IV. KINESTHETIC FEEDBACK

In this Section we describe the different kinesthetic haptic approaches we can consider during the design and development
of MAR applications. Kinesthetic devices display forces or motions through a tool which is usually grounded, Manipulandum
devices are not usually portable enough to consider in these scenarios. However, grasping haptic devices and exoskeletons (i.e.,
haptic gloves) include some wearable devices that can be used in the MAR ecosystem.

12

TABLE VI: Most distinctive manipulandum haptic devices

Type

Device

Author/Reference

Characteristics

Grounded

PHANToM Massiel et al. [81]
Omega
Haplet

Manipulandum haptic device
Omega Force Dimension12 Manipulandum haptic device
Gallacher et al. [82]

Portable manipulandum

(a) PHANToM [81].

(b) Omegaa.

ahttp://www.forcedimension.com/products/

omega-7/overview

Fig. 8: manipulandum (kinesthetic) haptic devices.

(c) Robot arm [85].

A. Manipulandum

Manipulandum are haptic devices that render the virtual forces using grounded systems with different DOFs, depending on

the characteristics of the device and/or the feedback to render.

In [80], the authors introduce a real-time AR system SmartTool). The proposed system uses real-time sensor and haptic
feedback to provide a real-time experience in comparison with other state-of-art devices at that time (2002). They use a
similar device design to the PHANToM ( [81]) with 6DOF, sub-ﬁgure 8a. For real-time sensors, they have different approaches
depending on the scenario such as optical sensors (i.e., cutting tissue with a scalpel), electric conductivity sensor (i.e., different
ﬂuids sensing). The use of different sensor to provide better haptic feedback is interesting in the ﬁeld of AR/MAR, as most
of smartphones include multiple sensors.

In [83] present a collaborative “visuo-haptic AR ping-pong game”. The system uses satellite landmarks and a hybrid tracking
(IR marker based) to track the PHANToM device, which provides the haptic kinesthetic feedback. Remote collaborative
scenarios, where we can share/interact with the visual-haptic feedback is still in its ﬁrst deployment steps. However, it can
introduce novel sharing experiences (i.e., collaborative applications with haptic feedback) in the MAR ecosystem.

In [84], the authors propose a visio-haptic AR setup for medical simulations. They describe in detail the calibration process
and their hybrid tracking techniques to provide a precise and accurate simulation environment, which is important in medical
applications. For the haptic, the system uses a PHANToM device, and satellite landmarks for the tracking techniques. The
system also detects occlusions of the aforementioned landmarks. The paper presents the calibration methods, hybrid tracking,
synchronization and the distributed computational framework (graphics and physics distributed) to achieve less than 100 ms
delay. The aforementioned delay and accuracy in AR applications is primordial to offer a good interaction between users and
virtual objects.

Aleotti et al. [85] investigate the potential of visuo-haptic AR in programming manipulation tasks by demonstration scenarios.
The system overlays a virtual environment over the physical and provides forced feedback using a 3DOF grounded device (i.e.,
Comau SMART SiX, similar to Omega devices), sub-ﬁgure 8c. The system allows the user to select and manipulate virtual
overlay objects. The authors also study the possibilities of the AR system in programming manipulation tasks by demonstration.
The system tracks and recognizes objects in a 3D environment and the haptic interaction is achieved using a virtual proxy
which interacts with virtual objects. The system is programmed to recognized simple user’s actions, the position of the objects
and its manipulation. Therefore, the AR haptic environment provides not only the actions but the force the robot arm has to
apply to the object during its manipulation. Haplet [82] is an open-source portable haptic device which features visual, force
and tactile feedback. The device has a robotic transparent arm with a vibroactuator at the tip, sub-ﬁgure 13b. The device can
be attached to any laptop, and smartphone display. The device is also limited in its DOF, but is an affordable device that
provides good feedback for rendering textures.

The nature of manipulandum (i.e., grounded device) makes the wearability of these devices difﬁcult and less feasible than

other approaches for MAR applications.

13

TABLE VII: Most distinctive manipulandum haptic devices

Type

Device

Author/Reference

Characteristics

Grounded

Grasp+PHANToM Najdovski et al. [87]

Grounded grasping device

Finger-based

Wolverine
3D Grasping
Tangencial force

Choi et al. [88]
Handa et al. [89]
Minamizawa et al. [86]

Finger object grasping device
3D grasping device for 3 ﬁngertips
Finger belt, skin stretch

(a) Pinch grasp + manipulandum device [87].

Fig. 9: Grasping devices.

(b) Hand Ons ﬂow diagram [90].

B. Grasp devices

A grasp action in the virtual world enables users interactions between user’s hands and virtual objects. Then, users can
push, pull, and move virtual objects as it is done in the physical world. For example, holding a glass requires a haptic device
to render force or vibration on the ﬁngertips. Furthermore, the gravity forces while holding an object can be displayed using
these grasping haptic devices.

Minamizawa et al. [86] propose a device to present virtual object weights. The device consists of two motors that move a
belt which surrounds the user’s ﬁngertip. When the two motors spin in the same direction, the belt applies a tangential force
over the user’s ﬁngertip, similar to sub-ﬁgure 2c. According to the motors’ spin, the user can perceive the weight of virtual
objects (i.e., two devices for each index and thumb ﬁngers) while grasping.

In [87], the authors present an optimal conﬁguration for a haptic device to enable pinch and grasp interactions (sub-ﬁgure 9a).
The proposed device features bidirectional force on user’s ﬁngers and rotational force on the user’s wrist. The pinching-grasping
prototype is attached to the kinematics and force of a commercial device, Sensable Phantom Ommni13, which features 3DOF.
The proposed device enables haptic feedback on the user’s thumb and index ﬁngers. The device’s actuation is generated by a
cable drive system, whose actuators are placed remotely. The system generates force on the user’s ﬁnger rotation and push to
render the pinch-grasp interaction. The addition of multiple feedback devices to achieve a deeper haptic experience is commonly
used in grounded devices, and it is an approach that needs to be considered to enable more complex haptic feedback.

Hand Ons [90] is a real-time adaptive animation interface for animating contact and precision manipulations of virtual
objects with haptic feedback. The system provides contact and force information of virtual objects. Haptic feedback enhances
the control and interaction with virtual objects. sub-ﬁgure 9b illustrates the system steps to render a full hand pose in the
virtual world with its corresponding haptic feedback. One of the limitations with the proposed system is the animation and
haptic feedback changes. It is the designers/developers task to provide the corresponding cues for each scenario (i.e., object
and manipulation such as grasping/holding). The authors introduce an abstraction layer the ”abstract grasper” to offer more
ﬂexibility for hand animation. The hand animation is very important for providing a realistic feedback to users, the so-called
visio-haptic feedback, which enhances the overall user experience in VR/AR scenarios.

Wolverine [88] is a mobile wearable haptic device designed for grasping rigid virtual objects. The authors created a light
and low cost device, which renders force between the user’s thumb and the three other ﬁngers, sub-ﬁgure 10a. The device uses
low-power brake-based locking sliders to simulate the force feedback. This force mechanism renders the virtual object shape
on user’s hand. When users open their hand the device unlocks the braking mechanisms. In comparison with other devices
such as gloves, Wolverine offers a compact design, and is an energy efﬁcient and low cost device.

13http://www.geomagic.com/archives/phantom-omni/speciﬁcations/

14

(a) Wolverine [88], grasping display of virtual objects.

(b) 3D grasping device [89], linear actuators on each of
the three ﬁngers.

Fig. 10: Grasping devices.

TABLE VIII: Most distinctive exoskeleton haptic devices

Type

Device

Author/Reference

Characteristics

Gloves

Rutgers Master II
Smart glove
Jointless glove

Boutzi [91]
Nam et al. [92]
In et al. [93]

Glove mechanical haptic device
Fluid-based (MR) actuators
Wire-based glove

Handa et al. [89] developed a haptic display which conveys virtual objects’ shapes, hardness, and textures. The proposed
multi-ﬁngered haptic 3D shape display simulates grasping actions using three ﬁngers, sub-ﬁgure 10b. The device uses external
linear actuators to activate the nine spheres within the device. This paper provides an innovative device to render surfaces and
simulate the user’s grasping actions.

C. Exoskeleton devices

Exoskeleton devices are worn by a user (grounded on the body), and hence designers must decide on the importance of
user’s mobility while using the device and the nature of the feedback to be rendered. These haptic devices provide force on
natural DOF of the body. Depending on the device feedback the size, weight and complexity could vary. For example: with
grasping interfaces, haptic gloves may be sufﬁcient; in other scenarios such as walking/running a device it may be necessary
to attach a device to the legs.

Haptic gloves are seen as a feasible and light weight approach among exoskeleton wearable devices. They provide new
force feedbacks by allowing users to pick up, grab, and feel virtual objects in a natural way. In the last decade companies
have developed haptic glove devices such as Cybergrasp14, and Rutgers Master II [91]. One of the issues with these devices
is related to the stability of haptic feedback and safety, as many of them use pneumatic/hydraulic actuators. The slow reaction
time of these mechanical/ﬂuid feedback can also hinder the overall UX. Due to the high DOF number of human hand, haptic
gloves devices can focus on particular interaction approaches such as grasping, touching, pulling. In order to simplify the device
design, and provide better haptic feedback, designers need to consider the haptic feedback scenarios. The actuator placement
can also restrict the user’s hand movement. Besides, the device design needs to be considered for mobile environments such as
streets, and shops. Furthermore, the haptic interface complexity can difﬁcult the design and implementation of the actuators,
and their size and weight. In [94], the authors present a virtual glove to recreate the “Simple Test for Evaluation Hand Function”
(STEF). In comparison with other grounded devices such as manipulandums, this prototype is cheaper, and simpler to setup.
The rendered force is generated by electromagnetic brakes, and the force is transmitted to the ﬁngers by wire-pulley system,
see sub-ﬁgure 11a. This passive force display glove system aims to help during the rehabilitation of stroke patients. Smart
glove [92] is a glove which renders haptic feedback on user’s ﬁngertips. The device uses ﬂuid-based (magneto-rheological)
actuators and a ﬂexible link mechanisms to transmit force to the user’s ﬁngertips, see sub-ﬁgure 11b. The MR ﬂuid actuator
mechanism activates when a magnetic ﬁeld, by an external current, is applied. These electric activation interfaces enable faster
response times. Once the the MR ﬂuid actuator triggers, it moves the ﬂexible link mechanism. The device provides a better
reaction time for the haptic feedback than other pneumatic/hydraulic actuators, due to the use of MR ﬂuids. Blake et al. [95]
continue the MR ﬂuid actuator approach to develop a compact haptic glove device (sub-ﬁgure 11c). However, the device does

14http://www.cyberglovesystems.com/cybergrasp/

15

(a) VR-STEF glove.

(b) Step diagram.

(c) Step diagram.

(d) jointless glove.

Fig. 11: Grasping devices.

(a) EMS.

(b) Sensory subtraction.

Fig. 12: Grasping devices.

(c) Affordance++.

not use a valve-type design, instead the authors opted for a disc-type design due to its portability. The device has a response
time from 67 to 100 ms and a brake system (cylinder-design) 399 to 821 N·mm torque. In [93], the authors propose a jointless
structure in a reduced sized for hand exoskeleton. With a jointless design the device avoids conventional pin joint structures
issues and enables actuator’s compactness. The device uses two wires as tendons to simulate extension and ﬂexion, each pair
of wires are embedded inside a glove (sub-ﬁgure 11d). The device aims to help stroke patients with their rehabilitation.

Exoskeletons in combination with other actuators such as vibration can provide haptic feedback for complex interactions.
For example: grasping, holding, pushing-moving, and touching. One of the main uses of this wearable device is the reduced
mobility of the user’s hand for some of the prototypes mentioned, and also the hand touch isolation of the hand which is
wearing the glove.

D. Other kinesthetic approaches

There are other kinesthetic approaches that use electro muscle stimulation (EMS) to display force feedback. Although, the
portability and autonomy of these electronic devices are demonstrated, the rendered force lacks of continuity and can be
violent in some situations. MAR application design needs to consider the features and also the constrains of EMS devices
while designing an AR ecosystem.

In [96], the authors introduced an EMS-based feedback for mobile devices. Due to vibrotactile motor physical characteristics,
the proposed system is smaller and energy efﬁcient, sub-ﬁgure 12a. The authors designed a mobile game experiment to
demonstrate the improved interaction with an interactive video game using this force feedback.

Pfeiffer et al [97] compare EMS and vibration haptic feedback devices performance in free-hand interaction scenarios. They
conducted several experiments to investigate the user’s perception intensity for both approaches and the haptic feedback desing
(i.e., vibration, EMS) to best reﬂect the hand-gesture. The ﬁrst experiment shows that participants can differentiate in most of
the cases (i.e., 80% vibration, 90% EMS) between two levels of intensity. The second study focus on three common gestures
such as grabbing, touching, and punching and object. The results in the last experiment show that participants feel better the
EMS feedback. The overall results indicate that participants feel not only better with EMS but it provides a more realistic
feeling than the vibration feedback. Besides, the authors mention the privacy considerations with vibration devices, as they can
be detected by others around (i.e., noise and movement), in opposition with to EMS.

In [98], the authors present a hybrid force-cutaneous feedback approach for a robot-assisted surgery. The setup consists of
a bimanual 7DOF system (i.e., provided by Omega 715) and a ﬁngerprint skin deformation device to provide tactile feedback
[99], [21]), see sub-ﬁgure 12b. Furthermore, the paper presents a technique called sensory subtraction
(cutaneous devices,
to substitute haptic force with cutaneous stimuli. Therefore, the device can emulate haptic force on the user’s skin with
both kinesthetic and cutaneous devices. The proposed device aims to outperform other sensory approaches such as substitution

15http://www.forcedimension.com/products/omega-7/overview

16

(a) Shape changing.

Fig. 13: Other wearable haptic devices.

(b) Haplet.

(c) Mouse haptic.

techniques (i.e., substituting kinesthetic forces for other haptic feedbacks). This paper demonstrates the beneﬁts of hybrid haptic
feedback approaches such as kinesthetic plus cutaneous (i.e., subtraction of haptic feedbacks instead of feedback replacement).
Impacto [100] uses a wearable device to stimulate the physical impact in VR environments. It combines tactile and EMS
devices to display haptic sensations of hitting and being hit. Lopes et al. [101] introduce a novel concept to enhance dynamic
object use. The authors propose affordance++, an EMS device to enable object-user dynamic communication (sub-ﬁgure 12c).
Affordance++ allows the users actuate with virtual objects, and show how the movement should be. Although EMS can be
sharp and strong for mobile scenarios, the concept idea of object-behavior dynamic communication can be very useful in MAR
environments.

In [102], the authors describe the properties of proprioceptive sensations induced by non-grounded haptic devices. They
use a vibration speaker which generates a perceived force that pulls or pushes user arms in a particular direction. This haptic
asymmetric vibration design induces the sensory illusion of pulling and pushing user’s hand. The experiment results show that
changes in the vibroactuator input signal can alternate direction and magnitude force on users.

V. OTHER HAPTIC DEVICES

Lee at al. [103] propose a graspable and touchable interface based on 3D foam for AR scenarios. They use a 3D foam as
a passive object that is tangible, traceable and rendered in an AR application. Therefore, the user can interact with the object
in the virtual environment with overlaid information and touch/grasp the object in the physical world. The idea of passive and
tangible objects to provide the haptic feedback is commonly used in VR applications. However, the usability in AR scenarios
can be another feature that can improve the overall UX in MAR. For example, the addition of a simple 3D object that is
rendered as another virtual object in AR application, offers not only the grasping/touching sensation but the possibility of
moving a virtual-complex object on the real one.

Previous studies have focused on vibration intensity and duration along with perceived stimulus. Blum at al. [104] propose
the addition of accelerometers data to improve haptic feedback of vibration actuators. For example, if the user is running the
vibration will be more intense as in cases where the user is still. The addition of other surrounding information to provide
better haptics according to the situation will improve the haptic feedback perception.

In [105], the authors introduce a passive haptic retargeting feedback, using dynamically aligning physical and virtual objects
using our vision system (hacking human perception). The paper demonstrates three approaches that use dynamic remapping
and body alignment to reuse passive haptics of same physical objects across multiple virtual objects. Although the paper focus
on VR, passive haptics and dynamic remapping could be useful in the future AR/MAR applications.

Spiers et al. [106] propose a shape-changing haptic interface for navigation systems. The authors compare their device
with the more common vibrotactile devices used in navigation systems. The device consists of a cube shaped object with
an upper half which is able to rotate and slide over its other half to display navigation cues such as forward, turn left, turn
right, backwards. The experiment in an outdoor public environment shows that participants ﬁnd more intuitive and pleasant
the shape-changing device to navigate than the vibrotactile equivalent device, sub-ﬁgure 13a. The addition of wearable shape-
changing devices in the AR/MAR ecosystem (i.e., textiles) enable other ways of stimuli that can be useful in no display vision
situations. Furthermore, vibration feedbacks are sometimes (i.e., walking) difﬁcult to perceive.

Price et al. [107] present a touchscreen-based haptic system which features kinesthetic force feedbacks, sub-ﬁgure 13c. The
device provides static friction to simulate virtual constraints such as boundaries, area-of-effect ﬁelds and paths. The experiments
demonstrate these haptic functions in a virtual maze and walls. The device aims to provide rehabilitation for upper limb stroke
patients, as it engages patients with a haptic feedback. Haptic simulation of contours, boundaries and textures of virtual objects
is an important topic that AR/MAR applications need to integrate in order to provide a full interaction and better UX.

17

(a) Oculus Rifta.

(b) HTC Vivea.

ahttps://www.oculus.com/rift/

ahttps://www.vive.com/de/

(c) Samsung Gear VRa.

ahttp://www.samsung.com/global/galaxy/

gear-vr/

(d) Microsoft Hololensa.

ahttps://www.microsoft.com/en-us/hololens

(e) Google Tango projecta.

(f) Apple ARKita.

ahttps://get.google.com/tango/

ahttps://developer.apple.com/arkit/

Fig. 14: Commercial VR, AR ecosystems.

Israr et el. [108] propose Stereohaptics: “a haptic interaction toolkit for tangible virtual experiences”. They aim to provide

a framework and ultrasonic haptic devices to create, edit, and render rich dynamic haptic using audio-based devices.

Nakamura et al. [109] propose a contact pad that can emulate the sensation of softness. The haptic display provides lateral
force feedback and softness rendering with electroadhesion using contact pads on the screens. The device also improves UX
on displays, but it is limited to pushing and lateral force feedback.

VI. THE VR AND AR ECOSYSTEM

The VR and AR ecosystems has been gaining more importance in recent years with the commercialization of several VR
and AR devices such as Oculus Rift, HTC Vive for gaming, Google Glass, and Microsoft Hololens for AR applications
(Figure14). Furthermore, important software and device companies are opening their frameworks to developers for AR and
MAR applications (i.e., Apple ARKit16).

Although many devices and technologies focus their attention on VR ecosystem, most of these solutions can be extrapolated
to AR scenarios. In case of MAR applications, the compactness, weight design are important due to the mobile nature of the
scenarios. Some devices such as HTC Vive and Oculus Rift provide controllers with haptic feedback (i.e., vibration) to interact
and feel virtual environments. These device were designed for VR scenarios, but their features make them feasible for AR and
MAR applications, due to the compactness and light weight of these devices.

Google Glass started the revolution of MAR devices. However, due to privacy concerns and other issues the Google Glass
project stopped. The new environment of this device, Google Glass 217 spins around enterprises such as medicine, logistics,
and manufacturing, Figure 15. We can realize about the high spec AR devices that the industry provides, but there is still a
gap in the communication between the virtual objects and the users. Haptic feedback provides the best approach to enhance
and improve the other audio-visual feedback channels, and achieve an overall better UX.

VII. CHALLENGES AND FUTURE DIRECTIONS

Pacchierotti et al [110] present a taxonomy of current wearable haptic devices for hands and ﬁngerprints. They also discuss
the future paths and challenges of wearable haptic interfaces. The paper focuses its attention on ﬁnger and hand wearable
devices. The authors describe in detail the current devices and different interaction methods, and wearable haptic feedback
that has been proposed. They also enumerate the requirements and considerations needed to design wearable devices, such as

16https://developer.apple.com/arkit/
17https://x.company/glass/

18

Fig. 15: Google glass X, industry focused.

comfort, impairment (i.e., mobility of the user while using the wearable device), size and weight. The authors estimate that
gaming and VR are the paths to follow as the current and future gaming market will greatly beneﬁt from haptic advances.
Aside from that, robotic teleoperation is another topic which has been studied and will continue to be relevant in the haptic
ﬁeld. To conclude, the authors mention the possibilities for assistive and privacy aware applications. In the latter, wearable
devices will enable new notiﬁcations interfaces that only the receiver will notice.

The network requirements for future MAR application feasibility needs to be tackled. Network requirements such as latency
and transmission errors (e.g., teleoperation robots) are important topics to study. New advances in electronics and future
wireless communications will lead to real-time interactions with our distant environments (e.g., teleoperations), Fettweis [12],
coined the ecosystem of the Tactile Internet. The Tactile Internet presents several challenges for mobile networks and also
Internet’s backbone such as latency and ultra-high reliability [111] [112] [113], Figure 16. The Tactile Internet requires 1 ms
delay to achieve real-time haptic performance in scenarios such as surgery teleoperations. Pilz et al. [114] demonstrate the
implementation of wireless network towards 5G with 1ms delay requirements. In [115], the authors analyze the current video
protocols to show that current technologies are not ready in terms of requirements for the Tactile Internet. Only the delay
from glass-to-glass (i.e., time between video recorded by smartphone camera until is rendered in the smartphone screen) is
considerable bigger (19.18ms) than the expected 1ms for the round-trip delay of the Tactile Internet. Therefore, the accumulated
delay from the mobile network will not satisfy the latency Tactile Internet demands. Popovski [116] analyzes the current
mechanisms to provide Ultra-Reliable Communication (URC) in 5G wireless systems. URC will bring high reliable connectivity
for the next generation applications such as vehicular-to-vehicular communications, Tactile Internet, and sensor networks over
5G cellular networks. However, the high reliability capabilities can affect the stringent latency requirements of the mentioned
services. Besides the number of users contribute to this trade-off latency–high-reliability.

Furthermore, the demanding requirement of 1 ms latency is not only limited by mobile and backbone networks but from
sensing devices. The authors in [117] [118] analyze multimodal integration approaches to aggregate different haptic human
sensing information to the network to achieve the latency requirements and not affect the reliability of the ecosystem. Steinbach
et al [117] study how human perceptions work and how human brain combines the sensory information. The authors aim to
integrate different sensory information without decreasing the percept effectiveness and accuracy. Besides, they study delay
effects in haptic sensing tasks. In [118], the authors study the overall performance for teleoperation tasks according to different
haptic feedbacks.

In order to design haptic feedback devices and its mechanical interfaces, there is an approach that assumes passivity for the
haptic device and stable human interaction with the device. The passivity condition formulated by Colgate and Schenkel [119]
utilizes this fact. The authors in [120] extend this approach to virtual walls (1DOF). The device used for the analysis consists
of discrete-time spring-damper system. They demonstrate why passivity is conservative for haptic systems in terms of stability.
Muller et al. [121] propose a collaborative mixed reality interface. The immerse exploration can help to explore and understand

health related data. Collaborative techniques can enhance of current AR/MAR application usability.

Deadband compression techniques transmit new haptic data only when the stimuli can be perceived by user (based on
Just Noticeable Difference, JND). In [122], the authors extend the work on deadband approaches for cutaneous haptic data.
The proposed device to measure the JND consists of an Omega 3 interface and a custom 3DOF cutaneous device. The
compression technique for cutaneous haptic feedback uses the JND as the perceptual threshold for the compression algorithm.
The result shows an average reduction of around 60%. The beneﬁts of deadband compression algorithms are plausible and the
implementation of the JND threshold algorithm feasible. Difﬁculties can appear in the perception measures, as it is a personal
characteristic that can vary between users.

The design and development of AR/MAR applications need to consider other populations such as the elderly, or visually
impaired to not leave them behind in the forthcoming AR/MAR era. Liang [123] presents the eight elements that need to
be considered in an AR architecture such as user, interaction, device, virtual content, real content, transmission, server and
physical world. The author also describes ﬁve preliminary design principles of AR systems: changeability, synchronization,

19

(a) Revolutions.

Fig. 16: Tactile Internet the next revolution, Figures by [111]

(b) Network challenges.

partial one to one, antecedent and hidden reality. The authors identify an AR pillbox as an example of AR for the aging
population. Bach-y-Rita et al. [124] present one of the ﬁrst works to substitute vision by tactile sensing. Pawluk et al. describe
the relevant issues on designing haptic assistive technologies for the visually impaired. The ﬁrst issue using tactile mechanical
feedback as a substitution for vision is the low space resolution of the tactile sensing. Besides the ﬁeld-of-view for vision is
considerably smaller for touch. Another approach uses eletrotactile displays [79] to simulate a mechanical interaction. However,
it suffers from similar limitations. One well known example of vision substitution using eletroctactile feedback is the tongue-
based feedback device proposed by Bach-y-Rita et al. [125] and Lozano et al. [126]. Furthermore, performance of assistive
technologies differs greatly from laboratories (i.e., testing environments) and the real world. The authors also mention the
effect of affective touch experiences, and their importance in visual substitution approaches. There are several areas of haptic
assistive technology such as Braille devices (i.e., piezolectric pin arrays, virtual Braille displays), tactile rendering techniques
for graphics and mobility devices (i.e., smart canes). One of the main differences in the way we understand information is that
it is not possible to map the visual scene into the tactile sensory skin. The visually impaired population should be considered
not only for assistive technologies but for the AR/MAR ecosystem (i.e., not only visual feedback).

The addition of haptic feedback to the learning process has been studied in several works. In [127], the authors include
kinesthetic feedback in physic simulations to help students understand forces involved in gears mechanical behavior. Okamura et
al. [128], follow a similar approach using a force feedback joystick to teaching dynamic systems. Minogue et al. [129], analyze
the understanding improvements using touch roles in the cognitive-learning process, and efﬁcacy in haptic augmentation.
Another important ﬁeld that can be enhanced by haptic feedback is medical training [130]. Haptic feedback can facilitate
the learning process and also simulation training with more realistic scenarios. In [131], the authors realize a survey about
olfactory feedback, haptic feedbacks and immersive applications applied in teaching and learning; for example, to teach the
abstract concept of the Bohr atomic model. Haptic feedbacks can also provide novel ways of passive learning. For example,
the use of haptic actuators can play a similar role as learning by seeing: as learning by haptic feeling. In [132], the authors
propose a passive haptic learning (PHL) stimulation method to teach piano pieces. The authors use a pair of wearable gloves
with vibroactuators on the back of each ﬁnger. They study the efﬁciency of this PHL approach together with audio and only
vibration feedback. The results show that vibration and vibration + audio techniques are useful for learning and retention, as
they offer better performance than without them (i.e., only audio). These innovative techniques to improve the learning process
are worth studying and can be a new part of development of tools together with MAR/AR applications.

VIII. CONCLUSION

In this survey, we depict the state-of-the-art of several haptic devices and their capabilities as wearables in MAR ecosystem.
Furthermore, we classify the haptic feedback devices by their sensory nature and their design characteristics, such as mid-air,
and exoskeleton. We start with a brief description of the main features of haptic devices and the importance of audio and
visual as non-haptic devices in enhancing the UX and improving the overall interaction performance. We analyze the main
characteristics of the proposed devices, and their applicability as wearables for MAR applications. Although there are many
works and commercial products, an affordable, portable and simple approach for haptic wearable devices is still missing.
Moreover, the ﬁdelity of these devices is limited to one scenario such as surface/texture rendering, grasping, or pushing. The
combination of more haptic devices to achieve better feedback has been done by several authors but the size, or difﬁcult
implementation hinders their deployment in mobile environments, where the scenarios and circumstances surrounding the user
can change. With this work we aim to provide a better understanding of mechanisms, challenges and future possibilities of
haptic feedback in the MAR ﬁeld.

20

REFERENCES

[1] B.-G. Chun, S. Ihm, P. Maniatis, M. Naik, and A. Patti, “Clonecloud: elastic execution between mobile device and cloud,” in Proceedings of the sixth

conference on Computer systems. ACM, 2011, pp. 301–314.

[2] H. T. Dinh, C. Lee, D. Niyato, and P. Wang, “A survey of mobile cloud computing: architecture, applications, and approaches,” Wireless communications

and mobile computing, vol. 13, no. 18, pp. 1587–1611, 2013.

[3] E. Cuervo, A. Balasubramanian, D.-k. Cho, A. Wolman, S. Saroiu, R. Chandra, and P. Bahl, “Maui: making smartphones last longer with code ofﬂoad,”

in Proceedings of the 8th international conference on Mobile systems, applications, and services. ACM, 2010, pp. 49–62.

[4] S. Kosta, A. Aucinas, P. Hui, R. Mortier, and X. Zhang, “Thinkair: Dynamic resource allocation and parallel execution in the cloud for mobile code

ofﬂoading,” in Infocom, 2012 Proceedings IEEE.

IEEE, 2012, pp. 945–953.

[5] S. Bouzefrane, A. F. B. Mostefa, F. Houacine, and H. Cagnon, “Cloudlets authentication in nfc-based mobile computing,” in Mobile Cloud Computing,

Services, and Engineering (MobileCloud), 2014 2nd IEEE International Conference on, April 2014, pp. 267–272.

[6] F. Bonomi, R. Milito, J. Zhu, and S. Addepalli, “Fog computing and its role in the internet of things,” in Proceedings of the ﬁrst edition of the MCC

workshop on Mobile cloud computing. ACM, 2012, pp. 13–16.

[7] T. Braud, F. Hassani Bijarbooneh, D. Chatzopoulos, and P. Hui, “Future networking challenges: the case of mobile augmented reality,” in Proceedings

of the Annual IEEE International Conference on Distributed Computing Systems (ICDCS 2017), Atlanta, GA.

IEEE, 2017.

[8] V. Hayward, O. R. Astley, M. Cruz-Hernandez, D. Grant, and G. Robles-De-La-Torre, “Haptic interfaces and devices,” Sensor Review, vol. 24, no. 1,

pp. 16–29, 2004.

[9] M. Antona and C. Stephanidis, Universal Access in Human-Computer Interaction. Access to the Human Environment and Culture: 9th International
Conference, UAHCI 2015, Held as Part of HCI International 2015, Los Angeles, CA, USA, August 2-7, 2015, Proceedings. Springer, 2015, vol. 9178.
[10] E. Arai, T. Arai, and M. Takano, “Haptics: a key to fast paced interactivity,” in Human Friendly Mechatronics: Selected Papers of the International

Conference on Machine Automation: ICMA 2000: September 27-29, 2000, Osaka, Japan. Elsevier Science, 2001, p. 11.

[11] S. Allin, Y. Matsuoka, and R. Klatzky, “Measuring just noticeable differences for haptic force feedback: implications for rehabilitation,” in Haptic

Interfaces for Virtual Environment and Teleoperator Systems, 2002. HAPTICS 2002. Proceedings. 10th Symposium on.

IEEE, 2002, pp. 299–302.

[12] G. Fettweis and S. Alamouti, “5G: Personal mobile internet beyond what cellular did to telephony,” IEEE Communications Magazine, vol. 52, no. 2,

pp. 140–145, 2014.

[13] Y. Lee, I. Jang, and D. Lee, “Enlarging just noticeable differences of visual-proprioceptive conﬂict in VR using haptic feedback,” IEEE World Haptics

Conference, WHC 2015, pp. 19–24, 2015.

[14] K. NAz and H. Epps, “Relationship between color and emotion: A study of college students,” College Student J, vol. 38, no. 3, p. 396, 2004.
[15] C. K. Madsen, “Emotional response to music.” Psychomusicology: A Journal of Research in Music Cognition, vol. 16, no. 1-2, p. 59, 1997.
[16] Y. Yoo, T. Yoo, J. Kong, and S. Choi, “Emotional responses of tactile icons: Effects of amplitude, frequency, duration, and envelope,” in World Haptics

Conference (WHC), 2015 IEEE.

IEEE, 2015, pp. 235–240.

[17] J. Posner, J. A. Russell, and B. S. Peterson, “The circumplex model of affect: An integrative approach to affective neuroscience, cognitive development,

and psychopathology,” Development and psychopathology, vol. 17, no. 3, pp. 715–734, 2005.

[18] S. Papetti, H. Jarvelainen, and G. M. Schmid, “Vibrotactile sensitivity in active ﬁnger pressing,” IEEE World Haptics Conference, WHC 2015, pp.

457–462, 2015.

[19] M. Azadi and L. A. Jones, “Vibrotactile Actuators : Effect of Load and Body Site on Performance,” pp. 351–356, 2014.
[20] S. W. Keele and M. I. Posner, “Processing of visual feedback in rapid movements.” Journal of experimental psychology, vol. 77, no. 1, p. 155, 1968.
[21] C. Pacchierotti, A. Tirmizi, and D. Prattichizzo, “Improving transparency in teleoperation by means of cutaneous tactile force feedback,” ACM

Transactions on Applied Perception (TAP), vol. 11, no. 1, p. 4, 2014.

[22] N. Navolio, G. Lemaitre, A. Forget, and L. M. Heller, “The egocentric nature of action-sound associations,” Frontiers in psychology, vol. 7, 2016.
[23] A. Chang and C. O’Sullivan, “Audio-haptic feedback in mobile phones,” in CHI’05 extended abstracts on Human factors in computing systems. ACM,

2005, pp. 1264–1267.

[24] M. Akamatsu, I. S. MacKenzie, and T. Hasbroucq, “A comparison of tactile, auditory, and visual feedback in a pointing task using a mouse-type

device,” Ergonomics, vol. 38, no. 4, pp. 816–827, 1995.

[25] D. Chatzopoulos, C. Bermejo, Z. Huang, and P. Hui, “Mobile augmented reality survey: From where we are to where we go,” IEEE Access, 2017.
[26] I. Poupyrev, S. Maruyama, and J. Rekimoto, “TouchEngine : A Tactile Display for Handheld Devices,” Design, no. c, pp. 644–645, 2002. [Online].

Available: http://portal.acm.org/citation.cfm?doid=506443.506525

[27] H. Tinwala and I. S. MacKenzie, “Eyes-free text entry on a touchscreen phone,” TIC-STH’09: 2009 IEEE Toronto International Conference - Science

and Technology for Humanity, pp. 83–88, 2009.

[28] I. S. MacKenzie and S. X. Zhang, “The immediate usability of grafﬁti,” in Graphics Interface, vol. 97, 1997, pp. 129–137.
[29] M. G. Carcedo, S. H. Chua, S. Perrault, R. Wozniak Pawełand Joshi, M. Obaid, M. Fjeld, and S. Zhao, “HaptiColor: Interpolating Color Information
As Haptic Feedback to Assist the Colorblind,” Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems, pp. 3572–3583,
2016. [Online]. Available: http://doi.acm.org/10.1145/2858036.2858220

[30] E. Hoggan, A. Crossan, S. A. Brewster, and T. Kaaresoja, “Audio or tactile feedback,” Proceedings of the 27th international conference on Human

factors in computing systems - CHI 09, p. 2253, 2009. [Online]. Available: http://dl.acm.org/citation.cfm?doid=1518701.1519045

[31] F. Chinello, M. Malvezzi, C. Pacchierotti, and D. Prattichizzo, “Design and development of a 3rrs wearable ﬁngertip cutaneous device,” in Advanced

Intelligent Mechatronics (AIM), 2015 IEEE International Conference on.

IEEE, 2015, pp. 293–298.

[32] H. Kim, M. Kim, and W. Lee, “HapThimble : A Wearable Haptic Device towards Usable Virtual Touch Screen,” Chi ’16, pp. 3694–3705, 2016.
[33] M. Gabardi, M. Solazzi, D. Leonardis, and A. Frisoli, “A new wearable ﬁngertip haptic interface for the rendering of virtual shapes and surface

features,” IEEE Haptics Symposium, HAPTICS, vol. 2016-April, pp. 140–146, 2016.

[34] F. Chinello, C. Pacchierotti, N. G. Tsagarakis, and D. Prattichizzo, “Design of a wearable skin stretch cutaneous device for the upper limb,” IEEE

Haptics Symposium, HAPTICS, vol. 2016-April, pp. 14–20, 2016.

[35] H. Culbertson, J. M. Walker, and A. M. Okamura, “Modeling and design of asymmetric vibrations to induce ungrounded pulling sensation through

asymmetric skin displacement,” IEEE Haptics Symposium, HAPTICS, vol. 2016-April, pp. 27–33, 2016.

[36] S. Horvath, J. Galeotti, B. Wu, R. Klatzky, M. Siegel, and G. Stetten, “FingerSight: Fingertip haptic sensing of the visual environment,” IEEE Journal

of Translational Engineering in Health and Medicine, vol. 2, no. 1, 2014.

[37] E. Strasnick, J. R. Cauchard, and J. A. Landay, “Brushtouch: Exploring an alternative tactile method for wearable haptics,” in Proceedings of the 2017

CHI Conference on Human Factors in Computing Systems. ACM, 2017, pp. 3120–3125.

[38] D. Lee, A. Franchi, H. I. Son, C. Ha, H. H. Bulthoff, and P. R. Giordano, “Semiautonomous haptic teleoperation control architecture of multiple

unmanned aerial vehicles,” IEEE/ASME Transactions on Mechatronics, vol. 18, no. 4, pp. 1334–1345, 2013.

[39] D. S. Alles, “Information transmission by phantom sensations,” IEEE transactions on man-machine systems, vol. 11, no. 1, pp. 85–91, 1970.
[40] J. Seo and S. Choi, “Edge ﬂows: Improving information transmission in mobile devices using two-dimensional vibrotactile ﬂows,” IEEE World Haptics

Conference, WHC 2015, pp. 25–30, 2015.

[41] C. Pacchierotti, G. Salvietti, I. Hussain, L. Meli, and D. Prattichizzo, “The hring: A wearable haptic device to avoid occlusions in hand tracking,” in

Haptics Symposium (HAPTICS), 2016 IEEE.

IEEE, 2016, pp. 134–139.

21

[42] J. Rantala and R. Raisamo, “Preferences for touch gestures in audio-tactile communication,” 2014 IEEE Haptics Symposium (HAPTICS), pp. 247–250,

2014. [Online]. Available: http://ieeexplore.ieee.org/document/6775462/

[43] J. Kangas, D. Akkil, J. Rantala, P.

Isokoski, P. Majaranta, and R. Raisamo, “Gaze gestures and haptic feedback in mobile devices,”
Proceedings of the 32nd annual ACM conference on Human factors in computing systems - CHI ’14, pp. 435–438, 2014. [Online]. Available:
http://dl.acm.org/citation.cfm?doid=2556288.2557040

[44] Y.-s. Chen, K.-c. Lee, C.-h. Chou, and K.-y. Lu, “SoEs : Attachable Augmented Haptic on Gaming Controller for Immersive Interaction,” pp. 71–72,

2016.

[45] A. Roudaut, A. Rau, C. Sterz, M. Plauth, P. Lopes, and P. Baudisch, “Gesture Output: Eyes-Free Output Using a Force Feedback Touch
the SIGCHI Conference on Human Factors in Computing Systems - CHI ’13, p. 2547, 2013. [Online]. Available:

Surface,” Proceedings of
http://dl.acm.org/citation.cfm?id=2470654.2481352

[46] T. Hachisu and H. Kajimoto, “HACHIStack: Dual-Layer Photo Touch Sensing for Haptic and Auditory Tapping Interaction,” Proceedings of the
SIGCHI Conference on Human Factors in Computing Systems - CHI ’13, p. 1411, 2013. [Online]. Available: http://ucalgary.summon.serialssolutions.
com/2.0.0/link/0/eLvHCXMwY2BQSAK26Y0TDZIskg3ME4Et8MRkSzPLJPOktNSklEQLi1SUq4CQSnM3IQam1DxRBik31xBnD93E5Nz4AsiRC{ }
FGJsB6BNi2F2NgAfaKU8UZWNOAsQOkgSWmOFC3OANHhKWXm4GzhSGEKwTj6hWDdy{ }pFZaIAwtocOTqmuoZAACBCia{ }{%}5Cnhttp:
//dl.

[47] C. Corsten, T. Karrer, and J. Borchers, “HaptiCase : Back-of-Device Tactile Landmarks for Eyes-Free Absolute Indirect Touch,” pp. 2171–2180, 2015.
[48] C. Hudin, J. Lozada, and V. Hayward, “Localized tactile feedback on a transparent surface through time-reversal wave focusing,” IEEE Transactions

on Haptics, vol. 8, no. 2, pp. 188–198, 2015.

[49] D. Tsetserukou, S. Hosokawa, and K. Terashima, “Linktouch: A wearable haptic device with ﬁve-bar linkage mechanism for presentation of two-dof

force feedback at the ﬁngerpad,” in Haptics Symposium (HAPTICS), 2014 IEEE.

IEEE, 2014, pp. 307–312.

[50] I. Jang and D. Lee, “On utilizing pseudo-haptics for cutaneous ﬁngertip haptic device,” IEEE Haptics Symposium, HAPTICS, pp. 635–639, 2014.
[51] C. Pacchierotti, D. Prattichizzo, and K. J. Kuchenbecker, “Displaying Sensed Tactile Cues with a Fingertip Haptic Device,” IEEE Transactions on

Haptics, vol. 8, no. 4, pp. 384–396, 2015.

[52] E. C. Chubb, J. E. Colgate, and M. A. Peshkin, “Shiverpad: A glass haptic surface that produces shear force on a bare ﬁnger,” IEEE Transactions on

Haptics, vol. 3, no. 3, pp. 189–198, 2010.

[53] M. Mohammadi, T. L. Baldi, S. Scheggi, and D. Prattichizzo, “Fingertip force estimation via inertial and magnetic sensors in deformable object

manipulation,” IEEE Haptics Symposium, HAPTICS, vol. 2016-April, pp. 284–289, 2016.

[54] M. Bianchi, E. Battaglia, M. Poggiani, S. Ciotti, and A. Bicchi, “A Wearable Fabric-based display for haptic multi-cue delivery,” IEEE Haptics

Symposium, HAPTICS, vol. 2016-April, pp. 277–283, 2016.

[55] M. Maisto, C. Pacchierotti, F. Chinello, G. Salvietti, A. De Luca, and D. Prattichizzo, “Evaluation of wearable haptic systems for the ﬁngers in

augmented reality applications,” IEEE Transactions on Haptics, 2017.

[56] Y. Rekik, E. Vezzoli, L. Grisoni, and F. Giraud, “Localized haptic texture: A rendering technique based on taxels for high density tactile feedback,” in

Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems. ACM, 2017, pp. 5006–5015.

[57] A. Doucette, R. L. Mandryk, C. Gutwin, M. Nacenta, and A. Pavlovych, “The effects of tactile feedback and movement alteration on interaction and
awareness with digital embodiments,” Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI ’13, p. 1891, 2013.
[Online]. Available: http://dl.acm.org/citation.cfm?doid=2470654.2466250

[58] M. Obrist, S. Subramanian, E. Gatti, B. Long, and T. Carter, “Emotions Mediated Through Mid-Air Haptics,” pp. 2053–2062, 2015.
[59] H. Iwata, H. Yano, F. Nakaizumi, and R. Kawamura, “Project feelex: adding haptic surface to graphics,” in Proceedings of the 28th annual conference

on Computer graphics and interactive techniques. ACM, 2001, pp. 469–476.

[60] D. Leithinger and H. Ishii, “Relief: a scalable actuated shape display,” in Proceedings of the fourth international conference on Tangible, embedded,

and embodied interaction. ACM, 2010, pp. 221–222.

[61] R. Velazquez, E. Pissaloux, M. Hafez, and J. Szewczyk, “A low-cost highly-portable tactile display based on shape memory alloy micro-actuators,”
in Virtual Environments, Human-Computer Interfaces and Measurement Systems, 2005. VECIMS 2005. Proceedings of the 2005 IEEE International
Conference on.

IEEE, 2005, pp. 6–pp.

[62] S. Jang, L. H. Kim, K. Tanner, H. Ishii, and S. Follmer, “Haptic Edge Display for Mobile Tactile Interaction,” Proceedings of the 2016 CHI Conference
on Human Factors in Computing Systems - CHI ’16, pp. 3706–3716, 2016. [Online]. Available: http://dl.acm.org/citation.cfm?doid=2858036.2858264
[63] A. A. Stanley, J. C. Gwilliam, and A. M. Okamura, “Haptic jamming: A deformable geometry, variable stiffness tactile display using pneumatics and

particle jamming,” in World Haptics Conference (WHC), 2013.

IEEE, 2013, pp. 25–30.

[64] H. Benko, C. Holz, M. Sinclair, and E. Ofek, “Normaltouch and texturetouch: High-ﬁdelity 3d haptic shape rendering on handheld virtual reality

controllers,” in Proceedings of the 29th Annual Symposium on User Interface Software and Technology. ACM, 2016, pp. 717–728.

[65] A. A. Stanley and A. M. Okamura, “Controllable surface haptics via particle jamming and pneumatics,” IEEE transactions on haptics, vol. 8, no. 1,

pp. 20–30, 2015.

[66] D. J. Meyer, M. A. Peshkin, and J. E. Colgate, “Tactile Paintbrush: A procedural method for generating spatial haptic texture,” IEEE Haptics Symposium,

HAPTICS, vol. 2016-April, pp. 259–264, 2016.

[67] J. Jung, E. Youn, and G. Lee, “Pinpad: Touchpad interaction with fast and high-resolution tactile output,” in Proceedings of the 2017 CHI Conference

on Human Factors in Computing Systems. ACM, 2017, pp. 2416–2425.

[68] T. Hoshi, D. Abe, and H. Shinoda, “Adding tactile reaction to hologram,” in Robot and Human Interactive Communication, 2009. RO-MAN 2009. The

18th IEEE International Symposium on.

IEEE, 2009, pp. 7–11.

[69] T. Carter, S. A. Seah, B. Long, B. Drinkwater, and S. Subramanian, “UltraHaptics : Multi-Point Mid-Air Haptic Feedback for Touch Surfaces,” 2013.
[70] Y. Monnai, K. Hasegawa, M. Fujiwara, K. Yoshino, S. Inoue, H. Shinoda, and S. Inoue, “HaptoMime : Mid-Air Haptic Interaction with a Floating

Virtual Screen,” pp. 663–667, 2014.

[71] K. Yoshino and H. Shinoda, “Visio-Acoustic screen for contactless touch interface with tactile sensation,” 2013 World Haptics Conference, WHC 2013,

pp. 419–423, 2013.

[72] D. Spelmezan, R. M. Gonz??lez, and S. Subramanian, “SkinHaptics: Ultrasound focused in the hand creates tactile sensations,” IEEE Haptics Symposium,

HAPTICS, vol. 2016-April, pp. 98–105, 2016.

[73] Y. Makino, Y. Furuyama, S. Inoue, and H. Shinoda, “HaptoClone ( Haptic-Optical Clone ) for Mutual Tele-Environment by Real-time 3D Image

Transfer with Midair Force Feedback,” pp. 1980–1990, 2016.

[74] R. Sodhi, I. Poupyrev, M. Glisson, and A. Israr, “Aireal: interactive tactile experiences in free air,” ACM Transactions on Graphics (TOG), vol. 32,

no. 4, p. 134, 2013.

[75] T. Hachisu and M. Fukumoto, “VacuumTouch: Attractive Force Feedback Interface for Haptic Interactive Surface using Air Suction,”
Proceedings of the 32nd annual ACM conference on Human factors in computing systems - CHI ’14, pp. 411–420, 2014. [Online]. Available:
http://dl.acm.org/citation.cfm?doid=2556288.2557252

[76] F. Arafsha, L. Zhang, H. Dong, and A. E. Saddik, “Contactless haptic feedback: State of the art,” 2015 IEEE International Symposium on Haptic,

Audio and Visual Environments and Games, HAVE 2015 - Proceedings, 2015.

[77] H. Lee, H. Cha, J. Park, S. Choi, H.-S. Kim, and S.-C. Chung, “LaserStroke,” Proceedings of the 29th Annual Symposium on User Interface Software

and Technology - UIST ’16 Adjunct, pp. 73–74, 2016. [Online]. Available: http://dl.acm.org/citation.cfm?doid=2984751.2985708

22

[78] Y. Ochiai, K. Kumagai, T. Hoshi, S. Hasegawa, and Y. Hayasaki, “Cross-Field Aerial Haptics : Rendering Haptic Feedback in Air with Light and

Acoustic Fields,” Chi ’16, pp. 3238–3247, 2016.

[79] D. Spelmezan, D. R. Sahoo, and S. Subramanian, “Sparkle: Towards haptic hover-feedback with electric arcs,” in Proceedings of the 29th Annual

Symposium on User Interface Software and Technology. ACM, 2016, pp. 55–57.

[80] T. Nojima, D. Sekiguchi, M. Inami, and S. Tachi, “The SmartTool: a system for augmented reality of haptics,” Proceedings IEEE Virtual Reality 2002,

vol. 2002, pp. 67–72, 2002.

[81] T. H. Massie, J. K. Salisbury et al., “The phantom haptic interface: A device for probing virtual objects,” in Proceedings of the ASME winter annual

meeting, symposium on haptic interfaces for virtual environment and teleoperator systems, vol. 55, no. 1. Chicago, IL, 1994, pp. 295–300.

[82] C. Gallacher, A. Mohtat, S. Ding, and J. K??vecses, “Toward open-source portable haptic displays with visual-force-tactile feedback colocation,” IEEE

Haptics Symposium, HAPTICS, vol. 2016-April, pp. 65–71, 2016.

[83] B. Knoerlein, G. Sz´ekely, and M. Harders, “Visuo-haptic collaborative augmented reality ping-pong,” Proceedings of the international conference on
Advances in computer entertainment technology - ACE ’07, p. 91, 2007. [Online]. Available: http://portal.acm.org/citation.cfm?doid=1255047.1255065
[84] M. Harders, G. Bianchi, B. Knoerlein, and G. Szekely, “Calibration, registration, and synchronization for high precision augmented reality haptics,”

IEEE Transactions on Visualization and Computer Graphics, vol. 15, no. 1, pp. 138–149, 2009.

[85] J. Aleotti, G. Micconi, and S. Caselli, “Programming manipulation tasks by demonstration in visuo-haptic augmented reality,” 2014 IEEE International

Symposium on Haptic, Audio and Visual Environments and Games, HAVE 2014 - Proceedings, pp. 13–18, 2014.

[86] K. Minamizawa, S. Fukamachi, H. Kajimoto, N. Kawakami, and S. Tachi, “Gravity grabber: wearable haptic display to present virtual mass sensation,”

in ACM SIGGRAPH 2007 emerging technologies. ACM, 2007, p. 8.

[87] Z. Najdovski, S. Nahavandi, and T. Fukuda, “Design, development, and evaluation of a pinch–grasp haptic interface,” IEEE/ASME transactions on

mechatronics, vol. 19, no. 1, pp. 45–54, 2014.

[88] I. Choi, E. W. Hawkes, D. L. Christensen, C. J. Ploch, and S. Follmer, “Wolverine: A wearable haptic interface for grasping in virtual reality,” IEEE

International Conference on Intelligent Robots and Systems, vol. 2016-Novem, pp. 986–993, 2016.

[89] T. Handa, K. Murase, M. Azuma, T. Shimizu, S. Kondo, and H. Shinoda, “A haptic three-dimensional shape display with three ﬁngers grasping,” in

Virtual Reality (VR), 2017 IEEE.

IEEE, 2017, pp. 325–326.

[90] B. Humberston and D. K. Pai, “Hands on:

interactive animation of precision manipulation and contact,” in Proceedings of

the 14th ACM

SIGGRAPH/Eurographics Symposium on Computer Animation. ACM, 2015, pp. 63–72.

[91] M. Bouzit, G. Burdea, G. Popescu, and R. Boian, “The rutgers master ii-new design force-feedback glove,” IEEE/ASME Transactions on mechatronics,

vol. 7, no. 2, pp. 256–263, 2002.

[92] Y. Nam, M. Park, and R. Yamane, “Smart glove: hand master using magnetorheological ﬂuid actuators,” in Proc. SPIE, vol. 6794, 2007, pp. 679 434–1.
[93] H. In, K.-J. Cho, K. Kim, and B. Lee, “Jointless structure and under-actuation mechanism for compact hand exoskeleton,” in Rehabilitation Robotics

(ICORR), 2011 IEEE International Conference on.

IEEE, 2011, pp. 1–6.

[94] K. Koyanagi, Y. Fujii, and J. Furusho, “Development of vr-stef system with force display glove system,” in Proceedings of the 2005 international

conference on Augmented tele-existence. ACM, 2005, pp. 91–97.

[95] J. Blake and H. B. Gurocak, “Haptic glove with mr brakes for virtual reality,” IEEE/ASME Transactions On Mechatronics, vol. 14, no. 5, pp. 606–615,

2009.

[96] P. Lopes and P. Baudisch, “Muscle-Propelled Force Feedback: bringing force feedback to mobile devices,” Proceedings of CHI 2013, pp. 2577–2580,

2013. [Online]. Available: http://dl.acm.org/citation.cfm?id=2481355{%}5Cnpapers://c80d98e4-9a96-4487-8d06-8e1acc780d86/Paper/p10612

[97] M. Pfeiffer, S. Schneegass, F. Alt, and M. Rohs, “Let me grab this: a comparison of EMS and vibration for haptic feedback in
the 5th Augmented Human International Conference on - AH ’14, pp. 1–8, 2014. [Online]. Available:

free-hand interaction,” Proceedings of
http://dl.acm.org/citation.cfm?id=2582051.2582099

[98] L. Meli, S. Member, C. Pacchierotti, and S. Member, “Sensory Subtraction in Robot-Assisted Surgery : Fingertip Skin Deformation Feedback to Ensure

Safety and Improve Transparency in Bimanual Haptic Interaction,” vol. 61, no. 4, pp. 1318–1327, 2014.

[99] D. Prattichizzo, F. Chinello, C. Pacchierotti, and M. Malvezzi, “Towards wearability in ﬁngertip haptics: A 3-dof wearable device for cutaneous force

feedback,” IEEE Transactions on Haptics, vol. 6, no. 4, pp. 506–516, Oct 2013.

[100] P. Lopes, A. Ion, and P. Baudisch, “Impacto: Simulating physical impact by combining tactile stimulation with electrical muscle stimulation,” in

Proceedings of the 28th Annual ACM Symposium on User Interface Software & Technology. ACM, 2015, pp. 11–19.

[101] P. Lopes, P. Jonell, and P. Baudisch, “Affordance++,” Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems -

CHI ’15, no. July, pp. 2515–2524, 2015. [Online]. Available: http://dl.acm.org/citation.cfm?doid=2702123.2702128

[102] T. Tanabe, H. Yano, and H. Iwata, “Properties of proprioceptive sensation with a vibration speaker-type non-grounded haptic interface,” IEEE Haptics

Symposium, HAPTICS, vol. 2016-April, pp. 21–26, 2016.

[103] W. Lee and J. Park, “Augmented foam: A tangible augmented reality for product design,” Proceedings - Fourth IEEE and ACM International Symposium

on Symposium on Mixed and Augmented Reality, ISMAR 2005, vol. 2005, pp. 106–109, 2005.

[104] J. R. Blum, I. Frissen, and J. R. Cooperstock, “Improving Haptic Feedback on Wearable Devices through Accelerometer Measurements,”
Proceedings of the 28th Annual ACM Symposium on User Interface Software & Technology - UIST ’15, pp. 31–36, 2015. [Online]. Available:
http://dl.acm.org/citation.cfm?doid=2807442.2807474

[105] M. Azmandian, M. Hancock, H. Benko, E. Ofek, and A. D. Wilson, “Haptic Retargeting: Dynamic Repurposing of Passive Haptics for Enhanced

Virtual Reality Experiences,” Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems - CHI ’16, pp. 1968–1979, 2016.

[106] A. J. Spiers and A. M. Dollar, “Outdoor pedestrian navigation assistance with a shape-changing haptic interface and comparison with a vibrotactile

device,” IEEE Haptics Symposium, HAPTICS, vol. 2016-April, pp. 34–40, 2016.

[107] M. Price and F. C. Sup, “A robotic touchscreen totem for two-dimensional haptic force display,” IEEE Haptics Symposium, HAPTICS, vol. 2016-April,

pp. 72–77, 2016.

[108] A. Israr, S. Zhao, K. Mcintosh, Z. Schwemler, A. Fritz, J. Mars, J. Bedford, C. Frisson, I. Huerta, M. Kosek et al., “Stereohaptics: a haptic interaction

toolkit for tangible virtual experiences,” in ACM SIGGRAPH 2016 Studio. ACM, 2016, p. 13.

[109] T. Nakamura and A. Yamamoto, “Extension of an electrostatic visuo-haptic display to provide softness sensation,” IEEE Haptics Symposium, HAPTICS,

vol. 2016-April, pp. 78–83, 2016.

[110] C. Pacchierotti, S. Sinclair, M. Solazzi, A. Frisoli, V. Hayward, and D. Prattichizzo, “Wearable haptic systems for the ﬁngertip and the hand: taxonomy,

review, and perspectives,” IEEE Transactions on Haptics, 2017.

[111] M. Maier, M. Chowdhury, B. P. Rimal, and D. P. Van, “The tactile internet: vision, recent progress, and open challenges,” IEEE Communications

Magazine, vol. 54, no. 5, pp. 138–145, 2016.

[112] A. Aijaz, M. Dohler, A. H. Aghvami, V. Friderikos, and M. Frodigh, “Realizing the Tactile Internet: Haptic Communications over Next Generation 5G

Cellular Networks,” IEEE Wireless Communications, pp. 1–8, 2016.

[113] M. Simsek, A. Aijaz, M. Dohler, J. Sachs, and G. Fettweis, “5G-Enabled Tactile Internet,” IEEE Journal on Selected Areas in Communications,
vol. 34, no. 3, pp. 460–473, 2016. [Online]. Available: https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963718779{&}partnerID=40{&}md5=
b45daacb61a1fd2659ee52013b4c168e

[114] J. Pilz, M. Mehlhose, T. Wirth, D. Wieruch, B. Holfeld, and T. Haustein, “A Tactile Internet demonstration: 1ms ultra low delay for wireless

communications towards 5G,” Proceedings - IEEE INFOCOM, vol. 2016-Septe, no. Keystone I, pp. 862–863, 2016.

23

[115] C. Bachhuber and E. Steinbach, “Are Today’s Video Communication Solutions Ready for the Tactile Internet?” 2017. [Online]. Available:

http://www.lmt.ei.tum.de/forschung/publikationen/dateien/Bachhuber2017AreTodaysVideoCommunication.pdf

[116] P. Popovski, “Ultra-Reliable Communication in 5G Wireless Systems,” pp. 146–151, 2014. [Online]. Available: http://arxiv.org/abs/1410.4330
[117] E. Steinbach, S. Hirche, M. Ernst, F. Brandi, R. Chaudhari, J. Kammerl, and I. Vittorias, “Haptic communications,” Proceedings of the IEEE, vol. 100,

no. 4, pp. 937–956, 2012.

[118] J. G. W. Wildenbeest, D. A. Abbink, C. J. M. Heemskerk, F. C. T. Van Der Helm, and H. Boessenkool, “The impact of haptic feedback quality on the

performance of teleoperated assembly tasks,” IEEE Transactions on Haptics, vol. 6, no. 2, pp. 242–252, 2013.

[119] J. E. Colgate and G. Schenkel, “Passivity of a class of sampled-data systems: Application to haptic interfaces,” in American Control Conference, 1994,

vol. 3.

IEEE, 1994, pp. 3236–3240.

[120] T. Hulin, A. Albu-Schaffer, and G. Hirzinger, “Passivity and stability boundaries for haptic systems with time delay,” IEEE Transactions on Control

Systems Technology, vol. 22, no. 4, pp. 1297–1309, 2014.

[121] J. M¨uller, S. Butscher, and H. Reiterer, “Immersive analysis of health-related data with mixed reality interfaces: Potentials and open question,” in

Proceedings of the 2016 ACM Companion on Interactive Surfaces and Spaces. ACM, 2016, pp. 71–76.

[122] A. Tirmizi, C. Pacchierotti, I. Hussain, G. Alberico, and D. Prattichizzo, “A perceptually-motivated deadband compression approach for cutaneous

haptic feedback,” IEEE Haptics Symposium, HAPTICS, vol. 2016-April, pp. 223–228, 2016.

[123] S. Liang, “Design principles of augmented reality focusing on the ageing population,” in Proceedings of the 30th International BCS Human Computer

Interaction Conference: Fusion! BCS Learning & Development Ltd., 2016, p. 2.

[124] P. Bach-y Rita, C. C. Collins, F. A. Saunders, B. White, and L. Scadden, “Vision substitution by tactile image projection,” Nature, vol. 221, no. 5184,

pp. 963–964, 1969.

[125] P. Bach-y Rita, K. A. Kaczmarek, M. E. Tyler, and J. Garcia-Lara, “Form perception with a 49-point electrotactile stimulus array on the tongue: a

technical note,” Journal of rehabilitation research and development, vol. 35, no. 4, p. 427, 1998.

[126] C. A. Lozano, K. A. Kaczmarek, and M. Santello, “Electrotactile stimulation on the tongue: Intensity perception, discrimination, and cross-modality

estimation,” Somatosensory & motor research, vol. 26, no. 2-3, pp. 50–63, 2009.

[127] I. Han and J. B. Black, “Incorporating haptic feedback in simulation for learning physics,” Computers & Education, vol. 57, no. 4, pp. 2281–2290,

2011.

[128] A. M. Okamura, C. Richard, M. Cutkosky et al., “Feeling is believing: Using a force-feedback joystick to teach dynamic systems,” Journal of Engineering

Education, vol. 91, no. 3, pp. 345–349, 2002.

[129] J. Minogue and M. G. Jones, “Haptics in education: Exploring an untapped sensory modality,” Review of Educational Research, vol. 76, no. 3, pp.

317–348, 2006.

[130] T. R. Coles, D. Meglan, and N. W. John, “The role of haptics in medical training simulators: A survey of the state of the art,” IEEE Transactions on

haptics, vol. 4, no. 1, pp. 51–66, 2011.

[131] E. Richard, A. Tijou, P. Richard, and J.-L. Ferrier, “Multi-modal virtual environments for education with haptic and olfactory feedback,” Virtual Reality,

vol. 10, no. 3-4, pp. 207–225, 2006.

[132] C. Seim, T. Estes, and T. Starner, “Towards Passive Haptic Learning of piano songs,” IEEE World Haptics Conference, WHC 2015, pp. 445–450, 2015.

24

