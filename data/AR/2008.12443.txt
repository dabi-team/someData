0
2
0
2

c
e
D
2

]
T
S
.
h
t
a
m

[

5
v
3
4
4
2
1
.
8
0
0
2
:
v
i
X
r
a

SECOND MOMENT ESTIMATOR FOR AN AR(1) MODEL DRIVEN BY A
LONG MEMORY GAUSSIAN NOISE

YONG CHEN, LI TIAN, AND YING LI

Abstract. In this paper, we consider an inference problem for the ﬁrst order autoregressive

process driven by a long memory stationary Gaussian process. Suppose that the covariance
2H−2
function of the noise can be expressed as |k|
The fractional Gaussian noise and the fractional ARIMA model and some others Gaussian

times a function slowly varying at inﬁnity.

noise are special examples that satisfy this assumption. We propose a second moment esti-

mator and prove the strong consistency, the asymptotic normality and the almost sure central

limit theorem. Moreover, we give the upper Berry-Esséen bound by means of Fourth moment

theorem.
Keywords: Gaussian process; asymptotic normality; almost sure central limit theorem;

Berry-Esséen bound; Breuer-Major theorem; Fourth moment theorem.
MSC 2010: 60G10; 60F25; 62M10

1.

Introduction

For the ﬁrst order autoregressive model (Xt, t

(ξt, t

Z):

∈

N) driven by a given noise sequence ξ =

∈

Xt = θXt

−

1 + ξt,

N

t

∈

(1.1)

with X0 = 0, the inference problem regarding the parameter θ has been extensively studied
in probability and statistics literatures. For when ξ is independent identical distribution or a
martingale diﬀerence sequence, this problem has been widely studied over the past decades (see

[1, 2] and the references therein). In the heavy-tailed noise case, the least square estimator (LSE)
of AR(p) models was studied in [3]. The maximum likelihood estimator (MLE) of AR(p) was

investigated in [4] for regular stationary Gaussian noise, in which they transform the observation

model into an “equivalent” model with Gaussian white noise. In [4], it is pointed out that for
the strongly dependent noises the LSE is generally not consistent. Very recently, in case of long-

memory noise, the detection of a change of the above parameter θ is studied by means of the
likelihood ratio test [5].

In this paper, we will discuss the long-range dependence Gaussian noise case and propose a
second moment estimator. First, we ﬁnd that it is very convenient to construct a second moment

estimator when we restrict the domain of the parameter θ in

Θ =

R

θ

{

∈

|
1

0 < θ < 1

.

}

 
 
 
 
 
 
2

Y. CHEN, L. TIAN, AND Y. LI

It seems that this restriction is very reasonable for real-world context sometimes. In fact,
< 1
is an assumption to ensure the model (1.1) to have a stationary solution. We rule out the case

θ

|

|

1 < θ < 0 in which the series tends to oscillate rapidly. We also rule out the case of θ = 0

of
in which Xt is not an autoregressive model any more.

−

Next, we assume that the stationary Gaussian nosie ξ satisﬁes the following Hypothesis 1.1:

Hypothesis 1.1. The covariance function ρ(k) = E(ξ0ξk) for any k

Z satisﬁes

∈

ρ(k) = L(k)
|

k

2H

2, H

−

∈
(cid:18)
) is slowly varying at inﬁnity in Zygmund’s sense and ˜L(λ) := L( 1

(cid:19)

|

(1.2)

λ ) is

1
2

, 1

with L : (0,
of bounded variation on (a, π) for any a > 0. Moreover, ρ(0) = 1.

)
∞

(0,

→

∞

It is well known that Eq. (1.2) is equivalent to the spectral density of ξ satisfying

with CH = π−

1Γ(2H

1) sin(π

−

−

hξ(λ)

CH L(λ−

1

2H ,

−

1)
|

λ
|

as λ

0

∼
πH). Please refer to [6] or Lemma 2.3 below.

→

We will see that the fractional Gaussian noise, the fractional ARIMA model driven by Gaussian

white noise and some other long memory Gaussian processes are special examples satisfying
Hypothesis 1.1.

When

θ

|

|

< 1, the stationary solution to the model (1.1) is

Yt =

θjξt
−

j,

∞

j=0
X

and the solution with initial value X0 = 0 can be represented as:

where ζ is a normal random variable with zero mean. It is clear that the second moment of Yt
is:

Xt = Yt + θtζ,

(1.3)

f (θ) := E(Y 2

t ) =

∞

θi+j ρ(i

i,j=0
X

j).

−

If 0 < θ < 1 then f (θ) is positive and strictly increasing. Denote f −
of f (
·

). We propose a second moment estimator of θ as:

) is the inverse function

1(
·

˜θn = f −

1

1
n

n

t=1
X

X 2
t

.

!

(1.4)

In this paper, we will show the strong consistency and give the asymptotic distribution for
that estimator. Moreover, we also give the Berry-Esséen bound when the limit distribution is

Gaussian. These results are stated in the following theorems:

Theorem 1.2. Under Hypothesis 1.1, the estimator ˜θn is strongly consistent, i.e.,

˜θn = θ

a.s..

lim
n
→∞

 
SECOND MOMENT ESTIMATOR FOR AR(1) MODEL

3

Theorem 1.3. Under Hypothesis 1.1 and suppose H
distribution of ˜θn as n

:
→ ∞

2 , 3
( 1

4 ), we have the following asymptotic

∈

√n(˜θn

σ2
H
[f ′(θ)]2
−
Z R2(k) and f ′(θ) is the derivative of f (θ).

law
−−→ N

θ)

0,

(cid:18)

,

(cid:19)

(1.5)

where σ2

H = 2

k

∈

P
Remark 1.4. The case of H

[ 3
4 , 1) will be treated in a separate paper.

∈

Theorem 1.5. Let Z be a standard Gaussian random variable. Under Hypothesis 1.1 and

suppose H

( 1
2 , 3

4 ), then

∈

Gn :=

f ′(θ)√n(˜θn

σH

θ)

−

satisﬁes an almost sure central limit theorem (ASCLT). In other words, almost surely, for all
z

R,

∈

Gk≤
or, equivalently, almost surely, for all continuous and bounded functions ϕ : R

k=1
X

} →

≤

{

z

P (Z

z)

as n

,
→ ∞

1
log n

n

1

1
k

R,

→

1
log n

n

k=1
X

1
k

ϕ(Gk)

→

E[ϕ(Z)]

as n

.
→ ∞

Theorem 1.6. Let Z be a standard Gaussian random variable. Under Hypothesis 1.1 and
suppose H

4 ), there exists a constant Cθ,H > 0 such that when n is large enough,

( 1
2 , 3

∈

f ′(θ)√n(˜θn

σH

θ)

−

P

(

z

≤

) −

P

Z

{

≤

Cθ,Hϕ(n),

≤

(1.6)

∈

sup
R (cid:12)
z
(cid:12)
(cid:12)
(cid:12)
(cid:12)

where σH is given in Theorem 1.3 and

ϕ(n) =

(

n

,

1
1
2 −
1
4H

n3

−

Here 1

2 −

means that 1

2 −

ǫ for any ǫ > 0.

,

−

if H

if H

∈

∈

(cid:0)

(cid:2)

z

}(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
2 , 5
8 , 3

4

8

1

5

;

.

(cid:1)

(cid:1)

Remark 1.7. We do not know how to improve the upper bound 1/n

1

2 − to 1/√n when H

Next, we give some sequences that satisfy Hypothesis 1.1.

Example 1.8. The fractional Gaussian noise with covariance function

∈

(cid:0)

1

2 , 5

8

.

(cid:1)

satisﬁes Hypothesis 1.1 when H > 1

ρ(k) =

1
2

|
(cid:0)

k + 1

2H +

k

2H

1

2H

2

k

,

|

|

−
(cid:1)
2 . It is well-known that ρ(k)
∼

−

|

|

|

Z

k

∈

H(2H

k

1)
|

|

−

2H

2 as k

−

.
→ ∞

Example 1.9. The fractional ARIMA(0,d,0) model driven by a Gaussian white noisesatisﬁes

Hypothesis 1.1 when d

∈

(0, 1

2 ). It is well-known that it’s the spectral density satsiﬁes

hξ(λ)

1
2π |

λ
|

∼

2d,

−

λ

0

→

4

Y. CHEN, L. TIAN, AND Y. LI

Example 1.10. ξt is linear stationary sequence deﬁned by

kεt,

bt

−

Z.

t

∈

ξt =

t
k
X
≤

Here
Assume that the weights decay slowly hyperbolically:

is a Gaussian white noise, and the weights

εt, t
{

∈

Z
}

bt, t
{

∈

N+}

satisfy that

b2
t <

.
∞

P

bk = L0(k)
|

k

|

H

3

2 ,

−

) is a slowly varying function. Then Hypothesis 1.1 is valid if we take

where H
L(k)

( 1
2 , 1), and L0(
·

∈
L2
0(k) (see [7]).

∝

In the remainder of this paper, C and c will be a generic positive constant independent of n

the value of which may diﬀer from line to line.

2. Preliminary

In this section, we list the main deﬁnitions and theorems that is used to show our results. The

following two deﬁnitions are cited from Deﬁnition 1.1 and 1.2 of [6] respectively.

Deﬁnition 2.1. A positive function L(x) deﬁned for x > x0 is called a slowly varying at inﬁnity
function in Zygmund’s sense if, for any δ > 0, p1(x) = xδL(x) is an increasing, and p2(x) =
δL(x) is a decreasing, function of x for x large enough. Similarly, L is called slowly varying
x−
at the origin if ˜L(x) = L(x−

1) is slowly varying at inﬁnity.

It is known that if L(x) is slowly varying at inﬁnity then

for every ﬁxed u > 0, and even uniformly in every interval a

u

≤

≤

1
a , 0 < a < 1 [8, p.186].

L(ux)
L(x)

lim
x
→∞

= 1

(2.1)

Deﬁnition 2.2. Let
ρ(k)(k

Z) and spectral density

ξt

{

}

∈

be a second-order stationary process with autocovariance function

hξ(λ) = (2π)−

1

∞

k=

X

−∞

ρ(k) exp(

−

ikλ),

λ

[

−

∈

π, π].

Then

ξt
{

}

is said to exhibit linear long-range dependence, if

hξ(λ) = Lh(λ)
|

λ
|

1

−

2H ,

where Lh(λ) > 0 is a symmetric function that is slowly varying at zero and H

( 1
2 , 1).

∈

The following theorem is well-known and is cited from Theorem 1.3 of [6]:

Theorem 2.3. Let R(k)(k

Z) and h(λ)(λ

∈
density respectively of a second-order stationary process

∈

[π, π]) be the autocovariance function and spectral

. Then the following holds:

ξt

{

}

SECOND MOMENT ESTIMATOR FOR AR(1) MODEL

5

(1) If

where LR(k) is slowly varying at inﬁnity in Zygmund’s sense, and H

R(k) = LR(k)
|

k

|

2H

2,

−

Z,

k

∈

( 1
2 , 1), then

∈

h(λ)

Lh(λ)
|

λ
|

∼

1

−

2H ,

λ

0,

→

Lh(λ) = LR(λ−

1)π−

1Γ(2H

1) sin(π

πH).

−

−

where

(2) If

h(λ) = Lh(λ)
|

λ
|

1

−

2H ,

0 < λ < π,

where H
bounded variation on (a, π) for any a > 0, then

∈

( 1
2 , 1), and Lh(λ) is slowly varying at the origin in Zygmund’s sense and of

where

R(k)

LR(k)
|

k

|

∼

2H

2,

−

k

,
→ ∞

LR(k) = 2Lh(k−

1)Γ(2

−

2H) sin

πH

(cid:18)

1
2

π

.

(cid:19)

−

The following Breuer-Major theorem is well-known, for example, see [9] and [10]:

Theorem 2.4 (Breuer-Major theorem). Y = (Yt, t
sequence. Set R(k) = E(Y0Yk). Deﬁne

∈

Z) is a centered stationary Gaussian

If

then

Vn :=

1
√n

n

[Y 2

t −

t=1
X

E(Y 2

t )],

1.

n

≥

(2.2)

σ2
H := 2

R2(k) <

Z
k
X
∈

,
∞

Vn =

1
√n

n

t=1
X

[Y 2

t −

E(Y 2

t )]

law
−−→ N

(0, σ2

H ),

as n

,
→ ∞

The following theorem is taken form Theorem 7.3.1 of [10], which is a corollary of Fourth

Moment Berry-Esséen bound.

(0, 1) and let R(k), Vn be given in Theorem 2.4. Set v2

n = E(V 2

n ).

Theorem 2.5. Set Z

Then, for all n

1,

≥

∼ N

dT V (Vn/vn, Z)

4√2
v2
n√n  

≤

1

n

−

k=

n+1

X
−

3
2

.

4
3

R(k)
|

|

!

The following theorem is rephrased from Theorem 5.1 and Proposition 5.2 of [11], which gives

a suﬃcient condition to the almost sure central limit theorem of ¯Vn = Vn
vn

.

Theorem 2.6. Let Vn be given by (2.2) and Z
, for some β > 1
k
R(k)
Vn
vn

| → ∞
satisﬁes an almost sure central limit theorem.

β L(
|

), as

∼ |

−
|

k

k

|

|

∼ N

(0, 1). Set v2

n = E(V 2

n ). Assume that
). Then

2 and some slowly varying function L(
·

n

o

6

Y. CHEN, L. TIAN, AND Y. LI

3. Proofs of the Strong Consistency, The Asymptotic Normality and ASCLT

Lemma 3.1. Let (Yt)t
variable Cε such that

∈

N and ζ be given by (1.3). Then for all ε > 0 there exists a random

n

for all n

N, and moreover, E
|

Cε

|

∈

Proof. Fix p

≥

1 and denote by

k · k

ζ

θtYt

Cεnε

a.s.

≤

t=1
X
∞

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
p <
for all p
(cid:12)
(cid:12)
(cid:12)
(cid:12)
p the Lp-norm. Since

1.

≥

n
t=1 θtYt is Gaussian, we have

n

n

θtYt

c2

≤

θtYt

(cid:13)
(cid:13)
2p
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
N. The Hölder equality implies that
(cid:13)
(cid:13)
(cid:13)
(cid:13)

t=1
X

t=1
X

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

n

P
θi+j E[YiYj ]

i,j=1
X

= c2

v
u
u
t

for all n

∈

θtYt

n

t=1
X

(cid:13)
p
(cid:13)
(cid:13)
(cid:13)
(cid:13)

ζ
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

ζ

k2p

≤ k

n

θtYt

t=1
X

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

2p

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

n

θtYt

c1

≤

t=1
X

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

Cθ,

≤

Cθ,

≤

2p

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

≥

which implies (3.1) from Lemma 2.1 of [12]. In fact, it is easy to check that the conclusion of
(cid:3)
Lemma 2.1 of [12] is valid if its assumption α > 0 is changed to α

0.

(3.1)

(3.2)

Proof of Theorem 1.2. It is obvious that the covariance function of Yt is

R(k) = Cov(Yt, Yt+k) =

∞

θi+j ρ(k

i,j=0
X

i + j).

−

Since ρ(k)

0 as k

→

→ ∞

and when 0 < θ < 1,

∞

θi+j ρ(k

i + j)

−

i,j=0
X

the dominated convergence theorem implies that

∞

≤

i,j=0
X

θi+j <

,
∞

Hence, the stationary Gaussian sequence Yt is ergodic. Since EY 2

t = f (θ), we obtain

R(k) = 0.

lim
k
→∞

Lemma 3.1 implies that for 0 < ε < 1 there exists a random variable Cε such that

1
n

lim
n
→∞

t=1
X

1
n

ζ

n

t=1
X

θtYt

n

Y 2
t = f (θ) a.s.

Cεn−

1+ε,

a.s.

≤

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
n

t=1
X

1
n

ζ

θtYt

0

→

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Hence, as n

,
→ ∞

almost surely. Thus,

1
n

lim
n
→∞

n

t=1
X

X 2

t = f (θ) a.s.,

(3.3)

SECOND MOMENT ESTIMATOR FOR AR(1) MODEL

which, together with the continuous mapping theorem, implies that as n

,
→ ∞

˜θn

→

θ,

a.s.

7

✷

Remark 3.2. Since f (θ), θ
when n is large enough, 1
n

(0, 1) is a strictly increasing function, the limit (3.3) implies that
t belongs to the range of f (θ). Hence, when n is large enough,

∈
n
t=1 X 2

P

(0, 1)

∋

˜θn = f −

1(

1
n

n

t=1
X

X 2

t ),

a.s.

(3.4)

Lemma 3.3. Under Hypothesis 1.1, the stationary solution Yt to the model (1.1) exhibits linear
long-range dependence. Namely,

λ
|
where the constant Cθ,H > 0 depends on θ and H.

Cθ,H L(λ−

hY (λ)

∼

1)
|

1

−

2H ,

as λ

0,

→

(3.5)

Proof. The spectral density of the stationary solution Yt to the model (1.1) satisﬁes

hY (λ) =

1

|

−

iλ

θe−

|

2hξ(λ).

−

Under Hypothesis 1.1, Theorem 2.3(1) implies that as λ

0,

→
2H .

−

1

hξ(λ)

∼

CH L(λ−

1)
|

λ
|

0,

Hence, we obtain that as λ

→
hY (λ)

(1

−

∼

θ)−

2hξ(λ)

∼

Cθ,HL(λ−

1

−

2H .

1)
|

λ
|

(cid:3)

Lemma 3.4. Let Yt be the stationary solution to the model (1.1) and R(k) = E(YkY0). Let Vn
be given in Theorem 2.4 and v2

n ). When H

n = E(V 2

4 ),

2 , 3
( 1
∈
R2(k) <

.
∞

v2
n = 2

lim
n
→∞

Z
k
X
∈

Proof. It is well known the following two identities hold:

v2
n =

2
n

n

k,l=1
X

R2(k

−

l) = 2

k
X|
|

−

1
<n (cid:18)
R2(k) <

lim
n
→∞

v2
n = 2

R2(k),

if

Z
k
X
∈

Z
k
X
∈

see, for example, (7.2.6) of [10].

(3.6)

(3.7)

R2(k),

k
|
|
n

(cid:19)

,
∞

Thus, to check (3.6) for H

∞
holds. In fact, Theorem 2.3 (2) and the identity (3.5) imply that the covariance function of Yt
satisﬁes that when 1

P

∈

∈

( 1
2 , 3

4 ), we need only to show the condition

Z R2(k) <

k

2 < H < 1,

R(k)

C′θ,HL(k)
|

k

|

∼

2H

2,

−

as k

.
→ ∞

(3.8)

8

Y. CHEN, L. TIAN, AND Y. LI

Recall that L(k) < c
Hence,

|

k

|

if and only if

δ for any ﬁxed δ > 0 and k large enough (see, for example, [13, p.277] ).

R2(k) <

∞

Z
k
X
∈

Note that δ > 0 is arbitrary, we obtain that when 1
holds.

4H

4 + 2δ <

−

−

1.
2 < H < 3

4 , the condition

Z R2(k) <

∞
(cid:3)

k

∈

P

Proof of Theorem 1.3. Using the identity (1.3), we have that

n

1
√n

t=1
X

(cid:0)

X 2

t −

f (θ)

=

1
√n

(cid:1)

n

t=1
X

(cid:0)

Y 2
t −

f (θ)

+

(cid:1)

2ζ
√n

n

t=1
X

θtYt +

ζ2
√n

n

t=1
X

θ2t.

(3.9)

Theorem 2.4 and Lemma 3.4 imply that when 1

2 < H < 3

4 , as n

,
→ ∞

Lemma 3.1 implies that as n

n

1
√n

t=1
X
,
→ ∞

It is clear that as n

,
→ ∞

ζ
√n

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Y 2
t −

f (θ)

law
−−→ N

(0, σ2

H ).

(cid:0)

n

t=1
X

θtYt

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
n

ζ2
√n

(cid:1)

Cǫnǫ

−

1
2

≤

0.

→

θ2t

0.

→

n

1
√n

t=1
X

(cid:0)

X 2

t −

f (θ)

law
−−→ N

(0, σ2

H ),

(cid:1)

which implies the desired (1.5) from the delta method.

t=1
X
Substituting the above three limits into (3.9), we deduce from Slutsky’s theorem that as n

(3.10)

(3.11)

,
→ ∞

✷

Proof of Theorem 1.5. The ASCLT can be obtained by arguments similar to those of Theorem

4.3 in [14].

First, Eq.(3.8) and Theorem 2.6 imply that when 1

2 < H < 3
4 ,

satisﬁes the ASCLT.

Second, by (3.7), (3.9)-(3.11), we have that

Vn
vn

(cid:26)

: n

1

≥

(cid:27)

n

1
√nσH

t=1
X

(cid:0)

X 2

t −

f (θ)

=

Vn
vn

vn
σH

+

2ζ
√nσH

(cid:1)

n

θtYt +

t=1
X

ζ2
√nσH

θ2t,

n

t=1
X

which, together with Theorems 3.1 and 3.2 of [14], implies that

1
√nσH

(

n

X 2

t −

t=1
X

(cid:0)

f (θ)

: n

(cid:1)

1

)

≥

SECOND MOMENT ESTIMATOR FOR AR(1) MODEL

9

satisﬁes the ASCLT.

Third, the mean value theorem implies that

f ′(θ)√n(˜θn

θ)

−

=

f ′(θ)√n
f ′(ηn)σH

1
n

σH

n

t=1
X
where ηn is a random variable between 1
n
f ′(θ)
f ′(ηn) →

1 almost surely as n

→ ∞

(cid:16)

satisﬁes the ASCLT.

(

σH

X 2

t −

f (θ)

=

f ′(θ)
f ′(ηn)

1
√nσH

n

X 2

t −

f (θ)

(cid:17)

t=1
X
t and f (θ). The convergence (3.3) leads to

(cid:1)

(cid:0)

n
t=1 X 2

. It follows from Theorems 3.1 of [14] that

P
f ′(θ)√n(˜θn

θ)

−

: n

1

)

≥

✷

4. The Berry-Esséen Bound

The following Fourth Moment Berry-Esséen bound is similar to Corollary 7.4.3 of [10].

Proposition 4.1. Let Vn be given in Theorem 2.4 and v2
exists a constant cH > 0 such that, for all n

2:

n = E(V 2

n ). When H

≥

dT V (Vn/vn, Z)

cH ϕ1(n),

≤

where

ϕ1(n) =

(

n−

n3

1
2 ,
1
4H
−

if H
if H

∈
∈

(cid:0)

,

−

8

1

2 , 5
8 , 3

5

4

;
.

(cid:1)

( 1
2 , 3

4 ), there

∈

(4.1)

Remark 4.2. When the function slowly varying at inﬁnity L(k) degenerates to a positive con-
1
4H in the case of

stant as k large enough, we can improve the upper bound
H

1
4H

to

n3

n3

.

−

−

−

(cid:2)

(cid:1)

5

8 , 3

4

∈

(cid:1)
Proof. Since L(k) < c

(cid:0)

k

|

|

δ for any ﬁxed δ > 0 and k large enough, we have that

R(k)
|
8 ), we can take δ > 0 small enough such that 4

≤

C

k

−

|

|

|

4
3 (2H

2+δ).

4
3

3 (2H

2 , 5
( 1
When H
following series converges,

∈

(4.2)

2 + δ) <

1 and the

−

−

Together with the limit (3.6), we obtain form Theorem 2.5 that the desired bound (4.1) holds.
When 5/8

H < 3/4, the inequality (4.2) implies that

≤

R(k)
|

4
3 <

.
∞

Z |
k
X
∈

3
2

4
3

R(k)
|

|

≤

!

Cn4H

−

5

2 +2δ.

1

n

−

k=

n+1

X
−

Again by (3.6) and Theorem 2.5, we have the desired bound (4.1) since δ > 0 is arbitary.

The following result is Lemma 2 of [15].

(cid:3)

 
10

Y. CHEN, L. TIAN, AND Y. LI

Lemma 4.3. For any random variable ξ, η and real constant a > 0,

sup
R |
u

∈

P (ξ + η

u)

−

≤

Φ(u)

| ≤

sup
R |
u

∈

P (ξ

u)

Φ(u)
|

+ P (
|

η

|

−

≤

> a) +

a
√2π

,

(4.3)

where Φ(u) stands for the standard normal distribution function.

Proof of Theorem 1.6. The Berry-Esseen bound (1.6) can be obtained by arguments similar to
those of Theorem 3.2 in [16]. Denote

A := P

(

f ′(θ)√n(˜θn

σH

θ)

−

z

≤

) −

P

Z

{

z

.

}

≤

Since ˜θn

∈

(0, 1) almost surely (see Remark 3.2), we shall suppose that z

:

∈ D

D

:=

z :

(cid:26)

f ′(θ)√n
σH

−

θ < z <

f ′(θ)√n
σH

(1

θ

∧

−

(cid:16)

(cid:17)(cid:27)

θ)

.

(4.4)

Otherwise, the upper-tail inequality for standard normal distribution

yields

P (Z

t)

≤

≥

t2
2

e−
t√2π

,

t > 0

{
Since f (θ) is strictly increasing and continuous, we have by (1.4) for the formula of ˜θn

|} ≤

|

|

= P

Z >

z

A
|

C
√n

.

A = P

(

f ′(θ)√n(˜θn

σH

θ)

−

z

≤

) −

P

Z

{

z

}

≤

≤
n

= P

˜θn

(cid:26)

(

(

1
n

1
n

= P

= P

θ +

σH
f ′(θ)√n

z

(cid:27)

P

Z

{

−

z

}

≤

X 2

t ≤

f

θ +

(cid:18)

t=1
X
n

σH
f ′(θ)√n

z

X 2

t −

f (θ)

≤

f

θ +

(cid:18)

t=1
X

(cid:19)) −
σH
f ′(θ)√n

P

Z

{

z

}

≤

z

−

(cid:19)

f (θ)

) −

P

Z

{

z

.

}

≤

We take the short-hand notation

u(z) =

√n
σH (cid:20)

f

θ +

σH
f ′(θ)√n

z

f (θ)

(cid:21)

−

(cid:19)

(4.5)

and

w =

2ζ
σH √n

(cid:18)

n

t=1
X

θtYt +

ζ2
σH √n

n

θ2t.

t=1
X

By (1.3) the relationship between Xt and Yt and (2.2) the formula of Vn, we have

Vn
σH
Vn
σH

(cid:26)

(cid:26)

+ w

+ w

u

u

≤

≤

A
|

|

=

P

≤

P

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

}

(cid:12)
(cid:12)
(cid:12)
Φ(u)
(cid:12)

P

Z

{

−

≤

z

(cid:27)

−

(cid:27)

+

|

Φ(u)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

P

Z

{

−

.

z

}|

≤

SECOND MOMENT ESTIMATOR FOR AR(1) MODEL

11

The second term is bounded by Cn−

1

2 from Lemma 4.4 below. Hence, we need only show that

In fact, Lemma 4.3 implies that for any γ > 0,

Vn
σH

(cid:26)

+ w

u

≤

−

(cid:27)

P

sup
R (cid:12)
u
(cid:12)
(cid:12)
(cid:12)

∈

∈
By Proposition 4.1, the Fourth moment Berry-Esséen bound, we have that

(cid:9)

+ P

> n−

γ

+

w

|

|
(cid:8)

Φ(u)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

sup
R
u

∈

P
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Φ(u)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

sup
R
u

∈

≤

P
(cid:12)
(cid:12)
(cid:12)
(cid:12)

P

Vn
σH

(cid:26)

+ w

u

≤

−

(cid:27)

Cϕ(n).

≤

Φ(u)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Vn
σH ≤

u

−

(cid:27)

(cid:26)

P

sup
R (cid:12)
u
(cid:12)
(cid:12)
(cid:12)
Vn
σH ≤

(cid:26)

u

−

(cid:27)

> n−

γ

w

|

|
(cid:8)

(cid:9)

Cϕ1(n).

≤

Φ(u)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
npγE
|

≤

p.

w

|

Chebyshev’s inequality implies that

(4.6)

γ

n−
√2π

.

(4.7)

The triangular inequality and the inequality (3.2) imply that

w

k

kp ≤

from which we have that

2
ζ
σH √n (cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

θtYt

n

t=1
X

+

(cid:13)
p
(cid:13)
(cid:13)
(cid:13)
(cid:13)

P

w

|

|

> n−

γ

1
σH √n (cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
Cnp(γ

≤

1

2 ).

−

C
√n

,

≤

n

ζ2

θ2t

t=1
X

(cid:13)
p
(cid:13)
(cid:13)
(cid:13)
(cid:13)

We take γ < 1

2 , p large enough, and substitute the above inequalities into (4.7) to get (4.6). ✷

(cid:8)

(cid:9)

Lemma 4.4. Let u(z) be given by (4.5) and the interval
some positive number Cθ,H independent of n such that

D

given by (4.4). Then there exists

sup
z

∈D

|

Φ(u(z))

Φ(z)

| ≤

−

Cθ,H
√n

.

Proof. We follow the line of the proof of Theorem 3.2 in [16]. By the mean value theorem, there
f ′(θ)√n z, θ) when z < 0 such

f ′(θ)√n z) when z > 0 or η

(θ, θ + σH

(θ + σH

∈

exists some number η
that

∈

u(z) =

√n
σH (cid:20)

f

(cid:18)

θ +

σH
f ′(θ)√n

z

f (θ)

=

(cid:21)

f ′(η)
f ′(θ)

z.

−

(cid:19)
z. Hence,

Since f (θ) is convex, we have for any z, u(z)

≥

Since the function

Φ(u)

|

Φ(z)
|

−

=

f ′(η)
f ′ (θ)

z

t2
2 dt.

e−

1
√2π

z
Z

f (x, z) = z2e−

x2z2
2

x

|

1

|

−

is uniformly bounded, we have when z < 0,

f ′ (η)
f ′ (θ)

z

1
√2π

z
Z

t2
2 dt

e−

1
√2π |

z

|

≤

z2
2

e−

f ′ (η)
f ′ (θ)

2

(cid:0)

(cid:1)

f ′(η)
f ′(θ) −

1

(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤

(cid:12)
(cid:12)
(cid:12)
(cid:12)

Cθ,H
z

|

|

.

12

Y. CHEN, L. TIAN, AND Y. LI

Thus, we obtain that

sup
θ<z

≤−

f ′(θ)√n
2σH

θ |

f ′ (θ)√n
σH

−

Φ(u)

Φ(z)

| ≤

−

Cθ,H
√n

.

When

f ′(θ)√n
2σH

θ < z < f ′(θ)√n

θ

2σH

−

∧
change of variable t = z2s, together with the fact that f2(s, z) = z2e−
bounded, we conclude that there exists a number

−

(cid:16)

(cid:17)

(1

θ)

, using the mean value theorem and making the

s2z4
2

is also uniformly

such that

η1 ∈

(θ, η)

⊂

[θ, θ +

1
2

θ

(cid:16)

(1

∧

−

θ)

]

(cid:17)

f ′ (η)
f ′ (θ)

z

t2
2 dt =

e−

f ′ (η)
f ′ (θ)

1
z

z2e−

s2 z4

2 ds

z
Z

1
z

Z

Cθ,H

Cθ,H

Cθ,H
√n

≤

≤

≤

1
z
1
z

|

|
,

f ′(η)

|

f ′(θ)
|

−

f ′′(η1)
|

|

z
|
|
√n

|

|

since the second derivative function f ′′ is bound on the close interval [θ, θ + 1
2

θ

When z

f ′(θ)√n
2σH

≥

(1

θ

∧

−

(cid:16)

1
√2π

θ)

,

(cid:17)
f ′(η)
f ′ (θ)

z

z
Z

t2
2 dt

e−

1
√2π

≤

z
Z

∞

t2
2 dt

e−

Cθ,H
√n

.

≤

(cid:16)

(1

∧

−

θ)

] .

(cid:17)

(cid:3)

Acknowledgements: This research is partly supported by NSFC (No.11961033).

References

[1] Anderson, T. W. and Taylor, J. B. 1979. Strong consistency of least squares estimates in dynamic models.

Ann. Statist. 7(3): 484-489.

[2] Lai, T. L. and Wei, C. Z. 1983. Asymptotic properties of general autoregressive models and strong consistency

of least squares estimates of their parameters. J. Multivar. Anal. 13(1): 1-23.

[3] Zhang, R. M. and Ling, S. Q. 2015. Asymptotic inference for AR models with heavy-tailed G-GARCH noises.

Econometric Theory, 31(4): 880-890.

[4] Brouste, A. Cai, C. and Kleptsyna M. 2014. Asymptotic properties of the MLE for the autoregressive process

coeﬃcients under stationary Gaussian noise. Mathematical Methods of Statistics, 23(2): 103-115.

[5] Brouste, A., Cai, C., Soltane, M. Wang, L. 2020. Testing for the change of the mean-reverting parameter of

an autoregressive model with stationary Gaussian noise. Stat Inference Stoch Process 23, 301-318.

[6] Beran, J., Feng, Y., Ghosh, S. and Kulik, R. 2013. Long-memory processes: probabilistic properties and

statistical methods. Berlin: Springer-Verlag.

[7] Giraitis, L., Koul, H. L. and Surgailis, D. 1996. Asymptotic normality of regression estimators with long

memory errors. Statistics & Probability Letters, 29(4): 317-335.

SECOND MOMENT ESTIMATOR FOR AR(1) MODEL

13

[8] Zygmund, A. 1968. Trigonometric series. Vol. 1. Cambridge: Cambridge University Press.

[9] Breuer, P. and Major, P. 1983. Central limit theorems for non-linear functionals of Gaussian ﬁelds. J. Mult.

Anal. 13(3): 425-441.

[10] Nourdin, I. and Peccati, G. 2012. Normal approximations with Malliavin calculus: from Stein’s method to

universality. Vol. 192. Cambridge: Cambridge University Press.

[11] Bercu, B., Nourdin, I. and Taqqu, M. 2010. Almost sure central limit theorem on the Wiener space, Stochastic

Process. Appl. 120, 1607-1628

[12] Kloeden, P. and Neuenkirch, A. 2007. The pathwise convergence of approximation schemes for stochastic

diﬀerential equations. LMS J. Comput. Math. 10: 235-253.

[13] Feller, W. 1971. An introduction to probability and its applications. Vol. 2. New York: Wiley.

[14] Cénac, P. and Es-Sebaiy, K. 2015. Almost sure central limit theorems for random ratios and applications to

LSE for fractional Ornstein-Uhlenbeck processes. Probab. Math. Statist. 35(2): 285-300.

[15] Chang, M. N. and Rao, P. V. 1989. Berry-Esseen bound for the Kaplan-Meier estimator. Comm. Statist.

Theory Methods, 18(12): 4647-4664.

[16] Sottinen, T. and Viitasaari, L. 2018. Parameter estimation for the Langevin equation with stationary-

increment Gaussian noise. Stat Inference Stoch Process, 21(3): 569-601.

School of Mathematics and Statistics, Jiangxi Normal University, Nanchang, 330022, Jiangxi,

P. R. China

Email address: zhishi@pku.org.cn

School of Mathematics and Statistics, Jiangxi Normal University, Nanchang, 330022, Jiangxi,

P. R. China

Email address: tianli@jxnu.edu.cn

School of Mathematics and Computational Science, Xiangtan University, Xiangtan, 411105,

Hunan, P. R. China (Corresponding author.)

Email address: liying@xtu.edu.cn

