CREATING SPEECH-TO-SPEECH CORPUS FROM DUBBED SERIES

Massa Baali1, Wassim El-Hajj1, Ahmed Ali2

1American University of Beirut, Computer Science Department, Beirut, Lebanon
2Qatar Computing Research Institute Doha, HBKU, Doha, Qatar

2
2
0
2

r
a

M
7

]
L
C
.
s
c
[

1
v
1
0
6
3
0
.
3
0
2
2
:
v
i
X
r
a

ABSTRACT

Dubbed series are gaining a lot of popularity in recent years with
strong support from major media service providers. Such popu-
larity is fueled by studies that showed that dubbed versions of TV
shows are more popular than their subtitled equivalents. We pro-
pose an unsupervised approach to construct speech-to-speech cor-
pus, aligned on short segment levels, to produce a parallel speech
corpus in the source- and target- languages. Our methodology ex-
ploits video frames, speech recognition, machine translation, and
noisy frames removal algorithms to match segments in both lan-
guages. To verify the performance of the proposed method, we apply
it on long and short dubbed clips. Out of 36 hours TR-AR dubbed se-
ries, our pipeline was able to generate 17 hours of paired segments,
which is about 47% of the corpus. We applied our method on an-
other language pair, EN-AR, to ensure it is robust enough and not
tuned for a speciﬁc language or a speciﬁc corpus. Regardless of the
language pairs, the accuracy of the paired segments was around 70%
when evaluated using human subjective evaluation. The corpus will
be freely available for the research community.

Index Terms— speech to speech, corpus building, parallel speech

1. INTRODUCTION

Clean and large parallel speech corpora constitute a major building
block in developing speech-to-speech translation systems [1]. How-
ever, building such a big corpora is a very costly and lengthy pro-
cess that normally relies on manual labor. Many researchers focused
on creating parallel corpora addressing manual annotations or text
inputs, such as subtitle [2, 3, 4, 5, 6, 7, 8]. Unlike the text cor-
pora, speech corpora are very limited. Thus, the solutions that are
proposed for building parallel speech resources are very few. De-
spite the increasing attention speech-to-speech research has received
in the last few years [1, 9, 10], researchers in the ﬁeld are lacking
publicly available corpora to train data-hungry deep space systems.
Given that there are very few language pairs that are covered, there
is an urgent need for research and investigations in this domain.

Researchers in [11] extracted parallel speech corpora based on
any language pairs from dubbed movies, in which some correspond-
ing prosodic parameters are extracted, making it useful for other re-
search ﬁelds, such as large comparative linguistic and prosodic stud-
ies. The authors, however, applied their version on a very small
corpus (80 minutes), not to mention that they relied on subtitles.
Tsiartas et al. [12] explored a method based on machine learning for
automatically extracting bilingual audio subtitle pairs from movies.
They used raw movie data and deﬁned the long term spectral dis-
tance, subtitles time distance, and subtitle time-stamps to segment
the bilingual speech regions. This work, however, followed a su-
pervised approach. It also depended on subtitles that are manually
tagged which is time consuming.

In this work, we propose an unsupervised approach that takes
a dubbed series input and produces a Speech-to-Speech Corpus in
the respective languages of the dubbed series. We address the fol-
lowing major challenges: (i) removing the noisy voice segments,
such as commercials, from each dubbed version to produce dubbed
versions that have almost the same duration; (ii) reducing the effect
of the background noise or music when matching the speech seg-
ments; and (iii) ﬁnally, carefully optimizing a set of heuristic rules
for segments’ matching, where hyperparameters can be tuned to op-
timize the tradeoff between quality and corpus size. The proposed
approach is language independent in the sense that it accepts dubbed
clips in any two languages and generate the parallel corpus in these
languages. Hence, to demonstrate the efﬁciency of the proposed ap-
proach and its language independence, we test our pipeline on long
TR-AR series and short EN-AR series. We picked TR-AR series
following studies that showed that 85 million Arab viewers watch
Syrian-dubbed Turkish series [13]. Additionally, Arabic is a mor-
phologically complex language with high degree of afﬁxation and
re-ordering when it is compared with English, making it the perfect
language for such studies.

In our study, the original input of the Turkish series was 51 hours
that correspond to 54 hours of the dubbed Arabic version (in the
Arabic version there were a lot of commercials). After cleaning the
versions from the commercials, noise segments, and unrecognized
segments, 36 hours of each dubbed version were produced. After
applying the segments’ matching rules and tuning the hyperparame-
ters to gain good quality parallel corpus, 17 hours of parallel TR-AR
speech corpus was produced. The evaluation that relied on random
samples annotated by bilingual speakers showed a 70% overall ac-
curacy. To demonstrate language independence, we also experiment
with different languages, mainly EN-AR clips, where similar results
were produced.

Our proposed system builds a parallel speech corpora starting
by analyzing the video modality. Table 1 benchmarks our approach
compared to previous studies.

Up to the best of our knowledge, this is the ﬁrst study to use
visual information as well as acoustic information to automate the
process of creating speech to speech corpus.

2. PARALLEL SPEECH CORPUS CONSTRUCTION

The process of creating parallel speech corpus from dubbed series
goes through the following steps: (i) data collection, (ii) voice activ-
ity detection, (iii) segmentation, (iv) automatic speech recognition,
(v) textual machine translation, and ﬁnally (vi) segments matching.
Figure 1 illustrates an overview of this pipeline. Each step of the
pipeline is explained using examples from the TR-AR dubbed se-
ries. The same pipeline could be used for any dubbed series or any
language pairs without losing generality.

 
 
 
 
 
 
Table 1. Benchmarking our studies with previous work.
Language
Any

Speech Bilingual Dub
Y

Y

Y

Paper Unsupervised
Ours

Y

[11]
[12]

[7]
[6]

[5]
[4]

[2]

[3]

Y
N

N
N

N
N

N

Y

Any
English-French

English-Czech,German,Spanish
English and 6 languages
from Indian sub-contents
Czech-English
29 languages

Chinese-English

English

Result
70% overall
accuracy
-
LTSD 41.39%
Subtitle 37.88%
BLEU 18.95
5 votes cast on 65%
of sentences
-
85% correct
alignment
90% annotation
agreement
BLEU 26.3

Y
Y

N
N

N
N

N

N

Y
Y

Y
Y

Y
Y

Y

N

Y
Y

N
N

N
N

N

N

Fig. 1. Pipeline for the Parallel Speech Corpus Construction: data
collection, voice activity detection (VAD), automatic speech recog-
nition (ASR), machine translation (MT).

spond to images from commercial segment will be removed, since
no matching frames exist for them in D2.

Algorithm 1: Proposed procedure to remove noise frames
from the dubbed videos
D1: Dubbed version 1
D2: Dubbed version 2
Result: Clean and matched dubbed videos
Convert D1 and D2 into frames, 30 frames per second;
while there are more frames f in D1 do

Extract frame f at time t from D1
t’ = t
while i <500 do

Extract frame f’ at time t’ from D2
r = skimage(f, f’)
if r ≥ 0.75 then
break;

else

end

t’=t’+1
i=i+1

end
if i==499 then

remove frame f at time t from D1

end
t = t + 1 (move to analyze the next frame in D1)

end

To apply the algorithm on real data, we downloaded three Turk-
ish (TR) - Arabic (AR) dubbed series from YouTube. The three
Turkish series have total durations of 12, 9, and 30 hours, respec-
tively. The corresponding Arabic series have total durations of 13,
10, and 31 hours, respectively. When checking the series manu-
ally, we noticed that the Arabic version contained a lot of commer-
cials, the fact that explains the extra duration of the Arabic series
when compared to the Turkish series. After running Algorithm 1,
we ended up with a matching duration of both series equivalent to
12, 9, and 30 hours respectively. Moving forward, we operate on all
series as one big video; i.e., the new input is a TR-AR dubbed video
pair composed of 51 hours each.

2.1. Data Collection and Video Matching

In this phase, we aim to download and clean a dubbed video in
any language pair.
In some cases, you might be able to get hold
of dubbed videos with equal duration and without any additional
noise segments such as commercials; in which case, the videos are
ready to be processed. In other cases, one or both of the dubbed ver-
sions might include noise segments that should be removed, in order
to end up with a clean version of the dubbed videos. Algorithm 1
takes as input the dubbed versions and cleans them up. The ﬁrst step
in the algorithm is to convert every dubbed video into frames, 30
frames per second. At time t, we pick the frame in D1 and compare
it with 500 consecutive frames in D2 (also starting at t) i.e. we are
searching whether the frame in D1 at time t exists in a 16 seconds
video segment in D2 starting at time t. Every two frames are com-
pared using skimage [14] to compute the mean structural similarity
index between two images. The values returned by skimage are in
the range of [0,1], where a value close to 1 indicates high similarity
between the two images. In case two frames returned a similarity
greater than 0.75, we assume that these two images are similar, and
thus the frame we are investigating is a valid frame, and not a noise
frame. This process is repeated every frame in D1. A frame in D1
with low similarity in the corresponding consecutive frames in D2
is removed. Once done, we run the same algorithm starting with
the other dubbed version. For instance, all frames in D1 that corre-

Frame Matching to Remove Noisy FramesOriginal Video (D1)Dubbed Video (D2)Voice Activity Detection (segment labeling and removing noise and no-Energy segments)Speech RecognitionTranscribe D1’’ and translate it to D2 languageText Similarity using TextBlobD1’D1’’D2’D2’’Transcribe D2’’Hyper-parameters tuningOutput: Matched parallel segments2.2. Voice Activity Detection

Algorithm 1 produced matching dubbed videos without noise
frames, such as commercials. We now extract the audio from the
dubbed video samples at 16Khz. Then, we process the audio ﬁles of
each dubbed videos. We investigate the impact of segmentation us-
ing voice activity detection (VAD). We use the inaSpeechSegmenter
pre-trained model [15] that extract meta-data from each audio ﬁle;
gender, noise, music and noEnergy (silence).

By applying this phase on the TR-AR dubbed series. Figure 2
shows the distribution of the ﬁve labels in the Turkish Dubbed ver-
sion and the Arabic Dubbed version. The frequency of most labels
is similar with the exception of noEnergy. We attribute this to the
Arabic dubbing, where the actors ignore the amount of silence in the
original video while abiding by the scene time.

Fig. 3. Similarity matching between a segment in D1 (ﬁrst dubbed
version) and all segments in D2 (second dubbed version)

The next step is to match one or more segments in the ﬁrst
dubbed version (e.g. TR) to one or more segments in the second
dubbed version (e.g. AR), with the following three possibilities:

1. Matching one segment in the ﬁrst dubbed version to one seg-

ment in the second dubbed version.

2. Matching one long segment in the ﬁrst dubbed version to

many segments in the second dubbed version.

Fig. 2. Frequency of TR-AR labeled segments, y-axis is the number
of segments.

3. Matching many segments in the ﬁrst dubbed version to one

long segment in the second dubbed version.

2.3. Speech Segments Matching

The VAD 2 produced speech segments that are labeled as female,
male, and music. Every segment has a start time, end time, and du-
ration. In this phase, We transcribe and translate each segment using
Google API. It is worth noting that we found that transcription ac-
curacy varies a lot as some audio sections can be very challenging
to recognize. As a result of that, we obtain transcription for ev-
ery segment. We then translate the segments’ transcription of one
dubbed version to the language of the other dubbed version. For
example, we translate the Turkish transcribed segments into Mod-
ern Standard Arabic (MSA). Any segment that was not recognized
by the transcription API is removed. By applying this step on the
TR-AR dubbed versions, we ended up with 36 hours videos that are
candidates for matching. Next, we calculate the similarity between
every translated segment in one dubbed version (e.g.
the Arabic
text translated from the Turkish transcribed segments), and all the
transcribed segments in the other version (e.g. the Arabic text tran-
scribed from the Arabic segments). Since the pipeline comprises of
speech recognition, machins translation as well as the nature of the
dubbing is not verbatim, we decide not to use the standard word er-
ror rate evaluation metric and look for text similarity score between
the source- and target- segments. We use TextBlob [16], a library
which is based on gensim and Fasttext pretrained word2vec model
to measure the distance using cosine similarity. Figure 3 captures
the data structure that is used to record the similarity between every
translated segment in the ﬁrst dubbed version and its corresponding
transcribed version.

To do the matching according to one of three mentioned possibilities,
we devise the following set of matching rules based on the start time,
end time, duration, label, and similarity score of each segment:

• Rule 1: Difference between the segment start time in the ﬁrst
version and the segment start time in the second version, is
below a certain threshold.

• Rule 2: Difference between the segment duration in the ﬁrst
version and the segment duration in the second version, is
below a certain threshold.

• Rule 3: The segments in both versions should have the same

label.

• Rule 4: The similarity score between two segments is above

0.5.

• Rule 5: To combine multiple short segments in one version
and match them with one long segment in another version,
we adopt a sliding window approach that starts with one short
segment abiding by Rule 1, then continues adding more con-
secutive short segments in the same version until the cumula-
tive duration of the short segments abides by Rule 2, and the
similarity score abides by Rule 4.

We run multiple experiments tuning these hyper-parameters
aiming to maximize the corpus size while maintaining a similarity
score above 0.5. We applied the algorithms and rules mentioned
in this section to the TR-AR dubbed video. Table 2 highlights the
application of the proposed methodology on dubbed videos. Every

s11,e11,d11, id11s12,e12,d12, id12s1n,e1n,d1n, id1ns21,e21,d21, id21,sim21Segments of D1 (Transcribed then translated segments)Segments of D2 (Transcribed)s1i,e1i,d1i, id1i......s2m,e2m,d2m, id2m,sim2ms21,e21,d21, id21,sim21s2m,e2m,d2m, id2m,sim2ms1i - start time of segment i in D1e1i - end time of segment i in D1d1i - duration of segment i in D1id1i - ID of segment i in D1s2k - start time of segment k in D2e2k - end time of segment k in D2d2k - duration of segment k in D2id2k - ID of segment k in D2sim2k - similarity between segment with id id1i from D1 with  segment k in D2s2k,e2k,d2k, id2k,sim2ks2k,e2k,d2k, id2k,sim2kInput Duration

Lang.
TR-AR 36 hrs
TR-AR 36 hrs

Table 2. Results when running the pipeline on 36 hours of TR-AR dubbed series
Input Segments Dif Start Time Dif Dur Output Segments Output Duration Avg Similarity
28,800;27,678
28,800;27,678

9,998;9,887
11,581;10,060

14.3hrs
17.6hrs

<=3
<=9

<=2
<=8

0.55
0.54

Table 3. Results when running the pipeline on 11 minutes and 1.2 hours of EN-AR dubbed clips

Input Duration

Lang.
EN-AR 11 mins
EN-AR 11 mins
EN-AR 1.2 hrs
EN-AR 1.2 hrs

Input Segments Dif Start Time Dif Dur Output Segments Output Duration Avg Similarity
61;112
61;112
1000;928
1000;928

10;10
18;18
400;400
650;650

2 mins
4 mins
15 mins
20 mins

<=3
<=9
<=3
<=9

<=2
<=8
<=2
<=8

0.56
0.54
0.55
0.54

Percent
39%
48%

Percent
18%
36%
25%
33%

row in the table presents the original duration (Input Duration) of
the video and the corresponding number of extracted segments (In-
put Segments). Every row also contains values that represent the
hyper-parameters, namely, Dif Start Time and Dif Dur as presented
in Rules 1 and 2. The last four entries in every row indicate the
ﬁnal number of parallel segments (Output Segments), their duration
(Output Duration), their average similarity score (Avg Sim), and the
percentage duration of parallel corpus produced.

As shown in table 2, the difference-start-time threshold of 9 sec-
onds and the difference-duration threshold of 8 seconds, produce
a parallel corpus of duration 17.6 hours; i.e. 48% of the cleaned
dubbed videos can be transformed into a parallel corpus. When
compared to the original dubbed TR-AR videos before cleaning (51
hours), we are able to produce a parallel speech corpus of duration
36% of the original videos.
It is also worth noting that the rules
above produce an appropriate average similarity score when vary-
ing the thresholds, while being able to extract a good percentage of
parallel corpus duration.

To evaluate whether the proposed approach generalizes well to
other languages, we applied the same pipeline on EN-AR dubbed
clips without any tuning. We chose two clips; short one, 11 min-
utes long, and another longer clip, 1.2 hours long. Using the same
hyperparameters that gave the best results on the TR-AR dubbed
series (difference-start-time = 9, and difference-duration = 8), the
algorithms produced 4 minutes and 20 minutes of parallel speech
segments respectively. On average, 35% of the original clips time.
Table 3 captures the results. We can cautiously claim that the pro-
posed pipeline produces around 35% of parallel speech duration with
respect to the duration of the original dubbed input, regardless of
the language. We next evaluate the quality of the produced parallel
speech corpus.

Fig. 4. Examples for Human Evaluation from TR-AR Corpus.

3. EVALUATION

Finally, we conduct subjective evaluation using mean opinion score
MOS on the segments matching. We recruited two annotators and

Table 4. Label distribution for each score.

Score
1
0.5
0

total
512
163
325

female male music
154
265
53
88
102
173

93
22
50

we asked them to rate the segments match on 3-point scale; 1 for ex-
act match, 0.5 for semantic match and 0 for no match at all. To make
sure that the annotation process is consistent, we asked the workers
to annotate 100 samples from each of the two corpora TR-AR and
EN-AR. We used Cohen’s kappa coefﬁcient to measure the Inter-
Annotator Agreement. The Kappa score was 0.79 for the TR-AR
segments, and 0.75 for the EN-AR segments indicating substantial
agreement. For the TR-AR segments, a random sample of 1, 000
speech segments were given to the two bilingual annotators. The
duration of the segments ranged from 2 to 10 seconds. Figure 4
shows example for scoring for the following three criteria: (i) score
1 if the pairs should hold the same exact meaning; score (ii) 0.5 if
the pairs are semantically matched, the video segments should be
almost identical. It is worth noting, that semantic matching varies
from country/language to another due to exaggeration applied by the
dubbers in particular scenes to ﬁt the culture of that country in terms
of way of speaking. In the example shown in table 4, it represents a
scene for a person riding his bike and by accident he collided with
a pedestrian. The expression was acted differently in each language
but they stills hold the same meaning for that scene and referring to
the same video section. (iii) Finally, score 0 means that the pair is not
matching. Table 4 shows the agreement results, where 512 (52%)
segment pairs were identical, 163 (17%) segment pairs are seman-
tically similar, and 352 (31%) segments pairs were not similar. We
can argue that the overall the similarity is around 70% given that 0.5
is a correct match as far as the dubbed corpus is concerned.We did
the same exercises for the EN-AR segments and we achieved around
70% segment match; both identical and semantic match.

4. CONCLUSION

In this work, we introduced an unsupervised approach to creating
parallel speech corpora from dubbed videos. Unlike existing ap-
proaches that are either supervised, or unsupervised but inefﬁcient,
the proposed approach can be tuned to produce large parallel speech
corpora with high quality. For future work, we plan to study the
effectiveness of the approach on dubbed clips in various languages.

  Examples for subjective evaluation of the speech-to-speech corpus  Score Match  Turkish-Sentence Arabic-Sentence 1 Exact match  Ne kadar g ̈uzelsin Bo ̆gazic ̧i  روفسوب اي كلاحا ام English Translation How beautiful you are Bosporus 0.5 Semantic match  aman allahım  فاش كلام وش English Translation  Oh my goodness! Couldn’t you see this? 0 No match ne yapıyorsun  ةليضف بحارم English Translation  what are you doing Hello Fadileh  5. REFERENCES

[1] Y Jia, RJ Weiss, F Biadsy, W Macherey, M Johnson,
Z Chen, and Y Wu,
“Direct speech-to-speech translation
with a sequence-to-sequence model (2019),” arXiv preprint
arXiv:1904.06037, 1904.

[2] W. Feng, H. Ren, X. Li, and H. Guo, “Building a parallel
corpus with bilingual discourse alignment,” in Workshop on
Chinese Lexical Semantics, 2017.

[3] T. Kajiwara and M. Komachi, “Building a monolingual parallel
corpus for text simpliﬁcation using sentence similarity based
on alignment between word embeddings,” in Proceedings of
COLING 2016, the 26th International Conference on Compu-
tational Linguistics: Technical Papers, 2016.

[4] J. Tiedemann, “Improved sentence alignment for building a
parallel subtitle corpus: Building a multilingual parallel subti-
tle corpus,” LOT Occasional Series, 2007.

[5] J. Cuˇr´ın, M. ˇCmejrek, J. Havelka, and V. Kuboˇn, “Building a
parallel bilingual syntactically annotated corpus,” in Interna-
tional Conference on Natural Language Processing, 2004.

[6] M. Post, C. Callison-Burch, and M. Osborne, “Constructing
parallel corpora for six indian languages via crowdsourcing,”
in Proceedings of the Seventh Workshop on Statistical Machine
Translation, 2012.

[7] E. Avramidis, M. Ruiz Costa-Juss`a, C. Federmann, M. Melero,
“A richly annotated, mul-
P. Pecina, and J. Van Genabith,
tilingual parallel corpus for hybrid machine translation,” in
LREC’12, 2012.

[8] R. Cattoni, M. A. Di Gangi, L. Bentivogli, M. Negri, and
“Must-c: A multilingual corpus for end-to-end

M. Turchi,
speech translation,” Computer Speech & Language, 2021.

[9] T. Kano, S. Sakti, and S. Nakamura, “Structured-based cur-
riculum learning for end-to-end english-japanese speech trans-
lation,” arXiv preprint arXiv:1802.06003, 2018.

[10] T. Kano, S. Sakti, and N., “Transformer-based direct speech-

to-speech translation with transcoder,” in SLT, 2021.

[11] A. ¨Oktem, M. Farr´us, and L. Wanner, “Automatic extraction of
parallel speech corpora from dubbed movies,” in ACL, 2017.

[12] A. Tsiartas, P. Ghosh, P. Georgiou, and S Narayanan, “Bilin-
gual audio-subtitle extraction using automatic segmentation of
movie audio,” in ICASSP, 2011.

[13] A. Buccianti, “Dubbed turkish soap operas conquering the arab
world: social liberation or cultural alienation?,” Arab Media
and Society, 2010.

[14] S. Van der Walt, Johannes L Sch¨onberger, J. Nunez-Iglesias,
Franc¸ois Boulogne, J. D. Warner, N. Yager, E. Gouillart, and
image processing in python,” PeerJ,
T. Yu, “scikit-image:
2014.

[15] D. Doukhan, J. Carrive, F. Vallet, A. Larcher, and S. Meignier,
“An open-source speaker gender detection framework for mon-
itoring gender equality,” in ICASSP, 2018.

[16] S. Loria, “textblob documentation,” Release 0.15, vol. 2, 2018.

