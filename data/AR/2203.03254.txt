Augmented Reality and Robotics: A Survey and Taxonomy for
AR-enhanced Human-Robot Interaction and Robotic Interfaces
Adnan Karim
University of Calgary
Calgary, AB, Canada
adnan.karim@ucalgary.ca

Ryo Suzuki
University of Calgary
Calgary, AB, Canada
ryo.suzuki@ucalgary.ca

Tian Xia
University of Calgary
Calgary, AB, Canada
tian.xia2@ucalgary.ca

2
2
0
2

r
a

M
7

]

O
R
.
s
c
[

1
v
4
5
2
3
0
.
3
0
2
2
:
v
i
X
r
a

Hooman Hedayati
UNC at Chapel Hill
Chapel Hill, NC, U.S.A.
hooman@cs.unc.edu

Nicolai Marquardt
University College London
London, U.K.
n.marquardt@ucl.ac.uk

Figure 1: Visual abstract of our survey and taxonomy of augmented reality interfaces used with robotics, summarizing eight
key dimensions of the design space. All sketches and illustrations are made by the authors (Nicolai Marquardt for Figure 1
and 3 and Ryo Suzuki for Figure 3-15) and are available under CC-BY 4.0 L M with the credit of original citation. All materials
and an interactive gallery of all cited papers are available at https://ilab.ucalgary.ca/ar-and-robotics/

ABSTRACT
This paper contributes to a taxonomy of augmented reality and
robotics based on a survey of 460 research papers. Augmented and
mixed reality (AR/MR) have emerged as a new way to enhance
human-robot interaction (HRI) and robotic interfaces (e.g., actuated
and shape-changing interfaces). Recently, an increasing number of
studies in HCI, HRI, and robotics have demonstrated how AR en-
ables better interactions between people and robots. However, often
research remains focused on individual explorations and key design
strategies, and research questions are rarely analyzed systemati-
cally. In this paper, we synthesize and categorize this research field
in the following dimensions: 1) approaches to augmenting reality; 2)

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
CHI ’22, April 29-May 5, 2022, New Orleans, LA, USA
© 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-1-4503-9157-3/22/04. . . $15.00
https://doi.org/10.1145/3491102.3517719

characteristics of robots; 3) purposes and benefits; 4) classification
of presented information; 5) design components and strategies for
visual augmentation; 6) interaction techniques and modalities; 7)
application domains; and 8) evaluation strategies. We formulate key
challenges and opportunities to guide and inform future research
in AR and robotics.

CCS CONCEPTS
• Human-centered computing → Mixed / augmented reality;
• Computer systems organization → Robotics.

KEYWORDS
survey; augmented reality; mixed reality; robotics; human-robot
interaction; actuated tangible interfaces; shape-changing interfaces

ACM Reference Format:
Ryo Suzuki, Adnan Karim, Tian Xia, Hooman Hedayati, and Nicolai Mar-
quardt. 2022. Augmented Reality and Robotics: A Survey and Taxonomy
for AR-enhanced Human-Robot Interaction and Robotic Interfaces. In CHI
Conference on Human Factors in Computing Systems (CHI ’22), April 29-
May 5, 2022, New Orleans, LA, USA. ACM, New York, NY, USA, 33 pages.
https://doi.org/10.1145/3491102.3517719

àReducing cognitive loadàAR-based Safety Trade-offsàTechnological challengesàBridging gap between studies and systemsàIn-the-wild deploymentsàNew techniques•Domesticand everyday use (32)•Industry(122)•Entertainment(30)•Educationand training (15)•Socialinteraction (18)•Designand creative tasks (10)•Medicaland health (15)•Remote collaboration(6)•Mobilityand transportation (11)•Searchand rescue (20)•Workspaceand knowledge work (9)•Dataphysicalization (12)Opportunities/ChallengesResearch Field of Augmented Reality and RoboticsOn-Body7| Applications1 | Approaches to augmenting reality for HRIAugment robotsAugment surroundingsOn-EnvironmentOn-Robot2 | Characteristics of augmented robotsForm factorRelationshipScaleProximity3 | Purposes and benefitsProgramming, controlFacilitate programmingUnderstanding, interpretation, communicationReal-time controlImprove safetyCommunicate intentIncrease expressiveness4 | Types of information•Demonstration(96)•Technicalevaluation (73)•User studies (96)8| Evaluation strategiesInternal informationExternal informationPlan and activitySupplemental contentTangibleTouchControllerGestureVoiceProximityGaze6| Interaction modalities5 | Design components and strategiesUser interface and widgetsMenusSpatial references and visualizationsEmbedded visual effectsInformation panelsLabels and annotationsControls and handlesMonitors and displaysPoints and locationPaths and trajectoriesAreas and boundariesOther visualizationsAnthropomorphic effectsVirtual replicas and ghost effectsTexture mapping effectsLocation where augmentation device is positioned:Target location of visual augmentation: 
 
 
 
 
 
1 INTRODUCTION
As robots become more ubiquitous, designing the best possible
interaction between people and robots is becoming increasingly
important. Traditionally, interaction with robots often relies on the
robot’s internal physical or visual feedback capabilities, such as
robots’ movements [106, 280, 426, 490], gestural motion [81, 186,
321], gaze outputs [22, 28, 202, 307], physical transformation [170],
or visual feedback through lights [34, 60, 403, 427] or small dis-
plays [129, 172, 446]. However, such modalities have several key
limitations. For example, the robot’s form factor cannot be easily
modified on demand, thus it is often difficult to provide expres-
sive physical feedback that goes beyond internal capabilities [450].
While visual feedback such as lights or displays can be more flexible,
the expression of such visual outputs is still bound to the fixed phys-
ical design of the robot. For example, it can be challenging to present
expressive information given the fixed size of a small display, where
it cannot show the data or information associated with the physical
space that is situated outside the screen. Augmented reality (AR)
interfaces promise to address these challenges, as AR enables us to
design expressive visual feedback without many of the constraints
of physical reality. In addition, AR can present visual feedback in
one’s line of sight, tightly coupled with the physical interaction
space, which reduces the user’s cognitive load when switching the
context and attention between the robot and an external display.
Recent advances in AR opened up exciting new opportunities for
human-robot interaction research, and over the last decades, an
increasing number of works have started investigating how AR can
be integrated into robotics to augment their inherent visual and
physical output capabilities. However, often these research projects
are individual explorations, and key design strategies, common
practices, and open research questions in AR and robotics research
are rarely analyzed systematically, especially from an interaction
design perspective. With the recent proliferation of this research
field, we see a need to synthesize the existing works to facilitate
further advances in both HCI and robotics communities.

In this paper, we review a corpus of 460 papers to synthesize the
taxonomy for AR and robotics research. In particular, we synthe-
sized the research field into the following design space dimensions
(with a brief visual summary in Figure 1): 1) approaches to aug-
menting reality for HRI; 2) characteristics of augmented robots;
3) purposes and benefits of the use of AR; 4) classification of pre-
sented information; 5) design components and strategies for visual
augmentation; 6) interaction techniques and modalities; 7) applica-
tion domains; and 8) evaluation strategies. Our goal is to provide
a common ground and understanding for researchers in the field,
which both includes AR-enhanced human-robot interaction [151]
and robotic user interfaces [37, 218] research (such as actuated tangi-
ble [349] and shape-changing interfaces [16, 87, 359]). We envision
this paper can help researchers situate their work within the large
design space and explore novel interfaces for AR-enhanced human-
robot interaction (AR-HRI). Furthermore, our taxonomy and de-
tailed design space dimensions (together with the comprehensive
index linking to related work) can help readers to more rapidly
find practical AR-HRI techniques, which they can then use, iterate
and evolve into their own future designs. Finally, we formulate

open research questions, challenges, and opportunities to guide
and stimulate the research communities of HCI, HRI, and robotics.

2 SCOPE, CONTRIBUTIONS, AND

METHODOLOGY

2.1 Scope and Definitions
The topic covered by this paper is “robotic systems that utilize AR
for interaction”. In this section, we describe this scope in more detail
and clarify what is included and what is not.

2.1.1 Human-Robot Interaction and Robotic Interfaces. “Robotic sys-
tems” could take different forms—from traditional industrial robots
to self-driving cars or actuated user interfaces. In this paper, we
do not limit the scope of robots and include any type of robotic or
actuated systems that are designed to interact with people. More
specifically, our paper also covers robotic interface [37, 218] research.
Here, robotic interfaces refer to interfaces that use robots and/or
actuated systems as a medium for human-computer interaction 1.
This includes actuated tangible interfaces [349], adaptive environ-
ments [154, 399], swarm user interfaces [235], and shape-changing
interfaces [16, 87, 359].

2.1.2 AR vs VR. Among HRI and robotic interface research, we
specifically focus on AR, but not on VR. In the robotics literature,
VR has been used for many different purposes, such as interactive
simulation [173, 276, 277] or haptic environments [291, 421, 449].
However, our focus is on visual augmentation in the real world
to enhance real robots in the physical space, thus we specifically
investigate systems that uses AR for robotics.

2.1.3 What is AR. The definition of AR can also vary based on the
context [405]. For example, Azuma defines AR as “systems that have
the following three characteristics: 1) combines real and virtual, 2)
interactive in real time, 3) registered in 3D” [32]. Milgram and Kishino
also describe this with the reality-virtuality continuum [293]. More
broadly, Bimber and Rasker [41] also discuss spatial augmented
reality (SAR) as one of the categories in AR. In this paper, we
take AR as a broader scope and include any systems that augment
physical objects or surroundings environments in the real world,
regardless of the technology used.

2.2 Contributions
Augmented reality in the field of robotics has been the scope of
other related review papers (e.g., [100, 155, 281, 356]) that our tax-
onomy expands upon. Most of these earlier papers reviewed key
application use cases in the research field. For example, Makhataeva
and Varol surveyed example applications of AR for robotics in a
5-year timeframe [281] and Qian et al. reviewed AR applications for
robotic surgery in particular [356]. From the HRI perspective, Green
et al. provide a literature review research for collaborative HRI [155],
which focuses in particular on collaboration through the means
AR technologies. And more recently, human-robot interaction and
VR/MR/AR (VAM-HRI) as also been the topic of workshops [466].
Our taxonomy builds on and extends beyond these earlier re-
views. In particular, we provide the following contributions. First,

1We only cover internally actuated systems but do not cover externally actuated
systems, which actuate passive objects with external force [298, 331, 339, 422].

2

Figure 2: Examples of augmented reality and robotics research: A) collaborative programming [33], B) augmented arms for
social robots [158], C) drone teleoperation [171], D) a drone-mounted projector [58], E) projected background for data phys-
icalization [425], F) drone navigation for invisible areas [114], G) trajectory visualization [450], H) motion intent for pedes-
trian [458], I) a holographic avatar for telepresence robots [198], J) surface augmentation for shape displays [243].

we present a taxonomy with a novel set of design space dimen-
sions, providing a holistic view based on the different dimensions
unifying the design space, with a focus on interaction and visual
augmentation design perspectives. Second, our paper also systemati-
cally covers a broader scope of HCI and HRI literature, including
robotic, actuated, and shape-changing user interfaces. This field is
increasingly popular in the field of HCI, [16, 87, 349, 359] but not
well explored in terms of the combination with AR. By incorporat-
ing this research, our paper provides a more comprehensive view to
position and design novel AR/MR interactions for robotic systems.
Third, we also discuss open research questions and opportuni-
ties that facilitate further research in this field. We believe that
our taxonomy — with the design classifications and their insights,
and the articulation of open research questions — will be invalu-
able tools for providing a common ground and understanding when
designing AR/MR interfaces for HRI. This will help researchers iden-
tify or explore novel interactions. Finally, we also compiled a large
corpus of research literature using our taxonomy as an interactive
2, which can provide a more content-rich, up-to-date, and
website
extensible literature review. Inspired by similar attempts in personal
fabrication [5, 36], data physicalization [1, 192], and material-based
shape-changing interactions [6, 353], our website, along with this
paper, could provide similar benefits to the broader community of
both researchers and practitioners.

2.3 Methodology
2.3.1 Dataset and Inclusion Criteria. To collect a representative set
of AR and robotics papers, we conducted a systematic search in the
ACM Digital Library, IEEE Xplore, MDPI, Springer, and Elsevier.
Our search terms include the combination of “augmented reality”
AND “robot” in the title and/or author keywords since 2000. We also
searched for synonyms of each keyword, such as “mixed reality”,
“AR”, “MR” for augmented reality and “robotic”, “actuated”, “shape-
changing” for robot. This gave us a total of 925 papers after removing
duplicates. Then, four authors individually looked at each paper
to exclude out-of-scope papers, which, for example, only focus on

2https://ilab.ucalgary.ca/ar-and-robotics/

3

AR-based tracking but not on visual augmentation, or were concept
or position papers, etc. After this process, we obtained 396 papers
in total. To complement this keyword search, we also identified an
additional relevant 64 papers by leveraging the authors’ expertise
in HCI, HRI, and robotic interfaces. By merging these papers, we
finally selected a corpus of 460 papers for our literature review.

While our systematic compilation of this corpus provides an
in-depth view into the research space, this set can not be a com-
plete or exhaustive list in this domain. The boundaries and scope
of our corpus may not be clear cut, and as with any selection of
papers, there were many papers right at the boundaries of our
inclusion/exclusion criteria. Nevertheless, our focus was on the de-
velopment of a taxonomy and this corpus serves as a representative
subset of the most relevant papers. We aim to address this inher-
ent limitation of any taxonomy by making our coding and dataset
open-source, available for others to iterate and expand upon.

2.3.2 Analysis and Synthesis. The dataset was analyzed through
a multi-step process. One of the authors conducted open-coding
on a small subset of our sample to identify a first approximation
of the dimensions and categories within the design space. Next,
all authors reflected upon the initial design space classification
to discuss the consistency and comprehensiveness of the catego-
rization methods, where then categories were merged, expanded,
and removed. Next, three other co-authors performed systematic
coding with individual tagging for categorization of the complete
dataset. Finally, we reflected upon the individual tagging to resolve
the discrepancies to obtain the final coding results.

In the following sections, we present our results and findings of
this classification by using color-coded text and figures. We provide
a list of key citations directly within the figures, with the goal of
facilitating lookup of relevant papers within each dimension and all
of the corresponding sub-categories. Furthermore, in the appendix
of this paper we included several tables with a complete compilation
of all citations and count of the papers in our corpus that fall within
each of the categories and sub-categories of the design space —
which we hope will help researchers to more easily find relevant
papers (e.g., finding all papers that use AR for "improving safety"

Figure 3: Approaches to augmenting reality in robotics.

with robots, "augment surroundings" of robots, or provide visual
feedback of "paths and trajectories").

walls [431] or handheld shape-changing interfaces [258, 374] are
also directly augmented with the overlaid animation of information.

3 APPROACHES TO AUGMENTING REALITY

IN ROBOTICS

In this section, we discuss the different approaches to augmenting
reality in robotics (Figure 3). To classify how to augment reality, we
propose to categorize based on two dimensions: First, we categorize
the approaches based on the placement of the augmented real-
ity hardware (i.e., where the optical path is overridden with digital
information). For our purpose, we adopt and extend Bimber and
Raskar’s [41] classification in the context of robotics research. Here,
we propose three different locations: 1) on-body, 2) on-environment,
and 3) on-robot. Second, we classify based on the target location of
visual augmentation, i.e., where is augmented. We can categorize
this based on 1) augmenting robots or 2) augmenting surroundings.
Given these two dimensions, we can map the existing works into
the design space (Figure 3 Right). Walker et al. [450] include aug-
menting user interface (UI) as another category. Since the research
that has been done in this area can be roughly considered augment-
ing the environment, we decided to not include it as a separate
category.

Approach-1. Augment Robots: AR is used to augment robots
themselves by overlaying or anchoring additional information on
top of the robots (Figure 3 Top).

— On-Body: The first category augments robots through on-body AR
devices. This can be either 1) head-mounted displays (HMD) [197,
372, 450] or 2) mobile AR interfaces [76, 209]. For example, VRoom [197,
198] augments the telepresence robot’s appearance by overlaying a
remote user. Similarly, Young et al. [481] demonstrated adding an
animated face onto a Roomba robot to show an expressive emotion
on mobile AR devices.

— On-Environment: The second category augments robots with
devices embedded in the surrounding environment. Technologies
often used with this approach include 1) environment-attached pro-
jectors [21] or 2) see-through displays [243]. For example, Drone-
SAR [96] also shows how we can augment the drone itself with pro-
jection mapping. Showing the overlaid information on top of robotic
interfaces can also fall into this category. Similarly, shape-shifting

4

— On-Robot: In the third category, the robots augment their own
appearance, which is unique in AR and robotics research, compared
to Bimber and Raskar’s taxonomy [41]. For example, Furhat [14] an-
imates a face with a back-projected robot head, so that the robot can
augment its own face without an external AR device. The common
technologies used are robot-attached projectors [418, 436], which
augments itself by using its own body as a screen. Alternatively,
robot-attached displays can also fall into this category [445, 475].

Approach-2. Augment Surroundings: Alternatively, AR is also
used to augment the surroundings of the robots. Here, the sur-
roundings include 1) surrounding mid-air 3D space, 2) surrounding
physical objects, or 3) surrounding physical environments, such as
wall, floor, ceiling, etc (Figure 3 Bottom).

— On-Body: Similarly, this category augments robots’ surroundings
through 1) HMD [372, 450], 2) mobile AR devices [76], or 3) hand-
held projector [177]. One benefit of HMD or mobile AR devices is an
expressive rendering capability enabled by leveraging 3D graphics
and spatial scene understanding. For example, Drone Augmented
Human Vision [114] uses HMD-based AR to change the appearance
of the wall for remote control of drones. RoMA [341] uses HMD
for overlaying the interactive 3D models on a robotic 3D printer.

— On-Environment: In contrast to HMD or handheld devices, the
on-environment approach allows much easier ways to share the
AR experiences with co-located users. Augmentation can be done
through 1) projection mapping [425] or 2) surface displays [163].
For example, Touch and Toys [163] leverage a large surface dis-
play to show additional information in the surroundings of robots.
Andersen et al. [21] investigates the use of projection mapping
to highlight or augment surrounding objects to communicate the
robot’s intentions. While it allows the shared content for multiple
people, the drawback of this approach is a fixed location due to the
requirements of installed-equipment, which may limit the flexibility
and mobility for outdoor scenarios.

— On-Robot: In this category, the robots themselves augment the sur-
rounding environments. We identified that the common approach
is to utilize the robot-attached projector to augment surrounding
physical environments [210, 382]. For example, Kasetani et al. [210]

On-bodyHead-based, eye-wornOn-environmentOn-robotHand-heldSpatialAttached to robotHead-mounted displayHead-worn projectorHand-held projectorProjector in spaceAugmented RobotAugmented SurroundingsHand-held display, Mobile ARProjector attached to robotSee-through display on robotSee-through window in spacePlacement of augmented reality hardware:Augment RobotsAugment SurroundingsHead-based, Eye-wornHandheldSpatialAttached to robotOn-BodyRoMA [341]Mobile AR Interface [133]PATI [140]Self-Actuated Projector [112][27, 33, 51, 171, 182, 262, 270, 325, 341, 358, 372, 450, 451, 479][133, 177, 305, 407, 494][21, 33, 140, 163, 169, 178, 191, 212, 221, 346, 369, 425, 467][91, 112, 188, 210, 257, 261, 300, 320, 370, 382, 445, 458]VRoom [198]Cartoon Face [481]Shape-shifting Walls [431]Furhat [14][76, 158, 197, 198, 209, 262, 285, 302, 372, 443, 450, 479, 481][53, 133, 153, 209, 481][21, 96, 116, 127, 159, 243, 258, 374, 430, 431, 472][14, 210, 382, 418, 436, 445, 475]On-EnvironmentOn-Robot12attach a projector to a mobile robot to make a self-propelled projec-
tor for ubiquitous displays. Moreover, DisplayDrone [382] shows
a projected image onto the surrounding walls for on-demand dis-
plays. The main benefit of this approach is that the user does not
require any on-body or environment-instrumented devices, thus it
enables mobile, flexible, and deployable experiences for different
situations.

Figure 4: Characteristics of augmented robots.

4 CHARACTERISTICS OF AUGMENTED

ROBOTS

Next, we classify research projects based on the characteristics
of augmented robots. Possible design space dimensions span 1)
the form factor of robots, 2) the relationship between the users
and robots, 3) size and scale of the robots, and 4) proximity for
interactions (Figure 4).

Dimension-1. Form Factor: This category includes the types of
robots that have been investigated in the literature. The form factor
of robots include: robotic arms [341, 354], drones [58, 171], mobile
robots [197, 468], humanoid robots [254, 439], vehicles [300, 320],
actuated objects [159, 443], the combination of multiple form fac-
tors [169], and other types such as fabrication machines [304, 476].

Dimension-2. Relationship: Research also explores different people-
to-robot relationships. In the most common case, one person inter-
acts with a single robot (1:1), but the existing research also explores
a situation where one person interacts with multiple robots (1:m).
AR for swarm robots falls into this category [176, 235, 328, 425].
On the other hand, collaborative robots require multiple people to

5

interact with a single robotic interface (n:1) [430] or a swarm of
robots (n:m) [328].

Dimension-3. Scale: Augmented robots are of different sizes,
along a spectrum from small to large: from a small handheld-scale
which can be grasped with a single hand [425], tabletop-scale which
can fit onto the table [257], and body-scale which is about the same
size as human bodies like industrial robotic arms [27, 285]. Large-
scale robots are possible, such as vehicles [2, 7, 320] or even building
construction robots.

Dimension-4. Proximity: Proximity refers to the distance be-
tween the user and robots when interaction happens. Interactions
can vary across the dimension of proximity, from near to far. The
proximity can be classified as the spectrum between 1) co-located
or 2) remote. The proximity of the robots can influence whether the
robots are directly touchable [227, 316] or situated in distance [96].
It can also affect how to augment reality, based on whether the
robots are visible to the user [171] or out-of-sight for remote inter-
action [114].

5 PURPOSES AND BENEFITS OF VISUAL

AUGMENTATION

Visual augmentation has many benefits for effective human-robot
interaction. In this section, we categorize the purposes of why vi-
sual augmentation is used in robotics research. On a higher level,
purposes and benefits can be largely categorized as 1) for program-
ming and control, and 2) for understanding, interpretation, and
communications (Figure 5).

Purpose-1. Facilitate Programming: First, the AR interface pro-
vides a powerful assistant to facilitate programming robots [33].
One way to facilitate the programming is to simulate programmed
behaviors [162], which has been explored since early 1990s [32, 219,
294]. For example, GhostAR [51] shows the trajectory of robots to
help the user see how the robots will behave. Such visual simulation
helps the user to program the robots in industry applications [358]
or home automation [263]. Another aspect of programming assis-
tance is to directly map with the real world. Robot programming
often involves interaction with real-world objects, and going back
and forth between physical and virtual worlds is tedious and time-
consuming. AR interfaces allow the user to directly indicate objects
or locations in the physical world. For example, Gong et al. [150] uti-
lizes projection-based AR to support the programming of grasping
tasks.

Purpose-2. Support Real-time Control and Navigation: Simi-
lar to the previous category, AR interfaces facilitate the control, nav-
igation, and teleoperation of the robot. In contrast to programming
the behaviors, this category focuses on the real-time operation of
the robot, either remote or co-located. For example, exTouch [209]
and PinpointFly [76] allows the user to interactively control robots
with the visual feedback on a touch screen. AR interfaces also sup-
port showing additional information or parameters related to the
navigation and control. For example, a world-in-miniature of the
physical world [4] or real-time camera view [171] is used to support
remote navigation of drones.

Robotic Arms[21, 25, 27, 33, 51, 59, 79, 105, 133, 136, 140, 153, 182, 262, 270, 282, 285, 325, 326, 341, 354, 358, 372]Drones[15, 58, 76, 96, 114, 171, 261, 332, 382, 436, 450, 451, 475, 482, 494]Mobile Robots[53, 91, 112, 163, 169, 177, 191, 197, 209, 210, 212,221, 263, 305, 328, 346, 370, 407, 418, 445, 467, 468]Humanoid Robots[14, 29, 59, 158, 183, 254, 262, 372, 429, 439, 440]Vehicles[2, 7, 223, 300, 314, 320, 438, 458, 495]Actuated Objects[116, 117, 127, 159, 167, 243–245, 258, 431, 443, 472, 473]Combinations[29, 52, 53, 66, 98, 101, 117, 136, 169, 177, 209, 225, 327]Other Types[31, 65, 86, 136, 206, 239, 304, 337, 338, 443, 476]1 : 1[25, 30, 33, 54, 67, 76, 103, 118, 133, 153, 216, 226, 228, 251, 252, 310, 313, 329, 358, 361, 450, 481, 482, 494]1 : m[131, 142, 145, 163, 176, 177, 212, 235, 246, 328, 416, 425]n : 1[31, 112, 159, 275, 317, 338, 345, 364, 369, 382, 430]n : m[200, 221, 328, 408, 415, 431]Small[86, 179, 258, 328, 374, 425, 443][11, 14, 18, 19, 49, 55, 116, 117, 127, 130, 136, 163, 167, 193, 221, 242, 244, 245, 257, 335, 340, 407, 408, 429, 445, 451][29, 158, 162, 191, 210, 220, 229, 230, 262, 272, 285, 289, 315, 337, 342, 346, 350, 370, 372, 406, 429, 464, 467]Large[2, 7, 320, 458, 475]NearFarRelationshipScaleProximityForm FactorHandheld-scale [179]Tabletop-scale [257]Body-scale [289]Building/City-scale [2][31, 85, 98, 116, 125, 128, 159, 167, 227, 243, 245, 270, 289, 290, 316, 328, 340, 355, 369, 379, 430, 443, 472, 473][33, 93, 96, 98, 102, 108, 115, 137, 140, 141, 158, 165, 171, 180, 271, 315, 357, 377, 393, 440, 443, 451, 464, 468, 469, 481][15, 38, 58, 62, 76, 114, 163, 184, 227, 317, 382, 436, 450, 475, 482, 494][4, 101, 149, 169, 212, 263, 313, 319, 334, 342, 377, 417, 436]Co-located [98]Co-located with Distance [451]Semi-Remote [114]Remote [377]Common HRI [153]Swarm Robots [131]Collaboration (Single Robot) [430]Collaboration (Multiple Robots) [328]1234On-Vehicle Projector [320]Actuated Pin Display [245]Mobile Robotic Arm [98]Augmented Laser Cutter [304]AR Control of Robotic Arm [79]DroneSAR [96]Mobile Robot Path Planning [468]MR Interaction with Pepper [440]Lorem ipsum dolor sit amet, Figure 5: Purposes and benefits of visual augmentation.

Purpose-3. Improve Safety: By leveraging visual augmentation,
AR/MR interfaces can improve safety awareness when interacting
with robots. For example, Safety Aura Visualization [282] explores
spatial color mapping to indicate the safe and dangerous zones.
Virtual barriers in AR [68, 182] help the user avoid unexpected
collisions with the robots.

Purpose-4. Communicate Intent: AR interfaces can also help
to communicate the robot’s intention to the user through spatial
information. For example, Walker et al. show that the AR repre-
sentations can better communicate the drone’s intent through the
experiments using three different designs [450]. Similarly, Rosen et
al. reveal that the AR visualization can better present the robotic
arm’s intent through the spatial trajectory, compared to the tradi-
tional interfaces [372]. AR interfaces can be also used to indicate
the state of robot manipulation such as indicating warning or com-
pletion of the task [21] or communicating intent with passersby or
pedestrians for wheelchairs [458] or self-driving cars [320].

Purpose-5. Increase the Expressiveness: Finally, AR can also be
used to augment the robot’s expression [3]. For example, Groechel
et al. [158] uses an AR view to provide virtual arms to a social
robot (e.g., Kuri Robot) to enhance the social expressions when
communicating with the users. Examples include adding facial ex-
pressions [481], overlaying remote users [198, 401], and interactive
content [96] onto robots. AR is a helpful medium to increase the
expressiveness of shape-changing interfaces [258]. For example,
Sublimate [243] or inFORM [127] uses see-through display or pro-
jection mapping to provide a virtual surface on a shape display.

6 CLASSIFICATION OF PRESENTED

INFORMATION

This section summarizes types of information presented in AR
interfaces. The categories we identified include 1) robot’s internal
information, 2) external information about the environment, 3) plan
and activity, and 4) supplemental content (Figure 6).

Information-1. Robot’s Internal Information: The first cate-
gory is the robot’s internal information. This can include 1) robot’s
internal status, 2) robot’s software and hardware condition, 3) ro-
bot’s internal functionality and capability. Examples include the
robot’s emotional state for social interaction [158, 481], a warning
sign when the user’s program is wrong [262], the drone’s current
information such as altitude, flight mode, flight status, and dilution
of precision [15, 332], and the robot’s reachable region to indicate
safe and dangerous zones [282]. Showing the robot’s hardware com-
ponents is also included in this category. For example, showing or
highlighting physical parts of the robot for maintenance [285, 302]
is also classified as this category.

Information-2. External Information about the Environment:
Another category is external information about the environment.
This includes 1) sensor data from the internal or external sensors,
2) camera or video feed, 3) information about external objects, 4)
depth map or 3D reconstructed scene of the environment. Exam-
ples include camera feeds for remote drone operations [171], the
world in miniature of the environment [4], sensor stream data of
the environment [15], visualization of obstacles [263], a local cost
map for search task [305], a 3D reconstructed view of the envi-
ronment [114, 332], a warning sign projected onto an object that

Figure 6: Types of presented information

6

Facilitate ProgrammingSupport Real-time ControlImprove SafetyCommunicate IntentIncrease ExpressivenessCollaborative Robot Programming [33]AR-supported Drone Navigation [494]Workspace Visualization [326]Robot Arm Motion Intent [372]AR Arms for Social Expression [158][12, 24, 33, 51, 53, 69, 105, 124, 131, 133, 136, 137, 140, 150, 153, 162, 201, 206, 207, 232, 261–263, 270, 275, 289, 313, 324–326, 358, 379, 407, 416, 496][4, 15, 25, 27, 58, 59, 76, 86, 114, 133, 163, 169, 171, 177, 191, 209, 212, 310, 332, 404, 451, 469, 482, 494][43, 56, 64, 68, 182, 200, 282, 326][21, 29, 64, 89, 141, 188, 223, 241, 300, 305, 320, 361, 363, 367, 372, 375, 429, 440, 445, 450, 458, 464, 476, 479][3, 14, 91, 96, 112, 127, 158, 159, 180, 189, 197, 198, 210, 221, 242, 243, 257, 258, 261, 328, 340, 346, 369, 370, 382, 401, 418, 425, 431, 436, 443, 467, 475, 481]Internal InformationExternal InformationPlan and ActivitySupplemental ContentInternal Status [479]Robot’s Capability [66]Object Status [21]Sensor/Camera Data [377]Plan and Target [136]Simulation [76]Interactive Content [386]Virtual Background [8][13, 15, 18, 19, 66, 86, 98, 101, 124, 158, 195, 262, 282, 285, 302, 324, 332, 375, 474, 479, 481][4, 15, 21, 24, 38, 47, 79, 84, 89, 102, 114, 143, 149, 150, 153, 171, 174, 178, 182, 184, 185, 201, 216, 230, 232, 241, 263, 265, 266, 269, 274, 289, 295, 305, 314, 332, 357, 377, 406, 464, 468, 486, 496][27, 29, 33, 51, 52, 67, 76, 105, 118, 130, 136, 141, 162, 188, 191, 206, 221, 270, 275, 284, 285, 305, 319, 320, 363, 372, 450, 467, 470, 476, 482, 494][8, 27, 49, 58, 65, 91, 93, 112, 116, 117, 127, 128, 142, 159, 168, 197, 207, 243– 245, 258, 317, 337, 338, 346, 368, 369, 374, 377, 382, 386, 393, 401, 414, 417, 418, 425, 429–431, 436, 475]Figure 7: Design components and strategies for visual augmentation

indicates the robot’s intention [21], visual feedback about the lo-
calization of the robot [468], and position and label of objects for
grasping tasks [153]. Such embedded external information improves
the situation awareness and comprehension of the task, especially
for real-time control and navigation.

Information-3. Plan and Activity: The previous two categories
focus on the current information, but plan and activity are related
to future information about the robot’s behavior. This includes 1)
a plan of the robot’s motion and behavior, 2) simulation results of
the programmed behavior, 3) visualization of a target and goal, 4)
progress of the current task. Examples include the future trajectory
of the drone [450], the direction of the mobile robots or vehicles [188,
320], a highlight of the object the robot is about to grasp [33], the
location of the robot’s target position [482], and a simulation of the
programmed robotic arm’s motion and behavior [372]. This type of
information helps the user better understand and expect the robot’s
behavior and intention.

Information-4. Supplemental Content: Finally, AR is also used
to show supplemental content for expressive interaction, such as
showing interactive content on robots or background images for
their surroundings. Examples include a holographic remote user
for remote collaboration and telepresence [197, 401], a visual scene
for games and entertainment [346, 369], an overlaid animation or
visual content for shape-changing interfaces [243, 258], showing
the menu for available actions [27, 58], and aided color coding or
background for dynamic data physicalization [127, 425].

7 DESIGN COMPONENTS AND STRATEGIES

FOR VISUAL AUGMENTATION

Different from the previous section that discusses what to show
in AR, this section focuses on how to show AR content. To this
end, we classify common design practices across the existing vi-
sual augmentation examples. At a higher level, we identified the
following design strategies and components: 1) UIs and widgets, 2)
spatial references and visualizations, and 3) embedded visual effects
(Figure 7).

Design-1. UIs and Widgets: UIs and widgets are a common design
practice in AR for robotics to help the user see, understand, and
interact with the information related to robots (Figure 7 Top).

— Menus: The menu is often used in mixed reality interfaces for
human-robot interaction [140, 326, 416]. The menu helps the user
to see and select the available options [325]. The user can also
control or communicate with robots through a menu and gestural
interaction [58].

— Information Panels: Information panels show the robot’s internal
or external status as floating windows [443] with either textual
or visual representations. Textual information can be effective to
present precise information such as the current altitude of the
drone [15] or the measured length [96]. More complex visual infor-
mation can also shown such as a network graph of the current task
and program [262].

— Labels and Annotations: Labels and annotations are used to show
information about the object. Also, they are used to annotate ob-
jects [96].

7

Spatial References and VisualizationsEmbedded Visual EffectsUIs and Widgets123MenusInformation PanelLabels and AnnotationsControls and HandlesMonitors and DisplaysPoints and LocationsPaths and TrajectoriesAreas and BoundariesOther VisualizationsAnthropomorphic EffectsVirtual Replicas and Ghost EffectsTexture Mapping EffectsPoint References [358, 435]Landmarks [451]Control Points [325]Simulated Trajectories [76, 450, 494]Bounding Box [182]Grouping [145, 191]Object Highlight [21]Force Map [177, 212]Radar Map [322]Color and Heat Map [282]Robot’s Body [3, 158]Robot’s Face [180]Human Body and Avatar [197, 242]Character Animation [473]Robot’s Virtual Replica [163, 209, 451Ghost Effects [372]Environment Replica [114]World in Miniature [4]Texture Mapping based on Shapes [245, 308, 374]Supplemental Background Images [328, 425, 429][27, 33, 58, 105, 140, 325, 326, 340, 385, 407, 416][18, 19, 111, 135, 136, 143, 325, 358, 373, 435, 451, 468, 482][15, 96, 145, 185, 262, 332, 443][96, 188, 302, 443, 492][130, 163, 169, 206, 340, 416, 469][59, 96, 171, 317, 332, 354, 382, 418, 443, 445, 475][59, 75, 76, 103–105, 136, 160, 209, 229, 270, 336, 372, 407, 450, 451, 458, 467, 476, 494][21, 33, 68, 105, 115, 133, 145, 153, 182, 191, 263, 302, 326, 407][15, 27, 59, 136, 176, 177, 212, 262, 282, 305, 322, 332, 494][3, 14, 23, 158, 180, 197, 198, 242, 401, 436, 450, 472, 473, 481][4, 25, 27, 33, 51, 76, 114, 153, 163, 169, 182, 209, 270, 285, 302, 326, 332, 354, 358, 372, 407, 450, 451, 494][116, 127, 159, 175, 193, 243–245, 258, 308, 328, 340, 369, 370, 374, 425, 429, 431]Connections and Relationships [136, 206]— Controls and Handles: Controls and handles are another user
interface example. They allow the user to control robots through a
virtual handle [169]. Also, AR can show the control value surround-
ing the robot [340].

— Monitors and Displays: Monitor or displays help the user to situate
themselves in the remote environment [445]. Camera monitors
allow the user to better navigate the drone for inspection or aerial
photography tasks [171]. The camera feed can be also combined
with the real-time 3D reconstruction [332]. In contrast, monitor or
display are also used to display spatially registered content in the
surrounding environment [382] or on top of the robot [443]

Design-2. Spatial References and Visualizations: Spatial ref-
erences and visualizations are a technique used to overlay data
spatially. Similar to embedded visualizations [463], this design can
directly embed data on top of their corresponding physical refer-
ents. The representation can be from a simple graphical element,
such as points (0D), paths (1D), or areas (2D/3D), to more complex
visualizations like color maps (Figure 7 Middle).

— Points and Locations: Points are used to visualize a specific location
in AR. These points can be used to highlight a landmark [136], target
location [482], or way point [451], which is associated to the geo-
spatial information. Additionally, points can be used as a control or
anchor point to manipulate virtual objects or boundaries [325].

— Paths and Trajectories: Similarly, paths and trajectories are another
common approaches to represent spatial references as lines [358,
407, 450]. For example, paths are commonly used to visualize the
expected behaviors for real-time or programmed control [75, 76,
494]. By combining the interactive points, the user can modify these
paths by adding, editing, or deleting the way points [451].

— Areas and Boundaries: Areas and boundaries are used to highlight
specific regions of the physical environment. They can visualize
a virtual bounding box for safety purposes [68, 182] or highlight
a region to show the robot’s intent [21, 105]. Alternatively, the
areas and boundaries are also visualized as a group of objects or
robots [191]. Some research projects also demonstrated the use
of interactive sketching for specifying the boundaries in home
automation [191, 263].

— Other Visualizations: Spatial visualizations can also take more
complex and expressive forms. For example, spatial color/heat
map visualization can indicate the safe and danger zones in the
workspace, based on the robot’s reachable areas [282]. Alternatively,
a force map visualizes the field of force to provide visual affordance
for the robot’s control [176, 177, 212].

Design-3. Embedded Visual Effects: Embedded visual effects
refer to graphical content directly embedded in the real world.
In contrast to spatial visualization, embedded visualization does
not need to encode data. Common embedded visual effects are 1)
anthropomorphic effects, 2) virtual replica, and 3) texture mapping
of physical objects (Figure 7 Bottom).

— Anthropomorphic Effects: Anthropomorphic effects are visual aug-
mentations that render human-inspired graphics. Such design can
add an interactive effect of 1) a robot’s body [3], such as arms [158]
and eyes [450], 2) faces and facial expressions [14, 180, 481], 3) a
human-avatar [198, 401, 472], or 4) character animation [23, 473],

on top of the robots. For example, it can augment the robot’s face
by animated facial expression with realistic images [14] or cartoon-
like animation [481], which can improve the social expression of
the robots [158] and engage more interaction [23, 473]. In addition
to augmenting a robot’s body, it can also show the image of a real
person to facilitate remote communication [197, 401, 436, 472].

— Virtual Replica and Ghost Effects: A virtual replica is a 3D ren-
dering of robots, objects, or external environments. By combining
with spatial references, a virtual replica is helpful to visualize the
simulated behaviors [76, 169, 372, 451, 494]. By rendering multiple
virtual replicas, the system can also show the ghost effect with a
series of semi-transparent replica [51, 372]. In addition, a replica
of external objects or environments is also used to facilitate co-
located programming [25, 33] or real-time navigation in the hidden
space [114]. Also, a miniaturized replica of the environment (i.e.,
the world in miniature) helps drone navigation [4].

— Texture Mapping Effects based on Shape: Finally, texture mapping
overlays interactive content onto physical objects to increase expres-
siveness. This technique is often used to enhance shape-changing
interfaces and displays [127, 175, 308, 374], such as overlaying ter-
rain [244, 245], landscape [116], animated game elements [258, 431],
colored texture [308], or NURBS (Non-Uniform Rational Basis
Spline) surface effects [243]. Texture effects can also augment the
surrounding background of the robot. For example, by overlaying
the background texture onto the surrounding walls or surfaces, AR
can contextualize the robots with the background of an immersive
educational game [369, 370], a visual map [340, 425, 431], or a solar
system [328].

8 INTERACTIONS

Dimension-1. Level of Interactivity: In this section, we survey
the interactions in AR and robotics research. The first dimension is
level of interactivity (Figure 8).

— No Interaction (Only Output): In this category, the system uses
AR solely for visual output and disregards user input [158, 261, 332,
372, 382, 418, 436, 450, 481]. Examples include visualization of the
robot’s motion or capability [282, 372, 450], but these systems often
focus on visual outputs, independent of the user’s action.

Figure 8: Level of interactivity

— Implicit Interaction: Implicit interaction takes the user’s implicit
motion as input, such as the user’s position or proximity to the
robot [458]. Sometimes, the user may not necessarily realize the
association between their actions and effects, but the robots respond

8

Level of InteractivityLowHigh[158, 372, 382, 418, 436, 450, 481][38, 357, 414, 458][51, 96, 171, 191, 325, 346, 429, 451][104, 127, 163, 243, 258, 316, 328, 354]Only OutputImplicitExplicit and IndirectExplicit and DirectProgrammed Visualization [450]Proximity with Passerby [458]Body Gesture and Motion [346]Direct Physical Manipulation [316]Figure 9: Interaction modalities and techniques

implicitly to the users’ physical movements (e.g., approaching to
the robot).

— Explicit and Indirect Manipulation: Indirect manipulation is the
user’s input through remote manipulation without any physical
contact. The interaction can take place through pointing out ob-
jects [325], selecting and drawing [191], or explicitly determining
actions with body motion (e.g., changing the setting in a virtual
menu [58])

— Explicit and Direct Physical Manipulation: Finally, this category in-
volves the user’s direct touch inputs with their hands or bodies. The
user can physically interact with the robots through embodied body
interaction [369]. Several interaction techniques utilize the defor-
mation of objects or robots [243], grasping and manipulating [163],
or physically demonstrating [270].

Dimension-2. Interaction Modalities: Next, we synthesize cate-
gories based on the interaction modalities (Figure 9).

— Tangible: The user can interact with robots by changing the shape
or by physically deforming the object [243, 258], moving robots
by grasping and moving tangible objects [163, 340], or controlling
robots by grasping and manipulating robots themselves [328].

— Touch: Touch interactions often involve the touch screen of mo-
biles, tablets, or other interactive surfaces. The user can interact
with robots by dragging or drawing on a tablet [76, 209, 212], touch-
ing and pointing the target position [163], and manipulating virtual
menus on a smartphone [53]. The touch interaction is particularly
useful when requiring precise input for controlling [153, 169] or
programming the robot’s motion [136, 407].

— Pointer and Controller: The pointer and controller allow the
user to manipulate robots through spatial interaction or device
action. Since the controller provides tactile feedback, it reduces the
effort to manipulate robots [171]. While many controller inputs
are explicit interactions [191, 467], the user can also implicitly
communicate with robots, such as designing a 3D virtual object
with the pointer [341].

— Spatial Gesture: Spatial gestures are a common interaction modal-
ity for HMD-based interfaces [27, 33, 58, 59, 114, 262, 320, 326].
With these kinds of gestures, users can manipulate virtual way
points [325, 358] or operate robots with a virtual menu [58]. The
spatial gesture is also used to implicitly manipulate swarm robots
through remote interaction [401].

— Gaze: Gaze is often used to accompany the spatial gesture [27,
114, 262, 300, 325, 358, 482], such as when performing menu selec-
tion [27]. But, some works investigate the gaze itself to control the
robot by pointing out the location in 3D space [28].

— Voice: Some research leveraged voice input to execute commands
for the robot operation [27, 182, 197, 354], especially in co-located
settings.

— Proximity: Finally, proximity is used as an implicit form of inter-
action to communicate with robots [14, 182, 305]. For example, the
AR’s trajectory will be updated to show the robot’s intent when the
a passerby approaches the robot [458]. Also, the shape-shifting wall
can change the content on the robot based on the user’s behavior
and position [431].

9 APPLICATION DOMAINS
We identified a range of different application domains in AR and
robotics. Figure 10 summarizes the each category and the list of
related papers. We classified the existing works into the following
high-level application-type clusters: 1) domestic and everyday use, 2)
industry applications, 3) entertainment, 4) education and training, 5)
social interaction, 6) design and creative tasks, 7) medical and health,
8) telepresence and remote collaboration, 9) mobility and transporta-
tion, 10) search and rescue, 11) robots for workspaces, and 12) data
physicalization.

Detailed lists of application use cases within each of these cat-
egories can be found in Figure 10, as well as appendix, including
detailed lists of references we identified. The largest category is
industry. For example, industry application includes manufacturing,
assembly, maintenance, and factory automation. In many of these
cases, AR can help the user to reduce the assembly or maintenance
workload or program the robots for automation. Another large cate-
gory we found emerging is domestic and everyday use scenarios. For
example, AR is used to program robots for household tasks. Also,
there are some other sub-categories, such as photography, tour
guide, advertisement, and wearable robots. Games and entertain-
ment are popular with robotic user interfaces. In these examples,
the combination of robots and AR is used to provide an immersive
game experience, or used for storytelling, music, or museums. Fig-
ure 10 suggests that there are less explored application domains,
which can be investigated in the future, which include design and
creative tasks, remote collaboration, and workspace applications.

9

Tangible[163, 243, 258, 328, 340, 354, 369, 374, 425, 443, 472, 473]Touch and Toy [163]TouchMe [169][53, 76, 133, 136, 140, 153, 159, 163, 169, 201, 209, 212, 285, 407]Laser-based Sketch [191][15, 96, 171, 177, 191, 332, 341, 370, 451, 467]Drone.io[58][3, 27, 33, 58, 59, 105, 114, 262, 270, 320, 325, 326, 358, 401]Gaze-driven Navigation [482][27, 28, 33, 38, 67, 114, 262, 300, 320, 325, 336, 358, 429, 482]Voice-based Control [184][14, 27, 54, 67, 108, 182, 184, 197, 239, 336, 354, 367, 404, 406, 439]Collision Avoidance [200][200, 229, 305, 350, 430, 431, 458, 467]TouchControllerGestureGazeVoiceProximityFigure 10: Use cases and application domains

10 EVALUATION STRATEGIES
In this section, we report our analysis of evaluation strategies for
augmented reality and robotics. The main categories we identified
are following the classification by Ledo et al. [236]: 1) evaluation
through demonstration, (2) technical evaluations, and (3) user eval-
uations. The goal of this section is to help researchers finding the
best technique to evaluate their systems, when designing AR for
robotic systems.

Evaluation-1. Evaluation through Demonstration: Evaluation
through demonstration is a technique to see how well a system will
potentially work in certain situations. The most common approach
from our findings include showing example applications [84, 127,
142, 209, 245, 325, 366, 382, 418, 476] and proof-of-concept demon-
strations of a system [24, 55, 117, 162, 199, 221, 270, 305, 375, 479].
Other common approaches include demonstrating a system through
a workshop [244, 246, 476], demonstrating the idea to a focus
group [9, 367, 472, 492], carrying out case studies [105, 189, 324],
and providing a conceptual idea [200, 367, 374, 472, 492].

Evaluation-2. Technical Evaluation: Technical Evaluation refers
to how well a system performs based on internal technical mea-
sures of the system. The most common approaches for technical
evaluation are measuring latency [48, 51, 59, 69, 318, 495], accuracy
of tracking [15, 58, 59, 64, 486], and success rate [262, 314]. Also, we
found some works evaluate their system performances based on the
comparison with other systems, which for example, include compar-
ing tracking algorithms with other approaches [38, 59, 63, 132, 406].

Evaluation-3. User Evaluation: User evaluation refers to measur-
ing the effectiveness of a system through user studies. To measure
the user performance when interacting with the system, there are
many different approaches and methods that are used. For example,
the NASA TLX questionnaire is a very popular technique for user
evaluation [15, 63, 67, 69, 358], which can be found used mostly

for industry related applications. Other approaches include run-
ning quantitative [38, 171, 184] and qualitative [21, 138, 165] lab
studies, through interviews [64, 103, 443] and questionnaires [48,
59, 495]. Sometimes systems combine user evaluations techniques
with demonstration [111, 382] or technical evaluations [15, 406]. In
observational studies [58, 369, 382], researchers can also get user
feedback through observations [135, 448]. Finally, some systems
also ran lab studies through expert interviews [10, 116, 340] to get
specific feedback from the expert’s perspectives.

11 DISCUSSION AND FINDINGS
Based on the analysis of our taxonomy, Figure 11 shows a summary
of the number of papers for each dimension. In this section, we dis-
cuss common strategies and gaps across characteristics of selected
dimensions.

In terms of proximity, co-located
Robot - Proximity Category:
with distance are the preferred method in AR-HRI systems (251
papers). This means that the current trend for AR-HRI systems is
to have users co-located with the robot, but to not make any sort of
contact with it. This also suggests that AR devices provide a promis-
ing way to interact with robots without having the need to directly
make contact with it, such as performing robotic manipulation
programming through AR [326].

In terms of the Design - UI
Design - UI and Widget Category:
and Widgets category, labels and annotations are the most common
choice (241 papers) used in AR systems. Given that AR enables
us to design visual feedback without many of the constraints of
physical reality, researchers of AR-HRI systems take advantage of
that fact to provide relevant information about the robot and/or
other points of interest such as the environment and objects [96].
Increasingly, other widgets are used, such as information panels,
floating displays, or menus. It is notable that only 24 papers made

10

Domestic and Everyday Use (35)Industry (166)Entertainment (32)Education and Training (22)Social Interaction (21)Design and Creativity Tasks (11)Medical and Health (36)Remote Collaboration (6)Mobility and Transportation (12)Search and Rescue (35)Workspace (9)Data Physicalization (12)Manufacturing: joint assembly [21, 30, 43, 138, 289], grasping and manipulation [[67, 79, 153, 185, 379], tutorial and simulation [52], welding [313] Maintenance: maintenance of robots [144, 252], remote repair [31, 48], per-formance monitoring [126, 128], setup and calibration [352], debugging [373]  Safety and Inspection: nuclear detection [15], drone monitoring [76, 114, 171, 482], safety feature [44, 66, 104, 379, 385], ground moni-toring [211, 269, 479] Automation and Tele-operation: interactive programming inter-face [118, 131, 136, 177, 270, 288, 324, 496] Logistics: package delivery [265] Aerospace: surface exploration [54], teleoperated manip-ulator [310], spacecraft maintenance [470]Household Task: authoring home automa-tion [53, 111, 135, 163, 177, 191, 212, 228, 263, 387], item movement and de- livery [76, 108, 209, 265], multi-purpose table [430] Photog-raphy: drone photography [114, 171], Adver-tisement: mid-air advertisement [317, 382, 418, 475] Wearables Interactive Devices: haptic interaction [443], fog screens [418], head-worn projector for sharable AR scenes [168] Assistance and Companionship: elder care [65], personal assistant [239, 367, 368] Tour and Exhibition Guide: tour guide [295], museum exhibition guide [168, 368], guiding crowds [475], indoor building guide [89], museum interactive display [112]Games: interactive treasure protection game [350], pong-like game [346, 370], labyrinth game [258], tangible game [49, 178, 230], air hockey [91, 451], tank battle [90, 221], adven-ture game [55], role-play game [229], checker [242], domino [246], ball target throwing game [337], multiplayer game [115, 404], vir-tual playground [272] Storytelling: immer-sive storytelling [328, 369, 395, 415, 467] En-hanced Display: immersive gaming and digi-tal media [431] Music: animated piano key press [472, 473], tangible tabletop music mixer [340] Festivals: festival greetings [382] Aquarium: robotic and virtual fish [240]Remote Teaching: remote live instruction [445] Training: military training for working with robot teammates [199], piano instruc-tion [472, 473], robotic environment setup [142], robot assembly guide [18], driving review [11], posture analysis and correction [183, 437] Tangible Learning: group activity [275, 328, 337, 408, 467], programming edu-cation [416]Human-Robot Social Interaction: reaction to human behaviors [93, 108, 414], cartoon art expression [377, 481], human-like robot head [14], co-eating [134], trust building [141], task assignment [439, 440] Robot-As-sisted Social Interaction: projected text message conversations [382] Inter-Robot In-teraction: human-like robot interaction [107]Fabrication: augmented 3D printer [476],in-teractive 3D modelling [341], augmented laser cutter [304], design simulation [52] Design Tools: circuit design guide [445], ro-botic debugging interface [145], design and annotation tool [96], augmenting physical 3D objects [210] Theatre: children’s play [10]Medical Assistance: robotic-assisted surgery [13, 82, 84, 85, 125, 181, 290, 354, 355, 460], doctors doing hospital rounds [228] Accessi-bility: robotic prostheses [86, 136] Rehabili-tation: autism rehabilitation [29], walking support [334]Remote Physical Synchronization: physical manipulation by virtual avatar [242] Avatar Enhancement: life-sized avatar [197], floating avatar [436, 475], life-sized avatar and sur-rounding objects [189] Human-like Embodi-ment: traffic police [149]Human-Vehicle Interaction: projected guid-ance [2, 7], interaction with pedestrians [64, 320], display for passengers [223] Augment-ed Wheelchair: projecting intentions [458], displaying virtual hands to convey intentions [300], self-navigating wheelchair [314], assis-tive features [495] Navigation: tangible 3D map [258], automobile navigation [438]Ground Search: collaborative ground search [363], target detection and notification [211, 241, 266, 269, 305, 361, 468, 479], teleoperat-ed ground search [54, 102, 267, 417, 469, 483, 486] Aerial Search: drone-assisted search and rescue [114, 332, 482], target detection and highlight [464]Adaptive Workspaces: individual and collab-orative workspace transformation [159, 430, 431] Supporting Workers: reducingworkload for industrial robot programmers [407], mobile presentation [168, 257, 338], multitasking with reduced head turns [38], virtual object manipulation [357]Physical Data Encoding: physical bar charts [167, 429], embedded physical 3D bar charts [425] Scientific Physicalization: mathemati-cal visualization [127, 243], terrain visualiza-tion [116, 117, 243, 245, 308], medical data vi-sualization [243] Physicalizing Digial Con-tent: handheld shape-changing display [258, 374]Figure 11: A visualization with overall counts of characteristics across all dimensions.

use of virtual control handles, possibly implying that AR is not yet
commonly used for providing direct control to robots.

hope this section will guide, inspire, and stimulate the future of
AR-enhanced Human-Robot Interaction (AR-HRI) research.

Interactions - Level of Interactivity Category: For the Interac-
tion Level category, we observed that explicit and indirect input is
the most common approach within AR-HRI systems (295 papers).
This means that user input through AR to interact with the robot
must go through some sort of input mapping to accurately interact
with the robot. This is an area that should be further explored,
which we mention in Section 11 - Immersive Authoring and
Prototyping Environments for AR-HRI. However, while AR
may not be the popular approach in terms of controlling a robot’s
movement, as mentioned above, it is still an effective medium to
provide other sorts of input, such as path trajectories [358], for
robots.

Interactions - Modality Category: In the Interaction Modality cat-
egory, pointers and controllers (136 papers) and spatial gestures (116
papers) are most commonly used. Spatial gestures, for example, are
used in applications such as robot gaming [273]. Furthermore, touch
(66 papers) and tangibles (68 papers) are also common interaction
modalities, indicating that these traditional forms of modality are
seen as effective options for AR-HRI systems (for example, in appli-
cations such as medical robots [459] and collaborative robots [493]).
It is promising to see how many AR-HRI systems are using tan-
gible modalities to provide shape-changing elements [243] and
control [340] to robots. Gaze and voice input are less common
across the papers in our corpus, similar to proximity-based input,
pointing to interesting opportunities for future work to explore
these modalities in the AR-HRI context.

12 FUTURE OPPORTUNITIES
Finally, we formulate open research questions, challenges, and
opportunities for AR and robotics research. For each opportunity,
we also discuss potential research directions, providing sketches
and relevant sections or references as a source of inspiration. We

11

Figure 12: Opportunity-1: Making AR-HRI Practical and
Ubiquitous. Left: Improvement of display and tracking tech-
nologies would broaden practical applications like robotic
surgery. Right: Deployment in-the-wild outside the research
lab, such as construction sites, would benefit from user-
centered design.

Opportunity-1. Making AR-HRI Practical and Ubiquitous:
— Technological and Practical Challenges: While AR-HRI has a great
promise, there are many technological and practical challenges
ahead of us. For example, the accurate realistic superimposition or
occlusion of virtual elements is still very challenging due to noisy
real-time tracking. The improvement of display and tracking
technologies would broaden the range of practical applications, es-
pecially when more precise alignments are needed, such as robotic-
assisted surgery or medical applications (Section 9.7). Moreover,
error-reliable system design is also important for practical appli-
cations. AR-HRI is used to improve safety for human co-workers
(Section 5.2), however, if the AR system fails in such a safety-
critical situation, users might be at risk (e.g., device malfunctions,
content misalignment, obscured critical objects with inappropriate
content overlap, etc). It is important to increase the reliability of

approachapproachapproachapproachapproachapproachapproachapproachform factorsform factorsform factorsform factorsform factorsform factorsform factorsform factorsrelationshiprelationshiprelationshiprelationshipscalescalescalescaleproximityproximityproximityproximitypurposespurposespurposespurposespurposesinformationinformationinformationinformationuis and widgetsuis and widgetsuis and widgetsuis and widgetsuis and widgetsspatial references and visualizationsspatial references and visualizationsspatial references and visualizationsspatial references and visualizationsembedded visual effectsembedded visual effectsembedded visual effectsinteractivityinteractivityinteractivityinteractivityinteraction modalitiesinteraction modalitiesinteraction modalitiesinteraction modalitiesinteraction modalitiesinteraction modalitiesinteraction modalitiesapplicationsapplicationsapplicationsapplicationsapplicationsapplicationsapplicationsapplicationsapplicationsapplicationsapplicationsapplicationsevaluationevaluationevaluationevaluationevaluationevaluationevaluationevaluationevaluationevaluationevaluationevaluationevaluationevaluationevaluationevaluationevaluationevaluationevaluationevaluationon-body (head/eye) 103on-body (handheld) 26on-environment 74on-robot 8on-body (head/eye) 146on-body (handheld) 28on-environment 138on-robot 33robotic arms 172drones 25mobile robots 133humanoid robots 33vehicles 9actuated objects 28combinations 14other types 121 : 1 3241 : m 34n : 1 25n : m 10handheld-scale 19tabletop-scale 70body-scale 255building/city-scale 6co-located 69co-located with distance 251semi-remote 19remote 66facilitate programming 154support real-time control 126improve safety 32communicate intent 82increase expressiveness 115internal information 94external information 230plan and activity 151supplement content 117menus 115information planels 80labels and annotations 241controls and handles 24monitors and displays 47points and locations 208paths and trajectories 154areas and boundaries 178other visualizations 91anthropomorphic 22visual replica 144texture mapping 32only output 27implicit 11indirect 295direct 72tangible 68touch 66controller 136gesture 116gaze 14voice 18proximity 21domestic and everyday use 35industry 166entertainment 32education and training 22social interaction 21design and creative tasks 11medical and health 36remote collaboration 6mobility and transportation 12search and rescue 35workspace 9data physicalization 12workshop 7theoretical framework 2example applications 34focus groups 2early demonstrations 39case studies 6conceptual 7time 19accuracy 30system performance 32success rate 2nasa tls 20interviews 9questionnaires 31lab study with users 8qualitative study 18quantitative study 28lab study with experts 3observational 5user feedback 2Making AR-HRI Practical and UbiquitousTechnological and Practical ChallengesDeployment and Evaluation In-the-WildAR systems from both systems design and user interaction perspec-
tives (e.g., What extent should users rely on AR systems in case
the system fails? How can we avoid visual clutter or the occlusion
of critical information in a dangerous area? etc). These technical
and practical challenges should be addressed before we can see AR
devices be common in everyday life.

— Deployment and Evaluation In-the-Wild: Related to the above,
most prior AR-HRI research has been done in controlled labora-
tory conditions. It is still questionable whether these systems and
findings can be directly applied to a real-world situation. For exam-
ple, outdoor scenarios like search-and-rescue or building construc-
tion (Section 9) may require very different technical requirements
than indoor scenarios (e.g., Is projection mapping visible enough
outdoors? Can outside-in tracking sufficiently cover the area that
needs to be tracked?). On the other hand, the current HMD de-
vices still have many usability and technical limitations, such as
display resolution, visual comfort, battery life, weight, the field of
view, and latency issues. To appropriately design a practical sys-
tem for real-world applications, it is important to design based on
the user’s needs through user-centered design by conducting a
repeated cycle of interviews, prototyping, and evaluation. In par-
ticular, researchers need to carefully consider different approaches
or technological choices (Section 3) to meet the user’s needs. The
deployment and evaluation in the wild will allow us to develop a
better understanding of what kind of designs or techniques should
work and what should not in real-world situations.

opportunity for augmented virtual skins of the robots by fully
leveraging the unlimited visual expressions. In addition, there is also
a rich design space of dynamic appearance change by leveraging
visual illusion [259, 260], such as making robots disappear [299, 369],
change color [152, 453], or transform its shape [170, 392, 424, 425]
with the power of AR. By increasing the expressiveness of robots
(Section 5.5), this could improve the engagement of the users and
enable interesting applications (e.g., using drones that have facial
expression [173] or human body/face [148] for remote telepres-
ence [197]). We argue that there are still many opportunities for
such unconventional robot design with expressive visual aug-
mentation. We invite and encourage researchers to re-imagine such
possibilities for the upcoming AR/MR era.

— Immersive Authoring and Prototyping Environments for AR-HRI:
Prototyping functional AR-HRI systems is still very hard, given
the high barrier of requirements in both software and hardware
skills. Moreover, the development of such systems is pretty time-
consuming—people need to continuously move back and forth
between the computer screen and the real world, which hinders
the rapid design exploration and evaluation. To address this, we
need a better authoring and prototyping tool that allows even non-
programmers to design and prototype to broaden the AR-HRI
research community. For example, what if, users can design and
prototype interactions through direct manipulation within AR,
rather than coding on a computer screen? (e.g., one metaphor is, for
example, Figma for app development or Adobe Character Anima-
tor for animation) In such tools, users also must be able to design
without the need for low-level robot programming, such as ac-
tuation control, sensor access, and networking. Such AR authoring
tools have been explored in the HCI context [40, 247, 311, 455] but
still relatively unexplored in the domain of AR-HRI except for a few
examples [51, 422]. We envision the future of intuitive authoring
tools should invoke further design explorations of AR-HRI sys-
tems (Section 7) by democratizing the opportunity to the broader
community.

Figure 13: Opportunity-2: Designing and Exploring New AR-
HRI. Left: AR-HRI can open up a new opportunity for un-
conventional robot design like fairy or fictional characters
with the power of AR. Right: Immersive authoring tools al-
low us to prototype interactions through direct manipula-
tion within AR.

Opportunity-2. Designing and Exploring New AR-HRI:
— Re-imagining Robot Design without Physical Constraints: With
AR-HRI, we have a unique opportunity to re-imagine robots de-
sign without constraints of physical reality. For example, we have
covered interesting attempts from the prior works, like making non-
humanoid robots humanoid [180, 198, 481] or making robots visually
animated [3, 14, 158] (Section 7.3), either through HMD [197] or
projecion [14] (Section 3). However, this is just the tip of the iceberg
of such possibilities. For example, what if robots would look like a
fictional character [57, 208] or behave like Disney’s character anima-
tion? [214, 434, 444] We believe there is still a huge untapped design

Figure 14: Opportunity-3: AR-HRI for Better Decision-
Making. Left: Real-time embedded and immersive visualiza-
tions help an operator’s decision-making in drone naviga-
tion for search-and-rescue. Right: AR-based explainable ro-
botics enables the user to understand a robot’s path plan-
ning behavior through physical explorations.

Opportunity-3. AR-HRI for Better Decision-Making:
— Real-time Embedded Data Visualization for AR-HRI: AR interfaces
promise to support operators’ complex decision-making (Section

12

Designing and Exploring New AR-HRIRe-imagining Robot DesignImmersive Authoring and PrototypingAR-HRI for Better Decision-MakingReal-time Embedded Data Viz for AR-HRIExplainable and Explorable Robotics5.2) by aggregating and visualizing various data sources, such as
internal, external, or goal-related information (Section 6.1-6.3).
Currently, such visualizations are mainly limited with simple spa-
tial references of user-defined data points (Section 7.2), but there
is still a huge potential to connect data visualization to HRI [428]
in the context of AR-HRI. For example, what if AR interfaces can
directly embed real-time data onto the real world, rather than
on a computer screen? We could even combine real-time visualiza-
tions with a world-in-miniature [95] to facilitate navigation in a
large area, such as drone navigation for search-and-rescue. We can
take inspiration from existing immersive data analysis [70, 113] or
real-time embedded data visualization research [423, 462, 463] to
better design such data-centric interfaces for AR-HRI. We encour-
age the researchers to start thinking about how we can apply these
emerging data visualization practices for AR-HRI systems in the
future.

— Explainable and Explorable Robotics through AR-HRI: As robots be-
come more and more intelligent and autonomous, it becomes more
important to make the robot’s decision-making process visible and
interpretable. This is often called Explainable AI in the context of
machine learning and AI research, but it is also becoming relevant
to robotics research as Explainable Robotics [97, 390]. Currently,
such explanations are represented as descriptive text or visuals on
a screen [97]. However, by leveraging AR-HRI systems, users can
better understand the robots’ behavior by seeing what they see
(sensing), how they think (decision making), and how they respond
(actions) in the real world. For example, what if users can see what
a robot recognizes as obstacles or how it chooses the optimal path
when navigating in a crowded place? More importantly, these vi-
sualizations are also explorable—users can interactively explore
to see how the robot’s decision would change when the physical
world changes (e.g., directly manipulating physical obstacles to
see how the robot’s optimal path updates). Such interfaces could
help programmers, operators, or co-workers understand the robot’s
behavior more easily and interactively. Future research should con-
nect explainable robotics with AR to better visualize the robot’s
decision-making process embedded in real-world.

Figure 15: Opportunity-4: Novel Interaction Design enabled
by AR-HRI. Left: The user can interact with a swarm of
drones with an expressive two-handed gesture like a mid-
air drawing. Right: Programmable physical actuation and
reconfiguration enable us to further blend the virtual and
physical worlds, like a virtual remote user can “physically”
move a chess piece with tabletop actuation.

13

Opportunity-4. Novel Interaction Design enabled by AR-HRI:
— Natural Input Interactions with AR-HRI Devices: With the pro-
liferation of HMD devices, it is now possible to use expressive
inputs more casually and ubiquitously, including gesture, gaze,
head, voice, and proximity-based interaction (Section 8.2). In con-
trast to environment-installed tracking, HMD-based hand- and
gaze-tracking could enable more natural interactions without the
constraint of location. For example, with the hand-tracking capabil-
ity, we can now implement expressive gesture interactions, such as
finger-snap, hand-waving, hand-pointing, and mid-air drawing for
swarm drone controls in entertainment, search and rescue, firefight-
ing, or agricultural foraging [17, 217]). In addition, the combination
of multiple modalities, such as voice, gaze, and gesture is also an
interesting direction. For example, when the user says “Can you
bring this to there?”, it is usually difficult to clarify the ambiguity
(e.g., “this” or “there”), but with the combination of gaze and gesture,
it is much easier to clarify these ambiguities within the context.
AR-based visual feedback could also help the user clarify their inten-
tions. The user could even casually register or program such a new
input on-demand through end-user robot programming (Section
5.1). Exploring new interactions enabled by AR-HRI systems is also
an exciting opportunity.

— Further Blending the Virtual and Physical Worlds: As robots weave
themselves into the fabric of our everyday environment, the term
“robots” no longer refer to only traditional humanoid or industry
robots, but can become a variety of forms (Section 2.1 and Sec-
tion 4.1)—from self-driving cars [7] to robotic furniture [421, 478],
wearable robots [99], haptic devices [449], shape-changing dis-
plays [127], and actuated interfaces [331]. These ubiquitous robots
will be used to actuate our physical world to make the world
more dynamic and reconfigurable. By levering both AR and this
physical reconfigurability, we envision further blending virtual and
physical worlds with a seamless coupling between pixels and
atoms. Currently, AR is only used to visually augment appearances
of the physical world. However, what if AR can also “physically”
affect the real-world? For example, what if a virtual user pushes
a physical wall then it moves synchronously? What if virtual wind
can wave a physical cloth or flag? What if virtual explosion can make
a shock wave collapse physical boxes? Such virtual-physical in-
teractions would make AR more immersive with the power of
visual illusion, which can also have some practical applications
such as entertainment, remote collaboration, and education. Pre-
viously, such ideas were only partially explored [23, 401], but we
believe there still remains a rich design space to be further explored.
For future work, we should further seek to blend virtual and physi-
cal worlds by leveraging both visually (AR) and physically (robotic
reconfiguration) programmable environments.

13 CONCLUSION
In this paper, we present our survey results and taxonomy of AR
and robotics, synthesizing existing research approaches and designs
in the eight design space dimensions. Our goal is to provide a com-
mon ground for researchers to investigate the existing approaches
and design of AR-HRI systems. In addition, to further stimulate
the future of AR-HRI research, we discuss future research oppor-
tunities by pointing out eight possible directions: 1) technological

Novel Interaction Design Enabled by AR-HRINatural Input Interactions with AR-HRIBlending the Virtual and Physical Worldsand practical challenges, 2) deployment and evaluation in-the-wild,
3) re-imagining robot design, 4) immersive authoring and proto-
typing environments, 5) real-time embedded data visualization for
AR-HRI, 6) explainable and explorable robotics with AR, 7) novel
interactions techniques, and 8) further blending the virtual and
physical worlds with programmable augmentation and actuation.
We hope our survey, taxonomy, and open research opportunity will
guide and inspire the future of AR and robotics research.

REFERENCES

[1] 2014. List of Physical Visualizations and Related Artifacts. Retrieved on January

5, 2022 from http://dataphys.org/list/

[2] 2015. The Mercedes-Benz F 015 luxury in motion. Retrieved on January 5, 2022
from https://www.mercedes-benz.com/en/innovation/autonomous/research-
vehicle-f-015-luxury-in-motion/

[3] 2015. Microsoft Hololens Robot Demo at Build 2015. Retrieved on January 5,

2022 from https://www.youtube.com/watch?v=mSCrviBGTeQ

[4] 2016. Boeing: UAVs. Holograms. Wildfire. Retrieved on January 5, 2022 from

https://www.youtube.com/watch?v=omGoz66xHU8

[5] 2017. Personal Fabrication Research in HCI and Graphics: An Overview of
Related Work. Retrieved on January 5, 2022 from https://hcie.csail.mit.edu/
fabpub/

[6] 2018. MorphUI. Retrieved on January 5, 2022 from http://morphui.com/
[7] 2019.

Jaguar land rover lights up the road ahead for self-driving
Retrieved on January 5, 2022 from

vehicles of
https://media.jaguarlandrover.com/news/2019/01/jaguar-land-rover-lights-
road-ahead-self-driving-vehicles-future

the future.

[8] 2020. Nintendo Mario Kart Live: Home Circuit. Retrieved on January 5, 2022

from https://mklive.nintendo.com/

[9] Syed Mohsin Abbas, Syed Hassan, and Jongwon Yun. 2012. Augmented reality
based teaching pendant for industrial robot. In 2012 12th International Conference
on Control, Automation and Systems. IEEE, 2210–2213.

[10] Jong-gil Ahn, Gerard J Kim, Hyemin Yeon, Eunja Hyun, and Kyoung Choi.
2013. Supporting augmented reality based children’s play with pro-cam robot:
three user perspectives. In Proceedings of the 12th ACM SIGGRAPH International
Conference on Virtual-Reality Continuum and Its Applications in Industry. 17–24.
https://doi.org/10.1145/2534329.2534342

[11] Yuya Aikawa, Masayoshi Kanoh, Felix Jimenez, Mitsuhiro Hayase, Takahiro
Tanaka, and Hitoshi Kanamori. 2018. Comparison of gesture inputs for robot
system using mixed reality to encourage driving review. In 2018 Joint 10th
International Conference on Soft Computing and Intelligent Systems (SCIS) and
19th International Symposium on Advanced Intelligent Systems (ISIS). IEEE, 62–66.
https://doi.org/10.1109/scis-isis.2018.00020

[12] Batu Akan, Afshin Ameri, Baran Cürüklü, and Lars Asplund. 2011. Intuitive
industrial robot programming through incremental multimodal language and
augmented reality. In 2011 IEEE International Conference on Robotics and Au-
tomation. IEEE, 3934–3939. https://doi.org/10.1109/icra.2011.5979887

[13] Takintope Akinbiyi, Carol E Reiley, Sunipa Saha, Darius Burschka, Christopher J
Hasser, David D Yuh, and Allison M Okamura. 2006. Dynamic augmented reality
for sensory substitution in robot-assisted surgical systems. In 2006 International
Conference of the IEEE Engineering in Medicine and Biology Society. IEEE, 567–570.
https://doi.org/10.1109/iembs.2006.259707

[14] Samer Al Moubayed, Jonas Beskow, Gabriel Skantze, and Björn Granström.
2012. Furhat: a back-projected human-like robot head for multiparty human-
machine interaction. In Cognitive behavioural systems. Springer, 114–130. https:
//doi.org/10.1007/978-3-642-34584-5_9

[15] Jacopo Aleotti, Giorgio Micconi, Stefano Caselli, Giacomo Benassi, Nicola Zam-
belli, Manuele Bettelli, and Andrea Zappettini. 2017. Detection of nuclear
sources by UAV teleoperation using a visuo-haptic augmented reality interface.
Sensors 17, 10 (2017), 2234. https://doi.org/10.3390/s17102234

[16] Jason Alexander, Anne Roudaut, Jürgen Steimle, Kasper Hornbæk, Miguel
Bruns Alonso, Sean Follmer, and Timothy Merritt. 2018. Grand challenges
in shape-changing interface research. In Proceedings of the 2018 CHI conference
on human factors in computing systems. 1–14. https://doi.org/10.1145/3173574.
3173873

[17] Omri Alon, Sharon Rabinovich, Chana Fyodorov, and Jessica R Cauchard. 2021.
Drones in Firefighting: A User-Centered Design Perspective. In Proceedings of
the 23rd International Conference on Mobile Human-Computer Interaction. 1–11.
https://doi.org/10.1145/3447526.3472030

[18] Malek Alrashidi, Ahmed Alzahrani, Michael Gardner, and Vic Callaghan. 2016.
A pedagogical virtual machine for assembling mobile robot using augmented
reality. In Proceedings of the 7th Augmented Human International Conference
2016. 1–2. https://doi.org/10.1145/2875194.2875229

14

[19] Malek Alrashidi, Michael Gardner, and Vic Callaghan. 2017. Evaluating the use
of pedagogical virtual machine with augmented reality to support learning em-
bedded computing activity. In Proceedings of the 9th International Conference on
Computer and Automation Engineering. 44–50. https://doi.org/10.1145/3057039.
3057088

[20] Alborz Amir-Khalili, Masoud S Nosrati, Jean-Marc Peyrat, Ghassan Hamarneh,
and Rafeef Abugharbieh. 2013. Uncertainty-encoded augmented reality for
robot-assisted partial nephrectomy: A phantom study. In Augmented Reality
Environments for Medical Imaging and Computer-Assisted Interventions. Springer,
182–191. https://doi.org/10.1007/978-3-642-40843-4_20

[21] Rasmus S Andersen, Ole Madsen, Thomas B Moeslund, and Heni Ben Amor.
2016. Projecting robot intentions into human environments. In 2016 25th IEEE
International Symposium on Robot and Human Interactive Communication (RO-
MAN). IEEE, 294–301. https://doi.org/10.1109/ROMAN.2016.7745145

[22] Sean Andrist, Tomislav Pejsa, Bilge Mutlu, and Michael Gleicher. 2012. Designing
effective gaze mechanisms for virtual agents. In Proceedings of the SIGCHI
conference on Human factors in computing systems. ACM, 705–714.
https:
//doi.org/10.1145/2207676.2207777

[23] Takafumi Aoki, Takashi Matsushita, Yuichiro Iio, Hironori Mitake, Takashi
Toyama, Shoichi Hasegawa, Rikiya Ayukawa, Hiroshi Ichikawa, Makoto Sato,
Takatsugu Kuriyama, et al. 2005. Kobito: virtual brownies. In ACM SIGGRAPH
2005 emerging technologies. 11–es. https://doi.org/10.1145/1187297.1187309

[24] Dejanira Araiza-Illan, Alberto De San Bernabe, Fang Hongchao, and Leong Yong
Shin. 2019. Augmented reality for quick and intuitive robotic packing re-
programming. In 2019 14th ACM/IEEE International Conference on Human-Robot
Interaction (HRI). IEEE, 664–664. https://doi.org/10.1109/hri.2019.8673327
[25] Stephanie Arévalo Arboleda, Tim Dierks, Franziska Rücker, and Jens Gerken.
2020. There’s More than Meets the Eye: Enhancing Robot Control through
Augmented Visual Cues. In Companion of the 2020 ACM/IEEE International Con-
ference on Human-Robot Interaction. 104–106. https://doi.org/10.1145/3371382.
3378240

[26] Stephanie Arévalo Arboleda, Tim Dierks, Franziska Rücker, and Jens Gerken.
2021. Exploring the Visual Space to Improve Depth Perception in Robot Teleop-
eration Using Augmented Reality: The Role of Distance and Target’s Pose in
Time, Success, and Certainty. In IFIP Conference on Human-Computer Interaction.
Springer, 522–543. https://doi.org/10.1007/978-3-030-85623-6_31

[27] Stephanie Arevalo Arboleda, Franziska Rücker, Tim Dierks, and Jens Gerken.
2021. Assisting Manipulation and Grasping in Robot Teleoperation with Aug-
mented Reality Visual Cues. In Proceedings of the 2021 CHI Conference on Human
Factors in Computing Systems. 1–14. https://doi.org/10.1145/3411764.3445398
[28] Michael Argyle and Mark Cook. 1976. Gaze and mutual gaze. (1976). https:

//doi.org/10.1017/S0007125000073980

[29] Pasquale Arpaia, Carmela Bravaccio, Giuseppina Corrado, Luigi Duraccio, Nicola
Moccaldi, and Silvia Rossi. 2020. Robotic Autism Rehabilitation by Wearable
Brain-Computer Interface and Augmented Reality. In 2020 IEEE International
Symposium on Medical Measurements and Applications (MeMeA). IEEE, 1–6.
https://doi.org/10.1109/MeMeA49120.2020.9137144

[30] Doris Aschenbrenner, Jonas SI Rieder, Daniëlle Van Tol, Joris Van Dam, Zoltan
Rusak, Jan Olaf Blech, Mohammad Azangoo, Salo Panu, Karl Kruusamäe,
Houman Masnavi, et al. 2020. Mirrorlabs-creating accessible Digital Twins
of robotic production environment with Mixed Reality. In 2020 IEEE Interna-
tional Conference on Artificial Intelligence and Virtual Reality (AIVR). IEEE, 43–48.
https://doi.org/10.1109/aivr50618.2020.00017

[31] Doris Aschenbrenner, Michael Rojkov, Florian Leutert, Jouke Verlinden, Stephan
Lukosch, Marc Erich Latoschik, and Klaus Schilling. 2018. Comparing different
augmented reality support applications for cooperative repair of an industrial
robot. In 2018 IEEE International Symposium on Mixed and Augmented Reality
Adjunct (ISMAR-Adjunct). IEEE, 69–74. https://doi.org/10.1109/ismar-adjunct.
2018.00036

[32] Ronald T Azuma. 1997. A survey of augmented reality. Presence: teleoperators &
virtual environments 6, 4 (1997), 355–385. https://doi.org/10.1162/pres.1997.6.4.
355

[33] Daniel Bambuˆsek, Zdeněk Materna, Michal Kapinus, Vítězslav Beran, and Pavel
Smrž. 2019. Combining interactive spatial augmented reality with head-mounted
display for end-user collaborative robot programming. In 2019 28th IEEE Inter-
national Conference on Robot and Human Interactive Communication (RO-MAN).
IEEE, 1–8. https://doi.org/10.1109/RO-MAN46459.2019.8956315

[34] Kim Baraka, Ana Paiva, and Manuela Veloso. 2016. Expressive lights for reveal-
ing mobile service robot state. In Robot 2015: Second Iberian Robotics Conference.
Springer, 107–119. https://doi.org/10.1007/978-3-319-27146-0_9

[35] Zoltán Bárdosi, Christian Plattner, Yusuf Özbek, Thomas Hofmann, Srdjan
Milosavljevic, Volker Schartinger, and Wolfgang Freysinger. 2020. CIGuide: in
situ augmented reality laser guidance. International journal of computer assisted
radiology and surgery 15, 1 (2020), 49–57. https://doi.org/10.1007/s11548-019-
02066-1

[36] Patrick Baudisch, Stefanie Mueller, et al. 2017. Personal fabrication. Foundations
and Trends® in Human–Computer Interaction 10, 3–4 (2017), 165–293. https:
//doi.org/10.1561/1100000055

[37] Philipp Beckerle, Claudio Castellini, and Bigna Lenggenhager. 2019. Robotic
interfaces for cognitive psychology and embodiment research: a research
roadmap. Wiley Interdisciplinary Reviews: Cognitive Science 10, 2 (2019), e1486.
https://doi.org/10.1002/wcs.1486

[38] William Bentz, Sahib Dhanjal, and Dimitra Panagou. 2019. Unsupervised learn-
ing of assistive camera views by an aerial co-robot in augmented reality multi-
tasking environments. In 2019 International Conference on Robotics and Automa-
tion (ICRA). IEEE, 3003–3009. https://doi.org/10.1109/icra.2019.8793587
[39] Lorenzo Bianchi, Francesco Chessa, Andrea Angiolini, Laura Cercenelli, Simone
Lodi, Barbara Bortolani, Enrico Molinaroli, Carlo Casablanca, Matteo Droghetti,
Caterina Gaudiano, et al. 2021. The use of augmented reality to guide the intra-
operative frozen section during robot-assisted radical prostatectomy. European
Urology 80, 4 (2021), 480–488. https://doi.org/10.1016/j.eururo.2021.06.020
[40] Mark Billinghurst and Michael Nebeling. 2021. Rapid prototyping for XR. In
SIGGRAPH Asia 2021 Courses. 1–178. https://doi.org/10.1145/3476117.3483444
[41] Oliver Bimber and Ramesh Raskar. 2006. Modern approaches to augmented
reality. In ACM SIGGRAPH 2006 Courses. 1–es. https://doi.org/10.1145/1185657.
1185796

[42] Sebastian Blankemeyer, Rolf Wiemann, Lukas Posniak, Christoph Pregizer, and
Annika Raatz. 2018.
Intuitive robot programming using augmented reality.
Procedia CIRP 76 (2018), 155–160. https://doi.org/10.1016/J.PROCIR.2018.02.028
[43] Andrew Boateng and Yu Zhang. 2021. Virtual Shadow Rendering for Maintaining
Situation Awareness in Proximal Human-Robot Teaming. In Companion of the
2021 ACM/IEEE International Conference on Human-Robot Interaction. 494–498.
https://doi.org/10.1145/3434074.3447221

[44] Gabriele Bolano, Christian Juelg, Arne Roennau, and Ruediger Dillmann. 2019.
Transparent robot behavior using augmented reality in close human-robot
interaction. In 2019 28th IEEE International Conference on Robot and Human
Interactive Communication (RO-MAN). IEEE, 1–7. https://doi.org/10.1109/ro-
man46459.2019.8956296

[45] Gabriele Bolano, Arne Roennau, and Ruediger Dillmann. 2020. Planning and
Evaluation of Robotic Solutions in a Logistic Line Through Augmented Reality.
In 2020 Fourth IEEE International Conference on Robotic Computing (IRC). IEEE,
422–423. https://doi.org/10.1109/irc.2020.00075

[46] Jean Botev and Francisco J Rodríguez Lera. 2021. Immersive Robotic Telep-
resence for Remote Educational Scenarios. Sustainability 13, 9 (2021), 4717.
https://doi.org/10.3390/SU13094717

[47] Gustavo Caiza, Pablo Bonilla-Vasconez, Carlos A Garcia, and Marcelo V Garcia.
2020. Augmented Reality for Robot Control in Low-cost Automation Context
and IoT. In 2020 25th IEEE International Conference on Emerging Technologies and
Factory Automation (ETFA), Vol. 1. IEEE, 1461–1464. https://doi.org/10.1109/
etfa46521.2020.9212056

[48] Davide Calandra, Alberto Cannavò, and Fabrizio Lamberti. 2021. Evaluating an
Augmented Reality-Based Partially Assisted Approach to Remote Assistance in
Heterogeneous Robotic Applications. In 2021 IEEE 7th International Conference
on Virtual Reality (ICVR). IEEE, 380–387. https://doi.org/10.1109/icvr51878.2021.
9483849

[49] Daniel Calife, João Luiz Bernardes Jr, and Romero Tori. 2009. Robot Arena: An
augmented reality platform for game development. Computers in Entertainment
(CIE) 7, 1 (2009), 1–26. https://doi.org/10.1145/1486508.1486519

[50] Laura Cancedda, Alberto Cannavò, Giuseppe Garofalo, Fabrizio Lamberti, Paolo
Montuschi, and Gianluca Paravati. 2017. Mixed reality-based user interaction
feedback for a hand-controlled interface targeted to robot teleoperation. In
International Conference on Augmented Reality, Virtual Reality and Computer
Graphics. Springer, 447–463. https://doi.org/10.1007/978-3-319-60928-7_38

[51] Yuanzhi Cao, Tianyi Wang, Xun Qian, Pawan S Rao, Manav Wadhawan, Ke Huo,
and Karthik Ramani. 2019. GhostAR: A time-space editor for embodied authoring
of human-robot collaborative task with augmented reality. In Proceedings of
the 32nd Annual ACM Symposium on User Interface Software and Technology.
521–534. https://doi.org/10.1145/3332165.3347902

[52] Yuanzhi Cao, Zhuangying Xu, Terrell Glenn, Ke Huo, and Karthik Ramani.
2018. Ani-Bot: A Modular Robotics System Supporting Creation, Tweaking, and
Usage with Mixed-Reality Interactions. In Proceedings of the Twelfth International
Conference on Tangible, Embedded, and Embodied Interaction. 419–428. https:
//doi.org/10.1145/3173225.3173226

[53] Yuanzhi Cao, Zhuangying Xu, Fan Li, Wentao Zhong, Ke Huo, and Karthik
Ramani. 2019. V. Ra: An in-situ visual authoring system for robot-IoT task plan-
ning with augmented reality. In Proceedings of the 2019 on Designing Interactive
Systems Conference. 1059–1070. https://doi.org/10.1145/3322276.3322278
[54] Irvin Steve Cardenas, Kaleb Powlison, and Jong-Hoon Kim. 2021. Reducing
Cognitive Workload in Telepresence Lunar-Martian Environments Through
Audiovisual Feedback in Augmented Reality. In Companion of the 2021 ACM/IEEE
International Conference on Human-Robot Interaction. 463–466. https://doi.org/
10.1145/3434074.3447214

[55] Jon Carroll and Fabrizio Polo. 2013. Augmented reality gaming with sphero. In
ACM Siggraph 2013 Mobile. 1–1. https://doi.org/10.1145/2503512.2503535
[56] Giandomenico Caruso and Paolo Belluco. 2010. Robotic arm for car dash-
board layout assessment in mixed reality environment. In 19th International

15

Symposium in Robot and Human Interactive Communication. IEEE, 62–68.
https://doi.org/10.1109/ROMAN.2010.5598685

[57] Jessica Cauchard, Woody Gover, William Chen, Stephen Cartwright, and Ehud
Sharlin. 2021. Drones in Wonderland–Disentangling Collocated Interaction
Using Radical Form. IEEE Robotics and Automation Letters (2021). https://doi.
org/10.1109/lra.2021.3103653

[58] Jessica R Cauchard, Alex Tamkin, Cheng Yao Wang, Luke Vink, Michelle Park,
Tommy Fang, and James A Landay. 2019. Drone. io: A gestural and visual
interface for human-drone interaction. In 2019 14th ACM/IEEE International
Conference on Human-Robot Interaction (HRI). IEEE, 153–162. https://doi.org/
10.1109/HRI.2019.8673011

[59] Elizabeth Cha, Naomi T Fitter, Yunkyung Kim, Terrence Fong, and Maja J
Matarić. 2018. Effects of Robot Sound on Auditory Localization in Human-Robot
Collaboration. In Proceedings of the 2018 ACM/IEEE International Conference
on Human-Robot Interaction. ACM, 434–442. https://doi.org/10.1145/3171221.
3171285

[60] Elizabeth Cha and Maja Matarić. 2016. Using nonverbal signals to request help
during human-robot collaboration. In 2016 IEEE/RSJ International Conference on
Intelligent Robots and Systems (IROS). IEEE, 5070–5076. https://doi.org/10.1109/
IROS.2016.7759744

[61] Sonia Mary Chacko, Armando Granado, and Vikram Kapila. 2020. An augmented
reality framework for robotic tool-path teaching. Procedia CIRP 93 (2020), 1218–
1223. https://doi.org/10.1016/j.procir.2020.03.143

[62] Sonia Mary Chacko, Armando Granado, Ashwin RajKumar, and Vikram Kapila.
2020. An Augmented Reality Spatial Referencing System for Mobile Robots. In
2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS).
IEEE, 4446–4452. https://doi.org/10.1109/iros45743.2020.9340742

[63] Sonia Mary Chacko and Vikram Kapila. 2019. An augmented reality interface
for human-robot interaction in unconstrained environments. In 2019 IEEE/RSJ
International Conference on Intelligent Robots and Systems (IROS). IEEE, 3222–
3228. https://doi.org/10.1109/iros40897.2019.8967973

[64] Ravi Teja Chadalavada, Henrik Andreasson, Maike Schindler, Rainer Palm,
and Achim J Lilienthal. 2020. Bi-directional navigation intent communication
using spatial augmented reality and eye-tracking glasses for improved safety in
human–robot interaction. Robotics and Computer-Integrated Manufacturing 61
(2020), 101830. https://doi.org/10.1016/j.rcim.2019.101830

[65] Seungho Chae, Hyocheol Ro, Yoonsik Yang, and Tack-Don Han. 2018. A Per-
vasive Assistive Robot System Including Projection-Camera Technology for
Older Adults. In Companion of the 2018 ACM/IEEE International Conference on
Human-Robot Interaction. 83–84. https://doi.org/10.1145/3173386.3177007
[66] Tathagata Chakraborti, Sarath Sreedharan, Anagha Kulkarni, and Subbarao
Kambhampati. 2018. Projection-aware task planning and execution for human-
in-the-loop operation of robots in a mixed-reality workspace. In 2018 IEEE/RSJ
International Conference on Intelligent Robots and Systems (IROS). IEEE, 4476–
4482. https://doi.org/10.1109/IROS.2018.8593830

[67] Wesley P Chan, Geoffrey Hanks, Maram Sakr, Tiger Zuo, HF Machiel Van der
Loos, and Elizabeth Croft. 2020. An augmented reality human-robot physical
collaboration interface design for shared, large-scale, labour-intensive manufac-
turing tasks. In 2020 IEEE/RSJ International Conference on Intelligent Robots and
Systems (IROS). IEEE, 11308–11313. https://doi.org/10.1109/IROS45743.2020.
9341119

[68] Wesley P Chan, Adnan Karim, Camilo P Quintero, HF Machiel Van der Loos,
and Elizabeth Croft. 2018. Virtual barriers in augmented reality for safe human-
robot collaboration in manufacturing. In Robotic Co-Workers 4.0 2018: Human
Safety and Comfort in Human-Robot Interactive Social Environments.

[69] Wesley P Chan, Maram Sakr, Camilo Perez Quintero, Elizabeth Croft, and
HF Machiel Van der Loos. 2020. Towards a Multimodal System combining
Augmented Reality and Electromyography for Robot Trajectory Programming
and Execution. In 2020 29th IEEE International Conference on Robot and Human In-
teractive Communication (RO-MAN). IEEE, 419–424. https://doi.org/10.1109/RO-
MAN47096.2020.9223526

[70] Tom Chandler, Maxime Cordeil, Tobias Czauderna, Tim Dwyer, Jaroslaw
Glowacki, Cagatay Goncu, Matthias Klapperstueck, Karsten Klein, Kim Marriott,
Falk Schreiber, et al. 2015. Immersive analytics. In 2015 Big Data Visual Analytics
(BDVA). IEEE, 1–8. https://doi.org/10.1109/TVCG.2019.2929033

[71] Chih-Wei Chang, Jih-Hsien Lee, Chin-Yeh Wang, and Gwo-Dong Chen. 2010.
Improving the authentic learning experience by integrating robots into the
mixed-reality environment. Computers & Education 55, 4 (2010), 1572–1578.
https://doi.org/10.1016/j.compedu.2010.06.023

[72] Siam Charoenseang and Tarinee Tonggoed. 2011. Human–robot collabora-
tion with augmented reality. In International Conference on Human-Computer
Interaction. Springer, 93–97. https://doi.org/10.1007/978-3-642-22095-1_19
[73] Hua Chen, Oliver Wulf, and Bernardo Wagner. 2006. Object detection for a
mobile robot using mixed reality. In International Conference on Virtual Systems
and Multimedia. Springer, 466–475. https://doi.org/10.1007/11890881_51
[74] Ian Yen-Hung Chen, Bruce MacDonald, Burkhard Wünsche, Geoffrey Biggs,
and Tetsuo Kotoku. 2010. Analysing mixed reality simulation for industrial
applications: A case study in the development of a robotic screw remover

system. In International Conference on Simulation, Modeling, and Programming
for Autonomous Robots. Springer, 350–361. https://doi.org/10.1007/978-3-642-
17319-6_33

[75] Linfeng Chen, Akiyuki Ebi, Kazuki Takashima, Kazuyuki Fujita, and Yoshifumi
Kitamura. 2019. PinpointFly: An egocentric position-pointing drone interface
using mobile AR. In SIGGRAPH Asia 2019 Emerging Technologies. 34–35. https:
//doi.org/10.1145/3355049.3360534

[76] Linfeng Chen, Kazuki Takashima, Kazuyuki Fujita, and Yoshifumi Kitamura.
2021. PinpointFly: An Egocentric Position-control Drone Interface using Mobile
AR. In Proceedings of the 2021 CHI Conference on Human Factors in Computing
Systems. 1–13. https://doi.org/10.1145/3411764.3445110

[77] Long Chen, Fengfeng Zhang, Wei Zhan, Minfeng Gan, and Lining Sun. 2020.
Optimization of virtual and real registration technology based on augmented
reality in a surgical navigation system. Biomedical engineering online 19, 1
(2020), 1–28. https://doi.org/10.1186/s12938-019-0745-z

[78] Mingxuan Chen, Ping Zhang, Zebo Wu, and Xiaodan Chen. 2020. A multichannel
human-swarm robot interaction system in augmented reality. Virtual Reality &
Intelligent Hardware 2, 6 (2020), 518–533. https://doi.org/10.1016/j.vrih.2020.05.
006

[79] Xiaogang Chen, Xiaoshan Huang, Yijun Wang, and Xiaorong Gao. 2020. Combi-
nation of augmented reality based brain-computer interface and computer
vision for high-level control of a robotic arm.
IEEE Transactions on Neu-
ral Systems and Rehabilitation Engineering 28, 12 (2020), 3140–3147. https:
//doi.org/10.1109/tnsre.2020.3038209

[80] Zhe Chen, Zhuohang Cao, Peili Ma, and Lijun Xu. 2020.

Industrial Robot
Training Platform Based on Virtual Reality and Mixed Reality Technology.
In International Conference on Man-Machine-Environment System Engineering.
Springer, 891–898. https://doi.org/10.1007/978-981-15-6978-4_102

[81] Vijay Chidambaram, Yueh-Hsuan Chiang, and Bilge Mutlu. 2012. Designing
persuasive robots: how robots might persuade people using vocal and nonverbal
cues. In Proceedings of the seventh annual ACM/IEEE international conference on
Human-Robot Interaction. 293–300. https://doi.org/10.1145/2157689.2157798

[82] Seung Wook Choi, Hee Chan Kim, Heung Sik Kang, Seongjun Kim, and Jaesoon
Choi. 2013. A haptic augmented reality surgeon console for a laparoscopic
surgery robot system. In 2013 13th International Conference on Control, Automa-
tion and Systems (ICCAS 2013). IEEE, 355–357. https://doi.org/10.1109/iccas.
2013.6703923

[83] Jonathan Wun Shiung Chong, SKc Ong, Andrew YC Nee, and KB Youcef-Youmi.
2009. Robot programming using augmented reality: An interactive method for
planning collision-free paths. Robotics and Computer-Integrated Manufacturing
25, 3 (2009), 689–701. https://doi.org/10.1016/J.RCIM.2008.05.002

[84] Wusheng Chou, Tianmiao Wang, and Yuru Zhang. 2004. Augmented real-
ity based preoperative planning for robot assisted tele-neurosurgery. In 2004
IEEE International Conference on Systems, Man and Cybernetics (IEEE Cat. No.
04CH37583), Vol. 3. IEEE, 2901–2906. https://doi.org/10.1109/icsmc.2004.1400773
[85] Nicklas H Christensen, Oliver G Hjermitslev, Frederik Falk, Marco B Madsen,
Frederik H Østergaard, Martin Kibsgaard, Martin Kraus, Johan Poulsen, and Jane
Petersson. 2017. Depth cues in augmented reality for training of robot-assisted
minimally invasive surgery. In Proceedings of the 21st International Academic
Mindtrek Conference. 120–126. https://doi.org/10.1145/3131085.3131123
[86] Francesco Clemente, Strahinja Dosen, Luca Lonini, Marko Markovic, Dario
Farina, and Christian Cipriani. 2016. Humans can integrate augmented reality
feedback in their sensorimotor control of a robotic hand. IEEE Transactions on
Human-Machine Systems 47, 4 (2016), 583–589. https://doi.org/10.1109/thms.
2016.2611998

[87] Marcelo Coelho and Jamie Zigelbaum. 2011. Shape-changing interfaces. Personal
and Ubiquitous Computing 15, 2 (2011), 161–173. https://doi.org/10.1007/s00779-
010-0311-y

[88] Michael D Coovert, Tiffany Lee, Ivan Shindev, and Yu Sun. 2014. Spatial aug-
mented reality as a method for a mobile robot to communicate intended move-
ment. Computers in Human Behavior 34 (2014), 241–248. https://doi.org/10.
1016/j.chb.2014.02.001

[89] Austin Corotan and Jianna Jian Zhang Irgen-Gioro. 2019. An Indoor Navigation
Robot Using Augmented Reality. In 2019 5th International Conference on Control,
Automation and Robotics (ICCAR). IEEE, 111–116. https://doi.org/10.1109/iccar.
2019.8813348

[90] Hugo Costa, Peter Cebola, Tiago Cunha, and Armando Sousa. 2015. A mixed
reality game using 3Pi robots—“PiTanks”. In 2015 10th Iberian Conference on
Information Systems and Technologies (CISTI). IEEE, 1–6. https://doi.org/10.
1109/CISTI.2015.7170600

[91] Nuno Costa and Artur Arsenio. 2015. Augmented reality behind the wheel-
human interactive assistance by mobile robots. In 2015 6th International Con-
ference on Automation, Robotics and Applications (ICARA). IEEE, 63–69. https:
//doi.org/10.1109/ICARA.2015.7081126

[92] Ève Coste-Manière, Louaï Adhami, Fabien Mourgues, and Alain Carpentier. 2003.
Planning, simulation, and augmented reality for robotic cardiac procedures:
the STARS system of the ChIR team. In Seminars in thoracic and cardiovascular
surgery, Vol. 15. Elsevier, 141–156. https://doi.org/10.1016/S1043-0679(03)70022-

16

7

[93] Matthew Cousins, Chenguang Yang, Junshen Chen, Wei He, and Zhaojie Ju. 2017.
Development of a mixed reality based interface for human robot interaciotn.
In 2017 International Conference on Machine Learning and Cybernetics (ICMLC),
Vol. 1. IEEE, 27–34. https://doi.org/10.1109/icmlc.2017.8107738

[94] Oscar Danielsson, Anna Syberfeldt, Rodney Brewster, and Lihui Wang. 2017.
Assessing instructions in augmented reality for human-robot collaborative
assembly by using demonstrators. Procedia CIRP 63 (2017), 89–94. https:
//doi.org/10.1016/J.PROCIR.2017.02.038

[95] Kurtis Danyluk, Barrett Ens, Bernhard Jenny, and Wesley Willett. 2021. A
Design Space Exploration of Worlds in Miniature. In Proceedings of the 2021 CHI
Conference on Human Factors in Computing Systems. 1–15. https://doi.org/10.
1145/3411764.3445098

[96] Rajkumar Darbar, Joan Sol Roo, Thibault Lainé, and Martin Hachet. 2019. Drone-
SAR: extending physical spaces in spatial augmented reality using projection
on a drone. In Proceedings of the 18th International Conference on Mobile and
Ubiquitous Multimedia. 1–7. https://doi.org/10.1145/3365610.3365631

[97] Devleena Das, Siddhartha Banerjee, and Sonia Chernova. 2021. Explainable
ai for robot failures: Generating explanations that improve user assistance in
fault recovery. In Proceedings of the 2021 ACM/IEEE International Conference on
Human-Robot Interaction. 351–360. https://doi.org/10.1145/3434073.3444657

[98] Alessandro De Franco, Edoardo Lamon, Pietro Balatti, Elena De Momi, and Arash
Ajoudani. 2019. An Intuitive augmented reality interface for task scheduling,
monitoring, and work performance improvement in human-robot collaboration.
In 2019 IEEE International Work Conference on Bioinspired Intelligence (IWOBI).
IEEE, 75–80. https://doi.org/10.1109/iwobi47054.2019.9114472

[99] Artem Dementyev, Hsin-Liu Kao, Inrak Choi, Deborah Ajilo, Maggie Xu,
Joseph A Paradiso, Chris Schmandt, and Sean Follmer. 2016. Rovables: Minia-
ture on-body robots as mobile wearables. In Proceedings of the 29th Annual
Symposium on User Interface Software and Technology. 111–120. https://doi.org/
10.1145/2984511.2984531

[100] Morteza Dianatfar, Jyrki Latokartano, and Minna Lanz. 2021. Review on existing
VR/AR solutions in human–robot collaboration. Procedia CIRP 97 (2021), 407–
411. https://doi.org/10.1016/j.procir.2020.05.259

[101] Adhitha Dias, Hasitha Wellaboda, Yasod Rasanka, Menusha Munasinghe, Ranga
Rodrigo, and Peshala Jayasekara. 2020. Deep Learning of Augmented Reality
based Human Interactions for Automating a Robot Team. In 2020 6th Interna-
tional Conference on Control, Automation and Robotics (ICCAR). IEEE, 175–182.
https://doi.org/10.1109/iccar49639.2020.9108004

[102] Tiago Dias, Pedro Miraldo, Nuno Gonçalves, and Pedro U Lima. 2015. Augmented
reality on robot navigation using non-central catadioptric cameras. In 2015
IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE,
4999–5004. https://doi.org/10.1109/iros.2015.7354080

[103] Maximilian Diehl, Alexander Plopski, Hirokazu Kato, and Karinne Ramirez-
Amaro. 2020. Augmented Reality interface to verify Robot Learning. In 2020 29th
IEEE International Conference on Robot and Human Interactive Communication
(RO-MAN). IEEE, 378–383. https://doi.org/10.1109/ro-man47096.2020.9223502
[104] André Dietrich, Michael Schulze, Sebastian Zug, and Jörg Kaiser. 2010. Vi-
sualization of robot’s awareness and perception. In Proceedings of the First
International Workshop on Digital Engineering. 38–44. https://doi.org/10.1145/
1837154.1837160

[105] Huy Dinh, Quilong Yuan, Iastrebov Vietcheslav, and Gerald Seet. 2017. Aug-
mented reality interface for taping robot. In 2017 18th International Conference
on Advanced Robotics (ICAR). IEEE, 275–280. https://doi.org/10.1109/ICAR.2017.
8023530

[106] Anca D Dragan, Kenton CT Lee, and Siddhartha S Srinivasa. 2013. Legibility and
predictability of robot motion. In 2013 8th ACM/IEEE International Conference
on Human-Robot Interaction (HRI). IEEE, 301–308. https://doi.org/10.1109/HRI.
2013.6483603

[107] Mauro Dragone, Thomas Holz, and Gregory MP O’Hare. 2006. Mixing robotic
realities. In Proceedings of the 11th international conference on Intelligent user
interfaces. 261–263. https://doi.org/10.1145/1111449.1111504

[108] Mauro Dragone, Thomas Holz, and Gregory MP O’Hare. 2007. Using mixed
reality agents as social interfaces for robots. In RO-MAN 2007-The 16th IEEE
International Symposium on Robot and Human Interactive Communication. IEEE,
1161–1166. https://doi.org/10.1109/roman.2007.4415255

[109] Mauro Dragone, Thomas Holz, GMP O’Hare, and Michael J O’Grady. 2009.
Mixed Reality Agent (MiRA) Chameleons. In Agent-Based Ubiquitous Computing.
Springer, 13–33. https://doi.org/10.2991/978-94-91216-31-2_2

[110] Philip Edgcumbe, Rohit Singla, Philip Pratt, Caitlin Schneider, Christopher
Nguan, and Robert Rohling. 2016. Augmented reality imaging for robot-assisted
partial nephrectomy surgery. In International Conference on Medical Imaging
and Augmented Reality. Springer, 139–150. https://doi.org/10.1007/978-3-319-
43775-0_13

[111] Lotfi El Hafi, Hitoshi Nakamura, Akira Taniguchi, Yoshinobu Hagiwara, and
Tadahiro Taniguchi. 2021. Teaching system for multimodal object categorization
by human-robot interaction in mixed reality. In 2021 IEEE/SICE International
Symposium on System Integration (SII). IEEE, 320–324. https://doi.org/10.1109/

IEEECONF49454.2021.9382607

10.1109/indiancc.2016.7441163

[112] Ahmed Elsharkawy, Khawar Naheem, Dongwoo Koo, and Mun Sang Kim. 2021.
A UWB-Driven Self-Actuated Projector Platform for Interactive Augmented
Reality Applications. Applied Sciences 11, 6 (2021), 2871. https://doi.org/10.
3390/app11062871

[113] Barrett Ens, Benjamin Bach, Maxime Cordeil, Ulrich Engelke, Marcos Serrano,
Wesley Willett, Arnaud Prouzeau, Christoph Anthes, Wolfgang Büschel, Cody
Dunne, et al. 2021. Grand challenges in immersive analytics. In Proceedings of
the 2021 CHI Conference on Human Factors in Computing Systems. 1–17. https:
//doi.org/10.1145/3411764.3446866

[114] Okan Erat, Werner Alexander Isop, Denis Kalkofen, and Dieter Schmalstieg.
2018. Drone-augmented human vision: Exocentric control for drones exploring
hidden areas. IEEE transactions on visualization and computer graphics 24, 4
(2018), 1437–1446. https://doi.org/10.1109/TVCG.2018.2794058

[115] David Estevez, Juan G Victores, Santiago Morante, and Carlos Balaguer. 2015.
Robot devastation: Using DIY low-cost platforms for multiplayer interaction in
an augmented reality game. In 2015 7th International Conference on Intelligent
Technologies for Interactive Entertainment (INTETAIN). IEEE, 32–36. https:
//doi.org/10.4108/icst.intetain.2015.259753

[116] Aluna Everitt and Jason Alexander. 2017. PolySurface: a design approach for
rapid prototyping of shape-changing displays using semi-solid surfaces. In
Proceedings of the 2017 Conference on Designing Interactive Systems. 1283–1294.
https://doi.org/10.1145/3064663.3064677

[117] Aluna Everitt and Jason Alexander. 2019. 3D Printed Deformable Surfaces for
Shape-Changing Displays. Frontiers in Robotics and AI 6 (2019), 80. https:
//doi.org/10.3389/frobt.2019.00080

[118] A Evlampev and M Ostanin. 2019. Obstacle avoidance for robotic manipulator
using Mixed reality glasses. In 2019 3rd School on Dynamics of Complex Networks
and their Application in Intellectual Robotics (DCNAIR). IEEE, 46–48. https:
//doi.org/10.1109/dcnair.2019.8875555

[119] Volkmar Falk, Fabien Mourgues, Louaï Adhami, Stefan Jacobs, Holger Thiele,
Stefan Nitzsche, Friedrich W Mohr, and Ève Coste-Manière. 2005. Cardio navi-
gation: planning, simulation, and augmented reality in robotic assisted endo-
scopic bypass grafting. The Annals of thoracic surgery 79, 6 (2005), 2040–2047.
https://doi.org/10.1016/J.ATHORACSUR.2004.11.060

[120] HC Fang, SK Ong, and AYC Nee. 2012. Interactive robot trajectory planning
and simulation using augmented reality. Robotics and Computer-Integrated
Manufacturing 28, 2 (2012), 227–237. https://doi.org/10.1016/J.RCIM.2011.09.003
[121] HC Fang, SK Ong, and AYC Nee. 2012. Robot path and end-effector orientation
planning using augmented reality. Procedia CIRP 3 (2012), 191–196. https:
//doi.org/10.1016/J.PROCIR.2012.07.034

[122] HC Fang, SK Ong, and AYC Nee. 2013. Orientation planning of robot end-effector
using augmented reality. The International Journal of Advanced Manufacturing
Technology 67, 9-12 (2013), 2033–2049. https://doi.org/10.1007/S00170-012-
4629-7

[123] HC Fang, SK Ong, and AYC Nee. 2014. A novel augmented reality-based interface
for robot path planning. International Journal on Interactive Design and Manu-
facturing (IJIDeM) 8, 1 (2014), 33–42. https://doi.org/10.1007/S12008-013-0191-2
[124] Hongchao Fang, Soh Khim Ong, and Andrew Yeh-Ching Nee. 2009. Robot
programming using augmented reality. In 2009 International Conference on
CyberWorlds. IEEE, 13–20. https://doi.org/10.1109/CW.2009.14

[125] Federica Ferraguti, Marco Minelli, Saverio Farsoni, Stefano Bazzani, Marcello
Bonfè, Alexandre Vandanjon, Stefano Puliatti, Giampaolo Bianchi, and Cris-
tian Secchi. 2020. Augmented reality and robotic-assistance for percutaneous
nephrolithotomy. IEEE robotics and automation letters 5, 3 (2020), 4556–4563.
https://doi.org/10.1109/lra.2020.3002216

[126] Michael Filipenko, Alexander Poeppel, Alwin Hoffmann, Wolfgang Reif, Andreas
Monden, and Markus Sause. 2020. Virtual commissioning with mixed reality
for next-generation robot-based mechanical component testing. In ISR 2020;
52th International Symposium on Robotics. VDE, 1–6. https://doi.org/10.14236/
EWIC/EVA2008.3

[127] Sean Follmer, Daniel Leithinger, Alex Olwal, Akimitsu Hogge, and Hiroshi Ishii.
2013. inFORM: dynamic physical affordances and constraints through shape
and object actuation.. In Uist, Vol. 13. 2501988–2502032. https://doi.org/10.1145/
2501988.2502032

[128] Jason Fong, Renz Ocampo, Douglas P Gross, and Mahdi Tavakoli. 2019. A
robot with an augmented-reality display for functional capacity evaluation and
rehabilitation of injured workers. In 2019 IEEE 16th International Conference on
Rehabilitation Robotics (ICORR). IEEE, 181–186. https://doi.org/10.1109/icorr.
2019.8779417

[129] Jutta Fortmann, Tim Claudius Stratmann, Susanne Boll, Benjamin Poppinga,
and Wilko Heuten. 2013. Make me move at work! An ambient light display
to increase physical activity. In 2013 7th International Conference on Pervasive
Computing Technologies for Healthcare and Workshops. IEEE, 274–277. https:
//doi.org/10.4108/icst.pervasivehealth.2013.252089

[130] Jared A Frank and Vikram Kapila. 2016. Towards teleoperation-based interactive
learning of robot kinematics using a mobile augmented reality interface on a
tablet. In 2016 Indian Control Conference (ICC). IEEE, 385–392. https://doi.org/

17

[131] Jared Alan Frank, Sai Prasanth Krishnamoorthy, and Vikram Kapila. 2017. To-
ward mobile mixed-reality interaction with multi-robot systems. IEEE Robotics
and Automation Letters 2, 4 (2017), 1901–1908. https://doi.org/10.1109/LRA.
2017.2714128

[132] Jared A Frank, Matthew Moorhead, and Vikram Kapila. 2016. Realizing mixed-
reality environments with tablets for intuitive human-robot collaboration for
object manipulation tasks. In 2016 25th IEEE International Symposium on Robot
and Human Interactive Communication (RO-MAN). IEEE, 302–307. https://doi.
org/10.1109/ROMAN.2016.7745146

[133] Jared A Frank, Matthew Moorhead, and Vikram Kapila. 2017. Mobile mixed-
reality interfaces that enhance human–robot interaction in shared spaces. Fron-
tiers in Robotics and AI 4 (2017), 20. https://doi.org/10.3389/frobt.2017.00020

[134] Ayaka Fujii, Kanae Kochigami, Shingo Kitagawa, Kei Okada, and Masayuki
Inaba. 2020. Development and Evaluation of Mixed Reality Co-eating System:
Sharing the Behavior of Eating Food with a Robot Could Improve Our Dining
Experience. In 2020 29th IEEE International Conference on Robot and Human
Interactive Communication (RO-MAN). IEEE, 357–362. https://doi.org/10.1109/
ro-man47096.2020.9223518

[135] Richard Fung, Sunao Hashimoto, Masahiko Inami, and Takeo Igarashi. 2011. An
augmented reality system for teaching sequential tasks to a household robot. In
2011 RO-MAN. IEEE, 282–287. https://doi.org/10.1109/roman.2011.6005235

[136] Anna Fuste, Ben Reynolds, James Hobin, and Valentin Heun. 2020. Kinetic AR:
A Framework for Robotic Motion Systems in Spatial Computing. In Extended
Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems.
1–8. https://doi.org/10.1145/3334480.3382814

[137] Samir Yitzhak Gadre, Eric Rosen, Gary Chien, Elizabeth Phillips, Stefanie Tellex,
and George Konidaris. 2019. End-user robot programming using mixed reality. In
2019 International conference on robotics and automation (ICRA). IEEE, 2707–2713.
https://doi.org/10.1109/icra.2019.8793988

[138] Ramsundar Kalpagam Ganesan, Yash K Rathore, Heather M Ross, and Heni Ben
Amor. 2018. Better teaming through visual cues: how projecting imagery in a
workspace can improve human-robot collaboration. IEEE Robotics & Automation
Magazine 25, 2 (2018), 59–71. https://doi.org/10.1109/mra.2018.2815655
[139] Peng Gao, Brian Reily, Savannah Paul, and Hao Zhang. 2020. Visual reference
of ambiguous objects for augmented reality-powered human-robot communi-
cation in a shared workspace. In International Conference on Human-Computer
Interaction. Springer, 550–561. https://doi.org/10.1007/978-3-030-49695-1_37
[140] Yuxiang Gao and Chien-Ming Huang. 2019. PATI: a projection-based augmented
table-top interface for robot programming. In Proceedings of the 24th interna-
tional conference on intelligent user interfaces. 345–355. https://doi.org/10.1145/
3301275.3302326

[141] Yuan Gao, Elena Sibirtseva, Ginevra Castellano, and Danica Kragic. 2019. Fast
adaptation with meta-reinforcement learning for trust modelling in human-
robot interaction. In 2019 IEEE/RSJ International Conference on Intelligent Robots
and Systems (IROS). IEEE, 305–312. https://doi.org/10.1109/IROS40897.2019.
8967924

[142] Abraham Prieto García, Gervasio Varela Fernández, Blanca María Priego Torres,
and Fernando López-Peña. 2011. Educational autonomous robotics setup using
mixed reality. In 2011 7th International Conference on Next Generation Web
Services Practices. IEEE, 452–457. https://doi.org/10.1109/nwesp.2011.6088222
[143] Andre Gaschler, Maximilian Springer, Markus Rickert, and Alois Knoll. 2014.
Intuitive robot tasks with augmented reality and virtual obstacles. In 2014 IEEE
International Conference on Robotics and Automation (ICRA). IEEE, 6026–6031.
https://doi.org/10.1109/icra.2014.6907747

[144] Hakan GENÇTÜRK and Uğur YAYAN. [n.d.]. Development of Augmented
Reality Based Mobile Robot Maintenance Software. In 2019 Innovations in
Intelligent Systems and Applications Conference (ASYU). IEEE, 1–5.
https:
//doi.org/10.1109/asyu48272.2019.8946359

[145] Fabrizio Ghiringhelli, Jérôme Guzzi, Gianni A Di Caro, Vincenzo Caglioti,
Luca M Gambardella, and Alessandro Giusti. 2014.
Interactive augmented
reality for understanding and analyzing multi-robot systems. In 2014 IEEE/RSJ
International Conference on Intelligent Robots and Systems. IEEE, 1195–1201.
https://doi.org/10.1109/iros.2014.6942709

[146] Mario Gianni, Federico Ferri, and Fiora Pirri. 2013. ARE: Augmented reality
environment for mobile robots. In Conference Towards Autonomous Robotic
Systems. Springer, 470–483. https://doi.org/10.1007/978-3-662-43645-5_48

[147] Fabio Giannone, Emanuele Felli, Zineb Cherkaoui, Pietro Mascagni, and Patrick
Pessaux. 2021. Augmented Reality and Image-Guided Robotic Liver Surgery.
Cancers 13, 24 (2021), 6268. https://doi.org/10.3390/cancers13246268

[148] Antonio Gomes, Calvin Rubens, Sean Braley, and Roel Vertegaal. 2016. Bit-
drones: Towards using 3d nanocopter displays as interactive self-levitating
programmable matter. In Proceedings of the 2016 CHI Conference on Human Fac-
tors in Computing Systems. 770–780. https://doi.org/10.1145/2858036.2858519
[149] Liang Gong, Changyang Gong, Zhao Ma, Lujie Zhao, Zhenyu Wang, Xudong Li,
Xiaolong Jing, Haozhe Yang, and Chengliang Liu. 2017. Real-time human-in-the-
loop remote control for a life-size traffic police robot with multiple augmented
reality aided display terminals. In 2017 2nd International Conference on Advanced

Robotics and Mechatronics (ICARM). IEEE, 420–425. https://doi.org/10.1109/
icarm.2017.8273199

[150] LL Gong, SK Ong, and AYC Nee. 2019. Projection-based augmented reality
interface for robot grasping tasks. In Proceedings of the 2019 4th International
Conference on Robotics, Control and Automation. 100–104. https://doi.org/10.
1145/3351180.3351204

[151] Michael A Goodrich and Alan C Schultz. 2008. Human-robot interaction: a survey.

Now Publishers Inc. https://doi.org/10.1561/1100000005

[152] Gregory R Gossweiler, Cameron L Brown, Gihan B Hewage, Eitan Sapiro-
Gheiler, William J Trautman, Garrett W Welshofer, and Stephen L Craig. 2015.
Mechanochemically active soft robots. ACS applied materials & interfaces 7, 40
(2015), 22431–22435. https://doi.org/10.1021/acsami.5b06440

[153] Michael Gradmann, Eric M Orendt, Edgar Schmidt, Stephan Schweizer, and
Dominik Henrich. 2018. Augmented reality robot operation interface with
Google Tango. In ISR 2018; 50th International Symposium on Robotics. VDE, 1–8.
[154] Keith Evan Green. 2016. Architectural robotics: ecosystems of bits, bytes, and

biology. MIT Press.

[155] Scott A Green, Mark Billinghurst, XiaoQi Chen, and J Geoffrey Chase. 2008.
Human-robot collaboration: A literature review and augmented reality approach
in design. International journal of advanced robotic systems 5, 1 (2008), 1. https:
//doi.org/10.5772/5664

[156] Scott A Green, Xioa Qi Chen, Mark Billinghurst, and J Geoffrey Chase. 2008.
Collaborating with a mobile robot: An augmented reality multimodal interface.
IFAC Proceedings Volumes 41, 2 (2008), 15595–15600. https://doi.org/10.3182/
20080706-5-KR-1001.02637

[157] Santiago Grijalva and Wilbert G Aguilar. 2019. Landmark-Based Virtual Path
Estimation for Assisted UAV FPV Tele-Operation with Augmented Reality. In
International Conference on Intelligent Robotics and Applications. Springer, 688–
700. https://doi.org/10.1007/978-3-030-27529-7_58

[158] Thomas Groechel, Zhonghao Shi, Roxanna Pakkar, and Maja J Matarić. 2019.
Using socially expressive mixed reality arms for enhancing low-expressivity
robots. In 2019 28th IEEE International Conference on Robot and Human Interactive
Communication (RO-MAN). IEEE, 1–8. https://doi.org/10.1109/ro-man46459.
2019.8956458

[159] Jens Emil Grønbæk, Majken Kirkegaard Rasmussen, Kim Halskov, and Mari-
anne Graves Petersen. 2020. KirigamiTable: Designing for proxemic transitions
with a shape-changing tabletop. In Proceedings of the 2020 CHI Conference on
Human Factors in Computing Systems. 1–15. https://doi.org/10.1145/3313831.
3376834

[160] Uwe Gruenefeld, Lars Prädel, Jannike Illing, Tim Stratmann, Sandra Drolshagen,
and Max Pfingsthorn. 2020. Mind the ARm: realtime visualization of robot mo-
tion intent in head-mounted augmented reality. In Proceedings of the Conference
on Mensch und Computer. 259–266. https://doi.org/10.1145/3404983.3405509

[161] Jan Guhl, Johannes Hügle, and Jörg Krüger. 2018. Enabling human-robot-
interaction via virtual and augmented reality in distributed control systems.
Procedia CIRP 76 (2018), 167–170. https://doi.org/10.1016/J.PROCIR.2018.01.029
[162] Jan Guhl, Son Tung, and Jörg Kruger. 2017. Concept and architecture for
programming industrial robots using augmented reality with mobile devices
like microsoft HoloLens. In 2017 22nd IEEE International Conference on Emerging
Technologies and Factory Automation (ETFA). IEEE, 1–4. https://doi.org/10.1109/
etfa.2017.8247749

[163] Cheng Guo, James Everett Young, and Ehud Sharlin. 2009. Touch and toys:
new techniques for interaction with a remote group of robots. In Proceedings of
the SIGCHI conference on human factors in computing systems. 491–500. https:
//doi.org/10.1145/1518701.1518780

[164] Akihiro Hamada, Atsuro Sawada, Jin Kono, Masanao Koeda, Katsuhiko On-
ishi, Takashi Kobayashi, Toshinari Yamasaki, Takahiro Inoue, Hiroshi Noborio,
and Osamu Ogawa. 2020. The current status and challenges in augmented-
reality navigation system for robot-assisted laparoscopic partial nephrectomy.
In International Conference on Human-Computer Interaction. Springer, 620–629.
https://doi.org/10.1007/978-3-030-49062-1_42

[165] Jared Hamilton, Thao Phung, Nhan Tran, and Tom Williams. 2021. What’s
The Point? Tradeoffs Between Effectiveness and Social Perception When Using
Mixed Reality to Enhance Gesturally Limited Robots. In Proceedings of the
2021 ACM/IEEE International Conference on Human-Robot Interaction. 177–186.
https://doi.org/10.1145/3434073.3444676

[166] Jeonghye Han, Miheon Jo, Eunja Hyun, and Hyo-Jeong So. 2015. Examining
young children’s perception toward augmented reality-infused dramatic play.
Educational Technology Research and Development 63, 3 (2015), 455–474. https:
//doi.org/10.1007/S11423-015-9374-9

[167] John Hardy, Christian Weichel, Faisal Taher, John Vidler, and Jason Alexander.
2015. Shapeclip: towards rapid prototyping with shape-changing displays for
designers. In Proceedings of the 33rd Annual ACM Conference on Human Factors
in Computing Systems. 19–28. https://doi.org/10.1145/2702123.2702599
[168] Jeremy Hartmann, Yen-Ting Yeh, and Daniel Vogel. 2020. AAR: Augmenting a
wearable augmented reality display with an actuated head-mounted projector.
In Proceedings of the 33rd Annual ACM Symposium on User Interface Software
and Technology. 445–458. https://doi.org/10.1145/3379337.3415849

18

[169] Sunao Hashimoto, Akihiko Ishida, Masahiko Inami, and Takeo Igarashi. 2011.
Touchme: An augmented reality based remote robot manipulation. In The 21st
International Conference on Artificial Reality and Telexistence, Proceedings of
ICAT2011, Vol. 2.

[170] Hooman Hedayati, Ryo Suzuki, Daniel Leithinger, and Daniel Szafir. 2020. Puffer-
bot: Actuated expandable structures for aerial robots. In 2020 IEEE/RSJ Inter-
national Conference on Intelligent Robots and Systems (IROS). IEEE, 1338–1343.
https://doi.org/10.1109/iros45743.2020.9341088

[171] Hooman Hedayati, Michael Walker, and Daniel Szafir. 2018. Improving col-
located robot teleoperation with augmented reality. In Proceedings of the
2018 ACM/IEEE International Conference on Human-Robot Interaction. 78–86.
https://doi.org/10.1145/3171221.3171251

[172] Mary Hegarty, Matt S Canham, and Sara I Fabrikant. 2010. Thinking about the
weather: How display salience and knowledge affect performance in a graphic
inference task. Journal of Experimental Psychology: Learning, Memory, and
Cognition 36, 1 (2010), 37. https://doi.org/10.1037/a0017683

[173] Viviane Herdel, Anastasia Kuzminykh, Andrea Hildebrandt, and Jessica R
Cauchard. 2021. Drone in Love: Emotional Perception of Facial Expressions on
Flying Robots. In Proceedings of the 2021 CHI Conference on Human Factors in
Computing Systems. 1–20. https://doi.org/10.1145/3411764.3445495

[174] Juan David Hernández, Shlok Sobti, Anthony Sciola, Mark Moll, and Lydia E
Kavraki. 2020. Increasing robot autonomy via motion planning and an aug-
mented reality interface.
IEEE Robotics and Automation Letters 5, 2 (2020),
1017–1023. https://doi.org/10.1109/lra.2020.2967280

[175] Takayuki Hirai, Satoshi Nakamaru, Yoshihiro Kawahara, and Yasuaki Kakehi.
2018. xslate: A stiffness-controlled surface for shape-changing interfaces. In
Extended Abstracts of the 2018 CHI Conference on Human Factors in Computing
Systems. 1–4. https://doi.org/10.1145/3170427.3186496

[176] Takefumi Hiraki, Shogo Fukushima, Yoshihiro Kawahara, and Takeshi Naemura.
2018. Phygital field: An integrated field with physical robots and digital images
using projection-based localization and control method. SICE Journal of Control,
Measurement, and System Integration 11, 4 (2018), 302–311. https://doi.org/10.
9746/jcmsi.11.302

[177] Takefumi Hiraki, Shogo Fukushima, Yoshihiro Kawahara, and Takeshi Naemura.
2019. NavigaTorch: Projection-based Robot Control Interface using High-speed
Handheld Projector.
In SIGGRAPH Asia 2019 Emerging Technologies. 31–33.
https://doi.org/10.1145/3355049.3360538

[178] Takefumi Hiraki, Shogo Fukushima, and Takeshi Naemura. 2016. Phygital
field: an integrated field with a swarm of physical robots and digital images.
In SIGGRAPH ASIA 2016 Emerging Technologies. 1–2. https://doi.org/10.1145/
2988240.2988242

[179] Takefumi Hiraki, Issei Takahashi, Shotaro Goto, Shogo Fukushima, and Takeshi
Naemura. 2015. Phygital field: integrated field with visible images and robot
swarm controlled by invisible images. In ACM SIGGRAPH 2015 Posters. 1–1.
https://doi.org/10.1145/2787626.2792604

[180] Yutaka Hiroi, Shuhei Hisano, and Akinori Ito. 2010. Evaluation of head size
of an interactive robot using an augmented reality. In 2010 World Automation
Congress. IEEE, 1–6. https://doi.org/10.1109/ro-man46459.2019.8956315
[181] Tzu-Hsuan Ho and Kai-Tai Song. 2020. Supervised control for robot-assisted
surgery using augmented reality. In 2020 20th International Conference on Control,
Automation and Systems (ICCAS). IEEE, 329–334. https://doi.org/10.23919/
ICCAS50221.2020.9268278

[182] Khoa Cong Hoang, Wesley P Chan, Steven Lay, Akansel Cosgun, and Elizabeth
Croft. 2021. Virtual Barriers in Augmented Reality for Safe and Effective Human-
Robot Cooperation in Manufacturing. arXiv preprint arXiv:2104.05211 (2021).

[183] Ayanna M Howard, Luke Roberts, Sergio Garcia, and Rakale Quarells. 2012.
Using mixed reality to map human exercise demonstrations to a robot exercise
coach. In 2012 IEEE International Symposium on Mixed and Augmented Reality
(ISMAR). IEEE, 291–292. https://doi.org/10.1109/ismar.2012.6402579

[184] Baichuan Huang, Deniz Bayazit, Daniel Ullman, Nakul Gopalan, and Stefanie
Tellex. 2019. Flight, camera, action! using natural language and mixed reality to
control a drone. In 2019 International Conference on Robotics and Automation
(ICRA). IEEE, 6949–6956. https://doi.org/10.1109/ICRA.2019.8794200

[185] Bidan Huang, Nicholas Gerard Timmons, and Qiang Li. 2020. Augmented
reality with multi-view merging for robot teleoperation. In Companion of the
2020 ACM/IEEE International Conference on Human-Robot Interaction. 260–262.
https://doi.org/10.1145/3371382.3378336

[186] Chien-Ming Huang and Bilge Mutlu. 2013. Modeling and Evaluating Narrative
Gestures for Humanlike Robots.. In Robotics: Science and Systems. 57–64. https:
//doi.org/10.15607/RSS.2013.IX.026

[187] Tianqi Huang, Ruiyang Li, Yangxi Li, Xinran Zhang, and Hongen Liao. 2021.
Augmented reality-based autostereoscopic surgical visualization system for
telesurgery. International Journal of Computer Assisted Radiology and Surgery
16, 11 (2021), 1985–1997. https://doi.org/10.1007/s11548-021-02463-5
[188] Dinh Quang Huy, I Vietcheslav, and Gerald Seet Gim Lee. 2017. See-through and
spatial augmented reality-a novel framework for human-robot interaction. In
2017 3rd International Conference on Control, Automation and Robotics (ICCAR).
IEEE, 719–726. https://doi.org/10.1109/ICCAR.2017.7942791

[189] Jane Hwang, Sangyup Lee, Sang Chul Ahn, and Hyoung-gon Kim. 2008. Aug-
mented robot agent: Enhancing co-presence of the remote participant. In 2008
7th IEEE/ACM International Symposium on Mixed and Augmented Reality. IEEE,
161–162. https://doi.org/10.1109/ismar.2008.4637346

[190] Hisham Iqbal, Fabio Tatti, and Ferdinando Rodriguez y Baena. 2021. Augmented
reality in robotic assisted orthopaedic surgery: A pilot study. Journal of Biomed-
ical Informatics 120 (2021), 103841. https://doi.org/10.1016/j.jbi.2021.103841

[191] Kentaro Ishii, Shengdong Zhao, Masahiko Inami, Takeo Igarashi, and Michita
Imai. 2009. Designing laser gesture interface for robot control. In IFIP Conference
on Human-Computer Interaction. Springer, 479–492. https://doi.org/10.1007/978-
3-642-03658-3_52

[192] Yvonne Jansen, Pierre Dragicevic, Petra Isenberg, Jason Alexander, Abhijit
Karnik, Johan Kildal, Sriram Subramanian, and Kasper Hornbæk. 2015. Op-
portunities and challenges for data physicalization. In Proceedings of the 33rd
Annual ACM Conference on Human Factors in Computing Systems. 3227–3236.
https://doi.org/10.1145/2702123.2702180

[193] Yunwoo Jeong, Han-Jong Kim, and Tek-Jin Nam. 2018. Mechanism perfboard:
An augmented reality environment for linkage mechanism design and fabrica-
tion. In Proceedings of the 2018 CHI Conference on Human Factors in Computing
Systems. 1–11. https://doi.org/10.1145/3173574.3173985

[194] Zhenrui Ji, Quan Liu, Wenjun Xu, Bitao Yao, Jiayi Liu, and Zude Zhou. 2021. A
Closed-Loop Brain-Computer Interface with Augmented Reality Feedback for
Industrial Human-Robot Collaboration. (2021). https://doi.org/10.21203/RS.3.
RS-283263/V1

[195] Chun Jia and Zhenzhong Liu. 2020. Collision Detection Based on Augmented
Reality for Construction Robot. In 2020 5th International Conference on Advanced
Robotics and Mechatronics (ICARM). IEEE, 194–197. https://doi.org/10.1109/
icarm49381.2020.9195301

[196] Jingang Jiang, Yafeng Guo, Zhiyuan Huang, Yongde Zhang, Dianhao Wu, and
Yi Liu. 2021. Adjacent surface trajectory planning of robot-assisted tooth prepa-
ration based on augmented reality. Engineering Science and Technology, an
International Journal (2021). https://doi.org/10.1016/J.JESTCH.2021.05.005
[197] Brennan Jones, Yaying Zhang, Priscilla NY Wong, and Sean Rintel. 2020.
VROOM: Virtual Robot Overlay for Online Meetings. In Extended Abstracts
of the 2020 CHI Conference on Human Factors in Computing Systems. 1–10.
https://doi.org/10.1145/3334480.3382820

[198] Brennan Jones, Yaying Zhang, Priscilla NY Wong, and Sean Rintel. 2021. Be-
longing There: VROOM-ing into the Uncanny Valley of XR Telepresence. Pro-
ceedings of the ACM on Human-Computer Interaction 5, CSCW1 (2021), 1–31.
https://doi.org/10.1145/3449133

[199] Colin Jones, Michael Novitzky, and Christopher Korpela. 2021. AR/VR Tutorial
System for Human-Robot Teaming. In 2021 IEEE 11th Annual Computing and
Communication Workshop and Conference (CCWC). IEEE, 0878–0882. https:
//doi.org/10.1109/ccwc51732.2021.9375845

[200] Jana Jost, Thomas Kirks, Preity Gupta, Dennis Lünsch, and Jonas Stenzel. 2018.
Safe human-robot-interaction in highly flexible warehouses using augmented
reality and heterogenous fleet management system. In 2018 IEEE International
Conference on Intelligence and Safety for Robotics (ISR). IEEE, 256–260. https:
//doi.org/10.1109/IISR.2018.8535808

[201] Kevin Sebastian Kain, Susanne Stadler, Manuel Giuliani, Nicole Mirnig, Gerald
Stollnberger, and Manfred Tscheligi. 2017. Tablet-based augmented reality in the
factory: Influence of knowledge in computer programming on robot teaching
tasks. In Proceedings of the Companion of the 2017 ACM/IEEE International Con-
ference on Human-Robot Interaction. 151–152. https://doi.org/10.1145/3029798.
3038347

[202] Alisa Kalegina, Grace Schroeder, Aidan Allchin, Keara Berlin, and Maya Cakmak.
2018. Characterizing the design space of rendered robot faces. In Proceedings of
the 2018 ACM/IEEE International Conference on Human-Robot Interaction. ACM,
96–104. https://doi.org/10.1145/3171221.3171286

[203] Megha Kalia, Apeksha Avinash, Nassir Navab, and Septimiu Salcudean. 2021.
Preclinical evaluation of a markerless, real-time, augmented reality guidance
system for robot-assisted radical prostatectomy. International Journal of Com-
puter Assisted Radiology and Surgery (2021), 1–8. https://doi.org/10.1007/s11548-
021-02419-9

[204] Megha Kalia, Prateek Mathur, Keith Tsang, Peter Black, Nassir Navab, and Sep-
timiu Salcudean. 2020. Evaluation of a marker-less, intra-operative, augmented
reality guidance system for robot-assisted laparoscopic radical prostatectomy.
International Journal of Computer Assisted Radiology and Surgery 15 (2020),
1225–1233. https://doi.org/10.1007/s11548-020-02181-4

[205] Kenji Kansaku, Naoki Hata, and Kouji Takano. 2010. My thoughts through
a robot’s eyes: An augmented reality-brain–machine interface. Neuroscience
research 66, 2 (2010), 219–222. https://doi.org/10.1016/j.neures.2009.10.006

[206] Michal Kapinus, Vítězslav Beran, Zdeněk Materna, and Daniel Bambušek. 2019.
Spatially Situated End-User Robot Programming in Augmented Reality. In 2019
28th IEEE International Conference on Robot and Human Interactive Communica-
tion (RO-MAN). IEEE, 1–8. https://doi.org/10.1109/RO-MAN46459.2019.8956336
[207] Michal Kapinus, Zdeněk Materna, Daniel Bambušek, and Vitězslav Beran. 2020.
End-User Robot Programming Case Study: Augmented Reality vs. Teach Pen-
dant. In Companion of the 2020 ACM/IEEE International Conference on Human-
Robot Interaction. 281–283. https://doi.org/10.1145/3371382.3378266

19

[208] Mohamed Kari, Tobias Grosse-Puppendahl, Luis Falconeri Coelho, Andreas Rene
Fender, David Bethge, Reinhard Schütte, and Christian Holz. 2021. TransforMR:
Pose-Aware Object Substitution for Composing Alternate Mixed Realities. In
2021 IEEE International Symposium on Mixed and Augmented Reality (ISMAR).
IEEE, 69–79. https://doi.org/10.1109/ismar52148.2021.00021

[209] Shunichi Kasahara, Ryuma Niiyama, Valentin Heun, and Hiroshi Ishii. 2013.
exTouch: spatially-aware embodied manipulation of actuated objects mediated
by augmented reality. In Proceedings of the 7th International Conference on
Tangible, Embedded and Embodied Interaction. 223–228. https://doi.org/10.1145/
2460625.2460661

[210] Misaki Kasetani, Tomonobu Noguchi, Hirotake Yamazoe, and Joo-Ho Lee. 2015.
Projection mapping by mobile projector robot. In 2015 12th International Con-
ference on Ubiquitous Robots and Ambient Intelligence (URAI). IEEE, 13–17.
https://doi.org/10.1109/URAI.2015.7358918

[211] Linh Kästner and Jens Lambrecht. 2019. Augmented-reality-based visualization
of navigation data of mobile robots on the microsoft hololens-possibilities and
limitations. In 2019 IEEE International Conference on Cybernetics and Intelligent
Systems (CIS) and IEEE Conference on Robotics, Automation and Mechatronics
(RAM). IEEE, 344–349. https://doi.org/10.1109/cis-ram47153.2019.9095836
[212] Jun Kato, Daisuke Sakamoto, Masahiko Inami, and Takeo Igarashi. 2009. Multi-
touch interface for controlling multiple mobile robots.
In CHI’09 Extended
Abstracts on Human Factors in Computing Systems. 3443–3448. https://doi.org/
10.1145/1520340.1520500

[213] Yuta Kato, Yuya Aikawa, Masayoshi Kanoh, Felix Jimenez, Mitsuhiro Hayase,
Takahiro Tanaka, and Hitoshi Kanamori. 2019. A Robot System Using Mixed
Reality to Encourage Driving Review. In International Conference on Human-
Computer Interaction. Springer, 112–117. https://doi.org/10.1007/978-3-030-
23528-4_16

[214] Rubaiat Habib Kazi, Tovi Grossman, Nobuyuki Umetani, and George Fitzmaurice.
2016. Motion amplifiers: sketching dynamic illustrations using the principles of
2D animation. In Proceedings of the 2016 CHI Conference on Human Factors in
Computing Systems. 4599–4609. https://doi.org/10.1145/2858036.2858386
[215] Maram Khatib, Khaled Al Khudir, and Alessandro De Luca. 2021. Human-robot
contactless collaboration with mixed reality interface. Robotics and Computer-
Integrated Manufacturing 67 (2021), 102030. https://doi.org/10.1016/j.rcim.2020.
102030

[216] Hyoungnyoun Kim, Jun-Sik Kim, Kwanghyun Ryu, Seyoung Cheon, Yonghwan
Oh, and Ji-Hyung Park. 2014. Task-oriented teleoperation through natural 3D
user interaction. In 2014 11th International Conference on Ubiquitous Robots and
Ambient Intelligence (URAI). IEEE, 335–338. https://doi.org/10.1109/urai.2014.
7057536

[217] Lawrence H Kim, Daniel S Drew, Veronika Domova, and Sean Follmer. 2020.
User-defined swarm robot control. In Proceedings of the 2020 CHI Conference on
Human Factors in Computing Systems. 1–13. https://doi.org/10.1145/3313831.
3376814

[218] Lawrence H Kim and Sean Follmer. 2017. Ubiswarm: Ubiquitous robotic inter-
faces and investigation of abstract motion as a display. Proceedings of the ACM
on Interactive, Mobile, Wearable and Ubiquitous Technologies 1, 3 (2017), 1–20.
https://doi.org/10.1145/3130931

[219] Won S Kim. 1996. Virtual reality calibration and preview/predictive displays for
telerobotics. Presence: Teleoperators & Virtual Environments 5, 2 (1996), 173–190.
[220] Kazuhiko Kobayashi, Koichi Nishiwaki, Shinji Uchiyama, Hiroyuki Yamamoto,
Satoshi Kagami, and Takeo Kanade. 2007. Overlay what humanoid robot per-
ceives and thinks to the real-world by mixed reality system. In 2007 6th IEEE and
ACM International Symposium on Mixed and Augmented Reality. IEEE, 275–276.
https://doi.org/ismar.2007.4538864

[221] Minoru Kojima, Maki Sugimoto, Akihiro Nakamura, Masahiro Tomita, Hideaki
Nii, and Masahiko Inami. 2006. Augmented coliseum: An augmented game
environment with small vehicles. In First IEEE International Workshop on
Horizontal Interactive Human-Computer Systems (TABLETOP’06). IEEE, 6–pp.
https://doi.org/10.1109/TABLETOP.2006.3

[222] Abhishek Kolagunda, Scott Sorensen, Sherif Mehralivand, Philip Saponaro,
Wayne Treible, Baris Turkbey, Peter Pinto, Peter Choyke, and Chandra Kamb-
hamettu. 2018. A mixed reality guidance system for robot assisted laparoscopic
radical prostatectomy. In OR 2.0 Context-Aware Operating Theaters, Computer
Assisted Robotic Endoscopy, Clinical Image-Based Procedures, and Skin Image
Analysis. Springer, 164–174. https://doi.org/10.1007/978-3-030-01201-4_18

[223] Andreas Korthauer, Clemens Guenther, Andreas Hinrichs, Wen Ren, and Yi-
wen Yang. 2020. Watch Your Vehicle Driving at the City: Interior HMI
with Augmented Reality for Automated Driving. In 22nd International Con-
ference on Human-Computer Interaction with Mobile Devices and Services. 1–5.
https://doi.org/10.1145/3406324.3425895

[224] Tomáš Kot, Petr Novák, and Ján Babjak. 2017. Application of augmented reality
in mobile robot teleoperation. In International Workshop on Modelling and Simu-
lation for Autonomous Systems. Springer, 223–236. https://doi.org/10.1007/978-
3-319-76072-8_16

[225] Niki Kousi, Christos Stoubos, Christos Gkournelos, George Michalos, and Sotiris
Makris. 2019. Enabling human robot interaction in flexible robotic assembly
lines: An augmented reality based software suite. Procedia CIRP 81 (2019),
1429–1434. https://doi.org/10.1016/J.PROCIR.2019.04.328

[226] Dennis Krupke, Frank Steinicke, Paul Lubos, Yannick Jonetzko, Michael Görner,
and Jianwei Zhang. 2018. Comparison of multimodal heading and pointing
gestures for co-located mixed reality human-robot interaction. In 2018 IEEE/RSJ
International Conference on Intelligent Robots and Systems (IROS). IEEE, 1–9.
https://doi.org/10.1109/iros.2018.8594043

[227] Aleksander Krzywinski, Haipeng Mi, Weiqin Chen, and Masanori Sugimoto.
2009. RoboTable: a tabletop framework for tangible interaction with robots
in a mixed reality. In proceedings of the international conference on advances in
computer Enterntainment technology. 107–114. https://doi.org/10.1145/1690388.
1690407

[228] Eranda Lakshantha and Simon Egerton. 2014. Human Robot Interaction and
Control: Translating Diagrams into an Intuitive Augmented Reality Approach.
In 2014 International Conference on Intelligent Environments. IEEE, 111–116.
https://doi.org/10.1109/ie.2014.24

[229] Fabrizio Lamberti, Davide Calandra, Federica Bazzano, Filippo G Prattico, and
Davide M Destefanis. 2018. Robotquest: A robotic game based on projected
mixed reality and proximity interaction. In 2018 IEEE Games, Entertainment,
Media Conference (GEM). IEEE, 1–9. https://doi.org/10.1109/GEM.2018.8516501
[230] Fabrizio Lamberti, Alberto Cannavò, and Paolo Pirone. 2019. Designing in-
teractive robotic games based on mixed reality technology. In 2019 IEEE In-
https:
ternational Conference on Consumer Electronics (ICCE). IEEE, 1–4.
//doi.org/10.1109/icce.2019.8661911

[231] Jens Lambrecht, Linh Kästner, Jan Guhl, and Jörg Krüger. 2021. Towards com-
missioning, resilience and added value of Augmented Reality in robotics: Over-
coming technical obstacles to industrial applicability. Robotics and Computer-
Integrated Manufacturing 71 (2021), 102178. https://doi.org/10.1016/J.RCIM.
2021.102178

[232] Jens Lambrecht and Jörg Krüger. 2012. Spatial programming for industrial
robots based on gestures and augmented reality. In 2012 IEEE/RSJ International
Conference on Intelligent Robots and Systems. IEEE, 466–472. https://doi.org/10.
1109/IROS.2012.6385900

[233] Matheus Laranjeira, Aurélien Arnaubec, Lorenzo Brignone, Claire Dune, and
Jan Opderbecke. 2020. 3D Perception and Augmented Reality Developments in
Underwater Robotics for Ocean Sciences. Current Robotics Reports (2020), 1–8.
https://doi.org/10.1007/s43154-020-00014-5

[234] Tomas Lazna. 2018. The visualization of threats using the augmented reality
IFAC-PapersOnLine 51, 6 (2018), 444–449.

and a remotely controlled robot.
https://doi.org/10.1016/J.IFACOL.2018.07.113

[235] Mathieu Le Goc, Lawrence H Kim, Ali Parsaei, Jean-Daniel Fekete, Pierre Drag-
icevic, and Sean Follmer. 2016. Zooids: Building blocks for swarm user interfaces.
In Proceedings of the 29th Annual Symposium on User Interface Software and
Technology. 97–109. https://doi.org/10.1145/2984511.2984547

[236] David Ledo, Steven Houben, Jo Vermeulen, Nicolai Marquardt, Lora Oehlberg,
and Saul Greenberg. 2018. Evaluation strategies for HCI toolkit research. In
Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems.
1–17. https://doi.org/10.1145/3173574.3173610

[237] Ho-Dong Lee, Dongwon Kim, Min-Chul Park, and Gwi-Tae Park. 2008. Aug-
mented reality based vision system for network based mobile robot. In Asia-
Pacific Conference on Computer Human Interaction. Springer, 123–130. https:
//doi.org/10.1007/978-3-540-70585-7_14

[238] Ho-Dong Lee, Hyun-Gu Lee, Joo-Hyung Kim, Min-Chul Park, and Gwi-Tae Park.
2007. Human machine interface with augmented reality for the network based
mobile robot. In International Conference on Knowledge-Based and Intelligent
Information and Engineering Systems. Springer, 57–64. https://doi.org/10.1007/
978-3-540-74829-8_8

[239] Joo-Haeng Lee, Junho Kim, and Hyun Kim. 2011. A note on hybrid control
of robotic spatial augmented reality. In 2011 8th International Conference on
Ubiquitous Robots and Ambient Intelligence (URAI). IEEE, 621–626. https://doi.
org/10.1109/URAI.2011.6145895

[240] Jae Young Lee, Jong-Wook Lee, Teressa Talluri, Amarnathvarma Angani, and
Jeong Bea Lee. 2020. Realization of Robot Fish with 3D Hologram Fish using
Augmented Reality. In 2020 IEEE 2nd International Conference on Architecture,
Construction, Environment and Hydraulics (ICACEH). IEEE, 102–104. https:
//doi.org/10.1109/icaceh51803.2020.9366226

[241] Kevin Lee, Christopher Reardon, and Jonathan Fink. 2018. Augmented Reality
in Human-Robot Cooperative Search. In 2018 IEEE International Symposium on
Safety, Security, and Rescue Robotics (SSRR). IEEE, 1–1. https://doi.org/10.1109/
ssrr.2018.8468659

[242] Myungho Lee, Nahal Norouzi, Gerd Bruder, Pamela J Wisniewski, and Gre-
gory F Welch. 2018. The physical-virtual table: exploring the effects of a
virtual human’s physical influence on social interaction. In Proceedings of
the 24th ACM Symposium on Virtual Reality Software and Technology. 1–11.
https://doi.org/10.1145/3281505.3281533

[243] Daniel Leithinger, Sean Follmer, Alex Olwal, Samuel Luescher, Akimitsu Hogge,
Jinha Lee, and Hiroshi Ishii. 2013. Sublimate: state-changing virtual and physical
rendering to augment interaction with shape displays. In Proceedings of the
SIGCHI conference on human factors in computing systems. 1441–1450. https:
//doi.org/10.1145/2470654.2466191

[244] Daniel Leithinger and Hiroshi Ishii. 2010. Relief: a scalable actuated shape dis-
play. In Proceedings of the fourth international conference on Tangible, embedded,
and embodied interaction. 221–222. https://doi.org/10.1145/1709886.1709928

[245] Daniel Leithinger, David Lakatos, Anthony DeVincenzi, Matthew Blackshaw,
and Hiroshi Ishii. 2011. Direct and gestural interaction with relief: a 2.5 D shape
display. In Proceedings of the 24th annual ACM symposium on User interface
software and technology. 541–548. https://doi.org/10.1145/2047196.2047268

[246] Jakob Leitner, Michael Haller, Kyungdahm Yun, Woontack Woo, Maki Sugimoto,
Masahiko Inami, Adrian David Cheok, and HD Been-Lirn. 2010. Physical
interfaces for tabletop games. Computers in Entertainment (CIE) 7, 4 (2010), 1–21.
https://doi.org/10.1145/1658866.1658880

[247] Germán Leiva, Cuong Nguyen, Rubaiat Habib Kazi, and Paul Asente. 2020.
Pronto: Rapid augmented reality video prototyping using sketches and enaction.
In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems.
1–13. https://doi.org/10.1145/3313831.3376160

[248] Alexander Lenhardt and Helge Ritter. 2010. An augmented-reality based brain-
computer interface for robot control. In International Conference on Neural
Information Processing. Springer, 58–65. https://doi.org/10.1007/978-3-642-
17534-3_8

[249] Francisco J Lera, Víctor Rodríguez, Carlos Rodríguez, and Vicente Matellán.
2014. Augmented reality in robotic assistance for the elderly. In International
technology robotics applications. Springer, 3–11. https://doi.org/10.1007/978-3-
319-02332-8_1

[250] Mirna Lerotic, Adrian J Chung, George Mylonas, and Guang-Zhong Yang. 2007.
Pq-space based non-photorealistic rendering for augmented reality. In Interna-
tional Conference on Medical Image Computing and Computer-Assisted Interven-
tion. Springer, 102–109. https://doi.org/10.1007/978-3-540-75759-7_13
[251] Florian Leutert, Christian Herrmann, and Klaus Schilling. 2013. A spatial
augmented reality system for intuitive display of robotic data. In 2013 8th
ACM/IEEE International Conference on Human-Robot Interaction (HRI). IEEE,
179–180. https://doi.org/10.1109/hri.2013.6483560

[252] Florian Leutert and Klaus Schilling. 2012. Support of power plant telemain-
tenance with robots by augmented reality methods. In 2012 2nd International
Conference on Applied Robotics for the Power Industry (CARPI). IEEE, 45–49.
https://doi.org/10.1109/carpi.2012.6473362

[253] Florian Leutert and Klaus Schilling. 2015. Augmented reality for telemainte-
nance and-inspection in force-sensitive industrial robot applications.
IFAC-
PapersOnLine 48, 10 (2015), 153–158. https://doi.org/10.1016/J.IFACOL.2015.08.
124

[254] Chunxu Li, Ashraf Fahmy, and Johann Sienz. 2019. An augmented reality based
human-robot interaction interface using Kalman filter sensor fusion. Sensors 19,
20 (2019), 4586. https://doi.org/10.3390/s19204586

[255] Congyuan Liang, Chao Liu, Xiaofeng Liu, Long Cheng, and Chenguang Yang.
2019. Robot teleoperation system based on mixed reality. In 2019 IEEE 4Th
international conference on advanced robotics and mechatronics (ICARM). IEEE,
384–389. https://doi.org/10.1109/icarm.2019.8834302

[256] Li Lin, Yunyong Shi, Andy Tan, Melia Bogari, Ming Zhu, Yu Xin, Haisong Xu,
Yan Zhang, Le Xie, and Gang Chai. 2016. Mandibular angle split osteotomy
based on a novel augmented reality navigation using specialized robot-assisted
arms—A feasibility study. Journal of Cranio-Maxillofacial Surgery 44, 2 (2016),
215–223. https://doi.org/10.1016/j.jcms.2015.10.024

[257] Natan Linder and Pattie Maes. 2010. LuminAR: portable robotic augmented
reality interface design and prototype. In Adjunct proceedings of the 23nd annual
ACM symposium on User interface software and technology. 395–396. https:
//doi.org/10.1145/1866218.1866237

[258] David Lindlbauer, Jens Emil Grønbæk, Morten Birk, Kim Halskov, Marc Alexa,
and Jörg Müller. 2016. Combining shape-changing interfaces and spatial aug-
mented reality enables extended object appearance. In Proceedings of the 2016
CHI Conference on Human Factors in Computing Systems. 791–802.
https:
//doi.org/10.1145/2858036.2858457

[259] David Lindlbauer, Jörg Mueller, and Marc Alexa. 2017. Changing the appearance
of real-world objects by modifying their surroundings. In Proceedings of the
2017 CHI Conference on Human Factors in Computing Systems. 3954–3965. https:
//doi.org/10.1145/3025453.3025795

[260] David Lindlbauer and Andy D Wilson. 2018. Remixed reality: Manipulating
space and time in augmented reality. In Proceedings of the 2018 CHI Conference
on Human Factors in Computing Systems. 1–13. https://doi.org/10.1145/3173574.
3173703

[261] Ragavendra Lingamaneni, Thomas Kubitza, and Jürgen Scheible. 2017.
DroneCAST: towards a programming toolkit for airborne multimedia display
applications. In Proceedings of the 19th International Conference on Human-
Computer Interaction with Mobile Devices and Services. 1–8. https://doi.org/10.
1145/3098279.3122128

20

[262] Hangxin Liu, Yaofang Zhang, Wenwen Si, Xu Xie, Yixin Zhu, and Song-Chun
Zhu. 2018. Interactive robot knowledge patching using augmented reality. In
2018 IEEE International Conference on Robotics and Automation (ICRA). IEEE,
1947–1954. https://doi.org/10.1109/ICRA.2018.8462837

[263] Kexi Liu, Daisuke Sakamoto, Masahiko Inami, and Takeo Igarashi. 2011. Ro-
boshop: multi-layered sketching interface for robot housework assignment and
management. In Proceedings of the SIGCHI Conference on Human Factors in
Computing Systems. 647–656. https://doi.org/10.1145/1978942.1979035
[264] Wen P Liu, Jeremy D Richmon, Jonathan M Sorger, Mahdi Azizian, and Russell H
Taylor. 2015. Augmented reality and cone beam CT guidance for transoral
robotic surgery. Journal of robotic surgery 9, 3 (2015), 223–233. https://doi.org/
10.1007/s11701-015-0520-5

[265] Yuzhou Liu, Georg Novotny, Nikita Smirnov, Walter Morales-Alvarez, and
Cristina Olaverri-Monreal. 2020. Mobile Delivery Robots: Mixed Reality-Based
Simulation Relying on ROS and Unity 3D. In 2020 IEEE Intelligent Vehicles Sym-
posium (IV). IEEE, 15–20. https://doi.org/10.1109/IV47402.2020.9304701
[266] Salvatore Livatino, Filippo Banno, and Giovanni Muscato. 2011. 3-D integration
of robot vision and laser data with semiautomatic calibration in augmented
reality stereoscopic visual interface. IEEE Transactions on Industrial Informatics
8, 1 (2011), 69–77. https://doi.org/10.1109/tii.2011.2174062

[267] Salvatore Livatino, Dario C Guastella, Giovanni Muscato, Vincenzo Rinaldi,
Luciano Cantelli, Carmelo D Melita, Alessandro Caniglia, Riccardo Mazza, and
Gianluca Padula. 2021.
Intuitive robot teleoperation through multi-sensor
informed mixed reality visual aids. IEEE Access 9 (2021), 25795–25808. https:
//doi.org/10.1109/access.2021.3057808

[268] Salvatore Livatino, Giovanni Muscato, Filippo Banno, Davide De Tommaso, and
Marco Macaluso. 2010. Video and laser based augmented reality stereoscopic
viewing for mobile robot teleoperation. IFAC Proceedings Volumes 43, 23 (2010),
161–168. https://doi.org/10.3182/20101005-4-RO-2018.00049

[269] Salvatore Livatino, Giovanni Muscato, Davide De Tommaso, and Marco
Macaluso. 2010. Augmented reality stereoscopic visualization for intuitive
robot teleguide. In 2010 IEEE International Symposium on Industrial Electronics.
IEEE, 2828–2833. https://doi.org/10.1109/ISIE.2010.5636955

[270] Matthew B Luebbers, Connor Brooks, Minjae John Kim, Daniel Szafir, and
Bradley Hayes. 2019. Augmented reality interface for constrained learning
from demonstration. In Proceedings of the 2nd International Workshop on Virtual,
Augmented, and Mixed Reality for HRI (VAM-HRI).

[271] Dario Luipers and Anja Richert. 2021. Concept of an Intuitive Human-Robot-
Collaboration via Motion Tracking and Augmented Reality. In 2021 IEEE Interna-
tional Conference on Artificial Intelligence and Computer Applications (ICAICA).
IEEE, 423–427. https://doi.org/10.1109/icaica52286.2021.9498091

[272] Maria Luce Lupetti. 2016. Designing playful HRI: Acceptability of robots in
everyday life through play. In 2016 11th ACM/IEEE International Conference on
Human-Robot Interaction (HRI). IEEE, 631–632. https://doi.org/10.1109/hri.2016.
7451891

[273] Maria Luce Lupetti, Giovanni Piumatti, Claudio Germak, and Fabrizio Lamberti.
2018. Design and Evaluation of a Mixed-Reality Playground for Child-Robot
Games. Multimodal Technologies and Interaction 2, 4 (2018), 69. https://doi.org/
10.3390/mti2040069

[274] Andreas Luxenburger, Jonas Mohr, Torsten Spieldenner, Dieter Merkel, Fabio
Espinosa, Tim Schwartz, Florian Reinicke, Julian Ahlers, and Markus Stoyke.
2019. Augmented reality for human-robot cooperation in aircraft assembly. In
2019 IEEE International Conference on Artificial Intelligence and Virtual Reality
(AIVR). IEEE, 263–2633. https://doi.org/10.1109/AIVR46125.2019.00061
[275] Stéphane Magnenat, Morderchai Ben-Ari, Severin Klinger, and Robert W Sumner.
2015. Enhancing robot programming with visual feedback and augmented
reality. In Proceedings of the 2015 ACM conference on innovation and technology
in computer science education. 153–158. https://doi.org/10.1145/2729094.2742585
[276] Karthik Mahadevan, Elaheh Sanoubari, Sowmya Somanath, James E Young,
and Ehud Sharlin. 2019. AV-Pedestrian interaction design using a pedestrian
mixed traffic simulator. In Proceedings of the 2019 on designing interactive systems
conference. 475–486. https://doi.org/10.1145/3322276.3322328

[277] Karthik Mahadevan, Maurício Sousa, Anthony Tang, and Tovi Grossman. 2021.
“Grip-that-there”: An Investigation of Explicit and Implicit Task Allocation
Techniques for Human-Robot Collaboration. In Proceedings of the 2021 CHI
Conference on Human Factors in Computing Systems. 1–14. https://doi.org/10.
1145/3411764.3445355

[278] Kartik Mahajan, Thomas Groechel, Roxanna Pakkar, Julia Cordero, Haemin Lee,
and Maja J Matarić. 2020. Adapting Usability Metrics for a Socially Assistive,
Kinesthetic, Mixed Reality Robot Tutoring Environment. In International Con-
ference on Social Robotics. Springer, 381–391. https://doi.org/10.1007/978-3-030-
62056-1_32

[279] Madjid Maidi, Malik Mallem, Laredj Benchikh, and Samir Otmane. 2013. An
evaluation of camera pose methods for an augmented reality system: Application
to teaching industrial robots. In Transactions on Computational Science XVII.
Springer, 3–30. https://doi.org/10.1007/978-3-642-35840-1_1

[280] Jim Mainprice, Emrah Akin Sisbot, Thierry Siméon, and Rachid Alami. 2010.
Planning safe and legible hand-over motions for human-robot interaction. In

IARP/IEEE-RAS/EURON workshop on technical challenges for dependable robots
in human environments.

[281] Zhanat Makhataeva and Huseyin Atakan Varol. 2020. Augmented reality for ro-
botics: a review. Robotics 9, 2 (2020), 21. https://doi.org/10.3390/robotics9020021
[282] Zhanat Makhataeva, Altay Zhakatayev, and Huseyin Atakan Varol. 2019. Safety
Aura Visualization for Variable Impedance Actuated Robots. In 2019 IEEE/SICE
International Symposium on System Integration (SII). IEEE, 805–810. https:
//doi.org/10.1109/SII.2019.8700332

[283] Sotiris Makris, Panagiotis Karagiannis, Spyridon Koukas, and Aleksandros-
Stereos Matthaiakis. 2016. Augmented reality system for operator support in
human–robot collaborative assembly. CIRP Annals 65, 1 (2016), 61–64. https:
//doi.org/10.1016/J.CIRP.2016.04.038

[284] Ehsan Malayjerdi, Mahdi Yaghoobi, and Mohammad Kardan. 2017. Mobile robot
navigation based on fuzzy cognitive map optimized with grey wolf optimization
algorithm used in augmented reality. In 2017 5th RSI International Conference
on Robotics and Mechatronics (ICRoM). IEEE, 211–218. https://doi.org/10.1109/
icrom.2017.8466169

[285] Ivo Mal`y, David Sedláček, and Paulo Leitao. 2016. Augmented reality ex-
periments with industrial robot in industry 4.0 environment. In 2016 IEEE
14th international conference on industrial informatics (INDIN). IEEE, 176–181.
https://doi.org/10.1109/INDIN.2016.7819154

[286] Raúl Marín and Pedro J Sanz. 2002. Augmented reality to teleoperate a robot
https:

IFAC Proceedings Volumes 35, 1 (2002), 161–165.

through the Web.
//doi.org/10.3182/20020721-6-ES-1901.00933

[287] Andrés Martín-Barrio, Juan Jesús Roldán-Gómez, Iván Rodríguez, Jaime
Del Cerro, and Antonio Barrientos. 2020. Design of a Hyper-Redundant Robot
and Teleoperation Using Mixed Reality for Inspection Tasks. Sensors 20, 8 (2020),
2181. https://doi.org/10.3390/s20082181

[288] Zdeněk Materna, Michal Kapinus, Vítězslav Beran, Pavel SmrĚ, Manuel Giuliani,
Nicole Mirnig, Susanne Stadler, Gerald Stollnberger, and Manfred Tscheligi. 2017.
Using persona, scenario, and use case to develop a human-robot augmented
reality collaborative workspace. In Proceedings of the Companion of the 2017
ACM/IEEE International Conference on Human-Robot Interaction. 201–202. https:
//doi.org/10.1145/3029798.3038366

[289] Zdeněk Materna, Michal Kapinus, Vítězslav Beran, Pavel Smrž, and Pavel Zemčík.
2018. Interactive spatial augmented reality in collaborative robot programming:
User experience evaluation. In 2018 27th IEEE International Symposium on Robot
and Human Interactive Communication (RO-MAN). IEEE, 80–87. https://doi.org/
10.1109/roman.2018.8525662

[290] Florin Octavian Matu, Mikkel Thøgersen, Bo Galsgaard, Martin Møller Jensen,
and Martin Kraus. 2014. Stereoscopic augmented reality system for supervised
training on minimal invasive surgery robots. In Proceedings of the 2014 Virtual
Reality International Conference. 1–4. https://doi.org/10.1145/2617841.2620722
[291] William A McNeely. 1993. Robotic graphics: a new approach to force feedback
for virtual reality. In Proceedings of IEEE Virtual Reality Annual International
Symposium. IEEE, 336–341. https://doi.org/10.1109/VRAIS.1993.380761
[292] George Michalos, Panagiotis Karagiannis, Sotiris Makris, Önder Tokçalar, and
George Chryssolouris. 2016. Augmented reality (AR) applications for supporting
human-robot interactive cooperation. Procedia CIRP 41 (2016), 370–375. https:
//doi.org/10.1016/J.PROCIR.2015.12.005

[293] Paul Milgram and Fumio Kishino. 1994. A taxonomy of mixed reality visual
displays. IEICE TRANSACTIONS on Information and Systems 77, 12 (1994), 1321–
1329.

[294] Paul Milgram, Shumin Zhai, David Drascic, and Julius Grodski. 1993. Appli-
cations of augmented reality for human-robot communication. In Proceedings
of 1993 IEEE/RSJ International Conference on Intelligent Robots and Systems
(IROS’93), Vol. 3. IEEE, 1467–1472. https://doi.org/10.1109/IROS.1993.583833

[295] Omid Mohareri and Ahmad B Rad. 2011. Autonomous humanoid robot naviga-
tion using augmented reality technique. In 2011 IEEE International Conference
on Mechatronics. IEEE, 463–468. https://doi.org/10.1109/icmech.2011.5971330
[296] Nicolas Mollet, Ryad Chellali, and Luca Brayda. 2009. Virtual and augmented
reality tools for teleoperation: improving distant immersion and perception. In
Transactions on edutainment II. Springer, 135–159. https://doi.org/10.1007/978-
3-642-03270-7_10

[297] William Montalvo, Pablo Bonilla-Vasconez, Santiago Altamirano, Carlos A
Garcia, and Marcelo V Garcia. 2020. Industrial Control Robot Based on Aug-
mented Reality and IoT Protocol. In International Conference on Augmented
Reality, Virtual Reality and Computer Graphics. Springer, 345–363.
https:
//doi.org/10.1007/978-3-030-58468-9_25

[298] Rafael Morales, Asier Marzo, Sriram Subramanian, and Diego Martínez. 2019.
LeviProps: Animating levitated optimized fabric structures using holographic
acoustic tweezers. In Proceedings of the 32nd Annual ACM Symposium on User
Interface Software and Technology. 651–661. https://doi.org/10.1145/3332165.
3347882

[299] Stephen A Morin, Robert F Shepherd, Sen Wai Kwok, Adam A Stokes, Alex
Nemiroski, and George M Whitesides. 2012. Camouflage and display for soft
machines. Science 337, 6096 (2012), 828–832. https://doi.org/10.1126/science.
1222149

21

[300] Kohei Morita, Takefumi Hiraki, Haruka Matsukura, Daisuke Iwai, and Kosuke
Sato. 2020. Extension of Projection Area using Head Orientation in Projected
Virtual Hand Interface for Wheelchair Users. In 2020 59th Annual Conference of
the Society of Instrument and Control Engineers of Japan (SICE). IEEE, 421–426.
https://doi.org/10.23919/SICE48898.2020.9240271

[301] D Mourtzis, G Synodinos, J Angelopoulos, and N Panopoulos. 2020. An aug-
mented reality application for robotic cell customization. Procedia CIRP 90
(2020), 654–659. https://doi.org/10.1016/j.procir.2020.02.135

[302] Dimitris Mourtzis, Vasilios Zogopoulos, and E Vlachou. 2017. Augmented reality
application to support remote maintenance as a service in the robotics industry.
Procedia Cirp 63 (2017), 46–51. https://doi.org/10.1016/j.procir.2017.03.154
[303] Fabian Mueller, Christian Deuerlein, and Michael Koch. 2019. Intuitive weld-
IFAC-
ing robot programming via motion capture and augmented reality.
PapersOnLine 52, 10 (2019), 294–299. https://doi.org/10.1016/j.ifacol.2019.10.045
[304] Stefanie Mueller, Pedro Lopes, and Patrick Baudisch. 2012. Interactive construc-
tion: interactive fabrication of functional mechanical devices. In Proceedings
of the 25th annual ACM symposium on User interface software and technology.
599–606. https://doi.org/10.1145/2380116.2380191

[305] Faizan Muhammad, Amel Hassan, Andre Cleaver, and Jivko Sinapov. 2019.
Creating a shared reality with robots. In 2019 14th ACM/IEEE International
Conference on Human-Robot Interaction (HRI). IEEE, 614–615. https://doi.org/
10.1109/HRI.2019.8673191

[306] Alex Murphy and Alan G Millard. 2020. Prototyping Sensors and Actuators
for Robot Swarms in Mixed Reality. In Annual Conference Towards Autonomous
Robotic Systems. Springer, 377–386. https://doi.org/10.1007/978-3-030-63486-
5_39

[307] B MUTLU. [n.d.]. Modeling and Evaluation of Human-like Gaze Behavior.

Humanoids’ 06 ([n. d.]). https://doi.org/10.1109/ICHR.2006.321322

[308] Ken Nakagaki, Luke Vink, Jared Counts, Daniel Windham, Daniel Leithinger,
Sean Follmer, and Hiroshi Ishii. 2016. Materiable: Rendering dynamic material
properties in response to direct physical touch with shape changing interfaces. In
Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems.
2764–2772. https://doi.org/10.1145/2858036.2858104

[309] Nassir Navab, Christoph Hennersperger, Benjamin Frisch, and Bernhard Fürst.
2016. Personalized, relevance-based multimodal robotic imaging and augmented
reality for computer assisted interventions. , 64–71 pages. https://doi.org/10.
1016/j.media.2016.06.021

[310] Aditya Nawab, Keshav Chintamani, Darin Ellis, Gregory Auner, and Abhi-
lash Pandya. 2007. Joystick mapped augmented reality cues for end-effector
controlled tele-operated robots. In 2007 IEEE Virtual Reality Conference. IEEE,
263–266. https://doi.org/10.1109/vr.2007.352496

[311] Michael Nebeling, Janet Nebeling, Ao Yu, and Rob Rumble. 2018. Protoar:
Rapid physical-digital prototyping of mobile augmented reality applications. In
Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems.
1–12. https://doi.org/10.1145/3173574.3173927

[312] Chuen Leong Ng, Teck Chew Ng, Thi Anh Ngoc Nguyen, Guilin Yang, and
Wenjie Chen. 2010. Intuitive robot tool path teaching using laser and camera in
augmented reality environment. In 2010 11th International Conference on Control
Automation Robotics & Vision. IEEE, 114–119. https://doi.org/10.1109/icarcv.
2010.5707399

[313] D Ni, AWW Yew, SK Ong, and AYC Nee. 2017. Haptic and visual augmented
reality interface for programming welding robots. Advances in Manufacturing
5, 3 (2017), 191–198. https://doi.org/10.1007/s40436-017-0184-7

[314] Sivapong Nilwong and Genci Capi. 2020. Outdoor Robot Navigation System
using Game-Based DQN and Augmented Reality. In 2020 17th International
Conference on Ubiquitous Robots (UR). IEEE, 74–80. https://doi.org/10.1109/
ur49135.2020.9144838

[315] Koichi Nishiwaki, Kazuhiko Kobayashi, Shinji Uchiyama, Hiroyuki Yamamoto,
and Satoshi Kagami. 2008. Mixed reality environment for autonomous robot
development. In 2008 IEEE International Conference on Robotics and Automation.
IEEE, 2211–2212. https://doi.org/10.1109/ROBOT.2008.4543538

[316] Diana Nowacka, Karim Ladha, Nils Y Hammerla, Daniel Jackson, Cassim Ladha,
Enrico Rukzio, and Patrick Olivier. 2013. Touchbugs: Actuated tangibles on
multi-touch tables. In Proceedings of the SIGCHI conference on human factors in
computing systems. 759–762. https://doi.org/10.1145/2470654.2470761
[317] Hiroki Nozaki. 2014. Flying display: a movable display pairing projector and
screen in the air. In CHI’14 Extended Abstracts on Human Factors in Computing
Systems. 909–914. https://doi.org/10.1145/2559206.2579410

[318] R Nunez, JR Bandera, JM Perez-Lorenzo, and Francisco Sandoval. 2006. A human-
robot interaction system for navigation supervision based on augmented reality.
In MELECON 2006-2006 IEEE Mediterranean Electrotechnical Conference. IEEE,
441–444. https://doi.org/10.1109/melcon.2006.1653133

[319] Cristina Nuzzi, Stefano Ghidini, Roberto Pagani, Simone Pasinetti, Gabriele
Coffetti, and Giovanna Sansoni. 2020. Hands-Free: a robot augmented reality
teleoperation system. In 2020 17th International Conference on Ubiquitous Robots
(UR). IEEE, 617–624. https://doi.org/10.1109/ur49135.2020.9144841

[320] Yoichi Ochiai and Keisuke Toyoshima. 2011. Homunculus: the vehicle as aug-
mented clothes. In Proceedings of the 2nd Augmented Human International Con-
ference. 1–4. https://doi.org/10.1145/1959826.1959829

22

[321] Yusuke Okuno, Takayuki Kanda, Michita Imai, Hiroshi Ishiguro, and Norihiro
Hagita. 2009. Providing route directions: design of robot’s utterance, gesture,
and timing. In 2009 4th ACM/IEEE International Conference on Human-Robot
Interaction (HRI). IEEE, 53–60. https://doi.org/10.1145/1514095.1514108
[322] Shayegan Omidshafiei, Ali-Akbar Agha-Mohammadi, Yu Fan Chen, Nazim Ke-
mal Ure, Shih-Yuan Liu, Brett T Lopez, Rajeev Surati, Jonathan P How, and
John Vian. 2016. Measurable augmented reality for prototyping cyberphysi-
cal systems: A robotics platform to aid the hardware prototyping and perfor-
mance testing of algorithms. IEEE Control Systems Magazine 36, 6 (2016), 65–87.
https://doi.org/10.1109/mcs.2016.2602090

[323] SK Ong, AWW Yew, NK Thanigaivel, and AYC Nee. 2020. Augmented reality-
assisted robot programming system for industrial applications. Robotics and
Computer-Integrated Manufacturing 61 (2020), 101820. https://doi.org/10.1016/J.
RCIM.2019.101820

[324] Soh-Khim Ong, JWS Chong, and Andrew YC Nee. 2006. Methodologies for
immersive robot programming in an augmented reality environment. In Pro-
ceedings of the 4th international conference on computer graphics and interactive
techniques in Australasia and Southeast Asia. 237–244. https://doi.org/10.1145/
1174429.1174470

[325] Mikhail Ostanin and Alexandr Klimchik. 2018. Interactive robot programing
using mixed reality. IFAC-PapersOnLine 51, 22 (2018), 50–55. https://doi.org/10.
1016/j.ifacol.2018.11.517

[326] Mikhail Ostanin, Stanislav Mikhel, Alexey Evlampiev, Valeria Skvortsova, and
Alexandr Klimchik. 2020. Human-robot interaction for robotic manipulator
programming in Mixed Reality. In 2020 IEEE International Conference on Robotics
and Automation (ICRA). IEEE, 2805–2811. https://doi.org/10.1109/ICRA40945.
2020.9196965

[327] M Ostanin, R Yagfarov, and A Klimchik. 2019. Interactive Robots Control Using
Mixed Reality. IFAC-PapersOnLine 52, 13 (2019), 695–700. https://doi.org/10.
1016/j.ifacol.2019.11.307

[328] Ayberk Özgür, Séverin Lemaignan, Wafa Johal, Maria Beltran, Manon Briod, Léa
Pereyre, Francesco Mondada, and Pierre Dillenbourg. 2017. Cellulo: Versatile
handheld robots for education. In 2017 12th ACM/IEEE International Confer-
ence on Human-Robot Interaction (HRI. IEEE, 119–127. https://doi.org/10.1145/
2909824.3020247

[329] Yun Suen Pai, Hwa Jen Yap, Siti Zawiah Md Dawal, S Ramesh, and Sin Ye Phoon.
2016. Virtual planning, control, and machining for a modular-based automated
factory operation in an augmented reality environment. Scientific reports 6, 1
(2016), 1–19. https://doi.org/10.1038/srep27380

[330] Yong Pan, Chengjun Chen, Dongnian Li, Zhengxu Zhao, and Jun Hong. 2021.
Augmented reality-based robot teleoperation system using RGB-D imaging and
attitude teaching device. Robotics and Computer-Integrated Manufacturing 71
(2021), 102167. https://doi.org/10.1016/J.RCIM.2021.102167

[331] Gian Pangaro, Dan Maynes-Aminzade, and Hiroshi Ishii. 2002. The actuated
workbench: computer-controlled actuation in tabletop tangible interfaces. In
Proceedings of the 15th annual ACM symposium on User interface software and
technology. 181–190. https://doi.org/10.1145/571985.572011

[332] Christos Papachristos and Kostas Alexis. 2016. Augmented reality-enhanced
structural inspection using aerial robots. In 2016 IEEE international symposium
on intelligent control (ISIC). IEEE, 1–6. https://doi.org/10.1109/ISIC.2016.7579983
[333] Peter Papcun, Jan Cabadaj, Erik Kajati, David Romero, Lenka Landryova, Jan
Vascak, and Iveta Zolotova. 2019. Augmented Reality for Humans-Robots
Interaction in Dynamic Slotting “Chaotic Storage” Smart Warehouses. In IFIP In-
ternational Conference on Advances in Production Management Systems. Springer,
633–641. https://doi.org/10.1007/978-3-030-30000-5_77

[334] Hyeshin Park, Yo-An Lim, Aslam Pervez, Beom-Chan Lee, Sang-Goog Lee, and
Jeha Ryu. 2007. Teleoperation of a multi-purpose robot over the internet using
augmented reality. In 2007 International Conference on Control, Automation and
Systems. IEEE, 2456–2461. https://doi.org/10.1109/iccas.2007.4406776
[335] Jung Pil Park, Min Woo Park, and Soon Ki Jung. 2014. Qr-code based online robot
augmented reality system for education. In Proceedings of the 29th Annual ACM
Symposium on Applied Computing. 180–185. https://doi.org/10.1145/2554850.
2555038

[336] Kyeong-Beom Park, Sung Ho Choi, Jae Yeol Lee, Yalda Ghasemi, Mustafa Mo-
hammed, and Heejin Jeong. 2021. Hands-Free Human–Robot Interaction Using
Multimodal Gestures and Deep Learning in Wearable Mixed Reality. IEEE Access
9 (2021), 55448–55464. https://doi.org/10.1109/access.2021.3071364

[337] Yoon Jung Park, Hyocheol Ro, and Tack-Don Han. 2019. Deep-ChildAR bot:
educational activities and safety care augmented reality system with deep-
learning for preschool. In ACM SIGGRAPH 2019 Posters. 1–2. https://doi.org/10.
1145/3306214.3338589

[338] Yoon Jung Park, Yoonsik Yang, Hyocheol Ro, JungHyun Byun, Seougho Chae,
and Tack Don Han. 2018. Meet AR-bot: Meeting Anywhere, Anytime with Mov-
able Spatial AR Robot. In Proceedings of the 26th ACM international conference
on Multimedia. 1242–1243. https://doi.org/10.1145/3240508.3241390

[339] James Patten and Hiroshi Ishii. 2007. Mechanical constraints as computational
constraints in tabletop tangible interfaces. In Proceedings of the SIGCHI conference

on Human factors in computing systems. 809–818.
1240624.1240746

https://doi.org/10.1145/

[340] Esben Warming Pedersen and Kasper Hornbæk. 2011. Tangible bots: in-
teraction with active tangibles in tabletop interfaces. In Proceedings of the
SIGCHI Conference on Human Factors in Computing Systems. 2975–2984. https:
//doi.org/10.1145/1978942.1979384

[341] Huaishu Peng, Jimmy Briggs, Cheng-Yao Wang, Kevin Guo, Joseph Kider, Ste-
fanie Mueller, Patrick Baudisch, and François Guimbretière. 2018. RoMA: In-
teractive fabrication with augmented reality and a robotic 3D printer. In Pro-
ceedings of the 2018 CHI conference on human factors in computing systems. 1–12.
https://doi.org/10.1145/3173574.3174153

[342] Lorenzo Peppoloni, Filippo Brizzi, Emanuele Ruffaldi, and Carlo Alberto Aviz-
zano. 2015. Augmented reality-aided tele-presence system for robot manipu-
lation in industrial manufacturing. In Proceedings of the 21st ACM Symposium
on Virtual Reality Software and Technology. 237–240. https://doi.org/10.1145/
2821592.2821620

[343] Nate Phillips, Brady Kruse, Farzana Alam Khan, J Edward Swan II, and Cindy L
Bethel. 2020. A Robotic Augmented Reality Virtual Window for Law Enforce-
ment Operations. In International Conference on Human-Computer Interaction.
Springer, 591–610. https://doi.org/10.1007/978-3-030-49695-1_40

[344] Luis Piardi, Vivian Cremer Kalempa, Marcelo Limeira, André Schneider de
Oliveira, and Paulo Leitão. 2019. ARENA—augmented reality to enhanced
experimentation in smart warehouses. Sensors 19, 19 (2019), 4308.
https:
//doi.org/10.3390/s19194308

[345] Carlo Pinciroli, Mohamed S Talamali, Andreagiovanni Reina, James AR Marshall,
and Vito Trianni. 2018. Simulating kilobots within argos: models and experi-
mental validation. In International Conference on Swarm Intelligence. Springer,
176–187. https://doi.org/10.1007/978-3-030-00533-7_14

[346] Giovanni Piumatti, Andrea Sanna, Marco Gaspardone, and Fabrizio Lamberti.
2017. Spatial augmented reality meets robots: Human-machine interaction in
cloud-based projected gaming environments. In 2017 IEEE International Confer-
ence on Consumer Electronics (ICCE). IEEE, 176–179. https://doi.org/10.1109/
ICCE.2017.7889276

[347] Francesco Porpiglia, Enrico Checcucci, Daniele Amparore, Matteo Manfredi, Fed-
erica Massa, Pietro Piazzolla, Diego Manfrin, Alberto Piana, Daniele Tota, Enrico
Bollito, et al. 2019. Three-dimensional elastic augmented-reality robot-assisted
radical prostatectomy using hyperaccuracy three-dimensional reconstruction
technology: a step further in the identification of capsular involvement. Euro-
pean urology 76, 4 (2019), 505–514. https://doi.org/10.1016/j.eururo.2019.03.037
[348] Francesco Porpiglia, Enrico Checcucci, Daniele Amparore, Federico Piramide,
Gabriele Volpi, Stefano Granato, Paolo Verri, Matteo Manfredi, Andrea Bellin,
Pietro Piazzolla, et al. 2020. Three-dimensional augmented reality robot-assisted
partial nephrectomy in case of complex tumours (PADUA≥ 10): a new intraop-
erative tool overcoming the ultrasound guidance. European urology 78, 2 (2020),
229–238. https://doi.org/10.1016/j.eururo.2019.11.024

[349] Ivan Poupyrev, Tatsushi Nashida, and Makoto Okabe. 2007. Actuation and
tangible user interfaces: the Vaucanson duck, robots, and shape displays. In Pro-
ceedings of the 1st international conference on Tangible and embedded interaction.
205–212. https://doi.org/10.1145/1226969.1227012

[350] F Gabriele Pratticò, Alberto Cannavò, Junchao Chen, and Fabrizio Lamberti. 2019.
User Perception of Robot’s Role in Floor Projection-based Mixed-Reality Robotic
Games. In 2019 IEEE 23rd International Symposium on Consumer Technologies
(ISCT). IEEE, 76–81. https://doi.org/10.1109/isce.2019.8901037

[351] Filippo Gabriele Pratticò, Francisco Navarro Merino, and Fabrizio Lamberti.
2020. Is Learning by Teaching an Effective Approach in Mixed-Reality Robotic
Training Systems?. In International Conference on Intelligent Technologies for
Interactive Entertainment. Springer, 177–190. https://doi.org/10.1007/978-3-030-
76426-5_12

[352] David Puljiz, Franziska Krebs, Fabian Bosing, and Bjorn Hein. 2020. What
the HoloLens Maps Is Your Workspace: Fast Mapping and Set-up of Robot
Cells via Head Mounted Displays and Augmented Reality. In 2020 IEEE/RSJ
International Conference on Intelligent Robots and Systems (IROS). IEEE, 11445–
11451. https://doi.org/10.1109/iros45743.2020.9340879

[353] Isabel PS Qamar, Rainer Groh, David Holman, and Anne Roudaut. 2018. HCI
meets material science: A literature review of morphing materials for the design
of shape-changing interfaces. In Proceedings of the 2018 CHI Conference on
Human Factors in Computing Systems. 1–23. https://doi.org/10.1145/3173574.
3173948

[354] Long Qian, Anton Deguet, Zerui Wang, Yun-Hui Liu, and Peter Kazanzides. 2019.
Augmented reality assisted instrument insertion and tool manipulation for the
first assistant in robotic surgery. In 2019 International Conference on Robotics
and Automation (ICRA). IEEE, 5173–5179. https://doi.org/10.1109/ICRA.2019.
8794263

[355] Long Qian, Chengzhi Song, Yiwei Jiang, Qi Luo, Xin Ma, Philip Waiyan Chiu,
Zheng Li, and Peter Kazanzides. 2020. FlexiVision: Teleporting the Surgeon’s
Eyes via Robotic Flexible Endoscope and Head-Mounted Display. In 2020
IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE,
3281–3287. https://doi.org/10.1109/IROS45743.2020.9340716

23

[356] Long Qian, Jie Ying Wu, Simon P DiMaio, Nassir Navab, and Peter Kazanzides.
2019. A review of augmented reality in robotic-assisted surgery. IEEE Transac-
tions on Medical Robotics and Bionics 2, 1 (2019), 1–16. https://doi.org/10.1109/
tmrb.2019.2957061

[357] Shuwen Qiu, Hangxin Liu, Zeyu Zhang, Yixin Zhu, and Song-Chun Zhu. 2020.
Human-Robot Interaction in a Shared Augmented Reality Workspace. In 2020
IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE,
11413–11418. https://doi.org/10.1109/iros45743.2020.9340781

[358] Camilo Perez Quintero, Sarah Li, Matthew KXJ Pan, Wesley P Chan, HF Machiel
Van der Loos, and Elizabeth Croft. 2018. Robot programming through augmented
trajectories in augmented reality. In 2018 IEEE/RSJ International Conference on
Intelligent Robots and Systems (IROS). IEEE, 1838–1844. https://doi.org/10.1109/
IROS.2018.8593700

[359] Majken K Rasmussen, Esben W Pedersen, Marianne G Petersen, and Kasper
Hornbæk. 2012. Shape-changing interfaces: a review of the design space and
open research questions. In Proceedings of the SIGCHI Conference on Human
Factors in Computing Systems. 735–744. https://doi.org/10.1145/2207676.2207781
[360] Christopher Reardon, Jason Gregory, Carlos Nieto-Granda, and John G Rogers.
2020. Enabling Situational Awareness via Augmented Reality of Autonomous
Robot-Based Environmental Change Detection. In International Conference on
Human-Computer Interaction. Springer, 611–628. https://doi.org/10.1007/978-3-
030-49695-1_41

[361] Christopher Reardon, Kevin Lee, and Jonathan Fink. 2018. Come see this!
augmented reality to enable human-robot cooperative search. In 2018 IEEE
International Symposium on Safety, Security, and Rescue Robotics (SSRR). IEEE,
1–7. https://doi.org/10.1109/SSRR.2018.8468622

[362] Christopher Reardon, Kevin Lee, John G Rogers, and Jonathan Fink. 2019. Aug-
mented reality for human-robot teaming in field environments. In Interna-
tional Conference on Human-Computer Interaction. Springer, 79–92.
https:
//doi.org/10.1007/978-3-030-21565-1_6

[363] Christopher Reardon, Kevin Lee, John G Rogers, and Jonathan Fink. 2019. Com-
municating via augmented reality for human-robot teaming in field environ-
ments. In 2019 IEEE International Symposium on Safety, Security, and Rescue
Robotics (SSRR). IEEE, 94–101. https://doi.org/10.1109/SSRR.2019.8848971
[364] Andreagiovanni Reina, Mattia Salvaro, Gianpiero Francesca, Lorenzo Garattoni,
Carlo Pinciroli, Marco Dorigo, and Mauro Birattari. 2015. Augmented reality
for robots: virtual sensing technology applied to a swarm of e-pucks. In 2015
NASA/ESA Conference on Adaptive Hardware and Systems (AHS). IEEE, 1–6.
https://doi.org/10.1109/ahs.2015.7231154

[365] Ying Ren and Jiro Tanaka. 2019. Augmented Reality Based Actuated Monitor
Manipulation from Dual Point of View. In International Conference on Human-
Computer Interaction. Springer, 93–107.
https://doi.org/10.1007/978-3-030-
21565-1_7

[366] Patrick Renner, Florian Lier, Felix Friese, Thies Pfeiffer, and Sven Wachsmuth.
2018. Facilitating HRI by mixed reality techniques. In Companion of the 2018
ACM/IEEE International Conference on Human-Robot Interaction. 215–216. https:
//doi.org/10.1145/3173386.3177032

[367] Patrick Renner, Florian Lier, Felix Friese, Thies Pfeiffer, and Sven Wachsmuth.
2018. Wysiwicd: What you see is what i can do. In Companion of the 2018
ACM/IEEE International Conference on Human-Robot Interaction. 382–382. https:
//doi.org/10.1145/3173386.3177533

[368] Hyocheol Ro, Jung-Hyun Byun, Inhwan Kim, Yoon Jung Park, Kyuri Kim, and
Tack-Don Han. 2019. Projection-based augmented reality robot prototype with
human-awareness. In 2019 14th ACM/IEEE International Conference on Human-
Robot Interaction (HRI). IEEE, 598–599. https://doi.org/10.1109/HRI.2019.8673173
[369] David Robert and Cynthia Breazeal. 2012. Blended reality characters. In Proceed-
ings of the seventh annual ACM/IEEE international conference on Human-Robot
Interaction. 359–366. https://doi.org/10.1145/2157689.2157810

[370] David Robert, Ryan Wistorrt, Jesse Gray, and Cynthia Breazeal. 2010. Exploring
mixed reality robot gaming. In Proceedings of the fifth international conference
on tangible, embedded, and embodied interaction. 125–128. https://doi.org/10.
1145/1935701.1935726

[371] Nancy Rodriguez, Luis Jose Pulido, and Jean-Pierre Jessel. 2004. Enhancing a
telerobotics Java tool with augmented reality. In International Symposium and
School on Advancex Distributed Systems. Springer, 9–18. https://doi.org/10.1007/
978-3-540-25958-9_2

[372] Eric Rosen, David Whitney, Elizabeth Phillips, Gary Chien, James Tompkin,
George Konidaris, and Stefanie Tellex. 2020. Communicating robot arm mo-
tion intent through mixed reality head-mounted displays. In Robotics research.
Springer, 301–316. https://doi.org/10.1007/978-3-030-28619-4_26

[373] Alexandros Rotsidis, Andreas Theodorou, Joanna J Bryson, and Robert H
Wortham. 2019.
Improving robot transparency: An investigation with mo-
bile augmented reality. In 2019 28th IEEE International Conference on Robot and
Human Interactive Communication (RO-MAN). IEEE, 1–8. https://doi.org/10.
1109/ro-man46459.2019.8956390

[374] Anne Roudaut, Abhijit Karnik, Markus Löchtefeld, and Sriram Subramanian.
2013. Morphees: toward high" shape resolution" in self-actuated flexible mobile
devices. In Proceedings of the SIGCHI Conference on Human Factors in Computing

Systems. 593–602. https://doi.org/10.1145/2470654.2470738

https://doi.org/10.1002/adma.202002882

[375] Dávid Rozenberszki and Gábor Sörös. 2021. Towards Universal User Interfaces
for Mobile Robots. In Augmented Humans Conference 2021. 274–276. https:
//doi.org/10.1145/3458709.3458996

[376] Emanuele Ruffaldi, Filippo Brizzi, Franco Tecchia, and Sandro Bacinelli. 2016.
Third point of view augmented reality for robot intentions visualization. In
International Conference on Augmented Reality, Virtual Reality and Computer
Graphics. Springer, 471–478. https://doi.org/10.1007/978-3-319-40621-3_35

[377] JJ Ruiz, A Viguria, JR Martinez-de Dios, and A Ollero. 2015. Immersive displays
for building spatial knowledge in multi-UAV operations. In 2015 International
Conference on Unmanned Aircraft Systems (ICUAS). IEEE, 1043–1048. https:
//doi.org/10.1109/icuas.2015.7152395

[378] Golnoosh Samei, Keith Tsang, Claudia Kesch, Julio Lobo, Soheil Hor, Omid Mo-
hareri, Silvia Chang, S Larry Goldenberg, Peter C Black, and Septimiu Salcudean.
2020. A partial augmented reality system with live ultrasound and registered
preoperative MRI for guiding robot-assisted radical prostatectomy. Medical
image analysis 60 (2020), 101588. https://doi.org/10.1016/j.media.2019.101588
[379] Yasumitsu Sarai and Yusuke Maeda. 2017. Robot programming for manipulators
through volume sweeping and augmented reality. In 2017 13th ieee conference
on automation science and engineering (case). IEEE, 302–307. https://doi.org/10.
1109/COASE.2017.8256120

[380] Markus Sauer, Frauke Driewer, Manuel Göllnitz, and Klaus Schilling. 2007.
Potential and challenges of stereo augmented reality for mobile robot teleopera-
tion. IFAC Proceedings Volumes 40, 16 (2007), 183–188. https://doi.org/10.3182/
20070904-3-KR-2922.00032

[381] Markus Sauer, Martin Hess, and Klaus Schilling. 2009. Towards a predictive
mixed reality user interface for mobile robot teleoperation. IFAC Proceedings
Volumes 42, 22 (2009), 91–96. https://doi.org/10.3182/20091006-3-US-4006.00016
[382] Jürgen Scheible, Achim Hoth, Julian Saal, and Haifeng Su. 2013. Displaydrone:
a flying robot based interactive display. In Proceedings of the 2nd ACM Interna-
tional Symposium on Pervasive Displays. 49–54. https://doi.org/10.1145/2491568.
2491580

[383] Riccardo Schiavina, Lorenzo Bianchi, Francesco Chessa, Umberto Barbaresi,
Laura Cercenelli, Simone Lodi, Caterina Gaudiano, Barbara Bortolani, Andrea
Angiolini, Federico Mineo Bianchi, et al. 2021. Augmented reality to guide selec-
tive clamping and tumor dissection during robot-assisted partial nephrectomy:
a preliminary experience. Clinical genitourinary cancer 19, 3 (2021), e149–e155.
https://doi.org/10.1016/j.clgc.2020.09.005

[384] Riccardo Schiavina, Lorenzo Bianchi, Simone Lodi, Laura Cercenelli, Francesco
Chessa, Barbara Bortolani, Caterina Gaudiano, Carlo Casablanca, Matteo
Droghetti, Angelo Porreca, et al. 2021. Real-time augmented reality three-
dimensional guided robotic radical prostatectomy: preliminary experience and
evaluation of the impact on surgical planning. European urology focus 7, 6 (2021),
1260–1267. https://doi.org/10.1016/j.euf.2020.08.004

[385] Jan Schmitt, Andreas Hillenbrand, Philipp Kranz, and Tobias Kaupp. 2021. As-
sisted Human-Robot-Interaction for Industrial Assembly: Application of Spatial
Augmented Reality (SAR) for Collaborative Assembly Tasks. In Companion of
the 2021 ACM/IEEE International Conference on Human-Robot Interaction. 52–56.
https://doi.org/10.1145/3434074.3447127

[386] Julian Seifert, Sebastian Boring, Christian Winkler, Florian Schaub, Fabian
Schwab, Steffen Herrdum, Fabian Maier, Daniel Mayer, and Enrico Rukzio. 2014.
Hover Pad: interacting with autonomous and self-actuated displays in space. In
Proceedings of the 27th annual ACM symposium on User interface software and
technology. 139–147. https://doi.org/10.1145/2642918.2647385

[387] Ronny Seiger, Mandy Korzetz, Maria Gohlke, and Uwe Aßmann. 2017. Mixed
reality cyber-physical systems control and workflow composition. In Proceedings
of the 16th International Conference on Mobile and Ubiquitous Multimedia. 495–
500. https://doi.org/10.1145/3152832.3157808

[388] Martin Seleck`y, Jan Faigl, and Milan Rollo. 2019. Analysis of using mixed reality
simulations for incremental development of multi-uav systems. Journal of
Intelligent & Robotic Systems 95, 1 (2019), 211–227. https://doi.org/10.1007/
S10846-018-0875-8

[389] Atsushi Sengiku, Masanao Koeda, Atsuro Sawada, Jin Kono, Naoki Terada,
Toshinari Yamasaki, Kiminori Mizushino, Takahiro Kunii, Katsuhiko Onishi,
Hiroshi Noborio, et al. 2017. Augmented reality navigation system for robot-
assisted laparoscopic partial nephrectomy. In International Conference of Design,
User Experience, and Usability. Springer, 575–584. https://doi.org/10.1007/978-
3-319-58637-3_45

[390] Rossitza Setchi, Maryam Banitalebi Dehkordi, and Juwairiya Siraj Khan. 2020.
Explainable Robotics in Human-Robot Interactions. Procedia Computer Science
176 (2020), 3057–3066. https://doi.org/10.1016/j.procs.2020.09.198

[391] Nikitas M Sgouros and Sophia Kousidou. 2001. Generation and implementation
of mixed-reality, narrative performances involving robotic actors. In Interna-
tional Conference on Virtual Storytelling. Springer, 69–78. https://doi.org/10.
1007/3-540-45420-9_9

[392] Dylan Shah, Bilige Yang, Sam Kriegman, Michael Levin, Josh Bongard, and
Rebecca Kramer-Bottiglio. 2021. Shape changing robots: bioinspiration, sim-
ulation, and physical realization. Advanced Materials 33, 19 (2021), 2002882.

24

[393] Shyang Shao, Satoshi Muramatsu, Katsuhiko Inagaki, Daisuke Chugo, Syo
Yokota, and Hiroshi Hashimoto. 2019. Development of robot design evaluating
system using Augmented Reality for affinity robots. In 2019 IEEE 17th Inter-
national Conference on Industrial Informatics (INDIN), Vol. 1. IEEE, 815–820.
https://doi.org/10.1109/indin41052.2019.8972057

[394] Jun Shen, Nabil Zemiti, Christophe Taoum, Guillaume Aiche, Jean-Louis Dil-
lenseger, Philippe Rouanet, and Philippe Poignet. 2020. Transrectal ultra-
sound image-based real-time augmented reality guidance in robot-assisted
laparoscopic rectal surgery: a proof-of-concept study.
International jour-
nal of computer assisted radiology and surgery 15, 3 (2020), 531–543. https:
//doi.org/10.1007/s11548-019-02100-2

[395] Noriyoshi Shimizu, Maki Sugimoto, Dairoku Sekiguchi, Shoichi Hasegawa, and
Masahiko Inami. 2008. Mixed reality robotic user interface: virtual kinematics
to enhance robot motion. In Proceedings of the 2008 International Conference on
Advances in Computer Entertainment Technology. 166–169. https://doi.org/10.
1145/1501750.1501789

[396] Elena Sibirtseva, Ali Ghadirzadeh, Iolanda Leite, Mårten Björkman, and Dan-
ica Kragic. 2019. Exploring temporal dependencies in multimodal referring
expressions with mixed reality. In International Conference on Human-Computer
Interaction. Springer, 108–123. https://doi.org/10.1007/978-3-030-21565-1_8

[397] Dietmar Siegele, Dieter Steiner, Andrea Giusti, Michael Riedl, and Dominik T
Matt. 2021. Optimizing Collaborative Robotic Workspaces in Industry by Ap-
plying Mixed Reality. In International Conference on Augmented Reality, Virtual
Reality and Computer Graphics. Springer, 544–559. https://doi.org/10.1007/978-
3-030-87595-4_40

[398] Torsten Sebastian Sievers, Bianca Schmitt, Patrick Rückert, Maren Petersen,
and Kirsten Tracht. 2020. Concept of a Mixed-Reality Learning Environment
for Collaborative Robotics. Procedia Manufacturing 45 (2020), 19–24. https:
//doi.org/10.1016/j.promfg.2020.04.034

[399] David Sirkin, Brian Mok, Stephen Yang, and Wendy Ju. 2015. Mechanical
ottoman: how robotic furniture offers and withdraws support. In Proceedings of
the Tenth Annual ACM/IEEE International Conference on Human-Robot Interaction.
11–18. https://doi.org/10.1145/2696454.2696461

[400] Enrico Sita, Matthew Studley, Farid Dailami, Anthony Pipe, and Trygve Thome-
ssen. 2017. Towards multimodal interactions: robot jogging in mixed reality.
In Proceedings of the 23rd ACM Symposium on Virtual Reality Software and
Technology. 1–2. https://doi.org/10.1145/3139131.3141200

[401] Alexa F Siu, Shenli Yuan, Hieu Pham, Eric Gonzalez, Lawrence H Kim, Mathieu
Le Goc, and Sean Follmer. 2018. Investigating tangible collaboration for design
towards augmented physical telepresence. In Design thinking research. Springer,
131–145. https://doi.org/10.1007/978-3-319-60967-6_7

[402] J Ernesto Solanes, Adolfo Muñoz, Luis Gracia, Ana Martí, Vicent Girbés-Juan,
and Josep Tornero. 2020. Teleoperation of industrial robot manipulators based
on augmented reality. The International Journal of Advanced Manufacturing
Technology 111, 3 (2020), 1077–1097. https://doi.org/10.1007/s00170-020-05997-
1

[403] Sichao Song and Seiji Yamada. 2018. Bioluminescence-inspired human-robot
interaction: designing expressive lights that affect human’s willingness to inter-
act with a robot. In Proceedings of the 2018 ACM/IEEE International Conference
on Human-Robot Interaction. ACM, 224–232. https://doi.org/10.1145/3171221.
3171249

[404] Adam Sosa, Richard Stanton, Stepheny Perez, Christian Keyes-Garcia, Sara
Gonzalez, and Zachary O Toups. 2015.
Imperfect robot control in a mixed
reality game to teach hybrid human-robot team coordination. In Proceedings of
the 2015 Annual Symposium on Computer-Human Interaction in Play. 697–702.
https://doi.org/10.1145/2793107.2810288

[405] Maximilian Speicher, Brian D Hall, and Michael Nebeling. 2019. What is mixed
reality?. In Proceedings of the 2019 CHI Conference on Human Factors in Computing
Systems. 1–15. https://doi.org/10.1145/3290605.3300767

[406] Aaron St. Clair and Maja Mataric. 2015. How robot verbal feedback can improve
team performance in human-robot task collaborations. In Proceedings of the tenth
annual acm/ieee international conference on human-robot interaction. 213–220.
https://doi.org/10.1145/2696454.2696491

[407] Susanne Stadler, Kevin Kain, Manuel Giuliani, Nicole Mirnig, Gerald
Stollnberger, and Manfred Tscheligi. 2016. Augmented reality for industrial
robot programmers: Workload analysis for task-based, augmented reality-
supported robot control. In 2016 25th IEEE International Symposium on Ro-
bot and Human Interactive Communication (RO-MAN). IEEE, 179–184. https:
//doi.org/10.1109/ROMAN.2016.7745108

[408] Gordon Stein and Ákos Lédeczi. 2019. Mixed reality robotics for stem education.
In 2019 IEEE Blocks and Beyond Workshop (B&B). IEEE, 49–53. https://doi.org/
10.1109/bb48857.2019.8941229

[409] Camille Linick Stewart, Abigail Fong, Govinda Payyavula, Simon DiMaio, Kelly
Lafaro, Kirsten Tallmon, Sherry Wren, Jonathan Sorger, and Yuman Fong. 2021.
Study on augmented reality for robotic surgery bedside assistants. Journal of
Robotic Surgery (2021), 1–8. https://doi.org/10.1007/s11701-021-01335-z

[410] Dominykas Strazdas, Jan Hintz, and Ayoub Al-Hamadi. 2021. Robo-HUD:
Interaction Concept for Contactless Operation of Industrial Cobotic Systems.
Applied Sciences 11, 12 (2021), 5366. https://doi.org/10.3390/APP11125366
[411] Li-Ming Su, Balazs P Vagvolgyi, Rahul Agarwal, Carol E Reiley, Russell H
Taylor, and Gregory D Hager. 2009. Augmented reality during robot-assisted
laparoscopic partial nephrectomy: toward real-time 3D-CT to stereoscopic video
registration. Urology 73, 4 (2009), 896–900. https://doi.org/10.1016/j.urology.
2008.11.040

[412] Mu-Chun Su, Gwo-Dong Chen, Yi-Shan Tsai, Ren-Hao Yao, Chung-Kuang
Chou, Yohannes Budiono Jinawi, De-Yuan Huang, Yi-Zeng Hsieh, and Shih-
Chieh Lin. 2009. Design of an Interactive Table for Mixed-Reality Learning
Environments. In International Conference on Technologies for E-Learning and
Digital Entertainment. Springer, 489–494. https://doi.org/10.1007/978-3-642-
03364-3_59

[413] Yun-Peng Su, Xiao-Qi Chen, Tony Zhou, Christopher Pretty, and J Geoffrey
Chase. 2021. Mixed Reality-Enhanced Intuitive Teleoperation with Hybrid
Virtual Fixtures for Intelligent Robotic Welding. Applied Sciences 11, 23 (2021),
11280. https://doi.org/10.3390/app112311280

[414] EK Subin, Ashik Hameed, and AP Sudheer. 2017. Android based augmented
In Proceedings of the

reality as a social interface for low cost social robots.
Advances in Robotics. 1–4. https://doi.org/10.1145/3132446.3134907

[415] Masanori Sugimoto. 2011. A mobile mixed-reality environment for children’s
storytelling using a handheld projector and a robot.
IEEE Transactions on
Learning Technologies 4, 3 (2011), 249–260. https://doi.org/10.1109/tlt.2011.13

[416] Masanori Sugimoto, Tomoki Fujita, Haipeng Mi, and Aleksander Krzywinski.
2011. RoboTable2: a novel programming environment using physical robots
on a tabletop platform. In Proceedings of the 8th International Conference on
Advances in Computer Entertainment Technology. 1–8. https://doi.org/10.1145/
2071423.2071436

[417] Maki Sugimoto, Georges Kagotani, Hideaki Nii, Naoji Shiroma, Fumitoshi Mat-
suno, and Masahiko Inami. 2005. Time Follower’s Vision: a teleoperation inter-
face with past images. IEEE Computer Graphics and Applications 25, 1 (2005),
54–63. https://doi.org/10.1109/mcg.2005.23

[418] Ippei Suzuki, Shuntarou Yoshimitsu, Keisuke Kawahara, Nobutaka Ito, Atushi
Shinoda, Akira Ishii, Takatoshi Yoshida, and Yoichi Ochiai. 2016. Gushed dif-
fusers: Fast-moving, floating, and lightweight midair display. In Proceedings of
the 29th Annual Symposium on User Interface Software and Technology. 69–70.
https://doi.org/10.1145/2984751.2985706

[419] Naoki Suzuki and Asaki Hattori. 2012. Development of new augmented reality
function using intraperitoneal multi-view camera. In Workshop on Augmented
Environments for Computer-Assisted Interventions. Springer, 67–76. https://doi.
org/10.1007/978-3-642-38085-3_8

[420] Naoki Suzuki, Asaki Hattori, Kazuo Tanoue, Satoshi Ieiri, Kozo Konishi, Mori-
masa Tomikawa, Hajime Kenmotsu, and Makoto Hashizume. 2010. Scorpion
shaped endoscopic surgical robot for NOTES and SPS with augmented reality
functions. In International Workshop on Medical Imaging and Virtual Reality.
Springer, 541–550. https://doi.org/10.1007/978-3-642-15699-1_57

[421] Ryo Suzuki, Hooman Hedayati, Clement Zheng, James L Bohn, Daniel Szafir,
Ellen Yi-Luen Do, Mark D Gross, and Daniel Leithinger. 2020. Roomshift:
Room-scale dynamic haptics for vr with furniture-moving swarm robots. In
Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems.
1–11. https://doi.org/10.1145/3313831.3376523

[422] Ryo Suzuki, Jun Kato, Mark D Gross, and Tom Yeh. 2018. Reactile: Programming
swarm user interfaces through direct physical manipulation. In Proceedings of
the 2018 CHI Conference on Human Factors in Computing Systems. 1–13. https:
//doi.org/10.1145/3173574.3173773

[423] Ryo Suzuki, Rubaiat Habib Kazi, Li-Yi Wei, Stephen DiVerdi, Wilmot Li, and
Daniel Leithinger. 2020. RealitySketch: Embedding Responsive Graphics and
Visualizations in AR through Dynamic Sketching. In Proceedings of the 33rd
Annual ACM Symposium on User Interface Software and Technology. 166–181.
https://doi.org/10.1145/3379337.3415892

[424] Ryo Suzuki, Eyal Ofek, Mike Sinclair, Daniel Leithinger, and Mar Gonzalez-
Franco. 2021. HapticBots: Distributed Encountered-type Haptics for VR with
Multiple Shape-changing Mobile Robots. In The 34th Annual ACM Symposium
on User Interface Software and Technology. 1269–1281. https://doi.org/10.1145/
3472749.3474821

[425] Ryo Suzuki, Clement Zheng, Yasuaki Kakehi, Tom Yeh, Ellen Yi-Luen Do, Mark D
Gross, and Daniel Leithinger. 2019. Shapebots: Shape-changing swarm robots.
In Proceedings of the 32nd Annual ACM Symposium on User Interface Software
and Technology. 493–505. https://doi.org/10.1145/3332165.3347911

[426] Daniel Szafir, Bilge Mutlu, and Terrence Fong. 2014. Communication of intent in
assistive free flyers. In 2014 9th ACM/IEEE International Conference on Human-
Robot Interaction (HRI). IEEE, 358–365. https://doi.org/10.1145/2559636.2559672
[427] Daniel Szafir, Bilge Mutlu, and Terrence Fong. 2015. Communicating direction-
ality in flying robots. In 2015 10th ACM/IEEE International Conference on Human-
Robot Interaction (HRI). IEEE, 19–26. https://doi.org/10.1145/2696454.2696475
[428] Daniel Szafir and Danielle Albers Szafir. 2021. Connecting Human-Robot In-
teraction and Data Visualization. In Proceedings of the 2021 ACM/IEEE Interna-
tional Conference on Human-Robot Interaction. 281–292. https://doi.org/10.1145/

25

3434073.3444683

[429] Faisal Taher, John Hardy, Abhijit Karnik, Christian Weichel, Yvonne Jansen,
Kasper Hornbæk, and Jason Alexander. 2015. Exploring interactions with
physically dynamic bar charts. In Proceedings of the 33rd annual acm conference
on human factors in computing systems. 3237–3246. https://doi.org/10.1145/
2702123.2702604

[430] Kazuki Takashima, Naohiro Aida, Hitomi Yokoyama, and Yoshifumi Kitamura.
2013. TransformTable: a self-actuated shape-changing digital table. In Proceed-
ings of the 2013 ACM international conference on Interactive tabletops and surfaces.
179–188. https://doi.org/10.1145/2512349.2512818

[431] Kazuki Takashima, Takafumi Oyama, Yusuke Asari, Ehud Sharlin, Saul Green-
berg, and Yoshifumi Kitamura. 2016. Study and design of a shape-shifting wall
display. In Proceedings of the 2016 ACM Conference on Designing Interactive
Systems. 796–806. https://doi.org/10.1145/2901790.2901892

[432] Leonardo Tanzi, Pietro Piazzolla, Francesco Porpiglia, and Enrico Vezzetti. 2021.
Real-time deep learning semantic segmentation during intra-operative surgery
for 3D augmented reality assistance. International Journal of Computer Assisted
Radiology and Surgery 16, 9 (2021), 1435–1445. https://doi.org/10.1007/s11548-
021-02432-y

[433] Pedro Tavares, Carlos M Costa, Luís Rocha, Pedro Malaca, Pedro Costa, An-
tónio P Moreira, Armando Sousa, and Germano Veiga. 2019. Collaborative
welding system using BIM for robotic reprogramming and spatial augmented
reality. Automation in Construction 106 (2019), 102825. https://doi.org/10.1016/
J.AUTCON.2019.04.020

[434] Frank Thomas, Ollie Johnston, and Frank Thomas. 1995. The illusion of life:

Disney animation. Hyperion New York.

[435] Rundong Tian and Eric Paulos. 2021. Adroid: Augmenting Hands-on Making
with a Collaborative Robot. In Proceedings of the 34th Annual ACM Symposium on
User Interface Software and Technology. https://doi.org/10.1145/3472749.3474749
[436] Hiroaki Tobita, Shigeaki Maruyama, and Takuya Kuji. 2011. Floating avatar:
blimp-based telepresence system for communication and entertainment. In
ACM SIGGRAPH 2011 Emerging Technologies. 1–1. https://doi.org/10.1145/
2048259.2048263

[437] Junya Tominaga, Kensaku Kawauchi, and Jun Rekimoto. 2014. Around me: a
system with an escort robot providing a sports player’s self-images. In Pro-
ceedings of the 5th Augmented Human International Conference. 1–8. https:
//doi.org/10.1145/2582051.2582094

[438] Bethan Hannah Topliss, Sanna M Pampel, Gary Burnett, Lee Skrypchuk, and
Chrisminder Hare. 2018. Establishing the role of a virtual lead vehicle as a
novel augmented reality navigational aid. In Proceedings of the 10th International
Conference on Automotive User Interfaces and Interactive Vehicular Applications.
137–145. https://doi.org/10.1145/3239060.3239069

[439] Nhan Tran. 2020. Exploring mixed reality robot communication under different
types of mental workload. Colorado School of Mines. https://doi.org/10.31219/
osf.io/f3a8c

[440] Nhan Tran, Trevor Grant, Thao Phung, Leanne Hirshfield, Christopher Wick-
ens, and Tom Williams. 2021. Get This!? Mixed Reality Improves Robot
Communication Regardless of Mental Workload. In Companion of the 2021
ACM/IEEE International Conference on Human-Robot Interaction. 412–416. https:
//doi.org/10.1145/3434074.3447203

[441] Nhan Tran, Trevor Grant, Thao Phung, Leanne Hirshfield, Christopher Wick-
ens, and Tom Williams. 2021. Robot-Generated Mixed Reality Gestures Im-
prove Human-Robot Interaction. In International Conference on Social Robotics.
Springer, 768–773. https://doi.org/10.1007/978-3-030-90525-5_69

[442] Jörg Traub, Marco Feuerstein, Martin Bauer, Eva U Schirmbeck, Hesam Najafi,
Robert Bauernschmitt, and Gudrun Klinker. 2004. Augmented reality for port
placement and navigation in robotically assisted minimally invasive cardio-
vascular surgery. In International Congress Series, Vol. 1268. Elsevier, 735–740.
https://doi.org/10.1016/J.ICS.2004.03.049

[443] Jaryd Urbani, Mohammed Al-Sada, Tatsuo Nakajima, and Thomas Höglund.
2018. Exploring Augmented Reality Interaction for Everyday Multipurpose
Wearable Robots. In 2018 IEEE 24th International Conference on Embedded and
Real-Time Computing Systems and Applications (RTCSA). IEEE, 209–216. https:
//doi.org/10.1109/RTCSA.2018.00033

[444] AJN Van Breemen. 2004. Bringing robots to life: Applying principles of anima-
tion to robots. In Proceedings of Shapping Human-Robot Interaction workshop
held at CHI, Vol. 2004. Citeseer, 143–144.

[445] Ana M Villanueva, Ziyi Liu, Zhengzhe Zhu, Xin Du, Joey Huang, Kylie A Pep-
pler, and Karthik Ramani. 2021. RobotAR: An Augmented Reality Compatible
Teleconsulting Robotics Toolkit for Augmented Makerspace Experiences. In
Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems.
1–13. https://doi.org/10.1145/3411764.3445726

[446] Daniel Vogel and Ravin Balakrishnan. 2004. Interactive public ambient displays:
transitioning from implicit to explicit, public to personal, interaction with mul-
tiple users. In Proceedings of the 17th annual ACM symposium on User interface
software and technology. 137–146. https://doi.org/10.1145/1029632.1029656

[447] Francesco Volonté, François Pugin, Pascal Bucher, Maki Sugimoto, Osman Ratib,
and Philippe Morel. 2011. Augmented reality and image overlay navigation
with OsiriX in laparoscopic and robotic surgery: not only a matter of fashion.
Journal of Hepato-biliary-pancreatic Sciences 18, 4 (2011), 506–509. https://doi.
org/10.1007/s00534-011-0385-6

[448] Sebastian von Mammen, Heiko Hamann, and Michael Heider. 2016. Robot
gardens: an augmented reality prototype for plant-robot biohybrid systems.
In Proceedings of the 22nd ACM Conference on Virtual Reality Software and
Technology. 139–142. https://doi.org/10.1145/2993369.2993400

[449] Emanuel Vonach, Clemens Gatterer, and Hannes Kaufmann. 2017. VRRobot:
Robot actuated props in an infinite virtual environment. In 2017 IEEE Virtual
Reality (VR). IEEE, 74–83. https://doi.org/10.1109/VR.2017.7892233

[450] Michael Walker, Hooman Hedayati, Jennifer Lee, and Daniel Szafir. 2018. Com-
municating robot motion intent with augmented reality. In Proceedings of the
2018 ACM/IEEE International Conference on Human-Robot Interaction. 316–324.
https://doi.org/10.1145/3171221.3171253

[451] Michael E Walker, Hooman Hedayati, and Daniel Szafir. 2019. Robot tele-
operation with augmented reality virtual surrogates. In 2019 14th ACM/IEEE
International Conference on Human-Robot Interaction (HRI). IEEE, 202–210.
https://doi.org/10.1109/HRI.2019.8673306

[452] DA Wang, Fernando Bello, and Ara Darzi. 2004. Augmented reality provision in
robotically assisted minimally invasive surgery. In International Congress Series,
Vol. 1268. Elsevier, 527–532. https://doi.org/10.1016/J.ICS.2004.03.057

[453] Guoping Wang, Xuechen Chen, Sheng Liu, Chingping Wong, and Sheng Chu.
2016. Mechanical chameleon through dynamic real-time plasmonic tuning. Acs
Nano 10, 2 (2016), 1788–1794. https://doi.org/10.1021/acsnano.5b07472
[454] Qiang Wang, Xiumin Fan, Mingyu Luo, Xuyue Yin, and Wenmin Zhu. 2020. Con-
struction of Human-Robot Cooperation Assembly Simulation System Based on
Augmented Reality. In International Conference on Human-Computer Interaction.
Springer, 629–642. https://doi.org/10.1007/978-3-030-49695-1_42

[455] Tianyi Wang, Xun Qian, Fengming He, Xiyun Hu, Ke Huo, Yuanzhi Cao, and
Karthik Ramani. 2020. CAPturAR: An Augmented Reality Tool for Authoring
Human-Involved Context-Aware Applications. In Proceedings of the 33rd Annual
ACM Symposium on User Interface Software and Technology. 328–341. https:
//doi.org/10.1145/3379337.3415815

[456] Xi Vincent Wang, Lihui Wang, Mingtian Lei, and Yongqing Zhao. 2020. Closed-
loop augmented reality towards accurate human-robot collaboration. CIRP
annals 69, 1 (2020), 425–428. https://doi.org/10.1016/j.cirp.2020.03.014

[457] Jonas Wassermann, Axel Vick, and Jörg Krüger. 2018.

Intuitive robot pro-
gramming through environment perception, augmented reality simulation and
automated program verification. Procedia CIRP 76 (2018), 161–166. https:
//doi.org/10.1016/J.PROCIR.2018.01.036

[458] Atsushi Watanabe, Tetsushi Ikeda, Yoichi Morales, Kazuhiko Shinozawa,
Takahiro Miyashita, and Norihiro Hagita. 2015. Communicating robotic naviga-
tional intentions. In 2015 IEEE/RSJ International Conference on Intelligent Robots
and Systems (IROS). IEEE, 5763–5769. https://doi.org/10.1109/IROS.2015.7354195
[459] Rong Wen, Chin-Boon Chng, and Chee-Kong Chui. 2017. Augmented reality
guidance with multimodality imaging data and depth-perceived interaction
for robot-assisted surgery. Robotics 6, 2 (2017), 13. https://doi.org/10.3390/
robotics6020013

[460] Rong Wen, Chin-Boon Chng, Chee-Kong Chui, Kah-Bin Lim, Sim-Heng Ong,
and Stephen Kin-Yong Chang. 2012. Robot-assisted RF ablation with interactive
planning and mixed reality guidance. In 2012 IEEE/SICE International Symposium
on System Integration (SII). IEEE, 31–36. https://doi.org/10.1109/SII.2012.6426963
[461] Rong Wen, Wei-Liang Tay, Binh P Nguyen, Chin-Boon Chng, and Chee-Kong
Chui. 2014. Hand gesture guided robot-assisted surgery based on a direct
augmented reality interface. Computer methods and programs in biomedicine
116, 2 (2014), 68–80. https://doi.org/10.1016/j.cmpb.2013.12.018

[462] Wesley Willett, Bon Adriel Aseniero, Sheelagh Carpendale, Pierre Dragicevic,
Yvonne Jansen, Lora Oehlberg, and Petra Isenberg. 2021. Perception! Immersion!
Empowerment!: Superpowers as Inspiration for Visualization. IEEE Transactions
on Visualization and Computer Graphics (2021). https://doi.org/10.1109/TVCG.
2021.3114844

[463] Wesley Willett, Yvonne Jansen, and Pierre Dragicevic. 2016. Embedded data
representations. IEEE transactions on visualization and computer graphics 23, 1
(2016), 461–470. https://doi.org/10.1109/TVCG.2016.2598608

[464] Tom Williams, Matthew Bussing, Sebastian Cabrol, Elizabeth Boyle, and Nhan
Tran. 2019. Mixed reality deictic gesture for multi-modal robot communication.
In 2019 14th ACM/IEEE International Conference on Human-Robot Interaction
(HRI). IEEE, 191–201. https://doi.org/10.1109/hri.2019.8673275

[465] Tom Williams, Leanne Hirshfield, Nhan Tran, Trevor Grant, and Nicholas Wood-
ward. 2020. Using augmented reality to better study human-robot interaction.
In International Conference on Human-Computer Interaction. Springer, 643–654.
https://doi.org/10.1007/978-3-030-49695-1_43

[466] Tom Williams, Daniel Szafir, Tathagata Chakraborti, and Heni Ben Amor. 2018.
Virtual, augmented, and mixed reality for human-robot interaction. In Compan-
ion of the 2018 ACM/IEEE International Conference on Human-Robot Interaction.

403–404. https://doi.org/10.1145/3173386.3173561

[467] Ryan Wistort and Cynthia Breazeal. 2011. TofuDraw: A mixed-reality chore-
ography tool for authoring robot character performance. In Proceedings of
the 10th International Conference on Interaction Design and Children. 213–216.
https://doi.org/10.1145/1999030.1999064

[468] Mulun Wu, Shi-Lu Dai, and Chenguang Yang. 2020. Mixed reality enhanced user
interactive path planning for omnidirectional mobile robot. Applied Sciences 10,
3 (2020), 1135. https://doi.org/10.3390/app10031135

[469] Mulun Wu, Yanbin Xu, Chenguang Yang, and Ying Feng. 2018. Omnidirectional
mobile robot control based on mixed reality and semg signals. In 2018 Chinese
Automation Congress (CAC). IEEE, 1867–1872. https://doi.org/10.1109/cac.2018.
8623114

[470] Tian Xia, Simon Léonard, Anton Deguet, Louis Whitcomb, and Peter Kazanzides.
2012. Augmented reality environment with virtual fixtures for robotic tele-
manipulation in space. In 2012 IEEE/RSJ International Conference on Intelligent
Robots and Systems. IEEE, 5059–5064. https://doi.org/10.1109/iros.2012.6386169
[471] Siyuan Xiang, Ruoyu Wang, and Chen Feng. 2021. Mobile projective augmented
reality for collaborative robots in construction. Automation in Construction 127
(2021), 103704. https://doi.org/10.1016/J.AUTCON.2021.103704

[472] Xiao Xiao, Paula Aguilera, Jonathan Williams, and Hiroshi Ishii. 2013. Mir-
rorFugue iii: conjuring the recorded pianist.. In CHI Extended Abstracts. Citeseer,
2891–2892. https://doi.org/10.1145/2468356.2479564

[473] Xiao Xiao, Pablo Puentes, Edith Ackermann, and Hiroshi Ishii. 2016. Andantino:
Teaching children piano with projected animated characters. In Proceedings of
the the 15th international conference on interaction design and children. 37–45.
https://doi.org/10.1145/2930674.2930689

[474] Chung Xue, Yuansong Qiao, and Niall Murray. 2020. Enabling Human-Robot-
Interaction for remote robotic operation via Augmented Reality. In 2020 IEEE
21st International Symposium on" A World of Wireless, Mobile and Multimedia
Networks"(WoWMoM). IEEE, 194–196. https://doi.org/10.1109/wowmom49955.
2020.00046

[475] Wataru Yamada, Kazuhiro Yamada, Hiroyuki Manabe, and Daizo Ikeda. 2017.
iSphere: self-luminous spherical drone display. In Proceedings of the 30th Annual
ACM Symposium on User Interface Software and Technology. 635–643. https:
//doi.org/10.1145/3126594.3126631

[476] Junichi Yamaoka and Yasuaki Kakehi. 2016. MiragePrinter: interactive fabrica-
tion on a 3D printer with a mid-air display. In ACM SIGGRAPH 2016 Studio. 1–2.
https://doi.org/10.1145/2929484.2929489

[477] AWW Yew, SK Ong, and AYC Nee. 2017. Immersive augmented reality envi-
ronment for the teleoperation of maintenance robots. Procedia Cirp 61 (2017),
305–310. https://doi.org/10.1016/J.PROCIR.2016.11.183

[478] Yan Yixian, Kazuki Takashima, Anthony Tang, Takayuki Tanno, Kazuyuki Fujita,
and Yoshifumi Kitamura. 2020. Zoomwalls: Dynamic walls that simulate haptic
infrastructure for room-scale vr world. In Proceedings of the 33rd Annual ACM
Symposium on User Interface Software and Technology. 223–235. https://doi.org/
10.1145/3379337.3415859

[479] James Young and Ehud Sharlin. 2006. A Mixed Reality Approach to Human-

Robot Interaction. (2006). https://doi.org/10.11575/PRISM/30998

[480] James Young, Ehud Sharlin, and Takeo Igarashi. 2011. What is mixed reality,
anyway? Considering the boundaries of mixed reality in the context of robots.
In Mixed Reality and Human-Robot Interaction. Springer, 1–11. https://doi.org/
10.1007/978-94-007-0582-1_1

[481] James E Young, Min Xin, and Ehud Sharlin. 2007. Robot expressionism through
cartooning. In 2007 2nd ACM/IEEE International Conference on Human-Robot
Interaction (HRI). IEEE, 309–316. https://doi.org/10.1145/1228716.1228758
[482] Liangzhe Yuan, Christopher Reardon, Garrett Warnell, and Giuseppe Loianno.
2019. Human gaze-driven spatial tasking of an autonomous MAV. IEEE Robotics
and Automation Letters 4, 2 (2019), 1343–1350. https://doi.org/10.1109/LRA.
2019.2895419

[483] Ludek Zalud. 2007. Augmented reality user interface for reconnaissance robotic
missions. In RO-MAN 2007-The 16th IEEE International Symposium on Robot
and Human Interactive Communication. IEEE, 974–979. https://doi.org/10.1109/
roman.2007.4415224

[484] Ludek Zalud, Petra Kocmanova, Frantisek Burian, and Tomas Jilek. 2014. Color
and thermal image fusion for augmented reality in rescue robotics. In The 8th In-
ternational Conference on Robotic, Vision, Signal Processing & Power Applications.
Springer, 47–55. https://doi.org/10.1007/978-981-4585-42-2_6

[485] Bowei Zeng, Fanle Meng, Hui Ding, and Guangzhi Wang. 2017. A surgical
robot with augmented reality visualization for stereoelectroencephalography
electrode implantation. International journal of computer assisted radiology and
surgery 12, 8 (2017), 1355–1368. https://doi.org/10.1007/s11548-017-1634-1

[486] Dongpu Zhang, Lin Tian, Kewu Huang, and Jiwu Wang. 2020. Vision Tracking
Algorithm for Augmented Reality System of Teleoperation Mobile Robots. In
2020 3rd International Conference on Unmanned Systems (ICUS). IEEE, 1047–1052.
https://doi.org/10.1109/icus50048.2020.9274917

[487] Fengxin Zhang, Chow Yin Lai, Milan Simic, and Songlin Ding. 2020. Augmented
reality in robot programming. Procedia Computer Science 176 (2020), 1221–1230.
https://doi.org/10.1016/j.procs.2020.09.119

26

[488] Renjie Zhang, Xinyu Liu, Jiazhou Shuai, and Lianyu Zheng. 2020. Collabora-
tive robot and mixed reality assisted microgravity assembly for large space
mechanism. Procedia Manufacturing 51 (2020), 38–45. https://doi.org/10.1016/j.
promfg.2020.10.007

[489] Zhou Zhao, Panfeng Huang, Zhenyu Lu, and Zhengxiong Liu. 2017. Augmented
reality for enhancing tele-robotic system with force feedback. Robotics and
Autonomous Systems 96 (2017), 93–101. https://doi.org/10.1016/j.robot.2017.05.
017

[490] Allan Zhou, Dylan Hadfield-Menell, Anusha Nagabandi, and Anca D Dragan.
2017. Expressive robot motion timing. In Proceedings of the 2017 ACM/IEEE
International Conference on Human-Robot Interaction. ACM, 22–31. https://doi.
org/10.1145/2909824.3020221

[491] Chaozheng Zhou, Ming Zhu, Yunyong Shi, Li Lin, Gang Chai, Yan Zhang, and
Le Xie. 2017. Robot-assisted surgery for mandibular angle split osteotomy using
augmented reality: preliminary results on clinical animal experiment. Aesthetic
plastic surgery 41, 5 (2017), 1228–1236. https://doi.org/10.1007/s00266-017-0900-
5

[492] Danny Zhu and Manuela Veloso. 2016. Virtually adapted reality and algorithm
visualization for autonomous robots. In Robot World Cup. Springer, 452–464.

https://doi.org/10.1007/978-3-319-68792-6_38

[493] Kamil Židek, Ján Pitel’, Michal Balog, Alexander Hošovsk`y, Vratislav Hladk`y,
Peter Lazorík, Angelina Iakovets, and Jakub Demčák. 2021. CNN Training Using
3D Virtual Models for Assisted Assembly with Mixed Reality and Collaborative
Robots. Applied Sciences 11, 9 (2021), 4269. https://doi.org/10.3390/APP11094269
[494] Stefanie Zollmann, Christof Hoppe, Tobias Langlotz, and Gerhard Reitmayr.
2014. Flyar: Augmented reality supported micro aerial vehicle navigation.
IEEE transactions on visualization and computer graphics 20, 4 (2014), 560–568.
https://doi.org/10.1109/TVCG.2014.24

[495] Mark Zolotas, Joshua Elsdon, and Yiannis Demiris. 2018. Head-mounted aug-
mented reality for explainable robotic wheelchair assistance. In 2018 IEEE/RSJ
International Conference on Intelligent Robots and Systems (IROS). IEEE, 1823–
1829. https://doi.org/10.1109/iros.2018.8594002

[496] Wenchao Zou, Mayur Andulkar, and Ulrich Berger. 2018. Development of Robot
Programming System through the use of Augmented Reality for Assembly
Tasks. In ISR 2018; 50th International Symposium on Robotics. VDE, 1–7. https:
//doi.org/10.1201/9781439863992-10

27

Appendix Table: Full Citation List

Category

Count

Citations

Section 3. Approaches to Augmenting Reality in Robotics

Augment Robots
→ On-Body (Head and
Eye)

→ On-Body (Handheld)

→ On-Environment

→ On-Robot

Augment Surroundings
→ On-Body (Head and
Eye)

→ On-Body (Handheld)

→ On-Environment

→ On-Robot

103

26

74

9

146

28

139

33

Figure: [198] — [25–27, 30, 31, 33, 42, 43, 46–48, 51, 52, 59, 61, 67, 69, 76, 78, 80, 83, 85, 103, 107–109, 114, 118, 126, 134, 137,
156, 158, 160–162, 165, 174, 180, 181, 188, 189, 194, 197, 198, 200, 209, 211, 220, 224–226, 231, 252, 262, 267, 271, 283, 285,
287, 290, 292, 296, 297, 301, 302, 315, 323–327, 329, 342, 352, 354, 358, 360, 363, 366, 367, 372, 375, 376, 379, 387, 393, 396–
398, 400, 413, 443, 450, 451, 454, 456, 474, 477, 481, 482, 487, 488]
Figure: [481] — [9, 18, 19, 53, 55, 76, 101, 115, 130, 131, 133, 136, 144, 153, 195, 206, 209, 243, 263, 275, 305, 328, 373, 414, 481,
494]
Figure: [431] — [9, 13, 15, 21, 31, 44, 45, 48, 64, 71, 73, 74, 82, 84, 92, 96, 101, 102, 104, 110, 115, 116, 120–124, 127, 128,
143, 145, 146, 159, 163, 167, 169, 193, 212, 216, 221, 243–245, 253, 258, 265, 284, 286, 290, 306, 310, 312, 313, 328, 330, 333–
335, 365, 369, 374, 378, 380, 388, 395, 410, 417, 429–431, 442, 472, 489, 492]
Figure: [14] — [14, 210, 382, 391, 418, 436, 445, 475]

Figure: [341] — [11, 24–27, 29, 30, 33, 38, 39, 43, 47, 51, 54, 56, 59, 61, 66, 67, 69, 76, 79, 80, 85, 86, 93, 98, 105, 111, 114, 118, 119,
125, 126, 137, 139, 141, 149, 156, 158, 161, 165, 171, 174, 177, 181, 182, 184, 185, 189, 190, 194, 196, 199, 200, 205, 211, 213, 215,
220, 222, 224–226, 231, 234, 237, 238, 241, 242, 248, 252, 255, 256, 262, 267, 270, 274, 278, 282, 283, 285, 287, 292, 296, 297, 301,
302, 315, 323, 325–327, 329, 332, 333, 336, 341–343, 352, 354, 355, 357, 358, 360–363, 366, 367, 372, 376, 379, 381, 394, 396–
398, 400, 402, 409, 413, 439–441, 443, 448, 450, 451, 454, 456, 457, 464, 468–470, 477, 479, 480, 482, 483, 488, 491, 493, 495]
Figure: [133] — [19, 53, 55, 62, 63, 76, 89, 101, 131–133, 135, 136, 144, 149, 153, 177, 201, 206, 228, 232, 263, 275, 295, 305, 407,
481, 494]
Figure: [140] — [13, 15, 20, 21, 33, 35, 44, 45, 49, 50, 71–74, 77, 82, 88, 90–92, 94, 96, 102, 110, 116, 117, 120–124, 128, 138,
140, 142, 143, 145–147, 149, 150, 157, 163, 164, 167, 169, 177, 178, 183, 187, 188, 191, 207, 212, 216, 221, 223, 229, 230, 233,
239, 240, 246, 251, 253, 257, 264, 265, 268, 269, 272, 273, 279, 282, 284, 288, 289, 303, 306, 309, 310, 312, 313, 318, 319, 333–
335, 340, 343, 344, 346–348, 350, 351, 369–371, 377, 380, 383–385, 388, 389, 404, 406, 408, 410–412, 415, 416, 419, 420, 425,
433, 436, 438, 442, 447, 451, 452, 459–461, 465, 467, 468, 471, 473, 484–486, 489, 492, 496]
Figure: [112] — [10, 12, 58, 65, 91, 112, 166, 168, 188, 203, 204, 210, 249, 250, 261, 266, 300, 314, 317, 319, 320, 337, 338, 368,
370, 382, 418, 432, 435, 437, 445, 458, 476]

28

Category

Count

Citations

Section 4. Characteristics of Augmented Robots

Form Factors
→ Robotic Arms

→ Drones
→ Mobile Robots

→ Humanoid Robots

→ Vehicles
→ Actuated Objects

→ Combinations
→ Other Types

Relationship
→ 1 : 1

→ 1 : m

→ n : 1
→ n : m

Scale
→ Handheld-scale
→ Tabletop-scale

→ Body-scale

→ Building/City-scale

Proximity
→ Co-located

172

25
133

33

9
28

14
12

324

34

25
10

19
70

255

6

69

→ Co-located with Dis-
tance

251

→ Semi-Remote
→ Remote

19
66

Figure: [79] — [9, 12, 13, 20, 21, 24–27, 30, 33, 35, 39, 42, 44, 45, 47, 48, 51–53, 56, 59, 61, 63, 66, 67, 69, 72, 74, 77, 79, 80, 82–
85, 92, 94, 98, 101, 103–105, 110, 118–126, 128, 132, 133, 136, 138, 140, 143, 147, 150, 153, 160–162, 164, 169, 174, 181, 182, 185,
187, 190, 194–196, 203, 204, 207, 215, 216, 222, 225, 226, 231, 232, 248, 250–253, 255–257, 262, 264, 270, 274, 279, 282, 283, 285,
286, 288–290, 292, 297, 301, 309, 310, 312, 313, 319, 323–327, 329, 330, 336, 341, 347, 348, 352, 354, 355, 358, 372, 378, 379, 383–
385, 389, 394, 396–398, 400, 402, 409–411, 413, 420, 432, 433, 435, 442, 447, 448, 452, 454, 456, 457, 459–461, 470, 474, 477,
485, 487–489, 491, 493, 496]
Figure: [96] — [15, 38, 58, 76, 78, 96, 114, 157, 171, 177, 184, 209, 227, 261, 317, 332, 382, 388, 436, 450, 451, 475, 482, 492, 494]
Figure: [468] — [10, 18, 19, 29, 43, 49, 50, 52–55, 62, 64, 66, 71, 73, 88–91, 101, 102, 107–109, 112, 115, 117, 131, 135, 136, 142, 144–
146, 156, 163, 165, 169, 174, 177, 178, 180, 188, 189, 191, 197, 199–201, 205, 209–212, 221, 224, 225, 228–230, 233, 234, 237,
238, 241, 249, 263, 265–269, 272, 273, 275, 284, 296, 305, 306, 318, 327, 328, 333, 335, 340, 343, 344, 346, 350, 351, 357, 360–
363, 365, 368–371, 373, 375, 377, 380, 381, 391, 393, 404, 406–408, 412, 414–418, 425, 437, 445, 451, 464, 467–469, 471, 479–
481, 483, 484, 486]
Figure: [440] — [11, 14, 29, 46, 59, 93, 111, 134, 137, 139, 141, 149, 158, 166, 183, 213, 220, 254, 262, 271, 278, 295, 315, 342,
366, 367, 372, 376, 395, 439–441, 465]
Figure: [320] — [2, 7, 223, 300, 314, 320, 438, 458, 495]
Figure: [245] — [116, 117, 127, 130, 136, 159, 167, 168, 193, 209, 240, 242–246, 258, 287, 303, 334, 374, 387, 429–431, 443, 472,
473]
Figure: [98] — [29, 52, 53, 66, 98, 101, 117, 136, 169, 174, 177, 209, 225, 327]
Figure: [304] — [31, 65, 86, 136, 206, 239, 302, 304, 337, 338, 443, 476]

Figure: [153] — [9, 11, 12, 14, 15, 18–21, 24–27, 29, 30, 33, 35, 38, 39, 42–47, 49–56, 58, 59, 61–67, 69, 72–74, 76, 79, 80, 82–
86, 88, 89, 92–94, 96, 98, 102–105, 108–111, 114–128, 130, 132–141, 143, 144, 147, 149, 150, 153, 156–158, 160–162, 164, 165,
167, 169, 171, 174, 177, 180–185, 187–191, 193–197, 199, 201, 203–207, 209–211, 213, 215, 216, 220, 222–234, 237–239, 241–
245, 249–253, 255–258, 262, 264, 266–274, 278, 279, 282, 283, 285–290, 292, 295–297, 300, 302, 303, 305, 309, 310, 312–
315, 318, 319, 323–326, 329, 330, 332–337, 341, 342, 346–348, 350–352, 354, 355, 357, 358, 360–363, 365–368, 370–374, 376–
381, 383–385, 389, 393, 394, 396–398, 400, 402, 406, 407, 409, 411, 413, 414, 417, 419, 420, 429, 432, 433, 435, 437–443, 445,
447, 450, 452, 454, 456–460, 464, 465, 467–474, 476, 477, 479–489, 491–496]
Figure: [131] — [13, 48, 78, 90, 101, 107, 131, 142, 145, 163, 176–178, 212, 235, 240, 246, 248, 263, 265, 284, 301, 306, 327, 328,
340, 344, 387, 388, 391, 395, 416, 425, 448]
Figure: [430] — [10, 31, 77, 91, 112, 146, 159, 166, 168, 275, 317, 320, 338, 343, 345, 364, 369, 382, 404, 410, 418, 430, 436, 461, 475]
Figure: [328] — [71, 200, 221, 328, 375, 408, 412, 415, 431, 451]

Figure: [179] — [39, 46, 71, 86, 179, 237, 238, 258, 273, 296, 303, 306, 328, 351, 374, 412, 419, 425, 443]
Figure: [257] — [11, 14, 18, 19, 49, 50, 52, 55, 61, 72, 78, 83, 90, 116, 117, 127, 130, 136, 142, 146, 156, 157, 159, 163, 167, 178,
187, 193, 196, 201, 206, 212, 213, 221, 234, 239, 242–246, 257, 268, 275, 278, 284, 297, 327, 330, 333, 335, 340, 343, 348, 360,
362, 365, 371, 380, 407, 408, 416, 429, 445, 451, 476, 480, 484, 487, 489]
Figure: [289] — [9, 10, 12, 13, 20, 21, 24–27, 29–31, 33, 35, 38, 42, 44, 45, 47, 48, 51, 53, 56, 59, 63, 65, 67, 69, 73, 74, 76, 77, 79, 80,
82, 85, 88, 91–94, 96, 98, 101–105, 107–112, 115, 118–126, 128, 131–141, 143, 145, 147, 150, 153, 158, 160–166, 168, 169, 171,
174, 180–185, 189–191, 194, 195, 203–205, 207, 209–211, 215, 216, 220, 222, 224–232, 240, 248–253, 255, 256, 262–265, 269–
272, 274, 279, 282, 283, 285–290, 292, 300–302, 309, 310, 312, 313, 315, 319, 323–327, 329, 334, 336–338, 341, 342, 344, 346, 347,
350, 352, 354, 355, 357, 358, 369, 370, 372, 373, 375–379, 381, 383–385, 387–389, 391, 393–398, 400, 402, 404, 406, 409–411, 413–
415, 420, 430–433, 435, 439–442, 447, 448, 450, 452, 454, 456, 457, 459–461, 464, 465, 467–474, 477, 481, 482, 485, 488, 491–
493, 496]
Figure: [2] — Building/City-scale [2] — [2, 7, 233, 320, 458, 475]

Figure: [98] — [13, 31, 33, 72, 85, 86, 98, 104, 116, 119, 125, 127, 128, 140, 157, 159, 167, 178, 190, 193, 203–205, 222, 224, 227, 243–
246, 250, 258, 270, 289, 290, 300, 303, 314, 316, 324, 328, 340, 354, 355, 369, 374, 378, 379, 383, 394, 411, 416, 419, 420, 425, 429,
430, 432, 433, 435, 438, 443, 447, 451, 452, 461, 472, 473, 495]
Figure: [451] — [9–11, 14, 18, 19, 21, 24–27, 29, 30, 33, 35, 39, 42–45, 47–49, 51–53, 55, 56, 61, 63–67, 69, 71, 77–80, 82, 83, 88–
94, 96, 98, 102, 103, 105, 107–112, 115, 117, 118, 120, 121, 124, 126, 130–145, 147, 150, 153, 156, 158, 160–162, 164–166,
168, 171, 174, 177, 180–183, 185, 188, 189, 191, 194–197, 199–201, 206, 207, 209, 211, 213, 215, 220, 221, 223, 225, 226, 229–
232, 239–242, 248, 251–253, 255–257, 262, 264, 271–275, 278, 282, 283, 285, 288, 292, 295, 297, 301, 302, 305, 309, 310,
312, 315, 318, 320, 323–327, 329, 333, 335–338, 341, 346–348, 350–352, 357, 358, 360–363, 366–368, 370, 372, 373, 375–
377, 384, 385, 387, 389, 391, 393, 396–398, 400, 402, 404, 406–410, 412, 414, 415, 418, 431, 437, 439–443, 445, 448, 451, 454, 456–
460, 464, 465, 467–471, 474, 476, 477, 479–481, 485, 487, 488, 491, 493]
Figure: [114] — [15, 38, 58, 62, 76, 114, 184, 224, 227, 249, 317, 344, 381, 382, 436, 450, 475, 482, 494]
Figure: [377] — [4, 12, 46, 48, 50, 54, 59, 73, 74, 84, 101, 114, 122, 123, 145, 146, 149, 163, 169, 187, 209, 210, 212, 216, 228, 233,
234, 237, 238, 252, 263, 265–269, 279, 284, 286, 287, 296, 302, 306, 313, 319, 330, 332, 334, 342, 343, 365, 371, 377, 380, 388,
395, 404, 413, 417, 479, 483, 484, 486, 489, 492, 496]

29

Category

Count

Citations

Section 5. Purposes and Benefits of Visual Augmentation

Facilitate Programming

154

Support Real-time Control

126

Improve Safety

Communicate Intent

32

82

Increase Expressiveness

115

Figure: [33] — [9, 12, 18, 19, 24, 25, 27, 30, 33, 38, 42, 45, 47, 51–54, 61–63, 66, 67, 69, 72, 74, 76, 79, 80, 83, 84, 93, 94, 98, 101, 103–
105, 111, 115, 118, 120–124, 126, 130–133, 135–137, 140, 142–146, 150, 153, 160–163, 169, 174, 178, 181, 184, 188, 191, 195,
196, 199–201, 206, 207, 209, 211, 212, 215, 216, 220, 227, 228, 231, 232, 239, 242, 251–253, 262, 263, 265, 267, 270, 271, 273–
275, 279, 283–285, 288, 289, 295, 301, 303, 306, 312, 313, 315, 318, 319, 323–326, 329, 330, 333, 336, 341, 344, 352, 358, 366,
373, 379, 385, 387, 388, 397, 398, 400, 407, 408, 448, 454, 456, 457, 467, 468, 474, 487–489, 493, 494, 496]
Figure: [494] — [4, 13, 15, 19, 20, 25–27, 35, 39, 46, 50, 55, 58, 59, 73, 76–78, 82, 85, 86, 92, 102, 110, 114, 125, 133, 139, 146,
147, 149, 156, 157, 163, 164, 169, 171, 177, 183, 185, 187, 190, 191, 194, 203–205, 209, 212, 215, 221, 222, 224, 233, 234, 237,
238, 248, 250, 255, 256, 264, 266, 268, 269, 286, 287, 290, 296, 297, 309, 310, 314, 327, 332, 334, 342, 347, 348, 354, 355, 357, 365,
370, 371, 376, 378, 380, 381, 383, 384, 389, 394, 396, 402, 404, 407, 409–413, 417, 419, 420, 432, 435, 442, 447, 451, 452, 459–
461, 469, 470, 477, 480, 482–484, 486, 491, 494, 495]
Figure: [326] — [21, 43, 44, 56, 64, 66, 68, 88, 104, 138, 147, 160, 182, 188, 200, 225, 252, 282, 292, 326, 352, 354, 372, 379, 385,
397, 410, 420, 432, 450, 458, 495]
Figure: [372] — [21, 25, 27, 29, 33, 44, 51–54, 59, 64, 67, 69, 88, 89, 103, 104, 108, 118, 136–139, 141, 153, 160, 171, 188, 200,
211, 223, 224, 227, 241, 248, 251, 262, 267, 271, 292, 300, 305, 313, 318–320, 325, 326, 329, 342, 354, 358, 360–363, 366, 367,
372, 373, 375, 376, 387, 404, 410, 433, 439–441, 445, 450, 458, 464, 465, 473, 476, 479–481, 492, 494]
Figure: [158] — [3, 10, 11, 14, 31, 45, 48, 49, 55, 58, 65, 71, 82, 85, 90, 91, 93, 96, 107–109, 112, 116, 117, 119, 125–128, 134, 138,
142, 158, 159, 165–168, 171, 180, 183, 189, 193, 197, 198, 210, 213, 221, 229, 230, 240, 242–246, 257, 258, 261, 272, 273, 278, 300,
302, 317, 320, 328, 332, 335, 337, 338, 340, 343, 346, 350–352, 355, 368–370, 374, 377, 382, 391, 393, 395, 401, 406, 412, 414–
418, 425, 429–431, 436–438, 443, 445, 448, 451, 460, 467, 471–473, 475, 481, 485, 492]

Section 6. Classification of Presented Information

Internal Information

94

External Information

230

Plan and Activity

151

Supplemental Content

117

Figure: [66, 479] — [9, 13, 15, 18, 19, 21, 25, 30, 44, 45, 53–56, 63, 64, 66, 86, 98, 101, 104, 107, 115, 120, 124, 126, 131–133, 136–
138, 144, 145, 156, 158, 160, 181, 188, 195, 196, 199, 200, 209, 220, 223, 224, 228, 229, 231, 234, 251, 253, 262, 267, 271, 282, 283,
285, 292, 302, 310, 313, 315, 324, 329, 332, 334, 340, 352, 354, 355, 366, 367, 372, 373, 375, 379, 380, 385, 387, 410, 413, 416, 443,
450, 458, 474, 479, 481, 483, 484, 487, 488]
Figure: [21, 377] — [4, 10, 12, 15, 21, 24–26, 33, 35, 38, 39, 43–47, 50, 53, 54, 56, 59, 61–63, 66, 69, 71, 73, 74, 76, 77, 79, 80, 82,
84, 85, 89, 92, 94, 102, 103, 108, 110, 111, 114, 115, 119–123, 125, 131–133, 135, 136, 138–140, 143–147, 149, 150, 153, 156, 157,
161, 164–166, 169, 171, 174, 177, 178, 181, 182, 184, 185, 188, 190, 194, 196, 199–201, 205, 209, 211–213, 216, 220, 223, 224, 226–
234, 237, 238, 241, 246, 248, 251–253, 255, 256, 263–269, 274, 278, 279, 282, 283, 287–289, 292, 295–297, 301, 305, 306, 309,
310, 312, 314, 315, 318, 323, 325–327, 329, 330, 332–334, 336, 341–343, 347, 348, 352, 354, 355, 357, 358, 360–362, 367, 376–
380, 383, 384, 388, 389, 396–398, 400, 402, 404, 406, 407, 409–413, 415, 416, 420, 433, 435, 439–442, 445, 448, 451, 452, 454,
456, 459–461, 464, 465, 468, 471, 477, 479–481, 483–486, 488, 489, 491–493, 495, 496]
Figure: [76, 136] — [9, 12, 21, 25–27, 29, 30, 33, 35, 42–44, 51–54, 59, 62, 64, 66, 67, 69, 72–74, 76, 78, 82, 83, 88, 94, 103–
105, 108, 111, 118, 121–123, 126, 130, 131, 135–141, 146, 153, 156, 160, 162, 163, 169, 177, 187, 188, 191, 194, 196, 199,
200, 206, 209, 211, 212, 215, 220, 221, 223, 226–229, 248, 252, 255, 256, 262, 263, 267, 270, 271, 275, 284–288, 290, 292,
303, 305, 306, 312, 313, 315, 318–320, 325–327, 329, 333, 336, 341, 342, 344, 354, 358, 360–363, 365–367, 371–373, 379–
381, 387, 391, 400, 404, 407, 415, 416, 450, 457, 458, 467, 470, 472, 473, 476, 479–482, 488, 493–495]
Figure: [8, 386] — [8, 10, 11, 14, 20, 27, 31, 45, 48, 49, 55, 58, 65, 71, 77, 85, 90, 91, 93, 96, 107–109, 112, 116, 117, 119,
125, 127, 128, 134, 142, 158, 159, 165–168, 171, 180, 183, 189, 193, 197, 203, 204, 207, 210, 222, 225, 229, 240, 242–246, 249,
250, 255, 257, 258, 272, 273, 290, 300, 302, 317, 328, 335, 337, 338, 340, 346, 350–352, 367–370, 374, 377, 382, 385, 386, 393–
395, 401, 408, 412, 414–419, 425, 429–432, 436, 438, 443, 445, 447, 448, 451, 460, 472, 473, 475, 481, 485, 492]

30

Category

Count

Citations

Section 7. Design Components and Strategies

UIs and Widgets
→ Menus

115

→ Information Panels

80

→ Labels and Annotations

241

→ Controls and Handles
→ Monitors and Displays

24
47

Spatial References and
Visualizations
→ Points and Locations

208

→ Paths and Trajectories

154

→ Areas and Boundaries

178

→ Other Visualizations

91

Embedded Visual Effects

→ Anthropomorphic

→ Virtual Replica

22

144

→ Texture Mapping

32

Figure: [27, 58] — [9–12, 18, 19, 24, 25, 27, 45, 48, 51–53, 55, 58, 61–63, 65, 66, 71, 71, 79, 89, 92, 92, 94, 96, 101, 103, 105, 112,
116, 120, 126, 131, 136, 137, 140, 144, 149, 150, 153, 156, 159, 168, 174, 181, 188, 190, 190, 194, 194–196, 196, 199, 206, 207,
215, 224, 228, 231, 233, 234, 239, 242, 253, 253, 262, 263, 269, 271, 274, 275, 278, 278, 286, 289, 292, 292, 297, 301, 301, 302, 305,
325, 326, 328, 333, 336, 338, 344, 351, 351, 357, 360, 373, 375, 377, 380, 381, 385, 387, 397, 400, 407, 410, 412, 412, 416, 420, 420,
429, 437, 443, 445, 456, 459, 465, 469, 479, 484, 487, 488, 488, 496]
Figure: [15, 145] — [9, 15, 18, 44, 45, 48, 52–55, 59, 71, 86, 89, 92, 94, 96, 98, 104, 107, 112, 115, 120, 128, 136, 144, 145, 149,
156, 181, 185, 188, 190, 194–196, 199, 207, 223, 224, 228, 229, 232–234, 253, 262, 267, 278, 290, 292, 301, 315, 319, 329, 332,
336, 338, 344, 351, 355, 360, 373, 379–381, 412, 416, 420, 429, 437, 443, 451, 465, 470, 474, 479, 484, 488, 496]
Figure: [96, 492] — [9, 11–13, 15, 18–21, 24–27, 29, 31, 33, 35, 44, 47–54, 56, 59, 61–64, 66, 67, 69, 71–73, 76, 77, 79, 80, 82, 83, 85,
88, 92, 94, 96, 98, 101, 103–105, 110, 111, 115, 118–121, 124–126, 131–133, 135–141, 143–146, 150, 153, 156–158, 161, 164, 165,
168, 174, 177, 181, 184, 188, 190, 191, 194–196, 199–201, 203, 206, 209, 211–213, 220, 223, 225–228, 231–234, 237, 238, 241, 242,
246, 249–253, 256, 262–265, 267–271, 273–275, 278, 283–285, 288–290, 292, 301–303, 305, 306, 310, 312, 313, 315, 318, 323–
329, 333, 334, 336, 338, 341, 343, 347, 350, 351, 354, 358, 360–363, 366–368, 371, 373, 375–377, 380, 383, 384, 387–389, 396–
398, 400, 402, 406, 407, 410, 412, 413, 416, 420, 429, 430, 433, 435, 437, 439–442, 445, 447, 450, 451, 457, 459, 464, 465, 468,
472, 477, 479–485, 487–489, 491–496]
Figure: [169, 340] — [42, 46, 58, 83, 127, 130, 159, 163, 169, 193, 201, 205, 206, 212, 225, 238, 239, 328, 330, 340, 416, 443, 469, 472]
Figure: [171, 443] — [11, 39, 48, 54, 76, 96, 112, 131, 143, 168, 171, 190, 196, 203, 204, 213, 222, 224, 239, 253, 309, 315, 317,
332, 337, 338, 348, 355, 368, 380, 382, 391, 394, 402, 409, 418, 419, 431, 436, 443, 445, 447, 471, 472, 475, 476, 495]

Figure: [325, 358, 435, 451] — [9, 12, 13, 15, 18–21, 24–26, 26, 27, 29, 31, 33, 35, 35, 48, 50–54, 61, 61–63, 66, 67, 69, 71, 71,
73, 73, 74, 76, 77, 82, 83, 85, 88, 92, 94, 94, 96, 98, 101, 110, 110, 111, 118, 120, 120, 121, 121, 122, 122, 123, 123–126, 131–
133, 135–137, 139, 139–141, 143–146, 146, 150, 153, 156, 156–158, 161, 161, 163, 164, 174, 177, 181, 190, 190, 191, 194, 194,
196, 196, 199–201, 206, 212, 216, 220, 221, 226–228, 230, 231, 231–233, 233, 234, 237, 238, 241, 242, 246, 248, 248, 250–
253, 253, 256, 256, 262–264, 267, 268, 268, 271, 274, 278, 283, 283–285, 288, 292, 292, 301, 301–303, 303, 306, 306, 310, 312–
315, 318, 319, 323, 323–327, 327–330, 333, 333, 334, 341, 343, 344, 347, 350, 351, 354, 358, 360, 360–362, 362, 363, 371, 373, 375,
376, 376, 380, 380, 384, 387–389, 391, 396, 396, 397, 397, 398, 400, 402, 402, 407, 410, 410, 413, 416, 420, 425, 433, 435, 439–
442, 442, 445, 450, 451, 454, 456, 459, 464, 465, 468, 470, 472, 474, 477, 477, 480–485, 487, 487, 488, 488, 489, 491, 491–496]
Figure: [76, 136, 206, 450, 494] — [9, 21, 25–27, 30, 33, 35, 44, 51–54, 59, 61, 62, 64, 67, 69, 71, 73–76, 83, 88, 94, 103–105, 110,
118, 120–124, 130, 131, 136–140, 146, 149, 153, 156, 157, 160–163, 169, 177, 181, 188, 190, 191, 194, 196, 199, 201, 206, 209, 211,
212, 220, 221, 223, 226–229, 231–233, 241, 248, 251–253, 256, 262, 263, 267, 268, 270, 271, 283, 290, 292, 301–303, 305, 306,
312, 313, 315, 318, 320, 323, 325–327, 329, 333, 336, 342, 344, 354, 355, 358, 360–363, 366, 367, 372, 376, 379, 380, 387, 391, 396–
398, 400, 402, 407, 410, 415, 442, 450, 451, 454, 456, 458, 467, 470, 472, 473, 476, 477, 481, 482, 487, 488, 491, 492, 494, 495]
Figure: [21, 145, 182, 191] — [9, 15, 20, 21, 25, 26, 31, 35, 38, 39, 44, 47, 48, 51, 53, 56, 61–64, 68, 69, 71, 73, 74, 77, 82, 84, 89,
92, 101, 102, 105, 110, 115, 120–123, 126, 131–133, 136, 138–140, 142, 144–146, 149, 150, 153, 156, 161, 163, 164, 168, 171, 174,
177, 178, 182, 184, 188, 190, 191, 195, 196, 200, 201, 203, 206, 211, 212, 220, 221, 223, 227, 230, 231, 233, 237, 238, 242, 246,
248, 250, 252, 256, 263–269, 273, 274, 278, 282–284, 288, 289, 292, 301, 305, 306, 309, 312, 315, 318, 325–327, 332, 333, 340–
343, 346, 347, 350–352, 354, 358, 360, 362, 363, 366, 367, 376, 380, 384, 385, 388, 389, 396, 402, 406, 407, 410–413, 415–
417, 425, 432, 439, 440, 442, 445, 447, 451, 456, 459, 460, 465, 470, 471, 477, 483–485, 487–489, 491–493, 495, 496]
Figure: [177, 212, 282, 322] — [10, 14, 43, 45, 46, 49, 55, 58, 65, 66, 72, 78, 79, 90, 108, 114, 116, 124, 127, 128, 134, 147, 159,
165, 167, 176, 177, 180, 183, 187, 189, 193, 197, 204, 205, 210, 212, 215, 222, 224, 225, 239, 240, 243–245, 249, 257, 258, 272,
282, 286, 300, 317, 322, 330, 332, 335, 337, 338, 348, 368–370, 374, 377–379, 381–383, 394, 395, 404, 408, 411, 417–419, 429–
431, 436, 438, 443, 447, 452, 457, 461, 473, 475]

Figure: [3, 158, 180, 197, 242, 473] — [3, 14, 23, 51, 107, 108, 158, 165, 180, 189, 197, 198, 242, 320, 395, 401, 414, 436, 450, 472,
473, 481]
Figure: [4, 114, 163, 209, 372, 451] — [4, 12, 21, 25, 27, 30, 33, 43, 45, 47, 51–53, 59, 61, 66, 67, 69, 73, 74, 76, 80, 82–85, 92, 101–
104, 110, 111, 114, 118, 120–124, 126, 130, 131, 133, 134, 137, 138, 143, 144, 146, 147, 153, 156, 160–163, 169, 174, 181, 183, 185,
194, 195, 201, 206, 209, 211, 212, 216, 225–227, 231, 249, 251–253, 262, 265, 267, 270, 271, 273, 283, 285, 287, 290, 292, 296,
297, 301, 302, 306, 312, 313, 318, 323–327, 329, 333, 336, 341, 342, 344, 352, 354, 358, 360, 365, 372, 376, 380, 381, 388, 396–
398, 400, 410, 413, 417, 437, 438, 442, 451, 454, 456, 457, 459, 460, 470, 472, 474, 476, 477, 487–489, 494, 496]
Figure: [245, 308, 328, 374, 425, 429] — [20, 39, 116, 119, 127, 159, 175, 193, 222, 243–245, 250, 258, 308, 328, 340, 348, 369,
370, 374, 378, 383, 411, 425, 429, 431, 432, 447, 452, 461, 493]

31

Category

Count

Citations

Section 8. Interactions

Interactivity
→ Only Output

→ Implicit
→ Indirect

→ Direct

Interaction Modalities
→ Tangible

→ Touch

→ Controller

→ Gesture

→ Gaze
→ Voice
→ Proximity

27

11
295

72

68

66

136

116

14
18
21

Figure: [450] — [39, 56, 64, 89, 107, 117, 134, 145, 158, 180, 197, 223, 240, 261, 282, 314, 332, 363, 372, 382, 418, 433, 436, 438,
450, 464, 481]
Figure: [458] — [38, 44, 88, 249, 258, 292, 357, 410, 414, 458, 493]
Figure: [346] — [9–12, 14, 15, 18, 19, 21, 24–27, 29–31, 33, 35, 42, 43, 45, 47–55, 58, 59, 61–63, 66, 67, 69, 71–74, 76–80, 82–
86, 90–94, 96, 98, 101–103, 105, 108–112, 114, 115, 118, 120–124, 126, 130–133, 135–144, 146, 147, 149, 150, 153, 156, 158, 160–
166, 168, 169, 171, 174, 177, 181–185, 188, 189, 191, 194–196, 199–201, 206, 207, 209–211, 213, 215, 216, 220, 221, 226–234, 237–
239, 241–243, 248, 251–253, 255–257, 262–269, 271–275, 278, 279, 283–288, 295–297, 301–303, 305, 306, 309, 310, 312, 313,
315, 317–320, 323, 325–327, 329, 330, 332–338, 341–344, 346, 347, 350–352, 358, 360–362, 365–368, 370, 372, 373, 375–
377, 380, 382, 384, 385, 387–389, 391, 393, 395–397, 400, 402, 404, 406–409, 412, 413, 415, 417, 418, 431, 436, 437, 439–
443, 445, 448, 454, 456, 459–461, 465, 467–471, 474–477, 479–489, 491, 492, 494–496]
Figure: [316] — [13, 20, 30, 31, 33, 46, 49, 65, 98, 104, 116, 119, 125, 127, 128, 140, 157, 159, 163, 167, 178, 187, 190, 193, 203–
205, 212, 222, 224, 225, 243–246, 250, 270, 289, 290, 300, 316, 324, 328, 340, 348, 354, 355, 369, 371, 374, 378, 379, 381, 383,
394, 398, 411, 416, 419, 420, 425, 429, 430, 432, 435, 443, 447, 451, 452, 457, 472, 473]

Figure: [163] — [13, 21, 31, 33, 49, 72, 82, 85, 86, 94, 98, 104, 116, 117, 124, 125, 127, 128, 138, 140, 143, 144, 159, 163, 167, 178,
190, 193, 229, 243–246, 251, 258, 270, 275, 289, 290, 303, 313, 323, 324, 328, 329, 337, 338, 340, 354, 355, 369, 374, 379, 398,
409, 414, 416, 419, 420, 425, 429, 435, 443, 448, 451, 472, 473, 489]
Figure: [169] — [9, 10, 18, 19, 53, 55, 61–63, 65, 72, 76, 92, 101, 103, 130–133, 135, 136, 140, 144, 149, 153, 159, 163, 169, 195,
196, 201, 207, 209, 212, 228, 230, 231, 239, 243, 257, 263, 285, 288, 295, 300, 302, 305, 333, 346, 350, 373, 377, 382, 385, 404, 407,
416, 430, 445, 456, 459, 461, 479–481, 487]
Figure: [191] — [12, 15, 20, 31, 35, 39, 42, 45, 46, 48–50, 54, 71, 73, 74, 77, 83, 84, 90, 96, 101, 102, 105, 110, 115, 119–
123, 142, 146, 147, 149, 150, 156, 157, 160, 164, 171, 177, 185, 187, 188, 191, 203–205, 210, 215, 220–222, 224, 227, 228, 233,
234, 250, 252, 253, 256, 264, 266–269, 275, 279, 284, 286, 306, 309, 310, 312, 313, 315, 317–319, 330, 332–335, 338, 341, 343,
344, 347, 348, 351, 365, 370–372, 378, 380, 381, 383, 384, 388, 389, 391, 394, 395, 402, 411, 413, 415, 417, 418, 432, 433, 436,
442, 443, 447, 451, 452, 457, 460, 467, 468, 470, 475, 476, 483–486, 491–493, 495]
Figure: [58] — [3, 11, 24–27, 30, 33, 43, 47, 48, 51, 52, 58, 59, 66, 69, 78, 80, 91, 93, 96, 98, 103, 105, 109, 111, 112, 114, 118, 126,
136, 137, 139, 141, 156, 158, 161, 162, 165, 166, 168, 174, 181–183, 189, 194, 199, 200, 206, 211, 216, 225, 226, 231, 232, 237, 238,
242, 255, 257, 262, 270–274, 278, 285, 287, 296, 297, 301, 302, 320, 325–327, 329, 332, 336, 337, 341, 342, 352, 357, 358, 360,
362, 366, 368, 372, 375, 376, 387, 393, 396, 397, 400, 401, 410, 412, 431, 437, 439–441, 454, 465, 469, 474, 477, 488, 494, 496]
Figure: [482] — [27, 28, 33, 38, 67, 114, 262, 300, 320, 325, 336, 358, 362, 482]
Figure: [184] — [14, 27, 54, 67, 108, 156, 182, 184, 197, 213, 239, 336, 354, 362, 367, 396, 404, 406]
Figure: [200] — [14, 44, 88, 138, 182, 200, 229, 241, 265, 283, 292, 305, 350, 361, 368, 430, 431, 437, 458, 467, 471]

32

Category

Count

Citations

Section 9. Application Domains

Domestic and Everyday
Use

35

Industry

166

Entertainment

32

Education and Training

22

Social Interaction

Design and Creative Tasks

Medical and Health

Remote Collaboration

Mobility and Transporta-
tion

Search and Rescue

Workspace

Data Physicalization

21

11

36

6

13

35

9

12

Section 10. Evaluation Strategies

Demonstration

Technical

97

83

User Evaluation

122

Figure: [263] — Household Task authoring home automation [53, 111, 135, 163, 177, 191, 212, 228, 263, 327, 365, 387, 480],
item movement and delivery [76, 108, 209, 265], displaying cartoon-art while sweeping [481], multi-purpose table [430]
Photography drone photography [114, 171], Advertisement mid-air advertisement [317, 382, 418, 475] Wearable Interactive
Devices haptic interaction [443], fog screens [418], head-worn projector for sharable AR scenes [168] Assistance and
companionship elder care [65], personal assistant [239, 367, 368] Tour and exhibition guide tour guide [58, 295], museum
exhibition guide [168, 368], guiding crowds [475], indoor building guide [89], museum interactive display [112]
Figure: [21] — Manufacturing and Assembly joint assembly and manufacturing [21, 30, 43, 44, 80, 138–140, 161, 194, 231, 274,
283, 289, 297, 327, 366, 376, 396, 397, 410, 435, 454, 456, 493], grasping and manipulation [24–27, 33, 59, 67, 79, 132, 133, 137,
150, 153, 169, 185, 216, 226, 248, 255, 336, 342, 358, 379, 402], joint warehouse management [200], taping [105], tutorial and
simulation [52], welding [303, 313, 413] Maintenance maintenance of robots [144, 162, 252, 253, 285, 292, 302, 375], remote
collaborative repair [31, 48, 74, 477], performance evaluation and monitoring [98, 103, 126, 128, 492], setup and calibration [61,
94, 120–123, 301, 306, 319, 323, 333, 352, 398, 487, 488], debugging [373] Safety and Inspection nuclear detection [15, 234],
drone monitoring [76, 114, 171, 184, 227, 332, 482, 494], safety feature [21, 44, 56, 66, 104, 160, 182, 282, 292, 372, 379, 385,
450], cartoon-art warning [481], collaborative monitoring [363], ground monitoring [50, 73, 146, 211, 269, 380, 479, 484]
Automation and Teleoperation interactive programming interface [9, 12, 45, 47, 51, 62, 63, 66, 69, 93, 101, 104, 118, 124, 130,
131, 136, 143, 174, 177, 184, 188, 195, 201, 206, 207, 211, 212, 220, 227, 232, 251, 262, 270, 271, 284, 287, 288, 312, 315, 318, 324–
326, 329, 385, 400, 407, 468, 474, 496] Logistics package delivery [265] Aerospace surface exploration [54], teleoperated
manipulator [310], spacecraft maintenance [470]
Figure: [350] — Games interactive treasure protection game [350], pong-like game [346, 370], labyrinth game [258], tangible
game [49, 178, 230, 273], educational game [412], air hockey [91, 451], tank battle [90, 221], adventure game [55], role-
play game [229], checker [242], domino [246], ball target throwing game [337], multiplayer game [115, 404], virtual
playground [272] Storytelling immersive storytelling [328, 369, 395, 415, 467] Enhanced Display immersive gaming and
digital media [431] Music animated piano key press [472, 473], tangible tabletop music mixer [340] Festivals festival
greetings [382] Aquarium robotic and virtual fish [240]
Figure: [445] — Remote Teaching remote live instruction [445] Training military training for working with robot team-
mates [199, 360, 362], piano instruction [472, 473], robotic environment setup [142], robot assembly guide [18], driving
review [11, 213], posture analysis and correction [183, 437] Tangible Learning group activity [71, 275, 328, 337, 408, 412, 467],
programming education [278, 416], educational tool [351]
Figure [108] — Human-Robot Social Interaction reaction to human behaviors [93, 108, 109, 375, 406, 414], cartoon-art
expression [377, 481], human-like robot head [14], co-eating [134], teamwork [404], robots with virtual human-like body
parts [158, 165, 180], trust building [141], task assignment by robot [439–441, 465] Robot-Assisted Social Interaction projected
text message conversations [382] Inter-Robot Interaction human-like robot interaction [107]
Figure: [476] — Fabrication augmented 3D printer [476],interactive 3D modelling [341], augmented laser cutter [304], design
simulation [52] and evaluation [393] Design Tools circuit design guide [445], robotic debugging interface [145], design and
annotation tool [96], augmenting physical 3D objects [210] Theatre children’s play [10, 166]
Figure: [354] — Medical Assistance robotic-assisted surgery [13, 35, 77, 82, 84, 85, 92, 110, 125, 147, 164, 181, 190, 256, 264, 290,
309, 347, 354, 355, 384, 389, 409, 419, 420, 442, 459, 460, 485, 491], doctors doing hospital rounds [228], denture preparation
rounds [196] Accessibility robotic prostheses [86, 136] Rehabilitation autism rehabilitation [29], walking support [334]
Figure: [242] — Remote Physical Synchronization physical manipulation by virtual avatar [242] Avatar enhancement life-sized
avatar [197], floating avatar [436, 475], life-sized avatar and surrounding objects [189] Human-like embodiment traffic
police [149]
Figure: [7] — Human-vehicle interaction projected guidance [2, 7], interaction with pedestrians [64, 88, 320], display for
passengers [223] Augmented Wheelchair projecting intentions [458], displaying virtual hands to convey intentions [300],
self-navigating wheelchair [314], assistive features [495] Navigation tangible 3D map [258], automobile navigation [438]
Figure: [363] — Ground search collaborative ground search [296, 343, 360, 362, 363], target detection and notification [211,
241, 266, 269, 305, 361, 468, 479], teleoperated ground search [54, 73, 102, 146, 156, 234, 237, 238, 267, 268, 365, 380, 417, 469,
483, 484, 486] Aerial search drone-assisted search and rescue [114, 332, 482], target detection and highlight [464] Marine
search teleoperated underwater search [233]
Figure: [159] — Adaptive Workspaces individual and collaborative workspace transformation [159, 430, 431] Supporting
Workers reducing workload for industrial robot programmers [407], mobile presentation [168, 257, 338], multitasking with
reduced head turns [38], virtual object manipulation [357]
Physical Data Encoding physical bar charts [167, 429], embedded physical 3D bar charts [425] Scientific Physicalization
mathematical visualization [127, 243], terrain visualization [116, 117, 243–245, 308], medical data visualization [243] ,
Physicalizing Digital Content handheld shape-changing display [258, 374]

Workshop [44, 45, 167, 244, 246, 274, 476], Theoretical Framework [200, 374], Example Applications [30, 47, 66, 84, 124, 127, 142,
145, 159, 174, 195, 197, 209, 210, 243, 245, 252, 265, 269, 315, 325, 341, 342, 366, 368, 382, 385, 418, 425, 431, 476, 481, 483, 496],
Focus Groups [272, 467], Early Demonstrations [24, 49, 53, 55, 82, 89, 96, 102, 104, 111, 115, 117, 136, 149, 162, 199, 216, 221, 239–
241, 251, 255, 257, 270, 290, 305, 338, 346, 370, 375, 387, 400, 404, 408, 414, 436, 451, 479], Case Studies [65, 105, 189, 288, 324,
329], Conceptual [9, 220, 230, 271, 367, 472, 492]
Time [21, 48, 51, 59, 67, 69, 112, 126, 165, 171, 227, 284, 318, 358, 406, 417, 464, 486, 495], Accuracy [14, 15, 29, 58, 59,
64, 76, 84, 90, 101, 112, 124, 131, 150, 153, 165, 171, 181, 184, 226, 229, 232, 312, 317, 318, 320, 460, 464, 482, 486], System
Performance [38, 59, 62, 63, 79, 91, 93, 118, 125, 128, 130–132, 143, 168, 211, 282, 285, 302, 313, 319, 332, 334–336, 352, 355,
361, 363, 406, 468, 470], Success Rate [262, 314]
NASA TLX [15, 27, 33, 43, 63, 67, 69, 76, 112, 114, 125, 133, 160, 201, 226, 340, 358, 372, 377, 407], Interviews [64, 103, 114, 140,
158, 191, 369, 435, 443], Questionnaires [11, 18, 25, 31, 48, 59, 98, 132, 134, 138, 150, 160, 169, 182, 183, 193, 223, 242, 267, 275,
300, 350, 373, 377, 393, 416, 417, 430, 445, 464, 495], Lab Study with Users [12, 56, 169, 188, 328, 379, 415, 473], Qualitative
Study [13, 21, 25, 52, 54, 138, 158, 163, 165, 263, 266, 289, 357, 429, 437, 438, 443, 494], Quantitative Study [11, 13, 19, 25,
38, 43, 54, 76, 85, 86, 107, 108, 111, 138, 140, 141, 165, 171, 180, 184, 193, 266, 289, 328, 357, 439, 448, 494], Lab Study with
Experts [10, 116, 340], Observational [58, 135, 258, 369, 382], User Feedback [135, 448]

33

