Chaos Engineering For Understanding Consensus Algorithms Performance in
Permissioned Blockchains

Shiv Sondhi, Sherif Saad and Kevin Shi
University of Windsor
{sondhis,shsaad, shi12z}@uwindsor.ca

Mohammad Mamun
National Research Council of Canada
mohammad.mamun@nrc-cnrc.gc.ca

Issa Traore
University of Victoria
itraore@uvic.ca

1
2
0
2

g
u
A
9
1

]

C
D
.
s
c
[

1
v
1
4
4
8
0
.
8
0
1
2
:
v
i
X
r
a

Abstract—A critical component of any blockchain or dis-
tributed ledger technology (DLT) platform is the consensus
algorithm. Blockchain consensus algorithms are the primary
vehicle for the nodes within a blockchain network to reach
an agreement. In recent years, many blockchain consensus
algorithms have been proposed mainly for private and per-
missioned blockchain networks. However, the performance of
these algorithms and their reliability in hostile environments
or the presence of byzantine and other network failures are
not well understood. In addition, the testing and validation of
blockchain applications come with many technical challenges.
In this paper, we apply chaos engineering and testing to under-
stand the performance of consensus algorithms in the presence
of different loads, byzantine failure and other communication
failure scenarios. We apply chaos engineering to evaluate the
performance of three different consensus algorithms (PBFT,
Clique, Raft) and their respective blockchain platforms. We
measure the blockchain network’s throughput, latency, and
success rate while executing chaos and load tests. We develop
lightweight blockchain applications to execute our test in a
semi-production environment. Our results show that using
chaos engineering helps understand how different consensus
algorithms perform in a hostile or unreliable environment and
the limitations of blockchain platforms. Our work demon-
strates the beneﬁts of using chaos engineering in testing
complex distributed systems such as blockchain networks.

1. Introduction

A blockchain application is a distributed application run-
ning over a decentralized P2P network. The methods used
to store and validate the blockchain network data enable
data integrity, accountability, conﬁdentiality, availability, and
transparency. Blockchain introduced a new approach to
creating an immutable distributed ledger for storing data.
This ledger could also be tamper-resistant depending on
the consensus algorithm used, as in the case of bitcoin and
proof-of-work (PoW).

The two terms, blockchain and distributed ledger tech-
nology (DLT), are often used interchangably. Blockchain is
a type of DLT where transactions are recorded with an im-
mutable cryptographic signature called a hash. Blockchain
networks store data in logical blocks that are chained to-

gether using these hashes. The data model is like a dis-
tributed linked list, where each block in the chain stores
the address of the previous block, using a secure hashing
approach. It is computationally expensive (almost impossi-
ble) to modify the block’s data without being detected. The
main difference between blockchain and DLT is the data
model i.e. the structure used to capture the ledger’s state. In
a blockchain, the ledger is simply a linked list or a chain of
blocks, but DLT could use any other data structure, such as
a directed acyclic graph (DAG). Any DLT is based on three
components: a data model (e.g. linked-list, directed-acyclic
graph), a transaction language, and a consensus protocol.

The choice of consensus algorithm used in a given
blockchain application signiﬁcantly impacts the applica-
tion’s performance. In recent years, many consensus al-
gorithms have been proposed for both public and private
blockchain networks. Studying and comparing the perfor-
mance of these consensus algorithms is very important. In
this paper, we share our experience with analyzing the per-
formance of consensus algorithms used in private blockchain
networks.

In private or permissioned blockchain networks,
the
nodes and users are authorized before joining the network.
These networks are suitable for applications in ﬁnance,
logistics, healthcare, and other critical sectors. Several works
in the literature have employed empirical analyses and mod-
elling techniques to study the performance of consensus
algorithms in blockchain networks. However, the perfor-
mance of these algorithms, and their reliability in hostile
environments, or in the presence of byzantine and other
network failures is not well studied. In our work, we apply
chaos engineering and testing to study the performance of
consensus algorithms in permissioned blockchain networks.
The term chaos engineering was coined in 2014 by Bruce
Wong at Netﬂix, while the practice of chaos engineering
started in 2010 [1], [2], [3]. In chaos engineering, we
intentionally trigger faults into systems in production, to
observe their behaviours in a faulty environment. This will
help in implementing fault tolerance strategies that reduce
downtime while increasing resiliency. The primary motiva-
tion for this approach is to overcome uncertainties prevalent
in complex computer systems.

We

three different
the performance of
blockchain platforms and consensus algorithms, by design-

analyze

 
 
 
 
 
 
ing a lightweight blockchain application and implementing
them separately on each platform. Then, using chaos testing
principles, we perform load and chaos tests by introducing
different user loads and network faults. We study the perfor-
mance of the selected algorithms and platforms, under faulty
conditions including network delay, packet loss, message
corruption, crash failure, and Byzantine failure. To the best
of our knowledge, our work is the ﬁrst attempt to apply
chaos engineering to study the performance and behaviours
of consensus algorithms in blockchain networks.

The rest of this paper is structured as follows. Section 2
summarizes and discusses related works. In Section 3, we
introduce the consensus algorithms selected for analysis, and
the metrics used to analyze them. Then, in Section 4, we
discuss our chaos testing approach, the blockchain applica-
tion we designed for the test, and the blockchain platforms
we used to deploy and test the application. In Section 5,
we report the test results and discuss our observations and
ﬁndings. Finally, Section 6 makes some concluding remarks
and discusses our future work.

2. Related Work

Several works in the literature analyze the performance
of blockchain consensus algorithms, using empirical analy-
sis. Here, we brieﬂy discuss some of the most recent work
in the literature.

In [4], Ampel et al. used a performance benchmarking
tool called Hyperledger Caliper [5], to measure the per-
formance characteristics of a Hyperledger Sawtooth appli-
cation. This application uses the Raft consensus protocol
and the authors computed metrics like throughput, latency,
success rate, and node resource utilization. Throughput is
the number of transactions committed to the blockchain per
unit time. Latency is the amount of time it takes for a
transaction to appear on the blockchain since when it was
created. Success Rate is the ratio of successfully committed
blocks, to the total number of blocks created (including
invalid blocks). These metrics are plotted against batch size
(transactions per block) and the input workload. Noteworthy
ﬁndings indicate that throughput increases linearly, and la-
tency increases exponentially with batch size. Latency also
increases exponentially with an increasing workload, while
memory and CPU usage increase as well.

In [6], Hao et al. compared the performance of
Ethereum’s Proof-of-Work (PoW) against Hyperledger Fab-
ric’s Practical Byzantine Fault Tolerance (PBFT). Average
throughput and latency were once again used as comparison
metrics. The results indicate that PBFT is better than PoW in
terms of both metrics. For smaller input workloads (around
100 transactions per second), PBFT was only slightly better
than PoW, but as the workload increased, PBFT’s perfor-
mance grew far better than that of PoW. This is an indication
of the poor scalability of the lottery-based PoW consensus.
In [7], Angelis et al. studied Aura and Clique - two vari-
ants of the Proof-of-Authority class of consensus algorithms;
and classical PBFT, using the CAP (Consistency, Avail-
ability, Partition tolerance) theorem principles. The CAP

theorem states that a distributed system cannot achieve con-
sistency and availability when the network is partitioned in
a way that messages may be arbitrarily lost. In a blockchain
network, consistency refers to all nodes having the same
blockchain copy, and availability refers to the network’s
ability to accept new transactions. Through a qualitative
analysis, the authors showed that Aura and Clique tend to
prefer availability while PBFT prefers consistency.

Ahmad et al. compared ﬁve different algorithms [8] -
PoW, PBFT, Proof-of-Stake (PoS), Clique and Proof-of-
Elapsed Time (PoET) - on the basis of throughput and
latency. The metrics were also measured when the number of
network nodes was varied. They found that Clique and PoS
experienced the minimum latency, followed by PoET, PoW,
and PBFT. In terms of throughput they found that with up
to 50 network nodes, Clique achieved the best throughput,
followed by PoET and PoS. However, when the number of
nodes increased beyond 50, Clique’s throughput degraded.
PBFT always had a very low throughput.

In [9] Papadis et al. used modelling techniques to ana-
lyze block generation statistics of a blockchain system. They
compared the results using a blockchain application and a
simulated model. They also analyzed the impact of stochas-
tic components on the probability of attacks on the network.
They used an Ethereum testbed for building the application.
The authors found that the probability of a successful attack
increases with increasing delay, and decreases with a higher
number of transaction conﬁrmations.

All the work discussed above used empirical analysis.
The works in [4] and [9] measured the performance of
one protocol. References [6] and [8] studied two and ﬁve
protocols, respectively. While [4], [6] and [8] used simi-
lar metrics, including throughput and latency, [9] analysed
block generation statistics. [4] and [6] conducted load tests
although both used different methodologies. The former
computed the metrics while varying the input transactions
per second, while the latter varied the total number of
transactions sent by a client to the server. In our experiments
load is generated on the basis of the number of concurrent
users interacting with the system. The experiments in [8]
and [9] vary the number of validator nodes on the network.
The former goes over a larger range i.e. 50 to 250, whereas
the latter tests only a 2-node and a 5-node scenario. In
[7], Angelis et al. conducted a qualitative analysis of three
protocols using their algorithms and the CAP theorem.

Using the CAP theorem gives a different perspective on
the characteristics of a protocol, however, this is not enough
alone. The authors of [7] suggested that their analysis can be
backed up by implementing the scenarios described in their
paper, and computing various metrics including throughput,
latency and scalability. Eventually, the CAP theorem can
be used as a framework to analyze protocols, but metrics
like throughput and latency are important to validate the
model. The most common metrics used in previous works
to measure the performance of blockchain consensus algo-
rithms consist of throughput and latency. However, these two
metrics alone, are not good enough performance indicators
because these two metrics do not tell anything about the

2

TABLE 1. COMPARING THIS WORK TO THE EXISTING LITERATURE

Paper
[4]
[6]
[7]
[8]
[9]
This work
1 Throughput;

Protocol Families
Paxos
PoW, BFT
BFT, PoA
PoW, BFT, PoS, PoA, PoET
Ethereum (i.e. PoW or PoA)
BFT, PoA, Paxos

Performance Metrics
TP1, L2, SR 3, RU 4
TP, L
CAP
TP, L
Block generation statistics
TP, L, SR

Load / Chaos Testing
Load
Load
None
Load
Load
Both

2 Latency;

3 Success Rate;

4 Node Resource Utilization

consistency of local chains nor do they say anything about
the number of invalid, or rejected blocks.

Therefore, in our study we use throughput, latency, and
success rate to compare the selected consensus algorithms.
The success rate is taken as a ratio between the number
of accepted blocks and the total number of blocks created
(including the ones that were rejected). In addition, we
consider two secondary metrics i.e. load tolerance and fault
tolerance. Table 1 highlights the difference between our
proposed test strategy and the existing strategies in the
literature.

3. Consensus Algorithms and Performance
Metrics

There are close to a hundred consensus protocols used
in blockchain and distributed ledger systems today [10].
However, there is no single best protocol - the choice de-
pends on network structure, topology, desired conﬁrmation
times, security and other factors. We focus on permissioned
consensus algorithms and platforms suitable for healthcare,
logistics, ﬁnance and other sectors that deal with sensitive
and private users’ information. The taxonomy from [11] was
used to choose consensus algorithms which were consider-
ably different from each other. Their structural and perfor-
mance properties are most relevant here and are discussed
below.

3.1. Consensus Algorithms Selection

Structural properties of consensus algorithms can be

divided further into the following subcategories:

1) Node type - depending on the platform, a consen-
sus algorithm may deal with multiple node types
like full nodes (that store the entire blockchain
locally), validator nodes, endorsers (which only
validate transactions) and light clients (which verify
new blocks without storing the entire blockchain
locally).
Structure type – Consensus protocols can use sin-
gle or multiple committees to reach consensus i.e. a
single group of validators generates each next block
(as in PBFT, Tendermint and Clique), or multiple
committees work independently. Both types can
be static or dynamically changing. Furthermore, a

2)

single committee may be open or closed to new
members, and can have implicit or explicit forma-
tion rules. Multiple committee mechanisms must
have an overall topology (i.e. ﬂat or hierarchical).
Raft normally follows a single committee structure,
but when the network is partitioned, this splits into
multiple ﬂat committees. If any partition contains
more than two-third of the participating nodes, it
becomes the main committee and the others must
follow its decisions (hierarchical topology).
3) Underlying mechanism – This refers to the core
method of reaching consensus and can roughly
be classiﬁed as either a lottery-based (proof-of-
work), vote-based (BFT-based protocols) or coin-
age-based mechanism.

The consensus protocols selected for this research -
PBFT [12], Clique [13] and Raft [14] - belong to the
byzaninte fault-tolerant (BFT), proof-of-authority (PoA),
and Paxos-based protocol families, respectively. BFT-based
protocols are always byzantine fault-tolerant. This means
that they reach a consensus even when a portion of the
network’s nodes send contradicting messages to different
peers. Usually, BFT-based protocols follow multiple rounds
of voting to achieve consensus - like PBFT - but this is
not necessary. Many BFT-based protocols simply suggest
improvements over PBFT,
like reducing the number of
voting rounds, etc. PoA protocols are a popular class of non-
incentivized protocols that store proof of each validator’s
identity to monitor and limit malicious activity. While PoA
protocols are also byzantine fault-tolerant, they can reach
better performance than BFT-based protocols due to lighter
message exchanges. PoA protocols are best suited to sce-
narios where the validator set can be trusted, as is the case
with Ethereum’s Rinkeby, G¨orli and Kovan testnets.

Finally, Paxos-based protocols provide improvements
over the Paxos protocol proposed by Lamport
in 1989.
Raft (like Paxos) is not byzantine fault-tolerant but crash
fault-tolerant. It is sometimes also referred to as a Proof-of-
Capacity protocol [10]. Table 2 summarizes the properties
of the selected consensus protocols.

3.2. Selected Performance Metrics

Performance metrics are a way to quantify a system’s
performance. The performance properties of consensus pro-
latency, fault
tocols deﬁned in [11] include throughput,

3

TABLE 2. COMPARING THE SELECTED CONSENSUS PROTOCOLS

Protocol

Family

Platform

Fault Tolerance

Structure

Underlying Mechanism

PBFT

Clique

Raft

BFT-based

Hyperledger Sawtooth

PoA-based

Ethereum’s Rinkeby testnet

Paxos-based

Hyperledger Fabric

BFT

BFT

CFT

Single Committee

Single Committee

Vote-based

Leader-follower

Single / Multiple Committee

Vote-based

tolerance, and energy consumption. For our experiments, we
selected three metrics (primary metrics) to directly measure
aspects of the system; like throughput, latency and success
rate. However, these metrics alone are not sufﬁcient as they
don’t paint a holistic picture of system performance. We
employ chaos engineering techniques to study the effect of
input trafﬁc, failures, and combinations of failures on the
blockchain system. Our focus is on input load and network
failures. Therefore, we measure changes in the three primary
metrics while changing the input workload, and adding
network faults. The primary metrics we use to measure
the performance of blockchain consensus algorithms and
applications under different chaos conditions are:

• Write Throughput - The number of transactions

added to the blockchain per second.

T P =

(total transactions added to chain)
(total runtime)

• Average Write Latency - The amount of time it
takes for a transaction to appear on the blockchain,
from when it was made. We are concerned with the
average over all transactions. In the equation below,

L =

(cid:80)T Xtot

tx=1 (TtxCommitted − TtxCreated)
T Xtot

Where T Xtot is the total number of transactions,
TtxCommitted is the timestamp when a given trans-
action is committed, and TtxCreated is the timestamp
when a given transaction is created (e.g. made by the
user)
Success Rate - The ratio of the number of blocks
successfully added to the blockchain to the total
number of blocks created (includes invalid blocks).

•

SR =

(total successf ully added blocks)
(total blocks created)

4. Chaos Engineering for Blockchain

The majority of the work in the literature studied the
performance of consensus algorithms under normal opera-
tional scenarios or in a fault-free environment. These studies
are essential to understand how these algorithms behave
and help to optimize and improve consensus protocols for
blockchain networks. However, it is unrealistic to assume
that blockchain applications and networks will continuously
operate in a fault-free environment. It is critical to observe
the performance of consensus algorithms and blockchain
applications in faulty production environments. This can be
done using chaos engineering principles. Chaos engineering

is a new system quality assurance practice that focuses on
continuously testing complex distributed systems in produc-
tion environments with stochastic faulty scenarios.

Any blockchain system can be divided into four abstract
layers. These layers are the data model layer, consensus
layer, execution layer, and application layer [6], [15]. The
data-model layer deﬁnes what data goes into a block -
the data-structures and types. The consensus layer deals
with ﬁnding consensus on the network and creating new
blocks. The execution layer includes details of the runtime
environment, which is used to execute smart contracts1.
The application layer is the topmost layer and represents
decentralized applications (Dapps) that use smart contracts
and the blockchain to accomplish some business logic.

Consensus protocols are at the heart of the consensus
layer. They are a well-deﬁned instruction set that ensures
all network nodes agree on the blockchain state (data-model
layer). Therefore, the consensus and data-model layers are
tightly knit. Changes in the network or data-model layers,
like network delays, faulty nodes, corrupted messages, and
block size, can affect the network’s ability to reach consen-
sus.

We designed several chaos testing scenarios to test the
blockchain consensus algorithms and applications. We ex-
perimented with several users and transaction workloads
to examine how the system behaves under different loads.
In particular, we were interested in observing the point at
which the system would crash, or its performance would
severely degrade. We develop and execute various stochastic
faulty scenarios to examine the system behaviours in a
faulty production environment. The faulty stochastic sce-
narios included crash failure, Byzantine failure, and network
communication failure. A crash failure occurs when one or
more blockchain validator nodes randomly crash. Byzantine
failure happens when one or more of the validator nodes
randomly send contradicting messages over the network.
Network communication failure occurs as a result of lost
network packets or network delays.

We used virtualization software and various blockchain
platforms to build the blockchain networks for our testing.
To conduct our chaos testing experiment, we used several
chaos engineering tools. To deploy and run the nodes in the
blockchain networks, we used Docker, a virtualization con-
tainer [16]. We used Locust [17], an open-source distributed
performance testing framework for writing and executing
the load tests for different users and transactions. For exe-
cuting the faulty stochastic scenarios, we used Pumba [18],
a powerful chaos testing tool for injecting stochastic and

1. For example, Ethereum’s runtime environment is the Ethereum Virtual

Machine (EVM).

4

random failures in Docker, such as crash failures and net-
work failures. We generated Byzantine failure by randomly
corrupting outbound messages from a number of validator
nodes.

4.1. Blockchain Test Bed Construction

The shortlisted consensus algorithms are available on
different blockchain platforms. For our experiments, we
built an application on each platform, using the following
business logic:

• User A sends funds worth x units to User B.
• User A’s account balance is decreased by x units.
• User B’s account balance is increased by x units.

This is a simple asset

transfer application, however,
depending on the use case, the platforms allow for much
more functionality, including user registration and a fully
functional web application. We selected a simple application
in order to obtain results that were representative of the un-
derlying protocols’ performance - additional features would
result in performance overhead. Below is a brief overview
of the selected platforms.

4.1.1. Hyperledger Sawtooth. Hyperledger Sawtooth [19]
is a modular framework, that separates the system’s business
logic from application-level procedures, making it easier for
developers to work with. It supports dynamic consensus
i.e. the ability to switch between consensus protocols in-
between voting rounds, and pluggable consensus i.e. the
ability to choose from a list of protocols. Sawtooth supports
Go, Java, JavaScript and Python SDKs.

The Sawtooth application was built using version 1.2.6
with the PBFT consensus model. Each node had four docker
containers - a REST API endpoint, a consensus engine
(PBFT), a validator, and a transaction processor. The default
transaction processor was used in our experiments, which
allows for the following types of transactions: creating an
account with an initial balance, modifying and listing the
value of an account, and listing the values of all accounts.
For each node, the REST API’s port was exposed in the
docker ﬁle and used for communication over the network.

4.1.2. Go Ethereum. Go Ethereum (or geth) [20], is an
Ethereum client written in Go. Like other implementations
of Ethereum, it resides on every node of the network and
can run on the Ethereum mainnet as well as some testnets.
As a result, it offers the Ethash protocol (Ethereum’s PoW),
IBFT [21], and Clique. It works through a JSON-RPC API,
and web3 libraries which allow developers to run, maintain,
debug and monitor their nodes. Geth v1.10.3 was used in
these experiments.

The following steps were followed to build the Geth

application:

• Compile each node’s address into a static node list,

which is shared amongst the validators.
Start all the nodes using the geth command.

•

4.1.3. Hyperledger Fabric. Hyperledger Fabric [22] is a
permissioned DLT platform, with a modular and highly con-
ﬁgurable architecture. The ledger is shared by organizations,
each having its own peers and/or orderers. Fabric supports
Javascript, Go and Python for its chaincode, and supports the
Raft and Kafka consensus protocols. The transaction ﬂow
in a typical Fabric app is as follows:

• The client sends a transaction to every organization,
who validate it, and send back an endorsement if
valid.

• The client sends the transaction and endorsements
to an orderer organization that runs the consensus
protocol.

• Transactions endorsed by a majority are accepted.
• Once ordered the transactions are sent to the orga-

nizations and committed by their peers.

The application was built using Fabric 2.x and the

following steps were followed to build it:

• Create certiﬁcate authorities and generate certiﬁcates
for each organization using Docker and a Fabric
binary.

• Register orderers and peers with the organizations,

and create crypto-material for them.

• Generate the genesis block and other channel arti-

facts.

• Create the peers and orderers, along with their vol-

umes and environments, using Docker.
• Create the channel and join peers to it.
• Write the chaincode, install dependencies, package
the chaincode, install it at the endorsing peers, and
commit it if approved by a majority of the organi-
zations.

• Build the application using Node.js and Fabric API.

Once the applications are built, their performance is
compared using the metrics discussed in Section 3. Further,
each protocol
is compared on the basis of load testing
and chaos engineering, which is used to evaluate the fault
tolerance of our applications. Table 3 shows the important
parameters used in the tests, along with their values.

TABLE 3. APPLICATION TEST PARAMETERS

Parameter
Number of validators
Block size
Baseline user load
Load test user loads
Locust workers
Users per second per worker

Value
6
10 tx/block
250 or 50
250, 500, 1000, 1500
3
1, 2

• Create validator accounts (address, password, keys).
• Create the genesis block with Clique consensus,
designated block creators, and account balances.

Locust [17] is used to generate a constant, manageable
load on the application, and the metrics are tracked over an
entire test run. Locust interacts with the applications using

5

HTTP requests and records the time for a response, the type
of response (success or failure) and the total number of
successful responses per second. For the load tests, the load
is varied till the application crashes or performance degrades
noticeably. For chaos testing, Pumba [18] is used to generate
network delay, loss, and message corruption for relevant
network addresses. Pumba is used exclusively with Docker
containers, therefore, for the Geth application (which does
not use docker) each validator is created on a separate virtual
machine and the trafﬁc control (tc) tool within the Linux
iproute2 package (used by Pumba under the hood) is used
to introduce faults.

5. Discussion Of Results

Figure 1. Load test: PBFT

The baseline results presented in Table 4 show the
throughput, latency, and success rate of each application
calculated at a constant input load. Throughput and latency
were also measured while varying the load and while adding
faults to the blockchain network. The load test results are
plotted in Figs. 1-3 and chaos test (fault tolerance) results in
Figs. 4-6. Table 5 presents the chaos test results by providing
the average value for each metric (throughput and latency)
while each network fault is being injected into the network.
Load is generated for the blockchain applications in
terms of the number of users interacting with the app. In
Table 4, a manageable load of 250 users was selected in
order to get as stable results as possible. However, Raft
could not deal with 250 users. This is down to how en-
dorsement works in Hyperledger Fabric rather than due to
the protocol itself. In order to endorse a transaction, peers
ﬁrst process the transaction and obtain the resultant ledger
state, called the read set. After the transaction is accepted
and ordered, before being committed, it is processed once
again and the resultant state is called the write set. If the
read and write sets do not match, the transaction is cancelled.
This is not ideal for applications expecting large workloads
because the state changes several times between generation
of the read and write set. Companies like Boxer Construction
Analysts and Robinson Credit Company have implemented
independent solutions to deal with this issue [23]. Overall,
PBFT seems to perform better in terms of throughput, and
Clique in terms of average latency. Raft may perform better
if Hyperledger Fabric is conﬁgured to deal with large loads.
The load tests for each application were carried out until
the application crashed, or performance degraded visibly.
PBFT (Fig. 1) did well till the load reached 1000 users, after
which performance quickly degraded. PBFT’s throughput
and average latency ﬂuctuate when the load is changing,
but stabilize once the load stabilizes. The Clique application
in (Fig. 2) showed much better performance under load
- it crashed once it reached 1500 users causing system
performance to degrade. Raft (Fig. 3) performed the worst
under load. As discussed, Hyperledger Fabric’s inability to
naturally handle large loads explains why performance is
stable at lower loads but starts degrading/oscillating before
even 250 users are spawned.

Figure 2. Load test: Clique

The chaos tests for each application were conducted at
the same constant load as the baseline tests. The faults intro-
duced during the test were (in order): delay, loss, delay and
loss, corrupted messages from a single node, corrupted mes-
sages from half the network, corrupted messages (1 node)
with delay and loss, corrupted messages (half network) with
delay and loss, paused nodes. Here, corrupting outbound
messages has a similar effect to byzantine activity as dif-
ferent nodes receive different messages. Similarly, pausing
nodes is similar to crash failures. The metric values when
certain network faults were injected are speciﬁed in Table 5.

Figure 3. Load test: Raft

6

TABLE 4. BLOCKCHAIN APPLICATIONS: BASELINE PERFORMANCE RESULTS

Protocol Write Throughput (tx/s)

Avg. Latency (ms)

Success Rate

User Count (Load)

PBFT

Clique

Raft

50

27.3

5.8

1100

49

1850

0.88

1.0

0.98

250

250

50

TABLE 5. AVERAGE PERFORMANCE METRICS DURING CHAOS TEST

Protocol

Metric

baseline

delay (100ms)

loss (15%)

delay+loss

corrupted (50%)

corrupted+delay+loss

paused (50%)

PBFT

Clique

Raft

Throughput(tx/s)
Median Latency(ms)

Throughput(tx/s)
Median Latency (ms)

Throughput (tx/s)
Median Latency (ms)

50
18

27.3
6

5.8
1766

17.5
4463

28
105

5
3150

16.2
20.88

28.5
6

4.8
3300

24.78
4475

28.5
110

3.75
5100

10.5
2055

25.76
7

3.82
6271

16.5
4513

24
103

3.55
6430

4.9
Null

5
Null

2.33
18500

Figure 4. Fault tolerance: PBFT

Figure 6. Fault tolerance: Raft

the test, while the latency in Figs. 1-6 represents a running
average of the latency throughout the entire test run.

Entries with ’Null’ in Table 5 signify that no data is
available for that period of the test. This is usually accompa-
nied by a few short spikes where the latency metric degrades
heavily. While the median response time (median latency)
may remain relatively low during each spike, the maximum
response time shoots up. For instance, when half the net-
work was paused, the maximum response time degraded to
300000 ms in PBFT and 28000 ms in Clique. Apart from
these short spikes, there is no data for latency during the
periods in question. Pausing half the network nodes has
the most dramatic effect on performance compared to other
faults. One noteworthy observation is that network faults
affect PBFT’s throughput drastically, but have very little
effect on Clique’s throughput. On the other hand, Clique’s
and PBFT’s average latency does not change drastically,
whereas Raft’s average latency is continuously degrading
as network faults are added and removed from the network.
The Fabric application could not handle the test very
well and crashed thrice, hence the drops in the plots of
Fig. 6. In fact, this ﬁgure consists of three separate tests
whose results were combined together. The throughput plot
for Raft looks like it ﬂuctuates a lot, but this is due to

Figure 5. Fault tolerance: Clique

Figs. 4-6 depict the entire test during which the faults were
simulated consecutively. In these tests, after injecting each
fault, the network was returned to normal conditions for an
equal period of time, before injecting the next fault. This
can be observed in Fig. 4 where throughput returns to the
baseline periodically. The throughput in these plots can be
compared to the throughput in Table 5. However, the latency
in Table 5 refers to the median latency at each instant during

7

the scale of the y-axis and in reality, the extremes are not
separated by much at the baseline. Similar to Clique, the
faults affect Raft’s latency more than its throughput. It can
also be seen that Raft handles network delay or loss well,
compared to other faults.

6. Conclusion

In this paper, we summarized our experiences in apply-
ing chaos engineering principles to blockchain consensus
algorithms and applications. Using chaos engineering, we
can observe the performance of consensus algorithms and
blockchain applications in faulty production environments.
In particular, to ﬁnd the correlation between stochastic net-
work faults and system performance. We observed how the
performance of the selected consensus algorithms changes
as a result of user/transaction load and stochastic failures.
It is clear that the choice of consensus algorithm affects
system performance.

We found that Clique was able to handle load better than
PBFT and Raft, and also maintained its throughput in faulty
environments. However, PBFT showed a better throughput
overall while Raft performed the worst. In addition, our
experiments with blockchain platforms show that the choice
of blockchain platform plays an important role too. This
indicates that
if two different blockchain platforms use
the same blockchain algorithm or protocol, we should not
assume that they will have similar performance. This can be
due to restrictive architecture as in some platforms or extra
effort as in Hyperledger Fabric (Raft). Eventually, it is the
choice of consensus protocol as well as the platform that
decides the performance of a blockchain system.

In the future, we plan to extend our chaos testing sce-
narios to design a complete chaos test suite for blockchain
applications. We will
investigate the reliability of more
complex blockchain applications (functional scalability) in
the presence of failures. Investigate the impact of failure
on geographic scalability. Finally, we are interested in in-
vestigating the overhead introduced by different blockchain
platforms, particularly the platforms that use the same con-
sensus algorithms or protocols.

Acknowledgments

The authors would like to thank Canada NRC and the
Artiﬁcial Intelligence for Logistics Program. This project
was supported in part by collaborative research funding
from the National Research Council of Canada’s Artiﬁcial
Intelligence for Logistics Program. This research was par-
tially supported by the Scotiabank Global Trade Transac-
tions Initiative administered by the University of Windsor’s
Cross-Border Institute and Mitacs. We thank our colleagues
from Scotiabank and Cross-Border Institute, who provided
insight and expertise that greatly assisted the research.
However, they may not agree with all of the interpreta-
tions/conclusions of this paper.

8

References

[1] A. Basiri et al., ”Chaos Engineering,” in IEEE Software, vol. 33, no.

3, pp. 35-41, May-June 2016, doi: 10.1109/MS.2016.60.

[2] H. Tucker, L. Hochstein, N. Jones, A. Basiri and C. Rosen-
thal, ”The Business Case for Chaos Engineering,” in IEEE Cloud
Computing, vol. 5, no. 3, pp. 45-54, May./Jun. 2018, doi:
10.1109/MCC.2018.032591616.

[3] K. A. Torkura, M. I. H. Sukmana, F. Cheng and C. Meinel, ”Security
Chaos Engineering for Cloud Services: Work In Progress,” 2019 IEEE
18th International Sympm on Network Computing and Applications
(NCA), 2019, pp. 1-3, doi: 10.1109/NCA.2019.8935046.

[4] B. Ampel, M. Patton and H. Chen, “Performance Modeling of Hy-
perledger Sawtooth Blockchain,” IEEE International Conference on
Intelligence and Security Informatics (ISI), pp. 59-61, 2019.

[5] Hyperledger

Foundation,

“Hyperledger

Caliper,”

Github,

url. https://github.com/hyperledger/caliper, March 2018.

[7]

[6] Y. Hao, Y. Li, X. Dong, L. Fang and P. Chen, “Performance Analysis
of Consensus Algorithm in Private Blockchain,” June, 2018.
S.D. Angelis, L. Aniello, R. Baldoni, F. Lombardi, A. Margheri and
V. Sassone, “PBFT vs proof-of-authority: applying the CAP theorem
to permissioned blockchain,” Italian Conference on Cyber Security,
January 2018, url. https://eprints.soton.ac.uk/415083/.

[8] A. Ahmad, M. Saad, and J. Kim, D. Nyang, D. Mohaisen, “Per-
formance Evaluation of Consensus Protocols in Blockchain-based
Audit Systems,” International Conference on Information Networking
(ICOIN), pp. 654-656, 2021.

[9] N. Papadis, S. Borst, A. Walid, M. Grissa and L. Tassiulas, “Stochas-
tic Models and Wide-Area Network Measurements for Blockchain
Design and Analysis,” IEEE INFOCOM 2018 - IEEE Conference on
Computer Communications, pp. 2546-2554, 2018.

[10] C. Walter, “Blockchain Consensus Encyclopedia,” Github, August
2018, url. https://github.com/cedricwalter/blockchain-consensus.
[11] M.S. Ferdous, J.M.M. Chowdhury, M.A. Hoque and A. Col-
man, “Blockchain Consensus Algorithms: A Survey,” arXiv, eprint
2001.07091, 2020.

[12] M. Castro, B. Liskov, “Practical Byzantine Fault Tolerance,” Pro-
ceedings of the Third Symposium on Operating Systems Design and
Implementation, USA, pp. 173-186, 1999.

“Clique

consensus

[13] P. Szilagyi,

protocol,”
proof-of-authority
Ethereum, url. https://eips.ethereum.org/EIPS/eip-225, March 2017.
[14] D. Ongaro and J. Ousterhout, “In Search of an Understandable
Consensus Algorithm,” USENIX Annual Technical Conference, 2014.
[15] T. Dinh, A. Tuan, J. Wang, G. Chen, R. Liu, B. Ooi and K.L. Tan,
“BLOCKBENCH: A Framework for Analyzing Private Blockchains,”
Proceedings of the 2017 ACM International Conference on Manage-
ment of Data, USA, pp. 1085-1100, 2017.

[16] D. Merkel, “Docker: lightweight linux containers for consistent de-
velopment and deployment,”Linux journal, vol. 2014, no. 239,p. 2,
2014

[17] Locust, “Locust,” Github, url. https://github.com/locustio/locust, De-

cember 2011.
[18] A. Ledenev,

“Pumba,” Github,

url.

https://github.com/alexei-

led/pumba, April 2016.

[19] Hyperledger

Foundation,

“Hyperledger

Sawtooth,” Github,

url. https://github.com/hyperledger/sawtooth-core, January 2018.

[20] Ethereum,

“Go

Ethereum,”

Github,

url. https://github.com/ethereum/go-ethereum, July 2015.

[21] Y.T. Lin, A. Chen, M. Chen,

al.

tine
url. https://github.com/ethereum/EIPs/issues/650, June 2017.

Protocol,”

Tolerant

Fault

et.
Consensus

“Istanbul Byzan-
Github,

[22] Hyperledger

Foundation,

“Hyperledger

Fabric,”

Github,

url. https://github.com/hyperledger/sawtooth-core, September 2016.

[23] Hyperledger

“High-Throughput Network,” Github,
https://github.com/hyperledger/fabric-samples/tree/release/high-

Fabric,

url.
throughput, September 2017.

