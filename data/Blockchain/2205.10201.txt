On the Decentralization of Blockchain-enabled
Asynchronous Federated Learning

Francesc Wilhelmi
CTTC/CERCA
Barcelona, Spain
fwilhelmi@cttc.cat

Elia Guerra
CTTC/CERCA
Barcelona, Spain
eguerra@cttc.es

Paolo Dini
CTTC/CERCA
Barcelona, Spain
paolo.dini@cttc.es

2
2
0
2

y
a
M
0
2

]

R
C
.
s
c
[

1
v
1
0
2
0
1
.
5
0
2
2
:
v
i
X
r
a

Abstract—Federated learning (FL), thanks in part to the emer-
gence of the edge computing paradigm, is expected to enable true
real-time applications in production environments. However, its
original dependence on a central server for orchestration raises
several concerns in terms of security, privacy, and scalability. To
solve some of these worries, blockchain technology is expected
to bring decentralization, robustness, and enhanced trust to FL.
The empowerment of FL through blockchain (also referred to
as FLchain), however, has some implications in terms of ledger
inconsistencies and age of information (AoI), which are naturally
inherited from the blockchain’s fully decentralized operation.
Such issues stem from the fact that, given the temporary ledger
versions in the blockchain, FL devices may use different models
for training, and that, given the asynchronicity of
the FL
operation, stale local updates (computed using outdated models)
may be generated. In this paper, we shed light on the implications
of the FLchain setting and study the effect that both the AoI and
ledger inconsistencies have on the FL performance. To that end,
we provide a faithful simulation tool that allows capturing the
decentralized and asynchronous nature of the FLchain operation.

Index Terms—Asynchronous operation, blockchain, federated

learning, simulator, performance evaluation

I. INTRODUCTION

Edge intelligence, by means of edge computing, is expected
to bring artiﬁcial intelligence (AI) applications and optimiza-
tions closer to users by providing low-latency responses both
in training and inference phases [1]. Moreover,
the edge
computing paradigm is expected to provide more efﬁcient
usage of resources for handling massive amounts of user-
generated data with respect to traditional centralized learning
approaches, typically hosted at large data centers.

One paradigm increasing in popularity to enable edge
intelligence is federated learning (FL), whereby a set of
participants exchange model parameters to build a collabo-
rative model [2]. FL has been typically realized through the
orchestration of a central server, which is responsible for gath-
ering local updates computed by FL clients, aggregating the

This work was funded by the IN CERCA grant from the Secretaria
d’Universitats
i Coneixement
i Recerca del departament d’Empresa
de la Generalitat de Catalunya, and partially by the Spanish project
PID2020-113832RB-C22(ORIGIN)/MCIN/AEI/10.13039/50110001103,
the grant CHIST-ERA-20-SICT-004 (SONATA) by PCI2021-122043-
2A/AEI/10.13039/501100011033 and European Union’s Horizon 2020
research and innovation programme under Grant Agreement No. 953775
(GREENEDGE).

information to generate a new version of the global model, and
distributing new models to clients. Following this approach, an
FL algorithm can potentially reduce the overheads and enhance
the privacy of its centralized counterpart.

The traditional centralized FL approach, however, has se-
rious disadvantages that prevent its adoption in many large-
scale settings [3]. First, the central server represents a bot-
tleneck and a single point of failure, which can potentially
compromise performance and security. Second, given the de-
vices’ heterogeneity in terms of computation, communication,
and availability, centralized FL entails issues related to fault
tolerance (e.g., straggler effect). Moreover, the orchestration
of FL devices depends on a single server, typically controlled
by a single entity, which is detrimental to the democratization
of the training procedure. In this regard, an arbitrary set of
devices could be selected in each FL iteration to satisfy the
interests of a single party, therefore leading to biased models
(e.g., taking into consideration only portions of data that are
relevant for the orchestrator).

To address the issues of centralization in FL, we focus our
attention on blockchain, which can enable a secure, reliable,
and transparent server-less realization of FL, i.e., FLchain [4].
By means of cryptographic proof, blockchain provides trust
to a federated ecosystem whereby multiple (often unreliable)
parties can cooperate to train a collaborative global, each one
providing insights into their own data. In FLchain, blockchain
nodes (e.g., miners) maintain a distributed ledger that stores FL
model updates in blocks that are generated at regular intervals.
To secure the data and achieve consistency (i.e., nodes should
agree on the same history of the ledger), miners run a mining
mechanism like Proof-of-Work (PoW) and apply a certain set
of consensus rules (e.g., the main chain is the one with more
power invested).

The FLchain solution, on the one hand, allows removing
the single-point-of-failure problem of centralized FL,
thus
solving scalability issues, and providing enhanced trust thanks
to blockchain properties. On the other hand, the blockchain’s
decentralized architecture leads naturally to an asynchronous
operation whereby local updates are shared as they become
available, without any kind of coordination. Moreover, FL de-
vices may use potentially different global models for training,
as a result of the temporary inconsistencies in the ledger (for
instance, due to forks).

 
 
 
 
 
 
In this paper, we aim to shed light on the implications of
the server-less FL operation when realized through blockchain
technology. In particular, we focus on the asynchronous nature
of the learning procedure and on the ledger inconsistencies that
stem from the blockchain operation, whose impact remains
unclear. More speciﬁcally, we delve into the effects of using
stale and disparate model updates in FL training. In this regard,
we feature an age of information (AoI)-based metric [5],
introduced as age of block (AoB), to evaluate the freshness
of model updates in FLchain.

To drive our analysis, we provide an integration of the
BlockSim simulator [6], which characterizes any type of
blockchain system, with FL operation through Pytorch [7].
We name this integration BlockFLSim [8], which, to the best
of our knowledge, is the ﬁrst of its kind. Our implementation
is evaluated for a federated text recognition application, using
the MNIST dataset [9].

The rest of the paper is structured as follows: Section II
overviews the literature on server-less realizations of FL,
including FLchain. Section III presents the details of the
blockchain-enabled FL implementation, which deals with
asynchronous model updates. Section IV provides a set of sim-
ulation results to showcase the performance of the proposed
asynchronous FL application for text and image recognition
tasks, as well as the set of implications that stem from the
blockchain operation. Section V concludes the paper.

II. RELATED WORK

The trend of distributing and decentralizing ML operations
has been embraced as an appealing solution for addressing
many issues of centralization (connectivity, privacy, and secu-
rity). One appealing distributed learning solution is FL [2],
whereby different devices collaborate to train a model by
exchanging model parameters that are obtained from local
(and unshared) data. The FL framework has received a lot of
attention in the recent years, and its implementation in real-
world applications is extensive [10]. Nevertheless, traditional
FL settings still require the ﬁgure of a central server, respon-
sible for clients orchestration and model aggregation.

A prominent

solution to fully decentralize FL is
FLchain [4], [11], [12], where a blockchain system allows
FL clients to submit and retrieve model updates without the
need for a central server. Blockchain technology, beyond en-
abling decentralization, provides important enablers for trust,
including security, privacy, and traceability, thus outperforming
other existing decentralized FL solutions like the ones in [13]
(based on gossip learning) or [14] (based on BitTorrent).
FLchain, thanks to the abovementioned blockchain properties,
opens the door to an unprecedented way of building powerful
collaborative ML models through the participation of multiple
untrusted parties, and without the need for any regulating third
party. In this regard, we ﬁnd FLchain realizations for novel use
cases like autonomous vehicles [15] or fog computing [16].
For further details on the integration of FL and blockchain,
we refer the interested reader to the surveys in [17], [18].

While promising decentralization and trust, FLchain en-
tails a set of limitations that, to the date, have been barely
studied. One important consequence of the architectural shift
to decentralization stems from the way model updates are
included into blockchain blocks. Unlike for centralized set-
tings where a server decides the set of participating users
in each FL round, in blockchain, FL clients submit model
updates asynchronously. Moreover, these local updates are
incorporated into blocks of different length, which depends
on blockchain conﬁguration parameters like the maximum
block size and the block interval. An important aspect of
asynchronous learning stems from the fact that FL devices
may contribute to global training with outdated models (i.e.,
models that have been updated using past gradient information,
or models that have not been updated at all). This poses a set
of interesting challenges and trade-offs for incorporating data
from slow devices. The asynchronous property of FLchain
has been considered in a limited (and recent) number of
contributions [19]–[22].

The other important implication of FLchain is related to
the temporal inconsistencies of the ledger across blockchain
nodes, which results from the partitioning of the blockchain
due to forks. Since blockchain nodes provide global models
to FL devices, forking events may lead FL nodes to train on
different models. This aspect has not been examined in the lit-
erature and, typically, blockchain systems have been assumed
to act as a central server with a perfect synchronization among
blockchain nodes (see, e.g., [23]). In this paper, we focus on
realistic implementations of FLchain, where FL devices carry
out the asynchronous training operation in the presence of
ledger inconsistencies. Moreover, we delve into the effect of
incorporating or working with old models as a result of the
asynchronous FL operation implicit in blockchain architecture.
For that purpose, we study the staleness problem and analyze
the AoI of the FL models generated by FL devices.

III. DECENTRALIZED AYNCHRONOUS FLCHAIN
We adopt a PoW-based blockchain system, where trans-
actions are organized in blocks (with size SB), which are
generated (mined) at regular intervals BI (e.g., every 15
the transactions are the local
seconds) [24]. In FLchain,
updates submitted by FL devices, which are gathered and
spread throughout the blockchain’s peer-to-peer (P2P) network
by a set of full blockchain nodes M (with M = |M|), acting
as miners, as well. At this point, it is important to acknowledge
that the blockchain mining procedure and the FL operation are
executed in parallel, mostly independently one from the other.1
When it comes to the FL operation (illustrated in Fig. 1), a
dataset D distributed across a set of clients K (with K = |K|)
is used to train a global model w collaboratively. Under the
FLchain setting, each FL client k ∈ K with computational
power ξ(k) and local dataset D(k) of length D(k) generates
local model updates w(k) by following the next asynchronous
steps (and described also in Algorithm 1):

1Notice that blocks are mined sequentially one after another, regardless of

the incoming FL model updates.

thus disregarding the effect of temporal ledger inconsistencies.

1) Initialization with global model w0.
2) Compute a local model update w(k), using the latest
the client runs E
received global model. To do so,
epochs of SGD based on the target local loss function
l(k) and the batch size B applied to D(k) local data
points, so w(k) ← w(k) − η∇l(k)(w, D(k)), where η is
the learning rate.

3) Submit the local update w(k) to the blockchain, to be

included in a future block by a given miner.

4) Receive a new block and extract U updates for global
model aggregation. Aggregation is performed following
FedAvg, so that w ← (cid:80)U
u=1 α(u)w(u), where α(u) is a
scalar indicating the importance of weight w(u).

5) Repeat steps (2)-(4) until convergence.

Algorithm 1 Dec. Asynchronous FLchain (a-FLchain)

1: Initialize: η, E, B, D(k), ξ(k)
2: procedure MODELTRAINING():
3:
4:

while training do

Retrieve block b (with U updates) from miner m
Model aggregation: w(k) ← (cid:80)U
for e = 1, . . . , E do
e = w(k)

e (w, D(k), B, ξ(k))

u=1 α(u)w(u)

e − η∇l(k)

w(k)
end for
Send local model update w(k)

E to miner m

5:
6:

7:

8:
9:
10:
11: end procedure

end while

Different from typical centralized FL settings, in FLchain,
each client k ∈ K maintains its own FL timeline according
to the set of blocks it uses for keeping track of global
model updates. This is due to the decentralized operation of
the blockchain, whereby blockchain nodes (miners) may use
inconsistent versions of the ledger. In particular, when a miner
generates a block of depth n (the genesis block has a depth
equal to 0) as a result of the mining procedure (e.g., solving
a computational-intensive puzzle), it propagates it through the
blockchain P2P network for validation. A block is validated
when the rest of the miners append it to their version of
the ledger and start working on the next block (with depth
n + 1). The version of the ledger accepted by the majority of
the miners is referred to as the main chain. However, since
any participating miner can generate blocks, different miners
may come across a valid solution for appending a block with
the same depth simultaneously, which would lead to forks
(i.e., divergent (inconsistent) histories of the ledger). These
inconsistencies are eventually solved thanks to consensus,
which states that the longest chain (i.e., the one with the
highest invested computational power) is the valid one. The
longest chain is also referred to as main chain in the rest
of paper. Accordingly, as the main chain increases, miners
working on forked chains will abandon their chains and switch
to the valid one. In the literature (see, e.g., [20], [22]), a widely
adopted assumption is that all the FL clients in FLchain can
perfectly access the blockchain’s main chain simultaneously,

Fig. 1. Overview of the FLchain operation.

Regarding the freshness of global model updates in FLchain,
we deﬁne the age of block (AoB) as the mean peak AoI [5]
across all
the transactions in a block. The AoB can be
computed as the mean difference between the time clients
generate local updates and the time it takes to include them
into a block. Formally, the AoB of block i (∆i) is given by:

∆i =

1
U

U
(cid:88)

j=1

δi,j =

1
U

U
(cid:88)

j=1

i,j − t(1)
t(2)
i,j ,

(1)

where U is the number of client updates included in block i,
t(1)
is the time at which client j generates a local model
i,j
update, and t(2)
i,j is the time at which node j’s local update is
included into block i. It is worth noting that the AoB depends
on FL clients’ communication capabilities and blockchain
parameters (e.g., block size, block interval, capacity of P2P
links).

IV. PERFORMANCE EVALUATION

We study the impact of the the introduced AoB metric on
the learning performance under different types of blockchain
networks (e.g., Ethereum) and blockchain parameters (e.g.,
the maximum block size). The targeted FL application is
based on hand-written digits recognition. For that, we deﬁne a
four-layered feed-forward neural network (FNN) model (with
784, 200, 200, and 10 neurons in each layer), which is
trained individually by FL clients using SGD on the well-
known MNIST dataset [9], whose weights are aggregated
using FedAvg [25]. The MNIST dataset consists of 60,000
and 10,000 data samples for training and test, respectively.
For evaluation purposes, we have further split the test dataset
into test (30%) and validation (70%).

All the simulations are done in BlockFLsim [8], which
the real phenomena in FLchain.

allows studying in detail

M#2M#3P2P blockchainnetworkLedger copy (M#2)M#4Ledger copy (M#4)Miner 1(M#1)Ledger copy (M#1)Submit  local updateLedger copy (M#3)...........................Local model(Client 1)Local model(Client 2)Local model(Client K)HashPrev. hashTimest.DifficultyMerkle rootHeaderBlockNoncePayload(trans.)Table I collects all the simulation parameters.2 During the
simulations, we keep track of (i) the training accuracy of the
main chain (i.e., for the FL devices including transactions in
each block), (ii) the training loss of every FL device when
computing local model updates, and (iii) the test accuracy of
the resulting global model when the FL procedure stops.

TABLE I
MODEL/SIMULATION PARAMETERS

Parameter

Description

Value

SB
Tl
Th
M
Cp2p
Cn
BI
N B
N
ξ
E
B
n(l)
f nn
a
o
η
tf l

Max. block size
Transaction length
Block header length
Number of miners
P2P links’ capacity
FL devices links’ capacity
Block interval
Number of simulated blocks
Number of FL devices
Devices comp. power
Number of local epochs
Batch size
FNN’s neurons per layer
Activation functions
Optimizer
Learning rate
Time to compute local updates

{5, 10, 20} trans.
796.84 kbits
20 kbits
10
{10, ∞} Mbps
1 Mbps
{5, 15, 60} s
50
{10, 50, 100}
{4.744, 83.000} MIPS
3
20
[784, 200, 200, 10]
ReLU / Softmax
SGD
0.01
Exp(ξ)

(a) Effect of BI

(b) Effect of ξ

Fig. 2. Main chain’s test and training accuracy in different scenarios

Regarding the effect of BI on the accuracy (see Fig. 2(a)),
it has a different effect depending on the selected number of
FL devices. For small N values, the block interval BI has a
moderate effect on the test and training accuracy. However,
for N = 100, we observe a decrease in the training accuracy
as BI increases, which is due to the extra delay required to
incorporate all the clients’ local updates in the global model.
When it comes to the implications of using devices with
different computational capabilities (see Fig. 2(b)), we observe
that the best results are achieved for ξ =83.000 MIPS.

A. Performance in the main chain

B. Age of Blocks

We evaluate the performance of FLchain in the main chain,
which would match clients’ performance if the blockchain’s
information could be simultaneously accessed by all the FL
nodes across the network (i.e., without experiencing ledger
inconsistencies). This assumption has been widely adopted
in the literature but, as shown in the next subsections, does
not hold in reality due to the network heterogeneity and
temporal forks in the blockchain. Nevertheless,
the main
chain’s accuracy is a solid indicator of the overall performance.
Fig. 2 shows the main chain’s test and training accuracy for
various scenarios. While Fig. 2(a) focuses on the impact of the
block interval BI on the learning accuracy, Fig. 2(b) illustrates
the effect of the FL devices’ computational capabilities. In par-
ticular, we consider FL scenarios of N = {10, 50, 100} clients
with two different types of computational power ξ = {4.744,
83.000} MIPS (matching Raspberry Pi 2 and Intel Core i5-
2500K computational capabilities). Furthermore, we depict
different types of blockchain networks with BI = {5, 15, 60}
seconds and SB = {5, 10, 20} transactions. Notice that the
impact of SB is captured in each box of the plot. The
propagation delays in the blockchain are neglected in this part
of the analysis (i.e., Cp2p ≈ ∞).

As shown in Fig. 2, the test and training accuracy decreases
as the number of clients increases. This is because the whole
dataset is split into the considered number of devices, so
FL local updates become more meaningful as N decreases.

2For the sake of openness and reproducibility, all the source code used
in this paper is open and can be accessed at https://gitlab.cttc.es/supercom/
blockFLsim/-/tree/BlockFLsim (commit: 8fa5c48e).

We now focus on the impact of the AoB (presented in
Section III) on the FL performance. Fig. 3 illustrates, for each
number of FL devices N = {10, 50, 100}, both the AoB and
training accuracy achieved as the main chain increases. For
the sake of illustration, we show the simulations for BI = 5s
and BI = 60s, since they represent extreme cases with low
and high AoB.

Fig. 3. Evolution of the AoB (∆) and the training accuracy for each main
chain’s block.

As it can be observed in Fig. 3,

the training accuracy
increases with the main chain’s block number in all the cases.
As previously observed, a low N grants higher accuracy
values. As for the AoB, an important conclusion is that, even if
being very high (e.g., as for BI = 60s), it has a low impact on
the training accuracy. This indicates that, for the targeted FL
application, untimely local updates can contribute to improve
the performance of a collaboratively trained model. Another

observation is that a higher AoB may be reached for high
computational capabilities (ξ) and high block intervals (BI)
because the the clients are processing faster than the miners
and the blockchain system is not able to incorporate all the
local updates into blocks in a timely manner.

Next, Fig. 4 serves to further analyze the sensitivity of
the AoB to blockchain parameters. Each subplot represents
a different number of FL clients, whereas solid and dashed
bars refer to the two types of FL devices in terms of compu-
tational capabilities (ξ =4.744 MIPS and ξ =83.000 MIPS,
respectively). In this occasion, we plot the mean AoB (∆)
obtained throughout the entire simulations.

has been widely adopted in the existing literature, but differs
from reality. To illustrate the effect of the capacity of the
FL operation, Fig. 5 compares the FL training accuracy
achieved during the simulation in two cases: i) the blockchain
communication delays are disregarded (i.e., for Cp2p = ∞),
and ii) forks can occur as a result of using a limited capacity
in P2P links (i.e., for Cp2p = 10 Mbps). The block interval is
ﬁxed to BI = 5s (the worst case with respect to forks), while
SB = {5, 20} transactions are considered.

(a) SB = 5 transactions

Fig. 4. Mean AoB (∆) with respect to blockchain and FL parameters.

As previously illustrated in Fig. 3, Fig. 4 shows that the
mean AoB increases for larger numbers of FL devices (N ).
This effect is further accentuated for ξ = 83.000. An important
conclusion is that, due to the non-linearity shown by the AoB
across different scenarios, blockchain parameters (e.g., block
size, block interval) must be carefully selected to optimize
the blockchain delay, which would allow minimizing the
AoB. The optimization of blockchain parameters has been
previously targeted in [26].

C. Blockchain inconsistencies

An important consideration of FLchain lies in the models
used by individual FL devices for training. These models are
retrieved from the closest miner latest block, which, due to
the blockchain’s decentralized operation, may not be the same
for all the FL clients. One remarkable reason for such type of
inconsistencies lies in forks, which occur when two or more
miners generate a valid block simultaneously. The acceptance
of inconsistent blocks leads to forks, which create different
temporary versions of the ledger. These types of forks are
eventually solved thanks to the consensus protocol that states
that the longest chain (the chain with the highest invested
computational power) is the valid one. Nevertheless, during
the FL operation, different FL devices can potentially work
with different models in case forks occur.

To showcase the effect that ledger inconsistencies in the
form of forks have on the FL operation, we focus on the
P2P links’ capacity. In the previous subsections, we depicted
the ideal case whereby blockchain delays were disregarded,
so that all miners were virtually synchronized. This approach

(b) SB = 20 transactions

Fig. 5. Training accuracy for Cp2p = 10 Mbps and Cp2p = ∞ and SB = 5
tr. and SB = 20 tr..

As shown in Fig. 5, the ideal case (Cp2p = ∞) provides
better training accuracy than the realistic one (Cp2p = 10
Mbps). Such a difference is more noticeable for SB = 20
transactions (see Fig. 5(b)), which, as shown later in this
section, leads to a higher probability of experiencing forks.
In particular, the stability of the blockchain favors the FL
operation by providing timely and consistent models to FL
devices. In contrast, for Cp2p = 10 Mbps, the effect of forks
and model inconsistencies has a detrimental effect on the
training procedure, which is slowed down and experiences
more instability.

Finally, we focus on the impact that forks have on the
test accuracy, which is obtained by evaluating the global
model from the last simulated block on the test dataset.
Fig. 6 shows the difference in the test accuracy for the two
capacities considered above (Cp2p = {10 Mbps, ∞}). The
results are shown for BI = {5, 15, 60} s and SB = {5, 10, 20}
transactions, and each boxplot includes the results for all the
considered ξ and N values.

[6] M. Alharby and A. Van Moorsel, “Blocksim: a simulation framework
for blockchain systems,” ACM SIGMETRICS Performance Evaluation
Review, vol. 46, no. 3, pp. 135–138, 2019.

[7] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan,
T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. Kopf,
E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner,
L. Fang, J. Bai, and S. Chintala, “Pytorch: An imperative style, high-
performance deep learning library,” in Advances in Neural Information
Processing Systems 32. Curran Associates, Inc., 2019, pp. 8024–8035.
[8] F. Wilhelmi, “FLchainSim: an integration of BlockSim and TFF.” 2022.

[Online]. Available: https://gitlab.cttc.es/fwilhelmi/blocksim

[9] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, “Gradient-based learning
applied to document recognition,” Proceedings of the IEEE, vol. 86,
no. 11, pp. 2278–2324, 1998.

[10] M. Aledhari, R. Razzak, R. M. Parizi, and F. Saeed, “Federated learning:
A survey on enabling technologies, protocols, and applications,” IEEE
Access, vol. 8, pp. 140 699–140 725, 2020.

[11] U. Majeed and C. S. Hong, “Flchain: Federated learning via mec-enabled
blockchain network,” in 2019 20th Asia-Paciﬁc Network Operations and
Management Symposium (APNOMS).

IEEE, 2019, pp. 1–4.

[12] X. Bao, C. Su, Y. Xiong, W. Huang, and Y. Hu, “Flchain: A blockchain
for auditable federated learning with trust and incentive,” in 2019 5th
International Conference on Big Data Computing and Communications
(BIGCOM).

IEEE, 2019, pp. 151–159.

[13] C. Hu, J. Jiang, and Z. Wang, “Decentralized federated learning: A

segmented gossip approach,” arXiv preprint arXiv:1908.07782, 2019.

[14] A. G. Roy, S. Siddiqui, S. P¨olsterl, N. Navab, and C. Wachinger,
“Braintorrent: A peer-to-peer environment for decentralized federated
learning,” arXiv preprint arXiv:1905.06731, 2019.

[15] S. R. Pokhrel and J. Choi, “Federated learning with blockchain for au-
tonomous vehicles: Analysis and design challenges,” IEEE Transactions
on Communications, vol. 68, no. 8, pp. 4734–4746, 2020.

[16] Y. Qu, L. Gao, T. H. Luan, Y. Xiang, S. Yu, B. Li, and G. Zheng,
“Decentralized privacy using blockchain-enabled federated learning in
fog computing,” IEEE Internet of Things Journal, vol. 7, no. 6, pp.
5171–5183, 2020.

[17] D. C. Nguyen, M. Ding, Q.-V. Pham, P. N. Pathirana, L. B. Le,
A. Seneviratne, J. Li, D. Niyato, and H. V. Poor, “Federated learning
meets blockchain in edge computing: Opportunities and challenges,”
IEEE Internet of Things Journal, 2021.

[18] D. Hou, J. Zhang, K. L. Man, J. Ma, and Z. Peng, “A systematic
literature review of blockchain-based federated learning: Architectures,
applications and issues,” in 2021 2nd Information Communication
Technologies Conference (ICTC).

IEEE, 2021, pp. 302–307.

[19] Y. Lu, X. Huang, K. Zhang, S. Maharjan, and Y. Zhang, “Blockchain
empowered asynchronous federated learning for secure data sharing
in internet of vehicles,” IEEE Transactions on Vehicular Technology,
vol. 69, no. 4, pp. 4298–4311, 2020.

[20] Y. Liu, Y. Qu, C. Xu, Z. Hao, and B. Gu, “Blockchain-enabled
asynchronous federated learning in edge computing,” Sensors, vol. 21,
no. 10, p. 3335, 2021.

[21] R. Wang and W.-T. Tsai, “Asynchronous federated learning system based

on permissioned blockchains,” Sensors, vol. 22, no. 4, p. 1672, 2022.

[22] L. Feng, Y. Zhao, S. Guo, X. Qiu, W. Li, and P. Yu, “Baﬂ: A blockchain-
based asynchronous federated learning framework,” IEEE Transactions
on Computers, vol. 71, no. 05, pp. 1092–1103, 2022.

[23] S. R. Pokhrel and J. Choi, “A decentralized federated learning approach
for connected autonomous vehicles,” in 2020 IEEE Wireless Commu-
nications and Networking Conference Workshops (WCNCW).
IEEE,
2020, pp. 1–6.

[24] A. Gervais, G. O. Karame, K. W¨ust, V. Glykantzis, H. Ritzdorf,
and S. Capkun, “On the security and performance of proof of work
blockchains,” in Proceedings of the 2016 ACM SIGSAC conference on
computer and communications security, 2016, pp. 3–16.

[25] B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas,
“Communication-efﬁcient learning of deep networks from decentralized
data,” in Artiﬁcial intelligence and statistics. PMLR, 2017, pp. 1273–
1282.

[26] F. Wilhelmi, S. Barrachina-Mu˜noz, and P. Dini, “End-to-end latency
analysis and optimal block size of proof-of-work blockchain applica-
tions,” arXiv preprint arXiv:2202.01497, 2022.

Fig. 6. FL test accuracy difference for Cp2p = ∞ and Cp2p = 10 Mbps.
The fork probability is illustrated with green circles.

As shown in Fig. 6, the forks occurrence increases with the
maximum block size (SB) and for shorter BI values, which
allows reducing the block propagation delay directly (the most
stable conﬁguration is obtained for BI = 60 s and SB = 5).
As for the difference in the test accuracy, it increases with the
fork probability, where more signiﬁcant ledger inconsistencies
are expected. Still, the difference is very low, which suggests a
very interesting result: even in the presence of inconsistencies,
FL devices can still learn collaboratively.

V. CONCLUSIONS

FLchain emerges as an appealing solution for secure and
robust decentralized learning framework. However, the impli-
cations that decentralization of blockchain technology have
on FL optimization have not been studied closely yet. In
this paper, we have studied the implications of running an
FL application in a blockchain, including the temporal in-
consistencies and the age of information that are implicit in
such a technology. To do so, we have proposed a simulation
tool (BlockFLsim, an extension of BlockSim) for realistic
FLchain operation. Our results show that both the age of
information and model
inconsistencies (forks) have a low
impact on the accuracy of an FL application (trained with
the MNIST dataset) running over a blockchain. This result
suggests that FL is a robust framework that works effectively
even if clients use outdated information or different models
for collaborative training. Future work includes studying more
complex datasets with unbalanced settings and non-IID data.

REFERENCES

[1] Z. Zhou, X. Chen, E. Li, L. Zeng, K. Luo, and J. Zhang, “Edge
intelligence: Paving the last mile of artiﬁcial intelligence with edge
computing,” Proceedings of the IEEE, vol. 107, no. 8, pp. 1738–1762,
2019.

[2] J. Koneˇcn`y, H. B. McMahan, F. X. Yu, P. Richt´arik, A. T. Suresh, and
D. Bacon, “Federated learning: Strategies for improving communication
efﬁciency,” arXiv preprint arXiv:1610.05492, 2016.

[3] T. Li, A. K. Sahu, A. Talwalkar, and V. Smith, “Federated learning:
Challenges, methods, and future directions,” IEEE Signal Processing
Magazine, vol. 37, no. 3, pp. 50–60, 2020.

[4] H. Kim, J. Park, M. Bennis, and S.-L. Kim, “Blockchained on-device
federated learning,” IEEE Communications Letters, vol. 24, no. 6, pp.
1279–1283, 2019.

[5] R. D. Yates, Y. Sun, D. R. Brown, S. K. Kaul, E. Modiano, and
S. Ulukus, “Age of information: An introduction and survey,” IEEE
Journal on Selected Areas in Communications, vol. 39, no. 5, pp. 1183–
1210, 2021.

