DFL: High-Performance Blockchain-Based Federated Learning

Yongding Tian∗, Zhuoran Guo†, Jiaxuan Zhang‡, Zaid Al-Ars§
Delft University of Technology
Delft, The Netherlands
Email: Y.Tian-14, Z.Guo-4, J.Zhang-42@student.tudelft.nl
z.al-ars@tudelft.nl

1
2
0
2

t
c
O
8
2

]

C
D
.
s
c
[

1
v
7
5
4
5
1
.
0
1
1
2
:
v
i
X
r
a

Abstract—Many researchers are trying to replace the aggregation
server in federated learning with a blockchain system to achieve better
privacy, robustness and scalability. In this case, clients will upload their
updated models to the blockchain ledger, and use a smart contract on
the blockchain system to perform model averaging. However, running
machine learning applications on the blockchain is almost impossible
because a blockchain system, which usually takes over half minute to
generate a block,
is extremely slow and unable to support machine
learning applications.

This paper proposes a completely new public blockchain architecture
called DFL, which is specially optimized for distributed federated
machine learning. This architecture inherits most traditional blockchain
merits and achieves extremely high performance with low resource con-
sumption by waiving global consensus. To characterize the performance
and robustness of our architecture, we implement the architecture as
a prototype and test it on a physical four-node network. To test more
nodes and more complex situations, we build a simulator to simulate
the network. The LeNet results indicate our system can reach over
90% accuracy for non-I.I.D. datasets even while facing model poisoning
attacks, with the blockchain consuming less than 5% of hardware
resources.

Index Terms—Federated machine learning, blockchain, partial consen-

sus, reputation.

I. INTRODUCTION

In the past ten years, people witness the rise of crypto-currency
and the growth of decentralized ﬁnance (DeFi). However, current
blockchain applications are limited to ﬁnancial domain because it
is non-sensitive to performance limitations, but extremely relies on
security and integrity. However, only limited applications exist in
other areas due to the low performance and high latency of blockchain
systems.

Mainstream research in federated learning on blockchain uses
smart contracts to perform model aggregation, which results in
extremely poor performance [1] [2]. In this paper, we propose a
new blockchain architecture turning the blockchain database from
a distributed ledger into a proof of contribution to the machine
learning model, where each node can generate their own blocks
asynchronously, greatly improving the overall federated learning
efﬁciency. The new architecture also shows reasonable stability and
ﬂexibility, with suitable node reputation strategy and weighted Fe-
dAvg implementation for non-I.I.D. datasets and under model poising
attacks.

This paper will ﬁrst discuss current federated machine learning and
blockchain systems in Section II, with a concentration on why current
blockchain systems do not satisfy the needs of machine learning ap-
plications. Section III will elaborate on important components of the
DFL system such as partial consensus and the reputation system. The
detailed elaboration, including data format, blockchain structure and
federated machine learning implementation, is presented in Section
IV. Security issues and potential attacks are discussed in Section V.
The blockchain performance and federated machine learning results
and discussion are given in Section VI. The conclusions and future
works are presented in Section VII.

A. Federated machine learning (federated ML)

II. RELATED WORK

Federate ML continues to draw increasing attention from the
research community. A typical federated ML system contains a
central server and many clients. The central server is responsible
for aggregating and updating the models trained by clients, which
then are sent back to clients for next training iterations. This method
of aggregating and updating the central model is called FedAvg
[3]. Federate ML has advantages in both data privacy since the
training data is privately kept to the client, and model training quality
achieving high accuracy for I.I.D., non-I.I.D. and unbalanced datasets
[4].

However, federated ML also brings new vulnerabilities to the
system, including data poisoning [5] [6], model poisoning [7] and
privacy attack from the server [8]. Data poisoning and model poi-
soning aim to provide wrong models to the central server to harm
the training process of the global model. Besides, the central server is
able to recover the protected dataset stored on the client by applying
Generative Adversarial Networks (GANs) [8] [9].

Even though federated ML decentralizes ML systems from central
servers to client devices, the central server must be kept to perform
model aggregation and updating. This makes the central server a
single point of failure to the entire system. To deal with this single
point of failure and potential attacks on privacy, solutions were
proposed where the central server can be substituted by a blockchain
[1] [2] [10].

B. Blockchain system

The blockchain system was ﬁrst introduced in Bitcoin [11], a
peer-to-peer, public and immutable ledger system. All transactions
in Bitcoin will be sealed into blocks to achieve immutability. These
blocks will then be stored in a directed chain where each block
contains the digest of the previous block. The directed chain ensures
that any changes in a block will result in the change of all following
blocks. Additionally, a global consensus algorithm is applied to make
sure that only one chain exists through all nodes in this peer-to-
peer network. Bitcoin uses proof of work (PoW) to reach the global
consensus, where bitcoin miners must ”dig” for a random number
to make the block digest meet a certain pattern. The difﬁculty of
ﬁnding such a random number varies with the computation power in
the whole blockchain network - it becomes more difﬁcult to ﬁnd a
qualiﬁed random number with more computational power in total.

C. Bifurcation and block generation speed

In a real blockchain system, the computation power is measured by
the speed of generating blocks, or equivalently, the interval between
two adjacent blocks. This interval time is extremely important for the
blockchain system as it determines the possibilities of bifurcation.
The term bifurcation describes a situation where in a blockchain
network, two nodes generate two correct blocks simultaneously. Since

 
 
 
 
 
 
each node only accepts the ﬁrst block received, one of the blocks
would be rejected by over half of all miners. All miners will try
to generate the next block based on their current received block.
In bifurcation, one block is more likely to generate the next block
once it is accepted by most miners, because more miners means more
computation resources to ﬁnd the next block. So as time goes by, one
chain will be much longer than the other chains, and be accepted
by all miners, where the global consensus is reached again. The
bifurcation can greatly degrade the performance of the blockchain
system and should be avoided. One common solution is increasing the
mining difﬁculty, which leads to a longer block generation interval.
Meanwhile, a longer interval also brings higher latency. However,
The bifurcation problem exists in all blockchain systems no matter
the consensus algorithms.

More discussion about global consensus will be given in Section

III-B.

D. Federated machine learning on blockchain

Bitcoin, as the ﬁrst blockchain system, has limited functionality
as it only provides a distributed and immutable ledger. The next
generation blockchain systems, such as Ethereum [12], provide smart
contracts to allow users to execute certain small applications on the
blockchain system. The smart contract is more reliable because its
source code is open to the public on the blockchain and all miners
will execute it to ensure the integrity of the results. Some researchers
hence attempt to integrate federated ML with Ethereum and replace
the model aggregating server with smart contracts [1] [2]. However,
applying smart contracts on public blockchain systems is expensive
due to the miner fee. Federated ML on blockchains would also
be slow because every data aggregation and computation will be
performed on all nodes. A brief cost analysis is available in [2],
where federated ML costs over 108 Ethereum gas - a cost that would
be even greater for larger and more complex ML tasks. This is also
the reason why [1] uses a private blockchain system to avoid high
costs. In addition to the above problems, the storage also becomes
expensive for federated ML because every model from all nodes will
be stored on all nodes.

E. Problems for ML on blockchain

Based on the previous discussion, here is a summary about the

problems of current ML approaches on blockchain systems.

• Low blockchain performance: limited transactions per block and
high latency resulting from long block generation interval.
• Mining cost: machine learning applications must afford the

miner fee, which is an extra expense for ML applications.

• Excessive storage cost: ML models will be stored on all
blockchain miner nodes because the models are stored in a
transaction, which will be packed to a block. All miners keep
the blockchain database so each single model will be stored on
each miner’s disk.

The above three problems hinder the integration of federated
ML on blockchain. Most of existing research has only theoretical
value [10] [13] [14], with only a few of them built on a real
blockchain system [2] [1]. In this paper, we propose a practical
modiﬁed blockchain structure to support federated ML applications
with extremely low performance overhead.

A. Transaction and ML models

The ML models are stored by smart contracts on traditional
blockchain systems [2] [10]. Smart contracts surely provide function
ﬂexibility for blockchain, and makes ML over blockchain possible.
However, smart contracts guarantee global consensus because all
nodes will execute the smart contracts.

DFL is anti-global-consensus (discussed in Section III-B), since it
does not require smart contracts. For simplicity, the ML models will
be directly stored in the transactions in the DFL system.

B. Partial consensus

As discussed in Section II-B, most blockchain systems keep a
global consensus. This consensus is essential for a ledger system
since miners must validate new transactions based on the current
state of each account. Assume two transactions take place in a very
short time interval, which are then broadcast to two different nodes
quite remote in the network. In this situation, miners would only treat
the ﬁrst received transaction as valid, resulting in different accepted
transactions for different miners. But in the blockchain system, only
one miner can generate a block which only validates one certain
transaction. So the global consensus is critical for the ledger system.
In contemporary cloud computing systems and cloud storage

systems, the global consensus is achieved by a centralized server.

However, for ML applications, the global consensus would not be
necessary for two reasons. First, the arriving order of transactions
is not critical as a late arrived transaction can also be used due
to the high tolerance of FedAvg. Second, the model weights in
transactions are not critical. Two models might have exactly the same
accuracy provided the same test dataset, but their model weights do
not necessarily have to be the same.

As discussed in Section II-B, global consensus is the cause
of bifurcation and contemporary blockchain systems increase the
block interval to avoid bifurcation. Nonetheless, the DFL system
uses partial consensus rather than global consensus to build a high
performance blockchain system for federated ML.

the closer the nodes are,

Partial consensus indicates that the models stored in a limited range
of nodes should be similar to each other. From the view of network
distance,
the more similar the models
should be. In practice, partial consensus means that the models are
only broadcast for a limited distance. All nodes having received
this model should take the received model into the next FedAvg
iteration. Partial consensus could be an efﬁcient choice because each
node does not wait for a global block, which means the traditional
synchronized blockchain systems become asynchronous systems. The
detailed implementation for the partial consensus could be found in
Section IV-A.

Few distributed systems apply partial consensus because of the
necessity of a global state. However, as mentioned before, a global
state is redundant for a machine learning system. The blockchain
database hence becomes the proof of behavior to the contribution of
the owner to the model. More details about the blockchain database
can be found in Section III-F. The proof of behavior in the blockchain
also implies the need of having a witness to verify the behavior
during the block generation process. In DFL, the witness is the set
of neighbors in the p2p network, whose details will be discussed in
Section IV-A.

C. Reputation

III. BLOCKCHAIN AND FEDERATED MACHINE LEARNING

This section discusses the design principles of our DFL system

and the inspiration behind it.

The global consensus in a traditional blockchain system ensures
no malicious node would dominate the next block, unless they take
over more than half of the computation power, called a 51% attack

[15] (the reason behind 51% attack is discussed in Section II-C).
However, partial consensus allows for malicious behavior such as
model poisoning where attackers could easily hide themselves in this
network, resulting in new vulnerabilities in the system. A reputation
system will be introduced to deal with this problem.

The reputation system is responsible for distinguishing the mali-
cious node in partial consensus. A malicious node can be a node
frequently broadcasting low accuracy models, or performing a data
poisoning attack or a model poisoning attack. The reputation is
distributed and different nodes may rate the same node independently.
For example, considering a three-node system - the reputation of node
C in node A might be different from that stored in node B.

The reputation is updated according to the model accuracy. Broad-
casting models with higher accuracy earns the node higher reputation
in the system. The reputation will be stored locally and not shared
with other nodes.

D. Weighted federated averaging

To protect the system, a malicious node should have a lower
reputation in the reputation system. Meanwhile, the inﬂuence of
malicious models should be lowered as well. We propose using
the weighted federated averaging method [16], a modiﬁed version
of FedAvg, in this paper. The modiﬁed FedAvg mechanism works
together with the reputation system to keep the federated ML secured
and stable.

E. Reputation and weighted FedAvg implementation

In section IV, we implement a sample reputation and weighted
average system. However, more factors should be considered for a
more universal implementation. These factors include but are not
limited to the malicious rate (proportion of malicious node), training
performance (the accuracy curve), the data distribution (non-I.I.D.
dataset). The variances in these factors will lead to quite different
implementations of reputation and weighted FedAvg system.

DFL is designed to be ﬂexible towards the above variations, such
as different dataset distribution, malicious rate, network topology. The
reputation implementation and new federated averaging algorithm
can be dynamically linked to the DFL framework, while basic tests
are performed to ensure the correctness of external reputation and
weighted federated average implementation.

F. The blockchain database

In traditional blockchain systems, the blockchain database stores
all veriﬁed transactions and all miners need to process the transactions
on the blocks. However, in a partial consensus system, there is no
such unique blockchain database, otherwise the system will fall back
to a global consensus and struggle with bifurcation.

In DFL, the blockchain database works as a proof of contribution.
The generator needs to generate a block recording the contribution
in model training. These contributions should be able to be veriﬁed
by external nodes in the network. The veriﬁcation results will be
appended to the block forming a ﬁnal block, whose digest will be
used to generate the next block.

With this method, the generator cannot modify the blockchain
database because there is no chance to get a second veriﬁcation. The
previous block also cannot be modiﬁed as each block is chained
by the digest. This property corresponds to the immutability of
traditional blockchain.

Though blockchain database is a proof of contribution, it is not
a measure of contribution. In traditional blockchain systems, miners
can get tokens as rewards for their participation in consensus. In a

TABLE I
NOTION TABLE FOR SECTION IV-A

Notion

pri keynode
pub keynode
addressnode
hash(content)

signature(pri key, hash)

generator

neighbors of x

f urther neighbors of x

Deﬁnition

The private key for a node.
The public key for a node.
An unique identiﬁer for a node.
The hash for a certain content. The con-
tent will be immutable once hash is
calculated.
The signature generated by the pri key
and hash.

The node of generating transactions and
blocks.
All nodes whose network distance to
node x is 1.
All nodes whose network distance to
node x is larger than 1.

federated ML system, nodes should be able to get rewards if they
trained the model based on their own data. However, the reward
is not measured by the amount of transactions (models) in the
blockchain database. The reward should be the mixture of reputation
and blockchain database.

A. DFL workﬂow

IV. DFL FRAMEWORK

In this section, a fully-connected four-node DFL system will be
used as an example to explain the workﬂows and structures. Please
keep in mind that DFL does not rely on a fully-connected network
to operate, less p2p connections also ensures the system correctness.
Table I lists some important concepts which will be used in
this section. Either one of the mainstream asymmetric encryption
methods, such as ECDSA and RSA, could be applied in the system.
1) Node setup: Each node in the DFL network generates a pair
of public/private keys and an address to join the DFL network.
Then the key pair can be generated by most cryptology methods,
while the address is produced according to the rule addressnode =
hash(pub keynode).

2) Generating transaction and receipt: Figure 1 describes the

procedures to generate transactions.

Suppose in our DFL system, node 1 is training at the beginning.
The model weights after training are sent to the neighbors of node
1, i.e. node 2, 3 and 4 since the network is fully connected. Once
other nodes receive the weights, they will start to measure the model
accuracy based on their own collected dataset. Each receiver will
generate a receipt describing the accuracy of the model, which is
then appended at the end of transaction. The transaction, together
with the receipt, will be sent from each receiver to all its neighbors.
For instance, node 2 as receiver of model weights from node 1, will
send the transaction and receipt to, not only node 1, but node 3 and 4
as well. However, node 3 and 4 will ignore the transaction and receipt
from node 2, as this transaction has been processed beforehand. Node
1 will collect the receipt for generating block afterwards.

3) Generate block: Figure 2 describes the procedures to generate

blocks.

Node 1 will gather a certain amount of transactions and corre-
sponding receipts after a certain time. A block would be generated to
record the transactions and receipts in order to make them immutable.
The procedure generating such a block can be divided into two
phases. The block generator creates a draft block in the ﬁrst phase.
The second phase is to gather conﬁrmations from neighbors of the

Fig. 1. Procedures of generating transaction and receipt

block generator, and then form a ﬁnal block. With conﬁrmations
appended to the ﬁnal block, the block generator cannot easily modify
the previous block as it cannot fake a conﬁrmation.

Here is one point that transactions will be forwarded but blocks will
not, because it is most likely for the generator to receive the receipts
from its neighbors. The neighbors also will not forward receipts from
further neighbors to the generator because the transactions will be
treated as duplicate.

Take the four-node system again as an example. Node 1 sends a
draft block to node 2, then node 2 will look through the receipts to
ﬁgure out whether some receipts are produced by node 2 and generate
conﬁrmations if there are any. Node 1 will gather all conﬁrmations
from node 2, 3 and 4 and append the conﬁrmations to ﬁnalize the
block. The digest of the ﬁnal block will then be used to generate the
next block.

The component diagram is available in Figure 3 and Figure 4. They
present the same process logic as Figure 1 and Figure 2, so they will
not be discussed again.

B. DFL blockchain data format

In this section, the data formats in the blockchain system will be
discussed based on the workﬂow in previous sections. To be speciﬁc,
the data formats include the format of the transaction, the transaction

Fig. 2. Procedures of generating a block

receipt, the block and the block conﬁrmation. These data formats will
be used to prove the robustness of the DFL blockchain system.

These data formats will be illustrated in Table II with some special

notions.

1) Node information (UML available in Figure 5): The name of
a node is its address. Since the node address is decoupled from the
network address, it is hard to access a speciﬁc node by its name.
This property could achieve similar anonymity as the blockchain
since nodes can only receive a trusted transaction from the node
but hard to access the generator. For neighbors, they can know each
other’s network address because they will receive a transaction with
no receipt.

2) Transaction format (UML available in Figure 6): Among all
the transaction ﬁelds, the generator, create time, expire time,
ml model and ttl are protected by signature, which can only be
modiﬁed by the transaction creator. A modiﬁed transaction with
changed digest will be treated as a new transaction.

The ttl (time to live) is an integer value decreased by 1 when
forwarded by a node. The transaction will cease being forwarded if
its ttl reaches zero. The ttl value is critical for the range of partial
consensus. Larger ttl results in a wider partial consensus range with

Node 1time lineNode 2Node 3Node 4collectdatatrainmodelreceivemodelandmeasureaccuracyreceivemodelandmeasureaccuracyreceivemodelandmeasureaccuracysend model(transaction)datasourcecollectreceiptsgeneratereceipt,appendreceipt totransgeneratereceipt,appendreceipt totransgeneratereceipt,appendreceipt totransignoreduplicatetransignoreduplicatetransignoreduplicatetransNode 1time lineNode 2Node 3Node 4collectconfirm-ationsandfinalizeblockgenerateconfirm-ation forreceipttrans+receiptstrans+receiptstrans+receiptsgenerateblockreceiveblocksandverify thereceiptsreceiveblocksandverify thereceiptsreceiveblocksandverify thereceiptsbroadcast blockgenerateconfirm-ation forreceiptgenerateconfirm-ation forreceiptget finalhashused to generate the next blockTABLE II
NOTION TABLE FOR SECTION IV-B

Notion

= (operation)

hash (content)

signature (pri key, hash)

digest protected (digest d)

signature protected (signature s,
node o)

Deﬁnition

This data is obtained by performing
the operation.
The hash for a certain content, the
content will be immutable once
hash is calculated.
The signature generated by the
pri key and hash.
This data is protected by the digest
d. If the original data is modiﬁed,
the digest veriﬁcation will fail.
This data is deﬁnitely generated by
node o and has not been mod-
iﬁed once the signature veriﬁca-
tion passes. Because a signature is
generated based on a digest, all
signatures protected data is digest
protected.

Fig. 6. UML graph for Transaction

more nodes involved in the accuracy testing for this transaction. So
an inﬁnite ttl should be avoided because it might consume too much
computing resources. The mechanism to punish a node generating
large-ttl transactions can be achieved in the reputation system.

The expire time is designed to avoid a late arrived transaction,
whose ML model has been already outdated, i.e. processed by other
nodes. Those outdated ML models can harm the overall training
process and reduce the creator reputation. This expire time can as
well avoid attacker’s attempt to decrease the reputation of the victims
by delaying their transactions.

3) Receipt format (UML available in Figure 7): The creator is
the receipt creator, corresponding to node 2,3 and 4 in the four nodes
example in Figure 1. The transaction digest is the digest of the
received transaction. The transaction digest will not change after
appending new receipts because the transaction digest does not take

Fig. 3. Generating transaction (block diagram)

Fig. 4. Generating block (block diagram)

Fig. 5. UML graph for NodeInformation

Fig. 7. UML graph for Receipt

Machine Learning FrameworkCaffe1.collect dataData source22.train model3.model and accuracyNetwork layer5. use private/public key pair to generate a transactioncryptologyengine4. add timestamp, node information, etc6.get the transaction with signature7.broadcast the transaction to all peersGenerate a new transactionMachine Learning FrameworkCaffeNetwork layercryptologyengineopenSSL orcryptoPP1. receive a new transactionReceive a new transaction3. verify signature2. verify timestamp and message hash4. Test the accuracy and update the modelReceipt storage5.Append receipt to transaction Reputationsystem5.Update peer reputationProof of training quality3. Get the reputation of transaction generator7.broadcast the transaction to all peers6.add signature to the receiptStore the receipttransactionsand receiptsNetwork layer1. use private key pair to generate signaturecryptologyengine2. get a draft block3.broadcast the transaction to all peersGenerate a blockNetwork layercryptologyengineNode 1Node 2Receipt storage4. verify signature, digest, timestamp, etc5. verify receipt in the block 6. generate confirmation for each receipt7. generate a signature for each confirmation8. reply the confirmationsblockchaindatabase9. append confirmations and finalize blockthe digest of previous hashNodeInformation+address:string=hash(public_key)+public_key: stringTransaction+generator:NodeInformation+create_time: time_t+expire_time:time_t+ml_model: hex+ttl:int+d: digest = hash(generator, create_time, expire_time,ml_model,ttl)+sig: signature = signature(generator,d)+receipts:array(Receipt)Receipt+creator:NodeInformation+create_time: time_t+received_at_ttl:int+accuracy: string+transaction_digest:digest=digestoftransaction+d: digest = hash(creator, create_time, received_at_ttl,accuracy,transaction_digest)+sig: signature = signature(creator, d)C. DFL blockchain properties

1) Anonymity: All nodes are anonymous on the network, their
IP address are hidden behind the node address. However, the node
address might be an identiﬁer for an attacker who wants to recover
the dataset held by victims.

2) Immutability: For block generators, a single block will become
immutable once it is ﬁnalized, because neighbors will not generate
two block conﬁrmations for a receipt. The block generator cannot
modify any content in the receipts or conﬁrmation because they are
protected by the signature from other nodes.

3) Asynchronous: All nodes are able to work asynchronously be-
cause of the absence of global consensus. The generating transaction,
block, receipt and conﬁrmation processes on different nodes are
independent of each other.

D. Sample reputation and weighted FedAvg implementation

As discussed in Section III-E, users can deﬁne their own reputation
and weighted federated averaging implementations and dynamically
link them to DFL. Here we just propose a simple implementation.
All results in Section VI are obtained with this implementation.

1) Reputation: In our implementation, the reputation can only de-
crease and never increase. This mechanism can avoid some malicious
behaviors such as mixing malicious models and normal models to
attack the whole network and keep a normal reputation.

The initial default reputation for each node is 1. The reputation is
created when the node is ﬁrst seen on the network. For each weighted
federated averaging, the node, which generates the lowest accuracy
model, will be punished by decreasing their reputation by 0.01. The
lowest reputation is 0. If more than one model have the same lowest
accuracy, all their generators are punished. Notice that the reputation
might be empty if the distance of two nodes is much larger than the
ttl in a transaction.

2) Weighted federated average: The weight of a model is obtained

by the following formula.

weight = reputation · accuracy

(2)

reputation is the reputation of the transaction generator, ranging
from 0 to 1. accuracy stands for the calculated accuracy from the
model in the transaction, ranging from 0 to 1.

Suppose there are N models in the FedAvg buffer, the total weight
(weightT ) is obtained by summing all weights, the previous model
is marked as modelprev.

The ﬁnal output model is obtained by the formula below.

modeloutput =

(cid:80)N

n=1

weightn
weightT

· modeln + modelprev

2

(3)

V. SECURITY ANALYSIS

In this section, we discuss a number of attack methods [17], [18]
that can be used by malicious nodes and how DFL can protect against
them.

A. Model poisoning attacks

The DFL system by design is able to defend against model
poisoning attacks by gradually decreasing the reputation of malicious
nodes. Ideally, the attack can be fully neutralized when the reputation
reaches its lowest point. This process may take time.

Another potential risk for DFL is that the reputation system might
fail once the model poisoning attack succeeds, because the victims
cannot generate a model with better accuracy. The reputation system

Fig. 8. UML graph for Block

Fig. 9. UML graph for BlockConﬁrmation

receipts into computation. The received at ttl is calculated by the
following equation.

received at ttl = min(trans.ttl,

trans.receipts.received at ttl) − 1

(1)
The accuracy is tested based on the receipt creator’s dataset.
In DFL, all nodes will not shufﬂe their dataset, so a model can
have different accuracy for different nodes, especially for non-I.I.D.
dataset. The accuracy will be forwarded back to the transaction
creator and other nodes. This accuracy information might be helpful
for other nodes because it assists to judge the receipt creator’s
reputation, but it has not yet been implemented in the current DFL
version.

4) Block format (UML available in Figure 8): The transactions
in the block is the ﬁnal collected transactions which contains receipts.
Neighbors should check whether there are any receipts generated by
themselves, if yes, then they will generate a block conﬁrmation after
verifying the receipts. The block generator should collect at least a
certain amount of conﬁrmations for each transaction to ﬁnalize the
block.

The genesis digest is the digest of the genesis block. The genesis
block will record the ML network structure to ensure all nodes use
the same ML network. So all nodes in DFL should have the same
genesis digest if they are training the same model.

The f inal digest is calculated after the block generator collects
enough conﬁrmations for each transaction. And it will be used in the
next block to form a chain.

5) Block conﬁrmation format (UML available in Figure 9): The
block conﬁrmation contains transaction digest, receipt digest
and block digest to prove the block integrity, and the three data
segments are signature protected.

Block+previous_block_digest:digest+height: int+creator:NodeInformation+create_time: time_t+transactions:map(transactiondigest,Transaction)+genesis_digest: digest+d:digest=hash(previous_block_digest,height,creator,create_time, transactions, genesis_digest)+sig:signature=signature(creator,d)+conﬁrmations: map(conﬁrmation_digest, BlockConﬁrmation)+ﬁnalization_time:time_t+ﬁnal_digest: digestBlockConﬁrmation+creator:NodeInformation=conﬁrmationcreatorinformation+transaction_digest: digest = the transaction digest of the receipt+receipt_digest: digest = the receipt digest to conﬁrm+block_digest:digest=doftheblock+d: digest = hash(creator, transaction_digest, receipt_digest, block_digest)+sig: signature = signature(creator, d)may become ”blind” to the malicious nodes because all nodes are
producing similar models with similar poor quality. However, it is
almost impossible for attackers to compromise all nodes; in this
situation, the network can gradually recover by correcting the nodes
at the edge of compromised nodes because the innocent nodes can
create better-accuracy models.

B. Data poisoning and server attacks

Data poisoning attacks are similar to model poisoning attacks. Both
of these attacks can be prevented by decreasing the reputation of the
malicious node.

On the other hand, traditional server attacks can be completely
prevented because there is no central server in DFL. The ML model
on a node is generated by the node itself rather than receiving the
crafted models from the server.

C. Model duplication attacks

Model duplication attacks are a new attacking method which only
exits in blockchain-based federated ML systems. The attack method
can be described as an attacker that duplicates the model from other
innocent nodes and generates a new transaction with the duplicated
model to gain contributions. Verifying the digest of a transaction does
not help because the attacker can slightly modify the model weights
to get a completely different digest while retaining similar accuracy.
There are two mainstream solutions to this attack. One is to use
Trusted Execution Environments (TEEs) [19] on all nodes to protect
the model data. The other way is to use homomorphic encryption
[20] to encrypt the model, which allows other nodes to use the model
without decrypting it.

Future versions of DFL will integrate these solutions to prevent

these types of attacks.

VI. RESULT AND DISCUSSION

In this section, we use the MNIST [21] dataset to investigate the
properties and evaluate the performance of the DFL system. The
experiments and simulations will use the LeNet network [22], a
CNN architecture that performs well on the MNIST dataset. We
conduct experiments on a couple of realistic scenarios, and illustrate
the blockchain performance overhead results in detail. As mentioned
before, federated ML may perform poorly on non-I.I.D. datasets.
In this sense, we simulate the non-I.I.D. cases to evaluate the
performance on the non-I.I.D. datasets.

A. Experiment settings

The node conﬁguration utilized in the experiments could be found

in Table III.

TABLE III
NODE CONFIGURATION

node

node-1
node-2
node-3
node-4

OS

CPU

Ubuntu 20.04
Ubuntu 20.04
Ubuntu 20.04
Ubuntu 20.04

AMD Ryzen 5 4500U
Intel i7-10710U CPU@1.1GHz
Intel i5-9300HF CPU@2.40GHz
AMD Ryzen 5 4500U

Memory

16GB
16GB
16GB
16GB

Fig. 10. Test accuracy for 2 nodes

Fig. 11. Test accuracy for 4 nodes

B. Model training performance (experiment)

In these experiments, we investigate the effect of our reputation
implementation in the DFL system on the convergence of the models.
Experiments on two and four nodes are presented. 16 data samples
are collected by the system every second (for two-node experiments,
8 samples per node are collected, while for four-node experiments,
4 samples per node are collected). The results are shown in Figure
10 and Figure 11.

The ﬁgures show that the DFL system is able to successfully
converge based on our reputation implementation with an accuracy
of up to 90%. A peak appears at the beginning stage and the accuracy
drops afterwards, because initially, each node only holds limited data
and their local models are only based on their local data, therefore
the accuracy increase rapidly. Once a node applies the federated
averaging algorithm to update its model based on models from other

nodes, the accuracy may decrease as the updated model may not
perform well on its local dataset.

Besides, The ﬁgures shows that the experiment with four nodes
converges slower. It is because with the same data collecting speed,
each node in the 4-node experiment obtains less data. Each node will
train slower and each node’s model may not be beneﬁcial for other
nodes.

C. Blockchain performance overhead (experiment)

In this section, we choose node-1 and node-2 introduced in
Table III to evaluate the performance of the DFL system. Table
IV and Table V represent the measurements of the overhead of the
blockchain-related sub-process in terms of block generation, transac-
tion and model update. Table VI also shows the other blockchain-
related data for each experiments, such as the amount of transactions
per block.

TABLE IV
NODE-1 PERFORMANCE PROFILING

node-1
2-node case

node-1
4-node case

Generate block (s)
O/H 8
GC1
3.48
21.52
Receive transaction (s)
CA6
BGT 5
80.57
341.73
Generate block (s)
GC
56.63
Receive transaction (s)
BGT
214.48

CA
1944.49

O/H
3.17

Generate transaction (s)
T GB4
BT 2 M A3
25.00
362.83
59.10
Update model (s)
CSA7
O/H
O/H
5,50
0.94
82.94
Generate transaction (s)
TGB
MA
BT
59.60
364.72
71.72
Update models (s)
CSA
90.34

O/H
23.33

O/H
0.74

O/H
18.31

O/H
8.28

1 gather conﬁrmation
2 broadcast transaction
3 measure accuracy
4 triggered generating block
5 broadcast generated transaction
6 calculate accuracy
7 calculate self-accuracy
8 blockchain overhead

TABLE V
NODE-2 PERFORMANCE PROFILING

node-2
2-node

node-2
4-node

Generate Block (s)
O/H
GC
22.22
2.40
Receive Transaction (s)
CA
BGT
310.63
44.89
Generate Block (s)
O/H
GC
105.66
4.00
Receive Transaction (s)
CA
BGT
1438.01
299.28

Generate Transaction(s)
TGB
MA
BT
312.30
24.62
72.81
Update Model (s)
O/H
CSA
O/H
0.43
77.16
3.62
Generate Transaction (s)
TGB
MA
BT
399.76
109.66
88.89
Update Models(s)
CSA
99.67

O/H
20.07

O/H
3.45

O/H
7.18

O/H
8.86

TABLE VI
STATISTICS OF BLOCKCHAIN-RELATED DATA

node-1
node-2
node-1
node-2

Transaction/Block
4
4
12
12

Conﬁrmation/Block
4
4
35.11
35.67

Peers
1
1
3
3

Block
24
24
9
9

We list the overhead results proportional to total overhead time in
Table VII, which was calculated from Table IV and Table V. For

TABLE VII
OVERHEAD PERFORMANCE

GEN Block

GEN Trans

RECV Tran

Update Model

node-1
2-node
node-2
2-node
node-1
4-node
node-2
4-node

13.92%

9.74%

5.30%

3.64%

3.93%

1.72%

1.64%

1.46%

1.28%

1.00%

1.07%

1.14%

1.11%

0.56%

0.81%

1.15%

instance, the GEN Block results in Table VII is the ratio of O/H time
to the Generate Block time as shown in Equation 4.

Po/h =

To/h
Tsub−process

(4)

Each node’s conﬁguration signiﬁcantly inﬂuences the block gener-
ation and update model overhead. As for the blockchain performance,
block generation has the highest overhead in all cases, followed by
the transaction generation and receive transaction overheads.

According to the measurement results table, We have the following

equations:

TBGC ∝ Npeers · NT P B

TBT ∝ Npeers

(5)

(6)

where TBGC refers to the gather conﬁrmation time per block,
Npeers refers to the number of peers, NT P B refers to the number of
transactions per block, and TBT refers to the broadcast transaction
time. We investigate that the gather conﬁrmation time per block is
in direct proportional to the number of peers as well as the number
of transactions per block. Naturally, the broadcast transaction time is
also in direct proportional to the number of peers in the network.

D. Non-I.I.D. dataset performance (simulation)

In real-life cases, nodes or devices may have highly non-I.I.D.
datasets. This section presents simulations using the non-I.I.D. dataset
scenarios to evaluate the system performance on these datasets.

For the non-I.I.D. dataset setting, we implement the Distribution-
based label imbalance partition implementation [23]: We distribute
the MNIST dataset over 5 nodes using Dirichlet distributions Dir5(1)
and Dir5(0.1).

As each node updates its model locally and the training process
is not synchronized, we introduce the tick time-keeping concept, a
virtual time scale in our simulations. Each node takes its actions
in a random number of ticks. In simulations, the weighted FedAvg
buffer (with a size of 5 models) works as the container for storing
the received transactions in each node. When the buffer is full, a
node would take advantage of the received models, and update its
local model by implementing FedAvg, and send the updated model
to other peers for the next iteration.

With regards to the hyper-parameters setting, we employ the default

parameters deﬁned in the Caffe LeNet Solver[24].

The simulation results could be found in Figure 12 and Figure 13.
In the Dirichlet distribution, a smaller Dirichlet value will lead to
a more unbalanced distribution. The DFL system could achieve
over 90% accuracy for the Dir5(1) case, and for the extremely
uneven case Dir5(0.1), the curve still converges and ﬁnally achieves
about 70% accuracy. The curves indicate that our FedAvg algorithm
performs positively over the non-I.I.D. dataset.

Fig. 12. Non-I.I.D. test accuracy for a Dirichlet Distribution with α = 1

Fig. 14. Accuracy simulation with one malicious node for implementation 1
(unsuccessful)

Fig. 13. Non-I.I.D.test accuracy for a Dirichlet Distribution with α = 0.1

E. Malicious Node Performance (simulation)

A federated learning system should have good robustness charac-
teristics under attack. In the DFL system, robustness depends on the
reputation implementation.

This section presents two 5-node simulations with one malicious
node under two different reputation implementations. In the simula-
tions, node-1 is malicious and the other 4 nodes are normal nodes
with an I.I.D. dataset. Node-1 always sends an arbitrary model to
other nodes.

A malicious node that generates a random model behaves quite
differently from a non-I.I.D. node. A non-I.I.D. node will keep
improving its model based on the received models and its own model.
The model generated by the non-I.I.D. node will result in improving
model accuracy for other nodes at the long run, while the model from
the malicious node will aim to be inaccurate and harmful to other

Fig. 15. Reputation history simulation with one malicious node for imple-
mentation 1 (unsuccessful)

nodes.

1) Example of an unsuccessful reputation implementation: In this
experiment, we keep the reputation implementation the same as
previous simulations. In this implementation, the reputation of the
least-accurate node will decrease by 0.01 each round. Each node’s
reputation history will be presented, the reputation of a node is
calculated as the average of its reputation on other nodes.

The accuracy of each node is shown in Figure 14 and the reputation
history of each node is shown in Figure 15. Figure 14 shows that after
a long term simulation, these nodes ﬁnally cannot converge. Figure
15 shows that node-1’s reputation descend most rapidly compared
to others, which means the reputation implementation successfully
distinguished the malicious node.

A temporary peak appears at about 500 ticks in Figure 14. This is
because the learning rate at the beginning is large enough to offset the
inﬂuence of the malicious node. After that, the learning rate gradually
becomes too small to offset the malicious inﬂuence, even though the
reputation of the malicious node is much smaller than that of a normal
node.

These two examples imply that if a suitable reputation implemen-
tation and learning rate settings are given, the DFL system has the
ability to be robust under malicious attacks.

VII. CONCLUSION

In this paper, we proposed a new blockchain architecture that en-
ables high performance federated machine learning. We implemented
the architecture as DFL, which contains an executable prototype and
a simulator. In a four node experiment with MNIST and LeNet, the
training accuracy can be up to 90% while the blockchain overhead
can be kept limited to within only 5% of total execution time. The
simulation result also shows reasonable training accuracy under both
non-I.I.D. datasets as well as malicious node attacks.

For the machine learning research area, DFL reveals a path to
create a decentralized machine learning system with nearly inﬁnite
GPU resources which are used to mine cryptocurrency before, once
providing proper rewards to participants. For the blockchain research
area, DFL establishes a framework for machine learning applications
on public blockchain systems, with better privacy, anonymity and
performance.

The code of the DFL framework is open sourced and publicly

available at https://github.com/twoentartian/DFL.

VIII. ACKNOWLEDGMENT

We want to thank Xun Gui and Zixuan Xie for their discussions
and ideas on blockchain and machine learning. Their deep insights
in blockchain and machine learning helped improve this project. We
also thank Kewei Du, Xiaoning Shi and Li Xu for reviewing this
paper and providing precious feedback.

REFERENCES

[1] C. Korkmaz, H. E. Kocas, A. Uysal, A. Masry, O. Ozkasap, and
B. Akgun, “Chain ﬂ: Decentralized federated machine learning via
blockchain,” in 2020 Second International Conference on Blockchain
Computing and Applications (BCCA), 2020, pp. 140–146.

[2] P. Ramanan and K. Nakayama, “Bafﬂe : Blockchain based aggregator
free federated learning,” in 2020 IEEE International Conference on
Blockchain (Blockchain), 2020, pp. 72–81.

[3] H. B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas,
“Communication-efﬁcient learning of deep networks from decentralized
data,” 2017.

[4] C. He, S. Li, J. So, M. Zhang, H. Wang, X. Wang, P. Vepakomma,
A. Singh, H. Qiu, L. Shen, P. Zhao, Y. Kang, Y. Liu, R. Raskar, Q. Yang,
M. Annavaram, and S. Avestimehr, “Fedml: A research library and
benchmark for federated machine learning,” CoRR, vol. abs/2007.13518,
2020. [Online]. Available: https://arxiv.org/abs/2007.13518

[5] E. Bagdasaryan, A. Veit, Y. Hua, D. Estrin, and V. Shmatikov, “How
to backdoor federated learning,” CoRR, vol. abs/1807.00459, 2018.
[Online]. Available: http://arxiv.org/abs/1807.00459

[6] J. Zhang, B. Chen, X. Cheng, H. T. T. Binh, and S. Yu, “Poisongan: Gen-
erative poisoning attacks against federated learning in edge computing
systems,” IEEE Internet of Things Journal, vol. 8, no. 5, pp. 3310–3322,
2021.

[7] M. Fang, X. Cao, J. Jia, and N. Gong, “Local model poisoning attacks
federated learning,” in 29th USENIX Security
to byzantine-robust
Symposium (USENIX Security 20). USENIX Association, Aug. 2020,
pp. 1605–1622. [Online]. Available: https://www.usenix.org/conference/
usenixsecurity20/presentation/fang

[8] M. Song, Z. Wang, Z. Zhang, Y. Song, Q. Wang, J. Ren, and H. Qi,
“Analyzing user-level privacy attack against federated learning,” IEEE
Journal on Selected Areas in Communications, vol. 38, no. 10, pp. 2430–
2444, 2020.

[9] D. Enthoven and Z. Al-Ars, “An overview of federated deep learning
privacy attacks and defensive strategies,” CoRR, vol. abs/2004.04676,
2020. [Online]. Available: https://arxiv.org/abs/2004.04676

[10] H. Kim, J. Park, M. Bennis, and S.-L. Kim, “Blockchained on-device
federated learning,” IEEE Communications Letters, vol. 24, no. 6, pp.
1279–1283, 2020.

Fig. 16. Accuracy simulation with one malicious node for implementation 2
(successful)

Fig. 17. Reputation history simulation for one malicious node for implemen-
tation 2 (successful)

F. Example of a successful reputation implementation

In the new reputation implementation, the reputation of the least-
accurate node will decrease by 0.05 each round and the FedAvg
buffer size of all nodes is increased from 5 to 10. With the new
implementation, when the reputation of malicious node is small, each
node can still keep a large learning rate. The accuracy history is
shown in Figure 16 and the reputation history is shown in Figure
17. The ﬁgure shows that the nodes converge successfully with an
accuracy of up to 90%, which indicates that the system is robust under
the simple malicious attack with the new reputation implementation.
There is a constant accuracy curve between 300 to 700 ticks: the
accuracy of the nodes stays almost unchanged. Based on Figure
17, during this period, the difference between the malicious node
and normal node increases and the reputation of the malicious node
reaches 0, which means the reputation implementation gradually
adjusts the reputation to reduce the inﬂuence of the malicious node.
After the adjustment, the accuracy increases and the models ﬁnally
converge.

[11] S. Nakamoto, “Bitcoin: A peer-to-peer electronic cash system,” Cryp-

tography Mailing list at https://metzdowd.com, 03 2009.

[12] G. Wood, “Ethereum: A secure decentralised generalised transaction
ledger,” Ethereum project yellow paper, vol. 151, pp. 1–32, 2014.
[13] L. Witt, U. Zafar, K. Shen, F. Sattler, D. Li, and W. Samek,
“Reward-based 1-bit compressed federated distillation on blockchain,”
CoRR, vol. abs/2106.14265, 2021. [Online]. Available: https://arxiv.org/
abs/2106.14265

[14] Y. Lu, X. Huang, Y. Dai, S. Maharjan, and Y. Zhang, “Blockchain and
federated learning for privacy-preserved data sharing in industrial iot,”
IEEE Transactions on Industrial Informatics, vol. 16, no. 6, pp. 4177–
4186, 2020.

[15] X. Yang, Y. Chen, and X. Chen, “Effective scheme against 51% attack on
proof-of-work blockchain with history weighted information,” in 2019
IEEE International Conference on Blockchain (Blockchain), 2019, pp.
261–265.

[16] H. Wu and P. Wang, “Fast-convergent federated learning with adaptive
weighting,” IEEE Transactions on Cognitive Communications and Net-
working, pp. 1–1, 2021.

[17] D. Enthoven and Z. Al-Ars, “An overview of federated deep learning
privacy attacks and defensive strategies,” arXiv:2004.04676, 2020.
[18] ——, “Fidel: Reconstructing private training samples from weight
updates in federated learning,” arXiv preprint arXiv:2101.00159, 2021.
[19] Y. Wang, J. Li, S. Zhao, and F. Yu, “Hybridchain: A novel architecture
for conﬁdentiality-preserving and performant permissioned blockchain
using trusted execution environment,” IEEE Access, vol. 8, pp. 190 652–
190 662, 2020.

[20] Z. H. Mahmood and M. K. Ibrahem, “New fully homomorphic encryp-
tion scheme based on multistage partial homomorphic encryption applied
in cloud computing,” in 2018 1st Annual International Conference on
Information and Sciences (AiCIS), 2018, pp. 182–186.

[21] L. Deng, “The mnist database of handwritten digit images for machine
learning research,” IEEE Signal Processing Magazine, vol. 29, no. 6,
pp. 141–142, 2012.

[22] Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner, “Gradient-based learning
applied to document recognition,” Proceedings of the IEEE, vol. 86,
no. 11, pp. 2278–2324, 1998.

[23] Q. Li, Y. Diao, Q. Chen, and B. He, “Federated learning on non-iid data

silos: An experimental study,” 2021.

[24] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick,
S. Guadarrama, and T. Darrell, “Caffe: Convolutional architecture for
fast feature embedding,” arXiv preprint arXiv:1408.5093, 2014.

