2
2
0
2

n
a
J

7
1

]

G
L
.
s
c
[

2
v
6
8
2
5
0
.
1
0
2
2
:
v
i
X
r
a

Demystifying Swarm Learning: A New Paradigm of
Blockchain-based Decentralized Federated Learning

JIALIANG HAN, Key Lab of High-Confidence Software Technology, MoE (Peking University), Beijing, China
YUN MAâˆ—, Institute for Artificial Intelligence, Peking University, Beijing, China
YUDONG HAN, Key Lab of High-Confidence Software Technology, MoE (Peking University), Beijing, China

Federated learning (FL) is an emerging promising privacy-preserving machine learning paradigm and has raised more and
more attention from researchers and developers. FL keeps usersâ€™ private data on devices and exchanges the gradients of
local models to cooperatively train a shared Deep Learning (DL) model on central custodians. However, the security and
fault tolerance of FL have been increasingly discussed, because its central custodian mechanism or star-shaped architecture
can be vulnerable to malicious attacks or software failures. To address these problems, Swarm Learning (SL) introduces a
permissioned blockchain to securely onboard members and dynamically elect the leader, which allows performing DL in
an extremely decentralized manner. Compared with tremendous attention to SL, there are few empirical studies on SL or
blockchain-based decentralized FL, which provide comprehensive knowledge of best practices and precautions of deploying
SL in real-world scenarios. Therefore, we conduct the first comprehensive study of SL to date, to fill the knowledge gap
between SL deployment and developers, as far as we are concerned. In this paper, we conduct various experiments on 3 public
datasets of 5 research questions, present interesting findings, quantitatively analyze the reasons behind these findings, and
provide developers and researchers with practical suggestions. The findings have evidenced that SL is supposed to be suitable
for most application scenarios, no matter whether the dataset is balanced, polluted, or biased over irrelevant features.

CCS Concepts: â€¢ General and reference â†’ Measurement; â€¢ Social and professional topics â†’ Centralization / decen-
tralization; â€¢ Computing methodologies â†’ Machine learning.

Additional Key Words and Phrases: Swarm Learning, Decentralized Federated Learning, Empirical Study

1

INTRODUCTION

In many real-world scenarios, such as medical institutions, Internet of Things (IoT) devices, etc., data are distributed
in a decentralized manner and the volume of local data is insufficient to train reliable and robust models. It is a
common practice to feed local data into a global model and train in a centralized manner, i.e. Centralized Learning
(CL), to address the local limitations. Undoubtedly, it raises concerns about data ownership, confidentiality,
privacy, security, and monopolies. Recently, the emerging popular Federated Learning (FL) [25] mitigates some of
those concerns, where data are kept locally and local confidentiality issues are addressed [32]. Although FL has
drawn numerous attention from researchers and developers [16, 21, 22, 43, 48, 51], it uses central custodians to
keep model parameters, which could still be attacked to infer usersâ€™ identities and interests [3, 27, 33, 35, 38], even
with privacy-preserving Deep Learning (DL) techniques. Besides, the star-shaped architecture of FL damages
fault tolerance.

There are mainly two paths to address the preceding problems raised by FL. One path is on-device training
with no raw data or intermediate results uploading [10, 47], which suffers the over-fitting problem. The other path
is decentralized FL [14, 15, 17, 19, 30, 39, 40], a new FL paradigm to leverage a blockchain to coordinate the model
aggregation and update parameters in a decentralized manner. Swarm Learning (SL) is the most representative
and state-of-the-art decentralized FL paradigm [40], which combines decentralized hardware infrastructures,
distributed machine learning with a permissioned blockchain to securely onboard members, dynamically elect

âˆ—Corresponding author

Authorsâ€™ addresses: Jialiang Han, Key Lab of High-Confidence Software Technology, MoE (Peking University), Beijing, China, hanjialiang@
pku.edu.cn; Yun Ma, Institute for Artificial Intelligence, Peking University, Beijing, China, mayun@pku.edu.cn; Yudong Han, Key Lab of
High-Confidence Software Technology, MoE (Peking University), Beijing, China, hanyd@pku.edu.cn.

, Vol. 1, No. 1, Article . Publication date: January 2022.

 
 
 
 
 
 
2

â€¢ Han et al.

the leader, and merge model parameters. As shown in Figure 1, SL shares the parameters via the Swarm network
and builds the models independently on private data at Swarm edge nodes, without the need of a central custodian.
With the benefit of blockchain, SL secures data sovereignty, security, and confidentiality. Each participant is well
defined and only pre-authorized participants can be onboarded and execute transactions. In the workflow of SL,
a new edge node enrolls via a blockchain smart contract, obtains the model, and performs localized training until
a user-defined Synchronization Interval (SI). Then, local model parameters are exchanged and merged to update
the global model before the next training round. Apart from Swarm edge nodes, there are Swarm coordinator
nodes responsible for maintaining metadata like the model state, training progress, and licenses, without model
parameters.

Fig. 1. The architecture of Swarm Learning

Despite the increasing attention on the SL paradigm both in industries and academics [5, 28, 37, 41, 42],
there lacks comprehensive knowledge of best practices and precautions of deploying SL in real-world scenarios.
Although the Swarm Learning Library (SLL) is open-sourced in binary format for non-commercial use1 and has
been followed by many researchers and developers, numerous issues about deployment have been raised, such
as adaption on different datasets or data distributions, encapsulated features, license assignment, connectivity,
heterogeneity in operation systems, hardware infrastructures, DL platforms, and models.

To fill the knowledge gap between SL deployment and developers, in this paper, we use several publicly
available datasets and mainstream DL models to measure SL in real-world scenarios, following the methodology
in previous measurement studies [8, 20, 23, 24, 45, 46]. Specifically, the research questions are listed as follows.
On the one hand, how does SL perform in terms of prediction accuracy (RQ1)? Specifically, considering real-world
scenarios, how does SL perform in face of unbalanced datasets (RQ2), biased datasets over irrelevant features
(RQ3), or polluted datasets (RQ4)? On the other hand, how does SL perform in terms of resource overhead (RQ1)?
Specifically, whether and how much does scaling the nodes affect resource overhead (RQ5)? Besides, we formulate
the SL paradigm, quantitatively analyze possible reasons behind findings, discuss encapsulated features, and
provide developers and researchers with practical suggestions. As far as we are concerned, this paper is the first
comprehensively empirical study focusing on the SL paradigm. Our findings and implications of this paper can
be summarized as follows.

â€¢ Even if the dataset is unbalanced or polluted, developers can trust SL to achieve similar accuracy to CL.
â€¢ When the dataset is biased over irrelevant features, developers can trust SL to achieve higher fairness than
CL. In other words, models trained on different SL nodes provide similar accuracy for the same testing set.

1https://github.com/HewlettPackard/swarm-learning

, Vol. 1, No. 1, Article . Publication date: January 2022.

Demystifying Swarm Learning: A New Paradigm of Blockchain-based Decentralized Federated Learning

â€¢

3

â€¢ Swarm edge nodes should be deployed on instances with more computational resources and network
bandwidth than Swarm coordinator nodes. And some edge nodes consume much more network bandwidth
than other edge nodes, perhaps because of the unfairness of the leader election algorithm.

â€¢ Developers should feel free to increase the number of Swarm coordinator nodes but feel cautious to increase
that of Swarm edge nodes, because as the former increases, resource overhead remains basically unchanged,
while as the latter increases, resource overhead increases linearly. This means that SL encourages new
institution enrollment in the framework, which would hardly consume extra resources.

2 BACKGROUND

In this section, we introduce the basic components of SL, how those components interact with each other, and
the overall workflow of an SL node.

2.1 Components

There are mainly five components in the SL framework.

Swarm Learning (SL) node runs a user-defined DL algorithm, which iteratively updates the local model.
Swarm Network (SN) node interacts with each other to maintain global state information about the model
and tracks training progress via the Ethereum blockchain platform. Besides, each SL node registers itself with an
SN node when initialization and each SN node coordinates the training pipeline of its SL nodes. Note that the
blockchain records only metadata like the model state and training progress, without model parameters.

Swarm Learning Command Interface (SWCI) node securely connects to SN nodes to view the status,

control, and manage the SL framework.

SPIFFE SPIRE Server node guarantees the security of the SL framework. Each SN node or SL node includes a
SPIRE2 Agent Workload Attestor plugin that communicates with the SPIRE Server nodes to attest their identities
and to acquire and manage a SPIFFE3 Verifiable Identity Document (SVID).

License Server (LS) node installs and manages the license to run the SL framework.
Note that we refer to SL nodes as Swarm edge nodes, and collectively refer to SN nodes, SWCI nodes, SPIRE

Server nodes, and LS nodes as Swarm coordinator nodes in Section 1 for simplicity and clarity.

2.2 Component interactions

As shown in Figure 2, components of SL interact with each other in different ports dedicated for each purpose.
Swarm Network Peer-to-Peer Port is used by SN nodes to share state information about the blockchain

with each other, i.e. Line 1 in Figure 2.

Swarm Network File Server Port is used by SN nodes to run file servers and share state information about

the SL framework with each other, i.e. Line 2 in Figure 2.

Swarm Network API Port is used by SN nodes to run REST-based API servers, i.e. Line 3 in Figure 2. The
API server is used by SL nodes to send and receive state information from the SN node they are registered with.
It is also used by SWCI nodes to view and manage the status of the SL framework.

Swarm Learning File Server Port is used by SL nodes to run file servers and share learned parameters of

the model with each other, i.e. Line 4 in Figure 2.

SPIRE Server API Port is used by SPIRE Server nodes to run gRPC-based API servers for SN nodes and SL

nodes to connect to the SPIRE Server and acquire SVIDs, i.e. Line 5 in Figure 2.

SPIRE Server Federation Port is used by SPIRE Server nodes to connect each other in the SPIRE federation

and send and receive trust bundles, i.e. Line 6 in Figure 2.

2https://github.com/spiffe/spire
3https://spiffe.io/docs/latest/spiffe-about/overview/

, Vol. 1, No. 1, Article . Publication date: January 2022.

4

â€¢ Han et al.

Fig. 2. Component interactions of Swarm Learning

License Server API Port is used by the LS node to run a REST-based API server and a management interface,
i.e. Line 7 in Figure 2. The API server is used by SN nodes and SL nodes to connect to the LS node and acquire
licenses. The management interface is used by the SL framework administrators to connect to the LS node from
browsers and administer licenses.

2.3 Workflow of a Swarm Learning node

An SL node initializes in the following pipeline. First, it acquires a license from the LS node. Second, it acquires
an SVID from the SPIRE Server node. Third, it registers itself with an SN node. Fourth, it starts a file server and
announces to the SN node that it is ready to run the training program. Fifth, it starts running the user-specified
model training program.

After initialization, each SL node regularly shares its learned parameters with other SL nodes and merges their
parameters. This merging periodicity is defined by a Synchronization Interval (SI), which specifies the number of
training batches after which SL nodes merge their learned parameters. At the end of each SI, one of the SL nodes
is elected as the leader by the blockchain. The leader collects local models from each SL node and merges them
into a global model by averaging their learned parameters. After that, each SL node receives this merged model
and starts the next SI.

3 METHODOLOGY

In this section, we introduce how we adapt different models on different public datasets in an SL setting. We
set up 3 tasks and 5 research questions to measure the performance of SL in different scenarios. We provide
the hyper-parameters in our experimental settings for reproducibility, which we carefully choose through cross
validation.

Task A: diagnosis of chest X-rays. We use NIH chest X-ray dataset4 and DenseNet-49 as the classification
model. This dataset consists of 112,120 X-ray images with disease labels from 30,805 unique patients [36]. We

4https://nihcc.app.box.com/v/ChestXray-NIHCC

, Vol. 1, No. 1, Article . Publication date: January 2022.

Demystifying Swarm Learning: A New Paradigm of Blockchain-based Decentralized Federated Learning

â€¢

5

filter out patients whose ages are above 100 and resize the images to 256*256 pixels.Note that some samples may
have multiple disease labels, therefore, this is a multi-class classification. We use Dense Convolutional Network
(DenseNet) as the classifier, which connects layers to each other in a feed-forward fashion. DenseNets have several
compelling advantages, including alleviating the vanishing-gradient problem, strengthening feature propagation,
encouraging feature reuse, and substantially reducing parameters [12]. The block sizes of DenseNet-49 are (4, 4,
8, 6). The batch size is 32. The dropout rate of the fully connected layer is 0.15. The initial learning rate is 1e-3,
which decays every 40 epochs with the decay ratio ğ›¼ = 0.1.

Task B: object recognition. We use CIFAR-10 dataset5 and DenseNet-BC as the classifier. The CIFAR-10
dataset consists of 60,000 32x32 color images in 10 classes, with 6,000 images per class. The depth of DenseNet-BC
is 100 and the growth rate is 12. The batch size is 32. The initial learning rate is 1e-3, which decays at the 80th,
120th, 160th epochs with the decay ratio ğ›¼ = 0.1.

Task C: sentiment analysis. We use IMDB review dataset6 and Attention-based Bidirectional Long Short-
Term Memory (Bi-LSTM) as the classifier. The IMDB dataset consists of 50,000 highly polar movie reviews with
ratings. Bi-LSTM units have access to both past and future context information and mitigate the vanishing gradient
problem to capture long-distance dependencies. And the attention mechanism captures the most important
context information in a sentence. The length of the word embedding is 128. The number of Bi-LSTM units is 64.
The batch size is 64. The dropout rate of the fully connected layer is 0.05.

4 MEASUREMENT RESULTS

In this section, we measure the performance in Section 4.1, the effect of imbalance in Section 4.2, the fairness in
Section 4.3, fault tolerance in Section 4.4, and the scalability of SL in Section 4.5.

4.1 RQ1: Performance

In this research question, we measure the performance of SL, including prediction accuracy, computational
overhead, network overhead, and storage overhead.

In each task, we randomly partition the training set into three (four) sub-datasets with an equal number of
samples to deploy on three (four) SL nodes. The baseline experimental setting is Centralized Learning (CL) with
the whole training set. The testing set of SL and CL is the same for fairness. The detailed statistics are shown in
Table 1. The results show that the accuracy of SL is similar or even slightly higher than that of CL. The explanation
is that SL introduces ensemble methods that contribute to better generalizability and robustness than CL.

Table 1. Accuracy in RQ1

Baseline

Node 1

Node 2

Node 3

Node 4

SL

Task A
Task B
Task C

0.8850
0.9350
0.8940

0.8780
0.8922
0.8472

0.8768
0.8855
0.8449

0.8759
0.8884
0.8543

-
-
0.8484

0.9090
0.9304
0.8875

The resource overhead of Task A is shown in Figure 3. We notice that SPIRE Server (SS) nodes, License Server
(LS) nodes, and Swarm Network (SN) nodes hardly consume CPU resources, while each SL node consumes
500%-1000% of single-core CPU resources. From the most to the least memory footprint, each SL node consumes
about 8 GB memory, the LS node consumes about 7 GB memory, the SN node consumes about 2 GB memory,
while other nodes hardly consume memory resources. As for network overhead, SL nodes take turns consuming
60 to 120 MB per timestamp, i.e. per 5 seconds. The reason behind this take-turn behavior is that the blockchain

5https://www.cs.toronto.edu/âˆ¼kriz/cifar.html
6http://ai.stanford.edu/âˆ¼amaas/data/sentiment/

, Vol. 1, No. 1, Article . Publication date: January 2022.

6

â€¢ Han et al.

dynamically elects the leader among members. When a node becomes the current leader, its network overhead
would increase significantly because other nodes would send and receive parameters from it. Interestingly, SL-2
consumes much more network bandwidth than other SL nodes. The underlying reason might be the unfairness
of the leader election algorithm, which we will discuss in Section 6. Besides, all kinds of nodes barely consume
storage resources. Resource overhead of Task B and Task C is similar to that of Task A. Therefore, we omit these
results because of the page limit.

(a) CPU

(b) Memory

(c) Network (in)

(d) Network (out)

(e) Disk (in)

(f) Disk (out)

Fig. 3. Overhead of Task A in RQ1

Summary: When the dataset is balanced, the accuracy of SL is similar to that of CL. And SL nodes take turns
consuming much more computational resources and network bandwidth than other nodes. Besides, some SL nodes
consume much more network bandwidth than other SL nodes.

4.2 RQ2: Imbalance

In this research question, we measure the prediction accuracy of SL in face of unbalanced samples on different
nodes in Section 4.2.1, and unbalanced labels of samples on each node in Section 4.2.2.

4.2.1 RQ2.1: Imbalance of Samples on Nodes. We randomly partition the training set into three (four) sub-datasets
with 1:2:3 (1:2:3:4) samples to deploy on three (four) SL nodes. The other experimental settings are almost the

, Vol. 1, No. 1, Article . Publication date: January 2022.

Demystifying Swarm Learning: A New Paradigm of Blockchain-based Decentralized Federated Learning

â€¢

7

same as those of Section 4.1. The detailed statistics are shown in Table 2, which show that the accuracy of SL is
similar to or would not drop much compared with that of CL.

Table 2. Accuracy in RQ2.1

Baseline

Node 1

Node 2

Node 3

Node 4

SL

Task A
Task B
Task C

0.8850
0.9350
0.8940

0.8750
0.8432
0.8187

0.8758
0.8878
0.8437

0.8782
0.9086
0.8469

-
-
0.8717

0.8830
0.9226
0.8943

4.2.2 RQ2.2: Imbalance of Labels of Samples. Because Task A is a multi-class classification task, it is difficult to
manually truncate this dataset to satisfy the conditions of this research question. In Task B, we manually truncate
the training set of CIFAR-10 to have 500 to 5000 samples of each class, satisfying the power-law distribution.
In Task C, we manually truncate the training set of IMDB to have 12,000 negative samples and 4,000 positive
samples. The other experimental settings are almost the same as those of Section 4.1. The detailed statistics are
shown in Table 3, which shows that the accuracy of SL is similar to or even slightly higher than that of CL. The
explanation is that the generalizability and robustness improvement of SL benefits accuracy when labels are
unbalanced and especially when the dataset is relatively small.

Table 3. Accuracy in RQ2.2

Baseline

Node 1

Node 2

Node 3

Node 4

SL

Task B
Task C

0.8699
0.8591

0.7726
0.8146

0.7606
0.8281

0.7746
0.8197

-
0.8061

0.8559
0.8629

Summary: If SL nodes have different numbers of samples, or the labels of samples are unbalanced, the accuracy of
SL would not drop much compared with that of CL.

4.3 RQ3: Fairness

In real-world situations, different SL nodes may well have a biased distribution of data over irrelevant features to
the prediction results. For example, patients in different medical institutions would have a different distribution
of gender, age, ethnicity, etc. If those features are irrelevant to the prediction results, models trained on different
SL nodes should provide similar accuracy for the same testing set, in consideration of fairness. In this research
question, we measure the fairness of SL, i.e. the accuracy of different SL nodes on the testing sets of themselves
and others. To achieve this, we randomly partition the dataset on each SL node into a local training set and a
local testing set and remain the global testing set mentioned in Section 4.1. Because there are irrelevant features
like gender and age in the dataset of Task A, which helps us generate biased sub-datasets, we only test Task A in
this research question and generate an NIH-age and NIH-gender sub-dataset groups.

In NIH-age, we partition the training set into three sub-datasets according to the ages of patients. The ages of
patients on Node 1 are from 0 to 30, the ages of patients on Node 2 are from 30 to 60, and the ages of patients on
Node 3 are from 60 to 100. Note that the number of samples on each node is close to each other. To measure the
fairness of SL, we use Localized Learning (LL) as the baseline, where each node performs training on its local
training set. In both LL and SL settings, each node performs inference on local testing sets of all nodes and the
global testing set. We present the Receiver Operation Characteristics Area Under Curve (ROC-AUC) of LL and
SL in Table 4 to reveal the accuracy gap. The results show that during LL, each node achieves a slightly higher

, Vol. 1, No. 1, Article . Publication date: January 2022.

8

â€¢ Han et al.

accuracy on its local testing set, which damages the fairness of the model. In contrast, during SL, each node
achieves similar accuracy (about 0.71) on Node 1, similar accuracy (about 0.74) on Node 2, and similar accuracy
(about 0.74) on Node 3, regardless of whether being its local testing set or not.

Table 4. ROC-AUC of NIH-age in RQ3

LL/SL

Node 1
Node 2
Node 3

Node 1 Test-
ing

Node 2 Test-
ing

Node 3 Test-
ing

Global
Testing

0.6827/0.7159
0.6740/0.7122
0.6748/0.7113

0.6795/0.7359
0.6989/0.7390
0.7044/0.7370

0.6733/0.7368
0.6913/0.7423
0.7074/0.7398

0.7039/0.7402
0.6931/0.7396
0.6803/0.7395

In NIH-gender, we partition the training set into three sub-datasets according to the genders of patients. The
female to male ratio of Node 1 is 9:1, the female to male ratio of Node 2 is 5:5, and the female to male ratio of Node
3 is 1:9. Note that the number of samples on each node is close to each other. The other experimental settings
are similar to those of Section 4.3. We present the ROC-AUC of Localized Learning (LL) and SL in Table 5. The
results show that during LL, Node 2 achieves a slightly higher accuracy on its own testing set, which damages the
fairness of the model. In contrast, during SL, Node 2 and Node 3 achieve similar accuracy (about 0.73) on Node 1,
similar accuracy (about 0.74) on Node 2, and similar accuracy (about 0.74) on Node 3, regardless of whether being
its own testing set or not. The reason behind the lower accuracy of SL on Node 3 might be that gender is not
completely irrelevant to disease labels.

Table 5. ROC-AUC of NIH-gender in RQ3

LL/SL

Node 1
Node 2
Node 3

Node 1 Test-
ing

Node 2 Test-
ing

Node 3 Test-
ing

Global
Testing

0.6827/0.7319
0.6836/0.7333
0.6837/0.7120

0.6832/0.7394
0.7029/0.7425
0.6969/0.7184

0.6851/0.7465
0.6967/0.7414
0.6934/0.7251

0.6986/0.7369
0.7042/0.7386
0.7033/0.7363

Summary: When the dataset is biased over irrelevant features, models trained on different SL nodes provide similar
accuracy for the same testing set. Therefore, the fairness of SL is better than that of CL.

4.4 RQ4: Fault Tolerance

In this research question, we measure the fault tolerance of SL in face of low-quality nodes (LQNs), where more
than half of samples are incorrectly labeled.

We modify the labels of half of the samples on one node, i.e. LQN, to be incorrect. The baseline is CL with the
whole training set, i.e. the union of unpolluted and polluted data. The other experimental settings are almost the
same as those of Section 4.1. The detailed statistics are shown in Table 6. In Task A and Task B, although the
accuracy of LQN is much lower than other nodes, the accuracy of SL is similar to or even slightly higher than
that of CL. The reason behind this is the mechanism of training separately on unpolluted and polluted data and
then merging periodically, and that localized training on unpolluted data would mitigate the adverse effect of
polluted data on accuracy. However, in Task C, the accuracy of SL is lower than that of CL. The reason behind
this is that the IMDB dataset is relatively small and the model on each node is easy to converge, however, the
SL model cannot converge eventually, because the models trained on unpolluted and polluted data are quite

, Vol. 1, No. 1, Article . Publication date: January 2022.

Demystifying Swarm Learning: A New Paradigm of Blockchain-based Decentralized Federated Learning

â€¢

9

different. Even so, SL mitigates the adverse effect of polluted data to some extent because the accuracy of the
LQN is even much lower than that of SL.

Table 6. Accuracy in RQ4

Metric

Task A
Task B
Task C

Baseline

Node 1

Node 2

Node 3

LQN

SL

0.8841
0.8673
0.8646

0.8795
0.8933
0.8322

0.8805
0.8880
0.8500

-
-
0.8380

0.8796
0.4480
0.5002

0.8840
0.8897
0.7955

Summary: If there exist low-quality nodes, where more than half of samples are incorrectly labeled, in most scenarios,
the accuracy of SL would be similar to that of CL.

4.5 RQ5: Scalability

In this research question, we measure the scalability of SL, i.e. computational overhead, network overhead, and
storage overhead as SN nodes scale in Section 4.5.1 or as SL nodes scale in Section 4.5.2. Note that the number
of SN nodes here refers to the total number of SL nodes attached to all SN nodes. To measure computational
overhead, we measure the average usage percentage of CPU, the average memory footprint. To measure network
overhead, we measure the amount of data which have been sent or received over the network interface. To
measure storage overhead, we measure the amount of data which have been read from or written to block devices.
Note that as for network and storage overhead, we measure the total amount of data in or out, instead of the
average, because the convergence time of training on different numbers of SL nodes varies significantly when
the dataset is fixed, which makes measuring the total overhead more reasonable than the average. We measure
the above resource overhead on SWCI nodes, SPIRE Server (SS) nodes, License Server (LS) nodes, SN nodes, and
SL nodes mentioned in Figure 2. Due to the page limit, we only show the results of Task B. Because more SL
nodes consume more GPU memory, we choose EfficientNetB2 [34] to save GPU memory instead of DenseNet-BC
in previous research questions for Task B.

4.5.1 RQ5.1: Scaling SN nodes. We fix the total number of SL nodes to 4 and vary the number of SN nodes from
1 to 4.

Scenario 1. When the number of SN nodes is 1, 4 SL nodes are linked to each SN node. The detailed statistics
are shown in Table 7. The findings are similar to those in Section 4.1, only that the absolute values are different.

Table 7. Overhead of Task B (SN = 1, SL = 4)

Node

CPU (%)

Mem (MB)

Net/in
(MB)

Net/out
(MB)

Block/in
(MB)

Block/out
(MB)

SWCI
SS
LS
SN-0
SL-0-0
SL-0-1
SL-0-2
SL-0-3

0.26
0.68
0.36
11.25
205.56
205.20
206.58
206.38

91.29
143.19
2392.41
1386.77
2920.06
2978.02
3034.54
3013.60

10.4
23.7
6.5
28.9
8820.0
10700.0
17800.0
15200.0

4.2
58.3
5.1
17.7
8820.0
10700.0
17800.0
15200.0

3.08
89.60
120.00
0.11
0.11
0.05
0.00
0.01

0.02
0.02
0.73
1.76
1.56
1.56
1.56
1.56

, Vol. 1, No. 1, Article . Publication date: January 2022.

10

â€¢ Han et al.

Scenario 2. When the number of SN nodes is 2, 2 SL nodes are linked to each SN node. The detailed statistics
are shown in Table 8. In terms of computational overhead, we notice similar findings to Scenario 1, except that
two SN nodes consume significantly different CPU and memory resources from each other. In terms of network
overhead, the SWCI node, the SS node, the LS node consume about 2 times network resources as many as Scenario
1. Each SN node and each SL node consume similar network resources to Scenario 1, except SL-0-1, perhaps
because SL-0-1 is elected as the leader more frequently than others. In terms of storage overhead, each node
consumes similar storage resources to Scenario 1.

Table 8. Overhead of Task B (SN = 2, SL = 4)

Node

CPU (%)

Mem (MB)

Net/in
(MB)

Net/out
(MB)

Block/in
(MB)

Block/out
(MB)

SWCI
SS
LS
SN-0
SN-1
SL-0-0
SL-0-1
SL-1-2
SL-1-3

0.21
0.78
0.38
10.79
2.73
201.98
203.47
201.35
206.07

93.89
146.34
3102.69
1295.24
658.53
3029.52
3035.19
2956.23
2934.75

18.2
49.2
13.7
28.3
35.0
9120.0
25100.0
9640.0
8770.0

8.11
109.0
10.7
29.2
20.7
9120.0
25100.0
9640.0
8770.0

3.08
90.30
120.00
0.14
0.00
0.02
0.00
0.24
0.00

0.05
0.02
1.10
1.71
0.59
1.56
1.56
1.56
1.56

Scenario 3. When the number of SN nodes is 4, only 1 SL node is linked to each SN node. The detailed statistics
are shown in Table 9. In terms of computational overhead, we notice similar findings to Scenario 1, except that
SN-0 consumes significantly more CPU and memory resources than others. In terms of network overhead, the
SWCI node, the SS node, the LS node consume about 4 times network resources as many as Scenario 1. Each SN
node and each SL node consume similar network resources to Scenario 1, except SL-1-1. And the reason behind
this is the same as that when the number of SN nodes grows from 1 to 2. In terms of storage overhead, each node
consumes similar storage resources to Scenario 1.

Table 9. Overhead of Task B (SN = 4, SL = 4)

Node

CPU (%)

Mem (MB)

Net/in
(MB)

Net/out
(MB)

Block/in
(MB)

Block/out
(MB)

SWCI
SS
LS
SN-0
SN-1
SN-2
SN-3
SL-0-0
SL-1-1
SL-2-2
SL-3-3

0.46
0.63
0.32
10.58
2.60
2.43
2.22
204.77
202.29
203.00
203.46

94.89
149.55
4025.16
1442.41
752.57
659.89
635.66
2991.51
2856.76
2821.84
2939.63

39.3
103.0
28.0
45.0
34.7
32.2
30.5
9000.0
22300.0
12000.0
9230.0

18.7
216.0
21.9
64.3
20.4
18.6
17.7
9010.00
22300.00
12000.00
9240.00

3.08
91.00
120.00
0.25
0.13
0.11
0.00
0.00
0.00
0.01
0.00

0.05
0.25
1.74
0.59
1.53
0.77
0.56
1.56
1.58
1.55
1.55

, Vol. 1, No. 1, Article . Publication date: January 2022.

Demystifying Swarm Learning: A New Paradigm of Blockchain-based Decentralized Federated Learning

â€¢

11

In summary, as the number of SN nodes increases, resource overhead remains basically unchanged, because SL

nodes account for the most resource overhead.

4.5.2 RQ5.2: Scaling SL nodes. We fix the number of SN nodes to 2 and vary the number of SL nodes from 2 to 8.
Scenario 4. When the number of SL nodes is 2, only 1 SL node is linked to each SN node. The detailed statistics
are shown in Table 10. The findings are similar to those in Section 4.1, only that the absolute values are different.

Table 10. Overhead of Task B (SN = 2, SL = 2)

Node

CPU (%)

Mem (MB)

Net/in
(MB)

Net/out
(MB)

Block/in
(MB)

Block/out
(MB)

SWCI
SS
LS
SN-0
SN-1
SL-0-0
SL-1-1

0.20
0.49
0.21
9.70
2.02
226.36
226.61

96.88
153.28
4341.82
1528.33
662.85
3244.31
3241.23

78.7
208.0
56.9
27.8
38.2
8780.0
8760.0

38.4
427.0
46.5
31.8
22.2
8760.00
8780.00

3.08
91.20
127.00
0.00
0.00
0.00
0.01

0.07
0.25
3.14
1.86
0.63
1.66
1.66

When the number of SL nodes is 4, 2 SL nodes are linked to each SN node. This experimental setting is the same
as Scenario 2 in Section 4.5.1. Therefore, the detailed statistics are shown in Table 8. In terms of computational
overhead, the SWCI node, the SS node, the LS node, and SN nodes consume similar CPU and memory resources
to Scenario 4. The computational overhead of each SL node decreases slightly. In terms of network overhead, the
SWCI node, the SS node, the LS node, each SN node, and most SL nodes consume similar network bandwidth
to Scenario 4. However, SL-0-1 consumes about 2.5 times network bandwidth as much as Scenario 4, perhaps
because SL-0-1 is elected as the leader more frequently than others. Besides, each node consumes similar storage
resources to Scenario 4.

Scenario 5. When the number of SL nodes is 8, 4 SL nodes are linked to each SN node. The detailed statistics
are shown in Table 11. In terms of computational overhead, the SWCI node, the SS node, the LS node, and SN
nodes consume similar CPU and memory resources to Scenario 4 in Section 4.5.2. The computational overhead of
each SL node decreases slightly. In terms of network overhead, the SWCI node, the SS node, the LS node, each SN
node, and most SL nodes consume similar network resources to Scenario 4, except that SL-0-1 consumes about
1.5 times network resources as many as Scenario 4, and SL-0-2 consumes about 5 times network resources as
many as Scenario 4. And the reason behind this is the same as that when the number of SL nodes grows from 2
to 4. Besides, each node consumes similar storage resources to Scenario 4.

In summary, as the number of SL nodes increases, computational overhead increases linearly (but slower),
because the sub-dataset on each SL node grows smaller. And network overhead increases linearly (but faster)
because dividing the dataset into more SL nodes introduces extra difficulties in convergence and some SL nodes
are elected as the leader more frequently than others. The storage overhead of each node remains unchanged.

Summary: As the number of SN nodes increases, resource overhead remains basically unchanged. As the number of
SL nodes increases, computational overhead and network overhead increase linearly, while storage overhead remains
unchanged.

, Vol. 1, No. 1, Article . Publication date: January 2022.

12

â€¢ Han et al.

Table 11. Overhead of Task B (SN = 2, SL = 8)

Node

CPU (%)

Mem (MB)

Net/in
(MB)

Net/out
(MB)

Block/in
(MB)

Block/out
(MB)

SWCI
SS
LS
SN-0
SN-1
SL-0-0
SL-0-1
SL-0-2
SL-0-3
SL-1-4
SL-1-5
SL-1-6
SL-1-7

0.29
0.78
0.40
11.68
3.89
176.14
176.64
182.21
175.33
176.72
175.20
175.48
175.94
5 FORMULATION AND QUANTITATIVE ANALYSIS

96.52
153.39
4228.73
1626.26
675.86
2896.07
2926.51
2807.66
2876.95
2870.63
2834.57
2899.40
2670.77

56.5
165.0
46.1
44.7
50.9
10500.0
14900.0
51100.0
9100.0
9980.0
8760.0
9450.0
8760.0

27.3
340.0
38.1
41.0
31.5
10500.0
14900.0
51100.0
9110.0
9980.0
8760.0
9450.0
8760.0

3.08
91.10
127.00
0.00
0.00
0.14
0.02
0.13
0.00
0.00
0.00
0.02
0.00

0.07
0.25
2.69
1.74
0.58
1.57
1.56
1.56
1.56
1.57
1.56
1.57
1.57

ğ‘˜=1

ğ‘˜=1

(cid:205)ğ‘– âˆˆ Pğ‘˜

ğ‘ğ‘˜ ğ¹ğ‘˜ (ğ‘¤) = (cid:205)ğ¾

In Section 4, we mostly analyze experimental results qualitatively. In this section, we formulate the SL optimization
problem and analyze the above results quantitatively.

The objective of SL is to minimize the weighted average loss ğ‘“ (ğ‘¤) = (cid:205)ğ¾

ğ‘›ğ‘˜
ğ‘› ğ¹ğ‘˜ (ğ‘¤), where ğ‘¤
is model parameters, and data are distributed on ğ¾ peers, with Pğ‘˜ the set of samples on peer ğ‘˜, with ğ‘›ğ‘˜ = |Pğ‘˜ |. The
local objective of a SL peer is to minimize the average loss ğ¹ğ‘˜ (ğ‘¤) = 1
ğ‘“ğ‘– (ğ‘¤), where ğ‘“ğ‘– (ğ‘¤) = â„“ (ğ‘¥ğ‘–, ğ‘¦ğ‘– ; ğ‘¤).
ğ‘›ğ‘˜
During each Synchronization Interval (SI), each peer locally takes several batches of Stochastic Gradient Descents
(SGDs) on the current model using its local data, and then the leader takes a weighted average of the resulting
parameters and assigns it to each peer. This model aggregation process is similar to FedAvg in FL [25], except
that the leader is elected dynamically through a blockchain instead of using a fixed central custodian. Therefore,
the convergence analysis of FL [18] can be applied to SL, where FedAvg has O ( 1
ğ‘‡ ) convergence rate for strongly
convex and smooth problems, where ğ‘‡ is the number of SGDs. Besides, there is a trade-off between communication
efficiency and convergence rate, even with non-iid data (i.e. data are not identically distributed) or partial device
participation. Theoretically, this explains the findings that the convergence accuracy of SL is similar to that of
Centralized Learning (CL) in most scenarios and reminds developers to choose an optimal SI. The reason why the
accuracy of SL is sometimes higher than that of CL is that ensemble methods benefit the robustness and reach a
better local optimal point for non-convex objectives [2]. Besides, the reason behind the fairness of SL is that SL
models converge to optimal points of the global training set and Localized Learning (LL) models converge to
those of the local training set.

As for scalability, because SN nodes are not responsible for model training, resource overhead remains
basically unchanged as SN nodes scale. Besides, the reason behind the changes of computational overhead and
storage overhead as SL nodes scale is apparent and explained in Section 4.5.2, while the reason behind the
changes of network overhead is interesting. As mentioned in the convergence analysis of FL [18], the number of
communications is formulated as

ğ‘‡
ğ¸

= O

(cid:32) (cid:18)

(cid:34)

1
ğœ–

1 +

(cid:19)

1
ğ¾

ğ¸ğº 2 +

(cid:205)ğ¾

ğ‘˜=1

ğ‘2
ğ‘˜

ğ‘˜ + Î“ + ğº 2
ğœ 2
ğ¸

(cid:33)(cid:35)

+ ğº 2

,

(1)

, Vol. 1, No. 1, Article . Publication date: January 2022.

Demystifying Swarm Learning: A New Paradigm of Blockchain-based Decentralized Federated Learning

â€¢

13

where ğ¸ is the number of local iterations performed in a peer between synchronizations, ğœ– is a fixed precision, ğº
and ğœğ‘˜ are problem-related boundaries, Î“ quantifies the degree of non-iid, and ğ‘ğ‘˜ = 1
ğ¾ when the distribution of
data is balanced. Therefore, the convergence network overhead of SL is formulated as
(cid:34)

(cid:33)(cid:35)

(cid:32)

N O = Mğ¾

= O

(ğ¾ + 1) ğ¸ğº 2 +

ğ‘‡
ğ¸

M
ğœ–

ğ‘˜ + ğ¾ Î“ + ğ¾ğº 2
ğœ 2
ğ¸

+ ğ¾ğº 2

(cid:32) (cid:18)

(cid:34)

M
ğœ–

= O

(ğ¸ + 1)ğº 2 +

(cid:19)

Î“ + ğº 2
ğ¸

ğ¾ + ğ¸ğº 2 +

(cid:33)(cid:35)

,

ğœ 2
ğ‘˜
ğ¸

(2)

where M is the size of the model. Equation 2 explains the linear increase and the increase slope of network
overhead as the number of SL nodes ğ¾ scales. Besides, Equation 2 can be easily extended to the scenario where
the distribution of data is unbalanced.

6

IMPLICATION

According to findings in Section 4, we provide suggestions to developers deploying SL on real-world applications
on what is recommended to do and what is not recommended to do. Besides, we provide suggestions to researchers
on what to improve in SL.

6.1 What to Do?

No matter whether the dataset is balanced on SL nodes, developers can trust SL to achieve similar accuracy to
Centralized Learning (CL), according to Section 4.1 and Section 4.2. No matter whether the dataset is biased over
irrelevant features to the prediction results, developers can trust SL to achieve higher fairness than CL, according
to Section 4.3.

If developers are uncertain whether there exist low-quality nodes (LQNs) with polluted data, they should pay
attention to the size of the dataset and the convergence of SL. If the dataset is sufficient enough, SL is more likely
to converge, and developers can trust SL to achieve similar accuracy to CL, according to Section 4.4.

As for deployment, according to Section 4.1 and Section 4.5, SL nodes should be deployed on instances with
sufficient CPU resources, memory, and network bandwidth. SN nodes and License Server (LS) nodes should be
deployed on instances with sufficient memory. Other nodes have few special requirements of all resources, which
indicates that developers can deploy these nodes on economical instances to save costs.

Developers should feel free to increase the number of SN nodes, with few worries about consuming extra
resources, according to Section 4.5.1. This implication is meaningful in real-world applications because each SN
node represents an institution with new data and resources. For example, in a scenario that medical institutions
around the world are designing an effective vaccine of COVID-19 based on SL, a newly enrolled medical institution
can contribute to the research and development (R & D) process without requiring extra resources from other
institutions, which encourages more institutions to participate and discourages existing institutions from quitting.

6.2 What Not to Do?

When there exist LQNs with polluted data and the dataset is relatively small, SL is more likely to not converge
and fail to achieve similar accuracy to CL, according to Section 4.4. Therefore, developers are not recommended
to perform SL in this scenario.

Peers in the SL framework should feel cautious to increase the number of SL nodes. According to Section 4.5.2,
as the number of SL nodes increases, network overhead increases linearly, but faster. Blindly adding SL nodes in
one SN node would indeed save training time, but significantly increase network overhead. Therefore, it is not
recommended to blindly add SL nodes in one SN node.

, Vol. 1, No. 1, Article . Publication date: January 2022.

14

â€¢ Han et al.

6.3 Where to Improve?

According to Section 4.1 and Section 4.5, some SL nodes consume much more network bandwidth than others.
This finding leads to two disadvantages. First, peers with more overhead would complain about that unfairness
and tend to quit from the SL framework. Second, if hackers detect which SL node sends or receives much more
data than others, they would assume that this node is elected as the leader more frequently, and this would reduce
the difficulty from attacking all SL nodes to attacking only one SL node with the most overhead, to infer usersâ€™
identities and interests. As mentioned in Section 7, the Leader Election Algorithm (LEA) is not open-sourced.
Therefore, we assume that the underlying reason might be the unfairness of LEA.

This unfairness means that the probability of different SL nodes being elected as the leader is not equivalent or
proportional to computing power. Under this assumption, LEA could be Proof of Stake (PoS) [53] or its variants,
where the leader election possibility is proportional to stakes or account balance of nodes. Therefore, it leads to
severe unfairness when stakes on SL nodes are significantly different for some reasons. To mitigate the unfairness
of PoS, we can change the LEA into Proof of Work (PoW) [26] or its variants. In our PoW setting, each SL node
elects itself as the leader and frequently changes its block header to get different hash values. When one SL node
reaches the consensus that the calculated hash value is equal to or smaller than a certain given value, it would
broadcast to other nodes that it is the current leader. The difficulty of reaching consensus can be controlled by
targeting different amounts of hash value bits to satisfy requirements. Theoretically, PoW ensures the leader
election probability to be proportional to computing power. And when SL nodes share similar computing power,
this refinement for LEA ensures this probability to be equivalent.

We also conduct simulation experiments to measure the network overhead of the PoS-based LEA with different
stakes among nodes and the PoW-based LEA. In the PoS-based LEA, according to the results in Section 4.5.2 and
the above assumption, we assign stakes proportional to the network overhead of SL nodes. As shown in Figure 4,
in the PoS-based LEA, SL-1 and SL-2 are elected as leaders more frequently than others and consume much
more network bandwidth than others, which is consistent with the observations in Section 4.1 and Section 4.5.
In contrast, in the PoW-based LEA, the frequency of being elected as the leader and network overhead of SL
nodes are close to each other. Therefore, the results of simulation experiments would verify our assumption and
refinements. In future, we will try to cooperate with Hewlett Packard Enterprise (HPE) to improve the fairness of
the LEA in SL.

(a) PoS-based LEA with different stakes
among nodes

(b) PoW-based LEA

Fig. 4. Network Overhead of Different LEAs

7 THREATS TO VALIDITY

In this section, we discuss some threats to our methodology and experimental results of research questions.

, Vol. 1, No. 1, Article . Publication date: January 2022.

Demystifying Swarm Learning: A New Paradigm of Blockchain-based Decentralized Federated Learning

â€¢

15

Black-box testing. As we mention in Section 8, although Hewlett Packard Enterprise (HPE) open-sources the
Swarm Learning Library (SLL) in binary format, the core source code of the SL framework, especially blockchain-
related code, is not open-sourced. Therefore, we have to conduct black-box testing on SL and keep encapsulated
features of SL untouched. For example, as mentioned in Section 6, we are uncertain about the underlying reasons
for the unfairness of the Leader Election Algorithm (LEA) and have difficulty polishing it because it is not
open-sourced. Besides, SL uses a blockchain to securely onboard members, dynamically elect the leader, and
merge model parameters [40]. However, these features are not open-sourced, therefore, we are unknown about
the communication pattern of blockchain, the model aggregation algorithm, etc.

Failure of measuring connectivity. Actually, we design an extra research question to measure the connec-
tivity of SL. In this research question, we simulate a real-world scenario when SL nodes randomly enroll and quit
the SL framework, to measure the prediction accuracy of SL. We set a connection probability ğ‘ for each node
to individually decide whether to connect to the SL framework during each Synchronization Interval (SI). The
other experimental settings are similar to those of Section 4.1. Note that users can define min_peers to specify
the minimum number of peers required for synchronization in the SL framework. If the online SL nodes are
fewer than min_peers, the synchronization process would be blocked until the required number of online peers is
satisfied. Therefore, as for the relationship between the number of online SL nodes (ğ‘ ) and prediction accuracy,
we expect that only when ğ‘ is above min_peers, would the SL framework start to work, and the accuracy would
increase as ğ‘ increases. Besides, we expect that only when ğ‘ is above a convergence threshold ğ‘0, where
ğ‘0 â‰¥ ğ‘šğ‘–ğ‘›_ğ‘ğ‘’ğ‘’ğ‘Ÿğ‘ , would the SL framework start to converge because of the sufficiency of training data.

However, we fail to carry out this research question due to the following reasons. On the one hand, if we
treat each SL node as a node once enrolled in the SL framework, when we shut down its network connection, it
would only retry several times before its container is automatically shut down and removed, and the dataset and
learned parameters in its container would be wasted. On the other hand, if we simply abandon the dataset and
parameters in the container of each disconnected SL node and treat it as a newly enrolled one, we would hit the
wall of the capacity of licenses. HPE limits the capacity of licenses assigned for non-commercial use to have at
most 4 SN nodes and 16 SL nodes, and the licenses of disconnected nodes could not be revoked immediately. A
new SL node is supposed to wait for at least 30 minutes for the expiry of license tokens of disconnected nodes to
be assigned a new license for enrollment. The limited license capacity prevents us from continuously enrolling
new nodes into the SL framework. Besides, the Synchronization Intervals (SIs) of most experiments are shorter
than 30 minutes, which means SL nodes would waste redundant time in waiting for the assignment of a new
license. In future work, we will try to cooperate with HPE to measure the connectivity of SL and improve the
robustness of SL in face of network disconnection.

Differences between the results of SL and FL. As mentioned in Section 5, the convergence accuracy of SL
and FL is theoretically the same. However, except for the empirical study of Yang et al. [48], which focuses on the
impacts of heterogeneity in FL, there are few empirical studies comprehensively measuring the performance,
imbalance, fairness, fault tolerance, and scalability of either FL or SL, as far as we are concerned. Therefore, it
provides researchers and developers with valuable suggestions to measure those properties of SL. Besides, the
architectures of FL and SL are essentially different because of the introduction of blockchain, which gives us
more reasons to demystify SL.

Threats to the number of nodes. The experiments of the first 4 research questions are conducted when the
number of SN nodes is 1 and the number of SL nodes is 3 or 4. Therefore, some might argue that the results are
limited to the number of nodes, which can be disassembled into two aspects, i.e. prediction accuracy and resource
overhead. First, as mentioned in Section 5, theoretically, when the dataset is fixed, the number of nodes would
not affect convergence accuracy significantly. Second, the findings of Section 4.5 show that as the number of SN
nodes increases, resource overhead of each node would remain basically unchanged, while as the number of SL

, Vol. 1, No. 1, Article . Publication date: January 2022.

16

â€¢ Han et al.

nodes increases, computational overhead and network overhead would increase linearly. Therefore, as SN nodes
or SL nodes scale, resource overhead could be easily inferred.

8 RELATED WORK

In this section, we introduce related work about newly emerging distributed machine learning paradigms, i.e. FL
and SL, and state the position of our paper.

Federated learning (FL). To allow users to collectively reap the benefits of shared models trained from rich
data without transmitting raw data, McMahan et al. propose FL [25]. To improve communication efficiency,
KoneÄn`y et al. [13] propose structured update and sketched update. Bonawitz et al. [1] introduce the protocol
of FL, detailed system design on devices and servers, and some specific challenges of implementation. Yang et
al. [49] categorize FL into Horizontal FL, Vertical FL, and Federated Transfer Learning. Zhang et al. [51] propose
a FL incentive mechanism based on reputation and reverse auction theory. Liu et al. [21] adapt trained models
with similar data distributions to achieve better personalization results for FL. As for applications, researchers
utilize FL to help query suggestion [50], keyboard prediction [11], health data analytics [22], human activity
recognition [16], and user modeling [43]. As for empirical studies, Yang et al. [48] conduct the first empirical study
to characterize the impacts of heterogeneity in FL. However, FL uses central custodians to keep model parameters,
which could still be attacked to infer usersâ€™ identities and interests [3, 27, 33, 35, 38], even with privacy-preserving
deep learning techniques, such as the shared model [31], multi-party computation [9], transformation [7, 52],
partial sharing [29], model splitting [4], and encryption [6, 44]. Besides, the star-shaped architecture of FL
damages fault tolerance.

Decentralized Federated Learning and Swarm Learning (SL). To mitigate security and fault tolerance
concerns of FL, Lalitha et al. [14, 15] propose fully decentralized FL, where users update their belief by aggregating
information from their one-hop neighbors. Ramanan et al. [30] leverage Smart Contracts (SC) to coordinate the
round delineation, model aggregation, and update tasks in FL. Li et al. [17] allow each client to broadcast the
trained model to other clients, to aggregate its own model with received ones, and then to compete to generate
a block before its local training of the next round. Li et al. [19] use blockchain for global model storage and
local model update exchange and devise an innovative committee consensus mechanism to reduce consensus
computing and malicious attacks. Warnat et al. [39, 40] propose SL, a state-of-the-art decentralized FL paradigm
that unites edge computing, blockchain-based peer-to-peer networking, and coordination while maintaining
confidentiality without the need for a central coordinator, as we introduced in Section 2. Cooperating with Hewlett
Packard Enterprise (HPE)7, they develop a commercial SL software and open-source the Swarm Learning Library
(SLL) in binary format for non-commercial use under evaluation license. Numerous researchers and developers
are following their project on GitHub, however, they raise issues about the best practice and precautions of
SL deployment8, as mentioned in Section 1. As far as we are concerned, there are few empirical studies on SL
or blockchain-based decentralized FL paradigm, therefore, we conduct this measurement to fill the knowledge
gap between SL deployment and developers, and provide practical suggestions to developers and researchers.
As for applications, researchers utilize SL to help diagnosis of COVID-19, tuberculosis, leukaemia and lung
pathologies [40], feature selection for beta-amyloid and TAU pathology [42], the Internet of Vehicles [37], skin
lesion classification fairness [5], genomics data sharing [28], and risk prediction of cardiovascular events [41].

9 CONCLUSION

As far as we are concerned, this paper has conducted the first comprehensive study on Swarm Learning (SL), a
new paradigm of blockchain-based decentralized Federated Learning (FL), to fill the knowledge gap between SL

7https://www.hpe.com/us/en/home.html
8https://github.com/HewlettPackard/swarm-learning/issues

, Vol. 1, No. 1, Article . Publication date: January 2022.

Demystifying Swarm Learning: A New Paradigm of Blockchain-based Decentralized Federated Learning

â€¢

17

deployment and developers. We have conducted various experiments on 3 public datasets of 5 research questions,
i.e. performance, imbalance, fairness, fault tolerance, and scalability. The findings have evidenced that SL is
supposed to be suitable for most application scenarios, no matter whether the dataset is balanced, polluted,
or biased over irrelevant features. Besides, we have quantitatively analyzed the reasons behind findings, and
provided practical suggestions for developers to deploy SL on real-world applications and possible directions for
researchers to optimize SL.

, Vol. 1, No. 1, Article . Publication date: January 2022.

18

â€¢ Han et al.

REFERENCES

[1] Kallista A. Bonawitz, Hubert Eichner, Wolfgang Grieskamp, Dzmitry Huba, Alex Ingerman, Vladimir Ivanov, ChloÃ© Kiddon, Jakub
KoneÄnÃ½, Stefano Mazzocchi, Brendan McMahan, Timon Van Overveldt, David Petrou, Daniel Ramage, and Jason Roselander. 2019.
Towards Federated Learning at Scale: System Design. In Proceedings of Machine Learning and Systems 2019, MLSys 2019, Stanford, CA,
USA, March 31 - April 2, 2019.

[2] Robert S. Chen, Brendan Lucier, Yaron Singer, and Vasilis Syrgkanis. 2017. Robust Optimization for Non-Convex Objectives. In Advances
in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long
Beach, CA, USA. 4705â€“4714.

[3] Mauro Conti, Luigi Vincenzo Mancini, Riccardo Spolaor, and Nino Vincenzo Verde. 2016. Analyzing Android Encrypted Network Traffic

to Identify User Actions. IEEE Trans. Inf. Forensics Secur. 11, 1 (2016), 114â€“125.

[4] Hao Dong, Chao Wu, Zhen Wei, and Yike Guo. 2018. Dropping Activation Outputs With Localized First-Layer Deep Network for

Enhancing User Privacy and Data Security. IEEE Trans. Inf. Forensics Secur. 13, 3 (2018), 662â€“670.

[5] Di Fan, Yifan Wu, and Xiaoxiao Li. 2021. On the Fairness of Swarm Learning in Skin Lesion Classification. CoRR abs/2109.12176 (2021).
[6] Caroline Fontaine and Fabien Galand. 2007. A Survey of Homomorphic Encryption for Nonspecialists. EURASIP J. Inf. Secur. 2007

(2007).

[7] Yingwei Fu, Huaimin Wang, Kele Xu, Haibo Mi, and Yijie Wang. 2019. Mixup Based Privacy Preserving Mixed Collaboration Learning.

In Proceedings of the 13th IEEE International Conference on Service-Oriented System Engineering, SOSE â€™19.

[8] Yuhao Gao, Haoyu Wang, Li Li, Xiapu Luo, Guoai Xu, and Xuanzhe Liu. 2021. Demystifying Illegal Mobile Gambling Apps. In WWW

â€™21: The Web Conference 2021, Virtual Event / Ljubljana, Slovenia, April 19-23, 2021. ACM / IW3C2, 1447â€“1458.

[9] Oded Goldreich. 1998. Secure Multi-party Computation. Manuscript. Preliminary version 78 (1998).
[10] Jialiang Han, Yun Ma, Qiaozhu Mei, and Xuanzhe Liu. 2021. DeepRec: On-device Deep Learning for Privacy-Preserving Sequential
Recommendation in Mobile Commerce. In WWW â€™21: The Web Conference 2021, Virtual Event / Ljubljana, Slovenia, April 19-23, 2021.
ACM / IW3C2, 900â€“911.

[11] Andrew Hard, Kanishka Rao, Rajiv Mathews, FranÃ§oise Beaufays, Sean Augenstein, Hubert Eichner, ChloÃ© Kiddon, and Daniel Ramage.

2018. Federated Learning for Mobile Keyboard Prediction. CoRR abs/1811.03604 (2018).

[12] Gao Huang, Zhuang Liu, Laurens van der Maaten, and Kilian Q. Weinberger. 2017. Densely Connected Convolutional Networks. In 2017

IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017. 2261â€“2269.

[13] Jakub KoneÄnÃ½, H. Brendan McMahan, Daniel Ramage, and Peter RichtÃ¡rik. 2016. Federated Optimization: Distributed Machine Learning

for On-Device Intelligence. CoRR abs/1610.02527 (2016).

[14] Anusha Lalitha, Osman Cihan Kilinc, Tara Javidi, and Farinaz Koushanfar. 2019. Peer-to-peer Federated Learning on Graphs. CoRR

abs/1901.11173 (2019).

[15] Anusha Lalitha, Shubhanshu Shekhar, Tara Javidi, and Farinaz Koushanfar. 2018. Fully decentralized federated learning. In Third

workshop on Bayesian Deep Learning (NeurIPS).

[16] Chenglin Li, Di Niu, Bei Jiang, Xiao Zuo, and Jianming Yang. 2021. Meta-HAR: Federated Representation Learning for Human Activity

Recognition. In WWW â€™21: The Web Conference 2021, Virtual Event / Ljubljana, Slovenia, April 19-23, 2021. 912â€“922.

[17] Jun Li, Yumeng Shao, Kang Wei, Ming Ding, Chuan Ma, Long Shi, Zhu Han, and H. Vincent Poor. 2021. Blockchain Assisted Decentralized

Federated Learning (BLADE-FL): Performance Analysis and Resource Allocation. CoRR abs/2101.06905 (2021).

[18] Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, and Zhihua Zhang. 2020. On the Convergence of FedAvg on Non-IID Data. In

8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020.

[19] Yuzheng Li, Chuan Chen, Nan Liu, Huawei Huang, Zibin Zheng, and Qiang Yan. 2021. A Blockchain-Based Decentralized Federated

Learning Framework with Committee Consensus. IEEE Netw. 35, 1 (2021), 234â€“241.

[20] Fuqi Lin, Haoyu Wang, Liu Wang, and Xuanzhe Liu. 2021. A Longitudinal Study of Removed Apps in iOS App Store. In WWW â€™21: The

Web Conference 2021, Virtual Event / Ljubljana, Slovenia, April 19-23, 2021. ACM / IW3C2, 1435â€“1446.

[21] Bingyan Liu, Yao Guo, and Xiangqun Chen. 2021. PFA: Privacy-preserving Federated Adaptation for Effective Model Personalization. In

WWW â€™21: The Web Conference 2021, Virtual Event / Ljubljana, Slovenia, April 19-23, 2021. 923â€“934.

[22] Jing Ma, Qiuchen Zhang, Jian Lou, Li Xiong, and Joyce C. Ho. 2021. Communication Efficient Federated Generalized Tensor Factorization
for Collaborative Health Data Analytics. In WWW â€™21: The Web Conference 2021, Virtual Event / Ljubljana, Slovenia, April 19-23, 2021.
171â€“182.

[23] Yun Ma, Ziniu Hu, Diandian Gu, Li Zhou, Qiaozhu Mei, Gang Huang, and Xuanzhe Liu. 2020. Roaming Through the Castle Tunnels: An

Empirical Analysis of Inter-app Navigation of Android Apps. ACM Trans. Web 14, 3 (2020), 14:1â€“14:24.

[24] Yun Ma, Dongwei Xiang, Shuyu Zheng, Deyu Tian, and Xuanzhe Liu. 2019. Moving Deep Learning into Web Browser: How Far Can We

Go?. In The World Wide Web Conference, WWW 2019, San Francisco, CA, USA, May 13-17, 2019. ACM, 1234â€“1244.

[25] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise AgÃ¼era y Arcas. 2017. Communication-Efficient Learning
of Deep Networks from Decentralized Data. In Proceedings of the 20th International Conference on Artificial Intelligence and Statistics,

, Vol. 1, No. 1, Article . Publication date: January 2022.

Demystifying Swarm Learning: A New Paradigm of Blockchain-based Decentralized Federated Learning

â€¢

19

AISTATS 2017, 20-22 April 2017, Fort Lauderdale, FL, USA.

[26] Satoshi Nakamoto. 2008. Bitcoin: A peer-to-peer electronic cash system. Decentralized Business Review (2008), 21260.
[27] Milad Nasr, Reza Shokri, and Amir Houmansadr. 2019. Comprehensive Privacy Analysis of Deep Learning: Passive and Active White-box
Inference Attacks against Centralized and Federated Learning. In Proceedings of the 2019 IEEE Symposium on Security and Privacy, SP â€™19.
739â€“753.

[28] Marie Oestreich, Dingfan Chen, Joachim L Schultze, Mario Fritz, and Matthias Becker. 2021. Privacy considerations for sharing genomics

data. EXCLI journal 20 (2021), 1243.

[29] Le Trieu Phong, Yoshinori Aono, Takuya Hayashi, Lihua Wang, and Shiho Moriai. 2017. Privacy-Preserving Deep Learning: Revisited and
Enhanced. In Proceedings of Applications and Techniques in Information Security - 8th International Conference, ATIS â€™17 (Communications
in Computer and Information Science, Vol. 719). 100â€“110.

[30] Paritosh Ramanan and Kiyoshi Nakayama. 2020. BAFFLE : Blockchain Based Aggregator Free Federated Learning. In IEEE International

Conference on Blockchain, Blockchain 2020, Rhodes, Greece, November 2-6, 2020. 72â€“81.

[31] Sandra Servia RodrÃ­guez, Liang Wang, Jianxin R. Zhao, Richard Mortier, and Hamed Haddadi. 2018. Privacy-Preserving Personal Model
Training. In Proceedings of the 2018 IEEE/ACM Third International Conference on Internet-of-Things Design and Implementation, IoTDI â€™18.
153â€“164.

[32] Reza Shokri and Vitaly Shmatikov. 2015. Privacy-Preserving Deep Learning. In Proceedings of the 22nd ACM SIGSAC Conference on

Computer and Communications Security, Denver, CO, USA, October 12-16, 2015. 1310â€“1321.

[33] Mengkai Song, Zhibo Wang, Zhifei Zhang, Yang Song, Qian Wang, Ju Ren, and Hairong Qi. 2020. Analyzing User-Level Privacy Attack

Against Federated Learning. IEEE J. Sel. Areas Commun. 38, 10 (2020), 2430â€“2444.

[34] Mingxing Tan and Quoc V. Le. 2019. EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks. In Proceedings of the

36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA.

[35] Qinglong Wang, Amir Yahyavi, Bettina Kemme, and Wenbo He. 2015. I Know What You Did on Your Smartphone: Inferring App Usage

over Encrypted Data Traffic. In Proceedings of the 2015 IEEE Conference on Communications and Network Security, CNS â€™15. 433â€“441.

[36] Xiaosong Wang, Yifan Peng, Le Lu, Zhiyong Lu, Mohammadhadi Bagheri, and Ronald M. Summers. 2017. ChestX-Ray8: Hospital-Scale
Chest X-Ray Database and Benchmarks on Weakly-Supervised Classification and Localization of Common Thorax Diseases. In 2017
IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017. 3462â€“3471.

[37] Zhe Wang, Xinhang Li, Tianhao Wu, Chen Xu, and Lin Zhang. 2021. A Credibility-aware Swarm-Federated Deep Learning Framework

in Internet of Vehicles. CoRR abs/2108.03981 (2021).

[38] Zhibo Wang, Mengkai Song, Zhifei Zhang, Yang Song, Qian Wang, and Hairong Qi. 2019. Beyond Inferring Class Representatives:
User-Level Privacy Leakage From Federated Learning. In Proceedings of the 2019 IEEE Conference on Computer Communications, INFOCOM
â€™19. 2512â€“2520.

[39] Stefanie Warnat-Herresthal, Hartmut Schultze, Krishnaprasad Lingadahalli Shastry, Sathyanarayanan Manamohan, Saikat Mukherjee,
Vishesh Garg, Ravi Sarveswara, Kristian HÃ¤ndler, Peter Pickkers, N Ahmad Aziz, et al. 2020. Swarm Learning as a privacy-preserving
machine learning approach for disease classification. BioRxiv (2020).

[40] Stefanie Warnat-Herresthal, Hartmut Schultze, Krishnaprasad Lingadahalli Shastry, Sathyanarayanan Manamohan, Saikat Mukherjee,
Vishesh Garg, Ravi Sarveswara, Kristian HÃ¤ndler, Peter Pickkers, N Ahmad Aziz, et al. 2021. Swarm Learning for decentralized and
confidential clinical machine learning. Nature 594, 7862 (2021), 265â€“270.

[41] Annie M Westerlund, Johann S Hawe, Matthias Heinig, and Heribert Schunkert. 2021. Risk Prediction of Cardiovascular Events by
Exploration of Molecular Data with Explainable Artificial Intelligence. International Journal of Molecular Sciences 22, 19 (2021), 10291.
[42] Jianfeng Wu, Qunxi Dong, Jie Zhang, Yi Su, Teresa Wu, Richard J Caselli, Eric M Reiman, Jieping Ye, Natasha Lepore, Kewei Chen, et al.
2021. FEDERATED MORPHOMETRY FEATURE SELECTION FOR HIPPOCAMPAL MORPHOMETRY ASSOCIATED BETA-AMYLOID
AND TAU PATHOLOGY. bioRxiv (2021).

[43] Jinze Wu, Qi Liu, Zhenya Huang, Yuting Ning, Hao Wang, Enhong Chen, Jinfeng Yi, and Bowen Zhou. 2021. Hierarchical Personalized
Federated Learning for User Modeling. In WWW â€™21: The Web Conference 2021, Virtual Event / Ljubljana, Slovenia, April 19-23, 2021.
957â€“968.

[44] Pengtao Xie, Misha Bilenko, Tom Finley, Ran Gilad-Bachrach, Kristin E. Lauter, and Michael Naehrig. 2014. Crypto-Nets: Neural

Networks over Encrypted Data. CoRR abs/1412.6181 (2014).

[45] Mengwei Xu, Zhe Fu, Xiao Ma, Li Zhang, Yanan Li, Feng Qian, Shangguang Wang, Ke Li, Jingyu Yang, and Xuanzhe Liu. 2021. From
cloud to edge: a first look at public edge platforms. In IMC â€™21: ACM Internet Measurement Conference, Virtual Event, USA, November 2-4,
2021. ACM, 37â€“53.

[46] Mengwei Xu, Jiawei Liu, Yuanqiang Liu, Felix Xiaozhu Lin, Yunxin Liu, and Xuanzhe Liu. 2019. A First Look at Deep Learning Apps on

Smartphones. In The World Wide Web Conference, WWW 2019, San Francisco, CA, USA, May 13-17, 2019. ACM, 2125â€“2136.

[47] Mengwei Xu, Feng Qian, Qiaozhu Mei, Kang Huang, and Xuanzhe Liu. 2018. DeepType: On-Device Deep Learning for Input Personal-
ization Service with Minimal Privacy Concern. Proc. ACM Interact. Mob. Wearable Ubiquitous Technol. 2, 4 (2018), 197:1â€“197:26.

, Vol. 1, No. 1, Article . Publication date: January 2022.

20

â€¢ Han et al.

[48] Chengxu Yang, Qipeng Wang, Mengwei Xu, Zhenpeng Chen, Kaigui Bian, Yunxin Liu, and Xuanzhe Liu. 2021. Characterizing Impacts
of Heterogeneity in Federated Learning upon Large-Scale Smartphone Data. In WWW â€™21: The Web Conference 2021, Virtual Event /
Ljubljana, Slovenia, April 19-23, 2021. 935â€“946.

[49] Qiang Yang, Yang Liu, Tianjian Chen, and Yongxin Tong. 2019. Federated Machine Learning: Concept and Applications. ACM Trans.

Intell. Syst. Technol. 10, 2 (2019), 12:1â€“12:19.

[50] Timothy Yang, Galen Andrew, Hubert Eichner, Haicheng Sun, Wei Li, Nicholas Kong, Daniel Ramage, and FranÃ§oise Beaufays. 2018.

Applied Federated Learning: Improving Google Keyboard Query Suggestions. CoRR abs/1812.02903 (2018).

[51] Jingwen Zhang, Yuezhou Wu, and Rong Pan. 2021. Incentive Mechanism for Horizontal Federated Learning Based on Reputation and

Reverse Auction. In WWW â€™21: The Web Conference 2021, Virtual Event / Ljubljana, Slovenia, April 19-23, 2021. 947â€“956.

[52] Lingchen Zhao, Qian Wang, Qin Zou, Yan Zhang, and Yanjiao Chen. 2020. Privacy-Preserving Collaborative Deep Learning With

Unreliable Participants. IEEE Trans. Inf. Forensics Secur. 15 (2020), 1486â€“1500.

[53] Zibin Zheng, Shaoan Xie, Hongning Dai, Xiangping Chen, and Huaimin Wang. 2017. An Overview of Blockchain Technology:
Architecture, Consensus, and Future Trends. In 2017 IEEE International Congress on Big Data, BigData Congress 2017, Honolulu, HI, USA,
June 25-30, 2017. 557â€“564.

, Vol. 1, No. 1, Article . Publication date: January 2022.

