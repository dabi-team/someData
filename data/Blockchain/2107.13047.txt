RingBFT: Resilient Consensus over Sharded Ring Topology

Sajjad Rahnama

Suyash Gupta

Rohan Sogani Dhruv Krishnan Mohammad Sadoghi
Exploratory Systems Lab
University of California Davis

ABSTRACT

Scalability

2
2
0
2

r
a

M
3
2

]

B
D
.
s
c
[

2
v
7
4
0
3
1
.
7
0
1
2
:
v
i
X
r
a

The recent surge in federated data management applications has
brought forth concerns about the security of underlying data and
the consistency of replicas in the presence of malicious attacks. A
prominent solution in this direction is to employ a permissioned
blockchain framework that is modeled around traditional Byzantine
Fault-Tolerant (Bft) consensus protocols. Any federated application
expects its data to be globally scattered to achieve faster access. But,
prior works have shown that traditional Bft protocols are slow.

This has led to the rise of sharded-replicated blockchains. Ex-
isting Bft protocols for these sharded blockchains are efficient if
client transactions require access to a single-shard, but face perfor-
mance degradation if there is a cross-shard transaction that requires
access to multiple shards. As cross-shard transactions are common,
to resolve this dilemma, we present RingBFT, a novel meta-Bft pro-
tocol for sharded blockchains. RingBFT requires shards to adhere
to the ring order, and follow the principle of process, forward, and
re-transmit while ensuring the communication between shards is
linear. Our evaluation of RingBFT against state-of-the-art sharding
Bft protocols illustrates that RingBFT achieves up to 18√ó higher
throughput, gracefully scales to nearly 500 globally distributed
nodes, and achieves a peak throughput of 1.2 million transactions
per second.

1 INTRODUCTION
A growing interest in federated data management illustrates an
increased demand for multi-party database management [8, 15, 60].
In these multi-party systems, a common database is maintained
by several parties. As all of these parties cannot be at the same
location, so the system needs to be decentralized, which implies
that the database is distributed. There are two key ways in which a
distributed database can be managed by multiple parties: replication
and sharding [35, 37, 38, 57, 59, 65].

In a replicated system, each party holds a copy of the database. As
a result, the effects of each client transaction are replicated across
all the parties (replicas). In a sharded system, each party maintains a
subset (shard) of the database. Hence, each party can independently
handle incoming client transactions that require access to its shard.
One of the factors that advocates the use of replicated databases
is their ability to handle failure of one or more replicas. This neces-
sitates the need for keeping all the replicas at the same state. To
achieve this task, databases employ crash-fault tolerant protocols
such as Paxos [47] and Raft [55] to help all replicas reach a common
order for each client transaction. However, one or more replicas can
get compromised due to a malicious attack. A compromised replica
may wish: (i) to exclude transactions of some clients, (ii) to make the
system unavailable to clients, and (iii) to make replicas inconsistent.
These malicious attacks are so common that one estimate shows
that cyberattacks alone cost the U.S. economy around $57 billion
dollars in 2016 [54]. As a result, not all the replicas can be trusted.

)
s
/
n
x
t
(

t
u
p
h
g
u
o
r
h
T

l
a
t
o
T

1.4M
1.2M
1M
800K
600K
400K
200K
0

RingBFT
RingBFTùëã
Pbft
Sbft
HotStuff
Rcc
PoE
Zyzzyva

4

16

32

Number of Nodes

Figure 1: Comparing scalability of different Bft protocols.
In this figure, we depict throughput of single-primary,
multiple-primaries, geographically-scalable, and sharding
Bft protocols. For RingBFT, we require each shard to have
number of replicas stated on x-axis.

A recent solution to guarantee secure federated data manage-
ment is through the use of permissioned blockchain technology [30,
36]. These permissioned blockchains require their replicas to agree
on the order for each transaction by participating in a Byzantine-
Fault Tolerant (Bft) consensus protocol. Post consensus, each replica
logs the ordered transaction in a block that is part of an immutable
append-only ledger‚Äìblockchain. A blockchain is termed as im-
mutable because each new block includes the hash of the previous
block, and it allows verifying the state of the participating replicas.
In this paper, we present a novel meta-Bft protocol RingBFT that
guards against Byzantine attacks, achieves high throughput, and
incurs low latency. Our RingBFT protocol explores the landscape
of sharded-replicated databases, and helps to scale permissioned
blockchains, which in turn helps in designing efficient federated
data management systems. RingBFT aims to make consensus inex-
pensive even when transactions require access to multiple shards.
In the rest of this section, we motivate our design choices. To high-
light the need for RingBFT, we will be referring to Figure 1, which
illustrates the throughput attained by the system when employing
different Bft consensus protocols.

1.1 Challenges for Efficient BFT Consensus

Existing permissioned blockchain applications employ traditional
Bft protocols to achieve consensus among their replicas [3, 4, 9, 45].
Over the past two decades, these Bft protocols have undergone a se-
ries of evolutions to guarantee resilience against Byzantine attacks,
while ensuring high throughput and low latency. The seminal work
by Castro and Liskov [9, 10] led to the design of the first practical
Bft protocol, Pbft, which advocates a primary-backup paradigm
where primary initiates the consensus and all the backups follow
primary‚Äôs lead. Pbft achieves consensus among the replicas in three

 
 
 
 
 
 
Sajjad Rahnama

Suyash Gupta

Rohan Sogani Dhruv Krishnan Mohammad Sadoghi

phases, of which two require quadratic communication complexity.
Following this, several exciting primary-backup protocols, such as
Zyzzyva [46], Sbft [22], and PoE [28], have been proposed that try
to yield higher throughputs from Bft consensuses. We use Figure 1
to illustrate the benefits of these optimizations over Pbft. Prior
works [2, 32] have illustrated that these single primary protocols
are essentially centralized and prevent scaling the system to a large
number of replicas.

An emerging solution to balance load among replicas is to employ
multi-primary protocols like Honeybadger [53] and Rcc [29, 31]
that permit all replicas to act as primaries by running multiple
consensuses concurrently. However, multi-primary protocols also
face scalability limitations as despite concurrent consensuses, each
transaction requires communication between all the replicas. More-
over, if the replicas are separated by geographically large distances,
then these protocols incur low throughput and high latency due to
low bandwidth and high round-trip time. This led to the design of
topology-aware protocols, such as Steward [2] and Geobft [32],
which cluster replicas based on their geographical distances. For
instance, Geobft expects each cluster to first locally order its client
transaction by running the Pbft protocol, and then exchange this
ordered transaction with all the other clusters. Although Geobft is
highly scalable, it necessitates total replication, which forces com-
municating large messages among geographically distant replicas.

1.2 The Landscape for Sharding

To mitigate the costs associated with replicated databases, a com-
mon strategy is to employ the sharded-replicated paradigm [56].
In a sharded-replicated database, the data is distributed across a
set of shards where each shard manages a unique partition of the
data. Further, each shard replicates its partition of data to ensure
availability under failures. If each transaction accesses only one
shard, then these sharded systems can fetch high throughput as
consensus is restricted to a subset of replicas.

AHL [14] was the first permissioned blockchain system to em-
ploy principles of sharding. AHL‚Äôs novel design helps to scale block-
chain systems to hundreds of replicas across the globe and achieve
high throughput for single-shard transactions. To tackle cross-shard
transactions that require access to data in multiple shards, AHL
designates a set of replicas as a reference committee, which globally
orders all such transactions. Following AHL‚Äôs design, Sharper [4]
presents a sharding protocol that eliminates the barrier to rely on
the reference committee for ordering cross-shard transactions, but
necessitates global and quadratic communication among all replicas
of all the participating shards.

Why RingBFT? Decades of research in database community
has illustrated that cross-shard transactions are common [13, 16, 38,
65]. In fact, heavy presence of these cross-shard transactions has
led to development of several concurrency control [7, 38, 57] and
commit protocols [23, 35, 61]. Hence, in this paper, we present our
RingBFT protocol that significantly reduces the costs associated
with cross-shard transactions.

Akin to AHL and Sharper, RingBFT assumes that the read-write
sets of each transaction are known prior to the start of consensus.
Given this, RingBFT guarantees consensus for each cross-shard

transaction in at most two rotations around the ring. In spe-
cific, RingBFT envisions each shard participating in multiple cir-
cular flows or rings, simultaneously. For each cross-shard trans-
action, RingBFT follows the principle of process, forward, and
re-transmit. This implies that each shard performs consensus on
the transaction and forwards it to the next shard. This flow contin-
ues until each shard is aware of the fate of the transaction. How-
ever, the real challenge with cross-shard transactions is to manage
conflicts and to prevent deadlocks, which RingBFT achieves by
requiring cross-shard transactions to travel in ring order. Despite
all of this, RingBFT ensures communication between the shards
is linear, exhibiting a neighbor-to-neighbor communication. This
minimalistic design has allowed RingBFT to achieve unprecedented
gains in throughput and has allowed us to scale Bft protocols to
nearly 500 nodes globally. The benefits of our RingBFT protocol are
visible from Figure 1 where we run RingBFT in a system of 9 shards
with each shard having 4, 16 and 32 replicas. Further, we show the
throughput with 0 (RingBFT) and 15% (RingBFTùëã ) cross-shard
transactions. We now list down our contributions.

(1) We present a novel meta-Bft protocol for sharded-replicated
permissioned blockchain systems that requires participating shards
to adhere to the ring order. We term RingBFT as ‚Äúmeta‚Äù because it
can employ any single-primary protocols within each shard.

(2) Our RingBFT protocol presents a scalable consensus for
cross-shard transactions that neither depends on any centralized
committee nor requires all-to-all communication.

(3) We show that the cross-shard consensus provided by Ring-

BFT is safe, and live, despite any Byzantine attacks.

(4) We evaluate RingBFT on our ResilientDB1 framework [25‚Äì
28, 31, 32, 34, 58] against two state-of-the-art Bft protocols for
permissioned sharded systems, AHL [14], and Sharper [4]. Our
results show that RingBFT easily scales to 428 globally-distributed
nodes, and achieves up to 18√ó and 4√ó times higher throughput than
AHL and Sharper, respectively.

2 CROSS-SHARD DILEMMA

For any sharded system, ordering a single-shard transaction is triv-
ial as such a transaction requires access to only one shard. This
implies that achieving consensus on a single-shard transaction just
requires running a standard Bft protocol. Further, single-shard
transactions support parallelism as each shard can order its transac-
tion in parallel, this without any communication between shards.
On the other hand, cross-shard transactions are complex. Not
only do they require communication between shards but also their
fate depends on the consent of each of the involved shards. Further,
two or more cross-shard transactions can conflict if they require
access to same data. Such conflicts can cause one or more trans-
actions to abort or worse, can create a deadlock. Hence, we need
an efficient protocol to order these cross-shard transactions, which
ensures that the system is both safe and live.

Designated Committee (AHL). One way to order cross-shard
transactions is to designate a set of replicas with this task. AHL [14]
defines a reference committee that assigns an order to each cross-
shard transaction, which requires running Pbft protocol among all

1ResilientDB is open-sourced at https://resilientdb.com/ and its source-code is avail-
able at https://github.com/resilientdb/resilientdb.

RingBFT: Resilient Consensus over Sharded Ring Topology

the members of the reference committee. Next, reference committee
members run the Two-phase commit (2pc) protocol with all the
replicas of involved shards. Notice that the 2pc protocol requires:
(1) each shard to send a vote to the reference committee, (2) refer-
ence committee collects these votes and takes a decision (abort or
commit), and (3) each shard implements the decision. Firstly, this
solution requires each shard to run the Pbft protocol to decide on
the vote. Secondly, reference committee needs to again run Pbft
to reach a common decision. Finally, these multiple phases of 2PC
require all-to-all communication between the replicas of each shard
and the replicas of reference committee.

Initiator Shard (Sharper). Another way to process a cross-
shard transaction is to designate one of the involved shards as the
initiator shard. Sharper [4] employs this approach by requiring
each cross-shard transaction to be managed by the primary replica
of one of the involved shards. This initiator primary proposes the
transaction to the primaries of other shards. Next, these primaries
propose this transaction within their own shards. Following this
there is an all-to-all communication between replicas of all the
involved shards.

3 SYSTEM MODEL

To explain our RingBFT protocol in detail, we first lay down some
notations and assumptions. Our system comprises of a set ùîñ of
shards where each shard S provides a replicated service. In spe-
cific, each shard S manages a unique partition of the data, which is
replicated by a set ‚ÑúS of replicas.

In each shard S, there are F ‚äÜ ‚ÑúS Byzantine replicas, of which
N F = ‚ÑúS \F are non-faulty replicas. We expect non-faulty replicas
to follow the protocol and act deterministic, that is, on identical
inputs, all non-faulty replicas must produce identical outputs. We
write z = |ùîñ| to denote the total number of shards and n = |‚ÑúS|,
f = |F |, and nf = |N F | to denote the number of replicas, faulty
replicas, and non-faulty replicas, respectively, in each shard.

Fault-Tolerance Requirement. Traditional, Bft protocols such
as Pbft, Zyzzyva, and Sbft expect a total replicated system where
the total number of Byzantine replicas are less than one-third of the
total replicas in the system. In our sharded-replicated model, we
adopt a slightly weaker setting where at each shard the total number
of Byzantine replicas are less than one-third of the total replicas in
that shard. In specific, at each shard S, we expect n ‚â• 3f + 1. This
does not imply that we want each shard to have an equal number of
replicas. Each shard can have a different number of replicas till less
than one-third are byzantine. This requirement is in accordance
with existing works in Byzantine sharding space [4, 14].

Cross-Shard Transactions. Each shard S ‚àà ùîñ can receive a
single-shard or cross-shard transaction. A single-shard transaction
for S leads to intra-shard communication, that is, all the messages
necessary to order this transaction are exchanged among the repli-
cas of S. On the other hand, a cross-shard transaction requires
access to data from a subset of shards (henceforth we use the ab-
breviation cst to refer to a cross-shard transaction). We denote
this subset of shards as ‚Ñë where ‚Ñë ‚äÜ ùîñ, and refer to it as involved
shards. Each cst can be termed as simple or complex. A simple cst is
a collection of fragments where each shard can independently run
consensus and execute its fragment. On the other hand, a complex

cst includes dependencies, that is, an involved shard may require
access to data from other involved shards to execute its fragment.
Deterministic Transactions. We define a deterministic trans-
action as the transaction for which the data-items it will read/write
are known prior to the start of the consensus [57, 65]. Given a
deterministic transaction, a replica can determine which data-items
accessed by this transaction are present in its shard.

Ring Order. We assume shards in set ùîñ are logically arranged in
a ring topology. In specific, each shard S ‚àà ùîñ has a position in the
ring, which we denote by id(S), 1 ‚â§ id(S) ‚â§ |ùîñ|. RingBFT employs
these identifiers to specify the flow of a cst or ring order. For in-
stance, a simple ring policy can be that each cst is processed by the
involved shards in the increasing order of their identifiers. RingBFT
can also adopt other complex permutations of these identifiers for
determining the flow across the ring.

Authenticated Communication. We assume that each mes-
sage exchanged among clients and replicas is authenticated. Fur-
ther, we assume that Byzantine replicas are unable to impersonate
non-faulty replicas. Notice that authenticated communication is a
minimal requirement to deal with Byzantine behavior. For intra-
shard communication, we employ cheap message authentication
codes (MACs), while for cross-shard communication we employ dig-
ital signatures (DS) to achieve authenticated communication. MACs
facilitate symmetric cryptography by requiring each pair of commu-
nicating nodes to share a secret key. We expect non-faulty replicas
to keep their secret keys hidden. DS follow asymmetric cryptogra-
phy. In specific, prior to signing a message, each replica generates a
pair of public-key and private-key. The signer keeps the private-key
hidden and uses it to sign a message. Each receiver authenticates
the message using the corresponding public-key. Although MACs
are cheaper than DS, they cannot guarantee non-repudiation. We
require non-repudiation property during cross-shard communica-
tion as it helps to prove that the message communicated was sent
by the sender and the message‚Äôs contents were not fabricated.

In the rest of this manuscript, if a message ùëö is signed by a
replica r using DS, we represent it as ‚ü®ùëö‚ü©r to explicitly identify
replica r. Otherwise, we assume that the message employs MAC.
To ensure message integrity, we employ a collision-resistant cryp-
tographic hash function ùêª (¬∑) that maps an arbitrary value ùë£ to a
constant-sized digest ùêª (ùë£) [43]. We assume that there is a negligible
probability to find another value ùë£ ‚Ä≤, ùë£ ‚â† ùë£ ‚Ä≤, such that ùêª (ùë£) = ùêª (ùë£ ‚Ä≤).
Further, we refer to a message as well-formed if a non-faulty receiver
can validate the DS or MAC, verify the integrity of the message
digest, and determine that the sender of the message is also the
creator.

4 RINGBFT CONSENSUS PROTOCOL

To achieve efficient consensus in sharded-replicated databases, we
employ our RingBFT protocol. While designing our RingBFT pro-
tocol, we set following goals:

(G1) Inexpensive consensus of single-shard transactions.
(G2) Flexibility of employing different existing consensus proto-

cols for intra-shard consensus.

(G3) Deadlock-free two-ring consensus of deterministic cross-

shard transactions.

(G4) Cheap communication between globally distributed shards.

Sajjad Rahnama

Suyash Gupta

Rohan Sogani Dhruv Krishnan Mohammad Sadoghi

gClient c1
gClient c2
gClient c3

T3

Shard S

Shard U

Shard V

ÔáÄ2 ÔáÄ2 ÔáÄ2
T1

T2

Pbft

Pbft
ùë£1

ùë£3

ùë£2

Pbft
g Client c1
g Client c2
g Client c3

Figure 2: RingBFT consensus for single-shard transactions.
Each of the three shards S, U, and V receive transactions T1,
T2, and T3 from their respective clients c1, c2, and c3 to ex-
ecute. Each shard independently run Pbft consensus, and
sends responses to respective clients.

We define the safety and liveness guarantees provided by our

RingBFT protocol.
Definition 4.1. Let ùîñ be a system of shards and ‚ÑúS be a set of
replicas in some shard S ‚àà ùîñ. Each run of a consensus protocol in
this system should satisfy the following requirements:
Involvement Each S ‚àà ùîñ processes a transaction if S ‚àà ‚Ñë.
Termination Each non-faulty replica in ‚ÑúS executes a transaction.
Non-divergence (intra-shard) All non-faulty replicas in ‚ÑúS exe-

cute the same transaction.

Consistence (cross-shard) Each non-faulty replica in ùîñ executes

a conflicting transaction in same order.

In traditional replicated systems, non-divergence implies safety,
while termination implies liveness. For a sharded-replicated system
like RingBFT, we need stronger guarantees. If a transaction requires
access to only one shard, safety is provided by involvement and
non-divergence, while termination sufficiently guarantees liveness.
For a cross-shard transaction, to guarantee safety, we also need
consistency apart from involvement and non-divergence, while
liveness is provided using involvement and termination.

RingBFT guarantees safety in an asynchronous setting. In such
a setting, messages may get lost, delayed, or duplicated, and up
to f replicas in each shard may act Byzantine. However, RingBFT
can only provide liveness during periods of synchrony. Notice that
these assumptions are no harder than those required by existing
protocols [4, 9, 14].

4.1 Single-Shard Consensus

To order and execute single-shard transactions is trivial. For this
task, RingBFT employs one of the many available primary-backup
consensus protocols and runs them at each shard. In the rest of
this section, without the loss of generality, we assume that Ring-
BFT employs the Pbft consensus protocol to order single-shard
transactions. We use the following example to explain RingBFT‚Äôs
single-shard consensus.
Example 4.2. Assume a system that comprises of three shards S,
U, and V. Say client c1 sends T1 to S, c2 sends T2 to U, and client c3

sends T3 to V. On receiving the client transaction, the primary of
each shard initiates the Pbft consensus protocol among its replicas.
Once each replica successfully orders the transaction, it sends a
response to the client. Such a flow is depicted in Figure 2.

It is evident from Example 4.2 that there is no communication
among the shards. This is the case because each transaction requires
access to data available inside only one shard. Hence, ordering
single-shard transactions for shard S requires running the Pbft
protocol among the replicas of S without any synchronization with
other shards.

4.2 Cross-Shard Consensus: Process & Forward

In this section, we illustrate how RingBFT guarantees consensus
of every deterministic cross-shard transaction (cst) in at most two
rotations across the ring. To order a cst, RingBFT requires shards to
adhere to the ring order, and follow the principle of process, forward,
and re-transmit while ensuring the communication between shards
is linear. We use the following example to illustrate what we mean
by following the ring order.

Example 4.3. Assume a system that comprises of four shards S, U,
V, and W where the ring order has been defined as S ‚Üí U ‚Üí V ‚Üí
W. Say client c1 wants to process a transaction TS,U,V that requires
access to data from shards S, U, and V, and client c2 wants to process
a transaction TU,V,W that requires access to data from shards U, V,
and W (refer to Figure 3). In this case, client c1 sends its transaction
to the primary of shard S while c2 sends its transaction to primary
of U. On receiving TS,U,V, replicas of S process the transaction and
forward it to replicas of U. Next, replicas of U process TS,U,V and
forward it to replicas of V. Finally, replicas of V process TS,U,V and
send it back to replicas of S, which reply to client c1. Similar flow
takes place while ordering transaction TU,V,W.

Although Example 4.3 illustrates RingBFT‚Äôs design, it is unclear
how multiple concurrent cst are ordered in a deadlock-free manner.
In specific, we wish to answer following questions regarding the
design of our RingBFT protocol.

(Q1) Can a shard concurrently order multiple cst?
(Q2) How does RingBFT handle conflicting transactions?
(Q3) Can shards running RingBFT protocol deadlock?
(Q4) How much communication is required between two shards?
To answer these questions, we first present the transactional flow
of a cross-shard transaction undergoing RingBFT consensus, fol-
lowing which we lay down the steps of our RingBFT consensus
protocol.

4.2.1 Cross-shard Transactional Flow. RingBFT assumes shards are
arranged in a logical ring. For the sake of explanation, we assume
the ring order of lowest to highest identifier. For each cst, we denote
one shard as the initiator shard, which is responsible for starting
consensus on the client transaction. How do we select the initiator
shard? Of all the involved shards a cst accesses, the shard with the
lowest identifier in ring order is denoted as the initiator shard.

RingBFT also guarantees consensus for each deterministic cst in
at most two rotations across the ring. This implies that for achieving
consensus on a deterministic cst, each involved shard S ‚àà ‚Ñë needs
to process it at most two times. Notice that if a cst is simple, then

RingBFT: Resilient Consensus over Sharded Ring Topology

TS,U,V
g

Client c1

process

ÔáÄ2

Shard S

ÔáÄ2

Shard V

forward

forward

process

TU,V,W
g

Client c2

ÔáÄ2

Shard U

ÔáÄ2

Shard W

Figure 3: RingBFT‚Äôs concurrent consensus of two cross-shard
transactions TS,U,V and TU,V,W across four shards. The pre-
scribed ring order is S ‚Üí U ‚Üí V ‚Üí W.

a single rotation around the ring is sufficient to ensure that each
involved shard S safely executes its fragment.

Prior to presenting our RingBFT‚Äôs consensus protocol that safely
orders each cst, we sketch the flow of a cst in Figure 4. In this
figure, we assume a system of four shards: S, U, V, and W where
id(S) < id(U) < id(V) < id(W). The client creates a transaction
TS,U,W that requires access to data in shards S, U, and W and sends
this transaction to the primary pùëÜ of S. On receiving this transaction,
pùëÜ initiates the Pbft consensus protocol (local replication) among
its replicas. If the local replication is successful, then all the replicas
of S lock the corresponding data. This locking of data-items in the
ring-order helps in preventing deadlocks. Next, replicas of S for-
ward the transaction to replicas of shard U. Notice that only linear
communication takes place between replicas of S and U. Hence, to
handle any failures, replicas of U share this message among them-
selves. Next, replicas of U also follow similar steps and forward
transaction to W. As W is the last shard in the ring of involved
shards, it goes ahead and executes the cst if all the dependencies are
met. Finally, replicas of shards S and U also execute the transaction
and replicas of S send the result of execution to the client.

4.3 Cross-Shard Consensus Algorithm

We use Figure 5 to present RingBFT‚Äôs algorithm for ordering cross-
shard transactions. Next, we discuss these steps in detail.

4.3.1 Client Request. When a client c wants to process a cross-
shard transaction T‚Ñë, it creates a ‚ü®T‚Ñë‚ü©c message and sends it to the
primary of the first shard in ring order. As part of this transaction,
the client c specifies the information regarding all the involved
shards (‚Ñë), such as their identifiers and the necessary read-write
sets of each shard. Notice that the client signs this message using
DS to prevent repudiation attacks.

4.3.2 Client Request Reception. When the primary pS of shard S
receives a client request T‚Ñë, it first checks if the message is well-
formed. If this is the case, then pS checks if among the set of involved
shards ‚Ñë, S is the first shard in ring order. If this condition is met,
then pS assigns this request a linearly increasing sequence number
ùëò, calculates the digest Œî, and broadcasts a Preprepare message to
all the replicas ‚ÑúS of its shard. In the case when S is not the first
shard in the ring order, pS forwards the transaction to the primary
of the appropriate shard.

4.3.3 Pre-prepare Phase. When a replica r ‚àà ‚ÑúS receives the
Preprepare message from pS, it checks if the request is well-formed.
If this is the case and if r has not agreed to support any other re-
quest from pS as the ùëò-th request, then it broadcasts a Prepare
message in its shard S.

4.3.4 Prepare Phase. When a replica r ‚àà ‚ÑúS receives identical
Prepare messages from nf distinct replicas, it gets an assurance
that a majority of non-faulty replicas are supporting this request.
At this point, each replica r broadcasts a Commit message to all
the replicas in S. Once a transaction passes this phase, the replica r
marks it prepared.

4.3.5 Commit and Data Locking. When a replica r receives well-
formed identical Commit messages from nf distinct replicas in S, it
checks if it also prepared this transaction at same sequence number.
If this is the case, RingBFT requires each replica r to lock all the
read-write sets that transaction T‚Ñë needs to access in shard S. In
RingBFT, we allow replicas to process and broadcast Prepare and
Commit messages out-of-order, but require each replica to acquire
locks on data in transactional sequence order. This out-of-ordering
helps replicas to continuously perform useful work by concurrently
participating in consensus of several transactions. To achieve these
tasks, each replica r tracks the maximum sequence number (ùëòmax),
which indicates the sequence number of the last transaction to lock
data. If sequence number ùëò for a transaction T‚Ñë is greater than
ùëòmax + 1, we store the transaction in a list ùúã until transaction at
ùëòmax +1 has acquired the locks. Once the ùëòmax +1-th transaction has
acquired locks, we gradually release transactions in ùúã until there
is a transaction that wishes to lock already locked data-fragments.
We illustrate this through the following example.

2,ùëè , T3,ùëé, and T4,ùëê before T1,ùëé. Hence, ùúã = {T

Example 4.4. Assume the use of following notations for four trans-
actions and the data-fragments they access at shard S: T1,ùëé, T
2,ùëè ,
T3,ùëé, and T4,ùëê . For instance, T1,ùëé implies that transaction at sequence
1 requires access to data-item ùëé. Next, due to out-of-order message
processing, assume a replica r in S receives nf Commit messages
2,ùëè, T3,ùëé, T4,ùëê }.
for T
Once r receiving nf Commit messages for T1,ùëé, it locks data-item ùëé
and extracts T
2,ùëè wishes to lock a distinct data-item,
so r continues processing T
2,ùëè . Next, r moves to T3,ùëé but it cannot
process T3,ùëé due to lock-conflicts. Hence, it places back T3,ùëé in ùúã
and stops processing transactions in ùúã until lock is available for
T3,ùëé.

2,ùëè from ùúã. As T

Notice that if the client transaction T‚Ñë is a single-shard transac-
tion, it requires access to data-items in only this shard. In such a
case, this commit phase is the final phase of consensus and each
replica executes T‚Ñë and replies to the client when the lock for the
corresponding data-item is available.

Forward to next Shard via Linear Communication. Once a
4.3.6
replica r in S locks the data corresponding to cst T‚Ñë, it sends a
Forward message to only one replica q of the next shard in ring
order. As one of the key goals of RingBFT is to ensure communica-
tion between two shards is linear, so we design a communication
primitive that builds on top of the optimal bound for communica-
tion between two shards [32, 41]. We define RingBFT‚Äôs cross-shard
communication primitive as follows:

Sajjad Rahnama

Suyash Gupta

Rohan Sogani Dhruv Krishnan Mohammad Sadoghi

TS,U,W

Local Pbft

Consensus
on TS,U,W

Local Pbft

Consensus
on TS,U,W

E
x
e
c
u
t
e

E
x
e
c
u
t
e

Local

Local

Forward

Local

Local

Global

Local

Local Pbft

Consensus
on TS,U,W
Local

E
x
e
c
u
t
e

Global

Local

Client

c

pS
r3
r2
r1

pU
r3
r2
r1

pV
r3
r2
r1

pW
r3
r2
r1

S

U

V

W

Request

Replication

Sharing

Replication

Sharing

Sharing

Replication

Sharing

Sharing

Response

Round 1

Round 2

Figure 4: Representation of the normal-case flow of RingBFT in a system of four shards where client sends a cross-shard
transaction TS,U,W that requires access to data in three shards: S, U, and W.

Linear Communication Primitive. In a system ùîñ of shards,
where each shard S, U ‚àà ùîñ has at most f Byzantine replicas, if each
replica in shard S communicates with a distinct replica in shard U,
then at least f + 1 non-faulty replicas from S will communicate with
f + 1 non-faulty replicas in U.

Our linear communication primitive guarantees that to reliably
communicate a message ùëö between two shards requires only send-
ing a linear number of messages in comparison to protocols like
AHL and Sharper which require quadratic communication. Using
this communication primitive, to communicate message ùëö from
shard S to shard U, we need to exchange only n messages.

So, how does RingBFT achieve this task? We require each replica
of S to initiate communication with the replica of U having the same
identifier. Hence, replica r of shard S sends a Forward message to
replica q in shard U such that id(r) = id(q). By sending a Forward
message, r is requesting q to initiate consensus on ‚ü®T‚Ñë‚ü©c . For q to
support such a request, it needs a proof that ‚ü®T‚Ñë‚ü©c was successfully
ordered in shard S. Hence, r includes the DS on Commit messages
from nf distinct replicas (Line 16).

Until now, we assumed that each shard has an equal number of
replicas. If we forgo this assumption, it will not affect the intra-shard
consensus, that is, the Bft consensus protocol running at each shard
remains unchanged. Further, the transaction execution explained
in the next section also remains unaffected. The only visible change
occurs in our linear communication primitive. However, even this
change does not impact the correctness of our RingBFT protocol as
our linear communication primitive builds on the optimal bound
for communication between two shards, which permits shards

to have a different number of replicas while guaranteeing linear
communication complexity [32, 41].

Finally, in Section 5.1.2, we illustrate how our linear communi-

cation primitive can handle attacks by byzantine replicas.

4.3.7 Execution and Final Rotation. Once a client request has been
ordered on all the involved shards, we call it one complete rotation
around the ring. This is a significant event because it implies that
all the necessary data-fragments have been locked by each of the
involved shards. If a cst is simple, then each shard can indepen-
dently execute its fragment without any further communication
between the shards. In the case a cst is complex, at the end of the
first rotation, the replicas of the first shard in ring order (S) will
receive a Forward message from the replicas of the last shard in
ring order.

Next, the replicas of S will attempt to execute parts of transac-
tion, which are their responsibility. Post execution, replicas of S
send Execute messages to the replicas in next shard using our com-
munication primitive. Notice that the Execute message includes
updated write sets (Œ£‚Ñë), which help in resolving any dependencies
during execution. Finally, when the execution is completed across
all the shards, the first shard in ring order replies to the client.

5 UNCIVIL EXECUTIONS

In previous sections, we discussed transactional flows under the
assumption that the network is stable and replicas will follow the
stated protocol. However, any Byzantine-Fault Tolerant protocol
should provide safety under asynchronous settings and liveness in
the period of synchrony even if up to f replicas are Byzantine.

RingBFT: Resilient Consensus over Sharded Ring Topology

Initialization:
// ùëòmax :=0 (maximum sequence number in shard S)
// Œ£‚Ñë := ‚àÖ (set of data-fragments of each shard)
// ùúã := ‚àÖ (list of pending transactions at a replica)

Client-role (used by client c to request transaction T‚Ñë) :

1: Sends ‚ü®T‚Ñë ‚ü©c to the primary pS of shard S.
2: Awaits receipt of messages Response( ‚ü®T‚Ñë ‚ü©c , ùëò, ùëü ) from f + 1 replicas of S.
3: Considers T‚Ñë executed, with result ùëü , as the ùëò-th transaction.

// This event is only triggered at the primary replica of each shard.
Primary-role (running at the primary pS of shard S) :

4: event pS receives ‚ü®T‚Ñë ‚ü©c do
5:
6:
7:
8:
9:

else

if S ‚àà ‚Ñë ‚àß id(S) = FirstInRingOrder(‚Ñë) then

Calculate digest Œî := ùêª ( ‚ü®T ‚ü©c ).
Broadcast Preprepare( ‚ü®T‚Ñë ‚ü©c , Œî, ùëò) in shard S (order at sequence ùëò).

Send to primary pU of shard U, U ‚àà ùîñ ‚àß id(U) = FirstInRingOrder(‚Ñë)

// This event is only triggered at a non-primary replica.
Non-Primary Replica-role (running at the replica r of shard S) :

10: event r receives Preprepare( ‚ü®T‚Ñë ‚ü©c , Œî, ùëò) from pS such that:

message is well-formed, and r did not accept a ùëò-th proposal from pS. do
Broadcast Prepare(Œî, ùëò) to replicas in ‚ÑúS.

11:

// Following events are triggered at every replica irrespective of whether it is the primary or a
non-primary replica.
Replica-role (running at any replica r of shard S) :

12: event r receives well-formed Prepare(Œî, ùëò) messages from nf replicas in S do
13:

Broadcast ‚ü®Commit(Œî, ùëò) ‚ü©r to replicas in ‚ÑúS.

14: event r receives nf ùëö := ‚ü®Commit(Œî, ùëò) ‚ü©q messages such that:

each message ùëö is well-formed and is sent by a distinct replica q ‚àà ‚ÑúS. do
U be the shard to forward such that id(U) = NextInRingOrder(‚Ñë).
ùê¥ := set of DS of these nf messages.
if ùëò = ùëòmax + 1 // Forward to next shard then

Lock data-fragment corresponding to ‚ü®T‚Ñë ‚ü©c .
Send ‚ü®Forward( ‚ü®T‚Ñë ‚ü©c , ùê¥, ùëö, Œî, ) ‚ü©r to replica o, where o ‚àà ‚ÑúU ‚àß id(r) = id(o)

else

Store ‚ü®Forward( ‚ü®T‚Ñë ‚ü©c , ùê¥, ùëö, Œî, ) ‚ü©r in ùúã .
while ùúã ! = ‚àÖ // Pop out waiting transaction. do

Extract transaction at ùëòmax + 1 from ùúã (if any).
if Corresponding data-fragment is not locked then

ùëòmax = ùëòmax + 1
Follow lines 18 and 19.

else

Store transaction at ùëòmax in ùúã and exit the loop.

// Locally share any message from previous shard.

29: event r receives message ùëö := ‚ü®message-type‚ü©q such that:
ùëö is well-formed and sent by replica q, where

id(U) = PrevInRingOrder(‚Ñë), q ‚àà ‚ÑúU ‚àß id(r) = id(q) do

30:

Broadcast ùëö to all replicas in S.

// Forward message from previous shard.

31: event r receives f + 1 ùëö‚Ä≤ := ‚ü®Forward( ‚ü®T‚Ñë ‚ü©c , ùê¥, ùëö, Œî) ‚ü©q such that:

each ùëö‚Ä≤ is well-formed; and set ùê¥ includes valid DS from nf replicas for ùëö. do
if Data-fragment corresponding to ‚ü®T‚Ñë ‚ü©c is locked // Second Rotation then

Execute data-fragment of ‚ü®T‚Ñë ‚ü©c and add to log.
Push result to set Œ£‚Ñë.
Release the locks from corresponding data-fragment.
V be the shard to forward such that id(V) = NextInRingOrder(‚Ñë).
Send ‚ü®Execute(Œî, Œ£‚Ñë) ‚ü©r to replica o, where o ‚àà ‚ÑúV ‚àß id(r) = id(o).
Broadcast Preprepare( ‚ü®T‚Ñë ‚ü©c , Œî, ùëò‚Ä≤) in shard S (order at sequence ùëò‚Ä≤).

else if r = pS // Primary initiates consensus then

40: event r receives ùëö‚Ä≤ := ‚ü®Execute(Œî, Œ£‚Ñë) ‚ü©q such that:

ùëö‚Ä≤ is sent by replica q, where q ‚àà ‚ÑúU ‚àß id(r) = id(q) do
if Already executed ‚ü®T‚Ñë ‚ü©c // Reply to client then

Send client c the result ùëü .

else

Follow lines 33 to 37.

15:
16:
17:
18:
19:
20:
21:
22:
23:
24:
25:
26:
27:
28:

32:
33:

34:
35:
36:

37:
38:
39:

41:
42:
43:
44:

Figure 5: The event-based normal-case algorithm of RingBFT.
Depending on the type of message the primary replica or a
non-primary replica receives, specific events are triggered.

RingBFT offers safety in an asynchronous environment. To guar-
antee liveness during periods of synchrony, RingBFT offers several

recovery protocols, such as checkpoint, retransmission, and view-
change, to counter malicious attacks. The first step in recovery
against any attack is detection. To do so, we require each replica r
to employ a set of timers. When a timer at a replica r timeouts, then
r initiates an appropriate recovery mechanism. In specific, each
replica r sets following timers:

‚Ä¢ Local Timer ‚Äì To track successful replication of a transac-

tion in its shard.

‚Ä¢ Transmit Timer ‚Äì To re-transmit a successfully replicated

cross-shard transaction to next shard.

‚Ä¢ Remote Timer ‚Äì To track replication of a cross-shard trans-

action in the previous shard.

Each of these timers is initiated at the occurrence of a distinct
event and its timeout leads to running a specific recovery mecha-
nism. When a local timer expires, then the corresponding replica
initiates replacement of the primary of its shard (view-change),
while a remote timer timeout requires the replica to inform the
previous shard in ring order about the insufficient communication.
This brings us to following observation regarding the consensus
offered by RingBFT:

Proposition 5.1. If the network is reliable and the primary of
each shard is non-faulty, then the Byzantine replicas in the system
cannot affect the consensus protocol.

Notice that Proposition 5.1 holds implicitly as no step in Figure 5
depends on the correct working of non-primary Byzantine replicas;
in each shard S, local replication of each transaction is managed
by the primary of S and communication between any two shards
S and U involves all the replicas. This implies that we need only
consider cases when the network is unreliable and/or primary is
Byzantine. We know that RingBFT guarantees safety even in unre-
liable communication and requires a reliable network for assuring
liveness. Hence, we will illustrate mechanisms to tackle attacks by
Byzantine primaries. Next, we illustrate how RingBFT resolves all
the possible attacks it encounters.

(A1) Client Behavior and Attacks. In the case, the primary
is Byzantine and/or network is unreliable, client is the key entity
at loss. Client requested the primary to process its transaction, but
due to an ongoing Byzantine attack, client did not receive sufficient
responses. Clearly, client cannot wait indefinitely to receive valid
responses. Hence, we require each client c to start a timer when it
sends its transaction T‚Ñë to the primary pS of shard S. If the timer
timeouts prior to c receiving at least f + 1 identical responses, c
broadcasts T‚Ñë to all the replicas r ‚àà ‚ÑúS of shard S.

When a non-primary replica r receives a transaction from c,
it forwards that transaction to pS and waits on a timer for pS to
initiate consensus on T‚Ñë. During this time, r expects pS to start
consensus on at least one transaction from c, otherwise it initiates
view-change protocol. Notice that a Byzantine client can always
forward its request to all the replicas of some shard to blame a
non-faulty primary. Such an attack will not succeed as if c sends
to r an already executed request, r simply replies with the stored
response. Moreover, if r belongs to some shard S, which is not the
first shard in ring order, then r ignores the client transaction.

(A2) Faulty Primary and/or Unreliable network. A faulty
primary can prevent successful consensus of a client transaction.

Sajjad Rahnama

Suyash Gupta

Rohan Sogani Dhruv Krishnan Mohammad Sadoghi

Such a primary can be trivially detected as at most f non-faulty repli-
cas would have successfully committed the transaction (received at
least n ‚àí f Commit messages).

An unreliable network can cause messages to get lost or indefi-
nitely delayed. Such an attack is difficult to detect and non-faulty
replicas may blame the primary.

Each primary represents a view of a shard. Hence, the term view-
change is often used to imply primary replacement. Notice that
each shard in RingBFT is a replicated system. Further, RingBFT
is a meta-protocol, which employs existing Bft protocols, such as
Pbft, to run consensus. These properties allow RingBFT to use the
accompanying view-change protocol. Specifically, in this paper, we
use Pbft‚Äôs view change protocol (for MAC-based authentication)
to detect and replace a faulty primary [10].

A replica r ‚àà ‚ÑúS initiates the view-change protocol to replace
its primary pS in response to a timeout. As discussed earlier in this
section, there are two main causes for such timeouts: (i) r does not
receive nf identical Commit messages from distinct replicas, and
(ii) pS fails to propose a request from client c.

(A3) Malicious Primary. A malicious primary p can ensure
that up to f non-faulty replicas in its shard S are unable to make
progress (in dark). Under such conditions, the affected non-faulty
replicas will request a view-change, but they will not be success-
ful as the next primary may not receive sufficient ViewChange
messages (from at least nf replicas) to initiate a new view. Fur-
ther, the remaining f + 1 non-faulty replicas will not support such
ViewChange requests as it is impossible for them to distinguish
between this set of f non-faulty replicas and the actual f Byzantine
replicas.

To ensure these replicas in dark make progress, traditional pro-
tocols periodically send checkpoint messages. These checkpoint
messages include all client transactions and the corresponding nf
Commit messages since the last checkpoint.

5.1 Cross-Shard Attacks

Until now, we have discussed attacks that can be resolved by repli-
cas of any shard independent of the functioning of other shards.
However, the existence of cross-shard transactions unravels new at-
tacks, which may span multiple shards. We use the term cross-shard
attacks to denote attacks that thwart successful consensus of a cst,
First, we describe such attacks, and then we present solutions to
recover from these attacks.

In RingBFT, we know that the consensus of each cst follows
a ring order. In specific, for a cross-shard transaction T‚Ñë, each of
its involved shards S, U ‚àà ‚Ñë first run a local consensus and then
communicate the data to the next shard in ring order. Earlier in this
section, we observed that if at least f + 1 non-faulty replicas of any
shard are unable to reach consensus on T‚Ñë, then that shard will
undergo local view-change. Hence, we are interested in those cross-
shard attacks where neither the involved shards are able to trigger
local view change by themselves, nor are they able to execute the
transaction and reply to the client. This can only occur when all the
involved shards of a cross-shard transaction T‚Ñë, either successfully
completed consensus on T‚Ñë, or are unable to initiate the consensus
on T‚Ñë. Next, we describe these attacks.

Replica-role (running at the replica q of shard U) :

1: event Remote timer of q timeouts such that:

q has received at most f ‚ü®Forward( ‚ü®T‚Ñë ‚ü©c , ùê¥, ùëö, Œî, ) ‚ü©r messages, where

id(S) = PrevInRingOrder(‚Ñë), r ‚àà ‚ÑúS do

2:

Send ‚ü®RemoteView( ‚ü®T‚Ñë ‚ü©c , Œî) ‚ü©q to replica o, where o ‚àà ‚ÑúS ‚àß id(q) = id(o)

3: event r receives message ùëö := ‚ü®RemoteView( ‚ü®T‚Ñë ‚ü©c , Œî) ‚ü©q such that:

ùëö is well-formed and sent by replica q, where

id(U) = NextInRingOrder(‚Ñë), q ‚àà ‚ÑúU ‚àß id(r) = id(q) do

4:

Broadcast ùëö to all replicas in S.

5: event r receives f + 1 ‚ü®RemoteView( ‚ü®T‚Ñë ‚ü©c , Œî) ‚ü©q messages do
6:

Initiate Local view-change protocol.

Figure 6: The remote view-change algorithm of RingBFT.

Assume ‚ÑúùëÜ and ‚Ñúùëà represent the sets of replicas in shards S

and U, respectively.

(C1) No Communication. Under a no communication attack,
we expect that the replicas in ‚ÑúùëÜ are unable to send any messages
to replicas of ‚Ñúùëà .

(C2) Partial Communication. Under a partial communication
attack, we expect that at least f + 1 replicas in ‚Ñúùëà receive less than
f + 1 Forward messages from replicas in ‚ÑúùëÜ .

Both of these attacks could occur solely due to an unreliable net-
work that causes message loss or indefinite message delays. Further,
a malicious primary can collude with an adversarial network to
accelerate the frequency of such attacks. In either of the cases, to
recover from such cross-shard attacks, all the involved shards may
need to communicate among themselves.

5.1.1 Message Retransmission. In RingBFT, to handle a no com-
munication attack, affected replicas of the preceding shard retrans-
mit their original message to the next shard in ring order. Specifi-
cally, when a replica r of shard S successfully completes the con-
sensus on transaction T‚Ñë, it sets the transmit timer for this request
prior to sending the Forward message to replica Q of shard U (next
shard in ring order). When the transmit timer of r timeouts, it again
sends the Forward message to q.

5.1.2 Remote View Change. A partial communication attack could
be either due to a Byzantine primary or unreliable network. If the
primary pS of shard S is Byzantine, then it will ensure that at most f
non-faulty replicas replicate a cross-shard transaction T‚Ñë (S, U ‚àà ‚Ñë),
locally. As a result, replicas of next shard U will receive at most f
Forward messages. Another case is where the network is unreli-
able, and under such conditions, replicas of U may again receive at
most f Forward messages.

From Figure 5, we know that when replica q of shard U receives
a Forward message from replica r of shard S such that id(r) =
id(q), then q broadcasts this Forward message to all the replicas
in U. At this point, RingBFT also requires replica q to start the
remote timer. If any replica q in shard U does not receive identical
Forward messages from f + 1 distinct replicas of shard S, prior
to the timeout of its remote timer, then q detects a cross-shard
attack and sends a RemoteView message to the replica r of shard
S, where id(r) = id(q). Following this, r broadcasts the received
RemoteView message to all the replicas in S. Finally, when any
replica r of shard S receives RemoteView messages from f + 1
replicas of U, it supports the view change request and initiates the
view-change protocol. We illustrate this process in Figure 6.

RingBFT: Resilient Consensus over Sharded Ring Topology

Triggering of Timers. In RingBFT, we know that for each cross-
shard transaction, each replica r of S sets three distinct timers.
Although each timer helps in recovering against a specific attack,
there needs to be an order in which they timeout. As local timers
lead to detecting a local malicious primary, we expect a local timer
to have the shortest duration. Further, a remote timer helps to detect
a lack of communication due to which it has a longer duration than
local timers. Similarly, we require the duration of retransmit timer
to be the longest.

6 RINGBFT GUARANTEES

We now state the safety, liveness, and no deadlock guarantees
provided by our RingBFT protocol.

Proposition 6.1. Let Rùëñ , ùëñ ‚àà {1, 2}, be two non-faulty replicas in
shard S that committed to ‚ü®Tùëñ ‚ü©cùëñ as the ùëò-th transaction sent by p. If
n > 3f, then ‚ü®T1‚ü©c1 = ‚ü®T2‚ü©c2

.

Proof. Replica rùëñ only committed to ‚ü®T‚ü©cùëñ after rùëñ received
identical Commit(Œî, ùëò) messages from nf distinct replicas in S. Let
ùëãùëñ be the set of such nf replicas and ùëåùëñ = ùëãùëñ \ F be the non-faulty
replicas in ùëãùëñ . As |F | = f, so |ùëåùëñ | ‚â• nf ‚àí f. We know that each non-
faulty replica only supports one transaction from primary p as the
ùëò-th transaction, and it will send only one Prepare message. This
implies that sets ùëå1 and ùëå2 must not overlap. Hence, |ùëã1 ‚à™ ùëã2| ‚â•
2(nf ‚àíf). As |ùëã1‚à™ùëã2| = nf, the above inequality simplifies to 3f ‚â• n,
which contradicts n > 3f. Thus, we conclude ‚ü®T1‚ü©c1 = ‚ü®T2‚ü©c2 . ‚ñ°
Theorem 6.2. No Deadlock: In a system ùîñ of shards, where S, U ‚àà
ùîñ and S ‚â† U, no two replicas r ‚àà S and q ‚àà U that order two
conflicting transactions T‚Ñë1
and T‚Ñë2
such that S, U ‚àà ‚Ñë1 ‚à© ‚Ñë2 will
execute T‚Ñë1

in different orders.

and T‚Ñë2

Proof. We know that RingBFT associates an identifier with
each shard and uses this identifier to define a ring order. Let id(S) <
id(U), and the ring order be defined as lowest to highest identifier.
Assume that the conflicting transactions T‚Ñë1 and T‚Ñë2 are in a dead-
lock at shards S and U, where S, U ‚àà ‚Ñë1 ‚à© ‚Ñë2. This implies that each
non-faulty replica r ‚àà S has locked some data-item for T‚Ñë1 that is
required by T‚Ñë2 while each non-faulty replica q ‚àà U has locked
some data-item for T‚Ñë2 that is required by T‚Ñë1 or vice versa.

As each transaction T‚Ñëùëñ , ùëñ ‚àà [1, 2] accesses S and U in ring
order, so each transaction T‚Ñëùëñ was initiated by S. This implies that
the primary of S would have assigned these transactions distinct
sequence numbers ùëòùëñ, ùëñ ‚àà [1, 2], such that ùëò1 < ùëò2 or ùëò1 > ùëò2 (ùëò1 =
ùëò2 is not possible as it will be detected as a Byzantine attack). During
the commit phase, each replica r will put the transaction with larger
sequence number ùëòùëñ in the ùúã list and lock the corresponding data-
item (Figure 5, Line 23), while the transaction with smaller ùëòùëñ is
forwarded to the next shard U. The transaction present in the ùúã list
is only extracted once the data-item is unlocked. Hence, there is a
contradiction, that is, shards S and U will not suffer deadlock. ‚ñ°

Theorem 6.3. Safety: In a system ùîñ of shards, where each shard
S ‚àà ùîñ has at most f Byzantine replicas, each replica r follows the
Involvement, Non-divergence, and Consistence properties. Specifically,
all the replicas of S execute each transaction in the same order, and
every conflicting cross-shard transaction is executed by all the replicas
of all the involved shards in the same order.

Proof. Using Proposition 6.1 we have already illustrated that
RingBFT safely replicates a single-shard transaction, despite a mali-
cious primary and/or unreliable network. In specific, any non-faulty
replica R ‚àà ‚ÑúS will only commit a single-shard transaction if it
receives Commit messages from nf distinct replicas in ‚ÑúS. When
a non-faulty replica receives less than nf Commit messages, then
eventually its local timer will timeout and it will participate in the
view-change protocol. Post the view-change protocol, any request
that was committed by at least one non-faulty replica will persist
across views.

Similarly, we can show that each cross-shard transaction is also
safely replicated across all replicas of all the involved shards. In
RingBFT, each cross-shard transaction T‚Ñë is processed in ring order
by all the involved shards ‚Ñë. Let shards S, U ‚àà ‚Ñë and id(S) < id(U)
such that ring order is based on lowest to highest identifier. Hence,
replicas of shard U will only start consensus on T‚Ñë if they receive
Forward messages from f + 1 distinct replicas of S. Further, each
of these Forward messages includes DS from nf distinct replicas
of S on identical Commit messages corresponding to T‚Ñë, which
guarantees that T‚Ñë was replicated in S. If the network is unreliable
and/or primary of shard S is Byzantine, then replicas of U will
receive less than f + 1 Forward messages. In such a case, either
the remote timer at replicas of U will timeout, or one of the two
timers (local timer or transmit timer) of replicas of S will timeout.
In any case, following the specific recovery procedure, replicas of
‚ñ°
U will receive a sufficient number of Forward messages.

Theorem 6.4. Liveness: In a system ùîñ of shards, where each shard
S ‚àà ùîñ has at most f Byzantine replica, if the network is reliable, then
each replica r follows the Involvement and Termination properties.
Specifically, all the replicas continue making progress, and good clients
continue receiving responses for their transactions.

Proof. In the case of a single-shard transaction, if the primary
is non-faulty, then each replica will continue processing client
transactions. If the primary is faulty, and prevents a request from
replicating by allowing at most f replicas to receive Commit mes-
sages, then such a primary will be replaced through view-change
protocol, following which a new primary will ensure that the repli-
cas continue processing subsequent transactions. Notice that there
can be at most f such faulty primaries, and the system will even-
tually make progress. If the primary is malicious, then it can keep
up to f non-faulty replicas in dark, which will continue making
progress through periodic checkpoints. In the case of a cross-shard
transaction, there is nothing extra that a faulty primary pS can do
than preventing local replication of the transaction. If pS does that,
then as discussed above, pS will be replaced. Further, during any
communication between two shards, primary has no extra advan-
tage over other replicas in the system. Further, the existence of
transmit and remote timers help replicas of all the involved shards
‚ñ°
to keep track of any malicious act by primaries.

7 DESIGN & IMPLEMENTATION

RingBFT aims to scale permissioned blockchains to hundreds of
replicas through efficient sharding. To argue the benefits of our
RingBFT protocol, we need to first implement it in a permissioned
blockchain fabric. For this purpose, we employed a state-of-the-art

Sajjad Rahnama

Suyash Gupta

Rohan Sogani Dhruv Krishnan Mohammad Sadoghi

where chaining is guaranteed by requiring each block to include
the hash of the previous block:

ùîÖùëò = {ùëò, Œî, pSùëñ , ùêª (ùîÖùëò‚àí1)}

(3)

In ResilientDB, for efficient processing, we follow existing liter-
ature and require the primary pSùëñ of shard Sùëñ to aggregate transac-
tions in a batch and perform consensus on this batch. Hence, each
ùëò-th block ùîÖùëò in ùîèSùëñ represents a batch of transactions that replicas
of Sùëñ successfully committed at sequence ùëò. Note: we expect each
block to include all the transactions that access the same shards.

If a block includes cross-shard transactions, then such a block is
appended to the ledger of all the involved shards ‚Ñë. In specific, if a
block ùîÖ includes a transaction T‚Ñë, such that Sùëñ, Sùëó ‚àà ‚Ñë, then ùîÖ ‚àà
ùîèSùëñ and ùîÖ ‚àà ùîèSùëó . Notice that the order in which these blocks appear
in each individual chain can be different. However, if two blocks
ùîÖùë• and ùîÖùë¶ include conflicting transactions that access intersecting
set of shards, and consensus on ùîÖùë• happens before ùîÖùë¶, then in each
ledger ùîÖùë• is appended before ùîÖùë¶.

Depending on the choice of storage, each block can include
either all the transactional information or the Merkle Root [52] of
all transactions in the block. A Merkle Root (Œî) helps to optimize the
size of each block, and is generated by assuming all the transactions
in a batch as leaf nodes, followed by a pair-wise hashing up till the
root. To initialize each blockchain, every replica adds an agreed
upon dummy block termed as the genesis block [36].

8 EVALUATION

In this section, we evaluate our RingBFT protocol. To do so, we
implement RingBFT on our high throughput yielding permissioned
blockchain fabric, ResilientDB.

For experimentation, we deploy ResilientDB on Google Cloud
Platform (GCP) in fifteen regions across five continents, namely:
Oregon, Iowa, Montreal, Netherlands, Taiwan, Sydney, Singapore,
South Carolina, North Virginia, Los Angeles, Las Vegas, London,
Belgium, Tokyo, and Hong Kong. In any experiment involving less
than 15 shards, the choice of the shards is in the order we have
mentioned above. We deploy each replica on a 16-core N1 machine
having Intel Broadwell CPUs with a 2.2GHz clock and 32GB RAM.
For deploying clients, we use the 4-core variants having 16GB RAM.
For each experiment, we equally distribute the clients in all regions.
Benchmark. To provide workload for our experiments, we use
the Yahoo Cloud Serving Benchmark from the BlockBench suite
(YCSB) [12, 17] . Each client transaction queries a YCSB table with an
active set of 600 k records. For our evaluation, we adopt transactions
that read and modify existing records. Prior to each experiment,
each replica initializes an identical copy of the YCSB table. YCSB
workloads help us to create cross-shard client transactions with
varying degrees of conflict, while other workloads aim to evaluate
the cost of executing a transaction, which is orthogonal to our
RingBFT consensus.

Existing Protocols. In all our experiments, we compare the per-
formance of RingBFT against two other state-of-the-art sharding
Bft protocols, AHL [14] and Sharper [4]. In Section 2, we high-
lighted key properties of these protocols. Like RingBFT, both AHL
and Sharper employ Pbft to achieve consensus on single-shard

Figure 7: The parallel-pipelined architecture provided by
ResilientDB fabric for efficiently implementing RingBFT.

permissioned blockchain fabric, ResilientDB [25‚Äì28, 31, 32, 34, 58].
In our prior works, we illustrated how ResilientDB offers an op-
timal system-centric design that eases implementing novel Bft
consensus protocols. Further, ResilientDB presents an architec-
ture that allows even classical protocols like Pbft to achieve high
throughputs and low latencies.

In this section, we describe in brief ResilientDB‚Äôs architecture
and explain the design decisions we took to implement RingBFT.
Network Layer. ResilientDB provides a network layer to man-
age communication among clients and replicas. The network layer
provides TCP/IP capabilities through Nanomsg-NG to communi-
cate messages. To facilitate uninterrupted processing of millions of
messages, at each replica, ResilientDB offers multiple input and
output threads to communicate with the network.

Pipelined Consensus. Once a message is received from the net-
work, the key challenge is to process it efficiently. If all the ensuing
consensus tasks are performed sequentially, the resulting system
output would be abysmally low. Moreover, such a system would be
unable to utilize the available computational and network capabil-
ities. Hence, ResilientDB associates with each replica a parallel
pipelined architecture, which we illustrate in Figure 7.

When an input thread receives a message from the network, it
places them in a specific work queues based on the type of the
message. As depicted in Figure 7, ResilientDB provides dedicated
threads for processing each type of message.

Blockchain. To securely record each successfully replicated
transaction, we also implement an immutable ledger‚Äìblockchain.
For systems running fully-replicated Bft consensus protocols like
Pbft and Zyzzyva, blockchain is maintained as a single linked-list
of all transactions where each replica stores a copy of the block-
chain. However, in the case of sharding protocols like RingBFT,
each shard maintains its own blockchain. As a result, no single shard
can provide a complete state of all the transactions. Hence, we refer
to the ledger maintained at each shard as a partial-blockchain.

Let, ùîñ be the system of z = |ùîñ| shards. Say, we use the represen-
tation S1, S2, ..., Sùëñ ‚àà ùîñ, to denote the shards in ùîñ where 1 ‚â§ ùëñ ‚â§ ùëß.
In this sharded system, we represent the blockchain ledger main-
tained by replicas of Sùëñ as ùîèSùëñ . Hence the complete state of the
system can be expressed as:

ùîèS1 ‚à™ ùîèS2 ‚à™ ... ‚à™ ùîèSùëñ ‚à™ ... ‚à™ ùîèSùëß

Further, we know that each ledger ùîèSùëñ is a linked list of blocks:

ùîèSùëñ = {ùîÖ1, ùîÖ2, ..., ùîÖùëò }

(1)

(2)

NetworkClient	RequestsConsensus	MessagesInputThreadInputThreadInputThreadInputThreadWork	QueueClient	RequestsWork	QueueCommitCertificateWork	QueuePrepare,	CommitWork	QueueCheckpointOutputThreadOutputThreadOutputThreadOutputThreadBatchingThreadCertifyThreadWorkerThreadCheckpointThreadExecuteThreadNetworkRingBFT: Resilient Consensus over Sharded Ring Topology

transactions. Hence, all three protocols have identical implementa-
tions for replicating single-shard transactions. For achieving consen-
sus on cross-shard transactions, we follow the respective algorithms
and modify ResilientDB appropriately.

WAN Bandwidth and Round-Trip Costs. As the majority of
experiments take place in a geo-scaled WAN environment span-
ning multiple continents, available bandwidth and round-trip costs
between two regions play a crucial role. Prior works [2, 32] have il-
lustrated that if the available bandwidth is low and round-trip costs
are high, then the protocols dependent on a subset of replicas face
performance degradation. In the case of AHL, the reference com-
mittee is responsible for managing cross-shard consensus, while for
Sharper, the primary of coordinating shard leads the cross-shard
consensus. Hence, both of these protocols observe low throughput
and high latency in proportion to available bandwidth and round-
trip costs. Although RingBFT requires cross-shard communication
in the form of Forward and Execute messages, the system is com-
parably less burdened as all the replicas participate equally in this
communication process.

Standard Settings. Unless explicitly stated, we use the following
settings for all our experiments. We run with a mixture of single-
shard and cross-shard transactions, of which 30% are cross-shard
transactions. Each cross-shard transaction accesses all the 15 re-
gions, and in each shard we deploy 28 replicas, that is, a total of
420 globally distributed replicas. The number of key-value pairs
accessed by each transaction varies in accordance with the number
of regions accessed. For example, if a transaction accesses three re-
gions, then it accesses three key-value pairs. In these experiments,
we allow up to 50K clients to send transactions. Further, we re-
quire clients and replicas to employ batching and create batches of
transactions of size 100.

The sizes of messages communicated during RingBFT consensus
are: Preprepare (5408B), Prepare (216B), Commit (269B), Forward
(6147B), Checkpoint (164B), and Execute (1732B).

Note: Our RingBFT protocol provides support for standard multi-
statement transactions that are widely adopted by deterministic
databases [35, 38, 57, 59, 65]. Hence, the complexity of designing
RingBFT is similar to running an application on top of a determin-
istic database. Hence, we believe a developer would not face any
new challenges.

Through our experiments, we want to answer the following:
(Q1) What is the effect of increasing the number of shards on

consensus provided by RingBFT?

(Q2) How does varying the number of replicas per shard affects

the performance of RingBFT?

(Q3) What is the impact of increasing the percentage of cross-

shard transactions on RingBFT?

(Q4) How does batching affect the system performance?
(Q5) What is the effect of varying the number of involved shards

in a cross-shard transaction on RingBFT?

(Q6) What is the impact of varying number of clients on con-

sensus provided by RingBFT?

(Q7) How do faulty primary and view change affect the perfor-

mance of RingBFT?

8.1 Scaling Number of Shards.

For our first set of experiments, we study the effect of scaling the
number of shards. In specific, we require clients to send cross-shard
transactions that can access from 3, 5, 7, 9, 11, and 15 shards, while
keeping other parameters at the standard setting. We use Figures 8
(I) and (II) to illustrate the throughput and latency metrics.

RingBFT achieves 16√ó and 4√ó higher throughput than AHL and
Sharper in the 15 shard setting, respectively. An increase in the
number of shards only increases the length of the ring while keeping
the amount of communication between two shards at constant. As
a result, for RingBFT, we observe an increase in latency as there is
an increase in time to go around the ring, namely, a linear neighbor-
to-neighbor communication. From three shards to 15 shards, the
latency increases from 1.17s to 6.82s. Notice that the throughput for
RingBFT is nearly constant since the size of shards and the amount
of communication among shards are constant. This is a consequence
of an increase in the number of shards that can perform consensus
on single-shard transactions in parallel. Although on increasing the
number of shards, there is a proportional increase in the number of
involved shards per transaction, the linear communication pattern
of RingBFT prevents throughput degradation.

In the case of AHL, the consensus on cross-shard transactions
is led by the reference committee, which essentially centralizes
the communication in the global setting and affects the system
performance. In contrast, Sharper scales better because there is
no single reference committee leading all cross-shard consensuses.
However, even Sharper sees a fall in throughput due to two rounds
of communication between all replicas of all the involved shards.
For a system where all the shards are globally scattered, quadratic
communication complexity and communication between all the
shards impacts the scalability of the system.

8.2 Scaling Number of Replicas per Shard.

We now study the effects of varying different parameters within
a single shard. Our next set of experiments aim to increase the
amount of replication within a single shard. In specific, we allow
each shard to have 10, 16, 22, and 28 replicas. We use Figures 8 (III)
and (IV) to illustrate the throughput and latency metrics.

These plots reaffirm our theory that RingBFT ensures up to 16√ó
higher throughput and 11√ó lower latency than the other two pro-
tocols. As the number of replicas in each shard increases, there is a
corresponding decrease in throughput for RingBFT. This decrease
is not surprising because RingBFT employs the Pbft protocol for
local replication, which necessitates two phases of quadratic com-
munication complexity. This, in turn increases the size (and as a
result cost) of Forward messages communicated between shards.
In the case of AHL, the existence of a reference committee acts
as a performance bottleneck to an extent that 30% cross-shard
transactions involving all the 15 shards subsides the benefits due
to reduced replication (10 or 16 replicas). Sharper also observes
a drop in its performance as it relies on Pbft, and is unable to
scale at smaller configurations due to expensive communication
that requires an all-to-all communication between the replicas of
involved shards. To summarize: RingBFT achieves up to 4√ó and
16√ó higher throughput than Sharper and AHL, respectively.

Sajjad Rahnama

Suyash Gupta

Rohan Sogani Dhruv Krishnan Mohammad Sadoghi

RingBFT

Sharper

AHL

(I) Impact of Shards (Throughput)

(II) Impact of Shards (Latency)

(III) Impact of Nodes per Shards (Throughput)

(IV) Impact of Nodes per Shards (Latency)

)
s
/
n
x
t
(

t
u
p
h
g
u
o
r
h
T

l
a
t
o
T

80K

60K

40K

20K

0

3

5

7

9
Number of Shards (s)

11

)
s
(
y
c
n
e
t
a
L

80.0

60.0

40.0

20.0

0.0

15

3

5

7

9
Number of Shards (s)

11

)
s
/
n
x
t
(

t
u
p
h
g
u
o
r
h
T

l
a
t
o
T

200K

150K

100K

50K

0

15

10

16

22

28

Number of Nodes Per Shard (n)

)
s
(
y
c
n
e
t
a
L

80.0

60.0

40.0

20.0

0.0

10

16

22

28

Number of Nodes Per Shard (n)

(V) Impact of X-Shard Workload Rate (Throughput)

(VI) Impact of X-Shard Workload Rate (Latency)

(VII) Impact of Batch Size (Throughput)

(VIII) Impact of Batch Size (Latency)

)
s
/
n
x
t
(

t
u
p
h
g
u
o
r
h
T

l
a
t
o
T

1M

200K

50K

10K

2K

1M

200K

50K

10K

)
s
/
n
x
t
(

t
u
p
h
g
u
o
r
h
T

l
a
t
o
T

)
s
(
y
c
n
e
t
a
L

100.0

50.0

0.0

)
s
/
n
x
t
(

t
u
p
h
g
u
o
r
h
T

l
a
t
o
T

200K

150K

100K

50K

0

150.0

100.0

50.0

0.0

)
s
(
y
c
n
e
t
a
L

0

5

10 15

30

60 100

0

5

10 15

30

60 100

10

50 100

500 1K 1.5K

10

50 100

500 1K 1.5K

Cross-Shard Workload Rate

Cross-Shard Workload Rate

Batch Size

Batch Size

(IX) Impact of Involved Shards (Throughput)

(X) Impact of Involved Shards (Latency)

(XI) Impact of Inflight Transactions (Throughput)

(XII) Impact of Inflight Transactions (Latency)

)
s
(
y
c
n
e
t
a
L

80.0

60.0

40.0

20.0

0.0

)
s
/
n
x
t
(

t
u
p
h
g
u
o
r
h
T

l
a
t
o
T

80K

60K

40K

20K

0

)
s
(
y
c
n
e
t
a
L

100.0

50.0

0.0

1

3

6

9

15

1

3

6

9

15

3K 5K

10K

15K

20K

3K 5K

10K

15K

20K

Involved Shards

Involved Shards

Clients

Clients

Figure 8: Measuring system throughput and average latency on running different Bft sharding consensus protocols.

8.3 Varying percentage of Cross-shard

Transactions.

For our next study, we allow client workloads to have 0, 5%, 10%,
15%, 30%, 60%, and 100% cross-shard transactions. We use Figures 8
(V) and (VI) to illustrate the throughput and latency metrics.

When the workload contains no cross-shard transactions, it sim-
ply indicates a system where all the transactions access only one
shard. In this case, all the three protocols attain the same through-
put and latency as all of them employ Pbft for reaching consen-
sus on single-shard transactions. They achieve 1.2 Million txn/s
throughput among 500 nodes in 15 globally distributed regions.
With a small (5%) introduction of cross-shard transactions in the
workload, there is a significant decrease for all the protocols. The
amount of decrease is in accordance to the reasons we discussed
in previous sections. However, RingBFT continues to outperform
other protocols. In the extreme case of 100% cross-shard workload,
RingBFTachieve 4√ó and 18√ó higher throughput and 3.3√ó and 7.8√ó
lower latency than Sharper and AHL, respectively.

8.4 Varying the Batch Size.

Next, we study the impact of batching transactions on system per-
formance. We require the three protocols to run consensus on
batches of client transactions with sizes 10, 50, 100, 500, 1K, and 5K.

We use Figures 8 (VII) and (VIII) to illustrate the throughput and
latency metrics.

As the number of transactions in a batch increases, there is a
proportional decrease in the number of consensuses. For example,
with a batch size of 10 and 100 for 5000 transactions, we need 500
and 50 instances of consensus. However, larger batches also cause
an increase in latency due to the increased cost of communication
and time for processing all the transactions in the batch. Hence, we
observe an increase in throughput on moving from small batches
of 10 transactions to large batches of 1K transactions. On further
increase (after 1.5K), the system throughput hits saturation and
eventually decreases as benefits of batching are over-shadowed by
increased communication costs.

Starting from the batch size of 10, on increasing the batch size,
the throughput increases up to 27√ó in RingBFT because, with less
communication and fewer messages, we are processing more trans-
actions. This trend lasts until the system reaches its saturation point
in terms of communication and computation, which is the batch
size of 1.5K for RingBFT. Once the system is at filling its network
bandwidth, adding more transactions to the batch will not increase
the throughput because it cannot process more, and sending those
batches will be a bottleneck for the system. Ideally, it should get
constant after some point but because of implementation details
and queuing, it drops slightly after some time.

RingBFT: Resilient Consensus over Sharded Ring Topology

Ideally, we expect the latency to also decrease with an increase
in batch size. However, for RingBFT, more transactions in a batch
implies more time spent processing the transactions around the ring.
This causes an increase in latency for the client. To summarize:
Using the optimal batch size improve the throughput of RingBFT,
Sharper and AHL, 27√ó, 45√ó, and 3√ó respectively.

8.5 Varying Number of Involved Shards.

We now keep the number of shards fixed at 15 and require all clients
to create transactions that access a subset of these shards. In specific,
clients send transactions that access 1, 3, 6, 9, and 15 shards. As
our selected order for shards gives no preference to their proximity
to each other (to prevent any bias), our clients select consecutive
shards in order to generate the workload.

We use Figures 8 (IX) and (X) to illustrate the throughput and
latency metrics. As expected, all three protocols observe a drop
in performance on the increase in the number of involved shards.
However, RingBFT still outperforms the other two protocols. As
we increase the number of involved shards, the performance gap
between RingBFT and the other two protocols increases. As shown
in the graph, with three shards involved, RingBFT has a 4% perfor-
mance gap, increasing to 4√ó with 15 shards involved.

8.6 Varying Number of Clients.

Each system can reach optimal latency only if it is not overwhelmed
by incoming client requests. In this section, we study the impact of
the same by varying the number of incoming client transactions
through a gradual increase in the number of clients from 5K to 20K.
We use Figures 8 (XI) and (XII) to illustrate resulting throughput and
latency metrics. As we increase the number of clients transmitting
transactions, we observe a 15 ‚àí 20% increase in throughput, reach-
ing the saturation point. Having more clients causes a decrease
between 7% and 9%, which is a result of various queues being full
with incoming requests, which in turn causes a replica to perform
extensive memory management. Due to similar reasons, there is a
significant increase in latency as the time to process each request
has increased proportionally. We observed 32.75s, 58.21s, and 59.64s
increase in RingBFT, Sharper, and AHL respectively. Despite this,
RingBFT scales better than other protocols even when the system
is overwhelmed by clients.

8.7 Impact of Primary Failure.

Next, we evaluate the effect of replacing a faulty primary in different
shards. For this experiment, we run experiments with 9 shards and
allow workload to consist of 30% cross-shard transactions. We use
Figure 9 to show the throughput attained by RingBFT when the
primary of the first three shards fail, and the replicas run the view
change protocol to replace the faulty primary. The primaries of
these shards fail at 10ùë†, and the system‚Äôs average throughput starts
decreasing while other shards are processing their clients‚Äô requests.
RingBFT observes a 15% decrease in throughput and post view
change; it again observes an increase in throughput.

Impact of Primary Failure in Three Shards

)
s
/
n
x
t
(

t
u
p
h
g
u
o
r
h
T

l
a
t
o
T

0 10 20 30 40 50 60 70 80 90 100110
Time

Figure 9: RingBFT‚Äôs throughput under the primary failure of
three shards out of nine. (s = 10) primary fails; (s = 20) repli-
cas timeout and send view-change messages; (s = 30) new
primary starts the new view; (s = 35) system‚Äôs throughput
start increasing and returns back to normal at s = 55.

8.8 Impact of Complex Cross-Shard

Transactions.

Until now, we have experimented with simple cst where for a given
cst each shard could independently execute its data-fragment. How-
ever, a sharded system may encounter a complex cst where each
shard may require access to data (and needs to check constraints)
present in other shards while executing its data-fragment. These
data-access dependencies require each shard to read the data from
remote shards.

Our RingBFT protocol performs this task by requiring each shard
to send its read-write sets along with the Forward message. In this
section, we study the cost of communicating the read-write sets of a
complex cst on our RingBFT protocol. We use Figure 10 to illustrate
the throughput and latency metrics on varying the number of data-
access dependencies from 0 to 64 distributed randomly across 15
shards. These figures illustrate that our RingBFT protocol provides
reasonable throughput and latency even for a cst with extensive
dependencies.

Note that we have not included Sharper and AHL in Figure 10
as supporting complex cst is not covered in [4, 14] and remains
as an open problem. For example, to support remote reads, first,
there must be a consensus on the remote shard to agree on the
requested operations and their values. Second, on the receiving end,
there must be another local consensus on the values received. If the
remote values are not received, then a consensus is needed to detect
failures in order to invoke remote recovery to restore liveness. Now
Sharper has a single global consensus that coordinates among all
shards and their replicas. Thus, extending Sharper is nontrivial
because it is unclear as to when and how the additional remote
consensuses and recoveries could be invoked. In the case of AHL,
due to its 2PC design, invoking remote consensus on each shard to
process remote read is simple, but it is challenging to invoke remote
view change when the network is unreliable or the primary of the
remote shard behaves maliciously. Moreover, keeping the question
of feasibility aside, we observe that in Figure 8(I), at 15 shards,
the throughputs of both Sharper and AHL are under 20K while
RingBFT sustains 80K transactions/second. However, in Figure 10,
when we scale up to 64 remote operations across 15 shards, RingBFT

Sajjad Rahnama

Suyash Gupta

Rohan Sogani Dhruv Krishnan Mohammad Sadoghi

RingBFT

(I) Impact of Remote Reads (Throughput)

(II) Impact of Remote Reads (Latency)

)
s
/
n
x
t
(

t
u
p
h
g
u
o
r
h
T

l
a
t
o
T

80K

60K

40K

20K

0.0
0
0

8

16

32
Number of Remote Reads
Each Txn Requires

48

15.0

10.0

5.0

)
s
(
y
c
n
e
t
a
L

64

0.0
0.0
0

8

16

32
Number of Remote Reads
Each Txn Requires

48

64

Figure 10: RingBFT‚Äôs throughput and latency on encoun-
tering complex cross-shard transactions with dependencies
varying from 0 to 64.

yields a throughput of at least 45K transactions/second, surpassing
both baselines with no remote operations.

9 RELATED WORK

In Section 1, we presented an overview of different types of Bft
protocols. Further, we have extensively studied the architecture
of state-of-the-art permissioned sharding Bft protocols, AHL and
Sharper. We now summarize other works in the space of Byzantine
Fault-Tolerance consensus.

Traditional Bft consensus. The consensus problems such as Byzan-
tine Agreement and Interactive Consistency have been studied in
literature in great detail [18‚Äì21, 63, 64]. With the introduction of
Pbft-powered BFS‚Äîa fault-tolerant version of the networked file
system [39]‚Äîby Castro et al. [9, 10] there has been an unprece-
dented interest in the design of high-performance Bft consensus
protocols. This has led to the design of several consensus proto-
cols that have optimized different aspects of Pbft, e.g, Zyzzyva,
Sbft, and PoE, as discussed in the Introduction. To further improve
on the performance of Pbft, some consensus protocols consider
providing less failure resilience [1, 50, 51], focused on a theoretical
framework to support weaker consistency and isolation semantics
such as dirty reads and committed reads [42], or rely on trusted
components [5, 11, 33].

Guerraoui et al. [24] introduced the StretchingBFT protocol that
aims to improve on Pbft by arranging replicas in a ring-like topol-
ogy where each replica communicates with its two neighbors. Our
RingBFT is a meta-protocol that can utilize any of these Bft proto-
col to achieve optimal intra-shard consensus. Hence, these protocols
complement our design. Further, these protocols cannot scale to
hundreds of replicas scattered across the globe, and this is where
our vision of RingBFT acts as a resolve.

Permissionless Sharded Blockchains. Permissionless space includes
several sharding Bft consensus protocols, such as Conflux [48],
Elastico [49], MeshCash [6], OmniLedger [44], and Spectre [62].
All of these protocols require each of their shards to run either the
Proof-of-Work or Proof-of-Stake protocol during some phase of
the consensus. As a result these protocols offer a magnitude lower
throughput than both AHL and Sharper, which are included in
our evaluation.

In our recent sharding work, we have developed a comprehen-
sive theoretical framework to study a wide range of consistency

models and isolation semantics (e.g., dirty reads, committed reads,
serializability) and communication patterns (e.g., centralized vs.
distributed) [42]. We have further developed a hybrid sharding
protocol intended for the permissionless setting optimized for the
widely used unspent transaction model [40].

10 CONCLUSIONS

In this paper, we present RingBFT‚Äìa novel meta-Bft protocol for
permissioned sharded blockchains. For a single-shard transaction,
RingBFT performs as efficient as any state-of-the-art sharding Bft
consensus protocol. However, existing sharding Bft protocols face
severe fall in throughput when they have to achieve consensus
on a cross-shard transaction. RingBFT resolves this situation by
requiring each shard to participate in at most two rotations around
the ring. In specific, RingBFT expects each shard to adhere to the
prescribed ring order, and follow the principle of process, forward,
and re-transmit, while ensuring the communication between shards
is linear. We implement RingBFT on our efficient ResilientDB
fabric, and evaluate it against state-of-the-art sharding Bft proto-
cols. Our results illustrates that RingBFT achieves up to 18√ó higher
throughput than the most recent sharding protocols and easily
scales to nearly 500 globally-distributed nodes.

REFERENCES
[1] Michael Abd-El-Malek, Gregory R. Ganger, Garth R. Goodson, Michael K. Reiter,
and Jay J. Wylie. 2005. Fault-scalable Byzantine Fault-tolerant Services. In Pro-
ceedings of the Twentieth ACM Symposium on Operating Systems Principles. ACM,
59‚Äì74. https://doi.org/10.1145/1095810.1095817

[2] Yair Amir, Claudiu Danilov, Jonathan Kirsch, John Lane, Danny Dolev, Cristina
Nita-Rotaru, Josh Olsen, and David Zage. 2006. Scaling Byzantine Fault-Tolerant
Replication to Wide Area Networks. In International Conference on Dependable
Systems and Networks (DSN‚Äô06). 105‚Äì114. https://doi.org/10.1109/DSN.2006.63
[3] Mohammad Javad Amiri, Divyakant Agrawal, and Amr El Abbadi. 2019. CAPER:
A Cross-application Permissioned Blockchain. Proc. VLDB Endow. 12, 11 (2019),
1385‚Äì1398. https://doi.org/10.14778/3342263.3342275

[4] Mohammad Javad Amiri, Divyakant Agrawal, and Amr El Abbadi. 2019. SharPer:
Sharding Permissioned Blockchains Over Network Clusters. https://arxiv.org/
abs/1910.00765v1

[5] Johannes Behl, Tobias Distler, and R√ºdiger Kapitza. 2017. Hybrids on Steroids:
SGX-Based High Performance BFT. In Proceedings of the Twelfth European Con-
ference on Computer Systems. ACM, 222‚Äì237. https://doi.org/10.1145/3064176.
3064213

[6] Iddo Bentov, Pavel Hub√°ƒçek, Tal Moran, and Asaf Nadler. 2017. Tortoise and
Hares Consensus: the Meshcash Framework for Incentive-Compatible, Scalable
Cryptocurrencies. https://eprint.iacr.org/2017/300

[7] P. A. Bernstein and N. Goodman. 1983. Multiversion Concurrency Control -

Theory and Algorithms. ACM TODS 8, 4 (1983), 465‚Äì483.

[8] Matthias Butenuth, Guido v. G√∂sseln, Michael Tiedge, Christian Heipke, Udo
Lipeck, and Monika Sester. 2007. Integration of heterogeneous geospatial data in
a federated database. ISPRS Journal of Photogrammetry and Remote Sensing 62, 5
(2007), 328 ‚Äì 346. https://doi.org/10.1016/j.isprsjprs.2007.04.003 Theme Issue:
Distributed Geoinformatics.

[9] Miguel Castro and Barbara Liskov. 1999. Practical Byzantine Fault Tolerance. In
Proceedings of the Third Symposium on Operating Systems Design and Implemen-
tation. USENIX, USA, 173‚Äì186.

[10] Miguel Castro and Barbara Liskov. 2002. Practical Byzantine Fault Tolerance
and Proactive Recovery. ACM Trans. Comput. Syst. 20, 4 (2002), 398‚Äì461. https:
//doi.org/10.1145/571637.571640

[11] Byung-Gon Chun, Petros Maniatis, Scott Shenker, and John Kubiatowicz. 2007.
Attested Append-only Memory: Making Adversaries Stick to Their Word. In Pro-
ceedings of Twenty-first ACM SIGOPS Symposium on Operating Systems Principles.
ACM, 189‚Äì204. https://doi.org/10.1145/1294261.1294280

[12] Brian F. Cooper, Adam Silberstein, Erwin Tam, Raghu Ramakrishnan, and Russell
Sears. 2010. Benchmarking Cloud Serving Systems with YCSB. In Proceedings of
the 1st ACM Symposium on Cloud Computing. ACM, 143‚Äì154. https://doi.org/10.
1145/1807128.1807152

[13] J. C. Corbett, Jeffrey Dean, Michael Epstein, Andrew Fikes, Christopher Frost,
JJ Furman, Sanjay Ghemawat, Andrey Gubarev, Christopher Heiser, Peter

RingBFT: Resilient Consensus over Sharded Ring Topology

Hochschild, Wilson Hsieh, Sebastian Kanthak, Eugene Kogan, Hongyi Li, Alexan-
der Lloyd, Sergey Melnik, David Mwaura, David Nagle, Sean Quinlan, Rajesh Rao,
Lindsay Rolig, Yasushi Saito, Michal Szymaniak, Christopher Taylor, Ruth Wang,
and Dale Woodford. 2012. Spanner: Google‚Äôs Globally-Distributed Database. In
10th USENIX Symposium on Operating Systems Design and Implementation (OSDI
12). USENIX Association, 261‚Äì264.

[14] Hung Dang, Tien Tuan Anh Dinh, Dumitrel Loghin, Ee-Chien Chang, Qian Lin,
and Beng Chin Ooi. 2019. Towards Scaling Blockchain Systems via Sharding. In
Proceedings of the 2019 International Conference on Management of Data. ACM,
123‚Äì140. https://doi.org/10.1145/3299869.3319889

[15] A. Deshpande and J. M. Hellerstein. 2002. Decoupled query optimization for
federated database systems. In Proceedings 18th International Conference on Data
Engineering. 716‚Äì727. https://doi.org/10.1109/ICDE.2002.994788

[16] C. Diaconu, C. Freedman, E. Ismert, P.-A. Larson, P. Mittal, R. Stonecipher, N.
Verma, and M. Zwilling. 2013. Hekaton: SQL Server‚Äôs Memory-optimized OLTP
Engine. ACM, 1243‚Äì1254. https://doi.org/10.1145/2463676.2463710

[17] Tien Tuan Anh Dinh, Ji Wang, Gang Chen, Rui Liu, Beng Chin Ooi, and Kian-Lee
Tan. 2017. BLOCKBENCH: A Framework for Analyzing Private Blockchains. In
Proceedings of the 2017 ACM International Conference on Management of Data.
ACM, 1085‚Äì1100. https://doi.org/10.1145/3035918.3064033

[18] Danny Dolev. 1982. The Byzantine generals strike again. Journal of Algorithms

3, 1 (1982), 14‚Äì30. https://doi.org/10.1016/0196-6774(82)90004-9

[19] Danny Dolev and R√ºdiger Reischuk. 1985. Bounds on Information Exchange for
Byzantine Agreement. J. ACM 32, 1 (1985), 191‚Äì204. https://doi.org/10.1145/
2455.214112

[20] Michael J. Fischer and Nancy A. Lynch. 1982. A lower bound for the time
Inform. Process. Lett. 14, 4 (1982), 183‚Äì186.

to assure interactive consistency.
https://doi.org/10.1016/0020-0190(82)90033-3

[21] Michael J. Fischer, Nancy A. Lynch, and Michael S. Paterson. 1985. Impossibility
of Distributed Consensus with One Faulty Process. J. ACM 32, 2 (1985), 374‚Äì382.
https://doi.org/10.1145/3149.214121

[22] Guy Golan Gueta, Ittai Abraham, Shelly Grossman, Dahlia Malkhi, Benny Pinkas,
Michael Reiter, Dragos-Adrian Seredinschi, Orr Tamir, and Alin Tomescu. 2019.
SBFT: A Scalable and Decentralized Trust Infrastructure. In 2019 49th Annual
IEEE/IFIP International Conference on Dependable Systems and Networks (DSN).
IEEE, 568‚Äì580. https://doi.org/10.1109/DSN.2019.00063
[23] Jim Gray. 1978. Notes on Data Base Operating Systems.
[24] Rachid Guerraoui, Nikola Knezevic, Vivien Quema, and Marko Vukolic. 2010.

Stretching BFT. Infoscience EPFL.

[25] Suyash Gupta. 2020. Resilient and Scalable Architecture for Permissioned Block-
chain Fabrics. In Proceedings of the VLDB 2020 PhD Workshop co-located with the
46th International Conference on Very Large Databases (CEUR Workshop Proceed-
ings), Vol. 2652. CEUR-WS.org.

[26] Suyash Gupta, Jelle Hellings, Sajjad Rahnama, and Mohammad Sadoghi. 2019. An
In-Depth Look of BFT Consensus in Blockchain: Challenges and Opportunities. In
Proceedings of the 20th International Middleware Conference Tutorials, Middleware.
ACM, 6‚Äì10. https://doi.org/10.1145/3366625.3369437

[27] Suyash Gupta, Jelle Hellings, Sajjad Rahnama, and Mohammad Sadoghi. 2020.
Building High Throughput Permissioned Blockchain Fabrics: Challenges and
Opportunities. Proc. VLDB Endow. 13, 12 (2020), 3441‚Äì3444. https://doi.org/10.
14778/3415478.3415565

[28] Suyash Gupta, Jelle Hellings, Sajjad Rahnama, and Mohammad Sadoghi. 2021.
Proof-of-Execution: Reaching Consensus through Fault-Tolerant Speculation. In
Proceedings of the 24th International Conference on Extending Database Technology,
EDBT. OpenProceedings.org, 301‚Äì312. https://doi.org/10.5441/002/edbt.2021.27
[29] Suyash Gupta, Jelle Hellings, and Mohammad Sadoghi. 2019. Brief Announce-
ment: Revisiting Consensus Protocols through Wait-Free Parallelization. In 33rd
International Symposium on Distributed Computing, DISC (LIPIcs), Vol. 146. Schloss
Dagstuhl - Leibniz-Zentrum f√ºr Informatik, 44:1‚Äì44:3. https://doi.org/10.4230/
LIPIcs.DISC.2019.44

[30] Suyash Gupta, Jelle Hellings, and Mohammad Sadoghi. 2021. Fault-Tolerant
Distributed Transactions on Blockchain. Morgan & Claypool Publishers. https:
//doi.org/10.2200/S01068ED1V01Y202012DTM065

[31] Suyash Gupta, Jelle Hellings, and Mohammad Sadoghi. 2021. RCC: Resilient
Concurrent Consensus for High-Throughput Secure Transaction Processing.
In 37th IEEE International Conference on Data Engineering, ICDE. 1392‚Äì1403.
https://doi.org/10.1109/ICDE51399.2021.00124

[32] Suyash Gupta, Sajjad Rahnama, Jelle Hellings, and Mohammad Sadoghi. 2020.
ResilientDB: Global Scale Resilient Blockchain Fabric. Proc. VLDB Endow. 13, 6
(2020), 868‚Äì883. https://doi.org/10.14778/3380750.3380757

[33] Suyash Gupta, Sajjad Rahnama, Shubham Pandey, Natacha Crooks, and Moham-
mad Sadoghi. 2022. Dissecting BFT Consensus: In Trusted Components we Trust!
CoRR abs/2202.01354 (2022). arXiv:2202.01354

[34] Suyash Gupta, Sajjad Rahnama, and Mohammad Sadoghi. 2020. Permissioned
Blockchain Through the Looking Glass: Architectural and Implementation
Lessons Learned. In 40th IEEE International Conference on Distributed Computing
Systems, ICDCS. 754‚Äì764. https://doi.org/10.1109/ICDCS47774.2020.00012

[35] Suyash Gupta and Mohammad Sadoghi. 2018. EasyCommit: A Non-blocking
Two-phase Commit Protocol. In Proceedings of the 21st International Conference
on Extending Database Technology, EDBT. OpenProceedings.org, 157‚Äì168. https:
//doi.org/10.5441/002/edbt.2018.15

[36] Suyash Gupta and Mohammad Sadoghi. 2019. Blockchain Transaction Processing.
In Encyclopedia of Big Data Technologies. Springer, 1‚Äì11. https://doi.org/10.1007/
978-3-319-63962-8_333-1

[37] Suyash Gupta and Mohammad Sadoghi. 2020. Efficient and non-blocking
agreement protocols. Distributed Parallel Databases 38, 2 (2020), 287‚Äì333.
https://doi.org/10.1007/s10619-019-07267-w

[38] R. Harding, D. Van Aken, A. Pavlo, and M. Stonebraker. 2017. An Evaluation
of Distributed Concurrency Control. Proc. VLDB Endow. 10, 5 (2017), 553‚Äì564.
https://doi.org/10.14778/3055540.3055548

[39] Thomas Haynes and David Noveck. 2015. RFC 7530: Network File System (NFS)

Version 4 Protocol. https://tools.ietf.org/html/rfc7530

[40] Jelle Hellings, Daniel P. Hughes, Joshua Primero, and Mohammad Sadoghi. 2020.
Cerberus: Minimalistic Multi-shard Byzantine-resilient Transaction Processing.
https://arxiv.org/abs/2008.04450

[41] Jelle Hellings and Mohammad Sadoghi. 2019. The fault-tolerant cluster-sending

problem. https://arxiv.org/abs/1908.01455

[42] Jelle Hellings and Mohammad Sadoghi. 2021. ByShard: Sharding in a Byzantine

Environment. Proc. VLDB Endow. 14, 11 (2021), 2230‚Äì2243.

[43] Jonathan Katz and Yehuda Lindell. 2014. Introduction to Modern Cryptography

(2nd ed.).

[44] Eleftherios Kokoris-Kogias, Philipp Jovanovic, Linus Gasser, Nicolas Gailly, Ewa
Syta, and Bryan Ford. 2018. OmniLedger: A Secure, Scale-Out, Decentralized
Ledger via Sharding. In 2018 IEEE Symposium on Security and Privacy (SP). 583‚Äì
598. https://doi.org/10.1109/SP.2018.000-5

[45] Ramakrishna Kotla, Lorenzo Alvisi, Mike Dahlin, Allen Clement, and Edmund
Wong. 2007. Zyzzyva: Speculative Byzantine Fault Tolerance. SIGOPS Oper. Syst.
Rev. 41, 6 (2007), 45‚Äì58. https://doi.org/10.1145/1323293.1294267

[46] Ramakrishna Kotla, Lorenzo Alvisi, Mike Dahlin, Allen Clement, and Edmund
Wong. 2010. Zyzzyva: Speculative Byzantine Fault Tolerance. ACM Trans. Comput.
Syst. 27, 4, Article 7 (2010), 39 pages. https://doi.org/10.1145/1658357.1658358

[47] Leslie Lamport. 1998. The Part-time Parliament. (1998).
[48] Chenxing Li, Peilun Li, Dong Zhou, Wei Xu, Fan Long, and Andrew Yao. 2018.
Scaling Nakamoto Consensus to Thousands of Transactions per Second. https:
//arxiv.org/abs/1805.03870

[49] Loi Luu, Viswesh Narayanan, Chaodong Zheng, Kunal Baweja, Seth Gilbert, and
Prateek Saxena. 2016. A Secure Sharding Protocol For Open Blockchains. In
Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications
Security. ACM, 17‚Äì30. https://doi.org/10.1145/2976749.2978389

[50] Dahlia Malkhi and Michael Reiter. 1998. Byzantine quorum systems. Distributed
Computing 11, 4 (1998), 203‚Äì213. https://doi.org/10.1007/s004460050050
[51] Dahlia Malkhi and Michael Reiter. 1998. Secure and scalable replication in Phalanx.
In Proceedings Seventeenth IEEE Symposium on Reliable Distributed Systems. IEEE,
51‚Äì58. https://doi.org/10.1109/RELDIS.1998.740474

[52] Ralph C. Merkle. 1988. A Digital Signature Based on a Conventional Encryption
Function. In Advances in Cryptology ‚Äî CRYPTO ‚Äô87. Springer, 369‚Äì378. https:
//doi.org/10.1007/3-540-48184-2_32

[53] Andrew Miller, Yu Xia, Kyle Croman, Elaine Shi, and Dawn Song. 2016. The Honey
Badger of BFT Protocols. In Proceedings of the 2016 ACM SIGSAC Conference on
Computer and Communications Security (CCS ‚Äô16). ACM, 31‚Äì42. https://doi.org/
10.1145/2976749.2978399

[54] The Council of Economic Advisers. 2018. The Cost of Malicious Cyber Activity
to the U.S. Economy. Technical Report. Executive Office of the President of the
United States. https://www.whitehouse.gov/wp-content/uploads/2018/03/The-
Cost-of-Malicious-Cyber-Activity-to-the-U.S.-Economy.pdf

[55] Diego Ongaro and John Ousterhout. 2014.

In Search of an Understandable

Consensus Algorithm. In ATC.

[56] M. Tamer √ñzsu and Patrick Valduriez. 2020. Principles of Distributed Database

Systems. Springer. https://doi.org/10.1007/978-3-030-26253-2

[57] Thamir Qadah, Suyash Gupta, and Mohammad Sadoghi. 2020. Q-Store: Dis-
tributed, Multi-partition Transactions via Queue-oriented Execution and Com-
munication. In Proceedings of the 23rd International Conference on Extending
Database Technology, EDBT. OpenProceedings.org, 73‚Äì84. https://doi.org/10.
5441/002/edbt.2020.08

[58] Sajjad Rahnama, Suyash Gupta, Thamir Qadah, Jelle Hellings, and Mohammad
Sadoghi. 2020. Scalable, Resilient and Configurable Permissioned Blockchain
Fabric. Proc. VLDB Endow. 13, 12 (2020), 2893‚Äì2896. https://doi.org/doi.org/10.
14778/3415478.3415502

[59] Mohammad Sadoghi and Spyros Blanas. 2019. Transaction Processing on Mod-
https://doi.org/10.2200/

ern Hardware. Morgan & Claypool Publishers.
S00896ED1V01Y201901DTM058

[60] Amit P. Sheth and James A. Larson. 1990. Federated Database Systems for Man-
aging Distributed, Heterogeneous, and Autonomous Databases. ACM Comput.
Surv. 22, 3 (Sept. 1990), 183‚Äì236. https://doi.org/10.1145/96602.96604

Sajjad Rahnama

Suyash Gupta

Rohan Sogani Dhruv Krishnan Mohammad Sadoghi

[61] Dale Skeen. 1982. A Quorum-Based Commit Protocol. Technical Report. Cornell

[64] Gerard Tel. 2001. Introduction to Distributed Algorithms (2nd ed.). Cambridge

University.

University Press.

[62] Yonatan Sompolinsky, Yoad Lewenberg, and Aviv Zohar. 2016. SPECTRE: A Fast
and Scalable Cryptocurrency Protocol. https://eprint.iacr.org/2016/1159.
[63] Gadi Taubenfeld and Shlomo Moran. 1996. Possibility and impossibility results
in a shared memory environment. Acta Informatica 33, 1 (1996), 1‚Äì20. https:
//doi.org/10.1007/s002360050034

[65] Alexander Thomson, Thaddeus Diamond, Shu-Chun Weng, Kun Ren, Philip
Shao, and Daniel J. Abadi. 2012. Calvin: Fast Distributed Transactions for
Partitioned Database Systems. In Proceedings of the 2012 ACM SIGMOD Inter-
national Conference on Management of Data (SIGMOD). ACM, 1‚Äì12.
https:
//doi.org/10.1145/2213836.2213838

