2
2
0
2

y
a
M
4
1

]

C
D
.
s
c
[

1
v
1
4
9
6
0
.
5
0
2
2
:
v
i
X
r
a

Blockchain Goes Green?
Part II: Characterizing the Performance and Cost of Blockchains
on the Cloud and at the Edge

Dumitrel Loghin
School of Computing
National University of Singapore
dumitrel@comp.nus.edu.sg

Chen Gang
College of Computer Science and
Technology, Zhejiang University,
China
cg@zju.edu.cn

Tien Tuan Anh Dinh
Information Systems Technology and
Design
Singapore University of Technology
and Design
dinhtta@comp.nus.edu.sg

Aung Maw
Information Systems Technology and
Design
Singapore University of Technology
and Design
aung_maw@comp.nus.edu.sg

Yong Meng Teo
School of Computing
National University of Singapore
teoym@comp.nus.edu.sg

Beng Chin Ooi
School of Computing
National University of Singapore
ooibc@comp.nus.edu.sg

ABSTRACT
While state-of-the-art permissioned blockchains can achieve thou-
sands of transactions per second on commodity hardware with
x86/64 architecture, their performance when running on diÔ¨Äerent
architectures is not clear. The goal of this work is to character-
ize the performance and cost of permissioned blockchains on dif-
ferent hardware systems, which is important as diverse applica-
tion domains are adopting t. To this end, we conduct extensive
cost and performance evaluation of two permissioned blockchains,
namely Hyperledger Fabric and ConsenSys Quorum, on Ô¨Åve diÔ¨Äer-
ent types of hardware covering both x86/64 and ARM architecture,
as well as, both cloud and edge computing. The hardware nodes in-
clude servers with Intel Xeon CPU, servers with ARM-based Ama-
zon Graviton CPU, and edge devices with ARM-based CPU. Our
results reveal a diverse proÔ¨Åle of the two blockchains across diÔ¨Äer-
ent settings, demonstrating the impact of hardware choices on the
overall performance and cost. We Ô¨Ånd that Graviton servers out-
perform Xeon servers in many settings, due to their powerful CPU
and high memory bandwidth. Edge devices with ARM architecture,
on the other hand, exhibit low performance. When comparing the
cloud with the edge, we show that the cost of the latter is much
smaller in the long run if manpower cost is not considered.

1 INTRODUCTION
Blockchain is increasingly adopted by users and businesses around
the globe. While blockchain technology became famous through
public, permissionless chains, such as Bitcoin [38] and Ethereum [20]
which are used primarily for cryptocurrency transfers, there is a
growing interest for permissioned blockchains1. These permis-
sioned blockchains, also termed as private, consortium or enter-
prise blockchains, allow only authenticated entities to be part of
the system. As such, they often relax the security aspect of the
consensus protocol by employing Byzantine fault-tolerant (BFT)
alternatives to Proof-of-Work (PoW) such as Practical Byzantine

1In this paper, we only analyze permissioned blockchains. Unless otherwise speciÔ¨Åed,
the term blockchain in this paper means permissioned blockchain.

1

Fault Tolerance (PBFT) [21] or even crash fault-tolerant (CFT) pro-
tocols such as Raft [39]. State-of-the-art permissioned blockchains
can achieve thousands of transactions per second, by improving
consensus, network performance, or by adopting database tech-
niques [22, 29, 55]. However, we note that these systems are de-
signed for traditional enterprise workloads, and are evaluated only
on commodity or cluster-grade servers with x86/64 architecture,
being it at the edge or on the cloud. For example, AHL [22] uses
general instances on Google Cloud Platform spanning multiple re-
gions.

As blockchains are maturing, we observe that more and more
applications are adopting them. Besides the traditional enterprise
data processing applications, such as payment systems, emerging
blockchain applications include blockchain cloud services [1, 13],
and edge computing [4, 14]. These applications come with diÔ¨Äerent
hardware resource demands, that is, they may not run eÔ¨Éciently
on x86/64 commodity hardware [33, 47, 48]. Besides x86/64, we
have witnessed the proliferation of ARM architecture, which Ô¨Årst
started with mobile devices. Recently, ARM has been adopted as
an alternative architecture in cloud computing [16]. However, the
performance and cost of running blockchains on ARM-based hard-
ware are not clear. As a result, users cannot make an informed
decision of what system to run their blockchain on.

Our goal is to provide an in-depth analysis of blockchain perfor-
mance and cost on diÔ¨Äerent CPU architectures, which helps inform
the design of emerging blockchain applications. To this end, we se-
lect Ô¨Åve representative edge and cloud systems, including servers
with Intel Xeon CPU, ARM-based Amazon Graviton instances [16],
high-end and low-end edge devices with ARM CPU represented by
Nvidia Jetson TX2 [26] and Raspberry Pi 4 [2], respectively. We run
two representative permissioned blockchains on these hardware
nodes, namely, Hyperledger Fabric [9, 19] and ConsenSys Quo-
rum [8]. The former is widely used in the industry and well studied
by the database community in the last few years [22, 25, 43, 44, 46].
Fabric adopts an execute-order-validate transaction Ô¨Çow and cur-
rently uses Raft as a consensus mechanism for its ordering service.
Quorum [8] adopts a more traditional order-execute transaction

 
 
 
 
 
 
Ô¨Çow. We use throughput and latency as performance metrics when
running the two blockchains on the selected hardware. Our cost
metric consists of both the Ô¨Åxed hardware cost and the continuing
energy cost. We note that our work is the Ô¨Årst to quantify the cost
of operating a permissioned blockchain.

In summary, the following key contributions are presented in

this paper:

‚Ä¢ We conduct a systematic performance analysis of two rep-
resentative permissioned blockchains, namely, Fabric and
Quorum, on Ô¨Åve types of systems covering both x86/64 and
ARM architectures, as well as both edge and cloud setups.
These Ô¨Åve types of systems are (i) AWS cloud instances with
Intel Xeon CPU, (ii) AWS cloud instances with ARM Gravi-
ton CPU, (iii) edge-based servers with Intel Xeon CPU, (iv)
edge-based Nvidia Jetson TX2 with ARM CPU, and (v) edge-
based Raspberry Pi 4 with ARM CPU.

‚Ä¢ We show that ARM-based Graviton instances are more cost-
eÔ¨Äective than Intel Xeon instances. For example, Graviton
achieves close to 10% higher throughput for Fabric and 25%
lower throughput for Quorum compared to Xeon, while be-
ing 35% cheaper. This, in turn, stems from the impressive
CPU, memory, and networking sub-systems of Graviton. For
example, we show that a Graviton instance exhibits 2√ó higher
main memory bandwidth compared to the Xeon-based in-
stance.

‚Ä¢ On the other hand, we show that both Fabric and Quorum
do not eÔ¨Éciently utilize the available hardware resources.
For example, the average number of cores used by Fabric
and Quorum is 1.3 and 0.6, respectively. In addition, Fabric
exhibits a high volume of cache misses that hinders eÔ¨Écient
execution.

‚Ä¢ Finally, we show that hosting blockchain nodes at the edge
(on-premise) may be cheaper if the manpower cost for op-
erating and maintaining them is disregarded. In contrast,
adding manpower cost leads to higher cost at the edge com-
pared to the cloud.

The rest of this paper is organized as follows. In Section 2 we
give an overview of blockchain frameworks and present related
works that analyze the performance of blockchains. In Section 3
we describe our experimental setup, and in Section 4 we conduct
our in-depth analysis. We conclude the paper in Section 5.

2 BACKGROUND AND RELATED WORK
In this section, we provide a background on blockchain and survey
the related work on time, energy, and cost analysis of blockchains.
Due to space constraints, our background on blockchain is brief.
We direct the reader to other works [24, 42, 43] for more details on
blockchain systems.

2.1 Blockchain
A blockchain is a distributed ledger managed by a network of mu-
tually distrusting nodes (or peers). The ledger is stored as a linked
list (or chain) of blocks, where each block consists of transactions.
The links in the chain are built using cryptographic pointers to en-
sure that no one can tamper with the chain or with the data inside
a block.

2

Blockchains are most famous for being the underlying technol-
ogy of cryptocurrencies, but many can support general-purpose
applications. This ability is determined by the execution engine
and data model. For example, Bitcoin [38] supports only opera-
tions related to cryptocurrency (or token) manipulation. On the
other hand, Ethereum [20] can run arbitrary computations on its
Turing-complete Ethereum Virtual Machine (EVM). At the data
model level, there are at least three alternatives used in practice.
The Unspent Transaction Output (UTXO) model, used by Bitcoin
among others, represents the ledger states as transaction ids and as-
sociated unspent amounts that are the input of future transactions.
The account/balance model resembles a classic banking ledger. A
more generic model used by Hyperledger Fabric consists of key-
value states. On top of the data model, developers can write general
applications that operate on the blockchain‚Äôs states. Such applica-
tions are called smart contracts. In this paper, we extend the smart
contracts from Blockbench [25] to support Fabric (v2.3.1) and Quo-
rum.

Depending on how nodes can join the network, the blockchain
is public (or permissionless) or private (or permissioned). In pub-
lic networks, anybody can join or leave and, thus, the security
risks are high. Most of the cryptocurrency blockchains are pub-
lic, such as Bitcoin [38] and Ethereum [20]. On the other hand,
private blockchains allow only authenticated peers to join the net-
work. Typically, private blockchains, such as Fabric [19] and Quo-
rum [8], are deployed inside or across big organizations.

Blockchains operate in a network of mutually distrusting peers,
where some peers may not be just faulty but malicious. Hence, they
assume a Byzantine environment [32], in contrast to the crash-
failure model used by the majority of distributed systems. For ex-
ample, Proof-of-Work (PoW) is a BFT consensus mechanism where
participating nodes, called miners, need to solve a diÔ¨Écult crypto-
graphic puzzle. The miner that solves the puzzle Ô¨Årst has the right
to append transactions to the ledger. Since this mining process is
both time and energy ineÔ¨Écient, some alternatives have been pro-
posed, such as Proof-of-Stake (PoS) [12], Proof-of-Authority (PoA),
and Proof-of-Elapsed-Time (PoET) [5]. In PoS, the nodes need to
set aside some coins (the stake) based on which the validator of
each block is chosen. In case a malicious validator is detected, it
loses its stake. Through this mechanism, malicious behavior is dis-
couraged. In PoA, some nodes with known identity, called valida-
tors, are trusted to validate all the transactions. In case a trusted
node acts maliciously, its reputation is aÔ¨Äected and it may be re-
moved from the network. In PoET, each node needs to wait for a
random period, before being able to propose a new block. The node
with the smallest wait period is the one appending the next block.
On the other hand, PBFT [21] consists of exchanging ùëÇ (ùëõ2) mes-
sages among the nodes to reach an agreement on the transactions
to be appended to the chain. However, BFT consensus does not
scale well and leads to low blockchain throughput [25, 43]. This is
one reason why permissioned blockchains started to replace BFT
with CFT consensus. For example, both Fabric and Quorum sup-
port Raft consensus [39]. Another reason for using CFT consensus
in permissioned chains is the higher accountability due to node
authentication in such platforms.

2.2 Performance Analysis of Blockchains
There are a number of related works that analyze the performance
of blockchains [24], [25], [40], [52], [54], [43]. However, only a few
include energy or cost analysis [15], [33], [45], [51], but their anal-
ysis is of limited depth.

Sankaran et al. [45] analyze the time and energy performance
of an in-house Ethereum network consisting of high-performance
mining servers and low-power Raspberry Pi 3 clients. These low-
power nodes cannot run Ethereum mining due to their limited
memory size, hence, they only take the role of clients. In this pa-
per, we focus on permissioned blockchains and use Quorum as a
representative of Ethereum-based blockchains. On the other hand,
we use low-power devices with higher performance, such as Jetson
TX2 and Raspberry Pi 4.

MobiChain [51] is an approach that allows mining on mobile
devices running Android OS, in the context of mobile e-commerce.
While it provides an analysis of both time and energy performance,
MobiChain has no comparison to other blockchains. In terms of en-
ergy analysis, the authors show that it is more energy-eÔ¨Écient to
group multiple transactions in a single block since there is less min-
ing work and therefore less time and power wasted in this process.
However, larger blocks increase latency and result in a poor user
experience.

Jupiter [30] is a blockchain designed for mobile devices. It aims
to address the problem of storing a large ledger on mobile devices
with limited storage capacity. The testbed in [28] is based on 14
Raspberry Pi 3 nodes running Hyperledger Fabric version 1.0. How-
ever, there is no time or energy performance evaluation in both of
these works [30], [28].

Ruan et al. [43] compare blockchains with distributed database
systems using a taxonomy with four dimensions, namely, replica-
tion, concurrency, storage, and sharding. Similar to this paper, they
analyze Fabric and Quorum but their analysis is only focusing on
the time performance and it is conducted on commodity x86/64
servers. Complementary to our analysis of blockchains on diÔ¨Äerent
hardware systems, [43] evaluates the eÔ¨Äect of diÔ¨Äerent blockchain
and benchmarking parameters, such as record size, block size, repli-
cation model, failure model, among others.

Blockbench [25] is a benchmarking suite comprising both sim-
ple (micro) benchmarks and complex (macro) benchmarks. The
microbenchmarks, namely CPUHeavy, IOHeavy, and Analytics are
stressing diÔ¨Äerent hardware subsystems such as the CPU, memory,
and IO. At the same time, the microbenchmarks evaluate the per-
formance of diÔ¨Äerent blockchain layers. For example, CPUHeavy
evaluates the performance of the execution engine, while IOHeavy
evaluates the performance of the data storage. The macro bench-
marks are represented by YCSB, Smallbank, and Donothing. The
YCSB macro benchmark implements a key-value storage, while
Smallbank represents OLTP and simulates basic banking opera-
tions. The Donothing benchmark is used to evaluate the consen-
sus protocol since it does not engage the execution and data stor-
age layers. The performance in terms of throughput and latency is
evaluated on traditional high-performance servers with Intel Xeon

(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:2)(cid:6)(cid:6)(cid:7)(cid:5)

(cid:12)(cid:2)(cid:23)(cid:7)(cid:5)
(cid:24)(cid:7)(cid:4)(cid:7)(cid:5)

(cid:27)(cid:4)(cid:16)(cid:5)(cid:4)(cid:9)(cid:1)(cid:6)(cid:8)(cid:7)(cid:3)(cid:4)(cid:14)(cid:28)
(cid:29)(cid:7)(cid:4)(cid:9)(cid:30)(cid:7)(cid:14)(cid:19)(cid:6)(cid:4)(cid:14)

(cid:27)(cid:7)(cid:4)(cid:19)(cid:31)(cid:9)(cid:16)(cid:3)(cid:15)(cid:9)(cid:27)(cid:4)(cid:16)(cid:5)(cid:4)(cid:9)(cid:12)(cid:7)(cid:7)(cid:5)(cid:14)(cid:28)
(cid:29)(cid:7)(cid:4)(cid:9)(cid:30)(cid:7)(cid:14)(cid:19)(cid:6)(cid:4)(cid:14)

(cid:1)(cid:2)(cid:6)(cid:6)(cid:7)(cid:17)(cid:4)(cid:9)(cid:12)(cid:2)(cid:23)(cid:7)(cid:5)(cid:9)
(cid:16)(cid:3)(cid:15)(cid:9)(cid:25)(cid:3)(cid:7)(cid:5)(cid:18)(cid:26)

(cid:14)(cid:7)(cid:3)(cid:15)(cid:9)(cid:4)(cid:5)(cid:16)(cid:3)(cid:14)(cid:16)(cid:17)(cid:4)(cid:8)(cid:2)(cid:3)(cid:14)

(cid:18)(cid:7)(cid:4)(cid:9)(cid:4)(cid:5)(cid:16)(cid:3)(cid:14)(cid:16)(cid:17)(cid:4)(cid:8)(cid:2)(cid:3)(cid:14)(cid:9)(cid:14)(cid:4)(cid:16)(cid:4)(cid:19)(cid:14)

(cid:14)(cid:7)(cid:3)(cid:15)(cid:9)(cid:4)(cid:5)(cid:16)(cid:3)(cid:14)(cid:16)(cid:17)(cid:4)(cid:8)(cid:2)(cid:3)(cid:14)

(cid:1)(cid:6)(cid:8)(cid:7)(cid:3)(cid:4)(cid:9)(cid:10)

(cid:13)(cid:9)(cid:13)(cid:9)(cid:13)

(cid:1)(cid:6)(cid:8)(cid:7)(cid:3)(cid:4)(cid:9)(cid:11)

(cid:12)(cid:7)(cid:7)(cid:5)(cid:9)(cid:10)

(cid:13)(cid:9)(cid:13)(cid:9)(cid:13)

(cid:12)(cid:7)(cid:7)(cid:5)(cid:9)(cid:11)

(cid:1)

(cid:18)(cid:7)(cid:4)(cid:9)(cid:4)(cid:5)(cid:16)(cid:3)(cid:14)(cid:16)(cid:17)(cid:4)(cid:8)(cid:2)(cid:3)(cid:14)(cid:9)(cid:14)(cid:4)(cid:16)(cid:4)(cid:19)(cid:14)

(cid:20)(cid:6)(cid:2)(cid:17)(cid:21)(cid:17)(cid:22)(cid:16)(cid:8)(cid:3)(cid:9)
(cid:11)(cid:7)(cid:4)(cid:23)(cid:2)(cid:5)(cid:21)

(cid:1)

Figure 1: Experimental Approach

CPU. We note that Blockbench analyzes the performance of Fab-
ric v0.6 with PBFT. In this paper, we extend Blockbench to sup-
port Fabric v2.3.1 and Quorum v20.10.0. Moreover, we do not ana-
lyze only the time performance, but also the power and cost of a
wider range of node types, including both x86/64 and ARM archi-
tectures. To the best of our knowledge, we provide the Ô¨Årst exten-
sive time, energy, and cost performance analysis of permissioned
blockchains on both x86/64 and ARM architectures, as well as both
at the edge and on the cloud.

3 EXPERIMENTAL SETUP
In this section, we describe our experimental setup in terms of
blockchains, benchmarks, and hardware nodes.

3.1 Overview
We illustrate our experimental approach in Figure 1. The setup con-
sists of a blockchain network comprising ùëÅ peers, a set of clients
that send transactions to the peers, a controller, and a power meter
to collect power and energy measurements. For simplicity, we con-
sider ùëÅ clients, each client sending transactions to one blockchain
peer. In reality, a client can send transactions to multiple peers. The
controller node is used to set up the benchmarking environment,
start the peers, set up the smart contracts, start the clients, and
collect the results, including power and energy.

3.2 Blockchains and Benchmarks
The permissioned blockchain frameworks analyzed in this paper
are Hyperledger Fabric (v2.3.1) and ConsenSys Quorum (v20.10.0).
Fabric [9] is a crash fault-tolerant (CFT) permissioned blockchain
that adopts an execute-order-validate transaction Ô¨Çow. In a Fabric
network, there are three types of nodes: peers, orderers, and clients.
A client sends a transaction request to a set of peers, depending on
an endorsement policy. For example, an AND policy including all
the peers means that the client needs to send the transaction and re-
ceive endorsements from all the peers. A peer processes the trans-
action request and creates read and write sets to mark which states

3

are touched by the transaction. However, the peer does not persist
the transaction‚Äôs eÔ¨Äects on its local database. When the client gets
all the endorsements from the peers, it sends the transaction to
the orderers such that they will pack it in a block. Once a block
is formed, the orderers send it to all the peers. Lastly, each peer
validates the transactions in a block and persists the changes to its
local database. Fabric v2.3.1 uses Raft [39] as the consensus mech-
anism among the orderers, and LevelDB [23] as its local database.
Quorum [8] is a permissioned blockchain that started as a fork
of Ethereum. Hence, Quorum supports Solidity smart contracts,
but it uses diÔ¨Äerent consensus mechanisms than Ethereum because
of its permissioned nature. In this paper, we analyze both a CFT
Quorum that uses Raft as the consensus protocol among the peers,
as well as a BFT version that uses Istanbul BFT (IBFT) [37] as the
consensus protocol. In contrast to Fabric, a Quorum network has
only peers and clients, and the transaction Ô¨Çow is order-execute.
This means that transactions are Ô¨Årst grouped into blocks and then
executed by each peer in the network. Similar to Fabric, Quorum
uses LevelDB as its local database.

For benchmarking these blockchains, we extend Blockbench2 [25]
to support Fabric v2.3.1 and Quorum v20.10.0. That is, we imple-
mented Fabric and Quorum smart contracts for the key-value store
benchmark and the scripts needed to run the benchmark. We use
the YCSB macro-benchmark consisting of 50% read and 50% write
operations of single key-value pairs of 1kB in size. One or more
client nodes send transactions to the blockchain peers in an asyn-
chronous mode. That is, the request threads of the clients do not
wait for the result of a transaction. Instead, a status thread period-
ically queries the blockchain to get the committed transactions.

In our experiments, the block size of Fabric is set to 500 transac-
tions and the runtime for the benchmark is set to 120s. In Quorum,
there is no limit to block size. The runtime is set to at least 240s
to account for a slower startup compared to Fabric. We vary the
number of blockchain peers from four to ten and we use the client
request rate that leads to the best performance. In this paper, we
report the best results out of three runs for each experiment. We
note that the standard deviation is lower than 10% of the mean for
each experiment.

3.3 Hardware Systems
In this paper, we are analyzing the performance of Ô¨Åve types of
systems (hardware nodes) covering both edge and cloud comput-
ing, as well as both x86/64 and ARM architectures. The speciÔ¨Åca-
tions of these systems are summarized in Table 1. We measure the
power and energy of the edge nodes with a Yokogawa power me-
ter connected to the AC lines. We report the AC power values in
this paper.

At the edge, we measure the time performance and power of the
following three types of nodes. First, we have x86/64 nodes with
Intel Xeon E5-1650 v3 CPU clocked at 3.5 GHz, 32 GB DDR3 mem-
ory, 2 TB hard-disk (HDD), and 1 Gbps networking interface card
(NIC). These nodes, termed Xeon(edge) in this paper, run Ubuntu
18.04. Second, we use high-end ARM-based devices represented by
Nvidia Jetson TX2 [26]. A TX2 node has a heterogeneous 6-core

2The Blockbench source code used in our experiments can be found at
https://github.com/dloghin/blockbench/tree/analysis2021.

4

64-bit CPU with two NVIDIA Denver cores and four ARM Cortex-
A57 cores clocked at more than 2GHz. Each node has 8 GB LPDDR4,
a 32 GB SD card, and 1 Gbps NIC. TX2 is equipped with an inte-
grated low-power GPU. However, we are not using the GPU in
our experiments. The TX2 nodes are running Ubuntu 16.04 which
is oÔ¨Écially supported by Nvidia on these systems. Third, we use
low-end ARM-based devices represented by Raspberry Pi 4 model
B (RP4) [2]. An RP4 has a 4-core ARM Cortex-A72 CPU of 64-bit
ARM architecture, 8 GB of low-power DDR4 memory, a 64 GB SD
card that acts as storage, and 1 Gbps NIC. RP4 runs the beta version
of the 64-bit Debian-based Raspberry Pi OS [6].

On the cloud, we use two types of instances from Amazon Web
Services (AWS) to represent both x86/64 and ARM architectures.
AWS was chosen because it is the only widely-known cloud provider
that oÔ¨Äers ARM-based instances. We use m5n.2xlarge [49] AWS
instances to represent the x86/64 architecture. At the time of run-
ning the experiments, one such instance costs $0.476 per hour in
the US West (Oregon) region. These nodes, termed Xeon(cloud)
in this paper, are equipped with Intel Xeon Platinum 8259CL CPUs
clocked at 2.5 GHz. Each node has 8 CPU cores, 32 GB RAM, 50 GB
SSD storage, and up to 25 Gbps networking. A Xeon(cloud) node
runs Ubuntu 18.04 OS. Then, we use large [7] AWS instances to
represent the emerging 64-bit ARM server market. These instances,
termed Graviton in this paper, are based on Amazon‚Äôs Graviton
processors that use 64-bit ARM Neoverse architecture [16]. Each
node has 8 CPU cores, 32 GB RAM, 50 GB SSD, up to 10 Gbps
networking, and runs Ubuntu 18.04. At the time of running the
experiments, one such instance costs $0.308 per hour in the US
West (Oregon), being 1.55√ó or 35% cheaper than one Xeon(cloud)
m5n.2xlarge instance.

In summary, we note that cloud nodes use CPUs with more
cores, higher frequency, bigger cache size, and more levels of cache,
as shown in Table 1. Moreover, cloud nodes have bigger RAM sizes
and faster networking. We shall see in the next sections how these
hardware characteristics impact the performance of blockchain sys-
tems.

4 PERFORMANCE ANALYSIS
In this section, we conduct a systematic performance analysis of
the selected blockchains and hardware nodes.

4.1 Overview
We Ô¨Årst highlight the key Ô¨Åndings of our measurement-based eval-
uation, followed by a systematic in-depth analysis of the hardware
systems and blockchain frameworks. These key observations are
based on the throughput, latency, and power results presented in
Figure 2 for all three blockchains on all Ô¨Åve types of nodes.

Observation 1. Surprisingly, Graviton exhibits up to 8% higher
performance compared to Xeon(cloud) when running Fabric, as shown
in Figure 2a. On the other hand, Graviton exhibits 20% and 26% lower
throughput when running Quorum(Raft) and Quorum(IBFT), respec-
tively, compared to Xeon(cloud), as shown in Figure 2c and Figure 2e,
respectively. In the context of 35% lower cost of Graviton compared to
Xeon(cloud), the ARM-based server is more cost-eÔ¨Écient.

(a) Throughput of Fabric.

(b) Latency of Fabric.

(c) Throughput of Quorum(Raft).

(d) Latency of Quorum(Raft).

(e) Throughput of Quorum(IBFT).

(f) Latency of Quorum(IBFT).

(g) Power usage of Fabric.

(h) Power usage of Quorum.

Figure 2: Throughput, latency, and power with increasing number of peers.

5

Table 1: Hardware systems characterization.

Characteristic

ISA
Cores
Frequency
L1 Data Cache
L2 Cache
L3 Cache
Memory
Storage
Networking
CoreMark (one core) [IPS]
System power [W]
CoreMark (all cores) [IPS]
System power [W]
Idle system power [W]
Write throughput [MB/s]
Read throughput [MB/s]
BuÔ¨Äered read throughput [GB/s]
Write latency [ms]
Read latency [ms]
TCP bandwidth [Mbits/s]
UDP bandwidth [Mbits/s]
Ping latency [ms]

Xeon(edge) Xeon(cloud)
x86-64
8
2.5 GHz
32 kB
1 MB (core)
35.8 MB
32 GB DDR4
50 GB SSD
up to 25 Gbit
24,061.6
-
137,126.8
-
-
132
134
6.5
0.51
0.22
9530
4540
0.1

x86-64
6 (12)
1.2-3.5 GHz
32 kB
256 kB (core)
12 MB
32 GB DDR3
1 TB SSD
Gbit
25,201.6
70.6
170,864.7
115.5
50.8
160
172
8.1
9.3
2.5
941
810
0.14

Specs

CPU

Storage

Network

System

TX2

Graviton
AARCH64
8
2.5 GHz
64 kB
1 MB (core)
32 MB
32 GB

RP4
AARCH64 AARCH64/ARMv7l
4
0.6-1.5 GHz
32 kB
1 MB
N/A
8 GB LPDDR4
64 GB SD card
Gbit
8,555.1
3.6
34,255.1
5.7
1.7
19.0
46
1.3
1.33
0.72
943
957
0.15

6
0.346-2.04 GHz
32-128 kB
2 MB
N/A
8 GB LPDDR4
50 GB SSD 64 GB SD card
Gbit
9,936.1
4.2
68,092.3
10.4
2.4
16.3
89
2.7
17.1
2.8
943
546
0.3

up to 10 Gbit
20,266.2
-
162,054.4
-
-
58.3
141
6.9
0.49
0.22
9680
5800
0.09

Observation 2. As shown in Figure 2, Xeon(cloud) exhibits up
to 50% higher throughput compared to Xeon(edge) when running Fab-
ric, while Xeon(edge) exhibits up to 26% higher throughput compared
to Xeon(cloud) when running Quorum.

Observation 3. Surprisingly, the performance gap between Gravi-

ton and TX2 (or RP4) is big when running Fabric. As shown in Fig-
ure 2a, TX2 exhibits a throughput that is 6 ‚àí 9√ó lower compared to
Graviton. On the other hand, the throughput of TX2 running Quorum
is only 2 ‚àí 3√ó lower compared to Graviton.

Observation 4. While we expected TX2 to exhibit higher per-
formance than RP4, it is interesting to observe that TX2 is also more
power-eÔ¨Écient. This is because TX2 oÔ¨Äers more performance per unit
of energy compared to RP4, even if the latter uses less power.

Observation 5. The cost of hosting blockchain nodes at the
edge is much lower compared to the cloud, in the long run, when
manpower cost associated with operating and maintaining the edge
nodes is disregarded. Conversely, adding the manpower cost leads to
almost double cost at the edge compared to the cloud.

These observations give rise to a series of questions for which
we seek answers in the following sections. Here, we outline some
of these questions.

Question 1. Why does Graviton achieve higher performance
than Xeon(cloud) when running Fabric, while Xeon(cloud) has higher
performance when running Quorum?

Question 2. Why does Xeon(cloud) achieve higher performance
than Xeon(edge) when running Fabric, while Xeon(edge) has higher
performance when running Quorum?

6

Question 3. Why is there such a big performance gap between

TX2 and Graviton when running Fabric?

Question 4. Why is the power eÔ¨Éciency of RP4 lower compared

to TX2?

Question 5. Where and under what performance-cost circum-
stances should blockchain nodes be hosted: at the edge or on the cloud?

4.2 Systems Characterization
Before answering the above questions related to the blockchain
frameworks, we characterize the hardware systems using a series
of benchmarks that stress key sub-systems, such as CPU, memory,
storage, and networking.

To assess the performance of the CPU of each type of node, we
Ô¨Årst use CoreMark [17], a modern benchmark from Embedded Mi-
croprocessor Benchmark Consortium (EEBMC) designed to char-
acterize CPU cores of both x86/64 and ARM architectures. Core-
Mark estimates the CPU performance in terms of iterations per
second (IPS). In Figure 3a and Figure 3b, we show the performance
and average power usage of the Ô¨Åve systems running CoreMark
on a single core and all cores, respectively. For multi-core analysis,
we enable all available cores, including virtual cores in systems
that support Hyper-threading. The exception is Xeon(edge) which
has 12 virtual cores, but we benchmark only 8 of them for a fair
comparison with Xeon(cloud) and Graviton.

At the single-core level, Xeon(edge) achieves the highest perfor-
mance, followed by Xeon(cloud), Graviton, TX2, and RP4, which
are 1.05, 1.2, 2.5, and 2.9 times slower, respectively. Even if Xeon(edge)
has a newer Xeon CPU, it exhibits lower performance compared to

(a) CoreMark on one core

(b) CoreMark on all cores

Figure 3: Performance and power at CPU level.

Xeon(edge) due to lower clock frequency. That is, the Intel Xeon E5-
1650 of Xeon(edge) runs at 3.5 GHz, while the Intel Xeon Platinum
8259CL of Xeon(cloud) runs at 2.5 GHz. Nevertheless, the newer
Intel Xeon Platinum 8259CL is more eÔ¨Écient since it yields higher
performance per GHz. On the other hand, Graviton achieves im-
pressive performance for an ARM-based CPU [35]. Even if its clock
frequency is not oÔ¨Écially stated, our measurements suggest that
Graviton CPU runs at 2.5 GHz. Lastly, the performance of TX2 and
RP4 is lower compared to the other systems, but their power con-
sumption is also much lower. For example, TX2 and RP4 use 17√ó
and 20√ó, respectively, less power compared to Xeon(edge).

At the multi-core level, the surprise is Graviton which achieves
higher performance than Xeon(cloud) on 8 cores, as shown in Fig-
ure 3b. Even if the Intel Xeon Platinum 8259CL CPU has 24 phys-
ical cores, our measurements suggest that a m5n.2xlarge instance
uses only 4 physical cores, while the other 4 are Hyper-threading
cores. In contrast, a m6g.2xlarge instance uses 8 physical Graviton
cores. This leads to a higher performance of Graviton with 8 cores
compared to Xeon(cloud). On the other hand, RP4 suÔ¨Äers from hav-
ing only 4 cores, being 5√ó and 2√ó slower than Xeon(edge) and TX2,
respectively.

A signiÔ¨Åcant aspect of blockchain is the usage of cryptogra-
phy operations which are, in general, compute-intensive. Both Fab-
ric and Quorum use elliptic curve, namely ECDSA with secp256r1
and with secp256k1 respectively. Our second CPU benchmarking
assesses the performance of such cryptography operations. Our
proÔ¨Åling of Fabric shows that a signiÔ¨Åcant proportion of the CPU
time is spent in p256MulInternal and p256SqrInternal func-
tions (see Listing 2) which are part of the crypto/ecdsa Go pack-
age. This package implements the elliptic curve cryptography us-
ing secp256r1, also known as NIST P-256 [31]. We isolated the ECDSA
operations in Fabric‚Äôs Go code and measure the performance of
both sign and verify operations. For this benchmarking, we run
100,000 operations on messages of 64 B in size. The results for sign-
ing, shown in Figure 4, follow the same trend as the CoreMark
benchmark. The veriÔ¨Åcation has similar trends, just that it takes
around 3√ó longer than signing on all the systems. We also observe
that Xeon systems are more eÔ¨Écient as they can pack around two
instructions per cycle (IPC) compared to around one instruction

7

per cycle for the ARM-based systems. We note that performance
counters could not be obtained for Xeon(cloud) due to AWS restric-
tions.

In Quorum, a signiÔ¨Åcant part of the CPU time is spent in the
secp256k1_fe_sqr_inner and secp256k1_fe_mul_inner
calls
(see Listing 1) which are part of the crypto/secp256k1 package.
Secp256k1 is a way to compute the elliptic curve which is also em-
ployed in Bitcoin [3]. Hence, we benchmark the sign operation
which uses this function and we show the results in Figure 4b.
Interestingly, this cryptographic algorithm is around 22% slower
on Xeon(cloud) compared to Xeon(edge). We attribute this diÔ¨Äer-
ence to the higher clock frequency of Xeon(edge) and the fact that
secp256k1 is very compute-intensive and highly optimized. This
can be observed from the high IPC yielded on all the systems. On
Xeon and Graviton, more than three instructions are executed in
one cycle, while on TX2 and RP4 there are more than two instruc-
tions per cycle. For such an application, the CPU clock frequency
matters more, hence, Xeon(edge) is the fastest system.

Next, we assess the performance of the memory sub-system in
terms of read-write bandwidth measured with lmbench [50]. The
results in Figure 5 represent the read-write bandwidth measured
with lmbench [50]. At level one cache (L1), Xeon systems have the
highest bandwidth, and this can be correlated with the clock fre-
quency of the cores. However, at the main memory level, we have
a surprise: Graviton exhibits close to 20 GB/s, two times more than
the Xeon systems. This is a remarkable feature of the system de-
signed by Amazon. TX2 and RP4 exhibit main memory bandwidths
of around 4 GB/s and 1.3 GB/s, respectively. This low bandwidth,
together with the relatively small memory size hinder the execu-
tion of modern workloads on these ARM-based edge nodes.

The storage IO sub-system plays a key role in blockchain since
the ledger and the state database need to be written to persistent
storage. Hence, we measure the bandwidth and latency of the stor-
age sub-system using dd and ioping Linux commands, respectively.
The results expose a complex landscape. While the Xeon systems
exhibit high bandwidth for both reads and writes, the ARM-based

(a) secp256r1 (Fabric).

(b) secp256k1 (Quorum).

Figure 4: Performance of ECDSA signing operations

Lastly, we discuss the diÔ¨Äerence in using the experimental 64-bit
OS on RP4 compared to the oÔ¨Écial 32-bit OS. A 64-bit OS better
matches the 64-bit ARM CPU of RP4, and this is well highlighted
by the benchmarking results. For example, there is a 10% improve-
ment in CoreMark performance when using the 64-bit OS, while
consuming the same power. For ECDSA the diÔ¨Äerence is much
more signiÔ¨Åcant. Both the signing and veriÔ¨Åcation are 8√ó faster on
the 64-bit OS. This is due to the very ineÔ¨Écient implementation of
the algorithms on the 32-bit Instruction Set Architecture (ISA). On
32-bit, there are 8√ó more instructions executed compared to the
64-bit ISA. At memory and networking levels we did not observe
any signiÔ¨Åcant diÔ¨Äerence between the two types of OS. However,
we observed a slight improvement in storage performance. There
is a 2√ó improvement in both direct write and buÔ¨Äered read band-
widths. This is due to more eÔ¨Écient drivers for the direct write,
and double access size for the buÔ¨Äered read which uses the main
memory. In summary, we recommend the use of the 64-bit OS on
RP4 for improved performance.

4.3 Performance Analysis
In this section, we aim to answer the questions related to the per-
formance of the blockchain frameworks. We start by answering
the Ô¨Årst part of Question 1, that is, why does Graviton achieve
higher performance than Xeon(cloud) when running Fabric? Based
on our benchmarking in Section 4.2, the advantage of Graviton
may stem from its higher performance when using all the cores or
from its higher memory bandwidth compared to Xeon(cloud). To
further investigate this, we proÔ¨Åle Fabric with perf Linux tool. We
observe that the average number of CPU cores used at runtime is
below 1.3, meaning that Fabric mostly uses a single core. Hence,
Graviton could not take advantage of its higher multi-core perfor-
mance. On the other hand, memory references have a higher im-
pact on the performance of Fabric. Our analysis shows between 5
and 6.5 billion last-level cache (LLC) references in 120 seconds, out
of which 10% are misses. Let us suppose that all LLC references are
hits, each reference takes 60 cycles [10], the references are not over-
lapped, and the clock frequency is 3.5 GHz. This results in 90-110s

8

Figure 5: Memory bandwidth.

systems expose asymmetric performance where the write opera-
tions are a few times slower than the read operations. This is ex-
pected since TX2 and RP4 are equipped with SD cards and Gravi-
ton is equipped with NVMe-based SSD. On the other hand, the
latency of both reads and writes is higher on Xeon(edge) com-
pared to Xeon(cloud) and we attribute this to mechanical hard-disk
(HDD) versus solid-state disk (SSD).

At the networking level, we measure the bandwidth and latency
using iperf and ping Linux commands, respectively, and summa-
rize the results in Table 1. As per their speciÔ¨Åcations, the cloud-
based systems have higher bandwidth which is close to 10 Gbps.
In contrast, edge-based systems have bandwidths close to 1 Gbps.
The slightly higher ping latency of TX2 and RP4 can be attributed
to the lower clock frequency of these wimpy nodes. To validate this
hypothesis, we measured the networking latency while disabling
the DVFS by Ô¨Åxing the clock frequency. TX2 supports twelve fre-
quency steps in the range 346 MHz-2.04 GHz. RP4 supports ten
frequency steps in the range 600 MHz-1.5 GHz. We obtained Pear-
son correlation coeÔ¨Écients of -0.93 and -0.84 for TX2 and RP4, re-
spectively, between the frequency and networking latency. These
coeÔ¨Écients suggest strong inverse proportionality and expose the
impact of CPU processing on the networking stack.

Figure 6: EÔ¨Äect of networking bandwidth on Fabric.

Figure 7: Throughput of Fabric with increasing number of
orderers.

out of the 120s spent in accessing the LLC. Although this is a sim-
pliÔ¨Åed analysis, it shows that Fabric‚Äôs execution is cache/memory-
intensive. As such, Graviton achieves higher performance due to
its higher LLC and main memory bandwidth.

In contrast, the execution of Quorum exhibits around 4.5 and 7.6
billion LLC references in 360s on follower peers and the leader peer,
respectively. With the same assumptions as above, the time spent
in accessing the LLC is around 80s, or 131s for the leader, out of the
360s of runtime. Hence, Quorum is less cache/memory-intensive
compared to Fabric. Even at CPU core utilization, Quorum is less
intensive compared to Fabric since it uses only 0.6 and 0.8 cores,
on average, with Raft and IBFT, respectively. Hence, Quorum ex-
hibits higher performance on the Xeon-based systems compared
to Graviton, but this is because of poor utilization of the hardware
resources.

Next, we answer Question 2, namely, why does Xeon(cloud)
achieve higher performance than Xeon(edge) when running Fab-
ric, while Xeon(edge) has higher performance when running Quo-
rum? We show in Section 4.2 that Xeon(edge) has better CPU per-
formance but lower storage access latency performance and net-
working bandwidth compared to Xeon(cloud). Hence, we investi-
gate the eÔ¨Äect of slower storage and diÔ¨Äerent networking band-
width on Fabric. To investigate the eÔ¨Äect of slower storage, we in-
troduce artiÔ¨Åcial delays during the read and write operations in the
YCSB smart contract. However, we do not observe any diÔ¨Äerence
in Fabric‚Äôs throughput.

To investigate the eÔ¨Äect of slower networking, we use tc Linux
tool to limit the bandwidth of the networking interface on each
Xeon(cloud) peer of the Fabric network. As shown in Figure 6 for
six Fabric peers and one orderer, networking bandwidth has a sig-
niÔ¨Åcant impact on the performance of Fabric. When limiting the
bandwidth of Xeon(cloud) to 1 Gbps or 500 Mbps, Fabric‚Äôs through-
put is similar to the one on Xeon(edge). If the bandwidth is further
limited to 200 and 100 Mbps, the throughput drastically decreases
to around 350 and 170 tps, respectively. This can be explained by
the interplay between transaction size, request rate, and network-
ing bandwidth. In our experiments, each transaction operates on a
record (key-value) of around 1 kB. The endorsement policy is AND,
meaning that each of the six peers needs to endorse the transaction
and send the read-write sets back to the client. When the client
submits the transaction to the ordering service, the overhead is

9

(ùëÅ + 1)√ó due to the read-write states endorsed by the ùëÅ peers plus
the original transaction. Let us take the example of the 3,000 tps
request rate. This leads to at least 21 MB/s sent to the ordering ser-
vice, without considering the overheads of meta-data. This is close
to the 25 MB/s bandwidth on a 200 Mbps link.

At a closer look, we observe that Fabric‚Äôs ordering service is a
bottleneck because all the transactions and their endorsements are
sent to this service. But in our default setup, we use a single orderer.
Hence, we increase the number of orders, selecting setups with 4, 6,
and 8 orders. We then run experiments in both the 10 Gbps and 200
Mbps networks. As shown in Figure 7, increasing the number of
orderers to a certain value may help in a low-bandwidth network.
For example, having four orderers instead of one leads to a change
in throughput from 350 tps to 550 tps in a 200 Mbps network. But
increasing the number of orders further leads to a drop in through-
put. For example, using 8 orderers in a 200 Mbps network results
in 175 tps. This is because of Raft overhead which grows with the
number of nodes that need to follow the leader. This overhead im-
pacts the performance of Fabric even in high-bandwidth networks,
as shown in Figure 7 for the 10 Gbps link.

Similarly, we can explain the drop in throughput shown by 10
Xeon(edge) peers in Figure 2a. When we limit the bandwidth of
Xeon(cloud) to 1 Gbps, we obtain a throughput of 607 tps on 10
peers, similar to 588 tps on Xeon(edge). This, together with the re-
sults presented above, highlight the trade-oÔ¨Ä between networking
bandwidth and the number of orders in Fabric. In particular, it is
not always possible to have high bandwidth networking in the real
world, especially in cross-continent setups. For example, our mea-
surements on AWS cloud expose bandwidths of around 10 Mbps
across continents. Hence, running Fabric in a cross-continent setup
is challenging.

In contrast to Fabric, networking has a negligible impact on the
performance of Quorum, as shown in Figure 8. Similar to the anal-
ysis of Fabric, we use Xeon(cloud) and the tc Linux tool to limit the
bandwidth of the networking interface on each Xeon(cloud) peer
of the Quorum network. When using Raft consensus, we observe a
small (5%) drop in throughput only in a 100 Mbps networking link.
When using IBFT, the best performance is achieved in a 500 Mbps
link, while in a 10 Gbps network, the throughput of Quorum(IBFT)

Listing 1: Example of perf report output for Quorum(IBFT)
on Xeon(edge).

5.08%
4.07%
3.99%
3.95%
3.89%

geth [.] s e c p 2 5 6 k 1 _ f e _ s qr _ in ne r
geth [.] runtime . scanobject
geth [.] runtime . findObject
geth [.] s e c p 2 5 6 k 1 _ f e _ m ul _ in ne r
geth [.] github . com / ethereum / go - ethereum / vendor /

golang . org / x / crypto / sha3 . keccakF1600

2.52%
1.63%

geth [.] runtime . greyobject
geth [.] github . com / ethereum / go - ethereum / rlp .(*

encbuf ) . encodeString

geth [.] s e c p 2 5 6 k 1 _ g e _ s e t_ xo _ va r

1.55%
...

Listing 2: Example of perf report output for Fabric on
Xeon(edge).

15.06% peer [.] p256MulIntern al
8.82% peer [.] p256SqrIntern al
5.01% peer [.] runtime . mallocgc
2.62% peer [.] runtime . scanobject
2.56% peer [.] github . com / golang / protobuf / proto .

u n m a r s h a l U i n t 64 V al ue

2.49% peer [.] crypto / elliptic . p256Select
2.38% peer [.] runtime . adjustframe
2.25% peer [.] runtime . entersyscall
2.19% peer [.] crypto / elliptic . p 2 5 6 P o i n t A d d A f f i ne A sm

in the network need to execute and endorse a transaction. At the
other extreme, there is the OR policy which means that only one
peer needs to endorse a transaction. As such, it is expected that
OR policy should yield higher throughput due to higher transac-
tion execution parallelism. Indeed, the throughput of Fabric on 4,
6, 8, and 10 Xeon(edge) peers, respectively, is 1.4, 1.5, 1.5, and 2.3
times higher when using OR compared to AND policy, as shown in
Figure 9a. In addition, with more peers in the network, the through-
put continues to grow as opposed to dropping in the case of AND
policy. This can be explained by both the increasing parallelism in
the execution phase and the smaller message size in the ordering
phase which leads to lower networking overhead. Note that only
one endorsement needs to be forwarded to the ordering service
when using OR policy.

On the other hand, the improvement in throughput on TX2 when
using OR endorsement policy is between 1.5√ó higher on 4 peers
and 2.3√ó higher on 10 peers. This gap is because (i) the throughput
of Fabric with OR policy slowly increases due to higher parallelism,
and (ii) the throughput of Fabric with AND policy decreases due
to networking overhead. Hence, the gap becomes bigger.

4.4 Power Analysis
In this section, we analyze the power usage of the edge-based sys-
tems, namely Xeon(edge), TX2, and RP4, when running Fabric and
Quorum. First, we observe that Fabric is more power-hungry than
Quorum. For example, Xeon(edge) uses around 1.4√ó and 1.5√ó more
power to run Fabric compared to Quorum(Raft) and Quorum(IBFT),
respectively. TX2 uses up to 1.6√ó and 1.8√ó more power to run Fab-
ric compared to Quorum(Raft) and Quorum(IBFT), respectively. Fi-
nally, RP4 uses up to 1.2√ó more power to run Fabric compared

Figure 8: EÔ¨Äect of networking bandwidth on Quorum.

is 7% lower compared to the 500 Mbps network. However, these val-
ues are within the standard deviation of the measurements, thus,
we do not attribute the throughput drop to networking bandwidth.
Next, we answer the second part of Question 2, namely, why
does Xeon(edge) have a higher performance than Xeon(cloud) when
running Quorum? Concretely, Xeon(edge) exhibits up to 15% and
26% improvement over Xeon(cloud) when running Quorum(Raft)
and Quorum(IBFT), respectively. We attribute this diÔ¨Äerence to the
higher CPU performance of Xeon(edge) compared to Xeon(cloud).
Recall that, in Section 4.2, we show that Xeon(edge) is 22% faster
than Xeon(cloud) in ECSDA operations. Since the signing opera-
tions take the most time in Quorum‚Äôs execution compared to other
functions, as shown in Listing 1, Xeon(edge) has an advantage over
the other systems.

Fabric uses a diÔ¨Äerent curve for its ECDSA operations, namely
secp256r1 as opposed to the secp256k1 in Quorum. Our measure-
ments in Section 4.2 show that it is less optimized. In particular,
Figure 4 shows that the IPC of secp256k1 is higher compared to
secp256r1. Nonetheless, the ECDSA operations take the most time
in Fabric‚Äôs execution compared to other functions, as shown in List-
ing 2.

To answer Question 3, we take a closer look at the performance
of Fabric on TX2. As shown in Figure 2a, TX2 exhibits a through-
put that is 6 ‚àí 9√ó lower compared to Graviton. In contrast, TX2
exhibits a throughput that is only around 2√ó smaller compared to
Graviton when running Quorum. For Quorum, the reason is clear:
TX2‚Äôs CPU core is around 2√ó slower compared to Graviton‚Äôs CPU
core. But for Fabric, there is an interplay among all the system‚Äôs
components. As shown previously, Fabric‚Äôs execution is memory-
intensive and the memory of TX2 has almost 5√ó lower bandwidth
compared to the memory of Graviton. Hence, when using 4 and 6
peers, the lower performance of the CPU and memory of TX2 lead
to lower Fabric throughput. On 8 and 10 nodes, besides the CPU
and memory, there is the 1 Gbps networking link of TX2 that hin-
ders the throughput. This is similar to our analysis on networking
bandwidth for Xeon(edge) versus Xeon(cloud). We also note that a
similar analysis applies to RP4, just that this edge device has even
lower performance compared to TX2.

Lastly, we evaluate the impact of the endorsement policy on the
performance of Fabric. As mentioned before, we use an AND pol-
icy by default in our experiments. This means that all the peers

10

(a) Throughput

(b) Power

Figure 9: EÔ¨Äect of endorsement policy on Fabric.

Table 2: Performance-to-power Ratio of the edge systems.

Blockchain

Fabric
Quorum(Raft)
Quorum(IBFT)

Peers

Xeon(edge)
PPR
[tpj]
1.2
0.34
0.5

6
10
6

TX2

RP4

Peers

4
10
6

PPR
[tpj]
5.4
2.4
2.4

Peers

4
10
6

PPR
[tpj]
4.3
1.1
1.1

to both Quorum(Raft) and Quorum(IBFT). This is because Fabric
is more CPU- and memory-intensive compared to Quorum. Recall
that Fabric uses 1.3 CPU cores, on average, compared to 0.6 and 0.8
CPU cores used by Quorum(Raft) and Quorum (IBFT), respectively.
Moreover, it is well-known that the CPU power accounts for the
most signiÔ¨Åcant part of a system‚Äôs power [18, 35].

Second, Quorum(Raft) uses more power than Quorum(IBFT), as
shown in Figure 2h. In particular, Quorum(Raft) uses up to 13%,
13%, and 1% more power that Quorum(IBFT) on Xeon(edge), TX2,
and RP4, respectively. This is intriguing since Quorum(IBFT) has a
slightly higher CPU utilization compared to Quorum(Raft). But at
a closer look using Linux perf tool, we observe that the IPC of Quo-
rum(Raft) is higher compared to Quorum(IBFT). This suggests that
some of the cycles of Quorum(IBFT)‚Äôs execution are spent on wait-
ing for cache/memory operations and these cycles usually incur
less power compared to the execution of other instructions [41, 53].
Indeed, Quorum(IBFT) exhibits up to 30% more cache references
compared to Quorum(Raft), out of which 21% are cache misses.

For Fabric, there is a slight increase in power when AND en-
dorsement policy is used compared to OR policy, as shown in Fig-
ure 9b. SpeciÔ¨Åcally, there is up to 13% and 14% increase in power
when using AND policy on Xeon(edge) and TX2, respectively. But
this is expected because AND policy uses all the peers during the
endorsement phase, thus, incurring more power compared to OR.
Third, we compare the power eÔ¨Éciency across the three edge
systems under test. Power eÔ¨Éciency is reÔ¨Çected by the performance-
to-power ratio (PPR), computed as
ùëÉùëÉùëÖ = ùëá‚Ñéùëüùëúùë¢ùëî‚Ñéùëùùë¢ùë°

(1)

= ùëáùëüùëéùëõùë†ùëéùëêùë°ùëñùëúùëõùë†
ùê∏ùëõùëíùëüùëîùë¶

ùëÉùëúùë§ùëíùëü

and expressed in transactions per Joule (ppj). For this, we select
the number of peers that yield the best performance and present
the results in Table 2. We observe that TX2 exhibits the best PPR
across all systems and when running all three blockchains. Com-
pared to Xeon(edge), TX2 exhibits better PPR because it uses much
lower power, as shown in Figure 2g and Figure 2h. Compared to
RP4, TX2 exhibits better PPR because it yields higher performance
using just a bit more power. For example, TX2 exhibits 35% higher
throughput while using just 7% more power when running Fabric
on 4 peers compared to RP4.

These PPR results give rise to Question 4, namely, why is the
power eÔ¨Éciency of RP4 lower compared to TX2? The answer to
this question is indicated in our system analysis in Section 4.2. For
example, one CPU core of RP4 has between 16% and 30% lower per-
formance compared to one TX2 CPU core, depending on the bench-
mark considered. For example, one TX2 core exhibits 16% higher
performance while using 17% more energy compared to RP4 when
running CoreMark. When running cryptography operations, TX2
has even higher performance while using slightly higher power.
Hence, its PPR is higher compared to RP4. Part of the lower per-
formance can be attributed to the 26% lower clock frequency of
RP4. Another source of ineÔ¨Éciency is the memory hierarchy. For
example, the memory bandwidth of TX2 is 3√ó higher compared to
RP4.

4.5 Cost Analysis
In this section, we answer Question 5 by conducting a cost analy-
sis across both edge and cloud systems. This analysis is useful for
permissioned blockchain users that want to host their own nodes.
It helps the users in determining whether to host these nodes at the
edge or on the cloud, and on what type of system. On the cloud,
the utilization of an instance is usually billed every second, based
on a price that may vary according to the datacenter location. At
the edge, we use a cost model to estimate the total cost of own-
ership (TCO) which considers the Ô¨Åxed hardware cost, the contin-
uing energy cost, and the manpower cost for system administra-
tion. To this end, we adopt the cost model used in our previous
works [34, 36].

11

Table 3: Cost model parameters.

Notation

Description

ùëá
ùëÅ
ùê∂ùëù‚Ñé
ùê∂ùëö‚Ñé
ùê∂ùë†

ùëÉùëé

cluster lifetime
number of cluster nodes
cost of electricity per hour [USD]
cost of manpower per hour [USD]
cost of acquisition per node [USD]
Fabric
average power
Quorum(Raft)
per node
Quorum(IBFT)
[W]

Values
Xeon(edge) TX2 RP4
3 years (26,280 hours)
10 (6)
0.10
6.85 (5,000 per month)

5,000
101.6
73.4
65

800
5.1
4.1
3.2

100
4.2
2.4
3.7

Figure 10: Comparison of cost across all systems.

The edge TCO model adopted in this paper consists of the equip-
ment cost over a period of time, which is usually three years [36],
and the cost of electricity,

(2)

ùê∂ = ùëÅ ¬∑ ùê∂ùë† + ùëÅ ¬∑ ùëá ¬∑ ùëÉùëé ¬∑ ùê∂ùëù‚Ñé + ùëá ¬∑ ùê∂ùëö‚Ñé
where ùëÅ is the number of nodes, ùê∂ùë† is the cost of buying one node,
ùëá is the lifespan of the cluster in hours, ùëÉùëé is the average power
of one node in kilo-Watts (kW), ùê∂ùëù‚Ñé is the electricity cost per kilo-
Watt-hour (kWh), ùê∂ùëö‚Ñé is the cost of manpower per hour. The as-
sumption is that node utilization, hence, the average power is con-
stant throughout the lifespan of the cluster. In reality, the power
depends on the request rate, but here we suppose we have a high
enough request rate to keep the blockchain busy. To compare edge
nodes with cloud nodes, we divide the TCO by the number of nodes
and the lifespan in hours to get the average price per hour for a sin-
gle edge node.

The model parameters are summarized in Table 3. For a fair com-
parison with the cloud costs used in this paper, which represent
the AWS region hosted in Oregon, we use the price of electricity
in Oregon, which is $0.09 USD for commercial entities [11]. For
manpower cost, we use $5,000 per month, which is in the lower
range of the salary of an Amazon datacenter technician [27]. The
acquisition price of the nodes is based on our own records. The
number of nodes and the corresponding average power per node
depend on the blockchain framework. We select the number of
nodes that yield the best performance, which is 10 for Fabric and
Quorum(Raft), and 6 for Quorum (IBFT).

When computing the TCO, we consider two scenarios. First, we
assume that there needs to be a technician in charge of setting up

12

and providing maintenance for the hardware nodes. Second, we as-
sume that both the edge and the cloud need a system administrator
to set up and operate the nodes, hence, we do not include this man-
power cost in the TCO. Here, we assume that the cloud is used as
Infrastructure-as-a-service (IaaS). The price per hour per node for
Fabric is shown in Figure 10 for both these scenarios. Our key ob-
servation is that the edge is much cheaper than the cloud when
manpower cost is disregarded. For example, Xeon(edge), TX2, and
RP4 are respectively 1.5√ó, 10√ó, and 75√ó cheaper than Graviton.
On the other hand, adding manpower cost results in almost 2√ó
higher cost of the edge nodes compared to the cloud nodes. For ex-
ample, Xeon(edge) costs $0.884 per hour, while Xeon(cloud) costs
$0.476 per hour. These results show that the cost of continuous op-
eration due to energy has little impact in the long run. Instead, the
hardware and manpower costs have a high impact, and this may
lead the users to choose cloud computing over on-premise setups.

5 CONCLUSIONS
In this paper, we conducted an in-depth performance evaluation
of two widely-used permissioned blockchains on a diverse range
of hardware systems. The two selected blockchains, Hyperledger
Fabric and ConsenSys Quorum represent execute-order-validate
and order-execute transaction Ô¨Çows, respectively. In addition, we
analyze the performance of both Quorum with Raft and Quorum
with IBFT to represent both CFT and BFT application scenarios.
The hardware systems represent both the cloud and the edge, as
well as, both x86/64 and ARM architectures. For example, we use
cloud instances with Intel Xeon and ARM-based Graviton CPUs,
edge servers with Intel Xeon CPU, and edge devices such as Jetson
TX2 and Raspberry Pi 4.

Among the key observations, we show that ARM-based Gravi-
ton cloud instances achieve higher performance than Xeon-based
instances due to impressive CPU and memory performance. More-
over, these instances are around 35% cheaper than their Xeon coun-
terparts. On the other hand, edge devices based on ARM CPUs ex-
hibit much lower performance but higher power eÔ¨Éciency com-
pared to Xeon-based servers. Depending on the application sce-
nario, if high throughput is not required, then low-power edge
nodes can be used to run the blockchain. In the long run, hosting
blockchain nodes at the edge (on-premise) may lead to substantial
savings in cost, if the manpower cost is not included in the total
cost of ownership.

ACKNOWLEDGMENTS
This research is supported by the National Research Foundation,
Singapore under its Emerging Areas Research Projects (EARP) Fund-
ing Initiative. Any opinions, Ô¨Åndings and conclusions or recom-
mendations expressed in this material are those of the author(s)
and do not reÔ¨Çect the views of National Research Foundation, Sin-
gapore.

REFERENCES
[1] Amazon Quantum Ledger Database (QLDB), https://archive.fo/Mey43, 2019.
[2] Raspberry Pi 4 Tech Specs, https://archive.fo/zQFoN, 2019.
[3] Secp256k1, https://archive.fo/uBDlQ, 2019.
[4] Hyperledger-powered Internet of Things applications, https://archive.fo/jzkDl,

2020.

[5] PoET 1.0 SpeciÔ¨Åcation, http://archive.fo/5gbba, 2020.

[35] D. Loghin, Y. M. Teo, The Time and Energy EÔ¨Éciency of Modern Multicore

Systems, Parallel Computing, 86:1‚Äì13, 2019.

[36] D. Loghin, B. M. Tudor, H. Zhang, B. C. Ooi, Y. M. Teo, A Performance Study of

Big Data on Small Nodes, VLDB Endowment, 8(7):762‚Äì773, 2014.
Consensus

Istanbul

[37] H. Moniz,

BFT

The

Algorithm,

https://arxiv.org/abs/2002.03613, 2020.

[38] S. Nakamoto,

Bitcoin: A Peer-to-peer Electronic Cash System,

http://archive.fo/CIl1Y, 2008.

[39] D. Ongaro, J. Ousterhout, In Search of an Understandable Consensus Algorithm,

Proc. of USENIX Annual Technical Conference, pages 305‚Äì319, 2014.

[40] S. Pongnumkul, C. Siripanpornchana, S. Thajchayapong, Performance Analysis
of Private Blockchain Platforms in Varying Workloads, Proc. of 26th International
Conference on Computer Communication and Networks, pages 1‚Äì6, 2017.
[41] L. Ramapantulu, B. M. Tudor, D. Loghin, T. Vu, Y. M. Teo, Modeling the Energy
EÔ¨Éciency of Heterogeneous Clusters, Proc. of 43rd International Conference on
Parallel Processing, pages 321‚Äì330, 2014.

[42] P. Ruan, T. T. A. Dinh, D. Loghin, M. Zhang, G. Chen, Blockchains: Decentralized

and VeriÔ¨Åable Data Systems, Preprint, 2022.

[43] P. Ruan, T. T. A. Dinh, D. Loghin, M. Zhang, G. Chen, Q. Lin, B. C. Ooi,
Blockchains vs. Distributed Databases: Dichotomy and Fusion, Proc. of 2021
ACM International Conference on Management of Data, pages 1504‚Äì1517, 2021.

[44] P. Ruan, D. Loghin, Q.-T. Ta, M. Zhang, G. Chen, B. C. Ooi, A Transactional Per-
spective on Execute-order-validate Blockchains, Proc. of 2020 ACM International
Conference on Management of Data, page 543‚Äì557, 2020.

[45] S. Sankaran, S. Sanju, K. Achuthan, Towards Realistic Energy ProÔ¨Åling of
Blockchains for Securing Internet of Things, Proc. of 38th IEEE International
Conference on Distributed Computing Systems, pages 1454‚Äì1459, 2018.

[46] A. Sharma, F. M. Schuhknecht, D. Agrawal, J. Dittrich, Blurring the Lines be-
tween Blockchains and Database Systems: The Case of Hyperledger Fabric, Proc.
of 2019 International Conference on Management of Data, page 105‚Äì122, 2019.

[47] J. Shuja, A. Gani, K. Ko, K. So, S. Mustafa, S. A. Madani, M. K. Khan, SIMDOM: A
Framework for SIMD Instruction Translation and OÔ¨Ñoading in Heterogeneous
Mobile Architectures, Trans. Emerg. Telecommun. Technol., 29(4), 2018.

[48] J. Shuja, S. Mustafa, R. W. Ahmad, S. A. Madani, A. Gani, M. Khurram Khan,
Analysis of Vector Code OÔ¨Ñoading Framework in Heterogeneous Cloud and
Edge Architectures, IEEE Access, 5:24542‚Äì24554, 2017.

[49] J. Simon, New M5n and R5n EC2 Instances, with up to 100 Gbps Networking,

https://archive.fo/JyBV8, 2019.

[50] C. Staelin, L. McVoy,

lmbench - system benchmarks, https://archive.fo/SYkvt,

2018.

[51] K. Suankaewmanee, D. T. Hoang, D. Niyato, S. Sawadsitang, P. Wang, Z. Han,
Performance Analysis and Application of Mobile Blockchain, Proc. of Interna-
tional Conference on Computing, Networking and Communications, pages 642‚Äì
646, 2018.

[52] P. Thakkar, S. Nathan, B. Viswanathan, Performance Benchmarking and Opti-
mizing Hyperledger Fabric Blockchain Platform, Proc. of 26th IEEE International
Symposium on Modeling, Analysis, and Simulation of Computer and Telecommu-
nication Systems (MASCOTS), pages 264‚Äì276, 2018.

[53] B. M. Tudor, Y. M. Teo, On Understanding the Energy Consumption of ARM-
Based Multicore Servers, Proc. of ACM SIGMETRICS/International Conference on
Measurement and Modeling of Computer Systems, page 267‚Äì278, 2013.

[54] X. Xu, G. Sun, L. Luo, H. Cao, H. Yu, A. V. Vasilakos, Latency Performance
Modeling and Analysis for Hyperledger Fabric Blockchain Network, Information
Processing and Management, 58(1):102436, 2021.

[55] M. Yin, D. Malkhi, M. K. Reiter, G. G. Gueta, I. Abraham, HotStuÔ¨Ä: BFT Consen-
sus with Linearity and Responsiveness, Proc. of ACM Symposium on Principles
of Distributed Computing, page 347‚Äì356, 2019.

[6] Raspberry Pi OS (64 bit) Beta Test Version, https://archive.fo/1B0Te, 2020.
[7] Amazon EC2 m6g Instances, https://archive.fo/MmHX0, 2021.
[8] GoQuorum, https://github.com/ConsenSys/quorum, 2021.
[9] Hyperledger Fabric, https://github.com/hyperledger/fabric, 2021.
[10] Intel Haswell, https://archive.fo/ZTEge, 2021.
[11] Oregon State Energy ProÔ¨Åle, https://archive.fo/Jqgxu, 2021.
[12] Proof-of-stake (POS), https://archive.fo/d2VgG, 2021.
[13] Chainstack - Managed Blockchain Infrastructure, https://chainstack.com/, 2022.
[14] Helium - People-Powered Networks, https://www.helium.com/, 2022.
[15] O. Ala-Peijari, Bitcoin The Virtual Currency: Energy EÔ¨Écient Mining of Bitcoins,

Master‚Äôs Thesis, 2014.

[16] Amazon, AWS Graviton Processor, https://archive.fo/lis8L, 2021.
[17] ARM,

ARM Announces Support For EEMBC CoreMark Benchmark,

http://www.webcitation.org/6RPwNECop, 2009.

[18] L. A. Barroso, J. Clidaras, U. Hoelzle, The Datacenter as a Computer: An Intro-
duction to the Design of Warehouse-Scale Machines, Second Edition, Morgan and
Claypool Publishers, 1st edition, 2013.

[19] T. Blummer, An Introduction to Hyperledger, http://archive.fo/oyiKf, 2018.
[20] V. Buterin, A Next-Generation Smart Contract and Decentralized Application

Platform, http://archive.fo/Sb4qa, 2013.

[21] M. Castro, B. Liskov, Practical Byzantine Fault Tolerance, Proc. of 3rd Symposium

on Operating Systems Design and Implementation, pages 173‚Äì186, 1999.

[22] H. Dang, T. T. A. Dinh, D. Loghin, E.-C. Chang, Q. Lin, B. C. Ooi, Towards
Scaling Blockchain Systems via Sharding, Proc. of 2019 International Conference
on Management of Data, SIGMOD ‚Äô19, page 123‚Äì140, 2019.

[23] J. Dean, S. Ghemawat,

LevelDB: A Fast Persistent Key-Value Store ,

https://archive.fo/UGcvE, 2011.

[24] T. T. A. Dinh, R. Liu, M. Zhang, G. Chen, B. C. Ooi, J. Wang, Untangling
Blockchain: A Data Processing View of Blockchain Systems, IEEE Transactions
on Knowledge and Data Engineering, 30(7):1366‚Äì1385, 2018.

[25] T. T. A. Dinh, J. Wang, G. Chen, R. Liu, B. C. Ooi, K.-L. Tan, BLOCKBENCH: A
Framework for Analyzing Private Blockchains, Proc. of 2017 ACM International
Conference on Management of Data, pages 1085‚Äì1100, 2017.

[26] D. Franklin, NVIDIA Jetson TX2 Delivers Twice the Intelligence to the Edge,

https://archive.fo/aoQCU, 2017.

[27] Glassdoor,

Amazon Data Center

Technician Monthly

Pay,

https://archive.fo/LWCjW, 2021.

[28] A. Goranoviƒá, M. Meisel, S. Wilker, T. Sauter, Hyperledger Fabric Smart Grid
Communication Testbed on Raspberry PI ARM Architecture, Proc. of 15th IEEE
International Workshop on Factory Communication Systems (WFCS), pages 1‚Äì4,
2019.

[29] S. Gupta, S. Rahnama, J. Hellings, M. Sadoghi, ResilientDB: Global Scale Resilient

Blockchain Fabric, Proc. VLDB Endow., 13(6):868‚Äì883, 2020.

[30] S. Han, Z. Xu, L. Chen, Jupiter: A Blockchain Platform for Mobile Devices, Proc.
of 34th IEEE International Conference on Data Engineering (ICDE), pages 1649‚Äì
1652, 2018.

[31] IETF, Elliptic Curve Cryptography (ECC) Cipher Suites for Transport Layer
Security (TLS) Versions 1.2 and Earlier, https://archive.fo/4wOUE, 2018.
[32] L. Lamport, R. Shostak, M. Pease, The Byzantine Generals Problem, ACM Trans.

Program. Lang. Syst., 4(3):382‚Äì401, 1982.

[33] D. Loghin, G. Chen, T. T. A. Dinh, B. C. Ooi, Y. M. Teo,

Blockchain
Low-Power Nodes,

Goes Green? An Analysis
https://arxiv.org/abs/1905.06520, 2019.

of Blockchain

on

[34] D. Loghin, L. Ramapantulu, Y. M. Teo, On Understanding Time, Energy and Cost
Performance of Wimpy Heterogeneous Systems for Edge Computing, Proc. of
IEEE International Conference on Edge Computing (EDGE), pages 1‚Äì8, 2017.

13

