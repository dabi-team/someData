Statistical Detection of Adversarial examples 
in Blockchain-based Federated Forest In-
vehicle Network Intrusion Detection 
Systems 

Ibrahim Aliyua, Sélinde van Engelenburgb, Muhammed Bashir Mu’azuc, Jinsul Kimd*, Chang 
Gyoon Lima*  

aDepartment of Computer Engineering, Chonnam National University, Yeosu, 50 Daehakro, Jeonnam 59626, South Korea 
bFaculty of Technology, Policy and Management, Delft University of Technology, the Netherlands 
cDepartment of Computer Engineering, Ahmadu Bello University, Zaria, Nigeria 
dDepartment of ICT Convergence System Engineering, Chonnam National University, Gwangju,, Korea 

*(Corresponding Author: Email: cglim@jnu.ac.kr, jsworld@jnu.ac.kr)  

Abstract 

The  internet-of-Vehicle  (IoV)  can  facilitate  seamless  connectivity  between  connected  vehicles  (CV),  autonomous  vehicles 
(AV), and other IoV entities. Intrusion Detection Systems (IDSs) for IoV networks can rely on machine learning (ML) to protect 
the in-vehicle network from cyber-attacks. Blockchain-based Federated Forests (BFFs) could be used to train ML models based 
on data from IoV entities while protecting the confidentiality of the data and reducing the risks of tampering with the data. 
However,  ML models  created this way are still  vulnerable to evasion, poisoning, and exploratory  attacks  using  adversarial 
examples.  This  paper  investigates  the  impact  of  various  possible  adversarial  examples  on  the  BFF-IDS.  We  proposed 
integrating a statistical detector to detect and extract unknown adversarial samples. By including the unknown detected samples 
into the dataset of the detector, we augment the BFF-IDS with an additional model to detect original known attacks and the 
new adversarial inputs. The statistical adversarial detector confidently detected adversarial examples at the sample size of 50 
and  100  input  samples.  Furthermore,  the  augmented  BFF-IDS  (BFF-IDS(AUG))  successfully  mitigates  the  adversarial 
examples with more than 96% accuracy. With this approach, the model will continue to be augmented in a sandbox whenever 
an adversarial sample is detected and subsequently adopt the BFF-IDS(AUG) as the active security model. Consequently, the 
proposed  integration  of  the  statistical  adversarial  detector  and  the  subsequent  augmentation  of  the  BFF-IDS  with  detected 
adversarial samples provides a sustainable security framework against adversarial examples and other unknown attacks. 

KEYWORDS: 
Adversarial examples, Artificial Intelligent (AI), Blockchain, Controller Area Network (CAN), Federated Learning, Intrusion 
detection system (IDS)

1. Introduction 

is  envisaged 

 With  the  tremendous  development  of  network 
communication technology such as 5G, transforming 
the modern transportation system into the concept of 
the Internet-of-vehicle (IoV) is becoming a reality [1]. 
to  provide  a  seamless 
The  IoV 
communication 
communication 
between  smart  vehicles,  such  as  connected  vehicles 
(CV)  and  autonomous  vehicles  (AV),  and  other  IoV 
entities 
road 
infrastructure/devices  [1,  2].  IoV  mainly  consists  of 
intra-vehicle Networks (IVNs), which consist of an In-
vehicle  communication  system  managed  by  a 

pedestrians 

framework 

such 

and 

for 

as 

controller  area  network  (CAN)  and  an  external 
the 
vehicular  network 
interaction  of  vehicles  to  outer  environments  by 
vehicle-to-everything (V2X) technology [2].  

is  concerned  with 

that 

The  CAN  manages  the  interaction  of  Electronic 
Control Units (ECU), which facilitates internal vehicle 
components  such  as  engine,  brake  and  telemetric 
systems through information exchange [3]. However, 
the CAN was designed with no security mechanism to 
deal  with  malicious  communication  broadcast  in  the 
CAN bus. This has left it vulnerable to attack through 
the  vehicle's  On-Board  Diagnostic  II  (OBD-II)  port, 
firmware (such as media player) or remotely through 
telematics [4, 5]. Machine Learning (ML) for a CAN 

1 

 
 
 
 
 
Intrusion  Detection  System  (IDS)  is  a  promising 
solution to protect the CAN due to its ability to learn 
non-linear  attack  patterns.  Tremendous  progress  has 
been made in adopting machine learning for CAN IDS 
[6-9]. Traditionally, the models are trained locally for 
a single vehicle due to limited support by automakers 
and car owners to share sensitive data. Therefore, the 
models  are  denied  the  benefit  of  access  to  rich  data 
available  in  the  vehicle  ecosystem.  The  ability  to 
create models based on data from various vehicles in 
an ecosystem allows for taking into account a higher 
number and variety of  attacks,  potentially  improving 
accuracy. 

(SDN)-enabled 

A  Blockchain-based  Federated  Forest  Software 
Defined  Networking 
Intrusion 
Detection  System  (BFF-IDS)  can  be  used  to  support 
ML  that  utilizes  the  data  available  in  the  whole 
ecosystem  of  vehicles  while  at  the  same  time 
protecting  sensitive  data  [10].  In  BFF-IDS,  each 
participating vehicle (miner) trains a partial model and 
stores it at an InterPlanetary File System (IPFS). They 
exchange  the  hashes  of  these  partial  models  via 
blockchain. The hashes are unique to their models and 
the 
difficult 
blockchain. Thus, storing the hashes on the blockchain 
provides a 'proof of existence' of the partial model [11]. 
This  makes  tampering  with  the  models  after  storing 
their  hashes  detectable  and  thus  provides  partial 
protection against poisoning attacks. 

to  change  once 

incorporated 

into 

Next,  the  final  federated  model  is  obtained  by 
aggregating the partial models at the user end. Tests in 
a  sandbox  environment  show  that  BFF-IDS  has  an 
accuracy  of  98.10%  in  detecting  fuzzy  attacks, 
impersonation  attacks,  DoS  attacks  and  attack-free 
traffic. This is considerably higher than other solutions, 
such as [12-15], which have an accuracy of 92.80%, 
97.00%, 97.00% and 98.0%, respectively. 

Adversarial ML is a machine learning technique that 
studies the vulnerabilities of fooling ML models in the 
face  of  deceptive  input  [16].  The  deceptive  inputs 
known  as  adversarial  examples  can  be  created  by 
adding imperceptible perturbation to the actual inputs 
[17]. Adversaries can target the model during training 
or testing time. 

instead  of 

raw  data  contributes 

Although the blockchain reduces the risk of model 
poisoning after its hash was stored and sharing partial 
models 
to 
confidentiality,  ML  models  are  often  susceptible  to 
adversarial attacks in which inputs are manipulated to 
cause  the  model  to  misclassify  [5,  18].  In  addition, 
sophisticated  novel  attacks  are  constantly  being 
developed  [19].  Providing  a  sustainable  way  of 
keeping  up  with  these  attacks  is  critical  for  the 
system's survival. 

The  adversarial  examples  pose  a 

tremendous 
security  threat  to  the  adoption  of  ML  as  IDS.  In 
particular, the adversarial example transferability has 

2 

shown that adversarial examples designed to cause one 
model, M1, to misclassify often cause another model, 
M2,  to  misclassify.  Thus,  this  makes  it  possible  to 
generate adversarial examples in one ML machine and 
attack  another  ML  system  without  knowledge  or 
access  to  the  underlying  model  [18,  20,  21].    Most 
current studies focus on providing solutions in image 
datasets,  such  as  generating  adversarial  examples 
based on the MNIST dataset and attacking traffic signs 
in  a  CV/  AV.  Still,  there  has  recently  been  keen 
interest in the impact of adversarial examples in other 
systems, such as security systems [22].  For instance, 
the impact of adversarial examples against CAN IDS 
build  using  Long  Short  Term  Memory  (LSTM)  was 
investigated [22]. Fast Gradient Sign Method (FGSM) 
and  Basic  Iterative  Method  (BIM)  attacks  were 
explored,  and  an  Adversarial  Attack  Defending 
System (AADS) was proposed to counter the attacks  

is 

With  the  use  of  blockchain,  the  BFF-IDS  offers 
defence  against  poisoning  but  has  no  measure  for 
evasion  attacks  which 
the  most  common 
attack/threat  faced  by  ML  models  [23].  Besides,  the 
effectiveness  of  transferability  in  CAN  IDS  has 
primarily  remained  untested  as  most  studies  focused 
on  image  classification  problems  [23].  In  practice, 
more knowledge is needed on its vulnerability against 
attacks using adversarial examples and how to protect 
against such attacks to enable deployment of the BFF-
IDS on a large scale in practice. Therefore, we set out 
to  obtain  the  following  objectives  following  the 
approach of [16] on traditional ML closely: 

  Determine  the  vulnerability  of  BFF-IDS  to 

adversarial examples. 

  Integrate a statistical adversarial detector for the 
detection  of  unknown  adversarial  examples  and 
augment  the  BFF-IDS  to  detect  the  unknown 
adversarial examples. 

  Investigate the robustness of the solutions of the 

preceding objective. 

transferability 

This study is an extension of the BFF-IDS where we 
the  resilience  of  BFF-IDS  against 
investigated 
unknown  evasion  attacks  using  the  principle  of 
adversarial  example 
to  generate 
adversarial  samples.  We  explore  the  integration  of  a 
statistical (adversarial) detector to check for unknown 
adversarial  attacks.  However,  statistical  attacks  can 
only  detect  adversarial  examples  in  large  batch 
samples [16]. To protect against single-input attacks, 
detected  attacks  are  extracted  into  a  sandbox  and 
added to the dataset as an adversarial class to augment 
the model by training a new BFF-IDS (AUG) model. 
This  way,  the  proposed  system  could  be  made 
sustainable 
in  detecting  unforeseen  attacks.  In 
particular, the main contributions of this study are thus 
summarized as follows: 

  We 

investigated 

impact  of  adversarial 
examples  against  BFF-IDS  using  various 

the 

 
 
 
algorithms  based  on  the  adversarial  sample 
transferability.  Our  experimental  results  suggest 
that  BFF-IDS  is  very  vulnerable  to  adversarial 
examples  attack  as  it  succeeded  in  significantly 
reducing the confidence (accuracy) of our model 
from more  than  97%  to  as  low  as  20%  in  some 
instances. 

to 

detect 

detector 

  We  investigated  the  integration  of  a  statistical 
adversarial 
unknown 
adversarial  examples.  The  integration  of  the 
detector effectively detected adversarial samples 
from benign distributions in large batch samples. 
However, the statistical test cannot identify which 
samples are adversarial. 

  To  address  the  limitation  of  the  adversarial 
detector,  which  can  only  detect  adversarial 
samples  from  a  group  (batch)  of  samples,  we 
augmented  the  BFF-IDS  by  adding  the  detected 
samples  to  the  dataset  and  adding  a  new  class, 
"adversarial  class",  to  the  output  of  the  model. 
The  retrain  model  significantly  improves  the 
model's  confidence  as  attacks  can  now  be 
detected on single input- bases. 

  We demonstrated the robustness of the statistical 
test  by  considering  the  mixture  of  adversarial 
samples  from  various  algorithms  and  benign 
samples. Also, we investigated the robustness of 
the BFF-IDS augmentation by investigating how 
the mixture of several combinations of adversarial 
examples from the different algorithms as training 
samples can affect the detection of the adversarial 
samples and the general performance of the model. 
Useful results and conclusions were derived. 

and 

learning, 

statistical 

The rest of the paper is organized as follows: Section 
2 presents the study's background, including federated 
sample 
forest,  Adversarial  Machine 
The 
transferability, 
methodology is given in section 3. Section 4 presents 
the  experimental  results  on  adversarial  attacks  on 
BFF-IDS, statistical detection of adversarial examples, 
BFF-IDS  augmentation,  and  the  proposed  solutions' 
robustness.  The  discussion  of  results  is  offered  in 
section 5, while section 6 concludes the paper. 

testing. 

2. Background 

2.1. Related work 

IDS for CAN has been proposed to offer protection 
to  CAN  using  various  ML  models.  For  instance,  a 
novel graph-based Gaussian naive Bayes (GGNB) is 
proposed  for  the  intrusion  detection  algorithm.  The 
GGNB  leverages  graph  properties  to  detect  CAN-
monitoring attacks without protocol modification [24]. 
Although the method recorded promising results, the 
authors  did  not  investigate  the  model's  impact  on 

3 

adversarial  example  attacks.  Meanwhile,  a  complex 
value neural network (CVNN) has been  proposed  to 
protect the CAN network to detect the arbitration field 
[25].  Encoders  were  employed  to  extract  valuable 
features for better generalization. However, this study 
did not consider adversarial examples and data sharing 
problem 

With  the  innovation  of  CAVs,  CAN  and  FlexRay 
are  being  replaced  with  Automotive  Ethernet  to 
support high-definition applications demand for high 
throughput  etc.  Jeong,  et  al.  [26]  offered  the  first 
intrusion  detection  method  in  this  domain  to  detect 
audio-video 
(AVTP)  stream 
injection  attacks  using 
feature  generation  and 
convolutional  neural  network  (CNN).  Although  the 
method proves the  approach  is  suitable for  real-time 
detection,  adversarial  examples  impact  and  possible 
countermeasure were not considered. 

transport  protocol 

Although previous studies on adversarial examples 
focused on the image domain, considering the impact 
of  adversarial  examples  on  IDS  is  gaining  traction 
[22]. For instance,  Yang et al. [23] demonstrate the 
vulnerability  of  deep  neural  networks  for  NIDS  to 
adversarial  examples  using  model  substitution  and 
black-box-based  zeroth-order  optimization  (ZOO) 
and generative adversarial network (GAN) attacks. In 
another  study,  a  testbed  consisting  of  an  adversarial 
model embedded in the IDS was designed to facilitate 
security  evaluation  of  CAN  system  design  [27]. 
However, the developed tool provides no function for 
testing mitigation measures. 

against 

provide 

countermeasures 

Furthermore,  adversarial  examples  have  been 
investigated  in  connected  and  autonomous  vehicles 
(CAV)  [28].  Although  valuable  results  have  been 
observed  on  the  impact  of  adversarial  examples  on 
several ML and deep learning models, the paper failed 
threats. 
to 
Additionally,  adversarial  examples  in  spam  filters, 
biometric  authentication  and  fraud  detection  have 
been  studied  in  [29-33].  A  recent  study  investigated 
the impact of adversarial examples against CAN IDS 
build using Long Short Term Memory (LSTM) [22]. 
Fast  Gradient  Sign  Method  (FGSM)  and  Basic 
Iterative Method (BIM) attacks were explored and had 
a  success  rate  of  about  98%. An  Adversarial  Attack 
Defending System (AADS) was developed to counter 
the  attacks  by  retraining  the  LSTM  model  with  the 
attack samples as part of the training data.  

As illustrated in Table 1, most existing studies are 
focused  on  traditional  training  methods.  Besides, 
adversarial examples' impact on the  proposed IDS  is 
mainly  unexplored.  Therefore,  this  paper  focuses  on 
adversarial  examples'  impact  and  countermeasure on 
BFF-IDS. 

 
 
 
 
 
Table 1 

Related works 

Year/Ref. 

IDS 
Domain 

2021, [22] 

CAN 

Automotive 
Ethernet 

2021, [26] 

2021, [24] 

2021, [25] 
2021, [10] 

2019,[27] 

LSTM 

CNN 

CAN 

CAN 
CAN 

CAN 

Graph-based Gaussian 
naive Bayes (GGNB) 
CVNN 
BFF-IDS 
K-Nearest Neighbor (k-
NN) 

IDS Model 

BC 

FL 

SDN 







 








 








 


Adv. Ex. 
impact 
 (intra-
technique) 







Adv. Ex. 
mitigation 

 








k-NN, Random                                                                                           

2019, [28] 

CAV 

forest (RF), Logistic 
regression  
(LR),  Long Short-Term 
Memory (LSTM) 

2018, [23] 

NIDS 

Deep Neural Network 













Our 
framework 

CAN 

BFF-IDS 

 

 

 

*Adv Ex: adversary examples, BC: Blockchain, FL: Federated Learning 

 

 (intra-
technique) 
 (cross-
technique) 





 

2.2. Federated Forest 

Federated learning (FL) is an innovative concept in 
which models are built on data sets distributed across 
multiple  devices  while  preventing  data  leakage  [34].  
Conventionally,  a  model,
,  is  trained  by 

 ℳ(cid:3)(cid:4)(cid:5)
, who wish to build a stronger model by  

(Ɗ(cid:8) … Ɗ(cid:10))

Ɗ = Ɗ(cid:8) ∪ … ∪ Ɗ(cid:10)

(cid:6)ℕ(cid:8) … ℕ(cid:10)(cid:11)
,  from  data  owners, 
combining  data, 
consolidating their data, 
. Unlike the 
traditional  learning  (TL)  method,  FL  enables  the 
,  in  such  a 
collaborative  training  of  the  model, 
ℳ(cid:17)(cid:18)
 from  any 
way  that  the  confidentiality  of  the  data 
owner 
the 
,  is close to the 
performance, 
performance, 
.  Formally, 
given 

 ℳ(cid:3)(cid:4)(cid:5)
 as a non-negative real number, if 

is  preserved  while  ensuring 

,  of  the  model,

, of the model, 

(cid:19)(cid:17)(cid:18)
(cid:19)(cid:3)(cid:4)(cid:5)

ℳ(cid:17)(cid:18)

Ɗ(cid:10)

ℕ(cid:10)

(cid:20)

the federated model is said to have 

-accuracy loss. 

|(cid:19)(cid:17)(cid:18) − (cid:19)(cid:3)(cid:4)(cid:5)| < (cid:20)                                (1)

(cid:20)

Based  on  the  data  distribution  among  owners 
(subsets),  FL  can  be  categorized  into  horizontal  FL, 
vertical  FL  and  federated  transfer  learning  [34].  For 
horizontal FL, the data set in each subset have the same 
feature space but a different number of samples. The 
datasets have the same number of samples in vertical 
FL but different feature spaces. A scenario where both 
the  sample  and  feature  space  differ  is  designated  as 
federated transfer learning.  

(cid:26)

ℕ(cid:8) …  ℕ(cid:27)

   distributed  among 

 federating  units 
 are  assumed  to  be  disjoint  such 

dataset 
Ɗ
(owners), 
that  only  a  subset  of  the  data 
(cid:29)(cid:27)
samples  are  used  by 
.  The 
goal is to build an accurate federated forest model such 
that: (1) a partial random forest model is trained and 
held by each owner (known as a miner), 

(cid:30)(cid:31)  unit,  where 

(cid:26) ∈ [1, (cid:26)]

Ɗ(cid:27) ⊆ Ɗ

   with 

(cid:26)

ℳ(cid:10), 1 ≤ (cid:25) ≤
, is aggregated at each user 

;  (2) the FL model, 
(cid:26)
end while minimizing  

-accuracy loss. 

ℳ(cid:17)(cid:18)
(cid:20)
2.3. Adversarial Machine learning and samples 
transferability 

Formally, let's assume the ML model, 

classified  benign  sample  S,  i.e.   
adversarial example, 
is perceptually indistinguishable from 
model to misclassify. i.e.  

, correctly 
.  An 
, can be constructed such that it 
but causes the 
.  

%
%(&) = ’(cid:30)()*

+

& 
%(+) ≠ ’(cid:30)()*

The adversarial example is crafted by adding a small 
is 
,  to  the  benign  sample 
perturbation, 
computed  by  the  approximation  of  the  following 
optimization problem iteratively until it gets classified 
by the by ML classifier:  

.  The   

- 

-

&

where

+ = &  + -/                                             (2) 

 -/ =  123 min7 %(& + -) ≠ %(&)  

Adversarial  examples  transferability  refers  to  the 
potential  of  adversarial  examples  generated  and  
design for model 
to also cause the misclassification 
in 

8 without access to the underlying model [18].  

% 

For  this  study,  horizontal  FL  is  utilized  to  build 
, for intrusion detection.  Given the 

federated forest, 
(cid:25)

4 

%

 
 
 
  
  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Considering  crafting  the  adversarial  examples  by 
solving  the  optimization  problem,  we  can  formalize 
the adversarial sample transferability notion as:  

8

8

8

) = |(cid:6)%
(& + -/) ∶ & ∈ <(cid:11)|   (3
 is  the  expected  task  input  distribution 

(&) ≠ %

)  

Ω:(%, %
where  set 
solved by models 

<

%

%

 and 

8.  
The  adversarial  sample  transferability  can  be 
categorized  into  intra-technique  and  cross-technique 
[20].  In  intra-technique  transferability,  the  cross 
models  are  trained  with  the  same  ML  technique  but 
different  parameter  initializations  or  datasets-  For 
instance,  both 
8 are  neural  networks  or 
decision  trees.  The  cross-technique  transferability 
deals  with  a  situation  where  the  models  are  trained 
 is a neural network 
using different techniques-e.g., 
and 
8 is  a  decision  tree.  In  this  study  we  focus  on 
cross-technique transferability as it represents the real-
life scenario of how attacks are conducted. 

 and 

%

%

%

%

2.4. Statistical Hypothesis Testing  

Two-sample  hypothetical  testing  is  introduced  to 
conduct  a  test  on  two  randomly  selected  samples  to 
the  statistically  significant  difference 
determine 
between the two samples, in other words, whether the 
samples originated from the same distribution. Credit 
for the statistical hypothesis and analysis goes to [16].  

K

@

&~?

 and 

&(cid:8)~?

? = B

&(cid:8) = C

&A = D

Formally,  let 

.  The  statistical  test 

;  the  null  hypothesis 

,  notation  be  considered  as  a 
 .  A  statistical  test 
 and 

sample  drawn  from  distribution 
can  be  formalized  as  follows:  Given 
where 
holds  that 

&A~B
EF
I
×
 takes in the sample's input and returns the 
&
p-value, which matches the significant level,
. The p-
 O
value gives the probability of obtaining the observed 
  relates to 
outcome or a more extreme one, while the 
the confidence of the test set as the threshold. In this 
study, we set the threshold at 0.05. Therefore, the 
is rejected if the p-value is less than the threshold. 

G(&(cid:8), &A): &

→ (cid:6)0. 1(cid:11)

EF

O

Several  two-sample  tests  have  been  proposed,  but 
we  adopt  kernel-based,  which  measures 
the 
probability of the distance between the two samples as 
a  biased  estimator  of  the  true  Maximum  Mean 
Discrepancy  (MMD).  We  also  compared  the  MMD 
with energy distance (ED) [16]. 

2.5 CAN Bus Dataset 

The  CAN-intrusion  dataset  (OTIDS)  used  in  this 
study  was  obtained 
the  Hacking  and 
from 
Countermeasure  Research  Lab  at  Korea  University 
[28].  The  dataset  was  obtained  from  real  attack 
scenarios and consists of four classes of traffic: fuzzy 
attack, DoS attack, Impersonation attack, and attack-
free  state.  The  datasets  were  created  from  a  real 
vehicle (KIA SOUL) by logging onto CAN traffic via 

the OBD-II port. These attack types have devastating 
consequences  on  the  CAN-  the  fuzzy  attack  can 
override  normal  function;  a  DoS  attack  can  deny 
access  to  a  legitimate  node,  while  an  impersonating 
attack can cause the vehicle to manifest an unintended 
state or action. 

3. Methodology 

3.1. BFF-IDS for IoV security 

is 

increasingly 

With the advent of smart cities and IoV, the network 
architecture 
faced  with  high-
performance  demand  regarding  latency,  scalability, 
network  bandwidth  usage,  data  privacy  and  security 
[35].  The  problem  is  exacerbated  by  the  diverse 
technologies  and  a  high  degree  of  interdependence 
between  various  system  components  in  the  network 
ecosystem  [36].  This  study  proposed  a  hybrid 
architecture  that  will  guarantee  network  scalability 
and  privacy  using  blockchain  and  SDN.  This 
architecture was initially proposed by Kim, et al. [35], 
but their system is expensive in terms of gas/ether (as 
in  Ethereum)  as  model  training  parameters  are 
exchanged through the blockchain.  

The hybrid architecture for the IDS consists of three 
planes-the  data  plane  consisting  of  the  individual 
vehicle  CAN;  the  control  plane,  which  manages  the 
interaction  of  the  CANs  through  blockchain  and  the 
control  plane,  which  entails  system  management 
authorities(stakeholders).  Each  vehicle  (known  as  a 
user  node)  builts  a  partial  model  using  its  data  and 
upload the model to IPFS  while  the location hash is 
exchanged 
the 
approach  of  [37],  only  the  cost  of  some  bytes  is 
incurred  in  the  process.  The  blockchain  network 
consists  of  miner  nodes  responsible  for  creating 
blocks and verifying proof-of-Authority. The vehicles 
are  SDN  enabled  to  aid  ease  deployment  costs  with 
high  agility  and  security.  The  stakeholders  include 
network  management,  ID  providers  and 
threat 
intelligence  agencies  that  further  investigate  attack 
trends and other security policies.  

the  blockchain-  unlike 

through 

system 

To  better  expose 

requirement 
the 
considering  the  complex  interaction  of  the  system 
component-SDN,  Ethereum  Blockchain,  IPFS  and 
machine learning libraries- object transformation and 
event  effects  on  the  system's  behaviour  must  be 
examined  to  build  a  testbed.  We  adopt  structured 
analysis  to  present  the  requirements  modeling  in 
which data and transformation processes are treated as 
separate  entities  [38].  Behavioral  modeling 
is 
considered  here  as  it  effectively  exposes  the  testbed 
design's  requirements  for  the  structured  requirement 
analysis. 

The  dynamic  behaviour  modeling  of  the  testbed 
during operation is accomplished by representing the 
various testbed components processes as a function of  

5 

 
 
 
 
 
 
Fig. 1. Testbed sequence diagram 

events and time. It exposed the details of the testbed 
response to external events (i.e., system components). 
We utilized a sequence diagram (SD) to illustrate how 
events caused transitions between components. Fig. 1 
presents a sequence diagram for the testbed operation, 
illustrating  events  and  corresponding 
transitions 
between  components.  The  arrows  depict  the  event-
driven transition/behaviour between components. The 
time  of  event  occurrence  is  measured  vertically 
downward  along  each  component.  The  vertical 
rectangles  along  the  components  represent  the  time 
spent  processing  and  the  activity.  Creating  network 
topology,  partial  model  training,  and  aggregation 
takes more time in each corresponding component of 
the testbed. At the end of the model aggregation, the 
session of FL miner and users are terminated by the 
Mininet emulator. More details on the testbed in [10, 
39]. 

3.2. Threat Model 

including the injection and feature extraction  stages. 
We  assume  the  adversary  launched  the  attack  to 
corrupt  the  BFF-IDS  based  on  its  knowledge  of  the 
feature extraction and access to source traffic. There 
are three possible attack scenarios at the attack surface: 
evasion, poisoning,  and  exploratory attack [40]. The 
Evasion  attack  is  possible  during  the  testing  phase, 
whereby the adversary manipulates test data to corrupt 
the model. The poisoning attack occurred during the 
training  phase, 
is 
contaminated  to  compromise  the  whole  learning 
process,  while  exploratory  uses 
the  black-box 
approach to learn about the underlying model and the 
training data pattern. In this study, we investigate the 
evasion  attack  on  the  BFF-IDS  by  manipulating  the 
extracted feature (see Fig. 2 for the illustration of the 
threat model). 

training  data 

in  which 

the 

the 

strength, 

This  section  identifies  the  fundamental  security 
objectives, threats, and vulnerabilities of the BFF-IDS, 
goals, 
adversary's 
considering 
knowledge, and capabilities. Firstly, we highlight the 
attack  surface  in  the  BFF-IDS  through  which  the 
adversary may attempt to launch an attack to subvert 
the system. We then decompose the threat model into 
four  aspects:  adversarial  knowledge,  capabilities, 
specificity and goals. 

The Attack Surface. This study defined the attack 
surface  concerning  the  data  processing  pipeline, 

6 

Fig. 2. Threat Model 

Adversarial  Knowledge.  Adversarial  knowledge 
can be divided into three categories: white box, black 
box  and  grey  box  [17].  In  a  white-box  attack,  the 
adversary is assumed to have complete knowledge of 
the underlying model and the dataset. In the block-box 

 
 
 
 
attack,  the  adversary  does  not  know  the  underlying 
model and access to the training data, whereas, in the 
Gray-box  attacks,  the  adversary  is  assumed  to  have 
partial  knowledge  of  the  target  model.  Since  we  are 
investigating  cross-technique 
transferability,  we 
crafted  the  adversarial  examples  (features)  using 
various models based on white-box attacks. The actual 
attack  on  the  BFF-IDS  is  conducted  in  a  block-box, 
assuming  the  adversary  only  knows  the  feature 
extraction. Thus, we categorize our approach to be a 
grey-box attack. 

In 

the 

this  study, 

testing/deploying  phase 

Adversarial  Capabilities. 

the 
adversary is assumed to be able to manipulate features 
extracted  during 
to 
corrupt  the  BFF-IDS.  Neither  the  trained  model, 
the 
process,  nor  data 
adversary  can  draw  surrogate  samples  from  the 
original  distribution  of  the  benign  dataset.  But  the 
attacker does not know the model algorithm or design. 
Thus, the attack is conducted on the trained BFF-IDS 
using 
the 
testing/deployment phase.  

is  affected.  Specifically, 

surrogate 

feature 

crafted 

the 

at 

Adversarial  Specificity.  The  adversarial  examples 
can be crafted to compromise the model's performance 
on  a  specific  class  (targeted attack)  or  to  reduce  the 
model's  classification  performance  confidence  (non-
targeted attack). However, this study is limited to non-
target attacks. 

includes 

reduction 

Adversarial  Goals:  In  this  study,  the  adversary's 
goal 
and 
confidence 
misclassification.  The  adversary  aims  to  reduce  the 
confidence  of  the  BFF-IDS  by  causing  output 
ambiguity and increasing misclassification by altering 
the detection output class to any class different from 
the original class.  

3.3. Adversarial examples/unknown attack detection 
Framework for BFF-IDS 

In the framework (see Fig. 3), the BFF-IDS Model 
is built using federated learning in which each miner 
partially  trains  its  model  and  uploads  it  into  IPFS, 
while  the  pointers  to  the  model  are  stored  in 
blockchain.  Legitimate  users  can  then  download  the 
models through the hash obtained from the blockchain. 
The federated model is then finally aggregated at the 
users' end. The detailed design and implementation of 
the BFF-IDS are provided in [10]-the CAN ID cycle 
(frequency  of  occurrence) 
and 
transformed  using  Fast  Fourier  Transform(FFT); 
statistical and entropy features are then extracted. The 
features include minimum, maximum, mean, standard 
deviation,  skewness,  kurtosis,  Shannon  entropy, 
sample  entropy  and permutation entropy.  The attack 
is assumed to be launched after deployment in black-
box  scenario-the  adversary  does  not  know  the  BFF-
IDS underlying structure. We investigate the effect of 
various  adversarial  attack  methods  on  the  BFF-IDS, 

extracted 

is 

to  detect 

(MMD,  ED) 

which  include  the  Fast  Gradient  Sign  Method 
(FGSMA),  Jacobian-based  Saliency  Map  Approach 
(JSMA),  Support  Vector  Machine  (SVM)  and 
Decision  Tree  (DT)  attacks.  Motivated  by  [16],  we 
proposed  the  integration  of  a  statistical  adversarial 
detector before the BFF-IDS. The detector utilized a 
the 
statistical  method 
adversaries  and  unknown  attack  patterns.  Detected 
attack traffic is then captured and saved into a sandbox 
to  retrain new  BFF-IDS  with an "adversarial attack" 
class as a new class of the detected traffic. Therefore, 
each user is expected to have the detector in its CAN 
system  to  monitor  unknown/adversarial  attacks  and 
retrain  its  model  using  its  old  datasets  and  the  new 
adversarial data. The adversarial attack class would be 
used as samples to train the model. This way, the BFF-
IDS can be sustainable as a new version would release 
over time to withstand any new adversarial or novel 
attack patterns.   

3.4. Adversarial examples crafting 

This  section  presents  the  different  adversarial 
generating methods employed in this work. Although 
there is no guarantee that the method described here 
will generate traffic that BFF-IDS will misclassify, the 
degenerated  samples  are  described  as  "adversarial 
examples"  or  "unknown  attacks".  We  employed 
differentiable machine learning models such as DNN 
and non-differentiable machine learning models such 
as Support Vector Machine (SVM) and Decision Tree 
(DT) to craft adversarial samples. 

FGSMA.  FGSM  attack  was  proposed  by 
Goodfellow [41]- This method utilizes the gradient of 
a model's output to perturb adversarial examples,
8, 
with respect to its input in that direction. Although the 
method  is  computational  efficient,  it  introduces  a 
input 
significant  perturbation 
distribution-This  may  not  be  acceptable  in  some 
domains [23]. The perturbation can be defined as: 

that  distorts 

the 

 P

 is the magnitude. 

Q = RS(cid:25)3CT∇VWX(Y, Z)[                                 (4)
where 
R
JSMA.  Papenot  proposed  JSMA  to  address  the 
problem  of  FGSMA  by  reducing  the  scale  of 
perturbation through the iterative computation of the 
best feature to perturb for misclassification [42]. This 
approach enables the extraction of the influence of an 
individual  feature  on  a  particular  class  through  a 
saliency  map.  In  contrast,  other  features  introduce 
perturbation  on  the  original  input,  resulting  in  a 
misclassification  [23].  However,  the  computational 
cost of JSMA is much significant. The JSMA attacks 
utilize  the  Jacobian  matrix  to  evaluate  the  model's 
output sensitivity. The matrix is expressed as: 

W](Y) =

^_(Y)
^’

= ‘

^_a(Y)
b
^Y(cid:10)

(cid:10)×a

                      (5)

7 

 
 
 
  
 
Fig. 3. Adversarial Detection Framework Overview 

SVM  Attack.  This  method  attacks  SVM  by 
selecting a point orthogonally in the direction of the 
hyperplane,  acting  as  the  decision  boundary  to  the 
SVM subclassifier [20]. The points are selected using 

jkk⃗ lmn(Y⃗)o
pjkk⃗ lmn(Y⃗)op

Φe

 and 

Φef(cid:8)

Φef(cid:8) = gY⃗   − i.
, 

Y⃗: Y⃗RΦeq   ∪ Φe    (6)
 are the previous, new training 
where 
sets and a fine-tuning parameter for augmentation step 
 is  the  weight  that  represents 
size,  respectively; 
the hyperplane direction of subclassifier 
 used for the 
implementation of a multi-class SVM. 

jkk⃗[(cid:26)]

(cid:26)

i

 DT  attack.  In  the  DT  attack,  the  shortest  path  is 
computed  between  the  current  leaf  at  which  the 
sample  is  and  the  nearest  leaf  of  another  class.  The 
feature  in  the  first  common  node  shared  by  the  two 
paths is repeatedly perturbed until misclassification is 
achieved  by  modifying  a  few  non-targeted  features 
[16].  

3.5. Statistical Metrics for Adversarial 
examples/unknown attacks detection and BFF-IDS 
Augmentation 

3.5.1. Statistical Metrics for Adversarial 
examples/unknown attacks detection 

To  distinguish  between  a  known/benign  sample 
distribution  and  an  adversarial  example  (unknown 
samples)  distribution,  we  investigate  two  statistical 
distance measures: MMD and ED. Motivated by the 
hypothesis of [16], the first hypothesis is to determine 
whether  MMD  and  ED  can  distinguish  between  the 

benign  CAN  feature  and  adversarial  examples 
generated from them.  

"Hypothesis  1.  Measurable  difference  between 
known benign samples and adversarial examples can 
be observed within a bounded number of n examples 
using a consistent statistical test T." 

However, this hypothesis is limited to (1) the finite 
number  of  samples needed to observe the difference 
and (2) the unknown attack detection is restricted to 
the  adversarial  examples  crafting  algorithm.  The 
validation of this hypothesis is presented in section 4. 
Formally,  the  statistical  divergence  measures  are 

defined as : 

Maximum  Mean  Discrepancy  (MMD)—Given 
two random samples, 
 (adversarial), 
the  MMD,  which  measures  the  distance/divergence 
between the two samples, is formalized as: 

(benign) and 

<A

<(cid:8)

     %%st[ℱ, <(cid:8), <A ]

I

1
C

= sup
]yℱ

z

K

{ _(Y(cid:8)(cid:10))
(cid:10)|(cid:8)

1
D

, 

−

Y(cid:8)(cid:10) ∈ <(cid:8)

}                           (7)
{ _(YA(cid:10))
(cid:10)|(cid:8)
-th data point in the 
where 
 are the 
(cid:25)
YA(cid:10) ∈ <A
first  and  second  samples,  respectively.  Kernel 
function 
 is  selected  to  maximize  the  distances 
between the samples. In our case, we use a Gaussian 
kernel. 

_ R ℱ

Energy distance (ED)—we also employed the ED 
to measure the statistical distance between benign data 
and  adversarial  examples  distributions.  The  ED  is  a 

8 

 
 
 
 
 
 
 
specific case of MMD in which no kernel is applied 
[16].  Formally,  for  a  d-dimensional  random  sample, 
 (adversarial), the ED is defined as  

(benign) and 

<(cid:8)
[43]:  

<A

8

<(cid:8)

   (cid:127)(<(cid:8), <A) = 2(cid:128)|<(cid:8) − <A |(cid:129) − (cid:128)|<(cid:8) − <(cid:8)

8

 |(cid:129)
 |(cid:129)                         (8)
8 is  an  independent 
,  and  

where 
and  identically  distributed  (iid)  copy  of  the 
. 

− 2(cid:128)|<A − <A
, 
(cid:128)|<A| < ∞

is an iid copy of 

(cid:128)|<(cid:8)| < ∞

<(cid:8)

8
The  statistical 

<A
test  would  be  beneficial 

<A
in 
monitoring  and  detecting  adversarial  examples  and 
unknown  traffic  in  the  CAN.  Each  user  node  can 
collect  any  unknown  detected  samples  for  further 
analysis  and  retrain  the  model  to  detect  the  new 
unknown  attacks.  However,  the  statistical  test  only 
detects adversarial/unknown samples in large batches 
[16].  This  means  that  a  single  input  attack  will  go 
undetected, thus the statistical test is not suitable for 
offering security for the CAN. 

3.5.2. BFF-IDS Augmentation 

To  address  the  problem  of  detecting  single  attack 
samples, the detected adversarial/unknown attacks in 
large batches by the statistical test can be utilized to 
retrain the model by augmenting the BFF-IDS with an 
additional "unknown" class 
. The new model can, 
after that, replace the old version. With this approach, 
regular updates or model versions can be retrained and 
released  whenever  a  new  attack  trend  is  detected. 
Therefore,  motivated  by  [16],  the  hypothesis  is  on 
whether  augmenting  the  BFF-IDS  model  to  detect 
new attacks is suitable for CAN security: 

(cid:132)(cid:133)(cid:129)(cid:134)

"Hypothesis  2.  Augmenting  the  BFF-IDS  model 
with unknown class training samples can successfully 
detect  adversarial  examples  and  other  unknown 
samples." 

s(*(cid:133)(cid:135) = (cid:6)<, (cid:132)(cid:11),

The  goal  of  the  BFF-IDS  augmentation  is  for  the 
model to be able to detect single adversarial attacks. 
In  the  augmentation  of  the  BFF-IDS,  the  initial 
original test  dataset, 
 where X  and Y 
are  features  and  label  is  used  to  craft  adversarial 
.  Different  crafting  algorithms  are 
features, 
<(cid:133)(cid:129)(cid:134)
employed  to  generate  the  samples,  which  are  then 
.  A  new  model,  BFF-IDS 
assigned  to  one  class, 
(AUG), is trained on the augmented dataset 
with the 
samples belonging to the same class, 
In  an  actual  application,  these 

. 
or  unknown 
attack  distribution  is  detected  by  the  statistical  test. 
Then existing datasets are augmented with the newly 
detected  class,  and  the  model  is  retrained.  The 
statistical  detectors  are 
the 
augmented  samples  as  the  new  known  samples  for 
testing  incoming  traffic.  The  validation  of  this 
hypothesis is presented in section 4. 

< ∪ <(cid:133)(cid:129)(cid:134)
having Y original label and all adversarial 

then  updated  with 

(cid:132)(cid:133)(cid:129)(cid:134)
<(cid:133)(cid:129)(cid:134) 

(cid:132)(cid:133)(cid:129)(cid:134)

< 

 3.6. Performance Evaluation Metric 

The  model  performances  are  investigated  using 
precision,  recall,  F1-score,  and  accuracy.  The 
investigation  includes  the  model's  performance  on 
benign data,  adversarial examples, and the BFF-IDS 
performance  after  augmentation.  In  addition,  the 
robustness of the statistical test detector and the model 
augmentation  are  also  investigated.  The  evaluation 
metrics are expressed as follows: 

+(cid:136)(cid:136)(cid:137)21(cid:136)’ =  

(cid:138)@ + (cid:138)(cid:29)
(cid:138)@ + (cid:138)(cid:29) + (cid:139)@ + (cid:139)(cid:29)

           (9)

@2(cid:141)(cid:136)(cid:25)S(cid:25)(cid:142)C =  

(cid:138)@
(cid:138)@ + (cid:139)@

                               (10)

(cid:143)(cid:141)(cid:136)1ZZ =  

(cid:138)@
(cid:138)@ + (cid:139)(cid:29)

                                      (11)

(cid:139)(cid:8) =  

2(cid:138)@
2(cid:138)@ + (cid:139)@ + (cid:139)(cid:29)

                                 (12)

where  TP,  TN,  FP,  and  FN  are  the  number  of  true-
positive, 
true-negative,  false-positive,  and  false-
negative cases, respectively. 

4. Experimental Results 

This section presents the experimental results of our 
investigation  of  various  attack  models  on  BFF-IDS. 
The detailed implementation, training and test results 
for the BFF-IDS are presented in [10]. Therefore, at 
this point, we assume the federated model  is  trained 
and  deployed.  We  first  present  the  adversarial 
examples  crafted  and  the  statistical  test  result  of  the 
adversarial  samples  with  benign  samples.  We  then 
present  the  detection  result  of  benign  data  and  the 
adversarial  samples  by  various  users'  nodes  in  the 
BFF-IDS model. Furthermore, we present the results 
and  model 
regarding 
augmentation. 

adversarial 

detectors 

Firstly,  we  investigated  the  performance  of  the 
BFF-IDS on benign test data. The BFF-IDS are built 
using miners ranging from 5 to 20. As indicated in Fig. 
4, 5 miners' model has the best generalization with the 
score of 0.97611,0.97516 and 0.97540 for precision, 
recall  and  F1-  score,  respectively.  The  lowest-
performing model is that of 20 miners with records of 
0.94345,  0.93936  and  0.94027  for  precision,  recall 
and F1-score, respectively. As evident from the results, 
the model's performance decreases with an increase in 
the number of miners. This is a result of the splitting 
of  the  training  data  based  on  the  number  of  miners. 
The  higher  the  number  of  miners,  the  fewer  the 
number  of  the  dataset  available  for  training  each 
model.  Therefore,  the  5  miners'  model  has  enough 

9 

 
 
 
 
 
 
 
 
 
 
 
data for training and thus better generalization result 
of benign test data. 

4.1. Identification of Adversarial examples using 
statistical metrics and test 

∈

This  section  presents  the  statistical  divergence 
measure on both benign and adversarial datasets using 
the MMD and ED. For  the  FGSM attack,  we varied 
the perturbation from 0.01 to 0.50 to observe how the 
statistical  score  varies.  As  indicated  in  Table  2,  the 
MMD  and  ED  of  adversarial  samples  generated  by 
FGSM  increased  with  perturbation 
.  Except  for 
JSMA,  the  MMD  values  of  adversarial  samples  are 
higher than that of benign data. Likewise, there is an 
increase in the ED values for the adversarial samples 
except for the DT attack.  Both JSMA and DT attacks 
show little increase in the MMD and ED, respectively, 
compared  to  the  benign  data.  Fig.  5  depicts  the 
distribution  of  the  statistical  measures  across  the 
benign and adversarial examples, with the MMD and 
ED  ranging  from  0.01856  to  0.  4027  and  2.8576  to 
4.1742,  respectively.  Consequently,  these  results 
suggest a considerable statistical distinction between 
benign  samples  and  generated  adversarial  samples. 
Thus,  the  statistical  approach  is  sufficient  to  detect 
adversarial attacks against BFF-IDS. 

4.2 Adversarial examples against BFF-IDS 

In  this  section,  we  attack  the  BFF-IDS  using  the 
generated  adversarial  examples.  The  use  of  various 
adversarial  crafting  models  to  attack  BFF-IDS  is 
based on the principle of cross-technique adversarial 
sample 
is 
considered in our investigation because an adversary 
can  launch  any  form  of  attack,  which  might  be 
different  from  what  the  BFF-IDS  was  trained  to 
detect-  The  adversary  can  generate  attack  traffic  on 
the CAN using an adversarial machine or model.    

[20].  This  principle 

transferability 

Fig. 4. BFF-IDS Performance on Benign Test Data 

Table 2: 

Maximum Mean Discrepancy (MMD) and Energy Distance (ED) 
between the Benign Distribution and Adversarial Samples 

Manipulation 

Benign 

FGSM  
FGSM  

FGSM  
FGSM   

FGSM   
FGSM  

FGSM  
FGSM  

JSMA 
DT attack 

SVM attack 

∈

0.01 
0.08 

0.15 
0.22 

0.29 
0.36 

0.40 
0.50 

MMD 

0.0226 

0.0278 
0.0422 

0.0855 
0.1371 

0.1953 
0.2522 

0.2917 
0.4027 

0.0185 
0.0599 

0.252 

ED 

2.8654 

2.8707 
2.9251 

3.0425 
3.2086 

3.4128 
3.6306 

3.7789 
4.1742 

2.8698 
2.8576 

3.3947 

The investigation focuses on the best model, i.e. 5 
miners, BFF-IDS. Fig. 6 presents the performance of 
the  BFF-IDS  under  attack.  The  performance  of  the 
model significantly drops from the score of 0.97611, 
0.97516, and 0.9754 in precision, recall and F1-score 
to about 0.12064 (SVM attack), 0.17828 (FGSM 0.36) 
and  0.13523(SVM  attack),  respectively.  The  DT 
attack  recorded  the  second  least  degradation  in 
performance of about 0.3 to 0.4 across the metric. On 
the other hand, the JSMA attack was unsuccessful in 
degrading the performance of the BFF-IDS. As shown 
in  Table  2,  the  MDD  value  for  the  JSMA  data  was 
lower than the benign data. 

Furthermore, we investigate the Federated Learning 
situation where each model, BFF-IDSi, is aggregated 
at the user end, i. We, therefore, assume that the attack 
is  conducted  at  the  user  end  of  various  numbers 
ranging  from  5  to  40.  The  benign  and  adversarial 
samples are split equally based on the number of users 

10 

Fig. 5. Statistical Measures for Benign and Adversarial 

Examples Distribution 

Fig. 6. BFF-IDS Performance on Benign Data and Adversarial 

Samples 

 
 
 
 
 
 
 
 
 
 
 
in 

succeeded 

under  consideration.  As  indicated  in  the  results  as 
shown  in  Fig.  7,  except  for  JSMA,  the  adversarial 
significantly  degrading 
samples 
accuracy  from  about  0.98  to  the  least  accuracy  of 
about  0.18,0.18, 0.20, 0.20 and 0.34 for FGSM  (eps 
0.29), FGSM (eps 0.29), FGSM (eps 0.29), SVM and 
DT, respectively. The least performance for all cases 
was recorded in 5 users' scenarios. 

4.3 Statistical Hypothesis for Adversarial examples 
detection 

Based on the hypothesis, H0, that "benign data are 
statistically  closed  to  the  training  data  distribution", 
we  compute  the  MMD  (using  Gaussian  Kernel)  and 
its  corresponding  p-values  as  implemented  by  [16]. 
The p-values are then compared against the threshold 
set  to  0.05.  For  legitimate  samples,  the  p-value  is 
expected  to  be  higher  than  the  threshold  for  the 
hypothesis  to  hold.  The  sample  size  is  critical  in 
detecting 
the  difference  between  benign  and 
adversarial samples. It becomes more complex as we 
deal  with  a  dataset  containing  four  classes  of  attack 
(DOS, fuzzy, impersonation, and attack-free).  

 We conducted several experiments with sample sizes 
of  50,  100,  500  and  1000  to  obtain  the  minimum 
sample size required to detect adversarial distribution 
for each class  of attack. As  indicated in Table  2, 50 
samples size is sufficient for most cases to discern the 
adversarial sample from the benign sample and reject 
the  H0-.  This  is  remarkable  when  compared  to  the 
training size of about 80,000 samples. 

∈

and 

For  DT 

However, about 100 and 500 sample sizes are needed 
for impersonation and fuzzy attack to be detected in 
 0.36)  and  JSMA  generated  samples, 
FGSM  (
respectively. 
JSMA-generated 
impersonation attacks, the statistical test fails to detect 
the adversarial samples from the benign sample. For 
lower  perturbation
 of  0.29  and  less  in  FGSM,  the 
statistical test could not detect the adversarial samples. 
Likewise,  the  statistical  test  fails  to  detect  the 
adversarial  samples  for  DT-generated  adversarial 
samples, except for DOS attack samples. 

 ∈

that 

adversarial 

These  results  are  consistent with  the  result  in  4.1, 
in  FGSM,  DT  and  JSMA 
which  shows  that  lower
 ∈ 
yielded 
less 
samples 
distinguishable from benign samples. For FGSM with 
high 
 (greater than 0.36) and SVM, which recorded 
high values in both MMD and ED compared to benign 
samples,  were  easily  detected  by  the  two-sample 
statistical test-50 samples size was sufficient to reject 
the H0. 

are 

∈

Furthermore, we further investigated the acceptance 
of  H0  based  on  the  adversarial  generation  method 

containing  a  random  collection  of  all  attack  classes. 
This approach aims to see the minimum size required 
when  the  class-wise  approach  is  not  applied-i.e.  the 
adversary  launch  attack  using  randomly  generated 
adversarial samples for all the attack classes. 

Table 3. 

Minimum  Samples  (Adversarial  Examples)  Size 
Required to Detect Adversarial Examples Confidently 

Manipul. 

Attack Class 

DOS 

Fuzzy 

Impers.   Attack
-free 

FGSM  
(
 0.36) 
∈
FGSM  
 0.50) 
(
∈
JSMA 
SVM 
DT 

50 

50 

50 
50 
500 

50 

50 

500 
50 
- 

100 

50 

- 
50 
- 

50 

50 

50 
50 
- 

∈

As  presented  in  Fig.  8,  only  FGSM  (

  0.5)  was 
detected with a sample size of 50. Meanwhile, FGSM 
(
  0.36)  and  SVM-generated  adversarial  samples 
∈
were detected at a sample size of 100. However, the 
statistical  test  also  fails  to  detect  JSMA  and  DT-
generated  adversarial  distribution 
this  case, 
confirming the earlier results in  section  4.1. Thus, it 
requires more sample size to detect adversarial sample 
distribution  containing  random  classes  of  all  the 
attacks  than  class-wise  statistical  tests  on  such  a 
similar distribution. 

in 

4.4 BFF-IDS Augmentation for Adversarial examples 
mitigation 

The previous section observed that the adversarial 
sample  distribution  differs  statistically  from  the 
benign  sample  distribution.  However,  the  statistical 
test  cannot  detect  adversarial  samples  on  a  single-
input  basis  and  its  confidence  diminishes  with  the 
decrease in the number of samples in a batch. More so, 
the  statistical  test  cannot  pinpoint  which  input  is 
adversarial in a group of sample-this is consistent with 
the findings of [16]. 

In  this  section,  experimental  results  regarding 
hypothesis  2  are  provided.  The  augmentation  BFF-
IDS model by training the model with the addition of 
samples having adversarial class as labels  should be 
effective  in  detecting  attacks  in  the  benign  data  and 
adversarial samples.  

We  conducted  two  experiments;  in  the  first 
experiment, we considered FGSMA (0.36) and SVM 
as the adversarial samples to be augmented into the 

11 

 
 
 
 
Fig. 7. BFF-IDS Performance for Different users on Benign Data and Adversarial Samples 

Fig. 8. Hypothesis Ho Acceptance concerning the Sample Size based on the Statistical Test 

dataset  as  an  adversarial  class.  These  adversarial 
samples  were  considered  because  the  statistical  test 
results in the previous section show that these samples 
are  easily  detected  with  fewer  samples.  The  new 
trained  augmented  model,  BFF-IDS(AUG),  is  then 
tested on the adversarial samples, including those not 
included in the training, as presented in Table 4. The 
detection  accuracy among FGSM  and  SVM- attacks 
increased  to  more  than  76.20  %  and  79.20%, 
respectively. The improvement results from including 
these  samples  in  training  as  part  of  the  adversarial 
class. On the other hand, the accuracy of JSMA and 
DT-attack 
-33.17%, 
respectively. The reduction results from the exclusion 

-97.46%  and 

reduced 

to 

of these samples in the training set. Furthermore, the 
 detection  saw 
FGSM  set  for  all 
improvement  despite  only 
 0.36  is  used  for  the 
training. 

the  value 

∈

∈

The 

second  experiment  considered  all 

the 
adversarial generation algorithms, i.e., FGSMA (0.36), 
SVM, JSMA and DT, as training samples belonging 
to  the  adversarial  class.  As  presented  in  Table  5, 
except  for  JSMA,  the  detection  rate  of  all  the 
adversarial samples improved, including that of DT, 
which  shows  significantly 
low  MMD  values 
compared to others. The DT detection rate increased 
to about 56.17%. 

12 

 
 
 
 
Considering that BFF-IDS is a federated model, we 
consider the best performances of the 5-miner model 
across  various  numbers  of  users.  For  the  first 
experiment  with  only  FGSMA  (0.36)  and  SVM  as 
augmented as training samples, the detection rates for 

Table 4. 

BFF-IDS Augmentation using FGSM 0.36 and SVM 

Manipul. 

BFF-
IDS 
Detect. 
Rate 

BFF-IDS 
(AUG) 
Detect. 
Rate 

Recovered 
rate  by  the 
BFF-IDS 
(AUG) 

 0.36)  

0.17860 

0.17919 

FGSM  
(
0.29) 
∈ 
FGSM 
(
∈
FGSM  
(
∈
JSMA 
SVM-
attack 
DT-attack  0.33895 

0.97520 
0.20586 

0.19620 

  0.50) 

0.96140 

78.221 % 

0.96345 

78.485% 

0.95890 

76.27% 

0.00060 
0.99795 

-97.46% 
79.209% 

0.00730 

-33.165% 

Table 5. 

BFF-IDS Augmentation using FGSM 0.36, SVM, JSMA, DT 

BFF-IDS 
Detection 
Rate 

BFF-IDS 
(AUG) 
Detection 
Rate 

Recovered 
rate  by  the 
BFF-
IDS(AUG) 

0.17919 

0.96443 

78.524% 

0.17860 

0.96515 

78.655% 

0.19620 

0.95958 

76.338% 

0.97520 
0.20586 

0.00175 
0.99935 

-97.35% 
79.349% 

0.33895 

0.90070 

56.175% 

Manipul. 

 0.36)  

FGSM  
0.29) 
(
∈ 
FGSM 
(
∈
FGSM  
(
∈
JSMA 
SVM-
attack 
DT-
attack 

  0.50) 

 Fig. 9. BFF-IDS(AUG) Performance for Various Users using FGSM 0.36 and SVM for Augmentation 

Fig. 10. BFF-IDS(AUG) Performance for Various Users using FGSM 0.36, SVM, JSMA and DT for Augmentation 

13 

 
 
 
 
 
 
 
 
 
the FGSMA and SVM across all users were more than 
0.96, as shown in Fig. 9.  As expected,  the detection 
rate  for  the  JSMA  and  DT  were  low  across  all  the 
users  and  were  between  0.001  to  0.009.  Also,  the 
detection rate for the second experiment with all the 
adversaries  included  in  the  training  increased  across 
all  users  except  for  JSMA,  which  shows  little  or  no 
improvement,  as  shown  in  Fig.  10.  This  is  also 
consistent  with  the  statistical  test  finding  in  the 
previous  section.    Therefore,  augmenting  the  model 
with an adversarial class would improve the detection 
rate of the model in a single-input adversarial example 
attack. 

4.5 Robustness of the statistical test and BFF-IDS 
Augmentation 

4.5.1 Robustness of the statistical test 

Section 4.3 assumes that the adversary can generate 
large  adversarial  samples,  and  the  detector  can 
conduct the statistical test on large sample sizes. This 
notion may not always be the case in practice, where 
the adversary is aware of the defense mechanism and 
samples 
decides 
simultaneously using different methods  to evade  the 
BFF-IDS  and  the  adversarial  detector.  As  such,  we 
investigate  the  confidence  of  our  framework  when 
presented with such a realistic situation.  

adversarial 

generate 

to 

We  considered  two  scenarios  to  demonstrate  the 
robustness of the statistical test as suggested by [16]. 
Firstly, we considered a situation where an adversary 
embeds  adversarial  samples  generated  from  a  single 
attack  model  in  a  benign  sample.  Each  adversarial 
sample mixes with varying percentages of the benign 
sample  (see  Fig.  11).  As  shown  in  the  results,  the 
confidence  of  the  statistical  test  decreases  with  an 
increase  in  the  proportion  of  benign  samples.  The 
confidence  in  detecting  FGSM  (
  0.5)  significantly 
degraded when the proportion of benign samples went 
to about 40%. For SVM, the confidence was affected 
at  just  beyond  10%  of  benign  sample  presents. 
Although  FGSM  (
  0.29)  shows  high  H0,  the 
confidence  equally  degraded  with  an  increase  in  the 
proportion of benign samples. JSMA and DT maintain 
high  H0  scores  across  all  the  mixtures.  For  this 
scenario,  the  H0  acceptance  amid  benign  samples  is 
consistent with the findings in section 4.3 – the higher 
the  MMD  value  of  a  technique  the  easier  it  is  to  be 
detected. 

∈

∈

Secondly,  we  considered  a  scenario  where  the 
adversary  simultaneously  launches  the  attack  using 
more  than  one  adversarial  crafting  algorithm  amid 
benign traffic. As indicated in Fig. 12, the confidence 
of  the  test  equally  degrades  with  an  increase  in  the 
proportion of the benign sample. The mixture of  

Fig. 11. Hypothesis Ho Test on Adversarial Examples and 

Benign Data. 

FGSM  (0.29)/SVM  shows  the  highest  acceptance 
score  of  0.36  at  10%  benign  proportion,  while  the 
mixture of FGSM (0.5)/SVM maintains the rejection 
of  the  H0  from  10%-20%  of  the  benign  proportion. 
Consequently,  the  detection  of  adversarial  examples 
mixed  benign  samples  becomes  difficult  among  a 
small set of inputs- the confidence of the statistical test 
degrades  with  a  decrease  in  the  proportion  of 
adversarial  samples  present 
the  mixture  of 
adversarial and benign samples.  

in 

Fig. 12. Hypothesis Ho Test on a Different Mixture of 

Adversarial Examples and Benign Data. 

4.5.2 Robustness of BFF-IDS Augmentation 

Section  4.4  presented 

the  BFF-IDS(AUG) 
performance in detecting the adversarial samples. In 
this section, we investigate the impact of the inclusion 
of  adversarial  samples  class  on 
the  general 
performance  of  the  model  using  the  two  training 
samples  used  in  the  augmentation  as  discussed  in 
section 4.4. 

Fig.  13  presents  the  impact  of  training  the  model 
with FGSM  (0.36) and SVM as  well as with FGSM 
(0.36),  SVM,  JSMA  and  DT  as  the  augmented 
samples for the adversarial class. For the FGSM (0.36) 
and SVM, the general performance of the model was 
significantly  high  with  0.97667,  0.97607,  0.97619, 
and 0.97607 scored for precision, recall, f1-score and 
accuracy,  respectively.  The  second  sample  with 
FGSM (0.36), SVM, JSMA and DT show a significant 
impact on the general performance of the model. The 
to  about  0.72963, 
model  performance  dropped 

14 

 
 
 
 
 
0.63266, 0.63447 and 0.63266 for precision, recall, f1-
score  and  accuracy,  respectively.  In  all  cases,  the 
model's confidence improved compared to the model 
without augmentation, which recorded about 0.15927, 
0.17860, 0.15779 and 0.17860 for precision, recall, f1-
score  and  accuracy,  respectively.  Likewise,  the 
confusion matrix shows how the adversarial example 
reduced  the  model's  confidence  by  confusing  the 
model  to  misclassify  most  of  the  samples  as  DOS 
attacks (see Fig. 14 (a)). Augmenting the model using 
the 
all 
performance.  However,  the  model  had  difficulty 
classifying  the  adversarial  samples  correctly,  attack-
free  from  impersonation  and  impersonation  from 
fuzzy attack (see Fig. 14 (b)). However, by removing 
those  samples  with  lower  MMD  values,  i.e.,  JSMA 
and  DT,  the  model  could  generalize  with  more 
superior accuracy (see Fig. 14 (c)). 

significantly 

improved 

samples 

the 

Fig. 13. Robustness of BFF-IDS (AUG) using Various 

Adversarial Examples. 

5. Discussion  

Adversarial  examples  significantly  impact 

the 
confidence  of  BFF-IDS.  In  particular,  FGSM  with 
perturbation  higher  than  0.08,  SVM  attack  and  DT 
attack  have  the  most  impact  on  the  model.  These 
attacks degraded the model's accuracy from more than 
0.975 to below 0.34. Among the adversarial examples 
investigated  in  this  study,  only  JSMA  non-targeted 
attack is found not to impact the model's confidence. 
This  result  is  consistent  with  statistical  divergence 
measures, MMD and ED, in section 4.1. 

However,  the  question  remains  which  metric 
between MMD and ED is a better indicator of how the 
BFF-IDS would be affected by an adversarial sample. 
Although the DT attack recorded a lower value for ED 
than benign data, it was still successful in degrading 
the  performance  of  the  BFF-IDS  as  its  MMD  value 
was higher than benign data. In the case of JSMA, the 
ED value was higher than benign data but still couldn't 
affect the model's performance as the MMD value was 
lower. Thus, the  MMD is a better measure to detect 
adversarial  examples/unknown  attacks  in  BFF-IDS 
for CAN.  

The  results  in  section  4.3  show  that  the  statistical 
hypothesis,  H0,  effectively  detects  adversarial 
examples.  Except  for  JSMA  and  DT  attacks,  a 
minimum  of  50-100  samples  are  sufficient  to  detect 
most of the attacks in class-wise and mixed samples 
containing all classes of attack samples. However, it 
that  mixed  samples  containing 
was  discovered 
random  classes  of  the  attacks  required  more  sample 
size than the class-wise for the statistical test to detect 
the  adversarial  samples.  Consequently,  there  is  a 
significant  statistical  distinction between  benign  and 
generated  adversarial  samples;  thus,  the  statistical 
approach is sufficient to detect adversarial samples. 

the 

statistical 

However, 

test  cannot  detect 
adversarial  samples  on  a  single-input  basis  and  its 
confidence diminishes with a decrease in the number 
of samples in a batch. Thus, the augmentation of the 
BFF-IDS by retraining the model with detect samples 
of  adversarial  examples  is  effective  in  detecting 
adversarial examples per input. Except for JSMA and 
DT,  the  augmentation  resulted  in  a  recovery  rate  of 
more than 76.20% in both the augmentation scenario 
considered in section 4.4. 

Furthermore, both the statistical hypothesis, H0, and 
the  BFF-IDS  augmentation,  BFF-IDS  (AUG),  are 
affected  by  the  size  and  type  of  adversarial  samples 
examined. As observed in section 4.5.1, the robustness 
of the hypothesis, H0, is affected by the proportion of 
benign  samples  among  adversarial  samples-  the 
confidence  of  the  statistical  test  degrades  with  a 
decrease  in  the  proportion  of  adversarial  samples 
present  in  the  mixture  of  adversarial  samples  and 
benign  samples.  On  the  other  hand,  the  overall 
effectiveness of the BFF-IDS(AUG) is affected by the 
type  of  samples.  Particularly,  the  addition  of  JSMA 
and  DT-attack  to  the  training  samples  significantly 
diminishes the model's overall performance (accuracy) 
from more than 0.97 to about 0.73. Consequently, the 
statistical test using  MMD provides a  good measure 
of  which  samples  should  be  included  for  the 
augmentation.  The  higher  the  MMD  value  of  the 
adversarial  example  from  the  benign  sample,  the 
better. In other words, the FGSM (<0.1), JSMA and 
DT-attack fail to achieve the goal of the adversary of 
reducing  the  confidence  of  the  model  and  including 
them in training data significantly affects the accuracy 
of the BFF-IDS(AUG). 

the 

test 

Although 

and  model 
statistical 
augmentation approaches were motivated by [16], the 
main difference between our works is that we used the 
cross-technique  transferability  principle  while  they 
focused  on 
transferability.  We 
investigated 
impact  of  several  adversarial 
examples crafting algorithms in which the adversary  

inter-technique 
the 

15 

 
 
 
 
(a)                                                                                                                         (b) 

Fig. 14. Confusion matrix for BFF-IDS and BFF-IDS (AUG) (a) adversarial example (FGSM 0.39) attack on initial BFF-IDS (b) BFF-
IDS (AUG) performance with FGSMA (0.39), SVM, JSMA and DT as augmentation data (c) BFF-IDS (AUG) performance with FGSMA 
(0.39) and SVM as augmentation data 

(c) 

has only knowledge of the features used to train  the 
model. Also, their works focus on MNIST, DREBIN 
and MicroRNA datasets using the traditional learning 
approach,  while  we  investigated  on  CAN  dataset 
using BFF-IDS built by the FL concept. In addition, 
we  proposed  the  continuous  retraining  of  the  model 
with  the  detected  unknown  samples  in  a  sandbox 
environment  to  enable  the  detection  of  unforeseen 
adversarial examples. 

Considering  other  related  works,  [23]  demonstrated 
how  ZOO  and  GAN  attacks  successfully  degraded 
NIDS  performance.  However,  there  was  no  defense 
mechanism  proposed  against the  attacks.  Unlike our 
study, which offers mitigation measures, the proposed 
testbed  in  [27]  facilitates  the  investigation  of  the 
impact of adversarial examples on IDS; the software 
offers  no  room  for  testing  mitigation  measures. 
Although the work of [28] is on CAV, the study focus 
only  considered  vehicular  ad-hoc  networks  using 
synthetic datasets and binary classification problems. 

On the other hand, we used real-world datasets while 
focusing on multi-class (five) classification problems, 
including the adversarial sample class. 

To  the  best  of  our  knowledge,  the  only  available 
work that directly deals with adversarial examples on 
the  CAN  bus  is  [22].  The  car  hacking  dataset 
(CARHD)  used  in  that  study  was  provided  and 
probably  collected  under  similar  conditions  by  the 
same laboratory as ours, OTID. Also, the adversarial 
mitigation  method,  AADS,  was  similar  to  our 
approach.  However,  we  utilized  CAN  ID  cycles  as 
features  while  they  employed  the  raw  data,  which 
needed  to  be  decoded  from  hexadecimal  to  decimal 
format,  making  their  method  more  complex.  In 
addition,  their  model  was  built  using  the  traditional 
method,  whose  limitation  was  highlighted  in  the 
introduction  section.  Compared  to  the  maximum 
, 0.5 we considered in our experiments, 
perturbation 
the authors considered a very high perturbation 
 of 5, 
which may be forbidden in the CAN bus specification- 

∈

∈

16 

 
 
 
 
High  perturbations  are  mostly  acceptable  in  image 
tasks.  Finally, 
their  proposed  system  has  no 
mechanism  to  detect  unknown  adversarial  examples 
and, therefore, is unsustainable in real-life deployment. 
As presented in Fig. 15, our proposed model relatively 
shows  competitive  results  despite  using  the  difficult 
features,  transferability  principle,  and  more  attack 
classes. 

6. Conclusion  

In this work, we set out to establish the vulnerability 
of  BFF-IDS  to  adversarial  examples,  detect  the 
adversarial  examples  and  augment  the  BFF-IDS  to 
detect such examples and make it more resilient. We 
investigated the vulnerability of BFF-IDS by relying 
on the adversarial sample transferability to investigate 
the  impact  of  several  adversarial  sample  algorithms, 
including FGSM, JSMA, SVM-attack, and DT-attack. 
We relied on a threat model to determine whether an 
adversary knows the features needed to significantly 
diminish  the  confidence  of  the  BFF-IDS  using  an 
adversarial  system.  To  augment  BFF-IDS  to  detect 
adversarial  examples,  we  relied  on  the  notion  that 
generated  samples  may  have  a  different  statistical 
distribution  from  benign  samples.  Resilience  is 
protected by augmenting BFF-IDS with an additional 
class  for  the  detected  samples,  which  allows  for 
examples.  We 
adversarial 
detecting 
demonstrate  the  robustness  of  the  statistical  test  by 
considering  the mixture  of  adversarial samples from 
various  algorithms  and  benign  samples.  Also,  we 
the  BFF-IDS 
investigated 
augmentation  by  investigating  how  the  mixture  of 

robustness  of 

single 

the 

several  combinations  of  adversarial  examples  from 
the different algorithms as training samples can affect 
the  detection  of  the  adversarial  samples  and  the 
general performance of the model. 

The most important results of this research are the 

following: 
  BFF-IDS 

is  very  vulnerable 

to  adversarial 
examples attacks as it succeeded in significantly 
reducing the confidence (accuracy) of our model 
from more than  97% to as low as 20% in  some 
instances. 

  MMD  is  an  effective  statistical  measure  for 
detecting  adversarial  examples  and  unknown 
samples.  The  statistical  detector  integrated  with 
BFF-IDS effectively detects adversarial samples 
from  benign  sample  distributions  in  large  batch 
samples.  However,  the  statistical  test  cannot 
identify which samples are adversarial. 

  We  found  that  BFF-IDS,  when  augmented  with 
the  class  of  detected  examples,  significantly 
improves the confidence and thus resilience of the 
model,  as  it  allows  for  detecting  attacks  on  a 
single input basis. 

significantly 
The  BFF-IDS(AUG)  performance 
depends  on  the  combination  of  adversarial  samples 
included 
In  particular, 
the  argumentation. 
adversarial samples with higher values of MMD when 
compared to benign data are more suitable as they do 
not diminish the overall accuracy of the model. 

for 

Therefore, we conclude that the proposed integration 
of the statistical test as an adversarial 

.

Fig. 15. Performance evaluation of BFF-IDS(AUG) against other works 

17 

 
 
 
 
detector and the subsequent augmentation of the BFF-
IDS  with  detected  adversarial  samples  provides  a 
sustainable  security  framework  against  adversarial 
examples  and  other  unknown  attacks.  In  the  bigger 
picture, this method helps deriving benefits from the 
huge  (big)  data  generated  in  the  smart  city  by 
detecting  unknown  samples  and  using  them  to 
augment  security  models  and  provide  other  room  to 
analyze unforeseen novel attacks. 

to  establish 

Further  studies  are  needed 

the 
acceptable  perturbation  based  on  the  CAN  protocol. 
Also, a black-box scenario where the adversary has no 
knowledge of the feature but can generate CAN traffic 
should be investigated. 

Declaration of Competing Interest 

The  authors  declare  that  they  have  no  known 
competing financial interests or personal relationships 
that  could  have  appeared  to  influence  the  work 
reported in this paper. 

Acknowledgements 

the 

This research is supported by Jeollannam-do (2021 
R&D  supporting  program  operated  by  Jeonnam 
Technopark)  and  financially  supported  by 
the 
Ministry of Trade, Industry and Energy (MOTIE) and 
Korea  Institute  for  Advancement  of  Technology 
(KIAT)  under 
research  project:  "National 
Innovation  Cluster  R&D  program"  (Grant  number: 
1415175592  &  P0016223).  It  is  also  supported  by 
communications 
Institute 
Technology  Planning  &  Evaluation  (IITP)  grant 
funded by the Korean government (MSIT) (No. 2021-
0-02068, Artificial Intelligence Innovation Hub). We 
acknowledge the authors, K. Grosse et al., for granting 
us  access  to  their  source  code  to  conduct  our 
investigations. 

Information  & 

of 

References 

[1] 

[2] 

[3] 

[4] 

X.  Li,  Z.  Hu,  M.  Xu,  Y.  Wang,  and  J.  Ma, 
"Transfer learning based intrusion detection 
scheme for Internet of vehicles," Information 
Sciences, vol. 547, pp. 119-135, 2021. 
L.  Yang,  A.  Moubayed,  and  A.  Shami, 
"MTH-IDS:  A  Multi-Tiered  Hybrid 
Intrusion  Detection  System  for  Internet  of 
Vehicles," IEEE Internet of Things Journal, 
2021. 
C.  Miller  and  C.  Valasek,  "A  survey  of 
remote  automotive  attack  surfaces,"  black 
hat USA, vol. 2014, p. 94, 2014. 
M.  L.  Han,  B.  I.  Kwak,  and  H.  K.  Kim, 
"Anomaly  intrusion  detection  method  for 

[5] 

[6] 

[7] 

[8] 

[9] 

[10] 

I.  Kotenko, 

vehicular  networks  based  on  survival 
analysis,"  Vehicular  communications,  vol. 
14, pp. 52-63, 2018. 
N.  Khatri,  R.  Shrestha,  and  S.  Y.  Nam, 
"Security  Issues  with  In-Vehicle  Networks, 
and  Enhanced  Countermeasures  Based  on 
Blockchain,"  Electronics,  vol.  10,  no.  8,  p. 
893, 2021. 
F. Fenzl, R. Rieke, Y. Chevalier, A. Dominik, 
and 
fields: 
"Continuous 
Enhanced 
in-vehicle  anomaly  detection 
using machine learning models," Simulation 
Modelling Practice and Theory, vol. 105, p. 
102143, 2020. 
I.  Berger,  R.  Rieke,  M.  Kolomeets,  A. 
Chechulin,  and  I.  Kotenko,  "Comparative 
study  of  machine  learning  methods  for  in-
vehicle  intrusion  detection,"  in  Computer 
Security: Springer, 2018, pp. 85-101. 
D.  Kosmanos  et  al.,  "A  novel  Intrusion 
Detection System against spoofing attacks in 
connected Electric Vehicles," Array, vol. 5, 
p. 100013, 2020. 
M. Aloqaily, S. Otoum, I. Al Ridhawi, and Y. 
Jararweh, "An intrusion detection system for 
connected vehicles in smart cities," Ad Hoc 
Networks, vol. 90, p. 101842, 2019. 
I.  Aliyu,  M.  C.  Feliciano,  S.  Van 
Engelenburg, D. O. Kim, and C. G. Lim, "A 
Blockchain-Based  Federated  Forest 
for 
SDN-Enabled In-Vehicle Network Intrusion 
Detection System," IEEE Access, vol. 9, pp. 
102593-102608, 2021. 

[13] 

[12] 

neural 

convolutional 

[11]  M. Crosby, P. Pattanayak, S. Verma, and V. 
technology: 
Kalyanaraman,  "Blockchain 
Beyond bitcoin," Applied Innovation, vol. 2, 
no. 6-10, p. 71, 2016. 
H.  M.  Song,  J.  Woo,  and  H.  K.  Kim,  "In-
vehicle  network  intrusion  detection  using 
network," 
deep 
Vehicular  Communications,  vol.  21,  p. 
100198, 2020. 
O. Avatefipour et al., "An intelligent secured 
framework  for  cyberattack  detection 
in 
electric  vehicles’  CAN  bus  using  machine 
learning," IEEE Access, vol. 7, pp. 127580-
127592, 2019. 
A.  Alshammari,  M.  A.  Zohdy,  D.  Debnath, 
and  G.  Corser,  "Classification  approach  for 
intrusion  detection  in  vehicle  systems," 
Wireless Engineering and Technology, vol. 9, 
no. 4, pp. 79-94, 2018. 
 E. Seo, H. M. Song, and H. K. Kim, "Gids: 
Gan based intrusion detection system for in-
vehicle  network,"  in  2018  16th  Annual 
Conference  on  Privacy,  Security  and  Trust 
(PST), 2018: IEEE, pp. 1-6.  
K.  Grosse,  P.  Manoharan,  N.  Papernot,  M. 
Backes, and P. McDaniel, "On the (statistical) 
detection  of  adversarial  examples,"  arXiv 
preprint arXiv:1702.06280, 2017. 

[14] 

[15] 

[16] 

18 

 
 
 
 
 
[17] 

[18] 

[19] 

[20] 

[21] 

[22] 

[23] 

[24] 

in 

A. Qayyum, M. Usama, J. Qadir, and A. Al-
Fuqaha, "Securing connected & autonomous 
vehicles:  Challenges  posed  by  adversarial 
machine  learning  and  the  way  forward," 
IEEE Communications Surveys & Tutorials, 
vol. 22, no. 2, pp. 998-1026, 2020. 
A.  Kurakin,  I.  Goodfellow,  and  S.  Bengio, 
the  physical 
"Adversarial  examples 
world," ed, 2016. 
A.  Singla  and  E.  Bertino,  "How  deep 
learning is making information security more 
intelligent,"  IEEE  Security  &  Privacy,  vol. 
17, no. 3, pp. 56-65, 2019. 
N. Papernot, P. McDaniel, and I. Goodfellow, 
"Transferability  in  machine  learning:  from 
to  black-box  attacks  using 
phenomena 
preprint 
arXiv 
samples," 
adversarial 
arXiv:1605.07277, 2016. 
 N. Papernot, P. McDaniel, I. Goodfellow, S. 
Jha,  Z.  B.  Celik,  and  A.  Swami,  "Practical 
black-box attacks against machine learning," 
in  Proceedings  of  the  2017  ACM  on  Asia 
conference 
and 
communications security, 2017, pp. 506-519.  
 Y. Li, J. Lin, and K. Xiong, "An Adversarial 
Attack  Defending  System  for  Securing  In-
Vehicle  Networks,"  in  2021  IEEE  18th 
Annual  Consumer  Communications  & 
Networking  Conference  (CCNC),  2021: 
IEEE, pp. 1-6.  
 K.  Yang,  J.  Liu,  C.  Zhang,  and  Y.  Fang, 
"Adversarial  examples  against  the  deep 
learning  based  network  intrusion  detection 
systems,"  in  MILCOM  2018-2018  IEEE 
Conference 
Communications 
Military 
(MILCOM), 2018: IEEE, pp. 559-564.  
R. Islam, M. K. Devnath, M. D. Samad, and 
S.  M.  J.  Al  Kadry,  "GGNB:  Graph-based 
Gaussian  naive  Bayes  intrusion  detection 
Vehicular 
system 
Communications, vol. 33, p. 100442, 2022. 

for  CAN 

computer 

bus," 

on 

[26] 

neural 

network," 

[25]  M. Han, P. Cheng, and S. Ma, "PPM-InVIDS: 
Privacy  protection  model  for  in-vehicle 
intrusion  detection  system  based  complex-
Vehicular 
valued 
Communications, vol. 31, p. 100374, 2021. 
S. Jeong, B. Jeon, B. Chung, and H. K. Kim, 
"Convolutional 
network-based 
neural 
intrusion detection system for AVTP streams 
in  automotive  Ethernet-based  networks," 
Vehicular  Communications,  vol.  29,  p. 
100338, 2021. 
 C.  Jichici,  B.  Groza,  and  P.-S.  Murvay, 
"Integrating Adversary Models and Intrusion 
Detection  Systems  for  In-vehicle  Networks 
in CANoe," in SECITC, 2019, pp. 241-256.  
 P. Sharma, D. Austin, and H. Liu, "Attacks 
on  machine  learning:  Adversarial  examples 
in  connected  and  autonomous  vehicles,"  in 
2019  IEEE  International  Symposium  on 
Technologies for Homeland Security (HST), 
2019: IEEE, pp. 1-7.  

[28] 

[27] 

[29] 

[30] 

[31] 

[32] 

[33] 

[34] 

[35] 

[36] 

[37] 

[38] 

of  Machine  Learning 

 N. Dalvi, P. Domingos, S. Sanghai, and D. 
Verma,  "Adversarial  classification," 
in 
Proceedings  of  the  tenth  ACM  SIGKDD 
international  conference  on  Knowledge 
discovery and data mining, 2004, pp. 99-108.  
 D.  Lowd  and  C.  Meek,  "Adversarial 
learning,"  in  Proceedings  of  the  eleventh 
ACM  SIGKDD  international  conference  on 
Knowledge discovery in data mining, 2005, 
pp. 641-647.  
B. Biggio, G. Fumera, and F. Roli, "Multiple 
classifier systems for robust classifier design 
in  adversarial  environments,"  International 
Journal 
and 
Cybernetics, vol. 1, no. 1-4, pp. 27-41, 2010. 
X.  Yuan,  P.  He,  Q.  Zhu,  and  X.  Li, 
"Adversarial examples: Attacks and defenses 
for  deep  learning,"  IEEE  transactions  on 
neural  networks  and  learning  systems,  vol. 
30, no. 9, pp. 2805-2824, 2019. 
F. Cartella, O. Anunciacao, Y. Funabiki, D. 
Yamaguchi,  T.  Akishita,  and  O.  Elshocht, 
"Adversarial  Attacks  for  Tabular  Data: 
Application 
and 
Imbalanced 
preprint 
arXiv:2101.08030, 2021. 
Q.  Yang,  Y.  Liu,  T.  Chen,  and  Y.  Tong, 
"Federated  machine  learning:  Concept  and 
applications,"  ACM 
on 
Intelligent  Systems  and  Technology  (TIST), 
vol. 10, no. 2, pp. 1-19, 2019. 
P.  K.  Sharma  and  J.  H.  Park,  "Blockchain 
based  hybrid  network  architecture  for  the 
smart  city,"  Future  Generation  Computer 
Systems, vol. 86, pp. 650-655, 2018. 
J.  Laufs,  H.  Borrion,  and  B.  Bradford, 
"Security  and  the  smart  city:  A  systematic 
review," Sustainable cities and society, vol. 
55, p. 102023, 2020. 
H. Kim, J. Park, M. Bennis, and S.-L. Kim, 
"Blockchained 
federated 
learning,"  IEEE  Communications  Letters, 
vol. 24, no. 6, pp. 1279-1283, 2019. 
R.  S.  Pressman,  Software  engineering:  a 
practitioner's approach. Palgrave macmillan, 
2005. 

to  Fraud  Detection 
Data," 

Transactions 

on-device 

arXiv 

in 

[40] 

emulated 

[39]  M.  C.  Feliciano,  "Analysis  of  blockchain 
technologies and benchmarking of NXT and 
network 
Ethereum 
environment,"  2020.  [Online].  Available: 
https://github.com/Shotokhan/blockchain-
benchmarking-on-mininet 
A.  Chakraborty,  M.  Alam,  V.  Dey,  A. 
Chattopadhyay,  and  D.  Mukhopadhyay, 
"Adversarial  attacks  and  defences:  A 
survey,"  arXiv  preprint  arXiv:1810.00069, 
2018. 
I. J. Goodfellow, J. Shlens, and C. Szegedy, 
"Explaining  and  harnessing  adversarial 
examples," arXiv preprint arXiv:1412.6572, 
2014. 

[41] 

19 

 
 
 
[42] 

[43] 

in 

2016 

 N.  Papernot,  P.  McDaniel,  S.  Jha,  M. 
Fredrikson, Z. B. Celik, and A. Swami, "The 
limitations  of  deep  learning  in  adversarial 
IEEE  European 
settings," 
symposium  on 
security  and  privacy 
(EuroS&P), 2016: IEEE, pp. 372-387.  
G.  J.  Székely  and  M.  L.  Rizzo,  "Energy 
statistics:  A  class  of  statistics  based  on 
distances,"  Journal  of  statistical  planning 
and inference, vol. 143, no. 8, pp. 1249-1272, 
2013. 

20 

 
 
 
 
 
