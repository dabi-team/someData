IEEE JOURNAL SUBMISSION

1

A Marketplace for Trading AI Models based on
Blockchain and Incentives for IoT Data

Lam Duc Nguyen, IEEE Graduate Member, Shashi Raj Pandey, IEEE Member, Soret Beatriz, IEEE Member,
Arne Br¨oring, and Petar Popovski, IEEE Fellow

1
2
0
2

c
e
D
6

]

G
L
.
s
c
[

1
v
0
7
8
2
0
.
2
1
1
2
:
v
i
X
r
a

Abstract—As Machine Learning (ML) models are becoming
increasingly complex, one of the central challenges is their deploy-
ment at scale, such that companies and organizations can create
value through Artiﬁcial Intelligence (AI). An emerging paradigm
in ML is a federated approach where the learning model is
delivered to a group of heterogeneous agents partially, allowing
agents to train the model locally with their own data. However,
the problem of valuation of models, as well the questions of
incentives for collaborative training and trading of data/models,
have received a limited treatment in the literature. In this paper,
a new ecosystem of ML model trading over a trusted Blockchain-
based network is proposed. The buyer can acquire the model of
interest from the ML market, and interested sellers spend local
computations on their data to enhance that model’s quality. In
doing so, the proportional relation between the local data and
the quality of trained models is considered, and the valuations
of seller’s data in training the models are estimated through the
distributed Data Shapley Value (DSV). At the same time, the
trustworthiness of the entire trading process is provided by the
Distributed Ledger Technology (DLT). Extensive experimental
evaluation of the proposed approach shows a competitive run-
time performance, with a 15% drop in the cost of execution, and
fairness in terms of incentives for the participants.

Index Terms—Blockchain, Federated Learning, Model Trad-

ing, Data Valuation, Shapley Value.

I. INTRODUCTION

Personal IoT devices keep generating an enormous amount
of sensing data that is expected to reach 79.4 Zettabytes (ZB)
globally in 2025 [1]. Several attempts to enhance and adapt
business workﬂows have been made towards exploiting the
provision of IoT data [2], [3]. In this regard, training machine
learning models and data sharing are two popular uses of IoT
data. Furthermore, emerging diverse platforms for accessing
and sharing IoT data connects various distributed IoT devices/-
data sources, thereby facilitating suppliers to exchange their
data [4].

For example, in IoT systems for air quality monitoring
and emission control, Air Quality Index (AQI) is a quantity
deﬁned to estimate the degree of severity for air pollution
and CO2 emission levels. AQI quantiﬁes the concentration of
various particles in the air, such as PM2.5 or PM5.0, using
state-of-the-art sensor devices [5]. There are two most popular
measurement methods for AQI: i) sensing-based [6], and ii)
vision-based [7]. In the sensing-based method, the IoT sensor

Lam Duc Nguyen, Shashi Raj Pandey, Soret Beatriz, and Petar Popovski are
with Connectivity Section, Electronic System, Aalborg University, Denmark.
Email: {ndl, srp, bsa, petarp}@es.aau.dk.

Arne Br¨oring is Senior Key Expert Research Scientist with Siemens AG,

Munich, Germany. Email: arne.broering@siemens.com

Figure 1: A motivation example: IoT devices contribute to train
an ML model for the buyer to predict CO2 emission levels and
get incentives from their contributions.

devices are delivered around the area interest, e.g., city, urban,
to collect the quality of the air and emission levels. These
measurements are then forwarded to the central server for
further analysis and calculation of AQI. In the vision-based
method, the devices with an embedded camera, such as a
camera station in the road, or individual mobile phones, can
take photos of a speciﬁc area and send them to the server.
The server then applies advanced image processing techniques
on these images to derive the analysis report of air quality.
However, both methods have problems due to (i) high energy
consumption for collecting data and transmission, (ii) require-
ment for a large dataset for a high quality AQI estimation,
(iii) the server acting as a single point of failure, and (iv)
data privacy concerns under General Protection Regulation
(GDPR). Several recent works have addressed issues related
to (i) using efﬁcient resource management techniques and
(ii) with dense sensory networks [8]. However, the primary
concerns about (iii) and (iv) remain a single point of failure
network topology and data privacy protection. They are yet to
be addressed in an efﬁcient manner. In addition, the regulations
such as EU’s GDPR, California’s Consumer Privacy Act
(CCPA), and China’s Cyber Security Law (CSL) [9] limits the
reckless use/collection of personal information and fosters data
privacy. Hence, a feasible joint solution to address challenges
raised in (iii) and (iv) is imperative for optimized operation of
the market for data exchange.

In this regard, more recently, Federated Learning (FL) has
been considered a key solution to address the privacy issue

Machine LearningBuyerModelsSellerSellerSellerPaymentModels 
 
 
 
 
 
IEEE JOURNAL SUBMISSION

2

in training learning models [10]. FL is a distributed model
training paradigm that aims to solve the challenges of data
governance and privacy by training algorithms collaboratively
rather than transferring the data itself. For example, in a typical
FL setting, at ﬁrst, as shown in Fig. 1, the IoT devices collect
the pollution and CO2 emission levels and store them in
their local database. Consider an interested organization or
individual, termed a buyer, willing to train an ML model to
predict a speciﬁc area’s emission level. However, they do not
possess sufﬁciently large datasets about sensing information or
image data. In such a scenario, they can send their initial model
to a model marketplace to ﬁnd appropriate parties interested
in contributing to the model training process. Then, the IoT
devices, termed sellers, can download the initial model and
train it using their local data. After that, the IoT devices can
share the updated model weights to the marketplace, where
there is an aggregator to aggregate submitted local models to
build global models. Based on the aggregated global models,
the model buyer can use the global model to predict the emis-
sion levels with acceptable precision. In this manner, using
the FL approach, we can address the problem of data privacy
where data is locally trained without the need to transmit to a
central server. However, from a systems perspective, a shared
IT environment, such as an aggregator in the marketplace, may
become a single point of failure in terms of data integrity, trust,
security, and transparency [11].

A conventional data market is often deployed as a central-
ized service platform that gathers and sells raw or processed
data from data owners (e.g.,
the trained learning models)
to the consumers [12] [13]. This leads to two important
concerns. First, this strategy exposes the platform as a single
point of security risk; the malfunctioning platform servers has
serious security concerns including data leakage, inaccurate
calculations results, and manipulation of data price. Second,
collaboration for model training raises questions in terms of
how to motivate participants to participate in such an ML
training endeavor. The incentive for each IoT client based
on their contribution should be fair and transparent. These
features are not present in a standard FL setup, as in many
applications there is no clear and natural incentive mechanism
for involved participants to provide quality information. This
calls for a carefully designed mechanisms to reward parties
economically and thus incentivize participation [14], [15].
For example, a ﬁxed price per data point could motivate
participants to collect massive amounts of low quality or fake
data if there is no intermediary process to check quality of
training data. Besides, another reason that may disincentives
parties from sharing data could stem from privacy and integrity
concerns regarding the use of participant’s data once it is
shared. For instance, the sellers can re-use the data which has
already been sold.

The aforementioned challenges can be effectively handled
by a Distributed Ledger Technologies (DLTs).1 DLTs and
Blockchains enable untrusted parties to share information in

1In this work, the terms Blockchain and DLT are used interchangeably.
Blockchains are a type of DLT, where nodes maintain a copy of the ledger
having embedded chains of blocks. These blocks are basically composed of
digital pieces of information, particularly deﬁned as transactions.

an immutable and transparent manner [16]. Outside of its key
role in ﬁnancial transactions, the applications of DLTs can
be seen as a key enabler for trusted and reliable distributed
IoT systems, e.g., a distributed IoT data marketplace For
instance, in a Blockchain-enabled IoT data marketplace [17],
Blockchain transactions include IoT sensing data, or system
control messages, and these are recorded and synchronized
in a distributed manner in all the involved participants of the
network [18]. Furthermore, DLTs enable the preservation of
all transactions in immutable records, with each record being
the decentralized
spread across several participants. Thus,
nature of DLTs ensures security, as does the use of robust
public-key encryption and cryptographic hashes. The advan-
tages of incorporating DLTs into trading AI models in IoT
systems include: i) ensuring immutability and transparency for
historical AI model trading records, ii) eliminating the need for
third parties, and iii) developing a transparent system for AI
model trading in heterogeneous networks to prevent tampering
and injection of fake data from the stakeholders, according to
[19], [20]. With the wide spread of ubiquitous marketplaces
recently, it became relevant to investigate the use of AI/ML
model trading in marketplace environments.

With the

the participants2 can get

aforementioned motivation, we propose

a
Blockchain-based model trading system which enables a se-
cured and trusted marketplace to collaboratively train ML
models as well as guarantees fair incentives for every par-
ticipants and privacy of data. Based on the quality of the
uploaded models, which is quantiﬁed by using a distributed
Data Shapley Value (DSV),
the
incentive based on the updated models, for example, as tokens
or ﬁats. Note that based on our proposed system, the parties do
not need to share their local data, but only provide customized
models or query interface to the marketplace. Consequently,
the proposed system allows multiple participants to jointly
train the ML models on the marketplace based on their own
training data. Buyers who need to train their ML model will
pay to the market for the improvement of their model, and
sellers who sell their contribution to train the ML models will
get paid by the market via smart contracts.

The main features of the proposed model trading are:

1) Trusted and transparent transactions: The DLT is
considered a trusted, tamper-proof, and transparent sys-
tem in which the participants can check and follow the
progress of a training task. Based on that, the model is
exchanged and traded securely and transparently.

2) Valuation of Data: The local models contributed by the
trainers (service sellers) are collected and evaluated via
Shapley Value (SV) extension to approximately estimate
the quality of the models.

3) Fair Payment: The participants receive their reward,
which is proportional to the usefulness of their data
in improving the models. The distributed incentive
mechanism for FL based on SV measures participants’
contributions in the marketplace.

2The terms “participants”,“clients” and “agents” in this work are used to

refer to “IoT devices”.

IEEE JOURNAL SUBMISSION

3

A. Contributions and Paper Organization

The major contributions are summarized as follows:
• ML Model Marketplace: A Blockchain-based model
trading system that allows participants to purchase learn-
ing models and sell contributions in training them. The
system records the trading details [17] in a tamper-proof
distributed ledger.

• Federated Data Shapley Value (SV): We use the
data SV to estimate the valuation of participants’ data
and evaluate their contribution to the model during local
training. We show that the standard SV value is inefﬁcient
for distributed ML and deploy an extension of standard
SV for our platform. The method is robust and allows
plugging any developed mapping functions related to
the device’s local data into the proposed distributed
Shapley mechanism for value quantiﬁcation. As a result,
one can design a contribution-based, efﬁcient incentive
mechanism to stimulate model trading.

• Performance Evaluation: We have conducted extensive
simulations and experiments, demonstrating that the pro-
posed approach shows a competitive run-time perfor-
mance, with a 15% drop in the cost of execution and
fairness in terms of incentives for the participants.

The rest of the article is organized as follows. Section II,
presents the concepts of DLTs, FL, as well as deﬁnition of
data valuation schemes used in this paper. This is followed by
description of the system model of the marketplace for ML
model trading and explain in detail how the system works.
In Section III, the value of ML models is calculated using
the Approximate Federated Shapley Value (AFS). Section IV
contains description of the testbed and experimental results.
Section V discusses related works and ﬁnally, Section VI
concludes the paper.

II. PRELIMINARIES

A. Standard FL

FL is a distributed machine learning setting in which nu-
merous entities (clients) cooperate on training a learning model
without disclosing their available raw data [10]. Instead, clients
distributively perform computations on their data and transfer
obtained local learning parameters updates to the server for
aggregation process. The aggregated model, i.e., the global
model, is broadcast back to the clients for the next round of
local computations resulting in the local learning parameters.
The interaction between the server and clients to solve the
learning problem continues until an acceptable level of model
accuracy is achieved [21]. In this manner, FL offers (i) privacy-
preserving beneﬁts in the model training approach by not
requiring clients to share their local data to the server, and
consequently, (ii) lower communication overhead by offering
distributed model training paradigm and exchange of model
parameters only. Therein, FL enables training AI/ML models
at edge networks
Fundamentally,

there exists two main actors in the FL
system: (i) the data owners, often termed as participants, and
(ii) the model owner, which is the FL server. Consider a set of
N data owners, deﬁned as N = {1, 2, . . . , N }, where each of

Figure 2: Standard FL.

Table I: Summary of key notations.

Notation Meaning
M Ii
N
Di
Mi
MG
M0
G
L(Mt
i)
η
Si
Bi
Pd
B
A
U (·)
φi
E
T
Ti
(cid:102)M

DLT miner i
Set of N participants (clients)
Private local dataset of user i ∈ N
Local model of user i ∈ N
Global model aggregated at DLTs
Initial Global model at DLTs
Loss function
Learning rate
Model trainer (or Seller) i
Model owner (or Buyer) i
Deposit from buyer
Training batch size
Training algorithm
Utility function
Valuation of data contributor i
Number of local epochs for a FL setting
Number of training interactions
A trade deal between Si and Bi
Approximated model (cid:102)M

them has a private dataset Di∈N . In Table I, we provide the
summary of key notations used in this paper. Each data owner
i trains a local model Mi using its dataset Di and sends only
the obtained local model parameters to the FL server. Then,
the FL server aggregates all the collected local models to build
a global model, MG = (cid:80)
i∈N Mi. This is where, in princi-
ple, the FL approach differs from the traditional centralized
training where D = ∪i∈N Di is used to train a model MT ,
i.e., data ﬁrst gets aggregated centrally before the actual model
training happens. In Fig. 2, we show a standard architecture
and an overview mechanism of the FL training process. We
assume that the data owners are honest, i.e., actual private data
will be used for the local training, and correspondingly, the
FL server will receive accurate local models from the data
owners. Following to which, the workﬂow of standard FL can
be described as below.

First, considering the target application,

the server de-

......AggregatedModelAggregatorBase stationBase stationBase stationTraning NodesLocal DataLocal DataLocal DataServerLocalTrainingLocalTrainingLocalTrainingIEEE JOURNAL SUBMISSION

4

Algorithm 1: Federated Averaging (FedAvg) Algo-
rithm
Input: Local mini-batch size B, number of participants

per interaction m, number of global
interactions T , number of local epochs E, and
learning rate η
1 . Output: Global Model MG
2 . LocalTraining(i, M): Split local dataset Di to

mini-batches of size B which are included into the set
B.

3 for each local epoch j from 1 to E do
4

for each b ∈ B do

5

M ← M − η∇L(M; b);

6 Return M to the server.
7 Initialized M0
G.
8 for each interaction t = {0, 1, 2, 3, . . . , T − 1} do
9

St ← (random set of m clients);
for each participant i ∈ St in parallel do
t+1 ← LocalTraining(i, Mt
(cid:80)N

Mi
12 Mt+1

i=1 DiMt+1

G =

G);

(cid:80)

11

10

;

i

1
i∈N Di

cides the training task and deﬁnes the corresponding data
requirements. Furthermore, the server also speciﬁes the hyper-
parameters of the global model and the training process, e.g.,
the learning rate η. The server then broadcasts the initialized
global model M0
G and the learning task to a subset of selected
participants. Next, based on the global model Mt
G, where t
denotes the current global iteration index, i.e., the commu-
nication rounds between the participants and the server, each
participant uses its local data to update their model parameters
M t
i . In doing so, during iteration t, the participant i ∈ N aims
at ﬁnding the optimal parameters Mt
i that minimize the local
loss problem L(M t
i ), deﬁned as the ﬁnite-sum of empirical
risk functions as

B. Distributed Ledger as a Service for FL

DLT is a peer-to-peer distributed ledger that records transac-
tions in a network in a transparent and immutable manner. Be-
sides, smart contract, which is considered as a key innovation
in DLT/Blockchain area, provide programmability contracts to
the DLTs, in the sense that the deﬁned agreements in contracts
are executed autonomously. With the mentioned nature advan-
tages of DLTs and smart contract, the FL framework running
on the top of DLT should be completely distributed and avoid
the single point of failure issue.

In the DLT-based FL, we assume each client device is
always connected to one of the DLT miners and, if the physical
connection with the current DLT miner becomes unavailable,
then the device will be automatically associated with another
DLT miner. In each miner-device pair, the DLT miner works
as the leader of the associated IoT devices, and they are
responsible for uploading and downloading data or training
models. During the training process, the IoT device downloads
the latest global model recorded in the ledger and trains for
the updated version of the local model using their private local
data. After completing the local training, the device uploads
the local model
to the paired DLT miner and the global
aggregation process starts. In the training time, all involved
IoT devices are allow to download the latest information of
associated DLT miners to receive the evaluation of the IoT
devices and global model updates. Finally, each IoT device
publishes its local training model and enters to a new round of
local iteration using the newest version of the obtained global
model. In this manner, the iterative ML model training process
is operated until the global model has achieved a satisfactory
accuracy or convergence.

Each miner has its veriﬁer and block to ensure that the real
models and the contributions of devices are updated. Each

Mt

i = arg min

M t
i

L(M t

i ).

(1)

We formally call it local iteration. Subsequently, the obtained
local model parameters from participants are sent back to the
server, where they are aggregated to get the global model pa-
rameters M t+1
G . Eventually, the global model is then broadcast
back to the data owners for the next round of local iteration;
hence, the iterative process is continued. In doing so, the server
minimizes the global loss function L(M t
G) as the following
approximation in the distributed setting of FL:

L (cid:0)Mt

(cid:1) =

G

(cid:88)N

L (cid:0)Mt

(cid:1) .

i

1
N

i=1
Note that
the FL process can train different ML models
that essentially use the SGD method such as Support Vector
Machines (SVMs), neural networks, and linear regression.

(4)

However, a single server dependency in the traditional FL
framework makes the system vulnerable to threats, such as
when the server behaves maliciously. Therefore, integrating
FL with DLTs should be a promising approach to address
limitations [22].

Figure 3: The accuracy of Standard FL and DLT-based FL.

020406080100Epochs0.900.920.940.960.981.00AccuracyFL-MNISTFL-MNIST E=1FL-MNIST E=3FL-MNIST E=10020406080100Epochs0.20.30.40.50.60.70.8AccuracyFL-CIFAR10FL-MNIST E=1FL-MNIST E=3FL-MNIST E=10020406080100Epochs0.900.920.940.960.981.00AccuracyDLT-FL-MNISTFL-MNIST E=1FL-MNIST E=3FL-MNIST E=10020406080100Epochs0.00.10.20.30.40.50.6AccuracyDLT-FL-CIFAR10FL-MNIST E=1FL-MNIST E=3FL-MNIST E=10IEEE JOURNAL SUBMISSION

5

block contains a head and body parts. The blockhead contains
a pointer to the next block, and the body part contains a
set of validated transaction information. The local models are
formed in transaction format and in order to make the solution
scalable, the local models are recorded in IFPS storage, such
that just a hash version of the models is recorded in the
distributed ledger. The basic comparison between standard
FL and DLT-based FL is presented in Fig.3. The accuracy
is similar in both standard FL and DLT-based FL, but the
time required for convergence of DLT-based FL is higher than
standard FL because of extra veriﬁcation and consensus in the
system.

C. Data valuation using Shapley value

Game theory is an economic tool best-suited to analyze
a system where two or more participants get involved in to
achieve a desired payoff. The Shapley Value (SV) is a solution
concept of fairly distributing the incentive and payoff for
the involved parties in coalition [23]. In this regard, the SV
applies mainly in scenarios where the contributions of each
involved participant are unequal, but all the participants work
in cooperation with each other to achieve the payoff. The SV
of user i is deﬁned as the average marginal contribution of i
to all possible subsets of D = {D1, D2, . . . , DN } formed by
other users as

φi(N, U ) =

1
N !

(cid:88)

S⊆N \{i}

U (MS∪{i}) − U (MS )
(cid:0)N −1
|S|

(cid:1)

,

(2)

where the function U (·) gives the value for any subset of
those users, e.g., let S be a subset of N , then U (S) gives
the value of that subset. This captures the average value
of the contributions of user i for subsets of all coalition
of users. Intuitively, assume that
the user’s data is to be
collected in a random order, and that every user i receives
its marginal contribution for the collected data. If we average
these contributions over all the possible orders of N users,
we obtain φi(N, U ). The importance of the SV is that it is
the unique value division scheme that satisﬁes the following
desirable properties described as follows.

• Symmetry: For all S ⊆ N \ {i, j}, if user i and j are
interchangeable, and U (S ∪ {i}) = U (S ∪ {j}), then,
φi = φj. Thus, the users i and j contribute the same
amount to every coalition of the other agents. Besides,
the symmetry axiom states that such agents should receive
the same payments.

• Dummy User: User i is considered as dummy user if
the amount that i contributes to coalition is exactly the
amount that i is able to achieve alone, i.e., ∀S, i /∈
S, U (S∪{i})−U (S) = U ({i}). According to the dummy
user axiom, dummy users should be compensated exactly
for the amount they achieve on their own. Users that make
zero marginal contributions to all subsets of the data set,
on the other hand, earn no compensation, for example,
φi = 0 if U (S ∪ {i}) = 0, ∀S ⊆ N \ {i}.

• Additivity: For any two U1 and U2, we have for any user
i, φi(N, U1 + U2) = φi(N, U1) + φi(N, U2), where the

game (N, U1+U2) is deﬁned by (U1+U2)(S) = U1(S)+
U2(S) for every coalition S.

Based on these background knowledge, we designed a
distributed marketplace for trading AI/ML models based on
Blockchain and incentive mechanism in IoT environment.

III. SYSTEM DESIGN AND ANALYSIS

A. System Components

The general architecture of DLT-based model trading in-
cludes three main components: model owners or buyers B,
model trainers or sellers S, and a distributed ledger, shown
in Fig. 4. We assume that each seller or buyer owns one
device in the network. Within a deal (a trade) by Ti, the seller
Si ∈ S and buyer Bi ∈ B communicate using wireless links.
The AI/ML model trading procedure occurs to complete a
trade between Si and Bi, exchanging models Mi ∈ M and
payment Pi. First and foremost, Bi completes the deposit Pd
to Si via smart contracts in reference to the requested training
models, Mi. After the sellers complete the requests of the
buyers, in terms of accuracy, convergence time, etc, the smart
contracts are autonomous executed to pay for the effort of
sellers using the amount of deposit Pd from buyers. Following
Fig. 4, the general procedure of interaction between a single
buyer Bi and a single seller Si can be described as follows:
1) Model Owner i (as buyer Bi): Bi could be an individual
or organization who needs model training. Bi sends a request
bi including task type, budget, deposit, amount of data, quality
of data, price, discount, etc, to the marketplace via smart
contracts. bi will be transmitted to selected Si and recorded
in the ledger via transaction Ti,add. After receiving the trained
and aggregated models from Si and marketplace which fulﬁlls
requirements regarding to, e.g., accuracy, the Bi generates
a transaction Ti,commit which executes payment from Bi’s
wallet to smart contract, forwards the payment to sellers, and
generates a acknowledgment message back to the distributed
ledger.

2) Model Trainer (as seller Si): The model trainers play
two main roles in the system: i) collects sensing data from
the environment (e.g., data from surveillance systems, envi-
ronmental sensing data, and geographical data), or acts as
a data hub gathering data from nearby physical devices; ii)
subscribes the model training requests from the buyers, and
train the models downloaded from the marketplace with the
local data. Seller Si earns the payment Pi from Bi after
successful delivery of Mi to Bi. After the trained models
achieve a certain accuracy based on the predeﬁned agreements
in the smart contract system, upon the appearance of Ti,commit
generated by Bi, the seller Si can receive the payment, e.g.,
via tokens, Pi, which is in fact the deposited amount Pd by the
buyer Bi, from the marketplace via smart contract. Finally, it
conﬁrms to the distributed ledger that the deal Ti is completed
via an acknowledgment message.

3) Distributed Ledgers: The Blockchain maintains a dis-
tributed ledger that stores the history of all traded models in
the form of blocks, which are connected in a chronological
order. On top of that, the smart contracts are deployed to
autonomously control the order and execute payments, e.g,.

IEEE JOURNAL SUBMISSION

6

Figure 4: DLT-based ML Model Trading Framework Architecture.

large payment or micro-payments from involved participants
the need of human intervention. In a distributed
without
manner, the smart contracts ensure transparency, trust and
automotive of exchanging data among parties. These features
can be deployed based on the negotiation between model
owners and customers via Ti,deploy. Furthermore, any change
in smart contracts, for example, the amount of data or the
model price, or updates in the discount offers, can be made
via Ti,update.

In a trading system, there are an enormous amount of data
exchanged among parties. Thus, increasing the number of
transactions leads to slower transaction processing time and,
consequently, the system’s overall speed. This is reasonable
as every Blockchain node needs to store and execute a com-
putational task to validate every single transaction. Therefore,
to minimize the cost of storage and execution, the trading
system should record only the important data, such as payment
history, aggregated global models, which could be hashed and
recorded at the distributed ledger. Meanwhile, the raw data can
be recorded in the distributed off-line storage component. In
detail, after both model sellers Si and model owners Bi have
fulﬁlled requirements deﬁned by smart contracts, the Ti,settle
is autonomously executed to query the payment Pi from Bi.
Then, the payment Pi is transferred to Si’s private wallet,
while the aggregated model Mi is delivered to the storage ad-
dress of Bi. In the scope of this study, we assume that the data
services (e.g., data storage, trading and task dispatching) are
implemented on top of a permissionless Ethereum Blockchain
[24]. In this work, the control data and AI/ML models are
formatted into normal Ethereum transactions. Furthermore, in
order to improve efﬁciency, only the digest of each transaction

is recorded in the distribute on-chain ledger, and the raw data
is stored off-chain by using IPFS (InterPlanetary File System).

B. Communication Workﬂow

In the DLT-based FL model trading network, we revisit the
notation used to denote data owners and deﬁne the number
of participants as N = {1, 2, 3, . . . , N }. The miner M Ii of
DLT network is associated randomly with the IoT device.
For simplicity, we consider the case which one miner is
assigned to each physical IoT device. Each IoT device has
to determine its own learning task-related dataset size and
upload it to the ledger system to receive a reward. A distributed
SV incorporates the quality of local data3 to determine the
corresponding quality of the local model. We realize that,
in some case e.g healthcare, it would be better to extract
features and measuring the quality based on real quality and
not quantity, but it is out of this research scope. In addition,
in resource-constraint IoT environments, to reduce latency and
optimize energy consumption, each device’s local controller
performs local optimizations to establish the best scheduling
policy for device resources, such as CPU cycles scheduling.

The workﬂow of the system is described as below.
Step 1 (Model Initialization): The buyer Bi initiates a model
Mi which needs to be trained and publishes to the DLT-
based marketplace. The initial model is formed in the DLT
transaction format Tpublish.

Step 2 (Publish initialized model to the ledger): Then, the
transaction Tpublish including initialized model Mi is veriﬁed

3The quality of data signiﬁes the size of dataset used in model training,
similar to [25]. In this study, we do not consider feature attributes of data to
quantify its quality.

......Base stationBase stationBase stationTraning NodesLocal DataLocal DataLocal DataLocalTrainingLocalTrainingLocalTraining...DLT NetworkSellerSellerSellerBuyer1SyncSync23456VersionState TXTXTXHash of Prev BlockNonceMerkleRootTargetTX...879...GlobalUpdatesLocalUpdatesPayment ChannelsIEEE JOURNAL SUBMISSION

7

and recorded to the distributed ledger. At the same time, there
might be many available models on the marketplace.

Step 3 (Model Seller download train-required models): The
potential seller Si can see the list of available models on the
marketplace and choose to download a copy of one or multiple
models of interest to train with its local data of Si.

Step 4 (Local Model Training): After downloading the
models from the distributed ledger, the sellers train the model
based on their local data. The device, i.e., the seller Si, has its
own local dataset Di. Local model training aims to minimize
the loss function f (Mi, Si), where Mi is the local model of
device di and Di is its local dataset.

Step 5 (Local trained model is updated to the ledger): Next,
the device Si is randomly associated with the miner M Ii to
which it uploads the trained local model to the distributed
ledger via smart contract. The smart contract has functions
to record the updated local models from clients via DLT
interface, e.g., Web3.

Step 6 (Cross-veriﬁcation of the local models): After re-
ceiving the local model published by IoT device in the format
of transactions, the DLT miner M Ii put the local model in
newly generated blocks and broadcasts the model to other DLT
miners in the network. Next, until other DLT miners receive
the broadcasted blocks, including the local models of clients,
they will verify the accuracy of local models and put the
models to the new generated blocks. During this process, all
the aggregated models are broadcasted to the all DLT miners
in the system, and DLT miners will compare the consistency
and accuracy among aggregated models. To that end, the most
one will be chosen as the correct global model. Then, DLTs
miners record the correct global model and the contribution of
the IoT devices into the distributed ledger via smart contracts
features. Otherwise, the rest of global models are considered
as faulty updates.

Step 7 (The generation and propagation of blocks): In
order to generate a new block in the distributed ledger, DLT
miners need to compute a block hash for mining and solve a
cryptographic puzzle based on SHA-256, which is a one-way
hash function. As deﬁned in popular Proof-of-Work (PoW)
Blockchain, e.g., Bitcoin, Ethereum, DLT miners perform a
PoW algorithm until it ﬁnds a desired nonce value or receives
a new generated block from other DLT miners [26]. There
is a case, however, that the M Ii ∈ M I acts as the DLT
miner that ﬁnds the needed nonce value at the earliest, and its
candidate block is generated as a new block and propagated
to the other DLT miners in the network. Meanwhile, the chain
can be engaged in forked problem in which multiple DLT
miners ﬁnd out a nonce value at the same moment. To address
this issue, we use an ACK message that allows DLT miners
to transmit only when each DLT miner gets the new block,
which determines whether there is a fork on the main chain.
Then, the DLT miner M I0, which creates that newly generated
block, will wait for a waiting time deﬁned by the block ACK.
Otherwise, if a fork is generated again, the process back to to
previous phase to resolve the issue.

Step 8 (Settlement): After the model accuracy achieves a
particular value in the smart contract, the smart contract settles
the deal between buyer and sellers. The ﬁnalized model is

Algorithm 2: Standard Federated Shapley FedAvg
Input: Local mini-batch size B, number of

participants m per interaction, T is number of
global interactions, number of local epochs E,
and learning rate η.

Output: : MT
1 . Initialize M0
2 for each subset S ⊆ N do
3

S, where S ⊆ N = {1, 2, . . . , n}.

for each round t = {0, 1, 2, . . . , T − 1} do

4

5

6

7

8

9

10

for each client i ∈ S = {1, 2, 3, . . . , n} do

Transmit Mt to n clients;
Mt
δ(t+1)
S,i ← Mt

i ← M odelU pdate(i, Mt);
S,i − Mt;

S ← Mt

S + (cid:80)n
Mt+1
for i ∈ S = {1, 2, 3, . . . , n} do

|Di|

i=1

(cid:80)n

i=1 |Di| · δt+1
S,i ;

φi = 1
n

(cid:80)

S⊆N \{i}

U (Mt

S∪{i})−U (Mt
(n−1
|S| )

S )

;

11 Return MT , and φ1, φ2, . . . , φn.
12 ModelUpdate (i, M):
13 for local epoch e = {1, 2, 3, . . . , E} do
14

for batch b do

15

16

M ← M − η∇L(M; b);
Return MT to central server.

updated to the buyer and the incentive is funded to the sellers.
Step 9 (Incentive to Sellers): Based on the contribution of
each seller, the smart contract computes their contribution
and transfer to sellers appropriate funds. The smart contract
provides a mechanism of transparent and immutable recording
and accounting contribution logs on the distributed ledger.
Based on the contribution history from ledger, the clients
can receive the incentives and rewards in tokens via off-chain
payment channels.

Step 10 (Record receipt to the Ledger): All bills and receipts
are recorded immutably in the distributed ledger, which allows
participants to check and control their deal. Besides, we also
implemented the off-chain storage solution named IPFS to
store hashes of data locations on the ledger instead of raw
data ﬁles. The hashes can be used to query the exact ﬁle or
models through the DLT systems.

C. Distributed Shapley Value (DSV) Calculation

The Standard Federated Shapley Value (SFSV) calculates
the SV of data contributors based on equation (2). SFSV trains
federated models based on the different subsets S of contrib-
utors, and these models are evaluated on the standard test set.
However, computing the SV directly according to SFSV is
time-consuming because models on all the combinations of
data sets need to be trained and evaluated. By default, the SV
computes the average contribution of a data source to every
possible subset of other data sources. So that, evaluating the
SV incurs signiﬁcant communication and computation cost
when the data is decentralized [27]. Consequently, for data SV

IEEE JOURNAL SUBMISSION

8

Algorithm 3: AFS Algorithm
Input: Input: Local minibatch size B, number of

participant m per interaction, T is number of
global interactions, number of local epochs E,
and learning rate η.

S , where S ⊆ N = {1, 2, 3 . . . , N }.

Output: Output: MT , and φ1, φ2, . . . , φn.

1 Initialize M0, (cid:102)M0
2 for each round t = {0, 1, 2, . . . , T } do
Transmit Mt to i ∈ S clients;
3
i ← M odelU pdate(i, Mt);
4 Mt
δt+1
i − Mt , ∀i ∈ N ;
i ← Mt
5
6 Mt+1 ← Mt + (cid:80)n
|Di|
for each S ⊆ N do
7
S ← (cid:102)Mt

S + (cid:80)

(cid:102)Mt+1

|Di|

i=1

(cid:80)n

i∈S

(cid:80)

8

i=1 |Di| · δt+1

i

;

i∈S |Di| · δt+1

i

;

9 Initialize m = 0.
10 while Convergence criteria not meet do
11

m = m + 1;
πm: random permutation of clients with data
samples to collaboratively train MT ;
vm
0 ← U ( (cid:102)M0
∅);
for n ∈ {1, 2, . . . , |S|} do
if |U (M|S|) − vm
n = vm
vm

n−1| < PT then

n−1;

else

S ← {πm[1], πm[2], . . . , πm[n]};
S ← (cid:80)
i∈S |Di| · (cid:102)MT
MT
i ;
i∈S
n ← U (MT
vm
φπm[n] ← m−1

(cid:80)
S );
m φπm−1[n] + 1

m (vm

|Di|

n − vm

n−1);

12

13

14

15

16

17

18

19

20 Return MT , and φ1, φ2, . . . , φn.
21 ModelUpdate (i, M):
22 for local epoch e = {1, 2, 3, . . . , E} do
23

for batch b do

24

25

M ← M − η∇L(M; b);

Return M to ledger.

in the FL environment, the methods in [25], [28] to calculate
SV introduce extra training rounds on combinations of datasets
from different data providers. Furthermore, the cost for extra
rounds for training models could be expensive when the data
volume is large. Therefore, there is a need for new strategies
to evaluate the data value in FL.

The main idea to that end is to exploit

the gradients
information during the training process of the global model
M to approximately reconstruct the local models trained with
different combinations of the client’s datasets. Thereby, our
approach (as described in Algorithm 3) eliminates the burden
for the local models to be frequently re-trained to evaluate
clients’ contributions. In fact, the SV does not consider the
order of data sources. However, in FL, it is of signiﬁcant
importance to take into account the order of data used for the
model training so as to ensure a fair convergence. Furthermore,
the updates of model are enforced to diminish over time by
using, for example, a decaying learning rate [29]. Hence,

the sources used towards the end of the learning process
could be less inﬂuential than those used earlier. Therefore,
to accommodate these attributes of learning properties in the
decentralized model training paradigm of FL, we need to
deﬁne new and efﬁcient ways to compute SV. In this regard,
based on the neutrality of FL, the SV for FL (FSV) could be
computed in two different strategies.

The ﬁrst method (called Single-Cal) reconstructs models by
updating the initial global model M in FL with the gradients
in different rounds and calculates the FSV by the performance
of these reconstructed models. For example, if we want to
reconstruct the model of M(i,j) trained on the datasets of Di
and Dj of corresponding users, use the gradients information
from sellers i and j in each round to update the initial global
model M generated by the buyer. Then, the contribution is
calculated using equation (2).

The second method (called Multi-Cal) calculates FSV in
each training round by updating the global model M from
the previous training round with the value of gradients in the
current training round. Next, the FSVs are aggregated from
multiple rounds to get the ﬁnal result. Therefore, there is no
extra training process needed; these methods are considered
efﬁcient. The main difference between these two strategies is
that the ﬁrst method approximates models through complete
global iterations and only evaluates them to ﬁnd SV after-
wards. The second one approximates and evaluates models for
every global iteration and calculates the marginal contribution
for each global iteration. So that makes the second method
more computationally expensive than the ﬁrst one. To address
this issue, we propose a new algorithm AFS based on the ﬁrst
approach with the use of Truncated Monte-Carlo (TMC) [25]
in Algorithm 3. In principle, AFS is an engineered derivative
of the TMC algorithm to characterize clients’ contributions
with their available data samples in the collaborative training
framework. In doing so, AFS evaluates the marginal contribu-
tion of each client instead of a subset of training data points,
unlike the TMC method; thus, allowing clients to engage in
the ML model trading with the offered incentive signals based
on their data contributions.

Speciﬁcally, the ﬁrst part of the algorithm shows the op-
eration of the distributed ledger, lines 1–8 and lines 10–19.
In line 1, the global model M0 and reconstructed models
based on different chosen subsets S ⊆ N = {1, 2, . . . , N } are
initialized. Next, the distributed ledgers broadcast the global
model Mt to n selected clients in each global training round
t in line 3, and then receive the updates Mt
i from these
clients in line 4 and the gradients of clients δt
i , ∀i ∈ S for
model aggregation are computed. After that, the global model
is updated in line 6 as

Mt+1 ← Mt +

n
(cid:88)

i=1

|Di|
i=1 |Di|

(cid:80)n

· δt+1
i

.

(3)

Next, instead of updating all the local models Mt
i, ∀i ∈ S
in every global interactions t = {0, 1, . . . , T }, we can update
only n models (cid:102)Mt
S directly as a weighted
i + |Di
average at the end, as (cid:102)Mt+1
i=1 |Di| δt+1
i

i and compute MT
i ← (cid:102)Mt

(cid:80)n

.

IEEE JOURNAL SUBMISSION

9

in terms of time, especially if the test set

We observe that evaluation of a model incurs a considerable
cost
is large.
And with the basic idea of a single-round algorithm, we
must reconstruct and evaluate 2n models. Hence, we applied
the method of TMC to decrease the computation cost and
developed a tailored variant of TMC, i.e., the AFS algorithm,
to address this issue. The details of the adapted TMC method
is as follows. First, we sample a random permutation of clients
πm with their data samples used to train the global model [28].
After that, we scan from the ﬁrst clients to the last client and
calculate the marginal contribution of every new client’s data
in the training process. By repeating this process over multiple
permutations, the approximation of SV is the average of all the
calculated marginal contributions. The while loop is run until
certain convergence criteria are met. In this work, we stop
the loop when the average percentage change after a TMC
iteration m is less than a certain threshold.

In line 21, the trained federated model MT and the SVs
are ﬁnally obtained. The local training part for the clients
(lines 22–25) show how the clients use private data to train the
model received from the distributed ledger. The clients use the
classical gradient descent algorithm and report their updated
local models Mi|i={1,2,3,...,n} to the distributed ledger.

D. Performance bound on AFS algorithm

An analytical bound on the AFS algorithm can be derived
by taking the properties of TMC sampling approach into
account [25]. We consider AFS estimates the contribution of
the individual client in the federated setting for a supervised
learning task with probability at least (1-α) that our estimator
error is (cid:15). Then, we are interested in evaluating the general
performance bound on AFS such that

Pr(|φF − φS| ≥ (cid:15)) ≤ α,

(4)

1 , φS

2 , . . . , φS

2 , . . . , φF

where φS =< φS
n > is the vector of Shapley
contributions generated by the standard SV and φF =<
φF
1 , φF
n > is the approximation FSV using the pro-
posed AFS method. Assume that we know the data distribution
of clients to evaluate its marginal contribution. Then,
the
sampling |S| made during the evaluation process reﬂects the
bound on the obtained approximation of AFS. Without loss of
generality, we assume it is possible to quantify the range v of
the client’s data marginal contribution in improving the global
model. Then, we have the following Lemma 1.

Lemma 1. Considering the TMC sampling approach [25] for
evaluating the Shapley value, we have a minimum permutation
|S| for a known range of the client’s marginal contribution v
(cid:18)

(cid:19)

on AFS such that α ≥

deﬁning an upper bound of O
(cid:19)

(cid:18)

v
|S|

2 exp

−2|S|(cid:15)2
v2

satisﬁes for 0 ≤ (cid:15), α ≤ 1.

Proof. The proof can be derived using Hoeffding theorem [30]
for a known range of marginal contribution of clients. In prac-
tice, the distributed ledger can reuse the average of marginal
contributions of clients, with known range v,
to derive a
(cid:80)

sampling permutation |S|. Then, we have Pr

(cid:18)

i∈S⊆N (φi −

Figure 5: Performance analysis of AFS algorithm.

(cid:19)

(cid:18)

(cid:19)

≤ 2 exp

−2|S|∆2
E(φi)) ≥ ∆
v2
marginal contributions on the left-hand side of the inequality,
and combining it with (4), we get Pr(|φF − φS| ≥ (cid:15)) ≤

. Taking the average of

2 exp

(cid:19)

(cid:18)

−2|S|(cid:15)2
v2

. This concludes the proof.

In Fig. 5, we show the results on performance analysis
of AFS algorithm. We observed variability in the minimum
permutation |S| required to ensure a deﬁned deviation between
the average value of contributions across clients, as measured
using AFS, and the standard SV. For tighter bounds,
the
number of required permutations is large. This is intuitive,
as the distributed ledger expects a larger sampling value |S|
to deﬁne better conﬁdence bound on the performance that
minimizes the approximation error using AFS.

IV. PERFORMANCE EVALUATION

A. Experimental Settings

In this

To demonstrate the applicability of our proposed system, we
implement a proof-of-concept for the trading model in an IoT
network. In this section, we introduce enabling technologies
involved with the prototype.
1) Distributed Ledger:

study, we implement
Ethereum platform for the experimental. Ethereum4 is a dis-
tributed public blockchain network that focuses on running
programming code of any decentralized application. Speciﬁ-
cally, Ethereum is a platform for sharing information across the
globe that cannot be manipulated or changed. Ethereum has its
own cryptocurrency, called Ether (ETH), and its own program-
ming language, called Solidity. The decentralized applications
on the network is called Dapps. Practically, Ethereum provides
a convenient platform for development and smart contracts
system to integrate with FL. We run Ethereum network via
Ganache5 which is a personal blockchain for rapid Ethereum
distributed application development.

4https://ethereum.org/
5https://www.trufﬂesuite.com/ganache

00.020.040.060.08025102050IEEE JOURNAL SUBMISSION

10

Table II: EXECUTION COSTS OF SMART CONTRACTS

Smart Contracts

From

Gas

Ether

Contract Registry
AddWorker
AddWorker
AddWorker
AddWorker
AddWorker
ModelTransmission
ModelTransmission
ModelTransmission
ModelTransmission
ModelTransmission
ModelTraining
ModelTraining
ModelTraining
ModelTraining
ModelTraining
ModelAggregation
ModelAggregation
ModelAggregation
ModelAggregation
ModelAggregation
Settlement
PayChannelExecute

15.9·10−5
45.2·10−5
45.2·10−5
45.2·10−5
45.2·10−5
45.2·10−5
19.3·10−5
24.3·10−5
22.3·10−5
25.3·10−5
19.3·10−5
22.3·10−5
25.3·10−5
19.3·10−5
25.3·10−5
19.3·10−5
32.4·10−5
22.4·10−5
21.4·10−5
25.3·10−5
19.3·10−5
21.3·10−5
21.2·10−5
* 1 Ether = 109 Gwei; 1 USD = 246,940.5627 Gwei

0x283D382F 1459430
0x283D382F 452467
0x283D382F 452545
0x283D382F 452436
0x283D382F 452545
0x283D382F 452436
0x5846F427 19374
0x9dD8Fd06 243482
0x98HF8F94 228779
0x8H9FH780 253924
0x0932FD99 263924
0x5846F427 223924
0x9DD8Fd06 253924
0x98HF8F94 193924
0x8H9FH780 253924
0x0932FD99 253924
0x5846F427 324942
0x9dD8Fd06 283445
0x98HF8F94 214939
0x8H9FH780 253924
0x0932FD99 193924
0x283D382F 212559
0x283D382F 212538

USD

0.0723
0.0692
0.0692
0.0692
0.0692
0.0692
0.1621
0.0902
0.1121
0.0951
0.0571
0.1021
0.0951
0.0571
0.0951
0.0571
0.0766
0.0408
0.0709
0.0951
0.0571
0.0712
0.0702

5) Scenarios:
• (S1). In scenario 1, the compared algorithms have same
distribution with same dataset size, i.e., each client dataset
Di, ∀i ∈ N has the same amount of training image
samples.

• (S2). In scenario 2, we introduce the case with same
distribution but different dataset size. The training set is
divided randomly into 5 parts with the same ratio of data
size.

• (S3). In scenario 3, we use different distribution with
same dataset size. Each client’s dataset Di, ∀i ∈
{1, 2, 3, 4, 5} has the same size, but the training images
are not equally divided for each digit.

• (S4). In scenario 4, we consider the case having an added
noise feature with same dataset. First, we split the training
set in a similar manner as (S1). Afterwards, we generate
Gaussian noise for the dataset. This is done by adjusting
the standard deviation of the normal distribution.

B. Results

1) Smart Contract Execution Cost: In this part, the proof-
of-concept of proposed model trading platform is deployed in
a private Ethereum Blockchain called Ganache6. In distributed
application Dapps, the smart contract plays as key role in
controlling and autonomously executing pre-deﬁned agree-
ments between the participants. We implemented and tested
smart contracts using Remix IDE7. In Ethereum network,
there is a fee called gas, needed to pay for any operation
or transaction execution that changes the DLT states, which
guarantees that smart contracts running in Ethereum Virtual
Machine (EVM) [24] will be terminated eventually. In the

Figure 6: Blockchain-enabled model
trading testbed. The
testbed includes DLT Ethereum network running over
Ganache, IPFS storage to address scalability issue, monitor
dashboard and 5 IoT raspberry devices standing for market-
place participants as well as DLT clients.

2) Datasets: In the scope of this study, we conducted the
experiments on the MNIST data set. The dataset contains
around 60,000 training images and over 10,000 testing images.
Each client holds a part of dataset locally depending on the
scenarios.

3) IoT Devices and Workstation: We use Raspberry Pi
3 with the following conﬁgurations: Pytorch, OS Raspbian
GNU/Linux 10, and Python version 3.7. We note that CUDA
is not available for the model. The workstation has the system
conﬁgurations as CPU i7-7700HQ, GPU GTX, Pytorch, OS
Linux Ubuntu 20.04 , Python version 3.7.8 using Anaconda,
and the CUDA version 11. These IoT devices are connected
via Wiﬁ access point.

4) Evaluation Metrics.: We consider several performance

metrics for comparison.

• Cost of Smart Contract: We study the fees made by
users to compensate for the computing energy required
to process and validate transactions on the Ethereum.
• Incentive per worker: The amount of tokens delivered to
sellers based on their contributions of training the model.
• Maximum Different: The performance score function
U is chosen to be the accuracy function. The SVs are
then calculated according to the different schemes. For
comparison of the accuracy of the SV, all SVs calculated
are ﬁrst standardized by scaling them by a common factor
such that (cid:80)n

i=1 φi = 1.

This is appropriate because proﬁt distribution will likely be
based on the percentage contribution. Then, the maximum
different Dmax measures the maximum difference that a
data provider should be allocated by the deﬁnition and by
approximated calculation. The calculation is shown as below:

Dmax = max

i∈{1,...,n}

|φF

i − φS
i |.

(5)

6https://www.trufﬂesuite.com/ganache
7https://remix.ethereum.org/

IEEE JOURNAL SUBMISSION

11

(a) Received incentive per client with the dataset distribution ratio
2:2:2:2:2.

(b) Received incentive per client with the dataset distribution ratio
3:3:2:1:1.

Figure 7: Incentive of clients.

scope of this research, we used Gwei8 to evaluate the cost of
different operations, for example, AddWorker, ModelTransmis-
son, ModelTraning, or Settlement in the model trading process.
The result is demonstrated in Table. II.

2) Incentive per client: In Fig. 7, we show the comparison
of received incentives by each training client based on their
contribution to the global model training. The incentive is
equivalent to tokens clients receive. As expected, in Fig. 7a,
is divided equally with a ratio
where the MNIST dataset
of 2:2:2:2:2 for ﬁve involved clients, the amount of tokens
they receive are almost similar as expected. In Fig. 7b, we
show the comparison of the received percentage of tokens
that clients can achieve with the dataset ratio of 3:3:2:1:1.
We observe that client 1 and client 2 has the same amount
of dataset, so they receive the same amount of tokens for
their contribution, similar to the case for clients 4 and 5.

8https://www.cryps.info/

Figure 8: Shapley Value of each seller in different methods

Note that the sellers can train the models with poor quality,
which, in fact, reduces the stability and performance of the
global models. In this regard, there exist several mechanisms
to handle such dishonest reporting of parameters in the FL
setting, such as [13], [31], [32]. Similar to this, the DLT
keeps track of the contribution of devices and the gradient
information and the size of data samples to regularly infer
(check) the relationship between the expected model quality,
reported data samples, and the obtained SV as Fig. 8; hence,
dealing untruthful reporting. However, the detailed study of
this mechanism is out of scope for this work. In Fig. 8, the
AFS shows a better performance while other methods turns
out quite random SVs, especially in scenario 2 and 4 where
the size of dataset is random and noise added.

3) Execution time and maximum different comparison: In
Fig. 9, we show the time performance of exact FL, Single-Cal,
Multi-Cal, and AFS protocol. The Multi-Cal algorithm is more
computational expensive than the Single-Cal algorithm. The
standard exact method is the slowest one because the standard
Shapley Value is naturally not compatible with the Federated
Learning. In the scenario 1, each worker has same quality
and quantity of dataset, so that we expect each worker has
same contribution and receive equally the amount of incentive.
The results show that
the Single-Cal and AFS algorithm
have higher efﬁciency in execution time. The exact method
is around 5 times slower than the rest of methods because

123456Training Round051015202530Incentive (% of received tokens)Seller 1Seller 2Seller 3Seller 4Seller 5123456Training Round010203040Incentive (% of received tokens)Seller 1Seller 2Seller 3Seller 4Seller 52341520123456Training Round051015202530Incentive (% of received tokens)Seller 1Seller 2Seller 3Seller 4Seller 5123456Training Round010203040Incentive (% of received tokens)Seller 1Seller 2Seller 3Seller 4Seller 52341520123450.150.20.25SVScenario 1ExactSingle-CalMulti-CalAFS1234500.20.40.6SVScenario 21234500.20.4SVScenario 312345Seller ID00.20.4SVScenario 4IEEE JOURNAL SUBMISSION

12

with envy-free guarantee. The authors studied two scenarios
including unit demand consumers and single minded con-
sumers, and showed the optimization problem is APX-hard
for both scenarios, which can be efﬁciently addressed by
a logarithmic approximation. The authors in [35] took into
account the trading of IoT streaming data with the presented
marketplace model, where fraudulent activity during data
exchange is limited. To do so, the authors introduced periodic
checkpoints during data trading. In [36], the authors proposed
another marketplace which ﬂows of IoT data are the main
digital assets exchanged utilizing Oracles for the off-chain
queries.The authors in [31] presented a trading mode based on
smart contracts. In particular, the authors employ arbitration
that handles disputes during the data trading, particularly, over
the data availability, and incorporates AI/ML to ensure fairness
during data exchange.

Data Valuation. Evaluating the value of data has been re-
ceived signiﬁcant attention from both academia and industrial
areas. Several works studied data valuation strategies and their
applications. In this regard, the authors in [37] deﬁned the
data valuation in several categories, such as: (i) query-based
pricing, where prices are attached to user-initiated queries
[21], [38], [39], (ii) data attribute-based pricing, where the
price model considers data attributes, such as the age of
data and its credibility, using the mechanism of public price
registries [40], and (iii) auction-based pricing, where the price
is dynamically set following auction mechanisms [41], [42].
In [25],multiple approximation strategies for optimizing the
computation complex of SV for training data are introduced.
Besides, the authors proposed an soltiion to compute exact SC
in speciﬁc scenario, e.g nearest neighbor classiﬁers. Besides,
the SV also is applied in various AI/ML application, for
example, to measure the importance of model features [43],
[44]. In speciﬁc, the authors addressed the problem when the
same data points get the same values, and relationship between
data distributions and SV function. In addition, the author-
sproposed an idea of distributional SV occurs resemblance
to the Aumann-SV [45]. In practical manner, the authors in
[46] proved that the performance of model training can be
improved by removing the data with low SV value. In contrast,
the performance will be decreased if we deleting the training
data with high SV values.

Figure 9: Comparison of execution time and Dmax between
algorithms.

of frequent model retrain process. Meanwhile, the Dmax of
methods are relatively low, around 0.05. Similar in scenario 2
with the same size of dataset and different distribution, AFS
and Single-Cal have better performance in running time and
the accuracy. However, in scenario 4 with more noisy data,
the Multi-Cal shows better results, ≈ 10% in run-time but
and ≈ 15% in terms of maximum different value.

V. RELATED WORKS
In this section, we ﬁrst present the current works on asset

trading based on Blockchain and data valuation.

Blockchain-based asset trading. With the spread of ubiq-
uitous marketplaces, it became relevant to explore the ap-
plication of IoT data trading in marketplace environments.
For instance, the authors in [33] considered a dynamic de-
centralized marketplace and introduced the architecture for
trading IoT data accordingly. The approach involves a 3-
tier method is used: 1) data provider, 2) broker and 3) data
consumer. The primary purpose of DLTs in their function is
to manage the conditions of agreements between the parties
involved. In addition, the design has a reputation system that
penalizes members and lowers their rating. The authors in [34]
invested the optimization problem of revenue maximization

VI. CONCLUSION
In this paper, we proposed a DLT-based marketplace for
trading ML models, which helps companies and organizations
train their learning models in a scalable and efﬁcient manner.
An incentive mechanism exists to stimulate participants in
joining and training the learning models on the marketplace,
which pays participants based on their contributions to train
the model. To that end, an extended Data Shapley Value (DSV)
for the federated environment is proposed to measure each
participant’s contribution in the model training process. Fi-
nally, with extensive experimental evaluations with Ethereum
Blockchain to build a marketplace for model trading using
smart contracts and IoT devices acting as participants, we
demonstrated the design and performance of the proposed
ecosystem.

Scenario 1Single-CalMulti-CalAFSExact050001000015000Run-timeScenario 2Single-CalMulti-CalAFSExact050001000015000Run-timeScenario 3Single-CalMulti-CalAFSExact050001000015000Run-timeScenario 4Single-CalMulti-CalAFSExactMethods050001000015000Run-timeScenario 1Single-CalMulti-CalAFS00.020.040.060.08DmaxScenario 2Single-CalMulti-CalAFS00.020.040.060.08DmaxScenario 3Single-CalMulti-CalAFS00.050.1DmaxScenario 4Single-CalMulti-CalAFSMethods00.020.040.060.08DmaxIEEE JOURNAL SUBMISSION

13

VII. ACKNOWLEDGMENT

This work has received funding from the European Union’s
Horizon 2020 research and innovation programme under grant
agreement No. 957218 (Project IntellIoT).

REFERENCES

[1] I. Report, “The growth in connected iot devices is expected to generate
79.4zb of data in 2025, according to a new idc forecast.” [Online]https:
//www.idc.com, 2019. (Accessed on 12/04/2020).

[2] Z. Huang, X. Su, Y. Zhang, C. Shi, H. Zhang, and L. Xie, “A decentral-
ized solution for iot data trusted exchange based-on blockchain,” in 2017
3rd IEEE International Conference on Computer and Communications
(ICCC), pp. 1180–1184, IEEE, 2017.

[3] C. Perera, “Sensing as a service (s2aas): Buying and selling iot data,”

arXiv preprint arXiv:1702.02380, 2017.

[4] W. Mao, Z. Zheng, and F. Wu, “Pricing for revenue maximization in iot
data markets: An information design perspective,” in IEEE INFOCOM
2019-IEEE Conference on Computer Communications, pp. 1837–1845,
IEEE, 2019.

[5] B. Bishoi, A. Prakash, V. Jain, et al., “A comparative study of air
quality index based on factor analysis and us-epa methods for an urban
environment,” Aerosol and Air Quality Research, vol. 9, no. 1, pp. 1–17,
2009.

[6] J. Jo, B. Jo, J. Kim, S. Kim, and W. Han, “Development of an iot-based
indoor air quality monitoring platform,” Journal of Sensors, vol. 2020,
2020.

[7] Y. Liu, J. Nie, X. Li, S. H. Ahmed, W. Y. B. Lim, and C. Miao, “Fed-
erated learning in the sky: Aerial-ground air quality sensing framework
with uav swarms,” IEEE Internet of Things Journal, vol. 8, no. 12,
pp. 9827–9837, 2021.

[8] S. Moltchanov, I. Levy, Y. Etzion, U. Lerner, D. M. Broday, and
B. Fishbain, “On the feasibility of measuring urban air pollution by
wireless distributed sensor networks,” Science of The Total Environment,
vol. 502, pp. 537–547, 2015.

[9] N. Gruschka, V. Mavroeidis, K. Vishi, and M. Jensen, “Privacy issues
and data protection in big data: a case study analysis under gdpr,” in
2018 IEEE International Conference on Big Data (Big Data), pp. 5027–
5033, IEEE, 2018.

[10] B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas,
“Communication-efﬁcient learning of deep networks from decentralized
data,” in Artiﬁcial intelligence and statistics, pp. 1273–1282, PMLR,
2017.

[11] B. Xu, L. Da Xu, H. Cai, C. Xie, J. Hu, and F. Bu, “Ubiquitous
data accessing method in iot-based information system for emergency
medical services,” IEEE Transactions on Industrial informatics, vol. 10,
no. 2, pp. 1578–1586, 2014.

[12] R. Radhakrishnan and B. Krishnamachari, “Streaming data payment
protocol (sdpp) for the internet of things,” in 2018 IEEE International
Conference on Internet of Things (iThings) and IEEE Green Computing
and Communications (GreenCom) and IEEE Cyber, Physical and Social
Computing (CPSCom) and IEEE Smart Data (SmartData), pp. 1679–
1684, IEEE, 2018.

[13] C. Niu, Z. Zheng, F. Wu, X. Gao, and G. Chen, “Achieving data truth-
fulness and privacy preservation in data markets,” IEEE Transactions on
Knowledge and Data Engineering, vol. 31, no. 1, pp. 105–119, 2018.

[14] D. C. Langevoort, “Fraud and insider trading in american securities
regulation: Its scope and philosophy in a global marketplace,” Hastings
Int’l & Comp. L. Rev., vol. 16, p. 175, 1992.

[15] S. R. Pandey, N. H. Tran, M. Bennis, Y. K. Tun, A. Manzoor, and
C. S. Hong, “A crowdsourcing framework for on-device federated
learning,” IEEE Transactions on Wireless Communications, vol. 19,
no. 5, pp. 3241–3256, 2020.

[16] S. Nakamoto, “Bitcoin: A peer-to-peer electronic cash system,” tech.

rep., Manubot, 2008.

[17] L. D. Nguyen, I. Leyva-Mayorga, A. N. Lewis, and P. Popovski,
“Modeling and analysis of data trading on blockchain-based market in
iot networks,” IEEE Internet of Things Journal, vol. 8, no. 8, pp. 6487–
6497, 2021.

[18] B. Chen, D. He, N. Kumar, H. Wang, and K.-K. R. Choo, “A blockchain-
based proxy re-encryption with equality test for vehicular communica-
tion systems,” IEEE Transactions on Network Science and Engineering,
vol. 8, no. 3, pp. 2048–2059, 2021.

[19] L. D. Nguyen, A. E. Kalor, I. Leyva-Mayorga, and P. Popovski,
“Trusted wireless monitoring based on distributed ledgers over nb-iot
connectivity,” IEEE Communications Magazine, vol. 58, no. 6, pp. 77–
83, 2020.

[20] T. Wang, C. Zhao, Q. Yang, S. Zhang, and S. C. Liew, “Ethna:
Analyzing the underlying peer-to-peer network of ethereum blockchain,”
IEEE Transactions on Network Science and Engineering, vol. 8, no. 3,
pp. 2131–2146, 2021.

[21] P. Kairouz, H. B. McMahan, B. Avent, A. Bellet, M. Bennis, A. N.
Bhagoji, K. Bonawitz, Z. Charles, G. Cormode, R. Cummings, et al.,
“Advances and open problems in federated learning,” arXiv preprint
arXiv:1912.04977, 2019.

[22] H. Kim, J. Park, M. Bennis, and S.-L. Kim, “Blockchained on-device
federated learning,” IEEE Communications Letters, vol. 24, no. 6,
pp. 1279–1283, 2019.

[23] A. E. Roth, The Shapley value: essays in honor of Lloyd S. Shapley.

Cambridge University Press, 1988.

[24] G. Wood et al., “Ethereum: A secure decentralised generalised transac-
tion ledger,” Ethereum project yellow paper, vol. 151, no. 2014, pp. 1–
32, 2014.

[25] R. Jia, D. Dao, B. Wang, F. A. Hubis, N. Hynes, N. M. G¨urel, B. Li,
C. Zhang, D. Song, and C. J. Spanos, “Towards efﬁcient data valuation
based on the shapley value,” in The 22nd International Conference on
Artiﬁcial Intelligence and Statistics, pp. 1167–1176, PMLR, 2019.
[26] X. Ding, J. Guo, D. Li, and W. Wu, “An incentive mechanism for build-
ing a secure blockchain-based internet of things,” IEEE Transactions on
Network Science and Engineering, vol. 8, no. 1, pp. 477–487, 2021.

[27] Y. Qu, L. Gao, T. H. Luan, Y. Xiang, S. Yu, B. Li, and G. Zheng,
“Decentralized privacy using blockchain-enabled federated learning in
fog computing,” IEEE Internet of Things Journal, vol. 7, no. 6, pp. 5171–
5183, 2020.

[28] A. Ghorbani and J. Zou, “Data shapley: Equitable valuation of data for
machine learning,” in International Conference on Machine Learning,
pp. 2242–2251, PMLR, 2019.

[29] Z. Charles and J. Koneˇcn`y, “On the outsized importance of learning
rates in local update methods,” arXiv preprint arXiv:2007.00878, 2020.
[30] W. Hoeffding, “A combinatorial central limit theorem,” The Annals of

Mathematical Statistics, pp. 558–566, 1951.

[31] W. Xiong and L. Xiong, “Smart contract based data trading mode using
blockchain and machine learning,” IEEE Access, vol. 7, pp. 102331–
102344, 2019.

[32] A. Agarwal, M. Dahleh, and T. Sarkar, “A marketplace for data: An
algorithmic solution,” in Proceedings of the 2019 ACM Conference on
Economics and Computation, pp. 701–726, 2019.

[33] P. Gupta, S. Kanhere, and R. Jurdak, “A decentralized iot data market-

place,” arXiv preprint arXiv:1906.01799, 2019.

[34] R. Iyengar, J. P. Near, D. Song, O. Thakkar, A. Thakurta, and L. Wang,
“Towards practical differentially private convex optimization,” in 2019
IEEE Symposium on Security and Privacy (SP), pp. 299–316, IEEE,
2019.

[35] S. Bajoudah, C. Dong, and P. Missier, “Toward a decentralized, trust-
less marketplace for brokered iot data trading using blockchain,” in 2019
IEEE International Conference on Blockchain (Blockchain), pp. 339–
346, IEEE, 2019.

[36] P. Missier, S. Bajoudah, A. Capossele, A. Gaglione, and M. Nati, “Mind
my value: a decentralized infrastructure for fair and trusted iot data
trading,” in Proceedings of the Seventh International Conference on the
Internet of Things, pp. 1–8, 2017.

[37] T. Wang, J. Rausch, C. Zhang, R. Jia, and D. Song, “A principled ap-
proach to data valuation for federated learning,” in Federated Learning,
pp. 153–167, Springer, 2020.

[38] D. Leroy, A. Coucke, T. Lavril, T. Gisselbrecht, and J. Dureau, “Fed-
erated learning for keyword spotting,” in ICASSP 2019-2019 IEEE
International Conference on Acoustics, Speech and Signal Processing
(ICASSP), pp. 6341–6345, IEEE, 2019.

[39] P. Upadhyaya, M. Balazinska, and D. Suciu, “Price-optimal querying
with data apis,” Proceedings of the VLDB Endowment, vol. 9, no. 14,
pp. 1695–1706, 2016.

[40] J. R. Heckman, E. L. Boehmer, E. H. Peters, M. Davaloo, and N. G. Ku-
rup, “A pricing model for data markets,” iConference 2015 Proceedings,
2015.

[41] M. Mihailescu and Y. M. Teo, “Dynamic resource pricing on federated
clouds,” in 2010 10th IEEE/ACM International Conference on Cluster,
Cloud and Grid Computing, pp. 513–517, IEEE, 2010.

[42] J.-S. Lee and B. Hoh, “Sell your experiences: a market mechanism
based incentive for participatory sensing,” in 2010 IEEE International

IEEE JOURNAL SUBMISSION

14

Conference on Pervasive Computing and Communications (PerCom),
pp. 60–68, IEEE, 2010.

[43] S. Cohen, G. Dror, and E. Ruppin, “Feature selection via coalitional
game theory,” Neural Computation, vol. 19, no. 7, pp. 1939–1961, 2007.
[44] E. Strumbelj and I. Kononenko, “An efﬁcient explanation of individual
classiﬁcations using game theory,” The Journal of Machine Learning
Research, vol. 11, pp. 1–18, 2010.

[45] R. J. Aumann and L. S. Shapley, Values of non-atomic games. Princeton

University Press, 2015.

[46] S. Tang, A. Ghorbani, R. Yamashita, S. Rehman, J. A. Dunnmon, J. Zou,
and D. L. Rubin, “Data valuation for medical imaging using shapley
value and application to a large-scale chest x-ray dataset,” Scientiﬁc
reports, vol. 11, no. 1, pp. 1–9, 2021.

Arne Br¨oring is a Senior Key Expert Research
Scientist at Siemens Technology in Munich. He
received his PhD in 2012 from the University of
Twente (Netherlands). Dr. Br¨oring has contributed
to over 90 publications in the ﬁeld of distributed
systems and has served on various program commit-
tees and editorial boards. His research interests range
from distributed system designs, over sensor net-
works, and Semantic Web, to the Internet of Things.
At Siemens, he has been in charge of the technical &
scientiﬁc coordination of large EU research projects
(BIG IoT and IntellIoT). Before joining Siemens, Dr. Br¨oring worked for
the Environmental Systems Research Institute in Zurich, the 52°North Open
Source Initiative, and led the Sensor Web and Simulation Lab at the University
of M¨unster.

Lam Duc Nguyen (S’20) is a Ph.D. Fellow at
Aalborg University. He obtained Master Degree in
Computer Science at Seoul National University, and
a Bachelor in Telecommunication at Hanoi Uni-
versity of Science and Technologies in 2019 and
2015, respectively. His research includes Distributed
Systems, Blockchain, Smart Contracts, the Internet
of Things, and applying Blockchain and Federated
Learning to enhance the efﬁciency of Blockchain-
based IoT monitoring Networks. He receives Out-
standing Paper Award for the research about scaling
Blockchain in Massive IoT at the IEEE World Forum Internet of Things
2020, travel grant from Linux Foundation 2020, Best Research Award for a
solution of Blockchain-based CO2 Emission Trading from VEHITS 2021. He
is Hyperledger Member, IEEE Student Member, and IEEE ComSoc Student
Member.

Shashi Raj Pandey (M’21) is currently working as a
Postdoctoral Researcher at the Connectivity Section,
Aalborg University. He received the B.E. degree in
Electrical and Electronics with a specialization in
Communication from Kathmandu University, Nepal
in 2013, and the Ph.D. degree in Computer Science
and Engineering from Kyung Hee University, Seoul,
South Korea,
in August, 2021. He served as a
Network Engineer at Huawei Technologies Nepal
Co. Pvt. Ltd, Nepal from 2013 to 2016. His research
interests include network economics, game theory,

wireless communications, data markets and distributed machine learning.

Beatriz Soret [M’11] received her M.Sc. and Ph.D.
degrees in Telecommunications from the Univer-
sidad de Malaga, Spain,
in 2002 and 2010, re-
spectively. She is currently an associate professor
at the Department of Electronic Systems, Aalborg
University, and a Senior Research Fellow at
the
Communications Engineering Department, Univer-
sity of Malaga. Her research interests include LEO
satellite communications, distributed and intelligent
IoT, timing in communications, and 5G and post-5G
systems.

Petar Popovski (Fellow, IEEE) is a Professor at
Aalborg University, where he heads the section on
Connectivity and a Visiting Excellence Chair at the
University of Bremen. He received his Dipl.-Ing and
M. Sc. degrees in communication engineering from
the University of Sts. Cyril and Methodius in Skopje
and the Ph.D. degree from Aalborg University in
2005. He is a Fellow of the IEEE. He received an
ERC Consolidator Grant (2015), the Danish Elite
Researcher award (2016), IEEE Fred W. Ellersick
prize (2016), IEEE Stephen O. Rice prize (2018),
Technical Achievement Award from the IEEE Technical Committee on Smart
Grid Communications (2019), the Danish Telecommunication Prize (2020)
and Villum Investigator Grant (2021). He is a Member at Large at
the
Board of Governors in IEEE Communication Society, Vice-Chair of the IEEE
Communication Theory Technical Committee and IEEE TRANSACTIONS
ON GREEN COMMUNICATIONS AND NETWORKING. He is currently
an Area Editor of the IEEE TRANSACTIONS ON WIRELESS COM-
MUNICATIONS and, from 2022, an Editor-in-Chief of IEEEE JOURNAL
ON SELECTED AREAS IN COMMUNICATIONS. Prof. Popovski was the
General Chair for IEEE SmartGridComm 2018 and IEEE Communication
Theory Workshop 2019. His research interests are in the area of wireless
communication and communication theory. He authored the book “Wireless
Connectivity: An Intuitive and Fundamental Guide”, published by Wiley in
2020.

