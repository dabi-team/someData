Decentralized Collaborative Learning with
Probabilistic Data Protection

Tsuyoshi Id´e
IBM Research, T. J. Watson Research Center
Yorktown Heights, NY, USA
tide@us.ibm.com

Rudy Raymond
IBM Research – Tokyo
Tokyo, Japan
rudyhar@jp.ibm.com

2
2
0
2

g
u
A
4
2

]

G
L
.
s
c
[

2
v
4
7
6
0
1
.
8
0
2
2
:
v
i
X
r
a

Abstract—We discuss future directions of Blockchain as a
collaborative value co-creation platform, in which network par-
ticipants can gain extra insights that cannot be accessed when
disconnected from the others. As such, we propose a decentralized
machine learning framework that is carefully designed to respect
the values of democracy, diversity, and privacy. Speciﬁcally,
we propose a federated multi-task learning framework that
integrates a privacy-preserving dynamic consensus algorithm.
We show that a speciﬁc network topology called the expander
graph dramatically improves the scalability of global consensus
building. We conclude the paper by making some remarks on
open problems.1

Index Terms—Blockchain, decentralized learning, multi-task
learning, federated learning, expander graphs, data privacy,
secret sharing

I. INTRODUCTION

Spurred by the remarkable commercial success of cryp-
tocurrencies, there have been many attempts to date to ex-
tend Blockchain as a decentralized management platform for
general business transactions. In most application scenarios,
such as product traceability [1]–[3] and those involving smart
contracts [4], Blockchain has been used essentially as an
immutable data storage whose goal
is simply to manage
identical replicas of data among distributed nodes. Although
this is a meaningful ﬁrst step, we argue that the true value of
Blockchain lies in its potential for value co-creation by net-
work participants (“agents”) through knowledge sharing [5]–
[7]. The platform should help the agents obtain extra insights
their own data
that cannot be accessed when looking at
alone, disconnected from the others. We envision that the next
generation of Blockchain will integrate collaborative learning
capabilities at the core.

Inspired by the Nakamoto’s original agenda [8], our collab-
orative learning platform features three main characteristics:
1) The entire learning procedure is done in a decentralized
manner, i.e., without relying on the central authority. 2) The
outcomes of the learning reﬂect speciﬁc situations of the
individual agents, resulting in generally different models for
each of the agents. 3) The data and the learned model of the
agents are protected as a private property.

1Published as: Tsuyoshi Id´e and Rudy Raymond, “Decentralized Collabo-
rative Learning with Probabilistic Data Protection,” Proceedings of the 2021
IEEE International Conference on Smart Data Services (SMDS 21, September
5-10, 2021, virtual), pp.234-243.

Fig. 1.
Illustration of decentralized multi-task learning illustrated on a 3-
regular expander graph of S = 7 nodes, where p(· | Θ, Πs) denotes a
statistical machine learning model for the agent s with Θ being common
model parameters and Πs being agent-speciﬁc model parameters.

The ﬁrst characteristic is to ensure democracy in the plat-
form. The agents voluntarily communicate with the others
through given network infrastructure, but there is no such thing
as the central server that collects a piece of data from the
agents and perform, e.g., stochastic gradient descent to train
a deep learning model. The second characteristic is to ensure
diversity in the platform. Here, a ‘model’ refers to a probability
distribution in general that captures patterns hidden in the data,
as implied in Fig. 1. In the machine learning literature, this
is commonly called multi-task learning because the goal is to
learn S different models (‘tasks’) simultaneously, where S is
the number of agents participated in the network. The third
characteristic is to ensure the privacy of the agents in the
platform. Although they learn from the other agents and share
their learnings in return in a certain way, their original data
and the resulting model must be protected.

This paper proposes a new framework of decentralized
collaborative learning with certain privacy guarantees. Given a
speciﬁc parametric model agreed upon, the goal of the agents
is to maximize the likelihood function of the entire system
in a collaborative manner. Higher likelihood implies higher
model ﬁdelity, which will lead to better business outcomes
through more accurate predictions. In our previous work [9],
we presented a decentralized multi-task learning protocol
for a mixture of exponential family distributions, where the
dynamic consensus algorithm [10]–[12] eliminates the need

 
 
 
 
 
 
for the central server. We found that the spectral structure of
network topology plays a critical role in the convergence of
the algorithm, but the analysis was mainly on the cycle graph,
in which an analytic form of the eigenspectrum is available. In
this paper, we show that a family of graphs called the expander
graph [13], [14] can dramatically speedup global consensus.
We also provide a systematic analysis on the probability of
privacy breach in a couple of different scenarios. Decentralized
collaborative learning is a new research ﬁeld. We conclude the
paper with some remarks on open problems.

II. RELATED WORK

One recent major trend in distributed learning is to distribut-
edly train deep neural networks using SGD (stochastic gradient
descent) [15]–[19], in which managing huge computational
overhead has been an issue. Although most of the existing
work falls into the category of distributed single-task learning,
some recent works point out that model biases and thus a lack
of diversity can be a serious issue [20], [21]. A multi-task
extension is discussed in [22] without a data privacy context.
A unique role of the exponential family in data privacy is
pointed out in [23].

For privacy preservation in distributed learning, a common
approach is to use differential privacy [24], [25], but for real-
valued data, performance degradation in learning due to intro-
duced noise is an open question, as discussed in [26]. Privacy
preservation in a decentralized environment is a challenging
task in general. Almost the only solution known so far is
the use of homomorphic encryption and its variants [15],
[27], [28], but its computational cost is known to be often
prohibitive.

III. MULTI-TASK LEARNING FRAMEWORK

This section summarizes the problem setting from our

previous work [9].

A. Problem setup

As shown in Fig. 1, there are S ≥ 3 agents in the network.

Each agent indexed by a privately keeps its own data set

Da (cid:44) {z(1), . . . , z(N a)}

(1)

about a random variable z. Here, N a is the number of samples
of the a-th agent. As a general rule, we use the superscript
such as (n) to represent the n-th instance of a random variable.
The random variable z can be a pair of a feature vector and
its label like z = (y, x) or simply a feature vector alone
z = x. We assume x is real-valued and noisy in general.
The goal of the agents is to learn a predictive distribution
for z. As illustrated in Fig. 1, the distribution will have model
parameters Θ and Πa, where Θ is parameters shared by all
the agents and Πa is agent-speciﬁc parameters. Both Θ and
Πa are to be learned from the total data D (cid:44) {D1, . . . , DS}.
The question is how the agents leverage information from the
other ones while keeping data privacy.

As part of the network infrastructure, a set of bi-directional
communication paths are given as an undirected graph, whose

nodes are the agents and the edges are pairwise communication
paths, as shown in the ﬁgure. As is the case in the IP
(internet protocol) network of the Internet, the infrastructure
is assumed to do basic bookkeeping jobs such as network
routing and clock synchronization without any interest in the
contents communicated. Network failures are unavoidable in
real networks but we do not consider them for simplicity. We
also assume a consortium-based network, where the agents
have veriﬁed identities. The agents are honest but curious,
meaning that they do not lie about computed statistics but they
always try to selﬁshly get as much information as possible
from the other agents.

B. Mixture of exponential family

To capture the diversity among the agent, we employ a
mixture of the exponential family. In what follows, we focus
on the case where z = x ∈ RM with M being the dimen-
sionality of the variable and the learning task is unsupervised
(a.k.a. density estimation). Extension to the supervised setting
can be done easily. Practical applications of this setting include
failure detection of industrial robots, where all the S agents
are assumed to have the same set of physical sensors. See [29]
for more detail.

Now the observation model of the a-th agent is given by:

p(x | Θ, ua) =

K
(cid:89)

k=1

f (x | θk)ua

k

f (x | θk) = G(θk)H(x) exp{η(θk)(cid:62)T (x)},

(2)

(3)

1, . . . , ua

where K is the number of mixture components and ua (cid:44)
(ua
K)(cid:62) is the one-hot indicator variable representing
cluster assignment. Notice that the dependency on a in ua rep-
resents diversity of the model. In f (z | θk), functional forms
of G, H (scalar function) and η, T (vector-valued function)
are given by a speciﬁc choice in the exponential family [30].
We will give a Gaussian-based model as an example below.

In the unsupervised scenario,

in
the form Eq. (2) is almost always combined with a prior
distribution of ua:

the observation model

p(ua | πa) = Cat(ua | πa) (cid:44)

K
(cid:89)

(πa

k)ua

k

(4)

k=1

where Cat denotes the categorical distribution. The parameter
πa (cid:44) (πa
K) can be viewed as the probability distribu-
tion over the K clusters and satisﬁes (cid:80)K

1 , . . . , πa

k=1 πa

k = 1.

For stable numerical estimation, prior distributions are im-

posed also on Θ (cid:44) {θk} and Π (cid:44) {πa} as

p(Θ) =

K
(cid:89)

k=1

p(θk),

p(Π) =

S
(cid:89)

a=1

p(πa),

(5)

where we have used p(·) to generically represent potentially
different probability distributions.

Here we give an example when f (z | θk) is the Gaus-
sian N (x | µk, (Λk)−1), where µk is the mean and Λk
the precision matrix. For θk = {µk, Λk}, one practically

recommended choice for the prior distribution is the Gauss-
Laplace distribution:

p(µk, Λk) ∝ N (µk|m0, (λ0Λk)−1) exp

(cid:16)

−

ρ
2

(cid:17)

(cid:107)Λk(cid:107)1

(6)

where ρ, λ0, m0 are predeﬁned constants and (cid:107) · (cid:107)1 is the (cid:96)1
norm. For p(πa), a common choice is the Dirichlet distribution
p(πa) ∝ (πa

K)γ with γ ∼ 1 is a given constant.

1 · · · πa
C. Model estimation algorithm

The probabilistic model presented above contains unknown
model parameters Θ, Π in addition to the latent variable U (cid:44)
{u1, . . . , uS}. The standard strategy to learn the model in such
a case is to maximize the log marginalized likelihood L0 with
respect to Θ, Π:

L0 (cid:44) ln p(Π)p(Θ)
(cid:40)

s
(cid:89)

N s
(cid:89)

(cid:88)

+ ln

U

a=1

n=1

p(za(n)|Θ, ua(n))p(ua(n) | πa)

.

(cid:41)

Unfortunately, this maximization problem is intractable even
in the simple Gaussian case. We instead maximize the lower
bound of L0 derived by applying Jensen’s inequality. We can
derive a simple iterative algorithm to estimate the unknown
model parameters Θ, Π as well as the posterior distribution
of the latent variable U :
S
(cid:89)

N s
(cid:89)

K
(cid:89)

(ra(n)
k

)ua(n)

k

.

(7)

Q(U ) =

a=1

n=1

k=1

Here, we summarize the result presented in our previous
k = 1, we

work [9]. With an initialized {ra(n)} so (cid:80)K
repeat the following steps until convergence:

k=1 ra(n)

• In each a ∈ {1, . . . , S}, with the latest {ra(n)

}, locally

k

compute

N a
k

(cid:44)

N a
(cid:88)

n=1

ra(n)
k

and T a
k

(cid:44)

N a
(cid:88)

n=1

ra(n)
k T (za(n)).

(8)

• Among all a = 1, . . . , S, build a global consensus on

S
(cid:88)

Nk (cid:44)

N a
k ,

and Tk (cid:44)

a=1
• In each a ∈ {1, . . . , S}, locally solve

S
(cid:88)

a=1

T a
k ,

(9)

(cid:8)ln p(θk) + Nk ln G(θk) + T (cid:62)

k η(θk)(cid:9)

(10)

max
θk

• In each a ∈ {1, . . . , S}, with the latest {θk} and {πa}

with πa

k = N a

k +γ

N a+Kγ , locally update

.

(11)

(cid:80)K

ra(n)
k =

mf (za(n) | θm)

πa
kf (za(n) | θk)
m=1 πa
Here, we assumed that we have used the Dirichlet distribution
K)γ for p(Π). Notice that agent-agent
p(πa) ∝ (πa
communication is involved only in the second step; All the
other steps need only local computation that can be complete
within each agent. The complexity per agent per iteration is
O(N a + M 3 + ln S), assuming Eq. (10) takes M 3 and the
consensus step Eq. (9) takes ln S (See Theorem 5).

1 · · · πa

D. Gaussian example

To be concrete, we provide parameter updating equations for
the Gaussian observation model with the Gauss-Laplace prior
Eq. (6). In this case, instead of T a

ma

k =

N s
(cid:88)

n=1

ra(n)
k xa(n), Ca

k =

n=1

k in Eq. (8), we compute
N s
(cid:88)

k xa(n)xa(n)(cid:62)
ra(n)

.

(12)

for each a ∈ {1, . . . , S} and each mixture component k ∈
{1, . . . , K}. Then, in the step of global consensus in Eq. (9),
we compute aggregated values as

¯mk =

S
(cid:88)

a=1

ma
k,

¯Ck =

S
(cid:88)

a=1

Ca
k.

(13)

Finally, in the step of optimization in Eq. (10), we compute

µk =

1
λ0 + Nk

(cid:26)

¯mk, Σk =

1
Nk

Λk = arg max
Λk

ln det Λk − Tr(ΛkΣk) −

¯Ck + µkµ(cid:62)
k ,

(14)

(cid:27)

(cid:107)Λk(cid:107)1

, (15)

ρ
Nk

where Tr is the matrix trace and det is the matrix determi-
nant. This optimization problem is well-known in covariance
selection and can be solved very efﬁciently with the graphical
lasso algorithm [31], [32]. To ensure that all the agents have
the same {µk, Λk}, they can run another global consensus step
to register the average as the ﬁnal outcome in each iteration
round.

IV. AGGREGATION VIA SECRET SHARING

This section focuses on Eq. (9),

i.e., how to securely
aggregate the local statistics. Since aggregation can be done
loss of generality, we consider the
element-wise, without
problem of computing the sum of scalars {ξa}:

¯ξ =

S
(cid:88)

a=1

ξa = 1(cid:62)

S ξ(0),

(16)

where ξa’s are constants to be summed, 1S is the S-
dimensional vector of ones, and we deﬁned ξa(0) = ξa.
Aggregation would be trivial if a trusted coordinator existed in
the network. The question is how to compute the summation
only through local communications and how to make it secure.

A. Aggregation through Markov transitions on graph

Let A ∈ {0, 1}S×S be the incidence matrix of the commu-
nication graph, where only connected nodes (or neighboring
nodes) can communicate with each other. As illustrated in
Fig. 1, the graph is undirected but may have self-loops and
multiple edges. Given A = [Aa,j], consider the following
updates:

ξa(t + 1) = ξa(t) + (cid:15)

S
(cid:88)

j=1

Aa,j[ξj(t) − ξa(t)],

(17)

where t is the number of update rounds, and (cid:15) is a given
parameter controlling convergence. All the S nodes perform

this update by communicating with their neighbors. In the
matrix form, Eq. (17) is written as

ξ(t + 1) = W(cid:15)ξ(t) with W(cid:15) (cid:44) IS − (cid:15)(D − A),

(18)

where D (cid:44) diag(d1, . . . , dS) is the degree matrix with ds
being the degree of the s-th node, IS is the S-dimensional
identity matrix, and ξ(t) (cid:44) (ξ1(t), . . . , ξS(t))(cid:62).

The key idea of multi-agent coordination is to associate a
stationary solution of the Markov transition deﬁned by Eq. (18)
with the process of consensus building. As can be easily
veriﬁed, u1 = 1√
1S is an (cid:96)2-normalized eigenvector of W(cid:15)
S
whose eigenvalue is λ1 = 1. If this is non-degenerated and
the other absolute eigenvalues are less than one, Eq. (18) will
converge to the stationary solution ξ∗

ξ∗ = W(cid:15)

∞ξ(0) ≈ λ1

∞u1u(cid:62)

1 ξ(0) =

1
S

¯ξ1S

(19)

because only the largest eigenvalue survives in the spectral
expansion after an inﬁnite number of transitions [33]. This
means that all of the agents end up having the same value of
¯ξ, which is the aggregation we wanted. We call this approach
1
S
the dynamical consensus algorithm.

One obvious limitation of this approach is that the connected
peers can see the original value in the ﬁrst iteration, which is
a privacy breach. To address this issue, we propose two secret
sharing approaches: One is Shamir’s secret sharing [34] and
the other is random chunking we proposed originally in [9].

B. Shamir’s secret sharing

In Shamir’s secret sharing scheme, each of the S agents ﬁrst
generates random numbers Ra
S−1 in a large enough
integer domain to deﬁne an (S − 1)-th order polynomial
function. For the a-th agent, the polynomial is deﬁned by

1, . . . , Ra

ga(n) = ξa + Ra

1n + · · · + Ra

S−1nS−1.

(20)

Using this polynomial, each agent locally computes S values
{gs(1), . . . , gs(S)}. Then, by repeating the dynamical consen-
sus algorithm S times (i.e., by setting ξs(0) = gs(n) for
n = 1, . . . , S in Eq. (17)), the agents build a consensus
on S aggregated values {¯g(1), . . . , ¯g(S)}, where ¯g(l) (cid:44)
(cid:80)S
1 etc. are random numbers, raw data {ξa}
will not be revealed to the peers in the communication process.
this point, each agent has the S input-output pairs

s=1 gs(l). Since Ra

At

{(1, ¯g(1)), . . . , (S, ¯g(S))} of the polynomial

¯g(n) = ¯ξ + ¯R1n + · · · + ¯RS−1nS−1

(21)
at hand, where ¯Ri (cid:44) (cid:80)S
i . Notice that we have ¯ξ on the
r.h.s. as the intercept. Since an (S − 1)-th order polynomial is
uniquely determined by distinctive S points, each agent can
uniquely identify the functional form of ¯g(n). With Lagrange’s
interpolation formula, the agents obtain the intercept by

a=1 Ra

¯ξ = ¯g(0) =

S
(cid:88)

(cid:89)

¯g(l)

l=1

m(cid:54)=l

m
(m − l)

.

(22)

Algorithm 1 Dynamical consensus with random chunking
1: Input: (cid:15), NC. Initialize ¯ξ = 0
2: Split ξa into NC chunks for ∀a.
3: for iC ← 1, NC do
4:

Randomly generate A. Conﬁrm all the links are avail-
able for communication (re-generate A otherwise).
Initialize ξa(0) as ξ[iC]
repeat

for ∀a.

a

Each agent synchronously perform (17).

5:
6:
7:
8:

until convergence
¯ξ ← ¯ξ + ¯ξ[iC]

9:
10: end for

large, the product form of Eq. (22) needs a special attention
the need for S repeated
to avoid numerical
consensus makes it computationally less efﬁcient, as will be
empirically shown in Section VII.

issues. Also,

C. Random chunking

Shamir’s algorithm is based on the idea of recovering the
data from seemingly non-informative multiple signatures. Here
it is interesting to see what happens if we use a datum itself
as the signature. Speciﬁcally, each of the agents secretly splits
a local datum ξs into a few chunks {ξ[h]
s =
ξs. Then, the agents run the dynamical consensus algorithm
on each of the chunks to get { ¯ξ[1], . . . , ¯ξ[NC]}, where NC (cid:28) S
is the predeﬁned number of chunks. Since aggregation is an
linear operation, we have

s } such that (cid:80)

h ξ[h]

¯ξ[1] + · · · + ¯ξ[NC] =

S
(cid:88)

(ξ[1]

s + · · · + ξ[N c]

s

) = ¯ξ.

(23)

s=1

This means that the total aggregation can be computed by per-
forming chunk-wise aggregations without sharing the raw data
{ξs} explicitly. The procedure is summarized in Algorithm 1.
Here, to prevent the agents from recovering the raw data by
receiving all the NC chunks, the chunk-wise aggregations must
be made on different incidence matrices. There are two major
solutions having different levels of intervention of the network
infrastructure (see Section III-A). For the higher intervention
side, upon starting aggregation, the router may randomly pick
A from a set of prepopulated incidence matrices, in which
any pair of the graphs do not share the same edge. If the
router always picks an A that is different from the previous
aggregation round, NC = 2 sufﬁces.

If we cannot expect too much from the infrastructure, the
security guarantee becomes probabilistic. This is reminiscent
of Bitcoin’s probabilistic security guarantee, which makes it
the probabilistic ﬁnality protocol [36]. In the next section,
we discuss the detail of probabilistic guarantee of the random
chunking algorithm.

V. PRIVACY BREACH ANALYSIS

Shamir’s algorithm is secure in the sense that it satisﬁes
rigorous cryptographic conditions [35]. However, when S is

This section provides probabilistic guarantees of the random

chunking algorithm under three major attack scenarios.

A. Independent agent scenario

for NL < S − ds, and 1 otherwise.

As suggested in Algorithm 1, one practical scenario is for
the router to randomly assign the node name whenever starting
aggregation, keeping the graph structure itself ﬁxed. To study
the risk of privacy breach, consider the event that the j-th
node is the breach node to s, meaning that the node j is able
to fully recover ξs by summing over NC chunks it received.
This happens when the j-th node stays as the neighbor over
the NC rounds, regardless of the other nodes. Since the j-th
node gets chosen in each of the NC chunking round with a
probability

(Proof ) For an s-th non-colluded node, privacy breach occurs
when at least one colluded node receives the chunk in all the
NC chunking rounds. Therefore, we have

c.breach = (1 − pL)NC ,
ps

(29)

where pL is the probability that no colluded nodes are chosen
in the neighbor set. The probability pL can be evaluated as

pL =

(cid:18)S − 1 − NL
ds

(cid:19)(cid:18)S − 1

(cid:19)−1

ds

(cid:18) S − 2
ds − 1

(cid:19)(cid:18)S − 1

(cid:19)−1

=

ds
S − 1

,

ds

(24)

=

(S − ds − 1)(S − ds − 2) · · · (S − ds − NL)
(S − 1)(S − 2) · · · (S − NL)

,

(30)

the probability that the j-th node is the breach node to s

regardless of the other nodes is given by
. Since
some of the other S − 2 nodes may be the breach node as
well, we have

(cid:17)NC

(cid:16) ds
S−1

ps
breach ≤

(cid:88)

(cid:18) ds

(cid:19)NC

S − 1

j(cid:54)=s

= (S − 1)

(cid:18) ds

(cid:19)NC

S − 1

(25)

by Boole’s inequality (or the union bound). Therefore, we
conclude that the probability of not having any breach node
in the network is lower-bounded as

psecure ≥

(cid:40)

1 − (S − 1)

(cid:19)NC (cid:41)

(cid:18) ds

S − 1

S
(cid:89)

s=1

≥ 1 − S(S − 1)

(cid:18) dmax
S − 1

(cid:19)NC

,

(26)

where the second inequality is due to Bernoulli’s inequality.
The second term of the bound can be made arbitrarily small
for sparse graphs dmax (cid:28) S by appropriately choosing the
value of NC. To summarize, we have proved the following
theorem:

Theorem 1. The dynamical consensus algorithm with random
chunking has a privacy guarantee of Eq. (26). The probability
of privacy breach can be made arbitrarily small.

B. Colluded agent scenario

Another interesting question is whether the algorithm is
secure under collusion. Although the risk of collusion is
minimal in consortium-based networks, where the identity and
the motivation of the agents are known to each other, we have
the following guarantee:

Theorem 2. Let NL be the number of colluded agents. The
probability that the privacy of a non-colluded node (indexed
by s) is compromised due to collusion is given by

(cid:40)

ps
c.breach =

1 −

NL(cid:89)

(cid:18)

1 −

l=1

(cid:40)

(cid:18)

≤ exp

−NC

1 −

(cid:19)(cid:41)NC

ds
S − l

(cid:19)NL(cid:41)

ds
S − NL

(27)

(28)

which holds only for NL ≤ S − ds − 1. If there are so many
colluded nodes that NL ≥ S − ds holds, it is impossible not
to choose a colluded nodes NC times in a row and thus pL
must be zero. From Eqs. (29) and (30), we have the equality
of Theorem 2:

(cid:40)

ps
c.breach =

1 −

(cid:19)(cid:41)NC

NL(cid:89)

(cid:18)

1 −

l=1

ds
S − l

.

(31)

Next,

let us derive the upper bound. By replacing the
product in Eq. (31) with the power of the minimum term and
apply the Bernoulli inequality, we have

ps
c.breach ≤

(cid:40)

(cid:18)

1 −

1 −

(cid:40)

ds
S − NL
(cid:18)

≤ exp

−NC

1 −

(cid:19)NL(cid:41)NC

(cid:19)NL (cid:41)

.,

(32)

ds
S − NL

which completes the proof. (cid:3)

Figure 2 (a) visualizes the privacy breach probability due to
collusion (Eq. (28)) as a function of NC and NL. As expected,
the probability is almost one if NL ∼ S
2 and NC ∼ 1.
However, we also see that the risk can be made negligible
by properly choosing NC, as claimed in Theorem 2. Also,
Fig. 2 (b) shows comparison among the exact probability and
its upper bound computed by Eq. (32).

The inequality enables us to reasonably set the number of
chunks, NC, so that the privacy of all non-colluded nodes is
guaranteed with a sufﬁciently high probability, say, at least
1 − η for a small η. This corresponds to ps
c.breach ≤ η and is
translated into

(cid:18)

NC ≥ | ln η|

1 −

ds
S − NL

(cid:19)−NL

.

(33)

As long as the graph is sparse (ds (cid:28) S) and NL is expected
to be much smaller than S, a useful rule-of-thumb will be
2| ln η|. In consortium-based networks, NL should be much
smaller than S. In that case, the use of graphs having ds (cid:28) S
is critical to guarantee data privacy under the risk of collusion.
Again, this motivates us to consider a family of sparse graphs,
as discussed in the next section.

Fig. 2. Privacy breach probability due to collusion computed for 3-regular
graph of S = 100. (a) Left panel: ps
c.breach computed by Eq. (27) with a
gradation from red being 1 to white being 0. (b) Right panel: ps
c.breach (solid
lines) and its upper bound (Eq. (32); dashed lines) at a few NC values. Best
viewed in color.

Fig. 3. Privacy breach probability due to eavesdropping computed for 3-
regular graph of S = 100. (a) Left panel: ps
e.breach computed by Eq. (34) as
a function of NC and the infected edge ratio NE/E with a gradation from
red being 1 to white being 0. (b) Right panel: ps
e.breach (solid lines) and its
upper bound (Eq. (35); dashed line) at a few values of NC. Best viewed in
color.

C. Eavesdropping scenario

We summarize this result as bellow:

Eavesdropping is one of the major attacks in communi-
cation network [37]. Although the risk of eavesdropping is
limited in consortium-based networks, we can compute the
probability of privacy breach in a similar way to the incidental
privacy breach. The Shamir-based algorithm is safe even under
eavesdropping and collusion because each agent sends only
obfuscated values as Eq. (20) to the connected agents.

In the random chunking algorithm, however, we again have
a probabilistic guarantee. Consider the scenario that an eaves-
dropper picks a set of the edges and captures all the contents
of communications. Let NE be the maximum number of edges
the eavesdropper can tap. Let ps
e.breach be the probability that
the s-th agent incurs a privacy breach under eavesdropping.
A breach occurs when the eavesdropper receives all of the
NC chunks. This happens when the eavesdropper successfully
tapped any of the ds edges from the s-th agent in every
chunking round. Thus

e.breach =[1 − Pr(No edges from s are eavesdropped)]NC ,
ps

=[1 − Pr(All the NE edges are chosen

from the other E − ds edges)]NC ,

(cid:40)

=

1 −

(cid:40)

=

1 −

(cid:18)E − ds
NE

(cid:19)(cid:18) E
NE

(cid:19)−1(cid:41)NC

(cid:19)(cid:41)NC

(cid:18)

1 −

ds−1
(cid:89)

l=0

NE
E − l

(34)

where Pr(·) denotes the probability of the event speciﬁed by
the argument, and E (cid:44) (cid:80)S
i=1 di is the total number of edges
of the graph. Using the Bernoulli’s inequality as in Eq. (32),
we have

Theorem 3. The probability of privacy breach due to eaves-
dropping in the dynamic consensus algorithm with random
chunking is upper-bounded as Eq. (35).

Note that for d-regular graphs, E = dS. Since S (cid:29) ds
for sparse graphs, as long as the infected edge ratio NE
E is
expected to be much smaller than one, we can always make
ps
e.breach negligible by choosing a sufﬁciently large NC. To
keep ps

e.breach less than η > 0, we need

(cid:18)

NC ≥ | ln η|

1 −

NE
E − ds + 1

(cid:19)−ds

.

(36)

For sparse graphs, unless a majority of edges are tapped, NC ∼
2| ln η| again is a useful rule-of-thumb.

Figure 3 illustrates ps

e.breach as a function of NE/E and NC
for a 3-regular graph with S = 100. As shown, even under
massive eavesdropping where 20% of the edges are tapped,
NC = 6 gives only about 1% of breach risk.

In addition to the above case, we may consider the case
where an eavesdropper hijacks an agent and capture all the
outgoing communications. In this case, the random chunking
algorithm is no longer secure although the Shamir-based
method is still safe. This issue may be handled by combining
with a more sophisticated secret sharing scheme such as
in [38]. Exploring this topic would be an interesting future
topic.

VI. NETWORK TOPOLOGY ANALYSIS

In the previous section, we pointed out

the graph
sparsity plays an important role in privacy preservation. This
section discusses another important aspect of the network
topology: Spectral structure. Speciﬁcally, we discuss proper-
ties of a speciﬁc type of graph called the expander graph.

that

ps
e.breach ≤

(cid:40)

(cid:18)

1 −

1 −

(cid:40)

NE
E − ds + 1
(cid:18)

≤ exp

−NC

1 −

(cid:19)ds(cid:41)NC

NE
E − ds + 1

A. Estimating the number of iterations to converge

(cid:19)ds(cid:41)

.

(35)

In the dynamic consensus algorithm discussed in Sec-
tion IV-A, the convergence speed is governed by the ratio
between the ﬁrst and second absolute largest eigenvalues. Let

24681001020304050N_cN_LS=100,d=3010203040500.00.20.40.60.81.0N_Lbreach probabilityNc=1Nc=2Nc=3Nc=4Nc=5Nc=62468100.00.10.20.30.40.5N_Cinfected edge ratioS=100,d=30.00.10.20.30.40.50.00.20.40.60.81.0infected edge ratiobreach probabilityNc=1Nc=2Nc=3Nc=4Nc=5Nc=6λ1 = 1, λ2, . . . , λS be the eigenvalues of W(cid:15) arranged in the
decreasing order. For ease of exposition, let us assume that (cid:15)
has been chosen so λ2 is the absolute second largest as well,
which is always possible.

We are interested in the scalability of the algorithm in
terms of the network size S. Let us ﬁrst think about how
the convergence rate is evaluated. Equation (19) shows that

√

(cid:107)

SW(cid:15)

tξ(0)(cid:107)2 →

(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
√
S

1S

(cid:13)
(cid:13)
¯ξ
(cid:13)
(cid:13)2

= | ¯ξ|

as t → ∞, where (cid:107) · (cid:107)2 denotes the (cid:96)2 norm. Thus it makes
sense to deﬁne the relative error at iteration round t as

e(cid:15)(t) (cid:44) 1
| ¯ξ|

(cid:113)

√
(cid:107)

SW(cid:15)

tξ(0)(cid:107)2

2 − ¯ξ2

(37)

Fig. 4. Example of 3-regular expander graphs with S = 11, 19, 31 (top row)
and 47, 79, 97 (bottom row) from left to right. Note that self-loops and double
edges (thick lines) exist.

Let v2 be the (cid:96)2-normalized eigenvector corresponding to λ2.
As t grows to inﬁnity, we asymptotically have:

e(cid:15)(t) →

√

S(λ2)t ×

v(cid:62)
2 ξ(0)
¯ξ

√

∼

S(λ2)t × O(1).

(38)

√

S(λ2)t is a reasonable nondimen-
Thus, we conclude that
sional metric for convergence. To achieve a relative error δ,
we need a number of iterations as t ∼ O
. Let us
summarize this result in Theorem 4:

(cid:16) ln(

| ln λ2|

S/δ)

(cid:17)

√

Theorem 4. In the dynamical consensus algorithm, the num-
ber of iterations t to achieve a relative error δ is given by

t ∼ O

(cid:32)

√

ln(

S/δ)

| ln(1 − ∆λ)|

(cid:33)

,

(39)

where ∆λ (cid:44) λ1 − λ2.

In Eq. (39), we have used the fact that λ1 = 1 and thus
λ2 = 1 − ∆λ. In our previous work [9], we showed that the
cycle graph scales quadratically as t ∼ S2 ln S. The question
is whether we can improve this (relatively poor) scalability by
using a different communication graph. The next subsection
answers this question.

B. Expander graph

Intuitively, we expect that the more information the agents
circulate, the faster convergence we will get. The complete
graph is clearly an extreme case, where the agents share
information most generously with the peers and thus data
privacy is least protected. Let µ1, . . . , µS be the eigenvalues
of A in the decreasing order. In the complete graph, it is easy
to verify that µ1 = S − 1 and µ2 = · · · = µS = −1. Hence,
we have ∆λ = (cid:15)(µ1 − µ2) = (cid:15)S. The complete graph is a
d-regular graph with d = S − 1. By Theorem 4, with a choice
of (cid:15) ∼ O(1/d), the convergence speed measured by t scales
as ln S, which is much better than the quadratic scalability of
the cycle graph.

Is there any sparse graph that behaves like the complete
graph when communicating with the other agents? This may
sound like a ridiculous question, but surprisingly, there exists
a class of sparse graph called the expander graph that has the

same logarithmic convergence. Formally, the expander graph
is deﬁned as a graph whose expansion constant is lower-
bounded, where

(expansion constant) (cid:44) inf
V1

(cid:26)

|∂V1|
min{|V1|, S − |V1|}

(cid:27)

(40)

with V1 being an arbitrary subset of the graph nodes and
|∂V1| is the number of outgoing edges from the subgraph [39].
Intuitively,
the more
the more the expansion constant
“talkative” the nodes tend to be. Very interestingly, this purely
geometric deﬁnition has a hidden connection to the graph
spectrum, and, in our case, we have

is,

∆λ ≥ (cid:15)

α2
2d

(d-regular expander graphs)

(41)

by Cheeger’s inequality [40], where α is the lower bound of
the expansion constant. Thus we again have the logarithmic
scalability t ∼ ln S with a choice of (cid:15) ∼ O(1/d). We now
summarize the result as follows.

Theorem
t ∼ O(ln(

√

5.

On

d-regular

expander

graphs,

S/δ)) with a choice of (cid:15) ∼ O(1/d).

Construction of expander graphs is nontrivial. Fortunately,
there are two ways available to obtain expander graphs for our
purpose. The ﬁrst one is a 3-regular expander graph called the
cycle with inverse chords [13]. As the name suggests, it starts
with the cycle graph (2-regular graph), and connects each node
s to the nodes j = s ± 1, (s − 1)(j − 1) = 1 mod S for a
prime S. See Figs. 1 and 4 for a few examples. As mentioned
before, the node indices need to be randomly shufﬂed prior
to each round of the random chunking algorithm. Since the
eigenspectrum is invariant to re-labeling the nodes, it does not
affect the convergence of dynamic consensus.

The other possible approach is to use a random graph. It
is known that uniformly sampled d-regular random graphs
approximate ∆λ of the expander graph Eq. (41) almost always
perfectly [41]. To construct such a graph, we can leverage the
conﬁguration model in random graph theory [42]. Speciﬁcally,
each agent simply picks d neighbors randomly and uniformly
to form a d-regular graph. This approach is preferable in

Fig. 5. Comparison of the number of iterations t for δ = 10−3.

the sense that it does not require any intervention of the
network router and does not assume network connection that
is stable throughout the entire consensus process. On the other
hand, however, the randomness in the network structure may
lead to some unpredictability in the eigenspectrum, which is
in contrast to the ﬁrst option. To avoid extra randomness,
we use the cycle with inverse chords in the empirical study
in the next section. Evaluating and designing random graph
construction algorithms in dynamic consensus is an interesting
future research topic.

VII. EXPERIMENTS

This section reports on experimental results of the proposed
model. All the experiments were conducted locally on a laptop
PC with a Core i7 Processor and 32 GB memory.

A. Number of iterations to converge

Figure 5 compares between the expander graph (cycle with
a random chord) and the 2-regular cycle graph (or the ring) on
the number of iterations t to achieve δ = 10−3 as deﬁned in
Theorem 4 and 5. It is interesting to observe that just adding an
extra edge to the ring by the rule (s − 1)(j − 1) = 1 mod S
drastically changes the convergence behavior. As mentioned
before, the ring scales quadratically as t ∼ S2 ln S. In contrast,
t grows very slowly in the expander graph, being consistent
to the logarithmic scalability predicted by Theorem 5.

The original construction of the 3-regular expander graph
is for S that is a prime. One interesting question in practice is
that the excellent convergence behavior is maintained for non-
prime S’s. We extended the original construction by giving a
self-loop whenever the equation (s − 1)(j − 1) = 1 mod S
the graph is always 3-
does not have a solution, so that
regular. Fortunately, apart from the visible ﬂuctuations, the
ﬁgure suggests that
the overall convergence behaviors are
robust and we should be able to safely use the model even
for non-primes.

Fig. 6. Actual computation time for aggregation on the 3-regular expander
graph. The right panel covers the range of 7 ≤ S ≤ 53.

B. Shamir vs. random chunking

Since one iteration needs one matrix-vector multiplication
(see Eq. (18)), actual computational time may have a different
scalability from that of the number of iterations to converge.
Speciﬁcally, one may expect an extra S ¯d factor, where ¯d is
the mean degree.

Figure 6 compares actual computation times in different
NCs. We randomly initialized ξs(0) with the uniform distri-
bution in [−1, 2]. Convergence was declared when the relative
error gets smaller than 10−5. The mean and standard deviation
(s.d.) were computed by repeating computation by 10 times
with a different random seed. The ﬁgure conﬁrms the expected
linear dependency in the random chunking algorithm.

Figure 6 also compares the Shamir’s secret sharing algo-
rithm with the random chunking algorithm. Since the Shamir-
based method needs to repeat the process of global consensus
S times, the actual computation time is expected to depend
quadratically on S, which is consistent with the ﬁgure. Apart
from S (cid:46) 10, where the both methods are comparable,
the Shamir-based method tended to have a larger variability
than the random chunking algorithm, which might suggest
numerical issues in the implementation.

Finally, we comment on the origin of non-smoothness of the
computational time in Fig. 6. In the ﬁgure, close inspection
shows that the computational times are correlated: whenever
the Shamir-based method gets much time, so does the random
chunking method. This suggests that the source of the non-
smoothness is in the spectral structure of the underlying
network because the same dynamical consensus algorithm was
shared by both. Although we have obtained a very solid result
on the convergence behavior, how the spectral gap ∆λ scales
as S increases is not straightforwardly predictable. Further
investigation on this point is left to future work.

C. Shamir vs. homomorphic encryption

Table I compares computation time for a few small S’s
between the proposed Shamir-based method and homomor-
phic encryption (HE)-based method of [28]. Both approaches
have cryptographic security and share the same dynamical
consensus algorithm. For a fair comparison, we ran both on
the same expander graph and used the same experimental
design as above. For HE, we used an implementation of the

020040060080010000200600# iterationsexpander020040060080010000e+004e+058e+05# iterationsringS02004006008001000012345Sactual time [s]ShamirNc=2Nc=4Nc=81020304050012345Sactual time [s]ShamirNc=2Nc=4Nc=8TABLE I
COMPARISON OF ACTUAL COMPUTATION TIMES IN AGGREGATION [SEC].

Shamir

HE

mean

0.159
0.485
0.458
0.671
2.24

s.d.

mean

s.d.

0.024
0.045
0.020
0.060
0.156

155
288
375
574
699

18.8
44.1
24.8
55.1
47.0

S

7
11
13
17
19

Paillier cryptosystem [43], which encrypts and decrypts every
communication between the agents with a new key. From the
table, and in the light of Fig. 6, we conclude that the proposed
random chunking-based method is several orders of magnitude
faster than the HE-based alternative.

D. Remarks on synchronization issues

In Section VII-B, we mentioned that actual computational
time of the random chunking algorithm on the expander
graph should have a linear dependency on S. This statement
implicitly assumes that computation is made sequentially.
Although in theory it is true that the agents can perform
updates in parallel, sequential execution should be a reason-
able assumption for evaluating computational time. In real
distributed environments, we always need to handle the issue
of synchronization across the network. If the cost of local
computation is negligible, network delays will almost always
dominate the time required to move on to the next iteration.
In that case, most of the agents would spend most of the time
waiting for the last message to be delivered to one of the
agents.

On the other hand, if the computational cost is on average
much higher than network delays, we will need to consider
parallelization as a realistic option. This is actually the case
in homomorphic encryption (HE)-based secure computation,
as recently pointed out in [27]. There may also be some room
for improvement in the HE implementation itself, where we
employed homomorpheR [43] with the key size of 1 024 to
follow the protocol proposed in [28], [44], although handling
signed ﬂoating-point numbers can be a subtle issue (see,
e.g. [45]). The key exchange protocol can be simpliﬁed, too.
Regarding Table I, it would be an interesting future research
topic to compare the random chunking method with a highly
optimized HE implementation in a realistic setting.

VIII. CONCLUDING REMARKS

We have presented new research directions of Blockchain
as a collaborative value co-creation platform rather than a
mere immutable data storage. Our platform is designed to
respect the values of democracy, diversity, and privacy in its
collaborative learning process. As such an instance, we have
proposed a multi-task federated learning framework combined
with a global consensus-building algorithm.

We discussed two topics in the global consensus-building
process. The ﬁrst topic was data privacy. We proposed two
secure consensus-building approaches built upon the idea of

secret sharing: The Shamir-based dynamic consensus, which
has a cryptographic security guarantee, and the random chunk-
ing algorithm, which falls into the category of the probabilistic
ﬁnality protocol. For the latter, we have provided the upper
bound of privacy breach under three different attack scenarios.
The second topic discussed was the issue of network design
in global consensus. Based on the profound results in spectral
graph theory, we showed that expander graphs dramatically
improve the scalability of the algorithm.

We conclude this paper by summarizing a few future

research topics:

a) Learning under network errors: We have assumed
perfectly synchronized and stable communication among the
agents. As suggested in Sections VI-B and VII-D, extensions
to include network errors is of primary importance to make
our platform truly useful.

b) Meta-agreement

issues: Another potential

issue in
practice is how to agree on the learning task itself (choice of
the algorithm, data dimensionality M , the deﬁnition of each
dimension, etc.) and how to initiate peer-to-peer communica-
tion (who to communicate with).

c) External data privacy: One interesting business sce-
nario is to sell the learned model to external parties. This calls
for a guarantee that is different from the internal data privacy
among the agents. It is known that the classical concept of
differential privacy has issues in evaluating noisy real-valued
variables [46]. Establishing a method of evaluating the degree
of information leak is important.

d) Randomness in graph spectra: As discussed in Sec-
tion VI-B, the origin of the non-smoothness shown in Fig. 6
and the consensus algorithm based on random graphs are still
an open question.

e) Security analysis: The random chunking algorithm
combined with the dynamic consensus algorithm appears to
have more ﬂexibility than traditional cryptographic methods.
We need to study further the pros and cons of those methods.
f) Use-cases: Finally, we need to develop practical use-
cases where the decentralized architecture is truly useful. The
lightweight probabilistic privacy guarantee seems suitable in
internet-of-things (IoT) applications [29], but more study is
needed. We hope that, just as Nakamoto’s Bitcoin changed
the landscape of ﬁnancial transaction management, our decen-
tralized collaborative learning platform as the next-generation
Blockchain has the potential to change the way of doing
business.

ACKNOWLEDGEMENT

T.I. thank Dr. Sachiko Yoshihama for her insightful suggestions.
T.I. is partially supported by the Department of Energy National En-
ergy Technology Laboratory under Award Number DE-OE0000911.
A part of this report was prepared as an account of work sponsored by
an agency of the United States Government. Neither the United States
Government nor any agency thereof, nor any of their employees,
makes any warranty, express or implied, or assumes any legal liability
or responsibility for the accuracy, completeness, or usefulness of any
information, apparatus, product, or process disclosed, or represents

that its use would not infringe privately owned rights. Reference
herein to any speciﬁc commercial product, process, or service by trade
name, trademark, manufacturer, or otherwise does not necessarily
constitute or imply its endorsement, recommendation, or favoring by
the United States Government or any agency thereof. The views and
opinions of authors expressed herein do not necessarily state or reﬂect
those of the United States Government or any agency thereof.

REFERENCES

[1] D. Tse, B. Zhang, Y. Yang, C. Cheng, and H. Mu, “Blockchain applica-
tion in food supply information security,” in Proceeding of 2017 IEEE
International Conference on Industrial Engineering and Engineering
Management (IEEM 2017).

IEEE, 2017, pp. 1357–1361.

[2] Q. Lu and X. Xu, “Adaptable blockchain-based systems: a case study
for product traceability,” IEEE Software, vol. 34, no. 6, pp. 21–27, 2017.
[3] K. Toyoda, P. T. Mathiopoulos, I. Sasase, and T. Ohtsuki, “A novel
blockchain-based product ownership management system (poms) for
anti-counterfeits in the post supply chain,” IEEE Access, vol. 5, pp.
17 465–17 477, 2017.

[4] K. Christidis and M. Devetsikiotis, “Blockchains and smart contracts for
the internet of things,” IEEE Access, vol. 4, pp. 2292–2303, 2016.
[5] O. Scekic, S. Nastic, and S. Dustdar, “Blockchain-supported smart city
platform for social value co-creation and exchange,” IEEE Internet
Computing, vol. 23, no. 1, pp. 19–28, 2019.

[6] S. Seebacher and R. Sch¨uritz, “Blockchain technology as an enabler
of service systems: A structured literature review,” in International
Conference on Exploring Services Science, 2017, pp. 12–23.

[7] G. Kondrateva, E. de Boissieu, C. Ammi, and E. Seulliet, “The potential
use of blockchain technology in co-creation ecosystems,” Journal of
Innovation Economics Management, pp. I104–27, 2021.

[8] S. Nakamoto, “Bitcoin: A peer-to-peer electronic cash system,” Preprint.

https:// bitcoin.org/ bitcoin.pdf , 2008.

[9] T. Id´e, R. Raymond, and D. T. Phan, “Efﬁcient protocol for collaborative
dictionary learning in decentralized networks.” in Proceeding of the
28th International Joint Conference on Artiﬁcial Intelligence (IJCAI-
19), 2019, pp. 2585–2591.

[10] C. W. Wu, “Agreement and consensus problems in groups of au-
tonomous agents with linear dynamics,” in Proceedings of the 2005
IEEE International Symposium on Circuits and Systems (ISCAS).
IEEE,
2005, pp. 292–295.

[11] W. Ren, R. W. Beard, and E. M. Atkins, “A survey of consensus
the 2005

problems in multi-agent coordination,” in Proceedings of
American Control Conference.
IEEE, 2005, pp. 1859–1864.

[12] R. Olfati-Saber, J. A. Fax, and R. M. Murray, “Consensus and coop-
eration in networked multi-agent systems,” Proceedings of the IEEE,
vol. 95, no. 1, pp. 215–233, 2007.

[13] S. P. Vadhan et al., “Pseudorandomness,” Foundations and Trends in
Theoretical Computer Science, vol. 7, no. 1–3, pp. 1–336, 2012.
[14] A. Lubotzky, Discrete groups, expanding graphs and invariant measures.

Springer Science & Business Media, 2010.

[15] P. Mohassel and Y. Zhang, “SecureML: A system for scalable privacy-
preserving machine learning,” in Proceedings of the 2017 IEEE Sympo-
sium on Security and Privacy (SP).

IEEE, 2017, pp. 19–38.

[16] B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y. Arcas,
“Communication-efﬁcient learning of deep networks from decentralized
data,” in Proceedings of the 20th International Conference on Artiﬁcial
Intelligence and Statistics (AISTATS), 2017, pp. 1273–1282.

[17] J. Koneˇcn´y, H. B. McMahan, F. X. Yu, P. Richt´arik, A. T. Suresh, and
D. Bacon, “Federated learning: Strategies for improving communication
efﬁciency,” in NIPS Workshop on Private Multi-Party Machine Learning,
2016.

[18] N. Agarwal, A. T. Suresh, F. Yu, S. Kumar, and H. B. McMahan,
“cpSGD: Communication-efﬁcient and differentially-private distributed
SGD,” in Advances in Neural Information Processing Systems, 2018,
pp. 7575–7586.

[19] Q. Yang, Y. Liu, T. Chen, and Y. Tong, “Federated machine learning:
Concept and applications,” ACM Transactions on Intelligent Systems and
Technology (TIST), vol. 10, no. 2, p. 12, 2019.

[20] M. Xu, B. Lakshminarayanan, Y. W. Teh, J. Zhu, and B. Zhang, “Dis-
tributed Bayesian posterior sampling via moment sharing,” in Advances
in Neural Information Processing Systems, 2014, pp. 3356–3364.

[21] M. Mohri, G. Sivek, and A. T. Suresh, “Agnostic federated learning,” in
Proceedings of the 36th International Conference on Machine Learning
(ICML 2019), 2019, pp. 4615–4625.

[22] V. Smith, C.-K. Chiang, M. Sanjabi, and A. S. Talwalkar, “Federated
multi-task learning,” in Advances in Neural Information Processing
Systems, 2017, pp. 4424–4434.

[23] G. Bernstein and D. R. Sheldon, “Differentially private Bayesian in-
ference for exponential families,” in Advances in Neural Information
Processing Systems, 2018, pp. 2924–2934.

[24] L. Xie, I. M. Baytas, K. Lin, and J. Zhou, “Privacy-preserving distributed
multi-task learning with asynchronous updates,” in Proceedings of the
23rd ACM SIGKDD International Conference on Knowledge Discovery
and Data Mining. ACM, 2017, pp. 1195–1204.

[25] M. Heikkil¨a, E. Lagerspetz, S. Kaski, K. Shimizu, S. Tarkoma, and
A. Honkela, “Differentially private Bayesian learning on distributed
data,” in Advances in neural information processing systems, 2017, pp.
3226–3235.

[26] B. Ding, H. Nori, P. Li, and J. Allen, “Comparing population means
under local differential privacy: with signiﬁcance and power,” in Pro-
ceedings of the Thirty-Second AAAI Conference on Artiﬁcial Intelligence
(AAAI-18), 2018.

[27] Y. Liu, Y. Kang, C. Xing, T. Chen, and Q. Yang, “A secure federated
transfer learning framework,” IEEE Intelligent Systems, vol. 35, no. 4,
pp. 70–82, 2020.

[28] M. Ruan, M. Ahmad, and Y. Wang, “Secure and privacy-preserving
average consensus,” in Proceedings of the 2017 Workshop on Cyber-
Physical Systems Security and Privacy. ACM, 2017, pp. 123–129.
[29] T. Id´e, “Collaborative anomaly detection on blockchain from noisy
sensor data,” in Proceeding of the 2018 IEEE International Conference
on Data Mining Workshops (ICDMW).

IEEE, 2018, pp. 120–127.

[30] E. L. Lehmann and G. Casella, Theory of point estimation.

Springer

Science & Business Media, 2006.

[31] J. Friedman, T. Hastie, and R. Tibshirani, “Sparse inverse covariance
estimation with the graphical lasso,” Biostatistics, vol. 9, no. 3, pp. 432–
441, 2008.

[32] C.-J. Hsieh, M. A. Sustik, I. S. Dhillon, and P. Ravikumar, “QUIC:
quadratic approximation for sparse inverse covariance estimation,” Jour-
nal of Machine Learning Research, vol. 15, no. 1, pp. 2911–2947, 2014.
[33] G. Strang, Linear Algebra and its Applications. Academic Press, 1976.
[34] A. Shamir, “How to share a secret,” Communications of the ACM,

vol. 22, no. 11, pp. 612–613, 1979.

[35] D. Boneh and V. Shoup, “A graduate course in applied cryptography,”

Draft of a book, version 0.3, December, 2016.

[36] Y. Xiao, N. Zhang, W. Lou, and Y. T. Hou, “A survey of distributed
consensus protocols for blockchain networks,” IEEE Communications
Surveys & Tutorials, vol. 22, no. 2, pp. 1432–1465, 2020.

[37] C. Karlof and D. Wagner, “Secure routing in wireless sensor networks:
Attacks and countermeasures,” in Proceedings of the First IEEE Interna-
tional Workshop on Sensor Network Protocols and Applications, 2003.
IEEE, 2003, pp. 113–127.

[38] N. Gupta, J. Katz, and N. Chopra, “Privacy in distributed average
consensus,” IFAC-PapersOnLine, vol. 50, no. 1, pp. 9515–9520, 2017.
[39] G. Davidoff, P. Sarnak, and A. Valette, Elementary number theory, group

theory and Ramanujan graphs. Cambridge University Press, 2003.

[40] A. E. Brouwer and W. H. Haemers, Spectra of graphs. Springer Science

& Business Media, 2011.

[41] J. Friedman, “A proof of Alon’s second eigenvalue conjecture,” in
Proceedings of the Thirty-Fifth Annual ACM Symposium on Theory of
Computing (STOC ’03), 2003, p. 720–724.

[42] J. Kim and V. Vu, “Generating random regular graphs,” Combinatorica,

vol. 26, no. 6, pp. 683–708, 2006.

[43] B. Narasimhan, “homomorpheR,” in CRAN (The Comprehensive R

Archive Network), 2019.

[44] M. Ruan, H. Gao, and Y. Wang, “Secure and privacy-preserving con-

sensus,” IEEE Transactions on Automatic Control, 2019.

[45] J. H. Cheon, A. Kim, M. Kim, and Y. Song, “Homomorphic encryption
for arithmetic of approximate numbers,” in Proc. the 2017 Intl. Conf.
the Theory and Application of Cryptology and Information Security
(ASIACRYPT 2017). Springer, 2017, pp. 409–437.

[46] K. Minami, H. Arai, I. Sato, and H. Nakagawa, “Differential privacy
without sensitivity,” in Advances in Neural Information Processing
Systems, 2016, pp. 956–964.

