2
2
0
2

t
c
O
3
1

]

C
D
.
s
c
[

4
1
v
0
5
6
3
1
.
2
1
1
2
:
v
i
X
r
a

Multiagent Transition Systems with Safety and
Liveness Faults: A Compositional Foundation for
Fault-Resilient Distributed Computing

Ehud Shapiro

Weizmann Institute of Science, Rehovot, Israel
ehud.shapiro@weizmann.ac.il

Abstract. We present a novel mathematical framework for the speci-
ﬁcation and analysis of distributed computing systems and their imple-
mentations, with the following components:
1. Transition systems that allow the speciﬁcation and analysis of com-
putations with safety and liveness faults and their fault resilience.
2. Notions of safe, live and complete implementations among transition
systems and their composition, with which the correctness (safety
and liveness) and completeness of a protocol stack as a whole fol-
lows from each protocol implementing correctly and completely the
protocol above it in the stack.

3. Applying the notion of monotonicity, pertinent to histories of dis-
tributed computing systems, to ease the speciﬁcation and proof of cor-
rectness of implementations among distributed computing systems.
4. Multiagent transition systems, further characterized as centralized/dis-
tributed and synchronous/asynchronous; safety and liveness fault-
resilience of implementations among them and their composition.
5. An algebraic and operational characterization of a protocol (family
of distributed multiagent transition systems) being grassroots, which
means that the protocol may be deployed independently at multiple
locations and over time, and that such deployments can subsequently
interoperate once interconnected; suﬃcient conditions for a protocol
to be grassroots; and the notion of a grassroots implementation among
protocols.

The framework is being employed in the speciﬁcation of a grassroots or-
dering consensus protocol stack [32], with sovereign cryptocurrencies [33],
an NFT trade protocol [33], and an eﬃcient Byzantine atomic broadcast
protocol [19] as the ﬁrst applications.

Keywords: Distributed Computing · Multiagent Transition Systems ·
Fault Resilience · Protocol Stack

1

Introduction and Related Work

This paper presents a mathematical framework for specifying and proving in a
compositional way the correctness and fault-resilience of a distributed protocol

 
 
 
 
 
 
2

Ehud Shapiro

stack. Diﬀerent aspects of this problem have been addressed for almost half a
century.

Process calculi have been proposed for the compositional speciﬁcation and
proof of concurrent systems [16,25,26], mostly focusing on synchronous commu-
nication, although variants for asynchronous distributed computing have been
investigated [3,10], including their resilience to fail-stop failures [11]

Transition systems are a standard way of specifying computing systems with-
out committing to a speciﬁc syntax. The use of transition systems for the
speciﬁcation of concurrent and distributed systems has been investigated ex-
tensively [15,1,24], including the notion of implementations among transition
systems and their composition [2,24,17]. The composition of implementations
has been investigated in the context of multi-phase compilation [22,28], where
the correctness of the compiler as a whole following from the correctness of each
phase in the compilation. Due to the deterministic and centralized nature of
compilation, this task did not require addressing questions of liveness, complete-
ness, and fault tolerance. Transition systems have been also employed to specify
and prove the fault-resilience of distributed systems [35].

Fault-resilient distributed computing, especially the problems of Byzantine
Agreement [34], Byzantine Reliable Broadcast [4,13,7], Byzantine Atomic Broad-
cast (ordering consensus)[36,18,12], and blockchain consensus [27], have been
investigated extensively. Methods for reasoning about distributed systems have
been developed [20,31,23], including their fault resilience [35], and formal frame-
works for the speciﬁcation and proof of distributed systems were developed [24,21,35].
However, the reality is that novel protocols and their proofs, e.g.
[6,36,18,12,8],
are typically presented outside any formal framework, probably due to the sheer
complexity of the protocols and their proofs.

To the best of our knowledge, a mathematical framework for specifying and
proving in a compositional way the correctness and fault-resilience of distributed
protocol stack, in which each protocol implements the protocol above it and
serves as a speciﬁcation for the protocol below it, is novel. We developed the
framework with the goal of specifying and proving the correctness and fault-
resilience of a particular protocol stack: One that commences with with an open
dissemination protocol that can support the grassroots formation of a peer-to-
peer social network; continues with a protocol for equivocation exclusion that
can support sovereign cryptocurrencies and an equivocation-resilient NFT trade
protocol [33]; and culminates in a group consensus protocol for ordering trans-
actions despite Byzantine faults, namely Byzantine Atomic Broadcast [19].

Here, we present, prove correct, and analyze the fault-resilience of two ab-
stract protocol stacks, depicted in Figure 1, as example applications of the math-
ematical framework. A more concrete, complex, and practical protocol stack
based on the blocklace (a partially-ordered generalization of the blockchain) is
presented and analyzed elsewhere [32], using the mathematical framework de-
veloped here.

A key objective of this work is the development grassroots protocols that can
be deployed independently at diﬀerent locations and over times, initially with

Multiagent Transition Systems with Faults

3

disjoint communities operating the protocol independently, and over time—once
connected–forming an ever-growing interacting networked community. Here we
characterize the notion of a protocol being grassroots algebraically and opera-
tionally, analyze whether protocols in the abstract protocol stack are grassroots,
and discuss whether client-server protocols (e.g., all major digital platforms),
consensus protocols (e.g. reliable broadcast, Byzantine agreement), majoritarian
decision making protocols (e.g. democratic voting), and protocols that employ
a non-composable data structure (e.g., blockchain), are grassroots, and if not,
then whether and how can they be made so.

Fig. 1. Example Transition Systems-Based Protocol Stacks. A. Generic (Example 1),
Single-Chain (Ex. 2), and Longest Chain (Ex. 3) transition systems and their imple-
mentations σ1 (Proposition 2) and σ2 (Def. 10 and Prop. 11). B. Generic Shared-
Memory (Ex. 4), Single-Chain Consensus (Ex. 5), Longest-Chain Consensus (Ex. 6),
and Asynchronous Block Dissemination (Ex. 7) multiagent transition systems and their
implementations σ1m (Prop. 6), σ2m (Prop. 7), and σ3 (Prop. 10).

Our approach is diﬀerent from that of universal composability [5], devised for
the analysis of cryptographic protocols, in at least two respects: First, it does not
assume, from the outset, a speciﬁc notion of communication. Second, its notion
of composition is diﬀerent: Universal composability uses function composition
as is common in the practice of protocol design (e.g. [18,7,30]). Here, we do not
compose protocols, but compose implementations among protocols, resulting in
a new single implementation that realizes the high-level protocol using the prim-
itives of the low-level protocol. For example, it seems that the universality results
of Sections 2 and 3 cannot be expressed in the model of universal composability.
Grassroots composition is an instance of parallel composition, a notion that
has been explored extensively [16,25,26,21,24]: Grassroots composition takes
components that have the capacity to operate independently of each other,
and ensures that they can still operate independently even when composed,

4

Ehud Shapiro

but also interact in novel ways after composition. While grassroots composition
leaves open the possibility that components may interact once composed, it does
not specify how they might interact; such interactions are a consequence of the
speciﬁcs of the composed transition systems.

IN the rest of the paper Section 2 presents transition systems, implementa-
tions among them, and the composition of such implementations, and includes
the example protocol stack of Figure 1A. It also introduces the notion of mono-
tonicity of transition systems [29,14], and shows that it can ease the proof of cor-
rectness of an implementation. Section 3 presents multiagent transition systems,
further characterized as centralized or distributed, with the latter being syn-
chronous or asynchronous, and includes the example multiagent protocol stack
of Figure 1B. Section 4 introduces safety faults and liveness faults, implementa-
tions that are resilient to such faults, and their composition. Section 5 introduces
the notion of a family of multiagent transition systems, aka protocol, introduces
grassroots protocols, their characterization and implementation. Section 6 con-
cludes the paper. Proof are relegated to Appendix A.

2 Transition Systems, Implementations and their

Composition

Here, we introduce the notions of transition systems, implementations among
them, and their composition, together with the examples of Figure 1A.

2.1 Transition Systems and Their Implementation

Given a set S, S∗ denotes the set of sequences over S, S+ the set of nonempty
sequences over S, and Λ the empty sequence. Given x, y ∈ S∗, x · y denotes
the concatenation of x and y, and x (cid:22) y denotes that x is a preﬁx of y. Two
sequences x, y ∈ S∗ are consistent if x (cid:22) y or y (cid:22) x, inconsistent otherwise.

Deﬁnition 1 (Transition System, Computation, Run). Given a set S,
referred to as states, the transitions over S are all pairs (s, s(cid:48)) ∈ S2, also
written s → s(cid:48). A transition system T S = (S, s0, T, λ) consists of a set of
states S, an initial state s0 ∈ S, a set of correct transitions T ⊆ S2, and
a liveness condition λ which is a set of sets correct transitions; when λ is
omitted the default liveness condition is λ = {T }. A computation of T S is a
sequence of transitions r = s −→ s(cid:48) −→ · · · ⊆ S2. A run of T S is a computation
that starts from s0.

Recall that safety requires that bad things don’t happen, and liveness that
good things do happen, eventually. For example, “a transition that is enabled
inﬁnitely often is eventually taken”. Heraclitus said that you cannot step into
the same river twice. Similarly, in a transition system you cannot take the same
transition in diﬀerent states as, by deﬁnition, it is a diﬀerent transition. Hence, a
liveness requirement is on a set of transitions, rather than a single transition. For

Multiagent Transition Systems with Faults

5

example, the set of all transitions in which ‘p receives message m from q’, even
if the state of p or of other agents changes. In multiagent transition systems,
deﬁned below, liveness may require each agent to act every so often. In such a
case we consider all transitions by the same agent for the liveness requirement.
To specify the liveness condition, λ considers sets of correct transitions with a
liveness requirement placed on each set.

Deﬁnition 2 (Safe, Live and Correct Run). Given a transition system
T S = (S, s0, T, λ), a computation r is safe, also r ⊆ T , if every transition
of r is correct, and s ∗−→ s(cid:48) ⊆ T denotes the existence of a safe computation
(empty if s = s(cid:48)) from s to s(cid:48).

A transition s(cid:48) → s(cid:48)(cid:48) ∈ S2 is enabled on s if s = s(cid:48). A run is live wrt
L ∈ λ if either r has a nonempty suﬃx in which no transition in L is enabled,
or every suﬃx of r includes an L transition. A run r is live if it is live wrt every
L ∈ λ. A run r is correct if it is safe and live.

Observation 1 (Final State) A state is ﬁnal if no correct transition is en-
abled on it. A live computation is ﬁnite only if its last state is ﬁnal.

The following is an example of a generic transition system over a given set
of states. Here and in the other examples in this section the liveness condition
λ is omitted and a computation is live if it is live wrt the correct transitions.

Example 1 (G: Generic). Given a set of states S with a designated initial state
s0 ∈ S, a generic transition system over S is G = (S, s0, T G) for some T G ⊆ S2.

Deﬁnition 3 (Speciﬁcation; Safe, Live, Correct and Complete Imple-
mentation). Given two transition systems T S = (S, s0, T, λ) (the speciﬁca-
tion) and T S(cid:48) = (S(cid:48), s(cid:48)
0, T (cid:48), λ(cid:48)), an implementation of T S by T S(cid:48) is a func-
0) = s0, in which case the pair (T S(cid:48), σ) is referred
tion σ : S(cid:48) → S where σ(s(cid:48)
1 → s(cid:48)
to as an implementation of T S. Given a computation r(cid:48) = s(cid:48)
2 → . . .
of T S(cid:48), σ(r(cid:48)) is the (possibly empty) computation σ(s(cid:48)
1) → σ(s(cid:48)
2) → . . ., with
i) = σ(s(cid:48)
stutter transitions in which σ(s(cid:48)
i+1) removed. The implementation
(T S(cid:48), σ) of T S is safe/live/correct if σ maps every safe/live/correct T S(cid:48) run
r(cid:48) to a safe/live/correct T S run σ(r(cid:48)), respectively, and is complete if every
correct run r of T S has a correct run r(cid:48) of T S(cid:48) such that σ(r(cid:48)) = r.

Deﬁnition 4 (σ: Locally Safe, Productive, Locally Complete). Given
0, T (cid:48), λ(cid:48)) and an im-
two transition systems T S = (S, s0, T, λ) and T S(cid:48) = (S(cid:48), s(cid:48)
plementation σ : S(cid:48) (cid:55)→ S. Then σ is:
1. Locally Safe if s(cid:48)
0
1) and x2 = σ(x(cid:48)

2) in S. If x1 = x2 then the T (cid:48) transition x(cid:48)

∗−→ y1 −→ y2 ⊆ T (cid:48) implies that s0

∗−→ x2 ⊆ T for
1 −→ x(cid:48)
2

∗−→ x1

x1 = σ(x(cid:48)
stutters T .

2. Productive if for every L ∈ λ and every correct run r(cid:48) of T S(cid:48), either r(cid:48) has
a nonempty suﬃx r(cid:48)(cid:48) such that L is not enabled in σ(r(cid:48)(cid:48)), or every suﬃx r(cid:48)(cid:48)
of r(cid:48) activates L, namely σ(r(cid:48)(cid:48)) has an L-transition.

3. Locally Complete if s0

2 ∈ S(cid:48) such that x1 = σ(x(cid:48)

∗−→ x1 −→ x2 ⊆ T , implies that s(cid:48)
0
1) and x2 = σ(x(cid:48)
2).

for some x(cid:48)

1, x(cid:48)

∗−→ x(cid:48)
1

∗−→ x(cid:48)

2 ⊆ T (cid:48)

6

Ehud Shapiro

Proposition 1 (σ Correct). If an implementation σ is locally safe and produc-
tive then it is correct, and if in addition it is locally complete then it is complete.

Intuitively, in an implementation (T S(cid:48), σ) of T S, T S(cid:48) can be thought of as the
‘virtual hardware’ (e.g. the instruction set of a virtual machine or the machine
language of an actual machine) and σ as specifying a ‘compiler’, that compiles
programs in the high-level language T S into machine-language programs in T S(cid:48).
The mapping σ from T S(cid:48) to T S is in inverse direction to that of a compiler;
it thus speciﬁes the intended behavior of compiled programs in terms of the
behavior of their source programs, and in doing so can serve as the basis for
proving a compiler correct. Note, though, that transition systems have no formal
syntax, and can be thought of as specifying the operational semantics of existing
or hypothetical programming languages.

Preparing an example implementation, we present the universal single-chain
transition system SC, and then show how it can implement any generic transition
system G, justifying the title ‘universal’.

Example 2 (SC: Single-Chain). Given a set S with a designated initial state
s0 ∈ S, the single-chain transition system over S is SC = (S+, s0, T SC), where
T SC includes every transition x → x · s for every x ∈ S∗ and s ∈ S.

Namely, an SC run can generate any sequence over S.

From a programming-language perspective, some transition systems we will
be concerned with are best viewed as providing the operational semantics for a
set of programs over a given domain. With this view, in the current abstract set-
ting, the programming of a transition system, namely choosing a program from
this potentially-inﬁnite set of programs, is akin to identifying a (computable)
subset of the transition system. In our example, for the universal single-chain
transition system SC to implement a speciﬁc instance of the generic transition
system G, a subset of SC has to be identiﬁed that corresponds to the transitions
of G, as shown next. But ﬁrst we deﬁne the notion of a transition system subset.

Deﬁnition 5 (Transition System Subset). Given a transition system T S =
(S, s0, T, λ), a transition system T S(cid:48) = (S(cid:48), s(cid:48)
0, T (cid:48), λ(cid:48)) is a subset of T S, T S(cid:48) ⊆
T S, if s(cid:48)

0 = s0, S(cid:48) ⊆ S, T (cid:48) ⊆ T , and λ(cid:48) is λ restricted to T (cid:48).

The deﬁnition suggests at least two speciﬁc ways to construct a subset: Choosing
a subset of the states and restricting the transitions to be only among these
states; or choosing a subset of the transitions. Speciﬁcally, (i ) Choose some
S(cid:48) ⊂ S and deﬁne T (cid:48) := T /S(cid:48), namely T (cid:48) := {(s −→ s(cid:48) ∈ T : s, s(cid:48) ∈ S(cid:48)}. (ii )
Choose some T (cid:48) ⊂ T . We note that in practice there must be restrictions on the
choice of a subset; to begin with, S(cid:48) and T (cid:48) should be computable.

We want to show that the universal single-chain transition system can im-

plement any generic transition system. Hence the following deﬁnition:

Deﬁnition 6 (Can Implement). Given transition systems T S = (S, s0, T, λ),
0, T (cid:48), λ(cid:48)), T S(cid:48) can implement T S if there is a subset T S(cid:48)(cid:48) =
T S(cid:48) = (S(cid:48), s(cid:48)
0, T (cid:48)(cid:48), λ(cid:48)(cid:48)), T S(cid:48)(cid:48) ⊆ T S(cid:48) and a correct and complete implementation σ :
(S(cid:48)(cid:48), s(cid:48)
S(cid:48)(cid:48) (cid:55)→ S of T S by T S(cid:48)(cid:48).

Multiagent Transition Systems with Faults

7

Next we demonstrates the application of the deﬁnitions above:

Proposition 2. The single-chain transition system SC over S can implement
any generic transition system G over S.

2.2 Composing Implementations

The key property of correct and complete implementations is their transitivity:

Proposition 3 (Transitivity of Correct & Complete Implementations).
The composition of safe/live/correct/complete implementations is safe/live/cor-
rect/complete, respectively.

Our next example is the longest-chain transition system, which can be viewed
as an abstraction of the longest-chain consensus protocols (e.g. Nakamoto [27]),
since its consistency requirement entails that only the longest chain may be freely
extended; other chains are bound to copy their next sequence element from a
longer chain till they catch up, if ever, and only then may contribute a new
element to the chain.

Example 3 (LC: Longest-Chain). Given a set S and n > 0, the LC longest-chain
transition system over S, LC = ((S∗)n, c0, T LC), has sets of n sequences over
S as states, referred to as n-chain conﬁgurations over S, initial state c0 = Λn,
and as transitions T LC every c → c(cid:48) where c(cid:48) is obtained from c by extending
one sequence x ∈ c to x · s, s ∈ S, provided that either x is a longest sequence
in c or x · s is a preﬁx of some y ∈ c.

We wish to prove that the longest-chain transition system LC can implement
the single-chain transition system SC, and by transitivity of correct implemen-
tations, also implement any generic transition systems G. The mathematical
machinery developed next will assist in achieving this.

2.3 Monotonic Transition Systems for Distributed Computing

Unlike shared-memory systems, distributed systems have a state that increases
in some natural sense as the computation progresses, e.g. through accumulating
messages and extending the history of local states. This notion of monotonicity,
once formalized, allows a simpler and more powerful mathematical treatment of
transition systems for distributed computing.

So far we have used (cid:22) to denote the preﬁx relation, which is a speciﬁc partial
order. In the following we also use (cid:22) to denote a general partial order; the use
should be clear from the context.

Deﬁnition 7 (Partial Order). A reﬂexive partial order on a set S is denoted
by (cid:22)S (with S omitted if clear from the context), s ≺ s(cid:48) stands for s (cid:22) s(cid:48) & s(cid:48) (cid:54)(cid:22)
s, and s (cid:39) s(cid:48) for s (cid:22) s(cid:48) & s(cid:48) (cid:22) s. The partial order is strict if s (cid:39) s(cid:48) implies
s = s(cid:48) and unbounded if for every s ∈ S there is an s(cid:48) ∈ S such that s ≺ s(cid:48),
has an inﬁnite descending chain if there is an inﬁnite sequence s1, s2, . . . such
that si+1 ≺ si for every i ≥ 1. We say that s, s(cid:48) ∈ S are consistent wrt (cid:22) if
s (cid:22) s(cid:48) or s(cid:48) (cid:22) s (or both).

8

Ehud Shapiro

It is often possible to associate a partial order with a distributed system, wrt
which the local state of each agent only increases. Therefore we focus on the
following type of transition systems:

Deﬁnition 8 (Monotonic & Monotonically-Complete Transition Sys-
tem). Given a partial order (cid:22) on S, a transition system T S = (S, s0, T, λ) is
monotonic with respect to (cid:22) if s → s(cid:48) ∈ T implies s (cid:22) s(cid:48). It is monotonically-
∗−→ s ⊆ T and s (cid:22) s(cid:48) implies that s ∗−→ s(cid:48) ⊆ T .
complete wrt (cid:22) if, in addition, s0

Namely, computations of a monotonically-complete transition system not only
ascend in the partial order, but may also reach, from any state, any larger
state in the partial order. Note that since the partial order is unbounded, a
monotonically-complete transition system has no ﬁnal states.

When transition systems are monotonically-complete wrt a partial order, the
following Deﬁnition 9 and Theorem 1 can be a powerful tool in proving that one
can correctly implement the other.

Deﬁnition 9 (Order-Preserving Implementation). Let transition systems
0, T (cid:48), λ(cid:48)) be monotonic wrt the partial orders
T S = (S, s0, T, λ) and T S(cid:48) = (S(cid:48), s(cid:48)
(cid:22) and (cid:22)(cid:48), respectively. Then an implementation σ : S(cid:48) → S of T S by T S(cid:48) is
order-preserving wrt (cid:22) and (cid:22)(cid:48) if:
1. Up condition: y1 (cid:22)(cid:48) y2 implies that σ(y1) (cid:22) σ(y2)
2. Down condition: s0

∗−→ x1 ⊆ T , x1 (cid:22) x2 implies that there are y1, y2 ∈ S(cid:48)
∗−→ y1 ⊆ T (cid:48) and y1 (cid:22)(cid:48) y2.

such that x1 = σ(y1), x2 = σ(y2), s(cid:48)
0

Note that if (cid:22)(cid:48)

is induced by σ and (cid:22), namely deﬁned by y1 (cid:22)(cid:48) y2 if
σ(y1) (cid:22) σ(y2), then the Up condition holds trivially. The following Theorem
is the linchpin of the proofs of protocol stack theorems here and in other dis-
tributed computing applications of the framework.

Theorem 1 (Correct & Complete Implementation Among Monotonically-
Complete Transition Systems). Assume two transition systems T S = (S, s0, T, λ)
and T S(cid:48) = (S(cid:48), s(cid:48)
0, T (cid:48), λ(cid:48)), monotonically-complete wrt the unbounded partial or-
ders (cid:22) and (cid:22)(cid:48), respectively, and an implementation σ : S(cid:48) → S of T S by T S(cid:48).
If σ is order-preserving and productive then it is correct and complete.

If all transition systems in a protocol stack are monotonically-complete, then
Theorem 1 makes it suﬃcient to establish that an implementation of one protocol
by the next is order-preserving and productive to prove it correct. A key challenge
in showing that Theorem 1 applies is proving that the implementation satisﬁes
the Down condition (Def. 9), which can be addressed by ﬁnding an ‘inverse’ to
σ as follows:

Observation 2 (Representative Implementation State) Assume T S and
T S(cid:48) as in Theorem 1 and an implementation σ : S(cid:48) −→ S that satisﬁes the Up
condition of Deﬁnition 9. If there is a function ˆσ : S −→ S(cid:48) such that x = σ(ˆσ(x))
for every x ∈ S, and x1 (cid:22) x2 implies that ˆσ(x1) (cid:22)(cid:48) ˆσ(x2), then σ also satisﬁes
the Down condition.

Multiagent Transition Systems with Faults

9

Proposition 4. LC can implement SC.

Proof (outline of Proposition 4). We show that both SC and LC are monotonically-
complete wrt the preﬁx relation (cid:22) (Observations 3, 4) and that the implementa-
tion σ2 of SC by LC is order preserving and productive (Proposition 11). Hence,
(cid:117)(cid:116)
according to Theorem 1, σ2 is correct and complete.

Deﬁnition 10 (σ2). The implementation σ2 maps every n-chain conﬁguration
c to the longest chain in c if it is unique, and is undeﬁned otherwise.

In our example, the longest-chain transition system LC implements the single-
chain transition system SC. But SC does not implement the generic transition
system G – a subset of it, SC1, does. So, in order to prove that LC can imple-
ment G, solely based on the implementation of SC by LC, without creating a
custom subset of LC for the task, the following Proposition is useful.

Proposition 5 (Restricting a Correct Implementation to a Subset). Let
σ : C2 (cid:55)→ S1 be an order-preserving implementation of T S1 = (S1, s1, T 1λ1)
by T S2 = (C2, s2, T 2λ2), monotonically-complete respectively with (cid:22)1 and (cid:22)2.
Let T S1(cid:48) = (S1(cid:48), s1, T 1(cid:48), λ1(cid:48)) ⊆ T S1 and T S2(cid:48) = (C2(cid:48), s2, T 2(cid:48), λ2(cid:48)) ⊆ T S2
deﬁned by C2(cid:48) := {s ∈ C2 : σ(s) ∈ S1(cid:48)}, with T 2(cid:48) := T 2/C2(cid:48), and assume
that both subsets are also monotonically-complete wrt (cid:22)1 and (cid:22)2, respectively.
If y1 −→ y2 ∈ T 2(cid:48) & σ(y1) ∈ S1(cid:48) implies that σ(y2) ∈ S1(cid:48) then the restriction of
σ to C2(cid:48) is a correct and complete implementation of T S1(cid:48) by T S2(cid:48).

Corollary 1. The longest-chain transition system LC is universal for generic
transition systems.

More generally, Proposition 5 is useful in the following scenario. Assume
that protocols are speciﬁed via transition systems, as elaborated below. Then
in a protocol stack of, say, three protocols P1, P2, P3, each implementing its
predecessor, it may be the case that for the middle protocol P2 to implement
the full top protocol P1, a subset P2(cid:48) of P2 is needed. But, it may be desirable
for P3 to implement the full protocol P2, not just its subset P2(cid:48), as P2 may
have additional applications beyond just implementing P1. In particular, there
are often application for which an implementation by a middle protocol in the
stack is more eﬃcient than an implementation by the full protocol stack. The
following proposition enables that, see Figure 2. Note that, as shown in the
ﬁgure, the implementing transition system T S2 that implements T S1 could in
turn be a subset of a broader unnamed transition system.

3 Multiagent Transition Systems: Centralized,
Distributed, Synchronous and Asynchronous

3.1 Multiagent Transition Systems

We assume a domain Π of agents. While the set Π may be inﬁnite, here we only
consider ﬁnite subsets of Π. In the following, we use a (cid:54)= b ∈ X as a shorthand
for a ∈ X ∧ b ∈ X ∧ a (cid:54)= b.

10

Ehud Shapiro

In the context of multiagent transition systems, the state of the system is
referred to as conﬁguration, so as not to confuse it with the local states of
agents in a distributed multiagent transition system, deﬁned next.

Deﬁnition 11 (Multiagent Transition System). Given agents P ⊆ Π, a
transition system T S = (C, c0, T, λ), with conﬁgurations C, initial conﬁguration
c0, correct transitions T ⊆ C 2, and a liveness condition λ on T , is multiagent
over P if there is a multiagent partition C 2 = (cid:83)
p∈P Tp of C 2 into disjoint
sets Tp indexed by P , Tp ∩ Tq = ∅ for every p (cid:54)= q ∈ P . A transition t = s →
s(cid:48) ∈ Tp is referred to as a p-transition, which is correct if t ∈ T .

Note that Tp includes all possible behaviors of agent p, both correct and incorrect.

Deﬁnition 12 (Safe, Live & Correct Agents). Given a multiagent transi-
tion system T S = (C, c0, T, λ) over P and a run r of T S, an agent p is safe
in r if r includes only correct p-transitions; is live in r if for every L ∈ λ for
which L ⊆ Tp, r is live wrt L; and is correct in r if p is safe and live in r.

Note that if λ = {Tp ∩T : p ∈ P }, namely the liveness condition is the multiagent
partition restricted to correct transitions, then an agent p is live if it is live wrt
its correct p-transitions Tp ∩ T .

Next, the generic transition system (Example 1) is modiﬁed to be multiagent.
In the generic shared-memory multiagent transition system GS deﬁned next, all
agents operate on the same shared global state. Yet, the transitions of diﬀerent
agents are made disjoint by capturing abstractly the reality of shared-memory
multiprocessor systems: Each conﬁguration incorporates, in addition to a shared
global state s ∈ S, also a unique program counter for each agent. The program
counter of agent p is advanced when a p-transition is taken.

Example 4 (GS: Generic Shared Memory). Given a set of agents P ⊆ Π and
states S with a designated initial state s0, a generic shared-memory multiagent
transition system over P and S, GS = (C, c0, T GS), has conﬁgurations C =
S × NP that include a shared global state in S and a program counter ip ∈ N
for each agent p ∈ P , initial state c0 = (s0, {0}P ), and transitions T GS =
(cid:83)
p∈P T GSp ⊆ C 2, where each p-transition (s, i) → (s(cid:48), i(cid:48)) ∈ T GSp satisﬁes
p = ip + 1 and i(cid:48)
i(cid:48)

q = iq for every q (cid:54)= p ∈ P .

Note that T GS is arbitrary, and diﬀerent agents may or may not be able to
change the shared global state in the same way. But each transition identiﬁes
the agent p making the change by advancing p’s program counter.

Next, the single-chain transition system SC (Example 2) is modiﬁed to the
multiagent transition system for single-chain consensus SCC. As SCC is mono-
tonic, program counters are not needed; it is suﬃcient to identify the agent
contributing the next element to the shared global chain to make transitions by
diﬀerent agents disjoint.

Example 5 (SCC: Single-Chain Consensus). Given a set of agents P ⊆ Π and
a set S, the single-chain consensus multiagent transition system over P and S

Multiagent Transition Systems with Faults

11

is SCC = ((S × P )∗, Λ, T SCC), with each conﬁguration being a sequence of
agent-identiﬁed states (s, p) of a state s ∈ S and an agent p ∈ P , and T SCC
includes every transition x → x · (s, p) for every x ∈ (S × P )∗, s ∈ S and p ∈ P .

Namely, an SCC run can generate any sequence of agent-identiﬁed elements of
S, where any agent may contribute any element to any position in the sequence.
Next, we show that SCC can implement GS, making single-chain consensus

universal for shared-memory multiagent transition systems.

Proposition 6. SCC over P ⊆ Π and S can implement any generic shared-
memory multiagent transition system GS over P and S.

3.2 Centralized and Distributed Multiagent Transition Systems

Having introduced centralized/shared-memory multiagent transition systems,
and before introducing distributed ones, we formalize the two notions:

Deﬁnition 13 (Centralized and Distributed Multiagent Transition Sys-
tem). A multiagent transition system T S = (C, c0, T, λ) over P is distributed
if:
1. C = SP for some set S, referred to as local states, namely each conﬁguration
c ∈ C consists of a set of local states in S indexed by P , in which case we use
cp ∈ S to denote the local state of p ∈ P in conﬁguration c ∈ C, and

2. Any p-transition c → c(cid:48) ∈ Tp satisﬁes that c(cid:48)

p (cid:54)= cp and c(cid:48)

q = cq for every

q (cid:54)= p ∈ P .

Else T S is centralized.

Namely, in a distributed transition system a p-transition (correct or incorrect)
can only change the local state of p. As a shorthand, we will omit ‘multiagent’
from distributed multiagent transition systems, and instead of presenting a dis-
tributed multiagent transition system over P and S as T S = (SP , c0, T, λ), we
will refer to it as the distributed transition system T S = (P, S, c0, T, λ).

Next, we modify the longest-chain transition system LC (Example 3) to be-
come the distributed transition system for longest-chain consensus LCC, in which
each agent has a chain as its local state.

Example 6 (LCC: Longest-Chain Consensus). Given a set of agents P ⊆ Π
and states S, the LCC distributed longest-chain transition system, LCC =
(P, (S × P )∗, c0, T LCC, λ), has sequences over S × P as local states, an empty
sequence as the initial local state c0 = {Λ}P , and as p-transitions T LCC every
c → c(cid:48) where c(cid:48) is obtained from c by only extending cp, c(cid:48)
p = cp · (s, p(cid:48)), s ∈ S,
p(cid:48) ∈ P , and c(cid:48)
q = cq for every q (cid:54)= p ∈ P , provided that either p = p(cid:48) and cp is
a longest sequence in c, or p(cid:48) (cid:54)= p and cp · (s, p(cid:48)) is a preﬁx of cq ∈ c for some
q (cid:54)= p ∈ P . The liveness condition λ = {Tp ∩ T : p ∈ P } is the multiagent
partition restricted to correct transitions.

12

Ehud Shapiro

Note that the transition system, while distributed, is synchronous (a notion de-
ﬁned formally below), as an agent’s ability to extend its local chain by a certain
element depends on the present local states of other agents. Next, we show that
LCC can implement SCC, making the longest-chain consensus distributed tran-
sition system LCC universal for shared-memory multiagent transition systems.

Proposition 7. LCC can implement SCC.

We noted informally why we consider LCC synchronous. Next, we deﬁne the
notions of synchronous and asynchronous distributed transition systems, prove
that LCC is synchronous and investigate an asynchronous distributed transition
system and its implementation of the LCC.

3.3 Synchronous and Asynchronous Distributed Multiagent

Transition Systems

A partial order (cid:22) over a set of local states S naturally extends to conﬁgurations
C = SP over P ⊂ Π and S by c (cid:22) c(cid:48) for c, c(cid:48) ∈ C if cp (cid:22) c(cid:48)

p for every p ∈ P .

Deﬁnition 14 (Distributed Transition System; Synchronous and Asyn-
chronous). Given agents P ⊆ Π, local states S, and a distributed transition
system T S = (P, S, c0, T, λ), then T S is asynchronous wrt a partial order (cid:22)
on S if:
1. T S is monotonic wrt (cid:22), and
2. for every p-transition c −→ c(cid:48) ∈ T , T also includes the p-transition d −→ d(cid:48) for

every d, d(cid:48) ∈ C that satisfy the following asynchrony condition:

c (cid:22) d, (cp → c(cid:48)

p) = (dp → d(cid:48)

p), and d(cid:48)

q = dq for every q (cid:54)= p ∈ P.

If no such partial order on S exists, then T S is synchronous.

With this deﬁnition, we note that the distributed longest-chain transition
system LCC is not asynchronous wrt the preﬁx relation, as an enabled transition
to extend the local chain can become disabled if some other chain extends and
becomes longer. We argue that this is the case wrt any partial order.

Proposition 8. LCC is synchronous.

Next we devise an asynchronous block dissemination transition system ABD,

and prove its universality by using it to implement the synchronous LCC.

Deﬁnition 15 (Block). Given agents P ⊆ Π and states S, a block over P and
S is a triple (p, i, s) ∈ P × N × S. Such a block is referred to as an i-indexed
p-block with payload s.

Example 7 (ABD: Asynchronous Distributed Block Dissemination). Given a set
of agents P ⊆ Π and states S that do not include the undeﬁned element ⊥ /∈ S,
the asynchronous distributed block dissemination transition system, ABD =

Multiagent Transition Systems with Faults

13

(P, B, c0, T ABD, λ), has local states B being all ﬁnite sets of blocks over P and
S ∪ {⊥}, an empty set as the initial local state c0 = {∅}P , and T ABD has every
p-transition c → c(cid:48) for every p ∈ P , where c(cid:48) is obtained from c by adding a
block b = (p(cid:48), i, s) to cp, c(cid:48)
1. p-Creates-b: p(cid:48) = p, i = i(cid:48) + 1, where i(cid:48) := max {j : (p, j, s) ∈ cp}, or
2. p-Receives-b: p(cid:48) (cid:54)= p, (p(cid:48), i, s) ∈ cq \ cp for some q (cid:54)= p ∈ P .
The liveness condition λ places two transitions in the same set if they have
identical labels, p-Creates-b or p-Receives-b as the case may be.

p = cp ∪ {b}, p(cid:48) ∈ P , i ∈ N, s ∈ S ∪ {⊥}, and either:

In other words, every agent p can either add a consecutively-indexed p-block to
its local state, possibly with ⊥ as payload, or obtain a block it does not have
from some other agent. Next, we explore some properties of ABD: Fault-resilient
dissemination and equivocation detection. We use ‘p knows b’ in a run r to mean
that b ∈ cp for some c ∈ r.

While in ABD agents do not explicitly disseminate blocks they know to other
agents, only receive blocks that they do not know from other agents, faulty agents
may cause partial dissemination by deleting a block from their local state after
only some of the agents have received it. The following proposition states that
faulty agents cannot prevent correct agents from eventually sharing all the blocks
that they know, including blocks created and partially disseminated by faulty
agents.

Proposition 9 (ABD Block Liveness). In an ABD run, if a correct agent
knows a block b then eventually all correct agents know b.

Deﬁnition 16 (Equivocation). An equivocation by agent p consists of two
p-blocks b = (p, i, s), b(cid:48) = (p, i(cid:48), s(cid:48)) where i = i(cid:48) but s (cid:54)= s(cid:48). An agent p is
an equivocator in B if B includes an equivocation by p. A set of blocks B is
equivocation-free if it does not include an equivocation.

The following corollary states that if an agent p tries to mislead (e.g. double
spend) correct agents by disseminating to diﬀerent agents equivocating blocks,
then eventually all correct agents will know that p is an equivocator.

Corollary 2 (ABD Equivocation Detection). In an ABD run, if two blocks
b, b(cid:48) of an equivocation by agent p are each known by a diﬀerent correct agent,
then eventually all correct agents know that p is an equivocator.

Next, we prove that asynchronous distributed block dissemination ABD can
implement the synchronous distributed longest-chain LCC. In fact, this imple-
mentation oﬀers a naive distributed asynchronous ordering consensus protocol.
Its lack of resilience to equivocation and to fail-stop agents, implied by the FLP
theorem [9], is discussed in the next section. This limitation reﬂects on the im-
plementation presented here and and not on ABD: The Cordial Miners family of
protocols [19] employs a more concrete and practical (blocklace-based [32]) vari-
ant of asynchronous block dissemination to construct Byzantine fault-resilient
order consensus protocols (aka Byzantine Atomic Broadcast) for the models of
asynchrony and eventual synchrony.

Proposition 10. ABD can implement LCC.

14

Ehud Shapiro

4 Safety Faults, Liveness Faults, and their Resilience

A safety fault is a subset (or all) of the incorrect transitions, and a liveness fault
is a subset of the liveness condition. A computation performs a safety fault F
if it includes an F transition. It performs a liveness fault λ(cid:48) ⊆ λ if it is not live
wrt a set L ∈ λ(cid:48). Formally:

Deﬁnition 17 (Safety and Liveness Faults). Given a transition system
T S = (S, s0, T, λ), a safety fault is a set of incorrect transitions F ⊆ S2 \ T .
A computation performs a safety fault F if it includes a transition from F .
A liveness fault is a a subset λ(cid:48) ⊆ λ of the liveness condition λ. An inﬁnite
run performs a liveness fault λ(cid:48) if it is not live wrt L for some L ∈ λ(cid:48).

Note that any safety fault can be modelled with the notion thus deﬁned,
by enlarging S and thus expanding the set of available incorrect transitions S2.
Similarly, any liveness fault can be modeled by revising λ accordingly.

Deﬁnition 18 (Safety-Fault Resilience). Given transition systems T S =
(S, s0, T λ), T S(cid:48) = (S(cid:48), s(cid:48)
0, T (cid:48), λ(cid:48)) and a safety fault F ⊆ S(cid:48)2 \ T (cid:48), a correct
implementation σ : S(cid:48) −→ S is F -resilient if for any live T S(cid:48) run r(cid:48) ⊆ T ∪ F ,
the run σ(r(cid:48)) is correct.

In other words, a safety-fault-resilient implementation does not produce incorrect
transitions of the speciﬁcation even if the implementation performs safety faults,
and it produces a live run if the implementation run is live.

Next we compare the resilience of single-chain consensus SCC and longest-
chain consensus LCC to the safety fault in which an agent trashes the chain
by adding junk to it. We show that the implementation of the generic shared-
memory GS by LCC is more resilient to such faults than the implementation by
SCC: In SCC such a faulty transition terminates the run, violating liveness; in
LCC it does not, as long as there is at least one non-faulty agent.

The following Theorem addresses the composition of safety-fault-resilient im-

plementations. See Figure 3.

Theorem 2 (Composing Safety-Fault-Resilient Implementations). As-
sume transition systems T S1 = (S1, s1, T 1, λ1), T S2 = (S2, s2, T 2λ2), T S3 =
(S3, s3, T 3λ3), correct implementations σ21 : S2 (cid:55)→ S1 and σ32 : S3 → S2, and
let σ31 := σ21 ◦ σ32. Then:
1. If σ32 is resilient to F 3 ⊆ S32 \ T 3, then σ31 is resilient to F 3.
2. If σ21 is resilient to F 2 ⊆ S22\T 2, and F 3 ⊆ S32\T 3 satisﬁes σ32(F 3) ⊆ F 2,

then σ31 is resilient to F 3.

3. These two types of safety-fault resilience can be combined for greater re-
silience: If σ21 is F 2-resilient, σ32 is F 3-resilient, F 3(cid:48) ⊆ S32 \ T 3, and
σ32(F 3(cid:48)) ⊆ F 2, then σ31 is resilient to F 3 ∪ F 3(cid:48).

Example 8 (Resilience to Safety Faults in Implementations by SCC and LCC).
For SCC, consider the safety fault F 1 to be the faulty q-transitions c → c · 0 for
every conﬁguration c and some q ∈ P . For LCC, consider the safety fault F 2 to

Multiagent Transition Systems with Faults

15

be the faulty q-transitions cq → cq · 0 for every conﬁguration c and some agent
q ∈ P . Then a faulty SCC run r with an F 1 transition cannot be continued,
and hence σ2(r) is not live and hence incorrect. On the other hand, in a faulty
LCC run r with F 2 transitions, the faulty transitions are mapped by σ2m to
stutter, the run can continue and the implementation is live as long as at least
one agent is not faulty. Note that this holds for the implementation of SCC by
LCC, as well as for the composed implementation of GS by LCC, as stated by
the following Theorem 2 (the F 3 case).

Next we consider the implementation of longest-chain consensus LCC by
asynchronous block dissemination ABD, and it non-resilience to the safety fault
of equivocation.

Example 9 (Non-Resilience to Equivocation of the implementation of LCC by
ABD). Consider the implementation σ3 of LCC = (P, (S × P )∗, c0, T LCC)
by ABD = (P, B, c0, T ABD, λ), and let F ⊂ (BP )2 include equivocations by
a certain agent p ∈ P for every conﬁguration, namely for every conﬁguration
c ∈ BP in which cp includes a p-block b = (p, i, s), F includes the p-transition
cp → cp ∪ {b(cid:48)} for b(cid:48) = (p, i, s(cid:48)) for some s(cid:48) (cid:54)= s ∈ S. A run r with such an equivo-
cating transition by p may include subsequently a q-Receives-b and q(cid:48)-Receives-b(cid:48)
transitions, following which, say in conﬁguration c(cid:48), the chain computed by σ3(c(cid:48))
for q and for q(cid:48) would not be consistent, indicating σ3(r) to be faulty (not safe).

Deﬁnition 19 (Can Implement with Safety-Fault Resilience). Given tran-
0, T (cid:48), λ(cid:48)) and F ⊆ S(cid:48)2\T (cid:48), T S(cid:48) can
sition systems T S = (S, s0, T, λ), T S(cid:48) = (S(cid:48), s(cid:48)
implement T S with F -resilience if there is a subset T S(cid:48)(cid:48) = (S(cid:48)(cid:48), s(cid:48)
0, T (cid:48)(cid:48), λ(cid:48)(cid:48)) ⊆
T S(cid:48), F ⊂ S(cid:48)(cid:48) × S(cid:48)(cid:48), and an F -resilient implementation σ : S(cid:48)(cid:48) → S of T S by
T S(cid:48)(cid:48).

The requirement F ⊂ S(cid:48)(cid:48) × S(cid:48)(cid:48) ensures that the subset T S(cid:48)(cid:48) does not simply
‘deﬁne away’ the faulty transitions F .

Deﬁnition 20 (Can Implement with Liveness-Fault Resilience). Given
transition systems T S = (S, s0, T, λ), T S(cid:48) = (S(cid:48), s(cid:48)
0, T (cid:48), λ(cid:48)), then T S(cid:48) can im-
plement T S with ¯λ-resilience, ¯λ ⊆ λ(cid:48), if there is a subset T S(cid:48)(cid:48) = (S(cid:48)(cid:48), s(cid:48)
T S(cid:48), and an implementation σ : S(cid:48)(cid:48) → S of T S by T S(cid:48)(cid:48), resilient to ¯λ restricted
to λ(cid:48)(cid:48).

0, T (cid:48)(cid:48), λ(cid:48)(cid:48)) ⊆

As an example of resilience to a liveness fault, consider the following:

Example 10 (Resilience to fail-stop agents of the implementation of SCC and
GC by LCC). Consider LCC = (P, (S × P )∗, c0, T LCC, λ), and recall that the
liveness condition λ = {Tp ∩ T : p ∈ P } is the multiagent partition restricted to
correct transitions. An LCC run r with a liveness fault λ(cid:48) may have all agents
p for which Tp ∈ λ(cid:48) fail-stop after some preﬁx of r. Still, at least one live agent
remains by the assumption that λ(cid:48) is a strict subset of λ, and hence σ2(r) is a live
(and hence correct) LCC run. Thus σ3 is resilient to any liveness fault of LCC
provided at least one agent remains live. Next, consider the implementation

16

Ehud Shapiro

of GC by LCC. First, we deﬁned a subset SCC1 of SCC to implement GC.
Then we deﬁned LCC1 a subset of LCC to implement SCC1. Such a composed
implementation is resilient to fail-stop agents (¯λ in the example above), where
their transitions are restricted to LCC1 (λ(cid:48)(cid:48) in the deﬁnition above).

5 Grassroots Protocols and their Implementation

In the following we assume that the set of local states S is a function of the
set of participating agents P ⊆ Π. Intuitively, the local states could be sets of
signed and/or encrypted messages sent by members of P to members of P ; NFTs
created by and transferred among members of P ; or blocks signed by members
of P , with hash pointers to blocks by other members of P . With this notion, we
deﬁne a family of transition systems to have for each set of agents P ⊂ Π one
transition system that speciﬁes all possible behaviors T (P ) of P over S(P ) and
a liveness condition λ(P ) over T (P ).

Deﬁnition 21 (Family of Multiagent Transition Systems; Protocol; Asyn-
chronous; Subset). Assume a function S that maps each set of agent P ⊂ Π
into a set of local states S(P ). A family F of multiagent transition systems
over S is a set of transition systems such that for each set of agents P ⊆ Π
there is one transition system T S(P ) = (C(P ), c0(P ), T (P ), λ(P )) ∈ F with
conﬁgurations C(P ) over P and S(P ), transitions T (P ) over C(P ) and a live-
ness condition λ(P ). The states of F are S(F) := (cid:83)
P ⊆Π S(P ). If a family of
multiagent transition systems is distributed then we refer to it as a protocol,
which is asynchronous if every transition system in F is asynchronous, and is
a subset of protocol F (cid:48) if for every P ⊆ Π, T S(P ) is a subset of T S(cid:48)(P ).

For simplicity and to avoid notational clutter, we often assume a given set of
agents P and refer to the representative member of F over P , rather than to the
entire family F. Furthermore, when the family F and the set of agents P are
given we sometimes refer to the protocol T S(P ) = (P, S(P ), c0(P ), T (P ), λ(P )) ∈
F simply as T S = (P, S, c0, T, λ). Next, we deﬁne the notion of a grassroots pro-
tocol. Intuitively, in a grassroots protocol, the behavior of agents in a small
community that runs the protocol is not constrained by its context, for example
by this community being composed with another community or, equivalently, be-
ing embedded within a larger community. Yet, the protocol allows agents in two
communities placed together to behave in new ways not possible when the two
communities operate the protocol in isolation. In particular, agents may inter-
act with each other across community boundaries. This supports the grassroots
deployment of a distributed system – multiple independent disjoint deployments
at diﬀerent locations and over time, which may subsequently interoperate once
interconnected.

We deﬁne the notion of a grassroots protocol with two auxiliary notions: the
projection of a conﬁguration and the union of distributed transition systems,
deﬁned next.

Multiagent Transition Systems with Faults

17

Deﬁnition 22 (Projection of a Conﬁguration). Let P (cid:48) ⊂ P ⊂ Π and
let T S = (P, S, c0, T ) be a distributed transition system. The projection of
a conﬁguration c ∈ SP on P (cid:48), c/P (cid:48), is the conﬁguration c(cid:48) over P (cid:48) satisfying
p = cp for all p ∈ P (cid:48).
c(cid:48)

The union of two distributed transition systems over disjoint sets of agents
includes all transitions in which one component makes its own transition and
the other component stands still, implying that the runs of the union include
exactly all interleavings of the runs of its components.

Deﬁnition 23 (Union of Distributed Transition Systems). Let T S1 =
(P 1, S1, c01, T 1), T S2 = (P 2, S2, c02, T 2) be two distributed transition systems,
P 1 ∩ P 2 = ∅. Then the union of T S1 and T S2, T S := T S1 ∪ T S2, is the
multiagent transition systems T S = (P 1 ∪ P 2, S1 ∪ S2, c0, T ) with initial state
c0 satisfying c0/P 1 = c01, c0/P 2 = c02, and all p-transitions c → c(cid:48) ∈ T , p ∈ P ,
satisfying p ∈ P 1 ∧ (c/P 1 → c(cid:48)/P 1) ∈ T 1 ∧ c/P 2 = c(cid:48)/P 2 or p ∈ P 2 ∧ (c/P 2 →
c(cid:48)/P 2) ∈ T 2 ∧ c/P 1 = c(cid:48)/P 1.

The notion of a grassroots composition requires the composed system to
include all computations each component can do on its own, and then some. In
other words, put together, each component of the composed transition system
can still behave independently, as if it is on its own; but the composed system
also has additional behaviors.

Deﬁnition 24 (Grassroots). A protocol F supports grassroots composi-
tion, or is grassroots, if for every ∅ ⊂ P 1, P 2 ⊂ Π such that P 1 ∩ P 2 = ∅, the
following holds:

T S(P 1) ∪ T S(P 2) ⊂ T S(P 1 ∪ P 2)

Example 11 (Synchronous LCC is not grassroots; Asynchronous ABD is). We
note that the union of two longest-chain consensus LCC transition systems over
disjoint sets of agents is not a subset of an LCC transition system. The reason is
that since the component transition systems operate independently, the chains
they create—in particular their longest chains—can be inconsistent. Hence their
union LCC(P 1) ∪ LCC(P 1) may have computations that reach inconsistent
states, computations that are not in LCC(P 1 ∪ P 2). Hence LCC is not grass-
roots. On the other hand, ABD is: In two disjoint components ABD(P 1) and
ABD(P 2) agents create and receive blocks among themselves, behaviors that
are also available in ABD(P 1 ∪ P 2). But they also include new ones, with p-
Receives-b transitions for blocks created by agents in the other component.

Next, we describe a suﬃcient condition for a protocol to be grassroots:

Deﬁnition 25 (Monotonic and Asynchronous Protocols). Let F be a pro-
tocol. A partial order (cid:22) over S(F) is preserved under projection if for every
P 2 ⊂ P 1 ⊆ Π and every two conﬁgurations c, c(cid:48) over P 1 and S(P 1), c (cid:22) c(cid:48)
implies that c/P 2 (cid:22) c(cid:48)/P 2. A protocol F is monotonic wrt a partial order (cid:22)
over S(F) if (cid:22) is preserved under projection and every member of F is mono-
tonic wrt (cid:22); it is asynchronous wrt (cid:22) if, in addition, every member of F is
asynchronous wrt (cid:22).

18

Ehud Shapiro

A protocol is interactive if two sets of agents can perform together compu-
tations they cannot perform in isolation, and is non-interfering if a transition
that can be carried out by a group of agents can still be carried out if there are
additional agents that observe it from their initial state. Formally:

Deﬁnition 26 (Interactive & Non-Interfering Protocol). A protocol F is
interactive if for every ∅ ⊂ P 1, P 2 ⊂ Π such that P 1 ∩ P 2 = ∅, the follow-
ing holds: T S(P 1 ∪ P 2) (cid:54)⊆ T S(P 1) ∪ T S(P 2). It is non-interfering if for
every P (cid:48) ⊂ P ⊂ Π, with transition systems T S = (P, S(P ), c0, T ), T S(cid:48) =
(P (cid:48), S(P (cid:48)), c0(cid:48), T (cid:48)) ∈ F, and every transition c1(cid:48) → c2(cid:48) ∈ T (cid:48), T includes the
transition c1 → c2 for which c1(cid:48) = c1/P (cid:48), c2(cid:48) = c2/P (cid:48), and c1p = c2p = c0p for
every p ∈ P \ P (cid:48).

Theorem 3 (Grassroots Protocol). An asynchronous, interactive, and non-
interfering protocol is grassroots.

Example 12 (ABD is grassroots, again). Examining ABD, it can be veriﬁed that
it is an asynchronous, interactive and non-interfering protocol. We have argued
above that ABD is interactive. It is non-interfering as agents in their initial
state do not interfere with other agents performing p-Creates-b or p-Receives-b
transitions amongst themselves. And we have already concluded (Example 7)
that ABD is asynchronous.

Theorem 3 and the examples above do not imply that ordering consensus pro-
tocols cannot be grassroots. In fact, they can be, provided that the set of agents
that participates in a particular ordering consensus protocol is not provided a
priori, an in LCC and in the standard descriptions of permissioned consensus
protocols, but is determined by the agents themselves in a grassroots fashion ac-
cording to the protocol. A blocklace-based grassroots consensus protocol stack
that demonstrates this is presented elsewhere [32].

6 Conclusions

Multiagent transition systems come equipped with powerful tools for specify-
ing distributed protocols and for proving the correctness and fault-resilience of
implementations among them. The tools are best applied if the transition sys-
tems are monotonically-complete wrt a partial order, as is often the case in
distributed protocols and algorithms. Employing this framework in the speci-
ﬁcation of a grassroots ordering consensus protocol stack has commenced [32],
with sovereign cryptocurrencies [33] and an eﬃcient Byzantine atomic broadcast
protocol [19] as the ﬁrst applications.

Multiagent Transition Systems with Faults

19

References

1. Abadi, M., Lamport, L.: The existence of reﬁnement mappings. Theoretical Com-

puter Science 82(2), 253–284 (1991)

2. Abadi, M., Lamport, L.: Composing speciﬁcations. ACM Transactions on Pro-

gramming Languages and Systems (TOPLAS) 15(1), 73–132 (1993)
3. Boudol, G.: Asynchrony and the pi-calculus. Ph.D. thesis, INRIA (1992)
4. Bracha, G.: Asynchronous byzantine agreement protocols. Information and Com-

putation 75(2), 130–143 (1987)

5. Canetti, R.: Universally composable security: A new paradigm for cryptographic
protocols. In: Proceedings 42nd IEEE Symposium on Foundations of Computer
Science. pp. 136–145. IEEE (2001), https://eprint.iacr.org/2000/067.pdf, re-
vised 2020

6. Cristian, F., Aghili, H., Strong, R., Dolev, D.: Atomic broadcast: From simple
message diﬀusion to byzantine agreement. Information and Computation 118(1),
158–179 (1995)

7. Das, S., Xiang, Z., Ren, L.: Asynchronous data dissemination and its applications.
In: Proceedings of the 2021 ACM SIGSAC Conference on Computer and Commu-
nications Security. pp. 2705–2721 (2021)

8. Das, S., Xiang, Z., Ren, L.: Asynchronous data dissemination and its applica-
tions. In: Proceedings of the 2021 ACM SIGSAC Conference on Computer and
Communications Security. p. 2705–2721. CCS ’21, Association for Computing Ma-
chinery, New York, NY, USA (2021). https://doi.org/10.1145/3460120.3484808,
https://doi.org/10.1145/3460120.3484808

9. Fischer, M.J., Lynch, N.A., Paterson, M.S.: Impossibility of distributed consensus
with one faulty process. Journal of the ACM (JACM) 32(2), 374–382 (1985)
10. Fournet, C., Gonthier, G.: The reﬂexive cham and the join-calculus. In: Proceedings
of the 23rd ACM SIGPLAN-SIGACT symposium on Principles of programming
languages. pp. 372–385 (1996)

11. Francalanza, A., Hennessy, M.: A theory for observational fault tolerance. The

Journal of Logic and Algebraic Programming 73(1-2), 22–50 (2007)

12. Giridharan, N., Kokoris-Kogias, L., Sonnino, A., Spiegelman, A.: Bullshark: Dag

bft protocols made practical. arXiv preprint arXiv:2201.05677 (2022)

13. Guerraoui, R., Kuznetsov, P., Monti, M., Pavloviˇc, M., Seredinschi, D.A.: The con-
sensus number of a cryptocurrency. In: Proceedings of the 2019 ACM Symposium
on Principles of Distributed Computing. pp. 307–316 (2019)

14. Hawblitzel, C., Howell, J., Kapritsos, M., Lorch, J.R., Parno, B., Roberts, M.L.,
Setty, S., Zill, B.: Ironﬂeet: proving safety and liveness of practical distributed
systems. Communications of the ACM 60(7), 83–92 (2017)

15. Hesselink, W.H.: Deadlock and fairness in morphisms of transition systems. The-

oretical computer science 59(3), 235–257 (1988)

16. Hoare, C.A.R.: Communicating sequential processes. Communications of the ACM

21(8), 666–677 (1978)

17. Hur, C.K., Dreyer, D., Neis, G., Vafeiadis, V.: The marriage of bisimulations and

kripke logical relations. ACM SIGPLAN Notices 47(1), 59–72 (2012)

18. Keidar, I., Kokoris-Kogias, E., Naor, O., Spiegelman, A.: All you need is dag.
In: Proceedings of the 2021 ACM Symposium on Principles of Distributed Com-
puting. p. 165–175. PODC’21, Association for Computing Machinery, New York,
NY, USA (2021). https://doi.org/10.1145/3465084.3467905, https://doi.org/
10.1145/3465084.3467905

20

Ehud Shapiro

19. Keidar, I., Naor, O., Shapiro, E.: Cordial miners: A family of simple, eﬃ-
cient and self-contained consensus protocols for every eventuality. arXiv preprint
arXiv:2205.09174 (2022)

20. Krogh-Jespersen, M., Timany, A., Ohlenbusch, M.E., Gregersen, S.O., Birkedal,
L.: Aneris: A mechanised logic for modular reasoning about distributed systems.
In: ESOP. pp. 336–365 (2020)

21. Lamport, L.: Specifying concurrent systems with tla+. NATO ASI SERIES F

COMPUTER AND SYSTEMS SCIENCES 173, 183–250 (1999)

22. Leroy, X.: A formally veriﬁed compiler back-end. Journal of Automated Reasoning

43(4), 363–446 (2009)

23. Lesani, M., Bell, C.J., Chlipala, A.: Chapar: certiﬁed causally consistent distributed

key-value stores. ACM SIGPLAN Notices 51(1), 357–370 (2016)

24. Lynch, N.A., Tuttle, M.R.: An introduction to input/output automata. Laboratory

for Computer Science, Massachusetts Institute of Technology (1988)

25. Milner, R.: A calculus of communicating systems. Springer (1980)
26. Milner, R.: Communicating and mobile systems: the pi calculus. Cambridge uni-

versity press (1999)

27. Nakamoto, S.: Bitcoin: A peer-to-peer electronic cash system (2008), https://

bitcoin.org/bitcoin.pdf

28. Paraskevopoulou, Z., Li, J.M., Appel, A.W.: Compositional optimizations for cer-
ticoq. Proceedings of the ACM on Programming Languages 5(ICFP), 1–30 (2021)
29. Pilkiewicz, A., Pottier, F.: The essence of monotonic state. In: Proceedings of the
7th ACM SIGPLAN workshop on Types in language design and implementation.
pp. 73–86 (2011)

30. Princehouse, L., Chenchu, R., Jiang, Z., Birman, K.P., Foster, N., Soul´e, R.: Mica:
A compositional architecture for gossip protocols. In: European Conference on
Object-Oriented Programming. pp. 644–669. Springer (2014)

31. Sergey, I., Wilcox, J.R., Tatlock, Z.: Programming and proving with distributed
protocols. Proceedings of the ACM on Programming Languages 2(POPL), 1–30
(2017)

32. Shapiro, E.: The blocklace: A partially-ordered generalization of the blockchain
and its grassroots consensus protocol stack. To appear. Also accessible as Section
4 of an earlier version of this paper: https://arxiv.org/abs/2112.13650v8 (2022)
33. Shapiro, E.: Sovereign cryptocurrencies: Foundation for grassroots cryptoeconomy.

arXiv preprint arXiv:2202.05619 (2022)

34. Shostak, R., Pease, M., Lamport, L.: The byzantine generals problem. ACM Trans-

actions on Programming Languages and Systems 4(3), 382–401 (1982)

35. Wilcox, J.R., Woos, D., Panchekha, P., Tatlock, Z., Wang, X., Ernst, M.D., An-
derson, T.: Verdi: a framework for implementing and formally verifying distributed
systems. In: Proceedings of the 36th ACM SIGPLAN Conference on Programming
Language Design and Implementation. pp. 357–368 (2015)

36. Yin, M., Malkhi, D., Reiter, M.K., Gueta, G.G., Abraham, I.: Hotstuﬀ: Bft consen-
sus with linearity and responsiveness. In: Proceedings of the 2019 ACM Symposium
on Principles of Distributed Computing. pp. 347–356 (2019)

Multiagent Transition Systems with Faults

21

A Proofs

Proof (of Observation 1). Assume by way of contradiction that the live com-
putation r is ﬁnite and its last state s is not ﬁnal. Hence there is a correct
transition t enabled on s, and r violates both liveness requirements: First, that r
has a nonempty suﬃx in which no correct transition is enabled, since t is enabled
on every nonempty suﬃx of r. Second,that every suﬃx of r includes an correct
transition, since the suﬃx that include only s does not. Hence r is not live. A
(cid:117)(cid:116)
contradiction.

Proof (of Proposition 1). We prove the proposition by way of contradiction.
Assume that σ is locally safe but not safe. Hence, there is a computation r(cid:48) ⊆ T (cid:48)
with an incorrect transition t ∈ σ(r(cid:48)) \ T . Consider a preﬁx r(cid:48)(cid:48) of r(cid:48) for which
t ∈ σ(r(cid:48)(cid:48)). This preﬁx violates local safety. A contradiction.

Assume that σ is productive but not live. Then there is a set of transitions
L ∈ λ and a computation r(cid:48) ⊆ T (cid:48) for which σ(r(cid:48)) is not live wrt L. This means
that in every nonempty suﬃx of σ(r(cid:48)) L is enabled, and there is a suﬃx of σ(r(cid:48))
that does not include an L transition. This violates both alternative conditions
for σ being productive: that r(cid:48) has a nonempty suﬃx r(cid:48)(cid:48) such that L is not
enabled in σ(r(cid:48)(cid:48)), and that every suﬃx r(cid:48)(cid:48) of r(cid:48) activates L. A contradiction.

Assume that σ is locally complete but not complete. Then there is a run
r ⊆ T for which there is no run r(cid:48) ⊆ T (cid:48) such that σ(r(cid:48)) = r. Then there must be
a preﬁx ¯r ≺ r of r for which for no run r(cid:48) ⊆ T (cid:48), ¯r (cid:22) σ(r(cid:48)). Thus ¯r violates local
(cid:117)(cid:116)
completeness, a contradiction. This completes the proof.

Proof (of Proposition 2). Given a generic transition system G = (S, s0, T G) over
S, we deﬁne a subset SC1 of SC and a mapping σ1 from SC1 to G that together
implement G. The transition system SC1 = (S+, s0, T SC1) has the transition
x · s → x · s · s(cid:48) ∈ T SC1 for every x ∈ S∗ and every transition s → s(cid:48) ∈ T G.
The mapping σ1 : S+ (cid:55)→ S takes the last element of its input sequence, namely
σ1(x · s) := s.

To prove that σ1 is correct we have to show that σ1 is:

1. Locally Safe: s0 ∗−→ y −→ y(cid:48) ⊆ T SC1 implies that s0 ∗−→ x ∗−→ x(cid:48) ⊆ T G for

x = σ0(y) and x(cid:48) = σ0(y(cid:48)) in S.
Let y = s0·s1·. . .·sk, y(cid:48) = y·sk+1, for k ≥ 1. For each transition s0·s1·. . .·si →
s0 · ·s1 . . . · si · si+1, i ≤ k, the transition si → si+1 ∈ T G by deﬁnition of
T SC1. Hence s0 ∗−→ x ∗−→ x(cid:48) ⊆ T G, satisfying the safety condition.

2. Productive: T SC1 is the only set in the liveness condition, and any T SC1

transition from any state of SC1 activates T G.

3. Locally Complete: s0 ∗−→ x −→ x(cid:48) ⊆ T G implies that there are y, y(cid:48) ∈ S+

such that x = σ1(y), x(cid:48) = σ1(y(cid:48)), and s0 ∗−→ y ∗−→ y(cid:48) ⊆ T SC1.
Let x = sk, x(cid:48) = sk+1, k ≥ 1, and s0 → s1 → . . . → sk → sk+1 ∈ T G. Then
y = s0 · s1 · . . . · sk and y(cid:48) = y · sk+1 satisfy the completeness condition.

This completes the proof.

(cid:117)(cid:116)

Proof (of Proposition 3). Assume transition systems T S1 = (S1, s1, T 1, λ1),
T S2 = (S2, s2, T 2, λ2), T S3 = (S3, s3, T 3, λ3) and implementations σ21 : S2 (cid:55)→
S1 and σ32 : S3 (cid:55)→ S2, and let σ31 := σ21 ◦ σ32.

22

Ehud Shapiro

Assume that σ32 and σ21 are safe. Let r ⊆ T 3 be a safe T S3 run. Then σ32(r)
is a safe T S2 run by the safety of σ32, and hence σ21(σ32(r)) is a safe run by the
safety of σ21. Hence σ31 is safe.

Assume that σ32 and σ21 are live. Let r ⊆ T 3 be a live T S3 run. Then σ32(r)
is a live T S2 run by the liveness of σ32, and hence σ21(σ32(r)) is a live run by
the liveness of σ21. Hence σ31 is live.

A safe and live run is correct, hence if σ32 and σ21 are correct then so is σ31.
Assume that σ32 and σ21 are complete. Let r1 ⊆ T 1 be a correct T S1 run. By
completeness of σ21 there is a correct T S2 run r2 ⊆ T 2 such that σ21(r2) = r1.
By completeness of σ32 there is a correct T S3 run r3 ⊆ T 3 such that σ32(r3) =
r2. Hence σ31(r3) = r1, establishing the completeness of σ31.

This completes the proof.

(cid:117)(cid:116)

Proof (of Theorem 1). According to Proposition 1, to show that a productive σ
is correct and complete it is suﬃcient to show that σ is:
1. Locally Safe: s(cid:48)
0

∗−→ y −→ y(cid:48) ⊆ T (cid:48) implies that s0

∗−→ x ∗−→ x(cid:48) ⊆ T for x = σ(y)

and x(cid:48) = σ(y(cid:48)) in S.
0 (cid:22) y ≺(cid:48) y(cid:48); by the Up condition on
By monotonicity of T S(cid:48) it follows that s(cid:48)
σ, it follows that s0 (cid:22) σ(y) (cid:22) σ(y(cid:48)); by assumption that T S is monotonically-
∗−→ x ∗−→ x(cid:48) ⊆ T for x = σ(y) and x(cid:48) = σ(y(cid:48)) in S.
complete it follows that s0
Hence σ is safe.

2. Locally Complete: s0

∗−→ x −→ x(cid:48) ⊆ T implies s(cid:48)
0

∗−→ y ∗−→∈ T (cid:48) for some

y, y(cid:48) ∈ S(cid:48) such that x = σ(y) and x(cid:48) = σ(y(cid:48)).
∗−→ x −→ x(cid:48) ⊆ T . By monotonicity of T S, s0 (cid:22) x (cid:22) x(cid:48); by the Down
Let s0
condition on σ, there are y, y(cid:48) ∈ S(cid:48) such that x = σ(y), x(cid:48) = σ(y(cid:48)), and y (cid:22) y(cid:48);
∗−→ y ∗−→ y(cid:48) ⊆ T (cid:48). Hence
by assumption that T S(cid:48) is monotonically-complete, s(cid:48)
0
σ is complete.

This completes the proof of correctness and completeness of σ.

(cid:117)(cid:116)

Proof (of Observation 2). As T S(cid:48) is monotonically-complete, it has a computa-
tion ˆσ(x) ∗−→ ˆσ(x(cid:48)) ⊆ T (cid:48) that satisﬁes the Down condition.
(cid:117)(cid:116)

Observation 3 SC is monotonically-complete wrt (cid:22).

Proof (of Observation 3). SC is monotonic wrt (cid:22) since every transition increases
its sequence. Given two sequences x, x(cid:48) ∈ S∗ such that x ≺ x(cid:48), let x(cid:48) = x · s1 · . . . ·
sk, for some k ≥ 1. Then x ∗−→ x(cid:48) via the sequence of transitions x → x · s1 →
(cid:117)(cid:116)
. . . → x · s1 . . . · sk. Hence SC is monotonically-complete.

Observation 4 LC is monotonically-complete wrt (cid:22).

The proof is similar to the proof of Observation 3.

Observation 5 (LC Conﬁgurations are Consistent) An n-chain conﬁgu-
ration c is consistent if every two chains in c are consistent. Let r be a run of
LC and c ∈ r a conﬁguration. Then c is consistent.

Multiagent Transition Systems with Faults

23

Proof (of Observation 5). The proof is by induction on the index k of a conﬁg-
uration in r. All empty sequences of the initial conﬁguration of r are pairwise
consistent. Assume the kth conﬁguration c of r is consistent and consider the
next r transition c → c(cid:48) ∈ T LC. The transition adds an element s to one se-
quence x ∈ c that either is a longest sequence, or x · s is consistent with another
longer sequence x(cid:48) ∈ c. As all sequences in c are pairwise consistent by assump-
tion, then they are also consistent with x · s by construction. Hence all sequences
of c(cid:48) are pairwise consistent and hence c(cid:48) is consistent.
(cid:117)(cid:116)

Hence the following implementation of SC by LC is well-deﬁned.

Proposition 11. σ2 is order-preserving wrt the preﬁx relation (cid:22) over consistent
n-chain conﬁgurations and is productive.

Proof (of Proposition 11). To show that σ2 is order-preserving it is suﬃcient to
show (Proposition 1) that:
1. Up condition: y (cid:22) y(cid:48) for y, y(cid:48) ∈ S1 implies that σ2(y) (cid:22) σ2(y(cid:48)) and y ≺ y(cid:48)

for y, y(cid:48) ∈ S1 implies that σ2(y) ≺ σ2(y(cid:48))

p := x(cid:48), and yq := y(cid:48)
q := Λ
p = σ2(y(cid:48)), c0 ∗−→ y ⊆ T 1 by

2. Down condition: s0

∗−→ x ∈ T 0, x (cid:22) x(cid:48) implies that there are y, y(cid:48) ∈ S1

such that x = σ2(y), x(cid:48) = σ2(y(cid:48)), c0

∗−→ y ⊆ T 1 and y (cid:22) y(cid:48).

Regarding the Up condition, assume that y (cid:22) y(cid:48) are consistent and that y(cid:48)
the unique longest chain in y(cid:48). Then σ2(y) = yp (cid:22) y(cid:48)
σ2(y) = yp ≺ y(cid:48)

p is
p = σ2(y(cid:48)), and if y ≺ y(cid:48)

p = σ2(y(cid:48)).

Regarding the Down condition, deﬁne yp := x, y(cid:48)

for every q (cid:54)= p ∈ P . Then x = yp = σ2(y), x(cid:48) = y(cid:48)
the same transitions that lead from s0 to x, and y (cid:22) y(cid:48) by construction.

To see that σ2 is productive, note that every LC transition extends one of
the chains in a conﬁguration. Hence, after a ﬁnite number of transitions, the
next LC chain will extend the longest chain in the conﬁguration, and activate
(cid:117)(cid:116)
SC.

Proof (of Proposition 5). Assume T S1, T S2, T S1(cid:48), T S2(cid:48) and σ as in the Propo-
sition and that y −→ y(cid:48) ⊆ T 2 & σ(y) ∈ S1(cid:48) implies that σ(y(cid:48)) ∈ S1(cid:48). Deﬁne
σ(cid:48) : C2(cid:48) −→ S1(cid:48) to be the restriction of σ to C2(cid:48). We have to show that σ(cid:48) is
correct. To do that, it is suﬃcient to show that σ(cid:48) is:
1. Locally Safe: s2 ∗−→ y −→ y(cid:48) ⊆ T 2(cid:48) implies that s1 ∗−→ x ∗−→ x(cid:48) ⊆ T 1(cid:48) for

x = σ(cid:48)(y) and x(cid:48) = σ(cid:48)(y(cid:48)) in S1.
This follows from the safety of σ, S1(cid:48) ⊆ S1 and the assumption that y −→
y(cid:48) ⊆ T 2(cid:48) & σ(y) ∈ S1(cid:48) implies that σ(y(cid:48)) ∈ S1(cid:48).

2. Productive: if any suﬃx of any inﬁnite correct computation of T S2(cid:48) activates

T 1(cid:48).
By monotonicity of T S2(cid:48), any inﬁnite correct computation r of T 2(cid:48) from x(cid:48)
1
has a transition t that is strictly increasing, and hence by σ satisfying the Up
condition, the transition t activates T 1(cid:48).

3. Locally Complete: s1 ∗−→ x −→ x(cid:48) ⊆ T 1(cid:48), implies that there are y, y(cid:48) ∈ C2(cid:48)

such that x = σ(cid:48)(y), x(cid:48) = σ(cid:48)(y(cid:48)), and s2 ∗−→ y ∗−→ y(cid:48) ⊆ T 2(cid:48).

24

Ehud Shapiro

Fig. 2. Some Steps in the Proof of Proposition 5 (with an example in yellow): While
T S2 (a subset of Messaging) implements T S1 (Dissemination), which in turn imple-
ments T S0 (Reliable Broadcast), T S1(cid:48) (a subset of Dissemination) is suﬃcient to im-
plement T S0. Hence, it may be more eﬃcient to employ the subset T S2(cid:48) (of Mes-
saging) instead of the full T S2 for the composed implementation of T S0. Still, T S1
(Dissemination) may have other applications (e.g. grassroots social network, sovereign
cryptocurrencies [33]), hence it would be useful to implement the entire T S1, but then
use only the subset T S2(cid:48) of T S2 in the composed implementation of T S0. Proposition
5 provides conditions that enable that.

By completeness of σ, there are y, y(cid:48) ∈ C2 such that x = σ(y), x(cid:48) = σ(y(cid:48)),
and s2 ∗−→ y ∗−→ y(cid:48) ⊆ T 2. By deﬁnition of C2(cid:48) as the domain of σ, y, y(cid:48) ∈ C2(cid:48).
As y ∗−→ y(cid:48) ⊆ T 2, then y (cid:22)2 y(cid:48). By assumption that T S2(cid:48) is monotonically-
complete, there is a computation s2 ∗−→ y ∗−→ y(cid:48) ⊆ T 2(cid:48).

This completes the proof.

(cid:117)(cid:116)

Proof (of Corollary 1). Given a generic transition system G over S, a correct
implementation σ1 of G by SC exists according to Proposition 2. The implemen-
tation σ2 of SC by LC is correct according to Proposition 4. Then, Propositions
3 and 5 ensure that even though a subset SC1 of SC was used in implementing
G, the result of the composition σ21 := σ2 ◦ σ1 is a correct implementation of G
(cid:117)(cid:116)
by LC.

Proof (outline of Proposition 6). The proof is similar to that of Proposition
2. Given a generic shared-memory multiagent transition system GS = ((S ×

Multiagent Transition Systems with Faults

25

N)P , c0, T GS) over P and S, we deﬁne a subset SCC1 of SCC and a mapping
σ1m from SCC to GS that together implement GS. The transition system SCC1
= ((S×P )+, s0, T SCC1) has the p-transition x·(q, s) → x·(q, s)·(p, s(cid:48)) ∈ T SCC1
for every x ∈ (S ×P )∗, q ∈ P , and every p-transition (s, i) → (s(cid:48), i(cid:48)) ∈ T GS. The
mapping σ1m : (S × P )+ (cid:55)→ S × NP takes the last element of its input sequence
and computes the ‘program counter’ of every agent based on the number of
elements by that agent in the input sequence, namely σ1m(x · (s, p)) := (s, i),
where i ∈ NP is deﬁned by iq being the number of occurrences of q in x · (s, p) for
every q ∈ P . The proof that σ1m is correct and complete has the same structure
(cid:117)(cid:116)
as the proof of σ1 in Proposition 2.

Proof (outline of Proposition 7). The proof is similar to that of Proposition 4.
We observe that, similarly to SC and LC, both SCC and LCC are monotonically-
complete wrt the preﬁx relation. For the implementation of SCC by LCC, σ2m
is the same as σ2, except that it returns the longest proper chain in its input,
namely a sequence over S × P (this will prove useful later in showing that σ2m
is resilient to certain faults). The proof that σ2m is order-preserving wrt (cid:22) and
productive is the same as that of Proposition 11. Hence, according to Theorem
(cid:117)(cid:116)
1, σ2m is correct and complete, which completes the proof.

Proof (of Proposition 8). We have to show that there is no partial order wrt
LCC is asynchronous. By way of contradiction, assume that for LCC = (P, (S ×
P )∗, c0, T LCC) there is a partial order (cid:22) on (S × P )∗ wrt which LCC is asyn-
chronous. In such a case, by deﬁnition, LCC is monotonic wrt (cid:22). Let c be a
conﬁguration in which cp is a longest chain, and let c ∗−→ ¯c be a computation
of q-transitions in which the chain of q is increased until ¯cq is longer than cp,
q (cid:54)= q ∈ P . By monotonicity of LCC, c (cid:22) ¯c. Let c → c(cid:48) be the p-transition
cp → cp · (s, p), with s ∈ S. Let d be the conﬁguration identical to c except that
p. Hence d, d(cid:48) satisfy the
dq := ¯cq, and let d(cid:48) be identical to d except that d(cid:48)
asynchrony condition (Deﬁnition 14) wrt c, c(cid:48), and by assumption that LCC is
asynchronous wrt (cid:22) it follows that the p-transition d → d(cid:48) ∈ T LCC. However,
this p-transition extends p even though dp is not the longest chain (dq is longer
(cid:117)(cid:116)
by construction). A contradiction.

p := c(cid:48)

Proof (of Proposition 9). If in conﬁguration c there is a block b known by q but
not by p, both correct, then this holds in every subsequent conﬁguration unless
p receives b. Hence, due to liveness of p-Receives-b, either the p-Receives-b from
q transition is eventually taken, or p receives b through a p-Receives-b transition
(cid:117)(cid:116)
from another agent. In either case, p eventually receives b.

Proof (outline of Proposition 10). Given LCC = (P, (S × P )∗, c0, T LCC) and
ABD = (P, B, c0, T ABD, λ), observe that ABD and LCC are monotonically-
complete wrt ⊆ and (cid:22), respectively. Deﬁne σ3 for each conﬁguration c ∈ C by
σ(c)p := σ(cid:48)
3 is deﬁned as follows. Given a set of blocks B, let sort(B)
be the sequence obtained by sorting B lexicographically, removing ⊥ blocks and
then possibly truncating the output sequence, where blocks (p, i, s) are sorted
ﬁrst according to the index of the block i ∈ N and then according to the agent

3(cp), where σ(cid:48)

26

Ehud Shapiro

p ∈ P , and truncation occurs at the ﬁrst gap if there is one, namely at the ﬁrst
index i for which the next agent in order is p but there is no block (p, i, s) ∈ B
for any s ∈ S ∪ {⊥}. Proposition 12 argues that σ3 is order-preserving, which
(cid:117)(cid:116)
allows the application of Theorem 1 and completes the proof.

Namely σ3 performs for each agent p a ‘round robin’ complete total ordering
of the set of block of its local state cp, removing undeﬁned elements along the
way, until some next block missing from cp prevents the completion of the total
order.

First, we observe that for every conﬁguration c ∈ r in an ABD run r, the
sequences in σ3(c) are consistent. Note that if x (cid:22) y and x(cid:48) (cid:22) y then x and x(cid:48)
are consistent.

Observation 6 (Consistency of σ3) Let r be a correct run of ABD. Then for
every conﬁguration c ∈ r, the chains of σ(c) are mutually consistent.

Proof (of Observation 6). First, note that in a correct run r, every conﬁguration
c ∈ r is equivocation free. Also note that σ(cid:48)
3 is monotonic wrt ⊆ and (cid:22), namely
3(B(cid:48)). For a
3(B) (cid:22) σ(cid:48)
if B ⊆ B(cid:48) and both B, B(cid:48) are equivocation free, then σ(cid:48)
conﬁguration c ∈ r, cp ⊆ B(c) for every p ∈ P and hence σ(cid:48)
3(cp) (cid:22) σ(cid:48)
3(B(c), and
therefore every two sequences σ(cid:48)
(cid:117)(cid:116)
3(cq) are consistent.

3(cp), σ(cid:48)

Next, we show that σ3 is order preserving.

Proposition 12. σ3 is order preserving wrt ⊆ and (cid:22).

Proof (of Proposition 12). According to deﬁnition 9, we have to prove two con-
ditions. For the Up condition, we it is easy to see from the deﬁnition of σ3 that
c(cid:48)
2 ∈ BP implies that σ(c(cid:48)
1 ⊆ c(cid:48)
2), as the output sequence of the
sort procedure can only increase if its input set increases.

1) (cid:22) σ(c(cid:48)

2 for c(cid:48)

1, c(cid:48)

For the Down condition, we construct an ABD representative conﬁguration
for a LCC conﬁguration so that if the ith element of the LCC longest chain is
(p, s), then the ABD conﬁguration has the block (p, i, s), as well as the blocks
(q, i, ⊥) for every other agents q (cid:54)= p. Speciﬁcally, given a LCC conﬁguration c
with a longest chain cl = (s0, p0), (s1, p1), . . . (sk, pk) for some l ∈ P , we deﬁne
the representative ABD conﬁguration c(cid:48) as follows. First, let B be the following
set of blocks B: For each i ∈ [k] B has the block (pi, i, si) and the blocks (q, i, ⊥)
3(B) = cl by construction. Let Bi := {(p, j, s) ∈
for every q (cid:54)= p ∈ P . Clearly σ(cid:48)
3(Bi) is the i-preﬁx of the
B : j ≤ i}. It is easy to see that for each i ∈ [k], σ(cid:48)
p := Bi. Hence,
longest chain cl. Then for each p ∈ P , where |cp| = i, we deﬁne c(cid:48)
3(c(cid:48)
σ(cid:48)
(cid:117)(cid:116)

p) = cp for every p ∈ P and thus σ3(c(cid:48)) = σ(c).

Proof (of Theorem 2). Assume transition systems and implementations as in the
theorem statement. As the composition of live implementations is live, and the
assumption is that the runs with safety faults are live, we only argue for safety
and conclude correctness.

Multiagent Transition Systems with Faults

27

Fig. 3. Some Steps in the Proof of Theorem 2: σ32 is resilient to F 3 and maps F 3(cid:48) to
F 2. σ21 is resilient to F 2. As a result, σ31 := σ21 ◦ σ32 is resilient to F 3 ∪ F 3(cid:48).

1. Assume that σ32 is resilient to F 3 ⊆ S32 \ T 3. We argue that σ31 is resilient
to F 3. Then For any T S3 run r ⊆ T 3 ∪ F 3, the run σ31(r) ∈ T S1 is correct,
namely σ31(r) ∈ T 1, since σ32 is F 3-resilient by assumption, and hence r(cid:48) =
σ32(r) is correct, and σ21 is correct by assumption, and hence σ21(r) is correct,
namely σ31(r) = σ21 ◦ σ32(r) ∈ T 1.

2. Assume σ21 is resilient to F 2 ⊆ S22 \ T 2, and F 3 ⊆ S32 \ T 3 satisﬁes
σ32(F 3) ⊆ F 2. We argue that σ31 is resilient to F 3. For any T S3 run
r ⊆ T 3 ∪ F 3, the run σ32(r) ∈ T S2 ∪ F 2 by assumption. As σ21 is F 2-resilient
by assumption, the run σ21 ◦ σ32(r) = σ31(r) is correct.

3. Assume that σ21 is F 2-resilient, σ32 is F 3-resilient, F 3(cid:48) ⊆ S32 \ T 3, and
σ32(F 3(cid:48)) ⊆ F 2. We argue that σ31 is resilient to F 3 ∪ F 3(cid:48). For any T S3 run
r ⊆ T 3 ∪ F 3 ∪ F 3(cid:48), the run σ32(r) ∈ T S2 ∪ F 2 by assumption. As σ21 is
F 2-resilient by assumption, the run σ21 ◦ σ32(r) = σ31(r) is correct.

(cid:117)(cid:116)

Proof (of Proposition 9). If in conﬁguration c there is a block b known by q but
not by p, both correct, then this holds in every subsequent conﬁguration unless
p receives b. Hence, due to liveness of p-Receives-b, either the p-Receives-b from
q transition is eventually taken, or p receives b through a p-Receives transition
(cid:117)(cid:116)
from another agent. In either case, p eventually receives b.

Next, we provide an operational characterization of grassroots via the notion

of interleaving.

Deﬁnition 27 (Joint Conﬁguration, Interleaving). Let F be a family of
distributed transition systems, P 1, P 2 ⊂ Π, P 1 ∩ P 2 = ∅. Then a joint conﬁg-

28

Ehud Shapiro

Fig. 4. Some Steps in the Proof of Theorem 5

uration c of c1 ∈ C(P 1) and c2 ∈ C(P 2), denoted by (c1, c2), is the conﬁgura-
tion over P 1 ∪ P 2 and S(P 1) ∪ S(P 2) satisfying c1 = c/P 1 and c2 = c/P 2. Let
r1 = c10 → c11 → c12 → . . . be a run of T S(P 1), r2 = c20 → c21 → c22 → . . .
a run of T S(P 2). Then an interleaving r of r1 and r2 is a sequence of joint
conﬁgurations r = c0 → c1 → c2 → . . . over P 1 ∪ P 2 and S(P 1) ∪ S(P 2)
satisfying:

1. c0 = (c10, c20)
2. For all i ≥ 0, if ci = (c1j, c2k) then ci+1 = (c1j(cid:48), c2k(cid:48)) where either j(cid:48) = j + 1

and k(cid:48) = k or j(cid:48) = j and k(cid:48) = k + 1.

Note that for ci = (c1j, c2k), i = j + k for all i ≥ 0.

Theorem 4 (Operational Characterization of Grassroots). Let F be a
family of distributed transition systems with no redundant conﬁgurations. Then
F is grassroots iﬀ for every ∅ ⊂ P 1, P 2 ⊂ Π such that P 1∩P 2 = ∅, the following
holds:

⊆ For every two runs r1 of T S(P 1) ∈ F and r2 of T S(P 2) ∈ F, every in-
terleaving r = c0 → c1 → . . . of r1 and r2 is a run of T S(P 1 ∪ P 1) ∈ F,
and

(cid:54)⊇ There is a run r of T S(P 1 ∪ P 1) ∈ F that is not an interleaving of any two

runs r1 of T S(P 1) ∈ F and r2 of T S(P 2) ∈ F.

Deﬁnition 29 of interactivity is algebraic. The following Proposition 13 pro-
vides an alternative operational characterization of interactivity via interleavings
of runs of its component transition systems, deﬁned next.

Proposition 13 (Operational Characterization of Interactivity). Let F
be a family of distributed transition systems. Then F is interactive iﬀ for every

Multiagent Transition Systems with Faults

29

∅ ⊂ P 1, P 2 ⊂ Π such that P 1 ∩ P 2 = ∅, there is a run of T S(P 1 ∪ P 1) ∈ F that
is not an interleaving of any run of T S(P 1) ∈ F and any run of T S(P 2) ∈ F.

Proof (outline of Theorem 4). The Theorem follows from the following Propo-
(cid:117)(cid:116)
sitions 13 and 14, and from Observation ??.

Deﬁnition 28 (Subsidiarity). A family of distributed transition systems F
upholds subsidiarity if for every ∅ ⊂ P 1, P 2 ⊂ Π such that P 1 ∩ P 2 = ∅, the
following holds:

T S(P 1) ∪ T S(P 2) ⊆ T S(P 1 ∪ P 2)

Deﬁnition 29 (Interactive). A family of distributed transition systems F is
interactive if for every ∅ ⊂ P 1, P 2 ⊂ Π such that P 1 ∩ P 2 = ∅, the following
holds:

T S(P 1 ∪ P 2) (cid:54)⊆ T S(P 1) ∪ T S(P 2)

We say that a conﬁguration in a transition system is redundant if it is not

reachable by a run of the transition system.

Proposition 14 (Operational Characterization of Subsidiarity). Let F
be a family of distributed transition systems with no redundant conﬁgurations.
Then F upholds subsidiarity iﬀ for every P 1, P 2 ⊂ Π such that P 1 ∩ P 2 = ∅,
and every two runs r1 of T S(P 1) ∈ F and r2 of T S(P 2) ∈ F, every interleaving
r = c0 → c1 → . . . of r1 and r2 is a run of T S(P 1 ∪ P 1) ∈ F.

Proof (of 14). To prove the ‘if’ direction by way of contradiction, assume that
the condition holds and that F is not interactive. Then there are P 1, P 2 as stated
for which every member of T S(P 1) ∪ T S(P 2) is a member of T S(P 1 ∪ P 2).
For every run r of of T S(P 1∪P 1) ∈ F there are two runs r1 of T S(P 1) ∈ F and
r2 of T S(P 2) ∈ F for which r is their interleaving. By Deﬁnition 27, T S(P 1∪P 1)
includes all p-transitions c → c(cid:48) ∈ T , p ∈ P , satisfying p ∈ P 1 ∧ (c/P 1 →
c(cid:48)/P 1) ∈ T 1 ∧ c/P 2 = c(cid:48)/P 2 or p ∈ P 2 ∧ (c/P 2 → c(cid:48)/P 2) ∈ T 2 ∧ c/P 1 = c(cid:48)/P 1.
This satisﬁes the condition of Deﬁnition 23 of union, namely T S(P 1)∪T S(P 1) ⊆
T S(P 1 ∪ P 2). A contradiction.

To prove the ‘only if’ direction by way of contradiction, assume that F is
interactive but that the condition does not hold. Hence for every run r of T S(P 1∪
P 1) ∈ F there are two runs r1 of T S(P 1) ∈ F and r2 of T S(P 2) ∈ F for which
r is their interleaving. By Deﬁnition 23 of union, this implies that T S(P 1) ∪
(cid:117)(cid:116)
T S(P 2) ⊇ T S(P 1 ∪ P 2), a contradiction. This completes the proof.

Proof (of Proposition 13). To prove the ‘if’ direction by way of contradiction,
assume that the condition holds and that F does not uphold subsidiarity. Then
there are P 1, P 2 satisfying the condition, and two transitions c1 → c1(cid:48) ∈ T (P 1),
c2 → c2(cid:48) ∈ T (P 2), such that the p-transition (c1, c2) → (c1(cid:48), c2(cid:48)) /∈ T (P 1 ∪ P 2).
Since F has no redundant conﬁgurations, there are runs r1 = c01 ∗−→ c1(cid:48) ∈ T (P 1)
and r2 = c02 ∗−→ c2(cid:48) ∈ T (P 1), and consider their interleaving r, in which the last
transition is (c1, c2) → (c1(cid:48), c2(cid:48)). By construction, the last transition of the run
r is (c1, c2) → (c1(cid:48), c2(cid:48)), implying that it is in T (P 1 ∪ P 2), a contradiction.

30

Ehud Shapiro

To prove the ‘only if’ direction by way of contradiction, assume that F up-
holds subsidiarity but that the condition does not hold. Hence there are runs
r1 and r2 of T S(P 1) and T S(P 2), with an interleaving r that is not a run of
T S(P 1 ∪ P 2). Consider the maximal preﬁx ˆr = (c10, c20) ∗−→ (c1, c2) of r that is
a run of T S(P 1∪P 2). Consider the r transition (c1, c2) → (c1(cid:48), c2(cid:48)) that extends
ˆr. By deﬁnition of union of transition systems, the transition (c1, c2) → (c1(cid:48), c2(cid:48))
is a transition of T S(P 1) ∪ T S(P 1). By the assumption that F is grassroots, it
follows that (c1, c2) → (c1(cid:48), c2(cid:48)) ∈ T S(P 1 ∪ P 2), and therefore ˆr is not maximal
(cid:117)(cid:116)
as constructed, a contradiction.

Fig. 5. Some Steps in the Proof of Theorem 3

Proof (of Theorem 3). Let F be a non-interfering family of distributed transition
systems that is monotonic and asynchronous wrt a partial order (cid:22), P 1, P 2 ⊂ Π
such that P 1 ∩ P 2 = ∅, r1 a run of T S(P 1) ∈ F, r2 a run of T S(P 2) ∈ F, r =
c0 → c1 → . . . an interleaving of r1 and r2. We argue that r is a run of T S(P 1 ∪
P 1) (See Figure 5). Consider any p-transition (c1, c2) → (c1(cid:48), c2(cid:48)) ∈ r. Wlog
assume that p ∈ P 1 (else p ∈ P 2 and the symmetric argument applies) and let
ˆc, ˆc(cid:48) be the T S(P 1 ∪ P 1) conﬁgurations for which ˆc/P 1 = c/P 1, ˆc(cid:48)/P 1 = c(cid:48)/P 1,
and ˆc/P 2 = ˆc(cid:48)/P 2 = c0/P 2. Since (c1 → c1(cid:48)) ∈ T (P 1), F is non-interfering, and
in ˆc, ˆc(cid:48) members of P 2 stay in their initial state, then by Deﬁnition 26 it follows
that the p-transition ˆc → ˆc(cid:48) ∈ T (P 1 ∪ P 2). Since cp = ˆcp by construction,
c0(P 2) (cid:22) c2 by monotonicity of T S(P 2), c0(P 1 ∪ P 2) (cid:22) c by monotonicity
of T S(P 1 ∪ P 2) and the assumption that (cid:22) is preserved under projection, it
follows that c → c(cid:48) ∈ T (P 1 ∪ P 2) by the assumption that it is asynchronous

Multiagent Transition Systems with Faults

31

wrt (cid:22) (Deﬁnition 11). As c → c(cid:48) is a generic transition of r, it follows that
r ⊆ T (P 1 ∪ P 2), satisfying the condition of Proposition 14, implying that F
upholds subsidiarity. Together with the assumption that F is interactive, we
(cid:117)(cid:116)
conclude that F is grassroots.

Deﬁnition 30 (Correct and Local Implementation among Protocols).
Let F, F (cid:48) be protocols over S, S(cid:48), respectively. A function σ that provides a map-
ping σ : S(cid:48)P (cid:55)→ SP for every P ⊆ Π is a correct implementation of F by F (cid:48)
if for every P ⊆ Π, σ is a correct implementation of T S(P ) by T S(cid:48)(P ), and it
is local if σ(c)p = σ(cp) for every conﬁguration c ∈ SP , P ⊆ Π.

Namely, σ is local if it is deﬁned for each local state independently of other local
states.

Example 13 (The implementation of LCC by ABD is not local). The reason why
σ3 that implements LCC by ABD is not local is that it depends on knowledge
of the set of agents P – the sort procedure cannot proceed beyond index i if an
i-indexed p-block is missing, for some p ∈ P .

In line with the example above, the following theorem shatters the hope of a
non-grassroots protocol to have a correct local implementation by a grassroots,
and can be used to show that a protocol is grassroots by presenting a correct
local implementation of it by another grassroots protocol.

Theorem 5 (Grassroots Implementation). A protocol that has a correct
local implementation by a grassroots protocol is grassroots.

Proof (of Theorem 5). We apply Theorem 4 in both directions. See Figure 4;
roman numerals (i )-(v ) in the proof refer to the Figure. Let F, F (cid:48) be families of
multiagent transition systems over S, S(cid:48), respectively, σ a correct implementation
of F by F (cid:48) and assume that F (cid:48) is grassroots. Let P 1, P 2 ⊂ Π such that P 1∩P 2 =
∅, r1 ⊆ T (P 1) a run of T S(P 1) ∈ F, r2 ∈ T (P 2) a run of T S(P 2) ∈ F,
(i ) and r = c0 → c1 → . . . an interleaving of r1 and r2 (Def. 27). Since σ
is a local and correct implementation of T S(P 1) by T S(cid:48)(P 1), there is a run
r1(cid:48) ∈ T (cid:48)(P 1) of T S(cid:48)(P 1), such that σ(r1(cid:48)) = r1; the same holds for P 2, r2(cid:48)
and r2. (ii ) Let r(cid:48) be the interleaving of r1(cid:48) and r2(cid:48) for which σ(r(cid:48)) = r; such
an interleaving can be constructed iteratively, with each p-transition of T S(P 1)
realized by the implementing computation of T S(cid:48)(P 1) for p ∈ P 1, and similarly
for p ∈ P 2. (iii ) Since F (cid:48) is grassroots by assumption, then by Proposition 4,
the ‘only if’ direction, r(cid:48) ∈ T (cid:48)(P 1 ∪ P 1) is a run of T S(cid:48)(P 1 ∪ P 1) ∈ F. (iv ) By
assumption, σ is a correct implementation of T S(P 1 ∪ P 2) by T S(cid:48)(P 1 ∪ P 2).
Hence σ(r(cid:48)) = r ∈ T (P 1 ∪ P 1) is a correct computation of T S(cid:48)(P 1 ∪ P 1), (v )
satisfying the conditions for the ‘if’ direction of Proposition 4, and concluding
(cid:117)(cid:116)
the F is grassroots.

