1

Blockchain-based Edge Resource Sharing for
Metaverse
Zhilin Wang†, Qin Hu† (Corresponding author), Minghui Xu‡, Honglu Jiang§
†Department of Computer & Information Science, Indiana University-Purdue University Indianapolis, USA
‡School of Computer Science & Technology, Shandong University, China
§Department of Computer Science & Software Engineering, Miami University, USA
Email: {wangzhil, qinhu}@iu.edu, mhxu@sdu.edu.cn, jiangh34@miamioh.edu

2
2
0
2

g
u
A
0
1

]

C
D
.
s
c
[

1
v
0
2
1
5
0
.
8
0
2
2
:
v
i
X
r
a

Abstract—Although Metaverse has recently been widely stud-
ied, its practical application still faces many challenges. One
of the severe challenges is the lack of sufﬁcient resources for
computing and communication on local devices, resulting in the
inability to access the Metaverse services. To address this issue,
this paper proposes a practical blockchain-based mobile edge
computing (MEC) platform for resource sharing and optimal
utilization to complete the requested ofﬂoading tasks, given the
heterogeneity of servers’ available resources and that of users’
task requests. To be speciﬁc, we ﬁrst elaborate the design of
our proposed system and then dive into the task allocation
mechanism to assign ofﬂoading tasks to proper servers. To
solve the multiple task allocation (MTA) problem in polynomial
time, we devise a learning-based algorithm. Since the objective
function and constraints of MTA are signiﬁcantly affected by the
servers uploading the tasks, we reformulate it as a reinforcement
learning problem and calculate the rewards for each state and
action considering the inﬂuences of servers. Finally, numerous
experiments are conducted to demonstrate the effectiveness and
efﬁciency of our proposed system and algorithms.

Index Terms—Metaverse, mobile edge computing, blockchain,

reinforcement learning

I. INTRODUCTION

Metaverse, which is considered as the next generation of
the Internet, has attracted researchers’ attention recently [1],
[2]. In Metaverse, people can interact with the virtual world
through technologies like virtual reality and augmented reality.
Currently, people typically access the servers of Metaverse
providers through Metaverse local devices, such as head-
mounted glasses, to get the Metaverse services. These devices
are required to assist users in accessing and experiencing
Metaverse services, as well as to process resource-intensive
computing tasks locally. However, using Metaverse local de-
vices for computing faces severe challenges: 1) the computing
and communication resources of the devices are limited; 2) the
locations of devices performing local computing are dispersed
and the devices may be constantly moving.

Fortunately, there is one existing solution for addressing
these challenges, namely mobile edge computing (MEC) [3],
[4]. Speciﬁcally, local devices can ofﬂoad their computing
tasks to proxy MEC servers, e.g., base stations; then, the MEC
servers ﬁnish those tasks and return the results to local devices.
Since the MEC servers are located close to local devices and
usually have enough computing resources, their involvement

This work is partly supported by the US NSF under grant CNS-2105004.

can reduce the latency of communication and computing, thus
providing low-latency and high-quality Metaverse services.

In practice,

the MEC servers in Metaverse are usually
responsible for computing multiple tasks from different users.
Since their resources are not inﬁnite, one single MEC server
may not be able to handle those ofﬂoading tasks in time, lead-
ing to low quality of Metaverse services. One possible solution
is to involve other MEC servers with extra resources to work
together for ofﬂoading computing in Metaverse, which makes
it necessary to establish a secure resource trading platform for
edge servers. To that aim, we propose a blockchain system
running on MEC servers to form a distributed computing
framework, named blockchain-based MEC platform, which
enables resource integration and optimal utilization among
MEC servers in a trustless environment.

Currently, there is no existing study focusing on implement-
ing blockchain-based MEC for resource sharing and optimiza-
tion in Metaverse. Although there exist several studies about
MEC in Metaverse, they focus on the latency analysis [5]
and incentive mechanism design [6]. While for research about
blockchain-based MEC [7], [8], no one has considered solving
the resource sharing problem in MEC.

To ﬁll this gap, the proposed blockchain-based MEC plat-
form aims to assist resource sharing and optimization in Meta-
verse via trading ofﬂoading tasks in a transparent but secure
way, so that more ofﬂoading tasks from Metaverse users can be
ﬁnished timely. This platform comprises multiple Metaverse
users, MEC servers, and a consortium blockchain system run-
ning the practical Byzantine fault tolerance (PBFT) consensus
protocol [9], [10]. And there are four main procedures in
the proposed system, i.e., data submission, task allocation,
ofﬂoading computing, and payment of tasks. As the pivotal
step, task allocation faces a critical challenge brought by the
heterogeneity of multi-task requests from users. Speciﬁcally,
given the limited computing and communication resources of
each MEC server and ofﬂoading tasks with different price
policies, data sizes, and completion time requirements, our
proposed system needs to assign the requested multiple tasks
to multiple servers under various constraints. In addition, since
the ofﬂoading tasks are time-sensitive, the task allocation step
is expected to make the (near) optimal decisions as fast as
possible so as to reduce the latency of the whole system.

To address these challenges, we design a learning-based task
allocation mechanism. Speciﬁcally, we ﬁrst formulate the task

 
 
 
 
 
 
allocation issue as an integer-programming problem; then, we
analyze the utilities of different decisions by considering the
resource constraints of servers and the time requirements of
tasks; according to our analysis, the multiple tasks allocation
(MTA) problem is NP-complete, and the time complexity is
too high, so we design a learning-based algorithm to ﬁnd
its approximate optimal value in polynomial time. Since the
objective function and constraints of the MTA problem are
conditional, which are affected by the servers uploading the
tasks to blockchain, in our proposed learning-based solution,
we ﬁrst transform the MTA problem into a reinforcement
learning problem and then calculate the rewards for each state
and action considering the inﬂuences of the server source of
tasks.

To the best of our knowledge, we are the ﬁrst to implement
blockchain-based MEC in Metaverse. The main contributions
of this paper are summarized as below:

• We are the ﬁrst to propose a practical blockchain-based
MEC platform for resource sharing and optimal uti-
lization in Metaverse. Our proposed system can satisfy
the demands of low latency, multiple requests, energy
efﬁciency, incentive compatibility, and data privacy pro-
tection for Metaverse users.

• We design a task allocation mechanism for edge resource
sharing. Under the heterogeneous constraints, we ﬁrst
formulate an MTA problem for reasonably distributing
ofﬂoading tasks among multiple MEC servers.

• We propose a learning-based solution to ﬁnd the ap-
proximate optimal solution for the MTA problem with
polynomial time complexity. We speed up the learning
process via identifying available actions for each state.
• We conduct extensive experiments to verify the effec-
tiveness, efﬁciency, and validity of our proposed system,
mechanisms, and algorithms.

II. SYSTEM MODEL

A. System Overview

Our proposed blockchain-based edge resource sharing plat-
form is illustrated in Fig. 1 for supporting the Metaverse
applications, consisting of mobile devices as Metaverse users,
MEC servers, and the consortium blockchain running with
practical Byzantine fault tolerance (PBFT) consensus. Here
we assume that users in our considered system are devices
with ofﬂoading requests to MEC servers. Based on the arrival
time of ofﬂoading tasks in the blockchain network, we assign
a speciﬁc task number to each of them. Speciﬁcally, we deﬁne
T = {t1, · · · , tj, · · · , tm} as the set of ofﬂoading tasks from
users with m denoting the number of all ofﬂoading tasks. Let
S = {s1, · · · , si, · · · , sn} denote the set of MEC servers in
the Metaverse with n being the total number of servers.

The workﬂow of our proposed system can be described as
below: 1) Data Submission: The Metaverse users upload their
raw data and the descriptions of tasks to their nearest MEC
servers. We use Rj < pj, Dj, τe,j > to denote the description
of task tj, where pj is the unit price of per CPU cycle of
computing for ﬁnishing tj, Dj is the data size of tj, and τe,j
is the time requirement for ﬁnishing tj. 2) Task Allocation:

2

Fig. 1: The illustration of our proposed system.

To protect

Once server si receives the data and Rj from users, the data
is stored locally and Rj is submitted to the blockchain for task
allocation which will be elaborated in the following sections.
3) Ofﬂoading Computing: According to the task allocation
results, server si will either send the data of tasks it received
to appropriate servers or ﬁnish the computing of the tasks
locally. Then, the servers start to work on the allocated tasks
and will broadcast the computing results to the blockchain
network when the tasks are completed. 4) Payment of Tasks:
The users ﬁrst pay ofﬂoading computing fees through the
blockchain network where they can also get the computing
results later. Each server can get its own payment through the
coordination of the blockchain.
the privacy of

the computing results, we
can employ the asymmetric encryption technique, such as
Rivest–Shamir–Adleman (RSA) algorithm [11], in this system.
Speciﬁcally, at the beginning of each round, the users submit
the public keys generated by RSA along with the ofﬂoading
task requests while keep their private keys secret. After the
tasks are ﬁnished by the MEC servers, the computing results
will be encrypted with the public keys of the corresponding
users. Then, the users can download the encrypted results via
accessing the blockchain and decrypt the results with their
private keys. Since the amount of tasks received by each server
can be different, we deﬁne the tasks submitted by server si
as Ts,i = {t1, t2, · · · , tm(cid:48)}, where m(cid:48) ≤ m is the number of
tasks from si. For example, if t1, t3, and t20 are submitted by
s1, then we have Ts,1 = {t1, t3, t20}.

B. Consortium Blockchain

The consortium blockchain is running on the MEC servers,
where all servers are authorized nodes and thus can be
trusted. Besides, as mentioned earlier, we implement PBFT,
to assist
a lightweight consensus protocol
servers in reaching consensus for generating blocks to help
disseminate task descriptions and allocation results, as well as
enforce payment distribution in a round-by-round manner.

in blockchain,

Here is the workﬂow of the blockchain network in our
the beginning of each round, each server

system: 1) At

Metaverse UsersMECServersVirtual WorldBlockchain-based MECInteractionpublishes its available resources for ﬁnishing tasks, including
both computing and communication resources (see Section III
for details), and uploads received task descriptions Rj to the
blockchain network. 2) All of the above-mentioned informa-
tion will be broadcast in the blockchain network so that the
leader node can allocate the received tasks to appropriate
servers with the help of the task allocation mechanism detailed
in the next section. Then the allocation decisions, resource
information of the MEC servers, and task descriptions will be
packaged into a new block. 3) If si is assigned to process
the tasks submitted by itself, it can start processing right
away; otherwise, it has to transfer the received data to other
servers for ﬁnishing the ofﬂoading tasks. 4) After tasks are
ﬁnished, the results will be broadcast so as to be recorded
on the blockchain. 5) The users can get the results from the
blockchain, and the servers will be paid accordingly.

Overall, the blockchain offers a decentralized and trusted
platform to conduct resource sharing for MEC empowered
Metaverse applications, thus enabling ofﬂoading computing
for Metaverse users to overcome the challenges posed by the
insufﬁcient computing power of individual server and device
mobility. With the combination of MEC and blockchain,
our proposed resource sharing platform can handle multiple
ofﬂoading tasks in a efﬁcient and distributed manner. However,
regarding the pivotal step, we need to design a task allocation
mechanism to assign ofﬂoading tasks to proper MEC servers
so as to achieve the optimal resource utilization while satisfy-
ing user requests, which will be discussed in the next section.

III. DESIGN OF TASK ALLOCATION MECHANISM

In this section, we detail the design of task allocation mech-
anism by modeling all possible computing and communication
costs for si

A. Computing Cost Model

i,j = αiµi,jf 2

According to [12], if task tj is allocated to server si, its
energy cost can be calculated as Ecomp
i , where αi
is the parameter related to the architecture of CPU, µi,j =
Djθi is the total CPU cycles required to ﬁnish task tj on
the MEC server si with θi being the CPU cycles required to
process unit data sample, and fi is the CPU frequency used to
compute the ofﬂoading tasks. Besides, the time consumption
of computing tj on si can be calculated by T comp
. Let
µi be the maximum available computing capacity of si. Then
αi, θi, fi and µi will be submitted to the blockchain as the
available computing resource of si for task allocation.

i,j = µi,j
fi

3

calculate the transmission time as T comm
. And the
energy consumption of data transmission can be calculated by
Ecomm
. Note that Bi, Hi, and Gi are uploaded
to the blockchain network as the available communication
resource of si.

i,j = HiT comm

i,j

i,j

= Dj
ri

C. Utility Model

After tj is completed, server si will receive the payment
from users, denoted by pjµi,j for task tj, which is the product
of unit price per CPU cycle and the total number of consumed
CPU cycles. Then, the utility of si regarding ﬁnishing tj in
the system is the difference between the received payment and
the local cost.

i,j

i,j = (pjµi,j − Ecomp

Since there are two possible task allocation results for any
server si with respect to processing task tj, i.e., computing tj
locally or not, which will signiﬁcantly affect the utility of si,
we deﬁne an indicator 1i,j to capture this: if tj is assigned
to si, then 1i,j = 1; otherwise, 1i,j = 0. If tj ∈ Ts,i, the
)1i,j +
utility can be expressed as U I
)(1−1i,j), where λpjµi,j is the intermediary
(λpjµi,j −Ecomm
fee paid by the server ﬁnishing tj, with λ being a predeﬁned
constant parameter that can be known globally; and the total
(1−1i,j). While
time consumption is T I
if tj /∈ Ts,i, the utility is deﬁned as U N
i,j = [(1 − λ)pjµi,j −
Ecomp
i,j +
i,j
i∗,j )1i,j, where T comm
T comm
is the transmission time of server
i∗,j
submitting tj. Thus, we can get the utility function and time
consumption as: if tj ∈ Ts,i, Ui,j = U I
i,j;
otherwise, Ui,j = U N

]1i,j, and the total time consumption is T N

i,j and Ti,j = T I

i,j and Ti,j = T N
i,j.

i,j = (T comp

i,j = T comp

1i,j +T comm

i,j

i,j

i,j

D. Problem Formulation

Recall the goal of task allocation for achieving resource
integration and optimal utilization in our system, we would
like to make sure that more tasks can be proceed in time and
thus servers can get more rewards. So we can formulate it into
a multi-task allocation (MTA) problem as follows:

MTA : arg max

1i,j

n
(cid:88)

m
(cid:88)

i=1

j=1

Ui,j

s.t. : C1 :

m
(cid:88)

j=1

µi,j1i,j ≤ µi,

C2 : Ti,j ≤ τe,j,

C3 :

m
(cid:88)

j=1

1i,j ≤ 1,

B. Communication Cost Model

If tj is submitted by si but not ﬁnally assigned to si,
si would only be responsible for transmitting the data of
tj to the determined server. We deﬁne Bi as the allocated
bandwidth of si for data transmission; let Hi and Gi be
the transmission power and channel power gain, respectively.
Based on Shannon Bound, we can get the data transmission
rate as ri = Bi log2(1 + HiGi
δ2 ), where 0 ≤ δ ≤ 1 is
the Gaussian noise during the transmission. Then we can

C4 : i ∈ {1, 2, · · · , n} , j ∈ {1, 2, · · · , m} , 0 < n ≤ m.

In the above MTA problem, the optimization objective is to
maximize the total utility of all servers given requested tasks
from Metaverse users; C1 is the computing capacity constraint
to make sure that the selected tasks can be proceeded by server
si; C2 is the time constraint, consisting of computing time and
transmission time to ensure that tj can be completed in time;
C3 guarantees that one task can only be assigned to one server,
and it is possible that some servers are not assigned with tasks;
C4 deﬁnes the domain of this optimization problem.

Theorem III.1. MTA is an NP-complete problem.

j and value p(cid:48)

i has its weight D(cid:48)

Proof. The MTA problem can be described as a complex
multiple knapsacks problem (MKP), which has been proved
to be NP-complete in [13]. Speciﬁcally, MKP is deﬁned as:
let T (cid:48) = {t1, t2, · · · , tm(cid:48)} denote the set of m items, and
let S (cid:48) = {s1, s2, · · · , sn(cid:48)} as the set of n knapsacks, and
m(cid:48) > n(cid:48). Each item t(cid:48)
j and
each knapsack has its maximum weight D(cid:48)
i. The goal is to
decide where should each item be placed in the knapsacks so
that the total utility is maximized. In other words, it aims to
(cid:80)m
(cid:80)n
1i,j, where
solve the following problem: max1i,j
j
1i,j is an indicator: when 1i,j = 1, item t(cid:48)
j will be assigned
i, and otherwise, 1i,j = 0. The time complexity
to knapsack s(cid:48)
is O(nm), which means MKP cannot be solved in polynomial
time. MKP can be reduced to the simpliﬁed MTA problem.
In the original MTA problem, we need to allocate multiple
tasks to multiple servers with computing resource and time
consumption constraints. The objective of MTA is to maximize
(cid:80)n
i Ui,j. If we remove the
the total utility, i.e., max1i,j
j
time constraints and assume that every server offers the same
computing and communication resources for each task, we get
the simpliﬁed MTA problem which is equivalent to MKP. As
we can see, the simpliﬁed MTA also has the time complexity
of O(nm), so MTA is an NP-complete problem.

i p(cid:48)
j

(cid:80)m

IV. LEARNING-BASED SOLUTION FOR MTA

Based on the analysis in Section III-D, we know that
the MTA problem is NP-complete, so we need to design a
computationally efﬁcient algorithm to solve it. In this section,
we design a learning-based algorithm with (cid:15)-greedy strategy.

A. Problem Reformulation

To begin with, we need to reformulate the MTA problem
based on the Q-learning algorithm [14], [15]. There are three
main components in Q-learning, i.e., state space, action space,
and reward function, which are detailed as follows.

1) State Space: We denote the state space of the agent,
i.e., the MEC server executing the Q-learning algorithm to
allocate tasks, as Ω < T , P, D, Te >. Speciﬁcally, T =
{t1, t2, · · · , tm} is the set of tasks, P is the set of the
i.e., P = {p1, p2, · · · , pm}, D =
unit prices of tasks,
{D1, D2, · · · , Dm} is the set of data sizes of tasks, and the
set of execution time is deﬁned as Te = {τe,1, τe,2, · · · , τe,m}.
In other words, the state space is composed of all tasks with
the corresponding descriptions, including prices, data sizes,
and execution time requirements. Thus, Ω is a matrix with m
rows and 4 columns. The agent selects an action for each state
based on the current task requirements and resource conditions
of all servers. Once the action is chosen at a state, the agent
will turn to the next state to conduct action selection.

2) Action Space: Since servers have different amount of
available resources, such as total CPU cycles, CPU cycle
frequencies, and communication bandwidth, we can denote
the action space as A < S, M, F, B, H, G, α, Θ >. In detail,
S is the set of servers, and M = {µ1, µ2, · · · , µn} is the set
of total available CPU cycles, and F = {f1, f2, · · · , fn} is the

4

set of CPU frequencies, and B = {B1, B2, · · · , Bn} is the set
of communication bandwidth; as for H = {H1, H2, · · · , Hn}
and G = {G1, G2, · · · , Gn},
they are the sets of trans-
mission power and channel power gain, respectively; and
α = {α1, α2, · · · , αn} is the set of the parameter correlated
to the CPU architectures and Θ = {θ1, θ2, · · · , θn} is the
set of CPU cycles required for processing one data sample.
Speciﬁcally, there are two statuses for each action, i.e., selected
and not selected, and the agent can choose only one action in
one state while one action can be selected multiple times in all
the states. This is to ensure that one task can only be assigned
to one server; however, one server could process multiple tasks
if it has sufﬁcient resources. In this way, we can know that
the action space is an n × 8 matrix.

3) Reward Function: The objective of MTA problem is to
maximize the utility by allocating tasks to proper servers,
so the rewards here are deﬁned by the utilities. In other
words, the rewards are determined by the allocation decision,
resource conditions, and time constraints. Besides, the rewards
would also be affected by where the task is submitted from
to the blockchain network. Thus, in the design of the reward
function, we need to consider all of these aspects.

We evaluate whether our system can process tj by two
criteria: 1) server si has enough computing power to be
devoted to the computation, i.e., µi,j ≤ µi; and 2) si is able
to process the task tj within the required time constraint, i.e.,
Ti,j ≤ τe,j. The second criteria is C2 while the ﬁrst criteria is a
weak C1. And C1 can only be used when selecting actions and
updating Q-value, which will be discussed in Section IV-B.
If tj ∈ Tsi, the reward function can be expressed as:

Ui,j =

(cid:40)

pjµi,j − Ecomp
,
λpjµi,j − Ecomm

i,j

i,j

µi,j ≤ µi and Ti,j ≤ τe,j,

, otherwise.

(1)

When tj is from si, if si is not assigned to process tj, it has
to transmit tj to another server and obtain some intermediary
fee, so the reward is λpjµi,j − Ecomm
; but if si is able to
process tj, it can get the payment piµi,j at the cost of the
computing energy consumption Ecomp
, and thus, the reward is
Ui,j = piµi,j − Ecomp

i,j

i,j

.

i,j

If tj /∈ Tsi, the reward function is as below:

Ui,j =

(cid:40)

(1 − λ)pjµi,j − Ecomp
0,

i,j

, µi,j ≤ µi and Ti,j ≤ τe,j,

otherwise.

(2)

The logic of (2) is similar to (1): when tj is not from si,
if si has the capability to process tj, it can get the reward the
same as in (1) but has to pay the intermediary fee; and it will
get nothing if it cannot process this task, which means that Ai
as one action cannot be chosen in state Rj.

Based on the above analysis, we know that the reward
functions are the transformation of the objective function
under certain constraints. For simplicity, we summarize the
calculation of reward functions in Algorithm 1. To get an n×m
matrix U = {U1,1, U1,2, · · · , Ui,j, · · · , Un,m} containing the
reward value for each state and each action, we need to
calculate the energy consumption and time cost for both
computing and communication processes (Lines 3-6). And

then we can calculate and return the rewards based on (1)
and (2) (Lines 7-25).

Algorithm 1 StateActionReward
Require: Ω, A
Ensure: U

1: for i ∈ {1, · · · , n} do
2:
3:

for j ∈ {1, · · · , m} do
Ecomp
i,j ← αiµi,jf 2
i
i,j ← µi,j
T comp
fi
i,j ← Dj
T comm
ri
Ecomm
i,j ← HiT comm
i,j
if sj ∈ Tsi then
Ti,j ← T comp
(1 − 1i,j)
1i,j + T comm
if (µi,j ≤ µi) and (Ti,j ≤ τe,j) then

i,j

i,j

Ui,j ← pjµi,j − Ecomp

i,j

else

Ui,j ← λpjµi,j − Ecomm

i,j

end if

end if
if sj /∈ Tsi then
Ti,j = (T comp
if (µi,j ≤ µi) and (Ti,j ≤ τe,j) then
Ui,j ← (1 − λ)pjµi,j − Ecomp

i,j + T comm

i∗,j )1i,j

i,j

else

Ui,j ← 0

end if

4:

5:
6:
7:
8:
9:
10:

11:
12:
13:
14:
15:
16:

17:
18:
19:
20:
21:
22:

end if
end for

23:
24: end for
25: return U

5

one server is selected multiple times due to its non-zero value
(this could happen in Q-learning, especially when the number
of servers is small), but its capacity of available computing
resources is not enough to handle all of these tasks even if the
reward for each task is positive or optimal.

To address this challenge, we design a dynamic table to
record the accumulative µi,j, which can be denoted by µacc
i,j .
This table shares the same dimensions with the rewards table U
generated in Algorithm 1. Hence, we can design the following
mechanism to get the available actions for each state.

1,j, Anz

2,j, · · · , Anz
n(cid:48),j

j = (cid:8)Anz

First, naively select the actions with non-zero values from
U and initialize the values as zero to reduce the computational
cost by avoiding going through the whole action space again.
(cid:9) be the set to contain the
Let Anz
actions with non-zero values at state Ωj. In other words, the
agent only needs to check the actions in Anz
j at state Ωj. Then,
based on the selected actions, we can get a value of µi,j. In
this way, after multiple rounds, we can get the accumulative
value µacc
i,j . This is a dynamic process, which means that only
those selected actions can contribute to their corresponding
accumulative values. Besides, we need to consider the time
constraint, ensuring that one server can compute the allocated
tasks in time. We use τ ava
to demonstrate the available
e,i
computing time for the actions in Anz
j , which is calculated
via τ ava
i,j )/fi. Next, we can decide whether an
action Anz
i,j < µi
and τ ava
i,j is considered as available action;
otherwise, Anz
j . In this way, we
can get a set of available actions Aava
at state Ωj, and the ﬁnal
action Aj should be chosen from it according to the selection
policy which will be discussed in Section IV-B2.

i,j is available by the following rules: if µacc

i,j will be discarded from Anz

e,j > τe,j, then Anz

e,i = (µi − µacc

j

The whole process of choosing available actions for a
speciﬁc state is summarized in Algorithm 2. At the beginning,

B. Learning Process

1) Available Actions: To avoid selecting unmatched servers
that cannot process such tasks as actions, and to improve the
learning efﬁciency by reducing the time complexity, we need
to clarify which set of actions are available in each state before
selecting one as the action. Thus, we deﬁne the concept of
available actions as below.

Deﬁnition IV.1 (Available Actions). Assume the agent is at
state Ωj, Aava
= {s1, s2, · · · , sn(cid:48)} is the set of available
actions, i.e., servers, that can process tj under the constraints
of C1−C4, with n(cid:48) ≤ n being the number of available actions.

j

Since the transfer of states is a dynamic process and the
states will affect each other, one of the most direct effects
is that once an action is selected, its computing power will
be reduced and therefore will constrain the action selection
of the subsequent states. Therefore, we evaluate the available
actions of each state one by one. Recall the discussions in
Section IV-A3, we get an n × m rewards table U, and we
can use Uj = {U1,j, U2,j, · · · , Un,j} to denote all the possible
rewards of Ωj. Generally speaking, Uj has three types of value:
positive, zero, and negative. We can select those actions with
non-zero values as the available actions. However, this naive
method can lead to some severe consequences. For example,

Algorithm 2 StateAvailableActions
Require: A, Ωj, U
Ensure: Aj

1: Initialize A, Aava
, µacc
i,j
2: for i ∈ {1, · · · , n} do
if Ui,j (cid:54)= 0 then
3:

j

Append Ai into Anz
j

i,j from Anz
j
e,i ← (µi − µacc
i,j )/fi
i,j ∈ Aava
then

j

4:
end if
5:
6: end for
7: Choose Anz
8: τ ava
9: if Anz
10:
11:
12:
13:
14: else
15:

if (µacc

end if

i,j into Aava

j

Append Anz
Aj ← Anz
i,j

16:
17: end if
18: µacc
19: return Aj

i,j ← µacc

i,j + µi,j

i,j > µi) or (tava

i,j < τe,j) then

Remove Anz
Aj ← choose another action from Anz
j

i,j from Aava

j

j

j , Aava

, and µacc
we initialize Anz
(Line 1), and we get the
i,j
non-zero values from U (Lines 2-6). Next, we choose an action
from Anz
and calculate its available computing time (Lines
j
7-8). Then, we can determine whether the selected action is
available or not (Line 9-17), and we update µacc
(Line 18).
i,j
Finally, we get the available action Aj at state Ωj. Please note
that the selection method in Lines 7 and 12 will be further
discussed in Section IV-B2. In general, Algorithm 2 is the
transformation of C1 and C2 in the MTA problem.

2) The Update of Q-table: The Q-table is applied during
the learning process, which is used to facilitate the action
selection. First, we generate an m × n matrix as the Q-table
Q(Ω, A), and initialize its value with 0. Then, we need to
update the Q-value at each state. Here we use the following
equation to update Q-value:

Q(Ωj, Ai)
= Q(Ωj, Ai) + α[Ui,j + γ max Q(Ω(cid:48)

j, Aava
j

) − Q(Ωj, Ai)],

j, Aava
j

where Q(Ω(cid:48)
) means all the possible Q-value at next
stage; 0 ≤ α ≤ 1 is the learning rate and 0 ≤ γ ≤ 1 is the
discount factor.

The agent selects the action based on the Q-value, and we
adopt the (cid:15)-greedy algorithm as the policy of action selection.
The (cid:15)-greedy strategy selects the action with the largest ex-
pected rewards most of the time, and the parameter (cid:15) balances
exploration and exploitation. We can use a larger value of (cid:15)
to allow the agent to exploit what have been learned, and the
agent will explore more actions that have not been learned with
a smaller (cid:15). The learning process requires multiple rounds so
that the agent can learn more about the rewards and ﬁnd the
best solution. For each episode k ∈ {1, 2, · · · , K} where K
is the total episodes of learning, we let the agent go through
all the states and ﬁnd a solution, and then we compare all the

Algorithm 3 Learning-based Algorithm for the MTA Problem

Require: Ω, A, K
Ensure: T S∗

j ← actions with non-zero values in U

Aj ← randomly selected from Anz
j

1: Initialize Q(Ω, A), k
2: U ← StateActionReward(Ω, A)
3: Anz
4: Generate a random value x
5: if x ≤ (cid:15) then
6:
7: else
8:
9: end if
10: for k ∈ {1, · · · , K} do
11:
12:
13:

for j ∈ {1, · · · , m} do

Aj ← max Q(Ωj, .)

Aj ← StateAvailableActions(A, Ωj, U)
Q(Ωj, Ai)
γ max Q(Ω(cid:48)

) − Q(Ωj, Ai)]

←
j, Aava
j

Q(Ωj, Ai) + α[Ui,j +

14:

end for
T S ← the sum of all rewards in this episode

15:
16: end for
17: T S∗ ← max T S
18: return T S∗

6

solutions T S and choose the one T S∗ with the highest sum
of rewards as the optimal solution.

Algorithm 3 is the detailed process of the learning-based
solution for the MTA problem. We ﬁrst initialize Q(Ω, A), k,
and get Ui,j via Algorithm 1 (Lines 1-2). The (cid:15)-greedy strategy
is implemented to select the action (Lines 3-9). Then, the
learning process will last until episode k reaches the predeﬁned
total number of episodes K (Lines 10-16), and the optimal
solution will be obtained and returned (Line 17-18).

C. Complexity Analysis

The

three

learning-based

algorithm comprises

sub-
algorithms, so the complexity analysis needs to take all
the computational complexity
of them into account. First,
of Algorithm 1 is O(m × n), and the complexity of
Algorithm 2 is O(n). As for
the time complexity of
Algorithm 3,
the time
it
complexity of
learning-based solution should be
O(m × n) = O(k × n) + O(n) + O(m × n), which means
that we can solve the MTA problem in polynomial time.

is O(k × n). Thus,
the

in general,

V. EXPERIMENTAL EVALUATION

A. Experimental Setting

In our experiments1, we consider a blockchain-based MEC
system in Metaverse with 20 servers and 50 ofﬂoading tasks as
the primary setting. We change relevant parameters to analyze
the impacts of the numbers of servers and tasks on the total
rewards and the time complexity, as well as the inﬂuence
of Q-learning parameters on the convergence. For clarity, we
summarize the basic parameter settings in Table I.

TABLE I: Basic Parameter Settings

m = 50

n = 20
pj = [1, 10] Dj = [10, 20]
fi = [1, 10] Hi = [5, 10]
Bi = [5, 10]
δ = 0.01

Di = [200, 400]
τe,j = [1, 100]
Gi = [5, 10]
(cid:15) = 0.9

θ = 0.01
γ = 0.9
α = 0.01
K = 500

B. Experimental Results

First, we design experiments to prove the effectiveness of
our proposed learning-based algorithm for the MTA problem.
To that aim, we compare the learning-based solution with two
benchmark algorithms, i.e., the random allocation algorithm
and the greedy-based algorithm. Speciﬁcally, The random al-
location algorithm allocates each task to one server randomly;
and the greedy-based algorithm selects the server with the
maximum reward for each task.We run these three algorithms
with different numbers of servers and tasks to obtain the com-
parison results as shown in Fig. 2. From Figs. 2(a) and 2(b),
we can see that
the learning-based algorithm can always
outperform the other two algorithms with higher rewards.
The random strategy ﬂuctuates a lot due to the randomness
of each selection. The greedy and learning algorithms, in
contrast, perform more smoothly. Intuitively, more servers

1The code is available in: https://github.com/wzljerry/Blockchain-based-

Edge-Resource-Sharing-for-Metaverse

this trend does not hold. There are at

will not result in higher total rewards; however, more tasks
would increase the overall revenue. But from our experimental
three
results,
reasons for it. First, C1–C4 constrain the allocation decisions,
resulting in dynamic decisions as the number of servers or
tasks changes, which does not have a cumulative effect on
rewards; second, although more tasks result in more rewards,
they also increase the cost of computation and communication;
and third, servers and tasks are heterogeneous with different
parameters, exacerbating the results from the ﬁrst two reasons.

least

(a) The Number of the MEC Servers

(b) The Number of Ofﬂoading Tasks

Fig. 2: The inﬂuences of the numbers of MEC servers and
ofﬂoading tasks on the total rewards.

Then, we explore the inﬂuences of unit prices and data sizes
of ofﬂoading tasks on the total rewards. We increase pj and
Dj with the percentage from 10% to 100%, and the results are
shown in Fig. 3. It is clear that both the increase of pj and Dj
will affect the total rewards. When the unit prices are larger,
the users will pay more to the system, so the total rewards will
increase. And, if the data size of tj is larger, then the CPU
cycles required to process tj will also be increased, and thus
the total rewards will be higher.

(a) The Increase Percentage of Unit
Price

(b) The Increase Percentage of Data
Size

Fig. 3: The inﬂuences of unit prices and data sizes of ofﬂoad-
ing tasks on the total rewards.

Last but not least, we mainly examine the convergence per-
formance of the learning algorithm. Q-learning is determined
by two main variables, i.e., α and γ. From Fig. 4(a), we can see
that when α is smaller, the current choice is more inﬂuenced
by experience and lacks further exploration, so although it
achieves a high payoff at the beginning, the convergence rate
does not improve effectively. From Fig. 4(b), we can see that
the larger the value of γ, the better the convergence. This is
because α takes into account the effect of future rewards on
current choices, so as γ becomes larger, it emphasizes rewards
more and thus learns the optimal combinations more efﬁciently
to speed up the convergence.

7

(a) The Learning Rate α.

(b) The Discount Factor γ.

Fig. 4: The convergence of the learning-based algorithm.

VI. CONCLUSION

In this paper, we establish a blockchain-based MEC plat-
form for resource sharing and optimization to facilitate Meta-
verse applications. In particular, we design a task allocation
scheme to assist our proposed system. To that aim, a learning-
based algorithm is proposed to help the system make task al-
location decisions in polynomial time. Numerous experiments
prove that our proposed system and algorithms are efﬁcient.

REFERENCES

[1] M. Xu, W. C. Ng, W. Y. B. Lim, J. Kang, Z. Xiong, D. Niyato,
Q. Yang, X. S. Shen, and C. Miao, “A full dive into realizing the edge-
enabled metaverse: Visions, enabling technologies, and challenges,”
arXiv preprint arXiv:2203.05471, 2022.

[2] S. Mystakidis, “Metaverse,” Encyclopedia, vol. 2, no. 1, pp. 486–497,

2022.

[3] N. Abbas, Y. Zhang, A. Taherkordi, and T. Skeie, “Mobile edge
computing: A survey,” IEEE Internet of Things Journal, vol. 5, no. 1,
pp. 450–465, 2017.

[4] S. A. Huda and S. Moh, “Survey on computation ofﬂoading in uav-
enabled mobile edge computing,” Journal of Network and Computer
Applications, p. 103341, 2022.

[5] S. Dhelim, T. Kechadi, L. Chen, N. Aung, H. Ning, and L. Atzori,
“Edge-enabled metaverse: The convergence of metaverse and mobile
edge computing,” arXiv preprint arXiv:2205.02764, 2022.

[6] M. Xu, D. Niyato, J. Kang, Z. Xiong, C. Miao, and D. I. Kim, “Wireless
edge-empowered metaverse: A learning-based incentive mechanism for
virtual reality,” arXiv preprint arXiv:2111.03776, 2021.

[7] H. Sheng, S. Wang, Y. Zhang, D. Yu, X. Cheng, W. Lyu, and Z. Xiong,
“Near-online tracking with co-occurrence constraints in blockchain-
based edge computing,” IEEE Internet of Things Journal, vol. 8, no. 4,
pp. 2193–2207, 2020.

[8] M. Zhaofeng, W. Xiaochang, D. K. Jain, H. Khan, G. Hongmin, and
W. Zhen, “A blockchain-based trusted data management scheme in edge
computing,” IEEE Transactions on Industrial Informatics, vol. 16, no. 3,
pp. 2013–2021, 2019.

[9] M. Castro, B. Liskov et al., “Practical byzantine fault tolerance,” in

OsDI, vol. 99, no. 1999, 1999, pp. 173–186.

[10] X. Xu, D. Zhu, X. Yang, S. Wang, L. Qi, and W. Dou, “Concurrent prac-
tical byzantine fault tolerance for integration of blockchain and supply
chain,” ACM Transactions on Internet Technology (TOIT), vol. 21, no. 1,
pp. 1–17, 2021.

[11] F. ´Izdemir, Z. ´Idemis¸ ´Izger et al., “Rivest-shamir-adleman algorithm,”
in Partially Homomorphic Encryption. Springer, 2021, pp. 37–41.
[12] Z. Wang, Q. Hu, R. Li, M. Xu, and Z. Xiong, “Incentive mechanism
design for joint resource allocation in blockchain-based federated learn-
ing,” arXiv preprint arXiv:2202.10938, 2022.

[13] X. Li and X. Zhang, “Multi-task allocation under time constraints
in mobile crowdsensing,” IEEE Transactions on Mobile Computing,
vol. 20, no. 4, pp. 1494–1510, 2019.

[14] K. Jiang, H. Zhou, D. Li, X. Liu, and S. Xu, “A q-learning based method
for energy-efﬁcient computation ofﬂoading in mobile edge computing,”
in 2020 29th International Conference on Computer Communications
and Networks (ICCCN).

IEEE, 2020, pp. 1–7.

[15] W.-C. Chien, H.-Y. Weng, and C.-F. Lai, “Q-learning based collaborative
cache allocation in mobile edge computing,” Future generation computer
systems, vol. 102, pp. 603–610, 2020.

102030405060The Number of MEC Servers2000022000240002600028000Total RewardsRandomGreedyLearning5060708090100The Number of Offloding Task24000260002800030000320003400036000Total RewardsRandomGreedyLearning0.00.20.40.60.81.0The Increase Percentage of pj180002000022000240002600028000300003200034000Total Rewards0.00.20.40.60.81.0The Increase Percentage of Dj800090001000011000120001300014000Total Rewards0100200300400500  Episode9000100001100012000130001400015000Total Utility=0.1=0.5=0.90100200300400500Episode800010000120001400016000Total Utility=0.1=0.5=0.9