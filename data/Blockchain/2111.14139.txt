1
2
0
2

v
o
N
8
2

]
E
S
.
s
c
[

1
v
9
3
1
4
1
.
1
1
1
2
:
v
i
X
r
a

Semantic Code Search for Smart Contracts

Chaochen Shi
Deakin University
Australia
shicha@deakin.edu.au

Jiangshan Yu
Monash University
Australia
jiangshan.yu@monash.edu

Yong Xiang
Deakin University
Australia
yong.xiang@deakin.edu.au

Longxiang Gao
Deakin University
Australia
longxiang.gao@deakin.edu.au

ABSTRACT
Semantic code search technology allows searching for existing
code snippets through natural language, which can greatly improve
programming efficiency. Smart contracts, programs that run on the
blockchain, have a code reuse rate of more than 90%, which means
developers have a great demand for semantic code search tools.
However, the existing code search models still have a semantic gap
between code and query, and perform poorly on specialized queries
of smart contracts. In this paper, we propose a Multi-Modal Smart
contract Code Search (MM-SCS) model. Specifically, we construct
a Contract Elements Dependency Graph (CEDG) for MM-SCS as
an additional modality to capture the data-flow and control-flow
information of the code. To make the model more focused on the
key contextual information, we use a multi-head attention network
to generate embeddings for code features. In addition, we use a fine-
tuned pretrained model to ensure the model’s effectiveness when
the training data is small. We compared MM-SCS with four state-
of-the-art models on a dataset with 470K (code, docstring) pairs
collected from Github and Etherscan. Experimental results show
that MM-SCS achieves an MRR (Mean Reciprocal Rank) of 0.572,
outperforming four state of-the-art models UNIF, DeepCS, CARLCS-
CNN, and TAB-CS by 34.2%, 59.3%, 36.8%, and 14.1%, respectively.
Additionally, the search speed of MM-SCS is second only to UNIF,
reaching 0.34s/query.

CCS CONCEPTS
• Software and its engineering → Search-based software engi-
neering.

KEYWORDS
code search, smart contract, attention mechanism, graph represen-
tation

ACM Reference Format:
Chaochen Shi, Yong Xiang, Jiangshan Yu, and Longxiang Gao. 2021. Se-
mantic Code Search for Smart Contracts. In Proceedings of ACM Conference

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
Conference’17, July 2017, Washington, DC, USA
© 2021 Association for Computing Machinery.
ACM ISBN 978-x-xxxx-xxxx-x/YY/MM. . . $15.00
https://doi.org/10.1145/nnnnnnn.nnnnnnn

(Conference’17). ACM, New York, NY, USA, 9 pages. https://doi.org/10.1145/
nnnnnnn.nnnnnnn

1 INTRODUCTION
The smart contract on blockchain is an auto-running digital proto-
col. By coding smart contracts, developers can construct personal-
ized decentralized applications (DAPPs) with a high-level program-
ming language Solidity. Over the recent years, DAPPs on blockchain
platforms represented by Ethereum have proliferated. As of 28th
August 2021, the number of smart contracts deployed on Ethereum
had reached 1.51 million, so that a double growth rate had been real-
ized as compared to the beginning of 2021. The communal ecology
where Ethereum is active has attracted numerous developers to put
their ideas into practice by developing smart contracts. The code
search engine is a vital tool to boost the programming efficiency of
smart contract developers. It allows developers to search existing
code snippets from a large-sized code repository for further refer-
ence or reuse. The code search methods provided by the current
mainstream code repositories (e.g. Github, Etherscan) are based on
tag or keyword search. However, quite a lot of work is underway to
explore semantic code search, a more natural code search method.
Semantic code search is a technique that operates by searching
in natural language from the code repository and returning the
code snippets that consist with query semantics. Early code search
techniques typically deemed the code as a text and compared the
text similarity between code and query using the information re-
trieval model [8]. This method is lacking in the ability of capturing
the semantics of code and query, hence inferior in performance.
The latest research has started to adopt neural networks – a method
referred to as neural code search – to build code search engines.
To bridge the gap between programming languages and natural
languages, neural code search typically maps code and query into
a shared vector space and measures their semantic similarity via
vector distance. Therefore, the key challenges to neural code search
are how to capture the semantics of code and query and generate
the accurate embedding.

There have been a few representative cutting-edge technologies.
NCS [23] is an unsupervised learning technology. It generates word
embeddings for code and query tokens through a shared fastText [6]
encoder and generates sentence- or document-level embeddings
according to TF-IDF weights. However, this method depends on
the overlapped words in the code snippet and query. If the query
contains a word that does not exist in the code corpus, the accuracy
of NCS model will be reduced significantly. The supervised learning

 
 
 
 
 
 
Conference’17, July 2017, Washington, DC, USA

Chaochen Shi, Yong Xiang, Jiangshan Yu, and Longxiang Gao

based model DeepCS [10] has solved this problem. DeepCS operates
by learning the embeddings of code and query via two Long Short-
Term Memory (LSTM) [12] networks, respectively, and comparing
them in the shared vector space. Most of the current state-of-the-
art models have been improved based on DeepCS, introduced in
Section 2.1. However, the performance and applicability of these
DeepCS based models are limited due to two key facts. First, the
models are hard to capture the deep dependency between the key
code elements. The textual features of these models include code
tokens, function names, API sequence, even the traversal of Abstract
Syntax Tree (AST). However, some types of structural information
of the code, such as control- and data-flow, are lacking support to
capture, thereby losing partial semantics of code. This greatly limits
its performance in terms of search accuracy. Second, such models
require large amounts of (code, query) pairs as a corpus for training,
and consume considerable computing resources to achieve an ideal
performance. This limits its applicability to other applications. For
example, there remains no large public corpus of Solidity analogous
to CodeSearchNet [13] for training at present, making it challenging
to adopt such algorithms in blockchain applications.

This paper proposes a model called Multi-Modal Smart contract
Code Search (MM-SCS) to address the limitations mentioned above.
Specifically, MM-SCS improves neural code search in three aspects:
Extra modality. There have been works [27] that use control-
and data-flow to represent code structure in program vulnerability
detection and code comprehension [5] tasks. Based on the hypothe-
sis that control- and data-flow can also help leverage the code struc-
tural information in semantic search, we propose the concept of
Contract Elements Dependency Graph (CEDG) as an extra modality
explained in Section 3. CEDG integrates both control- and data-flow
information in a single graph. Compared with AST and code prop-
erty graph [30], CEDG further highlights the dependency between
elements of code while becoming simpler-structured, which is ben-
eficial to the learning of key semantic features. Moreover, CEDG
considers the differences between Solidity and other programming
languages. For example, Solidity have a unique programmable fall-
back function with no name and no argument. If the contract is
called without matching any function or passing any data, the
fallback function would be triggered. Besides, Solidity has multi-
ple unique keywords and statements. For example, the keyword
modifier acts on a function, and the logic in the modifier can be
pre-executed either before or after the function; the exception han-
dling statement require() defines the conditions that the function
needs to meet to continue execution, etc. There are specific nodes
and edges designed for such characteristics of Solidity in CEDG.
Although CEDG is specificlly designed for Solidity, the general idea
of considering code elements dependency can also be applied to
other programming languages.

Code embedding mechanisms. MM-SCS adopts multi-head
self-attention networks [25] to embed three textual modalities (code
tokens, function name, and API sequence), and a modified graph
attention network [20] to embed CEDG, respectively, introduced in
Section 4.2. Compared with existing models with no attention mech-
anism [10] or with only single headed attention networks [7, 24, 29],
multi-head attention mechanism can effectively learn different fea-
tures from different heads and emphasize important features.

Pretained model. Husain [14] highlighted an open challenge
about measuring whether pretrained models are useful in code
search tasks. In MM-SCS, we adopt the fine-tuned pretrained model
ALBERT as the encoder of queries as in Section 4.3 to address this
challenge. Our target is to achieve the optimal performance under
the limited training data with the help of ALBERT’s large corpora.
To evaluate MM-SCS, we collected approximately 470K anno-
tated smart contract snippets written in Solidity from Github and
Etherscan.io and sorted them into (code, docstring) pairs. After
deduplication, there are 48K records remaining. The experimental
results in Section 5 demonstrate MM-SCS outperforms four state
of-the-art models UNIF [7], DeepCS [10], CARLCS-CNN [24], and
TAB-CS [29] by 34.2%, 59.3%, 36.8%, and 14.1% in terms of MRR, re-
spectively. In addition, MM-SCS achieves 0.34s/query in the testing
process, which is only second to UNIF.

The main contributions of this paper are listed as follows:

• We propose the model MM-SCS dedicated to smart contract
semantic code search, which improves neural code search
with extra modality, code embedding mechanisms and pre-
trained model. Experimental results in Section 5 show MM-
SCS outperforms four state-of-the-art models as baselines
by at least 14.1% in terms of MRR.

• We put forward CEDG, a novel graph representation of code
which integrates dependency between code elements includ-
ing control- and data-flow, as an extra modality used in
MM-SCS. Althrough CEDG is designed for Solidity in this
paper, the general idea is independent and can be applied to
other programming languages.

• We build a corpus with originally 470K (code, docstring)
pairs for experiments. The corpus will be made public to
promote research in the field of smart contract code search.

2 BACKGROUND AND RELATED WORK
2.1 Neural Code Search
As mentioned in Section 1, neural code search falls mainly into
unsupervised learning and supervised learning methods. A rep-
resentative unsupervised learning method is NCS proposed by
Facebook team [23]. NCS merely uses the tokens from the code
corpus for word embedding and highly depends on the overlapped
words between code and query. UNIF [7] is a supervised exten-
sion to NCS. It uses a bag-of-words-based network to learn the
mapping from code tokens into query tokens and introduces the
TF-IDF weight based attention mechanism. DeepCS [10] gener-
ates joint embedding from code tokens, function names and API
sequence, and learns the code-query relation via LSTM. Recent
works concentrated on the improvement of DeepCS. For example,
the method CARLCS-CNN [24] introduces a co-attentive mecha-
nism on the base of DeepCS architecture; Yan. et al. [29] designed
a two-stage-attention-based model for their TAB-CS model. Such
attention-based models have improved the primordial DeepCS to
better capture the long-range relationship between tokens, thereby
achieving a performance superior to DeepCS.

All the above-mentioned neural code search methods comply
with the architecture of pseudo-siamese network [7]. As shown in
Fig. 1, two subnetworks of pseudo-siamese network each receive a
tokenized sequence 𝑐1, . . . , 𝑐𝑛 and 𝑞1, . . . , 𝑞𝑛 from code and query

Semantic Code Search for Smart Contracts

Conference’17, July 2017, Washington, DC, USA

2.3 Graph Embedding
Graph embedding is a method to map the features of nodes and
edges of a graph into a low-dimensional vector space. Due to the
complexity of graph structure, a major challenge of graph embed-
ding is how to save as integral as possible the network topology
and node information, so as to achieve a better performance in
downstream tasks like deep learning. DeepWalk [22] is a typical
graph embedding method which learns the partial representation
of nodes in a graph through truncated random walk. Node2vec [9]
is an extension to DeepWalk, which uses skip-gram to adjust the
weight of random walk. Graph convolutional network (GCN) [15]
expands convolution operation from traditional data (e.g. image) to
graph data. Its core idea is to perform first-order approximation on
the convolution kernel around each node. Graph attention network
(GAT) [26] introduces the self-attention mechanism on the base
of GCN. The advantage is that any size of input can be processed,
with attention to the most relevant portions in topology.

In training, the above-mentioned models lay an emphasis on em-
bedding node features while overlooking edge features. In a knowl-
edge graph, edges typically contain rich features, with significant
contributions to the semantics of the entire graph. Nathani [20]
proposed a GAT based method for knowledge graph. This method
captures both node features and edge(relation) features in a knowl-
edge graph and exhibits a superior performance to the predecessors
in relation prediction tasks. Similar to knowledge graph, edges
play an important role of expressing program semantics in the
CEDG proposed in this paper. Therefore, we adopt the method pro-
posed by Nathani to generate embedding of CEDG. The experiment
evinces this method has achieved a satisfactory performance in
smart contract code search tasks.

3 CONTRACT ELEMENTS DEPENDENCY

GRAPH

3.1 Node Representation
Contract elements are classified into three categories of nodes: In-
vocation, Variable, and Fallback nodes. Invocation nodes represent
the function invocations in the contract, including self-defined
functions and the system functions of Solidity such as transfer(),
send(), call(), etc. Specifically, the function definition is also counted
as a special invocation, so as to refer to itself when the function
name appears for the first time. Variable nodes represent the vari-
ables in the contract, including self-defined variables and system
variables, such as msg.sender which refers to the address of the
external caller. Fallback nodes refer to the fallback function, which
is a special function of smart contracts [3]. It is executed when a
function not existing within the contract is being invoked or when
a function is being invoked without providing data. The fallback
function is devoid of name or parameter(s), and only one such can
be defined within a contract.

The attributes of nodes are [Category, Type, Name], which are
obtained by traversing through the AST. Among the attributes,
Category is one of the three categories of nodes, and Type is the
type of a function, modifier or variable. Depending on the definition
of Solidity documents, the type of function is classified into internal,
external, public or private, and the type of modifier is consistent

Figure 1: General architecture of neural code search

and output the corresponding embedding 𝑣𝑐 ∈ R𝑑 and 𝑣𝑞 ∈ R𝑑 ,
respectively. The similarity between the two inputs can be derived
from the distance between 𝑣𝑐 and 𝑣𝑞 calculated by Eq. (1). The
greater the value of the cosine distance between them, the closer
the semantics represented by them. Therefore the search result can
be returned by sorting the values of cosine distance.

𝑐𝑜𝑠 (𝑣𝑐, 𝑣𝑞) =

𝑣𝑐 · 𝑣𝑞
∥𝑣𝑐 ∥ · ∥𝑣𝑞 ∥

(1)

2.2 Sequence Embedding
In the context of neural network, embedding is a mapping process
from discrete variables into continuous vectors. One typical word
embedding technique is word2vec [19], which utilizes Continuous
Bag of Words Model (CBOW) or Skip-gram model to map words
into vectors and measures their semantic similarity using the vector
distance. By concatenating the vectors of single words in a sentence,
the sequence of words can be mapped into a vector that represents
semantics of the entire sentence [21].

In neural code search, we need to embed the input sequences
from code and query sentences, respectively. For code information,
UNIF only receives the code token sequences as the input. DeepCS
receives three inputs: code token, function name, and API sequence.
Among them, code tokens are embedded as 𝑣𝑡𝑜𝑘𝑒𝑛 via a Multi Layer
Perceptron (MLP), whereas function names and API sequences are
embedded as 𝑣𝑛𝑎𝑚𝑒 and 𝑣𝐴𝑃𝐼 , respectively, via LSTM. The final
output code embedding 𝑣𝑐 is as shown in Eq. (2). CARLCS-CNN has
replaced MLP and 𝐿𝑆𝑇 𝑀1 with CNN, as in Eq. (3), while TAB-CS
has replaced all code encoders with attention networks, as in Eq. (4).
𝑣𝑐 = 𝑀𝐿𝑃 (𝑣𝑡𝑜𝑘𝑒𝑛) + 𝐿𝑆𝑇 𝑀1 (𝑣𝑛𝑎𝑚𝑒 ) + 𝐿𝑆𝑇 𝑀2 (𝑣𝐴𝑃𝐼 )

(2)

𝑣𝑐 = 𝐶𝑁 𝑁1 (𝑣𝑡𝑜𝑘𝑒𝑛) + 𝐶𝑁 𝑁2 (𝑣𝑛𝑎𝑚𝑒 ) + 𝐿𝑆𝑇 𝑀 (𝑣𝐴𝑃𝐼 )

(3)

𝑣𝑐 = 𝐴𝑡𝑡1 (𝑣𝑡𝑜𝑘𝑒𝑛) + 𝐴𝑡𝑡2 (𝑣𝑛𝑎𝑚𝑒 ) + 𝐴𝑡𝑡3 (𝑣𝐴𝑃𝐼 )
Query is a natural language sentence made up of word sequences,
hence it can also be mapped into the vector space with the exact di-
mensions as code embedding. DeepCS and CARLCS-CNN generate
embedding 𝑣𝑞 of query sentence using LSTM and CNN, respectively,
while UNIF and TAB-CS use attention network as the encoder.

(4)

Source Code:Code Tokens  Query Tokens  Network ANetwork BCosine DistanceOther code embeddingsCode Embedding Query Embeddingd-dimentional vector space?Withdraw all the deposits of the caller?function withdraw() public      { ... }Query:Conference’17, July 2017, Washington, DC, USA

Chaochen Shi, Yong Xiang, Jiangshan Yu, and Longxiang Gao

Table 1: Edges Defined in CEDG

Category

Control-flow

Data-flow

Fallback

Type(Abbr.)
IF
IE
WH
FR
TC
AT
RT
RQ
BS
BE
NS
AS
AC
FB

Semantic Fact
if (...) then {...}
if (...) else {...}
while (...) do{...}
for (...) do{...}
try {...} catch {...}
assert(...)
revert(...)
require(...)
block start
block end
natural sequential relationship
assgin operation
access operation
fallback relationship

Table 2: Edges Shown in Fig. 2

𝒆2
𝒆1
𝑉1
𝐼1
𝑉1
𝑉2
BS AS
2
1

𝑽𝒔
𝑽𝒆
Type
Order

𝒆4
𝑉1
𝑉1

𝒆5
𝑉1
𝑉2

𝒆9
𝒆6
𝒆3
𝐼2
𝑉2
𝑉2
𝑉1
𝐼2
𝑉2
IF AC BS AS NS AC BE
9
6
3

𝒆8
𝐼2
𝑉1

𝒆7
𝑉2
𝐼2

4

8

5

7

𝒆10
𝐼2
𝐼2
BE
10

𝒆11
𝐹
𝐼1
FB
11

𝒆12
𝐹
𝐼2
FB
12

Figure 2: An example of Contract Elements Dependency
Graph.

with that of function; the type of variable may be uint, address, and
so forth. Name among the attributes may be either function name
or variable name. In Fig. 2, for example, 𝐼1 refers to the function
withdraw(), whose attributes are [Invocation, internal, withdraw];
𝐼2 refers to the variable amount, whose attributes are [Variable,
uint, amount]. Fallback node is devoid of name, and so we set its
type as fallback and name as 0.

3.2 Edge Representation
Enlightened by the contract graph for vulnerability detection in [32],
we have defined different types of edges to represent the control-
flow, data-flow and fallback relationship between the elements of
contract. See Table 1 for the specific types of edges, where the
control-flow edge corresponds to the conditional, loop and excep-
tion handling statements in code. Specifically, we add the edges
corresponding to the start and end of blocks in order to represent
the code blocks in the source code. The data-flow edge corresponds
to the usage of variables. The fallback edge represents the relation
between fallback nodes and the nodes that may trigger the fallback
function. The attributes of edges are [𝑉𝑠 ,𝑉𝑒 ,Type,Order], which are
obtained through pattern matching among code. Where 𝑉𝑠 and 𝑉𝑒
represent the start point and end point of an edge, respectively. For
the relation between neighboring statements, 𝑉𝑠 refers to the last
element in the anterior statement, whereas 𝑉𝑒 refers to the first
element in the posterior statement. For example, 𝑉𝑠 and 𝑉𝑒 of 𝑒3
in Fig. 2 correspond respectively to deposits[msg.sender] in code
line 8 and amount in line 9. Attribute Type is the type of the edges

shown in Table 1. Attribute Order represents the order in which
the edges appear, namely the time sequence information during
execution of the code. See Fig. 2 and Table 2 for examples of edges.

4 PROPOSED SEMANTIC CODE SEARCH

APPROACH

4.1 Overview
With the general architecture of neural code search in Fig. 1 as the
backbone of MM-SCS, the overall architecture is shown in Fig. 3.
The embedding 𝑣𝑐 of the code snippet (at function level) is the
concatenation of the embeddings of code tokens, function names,
API sequence, and CEDG. Query embedding 𝑣𝑞 is substituted by
the embedding 𝑣𝑑 of docstring (i.e. code comment) at the training
stage. The main modules in MM-SCS are outlined as below:

• Three multi-head self-attention modules used to embed the
textual information of code, including code tokens, function
name, and API sequence.

• One modified GAT module used to embed the CEDG gener-

ated from the code.

• One pretrained module used to embed query information.
• One neural network including LSTM and Dense layers used
to output 𝑣𝑐 to the vector space of the same dimension as 𝑣𝑑 .

These modules are applied respectively in the code embedding
and query embedding processes. The subsections of this section
will provide a detailed description of these modules as well as the
training process.

TextSmart Contract Source CodeContract Elements Dependency GraphFallback NodeInvocation NodeVariable NodeControl-flow edgeData-flow edgeFallback edgeSemantic Code Search for Smart Contracts

Conference’17, July 2017, Washington, DC, USA

Figure 3: Overall Framework of MM-SCS.

4.2 Code Embedding
The input of code embedding falls into textual information and
graph information. Textual information includes code tokens, func-
tion name, and API sequence; graph information stems from CEDG.
The objective of code embedding is to map these inputs into a vector
𝑣𝑐 that can represent the entire code snippet.

4.2.1 Embedding for Code Textual Information. Emulating DeepCS,
we extracted the textual information of code as 𝐶𝑡𝑒𝑥𝑡 = [𝑇 , 𝐹, 𝐴]
via regular expressions. Where, 𝑇 = 𝑡1, . . . , 𝑡𝑙𝑇 of length 𝑙𝑇 is the
code token sequence without special symbols. Each token written
in camel-case or snake-case shall be segmented into the original
word. For example, the token itemCount or item_count shall be
segmented into two separate words: item and count. 𝐹 = 𝑛1, . . . , 𝑛𝑙𝐹
of length 𝑙𝐹 and 𝐴 = 𝑎1, . . . , 𝑎𝑙𝐴 of length 𝑙𝐴 are the function name
sequence and the API invocation sequence, respectively, which
both are extracted from 𝑇 .

To make MM-SCS better capture key contextual information,
we use three multi-head self-attention modules to embed 𝑇 , 𝐹 and
𝐴, respectively. The structure of multi-head self-attention module
is shown in Fig 4. It consists of a two-layer network made up of
a multi-head self-attention and a position-wise feed-forward net-
work. Each layer has an Add & Norm sublayer, while Add denotes
Residual Connection [11] which is used to prevent network degen-
eration, Norm denotes Layer Normalization [4] which is used to
normalize the activation value of each layer. Given that the embed-
ding modules for 𝑇 , 𝐹 and 𝐴 differ only in parameters, we here take
the embedding process for 𝑇 as an example.

Similar to Transformer [25], we use positional encoding 𝑃𝐸 to
encode the relative positional information of words, as in Eq. (5).
Where 𝑝𝑜𝑠 denotes the position of a word in a sequence, 𝑑 denotes
the dimension of 𝑃𝐸 (the same as word embedding), and 𝑖 is the
dimension; thus 2𝑖 denotes an even dimension and 2𝑖 + 1 denotes
an odd dimension. In this way, the module can utilize the order
information of sequences without recurrent or convolutional net-
works. By adding embedding and 𝑃𝐸 of each word in 𝑇 , we can get

Figure 4: Multi-head Self-attention module.

an input matrix 𝑀𝑇 ∈ 𝑅𝑙𝑇 ×𝑑 for self-attention.

𝑃𝐸 (𝑝𝑜𝑠,2𝑖) = 𝑠𝑖𝑛(𝑝𝑜𝑠/100002𝑖/𝑑 ),
𝑃𝐸 (𝑝𝑜𝑠,2𝑖+1) = 𝑐𝑜𝑠 (𝑝𝑜𝑠/100002𝑖/𝑑 )
Then, we derive the coefficients of attention 𝑄, 𝐾 and 𝑉 by right-
multiplying 𝑀𝑇 by three linear transformation matrices 𝑊𝑄 ∈
𝑅𝑑×𝑑𝑘 , 𝑊𝐾 ∈ 𝑅𝑑×𝑑𝑘 and 𝑊𝑉 ∈ 𝑅𝑑×𝑑𝑘 , respectively, as in Eq. (6).

(5)

𝑄 = 𝑀𝑇𝑊𝑄 ; 𝐾 = 𝑀𝑇𝑊𝐾 ; 𝑉 = 𝑀𝑇𝑊𝑉

(6)

After that, we can calculate the output of self-attention as in
Eq. (7), where √︁𝑑𝑘 is the scaling factor to avoid gradient disap-
pearing. Specifically, we multiply the input by the padding mask
matrix prior to 𝑠𝑜 𝑓 𝑡𝑚𝑎𝑥 to shield the redundant information at the
padding position. The multi-head self-attention network consists
of multiple randomly initialized self-attention networks. Assume
the number of heads is 𝐽 , then the output 𝑀𝑢𝑙𝐴𝑡𝑡 of multi-head is
as in Eq. (8).

𝐴𝑡𝑡 (𝑄, 𝐾, 𝑉 ) = 𝑠𝑜 𝑓 𝑡𝑚𝑎𝑥 (

𝑄𝐾 T
√︁𝑑𝑘

)𝑉

(7)

𝑀𝑢𝑙𝐴𝑡𝑡 = 𝐶𝑜𝑛𝑐𝑎𝑡 (𝐴𝑡𝑡1, 𝐴𝑡𝑡2, . . . , 𝐴𝑡𝑡𝐽 )
(8)
The value of 𝑀𝑢𝑙𝐴𝑡𝑡 of 𝑇 is input into the feed forward network
after undergoing the Add & Norm layer to get the final output 𝑣𝑇

Smart ContractContract CodeCode TokensMethod NameAPI SequenceAttentionAttentionAttentionAST TraversalCEDG NodesCEDG EdgesCEDG G(V,E)AttentionMulti-head Self-attention ModuleModified GAT ModuleMerged Code Feature MatrixConcatenationDocstringLSTM LayersDense LayersPretrained ModelCosine SimlarityPattern MacthingPositional EncodingCode TokensSequence EmbeddingPadding MasksQMulti-head attentionAdd&NormalizeVKFeed ForwardAdd&NormalizeConference’17, July 2017, Washington, DC, USA

Chaochen Shi, Yong Xiang, Jiangshan Yu, and Longxiang Gao

As the lite version of BERT, ALBERT possesses a less parameters and
faster training speed at no sacrifice of effectiveness. Since docstring
is also a natural language text in English, using the pretrained
model can save training time and hardware resources and derive a
more exact vector representation based on contexts with billions
of sentences.

Although the considerably huge training data sets of ALBERT,
its corpus stems mainly from books and English Wikipedia. The
docstrings used to describe code snippets are slightly different from
the sentences in these corpuses, as the former contains the terms
that are related to blockchain and programming contexts. For in-
stance, “fallback” refers specifically to a function type in docstring,
carrying a completely different connotation from the “fallback”
in general texts. To help ALBERT better understand blockchain
contexts, we fine-tuned ALBERT using incremental training. Specif-
ically, we performed n-gram masking for the docstrings in the
training data sets and conducted training through sentence order
prediction. ALBERT team provides customized scripts to help in
this fine-tuning process [1]. Finally, we feed the docstrings into the
fine-tuned ALBERT model to get a corresponding 768-dimensional
vector representation 𝑣𝑑 .

4.4 Model Training
As shown in Fig. 3, we concatenate the code’s multi-modal vector
representations 𝑣𝑇 ,𝑣𝐹 ,𝑣𝐴,𝑣𝐺 into a merged code feature matrix 𝑀
as in Eq. (14).

𝑀 = 𝑣𝑇 ⊕ 𝑣𝐹 ⊕ 𝑣𝐴 ⊕ 𝑣𝐺

(14)

The task of model optimization is to make code embedding 𝑣𝑐 as
close as possible to the corresponding docstring embedding 𝑣𝑑 in
a shared vector space. Therefore, we send 𝑀 to the LSTM layer,
where it is further output and delivered to the dense layer, and
finally the 768-dimensional code embedding 𝑣𝑐 is output. At the
training stage, we provide a corresponding docstring embedding 𝑣 +
𝑑
and a randomly chosen negative docstring embedding 𝑣 −
𝑑 for each
𝑣𝑐 . Our goal is to maximize the similarity between 𝑣𝑐 and 𝑣 +
𝑑 and
minimize the similarity between 𝑣𝑐 and 𝑣 −
𝑑 . Accordingly, the goal
of training is to minimize the loss value 𝐿(𝜃 ) in Eq. (15). Where 𝜃
denotes the learnable parameters of the model, and 𝛽 is a constant
margin, typically set as 0.05.

𝐿(𝜃 ) = 𝑚𝑎𝑥 (0, 𝛽 + 𝑐𝑜𝑠 (𝑣𝑐, 𝑣 −

𝑑 ) − 𝑐𝑜𝑠 (𝑣𝑐, 𝑣 +

𝑑 ))

(15)

Figure 5: Modified GAT module.

as in Eq. (9), where Relu is the activation function, and 𝑊1, 𝑊2,
𝑏1, 𝑏2 are learnable parameters of the network. Likewise, we can
derive the outputs, 𝑣𝐹 and 𝑣𝐴, of 𝐹 and 𝐴, respectively.

𝑣𝑇 = Relu(𝑀𝑢𝑙𝐴𝑡𝑡 · 𝑊1 + 𝑏1)𝑊2 + 𝑏2

(9)

4.2.2 Embedding for CEDG. The modified GAT module for CEDG
embedding [20] is shown as Fig. 5. 𝐺 (𝑉 , 𝐸) represents the CEDG
constructed from code information. Split 𝐺 (𝑉 , 𝐸) into multiple
triples ( (cid:174)ℎ𝑖, (cid:174)ℎ 𝑗 , (cid:174)𝑒𝑘 ), where (cid:174)ℎ𝑖, (cid:174)ℎ 𝑗 , (cid:174)𝑒𝑘 are a vector representation for
the head and tail nodes and for the relation (edge) connecting both,
respectively. Multiply each triple by a linear transformation 𝑊1 to
learn the triple’s vector representation (cid:174)𝑐𝑖 𝑗𝑘 as in Eq. (10).

(cid:174)𝑐𝑖 𝑗𝑘 = 𝑊1 [ (cid:174)ℎ𝑖 ∥ (cid:174)ℎ 𝑗 ∥ (cid:174)𝑒𝑘 ]
(10)
Following this, we take (cid:174)𝑐𝑖 𝑗𝑘 through a nonlinear layer 𝑊2 ac-
tivated by LeakyRelu, to get the parameter of attention weight,
𝑏𝑖 𝑗𝑘 , as in Eq. (11). Afterwards the attention weight 𝛼𝑖 𝑗𝑘 for each
triple can be calculated as in Eq. (12).

𝑏𝑖 𝑗𝑘 = LeakyRelu(𝑊2𝑐𝑖 𝑗𝑘 )

(11)

𝑒𝑥𝑝 (𝑏𝑖 𝑗𝑘 )
(cid:205)𝑟 ∈𝑅𝑖𝑛

𝛼𝑖 𝑗𝑘 = 𝑠𝑜 𝑓 𝑡𝑚𝑎𝑥 𝑗𝑘 (𝑏𝑖 𝑗𝑘 ) =

𝑒𝑥𝑝 (𝑏𝑖 𝑗𝑘 )

(cid:205)𝑛 ∈𝑁𝑖
Where 𝑁𝑖 is the neighbor node to entity 𝑖, 𝑅𝑖𝑛 is the relation
linking entity 𝑖 and 𝑛. The multi-head attention mechanism has
also been included in this model to stabilize the learning process,
so that more neighbor information can be learned. Finally, the new
vector representation (cid:174)ℎ′

𝑖 for each node is calculated by Eq. (13).

(12)

(cid:174)ℎ′
𝑖 =

𝜎
𝐷

(

𝐷
∑︁

∑︁

∑︁

𝑑=1

𝑗 ∈𝑁𝑖

𝑘 ∈𝑅𝑖 𝑗

𝛼𝑚
𝑖 𝑗𝑘 (cid:174)𝑐𝑖 𝑗𝑘

𝑚)

(13)

Where 𝐷 is the number of attention heads, and 𝜎 is a nonlinearity.
So far, the nodes of CEDG have learned the n-hop neighbor rela-
tionship between each other. By concatenating the vectors (cid:174)ℎ′
𝑖 for
all the nodes of CEDG obtained after learning, we derive the vector
representation 𝑣𝐺 for the whole CEDG.

4.3 Embedding for Docstring(query)
In this paper we use ALBERT [16] model to embed docstring and
query. ALBERT is a pretrained model on large-sized corpus in
English, which is used to handle various downstream tasks of NLP.

4.5 Search Process
After obtaining the well-trained MM-SCS model, we build a code
repository for upcoming search tasks. We store the original code
snippets and corresponding 𝑣𝑐 and set up indexes for them. When
searching, MM-SCS embeds the query statement into vector 𝑣𝑞
using the fine-tuned ALBERT model. Since semantic similarity
was measured in terms of the cosine distance between vectors as
Eq. (1), MM-SCS searchs the most neighboring 𝑣𝑐 to 𝑣𝑞 in the vector
space using Nmslib [2] tool and returns the corresponding top-k
rank. Finally, we can extract the detailed information about the
corresponding code snippet as the search result via the index value
of returned 𝑣𝑐 .

G(V,E)Graph Attention layersRelatice attention valueSemantic Code Search for Smart Contracts

Conference’17, July 2017, Washington, DC, USA

Table 3: Input Modalities of Baselines. 𝑇 is Code Token Se-
quence, 𝐹 is Function Name, 𝐴 is API Sequence

Table 4: Effectiveness Comparison Between MM-SCS and
Baselines in terms of SR@1/5/10 and MRR

𝐴 AST CEDG

𝐹

𝑇
Model
✓
UNIF
✓ ✓ ✓
DeepCS
CARLCS-CNN ✓ ✓ ✓
✓ ✓ ✓
TAB-CS
✓ ✓ ✓
MM-SCS

✓

✓

5 EVALUATION
All the experiments in this paper are implemented with Python
3.7, and run on Google Colab with a NVIDIA Tesla P100 GPU. The
number of attention heads of both multi-head attention module
and modified GAT module is set to 8, which is the widely used
default setting in many tasks. Experiments investigate the following
research questions (RQs):

RQ1: How is the effectiveness of MM-SCS compared with the

state-of-the-art models?

RQ2: How are the training and operating efficiency of MM-SCS?
RQ3: How is the advantage of using CEDG as an extra modality?
RQ4: How much does the structural design affect the effective-

ness of MM-SCS?

5.1 Datasets and Baselines
To build training and testing datasets, we collected 400k code snip-
pets from Github and Etherscan.io using web clawers. After re-
moving duplicate entries, there are 48,578 snippets remained in
the dataset, including normal functions, modifiers and fallback
functions. We segmented the code snippets into (code, docstring)
pairs using regular expressions and conducted experiments using
10-fold cross validation method. Emulating baseline models, we
used docstrings to simulate real queries, since docstrings are easily
accessible and quite similar to queries in expression.

We take four state-of-the-art models mentioned in Section 2.1:
UNIF, DeepCS, CARLCS-CNN, and TAB-CS, as baselines for a com-
parison with the MM-SCS proposed herein. Besides the structural
difference, the input modalities of models are also different, as
shown in Table 3.

5.2 Evaluation Metrics
We used two metrics widely applied in code search tasks, Success-
Rate@k [17] (also referred to as Recall@k) and MRR [18, 31], to
evaluate the effectiveness of MM-SCS.

SuccessRate@k (SR@k): The percentage of queries for which
can hit the standard answer among the top-k results of a single
search. It is obtained by calculation from Eq. (16).

𝑆𝑅@𝑘 =

1
|𝑄 |

|𝑄 |
∑︁

𝛿 (𝑄𝑖 <= 𝑘)

(16)

𝑖=1
Where 𝑄 is a set of queries; 𝛿 () is an indicator function which
returns 1 if the 𝑖–th query 𝑄𝑖 has been hit successfully among
the top-k results, else 0. The higher the SR@k value of the model,
the better the effectiveness. Following the general settings in the
experiment of baselines, we calculate SR values at 𝑘=1, 5, and 10.

SR@1
Model
0.417
UNIF
DeepCS
0.347
CARLCS-CNN 0.434
0.489
TAB-CS
0.568
MM-SCS

SR@5
0.514
0.462
0.545
0.623
0.746

SR@10 MRR
0.426
0.567
0.359
0.501
0.418
0.573
0.501
0.664
0.572
0.798

MRR: The mean of reciprocal ranks of all queries, obtained by

calculating from Eq. (17).

𝑀𝑅𝑅 =

|𝑄 |
∑︁

1
|𝑄 |

1
𝑅𝑎𝑛𝑘𝑄𝑖

(17)

𝑖=1
Where 𝑅𝑎𝑛𝑘𝑄𝑖 refers to the rank of the first hit result of 𝑄𝑖
among all search results. If there is no code snippet corresponding
to 𝑄𝑖 among the top-k results, then let
= 0. The higher the
value of MRR, the more the result of the standard answer returned
by the search engine comes near to the top. Likewise, following the
general settings in baselines, we calculate MRR values at 𝑘=10.

1
𝑅𝑎𝑛𝑘𝑄𝑖

5.3 Experimental Results
This section renders the experimental result for answering the RQs.

5.3.1 RQ1: How is the effectiveness of MM-SCS compared with
the state-of-the-art models? Table 4 compares the effectivenesses
of MM-SCS and baselines on our dataset. MM-SCS achieves an
MRR of 0.572, and SR@1/5/10 with 0.568/0.746/0.798. As compared
to UNIF, DeepCS, CARLCS-CNN, and TAB-CS, MM-SCS has im-
proved by 34.2%, 59.3%, 36.8%, and 14.1% in terms of MRR; by
36.2%/45.1%/40.7%, 63.6%/61.4%/59.2%, 30.8%/36.8%39.2%, 16.1%/19.7
%/20.1% in terms of SR@1%/5%/10%, respectively. Moreover, we
have applied Wilcoxon signed-rank test [28] at a 5% significance
level on all metrics between models, to get the p-values<0.05. That
means the experimental data corresponding to each model comes
from entities with different distributions, proving the effectiveness
advantage of MM-SCS versus baselines is statistically significant.

5.3.2 RQ2: How are the training and operating efficiency of MM-
SCS?. Table 5 compares the training and testing time cost of MM-
SCS and baselines on our dataset. The result indicates that UNIF
has the highest training and query efficiency, mainly because it
only receives unimodal code input, as shown in Table 3. The train-
ing of MM-SCS is most time-consuming (10.2 hours), much slower
than all baselines. According to the statistics, MM-SCS has taken
9.8 hours on average to fine-tune ALBERT and 0.4 hours to train
the code embedding modules. Although the fine-tune process is
time-consuming, ALBERT performs fast in the test. the operating
efficiency of MM-SCS is 0.34s/query, which is only second to UNIF
(0.19s/query). One main reason is that MM-SCS uses the attention
mechanism which allows parallel computation in GPUs, while the
recurrent neural networks used in DeepCS and CARLCS-CNN can
only perform serial computation. Another reason is that compared
with AST used in TAB-CS, CEDG has a simpler structure, which
makes graph embedding much easier. Besides, the pretrained model

Conference’17, July 2017, Washington, DC, USA

Chaochen Shi, Yong Xiang, Jiangshan Yu, and Longxiang Gao

Table 5: Time Cost for Training and Testing of models

Table 7: Effectiveness Comparison between Baselines and
MM-SCS Under the Same Input Modalities

Model
Training
0.3 hours
UNIF
3.8 hours
DeepCS
CARLCS-CNN 1.4 hours
0.8 hours
TAB-CS
10.2 hours
MM-SCS

Testing
0.19s/q
1.01s/q
0.56s/q
0.45s/q
0.34s/q

Table 6: Performance of MM-SCS with Different Modalities

Modalities

SR@1
0.449
T
0.463
T+A+F
0.522
T+A+F+AST
T+A+F+CEDG
0.568
T+A+F+AST+CEDG 0.573

SR@5
0.548
0.579
0.664
0.746
0.737

SR@10 MRR
0.453
0.595
0.462
0.612
0.530
0.717
0.798
0.572
0.581
0.791

Testing
0.24s/q
0.29s/q
0.42s/q
0.34s/q
0.56s/q

ALBERT used in MM-SCS is lite. The operating time in our experi-
ments considers both embedding time and retrieving time. In real
repositories with indexes, embeddings would be stored in advance,
and the retrieving speed is usually less than a few milliseconds.

5.3.3 RQ3: How is the advantage of using CEDG as an extra modal-
ity? Table 6 compares the effect of different input modalities on
MM-SCS performance. According to the experimental result, when
the textual modalities of code are fixed as (𝑇 +𝐴+𝐹 ), MM-SCS can
improve by 22.6%, 28.8%, 30.3%, and 23.8% in terms of SR@1, SR@5,
SR@10 and MRR, respectively, with CEDG as the extra input modal-
ity. With AST as the extra input modality, MM-SCS can only im-
prove by 12.7%, 14.6%, 17.1%, and 14.7% in terms of SR@1, SR@5,
SR@10 and MRR, respectively. Besides, the promotion brought
about with AST+CEDG as the extra input modality is roughly the
same as when CEDG is used alone. One possible reason is that
CEDG has included the useful features for semantic search in AST,
and that targeted design has been made for smart contracts. More-
over, the operating efficiency of using (𝑇 +𝐴+𝐹 +CEDG) is 0.34/query,
which is 0.8s faster than using (𝑇 +𝐴+𝐹 +AST) in testing. Therefore,
we tend to use (𝑇 +𝐴+𝐹 +CEDG) as the input modalities of code in
the real application scenarios of MM-SCS.

5.3.4 RQ4: How much does the structural design affect the effective-
ness of MM-SCS?. Table 7 integrates Tables 3, 4 and 6 to compare
the performances of MM-SCS and baselines in SR@1, SR@5, SR@10
and MRR under the same input modalities. The target is to infer
the effects of different models’ structure on effectiveness. The ex-
perimental result indicates MM-SCS has improved by 7.6%, 6.6%,
4.9%, and 6.3%, respectively, versus UNIF, when the input modality
is 𝑇 . When the input modalities are (𝑇 +𝐴+𝐹 ) for all, MM-SCS has
improved by 33.4%, 25.3%, 22.1%, 28.6% and 6.6%, 6.2%, 6.8%, 10.5%
versus DeepCS and CARLCS-CNN, respectively. When the input
modalities are (𝑇 +𝐴+𝐹 +AST) for all, MM-SCS has improved by
6.7%, 6.5%, 7.9%, and 5.7%, respectively, versus TAB-CST. Except for
UNIF, MM-SCS also operates 0.72s/0.27s/0.03s faster than DeepCS,
CARLCS-CNN, and TAB-CS per query under same input modalities,
respectively. We can infer that given the same input modalities, the
structure of MM-SCS outperforms the structure of baselines.

Model

UNIF
MM-SCS(𝑇 )
DeepCS
CARLCS-CNN
MM-SCS(𝑇 +𝐴+𝐹 )
TAB-CS
MM-SCS(𝑇 +𝐴+𝐹 +AST)

SR@1
0.417
0.449
0.347
0.434
0.463
0.489
0.522

SR@5
0.514
0.548
0.462
0.545
0.579
0.623
0.664

SR@10 MRR
0.426
0.567
0.453
0.595
0.359
0.501
0.418
0.573
0.462
0.612
0.501
0.664
0.530
0.717

5.4 Case Study
Listing 1 shows the fist retrieved results of MM-SCS and baselines
for query “destroy tokens of a certain address”. MM-SCS returned
the most relevant result even there are no common words between
code and query, while baselines returned irrelevant results. Actu-
ally, the term “destroy” barely appears in our corpus, which means
supervised learning-based query embedding modules can hardly
learn how to embed “destroy”. Thus baselines can only understand
the remaining sequence “tokens of a certain address”, which leads
the irrelevant results. MM-SCS takes a fine-tuned pretrained model
to embed queries. During the fine-tune process, MM-SCS under-
stood the the synonymous relationship between “burn tokens” and
“destroy tokens” in the context of blockchain. It indicates MM-SCS
has better understanding of blockchain terms and contexts than
baselines.
// Query : " destroy tokens of a certain address "

// MM - SCS 's first retrieved result
function burn ( uint256 _value ) onlyOwner public returns (

bool success ) {

require ( _balanceOf [ _owner ] >= _value ) ;
require ( _totalSupply >= _value );
_balanceOf [ _owner ] -= _value ;
_totalSupply -= _value ;
Burn ( _owner , _value );
return true ;

}

// UNIF 's and DeepCS 's first retrieved result
function sendTokens ( address _to , uint _value ) public

onlyMinter validAddress ( _to ) {
// logic of sending tokens to a certain address
}

// CARLCS - CNN 's and TAB - CS 's first retrieved result
function createTokens ( uint256 _value ) {
// logic of creating new tokens
}

Listing 1: First retrieved results of MM-SCS and baselines for
query “destroy tokens of a certain address”.

6 CONCLUSION AND FUTURE WORK
In this paper, we propose an MM-SCS model for semantic search
of smart contract code. In contrast to existing approaches, we im-
plement multi-head self-attention mechanism for code embedding,
make MM-SCS pay more attention to relatively important seman-
tics. We also put forward a novel graph representation of smart

Semantic Code Search for Smart Contracts

Conference’17, July 2017, Washington, DC, USA

[23] Saksham Sachdev, Hongyu Li, Sifei Luan, Seohyun Kim, Koushik Sen, and Satish
Chandra. 2018. Retrieval on source code: a neural code search. In MAPL@PLDI.
ACM, 31–41.

[24] Jianhang Shuai, Ling Xu, Chao Liu, Meng Yan, Xin Xia, and Yan Lei. 2020. Im-
proving Code Search with Co-Attentive Representation Learning. In ICPC. ACM,
196–207.

[25] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is All
you Need. In NIPS. 5998–6008.

[26] Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro
Lio, and Yoshua Bengio. 2017. Graph attention networks. arXiv preprint
arXiv:1710.10903 (2017).

[27] Wenhan Wang, Ge Li, Bo Ma, Xin Xia, and Zhi Jin. 2020. Detecting code clones
with graph neural network and flow-augmented abstract syntax tree. In 2020 IEEE
27th International Conference on Software Analysis, Evolution and Reengineering
(SANER). IEEE, 261–271.

[28] Frank Wilcoxon. 1992. Individual comparisons by ranking methods. In Break-

throughs in statistics. Springer, 196–202.

[29] Ling Xu, Huanhuan Yang, Chao Liu, Jianhang Shuai, Meng Yan, Yan Lei, and
Zhou Xu. 2021. Two-Stage Attention-Based Model for Code Search with Textual
and Structural Features. In SANER. IEEE, 342–353.

[30] Fabian Yamaguchi, Nico Golde, Daniel Arp, and Konrad Rieck. 2014. Modeling
and Discovering Vulnerabilities with Code Property Graphs. In IEEE Symposium
on Security and Privacy. IEEE Computer Society, 590–604.

[31] Xin Ye, Razvan C. Bunescu, and Chang Liu. 2014. Learning to rank relevant files
for bug reports using domain knowledge. In SIGSOFT FSE. ACM, 689–699.
[32] Yuan Zhuang, Zhenguang Liu, Peng Qian, Qi Liu, Xiang Wang, and Qinming He.
2020. Smart Contract Vulnerability Detection using Graph Neural Network.. In
IJCAI. 3283–3290.

contract, CEDG, as an extra modality to explore the hidden seman-
tics between core code elements. Moreover, we adopt the fine-tuned
ALBERT model to generate embeddings for queries when training
data is limited. By comparing MM-SCS and the other four state-
of-the-art baselines on the dataset with 470K entries built by us,
the experimental result indicates MM-SCS is more suitable than
baselines for semantic code search tasks of smart contracts.

Although using docstrings to simulate queries is a widely used
method, authentic query data is conducive to enhancing the gen-
eralization ability of the model. Thus, we will enrich the size of
our data sets and add authentic (code, query) pairs as the ground
truth in the future. Moreover, We will add more types of nodes
and edges for CEDG to represent more informative dependency
between elements, and allow for the variances between different
versions of Solidity.

REFERENCES
[1] [n. d.]. ALBERT. https://github.com/google-research/albert (accessed 10 May

2021).

[2] [n. d.]. Nmslib. https://github.com/nmslib/nmslib (accessed 10 May 2021).
[3] [n. d.]. Solidity 0.8.4 Documentation. https://docs.soliditylang.org/en/v0.8.4/

(accessed 10 May 2021).

[4] Lei Jimmy Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. 2016. Layer Normaliza-

tion. CoRR abs/1607.06450 (2016).

[5] Tal Ben-Nun, Alice Shoshana Jakobovits, and Torsten Hoefler. 2018. Neural Code
Comprehension: A Learnable Representation of Code Semantics. In NeurIPS.
3589–3601.

[6] Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomás Mikolov. 2017.
Enriching Word Vectors with Subword Information. Trans. Assoc. Comput. Lin-
guistics 5 (2017), 135–146.

[7] José Cambronero, Hongyu Li, Seohyun Kim, Koushik Sen, and Satish Chandra.
2019. When deep learning met code search. In ESEC/SIGSOFT FSE. ACM, 964–974.
[8] Fazli Can. 1993. Information Retrieval Data Structures & Algorithms, by William
B. Frakes and Ricardo Baeza-Yates (Book Review). SIGIR Forum 27, 3 (1993),
24–25.

[9] Aditya Grover and Jure Leskovec. 2016. node2vec: Scalable Feature Learning for

Networks. In KDD. ACM, 855–864.

[10] Xiaodong Gu, Hongyu Zhang, and Sunghun Kim. 2018. Deep code search. In

ICSE. ACM, 933–944.

[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep Residual
Learning for Image Recognition. In CVPR. IEEE Computer Society, 770–778.
[12] Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long short-term memory. Neural

computation 9, 8 (1997), 1735–1780.

[13] Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc
Brockschmidt. 2019. CodeSearchNet Challenge: Evaluating the State of Semantic
Code Search. CoRR abs/1909.09436 (2019).

[14] Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc
Brockschmidt. 2019. Codesearchnet challenge: Evaluating the state of semantic
code search. arXiv preprint arXiv:1909.09436 (2019).

[15] Thomas N. Kipf and Max Welling. 2017. Semi-Supervised Classification with

Graph Convolutional Networks. In ICLR (Poster). OpenReview.net.

[16] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush
Sharma, and Radu Soricut. 2020. ALBERT: A Lite BERT for Self-supervised
Learning of Language Representations. In ICLR. OpenReview.net.

[17] Xuan Li, Zerui Wang, Qianxiang Wang, Shoumeng Yan, Tao Xie, and Hong Mei.
2016. Relationship-aware code search for JavaScript frameworks. In SIGSOFT
FSE. ACM, 690–701.

[18] Fei Lv, Hongyu Zhang, Jian-Guang Lou, Shaowei Wang, Dongmei Zhang, and
Jianjun Zhao. 2015. CodeHow: Effective Code Search Based on API Understanding
and Extended Boolean Model (E). In ASE. IEEE Computer Society, 260–270.
[19] Tomás Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient
Estimation of Word Representations in Vector Space. In ICLR (Workshop Poster).
[20] Deepak Nathani, Jatin Chauhan, Charu Sharma, and Manohar Kaul. 2019. Learn-
ing Attention-based Embeddings for Relation Prediction in Knowledge Graphs.
In ACL (1). Association for Computational Linguistics, 4710–4723.

[21] Hamid Palangi, Li Deng, Yelong Shen, Jianfeng Gao, Xiaodong He, Jianshu Chen,
Xinying Song, and Rabab K. Ward. 2015. Deep Sentence Embedding Using the
Long Short Term Memory Network: Analysis and Application to Information
Retrieval. CoRR abs/1502.06922 (2015).

[22] Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. 2014. DeepWalk: online learning

of social representations. In KDD. ACM, 701–710.

