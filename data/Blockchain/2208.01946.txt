2
2
0
2

g
u
A
3

]

C
D
.
s
c
[

1
v
6
4
9
1
0
.
8
0
2
2
:
v
i
X
r
a

Mixed Fault Tolerance Protocols with
Trusted Execution Environment

Mingyuan Gao
mingyuan.gao@u.nus.edu
National University of Singapore
Singapore

Ee-Chien Chang
changec@comp.nus.edu.sg
National University of Singapore
Singapore

Hung Dang
hungdk4@fpt.com.vn
FPT Blockchain Lab
Vietnam

Jialin Li
lijl@comp.nus.edu.sg
National University of Singapore
Singapore

ABSTRACT
Blockchain systems, or distributed ledgers, are designed, built and
operated in the presence of failures. There are two dominant failure
models, namely crash fault and Byzantine fault. Byzantine fault
tolerance (BFT) protocols offer stronger security guarantees, and
thus are widely used in blockchain systems. However, their security
guarantees come at a dear cost to their performance and scalability.
Several works have improved BFT protocols, and Trusted Execu-
tion Environment (TEE) has been shown to be an effective solution.
However, existing such works typically assume that each partici-
pating node is equipped with TEE. For blockchain systems wherein
participants typically have different hardware configurations, i.e.,
some nodes feature TEE while others do not, existing TEE-based BFT
protocols are not applicable.

This work studies the setting wherein not all participating nodes
feature TEE, under which we propose a new fault model called
mixed fault, which is a combination of crash and Byzantine faults.
We explore a new approach to designing efficient distributed fault-
tolerant protocols under the mixed fault model. In general, mixed
fault tolerance (MFT) protocols assume a network of ğ‘› nodes, among
which up to ğ‘“ = ğ‘›âˆ’2
can be subject to mixed faults. Among these
3
failures, some nodes may exhibit Byzantine behaviours, especially
equivocating, while other nodes fail only by crashing. We identify
two key principles for designing efficient MFT protocols, namely,
(i) prioritizing non-equivocating nodes in leading the protocol, and
(ii) advocating the use of public-key cryptographic primitives that
allow authenticated messages to be aggregated. We showcase these
design principles by prescribing an MFT protocol, namely MRaft,
which is based on the Raft [32] consensus protocol.

We implemented a prototype of MRaft using Intel SGX, inte-
grated it into the CCF [33] blockchain framework, and conducted
experiments on Microsoft Azure using nodes spanning across dif-
ferent geographical regions. Experimental results showed that MFT
protocols can obtain the same security guarantees as their BFT
counterparts while still providing better performance (both trans-
action throughput and latency) and scalability.

CCS CONCEPTS
â€¢ Security and privacy â†’ Distributed systems security;
Hardware-based security protocols; â€¢ Computer systems or-
ganization â†’ Reliability; Availability.

1

KEYWORDS
consensus protocol, trusted execution environment, TEE, Intel SGX,
blockchain, distributed ledger, Byzantine fault tolerance, BFT, crash
fault tolerance, CFT, distributed system

1 INTRODUCTION
Blockchain systems, or distributed ledgers, have received tremen-
dous interest from both academia and industry communities over
the last few years. They offer data integrity and immutability in
the presence of service disruption or adversarial attempts. These
systems achieve such security guarantees by building on distributed
fault-tolerant consensus protocols [16, 19, 32]. These protocols can
achieve both safety and liveness in the presence of failures. Safety
means that honest participants (aka nodes or replicas) agree on the
same value, whereas liveness means that these nodes eventually
agree on a value.

A fault-tolerant distributed system is designed, built and oper-
ated with respect to a particular threat model, which comprises
assumptions made on the nodes involved in upkeeping the system.
Crash fault tolerance (CFT) protocols assume faulty nodes fail only
by crashing, whereas Byzantine fault tolerance (BFT) protocols deal
with faulty nodes that deviate arbitrarily from their protocol de-
scription. Byzantine faults cover not only adversarial behaviour, but
also account for hostile environment wherein errors may arise from
hardware malfunctions, software bugs, or errant system administra-
tors causing data loss or state corruptions. There are clear trade-offs
among choices of the threat model. BFT protocols offer stronger
security guarantees in comparison to their CFT counterparts, for
they are designed to tolerate a more powerful adversary who is
able to equivocate at will. It has been shown that equivocation - the
act of a faulty node sending conflicting messages to other nodes
without being detected - is the chief cause of the complications and
overheads underlying BFT protocols [17]. Ensuring both safety and
liveness despite equivocation comes at a dear cost to the perfor-
mance (i.e., transaction throughput and latency) and scalability of
BFT protocols [20].

Consequently, a number of works have attempted to lessen this
gap via the use of hybrid fault model [11, 17, 19]. In such model, it
is assumed that each node is equipped with a small trusted subsys-
tem that fails only by crashing, i.e., stop processing and respond-
ing to any message from other nodes, whereas other untrusted

 
 
 
 
 
 
components in a node may fail or misbehave arbitrarily. Proto-
cols following this hybrid fault model have been shown to require
only ğ‘› = 2ğ‘“ + 1 nodes to tolerate ğ‘“ Byzantine faults, as opposed to
ğ‘› = 3ğ‘“ +1 in conventional BFT protocols (e.g., PBFT [16]). This leads
to lower computational and communication costs. Alternatively,
there exist attempts that deploy CFT protocols such as Raft [32]
in a Byzantine setting [33] via the use of Trusted Execution En-
vironment (TEE) [37]. In particular, one may run the entire CFT
consensus protocol inside a TEE with attested execution [31] so as
to curb adversarial capability of the faulty nodes. It is worth noting
that this line of protocols assume that each node in the system
is equipped with TEE, which may not be applicable to systems
wherein participants have different hardware configurations, i.e.,
some nodes feature TEE while others do not.

The motivations behind our work are twofold. On one hand, par-
ticipants of a fault-tolerant distributed system are likely to feature
heterogenous machines, which are also subject to different main-
tainance or administration. Consequently, it is rarely the case that
an adversary can coordinate a large number of Byzantine faults si-
multaneously as assumed in BFT threat models. On the other hand,
it is more likely the case that among the faulty nodes, only a few
of them feature Byzantine behaviours, while others are simply crash
faults. The latter claim can be justified in practice via the use of
TEE [33], which is widely available nowadays in commodity proces-
sors. This is so because running code inside TEE can significantly
restrict the malicious capability of a compromised node. That is,
when deploying procotol code inside TEE, the TEE-powered node
only fails by crashing.

Given that BFT protocols are not efficient for such replicated
systems, these two observations motivate us to design a new fault
model, namely mixed fault, which is a combination of crash and
Byzantine faults. In this fault model, a portion of these nodes may
exhibit Byzantine behaviours, deviating arbitrarily from the proto-
col description in an attempt to break safety and liveness, whereas
the remaining nodes fail only by crashing.

In this work, we explore a new approach to designing efficient
distributed fault-tolerant protocols under the mixed fault model,
and we refer to such protocols as Mixed Fault Tolerance (MFT)
protocols. In general, MFT protocols assume a network of ğ‘› nodes,
among which up to ğ‘“ = ğ‘›âˆ’2
can be subject to mixed faults. Among
3
the ğ‘› nodes, some are equipped with TEE, while others do not. We
identify two key principles for designing efficient MFT protocols.
Both principles are drawn from an insight that communication
overhead is a major hurdle that exacerbates protocolsâ€™ scalability.
Various works have demonstrated that communication overhead is
the main performance bottleneck of these protocols [20].

The first principle advocates prioritizing non-equivocating nodes
in leading the protocol. This is built upon an observation that the
complications underlying BFT protocols primarily arise from equiv-
ocation. In MFT protocols, only a few faulty nodes exhibit equivoca-
tion, as opposed to each faulty node in BFT protocols. Consequently,
as long as the protocol is led by a non-equivocating node, it may
be designed in such a way that a majority of messages are routed
through and verified by the leader, thereby reducing the overall
communication and computation overheads in the system.

The second principle advocates incorporating public-key crypto-
graphic primitives that allow authenticated messages to be aggregated.
Early proposals for consensus protocols favour symmetric-key Mes-
sage Authentication Code (MAC) to authenticate all-to-all commu-
nication in hostile environments, e.g., PBFT [16]. This choice was
made so as to avoid public-key operations, whose implementa-
tions and executions used to be prohibitive. Fortunately, there exist
various well-optimized asymmetric or hardware-based cryptosys-
tems [4, 12, 39] that render these costs far more affordable. Further-
more, by aggregating authenticated messages such that receiving
and verifying an aggregated message is equivalent to receiving
and verifying a set of individual messages, one can improve the
communication complexity of the system [19, 24].

To showcase these two principles, we prescribe an MFT proto-
col, namely, MRaft, which is based on a CFT consensus protocol
called Raft [32]. It is worthy to mention that MRaft features com-
munication complexity that is linear to the network size ğ‘›, i.e.,
ğ‘‚ (ğ‘›). Similar to Raft, MRaft is driven by a leader. However, the
leader election in MRaft favours nodes that are equipped with TEE.
The TEE-powered leader leverages TEE to verify and aggregate
messages from other nodes, generating a certificate attesting the
fact that a statement has been agreed by a quorum of the nodes.
Alternatively, in the very rare case that the leader is not equipped
with TEE, the protocol leverages Collective Signing (CoSi) [38] to
generate such certificates.

We implemented a prototype of MRaft using Intel SGX, inte-
grated it into the CCF [33] blockchain framework, and conducted
experiments on Microsoft Azure using nodes spanning across dif-
ferent geographical regions. Experimental results showed that MFT
protocols can obtain the same security gurantees as their BFT coun-
terparts while still providing better performance (both transcation
throughput and latency) and scalability.

In summary, we make the following contributions in this work.

(1) Leveraging TEE, we propose a new approach to designing
efficient distributed fault-tolerant protocols that tolerate a
combination of crash and Byzantine faults. That is, a new
fault model named mixed fault is proposed.

(2) We identify two key principles for designing efficient MFT
protocols. That is, (i) prioritizing non-equivocating nodes in
leading the protocol, and (ii) advocating the use of public-key
cryptographic primitives that allow authenticated messages
to be aggregated.

(3) We showcase the above two design principles by prescribing

an MFT protocol, namely, MRaft.

(4) We implemented a prototype of MRaft uisng Intel SGX, in-
tegrited it into the CCF blockchain framework [33], con-
ducted experiments in realistic deployment settings, and
demontrated the efficiency of MFT protocols.

2 PRELIMINARIES
This section provides prerequisite knowledge that is relevant for
this work. We first discuss key features of distributed consensus
protocols, focusing on Raft [32] and PBFT [16]. Subsequently, we
give a brief overview of TEE, in particular, Intel SGX. Finally, we
review the collective signing technique [38].

2

2.1 Consensus Protocols
Consensus protocols aim to achieve both safety and liveness in a
distributed environment, which is potentially hostile. Safety neces-
sitates non-faulty nodes to reach an agreement and never return
conflicting results for the same query, whereas liveness requires
that these nodes eventually agree on a value. There are two types of
node failures, namely crash fault and Byzantine fault. Crash fault-
tolerant (CFT) protocols assume faulty nodes fail only by crashing,
whereas Byzantine fault-tolerant (BFT) protocols deal with faulty
nodes that deviate arbitrarily from their expected behaviours. For
instance, a Byzantine node can equivocate, or delay its activity for
arbitrary duration [16].

Raft Consensus Protocol. Raft is arguably the most notable CFT
consensus protocol. A Raft system comprises ğ‘› deterministic nodes,
and could tolerate up to ğ‘“ = âŒŠ ğ‘›âˆ’1
2 âŒ‹ crash-failures. Each node main-
tains a log that contains a series of commands (or ledger). Raft
ensures that logs of non-faulty nodes converge, achieving safety re-
gardless of synchrony assumption. However, it necessarily relies on
timing to offer liveness [21] (e.g., network is partially synchronous
such that messages are delivered within an unknown but finite
bound).

The protocol is driven by a leader. All remaining nodes are re-
ferred to as followers. Each leader is associated with a unique term.
The leader exchanges heartbeats with the followers in order to
maintain its leadership. If a leader crashes, the protocol goes into
the leader election phase and safely replaces the faulty leader with
a non-faulty one. We refer readers to the Raft paper [32] for details
on the leader election.

The leader collects commands (e.g., requests from the clients),
records them in its log, and replicates them on the followers as
follows. First, it broadcasts the command to all followers. Each
command is identified by the leaderâ€™s term and an index in its log.
When a follower receives a command from the leader, it appends
the command to its own log, and responds to the leader with an
acknowledgement. The leader commits (i.e., execute) the command
once it has received a quorum of ğ‘“ + 1 or more acknowledgements.
The leader announces such commit to the followers, who then also
commit the command in their own local state.

Practical Byzantine Fault Tolerance (PBFT). PBFT is driven by
a leader, whose leadership is associated with a view. The proto-
col comprises three phases, namely Pre-Prepare, Prepare and
Commit. In the first phase, the leader collects requests from clients
and broadcasts them to other nodes in the network as pre-prepare
messages. Upon receiving a pre-prepare message from the leader,
each node verifies the validity of the request, before broadcasting
its responses in prepare messages. These messages constitute the
second phase, which ensures nodes agree on the ordering of the
requests. Upon receiving a quorum of valid and matching prepare
messages, nodes move to the third phase, broadcasting their commit
messages. They execute the requests once they receive a quorum of
commit messages. When the leader fails, the view change protocol
is triggered to replace the leader.

PBFT requires a network of ğ‘› = 3ğ‘“ + 1 nodes and a quorum
size of 2ğ‘“ + 1 to tolerate up to ğ‘“ Byzantine failures. The protocol
observes a communication complexity of ğ‘‚ (ğ‘›2). It attains safety

regardless of timing assumptions, whereas liveness is achieved in
partially synchronous networks.

2.2 Trusted Execution Environment

Enclave Execution. Trusted Execution Environment (TEE) offers
an isolated region that safeguards the integrity of the code run-
ning inside. In other words, an adversary is unable to tamper with
the execution of the protected components, or deviate them from
their expected behaviours. There are various hardware primitives
that provision TEEs, e.g., Intel SGX [31], KeyStone [26] and Sanc-
tum [18]. This work adopts Intel SGX due to its wide availability.
Intel SGX [31] is capable of provisioning hardware-protected
TEE (or enclave) for general computation. Each enclave is associated
with an address space (or enclave memory) which is guarded by
the CPU, and inaccessible by foreign (non-enclave) processes. In
particular, each enclave is segregated from the Operating System
(OS), user processes and other enclaves running on the same physi-
cal host. The enclave code, on the other hand, is able to invoke OS
services such as paging and I/O. It is worth noting that data residing
in the enclave memory are encrypted under the processorâ€™s key
prior to leaving the enclave.

Attestation. Enclaves are instantiated by the OS. A remote user
can verify the correct instantiation of an enclave based on a re-
mote attestation protocol [1]. The CPU produces a measurement
of the enclave right after it is instantiated, and signs the measure-
ment with its private key. Such measurement consists of the hash
of the enclaveâ€™s initial state. The user can validate the signature
using Attestation Services [1], and check the correctness of the
measurement.

Data Sealing. An enclave may persist its private state on non-
volatile storage via data sealing mechanisms. Data sealing begins
with the enclave obtaining a unique key bound to its measurement
from the CPU. The enclave then encrypts its private state under the
enclave-specific key before passing the encrypted data to the non-
volatile storage. It is guaranteed that the sealed data is retrievable
only by its owner (i.e., the enclave that sealed it). Nonetheless,
previous works have shown that data sealing may be susceptible
to rollback attacks in which a malicious OS attempts to provide the
enclave with properly sealed but stale data [13]. Defences against
such attack have been proposed in the literature [29].

2.3 Collective Signing
Collective Signing, or CoSi for short, allows a group of indepen-
dent nodes to validate and co-sign a statement [38]. The protocol
produces a collective signature attesting the fact that all nodes in
the group have endorsed the message. Such collective signature
has size and verification cost equivalent to those of an individual
signature.

CoSi builds upon Schnorr multi-signatures [35]. The protocol
takes advantage of communication trees [15, 42] to optimize its
communication cost, thereby achieving scalability. The protocol
assumes that each node in the group has a unique public key, and
that these keys are combined to generate an aggregate public key.

3

One node in the group is designated as a leader, who drives the pro-
tocol through the following four phases to generate the collective
signature for a message ğ‘€:

â€¢ Announcement: The leader triggers the new round by mul-
ticast an announcement along the communication tree. ğ‘€
may be embedded in the announcement. Alternatively, it can
be sent in the Challenge phase.

â€¢ Commitment: Upon receiving the announcement from the
leader, nodes pick a secret uniformly at random, and com-
pute a Schnorr commitment of their chosen secret. From
the bottom of the communication tree up, each node sends
its aggregated Schnorr commitment to its parent. The node
computes its aggregated commitment by combining its own
Schnorr commitment with those it collects from its children.
â€¢ Challenge: After receiving the aggregated Schnorr commit-
ment, the leader produces a collective Schnorr challenge. It
then sends the challenge along the communication tree. If
ğ‘€ has not been sent in the Announcement phase, the leader
embeds ğ‘€ in the challenge.

â€¢ Response: Given the collective challenge, nodes assemble
the aggregate responses in a manner similar to that of the
Commitment phase.

In case some nodes fail to respond to messages from the leader,
the protocol can still produce the collective signature. However,
this signature will include metadata that indicates which node did
or did not participate in the collective signing. Readers are referred
to the CoSi paper [38] for further details.

3 OVERVIEW OF MFT SYSTEMS
In this section, we give an overview of the MFT systems. First, we
present some example distributed systems that motivate the design
of the MFT model in Subsection 3.1. Then, we describe the MFT
model in detail in Subsection 3.2. Lastly, we elaborate on the threat
model of an MFT system in Subsection 3.3 and its system goals in
Subsection 3.4.

3.1 Motivating Examples
Before presenting the MFT model that this work studies, letâ€™s first
look at some example distributed systems that motiviate the design
of this model.

(E1) Consortium Blockchain: In a deployment of the consortium
blockchain, the distributed ledger is shared and maintained
by a group of independent parties. Typically, these parties
rely on a BFT protocol like PBFT [16] to provide safety and
liveness for the distributed ledger. It is highly likely that
these parties have different hardware configurations, e.g.,
some systems feature TEE while others do not.

(E2) Backward-Compatible Distributed Systems: Consider a large-
sized corporate whose operations span across multiple re-
gions, business activities at each region is administered by a
separate branch. These branches need to stay in synchroniza-
tion. Needless to say, the corporate can use a BFT protocol
to enable such synchronization. However, these branches
are highly likely to have different hardware configurations.

4

Some branches may have already upgraded their systems
which feature TEE while other branches do not.

(E3) Confidentiality-Preserving Replicated Systems: Confidential
Computing (CC) [3] protects data in use by performing com-
putation involving sensitive data in TEE, thus providing
confidentiality protection for the sensitive data. Since CC
can increase the security assurances for sensitive and reg-
ulated data, various initiatives have been actively focusing
on defining and accelerating its adoption. When deployed
in a replicated system, current CC platforms like CCF [33]
typically assume each node in the system is equipped with
TEE. In circumstances where such assumption is too strong,
i.e., not every node in the system feature TEE, one may still
wish to attain the same security gurantee as the system con-
figuration wherein all nodes feature TEE. In such cases, the
goal can be achieved by using the network only for reach-
ing consensus on the order of execution, while the actual
execution of the confidential computation is carried out on
the TEE-powered nodes. The computation results are then
replicated to other nodes.

The above distributed systems all aim to provision a replicated
service using a BFT protocol. One one hand, nodes in these systems
are likely to feature heterogenous machines, which are also subject
to different maintainance or administration. Consequently, it is
rarely the case that an adversary can coordinate a large number
of Byzantine faults simultaneously as assumed in BFT threat mod-
els. On the other hand, it is more likely the case that among the
faulty machines, only a few of them feature Byzantine behaviours,
while others are simply crash-faults. This claim can be justified in
practice via the use of TEE [33], which is widely available in recent
commodity processors. This is so because running code inside a
TEE can significantly restrict the malicious capability of a compro-
mised node. That is, when deploying procotol code inside a TEE, the
TEE-powered node only fails by crashing.

Given that BFT protocols are not efficient for such replicated
systems, the above two observations motivate us to design a new
fault model for such distributed systems.

3.2 System Model
We now present the system model that this work studies, with a
focus on the fault model.

We study a distributed system that comprises of ğ‘› deterministic
and independent nodes. The system provisions a replicated service
that receives requests from individual clients, and executes those
requests in a totally ordered sequence. In other words, the replicated
service appears to the clients as if it runs on a single non-faulty
machine.

Most distributed systems make an assumption that nodes in
the network are homogeneous. That is, the nodes are presumed to
share the same set of capabilities, and admit similar vulnerabilities.
Nonetheless, this is not always necessarily the case. As can be seen
from the above example systems, nodes are likely to have hetero-
geneous machines. In view of this and the wide availability of TEE
in commodity processors, we study a heterogeneous system wherein
some nodes feature hardware-based TEE, while others are powered
by legacy systems which place trust on their OSs or hypervisors.

Each pair of nodes in the network communicate through a reli-
able, authenticated point-to-point communication channel. In order
to sidestep the FLP impossibility [21], we assume that the commu-
nication channels are partially synchronous, i.e., messages that are
sent repeatedly with a finite timeout will be eventually delivered at
its destination. This assumption is commonly observed in existing
distributed and replicated systems [19, 24]. Besides, there is no
global clock. Nodes process messages and execute requests at their
own speed.

Mixed Fault Model: It has been shown that the complexity of BFT
protocols typically arises from the ability of a Byzantine node to
equivocate (i.e., issue conflicting statements to different nodes with-
out being detected) [17, 19]. Given that code executed inside a TEE
is integrity protected, the malicious behaviours of a compromised
node that is powered by TEE can be significantly restricted. That
is, when deploying protocol code inside the TEE, a TEE-powered
node, even compromised, is not able to equivocate. In other words,
a TEE-powered node only exibits crash failures. Thus, the use of
TEE can significantly reduce the communication complexity of BFT
protocols.

In heterogeneous distributed systems wherein some nodes fea-
ture TEE while others do not, different nodes can exibit different
types of failures. In particular, TEE-powered nodes only exhibit
crash failures, while non-TEE nodes feature Byzantine behaviours.
We refer to this fault model as the mixed fault, which is eleborated
in Section 4.

3.3 Threat Model
Our threat model assumes that the Byzantine nodes are under
adversarial control. They may access (i.e., read and write) to other
processesâ€™ memory, including that of the OS. They can also tamper
with data persisted on persistent storage, intercept and alter system
calls.

The adversary is also able to initialize, stop and invoke the TEE
enclaves of the TEE-powered nodes. Nevertheless, its control over
the TEE is limited, for we make an assumption that the TEEâ€™s at-
tested execution mechanism is secure. In contrast to Intel SGXâ€™s
threat model, we make no assumption on the confidentiality pro-
tection of the enclaves, except for a few critical cryptographic
primitives such as key generation, random number generation or
attestation. That is, the TEE-powered nodes run in the seal-glassed
proof model that is able to attest to the correct execution of the
codebase loaded inside, but its execution is transparent [40]. This
threat model is particularly relevant in view of recent side-channel
attacks against Intel SGX (e.g., [14]). While we leave attacks that
compromise confidentiality of attestation and other cryptographic
keys [41] out of scope, we remark that techniques, both software
and hardware-based, hardening critical cryptographic operations
against these attacks are available [23].

Finally, we assume the adversary is computationally bounded. It
is not able to break standard cryptographic assumptions. Besides,
we exclude denial-of-service attacks against the system in this
study.

3.4 System Goals
Our system goal is efficient state machine replication [34] under
the MFT model. It is desired that the system provides both safety
and liveness. That is, any two clients interact with the system re-
ceive consistent responses, and valid requests from the clients are
eventually executed.

In particular, we study a network of ğ‘› nodes, among which up
to ğ‘“ = ğ‘›âˆ’2
can be subject to mixed faults. The argument for this
3
is provided in Section 5.2. Here, ğ‘“ = ğ‘“ğ‘ + ğ‘“ğ‘ , wherein ğ‘“ğ‘ denotes
the number of crash-faulty nodes, and ğ‘“ğ‘ that of Byzantine nodes.
In addition, we require that the number of TEE-powered nodes
ğ‘›ğ‘¡ğ‘’ğ‘’ â‰¥ ğ‘“ + 1. We refer to protocols that enable such replication as
MFT protocols, which are elaborated in Section 4.

4 MIXED FAULT TOLERANCE
In this section, we first elaborate on our MFT model by contrasting
it against related conventional and non-conventional fault tolerance
models, thereby highlighting MFTâ€™s key characteristics. For clarity,
we shall review those models as we visit them. Then, we present
two design principles that allow MFT protocols to scale.

4.1 MFT vs. CFT/BFT
Recall that our system model presented in Section 3.2 does not
assume any global clock or known bounds of network latency. This
assumption is also observed by asynchronous CFT/BFT protocols.
The difference between MFT and asynchronous CFT/BFT arises
from our treatment of node faults.

CFT protocols, such as Raft [32], provide safety regardless of
network condition, and require partial synchrony to ensure liveness.
They tolerate up to ğ‘“ = ğ‘›âˆ’1
crash failures. However, as soon as
2
there exists a single Byzantine node in the network, CFTâ€™s threat
model is violated, and all security guarantees are voided. In contrast
to CFT, our MFT model affords some Byzantine nodes, and retains
safety and liveness as long as the number of faulty nodes does not
exceed a predefined threshold.

BFT protocols assume a powerful adversary who wields absolute
control over all faulty nodes, causing them to deviate arbitrarily
from their expected behaviours. The most prominent BFT protocol
is arguable PBFT [16], which tolerates up to ğ‘“ = ğ‘›âˆ’1
faults but
3
incurs quadratic communication complexity in term of the network
size. Such communication complexity hinders PBFTâ€™s scalability [19,
20]. In opposition to BFT, MFT assumes that only a portion of the
faulty nodes exhibit Byzantine behaviours, whereas other faulty
nodes only crash and do not misbehave. This assumption allows
MFT to trim down the communication overhead, thereby improving
the performance and scalability of the system.

4.2 MFT vs. Hybrid BFT
The complexity of BFT protocols typically arises from the ability of
a Byzantine node to equivocate [17, 19]. It has been shown that
without equivocation, it is possible to tolerate ğ‘“ Byzantine failures
with only ğ‘› = 2ğ‘“ + 1 nodes using the quorum size ğ‘“ + 1. The
smaller network and quorum sizes result in lower computational
and communicational cost incurred in tolerating the same number
of failures.

5

(a) Hybrid Fault Replicas.

(b) TEE-Powered Consensus.

(c) MFT Replicas.

Figure 1: Comparison between MFT and other hybrid fault models.

Building on this observation, a number of approaches have stud-
ied the hybrid fault model. In such models, each node in the network
is assumed to be equipped with a trusted subsystem that only fails
by crashing, whereas its other components are untrusted and may
fail arbitrarily. Figure 1a depicts this threat model. The trusted
subsystem is utilized to combat against undetected equivocation.
A common technique is to bind each message a node broadcasts
with a record in a log (which can be as simple as a monotonic
counter) maintained by the trusted subsystem. Since operations car-
ried out by the trusted subsystem cannot be equivocated, malicious
nodes cannot send conflicting messages without being convicted
by others.

Alternatively, one can also eliminate equivocation by running
the entire codebase of a consensus protocol inside a TEE (Figure 1b).
This approach, adopted by CCF [33], effectively reduces a nodeâ€™s
fault model from BFT to CFT. Consequently, any non-Byzantine
consensus protocols, such as Raft [32] or Paxos [25], can be ap-
plied. CFT consensus protocols could then tolerate ğ‘“ Byzantine
failures with only ğ‘› = 2ğ‘“ + 1 nodes using the quorum size of ğ‘“ + 1,
resulting in lower communication and computational overhead.
While this approach is similar to the hybrid fault model described
earlier in their assumptions on the availability of TEE at each node,
it incurs a large trusted code base (TCB) which is undesirable for
security [30]. A large TCB makes security analysis of the implemen-
tation bewildering, which likely exposes the system to potential
vulnerabilities.

In contrast to the above two hybrid fault models, MFT does not
require each node in the network to be equipped with TEE. Our MFT
model allows for a portion of the nodes to behave arbitrarily (i.e.,
Byzantine nodes), whereas hybrid fault models collapse as soon
as there exists a single Byzantine node in the network. Figure 1c
shows the heterogeneity of nodes in an MFT system.

4.3 MFT vs. Flexible BFT
Malkhi et al. [28] introduced Flexible BFT, which tolerates alive-but-
corrupt faults. Flexible BFT assumes that these alive-but-corrupt
nodes may exhibit Byzantine behaviours. However, they do so
strictly for the purpose of breaking the protocolâ€™s safety. In case they
are unable to compromise safety, they will not hinder the protocolâ€™s
liveness. Furthermore, Flexible BFT allows clients interacting with
the replicated service to hold different assumptions or beliefs about
the system, based on which they interpret the protocol transcript
and make commit decisions. Flexible BFT guarantees both safety
and liveness for all clients with correct beliefs.

6

Flexible BFT justifies the alive-but-corrupt faults based on an
observation that the adversary may benefit if safety is broken, for
instance in double-spending attacks, while it is unlikely to gain
anything from broken liveness. The authors [28] further argue that
alive-but-corrupt nodes are incentivised to keep the liveness as they
could collect service fee. These assumptions are in line with rational
protocol design treatment wherein the adversary is assumed to
misbehave only if such action yields (economic) gain [10].

On the contrary, MFT does not make any assumption on the
rationale of the corrupted nodes. Our threat model pays more atten-
tion to the capability that the adversary wields and the constraints
that it adheres to. For instance, if a TEEâ€™s attested execution protec-
tion is intact, the adversary may attempt to disconnect it from the
network, but it cannot compromise TEEâ€™s execution integrity. In
such a case, the adversary cannot cause the TEE-powered nodes
to violate safety, yet it can tamper with their I/O and network
connections in an attempt to prevent liveness.

4.4 MFT vs. XFT
Cross Fault Tolerance, or XFT for short [27], studies a system
model which admits both crash and Byzantine faults. Beyond crash
and Byzantine faults, XFT explicitly defines network fault as an
event wherein some non-faulty nodes could not communicate syn-
chronously with each other (i.e., a message exchanged between
two nodes is delivered and processed within a known latency Î”).
A node is considered partitioned if it does not belong to a largest
synchronous subset. In a network of ğ‘› nodes, XFT protocols are
able to tolerate up to ğ‘› crash faults without compromising safety,
and tolerate some Byzantine faults together with network faults,
so long as there exist a majority of nodes that are not faulty and
communicate synchronously.

The key difference between XFT and our MFT model is XFTâ€™s
separation of node and network faults. Similar to CFT and BFT,
our MFT model considers only machine faults, and relies on partial
synchrony to sidestep the FLP impossibility [21]. By separating
network from node faults, XFT can guarantees safety in two modes:
(i) there is no Byzantine faults, regardless of the number of crash-
faulty and partitioned replicas; (ii) there exist some Byzantine faults,
but a majority of nodes remain correct and not partitioned, i.e., the
total number of crash, Byzantine and network faults combined
does not exceed ğ‘›âˆ’1
2 . MFT, on the other hand, offers safety when
ğ‘“ â‰¤ ğ‘›âˆ’2
(ğ‘“ is the total number of crash and Byzantine nodes) in
3
partially synchronous network.

4.5 Design Principles for MFT Protocols
So far we have contrasted MFT against CFT, BFT, and other non-
conventional fault tolerance models. We now draw observations
from the discussion and comparison presented above, and codify
them into two design principles.

Our first observation, which is applicable not only to MFT but
also to other distributed systems in general, is that communication
complexity is typically a bottleneck to the performance and effi-
ciency of the system. Various works have shown that this is indeed
the case for many different fault tolerance models and protocol de-
signs [20]. Consequently, reducing the communication complexity
contributes to the improvement of the systemâ€™s performance.

MFT protocols essentially implement a replicated state machine
system that is driven by a designated node in the network, which
is typically called leader. That is, the leader processes requests
sequentially and the remaining nodes in the network merely follow.
When the leader becomes faulty, it is replaced by another node via
a subprotocol, during which the performance of the system suffers
significantly [19]. Our second observation is that in heterogeneous
settings such as those studied under the MFT model, the choice
of the leader plays a crucial role in tuning the performance of the
system.

These two observations motivate the following two design prin-

ciples for MFT protocols:

(P1) Leadership favours non-equivocating nodes. Recall that MFT
assumes both types of failures, namely crash and Byzantine
faults. It is well established that most complications under-
lying BFT protocols are due to equivocation. In MFT, only
Byzantine nodes may equivocate, whereas such behaviour
is never conducted by crash faulty nodes. Consequently, as
long as a protocol is led by a non-equivocating node, a major-
ity of consensus messages can be routed through and verified
by the leader on behalf of other nodes. This communication
pattern poses much less overhead on the network. Further-
more, computation cost incurred in verifying authenticated
messages can also be saved. We remark that this principle
advocates giving higher priority to non-equivocating node
in attaining the leadership, but it does not impose strict re-
striction on potentially Byzantine node never becoming the
leader.

(P2) Aggragation of Consensus Messages. In case the leadership
is taken by a node that may feature equivocation, it is im-
portant to ensure that its misbehaviour, if any, does not
compromise safety. Such node should not be trusted with
collecting and verifying consensus messages on othersâ€™ be-
half. Instead, the protocol should incorporate cryptographic
primitives that allow consensus messages to be efficiently
aggregated in such a way that receiving and verifying an
aggregated message is equivalent to receiving and verifying
a quorum of individual messages (e.g., CoSi [38]), without
relying on any trusted third party. This clearly improves the
communication complexity of the system.

Based on these two principals, we retrofitted the Raft procotol
for the MFT model, and refer to the resulting protocol as MRaft,
which is elaborated in Section 5.

7

5 MRAFT
In this section, we present MRaft, which is a distributed consensus
protocol designed to operate under the MFT model. MRaft is built
upon Raft [32]. However, instead of tolerating up to ğ‘“ crash-faults
using a network of ğ‘› = 2ğ‘“ + 1 nodes as Raft does, MRaft employs a
network of ğ‘› = 3ğ‘“ + 2 nodes to tolerate up to ğ‘“ mixed faults. Some
nodes in the network are assumed to be equipped with TEE with
intact integrity protection, and thus never equivocate or deviate
from the protocol description.

We require that the number of TEE-powered nodes ğ‘›ğ‘¡ğ‘’ğ‘’ â‰¥ ğ‘“ + 1.
This assumption is not strictly required, but it significantly increases
the chance that a TEE-powered node is elelected as the leader. We
further assume that each node should know TEE-capability of other
nodes in the network, i.e., whether the other node is equipped with
and running the consensus codebase inside a TEE, which can be
achieved via TEEâ€™s remote attestation mechanism [8].

Each node in the network implements a state machine, and
maintains a replicated log which records a sequence of commands
or requests the network has served. The goal of MRaft is to ensure
logs of different nodes in the network converge, and each node
commits (or executes) exactly the same sequence of commands.

Similar to Raft, nodes in MRaft can be in one of the three roles,
namely leader, follower and candidate. Our protocol proceeds in
terms. In each term, a node is selected to serve as a leader, while all
other nodes are followers. Leader election in MRaft favours TEE-
powered nodes, which are assumed to be non-equivocating thanks
to TEEâ€™s execution integrity protection. In the common case where
a TEE-powered node attains leadership, the message pattern and
communication complexity of MRaft is similar to those of Raft. The
leader processes most of the messages, while the followers passively
receive and respond to messages from the leader.

In a very rare case where all TEE-powered nodes fail to attain
leadership, the protocol is led by a non-TEE node. In such case, the
leader cannot be trusted to verify followersâ€™ messages on behalf of
the network. The first approach to sidestep this issue is to employ an
all-to-all communication pattern wherein a node broadcasts its mes-
sages to the network (similar to the message pattern of PBFT [16]).
However, this will results in a communication complexity of ğ‘‚ (ğ‘›2),
which is detrimental to the scalability of the protocol [19]. Alter-
natively, one can tasks the leader to collect messages containing
digital signatures from all followers. Once it has collected a quorum
of signatures, it broadcasts such quorum to the network, incurring a
communication complexity of ğ‘‚ (ğ‘›) instead of ğ‘‚ (ğ‘›2). Nonetheless,
the need of each node to independently verify quorum of signatures
may poses a hindrance on the performance of the protocol. In view
of these encumbrances, we follow Byzcoin [24] in using CoSi [38]
to implement collective signing. This implementation enables a
potentially equivocating leader to collect and aggregate messages
from the followers. It suffices for a follower to receive and verify
an aggregated message before proceeding, as opposed to verify a
quorum of messages as in the two naive approaches mentioned
earlier.

We detail below the concrete prescription of MRaft in a common
case wherein the protocol is led by a TEE-powered node (Section 5.1)
and in a very rare case wherein the non-TEE node attains leadership
(Section 5.3). For clarity of exposition, we shall denote the leader

by ğ¿, and a follower ğ‘– by ğ‘ğ‘– . Each leader ğ¿ is associated with a
term ğ‘¡, and indexes a request ğ‘Ÿ it receives from the client in its
log with a counter ğ‘—. Both the term number ğ‘¡ and the index ğ‘—
are increased monotonically. That is, each request ğ‘Ÿ is uniquely
identified by a 3-tuple âŸ¨ğ‘¡, ğ‘—, ğ‘Ÿ âŸ©. In addition to the replicated log,
each node keeps track of a LastCommitIndex which is the index
of the latest entry in its own replicated log that is known to be
committed, which also increases monotonically over time. With
each ğ‘ğ‘– , ğ¿ establishes an authenticated communication channel,
and a timeout ğ‘‡ğ‘– which is a window of time during which ğ‘ğ‘– expects
to receive a message from ğ¿. Should there be no request from the
client during the timeout window ğ‘‡ğ‘– , ğ¿ sends a HeartBeat message
containing ğ¿â€™s term number and LastCommitIndex to ğ‘ğ‘– so as to
maintain its leadership. If ğ‘ğ‘– fails to hear from ğ¿ once its timeout
ğ‘‡ğ‘– has passed, it assumes ğ¿ is faulty and requests for a new leader.
We defer the leader election mechanism to Section 5.2.

5.1 TEE-Powered Leader

Protocol. If the leader ğ¿ is equipped with TEE and running the
protocol inside the TEE, it is assumed that ğ¿ never deviates from its
expected behaviour (i.e., execution integrity is preserved). It may
fail only by crashing. In such case, the protocol proceeds as follows:
(1) Upon receiving a request ğ‘Ÿ from a client, the leader ğ¿ whose
term is ğ‘¡ assigns ğ‘Ÿ an index ğ‘—, and puts âŸ¨ğ‘¡, ğ‘—, ğ‘Ÿ âŸ© onto its log.

(2) ğ¿ broadcasts âŸ¨ğ‘¡, ğ‘—, ğ‘Ÿ âŸ© to the network.
(3) Upon receiving âŸ¨ğ‘¡, ğ‘—, ğ‘Ÿ âŸ©, a follower ğ‘ğ‘– checks if it was from
the node it believes to be the leader, and if ğ‘Ÿ is valid (i.e.,
committing ğ‘Ÿ does not compromise safety). If so, it puts
âŸ¨ğ‘¡, ğ‘—, ğ‘Ÿ âŸ© to its log and responds ğ¿ with an acknowledgement
âŸ¨ackğ‘–, ğ‘¡, ğ‘—, ğ‘Ÿ âŸ©.

(4) Upon receiving a quorum of ğ‘ = 2ğ‘“ + 1 acknowledgements
for ğ‘Ÿ , the leader commits ğ‘Ÿ (i.e., executes ğ‘Ÿ and applies the
result to its state machine) and all uncommitted requests in
its log whose index is smaller than that of ğ‘Ÿ , if any. It pro-
duces a certificate Certğ‘Ÿ , which attests a fact that ğ‘Ÿ has been
replicated on a quorum of nodes. Subsequently, ğ¿ broadcasts
Certğ‘Ÿ to the network.

(5) Upon receiving Certğ‘Ÿ from ğ¿, a follower ğ‘ğ‘– commits ğ‘Ÿ and
all uncommitted requests in its log whose index is smaller
than that of ğ‘Ÿ , if any.

Remarks. Our protocol requires the leader ğ¿ to broadcast Certğ‘Ÿ
in its announcement of request ğ‘Ÿ â€™s commit (Step 4). This enables
every node in the network to independently verify that ğ‘Ÿ has been
replicated on a quorum of nodes, and that the quorum agrees on
the total order of requests. Since quorum size in our protocol is
ğ‘ = 2ğ‘“ + 1, quorums must be intersected at at least one honest node.
Consequently, while a Byzantine node could equivocate, or tamper
with term and index value in its messages, it is unable to cause the
entire network to violate safety.

5.2 Leader Election
As mentioned earlier, during normal operation, ğ¿ periodically
sends HeartBeat message containing its term number and
LastCommitIndex to followers so as to maintain its leadership.
Communication during the consensus round can also be deemed
as HeartBeat messages, for it conveys ğ¿â€™s term number and

8

LastCommitIndex, as well as the fact that ğ¿ is fully functional.
When a follower ğ‘ğ‘– receives such messages, it acknowledges with a
corresponding ack via an authenticated channel. ğ‘ğ‘– may rely on ğ¿â€™s
LastCommitIndex to assure that its replicated log and state are in
sync with ğ¿â€™s. Should ğ‘ğ‘– finds its log and state outdated, it retrieves
the missing requests (i.e., log entries) and commits them, thereby
updating its own state to match that of ğ¿.

For each follower ğ‘ğ‘– , ğ¿ institutes a unique and randomised
timeout ğ‘‡ğ‘– chosen from a fixed interval. Since MRaft favors TEE-
powered node in obtaining leadership, the timeout interval (i.e.,
a fixed interval from which ğ‘‡ğ‘– is drawn) between ğ¿ and a TEE-
powered node, say [ğ‘‡ğ‘,ğ‘‡ğ‘ ] is configured to be smaller than that be-
tween ğ¿ and a non-TEE follower, say [ğ‘‡ğ‘,ğ‘‡ğ‘‘ ]. That is, ğ‘‡ğ‘ < ğ‘‡ğ‘ âˆ§ğ‘‡ğ‘ <
ğ‘‡ğ‘‘ .

Should a follower ğ‘ğ‘– fail to receive any message from ğ¿ after
its ğ‘‡ğ‘– has elapsed, it switches its role to candidate and increases
its term number. Subsequently, ğ‘ğ‘– broadcasts a RequestVote =
âŸ¨ğ‘ğ‘–, ğ‘›ğ‘’ğ‘¤ğ‘‡ ğ‘’ğ‘Ÿğ‘š, ğ‘™ğ‘ğ‘ ğ‘¡ğ¿ğ‘œğ‘”ğ¼ğ‘›ğ‘‘ğ‘’ğ‘¥âŸ© to the network in an attempt to as-
sume leadership, in which ğ‘›ğ‘’ğ‘¤ğ‘‡ ğ‘’ğ‘Ÿğ‘š is its current term number, and
ğ‘™ğ‘ğ‘ ğ‘¡ğ¿ğ‘œğ‘”ğ¼ğ‘›ğ‘‘ğ‘’ğ‘¥ is the index of the latest entry in its replicated log. A
leader election in MRaft is closely related to that of Raft, with some
adjustments in how a recipient handles a RequestVote message.
Upon receiving a RequestVote from ğ‘ğ‘– , a node ğ‘ ğ‘— grants its vote

if all following conditions are met:

â€¢ ğ‘ğ‘– is indeed a node within the network, and RequestVote is

properly signed by ğ‘ğ‘–

â€¢ ğ‘ ğ‘— â€™s own timeout ğ‘‡ğ‘— has elapsed and it has not received any

message from its current leader.

â€¢ ğ‘›ğ‘’ğ‘¤ğ‘‡ ğ‘’ğ‘Ÿğ‘š in the RequestVote is larger than its own current

term.

â€¢ ğ‘ ğ‘— has not granted its vote to any other candidate.
â€¢ ğ‘ğ‘– â€™s replicated log is more up-to-date than that of ğ‘ ğ‘— , as

determined by the indice of the last entries.

In case ğ‘ ğ‘— receives a RequestVote from ğ‘ğ‘– before its timeout ğ‘‡ğ‘—
with its current leader has not elapsed, it queues ğ‘ğ‘– â€™s RequestVote,
if it has not queued any other candidateâ€™s RequestVote, and ğ‘ğ‘– â€™s
log is more up-to-date than its own replicated log. When there is
a competing RequestVote, ğ‘ ğ‘— keeps that of a node which is more
up-to-date, and discards the other. If ğ‘ ğ‘— receives a valid message
from its current leader, it discards any RequestVote that it has
queued. When ğ‘‡ğ‘— has elapsed, ğ‘ ğ‘— grants its vote to ğ‘ğ‘– .

A vote is essentially an authenticated message from ğ‘ ğ‘— that is
publicly verifiable. A node wins an election once it has collected
votes from a quorum of ğ‘ = 2ğ‘“ + 2 nodes. If the leader is equipped
with TEE, it can leverage the TEE to produce a compact proof of
leadership by aggregating the votes, as in step (4) of the protocol
described in Section 5.1. On the other hand, if a non-TEE candidate
wins the election, the proof of its leadership is a collection of the
votes it has thus received. The new leader announces its author-
ity by broadcasting a HeartBeat message containing its proof of
leadership along with the new term number to the network. Upon
receiving such message, a follower verifies if the proof of leadership
is valid before switching to the new leader and updating its term
number accordingly.

Remarks. We remark that the quorum size necessitated for leader
election is ğ‘ = 2ğ‘“ + 2. Hence, the network needs ğ‘› = 3ğ‘“ + 2 nodes to

be correctly operational. This is so because there exists a possibility
that an honest node is disconnected from the network during the
leader election. In such a scenario, it may happen that there are ğ‘“
Byzantine followers, ğ‘“ honest and up-to-date followers, ğ‘“ honest
followers that hold stale view of the replicated log, and a candidate
that may have a stale view. If the quorum size for leader election
is ğ‘ = 2ğ‘“ + 1, it may happen that the log of elected leader miss
some entries committed by the previous leader, and his leadership
in the new term may accidentally undo the requests that had been
committed earlier. To avoid this, the candidate must obtain ğ‘ = 2ğ‘“ +2
votes. Therefore, the network needs ğ‘› = 3ğ‘“ + 2 nodes to tolerate ğ‘“
failures.

5.3 Non-TEE Leader
Note that MRaft employs a network of ğ‘› = 3ğ‘“ + 2 to tolerate up to ğ‘“
mixed faults, among which up to ğ‘“ can be Byzantine faults. Since we
require that the number of TEE-powered nodes ğ‘›ğ‘¡ğ‘’ğ‘’ â‰¥ ğ‘“ + 1, thus,
it is high likely that a TEE node becomes leader during the leader
election process. In a very rare case that the leader election fails to
elect a TEE-powered node, we can repeat the leader election process
until a TEE-powered node is elected as the leader. However, this
approach would sacrifice the systemâ€™s liveness during the leader
election process. Alternatively, we can set a timeout value for the
period that the system is electing a TEE-powered node as leader.
When this timeout is reached, we can temporarily fall back to a safe
BFT protocol until a TEE node is available and elected as leader.
That is, we use the fallback BFT protocol to ensure that the systemâ€™s
liveness is lost only during the timeout period.

Below is the fallback protocol we prescribed for the circumstance
when a TEE-powered node is temporarily not eleteced as the leader.

Protocol. When the leader is not equipped with TEE, it cannot
be trusted to aggregate responses from the followers as in the
previous case. Consequently, MRaft employs CoSi to save on the
communication complexity. The protocol proceeds as follows:

(1) Upon receiving a request ğ‘Ÿ from a client, the leader ğ¿ whose
term is ğ‘¡ assigns ğ‘Ÿ an index ğ‘—, and puts âŸ¨ğ‘¡, ğ‘—, ğ‘Ÿ âŸ© onto its log.
(2) ğ¿ initiates a CoSi round to drive the network to generate the
collective signature for a message âŸ¨ğ‘¡, ğ‘—, ğ‘Ÿ âŸ©. A successful CoSi
round effectively replicates âŸ¨ğ‘¡, ğ‘—, ğ‘Ÿ âŸ© on the followersâ€™ logs.
(3) Upon receiving âŸ¨ğ‘¡, ğ‘—, ğ‘Ÿ âŸ©, during the execution of the CoSi
protocol (described in Section 2.3), a follower ğ‘ğ‘– checks if it
was from the node it believes to be the leader, if ğ‘Ÿ is valid, and
if the term ğ‘¡ and index ğ‘— match its log (ğ‘— should immediately
follow the latest committed entry in its log). If so, it puts
âŸ¨ğ‘¡, ğ‘—, ğ‘Ÿ âŸ© to its log and completes the final phase of the CoSi
protocol.

(4) Once the network has completed the CoSi rounds, ğ¿ should
have obtained the collective signature CoSigğ‘Ÿ for âŸ¨ğ‘¡, ğ‘—, ğ‘Ÿ âŸ©. It
checks if a quorum of ğ‘ = 2ğ‘“ + 1 has partaken in the collec-
tive signing using the metadata contained in the collective
signature. If this is the case, it is assured that the âŸ¨ğ‘¡, ğ‘—, ğ‘Ÿ âŸ©
has been replicated on a quorum of nodes, and it is safe
for ğ¿ to commit ğ‘Ÿ and all uncommitted requests in its log
whose index is smaller than that of ğ‘Ÿ , if any. Subsequently, ğ¿
broadcasts CoSigğ‘Ÿ to the network.

9

(5) Upon receiving CoSigğ‘Ÿ , a follower checks if a quorum of
ğ‘ = 2ğ‘“ + 1 nodes have co-signed CoSigğ‘Ÿ . If so, ğ‘ğ‘– commits
ğ‘Ÿ and all uncommitted requests in its log whose index is
smaller than that of ğ‘Ÿ , if any.

Remarks. In case the leader ğ¿ is equipped with a TEE, MRaft re-
lies on the trusted execution to produce a certificate Certğ‘Ÿ which
attests a fact that a request ğ‘Ÿ has been replicated on a quorum of
nodes. In case ğ¿ does not feature TEE, MRaft resorts to the CoSi pro-
tocol (and relies on its security) to produce CoSigğ‘Ÿ , which conveys
the same significance that Certğ‘Ÿ does.

6 IMPLEMENTATION
In this section, we describe the implementation details of our pro-
totype MRaft. Our prototype is based on the codebase of CCF [33],
which is an open-source framework for building confidential repli-
cated services.

A CCF network [2] consists of several nodes, each running on top
of a TEE, such as Intel SGX. Each node runs the same application,
which can mutate or read the in-enclave-memory key-value store
that is replicated across all nodes in the network. The key-value
store is a collection of maps (associating a key to a value) that are
defined by the application. Changes to the key-value store must be
agreed by a quorum number of nodes before being applied, wherein
the quorum value depends on the consensus algorithm selected.

CCF supports two consensus protocols, i.e., CFT and BFT. CFT is
the default consensus protocol and its implementation is based on
Raft [32]. The BFT implementation is a derivative from PBFT-PK
(PBFT using signatures) [16], with additional features specific to
CCF [36].

Each CCF network has a network identity public-key certificate
(aka, service certificate), used for TLS server authentication, and the
corresponding private key always resides in enclave memory. This
key pair is generated when the first node starts. Each CCF node is
identified by a fresh public-key certificate endorsed by the enclave
quote. This node-identity certificate is used to authenticate the node
when it joins the network, and to sign entries committed by the
node to the ledger during its time as primary.

Modifications to CCFâ€™s Codebase. A CCF network only allows
TEE nodes with a valid enclave quote to join the network, which
is achieved by verifying a joining nodeâ€™s enclave quote through
remote attestation. For non-TEE nodes, we similary identify them
using a public-key certificate issued by the service certificate, i.e.,
the service certificate acts as the root CA for these node-identity
certificates. We retrofitted CCFâ€™s codebase to allow non-TEE nodes
with a valid node-identify certificate to join the network.

In CCF, each node to node pair establish a symmetric traffic key,
using an authenticated Diffie-Hellman key exchange. This key is
used to authenticate messages sent between nodes. For messages
sent from non-TEE nodes, we retrofitted CCFâ€™s codebase to append
a signature to such messages; the signatures are generated using
the private key corresponding to the node-identity public key. Thus,
TEE nodes can verify the authenticity of such messages using the
contained signature.

Changes to CCFâ€™s Raft Implementation. To implement MRaft, we
retrofitted the Raft implementation in ccf-1.0.0, and changed its

Table 1: Network latency (ms) between nodes on Azure

Table 2: Comparison of MRaft with PBFT and Raft-TEE

Datacenter

East US Canada Central UK South West Europe

East US
Canada Central
UK South
West Europe
Southeast Asia

1.71
27.89
75.34
82.82
219.86

27.89
3.50
90.0
93.94
218.11

75.34
90.0
1.27
8.95
156.12

82.82
93.94
8.95
2.35
160.39

Southeast Asia
219.86
218.11
156.12
160.39
2.12

TEE Availability
Fault-Tolerance
Threshold

MRaft
Some nodes
ğ‘“ = ğ‘›âˆ’2
3

PBFT
None
ğ‘“ = ğ‘›âˆ’1
3

Raft-TEE
All nodes
ğ‘“ = ğ‘›âˆ’1
2

quorum size to 2ğ‘“ + 2, wherein ğ‘“ = ğ‘›âˆ’2
3 . Specifically, we modified
the code so that the TEE-powered leader institutes smaller leader-
election timeout values for TEE nodes than for non-TEE nodes,
which makes leader election favors TEE nodes. We also modified
the code so that, during normal operation, whenever a TEE-powered
leader commits entries, it generates a commit certificate for these
entries. Followers will update their commit_idx according to the
correponding comit certificates.

Remarks. We remark that we did not implement the rare case
when a non-TEE node becomes leader. In such a scenario, the CoSi
scheme [38] is used to aggregate authenticated messages, and the
performance is expected to be worse than the case when a TEE
node leads the protocol. In our current implementation, when a
non-TEE node becomes leader, we make the protocol â€œidle-waitingâ€
until a TEE node is available in the network.

In addition, transactions in CCF are committed in batch, rather
than one by one. The maximal batch size for transcations defaults to
20,000 bytes (20 KiB). However, when the request timeout reaches, it
triggers the comitting for transcations since last commit. Therefore,
the batch size for transactions varies, with a maximal size of 20
KiB.

7 EVALUATION
This section presents our experimental study of MRaft, focusing
on its performance (i.e., transaction throughput and latency) and
scalability.

We conducted experiments on Microsoft Azure cloud platform
using SGX-enabled virtual machines (VMs) backed by Intel Xeon
E-2288G processor, i.e., DCsv2-series Confidential Computing VMs
[5]. We chose size â€œStandard_DC4s_v2â€ for all VMs, each configured
with 4 vCPUs, 112 MiB EPC memory, 16 GiB memory, and 30 GiB
SSD.

For all experiments, we run each MRaft node in a separate VM,
running Ubuntu 18.04.5. We deploy these VMs evenly across five
Azure datacenters, i.e., â€œEast USâ€, â€œCanada Centralâ€, â€œUK Southâ€,
â€œWest Europeâ€, and â€œSoutheast Asiaâ€. We issue client requests to the
MRaft backed service using a â€œStandard D8as_v4â€ VM located at
the â€œEast USâ€ datacenter; this VM is configured with 8 vCPUs, 32
GiB RAM and 30 GiB SSD, running Ubuntu 20.04.2. We report the
average communication latency between these nodes in Table 1.

Benchmarks. We use two benchmarks in the experiments. The
first benchmark runs a Logging application, which supports stor-
ing a message with id and retriving the stored message with a
given id. This benchmark invovles 100,000 transcations of storing
messages of the form âŸ¨id, msgâŸ©, wherein id is a unique integer and
msg is the SHA256 checksum of id. Since this benchmark only
involves transactions on a single table, for the second one, we use
the more complicated TPC-C benchmark [7]. The TPC-C database

10

MRaft

PBFT

Raft-TEE

Â·104

2

d
n
o
c
e
s

r
e
p
s
n
o
i
t
c
a
s
n
a
r
T

1.5

1

0.5

0

5

11

20
ğ‘›

32

47

Â·104

2

d
n
o
c
e
s

r
e
p
s
n
o
i
t
c
a
s
n
a
r
T

1.5

1

0.5

0

5

11

20
ğ‘›

32

47

(a) Logging

(b) TPC-C

Figure 2: Throughput of MRaft, PBFT and Raft-TEE with respect to dif-
ferent cluster sizes (ğ‘›) on Azure. Here, Raft-TEE is the case wherein
each node runs Raft within a TEE.

is composed of nine types of tables with a wide range of row sizes
and cardinalities. TPC-C involves a mix of five concurrent transac-
tions of different types and complexity. Therefore, there is greater
diversity in the data manipulated by the five types of transactions
and thus greater database contention. In the second benchmark, we
also issue 100,000 transcations.

In all experiments, transaction throughput is measured at the
leader replica and latency at the clients. Latency is averaged over
all transactions in an experiment and counts the time from sending
a command on the client to receiving a global commit confirmation.
Unless otherwise stated, the results presented in this section are
averaged over 10 independent runs. We focus on normal operation,
and do not report performance of the system in case that the leader
crashes or during the leader election process [32].

Baselines. We compare MRaft against two baselines: (i) PBFT,
wherein each node runs PBFT without any TEE hardening; and
(ii) Raft-TEE, wherein each node runs Raft within a TEE. Note
that MRaft, PBFT and Raft-TEE all aim to achieve the same goal,
i.e., state machine replication in a network wherein all nodes are
running within adversarial environments. A brief comparison of
them is shown in Table 2. For all experiments, we configure the
number of TEE nodes in MRaft to be âŒŠ ğ‘›
2 âŒ‹, wherein ğ‘› is the network
(i.e., cluster) size.

Experimental Results. Now we present our evaluation results. We
remark that in all comparisons, we have normalized Raft-TEEâ€™s
values to settings such that it has the same fault threashold ğ‘“ with
MRaft and PBFT.

Figure 2 presents the throughput of MRaft, PBFT, and Raft-TEE
with respect to different cluster sizes (ğ‘›) on Azure. As can be seen,
MRaftâ€™s throughput outperform both PBFT and Raft-TEE in both
benchmarks, regardless of the cluster size.

MRaft

PBFT

Raft-TEE

)
s

m

(
y
c
n
e
t
a
L

15

10

5

0

5

11

20
ğ‘›

32

47

)
s

m

(
y
c
n
e
t
a
L

15

10

5

0

5

11

20
ğ‘›

32

47

(a) Logging

(b) TPC-C

Figure 3: Latency of MRaft, PBFT and Raft-TEE with respect to dif-
ferent cluster sizes (ğ‘›) on Azure. Here, Raft-TEE is the case wherein
each node runs Raft within a TEE.

Figure 3 depicts the latency of MRaft, PBFT, and Raft-TEE with
respect to different cluster sizes (ğ‘›) on Azure. Interestingly, MRaftâ€™s
latency is smaller than both PBFT and Raft-TEE in both bench-
marks, regardless of the cluster size. That is, in terms of performance
(i.e., transaction throughput and latency), MRaft outperforms both
PBFT and Raft-TEE.

Next, we compare the scalability of MRaft with that of PBFT and
Raft-TEE. As shown in Figure 2, as the cluster size increases, the
throughput of PBFT and Raft-TEE drops much faster than MRaft.
Similarly, as can be seen in Figure 3, the latency of PBFT and
Raft-TEE increases much faster than MRaft when the cluster size
increases. Even when the cluster size increases to ğ‘› = 47, MRaftâ€™s
throughput still does not drop that much, and its latency also does
not increase that much, as compared with PBFT and Raft-TEE.
These results demonstrated MRaftâ€™s excellent scalability.

In summary, MRaftâ€™s performance (i.e., transaction throughput
and latency) outperforms both PBFT and Raft-TEE. At the same
time, MRaft also provides better scalability than PBFT and Raft-TEE.
That is, MFT protocols achieve the same security gurantees as their
BFT counterparts, but also provide better performance and scalability.

8 RELATED WORK
The bottleneck of performance (i.e., transaction throughput and
latency) and scalability in blockchain systems or distributed ledger
systems is typically the underlying consensus protocol. Consensus
protocols are used by replicas to agree on an order for transactions.
A majority of current ledger systems [6, 9] rely on BFT consensus
protocols.

Improving BFT Protocols. Several recent works have improved the
scalability of BFT protocols. Using threshold cryptograph, SBFT [22]
proposes a variant of PBFT that scales to larger consensus groups.
Byzcoin [24] also builds on PBFT and dynamically forms consensus
groups. HotStuff [43] can also scale to hundreds of replicas using
threshold cryptography.

Improving Consensus Protocols using TEE. Several works have
proposed to improve the efficiency of BFT protocols using TEE
[11, 17, 33]. These systems typically assume that each node is
equipped with a small trusted subsystem that fails only by crash-
ing, whereas other untrusted components in a node may fail or

misbehave arbitarily. The use of such trusted subsystems reduces
the number of requried nodes to tolerate ğ‘“ failures. However, this
line of protocols impose a trust assumption on each and every node
participating in the system, which may not be applicable to set-
tings wherein participants have different hardware configurations.
Unlike existing works, we explore a new approach to designing
efficient distributed fault-tolerant systems that tolerate a combina-
tion of crash and Byzantine faults, which we refer to as mixed fault
tolerance (MFT).

9 CONCLUSION
We proposed a new approach, which leverages TEE, to designing
efficient distributed fault-tolerant protocols (i.e., MFT protocols)
that tolerate a combination of crash and Byzantine faults. We iden-
tified two key principles for designing efficient MFT protocols, and
showcased these two principles by prescribing an MFT protocol,
namely, MRaft. We implemented a prototype of MRaft, integrated
it into the CCF [33] blockchain framework, conducted experiments
in realistic deployment settting, and demonstrated the efficiency of
our approach.

REFERENCES
[1] [n. d.].

Attestation Services for Intel SGX.

https://software.intel.com/

content/www/us/en/develop/topics/software-guard-extensions/attestation-
services.html.

[2] [n. d.]. CCF Documentation. https://microsoft.github.io/CCF/main/index.html.
[3] [n. d.]. Confidential Computing Consortium. https://confidentialcomputing.io/.
Intel 64 and IA-32 Architectures Software Developerâ€™s Manual
[4] [n. d.].
. https://software.intel.com/sites/default/files/managed/39/c5/325462-sdm-vol-1-
2abcd-3abcd.pdf/.

[5] [n. d.]. Microsoft Azure Confidential Computing. https://azure.microsoft.com/en-

us/solutions/confidential-compute/.

[6] [n. d.]. Quorum. https://github.com/ConsenSys/quorum.
[7] [n. d.]. TPC-C. http://www.tpc.org/tpcc.
[8] Ittai Anati, Shay Gueron, Simon Johnson, and Vincent Scarlata. 2013. Innovative
technology for CPU based attestation and sealing. In Proceedings of the 2nd
international workshop on hardware and architectural support for security and
privacy, Vol. 13. ACM New York, NY, USA.

[9] Elli Androulaki, Artem Barger, Vita Bortnikov, Christian Cachin, Konstanti-
nos Christidis, Angelo De Caro, David Enyeart, Christopher Ferris, Gennady
Laventman, Yacov Manevich, Srinivasan Muralidharan, Chet Murthy, Binh
Nguyen, Manish Sethi, Gari Singh, Keith Smith, Alessandro Sorniotti, Chrysoula
Stathakopoulou, Marko Vukolic, Sharon Weed Cocco, and Jason Yellick. 2018.
Hyperledger fabric: a distributed operating system for permissioned blockchains.
In EuroSys. ACM, 30:1â€“30:15.

[10] Christian Badertscher, Juan Garay, Ueli Maurer, Daniel Tschudi, and Vassilis Zikas.
2018. But why does it work? A rational protocol design treatment of bitcoin. In
Annual international conference on the theory and applications of cryptographic
techniques. Springer, 34â€“65.

[11] Johannes Behl, Tobias Distler, and RÃ¼diger Kapitza. 2017. Hybrids on steroids:
SGX-based high performance BFT. In Proceedings of the Twelfth European Confer-
ence on Computer Systems. 222â€“237.

[12] Daniel J Bernstein, Niels Duif, Tanja Lange, Peter Schwabe, and Bo-Yin Yang.
2012. High-speed high-security signatures. Journal of cryptographic engineering
2, 2 (2012), 77â€“89.

[13] Marcus Brandenburger, Christian Cachin, Matthias Lorenz, and RÃ¼diger Kapitza.
2017. Rollback and forking detection for trusted execution environments using
lightweight collective memory. In DSN.

[14] Ferdinand Brasser, Urs MÃ¼ller, Alexandra Dmitrienko, Kari Kostiainen, Srdjan
Capkun, and Ahmad-Reza Sadeghi. 2017. Software grand exposure:{SGX} cache
attacks are practical. In 11th {USENIX} Workshop on Offensive Technologies
({WOOT} 17).

[15] Miguel Castro, Peter Druschel, Anne-Marie Kermarrec, Animesh Nandi, Antony
Rowstron, and Atul Singh. 2003. SplitStream: high-bandwidth multicast in co-
operative environments. ACM SIGOPS Operating Systems Review 37, 5 (2003),
298â€“313.

[16] Miguel Castro, Barbara Liskov, et al. 1999. Practical Byzantine fault tolerance. In

OSDI, Vol. 99. 173â€“186.

11

Security 18). 991â€“1008.

[42] Vidhyashankar Venkataraman, Kaouru Yoshida, and Paul Francis. 2006.
Chunkyspread: Heterogeneous unstructured tree-based peer-to-peer multicast.
In Proceedings of the 2006 IEEE International Conference on Network Protocols.
IEEE, 2â€“11.

[43] Maofan Yin, Dahlia Malkhi, Michael K. Reiter, Guy Golan-Gueta, and Ittai Abra-
ham. 2019. HotStuff: BFT Consensus with Linearity and Responsiveness. In
PODC. ACM, 347â€“356.

[17] Byung-Gon Chun, Petros Maniatis, Scott Shenker, and John Kubiatowicz. 2007.
Attested append-only memory: Making adversaries stick to their word. ACM
SIGOPS Operating Systems Review 41, 6 (2007), 189â€“204.

[18] Victor Costan, Ilia Lebedev, and Srinivas Devadas. 2016. Sanctum: Minimal
hardware extensions for strong software isolation. In 25th {USENIX} Security
Symposium ({USENIX} Security 16).

[19] Hung Dang, Tien Tuan Anh Dinh, Dumitrel Loghin, Ee-Chien Chang, Qian Lin,
and Beng Chin Ooi. 2019. Towards scaling blockchain systems via sharding. In
Proceedings of the 2019 International Conference on Management of Data.
[20] Tien Tuan Anh Dinh, Ji Wang, Gang Chen, Rui Liu, Beng Chin Ooi, and Kian-
Lee Tan. 2017. Blockbench: A framework for analyzing private blockchains. In
Proceedings of the 2017 ACM International Conference on Management of Data.
1085â€“1100.

[21] Michael J Fischer, Nancy A Lynch, and Michael S Paterson. 1982. Impossibility of
distributed consensus with one faulty process. Technical Report. Massachusetts
Inst of Tech Cambridge lab for Computer Science.

[22] Guy Golan-Gueta, Ittai Abraham, Shelly Grossman, Dahlia Malkhi, Benny Pinkas,
Michael K. Reiter, Dragos-Adrian Seredinschi, Orr Tamir, and Alin Tomescu. 2019.
SBFT: A Scalable and Decentralized Trust Infrastructure. In DSN. IEEE, 568â€“580.
[23] Vladimir Kiriansky, Ilia Lebedev, Saman Amarasinghe, Srinivas Devadas, and
Joel Emer. 2018. DAWG: A defense against cache timing attacks in speculative
execution processors. In 2018 51st Annual IEEE/ACM International Symposium on
Microarchitecture (MICRO). IEEE, 974â€“987.

[24] Eleftherios Kokoris Kogias, Philipp Jovanovic, Nicolas Gailly, Ismail Khoffi, Linus
Gasser, and Bryan Ford. 2016. Enhancing bitcoin security and performance with
strong consistency via collective signing. In 25th {usenix} security symposium
({usenix} security 16). 279â€“296.

[25] Leslie Lamport et al. 2001. Paxos made simple. ACM Sigact News 32, 4 (2001),

18â€“25.

[26] Dayeol Lee, David Kohlbrenner, Shweta Shinde, Dawn Song, and Krste Asanovic.
2019. Keystone: An Open Framework for Architecting TEEs. arXiv:1907.10119
[27] Shengyun Liu, Paolo Viotti, Christian Cachin, Vivien QuÃ©ma, and Marko Vukolic.

2016. XFT: Practical Fault Tolerance beyond Crashes. In OSDI.

[28] Dahlia Malkhi, Kartik Nayak, and Ling Ren. 2019. Flexible byzantine fault tol-
erance. In Proceedings of the 2019 ACM SIGSAC Conference on Computer and
Communications Security. 1041â€“1053.

[29] Sinisa Matetic, Mansoor Ahmed, Kari Kostiainen, Aritra Dhar, David Sommer,
Arthur Gervais, Ari Juels, and Srdjan Capkun. 2017. ROTE: Rollback Protection
for Trusted Execution. IACR Cryptology ePrint Archive (2017).

[30] Jonathan M McCune, Bryan Parno, Adrian Perrig, Michael K Reiter, and Arvind
Seshadri. 2007. Minimal TCB code execution. In 2007 IEEE Symposium on Security
and Privacy (SPâ€™07). IEEE, 267â€“272.

[31] Frank McKeen, Ilya Alexandrovich, Alex Berenzon, Carlos V Rozas, Hisham Shafi,
Vedvyas Shanbhogue, and Uday R Savagaonkar. 2013. Innovative instructions
and software model for isolated execution. HASP@ ISCA 10 (2013).

[32] Diego Ongaro and John K Ousterhout. 2014. In search of an understandable
consensus algorithm.. In USENIX Annual Technical Conference. 305â€“319.
[33] Mark Russinovich, Edward Ashton, Christine Avanessians, Miguel Castro,
Amaury Chamayou, Sylvan Clebsch, Manuel Costa, CÃ©dric Fournet, Matthew
Kerner, Sid Krishna, et al. 2019. CCF: A framework for building confidential
verifiable replicated services. Technical Report MSR-TR-201916 (2019).

[34] Fred B. Schneider. 1990. Implementing Fault-Tolerant Services using the State
Machine Approach: A Tutorial. ACM Comput. Surv. 22, 4 (Dec. 1990), 299â€“319.
[35] Claus-Peter Schnorr. 1991. Efficient signature generation by smart cards. Journal

of cryptology 4, 3 (1991), 161â€“174.

[36] Alex Shamis, Peter Pietzuch, Miguel Castro, Edward Ashton, Amaury Chamayou,
Sylvan Clebsch, Antoine Delignat-Lavaud, Cedric Fournet, Matthew Kerner,
Julien Maffre, Manuel Costa, and Mark Russinovich. 2021. PAC: Practical Ac-
countability for CCF. arXiv:2105.13116 [cs.DC]

[37] Pramod Subramanyan, Rohit Sinha, Ilia Lebedev, Srinivas Devadas, and Sanjit A
Seshia. 2017. A formal foundation for secure remote execution of enclaves. In
Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications
Security. ACM, 2435â€“2450.

[38] Ewa Syta, Iulia Tamas, Dylan Visher, David Isaac Wolinsky, Philipp Jovanovic,
Linus Gasser, Nicolas Gailly, Ismail Khoffi, and Bryan Ford. 2016. Keeping
authorities" honest or bust" with decentralized witness cosigning. In 2016 IEEE
Symposium on Security and Privacy (SP). Ieee, 526â€“545.

[39] Robert Szerwinski and Tim GÃ¼neysu. 2008. Exploiting the power of GPUs for
asymmetric cryptography. In International Workshop on Cryptographic hardware
and embedded systems. Springer, 79â€“99.

[40] Florian Tramer, Fan Zhang, Huang Lin, Jean-Pierre Hubaux, Ari Juels, and Elaine
Shi. 2017. Sealed-glass proofs: Using transparent enclaves to prove and sell
knowledge. In 2017 IEEE European Symposium on Security and Privacy (EuroS&P).
IEEE, 19â€“34.

[41] Jo Van Bulck, Marina Minkin, Ofir Weisse, Daniel Genkin, Baris Kasikci, Frank
Piessens, Mark Silberstein, Thomas F Wenisch, Yuval Yarom, and Raoul Strackx.
2018. Foreshadow: Extracting the keys to the intel {SGX} kingdom with tran-
sient out-of-order execution. In 27th {USENIX} Security Symposium ({USENIX}

12

