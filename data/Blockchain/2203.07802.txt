2
2
0
2

r
a

M
5
1

]

G
L
.
s
c
[

1
v
2
0
8
7
0
.
3
0
2
2
:
v
i
X
r
a

A Framework for Veriﬁable and Auditable Federated Anomaly
Detection

Gabriele Santin∗1, Inna Skarbovsky†2, Fabiana Fournier‡2, and Bruno Lepri§1

1Digital Society Center, Bruno Kessler Foundation (FBK), Trento, Italy
2IBM Research, Haifa, Israel

March 16, 2022

Abstract

1

Introduction

Federated Leaning is an emerging approach to man-
age cooperation between a group of agents for the
solution of Machine Learning tasks, with the goal of
improving each agent’s performance without disclos-
ing any data.

In this paper we present a novel algorithmic ar-
chitecture that tackle this problem in the particular
case of Anomaly Detection (or classiﬁcation or rare
events), a setting where typical applications often
comprise data with sensible information, but where
the scarcity of anomalous examples encourages col-
laboration.

We show how Random Forests can be used as a
tool for the development of accurate classiﬁers with
an eﬀective insight-sharing mechanism that does not
break the data integrity. Moreover, we explain how
the new architecture can be readily integrated in a
blockchain infrastructure to ensure the veriﬁable and
auditable execution of the algorithm.

Furthermore, we discuss how this work may set
the basis for a more general approach for the design
of federated ensemble-learning methods beyond the
speciﬁc task and architecture discussed in this paper.

∗gsantin@fbk.eu
†inna@il.ibm.com
‡fabiana@il.ibm.com
§lepri@fbk.eu

In a data-driven world, Machine Learning (ML) has
progressively established itself as a fundamental tool
that spreads across multiple ﬁelds and permeates an
increasing variety of applications. After a decade of
fast technological developments mainly driven by the
exceptional new results achieved by Deep Learning
[16, 26], a new wave of reﬂection is emerging about
the scope, applicability, and technical limitations of
these techniques.
In particular, an increasing new
attention is devoted to the issues of data ownership,
data privacy, and data trading.

In this setting, multiple related aspects are be-
ing analyzed and systematized within the framework
of Federated Learning (FL) [22, 32, 50, 52], and sev-
eral real-world problems have been approached with
in the banking [30, 51] and
these techniques, e.g.
health [12, 41] sectors, even beyond classical domains
[29], and considering privacy and fairness constraints
[24,42]. This new ﬁeld deals with the study of various
scenarios where multiple agents own separate batches
of data, and they are willing to cooperate for the
construction of some ML models. This collaboration
leverages diﬀerent communication strategies to over-
come the limitations of the single agents, which can
be due to scarcity of data or scarcity of computa-
tional resources, but with the important constraint
that data should never leave the location where it re-
sides. This approach is in stark contrast with more

1

 
 
 
 
 
 
traditional data-centralized methods, and it paves
the way for a number of new algorithms that focus
on various aspects of data ownership.

to give them away once for all, but would rather like
to develop an on-purpose sharing. This option is in-
herently diﬃcult with easily copyable digital data.

For a comprehensive analysis of the key goals, ap-
plications, and challenges of FL we refer to the re-
cent overviews [23, 49, 50]. To put our approach
into context, we just recall that there is an impor-
tant distinction between centralized and decentral-
In the ﬁrst case, a central or-
ized FL [2, 14, 23].
chestrator coordinates a set of distributed agents (or
nodes) and their computational resources to improve
the ﬁtting of a central model.
In the second case,
instead, the entire process is collectively directed by
the distributed agents. Heterogeneous cases are also
of interest, where the central controller acts, or is
queried, only when needed. In all cases, the focus of
current research are the issues related to communica-
tion eﬃciency, to the inﬂuence of the topology of the
connections in the agents’ network, and the quality of
the learned model. Additionally, in the decentralized
approach the absence of an omniscient orchestrator
opens the way for new possibilities for privacy preser-
vation and ﬂexibility, but it poses new challenges for
the security of the communications, the integrity of
the system, and the accuracy of the algorithmic pro-
cedure.

In this paper we focus on Anomaly Detection (AD)
[3, 9] as a use case for FL. The scenario is motivated
by AD systems that are common in the ﬁnancial
industry, such as fraud detectors or default predic-
tors. The peculiar characteristic of these applica-
tions is that a classiﬁer has to be trained to identify
anomalous cases, i.e., events that are unusual com-
pared to the most frequent patterns observed in the
data. In particular, anomalous examples are scarce
by deﬁnition. As a consequence, diﬀerent agents such
as banks, ﬁnancial institutions, insurance companies
may foresee a beneﬁt in collaborating with their peers
in order to trade knowledge and improve their indi-
vidual models. On the other hand, the data that is
used to train these systems is usually shared with cau-
tion, since it typically comprises sensitive personal
information regarding the ﬁnancial position or the in-
dividual characteristics of the clients. Moreover, the
possession of these data is often an important asset
for the single agents, which are possibly not willing

With these constraints and goals in mind, we
present in this paper a fully decentralized FL system
where multiple agents collaborate for the training of
one model per agent, and which is privacy preserving
by design, robust to changes in the network topology
and to asynchronous communications, and resistant
to malicious intrusions and adversarial attacks.

The system is designed so that each agent trains
an ensemble classiﬁer [38], i.e., a ML model that is
made of multiple simple estimators that are combined
as atomic building blocks. This structure makes it
easy to iteratively improve local models as well as
exchanging knowledge between agents by sharing the
top performing blocks. We use in particular Ran-
dom Forests (RFs) [7] as ensemble models, as they
are well-suited for anomaly detection problems and
robust to missing data, but we comment along the
paper how this is not a restrictive choice and other
ensembles could be adopted. Moreover, the chosen
design of the ML algorithm permits to integrate the
system in a blockchain (BC) infrastructure that guar-
antees trustable and veriﬁable execution of the algo-
rithm, and certiﬁes the communication between the
nodes.

Other works have proposed solutions for the inte-
gration of FL in a BC environment [27, 28, 31, 36, 44,
45, 48]. In this work, we introduce two main novel-
ties over existing approaches: (i) The framework sup-
ports federations where the agents are connected by
means of a time-varying network in a fully decentral-
ized scenario. This includes the case of single agents
joining or leaving the group at diﬀerent times, or ex-
ploiting the collaboration in an on-demand fashion.
This opens the way for treating the participation in
the federation as a tradable utility (see Section 3.2
and Section 4.2), and leverages the BC as a veriﬁca-
tion tool; (ii) The solution is algorithm-agnostic in
its main components, meaning that it can be applied
on top of a large class of machine learning models,
provided that some atomic operations can be deﬁned
(see Section 3.1). In particular, the algorithm is not
bound to speciﬁc architectures or optimization meth-
ods.

2

The paper is organized as follows. We start by
recalling the necessary background on RF and BC
in Section 2, and with these tools we introduce the
novel algorithm in Section 3 and discuss the full BC
solution in Section 4. We validate our new system
through a number of experiments in Section 5, and
conclude by discussing some perspectives and open
problems in Section 6.

2 Background

We start by recalling some background details in
order to facilitate the reading of the paper by re-
searchers from both the ML and BC communities.

2.1 Setting of the ML algorithm

In the following we assume that each agent has a
labeled dataset of examples, where each data point
(e.g., a transaction) is represented by a d-dimensional
vector x := (x1, . . . , xd) ∈ Rd, collecting d features
xi (e.g., the ID of the user performing the transac-
tion, its timestamp, the amount transferred, etc.).
Each example is associated to a label yi ∈ {0, 1} in-
dicating whether the i-th example is normal (yi = 0)
or anomalous (yi = 1). These examples are col-
lected in a dataset (X , Y) of m ∈ N data points
X := {x1, . . . , xm} with labels Y := {y1, . . . , ym}.
In this paper we work with tabular data, but this is
not required in general and other data types may be
supported, such as images or texts.

For the detection of anomalous examples each
agent trains its own classiﬁer, i.e., a map Φ : X →
[0, 1] that is optimized on the training set, and that
can be used to approximately predict the class of an
unseen data point x, with the usual convention that
the example is classiﬁed as normal if Φ(x) ≤ 0.5 and
as anomalous if Φ(x) > 0.5. We consider ensemble
classiﬁers, which means that we actually train a set
of n ∈ N simpler classiﬁers (or estimators) φi : X →
[0, 1], 1 ≤ i ≤ n, each trained on the same classiﬁca-
tion task, and deﬁne the global prediction of Φ either
by averaging, i.e., Φ(x) := 1
i=1 φi(x), or by ma-
n
jority voting among the n predictions {φi(x)}n
i=1. To
explicitly denote the transformation from the ensem-

(cid:80)n

i=1

(cid:1) and {φi}n

ble to the estimators and vice-versa, we use the no-
tation Φ := Ens (cid:0){φi}n
i=1 := Estim(Φ).
This kind of classiﬁers will be instrumental for our
construction, since they are quite straightforward to
improve by enlarging the ensemble size n and adding
new simple learners, and it is possible to mix diﬀerent
classiﬁers Φ and Φ(cid:48) by mixing their simple learners.
As a prototype of ensemble classiﬁers, in this pa-
per we focus on RFs [21], which use decision trees
as their simple learners. Decision trees [8] are maps
φi : X → [0, 1] that compute their prediction accord-
ing to a binary tree: Once the tree is trained on the
data, at prediction time an input enters the tree from
its root, and it follows a sequence of binary tests un-
til it reaches a leaf node. Each of these leaf nodes is
associated to a unique label, which is the prediction
assigned by the tree to each input that falls into this
leaf. At each non-leaf node, instead, the splitting is
decided by the value of a single feature of the input,
and thus a decision tree can be understood as a se-
quence of binary splits of the input space according
to a subset of features at given splitting values. The
training of this structure requires to select the se-
quence of features and the threshold values to deﬁne
the splitting, and this is usually realized by guaran-
teeing that the examples in the training set are dis-
tributed in a balanced manner among the leaf nodes,
and adopting criteria for the growth of the tree in
depth and width. We refer to [7] for a detailed treat-
ment of this topic.

In addition to their basic ensemble structure, RFs
perform two randomization operations to improve
their accuracy and robustness. Namely, RFs are
trained by bootstrap aggregation, i.e., each tree in
the ensemble is trained on a random subset of the
full dataset, extracted by a sampling with replace-
ment. Additionally, the single trees are trained with
feature bagging, i.e., each splitting of each tree is con-
structed by considering only a uniformly randomly
selected subset of the features of the data.

RFs are particularly suited for tabular data and
they can deal quite eﬀectively with missing entries
thanks to their structure that do not require the
knowledge of each single feature. Moreover, their
training is quite simple and thus suitable to be per-
formed repeatedly, as will be the case in our algo-

3

rithm.

2.2 Setting of the BC solution

A BC is essentially a digital ledger of transactions
that is duplicated and distributed across the partici-
pants in the BC network. Transactions are recorded
in a ﬁnal and immutable manner by the BC, pro-
viding all network members with an identical and
trustworthy real-time view of the state. Due to its
inherent characteristics, BC is the natural platform
to support privacy and trust as well as a secure ex-
ecution environment [10, 17, 33]. Our proposed BC
solution ensures a secure, auditable, and veriﬁable
framework for execution of federated learning algo-
rithms.

The idea is that each learning node in the BC net-
work publishes intermediate results at the end of each
iteration. These results can be consumed by other
learning nodes to improve the accuracy of their next
computations. Our solution is generic and can sup-
port any ML algorithm having the following proper-
ties: The algorithm can be represented as a portable
computation workload (e.g., a docker image which
can be instantiated to a container running the algo-
rithm’s computation); the algorithm can be iterative
or single-step; and it can either be centralized and re-
quire orchestration and synchronization between it-
erations or be distributed and thus self-orchestrating.
For our proposed framework, as underlying BC
technology we leverage Hyperledger Fabric (or simply
Fabric) [1, 4], which is one of the most promising BC
platforms for enterprises (see e.g., [18] for a compre-
hensive and foundational analysis of the BC solutions
and services for enterprises).

3 Federated training of ensem-

ble classiﬁers

With these tools in hand we now introduce the feder-
ated learning algorithm. We will ﬁrst formulate the
algorithm under as general assumptions as possible,
and then we provide some speciﬁcations in the case
of RFs. We will anyhow comment on how these can
be generalized to diﬀerent scenarios.

3.1 Agents and atomic operations

We assume to have a number N ∈ N of agents (or
nodes) participating in the federation, and denote
them as V := {v1, . . . , vN }. Each node vj has an
own dataset (Xj, Yj) of size mj of the form described
in Section 2.1, and its goal is to obtain an ensemble
classiﬁer Φj for the detection of anomalies, working
possibly beyond its own data.

We consider three atomic operations to modify an
ensemble: one enlarges the ensemble, one keeps its
size bounded, and one selects the top performing es-
timators. Assuming that Φ is an existing ensemble
with n estimators, {φi}n(cid:48)
i=1 is another set of estima-
tors, and k ∈ N is an integer parameter, the three
operations are formally deﬁned as follows:

• ADD(Φ, {φi}n(cid:48)
Φ(cid:48) := Ens

(cid:16)

i=1) returns the enlarged ensemble
Estim(Φ) ∪ {φi}n(cid:48)
i=1

(cid:17)

.

• GET TOP(Φ, k) sorts the n estimators of Φ accord-
ing to some order that needs to be speciﬁed, and
returns the top k. If n ≤ k all the n estimators
are returned.

• CROP(Φ, k) keeps only the k best estimators of an
ensemble Φ, i.e., it sets Φ = Ens(GET TOP(Φ, k)).

3.2 Federated learning

The group of agents is partially connected according
to a network represented by an undirected graph G =
(V, E), where there is an edge (vi, vj) ∈ E if and only
if a connection is active between the i-th and j-th
node.

The assumption that the connection graph G is
ﬁxed is only made for simplicity of exposition, but it
is straightforward to deal with time-varying graphs
that may represent e.g. agents entering and leaving
the federation, or temporary failures in the connec-
tion system.
Indeed, for the algorithm to run it is
suﬃcient to assume that each node vj, whenever it is
interested in a communication, is able to get the list
of its ﬁrst order neighbors, i.e., the set of all agents vi
such that there is a link (vj, vi) ∈ E. Moreover, each
node in practice has no need to know the entire graph,

4

and has no option to modify it. More advanced sce-
narios could be envisioned and investigated, for ex-
ample by assigning to the agents a certain budget
that can be used to establish optimized connections
to certain nodes, or by using the knowledge of the
entire connection graph to take some decision on the
learning mechanism. We leave these extensions for
future work.

To manage the communication, each node vj has
a registry Rj with a slot Rj(vi) for each of the other
nodes vi. We assume that each node vi can write a
message to the slot Rj(vi) in the registry of the node
vj if this is one of its ﬁrst order neighbors.

Using the registry and the atomic operations on the
ensemble, we are in the position to deﬁne the three
fundamental operations that each agent vj can per-
form to change its status at each iteration. They are
controlled by three parameters nnew, nmax, nshare ∈ N
that we assume to be globally set, even if local pa-
rameters (i.e., node-dependent) may be used without
signiﬁcant modiﬁcations. The three operations are
the following:

1. FIT: A number nnew ∈ N of simple learners
{φi}nnew
i=1 are trained by the agent on its own
dataset (Xj, Yj), and the ensemble Φj is enlarged
as Φj :=ADD(Φj, {φi}nnew
i=1 ). If the resulting num-
ber of estimators is larger than nmax, then the
method CROP(Φj, nmax) is used to keep only the
best ones.

2. SHARE: The agent identiﬁes its top nshare estima-
tors, and writes them to the registry of each of its
ﬁrst order neighbors. If a registry slot contains
already some estimators from previous commu-
nications, they are overwritten.

3. GET: The agent reads its registry slots to col-
lect all the estimators received in the previous
iterations (if any), and adds them to its current
ensemble by using the ADD method. If this op-
eration makes the ensemble larger than nmax,
excess estimators are removed by a call to the
CROP method.

Finally, the algorithm requires initialization and
termination conditions. For simplicity we assume

that each agent vj starts with an empty ensemble
Φj := Ens(∅) and runs FIT as its ﬁrst operation.
Moreover, each agent terminates its execution when
the prescribed iterations are executed.

3.3 Properties of the algorithm

The entire algorithm is completely decentralized,
since it only requires the existence of a communi-
cation network and the agreement on a set of ini-
tial parameters. The model supports time-varying
networks, and it allows for completely asynchronous
communication,
including the option for diﬀerent
nodes to join or leave the federation at diﬀerent times.
Observe that all the operations except for GET TOP
are well deﬁned for any type of ensemble classiﬁer,
and do not require further speciﬁcation to be imple-
mentable. The only method-speciﬁc operation is thus
GET TOP, that requires to deﬁne a way to rank the es-
timators within an ensemble. We discuss our solution
in the case of RFs in the next section, but we remark
that this choice is not unique, and that similar de-
sign principles could be adopted to work with more
general ensembles.
In this sense, the present algo-
rithm may be understood as a family of algorithms,
parametrized by the method that is used to promote
some estimators with respect to other ones.

The importance of this ranking system is reﬂected
in the fact that we are employing a registry with
slots that stores only the last written information. In
this way, when a node reads its registry via the GET
method, it only reads the result of the most recent
call of GET TOP transmitted by its neighbors.

This solution is used also to guarantee that the reg-
istry has bounded memory footprint, since in this way
it needs to store at most nshare · N estimators at each
time. Similarly, the bound nmax on the number of
estimators held by each single node controls the size
of each ensemble classiﬁer. These two requirements
can be translated to memory bounds if we assume
that each estimator has a maximal memory size.

Moreover, the only operation that can create new
estimators is FIT. Whenever this method is called,
the newly constructed estimators are labeled with
identiﬁers (vj, i), where vj is the identiﬁer of the cre-
ator node, and i is a progressive counter maintained

5

by vj. In this way each estimator in the federation is
uniquely identiﬁed, and it is always possible to know
which nodes trained it. Moreover, communication be-
tween diﬀerent nodes amounts only at the exchange
of estimators via the SHARE and GET methods. Both
the operations of creation and sharing are thus easily
secured by means of the BC integration that we are
discussing in detail in Section 4, so that the federation
is protected against anomalous agents and malicious
injections of information.

3.4 Ranking of the estimators for RF

To obtain a fully functioning algorithm, it remains to
specify the mechanism used to rank the estimators
within each ensemble, i.e., to deﬁne the GET TOP op-
eration. We deﬁne it for RFs, which are the method
of choice of this paper.

As discussed before, the sorting of the estimators
is the most delicate operations and the one that have
the largest potential to aﬀect the result of the al-
gorithm. In general terms, we aim at using unsuper-
vised methods for this task, namely, we do not use the
labels of the data to sort the estimators. The reason
for this choice is that any supervised operation must
rely on the data available to each node, and using the
same local data that are used for training to rank the
estimators is very likely to lead to a downplay of the
importance of the estimators received from the other
nodes. For this reason, we decided to analyze only
methods that rely on the structure of the estimators.
Although diﬀerent RF pruning schemes have been
introduced [15, 25, 34], we use here a mechanism that
allows us to obtain a full sorting of the set of trees,
and not only a reduction of its number. To this
end, we recall that each estimator is a decision tree,
and thus it can be represented by a tree where each
non terminal node v is associated with the index
s(v) ∈ {1, . . . , d} of the splitting feature, and the cor-
responding splitting value x(v) ∈ R (see section 2.1).
We use the splitting index s(v) to identify the type of
a node, and we regard x(v) as node feature, so that
each decision tree can be identiﬁed as D := (T, X),
where T is a tree with labeled nodes, and X is a vec-
tor of node features associated to the non-terminal
nodes.

Given a pair of decision trees D := (T, X), D(cid:48) :=
(T (cid:48), X (cid:48)), we deﬁne a similarity measure that is used
to compute the estimators’ ranking in a structure-
dependent way, i.e., one that takes into account the
deﬁnition of each single estimator. To this end we de-
ﬁne a positive deﬁnite and symmetric kernel k(D, D(cid:48))
over pairs of decision trees. The kernel is a modiﬁ-
cation of the tree kernel of [20], and we provide its
explicit construction in Section A. We refer to [40,46]
for a detailed treatment of the topic of kernel meth-
ods, and we recall here that k can be used to encode
general data (decision trees in this case) in a possibly
high dimensional Hilbert space where standard nu-
merical techniques are available. Moreover, the same
method can be extended to other ensembles as soon
as a kernel can be deﬁned on its building blocks, and
thus the present method has the potential to be ap-
plied in more general settings.

In particular, it is possible to deﬁne a Gaussian
Process [37] with covariance function k over the space
of decision trees. Given the process, one may select
a subset of the set of trees so that, conditioning the
process on the labels associated to these trees, the
maximal standard deviation of the posterior process
is minimized. In this sense, this subset of trees may
be regarded as the one that control the maximal vari-
ation in the data. This problem may be eﬃciently
approximated by a greedy algorithm [13] that selects
this set in an iterative way, and this gives the order-
ing of the estimators that we are looking after. It can
be shown that this process is quasi-optimal [39, 47],
meaning that the greedy selection is as eﬀective as a
global optimization, up to a constant. Running this
algorithm until it selects k elements, we obtain an
ordered sequence D1, . . . , Dk representing the n most
important estimator, thus implementing the GET TOP
operation.

4 A Blockchain solution for se-
cure and trustworthy feder-
ated learning

We describe now in detail our proposed framework,
and we refer to [1, 6] for more details on Hyperledger

6

cates all participants in the network including (i)
the speciﬁcation of Certiﬁcation Authority (CA)
servers (deﬁned as part of the BC network con-
ﬁguration), (ii) the certiﬁcation of users and ap-
plications using these CA servers, and (iii) mech-
anisms to sign and validate the signatures of all
transactions and messages submitted to the net-
work.

• Federated Learning Artifacts Store: The chain-
codes implementing the business logic for stor-
ing, updating, retrieving, and querying business
artifacts related to federated learning, i.e., algo-
rithm images’ metadata, metadata of the learn-
ing process, and intermediate results and mod-
els.

• Artifacts Usage Audit: The inherent function-
alities in chaincodes which allow to query the
history of updates for each artifact stored in the
ledger, thus allowing to present a clear and com-
plete picture of the artifact’s provenance.

• Secure Execution: This module securely runs
the computation tasks of the ML algorithm (we
refer to computation task or workload as the
ML algorithm instance or iteration), producing
signed outputs (i.e., the insights from the learn-
ing round), and storing these outputs in the
ledger. In the case of federated learning, it helps
to establish the auditability and veriﬁability of
the execution of local ML models and to improve
the trust among the participants. Moreover, in
the case of updates to the learning algorithm,
it is guaranteed that all the parties are aware
of the correct image version and are enforced to
use the correct one to participate in the learning
process.

4.2 Veriﬁability of the execution

Our proposed approach allows delegating the com-
putation over sensitive data to the data owner, while
establishing trust of the rest of the stakeholders in
the computation result. This is achieved via imple-
mentation of the following core characteristics:

7

Figure 1: Veriﬁable and auditable federated machine
learning framework

Fabric.

4.1 Structure of the blockchain solu-

tion

The framework consist of diﬀerent conceptual ele-
ments (see Figure 1): (i) a Data Scientist, respon-
sible for creating and pushing the federated learning
algorithm image to the algorithm image registry af-
ter the training phase of the algorithm is over; (ii)
the Algorithm image registry, which is any kind of
local or hosted image registry for storage of docker
images representing the ML algorithms; and (iii) the
Learning Nodes, which are the organizational nodes
of organizations in the BC network participating in
the federated learning process. Here, production data
is stored in premises and only intermediate and ﬁnal
results of the algorithm execution are stored in the
BC ledger.

Additionally, the system comprises a BC execution
i.e., a secure execution environment
environment,
that provides a veriﬁable privacy-preserving compu-
tation environment for federated learning scenarios.
The environment comprises the following modules:

• Identity Management: A built-in service in Hy-
perledger Fabric that provides a membership
identity that manages user IDs and authenti-

• The computation workload is portable so that
it is possible to deploy it in the data owner’s
environment.

• The integrity of the computation workload is
veriﬁable,
i.e., computation stakeholders have
guarantees that the actual computation was per-
formed on the respective data.

• The provenance over the input data, the output
of the computation, and the computation logic
is tracked.

We implement the portability characteristic by
packaging the computation logic in a portable arti-
fact. A docker image is an example of such a portable
artifact, which is suitable for relatively simple com-
putations that allow incorporating the entire logic
into a single image. In cases where the computation
involves multiple steps and components, it can be
packaged as a composite asset, consisting of a set of
images (each incorporating a relevant phase or func-
tion in the computation) and an artifact (or a set of
artifacts) that deﬁne the orchestration and the chore-
ography of the composite computation.

To establish correctness and integrity guarantees
over the computation logic, we propose to manage
computation workloads metadata in BC. Having the
metadata record in a shared distributed ledger en-
sures that all the parties have joint understanding of
how to verify that a given portable deployable arti-
fact is of the correct version and its contents have
not been tampered with. For the algorithm images,
we store a SHA256 hash of the docker image on the
ledger. At the time of computation task creation from
an image, when pulling the image from the algorithm
image registry, we can verify image authenticity by
calculating and comparing the image’s hash to the
one stored in the ledger. For the computation task
results we use public/private key veriﬁcation. When
creating a computation task, we use a crypto library
to generate private/public key pairs. A public key of
the pair is stored in the ledger in the execution task
record, while the private key is passed to the com-
putation task runtime. Once it ﬁnalizes, the compu-
tation task updates its record in the chain with the
results of the execution signed with the private key.

Figure 2: Blockchain solution building blocks and
ﬂows.

The updating chaincode then veriﬁes the signed re-
sult element with the public key of the computation
task to ensure that the results are being updated by
the entity with the correct private key.

Trackability and provenance is gained by provid-
ing auditing and veriﬁability capabilities for manag-
ing the ML algorithm image lifecycle (e.g., publishing
a new algorithm image and usage of the algorithm
image), for secure execution (ensuring, for example,
that the correct algorithm image is used in each ex-
ecution of computation task), and for recording of
intermediate results (allowing to answer questions,
such as which artifacts were published at the end of
each run for a particular learning process, or what ar-
tifacts a particular organization published for a par-
ticular learning task).

8

Figure 2 depicts the interactions between the
chaincodes in our BC network and the other building
blocks comprising our secure execution environment.
These building blocks are: (i) the algorithm image
registry, where the images of the ML algorithms are
stored; (ii) the secure container executor and compu-
tation task runtime components that are the libraries
(Python scripts) supporting the creation of compu-
tation tasks from the ML algorithm image; and (iii)
the intermediate and completed models that are the
outcome artifacts produced by the learning process
which are stored in the ledger.

The following ﬂow describes

the interactions
among the diﬀerent building blocks and relate to the
cycle of creating a ML algorithm image, executing
this image securely on learning nodes, and sharing
the insights:

The image for the ML algorithm, intended to be
run as a particular instance of a federated learning
process,
is stored in the algorithm image registry
(which can be either a shared or a private image
repository). The metadata describing the ML algo-
rithm image, speciﬁcally an identiﬁer of the respec-
tive artifact in the external repository and a cryp-
tographic ﬁngerprint (i.e., a hash value) that can be
used to verify the integrity of the artifact are stored
in the BC ledger using the image chaincode.

During the instantiation of the execution phase,
learning process metadata is created using the learn-
ing process orchestration chaincode. This metadata
includes the unique ID for the learning process, the
algorithm image this process is intended to execute,
the consortium of organizational nodes participating
in the federated learning process, indicators of the
current state of the learning process (e.g., current
iteration in case of iterative learning process), and
current execution status. After the learning process
metadata record is created on the chain, the ML al-
gorithm image is pulled from the algorithm image
registry and instantiated as computation task run-
time on each learning node by the secure container
executor. The task is instantiated with the ML algo-
rithm runtime, the parameters for the run, and the
initial state model.

During the algorithm execution phase, the learn-
ing node reads the relevant insights from previous

rounds by the learning nodes, runs the algorithm im-
age on the relevant inputs (the insights from previous
rounds, the input parameters, and the organizational
datasets), and publishes the resulting insights or com-
pleted model to the ledger using the execution record
and the model chaincodes. Once the learning process
is completed, the status of the learning task on chain
is updated to completed.

As shown in Figure 2, our BC solution comprises
four chaincodes:
the image and execution record
chaincodes which form the secure execution mod-
ule (Figure 1), and the learning process orchestra-
tion and model chaincodes which form the federated
learning artifacts store module (Figure 1). The im-
age chaincode provides the functionality for storing
and retrieving the ML algorithm image metadata.
This chaincode also provides queries helpful in de-
termining the provenance of the image, e.g., who is
the creating organization or when the image was cre-
ated. The learning process orchestration chaincode
records the information about the federated learning
task, including the deﬁnition of the algorithm image
the learning task is about to execute; the consortium
of organizations participating in the learning process
and nodes which will run the computation tasks; and
the current status of the learning process (current
iteration, completion status). The execution record
chaincode stores the execution task metadata in the
ledger. Once the outcome of a single-step computa-
tion task, or of the particular learning round task (for
iterative learning algorithms) is completed at a node,
it publishes the insights to the chain (the estimators
in our case of a fraud detection algorithm) updat-
ing the execution record. It also updates the model
chaincode in case the learning process is completed.
The model chaincode is responsible for publishing the
completed model to the ledger once the learning task
ﬁnishes. The complete model record contains a list
of organizations allowed to access these models which
initially equals the consortium members.

As stressed before, one of the built-in core proper-
ties of BC platforms is an immutable chain of blocks
of transactions, establishing veriﬁable and transpar-
ent history of updates for each artifact stored in the
chain. This is of fundamental importance when striv-
ing for trust and transparency of the execution of

9

federated learning scenarios. Proven, veriﬁable, and
immutable audit trail of execution tasks producing
federated learning models can help establish without
doubt, for example, that the models are derived from
the desired ML algorithm, the speciﬁc version of the
algorithm, and the executing organizations. To this
end, the artifacts usage audit logical module in Fig-
ure 1 supports provenance for algorithm images, com-
putation tasks executed, and for model metadata.

5 Experiments

The implementation of the algorithm and the code to
replicate the experiments presented in this section is
available on GitHub1.

We test the algorithm on a benchmark dataset
for fraud detection2. This dataset collects electronic
credit card transactions that have been executed in
some European banks during September 2013. Each
transaction is represented by 28 numerical features
which are obtained after applying a Principal Com-
ponent Analysis (PCA) on the original features, in
order to hide any sensitive information, and it is la-
beled either as normal or as a fraud. The dataset con-
tains 284807 transactions, of which 492 (the 0.17%)
are frauds, making the dataset highly unbalanced.

We simulate a scenario with N := 20 agents, each
holding its own private data. To create a suitable
setup, we split the given dataset into N disjoint sub-
sets by random sampling. To make the problem more
challenging and interesting for the testing of a feder-
ated scenario, we perform an unbalanced sampling:
instead of splitting the positive and negative exam-
ples into N groups of 284807/N agents, we allow for
each group to contain up to 70% more or less ele-
ments than the average. Additionally, each of the
resulting datasets is split into a train dataset (90%
of the samples) and test dataset. The actual num-
ber of samples for each node and the corresponding
statistics are reported in Table 1. To simplify the
measurement of the performances of the algorithm,
we artiﬁcially create a unique and centralized test

set obtained by joining the N test sets of the single
nodes, so that all the test metrics are computed on
the same test set. This breaks the absence of central-
ized orchestration in the design of the algorithm, but
it is only a convenience choice made for the purpose
of exposition.

To analyze the eﬀect of diﬀerent conﬁgurations of
the federation, we analyze three diﬀerent connection
scenarios (see Figure 3): (i) a fully disconnected set-
ting, (ii) a pairwise connected setting (i.e., each node
is connected to exactly two nodes), and (iii) a fully
connected setting. The disconnected case serves as a
baseline, since it represents the case where no feder-
ation takes place and each node can only rely on its
own dataset.

(a)

(b)

(c)

Figure 3: Connection conﬁgurations tested in the ex-
periments: disconnected (Figure 3a), pairwise con-
nected (Figure 3b), fully connected (Figure 3c).

1https://github.com/GabrieleSantin/federated_fraud_

detection

2https://www.kaggle.com/mlg-ulb/creditcardfraud

For each conﬁguration, we train the federated algo-
rithm by letting each node execute the same sequence

10

of operations (see Section 3.2). Namely, in the base
case of the disconnected topology we run four repeti-
tion of FIT, i.e., each node creates its own model and
reﬁnes it three times. In the connected cases, instead,
we add a SHARE and GET operation after each ﬁt. In
this way, after each training on the local dataset each
node shares its insights to its neighbors, and sub-
sequently reads and incorporates the knowledge re-
ceived by the neighbors themselves. The algorithm is
run with values nnew := 10, nshare := 10, nmax := 50
for the parameters deﬁned in Section 3.2.

To measure the eﬃcacy of the models we use three
metrics, namely the balanced accuracy BAcc, the
precision Prec, and the recall Rec. Given the true test
labels and the predicted test labels, we may count the
number of false positive F P , true positive T P , false
negative F N , false positive F P . With these num-
bers, the three metrics are deﬁned as

Prec :=

BAcc :=

, Rec :=

T P
T P + F P
(cid:18)
1
2

T P
T P + F N

T P
T P + F N
(cid:19)
T N
T N + F P

,

.

+

It should be noted that all the metrics have value in
[0, 1].

We use these metrics to assess the improvement
of the federated models over the scenario where each
node is isolated. To this end, for each node we com-
pute on the test set the metrics in the two federated
cases (pairwise connected and fully connected) and
their diﬀerence with the corresponding value in the
disconnected case. We report in Table 2 the nodes
for which these diﬀerences are maximal and minimal,
and the corresponding values. It should be noted that
for some nodes there is indeed a negative improve-
ment, which means that the participation in the fed-
eration has a negative eﬀect, but the corresponding
values are of order at most 10−2. This is expected
since the algorithm has a randomization component,
and a change of this order of magnitude may be con-
sidered as a reasonable ﬂuctuation. On the other
hand, the maximal improvement is of order 10−1.
In all connection scenarios and for all metrics, the
node of maximal improvement is Node2: looking at
Table 1, it appears that this node has no frauds in

the training set, and it is thus not capable of learn-
ing any meaningful classiﬁer when isolated. On the
other hand, participating in the federation it receives
insights from its neighbors, and it is able to improve
its model in a very signiﬁcant way, up to an improve-
ment of 0.9 for the Prec metric.

Apart from these extreme values, we compute the
mean and median of these diﬀerences over the 20
nodes. These values are reported in Figure 4b, and it
can be observed that overall there is a signiﬁcant in-
crease (0.1 − 0.2) both in the mean and the median,
and for all the three metrics. This conﬁrms that,
apart from the case of single nodes, the federation is
very eﬀective to improve the classiﬁers.

To oﬀer an additional insight into the functioning
of the sharing mechanism, we visualize in Figure 4b
the same metrics, but computed over the train sets
of each single node. In this case, it is remarkable to
observe that both the mean and median are negative,
meaning that the accuracy is decreasing on the train
set when entering the federation. Since the test met-
rics are instead increasing, this is a good sign that the
federated algorithm is able to equip each node with a
model that has an accuracy that goes far beyond the
own dataset, and is eﬀectively able to share insights
not present in each single node.

All these results make it clear that the beneﬁt of
the federation is increased for the fully connected sce-
nario, as one may reasonably expect. On the other
hand, the pairwise connected setting is almost as ef-
fective. This fact is interesting in possible real appli-
cations since one may foresee that establishing and
utilizing a connection may be expensive in diﬀerent
terms, and thus the nodes should be interested in
establishing the minimal set of connections that are
suﬃcient to obtain the desired improvement in the
model. In more general terms, the eﬀect of the topol-
ogy of the connections on the outcome of the algo-
rithm is an interesting aspect to explore. As a ﬁrst
element to explain the quite good eﬀectiveness of the
pairwise interaction, we show in Figure 5 the distri-
bution of the estimators over the N = 20 nodes at
the end of the iteration. Namely, since each estima-
tor is uniquely identiﬁed, it is possible at each mo-
ment to check where the estimators of each node have
been ﬁtted. In the ﬁgure, we show in each row the

11

(a)

(b)

Figure 4: Mean and median improvement in the three metrics over the disconnected case for the two federated
scenarios (Fully connected and Pairwise). The metrics are computed over the train set (Figure 4a) and the
test set (Figure 4b).

origin of the estimators of each node. In the discon-
nected case (left panel) there is no mix, and indeed
each node owns only estimators that it ﬁtted itself.

In the fully connected case (right panel) a quite uni-
form mixing can instead be observed, with the addi-
tion that some nodes (Node0, Node2, Node5, Node6,

12

Node8, Node10) produce almost no estimators that
are used by the other ones. The fact that the mixing
is quite stable among the nodes is an indication of
the eﬀectiveness of the sharing and ranking mecha-
nism. In the intermediate case of the pairwise con-
nected nodes (central panel) the mixing reﬂects the
connection pattern, since each node holds estimators
from its direct neighbors. In this case it is worth re-
marking that the estimators are eﬀectively transmit-
ted beyond the ﬁrst order neighbors of a node, and
this suggests that even a not fully connected network
may be eﬀective for the federation to work.

in the BC for data sharing and trading in a data
marketplace. Data marketplaces for ML models are
an emerging trend [19, 35, 43, 53], which provide the
opportunity to decentralize model development and
lower the entry barrier into ML usage for companies
which do not have either the skills, the capacity, or
the access to learning data to develop the algorithms
and train the models. Chaincodes in the BC network
could control the access and permissions to the diﬀer-
ent models stored in chain applying governance rules
deﬁned by the consortium organizations.

6 Conclusions and future work

In this paper we developed and presented a feder-
ated anomaly detection algorithm that can leverage
the communication between collaborating nodes in
order to improve the models’ performances. The al-
gorithm is designed according to a fully decentralized
structure, and it allows the sharing of algorithmic in-
sights without the movement of any data. Although
the classiﬁers are deﬁned on top of Random Forests,
we discussed how the same structure can be adapted
to more general scenarios.

Remarkably, the federated learning algorithm is de-
veloped in order to be fully integrated into a BC so-
lution that ensures privacy-preserving guarantees on
the execution and on its results. This component
make it possible to verify the identity of the partici-
pating nodes, and to audit the execution of the algo-
rithms and the correct functioning of the federation.
Also in this case, the BC solution is not bounded
to the speciﬁc algorithm of choice, and we discussed
how more general machine learning models may be
secured within the same framework.

Extensions of the basic algorithmic structure will
be analyzed in future work, where more complex
building blocks can be exploited in place of Ran-
dom Forests and Decision Trees. Moreover, the ef-
fect of the connection topology on the behavior of
the algorithm has been only partially explored in this
work, and interesting options for its optimization re-
main open. Ultimately, we may foresee the applica-
tion of these techniques to leverage the models stored

Acknowledgments

The work of the authors was partially supported by
the H2020 INFINITECH project, grant agreement
number 856632.

References

[1] Hyperledger

fabric.
hyperledger.org/use/fabric, 2021.

https://www.

[2] S. Abdulrahman, H. Tout, H. Ould-Slimane,
A. Mourad, C. Talhi, and M. Guizani. A survey
on federated learning: The journey from central-
ized to distributed on-site learning and beyond.
IEEE Internet of Things Journal, 8(7):5476–
5497, 2021.

[3] M. Ahmed, A. Naser Mahmood, and J. Hu. A
survey of network anomaly detection techniques.
Journal of Network and Computer Applications,
60:19–31, 2016.

[4] E. Androulaki, A. Barger, V. Bortnikov,
C. Cachin, K. Christidis, A. De Caro,
D. Enyeart, C. Ferris, G. Laventman,
Y. Manevich, S. Muralidharan, C. Murthy,
B. Nguyen, M. Sethi, G. Singh, K. Smith,
A. Sorniotti, C. Stathakopoulou, M. Vukoli´c,
S. W. Cocco, and J. Yellick. Hyperledger fabric:
A distributed operating system for permissioned
blockchains.
In Proceedings of the Thirteenth
EuroSys Conference, EuroSys ’18, New York,

13

Figure 5: Origin of the estimators selected by each node at the end of the iteration for the three connection
settings. Each row represents a node, and the columns indicate the origin of its estimators. The values of
each row are normalized as percentages which sum to 100%.

NY, USA, 2018. Association for Computing
Machinery.

[5] N. Aronszajn. Theory of reproducing kernels.
Transactions of the American Mathematical So-
ciety, 68:337–404, 1950.

[6] S. A. Baset, L. Desrosiers, N. Gaur, P. Novotny,
A. O’Dowd, and V. Ramakrishna. Hands-on
blockchain with Hyperledger: building decentral-
ized applications with Hyperledger Fabric and
composer. Packt Publishing Ltd, 2018.

[7] L. Breiman. Random forests. Machine Learning,

45(1):5–32, Oct 2001.

[8] L. Breiman, J. H. Friedman, R. A. Olshen, and
C. J. Stone. Classiﬁcation and regression trees.
Routledge, 2017.

[9] V. Chandola, A. Banerjee, and V. Kumar.
Anomaly detection: A survey. ACM Comput.
Surv., 41(3), jul 2009.

[10] K. Christidis and M. Devetsikiotis. Blockchains
and smart contracts for the internet of things.
IEEE Access, 4:2292–2303, 2016.

language.

[11] M. Collins and N. Duﬀy. Convolution kernels
In Proceedings of the
for natural
14th International Conference on Neural Infor-
mation Processing Systems: Natural and Syn-
thetic, NIPS’01, page 625–632, Cambridge, MA,
USA, 2001. MIT Press.

[12] I. Dayan, H. R. Roth, A. Zhong, A. Harouni,
A. Gentili, A. Z. Abidin, A. Liu, A. B. Costa,
B. J. Wood, C.-S. Tsai, C.-H. Wang, C.-N. Hsu,
C. K. Lee, P. Ruan, D. Xu, D. Wu, E. Huang,
F. C. Kitamura, G. Lacey, G. C. de Antˆonio Cor-
radi, G. Nino, H.-H. Shin, H. Obinata, H. Ren,
J. C. Crane, J. Tetreault, J. Guan, J. W. Gar-
rett, J. D. Kaggie, J. G. Park, K. Dreyer, K. Ju-
luru, K. Kersten, M. A. B. C. Rockenbach, M. G.
Linguraru, M. A. Haider, M. AbdelMaseeh,
N. Rieke, P. F. Damasceno, P. M. C. e Silva,
P. Wang, S. Xu, S. Kawano, S. Sriswasdi, S. Y.
Park, T. M. Grist, V. Buch, W. Jantarabenjakul,
W. Wang, W. Y. Tak, X. Li, X. Lin, Y. J. Kwon,
A. Quraini, A. Feng, A. N. Priest, B. Turkbey,
B. Glicksberg, B. Bizzo, B. S. Kim, C. Tor-D´ıez,
C.-C. Lee, C.-J. Hsu, C. Lin, C.-L. Lai, C. P.
Hess, C. Compas, D. Bhatia, E. K. Oermann,
E. Leibovitz, H. Sasaki, H. Mori, I. Yang, J. H.
Sohn, K. N. K. Murthy, L.-C. Fu, M. R. F.

14

de Mendon¸ca, M. Fralick, M. K. Kang, M. Adil,
N. Gangai, P. Vateekul, P. Elnajjar, S. Hickman,
S. Majumdar, S. L. McLeod, S. Reed, S. Gr¨af,
S. Harmon, T. Kodama, T. Puthanakit, T. Maz-
zulli, V. L. de Lavor, Y. Rakvongthai, Y. R.
Lee, Y. Wen, F. J. Gilbert, M. G. Flores, and
Q. Li. Federated learning for predicting clini-
cal outcomes in patients with covid-19. Nature
Medicine, 27(10):1735–1743, Oct 2021.

[13] S. De Marchi, R. Schaback, and H. Wendland.
Near-optimal data-independent point locations
for radial basis function interpolation. Adv.
Comput. Math., 23(3):317–330, 2005.

[14] E. Diao, J. Ding, and V. Tarokh. Heteroﬂ:
Computation and communication eﬃcient feder-
ated learning for heterogeneous clients. CoRR,
abs/2010.01264, 2020.

[15] L. Giﬀon, C. Lamothe, L. Bouscarrat, P. Mi-
lanesi, F. Cherfaoui, and S. Ko¸co. Pruning Ran-
dom Forest with Orthogonal Matching Trees.
In
https://cap-rﬁap2020.sciencesconf.org/,
Vannes, France, June 2020.

[16] I. Goodfellow, Y. Bengio, and A. Courville. Deep

Learning. MIT Press, 2016.

[17] H. Guo and X. Yu. A survey on blockchain tech-
nology and its security. Blockchain: Research
and Applications, page 100067, 2022.

[18] S. Gupta, C. S. Oﬃcer, and M. Madhur. Hfs
top 10 enterprise blockchain services 2018. Cam-
bridge, MA, 2018.

[19] M. Ha, S. Kwon, Y. J. Lee, Y. Shim, and
J. Kim. Where wts meets wtb: A blockchain-
based marketplace for digital me to trade users’
private data. Pervasive and Mobile Computing,
59:101078, 2019.

[20] D. Haussler. Convolution kernels on discrete

structures, 1999.

[21] T. K. Ho. Random decision forests.

In Pro-
ceedings of 3rd international conference on doc-
ument analysis and recognition, volume 1, pages
278–282. IEEE, 1995.

[22] S. Horvath, S. Laskaridis, M. Almeida, I. Leon-
tiadis, S. I. Venieris, and N. D. Lane. Fjord:
Fair and accurate federated learning under het-
erogeneous targets with ordered dropout. CoRR,
abs/2102.13451, 2021.

[23] P. Kairouz, H. B. McMahan, B. Avent, A. Bel-
let, M. Bennis, A. N. Bhagoji, K. Bonawitz,
Z. Charles, G. Cormode, R. Cummings, R. G. L.
D’Oliveira, H. Eichner, S. E. Rouayheb,
D. Evans, J. Gardner, Z. Garrett, A. Gasc´on,
B. Ghazi, P. B. Gibbons, M. Gruteser, Z. Har-
chaoui, C. He, L. He, Z. Huo, B. Hutchinson,
J. Hsu, M. Jaggi, T. Javidi, G. Joshi, M. Kho-
dak, J. Konecn´y, A. Korolova, F. Koushan-
far, S. Koyejo, T. Lepoint, Y. Liu, P. Mittal,
M. Mohri, R. Nock, A. ¨Ozg¨ur, R. Pagh, H. Qi,
D. Ramage, R. Raskar, M. Raykova, D. Song,
W. Song, S. U. Stich, Z. Sun, A. T. Suresh,
F. Tram`er, P. Vepakomma, J. Wang, L. Xiong,
Z. Xu, Q. Yang, F. X. Yu, H. Yu, and S. Zhao.
Advances and open problems in federated learn-
ing. Foundations and Trends in Machine Learn-
ing, 14(1–2):1–210, 2021.

[24] Y. Kang, Y. Liu, Y. Wu, G. Ma, and Q. Yang.
Privacy-preserving federated adversarial domain
adaption over feature groups for interpretability.
arXiv preprint arXiv:2111.10934, 2021.

[25] V. Y. Kulkarni and P. K. Sinha. Pruning of
random forest classiﬁers: A survey and future
directions. In 2012 International Conference on
Data Science Engineering (ICDSE), pages 64–
68, 2012.

[26] Y. LeCun, Y. Bengio, and G. Hinton. Deep
learning. Nature, 521(7553):436–444, May 2015.

[27] C. Li, Y. Yuan, and F.-Y. Wang. Blockchain-
enabled federated learning: A survey. In 2021
IEEE 1st International Conference on Digital
Twins and Parallel Intelligence (DTPI), pages
286–289, 2021.

[28] D. Li, D. Han, T.-H. Weng, Z. Zheng, H. Li,
H. Liu, A. Castiglione, and K.-C. Li. Blockchain
for federated learning toward secure distributed

15

machine learning systems: a systemic survey.
Soft Computing, Nov 2021.

[29] R. Liu and H. Yu. Federated graph neural
networks: Overview, techniques and challenges.
arXiv preprint arXiv:2202.07256, 2022.

[30] G. Long, Y. Tan, J. Jiang, and C. Zhang. Feder-
ated Learning for Open Banking, pages 240–254.
Springer International Publishing, Cham, 2020.

[31] C. Ma, J. Li, M. Ding, L. Shi, T. Wang, Z. Han,
and H. V. Poor. When federated learning
meets blockchain: A new distributed learning
paradigm. CoRR, abs/2009.09338, 2020.

S. Hampson,

[32] H. B. McMahan, E. Moore, D. Ram-
and B. A. y Arcas.
age,
Communication-eﬃcient
deep
networks from decentralized data. In AISTATS,
2017.

learning

of

[33] W. Meng, E. W. Tischhauser, Q. Wang,
Y. Wang, and J. Han. When intrusion detection
meets blockchain technology: A review. IEEE
Access, 6:10179–10188, 2018.

[34] F. Nan, J. Wang, and V. Saligrama. Prun-
ing random forests for prediction on a budget.
In Proceedings of the 30th International Confer-
ence on Neural Information Processing Systems,
NIPS’16, page 2342–2350, Red Hook, NY, USA,
2016. Curran Associates Inc.

[35] D. Nasonov,

A. A. Visheratin,

and
A. Boukhanovsky.
Blockchain-based trans-
action integrity in distributed big data mar-
ketplace.
In Y. Shi, H. Fu, Y. Tian, V. V.
Krzhizhanovskaya, M. H. Lees, J. Dongarra,
and P. M. A. Sloot, editors, Computational
Science – ICCS 2018, pages 569–577, Cham,
2018. Springer International Publishing.

[37] C. E. Rasmussen and C. K. I. Williams. Gaus-
sian Processes for Machine Learning. The MIT
Press, 2006.

[38] O. Sagi and L. Rokach. Ensemble learning: A
survey. WIREs Data Mining and Knowledge
Discovery, 8(4):e1249, 2018.

[39] G. Santin and B. Haasdonk. Convergence rate
of the data-independent P-greedy algorithm in
kernel-based approximation. Dolomites Res.
Notes Approx., 10:68–78, 2017.

[40] J. Shawe-Taylor and N. Cristianini. Kernel
Methods for Pattern Analysis. Cambridge Uni-
versity Press, 2004.

[41] M. J. Sheller, B. Edwards, G. A. Reina, J. Mar-
tin, S. Pati, A. Kotrotsou, M. Milchenko, W. Xu,
D. Marcus, R. R. Colen, and S. Bakas. Fed-
erated learning in medicine:
facilitating multi-
institutional collaborations without sharing pa-
tient data. Scientiﬁc Reports, 10(1):12598, Jul
2020.

[42] Y. Shi, H. Yu, and C. Leung. A survey of
fairness-aware federated learning. arXiv preprint
arXiv:2111.01872, 2021.

[43] M. Travizano, C. Sarraute, M. Dolata, A. M.
French, and H. Treiblmaier. Wibson: A Case
Study of a Decentralized, Privacy-Preserving
Data Marketplace, pages 149–170. Springer In-
ternational Publishing, Cham, 2020.

[44] M. H. ur Rehman, K. Salah, E. Damiani,
and D. Svetinovic. Towards blockchain-based
In IEEE
reputation-aware federated learning.
INFOCOM 2020 - IEEE Conference on Com-
puter Communications Workshops (INFOCOM
WKSHPS), pages 183–188, 2020.

[45] Z. Wang and Q. Hu. Blockchain-based feder-
ated learning: A comprehensive survey. CoRR,
abs/2110.02182, 2021.

[36] S. R. Pokhrel and J. Choi. Federated learning
with blockchain for autonomous vehicles: Anal-
ysis and design challenges. IEEE Transactions
on Communications, 68(8):4734–4746, 2020.

[46] H. Wendland. Scattered Data Approximation,
volume 17 of Cambridge Monographs on Applied
and Computational Mathematics. Cambridge
University Press, Cambridge, 2005.

16

[47] T. Wenzel, G. Santin, and B. Haasdonk. A novel
class of stabilized greedy kernel approximation
algorithms: Convergence, stability and uniform
point distribution. Journal of Approximation
Theory, 262:105508, 2021.

[48] S. Xu, S. Liu, and G. He. A method of feder-
ated learning based on blockchain. In The 5th
International Conference on Computer Science
and Application Engineering, CSAE 2021, New
York, NY, USA, 2021. Association for Comput-
ing Machinery.

[49] Q. Yang, Y. Liu, T. Chen, and Y. Tong. Fed-
erated machine learning: Concept and applica-
tions. ACM Trans. Intell. Syst. Technol., 10(2),
2019.

[50] Q. Yang, Y. Liu, Y. Cheng, Y. Kang, T. Chen,
and H. Yu.
Synthesis
Lectures on Artiﬁcial Intelligence and Machine
Learning, 13(3):1–207, 2019.

Federated learning.

[51] W. Yang, Y. Zhang, K. Ye, L. Li, and C.-Z.
Xu. Ffd: A federated learning based method for
credit card fraud detection. In K. Chen, S. Se-
shadri, and L.-J. Zhang, editors, Big Data – Big-
Data 2019, pages 18–32, Cham, 2019. Springer
International Publishing.

[52] C. Zhang, Y. Xie, H. Bai, B. Yu, W. Li,
and Y. Gao. A survey on federated learning.
Knowledge-Based Systems, 216:106775, 2021.

[53] G. Zyskind, O. Nathan, and A. S. Pentland. De-
centralizing privacy: Using blockchain to protect
personal data. In 2015 IEEE Security and Pri-
vacy Workshops, pages 180–184, 2015.

A Construction of the tree ker-

nel

We consider a set D := {Di}nD
i=1 of nD ∈ N decision
trees, where Di := (Ti, Xi), Ti is a tree where each
non-terminal node v has a label s(v) ∈ {1, . . . , d},
d ∈ N, and for each node x(v) ∈ R is node feature.

We deﬁne a positive deﬁnite and symmetric kernel
over D by a modiﬁcation of the convolutional ker-
nel of [11, 20]. Namely, we ﬁrst enumerate the set
t1, . . . , tM of all subtrees of the trees in D. We re-
mark that the trees here are labeled, meaning that
the trees are equal only if the corresponding nodes
have the same label. Given a tree T and any node
v ∈ T , we then deﬁne a feature map

h(v) := [I1(v), . . . , IM (v)]T ∈ {0, 1}M ,

where Ii(v) = 1 if and only if the subtree ti is rooted
in v. This allow us to deﬁne the kernel knode of [11]
between two nodes v ∈ T , v ∈ T (cid:48), as

knode(v, v(cid:48)) := h(v)T h(v(cid:48)) =

M
(cid:88)

i=1

hi(v)hi(v(cid:48)).

It can be proven that C(v, v(cid:48)) can be eﬃciently com-
puted in polynomial time, and it simply counts the
number of common subtrees rooted at both v and v(cid:48)
(see [11]). This kernel can be used to deﬁne a tree
kernel k between T, T (cid:48) simply by aggregation over all
pairs of nodes, i.e.,

ktree(T, T (cid:48)) :=

(cid:88)

knode(v, v(cid:48)).

v∈T,v(cid:48)∈T (cid:48)

We extend this deﬁnition to a kernel k on our De-
cision Trees D := (T, X), D(cid:48) := (T (cid:48), X (cid:48)) ∈ D simply
by adding a second kernel that takes into account the
values of the node features, namely we sets

k(T, T (cid:48)) :=

(cid:88)

kfeat(x(v), x(v(cid:48)))knode(v, v(cid:48)),

v∈T,v(cid:48)∈T (cid:48)

where kfeat : R × R → R is any positive deﬁnite ker-
nel. Observe that k is positive deﬁnite because it is
obtained by sums and products of positive deﬁnite
kernels [5]. Moreover, for simplicity we use the linear
kernel kfeat(x(v), x(v(cid:48))) := x(v)x(v(cid:48)), and this make
it possible to write also k as an aggregation over node

17

kernels via

k(T, T (cid:48)) =

(cid:88)

kfeat(x(v), x(v(cid:48)))knode(v, v(cid:48))

=

=

v∈T,v(cid:48)∈T (cid:48)
(cid:88)

v∈T,v(cid:48)∈T (cid:48)
(cid:88)

v∈T,v(cid:48)∈T (cid:48)

x(v)x(v(cid:48))h(v)T h(v(cid:48))

hx(v)T hx(v(cid:48)),

where hx(v) := [x(v)I1(v), . . . , x(v)IM (v)]T ∈ RM .

ID

Type
Node00 Train
Test
Node01 Train
Test
Node02 Train
Test
Node03 Train
Test
Node04 Train
Test
Node05 Train
Test
Node06 Train
Test
Node07 Train
Test
Node08 Train
Test
Node09 Train
Test
Node10 Train
Test
Node11 Train
Test
Node12 Train
Test
Node13 Train
Test
Node14 Train
Test
Node15 Train
Test
Node16 Train
Test
Node17 Train
Test
Node18 Train
Test
Node19 Train
Test

Samples Frauds Fraud ratio

14733
28489
9570
28489
12992
28489
15544
28489
13064
28489
16036
28489
11149
28489
17571
28489
3297
28489
10820
28489
18365
28489
7875
28489
8298
28489
28249
28489
11155
28489
3363
28489
7894
28489
13798
28489
14098
28489
18450
28489

16
57
37
57
0
57
49
57
21
57
13
57
8
57
32
57
11
57
34
57
17
57
33
57
9
57
28
57
32
57
12
57
25
57
9
57
41
57
11
57

0.0011
0.0020
0.0039
0.0020
0.0000
0.0020
0.0032
0.0020
0.0016
0.0020
0.0008
0.0020
0.0007
0.0020
0.0018
0.0020
0.0033
0.0020
0.0031
0.0020
0.0009
0.0020
0.0042
0.0020
0.0011
0.0020
0.0010
0.0020
0.0029
0.0020
0.0036
0.0020
0.0032
0.0020
0.0007
0.0020
0.0029
0.0020
0.0006
0.0020

Table 1: Size of the datasets for the 20-nodes simula-
tion, and corresponding numbers and ratio of frauds.

18

Fully connected Node18
Node7
Pairwise

Fully connected Node10
Node10
Pairwise

BAcc

min

-1.75e-02 Node2
3.52e-05 Node2
Prec

min

-5.22e-02 Node2
-7.93e-02 Node2
Rec

max

3.95e-01
3.95e-01

max

9.18e-01
9.38e-01

min

max

Fully connected Node9
Node3
Pairwise

-3.51e-02 Node2
Node2

0

7.89e-01
7.89e-01

Table 2: Minimal and maximal improvement with re-
spect to the disconnected case for the two federated
scenarios (Fully connected and Pairwise), as mea-
sured by the three test metrics.

19

