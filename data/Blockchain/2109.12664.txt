BigBFT: A Multileader Byzantine Fault Tolerance
Protocol for High Throughput

Salem Alqahtani
Computer Science Department
University at Buffalo,SUNY
salemmoh@buffalo.edu

Murat Demirbas
Computer Science Department
University at Buffalo,SUNY
demirbas@buffalo.edu

1
2
0
2

t
c
O
2
1

]

C
D
.
s
c
[

2
v
4
6
6
2
1
.
9
0
1
2
:
v
i
X
r
a

Abstract—This paper describes BigBFT, a multi-leader Byzan-
tine fault tolerance protocol that achieves high throughput and
scalable consensus in blockchain systems. BigBFT achieves this
by (1) enabling every node to be a leader that can propose
and order the blocks in parallel, (2) piggybacking votes within
rounds, (3) pipelining blocks across rounds, and (4) using only
two communication steps to order blocks in the common case.

BigBFT has an amortized communication cost of O(n) over n
requests. We evaluate BigBFT’s performance both analytically,
using back-of-the-envelope load formulas to construct a cost
analysis, and also empirically by implementing it in our Pax-
iBFT framework. Our evaluation compares BigBFT with PBFT,
Tendermint, Streamlet, and Hotstuff under various workloads
using deployments of 4 to 20 nodes. Our results show that
BigBFT outperforms PBFT, Tendermint, Streamlet, and Hotstuff
protocols either in terms of latency (by up to 70%) or in terms
of throughput (by up to 190%).

Keywords: Byzantine fault tolerance, permissioned blockchain,

distributed ledgers consensus.

I. INTRODUCTION

The proliferation of cryptocurrency boosted the adop-
tion of Byzantine fault-tolerance (BFT) in many permis-
sioned blockchain systems. Compared to Proof-Of-Work ap-
proaches [1], BFT protocols provide the advantages of com-
putation efﬁciency and near-instant block ﬁnality.

BFT has been well-studied in the context of distributed
systems [2]. Tendermint [3] arose later and implemented PBFT
like protocol for permisssioned blockchain systems. Unfortu-
nately, both PBFT and Tendermint are communication heavy
protocols and only applicable to small-scale systems. When
extending the these protocols to a large-scale, its performance
may become unacceptable.

Many BFT consensus protocols emerged recently to im-
prove communication efﬁciency. These include Hotstuff [4],
Streamlet [5], Fast-Hotstuff [6], and SBFT [7]. Unfortunately,
these protocols still have limited scalability and performance
because all communication go through a single leader, which
constitutes some of the throughput bottlenecks [8]–[11]. The
leader performs a disproportionately large amount of work
compared to its N − 1 followers in some BFT protocols. This
affects efﬁciency, scalability, and prevents high throughput
when N is increased. For example,
in Hotstuff, for each
consensus instance, the followers receive one message from
the leader and send one message back. For instance, in a
cluster of N = 4 nodes, the leader handles 16 messages for

one consensus instance while followers handle 8 messages in
Hotstuff. To scale the system and to increase the throughput,
we need to reduce the disproportionate workload on the leader
and number of messages sent/received for one consensus
instance.

To alleviate the single leader bottleneck, multiple leader
protocols were introduced [12]–[15]. Mir-BFT is the ﬁrst
protocol
that enables leaders to operate independently on
different sequence spaces and reach consensus as long as
there are no conﬂicts. However, the protocol inherits PBFT’s
message complexity and does not use pipelining techniques.
Mir-BFT performance is reduced due to view change protocol.
In this paper, we design BigBFT, a parallel-leader BFT
protocol that addresses these bottlenecks and shortcomings.
BigBFT eliminates the single leader bottleneck and distributes
the load over all leaders. Similar to Mir-BFT [12], BigBFT
rotates the coordinator to assign sequence space to each leader
in the coordination phase. Unlike Mir-BFT [12], BigBFT
pipelines the coordination phase to detect Byzantine coordi-
nators by using a separate consensus/conﬁguration box. This
coordination phase is not on the critical path of the BigBFT
protocol and are done concurrently with the previous round of
the BigBFT consensus. For improving the protocol scalability,
the coordination phase signs a batch of sequence number to
each leader and all leaders monitor the protocol in every
sequence number. After a new coordinator proposes a new
batch of sequence numbers in the coordination phase, leaders
agree on all blocks in parallel in BigBFT protocol. This way,
BigBFT leaders can enforce the total order on all requests. A
malicious leader in BigBFT impacts only itself and due to an
incentive mechanism, the malicious leader has no incentives
to do so. BigBFT has two communication steps to order
blocks. Leaders propose the client requests for every other
leaders. Upon receiving the N −1 propose messages, the leader
piggybacks them together into a single voting message. Upon
receiving a quorum certiﬁcate of the N − F nodes, the node
commits the value and replies to the client in the second round.
We model check the correctness of the high-level protocol in
TLA+.

BigBFT pipelines commit of previous round with proposing
requests of next round and amortizes the cost of voting phase
over number of requests to reduce message complexity to be
linear in BigBFT. We provide a back-of-the-envelope perfor-

978-1-6654-4331-9/21/$31.00 ©2021 IEEE

 
 
 
 
 
 
mance analysis of BigBFT, and provide experimental results
and evaluate BigBFT in both LAN and WAN deployments.
BigBFT has an amortized communication cost of O(n) over
n requests. We evaluate BigBFT’s performance both analyt-
ically, using back-of-the-envelope load formulas to construct
a cost analysis, and also empirically by implementing it in
our PaxiBFT framework. Our evaluation compares BigBFT
with PBFT, Tendermint, Streamlet, and Hotstuff under various
workloads using deployments of 4 to 20 nodes. Our results
show that BigBFT’s latency is 70% better than HotStuff, 55%
better than PBFT, and 65% better than Streamlet. BigBFT
in WAN can provide 100% higher throughput than PBFT,
and 190% higher throughput than Streamlet, and can match
HotStuff’s throughput.

We implement BigBFT and compare and benchmark PBFT,
Tendermint, Streamlet, and Hotstuff. Our experiments, con-
ducted on AWS EC2 nodes with 4 to 20 nodes in various LAN
and WAN topologies, show that BigBFT is effective in scaling
consensus to large clusters. For 20 nodes LAN deployment,
PBFT throughput gets saturated at 500 requests per second,
Tendermint throughput reaches its limit of around 130 req/sec,
Streamlet throughput reaches its limit of around 700 req/sec,
whereas BigBFT scales beyond 2150 req/sec with little latency
increased. HotStuff throughput reaches around 2400 req/sec,
but with 40% higher latency than BigBFT.

II. BACKGROUND AND RELATED WORK

A. State Machine Replication

State Machine Replication (SMR) [16]–[18] is a general
method for building fault tolerant systems. In SMR, every
node stores a state of the system and applies the same set of
commands on the state even if a fraction of them are faulty.
In practice, SMR uses consensus protocols [2]–[4], [17], [18]
to reach consensus among all nodes on the system state.

Consensus protocols guarantee N on − triviality (the de-
cided value v was proposed by a correct node), Saf ety
(all correct nodes output the same value v), and Liveness
(eventually all correct nodes output some value).

The famous FLP impossibility result [19] proved that a
deterministic agreement protocol in an asynchronous systems
cannot guarantee liveness if one node may crash. Many
consensus protocols have been proposed to circumvent the
FLP impossibility to achieve an asynchronous consensus such
as failure detectors, randomness, and time assumptions. One
example of consensus protocol that deals with FLP impossibil-
ity result to tolerate Byzantine nodes is PBFT [2](details can
refer to Section II-C1). PBFT guarantees safety and liveness
in the partial synchronous network model [20], and this is
achieved under the 1/3 optimal resilience bound [21].

B. Leader Bottleneck

Consensus protocols rely on a strong single leader to
coordinate and to order the client requests in the system.
This strong leader, however, is often a bottleneck, especially
when every read and write operation has to go through it.
A strong leader needs to send messages to all the replicas,

and receive responses to know when the operation has been
successfully replicated in the state machine. In our recent
work in BFT protocols, we identiﬁed and studied single-leader
bottlenecks [8]. In the more common case, the leader will be
bottlenecked at the CPU serializing, deserializing, and process-
ing these messages. Too many messages are sent, received, and
processed by one node. To solve leader bottlenecks in a total
ordering, we alleviate the single leader bottlenecks by using
multi-leaders.

C. Single Leader

1) PBFT: PBFT protocol [2] provides the ﬁrst practical
solution to the Byzantine problem [17]. PBFT employs an
optimal bound of N ≥ 3F +1 leaders, where the Byzantine
adversaries can only control up to F leaders. PBFT uses
encrypted messages to prevent spooﬁng and to replay attacks,
as well as to detect corrupted messages. PBFT employs a
leader-based paradigm, guarantees safety in an asynchronous
model, and guarantees liveness in a partially synchronous
model. PBFT requires O(n2) transmissions in its best case
and O(n4) in the worst case.

2) Tendermint BFT: Tendermint [3], used by Cosmos net-
work [22], utilizes a proof-of-stake for leader election and
votes on appending a new block to the chain. Tendermint
rotates its leaders using a predeﬁned leader selection function
that priorities selecting a new leader based on its stake
value. The protocol employs a locking mechanism after the
ﬁrst phase to prevent any malicious attempt to make leaders
commit different transactions at the same height of the chain.
Each leader starts a new height by waiting for prepare and
commit votes from 2F + 1 leaders and relies on the gossip
network to spread votes among all leaders in both phases.
Tendermint prevents the hidden lock problem [3] by waiting
for δ time. The hidden lock problem occurs because receiving
N − F replies from participants (up to F of which may be
Byzantine) alone is not sufﬁcient to ensure that the leader
gets to see the highest lock; the highest lock value may be
hidden in the other F honest nodes which the leader did not
wait to hear from. Such an impatient leader may propose a
lower lock value than what is accepted and this in turn may
lead to a liveness violation.

3) HotStuff BFT: HotStuff protocol [4], is used in Face-
book’s Libra [23]. HotStuff rotates leaders for each block
using a rotation function. HotStuff is responsive; it operates
at network speed by moving to the next phase after the leader
receives N −F votes. This is achieved by adding a pre-commit
phase to the lock-precursor. To assign data and show proof of
message reception and progression, the protocol uses Quorum
Certiﬁcate(QC), which is a collection of N − F signatures
over a leader proposal. Moreover, HotStuff uses a one-to-all
communication. This reduces the number of message types
and communication cost to become linear. The good news is
that, since all phases become the same communication-pattern,
HotStuff uses a pipeline mechanism and performs four leader
blocks in parallel, thus improving the throughput by four.

4) Streamlet: Streamlet protocol is a consensus algorithm
that was proposed in 2020 [5] as a simpler alternative to
leverages the
PBFT-based blockchain protocols. Streamlet
blockchain infrastructure and the longest chain rule in the
Nakamoto protocol [1] to simplify consensus. Similar to both
Tendermint and HotStuff, Streamlet rotates its leader for each
block using a rotation function. The protocol proceeds in
consecutive and synchronized epochs where each epoch has
a dedicated leader known by all validators. Each epoch has
a leader-to-participants and participants-to-all communication
pattern. This reduces the number of message types, but the
communication cost is not linear O(N 3). Streamlet has a
single mode of execution and there is no separation between
the normal and the recovery mode. Streamlet guarantees safety
even under asynchronous environments with arbitrary network
delays and provides liveness when synchrony assumptions
start to hold.

D. Multi-Leaders

1) Mir-BFT: Mir-BFT [12] is a multi-leader consensus
protocol that aims to improve the scalability and throughput
of the system. Mir-BFT starts by partitioning the request
hash space among all
leaders to solve duplication attacks
and rotates request hash space among all leaders to solve
censorship attacks. It also uses batching and watermarks to
facilitate concurrent proposals of batches by multiple parallel
leaders. Mir-BFT proceeds in epochs and each epoch has a
single primary and a set of leaders. Each leader will run
an independent instance of PBFT [2]. Mir-BFT improves the
performance throughput in WAN deployment and introduces
a more robust BFT protocol. In terms of leaders failure,
the throughput can only recover after multiple view changes
discard the faulty leaders. If many leaders suspect the primary,
then they timeout the epoch, and ask for epoch to be changed.
2) FnF-BFT: FnF-BFT [13] is a parallel-leader BFT con-
sensus protocol that provides high throughput under malicious
behaviors. FnF-BFT uses a Byzantine resilient performance
metric to evaluate a BFT’s performance. FNF-BFT has view
change protocol, but has linear communication complex-
ity during the synchrony. FnF-BFT can achieve Byzantine-
resilient performance with a ratio of 16/27 while maintaining
both safety and liveness. FnF-BFT provides three properties
under a stable network which are optimistic performance,
Byzantine-resilient performance, and efﬁciency. To achieve
these three properties, FnF-BFT enables all replicas to con-
tinuously act as leaders in parallel to share the load of client’s
requests and does not replace leaders upon failure but based
on the performance history.

3) RCC: RCC [14] is concurrent leader protocol that in-
troduced a paradigm called RCC that enables any message
exchange patterns to run in parallel. The protocol requires
instances to unify after each request creating a signiﬁcant
overhead. Additionally, the protocol relies on failure detection,
which is only possible in synchronous networks. With BigBFT,
we allow leaders to make progress independently of each other
without any affect of failure detection.

III. PRELIMINARIES

A. System Model

We consider a permissioned blockchain system with ar-
bitrary number of clients and a ﬁnite set of leader nodes
N = 3F + 1,
indexed by i ∈ {1, ..., n}, where leaders
can tolerate up to F Byzantine leaders. Leaders maybe in
multiple geographical locations and have both different speeds
and different physical machines. All
leaders communicate
and synchronize by sending and receiving messages through
reliable channels. A correct leader follows its speciﬁcation
while Byzantine leaders control by an adversary and behave ar-
bitrary including sending wrong messages and colluding with
each other to harm the system. A computationally bounded
adversary can control the faulty nodes to compromise the
system if more than F compromised.

BigBFT protocol runs in rounds and each round consists of
two phases called propose and vote phases. We assume that at
the beginning of a round, every leader knows all other leaders
in the round. The coordination phase is not on the critical
path of the protocol. In each round, we assumes a unique
coordinator is known to every leader for every round. The
coordinator node can be one of the leaders. We also assume a
round-robin rotation for coordinator elections as in many BFT
protocols [3]–[5], [12]–[14].

PBFT has a complicated view-change sub-protocol with a
quadratic message complexity. Chain based protocols [3]–[5],
[24]–[26] have emerged recently that simplify view-change
to have linear message complexity. However,
sub-protocol
chain based protocols requires sequential order to propose
and commit proposals which affect concurrency executions,
which lead to low resource utilizations. Fortunately, BigBFT
coordination phase is not on the critical path of the protocol
and both BigBFT and its coordination phase are working in
parallel to maximize resource utilizations.

B. Communication Model

Following common practice in the literature, we assume a
partial synchrony communication model in our protocol [20],
as most BFT protocols of the same kind [2], [4], [5], [7], [27],
where there is a known network delay bound δ that will hold
after an unknown Global Stabilization Time (GST). After GST,
all messages between honest leaders will arrive within time δ.
When an honest leader sends a message in round r, an honest
recipient leader is guaranteed to receive it by the beginning
of round (r + δ). Although we assume partial synchrony,
the protocol achieves consistency (i.e., safety) regardless of
how long the message delays are or how badly the network
might be partitioned. The protocol executes in parallel with
a linear communication complexity over N nodes. Also, the
protocol maintains responsiveness which proceeds as network
delivers [4], [28].

C. Cryptographic Primitives

In this part, we introduce the cryptographic algorithms used
in BigBFT. We take the advantages of existing cryptographic
tools that are available. Similar to many BFT protocols [4],

[7], [11], [29], we assume standard digital signatures and
public-key infrastructure (PKI) that identify all leader and
client processes. The BigBFT’s message exchange patterns
combined with some cryptographic primitives to create digital
signatures. BigBFT uses a signature aggregation scheme [30]
that reduces the message complexity [31] and enables leaders
to convert a set of signatures into a single signature; however,
this can only happen when the set contains threshold value
which is N − F in BigBFT.

The scheme allows leaders to receive N − F partial
signatures σi = signi(Bj) from every leader i for ev-
ery block Bj, and combine them into a single signature
σ =AggSign(signi(Bj) i∈N and j∈Bj ). The leader then ag-
gregate all σ in a single signature AggQC.

BigBFT uses message digest

to detect corrupted mes-
sages. Both clients and leaders must be able to verify each
other’s leader public key and messages. We assume that all
cryptographic techniques cannot be broken. We also assume
cryptographic hash function H(.) that maps arbitrary input to
a ﬁxed size output. We assume the hash is collision resistant
where is no H(x) == H(y).

IV. BIGBFT PROTOCOL

A. Overview

BigBFT protocol has a designated coordinator Cr that has
chosen in a round-robin fashion for leading the round r, man-
aging the leader set Ls, and partitioning the set of sequence
numbers Z between leaders to avoid conﬂicts between leaders,
P artitionki ← Z/Ls. The P artitionk∈Z and i∈Ls means a
partition k assigned to a leader i from the leader set Ls.

BigBFT executes in rounds r = [0, +∞) where each
round has a dedicated leader set and a coordinator known
to all. The communication patterns when the network is
synchronous, no byzantine failures, and no contention, are
two communication phases in a normal-case: (i) leaders pro-
pose client requests in parallel(Phase-1). (ii) vote on client
requests in parallel(Phase-2). In Figure 1, we illustrate the
communication ﬂow of BigBFT protocol. The BigBFT would
only perform coordination phase(round-change) after many
consensus instances in parallel with BigBFT protocol(Phase-
1 and Phase-2). Coordination phase invokes to replace a

Figure 1: Communication pattern of BigBFT

Byzantine coordinator, a leader set, and a new partition of
the sequence numbers Z.

Any full node in the system have two roles called coordina-
tor (C) and leader (L). C is responsible for leading the round
and partitioning the sequence number space. L is responsible
to receive a partition space, proposing blocks and voting on
blocks. These roles can be co-located, that is, a single process
can be a coordinator or a leader.

Clients submit their requests to F + 1 leaders in the system
that are responsible to handle the client request. In this case,
non-faulty process can learn the request if the faulty leader
tries to prevent that request from being in the next proposal.
BigBFT’s client can send many independent requests on the
ﬂy to the leaders (cid:104)Request, t, O, id(cid:105), where t is the timestamp,
O is the operation, id is the client id. The client will receive
(cid:104)Reply, r, t, L(cid:105), where r is the round number and L is the
leaders identiﬁcations who executed the client request.

B. Data Structures

This part introduces message types and the structure of the
block in BigBFT before presenting the details of the protocol.
As shown in the Algorithm 1, RChange is the round change
message to change the current round that carries a new round
number r = r+1 where r is the current round, space partitions
Z, and a leader set Ls.

P repare message in the Algorithm 2 carries the digest
message of block Bj, the sequence number sn, the round
r, and the previous round AggQCr−1. The leaders always
use AggQCr−1 to proof the last blocks commit to propose a
new block. V ote message in the Algorithm 2 carries a set of
partial signatures of blocks for Bj where j ∈ Bj. An aggregate
quorum consists of set of QCs that each QC contains N − F
signed votes for a block from distinct leaders.

C. Coordination Phase

We design the coordination phase as a separate phase of
BigBFT to avoid any leader bottlenecks. The coordination

Algorithm 1: Coordination phase of BigBFT

Algorithm 2: BigBFT protocol

(cid:46) Coordination phase
foreach r ← 0, 1, 2... do

if Ci is coordinator then

RC ← RoundM sg(RChange, Z, r, L)
Broadcast RC

if Li is leader then

if RC is received from Ci then
if ( r >= local r ) then
σi = Sign(Ack, ski)
// sk = secret key of leader i
Send σi to Ci

if Ci received (N-F σi where i ∈ N ) in r then
RoundQC ← combine N-F σi where i ∈ N
broadcast RoundQC to N-1

if (Li receives an RoundQC from Ci) then

Call Algorithm 2

phase partitions the sequence space across replicas in r and
prepares leader set Ls for next round r + 1. In each round, the
new coordinator increases the round number by one and starts
the round, Ci+1 ← next.Ci. As described in Algorithm 1, the
coordinator Ci+1 sends round-change RChange message to
all leaders. Each leader receives and processes the RChange
its state. Then, each leader signs the coordinator
against
message RChange and sends the reply back to coordinator.
Upon receiving N − F replies from leaders, the coordinator
creates RoundQC quorum and broadcasts it to all leaders
in round r. The coordination phase executes in parallel with
BigBFT IV-D of previous round r − 1. After the execution
of coordination phase, each leader will have a designated
partition of sequence numbers assigned by the coordinator.

D. BigBFT protocol

After partitioning the set of sequence numbers across lead-
ers in coordination phase, BigBFT starts two communication
phases called prepare and vote. Below we describe how
prepare and vote phases works.

Prepare Phase. Upon receiving the block in round r, each
leader assigns the next available sequence number to the new
block. The leader also attaches the proof of vote AggQC from
previous round r − 1 in prepare message. Then, in prepare
phase, each leader proposes the prepare message to all leaders.
In normal path, the leaders receive N − F proposed blocks
that do not conﬂict with any other blocks. Every leader checks
the AggQCr−1 in order to commit the blocks from previous
round r − 1. If the leader is faulty, the BigBFT guarantees that
the faulty leader affect only its process and pending blocks go
to other honest leaders.

Vote Phase. Upon receiving N − F proposed blocks, the
leader signs each block and sends the vote message to all
leaders. To reduce the message complexity in the vote phase,
instead of sending vote messages N − F times, we combine

for each r ← 0, 1, 2... do

(cid:46) Prepare phase of Li
(cid:10) waits for Bj
li
if Bj == valid then

(cid:11) from client Then

msg = P ropose(prepare, AggQCr−1, Bj)
broadcast(msg, d(Bj), r)
(cid:46) Li receives prepare Msgs
if receives N-F prepare msgs then

// Same block cannot be assign to more

than one leader

while r = true do

if Bj.Li == Bj.Li + 1 then

r ← False

if AggQCr−1 == true then

((N − F )Bj ∈ r − 1) ← committed

(cid:46) Send Vote Phase of Li
if Li received (N-F Bj where j ∈ N ) in r then
j=1 σj = Signi(Bj)

{σ} = (cid:83)N −F
broadcast Vote(vote, {σ} , r)

(cid:46) Li receives Vote Msgs
QC(Bj) ← (cid:83)N −F
AggQCr ← (cid:83)N −F

i=1 σi
i=1 QCi

all vote messages in a single vote set {votes} and send it to
all leaders. Each vote message represents a partial signature
that signed by replica i for a block. Because we have N − F
blocks, we need to combined N − F signatures {σ} in one
vote message.

We describe the BigBFT protocol as follow.

1) Client nodes broadcast their blocks/requests to F + 1

leaders including the coordinator.

2) Upon receiving new block proposal from clients, every
leader veriﬁes the block, assigns sequence number to the
new block, and attaches the proof of vote AggQC from
previous round in the prepare message.

receiving

3) Leader broadcasts prepare message to all other leaders.
commit
4) Upon
AggQCr−1,
leaders commit previous round blocks.
Leaders sign each proposal message and combine all
signatures in a vote set {votes}.

prepare messages

and

5) Upon receiving the N − F vote set messages, leader
create a quorum certiﬁcate for every block and combined
all quorum certiﬁcate in aggregated quorum certiﬁcate
AggQC, Algorithm 2.

6) In the next round r + 1, leader nodes check the new pro-
pose blocks. If the prepare message carries the AggQCr,
then it commit the blocks and reply to the client. This is
what we have called across rounds pipelining.

Importantly, the entire protocol follows a uniﬁed coordination-
prepare-vote paradigm. The coordination protocol is pipelined
and not on the critical path of BigBFT.

V. BIGBFT CORRECTNESS

We prove that BigBFT achieves both safety and liveness
properties by showing BigBFT algorithm solves agreement,
validity, and termination in all possible distributed executions.
We also performed model checking of the high level BigBFT
protocol in T LA+ [32]. The speciﬁcation is available on the
GitHub.1

A. Safety

BigBFT guarantees its safety in any circumstances re-
gardless of the network delays and partitions. If there are
less than F < N
3 Byzantine leaders and N − F honest lead-
ers decide on blocks Bj∈(j,j+N −1) at blockchain height
{h where h = h to h = h + N − 1},
leader
will decide on any blocks other than Bj.

then no honest

Lemma 1 If we have AggQC1 and AggQC2 with F < N
3 ,

then both AggQCs are not on the same round r.

P roof. Suppose that blocks from Bj

to Bj+N −1 are
committed in AggQC1 at round r and blocks from Bj+N to
Bj+2N −1 are committed in AggQC2 at round r(cid:48). The number
of blocks are determined by the number of active leaders who
propose blocks in parallel. It must be that at least N −F leaders
denoted as S0, signed the block Bj∈(j,N −1), and at least N −F
leaders denoted as S1, signed the blocks Bj∈(j+N,j+2N −1).
Since there are only N leaders in total, S0 ∩ S1 must intersect
in at least 1
3 , and thus at least one honest leader is in S0 ∩ S1.
According to our protocol, every honest leader votes for at
most one time for each height in the blockchain. Therefore, it
must be that r (cid:54)= r(cid:48) and AggQC1 is a preﬁx of AggQC2.

Lemma 2 If at least one correct leader has received N −
F votes for block Bj in round r, then if some leaders have
increased their round numbers due to network partitions or
node failures, the round r is the last round that have the latest
valid votes before proposing the next round.

P roof. The leaders receive client’s request in round r. The
leader also have N − F votes from each request in previous
round r−1. When each leader proposing both the new requests
and the AggQC for r − 1 in round r, the leaders receive
proofs for the last blocks to be committed. If there is a correct
replica that received N − F votes for r, then it means that
there are N − F replicas sent their votes in the same round.
Then, after network partitions or node failures, some replicas
increase their rounds number r + k. However, the round r is
still the last round that have the parent blocks for the next
round. For simplicity, we assume the leader li+1 is in r and
has proposed Bj+2. A leader, say li+2, received a N −F votes
for Bj+1. The latest valid votes in leader li+1 should be Bj+1
in r. This is because the leaders cannot accept any proposal
without the proof of the previous block votes.

B. Liveness

BigBFT guarantees its liveness under

the partial syn-
chronous model. After GST, when network conditions are
good, the network is stable and delay time is known. If there

1https://github.com/salemmohammed/BigBFT/tree/main/tla

are less than F<N
reach a decision in a δ time.

3 Byzantine leaders, the honest leaders can

Lemma 3 To ensure liveness, each round has a coordinator,
a set of leaders, and the round number is incremented. The
protocol after GST, with a correct coordinators and leaders,
eventually become synchronized and the block will be added
to the blockchain.

P roof. Let us assume that we have a correct coordinator
called C, and leaders Ls for round r at time t. This means that
Ls have received the coordinator message RoundQC that at
least N − F leaders signed in r where F < N
3 . Thus, at least
F + 1 correct leaders assign C as a coordinator at time t. By
time t + δ, all correct leaders complete the protocol phases.

Lemma 4 A new leaders set is chosen based on the their
stakes. If a leader does not make progress, the leader Li will
be discarded from the leader set Ls.

P roof. At the beginning of the round, the leaders is chosen
by coordination phase. As per assumption all correct leaders
are in the same round, therefore the correct leader will propose
a prepare message with proof containing the latest votes
AggQC. Since all leaders are in the same round, therefore
leaders who are not successfully complete prepare and vote
will eliminate from the Ls.

VI. PERFORMANCE EVALUATION

A. Implementation and setup

We implemented BigBFT in Go using PaxiBFT frame-
work [33], which uses core network ﬁles from Paxi [34].
PaxiBFT is an open source for prototyping, evaluating, and
benchmarking BFT consensus and replication protocols. As
shown in Figure 2, PaxiBFT readily provides most func-
tionality that any coordination protocol needs for replication
protocols. The entire protocol is available as open source at
https://github.com/salemmohammed/BigBFT.

We evaluate the performance of our BigBFT prototype
protocol and compare it with PBFT [2], Tendermint [3],
Streamlet [5], and HotStuff [4], using the Amazon EC2
instances. We chose both Wide Area Network(WAN) across
5 AWS regions(Ohio, N.California, Oregon, N.Virginia, and
Canada) and Local Area Network(LAN).

We deployed BigBFT on up to 20 m5a.large virtual ma-
chines, each of which has 2 vCPU, 8GiB RAM, and 10Gbps
network throughput. To ensure that the client performance
does not impact the results, we used the larger m5a.xlarge
instances with 4 vCPUs for the clients. Based on our ex-
periments results,
the network size is appropriate to state
and conclude our ﬁndings. To push system throughput, we
varied the number of clients up to 90 clients and used a
small message sizes. In our experiments, message size did not
dominate consensus protocols performance, but the complexity
of consensus protocols dominates the performance.

We compare the performance of protocols when F = 0
and N ranges from 4 to 20 full nodes. The results are shown
in Figures 3 and 5. In each graph, the Y-axis shows the
throughput in tx/sec, and X-axis the number of nodes(N).
We deﬁne the throughput as the number of transactions per

second (tx/s for short) that leaders commit. As we can see in
both Figures 3 and 5, BigBFT achieves a better performance
than PBFT, Tendermint, and Streamlet in LAN deployment.
In a comparison to HotStuff, BigBFT is close to Hotstuff’s
throughput but has better latency. This is because, in both wide
and local environment, the network is the bottleneck and the
message patterns of BFT protocols, namely PBFT, Tendermint,
and Streamlet, tend to be expensive. BigBFT on the other hand
maintains a low number of message latency for each request
to be committed and simple message patterns.

The latency messages are 4 messages. However, Hotstuff
has 10 messages regardless of chain protocol as we summa-
rized in Table I. These latency messages are not important in
LAN due to the short distance between nodes. As a result,
BigBFT performs well in LAN deployment as you can see in
Figure 3. In contrast to LAN, there is a long delay in WAN
between nodes that helps BigBFT due to its low number of
message latency to commit the requests faster. Compared with
PBFT, Tendermint, Streamlet, and Hotstuff, BigBFT puts less
stress on the leaders. Moreover, the performance of BigBFT
is very close to that of HotStuff and Paxos [8].

Hotstuff has point to point communication while BigBFT is
still using broadcasting topology between nodes even though
we amortized the messages over the number of requests.
The combination of messages in the vote phase causes some
delay, but
that delay was unnoticed in WAN due to the
distance/latency gain from sending messages in parallel.

Figure 3: Throughput comparison in LAN

Figure 4: WAN’s throughput comparison in Virginia,
California, Oregon, Ohio, and Canada

Figure 2: The PaxiBFT architecture

B. Performance comparison between single and multi-leaders
protocols

To study the performance of BigBFT, we compare BigBFT
latency and throughput with PBFT [2], Tendermint [3], Hot-
stuff [4], and Streamlet [5]. We run all of these protocols
with no faults to make sure we capture their absolute best
performances. We chose these protocols to compare with
BigBFT because we had studied and analyzed them in our
previous work [8]. As a result of our previous work, we
introduced BigBFT to alleviate the bottlenecks. BigBFT relies
on a two phase common case commit protocol with 3F + 1
replicas illustrated in Figure 1. To provide a fair comparison,
all protocols implemented on the same framework PaxiBFT 2.

Figure 5: Latency comparison in LAN

Figures 6 and 7 illustrate the latency versus throughput
performance of these ﬁve protocols in a 20-node cluster. In
each graph, the X-axis shows the throughput in tx/sec, and Y-
axis the latency in ms. BigBFT and HotStuff did not saturated
very quickly in Figure 7. Pushing the system throughput to
its limit is difﬁcult in WAN. In LAN, pushing the system
throughput to its limit to get the system bottlenecks is easy
due to the short network pipe between instances.

At this cluster size, BigBFT shows better latency than other
protocols because it allows parallel executions. This removes
the single leader bottleneck and allows BigBFT to have more
throughput than many protocols. On the other hand, PBFT,
Tendermint, and Streamlet are signiﬁcantly limited by their
increased communication costs.

PBFT is limited by a single leader and quadratic exchanging
messages for every request. PBFT gets saturated quickly and
reaches its limit of around 480 requests per second in LAN and
360 requests per second in WAN. While BigBFT have higher
latency in LAN, but we see that it scales to higher throughput
than other protocols except Hotstuff with better latency than
all protocols. BigBFT provides great throughput improvements
over traditional PBFT in wide area deployments.

BigBFT maintains low latency for much higher levels of
throughput and shows higher throughput
than Tendermint
and Streamlet in WAN. Tendermint gets saturated quicker by
a more complicated messaging and processing despite our
workload having no conﬂicts.

C. System Payload Size

Payload size have an impact on the communication perfor-
mance of the system. With the large messages, system requires
more resources for serialization and more network capacity
for transmission. To study how different payload size impacts
the performance of our studied protocols and BigBFT, we
experiment with 16 node clusters. We measured the maximum
throughput on each system under a write-only workload by 90
clients. Figure 8 shows the maximum throughput of BigBFT
and Hotstuff at payload sizes varying from 128 to 2048 bytes.
While BigBFT show less throughput than HotStuff at the
beginning of payload sizes, both protocols exhibit a similar
relative level of degradation as the payload size increases.

Figure 6: System throughput and the latency on 20-node
LAN cluster

Figure 7: System throughput and the latency on 20-node
WAN cluster in Virginia, California, Oregon, Ohio, and
Canada

Figure 8: Maximum throughput at various payload sizes

VII. ANALYSIS AND DISCUSSION

In this section, we compare the strengths and weaknesses
of the BigBFT and provide back-of-the-envelope calculations
for estimating the latency and throughput performance. Table I
provides a synopsis of the blockchain protocols characteristics
we compared with BigBFT. We elaborate on these next.

Time Complexity. In Mir-BFT, and PBFT,

the normal
executions have a quadratic complexity. When the leader is
a malicious, the protocol changes the view with a different
leader using a view-change which contains at least 2F + 1
signed messages. Then, a new leader broadcasts a new-view
message including the proof of 2F + 1 signed view-change
messages. Leaders will check the new-view message and
broadcast it to have a match of 2F + 1 new-view message.
The view-change has then O(N 3) complexity and O(N 4) in
a cascading failure.

Tendermint reduces the message complexity that is caused
by view-change in PBFT, to a total O(N 3) messages in the
worst case. Since at each epoch all leaders broadcast messages,
it happens that during one epoch the protocol uses O(N 2)
messages. Thus, in the worst case scenario when there is F
faulty leaders, the message complexity is O(N 3) [3].

Streamlet has communication message complexity O(N 3).
Streamlet loses linear communication complexity due to all-to-
all communication in vote message. In the worst case scenario

Critical path
Normal Message Complexity
Multiple View Change
Responsive

PBFT [2]
5
O(N 2)
O(N 4)
Yes

HotStuff [4] Mir-BFT [12]

10
O(N )
O(N 2)
Yes

5
O(N 2)
O(N 4)
Yes

Streamlet [5]
4
O(N 3)
O(N 4)
No

Tendermint [3]
5
O(N 2)
O(N 3)
No

BigBFT(This paper)
4
O(N )
O(N )
Yes

Table I: Characteristics of BFT consensus protocols

when there is a leader cascading failure, the Streamlet message
complexity is O(N 4).

HotStuff all have linear message complexity. The worse case
communication cost in these protocols is O(N 2) considering
worst-case consecutive view-changes.

BigBFT has communication message complexity O(N ).
BigBFT has a linear communication complexity in vote phase
in the best case scenarios. In the worst case scenario when
there is a leader cascading failure,
the BigBFT message
complexity does not change because overlapping between
propose-vote phases and coordination phase.

A. Load and Capacity

BigBFT protocol reaches consensus once a quorum of
participants agrees on the same decision. A quorum can be
deﬁned as sets containing N −F majority leaders in the system
with every pairs of set has a non-empty intersection. To select
quorums Q, quorum system has a strategy S in place to do
that. The strategy leads to a load on each validator. The load
(cid:96)(S) is the minimum load on the busiest leader. The capacity
Cap(S) is the highest number of quorum accesses that the
system can possibly handle Cap(S) = 1

(cid:96)(S) .

In single leader protocols, the busiest node is the leader. In

BigBFT, all nodes are busy all the time.

(cid:96)(S) =

1
L

(Q − 1)N umQ + (1 −

1
L

)(Q − 1)

(1)

where Q is the quorum size chosen in both leader and fol-
lowers, NumQ is quorums number handled by leader/follower
for every block request, and L is the number of operation
leaders. There is a 1
L chance the node is the leader of a request.
Leader communicates with N − 1 = Q nodes. The probability
of the node being a follower is 1− 1
L , where it only handles one
received message in the best case. In the equations below, we
present the simpliﬁed form of BigBFT and PBFT protocols,
and calculate the result for N = 9 leaders. The protocols
perform better as the load decreases.

(cid:96)(BigBF T ) = 5

5
9

(2)

In BigBFT protocol, equation 2 with N leaders, and L = N,
quorum size Q = (cid:98) 2N
3 (cid:99), and number of quorums N umQ = 2.
Many requests R have overlapped in Q and N umQ due to
parallel executions. Therefore, we divide Q and N umQ over
number of requests R which equals to number of active leaders
N = R. PBFT is a single leader protocol with Q = (cid:98) 2N
3 (cid:99) and
N umQ = 2. The load on PBFT is (cid:96)(P BF T ) = 10

B. Latency

The formula 3 calculates the latency of BigBFT.

Latency(S) = Critical P ath + DL + δ

(3)

The critical path denotes the number of one-way message
delays. BigBFT’s critical path has a 4-message delay as
illustrated in Table I for multiple consensus. DL is the round
trip message between a client and designated leader. In Table I,
PBFT has a 5-message delay for single consensus.

VIII. FUTURE WORK

A. Improving Scalability by Enabling Local Communication

One direction for future work is to enable hierarchical
communications in order to scale the protocol to more nodes.
At each round, a global coordinator selects a local coordinator
in each region to maintain the conﬁguration blockchain and
to restrict the communication within each region. The global
coordinator will reach consensus with local coordinators on
sharding the space between regions. Local coordinator will
further shard the space among all local leaders. Then, each
region will run BigBFT locally among nodes to order blocks.

B. Client as a Coordinator

We are planning to enable client node to act as a system
coordinator to further improve BFT scalability by reducing
the interactions between replicas. The client can assign the
request’s sequence number, choose a set of trusted replicas
to form a quorum, and learn the status of the request from
the chain if committed. For instance, the client may choose a
quorum of nodes including well-known validators, as shown
in Stellar protocol [35], with the high stake values to commit
its blocks. Client may use blockchain to check recent honest
validators that are able to commit and add to the chain. For
more ﬂexibility, the client might develop some set of policies
to choose its quorum such as choosing common nodes between
recent last two committed blocks. However, the system should
have a security mechanism to prevent a malicious client from
violating the safety of the system.

IX. CONCLUDING REMARK

We have presented BigBFT, a BFT-based consensus pro-
tocol for blockchains. BigBFT alleviates the communication
bottlenecks in single leader BFT protocols by enabling multi-
leader executions, and reduces the number of communication
phases to be only two for reaching consensus on proposed
blocks. This is achieved by pipelining blocks both within a
round and across rounds. BigBFT also decouples the coordina-
tion phase from the main protocol and pipelines coordination
round with consensus rounds to improve the system perfor-
mance. We analyzed BigBFT performance by using a load

[24] E. Buchman, “Tendermint: Byzantine fault
blockchains,” Ph.D. dissertation, 2016.

tolerance in the age of

[25] V. Buterin and V. Grifﬁth, “Casper the friendly ﬁnality gadget,” arXiv

preprint arXiv:1710.09437, 2017.

[26] E. Shi, “Streamlined blockchains: A simple and elegant approach (a
tutorial and survey),” in International Conference on the Theory and
Application of Cryptology and Information Security.
Springer, 2019,
pp. 3–17.

[27] T.-H. H. Chan, R. Pass, and E. Shi, “Pala: A simple partially syn-
chronous blockchain.” IACR Cryptol. ePrint Arch., vol. 2018, p. 981,
2018.

[28] K. Hu, K. Guo, Q. Tang, Z. Zhang, H. Cheng, and Z. Zhao, “Don’t
count on one to carry the ball: Scaling bft without sacriﬁng efﬁciency,”
arXiv preprint arXiv:2106.08114, 2021.

[29] I. Abraham, G. Gueta, and D. Malkhi, “Hot-stuff the linear, optimal-
resilience, one-message bft devil,” CoRR, abs/1803.05069, 2018.
[30] D. Boneh, C. Gentry, B. Lynn, and H. Shacham, “Aggregate and
veriﬁably encrypted signatures from bilinear maps,” in International
conference on the theory and applications of cryptographic techniques.
Springer, 2003, pp. 416–432.

[31] C. Berger and H. P. Reiser, “Scaling byzantine consensus: A broad
analysis,” in Proceedings of the 2nd Workshop on Scalable and Resilient
Infrastructures for Distributed Ledgers, 2018, pp. 13–18.

[32] L. Lamport, Specifying systems. Addison-Wesley Boston, 2002, vol.

388.

[33] S. Alqahtani, “Paxibft,” https://github.com/salemmohammed/PaxiBFT,

2019.

[34] A. Ailijiang, A. Charapko, and M. Demirbas, “Paxi framework,” https:

//github.com/ailidani/paxi, 2018.

[35] D. Mazieres, “The stellar consensus protocol: A federated model for

internet-level consensus.”

formula and compared it with PBFT, Streamlet, Tendermint,
and Hotstuff protocols. Our experimental evaluations show
BigBFT’s advantages in latency or throughput over other
protocols.

X. ACKNOWLEDGEMENTS

This project is in part sponsored by the National Science

Foundation (NSF) under award number CNS-2008243.

REFERENCES

[1] S. Nakamoto, “Bitcoin: A peer-to-peer electronic cash system,?

http://bitcoin.org/bitcoin.pdf.”

[2] M. Castro, B. Liskov et al., “Practical byzantine fault tolerance,” in

OSDI, vol. 99, no. 1999, 1999, pp. 173–186.

[3] E. Buchman, J. Kwon, and Z. Milosevic, “The latest gossip on bft

consensus,” arXiv preprint arXiv:1807.04938, 2018.

[4] M. Yin, D. Malkhi, M. K. Reiter, G. G. Gueta, and I. Abraham, “Hot-
stuff: Bft consensus with linearity and responsiveness,” in Proceedings
of the 2019 ACM Symposium on Principles of Distributed Computing.
ACM, 2019, pp. 347–356.

[5] B. Y. Chan and E. Shi, “Streamlet: Textbook streamlined blockchains.”

2020.

[6] M. M. Jalalzai, J. Niu, and C. Feng, “Fast-hotstuff: A fast and resilient

hotstuff protocol,” arXiv preprint arXiv:2010.11454, 2020.

[7] G. Golan Gueta, I. Abraham, S. Grossman, D. Malkhi, B. Pinkas, M. K.
Reiter, D.-A. Seredinschi, O. Tamir, and A. Tomescu, “Sbft: A scalable
and decentralized trust infrastructure,” arXiv e-prints, pp. arXiv–1804,
2018.

[8] S. Alqahtani and M. Demirbas, “Bottlenecks in blockchain consensus

protocols,” arXiv preprint arXiv:2103.04234, 2021.

[9] F. Gai, A. Farahbakhsh, J. Niu, C. Feng,

I. Beschastnikh, and
H. Duan, “Dissecting the performance of chained-bft,” arXiv preprint
arXiv:2103.00777, 2021.

[10] A. Ailijiang, A. Charapko, and M. Demirbas, “Dissecting the perfor-
mance of strongly-consistent replication protocols,” in Proceedings of
the 2019 International Conference on Management of Data, 2019, pp.
1696–1710.

[11] Y. Yang, “Linbft: Linear-communication byzantine fault tolerance for

public blockchains,” arXiv preprint arXiv:1807.01829, 2018.

[12] C. Stathakopoulou, T. David, and M. Vukoli´c, “Mir-bft: High-throughput

bft for blockchains,” arXiv preprint arXiv:1906.05552, 2019.

[13] Z. Avarikioti, L. Heimbach, R. Schmid, and R. Wattenhofer, “Fnf-
bft: Exploring performance limits of bft protocols,” arXiv preprint
arXiv:2009.02235, 2020.

[14] S. Gupta, J. Hellings, and M. Sadoghi, “Rcc: Resilient concurrent
consensus for high-throughput secure transaction processing,” in Int.
Conf. on Data Engineering (ICDE). IEEE, 2021.

[15] C.-S. Barcelona, “Mencius: building efﬁcient replicated state machines
for wans,” in 8th USENIX Symposium on Operating Systems Design and
Implementation (OSDI 08), 2008.

[16] F. B. Schneider, “Implementing fault-tolerant services using the state ma-
chine approach: A tutorial,” ACM Computing Surveys (CSUR), vol. 22,
no. 4, pp. 299–319, 1990.

[17] L. Lamport, R. Shostak, and M. Pease, “The byzantine generals prob-
lem,” in Concurrency: the Works of Leslie Lamport, 2019, pp. 203–226.
[18] M. Pease, R. Shostak, and L. Lamport, “Reaching agreement in the
presence of faults,” Journal of the ACM (JACM), vol. 27, no. 2, pp.
228–234, 1980.

[19] M. J. Fischer, N. A. Lynch, and M. S. Paterson, “Impossibility of
distributed consensus with one faulty process,” Journal of the ACM
(JACM), vol. 32, no. 2, pp. 374–382, 1985.

[20] C. Dwork, N. Lynch, and L. Stockmeyer, “Consensus in the presence
of partial synchrony,” Journal of the ACM (JACM), vol. 35, no. 2, pp.
288–323, 1988.

[21] M. Ben-Or, “Another advantage of free choice (extended abstract)
completely asynchronous agreement protocols,” in Proceedings of the
second annual ACM symposium on Principles of distributed computing,
1983, pp. 27–30.

[22] J. Kwon and E. Buchman, “Cosmos: a network of distributed ledgers

(2016),” URL https://cosmos. network/whitepaper, 2016.

[23] Facebook, “Libra framework,” https://github.com/libra/libra, 2018.

