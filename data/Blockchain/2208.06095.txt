A Fast Blockchain-based Federated Learning
Framework with Compressed Communications
Laizhong Cui, Senior Member, IEEE, Xiaoxin Su, Student Member, IEEE, Yipeng Zhou∗, Member, IEEE

1

2
2
0
2

g
u
A
2
1

]

G
L
.
s
c
[

1
v
5
9
0
6
0
.
8
0
2
2
:
v
i
X
r
a

federated

Abstract—Recently,

learning
blockchain-based
(BFL) has attracted intensive research attention due to that
the training process
is auditable and the architecture is
serverless avoiding the single point failure of the parameter
server in vanilla federated learning (VFL). Nevertheless, BFL
tremendously escalates the communication trafﬁc volume because
all
local model updates (i.e., changes of model parameters)
obtained by BFL clients will be transmitted to all miners for
veriﬁcation and to all clients for aggregation. In contrast, the
parameter server and clients in VFL only retain aggregated
model updates. Consequently, the huge communication trafﬁc
in BFL will
inevitably impair the training efﬁciency and
hinder the deployment of BFL in reality. To improve the
practicality of BFL, we are among the ﬁrst to propose a fast
blockchain-based communication-efﬁcient
federated learning
framework by compressing communications in BFL, called
BCFL. Meanwhile, we derive the convergence rate of BCFL
with non-convex loss. To maximize the ﬁnal model accuracy,
we further formulate the problem to minimize the training loss
of the convergence rate subject to a limited training time with
respect to the compression rate and the block generation rate,
which is a bi-convex optimization problem and can be efﬁciently
solved. To the end, to demonstrate the efﬁciency of BCFL, we
carry out extensive experiments with standard CIFAR-10 and
FEMNIST datasets. Our experimental results not only verify
the correctness of our analysis, but also manifest that BCFL
can remarkably reduce the communication trafﬁc by 95-98%
or shorten the training time by 90-95% compared with BFL.

Index Terms—Federated Learning, Blockchain, Compression,

Convergence

I. INTRODUCTION

To mitigate the rising concern on data privacy leakage, the
federated learning (FL) paradigm emerges which can conduct
model training without touching raw data samples residing on
decentralized clients [1]. In vanilla FL (VFL), a parameter
server (PS) assists all clients in model aggregation. In each
round of global iteration, the PS will distribute the latest
model to selected clients before clients update the model by
conducting a few number of local iterations with their local
datasets. Then, updated model updates (i.e., changes of model

Manuscript received February 7, 2022; revised June 11, 2022; accepted
June 30, 2022. This work has been partially supported by National Key
R&D Program of China under Grant No.2018YFB1800302, National Natural
Science Foundation of China under Grant No.61772345, Shenzhen Science
and Technology Program under Grant No. RCYX20200714114645048, No.
JCYJ20190808142207420 and No. GJHZ20190822095416463.
L. Cui, X. Su, are with the College of Computer Science and Soft-
ware Engineering, Shenzhen University, Shenzhen 518060, China (email:
cuilz@szu.edu.cn; suxiaoxin2016@163.com).
Y. Zhou is with the School of Computing, FSE, Macquarie University,
Macquarie Park, 2113, NSW, Australia (email: yipeng.zhou@mq.edu.au).
Y. Zhou is the corresponding author.

parameters) are collected and aggregated by the PS. Multiple
global iterations are conducted by involving different clients
until the model converges [2], [3].

Although, VFL can prevent the disclosure of original data
samples, it confronts the following two challenges in that VFL
is highly dependent on the honesty and reliability of clients
and the PS. First, the PS needs to communicate with multiple
clients simultaneously, which probably chokes the PS such
that the training process halts [4]. Second, if the PS or clients
maliciously tamper model updates or injecting forged samples,
the model accuracy can be signiﬁcantly compromised [5].

To overcome the deﬁciencies of VFL, blockchain-based fed-
erated learning (BFL) was devised by [6]. On one hand, BFL
is more robust which is invulnerable to the failure of a single
point [7]. On the other hand, all intermediate model updates
generated by clients are auditable and traceable which can ef-
fectively prohibit adversarial behaviours [8]. For instance, Kim
et. al. designed BlockFL [6] with a decentralized FL training
mode combined with blockchain. Pokhrel et. al. [4] studied
how to use channel dynamic to minimize blockchain-based
FL training delays in autonomous vehicle scenarios. Feng et.
al. proposed BAFL [9] to ensure the security and efﬁciency
of federated learning through blockchain and asynchronous
training.

Despite these advantages, BFL suffers from the long com-
munication latency and the low training efﬁciency because of
its huge communication trafﬁc, which can be attributed to: 1)
In BFL, all intermediate model updates will be broadcasted
to all miners to maintain the blockchain; 2) Each client needs
to download all intermediate model updates to locally conduct
model aggregation. In contrast, only aggregated model updates
are maintained by the PS and downloaded by clients in VFL,
implying that the communication load of VFL is much lighter
than that of BFL. It was reported in [10] that the communica-
tion trafﬁc of BFL is too heavy making BFL impracticable in
real systems. Several existing works have attempted to prohibit
communication overhead in BFL. Xuan et. al. [11] proposed
a communication cost optimization method by proposing the
veriﬁcation mechanism to exclude useless or malicious nodes
to reduce communication cost in BFL. In [12], Wilhelmi et.
al. proposed an analytical model based on batch service queue
theory to implement asynchronous training in BFL in order to
reduce the block size and optimize communication latency.
Yet, compressing model updates to alleviate the communica-
tion load of BFL has not been explored by existing works.

In light of inefﬁcient communications in BFL, we are among
the ﬁrst to propose a fast blockchain-based communication-
efﬁcient federated learning framework with compressed com-

 
 
 
 
 
 
munications, called BCFL. Intuitively speaking, the communi-
cation load in BFL is proportional to the size of intermediate
model updates contributed by clients. Hence, the communi-
cation efﬁciency of BFL can be substantially improved if we
can effectively shrink the population of intermediate model up-
dates by discarding ones inessential for model training. BCFL
leverages the T opk algorithm [13] (one of the most effective
compression algorithms in federated learning) to compress
local model updates on each client in order to alleviate the
communication load of BCFL. Speciﬁcally, each BCFL client
only injects k most signiﬁcant model updates into BCFL,
where k can be much smaller than the model dimension.
The signiﬁcance of each model update is determined by its
absolute value. A model update close to 0 is regarded as
training which only slightly
an inessential one for model
affects the aggregated model [14]. In addition, we prove
the convergence of BCFL with non-IID sample distribution
and non-convex loss. To maximize the ﬁnal model accuracy,
we formulate the problem to minimize the convergence rate
subject
to the
to a ﬁxed training time span with respect
compression rate and the block generation rate of BCFL. We
prove that this is a bi-convex optimization problem, which can
be solved efﬁciently. In the end, the extraordinary performance
of BCFL is demonstrated by experiments conducted with the
standard CIFAR-10 and FEMNIST datasets.

In a word, the contributions of our work are unfolded as

follows.

• To alleviate the communication load in BFL, we are
among the ﬁrst to propose the BCFL framework, which
employs the T opk algorithm to compress model updates.
In addition, We derive the convergence rate of BCFL
under non-IID sample distribution and non-convex loss.
• We explore how to optimally set the compression rate
and the block generation rate in BFCL. Given a ﬁxed
training time span, we formulate the problem to minimize
the loss function with respect to the compression rate
and the block generation rate as a bi-convex optimization
problem, which can be easily solved.

• We conduct comprehensive experiments to evaluate the
performance of BCFL under both IID and non-IID data
distributions with CIFAR-10 and FEMNIST datasets.
The experimental results manifest that BCFL can shrink
the communication trafﬁc by 95-98%, or shorten the
training time by 90-95% compared with BFL without
compression.

The rest of the paper is organized as follows. Relevant works
are discussed in Sec. II. Sec. III introduces the preliminary
knowledge of federated learning. Then BCFL framework is
elaborated in Sec. IV. The convergence rate of BCFL and
the optimization of model accuracy are analyzed in Sec. V.
Experimental results used to evaluate BCFL performance are
presented in Sec. VI. Ultimately, we conclude our work in
Sec VII.

II. RELATED WORK
In this section, we discuss related works from three aspects:
FL, combination of blockchain and FL, and model compres-
sion in FL.

2

A. Federated Learning

The federated average algorithm (FedAvg) [1] is the most
fundamental FL algorithm that can realize privacy preserved
distributed model training. To date, the convergence of FedAvg
algorithm has been analyzed in [2] with strongly convex loss
and [3] with non-convex loss, respectively. Later on, variants
of FedAvg are devised to accommodate constrained resources
on clients. Based on the convergence property of FL, Wang
et al. dynamically adjusted local training epochs on clients
to minimize the loss function under restricted computation
and communication resources [15]. FEDL [16] modeled the
resource allocation problem, and decomposed the obtained
non-convex problem to optimize the convergence of the model.
Hu et al. [17] designed a model segmented gossip approach
to achieve decentralized federated learning to fully utilize the
bandwidth among nodes. Luo et al. [18] designed a multi-
variable optimization problem by taking into account
the
learning time and energy consumption in FL model training,
in order to minimize the total loss.

B. Combination of FL and Blockchain

Blockchain is decentralized, non-tamperable, and highly
transparent, which can naturally suit federated learning. A
bunch of existing works have explored to combine blockchain
with FL towards building a more robust and reliable model
training architecture.

Biscotti [19] was proposed as a fully decentralized peer-to-
peer multi-party learning framework that ensures data security
and privacy with the assistance of blockchain. In addition, the
end-to-end delay of BFL is analyzed to derive the optimal
block generation rate. In [20], a hierarchical blockchain frame-
work with FL was proposed for knowledge sharing in large-
scale vehicular networks. In [21], Lu et. al. proposed to reduce
the communication latency of BFL by integrating digital twin
into wireless network to reduce unreliable communication
between users and servers. In [22],
the authors designed
a blockchain-based federated learning platform, which im-
plements gradient aggregation by using smart contracts to
reduce the risk of data privacy leakage. ChainsFL [23] was
proposed as a two-layer blockchain-driven FL framework to
deal with the problems of high resource consumption, limited
throughput, and high communication complexity in BFL. In
spite of these efforts targeting to improve BFL efﬁciency, the
communication load of BFL is still too heavy in practice.

C. Communication Compression in FL

Considering limited bandwidth resources and increasingly
complex machine learning models in FL, tremendous efforts
have been dedicated to improve the communication efﬁciency
of FL by compressing communications. Existing compression
algorithms can be generally divided into two types: sparsiﬁ-
cation and quantization.

Sparsiﬁcation compression algorithms ﬁlter model updates
to be transmitted and only retain important updates for trans-
mission. The DGC algorithm proposed in [24] discarded
99.9% of insigniﬁcant model updates for uploading on dis-
tributed machine learning workers, and different methods

were designed to maintain the performance of the model.
ClusterGrad [25] was based on clustering to adaptively ﬁlter
and quantify important gradients according to the distribution
of model updates. Li et al. [26] took into account the heteroge-
neous resources on devices in FL. An optimization algorithm
was proposed accordingly to adjust the compression rate of
each device and local computation in order to minimize the to-
tal energy consumption. DC2 [27] was a heuristically designed
adaptive compression scheme that can adjust the amount of
data to be transmitted according to the variation of network
delay. Sattler et al. proposed the STC algorithm [28] to sparse
the transmitted data and quantize the sparsed updates into two
discrete values during upload and download processes, which
thereby greatly reduces communication time. The quantization
based compression algorithms quantize model updates to be
transmitted to a number of discrete values with compromised
parameter precision. In [29], the proposed QSGD algorithm
generated a random number for each transmitted update, and
used it to map the update to a centroid in an unbiased manner.
PQ algorithm proposed in [30] divided the difference between
maximum and minimum values of model updates into equidis-
tant intervals, and randomly quantized the updates in each
interval into its upper or lower bound. In [31], the TernGrad
algorithm was designed to ternary model updates to improve
the convergence of the algorithm by layer-wise ternarizing
and gradient clipping. Cui et. al. proposed MUCSC [32] to
quantify model updates by analyzing the effect of compression
error on model convergence, which can correspondingly select
centroids to minimize the error.

Although compressing communications have been widely
discussed in FL, there is little effort to apply model compres-
sion techniques in BFL and this gap will be bridged by our
work.

III. PRELIMINARY

To pave the discussion of our BCFL framework, we ﬁrst
brieﬂy explain the FL training process with the most funda-
mental FedAvg algorithm. In FL, data samples are distributed
across multiple clients. Without loss of generality, we assume
that there are a total of N clients and the dataset on client i
is Di. The local loss function of client i is deﬁned as

Fi(w, Di) =

1
|Di|

(cid:88)

ξ∈Di

f (w, ξ),

(1)

where w represents model parameters, |Di| is the size of
local dataset, ξ is a particular data sample and f () is the loss
function of a particular machine learning task. The goal of FL
is to train a model that minimizes the global loss function,
i.e.,

min
w

F (w) = min

w

piFi(w, Di),

(2)

N
(cid:88)

i=1

where pi is the weight of client i, and it is usually deﬁned as
pi =

|Di|

(cid:80)N
(cid:48)
i

=1

|D
i

(cid:48) | .

The objective of FedAvg is to train the model to minimize
the loss deﬁned in Eq. (2). FedAvg consists of multiple global
the parameter server (PS)
iterations. In global

iteration t,

3

randomly selects K clients as Kt to participate the t-th global
iteration. The selected client i will download the latest model
wi
t = wt from the PS, and then performs E local iterations
(a.k.a epochs). The gradient descent with a mini-batch derived
in each local iteration is as follow

wi

t+1 = wi

t − η∇Fi(wi

t, Bi

t),

(3)

where Bi
t is a mini-batch of samples with size b selected from
Di. Let gi
t+E denote model updates of client i by conducting
E local epochs in the t-th global iteration, which can be
exactly deﬁned as gi
t. After conducting E
local iterations, selected clients upload model updates to the
PS for the following global aggregation

t+E = wi

t+E − wi

wt+E = wt +

(cid:88)

i∈Kt

pigi

t+E.

(4)

With globally updated model parameters obtained in Eq. (4),
the PS can embark a new round of global iteration by involving
different participating clients.

Note that model updates are formally deﬁned as changes
of model parameters after conducting E local epochs on
clients. In other words, model updates from client i are
t+E = (cid:80)t+E−1
j, Bi
gi
j). Given that clients and the
PS merely exchange model updates during model training, our
work focuses on compressing model updates.

η∇Fi(wi

j=t

For simplicity, let t denote the index of all iterations and
there are total T iterations. The global model will be updated
only if t ∈ I = {E, 2E, 3E, . . . }. Note that the initial model
parameters can be randomly generated by the PS when t = 0.

IV. BCFL FRAMEWORK

In this section, we elaborate the training process of the
BCFL framework to illustrate the relation between the training
time cost of BCFL and the amount of communication trafﬁc.
Then, we present the BCFL algorithm by leveraging the T opk
compression algorithm.

A. BCFL Training Process

Fig. 1. BCFL Framework Process

Similar to previous works, we consider that there are N
clients and M miners in the blockchain-based learning system.
Let N and M denote the set of clients and miners, respec-
tively. We suppose that each client is connected to a particular
miner in each global iteration to upload and download model
updates.

Note that BCFL is a generic framework since different
compression algorithms and consensus mechanisms can be
adopted. However, to analyze the convergence rate of BCFL,
we specify the T opk algorithm [33] for model update compres-
sion and PoW (proof of work) as the consensus mechanism
hereafter. PoW is the most fundamental consensus mechanism
[34] used in blockchain, and also widely used in BFL [9], [35].
It is not difﬁcult to adopt a different consensus mechanism in
BCFL by modifying the mining model accordingly.

In BCFL, due to the involvement of miners and the dis-
tributed aggregation operated on each client,
the training
process of BCFL is much more complicated than that of VFL.
There are six steps in a round of global iteration. Next, we
will introduce each step in details.

t+ 1
2

t, Bi

= wi

t) where Bi

t−η∇Fi(wi

1) Local Model Training Each client uses batch gradient
descent to update local model parameters according to
wi
t is a batch of sam-
ples with size b selected from Di. If the iteration index
t+1 /∈ I, it is not a global synchronization iteration, and
thus wi
. Otherwise, if t + 1 ∈ I, it means
that the client has completed E local iterations and needs
to upload local model updates to its miner. The model
updates deﬁned as gi
t = wi
will be compressed by the T opk algorithm as below.

− wt+1−E − mi

t+1 = wi

t+1−E

t+ 1
2

t+ 1
2

T opk(gi

t[l]) =

(cid:40)

gi
t[l],
0,

|gi

t[l]| > φ,

otherwise,

(5)

t and gi

where φ is the threshold determined by the value of the
(k + 1)-th largest absolute value of items in gi
t[l]
t. Here mi
is the l-th item of gi
t+1−E is the compensation
error which is used to locally adjust model parameters
based on the compression error. The compensation error
is deﬁned as mi
t+1−E −
wt+1−E. Intuitively speaking, the T opk algorithm only
uploads k most signiﬁcant items. Let τlocal denote the
time spent on this step in BCFL.

t+1 = T opk(gi

t) + wi

− mi

t+ 1
2

8

2) Model Upload The upload time of model updates from
a particular client to its miner is determined by the
trafﬁc volume and the upload speed. Let d denote the
dimension of the model. Suppose that the trafﬁc volume
of each model update is s bytes. The trafﬁc volume of k
updates is k(s + (cid:100)log2 d(cid:101)
) bytes, where k is the number
of selected updates and (cid:100)log2 d(cid:101)
is the number of bytes
8
to represent indices of these updates. In contrast, the
trafﬁc volume will be ds in VFL with no compression.
Let u↑,i bytes/s denote the upload speed of client i, then
the elapsed time of this step is τ↑,i = k(s+ (cid:100)log2 d(cid:101)
. Note
that the remaining d−k insigniﬁcant model updates will
not be uploaded by client i, which will be absorbed by
the compensation error mi
t+1.

8
u↑,i

)

4

3) Cross Transaction In this step, all model updates
uploaded by clients will be stored in candidate blocks.
Thereafter, each miner needs to distribute transactions
(i.e., model updates) received from its clients to all other
miners. According to [6], the bottleneck for distributing
transactions lies in the download capacity of miners. Let
uj denote the download speed of miner j. Let Nj and
Nj denote the set of and the number of clients clients
connecting miner j, respectively. It takes τcross,j =
(N −Nj )k(s+ (cid:100)log2 d(cid:101)
for miner j to receive transactions
uj
from all other miners, where N − Nj represents the
number of clients not connecting to miner j.

)

8

4) Mine and Propagation In this step, each miner attempts
to generate a block to store all model updates from
clients into the blockchain. However, it is not trivial
to derive the elapsed time of this step because: 1) The
elapsed time is a random variable such that we can only
analyze its expected value; 2) We need to consider both
the time to generate and propagate a block, and the fork
probability. Once a fork event occurs, this step has to
start over to consume excessive time. In practice, miners
will not receive transactions in a synchronized manner.
However, according to previous works [6], [36], this
difference is negligible. To simplify analysis, we ignore
this difference in our work.
We ﬁrst consider the expected elapsed time for a winning
miner to generate a block (denoted by τmine) and
propagate the block to all miners (denoted by τpro)
without considering the occurrence of forks. The process
for the winning miner to generate a block is determined
by consensus mechanisms. In this work, we adopt the
PoW consensus mechanism for analysis. With PoW
mechanism, each miner enumerates different nonce and
hashes the block header until the hash value of one miner
is less than a given target value. Then this miner obtains
the right to connect its candidate block to the blockchain
and broadcast it to other miners. Other miners stop PoW
processes once they receive the transmitted block and
connect it to their own blockchains. Note that different
consensus mechanisms can be adopted for miners to
compete for the right of bookkeeping. We only need to
alter τmine slightly if a different consensus mechanism
is adopted.
According to [34], if PoW is adopted as the consensus
mechanism, the target value can be tuned ﬂexibly by
adjusting the difﬁculty to obtain the hash result, and
the process to generate a block can be modeled by a
random variable obeying exponential distribution. Let
random variable xmine denote the elapsed time of this
process. Let λ denote the parameter of the exponential
distribution, then the expected mining time is τmine =
E[xmine] = 1
λ where λ is a tunable parameter. How to
optimally set λ will be discussed later.
Let jw denote the ﬁrst miner to get a block. The miner
jw needs to send its block to all other miners. The
block size is Ω = N k(s + (cid:100)log2 d(cid:101)
) bytes containing
model updates of all clients. Recall that the miner set is

8

Ω
uj

λ +maxj∈M/jw

denoted by M. Then, the elapsed time for mining and
propagation is τmine +τpro = 1
. Here
M/jw denotes the set of all miners except miner jw.
Nonetheless, forks occur if any other miners also gen-
erate blocks before jw can propagate its block to these
miners. Once forks occur, this step starts over. By model-
ing xmine with exponential distribution, the fork proba-
bility can be estimated accurately. Let xmine,j denote the
time for miner j to generate a block with the same dis-
tribution as xmine. To avoid forking, the block generated
by miner jw should arrive at miner j before miner j ob-
tains a block. In other words, it is required that xmine,j −
xmine,jw > Ω
. Therefore, the forking probability is
uj
(cid:17)
pf ork = 1 − (cid:81)
.
Here we assume that all miners start competing simul-
taneously. In avoid forking, we must have

xmine,j − xmine,jw > Ω
uj

j∈M/jw

P r

(cid:16)

(cid:16)

P r

xmine,j − xmine,jw >

(cid:17)

Ω
uj

= P r

(cid:16)

xmine,j > xmine,jw +

|xmine,j > xmine,jw

(cid:17)

Ω
uj

= P r

(cid:16)

xmine,j >

(cid:17)

Ω
uj

= exp(−λ

Ω
uj

).

Here,
the derivation is based on the memoryless-
ness of the exponential distribution of xmine,j. Con-
sequently, the probability of forking is pf ork = 1 −
exp(−λ (cid:80)
By wrapping up, the expected elapsed time of this step
is

i∈M/jw

Ω
uj

).

(τmine + τpro)/(1 − pf ork)

=

(cid:18) 1
λ

+ max

j∈M/jw



exp

λ

(cid:19)

Ω
uj



 .

Ω
uj

(cid:88)

j∈M/jw

(6)

5) Model Download When a block is successfully gener-
ated by the winning miner, all clients need to download
the latest block containing model updates from all clients
to update the trained model. Let u↓,i denote the down-
load speed of client i. The download time consumed by
client i is τ↓,i = Ω
u↓,i

.

6) Model Aggregation Once clients obtain the latest block,
they can aggregate model updates to deduce updated
model parameters. Let τaggre denote the time consumed
by the simple aggregation operation, which can be easily
estimated based on the computation capacity of clients.
Discussion: Based on the analysis of the training process of
BCFL, we can conclude that the time cost of a round of global
iteration is the cumulative time cost of aforementioned six
steps, which is

h(k, λ) = τlocal + τaggre + max
i∈N
+ (τmine + τpro)/(1 − pf ork) + max
i∈N

τ↑,i + max
j∈M

τcross,j

(7)

τ↓,i,

where k is the parameter of the T opk compression algorithm.
The explanation of each symbol for the time cost analysis is
listed in Table V in Appendix.

5

Remark I: We can suppose that the computation and commu-
nication capacity of each client and miner can be measured in
advance. Given the volume of communication trafﬁc such as
Ω, the time cost h(k, λ) can be estimated accurately prior to
the commencement of BCFL.
Remark II: The time cost h(k, λ) is heavily affected by the
trafﬁc volume Ω. If k is smaller, it implies that the time cost
can be reduced effectively owing to smaller Ω. Consequently,
BCFL can complete a round of global iteration with a faster
speed.
Remark III: However, it is unreasonable to merely reduce
k without considering its inﬂuence on the convergence of
the trained model. To optimally set the compression rate in
BCFL, we conduct convergence analysis in the next section
to synthetically consider the time cost and the convergence
rate so as to maximize the ﬁnal model accuracy within a ﬁxed
span of training time.

B. BCFL Algorithm

To have a global picture of the BCFL framework and facil-
itate subsequent convergence analysis, we present the BCFL
algorithm in Alg. 1. To make our presentation uncluttered, the
step number has been marked as comments in the algorithm.
Different from VFL,
there is no PS in BCFL. Instead, a
number of miners engage in the training process. A client can
participate FL as long as the client connects with any miner.

V. CONVERGENCE ANALYSIS OF BCFL

In this section, we analyze the convergence of the BCFL
algorithm and formulate the optimization problem to maximize
the ﬁnal model accuracy based on the derived convergence.

A. Assumptions and Deﬁnitions

For the sake of the popularity of neural network models
[37], [38], we analyze the convergence of BCFL with non-
convex loss. Similar to previous works [16], [39], [40], we
make a few general assumptions.

Assumption 1. The loss functions, i.e., F1, F2, . . . , FN are all
L-smooth. In other words, given v and w, we have Fi(v) ≤
Fi(w) + (v − w)T ∇Fi(w) + L

2 ||v − w||2.

Let ξi
from client i.

t denote the sample randomly and uniformly selected

Assumption 2. The variance of the stochastic gradients in
each client is bounded, i.e., E[(cid:107)∇Fi(wi
t)(cid:107)2] ≤
σ2
i for i = 1, 2, . . . , N and t = 0, 1, . . . , T − 1.
Assumption 3. The expected square norm of stochastic gra-
dients is uniformly bounded, i.e., E[(cid:107)∇Fi(wi
t)(cid:107)2] ≤ G2 for
all i = 1, 2, . . . , N and t = 0, 1, . . . , T − 1.

t) − ∇Fi(wi

t, ξi

t, ξi

In most FL systems, the sample distribution on clients is
non-IID, which can heavily impact the convergence of FL. In
this work, we use the difference between local gradients and
global gradients to quantify the non-IID degree of the sample
distribution on clients, which is also used in the previous work
[15].

Algorithm 1: BCFL Framework Training Process
Input: Model w0, mi
Output: Final model wT

0 = 0, ∀i ∈ [N ]; learning rate η

1 Each client is randomly connected to a miner.
2 for t = 0 to T − 1 do
On Clients:
3
for i = 1 to N parallel do

4

// Lines 5 to 8 correspond to

Step 1)
= wi
wi
if t + 1 /∈ I then

t+ 1
2

t − η∇Fi(wi

t, Bi

t).

wi

t+1 = wi

.

t+ 1
2

end
else

t+1−E − wt+1−E)

t+ 1
2

− mi

// Correspond to Step 2)
Send
gi
t = T opk(wi
to miner.
t+1 = gi
mi
// Correspond to Step 5), 6)
Download gi
wi

t, ∀i from miner and set
(cid:80)N

t+1 = wt+1 = wt+1−E + 1
N

t + wi

− mi

t+ 1
2

t+1−E − wt+1−E.

i=1 gi
t.

end

end
On Miners:
for j = 1 to M parallel do

t if i ∈ Nj.

if t + 1 ∈ I then
Receive gi
// Correspond to Step 3)
(cid:48)
Cross received gi
(cid:54)= j.
if j
// Lines 20 to 31 correspond

t to miners j

(cid:48)

to Step 4)

while True do

Execute consensus mechanism
if receive generated block then

Send ACK.

end
else if generate block then

Send block to other miners.

end
if not forking then

break.

end

end

end

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

end

33
34 end
35 return wT

(Quantiﬁcation of non-IID) We use Γ2

G ≥
Deﬁnition 1.
E[(cid:107)∇Fi(wt)−∇F (wt)(cid:107)2], ∀i, t to quantize the degree of non-
IID sample distribution on clients.

Note that the quantiﬁcation becomes 0 if the sample distri-
bution is IID. It is common that the distribution is non-IID for

6

samples on decentralized clients in FL. Consistent with prior
works such as [3], [15], our analysis also shows that non-IID
distribution can slow down the speed of model training in FL.
The adverse inﬂuence of non-IID can be mitigated by devising
more advanced client participation schemes [41], [42]. Note
that this line of works is orthogonal to our work by considering
that our focus is on optimizing the compression rate and block
generate rate.

The T opk algorithm used in the BCFL framework is a well-
known biased compression algorithm [13], [26]. The compres-
sion error is a major factor inﬂuencing the convergence, which
is deﬁned as follows.

Deﬁnition 2. T opk is a reasonable compression operation
if there is a constant γ such that the following compression
property is satisﬁed,

E[(cid:107)x − T opk(x)(cid:107)2] ≤ (1 − γ)(cid:107)x(cid:107)2.

The proof of this property has been derived in the previous
work [13]. According to the derivation in [13], γ = k
d for
the T opk algorithm where d is the dimension of the trained
model while k is the number of reserved most signiﬁcant
model updates for communications. It is worth noting that our
framework is ﬂexible in the sense that a different compression
algorithm can be adopted here based on system needs. In
other words, γ in Deﬁnition 2 should be updated accordingly
if another compression algorithm is adopted. For example, γ
corresponding to STC is (cid:107)T opk(x)(cid:107)2

[28].

1

k(cid:107)x(cid:107)2

B. Convergence Rate

We complete the proof of the convergence of BCFL by
leveraging the the convergence proof sketched in [26], [43].
The difference lies in that [26], [43] failed to consider the
non-IID sample distribution in FL. We extend this proof by
taking the non-IID sample distribution into account.

Let zT denote the uniformly and randomly selected sample
t for ∀t, i. In other words, P r(zT =
N T where 1 ≤ i ≤ N and 1 ≤ t ≤ T . It turns

of historical gradients ∇wi
∇F (wi
out that the convergence rate of BCFL is:

t)) = 1

Theorem 1. Let Assumptions 1 to 3 hold. We set a ﬁxed
learning rate η = C√
16L , where C is a
T
constant. After conducting T iterations with Algo. 1, E(cid:107)zT (cid:107)2
is bounded by:

and η satisﬁes η ≤ 1

E(cid:107)zT (cid:107)2 ≤

(cid:32) E[F (w0)] − F ∗
C

+ 2CLΓ2

G +

+

(cid:18) 4

γ2 − 3

(cid:19) 16C 2L2G2E2
T

.

CL
bN 2

(cid:88)

σ2
i

i

(cid:33)

8
√
T

(8)

Here F ∗ is the optimal value of global loss function.

Please refer to Appendix B for the detailed proof. The key
to the proof is to construct an auxiliary sequence of centrally
trained models. We then analyze the model’s updates in this
sequence and its difference from the real trained model to
derive the convergence. To ease the interpretation of our proof,

the explanation of notations used for the convergence proof is
listed in Table VI in Appendix.
Remark I: According to Theorem 1, the term 4
γ2 − 3 in the
convergence rate is affected by the compression rate. Recall
that the compression rate is γ = k
d . We can observe that the
convergence rate is lower in terms of the number of iterations
if γ is smaller (implying a higher compression rate).
Remark II: On the other hand, our study in Eq. (7) indicates
that the time cost of each global iteration can be reduced by
raising the compression rate γ. Therefore, it is not trivial to
choose the optimal compression rate for BCFL. In view of
that, our next step is to formulate the problem to optimize
the ﬁnal model accuracy with a ﬁxed training time span by
regarding the compression rate as a tuneable parameter.

C. Optimizing Model Accuracy

In reality, the training time span for FL is limited. Let Y
denote the ﬁxed training time span for FL. Then, our question
is how to maximize the ﬁnal model accuracy within the ﬁxed
training time span by tuning parameters k and λ.

cost of one

two parameters

round of global

(cid:16) E[F (w0)]−F ∗
C

that k and λ are
time

affect-
Recall
iteration
ing the
in h(k, λ). To make our discussion concise, we
let
ΛA = 8√
+ 2CLΓ2
and ΛB =
E
16C 2L2G2E denote constant numbers not related with k or
λ in the convergence rate. Let R = T
E denote the total
number of conducted global iterations. The convergence rate
in Theorem 1 is simpliﬁed as

G + CL
bN 2

i σ2
i

(cid:80)

(cid:17)

7

We prove the following important property of P2 by relax-

ing that k is a positive real number.

Theorem 2. When λ is ﬁxed, J 2(k, λ) is a convex function
with respect to k. When k is ﬁxed, J 2(k, λ) is a convex
function with respect to λ. Given the linear constraints of k
and λ, P2 is a bi-convex optimization problem, which can be
solved efﬁciently by Alternate Convex Search (ACS) [44].

Please refer to Appendix C for the detailed proof. Therefore,
k and λ in the BCFL algorithm can be determined by solving
P2. In practice, we can set k as the nearest integer of its
theoretically optimal value.

D. Practical Implementation

From previous analysis, we can ﬁnd that solving P2 replies
on the knowledge of network conditions such as the commu-
nication speed between miners so as to quantify the function
h(k, λ). Unfortunately, the network status is dynamic in prac-
tice, resulting in the change of h(k, λ) and the solution of P2.
Thus, to implement BCFL, it is necessary to take the dynamic
network conditions into account.

In fact, the network conditions can be proactively estimated
and gauged through measurement [45]. Due to the bi-convex
property of P2, we propose that the solution of P2 can be
updated in real time according to the latest network conditions.
In other words, miners can periodically measure network
conditions to update h(k, λ), which is then substituted into
P2 to update the compression rate and block generation rate.

E(cid:107)zT (cid:107)2 ≤

ΛB

(cid:16) 4d2

(cid:17)
k2 − 3
R

.

ΛA√
R

+

Given that the total training time span is Y , the number
h(k,λ) . By substituting

of conducted global iterations is R = Y
R = Y

h(k,λ) into Eq. (9), we can deﬁne our objective as:

(9)

VI. PERFORMANCE EVALUATION

In this section, we conduct experiments with CIFAR-10 and
FEMNIST datasets to evaluate the performance of the BCFL
framework.

J (k, λ) =

ΛA

(cid:112)h(k, λ)
√
Y

+

ΛB

(cid:17)

(cid:16) 4d2

k2 − 3
Y

h(k, λ)

A. Experimental Settings

.

(10)

1) Dataset: In our experiments, we employ CIFAR-10 and

FEMNIST datasets for model training.

The problem to maximize the ﬁnal model accuracy (equivalent
to minimizing the bound of E(cid:107)zT (cid:107)2) is formally deﬁned as:

P1 :

min
k,λ
s.t.

J (k, λ)

0 < k ≤ d, λ > 0.

(11)

Considering that d (cid:29) k for compression algorithms in prac-

tice, the objective can be simpliﬁed as J (k, λ) =

ΛA

h(k,λ)

+

√
√

Y

ΛB

4d2
k2 h(k,λ)

Y

. Unfortunately, the objective function J (k, λ) is
not a convex or concave function. We resort to minimizing
J 2(k, λ). Our problem P1 is converted to:
P2 :

J 2(k, λ)

min
k,λ
s.t.

0 < k ≤ d, λ > 0

J 2(k, λ) =

+

8ΛAΛBd2
Y 3

2

Λ2

Ah(k, λ)
Y
(cid:18) h(k, λ)
k 4

3

(cid:19) 3

2

+

16Λ2
Bd4
Y 2

(cid:18) h(k, λ)
k2

(cid:19)2

.

(12)

• CIFAR-10 includes 60,000 data samples, each of which is
a 3*32*32 color image. This dataset contains ten labels,
and each label consists of 6,000 samples. We randomly
select 50,000 samples as the training set distributed on
clients, and the remaining 10,000 samples will be used as
the test set to evaluate the accuracy of the trained model.
Data samples are allocated to clients according to either
IID or non-IID distribution. For the former one, each
client uniformly and randomly draws the same number
of samples from the entire training set. For the latter one,
each client randomly selects the same number of images
from the training subset consisting of 5 random labels.
• FEMNIST [46] is a benchmark dataset for validating
FL. Each sample is a handwritten 28*28 picture of
digits and English characters, and there are 62 labels
in total. The distribution is naturally non-IID since this
dataset simulates the non-IID distribution of data samples
partitioned among different writers. In our experiments,
each client owns more than 400 samples drawn from a

single writer and 10% of these samples will be allocated
to the test set.

In summary, we have three sample distribution scenarios for
experiments, which are CIFAR-10+IID, CIFAR-10+non-IID
and FEMNIST.

2) Learning Model: We train a convolutional neural net-
work (CNN) model [47] to classify the CIFAR-10 dataset.
The CNN model consists of 3 convolutional layers, each of
which is 3*3 in size and the number of convolution kernels is
32, 64 and 64, respectively. Finally, two fully connected layers
are used to output predictions for 10 labels. The model has
122,570 parameters in total.

For the FEMNIST dataset, we refer to the model in [46]
for training, which consists of two convolutional layers with
max pooling layers and two fully connected layers. There are
111,902 parameters in total. The model outputs predictions for
62 labels.

According to our analysis, we set a ﬁxed learning rate as
η = 0.05 and C = 0.15 for the training of both models.
Because the number of data samples on each client is relatively
small, we execute the full-batch gradient descent algorithm to
conduct local iterations. Each client will perform E = 5 local
iterations in each global iteration.

3) Baseline Algorithm: In our experiments, we compare the
performance of BCFL with two kinds of baselines. The ﬁrst
one is the kind of BFL algorithms such as the one proposed in
[4] without compressing model updates for communications.
We implement the one in [6] in our experiments, which only
optimally sets λ once the communication trafﬁc is ﬁxed. For
the second kind, we randomly enumerate several different
compression rates for BCFL and compare their performance
with the one with the optimal compression rate to evaluate how
much performance can be improved through our analysis. By
default, we set k = d ∗ 1%, d ∗ 2% and d ∗ 3% for the second
kind of baselines. In fact, we have k = d for BFL without
model compression. Note that we let k∗ and λ∗ represent the
optimal settings in BCFL obtained by solving P2.

4) Network Simulator: We refer to [48], [49] to simulate
wireless communications between miners and clients (or min-
ers) in our experiments. The uplink and downlink transmission
rates of clients and miners are random variables obeying Gaus-
sian distribution. The mean value is set according to Shannon’s
formula µ = BW log2(1 + gPt
), where BW = 20M Hz is
Pn
the channel bandwidth and g = 10−8 is the channel gain.
Furthermore, we set the transmission power Pt = 0.5W and
the noise energy Pn = 10−10W to simulate the rate of wireless
transmission to measure the time required for communications
during training. We set the same rate for all clients and miners
for both downlink and uplink in our experiments. At each
transmission, we take samples from the Gaussian distribution
to simulate the ﬂuctuation of the transmission rate in real
networks. The mean and standard deviation of the Gaussian
distribution are µ and 0.1µ, respectively. Based on the network
speed model, we have u↑,i = u↓,i = uj = µ.

For the network topology, we set up 50 clients and 50
miners, i.e., M = N , by default unless we state otherwise.
According to the previous work [9], we set up a one-to-one
connection between miners and clients but all miners are fully

8

connected with each other. Note that the values of N and M
can be different in practice, and each miner can connect to
multiple clients. It only slightly alters coefﬁcients in h(k, λ)
without affecting our analysis framework.

By conducting local iterations in advance, we can measure
that τlocal ≈ 0.2s and 0.08s with CIFAR-10 and FEMNIST,
respectively. But τaggre ≈ 0 because of the simplicity of the
aggregation operation.
5) Solving P2:

In BCFL, it is required to solve P2 to
determine k∗ and λ∗. Before solving P2,
is necessary
to estimate a series of vital parameters in h(k, λ) and the
convergence rate. How to estimate these parameters are brieﬂy
described as below.

it

TABLE I
ESTIMATION OF PARAMETERS IN THE CONVERGENCE RATE.

Scenario
CIFAR-10+IID
CIFAR-10+non-IID
FEMNIST

L
0.45
0.16
6.49

G2
0.15
10.44
1.17

Γ2
G
0.00044
0.029
0.0035

F (w0) − F ∗
2.30
2.19
4.08

In the ﬁrst round of global

iteration, we arbitrarily set
k = 1%d for CIFAR-10 and k = 0.5%d for FEMNIST to
obtain w0 (initial parameters randomly generated) and wE.
Then, L in Assumption 1 can be estimated based on w0 and
wE. We use the loss function value with input wE as its
approximation of E[F (w0)] − F ∗ by assuming that F ∗ ≈ 0.
In our experiments, the full-batch of local samples will be used
for local iterations, and thus σi = 0.1. Meanwhile, clients are
also required to upload the norm of their gradients to miners
in the ﬁrst global iteration so that we can estimate G. Based
on the above method , we can estimate the values of these
parameters in the convergence rate which are listed in Table. I.
According to the expression of h(k, λ) in Eq. (7), we stil
need to estimate τlocal and τaggre before we can solve P2.
By executing local iterations in the ﬁrst global iteration, we
can easily measure that τlocal ≈ 0.2s and 0.08s with CIFAR-
10 and FEMNIST, respectively, on our experiment platform.
But τaggre ≈ 0 because of the simplicity of the aggregation
operation.

With these estimated parameters in hand, we can ﬁnally
solve P2 to deduce k∗ and λ∗ for our experiments. Their values
are also presented together with each experiment in the next
subsection.

B. Experimental Result

1) Comparison of Model Accuracy: We compare the model
accuracy of BCFL with baselines within a ﬁxed training time
span Y = 500s and 400s on CIFAR-10 and FEMNIST,
respectively. The results are plotted in Fig. 2, where the x-axis
represents the elapsed training time and the y-axis represents
the model accuracy on the test set. From experiment results
in Fig. 2, we can observe that:

• Compressing model updates in BFL can effectively im-
prove the training efﬁciency. As long as k is an arbitrary
number much smaller than d, BCFL can achieve much

1If a mini-batch is used for local iterations, σi is can be estimated locally

9

Fig. 2. Comparison of accuracy between BFL and BCFL with different compression rates under IID+CIFAR-10 (left), non-IID+CIFAR-10 (middle) and
FEMNIST (right).

higher model accuracy than BFL under all experimen-
tal scenarios. Apparently, the reason is that the heavy
communication load in BFL consumes excessive training
time.

• Setting k∗ and λ∗ in BCFL achieves the highest model
accuracy than all other baselines. In particular, for the
non-IID+CIFAR-10 case, the improvement is signiﬁcant.
• Since FEMNIST has 62 labels, which is much more than
that of CIFAR-10, more global iterations are required to
train an effective model. Therefore, the model trained
in the given time span has poor performance for BFL
without compressing communications.

2) Comparison of Model Accuracy with Fixed λ: In BCFL,
λ is a parameter to control the rate for block generation.
It is possible to ﬁx λ as any number to control the block
generation rate. We next compare the performance of BCFL
with baselines by arbitrarily ﬁxing λ = 0.4 in Fig. 3. In fact,
P2 degenerates to a convex optimization problem when λ is
ﬁxed in advance and it is expected that BCFL can still achieve
the best performance.

We keep other parameters the same as these in the previous
experiment. From the experimental results presented in Fig. 3,
we can also observe that the model accuracy of BCFL is
always better than all other baselines. Note that the improve-
ment of BCFL under the non-IID+CIFAR-10 case is not as
signiﬁcant as that in the previous experiment. The reason is
that λ is a ﬁxed number, not assigned with its optimal value.
3) Varying Compression Rates: To further validate that
BCFL can optimally set the compression rate to maximize
the ﬁnal model accuracy, we conduct more experiments by
i.e., setting k =
enumerating different compression rates,
0.1% ∗ d, 0.5% ∗ d, 1% ∗ d, 1.5% ∗ d, 2% ∗ d and 3% ∗ d,
respectively in BCFL. The training time span Y is ﬁxed as
500s, 500s, 400s for three experiments. For these experiments,

λ is set as its optimal value once k is ﬁxed.

Experiment results are shown in Fig. 4, in which the x-
axis represents the compression rate and the y-axis represents
the ﬁnal model accuracy after Y training time. The results in
Fig. 4 manifest that our theoretical optimal setting can achieve
the highest ﬁnal model accuracy among all baselines with
enumerated compression rates. It indicates the effectiveness
of our optimization analysis.2

TABLE II
COMPARISON OF DIFFERENT ALGORITHMS WITH IID DISTRIBUTION AND
61% MODEL ACCURACY IN CIFAR-10.

Comm.
Rate
1
44.43
65.31
32.65
21.77

Comm. Trafﬁc

Train Time

39021.31MB
867.80MB
648.54MB
1165.54MB
1673.93MB

4953.46s
260.12s
285.19s
380.26s
436.56s

train time
reduced to
100%
5.25%
5.76%
7.68%
8.81%

BFL
k∗
k = 1% ∗ d
k = 2% ∗ d
k = 3% ∗ d

TABLE III
COMPARISON OF DIFFERENT ALGORITHMS WITH NON-IID DISTRIBUTION
AND 58% MODEL ACCURACY IN CIFAR-10.

Comm.
Rate
1
59.37
65.31
32.65
21.77

Comm. Trafﬁc

Train Time

35148.92MB
686.81MB
628.07MB
1184.25MB
1673.93MB

4650.21s
245.11s
395.87s
450.59s
501.51s

train time
reduced to
100%
5.27%
8.51%
9.69%
10.78%

BFL
k∗
k = 1% ∗ d
k = 2% ∗ d
k = 3% ∗ d

4) Comparison of Training Time Consumption: To further
explore the underling reason how BCFL improves the model

2Due to limited computation capacity and the large value of d, we cannot

enumerate all possible k’s in our experiments.

0100200300400500Train Time (s)10203040506070Accuracy(%)k*=d*1.47%, λ*=0.53k=d*1%, λ*=0.79k=d*2%, λ*=0.39k=d*3%, λ*=0.26k=d, λ*=0.00780100200300400500Train Time (s)10203040506070Accuracy(%)k*=d*1.1%, λ*=0.71k=d*1%, λ*=0.79k=d*2%, λ*=0.39k=d*3%, λ*=0.26k=d, λ*=0.00780100200300400Train Time (s)20406080Accuracy(%)k*=d*0.78%, λ*=1.09k=d*1%, λ*=0.85k=d*2%, λ*=0.43k=d*3%, λ*=0.28k=d, λ*=0.008510

Fig. 3. Comparison of accuracy between BFL and BCFL with different compression rates and ﬁxed λ = 0.4 under IID+CIFAR-10 (left), non-IID+CIFAR-10
(middle) and FEMNIST (right).

Fig. 4. Comparison of ﬁnal model accuracy of BCFL by enumerating different compression rates under IID+CIFAR-10 (left), non-IID+CIFAR-10 (middle)
and FEMNIST (right)

TABLE IV
COMPARISON OF DIFFERENT ALGORITHMS WITH NON-IID DISTRIBUTION
AND 82% MODEL ACCURACY IN FEMNIST.

Comm.
Rate
1
85.73
65.31
32.65
21.77

Comm. Trafﬁc

Train Time

36460.87MB
513.52MB
605.22MB
1138.26MB
1668.04MB

4553.17s
185.42s
230.58s
290.09s
355.04s

train time
reduced to
100%
4.07%
5.06%
6.37%
7.80%

BFL
k∗
k = 1% ∗ d
k = 2% ∗ d
k = 3% ∗ d

accuracy, we compare the consumed training time of each al-
gorithm to reach the target model accuracy by ﬁxing the target
model accuracy as 61%, 58% and 82% for three experiment
scenarios, respectively. We keep other settings the same as
these in the experiment in Fig. 2. The comparisons of training
II (for CIFAR-10+IID),
time are shown in Tables
III (for
CIFAR-10+non-IID) and
IV (for FEMNIST), respectively.
Here BFL represents the blockchain-based FL without model
compression.

From results shown in these tables, we can draw the

following conclusions.

• The BCFL algorithm can always reach the target ﬁnal
model accuracy with the shortest training time. Despite

that BCFL may not be the one with the least amount
of communication trafﬁc, it takes a smaller number of
global iterations to complete model training such that the
total training time cost is minimized. More importantly,
BCFL can reduce the consumed training time by more
than 90% compared with BFL.

• Our results show the merit of our algorithm to optimally
adjust the compression rate in BCFL. If the compression
rate is too high,
it can considerably slow down the
convergence rate presented in Theorem 1. Although the
communication trafﬁc can reduced signiﬁcantly, it takes
more global iterations to reach the target model accuracy,
Inversely, if the compression rate is too low, it consumes
excessive communication trafﬁc to complete a round of
global iteration, which inevitably consumes exorbitant
training time.

5) Varying Client and Miner Population: In this experi-
ment, we change the system scale by setting the number of
clients and miners as N = M = 20, 30, 40 or 50, respectively,
to investigate the inﬂuence of the system scale on model
accuracy performance. Meanwhile, to evaluate the robustness
of BCFL, we consider the dynamics of networking by assum-
ing that each client has 10% probability not to return model
updates due to the sudden changes of network conditions such

0100200300400500Train Time (s)102030405060Accuracy(%)Optimalkk=d*1%k=d*2%k=d*3%k=d0100200300400500Train Time (s)102030405060Accuracy(%)0100200300400Train Time (s)20406080Accuracy(%)0123value of γ(%)62646668Accuracy(%)enumerationoptimal0123value of γ(%)565860626466Accuracy(%)0123value of γ(%)8384858687Accuracy(%)11

Fig. 5. Comparison of accuracy between BFL and BCFL by varying dynamic client and miner populations under CIFAR-10+IID (left), CIFAR-10+non-IID
(middle) and FEMNIST (right)

as network congestion or failure in each global iteration. When
we the change client population, the distribution of CIFAR-10
samples on clients is modiﬁed accordingly. For example, if
the number of clients is doubled, the number of samples on
each client is reduced by 50%. For FEMNIST, the samples
on each client are from a particular writer. Therefore, as the
number of clients decreases, the number of samples on each
client is unchanged, but the total number of samples in the
system becomes smaller.

Experiment results are shown in Fig. 5, where the x-
axis represents the number of clients, and the y-axis repre-
sents the ﬁnal trained model performance. The training time
spans used for three different data distribution scenarios are
Y = 500s, 500s and 400s, respectively. By inspecting the
experiment results, we can conclude that

• Experiment results show the robustness of BCFL. Even if
the network is dynamic with occasionally ofﬂine clients,
BCFL can still achieve the higher model training perfor-
mance than that of BFL.

• The model accuracy of BCFL under the ﬁrst two scenar-
ios becomes higher as the number of clients increases.
The reason lies in that more clients can bring more
computation capacity to the system. As the number of
samples allocated to each client is reduced, clients can
complete local iterations faster. Consequently, the ﬁnal
model accuracy is improved by conducting more global
iterations within a ﬁxed training time span.

• The increase of model accuracy of BCFL with the in-
crease of client population cannot be observed in the
experiment with FEMNIST. The reason is that the sample
population on each client is unchanged as the increase
of client population, and thereby the local training time
cannot be reduced.

• It

is worthy noting that

the model accuracy of BFL
becomes worse with the increase of client population
under all three experiment scenarios. The reason is that
BFL cannot effectively prohibit the increase of communi-
cation trafﬁc. As more clients reside in the system, more
communication trafﬁc is generated that can seriously
prolong the communication time resulting in a lower

model accuracy at last.

VII. CONCLUSION

Combing blockchain with federated learning is of essen-
tial importance to construct a decentralized, auditable and
trustworthy federated learning system. Nonetheless, the huge
communication trafﬁc generated by BFL has severely hinder
the application of BFL in reality. Our work makes a signiﬁcant
attempt trying to solve this critical issue by compressing model
updates in BFL. We proposed the novel BCFL framework, and
proved its convergence by leveraging the T opk compression
algorithm under non-IID sample distribution and non-convex
loss. Based on the derived convergence rate, we further formu-
late the optimization problem to maximize the ﬁnal model ac-
curacy with respect to compression rate and block generation
rate. The problem is a bi-convex optimization problem, which
can be solved efﬁciently. Lastly, we conduct extensive experi-
ments with public datasets to not only validate the correctness
of our analysis but also demonstrate the notable performance
of our BCFL algorithm. In particular, by compressing model
updates, our communication-efﬁcient framework can reduce
the training time by about 95% without compromising model
the prototype
accuracy. Our future work is to implement
of BCFL and investigate its performance in more practical
environment.

APPENDIX A
NOTATION LIST

To ease the interpretation of our analysis, we list brief
explanations of major notations used in the time cost analysis
and convergence analysis of Theorem 1 in Table V and
Table VI, respectively.

APPENDIX B
PROOF OF THEOREM 1

To analyze the convergence of BCFL, we ﬁrst construct an

auxiliary sequence as follow,

(cid:101)wi

t+1 = (cid:101)wi

t − η∇Fi(wi

t, Bi

t);

(cid:101)wt+1 =

1
N

(cid:88)

i

(cid:101)wi

t+1,

20304050Number of Clients or Miners405060Accuracy(%)Optimal kBFL20304050Number of Clients or Miners405060Accuracy(%)20304050Number of Clients or Miners20406080Accuracy(%)TABLE V
NOTATION MEANING IN TIME COST ANALYSIS

Notation
Y
τlocal

τ↑,i

τcross,j

τmine,j

τpro

τ↓,i

τaggre

Meaning
total training time
time required for clients to train the model locally
time required for client i to upload the model
updates to corresponding miner
time required for miner j to receive the
transactions crossed from other miners
time required for miner j to complete the
consensus mechanism
time required for winning miner to propagate the
block
time required for client i to download the latest
block from miner
time required for clients to aggregate the model
updates in blocks

TABLE VI
NOTATION MEANING IN CONVERGENCE ANALYSIS

Notation

zT

E
F ∗
C

L
Γ2
G
b
N
σi
R

Meaning
the gradients of a model extracted with a

probability of

1
N T

number of local epochs on the clients
optimal value of global loss function

a constant satisfying C√
T

≤ 1
16L

the loss functions satisfy L − smooth
quantiﬁcation of non-IID
batch data size
number of clients
upper bound on variance of stochastic gradient
number of global iterations

0 = wi

0. Further, we deﬁne (cid:98)wt = 1

where (cid:101)wi
i wi
t.
Accordingly, { (cid:101)wt, ∀t} represents centralized sequence trained
by uncompressed model updates, while { (cid:98)wt, ∀t} represents
centralized sequence trained by compressed model updates.

N

(cid:80)

A. Key Lemmas

To derive the convergence of the model, we leverage the

following lemmas.

Lemma 1. If the model will be updated E epochs in clients
in each global
then the
following inequality holds for any i and t,

iteration during training process,

E[(cid:107)mi

t(cid:107)2] ≤ 4

η2(1 − γ)2
γ2

E2G2.

Lemma 2. According to the deﬁnition of (cid:98)wt, we can get

(cid:98)wt − (cid:101)wt =

1
N

(cid:88)

mi
t.

i

B. Convergence of the Algorithm

According to [43], we can get

F ( (cid:101)wt+1) − F ( (cid:101)wt)
(cid:42)

≤ −η

∇F ( (cid:101)wt),

(cid:43)

∇Fi(wi

t, Bi
t)

1
N

(cid:88)

i

+ η2L(cid:107)

1
N

(cid:88)

i

∇Fi(wi

t)(cid:107)2 + η2L(cid:107)pt − ¯pt(cid:107)2,

12

(13)

(cid:80)

i ∇Fi(wi

where pt = 1
i ∇Fi(wi
t, Bi
t)
N
representing average gradients. To solve the expected value
of the stochastic gradient sampling in the t-th round and
< a, b >= 1

2 ((cid:107)a(cid:107)2 + (cid:107)b(cid:107)2 − (cid:107)a − b(cid:107)2), we can get

t), ¯pt = 1
N

(cid:80)

EF ( (cid:101)wt+1) − F ( (cid:101)wt)
≤ −

(cid:107)∇F ( (cid:101)wt)(cid:107)2 + (cid:107)

(cid:16)

η
2

1
N

(cid:88)

∇Fi(wi

t)(cid:107)2

i
∇Fi(wi

t)(cid:107)2(cid:17)

1
N

(cid:88)

i

∇Fi(wi

t)(cid:107)2 +

−(cid:107)∇F ( (cid:101)wt) −

+η2L(cid:107)

1
N

(cid:88)

i

= −

(cid:16)

η
2

(cid:107)∇F ( (cid:101)wt)(cid:107)2 + (cid:107)

(cid:88)

η2L
bN 2

(cid:88)

σ2
i

i
∇Fi(wi

t)(cid:107)2

i
(cid:88)

i

(cid:13)
∇Fi(wi
(cid:13)
t)
(cid:13)

2(cid:17)

1
N

1
N

(cid:13)
(cid:13)
(cid:13)

−

1
N

(cid:88)

∇Fi( (cid:101)wt) −

+η2L(cid:107)

i
1
N

(cid:88)

i

∇Fi(wi

t)(cid:107)2 +

η2L
bN 2

(cid:88)

σ2
i

i
1
N

1
N

≤

η2L
bN 2

(cid:88)

σ2
i −

(cid:16)

η
2

(cid:107)∇F ( (cid:101)wt)(cid:107)2 + (cid:107)

i
(cid:88)

i
(cid:88)

i
(cid:88)

1
N
η
2N

i
2η2L − η
2
(cid:88)

η
2N

t(cid:107)2(cid:17)

i

+ η2L(cid:107)

L2(cid:107) (cid:101)wt − wi
(cid:0)(cid:107)∇F ( (cid:101)wt)(cid:107)2 − L2(cid:107) (cid:101)wt − wi
η2L
bN 2
(cid:0)(cid:107)∇F ( (cid:101)wt)(cid:107)2 − L2(cid:107) (cid:101)wt − wi

∇Fi(wi

t)(cid:107)2 +

1
N

(cid:88)

(cid:107)

i

−

= −

+

≤ −

+

2η2L
2

i

(cid:107)

1
N

(cid:88)

i

∇Fi(wi

t)(cid:107)2 +

η2L
bN 2

(cid:88)

σ2
i

i

t(cid:107)2(cid:1)

(cid:88)

σ2
i

i

t(cid:107)2(cid:1)

≤

2η2L
2N
η
2N

−

(cid:88)

i
(cid:88)

(cid:107)∇Fi(wi

t) − ∇F (wi

t) + ∇F (wi

t)(cid:107)2

(cid:0)(cid:107)∇F ( (cid:101)wt)(cid:107)2 − L2(cid:107) (cid:101)wt − wi

t(cid:107)2(cid:1)

(cid:88)

∇Fi(wi

t)(cid:107)2

∇Fi(wi

t)(cid:107)2

Lemma 3. Assume that the number of local training epochs
of the clients is E, we can get

+

1
N

(cid:88)

i

E(cid:107) (cid:98)wt − wi

t(cid:107)2 ≤ η2G2E2.

≤

2η2L
N

The above lemmas have been proved in [43], which will

not be repeated in this paper.

+

2η2L
N

(cid:107)∇Fi(wi

t) − ∇F (wi

t)(cid:107)2

i
(cid:88)

i

(cid:107)∇F (wi

t)(cid:107)2 +

η2L
bN 2

(cid:88)

σ2
i

i

i
(cid:88)

σ2
i

η2L
bN 2

i
(cid:88)

(cid:0)(cid:107)∇F ( (cid:101)wt)(cid:107)2 − L2(cid:107) (cid:101)wt − wi

t(cid:107)2(cid:1)

APPENDIX C
PROOF OF THEOREM 2

13

(cid:107)∇F (wi

t)(cid:107)2 + 2η2LΓ2

G +

(cid:0)(cid:107)∇F ( (cid:101)wt)(cid:107)2 − L2(cid:107) (cid:101)wt − wi

(cid:88)

i

η2L
bN 2
t(cid:107)2(cid:1)

(cid:107)∇F (wi

t)(cid:107)2 + 2η2LΓ2
G

(cid:0)(cid:107)∇F ( (cid:101)wt)(cid:107)2 + L2(cid:107) (cid:101)wt − wi

t(cid:107)2(cid:1)

σ2
i

Proof. According to the content in [4], we can know that when
k is ﬁxed, h(k, λ) is a convex function with respect to λ.
At this time, the objective function J 2(k, λ) is also convex.
Therefore we next discuss the properties of J 2(k, λ) when λ
is ﬁxed.

To simplify the analysis process, we denote ω = s+ (cid:100)log2 d(cid:101)
as the size of each compressed model update consisting of
value and position ID. We deﬁne

8

(cid:88)

σ2
i +

i

ηL2
N

(cid:88)

i

(cid:107) (cid:101)wt − wi

t(cid:107)2.

ΛT = max
i∈N

ω
u↑,i

+ max
j∈M

(N − Nj)ω
uj

+ max
i∈N

N ω
u↓,i

,

≤

=

(cid:88)

i
(cid:88)

i
(cid:88)

i
(cid:88)

i
(cid:88)

−

η
2N

2η2L
N
η
2N

−

2η2L
N
η
2N

−

i
η2L
bN 2

+

The term (cid:107)∇F (wi

t)(cid:107)2 can be bounded as below.

(cid:107)∇F (wi

t)(cid:107)2 ≤ 2(cid:107)∇F (wi
≤ 2L2(cid:107)wi

t) − ∇F ( (cid:101)wt)(cid:107)2 + 2(cid:107)∇F ( (cid:101)wt)(cid:107)2

t − (cid:101)wt(cid:107)2 + 2(cid:107)∇F ( (cid:101)wt)(cid:107)2.

If η ≤ 1

η
8N

i

16L , we can get
(cid:88)

(cid:107)∇F (wi

t)(cid:107)2 ≤ F ( (cid:101)wt) − EF ( (cid:101)wt+1)

+ 2η2LΓ2

G +

η2L
bN 2

(cid:88)

σ2
i +

i

ηL2
N

(cid:88)

i

(cid:107) (cid:101)wt − wi

t(cid:107)2.

By taking the expected value of each item in the above
formula, we can get

(14)

(15)

ΛP = max

j∈M/jw

N ω
uj

, ΛF = λ

(cid:88)

j∈M/jw

N ω
uj

,

for parameters whose values are constant. Thence, h(k, λ) =
τlocal + τaggre + ΛT k + 1
λ exp(ΛF k) + ΛP k exp(ΛF k). It is
obvious that the ﬁrst term in the objective function J 2(k, λ)
is convex when λ is ﬁxed. So we will focus on the convexity
of second term.

h(k, λ)

k2 = (τlocal + τaggre)k−2 + ΛT k−1
+
+

ΛP exp(ΛF k)
k

1
λ exp(ΛF k)
k2

.

(19)

η
8N

(cid:88)

i

E(cid:107)∇F (wi

t)(cid:107)2 ≤ E[F ( (cid:101)wt)] − E[F ( (cid:101)wt+1)]

Both the ﬁrst and second terms in Eq. (19) are convex, thus

we discuss the convexity of the third term ﬁrst. We can get

+ 2η2LΓ2

G +

η2L
bN 2

(cid:88)

i

i + 2ηL2E(cid:107) (cid:101)wt − (cid:98)wt(cid:107)2
σ2

(16)

+

2ηL2
N

(cid:88)

i

E(cid:107) (cid:98)wt − wi

t(cid:107)2.

(cid:18) 1

λ exp(ΛF k)
k2

(cid:19)(cid:48)

=

1
λ

ΛF k−2 exp(ΛF k) − 2

1
λ

k−3 exp(ΛF k).

(20)

According to Lemmas 1 and 2, we can get E(cid:107) (cid:98)wt − (cid:101)wt(cid:107)2 ≤
4η2(1−γ2)
G2E2. Therefore, we can leverage Lemmas 1-3 to
γ2
derive
η
8N

t)(cid:107)2 ≤ E[F ( (cid:101)wt)] − E[F ( (cid:101)wt+1)]

E(cid:107)∇F (wi

(cid:88)

i

+ 2η2LΓ2

G +

η2L
bN 2

(cid:88)

σ2
i +

i

8η3(1 − γ2)
γ2

L2G2E2

(17)

+ 2η3L2G2E2.

By accumulating the above inequality from t = 0 to T − 1
and dividing T on both sides, we can get

1
8T N

(cid:88)

(cid:88)

t

i

E(cid:107)∇F (wi

t)(cid:107)2 ≤

E[F ( (cid:101)w0)] − F ∗
ηT

+ 2ηLΓ2

G +

ηL
bN 2

(cid:88)

σ2
i +

i

8η2(1 − γ2)
γ2

L2G2E2

(18)

+ 2η2L2G2E2.

By setting η = C√
T
we can prove Theorem 1.

and substituting into the above equation,

According to the result of the above formula, we can further
deduce that

(cid:18) 1

λ exp(ΛF k)
k2

(cid:19)(cid:48)(cid:48)

= −2

1
λ

ΛF k−3 exp(ΛF k)

+

Λ2

F k−2 exp(ΛF k) + 6

1
λ

k−4 exp(ΛF k)

(21)

ΛF k−3 exp(ΛF k)
(cid:18)

ΛF k−4 exp(ΛF k)

−4k + ΛF k2 +

(cid:19)

.

6
ΛF

1
λ
1
λ

− 2

=

1
λ

Considering ΛF > 0 and (−4)2 − 4 ∗ ΛF ∗ 6
ΛF
(cid:16) 1

(cid:17)(cid:48)(cid:48)

< 0, so

> 0, which means that

the third term in

λ exp(ΛF k)
k2

Eq. (19) is convex. Therefore, h(k,λ)
in the objective function
J 2(k, λ) is convex when λ is ﬁxed since we can prove that
ΛP exp(ΛF k)
k

k2

is convex in the same way.
Identically, we can prove that h(k,λ)
tion J 2(k, λ) is convex when λ is ﬁxed.

4
3

k

in the objective func-

Through the above analysis, Theorem 2 is proved.

REFERENCES

[1] B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas,
“Communication-efﬁcient learning of deep networks from decentralized
data,” in Artiﬁcial Intelligence and Statistics (AISTATS), 2017, pp. 1273–
1282.

[2] X. Li, K. Huang, W. Yang, S. Wang, and Z. Zhang, “On the Convergence
of FedAvg on Non-IID Data,” in International Conference on Learning
Representations (ICLR), 2020, pp. 1–26.

[3] H. Yang, M. Fang, and J. Liu, “Achieving Linear Speedup with Partial
Worker Participation in Non-IID Federated Learning,” in International
Conference on Learning Representations (ICLR), 2021, pp. 1–23.
[4] S. R. Pokhrel and J. Choi, “Federated learning with blockchain for au-
tonomous vehicles: Analysis and design challenges,” IEEE Transactions
on Communications (TCOM), vol. 68, no. 8, pp. 4734–4746, 2020.
[5] E. Bagdasaryan, A. Veit, Y. Hua, D. Estrin, and V. Shmatikov, “How to
backdoor federated learning,” in International Conference on Artiﬁcial
Intelligence and Statistics (AISTATS), 2020, pp. 2938–2948.

[6] H. Kim, J. Park, M. Bennis, and S.-L. Kim, “Blockchained on-device
federated learning,” IEEE Communications Letters, vol. 24, no. 6, pp.
1279–1283, 2019.

[7] S. Otoum, I. Al Ridhawi, and H. T. Mouftah, “Blockchain-supported
federated learning for trustworthy vehicular networks,” in Global Com-
munications Conference (GLOBECOM).

IEEE, 2020, pp. 1–6.

[8] X. Bao, C. Su, Y. Xiong, W. Huang, and Y. Hu, “Flchain: A blockchain
for auditable federated learning with trust and incentive,” in International
Conference on Big Data Computing and Communications (BIGCOM).
IEEE, 2019, pp. 151–159.

[9] L. Feng, Y. Zhao, S. Guo, X. Qiu, W. Li, and P. Yu, “Blockchain-
based Asynchronous Federated Learning for Internet of Things,” IEEE
Transactions on Computers (TC), pp. 1–1, 2021.

[10] D. C. Nguyen, M. Ding, Q.-V. Pham, P. N. Pathirana, L. B. Le,
A. Seneviratne, J. Li, D. Niyato, and H. V. Poor, “Federated learning
meets blockchain in edge computing: Opportunities and challenges,”
IEEE Internet of Things Journal (IoTJ), 2021.

[11] S. Xuan, M. Jin, X. Li, Z. Yao, W. Yang, and D. Man, “DAM-SE:
A Blockchain-Based Optimized Solution for the Counterattacks in the
Internet of Federated Learning Systems,” Security and Communication
Networks (SCN), vol. 2021, 2021.

[12] F. Wilhelmi, L. Giupponi, and P. Dini, “Blockchain-enabled Server-less

Federated Learning,” arXiv preprint arXiv:2112.07938, 2021.

[13] S. U. Stich, J.-B. Cordonnier, and M. Jaggi, “Sparsiﬁed SGD with mem-
ory,” in Advances in Neural Information Processing Systems (NIPS),
2018, pp. 4447–4458.

[14] A. F. Aji and K. Heaﬁeld, “Sparse Communication for Distributed
Gradient Descent,” in Conference on Empirical Methods in Natural
Language Processing (EMNLP), 2017, pp. 440–445.

[15] S. Wang, T. Tuor, T. Salonidis, K. K. Leung, C. Makaya, T. He, and
K. Chan, “Adaptive federated learning in resource constrained edge com-
puting systems,” IEEE Journal on Selected Areas in Communications
(JSAC), vol. 37, no. 6, pp. 1205–1221, 2019.

[16] C. T. Dinh, N. H. Tran, M. N. Nguyen, C. S. Hong, W. Bao, A. Y.
Zomaya, and V. Gramoli, “Federated learning over wireless networks:
Convergence analysis and resource allocation,” IEEE/ACM Transactions
on Networking (TON), vol. 29, no. 1, pp. 398–409, 2021.

[17] C. Hu, J. Jiang, and Z. Wang, “Decentralized federated learning: A
segmented gossip approach,” arXiv preprint arXiv:1908.07782, pp. 1–7,
2019.

[18] B. Luo, X. Li, S. Wang, J. Huang, and L. Tassiulas, “Cost-Effective
Federated Learning Design,” in International Conference on Computer
Communications (INFOCOM).

IEEE, 2021, pp. 1–10.

[19] M. Shayan, C. Fung, C. J. M. Yoon, and I. Beschastnikh, “Biscotti: A
Blockchain System for Private and Secure Federated Learning,” IEEE
Transactions on Parallel and Distributed Systems (TPDS), vol. 32, no. 7,
pp. 1513–1525, 2021.

[20] H. Chai, S. Leng, Y. Chen, and K. Zhang, “A Hierarchical Blockchain-
Enabled Federated Learning Algorithm for Knowledge Sharing in In-
ternet of Vehicles,” IEEE Transactions on Intelligent Transportation
Systems (TITS), vol. 22, no. 7, pp. 3975–3986, 2021.

[21] Y. Lu, X. Huang, K. Zhang, S. Maharjan, and Y. Zhang, “Low-latency
federated learning and blockchain for edge association in digital twin
empowered 6G networks,” IEEE Transactions on Industrial Informatics
(TII), vol. 17, no. 7, pp. 5098–5107, 2020.

[22] J. Zhao, X. Wu, Y. Zhang, Y. Wu, and Z. Wang, “A Blockchain Based
Decentralized Gradient Aggregation Design for Federated Learning,”
in International Conference on Artiﬁcial Neural Networks (ICANN).
Springer, 2021, pp. 359–371.

14

[23] S. Yuan, B. Cao, M. Peng, and Y. Sun, “ChainsFL: Blockchain-
driven Federated Learning from Design to Realization,” in Wireless
Communications and Networking Conference (WCNC).
IEEE, 2021,
pp. 1–6.

[24] Y. Lin, S. Han, H. Mao, Y. Wang, and B. Dally, “Deep Gradient
Compression: Reducing the Communication Bandwidth for Distributed
Training,” in International Conference on Learning Representations
(ICLR), 2018, pp. 1–14.

[25] L. Cui, X. Su, Y. Zhou, and L. Zhang, “ClusterGrad: Adaptive Gradient
Compression by Clustering in Federated Learning,” in GLOBECOM
2020-2020 IEEE Global Communications Conference (GLOBECOM).
IEEE, 2020, pp. 1–7.

[26] L. Li, D. Shi, R. Hou, H. Li, M. Pan, and Z. Han, “To Talk or to Work:
Flexible Communication Compression for Energy Efﬁcient Federated
Learning over Heterogeneous Mobile Edge Devices,” in International
Conference on Computer Communications (INFOCOM).
IEEE, 2021,
pp. 1–12.

[27] A. M. Abdelmoniem and M. Canini, “DC2: Delay-aware Compression
Control for Distributed Machine Learning,” in International Conference
on Computer Communications (INFOCOM).

IEEE, 2021, pp. 1–10.

[28] F. Sattler, S. Wiedemann, K.-R. M¨uller, and W. Samek, “Robust and
Communication-Efﬁcient Federated Learning From Non-i.i.d. Data,”
IEEE Transactions on Neural Networks and Learning Systems (TNNLS),
vol. 31, no. 9, pp. 3400–3413, 2020.

[29] D. Alistarh, D. Grubic, J. Li, R. Tomioka, and M. Vojnovic, “QSGD:
Communication-efﬁcient SGD via gradient quantization and encoding,”
in Advances in Neural Information Processing Systems (NIPS), 2017,
pp. 1709–1720.

[30] A. T. Suresh, X. Y. Felix, S. Kumar, and H. B. McMahan, “Distributed
mean estimation with limited communication,” in International Confer-
ence on Machine Learning (ICML), 2017, pp. 3329–3337.

[31] W. Wen, C. Xu, F. Yan, C. Wu, Y. Wang, Y. Chen, and H. Li, “TernGrad:
Ternary Gradients to Reduce Communication in Distributed Deep Learn-
ing,” in Annual Conference on Neural Information Processing Systems
(NIPS), 2017, pp. 1509–1519.

[32] L. Cui, X. Su, Y. Zhou, and Y. Pan, “Slashing Communication Trafﬁc
in Federated Learning by Transmitting Clustered Model Updates,” IEEE
Journal on Selected Areas in Communications (JSAC), vol. 39, no. 8,
pp. 2572–2589, 2021.

[33] P. Han, S. Wang, and K. K. Leung, “Adaptive Gradient Sparsiﬁcation
for Efﬁcient Federated Learning: An Online Learning Approach,” in
International Conference on Distributed Computing Systems (ICDCS),
2020, pp. 300–310.

[34] S. Nakamoto, “Bitcoin: A peer-to-peer electronic cash system,” Decen-

tralized Business Review, p. 21260, 2008.

[35] J. Li, Y. Shao, K. Wei, M. Ding, C. Ma, L. Shi, Z. Han, and H. V. Poor,
“Blockchain Assisted Decentralized Federated Learning (BLADE-FL):
Performance Analysis and Resource Allocation,” IEEE Transactions on
Parallel and Distributed Systems (TPDS), vol. 33, no. 10, pp. 2401–
2415, 2022.

[36] X. Deng, J. Li, C. Ma, K. Wei, L. Shi, M. Ding, W. Chen, and H. V. Poor,
“On Dynamic Resource Allocation for Blockchain Assisted Federated
Learning over Wireless Channels,” arXiv preprint arXiv:2105.14708, pp.
1–8, 2021.

[37] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, “Gradient-based learning
applied to document recognition,” Proceedings of the IEEE, vol. 86,
no. 11, pp. 2278–2324, 1998.

[38] S. Targ, D. Almeida, and K. Lyman, “Resnet in resnet: Generalizing
residual architectures,” in International Conference on Learning Repre-
sentations (ICLR) Workshop, 2016.

[39] H. Gao, A. Xu, and H. Huang, “On the Convergence of Communication-
Efﬁcient Local SGD for Federated Learning,” in AAAI Conference on
Artiﬁcial Intelligence (AAAI), vol. 35, no. 9, 2021, pp. 7510–7518.
[40] H. Yu, S. Yang, and S. Zhu, “Parallel restarted SGD with faster con-
vergence and less communication: Demystifying why model averaging
works for deep learning,” in AAAI Conference on Artiﬁcial Intelligence
(AAAI), 2019, pp. 5693–5700.

[41] H. Wang, Z. Kaplan, D. Niu, and B. Li, “Optimizing Federated Learning
on Non-IID Data with Reinforcement Learning,” in IEEE Conference on
Computer Communications (INFOCOM), 2020, pp. 1698–1707.
[42] Y. Zhao, M. Li, L. Lai, N. Suda, D. Civin, and V. Chandra, “Federated
learning with non-iid data,” arXiv preprint arXiv:1806.00582, pp. 1–13,
2018.

[43] D. Basu, D. Data, C. Karakus, and S. N. Diggavi, “Qsparse-Local-
SGD: Distributed SGD With Quantization, Sparsiﬁcation, and Local
Computations,” IEEE Journal on Selected Areas in Information Theory
(JSAIT), vol. 1, no. 1, pp. 217–226, 2020.

15

Xiaoxin Su received his bachelor’s degree from
Shenzhen University in 2020. He is studying for a
master’s degree at Shenzhen University. His research
interests include Federated Learning and Edge Com-
puting.

Yipeng Zhou is a senior lecturer in computer sci-
ence with School of Computing at Macquarie Uni-
versity, and the recipient of ARC DECRA in 2018.
From Aug. 2016 to Feb. 2018, he was a research
fellow with Institute for Telecommunications Re-
search (ITR) of University of South Australia. From
2013.9-2016.9, He was a lecturer with College of
Computer Science and Software Engineering, Shen-
zhen University. He was a Postdoctoral Fellow with
Institute of Network Coding (INC) of The Chinese
University of Hong Kong (CUHK) from Aug. 2012
to Aug. 2013. He won his PhD degree and Mphil degree from Information
Engineering (IE) Department of CUHK respectively. He got Bachelor degree
in Computer Science from University of Science and Technology of China
(USTC). His research interests lie in distributed/federated learning, privacy
protection and caching algorithm design in networks. He has published more
than 90 papers including IEEE INFOCOM, ICNP, IWQoS, IEEE ToN, JSAC,
TPDS, TMC, TMM, etc.

[44] J. Gorski, F. Pfeuffer, and K. Klamroth, “Biconvex sets and optimiza-
tion with biconvex functions: a survey and extensions,” Mathematical
methods of operations research, vol. 66, no. 3, pp. 373–407, 2007.
[45] K. Lai and M. Baker, “Measuring bandwidth,” in International Confer-
IEEE, 1999,

ence on Computer Communications (INFOCOM), vol. 1.
pp. 235–245.

[46] S. Caldas, S. M. K. Duddu, P. Wu, T. Li, J. Koneˇcn`y, H. B. McMahan,
V. Smith, and A. Talwalkar, “Leaf: A benchmark for federated settings,”
arXiv preprint arXiv:1812.01097, pp. 1–9, 2018.

[47] H. Wang, Z. Kaplan, D. Niu, and B. Li, “Optimizing Federated Learn-
ing on Non-IID Data with Reinforcement Learning,” in International
Conference on Computer Communications (INFOCOM).
IEEE, 2020,
pp. 1698–1707.

[48] Z. Zhong, Y. Zhou, D. Wu, X. Chen, M. Chen, C. Li, and Q. Z.
Sheng, “P-FedAvg: Parallelizing Federated Learning with Theoretical
Guarantees,” in International Conference on Computer Communications
(INFOCOM).

IEEE, 2021, pp. 1–10.

[49] N. H. Tran, W. Bao, A. Zomaya, M. N. Nguyen, and C. S. Hong, “Fed-
erated learning over wireless networks: Optimization model design and
analysis,” in International Conference on Computer Communications
(INFOCOM).

IEEE, 2019, pp. 1387–1395.

Laizhong Cui is currently a Professor in the College
of Computer Science and Software Engineering at
Shenzhen University, China. He received the B.S.
degree from Jilin University, Changchun, China, in
2007 and Ph.D. degree in computer science and tech-
nology from Tsinghua University, Beijing, China,
in 2012. His research interests include Future In-
ternet Architecture and Protocols, Edge Computing,
Multimedia Systems and Applications, Blockchain,
Internet of Things, Cloud and Big Data Computing,
Computational Intelligence and Machine Learning.
He led more than 10 scientiﬁc research projects, including National Key Re-
search and Development Plan of China, National Natural Science Foundation
of China, Guangdong Natural Science Foundation of China and Shenzhen
Basic Research Plan. He has published more than 100 papers, including IEEE
JSAC, IEEE TC, IEE TKDE, IEEE TMM, IEEE IoT Journal, IEEE TII,
IEEE TVT, IEEE TNSM, ACM TOIT, IEEE TCBB, IEEE Network, IEEE
INFOCOM, ACM MM, etc. He serves as an Associate Editor or a Member of
Editorial Board for several international journals, including IEEE IoT Journal,
IEEE Transactions on Network and Service Management, and International
Journal of Machine Learning and Cybernetics. He is a Senior Member of the
IEEE, and a Senior Member of the CCF.

