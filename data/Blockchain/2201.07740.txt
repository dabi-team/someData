More is Merrier in Collusion Mitigation

Tiantian Gong†, Ryan Henry‡,Alexandros Psomas†, Aniket Kate†
† Purdue University, Email: {tg, apsomas, aniket}@purdue.edu
‡ University of Calgary, Email: ryan.henry@ucalgary.ca

2
2
0
2

g
u
A
3
2

]

R
C
.
s
c
[

3
v
0
4
7
7
0
.
1
0
2
2
:
v
i
X
r
a

Abstract—For privacy-preserving solutions involving many
servers, assuming that they do not collude with each other
makes some secrecy problems solvable and reduces overheads
and computational hardness assumptions in others. While the
non-collusion assumption is pervasive among secure distributed
systems,
it remains highly susceptible to covert, undetectable
collusion among computing parties. This work stems from an
observation that if the number of available computing parties is
much higher than the number of parties required to perform a
secure computation, collusion attempts could be deterred.

We focus on the standard distributed protocol of multi-server
private information retrieval (PIR) that inherently assumes that
servers do not collude. For PIR application scenarios, such as
those for blockchain light clients, where the available servers can
be plentiful, a single server’s deviating action is not tremendously
beneﬁcial to itself. We can make deviations undesired via small
amounts of rewards and penalties, thus raising the bar for
collusion signiﬁcantly. For any given multi-server 1-private PIR
(i.e., the base PIR system is constructed assuming no pairwise
collusion), we design and implement a collusion mitigation
mechanism considering Byzantine and rational deviations. We
ﬁrst deﬁne a two-stage sequential game that captures how
rational servers interact with each other during collusion, then
determine the payment rules such that the game realizes the
unique sequential equilibrium: a non-collusion outcome. We also
offer privacy protection for an extended period after the query
executions and guarantee user compensation in case of a reported
privacy breach.

I. INTRODUCTION
A non-collusion assumption among computing parties is
prevalent across privacy-preserving technologies. It allows us
to solve some important problems [18], [22], [45], with no
known solutions without it. It also reduces overheads and
allows us to bypass computational hardness results in other
problems such as private information retrieval (PIR) [16].
the entire ﬁeld of secure multi-party computation
In fact,
(MPC) [8], [66] relies on the non-collusion assumption.

While most of the privacy-preserving distributed systems
above do allow collusion among pre-deﬁned subsets of parties,
all bets are off once the adversary can lure a superset of
parties. Indeed, using the non-collusion assumption remains
highly susceptible to undetectable collusion among the com-
puting parties: from a legal perspective, while collusion can
laws, collusion over Internet-
be penalized using antitrust
based secure systems can be difﬁcult to prove, if possible;
parties may collude without recognizing each other. As these
distributed systems are getting considered for applications
such as privacy-preserving machine learning (e.g., [41], [50]),
privacy-preserving database/directory searches [49], [54], and
blockchain privacy (e.g., [37], [55]), non-collusion is consid-
ered an Achilles’ heel for large adoption.

In the context of database querying service, a PIR protocol
allows a client to query a database without revealing which
piece of information is of interest. Chor et al. [16] formalize
the k ≥ 2 server PIR problem and proposed a mechanism for
fetching a single bit of an N -bit string, stored at (cid:96) ≥ k servers
(or parties/players), without disclosing the bit’s location to the
k queried servers. More generally, we can consider a setting
where (cid:96) servers store an identical database D comprising N
entries and offer data retrieval services for each entry.

√

PIR can be constructed from a single server and multiple
servers. Single server information-theoretic PIR or IT-PIR
achieves information-theoretic privacy and is not possible
beyond the trivial solution of sending the whole database to
the user. Computational PIR or cPIR [44] approaches this
problem by including computational hardness assumptions
for problems like quadratic residuosity. The computational
inefﬁciency of single-server cPIR schemes is well-known. The
state-of-the-art proposals [5], [20], [38] usually have O(
N )
communication complexity. OnionPIR [51] achieves O(log N )
communication costs, but it incurs heavy computation costs.
(Detailed efﬁciency comparisons reside in Appendix E.) By
involving more non-colluding servers (k ≥ 2), the communica-
tion and computation complexity can be substantially reduced,
which serves as a motivation for multi-server PIR schemes. For
example, in Haﬁz-Henry [35]’s multi-server PIR, the query
size is 130 log k((cid:100)log N (cid:101) − 7) bits and the total response size
k
k−1 x overhead. It has the lowest possible computation
has
cost [7]: for each bit being fetched, each server performs N
k−1
bit operations on the database in expectation. The throughput
is large: for 2 (or 26) queried servers, servers can process
the database at a rate around 5.7 (or 144) GiB/s. But the
key concern is that in reality, servers can easily collude over
various covert and anonymous channels [58], and the collusion
remains oblivious to PIR executions.

The difﬁculty of privacy-related collusion detection lies in
the fact that collusion can be orthogonal to protocol runs. The
problem is impossible to solve with the multi-server PIR pro-
tocol itself assuming secure communication channels or side
channels. Dong et al. [23] explore discouraging correctness-
related collusion in cloud computing. The key element is to
have colluding parties collude via a smart contract which can
be veriﬁed by another contract. Correctness naturally implies
veriﬁability and there, collusion is implemented with smart
contracts completely observable by others. However, privacy-
pertinent collusion can happen anywhere (e.g., outside the
system under discussion), any time (e.g., long after the run

 
 
 
 
 
 
of the protocol), and its existence is hard to verify. We follow
the intuition from a bit guessing game and seek to derive an
incentive design to achieve a non-collusion outcome.
Secret-Shared Bit Guessing Game. Consider a game where
the game host commits to a secret unbiased bit b and players
can make one open guess about b. The correctness of guesses
is announced after no more player wants to guess. A player
receives $2 for a correct guess and loses $4 for a wrong guess.
Rational players do not participate because the expected return
is negative ($-1). Suppose the host secret-shares the bit among
k parties and if the game rules stay the same, players can
collaborate to recover the bit and make correct guesses. The
collaboration is oblivious to the game.

To stop collusion, the host can change the rules to reward
the player that makes the ﬁrst correct guess with $2 and take
$4 from other k − 1 players, of which an additional $2 goes to
this player. Assuming equal speed of submitting a guess when
players are ready to guess, not colluding is strictly dominating
(i.e., generating strictly more returns) for k > 2 and weakly
dominating for k = 2. This implies that it is possible to make
collusion visible and undesired through a proper incentive
structure.

This simple game is already expressive, but it also contains
caveats that can cause the incentives to stop being effective in
practice, e.g., a malicious host, a player with private knowl-
edge about b, etc. Proper designing of the structure and de-
termination of the incentive-related parameters is crucial. We
observe that in many settings where the number of available
computing parties can be much higher than the required set,
an effective incentive structure can be constructed and made
practical. While this observation has extensive applicability,
this work focuses on the problem of mitigating collusion in
multi-server PIR protocols.

A. Contributions

We study an incentive structure to mitigate the collusion
problem in the multi-server PIR systems where at least two
of the (cid:96) servers are rational, with the remaining parties being
Byzantine (i.e., malicious). Unlike honest parties who follow
the default protocol, rational parties take actions that maximize
their utilities and Byzantine parties can behave arbitrarily. The
high-level idea of our solution is motivating rational colluders
to report collusion with veriﬁable evidence. Incentivizing
colluders to make such a desired move is one sub-problem.
Deﬁning and verifying evidence is the other. On top of the
two, we need to ensure the practicality of the design, e.g.,
reasonable payment amounts. Incentive issue is resolved via
a game-theoretic design, with intuitions from the bit-guessing
game; evidence veriﬁcation is settled cryptographically; eco-
nomic feasibility is achieved by having many available servers.
Overall, we make the following contributions.

First, we present a game-theoretic collusion deterrence
mechanism for multi-server PIR assuming many servers
being present so that one can enjoy the effectively perfect

efﬁciency of 1-private PIR. 1 Delegating collusion mitiga-
tion to our proposed mechanism external to PIR protocols
imposes little additional efﬁciency cost. The mechanism in-
duces a sequential game, where the non-colluding outcome
is the sequential equilibrium. We consider general collusion
protocols where parties can compute arbitrary functions on
received queries or inputs derived from them via a secure MPC
protocol, e.g, reconstructing the queried entry from responses.
They hold arbitrary private knowledge about clients and can
collude over any side channels. To achieve a practical incentive
structure, we have many servers and let clients send companion
queries. In this way, a single server does not bear too much
information worth to leak before it is discouraged to collude
(similar to when the players hesitate to recover the target bit
in the bit-guessing game). We also tackle manipulations of the
mechanism from parties of information advantage, e.g., users
or well-informed servers.

Second, we ensure longer-term privacy. Collusion can
happen anytime after the queries, while the incentive structure
is constructed for a single run of a PIR protocol. To ensure
privacy for an extended period, a privacy protection self-
insurance is constructed. Due to the ﬁnite nature of the
insurance pool, we set expiration dates for privacy protection.

Third, we extend the discussion to coalitional players.
Servers inside a coalition/clique have complete trust for each
member, thus immune to our incentive structure, and this is
where the difﬁculty lies. If the group size is smaller than
the number of queries needed for reconstruction, the problem
is reduced to the individual player case. For a larger group
size, we analyze the problem from a cooperative game theory
perspective and identify the conditions necessary to keep the
group size small.

Last, we implement and deploy the coordinator as a smart
contract on Ethereum. The contract instantiates the game rules,
maintains essential data, and resolves collusion accusations
through a normal or zero-knowledge veriﬁer. We apply opti-
mization to economize the gas costs, whose estimates reside
in Table I.

Efﬁciency. When at least two queried parties are rational and
the rest all (≤ k − 2) parties are Byzantine, the mechanism
induces the non-collusion outcome among the servers. The
coordinator does not need to be executed, thus preserving
the efﬁciency of the underlying 1-private PIR scheme. When
≥ k − 1 queried parties are Byzantine, the mechanism is
ineffective in stopping collusion because Byzantine servers
do not respond to incentives. The goal then is to avoid the
mechanism being exploited and ensure that this only adds
limited overhead (Appendix D).

11-private PIR schemes are constructed assuming no pairwise collusion and
provide effectively perfect efﬁciency in terms of downloading capacity, in the
sense that to fetch a b-bit entry, one only needs to download b + 1 bits [57]
(by having exponentially many servers). Haﬁz-Henry’s construction [35] of
k
k−1 b
1-private multi-server PIR achieves the best possible download cost,
bits, for a ﬁxed number of servers.

2

II. PRELIMINARIES AND OVERVIEW

B. Threat model

A. Preliminaries

Multi-server PIR. We follow the multi-server PIR deﬁni-
tion by Haﬁz et al. [35]. Let I, Q, R, Y respectively denote
the random variables for the entry index i ∈ [N ] of interest,
the query string q ∈ {0, 1}∗, the response string r ∈ {0, 1}∗
and the output y ∈ D reconstructed by the user.

Deﬁnition 1. The interaction described by (I, Q, R, Y ) pro-
vides correctness if Pr[Y = Di|I = i] = 1 where Di is the
i-th entry in database D.

The interaction described by (I, Q, R, Y ) provides perfect
1-privacy if ∀i, i∗ ∈ [N ], Pr[Q = q|I = i] = Pr[Q = q|I =
i∗] where the probability is over all the random coin tosses
made by the client.

The interaction described by (I, Q, R, Y ) provides compu-
tational 1-privacy if ∀i, i∗ ∈ [N ], the distribution ensembles
{Q|I = i}λ∈N and {Q|I = i∗}λ∈N are computationally
indistinguishable with security parameter λ.

We focus on k-out-of-k and 1-private PIR schemes. We
denote the multi-server PIR scheme as ((cid:96), k, 1)-PIR. A set of (cid:96)
servers S = {Si|i ∈ [(cid:96)]} maintain identical databases consist-
ing of N entries, which we represent with D = {Di|i ∈ [N ]}.
In each round, to query the entry at index a, user U generates k
queries and sends them to k (distinct) servers, which we denote
as SM ⊆ S (|SM | = k). Each queried server Sj ∈ SM locally
computes results corresponding to the query and responds
with answer string Aj. U then collects all k responses and
reconstructs Da.

There is an abundance of available PIR constructions,
including the ones for two servers [10], [16], [24], [56], [64],
and three or more servers [4], [6], [15], [25], [35], [39], [43],
[59], [67]. While all constructions sufﬁce for the purpose of
this work, we ﬁnd Boyle-Gilboa-Ishai’s construction for 2-
server cPIR [10] and Haﬁz and Henry’s construction for more
server case [35] to be suitable. Both constructions realize PIR
as a distributed point function and as elaborated in Appendix
E, are highly efﬁcient for million-sized databases.
Relevant concepts in game theory [52]. A normal-form
game can be characterized with a set of players P, actions
available to players A = ×i∈P Ai, and utility functions V =
{u1,· · · , u|P|} where ui : A (cid:55)→ R, mapping action proﬁles
to real-numbered utilities. Let s ∈ A be a strategy proﬁle,
which is simply a sample in universe A. A dominant strategy
si for a player Pi generates utility at least as good as any other
strategies: ∀s(cid:48)
i, s−i) where s−i
is a strategy proﬁle for all other players. If equality never
holds, then si is a strictly dominant strategy.

i (cid:54)= si, ∀s−i, ui(si, s−i) ≥ ui(s(cid:48)

A solution concept captures the equilibrium strategy proﬁle
for all players in a game. Nash equilibrium is one solution
concept where no player can increase her utility by unilaterally
deviating from the equilibrium strategy proﬁle. Our incentive
structure induces a sequential game and the solution concept
we employ is sequential equilibrium, which we discuss in more
detail in Section III-A.

We assume a threshold θ of Byzantine servers bounded with
respect to a statistical security parameter η such that the failure
probability is ≤ 1 − 2−η. As mentioned, while Byzantine
servers behave arbitrarily irrespective of their incentives, all
other servers are rational who take actions that maximize
their utilities. There are no honest servers. When more servers
are malicious, we aim to not
let malicious nodes exploit
the proposed scheme (More details reside in Appendix D).
Normally, servers provide PIR services and are compensated
by the system in the form of service fees. They can potentially
collude with each other to break user privacy and obtain
what the recovered private information is worth. Servers can
collude as individuals (Section III,IV), or form coalitions
(Section VII), or leave the service (Section VI).

To make the results more general, we handicap ourselves
by giving the adversary advantages in collusion. If these ad-
vantageous settings are not achieved in practice, the result still
holds. We allow servers to communicate with each other over
two-way anonymous secret channels, which provide sender
and receiver anonymity and keep the communication covert
and undetectable. Further, we assume the communication is
simulatable such that a server can generate the communication
transcripts on its own, and colluding servers can self-protect
by denying having the conversation. This is because the
correspondence scripts may not contain irrefutable proofs like
secure digital signatures. The nature of the communication
can be hard to automatically determine even if signatures
are present. If a server decides to collude, we assume there
exists a secure fair MPC protocol allowing colluding parties to
compute the result collectively. We consider the worst-case fair
collusion protocols because otherwise, parties not receiving
information gain in collusion are at a natural disadvantage,
thus not incentivized to participate: they provide private inputs
but are limited to no other actions besides offering inputs.

We assume the underlying PIR protocol is secure under
its assumptions, which means that it ensures correctness and
privacy (Deﬁnition 1). Each server receives at most one query
in a round (i.e., a complete run of PIR for one entry between
a user and selected servers). Servers hold a common prior
probability distribution p = (p1, ..., pN ) over all N entries
in database D. Servers can have arbitrary private information
about a user’s queried index.

C. Solution overview

We now describe the solution and present pseudocodes in
Figure 1. Servers are indexed and participants all know the
indices. Due to the nature of our discussion, we also address
servers as players or parties.
System. There are (cid:96) servers maintaining the same database
D. In the beginning, servers make deposits to a coordinator
contract (CC) which will be transferred back when servers
exit the PIR service. A user picks at random k servers and
ω ≥ 1 random entries Dr1, . . . , Drω . The user then generates
k queries for the intended entry Da for index a and for each
of the ω random entries (we call them companion queries).

3

Normal Service (for querying Da)

k(cid:105))

1, σ(cid:48)

i) (i ∈ [k]).

1(cid:105), . . . , (cid:104)c(cid:48)

1, . . . , Q(cid:48)
k.

i = CpkU (Q(cid:48)

k, σ(cid:48)
i), σi = SignpkU (ci)

1) User U runs one instance of PIR protocol for querying
index a and generates k queries, Q1, . . . , Qk.
2) User U samples an index ar uniformly at random, runs
one instance of PIR protocol for ar and generates k queries,
Q(cid:48)
3) User U commits to the 2k queries, and obtain
c = ((cid:104)c1, σ1(cid:105), . . . , (cid:104)ck, σk(cid:105)) and c(cid:48) = ((cid:104)c(cid:48)
where ci = CpkU (Qi), c(cid:48)
and σ(cid:48)
i = SignpkU (c(cid:48)
4) U samples k distinct integers in [(cid:96)] at random and obtains
k corresponding server indices, id = (id1, . . . , idk). U signs
and posts id, signature σid = SignpkU (id) onto BB.
5) U picks a permutation Π at random and posts Π(c, c(cid:48)) to
BB. U stores permutation Π locally.
6) U sends to server idi (i ∈ [k]) the corresponding
de-commit information over any secure channels.
7) Server idi veriﬁes and de-commits the i-th commitments,
retrieves the two queries, and computes the answers
Π(Ai, A(cid:48)
8) Server idi commits, signs the message and posts
Π((cid:104)cidi , σidi (cid:105), (cid:104)c(cid:48)
cidi = Cpkidi
and σ(cid:48)
to U over any secure channels.
9) U veriﬁes and de-commits the commitments, retrieves
responses, and reconstructs queried entry Da.

idi , σ(cid:48)
idi (cid:105)) onto BB where
(Ai), c(cid:48)
(cidi )
idi = Cpkidi
(c(cid:48)
idi ). idi sends de-commit information

i) locally against database D.

i), σi = Signpkidi

i = Signpkidi

(A(cid:48)

Collusion Resolution (Server idi accusing)

1) Server idi submits a collusion report to CC, (type, ed),
indicating evidence type and evidence ed. The evidence either
includes (I) information exchanged or (II) inputs used in
collusion, circuits for computing f (·) and outputs from MPC.
2) Accused server(s) idj provide auxiliary info:
• Type I-query: de-commit info for Π(cj, c(cid:48)
j)
• Type I-response: de-commit info for Π(cidj , c(cid:48)
• Type II: de-commit info for all possible input(s) to f (·)
3) If the accused server(s) fails to submit the proof in time,
CC conﬁrms the accusation. Otherwise:
• Type I: CC opens the commitment on BB with the

idj )

de-commit info. If one of the revealed values matches the
evidence ed, conﬁrm.

• Type II: if the de-commit info is incorrect or if the output

of f (·) on de-committed inputs match ed, conﬁrm.

Fig. 1: Overview of routines (ω = 1 companion query). We
omit token transfers and address servers by their IDs. Sign(·)
is a secure signature scheme. C(·) is a perfectly hiding and
computationally binding commitment scheme.

Without companion queries, we cannot distinguish between
the world where the queried servers collude to recover the
secret and another world where a queried server has complete
private knowledge of the secret. This forms k vectors of
queries. The user then randomly permutes the order of queries
in each vector and posts commitments of queries along with
signatures onto a bulletin board (BB). The client then sends
de-commit information to the k selected servers. The client
also posts the queried server list onto BB.

4

Queried servers post commitments of responses to BB
and send de-commit information to the user over any se-
cure channels. The user can reconstruct after accumulating
the responses. This work is not speciﬁcally concerned with
servers’ actions that disrupt integrity, including sending wrong
responses or not responding. But with signatures on messages,
it is straightforward to see that these two deviations can be
mitigated with slight changes to the system, e.g., giving CC
access to database D. More discussion resides in Section V-A.
How collusion proceeds. Queried servers participating in a
collusion protocol can compute some non-trivial (i.e., output
depends on the inputs) function f (·) on queries or responses
with a secure multi-party computation (MPC) protocol. They
provide secret information received from clients as inputs and
obtain the output. For example, they can input the received
queries to compute the queried index or its least signiﬁcant
bit. One special case is that f (·) is a fair exchange function
that allows servers to exchange information and later perform
any computations locally.
Intuition behind discouraging collusion. We establish a
reporting mechanism that selects the ﬁrst correct reporting
server for a PIR instance as the winner. The payment rules
include (1) charging reporting or accusation fees from servers
submitting reports; (2) distributing rewards to the winner; (3)
ﬁning colluding servers who do not report; (4) charging service
fees from clients. (1) The accusation fee is to discourage
servers from submitting false reports. (2) Rewarding the ﬁrst
reporter is to encourage the servers participating in collusion to
reveal its existence. (3) Fining the rest of the colluding group
is to discourage at least one party in the colluding group who
does not have the network advantage to be the ﬁrst reporter
from collusion. (4) Collecting service fees from clients is ﬁrst
to stop clients from exploiting the mechanism and causing
undue punishment on servers, and second to determine the
length of the longer-term privacy protection period.
Collusion Mitigation Coordinator. The coordinator algo-
rithm executes payments involved in both normal service and
collusion report resolution and veriﬁes collusion evidence. One
convenient implementation of coordinator is a smart contract
on a public blockchain system. We give more details in
Section V after exploring the sequential game.

The accuser provides one of the following entries as ev-
idence: (Type I) outputs from f (·) when it is an exchange
function; (Type II) the circuit for computing function f (·) (not
the MPC circuits), her inputs to f (·) and the output computed
from MPC. Type I evidence can be the original query received
by the accused or the query response of the accused. We
separate this subset of evidence from general (i.e., Type II)
ones because its veriﬁcation is much simpler compared with
general f (·).

Upon receiving an accusation, the contract counts down an
auxiliary information collection window. The accused parties
who failed to provide such information justifying the evidence
being false in time are marked as “culpable”. For Type I
evidence, we have the accused provide de-commit information
and check if one of the revealed values matches the evidence.

For Type II evidence, we have the accused server(s) open
the corresponding commitments and reveal inputs to f (·).
We verify correct opening, conﬁrm the non-triviality of f (·)
(details in Section V-A), and compare the output of f (·)
with the evidence. Note that we do not distinguish between
collusion that recovers the desired entry Da and a randomly
picked entry, nor do we reveal index a.

Next, we dive into the key analysis of designing the in-
centive structure to achieve the non-collusion outcome while
accounting for manipulations from both servers with private
in Section III, IV. Section V
knowledge and the client
summarizes the mechanism and presents an implementation.
Section VI, VII provide extended analysis (that does not affect
the design) on adversarial exiting strategies and coalitions.

III. COPING WITH TWO COLLUDING PARTIES
We consider (l, k, 1)-PIR and aim to provide the same pri-
vacy guarantees the backbone PIR implies with our mechanism
while relaxing the non-collusion assumption. Throughout this
section, we let k = 2. We continue to denote the intended
queried entry that can be reconstructed from the responses
as Da. As depicted in Appendix E, in Boyle-Gilboa-Ishai’s
construction, U sends the 2 function and key shares (fi, ki),
(fj, kj) to two servers Si, Sj. If the two collude together,
they may exchange the received shares or exchange responses
(Type I evidence), or compute the entry Da together without
information exchange (generating Type II evidence).

Colluding servers verify the information input by the other
party, e.g., by including circuits that examine the digital
signatures from the user. We consider occasions when servers
implement blind collusion in Appendix B, where they only
infer the authenticity of information but never directly verify
it.

A. A sequential game for (cid:96) = 2 parties
Order of events. We have two servers S1, S2. Consider the
following sequential game G0 in (2, 2, 1)-PIR:
(0) In the initialization round or round zero, server S1, S2
each decides whether to collude (C or ¯C). If < 2 parties
have colluding intention, the game immediately ends.
(1) In round one, each colluding server selects one of two
actions independently and simultaneously: amicable (A),
i.e., exchange the correct information, or deceitful (D),
i.e., send an incorrect or junk message.

(2) In round two, the two servers have exchanged information
or computed nontrivial functions. Crucially, a server
knows whether the other server played D in the previous
round. In this round, each server selects one of two
actions: report the collusion (R) or keep it secret ( ¯R).
Then the game terminates.

Payoffs.
If the game stops in round zero, servers receive
service fees vS. Otherwise, the payoffs are determined by
a payment rule φ(·), which takes as input
the actions of
the agents throughout the game. A server receives vC if he
learns the user’s queried index. Since the colluding servers also
retrieve ω randomly picked entries, the actual information gain

C ∈ [ vC

is worth v∆
ω+1 , vC], depending on the private information
parties hold2. When they report, they pay an accusation fee vA
and get a reporting bonus vR upon a successful accusation but
lose the deposit vI if the report is incorrect. An accused server
in a successful report loses deposit vI .

In more detail, (i) if they both play amicable (A) in round
1, they gain payoff v∆
C from recovering the user’s queried
entry Da. In round 2, when one party reports (R), it pays
an accusation fee vA. It receives reporting bonus vR and the
other party loses their deposit vI upon successful accusation.
This means that φ(A1R1, A2 ¯R2) = (vS + v∆
C + vR − vA, vS +
C − vI ). We abbreviate this strategy notation as (AR, A ¯R)
v∆
hereafter. (ii) If both parties play deceitful (D) in round 1,
they do not receive v∆
C . Other rules stay the same. (iii) If
only one of the two plays amicable (A), only the deceitful
party receives v∆
C for recovering the secret. Other payment
rules stay the same.

The complete payoff tree is shown in Figure 2. We focus on
the part of the tree where collusion can actually happen (the
shaded area in Figure 2). This is because if at least one party
does not participate, the collusion game ends in initialization
and is not effectively started. There are some subtleties when
implementing the payment rules. First, the service fee vS is
not transferred to servers right after a round of PIR service for
a user has been accomplished. Because collusion can happen
long after the service, we set a privacy protection period and
detain service fees until the protection expiration time (See
Section VI for more detailed discussion). Second, collusion
report bonus vR ≤ vI , because rewards for reporters come
from ﬁnes from the accused.

ω

ω+1 + ω

ω+1 + ω

Additionally, the accusation fee vA > ( 1
2(ω+1) )vR −
ω
ω+1 vI , where w is the number of companion queries (w ≥ 1).
Note that this is always satisﬁed for vA > 0, vI ≥ ( 2
ω +1)vR >
0. This indicates that
the upper bound of what one can
expect to gain from false accusations, ( 1
2(ω+1) )vR −
2(ω+1) vI − vA, is negative. We have this maximum expected
proﬁt because, with prior p and arbitrary private knowledge
about the true index a, an informed party can trivially make
successful false accusations with Type II evidence with prob-
ability ≤ 1
w+1 . The success probability is not 1 because the
accuser needs to specify which of the query in (or response
to) the received queries correspond to the one used in collu-
sion. Without actual collusion, one makes a guess. The user
randomly permutes the queries, so the probability of guessing
correctly is
Information sets. The game we have deﬁned is a game of
incomplete information: in the ﬁrst and second rounds, an
agent is not certain about which action the other agent is
taking. An information set for a player Si contains all decision
nodes, at which player Si takes an action, indistinguishable
by Si. Many information sets in our game are trivial, i.e.,
containing a single node. For example, in round 1, both agents

1
ω+1 .

2If they know nothing about the intended index, they gain vC

ω+1 . If they
possess a slight bit of information that allows them to recognize the true
queried index, their gain approaches vC .

5

Fig. 2: Extensive form for 2-party sequential collusion game. Nature decides the types of the two players at node 0, collusion-
prone or collusion-resistant. With probability z, a player is collusion-prone and may participate in collusion, and to consider
the worst case, we let z = 1 until Section IV-B. Each node enveloped with dashed lines is a decision node, with the preﬁx
of its name indicating the owner (e.g., “12” means S1 and S2 make decisions at this node). Actions a server can take are
speciﬁed on the one-directional arrows and the probability for each move is under the arrows in brackets, with αs for S1 and
βs for S2. The payoff vector is (x, y) where x is the returns for S1 and y for S2. In A1A2 colluding case, when they both
report, we apply a 1/2/ probability of either agent reporting ﬁrst and compute vR−vI

expected bonus.

2

are aware of the other agent’s interests in collusion. However,
there are non-trivial information sets, e.g., the ones preﬁxed
“12” in Figure 2, where the two make simultaneous decisions.
Solution concept. The game we have deﬁned is sequential.
Nash equilibria, the standard notion of equilibrium in game
theory, are known to predict “unreasonable” behaviors in
sequential games. In a Nash equilibrium of a sequential game a
player can commit to empty/non-credible threats.3 We consider
a reﬁnement of Nash equilibrium for sequential games, namely
Sequential Equilibrium [42]. In this type of equilibrium, a
player chooses the optimal action (the action that maximizes
expected utility from now on) at each information set, no
matter what has happened in the past. Moreover, a sequential
equilibrium not only prescribes a strategy for each player, but
also a belief about other players’ strategies.

To formalize the notion of sequential rationality, we ﬁrst

formalize the notion of a belief.

Deﬁnition 2. For each information set I, a belief assessment
b gives the conditional probability distribution b(·|I) over all
decision nodes v ∈ I.

For example, b(D2|I A

1 ) is the belief of player S1 that S2

3Consider the Ultimatum game with two players. One player proposes a
way to divide a sum of money between the two players. If the responding
player accepts the proposal, then the money is divided accordingly; otherwise,
both receive nothing. The responder can threaten to accept only fair offers,
but this threat is not credible: given any non-zero amount, the only rational
action for the responder is to accept.

6

takes action D in round one, given that she is in information
set I A
1 = {(A, A), (A, D)}. A belief assessment expresses all
such beliefs for all players at their information sets.

Deﬁnition 3. A strategy proﬁle and belief assessment pair
(s, b) is a sequential equilibrium if (s, b) is sequentially
rational and b is consistent with s.

Here, a strategy proﬁle s speciﬁes the actions of all players.
Given (s, b), s is called sequentially rational
if at each
information set I, the player to take an action maximizes her
expected payoff, given beliefs b(·|I) and that other players
follow s in the continuation (remaining) game. Given a strat-
egy proﬁle s, for any information set I on the path of play of
s, b(·|I) are consistent with s if and only if they are derived
using Bayes’ rule. For example, let player 1 be at information
set I = {x, y}. Suppose according to s, they are respectively
reached with probability 0.2, 0.3 (product of probabilities of
actions along the path). Then a consistent belief for player 1
is that b(x|I) = 0.2
Analysis. As mentioned before, we only consider the game
not ending in the initialization round, since otherwise, collu-
sion does not take place. As noted, the general setting Γ for
2 server collusion in (2, 2, 1)-PIR includes:
Γ(a) Payments are positive and ω
Γ(b) θ = 0 (both servers are rational);
Γ(c) Users do not manipulate the scheme.
Γ(c) is relieved after Section IV-A.

0.2+0.3 = 0.4 and b(y|I) = 0.6.

ω+2 vI ≥ vR > vA;

)"*−)"$!.$+!*−+!".$,!,!-!,"*−-!,"!".$!!!".!!"#!(!−$!)!−#!$!!".!#!$!!!.".!."!−#!(!−$!)!"."!".%!".&("(!#.$.("'(!'("(!#.(!−$.)!−#.$.'("'(!!−#.(!−$.)'("(!'("'(!'("'(!("'(!'("'(!#/$/!−$/!−#/("!,"!)("!,"!)("!,"!)!(",-−.#/.$"−"$,",-−.#/.$"−"$)("0+",-+"*−"$,",-−"1)(",-−"1,"0+",-+"*−"$)("0+",-,"0+",-)("0−"1,"0+",-+"*−"$)("0,"0+",-)("0+",-,"0)("0+",-+"*−"$,"0−"1)("!,"!)We now informally and formally state the main result for
the two-agent case. The remaining propositions and theorems
are to be stated only informally and the formal statement along
with proofs can be found in Appendix A.

Proposition 1 (Informal). In a 2-party collusion game in
(2, 2, 1)-PIR in setting Γ, the unique sequential equilibrium
is the non-collusion outcome.

In a 2-party collusion game in
Proposition 1 (Formal).
(2, 2, 1)-PIR in setting Γ, the unique sequential equilibrium
is the pair (s, b), where s = (D ¯R, D ¯R) and b satisﬁes
) = b(D1|I A
) = b(D1|I D
) = b(D2|I D
b(D2|I A
) =
S2
S2
S1
S1
) = b( ¯R1|I D∗
) = b( ¯R2|I D∗
b(R2|I A∗
) = b(R1|I A∗
) = 1,
S2
S2
S1
S1
and b(·|I) = 0 in all other cases.

Proof. Strategy s or the move probabilities, α2-α4 and β2-β4,
are shown in Figure 2. Belief probabilities at the information
sets preﬁxed with “12”, which we denote as b(·|I), are implicit
in the tree. Beliefs need to be consistent with s.

Several of the variables are straightforward to determine. At
information set 3 (node “12.3”), α4 = 1 because (v∆
C + vS +
vR − vA) > (v∆
C + vS). Likewise, β4 = 1. At information set
1 (node “12.1”), we also have α3 = 1 because S1’s expected
return is

vR − vI
2

α3β3(v∆
C +
+ (1 − α3)β3(v∆
vI − vR
2

= α3[β3

C +vS +vR −vA)

−vA)+α3(1−β3)(v∆
C − vI ) + (1 − α3)(1 − β3)(v∆
C + vS)
+ (vR − vA)] − β3(vI + vS) + vS + v∆
C

and vI > vR, vR > vA. Likewise, β3 = 1. The expected return
for both players is simply v∆

C + vR−vI
Working backward, we already know what players would
do at information sets 1-4. Now let’s consider information set
0 (node “12.0”). S1’s expected return can be calculated as
follows:

2 − vA.

C +

α2β2(v∆

vR − vI
2
+ (1 − α2)β2(v∆
vI − vR
2

= α2[β2(

− vA) + α2(1 − β2)(vS − vI )

C + vS + vR − vA) + (1 − α2)(1 − β2)(vS)

− vS) − vI ] − β2(v∆

C + vR − vA) + vS

S1’s expected gain seems to depend on S2’s strategy. More
speciﬁcally, if β2( vI −vR
2 − vS) − vI > 0, α2 = 1 and α2 = 0
if the quantity < 0. But as we can see, the quantity is negative
because vI + 2vS + vR > 0. Then to maximize the expected
return, α2 = 0. Similarly, β2 = 0.

Due to the payment rules of the game, the strategy does not
depend on beliefs. The current structure of the game allows
players to know the path they have taken after they make a
simultaneous move. This indicates that the beliefs consistent
with s are those corresponding to the other party’s move
probabilities. Therefore, we have b(D2|I A
) =
S1
) = 1 and b(R2|I A∗
b(D1|I A
) =
S1
S2
b(R1|I A∗
) = 1.
S2

) = b(D2|I D
S1
) = b( ¯R2|I D∗
S1

) = b(D1|I D
S2
) = b( ¯R1|I D∗
S2

We consider another structure for the sequential game in
Appendix B where colluding parties do not verify others’
inputs. In this setting, players no longer know for sure the
path being taken after making a simultaneous move. We deﬁne
a deceitful player problem there and give Theorem B.1 in
Appendix B.
Comments on the simultaneous move in round 1.
In
round 1, the two parties select action A or D simultaneously.
Alternatively, if simultaneity cannot be implemented, one party
has to move ﬁrst, e.g., to share the computed responses along
with veriﬁcation information ﬁrst. It is straightforward to see
no matter what move this initiator player takes, the follower’s
dominant strategy is D. Therefore,
to give the “initiator”
less disadvantage, we assume the existence of fair collusion
protocols and let the colluding parties move simultaneously.
To wrap up, as in the proof of Proposition 1, we used
conditions on the payment values to argue about which actions
players will pick. A natural question is whether feasible
solutions exist for this simple case with 2 servers.
Hard to be feasible for (cid:96) = 2. First, when the game is
repeated between the same 2 players, cooperation can become
equilibrium [47], especially when they expect to play the
game inﬁnitely: the Folk Theorem [28] states that if players
are patient enough, then repeated interaction can result in
virtually any average payoff in a Subgame Perfect Equilibrium.
If we assume the players are memoryless (i.e, play like they
are playing for the ﬁrst time), we can manufacture the non-
colluding equilibrium. With other assumptions, e.g., players
play deceitfully initially and adjust based on others’ responses,
the equilibrium can be different. We prefer not
to make
more assumptions about players other than rationality. This
means that the equilibrium is not unique for the two parties.
Second, there is only a single source of ﬁnes used for user
compensation and reporting reward. As the privacy protection
upper bound increases, the deposit amount increases and has
to be greater than privacy worth. This also makes cooperation
more attractive for multiple repetitions of this game. Third, if a
user manipulates the game to “steal” deposits, it is impossible
to stop it
in the current setting while keeping a feasible
solution. This is because the client only pays vS + vA (both
need to be low) and what it receives back is vR and breach
compensation (both may be high) from “fake” accusations.
Besides, θ = 0 can be demanding in certain environments.
We next study what happens when we increase (cid:96).

B. A sequential game for (cid:96) > 2 parties

Order of events. Let the set of agents be S = {S1, ..., S(cid:96)}.
Without loss of generality, we denote the two queried servers
as S1, S2. Consider the following sequential game G1 in
((cid:96), 2, 1)-PIR:

(0) In round zero, the client queries 2 servers. Each server

decides whether to collude.

(1) In round one, each colluding server selects one of two
actions: A or D. Non-queried servers play deceitful (D)
before learning any correct information.

7

(2) In round two, colluding servers either report (R) the

collusion or keep it secret ( ¯R).

Analysis. To achieve non-collusion outcome with probability
at least 1 − 2−η, we update setting Γ to Γ1 by changing Γ(b)
to Γ(b)∗:

Γ(b)∗ θ satisﬁes:

√
This gives us θ ≤ 1 − 1+

2

((1−θ)(cid:96)
((cid:96)
2)

)

≥ 1 − 2−η;

1+4(cid:96)((cid:96)−1)(1−2−η)

.

2(cid:96)

Our result for the 2-party collusion in (cid:96) > 2 server case is
the following with the formal statement and the proof residing
in Appendix A:

Proposition 2 (Informal). In a 2-party collusion game in
((cid:96), 2, 1)-PIR in setting Γ1, with probability 1−2−η, the unique
sequential equilibrium is the non-collusion outcome.

When there is no Byzantine server (θ = 0), we always
achieve the non-collusion outcome (2−η ≥ 0). Note that non-
queried servers do not learn any queries or responses, because
queried servers always play D when they are in a game with
non-queried servers.
What do we gain from increasing (cid:96). Now each server has a
smaller chance of being queried (2/(cid:96)). In a round, two players
2
play with each other with probability p∗ = 1
(cid:96)((cid:96)−1) .
((cid:96)
2)
This is not negligible and as mentioned, cooperation (not
reporting in round 2) can become an equilibrium strategy in
repeated prisoner’s dilemma. We ﬁrst suppose the players have
an inﬁnite horizon, and we apply discount rate δ ∈ (0, 1) on
future returns [52, Chap. 7.2]. From always cooperating, one
earns the service fee and the privacy worth in each round the
same 2 players play together:

=

(vS + v∆

C ) + p∗(vS + v∆
C )

∞
(cid:88)

(1 − δ)i

= vS + v∆

C + p∗(vS + v∆
C )

i=1
1 − δ
δ

From reporting collusion in the ﬁrst game, one expects to earn
bonuses in the ﬁrst round, and the equilibrium returns:

C + vS + vR − vA + p∗vS
v∆

∞
(cid:88)

(1 − δ)i

i=1
C + vS + vR − vA + p∗vS

= v∆

1 − δ
δ

1−δ
The difference between the two quantities is −p∗v∆
δ +
C
vR −vA. Therefore, as long as 2(1−δ)
< (cid:96)((cid:96)−1) or (cid:96) ≥

v∆
C
vR−vA
, one reports collusion. When δ = 0.01, we

1+

(cid:113) 2(1−δ)
δ

v∆
C
vR−vA

δ

need (cid:96) ≥ 1 + 14
C = 25(vR − vA).
If players have a limited horizon, the discovered equilibrium
is preserved with even looser requirements on (cid:96).

, e.g., (cid:96) ≥ 71 if v∆

C
vR−vA

(cid:113) v∆

IV. MORE COLLUDING PARTIES

In this section, we discuss user manipulation and apply our

sequential game design to more colluding parties.

8

A. Combating False Accusations

One issue arises with Type I-II evidence is the user U taking
advantage of the incentive design to steal deposits since she
knows her messages. Assuming γ-reimbursement, to make
manipulating the game not proﬁtable for U, we need

(k − 1)vS > (k − 1)(vR − vA) + γv∆
C

(1)

where the left-hand side is the service fee paid to the k − 1
“falsely accused” servers by U. The term on the right-hand side
is the maximum bonus from accusing k − 1 other servers with
Type II evidence. The accusation proposer who is queried and
controlled by U pays the accusation fee (k−1)vA for accusing
k − 1 other servers and receives (k − 1)vR after evidence is
validated. The intuition is that the service fee is of such an
amount that “deposit stealing” is not proﬁtable for U.

False accusations from servers with arbitrary knowledge
about the intended queried entry have been coped with in
Section III. Now we have settled the false accusation problem.

B. k server collusion

Suppose for a colluding server, ﬁnding one other queried
server who is collusion-prone can be accomplished with prob-
ability z ≤ 1. And ﬁnding k − 1 such queried servers and
having all of them join a collusion protocol can be fulﬁlled
with probability zk−1. This can diminish rather fast for z < 1,
e.g., 0.85 = 0.33. Therefore, as a next step, we increase k.

If k ≥ 2 servers are needed to reconstruct the secret index,
we mainly follow the same game as G1 in ((cid:96), k, 1)-PIR case.
One minor change in the new game G2 is: in round 0, the
user picks k agents instead of exactly 2. We denote the set of
selected servers as SM ⊂ S. Now we no longer need setting
Γ(c). For the non-collusion outcome with probability at least
1 − 2−η, we update setting Γ(b)∗ to Γ(b)#:
)( θ(cid:96)
((1−θ)(cid:96)
((1−θ)(cid:96)
)(θ(cid:96)
k−1)
k )
((cid:96)
((cid:96)
k)
k)

Γ(b)# θ satisﬁes:

≤ 2−η;

+

1

0

In the collusion game for k > 2 servers, the payoff tree has
a slightly different structure, and we depict the one for k = 3
in Figure 4. The following is the major theorem for k-server
collusion in ((cid:96), k, 1)-PIR.

In a k-party collusion game in
Theorem 1 (Informal).
((cid:96), k, 1)-PIR in setting Γ(a) and Γ(b)#, with probability
1−2−η, the unique sequential equilibrium is the non-collusion
outcome if vI > (k − 1)2vA and (k − 1)vS > (k − 1)(vR −
vA) + γv∆
C .

All players play deceitful in round 1 essentially because
the queried servers are incentivized to take action R in
round 2 when ≤ 1 party plays D. Working backward,
playing D in round 1 maximizes one’s expected gains in
the remaining game. Similar to before, when θ ≤ k−2
(at
(cid:96)
least 2 queried servers are rational even when all Byzantine
servers are queried), we always have the non-collusion
outcome. Additionally, for discount rate δ ∈ (0, 1),
if
(cid:0)(cid:96)
, the sequential equilibrium is preserved
k
even when the game is repeated with the same k players.
We defer the formal statement and proof to Appendix A.

vR−vA
v∆
C

(cid:1) < δ

1−δ

What do we gain from increasing k. First, the game is
started with probability ≤ zk, which decreases exponentially
with k when z < 1. But this effect becomes negligible after
a certain point. For example, let z = 0.5. We know that
0.55 = 0.03 and 0.510 = 0.001, at which point it becomes
less necessary to continue to raise k. Second, higher k means
more sources of ﬁnes, and we can have a lower deposit vI .
Third, when players have an inﬁnite horizon, the requirement
on (cid:96) for players to report collusion when the game is repeated
is further loosened. Because now the same players are picked
, which is ≤ 1
with probability 1
for 2 ≤ k ≤ (cid:96) − 2
((cid:96)
((cid:96)
2)
k)
(equality if and only if k = 2 or (cid:96) − 2).
Compensating victim user. When there’s a conﬁrmed privacy
breach and privacy protection has not expired, the user receives
reimbursement γv∆
C (γ < 1). Now we examine whether we
have enough funds to provide γ-reimbursement. Let Λ denote
the remaining funds after ﬁning the accused and rewarding the
accuser. In an ((cid:96), k, 1)-PIR service with ω companion queries,
the most efﬁcient case is when Type II evidence is presented
and k − 1 other servers are ﬁned. We need γv∆
C ≤ Λk :=
(k −1)(vI −vR +vA). The inequality is implied by Theorem 1
when vS < vI , which is convenient to enforce. In principle,
the service fee is desired to be small while the deposit can
be close to privacy value. Thus, it is not included as a core
condition for collusion mitigation.

V. A GAME DESIGN FLOW

Now we combine the previous analysis and give a demon-
stration for determining parameters in the sequential game
for a speciﬁc system. Assume ((cid:96), k, 1)-PIR, k ≥ 2 and (cid:96)
is sufﬁciently large. As a system designer, we evaluate the
participating servers and determine a recommended k accord-
ing to the security parameter η. We then decide the privacy
value upper bound v∗
C we aim to protect. We compute the
parameters for the sequential game according to inequalities
in Theorem 1 with Algorithm 2 in Appendix C. It outputs a
list of feasible value assignments for parameters. There is no
absolute standard to select one assignment over another but
rather depends on the priorities.

One natural question to ask is under what condition solu-
tions are guaranteed to exist and whether the solution is always
practical. We do not seek to provide an absolute deﬁnition for
“practicality” but to ensure affordable service fees vS, higher
reporting rewards vR, and lower deposit vI for higher privacy
worth vC. We deﬁne practical service fees mathematically as:

vS ≤

v∗
C
k

ξ

(2)

where ξ ∈ (0, 1) is a practicality parameter characterizing the
affordability of vS. We state the following theorem.

Theorem 2 (Existence of Solution). There exist practical
(satisfying Equation (2)) parameter assignments for server
collusion mitigation in ((cid:96), k, 1)-PIR satisfying inequality set
1(cid:13)- 3(cid:13) where
1(cid:13) vA < vR ≤ ω

ω+2 vI

2(cid:13) (k − 1)vS > (k − 1)(vR − vA) + γv∆
C
3(cid:13) vI > (k − 1)2vA

In the following proof, we by default let v∆

C = vC
ω+1 for
clearer representation (parameterize v∆
C with ω). We note
that the worst case of v∆
C approaching vC can be seen by
conceptually letting ω = 0 in the analysis for Inequality 2(cid:13).
Because there, colluding servers recognizing the true index
among all the recovered ones is mathematically equivalent to
no companion query being sent.

vC

+

γvC

Proof. Given a viable parameter set k, vA, vR derived from 1(cid:13),
and the maximum privacy worth vC to protect, we can solve
the remaining inequalities. From inequality 1(cid:13) and 3(cid:13), we can
set vI . Then we only need to make vS satisfy inequality 2(cid:13). We
show that we can have practical vS (satisfying Equation (2))
satisfying inequality 2(cid:13).

For inequality 2(cid:13), we need (k−1) vC

k ξ > (k−1)(vR −vA)+
w+1 , which gives ξ ≥ k(vR−vA)
(k−1)(w+1) . When we do
not provide reimbursement (γ = 0), we can have a feasible
ξ < 1 by mandating k(vR − vA) < vC. This is easy to satisfy
because the reward amount only needs to satisfy inequality
1(cid:13) and is not tied to privacy worth. In other words, we can
have assignments for vR, vA such that 0 < vR − vA < vC
k ,
which can be easily satisﬁed. For γ > 0, we need vA >
vR − 1
(k−1)(w+1) )vC. The right-hand side decreases with
w, so we can safely set w to 1 or conceptually 0 (we always
send companion queries) for a tighter bound. Then because
vA does not have other constraints, it can be made small to
satisfy this condition. vR is then adjusted accordingly.

k (1−

kγ

kγ

kγ

Overall, we ﬁrst decide vC, k, γ. Then we can decide
accusation fee vA according to condition vA > vR − 1
k (1 −
(k−1)(w+1) )vC (to satisfy inequality 2(cid:13)). In the next step, we
determine vR according to inequality 1(cid:13). We then determine
vI according to inequality 3(cid:13) and ﬁnally decide vS according
to inequality 2(cid:13). Because we incorporated the practicality
constraint in Equation 2 into the derivation, the service fee
is by construction practical. An example feasible parameter
region of vS, vI , vA in setting z = 1, (cid:96) = 100, v∗
C = 100, k =
{2, 3, 4, 5}, w = 1, γ = 0, vI ≤ v∗
C is shown in Figure 3. An
example solution is vS = 5, vI = 93, vR = 15, vA = 11. One
observation is that to get a low service fee and accusation fee,
we need a higher deposit.

A. Implementation

For

illustration purposes, we present abstract essential
functions for implementing the game and payment rules in
Algorithm 1. In Line 15, a user submits queries to a list
of servers. The coordinator initializes parameters for future
possible accusation resolutions. In Line 12, an accusation is
ﬁled, and we call the accusation validation routine. Line 4
checks the validity of the accusation. Line 7 calls payment
execution routine to realize payment rules.
Smart contract implementation of CC. We treat Ethereum
as a public bulletin board and conveniently implement the co-
ordinator contract as a smart contract on Ethereum. The source

9

function as a default f (·) inside the contract. So the accuser
can omit the step of providing a function circuit if f (·) is
the reconstruction algorithm. Otherwise, we ask the accuser
to address the function simply as f (bytes memory[k]) when
providing the circuit for a simpler calling convention. We
describe the zero-knowledge veriﬁer in Appendix D.

We summarize the gas costs of contract deployment and
each function call in Table I. We ﬁnd the costs to be accept-
able, e.g., posting requests cost about 0.7 dollars [1].

TABLE I: Cost estimates in Gas. CheckCircuits(·) needs to
additionally include payments to Chainlink oracle.

Normal service
Deploy contract
Deposit(·)
PostRequests(·)
SubmitResponse(·)
ClaimServiceFee(·)

Cost
4697299
105436
405657
97400
33103

Collusion resolution
CheckCircuits(·)
Accuse(·)
VerifyType1(·)
VerifyType2(·)
zkVerify(·)

Cost
66991+
223766
61822
275279
2286423

Cryptographic and Communication overhead.
If users
the
and servers always send valid queries and responses,
coordinator contract only pays overhead for computing SHA-3
hash, which costs 30 base gas and 6 gas every 256 bits [27].
We preserve the communication costs of the underlying multi-
server PIR since only a ﬁxed-length hash is additionally
transmitted. Otherwise, if parties may send invalid messages,
the contract additionally veriﬁes ECDSA signatures and po-
tentially accesses the database. Verifying ECDSA signatures
in Ethereum involves recovering the signer with ecrecover,
which consumes 3000 gas. The signatures are also of speciﬁc
sizes, so the communication complexity is preserved. In both
cases,
the computation complexity of the underlying PIR
protocol is sustained since parties only need to additionally
compute an efﬁcient cryptographic hash and an ECDSA sig-
nature if messages may be incorrect, which can be computed in
about 0.1ms [19]. In two efﬁcient single-server cPIR solutions,
XPIR [2] and SealPIR [5], it takes the server three orders
of magnitude more time to compute the response even for
small databases (N = 216). Note that the signatures can
be posted after PIR interactions. Also note that gas costs
related to signatures and verifying messages are not induced
by collusion mitigation, but to counter malicious behaviors of
sending incorrect messages in general.
Non-triviality of function circuits. When parties report
with Type II evidence, we need to check the non-triviality
of the function f (·) in addition to verifying function inputs
and outputs. We view a function as non-trivial if its output
depends on the inputs. Taint analysis [53] is employed for
this purpose. When we implement the design on Ethereum
with smart contracts, the functions do not contain pointers
and are deterministic. According to a survey by Di Angelo and
Salzer [21], one can perform control and data ﬂow analysis on
EVM (Ethereum Virtual Machine) bytecode of smart contracts
off-chain via EthIR [3] and Securify [60], among other tools.
However, accomplishing this task on-chain can be imprac-
tically expensive since it essentially requires compiling smart
contracts on-chain. Therefore, we treat the function circuit

Fig. 3: Example parameter feasibility regions. See a 3D
interactive plot at [32]. As the number of queries increases,
the service fee feasibility region shrinks due to Equation (2).
Note that here we intentionally let the deposit be under the
maximum privacy worth (v∗
C = 100), but this is not a must.

ALGORITHM 1: Coordinator (Essential Functions)
Input: S, k, l, vR, vI , vA, vS, pk, src, user, γ

1 Function AccusationVal(id, eT ype, evidence):
2

for pk ∈ Journal[id].pkList do

if !alreadyF ined[id][(cid:48)pk(cid:48)] then

if ValUtil(pk, id, eT ype, evidence) then

PaymentExec(src, id, eT ype)
5
6 Function PaymentExec(witness, id, eT ype):
Read from Journal[id] the usr, pkList;
7
Take ﬁnes vI from each accused server pk;
For each pk, let alreadyF ined[id][(cid:48)pk(cid:48)] = T rue;
Distribute reward vR to witness;
Distribute γ-compensation to usr;

8

9

10

11
12 On new (id, eType, evidence):
13

Charge vA from accuser msg.sender;
AccusationVal(id, eT ype, evidence)

14
15 On new (pkList, requests):
16

Lock vS from msg.sender for each pk ∈ pkList;
Issue a unique identiﬁer id for the request;
Store requests, pkList in Journal[id];
for pk ∈ pkList do

alreadyF ined[id][(cid:48)pk(cid:48)] = f alse;

3

4

17

18

19

20

code in Solidity is available [33]. The contract maintains the
complete life cycle of PIR service and resolves collusion
accusations. For evidence veriﬁcation, as shown in Figure 1,
the accuser submits the evidence of the corresponding type,
followed by the user or the involved server submitting aux-
iliary information. If no supplementary data is submitted in
time, the accusation is automatically marked as successful. The
accused server can be pinpointed through elimination in this
case. After collecting information, the veriﬁcation algorithm
veriﬁes commitments (Type I) or computes certain function
circuits after verifying function non-triviality and inputs valid-
ity (Type II). Any secure commitment and signature schemes
would sufﬁce, e.g., cryptographic hash functions as commit-
ment scheme and ECDSA signatures [40]. To facilitate Type
II evidence veriﬁcation, we provide a sample reconstruction

10

20406080100010203002040010203020406080100010203022.533.544.55kService feeDepositAccusation feeDepositAccusation feeReward𝑘provided by the accuser as non-trivial by default and only
verify its non-triviality if the accused indicates its triviality
before providing auxiliary information. To verify, we can
create an oracle contract and a job concerning verifying the
non-triviality of smart contracts on an oracle service like
Chainlink [12]. It collects responses in a decentralized manner.
The coordinator contract creates requests through API calls
to the oracle and receives responses. The veriﬁcation cost is
afforded by the accused initially and charged to the accuser if
the function is indeed trivial.
Limitation of supported functions. Overall we prefer f (·)
to be light computations, e.g., the entire or the most signiﬁcant
bit (MSB) of the reconstructed data entry. Because it might
become uneconomical to verify computation-intensive func-
tions when the privacy worth is not high enough. But we also
consider this requirement to be undemanding. No matter what
the colluding parties have computed with MPC, at least some
party, with arbitrary private knowledge, learns some bit(s) or
a fraction of a bit about the queried entry from the output of
MPC. Otherwise, we consider the collusion to be not effective.
If a party learns some bits, it should be able to generate a
simple function for veriﬁcation purposes.

Note that although we focus on f (·) that reveals explicit
bits about the index or entry, it’s theoretically feasible to
mitigate way smaller information gain (e.g., learning that the
index is not 1). We only need to update the second half of
inequality 1(cid:13) in Theorem 2, which counters false accusations
from informed servers. But it may not be practical in terms
of a reasonable deposit amount for some instances, e.g., a
database with millions of entries. Intuitively, when we consider
an explicit bit of a random index, a player guesses it correctly
with probability one half; when we consider whether the
random index is not 1, a server can guess it correctly with
probability ˜p = N −1
N . More speciﬁcally, in this case, the
second half of inequality 1(cid:13) is updated to vI > 1+ω ˜p
ω(1− ˜p) vR.
When ω = 1, this simply translates to vI > (2N − 1)vR. This
means that for larger databases, the small information gain of
learning that an index is not a speciﬁc one requires higher
deposits to discourage false accusations. In such applications,
one can make a trade-off between the smallest information
gain to catch and the maximum deposit amount to demand.

VI. ADVERSARIAL EXITING STRATEGIES

We have been focusing on mitigating collusion at the time
a user makes queries. In practice, it can happen long after the
query has taken place. Although we put no restriction on the
coordinator algorithm executions, there might be insufﬁcient
funds in servers’ deposit accounts to execute on if they carry
out a massive breach. This is a concern because storing
pertinent data is not impractical. One subtlety is that if there
exists no external utility, a server is not motivated to do so
if the future return is higher than what they can gain from
collusion. Nevertheless, there can be scenarios where servers
have intentions to stop providing the service. For example, the
owners of some machines are on the edge of bankruptcy and
are willing to take the chance and quit the service. We consider

the following two exiting strategies: (i) The server who is
leaving wants to collude and break user privacy before leaving.
Deposits and service fees may be lost during the collusion.
See Section VI-A. (ii) The server registers for withdrawing
from the service and receives the deposit, and service fees
back. After everything has been processed, the server sells its
database. Nothing happens to this server because it is outside
the jurisdiction of the game rules. See Section VI-B.

Note that the latter dominates the former exiting strategy if
the proﬁts from selling raw data are greater than the secrets
minus potential ﬁnes.

A. A Server is Leaving the System

The coordinator or the rules we have established can still
exert some power over this server. The maximum ﬁnes we can
take are the deposit vI and detained service fees. Our goal is to
make our ends meet: to reward collusion reports and reimburse
users who have their privacy breached.

We propose a Self-Insurance [26] design tailored for our
needs. Suppose during T time units, there have been a total of
kΩ queries (Ω users, k queries per user). The total accumulated
to kΩvS. The expected number of
service fees are equal
queries this exiting server Se receives is kΩ
thus the expected
l
maximum number of victim users is kΩ
l . We say “maximum”
here because users can be recurrent. The worst case is that all
collusion are successful (with probability psucc = z
).
We expect (k −1)/k of the collusion to be reported by collud-
ing servers other than Se. The expected total amount of ﬁnes
k−1
we cannot realize is (psucc
vI .
This quantity increases with Ω, indicating that more users
result in more dead weight.

k − 1)vI = (k−1)Ωpsucc−l

kΩ(k−1)
l

kΩ
l

l

On the other hand, the service fee pool also grows with Ω.
transaction fees available to cover the potential

There are k· kΩ
l
loss from Se’s malicious exiting. We use
(k−1)Ωpsucc−l
l
k kΩ
l vS

σ =

vI

=

(k − 1)psuccΩ − l
k2Ω

vI
vS

to represent the tension between the size of inexecutable ﬁnes
and the potential insurance pool. We want σ ≤ 1 to be able
to cover possible loss using self-insurance. In the worst case
where psucc = 1, we can have l signiﬁcantly larger than k at
the outset of system design to allow a small vS. The intuition
is that when we have many servers in the system, one server
does not carry so much information that the insurance pool
cannot afford to lose.

To be more realistic, we can assume an interest rate r per
time unit on these detained service fees and the accumulated
service fees become kΩvS(1+r)T . We can discount the reim-
bursement for the users across time, which would necessitate
fewer and fewer effective ﬁnes as a query becomes ancient.
We can simply apply a discount rate r(cid:48) on deposits and obtain
σ(cid:48)

σ(cid:48) =

(k − 1)psuccΩ − l
k2w

vI (1 − r(cid:48))T
vS(1 + r)T

We also want σ(cid:48) ≤ 1 to relieve the tension. As an illustration
let k = 2, l = 1000, T = 10000, Ω = 5000
example,

11

and assume the worst case that psucc = 1. We have σ(cid:48) =
vI (1−r(cid:48))10000
1
vS (1+r)10000 . Let r(cid:48) = r = 0.0001, we can have σ(cid:48) ≤ 1 when
5
vS ≥ 0.027vI . Note that although the intuition goes through,
the above is not the exact formulation as we are treating the
queries as coming at once at the beginning of the T time units.
For a detailed derivation and for more than one server exiting
during the T time units, see Appendix F.

B. A Server has Left the System

After a server has left the PIR service system, it escapes
the rules of our mechanism. This server can sell gathered
information to anyone inside or outside the system. One insider
server would need at least k − 1 servers to leave the system to
be able to break user privacy during the rounds these k servers
are queried. This may happen with practical probability if k
is small (e.g., k = 2). This is outside the scope of this work,
but two techniques can potentially counter this threat.
Proof of secure erasure (PoSE). When a server leaves, PoSE
ensures that it has erased its memory. With a PoSE attestment,
the server gets its deposit and service fees back.
cPIR to the rescue. We have been utilizing 1-private PIR
for efﬁciency purposes. We can seek to balance efﬁciency and
privacy guarantees by employing t-private cPIR.

VII. COALITIONS

In this section, we extend the analysis to consider trust
groups or strong coalitions formed by servers. The threat
model for group players is similar to the individual server
case but has three major changes. Servers form coalitions and
aim to break user privacy. Servers inside the coalition act in a
highly coordinated way. Information entering into the group is
shared among members, and members do not report collusion
inside.

A. Bound the Loss Within the Current Model

Suppose the pre-established coalition is of size s, and that
the number of servers to be queried by the user is k (≤ (cid:96)).
We consider the following two cases.

(s
k)
((cid:96)
k)

First, if s < k, there always exist some true queries that
land outside the clique. Requesting collusion with outsiders
is discouraged by our proposed design. The previous analysis
applies, with the probability of ﬁnding collusion-prone parties
changing from zk−1 to zk−s. Second, if s ≥ k, with some
probability ps, the clique contains all the queries. We can
. One observation is that with k
calculate ps as ps =
being the same, as (cid:96)/t grows, the probability goes down.
This implies that if the total number of servers is sufﬁciently
large compared to the colluding parties, the probability that
user privacy is compromised is small. Besides, servers inside
the colluding ally may not recognize the true user secret
among the (w + 1) total secrets retrieved. The expected utility
from colluding together is psv∆
C . This implies that including
more companion queries can decrease this loss. Overall, the
introduced collusion deterrence mechanism ensures privacy for
s < k case. The expected loss from privacy breach for the
s ≥ k case is psv∆
C .

B. Adding Spices

Cooperative game theory [11] studies the formation of
coalitions, and we consider strongly effective ones here. Our
goal is to try to cap the coalition size. A cooperative game
can be formalized with the set of (cid:96) players N = {S1, . . . , S(cid:96)},
each nonempty set of coalition S (of size s), and a characteris-
tic/value function mapping a coalition to its collective payoff
v : 2N (cid:55)→ R. For example, the grand coalition has value
C + kvS. An allocation x ∈ R(cid:96) speciﬁes how to
v(N ) = (cid:96)v∆
divide each coalition value among members.
Solution concept. We suppose the coalition structure (parti-
tion of N ) is non-overlapping and solve for the allocation in
the core of v(S). We parameterize the largest coalition and do
not solve for v(N ) directly because we want to cap the coali-
tion size. Such a core allocation is a feasible allocation that
no other coalition (⊂ S) can improve on (i.e., give a strictly
higher payoff for all). Here, feasibility simply requires that
one does not over-allocate the coalition value. For any S with
size s ≥ k, its core is: {∀Si ∈ S, x(Si) = psv∆
C + k/(cid:96) · vS}.
This leads to the grand coalition being the most desired.

C ) + k/(cid:96) · svS instead of ps · sv∆

Alternatively, we can consider a slightly different value
function where the coalition S only earns v(S) =
psD(s, v∆
C + k/(cid:96) · svS. Here
D(·) is a function non-increasing in s. This is reasonable
only if user privacy depreciates when more parties learn it.
For example, the price of some stock is frequently queried,
and suppose this indicates upcoming valuation changes of the
company. Individual proﬁts decrease as more players learn this
information.

C ) = cv∆

Constant depreciation (D(s, v∆

C where 0 < c < s is
a constant) promotes grand coalition. In this setting, the core
is {∀Sj ∈ S, x(Sj) = ps/s · cv∆
C + k/(cid:96) · vS}. By including
one more member to S, one existing member’s extra gain is
(ps+1/(s + 1) − ps/s) · cv∆

k ≥ 2 ⇒

s
s + 1 − k

C > 0 because
(cid:1)
(cid:0)s+1
k
(cid:1) > (s + 1)
(cid:0)(cid:96)
k

> 1 ⇒ s

(cid:1)

(cid:1)

(cid:0)s
k
(cid:0)(cid:96)
k

This means that the coalition S has an incentive to include
new members until it becomes the grand coalition.

(s−k)!

(s+1
k )
(s
k)

C ) = cv∆

= (s+1)!
(s+1−k)!

s! = s+1

Similarly, linear depreciation (D(s, v∆

C /s) promotes
grand coalition. This is essentially because the increasing
rate of ps is
s+1−k . Intuitively,
when D(s, v∆
C ) depreciates faster than ps’s increasing rate,
coalition size is kept at k. Consider quadratic depreciation
C /s2). In this scenario, the core allocation is
(D(s, v∆
{∀Sj ∈ S, x(Sj) = ps/s3 · cv∆
C + k/(cid:96) · vS}. By enforcing
s3ps+1 < (s + 1)3ps, s is kept small. This indicates that
(cid:1)
(cid:0)s
k
(cid:1) ⇒
(cid:0)(cid:96)
k

(cid:0)s+1
(cid:1)
(cid:1) < (s + 1)3
k
(cid:0)(cid:96)
k

s3
s + 1 − k

C ) = cv∆

< (s + 1)2

s3

⇒ k <

3s2 + 3s + 1
(s + 1)2

When k = 2, the inequality is satisﬁed. This means that
when the privacy worth gain depreciates quadratically in s,

12

the coalition size is kept at 2 for k = 2 and the expected
privacy loss is p2v∆
C . Note that as previously discussed, if the
2 members in the coalition respond to our provided incentives,
it is possible to make them report collusion inside by having
a large (cid:96).

VIII. RELATED WORK

Deterring correctness-related collusion. Yakira et al. [65]
presents a slashing mechanism for threshold cryptosystem
collusion mitigation. All participants register in an escrow
service. One can frame colluding players with proper evidence
to the service, which slashes all other agents by burning their
deposits and rewarding the reporting agent. There, privacy
leakage and active false accusations are not a concern, unlike
in PIR (ﬁnite number of entries).

Dong et al. [23] focus on ensuring correctness in replication-
based cloud computing and propose a solution involving
inducing betrayal among colluding parties. The four major
differences are that ﬁrstly, unlike privacy-related collusion,
correctness-related collusion violates security and affects the
protocol. Second, collusion for privacy breaches can happen
any time after the PIR service. This means that even if correct-
ness is ensured at the time of service, peers can still collude to
break privacy later. Third, in privacy-related collusion, players
exchange information and have arbitrary private knowledge.
Lastly,
the computing parties in [23] collude via a smart
contract. It can be limiting to expect collusion to happen via
explicit contracts.

Ciampi et al. [17] present collusion preserving secure (CP-
secure) computation protocol with a collateral and compen-
sation mechanism to disincentivize aborting. Parties deposit
collateral at the initialization round, and they can withdraw
only if all messages and executions are correct. Otherwise,
the collateral is taken from dishonest players to reward others.
In this design, correct messaging and executions are well-
deﬁned and self-contained and the aim is to stop subliminal
communication inside the protocol. Yu et al. [68] consider
collusion of rational peers in overlay multicast. They aim
to provide safety-net (i.e., minimum proﬁts) guarantees to
non-deviating peers. Debt links are introduced to allow links
to be used to send data only if a proper amount of debt
coins are being paid. There, collusion also affects the protocol
executions.

Wang et al. [62] consider collusion deterrence in MPC. One
game-theoretic approach proposed there and also in [63] is
to have undercover police disguised as corrupted clients to
catch colluding parties. In the context of this work, sending
fake collusion requests can reduce servers’ conﬁdence in other
servers’ compliance. One challenge of this approach is that one
needs to place a certain level of trust in the entities initiating
dummy collusion.
Rational secret sharing (RSS).
RSS [36] studies the
opposite problem of anti-collusion, which is how to have
rational agents cooperate in secret reconstruction, with only
assumptions on their preferences. Halpern and Teague [36]
present the impossibility results assuming a server prefers as

fewer servers learning the secret as possible, which is good
news for collusion mitigation. But a randomized protocol
can facilitate servers sharing correct shares. Moreover, if we
change the assumptions on server preferences, e.g., they only
prefer to learn the secret than not, the impossibility results
may no longer hold. This is bad news for collusion deterrence.
Thus, due to the difﬁculty in assessing servers’ preferences
accurately, we prefer working with tokenized incentives.

IX. CONCLUSION AND FUTURE WORK

In this work, we follow the intuition from a bit-guessing
game and explore a sequential game design that mitigates
collusion among individual players for ((cid:96), k, 1)-PIR systems,
assuming a large (cid:96). Longer-term privacy is ensured with a self-
insurance pool. The discussion is later extended to collusion
within coalitions. We also settle potential manipulation of the
design by users and servers with private knowledge. Because
we desire many servers to collectively provide the service,
blockchains become a suited application scenario. Overall,
privacy-related collusion is hard to tackle compared with
correctness-related ones and ﬁrm coalitions can cause even
more difﬁculty.
Future directions. Our work opens up an interesting line of
research. One intriguing direction for future work can be to
study mechanisms for robust multi-server PIR [31] and other
secret-sharing style applications or generic MPC. If we directly
apply the design to robust PIR, there’s fairness concern in
punishing colluding servers after Type II evidence accusation.
While the rationale behind our approach applies to many MPC
tasks, our analysis demonstrates that many nuanced issues will
have to be addressed both in analysis and implementation.

REFERENCES

[1] “Gwei to usd ethereum gas fee calculator,” https://automatedwebtools.

com/usd-eth-gas-fee/, accessed: 2022-08-17.

[2] C. Aguilar-Melchor, J. Barrier, L. Fousse, and M.-O. Killijian, “Xpir:
Private information retrieval for everyone,” Proceedings on Privacy
Enhancing Technologies, vol. 2, no. 2016, pp. 155–174, 2016.

[3] E. Albert, P. Gordillo, B. Livshits, A. Rubio, and I. Sergey, “Ethir: A
framework for high-level analysis of ethereum bytecode,” in ATVA, 2018,
pp. 513–520.

[4] A. Ambainis, “Upper bound on the communication complexity of private

information retrieval,” in ICALP, 1997, pp. 401–407.

[5] S. Angel, H. Chen, K. Laine, and S. Setty, “Pir with compressed queries
and amortized query processing,” in 2018 IEEE symposium on security
and privacy (SP).

IEEE, 2018, pp. 962–979.
[6] A. Beimel, Y. Ishai, E. Kushilevitz, and J.-F. Raymond, “Breaking
the o(n1/(2k−1)) barrier for information-theoretic private information
retrieval,” in IEEE FOCS, 2002, pp. 261–270.

[7] A. Beimel, Y. Ishai, and T. Malkin, “Reducing the servers computation
in private information retrieval: Pir with preprocessing,” in Annual
International Cryptology Conference, 2000, pp. 55–73.

[8] M. Ben-Or, S. Goldwasser, and A. Wigderson, “Completeness theorems
for non-cryptographic fault-tolerant distributed computation (extended
abstract),” in ACM STOC, 1988, pp. 1–10.

[9] S. Bowe, A. Gabizon, and I. Miers, “Scalable multi-party computation
for zk-snark parameters in the random beacon model,” Cryptology ePrint
Archive, 2017.

[10] E. Boyle, N. Gilboa, and Y. Ishai, “Function secret sharing: Improve-

ments and extensions,” in ACM CCS, 2016, pp. 1292–1303.

[11] R. Branzei, D. Dimitrov, and S. Tijs, Models in cooperative game theory.

Springer Science & Business Media, 2008, vol. 556.

13

[12] L. Breidenbach, C. Cachin, B. Chan, A. Coventry, S. Ellis, A. Juels,
F. Koushanfar, A. Miller, B. Magauran et al., “Chainlink 2.0: Next steps
in the evolution of decentralized oracle networks,” 2021.

[41] C. Juvekar, V. Vaikuntanathan, and A. Chandrakasan, “GAZELLE: A
low latency framework for secure neural network inference,” in 27th
USENIX Security Symposium, 2018, pp. 1651–1669.

[13] M. Campanelli, D. Fiore, and A. Querol, “Legosnark: Modular design
and composition of succinct zero-knowledge proofs,” in ACM CCS,
2019, pp. 2075–2092.

[14] R. Canetti, Y. Lindell, R. Ostrovsky, and A. Sahai, “Universally com-
posable two-party and multi-party secure computation,” in ACM STOC,
2002, pp. 494–503.

[15] Y. M. Chee, T. Feng, S. Ling, H. Wang, and L. F. Zhang, “Query-efﬁcient
locally decodable codes of subexponential length,” computational com-
plexity, vol. 22, no. 1, pp. 159–189, 2013.

[16] B. Chor, O. Goldreich, E. Kushilevitz, and M. Sudan, “Private informa-

tion retrieval,” in IEEE FOCS, 1995, pp. 41–50.

[17] M. Ciampi, Y. Lu, and V. Zikas, “Collusion-preserving computation

without a mediator.” IACR Cryptol. ePrint Arch., 2020.

[18] R. Cleve, “Limits on the security of coin ﬂips when half the processors

are faulty,” in ACM STOC, 1986, pp. 364–369.

[19] Cloudﬂare,

“Ecdsa

computation,”

https://blog.cloudﬂare.com/

ecdsa-the-digital-signature-algorithm-of-a-better-internet/.

[20] H. Corrigan-Gibbs, A. Henzinger, and D. Kogan, “Single-server private
information retrieval with sublinear amortized time,” in Annual Inter-
national Conference on the Theory and Applications of Cryptographic
Techniques, 2022, pp. 3–33.

[21] M. Di Angelo and G. Salzer, “A survey of tools for analyzing ethereum

smart contracts,” in (DAPPCON), 2019, pp. 69–78.

[22] Y. Dodis, A. Elbaz, R. Oliveira, and R. Raz, “Improved randomness

extraction from two independent sources,” in RANDOM, 2004.

[23] C. Dong, Y. Wang, A. Aldweesh, P. McCorry, and A. van Moorsel,
“Betrayal, distrust, and rationality: Smart counter-collusion contracts for
veriﬁable cloud computing,” in ACM CCS, 2017.

[24] Z. Dvir and S. Gopi, “2-server pir with subpolynomial communication,”

Journal of the ACM (JACM), vol. 63, no. 4, pp. 1–15, 2016.

[25] K. Efremenko, “3-query locally decodable codes of subexponential

length,” SIAM Journal on Computing, vol. 41, no. 6, 2012.

[26] I. Ehrlich and G. S. Becker, “Market insurance, self-insurance, and self-
protection,” Journal of political Economy, vol. 80, no. 4, 1972.

[27] Ethereum,

“Ethereum yellow paper,” https://github.com/ethereum/

yellowpaper.

[28] J. W. Friedman, “A non-cooperative equilibrium for supergames,” The

Review of Economic Studies, vol. 38, no. 1, pp. 1–12, 1971.

[29] A. Gabizon, Z. J. Williamson, and O. Ciobotaru, “Plonk: Permutations
over lagrange-bases for oecumenical noninteractive arguments of knowl-
edge,” Cryptology ePrint Archive, 2019.

[30] N. Gilboa and Y. Ishai, “Distributed point functions and their ap-
plications,” in Annual International Conference on the Theory and
Applications of Cryptographic Techniques. Springer, 2014.

[31] I. Goldberg, “Improving the robustness of private information retrieval,”

in IEEE Symposium on S&P, 2007, pp. 131–148.

[32] T. Gong, “Feasibility region 3d plot,” https://sites.google.com/view/

parameter-feasibility-region.

[33] ——, “Source code of the contract,” https://github.com/haas256/PIR.
[34] J. Groth, “On the size of pairing-based non-interactive arguments,”
in Annual international conference on the theory and applications of
cryptographic techniques, 2016, pp. 305–326.

[35] S. M. Haﬁz and R. Henry, “A bit more than a bit is more than a bit
better: Faster (essentially) optimal-rate many-server pir,” PoPETS, vol.
2019, no. 4, pp. 112–131, 2019.

[36] J. Halpern and V. Teague, “Rational secret sharing and multiparty

computation,” in STOC, 2004, pp. 623–632.

[37] R. Henry, A. Herzberg, and A. Kate, “Blockchain access privacy:
Challenges and directions,” IEEE Security & Privacy, vol. 16, no. 4,
pp. 38–45, 2018.

[38] A. Henzinger, M. M. Hong, H. Corrigan-Gibbs, S. Meiklejohn, and
V. Vaikuntanathan, “One server for the price of two: Simple and fast
single-server private information retrieval,” Cryptology ePrint Archive,
2022.

[39] T. Itoh and Y. Suzuki, “Improved constructions for query-efﬁcient locally
length,” IEICE Transactions on

decodable codes of subexponential
Information and Systems, vol. 93, no. 2, pp. 263–270, 2010.

[42] D. M. Kreps and R. Wilson, “Sequential equilibria,” Econometrica:

Journal of the Econometric Society, pp. 863–894, 1982.

[43] S. Kumar, E. Rosnes, and A. G. i Amat, “Private information retrieval
in distributed storage systems using an arbitrary linear code,” in ISIT,
2017, pp. 1421–1425.

[44] E. Kushilevitz and R. Ostrovsky, “Replication is not needed: Single
database, computationally-private information retrieval,” in IEEE FOCS,
1997, pp. 364–373.

[45] L. Lamport, R. E. Shostak, and M. C. Pease, “The byzantine generals
problem,” ACM Trans. Program. Lang. Syst., vol. 4, no. 3, pp. 382–401,
1982.

[46] D. V. Le, L. T. Hurtado, A. Ahmad, M. Minaei, B. Lee, and A. Kate,
“A tale of two trees: One writes, and other reads,” PoPETS, vol. 2020,
no. 2, 2020.

[47] G. J. Mailath and L. Samuelson, Repeated games and reputations: long-

run relationships. Oxford university press, 2006.

[48] S. Matetic, K. W¨ust, M. Schneider, K. Kostiainen, G. Karame, and
S. Capkun, “BITE: Bitcoin lightweight client privacy using trusted
execution,” in USENIX Security, 2019, pp. 783–800.

[49] P. Mittal, F. G. Olumoﬁn, C. Troncoso, N. Borisov, and I. Goldberg,
“Pir-tor: Scalable anonymous communication using private information
retrieval,” in 20th USENIX Security Symposium, 2011.

[50] P. Mohassel and Y. Zhang, “Secureml: A system for scalable privacy-
preserving machine learning,” in 2017 IEEE Symposium on Security and
Privacy (SP), 2017, pp. 19–38.

[51] M. H. Mughees, H. Chen, and L. Ren, “Onionpir: Response efﬁcient

single-server pir,” in ACM CCS, 2021, pp. 2292–2306.

[52] R. B. Myerson, Game theory: analysis of conﬂict. Harvard university

press, 1997.

[53] J. Newsome and D. X. Song, “Dynamic taint analysis for automatic
detection, analysis, and signaturegeneration of exploits on commodity
software.” in NDSS, vol. 5. Citeseer, 2005, pp. 3–4.

[54] V. Pappas, F. Krell, B. Vo, V. Kolesnikov, T. Malkin, S. G. Choi,
W. George, A. Keromytis, and S. Bellovin, “Blind seer: A scalable
private dbms,” in IEEE S&P Symposium, 2014, pp. 359–374.

[55] K. Qin, H. Hadass, A. Gervais, and J. Reardon, “Applying private
information retrieval to lightweight bitcoin clients,” in Crypto Valley
Conference on Blockchain Technology (CVCBT), 2019, pp. 60–72.
[56] A. A. Razborov and S. Yekhanin, “An\omega (nˆ 1/3) lower bound for
bilinear group based private information retrieval,” in IEEE FOCS, 2006,
pp. 739–748.

[57] N. B. Shah, K. Rashmi, and K. Ramchandran, “One extra bit of
download ensures perfectly private information retrieval,” in ISIT. IEEE,
2014, pp. 856–860.

[58] P. Syverson, R. Dingledine, and N. Mathewson, “Tor: The second
generation onion router,” in Usenix Security, 2004, pp. 303–320.
[59] R. Tajeddine, O. W. Gnilke, and S. El Rouayheb, “Private information
retrieval from mds coded data in distributed storage systems,” IEEE
ToIT, vol. 64, no. 11, pp. 7081–7093, 2018.

[60] P. Tsankov, A. Dan, D. Drachsler-Cohen, A. Gervais, F. Buenzli, and
M. Vechev, “Securify: Practical security analysis of smart contracts,” in
ACM CCS, 2018, pp. 67–82.

[61] M. Veeningen, “Pinocchio-based adaptive zk-snarks and secure/correct
adaptive function evaluation,” in International Conference on Cryptology
in Africa, 2017, pp. 21–39.

[62] Z. Wang, S.-C. S. Cheung, and Y. Luo, “Information-theoretic secure
multi-party computation with collusion deterrence,” IEEE TIFS, vol. 12,
no. 4, pp. 980–995, 2016.

[63] Z. Wang, Y. Luo, and S.-c. Cheung, “Efﬁcient multi-party computation
IEEE, 2014, pp.

with collusion-deterred secret sharing,” in ICASSP.
7401–7405.

[64] S. Wehner and R. De Wolf, “Improved lower bounds for locally
decodable codes and private information retrieval,” in ICALP, 2005, pp.
1424–1436.

[65] D. Yakira, I. Grayevsky, and A. Asayag, “Rational threshold cryptosys-

tems,” arXiv preprint arXiv:1901.01148, 2019.

[66] A. C. Yao, “Protocols for secure computations,” in FOCS, 1982, pp.

160–164.

[40] D. Johnson, A. Menezes, and S. Vanstone, “The elliptic curve digital
signature algorithm (ecdsa),” IJIS, vol. 1, no. 1, pp. 36–63, 2001.

[67] S. Yekhanin, “Towards 3-query locally decodable codes of subexponen-
tial length,” Journal of the ACM, vol. 55, no. 1, pp. 1–16, 2008.

14

[68] H. Yu, P. B. Gibbons, and C. Shi, “Dcast: Sustaining collaboration

despite rational collusion,” 2011.

[69] Y. Zhang, D. Genkin, J. Katz, D. Papadopoulos, and C. Papamanthou,
“A zero-knowledge version of vsql,” Cryptology ePrint Archive, 2017.

vI − vR
2

(v∆

−2vA)+α3(1−β3)(1−ζ3)(v∆

C −
+ (1 − α3)[β3ζ3 + β3(1 − ζ3) + (1 − β3)ζ3](v∆
+ (1 − α3)(1 − β3)(1 − ζ3)(v∆

C + vS)

C − vI )

C +vS+2(vR−vA))

APPENDIX A
PROOFS

= α3

(cid:104)

β3ζ3

5vR − 2vI
3

+ β3

vI − 3vR
2

+ ζ3

vI − 3vR
2
(cid:105)
+ 2(vR − vA)

+ ˜R

Proposition 2. In the 2-party collusion game in ((cid:96), 2, 1)-
PIR in setting Γ1, with probability 1 − 2−η, the pair (s, b)
of strategy proﬁle s = (D ¯R, ..., D ¯R, D ¯R) for all (cid:96) servers
and belief assessment b is the unique sequential equilibrium,
where ∀Si ∈ S, b satisﬁes b(Ds−i |I A
) =
Si
) = b( ¯Rs−i|I D∗
b(Rs−i|I A∗
) = 1, and b(·|I) = 0 in all other
Si
Si
cases.

) = b(Ds−i|I D
Si

Proof. With probability 1 − 2−η, the two queried servers S1
and S2 are both rational because θ satisﬁes Γ(b)∗. For the two
queried rational servers S1 and S2, Proposition 1 implies that
they play D ¯R in equilibrium and have beliefs consistent with
the equilibrium strategy.

For a non-queried server S3, it can only play D in the
ﬁrst collusion game it plays. We are interested in whether
they can learn anything through collusion. This is non-trivial
because they are not queried and thus not subject to penalties
in playing the game. Suppose the counterparty is also a non-
queried server S4. Then they can only play D ¯R initially and
do not learn any information about the queries or responses. If
the counterparty is a queried server, say S1, then S1 plays the
equilibrium strategy D ¯R if S1 does not know whether S3 has
the corresponding information and plays the dominant strategy
D if S1 knows that S3 does not possess the data. Thus, non-
queried servers do not learn more information and can only
play D in their collusion games.

Theorem 1. In k-party collusion game in ((cid:96), k, 1)-PIR in set-
ting Γ(a) and Γ(b)#, with probability 1 − 2−η, the pair (s, b)
of strategy proﬁle s = (D ¯R, ..., D ¯R, D ¯R) for all (cid:96) servers and
belief assessment b is the unique sequential equilibrium if vI >
(k−1)2vA and vS satisﬁes (k−1)vS > (k−1)(vR−vA)+γv∆
C ,
where ∀Si ∈ S, b satisﬁes b(Ds−i |I A
) =
Si
) = b( ¯Rs−i|I D∗
b(Rs−i|I A∗
) = 1, and b(·|I) = 0 in other
Si
Si
cases.

) = b(Ds−i|I D
Si

Proof. With probability 1 − 2−η, at least two queried servers
are rational because θ satisﬁes Γ(b)#. According to Sec-
tion IV-A, inequality (k − 1)vS > (k − 1)(vR − vA) + γv∆
C
stops user manipulation. Therefore, we do not need to treat the
client as a player in the following discussion and can focus
on reasoning about the servers’ actions.

We ﬁrst let k = 3. We have a new payoff tree with a similar
structure as Figure 2 which we demonstrate in Figure 4. Let
S1, S2, S3 be 3 queried servers. As depicted in Figure 4, when
there’s one deceitful player, it plays R at information set 2
(e.g., ζ4 = 1) because vR > vA. At information set 1, S1’s
expected return can be calculated as

α3β3ζ3(v∆

C −

2(vI − vR)
3

−2vA)+α3[β3(1−ζ3)+(1−β3)ζ3]·

15

Here ˜R contains all the remaining terms independent of α3.
Since vI > 4vA and vR > vA, the coefﬁcient term of α3 is
always positive, thus α3 = 1. More speciﬁcally, the coefﬁcient
term is positive when β3 = ζ3 = 0. Putting aside this special
case, it takes the minimum when more parties play R (β3 =
ζ3 = 1). Likewise, we have β3, ζ3 = 1. The expected return
for all players are simply (v∆
− 2vA). Then it is
3
obvious to see that α2 = β2 = ζ2 = 0.

C − 2(vI −vR)

Similarly, for a general k ≥ 3, let S1, . . . , Sk be the queried
servers. We know that when there’s more than one deceitful
player, all parties play ¯R. When there’s exactly one deceitful
player, this player plays R at the corresponding information
set (e.g., player 3 at the information set 2 in Figure 4). At
information set 1 where all have played A in round 1, all
players play R because vI > (k − 1)2vA and vR > vA. More
speciﬁcally, for S1, when all other players report collusion,
the difference between reporting and not reporting is vI −
k−1
k (vI −vR)−(k−1)vA. The term is positive when vI > (k−
1)2vA. Thus, all players play R in round 2 and obtain expected
return (v∆
k (vI − vR) − (k − 1)vA). Working backward,
playing A in round 1 cannot be equilibrium since playing D
generates more proﬁts at the information set 0. Similar to the
arguments in Proposition 2, non-queried servers play D in the
ﬁrst round in collusion games. Overall, the unique sequentially
rational equilibrium s dictates that all players select D in round
1 and ¯R in round 2, with the unique corresponding belief
assessment that is consistent with s.

C − k−1

APPENDIX B
DECEITFUL PLAYER PROBLEM
Forging information in collusion. When colluding parties do
not verify the information shared by colluding partners, one
interesting scenario is where a server prefers to retrieve the
index alone or desires to hide its information to avoid being
reported. We are interested in the probability of colluding
servers fabricating information and fooling each other, i.e.,
the counterparties mistake the provided fake data as correct. In
Boyle-Gilboa-Ishai’s construction, a query (I) is computation-
ally hard to forge except with negligible probability. Responses
(I) can be correctly forged with non-negligible probability. The
forgeability of inputs into an MPC protocol (II) depends on
the input type.

We denote the probability of one party fooling the other with
respect to a prior p as p† hereafter. As a simple illustration,
consider a simple xor-based naive PIR: let server Si receive
index vector (j1, j2, jm) and Sj receive (j1, j2, jm, a) so that
the xor of the two vectors recovers index a. In collusion, to
fool the other party, a server can guess an index and constructs

Fig. 4: Extensive form for 3-party sequential collusion game. Due to the symmetry in the game structure, we omit information
sets that are equivalent to the ones in the graph after changing labels.

a fake vector to redirect the result to some other index. But
they both can cheat. If they have the same redirecting strategy
jx → jy (guessing jx, redirecting to jy), then apparently
they recover a. Other scenarios where they recover a single
index are shown in Figure 5. Overall we prefer working with
constructions that make p† arbitrarily close to 0.

entry. To form a redirecting strategy, a server ﬁrst guesses the
intended query. One can guess Da correctly with probability
pa (pa ≤ maxi p). Then the server creates a fake input to
redirect the output to another index. Take the naive xor-based
construction as an example. We let q1 = (j1, j2, ..., jm), q2 =
j1, j2, ..., jm, a, and summarize the strategies as follows:

Si
j1, j2, ..., jm
j1, j2, ..., jm−1, a
j1, j2, ..., jm−1, jm+1
j2, ..., jm−1
j1, j2, ..., jm−1, a
j1, j2, ..., jm, a, jm+1
j1, ..., jm, jm+1, jm+2
j1, ..., jm−1, jm+1
j1, ..., jm, a, jm+1

Sj
j1, j2, ..., jm, a
j1, j2, ..., jm−1
j1, j2, ..., jm−1, a, jm+1
j2, ..., jm−1, a
j1, j2, ..., jm−1
j1, j2, ..., jm, jm+1
j1, ..., jm, a, jm+1, jm+2
j1, ..., jm−1, a, jm+1
j1, ..., jm, jm+1

Rationale
Amicable
jm → a
jm → jm+1
jm → j1
a → jm
a → jm+1
jm+1 → jm+2
jm+1 → jm
jm+1 → a

TABLE II: Redirecting strategy summary. Each time an index
gets “guessed” or “redirected to”, its presence in the output is
ﬂipped. Originally only index a is ﬂipped.

Intuitively, if the outputs after collusion protocol contain
more than one index or gibberish when a server itself provides
correct inputs, this server knows that the colluding party is
cheating. Otherwise, if the collusion output contains only one
index, the server may believe collusion is successful. To ﬁnd
out this probability, we summarize four scenarios where a
single index is outputted in Table III.

(Si, Sj )
(A, A)
(A, D)
(D, A)
(D, D)

Possible Single Output
a
{jm, jm+1}
{jm, jm+1}
{jm, jm+1, jm+2, a, j1}

Probability p†
1
pa(2 − pa)
pa(2 − pa)
P ∗

TABLE III: 2-server collusion with single index output. Here
P ∗ = (cid:80)
i

j(cid:54)=i,a 4pap2

j + (cid:80)

j(cid:54)=i 2p2

i pj.

i p2

(cid:80)

(cid:80)

i(cid:54)=a

Fig. 5: Deceitful strategy single output scenarios. If a player
plays jx → jy, we add a directed edge from node jx to jy.
The indices in squares are outputs. We omit the scenario when
the two players play the same strategy.

A deceitful player guesses an index and tries to redirect the
collusion protocol to output a targeted index. If the underlying
PIR protocol ensures that faking information (a query or
response) in collusion is computationally hard, then pf aking,
the probability of creating a valid fake input, is 0. This would
mean p† = 0, and we are back to the discussion we have in the
main body. Here we consider pf aking > 0. More speciﬁcally,
for the worst case, we consider pf aking = 1. This means that
as long as a server can picture a redirecting strategy, it can
always fabricate such an input.
In the deﬁned ((cid:96), k, 1)-PIR scheme,
Problem statement.
queried servers participate in a collusion protocol without
verifying others’ inputs directly. They only infer their validity
through the output of the collusion protocol. What is the prob-
ability of servers being fooled into believing the correctness
of others’ fake inputs?
Analysis. We ﬁrst discuss k = 2. Let server Si receive query
q1 and Sj (i (cid:54)= j) receive query q2 such that Da is the queried

16

"#&.%!!!"!4!!"""4!!"!(&−.!)#"%−'"(%−.")"#&."#"'"."!!!""4"!"""4&−!!(&−"!)(&−.!)"#&.#"#&.&"#&.')!)")4#4'4.4)!)"/)4)!/)"/)4#4'4(%−.4)$""−%"("−'"))*#)*!)*""−$"("−%")("−'")/)!/)")4/)!/)"/)4/)!/)"/)4/)!/)"/)4.5%−.5%%All:%#$−%('$('%)*−)%+(%#$−'$('%%−)%+, %#$−'$('%%−)%+,%#$−%,)(%-+%#$+)(%.−%+),%#$−%,,%#$−%,)All:	%-+%#$(%-−%,,%-−%,,%-+%#$+)(%.−%+))()+,)+,)++),-)()+,)+,)+)().,).,)+)𝑎𝑗!𝑗"𝑗"𝑗!𝑎𝑎𝑗!𝑗"𝑎𝑗!𝑎𝑎𝑗!𝑗"𝑎𝑗!𝑗"i

Row 1 is straightforward. Index a is retrieved w.p. 1. Row
2 and 3 are because there is a single output in one cheating
player case when the deceitful party either guesses the index
correctly (with probability pa) or guesses the index incorrectly
(w.p. (1 − pa)) and redirects the result to Da (w.p. pa). And
pa + (1 − pa)pa = pa(2 − pa), which we will denote as p†
1.
Index a is never returned because the only deceitful player
either ﬂips a or produces more than one index in the output.
The deceitful player can always recover a through deduction.
In the (D, D) case, by observing Table II, we notice that
when both parties have the same strategy or opposite strategies,
they recover a. The intuition is that the two servers cancel out
each other’s actions by ﬂipping the ﬂipped back. This happens
w.p. p†
j . For other cases of single output,
as presented in Figure 5, in the left four subgraphs, the node
jx with an edge from itself to a or from a to itself becomes the
potential output because a is the original true index, and it gets
canceled out. When the other deceitful player plays a strategy
that goes from jy to jx or from node jx to jy, jy becomes
the output because jx is now canceled out. These occur with
probability p†
i pj. Note that i = a
or j = a case has been included in output a category.
Now index a is returned w.p. p†
2,1 and because deceitful
servers cannot differentiate between different subcases, they
cannot make correct accusations deterministically but rather
probabilistically, w.p. p†
2,1. This is to say, a deceitful party
2,2 and succeed w.p. p†
2,1 + p†
report w.p. p†

j(cid:54)=i,a 4pap2

2,2 = (cid:80)

2,1 = (cid:80)

j(cid:54)=i 2p2

2 = p†

i p2

2,1.

(cid:80)

(cid:80)

Now we can extend our discussion to a more general k > 2
case. When all servers play amicable, a is output w.p. 1. When
1 server plays deceitful, the collusion protocol outputs a single
index w.p. pa(2 − pa). Similarly, index a is never returned,
and the only deceitful player can always make successful
accusations through deductions. When 2 servers play deceitful,
they have a single output w.p. (cid:80)
j where the 2
i
deceitful agents either have the same strategy or opposite
strategies.

j(cid:54)=i 2p2

i p2

(cid:80)

i(cid:54)=a

When 2 < k(cid:48) < k servers play deceitful, they have a single

output w.p.

(cid:88)

(cid:88)

...

(cid:88)

2p2
i1

p2
i2

...p2
ik(cid:48)

i1

i2(cid:54)=i1

ik(cid:48) (cid:54)=ik(cid:48)−1
where the k(cid:48) deceitful agents either have the same strategy or
opposite strategies. When k servers play deceitful, they output
a single index w.p.

p†
k =

(cid:88)

(cid:88)

...

(cid:88)

2p2
i1

p2
i2

...p2
ik

+

i1

i2(cid:54)=i1

(cid:88)

ik(cid:54)=i1,...,ik−1
(cid:88)

...

(cid:88)

i1(cid:54)=a

i2(cid:54)=i1,a

ik(cid:54)=ik−1,a

2kpap2
i1

p2
i2

...p2

ik−1

pik

In these scenarios with 2 or more deceitful players, deceitful
servers cannot distinguish between different subcases with a
single output.

A. Implications for the sequential game

#D
0
1
2

2 < k(cid:48) < k (cid:80)

k

Probability P †
1
pa(2 − pa)
i p2
j(cid:54)=i 2p2
j
2p2
ik(cid:48) (cid:54)=ik(cid:48)−1
i1
P ∗

(cid:80)
(cid:80)
i
... (cid:80)

p2
i2

...p2

ik(cid:48)

(cid:80)

i2(cid:54)=i1

i1

TABLE IV: k-server collusion with single index output.
... (cid:80)
Here P ∗ = (cid:80)
(cid:80)
+
i1
i2(cid:54)=i1
(cid:80)
(cid:80)
i2(cid:54)=i1,a ... (cid:80)
ik(cid:54)=ik−1,a 2kpap2
i1

ik(cid:54)=i1,...,ik−1
...p2
p2
i2

p2
2p2
i2
i1
pik .

...p2
ik

i1(cid:54)=a

ik−1

top to bottom, the four nodes are reached with probability
α2β2, α2(1 − β2)p†

1, (1 − α2)(1 − β2)p†
2.

1, (1 − α2)β2p†
We have the following theorem:

In 2 server collusion game without

input
Theorem B.1.
veriﬁcation in ((cid:96), 2, 1)-PIR in setting Γ(a) and Γ(b)#, with
probability 1 − 2−η, the pair (s, b) of strategy proﬁle s =
(D ¯R, D ¯R) for servers Si ∈ S and belief assessment b is the
unique sequential equilibrium if vS satisﬁes the following:

• (k − 1)vS > (k − 1)(vR − vA) + γv∆
C
• (2−p†
2,1)vS < (p†
2p†
p†

1+p†
2,1v∆
C

p†
2p†
2,1
2

1+p†

2p†

2+

)vA+(1− p†

1p†
2 )vI +

2

where ∀Si ∈ S, b satisﬁes b(Ds−i|I A
Si
b(Rs−i|I A∗
Si
b(Rs−i|I 1
Si

) = b(Rs−i|I DA
Si
) = 1, and b(.|I) = 0 in other cases.

) = b(Ds−i|I D
) =
Si
) =

) = b( ¯Rs−i |I DD
Si

Here = b(Rs−i|I 1
Si

) means that Si is at information set 1.

Proof. With probability 1 − 2−η, at least two queried servers
are rational because θ satisﬁes Γ(b)#. According to Sec-
tion IV-A, inequality (k − 1)vS > (k − 1)(vR − vA) + γv∆
C
stops user manipulation. Therefore, we can focus on reasoning
about servers’ actions.

We ﬁrst look at the information set 1. Without loss of gen-
erality, we examine from one queried party S1’s perspective.
Players hold consistent beliefs. By Bayes’ formula, when S1
plays A at the information set 0, it believes that it arrives at
the information set 1 along move AA with probability:

b(A2|I1A
S1

) =

β2
β2 + (1 − β2)p†
1

1

2

β2p†
1

) =

1+(1−β2)p†

) = (1−β2)p†
β2+(1−β2)p†
1

Similarly, it believes it arrives at information set 1 along move
AD with probability b(D2|I1A
. When S1
S1
plays D at information set 0, we can calculate b(A2|I1D
) =
S1
and b(D2|I1D
If S1
S1
β2p†
chooses R (α3 = 1), then sequential rationality requires that
By laying out the expected returns, it is easy to verify that
when one plays D and arrives at the corresponding information
set 1, R is the dominant strategy. This means that α4 = 1.
Similarly, β4 = 1. This is not to say that when one plays
D, they always report (a counterexample is information set
4). But rather, if one plays D and happens to arrive at the
information set 1 instead of 4, it always reports.

(1−β2)p†
2
1+(1−β2)p†

β2p†

.

2

As shown in Figure 6, the information set 1 now consists
of four nodes. This means that α3, β3 are no longer 1. From

Now we solve for α3 and β3. For a clearer representa-
) as x. At information

tion, we temporarily denote b(A2|I1A
S1

17

Fig. 6: Extensive form for 2-party sequential collusion game without veriﬁcation.

set 1, there are three possible supports to consider for S1:
{R}, { ¯R}, {R, ¯R}. First, we consider support {R}. To make
R sequentially rational, then we need the expected returns
from playing R to be higher than playing ¯R:
vI − vR
2

− vA) + (1 − β3)·

β3(v∆

C −

R :

(cid:104)

(cid:105)
x + (−vI − vA)(1 − x)
C + vS + vR − vA)

(v∆

(cid:104)

¯R :

β3(v∆

C − vI ) + (1 − β3)(v∆

(cid:105)
C + vS)

x + vA(x − 1)

Because we have vR > vA and vI > vA, the difference
between the ﬁrst and second quantity is positive. This means
that playing R always yields higher returns regardless of
the other parties’ actions. Then because S1 is capable of
selﬁshness, we have α3 = 1. Similarly, β3 = 1.

We next solve for α2 and β2. At information set 0, there are
three possible supports to consider for S1: {A}, {D}, {A, D}.
First, we consider support {A}. To make A sequentially
rational, then we need the expected returns from playing A
to be higher than playing D. The simpliﬁed expected returns
from playing the two moves are as follows:

A : β2(v∆
C −
+ (1 − β2)(cid:2)p†
D : β2(v∆
+ (1 − β2)(cid:2)p†

vI − vR
2

− vA)

1(−vA − vS) + vS − vI

(cid:3)

6

7

8

9

C + vS + vR − vA)
2,1(v∆

C −

2p†

vI − vR
2

− vS) − p†

2vA + vS

(cid:3)

The difference of the ﬁrst and second quantity is negative since
2,1)vS < (p†
vR > vA and (2−p†
)vA +(1−

1 +p†

1 +p†

p†
2p†
2,1
2

2p†

2 +

2

2p†

2,1v∆

p†
1p†
2 )vI + p†
C . This means that α2 = 0. Likewise, β2 =
0. Then the unique sequential equilibrium is all players playing
D ¯R and players have beliefs consistent with the equilibrium
strategy.

APPENDIX C
ALGORITHM

The algorithm follows from Theorem 1. The system de-
signer only needs to decide on one set of parameters be-
forehand and the algorithm outputs all possible parameter
assignments in the designated range.

ALGORITHM 2: Designer
Input: l, k, t, v∗
Output: vR, vI , vA, vS, expLoss

C , p, γ, ω

C /(ω + 1),expLoss ← 0;

C ← v∗

1 assign ← [],v∆
2 vu
S = (cid:98)vC/k(cid:99);
3 for vS in range(1, vu
S) do
vl
I = v∗
C /k, vu
I = v∗
C ;
4
I , vu
for vI in range(vl

5

I ) do
for vR in range(1, ω

ω+2 vI ) do

for vA in range(1, vR − 1) do

if (k − 1)vS > ((k − 1)(vR − vA) + γv∆
vI > (k − 1)2vA then

C ) and

assign.append(vS, vI , vR, vA);

10 if t ≥ k then

11

(q
k)
expLoss ← (cid:80)t
((cid:96)
k)
12 return assign, expLoss

q=k

v∆
C ;

18

"#.%!!!""!!"#"(%−'")"−$!%!"#."!!"!!!"""!""%−#"(%−'")"#.#"#.&"#.')!)"!"""##$#!#$!$"!"(&−"")&−!"""$##$#!&−!"(&−"")$###!$##$#!##$#!&&&Both: *%&−'$('%"−*)(**+*%&+*$−*),*%&−*+)(*%&−*+,**+*%&+*$−*))(**+*%&,**+*%&)(**−*+,**+*%&+*$−*))(**+*%&+*$−*),**−*+)(%",%")"#.""#.""#."%)#&&−)#&)#&&−)#&)!&&−)!&(**+*%&+*$−*),−*+−*))(**+*%&+*$−*),**−*))(**+*%&,−*+)(**+*%&,**)(*'+)!,#&(*)*+*%)−*+, *'+)!,#&(*)*−*$−,,))(*'+)!,#&(*)*−*$−,,),*'+)!,#&(*)*+*%)−*+)(**+-",!-*%&,**+-",!-*%&)Both: **+-",!-(*%&−'$('%"−**)−*)!-""!-(&−"")&−!-""&−!-(&−"")!-"-!-(&−"-)&−!-"-&−!-(&−"-)(−*+−*),**+*%&+*$−*))(**−*), **+*%&)(−*+,**+*%&+*$−*))(**,**+*%&)!""-!"(&−"-)&−!""-&−!"(&−"-)APPENDIX D
ZERO-KNOWLEDGE COORDINATOR

To avoid involving the user, the veriﬁcation method we
employ in the main body is to ask involved parties to open
the commitment and then perform computations on revealed
values. With the incentive structure we have built, rational
parties are not incentivized to collude or falsely accuse and
privacy is protected with probability 1−2η. However, although
false accusations are penalized, malicious parties can force
veriﬁcation to learn the secret. To counter this risk, we enforce
zero-knowledge accusation veriﬁcation. Note that another ap-
proach is to mitigate it through implementation details: (i)
move the veriﬁcation process to after the protection period
(Section VI); (ii) notify the user and wait for a certain period
before veriﬁcation to allow for any remedies at the user’s side.
Upon receiving an accusation, the contract counts down a
proof collection window. The accused parties who failed to
provide a zkSNARK (zero-knowledge succinct non-interactive
arguments of knowledge) π justifying the evidence being false
in time are marked as “culpable”. If the accuser somehow
obtains a zkSNARK π∗ for the knowledge of output of
function f (·), π∗ can be submitted as Type I evidence and
the contract checks π∗ immediately.
Construct zkSNARKs. We follow the commit-and-prove
technique [14], which allows a veriﬁer with a commitment
c on a value v to test properties (e.g., set membership) on v,
and focus on composable zkSNARKs [13]. We desire compos-
ability because colluding parties can compute a general f (·)
in collusion. Consider the general relation R to be proven on a
statement and a committed witness v and another free witness.
R can be arithmetic circuits, range, Boolean circuits, matrix
multiplication, etc., and their combinations. legoSNARKs [13]
provides a linker to securely and efﬁciently aggregate different
proof gadgets.

Zero-Knowledge Collusion Resolution (idi accusing)

1) Server idi submits a collusion report to CC,
(type, ed, decommit), indicating evidence type, evidence ed
and the de-commit info for either idi’s query or response
commitment depending on the inputs to function f (·). The
evidence is the (masked) output of computing f (·). If f (·) is
the exchange function, decommit is set to empty.
2) Accused server(s) idj submit a zkSNARK π:
• Type I-query: idj submits π proving that ed does not match

the corresponding query commitment.

• Type I-response: idj submits π proving that ed does not

match the corresponding response commitment.

• Type II: (k − 1) accused servers collectively provide π

proving that ed does not match the function output of f (·).

3) If the accused server(s) fails to submit the proof in time,
CC conﬁrms the accusation. Otherwise, CC checks ed and π,
conﬁrms the accusation if π is rejected.

Fig. 7: Overview of collusion resolution with zkSNARK (ω =
1 companion query).

When the evidence that comes along with the accusation is
a query or response that has been committed to on the board,
the server being accused can submit proof of inequality. If
the accuser does not specify the accused, all other servers
queried in this instance need to provide inequality proofs.
More concretely, suppose we work with Pedersen commitment
with common generators g, h and modulus N . It is perfectly
hiding and satisﬁes our needs. Let m be a message committed
to in c = gmhr with r being the randomness. Suppose a party
accuses with Type I evidence which says that “the exponent of
gs is the message committed to in commitment c”. Note that
the accuser does not reveal s directly. Then what the accused
essentially needs to prove is the statement that c(cid:48) = c/gs is
not a commitment of zero.

Now we consider the scenario where the evidence is a
general function circuit along with its output. Suppose we still
work with Pedersen commitment with common generators g, h
and modulus N . Now suppose a party accuses with Type II
evidence saying that “the exponent of gs is the output of f (·)
which is not an exchange function”. If f (·) is expressed as an
arithmetic circuit, we can employ Groth16 [34], Plonk [29],
legoGroth16 [13] or zk-vSQL [69] (for low-depth ones) with
a polynomial commitment. If f (·) is a Boolean circuit, we can
utilize Adaptive Pinocchio [61] or legoSNARKs with extended
Pedersen commitment.

We implement a zero-knowledge coordinator available at
the same source [33]. We additionally demonstrate a zero-
knowledge veriﬁer contract for the exchange function f (·).
Plonk [29] is employed for proof generation and veriﬁcation.
(1) The ﬁrst step is to generate the arithmetic circuits and
compile them into R1CS (rank-1 constraint system). (2) Then
we generate the witness which is an assignment of input
signals to the R1CS. (3) Plonk necessitates the “Powers of
τ ” phase [9] for a trusted setup. By the end of this step, we
create the proof veriﬁcation key and hand it to the contract.
(4) After the setup, we generate the proof using the proving
key and the witness. (5) The contract then veriﬁes the proof
with the veriﬁcation key. In Table I, we provide the gas costs
under function zkVerify(·).

APPENDIX E
MULTI-SERVER PIR CONSTRUCTIONS

A. 2-server PIR

We ﬁrst describe two related primitives, Function Secret
Sharing (FSS) and its special case Distributed Point Function
(DPF). An m-party FSS scheme for a function family F from
{0, 1}n to an Abelian group G comprises key generation func-
tion Gen(·) and an evaluation function Eval(·). For a function
f ∈ F, algorithm Gen(·) returns m-tuple keys k1, ..., km
where each key ki deﬁnes function fi(x) = Eval(i, ki, x) and
f (x) = (cid:80)m
i=1 fi(x). With less than m keys, it is computation-
ally hard to derive f . DPF [30] is FSS for point functions. A
point function fa,b : {0, 1}n → G for a ∈ {0, 1}n, b ∈ G
evaluates to b on input a and 0 otherwise.
Boyle-Gilboa-Ishai’s Construction for 2-server cPIR. This
construction utilizes a PRG-based 2-party DPF scheme. The

19

client wants to retrieve Da from D = (D1, ..., DN ) and needs
to distribute the point function fa,1 : [N ] → Z2 to two servers.
She generates a pair of keys (k1, k2) with a secure PRG,
obtains function shares (f1, f2) and sends (fi, ki) to server i.
Server i responds with (cid:80)N
Djfi(j). The client then obtains
j=1
Da by xor-ing two responses.

costs, e.g., 124 MB hint plus 242 KB per query communication
for a 1 GiB database.

For TEE-based solutions, the processing time for Bite [48]
(oblivious database) is around 0.5s for incoming client requests
and around 79s for new blocks. T 3 [46] consumes about 0.64-
2.43ms for smaller Oblivious RAM block sizes and read-only
accesses. The two multi-server PIR schemes are more efﬁcient.

APPENDIX F
SELF-INSURANCE

are Ω
(cid:80)T
(cid:80)T

Suppose users arrive at rate Ω

T . At each time unit, there
T queries. The amount of accumulated service fees is
T vS(1 + r)T −t and the amount of ﬁnes needed is
(k−1)psucc
T vI (1−r(cid:48))T −t. We continue to take the ratio
k
of the two quantities

t=1

t=1

Ω

Ω

(cid:80)T

t=0

σ =

(k−1)psucc
l

(cid:80)T

t=1

k2
l

Ω

T vI (1 − r(cid:48))T −t − vI
Ω
T vS(1 + r)T −t

(cid:80)T

=

t=0(k − 1)psuccvI (1 − r(cid:48))T −t − lT
t=1 k2vS(1 + r)T −t

(cid:80)T

Ω vI

We need σ ≤ 1. After setting a privacy protection period T ,
we determine the privacy devaluation rate r(cid:48) and service fee
interest rate r to balance the tension between inexecutable ﬁnes
and insurance caused by a server’s exiting strategy.

If there are s > 1 servers adopting this exiting strategy
during T time units, the amount of ﬁnes needed becomes
(cid:80)T
T svI (1 − r(cid:48))T −t − svI . The accumulated

(k−s)psucc
l

t=1

Ω

service fees stay the same. We can derive σ accordingly.

B. k-server PIR

0 ,k(0)

We have represented the database D as a vector of entries
D = (D1, ..., DN ). Alternatively, we can work in an arbitrary
ﬁnite ﬁeld F and represent each entry in F. Suppose it takes
s elements to describe each entry. Then D can be represented
in F as an s × N matrix. We denote each entry as a vector
Di in this subsection instead of Di to signal this s-element
representation. Dij locates the j-th element of Di.
Haﬁz-Henry’s construction for k-server cPIR. We present
the protocol for querying k = 2K, K ∈ Z+ servers. The client
intends to fetch Da and needs to distribute the point function
fa,1 : [N ] → Z2 to k servers. She generates K independent
1 ),...,(k(K−1)
(2,2)-DPF key pairs, (k(0)
,k(K−1)
), for the
1
point function fa,1. Here k(i)
(i ∈ {0, . . . , K − 1}) represents
i
the key and is different from the parameter k. The client now
sends to each server j the key share (kK−1
). Each
jK−1
server j expands each of these keys into a length-N vector of
bits and then concatenates the K vectors component-wise to
obtain a length-N vector v of K-bit strings. Server j then goes
through v component by component xor-ing the v[i]th word
of Di into a running total when v[i] = 1. By construction,
the a-th bit produced by each DPF key pair differs and all
others are equal; thus, the XOR that each server produces is
identical up to but not including which word of Da it includes;
moreover, one server includes no word of Da at all. Taking this
latter server’s response and xor-ing it with the responses from
each of the other servers yields each of the words comprising
Da (in some random, but known to the querier, order).

, ..., k0
j0

0

For performance evaluation in [35], the experiments are
conducted on a workstation running Ubuntu 18.04.2 LTS on
an AMD Ryzen 7 2700x eight-core CPU @ 4.30 GHz with
16 GiB of RAM and a 1 TB NVMe M.2 SSD. According
to the results therein, sampling DPF keys at the clients’ side
takes less than 100 microseconds and key expansion on the
server side takes about 0.5ms for a database with 220 rows. The
answer reconstruction time is about constant for each extra bit
retrieved. To reconstruct an entry of 1 GiB, the reconstruction
time is about 130 ms.
N ) communi-
Efﬁciency comparison. SealPIR [5] has O(
cation complexity, and incurs ≈ 100x response size overhead
and heavy computation (e.g., 400s for a 30 KB entry with N =
220); OnionPIR [51] has O(log N ) communication costs and
results in 4.2x response size overhead and similar computation
N )
costs; The Corrigan-Gibbss et al. proposal [20] has O(
communication costs and achieves amortized sublinear server
side computation time by having the client download a one-
time hint of size sublinear in |D|; SimplePIR [38] achieves 6.5
N ) communication
GiB/s/core throughput but requires O(

√

√

√

20

