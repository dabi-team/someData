2
2
0
2

b
e
F
8
1

]

R
C
.
s
c
[

1
v
8
3
9
0
1
.
2
0
2
2
:
v
i
X
r
a

Incentive Mechanism Design for Joint Resource
Allocation in Blockchain-based Federated
Learning

Zhilin Wang, Qin Hu, Ruinian Li, Minghui Xu, and Zehui Xiong

1

Abstract—Blockchain-based federated learning (BCFL) has recently gained tremendous attention because of its advantages such as
decentralization and privacy protection of raw data. However, there has been few research focusing on the allocation of resources for
clients in BCFL. In the BCFL framework where the FL clients and the blockchain miners are the same devices, clients broadcast the
trained model updates to the blockchain network and then perform mining to generate new blocks. Since each client has a limited
amount of computing resources, the problem of allocating computing resources into training and mining needs to be carefully
addressed. In this paper, we design an incentive mechanism to assign each client appropriate rewards for training and mining, and
then the client will determine the amount of computing power to allocate for each subtask based on these rewards using the two-stage
Stackelberg game. After analyzing the utilities of the model owner (MO) (i.e., the BCFL task publisher) and clients, we transform the
game model into two optimization problems, which are sequentially solved to derive the optimal strategies for both the MO and clients.
Further, considering the fact that local training related information of each client may not be known by others, we extend the game
model with analytical solutions to the incomplete information scenario. Extensive experimental results demonstrate the validity of our
proposed schemes.

Index Terms—Federated learning, blockchain, resource allocation, incentive mechanism, game theory

(cid:70)

1 INTRODUCTION

S INCE its emergence in 2016, federated learning (FL) has

been greatly developed and widely applied in many
ﬁelds, such as Internet of Things [1]–[3], smart transporta-
tion [4], [5] and healthcare [6]–[8]. One of the most important
advantages of FL is that there is no transmission of raw
data from mobile devices to the centralized server for model
training; instead, by training models on local devices (a.k.a.,
clients) and averaging all local models, FL signiﬁcantly
reduces the possibility of leaking data privacy to a large
extent [9]. However, there are still some challenges that may
restrain the implementation and wide application of FL, e.g.,
the risk of the single point of failure, malicious attacks from
participated clients, and the lack of participation incentives
[10]–[13].

In recent years, researchers resort to blockchain technol-
ogy to tackle the challenges of FL, where the blockchain
system usually works as a decentralized system to provide
incentives and data veriﬁcation [14]–[16]. The combination
of blockchain and FL is termed as blockchain-based FL

This work is partly supported by the US NSF under grant CNS-2105004.

• Zhilin Wang and Qin Hu (corresponding author) are with the Department
of Computer and Information Science, Indiana University-Purdue Uni-
versity Indianapolis, IN, 46202, USA. E-mail: {wangzhil,qinhu}@iu.edu
is with the Department of Computer Science, Bowling
Green State University, Bowling Green, Ohio, 43551, USA. E-mail:
lir@bgsu.edu

• Ruinian Li

• Minghui Xu is with the School of Computer Science and Technology,

Shandong University, China. E-mail: mhxu@sdu.edu.cn

• Zehui Xiong is with Pillar of Information Systems Technology and
Design, Singapore University of Technology Design, Singapore. E-mail:
zehui xiong@sutd.edu.sg

(BCFL). In the BCFL framework, model updates submitted
by clients will be veriﬁed by miners before the global ag-
gregation algorithm is conducted. Once the global model is
obtained, it will be updated into the main chain that can be
accessed by all qualiﬁed participants. Though BCFL can par-
tially address the aforementioned challenges of traditional
FL, there are still some remaining issues that need to be
addressed.

One of the most critical problems in BCFL is the resource
allocation on local devices. Firstly, local devices with het-
erogeneous computational power usually have their own
tasks to ﬁnish, so a universal resource allocation scheme
for all the mobile devices is not practical. In addition, the
whole system may not work effectively and sustainably if
there are no reasonable rewards allocated to clients. Fur-
thermore, both training and mining in the framework of
BCFL consume signiﬁcant amount of resources and time,
and thus it is difﬁcult for clients to appropriately allocate
their limited resources to ensure the performance of the
global model during the required time period. Lastly, since
the system may not be aware of the amount of training data
that each client owns, it can be challenging for the model
owner (MO), i.e., the BCFL task publisher, to make proper
decisions regarding the reward distribution.

There exist very few studies that tackle the above chal-
lenges [17], [18]. The existing studies are based on two
assumptions which are not practical: 1) all clients have
identical computational power and data volume; and 2) the
system knows all the information about clients. To ﬁll the
gap, we propose an incentive mechanism for joint resource
allocation on mobile devices in BCFL that is applicable to
incomplete information scenario.

 
 
 
 
 
 
For the ﬁrst challenge regarding the unbalanced distribu-
tion of resources on mobile devices, we let the clients decide
how much computational power they are willing to devote
into the training and mining tasks by themselves. By this
means, clients can ﬂexibly allocate computation resources
for their own tasks. In addition, in our model, training
and mining are performed sequentially, and the amount
of computational power devoted to these two tasks can be
different.

To overcome the second challenge of motivating clients
to join BCFL, we design an incentive mechanism to reward
clients. Training and mining are two different tasks that
require different amount of computational power, and thus
the rewards should also be different. To ensure a fair dis-
tribution of rewards to all clients, we employ the approach
of Shapely Value (SV) [19] to determine the contributions
of clients in the training process, which will affect the
constraints in their respective optimization problems.

To address the last two challenges, we build the Stackel-
berg game model under both the complete and incomplete
information situations, which are solved separately but with
different insights. Based on the derived optimal solutions,
our system can make optimal decisions in different infor-
mation conditions.

In summary, our contributions in this work can be sum-

marized as below:

• We model the BCFL resource allocation problem as
a two-stage Stackelberg game to help the MO make
decisions on assigning how many rewards to each
client for training and mining and to assist clients
in determining the corresponding amount of com-
putational power to be devoted in each subtask, via
maximizing their respective utilities.
In order to maintain the stability and sustainability
of the whole BCFL system, we design a fair reward
allocation scheme inspired by SV to calculate the
rewards for clients based on their contributions in
the training process.

•

• Considering that the training related information of
devices may not be known to others in the practical
application scenario, we further study the resource
allocation mechanism under the incomplete infor-
mation situation and derive the optimal solutions
accordingly.

• We test our proposed resource allocation mecha-
nisms through extensive experiments. The experi-
mental results show that these mechanisms are ef-
fective.

The rest of this paper is organized as follows. We intro-
duce the system model and problem formulation based on
the two-stage Stackelberg game in Section 2. The detailed
models and solutions under complete and incomplete in-
formation scenarios are reported in Section 3 and Section
4, respectively. Experimental evaluations are presented in
Section 5. We present the related work in Section 6. Finally,
we conclude this paper in Section 7.

2 SYSTEM MODEL AND PROBLEM FORMULATION
In this section, we will illustrate the system model of our
considered blockchain-based federated learning (BCFL) and

then formulate the problem from the perspective of resource
allocation and incentive mechanism design based on Stack-
elberg game. For convenience, we list the key notations in
Table 1.

2

TABLE 1: Key Notations.

Meaning

The set of clients
The total number of clients
The maximum number of client i ’s CPU cycle per
second
The number of CPU cycles per second used to train
The number of CPU cycles per second used to mine
The unit price for training to client i
The unit price for mining to client i
The number of training iterations for clients during
one round of BCFL to submit model update
The data size of client i
The number of CPU cycles used for training each
data sample
The total CPU cycles required to ﬁnish the local
training for generating model updates
The time spent on training for client i
The total CPU cycles used to mine for each client
The time spent on mining for client i
The utility of client i in one round of BCFL
The utility of the MO in one round of BCFL
The optimal CPU cycles per second for training
The optimal CPU cycles per second for mining
The optimal unit price for training to client i
The optimal unit price for mining to client i

Notation
N
N

qi

qti
qmi
pti
pmi

π

Di

di

µi

Ti
ψ
Tmi
Ui
Umo
q∗
ti
q∗
mi
p∗
ti
p∗
mi

2.1 System Overview

Inspired by [20], we consider a fully coupled BCFL system,
which runs FL on a consortium blockchain network. In such
a decentralized BCFL system, the participants in FL work as
the blockchain nodes as well. Speciﬁcally, there are multiple
local devices, termed as clients, working collaboratively to
train a machine learning model, i.e., the global model. The
set of clients can be denoted as N = {1, · · · , i, · · · , N }
with N representing the total number of clients in the
BCFL system. For simplicity, we refer to the work done by
the blockchain for FL as mining in a uniform way, which
does not imply that clients perform mining jobs consuming
excessive amount of computing power like Proof of Work
(PoW) [21]. In our system, we consider that lightweight
consensus algorithms, such as Practical Byzantine Fault
Tolerance (PBFT) [22] and Delegated Proof of Stake (DPoS)
[23], are utilized in the consortium blockchain system.

In our considered BCFL system, each client should be re-
sponsible for both training and mining. Since the workﬂow
of the fully coupled BCFL is that mining starts only after the
training is completed, we assume that training and mining
are not parallel in this paper. Once the training is ﬁnished,
all the clients will upload their local model updates to the
blockchain network so as to be recorded on the blockchain.
Here we deﬁne one round of BCFL as ﬁnishing both the
training and mining processes. Since mobile devices usually
have their own tasks to ﬁnish rather than only contributing
to BCFL, we assume that they will not use all of their
computation resources. In other words, CPU cycles per
second for training and mining can be adjusted strategically
in each round of BCFL.

3

a mechanism because the MO has limited rewards budget,
and it is necessary to make sure that the time consumption
of training and mining can be shortened and a well per-
formed global ML model can be obtained at the same time.
Thus, in the following part, we build the utility models of
both the MO and client from the perspectives of resource
allocation and incentive mechanism.

2.2.1 Client’s Utility
We assume that the maximum number of client i’s CPU
cycles per second is qi, and the number of CPU cycles per
second used to train and mine are qti and qmi, respectively.
Then we have qti, qmi ≤ qi. Let π be the number of training
iterations for clients during one round of BCFL to submit
model update, which is usually ﬁxed for all clients. Let Di be
the number of the data size of client i, and di be the number
of CPU cycles used for training each data sample. Therefore,
we can deﬁne the total CPU cycles required to ﬁnish the
local training to generate model updates as µi = πdiDi.

Since any client i can decide its CPU cycles used to train
the local model, the time used to ﬁnish the local training
varies for each client. We can calculate the time spent on
training for client i via Tti = µi
. Besides, we denote the
qti
total CPU cycles used to mine for each client as ψ, which
is the same for all clients since mining a new block in
blockchain system usually consumes ﬁxed computational
resources. Thus, the time spent on mining can be calculated
as Tmi = ψ
. So we can have the total time cost of client i
qmi
to ﬁnish a round of BCFL task as Ti = Tti + Tmi. Since it is
not possible to let Tti and Tmi be limitless according to the
convergence time requirement, we denote the upper bound
of time consumption in one round of BCFL by T . Thus, we
have Ti ≤ T .

In order to encourage clients to join BCFL, the MO
provides some rewards to clients, where the prices per
second for training and mining are denoted as pti and pmi,
respectively. Clients can allocate unit CPU cycles for training
and mining based on the unit prices given by the MO. Then
the rewards of client i for training and mining to generate
one round of local model updates are Rti = Ttipti and
Rmi = Tmipmi, respectively. Thus, the total rewards for
client i in one round of BCFL is Ri = Rti + Rmi.

Next, we can calculate the energy costs for training and
mining as Cti = ρiµiq2
mi base on a
widely used energy consumption model [24], where ρi is
the parameter correlated to the chip architecture.

ti and Cmi = ρiψq2

In this way, the total cost1 can be calculated as Ci =

Cti + Cmi.

Finally, we can obtain the utility of client i in one round

of BCFL as

Ui = Ri − Ci
µi
qti

pti +

=

ψ
qmi

pmi − ρiµiq2

ti − ρiψq2

mi.

(1)

2.2.2 MO’s Utility

The main concerns related to the utility of the MO are the
performance of the global model, the time consumption and

1. As for the communication cost, since the submissions of clients
are the same, we can consider it as a constant value, which cannot be
optimized anymore and thus is omitted here.

Fig. 1: An illustration of the BCFL system.

The MO is the requester of the FL task, aiming at to
receive a well-trained ﬁnal global model from the BCFL
system. After the FL task is published on the blockchain,
clients start to train their local models and then broadcast
the obtained model updates to the blockchain network once
the local training process is ﬁnished. By this means, the MO
can only access the model updates from all clients rather
than raw data of devices, thus preventing the leakage of
private information for participants. An illustration of our
system model is shown in Fig. 1.

The detailed workﬂow to ﬁnish one BCFL task is as

below:

• The MO publishes an FL task, with the rewards for

training and mining.

• Clients determine the computational resources, i.e.,
CPU cycles per second, used to train the model
and mine for the blockchain based on the rewards
provided by the MO.

• Each client trains the local model, and then broad-
casts the model updates to the blockchain network.
Then, clients start to mine the block.

• Once the block is generated, the model updates are
stored on the blockchain, and the rewards will be
delivered to each client.

• Clients calculate the global model with the veriﬁed
model updates on chain. As long as the expected
performance of the global modal is not reached,
clients will start the next round of training based on
the aggregated global model.

2.2 Utility Models
Since the computational resource of client i is limited and
we assume that client i has other tasks to ﬁnish rather
it is essential to design a
only working for the BCFL,
decision mechanism for client i to allocate CPU cycles for
training and mining, respectively. What’s more, an incentive
mechanism is necessary because clients will be reluctant
to contribute to the BCFL task without receiving enough
compensation for their efforts. It is difﬁcult to design such

Model OwnerBlockchain Client Blockchain-based Federated Learningthe rewards paid to all participants in each round of BCFL,
where the ﬁrst one is a sort of revenue and the last two are
related to the cost for the MO.

Generally, the performance of an ML model will be
affected by the number of CPU cycles spent for training.
Thus, we deﬁne the performance of the global model after
one round of local training and mining as G which can be
calculated by G = f ((cid:80)N
i=1 µi). Here f (·) is a monotonically
increasing function, indicating that more CPU cycles used
for the local training by all clients, the better performance of
the global model after aggregation will be achieved. As for
the MO, its utility depends on the performance of the BCFL
system (G), total time cost ((cid:80)N
i=1(Ti)), and total rewards
distributed to clients ((cid:80)N
i=1(Ri)). Thus, the utility of the
MO in one round of BCFL can be expressed as

Umo = f

= f

(cid:33)

µi

− ξ

(cid:33)

µi

− ξ

(cid:32) N
(cid:88)

i=1
(cid:32) N
(cid:88)

i=1

N
(cid:88)

i=1
N
(cid:88)

i=1

(Ti + Ri)

(cid:18) µi
qti

+

ψ
qmi

+

µi
qti

pti +

(cid:19)

,

pmi

ψ
qmi

(2)

where ξ > 0 is a scalar parameter to balance the revenue
and cost.

2.3 Problem Formulation using Two-stage Stackelberg
Game

According to the above analysis of our system model, client
i provides its computational power to ﬁnish BCFL tasks
based on the rewards given by the MO. In other words, the
unit prices pti and pmi determine the unit computational
power qti and qmi. We can formulate the interactions be-
tween clients and the MO as a two-stage Stackelberg game,
which is widely used for the complete information dynamic
game [25]. In this game, the MO determines the unit prices
of the CPU-cycle frequency used for training and mining,
and then client i decides its CPU cycles per second based on
the received prices, which means that the decision of client
i is impacted by the decision of the MO. In this case, we
can deﬁne the process of the two-stage Stackelberg game as
below:

•

•

Stage I: The MO sets the unit prices per second for
training and learning for each client, i.e., pti and pmi,
via maximizing its own utility, which is speciﬁcally
based on its budget and the total number of CPU cy-
cles consumed for training submitted by each client.
Taking into account the fairness of the distribution
for setting price, we need to design a fair reward
allocation scheme here.
Stage II: After receiving the unit prices from the MO,
clients determine their corresponding computational
power, i.e., qti and qmi, through optimizing their
respective utilities.

In practical situations, qti and qmi are not independent of
each other because of time and reward budget constraints;
similarly, pti and pmi inﬂuence each other as well. There-
fore, we should take these constraints into account when
modeling to make the decisions reasonable.

4

Recall µi in Section 2.2.1, we know that µi is a variable
correlated to the data size of client i and the performance
of the corresponding device, which may not always known
to the MO or the system. As for ψ, it can be predeﬁned by
the system since generating a new block usually consumes
a constant amount of resources. Therefore, we can classify
the two-stage Stackelberg game into information complete
and incomplete scenarios based on whether µi is known to
the MO. The models derived for these two scenarios are
different, and hence the strategies of the MO and clients are
different accordingly, which will be explored in Sections 3
and 4.

3 RESOURCE ALLOCATION WITH COMPLETE IN-
FORMATION

In this section, we will elaborate the expressions of the
proposed Stackelberg game model and the corresponding
solutions for the clients and the MO in the scenario of
complete information, which means that the MO makes
its decisions when µi of each client is known as a prior.
First, we propose a fair reward allocation scheme for clients,
and then we transfer the two-stage Stackelberg game into
two separate optimization problems that are resolved se-
quentially. The methodology we adopt to solve the two
problems is backward induction, which requires analyzing
the optimal strategies of Stage II ﬁrst and then the strategies
of Stage I.

3.1 Fair Reward Allocation

Before we formulate the game model, we should clarify
the fair reward allocation scheme ﬁrst. In our system, we
consider that each client has an equal chance to participate
in both the training and mining processes with fair rewards.
And since the allocation of rewards to each client in training
and mining has a signiﬁcant impact on the system fairness
and further the participation willingness, we need to design
a fair reward allocation scheme. Although we have already
deﬁned the payoff of each client during the training and
mining processes in the above section, it is necessary to
investigate their upper bounds based on the MO’s rewards
budget. And the rewards distribution should not only be
associated with the computing power of the device, but also
take into account the performance of its work. On the one
hand, the reward budget of the MO and the rewards that
each client can get are limited; on the other hand, if the
resources are allocated only based on the computing power
devoted, it could lead to the situation where devices with
sufﬁcient computing power take most of the rewards, while
devices with less power cannot get enough rewards, making
the system unstable and unsustainable.

3.1.1 Upper Bound of Rewards for Mining
For simplicity, we set a ﬁxed total reward budget η in each
round of BCFL. Since the computational power consumed
by generating a new block is constant, with ηm denoting the
upper bound of the reward for mining that all clients can
receive, we have:

ψ
qmi

pmi ≤ Rmi =

ηm
N

,

(3)

where Rmi is the upper bound of the reward for mining that
each client can get.

3.1.2 Upper Bound of Rewards for Training

Since the devices in our BCFL system are assumed to be
homogeneous, and they may have different computing ca-
pabilities, we cannot simply distribute the rewards evenly to
each client. To guarantee the fairness of reward distribution,
we allocate rewards based on the contribution of each client
in the training process. Considering that Shapely Value (SV)
[19] is a methodology which can distribute the rewards to
participants according to their respective contributions, here
we apply it to facilitate reward distribution. The SV of client
i is deﬁned as

SVi(N , v) =

(cid:88)

i /∈S,S⊆N

s!(N − s − 1)!
N !

(v(S ∪ i) − v(S)),

(4)

where S ⊆ N is a subset of clients and s = |S| is the
number of devices in the set S; v(S) is a function describing
the performance of the training result with the client set
S. Then, we give the expression of function v(S). Recall
G in Section 2.2.2, we can assume that v(S) is a function
correlated to G and it can be deﬁned as

v(S) = W −

(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:80)s

i=1 G
s

− g

(cid:13)
(cid:13)
(cid:13)
(cid:13)2

,

(5)

(cid:80)s

(cid:13)
(cid:13)
(cid:13)

and (cid:107)·(cid:107)2 is the Eu-
where W = maxS⊆N
clidean norm; g is the targeted performance value. Then, we
can calculate the upper bound of the reward distributed to
each device for training as:

i=1 G
s − g

(cid:13)
(cid:13)
(cid:13)2

Rti =

SVi(N , v)
v(N )

(η − ηm).

(6)

For each client, its rewards should not exceed the upper

bound, so we can have the following constraint:
µi
qti

pti ≤ Rti.

(7)

3.2 Stage II: Clients Set CPU Cycles Per Second based
on Unit Rewards
Since each client i has a limited amount of computational
resource and should follow the working rules of BCFL, the
goal of client i is to maximize its utility as follows:

s.t.

Problem 1: max : Ui,
µi
qti
ψ
qmi
µi
qti

pti − ρiµiq2

ti ≥ 0,

pmi − ρiψq2

mi ≥ 0,

(8)

(9)

+

ψ
qmi

≤ T, ∀i ∈ N ,

(10)

where the ﬁrst two constraints (8) and (9) mean that client
i wishes to gain non-negative payoffs in both training and
mining; and the last constraint (10) indicates that client i
should ﬁnish the working process, including training and
mining, within the time period T .

5

method of Karush-Kuhn-Tucker (KKT) conditions to solve
it. First, we need to demonstrate that Problem 1 can be
= − µipti
resolved. We can calculate ∂Ui
− 2ρqti < 0 and
∂qti
qti
∂Ui
− 2ρqmi < 0, so it can be proved that Ui is
∂qmi
concave and it has the maximum value. By solving Problem
1, we get the following theorem:

= − µipmi
qmi

Theorem 3.1. The optimal strategies of client i in the scenario of
complete information are given by
(cid:18) pti
ρi

q∗
ti =

(11)

(cid:19) 1

,

3

q∗
mi =

ψ
(cid:16) ρi
pti

T − µi

.

(cid:17) 1

3

(12)

The detailed proof of Theorem 3.1 is in Appendix A.
From the above theorem, we can see that the number of
optimal CPU cycles per second client i putting into training
grows as the unit price for training given by the MO
increases. The optimal CPU cycles per second devoted to
mining is constraint by ψ, indicating that if the mining work
requires more CPU cycles, client i should mine with a larger
q∗
mi.

3.3 Stage I: MO Sets Unit Prices for Clients

The MO expects to get a global model with good per-
formance consuming time and cost for rewards as less as
possible, so its goal is to maximize the utility function Umo,
and the optimization problem can be formulated as follows:

Problem 2: max : Umo,

s.t.

pti ≤ Rti,

µi
qti
ψ
qmi
where (13) and (14) are the constraints of individual rewards
from training and mining to meet the MO’s budget.

pmi ≤ Rmi, ∀i ∈ N ,

(14)

(13)

It is clear that Problem 2 is also a nonlinear optimization
problem, and Umo is also concave, so we can list all the KKT
conditions to ﬁnd its maximum value. Via solving Problem
2, we can have:

Theorem 3.2. The optimal strategies of the MO in the scenario
of complete information are:

p∗
ti =

(cid:18) 1
ρi

(cid:19) 1

2

(cid:32)

(cid:33) 3

2

,

Rti
µi

p∗
mi =

Rmi

T − (ρiµi) 3

2

.

(cid:17) 1

2

(cid:16) 1
Rti

(15)

(16)

The proof of Theorem 3.2 is shown in Appendix B. In the
mi and p∗
optimal solutions above, p∗
ti are highly correlated.
This is because there are time and budget constraints so that
p∗
ti and p∗
mi are not independent variables from each other.
In other words, the MO needs to balance p∗
mi to
satisfy the constraints when making decisions. Furthermore,
we can ﬁnd that µi and ψ inﬂuence the optimal decisions as
well.

ti and p∗

It is clear that Problem 1 is a nonlinear optimiza-
tion problem with inequality constraints, so we adopt the

We summarize the resource allocation mechanism with
complete information in Algorithm 1. The MO calculates the

unit prices given to the client for training and mining ﬁrst,
and then calculates its utility based on the previous unit
prices (Lines 1-2). If Umo is the optimal utility for the MO,
then the optimal decisions of MO can be obtained (Lines 3-
5). Next, the MO sends the unit prices to clients, and each
client calculates the the numbers of CPU cycles per second
used for training and mining; if the utility for client i is
optimal, client i can make its optimal decisions and start to
train and mine (Lines 6-12).

Algorithm 1 Resource Allocation Mechanism with Com-
plete Information

ti, q∗

mi, p∗

ti, p∗
mi

ti ← ˆpti, p∗
p∗

Require: T , µi, ψ, ρi, η, Rmi
Ensure: q∗
1: The MO calculates ˆpti and ˆpmi via (15) and (16)
2: The MO calculates Umo based on ˆpti and ˆpmi via (2)
3: if Umo(ˆpti, ˆpmi) ≥ Umo(pti, pmi) then
mi ← ˆpmi
4:
5: end if
6: The MO sends p∗
7: for i ∈ N do
8:
9:
10:
11:
end if
12:
13: end for
14: return q∗

Calculate ˆqti and ˆqmi via (??) and (12)
if Ui(ˆqti, ˆqmi) ≥ Ui(qti, qmi) then
mi ← ˆqmi

q∗
ti ← ˆqti, q∗
Client i uses q∗

ti to train and q∗

mi to the client i

mi to mine

ti and p∗

ti, q∗

mi, p∗

ti, p∗
mi

In general, the case of complete information is an ideal
situation, and we ﬁnd that it mainly inﬂuences the optimal
decisions of the MO. Therefore, we can study the optimal
decisions in the case of incomplete information by adjusting
the decision mechanism of the MO.

4 RESOURCE ALLOCATION WITH INCOMPLETE IN-
FORMATION

In this section, we will discuss the game model in the case
of incomplete information where the MO has no knowledge
of the true value of µi for each client. Thus, the MO needs
to set the unit price in such a way that each client has a
non-negative payoff, while ensures that the clients report
the value of µi honestly. Before designing the resource
allocation mechanism, we ﬁrst give two deﬁnitions below.

Deﬁnition 4.1. (Individual Rationality). The incentive mecha-
nism for resource allocation is individually rational if the utility
of client i given the rewards provided by the MO is non-negative,
i.e.,

Ui(qti, qmi, pti, pmi, µi) ≥ 0, ∀i.

(17)

Deﬁnition 4.2. (Incentive Compatibility). The incentive mecha-
nism for resource allocation is incentive compatible if each client
can get the optimal utility by reporting its µi truthfully, i.e.,

Ui(qti, qmi, pti, pmi, µi) ≥ Ui(qti, qmi, pti, pmi, ˆµi), ∀i, (18)

where ˆµi represents any value of µi.

Based on the previous analysis, we know that clients’
decisions are made based on their non-negative utility. Since

6

clients should ensure that the rewards they receive are not
less than the total costs they spend, in such a situation,
they can participate in the BCFL task. So in the situation
of incomplete information, the MO needs to guarantee that
its decisions should satisfy (17) to encourage clients to join
the work. Besides, µi of client i is not known by the MO,
and the decisions of the MO are required to be based on the
correct value of µi reported by clients, so the MO needs to
satisfy (18) when making the decisions.

Since the client sets the CPU cycles per second after the
unit prices are given by the MO, the decisions of the client
in the case of incomplete information are the same as those
made under the complete information case as discussed in
Section 3.2. Therefore, we will only focus on the derivation
of the optimal strategies of the MO in this section.

With incomplete information, the MO has to ensure that
the allocation of rewards to all clients is fair, the clients’
utilities are non-negative, and clients report µi truthfully.
Thus, the decision-making problem of the MO with incom-
plete information can be transformed into the following
optimization problem:

Problem 3: max : Umo

s.t.

(13), (14), (17), (18),
∀i ∈ N ,

where (17) and (18) are the constraints of individual ra-
tionality and incentive compatibility for the mechanism;
(13) and (14) are the constraints of individual rewards for
meeting the MO’s budget.

To solve Problem 3, we can ﬁrst write it in Lagrangian
form according to its optimization objective and constraints,
and then analyze its KKT conditions. The optimal solutions
can be solved as below:

Theorem 4.1. The optimal strategies of the MO in the scenario
of incomplete information are

p∗
ti =

(cid:19) 1

2

(cid:18) 1
ρi

p∗
mi =

(cid:18)

T − µi

(cid:32)

Rti
µi
ρi ψ3
(cid:16) ρi
p∗
ti

(cid:33) 3

2

,

(cid:19)3 .

(cid:17) 1

3

(19)

(20)

The proof of Theorem 4.1 is presented in Appendix C.
The optimal solution for p∗
ti in the incomplete information
case is the same as the optimal solution in the complete
information case, while p∗
mi is different. Since the decision
of the MO in the case of incomplete information is not only
inﬂuenced by the budget of the reward, but also required
to satisfy the two conditions (17) and (18) in the above
deﬁnitions. In other words, the decisions in this case is more
conservative so the MO would prefer to minimize its cost
by reducing the payments to training and mining. We will
illustrate the speciﬁc differences about the decisions in the
two scenarios through experiments in Section 5.

The resource allocation mechanism in the incomplete-
information case is presented in Algorithm 2, which is
similar to Algorithm 1, except the decision process of the
MO. In the scenario, the MO should make sure that its utility

7

5.2 Experimental Results

First, we prove the correctness of the optimal strategies
derived from our models. We assume there are 50 clients
in total and each client has the same data size, so we
set µi = 10. In our experiments, for clients and the MO,
there are four strategy combinations, i.e., both sides choose
the optimal strategies, one chooses the random strategies
while the other chooses optimal strategies, and both choose
the random strategies. For example, we deﬁne the strategy
combination Random vs. Optimal as the clients choose the
random strategies and the MO chooses the optimal strategy.
We compare the utilities of clients and the MO with random
strategies and optimal strategies, respectively. The results in
Fig. 2 illustrate that clients and the MO can obtain the higher
utilities than all other strategies when they both choose
the optimal strategies, proving the validity of our proposed
optimal strategies.

is optimal and the utility for each client is non-negative
(Lines 2-7).

Algorithm 2 Resource Allocation Mechanism with Incom-
plete Information

ti, q∗

mi, p∗

ti, p∗
mi

Require: T , µi, ψ, ρi, η, Rmi
Ensure: q∗
1: The MO calculates ˆpti and ˆpmi via (19) and (20)
2: if Umo(ˆpti, ˆpmi) ≥ Umo(pti, pmi) then
3:

The MO calculates the expected utility ˆUi of client i
if ˆUi ≥ 0 then
ti ← ˆpti, p∗
p∗
end if

mi ← ˆpmi

4:
5:
6:
7: end if
8: The MO sends p∗
9: for i ∈ N do
10:
11:
12:
13:
end if
14:
15: end for
16: return q∗

ti, q∗

q∗
ti ← ˆqti, q∗
Client i uses q∗

mi, p∗

ti, p∗
mi

ti and p∗

mi to client i

Calculate ˆqti and ˆqmi
if Ui(ˆqti, ˆqmi) ≥ Ui(qti, qmi) then
mi ← ˆqmi

ti to train and q∗

mi to mine

We can see that the time complexity of both Algorithm
1 and Algorithm 2 is O(n), which means that the time
consumption of solving these two optimization problems
increases with the number of clients linearly. Therefore, our
proposed algorithms can work efﬁciently in practice.

5 EXPERIMENTAL EVALUATION

In this section, we will conduct numerical experiments to
verify and support our designed mechanism. We ﬁrst clarify
the experimental settings and then illustrate the results. We
implement the simulations using Matlab 2019b in macOS
11.0.1 running on Intel i7 processor with 32 GB RAM and 1
TB SSD.

5.1 Experimental Setting

Fig. 2: Utilities changing with strategy pairs.

Then, the experiments will be designed to study the im-
pacts of µi and ψ on the utility of clients and the MO under
the situations of complete and incomplete information. We
set µi ∈ [0, 5] and ψ ∈ [0, 5]. The simulation results are
shown in Fig. 3. We can see that both µi and ψ have a
signiﬁcant impact on the utility of the MO. That is because
the higher CPU power will shorten the time in each round
and improve the performance of the global model. However,
for clients, devoting more CPU cycles does not result in
more utility due to higher energy consumption.

In our experiments, we mainly focus on the impacts of four
variables (i.e., µi, ψ, pti and pmi) on our designed models
under complete and incomplete situations. The basic setting
for these simulations are slight different, and we will clarify
the different parts of the settings in each experiment. For
simplicity of calculation and presentation, we use GHz as
the unit of CPU cycles per second and minute as the unit of
time. We ﬁrst set η = 1500 and Rmi = 5. Since we adopt
SV to calculate the total rewards distributed to individual
client and SV is correlated to the value of µi (see (4) and
(5)), we let G =
. By running the algorithm of SV
we can get the value of Rti for each client. The settings
for other parameters are ρi = 0.01, ξ = 0.1, g = 10 and
T = 15. Note that we conducted extensive experiments with
other experimental settings, while we found that different
values of the parameters would not inﬂuence the trends of
the results. So we only present the results of the experiments
based on the above settings.

i=1 µi
N

(cid:80)N

Fig. 3: Utilities of the client and the MO changing with the
total CPU cycles for training and mining.

We then study the effect of pti and pmi on the utility of
the MO and clients. We set pti ∈ [0, 10] and pmi ∈ [0, 10].
The results are shown in Fig. 4. If the unit price of training
increases, clients can be stimulated to provide more com-
puting power, which reduces the time cost and improves
the model performance, so the MO utility will be improved.
However, the revenue of clients does not grow signiﬁcantly
with the increase of the unit price of training, because the

01020304050The Number of Clients020040060080010001200Total Utility of ClientsOptimal vs. OptimalRandom vs. OptimalOptimal vs. RandomRandom vs. Random01020304050The Number of Clients-1000100200300400Utility of the MOOptimal vs. OptimalRandom vs. OptimalOptimal vs. RandomRandom vs. Random012345Total CPU Cycles for Training (GHz)0100200300400500UtilityClient-CompleteMO-CompleteClient-IncompleteMO-Incomplete012345Total CPU Cycles for Mining (GHz)0200400600800UtilityClient-CompleteMO-CompleteClient-IncompleteMO-Incompletecost of energy consumption also rises. pmi has the same ef-
fect on utility for both complete and incomplete information
cases, and the results are shown on the right side of Fig. 4.
When the unit price of mining increases, the utility of both
clients and the MO can be improved. This is because with
the increase of pmi, clients can receive more mining revenue
by devoting more qmi. At the same time, the MO can reduce
the time cost and improve its utility by encourage clients to
devote more CPU cycles per second for mining.

8

Fig. 6: CPU cycles per second for training and mining
changing with pti.

6 RELATED WORK
Most of the existing studies related to BCFL focus on pro-
tecting privacy, achieving decentralization and improving
the performance of model training [26]–[30]. In our paper,
we mainly focus on the resource allocation and incentive
mechanism design in BCFL. Thus, we provide the literature
review about these two areas in this section.

6.1 Resource Allocation in BCFL

As for resource allocation, researchers mainly consider the
homogeneous computational power of all clients, and make
decisions through the reinforcement learning approach.

In [17], the resource allocation problem is resolved for
the local devices with the same computational power in
BCFL. An upper-bound of the global loss function was
proposed to evaluate the performance of training; in the
meantime, the relationship among update rounds, block
generation rate, and learning rate was explored. Although
the proposed method can easily control the training and
mining time by adjusting the number of updates to allocate
resources, it is based on the assumption that all clients have
the same amount of computing resources and local data,
which is not practical.

Hieu et al. [18] designed a deep reinforcement learning
approach to help mobile devices determine the data volume
and energy used for training and to assist the system to
decide the block generation rate. Neither of the two works
considers how to motivate clients to work honestly and
efﬁciently.

According to the above discussion, it can be seen that
the studies related to resource allocation in BCFL are are
insufﬁcient. One of the reasons is that the research regarding
BCFL is still in the early stage. Another reason is that there
are many types of BCFL structures depending on the role
of the blockchain playing in FL, making it difﬁcult to have
a common framework for resource allocation. In order to
assist the MO and the clients of the BCFL system to make
the proper decisions, we design the mechanisms based on
the two-stage Stackelberg game in this paper. Besides, we
consider allocating resources in the fully coupled BCFL with
FL clients working as blockchain nodes.

6.2 Incentive Mechanism in BCFL

There are some studies about BCFL that focus on regulat-
ing the behaviors of clients through incentive mechanism

Fig. 4: Utilities of the client and the MO changing with the
unit prices of training and mining.

Next, we conduct experiments to analyze the relation-
ship between µi and the unit price for training and mining.
We set µi ∈ [0, 5], and the results are illustrated in Fig. 5.
We can see that both the unit price and the number of CPU
cycles for training increase with µi. This is because if µi
increases, more rewards are needed to motivate clients to
put more computational resources in training. In general,
µi does not affect pmi and qmi a lot, as the beneﬁts of
mining are relatively constant and are more inﬂuenced by
the resource allocation scheme.

Fig. 5: Impacts of µi on CPU cycles per second and unit
prices for training and mining.

In the end, we explore the inﬂuence of pti on both qti
and qmi to ﬁgure out how the decisions of MO inﬂuence the
decisions of client i. We set µi = 10 and pti ∈ [0, 10]. In this
setting, the simulation results are shown in Fig. 6. We can
see that the unit CPU cycles used in local model training has
a positive relationship with the unit price of training offered
by the MO, because more unit rewards for training will in-
centivize clients to put more computational power on model
training. As for CPU cycles per second used in mining, it
decreases with the increase of pti. This makes sense because
if clients are motivated to put more computing power into
training, the training time will be reduced and the mining
time will be correspondingly increased. In this way, clients
do not need to set a high qmi for mining.

0246810The Unit Price of Training020406080100UtilityClient-CompleteMO-CompleteClient-IncompleteMO-Incomplete0246810The Unit Price of Mning020406080100120UtilityClientMO012345Total CPU Cycles for Training012345CPU Cycles Per SecondTrainingMining012345Total CPU Cycles for Training0246810Unit PriceTrainingMining0246810The Unit Price of Training012345CPU Cycles Per Second (GHz)TrainingMiningdesign, thus encouraging them to work honestly and efﬁ-
ciently according to the predeﬁned rules.

Toyoda et al. [31] proposed an economic approach based
on the assumption that clients would act rationally, where
the repeated competition method was utilized to ensure that
clients will follow the protocol. Bao et al. [32] designed an in-
centive mechanism to attract more data and computational
power contributing to the framework of BCFL. In their
proposed system, honest clients can gain fairly partitioned
rewards while the malicious clients will be punished via
timely behaviour detection scheme. In [33], an incentive
mechanism that integrated reputation and contract theory
was proposed to encourage clients to provide high-quality
data to train the local models. As for the fairness of reward
allocation, Liu et al. [34] used Shapley Value (SV) to calculate
the contributions of clients of the FL system and then
allocate the rewards accordingly. However, this approach is
not able to make incentive decisions for training and mining,
respectively.

The existing studies about incentive mechanism de-
sign in BCFL focus on how to provide incentives for FL
through blockchain, without considering the incentives for
blockchain and FL in a systematical manner. In other words,
blockchain and FL are in different phases for BCFL, so they
should both have reasonable incentives. In our paper, we
design a pricing mechanism for the MO based on the com-
puting power provided by clients, thus providing incentives
to the whole BCFL system.

In general, the existing studies have paid little attention
to resource allocation for mobile devices in BCFL and as-
sume that clients join the task voluntarily. To address this
challenge, we design a resource allocation mechanism for
mobile devices, which also offers reward suggestions to the
MO so as to motivate clients to participate in BCFL.

7 CONCLUSION

This paper studies the resource allocation of clients in BCFL
by designing incentive mechanism. We describe the interac-
tions between clients and the MO as a two-stage Stakelberg
game. Within our model, clients with varying computing
power can determine the resources to invest in training and
mining based on the rewards provided by the MO through
maximizing their utilities, while the MO can also obtain the
optimal utility. Since the local training related information
of clients may be not known to the MO, we further study
the game model and optimal solutions in the incomplete
information case. Numerous experimental results show that
our proposed mechanisms are effective.

APPENDIX A
PROOF OF THEOREM 3.1
The Lagrangian correlated to Problem 1 is expressed as:

L1 =

ψ
qmi

pti +

µi
qti
(cid:16)
ρiµiq2

ti −

− λ1

pmi − ρiµiq2

ti − ρiψ

(cid:17)2

(cid:16)

qmi

(cid:17)

pti

− λ2

(cid:16)
ρiψq2

mi −

µi
qti

(cid:17)

pmi

ψ
qmi

(21)

− λ3

(cid:16) µi
qti

+

ψ
qmi

(cid:17)

, ∀i,

− T

where λ1, λ2, and λ3 are non-negative parameters correlated
to the constraints of Problem 1.

9

The KKT conditions are as below:
∂L1
∂qmi

∂L1
∂qti

=

= 0, ∀i,

(cid:19)

pti

ti −

= 0, ∀i,

ρiµiq2

λ1 ≥ 0, λ2 ≥ 0, λ3 ≥ 0, ∀i,
(cid:18)
µi
qti
ψ
qmi
ψ
qmi

= 0, ∀i,

= 0, ∀i,

mi −

pmi

− T

+

(cid:19)

(cid:19)

t

pti − ρiµiq2

ti ≥ 0, ∀i,

λ1

(cid:18)

λ2

λ3

ρiψq2
(cid:18) µi
qti
µi
qti
ψ
qmi

pmi − ρiψq2

mi ≥ 0, ∀i,

µi
qti

+

ψ
qmi

≤ T, ∀i.

(22)

(23)

(24)

(25)

(26)

(27)

(28)

(29)

According to (22), we can have ∂L1
− 2(1 +
∂qti
λ1)ρiuiqti, ∀i. Let the above equation equal to 0 and we have

= uiλ3
q2
ti

λ3
q2
ti

= 2(1 + λ1)ρiqti, ∀i.

Similarly, we have

λ3
q2
mi

= 2(1 + λ2)ρiqmi, ∀i.

(30)

(31)

+ ψ
qmi

Then, let’s consider equation (29). Assume that µi
−
qti
T (cid:54)= 0, ∀i, according to (29), we have λ3 = 0. From (30), we
can see that if λ3 = 0, this equation will be 2(1 + λ1)ρiqti =
0, ∀i, then we have λ1 = −1 < 0. Since λ1 is constrained
by (23), it should always be non-negative. Therefore, this
assumption is invalid. We can obtain the same conclusion
from (31) as well. So we can conclude that for any i, equation
+ ψ
µi
− T = 0 is always satisﬁed. In this way, λ3 > 0
qmi
qti
can be deduced.

Based on the KKT conditions and µi
qti

− T (cid:54)= 0, ∀i,
we can analyze the optimal solutions of Problem 1 as
follows:

+ ψ
qmi

(cid:113) λ3
2ρi

− T = 0, ∀i.

Case 1: λ1 = λ2 = 0, µi
+ ψ
qmi
qti
In this case, since λ1 = λ2 = 0, we can derive qti =
qmi = 3
≥ 0 using (30) and (31), respectively. But λ3
is a non-negative parameter, and it is not a constant value,
so we still can not get the optimal solutions of Problem 1.
Thus, this case is not suitable.

pti = ρiψq2

mi − ψ
qmi

pmi = 0, ∀i, µi
qti

+

ti − µi
qti

ψ
qmi

Case 2: ρiµiq2
− T = 0, ∀i.
By solving ρiµiq2
0, ∀i, we have qti = 3

ti− µi
qti
(cid:113) pti
ρi

pti = 0, ∀i and ρiψq2
(cid:113) pmi
, ∀i and qmi = 3
ρi

mi− ψ
qmi
, ∀i.

pmi =

+ ψ
qmi

Since µi
qti

− T = 0, ∀i, even though the above two
functions can give the expression of the solution of Problem
1, it is still constrained by this function. In other words, the
one of the KKT conditions, i.e., (29), is not satisﬁed. Thus,
this case is not suitable.
(cid:16)
ti − µi
ρiµiq2
qti

= 0, λ2 = 0, µi
qti

+ ψ
qmi

Case 3:

− T =

pti

(cid:17)

0, ∀i.

From (29), we can get the relationship between qti and
(cid:16)
ρiµiq2
= 0 yields
(cid:113) pti
ρi

qmi is qmi = ψ
(cid:113) pti
ρi

, ∀i. Based on qmi = ψ

. Solving

qti = 3

T − µi
qti

, ∀i,

pti

(cid:17)

then we can derive that qm(t) =

, ∀i. From (30) and

ti − µi
qti
, we let qti = 3
T − µi
qti
ψ
T − µi
(cid:114) pti
3
ρi

mi

− 1 and λ2 = λ3
2ρiq3

(31), we have λ1 = λ3
− 1. Since λ3,
2ρiq3
ti
qti, qmi and ρi are positive, so λ1 = λ3
> 0, and λ3 can
2ρiq3
ti
be large enough to make sure λ1 = λ3
≥ 1, thus λ1 ≥ 0
2ρiq3
ti
can be guaranteed. Similarly, λ2 ≥ 0 can be derived. From
the above analysis, Case 3 satisﬁes all the KKT conditions,
therefore the optimal solutions are obtained.
mi − ψ
qmi
T = 0, ∀i. This case is similar to Case 3.

= 0, ∀i, λ1 = 0, µi
qti

+ ψ
qmi

Case 4:

ρiψq2

pmi

−

(cid:17)

(cid:16)

Based on the above analysis, the optimal solutions of
3 , ∀i, and q∗

Problem 1 are q∗

(cid:17) 1

.

mi =

ti =

(cid:16) pti
ρi

ψ
(cid:16) ρi
pti

(cid:17) 1
3

T −µi

Thus Theorem 3.1 is proved.

APPENDIX B
PROOF OF THEOREM 3.2

The Lagrangian correlated to Problem 2 is

L2 = f

− θ1

(cid:32) N
(cid:88)

i=1
(cid:18) µi
qti

(cid:33)

µi

− ξ

N
(cid:88)

i=1
(cid:19)

pti − Rti

− θ2

(cid:18) µi
qti

+

ψ
qmi

+

µi
qti

pti +

(cid:19)

pmi

ψ
qmi

(cid:18) ψ
qmi

pmi − Rmi

(cid:19)

,

(32)

where θ1 and θ2 are the Lagrange multipliers correlated
to the constraints of Problem2. The following constrains
should be met:

∂L2
∂pti

=

∂L2
∂pmi

= 0, ∀i,

θ1

(cid:18) µi
qti
ψ
qmi

θ

(cid:18) µi
qti

pti +

θ1 ≥ 0, θ2 ≥ 0, ∀i,
(cid:19)

pti − Rti

= 0, ∀i,

(cid:19)

pmi − ω

= 0, ∀i,

µi
qti

pti +

ψ
qmi
µi
qti
ψ
qmi

pmi ≤ ω, ∀i.

pti ≤ Rti, ∀i,

pmi ≤ Rmi, ∀i.

(33)

(34)

(35)

(36)

(37)

(38)

(39)

First, let qti = q∗
Case 1: θ1 = 0, ψ
case,
this
In
−(µi(ξpmi+2ξpti+θ2pmi))

ti and qmi = qi(n)∗.
qmi pmi − Rmi = 0, ∀i.

we

can

=
have
, ∀i. Setting this equation equal

∂L2
∂pti

3pti

3(cid:113) pti
ρi
to 0 yields pti = −pmi(ξ+θ2)
. Obviously, we cannot ﬁnd a
positive θ2 to satisfy this equation, making this solution
invalid.

2ξ

Case 2: θ2 = 0, ψ
qti
This case is similar to Case 1.
Case 3: ψ
qti

pti − Rti = 0, ψ

pti − Rti = 0, ∀i.

qmi pmi − Rmi = 0, ∀i.

10

By solving ψ
qti

pti − Rti = 0, ψ

qmi pmi − Rmi = 0, ∀i, we
can get (15) and (16). We can also prove that this case satisfy
the rest of the KKT conditions.

Thus, Theorem 3.2 is proved.

APPENDIX C
PROOF OF THEOREM 4.1

Then, we will provide the solution of Problem 3. The La-
grangian of Problem 3 can be written as

(cid:32) N
(cid:88)

(cid:33)

µi

− ξ

N
(cid:88)

L3 = f

(cid:18) µi
qti

+

ψ
qmi

+

µi
qti

pti +

(cid:19)

pmi

ψ
qmi

i=1
(cid:18) µi
qti
(cid:18) µi
qti

− α1

− α3

pti − Rti

pti +

ψ
qmi

i=1
(cid:19)

− α2

(cid:18) ψ
qmi
pmi − ρiµiq2

(cid:19)

pmi − Rmi

ti − ρiψq2
mi

(cid:19)

.

(40)

where α1, α2 and α3 are the Lagrange multipliers. The KKT
conditions are similar with Problem 2 except the following
three conditions:

pmi − ρiµiq2

ti − ρiψq2
mi

α3 ≥ 0, ∀i,
(cid:19)

= 0, ∀i,

pti +

pmi − ρiµiq2

ti − ρiψq2

mi ≥ 0, ∀i.

α3

(cid:18) µi
qti

pti +

µi
qti

ψ
qmi
ψ
qmi

(41)

(42)

(43)

We then analyze the solutions under different cases.
Actually, there should be nine cases in this problem, but we
only give two of them to analyze since the other situation
can be interpreted similarly.

Case1 : α1 = α2 = α3 = 0, ∀i.
In this case, we can have ∂L3
(cid:113) pti
∂pti
4
3
ρi
it equal to 0 we can get pti = −pmi
. Obviously, since pti and
pmi are non-negative values, we cannot ﬁnd a pmi to satisfy
the above equation. So this case is invalid.

= −(µiξ(pmi+2pti))
3ρi

, and let

pti + ψ
qmi

Case2 : µi
qti

pmi − ρiµiq2
pti −
Rti = 0, α2 = 0, ∀i. By solving the above equations, we
get (19) and (20). We can verify that the solutions above is
incentive compatible and satisfy all the KKT conditions.

ti − ρiψq2

mi = 0, ψ
qti

2

Thus Theorem 4.1 is proved.

REFERENCES

[1] Y. Zhao, J. Zhao, L. Jiang, R. Tan, and D. Niyato, “Mobile
edge computing, blockchain and reputation-based crowdsourc-
ing iot federated learning: A secure, decentralized and privacy-
preserving system,” 2020.

[2] W. Zhang, Q. Lu, Q. Yu, Z. Li, Y. Liu, S. K. Lo, S. Chen, X. Xu, and
L. Zhu, “Blockchain-based federated learning for device failure
detection in industrial iot,” IEEE Internet of Things Journal, vol. 8,
no. 7, pp. 5926–5937, 2020.

[3] Y. Lu, X. Huang, Y. Dai, S. Maharjan, and Y. Zhang, “Blockchain
and federated learning for privacy-preserved data sharing in
industrial iot,” IEEE Transactions on Industrial Informatics, vol. 16,
no. 6, pp. 4177–4186, 2019.

[4] Y. Qi, M. S. Hossain, J. Nie, and X. Li, “Privacy-preserving
blockchain-based federated learning for trafﬁc ﬂow prediction,”
Future Generation Computer Systems, vol. 117, pp. 328–337, 2021.
[5] G. Hua, L. Zhu, J. Wu, C. Shen, L. Zhou, and Q. Lin, “Blockchain-
based federated learning for intelligent control in heavy haul
railway,” IEEE Access, vol. 8, pp. 176 830–176 839, 2020.

[6]

[7]

J. Passerat-Palmbach, T. Farnan, R. Miller, M. S. Gross, H. L.
Flannery, and B. Gleim, “A blockchain-orchestrated federated
learning architecture for healthcare consortia,” arXiv preprint
arXiv:1910.12603, 2019.
S. Aich, N. K. Sinai, S. Kumar, M. Ali, Y. R. Choi, M.-I. Joo, and H.-
C. Kim, “Protecting personal healthcare record using blockchain
& federated learning technologies,” in 2021 23rd International
Conference on Advanced Communication Technology (ICACT).
IEEE,
2021, pp. 109–112.

[8] R. Kumar, A. A. Khan, J. Kumar, A. Zakria, N. A. Golilarz,
S. Zhang, Y. Ting, C. Zheng, and W. Wang, “Blockchain-federated-
learning and deep learning models for covid-19 detection using ct
imaging,” IEEE Sensors Journal, 2021.

[9] B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Ar-
cas, “Communication-efﬁcient learning of deep networks from
decentralized data,” in Artiﬁcial intelligence and statistics. PMLR,
2017, pp. 1273–1282.

[10] E. Bagdasaryan, A. Veit, Y. Hua, D. Estrin, and V. Shmatikov,
“How to backdoor federated learning,” in International Conference
on Artiﬁcial Intelligence and Statistics. PMLR, 2020, pp. 2938–2948.
[11] F. Sattler, S. Wiedemann, K.-R. M ¨uller, and W. Samek, “Robust and
communication-efﬁcient federated learning from non-iid data,”
IEEE transactions on neural networks and learning systems, vol. 31,
no. 9, pp. 3400–3413, 2019.

[12] L. Lyu, H. Yu, and Q. Yang, “Threats to federated learning: A

survey,” arXiv preprint arXiv:2003.02133, 2020.

[13] V. Mothukuri, R. M. Parizi, S. Pouriyeh, Y. Huang, A. Dehghan-
tanha, and G. Srivastava, “A survey on security and privacy of
federated learning,” Future Generation Computer Systems, vol. 115,
pp. 619–640, 2021.

[14] P. Ramanan and K. Nakayama, “Bafﬂe: Blockchain based aggrega-
tor free federated learning,” in 2020 IEEE International Conference
on Blockchain (Blockchain).

IEEE, 2020, pp. 72–81.

[15] Y. J. Kim and C. S. Hong, “Blockchain-based node-aware dy-
namic weighting methods for improving federated learning per-
formance,” in 2019 20th Asia-Paciﬁc Network Operations and Man-
agement Symposium (APNOMS).

IEEE, 2019, pp. 1–4.

[16] Y. Liu, Z. Ai, S. Sun, S. Zhang, Z. Liu, and H. Yu, “Fedcoin: A
peer-to-peer payment system for federated learning,” in Federated
Learning. Springer, 2020, pp. 125–138.

[17] J. Li, Y. Shao, K. Wei, M. Ding, C. Ma, L. Shi, Z. Han, and H. V.
Poor, “Blockchain Assisted Decentralized Federated Learning
(BLADE-FL): Performance Analysis and Resource Allocation,” pp.
1–12, 2021. [Online]. Available: http://arxiv.org/abs/2101.06905

[18] N. Q. Hieu, T. T. Anh, N. C. Luong, D. Niyato, D. I. Kim,
and E. Elmroth, “Resource Management for Blockchain-enabled
Federated Learning: A Deep Reinforcement Learning Approach,”
2020. [Online]. Available: http://arxiv.org/abs/2004.04104
[19] X. Qu, Q. Hu, and S. Wang, “Privacy-preserving model training
architecture for intelligent edge computing,” Computer Communi-
cations, vol. 162, pp. 94–101, 2020.

[20] Z. Wang and Q. Hu, “Blockchain-based federated learning: A
comprehensive survey,” arXiv preprint arXiv:2110.02182, 2021.
[21] S. Nakamoto, “Bitcoin: A peer-to-peer electronic cash system,”

Decentralized Business Review, p. 21260, 2008.

[22] M. Castro, B. Liskov et al., “Practical byzantine fault tolerance,” in

11

[28] Y. Lu, X. Huang, K. Zhang, S. Maharjan, and Y. Zhang, “Blockchain
empowered asynchronous federated learning for secure data shar-
ing in internet of vehicles,” IEEE Transactions on Vehicular Technol-
ogy, vol. 69, no. 4, pp. 4298–4311, 2020.

[29] H. Kim, J. Park, M. Bennis, and S.-L. Kim, “On-device federated
learning via blockchain and its latency analysis,” arXiv preprint
arXiv:1808.03949, 2018.

[30] Q. Hu, Z. Wang, M. Xu, and X. Cheng, “Blockchain and federated
edge learning for privacy-preserving mobile crowdsensing,” IEEE
Internet of Things Journal, 2021.

[31] K. Toyoda and A. N. Zhang, “Mechanism design for an incentive-
aware blockchain-enabled federated learning platform,” in 2019
IEEE International Conference on Big Data (Big Data).
IEEE, 2019,
pp. 395–403.

[32] X. Bao, C. Su, Y. Xiong, W. Huang, and Y. Hu, “Flchain: A
blockchain for auditable federated learning with trust and incen-
tive,” in 2019 5th International Conference on Big Data Computing and
Communications (BIGCOM).

IEEE, 2019, pp. 151–159.

[33] J. Kang, Z. Xiong, D. Niyato, S. Xie, and J. Zhang, “Incentive
Mechanism for Reliable Federated Learning: A Joint Optimization
Approach to Combining Reputation and Contract Theory,” IEEE
Internet of Things Journal, vol. 6, no. 6, pp. 10 700–10 714, 2019.
[34] Y. Liu, S. Sun, Z. Ai, S. Zhang, Z. Liu, and H. Yu, “FedCoin:
A Peer-to-Peer Payment System for Federated Learning,” 2020.
[Online]. Available: http://arxiv.org/abs/2002.11711

Zhilin Wang received his B.S. from Nanchang
University in 2020. He is currently pursuing his
Ph.D. degree of Computer and Information Sci-
ence In Indiana University-Purdue University In-
dianapolis (IUPUI). He is a Research Assis-
tant with IUPUI, and he is also a reviewer of
2022 IEEE International Conference on Commu-
nications (ICC). His research interests include
blockchain, federated learning, edge computing,
and Internet of Things (IoT).

Qin Hu received her Ph.D. degree in Computer
Science from the George Washington University
in 2019. She is currently an Assistant Professor
with the Department of Computer and Informa-
tion Science, Indiana University-Purdue Univer-
sity Indianapolis (IUPUI). She has served on the
Editorial Board of two journals, the Guest Editor
for two journals, the TPC/Publicity Co-chair for
several workshops, and the TPC Member for
several international conferences. Her research
interests include wireless and mobile security,

OSDI, vol. 99, no. 1999, 1999, pp. 173–186.

edge computing, blockchain, and crowdsensing.

[23] D. Mingxiao, M. Xiaofeng, Z. Zhe, W. Xiangwei, and C. Qijun, “A
review on consensus algorithm of blockchain,” in 2017 IEEE inter-
national conference on systems, man, and cybernetics (SMC).
IEEE,
2017, pp. 2567–2572.

[24] T. D. Burd and R. W. Brodersen, “Processor design for portable
systems,” Journal of VLSI signal processing systems for signal, image
and video technology, vol. 13, no. 2, pp. 203–221, 1996.

[25] J. Zhang and Q. Zhang, “Stackelberg game for utility-based coop-
erative cognitiveradio networks,” in Proceedings of the tenth ACM
international symposium on Mobile ad hoc networking and computing,
2009, pp. 23–32.

[26] Z. Peng, J. Xu, X. Chu, S. Gao, Y. Yao, R. Gu, and Y. Tang,
“Vfchain: Enabling veriﬁable and auditable federated learning
via blockchain systems,” IEEE Transactions on Network Science and
Engineering, 2021.

[27] H. B. Desai, M. S. Ozdayi, and M. Kantarcioglu, “Blockﬂa: Ac-
countable federated learning via hybrid blockchain architecture,”
in Proceedings of the Eleventh ACM Conference on Data and Applica-
tion Security and Privacy, 2021, pp. 101–112.

Ruinian Li received the PhD degree in computer
science from the George Washington University
in 2018. He is currently an assistant professor
at the Department of Computer Science, Bowl-
ing Green State University (BGSU), USA. His
research interests include security and privacy-
preserving computations, applied cryptography,
and blockchain technology. He has been working
in a wide area of social networks, auction sys-
tems, and IoT, and his work has been published
in top-tier journals, such as IEEE Transactions
on Services Computing, and IEEE Transactions on Network Science
and Engineering.

Minghui Xu received the BS degree in Physics
from the Beijing Normal University, Beijing,
China, in 2018, and the PhD degree in Computer
Science from The George Washington Univer-
sity, Washington DC, USA, in 2021. He is cur-
rently an Assistant Professor in the School of
Computer Science and Technology, Shandong
University, China. His current research focuses
on blockchain, distributed computing, and quan-
tum computing.

12

Zehui Xiong is currently an Assistant Professor
in the Pillar of Information Systems Technology
and Design, Singapore University of Technol-
ogy and Design. Prior to that, he was a re-
searcher with Alibaba-NTU Joint Research In-
stitute, Singapore. He received the PhD degree
in Nanyang Technological University, Singapore.
He was the visiting scholar at Princeton Uni-
versity and University of Waterloo. His research
interests include wireless communications, net-
work games and economics, blockchain, and
edge intelligence. He has published more than 140 research papers
in leading journals and ﬂagship conferences and many of them are
ESI Highly Cited Papers. He has won over 10 Best Paper Awards in
international conferences and is listed in the World’s Top 2% Scientists
identiﬁed by Stanford University. He is now serving as the editor or
guest editor for many leading journals including IEEE JSAC, TVT, IoTJ,
TCCN, TNSE, ISJ, JAS. He is the recipient of IEEE TCSC Early Career
Researcher Award for Excellence in Scalable Computing, IEEE CSIM
Technical Committee Best Journal Paper Award, IEEE SPCC Technical
Committee Best Paper Award, IEEE VTS Singapore Best Paper Award,
Chinese Government Award for Outstanding Students Abroad, and NTU
SCSE Best PhD Thesis Runner-Up Award. He is the Founding Vice
Chair of Special Interest Group on Wireless Blockchain Networks in
IEEE Cognitive Networks Technical Committee.

