FAIR-BFL: Flexible and Incentive Redesign
for Blockchain-based Federated Learning∗

2
2
0
2

n
u
J

6
2

]

C
D
.
s
c
[

1
v
9
9
8
2
1
.
6
0
2
2
:
v
i
X
r
a

Rongxin Xu
Hunan Key Laboratory of Data Science & Blockchain,
Business School, Hunan University
Changsha 410082, China
rongxinxu@hnu.edu.cn

Shiva Raj Pokhrel†
School of IT, Deakin University
Geelong, VIC 3216, Australia
shiva.pokhrel@deakin.edu.au

Qiujun Lan
Hunan Key Laboratory of Data Science & Blockchain,
Business School, Hunan University
Changsha 410082, China
lanqiujun@hnu.edu.cn

Gang Li
Centre for Cyber Security Research and Innovation,
Deakin University
Geelong, VIC 3216, Australia
gang.li@deakin.edu.au

ABSTRACT
Vanilla Federated learning (FL) relies on the centralized global ag-
gregation mechanism and assumes that all clients are honest. This
makes it a challenge for FL to alleviate the single point of failure and
dishonest clients. These impending challenges in the design philos-
ophy of FL call for blockchain-based federated learning (BFL) due
to the beneﬁts of coupling FL and blockchain (e.g., democracy, in-
centive, and immutability). However, one problem in vanilla BFL is
that its capabilities do not follow adopters’ needs in a dynamic fash-
ion. Besides, vanilla BFL relies on unveriﬁable clients’ self-reported
contributions like data size because checking clients’ raw data is
not allowed in FL for privacy concerns. We design and evaluate
a novel BFL framework, and resolve the identiﬁed challenges in
vanilla BFL with greater ﬂexibility and incentive mechanism called
FAIR-BFL. In contrast to existing works, FAIR-BFL oﬀers unprece-
dented ﬂexibility via the modular design, allowing adopters to ad-
just its capabilities following business demands in a dynamic fash-
ion. Our design accounts for BFL’s ability to quantify each client’s
contribution to the global learning process. Such quantiﬁcation
provides a rational metric for distributing the rewards among fed-
erated clients and helps discover malicious participants that may
poison the global model.

KEYWORDS
Federated Learning, Blockchain, Incentive, Security and Privacy

1 INTRODUCTION
The advent of federated learning (FL) [8] has ameliorated the short-
comings of the centralized ML techniques, which were caused by
the ever-increasing data scale and model complexity. FL addresses
the concerns on data ownership and privacy by ensuring that no
raw data leave the distributed end devices (also referred to as clients).
It successfully employs a single global server in a distributed sys-
tem to collect updates from end devices [21]. FL performs the re-
newal aggregation and iteratively distributes new global learning

∗To appear in ICPP ’22
†Corresponding Author

model to the clients. However, such a FL setup based on central-
ized server suﬀers from issues such as single point of failure and
instability [22]. Moreover, attacks against distributed training of
FL have revealed that malicious or compromised clients/central
servers may upload modiﬁed global parameters, causing model poi-
soning, because attackers can forge local updates to launch infer-
ence attacks [16]. Therefore, the design of a robust FL mechanism
is essential to the stability and security of distributed computing
systems.

As a proven decentralized framework, blockchain naturally pon-
ders the beneﬁts of merging with FL [17], including immutabil-
ity, traceability, and incentive mechanisms, let alone the fact that
both blockchain and FL are inherently distributed by nature. Sev-
eral recent works have been proposed to empower FL’s robustness,
intelligence, and privacy-preserving capabilities by incorporating
blockchain. Blockchain-based Federated Learning (BFL), proposed
in [21], has been considered as one promising and celebrated ap-
proach to facilitating distributed computing and learning. Notable
studies along this line of research include [18–21]. In BFL, local up-
dates and global models can be recorded through the blockchain
to ensure security, and clients would automatically acquire new
global parameters through a consensus mechanism. However, BFL
requires ﬂexibility because adopters’ needs are dynamic, e.g., when
business shrinks, adopters may expect to quickly switch from BFL
to degraded versions (FL or blockchain) for the sake of cost reduc-
tion. Moreover, blockchain rewards those nodes that win the min-
ing competition, but in BFL, we desire to attract potential partici-
pants and keep clients who make great contributions to global up-
dates. Therefore, BFL also needs a novel incentive mechanism. Un-
fortunately, existing works have not adequately studied the ﬂexi-
bility and incentive mechanism in BFL, which we refer to as vanilla
BFL, thus it is diﬃcult for them to move toward practical use. There
exist three more challenges in moving vanilla BFL towards ﬂexibil-
ity and eﬀective incentive.

Tightly coupling blockchain and FL FL has a periodic learning-
updating-waiting process while the blockchain keeps run-
ning. In vanilla BFL, these two play almost independently,

 
 
 
 
 
 
ICPP ’22, August 29-September 1, 2022, Bordeaux, France

Rongxin Xu, Shiva Raj Pokhrel, Qiujun Lan, and Gang Li

thus posing severe concerns. One prevalent concern is un-
wanted consequences such as “forking is inevitable” [21]. Ame-
liorating the impact of forking (as studied in [3, 13]) is non-
trivial as it often loses some local updates and adversely
impacts global learning. It becomes intractable when local
updates recorded in the block can not reﬂect the actual FL
stage and generates empty blocks [2]. Therefore, it is crucial
to tightly couple both blockchain and FL in BFL, especially
to coordinate miners’ behavior.

The rest of this paper is organized as follows. We start with
Section 2, which provides background knowledge on BFL, related
work, and challenges faced by vanilla BFL. Then, we propose FAIR-
BFL in Section 3 and show how it overcomes the challenges with
several novel insights. In Section 4, we reveal how FAIR-BFL pro-
vides unprecedented ﬂexibility through functional scaling and an-
alyze its performance. After that, we move to experiments in Sec-
tion 5 to demonstrate the performance, latency, and security of
FAIR-BFL. Finally, Section 6 concludes this work.

Deﬁning block’s data scope All nodes in the blockchain net-
work have access to the data in the block, so any data which
may reveal the privacy must be avoided from being made
public. As the block size is limited, vanilla BFL may generate
more blocks because it records all the data to complete the
same round of learning. Moreover, each block results from a
round of mining competition, and large blocks can increase
the transfer time. Thus, the delay of vanilla BFL can be high.
So the data recorded in the block should be carefully deﬁned
to reduce the delay of BFL.

Incentivizing based on contribution Blockchain can provide
incentives for FL, whereas it rewards those miners that suc-
cessfully mine blocks. BFL desires to reward clients who
contribute more to the global aggregation to attract poten-
tial participants, especially in data-intensive tasks. To this
end, a method is needed to help BFL diﬀerentiate the client
contributions. At the same time, such a method should not
rely on clients’ self-reported contributions. Otherwise, clients
could have good reasons to cheat, and the BFL cannot ver-
ify who are dishonest since the limitation in checking the
client’s raw data. Unfortunately, vanilla BFL mainly relies
on the client’s self-reporting contributions or checking raw
data to determine rewards.

Therefore, ﬂexibility and eﬀective incentive should be consid-
ered in BFL to fully move towards practical use. It requires enhanc-
ing vanilla BFL with a tightly coupled framework and contribution-
based incentive mechanism to improve performance and security.
The above challenges motivate us to develop new insights in de-
signing the BFL framework. To this end, we propose FAIR-BFL, a
novel BFL framework with ﬂexible and incentive redesign, which
mitigates the above-mentioned issues in vanilla BFL, and our main
contributions can be summarized as follows:

(1) We develop new insights in designing blockchain-based fed-
erated learning framework by coordinating miners’ behav-
ior, recording the desirable global gradients, thus, jointly
improving the ﬂexibility and enhancing performance-cum-
security.

(2) We propose a contribution-based incentive mechanism that
supports quantifying each client’s contribution with vari-
ous clustering algorithms, defending against malicious at-
tacks, and selecting high-contributing clients to speed up
model convergence.

(3) With the incentive mechanism in FAIR-BFL, we propose an
aggregation method to assign clients’ weights based on their
contributions, which improves the performance consider-
ably with guaranteed fairness and convergence.

2 BACKGROUND AND RELATED WORK
Blockchain maintains a distributed ledger to securely record con-
clusive information (called block), in which the nodes compete for
bookkeeping in a mining competition and reach agreement through
a consensus mechanism. The newly generated block is broadcasted
in the network, and those who receive the message will stop their
current computation.

FL employs a distributed learning which allows end devices to
train their own models locally and then aggregates intermediate
information to provide global insights by using local learnings at
a central server. It aims to solve the problem of data island 1 and
beneﬁt from aggregate modelling. Speciﬁcally, at the beginning of
each communication round, the clients update their local models
with their data and upload the obtained gradients to the central
server. After that, the central server computes the global updates
by aggregating all received local gradients and supplies the global
gradients to the clients. Finally, the clients apply the global gradi-
ents to update their local models independently. Thus, FL dynam-
ics evolves from one round to another.

Some notable studies along the lines of BFL include [1, 11, 12,
14, 21]. Among them, Awan et al. [1] designed a variant of the Pail-
lier cryptosystem to support additional homomorphic encryption
and proxy re-encryption, so that the privacy of the transmission
is protected in the BFL. Majeed and Hong [14] adopted the con-
cept of “channels” in their BFL framework, FLchain, to store the
gradient of the local model in the block of a channel. Components
such as Ethereum, extend the capability of FLchain in executing the
global updates. Lu et al. [12] incorporated federated learning with
diﬀerential privacy into permissioned blockchain to alleviate the
centralized trust problem. Li et al. [11] applied the blockchain to
store the global model and exchange the local updates, thus elim-
inating the central server and resisting privacy attacks from ma-
licious clients and central server, it also reduces the computation
time using committee-based consensus.

The aforementioned works developed the vanilla BFL frame-
work for better privacy and data security. However, vanilla BFL de-
sign still faces some non-trivial challenges. For example, the asyn-
chronous nature of blockchain requires in-depth integration into

1Imagine data as a ﬂowing ocean from which some entities collect data and keep it
locally rather than sharing it, e.g., ﬁnancial institutions. Thus, these stagnant data
become islands one after another.

FAIR-BFL: Flexible and Incentive Redesign for Blockchain-based Federated Learning

ICPP ’22, August 29-September 1, 2022, Bordeaux, France

the FL’s communication rounds mechanism. In vanilla BFL frame-
works, blockchain and federated learning are more like two dis-
parate parts that are unnecessarily aligned in terms of their work-
ing states, making it problematic to ensure coherent BFL opera-
tions. Moreover, in most existing vanilla BFL frameworks, objec-
tive evaluation of the client’s contribution has been considered as
irrelevant, while recording all local updates in a few of them [7,
21] arose serious privacy leakage concerns. None of the aforemen-
tioned works focus on the ﬂexibility and incentive mechanism of the
BFL by design. In this paper, we propose FAIR-BFL with a modular
design, novel insights, a fairer aggregation scheme and a contribution-
based incentive mechanism, thus jointly enhancing the performance
and the security.

3 FLEXIBLE AND INCENTIVE REDESIGN FOR

BFL

In this section, we propose FAIR-BFL and develop the algorithm in
detail, and then we demonstrate how to integrate blockchain and
FL tightly by utilizing their internal working principles. Table 1
summarizes all the notations used in this paper.

Table 1: Summary of notations

Notations in this work

The client 𝑖 in BFL and FL, or a worker in blockchain.
The miner 𝑘 in BFL and blockchain, or a server in FL.

𝐶𝑖
𝑆𝑘
D The Data set we used
𝜆
𝜂
𝐸
𝐵
𝑛
𝑚 The number of miners
B Sub-data sets divided by batch size
𝑤

The ratio of randomly selected clients in each round
The learning rate of the model we use
The number of epochs of the client’s local model
The batch size of client’s local model
The number of clients or workers

The gradient in FL or BFL

Identify Contributions

Miner

Reward

Blockchain

Federated 
Learning

Attacker

Figure 1: The framework of FAIR-BFL

Algorithm 1 FAIR-BFL Algorithm
1: Initialization: {𝐶𝑖 }𝑛
𝑖=1, {𝑆𝑘 }𝑚
2: for each round 𝑟 = 1, 2, 3... do
3:

{𝐶𝑖 }𝜆𝑛
for all 𝐶𝑖 ∈ {𝐶𝑖 }𝜆𝑛

𝑘=1, D, 𝜆, 𝜂, 𝐸, 𝐵

𝑖=1 ← 𝑅𝑎𝑛𝑑𝑜𝑚𝑙𝑦 𝑠𝑒𝑙𝑒𝑐𝑡 𝜆𝑛 𝐶𝑖 ∈ {𝐶𝑖 }𝑛

𝑖=1

𝑖=1 do
allocate D𝑖 ∼D to 𝐶𝑖
procedure Local model update(𝐶𝑖, D𝑖 , 𝐵, 𝐸)

read global gradient 𝑤𝑟 from the latest block
B ← 𝑠𝑝𝑙𝑖𝑡 D𝑖 𝑖𝑛𝑡𝑜 𝑏𝑎𝑡𝑐ℎ𝑒𝑠 𝑜 𝑓 𝑠𝑖𝑧𝑒 𝐵
for 𝑒𝑎𝑐ℎ 𝑒𝑝𝑜𝑐ℎ 𝑖 𝑓 𝑟𝑜𝑚 1 𝑡𝑜 𝐸 do
for 𝑒𝑎𝑐ℎ 𝑏𝑎𝑡𝑐ℎ 𝑏 ∈ B do
𝑟 − 𝜂∇ℓ(𝑤𝑖

𝑤𝑖
𝑟 +1 ← 𝑤𝑖

𝑟 ; 𝑏)

procedure Upload local gradients(𝐶𝑖, 𝑤𝑖

𝑟 +1, 𝑆𝑘 )

randomly associate 𝐶𝑖 to 𝑆𝑘
upload updated gradient 𝑤𝑖

𝑟 +1 to 𝑆𝑘

for all 𝑆𝑘 ∈ {𝑆𝑘 }𝑚

do
procedure Exchange gradients({𝑤𝑖

𝑘=1

𝑟 +1}, 𝑆𝑘 )

𝑟 +1, 𝑖 = 𝑖𝑛𝑑𝑒𝑥 𝑜 𝑓 𝑎𝑠𝑠𝑜𝑐𝑖𝑎𝑡𝑒 𝑐𝑙𝑖𝑒𝑛𝑡𝑠 }

𝑊 𝑘
𝑟 +1 ← {𝑤𝑖
broadcast clients updated gradient 𝑊 𝑘
received updated gradient 𝑊 𝑣
for 𝑤 ∈ 𝑊 𝑣
𝑟 +1 do

𝑟 +1
𝑟 +1 form 𝑆𝑣

if 𝑤 /∈ 𝑊 𝑘
𝑊 𝑘

𝑟 +1 then
𝑟 +1 append 𝑤

procedure Computing Global Updates(𝑊 𝑘
𝑤𝑖
𝑟 +1, 𝑤𝑖

𝑤𝑟 +1 ← 1
𝑛

𝑟 +1 ∈ 𝑊 𝑘

𝑟 +1

𝑛

𝑟 +1, 𝑆𝑘 )
⊲ Simple

Average

𝑖=1
P

𝑟 +1 append 𝑤𝑟 +1

𝑊 𝑘
𝐶𝑜𝑛𝑡𝑟𝑖𝑏𝑢𝑡𝑖𝑜𝑛-𝑏𝑎𝑠𝑒𝑑 𝐼𝑛𝑐𝑒𝑛𝑡𝑖𝑣𝑒 𝑀𝑒𝑐ℎ𝑎𝑛𝑖𝑠𝑚(𝑊 𝑘
𝐹𝑎𝑖𝑟 𝐴𝑔𝑔𝑟𝑒𝑔𝑎𝑡𝑖𝑜𝑛(𝑊 𝑘
𝑟 +1)
procedure Block Mining and Consensus(𝑤𝑟 +1, 𝑆𝑘 )

𝑟 +1)
⊲ By Equation (1)

do 𝑝𝑟𝑜𝑜 𝑓 𝑜 𝑓 𝑤𝑜𝑟𝑘
if hash satisﬁes target then
𝑇𝑟𝑎𝑛𝑠. ← 𝑟𝑒𝑤𝑎𝑟𝑑 𝑙𝑖𝑠𝑡
generate and add 𝑏𝑙𝑜𝑐𝑘(𝑇𝑟𝑎𝑛𝑠., 𝑤𝑟 +1)
broadcast

if received 𝑏𝑙𝑜𝑐𝑘𝑖 then

verify 𝑝𝑟𝑜𝑜 𝑓 𝑜 𝑓 𝑤𝑜𝑟𝑘
if hash satisﬁes target then

stop current 𝑝𝑟𝑜𝑜 𝑓 𝑜 𝑓 𝑤𝑜𝑟𝑘
blockchain add 𝑏𝑙𝑜𝑐𝑘𝑖

4:

5:

6:

7:

8:

9:

10:

11:

12:

13:

14:

15:

16:

17:

18:

19:

20:

21:

22:

23:

24:

25:

26:

27:

28:

29:

30:

31:

32:

33:

34:

35:

36:

37:

38:

A high-level view of FAIR-BFL framework is shown in Figure 1,
in which the circled number indicates the corresponding proce-
dure deﬁned and explained in Algorithm 1 and Section 4. We sum-
marize the entire process of BFL into ﬁve procedures that interact
among diﬀerent entities: i) the client reads the global parameters
from the latest block and updates its local model; ii) the client con-
nects to a miner and uploads its local gradient, please note that
some clients may be malicious; iii) miners exchange gradient sets

ICPP ’22, August 29-September 1, 2022, Bordeaux, France

Rongxin Xu, Shiva Raj Pokhrel, Qiujun Lan, and Gang Li

and start the mining competition; iv) the winner identiﬁes the con-
tributions and computes the global updates, and this will help dis-
regard the information forged by the attackers due to low contribu-
tion; v) the winner packs the global update and reward information
into a new block, and then all miners reach an agreement through
the consensus mechanism. More speciﬁcally, the whole process is a
holistic approach with multiple rounds of communication among
𝑛 clients {𝐶𝑖 }𝑛
𝑖=1 with a set of 𝑚 miners {𝑆𝑘 }𝑚
𝑘=1 to handle the
blockchain process. We then utilize data set D, by assuming that
the sub dataset D𝑖 is at the client 𝐶𝑖 before the start of each com-
munication round.

3.1 Coupled BFL design
Vanilla BFL design faces some challenges in ﬂexibility and privacy [21].
On the one hand, the workﬂows of FL and blockchain are incon-
sistent in the vanilla BFL. Hence, miners will continue to compete
for excavation without stopping, which undoubtedly increases re-
source consumption. For example, if a miner does not receive any
gradient update but completes the hash puzzle ahead of other min-
ers, it will generate an empty block, which does not beneﬁt the
FL part. However, [4] have shown that performing SGD based on
communication rounds in FL is better than asynchronous meth-
ods. To this end, we bring it into BFL to achieve the tight coupling
between blockchain and FL, also alleviate issues such as forking,
empty blocks, and resource cost. Note that Assumption 1 has been
made in [7], but only to simplify the problems for analysis. We
have the following Assumption 1.

Assumption 1 (Tight coupling). Clients and miners are fully

synchronized in every communication rounds.

On the other hand, vanilla BFL records every local gradient in
the blockchain, and workers read the block’s information to cal-
culate the global updates themselves. In this case, vanilla BFL is
a white-box for the attacker, malicious nodes can use this infor-
mation to perform privacy attacks and easily track the changes
in a worker’s local gradient to launch more severe model inver-
sion attacks [6]. Furthermore, imagining the BFL applications in
large-scale scenarios, where thousands of local gradients could be
waiting for miners to pack. However, the block size is limited due
to the communication cost and delay, and many local gradients
will miss the current block. Eventually, to calculate the global gra-
dient, workers have to wait for a new block to be generated until
all local gradients have been recorded in the blockchain, which un-
doubtedly increases the latency and the communication costs. To
address the concerns we have the following Assumption 2.

Assumption 2 (Bounding block’s data scope). Miners pack
only the global gradients into blocks. In the end, each block contains
only the global gradient of a speciﬁc round.

Observe that Assumption 2 is to bound the block’s data scope,
also protects the security of FAIR-BFL and alleviates the transaction
queuing caused by the limitation of block size in the asynchronous
design. To the best of our knowledge, this is the ﬁrst attempt to use
Assumption 2 in the BFL design and validate its capability.

Algorithm 2 Client’s Contribution Identiﬁcation Algorithm

Input: 𝑊 𝑘

𝑟 +1, 𝑚𝑜𝑑𝑒𝑙 𝑛𝑎𝑚𝑒, 𝑆𝑡𝑟𝑎𝑡𝑒𝑔𝑦
1: 𝐺𝑟𝑜𝑢𝑝 𝐿𝑖𝑠𝑡 ← 𝐶𝑙𝑢𝑠𝑡𝑒𝑟𝑖𝑛𝑔(𝑚𝑜𝑑𝑒𝑙 𝑛𝑎𝑚𝑒,𝑊 𝑘
2: for 𝑙𝑖 ∈ 𝐺𝑟𝑜𝑢𝑝 𝐿𝑖𝑠𝑡 do
if 𝑤𝑟 +1 ∈ 𝑙𝑖 then
3:

𝑟 +1)

for 𝑤𝑖

𝑟 +1 ∈ 𝑙𝑖 do

𝜃𝑖 ← 𝐶𝑜𝑠𝑖𝑛𝑒 𝐷𝑖𝑠𝑡𝑎𝑛𝑐𝑒(𝑤𝑖
Label 𝐶𝑖 as high contribution

𝑟 +1, 𝑤𝑟 +1)

Append h𝐶𝑖, 𝜃𝑖/

𝜃𝑘 ∗ 𝑏𝑎𝑠𝑒i to 𝑟𝑒𝑤𝑎𝑟𝑑 𝑙𝑖𝑠𝑡

𝜆𝑛

𝑘=1
P

if 𝑤𝑟 +1 /∈ 𝑙𝑖 then
for all 𝑤𝑖

𝑟 +1 ∈ 𝑙𝑖 do

Label 𝐶𝑖 as low contribution

10:
11: 𝑊 𝑘
Output: 𝑟𝑒𝑤𝑎𝑟𝑑 𝑙𝑖𝑠𝑡, 𝑊 𝑘

𝑟 +1 ← 𝑆𝑡𝑟𝑎𝑡𝑒𝑔𝑦(𝑟𝑒𝑤𝑎𝑟𝑑 𝑙𝑖𝑠𝑡,𝑊 𝑘

𝑟 +1)

𝑟 +1

4:

5:

6:

7:

8:

9:

3.2 Accounting Client’s Contribution
Algorithm 2 implements our method to identify contributions in
Line 26 of Algorithm 1. Various clusters of gradients are found by
applying a clustering algorithm on 𝑊 𝑘
𝑟 +1; moreover, they imply
diﬀerent contributions. Note that any suitable clustering algorithm
can be used here as needed, However, we use DBSCAN in experi-
ments by default because it is eﬃcient and straightforward. Those
clients belonging to the same cluster as the global gradient can be
considered a high contribution and be rewarded, while those far
from the global gradient can be considered a low contribution and
adopt a predetermined strategy. There are two strategies: i) keep
all gradients; ii) discard low-contributing local gradients and recal-
culate the global updates 𝑤𝑟 +1. The cosine distance 𝜃𝑖 (the larger
the 𝜃, the farther the distance.) between its local gradient and the
global update is calculated as the weight of its contribution to the
global update for a high contributing client 𝐶𝑖. We can set a 𝑏𝑎𝑠𝑒

and multiply it by 𝜃𝑖/

𝜆𝑛

value pairs h𝐶𝑖, 𝜃𝑖 /

𝜃𝑘 as the ﬁnal reward for client 𝐶𝑖. Key-

𝑘=1
P
𝜃𝑘 ∗𝑏𝑎𝑠𝑒i represent the reward information,

and they are recorded in the 𝑟𝑒𝑤𝑎𝑟𝑑 𝑙𝑖𝑠𝑡. Eventually, when a miner
generates a new block, the reward is distributed according to the
reward list and appended to the current block as transactions. Af-
ter the blockchain consensus is achieved, clients will get these re-
wards. We explain the intuition behind Algorithm 2 as follows.

𝜆𝑛

𝑘=1
P

Privacy preservation Vanilla BFL requires clients to report
their data dimensions for rewards determination. Therefore,
clients have suﬃcient motivation to cheat for more rewards.
We cannot recognize this deception because it is impossible
to check the actual data set, which violates FL’s guidelines.
On the contrary, as the intermediate information, the gradi-
ents can reﬂect both the data size and the data quality. Using
them to perform Algorithm 2 can provide a more objective
assessment and ensure privacy.

FAIR-BFL: Flexible and Incentive Redesign for Blockchain-based Federated Learning

ICPP ’22, August 29-September 1, 2022, Bordeaux, France

Malicious attack resistance Malicious clients may upload fake
local gradients to attack the global model. The clustering al-
gorithm can ﬁnd these fake gradients because they are dif-
ferent from the real ones [16]. We can employ the discard-
ing strategy to avoid skewing the global model with these
spurious gradients, ultimately maintaining the security of
FAIR-BFL.

Clients selection If we adopt the discarding strategy, at the
same time, the corresponding workers will no longer partici-
pate before the round. In this respect, this approach can also
be considered as a new method of clients selection, rather
than simply random selection.

We will thoroughly evaluate our contribution-based incentive

approach in Section 5.

3.3 Fair Aggregation
The optimization problem considered by FAIR-BFL is

𝐹 (w) ,

min
𝑤 (

𝑛

𝑖=1
X

𝑝𝑖𝐹𝑖 (w)

,

)

where 𝐹𝑖(w), 𝑝𝑖 are the local objective function and weight of clients
𝑖, respectively. Consider the simple average aggregation, which
means 𝑝1 = 𝑝2 = ... = 𝑝𝑖 = 1
𝑛 :

𝑤𝑟 +1 ←

1
𝑛

𝑤𝑖

𝑟 +1

𝑛

𝑖=1
X

This is simple average aggregation that treats all clients’ gradi-
ents equally and averages them. However, clients may not have
same sample sizes. Thus, simple averaging does not reﬂect such a
contribution diﬀerence. Instead, we use the following method to
aggregate the global gradients for fairness.

𝑤𝑟 +1 ←

1
𝜆

𝑛

𝑖=1
X

𝑝𝑖𝑤𝑖

𝑟 +1, where 𝑝𝑖 = 𝜃𝑖/

𝜆𝑛

𝜃𝑘

(1)

𝑘=1
X

That is, we assign aggregation weights based on the contribu-
tion of clients to avoid model skew and improve accuracy. At the
same time, it is impractical to require all devices to participate in
the learning process [5, 24], so we assume that all devices are ac-
tivated before the communication round begins. However, only
some devices are selected to upload local gradients.

Although we use fairness aggregation and partial participation,
we can still reveal the stability and convergence dynamics of FAIR-
BFL. For tractability, we have used the following four well-known
assumptions in literature [9, 10, 23, 26].

Assumption 3 (L-smooth). Consider 𝐹𝑖(𝑤) , 1
𝑛

and 𝐹𝑖 is L-smooth, then for all v and w,

ℓ (𝑤; 𝑏𝑖 )

𝑛

𝑖=1
P

𝐹𝑖 (v) ≤ 𝐹𝑖 (w) + (v − w)𝑇 ∇𝐹𝑖(w) +

𝐿
2

kv − wk2
2.

Assumption 4 (µ-strongly). 𝐹𝑖 is u-strongly convex, for all v

and w,

𝐹𝑖(v) ≥ 𝐹𝑖(w) + (v − w)𝑇 ∇𝐹𝑖(w) +

𝜇
2

kv − wk2
2.

Assumption 5 (bounded variance). The variance of stochastic

gradients in each client is bounded by:

E

∇𝐹𝑖

w𝑖

𝑟 , 𝑏𝑖

− ∇𝐹𝑖

w𝑖
𝑟

2

≤ 𝜎2
𝑖

(cid:16)

(cid:17)

(cid:16)

Assumption 6 (bounded stochastic gradient). The expected
squared norm of stochastic gradients is uniformly bounded, thus for
all 𝑖 = 1, · · · , 𝑛 and 𝑟 = 1, · · · , 𝑟 − 1, we have

(cid:17)(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)

E

∇𝐹𝑖

w𝑖

𝑡 , 𝑏𝑖

2

≤ 𝐺 2

(cid:16)

(cid:13)
(cid:13)
(cid:13)

(cid:17)(cid:13)
Assumptions 3 and 4 are essential for analyzing the convergence [9,
(cid:13)
(cid:13)

23, 26]. Both Assumptions 3 and 4 mandate requirements on the
fundamental properties of the loss function. That is, the function
does not change too fast (Assumption 1) or too slow (Assump-
tion 2). Assumptions 5 and 6 are made in [10]. They enable us to
use 𝑤 − 𝑤 ∗ to approximate 𝐹 − 𝐹 ∗.

Theorem 3.1. Given Assumptions 1 to 6 hold, Algorithm 1 con-

verges as follows

E [𝐹 (w𝑟 )] − 𝐹 ∗ ≤

𝜅
𝛾 + 𝑟

(cid:18)

2(𝐵 + 𝐶)
𝜇

+

𝜇(𝛾 + 1)
2

w1 − w∗

2

(cid:13)
(cid:13)

(cid:19)(2)
(cid:13)
(cid:13)
𝜇(𝛾+𝑟 ) , and

𝜇 , 𝛾 = max{8𝜅, 𝐸}, the learning rate 𝜂𝑟 = 2

where 𝜅 = 𝐿
𝐶 = 4
𝐾 𝐸2𝐺 2.

Equation (2) shows that the distance between the actual model
𝐹 and the optimal model 𝐹 ∗ decreases with increasing communica-
tion rounds. FAIR-BFL can converge regardless of the data distribu-
tion because we did not make an IID assumption, and it establishes
the condition that guarantees the convergence of Algorithm 1. The
detailed proof of Theorem 3.1 is provided in Appendix A, which is
further supported by the experimental results in Section 5.

4 FLEXIBILITY BY DESIGN
We re-examined the entire process of vanilla BFL and identiﬁed the
opportunity to achieve ﬂexibility. More speciﬁcally, apart from the
necessary work in the preparation phase, we divide the remaining
part into ﬁve procedures, as shown in Algorithm 1. Depending on
the application’s needs, these ﬁve procedures can be coupled ﬂex-
ibly and dynamically. We present these procedures in detail and
reveal this ﬂexibility, and we further determine the possible delays
in each link to model approximate performance.

4.1 Local Learning and Update
At the beginning of round 𝑟 + 1, each client reads the global gra-
dient 𝑤𝑟 (if any exists) from the last block in the blockchain, and
updates the local model with 𝑤𝑟 . Next, the allocated sub-data sets
D𝑖 will be divided according to the speciﬁed batch size 𝐵. For each
epoch 𝑖 ∈ {1, 2, 3, . . . , 𝐸}, the client 𝐶𝑖 obtains the gradient 𝑤𝑖
𝑟 +1
of round 𝑟 + 1 by performing the SGD, as shown in Equation (3),
where ℓ and 𝜂 are the loss function and the learning rate, respec-
tively.

𝑟 +1 ← 𝑤𝑖
𝑤𝑖

𝑟 − 𝜂∇ℓ(𝑤𝑖

𝑟 ; 𝑏)

(3)

Equation (3) can be calculated D𝑖

𝐵 times with the speciﬁed batch
size 𝐵. Therefore, the time complexity of eq. (3) is O(𝐸 ∗ D𝑖
𝐵 ). Fur-
thermore, we deﬁne the calculation time of this step as the delay

ICPP ’22, August 29-September 1, 2022, Bordeaux, France

Rongxin Xu, Shiva Raj Pokhrel, Qiujun Lan, and Gang Li

T𝑙𝑜𝑐𝑎𝑙 . However, please note that 𝐸 and 𝐵 are set as small constants
for all clients under normal circumstances, so the time complexity
of eq. (3) is linear O(𝑛). The learning rate 𝜂 is often a more con-
cerning issue, as it will signiﬁcantly aﬀect the performance and the
convergence rate. We will explore the impact of the learning rate
on FAIR-BFL in section 5.2.2.

4.2 Uploading the gradient for mining
After the Procedure-I, the client 𝐶𝑖 will get the updated gradient
𝑤𝑖
𝑟 +1 of round 𝑟 +1 and upload it to the miners. There are multiple
miners in the network, and they will pack gradients into blocks. In
addition, BFL miners also need to play a role similar to the central
server. The client does not have to contact all the miners, which
will increase the communication cost, and a better choice is that
each client only uploads gradients to one miner. Here, we make the
probability of selecting each miner as uniform as possible, which
is determined based on the speciﬁc application scenario. Speciﬁ-
cally, the client 𝐶𝑖 generates the miner’s index 𝑘 uniformly and
randomly, then it associates the miner 𝑆𝑘 and uploads the updated
gradient 𝑤𝑖

𝑟 +1.

Note that it is highly risky to directly use these local gradients
for subsequent global updates without verifying, because malicious
clients can easily forge the information and launch the gradient at-
tacks [16]. To avoid this risk, we use the RSA encryption algorithm
to ensure that the identities of both parties are veriﬁed. In the be-
ginning, each client is assigned a unique private key according to
its ID, and the corresponding public key will be held by the miners.
The gradient information received by the miner is signed with the
private key, so the information can be veriﬁed by the public key, as
shown in Figure 2. Further, local gradients can be encrypted using
RSA to ensure data privacy.

Transaction

Signature

Transaction

Clinet

Private key

Public key

Miner

Figure 2: Miners verify transactions through RSA

The procedure will parallelly perform above steps for all clients
in the current round, and the time complexity is O(1). However,
whether it is selecting miners or uploading gradients, the opera-
tion itself will be simple. Nevertheless, the clients are often at the
edge of the network, and the quality of the channel is diﬃcult to
guarantee. It may also be subject to other external disturbances,
where more signiﬁcant delays are possible. For the above consid-
erations, we regard the communication time as the main delay in
this link and record it as T𝑢𝑝 .

4.3 Exchanging Gradients
A miner 𝑆𝑘 , will get the updated gradient set {𝑤𝑖
𝑟 +1} from the asso-
ciated client set {𝐶𝑖 }, where 𝑖 is the index of the clients associated
with 𝑆𝑘 . In the meantime, each miner will broadcast its own gradi-
ent set. Note that we cancel the queuing here by Assumption 1. The

miner will check whether the received transaction exists in the cur-
rent gradient set {𝑤𝑖
𝑟 +1}, and if not, it will append this transaction.
In the end, all miners have the same gradient set.

Miners will also use the RSA encryption algorithm to validate
the transactions from other miners to ensure that the data has not
been tampered with, as described in Figure 2. The above steps are
parallel. For each miner, it does only three things: i) broadcasts the
gradient set owned. ii) receives the gradient sets from other miners.
iii) adds the local gradients which it does not own. That means, the
time complexity of the current procedure is O(𝑚), and we denote
the time required from the start to the moment when all miners
have the same gradient set as T𝑒𝑥 . Normally, the number of miners
will be scarce, so it is easy to ensure good communications among
them, which is also the need of the practical application. Under
such circumstances, T𝑒𝑥 is insigniﬁcant.

4.4 Computing Global Updates
So far, every miner will have all the local updates in this round. In
order to obtain the global gradient 𝑤𝑟 +1, they only need to per-
form fairness aggregation by Equation (1). So that the clients can
initialize the model parameters in the 𝑟 + 1 round.

After that, to evaluate each client’s contribution in this round,
the global gradient 𝑤𝑟 +1 is appended to the current local update
set 𝑊 𝑘
𝑟 +1 to identify client
contributions and issue rewards.

𝑟 +1, then we perform Algorithm 2 on 𝑊 𝑘

The current procedure only needs to compute the global gradi-
ent using Equation (1) and then perform Algorithm 2, so the time
complexity depends on the clustering algorithm, represented as
O(𝑐𝑙𝑢𝑠𝑡𝑒𝑟𝑖𝑛𝑔). Also, we denote the time cost as the delay T𝑔𝑙 .

4.5 Block Mining and Consensus
Once the global gradient calculation is completed, all miners will
immediately enter the mining competition. Speciﬁcally, the miner
will continuously change the nonce in the block header, and then
calculate whether the block’s hash meets the 𝑇 𝑎𝑟𝑔𝑒𝑡 by SHA256.
The whole process can be explained as Equation (4), where 𝑇 𝑎𝑟𝑔𝑒𝑡1
is a large constant, representing the maximum mining diﬃculty.
Note that 𝑇 𝑎𝑟𝑔𝑒𝑡 is the same for all miners, and mining diﬃculty
will be speciﬁed before the algorithm starts. Therefore, the proba-
bility that a miner obtains the right to generate blocks will depend
on the speed of the hash calculation.

𝐻 ( 𝑛𝑜𝑛𝑐𝑒 + 𝐵𝑙𝑜𝑐𝑘) < 𝑇 𝑎𝑟𝑔𝑒𝑡 =

𝑇 𝑎𝑟𝑔𝑒𝑡1
𝑑𝑖 𝑓 𝑓 𝑖𝑐𝑢𝑙𝑡𝑦

(4)

If a miner gets the solution of Equation (4) ahead of other min-
ers, it will immediately pack the global gradient 𝑤𝑟 +1 and reward
information into a new block, and then broadcast this new block.
After receiving this new block, other miners will immediately stop
the current hash calculation, and append the new block to their
blockchain copies, once the validity of the new block is veriﬁed.
Then, it will enter the next communication round. Again, please
recall Assumption 2, with this setting, at the end of a communica-
tion round, the blockchain will only generate one block, and the
blockchain copies of all miners will be the same, which means that
we avoid the blockchain forking, thus there is no need to resolve
ledger conﬂicts while reducing the risk of gradient abandonment

FAIR-BFL: Flexible and Incentive Redesign for Blockchain-based Federated Learning

ICPP ’22, August 29-September 1, 2022, Bordeaux, France

and consensus delay. According to Equation (4), the hash value
will be calculated several times until 𝑇 𝑎𝑟𝑔𝑒𝑡 is met, and the time
of the hash calculation is related to the length of the string. Based
on above discussion, the time complexity is O(𝑛). We record the
time cost here as T𝑏𝑙 , which can be more signiﬁcant compared with
others.

Procedure I. 
Local Learning and Update

Procedure III. 
Exchanging Gradients 

Procedure II. 
Uploading gradient for mining 

Procedure IV. 
Computing Global Updates 

Procedure V. 
Block Mining and Consensus 

Blockchain

Federated Learning

Figure 3: The Coupling structure and complexity of FAIR-
BFL

4.6 Approximate Performance of FAIR-BFL
As shown Figure 3, the aforementioned ﬁve procedures are tightly
coupled to form FAIR-BFL.

Flexibility Guarantee If we remove the Procedure-I and

Procedure-IV, then FAIR-BFL boils down to a pure blockchain
algorithm (see dashed purple rectangle, Figure 3). On the
contrary, if we remove Procedure-III and Procedure-V,
it will be equivalent to the pure FL algorithm (see dashed
orange rectangle, Figure 3). This scale back functionality by
design enables us to easily compare the performance and
delay of those three approaches under the same setup using
the same data set for their comparison. Moreover, we de-
velop analytic model to quantify this ﬂexibility and analyse
the delay of the system Figure 3.

where the above discussion has determined that the delay
here is T𝑏𝑙 . Therefore, the overall complexity of FAIR-BFL is
close to O(𝑛), while with 𝑛 workers and 𝑚 miners, the over-
all delay is 𝑇(𝑛,𝑚) = T𝑙𝑜𝑐𝑎𝑙 + T𝑢𝑝 + T𝑒𝑥 + T𝑔𝑙 + T𝑏𝑙 , which
is compatible with vanilla BFL, so it can quickly learn from
[21] to optimize the block arrival rate to obtain the best de-
lay.

5 EVALUATION AND DISCUSSION
In this section, we conducted a series of experiments to compre-
hensively evaluate the performance of FAIR-BFL on real data set,
Then we reported the changes in performance and delay under
various conditions by adjusting parameters. At last, some novel in-
sights such as the trade-oﬀ between performance and latency are
presented.

5.1 Experimental setup
Our baseline methods for comparison include the Blockchain, Fe-
dAvg [15], and the state-of-the-art FL algorithm FedProx [9]. Then,
we compare the performance of FAIR-BFL and baselines on the
benchmark data set MNIST. The metrics for comparison are the
average delay and the average performance. We calculate the av-
𝑎𝑐𝑐𝑖/𝑛,

𝑑𝑖/𝑟 , and the average accuracy by

erage delay by

𝑛

𝑟

where 𝑑𝑖 represents the delay of the communication round 𝑖, 𝑎𝑐𝑐𝑖
is the veriﬁcation accuracy of client 𝐶𝑖 in a communication round.
By default, we assign data to clients following the non-IID dy-
namics, and we set 𝑛 = 100 and 𝑚 = 2, 𝜂 = 0.01, 𝐸 = 5, and
𝐵 = 10, respectively.

𝑖=1
P

𝑖=1
P

5.2 Performance Impact
For all experiments, We consider the model as converged when the
accuracy in change is within 0.5% for 5 consecutive communica-
tion rounds, and perform 100 communication rounds by default.

5.2.1 General analysis of latency and performance. Figure 4 shows
the simulation results of the general delay and performance. As
shown in Figure 4a, the average delay of FAIR-BFL is between blockchain
and FedAvg, rather than above the blockchain. This implies that As-
sumptions 1 and 2 can eﬀectively reduce the BFL delay. In addition,
from Figure 4b, FAIR-BFL has almost the same model performance
as the FedAvg. FedProx has a lower accuracy than FAIR-BFL, and
the accuracy still ﬂuctuates after the model converges, which is
because it uses the inexact solution to speed up the convergence.

Approximate Performance Analysis The interactions in Fig-
ure 3 are explained as follows. Procedure-I receives the
initial parameters and data set, executes in parallel on each
client, and after a delay T𝑙𝑜𝑐𝑎𝑙 it eventually returns the local
𝑟 +1 for a particular client. Procedure-II runs on
gradient 𝑤𝑖
each miner, receives 𝑤𝑖
𝑟 +1 from the associated client, and
spends T𝑢𝑝 time to return the gradient set {𝑤𝑖
𝑟 +1}. After
that, Procedure-III receives the gradient set {𝑤𝑖
𝑟 +1} of all
miners and then waits for T𝑒𝑥 time to get the complete local
𝑟 +1. Procedure-IV uses 𝑊 𝑘
gradient set 𝑊 𝑘
𝑟 +1 to calculate
the global update 𝑤𝑟 +1 of this round, and the time T𝑔𝑙 con-
sumed depends on the clustering algorithm used. Procedure-V
packs the global gradient 𝑤𝑟 +1 and generates 𝐵𝑙𝑜𝑐𝑘(𝑇𝑟𝑎𝑛𝑠., 𝑤𝑟 +1),

16

14

12

10

8

6

)
s
(
y
a
l
e
d
e
g
a
r
e
v
A

FAIR

Blockchain
FedAvg

1.0

0.8

0.6

0.4

0.2

y
c
a
r
u
c
c
a

e
g
a
r
e
v
A

FAIR

FedAvg
FedProx

0

20

40

60

80

100

Communication rounds

0

200

400
Time in Seconds

600

800

(a) Delay comparison

(b) Accuracy comparison

Figure 4: General comparison of FAIR-BFL and baselines

ICPP ’22, August 29-September 1, 2022, Bordeaux, France

Rongxin Xu, Shiva Raj Pokhrel, Qiujun Lan, and Gang Li

5.2.2
Impact of the Learning rate. We conducted multiple experi-
ments, where 𝜂 ∈ [0.01, 0.05, 0.10, 0.15, 0.20]. The result is shown
in Figure 5. From Figure 5a, we can see that the eﬀect of the learn-
ing rate on the average delay for FAIR-BFL and FedAvg is negligible,
and we attribute it to the distributed (parallel) learning method. It
is interesting to note that FedProx peaks at the beginning, which
implies that it may need a larger 𝜂. Although there is no obvious
impact on the delay, the accuracy is very diﬀerent, as shown in Fig-
ure 5b. For FAIR-BFL and FedAvg, there is an optimal 𝜂 such that
the average accuracy is the highest. For FedProx, the learning rate
does not signiﬁcantly aﬀect the average accuracy. To this end, we
have the following insights.

Insight 1: Due to the beneﬁts of distributed learning, we set the
best learning rate in BFL to ensure the model performance. The
delay overhead for this is acceptable.

s
d
n
o
c
e
s
n
i

y
a
l
e
D

12

10

8

6

4

2

FAIR

FedAvg

0.95

0.90

0.85

0.80

0.75

y
c
a
r
u
c
c
a

e
g
a
r
e
v
A

FAIR

FedAvg
FedProx

0.05

0.10
Learning rate

0.15

0.20

0.05

0.10
Learning rate

0.15

0.20

(a) Average delay changes

(b) Accuracy changes

Figure 5: Performance and delay under various learning
rates

5.2.3
Impact of the number of workers. The increase in the num-
ber of workers will lead to an increase in transactions, thus impact-
ing the delay. Figure 6a shows the delay changes in this case. We
can see that with the increase in the number of workers 𝑛, the de-
lay of blockchain increases, and this is because the total number
of new transactions is rising, while the block size is ﬁxed. When
a block cannot contain all transactions, transaction queuing will
occur, which is regarded as a scalability issue [25] in blockchain.
Please note that when the number of new transactions is much
smaller than the block size, the delay caused by the increase of the
clients will be minimal, such as the curve in interval 𝑛 ∈ [20, 100];
when the total size of new transactions crosses the block size (𝑛 ≥
100), the delay caused by queuing will become more apparent, even-
tually making the delay of blockchain greater than the delay of
FAIR-BFL, which is consistent with the result of Section 5.2.1. On
the contrary, FAIR-BFL achieves a delay similar to FedAvg, It is al-
most unaﬀected by the number of clients. Thanks to Assumptions 1
and 2, no matter how many clients there are, there will be no queu-
ing, because each block only contains the global gradient of the
current round.

Insight 2: The block size will signiﬁcantly aﬀect the delay in
large-scale scenarios. Assumptions 1 and 2 provide an eﬀective
way to solve this problem.

Impact of the number of miners. In contrast to Figure 6a, we
5.2.4
set the number of clients to 100 and increase the number of miners
to fully observe the impact of this change on delay in Figure 6b. It

s
d
n
o
c
e
s
n
i
y
a
l
e
d

12

11

10

9

8

7

6

5

4

3

FAIR

Blockchain
FedAvg

FAIR

Blockchain

s
d
n
o
c
e
s
n
i
y
a
l
e
d

25.0

22.5

20.0

17.5

15.0

12.5

10.0

7.5

20

40

60

80

100

120

Number of workers

(a) Workers

2

4

6
Number of miners

8

10

(b) Miners

Figure 6: Average delay changes with the number of workers
and miners

can be seen that in blockchain, the delay increases approximately
exponentially as the number of miners increases. Because when
more and more nodes participate in the mining competition, the
probability of forking will signiﬁcantly increase, which will take
more time to merge conﬂicts. For FAIR-BFL, this issue is avoided
due to Assumptions 1 and 2. Therefore, the increase in the number
of miners does not signiﬁcantly increase the delay.

Insight 3: Too many miners may cause delay, so the number of
miners should be set appropriately. In BFL, we can alleviate this
issue by Assumptions 1 and 2.

5.3 Cost-eﬀectiveness
Here, we observe how Algorithm 2 with discarding strategy aﬀects
the delay, the accuracy and the convergence rate of FAIR-BFL. We
use DBSCAN as the default clustering algorithm. Note that FedProx
also drops clients to improve both the convergence rate and the
model accuracy. However, FedProx avoids the global model skew
by discarding stragglers, while we discard the low-contributing
clients implied by the clustering algorithm. To demonstrate the ef-
fectiveness of our contribution-based incentive mechanism, we set
the 𝑑𝑟𝑜𝑝_𝑝𝑒𝑟𝑐𝑒𝑛𝑡 of FedProx to 0.02 as a new baseline for compar-
ison.

16

14

12

10

8

6

)
s
(
y
a
l
e
d
e
g
a
r
e
v
A

FAIR-Discard
FAIR

Blockchain
FedAvg

1.0

0.8

0.6

0.4

0.2

y
c
a
r
u
c
c
a

e
g
a
r
e
v
A

0

20

40

60

80

100

0

100

Communication rounds

(a) Delay

FAIR-Discard
FAIR

FedAvg
FedProx-Drop(0.02)

200

300
Time in Seconds

400

(b) Accuracy

Figure 7: FAIR-BFL is faster without reducing accuracy

Then in Figure 7, we observe the diﬀerence in accuracy and de-
lay with and without the discarding strategy, respectively. It can
be seen from Figure 7a that the discarding strategy signiﬁcantly
reduces the average delay, even lower than that of FedAvg. The rea-
son is that those workers with lower contributions no longer par-
ticipate in the current communication round, which means fewer

FAIR-BFL: Flexible and Incentive Redesign for Blockchain-based Federated Learning

ICPP ’22, August 29-September 1, 2022, Bordeaux, France

workers and local gradients. On the one hand, fewer workers re-
duce the time cost for local updates and upload gradients and re-
duce the total number of communications, thus reducing costs. On
the other hand, fewer local gradients accelerate the updates ex-
change and global aggregation, thus reduces the size of data pack-
ets in the network, to save the traﬃc and reduce the channel delays.
More importantly, the model converges better and faster so that
the FAIR-BFL with discarding strategy in Figure 7b lies above the
FedAvg and the original FAIR and reaches the convergence point
between 250 and 300 seconds. Although FedProx also converges
better initially, its accuracy stabilizes around 84%, which is lower
than FAIR-BFL. The reason is that the low-contributing clients no
longer participate in global aggregation, thus reducing the noise
from low-quality data, eﬀectively preventing the global model from
falling into local optimal points, and improving the accuracy. As
discussed above and shown in Figure 7a, the discarding strategy
signiﬁcantly reduces the average delay, and further reduces the
time to reach the convergence. In conclusion, above results con-
ﬁrm that FAIR-BFL is more economic and faster.

Insight 4: Use the discarding strategy in large-scale scenarios
to speed up model convergence and reduce the cost of communi-
cations and traﬃc.

Table 2: Detecting malicious attacks using our contribution-
based incentive mechanism

Distribution Round Attacker Index Drop Index Detection Rate

Non-IID

1
2
3
4
5
6
7
8
9
10

Average Detection Rate

IID

1
2
3
4
5
6
7
8
9
10

[3, 7]
[3, 6, 2]
[6, 4, 7]
[1, 6, 0]
[2, 8, 0]
[7, 0]
[0]
[3, 9]
[6, 0, 8]
[6, 5]

[0, 6, 1]
[0, 3, 6]
[9]
[2]
[6, 3, 1]
[5, 9]
[3]
[7, 0]
[1, 7, 2]
[9]

Average Detection Rate

[2, 4, 5, 6]
[2, 6]
[4, 6]
[6]
[0, 8]
[0, 7]
[0]
[3]
[0, 8]
[5, 6]

[0, 1]
[3, 6, 8]
[9]
[2]
[1, 3]
[5]
[3]
[7]
[1, 7]
[9]

0%
66.66%
66.66%
33%
66.66%
100%
100%
50%
66.66%
100%

64.96%

66.66%
66.66%
100%
100%
66.66%
50%
100%
50%
66.66%
100%

75%

5.4 Security by design
In Section 5.3, we have shown that implementing Algorithm 2 with
a discarding strategy for client selection is eﬀective. Here, we will
demonstrate its security. We set malicious nodes, which modify
the actual local gradients to skew the global model. At the same
time, DBSCAN is also adopted to ﬁnd the diﬀerence in contribu-
tion. There are 10 indexed clients, and in each communication

round, randomly designate 1 to 3 clients as malicious nodes, and
10 rounds are executed in total, as shown in Table 2.

We can see that when there are few malicious nodes (1 mali-
cious node in this experiment), the detection rate almost always
reaches 100%, and FAIR-BFL identiﬁes the forged gradients as with
the low contributions, e.g., in the communication round 7. It means
that with the vast majority of nodes remaining honest, the behav-
ior of the malicious nodes is evident, because the modiﬁed local
gradients are distant from the normal ones. As the number of ma-
licious nodes increase, some forged gradients successfully cheat
this mechanism, so the detection rate decreases, because anom-
alies that are obvious enough may mask those that are not obvious.
Even so, the detection rate is maintained at an optimistic level, for
example, in round 9. We also found that the average detection rate
is higher in the case of IID, which is attributed to the fact that a
good distribution of data makes the normal gradients more spa-
tially concentrated and therefore easier to discover anomalies. In-
terestingly, in general, the detection rate increases as the model
converges. The reason is that as the model converges, individual
local gradients are getting similar. The results and the above dis-
cussion indicate that FAIR-BFL can resist malicious attacks to the
greatest extent even in the case of non-IID.

Let’s recall the major design aspects considered in FAIR-BFL to
ensure security: i) we use the RSA algorithm to sign the local gradi-
ent to avoid modiﬁcation during the upload process (see Figure 2).
ii) the data recorded on the blockchain is immutable; iii) we use Al-
gorithm 2 to reveal the contribution diﬀerences among nodes and
discard low contributing local gradients (forged gradients) to resist
malicious attacks; iv) we do not record any local gradients in the
blockchain, so all nodes cannot observe and exploit this informa-
tion (see Assumption 2). Thus, FAIR-BFL provides the privacy and
security guarantee by design for the whole system dynamics.

6 CONCLUSION
We present a new research problem and develop valuable insights
toward modelling blockchain-based federated learning. FAIR-BFL
comes with a modular design that can dynamically scale functions
according to the adopter’s needs, thus providing unprecedented
ﬂexibility. Moreover, we provably alleviate the impending challenges
of the vanilla BFL in terms of adjusting delay, performance, and
aggregation by tight coupling blockchain and FL and fairness ag-
gregation. This is one of the ﬁrst attempts to redeﬁne the block’s
data scope in BFL in order to prevent clients from observing others’
gradients, thus enhancing privacy and security. More importantly,
FAIR-BFL motivates all clients to contribute actively by identifying
the client’s contribution and issuing uneven rewards. Our exper-
imental results show that FAIR-BFL can achieve desirable perfor-
mance beyond the capacity of existing approaches. Furthermore,
FAIR-BFL that employs the discarding strategy can naturally reap
the beneﬁts of privacy, malicious attack resistance, and client se-
lection.

ACKNOWLEDGMENTS
This research is supported by the National Natural Science Fund
of China (Project No. 71871090), and Hunan Provincial Science &
Technology Innovation Leading Project (2020GK2005).

ICPP ’22, August 29-September 1, 2022, Bordeaux, France

Rongxin Xu, Shiva Raj Pokhrel, Qiujun Lan, and Gang Li

A PROOF OF THEOREM 3.1
As mentioned earlier, w𝑖
𝑟 is the local gradient of the client 𝐶𝑖 at
communication round 𝑟 . For tracking the learning process at each
local epoch, we use the notion of global epoch and I𝐸 = {𝑛𝐸 | 𝑛 =
1, 2, · · · 𝑛}. For convenience, we denote by C𝑟 is the most recent
set of clients (with size 𝐾) selected in the communication round
𝑟 . Note that C𝑟 results from the random selection, and ergodicity
comes from the stochastic gradients. So we slightly abuse the no-
tation EC𝑟 (·), taking the expectation means that we eliminate the
former kind of randomness.

With w𝑟 +1 denoting the global gradient at round 𝑟 +1 and v𝑟 +1
tracking the weighted average of all client-side gradients, we can
see that w𝑟 +1 is unbiased. In particular, w𝑟 +1 = v𝑟 +1. Now, we
formulate the following Lemma A.1 to bound the variance of w𝑟 .

Lemma A.1. For 𝑟 + 1 ∈ I𝐸 , if 𝜂𝑟 is non-increasing and 𝜂𝑟 ≤
2𝜂𝑟 +𝐸 for all 𝑟 ≥ 0, then the expected diﬀerence between v𝑟 +1 and
w𝑟 +1 is

EC𝑟 kv𝑟 +1 − w𝑟 +1 k2 ≤

𝑟 𝐸2𝐺 2
𝜂2

We ﬁrst provide the proof of Lemma A.1 which builds the foun-

dation for proving Theorem 3.1.

Proof. Taking expectation over C𝑟 +1, we have

4
𝐾

𝐾

EC𝑟 kw𝑟 +1 − v𝑟 +1k2 = EC𝑟

v𝑖𝑙
𝑟 +1 − v𝑟 +1

2

(cid:13)
(cid:13)
where the ﬁrst equality follows from v𝑖𝑙
(cid:13)
𝑟 +1 are independent and
unbiased. Since 𝑟 + 1 ∈ I𝐸 , the time 𝑟0 = 𝑟 − 𝐸 + 1 ∈ I𝐸 is
the underlying communication time, which implies
are
identical. Furthermore,
𝑛

w𝑘
𝑟0

𝑘=1

(cid:13)
(cid:13)
(cid:13)

n

o

𝑛

𝑛

2

𝑝𝑖

𝑖=1
X

v𝑖
𝑟 +1 − v𝑟 +1

2

(cid:13)
(cid:13)

(cid:13)
(cid:13)

v𝑖
𝑟 +1 − w𝑟0

−

v𝑟 +1 − w𝑟0

(cid:13)
(cid:16)
(cid:13)
(cid:13)
v𝑖
𝑟 +1 − w𝑟0

(cid:17)
2

(cid:0)

(cid:1)(cid:13)
(cid:13)
(cid:13)

=

≤

𝑖=1
X
𝑛

𝑝𝑖

𝑝𝑖

𝑖=1
X

(cid:13)
(cid:13)

where the last inequality results from

(cid:13)
(cid:13)

𝑛

𝑖=1
X

𝑝𝑖

v𝑖
𝑟 +1 − w𝑟0

= v𝑟 +1 − w𝑡0

(cid:16)

(cid:17)

Ek𝑥 − E𝑥 k2≤ Ek𝑥 k2.

and

Therefore

𝑝𝑖E

v𝑖
𝑟 +1 − w𝑟0

(cid:13)
(cid:13)

𝑝𝑖E

𝑟 +1 − w𝑖
v𝑖
𝑟0

2

(cid:13)
(cid:13)
2

𝑛

𝑖=1
X
𝑛

𝑖=1
X
𝑛

EC𝑟 kw𝑟 +1 − v𝑟 +1 k2 ≤

≤

≤

≤

1
𝐾

1
𝐾

1
𝐾

1
𝐾

1
𝐾 2
𝑛

𝑘=1
X

=

1
𝐾

𝑙 =1 (cid:13)
X
(cid:13)
(cid:13)
v𝑘
𝑟 +1 − v𝑟 +1
𝑝𝑖

(cid:13)
(cid:13)
(cid:13)

2

(cid:13)
(cid:13)
where

As shown in Section 4.4 we adopt the partial aggregation and

therefore the temporal evolution follow

𝑟 +1 = 𝑤𝑖
v𝑖

w𝑖

𝑟 +1 =

(

𝑤𝑖
(cid:16)

𝑟 ; 𝑏

𝑟 − 𝜂𝑖 ∇𝐹𝑖
v𝑖
𝑟 +1
samples C𝑟 and average

(cid:17)

Nex, we have

if 𝑟 + 1 /∈ I𝐸
if 𝑟 + 1 ∈ I𝐸 .

v𝑖
𝑟 +1

n

o

w𝑟 +1 − w∗

2 =

w𝑟 +1 − v𝑟 +1 + v𝑟 +1 − w∗

2

(cid:13)
(cid:13)

(cid:13)
(cid:13)

= kw𝑟 +1 − v𝑟 +1 k2

(cid:13)
(cid:13)

+

v𝑟 +1 − w∗

(cid:13)
(cid:13)

2

𝐴1

𝐴2
w𝑟 +1 − v𝑟 +1, v𝑟 +1 − w∗
}
{z

{z

(cid:13)
(cid:13)
|

(cid:13)
(cid:13)
}

𝐴3

(cid:11)

|
+ 2

(cid:10)

Considering unbiasedness of 𝑤𝑟 +1, 𝐴3 will be vanished when we
take expectation over C𝑟 +1. If 𝑟 + 1 /∈ I𝐸, 𝐴1 vanishes since
w𝑟 +1 = v𝑟 +1. Using Lemma A.1 to bound 𝐴2, we get

{z

|

}

E

w𝑟 +1 − w∗

2 ≤ (1 − 𝜂𝑟 𝜇) E

★
w𝑟 − w

2 + 𝜂2

𝑟 𝐵.

When 𝑟 + 1 ∈ I𝐸 , 𝐴1, we apply Lemma A.1 to bound 𝐴1 and then
2 = E kw𝑟 +1 − v𝑟 +1 k2 + E

(cid:13)
(cid:13)
w𝑟 +1 − w∗

(cid:13)
(cid:13)
v𝑟 +1 − w∗

(cid:13)
(cid:13)

(cid:13)
(cid:13)

E

2

(5)

≤ (1 − 𝜂𝑟 𝜇) E

★
w𝑟 − w

2 + 𝜂2
(cid:13)
(cid:13)

(cid:13)
𝑟 (𝐵 + 𝐶)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

EC𝑟 kv𝑟 +1 − w𝑟 +1 k2 .

𝐶 ≥

1
𝜂2
𝑟

Equation (5) recursively portrays the distance between w𝑟 +1
and w∗, and we will show how it is a decreasing function. Let
∆𝑟 = E kw𝑟 − w∗ k2, Equation (5) can be simpliﬁed to
∆𝑟 +1 6 (1 − 𝜂𝑟 𝜇)∆𝑟 + 𝜂𝑟

2(𝐵 + 𝐶)

(6)

For decreasing stepsize 𝜂𝑟 = 𝛽
= 1
min
Equation (6). Therefore, we can see

𝑟 +𝛾 , we need 𝛽 > 1
𝜇 and 𝛾 > 0. 𝜂1 ≤
4𝐿 and 𝜂𝑟 ≤ 2𝜂𝑟 +𝐸 is an important condition for

𝜇 , 1
1
4𝐿

n

o

∆𝑟 +1 ≤

𝑣
𝛾 + 𝑟

(7)

when

𝑣 = max

𝛽2(𝐵 + 𝐶)
𝛽𝜇 − 1

(cid:26)

, (𝛾 + 1)

w1 − w∗

2

.

(cid:13)
(cid:13)

(cid:27)

(cid:13)
(cid:13)

Now we use mathematical induction. When 𝑟 = 1, Equation (7)
holds from the deﬁnition of 𝑣. Assuming that the conclusion holds
for some 𝑟 , then at 𝑟 + 1,

∆𝑟 +1 ≤ (1 − 𝜂𝑟 𝜇) ∆𝑟 + 𝜂2
𝑟 𝐵

(cid:13)
(cid:13)

𝑟

𝑝𝑖𝐸

𝑖=1
X
𝐸2𝜂2

𝑡=𝑟0
X
𝑟0𝐺 2 ≤

4
𝐾

E

(cid:13)
(cid:13)
𝜂𝑟 ∇𝐹𝑖
(cid:13)
(cid:13)
𝜂2
𝑟 𝐸2𝐺 2
(cid:13)

2

w𝑖

𝑟 , 𝑏

(cid:16)

(cid:17)(cid:13)
(cid:13)
(cid:13)

(cid:3)

≤

=

≤

𝑣
𝑟 + 𝛾

+

𝛽2𝐵
(𝑟 + 𝛾)2

𝛽2𝐵
(𝑟 + 𝛾)2

−

𝛽𝜇 − 1
(𝑟 + 𝛾)2

𝑣

(cid:21)

(cid:20)

1 −

𝛽𝜇
𝑟 + 𝛾
(cid:18)
𝑟 + 𝛾 − 1
(𝑟 + 𝛾)2
𝑣
𝑟 + 𝛾 + 1

(cid:19)

𝑣 +

                
                
            
            
                                 
                                 
FAIR-BFL: Flexible and Incentive Redesign for Blockchain-based Federated Learning

ICPP ’22, August 29-September 1, 2022, Bordeaux, France

[16] Milad Nasr, Reza Shokri, and Amir Houmansadr. 2019. Comprehensive Privacy
Analysis of Deep Learning: Passive and Active White-box Inference Attacks
against Centralized and Federated Learning. In 2019 IEEE Symposium on Secu-
rity and Privacy (SP). 739–753. https://doi.org/10.1109/SP.2019.00065

[17] Dinh C. Nguyen, Ming Ding, Quoc-Viet Pham, Pubudu N. Pathirana, Long Bao
Le, Aruna Seneviratne, Jun Li, Dusit Niyato, and H. Vincent Poor. 2021. Fed-
erated Learning Meets Blockchain in Edge Computing: Opportunities and
Challenges.
IEEE Internet of Things Journal 8, 16 (Aug. 2021), 12806–12825.
https://doi.org/10.1109/JIOT.2021.3072611

[18] Shiva Raj Pokhrel. 2020. Federated learning meets blockchain at 6G edge: a
drone-assisted networking for disaster response. In Proceedings of the 2nd ACM
MobiCom Workshop on Drone Assisted Wireless Communications for 5G and Be-
yond (DroneCom ’20). Association for Computing Machinery, New York, NY,
USA, 49–54. https://doi.org/10.1145/3414045.3415949

[19] Shiva Raj Pokhrel. 2021.

Blockchain Brings Trust

to Collaborative
Drones and LEO Satellites: An Intelligent Decentralized Learning in
the Space.
IEEE Sensors Journal 21, 22 (Nov. 2021), 25331–25339.
https://doi.org/10.1109/JSEN.2021.3060185 Conference Name: IEEE Sensors
Journal.

[20] Shiva Raj Pokhrel and Jinho Choi. 2020. A Decentralized Federated Learn-
ing Approach for Connected Autonomous Vehicles.
In 2020 IEEE Wire-
less Communications and Networking Conference Workshops (WCNCW). 1–6.
https://doi.org/10.1109/WCNCW48565.2020.9124733

[21] Shiva Raj Pokhrel and Jinho Choi. 2020.

Federated Learning With
Blockchain for Autonomous Vehicles: Analysis and Design Challenges.
IEEE Transactions on Communications 68, 8 (Aug. 2020), 4734–4746.
https://doi.org/10.1109/TCOMM.2020.2990686

[22] Rodrigo Roman,

features
ternet of
https://doi.org/10.1016/j.comnet.2012.12.018

and challenges of
things.

Jianying Zhou, and Javier Lopez. 2013.

On the
security and privacy in distributed in-
Computer Networks 57, 10 (July 2013), 2266–2279.

[23] Sebastian U. Stich. 2019. Local SGD converges fast and communicates little. In
7th international conference on learning representations, ICLR 2019, new orleans,
LA, USA, may 6-9, 2019. OpenReview.net. https://arxiv.org/abs/1805.09767
[24] Jianyu Wang, Qinghua Liu, Hao Liang, Gauri Joshi, and H. Vincent Poor. 2020.
Tackling the Objective Inconsistency Problem in Heterogeneous Federated Opti-
mization. In Advances in Neural Information Processing Systems, Vol. 33. Curran
Associates, Inc., 7611–7623.

[25] Kaiwen Zhang and Hans-Arno Jacobsen. 2018. Towards Dependable, Scalable,
and Pervasive Distributed Ledgers with Blockchains.. In ICDCS. 1337–1346.
[26] Yuchen Zhang, Martin J Wainwright, and John C Duchi. 2012. Communication-
eﬃcient algorithms for statistical optimization. In Advances in neural informa-
tion processing systems, F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger
(Eds.), Vol. 25. Curran Associates, Inc. https://doi.org/10.1109/cdc.2012.6426691

Using strong convexity of 𝐹 (·),

E [𝐹 (w𝑟 )] − 𝐹 ∗ ≤

𝐿
2

∆𝑟 ≤

𝐿
2

𝑣
𝛾 + 𝑟

.

When 𝛽 = 2
𝜂𝑟 = 2
𝜇

8 𝐿
𝜇 , 𝐸
1
𝛾+𝑟 , which satisﬁes 𝜂𝑟 ≤ 2𝜂𝑟 +𝐸. Therefore,

𝜇 , 𝛾 = max

− 1 and with 𝜅 = 𝐿

n

o

𝜇 , we have

E [𝐹 (w𝑟 )] − 𝐹 ∗ ≤

𝜅
𝛾 + 𝑟

(cid:18)

2(𝐵 + 𝐶)
𝜇

+

𝜇(𝛾 + 1)
2

This proves Theorem 3.1.

w1 − w∗

2

(cid:13)
(cid:13)

(cid:19)

(cid:13)
(cid:13)

REFERENCES
[1] Sana Awan, Fengjun Li, Bo Luo, and Mei Liu. 2019.

Poster: A Reliable
and Accountable Privacy-Preserving Federated Learning Framework using the
Blockchain. In Proceedings of the 2019 ACM SIGSAC Conference on Computer and
Communications Security (CCS ’19). Association for Computing Machinery, New
York, NY, USA, 2561–2563. https://doi.org/10.1145/3319535.3363256

[2] Xianglin Bao, Cheng Su, Yan Xiong, Wenchao Huang, and Yifei Hu. 2019.
FLChain: A Blockchain for Auditable Federated Learning with Trust and Incen-
tive. In 2019 5th International Conference on Big Data Computing and Communi-
cations (BIGCOM). 151–159. https://doi.org/10.1109/BIGCOM.2019.00030
[3] Bin Cao, Yixin Li, Lei Zhang, Long Zhang, Shahid Mumtaz, Zhenyu Zhou,
and Mugen Peng. 2019. When Internet of Things Meets Blockchain: Chal-
lenges in Distributed Consensus.
IEEE Network 33, 6 (Nov. 2019), 133–139.
https://doi.org/10.1109/MNET.2019.1900002

[4] Jianmin Chen, Xinghao Pan, Rajat Monga, Samy Bengio, and Rafal Jozefowicz.
2017. Revisiting Distributed Synchronous SGD. arXiv:1604.00981 [cs] (March
2017). http://arxiv.org/abs/1604.00981

[5] Yae Jee Cho, Jianyu Wang, and Gauri Joshi. 2022. Towards Understanding Bi-
ased Client Selection in Federated Learning. In Proceedings of The 25th Inter-
national Conference on Artiﬁcial Intelligence and Statistics. PMLR, 10351–10375.
https://proceedings.mlr.press/v151/jee-cho22a.html ISSN: 2640-3498.

[6] Matt Fredrikson, Somesh Jha, and Thomas Ristenpart. 2015. Model Inversion At-
tacks that Exploit Conﬁdence Information and Basic Countermeasures. In Pro-
ceedings of the 22nd ACM SIGSAC Conference on Computer and Communications
Security (CCS ’15). Association for Computing Machinery, New York, NY, USA,
1322–1333. https://doi.org/10.1145/2810103.2813677

[7] Hyesung Kim,

Jihong Park, Mehdi Bennis, and Seong-Lyun Kim. 2020.
Blockchained On-Device Federated Learning. IEEE Communications Letters 24,
6 (June 2020), 1279–1283. https://doi.org/10.1109/LCOMM.2019.2921755

[8] Jakub Konečný, H. Brendan McMahan, Daniel Ramage, and Peter Richtárik. 2016.
Federated Optimization: Distributed Machine Learning for On-Device Intelli-
gence. arXiv:1610.02527 [cs] (Oct. 2016). http://arxiv.org/abs/1610.02527

[9] Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and
Virginia Smith. 2020. Federated optimization in heterogeneous networks. In
Proceedings of machine learning and systems, I. Dhillon, D. Papailiopoulos, and
V. Sze (Eds.), Vol. 2. 429–450.

[10] Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, and Zhihua Zhang. 2020.
On the convergence of FedAvg on non-iid data. In 8th international conference
on learning representations, ICLR 2020, addis ababa, ethiopia, april 26-30, 2020.
OpenReview.net. https://openreview.net/forum?id=HJxNAnVtDS

[11] Yuzheng Li, Chuan Chen, Nan Liu, Huawei Huang, Zibin Zheng, and Qiang
Yan. 2021. A Blockchain-Based Decentralized Federated Learning Frame-
work with Committee Consensus.
IEEE Network 35, 1 (Jan. 2021), 234–241.
https://doi.org/10.1109/MNET.011.2000263

[12] Yunlong Lu, Xiaohong Huang, Yueyue Dai, Sabita Maharjan, and Yan Zhang.
2020. Blockchain and Federated Learning for Privacy-Preserved Data Sharing
in Industrial IoT.
IEEE Transactions on Industrial Informatics 16, 6 (June 2020),
4177–4186. https://doi.org/10.1109/TII.2019.2942190

[13] Chuan Ma, Jun Li, Ming Ding, Long Shi, Taotao Wang, Zhu Han, and
H. Vincent Poor. 2021. When Federated Learning Meets Blockchain: A
New Distributed Learning Paradigm.
arXiv:2009.09338 [cs] (June 2021).
http://arxiv.org/abs/2009.09338

[14] Umer Majeed and Choong Seon Hong. 2019.

FLchain: Federated
Learning via MEC-enabled Blockchain Network.
In 2019 20th Asia-
Paciﬁc Network Operations and Management Symposium (APNOMS). 1–4.
https://doi.org/10.23919/APNOMS.2019.8892848

[15] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and
Blaise Aguera y Arcas. 2017. Communication-Eﬃcient Learning of Deep
Networks from Decentralized Data.
the 20th Interna-
tional Conference on Artiﬁcial Intelligence and Statistics. PMLR, 1273–1282.
https://proceedings.mlr.press/v54/mcmahan17a.html

In Proceedings of

