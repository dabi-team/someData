FAIR-BFL: Flexible and Incentive Redesign
for Blockchain-based Federated Learningâˆ—

2
2
0
2

n
u
J

6
2

]

C
D
.
s
c
[

1
v
9
9
8
2
1
.
6
0
2
2
:
v
i
X
r
a

Rongxin Xu
Hunan Key Laboratory of Data Science & Blockchain,
Business School, Hunan University
Changsha 410082, China
rongxinxu@hnu.edu.cn

Shiva Raj Pokhrelâ€ 
School of IT, Deakin University
Geelong, VIC 3216, Australia
shiva.pokhrel@deakin.edu.au

Qiujun Lan
Hunan Key Laboratory of Data Science & Blockchain,
Business School, Hunan University
Changsha 410082, China
lanqiujun@hnu.edu.cn

Gang Li
Centre for Cyber Security Research and Innovation,
Deakin University
Geelong, VIC 3216, Australia
gang.li@deakin.edu.au

ABSTRACT
Vanilla Federated learning (FL) relies on the centralized global ag-
gregation mechanism and assumes that all clients are honest. This
makes it a challenge for FL to alleviate the single point of failure and
dishonest clients. These impending challenges in the design philos-
ophy of FL call for blockchain-based federated learning (BFL) due
to the beneï¬ts of coupling FL and blockchain (e.g., democracy, in-
centive, and immutability). However, one problem in vanilla BFL is
that its capabilities do not follow adoptersâ€™ needs in a dynamic fash-
ion. Besides, vanilla BFL relies on unveriï¬able clientsâ€™ self-reported
contributions like data size because checking clientsâ€™ raw data is
not allowed in FL for privacy concerns. We design and evaluate
a novel BFL framework, and resolve the identiï¬ed challenges in
vanilla BFL with greater ï¬‚exibility and incentive mechanism called
FAIR-BFL. In contrast to existing works, FAIR-BFL oï¬€ers unprece-
dented ï¬‚exibility via the modular design, allowing adopters to ad-
just its capabilities following business demands in a dynamic fash-
ion. Our design accounts for BFLâ€™s ability to quantify each clientâ€™s
contribution to the global learning process. Such quantiï¬cation
provides a rational metric for distributing the rewards among fed-
erated clients and helps discover malicious participants that may
poison the global model.

KEYWORDS
Federated Learning, Blockchain, Incentive, Security and Privacy

1 INTRODUCTION
The advent of federated learning (FL) [8] has ameliorated the short-
comings of the centralized ML techniques, which were caused by
the ever-increasing data scale and model complexity. FL addresses
the concerns on data ownership and privacy by ensuring that no
raw data leave the distributed end devices (also referred to as clients).
It successfully employs a single global server in a distributed sys-
tem to collect updates from end devices [21]. FL performs the re-
newal aggregation and iteratively distributes new global learning

âˆ—To appear in ICPP â€™22
â€ Corresponding Author

model to the clients. However, such a FL setup based on central-
ized server suï¬€ers from issues such as single point of failure and
instability [22]. Moreover, attacks against distributed training of
FL have revealed that malicious or compromised clients/central
servers may upload modiï¬ed global parameters, causing model poi-
soning, because attackers can forge local updates to launch infer-
ence attacks [16]. Therefore, the design of a robust FL mechanism
is essential to the stability and security of distributed computing
systems.

As a proven decentralized framework, blockchain naturally pon-
ders the beneï¬ts of merging with FL [17], including immutabil-
ity, traceability, and incentive mechanisms, let alone the fact that
both blockchain and FL are inherently distributed by nature. Sev-
eral recent works have been proposed to empower FLâ€™s robustness,
intelligence, and privacy-preserving capabilities by incorporating
blockchain. Blockchain-based Federated Learning (BFL), proposed
in [21], has been considered as one promising and celebrated ap-
proach to facilitating distributed computing and learning. Notable
studies along this line of research include [18â€“21]. In BFL, local up-
dates and global models can be recorded through the blockchain
to ensure security, and clients would automatically acquire new
global parameters through a consensus mechanism. However, BFL
requires ï¬‚exibility because adoptersâ€™ needs are dynamic, e.g., when
business shrinks, adopters may expect to quickly switch from BFL
to degraded versions (FL or blockchain) for the sake of cost reduc-
tion. Moreover, blockchain rewards those nodes that win the min-
ing competition, but in BFL, we desire to attract potential partici-
pants and keep clients who make great contributions to global up-
dates. Therefore, BFL also needs a novel incentive mechanism. Un-
fortunately, existing works have not adequately studied the ï¬‚exi-
bility and incentive mechanism in BFL, which we refer to as vanilla
BFL, thus it is diï¬ƒcult for them to move toward practical use. There
exist three more challenges in moving vanilla BFL towards ï¬‚exibil-
ity and eï¬€ective incentive.

Tightly coupling blockchain and FL FL has a periodic learning-
updating-waiting process while the blockchain keeps run-
ning. In vanilla BFL, these two play almost independently,

 
 
 
 
 
 
ICPP â€™22, August 29-September 1, 2022, Bordeaux, France

Rongxin Xu, Shiva Raj Pokhrel, Qiujun Lan, and Gang Li

thus posing severe concerns. One prevalent concern is un-
wanted consequences such as â€œforking is inevitableâ€ [21]. Ame-
liorating the impact of forking (as studied in [3, 13]) is non-
trivial as it often loses some local updates and adversely
impacts global learning. It becomes intractable when local
updates recorded in the block can not reï¬‚ect the actual FL
stage and generates empty blocks [2]. Therefore, it is crucial
to tightly couple both blockchain and FL in BFL, especially
to coordinate minersâ€™ behavior.

The rest of this paper is organized as follows. We start with
Section 2, which provides background knowledge on BFL, related
work, and challenges faced by vanilla BFL. Then, we propose FAIR-
BFL in Section 3 and show how it overcomes the challenges with
several novel insights. In Section 4, we reveal how FAIR-BFL pro-
vides unprecedented ï¬‚exibility through functional scaling and an-
alyze its performance. After that, we move to experiments in Sec-
tion 5 to demonstrate the performance, latency, and security of
FAIR-BFL. Finally, Section 6 concludes this work.

Deï¬ning blockâ€™s data scope All nodes in the blockchain net-
work have access to the data in the block, so any data which
may reveal the privacy must be avoided from being made
public. As the block size is limited, vanilla BFL may generate
more blocks because it records all the data to complete the
same round of learning. Moreover, each block results from a
round of mining competition, and large blocks can increase
the transfer time. Thus, the delay of vanilla BFL can be high.
So the data recorded in the block should be carefully deï¬ned
to reduce the delay of BFL.

Incentivizing based on contribution Blockchain can provide
incentives for FL, whereas it rewards those miners that suc-
cessfully mine blocks. BFL desires to reward clients who
contribute more to the global aggregation to attract poten-
tial participants, especially in data-intensive tasks. To this
end, a method is needed to help BFL diï¬€erentiate the client
contributions. At the same time, such a method should not
rely on clientsâ€™ self-reported contributions. Otherwise, clients
could have good reasons to cheat, and the BFL cannot ver-
ify who are dishonest since the limitation in checking the
clientâ€™s raw data. Unfortunately, vanilla BFL mainly relies
on the clientâ€™s self-reporting contributions or checking raw
data to determine rewards.

Therefore, ï¬‚exibility and eï¬€ective incentive should be consid-
ered in BFL to fully move towards practical use. It requires enhanc-
ing vanilla BFL with a tightly coupled framework and contribution-
based incentive mechanism to improve performance and security.
The above challenges motivate us to develop new insights in de-
signing the BFL framework. To this end, we propose FAIR-BFL, a
novel BFL framework with ï¬‚exible and incentive redesign, which
mitigates the above-mentioned issues in vanilla BFL, and our main
contributions can be summarized as follows:

(1) We develop new insights in designing blockchain-based fed-
erated learning framework by coordinating minersâ€™ behav-
ior, recording the desirable global gradients, thus, jointly
improving the ï¬‚exibility and enhancing performance-cum-
security.

(2) We propose a contribution-based incentive mechanism that
supports quantifying each clientâ€™s contribution with vari-
ous clustering algorithms, defending against malicious at-
tacks, and selecting high-contributing clients to speed up
model convergence.

(3) With the incentive mechanism in FAIR-BFL, we propose an
aggregation method to assign clientsâ€™ weights based on their
contributions, which improves the performance consider-
ably with guaranteed fairness and convergence.

2 BACKGROUND AND RELATED WORK
Blockchain maintains a distributed ledger to securely record con-
clusive information (called block), in which the nodes compete for
bookkeeping in a mining competition and reach agreement through
a consensus mechanism. The newly generated block is broadcasted
in the network, and those who receive the message will stop their
current computation.

FL employs a distributed learning which allows end devices to
train their own models locally and then aggregates intermediate
information to provide global insights by using local learnings at
a central server. It aims to solve the problem of data island 1 and
beneï¬t from aggregate modelling. Speciï¬cally, at the beginning of
each communication round, the clients update their local models
with their data and upload the obtained gradients to the central
server. After that, the central server computes the global updates
by aggregating all received local gradients and supplies the global
gradients to the clients. Finally, the clients apply the global gradi-
ents to update their local models independently. Thus, FL dynam-
ics evolves from one round to another.

Some notable studies along the lines of BFL include [1, 11, 12,
14, 21]. Among them, Awan et al. [1] designed a variant of the Pail-
lier cryptosystem to support additional homomorphic encryption
and proxy re-encryption, so that the privacy of the transmission
is protected in the BFL. Majeed and Hong [14] adopted the con-
cept of â€œchannelsâ€ in their BFL framework, FLchain, to store the
gradient of the local model in the block of a channel. Components
such as Ethereum, extend the capability of FLchain in executing the
global updates. Lu et al. [12] incorporated federated learning with
diï¬€erential privacy into permissioned blockchain to alleviate the
centralized trust problem. Li et al. [11] applied the blockchain to
store the global model and exchange the local updates, thus elim-
inating the central server and resisting privacy attacks from ma-
licious clients and central server, it also reduces the computation
time using committee-based consensus.

The aforementioned works developed the vanilla BFL frame-
work for better privacy and data security. However, vanilla BFL de-
sign still faces some non-trivial challenges. For example, the asyn-
chronous nature of blockchain requires in-depth integration into

1Imagine data as a ï¬‚owing ocean from which some entities collect data and keep it
locally rather than sharing it, e.g., ï¬nancial institutions. Thus, these stagnant data
become islands one after another.

FAIR-BFL: Flexible and Incentive Redesign for Blockchain-based Federated Learning

ICPP â€™22, August 29-September 1, 2022, Bordeaux, France

the FLâ€™s communication rounds mechanism. In vanilla BFL frame-
works, blockchain and federated learning are more like two dis-
parate parts that are unnecessarily aligned in terms of their work-
ing states, making it problematic to ensure coherent BFL opera-
tions. Moreover, in most existing vanilla BFL frameworks, objec-
tive evaluation of the clientâ€™s contribution has been considered as
irrelevant, while recording all local updates in a few of them [7,
21] arose serious privacy leakage concerns. None of the aforemen-
tioned works focus on the ï¬‚exibility and incentive mechanism of the
BFL by design. In this paper, we propose FAIR-BFL with a modular
design, novel insights, a fairer aggregation scheme and a contribution-
based incentive mechanism, thus jointly enhancing the performance
and the security.

3 FLEXIBLE AND INCENTIVE REDESIGN FOR

BFL

In this section, we propose FAIR-BFL and develop the algorithm in
detail, and then we demonstrate how to integrate blockchain and
FL tightly by utilizing their internal working principles. Table 1
summarizes all the notations used in this paper.

Table 1: Summary of notations

Notations in this work

The client ğ‘– in BFL and FL, or a worker in blockchain.
The miner ğ‘˜ in BFL and blockchain, or a server in FL.

ğ¶ğ‘–
ğ‘†ğ‘˜
D The Data set we used
ğœ†
ğœ‚
ğ¸
ğµ
ğ‘›
ğ‘š The number of miners
B Sub-data sets divided by batch size
ğ‘¤

The ratio of randomly selected clients in each round
The learning rate of the model we use
The number of epochs of the clientâ€™s local model
The batch size of clientâ€™s local model
The number of clients or workers

The gradient in FL or BFL

Identify Contributions

Miner

Reward

Blockchain

Federated 
Learning

Attacker

Figure 1: The framework of FAIR-BFL

Algorithm 1 FAIR-BFL Algorithm
1: Initialization: {ğ¶ğ‘– }ğ‘›
ğ‘–=1, {ğ‘†ğ‘˜ }ğ‘š
2: for each round ğ‘Ÿ = 1, 2, 3... do
3:

{ğ¶ğ‘– }ğœ†ğ‘›
for all ğ¶ğ‘– âˆˆ {ğ¶ğ‘– }ğœ†ğ‘›

ğ‘˜=1, D, ğœ†, ğœ‚, ğ¸, ğµ

ğ‘–=1 â† ğ‘…ğ‘ğ‘›ğ‘‘ğ‘œğ‘šğ‘™ğ‘¦ ğ‘ ğ‘’ğ‘™ğ‘’ğ‘ğ‘¡ ğœ†ğ‘› ğ¶ğ‘– âˆˆ {ğ¶ğ‘– }ğ‘›

ğ‘–=1

ğ‘–=1 do
allocate Dğ‘– âˆ¼D to ğ¶ğ‘–
procedure Local model update(ğ¶ğ‘–, Dğ‘– , ğµ, ğ¸)

read global gradient ğ‘¤ğ‘Ÿ from the latest block
B â† ğ‘ ğ‘ğ‘™ğ‘–ğ‘¡ Dğ‘– ğ‘–ğ‘›ğ‘¡ğ‘œ ğ‘ğ‘ğ‘¡ğ‘â„ğ‘’ğ‘  ğ‘œ ğ‘“ ğ‘ ğ‘–ğ‘§ğ‘’ ğµ
for ğ‘’ğ‘ğ‘â„ ğ‘’ğ‘ğ‘œğ‘â„ ğ‘– ğ‘“ ğ‘Ÿğ‘œğ‘š 1 ğ‘¡ğ‘œ ğ¸ do
for ğ‘’ğ‘ğ‘â„ ğ‘ğ‘ğ‘¡ğ‘â„ ğ‘ âˆˆ B do
ğ‘Ÿ âˆ’ ğœ‚âˆ‡â„“(ğ‘¤ğ‘–

ğ‘¤ğ‘–
ğ‘Ÿ +1 â† ğ‘¤ğ‘–

ğ‘Ÿ ; ğ‘)

procedure Upload local gradients(ğ¶ğ‘–, ğ‘¤ğ‘–

ğ‘Ÿ +1, ğ‘†ğ‘˜ )

randomly associate ğ¶ğ‘– to ğ‘†ğ‘˜
upload updated gradient ğ‘¤ğ‘–

ğ‘Ÿ +1 to ğ‘†ğ‘˜

for all ğ‘†ğ‘˜ âˆˆ {ğ‘†ğ‘˜ }ğ‘š

do
procedure Exchange gradients({ğ‘¤ğ‘–

ğ‘˜=1

ğ‘Ÿ +1}, ğ‘†ğ‘˜ )

ğ‘Ÿ +1, ğ‘– = ğ‘–ğ‘›ğ‘‘ğ‘’ğ‘¥ ğ‘œ ğ‘“ ğ‘ğ‘ ğ‘ ğ‘œğ‘ğ‘–ğ‘ğ‘¡ğ‘’ ğ‘ğ‘™ğ‘–ğ‘’ğ‘›ğ‘¡ğ‘  }

ğ‘Š ğ‘˜
ğ‘Ÿ +1 â† {ğ‘¤ğ‘–
broadcast clients updated gradient ğ‘Š ğ‘˜
received updated gradient ğ‘Š ğ‘£
for ğ‘¤ âˆˆ ğ‘Š ğ‘£
ğ‘Ÿ +1 do

ğ‘Ÿ +1
ğ‘Ÿ +1 form ğ‘†ğ‘£

if ğ‘¤ /âˆˆ ğ‘Š ğ‘˜
ğ‘Š ğ‘˜

ğ‘Ÿ +1 then
ğ‘Ÿ +1 append ğ‘¤

procedure Computing Global Updates(ğ‘Š ğ‘˜
ğ‘¤ğ‘–
ğ‘Ÿ +1, ğ‘¤ğ‘–

ğ‘¤ğ‘Ÿ +1 â† 1
ğ‘›

ğ‘Ÿ +1 âˆˆ ğ‘Š ğ‘˜

ğ‘Ÿ +1

ğ‘›

ğ‘Ÿ +1, ğ‘†ğ‘˜ )
âŠ² Simple

Average

ğ‘–=1
P

ğ‘Ÿ +1 append ğ‘¤ğ‘Ÿ +1

ğ‘Š ğ‘˜
ğ¶ğ‘œğ‘›ğ‘¡ğ‘Ÿğ‘–ğ‘ğ‘¢ğ‘¡ğ‘–ğ‘œğ‘›-ğ‘ğ‘ğ‘ ğ‘’ğ‘‘ ğ¼ğ‘›ğ‘ğ‘’ğ‘›ğ‘¡ğ‘–ğ‘£ğ‘’ ğ‘€ğ‘’ğ‘â„ğ‘ğ‘›ğ‘–ğ‘ ğ‘š(ğ‘Š ğ‘˜
ğ¹ğ‘ğ‘–ğ‘Ÿ ğ´ğ‘”ğ‘”ğ‘Ÿğ‘’ğ‘”ğ‘ğ‘¡ğ‘–ğ‘œğ‘›(ğ‘Š ğ‘˜
ğ‘Ÿ +1)
procedure Block Mining and Consensus(ğ‘¤ğ‘Ÿ +1, ğ‘†ğ‘˜ )

ğ‘Ÿ +1)
âŠ² By Equation (1)

do ğ‘ğ‘Ÿğ‘œğ‘œ ğ‘“ ğ‘œ ğ‘“ ğ‘¤ğ‘œğ‘Ÿğ‘˜
if hash satisï¬es target then
ğ‘‡ğ‘Ÿğ‘ğ‘›ğ‘ . â† ğ‘Ÿğ‘’ğ‘¤ğ‘ğ‘Ÿğ‘‘ ğ‘™ğ‘–ğ‘ ğ‘¡
generate and add ğ‘ğ‘™ğ‘œğ‘ğ‘˜(ğ‘‡ğ‘Ÿğ‘ğ‘›ğ‘ ., ğ‘¤ğ‘Ÿ +1)
broadcast

if received ğ‘ğ‘™ğ‘œğ‘ğ‘˜ğ‘– then

verify ğ‘ğ‘Ÿğ‘œğ‘œ ğ‘“ ğ‘œ ğ‘“ ğ‘¤ğ‘œğ‘Ÿğ‘˜
if hash satisï¬es target then

stop current ğ‘ğ‘Ÿğ‘œğ‘œ ğ‘“ ğ‘œ ğ‘“ ğ‘¤ğ‘œğ‘Ÿğ‘˜
blockchain add ğ‘ğ‘™ğ‘œğ‘ğ‘˜ğ‘–

4:

5:

6:

7:

8:

9:

10:

11:

12:

13:

14:

15:

16:

17:

18:

19:

20:

21:

22:

23:

24:

25:

26:

27:

28:

29:

30:

31:

32:

33:

34:

35:

36:

37:

38:

A high-level view of FAIR-BFL framework is shown in Figure 1,
in which the circled number indicates the corresponding proce-
dure deï¬ned and explained in Algorithm 1 and Section 4. We sum-
marize the entire process of BFL into ï¬ve procedures that interact
among diï¬€erent entities: i) the client reads the global parameters
from the latest block and updates its local model; ii) the client con-
nects to a miner and uploads its local gradient, please note that
some clients may be malicious; iii) miners exchange gradient sets

ICPP â€™22, August 29-September 1, 2022, Bordeaux, France

Rongxin Xu, Shiva Raj Pokhrel, Qiujun Lan, and Gang Li

and start the mining competition; iv) the winner identiï¬es the con-
tributions and computes the global updates, and this will help dis-
regard the information forged by the attackers due to low contribu-
tion; v) the winner packs the global update and reward information
into a new block, and then all miners reach an agreement through
the consensus mechanism. More speciï¬cally, the whole process is a
holistic approach with multiple rounds of communication among
ğ‘› clients {ğ¶ğ‘– }ğ‘›
ğ‘–=1 with a set of ğ‘š miners {ğ‘†ğ‘˜ }ğ‘š
ğ‘˜=1 to handle the
blockchain process. We then utilize data set D, by assuming that
the sub dataset Dğ‘– is at the client ğ¶ğ‘– before the start of each com-
munication round.

3.1 Coupled BFL design
Vanilla BFL design faces some challenges in ï¬‚exibility and privacy [21].
On the one hand, the workï¬‚ows of FL and blockchain are incon-
sistent in the vanilla BFL. Hence, miners will continue to compete
for excavation without stopping, which undoubtedly increases re-
source consumption. For example, if a miner does not receive any
gradient update but completes the hash puzzle ahead of other min-
ers, it will generate an empty block, which does not beneï¬t the
FL part. However, [4] have shown that performing SGD based on
communication rounds in FL is better than asynchronous meth-
ods. To this end, we bring it into BFL to achieve the tight coupling
between blockchain and FL, also alleviate issues such as forking,
empty blocks, and resource cost. Note that Assumption 1 has been
made in [7], but only to simplify the problems for analysis. We
have the following Assumption 1.

Assumption 1 (Tight coupling). Clients and miners are fully

synchronized in every communication rounds.

On the other hand, vanilla BFL records every local gradient in
the blockchain, and workers read the blockâ€™s information to cal-
culate the global updates themselves. In this case, vanilla BFL is
a white-box for the attacker, malicious nodes can use this infor-
mation to perform privacy attacks and easily track the changes
in a workerâ€™s local gradient to launch more severe model inver-
sion attacks [6]. Furthermore, imagining the BFL applications in
large-scale scenarios, where thousands of local gradients could be
waiting for miners to pack. However, the block size is limited due
to the communication cost and delay, and many local gradients
will miss the current block. Eventually, to calculate the global gra-
dient, workers have to wait for a new block to be generated until
all local gradients have been recorded in the blockchain, which un-
doubtedly increases the latency and the communication costs. To
address the concerns we have the following Assumption 2.

Assumption 2 (Bounding blockâ€™s data scope). Miners pack
only the global gradients into blocks. In the end, each block contains
only the global gradient of a speciï¬c round.

Observe that Assumption 2 is to bound the blockâ€™s data scope,
also protects the security of FAIR-BFL and alleviates the transaction
queuing caused by the limitation of block size in the asynchronous
design. To the best of our knowledge, this is the ï¬rst attempt to use
Assumption 2 in the BFL design and validate its capability.

Algorithm 2 Clientâ€™s Contribution Identiï¬cation Algorithm

Input: ğ‘Š ğ‘˜

ğ‘Ÿ +1, ğ‘šğ‘œğ‘‘ğ‘’ğ‘™ ğ‘›ğ‘ğ‘šğ‘’, ğ‘†ğ‘¡ğ‘Ÿğ‘ğ‘¡ğ‘’ğ‘”ğ‘¦
1: ğºğ‘Ÿğ‘œğ‘¢ğ‘ ğ¿ğ‘–ğ‘ ğ‘¡ â† ğ¶ğ‘™ğ‘¢ğ‘ ğ‘¡ğ‘’ğ‘Ÿğ‘–ğ‘›ğ‘”(ğ‘šğ‘œğ‘‘ğ‘’ğ‘™ ğ‘›ğ‘ğ‘šğ‘’,ğ‘Š ğ‘˜
2: for ğ‘™ğ‘– âˆˆ ğºğ‘Ÿğ‘œğ‘¢ğ‘ ğ¿ğ‘–ğ‘ ğ‘¡ do
if ğ‘¤ğ‘Ÿ +1 âˆˆ ğ‘™ğ‘– then
3:

ğ‘Ÿ +1)

for ğ‘¤ğ‘–

ğ‘Ÿ +1 âˆˆ ğ‘™ğ‘– do

ğœƒğ‘– â† ğ¶ğ‘œğ‘ ğ‘–ğ‘›ğ‘’ ğ·ğ‘–ğ‘ ğ‘¡ğ‘ğ‘›ğ‘ğ‘’(ğ‘¤ğ‘–
Label ğ¶ğ‘– as high contribution

ğ‘Ÿ +1, ğ‘¤ğ‘Ÿ +1)

Append hğ¶ğ‘–, ğœƒğ‘–/

ğœƒğ‘˜ âˆ— ğ‘ğ‘ğ‘ ğ‘’i to ğ‘Ÿğ‘’ğ‘¤ğ‘ğ‘Ÿğ‘‘ ğ‘™ğ‘–ğ‘ ğ‘¡

ğœ†ğ‘›

ğ‘˜=1
P

if ğ‘¤ğ‘Ÿ +1 /âˆˆ ğ‘™ğ‘– then
for all ğ‘¤ğ‘–

ğ‘Ÿ +1 âˆˆ ğ‘™ğ‘– do

Label ğ¶ğ‘– as low contribution

10:
11: ğ‘Š ğ‘˜
Output: ğ‘Ÿğ‘’ğ‘¤ğ‘ğ‘Ÿğ‘‘ ğ‘™ğ‘–ğ‘ ğ‘¡, ğ‘Š ğ‘˜

ğ‘Ÿ +1 â† ğ‘†ğ‘¡ğ‘Ÿğ‘ğ‘¡ğ‘’ğ‘”ğ‘¦(ğ‘Ÿğ‘’ğ‘¤ğ‘ğ‘Ÿğ‘‘ ğ‘™ğ‘–ğ‘ ğ‘¡,ğ‘Š ğ‘˜

ğ‘Ÿ +1)

ğ‘Ÿ +1

4:

5:

6:

7:

8:

9:

3.2 Accounting Clientâ€™s Contribution
Algorithm 2 implements our method to identify contributions in
Line 26 of Algorithm 1. Various clusters of gradients are found by
applying a clustering algorithm on ğ‘Š ğ‘˜
ğ‘Ÿ +1; moreover, they imply
diï¬€erent contributions. Note that any suitable clustering algorithm
can be used here as needed, However, we use DBSCAN in experi-
ments by default because it is eï¬ƒcient and straightforward. Those
clients belonging to the same cluster as the global gradient can be
considered a high contribution and be rewarded, while those far
from the global gradient can be considered a low contribution and
adopt a predetermined strategy. There are two strategies: i) keep
all gradients; ii) discard low-contributing local gradients and recal-
culate the global updates ğ‘¤ğ‘Ÿ +1. The cosine distance ğœƒğ‘– (the larger
the ğœƒ, the farther the distance.) between its local gradient and the
global update is calculated as the weight of its contribution to the
global update for a high contributing client ğ¶ğ‘–. We can set a ğ‘ğ‘ğ‘ ğ‘’

and multiply it by ğœƒğ‘–/

ğœ†ğ‘›

value pairs hğ¶ğ‘–, ğœƒğ‘– /

ğœƒğ‘˜ as the ï¬nal reward for client ğ¶ğ‘–. Key-

ğ‘˜=1
P
ğœƒğ‘˜ âˆ—ğ‘ğ‘ğ‘ ğ‘’i represent the reward information,

and they are recorded in the ğ‘Ÿğ‘’ğ‘¤ğ‘ğ‘Ÿğ‘‘ ğ‘™ğ‘–ğ‘ ğ‘¡. Eventually, when a miner
generates a new block, the reward is distributed according to the
reward list and appended to the current block as transactions. Af-
ter the blockchain consensus is achieved, clients will get these re-
wards. We explain the intuition behind Algorithm 2 as follows.

ğœ†ğ‘›

ğ‘˜=1
P

Privacy preservation Vanilla BFL requires clients to report
their data dimensions for rewards determination. Therefore,
clients have suï¬ƒcient motivation to cheat for more rewards.
We cannot recognize this deception because it is impossible
to check the actual data set, which violates FLâ€™s guidelines.
On the contrary, as the intermediate information, the gradi-
ents can reï¬‚ect both the data size and the data quality. Using
them to perform Algorithm 2 can provide a more objective
assessment and ensure privacy.

FAIR-BFL: Flexible and Incentive Redesign for Blockchain-based Federated Learning

ICPP â€™22, August 29-September 1, 2022, Bordeaux, France

Malicious attack resistance Malicious clients may upload fake
local gradients to attack the global model. The clustering al-
gorithm can ï¬nd these fake gradients because they are dif-
ferent from the real ones [16]. We can employ the discard-
ing strategy to avoid skewing the global model with these
spurious gradients, ultimately maintaining the security of
FAIR-BFL.

Clients selection If we adopt the discarding strategy, at the
same time, the corresponding workers will no longer partici-
pate before the round. In this respect, this approach can also
be considered as a new method of clients selection, rather
than simply random selection.

We will thoroughly evaluate our contribution-based incentive

approach in Section 5.

3.3 Fair Aggregation
The optimization problem considered by FAIR-BFL is

ğ¹ (w) ,

min
ğ‘¤ (

ğ‘›

ğ‘–=1
X

ğ‘ğ‘–ğ¹ğ‘– (w)

,

)

where ğ¹ğ‘–(w), ğ‘ğ‘– are the local objective function and weight of clients
ğ‘–, respectively. Consider the simple average aggregation, which
means ğ‘1 = ğ‘2 = ... = ğ‘ğ‘– = 1
ğ‘› :

ğ‘¤ğ‘Ÿ +1 â†

1
ğ‘›

ğ‘¤ğ‘–

ğ‘Ÿ +1

ğ‘›

ğ‘–=1
X

This is simple average aggregation that treats all clientsâ€™ gradi-
ents equally and averages them. However, clients may not have
same sample sizes. Thus, simple averaging does not reï¬‚ect such a
contribution diï¬€erence. Instead, we use the following method to
aggregate the global gradients for fairness.

ğ‘¤ğ‘Ÿ +1 â†

1
ğœ†

ğ‘›

ğ‘–=1
X

ğ‘ğ‘–ğ‘¤ğ‘–

ğ‘Ÿ +1, where ğ‘ğ‘– = ğœƒğ‘–/

ğœ†ğ‘›

ğœƒğ‘˜

(1)

ğ‘˜=1
X

That is, we assign aggregation weights based on the contribu-
tion of clients to avoid model skew and improve accuracy. At the
same time, it is impractical to require all devices to participate in
the learning process [5, 24], so we assume that all devices are ac-
tivated before the communication round begins. However, only
some devices are selected to upload local gradients.

Although we use fairness aggregation and partial participation,
we can still reveal the stability and convergence dynamics of FAIR-
BFL. For tractability, we have used the following four well-known
assumptions in literature [9, 10, 23, 26].

Assumption 3 (L-smooth). Consider ğ¹ğ‘–(ğ‘¤) , 1
ğ‘›

and ğ¹ğ‘– is L-smooth, then for all v and w,

â„“ (ğ‘¤; ğ‘ğ‘– )

ğ‘›

ğ‘–=1
P

ğ¹ğ‘– (v) â‰¤ ğ¹ğ‘– (w) + (v âˆ’ w)ğ‘‡ âˆ‡ğ¹ğ‘–(w) +

ğ¿
2

kv âˆ’ wk2
2.

Assumption 4 (Âµ-strongly). ğ¹ğ‘– is u-strongly convex, for all v

and w,

ğ¹ğ‘–(v) â‰¥ ğ¹ğ‘–(w) + (v âˆ’ w)ğ‘‡ âˆ‡ğ¹ğ‘–(w) +

ğœ‡
2

kv âˆ’ wk2
2.

Assumption 5 (bounded variance). The variance of stochastic

gradients in each client is bounded by:

E

âˆ‡ğ¹ğ‘–

wğ‘–

ğ‘Ÿ , ğ‘ğ‘–

âˆ’ âˆ‡ğ¹ğ‘–

wğ‘–
ğ‘Ÿ

2

â‰¤ ğœ2
ğ‘–

(cid:16)

(cid:17)

(cid:16)

Assumption 6 (bounded stochastic gradient). The expected
squared norm of stochastic gradients is uniformly bounded, thus for
all ğ‘– = 1, Â· Â· Â· , ğ‘› and ğ‘Ÿ = 1, Â· Â· Â· , ğ‘Ÿ âˆ’ 1, we have

(cid:17)(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)

E

âˆ‡ğ¹ğ‘–

wğ‘–

ğ‘¡ , ğ‘ğ‘–

2

â‰¤ ğº 2

(cid:16)

(cid:13)
(cid:13)
(cid:13)

(cid:17)(cid:13)
Assumptions 3 and 4 are essential for analyzing the convergence [9,
(cid:13)
(cid:13)

23, 26]. Both Assumptions 3 and 4 mandate requirements on the
fundamental properties of the loss function. That is, the function
does not change too fast (Assumption 1) or too slow (Assump-
tion 2). Assumptions 5 and 6 are made in [10]. They enable us to
use ğ‘¤ âˆ’ ğ‘¤ âˆ— to approximate ğ¹ âˆ’ ğ¹ âˆ—.

Theorem 3.1. Given Assumptions 1 to 6 hold, Algorithm 1 con-

verges as follows

E [ğ¹ (wğ‘Ÿ )] âˆ’ ğ¹ âˆ— â‰¤

ğœ…
ğ›¾ + ğ‘Ÿ

(cid:18)

2(ğµ + ğ¶)
ğœ‡

+

ğœ‡(ğ›¾ + 1)
2

w1 âˆ’ wâˆ—

2

(cid:13)
(cid:13)

(cid:19)(2)
(cid:13)
(cid:13)
ğœ‡(ğ›¾+ğ‘Ÿ ) , and

ğœ‡ , ğ›¾ = max{8ğœ…, ğ¸}, the learning rate ğœ‚ğ‘Ÿ = 2

where ğœ… = ğ¿
ğ¶ = 4
ğ¾ ğ¸2ğº 2.

Equation (2) shows that the distance between the actual model
ğ¹ and the optimal model ğ¹ âˆ— decreases with increasing communica-
tion rounds. FAIR-BFL can converge regardless of the data distribu-
tion because we did not make an IID assumption, and it establishes
the condition that guarantees the convergence of Algorithm 1. The
detailed proof of Theorem 3.1 is provided in Appendix A, which is
further supported by the experimental results in Section 5.

4 FLEXIBILITY BY DESIGN
We re-examined the entire process of vanilla BFL and identiï¬ed the
opportunity to achieve ï¬‚exibility. More speciï¬cally, apart from the
necessary work in the preparation phase, we divide the remaining
part into ï¬ve procedures, as shown in Algorithm 1. Depending on
the applicationâ€™s needs, these ï¬ve procedures can be coupled ï¬‚ex-
ibly and dynamically. We present these procedures in detail and
reveal this ï¬‚exibility, and we further determine the possible delays
in each link to model approximate performance.

4.1 Local Learning and Update
At the beginning of round ğ‘Ÿ + 1, each client reads the global gra-
dient ğ‘¤ğ‘Ÿ (if any exists) from the last block in the blockchain, and
updates the local model with ğ‘¤ğ‘Ÿ . Next, the allocated sub-data sets
Dğ‘– will be divided according to the speciï¬ed batch size ğµ. For each
epoch ğ‘– âˆˆ {1, 2, 3, . . . , ğ¸}, the client ğ¶ğ‘– obtains the gradient ğ‘¤ğ‘–
ğ‘Ÿ +1
of round ğ‘Ÿ + 1 by performing the SGD, as shown in Equation (3),
where â„“ and ğœ‚ are the loss function and the learning rate, respec-
tively.

ğ‘Ÿ +1 â† ğ‘¤ğ‘–
ğ‘¤ğ‘–

ğ‘Ÿ âˆ’ ğœ‚âˆ‡â„“(ğ‘¤ğ‘–

ğ‘Ÿ ; ğ‘)

(3)

Equation (3) can be calculated Dğ‘–

ğµ times with the speciï¬ed batch
size ğµ. Therefore, the time complexity of eq. (3) is O(ğ¸ âˆ— Dğ‘–
ğµ ). Fur-
thermore, we deï¬ne the calculation time of this step as the delay

ICPP â€™22, August 29-September 1, 2022, Bordeaux, France

Rongxin Xu, Shiva Raj Pokhrel, Qiujun Lan, and Gang Li

Tğ‘™ğ‘œğ‘ğ‘ğ‘™ . However, please note that ğ¸ and ğµ are set as small constants
for all clients under normal circumstances, so the time complexity
of eq. (3) is linear O(ğ‘›). The learning rate ğœ‚ is often a more con-
cerning issue, as it will signiï¬cantly aï¬€ect the performance and the
convergence rate. We will explore the impact of the learning rate
on FAIR-BFL in section 5.2.2.

4.2 Uploading the gradient for mining
After the Procedure-I, the client ğ¶ğ‘– will get the updated gradient
ğ‘¤ğ‘–
ğ‘Ÿ +1 of round ğ‘Ÿ +1 and upload it to the miners. There are multiple
miners in the network, and they will pack gradients into blocks. In
addition, BFL miners also need to play a role similar to the central
server. The client does not have to contact all the miners, which
will increase the communication cost, and a better choice is that
each client only uploads gradients to one miner. Here, we make the
probability of selecting each miner as uniform as possible, which
is determined based on the speciï¬c application scenario. Speciï¬-
cally, the client ğ¶ğ‘– generates the minerâ€™s index ğ‘˜ uniformly and
randomly, then it associates the miner ğ‘†ğ‘˜ and uploads the updated
gradient ğ‘¤ğ‘–

ğ‘Ÿ +1.

Note that it is highly risky to directly use these local gradients
for subsequent global updates without verifying, because malicious
clients can easily forge the information and launch the gradient at-
tacks [16]. To avoid this risk, we use the RSA encryption algorithm
to ensure that the identities of both parties are veriï¬ed. In the be-
ginning, each client is assigned a unique private key according to
its ID, and the corresponding public key will be held by the miners.
The gradient information received by the miner is signed with the
private key, so the information can be veriï¬ed by the public key, as
shown in Figure 2. Further, local gradients can be encrypted using
RSA to ensure data privacy.

Transaction

Signature

Transaction

Clinet

Private key

Public key

Miner

Figure 2: Miners verify transactions through RSA

The procedure will parallelly perform above steps for all clients
in the current round, and the time complexity is O(1). However,
whether it is selecting miners or uploading gradients, the opera-
tion itself will be simple. Nevertheless, the clients are often at the
edge of the network, and the quality of the channel is diï¬ƒcult to
guarantee. It may also be subject to other external disturbances,
where more signiï¬cant delays are possible. For the above consid-
erations, we regard the communication time as the main delay in
this link and record it as Tğ‘¢ğ‘ .

4.3 Exchanging Gradients
A miner ğ‘†ğ‘˜ , will get the updated gradient set {ğ‘¤ğ‘–
ğ‘Ÿ +1} from the asso-
ciated client set {ğ¶ğ‘– }, where ğ‘– is the index of the clients associated
with ğ‘†ğ‘˜ . In the meantime, each miner will broadcast its own gradi-
ent set. Note that we cancel the queuing here by Assumption 1. The

miner will check whether the received transaction exists in the cur-
rent gradient set {ğ‘¤ğ‘–
ğ‘Ÿ +1}, and if not, it will append this transaction.
In the end, all miners have the same gradient set.

Miners will also use the RSA encryption algorithm to validate
the transactions from other miners to ensure that the data has not
been tampered with, as described in Figure 2. The above steps are
parallel. For each miner, it does only three things: i) broadcasts the
gradient set owned. ii) receives the gradient sets from other miners.
iii) adds the local gradients which it does not own. That means, the
time complexity of the current procedure is O(ğ‘š), and we denote
the time required from the start to the moment when all miners
have the same gradient set as Tğ‘’ğ‘¥ . Normally, the number of miners
will be scarce, so it is easy to ensure good communications among
them, which is also the need of the practical application. Under
such circumstances, Tğ‘’ğ‘¥ is insigniï¬cant.

4.4 Computing Global Updates
So far, every miner will have all the local updates in this round. In
order to obtain the global gradient ğ‘¤ğ‘Ÿ +1, they only need to per-
form fairness aggregation by Equation (1). So that the clients can
initialize the model parameters in the ğ‘Ÿ + 1 round.

After that, to evaluate each clientâ€™s contribution in this round,
the global gradient ğ‘¤ğ‘Ÿ +1 is appended to the current local update
set ğ‘Š ğ‘˜
ğ‘Ÿ +1 to identify client
contributions and issue rewards.

ğ‘Ÿ +1, then we perform Algorithm 2 on ğ‘Š ğ‘˜

The current procedure only needs to compute the global gradi-
ent using Equation (1) and then perform Algorithm 2, so the time
complexity depends on the clustering algorithm, represented as
O(ğ‘ğ‘™ğ‘¢ğ‘ ğ‘¡ğ‘’ğ‘Ÿğ‘–ğ‘›ğ‘”). Also, we denote the time cost as the delay Tğ‘”ğ‘™ .

4.5 Block Mining and Consensus
Once the global gradient calculation is completed, all miners will
immediately enter the mining competition. Speciï¬cally, the miner
will continuously change the nonce in the block header, and then
calculate whether the blockâ€™s hash meets the ğ‘‡ ğ‘ğ‘Ÿğ‘”ğ‘’ğ‘¡ by SHA256.
The whole process can be explained as Equation (4), where ğ‘‡ ğ‘ğ‘Ÿğ‘”ğ‘’ğ‘¡1
is a large constant, representing the maximum mining diï¬ƒculty.
Note that ğ‘‡ ğ‘ğ‘Ÿğ‘”ğ‘’ğ‘¡ is the same for all miners, and mining diï¬ƒculty
will be speciï¬ed before the algorithm starts. Therefore, the proba-
bility that a miner obtains the right to generate blocks will depend
on the speed of the hash calculation.

ğ» ( ğ‘›ğ‘œğ‘›ğ‘ğ‘’ + ğµğ‘™ğ‘œğ‘ğ‘˜) < ğ‘‡ ğ‘ğ‘Ÿğ‘”ğ‘’ğ‘¡ =

ğ‘‡ ğ‘ğ‘Ÿğ‘”ğ‘’ğ‘¡1
ğ‘‘ğ‘– ğ‘“ ğ‘“ ğ‘–ğ‘ğ‘¢ğ‘™ğ‘¡ğ‘¦

(4)

If a miner gets the solution of Equation (4) ahead of other min-
ers, it will immediately pack the global gradient ğ‘¤ğ‘Ÿ +1 and reward
information into a new block, and then broadcast this new block.
After receiving this new block, other miners will immediately stop
the current hash calculation, and append the new block to their
blockchain copies, once the validity of the new block is veriï¬ed.
Then, it will enter the next communication round. Again, please
recall Assumption 2, with this setting, at the end of a communica-
tion round, the blockchain will only generate one block, and the
blockchain copies of all miners will be the same, which means that
we avoid the blockchain forking, thus there is no need to resolve
ledger conï¬‚icts while reducing the risk of gradient abandonment

FAIR-BFL: Flexible and Incentive Redesign for Blockchain-based Federated Learning

ICPP â€™22, August 29-September 1, 2022, Bordeaux, France

and consensus delay. According to Equation (4), the hash value
will be calculated several times until ğ‘‡ ğ‘ğ‘Ÿğ‘”ğ‘’ğ‘¡ is met, and the time
of the hash calculation is related to the length of the string. Based
on above discussion, the time complexity is O(ğ‘›). We record the
time cost here as Tğ‘ğ‘™ , which can be more signiï¬cant compared with
others.

Procedure I. 
Local Learning and Update

Procedure III. 
Exchanging Gradients 

Procedure II. 
Uploading gradient for mining 

Procedure IV. 
Computing Global Updates 

Procedure V. 
Block Mining and Consensus 

Blockchain

Federated Learning

Figure 3: The Coupling structure and complexity of FAIR-
BFL

4.6 Approximate Performance of FAIR-BFL
As shown Figure 3, the aforementioned ï¬ve procedures are tightly
coupled to form FAIR-BFL.

Flexibility Guarantee If we remove the Procedure-I and

Procedure-IV, then FAIR-BFL boils down to a pure blockchain
algorithm (see dashed purple rectangle, Figure 3). On the
contrary, if we remove Procedure-III and Procedure-V,
it will be equivalent to the pure FL algorithm (see dashed
orange rectangle, Figure 3). This scale back functionality by
design enables us to easily compare the performance and
delay of those three approaches under the same setup using
the same data set for their comparison. Moreover, we de-
velop analytic model to quantify this ï¬‚exibility and analyse
the delay of the system Figure 3.

where the above discussion has determined that the delay
here is Tğ‘ğ‘™ . Therefore, the overall complexity of FAIR-BFL is
close to O(ğ‘›), while with ğ‘› workers and ğ‘š miners, the over-
all delay is ğ‘‡(ğ‘›,ğ‘š) = Tğ‘™ğ‘œğ‘ğ‘ğ‘™ + Tğ‘¢ğ‘ + Tğ‘’ğ‘¥ + Tğ‘”ğ‘™ + Tğ‘ğ‘™ , which
is compatible with vanilla BFL, so it can quickly learn from
[21] to optimize the block arrival rate to obtain the best de-
lay.

5 EVALUATION AND DISCUSSION
In this section, we conducted a series of experiments to compre-
hensively evaluate the performance of FAIR-BFL on real data set,
Then we reported the changes in performance and delay under
various conditions by adjusting parameters. At last, some novel in-
sights such as the trade-oï¬€ between performance and latency are
presented.

5.1 Experimental setup
Our baseline methods for comparison include the Blockchain, Fe-
dAvg [15], and the state-of-the-art FL algorithm FedProx [9]. Then,
we compare the performance of FAIR-BFL and baselines on the
benchmark data set MNIST. The metrics for comparison are the
average delay and the average performance. We calculate the av-
ğ‘ğ‘ğ‘ğ‘–/ğ‘›,

ğ‘‘ğ‘–/ğ‘Ÿ , and the average accuracy by

erage delay by

ğ‘›

ğ‘Ÿ

where ğ‘‘ğ‘– represents the delay of the communication round ğ‘–, ğ‘ğ‘ğ‘ğ‘–
is the veriï¬cation accuracy of client ğ¶ğ‘– in a communication round.
By default, we assign data to clients following the non-IID dy-
namics, and we set ğ‘› = 100 and ğ‘š = 2, ğœ‚ = 0.01, ğ¸ = 5, and
ğµ = 10, respectively.

ğ‘–=1
P

ğ‘–=1
P

5.2 Performance Impact
For all experiments, We consider the model as converged when the
accuracy in change is within 0.5% for 5 consecutive communica-
tion rounds, and perform 100 communication rounds by default.

5.2.1 General analysis of latency and performance. Figure 4 shows
the simulation results of the general delay and performance. As
shown in Figure 4a, the average delay of FAIR-BFL is between blockchain
and FedAvg, rather than above the blockchain. This implies that As-
sumptions 1 and 2 can eï¬€ectively reduce the BFL delay. In addition,
from Figure 4b, FAIR-BFL has almost the same model performance
as the FedAvg. FedProx has a lower accuracy than FAIR-BFL, and
the accuracy still ï¬‚uctuates after the model converges, which is
because it uses the inexact solution to speed up the convergence.

Approximate Performance Analysis The interactions in Fig-
ure 3 are explained as follows. Procedure-I receives the
initial parameters and data set, executes in parallel on each
client, and after a delay Tğ‘™ğ‘œğ‘ğ‘ğ‘™ it eventually returns the local
ğ‘Ÿ +1 for a particular client. Procedure-II runs on
gradient ğ‘¤ğ‘–
each miner, receives ğ‘¤ğ‘–
ğ‘Ÿ +1 from the associated client, and
spends Tğ‘¢ğ‘ time to return the gradient set {ğ‘¤ğ‘–
ğ‘Ÿ +1}. After
that, Procedure-III receives the gradient set {ğ‘¤ğ‘–
ğ‘Ÿ +1} of all
miners and then waits for Tğ‘’ğ‘¥ time to get the complete local
ğ‘Ÿ +1. Procedure-IV uses ğ‘Š ğ‘˜
gradient set ğ‘Š ğ‘˜
ğ‘Ÿ +1 to calculate
the global update ğ‘¤ğ‘Ÿ +1 of this round, and the time Tğ‘”ğ‘™ con-
sumed depends on the clustering algorithm used. Procedure-V
packs the global gradient ğ‘¤ğ‘Ÿ +1 and generates ğµğ‘™ğ‘œğ‘ğ‘˜(ğ‘‡ğ‘Ÿğ‘ğ‘›ğ‘ ., ğ‘¤ğ‘Ÿ +1),

16

14

12

10

8

6

)
s
(
y
a
l
e
d
e
g
a
r
e
v
A

FAIR

Blockchain
FedAvg

1.0

0.8

0.6

0.4

0.2

y
c
a
r
u
c
c
a

e
g
a
r
e
v
A

FAIR

FedAvg
FedProx

0

20

40

60

80

100

Communication rounds

0

200

400
Time in Seconds

600

800

(a) Delay comparison

(b) Accuracy comparison

Figure 4: General comparison of FAIR-BFL and baselines

ICPP â€™22, August 29-September 1, 2022, Bordeaux, France

Rongxin Xu, Shiva Raj Pokhrel, Qiujun Lan, and Gang Li

5.2.2
Impact of the Learning rate. We conducted multiple experi-
ments, where ğœ‚ âˆˆ [0.01, 0.05, 0.10, 0.15, 0.20]. The result is shown
in Figure 5. From Figure 5a, we can see that the eï¬€ect of the learn-
ing rate on the average delay for FAIR-BFL and FedAvg is negligible,
and we attribute it to the distributed (parallel) learning method. It
is interesting to note that FedProx peaks at the beginning, which
implies that it may need a larger ğœ‚. Although there is no obvious
impact on the delay, the accuracy is very diï¬€erent, as shown in Fig-
ure 5b. For FAIR-BFL and FedAvg, there is an optimal ğœ‚ such that
the average accuracy is the highest. For FedProx, the learning rate
does not signiï¬cantly aï¬€ect the average accuracy. To this end, we
have the following insights.

Insight 1: Due to the beneï¬ts of distributed learning, we set the
best learning rate in BFL to ensure the model performance. The
delay overhead for this is acceptable.

s
d
n
o
c
e
s
n
i

y
a
l
e
D

12

10

8

6

4

2

FAIR

FedAvg

0.95

0.90

0.85

0.80

0.75

y
c
a
r
u
c
c
a

e
g
a
r
e
v
A

FAIR

FedAvg
FedProx

0.05

0.10
Learning rate

0.15

0.20

0.05

0.10
Learning rate

0.15

0.20

(a) Average delay changes

(b) Accuracy changes

Figure 5: Performance and delay under various learning
rates

5.2.3
Impact of the number of workers. The increase in the num-
ber of workers will lead to an increase in transactions, thus impact-
ing the delay. Figure 6a shows the delay changes in this case. We
can see that with the increase in the number of workers ğ‘›, the de-
lay of blockchain increases, and this is because the total number
of new transactions is rising, while the block size is ï¬xed. When
a block cannot contain all transactions, transaction queuing will
occur, which is regarded as a scalability issue [25] in blockchain.
Please note that when the number of new transactions is much
smaller than the block size, the delay caused by the increase of the
clients will be minimal, such as the curve in interval ğ‘› âˆˆ [20, 100];
when the total size of new transactions crosses the block size (ğ‘› â‰¥
100), the delay caused by queuing will become more apparent, even-
tually making the delay of blockchain greater than the delay of
FAIR-BFL, which is consistent with the result of Section 5.2.1. On
the contrary, FAIR-BFL achieves a delay similar to FedAvg, It is al-
most unaï¬€ected by the number of clients. Thanks to Assumptions 1
and 2, no matter how many clients there are, there will be no queu-
ing, because each block only contains the global gradient of the
current round.

Insight 2: The block size will signiï¬cantly aï¬€ect the delay in
large-scale scenarios. Assumptions 1 and 2 provide an eï¬€ective
way to solve this problem.

Impact of the number of miners. In contrast to Figure 6a, we
5.2.4
set the number of clients to 100 and increase the number of miners
to fully observe the impact of this change on delay in Figure 6b. It

s
d
n
o
c
e
s
n
i
y
a
l
e
d

12

11

10

9

8

7

6

5

4

3

FAIR

Blockchain
FedAvg

FAIR

Blockchain

s
d
n
o
c
e
s
n
i
y
a
l
e
d

25.0

22.5

20.0

17.5

15.0

12.5

10.0

7.5

20

40

60

80

100

120

Number of workers

(a) Workers

2

4

6
Number of miners

8

10

(b) Miners

Figure 6: Average delay changes with the number of workers
and miners

can be seen that in blockchain, the delay increases approximately
exponentially as the number of miners increases. Because when
more and more nodes participate in the mining competition, the
probability of forking will signiï¬cantly increase, which will take
more time to merge conï¬‚icts. For FAIR-BFL, this issue is avoided
due to Assumptions 1 and 2. Therefore, the increase in the number
of miners does not signiï¬cantly increase the delay.

Insight 3: Too many miners may cause delay, so the number of
miners should be set appropriately. In BFL, we can alleviate this
issue by Assumptions 1 and 2.

5.3 Cost-eï¬€ectiveness
Here, we observe how Algorithm 2 with discarding strategy aï¬€ects
the delay, the accuracy and the convergence rate of FAIR-BFL. We
use DBSCAN as the default clustering algorithm. Note that FedProx
also drops clients to improve both the convergence rate and the
model accuracy. However, FedProx avoids the global model skew
by discarding stragglers, while we discard the low-contributing
clients implied by the clustering algorithm. To demonstrate the ef-
fectiveness of our contribution-based incentive mechanism, we set
the ğ‘‘ğ‘Ÿğ‘œğ‘_ğ‘ğ‘’ğ‘Ÿğ‘ğ‘’ğ‘›ğ‘¡ of FedProx to 0.02 as a new baseline for compar-
ison.

16

14

12

10

8

6

)
s
(
y
a
l
e
d
e
g
a
r
e
v
A

FAIR-Discard
FAIR

Blockchain
FedAvg

1.0

0.8

0.6

0.4

0.2

y
c
a
r
u
c
c
a

e
g
a
r
e
v
A

0

20

40

60

80

100

0

100

Communication rounds

(a) Delay

FAIR-Discard
FAIR

FedAvg
FedProx-Drop(0.02)

200

300
Time in Seconds

400

(b) Accuracy

Figure 7: FAIR-BFL is faster without reducing accuracy

Then in Figure 7, we observe the diï¬€erence in accuracy and de-
lay with and without the discarding strategy, respectively. It can
be seen from Figure 7a that the discarding strategy signiï¬cantly
reduces the average delay, even lower than that of FedAvg. The rea-
son is that those workers with lower contributions no longer par-
ticipate in the current communication round, which means fewer

FAIR-BFL: Flexible and Incentive Redesign for Blockchain-based Federated Learning

ICPP â€™22, August 29-September 1, 2022, Bordeaux, France

workers and local gradients. On the one hand, fewer workers re-
duce the time cost for local updates and upload gradients and re-
duce the total number of communications, thus reducing costs. On
the other hand, fewer local gradients accelerate the updates ex-
change and global aggregation, thus reduces the size of data pack-
ets in the network, to save the traï¬ƒc and reduce the channel delays.
More importantly, the model converges better and faster so that
the FAIR-BFL with discarding strategy in Figure 7b lies above the
FedAvg and the original FAIR and reaches the convergence point
between 250 and 300 seconds. Although FedProx also converges
better initially, its accuracy stabilizes around 84%, which is lower
than FAIR-BFL. The reason is that the low-contributing clients no
longer participate in global aggregation, thus reducing the noise
from low-quality data, eï¬€ectively preventing the global model from
falling into local optimal points, and improving the accuracy. As
discussed above and shown in Figure 7a, the discarding strategy
signiï¬cantly reduces the average delay, and further reduces the
time to reach the convergence. In conclusion, above results con-
ï¬rm that FAIR-BFL is more economic and faster.

Insight 4: Use the discarding strategy in large-scale scenarios
to speed up model convergence and reduce the cost of communi-
cations and traï¬ƒc.

Table 2: Detecting malicious attacks using our contribution-
based incentive mechanism

Distribution Round Attacker Index Drop Index Detection Rate

Non-IID

1
2
3
4
5
6
7
8
9
10

Average Detection Rate

IID

1
2
3
4
5
6
7
8
9
10

[3, 7]
[3, 6, 2]
[6, 4, 7]
[1, 6, 0]
[2, 8, 0]
[7, 0]
[0]
[3, 9]
[6, 0, 8]
[6, 5]

[0, 6, 1]
[0, 3, 6]
[9]
[2]
[6, 3, 1]
[5, 9]
[3]
[7, 0]
[1, 7, 2]
[9]

Average Detection Rate

[2, 4, 5, 6]
[2, 6]
[4, 6]
[6]
[0, 8]
[0, 7]
[0]
[3]
[0, 8]
[5, 6]

[0, 1]
[3, 6, 8]
[9]
[2]
[1, 3]
[5]
[3]
[7]
[1, 7]
[9]

0%
66.66%
66.66%
33%
66.66%
100%
100%
50%
66.66%
100%

64.96%

66.66%
66.66%
100%
100%
66.66%
50%
100%
50%
66.66%
100%

75%

5.4 Security by design
In Section 5.3, we have shown that implementing Algorithm 2 with
a discarding strategy for client selection is eï¬€ective. Here, we will
demonstrate its security. We set malicious nodes, which modify
the actual local gradients to skew the global model. At the same
time, DBSCAN is also adopted to ï¬nd the diï¬€erence in contribu-
tion. There are 10 indexed clients, and in each communication

round, randomly designate 1 to 3 clients as malicious nodes, and
10 rounds are executed in total, as shown in Table 2.

We can see that when there are few malicious nodes (1 mali-
cious node in this experiment), the detection rate almost always
reaches 100%, and FAIR-BFL identiï¬es the forged gradients as with
the low contributions, e.g., in the communication round 7. It means
that with the vast majority of nodes remaining honest, the behav-
ior of the malicious nodes is evident, because the modiï¬ed local
gradients are distant from the normal ones. As the number of ma-
licious nodes increase, some forged gradients successfully cheat
this mechanism, so the detection rate decreases, because anom-
alies that are obvious enough may mask those that are not obvious.
Even so, the detection rate is maintained at an optimistic level, for
example, in round 9. We also found that the average detection rate
is higher in the case of IID, which is attributed to the fact that a
good distribution of data makes the normal gradients more spa-
tially concentrated and therefore easier to discover anomalies. In-
terestingly, in general, the detection rate increases as the model
converges. The reason is that as the model converges, individual
local gradients are getting similar. The results and the above dis-
cussion indicate that FAIR-BFL can resist malicious attacks to the
greatest extent even in the case of non-IID.

Letâ€™s recall the major design aspects considered in FAIR-BFL to
ensure security: i) we use the RSA algorithm to sign the local gradi-
ent to avoid modiï¬cation during the upload process (see Figure 2).
ii) the data recorded on the blockchain is immutable; iii) we use Al-
gorithm 2 to reveal the contribution diï¬€erences among nodes and
discard low contributing local gradients (forged gradients) to resist
malicious attacks; iv) we do not record any local gradients in the
blockchain, so all nodes cannot observe and exploit this informa-
tion (see Assumption 2). Thus, FAIR-BFL provides the privacy and
security guarantee by design for the whole system dynamics.

6 CONCLUSION
We present a new research problem and develop valuable insights
toward modelling blockchain-based federated learning. FAIR-BFL
comes with a modular design that can dynamically scale functions
according to the adopterâ€™s needs, thus providing unprecedented
ï¬‚exibility. Moreover, we provably alleviate the impending challenges
of the vanilla BFL in terms of adjusting delay, performance, and
aggregation by tight coupling blockchain and FL and fairness ag-
gregation. This is one of the ï¬rst attempts to redeï¬ne the blockâ€™s
data scope in BFL in order to prevent clients from observing othersâ€™
gradients, thus enhancing privacy and security. More importantly,
FAIR-BFL motivates all clients to contribute actively by identifying
the clientâ€™s contribution and issuing uneven rewards. Our exper-
imental results show that FAIR-BFL can achieve desirable perfor-
mance beyond the capacity of existing approaches. Furthermore,
FAIR-BFL that employs the discarding strategy can naturally reap
the beneï¬ts of privacy, malicious attack resistance, and client se-
lection.

ACKNOWLEDGMENTS
This research is supported by the National Natural Science Fund
of China (Project No. 71871090), and Hunan Provincial Science &
Technology Innovation Leading Project (2020GK2005).

ICPP â€™22, August 29-September 1, 2022, Bordeaux, France

Rongxin Xu, Shiva Raj Pokhrel, Qiujun Lan, and Gang Li

A PROOF OF THEOREM 3.1
As mentioned earlier, wğ‘–
ğ‘Ÿ is the local gradient of the client ğ¶ğ‘– at
communication round ğ‘Ÿ . For tracking the learning process at each
local epoch, we use the notion of global epoch and Iğ¸ = {ğ‘›ğ¸ | ğ‘› =
1, 2, Â· Â· Â· ğ‘›}. For convenience, we denote by Cğ‘Ÿ is the most recent
set of clients (with size ğ¾) selected in the communication round
ğ‘Ÿ . Note that Cğ‘Ÿ results from the random selection, and ergodicity
comes from the stochastic gradients. So we slightly abuse the no-
tation ECğ‘Ÿ (Â·), taking the expectation means that we eliminate the
former kind of randomness.

With wğ‘Ÿ +1 denoting the global gradient at round ğ‘Ÿ +1 and vğ‘Ÿ +1
tracking the weighted average of all client-side gradients, we can
see that wğ‘Ÿ +1 is unbiased. In particular, wğ‘Ÿ +1 = vğ‘Ÿ +1. Now, we
formulate the following Lemma A.1 to bound the variance of wğ‘Ÿ .

Lemma A.1. For ğ‘Ÿ + 1 âˆˆ Iğ¸ , if ğœ‚ğ‘Ÿ is non-increasing and ğœ‚ğ‘Ÿ â‰¤
2ğœ‚ğ‘Ÿ +ğ¸ for all ğ‘Ÿ â‰¥ 0, then the expected diï¬€erence between vğ‘Ÿ +1 and
wğ‘Ÿ +1 is

ECğ‘Ÿ kvğ‘Ÿ +1 âˆ’ wğ‘Ÿ +1 k2 â‰¤

ğ‘Ÿ ğ¸2ğº 2
ğœ‚2

We ï¬rst provide the proof of Lemma A.1 which builds the foun-

dation for proving Theorem 3.1.

Proof. Taking expectation over Cğ‘Ÿ +1, we have

4
ğ¾

ğ¾

ECğ‘Ÿ kwğ‘Ÿ +1 âˆ’ vğ‘Ÿ +1k2 = ECğ‘Ÿ

vğ‘–ğ‘™
ğ‘Ÿ +1 âˆ’ vğ‘Ÿ +1

2

(cid:13)
(cid:13)
where the ï¬rst equality follows from vğ‘–ğ‘™
(cid:13)
ğ‘Ÿ +1 are independent and
unbiased. Since ğ‘Ÿ + 1 âˆˆ Iğ¸ , the time ğ‘Ÿ0 = ğ‘Ÿ âˆ’ ğ¸ + 1 âˆˆ Iğ¸ is
the underlying communication time, which implies
are
identical. Furthermore,
ğ‘›

wğ‘˜
ğ‘Ÿ0

ğ‘˜=1

(cid:13)
(cid:13)
(cid:13)

n

o

ğ‘›

ğ‘›

2

ğ‘ğ‘–

ğ‘–=1
X

vğ‘–
ğ‘Ÿ +1 âˆ’ vğ‘Ÿ +1

2

(cid:13)
(cid:13)

(cid:13)
(cid:13)

vğ‘–
ğ‘Ÿ +1 âˆ’ wğ‘Ÿ0

âˆ’

vğ‘Ÿ +1 âˆ’ wğ‘Ÿ0

(cid:13)
(cid:16)
(cid:13)
(cid:13)
vğ‘–
ğ‘Ÿ +1 âˆ’ wğ‘Ÿ0

(cid:17)
2

(cid:0)

(cid:1)(cid:13)
(cid:13)
(cid:13)

=

â‰¤

ğ‘–=1
X
ğ‘›

ğ‘ğ‘–

ğ‘ğ‘–

ğ‘–=1
X

(cid:13)
(cid:13)

where the last inequality results from

(cid:13)
(cid:13)

ğ‘›

ğ‘–=1
X

ğ‘ğ‘–

vğ‘–
ğ‘Ÿ +1 âˆ’ wğ‘Ÿ0

= vğ‘Ÿ +1 âˆ’ wğ‘¡0

(cid:16)

(cid:17)

Ekğ‘¥ âˆ’ Eğ‘¥ k2â‰¤ Ekğ‘¥ k2.

and

Therefore

ğ‘ğ‘–E

vğ‘–
ğ‘Ÿ +1 âˆ’ wğ‘Ÿ0

(cid:13)
(cid:13)

ğ‘ğ‘–E

ğ‘Ÿ +1 âˆ’ wğ‘–
vğ‘–
ğ‘Ÿ0

2

(cid:13)
(cid:13)
2

ğ‘›

ğ‘–=1
X
ğ‘›

ğ‘–=1
X
ğ‘›

ECğ‘Ÿ kwğ‘Ÿ +1 âˆ’ vğ‘Ÿ +1 k2 â‰¤

â‰¤

â‰¤

â‰¤

1
ğ¾

1
ğ¾

1
ğ¾

1
ğ¾

1
ğ¾ 2
ğ‘›

ğ‘˜=1
X

=

1
ğ¾

ğ‘™ =1 (cid:13)
X
(cid:13)
(cid:13)
vğ‘˜
ğ‘Ÿ +1 âˆ’ vğ‘Ÿ +1
ğ‘ğ‘–

(cid:13)
(cid:13)
(cid:13)

2

(cid:13)
(cid:13)
where

As shown in Section 4.4 we adopt the partial aggregation and

therefore the temporal evolution follow

ğ‘Ÿ +1 = ğ‘¤ğ‘–
vğ‘–

wğ‘–

ğ‘Ÿ +1 =

(

ğ‘¤ğ‘–
(cid:16)

ğ‘Ÿ ; ğ‘

ğ‘Ÿ âˆ’ ğœ‚ğ‘– âˆ‡ğ¹ğ‘–
vğ‘–
ğ‘Ÿ +1
samples Cğ‘Ÿ and average

(cid:17)

Nex, we have

if ğ‘Ÿ + 1 /âˆˆ Iğ¸
if ğ‘Ÿ + 1 âˆˆ Iğ¸ .

vğ‘–
ğ‘Ÿ +1

n

o

wğ‘Ÿ +1 âˆ’ wâˆ—

2 =

wğ‘Ÿ +1 âˆ’ vğ‘Ÿ +1 + vğ‘Ÿ +1 âˆ’ wâˆ—

2

(cid:13)
(cid:13)

(cid:13)
(cid:13)

= kwğ‘Ÿ +1 âˆ’ vğ‘Ÿ +1 k2

(cid:13)
(cid:13)

+

vğ‘Ÿ +1 âˆ’ wâˆ—

(cid:13)
(cid:13)

2

ğ´1

ğ´2
wğ‘Ÿ +1 âˆ’ vğ‘Ÿ +1, vğ‘Ÿ +1 âˆ’ wâˆ—
}
{z

{z

(cid:13)
(cid:13)
|

(cid:13)
(cid:13)
}

ğ´3

(cid:11)

|
+ 2

(cid:10)

Considering unbiasedness of ğ‘¤ğ‘Ÿ +1, ğ´3 will be vanished when we
take expectation over Cğ‘Ÿ +1. If ğ‘Ÿ + 1 /âˆˆ Iğ¸, ğ´1 vanishes since
wğ‘Ÿ +1 = vğ‘Ÿ +1. Using Lemma A.1 to bound ğ´2, we get

{z

|

}

E

wğ‘Ÿ +1 âˆ’ wâˆ—

2 â‰¤ (1 âˆ’ ğœ‚ğ‘Ÿ ğœ‡) E

â˜…
wğ‘Ÿ âˆ’ w

2 + ğœ‚2

ğ‘Ÿ ğµ.

When ğ‘Ÿ + 1 âˆˆ Iğ¸ , ğ´1, we apply Lemma A.1 to bound ğ´1 and then
2 = E kwğ‘Ÿ +1 âˆ’ vğ‘Ÿ +1 k2 + E

(cid:13)
(cid:13)
wğ‘Ÿ +1 âˆ’ wâˆ—

(cid:13)
(cid:13)
vğ‘Ÿ +1 âˆ’ wâˆ—

(cid:13)
(cid:13)

(cid:13)
(cid:13)

E

2

(5)

â‰¤ (1 âˆ’ ğœ‚ğ‘Ÿ ğœ‡) E

â˜…
wğ‘Ÿ âˆ’ w

2 + ğœ‚2
(cid:13)
(cid:13)

(cid:13)
ğ‘Ÿ (ğµ + ğ¶)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

ECğ‘Ÿ kvğ‘Ÿ +1 âˆ’ wğ‘Ÿ +1 k2 .

ğ¶ â‰¥

1
ğœ‚2
ğ‘Ÿ

Equation (5) recursively portrays the distance between wğ‘Ÿ +1
and wâˆ—, and we will show how it is a decreasing function. Let
âˆ†ğ‘Ÿ = E kwğ‘Ÿ âˆ’ wâˆ— k2, Equation (5) can be simpliï¬ed to
âˆ†ğ‘Ÿ +1 6 (1 âˆ’ ğœ‚ğ‘Ÿ ğœ‡)âˆ†ğ‘Ÿ + ğœ‚ğ‘Ÿ

2(ğµ + ğ¶)

(6)

For decreasing stepsize ğœ‚ğ‘Ÿ = ğ›½
= 1
min
Equation (6). Therefore, we can see

ğ‘Ÿ +ğ›¾ , we need ğ›½ > 1
ğœ‡ and ğ›¾ > 0. ğœ‚1 â‰¤
4ğ¿ and ğœ‚ğ‘Ÿ â‰¤ 2ğœ‚ğ‘Ÿ +ğ¸ is an important condition for

ğœ‡ , 1
1
4ğ¿

n

o

âˆ†ğ‘Ÿ +1 â‰¤

ğ‘£
ğ›¾ + ğ‘Ÿ

(7)

when

ğ‘£ = max

ğ›½2(ğµ + ğ¶)
ğ›½ğœ‡ âˆ’ 1

(cid:26)

, (ğ›¾ + 1)

w1 âˆ’ wâˆ—

2

.

(cid:13)
(cid:13)

(cid:27)

(cid:13)
(cid:13)

Now we use mathematical induction. When ğ‘Ÿ = 1, Equation (7)
holds from the deï¬nition of ğ‘£. Assuming that the conclusion holds
for some ğ‘Ÿ , then at ğ‘Ÿ + 1,

âˆ†ğ‘Ÿ +1 â‰¤ (1 âˆ’ ğœ‚ğ‘Ÿ ğœ‡) âˆ†ğ‘Ÿ + ğœ‚2
ğ‘Ÿ ğµ

(cid:13)
(cid:13)

ğ‘Ÿ

ğ‘ğ‘–ğ¸

ğ‘–=1
X
ğ¸2ğœ‚2

ğ‘¡=ğ‘Ÿ0
X
ğ‘Ÿ0ğº 2 â‰¤

4
ğ¾

E

(cid:13)
(cid:13)
ğœ‚ğ‘Ÿ âˆ‡ğ¹ğ‘–
(cid:13)
(cid:13)
ğœ‚2
ğ‘Ÿ ğ¸2ğº 2
(cid:13)

2

wğ‘–

ğ‘Ÿ , ğ‘

(cid:16)

(cid:17)(cid:13)
(cid:13)
(cid:13)

(cid:3)

â‰¤

=

â‰¤

ğ‘£
ğ‘Ÿ + ğ›¾

+

ğ›½2ğµ
(ğ‘Ÿ + ğ›¾)2

ğ›½2ğµ
(ğ‘Ÿ + ğ›¾)2

âˆ’

ğ›½ğœ‡ âˆ’ 1
(ğ‘Ÿ + ğ›¾)2

ğ‘£

(cid:21)

(cid:20)

1 âˆ’

ğ›½ğœ‡
ğ‘Ÿ + ğ›¾
(cid:18)
ğ‘Ÿ + ğ›¾ âˆ’ 1
(ğ‘Ÿ + ğ›¾)2
ğ‘£
ğ‘Ÿ + ğ›¾ + 1

(cid:19)

ğ‘£ +

                
                
            
            
                                 
                                 
FAIR-BFL: Flexible and Incentive Redesign for Blockchain-based Federated Learning

ICPP â€™22, August 29-September 1, 2022, Bordeaux, France

[16] Milad Nasr, Reza Shokri, and Amir Houmansadr. 2019. Comprehensive Privacy
Analysis of Deep Learning: Passive and Active White-box Inference Attacks
against Centralized and Federated Learning. In 2019 IEEE Symposium on Secu-
rity and Privacy (SP). 739â€“753. https://doi.org/10.1109/SP.2019.00065

[17] Dinh C. Nguyen, Ming Ding, Quoc-Viet Pham, Pubudu N. Pathirana, Long Bao
Le, Aruna Seneviratne, Jun Li, Dusit Niyato, and H. Vincent Poor. 2021. Fed-
erated Learning Meets Blockchain in Edge Computing: Opportunities and
Challenges.
IEEE Internet of Things Journal 8, 16 (Aug. 2021), 12806â€“12825.
https://doi.org/10.1109/JIOT.2021.3072611

[18] Shiva Raj Pokhrel. 2020. Federated learning meets blockchain at 6G edge: a
drone-assisted networking for disaster response. In Proceedings of the 2nd ACM
MobiCom Workshop on Drone Assisted Wireless Communications for 5G and Be-
yond (DroneCom â€™20). Association for Computing Machinery, New York, NY,
USA, 49â€“54. https://doi.org/10.1145/3414045.3415949

[19] Shiva Raj Pokhrel. 2021.

Blockchain Brings Trust

to Collaborative
Drones and LEO Satellites: An Intelligent Decentralized Learning in
the Space.
IEEE Sensors Journal 21, 22 (Nov. 2021), 25331â€“25339.
https://doi.org/10.1109/JSEN.2021.3060185 Conference Name: IEEE Sensors
Journal.

[20] Shiva Raj Pokhrel and Jinho Choi. 2020. A Decentralized Federated Learn-
ing Approach for Connected Autonomous Vehicles.
In 2020 IEEE Wire-
less Communications and Networking Conference Workshops (WCNCW). 1â€“6.
https://doi.org/10.1109/WCNCW48565.2020.9124733

[21] Shiva Raj Pokhrel and Jinho Choi. 2020.

Federated Learning With
Blockchain for Autonomous Vehicles: Analysis and Design Challenges.
IEEE Transactions on Communications 68, 8 (Aug. 2020), 4734â€“4746.
https://doi.org/10.1109/TCOMM.2020.2990686

[22] Rodrigo Roman,

features
ternet of
https://doi.org/10.1016/j.comnet.2012.12.018

and challenges of
things.

Jianying Zhou, and Javier Lopez. 2013.

On the
security and privacy in distributed in-
Computer Networks 57, 10 (July 2013), 2266â€“2279.

[23] Sebastian U. Stich. 2019. Local SGD converges fast and communicates little. In
7th international conference on learning representations, ICLR 2019, new orleans,
LA, USA, may 6-9, 2019. OpenReview.net. https://arxiv.org/abs/1805.09767
[24] Jianyu Wang, Qinghua Liu, Hao Liang, Gauri Joshi, and H. Vincent Poor. 2020.
Tackling the Objective Inconsistency Problem in Heterogeneous Federated Opti-
mization. In Advances in Neural Information Processing Systems, Vol. 33. Curran
Associates, Inc., 7611â€“7623.

[25] Kaiwen Zhang and Hans-Arno Jacobsen. 2018. Towards Dependable, Scalable,
and Pervasive Distributed Ledgers with Blockchains.. In ICDCS. 1337â€“1346.
[26] Yuchen Zhang, Martin J Wainwright, and John C Duchi. 2012. Communication-
eï¬ƒcient algorithms for statistical optimization. In Advances in neural informa-
tion processing systems, F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger
(Eds.), Vol. 25. Curran Associates, Inc. https://doi.org/10.1109/cdc.2012.6426691

Using strong convexity of ğ¹ (Â·),

E [ğ¹ (wğ‘Ÿ )] âˆ’ ğ¹ âˆ— â‰¤

ğ¿
2

âˆ†ğ‘Ÿ â‰¤

ğ¿
2

ğ‘£
ğ›¾ + ğ‘Ÿ

.

When ğ›½ = 2
ğœ‚ğ‘Ÿ = 2
ğœ‡

8 ğ¿
ğœ‡ , ğ¸
1
ğ›¾+ğ‘Ÿ , which satisï¬es ğœ‚ğ‘Ÿ â‰¤ 2ğœ‚ğ‘Ÿ +ğ¸. Therefore,

ğœ‡ , ğ›¾ = max

âˆ’ 1 and with ğœ… = ğ¿

n

o

ğœ‡ , we have

E [ğ¹ (wğ‘Ÿ )] âˆ’ ğ¹ âˆ— â‰¤

ğœ…
ğ›¾ + ğ‘Ÿ

(cid:18)

2(ğµ + ğ¶)
ğœ‡

+

ğœ‡(ğ›¾ + 1)
2

This proves Theorem 3.1.

w1 âˆ’ wâˆ—

2

(cid:13)
(cid:13)

(cid:19)

(cid:13)
(cid:13)

REFERENCES
[1] Sana Awan, Fengjun Li, Bo Luo, and Mei Liu. 2019.

Poster: A Reliable
and Accountable Privacy-Preserving Federated Learning Framework using the
Blockchain. In Proceedings of the 2019 ACM SIGSAC Conference on Computer and
Communications Security (CCS â€™19). Association for Computing Machinery, New
York, NY, USA, 2561â€“2563. https://doi.org/10.1145/3319535.3363256

[2] Xianglin Bao, Cheng Su, Yan Xiong, Wenchao Huang, and Yifei Hu. 2019.
FLChain: A Blockchain for Auditable Federated Learning with Trust and Incen-
tive. In 2019 5th International Conference on Big Data Computing and Communi-
cations (BIGCOM). 151â€“159. https://doi.org/10.1109/BIGCOM.2019.00030
[3] Bin Cao, Yixin Li, Lei Zhang, Long Zhang, Shahid Mumtaz, Zhenyu Zhou,
and Mugen Peng. 2019. When Internet of Things Meets Blockchain: Chal-
lenges in Distributed Consensus.
IEEE Network 33, 6 (Nov. 2019), 133â€“139.
https://doi.org/10.1109/MNET.2019.1900002

[4] Jianmin Chen, Xinghao Pan, Rajat Monga, Samy Bengio, and Rafal Jozefowicz.
2017. Revisiting Distributed Synchronous SGD. arXiv:1604.00981 [cs] (March
2017). http://arxiv.org/abs/1604.00981

[5] Yae Jee Cho, Jianyu Wang, and Gauri Joshi. 2022. Towards Understanding Bi-
ased Client Selection in Federated Learning. In Proceedings of The 25th Inter-
national Conference on Artiï¬cial Intelligence and Statistics. PMLR, 10351â€“10375.
https://proceedings.mlr.press/v151/jee-cho22a.html ISSN: 2640-3498.

[6] Matt Fredrikson, Somesh Jha, and Thomas Ristenpart. 2015. Model Inversion At-
tacks that Exploit Conï¬dence Information and Basic Countermeasures. In Pro-
ceedings of the 22nd ACM SIGSAC Conference on Computer and Communications
Security (CCS â€™15). Association for Computing Machinery, New York, NY, USA,
1322â€“1333. https://doi.org/10.1145/2810103.2813677

[7] Hyesung Kim,

Jihong Park, Mehdi Bennis, and Seong-Lyun Kim. 2020.
Blockchained On-Device Federated Learning. IEEE Communications Letters 24,
6 (June 2020), 1279â€“1283. https://doi.org/10.1109/LCOMM.2019.2921755

[8] Jakub KoneÄnÃ½, H. Brendan McMahan, Daniel Ramage, and Peter RichtÃ¡rik. 2016.
Federated Optimization: Distributed Machine Learning for On-Device Intelli-
gence. arXiv:1610.02527 [cs] (Oct. 2016). http://arxiv.org/abs/1610.02527

[9] Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and
Virginia Smith. 2020. Federated optimization in heterogeneous networks. In
Proceedings of machine learning and systems, I. Dhillon, D. Papailiopoulos, and
V. Sze (Eds.), Vol. 2. 429â€“450.

[10] Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, and Zhihua Zhang. 2020.
On the convergence of FedAvg on non-iid data. In 8th international conference
on learning representations, ICLR 2020, addis ababa, ethiopia, april 26-30, 2020.
OpenReview.net. https://openreview.net/forum?id=HJxNAnVtDS

[11] Yuzheng Li, Chuan Chen, Nan Liu, Huawei Huang, Zibin Zheng, and Qiang
Yan. 2021. A Blockchain-Based Decentralized Federated Learning Frame-
work with Committee Consensus.
IEEE Network 35, 1 (Jan. 2021), 234â€“241.
https://doi.org/10.1109/MNET.011.2000263

[12] Yunlong Lu, Xiaohong Huang, Yueyue Dai, Sabita Maharjan, and Yan Zhang.
2020. Blockchain and Federated Learning for Privacy-Preserved Data Sharing
in Industrial IoT.
IEEE Transactions on Industrial Informatics 16, 6 (June 2020),
4177â€“4186. https://doi.org/10.1109/TII.2019.2942190

[13] Chuan Ma, Jun Li, Ming Ding, Long Shi, Taotao Wang, Zhu Han, and
H. Vincent Poor. 2021. When Federated Learning Meets Blockchain: A
New Distributed Learning Paradigm.
arXiv:2009.09338 [cs] (June 2021).
http://arxiv.org/abs/2009.09338

[14] Umer Majeed and Choong Seon Hong. 2019.

FLchain: Federated
Learning via MEC-enabled Blockchain Network.
In 2019 20th Asia-
Paciï¬c Network Operations and Management Symposium (APNOMS). 1â€“4.
https://doi.org/10.23919/APNOMS.2019.8892848

[15] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and
Blaise Aguera y Arcas. 2017. Communication-Eï¬ƒcient Learning of Deep
Networks from Decentralized Data.
the 20th Interna-
tional Conference on Artiï¬cial Intelligence and Statistics. PMLR, 1273â€“1282.
https://proceedings.mlr.press/v54/mcmahan17a.html

In Proceedings of

