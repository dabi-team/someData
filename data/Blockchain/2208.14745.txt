Simulating BFT Protocol Implementations at Scale
Sadok Ben Toumia
bentou01@ads.uni-passau.de
University of Passau
Passau, Germany

Christian Berger
cb@sec.uni-passau.de
University of Passau
Passau, Germany

Hans P. Reiser
hansr@ru.is
Reykjav√≠k University
Reykjav√≠k, Iceland

2
2
0
2

p
e
S
6

]

C
D
.
s
c
[

2
v
5
4
7
4
1
.
8
0
2
2
:
v
i
X
r
a

Abstract
The novel blockchain generation of Byzantine fault-tolerant
(BFT) state machine replication (SMR) protocols focuses
on scalability and performance to meet requirements of
distributed ledger technology (DLT), e.g., decentralization
and geographic dispersion. Validating scalability and per-
formance of BFT protocol implementations requires careful
evaluation. While experiments with real protocol deploy-
ments usually offer the best realism, they are costly and
time-consuming. In this paper, we explore simulation of un-
modified BFT protocol implementations as as a method for
cheap and rapid protocol evaluation: We can accurately fore-
cast the performance of a BFT protocol while experimentally
scaling its environment, i.e., by varying the number of nodes
or geographic dispersion. Our approach is resource-friendly
and preserves application-realism, since existing BFT frame-
works can be simply plugged into the simulation engine
without requiring code modifications or re-implementation.

CCS Concepts: ‚Ä¢ General and reference ‚Üí Performance;
Evaluation; Experimentation; ‚Ä¢ Computing methodolo-
gies ‚Üí Distributed algorithms.

Keywords: simulation, emulation, Byzantine fault tolerance,
state machine replication, consensus, Shadow, Phantom

1 Introduction
The current transition towards Web3 presents many chal-
lenges in terms of scalability and performance of distributed
ledger technology (DLT). Proof-of-Work [23] is still widely
used today, even if it is not environmental sustainable and
can often not meet performance requirements of applica-
tions [7]. Consequently, coordination-based Byzantine fault-
tolerant (BFT) state machine replication (SMR) algorithms
experienced renewed research interest [3, 32] ‚Äì resulting
in many novel BFT protocols with focus on improving scal-
ability [9, 11, 24, 31, 36], or boosting performance under
geographic dispersion [5, 8, 20, 30].

It is a challenging endeavour to reason about the perfor-
mance and run-time behavior of these novel BFT protocols.
In fact, analyzing BFT protocols requires thorough evalu-
ation, which is why the research papers describing these
protocols contain evaluations with large-scale deployments
that are conducted on cloud platforms like AWS, where ex-
periments deploy up to several hundred nodes (e.g., like
in [9, 11, 20, 24, 36] and many more) to demonstrate a pro-
tocol‚Äôs performance and scalability. Evaluations using real

protocol deployments usually offer the best realism, but are
costly and time-consuming. Thus, a reasonable alternative
for cheap and rapid validation of BFT protocol implementa-
tions (that are possibly still in development stage) can be to
rely on either emulation or simulation.

Emulation vs. Simulation. Emulation tries to duplicate the
exact behavior of what is being emulated. A clear advan-
tage of emulation is how it preserves realism: BFT protocols
still operate in real time and use real kernel and network
protocols. As examples serve Mininet [15, 19], which cre-
ates a realistic virtual network running real kernel, switch
and application code on a single machine, or Kollaps [14], a
decentralized and dynamic topology emulator.

In contrast to emulation, simulation decouples simulated
time from real time and employs abstractions that help ac-
celerate executions: Aspects of interest are captured through
a model, which means the simulation only mimics the pro-
tocol‚Äôs environment or its behavior. This has the advantage
of easier experimental control, excellent reproducibility (i.e.,
deterministic protocol runs) and increased scalability when
compared to emulation. As potential drawback remains the
question of application-realism since the model may not
fairly enough reflect reality. Examples of simulators include
ns-3 [26] or Shadow [16], which are both discrete-event net-
work simulators for internet applications.

Evaluating BFT Protocols. BFTSim [29] is the first simu-
lator that was developed for an eye-to-eye comparison of
BFT protocols but it lacks the necessary scalability to be
useful for the newer ‚Äúblockchain generation‚Äù of BFT proto-
cols (and apparently only up to ùëõ = 32 PBFT [10] replicas
can be successfully simulated [34]). A more recent tool [34]
allows for scalable simulation of BFT protocols but it unfor-
tunately requires a complete re-implementation of the BFT
protocol in JavaScript. It also can not make predictions on
system throughput. Kollaps [14] was used to reproduce AWS-
deployed experiments with BFT-SMaRt [6] and WHEAT [30]
but it is not sufficiently resource-friendly as it executes the
real application code in real-time, thus requiring many phys-
ical machines to conduct large-scale experiments.

Research Questions & Contributions. We explore simula-
tion as a method to evaluate BFT protocol implementations,
which leads us to the following two research questions:

R1 What are properties of an ideal performance evalua-
tion tool for the "blockchain generation" of BFT proto-
cols?

 
 
 
 
 
 
BFTSim [29]
‚úó

BFT Simulator [34]
‚úó

Kollaps [14]
‚úì

application layer realism

realistic networking

scalability

resource friendliness

‚úì

‚úó

‚úì

Byzantine attacker

(only bengin faults)

(high level model)
‚úì

‚úì

‚úì

‚úì

‚úì

‚úó

‚úó

ns-3 [26] Mininet [15, 19]

‚úó

‚úì

‚úì

‚úì

‚úó

‚úì

‚úì

‚úó

‚úó

‚úó

Phantom [17]
‚úì

‚úì

‚úì

‚úì

‚úó

Table 1. Comparison of different emulators and simulators in the context of BFT protocol research.

R2 Can simulations help us to reason about the behavior
of real BFT protocol implementations at larger scale?

Our contributions aim for supporting validations of novel
BFT protocol implementations for their practical deploy-
ments in large-scale DLT systems. In the following, we sum-
marize our main findings:

‚Ä¢ We first compare existing simulators and emulators to
analyze properties of an ideal evaluation tool in the con-
text of BFT protocol research. A key finding is, that the
state-of-the art is deficient as there is no resource-friendly
evaluation tool to predict the performance (i.e., latency
and throughput) of BFT protocols at a larger scale.

‚Ä¢ We present a tool that automates large-scale simulations
of unmodified BFT protocol implementations through the
Phantom simulator [17] given a simple experimental de-
scription. For the first time, experiments with existing BFT
protocol implementations can be effortless setup, config-
ured and fed into a simulation engine (Sects. 3 and 4).
‚Ä¢ We discovered that we can faithfully forecast the perfor-
mance of BFT protocols because performance eventually
becomes network-bound at a larger scale. Our evaluations
compare results obtained from simulations with measure-
ments of real protocol deployments (Sect. 5).

2 Related Work & Background
BFTSim [29] was the first simulator especially tailored for
traditional BFT protocols like PBFT [10] or Zyzzyva [18].
Since these protocols were intended to be used for only small
groups of replicas, the limited scalability of the simulator
was at that time not an issue. However, it makes BFTSim
impractical for the newer BFT protocols. BFTSim demands a
BFT protocol to be modeled in the P2 language [21], which
is somewhat error-prone when considering the complexity
of, e.g., PBFT‚Äôs view change or Zyzzyva‚Äôs many corner cases.
Although BFTSim allows the simulation of faults, it only
considers non-malicious behavior and left the extension to
more sophisticated Byzantine attacks for future work. It
provides realistic networking using ns-2, and is resource-
friendly as it runs on a single machine.

Recently, a BFT simulator was presented by Wang et
al. [34] which demonstrated resource-friendliness, high scal-
ability, and comes with an attacker module which includes a
pre-defined set of attacks (partitioning, adaptive, rushing).

2

The simulator does not mimic real network protocols, in-
stead it tries to capture network characteristics in a high-level
model where messages can be delayed by some variable sam-
pled from a (to be defined) Gaussian or Poisson distribution.
Like BFTSim, it does not provide application layer realism
and demands the re-implementation of a BFT protocol in
JavaScript. A further drawback is that it cannot measure sys-
tem throughput, and is thus not suited for reasoning about
system performance. Further, related work also includes sto-
chastic modelling of BFT protocols [25] and validations of
BFT protocols through unit test generation [2].

There are simulators which are dedicated to blockchain
research, such as Shadow-Bitcoin [22], Bitcoin blockchain
simulator [13], BlockSim [12], SimBlock [1] or ChainSim [33].
These tools mainly focus on building models that capture
the characteristics of Proof-of-Work and thus can not easily
be adopted or used for BFT protocol research.

Further, there are tools to emulate or simulate distributed
applications: Mininet [15, 19] and Kollaps [14] are emulators
that allow to create realistic networks (running real internet
protocols) and real application code with time being synchro-
nous with the wallclock. Naturally, both approaches provide
a high degree of realism, which comes at the cost of being
less resource-friendly. Mininet is not scalable, a problem
which was addressed later by Maxinet [35], which allows
Mininet emulated networks to spawn over several physical
machines. Kollaps is a scalable emulator, but also requires
many physical machines for large-scale experiments. More-
over, ns-3 [26] is a resource-friendly and scalable network
simulator, but it requires the development of an application
model, and thus does not preserve application layer realism.
Phantom [17] uses a hybrid emulation/simulation archi-
tecture: It executes real applications as native OS processes,
co-opting the processes into a high-performance network
and kernel simulation and thus can scale to large system
sizes. An advantage of this is that it preserves application
layer realism as real BFT protocol implementations are exe-
cuted. At the same time, it is resource-friendly and runs on
a single machine. Through its hybrid architecture, Phantom
resides in a sweet-spot between ns-3 (pure simulator) and
Mininet (pure emulator): It still provides sufficient applica-
tion realism for the execution of BFT protocols, but is more
resource-friendly and scalable than the emulators are.

Figure 1. The Phantom architecture (high-level overview).

Figure 2. Our toolchain architecture for automating the
setup of simulation runs of BFT protocols with Phantom.

As shown in Table 1, there is no perfect solution for simu-
lating BFT protocols at scale yet. If we require both resource-
friendliness and scalability, which we think are necessary
characteristics to evaluate scalable BFT protocols in an in-
expensive way, then only the BFT Simulator of Wang et
al. [34] and Phantom [17] are viable options. Comparing
these two, we decided to build our evaluation toolchain on
top of Phantom, because it allows to plug and play BFT pro-
tocol implementations and can measure system throughput.

3 Preliminaries: Phantom
Phantom uses a hybrid simulation/emulation architecture, in
which real, unmodified applications execute as normal pro-
cesses on Linux and are hooked into the simulation through
a system call interface using standard kernel facilities [17].
In Phantom, a network topology (the environment) can be
described by specifying a graph, where virtual hosts are nodes
and communication links are edges. The graph is attributed:
For instance, virtual hosts specify available uplink/downlink
bandwidth and links specify latency and packet loss. Each
virtual host can be used to run one or more applications.
This results in the creation of real Linux processes that are
initialized by the simulator controller process as managed
processes (managed by a Phantom worker). The Phantom
worker uses LD_PRELOAD to preload a shared library (called
the shim) for co-opting its managed processes into the sim-
ulation (see Figure 1). LD_PRELOAD is extended by a second
interception strategy, which uses seccomp for cases in which
preloading does not work [17].

The shim constructs an inter-process communication chan-
nel (IPC) to the simulator controller process and intercepts
functions at the system call interface. While the shim may
directly emulate a few system calls, most system calls are
forwarded and handled by the simulator controller process,
which simulates kernel and networking functionality (for
example the passage of time, I/O operations on file, socket,
pipe, timer, event descriptors and packet transmissions) [17].

4 A Simulation Toolchain for BFT
Large-scale simulations of BFT protocols with Phantom re-
quires additional tooling support. This is mainly because
of the following reasons: First, Phantom requires to gener-
ate realistic and large network topologies for an arbitrary
system size and the characteristics of their communication
links should ideally resemble real-world deployments. This
is crucial to allow realistic simulation of wide-area network
environments. Second, we need aid in setting up the BFT
protocol implementations for their deployment, since boot-
strapping a BFT protocol in Phantom involves many steps
that can be tedious, error-prone and protocol-specific. This
means, for instance, the generation of protocol-specific run-
time artifacts like cryptographic key material, or configu-
ration files which differ for every BFT protocol. Third, in
the process of developing and testing BFT algorithms, dif-
ferent combinations of protocol settings result in numerous
experiments to be conducted. Since Phantom simulations
run in virtual time, they can take hours, depending on the
host system‚Äôs specifications. For the sake of user experience
and convenience, we find it is necessary for experiments to
be specified in bulk and ran sequentially without any need
for user-intervention. Fourth, we may want to track and
evaluate resources needed during simulation runs, such as
CPU utilization and memory usage. Fifth, when Phantom
produces results, they are resided in the file system and for
convenience we want to aggregate measurements of several
simulations and map these to diagrams displaying to-be-
specified metrics like throughput or latency.

These reasons led us to develop Delphi-BFT1, a tool on
top of Phantom to simplify and accelerate the evaluation of
unmodified BFT protocol implementations.

Architecture
Delphi-BFT is composed of several components (see Fig 2)
and follows a modular architecture, in that it is not tailored
to a specific BFT protocol, but is easily extensible.

1Code open-source available at https://github.com/Delphi-BFT/tool.

3

Process 1Shimlibc APIVirtual Host 1Process 2ShimVirtual Host 2...Process nShimVirtual Host nSimulator Controller ProcessSimulated OS KernelSimulated Network TransportSyscallSyscallSyscallShim interposes the system callIPC ChannelsOrchestratorEnvironmentGeneratorcloudpingHot-StuffThemisBFT-SMaRtProtocolConnectorsResourceMonitorPlotterPhantom Simulation    start nextprepare env.configurationresultsinitexperiments description file...Library of BFT ImplementationsBFT protocol

framework
libhotstuff [36] Hot-Stuff
themis [27]
bft-smart [6]
Table 2. BFT protocols that we employed for our evaluation.

repo on github.com
/hot-stuff/libhotstuff
/ibr-ds/themis
/bft-smart/library

language
C++
Rust
Java

PBFT
BFT-SMaRt

Orchestrator. The toolchain is administered by an orches-
trator that manages all tools, i.e., for preparing an environ-
ment, configuring runtime artifacts for a BFT protocol, and
initializing a resource monitor. The orchestrator invokes
protocol connectors to set up a BFT protocol and loads exper-
iments description files which contain a set of experiments to
be conducted for the specified BFT protocol. Finally, it starts
Phantom, once an experiment is ready for its execution.

Environment Generator. The environment generator cre-
ates network topologies as complete graph for any system
size. The network topologies resemble realistic deployment
scenarios for a LAN or WAN setting. To create network
graphs with network links reflecting a realistic geographic
dispersion of nodes, the environment generator employs a
cloudping component, which retrieves real round-trip laten-
cies between all AWS regions from Cloudping2. This allows
the tool to create network topologies which resemble real
BFT protocol deployments on the AWS cloud infrastructure.
Protocol Connectors. For each BFT protocol implementation
that we want to simulate, it is necessary to create protocol
configuration files and necessary keys. Since protocol op-
tions and cryptographic primitives vary depending on the
concrete BFT protocol, we implement the protocol-specific
setup routine as a tool called protocol connector, which is
invoked by the orchestrator. A connector must implement
the methods build() and configure(). This way, it is sim-
ple to extend our toolchain and support new BFT protocols,
as it only requires writing a new protocol connector (in our
experience this means writing between 100 and 200 LoC).

Resource Monitor. The orchestrator initializes a resource
monitor to collect information on resource consumption (like
allocated memory and CPU time) during simulation runs
and also the total simulation time. The user can use these
statistics as indicators towards a possible need for vertically
scaling the host machine and as rough estimates for the
necessary resources to run larger simulations.

Plotter. Results are stored to the file system by Phantom.
They can be aggregated and mapped to specific diagrams for
specifiable metrics like latency of throughput. For instance,
it can create diagrams that display the performance of a BFT
protocol for increasing system scale which aggregate several
simulation runs for an increasing ùëõ (or any other variable).

2See https://www.cloudping.co/grid.

4

Figure 3. Performance of HotStuff and Simulated-HotStuff.

5 Results
In this section, we try to find an answer to our second re-
search question. We first use results from the HotStuff pa-
per [36] to compare our simulation results with real mea-
surements and reason about resource utilization of our sim-
ulations. Then, we show that we can achieve realistic results
under geographic dispersion by simulating a WAN topology
for BFT-SMaRt [6] replicas. Further, we experiment with a
Rust-based PBFT implementation [27] to demonstrate com-
patibility with a third programming language (see Table 2).

5.1 HotStuff at Increasing System Scale
In our first evaluation, we try to mimic the evaluation setup
of the HotStuff paper [36] to compare their measurements
with our simulation results. Their setup consists of more than
hundred virtual machines deployed in an AWS data center;
each machine has up to 1.2 GB/s bandwidth and there is
less than 1 ms latency between each pair of machines (we
use 1 ms in the simulation). The employed batch size is
400. We compare against two measurement series: "p1024"
where the payload size of request and responses is 1024
bytes and "10ms" with empty payload, but the latency of all
communication links is set to 10 ms. Our goal is to investigate
how faithfully the performance of HotStuff can be predicted
by regarding only the networking capabilities of replicas,
which manifests at the point where the network becomes
the bottleneck for system performance.

Observations. We display our results in Figure 3. The sim-
ulation results for the payload experiment indicate a similar
trend as the real measurements, where performance starts

4163264128050100150200ReplicasThroughput[kOps/s]payload=1024bytesrealsim.41632641280255075100125150ReplicasLatency[ms]41632641280510152025ReplicasThroughput[kOps/s]latency=10msrealsim.41632641280255075100125150ReplicasLatency[ms]Figure 4. Resource consumption of simulations.

(a) Consensus latency.

(b) End-to-end request latencies ob-
served by clients in AWS regions.

Figure 5. Comparison of a real BFT-SMaRt WAN deploy-
ment on the AWS infrastructure with its simulated counter-
part.

to drop for ùëõ ‚â• 32. For a small sized replica group, the net-
work simulation predicts higher performance: 200k tx/s. This
equals the theoretical maximum limited only through the 1
ms link latency which leads to pipelined HotStuff commit-
ting a batch of 400 requests every 2 ms. The difference in
throughput decreases once the performance of HotStuff be-
comes more bandwidth-throttled (at ùëõ ‚â• 32). We also achieve
close results in the "10ms" setting: 80 ms in the simulation
vs 84.1 ms real, and 20k tx/s in the simulation vs. 19.2k tx/s
real for ùëõ = 4; but with an increasing difference for higher ùëõ,
i.e., 84 ms vs. 106 ms and 19k.2 tx/s vs. 15.1k tx/s for ùëõ = 128.
Resource Usage. Further, we investigate how resource uti-
lization, i.e. memory usage and simulation time, grows with
an increasing system scale. We run our HotStuff "10ms" sim-
ulations (which display a somewhat steady system perfor-
mance for increasing system scale) on an Ubuntu 20.04 VM
with 48 GB memory and 20 threads (16 threads used for sim-
ulation) on a host with an Intel Xeon Gold 6210U CPU at
2.5 GHz. We observe that active host memory and elapsed
time grow with increasing system scale (see Fig. 4). We think
it should be feasible to simulate up to 512 HotStuff replicas
with a well-equipped host (with e.g., 64 GB RAM).

5.2 BFT-SMaRt under Geographic Dispersion
Next, we experiment with geographic dispersion of BFT-
SMaRt replicas, where each replica is located in a distinct
AWS region. Our experimental setup is thus similar to ex-
periments found in papers that research on latency improve-
ments [4, 5, 30]. We employ a ùëõ = 4 configuration and choose
the regions Oregon, Ireland, S√£o Paulo and Sydney for the

5

Figure 6. Simulation results of PBFT vs. HotStuff for 1 KiB.

deployment of a replica and a client application each. We run
clients one after another, and each samples 1000 requests
without payload and measures end-to-end latency, while the
leader replica (in Oregon) measures the system‚Äôs consen-
sus latency. Further, we create an experiments description
file mimicing this experiment (see Appendix C) and run it
through our simulation toolchain to compare our simulation
results with real measurements.

Observations. We notice that consensus latency is slightly
higher in the simulation (237 ms vs. 249 ms), and further, the
simulation results also display slightly higher end-to-end
request latencies in all clients (see Figure 5). The deviation
between simulated and real execution is the lowest in Oregon
(1.3%) and the highest in S√£o Paulo (3.5%).

5.3 PBFT at Increasing System Scale
We run simulations with 1KiB payload with Themis [27] (a
Rust-based implementation of PBFT) to compare the results
against our HotStuff simulation results.

Observations. PBFT initially outperfroms HotStuff, but
then its throughput decreases more swiftly (as can be seen
in the sharper curve in Figure 6). At ùëõ = 128, PBFT achieves
up to 9.3k tx/s while HotStuff achieves up to 20k tx/s.

6 Future Work
Extending Evaluations. For future work, we intend to ex-
tend our evaluations to more BFT protocols, in particular, to
evaluate the effectiveness of different communication strate-
gies, like Gosig [20] (gossip) or Kauri [24] (tree-based) and
compare them with the results obtained from Hot-Stuff (star-
based) and PBFT (clique). In particular, we can explore the
performance of these protocols under different network char-
acteristics and for an increasing system scale. A high-level
simulation model previously studied the effect of different
message exchange patterns of BFT protocols [28] but it lacks
applicability for reasoning about real system metrics.

CPU Model. We think a CPU model could improve simula-
tion results for evaluations of either (1) small sized replica
groups or (2) experiments with empty payload ‚Äì in both
cases the CPU may be the dominating factor and not the
network. Up to now, we use Phantom only as a network

41632641280816ReplicasMemory[GB]4163264128015304560ReplicasTime[m]Consensus100200300400500600Latency[ms]realsim.OregonIrelandS√£oPauloSydney100200300400500600Latency[ms]realAWSsimulation4163264128050100150200250300ReplicasThroughput[kOps/s]PBFTHotStuff4163264128050100150ReplicasLatency[ms]simulator and all computations, such as creating or verify-
ing signatures, take no time. It might be possible to capture
most of the computational work by only modeling a few
methods, in particular, the cryptographic primitives (like in
BFTSim [29]). Currently, Phantom plans the introduction of
a CPU model as a future milestone for development and we
will try to utilize it to improve our simulation results.

Attacker Model. Moreover, we have in view to introduce
an attacker model to reason about the impact of attacks on
system performance. For this reason, we seek inspiration
from the Twins [2] methodology, a recent approach for vali-
dating BFT protocols: Twins is an unit test case generator
that can simulate Byzantine attacks by duplicating crypto-
graphic identities of replicas (which then leads to forgotten
protocol states, or equivocations) and it can be quite useful
in a simulator to explore a variety of attacking scenarios.

Acknowledgments
This work has been funded by the Deutsche Forschungs-
gemeinschaft (DFG, German Research Foundation) grant
number 446811880 (BFT2Chain). We are thankful for the
help we received from the Phantom development team.

References
[1] Yusuke Aoki, Kai Otsuki, Takeshi Kaneko, Ryohei Banno, and Kazuyuki
Shudo. 2019. Simblock: A blockchain network simulator. In IEEE Conf.
on Computer Communications Workshops. IEEE Comp. Soc., Washing-
ton, DC, USA, 325‚Äì329.

[2] Shehar Bano, Alberto Sonnino, Andrey Chursin, Dmitri Perelman,
Zekun Li, Avery Ching, and Dahlia Malkhi. 2022. Twins: BFT Systems
Made Robust. In 25th Int. Conf. on Principles of Distributed Systems,
Vol. 217. Schloss Dagstuhl ‚Äì Leibniz-Zentrum f√ºr Informatik, Dagstuhl,
Germany, 7:1‚Äì7:29.

[3] Christian Berger and Hans P. Reiser. 2018. Scaling Byzantine Consen-
sus: A Broad Analysis. In the 2nd Workshop on Scalable and Resilient
Infrastructures for Distributed Ledgers. ACM, New York, NY, 13‚Äì18.
[4] Christian Berger, Hans P. Reiser, and Alysson Bessani. 2021. Making
Reads in BFT State Machine Replication Fast, Linearizable, and Live.
In 40th Int. Symp. on Reliable Distributed Systems. IEEE Comp. Soc.,
Washington, DC, USA, 1‚Äì12.

[5] Christian Berger, Hans P. Reiser, Jo√£o Sousa, and Alysson Neves
Bessani. 2022. AWARE: Adaptive wide-area replication for fast and
resilient Byzantine consensus. IEEE Transactions on Dependable and
Secure Computing 19, 3 (2022), 1605‚Äì1620.

[6] Alysson Bessani, Jo√£o Sousa, and Eduardo EP Alchieri. 2014. State
machine replication for the masses with BFT-SMaRt. In 44th Annu.
IEEE/IFIP Int. Conf. on Dependable Systems and Networks (DSN). IEEE
Comp. Soc., Washington, DC, USA, 355‚Äì362.

[7] Joseph Bonneau, Andrew Miller, Jeremy Clark, Arvind Narayanan,
Joshua A Kroll, and Edward W Felten. 2015. Sok: Research perspectives
and challenges for bitcoin and cryptocurrencies. In IEEE Symp. on
security and privacy. IEEE Comp. Soc., Washington, DC, USA, 104‚Äì
121.

[8] Lo√Øck Bonniot, Christoph Neumann, and Fran√ßois Ta√Øani. 2020. Pnyxdb:
a lightweight leaderless democratic Byzantine fault tolerant replicated
datastore. In Int. Symp. on Reliable Distributed Systems. IEEE Comp.
Soc., Washington, DC, USA, 155‚Äì164.

[9] Daniel Cason, Enrique Fynn, Nenad Milosevic, Zarko Milosevic, Ethan
Buchman, and Fernando Pedone. 2021. The design, architecture and

6

performance of the Tendermint Blockchain Network. In 40th Int. Symp.
on Reliable Distributed Systems. IEEE Comp. Soc., Washington, DC,
USA, 23‚Äì33.

[10] Miguel Castro and Barbara Liskov. 1999. Practical Byzantine Fault
Tolerance. In OSDI. USENIX Association, Berkeley, CA, USA, 173‚Äì186.
[11] Tyler Crain, Christopher Natoli, and Vincent Gramoli. 2021. Red belly:
A secure, fair and scalable open blockchain. In IEEE Symp. on Security
and Privacy. IEEE Comp. Soc., Washington, DC, USA, 466‚Äì483.
[12] Carlos Faria and Miguel Correia. 2019. BlockSim: blockchain simulator.
In IEEE Int. Conf. on Blockchain. IEEE Comp. Soc., Washington, DC,
USA, 439‚Äì446.

[13] Arthur Gervais, Ghassan O Karame, Karl W√ºst, Vasileios Glykantzis,
Hubert Ritzdorf, and Srdjan Capkun. 2016. On the security and per-
formance of proof of work blockchains. In ACM SIGSAC CCS. ACM,
New York, NY, 3‚Äì16.

[14] Paulo Gouveia, Jo√£o Neves, Carlos Segarra, Luca Liechti, Shady Issa,
Valerio Schiavoni, and Miguel Matos. 2020. Kollaps: decentralized
and dynamic topology emulation. In 15th European Conf. on Computer
Systems. ACM, New York, NY, 1‚Äì16.

[15] Nikhil Handigol, Brandon Heller, Vimalkumar Jeyakumar, Bob Lantz,
and Nick McKeown. 2012. Reproducible network experiments using
container-based emulation. In 8th Int. Conf. on Emerging networking
experiments and technologies. ACM, New York, NY, 253‚Äì264.

[16] Rob Jansen and Nicholas Hopper. 2012. Shadow: Running Tor in a Box
for Accurate and Efficient Experimentation. In 19th Annu. NDSS. The
Internet Society.

[17] Rob Jansen, Jim Newsome, and Ryan Wails. 2022. Co-opting Linux
Processes for High-Performance Network Simulation. In USENIX ATC
22. USENIX Association, Berkeley, CA, USA, 327‚Äì350.

[18] Ramakrishna Kotla, Lorenzo Alvisi, Mike Dahlin, Allen Clement, and
Edmund Wong. 2007. Zyzzyva: speculative Byzantine fault tolerance.
In the 21st ACM SIGOPS SOSP. ACM, New York, NY, 45‚Äì58.

[19] Bob Lantz, Brandon Heller, and Nick McKeown. 2010. A network in a
laptop: rapid prototyping for software-defined networks. In 9th ACM
SIGCOMM Workshop on Hot Topics in Networks. ACM, New York, NY,
1‚Äì6.

[20] Peilun Li, Guosai Wang, Xiaoqi Chen, Fan Long, and Wei Xu. 2020.
Gosig: a scalable and high-performance byzantine consensus for con-
sortium blockchains. In 11th ACM Symp. on Cloud Computing. ACM,
New York, NY, 223‚Äì237.

[21] Boon Thau Loo, Tyson Condie, Joseph M Hellerstein, Petros Mani-
atis, Timothy Roscoe, and Ion Stoica. 2005. Implementing declarative
overlays. In 12th ACM SOSP. ACM, New York, NY, 75‚Äì90.

[22] Andrew Miller and Rob Jansen. 2015. Shadow-Bitcoin: Scalable Sim-
ulation via Direct Execution of Multi-Threaded Applications. In 8th
Workshop on Cyber Security Experimentation and Test. USENIX Associ-
ation, Berkeley, CA, USA.

[23] Satoshi Nakamoto. 2008. Bitcoin: A peer-to-peer electronic cash system.

https://bitcoin.org/bitcoin.pdf

[24] Ray Neiheiser, Miguel Matos, and Lu√≠s Rodrigues. 2021. Kauri: Scalable
bft consensus with pipelined tree-based dissemination and aggregation.
In ACM SIGOPS 28th SOSP. ACM, New York, NY, 35‚Äì48.

[25] Martin Nischwitz, Marko Esche, and Florian Tschorsch. 2021. Bernoulli
meets pbft: Modeling bft protocols in the presence of dynamic failures.
In 16th Conference on Computer Science and Intelligence Systems. IEEE
Comp. Soc., Washington, DC, USA, 291‚Äì300.

[26] George F Riley and Thomas R Henderson. 2010. The ns-3 network
In Modeling and tools for network simulation. Springer,

simulator.
Cham, 15‚Äì34.

[27] Signe R√ºsch, Kai Bleeke, and R√ºdiger Kapitza. 2019. Themis: An Effi-
cient and Memory-Safe BFT Framework in Rust: Research Statement.
In 3rd Workshop on Scalable and Resilient Infrastructures for Distributed
Ledgers. ACM, New York, NY, 9‚Äì10.

[28] F√°bio Silva, Ana Alonso, Jos√© Pereira, and Rui Oliveira. 2020. A com-
parison of message exchange patterns in BFT protocols. In IFIP Int.
Conf. on Distributed Applications and Interoperable Systems. Springer,
104‚Äì120.

[29] Atul Singh, Tathagata Das, Petros Maniatis, Peter Druschel, and Timo-
thy Roscoe. 2008. BFT Protocols Under Fire.. In NSDI, Vol. 8. USENIX
Association, Berkeley, CA, USA, 189‚Äì204.

[30] Jo√£o Sousa and Alysson Bessani. 2015. Separating the WHEAT from
the Chaff: An Empirical Design for Geo-Replicated State Machines.
In 34th IEEE Symp. on Reliable Distributed Systems. IEEE Comp. Soc.,
Washington, DC, USA, 146‚Äì155.

[31] Chrysoula Stathakopoulou, Tudor David, and Marko Vukoliƒá. 2019.
arXiv preprint

Mir-bft: High-throughput BFT for blockchains.
arXiv:1906.05552 (2019).

[32] Marko Vukoliƒá. 2015. The quest for scalable blockchain fabric: Proof-
of-work vs. BFT replication. In Int. Workshop on Open Problems in
Network Security. Springer, Cham, 112‚Äì125.

[33] Bozhi Wang, Shiping Chen, Lina Yao, and Qin Wang. 2020. ChainSim:
A P2P Blockchain Simulation Framework. In CCF China Blockchain
Conf. Springer, Singapore, 1‚Äì16.

[34] Ping-Lun Wang, Tzu-Wei Chao, Chia-Chien Wu, and Hsu-Chun Hsiao.
2022. Tool: An Efficient and Flexible Simulator for Byzantine Fault-
Tolerant Protocols. In 52th Annu. IEEE/IFIP Int. Conf. on Dependable
Systems and Networks. IEEE Comp. Soc., Washington, DC, USA, 287‚Äì
294.

[35] Philip Wette, Martin Dr√§xler, Arne Schwabe, Felix Wallaschek, Mo-
hammad Hassan Zahraee, and Holger Karl. 2014. Maxinet: Distributed
emulation of software-defined networks. In IFIP Networking Conf. IEEE
Comp. Soc., Washington, DC, USA, 1‚Äì9.

[36] Maofan Yin, Dahlia Malkhi, Michael K Reiter, Guy Golan Gueta, and
Ittai Abraham. 2018. HotStuff: BFT consensus in the lens of blockchain.
arXiv preprint arXiv:1803.05069 (2018).

Appendix
In this section, we first define the properties that we used
to create our comparison of evaluation tools (Sect. A). Fur-
ther, we explain how to (B) reproduce this paper‚Äôs simulation
results on your own machine, (C) create your own experi-
ments to run on our toolchain, and (D) connect your own
BFT protocol implementation with our toolchain by writing
a protocol connector.

A Properties of an Ideal Simulator for BFT Research
In the following, we briefly explain a set of distilled proper-
ties that we employed to create our comparison:

‚Ä¢ Realism: The simulator allows us to reason about the
real protocol behavior fairly enough. We can distinguish
this further into the characteristics of realistic networking
and application layer realism (which means the application
model of a BFT protocol matches its implementation).
‚Ä¢ Scalability: The simulator can handle up to the magnitude
of over 103 of nodes executing the BFT protocol and can
also handle the geographic dispersion of nodes, i.e., by
maintaining a large network topology.

‚Ä¢ Resource friendliness: To conduct experiments, it is not

necessary to have many physical machines at hand.

7

‚Ä¢ Reproducibility: Repeated runs give similar results (or
even the same results in the case of simulation runs which
can be deterministic).

‚Ä¢ Experimental controllability: It is simple to study iso-
latable factors, e.g., controlling the environment, or pa-
rameters of the protocol.

‚Ä¢ Fault induction / Byzantine attacker: The simulator
provides support to induce faults, for instance, dropping
messages, crashing nodes or more complex, in particular
malicious attacking behavior orchestrated by an attacker.

The first five properties are favorable for any simulations
of distributed systems. If models use re-implementation or
if simplifications are used, then application layer realism is
hard to achieve. This is because BFT protocols are generally
difficult to implement, and a re-implementation (that may
even simplify the protocol) can easily induce bugs (a fact
that is also stressed in BFTSim [29]). To counteract this, BFT-
Simulator compares execution traces of real deployments
with traces from the simulation for validation (the authors
are aware that this gives no strict guarantees for correct-
ness) [34].

The last property deserves explanation: It seems desirable
to also evaluate BFT protocols in an adverse environment,
such as when a portion of nodes becomes faulty. To our
best knowledge, BFTSim only supports benign faults (i.e.,
faulty replicas staying silent [29]), while the BFT Simulator
from Wang et al. [34] also supports some more sophisticated
attacks (such as partition, adaptive and rushing attacks [34]).
The generic simulators and emulators which were not crafted
for BFT research do not consider a global Byzantine attacker.
Reproducibility and experimental controllability are im-
portant but seem to be provided by most if not all simulators
and emulators so these properties are not used in our com-
parison.

B Reproduce our Results
Our evaluation results can be reproduced. First, it is neces-
sary to clone our toolchain repository and follow the setup in-
structions in the README file. For best compatibility, we rec-
ommend (and currently use) Ubuntu 20.04 LTS and Shadow
v2.2 (the newest version as of time of writing) and Node
version 16.3.0. If you want to simulate specific BFT protocols
like HotStuff, Themis or BFT-SMaRt you will need to install
their dependencies, too.

A series of experiments (like the "p1024" experiment row
with increasing ùëõ) is specified in an experiments description
file, in yaml format. The structure and description is easy to
understand. For instance, an experiment for Hotstuff with
128 replicas looks like this:

protocolName: hotstuff
protocolConnectorPath: ./connectors/hotstuff.js
experiments:
- 128rep:
misc:

duration: 30 s
parallelism: 16
useShortestPath: false

network:

bandwidthUp: 10 Gibits
bandwidthDown: 10 Gibits
latency:
uniform: true
replicas: 1000 us
clients: 1000 us

replica:

replicas: 128
blockSize: 400
replySize: 1024

client:

clients: 16
numberOfHosts: 2
startTime: 0 s
outStandingPerClient: 175
requestSize: 1024

We put a folder called examples/serial22 on the reposi-
tory that contains all experiments description files that we
used for the evaluation section of this paper. We also pro-
vide all of our data sets as a reference. After following the
README, you can run simulations by typing in a shell:

npm run simulation -- examples/hotstuff/hs3-aws.yaml

This will create a data set called results.csv in your

experiments directory.

C Create your own Experiments
It is also easy to setup experiments that simulate AWS de-
ployments. Here is an example:

protocolName: bftsmart
protocolConnectorPath: ./connectors/bftsmart.js
experiments:

- 4replicasAWS:

misc:
duration: 1200 s
parallelism: 16
useShortestPath: false
network:
bandwidthUp: 1 Gbit
bandwidthDown: 1 Gbit
latency:

uniform: false
replicas: ['us-west-1': 1, 'eu-west-1': 1,
'sa-east-1':1, 'ap-southeast-2':1]
clients: ['us-west-1': 1]

replica:
replicas: 4
blockSize: 100
replicaInterval: 100
replySize: 0
stateSize: 0
context: false
replicaSig: nosig
client:
clients: 1
threadsPerClient: 1

const processName = 'my-app'; // replace with the name of

// your protocol app!

function getProcessName() { return processName; }

function getExecutionDir() {...}

function getExperimentsOutputDirectory() {...}

async function build(replicaSettings,
clientSettings,
log) {...} // Mandatory

async function configure(replicaSettings,
clientSettings,
log) {...} // Mandatory

async function getStats(log) {...} // Optional, called
// after simulation

module.exports = {build, configure, getStats,
getProcessName, getExecutionDir,
getExperimentsOutputDirectory};

Listing 1. Stubs of a BFT protocol connector.

opPerClient: 2000
requestSize: 0
startTime: 30 s
clientInterval: 0
readOnly: false
verbose: true
clientSig: nosig

The geographic distribution of replicas can be simply spec-
ified by passing a mapping of regions and the number of
replicas to be placed in the respective region such as:

[ 'us-west-1': 1, 'eu-west-1': 1, 'sa-east-1':1, ..]

We think it is easy to adapt our experiment description
files. However, some care needs to be taken. In the following,
we want to share some insights we made:

Duration: This is one of the most important parameters
because it has a big impact on resource consumption of
simulations. It is better to use short durations to keep the
overall simulation time short.

Parallelism: Make sure to set this parameter to utilize multi-

core systems and speed up simulations.

BlockSize and OutStandingRequestsPerClient: In HotStuff,
if the number of in-flight requests is too low to fill the blocks,
then HotStuff replicas just wait for more requests (which
wont arrive). In this case the system halts and the simulation
fast-forwards and terminates. It is better to overestimate the
number of inflight requests for HotStuff.

8

Moreover, we recommend reading the documentation of
parameters3 and we also recommend to use tmux to run your
simulations in the background.

D Simulate your own BFT Protocol Implementation
You can implement your own BFT protocol connector by im-
plementing the stubs build, configure, getExecutionDir
getProcessName and getExperimentsOutputDirectory as
shown in Listing 1. Your connector can optionally also im-
plement a method getStats to automatically parse logs and
put the results in a comma separated value file. Experiment
description files have to adhere to a certain format:
protocolName: #name of your protocol
protocolConnectorPath: #path to your connector
experiments: #Array describing the experiments

-exp1: #description of an experiment

misc: #miscelleanous settings

duration: #duration of the experiment
useShortestPath: #default is false
parallelism: # multi-core awareness

network:

bandwidthUp: ..
bandwidthDown: ..
latency:

uniform: (true|false)
# if true:

replicas: #inter-replica
#latency ex. 1000 us
clients: #client-replica
#latency ex. 1000 us

# else:

describing

replicas: #Array
#AWS hosts format region: host
clients: #Array
#AWS hosts format region: host
#OR: a uniform client-replica latency

describing

replica:

#This is for protocol- and replica-specific
# configs; will be passed to your connector.

client:

#This is for protocol- and client-specific
# configs; will be passed to your connector.

You may leverage the .env file to make your experiment
description files and your connectors more concise.

E Determinism in Phantom
Throughout the simulation, Phantom preserves determinism:
It employs a pseudo-random generator, which is seeded from
a configuration file to emulate all randomness needed dur-
ing simulation, in particular the emulation of getrandom or
reads of /dev/*random. Each Phantom worker only allows
a single thread of execution across all processes it manages
so that each of the remaining managed processes/threads
are idle, thus preventing concurrent access of managed pro-
cesses‚Äô memory [17].

3https://shadow.github.io/docs/guide/shadow_config_spec.html

9

