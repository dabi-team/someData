2
2
0
2

y
a
M
6
1

]
T
I
.
s
c
[

2
v
7
8
2
7
0
.
1
0
2
2
:
v
i
X
r
a

Polar Coded Merkle Tree: Improved Detection of
Data Availability Attacks in Blockchain Systems

Debarnab Mitra, Lev Tauz, and Lara Dolecek
Department of Electrical and Computer Engineering, University of California, Los Angeles, USA
email: debarnabucla@ucla.edu, levtauz@ucla.edu, dolecek@ee.ucla.edu

Abstract—Light nodes in blockchain systems are known to be
vulnerable to data availability (DA) attacks where they accept an
invalid block with unavailable portions. Previous works have used
LDPC and 2-D Reed Solomon (2D-RS) codes with Merkle Trees
to mitigate DA attacks. While these codes have demonstrated
improved performance across a variety of metrics such as DA
detection probability, they are difﬁcult to apply to blockchains
with large blocks due to generally intractable code guarantees
for large codelengths (LDPC), large decoding complexity (2D-
RS), or large coding fraud proof sizes (2D-RS). We address these
issues by proposing the novel Polar Coded Merkle Tree (PCMT)
which is a Merkle Tree built from the encoding graphs of polar
codes and a specialized polar code construction called Sampling-
Efﬁcient Freezing (SEF). We demonstrate that the PCMT with
SEF polar codes performs well in detecting DA attacks for large
block sizes.

I. INTRODUCTION
Decentralization and security properties of blockchains have
led to their applications in a wide variety of ﬁelds [1]–[7]. A
blockchain is an immutable ledger of transaction blocks. Full
nodes in a blockchain system store the entire ledger and vali-
date transactions. However, for better scalability, blockchains
also run light nodes who store the header of each block and
cannot validate transactions. Light nodes rely on honest full
nodes for fraud proofs [8] in order to reject invalid blocks.

Blockchains where light nodes are connected to a majority
of malicious full nodes are vulnerable to data availability (DA)
attacks [8], [9]. In this attack, a malicious full node (i.e., an ad-
versary) generates a block with invalid transactions and hides
the invalid portion of the block from other nodes. This action
prevents honest nodes from sending fraud proofs to the light
nodes. Light nodes, in this scenario, randomly request/sample
chunks of the block from the block generator and detect a DA
attack if any request is rejected. To improve the detection of
a DA attack by light nodes, erasure coding has been proposed
to encode the block [8] which forces the adversary to hide a
larger fraction of the encoded block. However, erasure coding
allows the adversary to carry out an incorrect-coding (IC)
attack by incorrectly generating the coded block, in which
case honest full nodes can send an IC proof to the light nodes
to reject the block [8], [9]. Recently, [10] proposed a technique
to mitigate DA attacks without requiring IC-proofs, however
[10] employs complex cryptographic computations. We remark
that channel coding has been considered to mitigate a variety
of issues in blockchains [11]–[25].

The ﬁrst paper to address DA attacks via coding used 2D
Reed-Solomon (RS) codes to encode each block [8]. Their
method was recently optimized in [26]. While offering a high
probability of detecting DA attacks, 2D-RS codes result in
large IC proof sizes and decoding complexity. Large IC-proof

sizes can be exploited by the adversary to congest the network
and cause issues such as denial of service attacks, reduced
transaction throughput, etc.. To improve upon 2D-RS codes,
authors in [9] proposed the Coded Merkle Tree (CMT): a
Merkle tree [1] where each layer is encoded using a Low-
Density Parity-Check (LDPC) code. LDPC codes reduce the
IC proof size compared to 2D-RS codes due to their sparse
parity check equations. LDPC codes also allow the use of a
low complexity peeling decoder [27] for decoding the CMT
where the probability of failure to detect DA attacks depends
on the minimum stopping set size of the LDPC code [9].

Application of coding has to be carefully considered de-
pending on the size of the transaction blocks in blockchains
which can range from a few MBs (small block size), e.g.,
Bitcoin [28], Bitcoin Cash [29], to hundreds of MBs (large
block size), e.g., Bitcoin SV [30]. For large block sizes, large
code lengths are required since large code lengths allow for
smaller partitioning of the block, thereby reducing the net-
work bandwidth requirement. In regard to the CMT, previous
work in [9] has considered random LDPC codes for large
code lengths. However, due to the random construction, the
approach has a non-negligible probability of generating bad
codes [9] which undermines the security of the system. At the
same time, works in [11], [12] have provided deterministic
LDPC codes with short code lengths based on the PEG algo-
rithm [31] that result in a low probability of failure. However,
the NP-hardness of determining the minimum stopping set
size of LDPC codes [32] makes it difﬁcult to provide an
efﬁciently computable guarantee on the probability of failure
for large code lengths. As such, designing a CMT with LDPC
codes at large code lengths/block sizes is difﬁcult. To mitigate
these issues, we propose Polar Coded Merkle Tree (PCMT):
a CMT built using the encoding graph of polar codes [33]
(we refer to a CMT built using LDPC codes as an LCMT).
Although polar codes have dense parity check matrices [34],
they have sparse encoding graphs, which result in a small
IC proof size in the PCMT. We provide a specialized polar
code construction called Sampling-Efﬁcient Freezing (SEF)
Algorithm that i) provides a low probability of failure to detect
DA attacks; ii) allows ﬂexibility in designing polar codes of
any lengths. We demonstrate that SEF Polar codes have an
efﬁciently computable guarantee on the probability of failure
which simpliﬁes system design at large block sizes. We then
demonstrate that for large block sizes, a PCMT built using SEF
Polar codes result in a lower probability of failure compared
to LDPC codes designed by the PEG algorithm.

The rest of this paper is organized as follows. In Section II,

 
 
 
 
 
 
we provide the preliminaries and the system model. In Section
III, we provide our construction method for the PCMT. We
present the SEF algorithm in Section IV. Finally, we provide
simulation results and performance comparisons in Section V.
II. PRELIMINARIES AND SYSTEM MODEL
(cid:21)
(cid:20)1 0
1 1

Notation: Let F2 =

. For a vector a,

and T2 =

(cid:21)
(cid:20)1
2

let a(i) and min(a; k) denote the ith element and kth smallest
value of a, respectively. Let ⊗n denote the nth Kronecker
power. Let |S| be the cardinality of set S. All logarithms are
with base 2. For integers a and b deﬁne [a, b] = {i | a ≤ i ≤
b}, (a, b] = {i | a < i ≤ b}, and [a] = {i | 1 ≤ i ≤ a}, where
elements in the three sets are integers.

1) Coded Merke Tree preliminaries: A CMT, like a regular
Merkle Tree [1], is a cryptographic commitment generator that
is used to check the integrity of transactions in the block
[9]. Additionally, erasure coding allows to check for data
availability via random sampling. In this section, we provide
a general framework for the CMT construction that captures
its key properties. Later in Section III, we present the PCMT
within the general CMT framework.

Nl

A CMT parametrized by T = (k, R, q, l) is a coded version
of a Merkle tree [1]. It has (l + 1) layers L0, L1, . . . , Ll
is the base layer and L0 is the CMT root. Lj
where Ll
(qR)l−j coded symbols, where Nl = k
has Nj =
R . Let
CodeSym[j] = {Cj[i] | i ∈ [Nj]} be the coded symbols of
Lj, where data[j] = {Cj[i] | i ∈ [RNj]} and parity[j] =
{Cj[i] | i ∈ (RNj, Nj]} are the set of data and parity symbols
of Lj, respectively. The coded symbols in the CMT are formed
as follows: Set the data symbols of the base layer data[l] to
the chunks of the transaction block. For j = l, l − 1, . . . , 1:
a) form the parity symbols parity[j] from the data symbols
data[j] using a rate R systematic linear code via a procedure
parity[j] = encodeParity(data[j]); b) form the data
symbols data[j − 1] from the coded symbols CodeSym[j]
by a procedure data[j − 1] = formParent(CodeSym[j]).
The formParent() procedure has the property that each
data symbol Cj−1[i] in data[j − 1] contains the hashes of
q coded symbols of CodeSym[j]. Finally, data[0] forms the
root Root of the CMT and is the commitment to the block.
Each coded symbol τ in the CMT has a Merkle proof
Proof(τ ) which can be used to check the integrity of
the symbol given the Root using Verify-Inclusion(τ ,
Proof(τ ), Root). The CMT is decoded using a hash-aware
decoder which decodes the tree from the root to the base layer.
Each layer is decoded using a procedure decodeLayer(Lj)
and the hash of the decoded symbols are matched with their
hash provided in the parent layer of the CMT. Assume that the
decoded CMT symbols τ1, τ2, . . . , τd satisfy a degree d parity
check equation (of the erasure code used for encoding). Of
these symbols, if there exists a symbol τe whose hash does not
match with the hash provided by the parent of τe in the CMT,
an IC attack is detected. In this case, an IC proof consists of the
following: the symbols {τ1, τ2, . . . , τd} \ τe and their Merkle
proofs, and the Merkle proof of τe. The IC proof is veriﬁed by
ﬁrst verifying that each symbol τi, 1 ≤ i ≤ d, i (cid:54)= e, satisﬁes

Fig. 1: Left panel: FG G8 where circles represent VNs and squares represent
CNs. The black (red) VNs and CNs represent a stopping set (stopping tree);
Right panel: G5 obtained by removing the VNs from the last 3 rows of G8.
(removed VNs are shown in low opacity).
Verify-Inclusion(τi, Proof(τi), Root), then decoding
τe from the remaining symbols (as they form a parity check
equation) and then checking that τe does not satisfy Verify-
Inclusion(τe, Proof(τe), Root).

We consider a blockchain system similar to [9], [12] with
full nodes and light nodes where full nodes produce new
blocks. Light nodes only store the CMT root of each block
and use it to verify the Merkle proof of CMT symbols and
IC proofs. Similar to [8], [9], [11], [12], we assume that light
nodes are honest, are connected to at least one honest full node,
but can be connected to a majority of malicious full nodes.
For the purposes of a DA attack, consider one layer of the
CMT having N coded symbols. A malicious full node causes
a DA attack by a) generating an invalid block and producing its
CMT; b) hiding coded symbols (of the N coded symbols) such
that no honest full node is able to decode back all the coded
symbols. Light nodes detect this DA attack by anonymously
and randomly requesting (sampling) a small number of coded
symbols from the block producer and accepting the block if
all the requested samples are returned. A malicious node only
returns coded symbols that it has not hidden [8], [9], [11],
[12]. Let αmin, which we call the undecodable threshold, be
the minimum number of coded symbols that a malicious node
must hide to prevent honest full nodes from decoding all the
coded symbols. Then, the probability of failure for a light
node to detect a DA attack using s random i.i.d. samples is
Pf (s) = (1 − αmin
N )s. Note that we focus on the security
of the system on a per client basis similar to [8], [9]. The
following metrics are of importance for a CMT: i) IC proof
size (must be small in comparison to the original block size
since this proof is communicated to all light nodes and can be
used to congest the network), ii) undecodable threshold, iii)
complexity of computing the undecodable threshold (which
is important at large CMT code length N ), and iv) decoding
complexity. In this paper, we will demonstrate a construction
of CMT using polar codes called the PCMT that performs well
on all these metrics when the size of the block b is large.

2) Polar Codes preliminaries: An ( (cid:98)N , (cid:98)K) polar code of
codelength (cid:98)N = 2n for some integer n and information length
(cid:98)K is deﬁned by a transformation matrix F2n = F⊗n
2 . The
generator matrix of the polar code is a submatrix of F2n
having (cid:98)K of its rows corresponding to the data (information)
symbols, while the rest of the rows correspond to frozen
symbols (zero chunks in this paper). The factor graph (FG)

For the PCMT, deﬁne intermediate coded symbols (which
are used to form the PCMT) C j[k][i], j ∈ [l] where k ∈
[log Nj + 1], i ∈ [Nj]. Index k (i) is the column (row)
number of the VN that the symbol C j[k][i] corresponds to,
in the FG GNj . In the general CMT framework, for j ∈ [l],
we have data[j] = {C j[log Nj + 1][i]
i ∈ [RNj]},
parity[j] = {C j[log Nj + 1][i]
| i ∈ (RNj, Nj]} and
CodeSym[j] = data[j] ∪ parity[j].

|

1) Formation of PCMT symbols: The encodeParity()
procedure for a PCMT is as follows: for the data symbols
data[j], use a systematic polar encoder as described in
Section II to ﬁnd the parity symbols parity[j], where VNs
corresponding to frozen[j] = {C j[1][i] | i ∈ (RNj, Nj]}
in GNj are set as zero chunks. The systematic encoder also
provides the set of symbols dropped[j] = {C j[k][i] | k ∈
[log Nj], i ∈ [Nj]} which are dropped from the PCMT and
are not included in CodeSym[j]. However, before dropping,
we use them to form the parent layer in the PCMT. The
formParent() procedure for a PCMT is as follows. Let
x mod p := (x)p and let Hash and concat represent the
hash and string concatenation functions, respectively. We have

C j−1[log Nj−1 + 1][i]

(1)

= concat({Hash(C j[k][x]) | k ∈ [log Nj + 1],
x ∈ [Nj], i = 1 + (x − 1)RNj−1}), ∀i ∈ [RNj−1],

where data[j − 1] = {C j−1[log Nj−1 + 1][i] | i ∈ [RNj−1]}.
For a PCMT, the root (data[0]) has a size t = N1(log N1 +1)
hashes. In Fig. 2, the formation of C 1[3][2] is shown.

In the above formParent() procedure, data symbols in
data[j − 1] are formed by taking the hashes of all
the
Nj(log Nj + 1) intermediate coded symbols of layer j (i.e.,
dropped[j] ∪ CodeSym[j]) and concatenating q(log Nj + 1)
hashes together according to Eqn. (1). The intuition behind
taking the hashes of all the intermediate coded symbols is so
that the symbols in dropped[j] also get committed to the
root (i.e., these symbols also have a Merkle proof). Although
dropped, the symbols in dropped[j] can be decoded back by
a peeling decoder using the available (non-erased) symbols of
CodeSym[j]. Once decoded, they can be used to build small
IC proofs using the degree 2 and 3 CNs in the polar FG GNj .
2) Merkle proof of PCMT symbols: For the above PCMT
construction, symbols in CodeSym[j] and dropped[j] have
Merkle proofs. The Merkle proof of the symbols C j[k][i], k ∈
[log Nj + 1], i ∈ [Nj] consists of a data symbol and parity
symbol from each layer of the PCMT above Lj similar to
LCMT in [12], [24]. Precisely, it is given by the following:
Proof(C j[k][i]) = {C j(cid:48)[log Nj(cid:48) + 1][1 + (i − 1)RNj(cid:48) ], (2)
C j(cid:48)[log Nj(cid:48) + 1][1 + RNj(cid:48) + (i − 1)(1−R)Nj(cid:48) ] | j(cid:48) ∈ [j − 1]}.
The Merkle proof for C 2[4][4] is shown in Fig. 2. The data
symbols from each layer in Proof(C j[k][i]) lie on the path
of C j[k][i] to the PCMT root; this path is used to check the
integrity of C j[k][i] in a manner similar to an LCMT [9], [24].
3) Hash-aware peeling decoder and IC proofs: The PCMT
is decoded using a hash-aware peeling decoder similar to
LCMT in [9]. The decodeLayer(Lj) procedure for the

Fig. 2: PCMT T = (k = 4, R = 0.5, q = 4, l = 2). In the PCMT, the
coded symbols corresponding to all the columns of the polar FG are hashed
into the parent layer. The dropped symbols are shown in dotted. The symbols
in L2 are colored according to the column they belong to in FG G8. The
circled symbols in L1 are the Merkle proof of the red symbols in L2. The
data (parity) symbols in the Merkle proofs are shown in solid (dashed) circles.
representation [37] of F23 is shown in Fig. 1 left panel. In
general, the FG of F2n (denoted by G2n ) has n + 1 variable
node (VN) and n check node (CN) columns. Let vki (cki)
denote the VN (CN) in the kth column and ith row as shown
in Fig. 1. Note that the CNs have a degree of either 2 or 3.

Systematic encoding of ( (cid:98)N , (cid:98)K) polar codes [35], [36]
(required for CMT construction), can be performed using the
method described in [36]. Given information and frozen index
sets (cid:98)A ⊂ [ (cid:98)N ] and (cid:98)F = [ (cid:98)N ] \ (cid:98)A, such that | (cid:98)A| = (cid:98)K, the
systematic encoder of [36] determines the value of all the
|i ∈ (cid:98)F} are set to
VNs in the FG G
zero (frozen) symbols; ii) VNs in {v(n+1)i
| i ∈ (cid:98)A} and
{v(n+1)i | i ∈ (cid:98)F} are the provided data symbols and resultant
parity symbols in the systematic encoding. Details regarding
systematic encoding can be found in Appendix D.

(cid:98)N such that i) {v1i

A polar code can be decoded using a peeling decoder on
the code FG. Similar to LDPC codes, the peeling decoder on
the FG of a polar code fails if all VNs corresponding to a
stopping set (of the FG) are erased. A stopping set is a set of
VNs such that every CN connected to this set is connected to
at least two VNs in the set. Similar to [37], we call the VNs of
a stopping set ψ that are in the rightmost column of the FG as
its leaf set denoted by Leaf-Set(ψ). An important category
of stopping sets in the FG of polar codes is called stopping
trees [37]. A stopping tree is a stopping set that contains only
one VN from the leftmost column of the FG (called its root).
Fig. 1 left panel shows a general stopping set and a stopping
tree in the FG G8. As demonstrated in [37], each VN v1i,
i ∈ [ (cid:98)N ], is root of an unique stopping tree. Let ST 2n
be the
unique stopping tree with root VN v1i in the FG G2n and let
f 2n
i = |Leaf-Set(ST 2n
)|. It is easy to see (and also proved
in [37]) that f 2n

i = T2n (i), ∀i ∈ [ (cid:98)N ], where T2n = T⊗n
2 .

i

i

III. POLAR CODED MERKLE TREE (PCMT)

In this section, we describe the construction of a PCMT
under the general CMT framework of Section II. Assume
all Nj’s are powers of 2. In Section IV, we remove this
assumption. Let Aj (Fj) be the information (frozen) index
sets of the polar code used in layer j of the PCMT. We have
|Aj| = RNj and |Fj| = (1 − R)Nj. For convenience, we
re-index the row indices in FG Gj such that Aj and Fj are
the indices [1, RNj] and (RNj, Nj], respectively.

decoder is as follows. Its acts on the FG GNj . It takes as inputs
the frozen symbols frozen[j] and the non hidden symbols
of CodeSym[j]. Using a peeling decoder, it ﬁnds all symbols
in dropped[j] ∪ CodeSym[j]. The hash of every decoded
(peeled) symbol is matched with its hash provided by the
parent layer Lj−1. If hashes do not match, an IC attack is
detected. In this case, IC proof is generated using the degree
2 or 3 CN of the FG GNj as per the general CMT framework.
A. DA attacks on PCMT

Consider layer Lj, j ∈ [l], of the PCMT. For a given
information index set Aj, let ΨAj denote the set of all stopping
sets in the FG GNj that do not have any VNs corresponding to
frozen[j]. The hash-aware peeling decoder fails to decode
Lj if coded symbols corresponding to a stopping set in ΨAj
are erased. Since all the coded symbols except the rightmost
column of GNj are dropped, the peeling decoder will fail if
the adversary hides the leaf set of a stopping set in ΨAj .

To prevent a DA attack, light nodes randomly sample sym-
bols from the PCMT base layer, i.e., CodeSym[l]. Randomly
sampling the base layer ensures that the non-dropped symbols
of intermediate layer Lj, j ∈ [l − 1], i.e., CodeSym[j], are
also randomly sampled via the Merkle proofs of the base layer
samples similar to an LCMT in [9]. For subsequent analysis,
we assume (WLOG) that the adversary conducts a DA attack
on the base layer of the PCMT. To ﬁnd the adversary strategy
that leads to the largest probability of failure when the light
nodes use random sampling, we use the following important
property of stopping sets in polar FGs that was proved in [37]:
|Leaf-Set(ψ)| = min
(3)
i∈Al

min
ψ∈ΨAl

f Nl
i

.

Eqn. (3) implies that, when light nodes use random sam-
pling, the best strategy for the adversary (to maximize the
probability of failure) is to hide the smallest leaf set amongst
f Nl
all stopping trees with non frozen root. Thus, αmin = min
.
i
i∈Al
IV. SAMPLING-EFFICIENT FREEZING ALGORITHM

i

For the best adversary strategy, αmin = mini∈Al f Nl

. Based
on this result, a naïve frozen set selection method would be to
select the indices of RNl VNs from the leftmost column of the
FG Gl with the smallest stopping tree leaf set sizes f Nl
. Note
that for this naïve frozen set selection, the polar code becomes
equivalent to a Reed-Muller (RM) Code [38]. We call the naïve
frozen set selection as Naïve-RM (NRM) algorithm for which
it can be easily shown that αN RM
min = min (TNl ; (1 − R)Nl)
(which is the (1 − R)Nl-th smallest value of TNl ).

i

Next, we describe the Sampling-Efﬁcient Freezing (SEF)
algorithm and show that it results in a higher effective unde-
codable threshold compared to the NRM algorithm. Addition-
ally, our algorithm allows for polar codes of any length and
are not limited to powers of two. Assume that in this section,
for all FG G
(cid:98)N are indexed 1 to (cid:98)N from top to
bottom. The SEF algorithm is based on the following lemma.
(cid:98)N where (cid:98)N is a power of two.
Lemma 1. Consider FG G
Let (cid:98)F and (cid:98)A be the frozen and information index sets. For
a parameter µ, deﬁne the set of VNs V µ
[k] = {vki | i ∈
(cid:98)N
[ (cid:98)N − µ + 1, (cid:98)N ]}. If [ (cid:98)N − µ + 1, (cid:98)N ] ⊂ (cid:98)F, then i) ∀ ψ ∈ Ψ (cid:98)A,

(cid:98)N , the rows in G

Algorithm 1 SEF Algorithm

(cid:98)N , µ1 = (cid:98)N − N , i = N .
[k] | k ∈
(cid:98)N (also

1: Inputs: N , K Output: GN , F, TN
2: Initialize: (cid:98)N = 2(cid:100)log N (cid:101), FG G
3: GN = FG obtained by removing all VNs in {V µ1
(cid:98)N
[log (cid:98)N + 1]} and their connected edges from G
remove any CNs that have no connected edges)
(cid:98)N with last µ1 entries removed
4: TN = T
5: F = {e | e ∈ [N ], TN (e) < min(TN ; N − K)}
6: while |F| < N − K do
7:
ψ does not have any VNs in V µ
(cid:98)N
{V µ
(cid:98)N
Proof Idea. Assuming that a stopping set ψ ∈ Ψ (cid:98)A has a VN
from V µ
[log (cid:98)N + 1], we prove, by incorporating the deﬁnition
(cid:98)N
of stopping sets, that ∃ i, i ∈ [ (cid:98)N −µ+1, (cid:98)N ] such that v1i ∈ ψ.
This is a contradiction since ψ ∈ Ψ (cid:98)A and hence does not have
VNs in V µ
(cid:98)N

[1]. Full proof can be found in Appendix A.

[k] | k ∈ [log (cid:98)N + 1]} are zero chunks.

if i (cid:54)∈ F then F = F ∪ i end if; i = i − 1

[log (cid:98)N + 1]; ii) all VNs in

Lemma 1 states that, if the last µ rows (from the bottom) in
the leftmost column of G
(cid:98)N are all frozen, then no stopping set
in Ψ (cid:98)A can have a VN from the last µ rows in the rightmost
column of G
(cid:98)N . Thus, for a frozen index set (cid:98)F such that [ (cid:98)N −
µ + 1, (cid:98)N ] ⊂ (cid:98)F, the light nodes do not need to sample the
VNs in V µ
[log (cid:98)N + 1]. We leverage the above property to
(cid:98)N
improve the effective undecodable threshold of polar codes.
Additionally, since all the VNs in {V µ
[k] | k ∈ [log (cid:98)N + 1]},
(cid:98)N
which are all the VNs in the last µ rows of G
(cid:98)N , are zero
chunks, these VNs and their associated edges can be removed
from the FG. After this removal, we get the FG G
(cid:98)N −µ of a
polar code of length (cid:98)N − µ. We use this property to design
polar codes of lengths that are not powers of two. An example
of FG G5 is shown in Fig. 1 right panel. Algorithm 1 provides
the SEF algorithm to design the frozen index set F of an
(N, K) polar code; N is not necessarily a power of two.

In the SEF algorithm, we ﬁrst remove the VNs from the
last µ1 = (cid:98)N − N rows in FG G
(cid:98)N (step 3). This gives us
the FG GN which has N coded symbols and is used for the
construction of the PCMT. Then, in steps 4-7, we select the
frozen index set F of size N − K for the (N, K) polar code.
Note that TN (step 4) stores the stopping tree sizes of the
VNs v1i, i ∈ [N ]. For the selection of F, we ﬁrst select all
the indices e in [N ] whose corresponding VNs v1e have their
stopping tree sizes less than min(TN ; N − K) (step 5). Then,
the remaining indices in F are selected as the VN indices from
the bottom row of FG GN that are not already present in F
(steps 6-7). We have the following lemma.
Lemma 2. Let µ2 be the largest µ such that [N − µ + 1, N ] ⊂
F and let A = [N ]\F. For an (N, K) polar code produced by
the SEF algorithm, let the light nodes randomly sample among
the top N − µ2 VNs from the rightmost column of FG GN . For
this sampling strategy, the effective undecodable threshold is
min = mini∈A TN (i)∗N
αSEF
Proof Idea. We prove the lemma by using Lemma 1 and Eqn.
3. The full proof can be found in Appendix B.

. As such, Pf (s) =

1 − αSEF

N −µ2

min
N

(cid:17)s

(cid:16)

.

e
z
i
s

k
c
o
l
b

/

e
z
i
s

f
o
o
r
p

C

I

)
s
(
f
P

)
s
(
f
P

Block size b (MB)

Block size b (MB)

Block size b (MB)

)

0.54

80.10

PCMT

2D-RS

T2
2.56

T1
2.05

T2
5.82

O(N 1.5
l

3 and b

Root size (KB)

IC proof size (MB)

-
O(Nl)

Decoding complexity

5.80 16.40 1.54 1.54 0.53

33.80
O(Nl(cid:100)log Nl(cid:101))

Total sample download size (MB) 10.76 12.30 43.01

LCMT
T1
T1
T2
0.26 0.51 1.02

Fig. 3: All ﬁgures use b = cRNl. Left panel: Comparison of IC proof size normalized by block size b for different data symbol size c = b
k . We use
(R, q, l) = (0.5, 4, 4) in the ﬁgure. For IC proof size of LCMT, we use the maximum CN degree dc. For PCMT, the maximum CN degree dp = 3; Middle
and Right panel: Pf (s) vs. blocksize b for LCMT and PCMT. The two panels use (R, q, l) = (0.4, 5, 4), (0.5, 4, 4), and (0.75, 4, 3) and a constant data
symbol size c. Sample size s for PCMT and LCMT are selected such that total sample download size is b
5 for the middle and right panels, respectively.
In Fig. 3 left panel, we plot the IC proof size vs. block size
b for an LCMT and a PCMT and different data symbol sizes c.
We see that for c = 256 and 16KB (large block sizes), the IC
proof size is smaller for a PCMT compared to an LCMT and
gets bigger than an LCMT for c = 1KB (small block sizes).
Remark 1. We note that for a PCMT and an LCMT with the
same CMT T parameters, the PCMT incurs an asymptotic
penalty of O(log b) in the IC-proof size over the LCMT due
to collecting the hashes of VNs in all the columns of the FG
(can be seen from the expressions in Appendix C). However for
practical block sizes of interest, the IC-proof size of a PCMT
can be signiﬁcantly lower compared to an LCMT, as shown
in Fig. 3, due to the low CN degree in the FG of polar codes.
In Fig. 3 middle and right panels, we compare the proba-
bility of failure Pf (s) to detect a DA attack conducted on the
base layer. We compare Pf (s) for large and short block sizes in
the middle and right panels, respectively. From Fig. 3, we see
that PCMT has a worse probability of failure compared to an
LCMT for small block sizes. However for large block sizes,
PCMT always has a lower probability of failure compared
to an LCMT across all rates R and block sizes b thanks to
a higher undecodable ratio for the SEF Polar codes and a
negligible penalty in the single sample download size.

Note

that due

to step 5 of

TABLE I: Comparison of various performance metrics for 2D-RS, LCMT
and PCMT. The table uses T1 = (k, R, q, l) = (512, 0.5, 4, 8), T2 =
(4096, 0.5, 4, 10), c = 256KB, and b = cRNl. Sample download size is
calculated such that Pf (s) is 0.01. Due to complexity of ﬁnding αmin for
LCMT, we do have a corresponding total sample download size value for T2.
the SEF algorithm,
mini∈A TN (i) ≥ min(TN ; N − K). Thus, the undecodable
ratio of the SEF algorithm is always as big as the NRM
algorithm. The FG GN output by the SEF algorithm is used for
constructing different layers of the PCMT. GN has (cid:100)log N (cid:101)+1
VN and (cid:100)log N (cid:101) CN columns each with N VNs or CNs. Thus,
for the PCMT construction described in Section III, we replace
all instances of log Nj with (cid:100)log Nj(cid:101) (where Nj’s need not be
powers of two). Rest of the construction remains the same.
V. SIMULATIONS AND PERFORMANCE COMPARISON
In this section, we demonstrate the beneﬁts of a PCMT
with respect to the CMT metrics i)-iv) described in Section II
when the size of the block b is large. We also compare the
performance of a PCMT with an LCMT and 2D-RS codes.
We denote the output size of the Hash function as y and
the size of the data chunk (symbol) in the base layer as c
where block size b = ck = cRNl. We use y = 256 bits.
All PCMTs are built using SEF Polar codes. All LCMTs are
built using LDPC codes constructed using the PEG algorithm
[31] where the degree of all VNs is set to 3. For the PEG
LDPC codes, the undecodable threshold αmin is evaluated by
solving an Integer Linear Program (ILP) as described in [39]
and is computationally infeasible for larger code lengths. In
contrast, SEF Polar codes have an easily computable αmin
using Lemma 2. Due to complexity issues, we compute αmin
for LDPC codes up to a feasible code length (and, thus, up
to a feasible block size). From the description of the PCMT
provided in Section III, derivation of its root size, IC proof
size, and single sample download size (base layer sample and
associated Merkle proof) is straight forward and is provided
in Appendix C (which we use to generate the plots in Fig. 3).

In Table I, we provide additional comparison of various
performance metrics for 2D-RS, LCMT, and PCMT. We can
see that PCMT outperforms LCMT w.r.t. IC-proof size and
total sample download size with small increase in root size and
decoding complexity. While PCMT has a O((cid:100)log Nl(cid:101)) factor
greater decoding complexity than an LCMT, the decoding
complexity is smaller than for 2D-RS codes which is O(N 1.5
)
[9]. At the same time, PCMT also has a lower IC proof size
and root size compared to 2D-RS codes while having a higher
sample download size. Going from T1 to T2, the IC-proof
size for 2D-RS codes increases 3 fold while the IC-proof size
remains almost constant for LCMT and PCMT. Note that for
2D-RS codes, the IC proof size, decoding complexity, and
header size do not scale well as the block size increases [9].
Overall, when the size of the transaction block b is large, a
PCMT built using SEF Polar codes has good performance w.r.t
metrics i)-iv) described in Section II and offers a new trade-off
in these metrics compared to LCMT and 2D-RS codes.

l

REFERENCES

[1] S. Nakamato, “Bitcoin: A peer to peer electronic cash system," 2008.

[Online] Available: https://bitcoin.org/bitcoin.pdf.

[2] G. Wood, “Ethereum: A secure decentralised generalised transaction

ledger," Ethereum project yellow paper, Apr. 2014.

[3] K. Salah, M. H. U. Rehman, N. Nizamuddin and A. Al-Fuqaha,
“Blockchain for AI: Review and open research challenges," IEEE Access,
vol. 7, pp. 10127-10149, 2019.

[4] K. Huang, X. Zhang, Y. Mu, F. Rezaeibagha, X. Du and N. Guizani,
“Achieving intelligent trust-layer for internet-of-things via self-redactable
blockchain," IEEE Transactions on Industrial Informatics, vol. 16, no. 4,
pp. 2677-2686, Apr. 2020.

[5] M. J. Casey and P. Wong, “Global supply chains are about to get better,
thanks to blockchain,” Harvard Business Review, Mar. 2017. [Online]
Available: https://hbr.org/2017/03/global-supply-chains-are-about-to-get-
better-thanks-to-blockchain.

[6] N. Teslya and I. Ryabchikov, “Blockchain-based platform architecture
for industrial IoT," 21-st Conference of Open Innovations Association
(FRUCT), pp. 321-329, Nov. 2017.

[7] M. Mettler, “Blockchain technology in healthcare: The revolution starts
here,” IEEE 18th International Conference on e-Health Networking,
Applications, and Services (Healthcom), pp. 1-3, Sept. 2016.

[8] M. Al-Bassam, A. Sonnino, V. Buterin, “Fraud and data availability
proofs: Detecting invalid blocks in light clients," International Conference
on Financial Cryptography and Data Security Springer, Mar. 2021.
[9] M. Yu, S. Sahraei, S. Li, S. Avestimehr, S. Kannan, and P. Viswanath,
“Coded merkle tree: Solving data availability attacks in blockchains,"
International Conference on Financial Cryptography and Data Security,
Springer, Cham, pp. 114-134, Feb. 2020.

[10] K. Nazirkhanova, J. Neu, and D. Tse, “Information dispersal with
provable retrievability for rollups," arXiv preprint arXiv:2111.12323 Nov.
2021.

[11] D. Mitra, L. Tauz, and L. Dolecek, “Concentrated stopping set design
for coded merkle tree: Improving security against data availability attacks
in blockchain systems," 2020 IEEE Information Theory Workshop (ITW),
pp. 1-5, Apr. 2021.

[12] D. Mitra, L. Tauz, and L. Dolecek, “Overcoming data availability attacks
in blockchain systems: LDPC code design for coded merkle tree," arXiv
preprint arXiv:2108.13332, Aug. 2021.

[13] S. Cao, S. Kadhe, and K. Ramchandran, “CoVer: Collaborative light-
node-only veriﬁcation and data availability for blockchains," IEEE Inter-
national Conference on Blockchain (Blockchain), pp. 45-52, Nov. 2020.
[14] D. Perard, J. Lacan, Y. Bachy, and J. Detchart, “Erasure code-based
low storage blockchain node," IEEE International Conference on Internet
of Things (iThings) and IEEE Green Computing and Communications
(GreenCom) and IEEE Cyber, Physical and Social Computing (CPSCom)
and IEEE Smart Data (SmartData), pp. 1622-1627, Jul. 2018.

[15] M. Dai, S. Zhang, H. Wang, and S. Jin, “A low storage room requirement
framework for distributed ledger in blockchain," IEEE Access, vol. 6,
pp. 22970-22975, Mar. 2018.

[16] Q. Huang, L. Quan, and S. Zhang, “Downsampling and transparent
coding for blockchain" arXiv preprint arXiv:1911.01778, Nov. 2019.
[17] D. Mitra and L. Dolecek, “Patterned erasure correcting codes for low
storage-overhead blockchain systems,” IEEE Asilomar Conference on
Signals, Systems, and Computers, pp. 1734-1738, Nov. 2019.

[18] S. Kadhe, J. Chung, and K. Ramchandran, “SeF: A secure fountain
architecture for slashing storage costs in blockchains," arXiv preprint
arXiv:1906.12140, Jun. 2019.

[19] A. Tiwari, and V. Lalitha, “Secure raptor encoder and decoder for
low storage blockchain," International Conference on COMmunication
Systems & NETworkS (COMSNETS), pp. 161-165, Jan. 2021.

[20] D. S. Gadiraju, V. Lalitha, and V. Aggarwal, “Secure regenerating codes
for reducing storage and bootstrap costs in sharded blockchains," IEEE
International Conference on Blockchain (Blockchain), pp. 229-236, Nov.
2020.

[21] B. Choi, J. Sohn, D. Han, and J. Moon, “Scalable network-coded PBFT
consensus algorithm," IEEE International Symposium on Information
Theory (ISIT), pp. 857-861, Jul. 2019.

[22] S. Li, M. Yu, C. Yang, A. S. Avestimehr, S. Kannan, and P. Viswanath,
“PolyShard: Coded sharding achieves linearly scaling efﬁciency and
security simultaneously," IEEE Transactions on Information Forensics
and Security, vol. 16, Jul. 2020.

[23] C. Wang, and N. Raviv, “Low latency cross-shard transactions in coded

blockchain," arXiv preprint arXiv:2011.00087, Oct. 2020.

[24] P. Sheng, B. Xue, S. Kannan, and P. Viswanath, “ACeD: Scalable data

availability oracle," arXiv preprint arXiv:2011.00102, Oct. 2020.

[25] D. Mitra, L. Tauz, and L. Dolecek, “Communication-efﬁcient LDPC
code design for data availability oracle in side blockchains," IEEE
Information Theory Workshop (ITW), pp. 1-6, May 2021.

[26] P. Santini,G. Rafaiani, M. Battaglioni, F. Chiaraluce, M. Baldi, et al.,
“Optimization of a Reed-Solomon code-based protocol against blockchain
data availability attacks", arXiv preprint arXiv:2201.08261v1, Jan. 2022.
[27] T. Richardson, and R. Urbanke, “Modern coding theory," Cambridge:

Cambridge University Press, 2008.

[28] Online:

https://www.blockchain.com/charts/avg-block-size,

accessed

11th Jan. 2022.

[29] Online: https://bitinfocharts.com/comparison/bitcoin%20cash-size.html#

3y, accessed 11th Jan. 2022.

[30] Online:

https://bitinfocharts.com/comparison/bitcoin%20sv-size.html#

3y, accessed 11th Jan. 2022.

[31] X.Y. Hu, E. Eleftheriou and D.M. Arnold, “Regular and irregular pro-
gressive edge-growth tanner graphs," IEEE Transactions on Information
Theory, vol. 51, no. 1, pp. 386-398, Jan. 2005.

[32] K. M. Krishnan, and P. Shankar, “Computing the stopping distance of
a Tanner graph is NP-hard," IEEE Transactions on Information Theory,
vol. 53, no. 6, pp. 2278-2280, Jun. 2007.

[33] E. Arikan, “Channel polarization: A method for constructing capacity-
achieving codes for symmetric binary-input memoryless channels,” IEEE
Transactions on Information Theory, vol. 55, no. 7, pp. 3051-3073,
Jul. 2009.

[34] N. Goela, S. B. Korada, and M. Gastpar, “On LP decoding of polar

codes," IEEE Information Theory Workshop, pp. 1-5, Aug. 2010.

[35] E. Arikan, “Systematic polar coding," IEEE Communications Letters

vol. 15, no. 8, pp. 860-862, Aug. 2011.

[36] L. Li and W. Zhang, “On the encoding complexity of systematic polar
codes," IEEE International System-on-Chip Conference (SOCC), pp. 415-
420, Sept. 2015.

[37] A. Eslami, H. Pishro-Nik, “On ﬁnite-length performance of polar codes:
stopping sets, error ﬂoor, and concatenated design," IEEE Transactions
on Communications, vol. 61, no. 3, pp. 919-929, Feb. 2013.

[38] D. E. Muller, “Application of boolean algebra to switching circuit design
and to error correction,” Transactions of the I.R.E. Professional Group on
Electronic Computers, vol. EC-3, no. 3, pp. 6–12, Sept. 1954.

[39] A. Sarıduman, A. E. Pusane, Z. C. Ta¸skın, “An integer programming-
based search technique for error-prone structures of LDPC codes," AEU-
Int. Journal of Electronics and Communications, vol. 8, no. 11, pp. 1097-
1105, Nov. 2014.

APPENDIX A
PROOF OF LEMMA 1
First of all, it is easy to see that when all the VNs in V µ
[1]
(cid:98)N
(i.e., the VNs in the last µ rows from the leftmost column of
FG G
(cid:98)N ) are frozen or set to zero chunks, all the VNs in the
last µ rows from all the columns will be zero chunks. This is
because every CN in row i of the FG is either connected to
VNs in the same row i or to VNs from a row below i (i.e.,
having a row index greater than i) in the FG. This proves the
second claim of the lemma.

For the ﬁrst claim, let ψ ∈ Ψ (cid:98)A and let Gψ
(cid:98)N

be the induced
subgraph of G
(cid:98)N corresponding to the set of VNs ψ. From the
deﬁnition of Ψ (cid:98)A, ψ does not have any frozen VNs from the
(cid:98)N , i.e., ψ does not have any VNs
leftmost column of the FG G
in the set {v1i | i ∈ (cid:98)F}. Since [ (cid:98)N − µ + 1, (cid:98)N ] ⊂ (cid:98)F, ψ does
not have any VNs in V µ
(cid:98)N

We prove the ﬁrst claim of the lemma by contradiction.
Assume that ψ has a VN from V µ
[log (cid:98)N + 1]. In particular,
(cid:98)N
assume that v(n+1)i1 ∈ ψ, where n = log (cid:98)N and i1 ∈ [ (cid:98)N −µ+
1, (cid:98)N ]. Now, by the property of stopping sets, cni1 ∈ Gψ
. Now,
(cid:98)N

[1].

to satisfy the stopping set property, either vni1 ∈ ψ or vni2 ∈ ψ
where i1 < i2 ≤ (cid:98)N and vni2 and cni1 are connected in G
(cid:98)N .
Thus, for the column number n, we have at least one index i,
i ∈ [ (cid:98)N − µ + 1, (cid:98)N ] such that vni ∈ ψ. Proceeding in a similar
manner as above, for the column number (n − 1), we have
at least one index i, i ∈ [ (cid:98)N − µ + 1, (cid:98)N ] such that v(n−1)i ∈
ψ. Repeating the same process until we reach the leftmost
column, we can ﬁnd at least one index i, i ∈ [ (cid:98)N − µ + 1, (cid:98)N ]
such that v1i ∈ ψ. However, this is a contradiction of the fact
that ψ does not have any VNs in set V µ
[1] = {v1i | i ∈
(cid:98)N
[ (cid:98)N − µ + 1, (cid:98)N ]}.

APPENDIX B
PROOF OF LEMMA 2

The SEF algorithm produces a (N, K) polar code with a
FG GN where the bottom µ2 VNs from the leftmost column
of GN are frozen. Moreover, GN is obtained from freezing
(and hence removing) the last (cid:98)N − N rows of G
(cid:98)N , where
(cid:98)N = 2(cid:100)log N (cid:101). Note that F is the output of the SEF algorithm
and A = [N ] \ F. Deﬁne (cid:98)F = F ∪ [N + 1, (cid:98)N ], (cid:98)A = [ (cid:98)N ] \ (cid:98)F.
Clearly, (cid:98)A and A are the same sets. Thus, the (N, K) polar
code can be seen as a code deﬁned on the FG G
(cid:98)N with frozen
index set (cid:98)F, and information index set A, where only the
top N VNs from the rightmost column of G
(cid:98)N are the coded
symbols.

Due to Lemma 1, VNs in the last µ2 + (cid:98)N − N rows in the
rightmost column of G
(cid:98)N are not part of any stopping set in
ΨA. This implies that VNs in the last µ2 rows in the rightmost
column of GN are not part of any stopping set in ΨA. Thus
in the FG GN , the light nodes do not need to sample the
VNs in the last µ2 rows in the rightmost column of GN and
only randomly sample the top N − µ2 VNs. Moreover, from
Eqn. (3) (applied on FG G
(cid:98)N ), the smallest leaf set size of all
stopping sets in ΨA is given by
f (cid:98)N
|Leaf-Set(ψ)| = min
i = min
min
i∈A
i∈A
ψ∈ΨA
Hence, the probability of failure Pf (s) = (1 − mini∈A TN (i)
resulting in an effective undecodable threshold of αSEF
mini∈A TN (i)∗N
N −µ2

)s
min =

(cid:98)N (i) = min
i∈A

N −µ2

T

.

TN (i).

APPENDIX C
PERFORMANCE ANALYSIS AND COMPARISON

Comparison of various performance metrics of an LCMT, a
PCMT and 2D-RS codes is provided in Table I. The metrics
for 2D-RS codes are calculated as described in [8]. Detailed
derivation of the root size, single sample download size, and
IC proof size for an LCMT and PCMT in Table I is as follows.
1) Root size: For an LCMT, the root consists of the hashes
of all the coded symbols in L1. Hence, the root consists of
N1 hashes and, thus, has a size of yN1. For a PCMT, the root
consists of hashes of all the coded and dropped symbols in L1,
i.e., the hashes of all the VNs in FG of the polar code used in
L1. Hence, the root consists of N1((cid:100)log N1(cid:101) + 1) hashes and,
thus, has a size of yN1((cid:100)log N1(cid:101) + 1).

2) Single sample download size: For an LCMT, as de-
scribed in [9], [12], [24], each sample request consists of a
base layer symbol of the CMT and the Merkle proof of the
base layer symbol. Moreover, the Merkle proof of the base
layer symbol consists of a data symbol and a parity symbol
from each layer above the base layer (i.e., layers Lj, j ∈ [l−1]
). The Merkle proof satisﬁes the property that the data symbol
in proof from layer Lj consists of the hash of the data symbol
in proof from layer Lj+1, j ∈ [l − 1]. Thus, of the q hashes
present in the data symbol of the Merkle proof from layer
Lj, j ∈ [l − 1], the hash corresponding to the data symbol of
the Merkle proof from layer Lj+1 is not communicated in the
Merkle proof (and it can be calculated by taking a hash of the
data symbol of the Merkle proof from layer Lj+1). Thus, there
are only (q − 1) hashes from each layer Lj, j ∈ [l − 1] for the
data part in the Merkle proofs. Thus, the size of the Merkle
proof of a base layer symbol is y(2q − 1)(l − 1) (since the
size of each parity symbol is yq). Finally, the overall download
size for a single sample request is b
k + y(2q − 1)(l − 1), where
b
k is the size of the base layer symbol.

For a PCMT, similar to an LCMT above, a sample request
consists of a base layer symbol of the PCMT and the Merkle
proof of the base layer symbol. The Merkle proof of the base
layer symbol in this case again consists of a data symbol and
a parity symbol from each layer above the base layer. Note
that, here, the data and the parity symbols in layer Lj are the
non-dropped symbols of the polar FG GNj corresponding to
the information Aj and frozen set Fj indices, respectively. In
contrast to an LCMT, in a PCMT, each data symbol of layer
Lj, j ∈ [l − 1] consists of qj = q((cid:100)log Nj(cid:101) + 1) hashes.

The Merkle proof in a PCMT also satisﬁes the property that
the data symbol in proof from layer Lj consists of the hash of
the data symbol in proof from layer Lj+1, j ∈ [l − 1]. Thus,
only (qj − 1) hashes from layers Lj, j ∈ [l − 1] are present
in the data part of the Merkle proofs. Thus, for a PCMT, the
size of the Merkle proof of a base layer symbol is

l−1
(cid:88)

j=1

y(2qj − 1) =

l−1
(cid:88)

j=1

y(2(q((cid:100)log Nj(cid:101) + 1)) − 1)

= y(2q − 1)(l − 1) + 2qy

l−1
(cid:88)

j=1

(cid:100)log Nj(cid:101)

the overall download size for a single sample
Finally,
k + y(2q − 1)(l − 1) + 2qy (cid:80)l−1
request is b
j=1(cid:100)log Nj(cid:101), where
b
k is the size of the base layer symbol in a PCMT. Note the
additional penalty factor of 2qy (cid:80)l−1
j=1(cid:100)log Nj(cid:101) in the single
sample download size for a PCMT compared to an LCMT.
However, for large block sizes when b
k is large compared to
y, the penalty is small.

3) IC proof size: As described in Section II, the IC proof
for a failed parity check equation with d symbols consists
of d − 1 symbols and the Merkle proofs of the d symbols.
Note that the proof size is largest for a failed parity check
equation in the base layer. Thus, we provide the IC proof
size when the d symbols are base layer symbols. Also note
that, in IC proofs, the Merkle proof of a symbol only consists

Root size

Single sample
download size

IC proof size

2D-RS
√

2y(cid:100)

Nl(cid:101)
√

b
k + y(cid:100)log

Nl(cid:101)

LCMT
yN1

b
k + y(2q − 1)(l − 1)

PCMT
yN1((cid:100)log N1(cid:101) + 1)
k +y(2q −1)(l−1)+2qy (cid:80)l−1

b

j=1(cid:100)log Nj (cid:101)

( b
k + y(cid:100)log

√

√

Nl(cid:101))(cid:100)

k(cid:101)

(dc−1)b
k

+ dcy(q − 1)(l − 1)

(dp−1)b
k

+ dpy(q − 1)(l − 1) +
j=1(cid:100)log Nj (cid:101)

dpqy (cid:80)l−1

Decoding complexity
αmin

O(N 1.5
l
Analytical expression in [8]

)

O(Nl)
NP-hard to compute

O(Nl(cid:100)log Nl(cid:101))
Lemma 2

TABLE II: Comparison of various performance metrics of 2D-RS codes, an LCMT, and a PCMT. The LCMT and PCMT have the same (k, R, q, l)
parameters. The maximum degree of CNs in the LDPC codes and polar FG are dc and dp = 3, respectively. The size of the block is b. 2D-RS has k data
symbols and (cid:100)log
R . For an LCMT, the number of layers l can be calculated such that the root size is some
ﬁxed constant t. The same l is used for the PCMT.

Nl(cid:101) layers in the Merkle tree where Nl = k

√

on the data symbols from each layer above the base layer
(the parity symbols included in the Merkle proofs of the
light node samples are only to get additional samples of the
intermediate layers) [9], [24]. Thus, in an LCMT, for a failed
parity check equation with d symbols, the size of the IC proof
is (d−1)b
k + dy(q − 1)(l − 1), where the (q − 1) term arises
due to the same reason as explained in the single sample
download size calculation. Hence, when the maximum CN
degree of the LDPC code is dc, the IC proof size becomes
(dc−1)b

k + dcy(q − 1)(l − 1).
For a PCMT, the IC proof for a failed parity check equation
with d symbols again consists of d−1 symbols and the Merkle
proofs of the d symbols. Note that the symbols here can be
both the dropped or non-dropped symbols of the polar FG.
Also, each data symbol of layer Lj, j ∈ [l − 1] in a PCMT
consists of qj = q((cid:100)log Nj(cid:101) + 1) hashes. Thus, for a failed
parity check equation with d symbols, the size of the IC proof
in a PCMT is
(d − 1)b
k

dy(qj − 1)

l−1
(cid:88)

+

j=1

=

=

(d − 1)b
k

+

l−1
(cid:88)

j=1

dy((q((cid:100)log Nj(cid:101) + 1)) − 1)

(d − 1)b
k

+ dy(q − 1)(l − 1) + dyq

l−1
(cid:88)

(cid:100)log Nj(cid:101).

j=1

For a PCMT, the maximum CN degree for a CN in the FG
of the polar code is dp = 3. In this case, the IC proof size
becomes (dp−1)b
j=1(cid:100)log Nj(cid:101).

+ dpy(q − 1)(l − 1) + dpyq (cid:80)l−1

k

A. Undecodable Threshold

The undecodable threshold αmin for an LCMT with the
LDPC codes described in Section V and a PCMT with SEF
Polar codes is shown in Fig. 4. From the ﬁgure, we see that a
PCMT has a higher αmin compared to an LCMT for different
values of rates R and code lengths N . Hence for large block
size b, when the penalty in the single sample download size
for a PCMT is small, they have a lower probability of failure
compared to an LCMT for the same total sample download
size. This result is shown in Fig. 3 right panel.

n
i
m
α

N
Fig. 4: Left Panel: Undecodable threshold αmin vs N for PEG LDPC codes
(with VN degree 3) and SEF Polar codes. For PEG LDPC codes, we only
list αmin that we were able to ﬁnd by solving the ILP in [39].

APPENDIX D
SYSTEMATIC ENCODING OF POLAR CODES

Systematic encoding of an ( (cid:98)N , (cid:98)K) polar codes can be
performed using the method described in [36]. It operates on
(cid:98)N of the code. Let (cid:98)A ⊂ [ (cid:98)N ] and (cid:98)F = [ (cid:98)N ] \ (cid:98)A be
the FG G
the index sets corresponding to the data and frozen symbols of
the ( (cid:98)N , (cid:98)K) polar code, respectively, where | (cid:98)A| = (cid:98)K. Also, let
(cid:98)N = 2n. The systematic encoding is performed on the FG G
(cid:98)N
by i) placing the (cid:98)K data symbols at the VNs {v(n+1)i|i ∈ (cid:98)A}
(in the rightmost column) and setting the VNs at {v1i|i ∈ (cid:98)F}
(in leftmost column) to zero symbols; ii) determining the rest
of the VNs using the check constraints of the FG G
(cid:98)N in a
two stage reverse and forward encoding on the FG [36]. The
coded symbols are the VNs v(n+1)i, i ∈ [ (cid:98)N ], and are by design
systematic. Systematic encoding can also be performed using
a peeling decoder as the encoder as described next.

In [35], author proposed to perform systematic encoding
of polar codes by using a successive cancellation decoder
on the code FG as the encoder. Inspired by this idea, we
show that systematic encoding of the polar codes can also
be performed using a peeling decoder as the encoder. The
encoder, which we call a peeling encoder for polar codes
(PEPC), works as follows. Consider the data and frozen index
sets (cid:98)A ⊂ [ (cid:98)N ] and (cid:98)F = [ (cid:98)N ] \ (cid:98)A of the polar code with

∈ ψ). If not,

∈ ψ,
imax must belong to the stopping set ψ (i.e., v(k−1)imax
v(k+1)imax
then to satisfy the stopping set
property, the CN must be connected to a VN vki ∈ ψ by
a slanted edge. Note that a slanted edge connects a CN
to a degree 3 VN in lower row. In other words, a slanted
edge connects a CN from row imax to a degree 3 VN in a
row with index greater than imax. This condition violates the
∈ ψ.
deﬁnition of imax. Thus, v(k−1)imax
Now, considering v(k−1)imax
as the starting
VN (similar to vkimax
), we can apply the above logic to
∈ ψ. Repeatedly
∈ ψ and v(k+2)imax
show that v(k−2)imax
applying the same argument, we can show that all the VNs in
{vkimax | k ∈ [n + 1]} belong to ψ, where n = (cid:100)log N (cid:101).

∈ ψ and v(k+1)imax

and v(k+1)imax

According to the above lemma, every stopping set has at
least one full row of VNs, i.e., it has VNs from all columns
along a single row. We use this property to prove Lemma 3.

Proof of Lemma 3:
Let V be the set of all VNs in the polar FG GN . Also, let
V e = {v(n+1)i | i ∈ A} ∪ {v1i | i ∈ F} and Ve = V \ V e.
VNs in Ve are uninitialized or initially erased by the PEPC and
are determined using a peeling decoder (for the purposes of
encoding). Thus, the PEPC can fail if Ve contains a stopping
set of GN . However, since A ∪ F = [N ] (i.e., they form a
partition of all the row indices), for all i ∈ [N ] either v(n+1)i ∈
V e or v1i ∈ V e. Thus, for all i ∈ [N ], v(n+1)i and v1i both
cannot simultaneously belong to Ve. Hence, Ve cannot contain
a full row of VNs. Using Lemma 4, we conclude that Ve
cannot contain a stopping set. Thus, the PEPC will always be
successful and will result in a valid codeword.

FG G
(cid:98)N . Place the data symbols at the VNs {v(n+1)i|i ∈ (cid:98)A}
(in the rightmost column) and set the VNs at {v1i|i ∈ (cid:98)F}
(in leftmost column) to zero symbols. Use a peeling decoder
to ﬁnd the remaining VNs of the FG. The coded symbols
ci = v(n+1)i, i ∈ [ (cid:98)N ], are by design systematic. We have the
following lemma corresponding to a PEPC.

Lemma 3. Systematic encoding of polar codes using a PEPC
always results in a valid codeword. In other words, the peeling
decoder never encounters a decoding failure when used for
encoding.

We prove Lemma 3 by proving the following important
property of stopping sets in the FG of polar codes produced by
the SEF algorithm. Note that the property holds true for regular
(cid:98)N where (cid:98)N is a power of 2. Additionally, the
polar FG G
property also holds true for the FG GN obtained by removing
VNs from the last few rows of FG of the form G
(cid:98)N , where (cid:98)N
is a power of 2. Thus, let n = (cid:100)log N (cid:101). The proof provides
insights on important properties of stopping sets in the FG
of polar codes. To our best knowledge, we have not seen the
following result before in literature and, hence, it may be of
independent interest.

Lemma 4. Consider a polar FG GN produced by the SEF
algorithm (this encompasses polar factor graphs G
(cid:98)N , where
(cid:98)N is a power of 2). Every stopping set of GN has a full row
of VNs i.e., it has all VNs in the set {vki | k ∈ [ (cid:100)log N (cid:101)+1 ]}
for some i ∈ [N ].

Proof. Let ψ be a stopping set of GN . Let Gψ
N be the induced
subgraph of GN corresponding to the set of VNs ψ. Observe
that the FG GN has two types of edges (see Fig. 1 for an
example): horizontal edges and slanted edges (which involves
a connection between a degree 3 VN and a degree 3 CN). We
consider two cases: i) Gψ
N does not have any slanted edges; ii)
Gψ
N has at least one slanted edge.
For case i), it is easy to see that the stopping set ψ must
include a full row of VNs. For case ii), note that it implies
Gψ
N has at least one slanted edge. This implies that ψ has
at least one VN of degree 3. In this situation, deﬁne the set
∆ψ = {(i, k) | i ∈ [N ], k ∈ [n], vki ∈ ψ, degree of vki = 3}.
Let imax = max({i|(i, k) ∈ ∆ψ for some k, k ∈ [n]}).
∆ψ contains the indices of all the degree 3 VNs of ψ and
imax denotes the largest row index such that ψ has a degree
3 VN from that row. Note that ∆ψ is non empty because
we are considering case ii). Now, we claim that ψ has all
the VNs in the row imax, i.e., ψ contains all the VNs in
{vkimax | k ∈ [n + 1]}. To see why this is true, let k, k ∈ [n],
be such that (imax, k) ∈ ∆ψ. This means that vkimax
∈ ψ.
Now by the deﬁnition of a stopping set, the CNs to the right
and left of vkimax
must belong to the induced subgraph graph
of the stopping set. In other words, c(k−1)imax
N and
ckimax
is the rightmost or the left most
VN in which case we will have only one CN neighbour). Now,
to satisfy the stopping set property, for both these CNs, their
corresponding VNs to their left and right in the same row

N (unless vkimax

∈ Gψ

∈ Gψ

