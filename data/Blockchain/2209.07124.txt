How Much Does It Cost to Train a Machine
Learning Model over Distributed Data Sources?

Elia Guerra, Francesc Wilhelmi, Marco Miozzo, Paolo Dini

2
2
0
2

p
e
S
5
1

]

G
L
.
s
c
[

1
v
4
2
1
7
0
.
9
0
2
2
:
v
i
X
r
a

Abstract—Federated learning (FL) is one of the most appealing
alternatives to the standard centralized learning paradigm, allow-
ing heterogeneous set of devices to train a machine learning model
without sharing their raw data. However, FL requires a central
server to coordinate the learning process, thus introducing poten-
tial scalability and security issues. In the literature, server-less FL
approaches like gossip federated learning (GFL) and blockchain-
enabled federated learning (BFL) have been proposed to mitigate
these issues. In this work, we propose a complete overview of
these three techniques proposing a comparison according to an
integral set of performance indicators, including model accuracy,
time complexity, communication overhead, convergence time and
energy consumption. An extensive simulation campaign permits
to draw a quantitative analysis. In particular, GFL is able to save
the 18% of training time, the 68% of energy and the 51% of
data to be shared with respect to the CFL solution, but it is not
able to reach the level of accuracy of CFL. On the other hand,
BFL represents a viable solution for implementing decentralized
learning with a higher level of security, at the cost of an extra
energy usage and data sharing. Finally, we identify open issues
on the two decentralized federated learning implementations and
provide insights on potential extensions and possible research
directions on this new research ﬁeld.

Index Terms—decentralized learning, edge computing, fed-
erated learning, gossip learning, blockchain, energy efﬁciency,
computational complexity, communication overhead

I. INTRODUCTION

Machine learning (ML) models, and in particular deep
neural networks, require a substantial amount of data and
computational power that might not be available on a single
machine. As a consequence, ML operations are normally
run at cloud servers (or data centers), where batteries of
powerful processing units enable short training and inference
computation times. However, training ML models in a data
center requires moving data from the information sources (e.g.,
edge devices) to the central system. This approach runs into
several issues:

• Communication overhead. Nowadays, the huge perva-
siveness of mobile services, devices, and network in-
frastructures makes data sources mainly distributed. As
testiﬁed recently by the Ericsson Mobility Report [1],
mobile network data trafﬁc grew exponentially over the
last 10 years, with a remarkable increase of 42% between
Q3 2020 and Q3 2021. Mobile data trafﬁc is projected

The authors are with Centre Tecnol`ogic de Telecomunicacions de Catalunya
(CTTC/CERCA). Corresponding author: eguerra@cttc.cat. This publication
has been partially funded by the Spanish project PID2020-113832RB-
C22(ORIGIN)/MCIN/AEI/10.13039/50110001103, European Union Horizon
2020 research and innovation programme under Grant Agreement No. 953775
(GREENEDGE) and the grant CHIST-ERA-20-SICT-004 (SONATA) by
PCI2021-122043-2A/AEI/10.13039/501100011033

to grow by over 4 times to reach 288 EB per month
by 2027 [1]. Moving such a big amount of data from
distributed sources to a central location for ML operations
may create network congestion and service outage.

• Latency. In several real-life scenarios, transmitting data
requires a stable and reliable connection to minimize
latency and ensure updated models, which cannot be
always guaranteed. For example, minimizing communica-
tion latency in connected vehicles is essential to guarantee
road safety [2].

• Energy consumption. Running ML models in cloud data
centers consumes a signiﬁcant amount of energy and
cannot be considered sustainable from an environmental
perspective. As reported in [3], from 2012 to 2018, the
computations required for training a deep learning model
have been doubling every 3.4 months, with an estimated
increase of 300000x. Estimates show that training a state-
of-the-art natural language processing model produces
more CO2 than an average car in one year lifetime [4].
• Privacy. With the growing awareness of data privacy
and security, it is often undesirable, or even unfeasible,
to collect and centralize users’ data [5]. For instance, a
single hospital may not be able to train a high-quality
model for a speciﬁc task on its own (due to the lack of
data), but it cannot share raw data due to various policies
or regulations on privacy [6]. Another example could be
the case of a mobile user that would like to employ a
good next-word predictor model without sharing his/her
private historical text data.

A. Edge AI and Federated Learning

To address the challenges that stem from cloud-based cen-
tralized ML, edge computing pushes cloud services to the
network edge and enables distributed ML operations, i.e. the
so-called edge intelligence [7]. In particular, AI on Edge [8]
is the paradigm of running AI models with a device-edge-
cloud synergy. It allows to relax the massive communication
requirements and privacy of cloud-based ML operations [9].
Moreover, distributing ML computation over the edge has
been demonstrated to save up to the 25% of the energy
consumption [10]. In fact, data may be directly processed
at the edge with smaller and more energy efﬁcient devices
(no need of air conditioning systems) and the energy cost
related to communication is limited due to unnecessary data
transmission.

Among the several

training paradigms enabled by edge
intelligence, federated learning (FL) has emerged as a popular

 
 
 
 
 
 
solution by providing low communication overhead, enhanced
user privacy and security to distributed learning [11]. With FL,
the ML model is trained cooperatively by edge devices without
sharing local data, but exchanging only model parameters.
The usual implementation envisages an iterative procedure
whereby a central server collects local updates from the clients
(e.g., edge devices) and returns an aggregated global model.
In the rest of the paper, we refer to centralized FL (CFL) to
the traditional server-dependent FL scenario. In this setting,
the server has to wait for all the clients before returning a
new global update. Therefore, high network latency, unreli-
able links, or straggled clients may slow down the training
process and even worsen model accuracy [12]. In addition, the
central server represents a single point of failure, i.e., if it is
unreachable due to network problems or an attack, the training
process cannot continue. Furthermore, it may also become a
bottleneck when the number of clients is very large [13].

Decentralized and server-less solutions for federated learn-
ing have been introduced in the literature, mainly to overcome
the single point of failure and the security problems [14],
[15]. In [16] a decentralized FL mechanism was proposed by
enabling one-hop communication among FL clients. Similarly,
Gossip FL (GFL) extends device-to-device (D2D) communi-
cations to compensate for the lack of an orchestrating central
server [17], [18]. It guarantees a low communication overhead
thanks to the reduced number of messages [19].

Beyond, we ﬁnd more sophisticated proposals,

like
blockchain-enabled federated learning (BFL), which adopts
blockchain to share FL information among devices,
thus
removing the ﬁgure of the orchestrating central server. In this
way, blockchain removes the single point of failure for the
sake of openness and decentralization and provides enhanced
security via tampered-proof properties [20].

B. Contributions

Despite in the literature it

is possible to ﬁnd papers
comparing classical centralized learning in data center with
CFL [21], [22], a comparison among the different federated
learning approaches (centralized versus decentralized) is still
missing.

In this work, we aim to ﬁll this gap and, thus, we focus on
two of the most popular and widely adopted approaches for
decentralizing FL: GFL and BFL. In particular, we provide a
comprehensive analysis of both methods and compare them
to traditional FL, i.e., CFL. Note that we combine standard
performance indicators for ML models, i.e., accuracy, with
indicators that quantify the efﬁciency of these algorithms,
i.e., time complexity, communication overhead, convergence
time and energy consumption. With our comparison under fair
conditions, we would like to provide the research community
with a complete overview of the three approaches together
with all the information to choose the best model according
to the speciﬁc use cases.

The contributions of this paper may be summarized as

follows:

• We overview the traditional FL setting and delve into
two approaches for decentralizing it. They are selected
since are two of the most popular in the literature and are
kind of diverging into two completely different solutions,
which are based on gossip communication and blockchain
technology, respectively.

• We provide a thorough analysis to derive the running
time complexity, the communication overhead and the
convergence time of each overviewed mechanism for FL,
including CFL, BFL, and GFL.
• We provide an energy model

to measure the energy
consumption of each solution, based on the associated
communication and computation overheads.

• We assess the performance of each method (CFL, GFL,
and BFL) through extensive simulations on widely used
TensorFlow libraries [23].

• We delve into the open aspects of decentralized FL and
provide insights on potential extensions and considera-
tions for GFL and BFL.

The rest of the paper is structured as follows: Section II
reviews the related work. Section III describes the three studied
algorithms (CFL, BFL and GFL). Section IV analyzes their
time complexity, the communication cost, and introduces the
communication model and the convergence time. Section V
provides the energy model used in this paper. Then, Section VI
compares the three mechanisms through simulation results. In
Section VII, we provide some open issues of GFL and BFL,
and we discuss on the correspondent optimizations and future
research directions. Finally, Section VIII concludes the paper
with ﬁnal remarks.

II. RELATED WORK

Distributing and decentralizing ML operations at the edge
has been embraced as an appealing solution for addressing
the issues of centralization (connectivity, privacy, and secu-
rity) [24]. With FL, different devices collaborate to train an
ML model by sharing local updates obtained from local and
private data. The traditional FL algorithm (FedAvg), referred
to as CFL in this paper, is introduced in [25]. In [11], the
authors propose techniques to improve its communication
efﬁciency. Nevertheless, CFL still requires a central server
responsible for clients orchestration and model aggregation.
The star topology is a weak aspect of CFL, since the central
entity represents a single point of failure, it may limit the
number of devices that can participate in the training process,
augments the communication cost of the learning process, and
presents privacy concerns [13], [26].

To address these challenges, decentralized federated learn-
ing has been proposed in [16]. The authors present a fully
decentralized model, in which each device can communicate
only with its one-hop neighbors. The authors also provide a
theoretical upper bound on the mean square error. Ormandi et
al. [17] introduce Gossip FL, a generic approach for Peer-to-
peer (P2P) learning on fully distributed data, i.e., every device
has only a single input sample to process at each round. The

same algorithm has been tested in [18] under real-world condi-
tions, i.e., devices have multiple input samples available (rather
than only one point, as originally stated in [17]), restricted
communication topology and, heterogeneous communication
and computation capabilities.

to

solution

Another

prominent

decentralize FL is
blockchain-enabled FL [27]–[29]. A blockchain system
allows clients to submit and retrieve model updates without
the central server. Additionally,
the usage of blockchain
guarantees security, trust, privacy, and traceability, however, it
introduces delays due to the distributed ledger technology. An
analysis of end-to-end latency and the effects of blockchain
parameters on the training procedure of BFL is proposed
in [20].

In the literature, there exist some comparisons across FL
techniques. The authors of [30] compare GFL and CFL with
a logistic regression model in terms of convergence time,
proportion of the misclassiﬁed examples in the test set (0-1
error), and used communication resources. When nodes have
a random subset of the learning samples, GFL performance is
comparable with CFL; instead, CFL converges faster when a
node has only labels from one class. Another comparison is
proposed in [31], where the performance of FL algorithms that
require a central server, e.g., FedAvg and Federated Stochastic
Reduced Gradient are analyzed. Results show that FedAvg
achieves the highest accuracy among the FL algorithms regard-
less of how data are partitioned. In addition, the comparison
between FedAvg and the standard centralized algorithm shows
that
they are equivalent when independent and identically
distributed (IID) datasets are used.

In [19], the authors compare GFL with the standard cen-
tralized data center based architecture in terms of accuracy
and energy consumption for two radio access network use-
cases. To achieve this goal, they use the machine learning
emission calculator [32] and green algorithms [33]. In [21],
the authors compare centralized data center based learning and
CFL in terms of carbon footprint using different datasets. The
assessment is done by sampling the CPU and GPU power
consumption. In [22], the authors propose a framework to
evaluate the energy consumption and the carbon footprint
of distributed ML models with focus on industrial Internet
of Things applications. The paper identiﬁes speciﬁc require-
ments on the communication network, dataset and model size
to guarantee the energy efﬁciency of CFL over centralized
learning approaches,
i.e., bounds on the local dataset or
model size. Differently from our work, the authors do not
consider Blockchain-enabled FL and evaluate the algorithm
performance in scenarios with limited number of devices
(i.e., 100). Moreover, we also empirically measure the energy
consumption of the devices based on the real load of the
computations realized during the training phase; we provide a
communication model to estimate overheads and convergence
time. Additionally, here we introduce an analysis on the
computational complexity of the three federated algorithms
under study.

In summary, in this paper, we bridge the gap in the literature

by providing a thorough comparison including performance
analysis and cost of the different federated approaches listed
above, i.e., CFL, BFL and GFL. Differently from the other
works in the literature, we combine standard metrics, i.e.,
accuracy, with indicators of the efﬁciency of these algorithms,
i.e., computational complexity, communication overhead, con-
vergence time and energy consumption. Our ﬁnal aim is to
contribute to the development of Green AI [34].

III. FEDERATED LEARNING IMPLEMENTATIONS

Let us consider a set of N clients (or devices) N =
{1, ..., N } with their datasets D1, ..., DN . Each local datatset
Di, ∀i ∈ N , contains pairs (xi, yi), where xi is the feature
vector, and yi its true label. The goal of a federated setting is
to train a global model (e.g., a set of weights w), that minimize
the weighted global loss function:

(cid:96) =

N
(cid:88)

i=1

(cid:96)i(w, xi, yi),

(1)

where (cid:96)i is the local loss experienced by client i. In this sce-
nario, devices do not share raw local data with other devices.
Instead, they exchange model parameters updates, computed
during several iterations by training the global model on local
data.

In this paper we study three different implementations to
solve the federated problem stated above, namely: CFL, BFL,
and GFL. The investigated solutions are depicted in Fig. 1 and
we will introduce them in what follows.

Fig. 1: Overview of the different FL scenarios.

A. Centralized Federated Learning (CFL)

At the beginning of a round t, a random subset of m devices
S t ⊆ N is selected, and the server sends the current global
model to the parties. Each client makes E epochs on the local
dataset with a mini-batch size of B, updates its local model
wt+1
and send it to the server. The server aggregates the local
k

Central serverBFLEdge deviceMinerEdge link (UL)Edge link (DL)Cloud linkCFLGFLupdates and generates the new global model by computing the
weighted average of the local updates as follows:

wt+1 =

(cid:88)

k∈S t

|Dk|
|D|

wt+1
k

,

(2)

where |D|= (cid:80)
k∈S t|Dk|. The process is repeated until the
model reaches convergence, e.g., the loss function does not
improve across subsequent epochs or a speciﬁc number of
training rounds have been executed. In this work, we consider
the FedAvg algorithm [25] as a merging method to generate
global model updates.

Algorithm 1 describes the CFL with FedAvg mechanism.
The procedure MAIN is executed by the server that coordinates
the whole training process. Each client executes the procedure
CLIENTUPDATE and applies the stochastic gradient descent
(SGD) algorithm on its local dataset with a learning rate η.

Algorithm 1 CFL

1: procedure MAIN
2:
3:

initialize w0
t ← 0
while convergence is not reached do
S t ← random set of m clients
for each client k ∈ S t in parallel do
Download the global model wt
wt+1
k ← CLIENTUPDATE(k, wt)
Send wt+1
to the server

k

4:
5:
6:
7:
8:
9:

k

k∈S t

end while

|Dk|
|D| wt+1

end for
wt+1 ← (cid:80)
t ← t + 1

10:
11:
12:
13:
14: end procedure
1: procedure CLIENTUPDATE(k, w)
2:
3:
4:

B ← split the local dataset into batches of size B
for E local epochs do
for batch b ∈ B do

(cid:46) Run on client k

w ← w − η∇(cid:96)(w, b)

end for

5:
6:
7:
8:
9: end procedure

end for
return w

B. Blockchain-enabled Federated Learning (BFL)

BFL is based on distributed ledger technology, which col-
lects data in form of transactions and organize it in blocks.
Indeed, a blockchain is a sequence of blocks chained one after
the other through advanced cryptographic techniques. Each
block contains the hash value of the previous one, leading
to a tampered-proof sequence and providing properties that
are essential to building trust in decentralized settings, such
as transparency and immutability. In a blockchain, a set of
participant nodes (miners) apply certain mining protocols and
consensus mechanisms to append new blocks to the blockchain
and agree on the status of the same. This procedure allows

devices to write concurrently on a distributed database and
guarantees that any malicious change on data would not be
accepted by the majority, so that data in a blockchain is
secured.

When a blockchain is applied to a federated setting, the

process is going as follows [20]:

1) Each device submits its local model updates in the
form of transactions to the blockchain peer-to-peer (P2P)
network of miners.

2) The transactions are shared and veriﬁed by miners.
3) Miners execute certain tasks to decide which node
updates the chain. One of the most popular mining
mechanisms, and studied in this paper, is Proof-of-Work
(PoW) [35], whereby miners spend their computational
power (denoted by λ) to solve computation-intensive
mathematical puzzles.

4) As a result of the concurrent mining operation, a new
block is created and propagated throughout the P2P
blockchain network every BI seconds (on average).
The block size SB is selected such that can include a
maximum of m transactions, each one representing a
local model submitted by a client.

5) Clients download the latest block from its associated
miner (as in [27], [36]), which would allow performing
on-device global model aggregation and local training.

An important consequence of the blockchain decentralized
consensus is forking. A fork occurs when two or more miners
generate a valid block simultaneously (i.e., before the winning
block succeeds to be propagated). The existence of forks can
be seen as a waste of resources, as it may lead to extra
computation and delay overheads [37].

1...wt

In this work, we consider the version of BFL reported in
Algorithm 2 [20], which entails the participation of Multi-
Access Edge Computing (MEC) servers and edge devices.
m ∈ bt contained
Each client downloads the updates wt
in the latest block, computes the new global wt, and trains
it on its local dataset with the CLIENTUPDATE procedure
described in Section III-A. The parameters of the new updated
model wt+1
are then submitted with the method SUBMITLO-
CALUPDATE, where Str is the transaction size. Once all the
local updates are uploaded to the blockchain, a new block
bt+1 is mined with MINEBLOCK, where the block generation
rate λ is derived from the total computational power of
blockchain nodes. Finally, the new block is shared across all
the blockchain nodes with the procedure PROPAGATEBLOCK,
which depends on the size of block bt+1 (ﬁxed to SB). The
process is repeated until convergence.

k

C. Gossip Federated Learning (GFL)

GFL is an asynchronous protocol that trains a global model
over decentralized data using a gossip communication algo-
rithm [17], [18].

We consider the general skeleton proposed in [18]. Overall,
the participating clients start from a common initialization.
The global model is then trained sequentially on local data

Algorithm 2 BFL

Algorithm 3 GFL

1: procedure MAIN
2:
3:
4:

initialize lastM odelk for each client k
t ← 0
while convergence is not reached do
S t ← random set of m clients
[k1, ..., km] ← GETSEQUENCE(St)
for i = 1, ..., m do

wt

ki−1
in the sequence

(cid:46) Model trained by the previous node

, lastM odelki )

ki−1

← MERGE(wt
← CLIENTUPDATE(ki, wt
ki

wt
ki
wt
ki
lastM odelki ← wt
Send model to the next client

ki−1

)

5:
6:
7:
8:

9:

10:

11:

end while

end for
t ← t + 1

12:
13:
14:
15:
16:
17: end procedure
18:
19: procedure MERGE(w, w(cid:48))
20:
21:
22: end procedure
23: procedure GETSEQUENCE(St)

w ← w+w(cid:48)
2
return w

i.i.d.
∼ U (St)

[k1, . . . , km]
return [k1, . . . , km]

24:
25:
26: end procedure

1: procedure MAIN
t ← 0
2:
initialize w0
3:
while convergence is not reached do
4:
S t ← random set of m clients
for each client k ∈ S t in parallel do
Download the latest block, bt
wt ← (cid:80)
|Dj |
|D| wt
wt+1
k ← CLIENTUPDATE(k, wt)
SUBMITLOCALUPDATE(Str)

5:
6:
7:
8:

j∈bt

j

9:
10:
11:

12:
13:
14:
15:
16:
17:

end for
bt+1 ←MINEBLOCK(λ)
PROPAGATEBLOCK(bt+1)
if bt+1 is not valid then

Go to line 12

end if
t ← t + 1

end while

18:
19: end procedure

and following a given path (e.g., random walk) of visiting
clients.

Algorithm 3 describes the GFL procedure. At each round
t, m nodes are randomly selected and ordered in a sequence
S t = [k1, ..., km]. Every node ki ∈ St receives the model
wt
from the previous node in the sequence and performs the
steps below, also reported in Fig. 2 for a better understanding:

ki−1

1) Run the procedure MERGE to combine wt

and the
model saved in its local cache, lastM odelki, i.e., the
model from the previous round in which the node have
been selected.

ki−1

2) Train the model generated in 1) on the local dataset
with the procedure CLIENTUPDATE described in Sec-
tion III-A.

3) Update the local cache (lastM odelki) with the model

received from the previous node wt

.

ki−1

4) Share the model trained on the local dataset wt
ki

with

the following node in the sequence.

A round is completed when the model has visited all the
nodes in the sequence. The algorithm stops when convergence
is reached (after a given number of rounds).

IV. COMPUTATIONAL AND COMMUNICATION COSTS

In this section, we introduce the mathematical statements for
the calculation of the time complexity of the three federated
algorithms considered in Section III. We elaborate also on
the data overhead due to the communication of the different
model updates during the several rounds of the process of
each implementation. Finally, we derive the equations for the
calculation of the time to reach the convergence of the three
analyzed federated approaches. The results proposed hereafter
are derived using the following assumptions:

1) Scalar operations (sums and products) cost O(1).

Fig. 2: Overview of the operations executed by a node in the
GFL algorithm.

2) The time complexity of the matrix multiplication is lin-
ear with the matrix size, i.e., A ∈ Ri×j and B ∈ Rj×k,
the cost of the product is O(i · j · k).

3) For a single input pair (xi, yi), the time complexity
required to compute ∇(cid:96) is linear with the number of
model’s weights, O(|w|).

4) During the mining process, with the PoW, a miner com-
putes the nonce of a block using brute force until ﬁnding

Model cache   MERGE1 CLIENT UPDATE234Edge deviceLocal dataset....................................a hash value lower or equal to a certain threshold [38],
referred to as the mining difﬁculty. Assume that the
hash value has b bits, and that its solution should be
smaller than 2b−l bits (being l a value determined by
the mining difﬁculty), if the miner samples the nonce
values at random, the probability of a valid value is
2−l. Henceforth, 2l sampling operations are required for
mining a block. The time complexity is O(2l).

5) The set of nodes that have a local copy of the blockchain
is NB = {1, ..., NB}, without loss of generality, is
assumed to be N ∩ NB = ∅.

6) We assume that convergence of the FL training proce-

dure is reached after R rounds.

Theorem 1. The time complexity of CFL is:

O(RmE|Dmax||w|),

(3)

where Dmax = maxk∈N |Dk|. The communication overhead
is given by

2Rm|w|

(4)

Proof. See Appendix A-A.

Theorem 2. The time complexity of BFL is:

O(R(|w|m2 + E|Dmax||w|m + 2l + m|w|NB)),

(5)

where NB is the number of nodes that have a local copy
of the blockchain and l is related to the PoW difﬁculty (see
Assumption 4). Its communication overhead is
(cid:1)
R (cid:0)|w|m2 + |w|m + m|w|NB

(6)

Proof. See Appendix A-B.

Theorem 3. The time complexity of GFL is

O(RmE|Dmax||w|)

and its communication overhead is

Rm|w|

Proof. See Appendix A-C.

(7)

(8)

The three algorithms have a time complexity that depends
on the dataset size |Dmax|. Additionally, the time complexity
of BFL (5) is also a function of the blockchain parameters NB
and l. In particular, the dominant term in (5) is R2l. Hence, the
time complexity of BFL is exponential in the PoW difﬁculty
l, while for CFL and GFL is polynomial in RmE|Dmax||w|.
Table 1 summarizes the different results obtained for time
complexity and communication overhead.

To ﬁnalize our analysis, we compute the total execution
time of each algorithm till convergence (convergence time) as a
function of the delay introduced by the communication rounds
and the computational operations. To do that, we characterize
types of nodes exchange
the links whereby the different
information (e.g.,
local model updates, blocks), having in
mind the topology introduced in Fig. 1. We classify two
different types of connections: cloud (solid arrows) and edge

(dotted and dashed arrows). Cloud connections (assumed to
be wired) are used by miners in the blockchain; instead, edge
connections (assumed to be wireless) are used by edge nodes
to upload/download models. Given its popularity and easiness
of deployment, we adopt IEEE 802.11ax links for edge con-
nections [39]. Since edge devices are often energy-constrained,
we consider different values of transmission power for the
edge connections. The central server and blockchain node use
a transmission power of P c
TX,
with P e
TX. The wired connection has a capacity CP 2P .
Additionally, we identify three main types of computational
operations during the federated learning processes: local model
training, model parameters exchange, and blockchain data
sharing. Based on this, we can compute the convergence time
of CFL, BFL, and GFL as follows:

TX, instead, edge devices use P e

TX ≤ P c

TCF L = Ttrain + Rm(T e
Tx),
TBF L = TBC + Ttrain + Rm(T e
Tx + T c
TGF L = Ttrain + RmT e
Tx,

Tx + T c

Tx),

(9)

(10)

(11)

where Ttrain is the total amount of time spent for training
the ML model locally, T c/e
is the transmission time of the
Tx
central server/blockchain node (c) or the edge device (e), and
computed according to the model detailed in Appendix B. TBC
is the delay introduced by blockchain and described in steps 2-
4 of the process in Section III-B.

V. ENERGY FOOTPRINT

In this section, we deﬁne the models used to characterize
the energy consumption that results from the FL operations.
Driven by (9), (10) and (11), the total amount of energy
consumed in each scenario is:

ECF L = Etrain + Rm(E e
EBF L = EBC + Etrain + Rm(E e
EGF L = Etrain + RmE e

Tx),
Tx + E c
Tx,

Tx + E c

Tx),

(12)

(13)

(14)

where Etrain is the energy consumed by each node during the
local training, and E c/e
the energy required to transmit the
Tx
model through the IEEE 802.11ax wireless links, from either
a central server/blockchain node (c) or an edge device (e).
Etrain is calculated as:

Etrain =

R−1
(cid:88)

r=0

P cpu

r ∆r,

(15)

r

where P cpu
is the average power consumed by the CPU and
DRAM during a round r, and ∆r is the duration of the oper-
ation. As described in Section IV, we may have two types of
communication links: cloud and edge. Considering that cloud
links are wired, we assume that their energy consumption is
negligible. Instead, we compute the energy consumption of the
edge connections according to the following equation:
Tx = T c/e
E c/e

Tx P c/e
Tx ,

(16)

Algorithm

Time complexity

Communication Overhead

CFL
BFL
GFL

O(RmE|Dmax||w|)

2Rm|w|

O(R(|w|m2 + E|Dmax||w|m + 2l + m|w|NB)) R(|w|m2 + |w|m + m|w|NB)

O(RmE|Dmax||w|)

Rm|w|

TABLE 1: Computational complexity and communication overhead for CFL, BFL and GFL.

Tx and P c/e

where T c/e
Tx are the transmission time and power of a
central server/miner (c) or an edge device (e). The additional
term EBC for BFL is associated to mining operations of the
PoW. We measure that consumed energy based on the model
proposed in [40] and according to the following equation:

EBC = Ph

1
λ

,

(17)

where Ph is the total hashing power of the network and λ is
the block generation rate.

VI. PERFORMANCE EVALUATION

In this section, we ﬁrst describe the experimental settings
adopted to compare the three federated approaches and then,
we present numerical results.

A. Simulation Setup

We use the Extended MNIST (EMNIST) dataset available on
Tensorﬂow Federated (TFF) library [41]. The input features
are black and white images that represent handwritten digits
in [0, 9], coded in a matrix of 28 × 28 pixels. Considering only
digits, it contains 341 873 training examples and 40 832 test
samples, both divided across 3 383 users. The training and the
test sets share the same users’ list so that each user has at least
one sample. In the EMNIST dataset, all the clients have a rich
number of samples for all the classes, thus data distribution
across them can be considered as IID. To evaluate the targeted
federated mechanisms in more challenging settings, we create
a new version of the EMNIST dataset, called EMNISTp, by
randomly restricting each client dataset to 3 classes only. Fig. 3
shows the available samples of the ﬁrst 4 clients, for both
versions of the dataset.

Fig. 3: Distribution of samples across the four ﬁrst clients for
both EMNIST and EMNISTp federated datasets.

To correctly classify these samples we choose a feed-
forward neural network (FFNN) with an input layer of 784
neurons, two hidden layers of 200 neurons activated with the
rectiﬁed linear unit (ReLU) function, and an output layer of 10
neurons with Softmax activation function. In total, the number
of trainable parameters |w| is 199 210. Assuming that each
parameter requires 4 bytes in memory, i.e., size of a ﬂoat32
variable, the total amount of space required (Sw) is 796.84 kB.
We opted for a FFNN to reproduce a realistic scenario where
edge devices might not have enough computational power to
train more sophisticated deep learning mechanisms, like NNs
based on convolutional architectures. Despite of its simplicity,
the selected FFNN model accurately classiﬁes the digits of the
EMNIST dataset, as shown next.

The three FL algorithms are implemented with Tensor-
ﬂow [23], Tensorﬂow Federated [42] and Keras [43] libraries.
We have extended the Bitcoin model provided by Block-
Sim [44] to simulate the blockchain behavior. BlockSim is an
event-based simulator that characterizes the operations carried
out to store data in a blockchain, from the submission of
transactions to mining blocks and reaching consensus in a de-
centralized manner. Accordingly, BlockSim allows simulating
the delays added by the blockchain in a BFL application, i.e.,
the TBC parameter deﬁned in Section IV.

We create a validation set by choosing a subset of 200
clients from the test set. The accuracy is computed at the end
of each learning round using the global model on the training
and validation sets. At the end of each simulation, we evaluate
the performance on the test set.

As for Etrain and Ttrain, we have used Carbontracker [45],
a Python library that periodically samples the hardware energy
consumption and measures the execution time. Moreover, P c
T x
is set to 20 dBm and P e
T x = 9 dBm. Table 2 reports all the
other parameters used in our simulations. We note here that
we have used the same FL parameters for a fair comparison,
being the number of rounds R of CFL and GFL equivalent to
the main chain’s length (Nchain) in BFL. In such a way, we
guarantee that the number of global rounds of each learning
algorithm is the same.

B. Result Analysis

Fig. 4 reports the accuracy of each algorithm implementa-
tion on the two considered datasets. CFL and BFL achieve
the best accuracy (both close to 0.9), instead, GFL presents
lower values. This result validates the claim that, under similar
setups as in our simulations (i.e., each block contains m local
updates organized in transactions), the central parameter server
of CFL can be replaced by a blockchain network, properly

Parameter

Description

Value

g
n
i
n
r
a
e
L

.
d
e
F

n
i
a
h
c
k
c
o
l
B

)
x
a
1
1
.
2
0
8

E
E
E
I
(

n
o
i
t
a
c
i
n
u
m
m
o
C

|w|
Sw
η
N
E
R
m
B
(cid:96)i

Nchain
BI
NB
Nm
CP 2P
SH
SB
Str
P e
Tx
P c
tx
σleg
Nsc
Nss
Te
TSIFS
TDIFS
TPHY
THE-SU
Ls
LRTS
LCTS
LACK
LSF
LMAC
CW

Number of model parameters
Model parameters size
Learning rate
Number of total clients
Local epochs number
Number of rounds
Number of clients for each round
Batch size
Local loss function

199 210
796.84 kB
0.2
3382
5
200
200
20
Sparse Cat. Crossentropy

Number of blocks in the main chain
Block interval
Number of blockchain nodes
Number of miners
Capacity of P2P links
Block header size
Block size
Transaction size

200
15 s
200
10
100 Mbps
25 KB
160.368 MB
796.84 KB

Tx power for edge devices
Tx power for a central server
Legacy OFDM symbol duration
Number of subcarriers (20 MHz)
Number of spatial streams
Empty slot duration
SIFS duration
DIFS duration
Preamble duration
HE single-user ﬁeld duration
Size OFDM symbol
Length of an RTS packet
Length of a CTS packet
Length of an ACK packet
Length of service ﬁeld
Length of MAC header
Contention window (ﬁxed)

9 dBm
20 dBm
4 µs
234
1
9 µs
16 µs
34 µs
20 µs
100 µs
24 bits
160 bits
112 bits
240 bits
16 bits
320 bits
15

rounds of the algorithms). The fastest and the most energy-
efﬁcient algorithm is GFL: it is able to save the 18% of training
time, the 68% of energy and the 51% of data to be shared with
respect to the CFL solution. However, GFL main drawback
resides in the poor accuracy achieved, as stated above. BFL is
the slowest and the most energy-hungry federated implementa-
tion, mainly due to the overhead introduced by the blockchain
network to secure data in a decentralized way. Additionally,
it is to be noted that communication is the most energy-
consuming task for both CFL and GFL. Instead, for BFL it is
the mining process, which drains alone 72900 kWh, i.e., the
96% of the total energy. We highlight here that our comparison
may be unfair in this respect, since both CFL and GFL are
not including any security mechanism. However, we believe
that it is worth to include BFL in our analysis on distributed
versus centralized federated learning, because from our results,
it emerges that the secure and decentralized method introduced
by the blockchain network, despite increasing algorithm costs,
does not jeopardize its accuracy compared to its centralized
counterpart CFL. Finally, GFL is the implementation that
requires the lowest communication overhead. To be more
precise, in this case, we need to include an extra cost to share
the global model across the nodes at the end of the last round
(not considered in the table), which is approximately 0.16 GB
(the cost of one extra round).

TABLE 2: Simulation parameters.

VII. OPEN ISSUES AND RESEARCH DIRECTIONS

dimensioned, without compromising the learning accuracy. On
the other hand, GFL achieves a validation accuracy around 0.5
after 200 rounds. We justify this behavior by noticing that,
the model received
before the CLIENTUPDATE procedure,
from the previous node in the sequence is merged with that
in the previous round. For the earliest training rounds, there
is a high probability that the merging procedure is with a
fresh model that has never been trained, hence disrupting the
knowledge form the previous clients. We analyze more in-
depth this phenomenon in Section VII-A.

Fig. 4: Training and validation accuracy of CFL, BFL, and
GFL on EMNIST and EMNISTp datasets.

The numerical values of training, validation and test ac-
curacy are reported in Table 3, which also details the con-
vergence time of each algorithm, the percentage of energy
consumed in the computations (as a percentage of the total
energy consumed), the total amount of energy needed and the
communication overhead (i.e., data to be shared during the

A. Open Aspects of GFL

As described before, GFL is not able to achieve the same
accuracy level as CFL and BFL. We identify two possible
reasons for this behavior:

1) The number of rounds is not enough to converge: the
number of visited nodes might not be sufﬁcient to hit
an acceptable accuracy.

2) The merge step negatively impacts the performance
of the learning algorithm: the model received in the
previous round and stored in the local cache slows down
the learning process.

To verify the ﬁrst hypothesis, we execute GFL algorithm
changing the number of rounds (R = {200, 400, 800}) and
varying the number of local computations (E = {5, 10}).
Table 4 shows the results obtained. Considering the EMNIST
dataset, the best results are achieved with R = 800 and
E = {5, 10}, i.e., a higher test accuracy of 0.66, but still
lower than CFL and BFL. Moreover, the model is overﬁtting
with R = 800 rounds; hence, a regularization method would
be needed, when increasing the number of rounds. On the
EMNISTp dataset, the accuracy is even lower for each com-
bination of the hyperparameters tested.

To verify the second hypothesis, we run GFL algorithm
without the merge step (GFL-NM). The pesudocode of this
algorithm is the same in Algorithm 3 but replacing the old
Line 10 with the new command wt
. Thus, in
ki
GFL-NM, given a sequence of clients St the model is trained
incrementally on the client’s datasets. GFL-NM achieves a

← wt

ki−1

0100200FL round00.51AccuracyCFL0100200FL round00.51AccuracyBFL0100200FL round00.51AccuracyGFLTrain (EMNIST)Val (EMNIST)Train (EMNISTp)Val (EMNISTp)Acc. Training

Acc. Validation

Acc. Test

Conv. Time (s)

Comp. Energy (%)

Tot. Energy (kWh)

Comm. Overhead (GB)

CFL
BFL
GFL

0.9 (0.76)
0.88 (0.74)
0.44 (0.36)

0.87 (0.77)
0.87 (0.78)
0.42 (0.12)

0.86 (0.76)
0.86 (0.77)
0.41 (0.11)

46458.56 (45571.86)
51036.87 (50077.75)
38201.67 (36821.67)

6.98e − 04 (5.48e − 04)
9.59e + 01 (9.59e + 01)
1.78e − 03 (8.93e − 04)

3094.2 (3094.2)
75994.2 (75994.2)
997.56 (997.55)

63.75
6725.33
31.87

TABLE 3: Simulation results on EMNIST (EMNISTp) datasets.

R

200
400
800
200
400
800

E

5
5
5
10
10
10

Acc. Training

Acc. Validation

Acc. Test

Conv. Time (s)

Comp. Energy (%)

Tot. Energy (kWh)

0.44 (0.36)
0.55 (0.41)
0.85 (0.64)
0.57 (0.58)
0.66 (0.37)
0.94 (0.58)

0.42 (0.12)
0.51 (0.16)
0.67 (0.19)
0.4 (0.09)
0.47 (0.17)
0.67 (0.3)

0.41 (0.11)
0.5 (0.15)
0.66 (0.17)
0.41 (0.1)
0.48 (0.16)
0.67 (0.29)

36401.67 (35021.67)
72791.81 (70029.31)
145651.71 (140069.23)
37377.47 (35448.58)
74989.72 (70857.46)
149448.14 (141730.77)

1.88e − 03 (9.41e − 04)
1.85e − 03 (9.39e − 04)
1.87e − 03 (9.38e − 04)
2.59e − 03 (1.24e − 03)
2.59e − 03 (1.22e − 03)
2.59e − 03 (1.23e − 03)

946.08 (946.07)
1892.17 (1892.15)
3784.34 (3784.3)
946.09 (946.08)
1892.18 (1892.16)
3784.36 (3784.31)

TABLE 4: Simulation results of GFL on EMNIST (EMNISTp) datasets with higher number of rounds and local computations.

training accuracy of ∼ 1.0 (0.94), a validation accuracy of
0.94 (0.78) and a test accuracy of 0.93 (0.78) on the EMNIST
(EMNISTp) dataset (see Fig. 5), higher than CFL and BFL.
These results suggest that the MERGE step compromises the
training performance. In fact, at the beginning of the learning
process, there is a high probability that a model visits a node
that has never been visited before and with lastM odel storing
initialization values. In this case, the received model is merged
with a model that has never been trained before, as shown in
Algorithm 3, which negatively impacts the resulting merged
weights.

Fig. 5: Training and validation accuracy of GFL-NM.

In conclusion, we have seen that both 1) and 2) inﬂuence the
achieved accuracy. Moreover, GFL-NM solves the accuracy
problem of standard GFL and reaches the best performance
from all
the metrics point of view. In our opinion, and
encouraged by our results, the investigation of new methods
for merging the model updates from the distributed sources to
achieve faster and higher accuracy is an interesting and open
research line. To the best of our knowledge, there are still very
few works that go in this direction in the literature. In [19],
the authors implement an incremental version of GFL with a
single round on the edge devices using the entire local dataset,
and, hence, without requiring any merge step. Similarly, [46]
proposes an iterative continual learning algorithm, where a
model is trained incrementally on the local datasets without
applying any merge operation.

B. Open Aspects of BFL

Blockchain technology, while enabling a reliable and secure
FL operation, entails very high overheads in terms of time
and energy for the sake of keeping decentralization. The
performance of a blockchain, typically measured in transac-
tions per second (tps), together with the granted degree of
security, strongly depends on the nature of the blockchain (e.g.,
degree of visibility, type of consensus, mining protocol), its
conﬁgurable parameters (e.g., block interval, difﬁculty), and
the size of the P2P network maintaining it. Furthermore, the
necessary energy to maintain a blockchain is correlated to
its performance in tps and security, thus leading to the well-
known performance, security, and energy trilemma.

To showcase the effect of using different types of blockchain
networks, Fig. 6 shows the total delay incurred by the
blockchain to the FL operation to generate up to 200 blocks
under different blockchain conﬁgurations. Notice that, in the
proposed setting, each block is equivalent to an FL round.
In particular, we vary the total number of miners (N =
{1, 10, 100}) and the block interval (BI = {5, 15, 600} s),
which affect the time required to achieve consensus.

First, a higher number of miners leads to a higher fork
probability, provided that more nodes need to agree on the
same status of the ledger. By contrast, a higher block interval
allows mitigating the effect of forks, since the probability that
two miners mine a block simultaneously is lower [47].

As shown in Fig. 6, the blockchain delay increases with
the block interval (BI), which indicates the average time for
mining a block. Notice that, in a PoW-based blockchain, the
block interval is ﬁxed by tuning the mining difﬁculty according
to the total computational power of miners. As for the impact
of N on the delay, its effects on the delay are more noticeable
for low BI values. In particular, a higher fork probability is
observed as N increases, thus incurring additional delays to
the FL application operating on top of the blockchain.

To optimize the performance of a blockchain, a widely
adopted approach consists of ﬁnding the best block generation
rate [27], which is controlled by tuning the mining difﬁculty.
Other approaches consider optimizing the block size [48],
which ﬁts better scenarios where the intensity of transactions
arrivals depends on the nature of the application running on

050100150200FL round00.20.40.60.81AccuracyTrain (EMNIST)Val (EMNIST)Train (EMNISTp)Val (EMNISTp)methods, like the poor accuracy achieved by GFL and the
blockchain overhead in BFL. Regarding GFL, we have ar-
gued that the main drawback lays in the method used to
merge model updates across the algorithm steps. We have
demonstrated that with an incremental approach, the modiﬁed
version of GFL is able to outperform CFL and BFL. As
for BFL, we have indicated that possible optimizations go
in the direction of ﬁnding the best block generation rate and
block size. Moreover, we have reasoned on the possibility of
reducing the time complexity by including the global model in
a block, which is aggregated by the same miner building the
block. Finally, we have pointed out the importance of further
studies on the implication of model inconsistencies due to
the fact that the blockchain cannot be perfectly shared and
accessed by (all) the FL devices.

APPENDIX A
PROOFS

A. Proof of Theorem I

(cid:16)

(cid:17)
B |w|

|Dmax||w|+2 |Dmax|

Let us consider the procedure CLIENTUPDATE, whose time
complexity is E
. In fact, a single
client k performs the training phase on its local dataset Dk
along E local epochs and updates the model parameters. The
ﬁrst operation has a time complexity of |Dk||w| and the second
2 |Dk|
B |w|. The update it is executed a number of times equal to
|Dk|
B , and requires a product and a sum. Each client performs
E local epochs, so the total cost is:

(cid:18)

E

|Dk||w|+2

(cid:19)

|w|

|Dk|
B

(cid:88)

k∈St

(18)

To obtain an upper bound that does not depend on k, we can
use |Dmax| as an upper bound of |Dk|:

(cid:88)

k∈St

(cid:18)

E

|Dk||w|+2

(cid:19)

|w|

≤

|Dk|
B

(cid:18)

mE

|Dmax||w|+2

|Dmax|
B

(cid:19)

|w|

(19)

We can divide the MAIN procedure in Algorithm 1 into two
blocks. The ﬁrst, up to Line 10, has a cost upper bounded by

(cid:18)

mE

|Dmax||w|+2

|Dmax|
B

(cid:19)

|w|

+ 2|w|m

(20)

In parallel every client downloads the global model, executes
CLIENTUPDATE, and sends the updated parameters back to
the server. The download and upload operations have a time
complexity proportional to |w|. Considering that the same
procedure is repeated by m clients, the upper bound in (20)
easily follows. The second block starts from Line 10, where
the server aggregates the local updates and computes the
new global model. The number of arithmetical operations
performed is:

2|w|m

(21)

Combining (20) and (21), and considering the number of total

Fig. 6: Blockchain delay as a function of the number of miners
(N ) and the block interval (B). The fork probability associated
with each N is shown in red.

top of the blockchain (e.g., FL updates provided by clients).
Regarding the communication cost of BFL, it can be im-
proved by leveraging the computational capacity of blockchain
miners to speed up the FL operation. In particular, instead of
including individual local models in a block, each block can
bring a global model, aggregated by the miner responsible for
building the block. This approach has been widely adopted in
the literature (see, e.g., [49]), and would lead to a reduced time
complexity and communication cost (see Appendix A-B).
important
FL lies

regarding
another
of
blockchain-enabled
decentralization on the learning procedure. In this paper, we
have assumed that
the blockchain is perfectly shared and
accessed by FL devices to carry out training, thus acting as a
central orchestrating server. However, the decentralized data
sharing in blockchain naturally leads to model inconsistencies,
provided that different FL devices can use the information
from different blocks to compute local model updates.

implications

Finally,

aspect

open

the

in

VIII. CONCLUSIONS

Decentralized server-less federated learning is an appealing
solution to overcome CFL limitations. However, ﬁnding the
best approach for each scenario is not trivial due to the lack of
comprehensive comparisons. In this work, we have proposed
a complete overview of these techniques and evaluated them
through several key performance indicators: accuracy, com-
putational complexity, communication overhead, convergence
time and energy consumption. To do so, we have proposed a
comprehensive theoretical analysis and an implementation of
these algorithms.

An extensive simulation campaign has driven our analysis.
From numerical results, it emerges that GFL is able to save
the 18% of training time, the 68% of energy and the 51% of
data to be shared with respect to the CFL solution, however
with lower accuracy. Instead, BFL represents a viable solution
for implementing decentralized learning with a high accuracy
and level of security at the cost of an extra energy usage and
data sharing.

Moreover, we have discussed some open issues and fu-
ture research directions for the two decentralized federated

rounds R required to reach convergence, the total cost of CFL
is given by:

(cid:20)

(cid:18)

R

mE

|Dmax||w|+2

(cid:19)

(cid:21)

+ 4m|w|

=

(22)

RmE|Dmax||w|+2RmE

|w|+4Rm|w|

|w|

|Dk|
B
|Dk|
B

The ﬁrst addend in (22) is the dominant term for the asymp-
totic time analysis, so this completes the proof to obtain (3).
When it comes to the communications overhead of CFL,
the result easily follows considering that, for each round, each
clients downloads and uploads the model parameters.

And the communication overhead is:

R(2|w|m + NB|w|)

(27)

C. Proof of Theorem III

Let ki be a client in the sequence [k1, ..., km]. Following the
steps of Algorithm 3, three main operations are performed:
1) MERGE, 2) CLIENTUPDATE and 3) send of the model
parameters to the next client of the sequence. The ﬁrst one
is the average of two model parameters, so its cost is 2|w|.
The cost of the second operation has already been computed
in (19) and the cost of parameter sharing is |w|. By summing
up these contributions we obtain:
(cid:18)

(cid:19)

(cid:20)

(cid:21)

|Dmax|
B

B. Proof of Theorem II

m

E

|Dmax||w|+2

|w|

+ 3|w|

(28)

In each algorithm’s round, every client in S t has to down-
load the latest block from the closest edge server (miner) to
obtain the current global model. These operations, as described
before, have a cost of |w|m and 2|w|m, respectively. Then,
after running the CLIENTUPDATE procedure in Algorithm 2,
clients submit the new model weights with a cost of |w|. These
steps are done by each node in S t (in total, m nodes), so the
total cost is:

This process is repeated for R rounds, so the time complexity
is:

(cid:20)

(cid:18)

Rm

E

|Dmax||w|+2

|Dmax|
B

(cid:19)

(cid:21)

|w|

+ 3|w|

,

(29)

where the ﬁrst addend is the dominant one.

Given that each client shares its local model only with the
following node in the sequence, the communication overhead
is given by (8).

(cid:32)

m

2|w|m + |w|m + E|Dmax||w|+

2E

|Dmax|
B

(cid:33)

|w|+|w|

When all

the local updates have been computed,

is
necessary to create a block, reach consensus throughout the
mining operation, and propagate the block across all
the
blockchain nodes. The cost of these operations is given by:

it

2l + m|w|NB

(24)

If we combine together (23) and (24), we obtain the total time
complexity of the algorithm

(cid:18)

R

3|w|m2 + E|Dmax||w|m+

2E

|Dmax|
B

|w|m + |w|m + 2l + m|w|NB

(cid:19)

(25)

The dominant addends are reported in (5).

The communication overhead of BFL can be easily derived

from the algorithm description.

In this analysis, we considered the less efﬁcient implemen-
tation, whereby each client has to perform the computation of
the new global model given the updates in the latest block. To
improve this, we can move the instruction in Line 8 outside
the for loop and execute it before the MINEBLOCK procedure.
In this way, the new block has size |w|, since it contains only
the parameters of the new model. Following the same analysis
described before, the computational complexity is:

(23)

APPENDIX B
EDGE CONNECTION MODEL

To compute the duration for transmitting model weights,
we assume IEEE 802.11ax channel access procedures [39],
which also include the overheads to carry out the Distributed
the
Coordination Function (DCF) operation. In particular,
duration of a packet transmission is deﬁned as:

TTx =TRTS + TSIFS + TCTS + TDATA+
TSIFS + TACK + TDIFS + Te,

(30)

where TRTS is the duration of the Ready-to-Send (RTS) control
frame, TSIFS is the Short Interframe Space (SIFS) duration,
TCTS is the duration of the Clear-to-Send (CTS) control frame,
TDATA is the duration of the data payload, TACK is the duration
of the acknowledgement (ACK) frame, and Te is the duration
of an empty slot.

To compute the duration of each type of IEEE 802.11ax
control frame, i.e., RTS, CTS, and ACK, we compute them
as:

TRTS/CTS/ACK = TPHY +

(cid:24) LSF + LRTS/CTS/ACK
Ls

(cid:25)

σleg,

(31)

where TPHY is the duration of the PHY preamble, LSF is the
length of the service ﬁeld (SF), LRTS/CTS/ACK is the length
of the control frame, Ls
is the length of an Orthogonal
Frequency Division Multiplexing (OFDM) symbol, and σleg
is the duration of a legacy OFDM symbol.

As for the duration of the data payload, it is computed as:

(cid:24) LSF + LMAC + LDATA
Ls

(cid:25)

σ,

(32)

O(R(mE|Dmax||w|+2l + NB|w|))

(26)

TDATA = THE-SU +

where THE-SU is the duration of the high-efﬁciency (HE)
single-user ﬁeld, LMAC is the length of the MAC header, LDATA
is the length of a single data packet (in our case, it matches
with the model size, Sw), and σ is the duration of an OFDM
symbol. The number of bits per OFDM symbol will vary, so
as the effective data rate, based on the employed modulation
and coding scheme (MCS), which depends on the transmission
power used.

REFERENCES

[1] “Ericsson mobility report november 2021,” 2021.
[2] J. Wang, J. Liu, and N. Kato, “Networking and communications in au-
tonomous driving: A survey,” IEEE Communications Surveys Tutorials,
vol. 21, no. 2, pp. 1243–1274, 2019.

[3] “AI and Compute,” May 2018.
[4] E. Strubell, A. Ganesh, and A. McCallum, “Energy and policy con-
siderations for deep learning in nlp,” arXiv preprint arXiv:1906.02243,
2019.

[5] T. Zhang and S. Mao, “An introduction to the federated learning
standard,” GetMobile: Mobile Computing and Communications, vol. 25,
no. 3, pp. 18–22, 2022.

[6] Q. Li, Z. Wen, Z. Wu, S. Hu, N. Wang, Y. Li, X. Liu, and B. He, “A
survey on federated learning systems: vision, hype and reality for data
privacy and protection,” arXiv preprint arXiv:1907.09693, 2019.
[7] M. Chen, D. G¨und¨uz, K. Huang, W. Saad, M. Bennis, A. V. Feljan,
and H. V. Poor, “Distributed learning in wireless networks: Recent
progress and future challenges,” IEEE Journal on Selected Areas in
Communications, 2021.

[8] S. Deng, H. Zhao, W. Fang, J. Yin, S. Dustdar, and A. Y. Zomaya,
“Edge intelligence: The conﬂuence of edge computing and artiﬁcial
intelligence,” IEEE Internet of Things Journal, vol. 7, no. 8, pp. 7457–
7469, 2020.

[9] Z. Zhou, X. Chen, E. Li, L. Zeng, K. Luo, and J. Zhang, “Edge
intelligence: Paving the last mile of artiﬁcial intelligence with edge
computing,” Proceedings of the IEEE, vol. 107, no. 8, pp. 1738–1762,
2019.

[10] E. Ahvar, A.-C. Orgerie, and A. Lebre, “Estimating energy consumption
of cloud, fog and edge computing infrastructures,” IEEE Transactions
on Sustainable Computing, pp. 1–1, 2019.

[11] J. Koneˇcn`y, H. B. McMahan, F. X. Yu, P. Richt´arik, A. T. Suresh, and
D. Bacon, “Federated learning: Strategies for improving communication
efﬁciency,” arXiv preprint arXiv:1610.05492, 2016.

[12] C. Xie, S. Koyejo, and I. Gupta, “Asynchronous federated optimization,”

arXiv preprint arXiv:1903.03934, 2019.

[13] P. Kairouz, H. B. McMahan, B. Avent, A. Bellet, M. Bennis, A. N.
Bhagoji, K. Bonawitz, Z. Charles, G. Cormode, R. Cummings, et al.,
“Advances and open problems in federated learning,” arXiv preprint
arXiv:1912.04977, 2019.

[14] V. Mothukuri, R. M. Parizi, S. Pouriyeh, Y. Huang, A. Dehghantanha,
and G. Srivastava, “A survey on security and privacy of federated
learning,” Future Generation Computer Systems, vol. 115, pp. 619–640,
2021.

[15] L. Barbieri, S. Savazzi, M. Brambilla, and M. Nicoli, “Decentralized
federated learning for extended sensing in 6g connected vehicles,”
Vehicular Communications, vol. 33, p. 100396, 2022.

[16] A. Lalitha, S. Shekhar, T. Javidi, and F. Koushanfar, “Fully decentralized
federated learning,” in Third workshop on Bayesian Deep Learning
(NeurIPS), 2018.

[17] R. Orm´andi, I. Heged¨us, and M. Jelasity, “Gossip learning with lin-
ear models on fully distributed data,” Concurrency and Computation:
Practice and Experience, vol. 25, no. 4, pp. 556–571, 2013.

[18] L. Giaretta and ˇS. Girdzijauskas, “Gossip learning: Off the beaten
path,” in 2019 IEEE International Conference on Big Data (Big Data),
pp. 1117–1124, IEEE, 2019.

[19] M. Miozzo, Z. Ali, L. Giupponi, and P. Dini, “Distributed and multi-task
learning at the edge for energy efﬁcient radio access networks,” IEEE
Access, vol. 9, pp. 12491–12505, 2021.

[20] F. Wilhelmi, L. Giupponi, and P. Dini, “Blockchain-enabled server-less

federated learning,” arXiv preprint arXiv:2112.07938, 2021.

[21] X. Qiu, T. Parcollet, D. J. Beutel, T. Topal, A. Mathur, and N. D.
Lane, “Can federated learning save the planet?,” arXiv preprint
arXiv:2010.06537, 2020.

[22] S. Savazzi, V. Rampa, S. Kianoush, and M. Bennis, “An energy and
carbon footprint analysis of distributed and federated learning,” arXiv
preprint arXiv:2206.10380, 2022.

[23] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin,
S. Ghemawat, G. Irving, M. Isard, et al., “{TensorFlow}: A system
for {Large-Scale} machine learning,” in 12th USENIX symposium on
operating systems design and implementation (OSDI 16), pp. 265–283,
2016.

[24] J. Verbraeken, M. Wolting, J. Katzy, J. Kloppenburg, T. Verbelen, and
J. S. Rellermeyer, “A survey on distributed machine learning,” ACM
Computing Surveys (CSUR), vol. 53, no. 2, pp. 1–33, 2020.

[25] B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. Ag¨uera y
Arcas, “Communication-efﬁcient learning of deep networks from de-
centralized data,” in Artiﬁcial intelligence and statistics, pp. 1273–1282,
PMLR, 2017.

[26] T. Li, A. K. Sahu, A. Talwalkar, and V. Smith, “Federated learning:
Challenges, methods, and future directions,” IEEE Signal Processing
Magazine, vol. 37, no. 3, pp. 50–60, 2020.

[27] H. Kim, J. Park, M. Bennis, and S.-L. Kim, “Blockchained on-device
federated learning,” IEEE Communications Letters, vol. 24, no. 6,
pp. 1279–1283, 2019.

[28] U. Majeed and C. S. Hong, “Flchain: Federated learning via mec-enabled
blockchain network,” in 2019 20th Asia-Paciﬁc Network Operations and
Management Symposium (APNOMS), pp. 1–4, IEEE, 2019.

[29] X. Bao, C. Su, Y. Xiong, W. Huang, and Y. Hu, “Flchain: A blockchain
for auditable federated learning with trust and incentive,” in 2019 5th
International Conference on Big Data Computing and Communications
(BIGCOM), pp. 151–159, 2019.

[30] I. Heged¨us, G. Danner, and M. Jelasity, “Decentralized learning works:
An empirical comparison of gossip learning and federated learning,”
Journal of Parallel and Distributed Computing, vol. 148, pp. 109–124,
2021.

[31] A. Nilsson, S. Smith, G. Ulm, E. Gustavsson, and M. Jirstrand, “A per-
formance evaluation of federated learning algorithms,” in Proceedings
of the second workshop on distributed infrastructures for deep learning,
pp. 1–8, 2018.

[32] A. Lacoste, A. Luccioni, V. Schmidt, and T. Dandres, “Quanti-
fying the carbon emissions of machine learning,” arXiv preprint
arXiv:1910.09700, 2019.

[33] L. Lannelongue, J. Grealey, and M. Inouye, “Green algorithms: Quan-
tifying the carbon footprint of computation,” Advanced Science, vol. 8,
no. 12, p. 2100707, 2021.

[34] R. Schwartz, J. Dodge, N. A. Smith, and O. Etzioni, “Green ai,”
Communications of the ACM, vol. 63, no. 12, pp. 54–63, 2020.
[35] S. Nakamoto, “Bitcoin: A Peer-to-Peer electronic cash system,” tech.

rep., 2008.

[36] D. C. Nguyen, M. Ding, Q.-V. Pham, P. N. Pathirana, L. B. Le,
A. Seneviratne, J. Li, D. Niyato, and H. V. Poor, “Federated learning
meets blockchain in edge computing: Opportunities and challenges,”
IEEE Internet of Things Journal, 2021.

[37] F. Wilhelmi and L. Giupponi, “Discrete-time analysis of wireless
blockchain networks,” in 2021 IEEE 32nd Annual International Sympo-
sium on Personal, Indoor and Mobile Radio Communications (PIMRC),
pp. 1011–1017, IEEE, 2021.

[38] Z. Zheng, S. Xie, H. Dai, X. Chen, and H. Wang, “An overview of
blockchain technology: Architecture, consensus, and future trends,” in
2017 IEEE international congress on big data (BigData congress),
pp. 557–564, IEEE, 2017.

[39] B. Bellalta, “Ieee 802.11 ax: High-efﬁciency wlans,” IEEE Wireless

Communications, vol. 23, no. 1, pp. 38–46, 2016.

[40] N. Lasla, L. Alsahan, M. Abdallah, and M. Younis, “Green-pow: An
energy-efﬁcient blockchain proof-of-work consensus algorithm,” arXiv
preprint arXiv:2007.04086, 2020.

[41] “Module: tff.simulation.datasets.emnist | TensorFlow Federated.”
[42] A. Ingerman and K. Ostrowski, “Introducing tensorﬂow federated,”

External Links: Link Cited by, vol. 4, 2019.

[43] “Keras: the Python deep learning API.”
[44] M. Alharby and A. van Moorsel, “Blocksim: An extensible simulation
tool for blockchain systems,” Frontiers in Blockchain, vol. 3, 2020.

[45] L. F. W. Anthony, B. Kanding, and R. Selvan, “Carbontracker: Tracking
and predicting the carbon footprint of training deep learning models,”
arXiv preprint arXiv:2007.03051, 2020.

[46] Y. Huang, C. Bert, S. Fischer, M. Schmidt, A. D¨orﬂer, A. Maier,
R. Fietkau, and F. Putz, “Continual learning for peer-to-peer federated
learning: A study on automated brain metastasis identiﬁcation,” arXiv
preprint arXiv:2204.13591, 2022.

[47] Y. Shahsavari, K. Zhang, and C. Talhi, “A theoretical model for fork
analysis in the bitcoin network,” in 2019 IEEE International Conference
on Blockchain (Blockchain), pp. 237–244, IEEE, 2019.

[48] F. Wilhelmi, S. Barrachina-Mu˜noz, and P. Dini, “End-to-end latency
analysis and optimal block size of proof-of-work blockchain applica-
tions,” arXiv preprint arXiv:2202.01497, 2022.

[49] S. R. Pokhrel and J. Choi, “Federated learning with blockchain for au-
tonomous vehicles: Analysis and design challenges,” IEEE Transactions
on Communications, vol. 68, no. 8, pp. 4734–4746, 2020.

Elia Guerra received his master’s degree in Com-
puter Engineering at the University of Padova (Italy)
in 2021. Prior to this, he got his bachelor’s degree in
Information Engineering in 2019. During his studies,
he developed a passion for Machine Learning and
Algorithms. He is a Ph.D. student at the Technical
University of Catalonia (UPC) and he is currently
working at CTTC for the GREENEDGE (MSCA
ETN) project. His main research lines are distribut-
ed/decentralized and sustainable machine learning
algorithms.

Francesc Wilhelmi holds a Ph.D. in information
and communication technologies (2020), from Uni-
versitat Pompeu Fabra (UPF). He is currently work-
ing as a postdoctoral researcher in the Mobile Net-
works department at Centre Tecnol`ogic de Teleco-
municacions de Catalunya (CTTC).

Marco Miozzo received his M.Sc. degree in
Telecommunication Engineering from the University
of Ferrara (Italy) in 2005 and the Ph.D. from the
Technical University of Catalonia (UPC) in 2018.
In June 2008 he joined the Centre Tecnologic de
Telecomunicacions de Catalunya (CTTC). In CTTC
he has been involved in several EU founded projects.
He participated in several R&D projects, among
them SCAVENGE, 5G-Crosshaul, Flex5Gware and
SANSA, working on environmental sustainable mo-
bile networks with energy harvesting capabilities
through learning techniques. Currently he is collaborating with the EU
founded H2020 GREENEDGE (MSCA ETN) and SONATA (CHIST-ERA).
His main research interests are: sustainable mobile networks, green wireless
networking, energy harvesting, multi-agent systems, machine learning, green
AI, explainable AI.

Paolo Dini
received M.Sc. and Ph.D. from the
Universit‘a di Roma La Sapienza, in 2001 and 2005,
respectively. He is currently a Senior Researcher
with the Centre Tecnologic de Telecomunicacions
de Catalunya (CTTC). His current research interests
include sustainable networking and computing, dis-
tributed optimization and optimal control, machine
learning, multi-agent systems and data analytics.
His research activity is documented in almost 90
peer-reviewed scientiﬁc journals and international
conference papers. He received two awards from the
Cisco Silicon Valley Foundation for his research on heterogeneous mobile
networks, in 2008 and 2011, respectively. He has been involved in more
than 25 research projects. He is currently the Coordinator of CHIST-ERA
SONATA project on sustainable computing and communication at the edge
and the Scientiﬁc Coordinator of the EU H2020 MSCA Greenedge European
Training Network on edge intelligence and sustainable computing. He serves
as a TPC in many international conferences and workshops and as a reviewer
for several scientiﬁc journals of the IEEE, Elsevier, ACM, Springer, Wiley.

