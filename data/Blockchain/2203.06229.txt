1

2
2
0
2

r
p
A
9
2

]
L
P
.
s
c
[

2
v
9
2
2
6
0
.
3
0
2
2
:
v
i
X
r
a

Veracity: Declarative Multicore Programming with
Commutativity

ADAM CHEN, Stevens Institute of Technology, USA
PARISA FATHOLOLUMI, Stevens Institute of Technology, USA
ERIC KOSKINEN, Stevens Institute of Technology, USA
JARED PINCUS, Stevens Institute of Technology, USA

There is an ongoing effort to provide programming abstractions that ease the burden of exploiting multicore
hardware. Many programming abstractions (e.g., concurrent objects, transactional memory, etc.) simplify
matters, but still involve intricate engineering. We argue that some difficulty of multicore programming can be
meliorated through a declarative programming style in which programmers directly express the independence
of fragments of sequential programs.

In our proposed paradigm, programmers write programs in a familiar, sequential manner, with the added
ability to explicitly express the conditions under which code fragments sequentially commute. Putting such
commutativity conditions into source code offers a new entry point for a compiler to exploit the known
connection between commutativity and parallelism. We give a semantics for the programmer’s sequential
perspective and, under a correctness condition, find that a compiler-transformed parallel execution is equivalent
to the sequential semantics. Serializability/linearizability are not the right fit for this condition, so we introduce
scoped serializability and show how it can be enforced with lock synthesis techniques.

We next describe a technique for automatically verifying and synthesizing commute conditions via a new
reduction from our commute blocks to logical specifications, upon which symbolic commutativity reasoning
can be performed. We implemented our work in a new language called Veracity, implemented in Multicore
OCaml. We show that commutativity conditions can be automatically generated across a variety of new
benchmark programs, confirm the expectation that concurrency speedups can be seen as the computation
increases, and apply our work to a small in-memory filesystem and an adaptation of a crowdfund blockchain
smart contract.

1 INTRODUCTION
Writing concurrent programs is difficult. Researchers and practitioners, seeking to make life easier,
have developed better paradigms for concurrent programming such as concurrent objects [Herlihy
and Wing 1990], transactional memory [Harris and Fraser 2003; Herlihy et al. 2003; Herlihy and Moss
1993; Saha et al. 2006], actors [Armstrong 1997], parallel for [Allen et al. 2005], goroutines [Prabhakar
and Kumar 2011], ownership [rus 2014], composable atomicity [Golan-Gueta et al. 2015], etc. As
another strategy, compiler designers have instead sought to automatically parallelize programmers’
sequential programs (see, e.g. [Blume et al. 1994; Blume and Eigenmann 1992; Li et al. 1990]). Others
have devised more declarative and/or domain-specific programming models such as Fortress [Steele
Jr. 2005]. Some have extended concurrent collection programming with graph-based languages
for GPUs [Grossman et al. 2010]; some have introduced DSLs for grid computing [Orchard et al.
2010]; some delay side effects in blocks [Lindley 2007]; others aim to further separate high-level
programming from low-level concurrent programming (e.g. the Habanero project [Barik et al.
2009]). (See also the DAMP Workshop1.) Still others aim to improve proof techniques (e.g. [Brookes
1Proceedings of the Workshop on Declarative Aspects of Multicore Programming (DAMP), 2007–2012.

Authors’ addresses: Adam Chen, Stevens Institute of Technology, USA, achen19@stevens.edu; Parisa Fathololumi, Stevens
Institute of Technology, USA, pfatholo1@stevens.edu; Eric Koskinen, Stevens Institute of Technology, USA, eric.koskinen@
stevens.edu; Jared Pincus, Stevens Institute of Technology, USA, jpincus@stevens.edu.

2018. 2475-1421/2018/1-ART1 $15.00
https://doi.org/

Proc. ACM Program. Lang., Vol. 1, No. CONF, Article 1. Publication date: January 2018.

 
 
 
 
 
 
1:2

Adam Chen, Parisa Fathololumi, Eric Koskinen, and Jared Pincus

and O’Hearn 2016; Elmas 2010; Owicki and Gries 1976; Vafeiadis 2010]) so that verification tools
can complement the error-prone task of concurrent programming.

Meanwhile, it has been long known that commutativity can be used for scalability in concurrent
programming, dating back to the database community [Bernstein 1966; Korth 1983; Weihl 1988,
1983] and, more recently, for system building [Clements et al. 2015]. There have thus been a
growing collection of works aimed at exploiting commutativity [Dickerson et al. 2017, 2019;
Hassan et al. 2014; Herlihy and Koskinen 2008; Kulkarni et al. 2011, 2008, 2007; Ni et al. 2007],
synthesizing/verifying commutativity [Aleen and Clark 2009; Bansal et al. 2018; Gehr et al. 2015; Kim
and Rinard 2011; Koskinen and Bansal 2021], and using commutativity analysis for parallelization
in compilers [Rinard and Diniz 1997] and blockchain smart contracts [Pîrlea et al. 2021].

Despite works on declarative programming models for concurrency and separate works that
exploit commutativity, to our knowledge none have yet sought to combine the two by giving the
programmer (or automated analysis) agency in specifying commutativity.

Programming with commutativity. In this paper, we introduce a new sequential programming
paradigm in which explicit commutativity conditions are part of the language, to be exploited by
the compiler for concurrent execution. Specifically, we introduce commute statements of the form:

commute (expr) {{ stmt1 } { stmt2 }}

The idea is to allow programmers to continue to write programs using sequential reasoning and,
still from that sequential perspective, express the conditions under which statements commute. For
example, a programmer may specify that statements incrementing and decrementing a counter c
commute when the counter’s value is above 0. Putting commutativity directly into the language
has three key benefits:

(1) Sequential programming: Programmers need only reason sequentially, which is far more
approachable than concurrent reasoning. The semantics are simply that non-deterministic
behavior is permitted when commute conditions hold and, otherwise, resort to sequencing in
the order written.

(2) Sequential verification/inference: These commutativity conditions can be automatically verified

or even synthesized without the need to consider all interleavings.

(3) Parallel execution: Once commutativity conditions are available, they can be used within the
compiler and combined with lock synthesis techniques (e.g. [Cherem et al. 2008; Vechev et al.
2010] and some new ones) to generate parallel compiled programs.

Since programmers write with the sequential semantics in mind, we naturally would like a
concurrent semantics that is equivalent to the sequential semantics but, interestingly, the de facto
standards of serializability [Papadimitriou 1979] and linearizability [Herlihy and Wing 1990] do
not quite fit the bill. Parallelizing programs with arbitrarily nested (and sequentially composed)
commute blocks, involves a recursive sub-threading shape. We introduce scoped serializability to
describe a sufficient condition under which the concurrent semantics is equivalent to the sequential
semantics, when used with valid commutativity conditions. Scoped serializability is stronger than
serializability; though that may at first seem distasteful, it is necessary to capture the structure of
nested commute blocks, and it is not bad given that the program can be written by only thinking
of its sequential semantics in contrast to, say, transactional memory where the programmer must
decide how to organize their program into threads and atomic sections. Scoped serializability can
then be enforced through lock synthesis techniques, another area of ongoing research (e.g. [Cherem
et al. 2008; Flanagan and Qadeer 2003; Golan-Gueta et al. 2015; Vechev et al. 2010]). While such
existing techniques can be applied here, they are geared to a more general setting. We describe

Proc. ACM Program. Lang., Vol. 1, No. CONF, Article 1. Publication date: January 2018.

Veracity: Declarative Multicore Programming with Commutativity

1:3

alternative locking and re-write strategies that incur less mutual exclusion and are more geared
toward scoped serializability.

Although commute conditions can be written by programmers, we next focus on correctness and
describe how such conditions can be automatically verified and even inferred. We describe a novel
translation from commute programs to an embedding as a logical specification, for which recent
techniques [Bansal et al. 2018] can verify or synthesize commutativity conditions. The pieces don’t
immediately align: commute blocks occur in program contexts (with local/global variables) and our
language provides support for nested commute blocks as well as builtin primitives such as dictio-
naries. We show that, nonetheless, these program pieces can be translated to logical specifications,
which can be used to synthesize commute conditions via abstraction-refinement [Bansal et al. 2018].
Synthesized conditions can sometimes be overly complex and/or describe only trivial cases (due to
incompleteness in those algorithms) so a programmer may instead wish to manually-provide a
commute condition which we then verify.

We implement our work in a front-end compiler/interpreter for a new language called Veracity2,
built on top of Multicore OCaml [mul 2014] 4.12.0, and a new commutativity condition verifier/syn-
thesizer, using a variety of underlying SMT solvers. Veracity uses the recent highly-concurrent
hashtable implementation libcuckoo [lib 2013]. We will publicly release Veracity, as well as 30
benchmark programs which are, as far as we know, the first to include commute blocks. We provide
a preliminary experimental evaluation, offering promising evidence that (i) commute conditions
can be automatically verified/inferred, (ii) as expected, speedups can be seen as the computation
size grows, and (iii) commute blocks could be used in applications such as in-memory file systems
and blockchain smart contracts.

Contributions. In summary, our contributions are:
• commute statements, a new sequential language primitive that weakens sequential composi-
tion, to explicitly express commutativity conditions, which are then used for parallelization.

• (Sec. 4) Sequential and concurrent semantics for the language.
• (Sec. 5) A correctness condition for parallelization called scoped serializability that, with
commutativity, implies that the concurrent semantics are equal to the sequential semantics.
We also present methods for automatically enforcing scoped serializability.

• (Sec. 6) A method for automatic verification and synthesis of commute conditions, via an

embedding into ADT specifications.

• (Sec. 7) A frontend compiler/interpreter for a new language Veracity, built in Multicore OCaml

with libcuckoo and a new implementation of Servois [Bansal 2018].

• (Sec. 8) A preliminary evaluation demonstrating synthesized commute conditions, scaling

speedups, and relevant applications.

Our implementation of Veracity and all benchmark programs can be found in the supplemental
materials. Benchmark sources are also given in Apx. H.

Limitations. While we have implemented a multicore interpreter, we reserve back-end compi-
lation questions to future research: there is much to be explored there, particularly with respect
to optimization. When commute blocks have loops in them, we infer/verify commute conditions
by first instrumenting loop summaries [Ernst 2020; Kroening et al. 2008; Silverman and Kincaid
2019; Xie et al. 2017] with havoc/assume. We used Korn3 and some manual reasoning to increase
Korn’s precision, but we plan to fully automate loop support in the future. Scoped serializability
is formalized in Sec. 5.4 and we argue that it can be automatically enforced. Because automated

2A portmanteau of “verified” and “commutativity.”
3https://github.com/gernst/korn

Proc. ACM Program. Lang., Vol. 1, No. CONF, Article 1. Publication date: January 2018.

1:4

Adam Chen, Parisa Fathololumi, Eric Koskinen, and Jared Pincus

lock synthesis is largely an orthogonal research area, not a core contribution of our work, and
would involve implementing other orthogonal components (such as alias analysis), our prototype
implementation does not perform lock synthesis automatically. Rather, we manually applied the
procedure of Sec. 5.4 and detail how it applied to our benchmarks in Sec. 8.

2 OVERVIEW
We begin with some simple examples to illustrate how a programmer may use commute statements
and, from these examples, outline the challenges addressed in this paper. The following is perhaps
the most trivial example: commuting blocks that increment and decrement a value.

commute(true) { { c = c-x; } { c = c+y; } }

These two operations clearly commute: the final value of c is the same regardless of the order in
which the blocks are executed, due to the natural commutativity of integer arithmetic. Moreover,
this holds for any initial value of c, so we have used the trivial commutativity condition true.
Compiler optimizations can exploit simple situations like this where commutativity always holds
and parallelize with a lock to protect the data race [Rinard and Diniz 1997]. In this example, the
overhead of parallelization is not worth it, so let us move to more examples. Now consider a case
where the condition is not simply true, and must be specified.

Example 2.1 (Conditional commutativity - simple.vcy).

commute (c >a) {

𝔣1 : { t = foo (c >b); a = a - |t |; }
𝔣2 : { u = bar (c >a); }

}

Above we refer to the two blocks labeled 𝔣1,𝔣2 as the two “fragments” (or co-fragments) of the
commute statement4. We assume that foo and bar are pure but costly computations. In this case,
the programmer has written the commute condition c>a. Although 𝔣2 reads the variable a which
is written by 𝔣1, it only observes whether or not c>a. Moreover, 𝔣1 may modify a, but will only
decrease its value, which does not impact the boolean observation c>a made by 𝔣2. Hence, the
fragments commute whenever c>a initially. It is necessary to include a condition because when the
condition does not hold, the operations may not always commute (they only commute if foo returns
a value with magnitude greater than a-c, which may be computationally difficult to determine a
priori) and we must resort to sequential execution. Note that, throughout this paper, we mostly
use commute statements with only two fragments although our implementation supports 𝑁 -ary
commute statements.

We emphasize that a programmer needs to reason only in the sequential setting. However, as
detailed below, our compiler can transform the above program into a concurrent one, permitting
foo and bar to execute in parallel when c>a and resort to sequential execution otherwise. Note that
it is not possible to parallelize these computations with a simple dataflow analysis. The challenge is
that bar(c>a) cannot be moved above a = a - |t| in the absence of protection from the commute
condition. More generally, the intuition is that each fragment may make observations about the
state and also perform mutations to the state. These fragments can be parallelized if the mutations
made by one do not change the observations made by the other, which is captured by the notion of
commutativity and specified in the commute expression. Furthermore, the compiler does not need
to introduce any locks because there is a read-write in one fragment, but only a single read in the
other, and the commute condition ensures that reading before or after the write does not matter.

4The terminology is chosen to avoid nebulousness words like “blocks,” “statements,” and ”nested.”

Proc. ACM Program. Lang., Vol. 1, No. CONF, Article 1. Publication date: January 2018.

Veracity: Declarative Multicore Programming with Commutativity

1:5

Example 2.2 (Counter control flow - calc.vcy).

commute (c >0) {

𝔣1 : { x = calc1 (a); c = c + (x*x); }
𝔣2 : { if (c >0 && y <0)

{ c = c -1; z = calc2 (y); }

else

{ z = calc3 (y); } }

}

This example considers commute blocks with fragments that have control flow. Below
calc1/calc2/calc3 are again pure calculations. Here, again using only sequential reasoning,
the programmer has provided the commute condition c>0. This is necessary because, e.g., if c is
0 initially, then in one order c will be x*x and in the other order, c will be (x*x)-1. A compiler
can use this sequential commutativity condition to parallelize these operations, provided that it
also ensures a new form of serializability (detailed in the next subsection) through lock synthesis
techniques. In this case, a single lock must be synthesized to protect access to c, yet allow parallel
execution of calc1 with calc2/calc3 (i.e., 𝔣1 holds the lock only during the c read/write, and 𝔣2
holds the lock from the beginning until before the call to calc2/calc3). This enforces serializability
of the fragments, so we may treat them as atomic points in the execution, and then, due to the
commute condition, it does not matter which point occurs first.

This example also shows that other orthogonal strategies such as future/promises [Chatterjee
1989; Liskov and Shrira 1988] do not subsume commute blocks. A promise for the value of x must
be evaluated at the end of 𝔣1 before beginning 𝔣2 because x is used in 𝔣1. Promises cannot avoid the
data-flow dependency across 𝔣1 into 𝔣2. By contrast commutativity allows a compiler to execute the
fragments out of order, effectively relaxing the data dependency (i.e. although c may change, c>0
will not). (In future work one could combine promises with commute blocks.)

Commutative computation appears in many contexts that go beyond counters, e.g., vector or

matrix multiplication:

Example 2.3 (Multiplication - matrix.vcy).

commute (y == 0) {

{ int sc = scale (y); int y0 = y;

x = x* sc ; y = 3* y; z = z - 2* y0 ; }

{ int y0 = y;

x = 5* x; y = 4* y; z = 3* z - y0 ;
out = summarize (z); }}

Here the resulting vector (x,y,z) can be the same in either order of the blocks, but only in certain
cases. This is almost a trivial diagonal matrix multiplication, except that there is a non-diagonal
contribution to z. Under the condition that y=0 initially, however, the contribution vanishes. Our
compiler can exploit this commutativity and execute these fragments in parallel, synthesizing a
lock for the read/writes to x/y/z. If the scale and summarize computations are time-consuming
then there is a large payoff for parallelizing: locks are held for only small windows, and execution
of the costly scale and summarize can overlap.

It is increasingly common for programming languages to have builtin abstract datatypes (ADTs),
such as dictionaries/hashtables, queues, stacks, etc. These present further opportunities for sequen-
tial commutativity-based programming and automatic parallelization through highly concurrent
implementations of those ADTs. Consider the following example:

Example 2.4 (Builtin dictionaries - dict.vcy).

commute ( res ≠ input ) {

{ t = calc (x); stats [ res ] = t; }
{ y = stats [ input ]; y = y + x; }

𝔣1 :
𝔣2 :

}

Proc. ACM Program. Lang., Vol. 1, No. CONF, Article 1. Publication date: January 2018.

1:6

Adam Chen, Parisa Fathololumi, Eric Koskinen, and Jared Pincus

Above, stats is a dictionary/hashtable, with a put operation in fragment 𝔣1, and a get in 𝔣2. The
programmer (or our analysis) has provided a commute condition that the keys are distinct, using
sequential reasoning.

Hashtables have appealing commutativity properties [Bronson et al. 2010; Herlihy and Koskinen
2008; Ni et al. 2007], well understood sequential semantics and recent high-performance concurrent
implementations [Li et al. 2014; Liu et al. 2014] such as libcuckoo [lib 2013]. For this example,
the memory accesses do not conflict except on the hashtable. Thus, the compiler can translate
the commute block to be executed concurrently simply by using a hashtable with a concurrent
implementation like libcuckoo, with no other locking needed. That linearizable implementation
ensures the atomicity of the operations on stat and the commute condition (res ≠ input) ensure
that 𝔣1 will commute with 𝔣2 regardless of the interleaving. commute blocks that use other builtin
ADTs such as Sets, Stacks or Queues can similarly benefit from this strategy (locking may be
necessary when there are multiple such operations in each fragment).

As seen in the examples above, our work is particularly suited to settings where there are
mixtures of long pure computation, mixed with some commutative updates to shared memory.
Examples might include in-memory filesystems, machine learning, and smart contracts. In this
paper we focus on the theoretical foundations, algorithms, and verification/synthesis with some
common-sense empirical results, but defer real-world applications and integration to future work.

2.1 Semantics of commute
Our proposed language has no explicit fork or clone command and so a user cannot explicitly
express concurrency. Instead, a user (and, later, our automated techniques) can express sequential
nondeterminism of code fragments through commute statements. Parallelism is only introduced by
our compiler, which exploits the commute-specified allowance of nondeterminism. Incorporating
such commute statements into a programming language has semantic implications. A natural goal
is to provide a concurrent semantics that somehow corresponds to a sequential semantics, e.g.
trace inclusion, simulation, etc. However, the programming model is also distinct from many
prior models that involve explicit fork/clone, like transactional memory or concurrent objects
and their respective correctness conditions: serializability/opacity [Guerraoui and Kapalka 2008;
Papadimitriou 1979] or linearizability [Herlihy and Wing 1990].

Because of this, there is also a distinction in what correctness condition is appropriate. To see the
distinction from transactional memory and serializability, consider the following commute blocks:

Example 2.5 (Nested commute blocks – nested.vcy).

y := 0; x := 1;
commute ( true ) {

{ commute ( true )

𝔣1 : { x := 0; }
𝔣2 : { x := x * 2; } }

𝔣3 :{ if (x >2) y := 1; }

}

A notion of serializability adapted for this setting might require that any execution be equivalent
to some serial order of 𝔣1, 𝔣2, and 𝔣3. This would permit the interleaved execution:

y:=0, x:=1,

𝔣2 : x:=x*2,

𝔣3 : y:=1,

𝔣1 : x:=0

ending in a final state where x=0 and y=1. However, the semantics of commute in our language
does not permit a serial order where 𝔣3 occurs between (𝔣1;𝔣2) nor between (𝔣2;𝔣1).

In Section 4 we introduce sequential and concurrent semantics for a simple language augmented
with commute statements. From the programmer’s perspective, the semantics of commute statements
are naturally captured through nondeterminism. At runtime however, our interpreter uses a parallel

Proc. ACM Program. Lang., Vol. 1, No. CONF, Article 1. Publication date: January 2018.

Veracity: Declarative Multicore Programming with Commutativity

1:7

execution semantics through recursively nested threading. We define a correctness condition called
scoped serializability which is stronger than serializability yet ensures an equivalence between the
concurrent semantics and the sequential semantics. This equivalence means that the program can
be automatically parallelized. We then give some techniques for enforcing scoped serializability.
One can naïvely use mutual exclusion or existing lock synthesis techniques (which were designed
for programs with explicit forking). We then discuss improved locking techniques that are more
geared to this nested, pair-wise setting. We show that, in some cases, some sound reordering of
fragments can minimize or even eliminate the need for locking.

2.2 Automated Verification and Inference of commute Conditions
A key benefit of commute statements is that we can use sequential reasoning to discover commutativ-
ity conditions (i.e. we do not have to consider interleavings). Commute conditions are often simple
enough for programmers to write, though they must be correct in order for parallel execution to be
safe. In Sec. 6 we turn to this correctness issue and discuss how to automatically verify or even
infer these conditions.

A collection of recent techniques and tools have emerged pertaining to commutativity reasoning.
Bansal et al. [2018] used a counterexample-guided abstraction refinement (CEGAR) strategy in
the Servois tool [Bansal 2018] to synthesize commutativity conditions of object methods’ pre/post
specifications, but not the ad hoc code that appears in our commute blocks. Koskinen and Bansal
[2021] verified commutativity conditions of source code through reachability. We found that in this
setting, the reachability approach introduced many unnecessary, extraneous paths and variables
(reducing performance and complicating conditions), and that SMT solvers struggled in many
cases. Finally, Pîrlea et al. [2021] described an abstract interpretation that tracks the linearity
of assignments to determine when operations always commute. Yet our commute blocks need
conditions for commutativity and have also non-linear behaviors such as matrix multiplication,
modulus, calls to builtin ADTs, etc.

We enable automated verification or inference of commute conditions through a novel embedding
of commute block fragments into logical specifications, fit for verification with SMT or inference
with the abstraction-refinement of Bansal et al. [2018]. The translation is not immediate: commute
statements occur within the context of a program, mutate global/local variables, invoke builtin ADTs
such as dictionaries, and may involve nested commute statements. The idea is thus to capture the
program context for the given commute block as the pre-state, and then translate each fragment’s
code into a logical post-condition, describing how the fragment mutates the context. Builtin
operations are treated by “inlining” their specification and nested commute blocks can be treated
as sequential composition, thanks to their sequential semantics. Synthesized commute conditions
can then be reverse translated back into the source language. Sometimes these conditions may be
overly complex or too trivial and thus a programmer still may wish to provide their own, which
may be verified via the same embedding.

2.3 The Veracity Programming Language
We implemented our work in a soon-to-be-released prototype front-end compiler and interpreter
for a new language called Veracity, built on top of Multicore OCaml [mul 2014] and a new imple-
mentation of the Bansal et al. [2018] algorithm. Our interpreter performs the embedding to verify
and synthesize commute conditions and uses Multicore OCaml domains and a foreign function
interface to libcuckoo [lib 2013] for concurrent execution of commute statements.

We create a suite of 30 benchmark programs with commute blocks, which we will also release
publicly. We first show that Veracity can synthesize commutativity conditions for most of these
programs, which include a variety of program features: linear and non-linear, vector multiplication,

Proc. ACM Program. Lang., Vol. 1, No. CONF, Article 1. Publication date: January 2018.

1:8

Adam Chen, Parisa Fathololumi, Eric Koskinen, and Jared Pincus

builtin ADTs, modulus, nesting, arrays and loops. When these synthesized conditions were too
complex, we showed that better manual ones could be verified.

We next confirmed the expectation that concurrent execution offers a speedup over sequential
execution, and that speedup increases as non-conflicting computations increases. Although no
existing programs are written with commute statements, we examined two case studies in which
we adapted programs to use commute statements, including an Algorand smart contract and an
in-memory file system.

3 PRELIMINARIES
We provide some basic background and definitions used throughout this paper.

States. A state 𝜎 ∈ Σ is a partial mapping 𝑉 𝑎𝑟𝑛𝑎𝑚𝑒𝑠 ⊔ 𝑀𝑒𝑚𝐿𝑜𝑐𝑠 ⇀ 𝑉 , i.e. the disjoint union of
variable names and memory locations, to program values. We assume a notion of scope, and that
each program variable statically refers to a unique element of the state. We assume that states can
be composed and decomposed by scope, i.e., 𝜎 = 𝜎0 ⊕ 𝜎1, and that relations on Σ are also relations
over the decomposed states. We work with equality on states ≃ in the usual way (e.g., equivalence
of valuation of accessible, primitive variables).

Syntax. We begin with syntax and contextual semantics for a standard programming language.

The language involves constants 𝑐, references, mutable variables and declarations:

𝑐
𝑙𝑣𝑎𝑙

::= 𝑖𝑛𝑡 | 𝑠𝑡𝑟𝑖𝑛𝑔 | 𝑏𝑜𝑜𝑙 | () | 𝑟𝑒 𝑓
::= varname | 𝑟𝑒 𝑓

𝑣-𝑑𝑒𝑐𝑙𝑠
𝑣-𝑑𝑒𝑐𝑙𝑠 ′

::= 𝜖 | 𝑣-𝑑𝑒𝑐𝑙𝑠 ′
::= 𝑡 𝑣 = 𝑒 | 𝑡 𝑣 = 𝑒, 𝑣-𝑑𝑒𝑐𝑙𝑠 ′

We next define redexes in the usual way:

𝑟

::= 𝑣 | deref 𝑝 | 𝑐0 [𝑐1] | new 𝑡 [𝑐] | new hashtable[𝑡0, 𝑡1] | uop 𝑐 | 𝑐0 bop 𝑐1

| true?𝑒0 : 𝑒1 | false?𝑒0 : 𝑒1 | 𝑐.fieldname | 𝑙𝑣𝑎𝑙 = 𝑐 | 𝑡 𝑣 = 𝑐 | while(𝑒){𝑠} | skip; 𝑠
| if(true){𝑠0}else{𝑠1} | if(false){𝑠0}else{𝑠1} | commute(false){{𝑠0}{𝑠1}}

Our language includes typical constructors, arrays, structures, as well as “builtin” ADTs. (Our
theory applies to general ADTs, but for illustration here we use hashtables.) The new feature of
the language is the commute statement, which we discuss in the next section. All semantics have a
commute(false) redex, which is evaluated sequentially. commute(true) is semantics-specific; see
Sec. 4. We next define contexts in the usual way. The notation 𝐻 [𝑟 ] means “𝐻 with the • replaced
by 𝑟 ”.

𝐻 ::= • | 𝐻 ; 𝑠 | uop 𝐻 | 𝑐 bop 𝐻 | 𝐻 bop 𝑒 | 𝐻 ?𝑒0 : 𝑒1

| 𝐻 [𝑒] | 𝑐 [𝐻 ] | 𝐻 .fieldname | 𝑙𝑣𝑎𝑙 = 𝐻 | 𝑡 𝑣 = 𝐻
| if(𝐻 ){𝑠0}else{𝑠1} | commute(𝐻 ){{𝑠0}{𝑠1}}

Small step semantics. Redex reduction rules are denoted ⟨𝑟, 𝜎⟩ (cid:123) ⟨𝑟 ′, 𝜎 ′⟩ and, for lack of space,
given in Apx A. For simplicity, these are all atomically reducible expressions. Note that memory
cannot be read and written in one atomic step. For each reduction, we use the Redex rule below:

⟨𝑟, 𝜎⟩ (cid:123) ⟨𝑟 ′, 𝜎 ′⟩

Redex

⟨𝑟, 𝜎⟩ (cid:123) ⟨𝑟 ′, 𝜎 ′⟩
⟨𝐻 [𝑟 ], 𝜎⟩ (cid:123) ⟨𝐻 [𝑟 ′], 𝜎 ′⟩

SS

For a program 𝑠 = 𝐻 [𝑟 ], a small step reduction is an application of the SS rule above. If 𝑠 is a redex,
we recover the redex rule by taking 𝐻 = •.

Suppose we are given program statements 𝑠0 and 𝑠1, and a predicate on states 𝜑𝑠0
𝑠1

: Σ → B.

Proc. ACM Program. Lang., Vol. 1, No. CONF, Article 1. Publication date: January 2018.

Veracity: Declarative Multicore Programming with Commutativity

1:9

Definition 3.1 (Sufficient Commutativity Condition). 𝜑𝑠0
𝑠1

for 𝑠0 and 𝑠1 when

is a sufficient commutativity condition

∀𝜎0, 𝜎𝑓0, 𝜎𝑓1 :

⟨𝑠0; 𝑠1, 𝜎0⟩ (cid:123)∗ ⟨𝑠𝑘𝑖𝑝, 𝜎𝑓0⟩∧
⟨𝑠1; 𝑠0, 𝜎0⟩ (cid:123)∗ ⟨𝑠𝑘𝑖𝑝, 𝜎𝑓1⟩ =⇒
𝜑𝑠0
𝑠1 (𝜎0) = true =⇒ 𝜎𝑓0 = 𝜎𝑓1

That is 𝜑𝑠0
𝑠1

tells when the order of execution has no effect on the final state.

4 SEMANTICS OF COMMUTE BLOCKS
In this section we define semantics for the language, which has been extended with the addition
of commute statements. We define three semantics, denoted 𝑠𝑒𝑚 ::= 𝑠𝑒𝑞 | 𝑛𝑑 | 𝑝𝑎𝑟 . We explain the
main rules below, with some details and reductions deferred to Apx. B.

Sequential semantics. For sequential semantics (cid:123)𝑠𝑒𝑞 in the language extended with commute

blocks, we add a simple reduction that treats commute as sequential composition:

⟨commute(true){{𝑠0}, {𝑠1}}, 𝜎⟩ (cid:123)𝑠𝑒𝑞

⟨𝑠0; 𝑠1, 𝜎⟩

Nondeterministic semantics. The nondeterministic semantics (cid:123)𝑛𝑑 has two possible reductions

that could take place when a commute condition has been reduced to true:

⟨commute(true){{𝑠0}, {𝑠1}}, 𝜎⟩ (cid:123)𝑛𝑑
⟨commute(true){{𝑠0}, {𝑠1}}, 𝜎⟩ (cid:123)𝑛𝑑

⟨𝑠0; 𝑠1, 𝜎⟩
⟨𝑠1; 𝑠0, 𝜎⟩

The two rules for commute(true) apply to the same syntax. This allows for non-determinism when
reducing this redex. We also introduce an SS-Nd rule akin to SS (see Apx. B).

Concurrent semantics. The next semantics permits the bodies of (possibly nested) commute blocks
to execute concurrently. To this end, we define a configuration ℭ that expresses the nested threading
nature of the semantics. Formally, there are two constructors of a configuration:

ℭ ::= ⟨𝑠, 𝜎⟩

|

⟨(ℭ0, ℭ1), 𝑠, 𝜎⟩

Configurations are either a top-most configuration (code and state), or a nested configuration,
executing in the context of an outer remaining code 𝑠 and outer state 𝜎. The statements of a
commute, which we will call fragments, execute code that naturally has access to variables defined
in outer scopes. Thus when fragments step, they need to access to outer scopes. To allow for this,
we define appending a state to a configuration (ℭ ⊕ 𝜎) as:

⟨𝑠, 𝜎0⟩ ⊕ 𝜎 ::= ⟨𝑠, 𝜎0 ⊕ 𝜎⟩

⟨(ℭ0, ℭ1), 𝑠, 𝜎0⟩ ⊕ 𝜎 ::= ⟨(ℭ0, ℭ1), 𝑠, 𝜎0 ⊕ 𝜎⟩
For convenience, as also define ℭ.𝑠𝑡, the state of a configuration, by: ⟨𝑠, 𝜎⟩.𝑠𝑡 = 𝜎 and
⟨(ℭ0, ℭ1), 𝑠, 𝜎⟩.𝑠𝑡 = 𝜎.

Nested configurations arise when reaching a commute block. That is, start with the following

redex and corresponding reduction:

⟨commute(true){{𝑠0}, {𝑠1}}⟩ (cid:123)𝑝𝑎𝑟 ⟨(⟨𝑠0, ∅⟩, ⟨𝑠1, ∅⟩), skip, 𝜎⟩

where ∅ represents new local (and empty) scopes of variables for each fragment. This is then
included in the 𝑝𝑎𝑟 semantics with the following rule:

⟨𝑟, 𝜎⟩ (cid:123)𝑝𝑎𝑟 ⟨(⟨𝑠0, 𝜎0⟩, ⟨𝑠1, 𝜎1⟩), 𝑟 ′, 𝜎⟩
⟨𝐻 [𝑟 ], 𝜎⟩ (cid:123)𝑝𝑎𝑟 ⟨(⟨𝑠0, 𝜎0⟩, ⟨𝑠1, 𝜎1⟩), 𝐻 [𝑟 ′], 𝜎⟩

Fork-Step

Proc. ACM Program. Lang., Vol. 1, No. CONF, Article 1. Publication date: January 2018.

1:10

Adam Chen, Parisa Fathololumi, Eric Koskinen, and Jared Pincus

Execution in the concurrent semantics involves (possibly recursively) descending into sub-
components to find a leaf fragment that can take a step. This is done using the following Left- (or,
similarly, Right-) projection rule:

ℭ0 ⊕ 𝜎 (cid:123)𝑝𝑎𝑟 ℭ′

0 ⊕ 𝜎 ′

Left-Proj
(mut. mut.) R-Proj

⟨(ℭ0, ℭ1), 𝑠, 𝜎⟩ (cid:123)𝑝𝑎𝑟 ⟨(ℭ′

0, ℭ1), 𝑠, 𝜎 ′⟩
When the two fragments of a commute have both reduced their statements to skip, the Join rule
can be used. Updates to the inner scope are lost, but updates to the outer scope 𝜎2 are preserved:

⟨(⟨skip, 𝜎0⟩, ⟨skip, 𝜎1⟩), 𝑠, 𝜎2⟩ (cid:123)𝑝𝑎𝑟 ⟨𝑠, 𝜎2⟩

Join

Note that join is blocking; both threads must reach skip before 𝑠 begins to reduce.

Executions. An execution 𝜀 (w.r.t. a semantics 𝑠𝑒𝑚) is a sequence of configurations with labeled
transitions. That is, let ℭ be the set of configurations and 𝑇 be the set of transitions. Then an
execution 𝜀 is an element of the set ℭ × (𝑇 × ℭ)∗, such that every transition is valid (𝑐𝑛 (cid:123)𝑡𝑛+1
𝑠𝑒𝑚 𝑐𝑛+1).
We assume all executions terminate and reach a final configuration ⟨skip, 𝜎𝑓 ⟩ for some 𝜎𝑓 . A
transition label ℓ has two parts

{𝐿𝑛, 𝑅𝑛 | 𝑛 ∈ N}∗ × ({𝜖} ∪ (𝑉 𝑎𝑟 × 𝑉 𝑎𝑙))
The first component is a fragment label, uniquely identifying some commute block’s fragment.
The label refers to the sequence of Proj-Left and Proj-Right rules used in the transition, with the
subscript referring to the number of sequentially composed commute blocks. In parallel executions,
the fragment label corresponds with a unique executing thread, and we may use the two words
interchangeably. As an example, if there are no sequentially composed commute blocks, applying
Proj-Left twice then Proj-Right once may yield a fragment label of 𝐿0𝐿0𝑅0. The second part of a
label is either 𝜖 or 𝑣 ↦→ 𝑐. The small steps only at most modify the value of one state element at a
time, so this is sufficient to capture all effects.

We write ℭ ⇓𝑠𝑒𝑚 𝜀 if the initial configuration of 𝜀 is ℭ, and let 𝜀𝑓 notate the final configuration

of the execution 𝜀.

5 A CONDITION FOR SAFE PARALLELIZATION
As discussed in Sec. 2.1, our programming model is distinct from, say, transactional memory in
which users explicitly fork threads. Our goal here is instead to safely interchange the parallel
semantics for the sequential semantics, i.e., show that
𝑠𝑒𝑞. We now define this notion of
(cid:75)
safe parallelization and then introduce a new correctness condition—scoped serializability—which
is distinct and slightly stronger than serializability and show that, when combined with valid
commutativity conditions, it achieves the goal.

𝑝𝑎𝑟 =
(cid:75)

𝑠
(cid:74)

𝑠
(cid:74)

5.1 Definitions and Properties
We choose to work with semantics based on post state equivalence rather than traces (projecting
actions out of executions) because, although the latter works well for serializability, the former is
simpler in our setting. Our notion of post-state equivalence also permits weaker notions of state
equivalence (i.e. observational equivalence), and corresponds more closely to standard definitions
of commutativity [Bansal et al. 2018; Dimitrov et al. 2014; Kim and Rinard 2011; Pîrlea et al. 2021].

Toward defining the goal, we begin with some definitions:

Definition 5.1 (Big-Step Semantics). For program 𝑠, let

𝑠𝑒𝑚 : Σ → P(Σ) be defined as:
(cid:75)
| ∃𝜀 : ⟨𝑠, 𝜎0⟩ ⇓𝑠𝑒𝑚 𝜀 ∧ 𝜀𝑓 .𝑠𝑡 = 𝜎𝑓 }

𝑠
(cid:74)

𝑠
(cid:74)

𝑠𝑒𝑚 ≡ 𝜆 𝜎0. {𝜎𝑓
(cid:75)

Proc. ACM Program. Lang., Vol. 1, No. CONF, Article 1. Publication date: January 2018.

Veracity: Declarative Multicore Programming with Commutativity

1:11

are equal as sets. We say for statements 𝑠0, 𝑠1, that
∀𝜎,

That is, the set of final states of executions. We say
𝑠0
(cid:74)

𝑠𝑒𝑚1 (𝜎01) when they
𝑠1
(cid:75)
(cid:74)
𝑠𝑒𝑚1 are equivalent when,
(cid:75)
𝑠𝑒𝑚 (𝜎) is singleton.
𝑠𝑒𝑚1 (𝜎) We say 𝑠 is deterministic in 𝑠𝑒𝑚 when ∀𝜎 :
𝑠
(cid:75)
(cid:75)
(cid:74)
𝑝𝑎𝑟 , i.e., defined in terms of end states of a program
(cid:75)
under different semantics. In the next subsection we will discuss a condition that ensures this goal.
There are a few key properties of the semantics, given below, that are later used in proving other

𝑠1
𝑠0
(cid:74)
(cid:74)
We say that 𝑠 is parallelizable if

𝑠0
(cid:74)
𝑠𝑒𝑚0 and
(cid:75)

𝑠𝑒𝑚0 (𝜎00) =
(cid:75)

𝑠𝑒𝑚0 (𝜎) =
(cid:75)

𝑠𝑒𝑞 =
(cid:75)

𝑠1
(cid:74)

𝑠
(cid:74)

𝑠
(cid:74)

lemmas or the main theorem. These properties are proved in Apx. B.4.

(1) Redex Uniqueness: ∀𝑠 : ∃!𝐻, 𝑟 : 𝑠 = 𝐻 [𝑟 ].
(2) Conditional Determinism: ∀𝑠, 𝜎, 𝐻, 𝑟 : 𝑠 = 𝐻 [𝑟 ] ∧ (∀𝑠0, 𝑠1 : 𝑟 ≠ commute(true){{𝑠0}{𝑠1}}) →
∃𝑠 ′, 𝜎 ′, 𝑠 ′′, 𝜎 ′′ : (⟨𝑠, 𝜎⟩ (cid:123)𝑠𝑒𝑚 ⟨𝑠 ′, 𝜎 ′⟩) ∧ (⟨𝑠, 𝜎⟩ (cid:123)𝑠𝑒𝑚 ⟨𝑠 ′′, 𝜎 ′′⟩) → (𝑠 ′ = 𝑠 ′′ ∧ 𝜎 ′ = 𝜎 ′′) .
(3) Commutativity Correctness: If every commute block guard in 𝑠 is a sufficient commutativity

condition then 𝑠 is deterministic in 𝑛𝑑.
𝑛𝑑 (𝜎) ⊆
𝑠
(cid:74)
(cid:75)

(4) Inclusion: ∀𝑠, 𝜎 :
(5) Deterministic Sequential: If 𝑠 is deterministic in 𝑠𝑒𝑚 then

𝑠𝑒𝑞 (𝜎) ⊆
(cid:75)

𝑝𝑎𝑟 (𝜎).
(cid:75)

𝑠
(cid:74)

𝑠
(cid:74)

𝑠
(cid:74)

𝑠𝑒𝑚 =
(cid:75)

𝑠
(cid:74)

𝑠𝑒𝑞.
(cid:75)

5.2 Scoped Serializability
We now define our correctness condition. We begin with a single execution:

Definition 5.2 (Scoped serial execution). Execution 𝜀 is scoped serial if:

∀𝑝 ∈ {𝐿𝑛, 𝑅𝑛 | 𝑛 ∈ N}∗ :
((∀ℓ, ℓ ′ ∈ 𝜀 :
∨(∀ℓ, ℓ ′ ∈ 𝜀 :

ℓ.fr has prefix 𝑝 · 𝐿𝑘 ∧ ℓ ′.fr has prefix 𝑝 · 𝑅𝑘 =⇒ ℓ ≤𝜀 ℓ ′)
ℓ.fr has prefix 𝑝 · 𝐿𝑘 ∧ ℓ ′.fr has prefix 𝑝 · 𝑅𝑘 =⇒ ℓ ′ ≤𝜀 ℓ))

Above, ℓ.fr is the fragment label of ℓ. The key idea here is to identify the scope of commute
fragments through labels, and then require a serializability condition for the L/R pair of the given
scope. Consider, e.g., a single commute block, possibly with children. For an execution to be scoped-
serial, we require all of the transitions from one of the fragments to execute prior to all the transitions
from its co-fragment (the other statement in the commute). Next, when there are nested commute
blocks, the quantification over prefixes requires that we expect this same property to hold locally
for all nested commute blocks. Without nesting, we recover the standard notion of serial. We now
define the following correctness condition, which requires that every execution is equivalent to
another scoped serial execution:

Definition 5.3 (Scoped Serializability). For execution 𝜀 such that ℭ ⇓𝑝𝑎𝑟 𝜀, 𝜀 is scoped serializable,

wrt ℭ if and only if

∃𝜀 ′ : ℭ ⇓𝑝𝑎𝑟 𝜀 ′ ∧ 𝜀 ′ is scoped serial ∧ 𝜀𝑓 .𝑠𝑡 = 𝜀 ′

𝑓 .𝑠𝑡 .

A program 𝑠 is scoped serializable (or s-serializable) if every execution of 𝑠 is scoped serializable.

S-Serializability vs Serializability. S-Serializability is distinct from serializability. Furthermore,
serializability is an insufficient condition for parallelization. Consider Ex. 2.5 from Sec. 2. Suppose
that the sub-statement x = x * 2 locks x so that it appears to execute atomically (we defer
formalization of locks). If we merely require that each thread’s own steps appear to act together,
then an acceptable serial execution (omitting intermediate configurations) of this program is:

(∅,

(𝜖, 𝑦 ↦→ 0), (𝜖, 𝑥 ↦→ 1),
(𝐿0𝑅0, 𝑥 ↦→ 𝑥 ∗ 2),
(𝑅0, 𝜖)
(𝑅0, 𝑦 ↦→ 1), (𝐿0𝐿1, 𝑥 ↦→ 0))

Here 𝑥 ∗ 2 = 2
Reads x’s value as 2

Proc. ACM Program. Lang., Vol. 1, No. CONF, Article 1. Publication date: January 2018.

1:12

Adam Chen, Parisa Fathololumi, Eric Koskinen, and Jared Pincus

After which we have 𝜎𝑓 = {𝑥 ↦→ 0, 𝑦 ↦→ 1}. However, no pairwise re-ordering of the commutative
blocks can result in such a state; if statement if (x>2) { y=1; } executes first then y will be 0,
and if it executes second, then y will still be 0.

To adapt the traditional notion to our setting, one would permit interleavings of outer commute
statements that could interfere with inner commute statements. The property (see Apx. B.5) would
specify order requirements on interactions between fragments that do not share the same prefix.
Naturally, our notion of S-Serializability implies this adapted notion of serializability.

5.3 Parallelizability
As our setting allows for a nested structure of commute statements, and also sequentially composed
commute statements, these must be reflected in the parallel semantics. To ensure parallelizability, this
structure must be preserved. While serializability is insufficient, we prove that scoped serializability
is a sufficient condition.

Lemma 5.4. S-Serializable(𝑠) =⇒

𝑠
(cid:74)

𝑛𝑑 =
(cid:75)

𝑠
(cid:74)

𝑝𝑎𝑟
(cid:75)

By adding Lemma B.3, we conclude our main theorem:

Theorem 5.5 (Sufficient Condition for Parallelizability). If every commutativity condition

in 𝑠 is valid and S-Serializable(𝑠), then 𝑠 is parallelizable.

Proof. By induction on maximum fragment length of transitions in 𝑝𝑎𝑟 executions of 𝑠. Full
□

proof in Apx. D.

5.4 Enforcing Scoped Serializability
Note that any sufficient condition for pairwise serializability admits a condition for s-serializability
by inductively applying it on each commute statement. Imagine that we have first enforced s-
serializability for a sub-configuration. Then we can use a strategy for enforcing pairwise serial-
izability as an inductive step for enforcing s-serializability in outer scopes. We thus now discuss
techniques for enforcing pairwise serializability (and thus s-serializability).

Pattern 0: Naïve Locking. To our knowledge, prior works have not focused on pairwise serializ-
ability, perhaps because it hasn’t been useful in other settings (e.g., parallel composition). However,
we may still take the naïve approach of locking from the first read/write on a common variable to
the last. For only two blocks, this likely would not yield performance gains, as it leads to mutual
exclusion, but in our nested commute setting, we can still aim for above 2× speedup due to the
nesting of commute statements.

Indeed because of the structure of our setting we can use and improve the possible gains from

pairwise serializability.

Pattern 1: Write/ReadOnly Intersection. In some cases, although both threads are reading/writing
multiple variables, it may be that, among the commonly accessed variables, one thread is only
reading from that set.

Let 𝑤𝑟 (𝑠) be the set of variables that 𝑠 writes to, and 𝑟𝑑 (𝑠) be the set of variables that 𝑠 reads from.
In the case of an instance of a builtin ADT, we include that builtin’s variable name (accounting for
aliasing) in both 𝑤𝑟 (𝑠) and 𝑟𝑑 (𝑠) if a method is invoked on it. Define the set of conflicting variables
be:

𝑐𝑜𝑛(𝑠0, 𝑠1) ≡ [𝑤𝑟 (𝑠0) ∩ 𝑤𝑟 (𝑠1)] ∪ [𝑤𝑟 (𝑠0) ∩ 𝑟𝑑 (𝑠1)] ∪ [𝑟𝑑 (𝑠0) ∩ 𝑤𝑟 (𝑠1)]
Now let us consider the case where that (𝑤𝑟 (𝑠0) ∪ 𝑟𝑑 (𝑠0)) ∩ 𝑤𝑟 (𝑠1) = ∅. Then 𝑐𝑜𝑛(𝑠0, 𝑠1) =
𝑤𝑟 (𝑠0) ∩ 𝑟𝑑 (𝑠1). We may consider the symmetrical case (with 𝑠0 and 𝑠1 swapped) as well. We

Proc. ACM Program. Lang., Vol. 1, No. CONF, Article 1. Publication date: January 2018.

Veracity: Declarative Multicore Programming with Commutativity

1:13

propose a simple program transformation can be used to enforce s-serializability in cases where
this pattern holds. For each (conflicting) variable 𝑣 ∈ 𝑐𝑜𝑛(𝑠0, 𝑠1),

(1) Replace all reads of 𝑣 in 𝑠1 to that of a new var 𝑣0.
(2) Before the commute block, add a statement 𝑣0 = 𝑣.

Then all of 𝑠1’s reads will refer to the state prior to execution of 𝑠0, regardless of whether 𝑠0 executes
first or not. So every interleaving will be as if it were the case 𝑠1; 𝑠0. One can think of the above
process as snapshotting the state prior to potential modifications that may cause issues when
interleaved.

Pattern 2: Narrowing mutual exclusion. Now we consider the more general case, when there are
variables written by both threads. We can still use a single lock to enforce scoped serializability. (For
a formal treatment of locking in our semantics, refer to Apx. C.) If the threads lock on a common
lock, they necessarily mutually exclude each other (and thus have no gains from parallelization), so
the goal is to merely minimize the amount of time that the lock is held. Intuitively, this means we
want all of the operations on conflicting variables as temporally close as possible in each thread.
As is done elsewhere [Čern`y et al. 2013], this can be done via program transformations.

We describe a method for reordering statements in commutative blocks so that each block may
be treated as an atomic point with respect to one another, i.e. the point when it acquires the singular
lock. Suppose we are provided a data-flow graph for 𝑠0 and 𝑠1. Identify all instructions (nodes) that
refer to the set of conflicting variables 𝑐𝑜𝑛(𝑠0, 𝑠1). Call this set CI𝑠0 (resp. CI𝑠1). Next, reorder
instructions to be in the following form:

(1) Instructions that are only ancestors of nodes in CI𝑠0 (i.e., instructions whose data only flows

into the conflicting variables);

(2) 𝑙𝑜𝑐𝑘 ();
(3) Instructions that are in, or that are both ancestors and descendants of (possibly distinct)

nodes in, CI𝑠0.
(4) Snapshot 𝑐𝑜𝑛(𝑠0, 𝑠1);
(5) 𝑢𝑛𝑙𝑜𝑐𝑘 ();
(6) Instructions that are only descendants of nodes in CI𝑠0 (i.e., instructions who are only

dependent on the conflicting variables).

(7) Instructions that are neither ancestors nor descendants of nodes in CI𝑠0.

We use the same notion of snapshotting as in Pattern 1. This transformation is correct (assuming
proper instruction ordering within each of the four groups), as it preserves all data dependencies,
and it enforces pairwise serializability, as we may treat the fragments as executing sequentially in
the order of reaching the lock. Indeed, we can re-order most transitions in an execution, as:

• The statements in (1) for 𝑠0 can not have a dependency to the statements in (3) for 𝑠1, as
otherwise they would cause writes in 𝑠0 that are read in 𝑠1, a contradiction as then they would
be in CI𝑠0, and thus in (3).

• The statements in (6) for 𝑠0 can not have dependencies from the statements in (3) for 𝑠1, as

they have been modified to refer only to the local snapshot.

• All data dependencies across the blocks must involve a statement from (3). So, for example,

(1) for 𝑠1 does not depend on (6) for 𝑠0.

Thus we can reorder all transitions in (1) in the second thread to after (3) of the first, and all
statements in (6) and (7) of the first thread to before the (3) of the second, obtaining a serial
execution.

Proc. ACM Program. Lang., Vol. 1, No. CONF, Article 1. Publication date: January 2018.

1:14

Adam Chen, Parisa Fathololumi, Eric Koskinen, and Jared Pincus

For load balancing reasons, it may be practical to put (7) at the beginning of the block when
reordering 𝑠1. If fact, even when locking is non-trivial, using this technique can ensure correctness
at minimal performance cost with proper load balancing.

6 VERIFICATION AND INFERENCE OF COMMUTATIVITY CONDITIONS
For the parallelized version of the program to be correct, commute conditions must be sound.
Although there are some recent techniques for verifying or inferring commutativity conditions,
these techniques do not quite fit our setting, as discussed in Sec. 9. For example, the abstraction-
refinement technique of Bansal et al. [2018] applies to logically specified abstract data-types (ADTs)
and not directly to code. In this section we will describe an embedding of programs with commute
statements (which may program context, involve nested commute statements, builtins hashtables,
loops, etc.) into such logical specifications, thus enabling us to use such abstraction-refinement
to synthesize conditions that can be used in our commute blocks, or to verify manually provided
conditions.

Since we target the abstraction-refinement synthesis algorithm of Bansal et al. [2018], we briefly

recall the input to this algorithm, which is a logically specified ADT O, defined as:

Definition 6.1 (Logical ADT specification).

(cid:26)

O =

state
eq

(𝑉 𝑎𝑟,𝑇𝑦𝑝𝑒)list;

:
⊆ state × state;

methods
spec

: Meth list;
: Meth → (𝑃, 𝑄)

(cid:27)

These objects include state defined as a list of variables, an equality relation on states, a finite list
of method signatures (without implementations), and logical specifications for each method. The
commutativity condition synthesis problem is then, for a given pair of method signatures 𝑚( ¯𝑥) : Meth
and 𝑛( ¯𝑦) : Meth, to find a logical condition 𝜑 in terms of the object’s state, and the arguments ¯𝑥 and
¯𝑦, such that 𝑚 and 𝑛 commute according to the method specifications from every state satisfying 𝜑.
Bansal et al. [2018] give a method for doing so based on abstraction-refinement.

6.1 Embedding commute blocks as logical ADT specifications.
The challenge we now address is how to translate programs with commute blocks into logical object
specifications.

For a given commute block with statements 𝑠1 and 𝑠2 and their program context 𝐸, our translation

Tr is defined below and yields an object O with two methods 𝑚𝑠1 and 𝑚𝑠2.

Definition 6.2 (Embedding Tr). For commute fragment statements 𝑠1 and 𝑠2 in program context 𝐸,

Tr (𝐸, commute 𝑠1 𝑠2) ≡ O where

• O.state: We create an object variable for every global or local variable in environment 𝐸 (that

is visible to the scope containing 𝑠1 and 𝑠2).
• O.eq: 𝜆𝑜1, 𝑜2. (cid:211)𝑣 ∈(map fst O.state) 𝑜1.𝑣 = 𝑜2.𝑣.
• O.methods: [𝑚𝑠1 : unit → unit, 𝑚𝑠2 : unit → unit]
• O.spec: [𝑚𝑠1 ↦→ (true, specOf(𝑠1, 𝐸)), 𝑚𝑠2 ↦→ (true, specOf(𝑠2, 𝐸))] with specOf in Fig. 1.

Intuitively the idea is to embed the current program context (including variables in scope and
builtins) as O’s state and 𝑚𝑠1 () and 𝑚𝑠2 () as nullary methods corresponding to 𝑠1 and 𝑠2, respectively,
whose pre/post specifications describe the changes to that context.

The last spec component translates commute fragment source code to a logical method specifica-
tion. The overall shape of the translation is akin to verification condition generation, introducing
indexed let-binding 𝑣𝑖 each time program variable 𝑣 is mutated and, ultimately, constraining the
value of 𝑣 in the postcondition (with respect to the 0th bindings) in terms of the most recent

Proc. ACM Program. Lang., Vol. 1, No. CONF, Article 1. Publication date: January 2018.

Veracity: Declarative Multicore Programming with Commutativity

1:15

type idmap = 𝑉 𝑎𝑟 → N

let rec Tr𝐸 (𝑒:expr) (id:idmap) :
idmap ×Φ× (id ×Φ)𝑙𝑖𝑠𝑡 =
match 𝑒 with
| 𝑣 → (id, 𝑣 (id v), [])
| 𝑒 ′⊗ 𝑒 ′′ →

let (𝜑𝐿, binds𝐿) = Tr𝐸 𝑒 ′ id in
let (𝜑𝑅, binds𝑅) = Tr𝐸 𝑒 ′′ id’ in
(id, 𝜑𝐿 ⊗𝜑𝑅, binds𝐿 ++ binds𝑅)

. . .
| builtin adt 𝑣𝑎𝑑𝑡 m ¯𝑒 →

let (id_ret :: 𝜑𝑎𝑟𝑔𝑠, binds) =
map (Tr𝐸 • id) ¯𝑒 in
let adt_id = id 𝑣𝑎𝑑𝑡 in
let (𝜑, spec_binds) =

spec adt 𝑣𝑎𝑑𝑡 m ¯𝜑𝑎𝑟𝑔𝑠 adt_id in

(id[idret 𝑚𝑎𝑝𝑠𝑡𝑜 id[idret] + 1],
𝜑, binds ++ spec_binds)

let specOf (𝑠 :stmt) : Φ =
let 𝑠𝑓 𝑙𝑎𝑡 = flatten 𝑠 in
let 𝑠 ′ = summarizeLoops(𝑠𝑓 𝑙𝑎𝑡 ) in
let id0 = [𝑣 ↦→ 0 | 𝑣 ∈ 𝑉 𝑎𝑟𝑠 (𝑠)] in
let inits = { (𝑣 0
0 𝑣 0) (𝑣 1
let (_, 𝜑) = (Tr𝑆 𝑠𝑓 𝑙𝑎𝑡 id0) in
(let

inits in 𝜑)

0 𝑣 1) ... | 𝑣𝑖 ∈ 𝑉 𝑎𝑟 (𝑠)} in

let rec Tr𝑆 (𝑠𝑡𝑚𝑡𝑠:stmt list) (id : idmap) : Φ =

match 𝑠 with
| [] → (cid:211)𝑣 ∈dom(id)
| (𝑣 = 𝑒) :: tl →

.𝑣𝑛𝑒𝑤 = 𝑣 (id v)

let (id’, 𝜑𝑒 , bindings) = (Tr𝐸 𝑒 id) in
let 𝑏𝑖𝑛𝑑𝑖𝑛𝑔𝑠, 𝑣 (id’ v+1) 𝜑𝑒 in

Tr𝑆 tl (id’[𝑣 ↦→ (id’ 𝑣)+1])

| (if 𝑒 then 𝑠1 else 𝑠2) :: tl →

let (id’, 𝜑𝑒 , bindings) = (Tr𝐸 𝑒 id) in
let 𝑏𝑖𝑛𝑑𝑖𝑛𝑔𝑠 in

(ite 𝜑𝑒 (Tr𝑆 (s1 ++ tl) id’)
(Tr𝑆 (s2 ++ tl) id’))

| (commute 𝑒 𝑠1 𝑠2) :: tl →

Tr𝑆 id (𝑠1 ++𝑠2 ++ tl)

| (assume 𝑒) :: tl →

let (_, 𝜑, _) = Tr𝐸 𝑒 id in
𝜑∧ Tr𝑆 tl id
| (havoc ℎ𝑖𝑑) :: tl →

let ℎ𝑖𝑑ℎ𝑎𝑣𝑜𝑐 = freshvar () in
exists ℎ𝑖𝑑ℎ𝑎𝑣𝑜𝑐 in
let ℎ𝑖𝑑 (id hid+1) ℎ𝑖𝑑ℎ𝑎𝑣𝑜𝑐 in
Tr𝑆 tl id[ℎ𝑖𝑑 ↦→ (id ℎ𝑖𝑑)+1])

Fig. 1. Reducing a commute fragment’s statement 𝑠 to a logical method specification.

let-binding. (Programs in our language are total and thus, preconditions are simply true.) The
output of our translation is a logical formula, written in SMTLIBv2 (see Section 5.2 of Barrett et al.
[2010]):

𝜑 ::= 𝜑 ∨ 𝜑 | ¬𝜑 | ite 𝜑 𝜑 𝜑 | 𝑓 ¯𝜑 | let 𝑣 𝜑 in 𝜑 | exists 𝑣 in 𝜑 | 𝜑 ⊗ 𝜑 | true
Greek letters are used for logical terms. Φ will be used for the type of terms. A formula 𝜑 is a well-
sorted term with sort boolean. The terms we use include let, ite, and exists, underlining formulae
constructors to distinguish from our language and algorithm.

The translation algorithm specOf is given in Fig. 1. First we replace all sequential composition
of statements with lists of statements, where skip is the empty list. We proceed in an SSA-like
manner, creating an initial mapping id0 of indices for each variable and construct a let term binding
𝑣0 with 𝑣 for each variable that is defined in the context 𝐸 and accessed in the body of the commute
fragment. This binding will let us ultimately relate variable 𝑣𝑛𝑒𝑤 in terms of its initial value 𝑣. We
then begin with translation Tr𝑆 on 𝑠𝑓 𝑙𝑎𝑡 . A call to (Tr𝑆 𝑠 id) recursively constructs the translation
to a logical expression representing the postcondition of the block. Within a given call to (Tr𝑆 𝑠 id),
if 𝑠 is a final skip statement (i.e., if s is the empty list), then an innermost formula is constructed
that relates each 𝑣𝑛𝑒𝑤 to the most recent binding for 𝑣 in the ID map. In the assignment case, 𝑒 is

Proc. ACM Program. Lang., Vol. 1, No. CONF, Article 1. Publication date: January 2018.

1:16

Adam Chen, Parisa Fathololumi, Eric Koskinen, and Jared Pincus

first translated, which may accumulate additional variable bindings, namely when 𝑒 contains a call
to a builtin ADT. Then a fresh binding for variable 𝑣 is created with the new value. In the if/else
case, we recursively translate the condition expression 𝑒 to obtain a logical condition 𝜑𝑒 . We then
represent branching via ite, with condition 𝜑𝑒 and the remainder of the block tl appended to both
the branches, recursively translated. Our semantics of commute statements allows this translation
to treat them as sequential composition in either order, so we simply choose 𝑠1; 𝑠2 and recursively
translate their concatenation. Loops are supported by instrumenting loop summaries with havoc
and assume, as detailed at the end of this subsection. As usual, havoc existentially quantifies a new
value and assume introduces a new specified constraint, both with respect to the current binding
IDs.

The translation of expressions Tr 𝑒 id returns a triple (an updated id map, a translation of 𝑒 into
logic, and possibly new variable bindings), and is mostly straight-forward, except in the case of
a builtin ADT. This case requires us to instantiate the ADT’s logical specification for the current
calling context, so we first recursively translate each of the arguments in ¯𝑒, obtaining a set of local
bindings binds. We then use a helper method spec (omitted) to instantiate the arguments into
the ADT’s post-condition, and construct an SMT expression 𝜑 describing the return value and
additional bindings for the return value(s) (on a logical level, we may consider the ADT’s updated
state to be a return value, e.g. a hashtable has a set of keys, an integer size, and a finite map of keys
to values). In this way, the specification internals are “ferried” through the nested let statements and
constraints in the final embedding. The translation of builtins is illustrated (among other things) in
the following example:

Example 6.3 (Illustrating the embedding).

commute (c>0) {
𝑠1 : { c=c +1; }
𝑠2 : { if ( c>0) { tbl [c] = 5; c = c -1; }

We label the fragments above by their statements. We describe how specOf converts 𝑠2 to a formula.
A map id0=[𝑐 ↦→ 0, 𝑏 ↦→ 0, 𝑡𝑏𝑙 ↦→ 0] is created, as well as let-bindings ((𝑏0 𝑏)(𝑐0 𝑐)(𝑡𝑏𝑙0 𝑡𝑏𝑙)). The
first Tr𝑆 call is for the if-then-else, and it recursively translates the condition: Tr𝐸 c>0 id0, which
takes the 𝑐 ⊗ 0 case, recursively translates 𝑐 to 𝑐id0 c and then returns (id0, (>𝑐00), []). Now in the
then branch, Tr𝑆 recursively translates the sequential composition. The assignment is syntactic
sugar for ht_put(tbl, c, 5). Here the recursive translation of the first argument translates
argument c to 𝑐0, and then a call is made to spec ht 𝑡𝑏𝑙 put 𝑐0 5 (id 𝑡𝑏𝑙). This helper function
instantiates the specification to put as follows:

𝜑𝑝𝑢𝑡 ≡

𝑏𝑖𝑛𝑑𝑠 ≡

(𝑡𝑏𝑙𝐾1(ite
(𝑡𝑙𝑏𝑆1 (ite
(𝑡𝑏𝑙𝑀1(ite

(cid:169)
(cid:173)
(cid:173)
(cid:173)
(cid:171)

(ite (𝑚𝑒𝑚 𝑐0 𝑡𝑏𝑙𝐾0) (≠ 5 (𝑠𝑒𝑙𝑒𝑐𝑡 𝑡𝑏𝑙𝑀0 𝑐0)) true)

(𝑚𝑒𝑚 𝑐0 𝑡𝑏𝑙𝐾0) 𝑡𝑏𝑙𝐾0 (𝑖𝑛𝑠 𝑐0 𝑡𝑏𝑙𝐾0))
(𝑚𝑒𝑚 𝑐0 𝑡𝑏𝑙𝐾0) 𝑡𝑙𝑏𝑆0 (𝑡𝑙𝑏𝑆0 + 1))
(𝑚𝑒𝑚 𝑐0 𝑡𝑏𝑙𝐾0)
(ite ((𝑠𝑒𝑙𝑒𝑐𝑡 𝑡𝑏𝑙𝑀0 𝑐0) = 5)
(𝑠𝑡𝑜𝑟𝑒 𝑡𝑏𝑙𝑀0 𝑐0 5)))

𝑡𝑏𝑙𝑀0

(𝑠𝑡𝑜𝑟𝑒 𝑡𝑏𝑙𝑀0 𝑐0 5))

The above bindings involve three variables 𝑡𝑏𝑙𝐾1, 𝑡𝑏𝑙𝑆1, 𝑡𝑏𝑙𝑀1 representing, resp., the set of keys, the
size, and the finite map of the hashtable after the get operation, with respect to the corresponding
previous variables 𝑡𝑏𝑙𝐾0, 𝑡𝑏𝑙𝑆0, 𝑡𝑏𝑙𝑀0. These variables model the standard hashtable semantics.
Returning to the Tr𝑆 of the assignment statement, the new value of the state is bound to, then the
increment of c is translated to 𝜆 𝑘.let (𝑐1(𝑐0 − 1)) in 𝑘 by a recursive call. After the assignment a
skip will be introduced, so an innermost constraint of the form (𝑐𝑛𝑒𝑤 = 𝑐1 ∧ 𝑡𝑏𝑙𝐾𝑛𝑒𝑤 = 𝑡𝑏𝑙𝐾1 ∧ ...) is

Proc. ACM Program. Lang., Vol. 1, No. CONF, Article 1. Publication date: January 2018.

Veracity: Declarative Multicore Programming with Commutativity

1:17

constructed. Returning to the original 𝑇𝑟 call on the if-then-else, the full translation is assembled:

let ((𝑐0 𝑐) . . . (𝑡𝑏𝑙𝐾0 𝑡𝑏𝑙𝐾) . . .)in true ∧

(ite (𝑐0 > 0)

(let 𝑏𝑖𝑛𝑑𝑠 in (let (𝑏1 𝜑𝑝𝑢𝑡 ) in

(let (𝑐1(𝑐0 − 1)) in

(𝑐𝑛𝑒𝑤 = 𝑐1 ∧ 𝑡𝑏𝑙𝐾𝑛𝑒𝑤 = 𝑡𝑏𝑙𝐾1 ∧ . . .))

Loop summaries. Loops are supported through summaries that can be computed by other tech-
niques (e.g. [Ernst 2020; Kroening et al. 2008; Silverman and Kincaid 2019; Xie et al. 2017]). We
calculated loop summaries (see Apx. F) using the Korn tool. In some cases, Korn was not precise
enough, so we manually strengthened the summaries, but we believe this work could be fully auto-
mated, and that it is orthogonal to our main contributions. We then replace loops in commute blocks
with their summaries using havoc and assume statements. Note that in general, approximation of
loops may not be precise enough to ensure commutativity [Koskinen and Bansal 2021], however,
this is not a problem in our setting because our commute conditions are verified/inferred with the
requirement that post-states be exactly equal. Thus, we would fail to verify/infer any conditions in
the presence of insufficiently precise loop summaries.

6.2 Verifying and Inferring commute conditions
Our embedding allows us to use SMT tools to verify commute conditions and the abstraction-
refinement algorithm of Bansal et al. [2018] to synthesize them.

Verification. To verify a provided commutativity condition 𝜑, we construct a series of constraints,
modeling the execution of 𝑜.𝑚𝑠1 and 𝑜.𝑚𝑠2 in either order denoted 𝑜.𝑚𝑠1 ⊲⊳ 𝑜.𝑚𝑠2 and assert the
negation of 𝜑 =⇒ 𝑜.𝑚𝑠1 ⊲⊳ 𝑜.𝑚𝑠2. The correctness of our technique is based on the soundness of
our embedding, as described in the following theorem:

Theorem 6.4 (Embedding soundness). For every commute 𝑠1 𝑠2, let 𝑜 = 𝑇𝑟 (𝐸, commute 𝑠1 𝑠2).
Then if valid(𝜑 =⇒ 𝑜.𝑚𝑠1 ⊲⊳ 𝑜.𝑚𝑠2) = 𝜑, then 𝜑 is a valid commutativity condition for commute 𝑠1 𝑠2.

Inference. Often it is even more convenient for commute conditions to be automatically syn-
thesized. To this end our embedding also allows us to use abstraction-refinement [Bansal et al.
2018], which takes as input the kinds of objects that we construct. As discussed in Sec. 7 we
re-implemented this algorithm, adding some improvements to that algorithm (e.g. in the predicate
selection process) that are beyond the scope of this paper. The resulting commutativity conditions
we infer are sound, again due to the soundness of our embedding (Thm. 6.4).

Completeness. Commute conditions are akin to Hoare logic preconditions and, as such, they are
subject to expressibility concerns of the assertion language. However, commute conditions need not
be complete for two reasons: (i) as opposed to preconditions, when commute conditions don’t hold
we default to sequential execution, rather than undefined behavior and (ii) consequently, it is always
fine to over-approximate commute conditions. As we show in Sec. 8, in our 30 benchmarks, there
were not any cases where either a precise commute condition or a sensible sound approximation
could be used.

7 IMPLEMENTATION: VERACITY
Our implementation comprises an interpreter capable of parallel execution (and sequential execution
for testing), as well as analyses for verification and synthesis of commute conditions (Sec 6). For
simplicity we focused on building a concurrent interpreter, deferring backend compilation matters
to future work.

Proc. ACM Program. Lang., Vol. 1, No. CONF, Article 1. Publication date: January 2018.

1:18

Adam Chen, Parisa Fathololumi, Eric Koskinen, and Jared Pincus

Interpreter. Threads are implemented with Multicore OCaml, as OCaml does not natively support
true multi-threading. Considerations are made in the interpreter to minimize shared memory
between threads. When a commute statement is reached and its guard evaluates to true, threads
are spawned for each parallel block. Using thread pools did not substantially improve performance
over spawning as-needed. Each thread is given a copy of the global environment and call stack,
permitting thread-local method calls. Sibling threads’ environments will only share references to
variables in shared scope, namely global variables, arguments of the current method, and variables
defined within the method in scope of the current block. When a commute statement’s guard
evaluates to false, the blocks are executed in sequence. The Veracity executable includes a flag
to force all commute guards to evaluate to false, allowing us to compare parallel-vs-sequential
execution times.

A key benefit of commute blocks is that hashtable operations can be implemented with a lineariz-
able hashtable (e.g. [Liu et al. 2014; Purcell and Harris 2005]) in the concurrent semantics. To confirm
the benefit, we built a foreign function interface to the high-performance NSDI’13/EuroSys’14
concurrent hashtable [Fan et al. 2013; Li et al. 2014] called libcuckoo [lib 2013].

There are minor differences and syntactic sugar between the formal semantics and the imple-
mented Veracity language. (See Apx. E.) Conversely, currently our theory permits only exactly two
fragments per commute statement, although our implementation is unbounded.

Verifying and inferring commute conditions. We implemented the embedding described in Sec. 6.
This translation then generates a specification for an object O, with commute blocks embedded
as logical specifications, given as OCaml SMTLIBv2 expressions. For verification and abstraction-
refinement inference, we implemented a new version of Servois [Bansal 2018] as an OCaml library/-
module for a few reasons:

• Improve performance and closer integration. O is now an OCaml type, constructed by

Veracity’s compiler and passed to our new abstraction-refinement library.

• Support more/other SMT solvers. We used CVC5 [Barbosa et al. 2022] for all of our experi-

ments, but can also use CVC4, Z3, and plug in other solvers.

• Improve/tune the predicate generation and predicate choice techniques.
• Add support for verifying user-provided commute conditions.

To improve abstraction-refinement we also provide possible terms from which candidate predicates
are constructed. We extract these terms from the syntax of the Veracity program, as well as
practically selected constants which helped reduce the size of the synthesized conditions. We also
extract terms from the builtin ADT specifications. Finally, after commute conditions are synthesized,
we then reverse translate back to Veracity expressions.

We will publicly release our implementation of all of the above.

8 EVALUATION
We evaluated our work against four goals:

(1) Determine whether commute conditions could be automatically generated (Sec. 6);
(2) Determine whether scoped serializability can be enforced using our locking patterns (Sec. 5.4);
(3) Confirm the common-sense expectation that speedups can be seen when the duration of

sufficiently independent commute fragments increase; and

(4) Whether, despite introducing a new programming paradigm, existing applications could be

adapted and exploit commute statements.

Experimental setup. All experiments below were run on a machine with a machine with an AMD

EPYC 7452 32-Core CPU, 125GB RAM, Ubuntu 20.04, and Multicore OCaml 4.12.0

Proc. ACM Program. Lang., Vol. 1, No. CONF, Article 1. Publication date: January 2018.

Veracity: Declarative Multicore Programming with Commutativity

1:19

Correctness of Commute Conditions

Time (s) Inferred Conditions

(cid:152) 1 == y && 0 != y && 1 > c && 1 != c || ... || 1 == c

0.63 i != j && x != y || x == y
0.75 1 != r[0] && r[0] + 1 != y && r[0] <= 1 || r[0] + 1 == y && r[0] <= 1
1.11 0 > a[0] && 1 != x || 1 == x
1.13 d != e && a != b || a == b

Group 1: Automatically Inferred Commute Conditions. All benchmarks, except those below in group (3).
Program
array-disjoint
array1
array2
array3
calc
conditional
counter
dict
dot-product
even-odd
ht-add-put
ht-cond-mem-get
ht-cond-size-get
ht-simple
linear-bool

0.18 x > 0
0.20 0 != c
3.82 i != r && c + x != y || c + x == y
0.24 true
1.18 x % 2 == x + y && 0 != y || 0 == y
2.24 tbl[z] == u + 1 && u + 1 != z
1.54 tbl[x] == tbl[z] && x != z || x == z
0.83 ht_size(tbl) <= 0 && 0 != z || 0 == z
30.64 x + a != z && 3 == tbl[z] && y != z
3.62 0 <= y && 3 == x && 2 != x && 1 != x && x > 0 && 0 != x || 0 > y + 3 * x &&

linear-cond
linear
loop-amt

loop-disjoint
loop-inter
loop-simple
matrix
nested-counter

2 == x && 1 != x && x > 0 && 0 != x

2.65 2 <= y && 2 != y && 1 != y || ... || 1 == y
0.25 true

(cid:152) 0 == i && amt == i_pre && ctr - 1 > i_pre && i_pre <= amt && 0 != i_pre &&
i_pre <= ctr && amt != amt_pre && ctr - 1 > amt_pre && amt_pre <= amt && 0
!= amt_pre && amt_pre <= ctr && ctr - 1 != 1 && 1 != ctr && 1 != amt && 1
== ctr + amt || ... || amt == i && 1 == ctr && 1 != amt && 1 == ctr + amt

0.02 true
4.63 0 == x && 0 != y || 0 == y
0.06 true
0.71 0 == y
6.25 0 != c && c != t || c == t; c != x && c <= x && 1 != x && t == x || ... ||

1 == x && t == x

0.02
0.07
0.02
0.04
0.02
0.02
0.04
0.05

Time (s) Verified? Complete? Provided Condition

0.25 true; 0 == x
1.42 0 == y
3.49 a <= c && a != b && a != c || a == b && a != c

nested
nonlinear
simple
Group 2: Automatically Verified Commute Conditions. Benchmarks for which inference output was suboptimal.
Program
array1
calc
counter
even-odd
linear-bool
linear-cond
loop-amt
nested-counter

✓ r[0] <= 0 || r[0] == 1 && y == 2
? c > 0
— true
✓ y % 2 == 0
✓ y < 0 - 3 * x && x == 2 || y >= 0 && x == 3
✓ y > 0 || 0 == y && x + 2 == z
✗ i % 2 == 0 && (ctr > 0 && amt > 0 || ctr <= 0 && amt < -ctr)
✓ First commute block: 0 != c && c != t || c == t
✗ Second commute block: x == t && (x > c || x == c && x > 1)
✗ c > a

(cont.)
simple
Group 3: Unverified Examples. Including case studies.
Manual commute condition
Program
true
crowdfund
(!s1 && !s2) || (s1 && !s2 && (r2 == 0 || (n % 2 == 0 && r2 == n / 2))) || (!s1 && s2
dihedral
&& (r1 == 0 || (n % 2 == 0 && r1 == n / 2))) || (s1 && s2 && r1 == r2)
fname1 != fname2

✓
✓
✗
✓
✓
✓
✓
✓
✓
✓

0.04

filesystem
ht-fizz-buzz true; true

Inference and/or verification of commute blocks. We first attempted to infer commute conditions for all
Fig. 2.
programs (Group 1), except for those with features like strings, side-effects, etc. (Group 3). When the inferred
condition in Group 1 was overly complex or reflected only trivial executions, we then manually provided a
condition and attempted to verify it (Group 2).

Proc. ACM Program. Lang., Vol. 1, No. CONF, Article 1. Publication date: January 2018.

1:20

Adam Chen, Parisa Fathololumi, Eric Koskinen, and Jared Pincus

Fig. 3. Parallel-to-sequential speedup for benchmark programs versus computation size. The computation
size x-axis is logarithmically scaled. Each trendline is for a different program. Many benchmarks had similar
performance characteristics. For legibility of the overall plot, we color them all (Group D) in medium gray.
The other group labels also indicate similar performance characteristics.

Benchmarks. We created a total of 30 example programs with commute blocks that use a variety
of program features including linear arithmetic, nonlinear arithmetic, arrays, builtin hashtables,
nested commute blocks, loops and some procedures. The complete set of benchmarks are given in
Apx. H and in the supplemental materials.

Inferring/verifying commute conditions. Fig. 2 shows the results of inference/verification. We
first applied our procedure to infer commute conditions for all benchmarks (Group 1), except for
crowdfund, dihedral, filesystem and ht-fizz-buzz, which have complicated loops or inter-
procedural calls that we could not translate to logic. We manually provided conditions for these
cases, as seen in Group 3 in Fig. 2. Those for which we were able to infer a condition are listed in
Group 1 in Fig. 2, along with the time taken to infer and the resulting condition. Inference was
set to time out at 120 seconds. The benchmarks for which inference timed out are marked with a
clock. In nested examples, there are multiple commute blocks, so we list all conditions. In most of
the benchmarks we were able to infer concise and useful commute conditions in a few seconds or
fractions of a second.

In cases where inference leads to an overly complex condition or a condition that only applies to
trivial states, we manually provided a better one and used our Sec. 6 embedding to verify it. For
this subset of the benchmarks, the commute conditions and results are given in the Group 2 of
Fig. 2. We also report the time, the verification result, and whether it found the provided condition
to be complete. We were able to verify or reject the condition in all cases in a fraction of a second,
and in all but one case we were able to report on completeness as well.

Proc. ACM Program. Lang., Vol. 1, No. CONF, Article 1. Publication date: January 2018.

Veracity: Declarative Multicore Programming with Commutativity

1:21

Enforcing scoped serializability. In Sec. 5.4 we describe how to enforce scoped serializability.
We applied these methodologies to the benchmarks, but it could be automated in the future. For
many benchmarks (e.g. counter), a single lock could be naïvely synthesized to protect against
updates to a shared set of variables (Pattern 0). We used our snapshot approach (Pattern 1) on
commute1 and simple to avoid the need for locks at all. Other programs (conditional, matrix,
and loop-disjoint) did not need any locks. For dict, the only conflict is a single access to an
already linearizable concurrent hashtable. dihedral and ht-fizz-buzz are serializable due to loop
disjointness. Note that the benchmarks displayed in Apx. H are the original programs, to avoid
confusion. For our manually applied locking, please see the folder benchmarks-preprocessed in
benchmarks.zip from the supplemental materials.

Speedup. We next sought to confirm that, depending on the size of the computation, parallelization
offers a speedup over sequential execution. We executed the programs and collected execution time
measurements. Our benchmarks (see Apx. H) all involve some pure computation(s) of parametric
size 𝑁 . We varied this problem size, and recorded the speedup ratio (sequential execution time
divided by parallel execution time).

We took the geometric mean of the speedup ratios across 4 trials and plotted it against problem
size. The graph is shown in Fig. 3. On the x-axis we increased the problem size 𝑁 exponentially.
Many benchmarks had very similar performance. For legibility of the overall plot, we grouped
and colored those with similar performance characteristics. (The group labels do not imply any
particular semantic connection between the benchmarks.) In most of the benchmarks, speedups
were observed and asymptotically approached the expected 2× mark. Most of the payoff occurred
when the problem size was above 1,000,000, with some payoff occurring when the problem size
was above 1,000. In dihedral, the two threads can have differently sized workloads.

For the four benchmarks involving loops, we initially found no speedup. While this is expected
for loop-inter (because it only commutes in trivial cases) and for loop-simple (because threads
had to hold the lock for the entire loop), we discovered that the interpreter exhibited contention
when concurrently accessing the shared (and pure) context. Consequently the loop-disjoint and
loop-amt examples also had no speedup. This could be a subtle bug in Multicore OCaml. To confirm
that this is merely a bug and that our approach should yield speedup, we edited loop-disjoint
so that the fragments only accessed local variables (we do not do this in the original benchmark
because the translation does not translate scope), and indeed we observed speedup, as plotted in
Fig. 3.

Case studies. As commute blocks are novel, there are no existing programs with them. Thus we
explored two case studies whose concurrency could be re-formulated as Veracity programs with
commute statements.

Crowdfund Smart Contract. Min-
ers and validators in blockchain
systems repeatedly execute enor-
mous workloads of smart contract
transactions. These transactions
are currently executing sequen-
tially and recent proposals have
been made to leverage multicore
architectures through paralleliza-
tion [Dickerson et al. 2017; Saraph
and Herlihy 2019; Tran et al. 2021]
or through sharding [Pîrlea et al.

(a) Crowdfund

(b) Filesystem

Proc. ACM Program. Lang., Vol. 1, No. CONF, Article 1. Publication date: January 2018.

Fig. 4. Benchmark results for case studies.

1:22

Adam Chen, Parisa Fathololumi, Eric Koskinen, and Jared Pincus

2021]. We modeled parts of an Al-
gorand crowdfund smart contract5
as a case study. For performance, we tested four donate operations in parallel. Busy waits were
added at the end of each donate. As expected, we saw significant speedups; the plot of speedup
against problem size can be found in Fig. 4a. This example also illustrates that even better speedups
can be achieved using commute blocks with more than 2 fragments.

Filesystem operations. A common low-conflict use case of hashtables is mapping directories to
files. This could be in a user application, but also potentially in the implementation of a filesystem
(e.g. ScaleFS [Clements et al. 2015; Eqbal 2014]). We modeled filesystem operations as veracity
programs. Writing to files commutes when different files are being written to. The results are
shown in Fig. 4b. When writing sufficiently large files, we achieved large speedups (approaching
about 1.85×).

9 RELATED WORK
To our knowledge no prior works have explored bringing commutativity conditions into the
programming language syntax, and the ramifications thereof. We now survey other related works.
It is long known that there is a fundamental connection between commutativity and concur-
rency, dating back to work from the database communities describing how to ensure atomicity in
concurrent executions with locking protocols based on commutativity (e.g. [Bernstein 1966; Korth
1983; Weihl 1988]). Compiler-based parallelization of such arithmetic operations was introduced
by Rinard and Diniz [1997]. Later commutativity conditions have been used in many concurrent
programming contexts such as optimistic parallelism in graph algorithms [Kulkarni et al. 2008],
transactional memory [Dickerson et al. 2019; Herlihy and Koskinen 2008; Ni et al. 2007], dynamic
analysis [Dimitrov et al. 2014], and blockchain smart contracts [Bansal et al. 2020; Dickerson et al.
2017; Pîrlea et al. 2021]. Closed or open nested transactions [Ni et al. 2007] permit syntactically
nested transactions. Although nested transactions (NT) might seem akin to nested commute state-
ments, NTs still fall within the realm of a programming model in which users write a concurrent
program (explicitly forking and so on), as opposed to our sequential commute statements.

In more recent years several works have focused on reasoning about commutativity. Gehr et al.
[2015] describe a method based on black-box sampling. Aleen and Clark [2009] and Tripp et al.
[2011] identify sequences of actions that commute (via random interpretation and dynamic analysis,
resp.). Kim and Rinard [2011] verify commutativity conditions from specifications. Bansal et al.
[2018] synthesize commutativity conditions from ADT specifications in the tool Servois [Bansal
2018] (which we build on top of in Sec. 6). Koskinen and Bansal [2021] verify commutativity
conditions from source code. Pîrlea et al. [2021] discover commutativity through a static analysis
based on linear types and cardinality constraints. Najafzadeh et al. [2016] describe a tool for
weak consistency that checks commutativity formulae. Houshmand and Lesani [2019] describe
commutativity checking for replicated data types.

Numerous works are focused on synthesizing locks. Flanagan and Qadeer [2003] described a
type-based approach, and Vechev et al. [2010] used abstraction-refinement. Cherem et al. [2008]
and Golan-Gueta et al. [2015] describe automatic locking techniques for ensuring atomicity of,
respectively, transactions and multi-object atomic sections.

10 DISCUSSION AND FUTURE WORK
To our knowledge we have presented the first work on introducing commute statements into the
programming language. We have given a formal semantics, a correctness condition, methods for

5https://developer.algorand.org/solutions/example-crowdfunding-stateful-smart-contract-application/

Proc. ACM Program. Lang., Vol. 1, No. CONF, Article 1. Publication date: January 2018.

Veracity: Declarative Multicore Programming with Commutativity

1:23

ensuring parallelizability, and techniques for inferring/verifying commute conditions. This work is
embodied in the new Veracity language and front-end compiler.

Future Work. On the practical side, top priorities for future work are to explore back-end compila-
tion strategies to emit, e.g., concurrent IR. Our work can also be combined with other parallelization
strategies such as promises/futures [Chatterjee 1989; Liskov and Shrira 1988]. In our ongoing
implementation we seek to integrate existing data flow analyses to reduce locking while enforcing
scoped serializability. On the theoretical side we could generalize to 𝑁 -way commute statements
and some notion of object-oriented encapsulation, i.e., arbitrary concurrent objects. With more
encapsulated expert-written concurrent objects, more sequential commute programs can be written
to exploit them.

Our work can also be combined with invariant generation, which can inform commutativity

inference, as seen in the following example:

Example 10.1 (Combine with static analysis).

{ y < 0 } /* Example invariant */
commute _ {

/* if x is negative , this will reduce y */
{ y = y + 3* x; }
{ if (y <0) { x =2; } else { x =3; } } /* sensitive to whether y went below 0 */

}

Without the static knowledge of the invariant y < 0 before the commute condition, an excessively
complex commute condition would be inferred. However, if we first perform a static analysis, there
are fewer choices to be made during abstraction-refinement, quickly leading to a simpler and more
context-sensitive commute condition (in this case: 0 > y + 3*x && 2 == x). Our embedding
already supports such context information so in the future we aim to integrate Veracity with an
abstract interpreter.

Proc. ACM Program. Lang., Vol. 1, No. CONF, Article 1. Publication date: January 2018.

1:24

Adam Chen, Parisa Fathololumi, Eric Koskinen, and Jared Pincus

REFERENCES
2013. libcuckoo: A high-performance, concurrent hash table. http://efficient.github.io/libcuckoo/.
2014. Multicore OCaml. https://github.com/ocaml-multicore/ocaml-multicore.
2014. The Rust Programming Language. https://www.rust-lang.org/.
Farhana Aleen and Nathan Clark. 2009. Commutativity analysis for software parallelization: letting program transformations
see the big picture. In Proceedings of the 14th international conference on Architectural support for programming languages
and operating systems (ASPLOS-XII), Mary Lou Soffa and Mary Jane Irwin (Eds.). ACM, 241–252.

Eric Allen, David Chase, Joe Hallett, Victor Luchangco, Jan-Willem Maessen, Sukyoung Ryu, Guy L Steele Jr, Sam Tobin-
Hochstadt, Joao Dias, Carl Eastlund, et al. 2005. The Fortress language specification. Sun Microsystems 139, 140 (2005),
116.

Joe Armstrong. 1997. The development of Erlang. In Proceedings of the second ACM SIGPLAN international conference on

Functional programming. 196–203.

Kshitij Bansal, Eric Koskinen, and Omer Tripp. 2018. Automatic Generation of Precise and Useful Commutativity Conditions.
In Tools and Algorithms for the Construction and Analysis of Systems - 24th International Conference, TACAS 2018 (Lecture
Notes in Computer Science, Vol. 10805), Dirk Beyer and Marieke Huisman (Eds.). Springer, 115–132. https://doi.org/10.
1007/978-3-319-89960-2_7

Kshitij Bansal, Eric Koskinen, and Omer Tripp. 2020. Synthesizing Precise and Useful Commutativity Conditions. J. Autom.

Reason. 64, 7 (2020), 1333–1359. https://doi.org/10.1007/s10817-020-09573-w

Tripp Bansal, Koskinen. 2018. Servois: Synthesizing Commutativity Conditions. https://github.com/kbansal/servois.
Haniel Barbosa, Clark W. Barrett, Martin Brain, Gereon Kremer, Hanna Lachnitt, Makai Mann, Abdalrhman Mohamed,
Mudathir Mohamed, Aina Niemetz, Andres Nötzli, Alex Ozdemir, Mathias Preiner, Andrew Reynolds, Ying Sheng,
Cesare Tinelli, and Yoni Zohar. 2022. cvc5: A Versatile and Industrial-Strength SMT Solver. In Tools and Algorithms
for the Construction and Analysis of Systems - 28th International Conference, TACAS 2022, Held as Part of the European
Joint Conferences on Theory and Practice of Software, ETAPS 2022, Munich, Germany, April 2-7, 2022, Proceedings, Part
I (Lecture Notes in Computer Science, Vol. 13243), Dana Fisman and Grigore Rosu (Eds.). Springer, 415–442. https:
//doi.org/10.1007/978-3-030-99524-9_24

Rajkishore Barik, Zoran Budimlic, Vincent Cave, Sanjay Chatterjee, Yi Guo, David Peixotto, Raghavan Raman, Jun Shirako,
Sağnak Taşırlar, Yonghong Yan, et al. 2009. The habanero multicore software research project. In Proceedings of the 24th
ACM SIGPLAN conference companion on Object oriented programming systems languages and applications. 735–736.
Clark Barrett, Aaron Stump, Cesare Tinelli, et al. 2010. The smt-lib standard: Version 2.0. In Proceedings of the 8th international

workshop on satisfiability modulo theories (Edinburgh, England), Vol. 13. 14.

Arthur Bernstein. 1966. Analysis of programs for parallel processing. IEEE Transactions on Electronic Computers 15, 5 (1966),

757–763.

Bill Blume, Rudolf Eigenmann, Keith Faigin, John Grout, Jay Hoeflinger, David Padua, Paul Petersen, Bill Pottenger, Lawrence
Rauchwerger, Peng Tu, et al. 1994. Polaris: The next generation in parallelizing compilers. In Proceedings of the Seventh
Workshop on Languages and Compilers for Parallel Computing. Citeseer, 141–154.

William Blume and Rudolf Eigenmann. 1992. Performance analysis of parallelizing compilers on the Perfect BenchmarksTM

Programs. IEEE Transactions on Parallel and Distributed Systems 3, 6 (1992), 643–656.

Nathan G Bronson, Jared Casper, Hassan Chafi, and Kunle Olukotun. 2010. Transactional predication: high-performance
concurrent sets and maps for stm. In Proceedings of the 29th ACM SIGACT-SIGOPS symposium on Principles of distributed
computing. 6–15.

Stephen Brookes and Peter W O’Hearn. 2016. Concurrent separation logic. ACM SIGLOG News 3, 3 (2016), 47–65.
Pavol Čern`y, Thomas A Henzinger, Arjun Radhakrishna, Leonid Ryzhyk, and Thorsten Tarrach. 2013. Efficient synthesis
for concurrency by semantics-preserving transformations. In International Conference on Computer Aided Verification.
Springer, 951–967.

Arunodaya Chatterjee. 1989. Futures: a mechanism for concurrency among objects. In Supercomputing’89: Proceedings of the

1989 ACM/IEEE conference on Supercomputing. IEEE, 562–567.

Sigmund Cherem, Trishul M. Chilimbi, and Sumit Gulwani. 2008. Inferring locks for atomic sections. In Proceedings of the

ACM SIGPLAN 2008 Conference on Programming Language Design and Implementation (PLDI’08). 304–315.

Austin T Clements, M Frans Kaashoek, Nickolai Zeldovich, Robert T Morris, and Eddie Kohler. 2015. The scalable commuta-
tivity rule: Designing scalable software for multicore processors. ACM Transactions on Computer Systems (TOCS) 32, 4
(2015), 1–47.

Thomas Dickerson, Paul Gazzillo, Maurice Herlihy, and Eric Koskinen. 2017. Adding Concurrency to Smart Contracts. In
Proceedings of the ACM Symposium on Principles of Distributed Computing (Washington, DC, USA) (PODC ’17). ACM,
New York, NY, USA, 303–312. https://doi.org/10.1145/3087801.3087835

Thomas D. Dickerson, Eric Koskinen, Paul Gazzillo, and Maurice Herlihy. 2019. Conflict Abstractions and Shadow Speculation
for Optimistic Transactional Objects. In Programming Languages and Systems - 17th Asian Symposium, APLAS 2019, Nusa

Proc. ACM Program. Lang., Vol. 1, No. CONF, Article 1. Publication date: January 2018.

Veracity: Declarative Multicore Programming with Commutativity

1:25

Dua, Bali, Indonesia, December 1-4, 2019, Proceedings. 313–331.

Dimitar Dimitrov, Veselin Raychev, Martin Vechev, and Eric Koskinen. 2014. Commutativity Race Detection. In Proceedings

of the 35th ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI’14).

Tayfun Elmas. 2010. QED: a proof system based on reduction and abstraction for the static verification of concurrent
software. In Proceedings of the 32nd ACM/IEEE International Conference on Software Engineering-Volume 2. 507–508.
Rasha Eqbal. 2014. ScaleFS: A multicore-scalable file system. Ph.D. Dissertation. Massachusetts Institute of Technology.
Gidon Ernst. 2020. A Complete Approach to Loop Verification with Invariants and Summaries. https://doi.org/10.48550/

ARXIV.2010.05812

Bin Fan, David G Andersen, and Michael Kaminsky. 2013. Memc3: Compact and concurrent memcache with dumber
caching and smarter hashing. In 10th {USENIX} Symposium on Networked Systems Design and Implementation ({NSDI}
13). 371–384.

Cormac Flanagan and Shaz Qadeer. 2003. A type and effect system for atomicity. In Proceedings of the ACM SIGPLAN 2003
Conference on Programming Language Design and Implementation 2003, San Diego, California, USA, June 9-11, 2003, Ron
Cytron and Rajiv Gupta (Eds.). ACM, 338–349. https://doi.org/10.1145/781131.781169

Timon Gehr, Dimitar Dimitrov, and Martin T. Vechev. 2015. Learning Commutativity Specifications. In Computer Aided
Verification - 27th International Conference, CAV 2015, San Francisco, CA, USA, July 18-24, 2015, Proceedings, Part I. 307–323.
https://doi.org/10.1007/978-3-319-21690-4_18

Guy Golan-Gueta, G Ramalingam, Mooly Sagiv, and Eran Yahav. 2015. Automatic scalable atomicity via semantic locking.

ACM SIGPLAN Notices 50, 8 (2015), 31–41.

Max Grossman, Alina Simion Sbirlea, Zoran Budimlić, and Vivek Sarkar. 2010. CnC-CUDA: declarative programming for

GPUs. In International Workshop on Languages and Compilers for Parallel Computing. Springer, 230–245.

Rachid Guerraoui and Michal Kapalka. 2008. On the correctness of transactional memory. In Proceedings of the 13th ACM
SIGPLAN Symposium on Principles and Practice of Parallel Programming (PPoPP’08). ACM, 175–184. https://doi.org/10.
1145/1345206.1345233

Tim Harris and Keir Fraser. 2003. Language support for lightweight transactions. In Proceedings of the 18th ACM SIGPLAN
conference on Object-Oriented Programing, Systems, Languages, and Applications (OOPSLA’03) (Anaheim, California, USA).
ACM Press, 388–402. https://doi.org/10.1145/949305.949340

Ahmed Hassan, Roberto Palmieri, and Binoy Ravindran. 2014. Optimistic transactional boosting. In Proceedings of the 19th

ACM SIGPLAN symposium on Principles and practice of parallel programming. 387–388.

Maurice Herlihy and Eric Koskinen. 2008. Transactional Boosting: A Methodology for Highly Concurrent Transactional
Objects. In Proceedings of the 13th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming (PPoPP’08)
(Salt Lake City, Utah, United States).

Maurice Herlihy, Victor Luchangco, Mark Moir, and William N. Scherer, III. 2003. Software transactional memory for
dynamic-sized data structures. In Proceedings of the 22nd annual symposium on Principles of distributed computing
(PODC’03). 92–101. https://doi.org/10.1145/872035.872048

Maurice Herlihy and J. Eliot B. Moss. 1993. Transactional memory: architectural support for lock-free data structures. In
Proceedings of the 20th Annual International Symposium on Computer Architecture (ISCA’93) (San Diego, California, United
States). ACM Press, 289–300. https://doi.org/10.1145/165123.165164

Maurice P. Herlihy and Jeannette M. Wing. 1990. Linearizability: a correctness condition for concurrent objects. ACM
Transactions on Programming Languages and Systems (TOPLAS) 12, 3 (1990), 463–492. https://doi.org/10.1145/78969.78972
Farzin Houshmand and Mohsen Lesani. 2019. Hamsaz: replication coordination analysis and synthesis. Proc. ACM Program.

Lang. 3, POPL (2019), 74:1–74:32. https://doi.org/10.1145/3290387

Deokhwan Kim and Martin C. Rinard. 2011. Verification of semantic commutativity conditions and inverse operations
on linked data structures. In Proceedings of the 32nd ACM SIGPLAN Conference on Programming Language Design and
Implementation (PLDI’11). 528–541.

Henry F. Korth. 1983. Locking Primitives in a Database System. J. ACM 30, 1 (1983), 55–79. https://doi.org/10.1145/322358.

322363

Eric Koskinen and Kshitij Bansal. 2021. Decomposing Data Structure Commutativity Proofs with $m\!n$-Differencing.
In Verification, Model Checking, and Abstract Interpretation - 22nd International Conference, VMCAI 2021, Copenhagen,
Denmark, January 17-19, 2021, Proceedings (Lecture Notes in Computer Science, Vol. 12597), Fritz Henglein, Sharon Shoham,
and Yakir Vizel (Eds.). Springer, 81–103. https://doi.org/10.1007/978-3-030-67067-2_5

Daniel Kroening, Natasha Sharygina, Stefano Tonetta, Aliaksei Tsitovich, and Christoph M Wintersteiger. 2008. Loop
summarization using abstract transformers. In International Symposium on Automated Technology for Verification and
Analysis. Springer, 111–125.

Milind Kulkarni, Donald Nguyen, Dimitrios Prountzos, Xin Sui, and Keshav Pingali. 2011. Exploiting the commutativity
lattice. In Proceedings of the 32nd ACM SIGPLAN Conference on Programming Language Design and Implementation
(PLDI’11). 542–555.

Proc. ACM Program. Lang., Vol. 1, No. CONF, Article 1. Publication date: January 2018.

1:26

Adam Chen, Parisa Fathololumi, Eric Koskinen, and Jared Pincus

Milind Kulkarni, Keshav Pingali, Ganesh Ramanarayanan, Bruce Walter, Kavita Bala, and L Paul Chew. 2008. Optimistic

parallelism benefits from data partitioning. ACM SIGOPS Operating Systems Review 42, 2 (2008), 233–243.

Milind Kulkarni, Keshav Pingali, Bruce Walter, Ganesh Ramanarayanan, Kavita Bala, and L. Paul Chew. 2007. Optimistic
parallelism requires abstractions. In Proceedings of the ACM SIGPLAN 2007 Conference on Programming Language Design
and Implementation (PLDI’07). 211–222.

Xiaozhou Li, David G Andersen, Michael Kaminsky, and Michael J Freedman. 2014. Algorithmic improvements for fast

concurrent cuckoo hashing. In Proceedings of the Ninth European Conference on Computer Systems. 1–14.

Zhiyuan Li, Pen-Chung Yew, and Chuan-Qi Zhu. 1990. An Efficient Data Dependence Analysis for Parallelizing Compilers.

IEEE Trans. Parallel Distributed Syst. 1, 1 (1990), 26–34. https://doi.org/10.1109/71.80122

Sam Lindley. 2007. Implementing deterministic declarative concurrency using sieves. In Proceedings of the 2007 workshop on

Declarative aspects of multicore programming. 45–49.

Barbara Liskov and Liuba Shrira. 1988. Promises: Linguistic support for efficient asynchronous procedure calls in distributed

systems. ACM Sigplan Notices 23, 7 (1988), 260–267.

Yujie Liu, Kunlong Zhang, and Michael Spear. 2014. Dynamic-sized nonblocking hash tables. In Proceedings of the 2014 ACM

symposium on Principles of distributed computing. 242–251.

Mahsa Najafzadeh, Alexey Gotsman, Hongseok Yang, Carla Ferreira, and Marc Shapiro. 2016. The CISE tool: proving
weakly-consistent applications correct. In Proceedings of the 2nd Workshop on the Principles and Practice of Consistency
for Distributed Data, PaPoC@EuroSys 2016, London, United Kingdom, April 18, 2016. 2:1–2:3.

Yang Ni, Vijay S. Menon, Ali-Reza Adl-Tabatabai, Antony L. Hosking, Richard L. Hudson, J. Eliot B. Moss, Bratin Saha, and
Tatiana Shpeisman. 2007. Open nesting in software transactional memory. In Proceedings of the 12th ACM SIGPLAN
symposium on Principles and Practice of Parallel Programming (PPoPP’07). 68–78.

Dominic A Orchard, Max Bolingbroke, and Alan Mycroft. 2010. Ypnos: declarative, parallel structured grid programming. In

Proceedings of the 5th ACM SIGPLAN workshop on Declarative aspects of multicore programming. 15–24.

Susan Owicki and David Gries. 1976. An axiomatic proof technique for parallel programs I. Acta informatica 6, 4 (1976),

319–340.

Christos H Papadimitriou. 1979. The serializability of concurrent database updates. Journal of the ACM (JACM) 26, 4 (1979),

631–653.

George Pîrlea, Amrit Kumar, and Ilya Sergey. 2021. Practical smart contract sharding with ownership and commutativity
analysis. In PLDI ’21: 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation,
Virtual Event, Canada, June 20-25, 20211, Stephen N. Freund and Eran Yahav (Eds.). ACM, 1327–1341. https://doi.org/10.
1145/3453483.3454112

Raghu Prabhakar and Rohit Kumar. 2011. Concurrent programming with Go. Technical Report. Citeseer.
Chris Purcell and Tim Harris. 2005. Non-blocking hashtables with open addressing. In International Symposium on Distributed

Computing. Springer, 108–121.

Martin C. Rinard and Pedro C. Diniz. 1997. Commutativity Analysis: A New Analysis Technique for Parallelizing Compilers.
ACM Transactions on Programming Languages and Systems (TOPLAS) 19, 6 (November 1997), 942–991. citeseer.ist.psu.
edu/rinard97commutativity.html

Bratin Saha, Ali-Reza Adl-Tabatabai, Richard L. Hudson, Chi Cao Minh, and Benjamin Hertzberg. 2006. McRT-STM: a high
performance software transactional memory system for a multi-core runtime. In Proceedings of the 11th ACM SIGPLAN
Symposium on Principles and Practice of Parallel Programming (PPoPP’06). 187–197.

Vikram Saraph and Maurice Herlihy. 2019. An empirical study of speculative concurrency in ethereum smart contracts.

arXiv preprint arXiv:1901.01376 (2019).

Jake Silverman and Zachary Kincaid. 2019. Loop summarization with rational vector addition systems. In International

Conference on Computer Aided Verification. Springer, 97–115.

Guy L. Steele Jr. 2005. Parallel Programming and Parallel Abstractions in Fortress. In 14th International Conference on Parallel
Architectures and Compilation Techniques (PACT 2005), 17-21 September 2005, St. Louis, MO, USA. IEEE Computer Society,
157. https://doi.org/10.1109/PACT.2005.34

Thi Hong Tran, Hoai Luan Pham, Tri Dung Phan, and Yasuhiko Nakashima. 2021. BCA: A 530-mW Multicore Blockchain
Accelerator for Power-Constrained Devices in Securing Decentralized Networks. IEEE Transactions on Circuits and
Systems I: Regular Papers 68, 10 (2021), 4245–4258.

Omer Tripp, Greta Yorsh, John Field, and Mooly Sagiv. 2011. HAWKEYE: effective discovery of dataflow impediments to
parallelization. In Proceedings of the 26th Annual ACM SIGPLAN Conference on Object-Oriented Programming, Systems,
Languages, and Applications, OOPSLA 2011. 207–224. https://doi.org/10.1145/2048066.2048085

Viktor Vafeiadis. 2010. Automatically proving linearizability. In International Conference on Computer Aided Verification.

Springer, 450–464.

Martin Vechev, Eran Yahav, and Greta Yorsh. 2010. Abstraction-guided synthesis of synchronization. In Proceedings of the

37th annual ACM SIGPLAN-SIGACT symposium on Principles of programming languages. 327–338.

Proc. ACM Program. Lang., Vol. 1, No. CONF, Article 1. Publication date: January 2018.

Veracity: Declarative Multicore Programming with Commutativity

1:27

William Weihl. 1988. Commutativity-Based Concurrency Control for Abstract Data Types. IEEE Trans. Comput. 37, 12

(1988), 1488–1505. https://doi.org/10.1109/12.9728

William E. Weihl. 1983. Data-dependent concurrency control and recovery (Extended Abstract). In Proceedings of the 2nd
annual ACM symposium on Principles of Distributed Computing (PODC’83). ACM Press, 63–75. https://doi.org/10.1145/
800221.806710

Xiaofei Xie, Bihuan Chen, Liang Zou, Yang Liu, Wei Le, and Xiaohong Li. 2017. Automatic loop summarization via path

dependency analysis. IEEE Transactions on Software Engineering 45, 6 (2017), 537–557.

Proc. ACM Program. Lang., Vol. 1, No. CONF, Article 1. Publication date: January 2018.

1:28

Adam Chen, Parisa Fathololumi, Eric Koskinen, and Jared Pincus

⟨𝑣, 𝜎⟩ (cid:123) ⟨𝜎 (𝑣), 𝜎⟩
⟨deref 𝑝, 𝜎⟩ (cid:123) ⟨𝜎 (𝑝), 𝜎⟩
⟨𝑐0 [𝑐1], 𝜎⟩ (cid:123) ⟨
𝑐0 [𝑐1] : 𝑟𝑒 𝑓
(cid:74)
⟨new 𝑡 [𝑐], 𝜎⟩ (cid:123) ⟨𝑝, 𝜎 [𝑝 ↦→ new 𝑡 [𝑐] : ref]⟩
⟨new hashtable[𝑡0, 𝑡1], 𝜎⟩ (cid:123) ⟨𝑝, 𝜎 [𝑝 ↦→ new hashtable[𝑡0, 𝑡1] : ref]⟩

𝜎, 𝜎⟩
(cid:75)

⟨uop 𝑐, 𝜎⟩ (cid:123) ⟨
uop 𝑐
(cid:74)
⟨𝑐0 bop 𝑐1, 𝜎⟩ (cid:123) ⟨
𝑐0 bop 𝑐1
(cid:74)
⟨true?𝑒0 : 𝑒1, 𝜎⟩ (cid:123) ⟨𝑒0, 𝜎⟩
⟨false?𝑒0 : 𝑒1, 𝜎⟩ (cid:123) ⟨𝑒1, 𝜎⟩

𝜎, 𝜎⟩
(cid:75)

𝜎, 𝜎⟩
(cid:75)

⟨𝑐.fieldname, 𝜎⟩ (cid:123) ⟨
𝑐.fieldname
(cid:74)

𝜎, 𝜎⟩
(cid:75)

⟨𝑙𝑣𝑎𝑙 = 𝑐, 𝜎⟩ (cid:123) ⟨skip, 𝜎 [𝑙𝑣𝑎𝑙 ↦→ 𝑐]⟩
⟨𝑡 𝑣 = 𝑐, 𝜎⟩ (cid:123) ⟨skip, 𝜎 [𝑣 ↦→ 𝑐]⟩

⟨if(true){𝑠0}else{𝑠1}, 𝜎⟩ (cid:123) ⟨𝑠0, 𝜎⟩
⟨if(false){𝑠1}else{𝑠1}, 𝜎⟩ (cid:123) ⟨𝑠1, 𝜎⟩
⟨for(𝑣-𝑑𝑒𝑐𝑙𝑠; 𝑒?; 𝑠0?){𝑠1}, 𝜎⟩ (cid:123) ⟨𝑣-𝑑𝑒𝑐𝑙𝑠; while(𝑒){𝑠1; 𝑠0}, 𝜎⟩†

⟨while(𝑒){𝑠}, 𝜎⟩ (cid:123) ⟨if(𝑒){𝑠; while(𝑒){𝑠}}else{skip}, 𝜎⟩

⟨skip; 𝑠, 𝜎⟩ (cid:123) ⟨𝑠, 𝜎⟩

⟨commute(false){{𝑠0}{𝑠1}}, 𝜎⟩ (cid:123) ⟨𝑠0; 𝑠1, 𝜎⟩

Fig. 5. Redex reductions.
†: To satisfy syntax in 𝑣-𝑑𝑒𝑐𝑙𝑠, replace , with ; and 𝜖 with skip.

A REDEX REDUCTION RULES
Reductions are of the form ⟨𝑟, 𝜎⟩ (cid:123) ⟨r’, 𝜎 ′⟩, where 𝑟 ′ is the reduced expression (not necessarily
another redex). See Figure 5 for a full list of redex reductions.

B OMITTED REDUCTION RULES
B.1 Sequential Semantics 𝑠𝑒𝑞
Lift the previously defined semantics.

⟨𝑠, 𝜎⟩ (cid:123) ⟨𝑠 ′, 𝜎 ′⟩
⟨𝑠, 𝜎⟩ (cid:123)𝑠𝑒𝑞 ⟨𝑠 ′, 𝜎 ′⟩

Lift-Seq

𝑟𝑠𝑒𝑞

::= commute(true){{𝑠1}{𝑠2}}

Add the redex:

With the redex reduction:

⟨commute(true){{𝑠0}{𝑠1}}, 𝜎⟩ (cid:123)𝑠𝑒𝑞

⟨𝑠0; 𝑠1, 𝜎⟩

And corresponding small step rule:

⟨𝑟𝑠𝑒𝑞, 𝜎⟩ (cid:123)𝑠𝑒𝑞 ⟨𝑟 ′
⟨𝐻 [𝑟𝑠𝑒𝑞], 𝜎⟩ (cid:123)𝑠𝑒𝑞 ⟨𝐻 [𝑟 ′

𝑠𝑒𝑞, 𝜎 ′⟩

𝑠𝑒𝑞], 𝜎 ′⟩

Small-Step-Seq

B.2 Nondeterministic Semantics 𝑛𝑑
Like 𝑠𝑒𝑞, for 𝑛𝑑, we similarly extend redexes:

𝑟𝑛𝑑

::= commute(true){{𝑠1}, {𝑠2}}

Proc. ACM Program. Lang., Vol. 1, No. CONF, Article 1. Publication date: January 2018.

Veracity: Declarative Multicore Programming with Commutativity

1:29

and lift the redex reductions into the (cid:123)𝑛𝑑 reductions. Much like the sequential case, lift the redex
semantics:

⟨𝑠, 𝜎⟩ (cid:123) ⟨𝑠 ′, 𝜎 ′⟩
⟨𝑠, 𝜎⟩ (cid:123)𝑛𝑑 ⟨𝑠 ′, 𝜎 ′⟩

Lift-Nd

And corresponding small step rule:

⟨𝑟𝑛𝑑, 𝜎⟩ (cid:123)𝑛𝑑 ⟨𝑟 ′
𝑛𝑑
⟨𝐻 [𝑟𝑛𝑑 ], 𝜎⟩ (cid:123)𝑛𝑑 ⟨𝐻 [𝑟 ′

, 𝜎 ′⟩
𝑛𝑑 ], 𝜎 ′⟩

Small-Step-Nd

B.3 Parallel Semantics 𝑝𝑎𝑟
Lifting the redex semantics:

⟨𝑠, 𝜎⟩ (cid:123) ⟨𝑠 ′, 𝜎 ′⟩
⟨𝑠, 𝜎⟩ (cid:123)𝑝𝑎𝑟 ⟨𝑠 ′, 𝜎 ′⟩

Lift-Par

B.4 Properties of the Given Semantics
We assert that the separation of a statement into a redex and a context is unique:

Lemma B.1 (Redex Uniqeness). ∀𝑠 : ∃!𝐻, 𝑟 : 𝑠 = 𝐻 [𝑟 ]

Proof. Proof via analysis of the grammatical structure of 𝐻 . Note that the nonterminal 𝐻 always
occurs on the left, or to the right of a constant, which is not reducible. Likewise, every redex is
□
syntactically distinct.

Note that this does not entail determinism, as there are multiple ways to reduce some redexes
and concurrent configurations. However, it does mean the syntax upon which the next step is
performed is well-defined. In the absence of commute structures, we are able to assert determinism:

Lemma B.2 (Conditional Determinism). ∀𝑠, 𝜎, 𝐻, 𝑟

𝑟 ≠
commute(true){{𝑠0}{𝑠1}}) → ∃𝑠 ′, 𝜎 ′, 𝑠 ′′, 𝜎 ′′ : (⟨𝑠, 𝜎⟩ (cid:123)𝑠𝑒𝑚 ⟨𝑠 ′, 𝜎 ′⟩) ∧ (⟨𝑠, 𝜎⟩ (cid:123)𝑠𝑒𝑚 ⟨𝑠 ′′, 𝜎 ′′⟩) →
(𝑠 ′ = 𝑠 ′′ ∧ 𝜎 ′ = 𝜎 ′′)

𝑠 = 𝐻 [𝑟 ] ∧ (∀𝑠0, 𝑠1

:

:

Proof. Proof via inversion of the redex rules. Each non-commute(true) syntax has a unique
□

rule that can apply to it.

Lemma B.3. Every commute block guard in 𝑠 is a sufficient commutativity condition =⇒ 𝑠 is

deterministic in 𝑛𝑑.

Proof. Recall Definition 3.1 (Sufficient Commutativity Condition). Proof by induction on
commute blocks. If the inside of a commute block is deterministic, and the order does not mat-
□
ter because they commute, then the end state is determined.

Lemma B.4. ∀𝑠, 𝜎 :

𝑠
(cid:74)

𝑠𝑒𝑞 (𝜎) ⊆
(cid:75)

𝑠
(cid:74)

𝑛𝑑 (𝜎) ⊆
(cid:75)

𝑠
(cid:74)

𝑝𝑎𝑟 (𝜎)
(cid:75)

Proof. Proof by the construction of executions. We may choose to apply the
⟨commute(true){{𝑠0}{𝑠1}}, 𝜎⟩ (cid:123)𝑛𝑑 ⟨𝑠0; 𝑠1, 𝜎⟩ rule every time, which is identical to the rule in
𝑠𝑒𝑞. Similarly, we may choose to apply Proj-Left (or Proj-Right) to completion to simulate a 𝑐𝑜𝑚
□
trace in 𝑝𝑎𝑟 .

Lemma B.5. 𝑠 is deterministic in 𝑠𝑒𝑚 =⇒

𝑠
(cid:74)

𝑠𝑒𝑚 =
(cid:75)

𝑠
(cid:74)

𝑠𝑒𝑞
(cid:75)

Lemma B.4, ∀𝜎 :

Proof. Proof: Note that 𝑠 is always deterministic in 𝑠𝑒𝑞, as redex reductions are all unique. By
□

𝑠𝑒𝑞. But they are both singleton sets. Thus they must be equal.
𝑠
(cid:75)
(cid:74)
Proc. ACM Program. Lang., Vol. 1, No. CONF, Article 1. Publication date: January 2018.

𝑠𝑒𝑚 ⊇
(cid:75)

𝑠
(cid:74)

1:30

Adam Chen, Parisa Fathololumi, Eric Koskinen, and Jared Pincus

B.5 Serializability adapted to nested commute blocks
Traditional definitions of serializability adapted to our setting may be defined as something like so:

Definition B.6 (Serial Execution). An execution 𝜀 is serial when:

∀𝑝, 𝑝 ′ ∈ {𝐿𝑛, 𝑅𝑛 | 𝑛 ∈ N}∗ :
𝑝 is not a prefix or extension of 𝑝 ′ =⇒

(∀ℓ, ℓ ′ ∈ 𝜀 : ℓ.fr = 𝑝∧
ℓ ′.fr = 𝑝 ′ =⇒ ℓ ≤𝜀 ℓ ′)∨
(∀ℓ, ℓ ′ ∈ 𝜀 : ℓ.fr = 𝑝∧
ℓ ′.fr = 𝑝 ′ =⇒ ℓ ′ ≤𝜀 ℓ)

To briefly argue for this definition, the assumption is that we are not talking about a thread’s
children (with which it will necessarily have interleaving). The body asserts that each thread’s
steps are completely separated from all (non-descendant) threads. As we did with s-serializability,
take a execution to be serializable if there is an equivalent execution that is serial, and a program 𝑠
to be serializable when all its executions are serializable.

Proposition 1. ∀𝑠 : 𝑅𝑆𝑒𝑟𝑖𝑎𝑙𝑖𝑧𝑎𝑏𝑙𝑒 (𝑠) =⇒ 𝑆𝑒𝑟𝑖𝑎𝑙𝑖𝑧𝑎𝑏𝑙𝑒 (𝑠)

This should be clear as for any prefix 𝑝, we have 𝑝𝐿𝑘0 is neither a prefix nor an extension of 𝑝𝑅𝑘1.

Thus any execution that is s-serializable is serializable, so the same holds for any program.

C LOCKING
Augment states to also hold a map 𝐿 : N → B representing whether a lock associated with a number
is held. That is, Σ := (𝑉 𝑎𝑟𝑛𝑎𝑚𝑒𝑠 ⊔ 𝑀𝑒𝑚𝐿𝑜𝑐𝑠 ⇀ 𝑉 ) × 𝐿. Let us suppose that we can distinguish
memory locations from bare natural numbers, so that we may use 𝜎 (𝑛) to unambiguously refer to
the acquired state of lock n.

Suppose that if we have the sum of states 𝜎0 ⊕ 𝜎1, we always try to acquire from the set of locks

associated with the right-most state (𝜎1 here). That is, there is only one global set of locks.

Add redexes:

Add contexts:

Add special rules:

𝑟

::= · · · | 𝑙𝑜𝑐𝑘 (𝑛) | 𝑢𝑛𝑙𝑜𝑐𝑘 (𝑛)

𝐻 ::= · · · | 𝑙𝑜𝑐𝑘 (𝐻 ) | 𝑢𝑛𝑙𝑜𝑐𝑘 (𝐻 )

𝜎 (𝑛) = false
⟨𝑙𝑜𝑐𝑘 (𝑛), 𝜎⟩ (cid:123)𝑝𝑎𝑟 ⟨skip, 𝜎 [𝑛 ↦→ true]⟩

𝐿𝑜𝑐𝑘

⟨𝑢𝑛𝑙𝑜𝑐𝑘 (𝑛), 𝜎⟩ (cid:123)𝑝𝑎𝑟 ⟨skip, 𝜎 [𝑛 ↦→ false]⟩

𝑈 𝑛𝑙𝑜𝑐𝑘

D PROOF OF THE MAIN THEOREM

Lemma 5.4. S-Serializable(𝑠) =⇒
Proof. Recall from Lemma B.4 that ∀𝜎 :

𝑛𝑑 =
(cid:75)

𝑠
(cid:74)

𝑠
(cid:74)

𝑝𝑎𝑟
(cid:75)

𝑠
(cid:74)

𝑛𝑑 (𝜎) ⊇
(cid:75)

𝑠
(cid:74)

𝑛𝑑 (𝜎) ⊆
𝑠
(cid:74)
(cid:75)
𝑝𝑎𝑟 (𝜎).
(cid:75)

𝑅𝑆𝑒𝑎𝑟𝑙𝑖𝑧𝑎𝑏𝑙𝑒 (𝑠) =⇒ ∀𝜎 :

Apply induction on the maximum nesting of commute blocks in s (alternatively, the maximum
fragment length for transitions in executions on 𝑠 in 𝑝𝑎𝑟 ). It is clear that if the maximum length of
a fragment is 0, every 𝑝𝑎𝑟 execution of never forks, and thus contains no commute(true). Stronger,
by (repeated application of) Lemma B.2, there is a unique execution. Furthermore, as the semantics
agree in the absence of commute(true), this is an execution in 𝑛𝑑. Thus the base case is satisfied.

𝑠
(cid:74)

𝑝𝑎𝑟 (𝜎). Thus it only remains to show
(cid:75)

Proc. ACM Program. Lang., Vol. 1, No. CONF, Article 1. Publication date: January 2018.

Veracity: Declarative Multicore Programming with Commutativity

1:31

Suppose the statement holds for all 𝑠 with commute blocks nested at most 𝑘 times. Suppose an
r-serializable 𝑠 is given, with commute blocks nested 𝑘 + 1 times. Suppose an arbitrary 𝜎 is given.
Consider an execution 𝜀, with ⟨𝑠, 𝜎⟩ ⇓𝑝𝑎𝑟 𝜀. Since 𝑠 is r-serializable, there exists an r-serial execution
𝜀 ′ with ⟨𝑠, 𝜎⟩ ⇓𝑝𝑎𝑟 𝜀 ′ and 𝜀𝑓 .𝑠𝑡 = 𝜀 ′
𝑓

.𝑠𝑡. Construct an execution 𝜀𝑛𝑑 .
While a transition in 𝜀 ′ is not that of a commute(true) block, make the same transition in 𝜀𝑛𝑑 .
When 𝜀 ′ performs a fork, the configuration will become ⟨(⟨𝑠𝐿, ∅⟩, ⟨𝑠𝑅, ∅⟩), 𝑠 ′, 𝜎 ′⟩. If the next step
is a Join, the case is trivial (the block is a no-op). Otherwise, the next transition will use either
Proj-Left or Proj-Right. Suppose it is a Proj-Left. Then by r-serializability, the 𝑠𝐿 will execute to
completion before and transitions happen on 𝑠𝑅. In other words, ⟨(⟨𝑠𝐿, ∅⟩, ⟨𝑠𝑅, ∅⟩), 𝑠 ′, 𝜎 ′⟩ (cid:123)∗
𝑝𝑎𝑟
⟨(⟨skip, 𝜎𝐿⟩, ⟨𝑠𝑅, ∅⟩), 𝑠 ′, 𝜎 ′
𝐿⟩. By applying the rule used in the premise of Proj-Left, obtain an execu-
𝐿.
tion ⟨𝑠𝐿, ∅ ⊕ 𝜎 ′⟩ ⇓𝑝𝑎𝑟 𝜀𝐿, with 𝜀𝐿,𝑓 .𝑠𝑡 = 𝜎𝐿 ⊕ 𝜎 ′

Note that 𝑠𝐿 is r-serializable; from any trace take only the transition relating to the current
fragment, and remove the leading 𝐿𝑘 ; as r-serializabilty held for all prefixes, it is preserved for this
set of prefixes that all started with 𝐿𝑘 . Furthermore, 𝑠𝐿 contains commute blocks with nesting at
𝑛𝑑 [∅ ⊕ 𝜎 ′]. Thus there exists an
most 𝑘, so by applying the inductive hypothesis, 𝜎𝐿 ⊕ 𝜎 ′
(cid:75)
execution ⟨𝑠𝐿, ∅ ⊕ 𝜎 ′⟩ ⇓𝑛𝑑 𝜀𝑛𝑑,𝐿. To 𝜀𝑛𝑑 , apply the rule

𝑠𝐿
(cid:74)

𝐿 ∈

⟨commute(true){{𝑠𝐿 }, {𝑠𝑅 }}, 𝜎⟩ (cid:123)𝑛𝑑 ⟨𝑠𝐿; 𝑠𝑅, 𝜎⟩

(with context H = • ; 𝑠 ′). Every redex reduction in 𝜀𝑛𝑑,𝐿 may be applied to 𝜀𝑛𝑑 ; apply the grammar
rule 𝐻 = 𝐻 ; 𝑠 ′ to each context in 𝜀𝑛𝑑,𝐿. Afterwards, apply the skip; 𝑠 ′ (cid:123) 𝑠 ′ transition (which does
not change the state). The left summand of the state may be discarded, as all values declared within
the commute block go out of scope. Reason similarly about the right fragment to append an 𝜀𝑛𝑑,𝑅 to
𝜀𝑛𝑑 . Finally, in 𝜀 ′, Join is used; as this only discards the local states, which are already lost to scope
in 𝜀𝑛𝑑 , it may be disregarded.

If Proj-Right was applied first, the case is symmetrical, and merely apply the transition

⟨commute(true){{𝑠𝐿 }{𝑠𝑅 }}, 𝜎⟩ (cid:123)𝑛𝑑 ⟨𝑠𝑅; 𝑠𝐿, 𝜎⟩

instead. Continue, repeating this process, until the end of 𝜀 ′. Note that at every point outside
of the commute(true) blocks, the state in 𝜀 ′ is equal to the state in our constructed 𝜀𝑛𝑑 . Thus
□
𝑝𝑎𝑟 (𝜎) and the proof is complete.
𝜀 ′
𝑓
(cid:75)

𝑛𝑑 (𝜎) ⊇
(cid:75)

.𝑠𝑡 = 𝜀𝑛𝑑,𝑓 .𝑠𝑡, so

𝑠
(cid:74)

𝑠
(cid:74)

Theorem 5.5 (Sufficient Condition for Parallelizability). If every commutativity condition

in 𝑠 is valid and S-Serializable(𝑠), then 𝑠 is parallelizable.

𝑠
(cid:74)

Proof. By Lemma 5.4,

𝑝𝑎𝑟 . By Lemma B.3,
(cid:75)
E SUGARING AND DIFFERENCES FROM FORMAL SEMANTICS
For theoretical or practical reasons, the implemented language in Veracity differs from the formal
language given in a few key ways.

𝑛𝑑 . Thus
(cid:75)

𝑠𝑒𝑞 =
(cid:75)

𝑠𝑒𝑞 =
(cid:75)

𝑛𝑑 =
(cid:75)

𝑝𝑎𝑟 .
(cid:75)

𝑠
(cid:74)

𝑠
(cid:74)

𝑠
(cid:74)

𝑠
(cid:74)

𝑠
(cid:74)

□

• No explicit dereferencing. We do not allow the programmer ways for explicitly manipulat-
ing pointers. Dereferences are performed automatically when evaluating indexing operations.
• Indexing operations as lvals. We allow the programmer to use nested indexing operations
on the left hand side of assign expressions. This can be seen as syntactic sugar for indexing
into a temporary reference, then assigning to that reference.

• IO. For practical purposes, it is useful to allow for IO operations. This includes both read-

ing/writing to stdout/stdin, and also file operations.

• Function calls. Veracity implements a call stack and allows for Veracity subroutines to be
called. Semantically, this is not dissimilar to inlining the body in a new scope. None of our

Proc. ACM Program. Lang., Vol. 1, No. CONF, Article 1. Publication date: January 2018.

1:32

Adam Chen, Parisa Fathololumi, Eric Koskinen, and Jared Pincus

benchmarks used recursion, and the behavior of commute blocks in recursive functions is
undefined.

• Commute seq/par. For testing purposes, Veracity commute statements are written as
commute_seq or commute_par. The former corresponds to the semantics in 𝑛𝑑, and the
latter to the semantics in 𝑝𝑎𝑟 .

Proc. ACM Program. Lang., Vol. 1, No. CONF, Article 1. Publication date: January 2018.

Veracity: Declarative Multicore Programming with Commutativity

1:33

F LOOP SUMMARY
We document how we summarized loops using a combination of Korn and manual reasoning. We
list loop excerpts from benchmarks with loops, with their summary in conjunctive normal form.

(1) Benchmark: loop-amt.vcy

(a) Loop:

while (i > 0) {

if (i % 2 == 0) { amt = amt * -1; }
else { amt = amt * i * -1; }
i = i - 1; }

Summary:
i == 0 ∧ (amt == 𝑎𝑚𝑡𝑜𝑙𝑑 ∨ 𝑖𝑜𝑙𝑑 % 2 ≠ 0)

(2) Benchmark: loop-disjoint.vcy

(a) Loop:

assume (x > 0);
while (x > 0) { x = x - 1; }
Summary:
x == 0

(b) Loop:

assume (y > 0);
while (y > 0) { y = y - 1; }
Summary:
y == 0

(3) Benchmark: loop-inter.vcy

(a) Loop:

assume (x > 0);
while (x > 0) { x = x - 1; s1 = s1 + 1; }
Summary:
x == 0 ∧ s1 == 𝑠1𝑜𝑙𝑑 + 𝑥𝑜𝑙𝑑

(b) Loop:

assume (y > 0);
while (y > 0) { y = y - 1; s2 = s2 + x; }
Summary:
y == 0 ∧ 𝑥𝑜𝑙𝑑 == x ∧ s2 == 𝑠2𝑜𝑙𝑑 + (𝑥𝑜𝑙𝑑 * 𝑦𝑜𝑙𝑑 )

(4) Benchmark: loop-simple.vcy

(a) Loop:

while (y > 0) { x = x + 1; y = y - 1; }
Summary:
y == 0 ∧ x == 𝑥𝑜𝑙𝑑 + 𝑦𝑜𝑙𝑑

(b) Loop:

while (z > 0) { x = x + 1; z = z - 1; }
Summary:
z == 0 ∧ x == 𝑥𝑜𝑙𝑑 + 𝑧𝑜𝑙𝑑

Proc. ACM Program. Lang., Vol. 1, No. CONF, Article 1. Publication date: January 2018.

1:34

Adam Chen, Parisa Fathololumi, Eric Koskinen, and Jared Pincus

Proc. ACM Program. Lang., Vol. 1, No. CONF, Article 1. Publication date: January 2018.

Veracity: Declarative Multicore Programming with Commutativity

1:35

G UNABRIDGED INFERENCE BENCHMARKS

Program
array-disjoint
array1
array2
array3
calc

Time (s)
0.63
0.75
1.11
1.13
120.10

conditional
counter
dict
dot-product
even-odd
ht-add-put
ht-cond-mem-get
ht-cond-size-get
ht-simple
linear-bool

linear-cond

linear
loop-amt

0.18
0.20
3.82
0.24
1.18
2.24
1.54
0.83
30.64
3.62

2.65

0.25
120.06

Inferred Conditions
i != j && x != y || x == y
1 != r[0] && r[0] + 1 != y && r[0] <= 1 || r[0] + 1 == y && r[0] <= 1
0 > a[0] && 1 != x || 1 == x
d != e && a != b || a == b
1 == y && 0 != y && 1 > c && 1 != c || 0 == y && 1 > c && 1 != c || 1
== c
x > 0
0 != c
i != r && c + x != y || c + x == y
true
x % 2 == x + y && 0 != y || 0 == y
tbl[z] == u + 1 && u + 1 != z
tbl[x] == tbl[z] && x != z || x == z
ht_size(tbl) <= 0 && 0 != z || 0 == z
x + a != z && 3 == tbl[z] && y != z
0 <= y && 3 == x && 2 != x && 1 != x && x > 0 && 0 != x || 0 > y + 3 *
x && 2 == x && 1 != x && x > 0 && 0 != x
2 <= y && 2 != y && 1 != y || 0 == y && x + 2 == z && 2 > y && 2 != y
&& 1 != y || 2 == y && 1 != y || 1 == y
true
0 == i && amt == i_pre && ctr - 1 > i_pre && i_pre <= amt && 0 != i_pre
&& i_pre <= ctr && amt != amt_pre && ctr - 1 > amt_pre && amt_pre <=
amt && 0 != amt_pre && amt_pre <= ctr && ctr - 1 != 1 && 1 != ctr &&
1 != amt && 1 == ctr + amt || 0 == i && i_pre > amt && 0 != i_pre &&
i_pre <= ctr && amt != amt_pre && ctr - 1 > amt_pre && amt_pre <= amt
&& 0 != amt_pre && amt_pre <= ctr && ctr - 1 != 1 && 1 != ctr && 1 !=
amt && 1 == ctr + amt || i == i_pre && 0 == i_pre && i_pre <= ctr &&
amt != amt_pre && ctr - 1 > amt_pre && amt_pre <= amt && 0 != amt_pre
&& amt_pre <= ctr && ctr - 1 != 1 && 1 != ctr && 1 != amt && 1 == ctr
+ amt || 0 == i && i <= ctr && i_pre > amt && amt != i_pre && 1 !=
i_pre && i_pre > 0 && 0 != i_pre && i_pre > ctr && amt != amt_pre &&
ctr - 1 > amt_pre && amt_pre <= amt && 0 != amt_pre && amt_pre <= ctr
&& ctr - 1 != 1 && 1 != ctr && 1 != amt && 1 == ctr + amt || 0 == i &&
amt == amt_pre && ctr - 1 > amt_pre && amt_pre <= amt && 0 != amt_pre
&& amt_pre <= ctr && ctr - 1 != 1 && 1 != ctr && 1 != amt && 1 == ctr
+ amt || 0 == i && amt_pre > amt && 0 != amt_pre && amt_pre <= ctr &&
ctr - 1 != 1 && 1 != ctr && 1 != amt && 1 == ctr + amt || amt_pre ==
i && 0 == amt_pre && amt_pre <= ctr && ctr - 1 != 1 && 1 != ctr && 1
!= amt && 1 == ctr + amt || 0 == i && i <= ctr && amt != i_pre && ctr
- 1 > i_pre && i_pre <= amt && 0 != i_pre && i_pre <= ctr && amt_pre >
amt && amt != amt_pre && 1 != amt_pre && amt_pre > 0 && 0 != amt_pre
&& amt_pre > ctr && ctr - 1 != 1 && 1 != ctr && 1 != amt && 1 == ctr
+ amt || 0 == i && amt == i_pre && ctr - 1 > i_pre && i_pre <= amt &&
0 != i_pre && i_pre <= ctr && amt_pre > amt && amt != amt_pre && 1 !=
amt_pre && amt_pre > 0 && 0 != amt_pre && amt_pre > ctr && ctr - 1 !=
1 && 1 != ctr && 1 != amt && 1 == ctr + amt || 0 == i && i_pre > amt
&& 0 != i_pre && i_pre <= ctr && amt_pre > amt && amt != amt_pre && 1
!= amt_pre && amt_pre > 0 && 0 != amt_pre && amt_pre > ctr && ctr -
1 != 1 && 1 != ctr && 1 != amt && 1 == ctr + amt || i == i_pre && 0
== i_pre && i_pre <= ctr && amt_pre > amt && amt != amt_pre && 1 !=
amt_pre && amt_pre > 0 && 0 != amt_pre && amt_pre > ctr && ctr - 1 !=
1 && 1 != ctr && 1 != amt && 1 == ctr + amt || 0 == i && i <= ctr &&
i_pre > amt && amt != i_pre && 1 != i_pre && i_pre > 0 && 0 != i_pre
&& i_pre > ctr && amt_pre > amt && amt != amt_pre && 1 != amt_pre &&
amt_pre > 0 && 0 != amt_pre && amt_pre > ctr && ctr - 1 != 1 && 1 !=
ctr && 1 != amt && 1 == ctr + amt || 0 == i && ctr - 1 == 1 && 1 !=
ctr && 1 != amt && 1 == ctr + amt || amt == i && 1 == ctr && 1 != amt
&& 1 == ctr + amt

Proc. ACM Program. Lang., Vol. 1, No. CONF, Article 1. Publication date: January 2018.

1:36

Adam Chen, Parisa Fathololumi, Eric Koskinen, and Jared Pincus

(continued)
loop-disjoint
loop-inter
loop-simple
matrix
nested-counter

nested
nonlinear
simple

0.02
4.63
0.06
0.71
6.25

0.25
1.42
3.49

true
0 == x && 0 != y || 0 == y
true
0 == y
0 != c && c != t || c == t; c != x && c <= x && 1 != x && t == x || c > 1
&& 0 != c && c == x && c <= x && 1 != x && t == x || 1 == x && t == x
true; 0 == x
0 == y
a <= c && a != b && a != c || a == b && a != c

Proc. ACM Program. Lang., Vol. 1, No. CONF, Article 1. Publication date: January 2018.

Veracity: Declarative Multicore Programming with Commutativity

1:37

H SOURCE CODE OF BENCHMARK PROGRAMS
Benchmark: array-disjoint.vcy

int main ( int argc , string [] argv ) {

int [] a = new int [5];

int c = int_of_string ( argv [1]) ;

int i = int_of_string ( argv [2]) ;
int j = int_of_string ( argv [3]) ;

int x = int_of_string ( argv [4]) ;
int y = int_of_string ( argv [5]) ;

commute _ {

{ a[i] = x; busy_wait (c) ;}
{ a[j] = y; busy_wait (c) ;}

}

return 0;

}

Benchmark: array1.vcy

int main ( int argc , string [] argv ) {
int n = int_of_string ( argv [1]) ;
int x = int_of_string ( argv [2]) ;
int y = int_of_string ( argv [3]) ;
int [] r = new int [1];
r [0] = x;

commute _ {

{ r [0] = r [0] + 1; busy_wait (n); }
{ if (r [0] > 1) { r [0] = y; }

busy_wait (n);

}

}

return 0;

}

Benchmark: array2.vcy

int main ( int argc , string [] argv ) {
int n = int_of_string ( argv [1]) ;
int x = int_of_string ( argv [2]) ;
int [] a = new int [1];
a [0] = 69;

commute _ {

{ busy_wait (n); a [0] = a [0] + 1; }
{

if (a [0] > 0) {a [0] = a [0] * x; } busy_wait ( n) ; }

}

return 0;

}

Benchmark: array3.vcy

int main ( int argc , string [] argv ) {

int a = 0;
int b = 1;
int c = 42;
int d = 0;
int n = 10;
int size = int_of_string ( argv [1]) ;

Proc. ACM Program. Lang., Vol. 1, No. CONF, Article 1. Publication date: January 2018.

1:38

Adam Chen, Parisa Fathololumi, Eric Koskinen, and Jared Pincus

int [] arr = new int [10];

int e = c % n;

commute _ {

{ busy_wait ( size ); a = a + 1; arr [ e] = b ; a = a - 1; }
{ arr [d] = a; busy_wait ( size ); }

}

return 0;

}

Benchmark: calc.vcy

int main ( int argc , string [] argv ) {

int size = int_of_string ( argv [1]) ;
int x = 0;
int y = int_of_string ( argv [2]) ;
int z = 0;
int a = int_of_string ( argv [3]) ;
int c = int_of_string ( argv [4]) ;

commute _ {

{

}
{

}

busy_wait ( size );
x = /* calc1 */ (a);
c = c + (x*x);

if (c >0 && y <0)

{ c = c - 1; busy_wait ( size ); z = /* calc2 */ (y); }

else

{ busy_wait ( size ); z = /* calc3 */ ( y) ; }

}

return x;

}

Benchmark: conditional.vcy

int main ( int argc , string [] argv ) {
int n = int_of_string ( argv [1]) ;
int x = 42;
bool r = false ;
commute _ {

{ x = 1; busy_wait (n); }
{ int t = x; busy_wait (n); r = t > 0; }

}
return 0;

}

Benchmark: counter.vcy

int main ( int argc , string [] argv ) {
int x = int_of_string ( argv [1]) ;
int c = 42;

commute _ {

{ c = c + 1; busy_wait (x) ;}
{ if (c > 0) { c = c - 1; busy_wait (x); } else { }}

}

print ( string_of_int (c) ^ "\n");
return 0;

}

Benchmark: crowdfund.vcy

Proc. ACM Program. Lang., Vol. 1, No. CONF, Article 1. Publication date: January 2018.

Veracity: Declarative Multicore Programming with Commutativity

1:39

void busy ( int fuel ) {

while ( fuel > 0) {

fuel = fuel - 1;

}
return ;

}

void lock ( int l) { mutex_lock (l); return ; }
void unlock ( int l)

{ mutex_unlock (l); return ; }

void addOrInit ( hashtable [ int , int ] table , int key , int amount ) {

table [ key ] = lookupDefault ( table , key , 0) + amount ;
return ;

}

int lookupDefault ( hashtable [ int , int ] table , int key , int v_0 ) {

if ( ht_mem ( table , key ) ) { return table [ key ]; }
else { return v_0 ; }

}

int f_start_date = 5;
int f_end_date = 10;
int f_goal = 10000;
int f_amount = 15;
int addr_f_escrow = 101;
int addr_f_creator = 102;
int addr_f_receiver = 103;
bool f_CloseRemainderTo = false ;
int f_close_date = 9;

int addr_noone = -1;

int x = 0;

/* How much an actor has given */
hashtable [ int , int ] given = new hashtable [ int , int ];
hashtable [ int , int ] balances = new hashtable [ int , int ];

int donate ( int donator , int amount , int nowTS ) {

int r = 0;
/* Ensures the donation is within the beginning and ending dates

of the fund . */

if ( nowTS <= f_start_date
{

nowTS >= f_end_date ) { r = 0; } else

/* Verifies that this is a grouped transaction with the

second one being a payment to the escrow . */

lock ( donator );
int curBal = lookupDefault ( balances , donator , 0) ;
unlock ( donator );
if ( curBal < amount ) { r = 0; } else {

lock ( donator );
lock (0) ;
lock ( addr_f_escrow );
f_amount = f_amount + amount ;
addOrInit ( balances , addr_f_escrow , amount ) ;
addOrInit ( balances , donator , - amount );
unlock (0) ;
unlock ( addr_f_escrow );
addOrInit ( given , donator , amount );
addOrInit ( balances , donator , amount );
unlock ( donator );
busy (x);
r = 1;

}
}
return r;

}

int reclaim ( int donator , int amount , int nowTS ) {

if ( f_close_date > nowTS ) { return 0; }

Proc. ACM Program. Lang., Vol. 1, No. CONF, Article 1. Publication date: January 2018.

1:40

Adam Chen, Parisa Fathololumi, Eric Koskinen, and Jared Pincus

if ( f_goal < f_amount ) { return 0; }

int fee = 0;
int t = amount + fee ;
lock ( donator );
int g = given [ donator ];
unlock ( donator );
if (t > g) { return 0; }
if ( f_CloseRemainderTo ) { return 0; }
if (! ht_mem ( balances , addr_f_escrow )) { return 0; }
lock ( donator );
lock ( addr_f_escrow );
addOrInit ( balances , donator , amount );
addOrInit ( balances , addr_f_escrow , - amount );
unlock ( addr_f_escrow );
unlock ( donator );
busy (x);
return 1;

}

int main ( int argc , string [] argv ) {
x = int_of_string ( argv [1]) ;
int nowTS = 7;

mutex_init ( addr_f_escrow );

balances [1] = 1000;
balances [2] = 1000;
balances [3] = 1000;
balances [4] = 1000;

commute {

{ donate (1 , 10 , nowTS ); }
{ donate (2 , 10 , nowTS ); }
{ donate (3 , 10 , nowTS ); }
{ donate (4 , 10 , nowTS ); }

}

commute {

{ reclaim (1 , 10 , nowTS ); }
{ reclaim (2 , 10 , nowTS ); }
{ reclaim (3 , 10 , nowTS ); }
{ reclaim (4 , 10 , nowTS ); }

}
return 0;

}

Benchmark: dict.vcy

int main ( int argc , string [] argv ) {
int n = int_of_string ( argv [1]) ;
int t = random (0 , 100) ;
int c = 69;
int r = random (0 , 100) ;
int y = 42 + 69;
int x = 42;
int i = r + 1;
hashtable [ int , int ] stats = new hashtable [ int , int ];
stats [r] = 0;
stats [i] = 0;

commute _ {

{ busy_wait (n); t = c + x; ht_put ( stats ,r ,t); }
{ ht_put ( stats ,i ,y); y = y + x; busy_wait (n); }

Proc. ACM Program. Lang., Vol. 1, No. CONF, Article 1. Publication date: January 2018.

Veracity: Declarative Multicore Programming with Commutativity

1:41

}

return x;

}

Benchmark: dihedral.vcy

/* https :// www . whitman . edu / documents / Academics / Mathematics /

SeniorProject_CodyClifton . pdf */

int n = 1;

/* r and s are generators of dihedral group Dn

* All rigid motions are representable as r^k or r^k*s
*/

int rk ( int a , int k) => (a + k) % n;
int r( int a)

=> rk (a , 1) ;

int s( int a) => (n - a) % n;

int [] id () {

int [] a = new int [n ];
for ( int i = 0; i < n; i = i + 1;) {

a[i] = i;

}
return a;

}

/* Apply rigid motions

started at the middle of the array and wrapped around ,
instead of starting at the beginning .

* Rotation and symmetry operations can be
*
*
* That way , if two threads are operating on the array ,
*
*
*/

one can be offset from the other ,
making it less likely that they will fight over values .

void mrk ( int [] x , int k , bool start_in_middle ) {

int m = start_in_middle ? n / 2 : 0;
for ( int i = m; i < n; i = i + 1;) {

x[i] = rk (x[i], k);

}
for ( int i = 0; i < m; i = i + 1;) {

x[i] = rk (x[i], k);

}
return ;

}

void mr ( int [] x) {
mrk (x , 1) ;
return ;

}

void ms ( int [] x , bool start_in_middle ) {
int m = start_in_middle ? n / 2 : 0;
for ( int i = m; i < n; i = i + 1;) {

x[i] = s(x[i ]) ;

}
for ( int i = 0; i < m; i = i + 1;) {

x[i] = s(x[i ]) ;

}
return ;

}

/* Inputs are

*
*
*

n > 1
r1 >= 0
s1 in [0 , 1]

Proc. ACM Program. Lang., Vol. 1, No. CONF, Article 1. Publication date: January 2018.

1:42

Adam Chen, Parisa Fathololumi, Eric Koskinen, and Jared Pincus

r2 >= 0
s2 in [0 , 1]

*
*
*
* First rigid motion is r^ r1 * s^ s1
* Second rigid motion is r^ r2 * s^ s2
*/

int main ( int argc , string [] argv ) {
n = int_of_string ( argv [1]) ;

int [] a = id () ;

/* Rigid motion 1 */
int r1 = int_of_string ( argv [2]) % n;
bool s1 = int_of_string ( argv [3]) > 0;

/* Rigid motion 2 */
int r2 = int_of_string ( argv [4]) % n;
bool s2 = int_of_string ( argv [5]) > 0;

/* If s1 and not s2 then commute if

n is even and r2 = 0 or r2 = n /2
n is odd and r2 = 0

If not s1 and s2 then commute if

n is even and r1 = 0 or r1 = n /2
n is odd and r1 = 0
If s1 and s2 then commute if

r1 = r2

If not s1 and not s2 then always commute

*/
bool phi =

(! s1 && ! s2 )
( s1 && ! s2 && ( r2 == 0
(! s1 && s2 && ( r1 == 0
( s1 && s2 && r1 == r2 );

(n % 2 == 0 && r2 == n / 2) ))
(n % 2 == 0 && r1 == n / 2) ))

/* print ( phi ? " true \n" : " false \n ") ; */

commute ( phi ) {

{ mrk (a , r1 , true );

if ( s1 ) { ms (a , true ); }

}
{ mrk (a , r2 , false );

if ( s2 ) { ms (a , false ); }

}

}

for ( int i = 0; i < n; i = i + 1;) {

print ( string_of_int (a[i ]) ^ " ") ;

}
print ("\n");

return ( phi ? 1 : 0) ;

}

Benchmark: dot-product.vcy

/* Dot product of 2D vectors */

int dot_product ( int [] x , int [] y , int d ) {

commute _ {

{ d = d + x [0] * y [0]; busy_wait (d) ;}
{ busy_wait (d); d = d + x [1] * y [1]; }

}
return d;

}

int main ( int argc , string [] argv ) {

Proc. ACM Program. Lang., Vol. 1, No. CONF, Article 1. Publication date: January 2018.

Veracity: Declarative Multicore Programming with Commutativity

1:43

int d = int_of_string ( argv [1]) ;

int [] x = new int [] {

int_of_string ( argv [2]) ,
int_of_string ( argv [3])

};

int [] y = new int [] {

int_of_string ( argv [4]) ,
int_of_string ( argv [5])

};

return dot_product (x , y , d);

}

Benchmark: even-odd.vcy

int main ( int argc , string [] argv ) {
int n = int_of_string ( argv [1]) ;
int x = int_of_string ( argv [2]) ;
int y = int_of_string ( argv [3]) ;

commute _ {

{ busy_wait (n);

if (x % 2 == 0) {

x = x + y;

}

}
{ if (x % 2 == 1) {
x = x + y;

}
busy_wait (n);

}

}

return x;

}

Benchmark: filesystem.vcy

int [] data = new int [1024];

int main ( int argc , string [] argv ) {
int n = int_of_string ( argv [1]) ;

hashtable [ string , string ] ht = new hashtable [ string , string ];

for ( int i = 0; i < 1024; i = i + 1;) {

data [i] = i;

}

string fname1 = " test1 . txt ";
string fname2 = " test2 . txt ";

commute ( fname1 != fname2 ) {

{

out_channel fout1 = open_write ( fname1 );
for ( int i =0; i < n; i = i + 1;) { write ( fout1 ,

string_of_int ( data [i %1024]) ^ "\n" ); }

close ( fout1 );

}
{

out_channel fout2 = open_write ( fname2 );
for ( int i=n -1; i >= 0; i = i - 1;) { write ( fout2 ,

string_of_int ( data [i %1024]) ^ "\n" ); }

close ( fout2 );

}

Proc. ACM Program. Lang., Vol. 1, No. CONF, Article 1. Publication date: January 2018.

1:44

Adam Chen, Parisa Fathololumi, Eric Koskinen, and Jared Pincus

}

return 0;

}

Benchmark: ht-add-put.vcy

int main ( int argc , string [] argv ) {

hashtable [ int , int ] tbl = new hashtable_seq [ int , int ];
int n = int_of_string ( argv [1]) ;
int y = int_of_string ( argv [2]) ;
int z = int_of_string ( argv [3]) ;
int u = int_of_string ( argv [4]) ;

tbl [z] = u + 1;

commute _ {

busy_wait (n);

y = u + 1;

ht_put ( tbl , y , u);

y = ht_get ( tbl , z);

busy_wait (n);

{

}
{

}

}

return 0;

}

Benchmark: ht-cond-mem-get.vcy

int main ( int argc , string [] argv ) {

hashtable [ int , int ] tbl = new hashtable [ int , int ];
int n = int_of_string ( argv [1]) ;
int x = int_of_string ( argv [2]) ;
int y = int_of_string ( argv [3]) ;
int z = int_of_string ( argv [4]) ;

tbl [x] = 42;
tbl [z] = 42;

commute _ {

{

busy_wait (n);
if ( ht_mem ( tbl , x)) {

y = ht_get ( tbl , x);

}

}
{ if ( ht_mem ( tbl , z)) {

y = ht_get ( tbl , z);

}
busy_wait (n) ;}

}

return 0;

}

Benchmark: ht-cond-size-get.vcy

int main ( int argc , string [] argv ) {

hashtable [ int , int ] tbl = new hashtable_seq [ int , int ];
int n = int_of_string ( argv [1]) ;
int y = int_of_string ( argv [2]) ;
int z = int_of_string ( argv [3]) ;

commute _ {

{

busy_wait (n);

if ( ht_size ( tbl ) > 0) {
y = y + z;

Proc. ACM Program. Lang., Vol. 1, No. CONF, Article 1. Publication date: January 2018.

Veracity: Declarative Multicore Programming with Commutativity

1:45

}

y = ht_get ( tbl ,z);

busy_wait (n);

}
{

}

}

return 0;

}

Benchmark: ht-fizz-buzz.vcy

void busy ( int fuel ) {

while ( fuel > 0) { fuel = fuel - 1; }
return ;

}

int main ( int argc , string [] argv ) {
int n = int_of_string ( argv [1]) ;

hashtable [ int , int ] s = new hashtable [ int , int ];

commute {

{ for ( int i = n - 1; i > 0; i = i - 2;) {

s[i] = 1;

}
} {

for ( int i = 3; i < n; i = i + 3;) {

s[i] = 1;

}
} {

for ( int i = 5; i < n; i = i + 5;) {

s[i] = 1;

}

}

}

int total = 0;

commute {
{

for ( int i = 0; i < n / 2; i = i + 1;) {

total = total + (( s[i] == 1) ? 1 : 0) ;

}

} {

for ( int i = n / 2; i < n; i = i + 1;) {

total = total + (( s[i] == 1) ? 1 : 0) ;

}

}

}

return total ;

}

Benchmark: ht-simple.vcy

int main ( int argc , string [] argv ) {

hashtable [ int , int ] tbl = new hashtable [ int , int ];
int n = int_of_string ( argv [1]) ;
int x = int_of_string ( argv [2]) ;
int y = int_of_string ( argv [3]) ;
int z = int_of_string ( argv [4]) ;
int a = int_of_string ( argv [5]) ;
int v = int_of_string ( argv [6]) ;
int g = int_of_string ( argv [7]) ;

tbl [z] = 3;

commute _ {

{ busy_wait (n);

Proc. ACM Program. Lang., Vol. 1, No. CONF, Article 1. Publication date: January 2018.

1:46

Adam Chen, Parisa Fathololumi, Eric Koskinen, and Jared Pincus

v = ht_get ( tbl , z);
v = v + 3;
g = x + a;
ht_put ( tbl , g , v); }

{ v = ht_get ( tbl , z);

v = v * 2;
ht_put ( tbl , y , v);
busy_wait (n); }

}

return 0;

}

Benchmark: linear-bool.vcy

int main ( int argc , string [] argv ) {

int size = int_of_string ( argv [1]) ;
int x = int_of_string ( argv [2]) ;
int y = int_of_string ( argv [2]) ;
int b = 0;

commute _ {

{ busy_wait ( size ); y = y + 3* x; }
{ if (y < 0) { x = 2; } else { x = 3; } busy_wait ( size ); }

}

return x;

}

Benchmark: linear-cond.vcy

int main ( int argc , string [] argv ) {

int size = int_of_string ( argv [1]) ;

int x = int_of_string ( argv [2]) ;
int y = int_of_string ( argv [3]) ;
int z = int_of_string ( argv [4]) ;

commute _ {

{

busy_wait ( size );

if (y > 0) {
x = x + 2;

} else {
x = z;

}

}
{ x = x + 1; y = y + 1; busy_wait ( size ); }

}

print (

string_of_int (x) ^ " " ^
string_of_int (y) ^ " " ^
string_of_int (z) ^ "\n"

);

return 0;

}

Benchmark: linear.vcy

int main ( int argc , string [] argv ) {

int size = int_of_string ( argv [1]) ;
int x = random ( -100 ,100) ;

Proc. ACM Program. Lang., Vol. 1, No. CONF, Article 1. Publication date: January 2018.

Veracity: Declarative Multicore Programming with Commutativity

1:47

int y = random ( -100 ,100) ;
int z = random ( -100 ,100) ;
int w = random ( -100 ,100) ;
int u = random ( -100 ,100) ;

commute _ {

{ x = x + 2* w + u; busy_wait ( size ); }
{ busy_wait ( size ); x = x + y + 3* z; }

}

return 0;

}

Benchmark: loop-amt.vcy

int main ( int argc , string [] argv ) {
int i = int_of_string ( argv [1]) ;
int ctr = int_of_string ( argv [2]) ;
int amt = 2;
int n = i;

commute _ {

{

/* this only works if i is initially even . */
while (i >0) {

if (i %2 == 0) {

amt = amt * -1;

} else {

amt = amt * i * -1;

}
i = i - 1;

}
/* need a strong enough loop invariarnt to show that amt

>0 here */

ctr = ctr + amt ;

}
{

}

}

if ( ctr > 0) { ctr = ctr - 1; } busy_wait (n);

return 0;

}

Benchmark: loop-disjoint.vcy

int main ( int argc , string [] argv ) {
int x = int_of_string ( argv [1]) ;
int y = int_of_string ( argv [2]) ;

havoc x; assume (x >0) ;
havoc y; assume (y >0) ;

commute _ {

{

}
{

}

while (x >0) { x = x - 1; }

while (y >0) { y = y - 1; }

}
return 0;

}

Benchmark: loop-inter.vcy

Proc. ACM Program. Lang., Vol. 1, No. CONF, Article 1. Publication date: January 2018.

1:48

Adam Chen, Parisa Fathololumi, Eric Koskinen, and Jared Pincus

int main ( int argc , string [] argv ) {
int x = int_of_string ( argv [1]) ;
int y = x;
int s1 = 0;
int s2 = 0;
int acc = 0;

havoc x; assume (x >0) ;
havoc y; assume (y >0) ;

havoc s1 ;
havoc s2 ;
commute _ {

{

}
{

}

while (x >0) { x = x - 1; s1 = s1 + 1; }
acc = acc + s1 ;

while (y >0) { y = y - 1; s2 = s2 + x; }
acc = acc + s2 ;

}
return 0;

}

Benchmark: loop-simple.vcy

int main ( int argc , string [] argv ) {
int y = int_of_string ( argv [1]) ;
int z = int_of_string ( argv [2]) ;
int x = 0;

havoc x;
havoc y;
havoc z;

commute _ {

{

}
{

}

while (y >0) { x = x + 1; y = y - 1; }

while (z >0) { x = x + 1; z = z - 1; }

}
return 0;

}

Benchmark: matrix.vcy

int main ( int argc , string [] argv ) {

int size = int_of_string ( argv [1]) ;
int s = 0;
int x = int_of_string ( argv [2]) ;
int y = int_of_string ( argv [3]) ;
int z = int_of_string ( argv [4]) ;
int yy = 0;
int out = 0;
commute _ {

{

}
{

yy = y; busy_wait ( size ); s = yy ;
x = s*x; y = 3* yy ;
z = z + 2* yy ;
s = 0; yy = 0;

yy = y;
x = 5* x; y = 4* yy ;
z = 3* z - yy ;
busy_wait ( size ); out = z;

Proc. ACM Program. Lang., Vol. 1, No. CONF, Article 1. Publication date: January 2018.

Veracity: Declarative Multicore Programming with Commutativity

1:49

yy = 0;

}

}

return x;

}

Benchmark: nested-counter.vcy

int main ( int argc , string [] argv ) {
int n = int_of_string ( argv [1]) ;
int x = 500;
int y = random (0 , 100) ;
int c = 420;
int t = 500;

commute _

/* 2 nd Condition */ {

{ commute _ /* 1 st Condition */ {

{ c = c + 1; }
{ /* t= */ busy_wait (n); if (c >0) { c = c - t; } }

} }
{ t = x;

busy_wait (n);
if (c >t) { x = x - c ; } }

}

return x;

}

Benchmark: nested.vcy

int main ( int argc , string [] argv ) {
int w = int_of_string ( argv [1]) ;
int x = 0;
int y = random (0 , 100) ;

commute _ {

{ commute _ {

{ x = 0; busy_wait (w); }
{ x = x * 2; }

} }

{ y = x; busy_wait (w); }

}

return x;

}

Benchmark: nonlinear.vcy

int main ( int argc , string [] argv ) {
int n = int_of_string ( argv [1]) ;
int x = int_of_string ( argv [2]) ;
int z = int_of_string ( argv [3]) ;
int w = int_of_string ( argv [4]) ;
int y = int_of_string ( argv [5]) ;
int t = 0;
int u = 0;
int o = 0;
int r = random (0 , 100) ;

/* to parallelize the busy 's , need commutativity

to know the order can be swapped */

commute _ {

{ t = r; /* t = */ busy_wait (n) ;

z = z * t;
y = 2 * y;
x = x + y;

Proc. ACM Program. Lang., Vol. 1, No. CONF, Article 1. Publication date: January 2018.

1:50

Adam Chen, Parisa Fathololumi, Eric Koskinen, and Jared Pincus

}

{

z = 3 * z;
x = x + y;
u = x;

busy_wait (n); o = u /* busy_wait (n , u ) */ ; }

}

return 0;

}

Benchmark: simple.vcy

void compute1 ( bool cond ) {}
void compute2 ( bool cond ) {}

int main ( int argc , string [] argv ) {
int n = int_of_string ( argv [1]) ;
int t = int_of_string ( argv [2]) ;
int a = int_of_string ( argv [3]) ;
int b = int_of_string ( argv [4]) ;
int c = int_of_string ( argv [5]) ;
int u = int_of_string ( argv [6]) ;
int temp = 0;

commute _ {

{

}
{

}

t = /* compute1 (c > b); */

(( c > b) ? 2 : 1) ; busy_wait (n);

a = a - (( t < 0) ? -t : t);

u = /* compute2 (c > a); */

(( c > a) ? 2 : 1) ; busy_wait (n);

}
return 0;

}

Proc. ACM Program. Lang., Vol. 1, No. CONF, Article 1. Publication date: January 2018.

Veracity: Declarative Multicore Programming with Commutativity

1:51

I USING SNAPSHOTS (SEC. 5.4) TO AVOID LOCKS
Result of using snapshots to avoid the need for locks for benchmark: array3.vcy

int main ( int argc , string [] argv ) {

int a = 0;
int b = 1;
int c = 42;
int d = 0;
int n = 10;
int size = int_of_string ( argv [1]) ;
int [] arr = new int [10];

int e = c % n;

if (!( d == e) && !( a == b)

a == b) {

int a0 = a;
commute (!( d == e) && !( a == b)

a == b) {

{ busy_wait ( size ); a = a + 1; arr [ e] = b ; a = a - 1; }
{ arr [d] = a0 ; busy_wait ( size ); }

}

} else {

busy_wait ( size ); a = a + 1; arr [e] = b; a = a - 1;
arr [d] = a; busy_wait ( size );

}

return 0;

}

Result of using snapshots to avoid the need for locks for benchmark: simple.vcy

void compute1 ( bool cond ) {}
void compute2 ( bool cond ) {}

int main ( int argc , string [] argv ) {
int n = int_of_string ( argv [1]) ;
int t = int_of_string ( argv [2]) ;
int a = int_of_string ( argv [3]) ;
int b = int_of_string ( argv [4]) ;
int c = int_of_string ( argv [5]) ;
int u = int_of_string ( argv [6]) ;
if (c >a) {

int a0 = a;
commute (c >a) {

{

}
{

}

}
} else {

t = /* compute1 (c > b); */

(( c > b) ? 2 : 1) ; busy_wait (n);

a = a - (( t < 0) ? -t : t);

u = /* compute2 (c > a); */

(( c > a0 ) ? 2 : 1) ; busy_wait (n );

t = /* compute1 (c > b); */

(( c > b) ? 2 : 1) ; busy_wait (n);

a = a - (( t < 0) ? -t : t);
u = /* compute2 (c > a); */

(( c > a) ? 2 : 1) ; busy_wait (n);

}
return 0;

}

Proc. ACM Program. Lang., Vol. 1, No. CONF, Article 1. Publication date: January 2018.

