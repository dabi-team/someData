Trustworthy Federated Learning via Blockchain

Zhanpeng Yang, Student Member, Yuanming Shi, Senior Member, IEEE, Yong Zhou, Member, IEEE,
Zixin Wang, Student Member, IEEE, Kai Yang, Member, IEEE

1

2
2
0
2

g
u
A
3
1

]

G
L
.
s
c
[

1
v
8
1
4
4
0
.
9
0
2
2
:
v
i
X
r
a

Abstract—The safety-critical scenarios of artiﬁcial intelligence
(AI), such as autonomous driving, Internet of Things, smart
healthcare, etc., have raised critical requirements of trustworthy
AI to guarantee the privacy and security with reliable decisions.
As a nascent branch for trustworthy AI, federated learning (FL)
has been regarded as a promising privacy preserving framework
for training a global AI model over collaborative devices. How-
ever, security challenges still exist in the FL framework, e.g.,
Byzantine attacks from malicious devices, and model tampering
attacks from malicious server, which will degrade or destroy
the accuracy of trained global AI model. In this paper, we shall
propose a decentralized blockchain based FL (B-FL) architecture
by using a secure global aggregation algorithm to resist malicious
devices, and deploying practical Byzantine fault tolerance consen-
sus protocol with high effectiveness and low energy consumption
among multiple edge servers to prevent model tampering from
the malicious server. However, to implement B-FL system at the
network edge, multiple rounds of cross-validation in blockchain
consensus protocol will induce long training latency. We thus
formulate a network optimization problem that jointly considers
bandwidth and power allocation for the minimization of long-
term average training latency consisting of progressive learning
rounds. We further propose to transform the network opti-
mization problem as a Markov decision process and leverage
the deep reinforcement learning based algorithm to provide
high system performance with low computational complexity.
Simulation results demonstrate that B-FL can resist malicious
attacks from edge devices and servers, and the training latency of
B-FL can be signiﬁcantly reduced by deep reinforcement learning
based algorithm compared with baseline algorithms.

Index Terms—Trustworthy AI, federated learning, blockchain,

long-term latency minimization, resource allocation.

I. INTRODUCTION

Artiﬁcial Intelligence (AI) has yielded a bloom of world-
wide developments and profoundly changed human life, such
as autonomous driving [1], Internet of Things (IoT) [2],
smart healthcare [3], etc. However, recent research results
have found that AI may cause potential vulnerabilities by, for
example, leaking privacy data or making unreliable decisions
under adversarial attacks (e.g., misdiagnosed disease in smart
healthcare). Consequently, a new AI paradigm is emerging,
named trustworthy AI [4], which aims at avoiding unfavor-
able impacts from AI and has garnered increased interest in
both academia and industry. In particular, the trustworthy AI
intends to achieve stable and sustained high learning accuracy
under the considerations of robustness, privacy, accountability,
fairness, interpretability, and environmental well-being [4]. As

Zhanpeng Yang, Yuanming Shi, Yong Zhou, and Zixin Wang are with
the School of Information Science and Technology, ShanghaiTech Uni-
versity, Shanghai 201210, China (e-mail: yangzhp@shanghaitech.edu.cn;
shiym@shanghaitech.edu.cn; zhouyong@shanghaitech.edu.cn; wangzx2@sha
nghaitech.edu.cn).

Kai Yang is with JD Technology Group, Beijing 100176, China (e-mail:

yangkai188@jd.com).

a promising framework of trustworthy AI, federated learning
(FL) has recently been proposed to guarantee data privacy in
the AI model training process, which collaboratively trains
a global AI model by coordinating multiple devices [5], [6].
In the server-client architecture of FL, each device executes
local training and uploads its updated local model to a server
without transmitting the private raw data. The server then
aggregates the collected local models for global AI model
update, followed by disseminating the updated global model
to devices [2]. Therefore, FL provides a privacy protection so-
lution for privacy-sensitive intelligent applications, e.g., smart
healthcare, ﬁnancial industry, and IoT [7]–[11].

However, security challenges still exist in FL due to the
potential malicious devices and malicious server, which will
degrade the performance of AI model training or destroy the
FL training process [7]. Although FL guarantees the privacy
protection of edge devices, the global model can be attacked by
malicious edge devices (e.g., model poisoning or adversarial
attacks) [12]. Various secure model aggregation algorithms
have been proposed to address this challenge, e.g., geometric
median, trimmed mean, and multi-Krum [13]. In particular, the
server-client architecture is vulnerable to server’s malfunction,
which includes a potential single point of failure or tamper of
global model. The failure or tamper in the server will collapse
the entire FL training. To address this issue, blockchain was
adopted to establish a decentralized blockchain-based FL (B-
FL) network [14]–[16], which leverages multiple servers to
execute global model aggregation and conﬁrm the correctness
of global model using consensus protocols (e.g., Proof of Work
(PoW), Proof of Stake (PoS), Raft, and practical Byzantine
fault tolerance (PBFT)) [17], [18]. The B-FL can thus resist
failures or attacks of servers and devices by building trust-
worthy global model aggregation with secure model aggrega-
tion based on blockchain consensus protocol among multiple
servers [19]–[22].

Moreover,

the latency of FL training process, one of
key metrics in the edge AI, is also crucial to improve the
communication efﬁciency of FL due to the fading nature
of wireless channels [23]–[26]. To implement the FL over
wireless networks, the edge devices transmit their local models
and the edge server disseminates the shared global model over
wireless links. This typically consists of computation latency
and communication latency, for which theoretical analysis and
resource allocation for FL latency have recently been provided
[27]–[30]. Speciﬁcally, Li et al. [27] characterized the delay
distribution for FL over arbitrary fading channels via the
saddle point approximation method and large deviation theory.
Chen et al. [28] proposed a probabilistic device scheduling
policy to minimize the overall training time for wireless FL.
Ren et al. [29] formulated a training acceleration optimization

 
 
 
 
 
 
2

problem developed the close-form expressions for joint batch
size selection and communication resource allocation. Yang
et al. [30] considered a delay minimization problem to obtain
the optimal solution by a bisection search algorithm. However,
the latency in wireless B-FL becomes much more compli-
cated due to the additional multiple rounds of cross-validation
among edge servers in blockchain consensus protocol. The
computation latency includes the local training, global model
aggregation and block validation, which depends on the com-
putation capability of edge devices and servers, and the size
of dataset and models. The communication latency includes
the uplink and downlink transmissions for model updates
and blockchain consensus protocol, which mainly depend on
wireless communication techniques, bandwidth, and power
budgets. It is thus critical to characterize the latency of the
wireless B-FL system and optimize the network resources to
reduce the overall learning latency.

In this paper, we focus on designing a B-FL architecture
to support trustworthy and low latency AI service under the
consensus protocol of blockchain. Speciﬁcally, we utilize the
PBFT consensus protocol [31] to achieve high effectiveness
and low energy consumption compared with the existing
consensus protocols such as proof-of-work (PoW), and Raft
with some loss of security [18]. The training latency in B-
FL system is characterized by considering the computation
latency and communication latency, which consists of local
training, global model aggregation, consensus protocol, and
model dissemination. We then propose to minimize the long-
term average learning latency by allocating bandwidth and
power resources. This long-term resource allocation problem
cannot be equivalently transformed to multiple one-shot prob-
lems with respect to the channel of each round, due to the
correlation in the long-term average power constraint [32].
Besides, as the optimization variables are coupled in the ob-
jective function and constraints, the conventional optimization-
based algorithms have prohibitive computational complexity
and may be inapplicable in the time-sensitive B-FL system
[15], [33], [34]. To address the challenges, we propose to trans-
form the long-term resource allocation problem to a Markov
decision process (MDP), for which a deep reinforcement
learning (DRL) based algorithm is developed to achieve efﬁ-
cient and adaptive resource allocation with low computational
complexity from a long-term perspective. The developed DRL-
based algorithm will minimize the cumulative latency from a
long-term perspective, which balances the latency in current
round and that in the future rounds by dynamic resource
allocation. Moreover,
the DRL-based algorithm establishes
a direct mapping from the current network information to
resource allocation by deep neural network (DNN), which can
signiﬁcantly reduce the computational complexity.

A. Contributions

In this paper, we propose a PBFT-based wireless B-FL
architecture to realize trustworthy AI training process. To
improve the communication efﬁciency of wireless B-FL, we
further propose to minimize long-term average training latency
by allocating the bandwidth and power resources. The long-
term resource allocation problem is modeled as an MDP,

followed by developing a DRL-based algorithm to achieve
high system performance and reduce computational complex-
ity. The major contributions of this paper are summarized as
follows

1) We proposed a PBFT-based wireless B-FL architecture
by combining the permissioned blockchain and wireless
FL system at the network edge, which builds a trustwor-
thy AI model training environment to resist the failures
and attacks from malicious edge servers and malicious
edge devices. The wireless B-FL utilizes PBFT consen-
sus protocol of blockchain to validate the correctness of
global model update, which achieves high effectiveness
and low energy consumption. The detailed procedures
of PBFT based wireless B-FL system are presented,
followed by characterizing the training latency by con-
sidering the communication and computation processes.
2) By jointly considering the consensus and training pro-
cesses in the long-term process of B-FL, we formulate a
long-term average latency minimization problem by op-
timizing bandwidth allocation and power allocation. We
adopt long-term average power constraint for dynamic
power allocation to assign more power in worse channel
conditions and less power in better channel conditions.
The long-term resource allocation problem turns out
to be highly intractable due to the correlation in the
long-term average power constraint, and thus cannot be
equivalently transformed to multiple one-shot problems.
3) To solve the long-term resource allocation problem, we
propose to transform the resource allocation problem
into an MDP, followed by developing a DRL based algo-
rithm to design efﬁcient and adaptive resource allocation
scheme with low computational complexity from a long-
term perspective. We then utilize a twin delayed deep
deterministic policy gradient (TD3) algorithm in DRL
to tackle the continuous optimization variables.

The simulation results demonstrate the effectiveness of
wireless B-FL to resist malicious attacks from edge devices
and servers, which are simulated in MINST dataset for
handwriting recognition and heart activity dataset for affect
recognition. We further simulate the proposed DRL-based
resource allocation scheme with various system parameters,
and demonstrate the effectiveness for achieving signiﬁcant
reduction on training latency compared with other baseline
algorithms.

B. Related Works

The blockchain-based FL system has recently received
signiﬁcant interests in designing trustworthy AI by leveraging
the consensus protocol of blockchain and a recent survey paper
has presented the fundamental concepts and opportunities
in the integration of FL and blockchain, which is called
FLchain [20]. In particular, Qu et al. [35], and Ma et al.
[36] proposed a B-FL architecture based on PoW consensus
protocol, which uses the miners in the blockchain as the
centralized aggregator to replace the single dedicated server
in the traditional FL system. Besides, Li et al. [37] proposed

3

TABLE I
LIST OF NOTATIONS.

Notation
M
K
B

D
sDk
S
Bp
wk
wg
f
D(B)
HB
N0
γ
ηa, ηc
κ
ϑ

Description
number of edge servers
number of devices
set of edge servers

set of edge devices
batch size of devices Dk
set of local datasets
index of primary edge server
local model of device Uk
global FL model
number of Byzantine edge servers
digest of block (e.g. hash value)
block height in blockchain
power spectral density of AWGN
discount factor
learning rate of actor and critic networks
update proportion of target networks
update frequency of target networks

Notation
bmax
¯p

ht

Dk,Bm
bt
pt
RDk,Bm
fBm
σ
δ
ρ
(cid:36)
SB
SM
φ, θ1, θ2
φ(cid:48), θ(cid:48)
1, θ(cid:48)
2
R
E

Description
maximum system bandwidth
long-term average power constraint
channel gain between edge device Dk and edge server Bm at t-th round
bandwidth allocation at t-th round
transmit power of edge servers and devices at t-th round
achievable transmission rate from edge device Dk to edge server Bm
CPU frequency of edge server Bm
unit CPU cycle for executing secure model aggregation
CPU cycle for training one sample
CPU cycle for generating or verifying one signature
size of transactions
size of blocks
size of consensus messages
parameters of online actor and critic networks
parameters of target actor and critic networks
reply memory buffer
steps of exploration

a blockchain-based federated learning framework with an in-
novative committee consensus mechanism to avoid malicious
central servers and realize effective decentralized storage.
in [38] designed a proof-of-federation (PoF)
The Biscotti
consensus protocol, which coordinates FL training between
devices to generate blockchain by providing beneﬁcial model
updates or by facilitating the consensus process. Lu et al.
[39] proposed a B-FL with delegated proof of stakes (DPoS),
which is developed in digital twin wireless networks. Based
on the above architectures of B-FL with different consensus
protocols,
there are several works optimizing the network
efﬁciency, e.g., latency and energy consumption. Kim et al.
[40], Li et al. [41], and Pokhrel et al., [42] analyzed an end-
to-end latency model of B-FL based on PoW, characterized
the close-form of optimal block generation rate in different
scenarios, and formulated an edge association by jointly
considering digital twin association, training data batch size,
and bandwidth allocation, respectively. The paper [43] con-
sidered blockchain-enabled FL with PoW consensus protocol
and achieved a certain model accuracy while minimizing the
energy consumption and training latency with a reasonable
payment by DRL approach. Nguyen et al. [44] employed
a decentralized FL model aggregation algorithm in a peer-
to-peer-based blockchain network and formulated a system
latency minimization problem by optimizing communication
and computation resources. Furthermore, to improve privacy
protection Zhao et al. [45] jointly enforced differential privacy
on the extracted features and proposed a new normalization
technique to protect customers’ privacy and improve the
test accuracy in the B-FL system with Algorand consensus
protocol. Moreover, several works are already deployed in
the real blockchain platforms. For example, Zhang et al. [46]
tackled the challenge of data heterogeneity in failure detection
of industrial IoT and implement the B-FL system in Ethereum
to evaluate the feasibility, accuracy, and performance. Kang et
al. [47] presented efﬁcient reputation management for mobile
devices based on consortium blockchain and establish the
reputation blockchain system on the Corda.

However, all the above works simply adopted the traditional
PoW consensus protocol [36], [40], [41] or proposed some
immature consensus protocols (e.g., committee consensus
mechanism [37], PoF [38]). In this paper, we utilize the
PBFT consensus protocol to build a permissioned blockchain,
which achieve high effectiveness and low energy consumption
compared with famous and widely-used PoW [18].

C. Organization

The rest of the paper is organized as follows. Section
II introduces the wireless B-FL system architecture and the
communication model. Section III analyzes the training la-
tency of wireless B-FL and formulates the long-term average
latency minimization problem. The TD3-based DRL algorithm
is developed in Section IV. Section V presents simulation
results of the proposed algorithm by comparing with other
baseline algorithms. The notations in this paper are listed in
Table I.

II. SYSTEM MODEL

In this section, we propose a blockchain-based FL archi-
tecture and describe the proposed B-FL system over wireless
networks. The procedures of our proposed wireless B-FL are
then provided.

A. Architecture of Wireless Blockchain-Based FL

In this paper, we instead consider a wireless B-FL system
consisting of M edge servers and a set of K edge devices, as
shown in Fig. 1. Each edge server is equipped with a mobile
edge computing (MEC) sever to execute computing tasks, and
the edge servers are indexed by
.
}
Each edge server has enough computation and storage re-
sources to execute global model aggregation and consensus
protocol [14]. Meanwhile, K edge devices are indexed by
D1,
. The edge devices with local
=
}
· · ·
D
{
,
have enough computing
dataset
=
· · ·
resources and storage resources to execute the local training

, Dk,
1,

, DK
k,

· · ·
,
S

, Bm,

, BM

B1,

· · ·

· · ·

· · ·

{S

=

B

S

S

}

{

K

4

Fig. 1. Wireless blockchain-based federated learning system.

process of B-FL. The edge servers communicate with each
other over wireless channels [14], [48]. In summary, the B-FL
architecture consists of three layers, i.e., the edge device layer
to train local model, the edge server layer to execute secure
mode aggregation by blockchain technique, and overlaided
blockchain network layer, as depicted in Fig. 1.

The conventional server-client architecture for wireless FL
consists of single edge server and multiple edge devices,
which collaboratively train a shared global model without
sharing private data of edge devices [49], [50]. However,
the server-client architecture is vulnerable to edge server’s
malfunction, which includes a potential single point of failure
and malicious global model poison [51]. A failure in the edge
server may collapse the entire FL network. In this paper,
the proposed wireless B-FL combines the wireless FL with
permissioned blockchain to execute global model aggregation
based on blockchain consensus protocol among multiple edge
servers, which can resist failures or attacks of edge servers for
trustworthy global model aggregation [19]. The permissioned
blockchain provides an attractive solution for malicious server
attacks in conventional single server wireless FL due to its
features such as decentralization, immutability, and traceability
[22], [36]. In particular, the wireless B-FL system consists
of multiple edge servers and multiple edge devices [52],
which generates a blockchain in a form of blocks linked by
cryptography under the control of a consensus protocol to
conﬁrm that the data in the block is correct and immutable.
The block is a data structure consisting of all information of
local models and global model. Therefore, the decentralized
blockchain technology is adopted to provide a trustworthy
model aggregation platform powered by distributed consensus
protocol, which mitigates the single point of failure and
malicious poison of edge server and ensures transparency and
immutable.

Speciﬁcally, the B-FL system comprises one primary edge

server and several validator edge servers to produce a new
block and reach a consensus. In each training round, each
edge device trains the local model based on its own dataset
followed by uploading the local model to the primary edge
server. The primary edge server validates the identities of
local models and aggregates them to the global model by
executing the smart contract. The smart contract consists of
lines of code, which is immutable and trackable to execute
a function, e.g, secure global model aggregation. Then, the
local models and global model are packed into a new block,
which is broadcasted among validator edge servers to validate
the correctness of global model. For example, the validator
edge servers recalculate the global model and compare it with
the global model from the primary edge server. The validator
edge servers will reach a consensus on the correctness of
global using consensus protocols (e.g., PoW, PBFT). B-FL
will enter the next training round if the global model is correct.
However, if the primary edge server computes incorrectly or
tampers with the global model, the validator edge servers will
ignore the global model from the malicious primary edge
server and choose another primary edge server to restart the
training round. After the validator edge servers reaching a
consensus, the new block is stored in the blockchain to ensures
transparency and traceability. The validated global model is
then disseminated to edge devices. Therefore, the consensus
protocol is the critical factor to ensure the uniformity and
security of the decentralized blockchain system.

In this paper, we adopt the PBFT consensus protocol in
the proposed B-FL system to realize secure global model
aggregation among edge servers. Compared with famous and
widely-used PoW consensus protocol [35], [36], [40]–[43],
PBFT has weaker security performance. Generally, PoW is
a permissionless consensus protocol, which means arbitrary
edge sever can participate in the consensus and can tolerate
50% computing power attack. PBFT is a permissioned con-
sensus protocol, which means only the authorized edge sever
can participate in the consensus and can just resist 33% ma-
licious edge servers. However, PBFT achieves more effective
consensus and lower electrical energy consumption than PoW.
In particular, the consensus rate of PBFT is hundreds of times
higher than that of PoW and the PoW-based Bitcoin costs as
much electrical energy as the whole of Switzerland [18]. To
achieve high effectiveness and low energy consumption, we
choose the PBFT consensus protocol in our proposed B-FL
system.

B. Procedures of Wireless B-FL

To train the ML models in the wireless B-FL system based
on PBFT consensus protocol, eight steps are required in each
round to achieve trustworthy global model aggregation to resist
failures and attacks of malicious edge servers. The eight steps
are also shown in Fig 2.

1) Local Training: Each edge device uses the local dataset
to train the local model through stochastic gradient descent.
For each edge device Dk
, the local loss function on

∈ D

OverlaidedBlockchain NetworkEdge ServersEdge DevicesEdgeDevicesValidator Edge ServersPrimaryEdge Server5

encoded into digital signals (e.g. polar code) and transmitted
by rate adaptation scheme to ensure error-free transmission.
It is worth noting that the primary edge server is appointed
the primary edge server
before each training round (e.g.,
rotates among all the edge servers) and all edge devices and
edge servers know the primary edge server before each round.
In this step,
the communication latency is the transaction
uploading for all edge devices.

,

(cid:105)

∀

Dk

∈ D

wk, Dk
(cid:104)

3) Global Model Aggregation: After collecting the data
packets
, the primary edge server vali-
dates the transactions to conﬁrm the local model owner valid
and executes smart contract to aggregate them into a global
model. The primary edge server aims at minimizing the global
loss function through global aggregation, which is deﬁned as
(cid:88)

(3)

F (w) =

Fk(w;

k).

S

Dk∈D

Although FL guarantees the privacy protection of edge devices
to a certain extent,
the global model can be attacked by
malicious edge devices (e.g., data or model poisoning) [12].
The proposed B-FL is compatible with all secure global ag-
gregation algorithms (e.g., geometric median, trimmed mean,
and Multi-Krum). We use multi-KRUM algorithm to realize
Byzantine-resilient secure global model aggregation in this
paper, which outperforms other algorithm in convergence
speed [13], [38], [53], i.e.,

wt

wt
g = multi KRUM(
k,
{

Dk

∈ D}
g is the global model in round t, and wt
where wt
model from edge device Dk. The function multi KRUM(
·

k is the local
)

∀

(4)

),

Fig. 2. The procedures of wireless B-FL.

the batch of samples
denoted by

k randomly from local dataset can be

S

Fk(w;

k) =

S

1
sDk

(cid:88)

(xi,yi)∈Sk

f (w; xi, yi),

(1)

where sDk denotes the batch size of edge device Dk, xi
and yi are features and label respectively, and w denotes
the parameter of the ML model. In particular, f (w; (xi, yi))
is the sample-wise loss function, which depends on the ML
model. Then, the local model parameter can be updated by the
stochastic gradient descent algorithm, i.e.,
k = wt−1

Fk(wt−1

wt

k),

(2)

η

;

k

k −

∇

S

Fk(wt−1

where wt
k denotes the trained model parameters of edge device
Dk in round t,
) at point
wt−1
, and η is the learning rate. In this step, the computation
k
latency is the local model parameter calculation at the edge
devices.

k) is the gradient of Fk(
·

∇

S

k

;

,

(cid:105)

∀

Dk

∈ D

, where

wk, Dk

2) Upload Local Models: With allocated wireless resources
(e.g., bandwidth, transmit power), the edge devices upload
the local models to the primary edge server as transactions
wk, Dk
denotes a data packet
(cid:104)
(cid:104)
consisting of local model parameter wk and the data packet is
signed by edge device Dk. The digital signature is a cryptology
technique, which is used to validate the authenticity and
integrity of data packets and guarantee that the information
in data packet is not be tampered with others. The transaction
is a concept in blockchain system involving cryptocurrency,
contracts, records or other information to record all behavior in
blockchain. In this paper, we assume that the data packets are

(cid:105)

Step 2: Upload Local ModelsStep 1: Local TrainingStep 4: Pre-PrepareStep 5: PrepareStep 6: CommitStep 8: Download Global ModelEdge DevicesPBFT ConsensusBlockBlock headerGlobal modelLocal modelLocal modelPrimary Edge ServerStep 7: ReplyStep 3: Global Model AggregationValidator Edge ServersValidator Edge ServersValidator Edge Servers6

Algorithm 1 Pseudocode of Wireless B-FL
Input: Edge servers set
, edge devices set

, the dataset of

D

Output: The global model wg and a blockchain that contains

edge devices

.

S

B

training information.

1: Initialize the blockchain B and global model w0
g.
2: for each round t = 1, 2, . . . do
3:
4:
5:
6:

Rotate primary edge server among edge servers.
Allocate bandwidth and power resources.
for each device Dk

do
Dk executes local training on its local data
generated local model wt
k.
Dk uploads its local model to primary edge server.

k to

∈ D

S

7:
8:
9:

10:

11:

12:

17:

end for
Primary edge server validates the local models

Uk∈U and aggregates them to a global model

wt
k}
{
wt
wt
g = multi KRUM(
k}
{
Primary edge server packs local models and global

Uk∈U ).

model to a new block and broadcasts the block to the
validator edge servers.

Primary edge server and validator edge servers reach
consensus by PBFT protocol consensus (including
pre-prepare, prepare, commit, reply steps) to
guarantee the correctness of global model and add
veriﬁed block to the blockchain.
Primary edge server broadcasts global model wt
edge devices.

g to

13: end for
14:
15: function multi KRUM(wt
16:

kDk∈D)

∀

Dk

∈ D

wj
f

, where

Dj (cid:54)=Dk (cid:107)

Compute the Euclidean distances
s(Dk) = (cid:80)
2,
wk
(cid:107)
−
the sum runs over the K
2 closest vectors and
−
−
f is a speciﬁed parameter with respect to Byzantine
tolerant.
Select the K
local models as ˆ
D ⊂ D
(cid:80)
Compute wg = 1
|S|
return wg.

f lowest distance and corresponding

Dk∈ ˆD wk by averaging.

−

.

18:
19:
20: end function

is the multi-KRUM algorithm, which is deﬁned in Algorithm
1. After that, the local models and global model are packed
into a new block, which is a data packer denoted by B =
, where Bp is the notation of
(cid:104){(cid:104)
primary edge server. In this step, the computation latency is the
transaction validation and model aggregation in the primary
edge server.

wg, Bp
(cid:104)

w1, D1

Dk∈D,

(cid:105)}

(cid:105)(cid:105)

i.e.,

PRE-PREPARE, HB, D(B), B, Bp

4) Pre-Prepare: After producing the new block, the pri-
mary edge server broadcasts the new block with a pre-
prepare message,
.
(cid:105)
HB is the block height (i.e., the number of valid blocks in
the blockchain), D(B) is a digest of B (e.g., hash value) and
the pre-prepare message is signed by primary edge server Bp.
The validator edge servers, which are all edge servers except
for the primary edge server, receive the new block and verify
the digital signature of transactions and new block to conﬁrm

(cid:104)

the valid digital signature of new block. Furthermore, the
global model is recalculated to conﬁrm that the primary edge
server computes correctly. After veriﬁcation, each validator
edge server stores the new block to local. In this step, the
computation latency is the digital signature validation of new
block and recalculation of global model. The communication
latency is the block transmission from the primary edge server.
5) Prepare: After veriﬁcation, the validator edge servers
,
(cid:105)
= Bp to others. The primary edge server
∀
and all validator edge servers receive prepare messages and
conﬁrm that a majority of validator edge servers have veriﬁed
the new block and agree with the new block. In this step,
the computation latency is the digital signature validation of
prepare messages in all edge servers. The communication
latency is the message transmission from the validator edge
servers.

PREPARE, HB, D(B), Bm
(cid:104)

transmit the prepare messages

, Bm

∈ B

Bm

6) Commit: After the agreement of new block, the val-
the commit messages
idator edge servers will broadcast
COMMIT, HB, D(B), Bm
to others. The val-
(cid:104)
∀
idator edge servers receive enough commit messages and
conﬁrm that most edge servers reach a consensus. In this
step, the computation latency is the digital signature validation
of commit messages in all edge servers. The communication
latency is the message transmission from all edge servers.

∈ B

Bm

,
(cid:105)

(cid:104)

REPLY, HB, D(B), Bm

7) Reply: After reaching the consensus,

the new block
becomes valid and all validator edge servers return the reply
= Bp to
message
the primary edge server, which indicates that they have stored
the new block in the blockchain ready for the next consensus
process. In this step, the computation latency is the digital sig-
nature validation of reply messages in the primary edge server.
The communication latency is the message transmission from
the validator edge servers.

, Bm

∈ B

Bm

∀

(cid:105)

,

8) Download Global Model: The primary edge server
sends back the validated global model to the edge devices,
which promotes the next round of training. In this step, the
communication latency is the global model downloading at the
edge devices.

The steps of pre-prepare, prepare, commit, and reply are
necessary in decentralized blockchain system to reach a con-
sensus when each edge server does not know who is malicious.
The consensus protocol ensures the correctness of global
model aggregation by identity authentication and veriﬁcation
of calculation results at validator edge servers. The pseudocode
of wireless B-FL is presented in Algorithm 1.

C. Communication Model

The

frequency

orthogonal

division multiple

access
(OFDMA) technique is adopted in wireless B-FL system. All
devices and edge servers are assumed to have a single antenna
for simpliﬁcation [14], [39], [44], which can be generalized
to the multi-antenna case by additionally allocating the power
for each antenna of each server. We assume that the channel
state information(CSI) is available for resource allocation and
static in one time-slot but vary from different time-slots. To
model the channel gain in each time-slot, the channel gain

(cid:54)
(cid:54)
gDk,Bm[s]
|

between edge device Dk and edge server Bm of time-slot
2, where
s is deﬁned by hDk,Bm[s] = ζDk,Bm|
ζDk,Bm and gDk,Bm [s] are large-scale path loss attenuation
and small-scale block fading between edge device Dk and
edge server Bm, respectively. In particular, the large-scale
path loss attenuation is deﬁned as ζDk,Bm = d−α
, where
dDk,Bm is the distance between edge device Dk and edge
server Bm and α is the path loss exponent. Based on the
Jakes’ fading model [54], small-scale block fading gDk,Bm [s]
is modeled as ﬁrst-order complex Gauss-Markov process, i.e.,

Dk,Bm

gDk,Bm[s] = (cid:37)gDk,Bm [s

1] +

(cid:112)
1

−

−

(cid:37)2(cid:15)Dk,Bm [s],

(5)

where (cid:37) = J0(2πfdT0) denotes the correlation coefﬁcient in
two consecutive time slots. (cid:15)Dk,Bm [s] follows an independent
and identically distributed (i.i.d.) circularly symmetric com-
plex Gaussian distribution with unit variance. Here, J0(
) is
·
the zeroth-order Bessel function of the ﬁrst kind, fd represents
the maximum Doppler frequency, and T0 is the duration of one
time slot.

In fact, the durations of time-slots are very short (e.g., the
frame time-slot of LTE protocol is 10 ms), but the duration of
one B-FL training round is often in the time-scale of second
because of the high computation and communication com-
plexity in model training and consensus protocol. Therefore,
each B-FL training round contains multiple time-slots [27],
[29]. Because the purpose of this paper is minimization total
latency from the long-term perspective with enormous rounds,
the channel dynamics of multiple time-slots in a single round is
negligible. Thus, we employ the average channel gain instead
of the instantaneous ones as the channel gain in one round to
simplify the problem, i.e., ht
s=1 hDk,Bm [tS +s],
which is channel gain between edge device Dk and edge server
Bm in training round t and S is the total time-slot in one round
[29].

Dk,Bm = 1

(cid:80)S

S

Thus, the achievable transmission rate from edge device Dk

to edge server Bm is denoted by

7

III. SYSTEM OPTIMIZATION

In this section, we will analyze the latency of proposed
wireless B-FL system, by considering both computation and
communication latency. To achieve low latency and high relia-
bility of wireless B-FL, we formulate the latency optimization
problem by jointly optimizing bandwidth allocation and power
allocation.

A. Latency Analysis for Wireless B-FL

In the following, we give detailed steps and characterize the
latency of each step in the t-th round. We adopt synchronized
transmission scheme in this paper to ensure all edge servers
and devices in the same step. Compared with conventional
wireless FL system without blockchain, the wireless B-FL
system with blockchain needs to validate the digital signature
of local model and reaches a consensus of global model
aggregation, which can be treated as the extra computation
overhead. We assume that generating or verifying one digital
signature requires ρ CPU cycles. The transmission processes
of data packets in PBFT consensus protocol are shown in Fig
3, where the request step contains the upload local models
step and global model aggregation step. The latency of eight
steps in Section II-B is presented as follows.

1) Local Training: The number of CPU cycles for devices
to execute backpropagation algorithm with one sample is
denoted by δ. The local model weight calculation latency can
be expressed as

T cmp
train = max
Dk∈D

(cid:26) sDk δ
fDk

(cid:27)

,

(8)

where sDk is the batch size of edge device Dk and fDk is the
CPU frequency of edge device Dk.

2) Upload Local Models: After the local training, each
updated local model
is packed into a transaction, which
consists of digital signature and local model. The computation
latency of digital signature is

Rt

Dk,Bm = bt

Dk log2

(cid:32)

1 +

ht

pt
Dk
N0

Dk,Bm
bt
Dk

(cid:33)

,

(6)

T cmp

up = max
Dk∈D

ρ
fDk

.

(9)

Dk

Dk,Bm

where bt
is transmission bandwidth assigned to edge device
Dk at communication round t, and ht
is the channel gain
from edge device Dk to edge server Bm. Here, pt
and N0
represent the transmit power in edge device Dk and the power
spectral density of additive white Gaussian noise (AWGN),
respectively. In this paper, the maximum system bandwidth
is denoted by bmax, i.e., (cid:80)
bmax, which means
the edge servers and edge devices share the same bandwidth
of broadband communication [14]. Therefore, the latency of
transmitting data packet, whose size is (cid:36), from device Dk to
edge server Bm can be written as

d∈D∪B bt

d ≤

Dk

TDk,Bm =

(cid:36)

Rt

Dk,Bm

.

(7)

In this paper, we assume that the data packets are encoded
into digital signals (e.g. polar code) and transmitted by rate
adaptation scheme to ensure error-free transmission.

In this step, we deﬁne the average size of one transaction is
denoted by (cid:36), which is related to the AI model size. The
transmission latency from edge device to primary edge server
can be expressed by

T com

up = max
Dk∈D

(cid:40)

(cid:41)

,

(cid:36)

Rt

Dk,Bp

(10)

where Bp
denotes the primary edge server, and RDk,Bp
is the transmission rate from edge device Dk to the primary
edge server Bp.

∈ B

3) Global Model Aggregation: The transactions are sub-
mitted to the primary edge server. The primary edge server
receives these transactions consisting of local models and
veriﬁes its digital signature to conﬁrm the identity. Then, the
primary edge server will execute smart contract to aggregate
local models and generate global model. The smart contract
consists of lines of code, which is immutable and trackable
to execute a function. Therefore, the secure global model

8

computation latency in this step is

T cmp

prep = max

Bm∈B/Bp

(cid:26) ∆prep,Bm
fBm

(cid:27)

.

(13)

5) Prepare: After accepting the new block generated by
primary edge server, each validator edge server will broadcast
a prepare message to all the other edge servers consisting pri-
mary edge server and validator edge servers. Each edge server
will validate the prepare message to ensure the consistent with
the pre-prepare message.

In this step, the communication latency consisting of broad-
casting the prepare message to all other validator edge servers,
which is calculated by

T com

pre =

max
Bm∈B/Bp,Bm(cid:48) ∈B,Bm(cid:54)=Bm(cid:48)

(cid:40)

(cid:41)

,

SM

Rt

Bm,Bm(cid:48)

(14)

where SM denotes the size of prepare message, which is
smaller than SB because it does not contain the new block.

For the computation,

the primary edge server needs to
validate 2f digital signatures from the other validator edge
servers, which can be expressed by ∆pre,Bp = 2f ρ. Each of
validator edge servers needs to generate one digital signature
for the prepare message. Then, 2f digital signatures are
needed to be validated. Thus, the amount of computation at
the validator edge servers Bm(Bm
= Bp) can be given by
∆pre,Bm = ρ + 2f ρ. The computation latency in this step is

T cmp

pre = max
Bm∈B

∆pre,Bm
fBm

.

(15)

6) Commit: After receiving 2f valid prepare messages
from the other validator edge servers, each edge server will
broadcast a commit message to all the other edge servers. The
communication latency can be characterized by

T com

cmit =

max
Bm∈B/Bp,Bm(cid:48) ∈B,Bm(cid:54)=,Bm(cid:48)

(cid:41)

(cid:40)

SM

Rt

Bm,Bm(cid:48)

.

(16)

In this step, each edge server needs to generate one digital
signature to construct the commit messages. After receiving
the commit messages, each edge server needs to verify 2f
digital signatures. Thus, the amount of computation at each
edge server is ∆cmit,Bm = ρ + 2f ρ. The computation latency
in this step is

T cmp
cmit = max
Bm∈B

(cid:26) ∆cmit,Bm
fBm

(cid:27)

.

(17)

7) Reply: After collecting 2f commit messages, the new
block is ensured to a valid block, and it will be appended to
the blockchain. A reply message will be transmitted to primary
edge server. In this step, the communication latency is

T com

rep = max

Bm∈B/Bp

(cid:40)

(cid:41)

.

SM

Rt

Bm,Bp

(18)

For the computation, each validator edge server needs to
generate one digital signatures for the primary edge server,
which can be given by ∆rep,Bm = ρ. For the primary
it needs to verify 2f signatures,
the amount
edge server,

Fig. 3. PBFT consensus protocol.

aggregation via multi-KRUM algorithm can be programmed
as a smart contract. After generating the global model, all the
local model transactions and the global model transaction are
packed into a new block.

The maximum number of transactions that can be packed in
one block is K + 1, where K is the number of edge devices,
i.e., local model number, and one global model transaction.
Thus, the block size can be presented as SB = (K + 1)(cid:36)
without considering the negligible size of block header. In
this step, the primary edge server needs to verify the digital
signatures for K transactions. Then, the primary edge server
executes the smart contract of secure global model aggrega-
tion, whose total CPU cycles can be deﬁned as σ. Therefore,
the total amount of computation at the primary edge server is
∆req,Bp = Kρ+σ. Then, the computation delay at the primary
edge server is

T cmp

agg =

∆req,Bp
fBp

.

(11)

4) Pre-Prepare: After generating the new block, the pri-
mary edge server will broadcast a pre-prepare message to
all the validator edge servers for validation. The pre-prepare
message consists of the digital signature of the primary edge
server and the information of the new block. The validator
edge servers, which are all edge servers except the primary
edge server, need to validate that the new block are generated
by the given primary edge server and validate the identities of
all transactions to ensure the new block is valid. Besides, the
validator edge servers are mainly responsible for correctness
validation of global model, which can use an intuitive method
by repeatedly calculating the global model through identiﬁed
local models, and comparing the computation results with the
global model of primary edge server.

If the pre-prepare message is validated by 2f validator edge
servers, it enters the next step. In particular, f is the number of
hypothetical malicious edge servers, which satisfy 3f +1 = M
[31]. PBFT is designed to ensure the correctness of 2/3 edge
servers, which is more efﬁcient than validating all edge server
but less secure. It is a trade-off between efﬁciency and security.

In this step, the latency can be presented as

T com

prep = max

Bm∈B/Bp

(cid:40)

(cid:41)

.

SB

Rt

Bp,Bm

(12)

The amount of computation at the validator edge servers is
= Bp. The
∆prep,Bm = ρ + (K + 1)ρ + σ, where there is Bm

Edge DevicesPrimaryEdge ServerValidatorEdge ServerValidatorEdge ServerRequestPre-PreparePrepareCommitReply(cid:54)
(cid:54)
of computation of which is given by ∆rep,Bp = 2f ρ. The
computation latency in this step is

computational complexity, the sub-optimal performance may
occur in the long-term resource allocation problem [55].

9

T cmp

rep = max
Bm∈B

(cid:26) ∆rep,Bm
fBm

(cid:27)

.

(19)

8) Download Global Model: After local mode validation,
global model aggregation, and the new block addition, the
global model is transmitted to the edge devices to enter the
next round. In this step, the communication latency is

T com

down = max
Dk∈D

(cid:40)

(cid:41)

.

(cid:36)

Rt

Bp,Dk

(20)

From the above analysis, the total latency in the t-th round
consists of communication latency and computation latency,
i.e.,

T (bt, pt) = T com + T cmp.

(21)

The communication time consumption can be calculated as

T com = T com

up + T com

prep + T com

pre + T com

cmit + T com

rep + T com
down,

(22)

and the computation time consumption is
T cmp = T cmp

up +T cmp

agg +T cmp

prep +T cmp

train +T cmp

pre +T cmp

cmit +T cmp

rep . (23)

B. Problem Formulation and Analysis

In this paper, we aim to minimize the long-term average
latency of wireless B-FL by optimizing bandwidth allocation,
and transmit power allocation from a long-term perspective.
We assume that the wireless B-FL training process is ﬁnished
in τ -th training round. The long-term average latency mini-
mization problem is given by

minimize
{bt,pt}t∈T

subject to

τ
(cid:88)

T (bt, pt)

1
τ

1
τ

t=1
(cid:88)

d∈D∪B
τ
(cid:88)

bt
d ≤

(cid:88)

t=1

d∈B∪D

bmax, bt

d ≥

0,

t
∀

∈ T

, (24a)

pt
d ≤

¯p, pt

d ≥

0,

(24b)

}

T

=

1, 2, . . . , τ
{

where
denotes the training round. Here,
(24a) restricts the maximum system bandwidth and (24b)
denotes the long-term average power constraint. The long-
term average power constraint can provide dynamic power
allocation scheme to assign more transmit power in worse
channel conditions and less power in better channel conditions.
Due to the correlation in the long-term average power con-
straint, problem (24) is a long-term resource allocation prob-
lem, which can not be equivalently transformed to multiple
one-shot problems with respect to the channel of each round.
Moreover, the optimization variables of the high-dimensional
optimization problem are coupled in the objective function and
constraints. As a result, the conventional optimization based
algorithms suffer from high computational complexity, which
is unacceptable in the time-sensitive B-FL system [33]. Mean-
while, although the supervised learning, and unsupervised
learning based resource allocation schemes can solve the one-
shot optimization problem to provide signiﬁcant reduction on

In this paper, we proposed a DRL-based algorithm to min-
imize the long-term cumulative latency by modeling problem
(24) as analyze MDP. In particular, the DRL can optimize
problem (24) from a long-term perspective by balancing the
latency in current round and that in the future rounds while
achieving signiﬁcant reduction on computational complexity
by establishing a direct mapping from the current network
information to resource allocation decisions as DNN-based
policy. Besides, the network information (i.e., channel con-
ditions, current latency) for the decision-making and policy
update of DRL agents is obtained by interacting with the
dynamic B-FL system, which enables the learned policy to
keep track of the environment dynamic. To this end, we
shall formulate the MDP for problem (24), and then we
shall develop a TD3 based algorithm to make decisions in
continuous action spaces, which will be described in detail in
the next section.

IV. TD3 BASED RESOURCE ALLOCATION ALGORITHM

In this section, we ﬁrst formulate the optimization problem
as an MDP, and then propose a TD3 based learning algorithm
to solve the MDP problem efﬁciently.

A. MDP Formulation

To obtain optimal decisions under stochastic environment,
we reformulated the optimization problem (24) into an MDP,
which can be solved by a DRL based scheme. The key
components of the MDP include the observed state, action,
and reward, which are summarized as follows:

• State: At each training round, the edge server collects
channel state information (CSI) between edge servers and
device devices and cumulative latency, which is necessary
for resource allocation. Therefore, The state at time step
t is determined by the cumulative latency and CSI in the
wireless communication system, which is denoted by st.
Particularly, the state contains the channel gain between
edge devices and primary edge server, i.e.,

∈
, which has K entries. Besides, the state contains the
D
channel gain between different validator edge servers, i.e.,
ht
,
1)
d1,d2 }
{
entries. Therefore, the state is presented as

= d2, which has M (M

d1, d2

∈ B

, d1

ht
,
d,Bp }

−

∀

∀

d

{

st =

(cid:110)(cid:80)t−1

i=0 T (bi, pi),

ht
,
d,Bp }
{
d1, d2

d

∀
, d1

,

∈ D
= d2

(cid:111)

.

(25)

ht
,
d1,d2}
{

∈ B

∀
Thus, the dimension of the state space is K + M (M
−
1) + 1. The proposed DRL-based resource allocation
algorithm can be directly applied when other correlated
wireless channel models are considered. However, the un-
correlated channel (e.g., Rayleigh channel) will decrease
the correlation of system state in two consecutive time,
which may affect the estimation accuracy of the expected
future reward and decrease the performance of resource
allocation decisions [54].

(cid:54)
(cid:54)
10

• Action: From the observed environment states, the agent
chooses optimal action, i.e., making decisions based on
the policy π. The action is constructed by the bandwidth
RM +K and transmit power allocation
allocation bt
RM +K of all edge servers and edge devices at
pt
training round t, i.e.,

∈

∈

at = (cid:8)bt, pt(cid:9) .

(26)

Therefore, the dimension of the action space is 2(M +K).
• Reward: After the action at has been taken by the
environment, the DRL agent will obtain a reward from
the environment and update the policy π to maximize
the reward in the future. To minimize the latency in
wireless B-FL training process, the reward at the t-th
step is deﬁned by negative latency, i.e., maximum reward
means minimum latency, which is presented as

(cid:26)

rt =

−
rp

T (bt, pt) when (24a), (24b) are satisﬁed,

otherwise,

(27)
When the constraints (24a) and (24b) are satisﬁed, the
immediate reward is the negative of the latency in this
round. When the constraints (24a) and (24b) are not
satisﬁed, the immediate reward is set to rp. rp is an
extremely small value, which is regarded as a penalty.
Therefore, the actions selected by the agent will satisfy
the constraints (24a) and (24b).

• MDP Optimization Problem: The action-value function is

deﬁned as

Qπφ(st, at) = Eπφ

(cid:34) ∞
(cid:88)

i=0

γirt+i

st, at
|

(cid:35)

,

(28)

∈

where (cid:80)∞
i=0 γirt+i denotes the total discount reward
from the i-th training round and γ
[0, 1] represents
the discount factor. Therefore, the action-value function
expresses a “myopic” evaluation if γ
0 or “fast-
→
sighted” evaluation if γ
1. The action-value function
can evaluate the action selected by a policy πφ with
parameters φ. Moreover, the action-value function can
be rewritten as the Bellman equation, i.e.,
(cid:104)

rt + γQπφ(s(t+1), a(t+1))

.
(29)
Therefore, optimal policy can be obtained by maximizing
the action-value function to solve the MDP problem, i.e.,

Qπφ(st, at) = Eπφ

st, at(cid:105)
|

→

π∗ = arg min
πφ

Qπφ(st, at).

(30)

The resource allocation problem (24)

is reformulated
as an MDP without having state transition probabilities
P (st+1
st, at) in prior due to the complexity and dynamics
|
of wireless B-FL systems. Therefore, we resort to adopting
model-free reinforcement learning based on value function ap-
proximation, which does not require the transition probability
distribution to make decisions.

B. TD3 algorithm

From the deﬁnition of the MDP problem, the state space
and action space are continuous, which indicates that inﬁnite

Algorithm 2 TD3-based bandwidth and transmit power allo-
cation algorithm
Output: Actor network πφ and critic networks

Qθ1, Qθ2}
and actor network πφ

{

1: Initialize critic networks

Qθ1, Qθ2}
{
with random parameters θ1, θ2, πφ.
θ1, θ(cid:48)

2: Initialize target networks θ(cid:48)
3: Initialize replay buffer
4: Initialize noise range c, noise variance σ2

1 ←

2 ←

R

.

θ2, φ(cid:48)

φ.

←
1, σ2

2, update

proportion δ, discount factor γ.

5: Explore E time steps with random policy, obtain the
reward rt and the next state st+1, and storage the transition
(st, at, rt, st+1) of the E steps into

.
R

6: while time step
7:

≤

(0, σ2

P do
Select action with exploration noise
at = πφ(st) + n1, n1
1).
∼ N
Obtain bandwidth allocation bt and transmit power
allocation pt from action at, and assign them to edge
servers and device devices.
Edge devices and edge servers begin B-FL training
process and get the latency in one round.
Calculate reward rt, update state st+1, and storage the
transition (st, at, rt, st+1) into
Sample a mini-batch of transitions from
Obtain target action a(cid:48)t+1 = πφ(cid:48)(st+1) + n2, n2
c, c).
∼
y = rt + γ mini=1,2 Qθ(cid:48)
Update critics θi
arg minθi
Update φ using the deterministic policy gradient
algorithm.
if step mod ϑ = 0 then

i(st+1, a(cid:48)t+1).
−

Qθi(st, at))2(cid:3).

E (cid:2)(y

(0, σ2

clip(

.
R

.
R

2),

←

N

−

8:

9:

10:

11:
12:

13:

14:

15:

16:
17:
18:

Update the target networks:
θ(cid:48)
1 = κθ1 + (1
θ(cid:48)
2 = κθ2 + (1
φ(cid:48) = κφ + (1

κ)θ(cid:48)
1,
κ)θ(cid:48)
2,
κ)φ(cid:48).

−
−
−

19:
20:
end if
21:
t
22:
23: end while

←

t + 1

dimension of action can not be solved by Q-learning or
deep Q network. Recently, deep deterministic policy gradient
(DDPG) algorithm has achieved considerable performance
for solving MDP problem with continuous action variables.
Moreover, the enhanced version of DDPG, i.e., twin delayed
deep deterministic policy gradient (TD3) [56], can reduce
the overestimation bias of Q value and avoid sub-optimal
policy updates, which improves the policy learning speed
and performance in continuous control domains. The TD3 is
widely used to solve complex resource allocation problem in
real-time. We thus propose a TD3 based resource allocation
algorithm to solve the MDP problem of B-FL system in this
paper. The TD3 algorithm is presented in Algorithm 2 in
details.

Particularly, the actor-critic architecture is adopted in TD3
algorithm, which combines the value-based and policy-based
reinforcement learning algorithm and is shown in Fig. 4. In
the actor-critic architecture, the actor will output the selected
actions in continuous space from the input observed state, i.e.,

11

feature extractor and the output layer. Then, we introduce the
training processes of actor-critic architecture and update rules
of TD3 algorithm as follows.

1) Critic Network Training: The critic network aims to
estimate the real Q table in continuous state and action spaces
as shown in Fig. 5(a), which approximates the mapping func-
tion from state and action values to Q value, i.e., Qθ(st, at).
Therefore, the loss function of critic network is the mean
square error (MSE) between estimated value and the target
value, i.e.,

L(θ) = E

(cid:104)(cid:0)y

Qθ(st, at)(cid:1)2(cid:105)

,

(31)

−
where y is the target value and Qθ(st, at) is the estimated
value under DNN parameter θ. Especially, y is generated by
target critic network, i.e.,

(32)

y = rt + γQθ(cid:48)(st+1, a(cid:48)t+1),
where θ(cid:48) is DNN parameter in target network of critic network
and γ is discount factor. θ(cid:48) is the periodically updated param-
eters of the target critic Qθ(cid:48)(st, a(cid:48)t), and a(cid:48)t+1 is the action
taken in the next state st+1 selected by the target actor πφ(cid:48).
However, the update of the value function with the above
target value have an overestimate bias, which will also affect
the accurate update of the actor [56]. To address the bias
problem, the TD3 uses two approximately independent target
to estimate the value function, in which
critics
Qθ1 is a biased estimator and Qθ2 is an estimator with less
overestimate bias. The minimum of the two estimators is used
in the update of the value function, i.e.,
y = rt + γ min (cid:8)Qθ(cid:48)
1 (st+1, a(cid:48)t+1), Qθ(cid:48)
Under the loss function in (31), the update rules of the two
critic network can be written as

2(st+1, a(cid:48)t+1)(cid:9) .

Qθ1, Qθ2}
{

(33)

θi = θi

ηc

θi L(θi),

i = 1, 2,

∇

−
where ηc is learning rate of critic network, and
the gradient of i-th critic network, i.e.,
Est [(yj

Qi(sj, aj

θiL(θi) =

∀

∇

−

−

(34)

θiL(θi) is

∇

θi))
|

∇

θiQi(sj, aj

θi)] .
|

(35)

Fig. 4. Framework of TD3 algorithm.

(a) Structure of critic network.

(b) Structure of actor network.

Fig. 5. Proposed structures of actor and critic networks in TD3 algorithm.

the policy at = πφ(st). The critic is used to approximate the
action-value function, which outputs Q value from the inputs
selected action and current state, i.e., Q(st, at). Due to the
complexity of the mapping function of actor and critic, and
vector structure of state and action, the actor and critic are
implemented by deep neural networks (DNN), which consist
of several full-connected layers of neurons to construct the

TD3 AgentActor NetworkCritic NetworksActionRewardActionStateState and ActionAction Value FunctionStateEvaluation NetworksTarget NetworksMinimumParameter updateBandwidth AllocationPower Allocation Training LatencyCSIEnvironmentInput LayerHidden LayersOutput LayerCSI in Wireless B-FL SystemBandwidth AllocationPowerAllocationQ ValueInput LayerHidden LayersOutput LayerCSI in Wireless B-FL SystemBandwidth AllocationPowerAllocation12

2) Actor Network Training: The actor network aims to
select optimal actions in continuous state and action spaces as
shown in Fig. 5(b), which approximates the mapping function
from state to action, i.e., at = πφ(st). The output layer of
the actor network is designed to have neurons for bandwidth
allocation and power allocation. The ﬁrst set of output neurons
is implemented by softmax function, which guarantees the sum
of output is equal to 1 to represent the percentage of bandwidth
allocation. The second set of output neurons is implemented
by Sigmoid function, which guarantees each output is between
0 and 1 to represent the percentage of the maximum transmit
power. The optimal action is selected to achieve the maximum
E [Q1(st, at
θ1)], which utilizes the DNN parameter of the
|
ﬁrst critic network to update actor network. Accordingly, the
actor can be updated using the deterministic policy gradient
algorithm (DPG), i.e.,
φJ(φ) = Eπφ∇

(36)
where πφ(st) is actor network with DNN parameter φ. There-
fore, the update rule of actor network is

(cid:2)Qπφ(st+1, at+1)

|a=πφ(st)∇

φπφ(st)(cid:3) ,

∇

a

Particularly, the online critic networks are updated by mini-
mizing the loss function (31) and the online actor network is
updated base on (37). After every ϑ steps, the parameters of
targets networks are updated (38)-(40) with update proportion
δ. The pseudocode of TD3 based resource allocation algorithm
is presented in Algorithm 2.

We assume the number of edge servers and edge devices
is invariable during the FL training process. The B-FL can
still be applied when the edge servers or edge devices exit the
system before the completion of FL, but suffer from system
security and global model accuracy degradation. The DRL-
based resource allocation algorithm is compatible with the case
of moving edge servers or edge devices. When the edge servers
or edge devices exit the system, the relevant parameters in the
states and actions are set to be null to cope with the smaller
number of edge servers or edge devices [14]. However, if the
number of edge servers and edge devices increases, the DRL-
based resource allocation algorithm needs to be retrained,
because the output layer dimension of actor network increases.

φ = φ

ηa

∇

−

φJ(φ),

(37)

C. Complexity Analysis

where ηa is learning rate of actor network.

3) Target Networks Updating: The two critic networks and
one actor network have corresponding target networks, which
are called target critic networks and target actor network. The
target critic networks aim to generate target values, which
approximates real Q values. The target actor network is used
to select actions. In other words, the critic networks and actor
network, a.k.a., online networks, are used to update the DNN
parameters in each step, and the target networks are assumed
as real Q value and optimal action. The DNN parameters of
target networks are updated based on the online networks with
delayed method, i.e., the target networks update after several
steps of online networks. In general, the update rule of target
networks is

θ(cid:48)
1 = κθ1 + (1
θ(cid:48)
2 = κθ2 + (1
φ(cid:48) = κφ + (1

κ)θ(cid:48)
1,
κ)θ(cid:48)
2,
κ)φ(cid:48),

−

−

−

(38)

(39)

(40)

where κ is update proportion, i.e., the proportion of DNN
parameters of online networks.

In summary, there are two types of networks, i.e., two
critic networks and one actor network. Each of them contains
two sub-nets, i.e., online network and target network. These
six DNNs consist various layers and all layers contain their
φ, φ(cid:48)
corresponding parameters. Actor network parameters
}
{
denote online network parameter and target network parameter
indi-
respectively. Critic network parameters
cate online network parameter and target network parameter
of the two critic networks, respectively.

θ1, θ2, θ(cid:48)
{

1, θ(cid:48)

2}

Besides, the TD3 uses a reply memory buffer

to store
the experience (st, at, rt, st+1) from the interaction between
agent and environment. In the training process, a mini-batch
of experience is randomly selected from the reply memory
to update the parameters of the online networks.
buffer

R

R

Based on the description of TD3 algorithm, the training
process of the DRL for the proposed B-FL systems can be
performed ofﬂine with simulated channel states to train the
actor network and critic network. Then,
the trained actor
network models are deployed online to allocate the wireless
resources based on the system state to minimize the long-term
latency of B-FL. Furthermore, the experience (st, at, rt, st+1)
generated by online actor network can be collected, which is
used to train and update the actor network.

(cid:17)

−

O

l zc

l za

l=1 zc

(cid:16)
2 (cid:80)La

l+1 + 4 (cid:80)Lc

In the training phase, 2 actor networks and 4 critic net-
works need to be trained, the most signiﬁcant complexity is
caused by the back propagation. Hence, the computational
, where Lc
l=1 za
complexity is
l+1
is the number of layers in critic network. Let zc
l denotes the
number of neurons in l-th layer of critic network. Hence,
zc
1)+1+2(M +K) is the number of neurons
1 = K +M (M
in input layer and za
L = 1 is the number of neurons in output
layer. Since the proposed algorithm selected Q mini-batch
transitions to train the actor networks and critic networks and
takes P training step in total. Therefore, total computational
l=1 za
complexity is
In the deployment phase, for each single decision-making,
the complexity only caused by the actor network, which can
l za
, Let La is the number of
l=1 za
be calculated as
l+1
layers in actor network. The za
l denotes the number of neurons
in l-th layer of actor network. Hence, za
1 = K +M (M
1)+1
−
is the number of neurons in input layer and za
L = 2(M + K)
is the number of neurons in output layer.

l+1 + 4 (cid:80)Lc

2 (cid:80)La

(cid:16)(cid:80)La

l=1 zc

l za

l zc

P Q

(cid:17)(cid:17)

l+1

O

O

(cid:17)

(cid:16)

(cid:16)

.

V. NUMERICAL RESULTS AND ANALYSIS

In this section, various simulation results are presented to
show the performance of wireless B-FL and related TD3 based
resource allocation algorithm. We ﬁrst introduce the simulation
settings, and then show the results of proposed algorithms.

TABLE II
THE ACCURACIES OF FL AND B-FL WITH MINST DATASET OVER DIFFERENT PERCENTAGE OF MALICIOUS DEVICES.

Malicious edge devices percentage
FL with FedAvg
B-FL with multi-KRUM

0%
97.92
97.93

10%
95.59
97.63

20%
92.38
97.93

30%
89.67
97.80

40%
86.42
97.90

50%
11.35
93.74

60%
11.35
88.14

70%
11.35
11.35

80%
11.35
11.35

90%
10.28
9.80

100%
9.74
9.74

13

Fig. 6. Test loss of FL and B-FL with malicious devices for handwriting
recognition.

Fig. 7. Test accuracy of FL and B-FL with malicious edge devices for
handwriting recognition.

A. Settings and Benchmarks

In the following simulations, we consider a wireless B-FL
system consisting of several edge servers and edge devices to
execute ML task, which guarantees security and privacy in the
training process based on blockchain technique. The details
of the system model and simulation scenario are deﬁned as
follows.

1) System Settings: In our simulation, we consider a cir-
cular wireless network with M = 4 edge server serving for
K = 10 edge devices, and the location of each edge server or
edge device is uniformly distributed in a circle with a radius
of 100 meters. The duration of time-slot T0 is set as 10ms
according to LTE standard and Doppler frequency fd is 5Hz
due to the assumption of low mobility in B-FL system. The
path loss exponent α is 2.5. In general, we assume that the
maximum CPU frequencies of edge servers and devices are
2.4GHz and 1GHz, respectively. The maximum system trans-
mission powers are set as 24dBm and the maximum system
bandwidth is 100MHz. The noise power spectral density is
N0 =

174dBm/Hz.

2) MINST Dataset for Handwriting Recognition: To sim-
ulate the performance of FL and B-FL, we consider training
a CNN model based on MINST dataset, which is a standard
dataset used for testing ML algorithm and consists of 60, 000
training handwritten image samples and 10, 000 test handwrit-
ten image samples. The handwritten image samples are 28
28
gray scale images denoting a handwritten number within 0
to 9. The dataset gives a typical and proper complexity in
ML tasks. We design the convolutional neural network (CNN)
5 convolutional layers (10 channels and 20
model by two 5
2 max-pooling layers, two fully
channels respectively), two 2
connected layers, and two dropout layers. The rectiﬁed linear

×

×

×

−

unit (ReLU) activation is used in each layer and the softmax
activation is employed in output layer. Therefore, each device
randomly has 6000 training samples and 1000 test samples.
In the training process, each device executes 2 local training
epoch with 128 batch size and the learning rate is set as 0.01.

3) CIFAR-10 Dataset for Image Classiﬁcation: To verify
the performance of B-FL in practical real-word dataset, we
utilize more popular and standard CIFAR-10 dataset and train
the AlexNet model. The CIFAR-10 consists of 50000 training
32 color images in 10 classes and 10000 test images. The
32
AlexNet is a typical CNN model with 5 convolutional layers
and max pooling and 3 fully-connected layers. Therefore, each
device randomly has 5000 training samples and executes 1
local training epoch with 128 batch size and the learning rate
is set as 0.01.

×

4) Heart Activity Dataset for Affect Recognition: More and
more researches have shown great interest to improve the
health of individuals via prevention and diagnostics by using
sensors of Internet of Medical Thing (IoMT). In particular,
affect recognition of stress is remarkable to improve the health
of individuals based on the biomedical informatics collected
from wearable IoT devices. In this paper, we utilize the
preprocessed dataset of 26 individuals from the paper [57].
The dataset consists of heart activity samples range from 60
to 125, which means a non-independent identically distributed
(non-iid) dataset. Each sample has a 16 dimension feature
vector and a label to indicate low stress and high stress, which
means it is a 2-class stress-level classiﬁcation. We design the
fully-connected neural network (FNN) model by two hidden
layers with ReLU activation and one output layer with sigmoid
activation. Each hidden layer is composed of 100 neurons. In
the training process, we set that the batch size is 32, the local

            5 R X Q G V                7 H V W  O R V V ) /   0 D O L F L R X V  G H Y L F H V       ) /   0 D O L F L R X V  G H Y L F H V        ) /   0 D O L F L R X V  G H Y L F H V        %  ) /  0 D O L F L R X V  G H Y L F H V        %  ) /  0 D O L F L R X V  G H Y L F H V                   5 R X Q G V                7 H V W  D F F X U D Q F \ ) /   0 D O L F L R X V  G H Y L F H V       ) /   0 D O L F L R X V  G H Y L F H V       ) /   0 D O L F L R X V  G H Y L F H V        %  ) /  0 D O L F L R X V  G H Y L F H V        %  ) /  0 D O L F L R X V  G H Y L F H V       14

Fig. 8. Test loss of FL and B-FL with malicious devices for affect recognition.

Fig. 9. Test accuracy of FL and B-FL with malicious edge devices for affect
recognition.

learning rate is 5e-6 and local training epoch is 1. Furthermore,
we partition the 26 individuals as 20 training edge devices to
train a shared global model and 6 test edge devices to evaluate
the accuracy of global model.

5) TD3 Algorithm Settings: Base on the details of TD3
algorithm in Section IV, we implement the proposed algorithm
by TensorFlow in a general computer with CPU Intel Xeon
E5-2643 v4 and GPU Nvidia GeForce GTX 1080 Ti. The
actor network is designed by 5 hidden layers, which has 512,
1024, 2048, 1024, and 512 neurons with ReLU activation
respectively. The output layer is a softmax layer to output
action with constrains. The critic network is constructed by 4
hidden layers, which has 512, 1024, 512, and 512 neurons
with ReLU activation respectively. The output
layer is a
linear layer to output Q value. The hyper-parameters in TD3
algorithm are set as follows, i.e., the size of replay buffer
10−3, discount factor
is 106, update proportion κ = 5
γ = 0.99, steps of exploration E = 512, update frequency
ϑ = 2, and the learning rate of the actor and critic networks
10−4. We train the networks with max
is ηa = ηc = 1
steps 5000, which can achieve stable performance in our
experiments.

×

×

6) Benchmarks: To verify the effectiveness of our proposed
TD3-based resource allocation algorithm, we choose three
other algorithms to compare, i.e.,

• Random Allocation: The random allocation scheme allo-
cates the bandwidth and transmit power to edge servers
and devices from uniformly random distribution.

• Average Allocation: The average allocation scheme uni-
formly allocates the bandwidth and transmit power to
edge servers and devices, i.e., all edge servers and edge
devices are assigned with the same resources.

• Monte Carlo Algorithm: The Monte Carlo algorithm
randomly samples C allocation solutions and choose the
best one, i.e., the latency is the smallest. Therefore, the
best allocation solution is close to the global optimal
solution if the C is large enough. We set C = 106 in
the paper.

B. Performance of wireless B-FL

We simulate the processes of B-FL following Algorithm
1. The malicious edge devices is assumed to upload local
models with random DNN parameter, which follow normal
(0, 1) and will obviously inﬂuence the con-
distribution
vergence performance of FL training [13]. Besides, we use
FedAvg algorithm as global model aggregation algorithm in
FL training process [2].

N

In the handwriting recognition task, we set that the per-
centage of malicious edge devices ranges from 0% to 100%
and train CNN in 100 global epochs. The results are shown
in Table II, which shows the malicious edge devices will
obviously decrease the accuracy. The proposed B-FL in this
paper shows robustness for the malicious edge devices, which
can not be affected below 50%. The partial loss and accuracy
of training rounds are shown in Fig. 7 and Fig. 6. The results
show that the proposed B-FL with 40% malicious devices and
FL with 0% malicious devices have the same performance,
which is because the secure global aggregation based on multi-
local models from
KRUM algorithm can eliminate partial
malicious devices. Therefore, the global model is not affected
by malicious local models. Furthermore, the inﬂuence of the
number of aggregated local models is inconspicuous in this
simulation settings.

In image classiﬁcation task, we set the percentage of ma-
licious edge devices as 0%, 20% and 40%. The test loss and
test accuracy are shown in Fig. 10, and Fig. 11, which indicate
that the malicious devices yield signiﬁcant inﬂuence on the
FL training process. However, the proposed B-FL with 40%
malicious edge devices can still maintain the similar accuracy
with FL with 0% malicious edge devices, which is mainly
because the proposed B-FL utilizes secure global aggregation
based on multi-KRUM algorithm to eliminate local models
from malicious devices.

In the affect recognition task, we assume the FL and B-
FL systems have 10% malicious edge devices to attack global
model respectively and present the performance of centralized
FL and proposed B-FL in different settings. The loss and

                 5 R X Q G V                   7 H V W  O R V V ) /   0 D O L F L R X V  G H Y L F H     ) /   0 D O L F L R X V  G H Y L F H      %  ) /   0 D O L F L R X V  G H Y L F H                      5 R X Q G V                      7 H V W  $ F F X U D Q F \ ) /   0 D O L F L R X V  G H Y L F H    ) /   0 D O L F L R X V  G H Y L F H     %  ) /   0 D O L F L R X V  G H Y L F H    15

Fig. 10. Test loss of FL and B-FL with malicious edge devices for CIFAR10
dataset.

Fig. 12. Convergence performance of TD3-based resource allocation algo-
rithm.

Fig. 11. Test accuracy of FL and B-FL with malicious edge devices for
CIFAR10 dataset.

accuracy of training rounds are shown in Fig. 9 and Fig. 8.
The proposed B-FL can conspicuously resist the attack of
malicious edge devices and achieve excellent performance.
We just plot the scenario of 10% malicious edge devices in
this task, which is because the test loss is diverging when the
number of malicious edge devices continues to increase.

On the other hand, the situation of malicious edge servers is
also considered in this paper. It’s obvious that the FL training
process will be discontinued or destroyed completely when the
only edge server is malicious. However, this will not happen
in B-FL system, which has multiple edge servers to reach
consensus and produce global model. When the primary edge
server is malicious, the other validator edge servers will not
accept its block and global model and will choose another new
primary edge server. If less than 1/3 validator edge servers are
malicious, the consensus can not be destroyed because of the
mechanism of PBFT. If more than 1/3 validator edge servers
are malicious, the consensus will be destroyed and the ML
training process will be discontinued or destroyed completely.
However, the situation of more than 1/3 malicious validator

Fig. 13. Latency (s) vs. maximum system bandwidth (MHz).

edge servers is almost impossible because the authorization is
generally required for validator edge servers to participate in
the consensus.

C. Convergence of TD3 Based Resource Allocation Algorithm

To evaluate the effectiveness of our proposed TD3 based
resource allocation algorithm, the convergence behavior of
TD3 is shown in Fig. 12. The performance is presented by
instant rewards following the training steps. We also illustrate
the performance over different learning rate, i.e., ηa = ηc =
. We choose the two learning rates because
1e
{
of the brief and clear performance improvement. It can be
seen that the rewards will converge with the increase of time
step t. And the convergence rate will be promoted with larger
learning rate.

4, 8e

5
}

−

−

D. Performance Impact of Different System Settings

To evaluate the performance of proposed TD3 based re-
source allocation algorithm, we compare it with three bench-

                                   / R V V ) /   0 D O L F L R X V  G H Y L F H     ) /   0 D O L F L R X V  G H Y L F H      %  ) /   0 D O L F L R X V  G H Y L F H      ) /   0 D O L F L R X V  G H Y L F H      %  ) /   0 D O L F L R X V  G H Y L F H                                                 $ F F X U D Q F \ ) /   0 D O L F L R X V  G H Y L F H     ) /   0 D O L F L R X V  G H Y L F H      %  ) /   0 D O L F L R X V  G H Y L F H      ) /   0 D O L F L R X V  G H Y L F H      %  ) /   0 D O L F L R X V  G H Y L F H                            7 U D L Q L Q J  V W H S V                   5 H Z D U G / H D U Q L Q J  U D W H      H   / H D U Q L Q J  U D W H      H  50100150200250300Maximumsystembandwidth(MHz)0.60.81.01.21.41.61.82.0Long-termAverageLatency(s)ProposedAlgorithmRandomAllocationAverageAllocationMonteCarloAlgorithm16

can signiﬁcantly decrease with increasing maximum system
bandwidth. Similarly, we present
the impact of maximum
system transmit power in Fig. 14.

Fig. 15 plots the long-term latency to present the scalability
to deal with different network scale of edge devices in the
system. The number of edge devices varies from 10 to 60
and the other system settings are ﬁxed. In the simulation,
we ﬁnd that TD3-based algorithm achieves greater gaps than
three benchmarks with increasing number of edge devices. The
long-term latency is increasing over more edge devices, which
is because the average resources decrease and the size of new
block increase.
For practical

the B-FL system can be
implemented by using the open source blockchain frameworks,
such as Ethereum, Hyperledger Fabric, Corda, and FISCO
BCOS. Based on the blockchain framework, the local model
can be submitted as a transaction and the global model
aggregation and validation algorithms can be deployed as
smart contracts. Particularly, [46] and [47] have already de-
ployed FL in the Ethereum framework and Corda framework,
respectively.

implementations,

Fig. 14. Latency (s) vs. maximum system transmit power (dBm).

VI. CONCLUSION

In this paper, we developed a B-FL architecture to en-
sure the security and privacy, which utilizes secure global
aggregation and blockchain technique to resist
the attacks
from malicious edge devices and servers. We utilized PBFT
consensus protocol in B-FL to achieve high effectiveness and
low energy consumption for trustworthy FL. The procedures
of PBFT-based wireless B-FL was presented at ﬁrst, and the
training latency was then analyzed. We have formulated an
optimization problem that jointly considers bandwidth alloca-
tion and transmit power allocation to minimize the long-term
average training latency. To solve this network optimization
problem, we derived TD3-based algorithm to achieve long-
term resource allocation and low computational complexity.
Finally, we simulated the learning performance of wireless B-
FL and efﬁciency of DRL based resource allocation algorithm,
which is compared with baseline algorithms (i.e., random al-
location, average allocation, and Monte Carlo algorithm). Our
simulation results shown that the wireless B-FL architecture
can resist the attacks from malicious servers and malicious
devices. Furthermore, the training latency of wireless B-FL can
be signiﬁcantly reduced by the developed TD3-based adaptive
resource allocation scheme.

REFERENCES

[1] J. Zhang and K. B. Letaief, “Mobile edge intelligence and computing
for the internet of vehicles,” Proc. IEEE, vol. 108, no. 2, pp. 246–261,
Feb. 2020.

[2] K. Yang, Y. Shi, Y. Zhou, Z. Yang, L. Fu, and W. Chen, “Federated ma-
chine learning for intelligent iot via reconﬁgurable intelligent surface,”
IEEE Netw., vol. 34, no. 5, pp. 16–22, Sep. 2020.

[3] V. Hayyolalam, M. Aloqaily, O. Ozkasap, and M. Guizani, “Edge in-
telligence for empowering iot-based healthcare systems,” IEEE Wireless
Commun., vol. 28, no. 3, pp. 6–14, Jun. 2021.

[4] H. Liu, Y. Wang, W. Fan, X. Liu, Y. Li, S. Jain, Y. Liu, A. K. Jain, and
J. Tang, “Trustworthy ai: A computational perspective,” arXiv preprint
arXiv:2107.06641, Aug. 2021.

Fig. 15. Latency (s) vs. number of edge devices.

mark algorithms, i.e., random allocation, average allocation,
and Monte Carlo algorithm. Thus, we plot three ﬁgures to
indicate the performance of our proposed algorithm and impact
of different system settings, i.e., maximum system bandwidth,
maximum system transmit power, and the number of edge
devices. We average the results over 500 realizations of the
system in our experiments to mitigate the randomness of
devices’ locations and channel states.

Fig. 13 illustrates the inﬂuence of maximum system band-
width bmax to the long-term latency during B-FL training pro-
cess. Firstly, one observation is that our proposed TD3-based
algorithm has the similar result with the benchmark Monte
Carlo algorithm, and is better than the random allocation
and average allocation schemes. In particular, the DRL-based
algorithm is a little better than Monte Carlo algorithm, because
the Monte Carlo method is computationally expensive and can
only obtain a sub-optimal solution by using a ﬁnite number of
samples. We take this result to present the performance of our
TD3-based algorithm, which chooses more efﬁcient and better
resource allocation scheme. Secondly, the long-term latency

−20−100102030Maximumsystemtransmitpower(dBm)1.01.52.02.53.0Long-termAverageLatency(s)ProposedAlgorithmRandomAllocationAverageAllocationMonteCarloAlgorithm102030405060Numberofdevices0.02.55.07.510.012.515.017.520.0Long-termAverageLatency(s)ProposedAlgorithmRandomAllocationAverageAllocationMonteCarloAlgorithm17

[5] K. Yang, T. Jiang, Y. Shi, and Z. Ding, “Federated learning via over-
the-air computation,” IEEE Trans. Wireless Commun., vol. 19, no. 3, pp.
2022–2035, Mar. 2020.

[28] M. Chen, H. V. Poor, W. Saad, and S. Cui, “Convergence time opti-
mization for federated learning over wireless networks,” IEEE Trans.
Wireless Commun., vol. 20, no. 4, pp. 2457–2471, Apr. 2021.

[6] W. Y. B. Lim, J. S. Ng, Z. Xiong, D. Niyato, C. Miao, and D. I. Kim,
“Dynamic edge association and resource allocation in self-organizing hi-
erarchical federated learning networks,” IEEE J. Select. Areas Commun.,
vol. 39, no. 12, pp. 3640–3653, Dec. 2021.

[7] K. B. Letaief, Y. Shi, J. Lu, and J. Lu, “Edge artiﬁcial intelligence for 6g:
Vision, enabling technologies, and applications,” IEEE J. Select. Areas
Commun., vol. 40, no. 1, pp. 5–36, Nov. 2021.

[8] Y. Shi, K. Yang, T. Jiang, J. Zhang, and K. B. Letaief, “Communication-
efﬁcient edge ai: Algorithms and systems,” IEEE Commun. Surv. Tuto-
rials, vol. 22, no. 4, pp. 2167–2191, Jul. 2020.

[9] Z. Wang, J. Qiu, Y. Zhou, Y. Shi, L. Fu, W. Chen, and K. B. Letaief,
“Federated learning via intelligent reﬂecting surface,” IEEE Trans.
Wireless Commun., vol. 21, no. 2, pp. 808–822, Feb. 2022.

[10] T. Yin, L. Li, D. Ma, W. Lin, J. Liang, and Z. Han, “Flight: Federated
learning with irs for grouped heterogeneous training,” J. Commun. Inf.
Netw., vol. 7, no. 2, pp. 135–144, Jun. 2022.

[11] S. Liu, G. Yu, R. Yin, J. Yuan, L. Shen, and C. Liu, “Joint model
pruning and device selection for communication-efﬁcient federated edge
learning,” IEEE Trans. Commun., vol. 70, no. 1, pp. 231–244, Jan. 2022.
[12] J. So, B. Guler, and A. S. Avestimehr, “Byzantine-resilient secure
federated learning,” IEEE J. Select. Areas Commun., vol. 39, no. 7,
pp. 2168–2181, Jul. 2021.

[13] P. Blanchard, E. M. E. Mhamdi, R. Guerraoui, and J. Stainer, “Machine
learning with adversaries: Byzantine tolerant gradient descent,” in Proc.
Adv. Neural Inf. Process. Syst. (NeurIPS), Dec. 2017, pp. 118–128.
[14] F. Guo, F. R. Yu, H. Zhang, H. Ji, M. Liu, and V. C. M. Leung, “Adaptive
resource allocation in future wireless networks with blockchain and
mobile edge computing,” IEEE Trans. Wireless Commun., vol. 19, no. 3,
pp. 1689–1703, Mar. 2020.

[15] Z. Xiong, S. Feng, W. Wang, D. Niyato, P. Wang, and Z. Han,
“Cloud/fog computing resource management and pricing for blockchain
networks,” IEEE Internet Things J., vol. 6, no. 3, pp. 4585–4600, Jun.
2019.

[16] J. Kang, Z. Xiong, D. Niyato, D. Ye, D. I. Kim, and J. Zhao, “Toward
secure blockchain-enabled internet of vehicles: Optimizing consensus
management using reputation and contract theory,” IEEE Trans. Veh.
Technol., vol. 68, no. 3, pp. 2906–2920, Mar. 2019.

[17] Z. Xiong, Y. Zhang, D. Niyato, P. Wang, and Z. Han, “When mobile
blockchain meets edge computing,” IEEE Commun. Mag., vol. 56, no. 8,
pp. 33–39, Aug. 2018.

[18] Y. Xiao, N. Zhang, W. Lou, and Y. T. Hou, “A survey of distributed
consensus protocols for blockchain networks,” IEEE Commun. Surv.
Tuts., vol. 22, no. 2, pp. 1432–1465, Jan. 2020.

[19] S. Warnat-Herresthal, H. Schultze, K. L. Shastry, S. Manamohan,
S. Mukherjee, V. Garg, R. Sarveswara, K. H¨andler, P. Pickkers, N. A.
Aziz et al., “Swarm learning for decentralized and conﬁdential clinical
machine learning,” Nature, vol. 594, no. 7862, pp. 265–270, Jun. 2021.
[20] D. C. Nguyen, M. Ding, Q.-V. Pham, P. N. Pathirana, L. B. Le,
A. Seneviratne, J. Li, D. Niyato, and H. V. Poor, “Federated learning
meets blockchain in edge computing: Opportunities and challenges,”
IEEE Internet Things J., vol. 8, no. 16, pp. 12 806–12 825, Aug. 2021.
[21] Z. Wang and Q. Hu, “Blockchain-based federated learning: A compre-

hensive survey,” arXiv preprint arXiv:2110.02182, Oct. 2021.

[22] S. Zhou, H. Huang, W. Chen, P. Zhou, Z. Zheng, and S. Guo, “Pirate:
A blockchain-based secure framework of distributed machine learning
in 5g networks,” IEEE Network, vol. 34, no. 6, pp. 84–91, Nov. 2020.
[23] K. B. Letaief, W. Chen, Y. Shi, J. Zhang, and Y.-J. A. Zhang, “The
roadmap to 6g: Ai empowered wireless networks,” IEEE Commun. Mag.,
vol. 57, no. 8, pp. 84–90, Aug. 2019.

[24] Y. Shi, K. Yang, Z. Yang, and Y. Zhou, Mobile Edge Artiﬁcial Intelli-

gence: Opportunities and Challenges. Elsevier, Aug. 2021.

[25] L. Chang, Z. Zhang, P. Li, S. Xi, W. Guo, Y. Shen, Z. Xiong, J. Kang,
D. Niyato, X. Qiao et al., “6g-enabled edge ai for metaverse: Challenges,
methods, and future research directions,” J. Commun. Inf. Netw., vol. 7,
no. 2, pp. 107–121, Jun. 2022.

[26] Z. Yang, Y. Zhou, Y. Wu, and Y. Shi, “Communication-efﬁcient quan-
tized sgd for learning polynomial neural network,” in Proc. IEEE Int.
Perform. Comput. Commun. Conf. (IPCCC), Oct. 2021, pp. 1–6.
[27] L. Li, L. Yang, X. Guo, Y. Shi, H. Wang, W. Chen, and K. B.
Letaief, “Delay analysis of wireless federated learning based on saddle
point approximation and large deviation theory,” IEEE J. Select. Areas
Commun., vol. 39, no. 12, pp. 3772–3789, Dec. 2021.

[29] J. Ren, G. Yu, and G. Ding, “Accelerating dnn training in wireless
federated edge learning systems,” IEEE J. Select. Areas Commun.,
vol. 39, no. 1, pp. 219–232, Jan. 2021.

[30] Z. Yang, M. Chen, W. Saad, C. S. Hong, M. Shikh-Bahaei, H. V. Poor,
and S. Cui, “Delay minimization for federated learning over wireless
communication networks,” arXiv preprint arXiv:2007.03462, Jul. 2020.
[31] M. Castro and B. Liskov, “Practical byzantine fault tolerance,” in Proc.
USENIX Symp. Oper. Syst. Design Implement. (OSDI), vol. 99, Feb.
1999, pp. 173–186.

[32] Y. Shen, Y. Shi, J. Zhang, and K. B. Letaief, “Lorm: Learning to
in wireless networks with few
optimize for resource management
training samples,” IEEE Trans. Wireless Commun., vol. 19, no. 1, pp.
665–679, Jan. 2020.

[33] D. Wen, M. Bennis, and K. Huang, “Joint parameter-and-bandwidth
allocation for improving the efﬁciency of partitioned edge learning,”
IEEE Trans. Wireless Commun., vol. 19, no. 12, pp. 8272–8286, Dec.
2020.

[34] Z. Xiong, Y. Zhang, W. Y. B. Lim, J. Kang, D. Niyato, C. Leung,
and C. Miao, “Uav-assisted wireless energy and data transfer with deep
reinforcement learning,” IEEE Trans. Cogn. Commun. Netw., vol. 7,
no. 1, pp. 85–99, Mar. 2021.

[35] Y. Qu, L. Gao, T. H. Luan, Y. Xiang, S. Yu, B. Li, and G. Zheng,
“Decentralized privacy using blockchain-enabled federated learning in
fog computing,” IEEE Internet Things J., vol. 7, no. 6, pp. 5171–5183,
Jun. 2020.

[36] C. Ma, J. Li, M. Ding, L. Shi, T. Wang, Z. Han, and H. V. Poor,
“When federated learning meets blockchain: A new distributed learning
paradigm,” arXiv preprint arXiv:2009.09338, Sep. 2020.

[37] Y. Li, C. Chen, N. Liu, H. Huang, Z. Zheng, and Q. Yan, “A blockchain-
based decentralized federated learning framework with committee con-
sensus,” IEEE Network, vol. 35, no. 1, pp. 234–241, Dec. 2020.
[38] M. Shayan, C. Fung, C. J. M. Yoon, and I. Beschastnikh, “Biscotti:
A blockchain system for private and secure federated learning,” IEEE
Trans. Parallel Distrib. Syst., vol. 32, no. 7, pp. 1513–1525, Dec. 2020.
[39] Y. Lu, X. Huang, K. Zhang, S. Maharjan, and Y. Zhang, “Low-latency
federated learning and blockchain for edge association in digital twin
empowered 6g networks,” IEEE Trans. Ind. Inf., vol. 17, no. 7, pp.
5098–5107, Jul. 2021.

[40] H. Kim, J. Park, M. Bennis, and S.-L. Kim, “Blockchained on-device
federated learning,” IEEE Commun. Lett., vol. 24, no. 6, pp. 1279–1283,
Jun. 2020.

[41] J. Li, Y. Shao, K. Wei, M. Ding, C. Ma, L. Shi, Z. Han, and
V. Poor, “Blockchain assisted decentralized federated learning (blade-
ﬂ): Performance analysis and resource allocation,” IEEE Trans. Parallel
Distrib. Syst., undeﬁned 2022, DOI: 10.1109/TPDS.2021.3138848.
[42] S. R. Pokhrel and J. Choi, “Federated learning with blockchain for
autonomous vehicles: Analysis and design challenges,” IEEE Trans.
Commun., vol. 68, no. 8, pp. 4734–4746, Aug. 2020.

[43] N. Q. Hieu, T. T. Anh, N. C. Luong, D. Niyato, D. I. Kim, and
E. Elmroth, “Resource management for blockchain-enabled federated
learning approach,” arXiv preprint
learning: A deep reinforcement
arXiv:2004.04104, May. 2020.

[44] D. C. Nguyen, S. Hosseinalipour, D. J. Love, P. N. Pathirana,
and C. G. Brinton, “Latency optimization for blockchain-empowered
federated learning in multi-server edge computing,” arXiv preprint
arXiv:2203.09670, Mar. 2022.

[45] Y. Zhao, J. Zhao, L. Jiang, R. Tan, D. Niyato, Z. Li, L. Lyu, and Y. Liu,
“Privacy-preserving blockchain-based federated learning for iot devices,”
IEEE Internet Things J., vol. 8, no. 3, pp. 1817–1829, Feb. 2021.
[46] W. Zhang, Q. Lu, Q. Yu, Z. Li, Y. Liu, S. K. Lo, S. Chen, X. Xu, and
L. Zhu, “Blockchain-based federated learning for device failure detection
in industrial iot,” IEEE Internet Things J., vol. 8, no. 7, pp. 5926–5937,
Apr. 2021.

[47] J. Kang, Z. Xiong, D. Niyato, S. Xie, and J. Zhang, “Incentive mech-
anism for reliable federated learning: A joint optimization approach
to combining reputation and contract theory,” IEEE Internet Things J.,
vol. 6, no. 6, pp. 10 700–10 714, Dec. 2019.

[48] X. Jiang, F. R. Yu, T. Song, and V. C. Leung, “Intelligent resource allo-
cation for video analytics in blockchain-enabled internet of autonomous
vehicles with edge computing,” IEEE Internet Things J., Sep. 2020,
DOI: 10.1109/JIOT.2020.3026354.

[49] P. Kairouz, H. B. McMahan, B. Avent, A. Bellet, M. Bennis,
A. Nitin Bhagoji, K. Bonawitz, Z. Charles, G. Cormode, R. Cummings

18

et al., “Advances and open problems in federated learning,” FNT in
Machine Learning, vol. 14, no. 1–2, pp. 1–210, Jun. 2021.

[50] W. Y. B. Lim, J. S. Ng, Z. Xiong, J. Jin, Y. Zhang, D. Niyato, C. Leung,
and C. Miao, “Decentralized edge intelligence: A dynamic resource
allocation framework for hierarchical federated learning,” IEEE Trans.
Parallel Distrib. Syst., vol. 33, no. 3, pp. 536–550, Mar. 2022.

[51] T. Li, A. K. Sahu, A. Talwalkar, and V. Smith, “Federated learning:
Challenges, methods, and future directions,” IEEE Signal Process. Mag.,
vol. 37, no. 3, pp. 50–60, May. 2020.

[52] R. Yang, F. R. Yu, P. Si, Z. Yang, and Y. Zhang, “Integrated blockchain
and edge computing systems: A survey, some research issues and
challenges,” IEEE Commun. Surv. Tuts., vol. 21, no. 2, pp. 1508–1532,
Jan. 2019.

[53] S. Huang, Y. Zhou, T. Wang, and Y. Shi, “Byzantine-resilient federated
machine learning via over-the-air computation,” in Pro. IEEE Int. Conf.
Commun. (ICC) Workshop.

IEEE, Jun. 2021, pp. 1–6.

[54] Z. Wang, J. Zong, Y. Zhou, Y. Shi, and V. W. S. Wong, “Decentralized
multi-agent power control in wireless networks with frequency reuse,”
IEEE Trans. Commun., vol. 70, no. 3, pp. 1666–1681, Mar. 2022.
[55] Y. Shen, Y. Shi, J. Zhang, and K. B. Letaief, “Graph neural networks for
scalable radio resource management: Architecture design and theoretical
analysis,” IEEE J. Select. Areas Commun., vol. 39, no. 1, pp. 101–115,
Jan. 2021.

[56] S. Fujimoto, “Addressing function approximation error in actor-critic
methods,” in Proc. Int. Conf. Mach. Learn. (ICML), Jul. 2018, pp. 1587–
1596.

[57] Y. S. Can and C. Ersoy, “Privacy-preserving federated deep learning
for wearable iot-based biomedical monitoring,” ACM Trans. Internet
Technol., vol. 21, no. 1, pp. 1–17, Feb. 2021.

