1
2
0
2

y
a
M
1
3

]

R

I
.
s
c
[

1
v
7
9
4
5
1
.
6
0
1
2
:
v
i
X
r
a

A Bytecode-based Approach for Smart Contract
Classiﬁcation

Chaochen Shi

, Yong Xiang , Senior Member, IEEE, Robin Ram Mohan Doss , Senior Member, IEEE,

Jiangshan Yu , Member, IEEE, Keshav Sood , Member, IEEE, and Longxiang Gao , Senior
Member, IEEE

1

Abstract—With the development of blockchain technologies, the number of smart contracts deployed on blockchain platforms is
growing exponentially, which makes it difﬁcult for users to ﬁnd desired services by manual screening. The automatic classiﬁcation of
smart contracts can provide blockchain users with keyword-based contract searching and helps to manage smart contracts effectively.
Current research on smart contract classiﬁcation focuses on Natural Language Processing (NLP) solutions which are based on
contract source code. However, more than 94% of smart contracts are not open-source, so the application scenarios of NLP methods
are very limited. Meanwhile, NLP models are vulnerable to adversarial attacks. This paper proposes a classiﬁcation model based on
features from contract bytecode instead of source code to solve these problems. We also use feature selection and ensemble learning
to optimize the model. Our experimental studies on over 3,300 real-world Ethereum smart contracts show that our model can classify
smart contracts without source code and has better performance than baseline models. Our model also has good resistance to
adversarial attacks compared with NLP-based models. In addition, our analysis reveals that account features used in many smart
contract classiﬁcation models have little effect on classiﬁcation and can be excluded.

Index Terms—Smart contract classiﬁcation, Bytecode, Blockchain, Ethereum.

(cid:70)

1 INTRODUCTION

A Smart contract is an event-driven program running on

distributed ledgers. The concept of smart contract was
originally introduced by Szabo [1], providing a commitment
deﬁned in a digital form. As of April 2020, the number of
smart contracts on Ethereum [2] exceeds two million [3]. As
the number of contracts increases, how to help users ﬁnd
the services they need in massive contracts has become an
important issue. The primary query APIs of smart contracts
provided by blockchain platforms are based on contract
address, block number, transaction hash, and timestamp.
Some commercial tools such as Google Bigquery [4] and
Dfuse [5] provide SQL and GraphQL supported blockchain
databases to realize complex queries on row data. How-
ever, as blockchain platforms are gradually evolving into
distributed data centers, users desire a more convenient
searching experience, e.g., searching by keywords [6] or
categories [7]. An essential step to conduct such searching
is labeling smart contracts accurately. Currently, the identi-
ﬁcation of smart contracts relies on manual labeling, which
is costly and inefﬁcient. Therefore, it is necessary to design
an effective classiﬁcation model to classify and label existing

• C. Shi, Y. Xiang and L. Gao are with the Deakin Blockchain Innovation
Lab, School of Information Technology, Deakin University, Geelong,
Australia.
E-mail: {shicha, yxiang, longxiang.gao}@deakin.edu.au.

• R. Doss and K. Sood are with the Centre for Cyber Security Research
and Innovation, School of Information Technology, Deakin University,
Geelong, Australia.
E-mail: {robin.doss, keshav.sood}@deakin.edu.au.
J. Yu is with the Monash Blockchain Technology Centre, Faculty of
Information Technology, Monash University, Australia.
E-mail: jiangshan.yu@monash.edu.

•

or newly uploaded contracts automatically. The goal of the
classiﬁcation is as follows:

The dataset is deﬁned as {Di, yj}, where Di refers to a
smart contract and yj belongs to Y which is a predeﬁned
collection of k categories, Y = {y1, y2, . . . , yk}. The goal is
to learn a mapping function h which maps an input Di to
the category yj it belongs to.

As shown in Fig. 1, a smart contract consists of source
code and comments. Currently, the prevalent method to
classify smart contracts is using NLP techniques such as the
attention-based LSTM network [8] to capture the semantic
features from source code and comments (described in
Section 2). However, there are two main problems in the
existing NLP-based models:

1) NLP-based models have limited application scenar-
ios. They can only classify open-source contracts [9].
However, open-source code is not mandatory for
contract developers. Less than 4% contracts are
open-source on Ethereum [3], which means that
NLP models cannot classify more than 96% of smart
contracts;

2) NLP-based models can be easily attacked by ad-
versarial examples [10]. Context features like com-
ments, variable names, and function names can be
easily modiﬁed in the source code without chang-
ing the logic of the code. Developers can write
their source code in different ways to perform the
same functions; however, any additions, deletions,
or modiﬁcation of the source code may fool the
NLP-based classiﬁer.

(Corresponding author: Yong Xiang.)

To solve these problems, we need models which can

 
 
 
 
 
 
2

Fig. 1. The composition of the smart contract.

classify smart contracts without the source code. Inspired
by the wide use of bytecode features in other areas of smart
contracts, such as contract vulnerability and fraud detection,
we found that bytecodes can reﬂect the functional features
from the logical aspect. We expect that bytecode features can
also be successfully applied to smart contract classiﬁcations.

In this paper, we propose a multi-classiﬁcation model for
smart contracts based on bytecode features. Compared with
NLP-based models, our bytecode-based approach can effec-
tively classify smart contracts without source code, which
signiﬁcantly expands the application scenarios. In addition,
adversarial attacks against the source code have little effect
on the bytecode-based model because the attacked semantic
information is discarded after being compiled. Considering
that the categories of smart contracts are unevenly dis-
tributed in the blockchain platforms, we propose employing
feature selection (Binary Particle Swarm Optimization [11])
and ensemble learning techniques (Adaboost [12]) to solve
the problem of data imbalance in our models. This paper
focuses on the Ethereum platform, but the approach can
easily be expanded to other blockchain platforms. The key
contributions of this paper are as follows.

1) We propose a bytecode-based approach, which is
the ﬁrst approach to classify smart contracts when
their source code is intentionally hidden.

2) We demonstrate that our bytecode-based approach
has better resistance to adversarial attacks than
state-of-the-art source code-based approaches.
3) We prove that feature selection and ensemble learn-
ing are competitive alternatives to solve data imbal-
ance problems in smart contracts classiﬁcation.
4) We determine that account features have little effect
on classiﬁcations compared with code features and
explain the reason. It has guiding signiﬁcance for
future research on the classiﬁcation model of smart
contracts.

TABLE 1
Examples of Ethereum opcodes.

Value
0x06
0x0a
0x10
0x33
0x5a
0x60
0x54

Mnemonic
MOD
EXP
LT
CALLER
GAS
PUSH1
SLOAD

Description
Modulo addition operation.
Exponential operation.
Less than operation
Get the caller address.
Get the amount of available gas.
Place a 1 byte item on the stack.
Load the ﬁrst word from storage.

2 BACKGROUND AND RELATED WORK

2.1 Background

Ethereum is one of
the most popular programmable
blockchain with a built-in Turing-complete instruction set.
Users can develop customized cryptocurrencies or decen-
tralized applications (Dapps) built on smart contracts on
the Ethereum platform. As the core of Ethereum,
the
Ethereum virtual machine (EVM) can compile high-level
programming languages such as Solidity into bytecode.
The bytecode consists of a series of bytes, and each byte
refers to a speciﬁc operation represented by a correspond-
ing mnemonic form predeﬁned in the Ethereum yellow
paper [13]. For example, the mnemonic of value 0x01 is
ADD, which means the add operation. These mnemonic
forms are called opcodes, which reﬂect the operational logic
of programs directly from EVM level. Table 1 lists some
frequently-used opcodes and their meanings.

Based on the EVM, developers can deploy smart con-
tracts on the Ethereum platform easily. The process can be
divided into three steps: ﬁrst, use a high-level language like
Solidity to write the smart contract source code; second,
compile the source code into bytecode through EVM; and
ﬁnally, deploy the compiled contract through Ethereum
clients.

Every user of Ethereum can hold an account. An
Ethereum account has a 20-byte address, including four
unique ﬁelds: nonce, balance, contract bytecode (if any), and
storage (usually empty). Only contract accounts have code
ﬁelds which store codeHash (the hash value of the EVM

code for this account). This ﬁeld cannot be modiﬁed after
creation, which means that the smart contract is immutable.
When the contract account receives a message, the contract
is activated. This allows it to read and write to the internal
storage, send messages out, or create a new contract. We use
both bytecode features and account information in training
the smart contract classiﬁer.

2.2 Related Work

There are few studies on the classiﬁcation of smart con-
tracts. Huang et al. [7] have introduced a smart contract
classiﬁcation method based on the word embedding model.
This method captures the semantics of the contract source
code through the LSTM network and obtains word vectors.
Finally, word vectors and account characteristics are input
into the feedforward neural network; the probability distri-
bution of the category labels is output. Gang et al. [9] have
proposed a novel classiﬁcation model called SCC-BiLSTM.
It employs the Gaussian LDA (GLDA) model and attention
mechanism to improve the classiﬁer’s performance. This
model solves the sparse semantic problem of annotations
in the source code, and the attention mechanism is used to
capture vital code features. The experimental results show
that this model achieves superior effectiveness on smart
contract classiﬁcation tasks, but it still relies on open-source
contracts.

Studies have used bytecode or opcode to analyze smart
contracts. Oyente [14] is a symbolic execution tool released
by Melonport to detect potential security vulnerabilities
such as reentrancy, timestamp dependence, and logic errors
in smart contracts. Oyente works directly with the EVM
bytecode and opcodes without access to high-level pro-
gramming languages like Solidity or Python. The research
by Chen et al. [15] has used features extracted from bytecode
to detect Ponzi schemes in Ethereum smart contracts. This
model extracts features from bytecode in manually labeled
contract samples and trains the regression tree model with
the XGBoost [16] algorithm. The most signiﬁcant innovation
is that by using this bytecode-based model Ponzi schemes
can be detected once contracts are created. Chen et al. [17]
conducted an investigation on Ethereum through graph
analysis. They collected all transaction data by customizing
Ethereum client using opcodes. Barati et al. [18] show that
some data privacy rules can be translated into smart con-
tracts and appear as opcodes to verify the way providers
operate user data automatically.

Unlike source code-based approaches, we use features
from contract bytecode to train the classiﬁcation model.
Since bytecode is immutable and is open to access, the
bytecode-based classiﬁer is universal to all contracts no mat-
ter they are open-source or not. This is the main difference
between our approach and NLP-based approaches.

3 PROPOSED METHODOLOGY
3.1 Framework

The overall framework is illustrated as Fig 2. We ﬁrst col-
lected veriﬁed smart contracts by crawling from Ethereum
explore (etherscan.io and stateofthedapps.com), including the
contract bytecode and related account information. The

3

second step converts the bytecode into opcodes and extracts
the code features to train the 0-day model, which classiﬁes
contracts once they are uploaded. The third step integrates
the contract behavior features from the transaction history
to train the full-feature model. To solve the problems of
feature redundancy and sample imbalance in the model
training, we also propose an ensemble learning-based multi-
classiﬁcation algorithm with a binary particle swarm opti-
mization (BPSO) method.

3.2 Data

To train and test our model, we collected 11,000 smart
contracts of top 100 Ethereum Dapps ranked by their user
activities (unique source addresses in transactions to DApp
contracts) over the past 30 days, as of May 1, 2020. All smart
contracts are collected from Ethereum explores etherscan.io
and stateofthedapps.com through web crawlers. After deleting
duplicate contracts and contracts which have never been
triggered, 3,381 contracts are left, and 1,501 contracts of
them are open-source. Each contract contains full informa-
tion, including the bytecode and account information. We
also collected all of the transaction histories of these con-
tracts, such as the number of transactions and the amount
of transferred Ether for further feature extraction. The col-
lected contracts are manually divided into six categories:
Governance, Finance, Gambling, Game, Wallet, Social according
to the Dapps to which they belong. The distribution of the
collected contracts is shown in Fig 3. The imbalance ratio of
the samples is 19, which is similar to the current Ethereum
environment; game and gambling contracts appear more
frequently than other categories.

3.3 Feature Extraction

Feature extraction and selection are key upstream capa-
bilities for building a high-performance classiﬁer. Previous
work, e.g., [14], [19] has extracted code features mainly from
bytecode for vulnerability detection and pattern recognition.
In other proposals, e.g.,
[9], [20], account and transaction
information have been selected as account features to char-
acterize contracts. In our work, we build a 0-day model that
is based on code features to classify contracts as early as
day 0. This is possible because the code features are avail-
able immediately and are immutable once the contracts are
uploaded. We integrate code features and account features
to train the full-feature model to improve the classiﬁcation
accuracy for the already deployed contracts.

3.3.1 Code Features

As the main body of a smart contract, the bytecode is
stored as a string of hexadecimal numbers with the contract
account in a Merkle Patricia tree. Unlike source code, byte-
code is transparent and can be easily obtained from every
contract. As mentioned in Section 1, each byte represents a
certain opcode, so we disassemble the bytecode into equiv-
alent opcode with evmdis1 to facilitate the feature extraction.
The opcode features can be directly used in classiﬁcation
without any modiﬁcations because they reﬂect all of the

1. https://github.com/Arachnid/evmdis, accessd May 1, 2020

4

Fig. 2. The framework of the smart contracts classiﬁcation approach.

These features all relate to stack operations. That is because
almost any operation, such as deﬁning variables and func-
tions, involves stack operations on the EVM. We also found
that MSTORE and MLOAD are more frequently used in
game contracts than in others. This outcome is reasonable
because some game data needs to store and load from mem-
ory. Other categories of contracts also have characteristic
feature distributions that reﬂect their unique characteristics.
Thus, we believe code features can be used in contract
classiﬁcation. Although there are some common frequently
appearing opcodes among different categories, we still use
all 61 opcodes as features because they may have hidden
connections with each other and cannot be excluded by a
simple standard.

3.3.2 Account Features

Account features are selected from account attributes and
related transaction history information. These features are
only available after the deployment of contracts and may
change over time, reﬂecting how contracts work in a real en-
vironment. Thus, we can extract these features from already
deployed contracts and combine them with code features to
train a full-feature model that classiﬁes deployed contracts.
Previous research [9], [19], [20] provides a variety of
account features. From these, we select features to model
smart contracts as follows:

• Balance: the balance of contract account, measured

by wei.

• Nonce: the nonce records the sequence of contract

creation.

• Nbr trans act and Nbr trans psv: the number of ac-
tive and passive transactions involving the contract.

Fig. 3. The distribution of collected smart contracts.

logical behaviors of contracts [21] from the perspective of
the EVM.

After disassembling bytecodes into opcodes, the fre-
quency of each kind of opcode is calculated and regarded as
a feature. Please note that for some opcodes with the same
functions, we merge them into one category. For example,
both DUP1 and DUP2 are considered as DUP; both PUSH1
and PUSH2 are considered as PUSH. Finally, we ﬁnd 61
different kinds of opcodes from all 1,501 contracts, which
means the dimension of the code feature is 62, including the
size of the bytecode. Table 2 shows the top 10 code features
(except size feature) ranked by their average values in three
categories.

According to Table 2, the distributions of code features
are different among the three categories. The most fre-
quently used features are PUSH, DUP, SWAP, and JUMP.

Etherscan.io & Stateofthedapps.comBalanceNonceNumber of Transactions...BytecodeOpcodeAccount FeaturesCode FeaturesDisassemble   0-day          ModelFull Feature ModelStep 1Step 2Step 35

TABLE 2
Top 10 code features ranked by average values.

Rank

1
2
3
4
5
6
7
8
9
10

Game contracts

Social contracts

Financial contracts

Feature
PUSH
DUP
SWAP
JUMP
POP
RETURN
MLOAD
CALL
MSTORE
ADD

Value (avg)
134.18
97.44
89.15
53.24
39.23
11.21
4.35
2.31
0.89
0.72

Feature
PUSH
JUMP
DUP
MSTORE
SSTORE
SWAP
CALL
POP
AND
ISZERO

Value (avg)
84.13
52.12
45.32
32.87
31.25
7.62
3.38
1.09
0.43
0.31

Feature
PUSH
JUMP
SWAP
RETURN
DUP
MUL
MSTORE
SUB
STOP
CREATE

Value (avg)
57.36
50.21
18.35
14.23
9.09
4.23
2.21
0.77
0.34
0.15

• Eth in and Eth out: the total amount of income and

output Ether of the contract.

• Eth avg and Eth sdev: the mean and standard devi-

ation of the Ether transferred by the contract.

• Lifetime: the time gap between the initial and the

last transaction.

• Trs gap avg and Trs gap sdev: the average and
standard deviation of the time gap between every
two transactions.

• Nbr addr: the total number of addresses the contract

interacted with.

complexity of BPSO is much lower since it does not con-
tain crossover and mutation operations. In this paper, we
choose BPSO as the feature selection method. Our method
follows the original BPSO algorithm and only changes the
particle representation and ﬁtness values. We encode binary
particles as a multi-dimensional vector with values [1, 0] and
each bit of the vector represents a feature which is selected
(value 1) or not (value 0). The ﬁtness value of a particle
is usually the classiﬁcation accuracy of the sample subset
indicated by the particle. Here, we choose the normalized
AUC area shown as Eq. (6) (decribed in Section 4.1) as the
ﬁtness value instead.

3.4 Feature Selection

According to the statistics from stateofthedapps.com, the dis-
tribution of the smart contract categories on Ethereum is
very uneven. The number of game and gambling contracts
is much larger than other contracts, while wallet and gov-
ernance contracts are rare. Thus, the smart contract classi-
ﬁcation problem can be regarded as a multi-classiﬁcation
problem on an imbalanced data set. Traditional classiﬁcation
algorithms such as Decision Tree, K-Nearest Neighbor, and
Support Vector Machine present challenges in achieving the
desired performance on an imbalanced data set because
of their bias towards the majority class. It may treat the
minority class samples as noise [12], which results in the
poor classiﬁcation performance of the minority class.

To improve the classiﬁcation performance, We integrate
feature selection in our classiﬁcation model. The feature
selection process can eliminate the irrelevant and redun-
dant features to reduce the noise in the sample space [22],
thereby improving the classiﬁcation performance of minor-
ity classes. In addition, feature selection also helps us ﬁnd
critical features and hidden relationships among a massive
number of original features and decreases the time complex-
ity.

When we eliminate the irrelevant features, there is also
a risk to the potential loss of useful information because
the feature selection procedure may alter the original data
distribution [23]. We prefer employing warpper methods
rather than ﬁlter methods such as Mutual Information [24]
or Relief-based algorithms [25] to imbalanced data classiﬁ-
cation, since the correlation between features and targets is
not clear. BPSO [11] is a stochastic evolutionary algorithm
which is widely used for solving optimization problems
in binary space. Compared with other wrapper methods
such as Genetic algorithm, Differential Evolution, etc., the

3.5 Classiﬁcation Model

Ensemble learning [26] is a machine learning method that
uses a speciﬁc rule to combine multiple classiﬁers as a
collection to achieve better predictive performance than
an individual classiﬁer. The idea of ensemble learning is
that even if a weak classiﬁer obtains an incorrect predic-
tion, other classiﬁers can correct it. Adaboost.M1 [27] is a
typical ensemble learning algorithm that has been widely
used to solve multi-classiﬁcation problems because of its
good performance, low complexity, and good resistance to
overﬁtting. Adaboost. M1 creates a simple weak learner for
each feature. Weak learners do not need high accuracy in
the initial stages, as long as their accuracy is higher than
random classiﬁcation. The weight of the correctly classi-
ﬁed samples decreases, and the weight of the incorrectly
classiﬁed samples remains unchanged after each iteration.
Suppose m is the number of samples and Y is the collection
of k categories, Y = {c1, c2, . . . , ck}. Then the classiﬁcation
error rate is

m
(cid:88)

ε =

D(i)[yi (cid:54)= h(xi)],

(1)

i=1

where D(i) is the weight of sample xi, yi ∈ Y , and h is the
weak hypothesis h : xi → Y . Setting a parameter

β =

ε
1 − ε

,

the weight would be updated as

Dt+1(i) = Dt(i)β1−[yi(cid:54)=ht(xi)]

t

,

(2)

(3)

where t is the current number of iterations. In this way,
the distribution of samples becomes more balanced after
each iteration. Finally, we can obtain a strong classiﬁer with

TABLE 3
The notations used in algorithm 1.

Notation
Data
m
Y
T
N
G max
w
c1, c2
H(x)
Sbest

Explanation
the training set;
the size of Data;
the collection of k categories, Y = {c1, c2, . . . , ck};
the number of iterations of Adaboost.M1;
the scale of particles;
the generation limit of BPSO;
the inertia weight of BPSO;
the acceleration factors of BPSO.
the strong hypothesis obtained by ensemble learning;
the optimal subset of features.

a superior predictive performance by combining the weak
learners obtained in iterations. The ﬁnal strong hypothesis
H(x) is

H(x) = arg max

y∈Y

T
(cid:88)
(

t=1

ln(

1
βt

)[y = ht(x)]),

(4)

where T is the total number of iterations.

Adaboost.M1 requires relatively high-performance weak
learners. We choose C4.5 [28] as the algorithm of the weak
learner based on two reasons:

1) There are numerous missing values in our code
features. Thus, C4.5 is suitable for our case as it has
good performance and low sensitivity to missing
values.

2) Although all of the binary classiﬁcation algorithms
can be expanded to multi-classiﬁcation versions via
the OvO or OvA strategy [29], this signiﬁcantly
increases the complexity of the algorithm. So, we
choose C4.5, which can be directly used in multi-
classiﬁcation problems.

We use the BPSO algorithm for feature extraction and
then put the sample subset selected from m samples by
the particles into the Adaboost.M1 algorithm. To compute
the AUC area as Eq. (6), the classiﬁer needs to output
a k-dimensional probability vector for each sample in k-
class classiﬁcation. Values in the probability vector are the
probabilities of a sample belongs to each class. So there are
two probability matrixes W eak score and Strong score
with size m × k belonging to the weak learner and the ﬁnal
strong classiﬁer respectively. Since εt and W eak scoret are
updated in each iteration, the Strong score could be the
average of weighted W eak score:

T
(cid:88)

(

t=1

(cid:80)T

Strong score =

W eak scoret).

1 − εt
t=1 1 − εt
Algorithm 1 presents the pseudocode of the whole clas-
siﬁcation algorithm, including the training and classiﬁcation
process. In additon to H(x), we can also obtain the predic-
tion results and the optimal subset of features Sbest. The
notations used in the algorithm are listed in Table 3.

(5)

4 EXPERIMENTS AND ANALYSIS
4.1 Evaluation Metrics

For imbalanced data, the Receiver Operating Characteristic
(ROC) curve [30] is a well-recognized evaluation metric of

6

Algorithm 1 Framework of the BPSO-Adaboost algorithm.
INPUT: Data with m samples and k categories; T , N ,

G max and c1, c2.

OUTPUT: H(x), Sbest and the classiﬁcation results;
1: Initialize each particle randomly as [11], G = 1;
2: while (Number of generations G < G max (cid:107) BPSO

3:
4:
5:
6:
7:

8:

9:
10:
11:
12:
13:
14:

15:

converged) do

for i = 1 to N do

Select sample subset Datai according to particle i;
Initialize distribution D1(i) ← 1/m;
for t = 1 to T do

Train C4.5 classiﬁer with Dt(i) to obtain weak
hypothesis ht : Datai → Y and W eak scoret;
Compute the classiﬁcation error rate εt as Eq. (1);

Set parameter βt as Eq. (2);
Update Dt+1(i) as Eq. (3);

end for
Obtain H(x) as Eq. (4);
Compute Strong score as Eq. (5);
Set f itness(xi) ← AU C areai, update the ﬁtness
value pbest of each particle and the global ﬁtness
value gbest based on f itness(xi);
Update the position and velocity of particles with
parameter w, c1 and c2;

end for
16:
17: G ← G + 1;
18: end while

classiﬁer performance. ROC curve comes from confusion
matrix as Table 4, taking FPR (F P /(T N + F P )) as X-axis
and TPR (T P /(T P + F N )) as Y-axis. However, the ROC
curve can not quantitatively evaluate the performance of
classiﬁers. Thus the area under the ROC curve (AUC) is
widely used as the evaluation metric. The bigger the AUC
is, the better the classiﬁer performance is.

Traditional AUC values can only be used in binary clas-
siﬁers. For n-class classiﬁcation problems, we can combine
classes in pairs and ﬁnd the AUC of each pair individually.
Finally, there are C 2
n AUC values. We put all AUC values
in a polar coordinate system and calculate the area of the
graph covered by all AUC values as the metric, called
AUC area [31]. The larger AUC area means better classi-
ﬁcation performance. Assuming there are q AUC values
r1, r2, . . . , rq respectively where q = C 2
n, the normalized
AUC area is:

AU C area =

1
q

q−1
(cid:88)
(

i=1

(ri × ri+1) + rq × r1)

(6)

AUC area is sensitive to categories which have poor
AUC values. If there is a poor AUC, the AUC area will also
be poor. Therefore, a classiﬁcation model needs to obtain
high AUC values on all categories to keep a high AUC area.
Compared with average AUC value, AUC area is more
suitable for our case since it has no bias toward the ma-
jority category. In this paper, we use normalized AUC area,
accuracy, and Micro-F1 score as evaluation metrics.

TABLE 4
Confusion matrix of binary classiﬁcation.

Actual Values
Positive Negative

Predictive
Values

Positive
Negative

TP
FN

FP
TN

TABLE 5
BPSO parameters used in our model.

Parameter
N
Gmax
w
c1
c2

Value
30
50
0.73
1.5
1.5

4.2 Experiment Settings

We train and test our 0-day model and full-feature model
(mentioned in section 3.2) with 10-fold cross-validation [32]
on all 3,381 contracts. We specify T = 30 and specify BPSO
related parameters as standard PSO algorithm settings,
given in Table 4.2. To evaluate the effect of feature selec-
tion and ensemble learning in smart contract classiﬁcation,
we compared our algorithm with C4.5, Adaboost.M1 and
BPSO-based C4.5.

We train models with the same data set and test them
on different occasions to compare the performance and
robustness between our approach and state-of-the-art NLP-
based approaches. Please note that even the data sets they
use are the same. The code features they use are from byte-
code and source code, respectively. Thus we train models
on 1,301 veriﬁed contracts that have both source code and
bytecode. We test these models on three different test sets:
veriﬁed contracts (by employing 10-fold cross-validation),
1,880 unveriﬁed contracts, and 1,020 contracts with adver-
sarial source code. Adversarial source code means each
comment, variable name and function name of the source
code is attacked by one of the four operations randomly:
add, drop, swap, and replacement. It is similar to real-world
attack settings.

4.3 Performance Evaluation

4.3.1 Performance comparison between 0-day model and
full-feature model

Table 6 and 7 show the performance of each algorithm on
the 0-day model and full-feature model respectively over
our data set. To compare the classiﬁcation performance
of different algorithms intuitively, Fig. 4 and 5 show the
polar graphs of the AUC values of these algorithms. In
the following ﬁgures, six categories: Governance, Finance,
Gambling, Game, Wallet, Social are numbered from 1 to 6 in
order. The results show that the gap between the two models
are tiny. In fact, 23 code features and only 2 account features,
(Balance and Trs gap avg) were selected in the best feature
subset Sbest of full-feature model. It means account features
have little impact on classiﬁcation performance. That is
reasonable because account features are not stable since they
change over time and can be inﬂuenced by many external
factors. For example, market volatility and policy changes

7

Fig. 4. The AUC area of algorithms on 0-day model

Fig. 5. The AUC area of algorithms on full-feature model

may lead to a substantial increase or decrease in Eth avg
and Eth sdev. Thus account features are not robust enough
to be as key features. In most cases, our 0-day model is
sufﬁcient for both newly uploaded and deployed contracts
because contract bytecode is immutable after uploading.

4.3.2 The Effect of Feature Selection and Ensemble Learn-
ing

According to the experimental results, the relationship be-
tween two evaluation metrics remain consistent: higher
AUC area values have higher accuracies. Overall, the per-
formance of the BPSO-Adaboost algorithm is superior to
the other three algorithms. The Adaboost.M1 algorithm
performs much better than C4.5 over all categories, demon-
strating that ensemble learning improves the performance
of individual weak classiﬁers on the imbalanced data set.
Besides, we found that algorithms with the BPSO algorithm
exhibit better performance those without in terms of both
overall accuracy and the accuracy of minority categories. It
conﬁrms our previous assumption that the BPSO algorithm
can exclude redundant features, which reduces the noise
in minority samples and improves the overall performance

AUC(1, 2)AUC(1, 3)AUC(1, 4)AUC(1, 5)AUC(1, 6)AUC(2, 3)AUC(2, 4)AUC(2, 5)AUC(2, 6)AUC(3, 4)AUC(3, 5)AUC(3, 6)AUC(4, 5)AUC(4, 6)AUC(5, 6)0.20.40.60.8BPSO-AdaboostAdaboostBPSO-C4.5C4.5AUC(1, 2)AUC(1, 3)AUC(1, 4)AUC(1, 5)AUC(1, 6)AUC(2, 3)AUC(2, 4)AUC(2, 5)AUC(2, 6)AUC(3, 4)AUC(3, 5)AUC(3, 6)AUC(4, 5)AUC(4, 6)AUC(5, 6)0.20.40.60.8BPSO-AdaboostAdaboostBPSO-C4.5C4.5TABLE 6
Performance of different algorithms on 0-day model.

Algorithm

AUC area Micro-F1

BPSO-Adaboost
Adaboost
BPSO-C4.5
C4.5

0.923
0.894
0.829
0.797

0.955
0.878
0.911
0.721

Overall
Accuracy
0.932
0.904
0.851
0.780

governance
0.917
0.792
0.788
0.745

Accuracy for each category

Finance Gambling Game Wallet
0.885
0.779
0.795
0.731

0.951
0.946
0.887
0.893

0.923
0.897
0.834
0.803

0.910
0.912
0.864
0.852

TABLE 7
Performance of different algorithms on a full-feature model.

Algorithm

AUC area Micro-F1

BPSO-Adaboost
Adaboost.M1
BPSO-C4.5
C4.5

0.931
0.904
0.845
0.803

0.964
0.893
0.920
0.738

Overall
Accuracy
0.939
0.918
0.877
0.774

governance
0.922
0.799
0.796
0.747

Accuracy for each category

Finance Gambling Game Wallet
0.891
0.792
0.795
0.723

0.960
0.934
0.899
0.897

0.928
0.904
0.857
0.812

0.915
0.911
0.875
0.861

TABLE 8
The performance of the BPSO-Adaboost and SCC-BiLSTM algorithms on different test sets.

8

Social
0.919
0.871
0.828
0.778

Social
0.924
0.841
0.833
0.769

Algorithm

BPSO-Adaboost
SCC-BiLSTM

Veriﬁed Contracts

Unveriﬁed Contracts
AUC area Micro-F1 Accuracy AUC area Micro-F1 Accuracy AUC area Micro-F1 Accuracy
0.938
0.457

Adversarial Examples

0.943
0.957

0.920
0.933

0.916
0.925

0.934
0.489

0.904
0.242

0.912
0.389

0.946
0.638

0.929
0.664

of the model. In conclusion, both feature selection and
ensemble learning have positive contributions to our model;
they play essential roles in classifying samples of minority
categories, which are of particular concern in imbalanced
data.

4.3.3 Robustness Comparison Between Our Approach and
the State-of-the-art NLP-based Approach

Table 8 and Fig. 6 show the performance of our approach
and a representative state-of-the-art NLP-based approach,
the SCC-BiLSTM algorithm [9], under the different test sets
mentioned in Section 5.2. For veriﬁed contracts, the SCC-
BiLSTM algorithm has a slightly better performance than
the proposed BPSO-Adaboost algorithm, but the results are
very close. For unveriﬁed contracts, the BPSO-Adaboost
algorithm retains its high performance; in contrast, the SCC-
BiLSTM algorithm degenerates into a random classiﬁer due
to a lack of code features. The performance of the SCC-
BiLSTM algorithm is also poor on adversarial examples,
but the BPSO-Adaboost algorithm is barely affected. The
reason is that the key features of the NLP-based approach
are mainly semantic features that are not robust at all once
attacked. For our bytecode-based approach, the attack on
the source code also causes bytecode changes. However,
the changes to equivalent opcodes happen only after KEC-
CAK256 [2] because the opcodes of all the variable names,
function names, and comments need to be computed as
keccak-256 hashes. Thus the weights of opcodes after KEC-
CAK256 are signiﬁcantly lower than opcodes that relate
to core operations and have little impact on classiﬁcation.
These results indicate that our bytecode-based approach
has similar performance with state-of-the-art NLP-based
approach, and has much better robustness.

Fig. 6. The AUC area of BPSO-Adaboost and SCC-BiLSTM under
different test sets.

5 CONCLUSION

This paper proposes a novel bytecode-based classiﬁcation
approach designed to effectively classify smart contracts
of blockchain platforms. Considering traditional classiﬁers
have poor performance on imbalanced data sets, we use a
feature selection method to reduce the noise of the samples
as well as an ensemble learning approaach to improve

the overall performance of the classiﬁer. Comparative ex-
periments prove the superiority of each element in our
algorithm. The result of feature selection also reveals why
the full-feature model has little improvement over the 0-
day model. Compared with a state-of-the-art NLP-based ap-
proach, our bytecode-based approach provides good perfor-
mance and offers two key advantages. First, it dramatically
expands the application scenarios for which the classiﬁer
can be used (i.e., bytecode for open-source, non-open-source
contracts). Second, our method can defend against semantic
attacks. These results demonstrate our bytecode-based ap-
proach has better robustness than approaches that depend
on contract source code.

This paper focuses on demonstrating the bytecode-based
model’s advantages in the classiﬁcation of smart contracts.
In the future, we plan to further this study this problem
from three aspects. The ﬁrst is to extend the data set as the
number of smart contracts proliferates. We will continually
improve the model with more ground truth data, including
promote the classiﬁcation accuracy and increase the number
of categories that the model can classify. However, some
smart contracts belong to categories which are hard to
classify clearly, such as contracts used for identiﬁcation [33]
or monitoring [34]. We expect there would be more detailed
deﬁnitions of existing smart contracts. The second is to
expand our smart contract classiﬁcation model to other
blockchain platforms. The potential targets are platforms
with similar virtual machine architecture to EVM, e.g.,
Hyperleger Fabric [35]. Finally, we plan to explore more
derivative functions based on the results of smart contract
classiﬁcation. For example, taking the popularity and gas
efﬁciency of contracts into account to do top-k searching,
or recommending speciﬁc categories of contracts to users
based on their preferences.

REFERENCES

[1] N. Szabo, “Smart contracts: building blocks for digital markets,”
EXTROPY: The Journal of Transhumanist Thought,(16), vol. 18, no. 2,
1996.

[3]
[4]

[2] V. Buterin et al., “A next-generation smart contract and decentral-
ized application platform,” white paper, vol. 3, no. 37, 2014.
(2020, Apr.) Etherscan. [Online]. Available: https://etherscan.io
(2020, Apr.) Bigquery. [Online]. Available: https://cloud.google.
com/bigquery
[5]
(2020, Apr.) Dfuse. [Online]. Available: https://www.dfuse.io
[6] P. Jiang, F. Guo, K. Liang, J. Lai, and Q. Wen, “Searchain:
Blockchain-based private keyword search in decentralized stor-
age,” Future Gener. Comput. Syst., vol. 107, pp. 781–792, 2020.
[7] B. T. Huang, Q. Liu, Q. M. He, Z. G. Liu, and J. H. Chen, “Towards
auto-matic smart-contract codes classiﬁcation by means of word
embedding model and transaction information,” Acta Automatica
Sinica, vol. 43, no. 9, pp. 1532–1543, 2017.

[8] D. Tang, B. Qin, X. Feng, and T. Liu, “Effective lstms for target-
dependent sentiment classiﬁcation,” in COLING 2016, 26th Inter-
national Conference on Computational Linguistics, Proceedings of the
Conference: Technical Papers, December 11-16, 2016, Osaka, Japan,
2016, pp. 3298–3307.

[9] G. Tian, Q. Wang, Y. Zhao, L. Guo, Z. Sun, and L. Lv, “Smart
contract classiﬁcation with a bi-lstm based approach,” IEEE Access,
vol. 8, pp. 43 806–43 816, 2020.

[10] W. E. Zhang, Q. Z. Sheng, A. A. F. Alhazmi, and C. Li, “Adversarial
attacks on deep-learning models in natural language processing:
A survey,” ACM Trans. Intell. Syst. Technol., vol. 11, no. 3, pp. 24:1–
24:41, 2020.

9

[11] J. Kennedy and R. C. Eberhart, “A discrete binary version of the
particle swarm algorithm,” in 1997 IEEE International conference on
systems, man, and cybernetics. Computational cybernetics and simula-
tion, vol. 5, 1997, pp. 4104–4108.

[12] Y. Sun, M. S. Kamel, A. K. C. Wong, and Y. Wang, “Cost-sensitive
boosting for classiﬁcation of imbalanced data,” Pattern Recognit.,
vol. 40, no. 12, pp. 3358–3378, 2007.

[13] G. Wood et al., “Ethereum: A secure decentralised generalised
transaction ledger,” Ethereum project yellow paper, vol. 151, no. 2014,
pp. 1–32, 2014.

[14] L. Luu, D. Chu, H. Olickel, P. Saxena, and A. Hobor, “Making
smart contracts smarter,” in Proceedings of the 2016 ACM SIGSAC
Conference on Computer and Communications Security, 2016, pp. 254–
269.

[15] W. Chen, Z. Zheng,

J. Cui, E. C. H. Ngai, P. Zheng, and
Y. Zhou, “Detecting ponzi schemes on ethereum: Towards health-
ier blockchain technology,” in Proceedings of the 2018 World Wide
Web Conference on World Wide Web, 2018, pp. 1409–1418.

[16] T. Chen and C. Guestrin, “Xgboost: A scalable tree boosting
system,” in Proceedings of the 22nd ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining, 2016, pp. 785–
794.

[17] T. Chen, Y. Zhu, Z. Li, J. Chen, X. Li, X. Luo, X. Lin, and
X. Zhang, “Understanding ethereum via graph analysis,” in 2018
IEEE Conference on Computer Communications, 2018, pp. 1484–1492.
[18] M. Barati and O. Rana, “Tracking GDPR compliance in cloud-
based service delivery,” IEEE Transactions on Services Computing,
06 2020, early access.

[19] T. Pham and S. Lee, “Anomaly detection in bitcoin network using
unsupervised learning methods,” CoRR, vol. abs/1611.03941,
2016. [Online]. Available: http://arxiv.org/abs/1611.03941
[20] M. Bartoletti, B. Pes, and S. Serusi, “Data mining for detecting
bitcoin ponzi schemes,” in Crypto Valley Conference on Blockchain
Technology, 2018, pp. 75–84.

[21] N. Atzei, M. Bartoletti, and T. Cimoli, “A survey of attacks on
ethereum smart contracts (sok),” in Principles of Security and Trust
- 6th International Conference, vol. 10204, 2017, pp. 164–186.

[22] G. Chandrashekar and F. Sahin, “A survey on feature selection
methods,” Comput. Electr. Eng., vol. 40, no. 1, pp. 16–28, 2014.
[23] Z. Sun, Q. Song, X. Zhu, H. Sun, B. Xu, and Y. Zhou, “A novel en-
semble method for classifying imbalanced data,” Pattern Recognit.,
vol. 48, no. 5, pp. 1623–1637, 2015.

[24] I. Guyon and A. Elisseeff, “An introduction to variable and feature

selection,” J. Mach. Learn. Res., vol. 3, pp. 1157–1182, 2003.

[25] R. J. Urbanowicz, M. Meeker, W. G. L. Cava, R. S. Olson, and J. H.
Moore, “Relief-based feature selection: Introduction and review,”
J. Biomed. Informatics, vol. 85, pp. 189–203, 2018.

[26] T. G. Dietterich et al., “Ensemble learning,” The handbook of brain

theory and neural networks, vol. 2, pp. 110–125, 2002.

[27] Y. Freund and R. E. Schapire, “Experiments with a new boosting
algorithm,” in Machine Learning, Proceedings of the Thirteenth Inter-
national Conference (ICML ’96), 1996, pp. 148–156.

[28] J. R. Quinlan, C4.5: Programs for Machine Learning. Morgan

Kaufmann, 1993.

[29] A. Fern´andez, V. L ´opez, M. Galar, M. J. del Jesus, and F. Her-
rera, “Analysing the classiﬁcation of imbalanced data-sets with
multiple classes: Binarization techniques and ad-hoc approaches,”
Knowl. Based Syst., vol. 42, pp. 97–110, 2013.

[30] T. Fawcett, “An introduction to ROC analysis,” Pattern Recognit.

Lett., vol. 27, no. 8, pp. 861–874, 2006.

[31] M. R. Hassan, K. Ramamohanarao, C. K. Karmakar, M. M. Hos-
sain, and J. Bailey, “A novel scalable multi-class ROC for effective
visualization and computation,” in Advances in Knowledge Discov-
ery and Data Mining, 14th Paciﬁc-Asia Conference, PAKDD, vol. 6118,
2010, pp. 107–120.

[32] R. Kohavi, “A study of cross-validation and bootstrap for accuracy
estimation and model selection,” in Proceedings of the Fourteenth
International Joint Conference on Artiﬁcial Intelligence, IJCAI, 1995,
pp. 1137–1145.

[33] Z. Cui, F. XUE, S. Zhang, X. Cai, Y. Cao, W. Zhang, and J. Chen, “A
hybrid blockchain-based identity authentication scheme for multi-
wsn,” IEEE Transactions on Services Computing, vol. 13, no. 2, pp.
241–251, 2020.

[34] M. Taghavi,

J. Bentahar, H. Otrok, and K. Bakhtiyari, “A
blockchain-based model for cloud service quality monitoring,”
IEEE Transactions on Services Computing, vol. 13, no. 2, pp. 276–288,
2020.

[35] E. Androulaki, A. Barger, V. Bortnikov, C. Cachin, K. Christidis,
A. D. Caro, D. Enyeart, C. Ferris, G. Laventman, Y. Manevich,
S. Muralidharan, C. Murthy, B. Nguyen, M. Sethi, G. Singh,
K. Smith, A. Sorniotti, C. Stathakopoulou, M. Vukolic, S. W. Cocco,
and J. Yellick, “Hyperledger fabric: a distributed operating system
for permissioned blockchains,” in Proceedings of the Thirteenth
EuroSys Conference, EuroSys, 2018, pp. 30:1–30:15.

10

