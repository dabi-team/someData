– Extended Version (Preprint) –
Advancing Blockchain-based Federated Learning
through Veriﬁable Off-chain Computations

Jonathan Heiss, Elias Gr¨unewald, Stefan Tai
Information Systems Engineering
TU Berlin
Berlin, Germany
{jh,eg,st}@ise.tu-berlin.de

Nikolas Haimerl
Distributed Systems Group
TU Wien
Wien, Austria
s01452766@student.tuwien.ac.at

Stefan Schulte
Christian Doppler Laboratory
for Blockchain Technologies
for the Internet of Things, TU Hamburg
Hamburg, Germany
stefan.schulte@tuhh.de

2
2
0
2

n
u
J

3
2

]

R
C
.
s
c
[

1
v
1
4
6
1
1
.
6
0
2
2
:
v
i
X
r
a

Abstract—Federated learning may be subject to both global
aggregation attacks and distributed poisoning attacks. Blockchain
technology along with incentive and penalty mechanisms have
been suggested to counter these. In this paper, we explore
veriﬁable off-chain computations (VOC) using zero-knowledge
proofs (ZKP) as an alternative to incentive and penalty mech-
anisms in blockchain-based federated learning. In our solution,
learning nodes, in addition to their computational duties, act
as off-chain provers submitting proofs to attest computational
correctness of parameters that can be veriﬁed on the blockchain.
We demonstrate and evaluate our solution through a health mon-
itoring use case and proof-of-concept implementation leveraging
the ZoKrates language and tools for smart contract-based on-
chain model management. Our research introduces veriﬁability
of correctness of learning processes, thus advancing blockchain-
based federated learning signiﬁcantly through VOC.

Index Terms—federated learning, blockchain, veriﬁable off-

chain computations, zero knowledge proofs, privacy

I. INTRODUCTION

Machine learning is these days seen as an indispensable
feature in many different ﬁelds, for instance smart cities [1] or
smart healthcare [2]. Despite the potential beneﬁts of machine
learning, privacy concerns are very often named as a major
hindrance for its wide-spread adoption. This is especially the
case if personal data needs to be processed [3]. In conventional
settings, machine learning is conducted in a centralized style,
very often making use of cloud-based computational resources
for the training of a machine learning model. This may lead to
a large communication overhead, if data from a large number
of distributed data sources, e.g., in the Internet of Things (IoT),
needs to be provided to the cloud for the learning tasks.

In order to overcome these issues, federated learning offers
a different approach to machine learning. Instead of the
traditional centralized setting, federated learning distributes
training among nodes, which are often located
the model
at the edge of the network and therefore close to the data
sources [4], [5]. Hence, the training data does not have to be
shared with any centralized node. Instead, only the learned
model is shared with some aggregation entity. Apart from the
apparent improvement with regard to data privacy due to the
lack of sharing of raw training data, this also decreases the
communication overhead signiﬁcantly.

Because of its distributed nature, federated learning may
however suffer from malicious contributors, i.e., distributed
learners which provide an inaccurate or even fake local
model to the aggregator [6], [7]. Such distributed poisoning
attacks may decrease the integrity of the resulting machine
learning model severely. In addition, the aggregator may itself
attack the integrity of the resulting model, if the aggregation
process is not done in a transparent way. For instance, the
aggregator could skew the resulting model by weighting some
local models higher than others. This is also known as a
global aggregation attack. Therefore, it is necessary to provide
mechanisms which impede the occurrence or consequences of
attacks both by the decentralized learners and the aggregators.
Several recent studies have proposed to apply blockchain
technologies for this, i.e., to do the model aggregation on-
chain [8]–[13], and to offer incentive and penalty mechanisms
which provide a stimulus to the decentralized learners to
deliver veriﬁed local models [14]–[17]. One common issue
with these solutions are the occurring costs for verifying the
local models: Especially if making use of a public blockchain,
carrying out heavyweight computations in smart contracts
may lead to (very) high gas costs [18], which diminishes the
number of application scenarios for which blockchain-based
federated learning is useful.

In this paper, we propose an alternative system archi-
tecture for trustworthy federated learning, leveraging smart
learning models, and zero-
contracts to aggregate global
knowledge proofs to make the computational correctness of
the local
learning processes veriﬁable on the blockchain.
For this, we model and implement federated learning using
blockchain technology along with veriﬁable off-chain com-
putations (VOC). Our contributions can be summarized as
follows:

• We propose and implement a system architecture for
federated learning that leverages blockchain technology
to provide decentralized, tamper-resistant, and globally
veriﬁable management of global learning parameters and
VOC to enable verifying the computational correctness
of local learning processes on the blockchain.

• We instantiate this architecture in a scenario where sensi-
tive data serve as inputs to the federated learning system
illustrating the feasibility of our approach.

Preprint (2022-06-23) before ﬁnal copy-editing. Extended version of a peer-reviewed paper to appear in the
Proceedings of the 5th IEEE International Conference on Blockchain (Blockchain 2022).

 
 
 
 
 
 
• We evaluate our ﬁndings with regard to learning per-
formance and computational costs. Moreover, we exten-
sively discuss the architecture with regard to alternative
deployments and possible extensions.

The remainder of this paper is structured as follows: First, we
summarize preliminaries on federated learning, blockchains,
and ZKPs in Section II. Afterwards, we introduce our gen-
eral system design (Section III), present a speciﬁc showcase
scenario from the ﬁeld of smart healthcare in Section IV, and
describe the system’s implementation (Section V). We apply
the presented scenario in the evaluation (Section VI), and
discuss the results (Section VII). Finally, we assess the related
work (Section VIII) and conclude our paper (Section IX).

II. BACKGROUND

Our research combines the two previously separated con-

cepts of federated learning and VOC.

A. Federated Learning

Federated learning provides an architectural approach to
apply machine learning models to a distributed setting where
many nodes jointly train a shared global model in an efﬁcient
and privacy-preserving manner [19].

A learning model typically consists of learning parame-
ters, e.g., weights and biases. In Artiﬁcial Neural Networks
(ANNs), the learning parameters are contained by the net-
work’s hidden layers and applied to the input data during
forward propagation to determine the learning output, e.g.,
a prediction class. In ANNs for supervised learning,
this
learning output
is compared to predetermined truth labels
during backpropagation to determine the learning error. Based
on that error, the learning parameters are re-calculated and
then applied to the model’s hidden layers for the next learning
cycle [20].

In federated learning, learning parameters are computed as
local models by distributed learning nodes on local data. Each
participating node provides only its model to an aggregator that
combines all received models into a global model which rep-
resents the collective learning insights of the learning network.
This global model is then locally applied by the learning nodes
in the next training cycle. Thereby, each learning node beneﬁts
from the training executed at other nodes without revealing
potentially conﬁdential inputs [19], [21].

While federated learning has originally been proposed as
centralized architecture [21] where the global model is man-
aged by a single aggregator, decentralized architectures have
also been proposed where the management of the global model
is jointly executed by the learning nodes, e.g.,

[8]–[17].

B. Veriﬁable Off-chain Computations

Blockchain technology enables mutually distrusting stake-
holders to jointly maintain state in a trustless manner, that
means, no single party needs to be trusted, but instead the sys-
tem design itself provides trust guarantees. It does this through
both, a cryptographically secured, append-only data structure
and an incentive-driven [22] or Byzantine [23] consensus

protocol. While the design of blockchains enables tamper-
resistance and public veriﬁability in distributed environments
and, thereby, represents a good ﬁt for managing global pa-
rameters in federated learning settings, it also causes weak
scalability and leaking privacy. One conceptual approach to
mitigate these deﬁciencies are VOC.

In VOC [24], computation that would originally be executed
on-chain is outsourced to an arbitrary off-chain node. In addi-
tion to the computational result, the off-chain node generates
a proof that attests to the computation’s correctness and can
be veriﬁed on the blockchain. Thereby, VOC enables on-
chain veriﬁcation of computations on conﬁdential data without
revealing it on-chain. Furthermore, it mitigates scalability lim-
itations since an arbitrary large computation can be executed
non-redundantly by a scalable off-chain node and only a
ﬁxed-size proof smaller than the computation itself is veriﬁed
as part of the costly consensus protocol. Thereby, extending
blockchain-based federated learning with VOC enables on-
chain veriﬁability of off-chain learning processes.

As enabler for VOC, ZoKrates [18] has been proposed,
a technology that provides a language and toolbox for
zkSNARKs-based VOC. zkSNARKs (zero-Knowledge Suc-
cinct Non-interactive Argument of Knowledge) represent a
speciﬁc zero-knowledge protocol that distinguishes through
non-interactivity and succinct proof sizes. ZoKrates hides
the peculiarities of zkSNARKs and provides a convenient,
developer-friendly means for implementing and deploying
both, the off-chain proving program and the on-chain veriﬁca-
tion smart contract for the Ethereum blockchain [22]. In this
paper, we leverage ZoKrates to implement the local learning
models as a proof-of-concept of our system design.

III. SYSTEM DESIGN

Addressing prevalent security issues in federated learning
systems exposed by the local model training and the global
model management, we propose a system design that strives
towards the following objectives:

• Tamper-resistance: No party can maliciously manipulate
a learning model, i.e., neither the local nor the global
model.

• Global veriﬁability: The computation of the local and the
global model must be veriﬁable by all participating nodes.
To achieve these objectives, our system architecture has two
distinguishing features. First, the global model is stored on
the blockchain and managed by smart contracts. This enables
public veriﬁability and tamper resistance through system guar-
antees of the underlying blockchain.

Second, the local model is implemented as a realization
of VOC as described in Section II-B. This extends public
veriﬁability and tamper resistance towards computations ex-
ecuted off-chain on the local model while preserving the
privacy guarantees of federated learning. We thereby address
the initially stated problem of non-trusted learning nodes
that weaken the global model’s integrity by deliberately or
accidentally corrupting local computations that return false
inputs for the global model updates.

that the updated local models are output of the expected off-
chain computation.

Once a ZKP is veriﬁed, the global model manager can ag-
gregate the local model updates into the global model. Serving
as a single endpoint, the on-chain aggregator receives these
updates from all participating learning nodes and, thereby,
maintains a single source of truth throughout the nodes of
the blockchain network. As a central challenge, the on-chain
aggregator must, thereby, guarantee fairness, i.e., only one
update is applied per node in each cycle, and liveness, i.e.,
a new cycle is initiated eventually. Therefore, the on-chain
aggregator keeps a list of all registered participants which,
for simplicity, we assume to be predetermined. Addressing
fairness,
the learning
nodes only submit one update per cycle at max. To guarantee
liveness, the on-chain aggregator initiates a new cycle after
a predeﬁned period, determined based on the block-based
timestamp. This is done even if it has not yet received an
update from each registered learning node.

the on-chain aggregator checks that

B. Workﬂow

As depicted in Figure 2, the system workﬂow comprises
an initial one-time setup and repeated updating cycles. In
the workﬂow, four parties are involved: the developer who
is exclusively involved in the setup phase, the on-chain ag-
gregator, the zk-trainer, and the data source. For simplicity, in
Figure 2, the on-chain aggregator is represented as a single
component and the node manager is omitted since it only
manages interactions.

Fig. 2: Workﬂow

Setup: During the one-time setup, the developer creates the
zk-trainer as the off-chain prover and the on-chain aggregator
as the on-chain veriﬁer. In this paper, we assume that this VOC
instantiation is realized with ZoKrates and, hence, follows the
setup ﬂow described in [18]: The developer deﬁnes the training
algorithm in the ZoKrates high-level language, compiles it
into an executable constraint system (ECS), and, based on
the ECS, generates the proving-veriﬁcation key pair. The zk-
trainer requires the proving key and the ECS, whereas the on-
chain aggregator holds the veriﬁcation key and implements the
ZKP veriﬁcation logic. The implemented on-chain aggregator
contracts for global model management and ZKP veriﬁcation

Fig. 1: Overview

A. System Overview

The system, as depicted in Figure 1, can be described from
the two federated learning perspectives: the local perspec-
tive comprises components running on individual off-chain
learning nodes and the global perspective considers the joint
management of the global model through all learning nodes
redundantly executing the on-chain aggregator.

1) Off-chain Learning Node: On the off-chain learning
node, the data source creates the data that is input to the local
training process and may contain conﬁdential information.

The node manager handles the interaction between the data
source, zk-trainer, and the on-chain aggregator. For this, it
implements a message broker to accept messages from the
data source which in turn are provided through the middleware
to the zk-trainer. Furthermore, it implements a blockchain
client to handle bi-directional communication with the on-
chain aggregator. The local model updates are transformed
into blockchain transactions and forwarded to the on-chain
aggregator. In turn, the blockchain client reads global model
updates from the on-chain aggregator and provides them to the
zk-trainer through the middleware. The middleware manages
inputs to and outputs of the zk-trainer.

The zk-trainer implements a learning algorithm as a zero-
knowledge computation to train the local model using both,
inputs from the data source and the iteratively updated global
model. In addition to the updated local model, the zk-trainer
produces a ZKP that attests to the computational correctness of
the local training processes. To create the ZKP, the zk-trainer
uses a dedicated proving key that is part of an asymmetric key
pair. ZKPs are veriﬁed using the corresponding veriﬁcation key
which is part of the on-chain aggregator.

2) On-chain Aggregator: The on-chain aggregator is im-
plemented as smart contracts that are redundantly executed by
the participating learning nodes on a blockchain infrastruc-
ture. The on-chain aggregator has two components: one for
verifying ZKPs and one for managing the global model.

The ZKP-veriﬁer uses the veriﬁcation key that is part of
the asymmetric key pair to verify ZKPs attached to local
model updates. Thereby, it validates that the updates have been
computed correctly. Since the local off-chain computation is
the same on all learning nodes, the same proving key is used
for all off-chain proof constructions. Consequently, the on-
chain aggregator can use a single veriﬁcation key to verify

are then deployed to the blockchain. Once deployed, learning
nodes can be registered with their blockchain account address
for participation.

Updating Cycles: At the beginning of a cycle, the zk-trainer
reads the latest global model from the on-chain aggregator and
applies the contained parameters, e.g. weights and biases, to
the local model. It receives the inputs for the training as a data
batch of ﬁxed size and data format from the data source. Based
on the global model parameters and the input data, the zk-
trainer executes the training as a zero-knowledge computation
in two steps. First, it executes the ECS which returns a witness,
i.e., an input-speciﬁc variable assignment of the ECS. Second,
the ZKP is generated using this witness and the proving key.
The resulting ZKP and the updated local model are then sent
to the on-chain aggregator as a blockchain transcation. On
receiving the transaction, the smart contract ﬁrst veriﬁes the
proof with the veriﬁcation key and, if successful, updates the
global model with the received local model parameters.

IV. APPLICATION

The proposed system architecture is applicable to various
federated learning contexts. Without mitigating this generality,
in this section, we introduce an application context where the
guarantee of the model’s integrity is of particular interest and,
hence, the veriﬁability of the local training and the global
model management of paramount importance. Then, we derive
a suitable learning model that ﬁts to the characteristics of the
application context and our proposed system architecture.

A. Use Case

Remote, in-home health tracking through wearables is par-
ticularly relevant if an individual’s health condition restricts
its mobility such that routine examinations at
the doctor
or hospitals cannot be fulﬁlled anymore [25], [26]. In such
contexts, “smart” or learning-based health applications that
monitor an individual’s health condition through wearables
and predict worsening health conditions can help saving lives,
e.g., by informing healthcare services or relatives on time.
Such applications often provide classiﬁcation problems, e.g.,
predicting health states (good, fair, serious, critical) from
measurements collected by wearables.

Federated learning has been applied for in-home health
monitoring to improve the learning models of such smart
health applications through a joint training of a global learning
model by connected households [25], [26]. Since personal
health data is highly conﬁdential and requires special pro-
tection as required by privacy regulations such as the EU’s
GDPR 1 or the United States’ HIPAA 2, federated learning can
help comply with these regulations and keep this data private.
Furthermore, the integrity of the global learning model is of
utmost important since false predictions of the health condition
can have dramatic consequences, e.g., lead to death. Therefore,
guarantees of global veriﬁability and tamper resistance have

high priority in such application contexts. This makes our
system proposal particularly suitable for such a scenario.

If an in-home health monitoring system as proposed in [25]
or [26] is realized with our system architecture, each house-
hold represents a learning node. The data source is represented
by a wearable, e.g., a smartwatch, which is worn by the patient
and continuously generates health data. The smartwatch trans-
mits the data to an in-home workstation which hosts all other
components of the learning node and the on-chain aggregator’s
smart contracts. Therefore, it also provides the networking
interface to other connected household and healthcare services.

B. Learning Model

In order to deﬁne a learning model for the described smart
healthcare use case, we select and design an ANN with respect
to the following aspects:

• Learning problem: The selected use case provides a
classiﬁcation problem where sensor data inputs must
be correctly assigned to a predeﬁned set of prediction
classes. These classes deﬁne the status of the user. For the
learning model design, aspects of the inputs, i.e., sensor
measurements, and the outputs, i.e., prediction classes,
must be taken into consideration.

• Execution environment: The construction of a veriﬁable
ZKP adds a considerable computational overhead to the
local training that depends on the input data size and the
training’s computational complexity [27]. Consequently,
a simple and efﬁcient learning model is desirable.
In this respect, we select a simple feedforward neural net-
work [20] with one hidden layer as depicted in Figure 3.

Fig. 3: Feedforward Neural Network with One Hidden Layer

A feedforward neural network is an ANN where information
always moves in one direction. As deﬁned in the following,
the depicted feedforward neural network consists of three
layers: The input layer which represents a n-sized vector X of
datapoints, a hidden layer which consists of a m-sized vector
Y of biases and a m-n-sized matrix of weights W , and the
output layer which, in this case, represents a single prediction
class.

X =






x1
...
xn




 , B =






b1
...
bm






 , W =




w11
...
wn1

· · · w1m
...
. . .
· · · mnm






1https://gdpr.eu
2https://www.hhs.gov/hipaa/for-professionals/privacy/index.html

The network is iteratively trained through cycles of forward
propagation and backpropagation. Learning insights generated

in one cycle are applied in the next cycle to improve the
weights and biases of the hidden layer.

During forward propagation, datapoints of the input layer
are fed through the hidden layer of the network to determine a
prediction class. On the hidden layer, the inputs are multiplied
with the weights, the biases are added, and the activation
function σ is applied which projects the calculated values to
the set of predetermined prediction classes:

ˆY = σ(XW + B) or

ˆyj = σ(bj +

(cid:88)

i

xiwij)

From the n-sized output vector ˆY , an aggregation function
determines the most suitable class as the output, in our case,
an argmax function which simply selects the highest value.

During backpropagation, the prediction’s quality is evalu-
ated and the neural network’s weights and biases are updated.
Using a loss function, the difference between the prediction
and the truth labels is determined. In our case, we realize the
loss function with the mean squared error:

L =

1
n

n
(cid:88)

(yi − ˆyi)2,

i

∂L
∂ ˆyj

=

1
n

(yj − ˆyj)

Using the loss function’s derivatives and the learning rate α,
the gradient is calculated, and the new weights W (cid:48) and biases
B(cid:48) are determined:

W (cid:48) := W − α

∂L
∂ ˆyj

∂ ˆyj
∂σj

∂σj
∂wij

, B(cid:48) := B − α

∂L
∂ ˆyj

∂ ˆyj
∂σj

∂σj
∂bj

.

The learning rate determines the impact
the gradient
imposes on the update of the current weights and is deﬁned as
a hyperparameter prior to the training. The updated weights
and biases are applied to the hidden layer and leveraged in the
next cycle.

that

Importantly, while we apply the discussed feedforward neu-
ral network in the implementation at hand, arbitrary learning
models could be applied.

V. IMPLEMENTATION

To demonstrate technical feasibility of our proposal, we
implement the presented system as a proof-of-concept (PoC)
for the previously introduced application context and realize
the described feedforward neural network as a ZoKrates-
enabled VOC3).

A. Off-chain Learning Node

In our PoC, the zk-trainer and the node manager represent

the core components of the off-chain learning node.

1) ZK-Trainer: As most distinguishing aspect of our PoC,
the previously described feedforward neural network is imple-
mented using the ZoKrates4 high-level language and executed
as a zkSNARKs-based computation. For this, we leverage
ZoKrates CLI commands with their current default settings,
the Groth16 proving scheme [28] and the alt bn128
i.e.,
(Barreto-Naehrig) curve.

The program takes the current batch of input data, the
learning rate as hyperparameter, and the latest global model
as input arguments. Based on the arguments,
it executes
the matrix calculations on the hidden layer and applies the
argmax function to determine the prediction. The loss function
is implemented as the mean squared error function and its
derivative as described in Section IV-B. The ZoKrates program
is compiled into an ECS as described in Section III-B and,
then, accessible through its main function. It returns ZKP as
well as the new weights and biases representing the updated
local model.

While providing Turing-completeness, the ZoKrates pro-
gramming language comes with some constraints that impact
our implementation of the learning algorithm, noteworthily
with regards to two aspects: First, in ZoKrates only unsigned
integers can be used as data types. Consequently, ﬂoat numbers
contained in the input data are scaled to enable integer-based
operations. Furthermore, for every value, a corresponding
Boolean is passed to the program which indicates whether
the value is positive or negative. Arithmetic operations are
rewritten to account for possible over- and underﬂows and
return the signs of the resulting values.

Second, in ZoKrates, the size and the type of the input
arguments must be determined at compile time. Consequently,
the number of datapoints of the input layer cannot be dy-
namically changed. This also impacts the implementation of
exponential activation functions where exponents are dynam-
ically calculated based on the input data. Hence, in our PoC
implementation, only linear relationships between the features
and truth labels are predicted.

2) Node Manager: The node manager’s message broker is
implemented with RabbitMQ5, a broker that enables simple
communication with low power consumption, suitable for
IoT settings. It supports multiple data sources and, for each,
provides a dedicated message queue. This does not only enable
off-chain learning nodes with multiple data sources, but also
helps to realize the testbed for our experiments described in
Section VI.

Published messages are consumed by the middleware which
is implemented in Python. To construct the input arguments for
the zk-trainer, it obtains the input data from the message broker
and the updated global model from the on-chain aggregator
through the blockchain client. Fixed input parameters, such
as the learning rate, are retrieved from local storage. The
middleware executes instances of the zk-trainer by calling the
ZoKrates CLI: First, the witness is computed based on the

3https://github.com/NikolasHaimerl/Advancing-Blockchain-Based-
Federated-Learning-Through-Veriﬁable-Off-Chain-Computations.git

4https://github.com/Zokrates/ZoKrates
5https://www.rabbitmq.com/

ECS with the compute-witness command which takes
all input arguments. Second, the proof is generated with the
generate-proof command which takes the witness and
the proving key as arguments. The intermediate witness ﬁle
is written to and read from the local ﬁle system. Outputs are
provided to the on-chain aggregator through the blockchain
client.

The blockchain client is also implemented in Python and
relies on the web3.py library6 to communicate with the on-
chain aggregator. It uses the blockchain account address of the
local learning node to transmit the ZKP and the local model
as a blockcain transaction to the on-chain aggregator.

B. On-chain Aggregator

The on-chain aggregator is implemented in Solidity7 and
separates the functionality into two Ethereum [22] smart
contracts: the learning and the veriﬁer contract. Ethereum is
chosen, since it is a very widely used smart contract-based
blockchain that allows for public and private deployment of
smart contracts.

1) Learning Contract: The learning contract provides the
updating mechanism for the global model and, as the common
endpoint for the off-chain learning nodes,
implements the
management of the updating cycles introduced in Section III.
Analogue to the local model, the global model consists
of the weight-vector and the bias-matrix as described in
Section IV-B. The updating mechanism requires the smart
contract to store two versions: One that is only updated at
the end of the cycle and is consumable by off-chain learning
nodes. And a temporary one that is used to aggregate the
updated local models of the participating learning nodes using
a simple moving average function. At the end of an updating
cycle, the temporary model replaces the other.

To enforce that only one update per learning node and cycle
is applied to the global model, the learning contract keeps a
list of all updates made during a cycle by using the respective
account addresses of the learning nodes. Furthermore, it de-
termines the beginning of a new cycle through a time interval
that is set by the constructor and based on the block-based
timestamp.

2) Veriﬁer Contract: The veriﬁer contract is called from
the learning contract before an update is applied to the
global model. It implements the ZKP veriﬁcation logic in
the verifyTx() function and treats the veriﬁcation key as hard-
coded parameter. The contract is automatically generated as a
Solidity smart contract by means of the ZoKrates CLI.

VI. EVALUATION

To evaluate the system, we deploy the PoC implementa-
tion in a test environment and analyze its behavior during
experimentation. Therefore, we ﬁrst select a dataset that ﬁts to
our application context, then describe our experimental setup,
and ﬁnally discuss results obtained from the experiments with
regards to computational and learning performance.

A. Dataset

To represent the described use case, we select a public
dataset that is closely related to the described use case in
that it is built upon sensor data generated from wearables
and provides a classiﬁcation problem. The dataset is available
under the UCI Repository for Daily and Sports Activities Data
Set8 and has been generated by eight subjects (four females
and four males between the ages 20 and 30) who executed
activities for a duration of ﬁve minutes each. To adopt the
dataset to our use case and make it suitable for our proposed
system, we apply two major changes:

First, we reduce the feature space. In total, the dataset
comprises 45 features which originate from measurements
in different spatial directions generated by different sensor
devices attached to different body parts. To adopt the dataset
to our use case where a single smartwatch is used as data
source, we reduce the dataset to measurements of one sensor
device resulting in nine features which determines the size of
the neural network’s input vector.

Second, we reduce the set of prediction classes. The dataset
comprises 19 different activities, i.e., prediction classes. To
reduce complexity and align the data with our use case
and learning model, we combined activities that characterize
through similar measurements, e.g., ascending and descending
stairs, or exercise in a stepper and on a cross trainer. This
results in a total of six prediction classes which determines
the size of the hidden layer’s biases vector and the weight
matrix.

B. Experimental Setup

Aligned with our use case, we spawn eight in-home health
monitoring systems that act as off-chain learning nodes and
are managed by a smart contract-based on-chain aggregator.
The on-chain aggregator runs on a virtual blockchain network
which is instantiated with Ganache9. Each household is rep-
resented by one data source, one ZoKrates-based zk-trainer,
and one blockchain account. To simulate the different data
sources, we implement a Python-based workload generator
that retrieves the dataset described in Section VI-A from the
ﬁlesystem and splits it into eight subsets each representing
a data source. From the subsets, the workload generator pub-
lishes data batches from each data source to the corresponding
queue provided by the message broker.

Given that our node manager implementation is capable
of managing multiple data sources and multiple zk-trainers,
it is not replicated for each household but instead, in our
experimental setup, manages multiple households instances.
As such, it employs eight zk-trainers which run in parallel
during a cycle. The resulting local models are provided to the
on-chain aggregator through the Ganache blockchain client
which provides the models of each household through the
corresponding account address.

6https://github.com/ethereum/web3.py
7https://github.com/ethereum/solidity

8 https://archive.ics.uci.edu/ml/datasets/daily+and+sports+activities#
9https://github.com/trufﬂesuite/ganache

Fig. 4: Memory requirements of compilation and setup with
different batch sizes.

Fig. 5: Memory requirements of computing witnesses and the
proof with different batch sizes.

To simulate this environment and conduct the experiments,
we use a single machine equipped with an AMD Ryzen
3950X@3.5 GHz CPU with 16 cores, memory capacity of
128 GB@3600 MHz (DDR4), and a solid-state-drive with
approximately 3500 MB/s read and write capabilities. The
experiments are executed three times and the results represent
the average of the measurements gathered in each run. In
addition, we show the standard deviations, where meaningful.

C. Computational Costs

To gain insights about the computational performance of
our system, we observe off- and on-chain computations dur-
ing the one-time setup and the updating cycles discussed
in Section III. We measure the execution time [sec] and
memory consumption [Bytes] for off-chain computations and
transaction costs [Gas10] for on-chain computation.

To understand the system behavior for varying workloads,
we conducted the experiments with four different batch sizes
comprising 10/20/30/40 datapoints respectively per cycle (cf.
Table I).

The setup phase comprises two off-chain computations: the
compilation of the ZoKrates program and the key generation
(in Figure 4 referred to as setup following the ZoKrates
command name), and the on-chain aggregator deployment as
on-chain computations. As depicted in Figure 4, execution
time and memory consumption behave rather linearly to the
increasing batch sizes. While during the key generation (setup)
the memory consumption is rather static, it grows over the
execution time during compilation. The latter may be caused
by an increasing memory space allocated for the constraint
system which is built during compilation. The initial one-
time deployment cost of the on-chain aggregator comprises
0.00624 Gas for the ZKP-veriﬁer and 0.0023 Gas for the
global model manager smart contracts.

10Gas is an Ethereum-speciﬁc metric for measuring transaction complexity.

TABLE I: Average execution time and memory consumption
for varying batch sizes.

Batch size [#]
Compile Time [sec]
STD [sec]
Compile Memory [GB]
STD [MB]
Setup Time [sec]
STD [sec]
Setup Memory [GB]
STD [MB]
Witness Time [sec]
STD [sec]
Witness Memory [GB]
STD [MB]
Proof Time [sec]
STD [sec]
Proof Memory [GB]
STD [MB]

10
27.79
0.02
2.26
80.5
651.35
2.00
2.02
23.82
8.90
0.50
0.95
2.45
39.55
0.03
4.39
0.00

20
56.55
0.87
4.62
18.9
1290.40
1.20
4.12
0.00
19.21
1.50
1.88
0.00
78.25
1.08
8.74
0.00

30
93.01
3.06
6.82
18.0
2107.88
7.33
6.68
157.51
27.43
0.50
2.96
0.00
145.50
1.05
13.43
0.00

40
127.46
5.70
9.31
56.3
2711.53
56.87
8.28
114.405
37.76
0.50
3.72
0.00
167.56
4.34
17.45
0.00

During the operational phase, in each cycle the witness
generation and the proof construction are executed as off-chain
computations whereas proof veriﬁcation and the model update
are executed on-chain. As depicted in Figure 5, memory
size and execution time grows rather linearly with increasing
batch sizes. Clearly, proof construction is more expensive
than witness generation. The transaction costs for veriﬁcation
range between approximately 0.0024 Gas (batch size: 10) and
0.0025 Gas (batch size: 40) per cycle. However, compared to
the one-time setup, repeating computations in the operational
phase require considerably less resources, take less time, and
are therefore evaluated as practically feasible.

D. Federated Learning Performance

In addition to the computational performance, insights about
the performance of the applied learning model are essential to
evaluate its applicability. Therefore, we measure the model’s
accuracy and how it behaves with a varying number of
participating off-chain learning nodes (2, 4, 6, 8). Again,
experiments are conducted with the four different batch sizes

020406080100Time [Seconds]02468Memory Usage [Bytes]1e9Compilation05001000150020002500Time [Seconds]0123456781e9SetupBatchsize:10Batchsize:20Batchsize:40Batchsize:300102030Time [Seconds]0.00.51.01.52.02.53.03.5Memory Usage [Bytes]1e9Witness0255075100125150Time [Seconds]0.000.250.500.751.001.251.501.751e10ProofBatchsize:10Batchsize:20Batchsize:40Batchsize:300.8

0.6

0.4

0.2

0.8

0.6

0.4

0.2

e
r
o
c
S
y
c
a
r
u
c
c
A

e
r
o
c
S
y
c
a
r
u
c
c
A

Participants 2

Participants 4

Participants 6

Participants 8

Batch size: 10

Batch size: 20

VII. DISCUSSION

The system’s evaluation underlines its technical feasibility
in an exemplary application context but also reveals limita-
tions. Addressing these, we ﬁrst discuss different deployment
scenarios and then present a system extension for end-to-end
integrity between data source and on-chain aggregator.

Batch size: 30

Batch size: 40

A. Deployment Scenarios

0

50

100

150
Round Number

200

250

300

0

50

100

150
Round Number

200

250

300

Fig. 6: Accuracy scores for different numbers of participants
and various batch sizes.

(10, 20, 30, 40) with a total number of 300 training cycles
each. Different input data batches are also randomly chosen
from the data set to avoid over-ﬁtting of the learning model.
Results are depicted in Figure 6.

As expected, the more datapoints are used per cycle, the
faster the model’s output gains accuracy. While the improve-
ment curves for batch sizes 20, 30 and 40 all have their
inﬂection point within 300 updating rounds, this is not yet
the case for batch size 10. With higher batch sizes, a better
accuracy is reached even more early. Our model converges
towards a 0.79 accuracy score with batch size 40, which is
considered a very satisfactory result with respect to the simple
feed-forward neural network applied in our proof-of-concept
architecture due to the limitations imposed by the ZoKrates
language. With a more sophisticated model – which itself was
deliberately not subject of our optimizations – the accuracy is
expected to be even higher.

Moreover, while all runs seem to converge towards the same
maximum accuracy performance eventually, a discrepancy
is noticeable between the number of participants involved.
Contrary to what was ﬁrst expected, the test runs with only
2 learning nodes started off better than the tests with 4, 6
or 8 participating nodes in all variations of the batch size.
After investigation, this was caused by the diversity of the data
assigned to the nodes. Naturally, some nodes are better suited
to train a global model since their data are by chance closer
to the general median than others. Therefore, we consider this
a minor irregularity in the dataset which would not occur in
real-world use cases with more participants.

Lastly, with more participants, a diminishing performance
improvement can be observed. While more participants may
the
in a slower start of the training improvement,
result
convergence accuracy score is the highest with the most
participants. This is to be expected as the experiment’s data
were sampled from all nodes.

The system’s resource consumption, i.e., for local off-chain
its

training and on-chain model management, can impact
deployability.

Computational overhead added through the ZKP gener-
ation as well as the connectivity-effort added through the
blockchain-based consensus protocol can present an impedi-
ment for the system’s deployment, particularly in a constrained
environment. If the learning problem is non-critical, as in
various non-healthcare use cases, and the resources are limited,
e.g., on mobile devices, the added overhead of our system
may be disproportional. Otherwise, however, if more powerful
hardware is available and the learning problem is critical,
such as in in-home health monitoring use cases, the resource
consumption of our proposed system is bearable and its
guarantees required.

On the blockchain, the per-cycle transaction costs depend
on the total cost of an individual update and the number
of participating learning nodes. Transactional costs, however,
have a different impact on the participants and the system
with regards to the applied blockchain type. A deployment to
permissionless blockchains, such as Ethereum [22] relies on
weaker trust assumptions and enables public accessibility and
veriﬁability beyond the deﬁned set of learning nodes through
third parties. However, given incentive-based consensus proto-
cols, e.g., Proof-of-Work (PoW) or Proof-of-Stake (PoS) that
rely on cryptocurrencies and require monetary transaction fees,
the costs for running the system may quickly become too
expensive. This problem is less severe for a deployment on
a permissioned blockchain, such as Hyperledger Fabric [29].
Here, all nodes are known in advance and can be held
accountable such that the consensus protocol can be realized
more efﬁcient and without monetary incentives. While in this
case operational costs may be lower, open accessibility and
public, third-party veriﬁability are restricted.

B. End-to-end Integrity

While our proposed model protects against attacks on the
local training and the global model management, the off-chain
learning nodes still expose an attack vector through the data
source: If false data is injected, the local model is corrupted
regardless the application of VOC.

To better understand this attack scenario in our proposed
system, we distinguish between two cases: In the ﬁrst case,
the data source is controlled by the learning node as depicted
in Figure 1. By protecting against attacks during local model
training, our trust model assumes the learning node to be
distrusted in that it either accidentally or deliberately corrupts
the local training. However, if the data source reveals another

attack vector, we must adjust the trust assumptions for the
learning nodes towards trustworthy data provisioning.

on stronger assumptions and highly rely on cryptocurrency-
enabled blockchains which restricts their applicability.

In the second case, the data source is controlled by an
external third party. If we assume that this party provides the
data in a trustworthy way, the learning node could still corrupt
the data before feeding it into the local training without being
noticed: Even if the data is signed by the data source, after
the training, the signature cannot be veriﬁed by the on-chain
aggregator without the input data.

While for the former case no practical solution exists that
does not add further trust assumptions, for the latter case end-
to-end integrity can be provided by applying the trustworthy
pre-processing model presented in [30]. Trustworthy pre-
processing enables a smart contract to verify authenticity and
integrity of data that has been signed by a trusted data source
but has been pre-processed by a distrusted intermediary, e.g.,
an oracle [31] or the off-chain learning node. In [30],
it
has been shown that the pre-processing model can, among
others, be realized by applying zkSNARKs. Consequently, it
represents a viable extension to our system to remove trust
assumptions from the learning node and to enable end-to-end-
integrity between the data source and the on-chain aggregator.

VIII. RELATED WORK

In this section, we discuss related work around the two
attack vectors that we mitigate with our system proposal, i.e.,
the local model training and the global model management.

Federated learning with a single, centralized aggregator
reveals a single point of failure and a surface for malicious
model manipulation through a non-trustworthy aggregation
node. Addressing these limitations of a centralized architec-
ture, in various works, more decentralized setups have been
proposed, with blockchains representing one possible techno-
logical enabler [8]–[13]. A blockchain-based setup, however,
also introduces undesirable overheads that can quickly stress
resource capacities in constrained environments. In this regard,
literature provides approaches for more efﬁcient federated
learning implementations as general approaches [8] or simply
by selecting permissioned blockchains that implement more
lightweight consensus protocols by design, e.g., [32].While
our proof-of-concept has not been optimized for storage and
computation, our proposal is blockchain agnostic which al-
lows the developer to decide for a deployment according to
requirements of the use case at hand.

The other attack vector is exposed by the local training
which, especially in distributed systems can leverage for
poisoning attacks by malicious learning nodes. Addressing this
attack in blockchain-based architectures, incentive schemes
have been proposed that reward honestly acting nodes while
penalizing dishonest nodes in turn [14]–[17]. However,
it
cannot be enforced in a decentralized manner without a
cryptocurrency which makes permissioned blockchains in-
applicable. Furthermore, it is assumed that learning nodes
act as homini oeconomici, steadily trying to increase their
utility. Compared to our approach, incentive schemes rely

As another approach,

training and the global
the local
model management can be executed by Trusted Execution
Environments (TEE) which guarantee for computational cor-
rectness and even enable non-revealing processing of conﬁ-
dential data [33]–[37]. TEEs provide hardware-level protection
against external access to data in use, i.e., during runtime, and
allow third parties to verify the authenticity of messages using
hardware-level asymmetric encryption. While TEEs represent
an alternative to ZKPs to realize off-chain computations as de-
scribed in [24], to date, they have not been used in blockchain-
based federated learning setups. Compared with ZKPs, TEEs
have weaker security guarantees and rely on a trusted third
party, i.e., the hardware manufacturer, which is not required
for ZKPs. However, they provide better performance and have
a smaller computational overhead as, for example, shown
in [30].

IX. CONCLUSION
We presented a model and ﬁrst system architecture for
blockchain-based federated learning with veriﬁable off-chain
computations. We instantiated the architecture in an e-health
scenario and evaluated and discussed our implementation and
ﬁndings also regarding learning performance and computa-
tional costs. We can conclude that on-chain veriﬁability of
correctness of off-chain learning processes not only is feasible
in speciﬁc deployments and learning scenarios, but presents an
important if not signiﬁcant direction to federated learning and
privacy-enhanced decentralized applications in general.

In our future work, we want to seize on aspects discussed in
Section VII-B by further investigating mechanisms to decrease
the costs for verifying federated learning results, e.g., by off-
chaining parts of the necessary storage, and to remove yet
existing trust assumptions of the learning nodes, e.g., through
trustworthy pre-processing [30]. In addition, we want to take
a look at approaches to scale the solution to handle larger
problem sizes.

REFERENCES

[1] Z. Ullah, F. Al-Turjman, L. Mostarda, and R. Gagliardi, “Applications of
Artiﬁcial Intelligence and Machine learning in smart cities,” Computer
Communications, vol. 154, pp. 313–323, 2020.

[2] W. Li, Y. Chai, F. Khan, S. R. U. Jan, S. Verma, V. G. Menon,
Kavita, and X. Li, “A Comprehensive Survey on Machine Learning-
Based Big Data Analytics for IoT-Enabled Smart Healthcare System,”
Mobile Networks and Applications, vol. 26, no. 1, pp. 234–252, 2021.
[3] T. S. Brisimi, R. Chen, T. Mela, A. Olshevsky, I. C. Paschalidis,
and W. Shi, “Federated learning of predictive models from federated
electronic health records,” International Journal of Medical Informatics,
vol. 112, pp. 59–67, 2018.

[4] K. Bonawitz, H. Eichner, W. Grieskamp, D. Huba, A. Ingerman,
V. Ivanov, C. Kiddon, J. Konecn´y, S. Mazzocchi, H. B. McMahan,
T. V. Overveldt, D. Petrou, D. Ramage, and J. Roselander, “Towards
federated learning at scale: System design,” CoRR, vol. abs/1902.01046,
2019. [Online]. Available: http://arxiv.org/abs/1902.01046

[5] T. Li, A. K. Sahu, A. Talwalkar, and V. Smith, “Federated learning:
Challenges, methods, and future directions,” IEEE Signal Processing
Magazine, vol. 37, no. 3, pp. 50–60, 2020.

[6] M. Fang, X. Cao, J. Jia, and N. Gong, “Local Model Poisoning Attacks
to Byzantine-Robust Federated Learning,” in 29th USENIX Security
Symposium. USENIX Association, 2020, pp. 1623–1640.

[7] C. Ma, J. Li, M. Ding, H. H. Yang, F. Shu, T. Q. S. Quek, and H. V.
Poort, “On Safeguarding Privacy and Security in the Framework of
Federated Learning,” IEEE Network, vol. 34, pp. 242–248, 2020.
[8] Y. Li, C. Chen, N. Liu, H. Huang, Z. Zheng, and Q. Yan, “A blockchain-
based decentralized learning framework with committee consensus,”
IEEE Network, vol. 35, pp. 234–241, 2020.

[9] Y. Lu, X. Huang, Y. Dai, S. Maharjan, and Y. Zhang, “Blockchain
and learning for privacy-preserved data sharing in industrial IoT,” IEEE
Transactions on Industrial Informatics, vol. 16, no. 6, p. 4177–4186,
2020.

[10] Y. J. Kim and C. S. Hong, “Blockchain-based node-aware dynamic
weighting methods for improving federated learning performance,” in
2019 20th Asia-Paciﬁc Network Operations and Management Sympo-
sium (APNOMS).

IEEE, 2019.

[11] J. Li, Y. Shao, K. Wei, M. Ding, C. Ma, L. Shi, Z. Han, and V. Poor,
“Blockchain assisted decentralized federated learning (BLADE-FL):
Performance analysis and resource allocation,” IEEE Transactions on
Parallel and Distributed Systems, pp. 1–1, 2021.

[12] S. K. Lo, Y. Liu, Q. Lu, C. Wang, X. Xu, H. Paik, and L. Zhu,
“Blockchain-based trustworthy federated learning architecture,” CoRR,
vol. abs/2108.06912, 2021. [Online]. Available: https://arxiv.org/abs/
2108.06912

[13] H. Kim, J. Park, M. Bennis, and S.-L. Kim, “Blockchained on-device
federated learning,” IEEE Communications Letters, vol. 24, pp. 1279–
1283, 2019.

[14] K. Toyoda, J. Zhao, A. Zhang, and P. T. Mathiopoulos, “Blockchain-
enabled federated learning with mechanism design,” IEEE Access, vol. 8,
pp. 219 744–219 756, 12 2020.

[15] J. Weng, J. Weng, J. Zhang, M. Li, Y. Zhang, and W. Luo, “Deepchain:
Auditable and privacy-preserving deep learning with blockchain-based
incentive,” IEEE Transactions on Dependable and Secure Computing,
vol. 18, no. 5, pp. 2438–2455, 2021.

[16] X. Bao, C. Su, Y. Xiong, W. Huang, and Y. Hu, “FLChain: A blockchain
for auditable federated learning with trust and incentive,” in 2019 5th
International Conference on Big Data Computing and Communications
(BIGCOM).

IEEE, 2019, pp. 151–159.

[17] L. Feng, Z. Yang, S. Guo, X. Qiu, W. Li, and P. Yu, “Two-layered
blockchain architecture for federated learning over mobile edge net-
work,” IEEE Network, pp. 1–14, 2021.

[18] J. Eberhardt and S. Tai, “ZoKrates – scalable privacy-preserving off-
chain computations,” 2018 IEEE International Conference on Internet
of Things (iThings) and IEEE Green Computing and Communications
(GreenCom) and IEEE Cyber, Physical and Social Computing (CP-
SCom) and IEEE Smart Data (SmartData), pp. 1084–1091, 2018.
[19] J. Koneˇcn´y, H. B. McMahan, F. X. Yu, P. Richt´arik, A. T.
Suresh, and D. Bacon, “Federated learning: Strategies for improving
communication efﬁciency,” CoRR, vol. abs/1610.05492, 2016. [Online].
Available: http://arxiv.org/abs/1610.05492

[20] G. Bebis and M. Georgiopoulos, “Feed-forward neural networks,” IEEE

Potentials, vol. 13, no. 4, pp. 27–31, 1994.

[21] Q. Yang, Y. Liu, T. Chen, and Y. Tong, “Federated machine learning:
Concept and applications,” ACM Transactions on Intelligent Systems and
Technology, vol. 10, no. 2, pp. 1–19, 2019.

[27] J. Eberhardt, “Scalable and privacy-preserving off-chain computations,”
Doctoral Thesis, Technische Universit¨at Berlin, Berlin, 2021. [Online].
Available: http://dx.doi.org/10.14279/depositonce-11883

[22] G. Wood et al., “Ethereum: A secure decentralised generalised transac-

tion ledger,” Ethereum Project Yellow Paper, 2014.
[23] M. Castro and B. Liskov, “Practical byzantine fault

tolerance and
proactive recovery,” ACM Transactions on Computer Systems, vol. 20,
no. 4, pp. 398–461, 2002.

[24] J. Eberhardt and J. Heiss, “Off-chaining Models and Approaches to
Off-chain Computations,” in 2nd Workshop on Scalable and Resilient
Infrastructures for Distributed Ledgers (SERIAL). ACM, 2018, pp.
7–12.

[25] Q. Wu, X. Chen, Z. Zhou, and J. Zhang, “FedHome: Cloud-Edge based
Personalized Federated Learning for In-Home Health Monitoring,” IEEE
Transactions on Mobile Computing, 2020.

[26] D. Gupta, O. Kayode, S. Bhatt, M. Gupta, and A. Tosun, “Hierarchical
Federated Learning based Anomaly Detection using Digital Twins for
Smart Healthcare,” in 2021 IEEE 7th International Conference on
Collaboration and Internet Computing (CIC).
IEEE, 2021, pp. 16–
25.

[28] J. Groth, “On the Size of Pairing-Based Non-Interactive Arguments,”
in 35th Annual International Conference on Advances in Cryptology
(EUROCRYPT 2016), ser. LNCS, vol. 9666.
Springer, 2016, p.
305–326.

[29] E. Androulaki, A. Barger, V. Bortnikov, C. Cachin, K. Christidis,
A. De Caro, D. Enyeart, C. Ferris, G. Laventman, Y. Manevich,
S. Muralidharan, C. Murthy, B. Nguyen, M. Sethi, G. Singh, K. Smith,
A. Sorniotti, C. Stathakopoulou, M. Vukoli´c, S. W. Cocco, and J. Yellick,
“Hyperledger fabric: A distributed operating system for permissioned
blockchains,” in Thirteenth EuroSys Conference (EuroSys ’18). ACM,
2018.

[30] J. Heiss, A. Busse, and S. Tai, “Trustworthy Pre-Processing of Sensor
Data in Data On-chaining Workﬂows for Blockchain-based IoT Applica-
tions,” in 19th International Conference on Service-Oriented Computing,
ser. LNCS, vol. 13121. Springer, 2021, pp. 627–640.

[31] J. Heiss, J. Eberhardt, and S. Tai, “From Oracles to Trustworthy Data
On-chaining Systems,” in IEEE International Conference on Blockchain.
IEEE, 2019, pp. 496–503.

[32] V. Mothukuri, R. M. Parizi, S. Pouriyeh, A. Dehghantanha, and K.-K. R.
Choo, “FabricFL: Blockchain-in-the-loop federated learning for trusted
decentralized systems,” IEEE Systems Journal, 2021.

[33] O. Ohrimenko, F. Schuster, C. Fournet, A. Mehta, S. Nowozin,
K. Vaswani, and M. Costa, “Oblivious multi-party machine learning
on trusted processors,” in 25th USENIX Security Symposium. USENIX
Association, 2016, pp. 619–636.

[34] N. Hynes, R. Cheng, and D. Song, “Efﬁcient deep learning on
multi-source private data,” CoRR, vol. abs/1807.06689, 2018. [Online].
Available: http://arxiv.org/abs/1807.06689

[35] J. G. Chamani and D. Papadopoulos, “Mitigating leakage in federated
learning with trusted hardware,” CoRR, vol. abs/2011.04948, 2020.
[Online]. Available: https://arxiv.org/abs/2011.04948

[36] Y. Chen, F. Luo, T. Li, T. Xiang, Z. Liu, and J. Li, “A training-integrity
privacy-preserving federated learning scheme with trusted execution
environment,” Information Sciences, vol. 522, pp. 69–79, 2020.
[37] F. Mo, H. Haddadi, K. Katevas, E. Marin, D. Perino, and N. Kourtellis,
“PPFL: Privacy-preserving federated learning with trusted execution
environments,” in 19th Annual International Conference on Mobile
Systems, Applications, and Services (MobiSys). ACM, 2021, pp. 94–
108.

