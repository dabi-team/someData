2
2
0
2

r
p
A
2
1

]

C
D
.
s
c
[

3
v
5
1
6
6
0
.
2
1
1
2
:
v
i
X
r
a

Quick Order Fairness

Christian Cachin1
University of Bern
cachin@inf.unibe.ch

Jovana Mi´ci´c1
University of Bern
jovana.micic@inf.unibe.ch

Nathalie Steinhauer1
University of Bern
nathalie.steinhauer@inf.unibe.ch

Luca Zanolini1
University of Bern
luca.zanolini@inf.unibe.ch

Abstract

Leader-based protocols for consensus, i.e., atomic broadcast, allow some processes to unilaterally
affect the ﬁnal order of transactions. This has become a problem for blockchain networks and decen-
tralized ﬁnance because it facilitates front-running and other attacks. To address this, order fairness
for payload messages has been introduced recently as a new safety property for atomic broadcast
complementing traditional agreement and liveness. We relate order fairness to the standard valid-
ity notions for consensus protocols and highlight some limitations with the existing formalization.
Based on this, we introduce a new differential order fairness property that ﬁxes these issues. We also
present the quick order-fair atomic broadcast protocol that guarantees payload message delivery in
a differentially fair order and is much more efﬁcient than existing order-fair consensus protocols. It
works for asynchronous and for eventually synchronous networks with optimal resilience, tolerating
corruptions of up to one third of the processes. Previous solutions required there to be less than one
fourth of faults. Furthermore, our protocol incurs only quadratic cost, in terms of amortized message
complexity per delivered payload.

Keywords. Consensus, atomic broadcast, decentralized ﬁnance, front-running attacks, differential
order fairness.

1 Introduction

The nascent ﬁeld of decentralized ﬁnance (or simply DeFi) suffers from insider attacks: Malicious miners
in permissionless blockchain networks or Byzantine leaders in permissioned atomic broadcast protocols
have the power of selecting messages that go into the ledger and determining their ﬁnal order. Selﬁsh
participants may also insert their own, fraudulent transactions and thereby extract value from the network
and its innocent users. For instance, a decentralized exchange can be exploited by front-running, where
a genuine message m carrying an exchange transaction is sandwiched between a message mbefore and
a message mafter. If m buys a particular asset, the insider acquires it as well using mbefore and sells it
again with mafter, typically at a higher price. Such front-running and other price-manipulation attacks
represent a serious threat. They are prohibited in traditional ﬁnance systems with centralized oversight
but must be prevented technically in DeFi. Daian et al. [7] have coined the term miner extractable value
(MEV) for the proﬁt that can be gained from such arbitrage opportunities.

The traditional properties of atomic broadcast, often somewhat imprecisely called consensus as well,
guarantee a total order: that all correct parties obtain the same sequence of messages and that any mes-
sage submitted to the network by a client is delivered in a reasonable lapse of time. However, these
properties do not further constrain which order is chosen, and malicious parties in the protocol may
therefore manipulate the order or insert their own messages to their beneﬁt. Kelkar et al. [14] have re-
cently introduced the new safety property of order fairness that addresses this in the Byzantine model.

1Institute of Computer Science, University of Bern, Neubr¨uckstrasse 10, 3012 CH-Bern, Switzerland.

1

 
 
 
 
 
 
Kursawe [15] and Zhang et al. [20] have formalized this problem as well and found different ways to
tackle it, relying on somewhat stronger assumptions.

Intuitively, order fairness aims at ensuring that messages received by “many” parties are scheduled
and delivered earlier than messages received by “few” parties. The Condorcet paradox demonstrates,
however, that such preference votes can easily lead to cycles even if the individual votes of majorities are
not circular. The solution offered through order fairness [14] may therefore output multiple messages
together as a set (or batch), such that there is no order among all messages in the same set. Kelkar
et al. [14] name this property block-order fairness but calling such a set a “block” may easily lead to
confusion with the low-level blocks in mining-based protocols.

In this paper, we investigate order fairness in networks with n processes of which f are faulty, for
asynchronous and eventually synchronous atomic broadcast. This covers the vast majority of relevant
applications, since timed protocols that assume synchronous clocks and permanently bounded message
delays have largely been abandoned in this space.

We ﬁrst revisit the notion of block-order fairness [14]. In our interpretation, this requires that when
n correct processes broadcast two payload messages m and m(cid:48), and γn of them broadcast m before m(cid:48)
for some γ > 1
2 , then m(cid:48) is not delivered by the protocol before m, although both messages may be
output together. This guarantee is difﬁcult to achieve in practice because Kelkar et al. [14] show that for
the relevant values of γ approaching one half, the resilience of any protocol decreases. Tolerating only a
small number of faulty parties seems prohibitive in realistic settings.
More importantly, we show that γ cannot be too close to 1

n−f is necessary for
any protocol. This result follows from establishing a link to the differential validity notion of consensus,
formalized by Fitzi and Garay [10]. Notice that block-order fairness is a relative measure. We are
convinced that a differential notion is better suited to address the problem. We, therefore, overcome this
inherent limitation of relative order fairness by introducing differential order fairness: When the number
of correct processes that broadcast a message m before a message m(cid:48) exceeds the number that broadcast
m(cid:48) before m by more than 2f + κ, for some κ ≥ 0, then the protocol must not deliver m(cid:48) before m (but
they may be delivered together). This notion takes into account existing results on differential validity
for consensus [10]. In particular, when the difference between how many processes prefer one of m and
m(cid:48) over the other is smaller than 2f , then no protocol exists to deliver them in fair order.

2 because γ ≥ 1

2 + f

Last but not least, we introduce a new protocol, called quick order-fair atomic broadcast, that imple-
ments differential order fairness and is much more efﬁcient than the previously existing algorithms. In
particular, it works with optimal resilience n > 3f , requires O(n2) messages to deliver one payload on
average and needs O(n2L+n3λ) bits of communication, with payloads of up to L bits and cryptographic
λ-bit signatures. This holds for any order-fairness parameter κ. For comparison, the asynchronous Ae-
quitas protocol [14] has resilience n > 4f or worse, depending on its order-fairness parameter, and needs
O(n4) messages.

To summarize, the contributions of this paper are as follows:

• It illustrates some limitations that are inherent in the notion of block-order fairness (Section 4.1).

• It introduces differential order fairness as a measure for deﬁning fair order in atomic broadcast

protocols (Section 4.2).

• It presents the quick order-fair atomic broadcast protocol for differentially order-fair Byzantine

atomic broadcast with optimal resilience n > 3f (Section 5).

• It demonstrates that the quick order-fairness protocol has quadratic amortized message complexity,
which is an n2-fold improvement compared to the most efﬁcient previous protocol for the same
task (Section 5.3).

The paper starts with a review of previous work (Section 2) and by describing our system model (Sec-
tion 3).

2

2 Related work

Over the last decades, extensive research efforts have explored the state-machine replication problem. A
large number of papers refer to this problem, but only a few of them consider fairness in the order of
delivered payload messages. In this section, we review the related work on fairness.

Kelkar et al. [14] introduce a new property called transaction order-fairness which prevents adver-
sarial manipulation of the ordering of transactions, i.e., payload messages. They investigate assumptions
needed for achieving this property in a permissioned setting and formulate a new class of consensus pro-
tocols, called Aequitas, that satisfy order fairness. A subsequent paper by Kelkar et al. [12] extends this
approach to a permissionless setting. Recently, Kelkar et al. [13] presented another permissioned Byzan-
tine atomic-broadcast protocol called Themis. It introduces a new technique called deferred ordering,
which overcomes a liveness problem of the Aequitas protocols.

Kursawe [15] and Zhang et al. [20] have independently postulated alternative deﬁnitions of order
fairness, called timed order fairness and ordering linearizability, respectively. Both notions are strictly
weaker than order fairness of transactions, however [12]. Timed order fairness assumes that all processes
have access to synchronized local clocks; it can ensure that if all correct processes saw message m to be
ordered before m(cid:48), then m is scheduled and delivered before m(cid:48). Similarly, ordering linearizability says
that if the highest timestamp provided by any correct process for a message m is lower than the lowest
timestamp provided by any correct process for a message m(cid:48), then m will appear before m(cid:48) in the output
sequence. The implementation of ordering linearizability [20] uses a median computation, which can
easily be manipulated by faulty processes [12].

The Hashgraph [3] consensus protocol also claims to achieve fairness. It uses gossip internally and
all processes build a hash graph reﬂecting all of the gossip events. However, there is no formal deﬁnition
of fairness and the presentation fails to recognize the impossibility of fair message-order resulting from
the Condorcet paradox. Kelkar et al. [14] also show an attack that allows a malicious process to control
the order of the messages delivered by Hashgraph.

A complementary measure to prevent message-reordering attacks relies on threshold cryptogra-
phy [18, 5, 8]: clients encrypt their input (payload) messages under a key shared by the group of pro-
cesses running the atomic broadcast protocol. They initially order the encrypted messages and subse-
quently collaborate for decrypting them. Hence, their contents become known only after the message
order has been decided. For instance, the Helix protocol [2] implements this approach and additionally
exploits in-protocol randomness for two additional goals: to elect the processes running the protocol
from a larger group and to determine which messages among all available ones must be included by a
process when proposing a block. This method provides resistance to censorship but still permits some
order-manipulation attacks.

3 System model and preliminaries

3.1 System model

Processes. We model our system as a set of n processes P = {p1, . . . , pn}, also called parties, that
communicate with each other. Processes interact with each other by exchanging messages reliably in
a network. A protocol for P consists of a collection of programs with instructions for all processes.
Processes are computationally bounded and protocols may use cryptographic primitives, in particular,
digital signature schemes.

Failures.
In our model, we distinguish two types of processes. Processes that follow the protocol as
expected are called correct. Contrary, the processes that deviate from the protocol speciﬁcation or may
crash are called Byzantine.

3

Communication. We assume that there exists a low-level mechanism for sending messages over re-
liable and authenticated point-to-point links between processes.
In our protocol implementation, we
describe this as “sending a message” and “receiving a message”. Additionally, we assume ﬁrst-in ﬁrst-
out (FIFO) ordering for the links. This ensures that messages broadcast by the same correct process are
delivered in the order in which they were sent by a correct recipient.

Timing. This work considers two models, asynchrony and partial synchrony. Together they cover
most scenarios used today in the context of secure distributed computing. In an asynchronous network,
no physical clock is available to any process and the delivery of messages may be delayed arbitrarily.
In such networks, it is only guaranteed that a message sent by a correct process will eventually arrive
at its destination. One can deﬁne asynchronous time based on logical clocks. A partially synchronous
network [9] operates asynchronously until some point in time (not known to the processes), after which
it becomes stable. This means that processing times and message delays are bounded afterwards, but the
maximal delays are not known to the protocol.

3.2 Byzantine FIFO Consistent Broadcast Channel

We are using a Byzantine FIFO consistent broadcast channel (BCCH) that allows the processes to deliver
multiple payloads and ensures a notion of consistency despite Byzantine senders. The interface of such
a channel provides two events involving payloads from a domain M:

• A process invokes bcch-broadcast(m) to broadcast a message m ∈ M to all processes.

• An event bcch-deliver(pj, l, m) delivers a message m ∈ M with label l ∈ {0, 1}∗ from a pro-

cess pj.

The label that comes with every delivered message is an arbitrary bit string generated by the channel.
Intuitively, the channel ensures that if a message is delivered with some label, then the message itself is
the same at all correct processes that deliver this label.

Deﬁnition 1 (Byzantine FIFO Consistent Broadcast Channel). A Byzantine FIFO consistent broad-
cast channel satisﬁes the following properties:

Validity: If a correct process broadcasts a message m, then every correct process eventually delivers m.

No duplication: For every process pj and label l, every correct process delivers at most one message

with label l and sender pj.

Integrity: If some correct process delivers a message m with sender pj and process pj is correct, then

m was previously broadcast by pj.

Consistency: If some correct process delivers a message m with label l and sender pj, and another

correct process delivers a message m(cid:48) with label l and sender pj, then m = m(cid:48).

FIFO delivery: If a correct process broadcasts some message m before it broadcasts a message m(cid:48), then

no correct process delivers m(cid:48) unless it has already delivered m.

This primitive can be implemented by running, for every sender pi, a sequence of standard consistent
Byzantine broadcast instances [4, Sec. 3.10] such that exactly one instance in each sequence is active
at every moment. Each consistent broadcast instance is identiﬁed by a per-sender sequence number.
When an instance delivers a message, the protocol advances the sequence number and initializes the next
instance. The sequence number serves as the label. Details of this protocol are described by Cachin et
al. [4, Sec. 3.12.2]; notice that their protocol also ensures FIFO delivery, although this is not explicitly
mentioned there.

In addition to the bcch-broadcast and bcch-deliver events, in our protocol we use the following meth-
ods to access the BCCH primitive: bcch-create-proof and bcch-verify-proof. Those methods ensure that

4

missing messages can be transferred in a veriﬁable way, and they are implemented as in the protocol for
veriﬁable consistent broadcast by Cachin et al. [5]. The input of bcch-create-proof is a list of messages
and it outputs a string s that contains a proof along with the list of messages to be sent. A process that
receives a message providing s can input this in bcch-verify-proof to verify the proof contained in s such
that it is impossible to forge a proof for a message that was not bcch-delivered.

Another two methods, bcch-get-length and bcch-get-messages, are used to get the number of sent

payload messages and to extract them.

3.3 Validated Byzantine Consensus

Validated Byzantine consensus [5] deﬁnes an external validity condition. It requires that the consensus
value is legal according to a global, efﬁciently computable predicate P , known to all processes. This
allows the protocol to recognize proposed values that are acceptable to an external application. Note
that it is not required that the decision value was proposed by a correct process, but all processes must
be able to verify the validity. A consensus primitive is accessed through the events vbc-propose(v) and
vbc-decide(v), where v ∈ V has a potentially large domain V and may contain a proof, which allows
processes to verify the validity of v.

Deﬁnition 2 (Validated Byzantine Consensus). A protocol solves validated Byzantine consensus with
validity predicate P if it satisﬁes the following conditions:

Termination: Every correct process eventually decides some value.

Integrity: No correct process decides twice.

Agreement: No two correct processes decide differently.

External validity: Every correct process only decides a value v such that P (v) = TRUE. Moreover, if

all processes are correct and propose v, then no correct process decides a value different from v.

We intend this notion to cover asynchronous protocols, which actually only terminate probabilisti-

cally, as well as eventually synchronous protocols. The difference is not essential to our use of them.

Originally, external validity has been deﬁned for asynchronous multi-valued Byzantine consensus,
which requires randomized implementations [5]. But the property applies equally to consensus protocols
with partial synchrony.

Among the asynchronous protocols, recent work by Abraham et al. [1] improves the expected com-
munication (bit) complexity to O(Ln2) from O(Ln3) in the earlier work [5], where L is the maximal
length of a proposed value.

In Dumbo-MVBA [16] the communication complexity of this primitive is further reduced to O(Ln)
through erasure coding, where the input of each process is split into coded fragments, distributed to every
process, and recovered later.

Byzantine consensus protocols in the partial-synchrony model can easily be enhanced to provide ex-
ternal validity, when each process veriﬁes P for every proposed value. For instance, the single-decision
versions of PBFT [6] and of HotStuff [19] achieve best-case complexities O(Ln2) and O(Ln), respec-
tively; these values increase by a factor of n in the worst case.

3.4 Atomic Broadcast

Atomic broadcast ensures that all processes deliver the same messages and that all messages are output
in the same order. This is equivalent to the processes agreeing on one sequence of messages that they
deliver. Atomic broadcast is also called “total-order broadcast” or simply “consensus” in the context of
blockchains because it is equivalent to running a sequence of consensus instances. Processes may broad-
cast a message m by invoking a-broadcast(m), and the protocol outputs messages through a-deliver(m)
events.

5

Deﬁnition 3 (Atomic Broadcast). A protocol for atomic broadcast satisﬁes the following properties:

Validity: If a correct process a-broadcasts a message m, then every correct process eventually a-delivers m.

No duplication: No message is a-delivered more than once.

Agreement: If a message m is a-delivered by some correct process, then m is eventually a-delivered

by every correct process.

Total order: Let m and m(cid:48) be two messages such that pi and pj are correct processes that a-deliver m

and m(cid:48). If pi a-delivers m before m(cid:48), then pj also a-delivers m before m(cid:48).

4 Revisiting order fairness

In this section, we discuss the challenges of deﬁning order fairness and highlight limitations of order
fairness notions from previous works. We then introduce our reﬁned notion of differential order-fair
atomic broadcast.

4.1 Limitations

Deﬁning a fair order for atomic broadcast in asynchronous networks is not straightforward since the
processes might locally receive messages for broadcasting in different orders. We assume here that
a correct process receives a payload to be broadcast (e.g., from a client) at the same time when it a-
broadcasts it. If a process broadcasts a payload message m before a payload message m(cid:48), according
to its local order, we denote this by m ≺ m(cid:48). Furthermore, we abandon the validity property above
in the context of atomic broadcast with order fairness and assume now that every payload message is
a-broadcast by all correct processes. This corresponds to the implicit assumption made for deploying
order-fair broadcast.

Even if all processes are correct, it can be impossible to deﬁne a fair order among all messages.
This is shown by a result from social science, known as the Condorcet paradox, which states that there
exist situations that lead to non-transitive collective voting preferences even if the individual preferences
are transitive. Kelkar et al. [14] apply this to atomic broadcast and show that delivering messages in a
fair order is not always possible. Their example considers three correct processes p1, p2, and p3 that
receive three payload messages ma, mb, and mc. While p1 receives these payload messages in the order
ma ≺ mb ≺ mc, process p2 receives them as mb ≺ mc ≺ ma and p3 in the order mc ≺ ma ≺ mb.
Obviously, a majority of the processes received ma before mb, mb before mc, but also mc before ma,
leading to a cyclic order. Consequently, a fair order cannot be speciﬁed even with only correct processes.
One way to handle situations with such cycles in the order is presented by Kelkar et al. [14] with
block-order fairness: their protocol delivers a “block” of payload messages at once. Typically, a block
will contain those payloads that are involved in a cyclic order. Their notion requires that if sufﬁciently
many processes receive a payload m before another payload m(cid:48), then no correct process delivers m after
m(cid:48), but they may both appear in the same block. Even though the order among the messages within a
block remains unspeciﬁed, the notion of block-order fairness respects a fair order up to this limit.

Kelkar et al. [14] specify “sufﬁciently many” as a γ-fraction of all processes, where γ represents an
order-fairness parameter such that 1
2 < γ ≤ 1. More precisely, block-order fairness considers a number
of processes η that all receive (and broadcast) two payload messages m and m(cid:48). Block-order fairness for
atomic broadcast requires that whenever there are at least γη processes that receive m before m(cid:48), then
no correct process delivers m after m(cid:48) (but they may deliver m and m(cid:48) in the same block).

Kelkar et al. [14] explicitly count faulty processes for their deﬁnition. Notice that this immediately
leads to problems: If γη < 2f , for instance, the notion relies on a majority of faulty processes, but no
guarantees are possible in this case. Therefore, we only count on events occurring at correct processes
here and deﬁne a block-order fairness parameter γ to denote the fraction of correct processes that receive
one message before the other.

6

Moreover, we assume w.l.o.g. that all correct processes eventually broadcast every payload, even if
this is initially input by a single process only. This simpliﬁes the treatment compared to original block-
order fairness, which considers only processes that broadcast both payload messages, m and m(cid:48) [14].
Our simpliﬁcation means that a correct process that has received only one payload will receive the other
payload as well later. This process should eventually include also the second payload for establishing
a fair order. It corresponds to how atomic broadcast is used in practice; hence, we set η = n − f .
In asynchronous networks, furthermore, one has to respect f additional correct processes that may be
delayed. Their absence reduces the strength of the formal notion of block-order fairness in asynchronous
networks even more.

In the following, we discuss the range of achievable values for γ. Since we focus on models that
allow asynchrony, we assume n > 3f throughout this work. Fundamental results on validity notions for
Byzantine consensus in asynchronous networks have been obtained by Fitzi and Garay [10]. Recall that
a consensus protocol satisﬁes termination, integrity, and agreement according to Deﬁnition 2. Standard
consensus additionally satisﬁes:

Validity: If all correct processes propose v, then all correct processes decide v.

Notice that this leaves the decision value completely open if only one correct process proposes something
different. In their notion of strong consensus, however, the values proposed by correct processes must be
better respected, under more circumstances:

Strong validity: If a correct process decides v, then some correct process has proposed v.

Unfortunately, strong consensus is not suitable for practical purposes because Fitzi and Garay [10,
Thm. 8] also show that if the proposal values are taken from a domain V, then the resilience depends on
|V|. In particular, strong consensus is only possible if n > |V|f .

Related to this, they also introduce δ-differential consensus, which respects how many times a value
is proposed by the correct processes. This notion ensures, in short, that the decision value has been
proposed by “sufﬁciently many” correct processes compared to how many processes proposed some
different value. More precisely, for an execution of consensus and any value v ∈ V, let c(v) denote the
number of correct processes that propose v:

δ-differential validity: If a correct process decides v, then every other value w proposed by some cor-

rect process satisﬁes c(w) ≤ c(v) + δ.

To summarize, whereas the standard notion of Byzantine consensus requires that all correct processes
start with the same value in order to decide on one of the correct processes’ input, strong consensus
achieves this in any case. It requires that the decision value has been proposed by some correct pro-
cess. However, it does not connect the decision value to how many correct processes have proposed it.
Consequently, strong consensus may decide a value proposed by just one correct process. Differential
consensus, ﬁnally, makes the initial plurality of the decision value explicit. For δ = 0, in particular, the
decision value must be one of the proposed values that is most common among the correct processes.
More importantly, differential validity can be achieved under the usual assumption that n > 3f .

We now give another characterization of δ-differential validity. For a particular execution of some
(asynchronous) Byzantine consensus protocol, let v∗ be (one of) the value(s) proposed most often by
correct processes, i.e.,

v∗ = arg max

v

c(v).

Lemma 1. A Byzantine consensus protocol satisﬁes δ-differential validity if and only if in every one of
its executions, it never decides a value w with c(w) < c(v∗) − δ.

Proof. Assume ﬁrst that the protocol satisﬁes δ-differential validity and a correct process decides any
value v in the domain. Then every other value w proposed by a correct process satisﬁes c(w) ≤ c(v) + δ.
In particular, this implies c(v∗) ≤ c(v) + δ, which is equivalent to, c(v) ≥ c(v∗) − δ. Hence, the protocol
never decides a value x with c(x) < c(v∗) − δ.

7

To show the reverse direction, suppose x is such that c(x) < c(v∗)−δ and a correct process decides x.
This does not satisfy δ-differential validity because also v∗ has been proposed by a correct process but
c(v∗) > c(x) + δ.

For consensus with a binary domain V = {0, 1}, this means that a consensus protocol satisﬁes δ-
differential validity if and only if in every one of its executions with, say, c(0) > c(1) + δ, every correct
process decides 0.

No asynchronous consensus algorithm for agreeing on the value proposed by a simple majority of
correct processes exists, however. Fitzi and Garay [10, Thm. 11] prove that δ-differential consensus in
asynchronous networks is not possible for δ < 2f :

Theorem 2 ([10]). In an asynchronous network, δ-differential consensus is achievable only if δ ≥ 2f .

The above discussion already hints at issues with achieving fair order in asynchronous systems.
Recall that Kelkar et al. [14] present atomic broadcast protocols with block-order fairness for the asyn-
chronous setting with order-fairness parameter γ (whose deﬁnition includes faulty processes). The cor-
ruption bound is stated as

n >

4f
2γ − 1

.

(1)

For γ = 1, which ensures fairness only in the most clear cases, there are n > 4f processes required. For
values of γ close to 1

2 , the condition becomes prohibitive for practical solutions.
In fact, even when using our interpretation, γ cannot be too close to 1

2 , as the following result
shows. It rules out the existence of γ-block-order-fair atomic broadcast in asynchronous or eventually
synchronous networks for γ < 1

2 + f

n−f .

Theorem 3. In an asynchronous network with n processes and f faults, implementing atomic broadcast
2 + f
with γ-fair block order is not possible unless γ ≥ 1

n−f .

Proof. Towards a contradiction, suppose there is an atomic broadcast protocol ensuring γ-fair block
order with 1
n−f . We will transform this into a differential consensus protocol that violates
Theorem 2.

2 < γ < 1

2 + f

The consensus protocol works like this. All processes initialize the atomic broadcast protocol. Upon
propose(v) with some value v, a process simply a-broadcasts v. When the ﬁrst value v(cid:48) is a-delivered by
atomic broadcast to a process, the process executes decide(v(cid:48)) and terminates.

Consider any execution of this protocol such that all correct processes propose one of two values,
m or m(cid:48). Suppose w.l.o.g. that c(m) = γ(n − f ) and c(m(cid:48)) = (1 − γ)(n − f ), i.e., m is proposed
c(m) times by correct processes and more often than m(cid:48), since γ > 1
2 . It follows that γ(n − f ) correct
processes a-broadcast m before m(cid:48) and (1 − γ)(n − f ) correct processes a-broadcast m(cid:48) before m.

According to the properties of atomic broadcast all correct processes a-deliver the same value ﬁrst
in every execution. Moreover, the atomic broadcast protocol a-delivers m before m(cid:48) by the γ-fair block
order property. This implies that the consensus protocol decides m in every execution and never m(cid:48).
Since no further restrictions are placed on m and on m(cid:48), this consensus protocol actually ensures δ-
differential validity for some δ < c(m) − c(m(cid:48)) by Lemma 1.

However, the c(m) and c(m(cid:48)) satisfy, respectively,

c(m) =

γ(n − f )

<

c(m(cid:48)) = (1 − γ)(n − f ) >

(cid:16)

(cid:16) 1
2 + f

n−f

(cid:17)

1 − 1

2 − f

n−f

(n − f )
(cid:17)

= n+f
2
(n − f ) = n−3f

2

and, therefore, δ < c(m) − c(m(cid:48)) < n+f
only possible when δ ≥ 2f , a contradiction.

2 − n−3f

2 = 2f . But δ-differential asynchronous consensus is

8

4.2 Differential Order-Fairness

The limitations discussed above have an inﬂuence on order fairness. The condition on δ to achieve δ-
differential consensus directly impacts any measure of fairness. It becomes clear that a relative notion
for block-order fairness, deﬁned through a fraction like γ, may not be expressive enough.

We now start to deﬁne our notion of order-fair atomic broadcast; it has almost the same interface
as regular atomic broadcast. The primitive is accessed with of-broadcast(m) for broadcasting a payload
message m and it outputs payload messages through of-deliver(M ) events, where M is a set of payloads
delivered at the same time; M corresponds the block of block-order fairness. We want to count the
number of correct processes that of-broadcast a message m before another message m(cid:48) and introduce a
function

b : M × M → N

for all m and m(cid:48) that were ever of-broadcast by correct processes. The value b(m, m(cid:48)) denotes the
number of correct processes that of-broadcast m before m(cid:48) in an execution. As above we assume w.l.o.g.
that a correct process will of-broadcast m and m(cid:48) eventually and that, therefore, b(m, m(cid:48)) + b(m(cid:48), m) =
n − f .

Can we achieve that if b(m, m(cid:48)) > b(m(cid:48), m), i.e., when there are more correct processes that of-
broadcast message m before m(cid:48) than correct processes that of-broadcast m(cid:48) before m, then no correct
process will of-deliver m(cid:48) before m? Using a reduction from δ-differential consensus, as in the previous
result, we can show that this condition is too weak.

Theorem 4. Consider an atomic broadcast protocol that satisﬁes the following notion of order fairness
for some µ ≥ 0:

Weak differential order fairness: For any m and m(cid:48), if b(m, m(cid:48)) > b(m(cid:48), m) + µ, then no correct

process a-delivers m(cid:48) before m.

Then it must hold µ ≥ 2f .

Proof. Towards a contradiction, suppose there is an atomic broadcast protocol, which ensures that for
all payload messages m and m(cid:48) with b(m, m(cid:48)) > b(m(cid:48), m) + µ and µ ≥ 0, no correct process a-delivers
m(cid:48) before m and that µ < 2f . We will transform this into a differential consensus protocol that violates
Theorem 2.

The consensus protocol works like this. All processes initialize the order-fair atomic broadcast pro-
tocol. Upon propose(v) with some value v, a process simply of-broadcasts v. When the ﬁrst value v(cid:48) is
of-delivered to a process, the process executes decide(v(cid:48)) and terminates.

Consider any execution of this protocol such that all correct processes propose one of two values,
m or m(cid:48). Suppose w.l.o.g. that m is proposed c(m) times by correct processes and more often than m(cid:48),
which is proposed c(m(cid:48)) times, with c(m) + c(m(cid:48)) = n − f and that c(m) > c(m(cid:48)) + µ. It follows that
b(m, m(cid:48)) = c(m) correct processes of-broadcast m before m(cid:48) and b(m(cid:48), m) = c(m(cid:48)) correct processes
of-broadcast m(cid:48) before m, hence, b(m, m(cid:48)) > b(m(cid:48), m) + µ.

According to the properties of atomic broadcast, all processes of-deliver the same value ﬁrst in every
execution. Moreover, the protocol of-delivers m before m(cid:48) because b(m, m(cid:48)) > b(m(cid:48), m) + µ. This
implies that the consensus protocol decides m in every such execution. Since no further restrictions
are placed on m and m(cid:48) and since c(m) − c(m(cid:48)) > µ, this consensus protocol actually implements µ-
differential consensus by Lemma 1. However, achieving µ-differential asynchronous consensus requires
that µ ≥ 2f according to Theorem 2. But µ < 2f by the above assumption. This is a contradiction.

On the basis of this result, we now formulate our notion of κ-differentially order-fair atomic broad-
cast, using a fairness parameter κ ≥ 0 to express the strength of the fairness. Smaller values of κ ensure
stronger fairness in the sense of how large the majority of processes that of-broadcast some m before m(cid:48)
must be to ensure that m will be of-delivered before m(cid:48) and in a fair order.

Recall that throughout this work, we assume that if one correct process of-broadcasts some pay-
load m, then every correct process eventually also of-broadcasts m. For reasons that are discussed later

9

(in the remarks after the protocol description in Section 5.2), we use a weaker formal notion of validity,
which considers executions with only correct processes.

Deﬁnition 4 (κ-Differentially Order-Fair Atomic Broadcast). A protocol for κ-differentially order-
fair atomic broadcast satisﬁes the properties no duplication, agreement and total order of atomic broad-
cast and additionally:

Weak validity: If all processes are correct and of-broadcast a ﬁnite number of messages, then every

correct process eventually of-delivers all of these of-broadcast messages.

κ-differential order fairness: If b(m, m(cid:48)) > b(m(cid:48), m) + 2f + κ, then no correct process of-delivers m(cid:48)

before m.

Compared to the above notion of weak differential order fairness, we have κ = µ − 2f . We show in

the next section how to implement κ-differentially order-fair atomic broadcast.

5 Quick order-fair atomic broadcast protocol

This section presents ﬁrst an overview of our quick order-fair atomic broadcast algorithm in Section 5.1.
A detailed description of the implementation follows in Section 5.2, along with the pseudocode in Algo-
rithm 1–2. Finally, the complexity of the algorithm is discussed in Section 5.3.

5.1 Overview

The protocol concurrently runs a Byzantine FIFO consistent broadcast channel (BCCH) and proceeds in
rounds of consensus. BCCH allows processes to deliver multiple messages consistently. An incoming
of-broadcast event with a payload message m triggers BCCH and bcch-broadcasts m to the network.
Additionally, every process keeps a local vector clock that counts the payloads that have been bcch-
delivered from each sending process. Every process also maintains an array of lists msgs such that
msgs[i] records all bcch-delivered payloads from pi.

When a process bcch-delivers the payload message m, it increments the corresponding vector-clock
entry and appends m to the appropriate list in msgs. As soon as sufﬁciently many new payloads are found
in msgs, a new round starts. Each process signs its vector clock and sends it to all others. The received
vector clocks are collected in a matrix, and once n − f valid vector clocks are recorded, a new validated
Byzantine consensus (VBC) instance is triggered. The process proposes the matrix and the signatures
for consensus, and VBC decides on a common matrix with valid signatures. This matrix deﬁnes a cut,
which is a vector of indices, with one index per process, such that the index for pj determines an entry in
msgs[j] up to which payload messages are considered for creating the fair order in the round. It may be
that the index points to messages that a process pi does not store in msgs[j] because they have not been
bcch-delivered yet. When the process detects such a missing payload, it asks all other processes to send
the missing payload directly and in a veriﬁable way, such that every process will store all payloads up to
the cut in msgs.

Once all processes received the payloads up to the cut, the algorithm starts to build a graph that rep-
resents the dependencies among messages that must be respected for a fair order. This graph resembles
the one used in Aequitas [14], but its semantics and implementation differ. The vertices in the graph here
are all new payload messages deﬁned by the cut and an edge (m, m(cid:48)) indicates that m should at most be
of-delivered before m(cid:48).

The graph results from two steps. In the ﬁrst step, the process creates a vertex for every payload
message that appears in a distinct lists in msgs and it is not yet of-delivered. In the second step, the
algorithm builds a matrix M such that M [m][m(cid:48)] counts how many times m appears before m(cid:48) in msgs
(up to the cut). M [m][m(cid:48)] can be interpreted as votes, counting how many processes want to order
m before m(cid:48). Notice that entries of M exist only for m and m(cid:48) where at least one of M [m][m(cid:48)] and
M [m(cid:48)][m] is non-zero.

10

If the difference between entries M [m][m(cid:48)] and M [m(cid:48)][m] is large enough, then the protocol adds a
directed edge (m, m(cid:48)) to the graph. The edge indicates that m(cid:48) must not be of-delivered before m. More
precisely, assuming that messages m and m(cid:48) have been observed by at least n − f processes, such an
edge is added for all m and m(cid:48) with M [m][m(cid:48)] > M [m(cid:48)][m] − f + κ. The condition is explained through
the following result.

Lemma 5. If b(m, m(cid:48)) > b(m(cid:48), m) + 2f + κ, then M [m][m(cid:48)] > M [m(cid:48)][m] − f + κ.

Proof. At least M [m][m(cid:48)] − f correct processes have of-broadcast m before m(cid:48) because M [m][m(cid:48)] may
include reports about m and m(cid:48) in msgs from up to f incorrect processes. In other words,

b(m, m(cid:48)) ≥ M [m][m(cid:48)] − f ⇐⇒ M [m][m(cid:48)] ≤ b(m, m(cid:48)) + f

At most M [m][m(cid:48)] + 2f correct processes have of-broadcast m before m(cid:48) because M [m][m(cid:48)] may
include reports about m or m(cid:48) in msgs from up to f incorrect processes, and there may be up to 2f
correct processes whose arrays were not considered in this number. That is,

b(m, m(cid:48)) ≤ M [m][m(cid:48)] + 2f ⇐⇒ M [m][m(cid:48)] ≥ b(m, m(cid:48)) − 2f

Suppose b(m, m(cid:48)) > b(m(cid:48), m) + 2f + κ. The above implies

M [m][m(cid:48)] ≥ b(m, m(cid:48)) − 2f

> b(m(cid:48), m) + 2f + κ − 2f
= b(m(cid:48), m) + κ
≥ M [m(cid:48)][m] − f + κ.

Thus, whenever M [m][m(cid:48)] > M [m(cid:48)][m] − f + κ, we need to prevent that the protocol of-delivers m(cid:48)
before m. We do this by adding an edge from m to m(cid:48) to the graph; as shown later, this ensures that m(cid:48)
is not of-delivered before m.

In the discussion so far, we have assumed that the two messages m and m(cid:48) were received by at
least n − f processes. Observe that every process can only contribute with 1 to either M [m][m(cid:48)] or to
M [m(cid:48)][m], but not to both. However, it may occur that only a few processes receive m and m(cid:48) before the
cut, which implies that M [m][m(cid:48)] may be very small, for example. But that count might actually grow
later and take on values up to n − f − M [m(cid:48)][m]. For this reason, we extend the condition derived from
Lemma 5 in the algorithm as follows: if n − f − M [m(cid:48)][m] > M [m(cid:48)][m] − f + κ (which implies that
M [m(cid:48)][m] is small, i.e., M [m(cid:48)][m] < n−κ
2 ), we also add add an edge between m and m(cid:48). In summary,
then, the algorithm adds an edge from m to m(cid:48) whenever

max (cid:8)M [m][m(cid:48)], n − f − M [m(cid:48)][m](cid:9) > M [m(cid:48)][m] − f + κ.

Creating the graph in this manner leads to a directed graph that represents constraints to be respected
by a fair order. Notice that two messages may be connected by edges in both directions when the
difference is small and κ < f , i.e., there may be a cycle (m, m(cid:48)) and (m(cid:48), m). This means that the
difference between the number of processes voting for one or the other order is too small to decide on a
fair order. Longer cycles may also exist. All payload messages with circular dependencies among them
will be of-delivered together as a set. For deriving this information, the algorithm repeatedly detects all
strongly connected components in the graph and collapses them to a vertex. In other words, any two
vertices m and m(cid:48) are merged when there exists a path from m to m(cid:48) and a path from m(cid:48) to m. This
technique also handles cases like those derived from the Condorcet paradox.

Finally, with the help of the collapsed graph, all payload messages deﬁned by the cut are of-delivered
in a fair order: First, all vertices without any incoming edges are selected. Secondly, these vertices are
sorted in a deterministic way and the corresponding payloads are of-delivered one after the other. Then
the processed vertices are removed from the graph and another iteration through the graph starts. As

11

Figure 1. The execution of Example 1, in which three correct processes p1, p2, p3 of-broadcast messages
that form a cycle, which makes it impossible to sort them in a fair order. After ma, and after the protocol
has computed cut c, an unbounded number of additional payloads might follow (see text).

soon as there are no vertices left, i.e., all payload messages are of-delivered, the protocol proceeds to the
next round.

Note that cycles may also extend beyond the cut, as shown by Kelkar et al. [13]. Therefore, the
algorithm holds back payload messages and does not of-deliver them while they may still become part
of a longer cycle. This is ensured by counting how many times a message appears in msgs up to the
cut. In particular, let C[m] count this number for a message m. We require that any message is only
of-delivered when C[m] ≥ n+f −κ
, i.e., after m appears in msgs often enough such that it cannot become
part of a cycle later or already be in a cycle that will grow later, e.g., through payloads that arrive after
the cut.

2

Example 1. Let us consider a system of n = 4 processes, of which three (p1, p2, and p3) are correct and
one (p4) is faulty (f = 1). We ﬁx the order-fairness parameter κ = 0, the notion is trivially satisﬁed for
higher values of κ. Every correct process of-broadcasts three messages ma, mb, and mc, in an order that
forms a Condorcet cycle. The Byzantine process p4 does not of-broadcast. Suppose all messages have
been bcch-delivered in round r to all correct processes, as shown in Figure 1. Then the protocol obtains
the cut c = (cid:2)3 2 1 0(cid:3).

From Algorithm 1-2 (L46), the matrix M and the corresponding graph (L48) are

M =





0 0 0
1 0 1
2 0 0





Notice that arbitrarily many payload messages that are of-broadcast immediately after ma by p1–p3
might follow and arrive only in a future round, after cut c. The protocol cannot know this yet and must
therefore postpone of-delivery of ma. As captured by the condition that C[mb] = 1 < n+f −κ
, no
payload message is of-delivered in this round. The protocol continues with another round r(cid:48) obtaining a
cut c(cid:48), cf. Figure 1. Then the matrix M and the graph become

2

M =





0 2 1
1 0 2
2 1 0





12

of-bc(mb)p1p3p2of-bc(mc)of-bc(ma)of-bc(mc)of-bc(mb)of-bc(mc)of-bc(mb)cc'of-bc(ma)of-bc(ma)p4At this point, the protocol of-delivers {ma, mb, mc} together, from a collapsed vertex, because now

C[mi] = 3 ≥ n+f −κ

2

for i ∈ {a, b, c}.

5.2

Implementation

Algorithm 1–2 shows the quick order-fair atomic broadcast protocol for a process pi. The protocol pro-
ceeds in rounds, maintains a round counter r (L1), and uses a boolean variable inround, which indicates
whether the consensus phase of a round is executing (L2).

Every process maintains two hash maps: msgs (L3) and vc (L4), in which process identiﬁers serve
as keys. Hash map msgs contains ordered lists of bcch-delivered payload from each process in the
system. Variable vc is a vector clock counting how many payload messages were bcch-delivered from
each process.

In each round, a matrix L (L5) and a list Σ (L6) are constructed as inputs for consensus.
Rounds.
The matrix L will consist of vector clocks from the processes and Σ will contain the signatures of the
processes. Additionally, every process maintains a list of integers called cut (L7) that are calculated in
every round. This cut represents an index for every list in msgs to determine the payload to be used
for creating the fair order. Initially, all values are zero. Finally, all of-delivered payload messages are
included in a set delivered (L8), to prevent a repeated delivery in future rounds.

The protocol starts when a client submits a payload message m using an of-broadcast(m) event.
BCCH then broadcasts m to all processes in the network (L11). When m with label l from process pj
is bcch-delivered (L12), the vector clock vc for process pj is incremented. The attached label l is not
used by the algorithm and only serves to deﬁne that all correct processes bcch-deliver the same payload
following Deﬁnition 1. Additionally, payload m is appended to the list msgs[j] using an operation
append (m) (L14).

When the length of pj’s list in msgs exceeds the cut value for pj, new payloads may have arrived
that should be ordered (L15). This tells the protocol to initiate a new round. A new round could also be
triggered later, as described in the remarks at the end of this section.

The ﬁrst step of round r is to set the ﬂag inround. Secondly, the protocol digitally signs the vector
clock vc and obtains a signature σ. The values r, σ, and vc are then sent in a STATUS message to all
processes (L16–L18). When process pi receives a STATUS message from pj, it validates the contained
signature σ(cid:48) using verify(j, vc(cid:48), σ(cid:48)) (L19). An additional security check is made by comparing the locally
stored round number r with the round number r(cid:48) from the message. If both conditions hold, the vector
clock vc(cid:48) is stored as row j in matrix L (L20) and σ(cid:48) is stored in list Σ at index j (L21).

Deﬁning a cut. As soon as pi has received n − f valid STATUS-messages (L22), it invokes consensus
(VBC, L23) for the round through vbc-propose with proposal (L, Σ). The predicate of VBC checks that
a proposal consists of a matrix L and a vector Σ such that for at least n − f values j, the entry Σ[j] is
a valid signature on row j of L. When the VBC protocol subsequently decides, it outputs a common
matrix L(cid:48) of vector clocks and a list Σ(cid:48) of signatures (L25). The process then uses L(cid:48) to calculate the cut,
where cut[j] is the largest value s such that at least f + 1 elements in column j in L(cid:48) are bigger or equal
than s (L28). In other words, cut[j] represents how many payload messages from pj were bcch-delivered
by enough processes. This value is used as index into msgs[j] to determine the payloads that will be
considered for creating the order in this round.

The algorithm then makes sure that all processes will hold at least all those payloads in msgs that
are deﬁned by cut. Each process detects missing payload messages from sender pj from any difference
between vc[j] and cut[j] (L30); if there are any, the process broadcasts a MISSING-message to all others.
When another process receives such a request from pj and already has the requested payloads in msgs, it
extracts them into a variable resend (L34). More precisely, it extracts a proof from the BCCH primitive
with which any other process can verify that the payload from this particular sender is genuine. This

13

is done by invoking bcch-create-proof(resend) (L35); the messages and the proof are then sent in a
RESEND-message to the requesting process pj (L36).

When process pi receives a RESEND-message with a missing payload from pk, it ﬁrst veriﬁes the
provided proof s(cid:48) from the message by invoking a bcch-verify-proof(s(cid:48)) function (L38). If the proof is
valid, pi extracts (L40) the payload messages through bcch-get-messages(s(cid:48)), appends them to msgs[k],
and increments vc[k] accordingly. The process repeats this until msgs contains all payloads included in
the cut.

Ordering messages. At this point, every process stores all payloads msgs that have been bcch-delivered
up to the cut. The remaining operations of the round are deterministic and executed by all processes in-
dependently. The next step is to construct the directed dependency graph G that expresses the constraints
on the fair order of the payload messages. Vertices (V ) in G represent payload messages that may be of-
delivered and edges (E) in G express constraints on the order among these payloads. First, all messages
within the cut that are not yet delivered are added as vertices to the set V (L 42).

Then, for each pair of messages m and m(cid:48) in V , the algorithm constructs M (L46) such that
M [m][m(cid:48)] counts how many times a payload m appears before payload m(cid:48) in the cut.
In the same
loop, the algorithm counts how many times message m appears within the cut and stores this result in ar-
ray C (L47). Finally, all entries M [m][m(cid:48)] and M [m(cid:48)][m] are compared and if condition max{M [m][m(cid:48)], n−
f − M [m(cid:48)][m]} > M [m(cid:48)][m] − f + κ holds, then a directed edge from m to m(cid:48) is added (L48). This
edge indicates that m must not be ordered after m(cid:48), i.e., that m is of-delivered before m(cid:48) or together
with m(cid:48).

Any payloads that cannot be ordered with respect to each other now correspond to strongly connected
components of G. A strongly connected component is a subgraph, which for each pair of vertices m and
m(cid:48) contains a path from m to m(cid:48) and one from m(cid:48) to m. In the next step, a graph H = (W, F ) is
created and all strongly connected components in H are repeatedly collapsed until H contains no more
cycles. This is done by contracting the edges in each connected component and merging all its vertices
(L49–L51).

2

The algorithm further considers all vertices w without incoming edges and which satisfy condition
C[m] ≥ n+f −κ
, checked in function stable(w) (L 60). All such w will be sorted in a deterministic
way (L 53). Notice that w may correspond to a message from M or a recursive set of sets of messages.
Therefore function ﬂatten(w) (L 62) is used to extract payload messages and of-deliver them (L 54). All
of-delivered payload messages are added to delivered (L55 to prevent a repeated processing. Finally, w
is removed from H (L56), and a next pass of extracting vertices with no incoming edge follows. This is
repeated until all vertices have been processed and of-delivered.

The algorithm then initializes L, sets inround to FALSE, increments the round number r, and starts

the next round (L57-L59).

Remarks. The condition for starting a round in L15 only waits until one single payload exists in msgs
that was not considered before. This is necessary for liveness but not very efﬁcient. This number can be
increased such that a new round starts only after K = Θ(n) new payload messages have arrived. Note
that this threshold affects the amortized message and bit complexities that are considered in Section 5.3.
Recall that our model assumes that every correct process of-broadcasts all payload messages. For
simplicity, though, our validity property has been formulated only for executions without faulty pro-
cesses. It could be strengthened so that it holds for all executions, in which the processes do not of-
broadcast an unbounded number of them that form a Condorcet cycle.

The protocol can also be changed to satisfy the even stronger liveness property of Kelkar et al. [13],
which the Themis protocol satisﬁes. To deal with Condorcet cycles of unbounded length, one would
modify the interface of order-fair broadcast so that it additionally outputs of-startblock and of-endblock
events that carry no parameters. Furthermore, of-deliver would only output single payload messages
from M. An output “block” then consists of all payloads that are of-delivered between a of-startblock

14

Algorithm 1 Quick order-fair atomic broadcast (code for pi).

State

1:
2:
3:
4:
5:
6:
7:
8:

9:

r ← 1: current round
inround ← FALSE
msgs ← [ ] : HashMap(cid:2){1, ..., n} → [ ](cid:3): array of ordered lists of bcch-delivered messages
vc ← [ ] : HashMap(cid:2){1, ..., n} → N(cid:3): array of counters for bcch-delivered messages
L ← [0]n×n: matrix of logical timestamps, constructed from n vector clocks
Σ ← [ ]n: list of signatures from STATUS messages
cut ← [0]n: the cut decided for the round
delivered ← ∅: set of delivered messages

Initialization

Byzantine FIFO consistent broadcast channel (bcch)

10: upon of-broadcast(m) do
11:
bcch-broadcast(m)

12: upon bcch-deliver(pj, l, m) do
13:
vc[j] ← vc[j] + 1
14:
msgs[j].append (m)

15: upon exists j such that len(msgs[j]) > cut[j] ∧ ¬inround do
16:
17:
18:

inround ← TRUE
σ ← sign(i, vc)
send message [STATUS, r, vc, σ] to all pj ∈ P

// perhaps waiting longer

19: upon receiving message [STATUS, r(cid:48), vc(cid:48), σ(cid:48)] from pj such that r(cid:48) = r ∧ verify(j, vc(cid:48), σ(cid:48)) do
20:
21:

L[j] ← vc(cid:48)
Σ[j] ← σ(cid:48)

22: upon (cid:12)
23:
24:

(cid:12){pj ∈ P | Σ[j] (cid:54)=⊥}(cid:12)
vbc-propose(cid:0)(L, Σ)(cid:1) for validated Byzantine consensus in round r
Σ ← [ ]n

(cid:12) ≥ n − f do

25: upon vbc-decide(cid:0)(L(cid:48), Σ(cid:48))(cid:1) in round r do
26:
27:
28:

for j ∈ {1, . . . , n} do

// cut[j] is the largest s such that at least f + 1 elements in column j in L(cid:48) are at least s
cut[j] ← max(cid:8)s | {k | (cid:12)
(cid:12) > f }(cid:9)

(cid:12){L(cid:48)[k][j] ≥ s}(cid:12)

// calculate the cut
// for each row in L(cid:48)

29:
30:
31:

for j ∈ {1, . . . , n} do

if vc[j] < cut[j] then

// check for missing messages
// some messages that are bcch-broadcast by pj are missing

send message [MISSING, r, j, vc[j]] to all pk ∈ P

if vc[k] ≥ cut[k] then

32: upon receiving message [MISSING, r(cid:48), k, index] from pj such that r(cid:48) = r do
33:
34:
35:
36:

resend ← msgs[k][index . . . cut[k]]
s ← bcch-create-proof(resend)
send message [RESEND, r, k, s] to pj

// copy messages from pk

// send missing messages to pj

37: upon receiving message [RESEND, r(cid:48), k(cid:48), s(cid:48)] from pj such that r(cid:48) = r ∧ len(msgs[k]) < cut[k] do
38:
39:
40:

vc[k] ← vc[k] + bcch-get-length(s(cid:48))
msgs[k].append(bcch-get-messages(s(cid:48)))

if bcch-verify-proof(s(cid:48)) then

15

47:

48:
49:
50:
51:

52:
53:
54:
55:
56:

57:
58:
59:

(cid:16)(cid:83)

\ delivered

j∈{1,...,n} msgs(cid:2)j(cid:3)(cid:2)1 . . . cut[k](cid:3)(cid:17)

Algorithm 2 Quick order-fair atomic broadcast (code for pi).
41: upon len(msgs[j]) ≥ cut[j] for all j ∈ {1, . . . , n} do
42:
43:
44:
45:
46:

V ←
M ← [ ] : HashMap(cid:2)M × M → N(cid:3)
C ← [ ] : HashMap(cid:2)M → N(cid:3)
for m, m(cid:48) ∈ V do
(cid:12)
(cid:8)j ∈ {1, . . . , n} (cid:12)
(cid:12)
M [m][m(cid:48)] ←
(cid:12)
(cid:12) m ∈ msgs(cid:2)j(cid:3)(cid:2)1 . . . cut[k](cid:3)(cid:9)(cid:12)
(cid:12)
(cid:8)pj
(cid:12)
(cid:12)
(cid:12)
(cid:111)
(cid:12) max(cid:8)M [m][m(cid:48)], n − f − M [m(cid:48)][m](cid:9) > M [m(cid:48)][m] − f + κ
(cid:12)

E ←
H ← (V, E)
while H contains some strongly connected subgraph H = (W , F ) ⊆ H do

(cid:12) m appears before m(cid:48) in msgs(cid:2)j(cid:3)(cid:2)1 . . . cut[k](cid:3)(cid:9)(cid:12)
(cid:12)
(cid:12)

(m, m(cid:48))

C[m] ←

(cid:12)
(cid:12)
(cid:12)

(cid:110)

// counts in how many msgs arrays m occurs before m(cid:48)
// counts how many times m appears in msgs arrays

// add all edges
// (V, E) = G

H ← H/F

// collapse vertices in W into a single vertex ¯w via edge contraction

// H = (W, F )
while ∃w ∈ sort(W ) : indegree(w) = 0 ∧ stable(w) do

of-deliver(ﬂatten(w))
delivered ← delivered ∪ ﬂatten(w)
W ← W \ {w}

L ← [0]n×n
inround ← FALSE
r ← r + 1

// in deterministic order
// w may be a message or a (recursive) set of sets of messages
// keep track of delivered messages

// move to the next round

60: function stable(w)
61:

return (cid:0)w ∈ M ∧ C[w] ≥ n+f −κ

(cid:1) ∨ (cid:86)

2

w(cid:48)∈w:w(cid:48)(cid:54)∈M stable(w(cid:48))

// w may be a message from M or a (recursive) set of sets of messages

62: function ﬂatten(w)
63:

return {m ∈ w | m ∈ M} ∪ (cid:83)

// w may be a message from M or a (recursive) set of sets of messages
w(cid:48)∈w:w(cid:48)(cid:54)∈M ﬂatten(w(cid:48))

16

event and the subsequent of-endblock event. However, long cycles occur very infrequently in realistic
scenarios, as shown by Kelkar et al. [13].

If consensus is not “black box” and treated in a modular way, more efﬁcient variations of this protocol
become possible. In particular, the ordering rounds may be integrated with a leader-based Byzantine
consensus protocol [4]. This implies that multiple leaders in successive consensus rounds (or “epochs”)
may be needed to agree on the cut of one ordering round. The Themis protocol [13] adopts this pattern.
The protocol satisﬁes another natural property, which has not been made explicit before in the lit-
erature, but is achieved by several existing protocols [14, 20, 13], not only by quick order-fair broad-
cast. Consider an execution in which the correct processes of-broadcast messages m1, . . . , ml such that
b(mi, mj) > 2f + κ for i = 1, . . . , l and j = i + 1, . . . , l and there are no further messages of-broadcast
that might include m1, . . . ml in a cycle: Then mi is actually of-delivered before mj. Note that differen-
tial order fairness is a safety condition and would not prevent that m1, . . . , ml are of-delivered jointly in
one set.

5.3 Complexity

In this section, we analyze the complexity of the quick order-fair atomic broadcast protocol. We use two
measures: message complexity and communication (bit) complexity. Moreover, we compare our results
with existing algorithms from the literature.

Message complexity.
If the Byzantine FIFO consistent broadcast channel (BCCH) is implemented
using “echo broadcast” [17], it takes O(n) protocol messages per payload message. Since more than f
processes of-broadcast each payload message and f is proportional to n, the overall message complexity
of BCCH is O(n2). Under high load, batching could be used to reduce the number of messages incurred
by BCCH. In the protocol itself, every process sends O(n) STATUS, MISSING, and RESEND messages,
which also amounts to O(n2) messages.

The cost of validated Byzantine consensus (VBC) depends on the assumptions used for implementing
it. In the asynchronous model, optimal protocols [1, 16] achieve O(n2) messages on average. Assuming
that K new payload messages are delivered in each round, this becomes O( n2
K ) per payload. Choosing
K = Ω(n) reduces the amortized cost of consensus to O(n) messages per payload message. Note that
when using an implementation of VBC with complexity O(n3), as the algorithm of Cachin et al. [5], we
can choose K proportional to n and may again obtain expected amortized message complexity O(n2).
With a partially synchronous consensus protocol according to Section 3.3, VBC uses O(n) messages
in the best case and O(n2) messages in the worst case. The total amortized cost of quick order-fair
atomic broadcast per payload, therefore, is also O(n2) messages in this implementation.

If digital signatures are of length λ and payload messages are at
Communication (bit) complexity.
most L bits, the bit complexity of BCCH for one sender is O(nL + n2λ), and since we assume that
O(n) processes broadcast each message, this becomes O(n2L + n3λ). Optimal asynchronous VBC
protocols [1, 16] have O(nL + n2λ) expected communication cost, for their payload length L. Since the
proposals for VBC are n × n matrices, the bit complexity of this phase is O(n3 + n2λ). Assuming that
K is proportional to n, the amortized bit complexity of VBC per payload is O(n2 + nλ). From this, it
follows that the amortized bit complexity of the algorithm per payload message is O(n2L + n3λ).

Discussion. Table 1 gives an overview of message complexities of algorithms with different notions for
fair payload message ordering. We compare our quick order-fair atomic broadcast with the algorithms
introduced by Kelkar et al. [14] and Zhang et al. [20]. We leave out from the overview the protocol by
Kursawe [15] since it has a completely different approach for solving fair payload message ordering.

The asynchronous Aequitas protocol [14, Sec. 7] provides fair order using a FIFO Broadcast prim-
itive, implemented by OARcast of Ho et al. [11]. The implementation of OARcast described there uses
n ARcasts [11] for each payload, where one ARcast causes Θ(n2) network messages. Since Aequitas

17

requires that every correct process broadcasts each payload, the total complexity increases by another
factor of n. Thus, each payload message incurs a cost of Θ(n4) messages in the gossip phase. Moreover,
one instance of set agreement is executed for each payload, and each one of them calls n binary consen-
sus protocols. Therefore Aequitas uses Ω(n4) messages for delivering one payload, which exceeds the
cost of quick order-fair broadcast at least by the factor n2.

Ordering linearizability [20] is deﬁned using a logical order of events observed on each process. Its
implementation in the Pomp¯e protocol, however, appears to require synchronized clocks in the sense
of knowing bounds on differences between local clocks. Hence, the complexity of Pomp¯e cannot be
compared to that of asynchronous protocols for order fairness. Irrespective of this difference, its cost
is O(n2) messages and one instance of Byzantine consensus per payload message. The communication
complexity of this protocol is O(n3L) since each process broadcasts a SEQUENCE-message to all others
with contents of length O(nL).

Themis [13] relies strongly on a leader p(cid:96) to construct a fair order. If p(cid:96) does not perform its task
timely, the protocol may switch to another leader, similarly to existing leader-based protocols. For as-
sessing the complexity of Themis here, we consider the optimistic case, but note that the complexities
stated for some other protocols, in particular for the quick order-fair broadcast, do not depend on timely
leaders.

Themis lets all processes send their local orderings to p(cid:96) ﬁrst. Suppose these consist of approximately
K = Θ(n) payload messages each. Then p(cid:96) constructs a graph G on these and sends G and some
justiﬁcation information to all processes. They maintain local graphs, update them in response, and
potentially output some payload messages. This incurs a cost of O(n) messages. Since G contains K
nodes and, in general, O(K2) edges, the average communication complexity is O(n2 + nL) in the best
case.

Notion
Block-Order-Fairness [14]
Ordering Linearizability [20] Pomp¯e∗ [20]
Themis [13]
Block-Order-Fairness [13]
Quick o.-f. broadcast
Differential Order Fairness

Algorithm
Async. Aequitas [14]

Avg. messages Avg. communication

O(n4)
O(n2)
O(n)
O(n2)

O(n4L)
O(n3L)
O(n2 + nL)
O(n2L + n3λ)

Table 1. Overview of different notions for fair message ordering and corresponding algorithms, with
their expected message and communication complexities. The summary assumes L ≥ λ. (∗ The Pomp¯e
protocol requires synchronized clocks.)

6 Analysis

In this section, we show that the quick order-fair atomic broadcast protocol in Algorithm 1–2 imple-
ments κ-differentially order-fair atomic broadcast. The properties to be satisﬁed are (Deﬁnition 4): no
duplication, agreement, total order, strong validity and κ-differential order fairness.

Lemma 6. No message is of-delivered more than once in Algorithm 1–2.

Proof. The check in L42 of the protocol implementation ensures that no payload message is of-delivered
more than once. In the step when the protocol creates graph vertices, payload messages that are already
contained in variable delivered are ﬁltered out. Those messages will not be included in the graph and
cannot be of-delivered again. Note that even in the case when a payload message m is bcch-delivered
multiple times, because of ﬁltering in L42, it is not possible that m is of-delivered more than once.

Lemma 7. In Algorithm 1–2, if a message m is of-delivered by some correct process, then m is eventually
of-delivered by every correct process.

18

Proof. Suppose that a payload message m is of-delivered by some correct process pi in round r. Follow-
ing the protocol steps, in round r all correct processes decide on the same L(cid:48) (L25). This is guaranteed
by the agreement property of the validated Byzantine consensus because no two correct processes de-
cide differently. Since the matrix L(cid:48) is used to construct the cut deterministically, all correct processes
construct the same cut (L28).

We can then distinguish two cases: In the ﬁrst case, all correct processes have already bcch-delivered
m and store it in msgs. In the second case, there are some correct processes that have never heard of m
simply because of some delays in the network. Then these correct processes send a MISSING-message
to all processes, requesting the delivery of their missing payloads. Since every message included in the
cut was announced by f + 1 processes, and therefore also by at least one correct process, some process
will respond with a RESEND-message containing m. Once all these messages are delivered, all correct
processes store m in msgs.

In the next step, every correct process builds graph G. Each vertex in the graph is constructed
deterministically from the same information by every process, concretely, from the payload messages
in msgs and excluding those that are already in the delivered set (L42). Following the protocol, every
correct process will eventually construct the same G and output the same sequence of payload messages,
also including m.

Lemma 8. Let m and m(cid:48) be two messages such that pi and pj are correct processes that of-deliver m
and m(cid:48). In Algorithm 1–2, if pi of-delivers m before m(cid:48), then pj also of-delivers m before m(cid:48).

Proof. Consider two distinct payload messages m and m(cid:48) and let pi and pj be any two correct processes
that of-deliver both messages. Assume that pi of-delivers m before m(cid:48). If pi of-delivers m and m(cid:48) in
round r, then both messages were included in the cut for pi. Due to the argument used to establish the
agreement property in Lemma 7, it must be that m and m(cid:48) were also included in the cut for process pj
in round r. The rest of the protocol, i.e., building a graph and of-delivering messages is deterministic.
Therefore, pj delivers these two messages in round r and also of-delivers m before m(cid:48). Extending
this argument over all rounds of the protocol, it follows that every correct process of-delivers the same
sequence of payload messages.

Lemma 9. If all processes are correct and of-broadcast a ﬁnite number of messages in Algorithm 1–2,
then every correct process eventually of-delivers these messages.

Proof. Let pi be some correct process that of-broadcasts a payload message m. Due to the validity prop-
erty of the underlying Byzantine FIFO consistent broadcast channel, every correct process eventually
bcch-delivers m. According to the algorithm, in every round r a process pi waits for n − f processes to
receive signed vector clocks to proposes a matrix of logical timestamps L for validated Byzantine con-
sensus (L23). The termination property of validated Byzantine consensus guarantees that every correct
process eventually decides some value and according to the agreement property, no two correct processes
decide differently. The resulting common L(cid:48) allows then each process to determine if m is considered in
the current round r. A message m is considered if at least f + 1 processes have bcch-delivered m and
reported it in their vector clock (L28). Additionally, if m is considered in the current round but some
process pi has not bcch-delivered m yet, pi will request that other processes send it the missing payload
message (L31). Further, all messages in msgs are added as vertices to the graph G (L42). Moreover, be-
cause every process of-broadcasts a ﬁnite number of messages, every possible graph that is created will
be ﬁnite. Since m was of-broadcast by a correct process pi, all processes are correct and of-broadcast a
ﬁnite number of messages, m will eventually be of-delivered.

Lemma 10. In Algorithm 1–2, if b(m, m(cid:48)) > b(m(cid:48), m) + 2f + κ, then no correct process of-delivers m(cid:48)
before m.

Proof. Recall that b(m, m(cid:48)) is the number of correct processes that receive and of-broadcast m before
m(cid:48). Consider any two payload messages m and m(cid:48) such that b(m, m(cid:48)) > b(m(cid:48), m) + 2f + κ.

19

Suppose m and m(cid:48) are both included in the cut of some round and none of them has been of-delivered
yet. The protocol deﬁnes a threshold based on M for creating an edge between two vertices. Lemma 5
shows that the condition for differential order fairness ensures that M [m][m(cid:48)] > M [m(cid:48)][m] − f + κ
in the protocol, where M [m][m(cid:48)] counts how many times m appears before m(cid:48) in msgs. Moreover, as
explained in connection with Lemma 5, the algorithm extends this condition for adding an edge (m, m(cid:48))
to

max (cid:8)M [m][m(cid:48)], n − f − M [m(cid:48)][m](cid:9) > M [m(cid:48)][m] − f + κ,
in order to cope with particularly small values of M [m(cid:48)][m]. This may be the case when the full relative
ordering information about m and m(cid:48), in the sense that M [m][m(cid:48)] + M [m(cid:48)][m] ≥ n − f , is not yet
available with the cut. The implementation then adds an edge from m to m(cid:48) to the graph (L48). This
implies that m(cid:48) will not be of-delivered before m because the algorithm respects this order by traversing
the graph starting with vertices that have no incoming edges. Therefore, m is either of-delivered before
m(cid:48) or both messages are delivered together, within the same set. Moreover, observe that (2) ensures that
graph generated by the protocol is connected.

(2)

Consider now the case that m(cid:48) is not included at all in the cut of the current round r. We want to
show that for all m ∈ V of the graph G, if m is of-delivered in round r, there cannot be such an m(cid:48),
for which an edge (m(cid:48), m) would be added at a later round and which might therefore violate κ-order
fairness. Recall that an edge (m(cid:48), m) is added to G in a round whenever (2) holds.

To be more precise, we show that the condition in L53 and the properties of stable() ensure κ-
differential order fairness for such m and m(cid:48). Let ¯w be a node of the graph as in L53. If every m in
ﬂatten( ¯w) appears at least n+f −κ
times in msgs up to the cut, i.e., satisﬁes stable(m), it means that no m(cid:48)
(not in the cut) can be ordered before the messages in G of subsequent rounds. In fact, let m ∈ ﬂatten( ¯w)
be such that stable(m) = TRUE and m(cid:48) not be in the cut. Since stable(m) holds, C[m] ≥ n+f −κ
also
means that M [m][m(cid:48)] ≥ n+f −κ

. Thus,

2

2

2

M [m(cid:48)][m] ≤ n − M [m][m(cid:48)] ≤ n −

n + f − κ
2

=

n − f + κ
2

in any future round as well. But this implies M [m(cid:48)][m]−M [m][m(cid:48)] ≤ −f +κ, and thus, no edge (m(cid:48), m)
is added according to (2). The argument given earlier then shows that order fairness is maintained.
Notice that this takes care of scenarios as in Example 1 that include some message ¯m ∈ ﬂatten( ¯w) with
¬stable( ¯m). There may exist a further message m(cid:48) not included in the cut such that m(cid:48) must be ordered
not after ¯m.

Lemmas 6–10 directly imply the following theorem, which concludes the analysis of the protocol.

Theorem 11. Algorithm 1–2 implements κ-differentially order-fair atomic broadcast.

7 Conclusion

The quick order-fair atomic broadcast protocol guarantees payload message delivery in a differentially
fair order. It works both for asynchronous and eventually synchronous networks with optimal resilience,
tolerating corruptions of up to one third of the processes. Compared to existing order-fair atomic broad-
cast protocols, our protocol is considerably more efﬁcient and incurs only quadratic cost in terms of
amortized message complexity per delivered payload.

Acknowledgments

We thank the anonymous reviewers for helpful suggestions and feedback. Special thanks go to Mahimna
Kelkar, who pointed out a problem in an earlier version of this paper.

This work has been funded by the Swiss National Science Foundation (SNSF) under grant agreement

Nr. 200021 188443 (Advanced Consensus Protocols).

20

References

[1] I. Abraham, D. Malkhi, and A. Spiegelman, “Asymptotically optimal validated asynchronous

byzantine agreement,” in PODC, pp. 337–346, ACM, 2019.

[2] A. Asayag, G. Cohen, I. Grayevsky, M. Leshkowitz, O. Rottenstreich, R. Tamari, and D. Yakira,
“A fair consensus protocol for transaction ordering,” in ICNP, pp. 55–65, IEEE Computer Society,
2018.

[3] L. Baird, “The Swirlds hashgraph consensus algorithm: Fair, fast, byzantine fault tolerance.”

Swirlds Tech Report, SWIRLDS-TR-2016-01, 2016.

[4] C. Cachin, R. Guerraoui, and L. E. T. Rodrigues, Introduction to Reliable and Secure Distributed

Programming (2. ed.). Springer, 2011.

[5] C. Cachin, K. Kursawe, F. Petzold, and V. Shoup, “Secure and efﬁcient asynchronous broadcast
protocols,” in CRYPTO, vol. 2139 of Lecture Notes in Computer Science, pp. 524–541, Springer,
2001.

[6] M. Castro and B. Liskov, “Practical byzantine fault tolerance and proactive recovery,” ACM Trans.

Comput. Syst., vol. 20, no. 4, pp. 398–461, 2002.

[7] P. Daian, S. Goldfeder, T. Kell, Y. Li, X. Zhao, I. Bentov, L. Breidenbach, and A. Juels, “Flash boys
2.0: Frontrunning in decentralized exchanges, miner extractable value, and consensus instability,”
in IEEE Symposium on Security and Privacy, pp. 910–927, IEEE, 2020.

[8] S. Duan, M. K. Reiter, and H. Zhang, “Secure causal atomic broadcast, revisited,” in DSN, pp. 61–

72, IEEE Computer Society, 2017.

[9] C. Dwork, N. A. Lynch, and L. J. Stockmeyer, “Consensus in the presence of partial synchrony,” J.

ACM, vol. 35, no. 2, pp. 288–323, 1988.

[10] M. Fitzi and J. A. Garay, “Efﬁcient player-optimal protocols for strong and differential consensus,”

in PODC, pp. 211–220, ACM, 2003.

[11] C. Ho, D. Dolev, and R. van Renesse, “Making distributed applications robust,” in OPODIS,

vol. 4878 of Lecture Notes in Computer Science, pp. 232–246, Springer, 2007.

[12] M. Kelkar, S. Deb, and S. Kannan, “Order-fair consensus in the permissionless setting,” IACR

Cryptol. ePrint Arch., no. 139, 2021. https://eprint.iacr.org/2021/139.

[13] M. Kelkar, S. Deb, S. Long, A. Juels, and S. Kannan, “Themis: Fast, strong order-fairness in
byzantine consensus,” IACR Cryptol. ePrint Arch., no. 1465, 2021. https://eprint.iacr.
org/2021/1465.

[14] M. Kelkar, F. Zhang, S. Goldfeder, and A. Juels, “Order-fairness for byzantine consensus,” in
CRYPTO (3), vol. 12172 of Lecture Notes in Computer Science, pp. 451–480, Springer, 2020.

[15] K. Kursawe, “Wendy, the good little fairness widget: Achieving order fairness for blockchains,” in

AFT, pp. 25–36, ACM, 2020.

[16] Y. Lu, Z. Lu, Q. Tang, and G. Wang, “Dumbo-mvba: Optimal multi-valued validated asynchronous

byzantine agreement, revisited,” in PODC, pp. 129–138, ACM, 2020.

[17] M. K. Reiter, “Secure agreement protocols: Reliable and atomic group multicast in rampart,” in

CCS, pp. 68–80, ACM, 1994.

21

[18] M. K. Reiter and K. P. Birman, “How to securely replicate services,” ACM Trans. Program. Lang.

Syst., vol. 16, no. 3, pp. 986–1009, 1994.

[19] M. Yin, D. Malkhi, M. K. Reiter, G. Golan-Gueta, and I. Abraham, “Hotstuff: BFT consensus with

linearity and responsiveness,” in PODC, pp. 347–356, ACM, 2019.

[20] Y. Zhang, S. T. V. Setty, Q. Chen, L. Zhou, and L. Alvisi, “Byzantine ordered consensus without

byzantine oligarchy,” in OSDI, pp. 633–649, USENIX Association, 2020.

22

