2
2
0
2

r
p
A
1
1

]
E
M

.
t
a
t
s
[

3
v
7
4
0
5
0
.
9
0
1
2
:
v
i
X
r
a

PAC Mode Estimation using PPR Martingale Conﬁdence Sequences

Shubham Anand Jain∗1
Inderjeet Nair†2

Rohan Shah†1
Jian Vora∗2

Sanit Gupta†2
Sushil Khyalia†

Denil Mehta†2
Sourav Das‡

Vinay J. Ribeiro†

Shivaram Kalyanakrishnan†

†Indian Institute of Technology Bombay, ∗Stanford University, ‡ University of Illinois Urbana-Champaign
(Authors marked 1 contributed equally; authors marked 2 contributed equally.)

Abstract

We consider the problem of correctly identi-
fying the mode of a discrete distribution P
with suﬃciently high probability by observ-
ing a sequence of i.i.d. samples drawn from
P. This problem reduces to the estimation of
a single parameter when P has a support set
of size K = 2. After noting that this special
case is tackled very well by prior-posterior-
ratio (PPR) martingale conﬁdence sequences
(Waudby-Smith and Ramdas, 2020), we pro-
pose a generalisation to mode estimation, in
which P may take K ≥ 2 values. To begin,
we show that the “one-versus-one” principle
to generalise from K = 2 to K ≥ 2 classes is
more eﬃcient than the “one-versus-rest” al-
ternative. We then prove that our resulting
stopping rule, denoted PPR-1v1, is asymp-
totically optimal (as the mistake probability
is taken to 0). PPR-1v1 is parameter-free
and computationally light, and incurs signif-
icantly fewer samples than competitors even
in the non-asymptotic regime. We demon-
strate its gains in two practical applications
of sampling: election forecasting and veriﬁ-
cation of smart contracts in blockchains.

1

INTRODUCTION

We investigate the problem of estimating the mode
of a given, arbitrary, discrete probability distribution
P = (p, v, K) by observing a sequence of i.i.d. samples
drawn according to P. Here P takes values from the
support set v = {v1, v2, . . . , vK} according to the prob-
ability vector p = {p1, p2, . . . , pK} for some K ≥ 2.

Proceedings of the 25th International Conference on Artiﬁ-
cial Intelligence and Statistics (AISTATS) 2022, Valencia,
Spain. PMLR: Volume 151. Copyright 2022 by the au-
thor(s).

For 1 ≤ i ≤ K, the probability of obtaining vi from P
is pi. We assume that P has a unique mode, and with-
out loss of generality, p1 > p2 ≥ p3 ≥ p4 ≥ · · · ≥ pK
(which makes v1 the mode).

Our aim is to provide a procedure L to identify the
mode of P. At each step t ≥ 1, L can either ask for
a sample xt ∼ P or it can terminate and declare its
answer. For “mistake probability” δ ∈ (0, 1), L is said
to be δ-correct if for every qualifying discrete distribu-
tion P, L terminates with probability 1 and correctly
identiﬁes the mode of P with probability at least 1−δ.
If L terminates after observing the sequence of sam-
ples x1, x2, . . . , xT for some T ≥ 1, we may assume
that its answer is the most frequent value of P in this
sequence, since it can be argued that no other choice
can decrease the mistake probability across all prob-
lem instances. Hence, it is convenient to view L simply
as a stopping rule, which only needs to decide when to
terminate. We aim to devise a δ-correct stopping rule
L with low sample complexity—informally the number
of samples T observed before stopping.

In order to make our problem “properly” PAC, we
could introduce a tolerance parameter (cid:15), with the im-
plication that any returned value with associated prob-
ability at least p1 − (cid:15) will be treated as correct. We
omit this generalisation, noting that it can be handled
quite easily by the methods proposed in the paper.
In fact, our version with (cid:15) = 0 exactly matches the
problem deﬁned by Shah et al. (2020), whose state-of-
the-art results are our primary baseline. Shah et al.
(2020) show the following lower bound.
Theorem 1 (Lower bound (Shah et al., 2020)). Fix
δ ∈ (0, 1), K ≥ 2, and a δ-correct stopping rule L.
For each categorical distribution P = (p, v, K), the ex-
pected number of samples observed by L is at least
(cid:18) 1
2.4δ

sup
P (cid:48):mode(P (cid:48))(cid:54)=mode(P)

1
KL(P||P (cid:48))

LB(P, δ) def=

ln

(cid:19)

,

where KL(P ||P (cid:48)) denotes the KL divergence between
categorical distributions P , P (cid:48) with same support set.

 
 
 
 
 
 
PAC Mode Estimation using PPR Martingale Conﬁdence Sequences

Shah et al. (2020) also give a stopping rule, denoted
A1, whose sample complexity is upper-bounded to
within a logarithmic factor of this lower bound.

In this paper, we approach the PAC mode estima-
In re-
tion problem from a diﬀerent perspective.
cent work, Waudby-Smith and Ramdas (2020) propose
prior-posterior-ratio (PPR) martingale conﬁdence se-
quences as a novel framework to obtain “anytime” con-
ﬁdence bounds on unknown parameters of a probabil-
ity distribution. The resulting stopping rule is simple,
with no need for tuning, and yet works surprisingly
well in practice. Encouraged by this empirical ﬁnding,
we investigate the application of the PPR martingale
test to PAC mode estimation. Below we summarise
the contents and contributions of our paper.

• We begin by reviewing the PPR martingale
test (Waudby-Smith and Ramdas, 2020) in Sec-
tion 2, and apply it directly to our “base case” of
K = 2. Empirical comparisons establish clear evi-
dence of the relative eﬃciency of this test.

• In Section 3, we propose three natural methods to
generalise the PPR martingale test to mode estima-
tion (K ≥ 2). Two of these are the “one-versus-one”
(1v1) and “one-versus-rest” (1vr) approaches used
commonly in multi-class machine learning tasks; the
third applies the multi-dimensional (MD) variant
of the PPR martingale test (Waudby-Smith and
Ramdas, 2020). The “one-versus-one” method, de-
noted PPR-1v1, is parameter-free, easy to imple-
ment, and computationally lighter than competi-
tors. Experiments indicate that PPR-1v1 is also the
most sample-eﬃcient among the algorithms.

• In Section 4, we provide two theoretical arguments

to explain the eﬃciency of PPR-1v1.

1. We prove that for many commonly used Chernoﬀ
bounds, the 1vr adaptation to mode estimation
cannot terminate before the 1v1 variant; addi-
tionally the MD variant of PPR cannot termi-
nate before PPR-1v1. These results hold for ev-
ery single run, and establish 1v1 as a clear choice
for mode estimation. Even A1, originally imple-
mented as a 1vr variant (Shah et al., 2020), is
seen to perform much better by switching to 1v1
(although it remains inferior to PPR-1v1).

2. We prove that PPR-1v1 is asymptotically opti-
mal, in the sense that for every categorical distri-
bution P, the ratio of the expected sample com-
plexity of PPR-1v1 and LB(P, δ) goes to 1 as the
mistake probability δ is taken to 0. To the best
of our knowledge, this guarantee is the ﬁrst of
its kind for mode estimation, although similar re-
sults have been provided in the multi-armed ban-
dits literature (Garivier and Kaufmann, 2016).

Interestingly, 1vr variants (such as A1) appear
not to be asymptotically optimal.

• Over the years, the mode estimation problem has re-
ceived attention in many diﬀerent contexts (Parzen,
1962; Manku and Motwani, 2002). In Section 5, we
illustrate the relevance of PPR-1v1 in two contrast-
ing real-life applications. First, we show that when
used as a subroutine, PPR-1v1 can reduce the sam-
ple complexity of winner-forecasting in indirect elec-
tions (Karandikar, 2018). Thereafter, we present its
application to probabilistic veriﬁcation in permis-
sionless blockchains (Das et al., 2019).

In short, our paper proposes PPR-1v1 as a novel stop-
ping rule for PAC mode estimation, and provides both
theoretical and empirical reasons to justify the choice.

2 THE PPR MARTINGALE TEST

In this section, we consider the “base case” of mode
estimation, in which P takes exactly K = 2 values.
Notice that P(p, v, 2) is a Bernoulli distribution that
generates v1 with probability p1 and v2 with proba-
bility p2 = 1 − p1. Treating p1 ∈ [0, 1] as the sole
parameter of the distribution, our task is to devise a
δ-correct stopping rule to test if p1 > 1
2 . Since p1 may
be arbitrarily close to 1
2 , it is not possible to decide be-
forehand how many samples suﬃce for the test to suc-
ceed. An unfortunate consequence of having a random
stopping time is that it cannot be used directly within
concentration inequalities such as Chernoﬀ bounds.
Rather, stopping rules invariably go through a union
bound over all possible stopping times, dividing the
mistake probability δ among them (Kalyanakrishnan
et al., 2012; Kaufmann and Kalyanakrishnan, 2013).
Although there has been progress towards optimising
this apportioning of δ (Jamieson et al., 2014; Garivier,
2013), resulting methods still have tunable parameters
in their “decay rates”, which govern the stopping time.

The recent development of “time-uniform” or “any-
time” Chernoﬀ bounds (Howard et al., 2020) relieve
the experimenter of tedious parameter-tuning. Aris-
ing from this line of research is the framework of
prior-posterior-ratio (PPR) martingale conﬁdence se-
quences (Waudby-Smith and Ramdas, 2020), which
yields a simple, intuitive stopping rule. Although the
rule may be applied more widely, we restrict our up-
coming discussion to the Bernoulli case at hand: that
is, to test whether p1 > 1
2 .

To apply the PPR martingale framework, we main-
tain a belief distribution π for p1 over its range [0, 1],
and update π according to Bayes’ rule as samples are
observed. Our aim is still to provide a frequentist

Jain, Shah, Gupta, Mehta, Nair, Vora, Khyalia, Das, Ribeiro, Kalyanakrishnan

guarantee that holds for all possible values of p1 (δ-
correctness). To this end we must ensure that the
prior distribution π0 gives non-zero density to all pos-
sible values of p1. We do so by adopting the uniform
prior π0(q) = 1 for q ∈ [0, 1]. For t ≥ 1, we update our
belief distribution after observing sample xt:

πt(q) =

πt−1(q) · (q)1[xt=v1] · (1 − q)1[xt=v2]
(cid:82) 1
ρ=0 πt−1(ρ) · (ρ)1[xt=v1] · (1 − ρ)1[xt=v2]dρ

.

The prior-posterior-ratio (PPR) at q ∈ [0, 1] is given
by Rt(q) = π0(q)
πt(q) . Waudby-Smith and Ramdas (2020)
t=0, where C t def={q :
show that the sequence of sets (C t)∞
Rt(q) < 1
is a (1 − δ)-conﬁdence sequence for
p1 (Waudby-Smith and Ramdas, 2020).
In other
words, we have the “anytime” guarantee that

δ },

P{ ∃ t ≥ 0 : p1 /∈ C t} ≤ δ.

(1)

The correctness of (1) is shown by establishing that
the PPR evaluated at the true parameter value, p1,
is a martingale, and then applying Ville’s inequality
for nonnegative supermartingales (Waudby-Smith and
Ramdas, 2020, see Appendix B.1). For our special case
of estimating the parameter of a Bernoulli distribution,
the belief distribution πt and hence the PPR Rt as-
sume a convenient form if initialised with the uniform
prior. Suppose the sequence of samples up to time t
is x1, x2, . . . , xt, which contains st
1 occurrences of v1
and st
2 = t − s1 occurrences of v2. Then for t ≥ 0 and
q ∈ [0, 1], we obtain πt(q) = Beta(q; st
1 + 1, st
2 + 1) (the
pdf of a Beta distribution with parameters st
1 + 1 and
st
2+1, evaluated at q). We can terminate as soon as the
(1 − δ)-conﬁdence sequence on p1 does not contain 1
2 .
For easy readability, let us deﬁne indices ﬁrst(t) and
second(t), where (ﬁrst(t), second(t)) ∈ {(1, 2), (2, 1)}
satisﬁes st
second(t). We obtain the following
simple stopping rule, applied at each time step t ≥ 1.

ﬁrst(t) ≥ st

p1 that hold with probability 1 − δt for each t ≥ 1,
satisfying (cid:80)∞
t=1 δt ≤ δ. The δ-correctness of the pro-
cedure is ensured by terminating only when the lower
conﬁdence bound exceeds 1
2 , or the upper conﬁdence
bound falls below 1
2 . We compare PPR-Bernoulli with
several variants from the literature. In Figure 1, we
plot the sample complexity of diﬀerent algorithms as
p1 and δ are varied.
A common choice is to set δt = k δ
tα , with con-
stants k and α tuned for eﬃciency, while ensuring
δ-correctness. As representatives of this approach,
we pick the LUCB and KL-LUCB algorithms (Kauf-

(a) p1 varied; δ = 0.01.

PPR-Bernoulli: Stop, declare vﬁrst(t) as mode iﬀ

(cid:16) 1

Beta

2 ; st

ﬁrst(t) + 1, st

second(t) + 1

≤ δ.

(cid:17)

Note that the LHS of the PPR-Bernoulli stopping rule
can be evaluated exactly as a rational, using integer
arithmetic, requiring only a lightweight incremental
update after each sample. As we see shortly, many
other stopping rules require much heavier computa-
tion, such as to perform numerical optimisation.

(b) δ varied; p1 = 0.65.

2.1 Empirical Comparisons, K = 2

For the problem of determining the sign of p1 − 1
2 from
samples, the predominant approach in the literature
is to construct lower and upper conﬁdence bounds on

Figure 1: Comparison of stopping rules for the
Bernoulli case (K = 2). Both plots show sample com-
plexity: in (a) as p1 is varied, and in (b) as δ is varied.
The results are averages from 100 runs. Error bars
show one standard error (in both plots very small).

PAC Mode Estimation using PPR Martingale Conﬁdence Sequences

mann and Kalyanakrishnan, 2013). The former in-
verts Hoeﬀding’s inequality to obtain lower and up-
per conﬁdence bounds, while the latter uses a tighter
Chernoﬀ bound. Although these algorithms them-
selves are meant for bandit applications, their eﬃ-
ciency crucially depends on the tightness of the conﬁ-
dence bounds applied to each arm. The tuned conﬁ-
dence bounds (Kaufmann and Kalyanakrishnan, 2013)
hence become suitable baselines for our comparison.1

With the intent of avoiding a na¨ıve union bound over
time, Garivier (2013) applies a so-called peeling argu-
ment to divide time into increasingly-sized slices. He
obtains conﬁdence regions by associating the random
stopping time with a self-normalised process. The re-
sulting stopping rule, which we denote KL-SN, still
has a tunable parameter “c”, which we set as recom-
mended by Garivier (2013). Although the A1 algo-
rithm (Shah et al., 2020) is designed speciﬁcally for
mode estimation, we include it in this comparison to
observe its performance when K = 2. In this special
case, the algorithm reduces to an application of an
empirical Bernstein bound (Maurer and Pontil, 2009).

The two plots in Figure 1 are remarkably consistent
as p1 and δ are varied. KL-LUCB shows a marginal
improvement over (Hoeﬀding) LUCB, while KL-SN
clearly outperforms both. However, PPR-Bernoulli
is signiﬁcantly more eﬃcient than even KL-SN. Sur-
prisingly, in spite of using variance information, A1
registers the worst performance among all the meth-
ods compared. We attribute this result to slack in the
constants used in its stopping rule.

The empirical evidence of its sample eﬃciency, along
with its simplicity and non-reliance on parameter-
tuning, make PPR-Bernoulli an attractive proposition
for stopping problems. In Section 3, we consider three
separate ways to generalise it to mode estimation. In
Section 4 we follow with theoretical analysis to explain
the empirical ﬁndings in sections 2 and 3.

3 GENERALISATION TO K ≥ 2

In the broader machine learning literature, the most
common approaches for generalising 2-class problems
to more classes are “one-versus-one” (denoted 1v1)
and “one-versus-rest” (denoted 1vr). We investigate
both approaches. We also consider the direct applica-
tion of the multi-dimensional (MD) variant of the PPR
martingale test (Waudby-Smith and Ramdas, 2020).

1Details of all our implementations are given in Ap-

pendix A; links to code are provided in Appendix B.

3.1 One-versus-one (1v1) Approach

In the ﬁrst t ≥ 1 samples, let the number of occur-
rences of value vi be st
i, 1 ≤ i ≤ K. The 1v1 generalisa-
tion is based on the idea that if vi is to be declared the
mode, we need to be suﬃciently sure that vi is more
probable than vj for j ∈ {1, 2, . . . , K}, i (cid:54)= j. Cor-
respondingly, we simultaneously run PPR-Bernoulli
δ
K−1 .
tests on each (i, j) pair with mistake probability
Each (i, j) test relies solely on the number of occur-
rences of vi and vj, disregarding other values. Hence
it amounts to observing samples from a Bernoulli vari-
, and verifying which side of
able with parameter
1
2 its mean lies. The overall procedure stops when some
i ∈ {1, 2, . . . , K} has won each of its tests. By a union
bound, with probability at least 1 − δ, the (true) mode
v1 will not ever lose a test. Thus, upon termination,
v1 is returned with probability at least 1 − δ.

pi
pi+pj

2

ﬁrst(t) ≥ st

Whereas the description above suggests we need to
monitor (cid:0)K
(cid:1) tests at each step, closer inspection re-
veals that a much lighter implementation is possi-
ble. As before, let ﬁrst(t) denote the index of the
most-frequently occurring value (with arbitrary tie-
breaking) after t samples: that is, st
i for
i ∈ {1, 2, . . . , K}. Now, if at all a winner is identi-
ﬁed after t samples, clearly it must be vﬁrst(t), which
has as many occurrences as any other value. Hence,
we only need to track tests involving vﬁrst(t). Now,
it is also immediate that vﬁrst(t) wins all its tests if
and only if it defeats the second most frequently oc-
curring value, which we denote second(t):
that is,
second(t) ∈ {1, 2, . . . , K}, second(t) (cid:54)= ﬁrst(t) satisﬁes
st
second(t) ≥ st
i for i ∈ {1, 2, . . . , K} \ {ﬁrst(t)}. Hence,
we may implement our stopping rule, denoted PPR-
1v1, using a single PPR-Bernoulli test at each t ≥ 1.

PPR-1v1: Stop and declare vﬁrst(t) as mode iﬀ

(cid:16) 1

Beta

2 ; st

ﬁrst(t) + 1, st

second(t) + 1

≤ δ

K−1 .

(cid:17)

Tracking ﬁrst(t) and second(t) is a simple computa-
tion; as observed earlier, it is also eﬃcient to compute
the Beta density at 1
2 . Indeed our experiments show
that PPR-1v1 is much faster computationally than
other mode estimation algorithms (see Appendix B).

3.2 One-versus-rest (1vr) Approach

Notice that under PPR-1v1, sample xt at each step
t ≥ 1 contributes only to the K − 1 PPR-Bernoulli
tests of the particular i from {1, 2, . . . , K} that satis-
ﬁes vi = xt. The (cid:0)K−1
(cid:1) tests corresponding to val-
ues other than xt receive no information. The 1vr
approach becomes an alternative to address this ap-
parent wastage of information. Under the 1vr scheme,
we associate a Bernoulli variable Bi with each value

2

Jain, Shah, Gupta, Mehta, Nair, Vora, Khyalia, Das, Ribeiro, Kalyanakrishnan

vi, 1 ≤ i ≤ K, which has probability pi of generating
vi, and probability 1 − pi of generating its negation
“¬vi”. Consequently, each sample of P adds to one
of the outcomes of Bi for each i ∈ {1, 2, . . . , K}. We
draw an anytime conﬁdence sequence for Bi with mis-
take probability δ
K , and terminate after t ≥ 1 samples
if the conﬁdence set of Bﬁrst(t) does not overlap with
any of the others. Invoking the PPR martingale conﬁ-
dence sequence, we note that with probability at least
1 − δ
i),
t ≥ 1, where LCBt
K }
and UCBt
K } can be com-
puted numerically. The δ-correctness of the 1vr rule,
given below, follows from a union bound on the mis-
take probabilities of each Bi, 1 ≤ i ≤ K.

i, UCBt
i = min{q ∈ [0, 1] : πt(q) = δ

K , pi will lie in all the intervals (LCBt

i = max{q ∈ [0, 1] : πt(q) = δ

PPR-1vr: Stop and declare vﬁrst(t) as mode iﬀ
ﬁrst(t) ≥ UCBt
for 1 ≤ i ≤ K, i (cid:54)= ﬁrst(t), LCBt
i.

3.3 Multi-dimensional (MD) PPR Test

PPR Martingale conﬁdence sequences can be directly
constructed for the multi-dimensional parameter vec-
tor of P (Waudby-Smith and Ramdas, 2020, see Ap-
pendix C). In this approach, denoted PPR-MD, at
each t ≥ 1 we maintain a conﬁdence set C t with
¯p ∈ [0, 1]k, such that C t def={¯p : Rt(¯p) < 1
δ }. We stop
at time t when all ¯p ∈ C t have the same unique mode.
With the Dirichlet distribution being the conjugate
prior of the categorical distribution, Rt(¯p) has a con-
venient form if initialised with a uniform prior:

Rt(¯p) =

1
(K − 1)!

×

×

(cid:81)K
Γ((cid:80)K

i=1 Γ(αt
i)
i=1 αt
i)

,

(cid:81)K

i−1

1
i=1 ¯pαt
i = st

i

where for 1 ≤ i ≤ K, αt
i + 1. Observe that
this formulation reduces to PPR-Bernoulli for K = 2.
However, for K > 2, checking for a unique mode in C t
does not simplify to a convenient formula; it requires
a numerical computation that increases steeply with
K. We do not perform extensive experiments with
PPR-MD—a choice justiﬁed by Lemma 2 (Section 4).

3.4 Empirical Comparisons

We compare PPR-1v1 and PPR-1vr with other mode
estimation algorithms on a variety of discrete distribu-
tions. Table 1 summarises the results.

The A1 algorithm (Shah et al., 2020) is essentially a
1vr approach that uses Empirical Bernstein conﬁdence
bounds (Maurer and Pontil, 2009). Noting that it can
just as well be implemented in a 1v1 form, we include
such a variant, denoted A1-1v1, in our experimental
comparisons with PPR. For good measure, we also in-
clude 1v1 and 1vr variants based on the KL-SN con-

ﬁdence bound (Garivier, 2013), which ﬁnished second
to PPR-Bernoulli for K = 2 (see Section 2).

In Table 1, we observe the same trend on each prob-
lem instance: (1) The 1v1 variant of each stopping
rule outperforms the corresponding 1vr variant, and
(2) PPR is most sample-eﬃcient, followed by KL-SN
and A1. Although the prohibitive running time of
PPR-MD prevents a thorough assessment, a few in-
formal runs indicate that its sample complexity is well
in excess of even PPR-1vr.

Whereas the results in Table 1 are for a ﬁxed value of
δ = 0.01, we conduct a second set of experiments to
compare the performance of the diﬀerent algorithms as
δ is varied. In particular, we investigate the “asymp-
totic” regime, in which δ is taken to 0. In Figure 2, we
plot ratio of the empirical sample complexity and the
lower bound from Theorem 1, varying δ while keeping
the distribution ﬁxed to P3 from Table 1. Observe that
once again, the relative order among the algorithms
remains the same. The 1v1 variant of each algorithm
performs better than its 1vr counterpart. Moreover,
the curves for PPR-1v1 and KLSN-1v1 suggest that
these rules might be asymptotically optimal.

The empirical evidence supporting the PPR martin-
gale test and the 1v1 approach to mode estimation is
compelling.
In the forthcoming section, we provide
theoretical reasons to explain our observations.2

4 THEORETICAL JUSTIFICATION

In our experiments, we observe that not only do the
1v1 variants of each method perform better than 1vr
in aggregate, they terminate before the 1vr variants on
every single run. We prove this result true for some

1

1

2After this paper was submitted for review, the authors
were pointed to recent related work by Haddenhorst et al.
(2021), who focus on the identiﬁcation of a generalised
Condorcet winner in multi-dueling bandits. Mode estima-
tion is a special case of the problem they consider. Their al-
gorithm, based on the Dvoretzky-Kiefer-Wolfowitz (DKW)
inequality, has an upper bound which improves upon that
In particular, the bound is
given by Shah et al. (2020).
independent of K, and depends on
(p1−p2) ,
(p1−p2)2 ln ln
(p1−p2)2 ln
rather than
(p1−p2) . However, our experiments
show that the two variants of their algorithm—DKW-1,
which is provided the knowledge of p1 − p2, and DKW-2,
which has no such prior knowledge—both perform worse
than PPR-1v1. For example, when run on problem in-
stance with K = 2, p1 = 0.66, δ = 0.01, PPR-Bernoulli
requires roughly 124 samples, whereas DKW-1 and DKW-
2 take roughly 469 and 871 samples, respectively. The
inferior empirical performance of the latter algorithms in
spite of their superior upper bound can be explained by the
accompanying constant factors. The notion of asymptotic
optimality that we present in sections 3 and 4 requires even
the constant factor to be tight, albeit as δ → 0.

1

1

PAC Mode Estimation using PPR Martingale Conﬁdence Sequences

Table 1: Sample complexity comparison for mode estimation, run with mistake probability δ = 0.01. The number
after the × symbol indicates the multiplicity of that particular probability value in the distribution; thus P1 has
p = (0.5, 0.25, 0.25). The values reported are averages from 100 or more runs, and show one standard error.

Distribution

P1: .5, .25 × 2

P2: .4, .2 × 3

P3: .2, .1 × 8

P4: .1, .05 × 18

P5: .35, .33, .12, .1 × 2

5

P6: .35, .33, .04 × 8

10

K Type A1 (Shah et al., 2020) KL-SN (Garivier, 2013)

PPR

3

4

9

19

1vr
1v1

1vr
1v1

1vr
1v1

1vr
1v1

1vr
1v1

1vr
1v1

1344±20
1158±19
1919±29
1516±24
5082±51
3340±43
12015±129
7352±88
155277±2356
117988±2078
158254±2442
121150±2183

418±14
346±13
632±18
468±15
1900±42
1138±31
4686±81
2554±57
63739±2238
47205±1291
66939±2241
49576±1341

262±12
218±11
397±15
298±13
1201±29
789±28
2850±55
1840±53
38001±1311
33660±1125
41963±1330
36693±1185

of the methods. We also formally establish that PPR-
MD and A1 cannot terminate before PPR-1v1.
Lemma 2. Let X = x1, x2, . . . be an inﬁnite sequence
of samples from P. For algorithm L and δ ∈ (0, 1), let
T (L, X, δ) be the stopping time of L on X, when run
with mistake probability δ. For algorithms L1, L2, let
the proposition G(L1, L2, X, δ) denote “If T (L2, X, δ)
is ﬁnite, then T (L1, X, δ) ≤ T (L2, X, δ)”. For all P,
for all X generated from P, for all δ ∈ (0, 1), we have

(i) G(LUCB-1v1, LUCB-1vr, X, δ),

(ii) G(A1-1v1, A1-1vr, X, δ),

(iii) G(PPR-1v1, PPR-MD, X, δ),

(iv) G(PPR-1v1, A1-1v1, X, δ).

We give a proof of the lemma in Appendix C.
The proofs of (i) and (ii) formalise the intuition
that although the 1vr variants update all their K
tests with each sample, the test to separate any
two variables is less eﬀective than that under the
corresponding 1v1 variant. We conjecture that
G(PPR-1v1, PPR-1vr, X, δ) is also true, in fact veri-
fying it to be the case for |X| ≤ 200. We also believe
that a similar result applies to the 1v1 and 1vr vari-
ants of KL-LUCB and KL-SN (but the proofs become
cumbersome). Result (iii) is straightforward to estab-
lish from ﬁrst principles. To show (iv), consider that
after t samples, A1-1v1 draws upper and lower con-
ﬁdence bounds on the mean as ˆpt ± βt, where ˆpt is
the empirical mean and βt the conﬁdence width. In
(cid:113) 2V tln(4t2/α)
particular, βt = βt
t
2 = 7ln(4t2/α)

1 + βt
1 =
, with V t being the empirical vari-
and βt
ance and α the input mistake probability. When ˆpt is
within a constant distance from 0.5 (hence V t is close
to its maximum of 0.25), we show that the PPR conﬁ-
1]. When ˆpt is
dence set is contained in [ˆpt − βt
either suﬃciently small or suﬃciently large, t (at ter-
mination) can itself be upper-bounded in terms of ˆpt,
in turn lower-bounding βt
2 and guaranteeing that the
2, ˆpt + βt
PPR conﬁdence set is contained in [ˆpt − βt
2].

2, where βt

1, ˆpt + βt

3(t−1)

Figure 2: Comparison of diﬀerent stopping rules on
P3, for small values of δ. The y axis plots the ratio
of the empirical stopping time (averaged over 100 or
more runs) and LB(P3, δ), deﬁned in Theorem 1.

Lemma 2 oﬀers theoretical justiﬁcation for many of the
trends observed in Table 1. The theoretical question
that remains open is an explanation of Figure 2:
is
PPR-1v1 indeed asymptotically optimal? We obtain
an aﬃrmative answer.
Theorem 3 (Optimality of PPR-1v1). Fix δ ∈ (0, 1),
K ≥ 2, and distribution P = (p, v, K). Let τ (P, δ)
be the expected stopping time of PPR-1v1 on P when

Jain, Shah, Gupta, Mehta, Nair, Vora, Khyalia, Das, Ribeiro, Kalyanakrishnan

run with mistake probability δ, and let LB(P, δ) be the
lower bound deﬁned in Theorem 1. Then

lim
δ→0

τ (P, δ)
LB(P, δ)

= 1.

The proof of the theorem is given in Appendix D. In
the proof, we establish that with a little over LB(P, δ)
samples from P, a suﬃcient number of samples are ob-
tained for separating value v1 from each of the others.
The proof of separation for each pair uses a similar se-
quence of steps as of Garivier and Kaufmann (2016).

In short, our theoretical analysis reinforces PPR-1v1
as the method of choice for PAC mode estimation. For
good measure, Appendix E presents an explicit upper
bound on the sample complexity of PPR-1v1, which
holds for all δ ∈ (0, 1). This bound improves upon
that of A1 (Shah et al., 2020) by a constant factor.

5 PRACTICAL APPLICATIONS

Our main motivation for devising better mode estima-
tion algorithms is their practical signiﬁcance, which we
illustrate through two contrasting applications.

5.1 Forecasting in Indirect Elections

Opinion polls to forecast the winner of an upcom-
ing election are a natural application of mode esti-
mation. In fact, the algorithms discussed in Section 3
can all be applied with only minor alterations to plu-
rality systems, wherein the task is precisely that of de-
termining the choice preferred by the largest fraction
of the target population. Waudby-Smith and Ramdas
(2020) illustrate the use of the PPR martingale test on
this application, while focusing on without-replacement
sampling.
In parliamentary democracies such as In-
dia (Karandikar et al., 2002) and the U.K. (Payne,
2003), a two-level voting system is used to elect gov-
ernments.
In this system, individuals in each con-
stituency (or seat)—typically a geographically con-
tiguous region—elect a party based on plurality; the
party winning the most seats forms the government.
Forecasting the winning party in such an indirect vot-
ing system calls for a more sophisticated sampling pro-
cedure. Whereas it would suﬃce to separately identify
the winner from each seat by sampling, it might be
wasteful to do so when the overall winning party has
a clear majority in its number of seats.

Formally, consider a setting where we have K par-
ties and N constituencies. Each constituency c ∈
{1, 2, . . . , N } represents a discrete probability distri-
bution P c = (pc, v, K), where for i ∈ {1, 2, . . . , K},
vi represents the political party i and pc
i denotes the
fraction of votes won by party i in constituency c (for

simplicity we have assumed all parties compete in all
constituencies). If i(cid:63)
c is the index of the mode of P c
(assumed unique), our objective is to determine

argmax
i∈{1,2,...,K}

N
(cid:88)

c=1

1[i = i(cid:63)
c ]

correctly with probability at least 1 − δ. The objec-
tive of the sampling rule is to minimise the total votes

sampled,

T c, where T c represents the number of

N
(cid:80)
c=1

votes queried in constituency c.

We consider a procedure that (1) keeps track of the
current winners and leaders at the aggregate level, and
(2) at each step samples the constituencies that ap-
pear most promising to conﬁrm the aggregate trend.
In principle, this algorithm, denoted DCB (for “Dif-
ference in Conﬁdence Bounds”) can be coupled with
any algorithm that uses conﬁdence bounds for mode
estimation. Yet, we obtain the best results when DCB
uses PPR-1v1 as a subroutine, thereby highlighting
the relevance of PPR-1v1 not only as a stopping rule,
but also as an input to on-line decision making.

DCB takes cue from the LUCB algorithm for best-
arm identiﬁcation in bandits (Kalyanakrishnan et al.,
2012). At each step t, it identiﬁes two parties, at and
bt, that appear the most promising to win the over-
all election: these parties are picked based on their
current number of wins and “leads” in individual con-
stituencies. Subsequently the algorithm chooses a con-
stituency each for at and bt, samples from which could
“most” help distinguish the tally of the two. We pro-
vide a detailed speciﬁcation of DCB in Appendix F.

We compare DCB with a round-robin strategy for
picking the next constituency to sample. Both ap-
proaches can be implemented with diﬀerent stopping
rules, which are also varied. Table 2 shows our re-
sults on the 2014 parliamentary elections in India.3
In this election, the winning party secured 282 seats
from among 543, giving it a very large victory over the
second-place party, which won 44 seats. Results from
a much closer contest, in the state of Bihar, are given
in Appendix G. We see a similar trend in both cases.

While it is not the central feature of this paper, it is
worth noting that the DCB strategy indeed improves
over round-robin polling by roughly a factor of two,
regardless of the stopping rule. As intended, it does
not waste samples on constituencies that are inconse-
quential to the overall result (observed in the “seats
resolved” columns). Of more direct relevance to the
theme of the paper is that even when embedded within

3Election results are in the public domain; the authors

accessed them at https://www.indiavotes.com/.

PAC Mode Estimation using PPR Martingale Conﬁdence Sequences

Table 2: Sample complexity of various stopping rules
when coupled with (1) round-robin (RR) polling of
constituencies and (2) DCB. All experiments are run
with mistake probability δ = 0.01. Values shown are
averages from 10 runs, and show one standard error.
“Seats resolved” indicates the number of constituen-
cies in which a winner was identiﬁed before the overall
procedure terminated.

Algorithm

RR-A1-1v1
RR-A1-1vr
RR-KLSN-1v1
RR-KLSN-1vr
RR-PPR-1v1
RR-PPR-1vr

DCB-A1-1v1
DCB-A1-1vr
DCB-KLSN-1v1
DCB-KLSN-1vr
DCB-PPR-1v1
DCB-PPR-1vr

India-2014 (543 seats)

Samples

Seats
Resolved

1578946 ± 10255
1767464 ± 15828
610181 ± 7072
726512 ± 6186
471661 ± 7373
560815 ± 7778
856678 ± 3935
951684 ± 5246
325265 ± 2096
376108 ± 4067
256911 ± 2096
296580 ± 2372

239 ± 2
234 ± 1
240 ± 2
236 ± 2
241 ± 3
238 ± 2
182 ± 1
180 ± 1
186 ± 2
181 ± 1
188 ± 1
184 ± 1

a decision-making outer loop, PPR continues to out-
perform A1 and KL-SN, and the 1v1 approach still
dominates 1vr. Surmising that the sheer eﬃciency of
PPR-1v1 makes it a good choice for embedding in more
complex systems, we proceed to our next application.

5.2 Verifying Blockchain Smart Contracts

Our second application of PAC mode estimation is in
a domain of growing contemporary relevance. Permis-
sionless blockchains such as Bitcoin (Nakamoto, 2009)
and Ethereum (Buterin, 2014) allow uncertiﬁed agents
to join a pool of service providers, also called nodes. A
recent feature that has emerged in such blockchains
is the execution of “smart contracts” (Das et al.,
2019; Buterin, 2014), which could include, for example,
running computationally-heavy jobs such as machine
learning algorithms. In an ideal world, a client who
requires some computation performed can simply en-
ter a smart contract with some particular node, and
pay a fee for the service. Unfortunately, there is no
guarantee that nodes in a permissionless blockchain
are honest. A “Byzantine” (malicious) node could po-
tentially return a quick-to-compute, incorrect output,
to the detriment of the client.

In recent work, Das et al. (2019) propose an approach
for the probabilistic veriﬁcation of smart contracts.
Abstractly, assume that the computation to be per-
formed for the client is deterministic, and it has a (yet
unknown) output ocorrect. The proposed model ac-
commodates any blockchain in which the fraction of

(cid:16)

Byzantine nodes f is at most fmax ∈ [0, 1
2 ). With this
assumption, it becomes feasible to give a probabilistic
guarantee on obtaining the correct output. For any
ﬁxed mistake probability δ ∈ (0, 1), the client could
(cid:17)
2 −fmax)2 log( 1
δ )
ship out the computation to Θ
nodes, and take their majority response as the an-
swer, thereby ensuring δ-correctness. Unfortunately,
transaction costs can be substantial, especially those
for computationally-intensive contracts. Hence, it is in
the client’s interest to minimise the number of nodes
queried to achieve the same probabilistic guarantee.
For example, a sequential procedure could potentially
query fewer nodes if f (cid:28) fmax.

( 1

1

Our contribution in the context of this application is to
propose PPR-1v1 as an alternative to the Sequential
Probability Ratio Test (SPRT) (Wald, 1945), which is
used by Das et al. (2019) for their veriﬁcation proce-
dure. This classical test ﬁnds use in many other engi-
neering applications (Gross and Humenik, 1991; Chen
et al., 2008), some of which could also beneﬁt from the
advantages of PPR-1v1 over SPRT.

To apply SPRT for verifying smart contracts, Das et al.
(2019) assume that out of the total of N nodes in
the blockchain, batches of size m, chosen uniformly
at random, can be queried in sequence (the m queries
per batch are performed in parallel, hence saving some
time). For simplicity assume the answers returned are
from the set {0, 1, 2, . . . }. Let q = m
N , and let ci,t
be the number of times answer i is reported in the tth
step, i ≥ 0, t ≥ 1. Deﬁning li,T = (cid:80)T
t=1(2ci,t −m)m, a
derivation (Das et al., 2019) establishes that δ-correct
SPRT stops at time T , giving i as the answer if
(cid:19) 2q(1 − q)N (1 − fmax)fmax
1 − 2fmax

(cid:18) 1 − δ
δ

li,T > ln

.

The primary disadvantage of SPRT herein is the need
for the user to provide fmax, which is used in the stop-
ping rule. While a lower value of fmax will improve
the eﬃciency of the rule, unfortunately δ-correctness
no longer holds if f , the true fraction of Byzantine
nodes, exceeds fmax. Figure 3a plots the empirical
error made by SPRT (averaged over 50,000 runs) on
a problem instance in which the Byzantine nodes all
give the same (incorrect) answer. We ﬁx fmax = 0.1,
and plot the error by varying f . The blockchain has
N = 1600 nodes, of which SPRT samples m = 20 at a
time. Although the test is run using mistake probabil-
ity δ = 0.005, observe that the empirical error exceeds
δ when f > fmax. Since the veriﬁcation task at hand
is precisely that of PAC mode estimation, PPR-1v1
becomes a viable alternative, especially since it does
not need the knowledge of fmax.
In fact, PPR-1v1
can identify the mode even if its associated probabil-
ity is less than 1
2 (although in this case, it can no

Jain, Shah, Gupta, Mehta, Nair, Vora, Khyalia, Das, Ribeiro, Kalyanakrishnan

sion of SPRT used is a 1vr adaptation of the basic
procedure (Das et al., 2019) to K = 10. First, we
observe that SPRT terminates before PPR-1v1 and
PPR-1vr at all values of f . The PPR algorithms pay
this price for having to assure δ-correctness at all val-
ues of f < 1
2 , unlike SPRT, which does so only for
f < fmax. In the same plot, we show the performance
of another PPR variant, denoted “PPR-Adaptive”. In
reality, we cannot be sure about the number of answers
K that will be returned by the blockchain’s nodes—
and hence cannot use it in our stopping rule. Under
PPR-Adaptive, which is a 1v1 strategy, the overall
mistake probability δ is divided into the inﬁnite se-
quence k δ
π2 ). Whenever
a new answer is revealed, it is inserted into the list
of possible answers, and its pairwise tests given mis-
take probabilities from the unused portion of the se-
quence. In principle, PPR-Adaptive can accommodate
any number of answers, incurring only a small increase
in sample complexity, as visible from Figure 3b.

32 , . . . (with k = 6

22 , k δ

12 , k δ

(a) K = 2 (single adversarial answer)

6 CONCLUSION

In this paper, we apply the framework of PPR Mar-
tingale conﬁdence sequences to the problem of PAC
mode estimation. Our investigation follows two diﬀer-
ent dimensions that play a signiﬁcant role in determin-
ing the eﬃciency of stopping rules. First is the tight-
ness of the conﬁdence bounds used internally in the
stopping rule. By separately focusing on the Bernoulli
case, we show that the PPR Martingale stopping rule
is sample-eﬃcient. The second aspect of mode estima-
tion is the template applied to generalise from K = 2
to K ≥ 2, which can typically be applied with any
valid conﬁdence bounds. Of the three major choices—
“one-versus-one” (1v1), “one-versus-rest” (1vr), and a
multi-dimensional test (MD)—we ﬁnd 1v1 to be the
most eﬃcient. Our empirical ﬁndings are aﬃrmed by
theoretical analysis that shows (1) regardless of the
problem instance and the mistake probability, the 1v1
approach is guaranteed to terminate no later than the
1vr approach for many popular conﬁdence bounds, and
(2) the PPR-1v1 stopping rule is indeed asymptotically
optimal. The PPR-1v1 algorithm is also parameter-
free and computationally much faster than the other
algorithms, making it a natural choice to apply to
practical mode estimation problems. We illustrate its
eﬃcacy on two distinct real-world tasks.

(b) K = 10

Figure 3: Comparisons with SPRT for the probabilistic
veriﬁcation of smart contracts, obtained with parame-
ter settings N = 1600, m = 20, δ = 0.005, fmax = 0.1.
Plot (a) shows the empirical error rates of the algo-
rithms as the true Byzantine fraction f is varied, tak-
ing K = 2. Plot (b) shows the sample complexity of
various algorithms against varying f , on an instance
with K = 10 answers.

longer be guaranteed that the mode is ocorrect, since
the Byzantine nodes may collude). Observe from Fig-
ure 3a that unlike SPRT, the empirical error rate of
PPR-1v1 (equivalent to PPR-Bernoulli since we have
set K = 2) remains within δ even for f > fmax.

In Figure 3b, we compare the sample complexities of
PPR-1v1 and SPRT. Whereas other problem param-
eters (including fmax) stay the same as before, we
consider an instance in which K = 10. The single
correct answer is given by a (1 − f )-fraction of the
nodes, while 9 diﬀerent incorrect answers are given by
the Byzantine nodes, each equally common. The ver-

Our paper opens several directions to explore in fu-
ture work, including the application of the PPR mar-
tingale test on pure exploration problems in stochastic
bandits and Markov Decision Problems. It could also
be of much practical beneﬁt to incorporate the PPR
martingale test (in place of existing ones) in large-scale
applications of sampling and decision making.

PAC Mode Estimation using PPR Martingale Conﬁdence Sequences

Acknowledgements

The authors thank anonymous reviewers for pro-
viding valuable suggestions.
Shivaram Kalyanakr-
ishnan was partially supported by SERB grant
ECR/2017/002479.

References

Vitalik Buterin. A next-generation smart contract
2014.
https://cryptorating.eu/whitepapers/

and decentralized application platform,
URL
Ethereum/Ethereum white paper.pdf.

R. Chen, J.-M. Park, and K. Bian. Robust distributed
spectrum sensing in cognitive radio networks.
In
IEEE INFOCOM 2008 - The 27th Conference on
Computer Communications, pages 1876–1884. IEEE
Press, 2008. doi: 10.1109/INFOCOM.2008.251.

Sourav Das, Vinay Joseph Ribeiro, and Abhijeet
Anand. YODA: enabling computationally intensive
contracts on blockchains with Byzantine and Self-
ish nodes. In 26th Annual Network and Distributed
System Security Symposium, NDSS. The Internet
Society, 2019.

David Galvin. Three tutorial lectures on entropy and
counting. arXiv preprint arXiv:1406.7872, 2014.

Aur´elien Garivier.

Informational conﬁdence bounds
for self-normalized averages and applications.
In
2013 IEEE Information Theory Workshop (ITW),
pages 1–5. IEEE press, 2013.
10.1109/
ITW.2013.6691311.

doi:

Aur´elien Garivier and Emilie Kaufmann. Optimal best
In 29th
arm identiﬁcation with ﬁxed conﬁdence.
Annual Conference on Learning Theory, volume 49
of Proceedings of Machine Learning Research, pages
998–1027. PMLR, 2016.

Kenny C. Gross and Keith E. Humenik.

Sequen-
tial probability ratio test for nuclear plant compo-
nent surveillance. Nuclear Technology, 93(2):131–
137, 1991. doi: 10.13182/NT91-A34499.

Identiﬁcation of

Bj¨orn Haddenhorst, Viktor Bengs,

and Eyke
the generalized
H¨ullermeier.
Condorcet winner in multi-dueling bandits.
In
A. Beygelzimer, Y. Dauphin, P. Liang, and
J. Wortman Vaughan, editors, Advances in Neu-
ral Information Processing Systems, 2021. URL
https://openreview.net/forum?id=omDF-uQ OZ.

Steven R. Howard, Aaditya Ramdas, Jon McAuliﬀe,
and Jasjeet Sekhon. Time-uniform Chernoﬀ bounds
via nonnegative supermartingales. Probability Sur-
veys, 17:257–317, 2020.

Kevin Jamieson, Matthew Malloy, Robert Nowak, and
S´ebastien Bubeck. lil’ ucb : An optimal exploration

algorithm for multi-armed bandits. In Proceedings of
The 27th Conference on Learning Theory, volume 35
of Proceedings of Machine Learning Research, pages
423–439. PMLR, 2014.

Shivaram Kalyanakrishnan, Ambuj Tewari, Peter
Auer, and Peter Stone. PAC subset selection in
stochastic multi-armed bandits.
In Proceedings of
the 29th International Conference on International
Conference on Machine Learning, page 227–234.
Omnipress, 2012.

Rajeeva Karandikar. Power and limitations of opinion
polls in the context of Indian parliamentary democ-
racy. In Special Proceeding of 20th Annual Confer-
ence of SSCA, pages 09 – 16. Society of Statistics,
Computer and Applications, 2018.

Rajeeva L. Karandikar, Clive Payne, and Yogendra
Yadav. Predicting the 1998 Indian parliamentary
election. Electoral Studies, 21(1):69–89, 2002. doi:
https://doi.org/10.1016/S0261-3794(00)00042-1.

Emilie Kaufmann and Shivaram Kalyanakrishnan. In-
formation complexity in bandit subset selection.
In Proceedings of the 26th Annual Conference on
Learning Theory, volume 30 of Proceedings of Ma-
chine Learning Research, pages 228–251. PMLR,
2013.

Gurmeet Singh Manku and Rajeev Motwani. Ap-
proximate frequency counts over data streams. In
Proceedings of the 28th International Conference on
Very Large Data Bases, page 346–357. VLDB En-
dowment, 2002.

Andreas Maurer and Massimiliano Pontil. Empirical
Bernstein bounds and sample variance penal-
In Proceedings of the 22nd Conference
ization.
on Learning Theory (COLT 2009), 2009. URL
http://www.cs.mcgill.ca/~colt2009/papers/
012.pdf#page=1.

Wolfgang Mulzer. Five proofs of Chernoﬀ’s bound
with applications. CoRR, abs/1801.03365, 2018.

Satoshi Nakamoto. Bitcoin: A peer-to-peer electronic
cash system, 2009. URL http://www.bitcoin.org/
bitcoin.pdf.

Emanuel Parzen. On estimation of a probability den-
sity function and mode. The Annals of Mathematical
Statistics, 33(3):1065–1076, 1962.

Clive Payne. Election forecasting in the UK: The
Euramerica, 33(1):193–234,

BBC’s experience.
2003.

Dhruti Shah, Tuhinangshu Choudhury, Nikhil Karam-
chandani, and Aditya Gopalan. Sequential mode es-
timation with oracle queries. In Proceedings of the
Thirty-Fourth AAAI Conference on Artiﬁcial Intel-
ligence, volume 34, pages 5644–5651. AAAI Press,
2020.

Jain, Shah, Gupta, Mehta, Nair, Vora, Khyalia, Das, Ribeiro, Kalyanakrishnan

A. Wald. Sequential Tests of Statistical Hypotheses.
The Annals of Mathematical Statistics, 16(2):117–
186, 1945.

Ian Waudby-Smith and Aaditya Ramdas. Conﬁdence
sequences for sampling without replacement. In Ad-
vances in Neural Information Processing Systems,
volume 33, pages 20204–20214. Curran Associates,
Inc., 2020.

Supplementary Material:
PAC Mode Estimation using PPR Martingale Conﬁdence Sequences

A IMPLEMENTATION DETAILS

We specify the various baseline algorithms used in our comparisons in sections 2 and 3. For actual code see
Appendix B.

Whether the implementation is 1v1 or 1vr, each algorithm has an atomic operation to maintain upper and lower
conﬁdence bounds on the parameter of a Bernoulli distribution.

• In the 1v1 approach, there is a separate Bernoulli associated with each pair of values vi and vj for 1 ≤ i <
δ
K−1 ,

). Each conﬁdence bound is drawn with mistake probability

pi
pi+pj

j ≤ k (hence a Bernoulli with mean
which ensures δ-correctness upon termination.

• In the 1vr approach, there is a separate Bernoulli associated with each value vi for i ≤ i ≤ k, taking ¬vi as
its other outcome (hence a Bernoulli with mean pi). To ensure δ-correctness upon termination, the mistake
probability used is δ
K .

The actual conﬁdence bounds used in diﬀerent algorithms are listed below. In 1v1, the total number of samples
(t) shown in the conﬁdence bounds is to be taken as the sum of the number of occurrences of the corresponding
(vi, vj) pair in question, while under 1vr, it is the total number of samples of P observed yet. Below we denote
the empirical mean after t samples ˆpt, and the mistake probability associated with the conﬁdence bound δ.

A.1 A1 (Shah et al., 2020)

We implement the algorithm as given in the original paper (Shah et al., 2020), which deﬁnes a conﬁdence width

(cid:114)

β(t, δ) =

2V tln(4t2/δ)
t

+

7ln(4t2/δ)
3(t − 1)

after t samples, where V t is the empirical variance of the samples. Lower and upper conﬁdence bounds are given
by ˆpt ± β(t, δ).

A.2 LUCB (Kalyanakrishnan et al., 2012) and KL-LUCB (Kaufmann and Kalyanakrishnan,

2013)

For LUCB (Kalyanakrishnan et al., 2012) and KL-LUCB (Kaufmann and Kalyanakrishnan, 2013), the “ex-
ploration rate” used is β(t, δ) = ln( 405.5t1.1
), which ensures δ correctness when a union bound over t is per-
formed (Kaufmann and Kalyanakrishnan, 2013).

δ

LUCB sets its lower and upper conﬁdence bounds as ˆpt ±
numerical computation to obtain the lower and upper conﬁdence bounds as given below.

. KL-LUCB obtains them by performing a

(cid:113) β(t,δ)
2t

KL-LUCB lower conﬁdence bound = min{q ∈ [0, ˆpt] : t × DKL(ˆpt||q) ≤ β(t, δ)};
KL-LUCB upper conﬁdence bound = max{q ∈ [ˆpt, 1] : t × DKL(ˆpt||q) ≤ β(t, δ)}.

Jain, Shah, Gupta, Mehta, Nair, Vora, Khyalia, Das, Ribeiro, Kalyanakrishnan

A.3 KL-SN (Garivier, 2013)

KL-SN (Garivier, 2013) uses a more sophisticated exploration rate so as to avoid a na¨ıve union bound over time.
We ﬁrst ﬁnd γ > 1 that satisﬁes the following equation, and thereafter set the exploration rate as given below.

2e2γe−γ = δ and β(t, δ) =

γ(1 + ln(γ))
(γ − 1)ln(γ)

ln(ln(t)) + γ.

Using the above exploration rate, the conﬁdence bounds are constructed in the same way as KL-LUCB.

KL-SN lower conﬁdence bound = min{q ∈ [0, ˆpt] : t × DKL(ˆpt||q) ≤ β(t, δ)};
KL-SN upper conﬁdence bound = max{q ∈ [ˆpt, 1] : t × DKL(ˆpt||q) ≤ β(t, δ)}.

B CODE DETAILS

The code used to run our experiments (from sections 2, 3, 5.1, and 5.2) is at https://github.com/rohanshah13/
pac mode estimation. Below we provide details on the running time of our algorithms on a couple of problem
instances, which are indicative of their relative order in general.

The results in Table 1 were obtained by performing each of the experiments for 100 iterations, with mistake
probability δ = 0.01. For the last 3 rows, for all algorithms except KL-SN 1v1, 200 iterations were used so as to
reduce the error bars. A running-time comparison of all the algorithms across the 100 runs is given in Table 3.
The runs were performed on an Intel Core i7-8750H CPU @ 2.20GHz processor.

Table 3: Average running time and one standard error of mode estimation algorithms across 100 iterations,
distribution P6 = (.35, .33, .04 × 8) from Table 1.

Algorithm Type Run time (seconds)

A1

.

KL-SN

PPR

1vr
1v1

1vr
1v1

1vr
1v1

9.01 ± 0.2
6.65 ± 0.17
169.47 ± 5.76
2.74 ± 0.10
51.5 ± 2.37
1.28 ± 0.06

The results in Table 2 and Table 4 were obtained by performing each of the experiments for 10 random seeds
and setting the mistake probability to 0.01. The sampling of the votes from each of the constituencies were done
with a batch size of 200. We used Google’s colaboratory services 4 for performing these experiments.

4https://colab.research.google.com/

PAC Mode Estimation using PPR Martingale Conﬁdence Sequences

C RELATING THE TERMINATION OF 1V1, 1VR, MD ALGORITHMS

In this section, we provide a proof of Lemma 2 and also comment on the plausible applicability of the result to
concentration bounds not covered by the lemma. Since the actual working is relatively verbose, we divide the
appendix into separate subsections. In Section C.1, we provide a relatively straightforward proof that the 1v1
versions of LUCB and A1 always terminate before their corresponding 1vr versions. In Section C.2, we work
towards proving that PPR-1v1 always terminates before PPR-1vr, and we reduce this claim to an inequality on
beta functions, which we conjecture to be true, and have veriﬁed empirically for a range of values. In Section C.3,
we consider the multi-dimensional version of the PPR martingale stopping rule (denoted PPR-MD), which serves
as an alternative for designing a stopping rule for mode estimation. This rule would use a Dirichlet prior (which
is the conjugate of the multinomial distribution), and is analogous to the mutlti-variate PPR considered in
Appendix C of Waudby-Smith and Ramdas (2020). We show that PPR-MD always stops after PPR-1v1 on
every run. In Section C.4, we provide a simple proof that shows A1-1v1 (Shah et al., 2020) always terminates
after PPR-1v1 on every run.

In summary, sections C.1, C.3 and C.4 complete the proof of Lemma 2, while Section C.2 concludes with a
conjecture, which if true, would mean the termination of PPR-1vr implies the termination of PPR-1v1.

C.1 LUCB and A1 Algorithms

LUCB: Suppose we run both the variants of LUCB algorithm with δ(cid:48) = δ
(in reality, we run the 1v1 with
LUCB-1v1 termination. The LUCB algorithm can diﬀerentiate some i, j ∈ {1, 2, .., K} such that i (cid:54)= j when

K , where δ is the mistake probability
K−1 ; so it is even better). We will show that LUCB-1vr termination implies

δ

(cid:115)

ˆpt
i −

(cid:1)

ln (cid:0) 405.5t1.1
δ(cid:48)
2t

≥ ˆpt

j +

(cid:115)

ln (cid:0) 405.5t1.1
δ(cid:48)
2t

(cid:1)

,

where ˆpt
diﬀerentiating i, j will be

i denotes the empirical mean corresponds to observation i. For 1v1, the corresponding condition for

ˆpt
i
ˆpt
i + ˆpt
j

−

(cid:118)
(cid:117)
(cid:117)
(cid:116)

ln

(cid:17)

ij )1.1

(cid:16) 405.5(st
δ(cid:48)
2st
ij

≥

ˆpt
j
ˆpt
i + ˆpt
j

+

(cid:118)
(cid:117)
(cid:117)
(cid:116)

ln

ij )1.1

(cid:16) 405.5(st
δ(cid:48)
2st
ij

(cid:17)

,

where st
1v1 termination, It is suﬃcient to prove that: If it is true that

ij denotes the total number of samples coming from pi and pj. Thus, to prove 1vr termination implies

(cid:115)

ˆpt
i −

(cid:1)

ln (cid:0) 405.5t1.1
δ(cid:48)
2t

≥ ˆpt

j +

(cid:115)

(cid:1)

ln (cid:0) 405.5t1.1
δ(cid:48)
2t

ˆpt
i
ˆpi + ˆpt
j

−

(cid:118)
(cid:117)
(cid:117)
(cid:116)

ln

(cid:17)

ij )1.1

(cid:16) 405.5(st
δ(cid:48)
2st
ij

≥

ˆpt
j
ˆpt
i + ˆpj

(cid:118)
(cid:117)
(cid:117)
(cid:116)

+

ln(

ij )1.1

405.5(st
δ(cid:48)
2st
ij

)

.

then, it is also true that

We have

i − ˆpt
ˆpt

j ≥ 2

(cid:115)

ln (cid:0) 405.5t1.1
δ(cid:48)
2t

(cid:1)

,

=⇒

ˆpt
i − ˆpt
j
(cid:115)

ln

(cid:18) 405.5(st
ij
δ(cid:48)
2st
ij

2(ˆpt

i + ˆpt
j)

≥

)1.1

(cid:19)

(cid:114)

ln

(cid:115)

(cid:17)

(cid:16) 405.5t1.1
δ(cid:48)
2t
(cid:18) 405.5(st
ij
δ(cid:48)
2st
ij

ln

(ˆpt

i + ˆpt
j)

.

)1.1

(cid:19)

Jain, Shah, Gupta, Mehta, Nair, Vora, Khyalia, Das, Ribeiro, Kalyanakrishnan

Using the fact that ˆpt

i + ˆpt

j =

st
ij
t and the above inequality, we get

ˆpt
i − ˆpt
j
(cid:115)

ln

(cid:18) 405.5(st
ij
δ(cid:48)
2st
ij

2(ˆpt

i + ˆpt
j)

≥

)1.1

(cid:19)

(cid:114)

(cid:1)

(cid:113)

δ(cid:48)

t ln (cid:0) 405.5t1.1
(cid:16) 405.5(st
δ(cid:48)

st
ij ln

ij )1.1

.

(cid:17)

Since

(cid:113)

t ln (cid:0) 405.5t1.1

δ(cid:48)

(cid:1) is an increasing function in t, and we know that st

ij ≤ t, we have

ˆpt
i − ˆpt
j
(cid:115)

ln

2(ˆpt

i + ˆpt
j)

(cid:18) 405.5(st
ij
δ(cid:48)
2st
ij

≥

(cid:113)

)1.1

(cid:19)

(cid:113)

t ln (cid:0) 405.5t1.1

δ(cid:48)

(cid:1)

st
ij ln(

405.5(st
δ(cid:48)

ij )1.1

)

≥ 1,

=⇒

ˆpt
i
ˆpt
i + ˆpt
j

−

(cid:118)
(cid:117)
(cid:117)
(cid:116)

ln

(cid:17)

ij )1.1

(cid:16) 405.5(st
δ(cid:48)
2st
ij

≥

ˆpt
j
ˆpt
i + ˆpt
j

+

(cid:118)
(cid:117)
(cid:117)
(cid:116)

ln

ij )1.1

(cid:16) 405.5(st
δ(cid:48)
2st
ij

(cid:17)

.

Hence, we showed that the 1v1 variant of LUCB algorithm always terminates before its 1vr variant.

A1 Algorithm: Suppose we run both the variants of A1 algorithm with δ(cid:48) = δ
take probability (As above, we actually run 1v1 with δ
diﬀerentiates between i, j ∈ {1, 2, .., K} such that i (cid:54)= j and i is the winner when

K , where δ is the mis-
K−1 , so 1v1 will terminate even faster). The A1 algorithm

(cid:115)

ˆpt
i −

2Vt(Z i) ln (cid:0) 4t2
t

δ(cid:48)

(cid:1)

−

7 ln

(cid:17)

(cid:16) 4t2
δ(cid:48)

3(t − 1)

(cid:115)

2Vt(Z j) ln (cid:0) 4t2
t

δ(cid:48)

(cid:1)

+

7 ln

(cid:17)

(cid:16) 4t2
δ(cid:48)

3(t − 1)

,

≥ ˆpt

j +

where

Vt(Z i) =

st
i(t − st
i)
t(t − 1)

,

in which st
between i, j in A1 1v1 will be:

i denotes the total number of samples coming from vi. Similarly, the condition for there to be a winner

(cid:118)
(cid:117)
(cid:117)
(cid:116)

−

ˆpt
i
i + ˆpt
ˆpt
j

2Vst

ij

(Z i) ln

(cid:16) 4(st
ij )2
δ(cid:48)

(cid:17)

st
ij

−

7 ln

(cid:16) 4(st
ij )2
δ(cid:48)
ij − 1)

(cid:17)

≥

ˆpt
j
i + ˆpt
ˆpt
j

(cid:118)
(cid:117)
(cid:117)
(cid:116)

+

2Vst

ij

(Z j) ln

(cid:16) 4(st
ij )2
δ(cid:48)

(cid:17)

st
ij

+

3(st

7 ln

(cid:16) 4(st
ij )2
δ(cid:48)
ij − 1)

(cid:17)

,

3(st

i + st

ij = st
where st
samples coming from vi.
To prove 1vr termination implies 1v1 termination, It is suﬃcient to prove that: If it is true that

j denotes the total number of samples coming from vi and vj, and st

i denotes the number of

(cid:115)

ˆpt
i −

2Vt(Z i) ln (cid:0) 4t2
t

δ(cid:48)

(cid:1)

−

7 ln

(cid:17)

(cid:16) 4t2
δ(cid:48)

3(t − 1)

(cid:115)

2Vt(Z j) ln (cid:0) 4t2
t

δ(cid:48)

(cid:1)

+

7 ln

(cid:17)

(cid:16) 4t2
δ(cid:48)

3(t − 1)

≥ ˆpt

j +

then, it is also true that

(cid:118)
(cid:117)
(cid:117)
(cid:116)

−

ˆpt
i
i + ˆpt
ˆpt
j

2Vst

ij

(Z i) ln

(cid:16) 4(st
ij )2
δ(cid:48)

(cid:17)

st
ij

−

7 ln

(cid:16) 4(st
ij )2
δ(cid:48)
ij − 1)

(cid:17)

≥

ˆpt
j
i + ˆpt
ˆpt
j

(cid:118)
(cid:117)
(cid:117)
(cid:116)

+

2Vst

ij

(Z j) ln

(cid:16) 4(st
ij )2
δ(cid:48)

(cid:17)

st
ij

+

3(st

(cid:17)

7 ln

(cid:16) 4(st
ij )2
δ(cid:48)
ij − 1)

3(st

PAC Mode Estimation using PPR Martingale Conﬁdence Sequences

Using the same steps as in LUCB, we get



(cid:115)

2(ˆpt

i + ˆpt
j)




2Vst
ij

(Zi) ln

)2

(cid:19)

(cid:18) 4(st
ij
δ(cid:48)

st
ij

i − ˆpt
ˆpt
j
(cid:115)

+

2Vst
ij

(Zj ) ln

)2

(cid:19)

(cid:18) 4(st
ij
δ(cid:48)

st
ij

14 ln

)2

(cid:18) 4(st
ij
δ(cid:48)
ij −1)

3(st

+

(cid:18)(cid:113)

2tVt(Z i) ln (cid:0) 4t2

δ(cid:48)

(cid:1) +

(cid:113)

2tVt(Z j) ln (cid:0) 4t2

δ(cid:48)

(cid:1) +

(cid:17)

(cid:19)

14t ln

(cid:16) 4t2
δ(cid:48)
3(t−1)

≥



(cid:114)



2st

ijVst

ij

(Z i) ln

(cid:16) 4(st
ij )2
δ(cid:48)

(cid:17)

+

(cid:114)

2st

ijVst

ij

(Z j) ln

(cid:16) 4(st
ij )2
δ(cid:48)

(cid:17)

+

14st

(cid:18) 4(st
ij
δ(cid:48)
ij −1)

ij ln
3(st



(cid:19)




)2

(cid:19)



.



We need to prove that the above expression is ≥ 1. We note that

increasing functions in t. Let f (t) =

(cid:17)

14t ln

(cid:16) 4t2
δ(cid:48)
3(t−1)

. We write down the diﬀerential:
(cid:18) 4t2
δ

(cid:19)

f (cid:48)(t) ∝ 2(t − 1) − ln

.

(cid:113)

2tVt(Z i) ln (cid:0) 4t2

δ(cid:48)

(cid:1),

(cid:113)

2tVt(Z j) ln (cid:0) 4t2

δ(cid:48)

(cid:1) are

(cid:17)

(cid:16) 4t2
δ

Thus, for t such that 2(t − 1) ≥ ln
, f (t) is an increasing function. We now look at the minimum value
of st
i possible, and we show that st
i. We proceed
to look at a scenario in which lowest possible value for st
i is observed. Consider the case in which we observe
tˆpt
i samples with value vi (thus, the empirical mode is ˆpt
i for vi). Let’s assume that A1 1vr algorithm declares
i as the mode after observing tˆpt
i samples of vi. We assume that j has 0 samples, and then ﬁnd the value of t
that arises. Termination of A1 algorithm after t observations suggests (we ignore the empirical mean term, as it
makes our lower bound on t only bigger):

i is always such that f (t) is an increasing function for t ≥ st

7 ln

(cid:17)

(cid:16) 4t2
δ(cid:48)

3(t − 1)

ˆpt
i −

7 ln

(cid:17)

(cid:16) 4t2
δ(cid:48)

3(t − 1)

≥ 0 +

=⇒ ˆpt

i ≥

(cid:17)

14 ln

(cid:16) 4t2
δ(cid:48)
3(t − 1)

.

Using the fact that tˆpt

i = st

i, we arrive at:

3ˆpt

i(t − 1)
14
i − ˆpt
i)
14

≥ ln

3(st

≥ ln

(cid:19)

(cid:18) 4t2
δ(cid:48)

(cid:19)

(cid:18) 4t2
δ(cid:48)

≥ ln

(cid:18) 4(st
i)2
δ(cid:48)

(cid:19)

.

=⇒

To show 2(ti − 1) ≥ ln
mild assumption that st
as the mode. Under this assumption, we get:

, it is suﬃcient to show that 2(st

i − 1) ≥ 3
i). For proving this, we make a
i ≥ 2 as practically A1 algorithm would need more than 2 samples from vi to declare vi

i − ˆpt

14 (st

(cid:17)

(cid:16) 4(st
i)2
δ

2(st

i − 1) −

3
14

Hence, we note that

(st

i − ˆpt

i) =

25
14

st
i − 2 +

3
14

ˆpt
i ≥

11
7

+

3
14

ˆpt
i > 0.

(cid:18)(cid:113)

2tVt(Z i) ln (cid:0) 4t2

δ(cid:48)

(cid:1) +

(cid:113)

2tVt(Z j) ln (cid:0) 4t2

δ(cid:48)

(cid:1) +

(cid:17)

(cid:19)

14t ln

(cid:16) 4t2
δ(cid:48)
3(t−1)





(cid:114)

2st

ijVst

ij

(Z i) ln

(cid:16) 4(st
ij )2
δ(cid:48)

(cid:17)

+

(cid:114)

2st

ijVst

ij

(Z j) ln

(cid:16) 4(st
ij )2
δ(cid:48)

(cid:17)

+

14st

(cid:18) 4(st
ij
δ(cid:48)
ij −1)

ij ln
3(st

)2

(cid:19)



≥ 1



since all the 3 terms are increasing, and thus A1 1vr always stops after A1 1v1.

Jain, Shah, Gupta, Mehta, Nair, Vora, Khyalia, Das, Ribeiro, Kalyanakrishnan

C.2 PPR

Suppose party i, j have conﬁdence sequences which do not intersect at some time t. Let the number of samples
from party i be st
i > st
j.
Let f be the number of samples from parties which are not i or j. Then, the condition for party i, j having
conﬁdence sequences which do not intersect can be written as

i, and the number of samples from party j be st

j, and without loss of generality assume st

∃ θ ∈

(cid:21)

(cid:20) st
j
t

,

st
i
t

1

such that

Beta (θ; st

i + 1, t − st

1

Beta (cid:0)θ; st

j + 1, t − st

≥

i + 1)
j + 1(cid:1) ≥

K
δ
K
δ

,

.

Parties i, j have a winner between them in PPR-1v1 when

Beta (cid:0) 1

2 ; st

1
i + 1, st

j + 1(cid:1) ≥

K − 1
δ

.

We will attempt to show that party i, j having disjoint conﬁdence sequences in PPR-1vr implies party i, j having
a winner between them in PPR-1v1. This will imply that PPR-1v1 always terminates before PPR-1vr. Noting
that

Beta (cid:0)θ; st

i, st
j

(cid:1) =

where B(st

i, st

j) =

θst

j −1

i−1(1 − θ)st
i, st
j)
i)Γ(st
j)
i + st
j)

B(st
Γ(st
Γ(st

,

we are given that

And we wish to show that

We denote

δ
K

δ
K

≥

≥

δ
K − 1

≥

i

θst
i (1 − θ)t−st
i + 1, t − st
B(st
θst
j (1 − θ)t−st
B(st
j + 1, t − st

j

i + 1)

j + 1)

,

.

i+st

2st

j B(st

1
i + 1, st

.

j + 1)

i

i (1 − θ)t−st
θst
i + 1, t − st
B(st
j (1 − θ)t−st
θst
B(st
j + 1, t − st

j

i + 1)

j + 1)

= L1(θ),

= L2(θ).

K − 1

K2st

i+st

j B(st

i + 1, st

j + 1)

.

Thus, it is suﬃcient to show that, ∀ θ ∈

(cid:104) st
t , st

i
t

j

(cid:105)

,

max (L1(θ), L2(θ)) ≥

Consider

L(cid:48)

1(θ) ∝ st
L(cid:48)

i(1 − θ) − (t − st
1(θ) ∝ st
i − tθ

i)θ,

=⇒ L(cid:48)

1(θ) ≥ 0 ∀ θ ≤

st
i
t

.

PAC Mode Estimation using PPR Martingale Conﬁdence Sequences

Similarly, consider

L(cid:48)

2(θ) ∝ st
L(cid:48)

j(1 − θ) − (t − st
j − tθ,

2(θ) ∝ st

j)θ

=⇒ L(cid:48)

2(θ) ≤ 0 ∀ θ ≥

st
j
t

.

Thus, we note that in our range of θ, L1(θ) is increasing and L2(θ) is decreasing. Consider θ∗ such that L1(θ∗) =
L2(θ∗). We will prove that considering only the value θ = θ∗ is suﬃcient to prove that

max (L1(θ), L2(θ)) ≥

K − 1

K2st

i+st

j B(st

i + 1, st

j + 1)

for the entire range of θ ∈
gives us the implicit equation

(cid:104) st
t , st

i
t

j

(cid:105)

. First, we prove that θ∗ lies in this range itself. The equation L1(θ∗) = L2(θ∗)

(cid:18) θ∗

(cid:19)

1 − θ∗

(cid:34)

=

i!(t − st
st
st
j!(t − st

i)!
j)!

(cid:35) 1
st
i

−st
j

.

We ﬁrst prove θ∗ ≥

st
j
t . This is the same as showing that

θ∗
1−θ∗ ≥

st
j
t−st
j

. We show that

st
j
t − st
j
i−st

(cid:33)st

j

(cid:32)

⇔

st
j
t − st
j

(cid:34)

≤

st
i!(t − st
j!(t − st
st

i)!
j)!

(cid:35) 1
st
i

−st
j

(cid:34)

≤

(st
(st

j + 1)(st
i + 1)(st

j + 2)...(t − st
i)
i + 2)...(t − st
j)

(cid:35)

.

To prove the above inequality, ﬁrst note that

st
j
t − st
j

≤

st
j + 1
t − st
j

.

Hence, it is suﬃcient now to show

(cid:32)

st
j
t − st
j

(cid:33)st

i−st

j −1

(cid:34)

≤

(st
i + 1)(st

j + 2)...(t − st
i)
i + 2)...(t − st

(st

j − 1)

(cid:35)

.

We know that st
hence it is true. Otherwise, if st

j < st

i. Suppose that st

j = st
i − 1, we have

j < st

i − 1; then the above equation has both LHS = 1 and RHS = 1,

And proceeding in a similar fashion, we can show inductively that

st
j
t − st
j

≤

st
j + 2
t − st

j − 1

.

In the same way, we can also show that

(cid:34)

≤

st
j
t − st
j

st
i!(t − st
j!(t − st
st

i)!
j)!

(cid:35) 1
st
i

−st
j

(cid:34)

≥

st
i
t − st
i

st
i!(t − st
j!(t − st
st

i)!
j)!

(cid:35) 1
st
i

−st
j

.

.

Jain, Shah, Gupta, Mehta, Nair, Vora, Khyalia, Das, Ribeiro, Kalyanakrishnan

This proves that θ∗ ∈

(cid:104) st
t , st

i
t

j

(cid:105)
. Moreover, since L1(θ) is increasing and L2(θ) is decreasing, we have that

L1(θ∗) ≤ L1(θ), θ ≥ θ∗,
L2(θ∗) ≥ L2(θ), θ ≤ θ∗.

Combining the above two equations and noting that L1(θ∗) = L2(θ∗), we get that

max (L1(θ∗), L2(θ∗)) ≤ max (L1(θ), L2(θ)) , θ ∈

(cid:20) st
j
t

,

st
i
t

(cid:21)

.

Thus, if we show that

max (L1(θ∗), L2(θ∗)) ≥

we are done. We need to show that

(θ∗)st
B(st

i (1 − θ∗)t−st
i + 1)

i + 1, t − st

i

≥

K − 1

K2st

i+st

j B(st

i + 1, st

j + 1)

K − 1

i+st

j B(st

K2st
(cid:34)

,

j + 1)

i + 1, st
(cid:35) 1
st
i

−st
j

.

where

(cid:18) θ∗

(cid:19)

1 − θ∗

=

st
i!(t − st
st
j!(t − st

i)!
j)!

We ran computer simulations to verify the correctness of the above equation exhaustively for all values of st
in [1, 200]. Thus, we conjecture that the above equation is true for all st

j, t > 0 such that st

i > st
j.

i, st

i, st

j, t

C.3 Comparison with Multidimensional PPR (PPR-MD)

The Dirichlet distribution is the conjugate prior of the Multinomial distribution, and hence can be used to
implement a PPR-based stopping rule, which is similar in principle to the multi-variate PPR given in Appendix
C in Waudby-Smith and Ramdas (2020). In speciﬁc, the PDF of the Dirichlet distribution is given as

pdf (¯p) =

B(α) =

,

(cid:81)K

i=1 ¯pαi−1
i
B(α)
i Γ(αi)
i αi)

(cid:81)
Γ((cid:80)

.

At each time-step t, we maintain a conﬁdence set C t with ¯p ∈ [0, 1]k, such that C t def={¯p : Rt(¯p) < 1
δ }. We stop at
time t when all the ¯p ∈ C t have the same unique mode. We call this stopping rule PPR-MD (Multi-dimensional).
We note that, with prior α = [1, ..., 1],

Rt(¯p) =

1
(K − 1)!

×

B(α)
i=1 ¯pαi−1

i

(cid:81)K

We will show that if the PPR-MD stopping rule stops at a point, then PPR-1v1 would also have stopped. In
other words, PPR-MD stopping implies PPR-1v1 stopping on every run.

Let the number of observations from v1, v2, ..., vK until timestep t be st
2 ≥ ... ≥ st
1 ≥ st
order them such that st
1+ ˆpt
, ˆpt
1+ ˆpt
PPR-MD stops, x1 = ( ˆpt
, ˆpt
2
2

2, ..., st
K. We denote the empirical means as ˆpt

K) must not lie in the conﬁdence set. Explicitly,

3, ..., ˆpt

1, st

2

2

i

K. Without loss of generality,
i = st
t . We note that, when

(cid:16) ˆpt

1+ ˆpt
2

2

(cid:17)st

1+st

2

(ˆpt
2!...st

3)st
K!

1!st
st

3 ...ˆpst

K
K

≤

δ(st

1 + st

2 + . . . + st
(K − 1)!

K + K − 1)!

.

We divide our proof into two parts. First, we show the implication for K = 3, and then we extend our proof to
general K ≥ 3.

PAC Mode Estimation using PPR Martingale Conﬁdence Sequences

C.3.1 Proof for K = 3

When K = 3, PPR-1v1 stops when

(cid:0) 1
2

(cid:1)st

1+st

2 + 1)!

2 (st
1 + st
1!st
st
2!

≤

δ
2

and PPR-MD stopping implies that (using that K = 3)

(cid:17)st

1+st

2

(cid:16) ˆpt

1+ ˆpt
2

2

If we can show that

(ˆpt

3)st
1!st
st

1 + st
3 (st
2!st
3!

2 + st

3 + 2)!

≤

δ
2

.

(cid:17)st

1+st

2

(cid:16) ˆpt

1+ ˆpt
2

2

(ˆpt

3)st
1!st
st

1 + st
3 (st
2!st
3!

2 + st

3 + 2)!

(cid:0) 1
2

≥

(cid:1)st

1+st

1 + st
2 (st
1!st
st
2!

2 + 1)!

,

2, st

1, st

for all st

3 such that st
We note that this is equivalent to showing

2 ≥ st

1 ≥ st

3, then PPR-MD stopping will imply PPR-1v1 stopping for K = 3.

(ˆpt

1 + ˆpt

2)st

1+st

2 (ˆpt

3)st

3 ≥

3

⇔

(st
1 + st
(st
1 + st
xxyy
(x + y)x+y ≥

2)st
2 + st

3)st
2+st
3

2(st
1+st

1+st
3)st
(x + 1)!y!
(x + y + 2)!

⇔

2 + 1)!
3 + 2)!

1 + st
2 + st

st
3!(st
(st
1 + st
st
3!(st
(st
1 + st

≥

1 + st
2 + st

2 + 1)!
3 + 2)!

, where x = st

1 + st

2, y = st
3

for all x, y, such that y ≥ 0, x ≥ 2y. Note that, when y = 0, the yy term tends to 1; hence, the above statement
is true. Subsequently, we will consider y > 0. Let

We want to show that

f (x, y) =

xxyy(x + 2)....(x + y + 2)
y!(x + y)x+y

f (x, y) ≥ 1 when y > 0, x ≥ 2y.

We show this in two steps. First, we show that f (x, y) is an increasing function of x when y is ﬁxed. Since f (x, y)
is increasing in x, we then choose the minimum value of x with ﬁxed y (i.e, x = 2y) and prove that f (2y, y) ≥ 1
for all y > 0, which implies that f (x, y) ≥ 1 for all y > 0, x ≥ 2y.

Showing that f (x, y) is Increasing in x

Recapping, we need to show that

f (x, y) =

yy(x + 2)...(x + y + 2)
y!(x + y)y (cid:0)1 + y

(cid:1)x

x

is increasing with respect to x while keeping y ﬁxed. We write down the partial derivative of ln(f (x, y)) with
respect to x as follows:

∂ ln(f (x, y))
∂x

=

y
(cid:88)

j=2

1
x + j

−

y
x + y

+

1
x + y + 2

+

1
x + y + 1

+ 1 −

x
x + y

− ln

(cid:19)

(cid:18) x + y
x

=⇒

∂ ln(f (x, y))
∂x

=

y
(cid:88)

j=2

1
x + j

+

1
x + y + 2

+

1
x + y + 1

− ln

(cid:18) x + y
x

(cid:19)

.

Jain, Shah, Gupta, Mehta, Nair, Vora, Khyalia, Das, Ribeiro, Kalyanakrishnan

We note that

1
x + j

≥

=⇒

Thus, if we show that

(cid:90) w=j+1

1
x + w

dw = ln

(cid:19)

(cid:18) x + j + 1
x + j
(cid:19)

1
x + j

≥ ln

(cid:18) x + y + 1
x + 2

.

w=j
y
(cid:88)

j=2

g(x, y) = ln

(cid:19)

(cid:18) x + y + 1
x + 2

+

1
x + y + 2

+

1
x + y + 1

− ln

(cid:19)

(cid:18) x + y
x

≥ 0 when y > 0, x ≥ 2y

this is suﬃcient to show that ∂ ln(f (x,y))

∂x

≥ 0 when y ≥ 0, x ≥ 2y. We combine terms to get

g(x, y) =

1
x + y + 2

+

1
x + y + 1

− ln

(cid:18) (x + y)(x + 2)
(x + y + 1)x

(cid:19)

We will now show that keeping x ﬁxed, g(x, y) is a decreasing function of y. Hence, it will be suﬃcient to consider
the maximum possible value of y to show that g(x, y) ≥ 0. We see that

∂g(x, y)
∂y

= −

(cid:18) 3x3 + x2(9y + 11) + x(9y2 + 22y + 13) + 3y3 + 11y2 + 13y + 4
(x + y)(x + y + 1)2(x + y + 2)2

(cid:19)

=⇒

∂g(x, y)
∂y

≤ 0, y > 0, x ≥ 2y.

Hence, g(x, y) is decreasing in y. Thus, it is enough to consider the maximal possible value of y for a ﬁxed x to
show that g(x, y) ≥ 0. For a ﬁxed x, the maximum possible y is x

2 . Hence, we have that

h(x) = g(x, x/2) =

2
3x + 4

+

2
3x + 2

− ln

(cid:18) 3(x + 2)
(3x + 2)

(cid:19)

,

and we need to show that h(x) ≥ 0 when x > 0. We have

h(cid:48)(x) = −

(cid:18)

8(9x2 + 21x + 14)
(x + 2)(3x + 2)2(3x + 4)2

(cid:19)

=⇒ h(cid:48)(x) ≤ 0 when x > 0.

Thus, h(x) is a decreasing function of x. We see that, as x → ∞, h(x) → 0. Thus, h(x) ≥ 0 when x > 0. This
means that g(x, x/2) ≥ 0 =⇒ g(x, y) ≥ 0 =⇒ ∂f (x,y)
∂x ≥ 0 which implies that f (x, y) is an increasing function
of x.

Showing that f (x, y) ≥ 1

Since f (x, y) is an increasing function of x when y is ﬁxed,
the minimum possible value of x. We note that the minimum possible value of x is x = 2y. We have

it is suﬃcient to show that f (x, y) ≥ 1 for

f (2y, y) − 1 =

yy(2y + 2)...(3y + 2) − y!(3y)y (cid:0) 3
y!(3y)y (cid:0) 3

(cid:1)2y

2

2

(cid:1)2y

.

The denominator is positive. We show that the numerator is positive for all y > 0.

h(2y, y) = yy(2y + 2)...(3y + 2) − y!(3y)y

=⇒ h(2y, y) = yy

(cid:18)

(2y + 2)...(3y + 2) − y!

(cid:19)2y

(cid:18) 3
2
(cid:18) 27
4

(cid:19)y(cid:19)

.

PAC Mode Estimation using PPR Martingale Conﬁdence Sequences

Let

c(y) = (2y + 2)...(3y + 2) − y!

(cid:19)y

.

(cid:18) 27
4

We will show that c(y) > 0 when y > 0. First, We note that c(1) > 0. We now use induction; assume that
c(y) > 0 for some y. We will show that c(y + 1) > 0. We have that

c(y) = (2y + 2)...(3y + 2) − y!

(cid:18) 27
4
(2y + 2)...(3y + 2)
y! (cid:0) 27

(cid:1)y

4

(cid:19)y

> 0

> 1.

=⇒ d(y) =

We want to show that

c(y + 1) = (2y + 4)...(3y + 5) − (y + 1)!

(cid:19)y+1

(cid:18) 27
4

> 0

⇔ d(y + 1) =

We will show that

(2y + 4)...(3y + 5)
(y + 1)! (cid:0) 27

(cid:1)y+1 > 1.

4

d(y + 1)
d(y)

=

(3y + 3)(3y + 4)(3y + 5)
(2y + 2)(2y + 3)(y + 1) 27
4

> 1

⇔ (3y + 3)(3y + 4)(3y + 5) − (2y + 2)(2y + 3)(y + 1)

27
4

> 0.

We note that the last expression turns out to be a quadratic in y which is always positive when y > 0. Hence,
d(y+1)
d(y) > 1 when y > 0. Thus, since by our induction hypothesis we have that d(y) > 1, y > 0 and by our proof
above we have that d(y+1)
d(y) > 1, y > 0, we note that this implies d(y + 1) = d(y) d(y+1)
d(y) > 1, which is what we
wanted to show in our induction step. Hence, proved.

C.3.2 Extending to General K

For a general K, PPR-1v1 stops when

(cid:0) 1
2

(cid:1)st

1+st

2 + 1)!

2 (st
1 + st
1!st
st
2!

≤

δ
K − 1

and when PPR-MD stops, it is true that (by choosing x =

(cid:16) ˆpt

1+ ˆpt
2

2

, ˆpt

1+ ˆpt
2

2

(cid:17)

, ˆpt

3, ...ˆpt
K

(cid:17)st

1+st

2

(cid:16) ˆpt

1+ ˆpt
2

2

(ˆpt

3)st

3(ˆpt

4)st

4...(ˆpt

K)st
1!st
st

K (st
3!st
2!st

1 + st
4!...st

2 + st
K!

3 + st

4 + ... + st

K + K − 1)!

≤

δ
(K − 1)!

.

If we show that

(K − 2)!

(cid:17)st

1+st

2

(cid:16) ˆpt

1+ ˆpt
2

2

(ˆpt

3)st

3(ˆpt

2 + st

3 + st

4 + ... + st

K + K − 1)!

≥

1 + st
K!

K)st
4)st
4..(ˆpt
K (st
2!st
st
3!st
1!st
4!...st
1+st
(cid:1)st
1 + st
2 (st
1!st
st
2!

(cid:0) 1
2

2 + 1)!

,

Jain, Shah, Gupta, Mehta, Nair, Vora, Khyalia, Das, Ribeiro, Kalyanakrishnan

then PPR-MD stopping will imply PPR-1v1 stopping. We will show this by using a sequence of inequalities as
follows;

(K − 2)!

(cid:17)st

1+st

2

(cid:16) ˆpt

1+ ˆpt
2

2

(ˆpt

3)st

3(ˆpt

2 + st

3 + st

4 + ... + st

K + K − 1)!

(K − 3)!

(cid:17)st

1+st

2

(cid:16) ˆpt

1+ ˆpt
2

2

(ˆpt

3)st

3 ...(ˆpt

1 + st

2 + st

3 + ... + st

K−1 + K − 2)!

≥

≥ ... ≥

K (st
4!...st

1 + st
K!

4)st
K)st
4..(ˆpt
1!st
3!st
2!st
st
K−1)st
2!st
1!st
st
1+st
(cid:1)st
(cid:0) 1
2

K−1(st
3!...st

K−1!
1 + st
2 (st
1!st
st
2!

2 + 1)!

.

We will show the ﬁrst inequality, and the rest follow the same structure except the last (for K = 3), which was
already shown in the subsection above. Note that the last inequality does not follow the same structure due to
the lack of ˆpt
term in the RHS, since this does not show up in the PPR-1v1 expression. Showing the ﬁrst
inequality is equivalent to showing

1+ ˆpt
2

2

(K − 2)(ˆpt

K)st

K ≥

K

K)st
(st
2 + ... + st

(st
1 + st
(x + K − 2)!y!
(x + y + K − 1)!

(st

1 + st
(st

2 + ... + st

2 + ... + st
1 + st
(st
1 + st
(st

1 + st

≥

K)st

K

K−1 + K − 2)!st
K + K − 1)!

K!

2 + ... + st

K−1 + K − 2)!st
K + K − 1)!

K!

2 + ... + st

, x ≥ (K − 1)y, where x = st

1 + st

2 + .. + st

K−1, y = st

K.

⇔ (K − 2)

yy
(x + y)y ≥

⇔ (K − 2)

We already know that

xxyy
(x + y)x+y ≥
from our proof for K = 3. In this equation, mutltiplying the LHS by (K−2)(x+y)x
(x+2)...(x+K−2)
1, and multiplying the RHS by

(x + 1)!y!
(x + y + 2)!

, where x ≥ 2y, y > 0

xx

which is a factor greater than
(x+y+3)...(x+y+K−1) , which is a factor lesser than 1, we have the inequality that

xx
(x + y)y ≥

(x + K − 2)!y!
(x + y + K − 1)!

, x ≥ 2y

which directly implies what we wanted to show. Hence, even in the case of K parties, PPR-MD stopping implies
PPR-1v1 stopping.

C.4 PPR-1v1 and A1-1v1

Both PPR-1v1 and A1-1v1 reduce to a single application of PPR-Bernoulli and A1, respectively, between vﬁrst(t)
and vsecond(t). Thus it suﬃces to prove G(PPR-Bernoulli, A1, X, δ), assuming X is generated by distribution P
over two values.

After any arbitrary t timesteps, without loss of generality, assume ﬁrst(t) = 1 and ˆpt
following:

1 = st

1

t . We prove the

¬T ERM t
T ERM t

P P R =⇒ ¬T ERM t
P P R ⇐⇒ Beta(0.5; st

A1, where

1 + 1, t − st

1 + 1) ≤ δ, and

T ERM t

A1 ⇐⇒ ˆpt

1 − β(t, δ) > 0.5,

(cid:114)

β(t, δ) =

2V tln(4t2/δ)
t

+

7ln(4t2/δ)
3(t − 1)

.

From Lemma 6 (see Appendix D), ¬T ERM t

P P R implies,

tKL (cid:0)ˆpt

1||0.5(cid:1) < ln

(cid:18) t + 1
δ

(cid:19)

.

PAC Mode Estimation using PPR Martingale Conﬁdence Sequences

Applying Pinsker’s inequality, we get,

0.50 ≤ ˆpt
Case 1:
Rearranging equation 2 gives us,

1 ≤ 0.77

t

(2(ˆpt

1 − 0.5))2
2 ln(2)

< ln

(cid:18) t + 1
δ

(cid:19)

.

(2)

ˆpt
1 < 0.5 +

(cid:115)

(cid:115)

=⇒ ˆpt

1 < 0.5 +

ln(2)
2t

ln

(cid:19)

(cid:18) t + 1
δ

2ˆpt

1(1 − ˆpt

1)ln(4t2/δ)

(t − 1)

(cid:32)(cid:115)

=⇒ ˆpt

1 < 0.5 +

2ˆpt

1(1 − ˆpt

1)ln(4t2/δ)

(t − 1)

1 < 0.5 + β(t, δ)

=⇒ ˆpt
=⇒ ¬T ERM t

A1.

Case 2:
The lower conﬁdence bound given by the A1 algorithm is

1 ≤ 1

0.77 < ˆpt

1 − β(t, δ) = ˆpt
ˆpt

1 −

(cid:32)(cid:114)

=⇒ ˆpt

1 − β(t, δ) ≤ ˆpt

1 −

=⇒ ˆpt

1 − β(t, δ) ≤ ˆpt

1 −

2V tln(4t2/δ)
t
(cid:18) t + 1
δ

(cid:19)

ln

7
3
14(0.5 − ˆpt
3 ln(2)

1)2

(cid:33)

+

7ln(4t2/δ)
3(t − 1)

(cid:33)

+

7ln(4t2/δ)
3(t − 1)

(using equation 2).

Since f (ˆpt
implies

1) = ˆpt

1)2
1 − 14(0.5− ˆpt

3 ln 2

is a decreasing function for ˆpt

1 ≥ 0.77, and f (0.77) = 0.28, we have ¬T ERM t

P P R

ˆpt
1 − β(t, δ) ≤ 0.5

=⇒ ¬T ERM t

A1.

Jain, Shah, Gupta, Mehta, Nair, Vora, Khyalia, Das, Ribeiro, Kalyanakrishnan

D PROOF OF ASYMPTOTIC OPTIMALITY OF PPR-1v1

We present a full proof of Theorem 3. Consider a categorical distribution P(p, v, K) having parameters p1 >
p2 . . . ≥ pK for K ≥ 2. We show that PPR-1v1 asymptotically matches the lower bound given in Theorem 1
i = pi. Observe that although P (cid:48) itself does not
and ∀i > 2, p(cid:48)
when P (cid:48) is deﬁned as follows: p(cid:48)
have v2 as its mode, it yields the supremum deﬁned in LB(P, δ) among distributions that do have v2 as mode.
Hence it is a suﬃcient choice for our proof, which proceeds in two major steps.

2 = p1+p2

1 = p(cid:48)

2

1. In subsection D.1 we ﬁx arbitrary (cid:15) > 0 as input, and deﬁne T ∗ such that lim
δ→0

KL(P||P (cid:48)) . Then
assuming we have T ∗ samples in total, we lower bound the number of samples obtained for each of the 1v1
tests involving v1.

T ∗
ln(1/δ) = (1+(cid:15))

2. In subsection D.2, we show that the number of samples obtained for each test is suﬃcient for PPR-Bernoulli
to terminate and declare v1 as the winner. The arguments in this subsection are along the lines of those
used by Garivier and Kaufmann (2016).

D.1 Lower Bounding the Number of Samples Obtained for each 1v1 Test

Given arbitrary (cid:15) > 0 and α > 0, let us introduce T ∗:

T ∗ def=

1 + (cid:15)
KL(P||P (cid:48))

(cid:18)

ln

(cid:19)

(cid:18) K − 1
δ

(cid:19)

+ max

j∈{2,3...K}

Hj

+

K
1 − Kξ

N (P, ξ, (cid:15), α) +

K
1 − Kξ

T0(γ).

We start by deﬁning the notation used:

• ξ(P, (cid:15)) : 0 < ξ < min |p1 − p2|/4 is small enough so that

(cid:16)

1 − ξ

p1+pK

(cid:17)

(1 + (cid:15)

2 ) > 1.

• γ((cid:15)): γ > 0 is small enough so that (1 + γ)(1 + (cid:15)

2 ) < (1 + (cid:15)).

• T0(γ) : T0 is large enough such that ∀t ≥ T0(γ), t1+γ > t + 1.

• Hj = ln

(cid:18)
e

(cid:16) (1+(cid:15)/2)(1−ξ/(p1+pj )
KL(p1/(p1+pj )||0.5)

(cid:17)1+γ(cid:19)

(cid:18)

(cid:18)

+ ln

ln

K−1
δ

(cid:16) (1+(cid:15)/2)(1−ξ/(p1+pj )
KL(p1/(p1+pj )||0.5)

(cid:17)1+γ(cid:19)(cid:19)

.

• ξ(cid:48)(P, ξ, (cid:15)): ξ(cid:48) is such that |ˆpt

i − pi| < ξ(cid:48) =⇒ D

(cid:16) ˆpt
1
1+ ˆpt
ˆpt
i

(cid:17)

||0.5

≥ D(p1/(p1+pi)||0.5)

(1−ξ/(p1+pK ))(1+(cid:15)/2) for i ∈ {2, 3, . . . , K}

(cid:110)

(cid:111)
2 , ξ(cid:48))

• N (P, ξ, (cid:15), α) : First, we introduce σ = max

t ∈ N ∗ : |ˆpt

i − pi| ≥ min( ξ

. By the law of large numbers,

for every α ∈ (0, 1), there exists N (P, ξ, (cid:15), α), such that P (σ ≤ N (P, ξ, (cid:15), α)) > 1 − α.

• Eα: We deﬁne the event Eα =

(cid:110)

∀t ≥ N (P, ξ, (cid:15), α), ∀i ∈ {1, 2 . . . K}

deﬁnition of N , P(Eα) > 1 − α.

|ˆpt

i − pi| < min( ξ

2 , ξ(cid:48)) = ξ(cid:48)(cid:48)(cid:111)

. From the

At time step t > 0 let st
t denote the empirical mean for
value vi. For convenience we use KL(q1||q2) to denote KL(Bernoulli(q1)||Bernoulli(q2)). We will now show that
conditioned on Eα, we have enough samples to separate each class i ∈ {2, 3, . . . , K} from the mode 1, i.e. st
1 + st
i
is suﬃciently large.

i denote the number of samples of vi, and ˆpt

i

i = st

PAC Mode Estimation using PPR Martingale Conﬁdence Sequences

1 + sT ∗
sT ∗

i = (ˆpT ∗

1 + ˆpT ∗
i )T ∗
≥ (p1 + pi − ξ)T ∗

+ K(p1 + pi)

=

≥

≥

(cid:19)

(p1 + pi) (1 − ξ/(p1 + pi)) (1 + (cid:15))
KL(P||P (cid:48))
(cid:18) 1 − ξ/(p1 + pi)
1 − Kξ
(cid:18) K − 1
(1 − ξ/(p1 + pi)) (1 + (cid:15))
δ
KL(p1/(p1 + pi)||0.5)
(cid:18)
(1 − ξ/(p1 + pi)) (1 + (cid:15)/2)(1 + γ)
KL(p1/(p1 + pi)||0.5)

ln

(cid:18)

(cid:18)

ln

(cid:19)

(cid:18) K − 1
δ

+ max

j∈{2,3...K}

Hj

(cid:19)

(N (P, ξ, (cid:15), α) + T0(γ))

(cid:19)

(cid:19)

+ Hi

+ N (P, ξ, (cid:15), α) + T0(γ)

ln

(cid:19)

(cid:18) K − 1
δ

(cid:19)

+ Hi

+ N (P, ξ, (cid:15), α) + T0(γ)

where the second inequality follows from Lemma 4.

D.2 Termination of the 1v1 Tests

We have shown that conditional on the event Eα,

sT ∗
1 + sT ∗

i ≥

(1 − ξ/(p1 + pi)) (1 + (cid:15)/2)(1 + γ)
KL(p1/(p1 + pi)||0.5)

(cid:18)

ln

(cid:19)

(cid:18) K − 1
δ

(cid:19)

+ Hi

+ N (P, ξ, (cid:15), α) + T0(γ).

(1−ξ/(p1+pi))(1+(cid:15)/2) , c2 = K−1

Consider c1 = KL(p1/(p1+pi)||0.5)
1 ≤ ln(2). If δ ≤ 0.5, we can see that c2
cα
cα
1
for δ ≤ 0.5,

δ

, α = 1 + γ. Since for p ∈ (0.5, 1], KL(p||0.5) ≤ ln(2) < 1 we have
ln(2) > e. This allows us to use Lemma 5 and state the following

≥ 2

(sT ∗

1 + sT ∗
i )

KL(p1/(p1 + pi)||0.5)
(1 − ξ/(p1 + pi))(1 + (cid:15)/2)

≥ ln

≥ ln

By the deﬁnition of N (P, ξ, (cid:15), α), for δ ≤ 0.5 we know that,

(cid:18) (K − 1)(sT ∗
1 + sT ∗
δ
(cid:18) (K − 1)(sT ∗
1 + sT ∗
δ

i )1+γ

(cid:19)

i + 1)

(cid:19)

.

(sT ∗

i

1 + sT ∗

i )KL(ˆpT ∗

1 /(ˆpT ∗

1 + ˆpT ∗

i )||0.5) ≥ ln

(cid:18) (K − 1)(sT ∗
1 + sT ∗
δ

i + 1)

(cid:19)

=⇒ Beta(0.5; sT ∗

1 + 1, sT ∗

i + 1) ≤

δ
K − 1

i

.

where the last implication follows from Lemma 6. At this stage the PPR-Bernoulli Stopping Rule declares that
1 > pT ∗
pT ∗
The above proof tells us that under the event Eα, after T ∗ samples, PPR-Bernoulli decides that for i ∈
{2, 3, . . . , K}, p1 > pi, i.e. v1 is the mode of the distribution. Therefore, if τ denotes the stopping time of
PPR-1v1, we have the following bound.

P(τ ≤ T ∗) > 1 − α =⇒ P

(cid:18)

lim
δ→0

τ
ln(1/δ)

≤

1 + (cid:15)
KL (P||P (cid:48))

(cid:19)

> 1 − α.

Jain, Shah, Gupta, Mehta, Nair, Vora, Khyalia, Das, Ribeiro, Kalyanakrishnan

To show that the lower bound of Shah et al. (2020) is matched, we upper bound the expected stopping time,
using a commonly-known5 result to bound the tail probability for a binomial random variable.

P(τ > T ∗) ≤ P(E(cid:123)

α).
K
(cid:88)

E(τ ) ≤ T ∗ +

∞
(cid:88)

P(|ˆpt

i − pi| > ξ(cid:48)(cid:48))

i=1

t=N (P,ξ,(cid:15),α)

K
(cid:88)

∞
(cid:88)

≤ T ∗ +

i=1

t=N (P,ξ,(cid:15),α)

exp (−2tKL(pi − ξ(cid:48)(cid:48)||ξ))

≤ T ∗ +

K
(cid:88)

∞
(cid:88)

i=1

t=0

exp (−2tKL(pi − ξ(cid:48)(cid:48)||ξ))

≤ T ∗ +

K
(cid:88)

i=1

1
1 − exp(2KL(pi − ξ(cid:48)(cid:48)||pi)

.

Hence, we have

which completes our proof.

EP (τ )
ln(1/δ)

lim
δ→0

≤

1 + (cid:15)
KL (P||P (cid:48))

,

We now furnish proofs of the lemmas that were used in the proofs above.
Lemma 4. For x ∈ [0, p2],

(p1 + x)KL

Proof. By deﬁnition,

(cid:18) p1

p1 + x

(cid:19)

||0.5

≥ KL(P||P (cid:48)).

(p1 + x)KL

(cid:18) p1

p1 + x

(cid:19)

||0.5

= p1 ln

(cid:18)

(cid:18)

≥ p1 ln

p1
(p1 + x)/2
p1
(p1 + p2)/2

(cid:19)

(cid:18)

+ x ln

(cid:19)

+ p2 ln

(cid:19)

x
(p1 + x)/2
(cid:18)

p2
(p1 + p2)/2

(cid:19)

= KL(P||P (cid:48)).

The second line follows from from the fact that p2 ≥ x and that F (x) = p1 ln

[0, p2] is a decreasing function (F (cid:48)(x) = ln

(cid:17)

(cid:16) 2x
p1+x

< 0).

(cid:16)

p1
(p1+x)/2

(cid:17)

(cid:16)

+ p ln

x
(p1+x)/2

(cid:17)

, x ∈

Lemma 5. For α ∈ [1, e/2] for any two constants c1 and c2 such that c2
cα
1
(cid:20)

c1x ≥ ln(c2xα), ∀x ≥ T0(c1, c2, α) where T0(c1, c2, α) =

≥ e, we have

α
c1

ln

(cid:19)

(cid:18) c2e
cα
1

+ ln ln

(cid:19)(cid:21)

.

(cid:18) c2
cα
1

Proof. Consider F (x) = c1x − ln (c2xα). We have F (cid:48)(x) = c1 − α
Using Lemma 18 by Garivier and Kaufmann (2016) we know that for x = T0(c1, c2, α), c1x ≥ c2xα, and c2
cα
1
e =⇒ T0(c1, c2, α) ≥ α
c1
our proof.

x which means that F (x) is increasing for x > α
c1
≥
. The fact that F (x) is increasing at this value of x completes

[ln(e2) + ln ln(e)] = 2 α
c1

.

Lemma 6. For i ∈ {2, 3, . . . , K},

(st

1 + st

i)KL(ˆpt

1/(ˆpt

1 + ˆpt

i)||0.5) ≥ ln

5See Theorem 2.1 by Mulzer (2018).

(cid:18) st

i + 1

1 + st
δ

(cid:19)

=⇒ Beta(0.5; st

1 + 1, st

i + 1) ≤ δ.

PAC Mode Estimation using PPR Martingale Conﬁdence Sequences

Proof.

=⇒ (st

1 + st
i)

ˆpt
1
1 + ˆpt
ˆpt
i

ln

(cid:19)

(cid:18) ˆpt
1
1 + ˆpt
ˆpt
i

+ (st

1 + st
i)

ˆpt
i
1 + ˆpt
ˆpt
i

=⇒ (st

1 + st

i)Hb

i)KL(ˆpt
(cid:19)

ln

(st

1 + st
(cid:18) ˆpt
i
1 + ˆpt
ˆpt
i
(cid:19)
(cid:18) ˆpt
i
1 + ˆpt
ˆpt
i

1/(ˆpt

1 + ˆpt

i)||0.5) ≥ ln

− (st

1 + st

i) ln(0.5) ≥ ln

(cid:18) st

(cid:18) st

1 + st
δ
1 + st
δ

i + 1

i + 1

(cid:19)

(cid:19)

+ (st

1 + st

i) log2(0.5) ≤ log2

(cid:18)

δ
1 + st
st

i + 1

(cid:19)

.

The last step follows from the deﬁnition of binary entropy function Hb for p ∈ [0, 1]:

Hb(p) =

(cid:40)
0
−p log2 p − (1 − p) log2(1 − p)

p ∈ {0, 1},
otherwise.

Writing the condition in this form allows us to use the following well known6 inequality, with α = ˆpt
as st

i
1+ ˆpt
ˆpt
i

i) and t = st

1 + st
i.

1 > st

(α < 1
2

(cid:19)

(cid:18) t
αt

≤ 2tHb(α) where t ∈ N and α ∈

(cid:20)

0,

(cid:21)

.

1
2

So we have

(st

1 + st

i)Hb

(cid:19)

(cid:18) ˆpt
1
1 + ˆpt
ˆpt
i

+ (st

1 + st

i) log2(0.5) ≤ log2

(cid:19)

(cid:18)

δ
1 + st
st

i + 1

+ (st

i) log2(0.5) + log2(st
1 + st
(cid:32)

1 + st

i + 1) ≤ log2 (δ)

1(0.5)st

i

(st

1 + st

i + 1)!(0.5)st
1)!(st
i)!
1 + 1, st
=⇒ Beta(0.5; st

(st

i + 1) ≤ δ.

(cid:33)

≤ log2 (δ)

=⇒ log2

=⇒ log2

(cid:19)(cid:19)

(cid:18)(cid:18)st

1 + st
i
st
1

6See, for example, Galvin (2014).

Jain, Shah, Gupta, Mehta, Nair, Vora, Khyalia, Das, Ribeiro, Kalyanakrishnan

E NON-ASYMPTOTIC UPPER BOUND FOR PPR-1v1

In Appendix D we showed that the PPR-1v1 stopping rule is asymptotically optimal, in the regime that δ → 0.
In this section, we show a non-asymptotic upper bound on its sample complexity: in other words, a result that
holds for all δ ∈ (0, 1). To the best of our knowledge, the tightest such upper bound given yet for the PAC mode
estimation problem is the recent result of Shah et al. (2020), which we reproduce below.

Theorem 7 (A1 upper bound Shah et al. (2020)). Fix δ ∈ (0, 1), K ≥ 2, and problem instance P = (p, v, K).
When A1 is run on P, with probability 1 − δ, the number of samples it observes is at most

592
3

p1
(p1 − p2)2 ln

(cid:114)

(cid:32)

592
3

K
δ

p1
(p1 − p2)2

(cid:33)

.

(p1−p2)2 factor is within a constant factor of LB(P, δ) (Shah et al., 2020).
It is easy to show that the leading
In this appendix, we derive a similar upper bound for PPR-1v1, albeit one that is tighter by a small constant
factor. Our ﬁrst step is to show an upper bound for the special case of K = 2 (wherein PPR-1v1 reduces to
PPR-Bernoulli) in Appendix E.1. In turn, this result is used to generalise to K ≥ 2 in Appendix E.2. The ﬁnal
upper bound, is given in Theorem 9.

p1

E.1 An Upper Bound for K = 2

Lemma 8 (PPR-Bernoulli upper bound). Fix δ ∈ (0, 1) and problem instance P = (p, v, 2). When PPR-
Bernoulli is run on P, with probability 1 − δ, the number of samples it observes is at most

20.775p1
(p1 − 1
2 )2

(cid:18)

ln

2.49
(p1 − 1

2 )2δ

(cid:19)

.

Proof. For a problem instance P = (p, v, 2), our parameters are p1 and p2 = 1 − p1, with p1 > 0.5. It suﬃces to
maintain a conﬁdence sequence on p1; termination is achieved when this conﬁdence sequence no longer contains
1
2 . To upper-bound the number of samples needed for termination, we proceed in three steps.

1. At time t, there are 2t possible 0-1 sequences that the Bernoulli variable can produce in t steps. Let X t be
1 denote the number of times v1 occurs in X t.
2 ) ≥ 1

a random variable denoting this t-length 0-1 sequence. Let st
1 for which 1 (cid:8)Rt( 1
In Subsection E.1.1, we ﬁnd the range of st
2. Next, in Appendix E.1.2, we use the range of k derived in Appendix E.1.1 to derive a suﬃcient condition

(cid:9).

δ

for t to be the sample complexity.

3. In Appendix E.1.3, we use the suﬃcient condition derived in Subsection E.1.2 to obtain a closed-form sample
complexity upper bound. We separately take up two cases, p1 ≤ 0.6, and p1 > 0.6, so as to tighten the
constants in the upper bound.

E.1.1 Finding a Range of k for which 1 (cid:8)Rt( 1

2 ) ≥ 1
The stopping rule does not terminate at t so long as Rt (cid:0) 1

δ

2

(cid:9)

(cid:18)

Rt

P

(cid:19)

(cid:18) 1
2

≥

(cid:19)

1
δ

(cid:26)

(cid:88)

1

=

X t

≥

(cid:27)

1
δ

Pp1 (X t).

(cid:1) < 1
δ .
(cid:19)
(cid:18) 1
2

Rt

The expression inside the indicator random variable can be simpliﬁed to obtain the following:

(cid:26)

1

Rt

(cid:19)

(cid:18) 1
2

≥

(cid:27)

1
δ

= 1

(cid:26) 1

πt(1/2)

≥

(cid:27)

1
δ

= 1

(cid:40) (cid:82) 1

η=0 Pη(X t)dη
P1/2(X t)

(cid:41)

.

1
δ

≥

The sum inside the indicator function is independent of the position of 0-1 outcomes within the sequence X t and
only depends on the number of zeros and ones in the instance. Suppose we consider the instances which contain

PAC Mode Estimation using PPR Martingale Conﬁdence Sequences

st
1 ones; then, we have

(cid:26)

1

Rt

(cid:19)

(cid:18) 1
2

≥

(cid:27)

1
δ

(cid:40)

= 1

δ ≥

(t + 1)!
1!(t − st
st

1)!

(cid:18) 1
2

(cid:19)st

1 (cid:18) 1
2

(cid:19)t−st

1 (cid:41)

.

From the above equations, we derive the exact expression for P(Rt( 1

(cid:18)

P

Rt

(cid:19)

(cid:18) 1
2

≥

(cid:19)

1
δ

=

t
(cid:88)

st
1=0

(cid:18) t
st
1

(cid:19)

pst
1 (1 − p1)t−st

1

1 1

(t + 1)!
1!(t − st
st

1)!

(cid:27)

.

2−t

δ ):

2 ) ≥ 1
(cid:26)

δ ≥

We now break the expression into two parts.

(cid:18)

Rt

P

(cid:19)

(cid:18) 1
2

≥

(cid:19)

1
δ

=

t
(cid:88)

st
1=0

(cid:19)

(cid:18) t
st
1

pst
1 (1 − p1)t−st

1

11

(cid:26)

δ ≥

(t + 1)!
1!(t − st
st

1)!

(cid:27)

2−t

=

(cid:98) t
2 (cid:99)
(cid:88)

st
1=0

(cid:19)

(cid:18) t
st
1

pst
1 (1 − p1)t−st

1

11

(cid:26) δ

t + 1

≥

(cid:19)

(cid:18) t
st
1

(cid:27)

+

2−t

t
(cid:88)

1=(cid:98) t
st

2 (cid:99)+1

(cid:19)

(cid:18) t
st
1

pst
1 (1 − p1)t−st

1

11

(cid:26) δ

t + 1

≥

(cid:18) t

t − st
1

(cid:19)

(cid:27)

.

2−t

We have split the summation in this manner as it allows to use the following commonly-known7 inequality for
the binomial coeﬃcients inside the indicator function.

(cid:19)

(cid:18) t
αt

≤ 2tHb(α) where t ∈ N and α ∈

(cid:20)

0,

(cid:21)

.

1
2

The deﬁnition of binary entropy function Hb for p ∈ [0, 1] is:

Hb(p) =

(cid:40)
0
−p log2 p − (1 − p) log2(1 − p)

p ∈ {0, 1},
otherwise.

Invoking this inequality in the probability expression, we arrive at the lower bound:

(cid:18)

P

Rt

(cid:19)

(cid:18) 1
2

≥

(cid:19)

1
δ

≥

(cid:98) t
2 (cid:99)
(cid:88)

st
1=0

(cid:19)

(cid:18) t
st
1

pst
1 (1 − p1)t−st

1

11

(cid:26) 1
t

log2

(cid:18) δ

(cid:19)

t + 1

+ 1 ≥ Hb

(cid:19)(cid:27)

+

(cid:18) st
1
t

t
(cid:88)

1=(cid:98) t
st

2 (cid:99)+1

(cid:19)

(cid:18) t
st
1

pst
1 (1 − p1)t−st

1

1 1

(cid:26) 1
t

log2

(cid:18) δ

(cid:19)

t + 1

+ 1 ≥ Hb

(cid:18) t − st
1
t

(cid:19)(cid:27)

.

Using the fact that

Hb(p) ≤ 2(cid:112)p(1 − p) if p ≤

1
2

,

the summation is further lower-bounded:
(cid:18) t
(cid:18) 1
st
2
1

t
(cid:88)

Rt

1
δ

≥

≥

(cid:18)

(cid:19)

(cid:19)

P

(cid:19)

st
1=0

pst
1 (1 − p1)t−st

1

1 1

(cid:26) 1
2

(cid:20)
log2

(cid:18) δ

(cid:19)

t + 1

(cid:21)

+ t

≥

(cid:113)

(cid:27)

1(t − st
st
1)

.

We will later see that for the t we ﬁnd as the bound, log2
both sides. By using that fact and solving the quadratic, the range of st
indicator function is:
(cid:34)

(cid:34)

(cid:35)

(cid:35)

st
1 ∈

0,

t − (cid:112)t2 − 4β2
2

(cid:91)

t + (cid:112)t2 − 4β2
2

, t

where β =

(cid:17)

(cid:16) δ
t+1

(cid:20)
log2

1
2

(cid:18) δ

(cid:19)

t + 1

(cid:21)

+ t

.

+ t ≥ 0. Hence, we can take the square of
1 which satisﬁes the condition inside the

7See, for example, Galvin (2014).

Jain, Shah, Gupta, Mehta, Nair, Vora, Khyalia, Das, Ribeiro, Kalyanakrishnan

Slightly loosening this bound, we get

(cid:113)

t −



st
1 ∈

0,

2t log2
2

(cid:0) t+1
δ



(cid:1)



(cid:91)





(cid:113)

t +

(cid:1)

(cid:0) t+1
δ



, t

 .

2t log2
2

In short, if st

1 lies in the range above, then it must satisfy 1 (cid:8)Rt( 1

2 ) ≥ 1

δ

(cid:9). Call this range R.

E.1.2 Finding a Suﬃcient Condition for t to be the Sample Complexity

We will lower bound the probability of (cid:80)
1 (1 − p1)t−st
1 by 1 − δ, and thus ﬁnd the values of t satisfying
this equation. To that end, we use another commonly-known 8 result to bound the tail probability for a binomial
random variable: for (cid:15) > 0,

1∈R pst

st

1

Using this in our working, considering H(t) as a binomial random variable, we get

P(H(t) ≤ (p1 − (cid:15))t) ≤ e−DKL(p1−(cid:15)||p1)t.

t
(cid:88)

st
1=(p1−(cid:15))t

(cid:19)

(cid:18) t
st
1

pst
1 (1 − p1)t−st

1

1 = P ((p1 − (cid:15)) t ≤ H(t))

(cid:18)

=⇒ P

Rt

≥ 1 − e−DKL(p1−(cid:15)||p1)t
(cid:19)
1
δ

(cid:18) 1
2

≤

(cid:19)

≤ e−DKL(p1−(cid:15)||p1)t.

We want P (cid:0)Rt (cid:0) 1

2

(cid:1) > 1

δ

(cid:1) ≥ 1 − δ. A suﬃcient condition for the above to hold is

And (cid:15) is given by,

e−DKL(p1−(cid:15)||p1)t ≤ δ

=⇒ t × DKL(p1 − (cid:15)||p1) ≥ ln

(cid:19)

(cid:18) 1
δ

(cid:15) = p1 −



(cid:115)

1 +

1
2

(cid:0) t+1
δ

2 log2
t



(cid:1)

 , (cid:15) ≥ 0.

We will note that (cid:15) ≥ 0 holds later. For now, we try to ﬁnd stronger suﬃcient conditions on t. To that end, we
use that

DKL(x||y) ≥

(x − y)2
2y

, x ≤ y

=⇒ DKL (p1 − (cid:15)||p1) ≥

t × DKL(p1 − (cid:15)||p1) ≥ ln

(cid:15)2
2p1

.

(cid:19)

(cid:18) 1
δ

So, a suﬃcient condition for

is given as

8See Theorem 2.1 in Mulzer (2018).

=⇒ t ≥

t(cid:15)2
2p1

≥ ln

(cid:19)

(cid:18) 1
δ
(cid:18) 1
δ

2p1
(cid:15)2 ln

(cid:19)

.

PAC Mode Estimation using PPR Martingale Conﬁdence Sequences

To ﬁnd a suﬃcient condition for t to satisfy the above, we can ﬁnd a lower bound on (cid:15)2 and use it in the
expression above. We have



(cid:18)



p1 −

(cid:15)2 =

(cid:19)

1
2

−

(cid:115)

log2

(cid:1)

(cid:0) t+1
δ

2t


2



(cid:18)

=

p1 −

(cid:19)2

1
2

+

log2

(cid:1)

(cid:0) t+1
δ

2t

(cid:18)

− 2

p1 −

=⇒ (cid:15)2 ≥

(cid:18)

p1 −

1
2

(cid:19)2

(cid:18)

− 2

p1 −

1
2

(cid:19)

(cid:19)

1
2
(cid:115)

(cid:115)

log2

(cid:1)

(cid:0) t+1
δ

2t

log2

(cid:1)

(cid:0) t+1
δ

2t

.

E.1.3 Constant Factor Tightening

To obtain our ﬁnal bound, we consider two cases: p1 ≤ 0.6, p1 > 0.6.

Case 1: p1 ≤ 0.6 We will ﬁnd a suﬃcient condition on t for the fact that

(cid:18)

t
C1

p1 −

(cid:19)2

1
2

≥ log2

(cid:19)

(cid:18) t + 1
δ

(3)

where C1 = 2.4488.
Earlier in the proof, we have used the fact that log2
(p1− 1
C1

≤ 1. So, from (3), we can easily see that log2

2 )2

(cid:17)

(cid:16) δ
t+1
(cid:16) δ
t+1

(cid:17)

+ t ≥ 0.

+ t ≥ 0. We ﬁrst prove this fact. For p1 ∈ ( 1

2 , 1],

We ﬁnd a suﬃcient t such that

(cid:18)

t
C1

We let t be of the form

=⇒ t ≥

p1 −

≥ log2

(cid:19)2

1
2
C1
(p1 − 1

log2

2 )2

(cid:19)

(cid:18) t + 1
δ
(cid:18) t + 1
δ

t =

C1C2
(p1 − 1

2 )2

(cid:18)

log2

C1
(p1 − 1

2 )2δ

(cid:19)

(cid:19)

.

,

where C2 is a constant. Note that, for this choice of t, (We will show later that t ≥ 4000 is also true for the ﬁrst
line below to hold),

C1
(p1 − 1
(cid:19)

2 )2

C1
(p1 − 1

2 )2

log2

(cid:18) t + 1
δ

≤

C1
(p1 − 1

t + 1 ≤ 1.00025t.

log2

(cid:19)

(cid:18) t + 1
δ

≤

C1
(p1 − 1

2 )2

log2

(cid:18) 1.00025t
δ

(cid:19)

.

log2(1.00025C2) + 0.3839

2 )2
C1
(p1 − 1

2 )2

(cid:18)

log2

C1
(p1 − 1

2 )2δ

(cid:19)

,

C1
(p1 − 1

2 )2

(cid:18)

log2

C1
(θ − 1

2 )2δ

(cid:19)

+

where the last step is true because

(cid:18)

(cid:18)

log2

log2

(cid:19)(cid:19)

C1
(p1 − 1

2 )2δ

≤ 0.3839 log2

(cid:19)

(cid:18)

C1
(p1 − 1

2 )2δ

using that, for x ≥ 200,

log2 (log2 (x)) ≤ 0.3839 log2 (x) .

Jain, Shah, Gupta, Mehta, Nair, Vora, Khyalia, Das, Ribeiro, Kalyanakrishnan

Thus, we get

C1
(p1 − 1

2 )2

log2

(cid:19)

(cid:18) t + 1
δ

≤

≤

2 )2

C1
(p1 − 1
C1
(p1 − 1

2 )2

(cid:18)

log2

(cid:18)

log2

C1
(p1 − 1
C1
(p1 − 1

2 )2δ

2 )2δ

(cid:19)

(cid:19)

(1.3839 + log2 (1.00025C2))

C2

where the last step is true when C2 = 2.9402. Thus,
(cid:19)

(cid:18)

t =

2C1C2p1
(p1 − 1
2 )2

log2

C1
(p1 − 1

2 )2δ

=⇒ t =

C3p1
(p1 − 1

2 )2

(cid:18)

ln

C1
(p1 − 1

2 )2δ

(cid:19)

,

where C3 = 20.775 ≥ 2C1C2 log2(e) satisﬁes the above inequality.

So now, if the condition on t is satisﬁed, we have

(cid:18)

p1 −

(cid:15)2 ≥

(cid:19)2

(cid:18)

− 2

p1 −

1
2

1
2

(cid:115)

(cid:19)

log2

(cid:1)

(cid:0) t+1
δ

2t

(cid:18)

≥

p1 −

1
2

(cid:19)2

(cid:18)

− 2

p1 −

(cid:1)

(cid:19) (cid:0)p1 − 1
2
2.21305

1
2

=⇒ (cid:15)2 ≥ C4

(cid:18)

p1 −

(cid:19)2

,

1
2

where C4 = 0.09627.
Previously, we had remarked that it was suﬃcient to have

t ≥

2p1
(cid:15)2 ln

(cid:18) 1
δ

(cid:19)

.

We can loosen this by using the lower bound on (cid:15) to: it is suﬃcient to satisfy

t ≥

C5p1
(cid:0)p1 − 1

2

(cid:19)

,

(cid:18) 1
δ

(cid:1)2 ln

where C5 = 20.775.
To make both the lower bound on (cid:15) and the above equation valid, we note that the following choice of t works:

t =

C5p1
(cid:0)p1 − 1

2

(cid:1)2 max

(cid:18)(cid:18)

(cid:18)

ln

C1
(p1 − 1

2 )2δ

(cid:19)

, ln

(cid:18) 1
δ

(cid:19)(cid:19)(cid:19)

We note that this leaves us with the following choice of t:

t =

20.775p1
(cid:1)2
(cid:0)p1 − 1

2

(cid:18)

(cid:18)

ln

2.45
(p1 − 1

2 )2δ

(cid:19)(cid:19)

Note that t ≥ 4000, since p1 ≤ 0.6.

Case 2: p1 > 0.6 We will ﬁnd a suﬃcient condition on t for the fact that

(cid:18)

t
C1

p1 −

(cid:19)2

1
2

≥ log2

(cid:19)

(cid:18) t + 1
δ

where C1 = 2.4877.
Using the same argument as in case 1, here also we can prove that log2
We ﬁnd a suﬃcient t such that

(cid:17)

(cid:16) δ
t+1

+ t ≥ 0.

p1 −

≥ log2

(cid:18)

t
C1

=⇒ t ≥

(cid:19)2

1
2
C1
(p1 − 1

log2

2 )2

(cid:19)

(cid:18) t + 1
δ
(cid:18) t + 1
δ

(cid:19)

.

PAC Mode Estimation using PPR Martingale Conﬁdence Sequences

We let t be of the form

t =

C1C2
(p1 − 1

2 )2

(cid:18)

log2

C1
(p1 − 1

2 )2δ

(cid:19)

,

where C2 is a constant.
Note that, for this choice of t, (We will show later that t ≥ 100 is also true for the ﬁrst line to hold)

C1
(p1 − 1
(cid:19)

2 )2

C1
(p1 − 1

2 )2

log2

(cid:18) t + 1
δ

≤

C1
(p1 − 1

(cid:19)

t + 1 ≤ 1.01t.
C1
(p1 − 1

≤

(cid:18) t + 1
δ

2 )2

log2

log2

(cid:18) 1.01t
δ

(cid:19)

.

2 )2
C1
(p1 − 1

2 )2

log2(1.01C2) + 0.5228

(cid:18)

log2

C1
(p1 − 1

2 )2δ

(cid:19)

C1
(p1 − 1

2 )2

(cid:18)

log2

C1
(θ − 1

2 )2δ

(cid:19)

+

where the last step is true because

(cid:18)

(cid:18)

log2

log2

(cid:19)(cid:19)

C1
(p1 − 1

2 )2δ

≤ 0.5228 log2

,

(cid:18)

(cid:19)

C1
(p1 − 1

2 )2δ

using that, for x ≥ 9.64,

Thus, we get

C1
(p1 − 1

2 )2

log2

(cid:19)

(cid:18) t + 1
δ

log2 (log2 ((x)) ≤ 0.5228 log2 (x)

(cid:19)

2 )2

≤

C1
(p1 − 1
C1
(p1 − 1

2 )2

log2

(cid:18)

log2

(cid:18)

C1
(p1 − 1
C1
(p1 − 1

2 )2δ

2 )2δ
(cid:19)

≤

(1.5228 + log2 (1.01C2))

C2,

where the last step is true when C2 = 3.228. Thus, using that p1 > 0.6 by writing that p1

0.6 ≥ 1, we have that

t =

1.67C1C2p1
(p1 − 1
2 )2

(cid:18)

log2

(cid:19)

C1
(p1 − 1

2 )2δ

=⇒ t =

C3p1
(p1 − 1

2 )2

(cid:18)

ln

C1
(p1 − 1

2 )2δ

(cid:19)

,

where C3 = 19.35 ≥ 1.67C1C2 log2(e) satisﬁes the above inequality.

So now, if the condition on t is satisﬁed, we have

(cid:18)

p1 −

(cid:15)2 ≥

(cid:19)2

(cid:18)

− 2

p1 −

1
2

1
2

(cid:115)

(cid:19)

log2

(cid:1)

(cid:0) t+1
δ

2t

(cid:18)

≥

p1 −

1
2

(cid:19)2

(cid:18)

− 2

p1 −

(cid:1)

(cid:19) (cid:0)p1 − 1
2
2.2305

1
2

=⇒ (cid:15)2 ≥ C4

(cid:18)

p1 −

(cid:19)2

,

1
2

where C4 = 0.1033.
Previously, we had remarked that it was suﬃcient to have

t ≥

2p1
(cid:15)2 ln

(cid:18) 1
δ

(cid:19)

.

We can loosen this by using the lower bound on (cid:15) to: it is suﬃcient to satisfy

t ≥

C5p1
(cid:0)p1 − 1

2

(cid:19)

,

(cid:18) 1
δ

(cid:1)2 ln

Jain, Shah, Gupta, Mehta, Nair, Vora, Khyalia, Das, Ribeiro, Kalyanakrishnan

where C5 = 19.36.
To make both the lower bound on (cid:15) and the above equation valid, we note that the following choice of t works:
(cid:18)

(cid:19)(cid:19)(cid:19)

(cid:18)(cid:18)

(cid:19)

t =

C5p1
(cid:0)p1 − 1

2

(cid:1)2 max

ln

C1
(p1 − 1

2 )2δ

, ln

(cid:18) 1
δ

.

We note that this leaves us with the following choice of t:

t =

19.36p1
(cid:0)p1 − 1
(cid:1)2

2

(cid:18)

(cid:18)

ln

2.49
(p1 − 1

2 )2δ

(cid:19)(cid:19)

Note that t ≥ 106.

Combining both the above bounds, we have that

is suﬃcient.

Looking over both cases, we note

t =

19.36p1
(cid:1)2
(cid:0)p1 − 1

2

(cid:18)

(cid:18)

ln

2.49
(p1 − 1

2 )2δ

(cid:19)(cid:19)

t =

20.775p1
(cid:1)2
(cid:0)p1 − 1

2

(cid:18)

(cid:18)

ln

2.49
(p1 − 1

2 )2δ

(cid:19)(cid:19)

is suﬃcient. We denote C6 = 20.775, C7 = 2.49 to get
(cid:18)

(cid:18)

C6p1
(cid:0)p1 − 1
as an upper bound on the sample complexity of PPR-Bernoulli with probability atleast 1 − δ.

C7
(p1 − 1

2 )2δ

t =

(cid:1)2

ln

2

(cid:19)(cid:19)

E.2 An Upper Bound for General K

Theorem 9 (PPR-1v1 upper bound). Fix δ ∈ (0, 1), K ≥ 2, and problem instance P = (p, v, K). When
PPR-1v1 is run on P, with probability 1 − δ, the number of samples it observes is at most

t(cid:63) =

194.07p1
(p1 − p2)2 ln

(cid:32)(cid:114)

79.68(K − 1)
δ

p1
(p1 − p2)

(cid:33)

.

Proof. Fix arbitrary t > t(cid:63) and j ≥ 2. Our strategy is to show that with suﬃciently high probability, v1 and
vj will have separated before t pulls. First, let st
ij denote the sum of the number of occurrences of v1 and vj
in the ﬁrst t samples: that is, st
ij is a binomial random variable with parameters t and

j. Clearly st

ij = st

1 + st

(cid:114)

2 ln( 1
δ(cid:48) )
(p1+p2)t .

(p1 + pj), We argue that st
A Chernoﬀ bound yields

ij cannot fall too far below its mean. Concretely, take δ(cid:48) =

δ

2(K−1) and l =

P{st

ij ≤ (1 − l)(p1 + pj)t} ≤ exp

(cid:18)

−

l2(p1 + pj)t
2

(cid:19)

≤ δ(cid:48)

for our choice of l. Thus, with probability at least 1−δ(cid:48), v1 and vj together have more than (1−l)(p1+pj)t samples.
Now, the test to separate v1 and vj is PPR-Bernoulli on a Bernoulli variable with parameter q1 = p1
> 1
2 .
Although PPR-1v1 runs this test with mistake probability
K−1 , imagine running it with mistake probability
δ(cid:48) =
2(K−1) . The latter test would necessarily incur equal or more samples on every run. Yet, from Lemma 8,
(cid:17)
we know that with probability at least 1 − δ(cid:48), the latter will terminate after at most u = 20.775q1
samples. It can be veriﬁed that for t > t(cid:63), u < (1 − l)(p1 + pj)t (calculation shown in Appendix E.2.1). In other
words, the probability that v1 and vj have not separated before t pulls is at most 2δ(cid:48).

2.49
(q1− 1

2 )2 ln

(q1− 1

2 )2δ(cid:48)

p1+pj

(cid:16)

δ

δ

Since the argument above holds for arbitrary j ≥ 2, a union bound establishes that with probability at least
1 − δ, v1 must have separated from all other values—implying termination—before t pulls.

PAC Mode Estimation using PPR Martingale Conﬁdence Sequences

E.2.1 Calculation in Proof of Theorem 9

We have to show u < (1 − l)(p1 + pj)t for t > t(cid:63), where

(cid:18)

ln

2.49
(q1 − 1

2 )2δ(cid:48)

(cid:19)

,

20.775q1
(q1 − 1
2 )2
p1
,
p1 + pj
(cid:115)

u =

q1 =

l =

2 ln(1/δ(cid:48))
(p1 + pj)t

, and

t(cid:63) =

194.07p1
(p1 − p2)2 ln

(cid:32)(cid:114)

79.68(k − 1)
δ

p1
(p1 − p2)

(cid:33)

.

First, observe that

u =

83.1p1(p1 + pj)
(p1 − pj)2

ln

(cid:18) 9.96(p1 + pj)2
(p1 − pj)2δ(cid:48)

(cid:19)

≤

83.1p1(p1 + pj)
(p1 − p2)2

ln

(cid:18) 39.84p2
1

(p1 − p2)2δ(cid:48)

(cid:19)

.

Thus, we have

u

(p1 + pj)t∗ < 0.85640

(cid:16) 39.84p2

1

(p1−p2)2δ(cid:48)

(cid:17)

ln

(cid:16) 79.68(k−1)
δ

ln

p2
1
(p1−p2)2

(cid:17) = 0.85640.

It suﬃces to show that l ≤ 0.1436, which is established by the steps below.

l2 ≤

2 ln (1/δ(cid:48)) (p1 − p2)2

(p1 + pj)97.035p1 ln

(cid:16) 79.68(k−1)p2
δ(p1−p2)2

1

(cid:17) ≤

2(p1 − p2)2
(p1 + pj)97.035p1

≤

2
97.035

=⇒ l ≤

1
(cid:112)97.035/2

< 0.1436.

Jain, Shah, Gupta, Mehta, Nair, Vora, Khyalia, Das, Ribeiro, Kalyanakrishnan

F DCB ALGORITHM

We describe the DCB (“Diﬀerence in Conﬁdence Bounds”) algorithm for sampling constituencies in the indirect
election winner-forecasting problem from Section 5.1.

Suppose we have C constituencies. For ease of explanation, we assume that all the constituencies have the same
set of K ≥ 2 parties (in reality we maintain a separate list for each constituency). Although constituencies
have ﬁnite populations, these are usually large enough to ignore the beneﬁt of without-replacement samples; we
simply view each response as a sample from a discrete distribution.

After t ≥ 1 samples have been obtained from the population, let LCBt(c, i) and UCBt(c, i) denote (1vr) lower and
upper conﬁdence bounds, respectively, on the the (true) fraction of votes to be cast for party i ∈ {1, 2, . . . , K}
in constituency c ∈ {1, 2, . . . , C}. If applying a 1v1 procedure for mode estimation, we have separate conﬁdence
bounds LCBt(c, i, j) and UCBt(c, i, j) for each pair of parties i, j ∈ {1, 2, . . . , K}, i (cid:54)= j. The permitted mistake
probability δ is divided equally among the constituencies.

The key idea in the DCB algorithm is to keep lower and upper conﬁdence bounds on the wins of each party
(across constituencies), and to use this information to guide sampling. For party i ∈ {1, 2, . . . , K}, the current
number of wins (winst
i) can be obtained by verifying whether its conﬁdence bounds within
each constituency have separated accordingly from other parties. We also use leadst
i to denote the number
of constituencies in which i has polled the most votes yet, but has not yet won. Thus, we have LCBt
i =
winst

i) and losses (lossest

i, and UCBt

i = C − lossest
i.

Observe that the overall winner can be declared as soon as one party’s LCB exceeds the UCB of all the other
parties. On the other hand, when the winner is yet to be identiﬁed, one would ideally like to focus on “po-
tential” winners, rather than query a constituency whose result does not seem relevant to the big picture.
Taking cue from the LUCB algorithm for bandits Kalyanakrishnan et al. (2012), the ﬁrst step under DCB
is to identify two contenders for the top position: party at = argmaxi∈{1,2,...,K}(winst
i), and party
bt = argmaxi∈{1,2,...,K}\{at} UCBt
i. Optimistic that sampling can reveal a win for at and a loss for bt—which
would take us closer to termination—DCB picks one “promising” constituency each for at and bt. These con-
stituencies, denoted ct
2, are deﬁned below for use with both 1v1 and 1vr conﬁdence bounds. The idea is
the same: ct
2 is the constituency
in which bt appears poised to lose by a large margin.

1 is the constituency in which at appears poised to win by a large margin, and ct

i + leadst

1 and ct

(cid:40)

(cid:40)

ct
1 =

ct
2 =

argmaxc minj∈{1,2,...,K}(UCB(c, at, j) − LCB(c, at, j))
argmaxc minj∈{1,2,...,K}(UCB(c, at) − LCB(c, j))
argmaxc maxj∈{1,2,...,K}(UCB(c, j, bt) − LCB(c, j, bt))
argmaxc maxj∈{1,2,...,K}(UCB(c, j) − LCB(c, bt))

(1v1),
(1vr),

(1v1),
(1vr).

The outer “argmax” in all the deﬁnitions above is over all constituencies where the concerned party (at or bt) is
still in contention: that is, it has not yet won or lost the constituency. The DCB algorithm queries ct
2 at
each time step t, and terminates once an overall winner has been identiﬁed.

1 and ct

PAC Mode Estimation using PPR Martingale Conﬁdence Sequences

G Winner Forecasting in Bihar Elections

Below we provide results from the 2015 Bihar state elections, a closer contest than the 2014 Indian national
elections whose results were shown in Table 2 in Section 5.1. Of a total 242 seats in this election, 80 went to the
winner, 71 to the second largest party, and 53 to the third largest.

Table 4: Sample complexity of various stopping rules when coupled with (1) round-robin (RR) polling of con-
stituencies and (2) DCB. All experiments are run with mistake probability δ = 0.01. Values shown are averages
from 10 runs, and show one standard error. “Seats resolved” indicates the number of constituencies in which a
winner was identiﬁed before the overall procedure terminated.

Algorithm

RR-A1-1v1
RR-A1-1vr
RR-KLSN-1v1
RR-KLSN-1vr
RR-PPR-1v1
RR-PPR-1vr

DCB-A1-1v1
DCB-A1-1vr
DCB-KLSN-1v1
DCB-KLSN-1vr
DCB-PPR-1v1
DCB-PPR-1vr

Bihar-2015 (242 seats)

Samples

Seats
Resolved

4201429 ± 43932
4841406 ± 58188
2198678 ± 44514
2668729 ± 47535
1813213 ± 42081
2054171 ± 40394
2301936 ± 36399
2481552 ± 21483
1127963 ± 22191
1312027 ± 22892
883389 ± 15581
993495 ± 18859

221 ± 1
222 ± 1
221 ± 1
222 ± 2
221 ± 1
221 ± 1
135 ± 1
134 ± 1
139 ± 2
139 ± 1
142 ± 2
139 ± 1

