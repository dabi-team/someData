1
2
0
2

n
u
J

7
2

]

G
L
.
s
c
[

1
v
6
0
4
5
1
.
6
0
1
2
:
v
i
X
r
a

1

A Comprehensive Survey of Incentive Mechanism
for Federated Learning
Rongfei Zeng∗§, Chao Zeng∗, Xingwei Wang†, Bo Li‡, and Xiaowen Chu§
∗ Software College, Northeastern University, Shenyang, China
† Department of Computer Science, Northeastern University, Shenyang, China
‡ Department of Computer Science and Engineering, HKUST, Hong Kong, China
§ Department of Computer Science, Hong Kong Baptist University, Hong Kong, China
Email: zengrf@swc.neu.edu.cn, zengc@gmail.com, wangxw@neu.edu.cn, bli@ust.hk, chxw@comp.hkbu.edu.hk

Abstract—Federated learning utilizes various resources pro-
vided by participants to collaboratively train a global model,
which potentially address the data privacy issue of machine
learning. In such promising paradigm, the performance will be
deteriorated without sufﬁcient training data and other resources
in the learning process. Thus, it is quite crucial to inspire more
participants to contribute their valuable resources with some
payments for federated learning. In this paper, we present a
comprehensive survey of incentive schemes for federate learning.
Speciﬁcally, we identify the incentive problem in federated
learning and then provide a taxonomy for various schemes.
Subsequently, we summarize the existing incentive mechanisms
in terms of the main techniques, such as Stackelberg game,
auction, contract theory, Shapley value, reinforcement learning,
blockchain. By reviewing and comparing some impressive results,
we ﬁgure out three directions for the future study.

Index Terms—Federated learning, incentive mechanism, per-

formance improvement.

I. INTRODUCTION
As a promising distributed deep learning paradigm, Feder-
ated Learning (FL) is proposed to collaboratively train a global
machine learning model with plenty of participants whose data
privacy is considered as top priority [1]. In FL, each participant
trains a local model with its private data samples and then
submits model parameters to the remote cloud instead of raw
training data. After collecting sufﬁcient parameters of local
models, a global model is aggregated and then distributed
to participants for next round of local training. The process
iterates until the global model satisﬁes the predeﬁned accuracy
requirement. From the training process, we can easily learn
that FL drastically improves the data privacy of participants,
without the upload of raw data.

The salient feature of FL enables its widespread applications
in both cross-device and cross-silo settings. In cross-device
FL, more clients are fascinated to contribute their resources to
improve their user experience. For example, Google applies
FL to its products Gboard to improve the performance [4].
Similarly, Apple employs FL to QuickType and “Hey Siri”
of iOS13. Besides that, FL also demonstrates its potential to
solve the dilemma problem of “isolated data island” faced
by companies/organizations who hesitate to share their vast
volume of data samples for business concerns and privacy
regulations [3]. Some industrial examples include biomedical
data analysis in Owkin, ﬁnance and insurance data analysis in

WeBank and Swiss Re, and drug discovery in MELLODDY
[5], [6].

The incentive issues are paramount and indispensable to FL
training. FL consumes plenty of resources from participants,
such as computation power, bandwidth, and private data,
some of which might be constraint in scenarios like mobile
networks and mobile edge computing. In addition, partici-
pants still worry about security and privacy threats in FL,
where some attacks have already been found recently [7]. All
these factors hinder the participation of clients in FL without
enough payback1. Furthermore, the training performance of
FL, e.g., model accuracy, training speed, will be deteriorated
without sufﬁcient training data, communication bandwidth,
and computation power provided by participants [8]. In other
words, deﬁcient participants might cause FL to malfunction in
reality. Therefore, incentive mechanisms are required to inspire
more clients with high-quality data and sufﬁcient resources
to engage in cooperative learning, which ﬁnally achieves the
performance improvement of FL.

Fortunately, incentive mechanism has attracted great interest
and some impressive studies mushroom in the last two years.
Among these results, Zhan publishes a representative survey
of incentive mechanism for FL [56], and they summarize the
existing studies into three categories, i.e., clients’ contribu-
tion, reputation, and resources allocation. Compared with this
valuable overview, we provide another survey of incentive
mechanism with the distinct understanding, comprehensive
taxonomy, totally innovative summary perspective, and differ-
ential insights for the future studies. These two complemen-
tary surveys provide a systemic and comprehensive summary
together to interested readers.

Initially, we identify the problem of incentive scheme in
FL and highlight the statement that the ﬁnal goal of incentive
design is to improve the training performance of FL. This is
because that a few high-quality participants outperform a large
number of staggers in FL. This special requirement makes
the incentive issue in FL quite different from those in other
scenarios. We further point out three components of incentive
mechanisms, i.e., contribution evaluation, node selection, and
payment allocation, and also present a novel taxonomy for

1In this paper, the payment includes cash payment, business reputation, the

share of well-trained model, etc.

 
 
 
 
 
 
further review.

Subsequently, we explicitly summarize the existing studies
in the roadmap of main techniques, which contain Shapley
value, Stackelberg game, auction, contract theory, reinforce-
ment learning, blockchain, etc. Among them, Shapley value
is usually adopted for contribution evaluation, while payment
allocation mostly involves Stackelberg game, auction, con-
tract theory, and non-convex maximization. Some cutting-edge
techniques such as reinforcement learning and blockchain are
adopted as auxiliary mechanisms for node selection, contribu-
tion evaluation and robustness improvement. It should be noted
that one technique is used for many functionalities instead of
only one. In this survey, we also underline the assumptions of
these studies, which is the key point of mechanism design and
will deﬁnitely beneﬁt future studies.

By comparing the pros and cons of prominent studies from
multi-dimensional views, we ﬁnally obtain some insights of
opportunities and challenges in the future studies. We believe
is the top priority
that (1) the performance improvement
of incentive mechanisms in FL. The implicit relationship
between incentive mechanisms and performance improvement
should be considered seriously in the design of FL; (2) some
scenarios such as Mobile Edge Computing (MEC), 5G/B5G,
and IoTs add special constraints to the incentive designs in FL.
The incentive mechanisms should be well designed for these
speciﬁc application scenarios; and (3) incentive mechanisms
for cross-silo FL are neglected in current studies, and we
should put more emphasis on cross-silo FL in the near future.
The rest of this paper is organized as follows. In Section II,
We present the introduction of FL, the problem statement of
incentive mechanism, and its taxonomy. Then, we separately
summarize the existing incentive schemes in terms of main
techniques adopted in Section III. In Section IV, we compare
some representative results and then point out three directions
in the future study. Section V concludes the entire paper.

II. FEDERATED LEARNING, PROBLEM STATEMENT, AND
THE TAXONOMY

A. Federated Learning

Federated learning is a distributed training paradigm which
aims at minimizing the loss function L(w) of global model
with many participants in a collaborative way. Typically, sev-
eral rounds of training are involved to obtain the optimal model
parameters w∗(t). In each round of training, the parameter
server ﬁrstly distributes the global parameter w(t) to some
participants. Then, chosen participant i performs local model
training. In horizontal FL, where each participant contains
distinct data samples with identical features and local model,
local training is to compute gradient descent as wi(t + 1) =
wi(t+1)−η∇Li(wi(t)) with the local data set Di. In vertical
FL, where participants contains identical data samples with
different features, data alignment and information exchange
are required among participants in the local training, besides
the computation of gradient descent. After ﬁnishing local
training, participants upload their local parameters wi(t + 1)
to the global server. When the global server obtains enough
updates, it aggregates these parameters with algorithms like

2

Fig. 1. The system architecture of FL

FedAgv to get a new parameter w(t+1) of global model. This
process iterates until the global model accuracy is satisﬁed
or the training time exceeds the predeﬁned threshold T . The
training procedure of FL is shown in Fig. 1. In Fig. 1,
participants might be mobile devices, edge nodes, and IoTs
devices in cross-device FL or giant companies in cross-silo
FL. They provide various types of resources instead of only
data, all of which are key factors to the training performance.

B. Problem Statement of Incentive Mechanisms in FL

Most Incentive mechanisms are to inspire more qualiﬁed
clients to participate in FL training with sufﬁcient resources.
In the following, we present the theoretical formulation of
incentive mechanism.

Deﬁnition 1 (Incentive Mechanism). (1) There exists a set
of |N | potential candidates N = {0, · · · , N − 1}, and each
candidate i has the multi-dimensional contribution denoted
by the vector qi and the type proﬁle vector θi. As a rational
individual, it maximizes the proﬁt as

πi = pi − ci(qi, θi),

(1)

where pi is the payment obtained from the model owner and
ci(·) is the corresponding cost function.

(2) The global server (or model owner) maximizes its proﬁt

function as

π = U (Q) −

(cid:88)

pi,

where U (·) : Q (cid:55)→ R is called the utility function and Q =
(q0, · · · , qN −1).

(3) The incentive mechanism is to ﬁnd the optimal Q in
order to obtain some desirable properties or maximize some
objective functions such as social welfare. Any candidate i
with qi = 0 indicates that it is not selected for FL training.

In Deﬁnition 1, three important issues are left to be explic-
itly explained. One issue is related to the optimal solution Q
for each player2. Among various solutions, Nash Equilibrium
(NE) strategy is desirable since no player has incentive to

2In this paper, we use the term player to indicate both participants and

global server (the model owner).

Incentive Mechanism of FL Payment AllocationNode selectionContribution EvaluationFairnessGoalIncentiveMechanismSettingsCross-device FLCross-silo FLTechniquesShapley ValueContractAuctionStackelberg GameBlockchainNode SelectionComplete InformationSub-problemInformation SymmetryPayment AllocationContribution Measurement Participant 0Participant N-1Participant iModel Income12Download ParametersLocal Model Training3Local Updates4Global AggregationData Value AnalysisPerformance Improvement Collusion ResistantIncentive CompatibilityIndividual RationalityPareto EfficientReinforcement LearningWeakly Complete InformationIncomplete InformationPayment allocationchoose another resource provision. The formal deﬁnition of
NE is given in Deﬁnition 2. Besides the NE strategy, there
exists some other types of equilibrium solutions, such as dom-
inant strategy equilibrium, Bayesian equilibrium, and perfect
Bayesian equilibrium. Interested readers refer to [9] for more
details.

3

Deﬁnition 2 (Nash Equilibrium). The strategy set of all the
players (qN E
, qN E
N −1) is a Nash Equilibrium, if any
1
participant i has

, · · · , qN E

0

πi(qN E
i
−i = (qN E

0

, qN E

−i ) ≥ πi(qi, qN E
, · · · , qN E

where qN E
Si is one of strategies for participant i.

i−1, qN E

−i ), ∀qi ≥ 0,

i+1, · · · , qN E

N −1), and qi ∈

Another issue is about the desirable properties of incentive
mechanisms. Most previous incentive schemes in other sce-
narios target at the properties of Incentive Compatibility (IC),
Individual Rationality (IR), fairness, Pareto Efﬁciency (PE),
Collusion Resistant (CR), and Budget Balance (BB), all of
which are also the focus of incentive mechanisms in FL. The
illustrations of these characteristics are listed as follows.

• Incentive Compatibility (IC): An incentive mechanism
has the property of IC when it is optimal for all the play-
ers to truthfully declare their contributions and cost types.
In other words, reporting the counterfeit information will
not beneﬁt the revenue for the malicious player.

• Individual Rationality (IR): An incentive mechanism is
IR only if all the participants have non-negative proﬁts,
i.e., πi ≥ 0, for ∀i. IR indicates that candidates hesitate
to join in FL when its payment is less than its cost.
• Fairness: Some predeﬁned fairness functions, such as
contribution fairness, regret distribution fairness, and
expectation fairness [11], are maximized, and then the
incentive mechanism achieves the property of fairness.
Fairness is key to the sustainable collaboration in FL.
• Pareto Efﬁciency (PE): When the social surplus (also
called social welfare), i.e., π + (cid:80)
i pi, is maximized, an
incentive mechanism is PE, which evaluates the overall
proﬁt of federated learning.

• Collusion Resistant (CR): An incentive mechanism is CR
if none of subgroups of participants can obtain higher
proﬁts by conducting fraudulent activities in collusion.
• Budget Balance (BB): A scheme is BB iff the sum of
payment for participants is no more than the budget given
by the model owner or the global server.

In FL, we argue that

the incentive mechanism should
have additional requirement of Performance Improvement (PI),
which makes it quite different from the classic incentive mech-
anisms. This speciﬁc requirement stems from the phenomenon
that high-quality participants outperforms many staggers in
FL. In other word, inspiring a large number of participants
with constraint resources and low-quality training data may
negatively impact
incentive
mechanism should achieve the property of PI in FL [8], [2].

the performance of FL. Thus,

Remarks 1 (Incentive Mechanisms of FL). The incentive
mechanisms in FL should ultimately improve the performance

Fig. 2. The framework of incentive mechanism in FL

of FL in terms of model accuracy, training speed, communi-
cation overhead, computational costs, etc.

The third issue involves the sub-components of incentive
mechanism. In FL,
incentive mechanism is composed of
contribution evaluation, node selection, and payment alloca-
tion. Contribution evaluation endeavors to get the accurate
measurement of contribution qi for each participant. The
simplest measurement is just the size of training data in many
existing studies. Meanwhile, Shapley value is a powerful tool
for contribution evaluation, which will be explicitly illustrated
in Section III.A. In addition, we can ﬁnd some studies of
data value in other research area, and it might be considered
as a ingredient of contribution evaluation. Interested readers
refer to [34] for more details. Node selection is to choose
a subset of qualiﬁed participants to join in FL training. In
FL, the criteria of node selection not only cover the basic
resources requirement but also involves the economic factors,
i.e., contributing most with least cost. Payment allocation
decides the payment for each chosen participant. It should
be mentioned that these three components are interdependent.
And a single technique, such as procurement auction, may
involve several sub-components. The framework of incentive
mechanism is shown in Fig. 2.

C. The Taxonomy of Incentive Mechanisms

The existing studies of incentive mechanisms can be cate-
gorized in terms of application settings, the FL phase, main
techniques, sub-problems, and the assumption of information
symmetry. Fig. 3 presents our proposed taxonomy of incentive
schemes in FL.

• Application Settings: FL can be applied to both cross-
device scenarios and cross-silo scenarios. In cross-device
FL, a large number (about 102 to 1010) of vulnerable
mobile node, IoTs devices, or smart edge with non-
IID training data and constraint resources dynamically
and collaboratively train a global model for the third
party such as the application service provider. On the
contrary,
less than 100 giant organizations/companies,
such as medical hospitals and ﬁnancial companies, train
a shared model with sufﬁcient data and geo-distributed
data centers in cross-silo settings. These two scenarios are
quite distinct in a series of aspects. More comparisons can

Payment AllocationNode SelectionContribution EvaluationFairnessGoalIncentiveMechanismSettingsCross-device FLCross-silo FLTechniquesShapley ValueContractAuctionStackelberg GameBlockchainNode SelectionComplete InformationSub-problemInformation SymmetryPayment AllocationContribution Measurement Participant 0Participant N-1Participant iModel Income12Download ParametersLocal Model Training3Local Updates4Global Aggregation    Performance Improvement Collusion ResistantIncentive CompatibilityIndividual RationalityPareto EfficientReinforcement LearningWeakly Complete InformationIncomplete InformationPayment allocationIncentive Mechanism of Federated Learning TrainingPredictingPhasesBudget Balance4

A. Shapley Value

Shapley value originated in cooperative game theory is
widely adopted by incentive mechanisms of FL, especially
for the contribution evaluation and proﬁt allocation [22], [24].
Since there does not exist an explicit linear relation between
data size of individual participant and accuracy of global
model, it is unreasonable to use the metric of data size to
directly evaluate its contribution for each participant. For
instance, providing lots of low-quality data sample may not
help building a global model [22]. Moreover, the situation
becomes much complicated when multi-dimensional resource
provision is considered in FL [8]. Based on these observations,
Shapley value is employed to evaluate the contribution from
the view of utility and impacts on the collaborative training.
The core idea of Shapley value is to calculate the weighted
average of marginal contribution as Shapley value. Let func-
tion v(S), where S ⊆ N , be the utility of model collabo-
ratively trained by the subset S. The Shapley value ϕ(i) of
participant i is given as

ϕ(i) =

(cid:88)

S⊆N \{i}

|S|!(N − |S| − 1)!
N !

{v(S ∪ i) − v(S)}.

(2)

From Eq. 2, we can ﬁnd that v(S ∪ i) − v(S) is marginal
contribution when participant i joins in the federated training
with the subset S. In the following, we present a toy example
to demonstrate the calculation of Shapley value.

Example 1. Suppose N = {0, 1, 2}. The utilities of different
coalitions are listed as follows:

v(∅) = 0

v({0}) = 5

v({1}) = 10

v({2}) = 15

v({0, 1}) = 30

v({0, 2}) = 40

v({1, 2}) = 60

v({0, 1, 2}) = 100

The calculation of each participant is shown in the following
table. In the ﬁrst line (0 ← 1 ← 2), the marginal contributions
of node {1} and {2} are separately v({0, 1}) − v({0}) = 25
and v({0, 1, 2}) − v({0, 1}) = 70. Similarly, we can get all
the marginal contribution for other lines. Finally, the Shapley
value for each node can be obtained as the average value of
corresponding column.

0 ← 1 ← 2
0 ← 2 ← 1
1 ← 0 ← 2
1 ← 2 ← 0
2 ← 0 ← 1
2 ← 1 ← 0
sum
ϕ(i)

0
5
5
20
40
25
40
135
22.5

1
25
60
10
10
60
45
210
35

2
70
35
70
50
15
15
255
42.5

However, Shapley value is extremely computationally ex-
pensive. The computation complexity of Shapley value is
O(N !), which performs exponential number of utility calcula-
tion [34]. For this computation problem, a series of studies in

Fig. 3. The taxonomy of incentive mechanism in FL

be found in [5]. Currently, most studies focus on cross-
devices cases [12], [13], [6], while a few studies such as
[24] and [53] target at cross-silo FL.

• FL Phase: The entire process of FL includes model train-
ing and model prediction. The training phase targets at
obtaining high-quality global model with efﬁcient training
performance, while model prediction focus on truthful
response of prediction results with many well-trained
models of participants. The design goals and assumptions
vary these two phases, which leads to distinct incentive
schemes [52].

• Main Techniques: The techniques adopted in incentive
mechanism include Stackelberg game, auction, contract,
Shapley value, blockchain, and reinforcement learning.
Stackelberg game [16], [17], [18], auction [8], [19], [20],
and contract [14], [21] are mainly employed by node
selection and payment allocation, while Shapley value
[22], [24] is used for contribution measurement. Both
blockchain [25] and reinforcement learning [19], [26]
are auxiliary techniques to improve the performance and
robust incentive schemes. In Section III, we summarize
the existing studies with a roadmap of main techniques.
• Sub-problems: As shown in the above subsection, the
sub-problems of incentive scheme include contribution
evaluation [27], node selection [26], and payment allo-
cation [25]. It should be noted that some studies solve
several sub-problems of incentive mechanism simultane-
ously instead of only a single sub-problem.

• Information Symmetry: According to the assumption
of information symmetry, the existing schemes can be
categorized into incentive schemes with incomplete in-
formation [31], [32], weakly complete information [21],
and complete information [16], [29], [30].

III. INCENTIVE MECHANISM OF FL

In this section, we summarize the state-of-art

incentive
mechanisms in a technical way. Speciﬁcally, we separately
present the schemes with Shapley value, Stackelberg game,
learning, blockchain, and
auction, contract, reinforcement
other techniques.

Payment AllocationNode SelectionContribution EvaluationFairnessGoalIncentiveMechanismSettingsCross-device FLCross-silo FLTechniquesShapley ValueContractAuctionStackelberg GameBlockchainNode SelectionComplete InformationSub-problemInformation SymmetryPayment AllocationContribution Measurement Participant 0Participant N-1Participant iModel Income12Download ParametersLocal Model Training3Local Updates4Global AggregationPerformance Improvement Collusion ResistantIncentive CompatibilityIndividual RationalityPareto EfficientReinforcement LearningWeakly Complete InformationIncomplete InformationPayment allocationIncentive Mechanism of Federated Learning TrainingPredictingPhasesdata science community have been published for approximat-
ing Shapley value with sampling techniques such as Monte
Carlo, group-testing based approach, probabilistic estimation,
and proper information sharing [33]. Interested readers refer
to [33] and [34] for more details.

When Shapley value is adopted by FL, the computation
optimization is also an interesting topic. In [24], Song et al.
consider the characteristics of FL and propose two gradient-
based methods to efﬁciently calculate model utilities for differ-
ent combinations of enterprise participants. The ﬁrst method
called one-round reconstruction based algorithm collects all
the local gradients in FL training and reconstructs all the
models after training to compute marginal contributions. The
other method utilize the additivity of Shapley value to calculate
contributions in each training round and then aggregate them to
get the ﬁnal result. These two methods trade additional storage
of local gradients for the calculation of marginal contributions
without the need of retraining each model.

Liu proposes a decentralized approximation approach with
blockchain system for Shapley value in FL [25]. In their
framework FedCoin, each miner independently and competi-
tively computes the approximated Shapley value with sampling
technique, and the miner whose result is the nearest to the
average of all the minders is chosen as the winner. The winner
generates a new block and records the payment allocation
according to the average Shapley value in an immutable man-
ner. FedCoin demonstrates its capability to calculate Shapley
value with an upper bound on the computational resource for
reaching consensus. In a nutshell, how to efﬁciently apply
Shapley value in resources-constraint FL is a troublesome
problem left for the future work.

Besides the computational issue, data privacy is another
potential problem for Shapley value in FL. The direct adoption
of Shapley value might reveal the protected feature value or
data sample distribution during the computation of marginal
utility for each coalition. In [22], Wang et al. ﬁrstly point out
this threat in vertical FL and adopt a variant version called
Shapley group value to measure the utility of a feature subset
without revealing the details of any private feature in vertical
FL. In detail, they combine some private features as a united
federated feature and compute the Shapley group value for
this federated feature in a two-participants case. When the
number of participant increases, the computation becomes an
intractable issue.

In FL, Shapley value might has a broad application such
as contribution measurement of participant [22], [24], [25],
participant behaviour analysis [23], and feature selection
[54]. Especially, Ng et al. implement multiple payoff-sharing
schemes (including Shapley value) in a visual tool of a multi-
player game to study how FL participants act under different
incentive schemes through crowdsourcing in [23].

B. Stackelberg Game

Stackelberg game is a sequential model of game theory
commonly used to formulate the interactions between different
players in the sell or procurement of common products,
making it more appropriate for the incentive design in FL

5

[16], [17], [35]. In Stackelberg game, there exist two categories
of players, i.e., leader and follower. The leader optimizes its
proﬁt as maxΦ π(q1, · · · , qN , Φ) by considering the expected
reactions of followers and moves ﬁrst to declare its decision Φ.
Subsequently, follower observes the action of leader, optimizes
its own proﬁts, and responses πi(i ∈ N ). Usually, the leader
or follower is a collection of players instead of one participant,
and a group of players perform Cournot game simultaneously.
In Stackelberg game, the NE solution can be obtained via
backward induction. In details, we ﬁrst get the optimal re-
sponses qi = gi(Φ) = arg max πi(q1, · · · , qN , Φ) by setting
ﬁrst-order derivative with 0, and then substitute them into the
objective function max π(g1(Φ), · · · , gN (Φ), Φ). In this way,
the NE solution (ΦN E, qN E

N ) can be computed.

, · · · , qN E

1

(2) What

Some major issues require to be tackled when Stackelberg
model is used in FL settings. (1) Who are the leader and
follower? Most of studies consider the FL ecosystem as a
monopoly market, where the single leader is the model owner
or the cloud server and the followers are multiple participants
performing Cournot game in the second stage. These studies
are conducted in scenarios of mobile edge computing [29],
distributed coded machine learning [35], IoTs [51], etc. On
the contrary, Feng argues that model owner uses the learning
service provided by mobile devices, and the model owner acts
as a single follower in the lower level of Stackelberg game with
multiple leaders [18]. In this mobile scenario, many mobile
leaders simultaneously perform Cournot game in the ﬁrst stage
of Stackelberg game, while the single follower responses them
with the total requirement of training data. The differences of
these two categories of studies lie in the market assumption of
FL scenarios, and their justiﬁcations are left for future studies.
is the “common product” in FL? Intuitively,
the product is training data samples evaluated by its size,
and model owner trade rewards for training data in [18],
[17]. The second type of product is computation power for
local training. In [16], Sarikaya employs Stackelberg game
model to motivate the CPU provisions of multiple workers
to reduce the local training time with given budget of the
leader in FL with a fully-synchronous SGD. Similarly, Ding
considers a multi-dimensional product distinguished by the
computation speed,
the start-up computation time and the
cost in distributed coded machine learning [12]. In order to
accelerate the computation of NE solution, they ingeniously
transform the multi-dimensional feature into a single metric.
Other impressive products include local
training accuracy
and data privacy. In [29], the leader decides uniform reward
rate (e.g., $/Accuracy Level), and the follower participants
optimize their utility functions with the variable of accuracy
level which can be further used in user selection. In [51],
Yu argues that clients should be compensated for their privacy
loss in FL and adopt the Stackelberg game approach to inspire
more clients contributing private data. The measurement of
privacy utilizes the ρ-zero-concentrated differential privacy
(ρ − zCDP), and the payment allocation is proportional to
the privacy budget of each client.

(3) How to obtain NE solution with complete informa-
tion and incomplete information? The NE solution can be
theoretically computed with backward induction in complete

information scenario [30]. For the practical case with incom-
plete information and stochastic information, Ding ﬁnds that
the computation complexity of NE solution is increased with
additional N (N −1) IC constraints [12]. They not only present
the optimal strategies but also study the impacts of workers’
private information on the NE solution. In [17], Zhan believes
that the assumption of shared decision information among
participants does not hold in realistic IoT applications and
the inaccurate contribution evaluation negatively impacts the
incentive mechanism design. Based on these observations,
they propose a Deep Reinforcement Learning (DRL) based
scheme to dynamically adjust players’ strategies and optimize
their proﬁts in the scenarios with incomplete information and
ambiguous contribution measurement. In the proposed DRL
scheme of Proximal Policy Optimization (PPO), the model
owner separately uses the actor network and critic network to
maintain its policy and estimated value function. In addition,
each edge node trains a complicated DRL model with high
computation overhead and uses an ofﬂine training mode to
learn its optimal strategies in a simulation environment. In
a nutshell, DRL seems to be a promising technique for
computing NE solution in game theory.

(4) Does the proposed model improve the performance of
FL? Two impressive studies correlate the incentive design
with the performance of FL model. In [12], Ding shows that
the optimal recovery threshold should be linearly proportional
to the total participator number when using MDS codes.
In cooperative relay networks, authors balance the tradeoff
between the provision of training data and the participation
of relay services [18]. Also,
they signiﬁcantly reduce the
congestion in the communication for both the model owner
and mobile devices. In sum, we believe that the performance-
aware incentive schemes are more appreciated by FL.

6

in mobile edge computing scenarios [8]. In the phase of
bid announcement, the auctioneer advertises a public scoring
function and its resource requirement. After receiving this ad-
vertisement, participants decide the type of resource provision
and the corresponding price by separately maximizing their
own proﬁts, and then send these bids back to the auctioneer.
With these bids, the remote server chooses K winners based
on the computed score and determines their payments. In [53],
Deng proposes a quality-aware auction scheme with the similar
process in a multi-task learning scenario. One innovative point
is that they formulate the winner selection problem as an
NP-hard Learning Quality Maximization (LQM) problem and
devise a greedy algorithm to perform real-time task allocation
and payment distribution based on Myerson’s theorem. Actu-
ally, most winner selection and payment allocation problems
are computationally intractable, and thus randomized auction
is adopted as a simpliﬁed and effective solution in FL. For
example, Le adopts the framework of randomized auction to
obtain the approximate solution to the knapsack problem of
social cost minimization. The proposed framework contains
three components: 1) an greedy algorithm which achieves
lnC-approximation efﬁciency, 2) convex decomposition, and
3) VCG-based payment allocation [20]. Similarly, Jiao pro-
poses a reverse multi-dimensional auction (RMA) mechanism
which follows a randomized and greedy method to choose
participants and decide the payments in wireless FL scenar-
ios [19]. Finally, Cong proposes an impressive VCG-based
incentive mechanism Fair-VCG which not only maximizes
social welfare but also minimizes unfairness of the federation
in [10]. In Fair-VCG, the proposed approach involves three
steps,
the
computation of VCG payment vector, and the computation
of adjusted payment vector, and its computation complexity is
undoubtedly NP-hard.

the computation of data acceptance vector,

i.e.,

C. Auction

Auction is another efﬁcient mathematical tool for pricing,
task allocation, node selection, etc., and it has been extensively
used in radio frequency spectrum allocation, advertising, and
bandwidth allocation in computer communities [37]. In auc-
tion, there exist two types of players, i.e., the auctioneer and
bidders. The single auctioneer served by the global model
owner or cloud server coordinates the process of auction,
while bidders are participants who response the auctioneer
with various local resources and their bids. The detailed
process of auction can be generalized as bid announcement,
bid collection, winner determination, and some other auxiliary
procedures such as clearing price, information revealing, etc.
In each step, it may contains some speciﬁc methods such
as ﬁrst-price payment, second-price payment, etc., which
generates a veritable menageries of auctions [38]. Interested
readers refer to [37] for deeper technical understanding. In
sum, auction allows participants to actively report their true
type, which makes it more impressive than other game tools.
Currently, [8], [10], [19], [20], and [53] are the representa-
tive studies with auction-based incentive scheme in FL. Specif-
ically, Zeng proposes a lightweight and multi-dimensional
incentive scheme FMore with procurement auction for FL

From these studies, we could ﬁnd some common features
of auction-based incentive schemes. (1) Most of the proposed
schemes are multi-dimensional. In [8], the server advertises
its requirements of various resources, such as data size,
computation power, communication bandwidth, etc. In the
step of winner determination, authors adopt functions like a
Cobb-Douglas function, a perfect complementary function, or
a perfect substitution function to transform multi-attributes into
a scalar, and evaluate this scalar value by a public scoring
function. In [19], Jiao considers data size, data distribution
evaluated by the metric of Earth Mover’s Distance (EMD),
and requested wireless channel in the incentive mechanism.
Instead of incorporating them into one metric, RMA treats
them separately in the random and greedy search of winners.
In [20], authors consider the uplink transmission power and
the computation resource (CPU cycle frequency) with a given
subchannel bundle for mobile participants in cellular networks.
(2) The performance improvement of FL is one of main
goals of incentive design. In [8], Zeng argues that the proposed
scheme FMore is able to inspire high-quality nodes with
sufﬁcient resources to join in FL training and further improve
the performance of FL. Results show that FMore speeds up
FL training via reducing training rounds by 51.3% on average
and improves the model accuracy by 28% for the tested CNN

and LSTM models. Similarly, Deng proposes the quality-aware
FL framework FAIR which integrates incentive mechanisms
with local learning quality estimation and model aggregation
in [53]. Interestingly, they adopt an exponential forgetting
function to generate local quality estimation for node selection,
while the proposed model aggregation considers both the data
size and estimated quality simultaneously. Simulation results
demonstrate that the model accuracy with FAIR outperforms
the knapsack greedy mechanism when the percentage of
mislabeled data ranges from 20% to 80%.

(3) Machine learning is smartly adopted as a auxiliary
technique for auction. In [19], Jiao proposes a deep reinforce-
ment learning-based auction (DRLA) mechanism to improve
the social welfare and simultaneously ensure IC and IR. In
addition, they also use graph neural networks to generate the
embeddings of conﬂict relationship between participants and
then apply these embeddings into the DRLA to automatically
determine service allocation and payment. Neural networks
can also be adopted to solve the functional optimization
problem, since they can approximate any continuous function
with the predeﬁned precision [10]. In Fair-VCG scheme, the
unsupervised and composite neural networks of 2n small
networks are developed to approximate two continuous and
increasing functions which are further used for winner deter-
mination and payment allocation.

D. Contract Theory

Contract

theory is to study how players construct and
develop optimal agreements with conﬂicting interests and dif-
ferent levels of information. Among various types of contract,
public procurement contract is widely adopted for the incentive
mechanism design. In the public procurement contract, the
server offers a menu of contracts to participants, without
being informed about the private cost of participants at the
time contracts are written, and each participant proactively
pick the option that was designed for its type. The public
procurement contract embodies the self-revealing property
which could elicit the optimal provisions from participants
with the presence of information asymmetry. On the contrary,
the performance of information revelation is determined by the
granularity of contract lists, since participants can only choose
a contract.

In FL, the existing studies can be categorized into two
groups,
i.e., contracts with different assumptions of infor-
mation asymmetry and multi-dimensional contracts [13]. In
the ﬁrst groups of studies, authors in [14] initially identify
the information asymmetry between the task publisher and
participants, since the private information of data size and
various resources are unknown for the task publisher like the
base station. They adopt contract theory to map the contributed
resources into a appropriate rewards via contract lists. In de-
tails, they consider the local computing power as the decision
variable for participant and assume a strongly incomplete
information scenario where they only have the knowledge of
the probability that a participant belongs to a certain type.
Formally, the base station optimizes its utility and publishes
a contract list (R(fn), fn), where R(fn) is the reward of the

7

computation resources of type-n participant. After receiving
the contract list, a participant chooses its own true type n
which can maximize its private utility. For NE solution, the
monotonicity constraint is added with IC and IR to solve
the optimization problem for the task publisher. They further
extend this study in [14] by combing contract theory with
reputation and blockchain in [15]. They assume that adversar-
ial participant might perform malicious parameter update to
deteriorate the performance of global model, adopt the metric
of reputation calculated with a multi-weight subjective logic
model to measure the reliability and trustworthiness of mobile
nodes, and then select some high-reputation participants as
candidates. In this study,
the consortium blockchain with
properties of tamper-proof and non-repudiation is leveraged
to secure the reputation management as well.

Another impressive incentive mechanism in the ﬁrst cate-
gory theoretically studies multi-dimensional contract for three
scenarios, i.e. complete informational scenario, weakly incom-
plete information scenario, and strongly incomplete informa-
tion scenario [21]. In complete information case where the
server knows the true type of each participant, the contract
design is to minimize the total sever cost with IR constraint. In
the scenario of weakly incomplete information, they consider
the same objective function with both IR and IR constraints,
since the server does not know the type information of user
and cannot force it to accept a certain contract. In strongly
incomplete information scenario, authors minimize the ex-
pected server cost due to the uncertainty of user type. The
objective optimization function is similar to that in weakly
incomplete information scenario excepted the expectation part.
Since the optimization problem in this case is not necessarily
convex, authors proposed a new contract structure called two-
part uniform contract, the performance of which approaches to
the optimal solution of original optimization problem in this
scenario.

The other group of studies relate to multi-dimensional
resources provisions in FL. In [31], Lim considers multiple
resources of heterogeneity of UAVs and leverages on multi-
dimensional contract theory to design a trustworthy incentive
mechanism for the UAV-enable Internet-of-Vehicles (IoV)
scenario. In this scheme, a participant is categorized into a
multi-dimensional type-(x, y, z, q), where x is traversal cost
type, y is sensing cost type, z is computation cost type, and
z is transmission cost type. The proposed contract design
involves two-step procedure, i.e., multi-dimensional contract
design and traversal cost compensation. The ﬁrst step converts
sensing cost and computation cost into a single-dimensional
contract design problem, while the second step adds a ﬁxed
compensation to derive the ﬁnal contract bundles by consider-
ing additional traversal cost and transmission cost. In [21],
Ding considers both data size and communication time as
their decision variables in the contract design. They argue
that selected participants should satisfy the requirement of
communication time and then the incentive design is simpliﬁed
into a single-dimensional contract problem. In details, they
ﬁrst compute the optimal rewards for any given data size, and
then substitute rewards with aforementioned results to derive
the optimal data size and maximum communication time.

E. Reinforcement Learning

Reinforcement Learning (RL) is a fascinating deep learning
technique to approach to the optimal solution in successive
decision-making scenarios, where an agent repeatedly ob-
serves the environment, performs its actions to maximize its
targets, and gets the response (usually called rewards) from
the environment [39]. It is quite appropriate for the incentive
design in FL. In FL training, the model owner considered as
an agent performs the action of node selection or payment
allocation to elicit high-quality participants to join in the
training of FL. The agent iteratively makes decisions by trial
and error and get the responses of participants (considered as
rewards) to achieve the optimal training performance. From
this description, we can easily ﬁnd that the incentive process
can be modeled by RL [40]. Moreover, most optimization
problems in incentive design are NP-hard, and the large
number of participants further aggravate this problem. Both
indicate that it is almost impossible to get NE solutions, and
we need an approximation technique to derive solutions for all
the parties. In sum, reinforcement learning can be innovatively
applied for the incentive design in FL.

The existing studies of incentive schemes with RL can be
classiﬁed into schemes with discrete action space and contin-
uous action space. As an impressive work with discrete action
set, [26] uses the technique of double Deep Q Network (DQN)
to select nodes from all the candidates to improve the perfor-
mance of FL by counterbalancing the data distribution bias. In
[26], the DQN agent calculates Q function of multiple devices,
and then selects a predeﬁned number of devices according to
the Q value. Wang further replaces DQN with double DQN
to make the estimation of the action-value function at the
agent more stable. In [19], Jiao also employs DQN to truthful
randomized multi-dimensional auction to improve the social
welfare. Speciﬁcally, the agent is to select winners from ﬁnite
candidates by considering channel conﬂicts and local data
distribution, and the reward is the social welfare improvement
contributed by the chosen participant. The proposed method
outperforms traditional reverse multi-dimensional auction in
terms of social welfare while guaranteeing the properties of
truthfulness and IR.

The other category of studies with continuous decision
space mainly concentrate on payment allocation in FL. In [17],
authors applied the impressive PPO algorithm to compute the
payment of participants in Stackelberg game with incomplete
information. PPO enables an agent without prior information
of other players to obtain the approximate solution of NE
strategy. In [55], Zhan reviews three challenges of incentive
mechanisms and points out that computational resources al-
location is key to a successful FL. Then, they focus on the
performance of federated learning systems in real-world set-
tings, and use PPO algorithm to design an effective incentive
mechanism to maximize system performance and minimize
costs simultaneously.

F. Others

Boosted by Bitcoin, blockchain can be considered as a
decentralized digital ledger in a P2P networks, where a copy

8

of append-only ledger with digitally signed and encrypted
transaction is maintained by each participant. Blockchain
provides the features of robust and tamper-proof to FL, and
thus some studies adopt it with incentive schemes to provide
secure or privacy-preserving FL. In [45], Weng points out two
serious problems of security threats from dishonest behaviors
and the deﬁciency of incentive component in FL. For these
problems, they propose a secure and decentralized framework
DeepChain based on blockchain-based incentive mechanism
and cryptographic primitives to provide data conﬁdentiality,
computation auditability, and incentives for parties in col-
laborative FL training. Similarly, Bao adopts the technique
of blockchain to provide auditable federated learning with
trust and incentive attributes [43]. [42] is another impressive
work to propose an economic incentive approach for FL on a
public blockchain. In [42], workers in a given round choose
the model updates submitted by workers in previous round
and use these model parameters to train their own models
with local data. Each worker votes top k previous models,
and the smart contract calculates the vote count of worker
in previous round. The reward is distributed according to
the vote count. In [44], a commitment scheme is proposed
to prevent malicious participants from simply copying and
reporting other’s output, and an incentive strategic game is
provided to inspire participants to behave correctly.

Besides blockchain, some other impressive techniques are
adopted in contribution evaluation [22], [27], [49], node selec-
tion [46], [28], cross-silo FL [6], the incentive of predication
phase [52], fairness-aware and sustainable incentive scheme
[11], [47], etc. In cross-silo setting, Tang ﬁrstly proposes an in-
centive scheme from a public goods perspective and formulate
this problem as a social welfare maximization problem, whose
objective function is non-convex [6]. They argue that
the
resources of organizations are non-excludable public goods.
The proposed scheme satisﬁes the properties of IR and budget
balance. In order to get the NE strategies, they also propose
a distributed algorithm with perfect
information to obtain
the NE solutions. In [52], Weng ﬁrst studies the incentive
issue in the predication phase of FL, where the prediction
accuracy and privacy are their top priority. Speciﬁcally, they
adopt Bayesian game theory to inspire participants to truthfully
make prediction for charged users, and use divergence-based
Bayesian Truth Serum method to fairly reward the participants
according to their truthfulness of prediction. They also employ
truth discovery algorithm and TEEs to boost the accuracy
of prediction and protect the model privacy. The proposed
framework and incentive scheme is IR, budge balance, and
the performance improvement.

For the fairness-aware and sustainable incentive scheme, Yu
studies the temporary mismatch issue between contributions
and rewards in FL, and propose the Federated Learning Incen-
tivizer (FLI) payoff-sharing scheme. FLI dynamically divides
a given budget in a context-aware manner by jointly maxi-
mizing the collective utility while minimizing the inequality
among the data owners. The objective function is value-minus-
regret drift optimization problem, which achieves contribution
fairness, regret distribution fairness, and expectation fairness
simultaneously. FLI is demonstrated to be able to produce

TABLE I
THE COMPARISON OF SOME PROMINENT INCENTIVE MECHANISMS OF FL

9

IC IR PE CR PI

Fairness BB
√

CC

√

√

Auction + NN
Multi-dimensional SG

Main Techniques
[6]
Non-convex Optimization
[8] Multi-dimensional Auction
[10]
[12]
[13] Coalition Game + Contract
[14]
[17]
[19]
[20]
[21]
[22]
[26]
[46]
[31] Multi-dimensional Contract
[45]
[52]
[53]

Contract
SG +DRL
Auction+DRL
Randomized Auction
Contract
Shapley Group Value
DRL
Greedy Algorithm

Blockchain
Bayesian Game theory
Auction

Functionality
PA
NS + PA
NS + PA
NS + PA
NS + PA
NS + PA
NS + PA
NS + PA
NS + PA
NS + PA
CM
NS
NS
NS + PA
NS + PA
CM + PA
CM + PA + NS

√
√ √ √
√ √ √
√ √
√ √
√ √
√
√ √ √
√ √ √
√ √

√ √
√ √
√
√ √

√

√

√

√
√

√
√

√

√
√

Efﬁcient
Large
Efﬁcient

Large

Efﬁcient

Large

√
√

Large

Efﬁcient

near-optimal collective utility while limiting data owner’s re-
gret [47], [11]. In [50], Chen studies the mechanism design of
incentive in multi-party machine learning. They assume a dis-
tinct setting called interdependent value with type-dependent
action spaces, where agents can never over-report its type.
They also provide the optimal truthful mechanism in the quasi-
monotone utility setting, and give the necessary and sufﬁcient
conditions for truthful mechanisms in the most general case.
In [28], authors adopt the technique of information elicitation
to theoretically study the scoring rule for truthful reporting of
local parameters with Bayesian Nash Equilibrium strategies
in two cases, i.e., elicitation with veriﬁcation and elicitation
without veriﬁcation.

not suitable for mobile scenarios and resource-constraint
participants.

• Those studies which adopt a single-dimensional metric
might not be appropriate for FL scenarios. Both resources
provided by participants and model performance should
be multi-dimensional in FL.

• Most current mechanisms only targets at a single aspect
of incentive scheme, and few studies involve node se-
lection, payment allocation, and contribution evaluation
simultaneously. However, these sub-problems are corre-
lated, and should be considered together from a systemic
point of view.

IV. COMPARISONS AND FUTURE STUDIES

B. Future Studies of Incentive in FL

A. Comparisons and Analysis

In this section, we compare some prominent studies from
the aspects of design goals and their main functionality, shown
in Table. I3. From these comparisons, we can derive the
following conclusions:

• All the node selection schemes target at the performance
improvement of FL, while some recent incentive schemes
such as [8] and [53] start to concentrate on the perfor-
mance improvement of FL, besides the properties of IR,
IC, fairness, etc.

• Most of studies use complicated techniques such as
reinforcement learning, blockchain, Shapley value, which
introduces large computational cost and communication
overhead. The complicated and expensive computation is

3Stackelberg Game (SG), Node Selection (NS), Payoff Allocation (PA),
Contribution Measurement (CM), Pareto Efﬁcient (PE), Collusion Resistant
(CR), Performance Improvement (PI), Budget Balance (BB), Computational
Cost (CC)

In FL,

the study of incentive mechanism is still

in its
infancy, and there exist many open issues. We just point out
some directions for the future studies.

(1) The future incentive scheme should aim at

improving
the performance of FL with low costs by inspiring more
participants to join in FL. One point is the ﬁnal goal of
incentive mechanism is deﬁnitely to improve the perfor-
mance of FL. Otherwise, it is useless to design incentive
schemes, even though they can inspire more participants.
Another key point is that the proposed incentive scheme
should be lightweight, since resource-constraint nodes
hesitate to perform expensive computation of incentive
method.

(2) Researchers should put more emphasis on cross-silo FL.
The decision behavior of large company/organization is
distinct from that of end users and mobile devices, which
further requires a totally new incentive method for cross-
silo FL. Moreover, FL has the widespread applications in

cross-silo setting, making incentive schemes more indis-
pensable and critical.

(3) Comprehensive incentive schemes are required for FL.
This indicates that we should consider a multi-dimensional
metric instead of a single-dimensional metric. In addi-
tion, future studies also appeal to multi-goals and multi-
functionalities in one scheme.

(4) Some cutting-edge technologies, like graph neural net-
works, generative adversarial networks, multi-agent rein-
forcement learning, etc., might ﬁnd its potential appli-
cation in the incentive design of new scenarios such as
mobile edge computing and 5G/B5G.

V. CONCLUSION

In this survey, we provide a comprehensive introduction to
the incentive mechanism design in FL scenarios. We formulate
the incentive problem and present a taxonomy and some
design goals of incentive mechanisms. The existing studies are
summarized according to the main techniques they adopt, i.e.,
Shapley value, Stackelberg game, auction, contract theory, re-
inforcement learning, blockchain, etc. For each technique, we
not only brieﬂy present the brief introduction but also present
the main problems in incentive design. After reviewing the
current results, we compare them in several aspects and then
point out four future directions, i.e., performance improvement
of FL with low cost, incentive schemes in cross-silo setting,
comprehensive incentive design, and the adoption of cutting-
edge technologies in new scenarios.

REFERENCES

[1] H. McMahan, E. Moore, D. Ramage, and B. Arcas, “Federated
Learning of Deep Networks using Model Averaging,” arXiv preprint
arXiv:1602.05629v2, 2017.

[2] R. Gu, C. Niu, F. Wu, G. Chen, C. Hu. C. Lu, and Z. Wu, “From Server-
based to Client-based Machine Learning: A Comprehensive Survey,”
ACM Computing Survey, 2021.

[3] Q. Yang, Y. Liu, T. Chen, and Y. Tong, “Federated Machine Learning:
Concept and Applications,” ACM Transactions on Intelligent Systems
and Technology (TIST), vol. 10, no. 2, pp. 1201-1215, 2019.

[4] A. Hard, K. Rao, R. Mathews, F. Beaufays, S. Augenstein, H. Eichner,
C. Kiddon, and D. Ramage, “Federated Learning for Mobile Keyboard
Prediction,” arXiv preprint arXiv:1811.03604, 2018.

[5] P. Kairou, H. McMahan, B. Avent, A. Bellet, M. Bennis, A. Bhagoji,
et al., “Advances and Open Problems in Federated Learning,” arXiv
preprint arXiv:1912.04977, 2019.

[6] M. Tang and V. Wong, “An Incentive Mechanism for Cross-silo
Federated Learning: A Public Goods Perspective,” in Proc. of IEEE
INFOCOM, 2021.

[7] Z. Wang, M. Song, Z. Zhang, Y. Song, Q. Wang, and H. Qi, “Beyond
Inferring Class Representatives: User-level Privacy Leakage From Fed-
erated Learning,” in Proc. of IEEE INFOCOM, 2019.

[8] R. Zeng, S. Zhang, J. Wang, and X. Chu, “FMore: An Incentive Scheme
of Multi-dimensional Auction for Federated Learning in MEC,” in Proc.
of IEEE ICDCS, 2020.

[9] M. Cong, X. Weng, H. Yu, and Z.Qu, “FML Incentive Mechanism
Design: Concepts, Basic Settings, and Taxonomy,” in Proc. of IEEE
the 1st International Workshop on Federated Learning for User Privacy
and Data Conﬁdentiality, 2019.

[10] M. Cong, H. Yu, X. Weng, J. Qu, Y. Liu, and S. Yiu, “A VCG-
based Fair Incentive Mechanism for Federated Learning,” arXiv preprint
arXiv:2008.06680v1, 2020.

[11] H. Yu, Z. Liu, Yang Liu, T. Chen, M Cong, X. Weng, D. Niyato, and
Q. Yang, “A Fairness-aware Incentive Scheme for Federated Learning,”
in Proc. of AAAI/ACM Conference on AI, Ethics, and Society (AIES),
2020.

10

[12] N. Ding, Z. Fang, L. Duan, and J. Huang, “Incentive Mechanism Design
for Distributed Coded Machine Learning,” in Proc. of IEEE INFOCOM,
2021.

[13] W. Lim, Z. Xiong, C. Miao, D. Niyato, Q. Yang, C. Leung, and H.
Poor, “Hierarchical Incentive Mechanism Design for Federated Machine
Learning in Mobile networks,” IEEE Internet of Things Journal, vol. 7,
no. 10, pp. 9575-9588, 2020.

[14] J. Kang, Z. Xiong, D. Niyato, H. Yu, Y. Liang, and D. Kim, “Incentive
Design for Efﬁcient Federated Learning in Mobile Networks: A Contract
Theory Approach,” arXiv preprint arXiv:1905.07479, 2019.

[15] J. Kang, Z. Xiong, D. Niyato, S. Xie, and J. Zhang, “Incentive Mecha-
nism for Reliable Federated Learning: A Joint Optimization Approach
to Combining Reputation and Contract Theory,” IEEE Internet of Things
Journal, vol. 6, no. 6, pp.10700-10714, 2019.

[16] Y. Sarikaya and O. Ercetin, “Motivating Workers in Federated Learning:
A Stackelberg Game Perspective”, IEEE Networking Letters, 2020.
[17] Y. Zhan, P. Li, Z. Qu, D. Zeng, and S. Guo, “A Learning-based Incentive
Mechanism for Federated Learning,” IEEE Internet of Things Journal,
vol. 7, no. 7, pp. 6360-6368, 2020.

[18] S. Feng, D. Niyato, P. Wang, et al., “Joint Service Pricing and Coop-
erative Relay Communication for Federated Learning,” arXiv preprint
arXiv:1811.12082 , 2018.

[19] Y. Jiao, P. Wang, D. Niyato, B. Lin, and D. Kim, “Toward an Automated
Auction Framework for Wireless Federated Learning Services Market,”
IEEE Transactions on Mobile Computing, 2020.

[20] T. Le, N. Tran, Y. Tun, Z. Han, and C. Hong, “Auction Based Incentive
Design for Efﬁcient Federated Learning in Cellular Wireless Networks,”
in Proc. of IEEE Wireless Communications and Networking Conference
(WCNC), 2020.

[21] N. Ding, Z. Fang, and J. Huang, “Incentive Mechanism Design for
Federated Learning with Multi-Dimensional Private Information,” in
Proc. of International Symposium on Modeling and Optimization in
Mobile, Ad Hoc, and Wireless Networks (WiOPT), 2020.

[22] G. Wang, C. Dang, and Z. Zhou, “Measure Contribution of Participants
in Federated Learning,” in Proc. ofIEEE International Conference on
Big Data, 2019.

[23] K. Ng, Z. Chen, Z. Liu, H. Yu, Y. Liu, and Q. Yang, “A Multi-player
Game for Studying Federated Learning Incentive Schemes,” in Proc. of
IEEE IJCAI, 2020.

[24] T. Song, Y. Tong and S. Wei, “Proﬁt Allocation for Federated Learning,”
in Proc. of IEEE International Conference on Big Data (BigData), 2019.
[25] Y. Liu, S. Sun, Z. Ai, S. Zhang, Z. Liu, and H. Yu, “FedCoin: A
Peer-to-Peer Payment System for Federated Learning,” arXiv preprint
arXiv:2002.11711v1, 2020.

[26] H. Wang, Z. Kaplan, D. Niu, and B. Li, “Optimizing Federated Learning
on Non-IID Data with Reinforcement Learning,” in Proc. of IEEE
INFOCOM, 2020.

[27] T. Nishio, R. Shinkuma, and N. Mandayam, “Estimation of Individual
Device Contributions for Incentivizing Federated Learning,” in Proc. of
IEEE Globecom, 2020.

[28] Y. Liu and J. Wei, “Incentives for Federated Learning: A Hypothesis
Elicitation Approach,” arXiv preprint arXiv:2007.10596v1, 2020.
[29] S. Pandey, N. Tran, M. Bennis, and Y. Kyaw, “Incentivize to Build: A
Crowdsourcing Framework for Federated Learning,”, in Proc. of IEEE
Globecom, 2019.

[30] L. Khan, S. Pandey, N. Tran, W. Saad, Z. Han, M. Nguyen, and C.
Hong, “Federated Learning for Edge Networks: Resource Optimization
and Incentive Mechanism,” arXiv preprint arXiv:1911.05642, 2019.
[31] W. Lim, J. Huang, Z. Xiong, et al., “Towards Federated Learning
in UAV-Enabled Internet of Vehicles: A Multi-Dimensional Contract-
Matching Approach,” arXiv preprint arXiv:2004.03877, 2020.

[32] J. Zhao, X. Chu, H. Liu, Y. Leung, and Z. Li, “Online Procurement Auc-
tions for Resource Pooling in Client-Assisted Cloud Storage Systems,”
in Proc. of IEEE INFOCOM, 2015.

[33] R. Jia, D. Dao, B. Wang, et al., “Towards Efﬁcient Data Valuation Based
on Shapley Value,” in Proc. of IEEE 22nd International Conference on
Artiﬁcial Intelligence and Statistics (AISTATS), 2019.

[34] A. Ghorbani and J. Zou, “Data Shapley: Equitable Valuation of Data for

Machine Learning,” arXiv preprint arXiv:1904.02868v2, 2019.

[35] H. Yu, G. Iosiﬁdis, B. Shou, and J. Huang, “Market Your Venue with
Mobile Applications: Collaboration of Online and Ofﬂine Business,” in
Proc. of IEEE INFOCOM, 2018.

[36] H. Yu, E. Wei, and R. Berry, “A Business Model Analysis of Mobile

Data Rewards,” in Proc. of IEEE INFOCOM, 2019.

[37] S. Parsons, J. Aguilar, and M. Klein, “Auctions and Bidding,” ACM

Computing Surveys, vol. 43, no. 2, pp. 1-66, 2011.

11

[38] P. Duetting, Z. Feng, H. Narasimhan, D. Parkes, and S. Ravindranath,
“Optimal Auctions Through Deep Learning,” in Proc. of IEEE ICML,
2019.

[39] K. Arulkumaran, M. Deisenroth, M. Brundage, and A. Bharath, “Deep
Reinforcement Learning: A Brief Survey,” IEEE Signal Processing
Magazine, vol. 34, no. 6, pp. 26-38, 2017.

[40] M. Hessel, J. Modayil, H. Hasselt, T. Schaul, et al., “Rainbow: Comb-
ing Improvements in Deep Reinforcement Learning,” arXiv preprint
arXiv:1710.02298v1, 2017.

[41] R. Yang, F. Yu, P. Si, Z. Yang, and Y. Zhang, “Integrated Blockchain
and Edge Computing Systems: A Survey, Some Research Issues and
Challenges,” IEEE Communications Surveys & Tutorials, vol. 21, no. 2,
pp. 1508-1532, 2019.

[42] K. Toyoda and A. Zhang, “Mechanism Design for An Incentive-aware
Blockchain-enabled Federated Learning Platform,” in Proc. of IEEE
International Conference on Big Data (Big Data), 2019.

[43] X. Bao, C. Su, Y. Xiong, W. Huang, and Y. Hu, “FLChain: A Blockchain
for Auditable Federated Learning with Trust and Incentive,” in Proc. of
IEEE BigCom, 2019.

[44] Y. Lu, Q. Tang, and G. Wang, “On Enabling Machine Learning Tasks
atop Public Blockchains: A Crowdsourcing Approach,” in Proc. of IEEE
International Conference on Data Mining Workshops (ICDM), 2018.

[45] J. Weng, J. Weng, J. Zhang, M. Li, Y. Zhang, and W. Luo, “Deepchain:
Auditable and Privacy-preserving Deep Learning with Blockchain-based
Incentive,” IEEE Transactions on Dependable and Secure Computing,
2019.

[46] T. Nishio and R. Yonetani, “Client Selection for Federated Learn-
ing with Heterogeneous Resources in Mobile Edge,” arXiv preprint
arXiv:1804.08333v2, 2018.

[47] H. Yu, Z. Liu, Y. Liu, T. Chen, M. Cong, X. Weng, D. Niyato, and Q.
Yang, “A Sustainable Incentive Scheme for Federated Learning,” IEEE
Intelligent Systems, 2019.

[48] B.Yan, Y. Zhou, J. Wang, L. Liu, Y. Zhang, and X. Nie, “A Real-
time Contribution Measurement Method for Participants in Federated
Learning,” arXiv preprint arXiv:2009.03510, 2020.

[49] H. Cai, D. Rueckert, and J. Palmbach, “2CP-Decentralized Protocols to
Transparently Evaluate Contributivity in Blockchain Federated Learning
Environments,” arXiv preprint arXiv:2011.07516v1, 2020.

[50] M. Chen, Y. Liu, W. Shen, Y. Shen, P. Tang, and Q. Yang, “Mech-
anism Design for Multi-Party Machine Learning,” arXiv preprint
arXiv:2001.08996, 2020.

[51] R. Hu, Y.Gong, “Trading Data For Learning: Incentive Mechanism for

On-Device Federated Learning,”, in Proc. of IEEE Globecom, 2020.

[52] J. Weng, J. Weng, H. Huang, C. Cai, and C. Wang, “FedServing: A Fed-
erated Prediction Serving Framework Based on Incentive Mechanism,”
in Proc. of IEEE INFOCOM, 2021.

[53] Y. Deng, F. Lyu, J. Ren, Y. Chen, P. Yang, Y. Zhou, and Y. Zhang,
“FAIR: Quality-Aware Federated Learning with Precise User Incentive
and Model Aggregation,” in Proc. of IEEE INFOCOM, 2021.

[54] S. Lundberg and S Lee, “A Uniﬁed Approach to Interpreting Model

Predictions,” in Proc. of NIPS, 2017.

[55] Y. Zhan, P. Li, S. Guo, and Z. Qu, “Incentive Mechanism Design
for Federated Learning: Challenges and Opportunites,” IEEE Networks,
2021.

[56] Y. Zhan, J. Zhang, Z. Hong, L. Wu, P. Li, and S. Guo, “A Survey of In-
centive Mechanism Design for Federated Learning,” IEEE Transactions
on Emerging Topics in Computing, 2021.

