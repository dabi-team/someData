HIGH PERFORMANCE CONSENSUS WITHOUT DUPLICATION:
MULTI-PIPELINE HOTSTUFF

2
2
0
2

l
u
J

7

]

C
D
.
s
c
[

4
v
9
7
1
4
0
.
5
0
2
2
:
v
i
X
r
a

Taining Cheng
School of Software
Yunnan University
Kunming, China
tirning@outlook.com

ABSTRACT

The state-of-the-art HotStuff operates an efﬁcient pipeline in which a stable leader drives decisions
with linear communication and two round-trips of message. However, the unifying proposing-voting
pattern is not sufﬁcient to improve the bandwidth and concurrency performance of the modern
In addition, the delay corresponding to two rounds of message to produce a certiﬁed
system.
proposal in that scheme is a signiﬁcant performance bottleneck. Thus, this study developed a new
consensus protocol, Multi-pipeline HotStuff, for permissioned blockchain. To the best of the authors’
knowledge, this is the ﬁrst protocol that combines multiple HotStuff instances to propose batches
in order without a concurrent proposal, such that proposals are made optimistically when a correct
replica realizes that the current proposal is valid and will be certiﬁed by quorum votes in the near
future. Because simultaneous proposing and voting are allowed by the proposed protocol without
transaction duplication, it produced more proposals in every two rounds of messages. In addition,
it further boosted the throughput at a comparable latency with that of HotStuff. The evaluation
experiment conducted conﬁrmed that the throughput of Multi-pipeline HotStuff outperformed that
of the state-of-the-art protocols by approximately 60% without signiﬁcantly increasing end-to-end
latency under varying system sizes. Moreover, the proposed optimization also performed better when
it suffers a bad network condition.

Keywords Blockchain · Consensus · Byzantine fault tolerant · Partial synchrony

1

Introduction

The emergence of blockchain has signiﬁcantly affected Byzantine fault-tolerant (BFT) consensus in the last decade.
As a core infrastructure of blockchain, the BFT consensus protocol [1, 2] build the basis for implementation of state
machine replica and the execution of smart contracts in the decentralized network. Through the BFT protocol, each
party of the system maintains the same state of data, even enduring arbitrary failures when a request that can change the
internal state of the system originates from outside. The protocol ensures that 2f + 1 out of n correct parties agree on
the order of requests (where n is system size and f is the threshold of failures, such that n > 3f ). Subsequently, the
state transition of the system occurs through the execution of requests in the same order. Recently, this idea was used to
build a blockchain service, a modern cryptocurrency system, in addition to the replication system. To reach a consensus
on the order of requests, the classical BFT protocol employs a leader-based method to drive consensus instances to
continuously maintain the state of the system under partial synchrony [3], where the message delay is at most ∆ after
an unknown global stabilization time (GST). Futher, in PBFT [4], each consensus decision amortizes the authenticator
cost of the network, O(n2), with the cost of view-change being more, namely O(n3).

However, signiﬁcant costs hindered the large-scale deployment of the system until the HotStuff is presented, which is a
new paradigm that allows optimistic responsiveness [5] and linear view-change. With threshold signature schema [6],
the quadratic cost of the normal case can be reduced to a linear form, and an additional consensus phase overcomes a
stumbling block in BFT consensus. Consequently, each block acquires the ﬁrst two phases to lock a safe position that
will never be forked in the future and an additional phase to commit the block in that position. However, the simple

 
 
 
 
 
 
A PREPRINT - JULY 8, 2022

addition of a phase is insufﬁcient. Thus, a chained structure wherein pipeline voting results in the following proposal
is introduced. This structure uniﬁes three phases of one block into a proposing-voting pattern, effectively improving
system performance with the features of a linear cost and responsiveness. Certain organizations, such as Meta, have
adopted it to implement permissioned blockchain services.

Although the introduction of the chained structure opened a new chapter that bridged the classical BFT and blockchain
consensus. a bottleneck gradually surfaced. In particular, in the context of this linear proposing-voting pattern, it
is referred to as a single-pipeline bottleneck in this paper, which describes the poor efﬁciency when it is used as a
critical commit path in the consensus protocol. In the proposing-voting pattern, a replica can have more than one role.
The leader collects quorum votes to generate QC when the validator sends its vote of the proposal to it. Thereafter, a
proposal extends the block associated with that QC. From a macro perspective, regardless of the role, either a proposal
drives replicas to validate the proposal and send a vote, or quorum votes drives a new leader to propose. Investigations
of the manner in which driven messages are produced and transmitted can entail more space to break through the
performance bottleneck.

The performance bottleneck. First, a simple proposing-voting pipeline results in the replica always facing a blank
period of time that requires no action, owing to no driven message being received. With the system operation, the
accumulation of unused blank time limits the performance of a system that cannot facilitate execution. Second, the
pipelined operation provides the ability that extends an on-consensus block before being committed. This is in contrast
to only extending a decided block based on safety property such as classical BFT. Thus, the system can now generate a
new block after a proposing and voting cycle and consequently commit a block. However, But the delay of two rounds
of messages remains essential. Third, every party requires considerable time to perform cryptographic primitives and
hash function, such as producing signatures and mapping arbitrary-size input to a ﬁxed-size output, each of which is
computationally intensive. Meanwhile, the transmission module is free when the replica performs computation on the
CPU, thereby resulting in the concurrent capability of the multi-core system and the bandwidth not being fully exerted.
Consequently, the single proposing-voting approach can be signiﬁcantly optimized.

Therefore, the natural intuition for optimizing the single pipeline pattern involves each replica producing the maximum
blocks possible to fully drain the bandwidth and the ability of concurrent [7, 8]. Some solution was built around
this idea was proposed, allowing parallel leaders to propose batches independently and concurrently, such as incase
of Mir BFT [8]. This led to improved performance, despite the need for an extra transaction partition strategy to
preclude duplication problems and duplication attacks, which are caused through independent proposing. While the
dilemma is that these duplications can simply be ﬁltered out after an ordering, but the damage has already been done—
excessive resources, bandwidth, and possibly CPU have been consumed, although the duplication is a consequence of
insufﬁcient optimization. Consequently, a natural question arises: Is it possible to design a consensus protocol that
avoids duplication while retaining the ability to overcome the performance bottleneck made by the single pipeline
pattern?

Contributions. To the best of the authors’ knowledge, Multi-pipeline HotStuff (MPH), which is proposed in this paper,
is the ﬁrst novel BFT protocol that ensures that one proposal is generated per round of messages. Without increasing
the latency, MPH produces twice as many proposals as those produced by HotStuff owing to the redesigning of the
consensus instance, which combines two HotStuff instances to drive consensus. Moreover, MPH provides efﬁcient
view-change and responsiveness [5] through the adoption of a three-phase proposing-voting pattern [9]. Thus, each
block is committed by its third QC and locked by its second QC.

The conservative approach of HotStuff involves the extension of the block that has a QC associated with it. In contrast,
the key idea involved in MPH is that the correct replica optimistically extends a block based on a veriﬁed (not certiﬁed)
proposal when it realizes that the proposal can be voted on, and it becomes the leader of the next view. Consequently,
this study redesigned a new view mechanism in MPH, which only comprised one round of messages, with the replicas
always waiting for votes and blocks. This optimistic extension resolved several of the challenges mentioned above. First,
simultaneous proposing and voting can facilitate concurrency when a replica can perform both roles. The multi-pipeline
pattern allows a new proposal to be made in each round of messages and for one proposal to be voted on in each round
of messages. In addition, the blank period of time in the single pipeline owing to no driven messages being received
in the single pipeline is eliminated. Second, the optimistic proposal by the correct leader reduces the two rounds of
delay to one. When a replica is voting and preparing a proposal, the computationally intensive operations and network
transmission are fully exploited. Third, MPH does not suffer from the duplication issue because it does not adopt
parallel leaders to address the performance bottleneck.

The challenge in MPH is ensuring the total order of all proposals that are optimistically proposed. Consider a case
wherein replicas have received no messages and timeouts, and two blocks are not certiﬁed because neither of them has
a QC. At this time, the replicas may be unaware of the timeout because of network latency or a malicious leader. In
addition, this study presented a view-change solution to guarantee that these two unsafe blocks are forked and a new

2

A PREPRINT - JULY 8, 2022

block is extended at least from the locked position, such that all the locked blocks are eventually committed after the
GST.

MPH was implemented in GO and deployed with up to 58 nodes in a LAN and WAN environment (with different
bandwidths). Thereafter, it was compared to the state-of-the-art BFT protocols such as HotStuff and Streamlet, which
have a pattern similar to the protocol proposed in this paper. The results showed that MPH convincingly outperformed
HotStuff in terms of throughput by 60% and outperformed Streamlet by more than 60%, regardless of the network
condition. Moreover, the performance under a faulty leader or network instability was also comparable to and sometimes
exceeded that of the state-of-the-art protocols.

The remainder of this paper is organized as follows. Section 2 describes the system model and preliminaries. Section
3 presents an analysis of the DiemBFT, a variant of HotStuff, and explores the factors contributing to performance
metrics. Sections 4 and 5 provide the design and implementation of the protocol, respectively. Section 6 presents the
safety and liveness proofs. Further, the evaluation and related works are presented in Sections 6 and 7, respectively.
Finally, the conclusions and future works are presented in Section 8.

2 Preliminaries

This study assumed a system comprising a ﬁxed set of n = 3f + 1 replicas, each replica indexed by number i,
where i ∈ {1, ..., n}. Replicas corrupted up to f of the total by the adversary are referred to as Byzantine faulty and
may arbitrarily deviate from the protocol, while the rest of the replicas are correct. Speciﬁcally, adaptive and static
adversaries were considered for the upper and lower bounds, respectively. However, the difference was that static type
of adversary must decide the replicas to be corrupted at the beginning of every run, whereas adaptive can decide the
same during operation.

2.1 Commnunication and Network

As a distributed system, Network communication links are reliable and authenticated. They are implemented on a
point-to-point basis; that is, all messages sent among correct replicas are eventually delivered if and only if the sender
sent that message to the receiver. However, the links can be controlled by the adversary; the adversary controls the
delivery time. This study referred to the network assumption proposed by Dwork et al. [3] to model the links, where
there is a known bound ∆ and an unknown Global Stabilization Time (GST). For instance, the messages transmitted
between two correct replicas arrive within time ∆ after GST, which is referred to as a partially synchronous. In contrast,
there is no guarantee that the message will arrive with ∆, that is, the network is asynchronous.

2.2 Cryptographic Assumptions

A standard digital signature was assumed, and public-key infrastructure (PKI) [10, 11, 6] was provided by a trusted
dealer. All replicas were equipped with a single public key, and each of the n replicas was assigned a distinct private
key. A replica i can use its private key to sign a partial signature on message m, which is denoted by ρi ← tsigni(m).
The (k, n)-threshold signature scheme, a set of partial signatures P = {ρi|ρi ← tsigni(m), i ∈ I, |I| = k} of any k
replicas can be used to generate a digital signature σ ← tcombine(m, P ) on message m, where I is the set of replica
indexes that contributed partial signatures. Further, any replica can verify the signature using the public key through
the function tverif y(m, σ). Moreover, a replica is aware that quorum replicas have given valid partial signature on
message m if the tverif y(m, σ) return true. Although perfect cryptographic schemes do not exist in practice focusing
on the distributed aspect of the problem in a cryptographic is sufﬁcient. Consequently, given a valid signature σ on m,
no adversary or adversaries has an overwhelming probability of generating a signature σ for m. This study adopted the
threshold of k = 2f + 1, where the valid signature σ on message m implied that quorum replicas in such a system had
received the message m.

In addition, a cryptographic hash function h(·) [12], which mapped an arbitrary length input to output of ﬁxed size was
assumed. The probability of any replica generating a couple of distinct inputs m and m
) is
negligible; and the output of the hash function is referred to as the digest of the input, which can be used as a unique
identiﬁer for input.

subject to h(m) = h(m

(cid:48)

(cid:48)

2.3 BFT SMR

In a distributed environment, the consistency of all replicas is obtained through State Machine Replica [1, 4], wherein
each replica begins with the same state and receives inputs in the same order to reach the same state. The BFT protocol

3

A PREPRINT - JULY 8, 2022

aids in each correct replica safely committing transactions from the client in a total order manner. It is expected that the
protocol never compromises the safety, and the following property must be guaranteed in all runs:

• Safety: All correct replicas commit the same transaction at the same log position.
• Liveness: Each transaction from client is eventually committed by all correct replicas.

Regarding validity, the committed transactions satisfying certain application-dependent predicate can be implemented by
the addition of validity [13] checks on transactions before replicas propose or vote. For simplicity, a valid “transaction
domain” was considered as a binary case, which is the value v ∈ V, V = {0, 1}.

• Validity: if all correct replicas propose the same value v ∈ V , then no correct replicas commit a value other

than v.

The SMR problem solved by HotStuff [14] provides the new pattern to be followed, which involves pipelining the
different voting stages of the same batch of transactions into the one-round voting of different batches. Consequently,
this study also followed the path to solving the BFT Agreement problem, with further details presented in section 3.

2.4 Notations

First, the notations used throughout the whole protocol are elucidated.

View Number. The BFT protocol is implemented via the preceding instance of single-shot Byzantine agreement
individually; at most, one block can be committed within one instance. Further, each replica indexes the instance by the
view number, which is initially to 0 and then increases monotonically.

Block structure. The block is denoted by a tuple b =< id, v, p, txs, qc, ρ >, where id = hash(v, p, txs, qc) is the
digest of the current block’s information, which is also used as block’s identiﬁcation. v is the view number of block;
each block must refer to the block id in the previous view as the precursor that indicates which block the current block
is based on (precursor ﬁeld is used in our protocol). Further, txs is a batch of pending transactions, ρ is the partial
signature from the proposer on block’s id, and qc is also deﬁned blew.

Quorum Certiﬁcate A Quorum Certiﬁcate(QC) is formatted as a tuple qc =< block, v, σ >.It is a data type for proof
where the block B is validly signed on block id, generated via the combination of the partial signatures from a quorum
n − f = 2f + 1 replicas. Consequently, it is implies that a block is received by a quorum replica and veriﬁed. Given a
data structure of x, x.y was used to refer to the element y of the original structure x throughout this study. In addition,
the blocks and QCs can be ordered based on their view number; the highest block or QC indicates that the view number
is the largest.

Timeout Certiﬁcate A timeout mechanism is required such that the system obtains liveness under partial synchrony. A
timeout certiﬁcate (TC) was generated through the combination of a quorum of n − f = 2f + 1 timeout message, the
partial threshold signature of timeout view v, and the highest QC qch of replica’s local. In contrast to QC, TC must
contain the respective qch in the timeout message from n − f replicas, denoted by tc =< v, qchigh, σ >, where qchigh
is the QC with the highest view number of its sender.

2.5 Complexity Measurement

We measure the complexity of protocol using the number of authenticators instead of message size because the message
size is not related to metrics of the cost for one transaction. In addition, the bit of message cannot be bounded well
to capture the cost. However, every message must be signed by the sender, and the receiver must verify the signature
and combine quorum partial signature shares; these cryptographic operations are computationally intensive. Thus, the
authenticator complexity is a suitable way to capture the overall costs of the protocol.

3 Description of Pipeline Paradigm

This section introduces a variant of HotStuff referred to as DiemBFT. There are two components of the protocol, with
the idea being inspired by PBFT [4], linear normal commit path progression, and quadratic cost view-change owing to
the malicious leader or asynchronous periods. During the normal process, a designated leader L proposes a block b that
extends the block associated with the highest QC of its own. Thereafter, other replicas attempt to update their locked
view and the highest QC to check if any block can be committed when receiving block b. Then, block b is voted for by
sending a partial signature to the next leader Lv+1. Subsequently, the next leader Lv+1 forms a QC for view v when it
collects n − f votes of the previous view v and enter view v + 1. Consequently, a new block is proposed by a leader for

4

A PREPRINT - JULY 8, 2022

that view. The normal case repeatedly drives the system to commit transactions linearly; each block extends a block of
the previous view, with a QC of that extended block being sufﬁcient to prove safety. However, in case of any timeout
owing to delay of the transmission or a malicious leader remaining silent, the timeout message being broadcast in view
v must carry a QC, which is the highest QC of the sender. This results in the new leader possessing the ability to prove
that no action more recent than the block of that QC is committed. The leader proposes a block with TC of view v,
where the TC contains a threshold signature on view v. Further, the proposed block in view v + 1 extends the block
with the highest QC in the timeout messages. When any replica receives a block with a TC, it attempts to advance the
view and checks if any block satisﬁes the 3-chain commit rule, which implies that the ﬁrst of three adjacent blocks that
have QC and all uncommitted blocks before the ﬁrst can be committed.

Vote Rules. Before a replica vote for the proposal, whether at least one of two rules is satisﬁed is examined as follows:

• b.v = b.qc.v + 1

• b.v = b.tc.v + 1 and b.qc.v ≥ qclocked.v

Block b contains the QC of the previous view or TC, which was formed in the previous or bad views. The ﬁrst rule
implies that voting for a block with a monotonically increasing view is safe and directly extends the block of b.qc.block,
owing to no possibility of two QCs forming in one view. If a block b(cid:48) is certiﬁed except the case when a block that has
QC and b(cid:48) s.t. b.v = b(cid:48).v + 1, f + 1 correct replica votes for b(cid:48) and updates its qch to QC of block b(cid:48). The second rule
implies that a valid block must extend at least a locked block, which is qchigh.block. Once a bad view occurs, at least
one QC qchigh of block b must be included in any future timeout message. Moreover, even if one of the certain logging
replicas proposes a block with a QC, all correct replicas’ vote rule predicate can be true, although a certain few are
ahead of the leader; thus, the block b will never be forked.

3.1 Complexity

From a theoretical perspective, the 3-chain commit rule brings higher efﬁciency under synchrony and correct leader
because multicast communication is disassembled into two linear broadcast communication through the introduction
of the threshold signature scheme. Considering the complexity in view-change phase, the three-phase consensus
mechanism eliminates the requirement if replica broadcasting its own messages set of “Prepare” to prove its state in
the view-change process from PBFT. In contrast, the timeout message is sent to a new leader by each replica. The
communication complexity of all cases in HotStuff remains linear, such as O(n). A block requires seven rounds of
message exchange to be committed, containing two views and a QC broadcast phase; bringing network assumption into
the rule implies that the latency of one commit is 7∆.

3.2 Throughput and Latency Analysis

Considering the study on dissecting the chained-BFT performance [15], the life-cycle of the manner in which a block
can be certiﬁed was analyzed to obtain the correlation between the permanence metrics and the vital environment
factors. In a distributed setting, data in any form that is transmitted over the network must be bit stream such that the
bandwidth is the upper limit of task processing in distributed cooperative systems. In pipelined methods, a consensus
instance comprises proposing and voting steps, where the time consumption of an instance can be used to calculate
the throughput and latency. A consensus instance begins with building a proposal, then transmitting it to every other
replica, and ends when the leader of the next view collects quorum votes on the previous proposal such that a QC is
formatted to advance the view to the next.

Here, the focus is on the synchronous network, while Byzantine faults are ignored. The average service time that a
block is certiﬁed can be divided into several parts:

ts = 3tCP U + 2tN IC + tL + tQ

(1)

where tCP U captures the delay of signature operations (can be a constant number), tN IC = 2mb−1 is the delay of
data frame conversion rate at sender and receiver, where m is the total size of a block, tL is the Round-Trip Time [16]
that captures the network transmission delay by the normal distribution, and tQ is the expected delay of collecting
a quorum of votes from replicas, which is modeled by ( 2N
3 − 1) order statistics of N − 1. As is known, each new
certiﬁed block commits an ancestor certiﬁed block, and ts is used to approximately compute TPS, which is denoted as
T P S ≈ |m|txt−1

s . Further, the BPS can be obtained as

BP S ≈

1
(3tCP U + 2tQ + tL) ∗ m−1 + 4 ∗ b−1

(2)

5

A PREPRINT - JULY 8, 2022

However, this is far from reaching the upper limit of bandwidth despite the ﬁrst term of (3tCP U + 2tQ + tL) ∗ m−1
being close to 0. Thus, regardless of b value, the overall BP S provided by any distributed cooperative system is much
smaller than the bandwidth, particularly when 3tCP U + 2tQ + tL contributes in excess. In addition, the average latency
can be modeled as

Latency ≈ 3ts + 6tL

(3)

3.3 Limitations.

The core contribution of the Pipeline method is that unifying the different phase in PBFT’s [4] consensus instance into a
proposing-voting pattern. Recall the model given above, it provides a quantitative methodology to show which part of
time consumption is contributing to the total block service time within one instance of Pipeline method. The term tCP U
and 2tN IC + tL occurs in sequence, resulting in the same period of time (∆ = 3tCP U + 2tN IC + tL), with either only
CPU or network contributing to the actual block service time. First, it is embodied in the consensus that only one block
is to be proposed in the two rounds of messages. Further, the period of time 2tN IC + tL is only used for sending or
receiving messages and not for building proposals or preparing votes simultaneously. In particular, the network delay
term signiﬁcantly exceeds the CPU term. It is similar to one single pipe running such that other non-leaders are not
fully utilized except for the period of process vote. Second, the safety of consensus requires that a view number can
only be assigned to one block; only the leader was eligible to propose. Moreover, the advancement of a view must be
after QC is formatted. However, the design of the single-pipe protocol itself becomes a performance bottleneck that is
difﬁcult to breakthrough.

As shown in Figure 1, the time required for both voting and proposing round during the running of implementation of
the standard HotStuff was measured; the consumption of two rounds is approximately equal except for abnormal data,
and each requires up to half of the time-consumption of each view. Further, the parallelism of the single pipeline is low
as replicas are required to wait for another round of voting after proposing, thereby resulting in a lower block generation
rate. Thus, the essence of the bottleneck is that the proposing and voting round is sequential in a single pipeline, thereby
generating insufﬁcient blocks. The next section shows that it is possible to better utilize each round by shrinking two
rounds of the view into one round to exploit each round. The validator votes in each round, whereas the leader proposes
in each round.

Figure 1: time consumption of proposing and voting phase within one view

4 M-Hotstuff Design

To strengthen the performance of this pipeline method optimization in partial synchronous BFT protocol such as
Hotstuff or its variants, we propose a multi-pipeline scheme that can make the replica propose and vote in every message
round. We call the new protocol Multi-pipeline Hotstuff (MPH), which has linear communication cost for normal phase
and quadratic cost for view-change phase. Although theoretically cost is the same cost as Hotstuff, the performance is
almost doubled. In this paper, we only take two pipelines into consideration and leave exploration to future work, the
full picture of protocol is presented in algorithm 1.

Protocol Overview Different from the original works within one view, other replicas do nothing, only run the timer
when the leader proposes. On the contrary, the idea behind our multi-pipeline is that the correct leader optimistically
makes a proposal that extends the latest block, which is considered to be safe to vote. At the same time, the none-leader
(validator) makes vote for block in that view. Note that, at most one replica has more than one role in view. In other

6

0246810Time Consuming(ms) Proposing VotingVotingProposing0246810(cid:5)(cid:8)(cid:9)(cid:6)(cid:1)(cid:4)(cid:11)(cid:10)(cid:12)(cid:13)(cid:9)(cid:8)(cid:10)(cid:7)(cid:2)(cid:9)(cid:12)(cid:3) 25%~75% Range within 1.5IQR Median Line Mean OutliersA PREPRINT - JULY 8, 2022

words, voting and proposing happen at the same time but for different blocks. Such as view v + 2, the leader of v + 2
proposes a block b + 2, the validators vote for block in view v + 1. As we can see from Figure 2, each view contains
only one round of message exchange for voting and proposing; more blocks will be proposed in the same number of
rounds compared to the original single-pipeline implementation. In MPH, each block contains two “links” to previous
block; one is block’s id of the parent and indicates the global order of block. And the other is the QC of the previous
block, which indicates that the previous block was voted by quorum replicas. The block can be committed once it is the
ﬁrst of three or two adjacent blocks linked by the QC “link” depending on the 3-chain or 2-chain rule from the Pipeline
paradigm.

Figure 2: Normal Phase Overview of Multi-pipeline Hotstuff

4.1 Complementary of model

Timeout Message. the timeout message is used to form a complete TC, which is denoted by a tuple tm =< vf, vs, ρ >.
Here vf is the current view that time outs and is used to form a TC that used to advance the ﬁrst view after view-change.
Further, vs is the next view of vs; and ρ is the partial signature shared on vs and vs.
Block structure. The block structure is deﬁned in multi-pipeline pattern as b =< id, v, p, pv, txs, qc, tcs, tc, ρ >,
where tc and tcs are current formed by replicas, tc is used by those lagging replicas and tcs is a set of TC, which will
be used to verify the tc is valid. Further, pv is the view of block with ID p and can be used to check safety rule when
time outs.

4.2 One Round Trip View

MPH also implements BFT protocol through consensus instances indexed by view numbers. Each view has a designated
leader, which is a replica deterministically deﬁned by the view number (e.g., id = v mod n, where n is the total number
of replicas). The difference lies in the fact that only one round message trip occurs within a view, the leader makes a
proposal and broadcasts it to other replicas, and then the view is updated through the QC in the proposal. As soon as the
replicas receive the proposal, the validator votes for the corresponding block, which implies that voting and proposing
to occur parallelly if a replica is leader. In contrast to HotStuff, the proposing and voting processes of the same block
occurs sequentially within the same view, and the replicas update the view after the block has a valid QC in the current
view. In MPH, owing to only one round trip occurring in each view, only one of the above events can be achieved. This
ensures the continuous advancement of the view each round, and consequently, the view number of the block’s QC is
computed by qc.v = b.v + 1.

7

0312v+1v+2v+3v+4pQC(b)QC(b+2)QC(b+1)QC(b+3)bb+1b+2b+3b+4v+5b+5QC(b+1)b+6QC(b+2)0312vv+1v+2v+3v+4pQC(b-1)QC(b)QC(b+2)QC(b+1)QC(b+3)bb+1b+2b+3b+4b-1v+5b+6QC(b+1)b+7QC(b+2)A PREPRINT - JULY 8, 2022

4.3 Block State

Owing to the design of one round trip view to more precisely elaborate the whole protocol, two types of block states
during the consensus process, the certiﬁed and veriﬁed states, were deﬁned. A block is considered certiﬁed if there
exists a QC for the block, that is, qc.block = b. Further, a block is veriﬁed if the proposer’s partial signature was
authenticated and the QC in the block was well-formatted by a quorum. The former state implies that a quorum of
replicas have seen the block and voted for it. Thus, following the 3-chain commit rule, the block can be committed after
another two certiﬁed blocks. A correct replica will perform the proposing and voting step based the current block state.
The new certiﬁed block will commit a preceding certiﬁed block, and the new veriﬁed block will be extended to build a
new proposal for next view.

4.4

3-chain Predicate in M-Hotstuff

While MPH performs in terms of the 3-chain rule, the protocol tracks certain vital variables of block and QC to satisfy
safety and liveness. The ﬁrst of two adjacent certiﬁed blocks chained by QC is referred to as the locked block, which
indicates that no higher block could have reached a commit state. Further, the QC corresponding to the block is referred
to as locked QC. Recall the Vote Rule referred to in Section 3, which stated that a proposal is accepted by correct
replicas if the branch of the new proposal extends from the currently locked block and the view number is monotonically
increasing. In this study, two types of locked variables were deﬁned: qccurlock used to keep track of the current locked
position to which pipeline the currently processing block belongs, and qclaslock used to keep track of the previous
locked position where another pipeline exists. Because there are multiple locked positions in the multi-pipeline setting,
a proposal is expected to extend at least a locked block, which is the block certiﬁed by qccurlock. In addition, another
two variables record the highest and the second-highest QC, denoted as qchigh and qcsechigh, respectively. These are
used to form a new chained block for their respective pipelines.

Figure 3: 3-chain predicate of Multi-pipeline HotStuff

4.5 Normal-Case Operation

In MPH, a transaction pool was designed to buffer transactions from the client to the provider proposer for fetching
payloads in batches. However, as the size of the pool affects system latency, it was set as a conﬁgured parameter in
subsequent evaluation. In a stable view, the Leader L, upon entering view v, collects votes for the proposal in the
previous view v − 2 to generate certiﬁcation (QC, suppose qc) through the combining of quorum partial signature shares.
Thereafter, a block b extends the latest veriﬁed block that is known to the leader. According to the Block Structure
deﬁned in Section 2, the QC is included inside the block, and a batch of transactions is pulled from the pool as payload
(txs). However, other metadata must be formatted correctly. Moreover, the leader no longer extends the certiﬁed block
by the highest QC. Instead, it extends the latest veriﬁed block, which is essential to ensure that each round of view
generates a new block.

As a validator, when receiving the ﬁrst view v block from L, any replica attempt to verify the authenticity of the
leader’s proposal b to check if the proposer is the leader of the block’s view and if the signature on the block is valid.
Subsequently, the block’s state becomes veriﬁed, and if the QC in block b is valid, then the block associated with QC
becomes certiﬁed, that is, qc.block. Thereafter, validators advance its view number, and the second-highest QC is
updated to the highest QC, followed by the highest QC to the latest QC in block b. Moreover, the laslock and curlock
are updated one step forward. Thus, batch transactions in the ﬁrst block of three adjacent certiﬁed blocks are chained by
QC, which are provided to the execution module to commit. Optimistically, a block that is considered veriﬁed will
eventually be certiﬁed in the next view unless the next leader is faulty. Based on this, two parallel sub-procedure were
designed for simultaneous proposing and voting. If the validator is the next leader, it sends a signal to the proposing

8

0312v+1v+2v+3v+4pQC(b)QC(b+2)QC(b+1)QC(b+3)bb+1b+2b+3b+4v+5b+5QC(b+1)b+6QC(b+2)0312vv+1v+2v+3v+4pQC(b-1)QC(b)QC(b+2)QC(b+1)QC(b+3)bb+1b+2b+3b+4b-1v+5b+6QC(b+1)b+7QC(b+2)pQC(b-1)QC(b)QC(b+2)QC(b+1)QC(b+3)bb+1b+2b+3b+4b-1QC(b+4)b+5b+6b`QC(b+5)b`QC(b+3)b`QC(b+1)qccurlock:QC(b+1)    qclaslock:QC(b+2)    qcsechigh:QC(b+3)    qchigh:QC(b+4)A PREPRINT - JULY 8, 2022

procedure to start making a proposal for that view, provided there is a valid QC of the previous view generated.
Meanwhile, the validator follows the 3-chain rules to check if this block can be voted. Consequently, according to the
3-chain predicate deﬁned above, the safety rule for voting in the normal case is obtained as

• Safety: b.v = b.qc.v + 1 and b.pv = verif iedB.v, the block b directly extends from the latest veriﬁed block

with latest QC.

If the proposal is satisﬁed with the safety rule, then a threshold signature on b is sent to the leader of view b.v + 2 as a
vote. Finally, the normal case repeatedly drives the system to process transactions.

4.6 View-Change Case

When the timer of a certain view v expires, owing to the lack of a mechanism to detect whether the timeout is caused by
asynchrony, the message is delayed, or the leader is faulty. Consequently, the view-change mechanism is triggered to
advance to the next view v + 1 and replace another leader for ensuring liveness. In the case of this study, two blocks
were considered uncertiﬁed. Consequently, the view-change period of MPH comprised two consecutive views, each
of which required a corresponding TC be advanced. As the timeout occurs, all replicas stop voting for that view and
multicast a timeout message containing a threshold signature share for v − 1, v and its second-highest QC qcsehigh as
well as highest QC qchigh. When any replicas receive a quorum of such timeout messages, ﬁrst, the timeout messages
are checked to keep having the latest QC. Thereafter, two TCs of view v and v + 1 are formed through the combination
of quorum partial signature shares, denoted as tc1 and tc2, respectively. Finally, the liveness rule for voting can be
expressed as:

• Liveness1: b.v = b.tc.v + 1 and b.qc.v = qccurlock.v + i ∗ 2, b.pv = qclaslock.v − 1 + i ∗ 2, i ∈ {0, 1}.
• Liveness2: b(cid:48).v = b(cid:48).tc.v + 1 and b(cid:48).qc.v = qclaslock.v + i ∗ 2, i ∈ {0, 1} and b(cid:48).pv = verif iedB.v.

In the ﬁrst view, after timeout, the leader of that view v + 1 ﬁrst enters through TC tc1, and then proposes a block
that directly extends the block of qchigh.block. Recall that in the necessary metadata of the structure of the block
deﬁned in Section 2, the two TCs tc1andtc2 and QC qcsechigh must be included inside the block. Once the validators
receive the block from the leader, it updates its own view based on TC and checks the cryptographic semantics of the
block. Further, it computes whether the block is a valid branch and changes the veriﬁed branch to the new block. The
Liveness1 rule states that a valid proposal in the ﬁrst timeout view must contain a QC that is at least as new as qccurlock,
and the block must directly extend from the successor block of b.qc.block. If Liveness1 is satisﬁed, then the same
parallel sub-procedure is repeated; the next leader ﬁrst updates its view through second TC tc2, and the vote of b is sent
to the leader of view b.v + 2. When the leader reaches the new view b.v + 2, it makes a proposal that directly extends
from the latest veriﬁed block, where the proposal also contains two TCs and the latest QC qchigh. In contrast to the
normal case, no votes are collected in this view. In the second timeout view, the validators also update the view number
by TC tc2 and check the authenticity of the proposer of the block using the Liveness2 predicate to determine whether
to accept the block. Here, the QC b(cid:48).qc cannot be higher than the QC qccurlock, and b(cid:48) must be a successor of latest
veriﬁed block. Consequently, all replicas vote for b(cid:48), the next leader also receives votes of view b.v while also proposing
for the next view. As now a view-change process after the timeout has ﬁnished, it is guaranteed that the system will
attempt to keep working in the event of timeout due to asynchrony or a faulty leader problem. Thus, it ﬁrst needs to
synchronize the view such that at least a quorum replicas start with the same view to prevent inconsistency problems
caused by out-of-sync blocks. Regardless of normal or view-change cases, the voting predicate is true provided either
one of the safety and liveness rules holds. Thus, any replica can use it to determine whether to vote for the proposal.

4.7 Complexity and Latency

Obviously, when the network is synchronous, and the leader is honest, a complete threshold signature of size O(1)
within QC needs at least quorum partial signature shares. Every share of size (1) is given by a unique replica’s
cryptographic tool as a vote and sent to the leader. Owing to a constant number of committing a transaction, it can be
concluded that the overall complexity of reaching a consensus decision in a normal case is O(n). Thus, each transaction
adopts the 3-chain to be committed, and one more round for all replicas receives the proof. This results in a latency of
seven rounds. In the view-change case, when asynchrony occurs, all replicas need broadcast timeout messages with QC
for view synchronization, and each message has size O(2), and the subsequent view-change message of the leader’s
proposal contains three authenticators (O(3)), corresponding to one QC and two TCs. Therefore, the responding vote
also has a single share of size O(1). Consequently, the view-change case yields O(n2) and O(n) complexities of view
synchronization and view-change, respectively. For latency, in a locked transaction that suffered asynchrony with a
latency of at least nine rounds, if the network returns to synchronous, there is at least one branch with two new blocks.

9

The branch costs two more extra rounds. However, the round of latency goes to an inﬁnite round if the GST never
occurs.

Algorithm 1: multi-pipeline HotStuff for replica i

A PREPRINT - JULY 8, 2022

1 Initialize: //initialize all relevant data and start protocol
2

3

timer.Reset();
currV iew ← 1;
Send newView MnewV iew to local

4
5 (cid:66) Normal phase
6 Procedure block Process loop:
7

while T rue do : wait event M

8

if M is a proposal message then P rocessP roposal(M );

9 Procedure proposing loop:
10

while true do : wait event M

11

if M is a newView message then P ropose(M );

12 Procedure vote process loop:
13

while true do : wait event M

14

15

if M is a timeout message then P rocessT M (M );
if M is a vote message then P rocessV ote(M );

16 (cid:66) Finally
17 Procedure timer loop:
while true do
18

19

20

21

if timer.stop() then //Check if the timer expires during any phase

M.timeout ←< vf : currV iew + 1, vs : vf + 1 >;
Broadcast timeout Mtimeout to all replicas

5

implementation

This study implemented all protocols discussed: DiemBFT, a productive variant version of HotStuff and MPH in Golang.
The TCP1 was used to build reliable point-to-point channels to implement the SMR-BFT abstractions correctly. Further,
secp256k12 was used for elliptic curve based signatures. Further, the implementation of Multi-pipeline3, HotStuff and
the corresponding single pattern protocol were open-sourced.

The main framework of the protocol is provided in Algorithm 1, which is described as message-driven. All messages
were generated to drive consensus in a view-by-view loop, and each type of message was given to the corresponding
parallel procedure. Further, a replica performed phase in terms of the received message in succession based on its
role, where the replica can have more than one role (the leader implemented using the round-robin-based way). The
protocol was activated by a new-view message in the initialization step, and then all following runs were based on inner
or external messages. In addition, a timer mechanism always detected the network fails or message delays to ensure
liveness. Ordering: The set of committed blocks must be equipped with a total order and certiﬁed by three adjacent
QC. Moreover, it must be a total ordered set. For instance, the b.p gives the causal order, indicating that the block b is
directly extend from the block with id p. Further, the block’s view number also can be used to describe the relation; for
example, blocks with large view numbers are generated after blocks with smaller view numbers. For agreement, every
replica should retain the same ordered pairs of the block. Considering the design of multi-pipeline, the unique ordered
set may be obstructed by two obstacles, one is the “location” where the branch is built in any case, and the other is
the monotonicity of consecutive advancement of the view when voting and proposing in parallel. Proposing: Similar
to the optimization presented in Section 4, the number of transactions processed by the protocol per unit time can be
improved through chaining and adopting an optimistic path for proposing in each round of messages. There must be
two uncertiﬁed blocks at any view. A normal commit path is that the new proposal carries the QC of the grandparent
block and extends the parent block, implying that the leader proposes and votes in time, and nothing goes bad. However,

1https://pkg.go.dev/net
2https://pkg.go.dev/crypto/elliptic
3https://github.com/tncheng/multipipelinehs

10

A PREPRINT - JULY 8, 2022

once the timer detects the failure of the network or leader, the protocol must drop the latest two uncertiﬁed blocks and
make a new branch from the latest certiﬁed block. Further, the corresponding QC embedded in the block should certify
the block where the previous of the current extends from. In short, one normal and two types of proposing after timeout
entails that the multi-pipeline guarantee the safety of BFT-SMR. The predicate for three types of proposing way is
presented in Algorithm 3, which guides the protocol to extend and certify block correctly.

Algorithm 2: utilities for protocol

1 Function QC(V):
2

4

3

qc.σ ← tcombine(vote.ρ|vote ∈ V );
qc.v ← vote.v + 1 : vote ∈ V ;
qc.bid ← vote.block.id : vote ∈ V ;
return qc;
6 Function TC(TM):
7

5

9

8

tcf.σ, tcs.σ ← tcombine(timeout.ρ|timeout ∈ T M );
tcf.v, tcs.v ← timeout.vf, timeout.vs + 1 : timeout ∈ T M ;
return {tcf, tcs}
10 Function ProcessQC(qc):
11

if currV iew < qc.v then return;
currV iew ← qc.v + 1;
secHighQC, highQC ← highQC, qc;
timer.Reset();

15 Function ProcessTC(tc,tcset):
16

if (tc =⊥) ∨ (tcset =⊥) then return;
else if (currV iew < tc.v) ∧ (tc.v = minv{t.v|t ∈ tcset}) then

currV iew ← tc.v + 1;
secHighT C, highT C ← argminv{t.v|t ∈ tcset}, argmaxv{t.v|t ∈ tcset}

else if (currV iew = tc.v) ∧ (tc.v = maxv{t.v|t ∈ tcset}) then

currV iew ← tc.v + 1;
secHighT C, highT C ←⊥, ⊥

23

timer.Reset();
24 Function Verify(b):
25

27

26

if not tverf y(b, b.σ) then return f alse;
if not tverf y(b.qc, b.qc.σ) then return f alse;
return true
28 Function VoteRule(b):
29

if b.v (cid:54)= currV iew + 1 then return f alse;
if b.tc (cid:54)=⊥ then return safety is true;
else if b.tc.v = minv{t.v|t ∈ b.T cSet} then

12

13

14

17

18

19

20

21

22

30

31

32

33

34

35

return liveness1 is true

else if b.tc.v = maxv{t.v|t ∈ b.T cSet} then

return liveness2 is true

return f alse

View increasing: At a high level, the synchrony of the network is used to model the transmission latency instead of
the ordering of the arrival of messages. Hence, even a message sent ﬁrst may arrive after messages sent later. The
proposing and voting design in multi-pipeline are logically parallel, as opposed to sequentially proposing and voting in
HotStuff. Consequently, the order in which votes of the previous view and the new proposal are received by a replica
is not consistent with the order in which they should be. Some replicas may receive votes of the previous view and
generate QC before the current block. Subsequently, the leader, if any, creates a new branch that conﬂicts with the
unreceived block. The view number should be monotonically increasing even if the replica observes a QC that does not
belong to the current view. A predicate of consecutive advancing view is presented in Algorithm 3, which prevents
block conﬂicts caused by inconsecutive QC.

In addition, the goal of the protocol is to reach an agreement on the order of transactions instead of the content itself. In
the implementation, a subsystem of mempool is built, which removes the transaction dissemination from the critical

11

A PREPRINT - JULY 8, 2022

path of consensus. Here, all the received transactions from clients are disseminated in batches instead of one by one.
Therefore, the transaction body need not be included in consensus blocks. In addition, the consensus only agrees on the
digest of these transactions, implying that all the replicas’ mempool is shared. This signiﬁcant optimization has been
proven by Narwhal [17], which can further improve performance compared to other studies that have not equipped
the sharing strategy. However, in a slight deviation from Narwhal, each digest of the transaction is included in the
consensus messages rather than a simple digest of a batch of transactions. This facilitates the determination of the order
for multiple batches in every consensus instance.

Algorithm 3: main implementation of Multi-pipeline HotStuff

1 Procedure ProcessProposal:
2

if not V erif y(M.block) then return;
P rocessQC(M.block.qc);
P rocessT C(M.block.tc);
B∗ ← M.block; B(cid:48) ← B∗.qc.block; B(cid:48)(cid:48) ← B(cid:48).qc.block; B(cid:48)(cid:48)(cid:48) ← B(cid:48)(cid:48).qc.block ;
if (B∗.qc.bid = B(cid:48).id) ∧ (B(cid:48).qc.bid = B(cid:48)(cid:48).id) ∧ (B(cid:48)qc.v > curlockQC.v) then

laslockQC, curlockQC ← curlockQC, B(cid:48).qc

if (M.block.tc (cid:54)=⊥) ∧ (M.block.T cSet (cid:54)=⊥) ∧ (M.block.tc.v = minv{t.v|t ∈ M.block.T cSet}) ∧ (i =
LeaderOf (M.block.v + 1)) then

MnewV iew.V type ← T imeoutS;
Send newView MnewV iew to local

if (B∗.qc.bid = B(cid:48).id) ∧ (B(cid:48).qc.bid = B(cid:48)(cid:48).id) ∧ (B(cid:48)(cid:48).qc.bid = B(cid:48)(cid:48)(cid:48).id) then

Commit B(cid:48)(cid:48)(cid:48), reply

if not V oteRule(M.block) then return;
verif iedB ← maxv{verif iedB, M.block};
Send vote Mvote to LeaderOf (M.block.v + 2)

16 Procedure Propose(M):
17

if i (cid:54)= LeaderOf (currV iew) then return;
if M.V type = N ormal then

b ← extends verif iedB with highQC

if M.V type = T imeoutF then

b ← extends highQC.block with secHighQC and {secHighT C, highT C}

if M.V type = T imeoutS then

b ← extends verif iedB with highQC and {secHighT C, highT C}

M.block ← b;
Broadcast Proposal Mproposal to all replicas

28

29

26 Procedure ProcessVote(M):
V ← V ∪ M.vote;
27
if (|V | = n − f ) then
qc ← QC(V );
Wait until currV iew = qc.v + 1 to consecutive advance view;
P rocessQC(qc);
M.V type ← N ormal; M.tc =⊥;
Send newView MnewV iew to local;

32

31

33

30

34 Procedure ProcessTM(M):
35

T M ← T M ∪ M.timeout;
if |T M | = n − f then

M.T cSet ← T C(T M );
P rocessT C(argminv{t.v|t ∈ M.T cSet});
M.V type ← T imeoutF ;
Send newView MnewV iew to local

12

3

4

5

6

7

8

9

10

11

12

13

14

15

18

19

20

21

22

23

24

25

36

37

38

39

40

A PREPRINT - JULY 8, 2022

6 Correctness Proof

Recall that as per the deﬁnition of BFT protocol in Section 2.3, it must satisfy two properties, safety, and liveness. If
a block is committed by a correct replica, then all other correct replicas should eventually commit the same block in
the same view, and subsequently, the committed block must form a linear chain linked by their respective QC. In this
Section, the proof of the safety and liveness of multi-pipeline HotStuff is presented.

6.1 Safety

We start with some deﬁnition that we will use:

• bi ← ∗bj indicates that the block bj extends the block bi.
• bi ← qci+1 ← bi+2 indicates that the block bi is certiﬁed by the QC of qci+1, which is contained in the block

bi + 2.

• prepareQCV iew(bi) := bi−4.v, such that bi−4 ← qci−3 ← bi−2 ← qci−1 ← bi.
• lockedV iew(r, bi) can return the locked view of replica r after voting for block bi.

Again, we denote an element y of data structure x as x.y.
Lemma 1. Given any valid two QCs, say qc1 and qc2, there must be qc1.v (cid:54)= qc2.v.
Proof. We prove this by contradiction, suppose there exist qc1.v = qc2.v. The section 2 tells that a valid threshold
signature in QC can be formed only with n − f = 2f + 1 partial signatures for it, and each correct replica only
votes once in the same view. Consider that, the set R1 is the replicas that have voted for block qc1.block, subject to
|R1| = 2f + 1. Likewise, N2 is for qc2.block, subject to |R2| = 2f + 1. Since the model of system is n = 2f + 1, we
have |R1| + |R2| − n = 1, by the quorum intersection, we can conclude that there must be a correct replica who voted
twice in the same view. It’s contradict with the assumption, qc1.v = qc2.v is proved.
Corollary 1. By the Lemma 1, If qc1, qc2 are two valid QCs, such that qc1.v (cid:54)= qc2.v, by the quorum intersection,
Rqc1 ∩ Rqc2 = r, then there must exist a correct replica r such that r ∈ qc1 and r ∈ qc2.
Lemma 2. Consider two certiﬁed blocks: b ← qc and b(cid:48) ← qc(cid:48). under BFT model, if b.v = b(cid:48).v then b = b(cid:48).
Proof. By Lemma 1, we have two valid QCs must be qc.v (cid:54)= qc(cid:48).v, unless qc = qc(cid:48). Two certiﬁed blocks has the same
view, by the rule of V oteRule and P rocessQC, there must be qc = qc(cid:48), then the Lemma 2 is true.
Lemma 3. Assuming that a chain consists of three adjacent certiﬁed blocks, which is linked by QC, and start at view v0
and end at view v4. For every certiﬁed block b ← qc such that b.v > v0, then we have that prepareQCV iew(b) > v0.
Proof. Let b0 ← qcb0 ← b2 ← qcb2 ← b4 ← qcb4 be that chain starting as view b0.v and ending view b4.v. By
Corollary 1, the intersection of QC qc2 and qc has at least one correct replica, denote as r. Since b.v > v4 and V oteRule,
r must votes on b4 ﬁrst, then update it’s lock on b0, that is, lockedV iew(r, b4) = v0. Because the locked view never
decreases, when r votes for the block after b4, such as b, the voting rule of r implies that prepareQCV iew(b) > v0.
Lemma 4. Assuming that a chain with three contiguous view starting with a block b0 at view v0. For every certiﬁed
block b ← qc such that b.v ≥ v0, then we have that b0 ← ∗b.
Proof. Again, let b0 ← qcb0 ← b2 ← qcb2 ← b4 ← qcb4 be the chain starting with b0 and other block’s view is
contiguous, such that: v0 + 4 = b0.v + 4 = b2.v + 2 = b4.v. There are two cases:

• If v0 ≤ b.v ≤ v0 + 4, then b.v can be one of the values: v0, v0 + 2, v0 + 4. By the Lemma 2, b is one of

blocks: b0, b2, b4. Hence, b0 ← ∗b.

• If b.v > v0 + 4. By the Lemma 3, we have prepareQCV iew(b) > v0, this means there exist a block before b,
which is certiﬁed by b.qc, say bi, such that bi.v ≥ v0. Since v0 ≤ bi.v < b.v, we could apply same induction
hypothesis on bi de deduce that b0 ← ∗bi. Finally, b0 ← bi ← b concludes the proof of b0 ← ∗b.

Lemma 5. Consider two conﬂict blocks: b0,b(cid:48)
by a correct replica.
Proof. We proof the by contradiction, suppose two blocks can be committed, then each block is extended by two
certiﬁed block. Let b0 ← qcb0 ← b2 ← qcb2 ← b4 ← qcb4 and b(cid:48)
are the commit
chain of block b0 and b(cid:48)
0.v > b0.v. By
Lemma 4, we must have b0 ← ∗b(cid:48)

4 ← qc(cid:48)
2 ← qc(cid:48)
← b(cid:48)
b(cid:48)
b(cid:48)
4
2
0.v, we assume that b(cid:48)
0. Therefore, Lemma 5 is true.

← b(cid:48)
0 ← qc(cid:48)
b(cid:48)
0
0 unless b0.v = b(cid:48)
0.p and b0 (cid:54)= b(cid:48)

0 respectively. By Lemma 2, b0 (cid:54)= b(cid:48)

0, this is contradict with b0.p = b(cid:48)

0, only one of them can be committed

0, such that b0.p = b(cid:48)

0.p and b0 (cid:54)= b(cid:48)

6.2 Liveness

Lemma 6. When a replica in view v − 1 receives a proposal for view v from another correct replica, it advances into
view v.

13

A PREPRINT - JULY 8, 2022

Proof. If a correct replica make a proposal for view v, it must have seen votes for v − 2 or timeout messages for v − 1,
and well formated a QC or TC of view v − 1. When a correct receives such a proposal with QC or TC, it will advance
the view and enter view v.
Lemma 7. After GST, the message delays between correct replicas are ﬁnite, then all correct replicas keep their view
monotonically increasing.
Proof. suppose that all correct replicas start at least view v, and let R is the set of correct replicas in view v. There are
two cases of set R:

• If all 2f + 1 = |R| correct replicas time out in view v, then all replicas in R will receive 2f + 1 timeout

messages to form a TC and enter view v + 1.

• Otherwise, at least one correct replica r(cid:48), not having sent timeout message for view v. r(cid:48) must have observed a
QC of view v − 1 and keep updated its qchigh accordingly. If r(cid:48) times out in any view> v, then the updated
qchigh, which is never decreased, must be contained in timeout message, it will trigger R to enter a view
higher than v. Otherwise, r(cid:48) must see a QC in all view> v. by Lemma 6, the proposal which contains that QC
from correct replica will eventually be delivered to R, triggering it to enter a higher view.

Lemma 8. If correct replicas has just entered view v, no QC has yet been generated and none of them has timed out.
when correct replica r receives a proposal from correct leader in view v, r will vote for the propposal.
Proof. the predicate in V oteRule checks that:

• View numbers are monotonically increasing. By assumption, none of them has timed out means that no TC
and QC could have been generated for view v and v − 1. Therefore, when predicate V oteRule execute on a
proposal from correct leader is the ﬁrst largest voting view.

• If TC in proposal is empty, the proposal must directly extends the latest veriﬁed block in view v − 1 and in

which QC must directly extends must have view v − 2, all view is consecutive.

• If TC in proposal is non empty, it is based on 2f + 1 timeout messages. A correct leader must track the latest
QC to have view at least as large as the qchigh of each timeout messages during collecting it. The predicate on
QC in proposal must satisfy with qc∗lock.v − b.qc.v ∈ {0, 2} and qchigh.ve > qc∗lock.v. Apparently, r send a
vote for the proposal from correct leader.

Lemma 9. After GST, at most 2∆ from the ﬁrst replica entering view v, all correct replicas receive the proposal from
the correct leader.
Proof. When the ﬁrst correct replica enters view v, if it is leader, all the correct replicas will receive proposal within ∆;
if not, it must have a TC for view v − 1. Next, it will forward TC to the leader of view v within ∆. Upon the leader
enters view v, it immediately makes a proposal and multicasts it to other correct replicas within ∆.
Lemma 10. After GST, for every correct replicas in view v eventually certify a block b and commit a block bc, such
that b.v > v, bc ← qcbc ← ∗b ← b.qc.
Proof. Recall that our system model deﬁned in section 2, at most f out of n are faults, such that n > 3f + 1. Each
leader of view designated by round-robin, let n = 4, f = 1, which is mean that there are three adjacent correct
primaries, say v(cid:48), v(cid:48) + 1, v(cid:48) + 2, v(cid:48) > v. By Lemma 7, all correct replicas keep advancing view. By Lemma 9, at
most 2∆ from any correct replica enters view v(cid:48), the proposal of v(cid:48) will be delivered to all correct replicas. By lemma
8, correct replicas vote for the proposal from correct leader. Repeatedly deducing the argument by lemma 7,8,9,
the block of view v(cid:48), v(cid:48) + 1, v(cid:48) + 2, say bv(cid:48), bv(cid:48)+1, bv(cid:48)+2, will append to the chain until next faulty leader comes up.
Where at least one(bv(cid:48)) of three new blocks will be certiﬁed by QC, which is contained in third block of view bv(cid:48)+2.
By the rule of P rocessQC and V oteRule, when a correct replica successfully perform the vote step on block of
view v(cid:48) + 2, the ﬁrst block bc among new three adjacent certiﬁed blocks linked by QC, will match with commit rule:
bv(cid:48) ← qcbv(cid:48) ← bv(cid:48)+1 ← qcbv(cid:48)+1 ← bv(cid:48)+2. Therefore, the block bv(cid:48) can be certiﬁed and bc
bc ← qcbc
can be committed. Each message in view must be delivered within ∆, all correct replica receives proposal of view
v(cid:48) + 2 after at most 4∆ = 2∆ + ∆ + ∆ time of the ﬁrst correct replica entering view v(cid:48).

certif ied blocks
←−

7 Evaluation

This section evaluates the throughput and latency of the proposed implementations through experiments on WAN
(Alibaba Cloud) and LAN. The leader aim was to demonstrate the better performance of the multi-pipeline mechanism
compared with that of HotStuff and other consensus protocol that have similar principle, such as Streamlet. The
multi-pipeline mechanism ﬁlls batches of transactions per round of message through separated proposing and voting,
which utilizes the gap time in a single pipeline to transport more transactions and efﬁciently utilize bandwidth. However,

14

A PREPRINT - JULY 8, 2022

the commit rule in the multi-pipeline mechanism also follows the 3-chain rule, which can maintain the same level of
commit latency as HotStuff.

A testbed on LAN containing 12 local machines, each supported by Intel Xeon Gold 6230 processors (20 physical
core), 256 GB of memory, and a Debian 11 server, was deployed. For a large-scale test, virtualization technology was
employed to provide up to ﬁve virtual instances per physical machine to run replicas. The maximum TCP bandwidth
measured by iperf was approximately 1 GB per second. In addition, the WAN used an ecs.g6.xlarge instance across 5
different regions: Zhangjiakou, Chengdu, Qingdao, Shenzheng, Hangzhou. They provided a maximum bandwidth of
12.5 MB, 4 virtual CPUs on a 2.5 GHz, Intel Xeon(Cascade Lake) Platinum 8269, and 8 GB of memory, while running
on Debian 11.

Throughput and end-to-end latency were utilized as performance metrics. Throughput is an overall metric that measures
the number of transactions that can be committed per second, denoted as TPS. Further, end-to-end latency measures
the average time required for a transaction to be submitted to the system until it is committed, including network
transmission time, waiting time in transaction buffer, and validation time. As mentioned, the mempool implementation
undertakes the transaction body transmission ability for a fairer measurement of the performance. All the compared
protocols had the exact shared mempool implementation.

7.1 Best Performance

First, the throughput and latency were measured on a small scale, (4, 10 replicas) replicas and 800 block size (digests).
Here, each payload size of the transaction was set to 1024 Bytes, and the mempool’s dissemination batch size was set to
512 KB. The network maintains continuous synchronicity and triggers no view-change.

(a)

(b)

Figure 4: Throughput vs. latency with network size 4,10 and batch size 800, on LAN and WAN

Figure 4 depicts the trend of latency with an increase in TPS under the setting of two network conditions and two
network sizes. Different transaction arrival rates were set to obtain different TPS when all replicas conducted correct
behavior, and the network was synchronous. As can be observed from Figs. 4(a) and 4(b), the setting 4 and 10 replicas
were denoted as “-4” and “-10.” Further, the LAN (4(a)) and WAN (4(b)) network condition results showed that
regardless of network conditions, the proposed protocol can break through performance bottlenecks and signiﬁcantly
improve TPS with approximately the same latency. As observed from both trends, the latency of the proposed protocol
was very close to that of HotStuff, and even the TPS increased up to approximately 160k on LAN. Owing to limited
network bandwidth and long-distance transmission, the maximum TPS was only 30K on WAN, while the latency was
greatly increased.

Under the same network environment, each committed block in multi-pipeline underwent the same certiﬁed process as
HotStuff; thus, they maintained the same latency level. When the network size was 4, the latency of Streamlet was
minimal and increased slowly with TPS. As the size scaled up to 10, the latency became larger than multi-pipeline.
The maximum throughput of multi-pipeline exceeded that of HotStuff and Streamlet by approximately 50% and 100%,
respectively. In addition, beneﬁting from the concurrent replicas and higher utilization of bandwidth, it can process

15

0k20k40k60k80k100k120k140k160k020406080100120140160180Latency(ms)Throughput(tx/s) MPH-4 MPH-10 Hotstuff-4 Hotstuff-10 Streamlet-4 Streamlet-105k10k15k20k25k30k2004006008001000120014001600Latency(ms)Throughput(tx/s) MPH-4 MPH-10 Hotstuff-4 Hotstuff-10 Streamlet-4 Streamlet-10A PREPRINT - JULY 8, 2022

more transactions in the same number of steps. However, the throughput of Streamlet is worse than both in this setting,
owing to its quadratic messages for each operation, including proposing and voting. The quadratic communication
complexity consumes more bandwidth and burdens the system performance as the system scales up.

7.2 Scalability

The scalability of three protocols with system scales of 4 to 58 was evaluated, where all the experiments were performed
in the same environment with no extra network delays. A 1024B-size payload, 512KB-size dissemination batch, and a
maximum 800-number transaction(digest) were used as the system conﬁguration. Further, the transaction arrival rate
was set to be maximum until the system saturated to record the maximum throughput and latency while varying the
number of replicas. In addition, to capture the error bars of each setting, the standard deviation was calculated for ten
runs with the same setting.

(a)

(b)

Figure 5: Scalability with batch size 800 on LAN(a) and WAN(b)

Figure 5 shows the throughput and latency over the number of replicas scale-up. As can be observed, the latency of
MPH is very close to that of HotStuff, and approximately 60% better throughput than HotStuff at each system scale on
LAN 5(a) and WAN 5(b). The throughput of Streamlet drops sharply from the same level as HotStuff at the beginning
owing to the quadratic communication cost. In general, when the system scales up to 58 in the LAN, the MPH provides
nearly 40k TPS while maintaining an average latency of 2 s. Further, the linear communication cost allows HotStuff
and MPH to maintain a slow latency growth, whereas, in Streamlet, it increases dramatically. Owing to the transmission
delay in the WAN, the latency of all protocols was higher than the LAN environment at each system scale and with a
slower increase than the LAN environment except Streamlet. However, the MPH still provided approximately 10k TPS
with a maximum latency of 3.5 s, which was better than HotStuff.

7.3 View-change

Three protocols with 22 replicas were executed with certain being set as faulty (0,1,2 or 3 crashed) at the beginning of
the experiment on LAN, denoted by “-1,-2,-3.” In addition, payload and dissemination sizes were set to 1024B and
512KB, with maximum 800 block size. Moreover, the timeout was set to 500 ms. Owing to the use of leader rotation in

16

4102234465820k40k60k80k100k120k140k160kThroughput(tx/a)Number of Replicas(n) MPH Hotstuff Streamlet41022344658012345Latency(s)Number of Replicas(n) MPH Hotstuff Streamlet410223446580k5k10k15k20k25k30kThroughput(tx/a)Number of Replicas(n) MPH Hotstuff Streamlet41022344658024681012(cid:3)(cid:4)(cid:9)(cid:6)(cid:7)(cid:5)(cid:10)(cid:1)(cid:8)(cid:2)Number of Replicas(n) MPH Hotstuff Streamletpipeline pattern to provide fairness, this conﬁguration forces the protocol to perform frequent view-change. Therefore,
the experiments exhibited the degree to which view-change slowed down the performance of consensus. As evident

A PREPRINT - JULY 8, 2022

Figure 6: Throughput vs. latency with network size 22 and batch size 800

from Figure 6, MPH outperformed HotStuff and Streamlet at the beginning, and with increase in the number of faulty
replicas, the frequent view-change rendered it closer to Hostuff. Because the feature of MPH inherently improves
the performance than HotStuff, the throughput loss of MPH for the same number of faulty replicas was lower than
other two. Starting from 30k improvement with no faulty replicas to 2k improvement with 3 faulty replicas, the MPH
performance gains compared to HotStuff and Streamlet gradually tapered off. The disappearance of multi-pipeline
optimization is primarily because of t, and, where more forked blocks than a single pipeline must be processed (at most
4 blocks will be forked in the proposed implementation, twice as many as HotStuff) and only three consecutive views
without malicious replicas can continue to effectively drive consensus after the timeout (assuming view v is the ﬁrst
view after the timeout, even a correct leader of view v + 2 can collect a quorum votes of view v, the new QC will never
be observed by other replicas once the leader of v + 1 is faulty; new view-change will be triggered by view v + 1). As
for latency, the maximum latency of MPH increased from 200 ms to approximately 2 s as more replicas became faulty.
When TPS reached its peak, MPH and HotStuff performance were at the same level; however, it was slightly higher
than that of Streamlet. The reason why MPH performed better than HotStuff is that more TPS signiﬁcantly reduced the
waiting time in the buffer. It is worse in the case of Streamlet because Streamlet has no view synchronization phase to
perform because it follows the longest branch extending rule regardless of the phase. Moreover, the timeout in Streamlet
was ﬁxed.

8 Related work

State Machine Replication abstracts the operation on multiple replicas into a whole. This abstraction relies on consensus
protocol at its core to order the requests from clients, such that they are committed in the same order. Reaching
consensus in the presence of Byzantine failures was abstracted as Byzantine Generals Problem by Lamport et al. [2],
who also introduced the term “Byzantine Faults,” a semantics of distributed failures. Several previous studies on
fault-tolerance did not involve Byzantine faults or only considered an ideal synchronous network (e.g., [18, 19]). To
solve crash fault-tolerance problems using SMR, Lamport proposed Paxos [20, 19], a leader-based protocol that drives
consensus decisions in the pipeline based on the designated leader, with linear communication complexity and one
round message exchange. However, Paxos is difﬁcult to understand. Consequently, a simpler Raft [21] was devised by
Diego and John, which decomposed the consensus problem into three independent components and provided a better
foundation for system building.

Tolerating Byzantine faults requires a much more complex protocol, which suffers from expensive message complexity
or time-consuming authenticator steps. The ﬁrst theoretical synchronization solution in a fault-tolerant system was
provided by Pease et al. [22], wherein an exponential number of messages was sent to reach an agreement. Thereafter,

17

5k6k7k8k9k10k15k20k30k40k50k80k0.00.51.01.52.0Latency(s)Throughput(tx/s) MPH MPH-1 MPH-2 MPH-3 Hotstuff Hotstuff-1 Hotstuff-2 Hotstuff-3 Streamlet Streamlet-1 Streamlet-2 Streamlet-3A PREPRINT - JULY 8, 2022

the ﬁrst practical polynomial algorithm was proposed by Dolev, and Strong [23, 24], with the tolerance of f < n. The
communication complexity was O(n3) in any case, with or without authentication. To further improve efﬁciency, the
randomized method was introduced [25, 26, 27, 28, 29]. Kztz and Koo [28] proposed an expected constant-round and
f < n/2 resilience protocol using key primitive referred to as moderated Veriﬁable Secret Sharing (VSS) and later
improved by Abraham and et al. [29]. The improved protocol has expected O(1) round complexity and an expected
O(n2) communication complexity. Recent studies of Abraham et al., Sync HotStuff [30] have shown a simple and
practical BFT solution that achieves consensus with a latency of 2∆ in the normal case(where ∆ is the upper bound
of message delay) and quadratic communication. Another study [31] allowed optimistic responsive leader rotation in
addition to shrinking Sync HotStuff’s latency upper and lower bounds.

The partial synchronous model was ﬁrst proposed by Dwork, Lynch, and Stockmeyer(DLS) [3], who also ﬁrst proposed
a protocol that preserved safety before GST and guaranteed liveness following GST costing O(n4) communication and
O(n) round per decision. However, expensive costs resulted in the protocols remaining theoretical until PBFT was
devised by Castro and Liskov [4], which is an efﬁcient protocol. PBFT implements BFT SMR using reliable broadcast
with O(n2) communication complexity and two round-trips per decision, in addition to O(n3) messages in view-change
case. Several studies have been conducted to reduce the communication cost, with quadratic cost in the normal and
view-change cases being referred to as Tendermint [32] having two round commit latency. Kotla et al., introduced
Zyzzyva [33], which considered that an optimistic path has a linear cost, while the view-change path remained O(n3).
Thereafter, a safety violation was found by Abraham et al., who also presented revised protocols [34]. In the best case,
the over-quadratic cost was the bottleneck that rendered the deployment of the system on a large scale challenging.
SBFT [10] was shown to address the challenge of scalability based on key ingredients: using threshold signatures to
reduce communication to linear and an optimistic path to reduce latency. Consequently, the state-of-the-art protocol
was HotStuff [14], which bridges the world of BFT consensus and blockchain, and has O(n) cost and optimistic
responsiveness. These features, along with the threshold signature scheme and pipeline operation, have provided a new
pattern for designing the modern protocol. From this, a variation of HotStuff, such as Fast-HotStuff [35], provided less
latency through the reduction of 3-chain structures to 2-chain one. Further, the DiemBFT [36] from Meta further reﬁned
HotStuff as a core of the blockchain system. It retained linear costs and introduced an explicit liveness mechanism,
which is its leader reputation mechanism that provides leader utilization under crash, thereby preventing unnecessary
delays. Inspired by past studies, Chan et al. described an extremely simple and natural paradigm with quadratic cost
referred to as Streamlet, [37], which ﬁnalized a record when it was extended by another two certiﬁed records.

Moreover, FLP [38] theory stated that reaching deterministic consensus is impossible in the face of even a single
failure. Fortunately, the impossibility was circumvented by harnessing randomness; several studies have focused on
asynchronous settings, including the ﬁrst practical asynchronous protocol given by Cachin et al. [13], HoneyBadgerBFT
[8], VABA [39], Dumbo-BFT [40], and DAG-Rider [41]. Most of these protocols have over-quadratic costs; however,
they are always alive even in asynchrony.

9 Conclusion and Future Work

This study proposed MPH, the ﬁrst protocol that improved performance without duplication problem caused by parallel
proposing. It achieved consensus by through the combination of two pipelines in HotStuff into one instance to overcome
the performance bottleneck when there are no faults. In combination with optimistically proposing, this allowed each
leader to produce a block within at most ∆ time after GST, further reducing the waiting time for block generation.
Consequently, MPH achieved unprecedented throughput and outperformed state-of-the-art protocols. Based on the
3-chain predicate in safety checking, the end-to-end latency does not signiﬁcantly increase. The evaluation conducted
showed that MPH outperformed other protocols with a proposing-voting pattern in terms of throughput and latency,
regardless of the network conditions.

An interesting future work is the independent processing of two HotStuff instances when time outs, which can further
improve the throughput and latency performance of the system when there are exits faults. Further, more HotStuff
instances may be combined with the multi-pipeline pattern for further optimization.

References

[1] Fred B. Schneider. Implementing fault-tolerant services using the state machine approach: A tutorial. ACM

Comput. Surv., 22(4):299–319, dec 1990.

[2] Leslie Lamport, Robert Shostak, and Marshall Pease. The Byzantine Generals Problem, page 203–226. Association

for Computing Machinery, New York, NY, USA, 2019.

18

A PREPRINT - JULY 8, 2022

[3] Cynthia Dwork, Nancy Lynch, and Larry Stockmeyer. Consensus in the presence of partial synchrony. J. ACM,

35(2):288–323, apr 1988.

[4] Miguel Castro and Barbara Liskov. Practical byzantine fault tolerance. In Proceedings of the Third Symposium on
Operating Systems Design and Implementation, OSDI ’99, page 173–186, USA, 1999. USENIX Association.

[5] Rafael Pass and Elaine Shi. Thunderella: Blockchains with optimistic instant conﬁrmation. In Jesper Buus Nielsen
and Vincent Rijmen, editors, Advances in Cryptology – EUROCRYPT 2018, pages 3–33, Cham, 2018. Springer
International Publishing.

[6] Victor Shoup. Practical threshold signatures. In Bart Preneel, editor, Advances in Cryptology — EUROCRYPT

2000, pages 207–220, Berlin, Heidelberg, 2000. Springer Berlin Heidelberg.

[7] Chrysoula Stathakopoulou, Tudor David, and Marko Vukolic. Mir-bft: High-throughput bft for blockchains. arXiv

preprint arXiv:1906.05552, 2019.

[8] Andrew Miller, Yu Xia, Kyle Croman, Elaine Shi, and Dawn Song. The honey badger of bft protocols. In
Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security, CCS ’16, page
31–42, New York, NY, USA, 2016. Association for Computing Machinery.

[9] Juan Garay, Aggelos Kiayias, and Nikos Leonardos. The bitcoin backbone protocol: Analysis and applications.
In Elisabeth Oswald and Marc Fischlin, editors, Advances in Cryptology - EUROCRYPT 2015, pages 281–310,
Berlin, Heidelberg, 2015. Springer Berlin Heidelberg.

[10] Guy Golan Gueta, Ittai Abraham, Shelly Grossman, Dahlia Malkhi, Benny Pinkas, Michael Reiter, Dragos-Adrian
Seredinschi, Orr Tamir, and Alin Tomescu. Sbft: A scalable and decentralized trust infrastructure. In 2019 49th
Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN), pages 568–580, 2019.

[11] Dan Boneh, Ben Lynn, and Hovav Shacham. Short signatures from the weil pairing.

In Colin Boyd, edi-
tor, Advances in Cryptology — ASIACRYPT 2001, pages 514–532, Berlin, Heidelberg, 2001. Springer Berlin
Heidelberg.

[12] Phillip Rogaway and Thomas Shrimpton. Cryptographic hash-function basics: Deﬁnitions, implications, and
separations for preimage resistance, second-preimage resistance, and collision resistance. In Bimal Roy and Willi
Meier, editors, Fast Software Encryption, pages 371–388, Berlin, Heidelberg, 2004. Springer Berlin Heidelberg.

[13] Christian Cachin, Klaus Kursawe, Frank Petzold, and Victor Shoup. Secure and efﬁcient asynchronous broadcast
protocols. In Joe Kilian, editor, Advances in Cryptology — CRYPTO 2001, pages 524–541, Berlin, Heidelberg,
2001. Springer Berlin Heidelberg.

[14] Maofan Yin, Dahlia Malkhi, Michael K. Reiter, Guy Golan Gueta, and Ittai Abraham. Hotstuff: Bft consensus
with linearity and responsiveness. In Proceedings of the 2019 ACM Symposium on Principles of Distributed
Computing, PODC ’19, page 347–356, New York, NY, USA, 2019. Association for Computing Machinery.

[15] Fangyu Gai, Ali Farahbakhsh, Jianyu Niu, Chen Feng, Ivan Beschastnikh, and Hao Duan. Dissecting the
performance of chained-bft. In 2021 IEEE 41st International Conference on Distributed Computing Systems
(ICDCS), pages 595–606, 2021.

[16] Ailidani Ailijiang, Aleksey Charapko, and Murat Demirbas. Dissecting the performance of strongly-consistent
replication protocols. In Proceedings of the 2019 International Conference on Management of Data, SIGMOD
’19, page 1696–1710, New York, NY, USA, 2019. Association for Computing Machinery.

[17] George Danezis, Lefteris Kokoris-Kogias, Alberto Sonnino, and Alexander Spiegelman. Narwhal and tusk: A
dag-based mempool and efﬁcient bft consensus. In Proceedings of the Seventeenth European Conference on
Computer Systems, EuroSys ’22, page 34–50, New York, NY, USA, 2022. Association for Computing Machinery.

[18] Leslie Lamport. Time, Clocks, and the Ordering of Events in a Distributed System, page 179–196. Association for

Computing Machinery, New York, NY, USA, 2019.

[19] Leslie Lamport. The Part-Time Parliament, page 277–317. Association for Computing Machinery, New York, NY,

USA, 2019.

[20] Leslie Lamport. Paxos made simple. ACM SIGACT News (Distributed Computing Column) 32, 4 (Whole Number

121, December 2001), pages 51–58, December 2001.

[21] Diego Ongaro and John Ousterhout. In search of an understandable consensus algorithm. In 2014 USENIX Annual

Technical Conference (USENIX ATC 14), pages 305–319, Philadelphia, PA, June 2014. USENIX Association.

[22] M. Pease, R. Shostak, and L. Lamport. Reaching agreement in the presence of faults. J. ACM, 27(2):228–234, apr

1980.

19

A PREPRINT - JULY 8, 2022

[23] Danny Dolev and H. Raymond Strong. Polynomial algorithms for multiple processor agreement. In Proceedings
of the Fourteenth Annual ACM Symposium on Theory of Computing, STOC ’82, page 401–407, New York, NY,
USA, 1982. Association for Computing Machinery.

[24] Danny Dolev and Rüdiger Reischuk. Bounds on information exchange for byzantine agreement. J. ACM,

32(1):191–204, jan 1985.

[25] Michael O. Rabin. Randomized byzantine generals. In 24th Annual Symposium on Foundations of Computer

Science (sfcs 1983), pages 403–409, 1983.

[26] Pesech Feldman and Silvio Micali. An optimal probabilistic protocol for synchronous byzantine agreement. SIAM

Journal on Computing, 26(4):873–933, 1997.

[27] Matthias Fitzi and Juan A. Garay. Efﬁcient player-optimal protocols for strong and differential consensus. In
Proceedings of the Twenty-Second Annual Symposium on Principles of Distributed Computing, PODC ’03, page
211–220, New York, NY, USA, 2003. Association for Computing Machinery.

[28] Jonathan Katz and Chiu-Yuen Koo. On expected constant-round protocols for byzantine agreement. In Cynthia
Dwork, editor, Advances in Cryptology - CRYPTO 2006, pages 445–462, Berlin, Heidelberg, 2006. Springer
Berlin Heidelberg.

[29] Ittai Abraham, Srinivas Devadas, Danny Dolev, Kartik Nayak, and Ling Ren. Synchronous byzantine agreement
with expected o(1) rounds, expected o(n2) communication, and optimal resilience. In Ian Goldberg and Tyler
Moore, editors, Financial Cryptography and Data Security, pages 320–334, Cham, 2019. Springer International
Publishing.

[30] Ittai Abraham, Dahlia Malkhi, Kartik Nayak, Ling Ren, and Maofan Yin. Sync hotstuff: Simple and practical
synchronous state machine replication. In 2020 IEEE Symposium on Security and Privacy (SP), pages 106–118,
2020.

[31] Ittai Abraham, Kartik Nayak, and Nibesh Shrestha. Optimal good-case latency for rotating leader synchronous bft.

Cryptology ePrint Archive, Report 2021/1138, 2021. https://ia.cr/2021/1138.

[32] Jae Kwon. Tendermint: Consensus without mining. Draft v. 0.6, fall, 1(11), 2014.
[33] Ramakrishna Kotla, Lorenzo Alvisi, Mike Dahlin, Allen Clement, and Edmund Wong. Zyzzyva: Speculative

byzantine fault tolerance. SIGOPS Oper. Syst. Rev., 41(6):45–58, oct 2007.

[34] Ittai Abraham, Guy Gueta, Dahlia Malkhi, Lorenzo Alvisi, Rama Kotla, and Jean-Philippe Martin. Revisiting fast

practical byzantine fault tolerance. arXiv preprint arXiv:1712.01367, 2017.

[35] Mohammad M Jalalzai, Jianyu Niu, Chen Feng, and Fangyu Gai. Fast-hotstuff: A fast and resilient hotstuff

protocol. arXiv preprint arXiv:2010.11454, 2020.

[36] Mathieu Baudet, Avery Ching, Andrey Chursin, George Danezis, François Garillot, Zekun Li, Dahlia Malkhi,
Oded Naor, Dmitri Perelman, and Alberto Sonnino. State machine replication in the libra blockchain. The Libra
Assn., Tech. Rep, 2019.

[37] Benjamin Y. Chan and Elaine Shi. Streamlet: Textbook streamlined blockchains. In Proceedings of the 2nd ACM
Conference on Advances in Financial Technologies, AFT ’20, page 1–11, New York, NY, USA, 2020. Association
for Computing Machinery.

[38] Michael J. Fischer, Nancy A. Lynch, and Michael S. Paterson. Impossibility of distributed consensus with one

faulty process. J. ACM, 32(2):374–382, apr 1985.

[39] Ittai Abraham, Dahlia Malkhi, and Alexander Spiegelman. Asymptotically optimal validated asynchronous
byzantine agreement. In Proceedings of the 2019 ACM Symposium on Principles of Distributed Computing,
PODC ’19, page 337–346, New York, NY, USA, 2019. Association for Computing Machinery.

[40] Bingyong Guo, Zhenliang Lu, Qiang Tang, Jing Xu, and Zhenfeng Zhang. Dumbo: Faster Asynchronous BFT

Protocols, page 803–818. Association for Computing Machinery, New York, NY, USA, 2020.

[41] Idit Keidar, Eleftherios Kokoris-Kogias, Oded Naor, and Alexander Spiegelman. All you need is dag.

In
Proceedings of the 2021 ACM Symposium on Principles of Distributed Computing, PODC’21, page 165–175,
New York, NY, USA, 2021. Association for Computing Machinery.

20

