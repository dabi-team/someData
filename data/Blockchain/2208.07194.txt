IEEE TRANSACTIONS ON VEHICULAR TECHNOLOGY

1

An Efﬁcient and Reliable Asynchronous Federated
Learning Scheme for Smart Public Transportation

Chenhao Xu, Youyang Qu, Member, IEEE, Tom H. Luan, Senior Member, IEEE, Peter W. Eklund,
Yong Xiang, Senior Member, IEEE, and Longxiang Gao, Senior Member, IEEE

2
2
0
2

g
u
A
0
3

]

G
L
.
s
c
[

2
v
4
9
1
7
0
.
8
0
2
2
:
v
i
X
r
a

Abstract—Since the trafﬁc conditions change over time, ma-
chine learning models that predict trafﬁc ﬂows must be updated
continuously and efﬁciently in smart public transportation. Fed-
erated learning (FL) is a distributed machine learning scheme
that allows buses to receive model updates without waiting for
model training on the cloud. However, FL is vulnerable to poi-
soning or DDoS attacks since buses travel in public. Some work
introduces blockchain to improve reliability, but the additional
latency from the consensus process reduces the efﬁciency of
FL. Asynchronous Federated Learning (AFL) is a scheme that
reduces the latency of aggregation to improve efﬁciency, but the
learning performance is unstable due to unreasonably weighted
local models. To address the above challenges, this paper offers
a blockchain-based asynchronous federated learning scheme
with a dynamic scaling factor (DBAFL). Speciﬁcally, the novel
committee-based consensus algorithm for blockchain improves
reliability at the lowest possible cost of time. Meanwhile, the
devised dynamic scaling factor allows AFL to assign reasonable
weights to stale local models. Extensive experiments conducted
on heterogeneous devices validate outperformed learning perfor-
mance, efﬁciency, and reliability of DBAFL.

Index Terms—Asynchronous Federated Learning, Blockchain,

Dynamic Scaling Factor, IoV.

I. INTRODUCTION

M ACHINE learning (ML) is a popular approach on

the Internet of Vehicles (IoV) to enable smart public
transportation [1], [2]. For example, buses with ML models are
able to forecast trafﬁc ﬂows and the time passengers wait at
stops, assisting drivers in improving driving safety and fuel
economy. However, because trafﬁc conditions change over
time, ML models must be updated continuously and efﬁciently.
Federated Learning (FL) is a distributed ML scheme that
allows models to be trained locally and updated frequently.
For instance, buses collect trafﬁc data and train local models,
while roadside units (RSUs) periodically aggregate the local
models to produce an accurate global model and send it back
to the buses [3].

However, FL raises efﬁciency and reliability concerns due to
the limited computing resources and continuous movement of

Longxiang Gao is the corresponding author.
Chenhao Xu and Yong Xiang are with the Deakin Blockchain Innovation
Lab, School of Information Technology, Deakin University, Geelong, Aus-
tralia. E-mail: {xueri and yong.xiang}@deakin.edu.au.

Youyang Qu is with Data 61 Australia Commonwealth Scientiﬁc and Indus-
trial Research Organization, Australia. E-mail: youyang.qu@data61.csiro.au.
Tom H. Luan is with School of Cyber Engineering, Xidian University,

Shaanxi, China. E-mail: tom.luan@xidian.edu.cn.

Peter W. Eklund is with School of Information Technology, Deakin Uni-

versity, Geelong, Australia. E-mail: peter.eklund@deakin.edu.au.

Longxiang Gao is with Qilu University of Technology and Shandong

Computer Science Center, China. E-mail: gaolx@sdas.org.

buses in smart public transportation. Firstly, the synchronous
aggregation strategy forces the aggregation server to collect
enough local models before aggregation, which is inefﬁcient
due to the difference of computing power and dataset sizes
among buses and RSUs [4]. Speciﬁcally, high-performance
nodes have to wait for lagging nodes to ﬁnish their training
before aggregation. Secondly, unreliable local models gathered
from buses pose FL at risk of poisoning attacks [5]. The
centralized aggregation server of FL is subject
to DDoS
attacks [1]. Both of these attacks reduce the reliability of FL.
To improve the efﬁciency of FL, asynchronous federated
learning (AFL) is proposed, which reduces the latency by per-
forming aggregation whenever a local model is received [6]–
[10]. However, due to the high aggregation frequency, there are
outdated global models in AFL, from which stale local models
trained usually have relatively low accuracy [11]. Existing
work either discards or assigns irrational weights to stale
local models, leading to unstable FL learning performance (i.e.
convergence speed and global model accuracy) [4].

Some work adopts blockchain to improve the reliability
of FL [12]. Due to its decentralized storage, attack-proof
consensus algorithm, and self-verifying smart contracts [13],
[14], blockchain allows FL to conduct a decentralized and
transparent training process, resulting in improved security and
trustability. However, consensus algorithms of the blockchain
are either compute-intensive (i.g. PoW) or communication-
intensive (i.g. PBFT) [4]. To improve efﬁciency, committee-
based consensus algorithms such as DPoS are proposed [15],
but their token or reputation systems are unsuitable for buses
that pass quickly. Although several blockchain-based AFL
schemes are proposed [16]–[18], their consensus processes are
still time-consuming.

In order to address the above challenges and better apply
FL into smart public transportation systems, this paper offers a
blockchain-based asynchronous federated learning framework
with a dynamic scaling factor (DBAFL). A novel committee-
based consensus algorithm is introduced to improve reliability
while bringing the least amount of communication burden.
Speciﬁcally, the committee leader, as the aggregation server,
identiﬁes low-accuracy local models based on its local dataset
to resist poisoning attacks. Without the need for communica-
tion and voting, a new committee leader is elected from RSUs
periodically based on the hash of the latest block to reduce
the risk of being subjected to DDoS attacks. Besides, when
performing aggregation, a dynamic scaling factor is designed
to assign appropriate weights to local models according to
their accuracy and correspondingly improves the learning

 
 
 
 
 
 
IEEE TRANSACTIONS ON VEHICULAR TECHNOLOGY

2

performance of FL. Experiments conducted on heterogeneous
devices evaluate the proposed scheme and demonstrate its
outstanding learning performance, efﬁciency, and reliability.
The main contributions of this paper are as follows.

• A blockchain-based asynchronous federated learning
scheme is designed for smart public transportation, con-
sidering learning performance, efﬁciency, and reliability
in heterogeneous computing environments.

• A dynamic scaling factor is designed to assign appropri-
ate weights to stale local models with the joint effort of
a committee-based consensus algorithm, allowing FL to
efﬁciently converge to higher accuracy while being highly
attack-resistant.

• An open-source prototype1 is implemented with compre-
hensive experiments conducted to validate the advantages
from three perspectives compared with state-of-the-art
schemes.

The remainder of this paper is organized as follows: Sec-
tion II presents related work. Section III models the proposed
scheme in detail. Section IV analyzes the proposed scheme
from several aspects. Section V evaluates the proposed scheme
experimentally. Finally, Section VI summarizes the paper and
outlines future work.

II. RELATED WORK

The related work of this paper includes the blockchain,
federated learning, IoV, and asynchronous federated learning.

A. Blockchain, Federated Learning, and IoV

Some existing work integrates the blockchain and FL to
resist attack in classic FL [1], [13], [16]–[26]. The blockchain
is a motivation for the nodes to participate in FL and contribute
high-quality local models [19], [20], [27], a distributed repu-
tation management system to resist repudiation and tamper-
ing [26], and an auditable distributed database that allows FL
to conduct a transparent training process [21]. However, none
of these schemes simultaneously take efﬁciency and reliability
into account. Shayan et al. [22] prove that the blockchain
effectively defends FL against poisoning attacks. Li et al. [23]
design a committee consensus algorithm for blockchain-based
FL without analyzing the communication burden brought by
the blockchain. Besides, the score-based committee election
in their scheme is unsuitable for fast-traveling buses. Kang et
al. [28] propose a hierarchical blockchain-based FL scheme
with improved efﬁciency and privacy, but miner election and
model quality cross-validation in the consensus process are
time-consuming and inappropriate for buses.

There are some work adopts FL in IoV. For example, Lim et
al. [29] propose a blockchain-based IoV network that matches
the lowest cost Unmanned Aerial Vehicles (UAVs) to each
subregion, but communication latency is not analyzed and
tested. Besides, some work utilizes the blockchain to ensure
a secure FL framework on IoV networks, including data shar-
ing [24], driving assistance [30], and intrusion detection [31].
However, these schemes are inefﬁcient on IoV networks due

1The Github link is https://github.com/xuchenhao001/AFL.

to the synchronous aggregation strategy. Pokhrel et al. [25]
optimize the latency by adjusting the block arrival rate, but
the PoW consensus in their scheme is still time-consuming.
Lu et al. [1] introduce the directed acyclic graph architecture
to the blockchain to improve efﬁciency, but security and
communication latency are not analyzed.

DBAFL introduces a novel committee-based consensus al-
gorithm to the blockchain, which brings the least communica-
tion latency to buses while ensuring reliability.

B. Asynchronous Federated Learning

FL, proposed in 2017 [3], is a distributed learning scheme
applied in various scenarios [32]–[35]. To reduce aggregation
latency and improve efﬁciency on resource-limited networks,
AFL is proposed [6]–[10], [16].

A semi-asynchronous FL scheme is proposed for mitigating
model staleness [8]. However, lagging models in their scheme
are given the same weight as normal models during aggre-
gation. Chen et al. [6] assign a higher weight to stale local
models due to large training datasets. But stale local models
in IoV networks may be due to inadequate computing power.
Chen et al. [7] improve efﬁciency by reducing the updating
frequency of parameters in deep layers, which is hard to apply
to other types of models. Liu et al. [16] propose an AFL
framework with a staleness coefﬁcient to adjust the weight
of stale local models, but analysis and validation are missing.
Lu et al. [9] present a new gradient compression approach to
improve efﬁciency at the cost of global model accuracy. Deng
et al. [10] propose a semi-AFL approach APFL that mixes the
parameters of local and global models to lower communication
frequency, but an additional training phase is required. Chen et
al. [30] present a blockchain-based AFL scheme BDFL with
all models saved in the blockchain and aggregated with the
same weight. As highly relevant work, APFL and BDFL are
included as benchmarks in Section V.

A dynamic scaling factor

is designed in DBAFL for
weighted aggregation according to model accuracy, which
improves learning performance and reliability.

III. DBAFL MODELING

This section explains the model of DBAFL in detail from
the aspect of the system model, workﬂow, dynamic scaling
factor, and committee-based consensus algorithm.

A. System Model

The public transportation system includes buses and RSUs
on a road, as shown in Fig. 1. The bus stop equipped with
sensors and an edge computing server is considered a kind of
RSU in this scenario. RSUs usually have higher computing
and communication power than buses. Buses establish short-
term Vehicle-to-RSU (V2R) and Vehicle-to-Vehicle (V2V)
connections with nearby RSUs or buses when they are within
a signal region, allowing them to transmit a certain amount of
data. Besides, RSUs have a high-speed Ethernet connection
with each other to support smart public transportation.

Buses and RSUs train ML models collaboratively to pre-
trafﬁc ﬂows and the time passengers wait at stops,

dict

IEEE TRANSACTIONS ON VEHICULAR TECHNOLOGY

3

maintains a distributed database with data synchronization to
share models. The committee leader is in charge of performing
aggregation whenever a new local model is received.

B. Workﬂow

Fig. 1. The architecture of DBAFL in public transportation.

allowing buses to travel safely and efﬁciently. Since trafﬁc
conditions change over time, buses and RSUs continuously
collect training dataset through their sensors to update models.
In an area, buses and RSUs usually observe similar trafﬁc
conditions and collect independent and identically distributed
(IID) data. However, the training time and model quality of
buses and RSUs differ due to differences in computing power
and dataset sizes, posing learning performance challenges to
FL. The limited connect time and bandwidth of V2R and V2V
connections pose efﬁciency challenges to FL. The poisoning
and DDoS attacks launched by attackers on the roadside pose
reliability challenges to FL.

In DBAFL, a dynamic scaling factor and a lightweight
consensus algorithm are designed on top of AFL and the
blockchain to improve learning performance, efﬁciency, and
reliability. Speciﬁcally, buses and RSUs act as local nodes
of FL and train local models based on their local data.
local nodes upload the hash value of the
After training,
to the blockchain for other nodes to verify.
local model
Apart from that, buses upload the local model
to nearby
RSUs for global model aggregation. During the aggregation,
the dynamic scaling factor assigns weights to local models
according to their accuracy. RSUs are committee members
and are eligible to be elected as the committee leader by the
consensus algorithm. The identity of the next committee leader
is determined by the hash of the most recent block, which
does not require voting or communication. The committee

Fig. 2. The workﬂow of DBAFL.

The workﬂow of DBAFL is illustrated in Fig. 2. In a smart
public transportation system, assume three nodes (Node A, B,
and C) are involved in the training process. Speciﬁcally, Node
A is an RSU, while Node B and Node C are buses. With
the passage of time (from t0 to t3), the nodes keep training
local models with the latest global model. The details are as
follows:

G) and Hash(w0

At t0, Node A trains a local model w0

L as the initial global
model w0
G. Then Node A uploads the hash values of them,
i.e. Hash(w0
L), to the blockchain to build up a
genesis block Block0 (The ﬁrst block in the blockchain). After
receiving the hash value of Block0, each node downloads w0
G
from Node A, and trains local models based on its local data.
At the same time, a committee leader is elected based on the
hash value of Block0.

At t1, Node B ﬁnishes its training and generates a new
local model w1
L. Next, Node B calculates the hash value
Hash(w1
L) and uploads it to the blockchain. Besides, Node
B uploads w1
L to the nearby RSU, where it will be stored
in the distributed database and shared with the committee
leader. Without waiting for other nodes, the committee leader
aggregates w0
L according to a dynamic scaling factor,
which is explained in Section III-C, to produce a new global
model w1
G) to
the blockchain, where a new block Block1 is generated. The
global model w1
G is then shared with all committee members.

G. The committee leader then uploads Hash(w1

G and w1

IEEE TRANSACTIONS ON VEHICULAR TECHNOLOGY

4

G and w2

L. Node A uploads Hash(w2

At t2, Node A ﬁnishes training based on w0

G and acquires
a stale local model w2
L) to the
blockchain, along with w2
L shared with the committee leader
through the distributed database. Based on w1
L, the
committee leader generates a new global model w2
G, along
with the hash value Hash(w2
G) uploaded to the blockchain.
After that, Block2 is created and appended to the blockchain.
After a long period of training, Node C ﬁnally ﬁnishes
L based on w0
training its local model w3
G. Although it is stale,
the local model w3
L uploaded by Node C is also acceptable
for the committee leader. Node C uploads Hash(w3
L) to the
L and w2
blockchain at the same time. After aggregating w3
G,
the committee leader generates a new global model w3
G,
and uploads Hash(w3
G) to the blockchain. Then, Block3 is
generated.

With the help of blockchain, the training process is consis-
tent, transparent, and trustable. Besides, the novel committee-
based consensus algorithm in the blockchain enables an attack-
resistant DBAFL with a generalized ML model. The advan-
tages of DBAFL are explained carefully in Section IV-B.

C. Dynamic Scaling Factor

In [6], [7], [16], the authors demonstrate that the weight
of local models during aggregation has a signiﬁcant impact
on federated learning performance. If a stale local model has
low accuracy due to the limited computing resources of the
node, relying too much on this particular local model results
in a deterioration of global model accuracy. On the contrary,
if the stale local model has relatively high accuracy due to
large volumes of data on the node, it is preferential to make
better use of it to improve the convergence speed of the global
model.

A dynamic scaling factor, denoted as (cid:15), is designed to assign
the newly arrived local model appropriate weight. To achieve
a global model with high accuracy, the committee leader tests
the accuracy of the models based on its local data before
aggregation in DBAFL. Assuming At
G denote the
test accuracy of the local model at time t and that of the global
model at time t − 1, respectively, (cid:15)t is deﬁned as

L and At−1

(cid:15)t =

At
L
At−1
G

.

(1)

Considering the dynamic scaling factor (cid:15)t, the new global
model at time t is calculated by

wt

G =

wt−1

G + (cid:15)t × wt
L
1 + (cid:15)t

.

(2)

From Eq. 2, it is straightforward that a greater value of (cid:15) means
a higher weight of the local model. If the local model has
higher accuracy than the global model, the committee leader
increases (cid:15) to assign the local model with higher weight, and
vice versa.

Since the committee leader is re-elected on a regular basis,
as explained in Section III-D, assessing the model accuracy
using local data by the committee leader is neutral and efﬁcient
while preserving data privacy. For example, as shown in Fig. 3,
after receiving the global model w0
G from RSU 1 and the local

G and w1

model w1
L from RSU 2, the committee leader, RSU 3, tests the
accuracy of w0
L based on its local data. Due to the
disparity in data samples across nodes, the model accuracy
assessing on RSU 3 is fairer than on RSU 1 or RSU 2.
Besides, to strengthen the generality of the global model, a
higher committee leader election frequency could be used.

Algorithm 1 Dynamic Scaling Factor

(cid:46) On the ﬁrst RSU

L) and Hash(w0
G to the distributed database

G) to the blockchain

(cid:46) On local nodes

for each local epoch in E do

G from the nearby RSU

L ← LocalTrain(wt−1

G , localTrainData)

L) to the blockchain

L to the nearby RSU

L as w0
G

initialize w0
upload Hash(w0
L and w0
save w0

1: function INITIALIZATION
2:
3:
4:
5: end function
6:
7: function CLIENTUPDATE
8:
9:
10:
11:

download wt−1
wt
upload Hash(wt
upload wt

end for

12:
13:
14: end function
15:
16: function AGGREGATION
17:
18:
19:
20:
21:
22:

wait wt
At
At−1
(cid:15)t ← At
G = (wt−1
wt
upload Hash(wt
save wt

L ← LocalTest(wt
G ← LocalTest(wt−1
L/At−1
G + (cid:15)t × wt

G

23:
24: end function

(cid:46) On the committee leader

L in the distributed database

L, localTestData)

G , localTestData)

L)/(1 + (cid:15)t)

G) to the blockchain
G to the distributed database

Algorithm 1 shows the implementation details of the dy-
namic scaling factor in DBAFL. There are three functions in
DBAFL: initialization (Lines 1 to 5), client update (Lines 7
to 14), and aggregation (Lines 16 to 24). The initialization
process runs on the ﬁrst RSU of the smart public transportation
system, which trains a local model w0
L as the ﬁrst global model
w0
G, as shown in Line 2. After uploading hash values of the
models to the blockchain, the original models are stored in the
distributed database on RSUs, as shown in Lines 3 and 4.

Before training its new local model wt

L, each local node
downloads the latest global model wt−1
G from the nearby RSU,
as shown in Lines 9 and 10. After training, each local node
uploads the hash value of the local model Hash(wt
L) to the
blockchain and uploads the original local model wt
L to the
nearby RSU, as shown in Lines 11 and 12. The loop continues
if the local epoch E is not reached, as shown in Lines 8.
Note that the time t in the client update function (running on
local nodes) does not one-to-one correspond to the one in the
aggregation function (running on the committee leader), due
to the asynchronous aggregation strategy. wt−1
G refers to the
latest global model downloaded from the nearby RSU.

From the view of the committee leader, the aggregation
progress starts when a new local model wt
L is uploaded to
the distributed database, as shown in Line 17. After testing
based on the local data, the accuracies of wt
G are

L and wt−1

IEEE TRANSACTIONS ON VEHICULAR TECHNOLOGY

5

obtained, as shown in Lines 18 and 19. Then, the dynamic
scaling factor (cid:15)t is calculated according to Eq. 1, as shown in
Line 20. Following that, a new global model wt
G is calculated
according to Eq. 2, as shown in Line 21. Finally, the committee
leader uploads the hash value of the global model Hash(wt
G)
to the blockchain and save the original global model wt
G to
the distributed database, as shown in Lines 22 and 23.

D. Committee-Based Consensus Algorithm

In DBAFL, a new committee leader is elected after sev-
eral new blocks are generated. Speciﬁcally, the frequency of
electing new committee leaders increases as the reliability
requirements grow. The identity of the new committee leader
is calculated by the hash value of the latest block.

As shown in Fig. 3, assuming the genesis block Block0
is generated on RSU 1,
the consensus algorithm in the
blockchain ensures the replicated blocks on the other nodes
are identical as the original Block0 on RSU 1. Based on the
hash value of Block0, i.e. Hash(Block0), all nodes acquire the
identity of the new committee leader by

ID1

L ≡ Hash(Block0) (mod M ),

(3)

where M is the total number of RSUs in DBAFL. Assuming
ID1
L = 3, RSU 3 is elected as the new committee leader at
Block1 and is placed in charge of aggregating and generating
subsequent global models. The election frequency of the com-
mittee leader is the reciprocal of the number of blocks to wait
before electing a new committee leader. Assume the frequency
of the committee leader election is 1/10, which means that the
identity of the new committee leader is calculated in the same
way as soon as 10 blocks are appended to the blockchain. Take
ID11
L = 2 as an example, RSU 2 is responsible for generating
global models in Block11 to Block20.

Let

the hash value of

time t is
Hash(Blockt−1) and the identity of RSU m is IDt
m. The
probability that RSU m becomes the leader at t is denoted
as PL(IDt

m) and is calculated by

the latest block at

PL(IDt

m) =

P [Hash(Blockt−1) (mod M ) ≡ IDm]
j=1 P [Hash(Blockt−1) (mod M ) ≡ IDj]

.

(cid:80)M

(4)
The duration of a speciﬁc RSU being a leader is limited to
prevent it from creating forks on the blockchain and doing
evil. Therefore, allowing all RSUs to have the same chance of
becoming the leader is the best way to safeguard the system.
Gini coefﬁcient, a statistical measure of wealth inequality,
is adopted to evaluate inequality in the probability of RSUs
becoming a leader in DBAFL, which is calculated by

G(t) =

(cid:80)M

m=1

(cid:80)M

j=1 |PL(IDt
(cid:80)M

m) − PL(IDt
j=1 PL(IDt
j)

m=1

j)|

2 (cid:80)M

.

(5)

It is empirically proved that when adopting SHA-256, a widely
used hash function, the non-randomness percentage of the hash
output is 31.25% [36]. Accordingly, it is trivial to know that
G(t) is less than 0.3125, which is close to 0 and demonstrates
that the probability of any RSUs becoming a leader in DBAFL
is sufﬁciently even. With more hash functions are developed,

adopting a more random hash function increases the equality
level
in the probability of RSUs becoming a leader and
subsequently improves the security level of DBAFL.

The original models are stored in the distributed database on
RSUs for buses to download. The hash values of the models
are stored in the blocks and synchronized to all nodes for the
purpose of validating the original model. If the downloaded
local model is modiﬁed and inconsistent with the hash value
stored on the blockchain, the committee leader would ignore
it and subsequent local models from that node until a new
committee leader is elected. Besides, the procedure of the
global model aggregation is also veriﬁable by all committee
members, preventing the committee leader from doing evil.

IV. MODEL ANALYSIS

In this section, DBAFL is theoretically analyzed from
latency,

including convergence, reliability,

several aspects,
mobility, and complexity.

A. Convergence Analysis

Assume there are K nodes in DBAFL and Dk is the local
data on node k. The number of samples on node k is nk =
|Dk|. N is the total number of samples across K nodes, which
is calculated by N = (cid:80)K
k=1 |Dk|. Assume that ∀k (cid:54)= k(cid:48), Dk ∩
Dk(cid:48) = ∅. The local empirical loss of node k is:

hk(wk) =

1
nk

(cid:88)

i∈Dk

(cid:96)i(wk),

(6)

where (cid:96)i(wk) is the corresponding loss function for data i and
wk is the local model parameter. Considering the existence of
(cid:15), the central objective function is calculated as:

F (w) =

K
(cid:88)

k=1

(cid:15)k
K

hk(wk).

(7)

where w is the aggregated global model. The goal of Eq. 7 is
to ﬁnd a model that satisﬁes w∗ = arg minw∈Rd F (w).

Following to [6], suppose that F (w) is L-smooth and µ-
strongly convex. The local functions hk(w) are B-locally
dissimilar at w, then:

F (wt+1) − F (wt) ≤ −∇F (wt)(cid:62)ηt
k

∇hk(wt)

(cid:15)(cid:48)
k
K
∇hk(wt)||2,

+

L
2

||ηt
k

(cid:15)(cid:48)
k
K

(8)

. µ is a non-negative value that satisﬁes

where ηk = 2µN (cid:48)
LB2n(cid:48)
k
(cid:15)(cid:48)
E(∇hk(w)) ≤ ||∇F (w)||. Since ∀(cid:15)k > 0, mk = ηt
K > 0.
k
k
Assuming that F (w) is bounded below, with the local bounded
gradient dissimilarity deﬁned in Chen et al. [6], it is trivial to
know that,

E(F (wt+1)) − F (wt) ≤ −mk(µ −

)||∇F (wt)||2

(9)
is still monotonically increasing. In DBAFL, the accuracy of
the initial global model must be greater than 1%. Therefore,

mkLB2
2

IEEE TRANSACTIONS ON VEHICULAR TECHNOLOGY

6

Fig. 3. The committee leader is decided by the hash value of the latest block. The hash values of the models are stored in the blockchain while the original
models are stored in distributed databases on RSUs. The relationship between the models in distributed databases and hash values on the blockchain is
demonstrated.

(cid:15)k < 100. Assume there are at least 100 total training samples
among all nodes, (cid:15)(cid:48)

(cid:15)(cid:48)
K < ηt
k

k and

k < K. Therefore, mk = ηt
k
kLB2
ηt
mkLB2
2
2

) < −ηt

k(µ −

−mk(µ −

).

(10)

So far, (cid:15) is already canceled out. As a result, the subsequent
proofs are the same as the proofs of Theorem 1 and Theorem
2 in Chen et al. [6]. Finally, it is proven that after E epochs,
DBAFL converges.

B. Reliability Analysis

1) Hash Values on Blockchain: The beneﬁts of uploading
models to the blockchain instead of a centralized aggregation
server are as follows: i) The consistency and reliability of
global models are guaranteed since the data in the blockchain
is immutable; ii) The training process becomes transparent and
auditable, preventing nodes from doing evil; iii) Buses become
trustable due to the existence of the consensus algorithm and
the smart contract. Time efﬁciency is critical for smart public
transportation systems, especially when training updated ML
models for trafﬁc condition prediction or driver assistance.
local nodes in
Since the blockchain is resource-intensive,
DBAFL only upload the hash values of the models to the
blockchain. The aforementioned beneﬁts are preserved, as the
model history is still
traceable by the hash values in the
blockchain and the original models are downloadable and
veriﬁable by all local nodes. Furthermore, this mechanism
greatly reduces the storage redundancy in the blockchain as
well as the storage requirements for buses.

2) Attack Resistance: The design of DBAFL is primarily
resistant to poisoning and DDoS attacks. When launching
poisoning attacks, the attackers manipulate the parameters of
local models and upload them to the global model, causing
the accuracy of the global model to decrease [4]. However, the
committee leader is able to identify the malicious local models
by testing their accuracy locally. As a result, the committee
leader will assign them relative low weight, which defends
poisoning attacks to a certain extent. Since the committee-
based consensus algorithm periodically elects a new leader
based on the hash value of the latest block, the accuracy

test result is ensured to be reliable and unbiased. In addition,
DBAFL introduces a stricter defense strategy that discards
local models with an accuracy below a speciﬁc threshold to
further reduce the inﬂuence of malicious local models. DDoS
attacks primarily aim to disrupt the centralized aggregation
server in traditional FL schemes by ﬂooding [37]. In DBAFL,
the periodically changed committee leader replaces the central-
ized aggregation server in classic FL, reducing the probability
of being targeted by trafﬁc ﬂooding in DDoS attacks.

3) Leader Election: A more frequent committee leader
election leads to the increased reliability of DBAFL and the
higher generality of the global model. However, it is almost
impossible to guarantee efﬁciency if the committee leader
is elected too frequently. Considering the mobile network is
unstable, the network latency is likely to result in committee
leader identities inconsistent and forks in the blockchain
during block propagation. Aside from that, block size has an
impact on efﬁciency as well. The decreased block size enables
more frequent committee leader elections, which results in
more blocks propagated in the blockchain and additional
network overhead.

4) Unstable Mobile Network: Buses may fall ofﬂine un-
expectedly in an unstable mobile network. As a comparison,
RSUs equipping with backup for disaster recovery are more
stable. Therefore, only RSUs are committee members and
eligible to be the committee leader, ensuring a stable global
model aggregation process. A bus that falls ofﬂine unexpect-
edly will not affect the training process on other nodes in
DBAFL due to its asynchronous aggregation strategy. Storing
models in InterPlanetary File System (IPFS) is a promising
solution to further improve data reliability [38] and is left for
future work.

C. Latency Analysis

According to [39], in a classic blockchain-based FL scheme,
the latency of a training round is the sum of local training
T e
local, model uploading T e
ag, block
generation T e
bp, and model downloading
dn. Therefore, the latency of e-th epoch T e is given as
T e
up + T e

up, model aggregation T e

bg, block propagation T e

local + T e

bp + T e
dn.

bg + T e

T e = T e

ag + T e

(11)

IEEE TRANSACTIONS ON VEHICULAR TECHNOLOGY

7

Speciﬁcally, in the local training phase, nodes do not need
communication with others. Since the asynchronous aggrega-
tion strategy does not wait for the local training on nodes
before aggregation, the local training latency T e
local is ignored
in DBAFL. As illustrated in Line 11, 12, and 17 of Algo-
rithm 1, the latency of model uploading phase is composed
of three parts: local model uploading T e
up model from buses to
RSUs, model hash uploading T e
up hash from buses to RSUs, and
model synchronization T e
sync model among RSUs. Therefore, the
latency of model uploading phase T e

up is expressed as

up hash + T e

sync model

up = T e
T e

up model + T e
Sw
BM log2(1 + γM )

=

+

Shash
BM log2(1 + γM )

+

Sw
BE

,

(12)

where Sw is the size of the model, Shash is the size of the
model hash, BE and BM are the bandwidth allocations of the
Ethernet network and the mobile network, respectively, and
γM is the received signal-to-noise ratio (SNR) of the devices
in the mobile network. As RSUs are connected with each other
through a high-speed Ethernet, there is no SNR considered in
T e
sync model.
After aggregation, the hash value of the global model is
uploaded to the blockchain while the original global model
is synchronized among RSUs, as shown in Line 22 and 23
of Algorithm 1. Assuming the time of processing aggregation
is negligible compared with the communication delays, the
latency of model aggregation phase T e

ag is calculated as

easy to be reduced to very small. As a result, T e
than T e and acceptable in public transportation scenarios.

bc is smaller

D. Mobility Analysis

Assume buses are traveling on a road in a built-up area,
where the 5G network coverage of an RSU is 300 meters and
the vehicle speed limit is 60 km/h [40]. By calculating, the
connection of a running bus to an RSU lasts 18 seconds at
most. Due to the device heterogeneity, it is hard to determine
how long it will take a bus to ﬁnish training a local model.
In DBAFL, the training of local models is independent for
each bus due to the asynchronous aggregation strategy, which
means that no network connection or waiting for others is
required during the training process. After training, buses need
to upload the local model to the nearby RSU and the hash
value to the blockchain before downloading a new global
model from the nearby RSU. As evaluated in Section V-B2,
the communication time cost in each training round of DBAFL
is always less than 5 seconds, which is much less than the
limitation of 18 seconds. After receiving the local model, the
committee leader performs aggregation locally and generates
a new global model, which is shared with other RSUs without
effects from the mobility of buses. Besides, the leader election
among committee members (RSUs) is also not affected by
the mobility of buses. Therefore, considering the mobility of
buses, DBAFL is still feasible in smart public transportation
systems.

ag = T e
T e

up hash + T e

sync model =

Shash
BM log2(1 + γM )

+

Sw
BE

.

E. Complexity Analysis

(13)
In order to improve efﬁciency, the committee-based consen-
sus algorithm in DBAFL does not involve mining or commu-
nicating during the block generation process, as demonstrated
in Section III-D. Therefore, the latency of generating blocks
T e
bg only involves a small amount of time spent computing
hash values and is considered negligible in comparison to the
communication delays. Thereafter, the newly generated block
is propagated throughout the network with latency

T e
bp =

Sblock
BM log2(1 + γM )

,

(14)

where Sblock is the size of the block. Finally, as shown in Line
9 of Algorithm 1, buses download the new global model from
the nearby RSU with latency

T e
dn =

Sw
BM log2(1 + γM )

.

(15)

Moreover, compared with an asynchronous federated learn-
ing scheme, the additional communication latency brought by
the blockchain T e

bc is
bc = 2T e
T e

up hash + 2T e

sync model + T e
bp,

(16)

due to the requirements of uploading hash values of global and
local models, synchronizing local and global models among
RSUs, and releasing the new block. Since the hash size is
much smaller than the model size, Shash and Sblock are much
up hash + T e
smaller than Sw. Thus, T e
dn. Moreover,
by increasing the bandwidth among RSUs BE, T e
sync model is

bp (cid:28) T e

up + T e

As the collected data changes over time, models are trained
on a regular basis. Assuming the newly arrived data size on a
bus is n, the computational complexity of training on the bus is
O(n). Since each bus has to perform training for E epochs, the
computational complexity of DBAFL on each bus is O(nE).
Considering DBAFL enables buses to train parallelly without
waiting for models from others, the overall computational
complexity of DBAFL is O(nE), which is acceptable.

Compared with classic FL, DBAFL has an additional com-
munication complexity of uploading hash of models to the
blockchain and electing new committee leaders. As the hash
values are small enough without effects of the model size,
it is easy to be packed into blocks and broadcast to nodes
through the P2P protocol. Since the identity of the new
committee leader is determined by the hash value of the
speciﬁc block, no more network communication is required
after the block is broadcasted to local nodes in IoV networks.
Therefore, the communication complexity for each training
round is O(log K), where K is the number of nodes in
DBAFL. Besides, the blockchain could be deployed purely
on RSUs to reduce the communication complexity of buses at
the cost of certain security and credibility. In that situation,
the communication complexity for each training round is
O(log M ), where M (M (cid:28) K) is the number of RSUs.

V. SYSTEM EVALUATION

In this section, experiments are conducted from three as-
pects, including learning performance, efﬁciency, and relia-

IEEE TRANSACTIONS ON VEHICULAR TECHNOLOGY

8

TABLE I
THE EXPERIMENT PARAMETER SETTINGS

Parameter
The number of nodes K
The local data size B
The number of epochs E
The learning rate η
The static scaling factor (cid:15)
The time to wait before creating a block
The maximum number of messages in a block
The maximum bytes of messages in a block

Value
5
1500
50
0.01
{0.5, 1.0, 1.5}
2s
10
10MB

bility, to evaluate DBAFL on IoV networks and answer the
following research questions:

• RQ1: How well does DBAFL improve learning perfor-

mance compared with state-of-the-art schemes?

• RQ2: Is there any advantage of DBAFL in efﬁciency

compared with state-of-the-art schemes?

• RQ3: Is DBAFL reliable enough to resist poisoning and

DDoS attacks?

A. Experiment Setup

Five virtual machines (VM) and four Raspberry PI B4
devices are set up as the experiment environment. Each
VM has 8 CPU cores and 8GB RAM to simulate an RSU.
Each Raspberry PI B4 has 4 CPU cores and 8GB RAM to
simulate a vehicle with limited computing resources. In terms
of software conﬁguration, the Ubuntu 20.10 operating system
is deployed on all nodes. ML models are trained with PyTorch
v1.8.1 based on Python 3.8. The smart contract is developed
on Hyperledger Fabric v2.3.0, an open-source blockchain
framework, to orchestrate model training and aggregation on
nodes and accept model hash values. Besides, a RESTful
service is developed on Express.js v4.17.1, allowing local
nodes to upload or download the hash values of models from
the blockchain. The implementation details are available at
https://github.com/xuchenhao001/AFL.

The default parameter settings for experiments are shown
in Table I. The number of nodes on a road is assumed to be
ﬁve by default, including an RSU and four buses traveling
under the network coverage of the RSU. In experiments, the
number of buses and RSUs ranges from one to four. Similar
to [3], [11], [35], the local data size B is 1500, the number
of local epochs E is 50, and the learning rate η is 0.01. To
examine the effectiveness of the dynamic setting strategy, the
value of (cid:15) is set to static, including 0.5, 1.0, and 1.5, to
reveal the impact of under-, equal-, and over-weighted stale
local models. According to the default parameter settings in
Hyperledger Fabric [41], a new block is generated when any
of the following conditions is reached: the waiting time for the
next invoke reaches 2 seconds, the number of invokes reaches
10, or the block size reaches 10 MB.

The ML models are trained on two benchmark datasets (i.e.
CIFAR-10 [11] and FMNIST [42]) and one real-world dataset
(i.e. LOOP [43]). Speciﬁcally, the LOOP dataset contains the
speed information collected by the inductive loop detectors

deployed on freeways in the Seattle area at intervals of 5
minutes. The ML models include MLP, CNN, and LSTM.

To evaluate DBAFL, several state-of-the-art schemes are

included in the scope for comparison.

1) BSFL: The synchronized version of DBAFL without the

dynamic scaling factor. Blockchain is included.

2) ASOFED: An AFL scheme with static decay coefﬁ-
cient balancing the previous and current gradients [6].
Blockchain is not included.

3) BDFL: An AFL scheme with all models saved in the

blockchain [30].

4) APFL: A semi-AFL scheme with reduced the commu-
nication frequency [10]. Blockchain is not included.
5) FedAVG: The traditional synchronous FL scheme [3].

Blockchain is not included.

6) Local: The traditional local training scheme that each
node trains the ML model on local data without com-
munication. Blockchain is not included.

Average test accuracy is calculated by averaging the accu-
racy of the local models on all nodes in a ﬁxed-time interval.
The average loss is calculated in the same way based on the
mean square error (MSE). The average time is calculated by
averaging the time cost of all nodes in each training round.
The average time per iteration is calculated by averaging the
time cost of all nodes across all training rounds.

To answer RQ1, the average test accuracy is compared with
state-of-the-art schemes when different models, datasets, and
IoV network settings are applied. Speciﬁcally, there are two
aspects of IoV network settings: (1) using different numbers of
IoT devices and VMs to mimic the differences in computing
resources among vehicles and RSUs; (2) distributing different
sizes of local data on VMs to mimic the disparity of data
collected among nodes. The node with a large volume of data
is the big node, whose local data size is set to 5000. For clear
comparison, the local data size for a small node is set to 500.
In addition, to validate the effectiveness of the dynamic setting,
the average test accuracy with the scaling factor adopted under
three static settings, including 0.5, 1.0, and 1.5, is compared
with that under the dynamic setting.

time cost

To answer RQ2,

in each
the average overall
training round is compared when adopting different models
and datasets in experiments. To further investigate the commu-
nication overhead brought by the blockchain, the average com-
munication time cost in each training round is also compared.
When training the CNN model on the CIFAR-10 dataset, the
average time costs on different stages and in different scales
of networks are compared.

To answer RQ3, Node 5 randomly adjusts the parameters
in local models before sending them to the committee leader
to simulate poisoning attacks. The test accuracy of DBAFL
with different levels of defense strategies is compared with
that of the classic AFL scheme without any defense strategy.
Moreover, the average test accuracy of DBAFL and that of the
classic AFL scheme are compared at different levels of DDoS
attacks, especially when 80% or 90% of the total trafﬁc is the
DDoS attack trafﬁc.

IEEE TRANSACTIONS ON VEHICULAR TECHNOLOGY

9

(a) CNN model on CIFAR-10

(b) CNN model on FMNIST

(c) MLP model on FMNIST

Fig. 4. Compare the average test accuracy of models in DBAFL with that of models in other schemes. There are ﬁve nodes in the network, two of which
are vehicles while others are RSUs.

(a) One Vehicle

(b) Two Vehicles

(c) Three Vehicles

(d) Four Vehicles

(e) One Vehicle

(f) Two Vehicles

(g) Three Vehicles

(h) Four Vehicles

Fig. 5. Compare the average test accuracy when different numbers of vehicles participate in training. There are ﬁve nodes in the network. Top row: The
average test accuracy when train CNN on CIFAR-10. Bottom row: The average loss evaluated by mean squared error when train LSTM on LOOP.

B. Results Analysis

performance of DBAFL.

1) RQ1. Convergence Speed and Model Accuracy: As
shown in Fig. 4, initially, the convergence speed of DBAFL is
a little lower than that of the local training scheme but higher
than the other ones. Subsequently, relatively early in the cycle
of the tests, the average test accuracy of DBAFL steps up to
an optimal level and converges faster than all other schemes.
The step-up is due to the contribution of stale local models
from vehicles. When training the CNN model, the step-up
happens earlier on CIFAR-10 (at around the 50th second) than
that on FMNIST (at around the 160th second), revealing that
FMNIST is more complex and harder for vehicles to train.
In addition, on the FMNIST dataset, DBAFL converges faster
when the trained model is MLP, rather than CNN, because the
MLP model is much simpler than the CNN model. Finally, the
average test accuracy of DBAFL when training CNN and MLP
models on two different datasets is always optimal compared
with that of other schemes, which reveals the stable learning

When training the CNN model on the CIFAR-10 dataset,
as shown in Fig. 5 (a) to (d), the advantage of DBAFL in
terms of convergence speed is more obvious compared with
BSFL, BDFL, APFL, and FedAVG, under the circumstance of
fewer vehicles. For example, the convergence time of DBAFL
is around 150 seconds earlier (note the 50th second and the
200th second marks) than that of BSFL and FedAVG when
only one vehicle is in the network, while the convergence time
of DBAFL is around 50 seconds earlier (note the 150th second
and the 200th second marks) than that of BSFL and FedAVG
when four vehicles are in the network. The reason is that more
nodes with rich computing resources involved in the network
contribute more local models at the early stage, resulting in
higher convergence speeds. When comparing the average test
accuracy of models, DBAFL performs more stable (at around
45%) than the local training scheme (from around 40% down
to around 20%) as the number of vehicles increases. This is

IEEE TRANSACTIONS ON VEHICULAR TECHNOLOGY

10

(a) One Big Node

(b) Two Big Nodes

(c) Three Big Nodes

(d) Four Big Nodes

(e) One Big Node

(f) Two Big Nodes

(g) Three Big Nodes

(h) Four Big Nodes

Fig. 6. Compare the average test accuracy when different numbers of big nodes participate in training. There are ﬁve nodes in the network. Top row: The
average test accuracy when train CNN on CIFAR-10. Bottom row: The average loss evaluated by mean squared error when train LSTM on LOOP.

(a) CNN model on CIFAR-10

(b) CNN model on FMNIST

(c) MLP model on FMNIST

Fig. 7. Compare the average test accuracy under dynamic scaling factor setting with that under static scaling factor settings. There are ﬁve nodes in the
network, two of which are vehicles while others are RSUs.

because a lagging node with limited computing resources is
unable to learn information from the high-performance nodes
in the local training scheme. As a result, more lagging nodes
in the network lead to lower average test accuracy among
all nodes. When training the LSTM model on the LOOP
dataset, as shown in Fig. 5 (e) to (h), the convergence speed
of DBAFL also decreases (as expected) with the increase of
vehicles in the network, although the ﬁnal convergence time is
almost identical. The reason is that the LSTM model ﬁts the
LOOP dataset easily and convergences after the ﬁrst round of
aggregation, while the ﬁnish time of the ﬁrst round of training
on vehicles is almost the same (at around the 100th second
mark).

To assess the impact of local data size on average test
accuracy, big nodes (nodes with a lot of data) and small
nodes (nodes with a small amount of data) have 5000 and 500
training samples, respectively. As shown in Fig. 6 (a) to (d),
DBAFL is able to achieve the optimal model accuracy com-

pared with the state-of-the-art schemes. With more big nodes
in the network, ASOFED and BDFL have decreased model
accuracy and slowed convergence speed. This is because too
much training data on big nodes slows down their training
process, while the local models from small nodes are not
weighted appropriately during the aggregation. When training
the LSTM model on the LOOP dataset, as shown in Fig. 6 (e)
to (h), the convergence speed of DBAFL is barely affected by
any increase in the number of big nodes in the network, which
is determined by the fastest node in the network. Nevertheless,
compared with other schemes, DBAFL is always the fastest
to converge.

Compared with static settings, the dynamic setting strategy
allows DBAFL to assign an optimal scaling factor during
the training process, resulting in the best convergence speed
and model accuracy in all situations, as shown in Fig. 7. In
addition, the ideal static scaling factor setting, which varies
when training various models on different datasets, follows no

IEEE TRANSACTIONS ON VEHICULAR TECHNOLOGY

11

(a) CNN model on CIFAR-10

(b) CNN model on FMNIST

(c) MLP model on FMNIST

(d) LSTM model on LOOP

Fig. 8. Compare the average overall time cost in each training round of DBAFL with other schemes. There are ﬁve nodes in the network, two of which are
vehicles while others are RSUs.

(a) CNN model on CIFAR-10

(b) CNN model on FMNIST

(c) MLP model on FMNIST

(d) LSTM model on LOOP

Fig. 9. Compare the average communication time cost in each training round of DBAFL with other schemes. There are ﬁve nodes in the network, two of
which are vehicles while others are RSUs.

Fig. 10. Compare the average time cost of four different stages, including
training, test, communication, and waiting, in the training round when training
the CNN model on the CIFAR-10 dataset. Training: Nodes train their local
models; Testing: Nodes test the accuracy of the local and global models;
Communication: Nodes upload and download local and global models;
Waiting: Nodes wait for the global model to be aggregated. Total: The sum
of the average time cost of four stages. There are ﬁve nodes in the network,
two of which are vehicles while others are RSUs.

Fig. 11. Compare the average time cost when training the CNN model on
the CIFAR-10 dataset with different numbers of nodes in the network. All
nodes are with the same computing resources as RSUs.

greater than 1.0 to amplify the impact of remarkable local
models when the fast nodes initially identify the worst learning
direction.

obvious pattern. Take the CNN model as an example: the ideal
static setting for the scaling factor when training on CIFAR-10
is 0.5, whereas it is 1.5 when training on FMINIST. Because
the learning process is random, all nodes have the same
probability of discovering an appropriate learning direction,
regardless of computing resources or local data size. When
the fast nodes discover the best learning direction ﬁrst, it is
desirable to set the scaling factor to a value less than 1.0
in order to reduce the impact of stale local models. On the
contrary, it is preferential to set the scaling factor to a value

Result 1: DBAFL has a superior convergence speed and
optimal model accuracy compared with state-of-the-art
schemes.

2) RQ2. Time Costs: As shown in Fig. 8, DBAFL has
the lowest overall time cost in each training round compared
with state-of-the-art schemes in all situations. Especially, when
training the CNN model on the FMNIST dataset, the average
overall time cost of DBAFL in each training round (around

IEEE TRANSACTIONS ON VEHICULAR TECHNOLOGY

12

(a) Classic AFL

(b) DBAFL without defense

(c) DBAFL with 80% defense

(d) DBAFL with 90% defense

Fig. 12. When Node 5 launches poisoning attacks, compare the average test accuracy of models in DBAFL at various degrees of defense thresholds with
that of models in the classic AFL scheme without any defense.

the training stage, DBAFL has a little higher average time
cost than BSFL, FedAVG, and Local Training (around 2.5
seconds higher), which is due to the more frequent aggregation
requests sent to the committee leader under the asynchronous
aggregation strategy. APFL has the highest average time cost
on training (almost twice the time than for the other schemes)
due to two training procedures in each training round. In terms
of the test stage, it is obvious that all schemes spend very
little time (less than 0.3 seconds), implying that the additional
accuracy-test stage in DBAFL has a negligible impact on the
efﬁciency of AFL. In terms of the communication stage, the
average time cost of DBAFL, BSFL, and BDFL is slightly
higher than that of FedAVG, APFL, and Local Training due
to the consensus process of the blockchain. However, DBAFL
has a minimal time cost in waiting for other nodes, which
is similar to ASOFED, BDFL, and Local Training, due to
its asynchronous aggregation strategy. On the other hand,
BSFL, APFL, and FedAVG waste nearly half of the time
in a round waiting instead of training or communicating,
resulting in lower training efﬁciency when compared with
DBAFL. After summing the average time cost of training,
test, communication, and waiting stages, the total round time
of DBAFL is only 0.97 seconds longer than that of ASOFED.
This shows that the proposed scheme effectively mitigates the
effects of the blockchain.

When increasing the network scale from 10 to 100, the
average time cost of a training round is demonstrated in
Fig. 11. As the number of nodes in the network increases,
high-performance nodes have to wait for more lagging nodes
in each training round when adopting synchronous aggre-
gation strategies, implying a longer waiting time. As a re-
sult, the schemes utilizing asynchronous aggregation strategies
(DBAFL, ASOFED, BDFL, and APFL) have better scalability
than those adopting synchronous aggregation strategies (BSFL
and FedAVG). Besides, even with the blockchain incorporated,
DBAFL achieves the same scalability as the pure AFL scheme
ASOFED and has higher scalability than other blockchain-
based schemes, due to the adoption of the proposed efﬁcient
consensus algorithm.

Result 2: DBAFL spends the shortest time in each training
round while making full use of the computing resources

(a) Classic AFL

(b) DBAFL

Fig. 13. When suffering different degrees of DDoS attacks, compare the
average test accuracy of models in DBAFL with that of models in the classic
AFL scheme.

50 seconds) is 150 seconds less than that of FedAVG (around
200 seconds). This is because the asynchronous aggregation
strategy shortens the waiting time before aggregation. As a
result, the advantage of DBAFL in terms of time cost in each
training round becomes ever more apparent if the training
process is more time-consuming. In addition, the periodic
spike in average overall time cost in APFL is mainly caused
by the waiting for the lagging nodes in every 10 rounds of
training.

The average communication time cost in each training round
is in line with the previous analysis, as shown in Fig. 9. The
average communication cost in each training round of DBAFL
is higher than that of FedAVG and APFL, and comparable to
that of BSFL. This is caused by the additional time cost in the
underlying blockchain architecture, including consensus and
block propagation. However, it is obvious that DBAFL has
a lower average communication time cost than BDFL, since
hash values instead of the original models are uploaded to
the blockchain. Moreover, compared with the average overall
time cost in each training round of classic FL (FedAVG), the
additional communication time cost incurred due to blockchain
is mostly negligible, especially when training the CNN model
on the FMNIST dataset (at 200 seconds compared with 4 sec-
onds). Since the additional communication time cost brought
by blockchain is 3 seconds with little variation, the impact of
blockchain becomes less as the model training task becomes
more complex.

When training the CNN model on the CIFAR-10 dataset,
the average time cost at different stages among all training
rounds and all nodes is summarized in Fig. 10. In terms of

IEEE TRANSACTIONS ON VEHICULAR TECHNOLOGY

13

on each node without time wasted waiting for other nodes.

3) RQ3. Attack Resistance: As shown in Fig. 12 (a) and
(b), the classic AFL scheme has difﬁculty converging under
to poisoning
poisoning attacks, while DBAFL is resistant
attacks initially to a certain extent. Despite multiple dips
during the training process, DBAFL eventually converges at an
appropriate accuracy level (at around the 50th second mark).
The outcome is consistent with the analysis in Section IV-B,
as the committee leader identiﬁes local models with low
accuracy and assigns a relatively small scaling factor to
them. Moreover, when adopting a stricter defense strategy, for
example, discarding local models with accuracy lower than
the threshold, the resistance of DBAFL towards poisoning
attacks is further improved. As shown in Fig. 12 (c) and (d),
when the defense threshold reaches 80% and 90%, both the
degree and the number of dips are reduced, as the impacts
of poisoned local models are mitigated more thoroughly. In
addition, with an increased degree of defense, the average test
accuracy of the model is also increased slightly (from 46% to
49%). This reveals that discarding poisoned local models has
no side effects for DBAFL, as the global model learns nothing
from the poisoned local models.

When suffering DDoS attacks, the aggregation server in
the classic AFL scheme becomes unresponsive to aggregation
requests, leading to a lower convergence speed of the global
model. As shown in Fig. 13 (a), the convergence time increases
from 40 seconds to 200 seconds as the DDoS attack trafﬁc
increases from 0% to 90%. However, the convergence speed
of DBAFL is barely affected even when the DDoS attacks
trafﬁc is increased to 90%. This is due to the periodic election
of a random committee leader, which reduces the likelihood
of the committee leader being the subject of DDoS attacks.

Result 3: DBAFL is natively resistant to both poisoning
and DDoS attacks with the potential to improve reliability
further.

VI. SUMMARY AND FUTURE WORK

This paper offers a blockchain-based asynchronous feder-
ated learning scheme with a dynamic scaling factor, aiming to
address the challenges faced by FL on IoV networks in terms
of learning performance, efﬁciency, and reliability. The novel
committee-based consensus algorithm in blockchain ensures
the reliability of DBAFL with the least cost in communica-
tion latency. In conjunction with the efﬁcient asynchronous
aggregation strategy, the dynamic scaling factor assigns rea-
sonable weights to stale local models and improves learning
performance for DBAFL. Extensive experiments conducted on
heterogeneous devices validate the advantages of DBAFL in
learning performance, efﬁciency, and reliability.

Future work includes recovering models when nodes go
ofﬂine unexpectedly, applying DBAFL on non-independent
and -identically distributed (Non-IID) datasets, and designing
effective strategies to resist other attacks.

ACKNOWLEDGMENT
Many thanks to Wanping Bai for her time and efforts in

helping proofread this paper.

REFERENCES

[1] Yunlong Lu, Xiaohong Huang, Ke Zhang, Sabita Maharjan, and Yan
Zhang. Blockchain empowered asynchronous federated learning for
IEEE Transactions on
secure data sharing in internet of vehicles.
Vehicular Technology, 69(4):4298–4311, 2020.

[2] Xiangjie Kong, Haoran Gao, Guojiang Shen, Gaohui Duan, and Sa-
jal K Das. Fedvcp: A federated-learning-based cooperative positioning
scheme for social internet of vehicles. IEEE Transactions on Computa-
tional Social Systems, 2021.

[3] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and
Blaise Aguera y Arcas. Communication-efﬁcient
learning of deep
networks from decentralized data. In Artiﬁcial Intelligence and Statistics,
pages 1273–1282. PMLR, 2017.

[4] Chenhao Xu, Youyang Qu, Yong Xiang, and Longxiang Gao. Asyn-
chronous federated learning on heterogeneous devices: A survey. arXiv
preprint arXiv:2109.04269, 2021.

[5] Lingjuan Lyu, Jiangshan Yu, Karthik Nandakumar, Yitong Li, Xingjun
Ma, Jiong Jin, Han Yu, and Kee Siong Ng. Towards fair and privacy-
preserving federated deep models. IEEE Transactions on Parallel and
Distributed Systems, 31(11):2524–2541, 2020.

[6] Yujing Chen, Yue Ning, Martin Slawski, and Huzefa Rangwala. Asyn-
chronous online federated learning for edge devices with non-iid data.
In 2020 IEEE International Conference on Big Data (Big Data), pages
15–24. IEEE, 2020.

[7] Yang Chen, Xiaoyan Sun, and Yaochu Jin. Communication-efﬁcient
federated deep learning with layerwise asynchronous model update and
temporally weighted aggregation. IEEE transactions on neural networks
and learning systems, 31(10):4229–4238, 2019.

[8] Wentai Wu, Ligang He, Weiwei Lin, Rui Mao, Carsten Maple, and
Stephen A Jarvis. Safa: a semi-asynchronous protocol for fast federated
learning with low overhead. IEEE Transactions on Computers, 2020.

[9] Xiaofeng Lu, Yuying Liao, Pietro Lio, and Pan Hui. Privacy-preserving
asynchronous federated learning mechanism for edge network comput-
ing. IEEE Access, 8:48970–48981, 2020.

[10] Yuyang Deng, Mohammad Mahdi Kamani, and Mehrdad Mah-
arXiv preprint

Adaptive personalized federated learning.

davi.
arXiv:2003.13461, 2020.

[11] Chenhao Xu, Youyang Qu, Peter W Eklund, Yong Xiang, and Longx-
iang Gao. Baﬂ: An efﬁcient blockchain-based asynchronous federated
In 2021 IEEE Symposium on Computers and
learning framework.
Communications (ISCC), pages 1–6. IEEE, 2021.

[12] Mansoor Ali, Hadis Karimipour, and Muhammad Tariq. Integration of
blockchain and federated learning for internet of things: Recent advances
and future challenges. Computers & Security, page 102355, 2021.
[13] Youyang Qu, Shiva Raj Pokhrel, Sahil Garg, Longxiang Gao, and Yong
Xiang. A blockchained federated learning framework for cognitive
computing in industry 4.0 networks. IEEE Transactions on Industrial
Informatics, 2020.

[14] Chenhao Xu, Youyang Qu, Tom H Luan, Peter W Eklund, Yong Xiang,
and Longxiang Gao. A light-weight and attack-proof bidirectional
IEEE Internet of Things
blockchain paradigm for internet of things.
Journal, 2021.

[15] Seyed Mojtaba Hosseini Bamakan, Amirhossein Motavali,

and
Alireza Babaei Bondarti. A survey of blockchain consensus algorithms
performance evaluation criteria. Expert Systems with Applications,
154:113385, 2020.

[16] Yinghui Liu, Youyang Qu, Chenhao Xu, Zhicheng Hao, and Bruce Gu.
Blockchain-enabled asynchronous federated learning in edge computing.
Sensors, 21(10):3335, 2021.

[17] Lei Feng, Yiqi Zhao, Shaoyong Guo, Xuesong Qiu, Wenjing Li, and
Peng Yu. Blockchain-based asynchronous federated learning for internet
of things. IEEE Transactions on Computers, 2021.
[18] Shuo Yuan, Bin Cao, Mugen Peng, and Yaohua Sun.

Chainsﬂ:
Blockchain-driven federated learning from design to realization. In 2021
IEEE Wireless Communications and Networking Conference (WCNC),
pages 1–6. IEEE, 2021.

[19] Muhammad Habib ur Rehman, Khaled Salah, Ernesto Damiani, and
Davor Svetinovic. Towards blockchain-based reputation-aware federated
In IEEE INFOCOM 2020-IEEE Conference on Computer
learning.
Communications Workshops (INFOCOM WKSHPS), pages 183–188.
IEEE, 2020.

IEEE TRANSACTIONS ON VEHICULAR TECHNOLOGY

14

[20] Jiasi Weng, Jian Weng, Jilian Zhang, Ming Li, Yue Zhang, and Weiqi
Luo. Deepchain: Auditable and privacy-preserving deep learning with
IEEE Transactions on Dependable and
blockchain-based incentive.
Secure Computing, 2019.

[21] Zhe Peng, Jianliang Xu, Xiaowen Chu, Shang Gao, Yuan Yao, Rong Gu,
and Yuzhe Tang. Vfchain: Enabling veriﬁable and auditable federated
learning via blockchain systems. IEEE Transactions on Network Science
and Engineering, 2021.

[22] Muhammad Shayan, Clement Fung, Chris JM Yoon, and Ivan Beschast-
nikh. Biscotti: A blockchain system for private and secure federated
learning. IEEE Transactions on Parallel and Distributed Systems, 2020.
[23] Yuzheng Li, Chuan Chen, Nan Liu, Huawei Huang, Zibin Zheng,
and Qiang Yan. A blockchain-based decentralized federated learning
framework with committee consensus. IEEE Network, 2020.

[24] Haoye Chai, Supeng Leng, Yijin Chen, and Ke Zhang. A hierarchical
blockchain-enabled federated learning algorithm for knowledge sharing
in internet of vehicles. IEEE Transactions on Intelligent Transportation
Systems, 2020.

[25] Shiva Raj Pokhrel and Jinho Choi. Federated learning with blockchain
for autonomous vehicles: Analysis and design challenges. IEEE Trans-
actions on Communications, 68(8):4734–4746, 2020.

[26] Jiawen Kang, Zehui Xiong, Dusit Niyato, Yuze Zou, Yang Zhang, and
Mohsen Guizani. Reliable federated learning for mobile networks. IEEE
Wireless Communications, 27(2):72–80, 2020.

[27] Jiawen Kang, Zehui Xiong, Xuandi Li, Yang Zhang, Dusit Niyato, Cyril
Leung, and Chunyan Miao. Optimizing task assignment for reliable
blockchain-empowered federated edge learning. IEEE Transactions on
Vehicular Technology, 70(2):1910–1923, 2021.

[28] Jiawen Kang, Zehui Xiong, Chunxiao Jiang, Yi Liu, Song Guo, Yang
Zhang, Dusit Niyato, Cyril Leung, and Chunyan Miao.
Scalable
and communication-efﬁcient decentralized federated edge learning with
multi-blockchain framework. In International Conference on Blockchain
and Trustworthy Systems, pages 152–165. Springer, 2020.

[29] Wei Yang Bryan Lim, Jianqiang Huang, Zehui Xiong, Jiawen Kang,
Dusit Niyato, Xian-Sheng Hua, Cyril Leung, and Chunyan Miao.
Towards federated learning in uav-enabled internet of vehicles: A
IEEE Transactions on
multi-dimensional contract-matching approach.
Intelligent Transportation Systems, 2021.

[30] Jin-Hua Chen, Min-Rong Chen, Guo-Qiang Zeng, and Jia-Si Weng.
Bdﬂ: A byzantine-fault-tolerance decentralized federated learning
IEEE Transactions on Vehicular
method for autonomous vehicle.
Technology, 70(9):8639–8652, 2021.

[31] Hong Liu, Shuaipeng Zhang, Pengfei Zhang, Xinqiang Zhou, Xuebin
Shao, Geguang Pu, and Yan Zhang. Blockchain and federated learning
for collaborative intrusion detection in vehicular edge computing. IEEE
Transactions on Vehicular Technology, 2021.

[32] Takayuki Nishio and Ryo Yonetani. Client selection for federated
In ICC 2019-
learning with heterogeneous resources in mobile edge.
2019 IEEE International Conference on Communications (ICC), pages
1–7. IEEE, 2019.

[33] Latif U Khan, Shashi Raj Pandey, Nguyen H Tran, Walid Saad, Zhu
Han, Minh NH Nguyen, and Choong Seon Hong. Federated learning for
edge networks: Resource optimization and incentive mechanism. IEEE
Communications Magazine, 58(10):88–93, 2020.

[34] Youyang Qu, Longxiang Gao, Tom H Luan, Yong Xiang, Shui Yu, Bai
Li, and Gavin Zheng. Decentralized privacy using blockchain-enabled
federated learning in fog computing. IEEE Internet of Things Journal,
7(6):5171–5183, 2020.

[35] Chenhao Xu, Yong Li, Yao Deng, Jiaqi Ge, Longxiang Gao, Meng-
Scei: A smart-contract
arXiv preprint

shi Zhang, Yong Xiang, and Xi Zheng.
driven edge intelligence framework for iot systems.
arXiv:2103.07050, 2021.

[36] Zeyad Al-Odat, Assad Abbas, and Samee U Khan. Randomness analyses
of the secure hash algorithms, sha-1, sha-2 and modiﬁed sha. In 2019
International Conference on Frontiers of Information Technology (FIT),
pages 316–3165. IEEE, 2019.

[37] Roberto Doriguzzi-Corin, Stuart Millar, Sandra Scott-Hayward, Jesus
Martinez-del Rincon, and Domenico Siracusa. Lucid: A practical,
IEEE
lightweight deep learning solution for ddos attack detection.
Transactions on Network and Service Management, 17(2):876–889,
2020.

[38] Sebastian Henningsen, Martin Florian, Sebastian Rust, and Bj¨orn
In 2020 IFIP

Scheuermann. Mapping the interplanetary ﬁlesystem.
Networking Conference (Networking), pages 289–297. IEEE, 2020.
[39] Hyesung Kim, Jihong Park, Mehdi Bennis, and Seong-Lyun Kim.
IEEE Communications

Blockchained on-device federated learning.
Letters, 24(6):1279–1283, 2019.

[40] Mansoor Shaﬁ, Andreas F Molisch, Peter J Smith, Thomas Haustein,
Peiying Zhu, Prasan De Silva, Fredrik Tufvesson, Anass Benjebbour,
5g: A tutorial overview of standards, trials,
and Gerhard Wunder.
challenges, deployment, and practice. IEEE journal on selected areas
in communications, 35(6):1201–1221, 2017.

[41] Elli Androulaki, Artem Barger, Vita Bortnikov, Christian Cachin, Kon-
stantinos Christidis, Angelo De Caro, David Enyeart, Christopher Ferris,
Gennady Laventman, Yacov Manevich, et al. Hyperledger fabric: a dis-
tributed operating system for permissioned blockchains. In Proceedings
of the thirteenth EuroSys conference, pages 1–15, 2018.

[42] Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel
image dataset for benchmarking machine learning algorithms. arXiv
preprint arXiv:1708.07747, 2017.

[43] Zhiyong Cui, Kristian Henrickson, Ruimin Ke, and Yinhai Wang.
Trafﬁc graph convolutional recurrent neural network: A deep learning
IEEE
framework for network-scale trafﬁc learning and forecasting.
Transactions on Intelligent Transportation Systems, 2019.

Chenhao Xu received a BS degree in Software
Engineering in 2018 from Beijing Institute of Tech-
nology, China. He is currently pursuing a Ph.D.
degree at the School of Information Technology,
Deakin University. His research interests include
blockchain, federated learning, and IoT.

Youyang Qu received his B.S. degree of Mechani-
cal Automation in 2002 and M.S. degree of Soft-
ware Engineering in 2015 from Beijing Institute
of Technology, respectively. He received his Ph.D.
degree at School of Information Technology, Deakin
University in 2019. His research interests focus
on dealing with security and customizable privacy
issues in Blockchian, Social Networks, Machine
Learning, and IoT. He is active in communication
society and has served as a TPC Member for IEEE
ﬂagship conferences including IEEE ICC and IEEE

Globecom.

in 2004,

Tom H. Luan received the B.Eng. degree from
Xi’an Jiao Tong University, China,
the
M.Phil. degree from The Hong Kong University of
Science and Technology in 2007, and the Ph.D.
degree from the University of Waterloo, Waterloo,
ON, Canada, in 2012. He is currently a Professor
with the School of Cyber Engineering, Xidian Uni-
versity, Xi’an, China. He has authored/co-authored
more than 40 journal papers and 30 technical papers
in conference proceedings, and has received one
U.S. patent. His research mainly focuses on content
distribution and media streaming in vehicular ad hoc networks and peerto-peer
networking, and the protocol design and performance evaluation of wireless
cloud computing and edge computing.

IEEE TRANSACTIONS ON VEHICULAR TECHNOLOGY

15

Peter W. Eklund is Professor of AI and Machine
Learning at Deakin University’s School of Infor-
mation Technology. Peter received his PhD from
Link¨oping University in Sweden, has an M.Phil from
Brighton University in the UK and was a graduate
in Mathematics from the University of Wollongong.
Peter has over 150 publications and is an elected
fellow of the Australian Computer Society.

Yong Xiang received his B.E. and M.E. degrees
from the University of Electronic Science and Tech-
nology of China, China, and PhD degree from
The University of Melbourne, Australia. He is a
Professor at the School of Information Technology,
Deakin University, Australia. He is also the Asso-
ciate Head of School (Research) and the Director
of the Artiﬁcial Intelligence and Data Analytics
Research Cluster. He has obtained a number of
research grants (including several ARC Discovery
and Linkage grants from the Australian Research
Council) and published numerous research papers in high-quality international
journals and conferences. He is the coinventor of two U.S. patents and some
of his research results have been commercialised. Dr Xiang is the Editor/Guest
Editor of several international journals. He has been invited to give keynote
speeches and chair committees in a number of international conferences,
review papers for many international
journals and conferences, serve on
conference program committees, and chair technical sessions in conferences.
Dr. Xiang is a senior member of the IEEE.

Longxiang Gao (SM17) received his PhD in Com-
puter Science from Deakin University, Australia. He
is currently a Professor at Qilu University of Tech-
nology (Shandong Academy of Sciences) and Shan-
dong Computer Science Center (National Supercom-
puter Center in Jinan). He was a Senior Lecturer
at School of Information Technology, Deakin Uni-
versity and a post-doctoral research fellow at IBM
Research & Development, Australia. His research
interests include Fog/Edge computing, Blockchain,
data analysis and privacy protection.

Dr. Gao has over 90 publications,

including patent, monograph, book
chapter, journal and conference papers. Some of his publications have been
published in the top venue, such as IEEE TMC, IEEE TPDS, IEEE IoTJ,
IEEE TDSC, IEEE TVT, IEEE TCSS, IEEE TII and IEEE TNSE. He has
being Chief Investigator (CI) for more than 20 research projects (the total
awarded amount is over $5 million), from pure research project to contracted
industry research. He is a Senior Member of IEEE.

