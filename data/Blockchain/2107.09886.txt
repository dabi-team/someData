Understanding the Scalability of Hyperledger Fabric

Minh Quang Nguyen
National University of
Singapore
quang.nguyen@u.nus.edu

Dumitrel Loghin
National University of
Singapore
dumitrel@comp.nus.edu.sg

Tien Tuan Anh Dinh
Singapore University of
Technology and Design
dinhtta@sutd.edu.sg

1
2
0
2

l
u
J

1
2

]

B
D
.
s
c
[

1
v
6
8
8
9
0
.
7
0
1
2
:
v
i
X
r
a

ABSTRACT
The rapid growth of blockchain systems leads to increasing
interest in understanding and comparing blockchain perfor-
mance at scale.
In this paper, we focus on analyzing the
performance of Hyperledger Fabric v1.1 — one of the most
popular permissioned blockchain systems. Prior works have
analyzed Hyperledger Fabric v0.6 in depth, but newer ver-
sions of the system undergo signiﬁcant changes that warrant
new analysis. Existing works on benchmarking the system
are limited in their scope: some consider only small net-
works, others consider scalability of only parts of the system
instead of the whole.

We perform a comprehensive performance analysis of Hy-
perledger Fabric v1.1 at scale. We extend an existing bench-
marking tool to conduct experiments over many servers while
scaling all important components of the system. Our results
demonstrate that Fabric v1.1’s scalability bottlenecks lie in
the communication overhead between the execution and or-
dering phase. Furthermore, we show that scaling the Kafka
cluster that is used for the ordering phase does not aﬀect
the overall throughput.

1.

INTRODUCTION

Blockchain technology is moving beyond cryptocurrency
applications [17, 14], and becoming a new platform for gen-
eral transaction processing. A blockchain is a distributed
ledger which is maintained by a network of nodes that do
not trust each other. At each node, the ledger is stored
as a chain of blocks, where each block is cryptographically
linked to the previous block. Compared to a traditional dis-
tributed database system, a blockchain can tolerate stronger
failures, namely Byzantine failures in which malicious nodes
can behave arbitrarily.

There are two types of blockchains: permissionless and
permissioned. The performance of both types of systems,
however, lags far behind that of a typical database. The
primary bottleneck is the consensus protocol used to en-
sure consistency among nodes. Proof-of-Work [14], for ex-

ample, is highly expensive and achieves very low through-
puts, whereas PBFT [9] does not scale to a large number of
nodes [11]. Beside consensus, another source of ineﬃciency
is the order-execute transaction model, in which transac-
tions are ﬁrst ordered into blocks, then they are executed
sequentially by every node. This model, adopted by pop-
ular blockchains such as Ethereum and Hyperledger Fabric
v0.6, is not eﬃcient because there is no concurrency in trans-
action execution [15].

New versions of Hyperledger Fabric, namely version v1.1
and later, implement a new transaction model called execute-
Inspired by optimistic concurrency
order-validate model.
control mechanisms in database systems, this model consists
of three phases. In the ﬁrst phase, transactions are executed
(or simulated) speculatively. This simulation does not aﬀect
the global state of the ledger. In the second phase, they are
ordered and grouped into blocks. In the third phase, called
validation or commit, they are checked for conﬂicts between
the order and the execution results. Finally, non-conﬂicting
transactions are committed to the ledger.

The ordering phase is performed by an ordering service
which is loosely-coupled with the blockchain. Hyperledger
Fabric oﬀers two types of ordering service: Solo which is
used for development and testing, and Kafka service which
is used for deployment in production system. The Kafka
service forwards transactions to an Apache Kafka cluster
for ordering [12]. By allowing parallel transaction execu-
tion, Hyperledger Fabric v1.1 can potentially achieve higher
transaction throughputs than systems that execute transac-
tions sequentially. However, it introduces an extra commu-
nication phase compared to the order-execute model, thus
incurring more overhead.

In this paper, we aim to provide a comprehensive per-
formance analysis of the execute-order-validate transaction
model. To this end, we evaluate the throughput and la-
tency of Hyperledger Fabric v1.1 in a local cluster of up to
48 nodes, running with Kafka ordering service. Our work
diﬀers from Blockbench [11], which benchmarks an earlier
version of Hyperledger Fabric with order-execute transac-
tion model. Its scope is more extensive than other recent
works that examine the performance of Hyperledger Fabric
v1.1 and later. For example, [6] does not consider the eﬀect
of scaling the Kafka cluster on the performance. Similarly,
[7, 15, 16] ﬁx the size of the Kafka cluster, and use fewer
than 10 nodes. In contrast, we examine the impact of scal-
ing Kafka cluster on the overall performance, using up to 48
nodes in a local cluster.

Hyperledger Caliper [3] is the oﬃcial benchmarking tool

1

 
 
 
 
 
 
for Hyperledger Fabric. However, it oﬀers little support and
documentation on how to benchmark a real distributed Fab-
ric setup with Kafka ordering service. Most of the docu-
mentation and scripts are considering the Solo orderer and
a single client. To overcome this, we developed Caliper++
by enhacing Caliper with a set of scripts to conﬁgure, start
and benchmark a distributed Fabric network with variable
number and type of nodes.

In summary, our main contributions are as follows:

• We extend the Caliper, Hyperledger’s benchmarking
tool, by adding support for distributed benchmarking.
The result is Caliper++, a benchmarking tool that can
start Fabric with varying sizes and conﬁgurations.

• We perform a comprehensive experimental evaluation
of Fabric v1.1 using Smallbank smart contract. We
scale the number of peers involved in all three trans-
action phases of the execute-order-validate model, by
using up to 48 nodes in a local cluster.

• We show that endorsing peer — the one that perform
the execute and validate phase — is the primary scala-
bility bottleneck. In particular, increasing the number
of endorsing peers not only incurs overheads in the
execute and order phase, but also leads to degraded
performance of the Kafka ordering service. On the
other hand, we observe that scaling the Kafka cluster
does not impact the overall throughput.

Section 2 discusses the background on blockchain systems,
and the architecture of Hyperledger Fabric v1.1. Section 3
describes related works on blockchain benchmarking. Sec-
tion 4 describes our benchmarking tool called Caliper++.
Section 5 presents the experiment setup and results, before
concluding in Section 6.

2. BACKGROUND

Blockchain networks can be classiﬁed as either permision-
less (or public) or permissioned (or private). In the former,
such as Ethereum [10] and Bitcoin [14], any node can join
the network, can issue and execute transactions. In the lat-
ter, such as Hyperledger Fabric, Tendermint [8], Chain [2],
or Quorum [5], nodes must be authenticated and authorized
to send or execute transactions.

The ledger stores all historical and current states of the
blockchain in the form of blocks linked together crypto-
graphically. To append a block, all nodes must agree. More
speciﬁcally, the nodes reach agreement by running a dis-
tributed consensus protocol. This protocol establishes a
global order of the transactions. The majority of permis-
sionless blockchains use computation-based consensus pro-
tocols, such as Proof-of-Work (PoW), to select a node that
decides which block is appended to the ledger. Permissioned
blockchains, on the other hand, use communication-based
protocols, such as PBFT [9], in which nodes have equal votes
and go through multiple rounds of communication to reach
consensus. Permissioned blockchains have been shown to
outperform permissionless ones, although its performance is
much lower to that of a database system [11]. Nevertheless,
they are useful for multi-organization applications where au-
thentication is required but participating organization do
not trust each other.

2

2.1 Hyperledger Fabric v1.1

Hyperledger Fabric v1.1 (or Fabric) is a permissioned (or
private) blockchain designed speciﬁcally for enterprise ap-
plications. A blockchain smart contract, also called chain-
code in Fabric, can be implemented in any programming
language. A Fabric network comprises four types of nodes:
endorsing nodes (or peers), non-endorsing nodes (or peers),
clients, and ordering service nodes. These nodes may belong
to diﬀerent organizations. Each node is given an identity by
a Membership Service Provider (MSP), which is run by one
of the organizations.

Endorsing and Non-Endorsing Peers. A peer in the
system stores a copy of the ledger in either GolevelDB [4]
or CouchDB [1] database.
It can be an endorsing peer if
speciﬁed so in the Endorsement Policies; otherwise it is non-
endorsing. Endorsing peers maintain the chaincode, execute
transactions, and create (or endorse) transactions to be for-
warded to the ordering nodes.

Endorsement Policies. A endorsement policy is asso-
ciated with a chaincode, and it speciﬁes the set of endorsing
peers. Only designated administrators can modify endorse-
ment policies.

System Chaincodes. In addition to running chaincode
speciﬁed by users, Fabric peers run a number of pre-deﬁne
system chaincodes. Theare are four system chaincodes: the
life cycle system chaincode (LSCC) for installing, instantiat-
ing and updating chaincodes, the endorsement system chain-
code (ESCC) for endorsing transactions by digitally signing
them, the validation system chaincode (VSCC) for validat-
ing endorsement signatures, and the conﬁguration system
chaincode (CSCC) for managing channel conﬁgurations.

Channel. Fabric supports multiple blockchains that use
the same ordering service. Each blockchain is identiﬁed by
a channel, where members may consist of diﬀerent sets of
peers. Transactions on a channel can only be viewed by
its members. The order of transactions in one channel is
isolated from those of in another channel and there is no
coordination between channels [6].

Ordering Service. The Ordering Service consists of
multiple Ordering Service Nodes (OSNs). The OSNs es-
tablish a global order of transactions and construct blocks
to be broadcast to peers. A block is created when one of the
following conditions is satisﬁed: (1) the number of transac-
tions reaches a speciﬁed threshold, (2) a speciﬁed timeout
is reached; (3) the block size reaches a speciﬁed threshold.
Endorsing peers receive blocks directly from the ordering
service, while non-endorsing peers get blocks via a gossip
protocol from other endorsing peers and from the ordering
service. A Solo ordering service consists of a single node (or
orderer) which serves all clients. A Kafka ordering service
consists of a Kafka cluster to which OSNs forward transac-
tions for ordering.

Client. A client sends transactions to endorsing peers,
wait until it receives all endorsement from these peers, then
broadcasts the endorsed transactions to OSNs.

Transaction. Separating the ordering service from the
peers, Hyperledger Fabric v1.1 adopts the execute-order-
validate (also called simulate-order-commit) model for ex-
ecuting a transaction. In contrast, Hyperledger Fabric v0.6
and other blockchain systems use the order-execute trans-
action model. In the next section, we describe the life cycle
of a transaction in Fabric v1.1.

E. Androulaki et al.[6] evaluate Fabric v1.1 using Kafka
ordering service. They use a Kafa-based network of 5 en-
dorsing peers, 4 Kafka nodes, 3 orderers, 3 Zookeeper nodes,
and a varying number of non-endorsing peers. We note that
non-endorsing peers do not play an active role in the trans-
action life cycle, as shown in Figure 1. Therefore, they are
not a potential scalability bottleneck. We argue that the
number of non-endorsing peers is not an important system
parameter. As a consequence, [6] falls short in the analysis
of system scalability.

Parth Thakkar et al.[16] also benchmark Fabric v1.1. How-
ever, their network is small, consisting of only 8 peers, 1
orderer and a Kafka cluster. The Kafka cluster runs on a
single node, which does not fully capture the communica-
tion overhead in a real system. Furthermore, they do not
consider scalability in terms of the number nodes. Ankur
Sharma et al.[15] use the Solo orderer which is not meant to
run in a real system. The network in [15] is small, with 4
peers, 1 client and 1 orderer distributed on 6 cluster nodes.
Similarly, A. Baliga et al. [7] use the Solo orderer. Although
they examine the eﬀect of scaling the number of endorsing
peers, they use a simple smart contract instead of ones rep-
resenting realistic transactional workloads like Smallbank or
YCSB.

4. CALIPER++

In this section, we present Caliper++1, our extension to
Hyperledger Caliper [3], the oﬃcial benchmarking tool for
Hyperledger Fabric.

4.1 Starting Up Fabric Network

The main challenge of benchmarking Kafka-based Fabric
network at scale lies in starting up the distributed Fabric
network. While Fabric v0.6 has only one type of nodes, rep-
resented by the peers themselves, Fabric v1.1 has six types of
nodes: endorsing peer, non-endorsing peer, orderer, Kafka
node (or broker), Zookeeper node, and client. This means
there are more system parameters to consider. It also makes
it more complex to automatically start up the network with
all these components. In fact, we found limited oﬃcial docu-
mentation on conﬁguring a Kafka-based system. There are
even fewer documentations on running Fabric on multiple
physical nodes.

At its current state, Caliper [3] – the oﬃcial benchmark-
ing tool for Hyperledger blockchains – only supports testing
on a local environment with a limited set of predeﬁned Fab-
ric network topologies, all of which use the Solo ordering
service. Caliper++ supports additional functionalities for
benchmarking Kafka-based Fabric network at scale. In par-
ticular, it provides a set of scripts to (a) auto-generate con-
ﬁguration ﬁles for any Fabric network topology, (b) bring up
large-scale Kafka-based Fabric network across cluster nodes,
(c) launch additional benchmarking tools at the operating
system level, such as dstat, strace, perf. In addition, it allows
benchmarking with distributed clients.

4.2 Benchmark Driver

Caliper++ implements the role of the client in Figure 1.
It can be conﬁgured to issue a given number of transactions
with a ﬁxed rate. After sending one transaction, it schedules

1The source code and documentation are available on
GitHub at https://github.com/quangtdn/caliper-plus

Figure 1: Transaction life cycle in Fabric v1.1.

2.2 Transaction Life Cycle in Fabric

Fabric v1.1 employs the novel execute-order-validate trans-
action model which comprises three phases, as depicted in
Figure 1.

Endorsement phase. A client submits a signed trans-
action to the endorsing peers. Each endorsing peer ver-
iﬁes if the client is authorized to invoke the transaction,
then speculatively executes the transaction against its local
blockchain state. This process is done in parallel without co-
ordination among endorsing peers. Output of the execution,
which consists of a read set and a write set, is used to create
an endorsement message. The peer signs the endorsement
and sends it back to the client.

Ordering phase: After collecting enough endorsements
according to the endorsement policy, the client creates an en-
dorsed transaction and forwards it OSNs. The ordering ser-
vice orders the transactions globally. It then creates blocks
and send them directly to endorsing peers or gossiping them
to non-endorsing peers.

Validation Phase: When receiving a block, every peer
validates transactions in the block against the endorsement
policy of the chaincode. After that, for every transaction,
the peer checks for read-write conﬂict by comparing the key
versions in the read set are the same as those in the cur-
rent ledger states. Any transaction that fails the validation
or conﬂict check is marked as invalid. Invalid transactions
are discarded and their eﬀects to the blockchain states are
rolled back. Finally, the block is appended to the peer’s
local ledger.

3. RELATED WORK

There has been considerable interest in benchmarking Hy-
perledger Fabric. Blockbench [11] is the ﬁrst framework for
benchmarking permissioned blockchains. It divides blockchain
stack into four layers: consensus, data, execution and appli-
cation. It contains many micro and macro benchmarks for
evaluating performance of every layer. However, Blockbench
only supports Fabric v0.6 whose architecture is highly dif-
ferent to that of v1.1. Although the results reported in [11]
cannot be extended to the new system, they present use-
ful baseline performance of the order-execute transaction
model.

3

Figure 2: Hyperledger Fabric Setup

sending the next transaction after such interval that ensures
the speciﬁed transaction rate is met. Responses from endors-
ing peers received during the execution phase are processed
asynchronously in the client’s main event loop.

We note that Caliper++ implements a benchmark client
(or driver) that is tightly coupled to the Fabric transac-
tion workﬂow. This is diﬀerent to Blockbench driver, which
is separated from the blockchain network.
In particular,
Blockbench driver sends transactions via JSON APIs, waits
for the transaction IDs from one of the blockchain nodes,
then sleeps an appropriate amount of time before sending
the next transaction. This driver is simply a workload gen-
erator, which is independent of the blockchain transaction
processing workﬂow. Caliper++, in contrast, implements
speciﬁc logic in which it waits and validates for responses
from endorsing peers against endorsement policies. Another
diﬀerence of Caliper++ is that it processes responses from
multiple nodes for each transaction, as opposed to from a
single node as in Blockbench driver.

5. PERFORMANCE ANALYSIS

In this section, we present our extensive benchmarking-
based analysis of Hyperledger Fabric v1.1 using Caliper++.
In this analysis, we focus on the following important per-
formance metrics: throughput, latency and scalability. The
throughput represents the number of successful transactions
per second (tps). This is the most common metric to eval-
uate the performance of distributed systems, including dis-
tributed databases and blockchains. Latency represents the
response time per transaction, in seconds.
In our experi-
ments, latency is taken as the average of the response time
of all transactions. Scalability represents the changes in
throughput and latency as the Fabric network scales up.

5.1 Experiments Setup

In this paper, a Hyperledger Fabric network topology is
described by the number of endorsing peers (or peers, for
simplicity) N , number of clients C with mutual transaction
send rate T , number of Fabric orderers O, number of Kafka
brokers K and number of Zookeeper nodes Z, as depicted
in Figure 2. All the peers belong to diﬀerent organizations
(orgs), are endorsing peers and use GolevelDB as the state
database. The endorsement policy contains all N endorsing
peers. The block size is set to 100 transactions/block, and
the timeout is set to 2s.

(a) Throughput for the setting N = C = K

(b) Throughput for scaling clients with ﬁxed rate T = 30

Figure 3: Throughput while scaling network size

In a Kafka ordering service, the Fabric orderers only act
as the proxies forwarding transactions to the Kafka brokers
which do the actual ordering. Since only the number of
Kafka brokers aﬀects directly the ordering capacity of the
ordering service, we ﬁx the number of Fabric orderers to
O = 4 throughout our experiments and vary the number
of Kafka brokers K to assess the impact on scaling Fabric.
We shall see that increasing the number of Fabric orderers
strongly degrades the system throughput due to communi-
cation overhead.

For each Kafka broker, we set min.insync.replicas = 2,
and def ault.replication.f actor = K − 1. This conﬁgura-
tion means that any transaction is written on K − 1 Kafka
brokers, and committed after being successfully written on
two Kafka brokers. If def ault.replication.f actor < K − 1,
there would be idle Kafka brokers not participating in the
ordering phase.
In practice, these idle Kafka brokers are
reserved for ordering other channels in the Fabric network.
However, in our testing environment with single-channel or-
dering service is not meaningful to have idle Kafka bro-
kers. Furthermore, setting a high value, such as K − 1, for
def ault.replication.f actor also increases the fault-tolerance
of the Kafka cluster, thereby providing us insights on the
trade-oﬀs between performance and fault-tolerance.

4

OrgNOrg2Org1P1P2PNO1O2OOK1K2KKZ1Z2ZZZookeeperKafka (backend)Orderer(frontend)PeersC1C2CCClientsCaliperFabricKafkaphase 1phase 2phase 2phase 3200250300350400450Total Request Rate C*T (tps)050100150200250300350400Total Throughput (tps)N=K=C=1N=K=C=2N=K=C=4N=K=C=8N=K=C=16468101214161820Number of peers N050100150200250Total Throughput (tps)K=4, fixed T=30K=8, fixed T=30K=16, fixed T=30(a) Latency for the setting N = C = K

Figure 5: Throughput with a single client C = 1

to the number of peers, K = N , and incrementally increase
the total request rate until the throughput signiﬁcantly de-
grades. The total throughput and latency are shown in Fig-
ure 3a and Figure 4a, respectively. We observe that the
throughput degrades as the network scales up. All net-
work sizes exhibit throughput degradation on request rates
equal or higher than 400 tps. The network with 16 peers
and Kafka orders is saturated with a request rate of 300
tps. The latency exhibits similar behavior, except on the
16 peers network where it starts to degrade from a request
rate of 250 tps. The ﬁxed request rates of C ∗ T = 300 tps
and C ∗ T = 400 tps represent, in general, the points before
and after the saturation, respectively. These request rates
are chosen to perform evaluate the costs of increasing the
number of peers or Kafka brokers.

Secondly, we ﬁx the number of Kafka brokers K and the
client’s request rate to T = 30 , while scaling the number
of peers and clients N = C. The network is linearly enlarg-
ing in terms of both number of peers and total request rate.
The total throughput and latency are shown in Figure 3b
and Figure 4b, respectively. We observe that the through-
put decreases and the latency increases when the network
consists of more than 8 peers. Moreover, from N = 12 on
all the tested network topologies, the throughput decreases
drastically. The cumulative request rate of the system at
this point is 360 tps.

Thirdly, we show that the client does not represent a
bottleneck by running the experiments with a single client,
C = 1, while scaling the number of peers and Kafka orderes,
N = K. The throughput, depicted in Figure 5, exhibits
similar performance as the throughput with more than one
client.

We observe similar patterns in all the above experiments,
exposing the same saturation points in all Fabric network
topologies under evaluation. This suggests that the sat-
uration of the networks is caused by Fabric’s incapability
of handling request rates beyond certain thresholds, rather
than the communication or capacity bound of clients. Before
the saturation point, the throughput of the Fabric network
with multiple clients is always slightly better than that of
Fabric network with a single client because, for the later, the
single client has to send a larger set of request transactions

(b) Latency for scaling clients with ﬁxed rate T = 30

Figure 4: Latency while scaling network size

We use the popular OLTP database benchmark workload
Smallbank [13] in our experiments. Simulating typical asset
transfer scenario and a large class of transactional workloads
such as TPC-C in general, Smallbank is suited to test the
Fabric system at scale. Furthermore, Smallbank is one of the
two macro-benchmark workloads used in Blockbench [11] to
benchmark Hyperledger Fabric v0.6, and thus gives us a
stand for comparison between v0.6 and v1.1.

The experiments were run on a 48-node commodity clus-
ter. Each server node has an Intel Xeon E5-1650 CPU
clocked at 3.5 GHz, 32 GB of RAM, 2 TB hard drive and
a Gigabit Ethernet card. The nodes are running Ubuntu
16.04 Xenial Xerus.

5.2 Determining the Saturation Point

In this section, we examine the capacity of the Fabric
system. We vary the number of peers N , while keeping the
number of clients equal to the number of peers, C = N , and
gradually increase the total request rate until the end-to-end
throughput is saturated. The saturation request rate C ∗ T
determined in these experiments is used in later experiments
where we investigate the system’s performance while scaling
the number of peers or Kafka brokers.

Firstly, we use a number of Kafka ordering brokers equal

5

200250300350400450Total Transaction Rate C*T (tps)0510152025Latency (s)N=K=C=1N=K=C=2N=K=C=4N=K=C=8N=K=C=16468101214161820Number of peers N0510152025Latency (s)K=4, fixed T=30K=8, fixed T=30K=16, fixed T=30200250300350400450Total Transaction Rate C*T (tps)050100150200250300350400Total Throughput (tps)N=K=1, C=1N=K=2, C=1N=K=4, C=1N=K=8, C=1N=K=16, C=1ordering.

5.4 Scaling the Number of Fabric Peers

In this set of experiments, we ﬁx the request rate and
investigate the impact of increasing the number of peers. We
ﬁx the total request rate to C ∗ T = 300 and C ∗ T = 400,
separately, as discussed in Section 5.2. We ﬁx the number of
Kafka orderers on a value in the set {4, 8, 16}, and increase
the number of peers up to 24. The average throughput and
latency of each experimental setting is shown in Figure 8
and 9, respectively.

Increasing the number of peers strongly degrades the sys-
tem’s throughput and limits Fabric’ scalability. To under-
stand the bottlenecks, we examine the system logs and ob-
serve that scaling the number of peers incurs overhead in
both the endorsement and ordering phases within the trans-
action ﬂow.

Firstly, increasing the number of endorsing peers means
that each client has to wait for a larger set of endorsements
from all the peers to prepare the endorser transaction. Ex-
amining the logs of Caliper’s clients, we observe that the
clients return a large number of timeout errors while collect-
ing the endorsements from peers and, accordingly, discard
those transaction proposals. As a result, clients send en-
dorser transactions to the orderers at a much lower rate due
to dropped transactions and the time overhead for collecting
endorsements.

Secondly, the orderers broadcast blocks to all the endors-
ing peers for validation. Hence, more peers correspond to
more communication overhead for the ordering service. In
the experiment with a ﬁxed request rate of 400 tps and
N = 24 peers, we observe that Caliper’s clients start to
return timeout errors while sending endorser transactions
to orderers. Further examining the orderers’ logs shows
that the rate at which orderers receive endorser transac-
tions from clients dominates the rate of returning conﬁrmed
transactions in the form of blocks. In particular, whenever
an orderer broadcasts a transaction to the Kafka cluster,
it logs a message ”[channel: mychannel] Enqueueing en-
velope. . . ”. After the transaction is ordered by the Kafka
and returned to the orderer for batching blocks, the orderer
logs a message ”[channel: mychannel] Envelope enqueued
successfully”, followed by the immediate acknowledgement
from the client that have just sent such transaction, ”[chan-
nel: mychannel] Broadcast has successfully enqueued mes-
sage of type ENDORSER TRANSACTION from. . . ”. We
count the number of such messages and compute the ra-
number of ”Envelope enqueued successfully” messages . On
tio r =
average, this ratio is larger than 1.7, suggesting that the
Kafka-based ordering service can only order and return to
the commit phase 1/1.7 ≈ 58.8% of the amount of endorser
transactions it receives. As increasing the number of peers
only lowers the endorser transaction rate sent to orderers,
as explained in the previous paragraph, and there is no in-
teraction between peers and Kafka cluster, the bottleneck
of orderers totally lies in the communication overhead for
broadcasting to a large number of peers.

number of ”Enqueueing envelope...” messages

Letting aside these issues, the scalability for Hyperledger
Fabric v1.1 improved against v0.6. Blockbench [11] shows
that v0.6 fails with more than 8 peers. However, the cur-
rent performance of Hyperledger Fabric v1.1 is still lim-
ited in terms of real-world applications. For example, other
two popular blockchain systems, Ethereum and Parity, scale

Figure 6: Impact of increasing the number of orderers

and accordingly incurs more overhead.

To further validate our observation, we proﬁle the systems
with dstat tool to get CPU and memory utilization, and net-
working traﬃc on each node. Figure 7 depicts the resource
utilization of diﬀerent types on nodes in a Hyperledger Fab-
ric network consisting of N = 8 peers, K = 8 Kafka nodes,
O = 4 Fabric orderers, Z = 3 Zookeeper nodes and C = 1
client. The trends are consistent with the benchmarking set-
ting, where the clients use eight diﬀerent transaction request
rates in the range C ∗ T ∈ [200, 425]. Each transaction re-
quest benchmark lasts for 120s. We observe that client and
peer resource utilization, especially at CPU and networking
level, increases with the transaction rate, as expected.

However, none of the Fabric network nodes is fully uti-
lizing the hardware. The CPU utilization is below 40% for
all nodes. While the clients and the peers use up to 30% of
the CPU, some Kafka brokers and Zookeeper nodes use less
than 5%. Memory utilization is also low. Only the front-
end orderer uses close to 20% (or around 6 GB) to buﬀer
the requests.

At networking level, the client, peers, Fabric orders and
Kafka leaders exhibit the highest traﬃc. This traﬃc rep-
resents both sent and received bytes. The high traﬃc of
Fabric orderer is explained by its double role, as receiver
of requests from the client and as dispatcher of requests to
Kafka, as shown in Figure 2. The high traﬃc of some Kafka
nodes, or leaders, is explained by their double role, as lead-
ers managing the replication inside the Kafka cluster and as
connecting links to the Fabric orderers. Compared to the
maximum capacity of 125 MB/s of the Gigabit Ethernet,
the traﬃc of Kafka leaders is below 50% or 60 MB/s, on
average.

5.3 Overhead of Fabric Orderers

In this section, we investigate the impact of scaling the
number of orderers. We take the network size with the low-
est performance in the previous section, N = K = 16, and
increase the number of orderers from O = 4 to O = 10. The
throughput plotted in Figure 6 shows that the network with
more orderers is performing worse. Increasing the number of
orderers only increases the communication overhead, since
orderers act as proxies broadcasting transactions rather than

6

200250300350400450Total Request Rate C*T (tps)050100150200250Total Throughput (tps)O=4, N=K=16O=10, N=K=16(a) Zookeeper

(b) Fabric Orderer

(c) Kafka Leader

(d) Kafka Broker

(e) Peer

(f) Client

Figure 7: Resource utilization of diﬀerent nodes in a Hyperledger Fabric network

up to more than 32 nodes while maintaining the steady
throughput around 100 and 50 tps respectively [11].

5.5 Communication Overhead within Kafka

Cluster

In this experiment, we ﬁx N = K = C = 16 and inves-
tigate the eﬀect of communication within the Kafka clus-
ter on Fabric’s throughput, by comparing two extreme set-
tings. In the Kafka-based Fabric ordering service, a transac-
tion must be written to def ault.replication.f actor number
of Kafka brokers, and committed after min.insync.replicas
number of successful writes. In our ﬁrst setting, we set these
two values to their maximum, def ault.replication.f actor =
15 and min.insync.replicas = 14, to cause a high com-
munication cost inside the Kafka cluster.
In the second
setting, we set these two values to the minimum possible,
def ault.replication.f actor = 1 and min.insync.replicas =
1, while violating fault-tolerance. Under the least commu-
nication setting, a transaction is committed immediately
after being written to one Kafka broker. After that, the
Kafka cluster still writes the transaction to the remaining
Kafka brokers, but the Fabric’s transaction ﬂow does not

halt for this operation, thereby incurring minimal commu-
nication overhead from Kafka service. Therefore, the gap
between these two cases relatively represents the overhead
of the Kafka ordering service under full fault-tolerance, rep-
resented by def ault.replication.f actor = K − 1.

The results, depicted in the Figure 10, show that the
throughput of the most intensive communication setting is
always dominated by the throughput of the least communi-
cation setting. However, as the diﬀerence is negligible, we
conclude that intra-cluster Kafka communication does not
aﬀect the overall throughput of Fabric.

5.6 Scaling Kafka brokers

In this set of experiments, we ﬁx the number of peers N ,
and investigate the eﬀect of increasing the number of Kafka
brokers. We ﬁx the total request rate to C ∗ T = 300 and
C ∗ T = 400. For each value of N ∈ {4, 8, 16}, followed by
the same number of clients, we vary the number of Kafka
brokers. The average throughput and latency of each setting
is shown in Figure 11 and Figure 12, respectively.

Generally, scaling the Kafka brokers does not impact the

throughput pattern or scalability of the system.

7

 0 10 20 30 40 50 0 200 400 600 800 1000 1200 0 20 40 60 80 100Utilization (%)Traffic (MB/s)Timeline (s)CPU UtilizationMemory UtilizationNetworking Traffic 0 10 20 30 40 50 0 200 400 600 800 1000 1200 0 20 40 60 80 100Utilization (%)Traffic (MB/s)Timeline (s)CPU UtilizationMemory UtilizationNetworking Traffic 0 10 20 30 40 50 0 200 400 600 800 1000 1200 0 20 40 60 80 100Utilization (%)Traffic (MB/s)Timeline (s)CPU UtilizationMemory UtilizationNetworking Traffic 0 10 20 30 40 50 0 200 400 600 800 1000 1200 0 20 40 60 80 100Utilization (%)Traffic (MB/s)Timeline (s)CPU UtilizationMemory UtilizationNetworking Traffic 0 10 20 30 40 50 0 200 400 600 800 1000 1200 0 20 40 60 80 100Utilization (%)Traffic (MB/s)Timeline (s)CPU UtilizationMemory UtilizationNetworking Traffic 0 10 20 30 40 50 0 200 400 600 800 1000 1200 0 20 40 60 80 100Utilization (%)Traffic (MB/s)Timeline (s)CPU UtilizationMemory UtilizationNetworking Traffic(a) Throughput for ﬁxed request rate C ∗ T = 300 tps

(a) Latency for ﬁxed request rate C ∗ T = 300 tps

(b) Throughput for ﬁxed request rate C ∗ T = 400 tps

Figure 8: Throughput while scaling the number of peers

6. CONCLUSION

In this work, we presented our benchmarking tool Caliper++,

which is speciﬁcally designed to examine the scalability of
Hyperledger Fabric v1.1, a permissioned blockchain plat-
form. Using Caliper++, we have conducted a comprehen-
sive study on the scalability performance of Fabric. By iden-
tifying major bottlenecks of system, we hope that the survey
and benchmarking tool would serve to guide the design and
implementation of Hyperledger Fabric in the future.

7. REFERENCES
[1] Apache CouchDB. http://couchdb.apache.org/.
[2] Chain. http://chain.com.
[3] Hyperledger Caliper.

https://www.hyperledger.org/projects/caliper.

[4] LevelDB in Go.

https://github.com/syndtr/goleveldb/.

[5] Quorum. http://www.jpmorgan.com/global/Quorum.
[6] E. Androulaki, A. Barger, V. Bortnikov, C. Cachin,
K. Christidis, A. D. Caro, D. Enyeart, C. Ferris,
G. Laventman, Y. Manevich, S. Muralidharan,
C. Murthy, B. Nguyen, M. Sethi, G. Singh, K. Smith,

(b) Latency for ﬁxed request rate C ∗ T = 400 tps

Figure 9: Latency while scaling the number of peers

A. Sorniotti, C. Stathakopoulou, M. Vukolic, S. W.
Cocco, and J. Yellick. Hyperledger fabric: A
distributed operating system for permissioned
blockchains. CoRR, abs/1801.10228, 2018.
[7] A. Baliga, N. Solanki, S. Verekar, A. Pednekar,
P. Kamat, and S. Chatterjee. Performance
characterization of hyperledger fabric. pages 65–74, 06
2018.

[8] E. Buchman. Tendermint: Byzantine fault tolerance in

the age of blockchains, Jun 2016. Accessed:
2017-02-06.

[9] M. Castro and B. Liskov. Practical byzantine fault

tolerance. In Proceedings of the Third Symposium on
Operating Systems Design and Implementation, OSDI
’99, pages 173–186, Berkeley, CA, USA, 1999.
USENIX Association.

[10] C. Dannen. Introducing Ethereum and Solidity:
Foundations of Cryptocurrency and Blockchain
Programming for Beginners. Apress, Berkely, CA,
USA, 1st edition, 2017.

[11] T. T. A. Dinh, J. Wang, G. Chen, R. Liu, B. C. Ooi,

8

4681012141618202224Number of peers N050100150200250300Total Throughput (tps)K=4, fixed C*T=300K=8, fixed C*T=300K=16, fixed C*T=3004681012141618202224Number of peers N050100150200250300Total Throughput (tps)K=4, fixed C*T=400K=8, fixed C*T=400K=16, fixed C*T=4004681012141618202224Number of peers N024681012141618Latency (s)K=4, fixed C*T=300K=8, fixed C*T=300K=16, fixed C*T=3004681012141618202224Number of peers N13.51414.51515.51616.51717.518Latency (s)K=4, fixed C*T=400K=8, fixed C*T=400K=16, fixed C*T=400Figure 10: Kafka communication test

and K.-L. Tan. Blockbench: A framework for
analyzing private blockchains. In Proceedings of the
2017 ACM International Conference on Management
of Data, SIGMOD ’17, pages 1085–1100, New York,
NY, USA, 2017. ACM.

[12] N. Garg. Apache Kafka. Packt Publishing, 2013.
[13] M. J. Cahill, U. R. Roehm, and A. David Fekete.
Serializable isolation for snapshot databases.
volume 34, pages 729–738, 01 2008.

[14] S. Nakamoto. Bitcoin: A peer-to-peer electronic cash

system. Cryptography Mailing list at
https://metzdowd.com, 03 2009.

[15] A. Sharma, F. M. Schuhknecht, D. Agrawal, and

J. Dittrich. How to databasify a blockchain: the case
of hyperledger fabric. CoRR, abs/1810.13177, 2018.

[16] P. Thakkar, S. Nathan, and B. Vishwanathan.
Performance benchmarking and optimizing
hyperledger fabric blockchain platform. CoRR,
abs/1805.11390, 2018.

[17] S. Underwood. Blockchain beyond bitcoin. Commun.

ACM, 59(11):15–17, Oct. 2016.

(a) Throughput for request rate C ∗ T = 300 tps

(b) Throughput for request rate C ∗ T = 400 tps

Figure 11: Throughput Performance

9

200250300350400450Total Request Rate C*T (tps)050100150200250Total Throughput (tps)N=K=16, max communicationN=K=16, min communication4816Number of Kafka brokers K050100150200250300350400Total Throughput (tps)N=4, fixed C*T=300N=8, fixed C*T=300N=16, fixed C*T=3004816Number of Kafka brokers K050100150200250300350400Total Throughput (tps)N=4, fixed C*T=400N=8, fixed C*T=400N=16, fixed C*T=400(a) Latency for request rate C ∗ T = 300 tps

(b) Latency for request rate C ∗ T = 400 tps

Figure 12: Latency Performance

10

4816Number of Kafka brokers K02468101214161820Latency (s)N=4, fixed C*T=300N=8, fixed C*T=300N=16, fixed C*T=3004816Number of Kafka brokers K02468101214161820Latency (s)N=4, fixed C*T=400N=8, fixed C*T=400N=16, fixed C*T=400