HDCoin: A Proof-of-Useful-Work Based
Blockchain for Hyperdimensional Computing

Dongning Ma, Sizhe Zhang, Xun Jiao
Villanova University
{dma2, szhang6, xun.jiao}@villanova.edu

2
2
0
2

b
e
F
7

]

R
C
.
s
c
[

1
v
4
6
9
2
0
.
2
0
2
2
:
v
i
X
r
a

Abstract—Various blockchain systems and schemes have been
proposed since Bitcoin was ﬁrst introduced by Nakamoto Satoshi
as a distributed ledger. However, blockchains usually face crit-
icisms, particularly on environmental concerns as their “proof-
of-work” based mining process usually consumes a considerable
amount of energy which hardly makes any useful contributions to
the real world. Therefore, the concept of “proof-of-useful-work”
(PoUW) is proposed to connect blockchain with practical applica-
tion domain problems so the computation power consumed in the
mining process can be spent on useful activities, such as solving
optimization problems or training machine learning models.
This paper introduces HDCoin, a blockchain-based framework
for an emerging machine learning scheme: the brain-inspired
hyperdimensional computing (HDC). We formulate the model
development of HDC as a problem that can be used in blockchain
mining. Speciﬁcally, we deﬁne the PoUW under the HDC scenario
and develop the entire mining process of HDCoin. During mining,
miners are competing to obtain the highest test accuracy on a
given dataset. The winner also has its model recorded in the
blockchain and are available for the public as a trustworthy
HDC model. In addition, we also quantitatively examine the
performance of mining under different HDC conﬁgurations to
illustrate the adaptive mining difﬁculty.

I. INTRODUCTION
The debut of Bitcoin as a distributed ledger was almost
14 years ago to serve as a peer-to-peer electronic cash
system [13]. Since then, many blockchain systems imple-
mented such as Ethereum [18], have obtained great success
for their advantages like decentralization, transparency and
durability over the ﬁat currency. Despite of those advantages,
blockchain also has its “pain-in-the-neck”. One of the biggest
concern are from the “proof-of-work” (PoW) principle inside
the competitive blockchain mining process [7], [5]. During
mining, to determine a winner, different miners need to solve a
cryptographic problem that is computationally expensive. For
example, in bitcoin mining, the miner is required to ﬁnd a
random number referred to as “nonce” that could contribute
to producing a hash with a certain number of zeros as preﬁx.
Therefore, the energy consumed in ﬁnding the number can
hardly yield any useful and practical outcomes beyond the
blockchain.

To tackle this issue, frameworks with the principle of
“proof-of-useful-work” (PoUW) are proposed, attempting to
connect the mining process with real-world applications, i.e.,
to replace the problem of solving a crypotographic hash
with a real world task that can yield useful outcomes. For
example, miners are required to work with NP-hard opti-
the computation power can be
mization problems so that

used to beneﬁt transportation transactions [8]. Such concepts
can be generalized applied to various optimization problems
that are equivalent to solve the NP-hard travelling salesman
problem [11]. In addition to optimization problems, the de-
velopment of machine learning models such as the training of
neural networks can also be elaborated as a computationally
expensive problem for miners to solve using their computation
power [2].

Inspired by those frameworks, we propose to introduce
the concept of blockchain and PoUW into an emerging ma-
chine learning scheme: the brain-inspired hyperdimensional
computing (HDC). HDC is an emerging machine learning
paradigm that utilize high dimensional patterns imitating brain
activation functions to complete learning tasks [9]. Recently,
HDC has shown comparable capability with SOTA machine
learning models under various application scenarios such as
bio-informatics [12], [10], natural language processing [17],
[15] and robotics [14]. Similar to other machine learning
models, HDC also require signiﬁcant amount of computation
to train and ﬁne-tune a model, which can serve as the
computationally expensive task during the blockchain mining
process. Compared with neural networks, HDC has an
advantage that the HDC model architecture can be directly
generated from a nonce, which does not require com-
plicated architectural mapping like neural networks [2].
HDC models also provide multiple knobs to tune the difﬁculty
of mining problem, enabling capabilities of an adaptive and
ﬂexible difﬁculty conﬁguration.

The main contributions of this paper are listed as follows:
• We introduce the concept of blockchain into HDC and
propose, HDCoin, for HDC model development. We
deﬁne “proof-of-useful-work” principles under the HDC
scenario and the mining cycle of HDCoin.

• During the mining cycle, miners compete against each
other to train an HDC model based on the given training
and inference set. The inference accuracy of the HDC
model is used as the benchmark to determine which miner
wins the competition. The nonce used in generating the
winning HDC model, i.e., the outcome of the “useful-
work” of HDCoin mining process is recorded into the
blockchain as well for public reference.

• We evaluate the performance on mining using three
datasets and illustrate the impact of HDC model hyper-
parameters, which can enable adaptive difﬁculty adjust-
ment during the mining process.

 
 
 
 
 
 
II. HDCOIN PRELIMINARIES

A. HDC Notions

Hypervectors (HV) are numerical vectors that are high-
dimensional, holographic vectors with i.i.d. elements [9]. We
can note a d−dimensional HV as (cid:126)H = (cid:104)h1, h2, . . . , hd(cid:105) ,
where hi refers to the numerical elements inside the HV.
In HDC, HVs are the fundamental component to represent
information in different types, modalities and layers of features
because of its high dimensionality.

To aggregate information from different HVs to establish
new HVs, HDC uses different operations. The three mostly
used operations in HDC are addition, multiplication and
permutation. Additions and multiplication are element-wise,
taking two HVs as input operands and perform addition or
multiplication with each element on the same index. Permu-
tation, on the other hand, takes only one HV as the input and
cyclically shifts the HV by a speciﬁc amount. Note that the
dimension of the HVs are not modiﬁed, i.e., the dimension of
the input HV and the output HV are identical.

To quantitatively measure the information likeness between
two HVs, similarity metrics such as Euclidean distance,
hamming distance (for binary vectors) and cosine similarity
are usually used. A higher similarity suggests more overlap in
the information the two HVs represent.

B. Developing HDC Model

To develop and HDC model for a classiﬁcation task, there

are three main steps: Encoding, Training, and Inference.

Encoding is the fundamental step to build an HDC model
as it is used in both the other two steps. In encoding, the
realistic features of an input sample is projected into their high-
dimensional space representatives: the HVs. This is done by
applying a set of application-speciﬁc HD operations. Without
losing generality, we can assume a sample has a feature
vector that contains m dimensions. For each dimension of
feature there is a corresponding item memory R. Suppose the
application-speciﬁc set of HD operations is Θ, the correspond-
ing encoded HV can then be denoted as (cid:126)H = Θ(R, (cid:126)F ).

Training is the step of establishing the associative memory.
This is done by adding up all the encoded HVs that shares
the same label into a HV which is referred to as class HV.
Associative memory stores the class HVs at the number of
classes in the learning task. Assume there are k classes in
the classiﬁcation task and the encoded the HVs (cid:126)H l for each
training sample where l refers to the label of the sample.
Training process in HDC is to build the associative memory
A is thus by: A = {(cid:80) (cid:126)H 1, (cid:80) (cid:126)H 2, . . . , (cid:80) (cid:126)H k}.

Inference is the step of predicting the label of a sample that
is unseen, using the associative memory established during
the training step. During inference, the unseen sample is ﬁrst
encoded into its representative HV referred to as query HV (cid:126)Hq,
using the same set of HD operations applied in training. Then,
the similarity metrics are calculated between the query HV and
each of the class HVs in the associative memory. The class
with highest similarity is then selected as the predicted label of

the unseen sample, because this indicates that the information
inside the unseen sample is the most similar to that class. The
inference step can be described as l = argmax({δ( (cid:126)Hq, A)}).

III. HDCOIN BLOCKCHAIN AND MINING

An overview of HDCoin mining cycle is illustrated
in Fig. 1. HDCoin has basically two main sections,
the
blockchain and the mechanisms inside each miner during the
mining process.

A. HDCoin Blockchain

The blockchain section of HDCoin resembles a typical
blockchain framework of verifying transactions and adding
it to the blocks. Assume there are k blocks already on the
blockchain, to create the k + 1 block, there requires to be the
following content to be hashed into the block: 1). hash of the
the previous block, 2). hash of the (conﬁrmed) transactions in
the block (based on Merkle root [16]), 3). the 32-bit nonce
from the winning miner that is used to generate the highest
competitive HDC model. There are also other information
hashed into the block,
including the timestamp, and the
difﬁculty of this block (which is an adaptive parameter), etc.

B. HDCoin Mining

The miner section consists of the training and inference of
an HDC model. During the mining process, the miner ﬁrst
picks the transactions they wish to verify from a pool of
unconﬁrmed transactions. At the same time, the miner obtains
the training and inference set of a speciﬁc learning task which
can be distributed by the blockchain or appointed from open
source datasets that are publicly available. The integrity of
the dataset can be veriﬁed via existing hashing protocols like
SHA-256.

Once the dataset is prepared, the miner starts to randomly
choose a nonce which is a 32-bit integer number. Setting
the nonce as a seed, the miner subsequently generates the
item memories for the learning task. The miner uses the item
memory to encode the samples from the training and inference
set to obtain corresponding encoded HVs to each sample.
According to Sec II-B, all the samples from the training set
are aggregated together to establish the associative memory.
After training, the associative memory is then evaluated by
the inference HVs to obtain the inference accuracy. Until
here, we obtain a mapping from the nonce to the accuracy
of the inference set. The miner can repeat the process and
perform trials on different nonces to achieve higher accuracy
as possible.

C. Proof-of-Useful-Work

From the mining process we can understand that, in HD-
Coin, rather than solving a cryptographic hash function prob-
lem as the “proof-of-work”, miners train an HDC model,
which is the “proof-of-useful-work”. The trained HDC model
becomes a “useful” outcome from the process of conﬁrming
the transactions which is one of the objective by a traditional
blockchain.

Fig. 1. HDCoin Framework: Blockchain (left) and Miners (right).

The nonce used to generate the HDC model is to be recorded
into the blockchain with the consensus of miners on the
network. This is to validate that the nonce genuinely has the
highest accuracy out of all miners. Therefore, all the miners on
the network takes the nonce and repeat the process of training
and inference to verify if the nonce can reach the declared
accuracy. Other format-wise validations during the veriﬁcation
process such as the data structure, timestamp, size checks are
also applied here. Once the veriﬁcation succeeds, the nonce
is recorded onto the blockchain the the network is proceeding
to the next block. The recorded nonce can act as an authentic
HDC model distribution to the public.

D. Adaptive Mining Difﬁculty

Similar to other blockchain frameworks, the difﬁculty of
mining in HDCoin needs to be adaptive. In neural network,
quality threshold like the accuracy is usually used to control
the difﬁculty of mining, i.e., when the bar of accuracy in-
creases, it becomes more difﬁcult to train the neural network.
However, we think it is not enough, or rather, not optimal
to use accuracy as the difﬁculty adjustment knob because the
accuracy of different application can vary at very large scales.
For example, for simple and benchmark-level datasets like
MNIST handwritten digits, a simple LeNet-like architecture
can achieve a very high accuracy within a short time period
even for a commodity desktop to train from scratch while
for more challenging datasets like ImageNet, to obtain SOTA
accuracy requires signiﬁcantly larger model and days of GPU
training time [6]. Particularly when there is a relatively new
application and/or dataset, it is not straightforward to know
what accuracy threshold to set as the bar for the mining
competition.

HDC however, has the advantage over neural networks
because apart from the accuracy, there is also another hyper-
parameter that can be used to tune the difﬁculty of the mining
task: the HV dimension. With a higher dimension of HV in the
HDC model, there requires more computations for encoding,
training and inference, as well as a larger vector space for HV

to represent information, thus adding difﬁculty for the overall
mining competition. The dimension as the difﬁculty parameter
is also required to be hashed into the blockchain since this
is the necessary information for generating the HDC model
together with the nonce.

IV. EVALUATION ON MINING PERFORMANCE

A. Setup

As a case study on the evaluation of mining performance,
we choose three applications from different domains: 1).
CARDIO: Cardiotocography dataset aiming at classifying
measurements of fetal heart rate (FHR) and uterine contraction
(UC) features into 10 classes [4]; 2). UCIHAR: Human activity
recognition dataset aiming at recognizing 12 types of human
activities [1]; and 3). ISOLET: Speech recognition dataset
aiming at recognizing voice audio of the 26 letters of the
English alphabet [3].

We choose the hyper-parameter HV dimension and sweep
from 3,000 to 15,000 for evaluating mining performance under
adaptive difﬁculty settings. All the experiments are conducted
using a commodity laptop with Intel Core i7-10700F (2.90
GHz) CPU and 32 GB memory.

B. Experimental Results

We ﬁrst present the results on the maximum accuracy a
miner can reach when generating HDC models with different
number of nonces during each mining cycle of all the three
applications. We can observe that, when the number of nonces
increases,
the maximum achieved accuracy also increases.
This is reasonable as the miner invests more computation
resources in training by attempting to generate and train
HDC model with more nonces, the overall accuracy can thus
increase. However, we also observe that different applications
have different range of accuracy increase. For CARDIO, the
accuracy grows more than 3% from 83.5% to 87.0% when the
number of nonces increases, while for ISOLET and UCIHAR,
the accuracy increases only within 1%. This indicates that

Previous k blocks on blockchainHash(Bk)Pool of unconfirmed transactionsPicked transactionsby minerBlock k+1: Hash(Bk+1)Nonce (Winner)NonceItem memoryTrainingsetItem memoryItem memoryHDC ModelValidationsetgenerateMinerEncodingTrainingHVsValidationHVsTrainingValidationAssoc. MemoryNonceItem memoryTrainingsetItem memoryItem memoryHDC ModelValidationsetgenerateMinerEncodingTrainingHVsValidationHVsTrainingValidationAssoc. MemoryNonceItem memoryTrainingsetItem memoryItem memoryHDC ModelInferencesetgenerateMinerEncodingTrainingHVsInferenceHVsTrainingInference(accuracy)Assoc. Memoryselect by highest accuracy and verifyOther info (difficulty, time, etc)(a) CARDIO

(b) ISOLET

(c) UCIHAR

Fig. 2. Maximum accuracy achieved by miner with different amounts of nonces.

there is inconsistency across applications on the sensitivity
against nonce changes.

Moreover, the accuracy can also saturate, i.e., reaching a
cap of accuracy increase after a certain time of generating and
training the HDC model with different nonces. For example,
accuracy of UCIHAR stops signiﬁcantly increase after trying
10 nonces. For CARDIO, the slope of accuracy increase also
lessens after 12 nonces. This is because when the accuracy
reaching the ceiling, the dominating factor affecting accuracy
is not the nonce for generating and training the HDC model,
but
the capability limit of the speciﬁc machine learning
algorithm itself.

In addition to the relationship between accuracy and the
number of nonces, we also evaluate the nonce time under
different HV dimensions. The nonce time is deﬁned by the
total CPU time spent on using one nonce to generate the item
memory, encoding, training and inference. From Table I, we
can observe that when HV dimension becomes higher, the
nonce time also increases as more operations are required with
higher dimensionality of the HVs.

For different applications, the nonce time also drastically
varies as the encoding mechanism, the size of the dataset and
the number of classes can change. For example, CARDIO
usually only requires less than 10 second for a nonce however,
the execution of ISOLET and UCIHAR needs more than one
minute of runtime. This provides a ﬂexible knob in addition to
accuracy threshold to enable HDC as a ﬂexible and easy-to-
conﬁgure mining task which can target at different scenarios
particularly according to the computation resources allocated
in the blockchain.

TABLE I
NONCE TIME VS HV DIMENSIONS IN HDCOIN

HV dimension

3000

5000

7000

10000

15000

nonce
time
(sec)

CARDIO
ISOLET
UCIHAR

2.88
63.72
70.68

4.27
90.56
98.11

5.17
113.78
125.59

6.89
148.81
161.58

10.16
202.79
231.47

V. CONCLUSION

This paper introduces HDCoin, a blockchain framework
for the emerging brain inspired hyperdimensional computing

scheme. Speciﬁcally, we use the training of HDC model as
the “proof-of-useful-work” (PoUF) as an alternative to the tra-
ditional “proof-of-work” schemes of solving a cryptographic
hash. We also elucidate the HDCoin mining process from
the distribution of dataset and the veriﬁcation of the winner.
The model from the winning miner is also recorded into
the blockchain as an authentic way of accomodating and
publicly sharing an HDC model. HDCoin enables adaptive
difﬁcult on mining by two aspects: the accuracy as well as the
hyper-parameter of HDC. We evaluate the mining performance
of HDCoin using three datasets from diverse application
domains. To the best of our knowledge, this is the ﬁrst attempt
to introduce blockchain into hyperdimensional computing and
shed light for potential future research in this direction.

REFERENCES

[1] Davide Anguita, Alessandro Ghio, Luca Oneto, Xavier Parra, and
Jorge Luis Reyes-Ortiz. A public domain dataset for human activity
recognition using smartphones. In Esann, volume 3, page 3, 2013.
[2] Alejandro Baldominos and Yago Saez. Coin. ai: A proof-of-useful-
work scheme for blockchain-based distributed deep learning. Entropy,
21(8):723, 2019.

[3] Ron Cole, Yeshwant Muthusamy, and Mark Fanty. The ISOLET spoken
letter database. Oregon Graduate Institute of Science and Technology,
Department of Computer . . . , 1990.

[4] Antonia Costa, Diogo Ayres-de Campos, Fernanda Costa, Cristina San-
tos, and Joao Bernardes. Prediction of neonatal acidemia by computer
analysis of fetal heart rate and st event signals. American journal of
obstetrics and gynecology, 201(5):464–e1, 2009.

[5] Alex De Vries. Bitcoin’s growing energy problem. Joule, 2(5):801–805,

2018.

[6] Lei Deng, Guoqi Li, Song Han, Luping Shi, and Yuan Xie. Model
compression and hardware acceleration for neural networks: A compre-
hensive survey. Proceedings of the IEEE, 108(4):485–532, 2020.
[7] Ulrich Gallersd¨orfer, Lena Klaaßen, and Christian Stoll. Energy con-
sumption of cryptocurrencies beyond bitcoin. Joule, 4(9):1843–1846,
2020.

[8] Mohamed Haouari, Mariem Mhiri, Mazen El-Masri, and Karim Al-Yaﬁ.
A novel proof of useful work for a blockchain storing transportation
Information Processing & Management, 59(1):102749,
transactions.
2022.

[9] Pentti Kanerva. Hyperdimensional computing: An introduction to
computing in distributed representation with high-dimensional random
vectors. Cognitive computation, 1(2):139–159, 2009.

[10] Yeseong Kim, Mohsen Imani, Niema Moshiri, and Tajana Rosing. Ge-
niehd: Efﬁcient dna pattern matching accelerator using hyperdimensional
computing. In 2020 Design, Automation & Test in Europe Conference
& Exhibition (DATE), pages 115–120. IEEE, 2020.

024681012141618#nonce83.584.084.585.085.586.086.587.0accuracy (%)max acc024681012141618#nonce91.992.092.192.292.392.4accuracy (%)max acc024681012141618#nonce89.489.689.890.090.290.490.6accuracy (%)max acc[11] Angelique Faye Loe and Elizabeth A Quaglia. Conquering generals: an
np-hard proof of useful work. In Proceedings of the 1st Workshop on
Cryptocurrencies and Blockchains for Distributed Systems, pages 54–59,
2018.

[12] Dongning Ma, Rahul Thapa, and Xun Jiao. Molehd: Drug discov-
ery using brain-inspired hyperdimensional computing. arXiv preprint
arXiv:2106.02894, 2021.

[13] Satoshi Nakamoto. Bitcoin: A peer-to-peer electronic cash system.

Decentralized Business Review, page 21260, 2008.

[14] Peer Neubert, Stefan Schubert, and Peter Protzel. An introduction to
hyperdimensional computing for robotics. KI-K¨unstliche Intelligenz,
33(4):319–330, 2019.

[15] Abbas Rahimi, Pentti Kanerva, and Jan M Rabaey. A robust and energy-
efﬁcient classiﬁer using brain-inspired hyperdimensional computing.
In Proceedings of the 2016 International Symposium on Low Power
Electronics and Design, pages 64–69, 2016.

[16] Michael Szydlo. Merkle tree traversal in log space and time. In Inter-
national Conference on the Theory and Applications of Cryptographic
Techniques, pages 541–554. Springer, 2004.

[17] Rahul Thapa, Bikal Lamichhane, Dongning Ma, and Xun Jiao. Spamhd:
Memory-efﬁcient text spam detection using brain-inspired hyperdimen-
sional computing. In 2021 IEEE Computer Society Annual Symposium
on VLSI (ISVLSI), pages 84–89. IEEE, 2021.

[18] Gavin Wood et al.

Ethereum: A secure decentralised generalised
transaction ledger. Ethereum project yellow paper, 151(2014):1–32,
2014.

