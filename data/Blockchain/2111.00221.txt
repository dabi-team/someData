Chaos Engineering of Ethereum Blockchain Clients

Long Zhang1, Javier Ron1, Benoit Baudry1, and Martin Monperrus1

1KTH Royal Institute of Technology, Sweden

1
2
0
2

t
c
O
0
3

]
E
S
.
s
c
[

1
v
1
2
2
0
0
.
1
1
1
2
:
v
i
X
r
a

Abstract—The Ethereum blockchain is the operational back-
bone of major decentralized ﬁnance platforms. As such, it is
expected to be exceptionally reliable. In this paper, we present
CHAOSETH, a chaos engineering tool for resilience assessment of
Ethereum clients. CHAOSETH operates in the following manner:
First, it monitors Ethereum clients to determine their normal
behavior. Then, it injects system call invocation errors into the
Ethereum clients and observes the resulting behavior under per-
turbation. Finally, CHAOSETH compares the behavior recorded
before, during, and after perturbation to assess the impact of
the injected system call invocation errors. The experiments are
performed on the two most popular Ethereum client implementa-
tions: GoEthereum and OpenEthereum. We experiment with 22
different types of system call invocation errors. We assess their
impact on the Ethereum clients with respect to 15 application-
level metrics. Our results reveal a broad spectrum of resilience
characteristics of Ethereum clients in the presence of system call
invocation errors, ranging from direct crashes to full resilience.
The experiments clearly demonstrate the feasibility of applying
chaos engineering principles to blockchains.

Index Terms—chaos engineering, Ethereum, fault injection,

resilience benchmarking

I. INTRODUCTION

Starting from the bitcoin system [39], blockchain techniques
have drawn much attention because of their strong trustfulness
and reliability characteristics [15], [49]. Ethereum is one of
the most popular blockchain platforms [17] that supports both
cryptocurrencies and decentralized ﬁnance [54]. In order to
join the Ethereum distributed network, participants run an
Ethereum client in their local environment. Then, the network
is composed by all clients interacting together. There are
multiple implementations of Ethereum clients [13], written in
different languages, all implementing the same speciﬁcation
and protocol [55].

As a decentralized system,

the resilience of the whole
Ethereum network results from the resilience of each partic-
ipating Ethereum client. It is essential to test and improve
the resilience of Ethereum client implementations. Ethereum
clients indeed have many reasons for sometimes malfunc-
tioning such as operating system overload, memory errors,
network partition, etc. While research on error-handling, re-
silience analysis and fault-injection is abundant [40], [57],
there is little work on assessing the resilience of blockchain
clients and Ethereum.

Chaos engineering is a novel methodology to assess and to
improve the error-handling mechanisms of software systems
[14]. To perform chaos engineering, developers actively inject
failures in the system in production, in a controlled manner.
The developers can then compare the behavior observed during

fault injection with the system’s steady state, which reﬂects
its normal behavior [41]. Similar behavior indicate resilience
to the inject faults, while discrepancies between both behavior
indicate resilience issues. In the context of an Ethereum client,
no one can test ofﬂine all the problems that an Ethereum
client will meet after it is deployed in production. The main
reason is that it is impossible to reproduce the actual Ethereum
blockchain evolution at scale. Chaos engineering is ﬁt
to
address this challenge, as it triggers an Ethereum client’s error-
handling code while the client is executing in a full-ﬂedged
production environment. The chaos engineering perturbations
are done in production, while the Ethereum client participating
in the blockchain consensus and validation process. Thus,
chaos engineering also contributes to analyzing the systemic
resilience of the distributed consensus.

In this paper, we present the design and implementation of
a novel chaos engineering methodology called CHAOSETH.
CHAOSETH intercepts an Ethereum client’s system call in-
vocations, actively injects error codes for these system call
invocations, and produces a resilience report directly from
production. CHAOSETH is applicable to any Ethereum client,
regardless of the programming language used. The resilience
assessment is based on advanced monitoring that captures the
steady state of an Ethereum client. As a result, CHAOSETH
is able to identify resilience strengths and weaknesses in
Ethereum clients.

CHAOSETH is evaluated by conducting chaos engineer-
ing experiments on GoEthereum v1.10.8 and OpenEthereum
v3.3.0-rc.8. During the experiments, CHAOSETH perturbs
GoEthereum and OpenEthereum while they interact with other
Ethereum nodes all over the world. The results show that
1) CHAOSETH successfully conducts chaos engineering ex-
periments using different error models, and 2) CHAOSETH
successfully identiﬁes different degrees of resilience with
respect to system call invocation errors in both clients. We also
present an original resilience benchmarking using 5 common
error models for a sound comparison of their resilience.

To the best of our knowledge, there is little research work
done on the reliability of blockchain systems [31]. Particularly,
no previous work has been done to research how chaos
engineering can help evaluate the resilience of blockchain
clients.

In summary, the main contributions of this paper are:
• A novel methodology to perform chaos engineering on
Ethereum clients for resilience assessment. To our knowl-
edge, we are the ﬁrst to explore chaos engineering for
blockchain systems. This assessment is done directly in

 
 
 
 
 
 
production to avoid biases from unrealistic fault injection
environments. The methodology is fully implemented in
a tool called CHAOSETH.

• An empirical evaluation of the resilience of GoEthereum
v1.10.8 and OpenEthereum v3.3.0-rc.8 in production,
with respect
the system
to realistic error models at
call level. The chaos engineering experiments highlight
strengths (including resilience) and weaknesses (includ-
ing crashes) for both clients, which are valuable for the
Ethereum community.

• An empirical and sound resilience benchmarking of
to 5
GoEthereum versus OpenEthereum with respect
common error models. We compare the differences of
error handling capabilities between these clients against
the same system call invocation errors, and show that no
client is consistently more resilient.

• An implementation of CHAOSETH that supports full
error injection for system call invocations in an Ethereum
client. The system is publicly available for future re-
search (https://github.com/KTH/royal-chaos/tree/master/
chaoseth).

The rest of the paper is structured as follows: Section II in-
troduces the background knowledge of blockchain techniques
and chaos engineering. Section III and Section IV present the
design and evaluation of CHAOSETH. Section V discusses the
threats to validity, experimental costs, and the applicability of
CHAOSETH to other Ethereum clients. Section VI presents
the related work, and Section VII concludes the paper.

II. BACKGROUND

A. Blockchain Technologies

Blockchain is the core technology behind the success of
cryptocurrencies such as Bitcoin [39]. Blockchain is a de-
centralized distributed ledger for recording information [56].
The information is saved in a sequence of “blocks” which
are shared by all of the participants on a blockchain network.
There are several key characteristics of a blockchain system:
1) the blocks are connected one-by-one like a chain, 2) the
chain is only appendable which means that no one can edit
the existing blocks in a blockchain, and 3) when a new block
is added into the blockchain, the majority of participants on
the network need to validate and conﬁrm the block following
a consensus algorithm.

In this work, we focus on Ethereum [55], an open-source
blockchain system that supports the execution of smart con-
tracts. A smart contract is a program that is persisted in the
Ethereum blockchain, and is executable by every user on
the Ethereum network. Smart contracts enable Ethereum to
support the transactions of the ETH cryptocurrency, as well
as to support so-called decentralized ﬁnance [5]. Ethereum
smart contracts are executable programs written in a speciﬁc
bytecode, executed by the Ethereum Virtual Machine (EVM).
There exist different Ethereum clients that
implement an
EVM in different programming languages. As introduced in
Ethereum’s ofﬁcial documentation [18], an Ethereum client is

an implementation of Ethereum that veriﬁes the blocks and
shares them with other peers over the Internet according to
the protocols deﬁned in the Ethereum Yellow Paper [55]. The
host that runs an Ethereum client is deﬁned as a “node”.

B. Chaos Engineering

Chaos engineering is a methodology that helps developers
learn about error-handling capabilities by actively triggering
failures in production [1]. For example, Netﬂix invented
ChaosMonkey, which randomly shuts down servers in produc-
tion to verify that the whole system is able to recover from
such a failure scenario [14]. Chaos engineering is a unique
complement to other software reliability techniques because it
is done after a system is deployed in production, where the
running environment is not as controlled as during testing or
staging [58].

The application of chaos engineering is based on a series of
steps [41]. First, the target system’s steady state needs to be
deﬁned. The steady state is a set of observable metrics which
characterize the system’s normal behavior. For instance, the
number of video streams opened by users per second could
be used as a steady state metric for a streaming application
[14]. After deﬁning the steady state, developers set hypotheses
that describe the expected behavior of the target system during
a chaos engineering experiment. In this context, a chaos
engineering experiment is an execution period during which
failures are injected into the target system. If a hypothesis
holds, it means that the target system behaves as expected
when a certain failure happens in production and developers
gain more conﬁdence in the system’s resilience capabilities.
Otherwise, if the hypothesis is falsiﬁed, actions need to be
taken based on the knowledge learned from the chaos engi-
neering experiment. Finally, as the experiment is performed
in production environments, developers have to limit the side
effects on user experience caused by the experiment, this is
also called “blast radius control”. For example, developers
could utilize containerization techniques to isolate the chaos
engineering experiment targets [43].

As many failure types exist, chaos engineering can be done
at different levels accordingly. For example, the injected faults
may be bit-ﬂip errors at the hardware level, timeouts at the
network level, and Java exceptions at the application level.
In this paper, we speciﬁcally focus on chaos engineering at
the system call level, which will be further elaborated in
Section III. A system call is a bridge between an application
and the kernel of an operating system [48]. System calls have
been shown to be valuable for capturing important information
about the behavior of programs [25]. When an application
needs to control some operating system resource such as
an I/O device, it has to interact with the kernel via system
call invocations. When a system call invocation succeeds, the
kernel returns the resource to the application. Otherwise, the
invocation returns an error code which indicates an error on
the kernel side.

Fig. 1: The components of CHAOSETH

III. DESIGN OF CHAOSETH

C. Overview

CHAOSETH assesses an Ethereum client’s resilience within
a real production environment. This section introduces the
motivation behind CHAOSETH, as well as its design and
implementation as follows.

A. Motivation

Regardless of how an Ethereum client is deployed, it has
to be executed on top of an operating system. The operating
system is responsible for providing the Ethereum client access
to critical resources such as network and storage, and it does
so by means of system calls. For example, during a short
1-minute observation period of the GoEthereum client, we
observed more than one million system calls (1, 128, 215) that
cover 36 different types such as read and write. Although
the client is behaving normally, the system call invocations are
not all successful: 14, 640 system call invocations fail during
the observation with 9 different kinds of error codes within the
same minute-long observation period. The fact that all those
natural errors do not crash the client shows that GoEthereum
is equipped with error-handling mechanisms. GoEthereum can
stay up and running even if some of the system call invocations
fail.

As a rule of thumb, both developers and users of Ethereum
clients seek robustness. CHAOSETH is meant to help them to
better understand and potentially improve this robustness.

B. Requirements

Based on the requirements mentioned in the previous sec-
tion, we design CHAOSETH as follows. CHAOSETH actively
injects errors at the level of system call invocations to trig-
ger different kinds of application-level errors. Fault injection
happens in production, while the Ethereum client is interacting
with other Ethereum nodes. As a key property, CHAOSETH is
independent of the programming language used by the client
and thus allow comparison.

CHAOSETH is designed with three components, and pro-
duces one resilience report, as summarized in Figure 1.
Ethereum is a distributed network with many nodes run-
ning together and achieving consensus on the blockchain.
CHAOSETH operates on one single node (signiﬁed by the
column titled Ethereum Node in Figure 1). In this speciﬁc
Ethereum node, CHAOSETH is attached to the client pro-
cess during its execution. The steady state analyzer collects
monitoring metrics and infers the system’s steady state. We
present this in more detail in Section III-D1. The system
call error injector injects different error codes into system
call invocations in a controlled manner, see Section III-D2.
With the help of the orchestrator, introduced in Section III-D3,
CHAOSETH systematically conducts chaos engineering exper-
iments. Finally, CHAOSETH produces a resilience report with
respect to system call errors for developers. We discuss the
resilience report in Section III-D4.

D. Components of CHAOSETH

In this section, we present the concrete design and imple-

mentation of CHAOSETH.

In order to achieve the goal of assessing the resilience
of Ethereum clients, the core requirements of CHAOSETH
include: 1) ﬁnding resilience problems related to production
environments, hence CHAOSETH should work on deployed
clients, 2) ﬁnding resilience problems related to real data,
hence CHAOSETH should operate with the real Ethereum
blockchain, and 3) being applicable to different implemen-
tations of Ethereum so that benchmarking [36] can be con-
ducted, for the community to identify the most robust clients.

1) Steady State Analyzer: As introduced in Section II-B,
deﬁning the steady state of the Ethereum client under study is
essential for a chaos engineering experiment. The steady state
analyzer collects resilience-related metrics directly from the
monitoring component provided by an Ethereum client. This
is because an Ethereum client is usually designed to have a
certain level of observability. For instance, the latest version of
the GoEthereum client exposes more than 500 different metrics
that describe the runtime status of the client. CHAOSETH

User spaceKernel spaceEthereum NodeCHAOSETH ComponentsCHAOSETH OutputApplication MetricsETH ClientSystem callsInvocationsError CodeClient's Steady StateSteady StateAnalyzerInjected ErrorsSystem CallError InjectorOrchestratorError ModelsProduction Resilience Report w.r.t. System Call ErrorsOther Nodes in aDistributed NetworkHypothesessteady state.

2) System Call Error Injector: In order to conduct chaos
engineering experiments, we need to inject chaos. In this
paper, we focus on system call errors and use a system call
error injector accordingly. When the injector is activated, it
listens to system call invocation events, and it replaces the
original return code with an error code. In CHAOSETH, the
error injector uses an error injection model to describe its
injection strategy. The error injection model is a triple (s,
e, r) that states that the system call s is injected with error
code e under the error rate r∈ [0, 1]. The error rate is a
likelihood that describes the probability of replacing the return
code by error code e when system call s is invoked during an
experiment. For example, an error injection model (read,
EAGAIN, 0.5) means that every time a read system call
is invoked, there is a 50% probability that a successful return
code is replaced with error code EAGAIN, which represents
that the resource being read is temporarily unavailable [47].

Considering that there are more than 300 unique types of
system calls and over 100 error codes in a modern Linux oper-
ating system [47], it is impractical to explore the combination
of all error models in our chaos engineering experiments. In
order to maintain high efﬁciency, CHAOSETH only focuses
on realistic error models, per previous work [59]. A realistic
error model means that the actual error s, e in this model
naturally happens in the production environment. But
the
original invocation error ratio may be very low because some
errors rarely happen in a normal execution. In CHAOSETH, we
consider realistic error models which preserve system call type
and error code as observed in the ﬁeld, but with an increased
error rate r, compared to what is observed in production.

Thanks to its system call error injector, CHAOSETH, can
perform ﬁne-grained chaos engineering experiments. There is
ﬂexibility of choosing which type of system call to perturb,
what error scenario to simulate, and how severe the error
scenario is, per developer preference. The higher the error rate
in an error injection model, the more frequently such errors
are injected into the target client, which means that the client
is subject to a higher resilience pressure.

3) Experiment Orchestrator: The experiment orchestrator
communicates with the previous two components to conduct
chaos engineering experiments. The orchestrator attaches the
steady state analyzer and the system call error injector to the
kernel before the Ethereum client starts. Then it activates the
error injector according to the experiment conﬁguration. An
experiment conﬁguration deﬁnes the duration of an experi-
ment, and the corresponding error injection model to be used.
The orchestrator divides each chaos engineering experiment
into ﬁve phases: 1) the warm-up phase, 2) the pre-checking
phase, 3) the error injection phase, 4) the recovery phase, and
5) the validation phase. During the warm-up phase, nothing
needs to be done by CHAOSETH. The target client is running
to reach its steady state naturally. During the pre-checking
phase, only the steady state analyzer is activated to monitor the
client and to check if the client has already reached its steady
state. During the error injection phase, both the steady state

(a) steady

(b) non-steady

Fig. 2: Steady state inference via comparison of Cumula-
tive Probability Functions of known_announcements and
known_broadcasts metrics.

reuses these pre-existing mechanisms, thereby also avoiding
extra monitoring overhead.

Developers can conﬁgure CHAOSETH by selecting an ap-
propriate subset of these metrics for steady state inference. The
principle behind selecting metrics is that a system call invoca-
tion error should have an impact on the metric. For instance,
the number of connected peers is a good metric because if
a system call invocation that is related to network operations
returns an error, the client might drop the connection with
its peer. On the contrary, the highest block number is not an
ideal metric for steady state inference because this metric is
determined by the whole blockchain network instead of the
client itself.

For all metrics, the monitoring is done per ‘monitoring
interval’, which is a period of time during which data is
collected and persisted. The monitoring interval
is set by
developers based on their requirements. The actual value of a
monitoring interval is an engineering trade-off for developers
between overhead and precision [59]. A shorter monitoring
interval may produce a better picture of the steady state, but
also means a larger amount of monitoring data and a higher
monitoring overhead.

Regarding the steady state inference task,

the analyzer
calculates the probability distribution of the values of each
metric and uses all the distributions to describe the steady
state from different perspectives. The distribution is computed
over a ‘monitoring epoch’. A monitoring epoch is deﬁned as a
sequence of contiguous monitoring intervals. If the client runs
normally, the longer a monitoring epoch is used for steady
state inference, the more accurate the results are. For example,
if the monitoring epoch is 3600 seconds and the monitoring
interval is 15 seconds, the analyzer uses 3600/15 = 240 data
points for each metric to analyze its steady state. Figure 2
depicts how monitoring epochs are compared for steady state
inference via probability distributions. Sub-ﬁgures (a) and (b)
show matching and non-matching probability distributions,
indicating that the analyzed metrics are respectively steady
and non-steady.

After inferring the client’s steady state, CHAOSETH can
conduct chaos engineering experiments, where the core idea is
to compare the state under error injection against the reference

24known_announcements / 15 sec.0.000.250.500.751.00Cumulative probabilityEpoch 1 CDFEpoch 2 CDF0.51.01.5known_broadcasts / 15 sec.0.000.250.500.751.00Cumulative probabilityEpoch 1 CDFEpoch 2 CDFanalyzer and the error injector are turned on so that system
call invocation errors are injected according to the given error
injection model. After the error injection phase is done, the
orchestrator gives the target client time to recover. After this
recovery phase, the validation phase takes place during which
the orchestrator monitors the client’s state again. The observed
behavior during this validation phase is compared with the
previously inferred steady state using the Mann–Whitney U
test. If the target client fails to recover back to its steady state
after the validation phase, it means that the injected errors have
caused an adverse side effect on the client. This can signify
to the developers that the client’s error handling mechanisms
may have failed with respect to the injected error.

4) Chaos Engineering Hypotheses: After a chaos engineer-
ing experiment, the orchestrator analyzes the client’s behavior
during the experiment and generates a resilience report for
developers. The report presents to what extent the implemen-
tation of the Ethereum client under evaluation is resilient to
the injected system call invocation errors. As introduced in
Section II-B, the client’s resilience is assessed by validating
chaos engineering hypotheses. CHAOSETH concerns two hy-
potheses about how a client reacts when errors are injected
using one speciﬁc error model.

a) Non-Crash hypothesis (HN ): The non-crash re-
silience hypothesis holds if the injected system call invocation
errors do not crash the Ethereum client and the process remains
alive.

b) Observability hypothesis (HO): The observability hy-
pothesis for a metric m holds if m is inﬂuenced by error
injections according to an error model.

For example,

c) Recovery hypothesis (HR): The recovery hypothesis
is valid if the client is able to recover to its steady state after
stopping the injection of system call invocation errors.
let us assume that

the system call error
injector follows the error model (read, EAGAIN, 0.5)
to conduct the experiment. The target client does not crash
during fault injection. However, a metric that records the rate
of read operations drops signiﬁcantly during fault injection.
Furthermore, the client is able to reach its steady state again
after the experiment stops. In this case, CHAOSETH reports
that all of the three hypotheses HN , HO and HR are validated,
meaning that the target client is resilient to the error model
(read, EAGAIN, 0.5). By validating or falsifying the
hypotheses above, developers learn more about the Ethereum
client’s resilience with respect to different types of system
call invocation errors. Such information is also helpful for
developers in prioritizing their work on improving the client’s
error handling capabilities.

In a resilience report, the best case scenario for developers is
that all the three hypotheses are validated. This shows that the
client is fully resilient with respect to the error injection model.
Such cases bring conﬁdence to the developers that the client
behaves correctly even when a speciﬁc error happens. On the
contrary, the worst case scenario is that all the hypotheses are
falsiﬁed. The target client crashes directly and does not restart
automatically when an error is injected. The worst case should

be given the highest priority by developers, as it causes the
most harmful impact on the client.

5) Implementation: The system call monitor and system
call error injector are reused from a tool called Phoebe, which
is a fault injection framework for system call invocation errors
[59]. The monitor and injector are implemented with the help
of the eBPF module (extended Berkeley Packet Filter) which is
a Linux kernel module for monitoring [46]. More speciﬁcally,
the monitor and error injector register their BPF programs to
the sys_enter and sys_exit events. When there is an
error that needs to be injected, the error injector calls the
BPF helper function bpf_override_return to replace
the original return code with the error code. The experiment
orchestrator is implemented in Python. The source code of
CHAOSETH is publicly-available at https://github.com/KTH/
royal-chaos/tree/master/chaoseth.

IV. EXPERIMENTATION

A. Overview

In order to evaluate the effectiveness of CHAOSETH, we

propose the following three research questions.

• RQ1: What is the feasibility of steady state characteriza-

tion with CHAOSETH for Ethereum clients?

• RQ2: What are CHAOSETH’s resilience properties actu-

ally veriﬁed in the Ethereum clients under study?

• RQ3: What are the resilience differences among the
considered Ethereum clients with respect to system call
errors?

All of

the research questions are answered quantita-
tively by conducting experiments on the selected Ethereum
client implementations. The selected Ethereum clients include
GoEthereum v1.10.8 and OpenEthereum v3.3.0-rc.8 which
will be presented in Section IV-B. We present the following
three categories of experiments: steady state analysis exper-
iments to address RQ1, chaos engineering experiments to
address RQ2, and resilience benchmarking experiments to
address RQ3. These will be described in detail in Section IV-C.
Figure 3 gives an overall introduction to the input and output
of each experiment.

B. Selection of Ethereum Clients

To maximize the relevance and impact of our research, we
aim at selecting the most popular Ethereum clients. According
information of Ethereum’s main network
to the statistical
to date, 62% of the clients
published at ethernodes.org,
are GoEthereum, which is the ofﬁcial implementation of
Ethereum using the Go programming language. The second
most popular Ethereum client is openethereum, which is
implemented in Rust and accounts for 28% of all the clients.
The descriptive statistics of these three clients including the
number of commits, contributors, merged pull requests in
2021, the programming language, and the selected version
are presented in Table I. The last row in Table I records
the application-level metrics that are selected for steady state
analysis as introduced in Section III-D1.

Fig. 3: Experimental Workﬂow

TABLE I: Description of The Selected Ethereum Clients

Description

GoEthereum OpenEthereum

Commits
Contributors
Merged PR in 2021
Language
Version
Lines of code

App-level observations

12.9K
648
593
Go
v1.10.8
368.8K

connected peers
disk read
disk write
announcements
broadcasts
memory used
trafﬁc ingress
trafﬁc egress

12.2K
202
94
Rust
v3.3.0-rc.8
134.8K

connected peers
kvdb read
kvdb write
blocks received
blocks highest
cache size
blocks imported

C. Experiment Protocol

To assess the resilience of the two considered Ethereum
clients, we conduct three different categories of experiments:
steady state analysis experiments, chaos engineering experi-
ments, both performed individually on each client; and re-
silience benchmarking experiments, performed collectively on
both of the clients.

1) Steady State Analysis Experiment: To answer RQ1, we
conduct a steady state analysis experiment on each client.
As introduced in Section II-B, characterizing an Ethereum
client’s steady state is a prerequisite for a chaos engineering
experiment. Conducting steady state analysis gives developers
a clear understanding of how the Ethereum client under
analysis behaves in a normal production environment.

active synchronization mode, which consists in downloading
and verifying the blocks from other peers on the Internet.
After starting up the client, we wait for the client to ﬁnish
synchronizing the existing blocks with the peers ﬁrst.

After a client reaches a synced state,

the main job of
the client is to verify newly generated blocks and to share
all the blocks with other peers. From this point, the steady
state analyzer is attached to the client, with a monitoring
interval of 15 seconds and a monitoring epoch for steady state
inference of 5 hours, per our conceptual framework detailed in
Section III-D1. These values ensure that a sufﬁcient number
of data points (5 × 60 × 60 ÷ 15 = 1200 points) are available
for each metric.

The output of a steady state analysis experiment is a set
of probability distributions related to the selected metrics in
Table I. This steady state will be used during the validation
phase in the subsequent chaos engineering experiment. In
order to check if the selected metrics are statistically stable, the
steady state analyzer monitors the client for two monitoring
epochs in total. The distribution of every metric per monitoring
epoch is compared with each other using the Mann Whitney
U test. The stable metrics are those whose distributions are
not statically different. They are selected for describing the
client’s steady state.

2) Chaos Engineering Experiment: To answer RQ2, we
conduct chaos engineering experiments on each client. During
a chaos engineering experiment, all of the components in
CHAOSETH are activated. The system call error injector
intercepts a speciﬁc type of system call invocation and replaces
the successful return code with an error code.

The ﬁrst row of Figure 3 illustrates our methodology to
model the steady state. Each Ethereum client (GoEthereum or
OpenEthereum), is started up using the recommended options
according to their documentation. The clients are started in

To perform the chaos engineering experiment, the results
of the steady state analysis experiment are required because
the inferred steady state is used during the validation phase
as mentioned in Section III-D3. The duration of the warm-up

Steady State Analysis Experiment (RQ1)Ethereum Client XDeployed in productionFully synced with peersMulti-layer monitoringtimelineSteady State of Ethereum Client XChaos Engineering Experiment (RQ2)Ethereum Client XtimelineWarm-upPre-checkErr. injectionRecoveryValidationResilience Benchmarking Experiment (RQ3)Resilience report ofEthereum Client XEthereum Client GEthereum Client OChaos Engineering ExperimenttimelineEach client's resilience reportComparisonBenchmarking reportCommon error modelfor Ethereum clients G, and OMonitored behaviorError Model for Ethereum Client XTABLE II: Steady State Analysis Experiment Results (The
colors blue and red in an evolution chart stand for the
data captured in the two different monitoring epochs. The
metrics that are struck out are excluded for further experiments
because of their instability.)

Client

Metric Name

Trend

GoEthereum

OpenEthereum

disk read
memory used
disk write
known announcements
known broadcasts
peer stats
trafﬁc egress
trafﬁc ingress

import blocks
kvdb write
sync highest
sync blocks received
blockchaincache size
kvdb read
peer stats

p-value

0.03
0.10
< 0.01
< 0.01
< 0.01
< 0.01
< 0.01
< 0.01

0.26
0.25
0.09
0.09
< 0.01
< 0.01
< 0.01

phase is 2 hours. The last four phases of a chaos engineering
experiment in Figure 3 are set to 5 minutes for each phase.

3) Resilience Benchmarking Experiment: RQ3 is about
resilience benchmarking experiments, which aim at providing
insights for end-users who wish to choose a suitable Ethereum
client, based on resilience requirements. Our benchmarking ex-
periments indicate which client has a better resilience against
some system call invocation errors. For a resilience bench-
marking experiment, we deﬁne a single, common error model
for all of the target clients in order to have a sound comparison.
A common error model (s, e, rm) is one error type (s,
e, rc) that has been observed on each client in isolation.
The common error rate rm is deﬁned as the maximum value
of rc. The last row in Figure 3 illustrates our procedure
for resilience benchmarking. Once both of the clients reach
their steady state, CHAOSETH conducts chaos engineering
experiments using the common error model on each client.
A cross-comparison is made among each client’s resilience
report that is generated in the chaos engineering experiment.
For example, CHAOSETH injects an ENOMEM (insufﬁcient
memory) error into a read system call invocation. If client A
directly crashes while client B continues to run and conducts
retries after this error is injected, client B is more resilient
with respect to the injected type of errors.

D. Experiment Results

1) Steady State Analysis Experiment Results: We perform
a steady state analysis experiment on each selected Ethereum
is
client per the protocol of Section IV-C1. Every client
observed for two monitoring epochs of 5 hours, amounting
to 10 hours in total. Within each epoch, the metrics of interest
are recorded and aggregated for every 15-second monitoring
interval.

In the case of GoEthereum, we consider 8 moni-
the bytes read from and writ-
the num-

toring metrics, namely,
ten into the disk disk_read, disk_write,

ber of announced blocks known_announcements,
the
number of broadcasted blocks known_broadcasts,
the
memory used by the client memory_used,
the average
number of connected peers peer_stats,
the amount
of data shared on (egress) and coming from (ingress)
the network traffic_egress, traffic_ingress. For
OpenEthereum, we consider 7 monitoring metrics: the size
of the local cache blockchaincache_size,
the num-
ber of imported blocks import_blocks, the total amount
of data read from and written to the key-value database
kvdb_read, kvdb_write, the number of connected peers
peer_stats, the highest number of all kinds of blocks
sync_highest, and the highest number of the received
blocks sync_blocks_received.

In Table II, we display the evolution of all the metrics
during the experiment. The ﬁrst half of each evolution chart (in
blue) is based on the data gathered during the ﬁrst monitoring
epoch. The second half of the chart (in red) is drawn based
on the data of the second monitoring epoch. The last column
indicates the p-values, with the Mann–Whitney U test, when
testing for the similarity between the two distributions. For
example, the ﬁrst row in Table II shows that the read operations
made by the GoEthereum client regularly have spikes during
these two monitoring epochs. The comparison of these two
plots yields a p-value of 0.03. The null hypothesis set by the
Mann–Whitney U test is that the two samples under com-
parison are not statistically distinguishable: if the conﬁdence
level is greater than 0.01, the null hypothesis is not rejected.
In other words, the test gives statistical guarantees about the
similarity between the evolution of metric disk_read the
two monitoring epochs. Another example that shows the null
hypothesis is rejected given the conﬁdence level 0.01 is metric
blockchaincache_size in OpenEthereum client. From
Table II the line chart in the third-to-last row also visually
conﬁrms that the metric does not evolve in the same way
during the two monitoring epochs. Considering the conﬁdence
level of 0.01, some of the metrics are not stable enough to
describe a client’s steady state and thus are excluded for further
experiments. These metrics are struck out in Table II.

This steady state analysis experiment shows that not all
the monitoring metrics provided by an Ethereum client are
suitable to describe the client’s steady state in a statically
valid manner. Since the experiments are done in production,
there are several factors that could affect a metric’s stability.
First, the node itself is not always stable. For instance, there
exist other applications that take more resources of the node.
Second, the network of the infrastructure is not stable. It is
possible that the node encounters network scans or attacks
every now and then [17]. Lastly, the behaviors of peers are
different. For example, when the node randomly connects with
some new peers who have different characters, a metric might
be inﬂuenced.

Answer to RQ1

The results of steady state analysis experiments show that
CHAOSETH is able to successfully identify monitoring
metrics that are stable in production. 2 out of 8 metrics in
GoEthereum, and 4 out of 7 metrics in OpenEthereum are
selected for describing the client’s steady state respectively
for further chaos engineering experiments and resilience
benchmarking experiments.

2) Chaos Engineering Experiment Results: During the ex-
periment for RQ1, we also know that GoEthereum runs with
10 different types of system calls, accumulating more than
138 million invocations in a 10-hour production run (two
monitoring epochs). Interestingly, none of the types of system
call invocations has a 100% success rate in the experiments.
We perform chaos engineering by increasing the error rates of
those system calls in production. The error rate ampliﬁcation
approach described in Section III-D2 and [59], produces 14
and 13 error models respectively for the GoEthereum client
and the OpenEthereum client, based on the clients’ behavior
during steady state analysis. For each error model, we make
a chaos engineering experiment (i.e. there is a one-to-one
mapping).

Table III describes the error models together with the chaos
engineering experiments of the selected clients. Every row
presents one error injection model, including the target system
call invocation signiﬁed by the column Syscall, the error code
to be injected signiﬁed by E.C., and the error rate signiﬁed
by E.R.. The last ﬁve columns record the corresponding
experiment result including the number of injected errors in
total (Inj.), and the results of whether the three hypotheses
(HN , HO, HR) are veriﬁed or falsiﬁed with respect to a
metric. The metrics that fail the pre-check phase are excluded
in the table. If CHAOSETH does not inject any error using
a speciﬁc error model due to the client does not invoke that
type of system call during the experiment, the corresponding
row is omitted in the table.

√

For the GoEthereum client, CHAOSETH conducts 12 chaos
engineering experiments. The results show that 5 out of 10
error models crash the GoEthereum client (the rows whose
HN column is marked with “
”). For the other 7 error models,
2 of them have a visible effect on the monitoring metrics (the
rows whose HO column is marked with “
”). The results
also show that the client is resilient against one error model,
(read, EAGAIN, 0.597). In this case, the error rate is
0.597, and is obtained by amplifying the natural error rate by
0.496, per the Phoebe algorithm [59]. Resilience against this
error model means that a visible effect is observed during the
error injection phase, and the metric gets back to the steady
state when CHAOSETH stops injecting errors.

√

Regarding the OpenEthereum client, there are 10 chaos
engineering experiments in total (second half of Table III).
The results show that only one error model, (read,
EAGAIN, 0.558),
to a
crash. Five error models cause a visible effect on at least

leads the OpenEthereum client

TABLE III: Chaos Engineering Experimental Results on the
Major Ethereum Clients

Inj. Metric

C

Syscall

E. C.

accept4
epoll ctl
epoll pwait

EAGAIN
EPERM
EINTR

G

futex
futex
newfstatat

read
read
recvfrom
write
write
write

EAGAIN
ETIME.
ENOENT

EAGAIN
ECONN.
EAGAIN
EAGAIN
ECONN.
EPIPE

E. R.

0.6
0.171
0.05

0.05
0.05
0.24

0.597
0.05
0.6
0.139
0.05
0.05

accept4

EAGAIN

0.6

476
147
3421

14
2
6

139
39
678
12
5
1

93

futex
futex
ioctl

read
readv

EAGAIN
ETIME.
ENOTTY

0.05
0.05
0.05

4
97
17211

EAGAIN
EAGAIN

0.558
0.6

2687
52

O

recvfrom

EAGAIN

0.252

7402

recvfrom

ECONN.

0.05

1382

sendto

EAFNO.

0.05

763

sendto

EPIPE

0.05

202

memory used
memory used
disk read
memory used
-
-
disk read
memory used
memory used
memory used
memory used
-
-
-

import blocks
kvdb write
sync highest
kvdb write
sync highest
import blocks
kvdb write
sync highest
sync recieved
-
import blocks
kvdb write
sync highest
sync recieved
import blocks
kvdb write
sync highest
sync recieved
import blocks
kvdb write
sync highest
sync recieved
import blocks
kvdb write
sync highest
sync recieved
import blocks
kvdb write

HN HO HR
√
√
√
√

X -
X -
X -
X -
-
-
X
X -

X -
X -
√ √
√
√ √ √
√
√

X -
X -
-
-
-

X -
X -
X -
√
√
√
√ √
√
√ √ √
√
√
√

X -
X -
X -
X
X -

X -
X -
X -
-
X -
√
X -
√
X -
√
X -
√
X -
√ √
X
√ √ √
√ √ √
√ √ √
√ √ √
√ √ √
√ √
X
√ √ √
√
√
√
√
√ √
√ √

X -
X -
X -
X -
X
X

√
√
√

’ if the injected errors crash the client, otherwise ‘X’.
’ if the injected errors have visible effect on the metric, otherwise ‘X’.
’ if the metric matches its steady state during the validation phase, otherwise

HN: ‘
HO: ‘
HR: ‘
‘X’.
If a hypothesis is left to be untested, it is marked as ‘-’.
There are two metrics selected for GoEthereum, namely disk read, and mem-
ory used. There are four metrics selected for OpenEthereum, namely kvdb write,
sync recieved, sync highest, and import blocks.

one metric during the error injection phase. The error mod-
els (recvfrom, EAGAIN, 0.252) and (recvfrom,
ECONNRESET, 0.05), have an impact on all of the four
selected metrics during the error injection phase. According
to the results, OpenEthereum is resilient to one error model,
which is (ioctl, ENOTTY, 0.05) with respect to metric
import_blocks.

This experiment has ﬁve main outcomes, with different

meanings for the Ethereum developers.

a) Crash (HN =X): The client directly crashes because
of the injected errors. This is considered as a severe case: this
means that an Ethereum node disappears from the distributed
consensus and validation process. As the client crashes, the
hypotheses HO and HR are left to be untested and are marked
as ‘-’ in Table III. For example, CHAOSETH detects that
the GoEthereum client directly crashes when an EAGAIN
error code is injected on an invocation to the system call

1 ...
2 INFO Imported new block headers count=1 elapsed=11.648ms

number=13,389,783 hash=cb66be..2aca64

3 INFO State heal in progress accounts=40848@2.33MiB

slots=58892@4.41MiB codes=13426@114.16MiB
nodes=321,569,325@94.88GiB pending=165,847
4 CRIT Failed to persist healing data err="write

/data/eth-data/geth/chaindata/1321635.log: resource
temporarily unavailable"

Listing 1: Error information found in the GoEthereum client’s
logs when error model (write, EAGAIN, 0.139) is
activated.

√

write. Since error code EAGAIN in Linux means that the
target resource is temporarily unavailable, crashing is an over-
reaction, the client should consider implementing a classical
retry mechanism instead of crashing directly. Some crashes
they leave debug
are better than others in the sense that
information for the Ethereum developers to understand what
happens. For example, GoEthereum logs an error message
before the client crashes because of a failure to persist data.
Listing 1 shows this log, in the critical message at the last
line. To sum up, CHAOSETH is able to identify cases where
an Ethereum client crashes while proper recovery would be
preferable.

b) Invisible effect (HN =

and HO=X): In some cases,
there is no visible effect detected during the error injection
phase. For example,
the OpenEthereum chaos experiment
using error model (readv, EAGAIN, 0.6) reveals such
a situation. In this experiment, CHAOSETH injects 52 system
call invocation errors to system call readv. During this error
injection phase, none of the four metrics have an abnormal
behavior. This indicates that the OpenEthereum client seems
to be functioning normally when a system call invocation
to readv returns an EAGAIN error code, which is good.
However, we cannot exclude that the client state is corrupted
in an invisible manner, because we do not have a prov-
ably perfect steady state oracle. Since CHAOSETH does not
capture anything abnormal during the error injection phase,
the veriﬁcation of hypothesis HR is skipped. Overall, the
presence of such invisible effect cases is good with respect to
consistency: if we would not perform steady state pre-checking
and observability hypothesis checking, developers may falsely
believe that the client state is valid according to the monitored
metrics.

c) Long-term effect (HN =

, HO=

and HR=X):

For some of the error models, the client under experiment
does not crash. However, during the error injection phase,
some metrics deviate from their steady state, and do not
recover after the given recovery phase. For instance, the ex-
periment result of error model (sendto, EPIPE, 0.05)
in the OpenEthereum client belongs to this category. Dur-
ing the pre-check phase, the metrics import_blocks and
kvdb_write match the steady state, and are thus kept for
the subsequent phases. When the errors are injected, both
metrics vary signiﬁcantly. Even after the error injection stops
and the recovery phase has passed, these two metrics are still
abnormal compared to the steady state. This means that either

√

√

√

√

√

it takes a longer time for the client to recover from the injected
errors, or that the injected errors lead the client to a stalled or
corrupted state. Overall, such cases show that CHAOSETH
gives Ethereum developers insights about
the timespan of
recovery for the clients.

, HO=

and HR=

d) Resilient case (HN =

): Certain
error models do not crash the client and also cause visi-
ble effects on the metrics. After the error injection stops,
the monitoring metrics recover to their steady state. This
indicates that the target client is equipped with a graceful
error-handling mechanism that brings the client back to nor-
mal after errors. For example, during the chaos engineering
experiment using error model (read, EAGAIN, 0.597)
in the GoEthereum client, the injected errors do not crash
the client, thus the HN hypothesis holds. During the error
injection phase, the metric memory_used no longer matches
the steady state. When the error injection stops, the client’s be-
havior related to memory usage is restored during the recovery
phase. During the validation phase, CHAOSETH checks the
metric again and conﬁrms that memory_used has recovered
to its steady state. By looking at the client logs, we indeed
conﬁrm that the client has resumed downloading, sharing and
verifying Ethereum blocks. Overall, CHAOSETH increases the
trust that developers have about speciﬁc resilience scenarios.

Answer to RQ2

CHAOSETH successfully conducts 12 and 10 different chaos
engineering experiments respectively on the GoEthereum
and the OpenEthereum clients,
in production. This ex-
perimental campaign shows that the clients have different
degrees of resilience with respect to system call invocation
errors, which will be further discussed in Section IV-D3.
CHAOSETH demonstrates that the clients crash under errors
that are recoverable in theory. Since clients may crash
concomitantly, this is a threat to the consensus and resilience
properties of the Ethereum blockchain network from a sys-
temic perspective. CHAOSETH gives valuable insights about
resilience structured around well-deﬁned chaos engineering
hypotheses.

3) Benchmarking Ethereum Clients: We cannot strictly
compare the considered clients in RQ2, because the error
models are different. To overcome this, we have introduced
in Section IV-C3 the idea of testing the clients under a
meaningful common error model. CHAOSETH identiﬁes 5
common error models for the selected clients. The results of
this resilience benchmarking experiment are summarized in
Table IV. Each row in the table presents the veriﬁcation of the
three hypotheses for both clients, according to a monitoring
metric. Only the metrics that pass the pre-check phase are
presented in the table. This table is interesting in the following
three aspects.

First of all, regarding the HN hypothesis (absence of
crash),
the results show that both the GoEthereum client
and the OpenEthereum client are crashed by different spe-
is crashed by
ciﬁc error models. The GoEthereum client

TABLE IV: Resilience Benchmarking Experiment Results (The metrics that do not pass the pre-check are excluded)

Common Error Model

GoEthereum

Syscall

Error Code

Error Rate

Injection Count Metric

accept4

EAGAIN

0.6

501

disk read

futex

EAGAIN

futex

ETIMEDOUT

read

EAGAIN

recvfrom

EAGAIN

0.05

0.05

0.597

0.6

2

3

-

-

184

disk read
memory used

619

memory used

HN
√

HO

X

X

X

√
√

√

-

-

√
√

X

HR

Injection Count Metric

OpenEthereum

-

-

-

√

X

-

459

176

12

import blocks
kvdb write
sync highest
sync recieved

import blocks
kvdb write

import blocks
kvdb write
sync highest
sync recieved

7930

-

5402

import blocks
kvdb write

HN
√
√
√
√

√
√

√
√
√
√

X

√
√

HO

HR

X
X
X
X

X
X

X
X
X
X

-

√
√

-
-
-
-

-
-

-
-
-
-

-

X
√

In contrast,

lead the OpenEthereum client

futex system call invocation errors including error codes
EAGAIN and ETIMEDOUT.
these errors do
to a crash. However,
not
the OpenEthereum client crashes with error model (read,
EAGAIN, 0.597), after handling 7930 injected errors.
Given the same error model, the GoEthereum client does
not produce any crash. Overall, there is no client which is
absolutely more robust than the other with respect to crashing.

Second, focusing on the HO hypothesis (observability),
both metrics in the GoEthereum client (disk_read and
memory_used) deviate from their steady state when er-
ror model (read, EAGAIN, 0.597) is applied. This
is evidence that
the metrics capture the client’s internal
state. For all the experiments on the OpenEthereum client,
CHAOSETH never observes any visible effect on 2 metrics
(sync_highest and sync_received), suggesting that
they are not useful. For the other 2 metrics, when error
model (recvfrom, EAGAIN, 0.6) is used, CHAOSETH
conﬁrms that the injected errors cause a visible effect on both
import_blocks and kvdb_write. This shows that fault
injection is great to deepen our understanding of the quality
of monitoring.

Third, considering the HR hypothesis, CHAOSETH suc-
cessfully identiﬁes resilient cases for the two Ethereum clients.
CHAOSETH shows that the GoEthereum client is resilient
to error model (read, EAGAIN, 0.597) with respect
to metric disk_read, while OpenEthereum crashes with
the same model. Now consider error model (recvfrom,
EAGAIN, 0.6), there is no crash in any client, yet the
resilience characteristics are different. On the one hand,
resilience cannot be proven for GoEthereum because HO
is falsiﬁed. On the other hand, for OpenEthereum, metric
kvdb_write veriﬁes HR while import_blocks, does
not, which prevents us from drawing a clear-cut conclusion
about resilience. As opposed to toy examples with perfect
oracles, assessing behavior of real-world software through
monitoring yields multiple shades of resilience.

Answer to RQ3

CHAOSETH identiﬁes 5 common error models for resilience
benchmarking. The results show that neither of the clients
is consistently more resilient than the other. For futex
system call invocation errors, the GoEthereum client crashes
directly. When injecting EAGAIN errors in system call
invocations to read, the OpenEthereum client crashes. In
both cases, given the nature of the injected errors (EAGAIN
and ETIMEDOUT), the clients could implement a retry mech-
anism for better resilience. The fact that there is no common
crash mode is good news from a systemic perspective: since
the network uses both clients in production, the risk that the
Ethereum blockchain network suffers from a single software
problem is low. This is well aligned with a recommendation
from the Ethereum community: encouraging a diversity of
Ethereum clients [3].

A. Threats to Validity

V. DISCUSSION

During a steady state analysis experiment, a set of sta-
tistically stable metrics needs to be identiﬁed. As a threat
to the internal validity, the selection criterion is based on
a Mann–Whitney U test. The threshold of p-value, or ap-
plying another statistical comparison methods would impact
the selection result and the outcome of the subsequent chaos
engineering experiment.

A threat

in the chaos
to the construct validity is that,
engineering experiment, errors are injected by replacing a
successful return code of a system call with an error return
code according to a pre-deﬁned error model. However, the
system call itself is still executed, potentially modifying the
is possible that
program’s state as originally intended. It
abnormal behavior observed under error injection is due to the
inconsistency between the returned error code and the internal
state of the program, and not to the returned error code alone.

B. Experimental Costs

As introduced in Section IV-C, CHAOSETH waits for an
Ethereum client to be fully synchronized with other peers on

the network. This is because the synchronization of existing
blocks only happens at the beginning of an Ethereum client’s
life cycle, while CHAOSETH aims at evaluating a client’s
resilience in its most common use case. This on the other
hand brings some extra experimental costs related to time
and computational resources. For clients GoEthereum and
OpenEthereum, it takes around 4 days for the clients to be
fully synced using a standard D2s v3 (2 vCPUs, 8 GiB RAM)
instance on Microsoft Azure. Both of these two clients require
at least 450 GB disk storage to save the existing blocks in their
default synchronization modes.

C. Applicability to Other Ethereum Clients

As CHAOSETH does chaos engineering experiments at the
system call invocation level, theoretically any Ethereum client
implementation that runs on top of a Linux operating system
can be evaluated by CHAOSETH. As long as the operating
system has the eBPF module installed, CHAOSETH is able to
monitor the system call invocations and inject speciﬁc errors.
No speciﬁc changes (in conﬁguration, for instance) of an
Ethereum client are required for attaching CHAOSETH to the
client.

VI. RELATED WORK

A. Blockchain Dependability

There are several surveys that focus on different directions
of research on blockchain systems. For example, Huang et al.
[31] conducted a survey of the state-of-the-art on blockchains
including the theories, modelings, and tools. Fan et al. [24]
presented a survey that focuses on performance evaluation of
blockchain systems. Chen et al. [17] and Want et al. [53]
conducted surveys on Ethereum systems security including
vulnerabilities, attacks, defenses, and future research oppor-
tunities. Atzei et al. [12] also conducted a survey speciﬁcally
on the attacks on Ethereum smart contracts. Based on these
surveys, we notice that there is limited amount of research on
using fault injection techniques to evaluate the reliability of
blockchain systems. Most of the recent works on this direction
is mainly about fuzzing smart contracts rather than perturbing
the runtime environment of an Ethereum node, as is done by
CHAOSETH.

levels,

Regarding performance, the existing research works focus
on different
including EVM opcode level [5], [7],
smart contract level [6], consensus algorithms level [4], [29],
blockchain system level [9], [10], [20]–[22], and blockchain-
based application level [51]. Compared with these works,
CHAOSETH does not focus on performance evaluation of
blockchain systems. Instead, performance-related metrics such
as memory usage are used by CHAOSETH to evaluate the
side effects caused by system call
invocation errors. The
most related work is done by Dinh et al. [22] who invented
BlockBench, a framework that evaluates a private blockchain’s
latency, scalability, and
performance using its throughput,
fault-tolerance capability as indicators. BlockBench investi-
gates how Byzantine failures affect a blockchain system’s
injecting
throughput and latency by crashing some nodes,

network delays, and corrupting messages among different
nodes. Compared with BlockBench, CHAOSETH assesses a
blockchain client using a public blockchain network, which
means developers do not need to have the full control of
the whole decentralized system. The failure models are also
different between these two works.

Regarding reliability analysis and improvement, Seol et al.
[42] implemented a blockchain analytics engine for assessing
the dependability of off-chain ﬁle systems. Sousa et al. [45]
designed a Byzantine fault-tolerant ordering service for Hy-
perledger Fabric. Zhang et al. [61] designed LedgerGuard, a
mechanism that keeps the integrity of a ledger via corrupted
blocks detection and recovery. Liu et al. [37] proposed an
evaluation methodology that applies a continuous-time Markov
chain model for blockchain-based IoT applications. It has
been proposed to use modeling techniques to study blockchain
reliability. For instance, Melo et al. [38] proposed a mod-
eling methodology that evaluates reliability and availability
of a blockchain-as-a-service environment. Kancharla et al.
[34], [35] applied simulation methods to demonstrate the
dependability of the proposed hybrid blockchain and slim
blockchain. Gonz´alez et al. [27] categorized different fault
injection techniques and discussed the possibilities to apply
them in blockchain-based applications resilience assessment.
Instead of using simulations or focusing on blockchain-based
applications, CHAOSETH uses real Internet trafﬁc to assess
the resilience of Ethereum clients in production.

Regarding security, Correas et al. [19] presented a static
proﬁling technique for Ethereum smart contracts that generate
upper-bound expressions that can be used to produce proﬁling
information. Alkhalifah et al. [8] discussed the root cause of
Ethereum reentrancy attacks and proposed a solution which
can detect, prevent, and identify the account address of an
attacker during the execution of a smart contract. Ashraf et al.
[11] presented GasFuzzer, a tool that fuzzes Ethereum smart
contract binaries to expose gas-oriented exception security vul-
nerabilities. Fu et al. [26] designed EVMFuzzer, a framework
that generates seed contracts via a set of predeﬁned mutators
to ﬁnd security bugs in different EVM implementations. Hajdu
et al. [28] proposed an approach that assesses the behavior of
permissioned blockchain systems by injecting faults into smart
contracts. Wang et al. [52] invented ContraMaster, an oracle-
supported fuzzing tool that detects exploitable vulnerabilities
in smart contracts. Zhang et al. [60] presented TxSpector,
a generic and logic-driven framework for detecting attacks
in Ethereum transactions at
the bytecode level. There is
also research about how to utilize the security feature of a
blockchain system to address other domain-speciﬁc problems.
For instance, Wu et al. [56] designed BlockEdge, a blockchain-
powered framework that delivers trusted collaborative edge
computing services. Du et al. [23] presented a storage auditing
design that improves both security and the overall efﬁciency
by using the blockchain paradigm. The most related work is
done by Aumasson et al. [13] who comprehensively reviewed
four Ethereum Beacon clients from a security perspective.
Their work consists of a benchmarking methodology as all

of the four clients are evaluated using the same set of security
problems. However it is different from this paper because
CHAOSETH focuses on resilience benchmarking instead of
security.

Moreover, none of these solutions support the speciﬁcation
and execution of experiments directly in a production-like
environment. None of the approaches focus on exploring
the impact of an unstable operating system on an Ethereum
client. CHAOSETH provides a systematic way for developers
to learn how their Ethereum client implementations react to
different system call invocation errors in production. Thanks
to CHAOSETH, developers acquire knowledge to enhance the
client’s error handling capabilities.

B. Chaos Engineering

Basiri et al. [14] presented the principles of chaos en-
gineering in 2016. The earliest known chaos engineering
tool
is called the ‘Chaos Monkey’ [14], which has never
been deployed to Ethereum, as opposed to the ‘Bored Ape’
[2]. Zhang et al. [58] designed ChaosMachine, a tool that
conducts chaos engineering experiments at the try-catch level
for Java applications. Jernberg et al. [33] designed a chaos
engineering framework based on the literature and a tool
survey, and validated the framework in a real commercial
web system. Simonsson et al. [43] proposed ChaosOrca, a
chaos engineering system that injects system call errors for
dockerized applications. Hern´andez-Serrato et al. [30] dis-
cussed the possibilities to apply machine learning techniques
for improving chaos engineering experiments. Ikeuchi et al.
[32] proposed a framework for learning a recovery policy
using deep reinforcement
learning and chaos engineering
techniques. Chaos engineering is also applicable in the ﬁeld
of security. Torkura et al. [50] proposed CloudStrike, a tool
that focuses on injecting failures which impact security i.e.
integrity, conﬁdentiality and availability. Regarding human
factors in chaos enginering, Canonico et al. [16] discussed
what aspects of AI would be used to make a system more
resilient
to perturbations and the results of these ﬁndings
against existing chaos engineering approaches.

The most related work that combines chaos engineering
and blockchains is by Sondhi et al. [44]. They apply chaos
engineering to evaluate the performance of different consen-
sus algorithms in permissioned blockchains. The perturba-
tion models were designed for representing network failures
and message corruptions. In comparison, CHAOSETH injects
realistic errors at the level of system call invocations. The
perturbations simulate an unstable operating system, which is
not researched by the previous related works.

VII. CONCLUSION

In this paper, we present a novel chaos engineering frame-
work called CHAOSETH which actively injects system call
implementations to
invocation errors into Ethereum client
assess their resilience in production. Our experiments show
that CHAOSETH is effective to detect
the error-handling
weaknesses and strengths ranging from direct crashes to

full resilience in the two most popular Ethereum clients,
GoEthereum and OpenEthereum. As a direction for future
research, it would be promising to investigate the multiple
levels for resilience improvement in an Ethereum client: at
the operating system level, at the level of standard libraries,
and in the client’s code.

REFERENCES

[1] Principles of chaos engineering.

http://principlesofchaos.org/, April

2018.
[2] Bored

Club.
0xBC4CA0EdA7647A8aB7C2061c2E118A18a936f13D, 2021.

https://etherscan.io/address/

Yacht

Ape

[3] Ethereum Is Only as Strong as Its Weakest Link. https://www.coindesk.

com/tech/2021/09/29/ethereum-is-only-as-strong-as-its-weakest-link/,
2021. Accessed: 2020-10-10.

[4] Ashar Ahmad, Muhammad Saad, Joongheon Kim, DaeHun Nyang, and
David Mohaisen. Performance evaluation of consensus protocols in
blockchain-based audit systems. In 2021 International Conference on
Information Networking (ICOIN), pages 654–656, 2021.

[5] Amjad Aldweesh, Maher Alharby, Maryam Mehrnezhad, and Aad
Van Moorsel. Opbench: A cpu performance benchmark for ethereum
smart contract operation code. In 2019 IEEE International Conference
on Blockchain (Blockchain), pages 274–281, 2019.

[6] Amjad Aldweesh, Maher Alharby, Ellis Solaiman, and Aad van Moorsel.
Performance benchmarking of smart contracts to assess miner incentives
in ethereum. In 2018 14th European Dependable Computing Conference
(EDCC), pages 144–149, 2018.

[7] Amjad Aldweesh, Maher Alharby, and Aad van Moorsel. Performance
In 2018 IEEE/ACS 15th Inter-
benchmarking for ethereum opcodes.
national Conference on Computer Systems and Applications (AICCSA),
pages 1–2, 2018.

[8] Ayman Alkhalifah, Alex Ng, Paul A. Watters, and A. S. M. Kayes. A
mechanism to detect and prevent ethereum blockchain smart contract
reentrancy attacks. Frontiers in Computer Science, 3:1, 2021.

[9] Benjamin Ampel, Mark Patton, and Hsinchun Chen. Performance mod-
eling of hyperledger sawtooth blockchain. In 2019 IEEE International
Conference on Intelligence and Security Informatics (ISI), pages 59–61,
2019.

[10] Frederik Armknecht, Jens-Matthias Bohli, Ghassan O. Karame, and
Regulating storage overhead in existing pow-based
Wenting Li.
In Proceedings of the 26th ACM Symposium on Access
blockchains.
Control Models and Technologies, SACMAT ’21, page 131–142, New
York, NY, USA, 2021. Association for Computing Machinery.

[11] I. Ashraf, X. Ma, B. Jiang, and W. K. Chan. Gasfuzzer: Fuzzing
ethereum smart contract binaries to expose gas-oriented exception secu-
rity vulnerabilities. IEEE Access, 8:99552–99564, 2020.

[12] Nicola Atzei, Massimo Bartoletti, and Tiziana Cimoli. A survey of
In Proceedings of the 6th
attacks on ethereum smart contracts sok.
International Conference on Principles of Security and Trust - Volume
10204, page 164–186, Berlin, Heidelberg, 2017. Springer-Verlag.
[13] Jean-Philippe Aumasson, Denis Kolegov, and Evangelia Stathopoulou.

Security review of ethereum beacon clients, 2021.

[14] A. Basiri, N. Behnam, R. de Rooij, L. Hochstein, L. Kosewski,
IEEE Software,

J. Reynolds, and C. Rosenthal. Chaos engineering.
33(3):35–41, May 2016.

[15] Chengjun Cai, Yifeng Zheng, Yuefeng Du, Zhan Qin, and Cong Wang.
Towards private, robust, and veriﬁable crowdsensing systems via public
blockchains. IEEE Transactions on Dependable and Secure Computing,
18(4):1893–1907, 2021.

[16] Lorenzo Barberis Canonico, Vimal Vakeel, James Dominic, Paige
Rodeghero, and Nathan McNeese. Human-ai partnerships for chaos
engineering. In Proceedings of the IEEE/ACM 42nd International Con-
ference on Software Engineering Workshops, ICSEW’20, page 499–503,
New York, NY, USA, 2020. Association for Computing Machinery.
[17] Huashan Chen, Marcus Pendleton, Laurent Njilla, and Shouhuai Xu.
A survey on ethereum systems security: Vulnerabilities, attacks, and
defenses. ACM Comput. Surv., 53(3), June 2020.

[18] Ethereum Community. Ethereum docs: Nodes and clients.

https:
//ethereum.org/en/developers/docs/nodes-and-clients/, 2021. Accessed:
2021-08-10.

[19] J. Correas, P. Gordillo, and G. Rom´an-D´ıez.

Static proﬁling and
optimization of ethereum smart contracts using resource analysis. IEEE
Access, 9:25495–25507, 2021.

[20] Mikel Cortes-Goicoechea, Luca Franceschini, and Leonardo Bautista-

Gomez. Resource analysis of ethereum 2.0 clients, 2020.

[21] Mohammad Dabbagh, Mohsen Kakavand, Mohammad Tahir, and An-
gela Amphawan. Performance analysis of blockchain platforms: Empir-
ical evaluation of hyperledger fabric and ethereum. In 2020 IEEE 2nd
International Conference on Artiﬁcial Intelligence in Engineering and
Technology (IICAIET), pages 1–6, 2020.

[22] Tien Tuan Anh Dinh, Ji Wang, Gang Chen, Rui Liu, Beng Chin Ooi,
and Kian-Lee Tan. Blockbench: A framework for analyzing private
blockchains. In Proceedings of the 2017 ACM International Conference
on Management of Data, SIGMOD ’17, page 1085–1100, New York,
NY, USA, 2017. Association for Computing Machinery.

[23] Yuefeng Du, Huayi Duan, Anxin Zhou, Cong Wang, Man Ho Au,
and Qian Wang. Enabling secure and efﬁcient decentralized storage
auditing with blockchain. IEEE Transactions on Dependable and Secure
Computing, pages 1–1, 2021.

[24] Caixiang Fan, Sara Ghaemi, Hamzeh Khazaei, and Petr Musilek. Per-
formance evaluation of blockchain systems: A systematic survey. IEEE
Access, 8:126927–126950, 2020.

[25] Stephanie Forrest, Steven A. Hofmeyr, Anil Somayaji, and Thomas A.
Longstaff. A sense of self for unix processes. In 1996 IEEE Symposium
on Security and Privacy, May 6-8, 1996, Oakland, CA, USA, pages 120–
128. IEEE Computer Society, 1996.

[26] Ying Fu, Meng Ren, Fuchen Ma, Heyuan Shi, Xin Yang, Yu Jiang,
Huizhong Li, and Xiang Shi. Evmfuzzer: Detect evm vulnerabilities
In Proceedings of the 2019 27th ACM Joint Meet-
via fuzz testing.
ing on European Software Engineering Conference and Symposium
on the Foundations of Software Engineering, ESEC/FSE 2019, page
1110–1114, New York, NY, USA, 2019. Association for Computing
Machinery.

[27] Miguel Alejandro Gonz´alez, Walter Rudametkin, Martin Monperrus, and
Romain Rouvoy. Challenging Anti-fragile Blockchain Applications. In
11th EuroSys Doctoral Workshop EuroDW’17, Belgrade, Serbia, April
2017.
´Akos Hajdu, Naghmeh Ivaki, Imre Kocsis, Attila Klenik, L´aszl´o G¨onczy,
Nuno Laranjeiro, Henrique Madeira, and Andr´as Pataricza. Using
fault injection to assess blockchain systems in presence of faulty smart
contracts. IEEE Access, 8:190760–190783, 2020.

[28]

[29] Yue Hao, Yi Li, Xinghua Dong, Li Fang, and Ping Chen. Performance
In 2018 IEEE

analysis of consensus algorithm in private blockchain.
Intelligent Vehicles Symposium (IV), pages 280–285, 2018.

[30] J. Hern´andez-Serrato, A. Velasco, Y. Niﬁo, and M. Linares-V´asquez.
In 2020 IEEE
Applying machine learning with chaos engineering.
International Symposium on Software Reliability Engineering Workshops
(ISSREW), pages 151–152, 2020.

[31] Huawei Huang, Wei Kong, Sicong Zhou, Zibin Zheng, and Song Guo.
A survey of state-of-the-art on blockchains: Theories, modelings, and
tools. ACM Comput. Surv., 54(2), March 2021.

[32] H. Ikeuchi, J. Ge, Y. Matsuo, and K. Watanabe. A framework for
automatic failure recovery in ict systems by deep reinforcement learning.
In 2020 IEEE 40th International Conference on Distributed Computing
Systems (ICDCS), pages 1310–1315, 2020.

[33] Hugo Jernberg, Per Runeson, and Emelie Engstr¨om. Getting started with
chaos engineering - design of an implementation framework in practice.
In Proceedings of the 14th ACM / IEEE International Symposium on
Empirical Software Engineering and Measurement (ESEM), ESEM ’20,
New York, NY, USA, 2020. Association for Computing Machinery.
[34] Abhilash Kancharla, Zuqiang Ke, Nohpill Park, and Hyeyoung Kim.
Hybrid chain and dependability. In Proceedings of the 2nd ACM Inter-
national Symposium on Blockchain and Secure Critical Infrastructure,
BSCI ’20, page 204–209, New York, NY, USA, 2020. Association for
Computing Machinery.

[35] Abhilash Kancharla, Jongho Seol, Nohpill Park, and Hyeyoung Kim.
Slim chain and dependability. BSCI ’20, page 180–185, New York, NY,
USA, 2020. Association for Computing Machinery.

[36] Karama Kanoun and Lisa Spainhower. Dependability Benchmarking for

Computer Systems. Wiley-IEEE Computer Society Pr, 2008.

[37] Ying Liu, Kai Zheng, Paul Craig, Yuexuan Li, Yangkai Luo, and
Xin Huang. Evaluating the reliability of blockchain based internet of
things applications. In 2018 1st IEEE International Conference on Hot
Information-Centric Networking (HotICN), pages 230–231, 2018.

[38] Carlos Melo, Jamilson Dantas, Danilo Oliveira, Iure F´e, Rubens Matos,
Renata Dantas, Ronierison Maciel, and Paulo Maciel. Dependability
In 2018 IEEE
evaluation of a blockchain-as-a-service environment.
Symposium on Computers and Communications (ISCC), pages 00909–
00914, 2018.

[39] Satoshi Nakamoto. Bitcoin: A peer-to-peer electronic cash system. 2008.
[40] Roberto Natella, Domenico Cotroneo, and Henrique S. Madeira. Assess-
ing dependability with software fault injection: A survey. ACM Comput.
Surv., 48(3), February 2016.

[41] Casey Rosenthal, Aaron Blohowiak Lorin Hochstein, Nora Jones, and
Ali Basiri. Chaos engineering - Building conﬁdence in system behavior
through experiments. O’Reilly, 2017.

[42] Jongho Seol, Abhilash Kancharla, Nicole Park, Nohpill Park, and Indy
Park. The dependability of crypto linked off-chain ﬁle systems in back-
International Journal of Networked
end blockchain analytics engine.
and Distributed Computing, 6:210–215, 2018.

[43] Jesper Simonsson, Long Zhang, Brice Morin, Benoit Baudry, and Martin
Monperrus. Observability and chaos engineering on system calls for
containerized applications in docker. Future Generation Computer
Systems, 2021.

[44] Shiv Sondhi, Sherif Saad, Kevin Shi, Mohammad Mamun, and Issa
Traore. Chaos engineering for understanding consensus algorithms
performance in permissioned blockchains, 2021.

[45] Jo˜ao Sousa, Alysson Bessani, and Marko Vukolic. A byzantine fault-
tolerant ordering service for the hyperledger fabric blockchain platform.
In 2018 48th Annual IEEE/IFIP International Conference on Depend-
able Systems and Networks (DSN), pages 51–58, 2018.

[46] BPF-HELPERS(7) - Linux Programmer’s Manual. https://www.man7.
org/linux/man-pages/man7/bpf-helpers.7.html, 2019. Accessed: 2020-
05-10.
[47] ERRNO(3)

- Linux Programmer’s Manual.
man-pages/man3/errno.3.html, 2019. Accessed: 2020-05-10.
- Linux Programmer’s Manual.

http://man7.org/linux/

http://man7.org/linux/

[48] syscalls(2)

man-pages/man2/syscalls.2.html, 2019. Accessed: 2020-04-20.

[49] Bo Tang, Hongjuan Kang, Jingwen Fan, Qi Li, and Ravi Sandhu. Iot
passport: A blockchain-based trust framework for collaborative internet-
of-things. In Proceedings of the 24th ACM Symposium on Access Control
Models and Technologies, SACMAT ’19, page 83–92, New York, NY,
USA, 2019. Association for Computing Machinery.

[50] K. A. Torkura, M. I. H. Sukmana, F. Cheng, and C. Meinel. Cloudstrike:
Chaos engineering for security and resiliency in cloud infrastructure.
IEEE Access, 8:123044–123060, 2020.

[51] Hong-Linh Truong and Filip Rydzi. Benchmarking blockchain interac-

tions in mobile edge cloud software systems. 04 2019.

[52] Haijun Wang, Ye Liu, Yi Li, Shang-Wei Lin, Cyrille Artho, Lei Ma,
and Yang Liu. Oracle-supported dynamic exploit generation for smart
IEEE Transactions on Dependable and Secure Computing,
contracts.
pages 1–1, 2020.

[53] Zeli Wang, Hai Jin, Weiqi Dai, Kim-Kwang Raymond Choo, and Deqing
Zou. Ethereum smart contract security research: survey and future
research opportunities. Frontiers Comput. Sci., 15(2):152802, 2021.
[54] Sam M. Werner, Daniel Perez, Lewis Gudgeon, Ariah Klages-Mundt,
Dominik Harz, and William J. Knottenbelt. Sok: Decentralized ﬁnance
(deﬁ), 2021.

[55] Gavin Wood. Ethereum: A secure decentralised generalised transaction

ledger, 2021. Accessed: 2021-04-10.

[56] Bo Wu, Ke Xu, Qi Li, Shoushou Ren, Zhuotao Liu, and Zhichao
Zhang. Toward blockchain-powered trusted collaborative services for
edge-centric networks. IEEE Network, 34(2):30–36, 2020.

[57] Yang Xiang, Mukaddim Pathan, Guiyi Wei, and Giancarlo Fortino.
Availability, resilience, and fault tolerance of internet and distributed
computing systems. Concurrency and Computation: Practice and
Experience, 27(10):2503–2505, 2015.

[58] L. Zhang, B. Morin, P. Haller, B. Baudry, and M. Monperrus. A
chaos engineering system for live analysis and falsiﬁcation of exception-
handling in the jvm. IEEE Transactions on Software Engineering, pages
1–1, 2019.

[59] Long Zhang, Brice Morin, Benoit Baudry, and Martin Monperrus.
Maximizing error injection realism for chaos engineering with system
calls. IEEE Transactions on Dependable and Secure Computing, pages
1–1, 2021.

[60] Mengya Zhang, Xiaokuan Zhang, Yinqian Zhang, and Zhiqiang Lin.
TXSPECTOR: Uncovering attacks in ethereum from transactions.
In
29th USENIX Security Symposium (USENIX Security 20), pages 2775–
2792. USENIX Association, August 2020.

[61] Qi Zhang, Petr Novotny, Salman Baset, Donna Dillenberger, Artem
Barger, and Yacov Manevich. Ledgerguard: Improving blockchain ledger
In Shiping Chen, Harry Wang, and Liang-Jie Zhang,
dependability.

editors, ICBC 2018, pages 251–258, Cham, 2018. Springer International
Publishing.

