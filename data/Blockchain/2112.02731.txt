Trozze et al.

RESEARCH

Detecting DeFi Securities Violations from Token
Smart Contract Code
Arianna Trozze1,2*, Bennett Kleinberg2,3 and Toby Davies2

*Correspondence:
arianna.trozze@ucl.ac.uk
1Department of Computer
Science, University College
London, Gower Street WC1E 6EA,
London, UK
Full list of author information is
available at the end of the article

2
2
0
2

r
p
A
6
2

]

G
L
.
s
c
[

2
v
1
3
7
2
0
.
2
1
1
2
:
v
i
X
r
a

Abstract

Decentralized Finance (DeFi) is a system of ﬁnancial products and services built
and delivered through smart contracts on various blockchains. In the past year,
DeFi has gained popularity and market capitalization. However, it has also been
connected to crime, in particular, various types of securities violations. The lack
of Know Your Customer requirements in DeFi poses challenges to governments
trying to mitigate potential oﬀending in this space. This study aims to uncover
whether this problem is suited to a machine learning approach, namely, whether
we can identify DeFi projects potentially engaging in securities violations based
on their tokens’ smart contract code. We adapt prior work on detecting speciﬁc
types of securities violations across Ethereum, building a random forest classiﬁer
based on features extracted from DeFi projects’ tokens’ smart contract code. The
ﬁnal classiﬁer achieves a 98.6% F1-score. From further feature-level analysis, we
ﬁnd a single feature makes this a highly detectable problem. The high reliance on
a single feature means that, at this stage, a complex machine learning model may
not be necessary or desirable for this problem. However, this may change as DeFi
securities violations become more sophisticated. Another contribution of our
study is a new dataset, comprised of (a) a veriﬁed ground truth dataset for
tokens involved in securities violations and (b) a set of legitimate tokens from a
reputable DeFi aggregator. This paper further discusses the potential use of a
model like ours by prosecutors in enforcement eﬀorts and connects it to the wider
legal context.

Keywords: DeFi; Decentralized Finance; Ethereum; Fraud; Cryptocurrency;
Machine Learning; Securities Law

Introduction

Decentralized Finance (DeFi) refers to a suite of ﬁnancial products and services

delivered in a decentralized and permissionless manner through smart contracts[1]

on a blockchain[2], most commonly Ethereum. Its promoters have proclaimed it to

be the future of ﬁnance [4], an assertion supported by the increase in its market

[1]Smart contracts are programs stored on a blockchain which automatically carry out speciﬁed
actions when certain conditions are met [1].
[2]A blockchain is a secure, decentralized database comprised of entries called blocks, which are
cryptographically connected to one another through a hash of the previous block, thereby ensuring
its security and resistance to fraud. In the case of cryptocurrencies, blockchains serve as a decen-
tralized, distributed public ledger that records all transactions [2, 3]. In this sense, blockchains
underpin the “decentralized” nature of “decentralized ﬁnance”, as they allow users to transact
with one another in a trustless manner, without the need for an intermediary ﬁnancial institution.

 
 
 
 
 
 
Trozze et al.

Page 2 of 38

capitalization by more than 8,000% between May 2020 and May 2021 [5]. Unfortu-

nately, criminal activity in the DeFi ecosystem has also grown in parallel with its

value. As of August 2021, 54% of cryptocurrency fraud was DeFi-related, compared

to only 3% the previous year [6]. Furthermore, vast numbers of new DeFi projects

are created every day and anyone is permitted to create one. Taken together, these

present a challenge for law enforcement. The volume of projects, coupled with the

magnitude of criminal oﬀending, makes the development of an automated fraud

detection method to guide investigative eﬀorts particularly critical.

Securities violations are one category of crime aﬀecting the cryptocurrency space

[7, 8, 9]. Securities violations refer to oﬀenses relating to the registration of securities

and misrepresentations in connection with the purchase or sale of securities, includ-

ing pyramid schemes and foreign exchange scams [10, 11]. Preliminary empirical

research on decentralized exchanges (one of DeFi’s core product oﬀerings) points

to the prevalence of speciﬁc types of securities violations (such as exit scams[3], ad-

vance fee fraud[4], and market manipulation) on these platforms [14], while others

have chronicled securities violations like Ponzi schemes on decentralized applica-

tions (dApps) [15].[5] This limited empirical work suggests possible approaches for

identifying general scam tokens and certain types of securities violations like Ponzi

schemes in the wider cryptocurrency universe. However, research has not yet ex-

plored automated detection a) of a broader type of violations (securities violations)

than scam tokens or Ponzi schemes, or b) across DeFi as a whole rather than a sub-

space. We consider an automated approach to be preferrable because of the sheer

volume of DeFi projects that exist and are being created.

Against this background, we seek to answer the following research questions: (1)

is a machine learning approach appropriate for identifying DeFi projects likely to

[3]Exit scams, also referred to as “rug pulls”, involve developers of a project stealing all funds paid
into or invested in their project [12].
[4]Advance fee fraud refers to a scammer convincing a victim to send them an amount of money
in exchange for returning the original amount plus a premium. The fraudster simply takes the
original funds [13].
[5]DApps are the user interfaces of DeFi-based products and services.

Trozze et al.

Page 3 of 38

be engaging in violations of U.S. securities laws?[6] and (2) what are the reasons, at

feature level, such a model is or is not successful for this classiﬁcation problem? This

study presents and critically evaluates the ﬁrst method for automated detection of

various types of securities violations in the DeFi ecosystem on the basis of their

token’s smart contract code, providing a tool which may identify starting points for

further investigation. The contributions of this study are as follows:

• We build the ﬁrst classiﬁer to detect DeFi projects committing various types

of securities violations. Our work is the ﬁrst to (a) expand existing machine

learning-based classiﬁcation models to encompass multiple types of securities

violations, and (b) apply a model based on token smart contract code-based

features to the DeFi ecosystem as a whole.

• We use and make available a new dataset of violations—veriﬁed by court

actions.

Decentralized Finance (DeFi)

DeFi refers to a collection of ﬁnancial products and services made possible by smart

contracts built on various blockchains, most commonly the Ethereum blockchain.

DeFi oﬀers traditional ﬁnancial products and services, such as loans, derivatives,

and currency exchange, in a decentralized manner through smart contracts. DeFi is

an open source, permissionless system, that is not operated by a central authority.

Rather than transacting with one another through an intermediary, users’ interac-

tions occur through dApps, created by smart contracts, on a blockchain [17]. In this

section we brieﬂy outline the main components of the DeFi ecosystem.

[6]Developers write smart contracts in a high-level programming language called Solidity [16].
Smart contracts are responsible for DeFi’s application infrastructure, as well as for creating cryp-
tocurrency tokens themselves.

Trozze et al.

Page 4 of 38

Ethereum

Ethereum functions as a distributed virtual machine and is the platform on which

much of the DeFi ecosystem currently runs. This paper focuses on explaining the

features of Ethereum which are most relevant to DeFi’s functioning.[7]

In addition to holding balances, Ethereum accounts can store smart contract code

and other information. A smart contract is a computer program that automatically

carries out certain actions when speciﬁed conditions—such as payments—are met

[3]. The smart contract code is immutable and publicly available on the blockchain.

Smart contracts allow parties who do not trust one another to enter into con-

tracts—rather than trusting each other or a third party to execute the contract,

smart contracts ensure the terms will be executed as coded into the contract [1].

Before smart contracts are executed, they must be compiled into bytecode to be

deployed and understood by the Ethereum Virtual Machine (EVM). Once compiled,

Ethereum smart contracts take the form of a string of alphanumeric characters

beginning with “0x” (bytecode). The EVM is a stack-based environment[8], with

a 256-bit stack size. It reads this bytecode as operational codes (opcodes) which

are, essentially, a set of instructions (from a set of 144 possible instructions). The

opcodes include actions like retrieving the address of the individual interacting with

the contract, various mathematical operations, and storing information [20, 18].

Tokens

Many DeFi projects also have associated tokens created by smart contracts, which

either entitle holders to something within the dApp (analogous to a video game’s in-

game currency) or serve as “governance tokens”. For example, UNI is the Uniswap

decentralized exchange’s governance token [21]. Another example is the SCRT token

which, in addition to being a governance token, is required to pay transaction fees

[7]For further details on Ethereum, see the Ethereum Yellow Paper [18].
[8]In stack-based programming, “all functions receive arguments from a numerical stack and return
their result by pushing it on the stack.” These speciﬁc functions come from a set of pre-deﬁned
functions [19].

Trozze et al.

Page 5 of 38

on the Secret network [22]. Holders of governance tokens can vote on the future

of projects; their voting power is proportional to the amount of governance tokens

they hold. Most DeFi tokens on Ethereum follow the ERC-20 (Ethereum Request for

Comment) standard, which facilitates interoperability among projects. The ERC-20

standard allows for token capabilities like transferring among accounts, maintaining

balances, and the supply of tokens, among others [23].

dApps

Developers create dApps which serve as the interfaces to execute these smart con-

tracts. DeFi’s current core product oﬀerings—including decentralized exchanges

(dexes), lending products, prediction markets, insurance, and other ﬁnancial prod-

ucts and services—are delivered through dApps [24]. Table 1 describes the primary

products that make up the DeFi ecosystem.

Figure 1 depicts the process of dApp creation and execution through smart con-

tracts, using Uniswap (a popular dex) as an example. As depicted in Figure 1,

DeFi users must have a Web3 software wallet to hold DeFi tokens and interact

with dApps. These wallets can be thought of as akin to a mobile banking applica-

tion and exhibit similar features (sending transactions, showing balances, etc.). The

diﬀerence is that, unlike with a banking application, users retain custody of their

funds and can send transactions and execute other functions directly, rather than

through an intermediary institution [25]. Using cryptographic digital signatures,

users approve connections to their Web3 wallets, “sign in” to dApps, and approve

interactions with the smart contracts on these platforms through their wallet.

The legal perspective

DeFi has raised alarm in regulatory circles due to its lack of Know Your Customer

(KYC)[10] [33] procedures—making it much more diﬃcult, if not impossible, to

[10]KYC is the process through which ﬁnancial institutions verify a customer’s identity, generally
using government-issued identiﬁcation, proof-of-address, and other documents [32]

Trozze et al.

Page 6 of 38

Figure 1 DApp creation and functioning. The ﬁrst box contains an excerpt from the Solidity code for the
popular dex Uniswap’s exchange function. The second box shows the same code compiled into EVM-readable
bytecode. The third box shows the transaction which deployed this code to the Ethereum blockchain, thereby
creating the dApp. The branch of Figure 1 labelled “front-end” shows Uniswap’s user interface for a sample
exchange operation of 645.49035 USDT to ETH. Below the exchange interface is a screenshot of the Web3
wallet MetaMask. To execute a transaction like the exchange depicted above, the user must connect their Web3
wallet to the relevant dApp via the wallet’s browser extension. From there, they can approve the transaction.
After the transaction is executed, the user’s MetaMask wallet automatically reﬂects the new balances of these
cryptocurrencies. On the back-end, the aforementioned exchange takes the form of bytecode (depicted on the
“back-end” branch of the ﬁgure), which is executed by the EVM. the ﬁnal box shows the hash of the executed
transaction exchanging USDT for ETH. Full details of this exchange can be found at
https://etherscan.io/tx/0x7febc16c960a177077ddf0562c9ba21ac9bd5585bacf969d88a6b678e756081a. The full
Solidity source code for the Uniswap V2 smart contract can be found at
https://etherscan.io/address/0x7a250d5630b4cf539739df2c5dacb4c659f2488d#code.

Compiled into bytecodeSolidity codeTransaction inputExecutionUniswap exchangeWeb3 wallet confirmationDeployed and stored on blockchainFront-endBack-endTrozze et al.

Page 7 of 38

Table 1 Overview of DeFi products.

DeFi lending

Decentralized exchanges These services allow users to exchange cryptocurren-
cies using liquidity provided by other users, generally
through Automated Market Makers, which algorithmi-
cally set prices [26]. Participants can provide liquidity
to liquidity pools for certain pairs of cryptocurrencies
and receive a Liquidity Provider token for doing so.
They can “stake” this token (i.e., lock it into the sys-
tem and agree not to withdraw it for a certain pe-
riod) and earn interest on it, usually paid in the decen-
tralized exchange’s governance token (referred to as
“yield farming”). The return on investment for these
yield farms may be in the hundreds or even thousands
of percent. Participants can stake governance tokens
in “pools” and earn further rewards. This incentivizes
users to provide liquidity to keep the exchanges run-
ning.[9]
Loans are issued through smart contracts rather
than intermediaries and use cryptocurrencies as
collateral
[27]. Loans are often issued in sta-
blecoins—cryptoassets whose value is pegged to
government-issued ﬁat currencies—and interest rates
tend to be set algorithmically [28]. Users can earn in-
terest for providing liquidity for loans and earn fees
lending innovation is
from loans. One primary DeFi
ﬂash loans, which are loans issued and repaid in a sin-
gle transaction. Because they are issued and repaid in
one transaction, they do not require collateral [12].
These allow users to bet on real-world outcomes—such
as sporting events or elections—through smart con-
tracts [29]. Prediction markets rely on blockchain or-
acles, which are external sources of information that
determine the outcome of the prediction market [12].
Based on this information about the outcome, the
smart contract releases the appropriate funds to the
winners [29].
DeFi insurance community members serve as under-
writers and share in premiums paid to the protocol.
Holders of the project’s governance token vote on
claims payouts. DeFi insurance remains a nascent in-
dustry, but some companies are attempting to handle
claims directly through smart contracts. So far, DeFi
insurance tends to insure only other DeFi protocols
[30].
A range of other ﬁnancial products, including those
not usually available to retail investors, can be imple-
mented within DeFi. These include derivatives trading,
margin trading, and other securities [31].

Other ﬁnancial products

Prediction markets

DeFi insurance

Trozze et al.

Page 8 of 38

identify perpetrators of crimes—and concerns over DeFi tokens’ potential conﬂict

with existing U.S. securities laws [34]. U.S. securities are primarily governed at the

federal level by the Securities Act and the Exchange Act, though the Sarbanes-Oxley

Act, Trust Indenture Act, Investment Advisors Act, and Investment Company Act

are also relevant. Enforcement of these laws is the responsibility of the Securities

and Exchange Commission (SEC) and the Financial Industry Regulatory Authority

(FINRA) [11].

The Securities Act relates to the oﬀer and sale of securities. One of the key pro-

visions which has been charged in cryptocurrency cases is Section 5, which requires

the registration of the oﬀer and sale of securities and stipulates speciﬁc provisions

thereof [11].[11] Other sections detail the required registration information[12] and

exemptions[13] [11].

The Exchange Act speciﬁes public companies’ reporting requirements and regu-

lates securities trading through securities exchanges. It also handles securities fraud;

under section 10(b) and 10b-5, fraud and manipulation in relation to buying or sell-

ing securities is illegal. One cannot make false or misleading statements (including

by omission) in relation to the sale or purchase of securities, including those exempt

from registration under the Securities Act. The registration of securities, securities

exchanges, brokers, dealers, and analysts is covered in Sections 12 and 15 of the Ex-

change Act [11]. Section 12 regulates registration of initial public oﬀerings, which

is relevant for cryptocurrency initial coin oﬀerings (ICOs).[14] Finally, Section 13

relates to companies’ reporting obligations under the Exchange Act [11].

In practice, in addition to various registration and reporting violations, securi-

ties laws in the U.S. tend to cover the following fraudulent conduct: high yield

[11]For cryptocurrency case law involving the Securities Act, see [35, 36, 37].
[12]Sections 7 and 10.
[13]Section 3, Section 4, Regulation S, Rule 144A, Regulation D, Rule 144, Rule 701, Section 28.
[14]For cryptocurrency case law involving the Exchange Act, see [35, 36, 37].

Trozze et al.

Page 9 of 38

investment programs, Ponzi schemes, pyramid schemes, advance fee fraud, foreign

exchange scams, and embezzlement by brokers, among others [10].[15]

Fraud opportunities and challenges

As previously mentioned, fraud is a problem in the DeFi ecosystem [6]. DeFi appears

to face several of the same fraud challenges as the cryptocurrency market more

broadly, including securities violations such as exit scams and market manipulation

[13, 15, 14]. There are also a number of fraud types which are speciﬁc to DeFi, such

as smart contract fraud, ﬂash loan abuse, governance attacks, and oracle attacks

[12]. Further details about the nature of these fraud types can be found in [12].

The sheer volume of DeFi projects in existence (and being created daily) is a

key challenge for DeFi fraud enforcement eﬀorts. A classiﬁer to detect fraud that

is occurring would be a useful ﬁrst step in making sense of the mass of information

across DeFi and facilitating future enforcement eﬀorts in the space.

Related Work on Detecting Fraud on Ethereum

Previous studies have used machine learning to detect certain speciﬁc types of

securities violations and fraud on Ethereum. The most common type of securities

violation examined in the literature is smart contract Ponzi schemes [38, 39, 40, 41,

42, 15, 43, 44, 45, 46]. The various machine learning algorithms these studies employ

to identify such Ponzi schemes—and their relative performance—can be found in

Table 2.

Previous work on smart contract Ponzi schemes has examined code-based fea-

tures [38, 42, 41]; transaction-based features [15]; or both [45, 44, 43, 46, 40, 47].

Code-based features include the frequency with which each opcode appears in a

smart contract and the length of the smart contract bytecode [43]. Transaction-

and account-based features refer to the number of unique addresses interacting

with the smart contract, and the volume of funds transferred into and out of the

[15]For deﬁnitions of these oﬀences, see [13].

Trozze et al.

Page 10 of 38

Table 2 Related work detecting Ethereum Ponzi schemes.

[38]

[41]

[42]

[15]

[45]

[44]

Study Method

Features
Code-based

classiﬁer
”a

Semantically-
aware
that
includes
heuristic-guided
symbolic
technique”
”Anti-leakage”
model
based
ordered boosting
Deep learning model Code-based

Code-based

execution

on

Long-term
short-
term memory neural
network

Transaction-
based

short-
Long-term
term memory neural
network
Heterogeneous
Graph Transformer
Networks

and

Code-
transaction-
based
Code-
transaction-
based

and

[46]

LightGBM

[40]

XGBoost

[43]

Decision trees, ran-
dom forest, stochas-
tic gradient descent

and

Code-
transaction-
based
Code-
transaction-
based
Code-
transaction-
based

and

and

[47]

Random forest

and

Code-
transaction-
based

Performance
Precision: 100%
Recall: 100%
F1: 100%

Precision: 95%
Recall: 96%
F1: 96%
Precision: 96.3%
Recall: 97.8%
F1: 97.1%
Precision: Between 88.2% and
96.9% for diﬀerent types of con-
tracts
Recall: Between 81.6% and 97.7%
for diﬀerent types of contracts
F1: Between 85% and 96.7% for
diﬀerent types of contracts
Precision: 97%
Recall: 96%
F1: 96%
F1: Between 78% and 82% for
fraudulent
smart contracts and
87% and 89% for normal smart
contracts for diﬀerent classiﬁca-
tion tasks
Precision: 96.7%
Recall: 96.7%
F1: 96.7%
Precision: 94%
Recall: 81%
F1: 86%
Precision: Between 90% and 98%
for diﬀerent models
Recall: Between 80% and 96% for
diﬀerent models
F1: Between 84% and 96% for dif-
ferent models
Precision: Between 64% and 95%
for diﬀerent features
Recall: Between 20% and 73% for
diﬀerent features
F1: Between 30% and 82% for dif-
ferent features

Trozze et al.

Page 11 of 38

smart contract, among others [43]. Notably, one study [38] identiﬁes four speciﬁc

Ponzi scheme typologies based on bytecode sequences.

In addition to smart contract Ponzi schemes, other studies examining smart con-

tracts use machine learning to detect general fraud and scams [48, 49, 50, 51, 14];

advance fee fraud [52]; smart contract honeypots [42, 53] and ICO scams [54, 55].

Most existing work in this ﬁeld examines Ethereum smart contracts in general,

but some speciﬁcally refer to dApps and DeFi in their work [41, 15, 45, 51, 14],

though they do so to varying degrees and, at times, conﬂate DeFi with Ethereum

more broadly. Notably, one study [15] uses machine learning to classify diﬀerent

types of dApp smart contracts into their various categories, including those for

gaming, gambling, and ﬁnance, among others.

Gaps and issues

While the results of the studies described in Table 2 suggest that approaches of

this nature can perform well for this task, a number of points of caution have also

been raised. Literature concerning smart contract Ponzi scheme detection points

to issues of overﬁtting, due to the imbalance of classiﬁcations in many datasets

[41]. Studies have addressed this using the over- and under-sampling techniques

[38, 41, 45, 46]. Other scholars [38] criticize the interpretability of results based on

opcode features, i.e., why the presence of certain opcodes would point to criminality.

Perhaps most fundamentally, however, these studies have given little consideration

to whether machine learning techniques are actually necessary for this task; or, at

least, superior to other, possibly simpler, approaches. While the methods applied

undoubtedly show high performance, it remains possible that similar metrics could

be achieved without recourse to these kinds of techniques.

Another issue with previous work is the repeated use of two particular datasets

[40, 56]. Of the studies cited above, four use the Bartoletti et al. [56] dataset [38, 42,

44, 43] and two use the Chen et al. [40] one [45, 46], while one study combined both

Trozze et al.

Page 12 of 38

datasets and added additional data [41]. While using the same datasets may be

useful for comparing performance, it may be less useful in practice for combatting

fraud, with any shortcomings of these datasets having a polluting eﬀect on the

literature. Indeed, when manually inspecting the Bartoletti et al. [56] dataset, one

study [38] identiﬁed issues involving duplication and bias. Finally, though used

to classify general fraud rather than Ponzi schemes, one article uses proprietary

company data [51], which hinders reproducibility and evaluations of results.

The majority of existing related work uses smart contracts in general as opposed

to token smart contracts (as [14] and we do). The only other work that speciﬁcally

examines DeFi token smart contracts using machine learning is [14]. However, their

work focuses on scam tokens in general (rather than securities violations) and on a

single dApp (the dex Uniswap).

Aims of this paper

This paper seeks to ﬁll these gaps by a) evaluating whether a machine learning

approach is appropriate for identifying DeFi projects likely to be engaging in secu-

rities violations; b) examining a broader type of securities violations (rather than

just scam tokens or Ponzi schemes); and c) investigating these violations across

DeFi as a whole, rather than speciﬁc subspaces. We also develop an entirely new

dataset of violating and legitimate tokens.

Method

This paper adapts its methods from prior research on the detection of Ethereum

smart contract Ponzi schemes, adapting an approach that performed well in that

context [43] and applying it to DeFi projects engaging in securities violations. We

build a random forest classiﬁer based on features extracted from DeFi tokens’ smart

contract code to classify the tokens into two categories: securities violations and

legitimate tokens. Figure 2 provides a summary of our methods.

Trozze et al.

Page 13 of 38

Figure 2 Methods for detecting securities violations from DeFi token smart contract code.

DatasetSmart contracts from Uniswap token listsFeature extraction Pyevmasm smart contract bytecode disassembler Classification Random forest modelsTokens subject to SEC casesBlockchain Association token list (47 tokens)Valid tokensZapper token list (2,146 tokens)SecuritiesviolationValidtokenModel  interpretation Feature importance Opcode analysisSmart contract bytecodeBytecode disassembled into opcodesFeatures: opcode frequency countsDetecting securities violations from ERC-20 token smart contract codeTrozze et al.

Page 14 of 38

Data Collection

To answer our question of whether it can be determined if a project may be en-

gaging in securities violations from its token’s smart contract code using machine

learning, we ﬁrst required ground truth sets of both securities violations and legiti-

mate tokens. One source of such information is provided by token lists compiled by

DeFi projects or companies around particular themes. One function of token lists is

to help combat token impersonation and scams; reputable token lists provide users

with some assurance that the tokens that appear are not fraudulent. Uniswap posts

lists which are contributed by projects in the community; users generally follow

lists from projects they trust [57]. The lists contain information like project web-

sites (important for avoiding phishing attempts), symbols, and their smart contract

code.

Securities Violations

The Blockchain Association (BA), a lawyer-led blockchain lobbying organization,

has created one such list of ERC-20 tokens which have been subject to U.S. SEC

enforcement actions.[16] This list contains 47 tokens and these represent our ground

truth for projects engaging in securities violations. Many of these actions involve

Initial Coin Oﬀerings (ICOs), primarily in the context of companies or individuals

failing to register their token as a security when it was required and/or making

fraudulent misrepresentations in connection with said token (for example, Coinseed

Token, Tierion, ShipChain SHIP, SALT, UnikoinGold, Boon Tech, and others) [58].

Other violations include Ponzi schemes (RGL, for example) and market manipula-

tion (Veritaseum, for example) [59, 60]. The defendants in these cases are distinct,

thereby supporting the independence of the tokens in our violations dataset. We

acknowledge the limitations of using such a small set, but this comprises all SEC

actions involving DeFi tokens/projects to date. Therefore, it makes for a more cred-

[16]https://tokenlists.org/token-list?url=https://raw.githubusercontent.com/The-Blockchain-
Association/sec-notice-list/master/ba-sec-list.json

Trozze et al.

Page 15 of 38

ible ground truth dataset than attempting to ﬁnd individual investment scams and

securities violations on blockchain forums (as other datasets do, including [56]) in

that it is more systematic and does not involve any subjective judgment of crimi-

nality. It is worth noting that the nature of the list itself highlights the need for a

systematic detection method: most of the actions either derived from the U.S. gov-

ernment whistleblower programme, or were well-publicized scams, suggesting that

enforcement is currently reliant on these sources. [61].

Legitimate Projects

The unregulated nature of the DeFi industry means that a substantial proportion

of tokens in general may be of questionable validity—even if they have not been

formally identiﬁed as violations—which poses a challenge when building a dataset

of legitimate tokens. Were we to simply take a random sample of all projects, for

example, it is likely that this would contain problematic tokens, compromising our

analysis. For this reason, we took an alternative approach, including only tokens for

which we had some evidence of credibility. To do this we use the token list main-

tained by the DeFi platform Zapper.[17] Zapper serves as a DeFi project aggregator

which allows users to monitor their liquidity provision, staking, yield farming, and

assets across diﬀerent DeFi protocols. As of November 2021, Zapper had over a

million total users, $11 billion worth of transaction volume, and raised $15 mil-

lion in venture funding [62]. While they do not claim to provide ﬁnancial advice

to users, they make an eﬀort to internally vet projects they list, opting for those

with audited contracts and reputable teams [63]. To be clear, inclusion in this list

does not provide any indication of the “quality” of a token—it is not analogous to a

list of “blue chip” stocks—but simply an indication of authenticity. The Zapper list

contains 2,146 tokens, and we used this as our ground truth of legitimate tokens.

We acknowledge that this may not be as representative of DeFi tokens in general

[17]https://tokenlists.org/token-list?url=https://zapper.ﬁ/api/token-list

Trozze et al.

Page 16 of 38

as a random sample, but this is the best available source of tokens which have some

marker of credibility.

Final Dataset

We extracted the project names and smart contracts from tokens on both of these

lists and combined them into a single dataset, with a binary indicator added to ﬂag

violations. This gave us an initial dataset of 2,193 smart contract addresses. We

were unable to extract features from a small subset of the smart contracts on the

Zapper list (n=105, 4.9%).[18] The ﬁnal dataset thus consisted of 2,088 tokens (47

of which constituted securities violations).

Features

Next, we extracted our classiﬁcation features from the token smart contract ad-

dresses we collected. We opted to use only code-based features, following other

recent studies which have achieved high levels of performance (including 100% pre-

cision and recall in [38]) in classifying Ethereum-based smart contract Ponzi schemes

[38, 41, 15]. Furthermore, using code-based features allows for classiﬁcation as soon

as smart contracts are deployed [43]—rather than waiting to examine the charac-

teristics of associated transactions—and permits analysis of smart contracts with

few transactions [38].

We used the frequency of each of the opcodes present in our smart contracts as

features, for a total of 129 features [43]. Prior work [43, 41] combined and eliminated

features, and also included features like the length of the smart contract bytecode,

but we chose not to do so, for the sake of simplicity. We do, however, note that

using the frequencies of features does, indirectly, capture the relative length of the

smart contract bytecode, since longer bytecodes would contain higher frequencies

of opcodes overall.

[18]The programme we used to disassemble the smart contracts into their respective opcodes was
not able to disassmble these contracts, nor were we able to disassemble them properly using
alternative disassembling software.

Trozze et al.

Page 17 of 38

For this initial analysis, our aim was to keep the classiﬁer as simple and as com-

putationally inexpensive as possible, representing a prototypical application of ma-

chine learning to this task. It is also the ﬁrst classiﬁer for DeFi securities violations,

so our aim was to obtain a baseline for this novel classiﬁcation problem in order to

determine if it is, in fact, suited to machine learning, rather than improving on the

state of the art for previously addressed problems (as [41, 38] and others have done

for smart contract Ponzi scheme classiﬁcation).

EVM bytecode can be computationally “disassembled” into its corresponding op-

codes. This process is illustrated in Fig. 2. Following [43], we used the frequencies

with which each opcode appeared in the smart contract as the features for our clas-

siﬁer. We used the Pyevmasm Python package [64] to disassemble each contract’s

bytecode into its equivalent opcodes and then used a counter to determine the num-

ber of times each opcode appeared in the contract. As discussed, we were unable to

disassemble a small subset (n=105, 4.9%) of smart contracts in our initial dataset.

We used the remaining 2,088 as our ﬁnal dataset to train and test our classiﬁer.

Classiﬁcation

We used a random forest classiﬁer to attempt to determine if a project was poten-

tially engaging in securities violations. We chose a random forest classiﬁer for the

following reasons:

1 Research involving data similar to ours achieved the best classiﬁcation results

with a random forest, compared with other classiﬁers ([14]; precision: 96.45%,

recall: 96.79%, F1-score: 96.62%).

2 While initial work on smart contract Ponzi schemes [43] has been optimized

in later studies (for example, [41]), our goal is to achieve a baseline of perfor-

mance for classifying DeFi securities violations. Previous work [43] found the

random forest algorithm performed the best on their dataset, when compared

Trozze et al.

Page 18 of 38

with other standard classiﬁcation algorithms (J48 decision tree and stochastic

gradient descent).

3 Given our primary goal of determining if machine learning methods are suit-

able for developing a classiﬁer useful for law enforcement investigations, using

a model with greater transparency and traceability is most informative.

Given the classiﬁcation imbalance in our data, we used down-sampling of the ma-

jority class to balance it with the minority class. Speciﬁcally, we randomly sampled

47 smart contracts from the majority class (i.e., from the n=2041 legitimate con-

tracts) and ran a random forest classiﬁer on the resulting balanced dataset (i.e., 47

violations vs. 47 legitimate tokens). This procedure was repeated 100 times with

diﬀerent random samples, and we report the average performance of these 100 it-

erations.[19]

For each iteration, we used 70% of our data to train our model and 30% for our

test set, following previous work in classifying smart contract Ponzi schemes [45].

We calculated accuracy, and weighted precision, recall, and F1-score to evaluate

our model [67]. We calculated the means of these metrics across our 100 iterations

to arrive at our ﬁnal performance scores. Once we built our model, we analyzed

the average feature importance (across the 100 iterations) and then built several

subsequent models based on this information.

Results

Classiﬁcation

We built our initial classiﬁcation model using the frequency of all opcodes contained

in our dataset (a total of 129 features), employing bootstrapped under-sampling to

evenly balance the classes in our dataset over 100 iterations. As can be seen from

[19]Under-sampling, combined with properly executed cross-validation, performs well on highly
imbalanced datasets [65]. While other, related work to ours [41] has used the Synthetic Minority
Over-Sampling Technique (SMOTE) to train imbalanced data, we chose the more conservative
under-sampling method. SMOTE combines majority class under-sampling and minority class over-
sampling and synthesizes additional data for the minority class [66]. While we used down-sampling
for our ﬁnal model, we also built models using SMOTE, the results of which can be found in
Appendix I.

Trozze et al.

Page 19 of 38

the evaluation metrics shown in Table 3, we achieved high performance with this

model. We then calculated the relative importance of the features included in the

model, the results of which are reported in Table 4. We note the exceptionally high

importance of the DUP9 feature to our model. The DUP9 opcode duplicates the

ninth item on the stack; there is no obvious reason why it would be connected to

illicit activity.

Table 3 Model performance with under-sampling.

Full-feature model
10-feature model
Two-feature model
Single-feature
model(DUP9)
128-feature model (excluding
DUP9)

Accuracy Precision Recall
0.982
0.982
0.986
0.986
0.980
0.980
0.986
0.986

0.983
0.987
0.982
0.987

F1-score Baseline

0.982
0.986
0.980
0.986

0.500
0.500
0.500
0.500

0.489

0.511

0.489

0.476

0.500

Table 4 Feature importance for models with under-sampling.

Full-feature model

128-feature model

10-feature model

2-feature model

Feature
DUP9
INVALID
MSTORE
SWAP13
SWAP9
SWAP8
SHR
SGT
GASPRICE
REVERT

Importance
0.449
0.049
0.013
0.012
0.011
0.011
0.010
0.009
0.008
0.007

Feature
INVALID
SLT
SHR
MSTORE
SWAP13
SGT
SWAP8
BYTE
SWAP9
REVERT

Importance
0.098
0.019
0.019
0.019
0.019
0.017
0.016
0.016
0.015
0.015

Feature
DUP9
INVALID
SGT
SWAP8
REVERT
SWAP9
MSTORE
GASPRICE
SWAP13
SHR

DUP9
INVALID

Importance Feature
0.786
0.085
0.019
0.018
0.018
0.018
0.017
0.014
0.013
0.012

Importance
0.896
0.104

Next, we built a model using only the 10 features with the highest importance in

our original model, then the two most important features, and, ﬁnally, the single

most important feature. The weighted precision, recall, and F1-score and accuracy

are reported in Table 3. We also assessed the feature importance of our further

models, which is reported in Table 4. Table 5 describes the opcodes whose frequency

in the smart contracts was determined to be of high importance to the models.

Using the F1-score as our primary metric, we achieved the best performance with

the single- and 10-feature model, followed by our models with two and 129 features.

Due to the high level of importance placed on the frequency of DUP9 in the smart

contracts, we also constructed a model with all our features except DUP9 (a total

of 128 features). As expected, this performed much less well and much closer to the

Trozze et al.

Page 20 of 38

Table 5 Feature opcode descriptions [20].

Description
Duplicate nth stack item
Purposeful invalid instruction
Store a word in memory
Exchange ﬁrst and nth stack item
“Logical shift right”
“Signed greater-than comparison”

Opcode
DUPn
INVALID
MSTORE
SWAPn
SHR
SGT
GASPRICE Obtain the current gas price[20]
REVERT

Halt “execution and revert state changes”, not consuming gas and
giving a reason

[20] “Gas” refers to the fees users must pay to execute actions on Ethereum. The “gas price” is
calculated based on the amount of computational resources required to carry out the requested
operation [68].

baseline for our classes (50%, when balanced using under-sampling of the majority

class). We ultimately chose the simplest model with the highest performance (the

single-feature model) as our ﬁnal model.

Opcodes

Even using a more conservative class balancing method like under-sampling, care-

fully constructed to avoid data leakage, we remained somewhat skeptical of the

high performance of our classiﬁer (even though it was close to previously published

results). In particular, we were concerned about potential confounding variables,

i.e., whether there was any factor that inﬂuenced our dependent variable which

we had not considered. This level of performance—coupled, in particular with the

exceptionally high importance of a single feature—raises the possibility that classi-

ﬁcation could be performed using a simpler approach, given the nature of the data.

To examine this further, we analyzed various opcodes in more detail, including the

top 10 most frequently occurring opcodes (of the 129 in our dataset) for each of our

classes. We also analyzed the opcodes which were not in the top 10 most frequent

for either class, but whose frequencies were among the top 10 most important fea-

tures in our full model. We analyzed a total of 20 opcodes by conducting a t-test to

Trozze et al.

Page 21 of 38

assess whether the average frequencies were signiﬁcantly diﬀerent between our two

classes.

Table 6 Mean comparisons of opcode frequencies and t-test results with Cohen’s d eﬀect size.

Opcode

Securities violations
Mean

St. dev

Legitimate tokens
Mean

St. dev

t-value

p-value

INVALID*
DUP9
SWAP13
SHR
SWAP9
SWAP8
REVERT
MSTORE
JUMPDEST
TIMESTAMP
STOP
SWAP2
MOD
LOG1
CREATE
JUMP
CODECOPY
DUP8
SGT
GASPRICE

6.06
1.02
0.17
0.15
0.15
0.15
0.13
0.13
0.11
0.11
0.04
0.09
0.04
0.00
0.02
0.02
0.02
0.06
0.11
0.11

3.42
0.15
0.43
0.36
0.42
0.36
0.40
0.40
0.31
0.38
0.20
0.35
0.20
0.00
0.15
0.15
0.15
0.25
0.31
0.31

5.39
0.03
0.05
0.05
0.04
0.04
0.05
0.05
0.05
0.05
0.08
0.06
0.06
0.06
0.06
0.06
0.05
0.05
0.05
0.05

3.53
0.21
0.22
0.23
0.21
0.20
0.22
0.22
0.21
0.21
0.61
0.25
0.24
0.24
0.24
0.24
0.24
0.23
0.23
0.23

1.30
32.53
1.91
1.79
1.73
2.09
1.38
1.66
1.33
1.12
-0.47
0.70
-0.42
-10.70
-0.95
-1.53
-1.51
0.28
1.20
1.18

0.194
<0.001
0.062
0.079
0.090
0.042
0.174
0.104
0.190
0.270
0.639
0.482
0.671
<0.001
0.344
0.131
0.137
0.778
0.236
0.244

Eﬀect
size
(Cohen’s
d)
0.19
4.80
0.05
0.41
0.49
0.55
0.35
0.36
0.28
0.29
-0.07
-0.10
-0.06
-0.24
-0.14
-0.14
-0.14
0.04
0.24
0.24

CI(99%)

[-0.19-0.57]
[4.37-5.22]
[0.16-0.92]
[0.03-0.79]
[0.11-0.87]
[0.17-0.93]
[-0.03-0.73]
[-0.02-0.74]
[-0.10-0.66]
[-0.09-0.67]
[-0.45-0.31]
[-0.28-0.48]
[-0.44-0.32]
[-0.62-0.14]
[-0.52-0.24]
[-0.52-0.24]
[-0.52-0.24]
[-0.34-0.42]
[-0.14, 0.62]
[-0.15-0.62]

*Opcodes in bold were among the top 10 most important features in our full-feature model.

Table 6 supports the analysis of feature importance for our ﬁnal model (reported

in Table 4); the frequency of the DUP9 opcode was signiﬁcantly diﬀerent between

the securities violations and legitimate token sets, with a much larger eﬀect size

than for any other opcode’s frequency. This is consistent with our ﬁnding that the

frequency of DUP9 in a smart contract was the single most important feature in

detecting Deﬁ securities violations. Our ﬁnal model included only this feature and

performed very well. Furthermore, our model’s performance was near our baseline

(50%) when we used all other features except the frequency of DUP9.

Of the top 10 features in our full-feature model, only one other opcode was signif-

icantly diﬀerent between our classes (SWAP8). However, the relative importance of

the features other than DUP9 is rather small and, as the performance of our model

using only the DUP9 feature shows, largely inconsequential.

Notably, the frequency of LOG1, one of the most frequently occurring opcodes

on average for our legitimate token set, was signiﬁcantly diﬀerent between our two

Trozze et al.

Page 22 of 38

classes but was not found to be a particularly important feature in in our full-feature

model.

Finally, we identify opcodes which were among the 10 most frequently occurring

for each of our classes, but which were not previously reported, in Table 7.

Table 7 Additional opcode descriptions [20].

Description
“Mark a valid destination for jumps”

Opcode
JUMPDEST
TIMESTAMP Obtain block timestamp
STOP
MOD
LOG1
CREATE
JUMP
CODECOPY Make a copy of currently running code and store in memory

“Halts execution”
Modulo operation
Append log record with a single topic
“Create a new account with associated code”
Change program counter

Comparing smart contracts

One reason our classiﬁer may have achieved high levels of performance is that many

projects reuse source code from similar projects. In fact, prior studies have found

that 96% of Ethereum smart contracts contain duplicative elements (though it is

unclear if this is the case in the DeFi ecosystem speciﬁcally)[69]. In that sense, if

legitimate projects borrow code from other legitimate projects and projects violating

securities laws borrow from other violating projects, smart contracts within each

class would have a high degree of internal consistency.

We compared the opcode frequencies for the smart contracts to one another using

cosine similarity. Cosine similarity measures the level of similarity between two

vectors; in this case, we converted the frequencies of the opcodes in each smart

contract into a vector and compared them. Cosine similarity is bound to the range

-1 to 1; a cosine similarity of -1 means the two vectors are perfectly opposite, 1

means they are identical, and 0 means the two vectors are orthogonal to one another

[70]. We calculated the cosine similarity for each possible combination of a) violating

smart contracts, b) legitimate smart contracts, and c) violating and legitimate smart

Trozze et al.

Page 23 of 38

contracts (“inter-class”). If code reuse were indeed a possible explanation, we would

expect the diﬀerence between the cosine similarities within each class and the cosine

similarities comparing the violating and legitimate contracts to be more pronounced

for violating smart contracts than for legitimate smart contracts. For each set, we

took the average of the calculated cosine similarities, the results of which can be

found in Table 8. We also compared the cosine similarities for the set of violating

contracts and for the set of legitimate tokens with the inter-class cosine similarities

using t-tests; these results are also reported in Table 8.

Table 8 Cosine similarity of smart contracts.

Class
Securities
violations
Legitimate

Cosine similarity Comparison with inter-class cosine similarity
Mean
0.274

p-value Cohen’s d
<0.001

CI(95%)
[0.077, 0.197]

St. dev
0.250

t-value
-4.493

0.137

0.312

0.273

0.168

0.866

0.001

[-0.006,
0.007]

Inter-class

0.311

0.274

Our results show that the legitimate smart contracts are more similar to each

other than the violating contracts are. The violating contracts are less similar to

each other than they are to the legitimate contracts (at least per the cosine similar-

ities). This suggests that there is more code reuse among legitimate contracts than

violating ones. However, the cosine similarity for both groups is rather small, which

suggests that code reuse is unlikely to be the primary reason for our classiﬁer’s high

performance. Further research analysing the code of each of the smart contracts

would be pertinent to conﬁrm these conclusions.

Discussion

This article sought to determine if it is useful to build a machine learning classiﬁer

to detect DeFi projects engaging in various types of securities violations from their

tokens’ smart contract code. Governments are currently struggling with how to

manage fraud in the DeFi ecosystem, particularly because these platforms do not

Trozze et al.

Page 24 of 38

require KYC; a classiﬁer could serve as a triage measure. In addition, we make

available a new dataset with a veriﬁed ground truth. Our research is also novel in

its use of token smart contract code to attempt to detect fraud across the whole

DeFi ecosystem.

Ultimately, we found that DeFi securities violations are a highly detectable prob-

lem; however, the disproportionate reliance on a single feature means that perhaps

a machine learning model is not yet necessary for this classiﬁcation problem. Our

ﬁnal model is a random forest classiﬁcation model using the frequency with which

the opcode DUP9 appeared in the smart contract as its only feature. This model

has an accuracy, recall, and F1-score of 98.6% and a precision of 98.7%.

Further analysis at the feature level indicated that the success of our model seemed

to be predicated on the high discriminatory value of the frequency of DUP9 in a

smart contract, but did not appear to result from code duplication across smart

contracts. We acknowledge that the high level of reliance on a single feature means

that fraudsters could evade detection with small adjustments to their token smart

contract code. This is a common challenge for detection systems against what is

likely to be an adaptive target. Therefore, while a machine learning classiﬁer may

not yet be necessary for this problem, as more violations (and more sophisticated

ones) are committed, it may become necessary. However, whether machine learning

methods would be capable of similar performance in this more challenging context

cannot be known a priori —future research could test this.

Comparisons with prior research

Since ours is the ﬁrst study to attempt to classify DeFi securities violations, we

are unable to compare our model’s performance with previous studies. We do note

that other studies which successfully built high performance classiﬁers for a related

classiﬁcation problem used more complex methods, which improved upon several

previous studies addressing the same classiﬁcation problem [38]. In addition, many

Trozze et al.

Page 25 of 38

previous studies, like ours, also used data with quite imbalanced classes, but they

did not always account for this in building their models. Fan et al. [41], in par-

ticular, criticize previous work, including [43], on this basis. This may make the

high performance metrics reported by some other studies slightly misleading. For

example, only 3.6% of the smart contracts in the Chen et al. [40] dataset are Ponzi

schemes.

In contrast to previous studies, ours considers whether machine learning classiﬁca-

tion is necessary or whether a simpler model may suﬃce to solve the same problem.

Previous studies have found that models built with, for example, logistic regression,

have been much less eﬀective than more complex ones [14]. However, our model’s

heavy reliance on a single feature suggests a simpler model may be suitable for our

speciﬁc classiﬁcation task. It would be useful for future research to explore this,

particularly as oﬀending in the space becomes more sophisticated.

The importance of the DUP9 feature

Interestingly, the frequency of a data processing opcode (DUP9) [45] was the most

important feature in this investigation and was the key to our model’s functioning.

The DUP9 opcode is a generic organizational function, and so it is unclear why its

use would indicate fraudulent activity. Though they addressed a slightly diﬀerent

classiﬁcation problem, other work on detecting Ponzi schemes in smart contracts [38]

also found seemingly innocuous opcodes to be highly important to their classiﬁer.

The high levels of performance of our single-feature model were unexpected given

the assumed complexity of our classiﬁcation problem. Ultimately, we cannot be cer-

tain there are no confounds, though our feature-level and smart contract similarity

analyses did not expose any. This analysis rather revealed merely that the frequency

of DUP9 in a smart contract is an almost perfectly distinguishing variable between

securities violations tokens’ and legitimate tokens’ smart contract code.

Trozze et al.

Page 26 of 38

However, we acknowledge there could still be some unknown issues given the ex-

tremely high accuracy of our classiﬁer. There may be something about the DUP9

opcode that makes it conducive to illegality which we have not been able to iden-

tify—if this is the case, more research is needed into this opcode and its role. Other-

wise, the importance of DUP9 we see here may be spurious, arising from some bias

in the data. The number of known violations in our dataset is very small and there

could potentially be something distinctive about these cases. If this is the case—and

the cases that have been prosecuted really are not representative of violations in

general—this classiﬁcation task is futile until a better ground truth is available.

The reliance on a single, programmatic opcode in classiﬁcation may become an

issue if nefarious developers become aware of this classiﬁer. They could relatively

easily avoid including DUP9 in their code to avoid classiﬁcation as a securities

violation, rather than needing to rely on more complex adversarial techniques. Ulti-

mately, DUP9 is merely a data processing opcode and related opcodes from DUP1-

DUP16 exist. Therefore, conceivably, developers could obtain the same outcome by

changing the order in which data is added to the stack (for example, pushing incon-

sequential information to the stack before that which they want to duplicate and

then using DUP10 instead of DUP9)[21]. Other scholars have raised similar issues

surrounding publicizing the workings of these classiﬁers and potential evasion by

developers, noting the importance of the constant evolution of these models [38].

Other noteworthy features

What was perhaps most surprising was the relative importance of the frequency

of deliberately invalid opcodes on the classiﬁcation outcomes, as well as the high

frequency of such opcodes even in smart contracts for legitimate projects. There

are a variety of reasons why an INVALID opcode might occur; INVALID opcodes

are used, for example, for internal exceptions and to instruct the EVM to use the

[21]Chen et al. [38] similarly consider it straightforward to change the bytecode of a smart contract
while preserving its functionality. They highlight one tool, BiAn, which “is a source-code level
obfuscation tool developed for Solidity smart contracts.”

Trozze et al.

Page 27 of 38

remaining gas and revert changes [71, 72]. This may be why it was one of the most

important features in our models but its mean frequencies were not signiﬁcantly dif-

ferent between the securities violations and legitimate classes. Alternatively, there

may potentially be some interaction between the INVALID feature and other fea-

tures, which further research could explore.

Like our model, Chen et al.’s [38] XGBoost algorithm classed an opcode which

stores a word (MSTORE in our model, SSTORE in Chen et al.’s [38]) as one

of its most important features. However, other features their model took as most

important, such as SLOAD and CALLDATALOAD were not among our 10 most

important features. Wang et al. [45] also found that call functions were more com-

mon in Ponzi scheme smart contracts than other contracts, as well as jump func-

tions. Though we did not ﬁnd this to be the case with respect to call functions,

JUMPDEST was one of the most frequently occurring opcodes on average for smart

contracts in our securities violations class; however, JUMP was in the top 10 most

frequent opcodes on average for legitimate tokens.

Like Chen et al. [38], we note the overall lack of interpretability of the results

of classiﬁers with code-based features. The opcodes that we (and others) observe

have no obvious interpretation with respect to illegal activity, but, equally, there

are not any opcodes which would oﬀer a straightforward interpretation—i.e., there

is no “STEAL” opcode or similar. Therefore, without dissecting the contracts, it

would be diﬃcult to draw any deﬁnitive conclusions simply from the opcode-based

features. Perhaps future work could manually examine the Solidity code for the 47

violating contracts to understand how they work and whether any characteristics

of their code may be responsible for illegality.

The use of code-based features

Like previous work [43, 38], we emphasize the usefulness of our classiﬁer immediately

upon deployment of the smart contract to the Ethereum blockchain and regardless

Trozze et al.

Page 28 of 38

of how many wallets interact with it. This is one of the key advantages of using code-

only features for classiﬁcation rather than transaction- or account-based features.

This makes such a model not only useful as a retroactive tool for investigators,

but also to prevent future fraud. It also enables investigators to monitor projects

which may engage in securities violations in the future. Since investigations and

prosecutions take such a long time (upwards of several years for complex cases),

it is important for prosecutors to be able to begin gathering evidence as early as

possible. However, we acknowledge that code-based analysis is merely one technique

among many; future research could explore alternatives.

Potential applications of our model

While it is not certain that machine learning is necessary, it is clear that eﬀec-

tive classiﬁcation is possible and so there is potential value in exploring the use of

computational triage systems. This is particularly important given that U.S. en-

forcement agencies seem to be relying heavily on submissions to their whistleblower

programs [73]; a machine learning model could reduce reliance on whistleblowers

and also avoid the government needing to pay out a portion of funds successfully

recovered to whistleblowers (which can be millions of dollars [74]).

A classiﬁer of the type we examine here would be more useful as a triage measure

rather than a source of evidence due to issues surrounding the admissibility of ma-

chine learning-generated evidence in U.S. courts. There may be questions about its

admissibility under the Fifth Amendment, the Sixth Amendment, and the Federal

Rules of Evidence, however legal scholars do not, ultimately, consider them impedi-

ments to its admission [75].[22] Even if it is admissible, however, questions about its

[22]Though a complete discussion of the admissibility of machine learning evidence is outside of
the scope of this paper, we provide a brief introduction here. The Fifth Amendment relates to an
individual’s right to due process (this could arise in the context of the “black box” of machine
learning calculations) and the Sixth Amendment includes the Confrontation Clause. This “black
box” not only refers to inexplicable machine learning algorithms but also lay people’s likely lack
of understanding about how these algorithms work. The Confrontation Clause requires experts
to testify in person and submit to cross-examination. However, this is unlikely to be an issue, as
the testimony of a machine learning expert should be satisfactory. The Federal Rules of Evidence
around relevance, prejudice, and authenticity may be pertinent as well. Lawyers must further prove
the accuracy of the evidence [76]. Some argue that under Rule 702 and Daubert v. Merrell Dow

Trozze et al.

Page 29 of 38

weight in court remain. In particular, explaining such evidence to a judge and jury

(especially the “black box” calculations involved in developing machine learning

models) may lead it being discounted. There is variation in levels of trust among

jurors in machines in general and jurors must further trust the expert testimony

which explains the machine learning tool [75]. This is exacerbated by the need for

prosecutors to already explain complex concepts related to cryptocurrency in these

cases. A machine learning-based tool would likely lead investigators to more com-

pelling transaction-based evidence or qualitative evidence (for example, marketing

material) which can be more easily understood by a judge and jury and which has

been eﬀective in prosecuting cryptocurrency-based ﬁnancial criminal oﬀences in the

past (See [77, 78]).

Considering the ambiguity around the Hinman standard[23]

in determining

whether a project is suﬃciently decentralized to avoid being classed as a security, a

machine learning model could also serve as an additional tool in developers’ arsenal

for determination thereof. Finally, we also perceive a machine learning model as po-

tentially useful for people who are interested in participating in the DeFi ecosystem,

as a way for them to research the validity of new projects to help protect themselves

from fraud.

Limitations and future research

Our research may suﬀer from various limitations. The ﬁrst is the potential for over-

ﬁtting our model, particularly in the face of imbalanced data [41]. However, since

we do not have a separate dataset with known labels on which to test our model,

overﬁtting could remain an issue despite our mitigation eﬀorts. Ultimately, we chose

a veriﬁed ground truth dataset that was much smaller than our set of legitimate

tokens. We do note that our dataset may be making this classiﬁcation problem

Pharmaceuticals, machine learning evidence would be admissible as expert testimony. Through
Daubert, the court developed a set of four considerations for evaluating expert testimony [75].
[23]The “Hinman standard” refers to William Hinman’s 2018 speech which considered the level of
decentralization of a project critical to determining whether it should be classed as a security [34].

Trozze et al.

Page 30 of 38

simpler than it is in reality. We chose a list of generally reputable projects for our

legitimate token set and those which are subject to government enforcement action

for our securities violations. In this sense, we believe that, given the experimental

nature of DeFi (and the high risk appetite of its participants), this set should cap-

ture projects that exist in the middle, rather than at only extremes of oﬀending and

non-oﬀending, which may be harder to classify. However, it would still be useful for

future research to develop more datasets of DeFi securities violations to further test

our model.

We accept that there are some limitations around the use of a classiﬁer like this in

practice. Our future research will speciﬁcally explore how we could use a classiﬁer

like this to conduct an investigation and build a viable legal case. This will involve

performing manual, in-depth analysis on tokens ﬂagged when applying our classiﬁer

to other datasets and interactions with their smart contracts (similarly to Xia et

al.’s [14] work on Uniswap scam tokens).

Finally, Chen et al. [38] raise the issue of bad actors using adversarial obfuscation

methods to trick classiﬁers like the one we propose. We did not explicitly account

for this possibility in building or model, nor did we test our model against known

obfuscation techniques. However, this may be a useful avenue for future research to

explore.

Conclusions

We achieve very high classiﬁcation performance (98.6% F1-score against a base-

line of 50%) of DeFi-based securities violations based on a single feature from the

projects’ tokens’ smart contract code: the frequency with which DUP9 occurred in

the contract. Essentially, our random forest classiﬁer served to identify an optimal

cutoﬀ for this variable in deciding whether a DeFi project may be engaging in se-

curities violations. Though further analysis did not reveal any confounds from the

candidate confounds we tested, the reliance of our model on a single feature raises

Trozze et al.

Page 31 of 38

concerns about its use in practice and means that perhaps a machine learning tool

is not strictly necessary for this classiﬁcation problem at present. Overall, a ma-

chine learning model like ours would be highly useful for investigators, but could

be circumvented by nefarious developers in the future. It is important, therefore, to

augment any model as further DeFi projects engaging in securities violations are

revealed. It may be the case that, as DeFi securities violations oﬀenses become more

sophisticated, a complex machine learning model is better suited to their detection.

This work constitutes the ﬁrst classiﬁer of securities violations in the emerging and

fast-growing DeFi ecosystem and is a useful ﬁrst step in tackling the documented

problem of DeFi fraud. Our work also contributes a novel dataset of DeFi securi-

ties violations with a veriﬁed ground truth and connects the use of such a classifer

with the wider legal context, including how law enforcement can use it from the

investigative to prosecution stages of a case.

Declarations

Availability of data and materials

The datasets generated and analysed during the current study are available in the OSF repository, Detecting Deﬁ

Securities Violations.

Competing interests

The authors declare that they have no competing interests.

Funding

This work was funded by the UK EPSRC grant EP/S022503/1 that supports the Centre for Doctoral Training in

Cybersecurity at UCL.

Author’s contributions

AT: conceptualization, data collection, analysis, interpretation, and drafting the ﬁnal manuscript. BK and TD:

conceptualization, study design, and feedback on the manuscript. All authors have reviewed the ﬁnal manuscript.

Acknowledgements

The authors also thank Lawrence Binding, Antonis Papasavva, and Antoine Vendeville for their contributions to our

code.

Author details

1Department of Computer Science, University College London, Gower Street WC1E 6EA, London, UK.

2Department of Security and Crime Science, University College London, 35 Tavistock Square, WC1H 9EZ London,

UK. 3Department of Methodology & Statistics, Tilburg University, Warandelaan 2, 5037 AB Tilburg, Netherlands.

Trozze et al.

Page 32 of 38

References

1. Bartoletti, M., Carta, S., Cimoli, T., Saia, S.: Dissecting ponzi schemes on ethereum: Identiﬁcation, analysis,

and impact. Future Generation Computer System 102, 259–277 (2020). doi:10.1016/j.future.2019.08.014

2. Binance Academy: Blockchain. https://academy.binance.com/en/glossary/blockchain Accessed 25

November 2021

3. Narayanan, A., Bonneau, J., Felten, E., Miller, A., Goldfeder, S.: Bitcoin and Cryptocurrency Technologies

(2016)

4. Gapusan, J.: DeFi: Who Will Build The Future Of Finance? (2 November 2021). https:

//www.forbes.com/sites/jeffgapusan/2021/11/02/defi-who-will-build-the-future-of-finance/

Accessed 18 November 2021

5. Wintermeyer, L.: After Growing 88x In A Year, Where Does DeFi Go From Here? (2 November 2021).

https://www.forbes.com/sites/lawrencewintermeyer/2021/05/20/

after-growing-88x-in-a-year-where-does-defi-go-from-here/ Accessed 18 November 2021

6. CipherTrace: Cryptocurrency Crime and Anti-Money Laundering Report, August 2021 (August 2021).

https://ciphertrace.com/cryptocurrency-crime-and-anti-money-laundering-report-august-2021/

7. Eversheds Sutherland Ltd.: Navigating the issues securities enforcement global update. Report (2018).

https://us.eversheds-sutherland.com/mobile/portalresource/lookup/poid/

Z1tOl9NPluKPtDNIqLMRV56Pab6TfzcRXncKbDtRr9tObDdEpW3CmS3!/fileUpload.name=

/Securities-Enforcement-Global-Update_Fall-2018.pdf

8. Musiala, R.A.J., Goody, T.M., Reynolds, V., Tenery, L., McGrath, M., Rowland, C., Sekhri, S.:

Cryptocurrencies: Forensic techniques to meet the challenge of new fraud and corruption risks — FVS Eye on

Fraud. Report, AICPA (Winter 2020). https://future.aicpa.org/resources/download/

cryptocurrencies-forensic-techniques-to-face-new-fraud-and-corruption-risks

9. Podgor, E.S.: Cryptocurrencies and securities fraud: In need of legal guidance. Available at SSRN 3413384

(2019)

10. FBI: Securities Fraud Awareness & Prevention Tips.

https://www.fbi.gov/stats-services/publications/securities-fraud Accessed 18 November 2021

11. Practical Law Corporate & Securities: US Securities Laws: Overview. Practice Note 3-383-6798, Thomson

Reuters

12. Kamps, J., Trozze, A., Kleinberg, B.: forthcoming. In: Wood, S., Hanoch, Y. (eds.) Cryptocurrency Fraud. A

Fresh look at Fraud: Theoretical and Applied Approaches. Routledge, forthcoming (2022)

13. Trozze, A., Kamps, J., Akartuna, E.A., Hetzel, F., Kleinberg, B., Davies, T., Johnson, S.: Cryptocurrencies and

future ﬁnancial crime. Crime Science (2022)

14. Xia, P., wang, H., Gao, B., Su, W., Yu, Z., Luo, X., Zhang, C., Xiao, X., Xu, G.: Demystifying scam tokens on

uniswap decentralized exchange. arXiv:2109.00229 [cs] (2021)

15. Hu, T., Liu, X., Chen, T., Zhang, X., Huang, X., Niu, W., Lu, J., Zhou, K., Liu, Y.: Transaction-based

classiﬁcation and detection approach for ethereum smart contract. Information Processing & Management

58(2), 102462 (2021). doi:10.1016/j.ipm.2020.102462

16. Cai, W., Wang, Z., Ernst, J.B., Hong, Z., Feng, C., Leung, V.C.M.: Decentralized applications: The

blockchain-empowered software system 6, 53019–53033. doi:10.1109/ACCESS.2018.2870644. Conference

Name: IEEE Access

17. Sch¨ar, F.: Decentralized ﬁnance: On blockchain- and smart contract-based ﬁnancial markets.

doi:10.20955/r.103.153-74. Accessed 2022-03-22

Trozze et al.

Page 33 of 38

18. Wood, G.: Ethereum: A Secure Decentralised Generalised Transaction Ledger. Ethereum (2 November 2021).

https://ethereum.github.io/yellowpaper/paper.pdf

19. Perkis, T.: Stack-based genetic programming. In: Proceedings of the First IEEE Conference on Evolutionary

Computation. IEEE World Congress on Computational Intelligence, pp. 148–1531.

doi:10.1109/ICEC.1994.350025

20. Crytic: Ethereum VM (EVM) Opcodes and Instruction Reference (2021).

https://github.com/crytic/evm-opcodes Accessed 17 November 2021

21. Uniswap: Uniswap Governance (2021). https://gov.uniswap.org/ Accessed 22 November 2021

22. Secret Network: About Secret (SCRT) (2021). https://scrt.network/ Accessed 22 November 2021

23. BitcoinWiki: ERC20 Token Standard – Ethereum Smart Contracts – BitcoinWiki (3 February 2021).

https://en.bitcoinwiki.org/wiki/ERC20 Accessed 18 November 2021

24. Hertig, A.: What Is DeFi? (18 September 2020). https://www.coindesk.com/learn/what-is-defi/

Accessed 25 October 2021

25. Ethereum: Ethereum Wallets (23 October 2021). https://ethereum.org/en/wallets/ Accessed 25 October

2021

26. Xu, J., Paruch, K., Cousaert, S., Feng, Y.: SoK: Decentralized exchanges (DEX) with automated market maker

(AMM) protocols. 2103.12732. Accessed 2022-03-22

27. Bartoletti, M., Chiang, J.H.-y., Lluch-Lafuente, A.: SoK: Lending pools in decentralized ﬁnance. 2012.13230.

Accessed 2022-03-22

28. Jagati, S.: DeFi lending and borrowing, explained (18 January 2021).

https://cointelegraph.com/explained/defi-lending-and-borrowing-explained Accessed 25 October

2021

29. Binance Academy: Blockchain Use Cases: Prediction Markets (29 April 2021).

https://academy.binance.com/en/articles/blockchain-use-cases-prediction-markets Accessed 25

October 2021

30. Coinbase: Around the Block #14: DeFi insurance (13 May 2021).

https://blog.coinbase.com/around-the-block-14-defi-insurance-ebf8e278da13 Accessed 25 October

2021

31. DeFi Prime: DeFi and Open Finance. https://defiprime.com/ Accessed 25 October 2021

32. Parra Moyano, J., Ross, O.: KYC optimization using distributed ledger technology. Business & Information

Systems Engineering 59(6), 411–423 (2017). doi:10.1007/s12599-017-0504-2

33. FATF: Updated guidance for a risk-based approach to virtual assets and virtual asset service providers. Report

(October 2021).

https://www.fatf-gafi.org/media/fatf/documents/recommendations/Updated-Guidance-VA-VASP.pdf

34. Blockchain Association: Understanding the SEC’s Guidance on Digital Tokens: The Hinman Token Standard

(2021). https://theblockchainassociation.org/

understanding-the-secs-guidance-on-digital-tokens-the-hinman-token-standard/ Accessed 8

November 2021

35. Securities and Exchange Commission V. AriseBank, Jared Rice Sr., and Stanley Ford. N.D. Tex. (23 January

2020). No. 3-18-cv-0186-M

36. Securities and Exchange Commission V. PlexCorps, Dominic LaCroix, and Sabrina Paradis-Royer. E.D.N.Y. (2

October 2019). No. 17-cv-7007 (CBA) (RML)

37. Securities and Exchange Commission V. REcoin Group Foundation, LLC, DRC World INC. A/k/a Diamond

Trozze et al.

Page 34 of 38

Reserve Club, and Maksim Zaslavskiy. E.D.N.Y (14 May 2018). Civil Action No. 17-cv-05725

38. Chen, W., Li, X., Sui, Y., He, N., Wang, H., Wu, L., Luo, X.: Sadponzi: Detecting and characterizing ponzi

schemes in ethereum smart contracts. Proceedings of the ACM on Measurement and Analysis of Computing

Systems 5(2), 26–12630 (2021). doi:10.1145/3460093

39. Cai, W., Wang, Z., Ernst, J.B., Hong, Z., Feng, C., Leung, V.C.M.: Decentralized applications: The

blockchain-empowered software system. IEEE Access 6, 53019–53033 (2018).

doi:10.1109/ACCESS.2018.2870644

40. Chen, W., Zheng, Z., Cui, J., Ngai, E., Zheng, P., Zhou, Y.: Detecting ponzi schemes on ethereum: Towards

healthier blockchain technology. In: Proceedings of the 2018 World Wide Web Conference on World Wide Web

- WWW ’18, pp. 1409–1418. doi:10.1145/3178876.3186046

41. Fan, S., Fu, S., Xu, H., Cheng, X.: Al-spsd: Anti-leakage smart ponzi schemes detection in blockchain.

Information Processing & Management 58(4), 102587 (2021). doi:10.1016/j.ipm.2021.102587

42. Hu, H., Xu, Y.: Scsguard: Deep scam detection for ethereum smart contracts. arXiv:2105.10426 [cs] (2021)

43. Jung, E., Le Tilly, M., Gehani, A., Ge, Y.: Data mining-based ethereum fraud detection, pp. 266–273. IEEE

44. Liu, L., Tsai, W.-T., Bhuiyan, M.Z.A., Peng, H., Liu, M.: Blockchain-enabled fraud discovery through abnormal

smart contract detection on ethereum. Future Generation Computer Systems 128, 158–166 (2022).

doi:10.1016/j.future.2021.08.023

45. Wang, L., Cheng, H., Zheng, Z., Yang, A., Zhu, X.: Ponzi scheme detection via oversampling-based long

short-term memory for smart contracts. Knowledge-Based Systems 228, 107312 (2021).

doi:10.1016/j.knosys.2021.107312

46. Zhang, Y., Yu, W., Li, Z., Raza, S., Cao, H.: Detecting ethereum ponzi schemes based on improved lightgbm

algorithm. IEEE Transactions on Computational Social Systems, 1–14 (2021). doi:10.1109/TCSS.2021.3088145

47. Chen, W., Zheng, Z., Ngai, E., C.-H., Zheng, P., Zhou, Y.: Exploiting blockchain data to detect smart ponzi

schemes on ethereum. IEEE Access 7, 37575–37586 (2019)

48. Chen, L., Fan, Y., Ye, Y.: Adversarial reprogramming of pretrained neural networks for fraud detection. CIKM

’21, pp. 2935–2939. Association for Computing Machinery, New York, NY, USA. doi:10.1145/3459637.3482053

49.

Ibrahim, R.F., Mohammad Elian, A., Ababneh, M.: Illicit account detection in the ethereum blockchain using

machine learning. In: 2021 International Conference on Information Technology (ICIT), pp. 488–493.

doi:10.1109/ICIT52682.2021.9491653

50. Laˇsas, K., Kasputyt˙e, G., Uˇzupyt˙e, R., Krilaviˇcius, T.: Fraudulent behaviour identiﬁcation in ethereum

blockchain

51. Li, J., Baldimtsi, F., Brandao, J.P., Kugler, M., Hulays, R., Showers, E., Ali, Z., Chang, J.: Measuring illicit

activity in deﬁ: The case of ethereum. Lecture Notes in Computer Science, pp. 197–203. Springer, Berlin,

Heidelberg. doi:10.1007/978-3-662-63958-0 18

52. Wilder, R.P., Heidi: Tracing cryptocurrency scams: Clustering replicated advance-fee and phishing websites.

arXiv preprint arXiv:2005.14440 (2020)

53. Chen, W., Guo, X., Chen, Z., Zheng, Z., Lu, Y., Li, Y.: Honeypot contract risk warning on ethereum smart

contracts, pp. 1–8. IEEE

54. Karimov, B., W´ojcik, P.: Identiﬁcation of scams in initial coin oﬀerings with machine learning. Frontiers in

Artiﬁcial Intelligence 4, 718450 (2021). doi:10.3389/frai.2021.718450

55. Wu, J., Lin, D., Zheng, Z., Yuan, Q.: T-edge: Temporal weighted multidigraph embedding for ethereum

transaction network analysis. Frontiers in Physics 8, 204 (2020). doi:10.3389/fphy.2020.00204

Trozze et al.

Page 35 of 38

56. Bartoletti, M., Carta, S., Cimoli, T., Saia, S.: Dissecting Ponzi schemes on Ethereum (2019)

57. Uniswap: Introducing Token Lists (26 August 2020). https://uniswap.org/blog/token-lists Accessed 18

November 2021

58. Commission, U.S.S.a.E.: Cyber Enforcement Actions (19 January 2022).

https://www.sec.gov/spotlight/cybersecurity-enforcement-actions Accessed 10 February 2022

59. Securities and Exchange Commission V. Natural Diamonds Investment Co., Eagle Financial Diamond Group Inc

A/k/a Diamante Atelier, Argyle Coin, LLC, Jose Angel Aman, Harold Seigel, and Jonathan H. Seigel. S.D. Fla.

(11 December 2019). 19-cv-80633

60. Securities and Exchange Commission V. Reginald Middleton, et Al. E.D.N.Y. (1 November 2019). 19-cv-4625

61. U.S. Securities and Exchange Commission: 2021 Annual Report to Congress Whistleblower Program.

https://www.sec.gov/files/2021_OW_AR_508.pdf Accessed 2022-04-04

62. Zapper: Your Homepage to DeFi (2021). https://zapper.fi/ Accessed 18 November 2021

63. zes: Is it safe to Zap into all liquidity pools on Zapper? (2020). https:

//zapper.crunch.help/zapper-fi-faq/is-it-safe-to-zap-into-all-liquidity-pools-on-zapper

Accessed 18 November 2021

64. Crytic: Pyevmasm. Crytic (2020)

65. Blagus, R., Lusa, L.: Joint use of over- and under-sampling techniques and cross-validation for the development

and assessment of prediction models. BMC Bioinformatics 16(1), 363 (2015). doi:10.1186/s12859-015-0784-9

66. Chawla, N.V., Bowyer, K.W., Hall, L.O., Kegelmeyer, W.P.: Smote: Synthetic minority over-sampling

technique. Journal of Artiﬁcial Intelligence Research 16, 321–357 (2002). doi:10.1613/jair.953

67. Prellberg, J., Kramer, O.: Acute lymphoblastic leukemia classiﬁcation from microscopic images using

convolutional neural networks. arXiv:1906.09020 [cs] (2020)

68. Sieradski, D.: Gas and fees (2021). https://ethereum.org/en/developers/docs/gas/

69. He, N., Wu, L., Wang, H., Guo, Y., Jiang, X.: Characterizing code clones in the ethereum smart contract

ecosystem. arXiv:1905.00272 [cs] (2019)

70. Han, J., Kamber, M., Pei, J.: Getting to know your data. In: Han, J., Kamber, M., Pei, J. (eds.) Data Mining

(Third Edition). The Morgan Kaufmann Series in Data Management Systems, pp. 39–82. Morgan Kaufmann.

doi:10.1016/B978-0-12-381479-1.00002-2.

https://www.sciencedirect.com/science/article/pii/B9780123814791000022 Accessed 2022-04-19

71. chriseth: Solidity (31 January 2017). https://github.com/ethereum/solidity/releases/tag/v0.4.9

72. Ethereum: Ethereum Glossary. https://ethereum.org/uk/glossary/ Accessed 25 November 2021

73. Commodity Futures Trading Commission: CFTC Whistleblower Alert: Be on the Lookout for Virtual Currency

Fraud (May 2019).

https://www.whistleblower.gov/whistleblower-alerts/Virtual_Currency_WBO_Alert.htm Accessed 22

November 2021

74. U.S. Securities and Exchange Commission: SEC Awards $22 Million to Two Whistleblowers (2021).

https://www.sec.gov/news/press-release/2021-81 Accessed 22 November 2021

75. Nutter, P.W.: Machine learning evidence: Admissibility and weight comments. University of Pennsylvania

Journal of Constitutional Law 21(3), 919–958 (2018)

76. Grossman, P.G., Maura: Artiﬁcial intelligence as evidence. In: Maryland State Bar Association Young Lawyer’s

Section (25 August 2021)

77. United States V. Costanzo. United States District Court, D. Arizona (10 August 2018). No.

2:17-cr-00585-GMS

Trozze et al.

Page 36 of 38

78. United States V. Murgio. United States District Court for the Southern District of New York (19 September

2016). 15-cr-769 (AJN)

79. Rahman, A., Nahid, N., Hassan, I., Ahad, M.A.R.: Nurse care activity recognition: using random forest to

handle imbalanced class problem. UbiComp-ISWC ’20, pp. 419–424. Association for Computing Machinery,

New York, NY, USA. doi:10.1145/3410530.3414334

Additional Files

Additional ﬁle 1 — List of abbreviations

List of abbreviations referred to in text.

Additional ﬁle 2 — Data

The datasets generated and analysed during the current study are available in the OSF repository,

https://osf.io/xcdz6/?view_only=5a61a06ae9154493b67b24fa4979eddb.

Appendix I

In addition to our models reported in our manuscript, we built an initial classiﬁ-

cation model using the frequency of all opcodes contained in our dataset (a total

of 129 features). The results of the evaluation metrics for this model are in Table

9. We then calculated the relative importance of the model’s features to its per-

formance, the results of which are reported in Table 10. We used stratiﬁed K-fold

cross validation on our data, with 10 folds. We chose stratiﬁed cross validation due

to the class imbalance in our dataset [79].

Table 9 Standard model performance.

Accuracy Precision Recall F1-score

Full-feature random forest
10-feature random forest
Top two-feature random forest
Top-feature random forest

0.9751
0.9756
0.9780
0.9756

0.9649
0.9735
0.9795
0.9816

0.9751
0.9756
0.9780
0.9756

0.9685
0.9734
0.9780
0.9769

Next, we built a model using only the 10 features with the highest importance in

our original model, then the two most important features, and, ﬁnally, the single

most important feature. The precision, recall, accuracy, and F1-score for these mod-

els are reported in Table 9. We also assessed the feature importance for our further

models, which is reported in Table 10. Using the F1-score as our primary metric, we

achieved the best performance with the two-feature model, followed by our mod-

Trozze et al.

Page 37 of 38

Two-feature model
Importance
Feature
0.81
DUP9
0.19
INVALID

Table 10 Feature importance.

Full-feature model

10-feature model

Feature
DUP9
INVALID
SWAP13
SWAP9
TIMESTAMP
REVERT
SWAP2
SHR
MSTORE
PUSH4

Importance
0.33
0.06
0.02
0.02
0.01
0.01
0.01
0.01
0.01
0.01

Feature
DUP9
INVALID
MSTORE
SWAP9
SHR
SWAP2
TIMESTAMP
PUSH4
REVERT
SWAP13

Importance
0.57
0.22
0.04
0.04
0.03
0.03
0.03
0.02
0.02
0.02

els with one and ten features. Our full-feature random forest classiﬁer performed

relatively less well.

While this performance appears very high, it is misleading due to our imbal-

anced classes; if the classiﬁer predicted every smart contract as legitimate, it would

achieve 97.75% accuracy, which is, in fact, better than the performance for some of

our models. After building these models with our veriﬁed (but small) ground truth

dataset of securities violations, we adjusted our models to address the classiﬁcation

imbalance in our data. Following [41], we built models on data augmented with

the Synthetic Minority Over-Sampling Technique (SMOTE) to train our data. In

short, SMOTE combines majority class under-sampling and minority class over-

sampling (including through synthesizing additional data for the minority class)

[66]. Our best performing model using SMOTE was a random forest classiﬁcation

model including all our 129 features, trained on data sampled with SMOTE, with an

F1-score of 99.39%. Overall, we achieved higher performance for all other combina-

tions of features tested when we trained our model with SMOTE data, rather than

our original, imbalanced dataset, particularly relative to our new 50% baseline. By

over-sampling from our securities violations class and synthesizing additional data,

SMOTE returned a set of 2041 securities violations class members and 2041 legiti-

Trozze et al.

Page 38 of 38

mate token class members. The performance metrics for these models are reported

in Table 11.

Table 11 Model performance with SMOTE.

Accuracy Precision Recall F1-score

Full-feature model with SMOTE
10-feature RF with SMOTE
Two-feature RF with SMOTE
Single-feature RF with SMOTE

0.9939
0.9880
0.9890
0.9890

0.9939
0.9881
0.9893
0.9893

0.9939
0.9880
0.9890
0.9890

0.9939
0.9880
0.9890
0.9890

We also tried using Borderline SMOTE, which oversamples cases near the bound-

ary of two classes from the minority class. However, this did not return a higher

number of samples in our securities violations class. The fact that each of our ex-

amples seems to ﬁt squarely within its class may account, in part, for the high

performance of our models.

Table 12 Feature importance for models with SMOTE.

Full-feature model

10-feature model

Feature
DUP9
SHR
DUP10
INVALID
SWAP13
CALL
MSTORE
ORIGIN
REVERT
SWAP9

Importance
0.50
0.03
0.03
0.03
0.03
0.02
0.02
0.02
0.02
0.02

Feature
DUP9
SWAP9
SHR
ORIGIN
REVERT
INVALID
SWAP13
CALL
DUP10
MSTORE

Importance
0.82
0.04
0.04
0.03
0.02
0.02
0.01
0.01
0.00
0.00

Two-feature model
Importance
Feature
0.986
DUP9
0.014
SHR

The feature importance for the models employing SMOTE is reported in Ta-

ble 12. Notably, the most important features in our models changed when we use

SMOTE, with the addition of DUP10, CALL, and ORIGIN to the most important

features, and the exclusion of TIMESTAMP, SWAP2, and PUSH4. However, the

most important feature, DUP9 remained consistent, and was much more critical to

the model than the remaining nine features.

