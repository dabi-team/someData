1
2
0
2

n
u
J

8
1

]

G
L
.
s
c
[

1
v
6
7
1
0
1
.
6
0
1
2
:
v
i
X
r
a

Self-supervised Incremental Deep Graph Learning
for Ethereum Phishing Scam Detection

Shucheng Li, Fengyuan Xu∗, Runchuan Wang, Sheng Zhong
National Key Lab for Novel Software Technology, Nanjing University, China
shuchengli@smail.nju.edu.cn

Abstract

In recent years, phishing scams have become the crime type with the largest
money involved on Ethereum, the second-largest blockchain platform. Meanwhile,
graph neural network (GNN) has shown promising performance in various node
classiﬁcation tasks. However, for Ethereum transaction data, which could be
naturally abstracted to a real-world complex graph, the scarcity of labels and the
huge volume of transaction data make it difﬁcult to take advantage of GNN methods.
Here in this paper, to address the two challenges, we propose a Self-supervised
IncrEmental deep Graph lEarning model (SIEGE), for the phishing scam detection
problem on Ethereum. In our model, two pretext tasks designed from spatial and
temporal perspectives help us effectively learn useful node embedding from the
huge amount of unlabelled transaction data. And the incremental paradigm allows
us to efﬁciently handle large-scale transaction data and help the model maintain
good performance when the data distribution is drastically changing. We collect
transaction records about half a year from Ethereum and our extensive experiments
show that our model consistently outperforms strong baselines in both transductive
and inductive settings.

1

Introduction

Ethereum [33], one of the most popular and scalable blockchains, has reached $28 billion market cap
so far. It unleashes the full potential of smart contracts which automatically manage and approve
transactions without a centralized entity, and thus attracts a lot of attention and resources via its
various decentralized ﬁnance (DeFi) applications atop of smart contracts. However, the thriving of
Ethereum also puts millions of its users at risk of malicious attacks [16]. Especially, many of these
attacks, such as the phishing scam [5][8][32][34][37] and Ponzi scheme [1][9], aim to extort money
from their victims in Ethereum, which is hard to defend with the software security approaches or
smart contract analysis alone. Therefore, intelligent analytics at the transactional activity level is
necessary.

Recently, deep graph learning[24][28][14][19][15][30] has shown its superior ability to learn good
node representations for graphs, which is useful to downstream tasks. Since all transactional activities
in Ethereum can naturally form a large-scale transaction graph (formulation details are described
in Section 2.1), the effectiveness of security analytics could be greatly improved if leveraging the
advantages of deep graph learning methods on Ethereum. Unfortunately, there are two key challenges
when introducing deep graph learning on such a large-scale transaction graph for the task category of
anomaly detections.

The ﬁrst challenge is the data scalability. The size of the Ethereum transaction graph is extremely
large, with over 1,151,443,821 transactions (excluding internal transactions) so far. This graph

∗Corresponding author.

Preprint. Under review.

 
 
 
 
 
 
continues evolving by adding new transactions every second. After the deployment of Ethereum
2.0 in 2022, there will be thousands of transactions generated in a second. The other challenge
is the label scarcity. It is in general hard to obtain positive samples for the training of anomaly
detections. Moreover, it is usually hard to ﬁnd labeled data in recently-generated graph parts which
need inspections most, creating an issue of temporal labeling imbalance.

Existing works [5][34][37] are not able to handle large evolving graph and few positive labels. Instead,
most of them alleviate these two challenges via the graph sampling mechanism. The whole graph
is shrunk to a proper computable size by picking nodes and edges related to labeled positive data.
However, such a biased sampling process could lead to serious data distribution distortion and still
suffers from the issue of temporal labeling imbalance. Additionally, most of these methods are
transductive and not suitable to inductive scenarios like Ethereum and all other evolving graphs.

In this paper, we attempt to address in a different way the challenging anomaly detection tasks on
large-scale evolving graphs. First, we would like to take advantage of all data we have, rather than
getting rid of them. The self-supervised learning (SSL) path is taken in our work because it can
dig out more information through the supervision signals directly from data themselves without
worrying about annotations. The SSL effectiveness has been proven by many successful cases in
natural language processing, computer vision, and graph data [17]. However, existing unsupervised
methods on graphs [24][28][14][20][15][30] cannot directly apply onto the large-size graph, and
their pretext tasks cannot fully capture both spatial and temporal dynamics in the graph evolving.
Second, we therefore would like to design an incremental learning paradigm for such a large and
evolving graph category which represents many real-world graph learning scenarios. Last, we would
like to make our method inductive, and use the anomaly detection task of Ethereum phishing scam as
an example to demonstrate the effectiveness of our work to the downstream tasks in blockchains.

Therefore, we propose a Self-supervised IncrEmental deep Graph lEarning model (SIEGE) for
anomaly detection tasks like phishing scam detection on large scale evolving graphs like Ethereum
transaction graph. Instead of learning the whole graph at once, SIEGE splits the transaction graph
into pieces of suitable size (i.e. a set of Ehereum transaction blocks) and incrementally consumes
them one by one in the temporal order to learn the node representation. The intermediate learning
results of one graph piece are passed by our design to the next piece to enable incremental computing.
In this way, the size of the in-memory graph is containable and adaptive to changes up to date. For
the learning inside a graph piece, to address the label scarcity, we employ self-supervised learning to
generate the node embedding with an unsupervised representation learning approach, and design two
spatial and temporal pretext tasks to ensure that the ﬁnal node representations are as informative as
possible, with respect to the phishing scam detection task.

To demonstrate the effectiveness of SIEGE, we collected transaction data of about half a year
on Ethereum to build the graph dataset (more than 70 million transactions), and then extensive
experiments conducted in both transductive and inductive settings show that SIEGE consistently
outperforms baselines by 4% ~16% in F-1 score.

2 Related work

We mainly introduce two research directions related to this work here: 1) Ethereum phishing scam
detection; 2) deep graph learning. For 1), we will show some background knowledge about the basics
of Ethereum and the Ethereum transaction graph, and then describe the methodology of existing
works for phishing scam detection on Ethereum. We will also make some comparisons about the
difference between Ethereum and other blockchain platforms (especially Bitcoin). For 2), we will
ﬁrst introduce some basic GNN methods. Then, we will present the self-supervised learning for
graphs.

2.1 Ethereum phishing scam detection

Ethereum basics.
Inspired by Bitcoin[23], Ethereum[33] aims to become a blockchain platform of
the next generation that supports both the cryptocurrency and decentralized applications (Dapps). The
key is the designed smart contracts on Ethereum, which can be easily deployed in a transaction and
execute preset functions when called by another account. It provides convenience for users because it
allows for trusted transactions between users without a third party. Speciﬁcally in Ethereum, two

2

types of accounts existed: external accounts controlled by users and contract accounts with code
stored together. If we abstract the account as a node and the transaction as an edge, then we can get a
transaction graph. It should be added that for Bitcoin, there is no concept of "accounts" or "balance",
the basic block of Bitcoin is the unspent transaction output (UTXO), and the transaction in Bitcoin
can have multiple outputs and inputs, while the transaction in Ethereum is one-on-one. This is part
of the reason why we studied the Ethereum transaction graph. [7] ﬁrstly employs graph analysis to
study the characteristics (degree distribution, clustering coefﬁcients, etc.) of transaction graphs on
Ethereum and proposes a rule-based anomaly detection method. It provides some insights into the
behavior pattern of accounts in Ethereum.

Phishing scams in Ethereum. The thriving Ethereum market has also bred a lot of criminal
activities like phishing scam[16][5][8][34], Ponzi scheme[1][9], ransomware[10], etc. For the
phishing scam detection problem on Ethereum, there are two main categories of existing methods,
the former mainly employs shallow models such as 1) traditional machine learning methods with
dedicated feature engineering[8], and 2) some random-walk based network embedding methods[34],
such as DeepWalk[24], Node2Vec[14], etc. The latter applies some deep graph models like the graph
convolutional network[19] (GCN) based methods. These methods show pretty good performance in
their data collected. However, as mentioned in Section 1, two problems limit the practical application
of these methods. 1) Most of the existing methods are transductive, which make predictions in a
single ﬁxed graph. Therefore, they do not naturally generalize to unseen nodes or sub-graphs, and
for a high-throughput active blockchain platform like Ethereum, an inductive method to process
fast-evolving in-coming transaction data is in need. 2) A biased sampling process is employed in
graph data generation. To alleviate the label scarcity problem and scalability problem, existing
methods[5] generate the graph data by a random walk sampling process with labeled phishing nodes
as the start, which will result in different node distribution between the sampled graph and original
graph. Hence, in this paper, we propose SIEGE to learn useful node representations from the original
transaction without sampling. It is also inductive and can be used for phishing node detection in an
entirely new transaction graph.

2.2 Deep graph learning

Graph neural networks. Deep learning has been a great success in computer vision, speech
recognition, and natural language processing due to the powerful representation learning capability.
Recently, increasing attention is paid to how to apply deep learning for non-euclidean data like graphs,
which is widely used in various tasks, such as social network[12], protein interface prediction[13],
knowledge graph embedding[31], etc. In retrospect, graph neural networks also help with lots of tasks
in computer vision or natural language processing, like object detection[27], action recognition[26],
machine translation[3][2], semantic parsing[4][21][35], etc. Speciﬁcally, most GNNs exploit a
neighborhood aggregation mechanism that could incorporate information from both the node attributes
and graph topology. The node embedding generated can then be used for downstream tasks such as
node classiﬁcation, link prediction, graph classiﬁcation (an extra readout module in need), etc.

Self-supervised learning for graphs. Self-supervised learning (SSL) has been proved pretty useful
when a large volume of unlabelled data is available[11][6]. Compared to supervised learning usually
with manual annotations used as groundtruth, supervised learning aims to acquire the groundtruth
from the data itself by different pretext tasks, which could somehow alleviate the poor generalization
resulted from over-ﬁtting, and weak robustness faced with adversarial attacks[22]. For SSL in graph,
the inherent topology indicates that the nodes are not independent, which makes it difﬁcult for direct
use of existing frameworks[17]. However, rich structural information in graphs could also help the
design of the pretext tasks. In a nutshell, the design of the pretext task remains the crucial point
for SSL in graphs. Existing work of SSL[36][25][17] in graph mostly focus on simple graphs, i.e.,
homogeneous attributed graphs in a relatively small size. And how to apply SSL on large-scale
complex graphs remains to be studied. To the best of our knowledge, we are the ﬁrst work that
proposes a new pretext task for a large-scale temporal graph.

3

3 Methodology of SIEGE

In this section, we will introduce each module of SIEGE. In Section 3.1, we will present how to
design speciﬁc pretext tasks, which are divided into two parts, spatial pretext task, and temporal
pretext task. And in Section 3.2, we will discuss the usage of the incremental learning module in
SIEGE and then give an overview of SIEGE.

3.1 Self-supervised training

Before performing self-supervised learning, we assume that the original graph data G has been divided
into N splits: {G1, G2, · · · , GN } in the order of block number. For graph split i, Gi = (Vi, Ei, Xi),
where Vi = {v1, v2, · · · , vNi} is the set of Ni nodes, Ei stands for the edge set and Xi is the node
feature matrix. Moreover, we use Ai as the adjacency matrix of graph split i, where Ai,m,n = 1
if there is a edge between node vi,m and vi,n else Ai,m,n = 0. To conclude, each graph split is a
directed graph with node attributes and without labels. The objective of our SSL module can be
abstracted to minimize the the pretext task loss Lpretext,

min
g

Lpretext(A, X, g) =

(cid:88)

vi∈Vpretext

D(g(G)vi, ypretexti)

(1)

where g is the GNN encoder (feature extractor) and the ypretexti stands for the groundtruth of node vi
acquired by pretext task. G is one of the graph splits. In addition, Vpretext is the node set used in the
pretext task and the discriminator D is used to measure the relationship between the node embedding
of vi and ypretexti . It should be noted that our self-supervised model involves only node-level pretext
task, for sub-graph or graph-level pretext task, we leave them for future work. After pre-training with
the pretext task, the parameters of GNN encoder g will be frozen to be used for generation of node
representation Z = {z1, z2, · · · , zNi}, which will be fed into the ﬁnal classiﬁer for phishing node
prediction downstream task.

Since the original graph has been split into multiple pieces by block number (equivalently, by time),
we ﬁrstly design a spatial pretext task for each graph piece to learn the distance information between
nodes. Then a temporal pretext task is applied to learn the relationship between different graphs.

Intuitively, for the phishing scam detection problem, a well-designed pretext task should meet the
following prerequisites: 1) The extracted data labels should reﬂect the characteristics of the data itself.
2) Due to the large scale of Ethereum transaction data, it should be possible for users to obtain the
extracted data labels with relatively low time complexity, otherwise, the time cost for pre-training
would be unacceptable. 3) The pretext task could incorporate some domain knowledge, but it should
not be overly detailed. Or else, the applicability of the task would be limited and vulnerable to
adversarial attacks.

Spatial pretext task. For the nodes in the Ethereum transaction graph, we believe that the nodes
with frequent transactions should have a similar node representation. To efﬁciently capture this kind
of spatial relationships (neighborhood relationships) between nodes in the graph, we design a spatial
pretext task, as shown in Figure 1. First, for a certain graph split Gi, we ﬁrstly calculate its node
embedding Zi = {zi,1, zi,2, · · · , zi,Ni} by GNN feature extractor. Then, for node vi,i, if node vi,j is
a k-hop neighbor of vi,i (here we do not consider the edge direction), we maximize the similarity
between them. Vice-versa, we let their node representations as dissimilar as possible. Formally,

Lspatial(A, X, g) =

1
|Pspatiali |

+

1
|P spatiali|

(cid:88)

−fsim(fl(zi,i), fl(zi,j))

(vi,i,vi,j )∈Pspatiali
(cid:88)

(vi,i,vi,k)∈P spatiali

fsim(fl(zi,i), fl(zi,k))

(2)

Where Pspatiali and P spatiali denote the set of node pairs in and not in the k-hop neighborhood for
Vi, respectively. fsim is the similarity function and we exploit fsim(a, b) = σ(aT b) here. fl is a
linear transformation layer and zi,j is the node embedding of vi,j, which is the node j in Gi.

4

Figure 1: An overview of the spatial pretext task in SIEGE. Where g is the GNN encoder and fl is a
linear transformation layer.

Figure 2: An overview of the temporal pretext task in SIEGE. Where gi is the GNN encoder for the
i-th graph split and fl is a linear transformation layer.

Temporal pretext task. Since the original graph has been divided into N splits and we learn the
spatial information in each of them with the pretext task above. However, it is also important for us
to consider the relationship between different graph splits while modeling each of them. Therefore,
we design a temporal pretext task as follows.

First, since each graph split contains a large amount of transaction data, and the behavior of each
account has continuity in the time dimension, a certain percentage of nodes (accounts) must overlap
between two adjacent splits. This set of nodes forms a bridge between two adjacent graph splits, and
we use this overlap node-set Vo to model the relationship between two graph splits in our temporal
pretext task. The overview of the temporal pretext task has been shown in Figure 2. Formally, we
minimize the loss Ltemporal,

Ltemporal(A, X, g) =

1
|Voi |

(cid:88)

vi,i∈Voi

5

(cid:107)fl(zi,i) − xi+1,i(cid:107)2

(3)

gflzi,jzi,ivi,ivi,jNode embedding after GNN encoder？Ethereum transaction graph split 𝒢iSimilarity lossflvi+1,ivi,izi,ixi+1,igîxi+1,i？Ethereum transaction graph split  and 𝒢i𝒢i+1Node embedding after GNN encoderFuture node attributes prediction lossHere, Voi is the overlap node set between graph split Gi and Gi+1. And xi+1,i is the node attributes
of node vi+1,i, which points to the same account as node vi,i.

It is important that for a node, the transactions that occur in split i will inﬂuence the behavior of this
node in the next split. And our experiments show that this temporal pretext task can indeed help us
learn this connection.

3.2

Incremental training

Due to the huge and rapidly increasing size of Ethereum trading data, it is not practical to feed all
transaction data into the model at once. It is why we utilize an incremental training method by feeding
partial training data at a time, which alleviates the data scalability problem on the one hand, and helps
the model adapt to new trading data distributions on the other hand, as Ethereum market changes are
more dramatic than traditional trading platforms.

In the following, we will brieﬂy conclude SIEGE to several steps with both the self-supervised
learning module in Section 3.1 and incremental training in this section.

1. For G = {G1, G2, · · · , GN }, with each Gi = (Ai, Xi).
2. In a graph split Gi. Let GNN encoder gi = g(cid:48)(cid:48)

i−1, which is the updated GNN encoder in
Gi−1 after pre-training. For Voi−1 , the set of nodes which have appeared in both Gi−1 and
Gi, obtain their ﬁnal node embeddings Z(cid:48)(cid:48)

oi−1 after pre-training in Gi−1.

3. Obtain node attributes Xi. Let Z(cid:48)(cid:48)

oi−1 go through a linear transformation and concatenate
them with Xi, obtain Xconcati. For node v /∈ Vo or when i == 1, concatenated with zero
vector.

4. Obtain node embeddings Zi = gi(Ai, Xconcati) through a GNN encoder gi.
5. With node embeddings Zi, GNN encoder gi, minimize the spatial pretext task loss Lspatial

in Equation 2. Update parameters of gi.

6. Obtain new node embeddings Z(cid:48)

i = g(cid:48)

i(Ai, Xconcati ) through a updated GNN encoder g(cid:48)

i in

Step 5.

7. With new node embeddings Z(cid:48)

i, updated GNN encoder g(cid:48)
i, minimize the temporal pretext
task loss Ltemporal in Equation 3. Update parameters of g(cid:48)
i.

8. Obtain the ﬁnal GNN ecoder g(cid:48)(cid:48)
i .

The parameters of g will then be frozen to be used as a feature extractor in the downstream phishing
scam detection task.

4 Experiments

In this section, we evaluate the effectiveness of SIEGE on Ethereum transaction data. We ﬁrstly
introduce our experimental setup in Section 4.1, including data collection, data pre-processing,
baseline models, hyperparameter selection, etc. We then present our experimental results with
discussion and analysis in Section 4.2, including both transductive and inductive settings. Furthermore,
we conduct ablation study experiments in Section 4.3 to investigate the effectiveness of each module.

4.1 Experimental setup

Data collection and pre-processing. The ﬁrst block of Ethereum was generated in Jul 2015.
Without loss of generality, we collected transaction data on Ethereum from January 2018 to May
2020, which is complex and contains lots of noisy information. To make experiments more clear, we
list our data preparation steps as follows:

1. There are two kinds of transactions in Ethereum: 1) external transactions which start from
user accounts and 2) internal transactions which start from smart contract accounts. Since
both 1) and 2) have a signiﬁcant impact on the transaction graph, we include both of them in
our dataset.

6

2. We then ﬁlter out those unsuccessful transactions and zero-value transactions, which are

meaningless for this task. And get a set of 75382756 transactions in total.

3. According to block number (equivalently, timestamp), we divided the collected transaction
data into ﬁve pieces and each piece was further divided into ﬁve splits. We conduct
experiments with multiple pieces of graph data to reduce variability. In each piece, different
graph splits can be used for incremental training.

4. In each graph split, since no available attributes of nodes can be found in the original
transaction data, and two nodes can have multiple transactions, which may lead to a multi-
edge transaction graph, to simplify these issues, we use the transaction data to generate node
attributes manually to get a single-edge directed graph with node attributes. Then we choose
the biggest weak connected component (WCC) as the ﬁnal training data. Detailed node
attributes generation process can be found in Appendix A.

Take the ﬁrst three graph splits for example, we show the data statistics in Table 1. For phishing
scam nodes used for evaluation, we collected labels from the Ethereum browser1 and some blacklists
released by companies2. In all, we obtained 6588 unique Ethereum account IDs related to phishing
scams.

Table 1: Data statistics for the ﬁrst three graph splits. For overlap ratio, we refer to the proportion of
overlap node set between the current and next split in the node set of the whole graph split.

Graph split

Nodes

Edges

Node features Phishing scam nodes Overlap ratio

Split 1
Split 2
Split 3

2216306
2201898
2620644

3978100
3406033
4668867

17
17
17

114
109
121

0.27
0.29
0.26

Hyperparameter settings and baseline models. For transductive settings, in each piece, the ﬁrst
three splits can be used for pre-training and incremental learning, and evaluated in the fourth split.
For inductive settings, the ﬁrst four splits can be used for pre-training and incremental learning. And
evaluated in the ﬁfth split (without any training). We randomly sample some nodes without phishing
scam labels as normal nodes from the current graph split. In each graph split, we set the number of
normal node labels to be three times that of phishing scam nodes. Then in these nodes with labels,
train/val/test sets are randomly split with the ratio of 50%/20%/30%.

We choose the 2-layer GraphSage[15] with mean-pooling aggregator as the backbone for the node
classiﬁcation model. We use a logistic regression model as our binary classiﬁer. In transductive
setting, we compared SEIGE with six baselines: 1) Raw features; 2) the DeepWalk algorithm[24];
3) concatenation of DeepWalk node embeddings and raw features; 4) GraphSage[15], a ﬂexible
inductive GNN method; 5) DGI[30], an unsupervised inductive GNN method through maximizing
the mutual information between local patch representations and high-level graph representation;
6) GCN[19] without training, according to [19], GCN without training can also be considered as
a powerful feature extractor. For inductive setting evaluated in an entirely new graph, the node
embeddings generated by DeepWalk will become rotated with respect to the original embedding
space, as pointed out in[15], so we do not compare with it in this setting.

The Adam[18] is used as our optimizer. We search the learning rate from {0.01, 0.001, 0.0001}.
We set the dropout to 0.5. The hidden size is searched from {32, 64, 128, 256}. For mini-batch
training, the batch size is set to 512. For GraphSage, we use mean-pooling as the aggregator. For
DGI, following[30], we set the two-layer GCN and three-layer GraphSage-GCN with skip connection
as an encoder in transductive and inductive settings, respectively. For our model SEIGE, the value
of k in the spatial pretext task is 2. And the number of graph splits used for incremental training is
searched from {1, 2, 3}.

1https://etherscan.io/
2https://github.com/CryptoScamDB/blacklist

7

4.2 Results

Our results for transductive and inductive settings are shown in Table 2 and Table 3, respectively.
For the transductive setting, it is obvious that SEIGE outperforms baselines with about 4% in F-1
score, which strongly demonstrates the effectiveness of our SEIGE model. We also tried to remove
the incremental learning module from SEIGE. The comparison with the original model shows that
the incremental learning method is also helpful for the detection of phishing scam nodes, which
actually is a general mechanism applicable to other inductive GNN methods. It can be seen that the
SEIGE without incremental training could also perform better than baselines, which demonstrates the
effectiveness of our SSL modules. Furthermore, we note that the DGI method slightly outperforms
the GraphSage method, and that an untrained GCN also yields good results.

Table 2: Prediction results in the transductive setting in terms of accuracy, recall, precision, and F1
score. The results are averaged over 5 runs in different graph pieces. ’-incremental’ means that we do
not use the incremental training.

Method

Acc(%)

Precision(%) Recall(%)

F-1(%)

Raw features
DeepWalk[24]
DeepWalk+features

GraphSage[15]
GCN-untrained[19]
DGI[30]

SEIGE
SEIGE-incremental

56.9
60.2
64.1

66.7
69.2
69.0

71.7
70.4

29.9
31.8
36.0

38.7
41.7
41.9

45.6
43.9

53.9
51.8
55.9

56.9
58.0
62.9

68.0
65.8

38.5
39.4
43.8

46.1
48.5
50.3

54.6
52.7

For inductive setting, as expected, we found that the raw feature method and the GCN-untrained
method are not considerably changed compared to transductive settings. While other methods
have a slight drop in performance, which may be caused by a slight change in the transaction data
distribution.

Table 3: Prediction results in the inductive setting in terms of accuracy, recall, precision, and F1
score. The results are averaged over 5 runs in different graph pieces. ’-incremental’ means that we do
not use the incremental training.

Method

Raw features

GraphSage[15]
GCN-untrained[19]
DGI[30]

SEIGE
SEIGE-incremental

Acc(%)

Precision(%) Recall(%)

F-1(%)

57.4

63.7
68.7
67.2

70.0
67.7

30.0

35.2
40.8
40.0

43.5
40.7

52.9

53.9
55.9
62.0

66.9
63.9

38.3

42.6
47.2
48.6

52.7
49.7

4.3 Ablation study

We performed an ablation study for SEIGE in the transductive setting and the results have shown that
both the spatial pretext task and temporal pretext task are signiﬁcant for SEIGE. It can also be seen
that compared to the temporal pretext task, the spatial pretext task is more important, which indicates
that maybe the spatial relationship has a larger impact on this phishing scam node detection.

5 Conclusion

In this paper, to solve the problem of phishing scam node detection on Ethereum transaction graph.
We propose a self-supervised deep graph learning method with incremental training to address the
label scarcity and the data scalability problem. In our self-supervised learning module, we design two

8

Table 4: Ablation study of SEIGE in the transductive setting in terms of accuracy, recall, precision,
and F1 score.

Method

Acc(%)

Precision(%) Recall(%)

F-1(%)

SEIGE
SEIGE-incremental
SEIGE-temporal
SEIGE-spatial

71.7
70.4
65.1
60.2

45.6
43.9
36.8
32.5

68.0
65.8
55.0
55.0

54.6
52.7
44.1
40.9

new pretext tasks from both spatial and temporal dimensions. Our extensive experimental results on
large-scale Ethereum transaction graphs show that our method strongly outperforms baseline models.

References

[1] Massimo Bartoletti, Salvatore Carta, Tiziana Cimoli, and Roberto Saia. Dissecting ponzi
schemes on ethereum: identiﬁcation, analysis, and impact. Future Generation Computer
Systems, 102:259–277, 2020.

[2] Joost Bastings, Ivan Titov, Wilker Aziz, Diego Marcheggiani, and Khalil Sima’an. Graph convo-
lutional encoders for syntax-aware neural machine translation. arXiv preprint arXiv:1704.04675,
2017.

[3] Daniel Beck, Gholamreza Haffari, and Trevor Cohn. Graph-to-sequence learning using gated

graph neural networks. arXiv preprint arXiv:1806.09835, 2018.

[4] Ben Bogin, Matt Gardner, and Jonathan Berant. Representing schema structure with graph

neural networks for text-to-sql parsing. arXiv preprint arXiv:1905.06241, 2019.

[5] Liang Chen, Jiaying Peng, Yang Liu, Jintang Li, Fenfang Xie, and Zibin Zheng. Phishing scams
detection in ethereum transaction network. ACM Transactions on Internet Technology (TOIT),
21(1):1–16, 2020.

[6] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework
for contrastive learning of visual representations. In International conference on machine
learning, pages 1597–1607. PMLR, 2020.

[7] Ting Chen, Zihao Li, Yuxiao Zhu, Jiachi Chen, Xiapu Luo, John Chi-Shing Lui, Xiaodong
Lin, and Xiaosong Zhang. Understanding ethereum via graph analysis. ACM Transactions on
Internet Technology (TOIT), 20(2):1–32, 2020.

[8] Weili Chen, Xiongfeng Guo, Zhiguang Chen, Zibin Zheng, and Yutong Lu. Phishing scam
detection on ethereum: Towards ﬁnancial security for blockchain ecosystem. In IJCAI, 2020.
[9] Weili Chen, Zibin Zheng, Jiahui Cui, Edith Ngai, Peilin Zheng, and Yuren Zhou. Detecting
ponzi schemes on ethereum: Towards healthier blockchain technology. In Proceedings of the
2018 World Wide Web Conference, pages 1409–1418, 2018.

[10] Oscar Delgado-Mohatar, José María Sierra-Cámara, and Eloy Anguiano. Blockchain-based

semi-autonomous ransomware. Future Generation Computer Systems, 112:589–603, 2020.

[11] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of
deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805,
2018.

[12] Wenqi Fan, Yao Ma, Qing Li, Yuan He, Eric Zhao, Jiliang Tang, and Dawei Yin. Graph neural
networks for social recommendation. In The World Wide Web Conference, pages 417–426,
2019.

[13] Alex M Fout. Protein interface prediction using graph convolutional networks. PhD thesis,

Colorado State University, 2017.

[14] Aditya Grover and Jure Leskovec. node2vec: Scalable feature learning for networks.

In
Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and
data mining, pages 855–864, 2016.

[15] William L Hamilton, Rex Ying, and Jure Leskovec. Inductive representation learning on large

graphs. arXiv preprint arXiv:1706.02216, 2017.

9

[16] Huawei Huang, Wei Kong, Sicong Zhou, Zibin Zheng, and Song Guo. A survey of state-of-
the-art on blockchains: Theories, modelings, and tools. ACM Computing Surveys (CSUR),
54(2):1–42, 2021.

[17] Longlong Jing and Yingli Tian. Self-supervised visual feature learning with deep neural
networks: A survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2020.
[18] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint

arXiv:1412.6980, 2014.

[19] Thomas N Kipf and Max Welling. Semi-supervised classiﬁcation with graph convolutional

networks. arXiv preprint arXiv:1609.02907, 2016.

[20] Thomas N Kipf and Max Welling. Variational graph auto-encoders.

arXiv preprint

arXiv:1611.07308, 2016.

[21] Shucheng Li, Lingfei Wu, Shiwei Feng, Fangli Xu, Fengyuan Xu, and Sheng Zhong. Graph-
to-tree neural networks for learning structured input-output translation with applications to
semantic parsing and math word problem. arXiv preprint arXiv:2004.13781, 2020.

[22] Xiao Liu, Fanjin Zhang, Zhenyu Hou, Zhaoyu Wang, Li Mian, Jing Zhang, and Jie Tang.
Self-supervised learning: Generative or contrastive. arXiv preprint arXiv:2006.08218, 1(2),
2020.

[23] Satoshi Nakamoto. Bitcoin: A peer-to-peer electronic cash system. Technical report, Manubot,

2019.

[24] Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. Deepwalk: Online learning of social repre-
sentations. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge
discovery and data mining, pages 701–710, 2014.

[25] Yu Rong, Yatao Bian, Tingyang Xu, Weiyang Xie, Ying Wei, Wenbing Huang, and Junzhou
Huang. Self-supervised graph transformer on large-scale molecular data. Advances in Neural
Information Processing Systems, 33, 2020.

[26] Lei Shi, Yifan Zhang, Jian Cheng, and Hanqing Lu. Skeleton-based action recognition with
directed graph neural networks. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pages 7912–7921, 2019.

[27] Weijing Shi and Raj Rajkumar. Point-gnn: Graph neural network for 3d object detection in
a point cloud. In Proceedings of the IEEE/CVF conference on computer vision and pattern
recognition, pages 1711–1719, 2020.

[28] Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei. Line: Large-
scale information network embedding. In Proceedings of the 24th international conference on
world wide web, pages 1067–1077, 2015.

[29] Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine

learning research, 9(11), 2008.

[30] Petar Veliˇckovi´c, William Fedus, William L Hamilton, Pietro Liò, Yoshua Bengio, and R Devon

Hjelm. Deep graph infomax. arXiv preprint arXiv:1809.10341, 2018.

[31] Hongwei Wang, Fuzheng Zhang, Mengdi Zhang, Jure Leskovec, Miao Zhao, Wenjie Li, and
Zhongyuan Wang. Knowledge-aware graph neural networks with label smoothness regular-
ization for recommender systems. In Proceedings of the 25th ACM SIGKDD International
Conference on Knowledge Discovery & Data Mining, pages 968–977, 2019.

[32] Jinhuan Wang, Pengtao Chen, Shanqing Yu, and Qi Xuan. Tsgn: Transaction subgraph networks

for identifying ethereum phishing accounts. arXiv preprint arXiv:2104.08767, 2021.

[33] Gavin Wood et al. Ethereum: A secure decentralised generalised transaction ledger. Ethereum

project yellow paper, 151(2014):1–32, 2014.

[34] Jiajing Wu, Qi Yuan, Dan Lin, Wei You, Weili Chen, Chuan Chen, and Zibin Zheng. Who are
the phishers? phishing scam detection on ethereum via network embedding. IEEE Transactions
on Systems, Man, and Cybernetics: Systems, 2020.

[35] Kun Xu, Lingfei Wu, Zhiguo Wang, Mo Yu, Liwei Chen, and Vadim Sheinin. Exploiting
rich syntactic information for semantic parsing with graph-to-sequence model. arXiv preprint
arXiv:1808.07624, 2018.

10

[36] Yuning You, Tianlong Chen, Zhangyang Wang, and Yang Shen. When does self-supervision
help graph convolutional networks? In International Conference on Machine Learning, pages
10871–10880. PMLR, 2020.

[37] Qi Yuan, Baoying Huang, Jie Zhang, Jiajing Wu, Haonan Zhang, and Xi Zhang. Detecting phish-
ing scams on ethereum based on transaction records. In 2020 IEEE International Symposium
on Circuits and Systems (ISCAS), pages 1–5. IEEE, 2020.

11

A Node attributes generation

Since the original Etheruem transaction graph does not have initial features, we explain our node
attributes (initial features) generation process in detail here 5. We extracted features from the raw
transaction data as follows,

Table 5: The node attributes (initial features) generation process. We selected 17 general features
from raw Ethereum transaction data as the node attributes.

Index Feature (for each node)

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17

is smart contract or not.
the in-degree.
the out-degree.
the number of in-transactions.
the number of out-transactions.
the sum of all transaction amount.
the sum of all in-transaction amount.
the sum of all out-transaction amount.
the average of all in-transaction amount.
the average of all out-transaction amount.
the time span of all transactions.
the time span of all in-transactions.
the time span of all out-transactions.
the frequenncy of all in-transactions.
the frequenncy of all out-transactions.
the proportion of multiple in-transactions between same node pair.
the proportion of multiple out-transactions between same node pair.

B Hardware and further discussion

In this section, we introduce the hardware we use in the experiments and give some further discussion
about the performance bottleneck and the incremental learning module in SEIGE.

We ran our experiments in a single machine with 4 GPUs of TITAN Xp (12Gb of RAM), 12 CPUs of
Intel Core i7-6850K (@ 3.60GHz). OS version: Ubuntu 16.04.4 LTS, and CUDA version: 11.0.

One of the main performance bottlenecks for the phishing scam detection task is the label scarcity,
which is why we are using unsupervised learning. If we can obtain more labels on Ethereum for
training then we may be able to achieve better results.

In addition, we would like to further discuss the incremental learning module in SEIGE. First, this
incremental learning module works well to alleviate the problem of data scalability. In fact, in our
experiments, for deep graph model baselines: GraphSage, DeepWalk, and DGI, if we combine two
graph splits into one and use it for training, it leads to out of memory problem. Moreover, the
incremental learning module is a general mechanism that can be used in other deep graph models as
well.

C Visualization

We show the node features visualization of three methods in Figure 3: i) raw features, ii) GCN-
untrained, iii) SEIGE. We can ﬁnd that points are better separated in GCN-untrained and SEIGE than
raw features. Besides, we can also notice that the points in SEIGE are better separated and more
concentrated compared to GCN-untrained. It further demonstrates the effectiveness of our SEIGE
model.

12

Figure 3: Node features visualization of raw features (left), GCN-untrained (middle), and SEIGE
(right) by t-SNE[29]. Orange points stand for phishing scam nodes and blue points are normal nodes.
To make the ﬁgure clear, we limit the number of points to about 500 and the ratio of normal nodes to
phishing nodes is about 4:1.

13

