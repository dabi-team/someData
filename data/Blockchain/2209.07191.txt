2
2
0
2

p
e
S
5
1

]
x
e
-
p
e
h
[

1
v
1
9
1
7
0
.
9
0
2
2
:
v
i
X
r
a

A First Application of Collaborative Learning In
Particle Physics

Stefano Vergani1,2, Attila Bagoly2
1 University of Cambridge, Cambridge CB3 0HE, United Kingdom
2 Fetch.AI

E-mail: sv408@hep.phy.cam.ac.uk

Abstract. Over the last ten years, the popularity of Machine Learning (ML) has grown
exponentially in all scientiﬁc ﬁelds, including particle physics. The industry has also developed
new powerful tools that, imported into academia, could revolutionise research. One recent
industry development that has not yet come to the attention of the particle physics community
is Collaborative Learning (CL), a framework that allows training the same ML model with
diﬀerent datasets. This work explores the potential of CL, testing the library Colearn with
neutrino physics simulation. Colearn, developed by the British Cambridge-based ﬁrm Fetch.AI,
enables decentralised machine learning tasks. Being a blockchain-mediated CL system, it allows
multiple stakeholders to build a shared ML model without needing to rely on a central authority.
A generic Liquid Argon Time-Projection Chamber (LArTPC) has been simulated and images
produced by ﬁctitious neutrino interactions have been used to produce several datasets. These
datasets, called learners, participated successfully in training a Deep Learning (DL) Keras model
using blockchain technologies in a decentralised way. This test explores the feasibility of training
a single ML model using diﬀerent simulation datasets coming from diﬀerent research groups.
In this work, we also discuss a framework that instead makes diﬀerent ML models compete
against each other on the same dataset. The ﬁnal goal is then to train the most performant
ML model across the entire scientiﬁc community for a given experiment, either using all of the
datasets available or selecting the model which performs best among every model developed in
the community.

1. Collaborative Learning Framework and Colearn Library
Various methods have been proposed to train the same ML model with separate data sets. For
the industry, there has been a strong focus on creating new frameworks to preserve privacy. An
example is Federated Learning (FL) [1, 2], which allows the model to be sent directly to the
source of data. A typical example could be an ML model used to recognise writing patterns.
A company starts with a standard model pre-trained in a certain language and then it sends it
to a smartphone. The model will be trained locally with the writing patterns contained in the
device and, after the training phase, it will be sent directly back to the warehouse. In this way,
the users will never need to send their data directly to the company, only the updated model is
shared. Adding noise during training [3] will improve privacy even more. A diﬀerent approach
is Collaborative Learning (CL), which is the framework Colearn [4] is based on. Colearn is a
library, created by the company Fetch.AI, that allows privacy-preserving decentralized machine
learning tasks using blockchain technologies. This library is particularly interesting because it
can be used to simultaneously train the model with several data sets as well as determine the

 
 
 
 
 
 
quality of each data set concerning its ability to train the chosen model. The process begins
when each stakeholder, that are called learners, bring its data set. This data set will not be
disclosed to the other participants, which is a particularly important feature for stakeholders
who value privacy. One learner is then randomly selected to train the model, and at the end
of the training, the weights are shown to the other learners. They will then test the model
with these new weights on their subset and vote on whether the set of weights constitutes an
improvement or not. If the majority agrees, the new weights are accepted and a new round
starts. For a visual representation of the process, see Figure 1. A ﬁrst test was done using the
MNIST data set [5] and breaking it into ﬁve sub data sets and training for 20 rounds as it can
be seen in Figure 2. Whilst new weights from every data set tend to be accepted during the ﬁrst
rounds, after a while, it can be seen how some sub data sets will be voted more than others.
Moreover, the model has been successfully trained using the 5 learners and reaching an accuracy
above 95%.

Figure 1: The Colearn process.

Figure 2: A Keras model trained using the MNIST data set split into 5 sub data sets. Top:
categorical accuracy for each learner (dotted lines) and average (full lines) for test (blue) and
vote (red). Below: votes for each training round. Green means the new weights have been
approved, red means the new weights have not been approved. When two or more learners
approved the weights, the new weights have been overall approved.

2. Particle Physics and Collaborative Learning
The ﬁeld of particle physics has seen an exponential usage of ML technologies over the last
years [6, 7]. It is customary for diﬀerent groups or experiments to produce their simulated data
set to train ML models or use data from real experiments. This work intends to explore the
capabilities of CL with particle physics data sets. One possible usage could be merging diﬀerent
simulated or real data sets to train the same ML models. This could be useful for example for
training an ML model to look for several low-interacting particles or rare events, where each
group can contribute bringing a sub data set focused only on a particular particle or events.
Another option could be instead of making several data sets compete against each other to
determine which data set is the most promising one. Given that Colearn is decentralised, this
could a particularly useful feature to ensure impartiality among diﬀerent research groups. The
last option, that will not be discussed in this paper, is to make diﬀerent ML models compete
against each other with the same data set. This is in theory possible but at the moment there
is no such implementation on the market.

3. The Neutrino Data Set
For this proof of concept, a neutrino data set has been created consisting of 10000 training and
2000 validation pictures. Each picture contains three sub-images that mimic the three wire-
planes of a typical Liquid Argon Time-Projection Chamber (LArTPC) such as a MicroBooNE
[8, 9] or ProtoDUNE Single Phase (SP) [10, 11]. Each picture is 200 × 200 pixels and represent
either a Neutrino Neutral Current (NNC), a Muon Neutrino Charged Current (NCC), or an
electron NCC. For an example of the data set, see Figure 3. The neutrino interactions were
created using GENIE [12] software with a neutrino ﬂux as a function of energy following a
Gaussian distribution with a mean of 2.5 GeV and width 0.5 GeV. The ﬁnal-state particles from
the neutrino interactions are passed into the GEANT4 [13] simulation, being tracked through a
monolithic liquid argon volume.

Figure 3: One example of a muon NCC from the neutrino data set. The three images mimic
the three wire plane views as seen in a LArTPC experiment.

4. Machine Learning Models
For this experiment, several available Keras [14] models have been tested with the neutrino data
set mentioned in Section 3. The two best performing have been ResNet-50 V2 and DensNet-169,
with an accuracy of 64.15% and 68.6% as shown in Figure 4. Their performance has been used
as a benchmark.

Figure 4: The Keras model ResNet-50 V2 (top) and DenseNet-169 (bottom) trained with the
entire neutrino data set. On the right-hand side is the confusion matrix for each one of them.
The accuracy curve suggests that even higher validation accuracy could have been achieved by
extended training. These performances represent only a term of comparison to benchmark the
proof of concept.

5. Collaborative Learning with Neutrino Data Set
To simulate the usage of separate data sets, the neutrino data set has been randomly split into
three separate subsets. These three sub sets have been used as three learners. Both the models
explained in Section 4 have been trained using the Colearn library for 15 rounds. Results are
shown in Figures 5a and 5b.

6. Results and Future Applications
This proof-of-concept has been successful and it showed that the Colearn library can be used
for particle physics’s data sets. Diﬀerent behaviours are observed between ResNet-50 V2 and
DenseNet-169. The former tends to be more stable but with lower eﬃciency (roughly 50% the
mean test accuracy), the latter presents an unstable but higher eﬃciency, with peaks around
70%. In both cases, a certain learner (learner 2 for ResNet-50 V2 and learner 0 for DenseNet-
169) was particularly underperforming in terms of having its set of weights approved. This is due
to random statistical diﬀerences in the sub-samples, but the fact that the framework is correctly
able to identify those diﬀerences shows its great potentialities. These results conﬁrm that the
ideas expressed in Section 2 can now be developed into a proper and independent framework
based on Colearn. This could be made to ﬁnd which simulated data set underperform in a
speciﬁc task since it would be the learner that most often does not bring accepted weights. It
could also be used pretty much straight out of the box to train the model with separate data
sets. Further studies on the oscillating behaviour of some models should be made to determine
whether this is a feature that can be used. The next step would most importantly be to try
a similar experiment with completely diﬀerent data sets coming from diﬀerent sources. This
would highlight the possible limits of this technology or its further potentialities.

(a)

(b)

Figure 5: Test results with Keras (a) ResNet-50 V2 and (b) DenseNet-169. Top: categorical
accuracy for each learner (dotted lines) and average (full lines) for test (blue) and vote (red).
Below: votes for each training round. Green means the new weights have been approved, red
means the new weights have not been approved. When two or more learners approved the
weights, the new weights have been overall approved.

References
[1] B. McMahan, E. Moore, D. Ramage, S. Hampson and B.A. y Arcas, Communication-eﬃcient learning of
deep networks from decentralized data, in Proceedings of the 20th International Conference on Artiﬁcial
Intelligence and Statistics, AISTATS 2017, 20-22 April 2017, Fort Lauderdale, FL, USA, A. Singh and
X.J. Zhu, eds., vol. 54 of Proceedings of Machine Learning Research, pp. 1273–1282, PMLR, 2017,
http://proceedings.mlr.press/v54/mcmahan17a.html.

[2] P. Kairouz, H.B. McMahan, B. Avent, A. Bellet, M. Bennis, A.N. Bhagoji et al., Advances and open

problems in federated learning, 2019, https://arxiv.org/abs/1912.04977.

[3] M. Abadi, A. Chu, I. Goodfellow, H.B. McMahan, I. Mironov, K. Talwar et al., Deep learning with
diﬀerential privacy, in Proceedings of the 2016 ACM SIGSAC Conference on Computer and
Communications Security, CCS ’16, (New York, NY, USA), p. 308–318, Association for Computing
Machinery, 2016, DOI.
[4] Fetch.AI, Colearn, 2022.
[5] Y. LeCun and C. Cortes, MNIST handwritten digit database, .
[6] P. Calaﬁura, D. Rousseau and K. Terao, Artiﬁcial Intelligence for High Energy Physics, World Scientiﬁc

(2022), 10.1142/12200.

[7] Machine learning at the energy and intensity frontiers of particle physics, Nature 560 (2018) pp. 41–48.
[8] R. Acciarri, C. Adams, R. An, A. Aparicio, S. Aponte, J. Asaadi et al., Design and construction of the

MicroBooNE detector, Journal of Instrumentation 12 (2017) P02017.

[9] K. Duﬀy, P. Abratenko, R. An, J. Antony, W. Van De Pontseele and C. Zhang, Search for an anomalous
excess of inclusive charged-current νe interactions in the microboone experiment using wire-cell
reconstruction, Physical Review D: Particles, Fields, Gravitation and Cosmology (2022) .

[10] DUNE collaboration, First results on ProtoDUNE-SP liquid argon time projection chamber performance

from a beam test at the CERN Neutrino Platform, JINST 15 (2020) P12004 [2007.06722].

[11] DUNE collaboration, Design, construction and operation of the ProtoDUNE-SP Liquid Argon TPC, JINST

17 (2022) P01005 [2108.01902].

[12] M. Alam et al., GENIE Production Release 2.10.0, 1512.06882.
[13] GEANT4 collaboration, GEANT4: A Simulation toolkit, Nucl. Instrum. Meth. A 506 (2003) 250.
[14] F. Chollet et al., Keras, 2015.

