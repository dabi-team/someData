Long-term Data Sharing under Exclusivity Attacks

YOTAM GAFNI yotam.gafni@campus.technion.ac.il
MOSHE TENNENHOLTZ moshet@ie.technion.ac.il

TECHNION - ISRAEL INSTITUTE OF TECHNOLOGY
The quality of learning generally improves with the scale and diversity of data. Companies and institutions
can therefore benefit from building models over shared data. Many cloud and blockchain platforms, as well as
government initiatives, are interested in providing this type of service.

These cooperative efforts face a challenge, which we call â€œexclusivity attacksâ€. A firm can share distorted
data, so that it learns the best model fit, but is also able to mislead others. We study protocols for long-term
interactions and their vulnerability to these attacks, in particular for regression and clustering tasks. We
conclude that the choice of protocol, as well as the number of Sybil identities an attacker may control, is
material to vulnerability.

2
2
0
2

n
a
J

2
2

]

R
C
.
s
c
[

1
v
7
3
1
9
0
.
1
0
2
2
:
v
i
X
r
a

 
 
 
 
 
 
1

1 THE WORK IN CONTEXT

1.1 Data Sharing among Firms
In todayâ€™s data-oriented economy [31], countless applications are based on the ability to extract
statistically significant models out of acquired user data. Still, firms are hesitant to share information
with other firms [8, 35], as data is viewed as a resource that must be protected. This is in tension
with the paradigm of the Wisdom of the crowds [38], which emphasizes the added predictive value of
aggregating multiple data sources. As early as 2001, the authors in [2] (note also a similar approach
in [17]) concluded that

â€œ... a logical next step for the research community would be to direct efforts towards
increasing the size of annotated training collections, while deemphasizing the focus on
comparing different learning techniques trained only on small training corpora.â€

Two popular frameworks to address issues arising in settings where data is shared are multi-party
computation [9] and differential privacy [11]. However, these paradigms are focused on addressing
the issue of privacy (whether of the individual user or the firmâ€™s data bank), but do not answer
the basic conundrum of sharing data with competing firms: On one hand, cooperation enables the
firm to enrich its own models, but at the same time enable other firms to do so as well. A firm is
thus tempted to game the mechanism to allow itself better inference than other firms. We call this
behavior exclusivity attacks. Even if supplying intentionally false information could be a legal risk,
the nature of data processing (rich with outliers, spam accounts, natural biases), allows firms to
have â€œreasonable justificationâ€ to alter the data they share with others.

In this work, we present a model of collaborative information sharing between firms. The goal
of every firm is first to have the best available model given the aggregate data. As a secondary goal,
every firm wishes the others to have a downgraded version of the model. An appropriate framework
to address this objective is the Non-cooperative computation (NCC) framework, introduced in [37].
The framework was considered with respect to one-shot data aggregation tasks in [22].

1.2 Open and Long-term Environments
In our work, we present a general communication protocol for collaborative data sharing among
firms, that can be associated with any specific machine learning or data aggregation algorithm.
The protocol possesses an online nature, when any participating firm may send (additional) data
points at any time. This is in contrast with previous NCC literature, which focuses on one-shot
data-sharing procedures. The long-term setting yields two, somewhat contradicting, attributes:

â€¢ A firm may send multiple subsequent inputs to the protocol, using it to learn how the modelâ€™s
parameters change after each contribution. For an attacker, this allows better inference of
the true modelâ€™s parameters, without revealing its true data points, as we demonstrate in
Example 1 below.

â€¢ A firm is not only interested in attaining the current correct parameters of the model, but also
has a future interest to be able to attain correct answers, given that more data is later added
by itself and its competitors. This has a chilling effect on attacks, as even a successful attack
in the one-shot case could result in data corruption. For example, a possible short-term attack
could be for a firm to send its true data, attain the correct parameters, and then send additional
garbage data. Since we do not have built-in protection against such actions in the mechanism
(for reasons further explained in Remark 1), this would result in data corruption for the other
firms. Nevertheless, if the firm itself is interested in attaining meaningful information from
the mechanism in the future, it would be disincentivized to do so.

2

We now give an example demonstrating the first point. In [22], the authors consider the problem of
collaboratively calculating the average of data points. They show in their Theorem 4.6 and Theorem
4.7 that whether the number of different data points is known is essential to the truthfulness of the
mechanism. When the number of data points is unknown, the denominator of the average term is
unknown, and it is impossible for an attacker to know with certainty how to attain the true average
from the average the mechanism reports given a false input of the attacker. We now show that in
a model where it is possible to send multiple requests (in fact, two), it is possible to report false
information and attain the correct average:

Example 1. Consider a firm with some data points ğ·ğ¼ with a total sum ğ‘†ğ¼ and number of points ğ‘ğ¼ .
Other firms have data points ğ·ğ‘‚ with a total sum ğ‘†ğ‘‚ and number of points ğ‘ğ‘‚ . Assume ğ‘†ğ¼ â‰  0, ğ‘ğ¼ = 2.1
Instead of reporting ğ·ğ¼ , the firm first reports ğ· â€² = [0], receives an average ğ‘1, then reports ğ· â€²â€² = [0]
and receives the updated average ğ‘2. The average that others, following the mechanism as given, attain
is
, and they are different by our assumption on ğ‘†ğ¼ , ğ‘ğ¼ . The firm is thus
successful in misleading others. Moreover, the firm can infer the true average. Given

ğ‘ğ‘‚ +2 , the true average is ğ‘†ğ¼ +ğ‘†ğ‘‚
ğ‘†ğ‘‚
ğ‘ğ¼ +ğ‘ğ‘‚

the firm2 can calculate

ğ‘1 =

ğ‘†ğ‘‚
ğ‘ğ‘‚ + 1

â‰ 

ğ‘†ğ‘‚
ğ‘ğ‘‚ + 2

= ğ‘2,

ğ‘ğ‘‚ =

ğ‘1 âˆ’ 2ğ‘2
ğ‘2 âˆ’ ğ‘1

, ğ‘†ğ‘‚ = ğ‘1(ğ‘ğ‘‚ + 1),

and thus have all the information required to calculate the true average.

Remark 1. Why should we not consider simply forbidding multiple subsequent updates by a firm?
As noted in [1, 13, 40], modern internet-based environments lack clear identities and allow for multiple
inputs by the same agent using multiple identities. A common distinction in blockchain networks
separates public (â€œpermissionlessâ€) and private (â€œpermissionedâ€) networks [24], where public networks
allow open access for everyone, while private networks require additional identification for participation.
In both cases, however, it is impossible to totally prevent false-name manipulation, where a firm uses
multiple identities to send her requests. Therefore, any â€œsimpleâ€ solution of the problem demonstrated in
Example 1 is impossible. The mechanism does not know whether multiple subsequent updates are really
sent by different firms, or they are in fact â€œsock puppetsâ€ of a single firm. The mechanism therefore can
not adjust appropriately (e.g., drop any request after the first one). In this work, we assume a firm may
control up to â„“ identities, and so in the formal model, we allow up to â„“ subsequent updates of a single
firm. The false identities are not part of the formal model: They instead are encapsulated by giving
firms this ability to update â„“ times subsequently.

1.3 Our Results

â€¢ We define two long-term data-sharing protocols (the continuous and periodic communica-
tion protocols) for data sharing among firms. The models differ in how communication is
structured temporally (whether the agents can communicate at any time, or are asked for
their inputs at given times). Each model can be coupled with any choice of algorithm to
aggregate the data shared by the agents.

â€¢ We give a condition for NCC-vulnerability of an algorithm (given the communication model)
in Definition 1. A successful NCC attack is one that (i) Can mislead the other agents, and
(ii) Maintains the attackerâ€™s ability to infer the true algorithm output. We give a stronger

1These assumptions are not required for the attack scheme to succeed, but make for a simpler demonstration.
2The only case where ğ‘1 = ğ‘2 is when ğ‘†ğ‘‚ = 0. In this case, upon having ğ‘1 = 0, we can choose ğ·â€²â€² = [1], and a similar
argument shows that we can infer the true average.

Vulnerable

Vulnerable*

ğ‘‘-LinearRegression Yes, for any â„“ â‰¥ 1

ğ‘˜-Center

Yes, for any â„“ â‰¥ 1

Yes

No

ï£±ï£´ï£²
ï£´
ï£³

â„“ â‰¥ ğ‘‘ + 2
â„“ â‰¤ ğ‘‘ âˆ’ 2
No

Table 1. A summary of vulnerability(*) results in the continuous communication protocol.

3

condition of NCC-vulnerability* that can moreover (i*) Mislead the other agents in every
possible scenario. As a simple example of using these definitions, we show in Appendix B
that finding the maximum over agent reports is NCC-vulnerable but not NCC-vulnerable*.
â€¢ For the ğ‘˜-center problem, we show that it is vulnerable under continuous communication but
not vulnerable under periodic communication. Moreover, we show that it is not vulnerable*
even in continuous communication, using a notion of explicitly-lying attacks.

â€¢ For Multiple Linear Regression, we show that it is vulnerable* under continuous communi-
cation but not vulnerable under periodic communication. The vulnerability* in continuous
communication depends on the number of identities an attacker can control: We show a form
of attack so that an attacker with ğ‘‘ + 2 identities (where ğ‘‘ is the dimension of the feature
space) is guaranteed to have an attack, and an attacker with less than ğ‘‘ âˆ’ 2 identities can not
attack.

The vulnerability(*) results for the continuous communication protocol are summarized in Table 1.

Both algorithms are not vulnerable(*) under the periodic communication protocol.

We overview related work in Appendix A.

2 MODEL AND VULNERABILITY NOTIONS
We consider a system where agents receive factual updates containing data points or states of
the world. The agents apply their reporting strategy, performing ledger updates. Upon any ledger
update, the ledger distributes the latest aggregate parameter calculation using ğœŒ, the computation
algorithm.

Formally, let [ğ‘›] = {1, . . . , ğ‘›} be a set of ğ‘› agents. An update ğ‘ˆ is of some type, depending on
the computational problem. An update with metadata Ë†ğ‘ˆ =< ğ‘—, ğ‘¡, ğ‘ˆ > complements an update ğ‘ˆ
with an agent ğ‘— âˆˆ N , and a type ğ‘¡ âˆˆ {ğ¹ğ‘ğ‘ğ‘¡ğ‘¢ğ‘ğ‘™, ğ¿ğ‘’ğ‘‘ğ‘”ğ‘’ğ‘Ÿ }, where â€œFactualâ€ updates represent a factual
state of nature observed by an agent, and â€œLedgerâ€ updates are what the agent shares with the
ledger, which may differ from what she factually observes. We note that the ledger (which for
simplicity we assume is a centralized third party) does not make the data public, but only shares the
algorithmâ€™s updated outputs according to the protocolâ€™s rules. The computation algorithm ğœŒ (Ut) is
an algorithm that receives a series of updates Ut = (ğ‘ˆ1, ..., ğ‘ˆğ‘¡ ) of any length ğ‘¡ and outputs a result.
In the continuous communication protocol, we have that algorithm outputs are shared with all
agents upon every ledger update.

In this section and Sections 3-4 we focus on the continuous communication protocol. The
continuous communication protocol simulates a system where agents may push updates at any
time, initiated by them and not by the system manager. We model this by allowing them to respond
to any change in the state of the system, including responding to their own ledger updates. The
only limit to an agent endlessly sending updates to the ledger is that we restrict it to update at most
â„“ times subsequently. The continuous communication protocol is a messaging protocol between
nature, the agents, and the ledger. A particular protocol run is instantiated with nature-input I,

4

Fig. 1. A continuous protocol run for I = (< 2, 90 >, < 2, ğ‘¦ >) with some 90 < ğ‘¦ < ğ‘¥ and the algorithm
ğœŒ = max, as explained in the proof of Proposition 1. An agentâ€™s observed history are all the nodes in her line,
or nodes that have an outgoing edge from a node in her line.

which is a series of some length |I| with each element being of the form < ğ‘—, ğ‘ˆ >, which is a tuple
comprised of agent ğ‘— âˆˆ N and an update ğ‘ˆ .
Protocol 1: The continuous communication protocol
Input: Nature-input I, Parameter â„“ the maximum number of subsequent updates by an

agent

Output: Full Messaging History

1 for factual message < ğ‘—, ğ‘ˆğ‘“ ğ‘ğ‘ğ‘¡ > in I do
2

Nature sends a message to ğ‘— with < ğ‘—, ğ¹ğ‘ğ‘ğ‘¡ğ‘¢ğ‘ğ‘™, ğ‘ˆğ‘“ ğ‘ğ‘ğ‘¡ >;
activeMessage â† True; // There is an active message
while activeMessage = True do

3

4

5

6

7

8

9

10

/* As long as some agent is responding */
activeMessage â† False;
for agent ğ‘– := 1 to ğ‘› do

if agent ğ‘– wishes to send a ledger update ğ‘ˆğ‘™ğ‘’ğ‘‘ğ‘” and last â„“ updates are not all of type
< ğ‘–, ğ¿ğ‘’ğ‘‘ğ‘”ğ‘’ğ‘Ÿ, ğ‘ˆ >3 then

ğ‘– sends a message to Ledger with < ğ‘–, ğ¿ğ‘’ğ‘‘ğ‘”ğ‘’ğ‘Ÿ, ğ‘ˆğ‘™ğ‘’ğ‘‘ğ‘” >;
Ledger sends a message to all with ğœŒâ€™s algorithm output over all the past
ledger updates;
activeMessage â† True;

For the analysis, we extract some useful variables from the run of the protocol that will be used

in subsequent examples and proofs.

Let a run ğ‘… be all the messages sent in the system during the application of the continuous
communication protocol with nature-input I (where messages sent to â€™allâ€™ appear once, and the
messages appear in their order of sending).

Let ğ¿ğ‘— (ğ‘…), ğ¹ ğ‘— (ğ‘…) be the sub-sequences of all ledger, factual updates respectively in ğ‘… of agent ğ‘— (if
the index ğ‘— is omitted, then simply all such updates, regardless of an agent). Let ğ‘‚ ğ‘— (ğ‘…) (â€œobserved
historyâ€ of ğ‘—) be all the messages in ğ‘… received or sent by ğ‘— during the run of the nature protocol:
These are all factual updates of ğ‘—, ledger updates by ğ‘—, and algorithm outputs shared by the ledger.
Let ğ‘‚ ğ‘— (ğ‘…)ğ‘–1:ğ‘–2 be the the elements of ğ‘‚ ğ‘— (ğ‘…) starting with index ğ‘–1 and until (and including) index ğ‘–2.

3We can perhaps question whether agent ğ‘– respects the condition that not all of the last â„“ updates are not all of type
< ğ‘–, ğ¿ğ‘’ğ‘‘ğ‘”ğ‘’ğ‘Ÿ, ğ‘ˆ > for some ğ‘ˆ . If she does not, she may send a message regardless of this constraint. But since nature can
choose not to accept/respond to it, we simplify the protocol by assuming the agents self-enforce the constraint.

An update strategy for ğ‘— is a mapping ğ‘  ğ‘— from an observed history ğ‘‚ ğ‘— (ğ‘…) to a ledger update ğ‘ˆğ‘™ğ‘’ğ‘‘ğ‘”
by agent ğ‘—. The truthful update strategy ğ‘¡ğ‘Ÿğ‘¢ğ‘¡â„ ğ‘— is the following: If the last element in ğ‘‚ ğ‘— (ğ‘…) is of
type < ğ‘—, ğ¹ğ‘ğ‘ğ‘¡ğ‘¢ğ‘ğ‘™, ğ‘ˆ >, update with < ğ‘—, ğ¿ğ‘’ğ‘‘ğ‘”ğ‘’ğ‘Ÿ, ğ‘ˆ >. Otherwise, do not update.

5

A full run of the protocol with nature input I and strategies ğ‘ 1, ..., ğ‘ ğ‘› is the run after completion
of the nature protocol where nature uses input I and each agent ğ‘— responds using strategy ğ‘  ğ‘— .
Since weâ€™re interested in the effect of one agent deviating from truthfulness, we say that we run
nature-input I with strategy ğ‘  ğ‘— , where ğ‘— is the deviating agent, and it is assumed that all other
agents ğ‘– â‰  ğ‘— play ğ‘¡ğ‘Ÿğ‘¢ğ‘¡â„ğ‘– . We denote the resulting run ğ‘…I,ğ‘  ğ‘—

We can now define an NCC-attack on the nature protocol given algorithm ğœŒ and updates

.

restriction â„“.

Definition 1. An algorithm ğœŒ is â„“ âˆ’ ğ‘ğ¶ğ¶ âˆ’ ğ‘£ğ‘¢ğ‘™ğ‘›ğ‘’ğ‘Ÿğ‘ğ‘ğ‘™ğ‘’ if there exists an agent ğ‘— and update

strategy ğ‘  ğ‘— such that:

i) There is a full run ğ‘…I,ğ‘  ğ‘—

of the protocol with some nature-input I and the strategy ğ‘  ğ‘— such that its

.
last algorithm output is different from the last algorithm output in ğ‘…I,ğ‘¡ğ‘Ÿğ‘¢ğ‘¡â„ ğ‘—
ii) For any two nature-inputs I, I â€² such that the observed histories satisfy

ğ‘‚ ğ‘— (ğ‘…I,ğ‘¡ğ‘Ÿğ‘¢ğ‘¡â„ ğ‘— ) â‰  ğ‘‚ ğ‘— (ğ‘…Iâ€²,ğ‘¡ğ‘Ÿğ‘¢ğ‘¡â„ ğ‘— ) =â‡’ ğ‘‚ ğ‘— (ğ‘…I,ğ‘  ğ‘— ) â‰  ğ‘‚ ğ‘— (ğ‘…Iâ€²,ğ‘  ğ‘— ).

In words, to consider strategy ğ‘  ğ‘— as a successful attack, the first condition requires that there is a
case where the rest of the agents other than ğ‘— observe something different than the factual truth.
Notice that we strictly require that the other agents (and not only the ledger) observe a different
outcome: If ğ‘  ğ‘— updates with a ledger update that does not match its factual update, but this does
not affect future algorithm outputs, we do not consider it an attack (It is a â€œTree that falls in a
forest unheardâ€). The second condition requires that the attacker is always able to infer (at least in
theory) the last true algorithm output. Under NCC utilities (which we omit formally defining, and
work instead directly with the logical formulation, similar to Definition 1 in [37]), failure to infer
the true algorithm output under strategy ğ‘  ğ‘— makes it worse than ğ‘¡ğ‘Ÿğ‘¢ğ‘¡â„ ğ‘— , no matter how much the
agent manages to mislead others (which is only its secondary goal).

We remark without formal discussion that being â„“-NCC-vulnerable is enough to show that
truthfulness is not an ex-post Nash equilibrium if the agents were to play a non-cooperative game
using strategies ğ‘  ğ‘— with NCC utilities. However, it does not suffice to show that truthfulness is not a
Bayesian-Nash equilibrium, as the cases where the deviation from truthfulness ğ‘  ğ‘— satisfies condition
(ğ‘–) may be of measure 0. We give a stronger definition we call â„“-NCC-vulnerable*, that would
guarantee the inexistence of the truthful Bayesian-Nash equilibrium for any possible probability
measure, by amending condition (ğ‘–) to hold for all cases:

Definition 2. An algorithm ğœŒ is â„“ âˆ’ ğ‘ğ¶ğ¶ âˆ’ ğ‘£ğ‘¢ğ‘™ğ‘›ğ‘’ğ‘Ÿğ‘ğ‘ğ‘™ğ‘’âˆ— if there exists an agent ğ‘— and update

strategy ğ‘  ğ‘— with both condition (ğ‘–ğ‘–) of Definition 1, and:

i*) For every full run ğ‘…I,ğ‘  ğ‘—

of the protocol with some nature-input I, the last algorithm output is

different than the last algorithm output in ğ‘…I,ğ‘¡ğ‘Ÿğ‘¢ğ‘¡â„ ğ‘—

.

As long as there is at least one full run of the protocol, it is clear that being â„“-NCC-vulnerable*
implies being â„“-NCC-vulnerable. Similarly being â„“-NCC-vulnerable(*) implies being (â„“ + 1)-NCC-
vulnerable(*) (i.e., the implication works for both the vulnerable and vulnerable* cases).

In Appendix B, we illustrate the difference between the two definitions, as well as simple proof

techniques, using a simple algorithm.

6

Fig. 2. A general template for the sneak attack. Until the special conditions are met, and after the re-sync is
done, the strategy behaves as ğ‘¡ğ‘Ÿğ‘¢ğ‘¡â„ ğ‘— .

3 ğ‘˜â€“CENTER AND ğ‘˜â€“MEDIAN IN THE CONTINUOUS COMMUNICATION

PROTOCOL

In this section, we analyze the performance of prominent clustering algorithms in terms of our
vulnerability(*) definitions. Together with Section 4 this demonstrates the applicability of the
approach for both unsupervised and supervised learning algorithms.

Definition 3. k-center: Each agentâ€™s update ğ‘ˆ is a set of data points, where each data point is of
the form ğ‘¥ âˆˆ Rğ‘‘ . A possible output of the algorithm is some ğ‘˜ centers that are among the data points
ğ‘¥1, . . . , ğ‘¥ğ‘˜ âˆˆ (cid:208)ğ‘ˆ âˆˆUt ğ‘ˆ . Let ğœ‚ğ‘– = {ğ‘¥ | arg minğ‘˜
ğ‘ˆ âˆˆUt ğ‘ˆ for 1 â‰¤ ğ‘– â‰¤ ğ‘˜ and some ğ¿ğ‘
norm function ||v||ğ‘ = ğ‘âˆšï¸ƒ
ğ‘–=1 ğ‘£ğ‘
(cid:205)ğ‘‘
ğ‘– with ğ‘ â‰¥ 1. In words, ğœ‚ğ‘– is the set of all agents that have ğ‘¥ğ‘– as their
closest point among ğ‘¥1, . . . , ğ‘¥ğ‘˜ . Let ğ¶ (ğ‘¥1, . . . , ğ‘¥ğ‘˜ ) = maxğ‘˜
ğ‘–=1 maxğ‘¥ âˆˆğœ‚ğ‘– ||ğ‘¥ âˆ’ ğ‘¥ğ‘– || be the cost function. In
words, the cost of a possible algorithm output ğ‘¥1, . . . , ğ‘¥ğ‘˜ is the maximum distance between a point and
a center it is attributed to. We then have

ğ‘—=1 ||ğ‘¥ âˆ’ ğ‘¥ ğ‘— ||ğ‘ = ğ‘–}ğ‘¥ âˆˆ(cid:208)

ğœŒğ‘˜âˆ’ğ‘ğ‘’ğ‘›ğ‘¡ğ‘’ğ‘Ÿ (Ut) = arg

min
ğ‘¥1,...,ğ‘¥ğ‘˜ âˆˆ(cid:208)

ğ‘ˆ âˆˆUt ğ‘ˆ

ğ¶ (ğ‘¥1, . . . , ğ‘¥ğ‘˜ ),

(1)

i.e., the ğ‘˜ centers are the ğ‘˜ points among the reported points that minimize the cost if chosen as
centers. Ties (both when determining ğœ‚ğ‘– and the final ğ‘˜ centers) are broken in favor of the candidate
with the smallest norm4.

3.1 Sneak Attacks and Vulnerability
In this subsection, we present a template for a class of attacks. We then show it is successful in
showing the vulnerability of the protocol for ğ‘˜-center.

4If this is not enough to determine, complement it with some arbitrary rule, e.g. over the radian coordinates of the points:
This does not matter for the argument.

7

Strategy Template 1: A template for a sneak attack
Input: Observed history ğ‘‚ ğ‘— . Parameters ğ‘ˆğ‘ğ‘œğ‘›ğ‘‘, ğœŒğ‘ğ‘œğ‘›ğ‘‘, ğ‘ˆğ‘ğ‘¡ğ‘¡ğ‘ğ‘ğ‘˜, ğ‘ˆğ‘Ÿğ‘’âˆ’ğ‘ ğ‘¦ğ‘›ğ‘
Output: A ledger update < ğ‘—, ğ¿ğ‘’ğ‘‘ğ‘”ğ‘’ğ‘Ÿ, ğ‘ˆ >
/* Condition to start attack */

1 if The last element in ğ‘‚ ğ‘— is < ğ‘—, ğ¹ğ‘ğ‘ğ‘¡ğ‘¢ğ‘ğ‘™, ğ‘ˆğ‘ğ‘œğ‘›ğ‘‘ >, the last algorithm output in ğ‘‚ ğ‘— is ğœŒğ‘ğ‘œğ‘›ğ‘‘ , and

the condition to start attack was not invoked before then

Return < ğ‘—, ğ¿ğ‘’ğ‘‘ğ‘”ğ‘’ğ‘Ÿ, ğ‘ˆğ‘ğ‘¡ğ‘¡ğ‘ğ‘ğ‘˜ >
/* Condition to end attack */

2

3 else if The condition to start attack was invoked, after that some agent (either ğ‘— or another)

received a factual update, but the condition to end attack was not yet invoked then
Let ğ‘ˆ be the last update in ğ‘‚ ğ‘— if it is a factual update for ğ‘—, or âˆ… otherwise.
Return < ğ‘—, ğ¿ğ‘’ğ‘‘ğ‘”ğ‘’ğ‘Ÿ, ğ‘ˆ âˆª ğ‘ˆğ‘Ÿğ‘’âˆ’ğ‘ ğ‘¦ğ‘›ğ‘ >

4

5

/* If the special conditions do not hold, act as ğ‘¡ğ‘Ÿğ‘¢ğ‘¡â„ ğ‘— */

6 else if Last update ğ‘ˆ in ğ‘‚ ğ‘— is factual for ğ‘— then
7

Return < ğ‘—, ğ¿ğ‘’ğ‘‘ğ‘”ğ‘’ğ‘Ÿ, ğ‘ˆ >

Notice that when we defined strategies, we required them to be memory-less, i.e., only observe
ğ‘‚ ğ‘— and not their own past behavior (which by itself anyway only depends on the past observed
histories, which are contained in ğ‘‚ ğ‘— ). However, the conditions in Strategy Template 1 require for
example to check whether the attack was initiated before. The technical lemma below shows that
this is possible to infer from ğ‘‚ ğ‘— .
Lemma 1. If ğ‘ˆğ‘ğ‘œğ‘›ğ‘‘ â‰  ğ‘ˆğ‘ğ‘¡ğ‘¡ğ‘ğ‘ğ‘˜ , the sneak attack is well defined, i.e., the conditions to start and end
attack can be implemented using only ğ‘‚ ğ‘— .

We defer the proof details to Appendix C.
Strategy Template 1 presents the general sneak attack form, which requires four parameters:
ğ‘ˆğ‘ğ‘œğ‘›ğ‘‘ , ğœŒğ‘ğ‘œğ‘›ğ‘‘ , the factual update and last algorithm output that serve as a signal for the attacker to
send ğ‘ˆğ‘ğ‘¡ğ‘¡ğ‘ğ‘ğ‘˜ - the deviation from truth performs, and ğ‘ˆğ‘Ÿğ‘’âˆ’ğ‘ ğ‘¦ğ‘›ğ‘ , the update returning the ledger to a
synced state.

Two properties are important for a successful sneak attack. First, the attacker must know with
certainty the algorithm output given the counter-factual that it would have sent ğ‘ˆğ‘ğ‘œğ‘›ğ‘‘ (as ğ‘¡ğ‘Ÿğ‘¢ğ‘¡â„ ğ‘—
would have), rather than ğ‘ˆğ‘ğ‘¡ğ‘¡ğ‘ğ‘ğ‘˜ . Second, after sending both ğ‘ˆğ‘ğ‘¡ğ‘¡ğ‘ğ‘ğ‘˜ and ğ‘ˆğ‘Ÿğ‘’âˆ’ğ‘ ğ‘¦ğ‘›ğ‘ , it should hold
that all future algorithm outputs are the same as if sending only ğ‘ˆğ‘ğ‘œğ‘›ğ‘‘ . For example, if updates
are sets of data points and the algorithm outputs some calculation over their union (later formally
defined in Definition 4 as a set algorithm), this holds if ğ‘ˆğ‘ğ‘œğ‘›ğ‘‘ = ğ‘ˆğ‘ğ‘¡ğ‘¡ğ‘ğ‘ğ‘˜ âˆª ğ‘ˆğ‘Ÿğ‘’âˆ’ğ‘ ğ‘¦ğ‘›ğ‘ .

We formalize this intuition in the following lemma:

Lemma 2. A sneak attack where ğ‘ˆğ‘ğ‘¡ğ‘¡ğ‘ğ‘ğ‘˜ âŠ† ğ‘ˆğ‘ğ‘œğ‘›ğ‘‘, ğ‘ˆğ‘Ÿğ‘’âˆ’ğ‘ ğ‘¦ğ‘›ğ‘ = ğ‘ˆğ‘ğ‘œğ‘›ğ‘‘ \ ğ‘ˆğ‘ğ‘¡ğ‘¡ğ‘ğ‘ğ‘˜ , and that moreover
can infer the last algorithm output in ğ‘…I,ğ‘¡ğ‘Ÿğ‘¢ğ‘¡â„ ğ‘—
after starting the attack and sending ğ‘ˆğ‘ğ‘¡ğ‘¡ğ‘ğ‘ğ‘˜ , satisfies
condition (ğ‘–ğ‘–).

The proof of the lemma is given in Appendix C.
We now give a sneak attack for ğ‘˜-center in R. The example can be extended to a general

dimension Rğ‘‘ by setting the remaining coordinates in the attack parameters to 0.
Example 2. ğ‘˜-center with ğ‘˜ â‰¥ 3 is 1-NCC-vulnerable using a sneak attack: Use Strategy Template 1
with ğ‘ˆğ‘ğ‘œğ‘›ğ‘‘ = {1, 2, 10, . . . , 10ğ‘˜âˆ’1}, ğ‘ˆğ‘ğ‘¡ğ‘¡ğ‘ğ‘ğ‘˜ = {1}, ğ‘ˆğ‘Ÿğ‘’âˆ’ğ‘ ğ‘¦ğ‘›ğ‘ = ğ‘ˆğ‘ğ‘œğ‘›ğ‘‘ \ğ‘ˆğ‘ğ‘¡ğ‘¡ğ‘ğ‘ğ‘˜, ğœŒğ‘ğ‘œğ‘›ğ‘‘ = {âˆ’ğœ–, 0,
with say ğœ– = 1

ğœ–
ğ‘˜âˆ’2,

Condition (ğ‘–) is satisfied for nature-input I = (< 1, , ğœŒğ‘ğ‘œğ‘›ğ‘‘ >, < 2, ğ‘ˆğ‘ğ‘œğ‘›ğ‘‘ >). The run with ğ‘¡ğ‘Ÿğ‘¢ğ‘¡â„2
ğ‘˜âˆ’2 } âˆª {1}.

yields algorithm outputs ğœŒğ‘ğ‘œğ‘›ğ‘‘, {1, 10, . . . , 10ğ‘˜âˆ’1} but the run with ğ‘ 2 yields ğœŒğ‘ğ‘œğ‘›ğ‘‘, ğœŒğ‘ğ‘œğ‘›ğ‘‘ \ { ğœ–

1000 .

ğœ–
ğ‘˜âˆ’3, . . . , ğœ–},

8

(a)

(b)

Fig. 3. An illustration of Example 2 with ğ‘˜ = 3. In (a), the fact that âˆ’ğœ–, 0, ğœ– is the algorithm output is enough
to show that all input elements are within [âˆ’2ğœ–, 2ğœ–], otherwise ğ‘€ would be a better choice for a center. In (b),
which is displayed on a logarithmic scale, we see that given that all prior input elements are within [âˆ’2ğœ–, 2ğœ–],
and with additional elements 1, 2, 10, 100, the algorithm must output {1, 10, 100} as centers for a small enough
ğœ–.

As for condition (ğ‘–ğ‘–): Let I be some nature-input, and let ğ‘¡ be the index of the element of I after
which the algorithm outputs ğœŒğ‘ğ‘œğ‘›ğ‘‘ (i.e., ğ‘¡ + 1 is < 2, ğ‘ˆğ‘ğ‘œğ‘›ğ‘‘ >, upon where agent 2 starts the attack).
Let ğ‘€+ = maxğ‘¥ âˆˆ(cid:208)
ğ‘ˆ âˆˆUt ğ‘ˆ . Assume for simplicity that |ğ‘€+| â‰¥ |ğ‘€âˆ’|, otherwise a
symmetric argument to the one we lay out follows. Given the algorithm output ğœŒğ‘ğ‘œğ‘›ğ‘‘ , we know that ğœ–
is the closest center to ğ‘€+. Thus, ğ‘€+ âˆ’ ğœ– â‰¤ ğ¶ (âˆ’ğœ–, 0, ğœ–) â‰¤ ğ¶ (ğ‘€âˆ’, 0, ğ‘€+) â‰¤ ğ‘€+
2 . The last inequality is
due to that every point is either in [ğ‘€âˆ’, 0] or [0, ğ‘€+], and so its distance from the closest center is at
most 1

2 . We thus have that ğ‘€+ â‰¤ 2ğœ– (as illustrated in Figure 3).

2 max{|ğ‘€âˆ’|, |ğ‘€+|} = ğ‘€+

ğ‘ˆ âˆˆUt ğ‘ˆ , ğ‘€âˆ’ = minğ‘¥ âˆˆ(cid:208)

Therefore, under ğ‘¡ğ‘Ÿğ‘¢ğ‘¡â„2, after agent 2 sends ğ‘ˆğ‘ğ‘œğ‘›ğ‘‘ = {1, 2, 10, . . . , 10ğ‘˜âˆ’1}, we have ğ¶ ({1, 10, . . . , 10ğ‘˜âˆ’1}) â‰¤

1+2ğœ–. For any other choice of ğ‘˜ centers ğ‘¥1, . . . , ğ‘¥ğ‘˜ (that may partially intersect), we have ğ¶ ({ğ‘¥1, . . . , ğ‘¥ğ‘˜ }) â‰¥
2 âˆ’ 2ğœ– (as illustrated in Figure 3). Choosing ğœ– < 1
4 we have that the algorithm output must be
{1, 10, . . . , 10ğ‘˜âˆ’1}. This shows that agent 2 can infer with certainty the algorithm output under ğ‘¡ğ‘Ÿğ‘¢ğ‘¡â„2.
We thus satisfy the conditions of Lemma 2, which guarantees condition (ğ‘–ğ‘–) is satisfied.

3.2 ğ‘˜â€“Center Vulnerability*
In the previous subsection, we have shown that ğ‘˜-Center is vulnerable. However, in this subsection,
we show it is not vulnerable*.

We note that a significant property of the ğ‘˜-center algorithm is that its output is a subset of its

input.

Definition 4. A set algorithm is an algorithm where each update is a set, and the algorithm is

defined over the union of all updates ğ‘† = (cid:208)ğ‘ˆ âˆˆUt ğ‘ˆ .

A multi-set algorithm is an algorithm where each update is a multi-set of data points, and the

algorithm is defined over the sum of all updates ğ‘† = (cid:210)ğ‘ˆ âˆˆUt ğ‘ˆ .

A set-choice algorithm is a set algorithm that satisfies ğœŒ (ğ‘†) âŠ† ğ‘†, i.e., the algorithm output is a

subset of the input.

Many common algorithms such as max, min, or median, are set-choice algorithms, as well as

ğ‘˜-center and ğ‘˜-median that we discuss.

We notice a property of the sneak attack in Example 2: ğ‘ˆğ‘ğ‘¡ğ‘¡ğ‘ğ‘ğ‘˜ deducts points that exist in the
factual update ğ‘ˆğ‘ğ‘œğ‘›ğ‘‘ and does not include them in the ledger update. In fact, throughout the run
of ğ‘ 2 the union of ledger updates by agent 2 is a subset of the union of its factual updates. This
leads us to develop the following distinction. We partition the space of attack strategies (all attacks,
not necessarily just sneak attacks) into two types, explicitly-lying attacks and omission attacks.
This distinction has importance beyond the technical discussion, because of legal and regulatory

9

issues. Strategic firms may be willing to omit data (which can be excused as operational issues,
data cleaning, etc), but not to fabricate data.

Formally, for set and multi-set algorithms, we can partition all non-truthful strategies in the

following way:

Definition 5. An explicitly lying strategy ğ‘  ğ‘— is a strategy that for some nature-input I has a point
ğ‘¥ âˆˆ ğ¿ğ‘— (ğ‘…I,ğ‘  ğ‘— ), ğ‘¥ âˆ‰ ğ¹ ğ‘— (ğ‘…I,ğ‘  ğ‘— ), i.e., the strategy sends a ledger update with a point that does not exist in
the union of all factual updates for that agent.

An omission strategy ğ‘  ğ‘— is a a strategy that satisfies condition (ğ‘–) (i.e., misleads others) that is not

explicitly-lying.

For an omission strategy it must hold that for every run the agent past ledger updates are a subset

of its factual updates, i.e., ğ¿ğ‘— (ğ‘…I,ğ‘  ğ‘— ) âŠ† ğ¹ ğ‘— (ğ‘…I,ğ‘  ğ‘— ).

We now use the notion of explicitly-lying strategy to prove that ğ‘˜-center and ğ‘˜-median are not

vulnerable*. For this we need one more technical notion:

Definition 6. A set-choice algorithm has forceable winners if for any set ğ‘† and a point ğ‘¥ âˆˆ ğ‘†,

there is a set Â¯ğ‘† with ğ‘¥ âˆ‰ Â¯ğ‘† so that ğ‘¥ âˆˆ ğœŒ (ğ‘† âˆª Â¯ğ‘†).

In words, if the point ğ‘¥ is part of the algorithm input, it is always possible to send an update to force
the point ğ‘¥ to be an output of the algorithm. It is interesting to compare this requirement with axioms
of multi-winner social choice functions, as detailed e.g. in [12].

Theorem 1. A set-choice algorithm with forceable winners is not â„“-NCC-vulnerable* for any â„“.

We prove the theorem using the two following claims.

Claim 1. A strategy ğ‘  ğ‘— that satisfies condition (ğ‘–âˆ—) for a set-choice algorithm is explicitly-lying.

Proof. Consider a nature-input where agent ğ‘— receives no factual updates. To satisfy condition
(ğ‘–âˆ—), it must send some ledger update for the algorithm output under ğ‘  ğ‘— to differ from that under
ğ‘¡ğ‘Ÿğ‘¢ğ‘¡â„ ğ‘— . Since the union of all its factual updates is an empty set, it must hold that it sends a data
â–¡
point that does not exist there.

Claim 2. An explicitly-lying strategy ğ‘  ğ‘— for a set-choice algorithm with forceable winners violates
condition (ğ‘–ğ‘–).

Proof. Consider the shortest nature-input I (in terms of number of elements) where ğ‘  ğ‘— sends
a ledger update with an explicit lie ğ‘¥, and let ğ¿ğ‘… = ğ¿ğ‘— (ğ‘…I,ğ‘  ğ‘— ), ğ¹ğ‘… = ğ¹ ğ‘— (ğ‘…I,ğ‘  ğ‘— ) be the union of all
ledger, factual updates respectively by ğ‘—. Let ğ‘† = ğ¹ğ‘… âˆª ğ¿ğ‘…, and < ğ‘–, Â¯ğ‘† > the nature-input element
that generates a factual update of an agent ğ‘– â‰  ğ‘— that forces ğ‘¥ âˆˆ ğœŒ (ğ‘† âˆª Â¯ğ‘†) (such an element exist by
the forceable winners condition). Let ğ¸1 = ğ¹ğ‘… âˆª ğ¿ğ‘… âˆª Â¯ğ‘†, ğ¸2 = ğ¸1 \ {ğ‘¥ }. Notice that ğ‘¥ âˆ‰ Â¯ğ‘† (as required
in Definition 6 of forceable winners), but ğ‘¥ âˆˆ ğ¿ğ‘…, and so ğ¸1 â‰  ğ¸2. Also note that ğ‘¥ âˆ‰ ğ¹ğ‘… (as it is an
explicit lie). Let I1, I2 be I with an additional last element ğ¸1, ğ¸2 respectively.

Now notice that ğ‘‚ ğ‘— (ğ‘…I1,ğ‘  ğ‘— ), ğ‘‚ ğ‘— (ğ‘…I2,ğ‘  ğ‘— ) are composed of the observed history ğ‘‚ ğ‘— (ğ‘…I,ğ‘  ğ‘— ), together
with the observations following each of their different last elements. As the last element is a
factual update of an agent ğ‘– â‰  ğ‘—, the agent sends a truthful ledger update. We then have ğ¿ğ‘… âˆª ğ¸2 =
ğ¿ğ‘… âˆª ((ğ¹ğ‘… âˆª ğ¿ğ‘… âˆª Â¯ğ‘†) \ {ğ‘¥ }) = (ğ¿ğ‘… âˆª {ğ‘¥ }) âˆª ((ğ¹ğ‘… âˆª ğ¿ğ‘… âˆª Â¯ğ‘†) \ {ğ‘¥ }) = ğ¿ğ‘… âˆª (ğ¹ğ‘… âˆª ğ¿ğ‘… âˆª Â¯ğ‘†) = ğ¿ğ‘… âˆª ğ¸1.
Thus, the immediate algorithm output, and any further algorithm output following some ledger
update by agent ğ‘— is taken over the same set, whether it is under I1 or I2, and so identifies. We
conclude that ğ‘‚ ğ‘— (ğ‘…I1,ğ‘  ğ‘— ) = ğ‘‚ ğ‘— (ğ‘…I2,ğ‘  ğ‘— ).

On the other hand, the last algorithm output in ğ‘‚ ğ‘— (ğ‘…I1,ğ‘¡ğ‘Ÿğ‘¢ğ‘¡â„ ğ‘— ) is ğœŒ (ğ¹ğ‘…âˆªğ¸1) = ğœŒ (ğ¹ğ‘…âˆª(ğ¹ğ‘…âˆªğ¿ğ‘…âˆª Â¯ğ‘†)) =
ğœŒ (ğ‘† âˆª Â¯ğ‘†), and thus has the element ğ‘¥ by Definition 6. On the other hand, the last algorithm output

10

Fig. 4. Demonstration of the proof of Claim 2. ğ‘¥ is an explicit lie by agent ğ‘—. ğ‘† is the state of the ledger
under ğ‘¡ğ‘Ÿğ‘¢ğ‘¡â„ ğ‘— . ğ‘† â€² is the state of the ledger under ğ‘  ğ‘— . Â¯ğ‘† is a complementary set to ğ‘† from Definition 6 (forceable
winners). Given that the next ledger update by a truthful agent is either Â¯ğ‘† or Â¯ğ‘† âˆª {ğ‘¥ } (which is represented by
the rows), then the behavior under the different strategies (represented by the columns) is such that under ğ‘  ğ‘— ,
the two underlying states of the world are the same, but not so under ğ‘¡ğ‘Ÿğ‘¢ğ‘¡â„ ğ‘— .

in ğ‘‚ ğ‘— (ğ‘…I2,ğ‘¡ğ‘Ÿğ‘¢ğ‘¡â„ ğ‘— ) is ğœŒ (ğ¹ğ‘… âˆª ğ¸2) = ğœŒ (ğ¹ğ‘… âˆª ((ğ¹ğ‘… âˆª ğ¿ğ‘… âˆª Â¯ğ‘†) \ {ğ‘¥ })) = ğœŒ ((ğ¹ğ‘… âˆª ğ¿ğ‘… âˆª Â¯ğ‘†) \ {ğ‘¥ }). Since ğœŒ is
â–¡
a set-choice algorithm, it does not output ğ‘¥ since it does not appear in the input set.

Corollary 1. ğ‘˜-center is not â„“-NCC-vulnerable* for any â„“.

Proof. ğ‘˜-center is a set-choice algorithm. We show that it has forceable winners. We show
the construction for ğ‘…, but the general ğ‘…ğ‘‘, ğ¿ğ‘ is similar. Let some ğ‘† âŠ† ğ‘… with ğ‘¥ âˆˆ ğ‘†. Let Î” =
max{maxğ‘  âˆˆğ‘† |ğ‘¥ âˆ’ ğ‘  |, 1}. Let Â¯ğ‘† = {ğ‘¥ + Î”, ğ‘¥ âˆ’ Î”} âˆª {ğ‘¥ + 10Î”, . . . , ğ‘¥ + 10ğ‘˜âˆ’1Î”}. It must hold that
ğœŒ (ğ‘† âˆª Â¯ğ‘†) = {ğ‘¥, ğ‘¥ + 10Î”, . . . , ğ‘¥ + 10ğ‘˜âˆ’1Î”}.
â–¡

Corollary 2. ğ‘˜-median is not â„“-NCC-vulnerable* for any â„“.

The proof is given in Appendix D.

4 LINEAR REGRESSION UNDER CONTINUOUS COMMUNICATION
In this section, we study the vulnerability(*) of linear regression.

Definition 7. Multiple linear regression in ğ‘‘ features ğ‘‘ âˆ’ ğ¿ğ‘…: Given a set of data points ğ‘† with ğ‘›
points, where the data points features are a (ğ‘‘ + 1) Ã— ğ‘› matrix X with all elements of the first column
normalized to 1, the targets are a 1 Ã— ğ‘› vector y, then

ğœŒğ‘‘âˆ’ğ¿ğ‘… (Ut) =

ğœŒğ‘‘âˆ’ğ¿ğ‘… (âˆªğ‘¡

ğ‘–=1ğ‘ˆğ‘– ) =

(cid:40)

(Xğ‘‡ X)âˆ’1Xğ‘‡ y X columns are linearly independent
ğ‘ğ‘¢ğ‘™ğ‘™

ğ‘‚ğ‘¡â„ğ‘’ğ‘Ÿğ‘¤ğ‘–ğ‘ ğ‘’.

We slightly abuse notation by defining ğœŒğ‘‘âˆ’ğ¿ğ‘… both as a function on a series of updates Ut, as well as
on a set of data points. The latter satisfies, as long as the columns are linearly independent, ğœŒğ‘‘âˆ’ğ¿ğ‘… (ğ‘†) =
arg minğ›½ âˆˆRğ‘‘ (cid:205)ğ‘– âˆˆ |ğ‘† | (ğ‘¦ğ‘– âˆ’ (cid:205)ğ‘‘
ğ‘– ğ›½ ğ‘— )2. We subsequently assume for simplicity that the columns are
always linearly independent (e.g., by having a first ledger update with ğ‘‘ linearly independent features.
The property is then automatically maintained with any future updates).

ğ‘—=1 ğ‘¥ ğ‘—

It is not difficult to find omission sneak attacks for linear regression, as we demonstrate in
Figure 5. In Example 4 in Appendix E, we show a more complicated explicitly-lying sneak attack for
1 âˆ’ ğ¿ğ‘… (also called â€œsimple linear regressionâ€). The attack can be generalized for ğ‘‘ âˆ’ ğ¿ğ‘…. This yields
Theorem 2. ğ‘‘-LR is 1-NCC-vulnerable.

11

Fig. 5. A sneak attack for simple linear regression. Since the points by others and the factual update of the
agent yield the same LR estimator Ë†ğœŒ, the result of running the regression on all points is Ë†ğœŒ regardless of what
are the actual points by others.

Fig. 6. A general template for the triangulation attack, with ğ‘˜ = 3. Until the special conditions are met, and
after the re-sync is done, the strategy behaves as ğ‘¡ğ‘Ÿğ‘¢ğ‘¡â„ ğ‘— .

4.1 Triangulation Attacks and Vulnerability*
To study vulnerability*, we now define a stronger type of attacks and show they exist for ğ‘‘ âˆ’ ğ¿ğ‘…,
as long as â„“ â‰¥ ğ‘‘ + 2. We name this type of attacks triangulation attacks, and present a template
parameterized by functions ğ‘“1, ..., ğ‘“â„“âˆ’1, â„ in Strategy Template 2.
Strategy Template 2: A template for a triangulation attack
Input: Observed history ğ‘‚ ğ‘— . Functions ğ‘“1, . . . , ğ‘“â„“âˆ’1, â„
Output: A ledger update Ë†ğ‘ˆ

1 Let ğ‘– = 1 if there is a factual update after the last ledger update by ğ‘—.
2 Otherwise, if a triangulation attack is ongoing, let 2 â‰¤ ğ‘– â‰¤ â„“ be its current step or else exit.
3 Let ğœŒğ‘–âˆ’1 be the last algorithm output in ğ‘‚ ğ‘— .
4 if 1 â‰¤ ğ‘– â‰¤ â„“ âˆ’ 1 then
5
6 else if ğ‘– = â„“ then
7

Return < ğ‘—, ğ¿ğ‘’ğ‘‘ğ‘”ğ‘’ğ‘Ÿ, ğ‘“ğ‘– (ğœŒğ‘–âˆ’1) >

Return < ğ‘—, ğ¿ğ‘’ğ‘‘ğ‘”ğ‘’ğ‘Ÿ, â„(ğœŒâ„“âˆ’1) >

The idea of triangulation attacks is that for any state of the ledger, the attacker can find â„“
subsequent updates so that it can both infer the algorithm output if it applied strategy ğ‘¡ğ‘Ÿğ‘¢ğ‘¡â„ ğ‘— instead

12

of ğ‘  ğ‘— (using ğ‘“1, ..., ğ‘“â„“âˆ’1 the â€œtriangulationsâ€), and mislead others by the final update â„. Informally,
this attack has the desirable property that regardless of the state of the ledger (and how corrupted
it may be by previous updates of the attacker), the attacker can infer the true state.

As in the case of the sneak attack, we should show the strategy template can be implemented

using only the information in ğ‘‚ ğ‘— .

Lemma 3. The triangulation attack is well defined, i.e., the conditions in lines 1 and 2 can be
implemented using only information available in ğ‘‚ ğ‘— . The assignment in line 3 is valid, that is, given
that line 3 is executed there exists an algorithm output in ğ‘‚ ğ‘— .

We defer the proof details to Appendix C.
We now prove there is a triangulation attack for ğ‘‘ âˆ’ ğ¿ğ‘… with â„“ â‰¥ ğ‘‘ + 2.

Theorem 3. ğ‘‘ âˆ’ ğ¿ğ‘… is (ğ‘‘ + 2)-NCC-vulnerable* using a triangulation attack ğ‘“1, ..., ğ‘“ğ‘‘+1, â„.

Proof. We shortly outline the overall flow of the proof. First, we give explicit construction of
the {ğ‘“ğ‘– }1â‰¤ğ‘– â‰¤ğ‘‘+1 functions. This suffices to show that condition (ğ‘–ğ‘–) is satisfied, which means there
is an inference function ğ‘– (ğ‘‚ ğ‘— ) that maps observed histories under ğ‘  ğ‘— to the last algorithm output
under ğ‘¡ğ‘Ÿğ‘¢ğ‘¡â„ ğ‘— . Given that inference function, we construct â„ and show that with it condition (ğ‘–âˆ—)
is satisfied. We give a formal treatment of inference function in Definition 10 and Lemma 5 of
Appendix B, but for our purpose in this proof it suffices that it is a map as specified.

Construction of {ğ‘“ğ‘– }1â‰¤ğ‘– â‰¤ğ‘‘+1 and condition (ğ‘–ğ‘–):
Let

be the last algorithm output before the application of ğ‘“ğ‘– . Define

ğœŒğ‘–âˆ’1 =

ğœŒ 1
ğ‘–âˆ’1
. . .
ğœŒğ‘‘+1
ğ‘–âˆ’1

ï£®
ï£¯
ï£¯
ï£¯
ï£¯
ï£°

ï£¹
ï£º
ï£º
ï£º
ï£º
ï£»

ğ‘“ğ‘– (ğœŒğ‘–âˆ’1) = (ğ‘‹ğ‘–, ğ‘¦ğ‘– ),
where ğ‘‹ğ‘– is the (ğ‘‘ + 1) Ã— 1 vector with ğ‘‹ 1

ğ‘– = ğ‘‹ ğ‘–

ğ‘– = 1, and

ğ‘¦ğ‘– =

(cid:40)ğœŒ 1
ğ‘–âˆ’1 + 1
ğœŒ 1
ğ‘–âˆ’1 + ğœŒğ‘–

ğ‘–âˆ’1 + 1

ğ‘– = 1
2 â‰¤ ğ‘– â‰¤ ğ‘‘ + 1

.

Let ğ‘…I,ğ‘  ğ‘—

be a run with some nature-input I and ğ‘  ğ‘— the triangulation attack with the specified
ğ‘“1, . . . , ğ‘“ğ‘‘+1 (and any function â„). Consider all the factual updates by agents â‰  ğ‘— induced by I. They
are each of the form of (ğ‘‹ â€², ğ‘¦ â€²), where ğ‘‹ â€² is of size ğ‘› Ã— (ğ‘‘ + 1) and ğ‘¦ â€² is ğ‘› Ã— 1, and where ğ‘› is
the number of data points in the update. To consider all factual updates of the agents â‰  ğ‘—, we
can vertically concatenate these matrices. Let this aggregate be denoted ğ‘‹ğ¹,âˆ’ğ‘—, ğ‘¦ğ¹,âˆ’ğ‘— . Similarly, let
ğ‘‹ğ¹,ğ‘—, ğ‘¦ğ¹,ğ‘— be the concatenation of all factual updates by ğ‘— . Let the concatenation of all ledger updates
by ğ‘— before submission of any of the ğ‘“ğ‘– updates be ğ‘‹ğ¿,ğ‘—, ğ‘¦ğ¿,ğ‘— . Recall that we denote by ğœŒ0, . . . , ğœŒğ‘‘+1
the algorithm outputs (right before, and after each ğ‘“ğ‘– , e.g. ğ‘“1 is applied after ğœŒ0 and generates ğœŒ1).
ğ‘– be the (concatenated) inputs to the ğ‘‘ âˆ’ ğ¿ğ‘… algorithm that generate ğœŒğ‘– . In terms of the
Let ğ‘‹ â€²
defined variables above, we can write:

ğ‘– , ğ‘¦ â€²

(ğ‘‹ â€²

ğ‘– )ğ‘‡ ğ‘‹ â€²

ğ‘– = (ğ‘‹ğ¹,âˆ’ğ‘— )ğ‘‡ ğ‘‹ğ¹,âˆ’ğ‘— + (ğ‘‹ğ¿,ğ‘— )ğ‘‡ ğ‘‹ğ¿,ğ‘— +

ğ‘–
âˆ‘ï¸

(ğ‘‹ğ‘– )ğ‘‡ ğ‘‹ğ‘–,

(ğ‘‹ â€²

ğ‘– )ğ‘‡ğ‘¦ â€²

ğ‘– = (ğ‘‹ğ¹,âˆ’ğ‘— )ğ‘‡ğ‘¦ğ¹,âˆ’ğ‘— + (ğ‘‹ğ¿,ğ‘— )ğ‘‡ğ‘¦ğ¿,ğ‘— +

ğ‘¡ =1
ğ‘–
âˆ‘ï¸

(ğ‘‹ğ‘– )ğ‘‡ğ‘¦ğ‘–,

ğ‘¡ =1

(2)

To show that condition (ğ‘–ğ‘–) holds, it suffices to show that we can infer the last algorithm output
. Let (ğ‘‹ğ¹ , ğ‘¦ğ¹ ) be the concatenation of all factual updates of all agents,

ğœŒğ‘¡ğ‘Ÿğ‘¢ğ‘¡â„ of the run ğ‘…I,ğ‘¡ğ‘Ÿğ‘¢ğ‘¡â„ ğ‘—
then it is the input that generates ğœŒğ‘¡ğ‘Ÿğ‘¢ğ‘¡â„, and it holds that:

(ğ‘‹ğ¹ )ğ‘‡ ğ‘‹ğ¹ = (ğ‘‹ğ¹,âˆ’ğ‘— )ğ‘‡ ğ‘‹ğ¹,âˆ’ğ‘— + (ğ‘‹ğ¹,ğ‘— )ğ‘‡ ğ‘‹ğ¹,ğ‘—
(ğ‘‹ğ¹ )ğ‘‡ğ‘¦ğ¹ = (ğ‘‹ğ¹,âˆ’ğ‘— )ğ‘‡ğ‘¦ğ¹,âˆ’ğ‘— + (ğ‘‹ğ¹,ğ‘— )ğ‘‡ğ‘¦ğ¹,ğ‘—

(3)

13

Since in Equation 3, besides ğ‘‹ğ¹,âˆ’ğ‘—, ğ‘¦ğ¹,âˆ’ğ‘— , all RHS variables are observed history under ğ‘  ğ‘— , we
ğ¹,âˆ’ğ‘—ğ‘¦ğ¹,âˆ’ğ‘— in order to infer (ğ‘‹ğ¹ )ğ‘‡ ğ‘‹ğ¹ , (ğ‘‹ğ¹ )ğ‘‡ğ‘¦ğ¹ , and

conclude that it is enough to deduce ğ‘‹ğ‘‡
thus also the last algorithm output under ğ‘¡ğ‘Ÿğ‘¢ğ‘¡â„ ğ‘— which is ((ğ‘‹ğ¹ )ğ‘‡ ğ‘‹ğ¹ )âˆ’1(ğ‘‹ğ¹ )ğ‘‡ğ‘¦ğ¹ .

ğ¹,âˆ’ğ‘—ğ‘‹ğ¹,âˆ’ğ‘—, ğ‘‹ğ‘‡

Let (ğ‘‹ğ¹,âˆ’ğ‘— )ğ‘‡ ğ‘‹ğ¹,âˆ’ğ‘—

ğ‘‘ğ‘’ ğ‘“
=

For every 0 â‰¤ ğ‘– â‰¤ ğ‘‘ + 1, we have

Î£1,1
. . .
Î£ğ‘‘+1,1

ï£®
ï£¯
ï£¯
ï£¯
ï£¯
ï£°

. . .

Î£1,ğ‘‘+1

. . . Î£ğ‘‘+1,ğ‘‘+1

, (ğ‘‹ğ¹,âˆ’ğ‘— )ğ‘‡ğ‘¦ğ¹,âˆ’ğ‘— =

ï£¹
ï£º
ï£º
ï£º
ï£º
ï£»

.

ğœ1
ï£®
ï£¯
. . .
ï£¯
ï£¯
ğœğ‘‘+1
ï£¯
ï£°

ï£¹
ï£º
ï£º
ï£º
ï£º
ï£»

ğ‘– ğœŒğ‘– = (ğ‘‹ â€²
(4)
By the construction of ğ‘“ğ‘– , we can rewrite these equations in the following way. Let ğ·ğ‘– be the
ğ‘–,ğ‘– = 1, and all other elements zero. Let ğ‘£ğ‘– be the

1,1 = ğ·ğ‘–

ğ‘–,1 = ğ·ğ‘–

1,ğ‘– = ğ·ğ‘–

(ğ‘‹ â€²

ğ‘– )ğ‘‡ğ‘¦ â€²
ğ‘– .

ğ‘– )ğ‘‡ ğ‘‹ â€²

(ğ‘‘ + 1) Ã— (ğ‘‘ + 1) matrix with ğ·ğ‘–
1 Ã— (ğ‘‘ + 1) vector with

1 = ğ‘£ğ‘–
ğ‘£ğ‘–

ğ‘– =

(cid:40)ğœŒ 1
0 + 1
ğœŒ 1
ğ‘–âˆ’1 + ğœŒğ‘–

ğ‘–âˆ’1 + 1

ğ‘– = 1
ğ‘– > 1

,

and all other elements zero.
We have for 0 â‰¤ ğ‘– â‰¤ ğ‘‘ + 1:

. . .

Î£1,ğ‘‘+1

ğ‘–
âˆ‘ï¸

+

ğ·ğ‘– )ğœŒğ‘– =

ğ‘–
âˆ‘ï¸

ğ‘£ğ‘– .

+

(5)

ğ‘¡ =1
If we examine the differences between the ğ‘– equation and the ğ‘– âˆ’1 equation, we get for 1 â‰¤ ğ‘– â‰¤ ğ‘‘ +1,

ğ‘¡ =1

. . . Î£ğ‘‘+1,ğ‘‘+1

ğœ1
ï£®
ï£¯
. . .
ï£¯
ï£¯
ğœğ‘‘+1
ï£¯
ï£°

ï£¹
ï£º
ï£º
ï£º
ï£º
ï£»

. . .

Î£1,ğ‘‘+1

ğ‘–âˆ’1
âˆ‘ï¸

+

ğ·ğ‘¡ )(ğœŒğ‘– âˆ’ ğœŒğ‘–âˆ’1) = ğ‘£ğ‘– âˆ’ ğ·ğ‘– ğœŒğ‘– .

(6)

ğ‘¡ =1
Notice that for any 1 â‰¤ ğ‘– â‰¤ ğ‘‘ + 1, ğ‘£ğ‘– âˆ’ ğ·ğ‘– ğœŒğ‘– is not the zero vector. If it was, since (ğ‘‹ğ¹,âˆ’ğ‘— )ğ‘‡ ğ‘‹ğ¹,âˆ’ğ‘— +
ğ‘¡ =1 ğ·ğ‘¡ is invertible, we will have that ğœŒğ‘– = ğœŒğ‘–âˆ’1, which would contradict the following claim:

(cid:205)ğ‘–âˆ’1

. . . Î£ğ‘‘+1,ğ‘‘+1

Î£1,1
. . .
Î£ğ‘‘+1,1

(

ï£®
ï£¯
ï£¯
ï£¯
ï£¯
ï£°

Î£1,1
. . .
Î£ğ‘‘+1,1

(

ï£®
ï£¯
ï£¯
ï£¯
ï£¯
ï£°

ï£¹
ï£º
ï£º
ï£º
ï£º
ï£»

ï£¹
ï£º
ï£º
ï£º
ï£º
ï£»

ğ›¼1
ï£®
ï£¯
. . .
ï£¯
ï£¯
ğ›¼ğ‘‘+1
ï£¯
ï£°

ï£¹
ï£º
ï£º
ï£º
ï£º
ï£»

Claim 3. For every algorithm output ğœŒ =

, and a single point update ğ‘ˆ âˆ— = (ğ‘‹ âˆ— = (cid:2)1

ğ‘¥1

. . .

ğ‘¥ğ‘‘ (cid:3) , ğ‘¦âˆ—)

so that ğ‘‹ âˆ— Â· ğœŒ â‰  ğ‘¦âˆ—, the new algorithm output ğœŒ â€² for the data with ğ‘ˆ âˆ— satisfies ğœŒ â€² â‰  ğœŒ, and has a
different value at ğ‘‹ âˆ— than ğ‘‹ âˆ— Â· ğœŒ.

ğ‘–âˆ’1 + ğœŒğ‘–

ğ‘–âˆ’1 + 1 âˆ’ ğœŒ 1

The proof of the claim is given in Appendix E.
Moreover, ğ‘£ğ‘– âˆ’ ğ·ğ‘– ğœŒğ‘– by definition is a vector that has all elements 0 besides element 1 and ğ‘– that
are ğœŒ 1
ğ‘– â‰  0 (since it is not a zero vector), and so the ğ‘–-th element of the vector
is non-zero. Therefore, for the vector ğ‘¤ğ‘– ğ‘‘ğ‘’ ğ‘“
ğ‘¡ =1 ğ·ğ‘¡ (ğœŒğ‘¡ âˆ’ ğœŒğ‘¡ âˆ’1), the ğ‘–-th element is
non-zero as well (Since (cid:205)ğ‘–âˆ’1
ğ‘¡ =1 ğ·ğ‘¡ ğœŒğ‘– has all elements with index higher than ğ‘– âˆ’ 1 as zero). For any ğ‘¤ğ‘¡
with ğ‘¡ â‰¤ ğ‘– âˆ’ 1, all elements with index higher than ğ‘– âˆ’ 1 are zero. Therefore, the set {ğ‘¤ğ‘– }1â‰¤ğ‘– â‰¤ğ‘‘+1 is
linearly independent, and the matrix ğ‘Š where each column ğ‘– is ğ‘¤ğ‘– is invertible. If we let ğ‘€ğœŒ be the

= ğ‘£ğ‘– âˆ’ ğ·ğ‘– ğœŒğ‘– âˆ’ (cid:205)ğ‘–âˆ’1

ğ‘– âˆ’ ğœŒğ‘–

14

Fig. 7. A script-run triangulation attack for 2-LR. The round red points represent an existing state of the ledger.
The yellow x points (in (1)) represent a new factual update for the strategic agent. The red line in (1) represents
the resulting linear regression estimator, if the agent reports truthfully. The four figures (2a)-(2d) show the
flow of our triangulation attack construction. In (2a) is the last state of the ledger before the triangulation,
with no triangulation point sent by the strategic agent. The rest of (2b)-(2d) consecutively add triangulation
points (blue triangles). At the end of the triangulation attack (after (2d)), the linear regression estimator is
different than in (1). It is possible to infer the estimator in (1) using knowledge of the triangulation points
and estimators of (2a)-(2d) (without knowledge of the red points).

matrix where each column ğ‘– is ğœŒğ‘– âˆ’ ğœŒğ‘–âˆ’1, we can rewrite Eq 6 as (ğ‘‹ğ¹,âˆ’ğ‘— )ğ‘‡ ğ‘‹ğ¹,âˆ’ğ‘— ğ‘€ğœŒğ‘Š âˆ’1 = ğ¼ , where ğ¼ is
the (ğ‘‘ + 1) Ã— (ğ‘‘ + 1) identity matrix. We conclude that ğ‘€ğœŒ is invertible and (ğ‘‹ğ¹,âˆ’ğ‘— )ğ‘‡ ğ‘‹ğ¹,âˆ’ğ‘— = ğ‘Š ğ‘€ âˆ’1
ğœŒ .
We can directly calculate the RHS of this expression from the observed history under ğ‘  ğ‘— , and by the
first equation of Eq 5 we can infer (ğ‘‹ğ¹,âˆ’ğ‘— )ğ‘‡ğ‘¦ğ¹,âˆ’ğ‘— = (ğ‘‹ğ¹,âˆ’ğ‘— )ğ‘‡ ğ‘‹ğ¹,âˆ’ğ‘— ğœŒ0, overall concluding the proof
for condition (ğ‘–ğ‘–).

Construction of â„ and condition (i*). Let ğ‘– be the inference function (which existence is
guaranteed by the previous discussion) that matches observed histories running ğ‘  ğ‘— with the true
algorithm outputs under ğ‘¡ğ‘Ÿğ‘¢ğ‘¡â„ ğ‘— . I.e., we has ğ‘– (ğ‘‚ ğ‘— ) = ğœŒğ‘¡ğ‘Ÿğ‘¢ğ‘¡â„. Let the last algorithm output in ğ‘‚ ğ‘— be
(cid:104)
ğœŒ 1
ğ‘™ğ‘ğ‘ ğ‘¡ + 1

. . .

(cid:40)

1

0

0

(cid:105)

(cid:104)

(cid:105)

)

(

,

ğœŒğ‘™ğ‘ğ‘ ğ‘¡ =

. Let â„(ğ‘‚ ğ‘— ) =

ğ‘– (ğ‘‚ ğ‘— ) = ğœŒğ‘™ğ‘ğ‘ ğ‘¡
ğ‘‚ğ‘¡â„ğ‘’ğ‘Ÿğ‘¤ğ‘–ğ‘ ğ‘’.

.

No update

If ğœŒğ‘™ğ‘ğ‘ ğ‘¡ â‰  ğœŒğ‘¡ğ‘Ÿğ‘¢ğ‘¡â„, â„ does not send an update, and so for the nature-input that has observed history
ğ‘‚ ğ‘— the last algorithm output under ğ‘  ğ‘— is different than that under ğ‘¡ğ‘Ÿğ‘¢ğ‘¡â„ ğ‘— , as required by condition
(ğ‘–âˆ—).

If ğœŒğ‘™ğ‘ğ‘ ğ‘¡ = ğœŒğ‘¡ğ‘Ÿğ‘¢ğ‘¡â„, â„ sends an update with a point (ğ‘‹, ğ‘¦) that satisfies ğ‘‹ Â· ğœŒğ‘™ğ‘ğ‘ ğ‘¡ = ğœŒ 1

ğ‘™ğ‘ğ‘ ğ‘¡ â‰  ğœŒ 1

ğ‘™ğ‘ğ‘ ğ‘¡ +1 = ğ‘¦.

By Claim 3, the resulting algorithm output is different from ğ‘– (ğ‘‚ ğ‘— ).

We demonstrate the construction and inference of the triangulation attack in an open-source
implementation https://github.com/yotam-gafni/triangulation_attack. Figure 7 shows a run of the
attack for a random example for 2-LR.

We show an asymptotically matching lower bound for triangulation attacks.

â–¡

ğœŒ 1
ğ‘™ğ‘ğ‘ ğ‘¡
. . .
ğœŒğ‘‘+1
ğ‘™ğ‘ğ‘ ğ‘¡

ï£®
ï£¯
ï£¯
ï£¯
ï£¯
ï£°

ï£¹
ï£º
ï£º
ï£º
ï£º
ï£»

15

Theorem 4. There is no triangulation attack for ğ‘‘ âˆ’ ğ¿ğ‘… with ğ‘‘ âˆ’ 2 or less functions (i.e., â„“ â‰¤ ğ‘‘ âˆ’ 2).
Proof. Consider all nature-input elements that are of the form < ğ‘–, (ğ‘‹, ğ‘¦) >, < ğ‘—, ( Â¯ğ‘‹ ğ‘—, Â¯ğ‘¦ ğ‘— ) >,
where ğ‘‹ is a (ğ‘‘ + 1) Ã— (ğ‘‘ + 1) matrix, and ğ‘¦ is the (ğ‘‘ + 1) Ã— 1 zero vector. ( Â¯ğ‘‹ ğ‘—, Â¯ğ‘¦ ğ‘— ) of the same
sizes but without any restriction over Â¯ğ‘¦ ğ‘— . We show that for any triangulation attack ğ‘  ğ‘— , we can find
two nature-inputs among this family with different observed history under ğ‘¡ğ‘Ÿğ‘¢ğ‘¡â„ ğ‘— , but the same
observed history under ğ‘  ğ‘— .

By the choice of ğ‘¦, the first algorithm output satisfies ğœŒ0 = (ğ‘‹ğ‘‡ ğ‘‹ )âˆ’1ğ‘‹ğ‘‡ğ‘¦ = 0. As we know
from the proof of Theorem 3, in particular Equation 5 (where it was done for a specific given
triangulation attack), that the attack generates ğ‘‘ âˆ’ 1 vector equations for ğ‘‹ğ‘‡ ğ‘‹ (including the one
over ğœŒ0). We also know that the first row of ğ‘‹ğ‘‡ is all 1 elements. We can make it a stricter constraint
by demanding that the first row of ğ‘‹ğ‘‡ ğ‘‹ is of the form (cid:2)ğ‘‘ + 1
0(cid:3). Then, the principal
sub-matrix of ğ‘‹ğ‘‡ ğ‘‹ (removing the first row and column) is a general PSD matrix (as a principal
submatrix of the ğ‘‹ğ‘‡ ğ‘‹ PSD matrix). To uniquely determine such a matrix of size ğ‘‘ Ã— ğ‘‘, we need
ğ‘‘ vector equations, but the triangulation equations only yield ğ‘‘ âˆ’ 1 such equations. So there are
some ğ‘‹1 â‰  ğ‘‹2 that are in the family of nature-inputs and have the same observed history under ğ‘  ğ‘— .
Fix some invertible Â¯ğ‘‹ ğ‘— . Since (ğ‘‹ğ‘‡

Â¯ğ‘‹ ğ‘— ), there must be some ğ‘£ so that

Â¯ğ‘‹ ğ‘— ) â‰  (ğ‘‹ğ‘‡

. . .

0

2 ğ‘‹2 + Â¯ğ‘‹ğ‘‡

1 ğ‘‹1 + Â¯ğ‘‹ğ‘‡

ğ‘—

ğ‘—

(ğ‘‹ğ‘‡

2 ğ‘‹2 + Â¯ğ‘‹ğ‘‡

ğ‘—

Â¯ğ‘‹ ğ‘— )âˆ’1ğ‘£ â‰  (ğ‘‹ğ‘‡

1 ğ‘‹1 + Â¯ğ‘‹ğ‘‡

ğ‘—

Â¯ğ‘‹ ğ‘— )âˆ’1ğ‘£.

If Â¯ğ‘‹ ğ‘— Â¯ğ‘¦ ğ‘— = ğ‘£, then the last algorithm outputs under ğ‘¡ğ‘Ÿğ‘¢ğ‘¡â„ ğ‘— are different for ğ‘‹1, ğ‘‹2, which holds
â–¡

choosing Â¯ğ‘¦ ğ‘— = ( Â¯ğ‘‹ ğ‘— )âˆ’1ğ‘£.

5 THE PERIODIC COMMUNICATION PROTOCOL
The periodic communication protocol simulates a system where update rounds are initiated by
the system manager (or ledger), and not by the agents themselves. After each round, the ledger
shares the algorithm output with all agents. The definitions of section 2 remain consistent with
this periodic setting, with the following minor changes:

â€¢ Since all updates by different agents in a certain round are aggregated together, the distinction

of â„“ subsequent updates becomes irrelevant and we omit it.

â€¢ An identifier of the round number ğ‘Ÿ is added to each nature-input element. That is, each

element is < ğ‘—, ğ‘ˆ , ğ‘Ÿ >, with an agent ğ‘— âˆˆ N , an update ğ‘ˆ , and a round number ğ‘Ÿ .5

Protocol 2: The periodic communication protocol
Input: Nature-input I
Output: Full Messaging History

1 Let ğ‘Ÿğ‘šğ‘ğ‘¥ = max< ğ‘—,ğ‘ˆ ,ğ‘Ÿ > âˆˆI ğ‘Ÿ .

/* For each round of updates */

2 for Â¯ğ‘Ÿ := 1 to ğ‘Ÿğ‘šğ‘ğ‘¥ do

/* For each update in round Â¯ğ‘Ÿ */
for Element < ğ‘—, ğ‘ˆğ‘“ ğ‘ğ‘ğ‘¡, ğ‘Ÿ > with ğ‘Ÿ = Â¯ğ‘Ÿ do

3

4

5

6

7

8

Nature sends a message to ğ‘— with < ğ‘—, ğ¹ğ‘ğ‘ğ‘¡ğ‘¢ğ‘ğ‘™, ğ‘ˆğ‘“ ğ‘ğ‘ğ‘¡ >;

for agent ğ‘– := 1 to ğ‘› do

if agent ğ‘– wishes to send a ledger update ğ‘ˆğ‘™ğ‘’ğ‘‘ğ‘” then

ğ‘– sends a message to Ledger with < ğ‘–, ğ¿ğ‘’ğ‘‘ğ‘”ğ‘’ğ‘Ÿ, ğ‘ˆğ‘™ğ‘’ğ‘‘ğ‘” >;

Ledger sends a message to all with ğœŒâ€™s algorithm output over all the past ledger updates;

5Round numbers are assumed to have natural properties: They are monotonically increasing with later elements of the
nature-input series, each agent has at most one nature-input element assigned to it per round. The first round is ğ‘Ÿ = 1.

16

Fig. 8. A periodic protocol run for I = (< 1, 90, 1 >, < 2, 90, 1 >). Both agents are truthful.

We now show that indeed periodic communication is strictly less vulnerable to attacks, both for

ğ‘˜-center and ğ‘‘ âˆ’ ğ¿ğ‘….
Theorem 5. ğ‘‘ âˆ’ ğ¿ğ‘… is not NCC-vulnerable in the periodic communication protocol.

We prove this theorem using a more general lemma. We first define three useful properties of a

minimization task:

Definition 8. A multi-set minimization problem ğ¶ is of the form ğœŒ (ğ‘†) = arg minğœŒâ€² ğ¶ (ğ‘†, ğœŒ â€²),

where ğ‘† is the algorithm input, ğ¶ is a cost function and ğœŒ â€² is some possible algorithm output.

A minimization problem ğ¶ is separable if ğ¶ (ğ‘†1âŠğ‘†2, ğœŒ) = ğ¶ (ğ‘†1, ğœŒ)+ğ¶ (ğ‘†2, ğœŒ). Separable minimization

problems are also homogeneous in the sense that: ğ¶ (ğ‘† Ã— ğœ†, ğœŒ) = ğœ†ğ¶ (ğ‘†, ğœŒ).

A minimization problem has a unique solution if for every input ğ‘† it has a single algorithm output

ğœŒ â€² that attains the optimal goal.

A minimization problem is non-negative if for every input ğ‘† and possible algorithm output ğœŒ â€²,

ğ¶ (ğ‘†, ğœŒ â€²) â‰¥ 0.

We know that ğ‘‘âˆ’ğ¿ğ‘… under the restriction mentioned (independent columns) has a unique solution.
It is also immediate from its definition as an optimization problem that it satisfies separability.
Theorem 5 now follows on the following general lemma:

Lemma 4. Any multi-set algorithm ğœŒ that can be formalized as a minimization problem with
separable, non-negative minimization goal ğ¶ with a unique solution is not NCC-vulnerable in the
periodic communication protocol.

Proof. Assume the algorithm is NCC-vulnerable in periodic communication with some strategy
ğ‘  ğ‘— . By condition (ğ‘–), there is nature input I so that the last algorithm output under ğ‘¡ğ‘Ÿğ‘¢ğ‘¡â„ ğ‘— is ğœŒ
and under ğ‘  ğ‘— is ğœŒ â€². Let ğ‘†, ğ‘† â€² be some underlying input to generate ğœŒ, ğœŒ â€² respectively. Since ğœŒ, ğœŒ â€²
are unique solutions, it must hold that 0 â‰¤ ğ¶ (ğ‘† â€², ğœŒ â€²) < ğ¶ (ğ‘† â€², ğœŒ), 0 â‰¤ ğ¶ (ğ‘†, ğœŒ) < ğ¶ (ğ‘†, ğœŒ â€²). Let
Î” = ğ¶ (ğ‘†, ğœŒ â€²) âˆ’ ğ¶ (ğ‘†, ğœŒ), ğ›¿ = ğ¶ (ğ‘† â€², ğœŒ) âˆ’ ğ¶ (ğ‘† â€², ğœŒ â€²), ğœ† = âŒˆ Î”
ğ›¿ âŒ‰ + 1. Now assume that some agent â‰  ğ‘— sends
ğ‘† â€² Ã— ğœ† in the last round of I (call this extension I â€². If all agents already send an update in this round,
add ğ‘† â€² Ã— ğœ† to one of these agentsâ€™ update). Under ğ‘  ğ‘— , we have that ğ¶ (ğ‘† â€² + (ğ‘† â€² Ã— ğœ†), Ë†ğœŒ) = (ğœ† + 1)ğ¶ (ğ‘† â€², Ë†ğœŒ)
and so ğœŒ remains the unique solution (The argmin does not change under multiplication of the cost
function). Under ğ‘¡ğ‘Ÿğ‘¢ğ‘¡â„ ğ‘— , we have

ğ¶ (ğ‘† + (ğ‘† â€² Ã— ğœ†), ğœŒ â€²) = ğ¶ (ğ‘†, ğœŒ â€²) + ğœ†ğ¶ (ğ‘† â€², ğœŒ â€²) = ğ¶ (ğ‘†, ğœŒ) + Î” + ğœ†(ğ¶ (ğ‘† â€², ğœŒ) âˆ’ ğ›¿) =
ğ¶ (ğ‘† âˆª (ğ‘† â€² Ã— ğœ†), ğœŒ) + Î” âˆ’ ğœ†ğ›¿ < ğ¶ (ğ‘† âˆª (ğ‘† â€² Ã— ğœ†), ğœŒ),

17

Fig. 9. Demonstration of the proof of Lemma 4. Under ğ‘  ğ‘— , the estimator for 1-LR is the same whether the
other agent additionally submits âˆ… or 3 Ã— ğ‘† â€², but not so under ğ‘¡ğ‘Ÿğ‘¢ğ‘¡â„ ğ‘— .

and so ğœŒ is not the optimal algorithm output.

We thus have a violation of condition (ğ‘–ğ‘–): There are two nature inputs (I, I â€²) with the same
â–¡

observed history under ğ‘  ğ‘— but different under ğ‘¡ğ‘Ÿğ‘¢ğ‘¡â„ ğ‘— .

In the appendix, we prove a similar result for ğ‘˜-center. The result also holds for ğ‘˜-median and is

done by extending the construction of Corollary 2.

Theorem 6. ğ‘˜-center is not vulnerable under the periodic communication protocol.

6 DISCUSSION
In this work, we lay the groundwork for the study of exclusivity attacks in long-term data sharing.
We present two protocols for long-term communication and show that the choice of protocol, as
well as the number of Sybil identities an attacker may control, matters for the safety of the system.
We do so by analyzing two representative and popular algorithms of supervised and unsupervised
learning, namely linear regression and k-center. We show that the distinction between omission
and explicitly-lying attacks has theoretical significance, and present two general attack templates
that are useful to consider against any possible algorithm. However, we believe that these are the
first steps and that there is much more to study regarding systemsâ€™ safety from exclusivity attacks.
We now expand on a few possible future directions.

6.1 Further Model Extensions
6.1.1 Varying Temporal Resilience. In our model, condition (ğ‘–ğ‘–) requires one pair of confounding
nature-inputs, i.e., one state of the world where the agent can not infer the true best model fit.
However, when dealing with collaborative computing, some organizations may have different
â€œtemporal resilienceâ€. While some depend daily on the learned parameters, others operate in longer

18

time scales such as issuing weekly or monthly reports. In such cases, an attacker ğ‘— may be willing
to incur being confounded, as long as the confusion is bounded within a small number of algorithm
outputs, after which it can again infer the true parameters. Adjusting the model to accommodate
such heterogeneous preferences and how they affect the results can be interesting.

6.1.2 Horizontal vs. Vertical Data Split. In multi-agent collaborative learning tasks, a common
distinction is between â€œHorizontalâ€ and â€œVerticalâ€ data split [39]. A horizontal split is when the set
of features is shared among agents, but the data points may differ. Vertical split is when the data
points are related to the same users, but the feature space is different among agents. While our
model is general and can accommodate both cases, our results largely deal with the horizontal case,
and it would be interesting to look into the vertical case as well.

6.1.3 Application to Silo-ed Federated Learning. A leading motivation for developing the theory in
this work is to apply it to federated learning, in particular in the context where the contributors are
a few large firms (referred to as Silo-ed federated learning in [21]). As we know from the case of the
Average algorithm [22], changing the amount of information shared with the agents can determine
the safety of the collaboration (In the Average case, whether the denominator of the number of
samples is shared alongside the average itself). Applied in the context of federated learning, design
choices such as split learning [15], keeping hyper-parameters at the aggregator level and not the
client level (Notice that this is in contrast with the design of the popular FederatedAveraging
algorithm [27]!), or varying the accuracy of the model supplied to agents [26], can be promising
ideas to deter NCC attacks. Another issue that needs to be addressed is that of learning being
resistant to permutations over the order of samples [34]. In the set and multi-set algorithms we
treat in this work, the order of the updates does not matter for the algorithm output, and so it is
possible to strategically control how and when to share factual data, for example in sneak attacks.
However, in training neural nets, the order of feeding samples can change the final model (See the
discussion in 1.4.2 in [28]).

6.1.4 Relaxing the NCC Requirements and Approximate Mechanisms. The requirement from exclu-
sivity attacks to be able to infer the exact true algorithm output seems harsh. This is especially true
when dealing with statistical estimators, that by their nature are prone to noise. So, it is interesting
to see how do the positive results of our work (in the sense of no-vulnerability of an algorithm
under some settings) hold when attackers are willing to suffer some ğœ– degradation of the algorithm
output in comparison with the true result (under some appropriate metric). Such a discussion also
opens the gate to a mechanism design problem. Once agents are willing to suffer some degradation
of the model, it is possible to consider approximate algorithms that have better incentive-compatible
properties than the standard algorithm. However, simply adding noise to an algorithm does not
guarantee that it is safer. For example, consider that we take the one-shot sum algorithm and add
some 0-mean noise with expected variance ğœ–. Under ğ‘¡ğ‘Ÿğ‘¢ğ‘¡â„ ğ‘— , agent ğ‘— will have a difference of ğœ– from
the true sum in expectation. If agent ğ‘— attacks by adding ğ›¿ to its true number in the ledger update,
and then reduces ğ›¿ from the algorithm output, its expected deviation from the true sum remains
ğœ–, but it is able (by choosing ğ›¿ right) to mislead others on average by more than ğœ–. Therefore, we
remark that a good approximate algorithm to deter attacks should somehow guarantee that the
attack process amplifies the error to hurt the attacker.

Another interesting option that is possible once dealing with a relaxation of NCC is to have
different algorithm outputs sent to different agents, i.e., the protocol does not share a global
algorithm output each time with all agents, but gives a different response to each, hopefully in a
way that helps enforce incentive compatibility.

19

ACKNOWLEDGEMENTS
Yotam Gafni and Moshe Tennenholtz were supported by the European Research Council (ERC)
under the European Unionâ€™s Horizon 2020 research and innovation programme (Grant No. 740435).

REFERENCES
[1] Yehuda Afek, Shaked Rafaeli, and Moshe Sulamy. 2017. Cheating by duplication: Equilibrium requires global knowledge.

(2017). arXiv:1711.04728

[2] Michele Banko and Eric Brill. 2001. Scaling to Very Very Large Corpora for Natural Language Disambiguation.
In Proceedings of the 39th Annual Meeting on Association for Computational Linguistics (ACL â€™01). Association for
Computational Linguistics, USA, 26â€“33. https://doi.org/10.3115/1073012.1073017

[3] Omer Ben-Porat and Moshe Tennenholtz. 2019. Regression Equilibrium. In Proceedings of the 2019 ACM Conference on
Economics and Computation (EC â€™19). Association for Computing Machinery, New York, NY, USA, 173â€“191. https:
//doi.org/10.1145/3328526.3329560

[4] Arnaud Braud, GaÃ«l Fromentoux, Benoit Radier, and Olivier Le Grand. 2021. The Road to European Digital Sovereignty

with Gaia-X and IDSA. IEEE Network 35, 2 (2021), 4â€“5. https://doi.org/10.1109/MNET.2021.9387709

[5] Yang Cai, Constantinos Daskalakis, and Christos Papadimitriou. 2015. Optimum Statistical Estimation with Strategic
Data Sources. In Proceedings of The 28th Conference on Learning Theory (July 3-6) (COLT â€™15). PMLR, 280â€“296. https:
//proceedings.mlr.press/v40/Cai15.html

[6] Hau Chan, Aris Filos-Ratsikas, Bo Li, Minming Li, and Chenhao Wang. 2021. Mechanism Design for Facility Location
Problems: A Survey. In Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence (IJCAI â€™21).
AAAI, 4356â€“4365. https://doi.org/10.24963/ijcai.2021/596

[7] Yiling Chen, Yang Liu, and Chara Podimata. 2020. Learning Strategy-Aware Linear Classifiers. In Advances in
Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020 (De-
cember 6-12) (NeurIPS â€™20). Curran Associates, Inc., 15265â€“15276. https://proceedings.neurips.cc/paper/2020/hash/
ae87a54e183c075c494c4d397d126a66-Abstract.html

[8] European Commission, Content Directorate-General for Communications Networks, Technology, E Scaria, A Bergh-
mans, M Pont, C Arnaut, and S Leconte. 2018. Study on data sharing between companies in Europe : final report.
Publications Office. https://doi.org/10.2759/354943

[9] Ronald Cramer, Ivan Bjerre DamgÃ¥rd, and Jesper Buus Nielsen. 2015. Secure Multiparty Computation and Secret Sharing.

Cambridge University Press. https://doi.org/10.1017/CBO9781107337756

[10] Ofer Dekel, Felix Fischer, and Ariel D Procaccia. 2010. Incentive compatible regression learning. J. Comput. System Sci.

76, 8 (2010), 759â€“777.

[11] Cynthia Dwork. 2008. Differential Privacy: A Survey of Results. In Theory and Applications of Models of Computation,
Manindra Agrawal, Dingzhu Du, Zhenhua Duan, and Angsheng Li (Eds.). Springer Berlin Heidelberg, Berlin, Heidelberg,
1â€“19.

[12] Edith Elkind, Piotr Faliszewski, Piotr Skowron, and Arkadii Slinko. 2017. Properties of multiwinner voting rules. Social

Choice and Welfare 48, 3 (2017), 599â€“632.

[13] Yotam Gafni, Ron Lavi, and Moshe Tennenholtz. 2020. VCG under Sybil (False-Name) Attacks - A Bayesian Analysis.
In Proceedings of the 34th AAAI Conference on Artificial Intelligence (February 7-12) (AAAI â€™20). AAAI, 1966â€“1973.
https://doi.org/10.1609/aaai.v34i02.5567

[14] Nicolas Gast, Stratis Ioannidis, Patrick Loiseau, and Benjamin Roussillon. 2020. Linear Regression from Strategic
Data Sources. ACM Transactions on Economics and Computation (TEAC) 8, 2, Article 10 (5 2020), 24 pages. https:
//doi.org/10.1145/3391436

[15] Otkrist Gupta and Ramesh Raskar. 2018. Distributed learning of deep neural network over multiple agents. Journal of

Network and Computer Applications 116 (2018), 1â€“8. https://doi.org/10.1016/j.jnca.2018.05.003

[16] S Louis Hakimi. 1964. Optimum locations of switching centers and the absolute centers and medians of a graph.

Operations research 12, 3 (1964), 450â€“459.

[17] Alon Halevy, Peter Norvig, and Fernando Pereira. 2009. The unreasonable effectiveness of data. IEEE Intelligent Systems

24, 2 (2009), 8â€“12.

[18] Justin D. Harris and Bo Waggoner. 2019. Decentralized and Collaborative AI on Blockchain. In Proceedings of
the Second IEEE International Conference on Blockchain (IEEE-Blockchain 2019). IEEE Computer Society, 368â€“375.
https://doi.org/10.1109/Blockchain.2019.00057

[19] Dorit S Hochbaum and David B Shmoys. 1985. A best possible heuristic for the k-center problem. Mathematics of

operations research 10, 2 (1985), 180â€“184.

[20] Nicole Immorlica, Adam Tauman Kalai, Brendan Lucier, Ankur Moitra, Andrew Postlewaite, and Moshe Tennenholtz.
2011. Dueling algorithms. In Proceedings of the 43rd ACM Symposium on Theory of Computing (June 6-8) (STOC â€™11).

20

ACM, 215â€“224. https://doi.org/10.1145/1993636.1993666

[21] Peter Kairouz, H. Brendan McMahan, Brendan Avent, AurÃ©lien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji, Kallista
Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, Rafael G. L. Dâ€™Oliveira, Hubert Eichner, Salim El
Rouayheb, David Evans, Josh Gardner, Zachary Garrett, AdriÃ  GascÃ³n, Badih Ghazi, Phillip B. Gibbons, Marco Gruteser,
Zaid Harchaoui, Chaoyang He, Lie He, Zhouyuan Huo, Ben Hutchinson, Justin Hsu, Martin Jaggi, Tara Javidi, Gauri
Joshi, Mikhail Khodak, Jakub KonecnÃ½, Aleksandra Korolova, Farinaz Koushanfar, Sanmi Koyejo, TancrÃ¨de Lepoint,
Yang Liu, Prateek Mittal, Mehryar Mohri, Richard Nock, Ayfer Ã–zgÃ¼r, Rasmus Pagh, Hang Qi, Daniel Ramage, Ramesh
Raskar, Mariana Raykova, Dawn Song, Weikang Song, Sebastian U. Stich, Ziteng Sun, Ananda Theertha Suresh, Florian
TramÃ¨r, Praneeth Vepakomma, Jianyu Wang, Li Xiong, Zheng Xu, Qiang Yang, Felix X. Yu, Han Yu, and Sen Zhao.
2021. Advances and Open Problems in Federated Learning. Foundations and TrendsÂ® in Machine Learning 14, 1â€“2
(2021), 1â€“210. https://doi.org/10.1561/2200000083

[22] Murat Kantarcioglu and Wei Jiang. 2013. Incentive Compatible Privacy-Preserving Data Analysis. IEEE Transactions

on Knowledge and Data Engineering 25, 6 (2013), 1323â€“1335. https://doi.org/10.1109/TKDE.2012.61

[23] Ming Li, Jian Weng, Anjia Yang, Wei Lu, Yue Zhang, Lin Hou, Jia-Nan Liu, Yang Xiang, and Robert H. Deng. 2019.
IEEE Transactions on Parallel and

CrowdBC: A Blockchain-Based Decentralized Framework for Crowdsourcing.
Distributed Systems 30, 6 (2019), 1251â€“1266. https://doi.org/10.1109/TPDS.2018.2881735

[24] Manlu Liu, Kean Wu, and Jennifer Jie Xu. 2019. How will blockchain technology impact auditing and accounting:

Permissionless versus permissioned blockchain. Current Issues in Auditing 13, 2 (2019), A19â€“A29.

[25] Yuan Lu, Qiang Tang, and Guiling Wang. 2018. On Enabling Machine Learning Tasks atop Public Blockchains: A
Crowdsourcing Approach. In 2018 IEEE International Conference on Data Mining Workshops (November 17-20) (ICDMW).
IEEE Computer Society, 81â€“88. https://doi.org/10.1109/ICDMW.2018.00019

[26] L. Lyu, J. Yu, K. Nandakumar, Y. Li, X. Ma, J. Jin, H. Yu, and K. Ng. 2020. Towards Fair and Privacy-Preserving
IEEE Transactions on Parallel & Distributed Systems 31, 11 (11 2020), 2524â€“2541. https:

Federated Deep Models.
//doi.org/10.1109/TPDS.2020.2996273

[27] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise AgÃ¼era y Arcas. 2017. Communication-
Efficient Learning of Deep Networks from Decentralized Data. In Proceedings of the 20th International Conference on
Artificial Intelligence and Statistics (April 20-22) (AISTATS â€™17), Vol. 54. PMLR, 1273â€“1282. http://proceedings.mlr.press/
v54/mcmahan17a.html

[28] GrÃ©goire Montavon, GeneviÃ¨ve Orr, and Klaus-Robert MÃ¼ller. 2012. Neural networks: tricks of the trade. Vol. 7700.

Springer.

[29] HervÃ© Moulin. 1980. On strategy-proofness and single peakedness. Public Choice 35, 4 (1980), 437â€“455.
[30] Robert Nix and Murat Kantarciouglu. 2011. Incentive compatible privacy-preserving distributed classification. IEEE

Transactions on Dependable and Secure Computing 9, 4 (2011), 451â€“462.

[31] OECD. 2015. Data-Driven Innovation: Big Data for Growth and Well-Being. OECD Publishing. https://doi.org/10.1787/

9789264229358-en

[32] Ariel D. Procaccia and Moshe Tennenholtz. 2013. Approximate Mechanism Design without Money. ACM Transactions
on Economics and Computation (TEAC) 1, 4, Article 18 (12 2013), 26 pages. https://doi.org/10.1145/2542174.2542175

[33] Ocean Protocol. 2021. Tools for the Web3 Data Economy. Retrieved January 19, 2022 from https://oceanprotocol.com/tech-

whitepaper.pdf

[34] Siamak Ravanbakhsh, Jeff G. Schneider, and BarnabÃ¡s PÃ³czos. 2016. Deep Learning with Sets and Point Clouds. (2016).

arXiv:1611.04500

[35] Heiko Richter and Peter R Slowinski. 2019. The data sharing economy: on the emergence of new intermediaries.

IIC-International Review of Intellectual Property and Competition Law 50, 1 (2019), 4â€“29.
[36] George AF Seber and Alan J Lee. 2012. Linear regression analysis. Vol. 329. John Wiley & Sons.
[37] Yoav Shoham and Moshe Tennenholtz. 2005. Non-Cooperative Computation: Boolean Functions with Correctness and

Exclusivity. Theor. Comput. Sci. 343, 1â€“2 (10 2005), 97â€“113. https://doi.org/10.1016/j.tcs.2005.05.009

[38] James Surowiecki. 2005. The wisdom of crowds. Anchor.
[39] Qiang Yang, Yang Liu, Tianjian Chen, and Yongxin Tong. 2019. Federated machine learning: Concept and applications.

ACM Transactions on Intelligent Systems and Technology (TIST) 10, 2 (2019), 1â€“19.

[40] Makoto Yokoo, Yuko Sakurai, and Shigeo Matsubara. 2004. The effect of false-name bids in combinatorial auctions:
new fraud in internet auctions. Games and Economic Behavior 46, 1 (2004), 174â€“188. https://doi.org/10.1016/S0899-
8256(03)00045-9

21

A RELATED WORK

A.1 Collaborative Machine Learning
Data sharing between companies and institutions is an emerging phenomenon in the data economy
[8], still under-performing its full potential. Many companies, cloud services, and government
initiatives [4] offer frameworks and APIs to facilitate such exchange, as well as some decentralized
blockchain services [33]. However, the current focus of these services is in organizing the nuts and
bolts of such procedures (e.g. in terms of software, scale, and cyber-security), and not in ensuring
incentive-compatibility, in particular dealing with exclusivity attacks.

Mechanisms based on VCG and the Shapley-value were suggested as a method to construct
general incentive-compatible mechanisms for data collaboration in [30]. We highlight three main
aspects of that work that differ from our approach: They assume the existence of a test set for each
agent to compare other firmsâ€™ inputs (separate from the data set it communicates with others); They
consider a one-shot process rather than a continuous one, and they use monetary transfers while
we consider data sharing a barter between firms without exchanging money. The assumptions
we share with this work are that the true output of the machine learning algorithm is the best
parameter possible to learn and that the agentsâ€™ utilities are the NCC framework utilities.

In [18] the authors consider continuous data sharing implemented by a blockchain, with various
incentive mechanisms depending on the assumptions for agentsâ€™ incentives. An essential difference
with our work is that the data is assumed to be posted publicly and is thus known to all agents.
This is an issue both by itself in terms of privacy, but also when designing incentives. As we will
see, the uncertainty regarding other agentsâ€™ data is essential for the safety of certain mechanisms
under the NCC assumptions.

Federated learning is a popular framework for decentralized machine learning with private
information [27]. The general scheme has each agent perform stochastic gradient descent (SGD) by
itself and share the gradients with an aggregator, in order to train a global model. The global model
is public, and this is inherent to the operation of the mechanism since the agents are expected to
calculate the gradients. There is a natural free-rider attack (mentioned in [21]) in such cases where
the agent shares no data (or, possibly, a small amount of the data it has) and later completes the
training locally based on the global model and its remaining private data. This attack form fits
within our framework of exclusivity attacks, and we discuss in Section 6 how our insights may
apply to it.

There is a line of work that is orthogonal to ours [23, 25], which focuses on assigning model
training tasks to workers, in order to offload computation from being done by the central authority,
or on-chain in the case of a decentralized blockchain. We note that mechanisms built for this task
are different in nature and purpose from data sharing mechanisms.

A.2 Linear Regression and ğ‘˜â€“Center in Adversarial Settings
In this work, we use linear regression and the ğ‘˜-Center and ğ‘˜-Median problems to examine our
NCC utilities framework.

Linear regression [36] is a well-known regression mechanism. We study Multiple Linear Regres-

sion with ğ‘‘ features.

In [7] the authors study linear regression with users that have privacy concerns. In[5] the
authors suggest a mechanism using optimal monetary transfers to induce statistical estimation
using reports by workers that exert effort to attain more precise estimations. The mechanism is
shown to generalize to more general classes of regression than linear regression. Following the
framework of â€œDueling algorithmsâ€ in [20], in [3] the authors consider firms optimizing their
regression models to better satisfy a subset of the users relative to the opponent. In [14] the authors

22

consider firms that control the level of noise they add to the dependent variable, and aim to balance
between privacy (more noise) and model accuracy (less). In [10] the authors consider a general
regression learning model where experts have strong opinions and wish to influence the resulting
model in their favor. As one can see, there are many strategic reasons to manipulate regression
tasks, but the NCC setting is a significant and understudied one.

The ğ‘˜-center and ğ‘˜-median problems [16, 19] are associated with clustering or facility location
algorithms. Facility location problems were studied extensively in strategic settings [6] [29]. The
main focus is usually on strategic users, that may manipulate reporting of their location to influence
the facility locationsâ€™ outcome [32]. For this purpose, strategy-proof mechanisms are developed,
with the goal of a small approximation ratio relative to the optimal (without strategic consideration)
algorithm. Our setting is different as we consider firms that acquired knowledge of usersâ€™ preferences
(or locations), and their goal of manipulation is not to benefit the users they have information
about but to know the resulting aggregate outcome better than the other firms.

B ILLUSTRATION OF PRELIMINARIES USING THE MAX ALGORITHM
We define the max algorithm:

Definition 9. Each update is a real number. ğœŒğ‘šğ‘ğ‘¥ (Ut) = max1â‰¤ğ‘– â‰¤ğ‘¡ ğ‘ˆğ‘– .

Proposition 1. max is not â„“-NCC-vulnerable* for any â„“.

Proof. Consider w.l.o.g. agent 1 has a strategy ğ‘ 1 that satisfies conditions (ğ‘–âˆ—) and (ğ‘–ğ‘–). For the
nature-input I = (< 2, 90 >), under ğ‘¡ğ‘Ÿğ‘¢ğ‘¡â„1 agent 2 receives a factual update < 2, ğ¹ğ‘ğ‘ğ‘¡ğ‘¢ğ‘ğ‘™, 90 > and
then updates with < 2, ğ¿ğ‘’ğ‘‘ğ‘”ğ‘’ğ‘Ÿ, 90 >, resulting in algorithm output 90. By condition (ğ‘–âˆ—), under ğ‘ 1
the algorithm output after the full run must differ from 90. Agent 1 must thus update with at least
one update of the form < 1, ğ¿ğ‘’ğ‘‘ğ‘”ğ‘’ğ‘Ÿ, ğ‘¥ > with ğ‘¥ larger than 90. Now consider the two nature-inputs
I â€² = (< 2, 90 >, < 2, 1
3 Â· 90 >), I â€²â€² = (< 2, 90 >, < 2, 2
3 Â· 90 >). Since ğ‘¥ â‰  90,
the observed histories under ğ‘¡ğ‘Ÿğ‘¢ğ‘¡â„1 for I â€², I â€²â€² are not the same, as the last algorithm output are
3 Â· ğ‘¥ + 2
1

3 Â· 90 respectively. I.e.,

3 Â· 90 â‰  2

3 Â· ğ‘¥ + 1

3 Â· ğ‘¥ + 2

3 Â· ğ‘¥ + 1

ğ‘‚1(ğ‘…Iâ€²,ğ‘¡ğ‘Ÿğ‘¢ğ‘¡â„1) â‰  ğ‘‚1(ğ‘…Iâ€²â€²,ğ‘¡ğ‘Ÿğ‘¢ğ‘¡â„1 ).
Since the prefix of I â€², I â€²â€² is I, we know that under ğ‘ 1 by the end of the first round the algorithm
output is ğ‘¥. After agent 2 receives the second factual update and updates truthfully, the observed
history (in both cases) for agent 1 is (90, < 1, ğ¿ğ‘’ğ‘‘ğ‘”ğ‘’ğ‘Ÿ, ğ‘¥ >, ğ‘¥, ğ‘¥) (where all but the second element
are algorithm outputs). Any strategy ğ‘ 1 response to this observed history will be the same for both
nature-inputs, and thus the observed histories of the full run satisfy

(7)

Equations 7,8 together contradict condition (ğ‘–ğ‘–).

ğ‘‚1(ğ‘…Iâ€²,ğ‘ 1) = ğ‘‚1(ğ‘…Iâ€²â€²,ğ‘ 1).

(8)

â–¡

Example 3. max is 1-NCC-vulnerable

Consider agent 1 with a strategy ğ‘ 1 that upon a factual update for agent 1, and given that there is a
previous algorithm output and the last algorithm output is ğœŒğ‘£, updates with < 1, ğ¿ğ‘’ğ‘‘ğ‘”ğ‘’ğ‘Ÿ, ğœŒğ‘£ >, i.e., the
attacker repeats the last algorithm output as her own ledger update. Condition (ğ‘–) is satisfied: For the
nature-input I = (< 2, 100 >, < 1, 110 >), the last algorithm output for the run with ğ‘ 1 is 100, while
for the run with ğ‘¡ğ‘Ÿğ‘¢ğ‘¡â„1 it is 110. Condition (ğ‘–ğ‘–) is also satisfied: Consider two nature-inputs I, I â€² that
have the same observed run under ğ‘ 1. Notice that the last algorithm output is the maximum over the
other agentsâ€™ truthful ledger updates. The last algorithm output under ğ‘¡ğ‘Ÿğ‘¢ğ‘¡â„1 is the maximum between
other agentsâ€™ ledger updates and agent 1 maximum factual update, which is also observed under ğ‘ 1.
Therefore, the last algorithm output under ğ‘¡ğ‘Ÿğ‘¢ğ‘¡â„1 is determined by the observed history under ğ‘ 1, and

23

the natural way for the attacker to infer it is by taking the max over observed algorithm outputs and
its own factual updates.

We call such methods to construct the algorithm outputs under ğ‘¡ğ‘Ÿğ‘¢ğ‘¡â„ ğ‘— out of the observed history

ğ‘‚ ğ‘— an inference function.

Definition 10. An inference function is a function from observed histories ğ‘‚ ğ‘— to algorithm ğœŒ

outputs.

Lemma 5. If there is an inference function ğ‘– ğ‘— so that for every run ğ‘… of nature-input I with ğ‘  ğ‘— ,
ğ‘– ğ‘— (ğ‘‚ ğ‘— (ğ‘…)) = ğœŒ, where ğœŒ is the last algorithm output of the run of I with ğ‘¡ğ‘Ÿğ‘¢ğ‘¡â„ ğ‘— , then condition (ğ‘–ğ‘–)
holds for ğ‘  ğ‘— .

Proof. Assume by contradiction there are two nature-inputs I, I â€² with the same ğ‘‚ ğ‘— when
running with ğ‘  ğ‘— . The nature-inputs must be of the same length ğ‘Ÿ , otherwise, there would be a
different amount of total factual updates, and thus either a different amount of algorithm updates
not initiated by ğ‘— ledger updates, or a different amount of ğ‘— factual updates, both of which are
â„“ be the nature-inputs of length â„“ that start the same as I, I â€² but end after â„“
observable. Let Iâ„“, I â€²
rounds. Since they have the same observed runs ğ‘‚ â„“
ğ‘— (parameterized by â„“), running with ğ‘¡ğ‘Ÿğ‘¢ğ‘’ ğ‘— they
must have ğ‘– ğ‘— (ğ‘‚ â„“
ğ‘— ) as the algorithm output after the round â„“. We conclude that all algorithm outputs
identify for the two nature-inputs running with ğ‘¡ğ‘Ÿğ‘¢ğ‘¡â„ ğ‘— . The factual updates for ğ‘— also identify for
both nature-inputs since ğ‘€ğ‘‚
identify, and since running with ğ‘¡ğ‘Ÿğ‘¢ğ‘¡â„ ğ‘— the ledger updates by ğ‘— are a
ğ‘—
copy of the factual updates of ğ‘—, they also identify for both nature-inputs. We conclude that the
observable runs for both nature-inputs running with ğ‘¡ğ‘Ÿğ‘¢ğ‘¡â„ ğ‘— identify, in compliance with condition
â–¡
(ğ‘–ğ‘–).

C TECHNICAL LEMMAS FOR THE STRATEGY TEMPLATES
Lemma 1. If ğ‘ˆğ‘ğ‘œğ‘›ğ‘‘ â‰  ğ‘ˆğ‘ğ‘¡ğ‘¡ğ‘ğ‘ğ‘˜ , the sneak attack is well defined, i.e., the conditions to start and end
attack can be implemented using only ğ‘‚ ğ‘— .

Proof. We show that the condition to start attack (line 1) was previously invoked by ğ‘  ğ‘— during
the run of the continuous protocol iff ğ‘‚ ğ‘— contains three subsequent elements, < ğ‘—, ğ¹ğ‘ğ‘ğ‘¡ğ‘¢ğ‘ğ‘™, ğ‘ˆğ‘ğ‘œğ‘›ğ‘‘ >
, < ğ‘—, ğ¿ğ‘’ğ‘‘ğ‘”ğ‘’ğ‘Ÿ, ğ‘ˆğ‘ğ‘¡ğ‘¡ğ‘ğ‘ğ‘˜ >, ğœŒğ‘£1 for some ğœŒğ‘£1: If the condition was invoked, then at that point the last
element in ğ‘‚ ğ‘— was < ğ‘—, ğ¹ğ‘ğ‘ğ‘¡ğ‘¢ğ‘ğ‘™, ğ‘ˆğ‘ğ‘œğ‘›ğ‘‘ >, and the agent updates with < ğ‘—, ğ¿ğ‘’ğ‘‘ğ‘”ğ‘’ğ‘Ÿ, ğ‘ˆğ‘ğ‘¡ğ‘¡ğ‘ğ‘ğ‘˜ >, and
finally the ledger updates all with some algorithm output ğœŒğ‘£1. If it was not invoked before, then
the condition to end attack (in line 3) was not as well (as it depends on the condition to start
attack being previously invoked). Therefore all ledger updates by ğ‘— are of the form of an algorithm
output following some < ğ‘—, ğ¿ğ‘’ğ‘‘ğ‘”ğ‘’ğ‘Ÿ, ğ‘ˆ > after < ğ‘—, ğ¹ğ‘ğ‘ğ‘¡ğ‘¢ğ‘ğ‘™, ğ‘ˆ >. Since ğ‘ˆğ‘ğ‘œğ‘›ğ‘‘ â‰  ğ‘ˆğ‘ğ‘¡ğ‘¡ğ‘ğ‘ğ‘˜ , the pattern
< ğ‘—, ğ¹ğ‘ğ‘ğ‘¡ğ‘¢ğ‘ğ‘™, ğ‘ˆğ‘ğ‘œğ‘›ğ‘‘ >, < ğ‘—, ğ¿ğ‘’ğ‘‘ğ‘”ğ‘’ğ‘Ÿ, ğ‘ˆğ‘ğ‘¡ğ‘¡ğ‘ğ‘ğ‘˜ >, ğœŒğ‘£1 can not appear.

We can thus use the above signature (together with the additional conditions given in line 1) to

decide whether to invoke the condition to start the attack.

The condition to end attack is invoked iff ğ‘‚ ğ‘— last four elements are either of the form

< ğ‘—, ğ¹ğ‘ğ‘ğ‘¡ğ‘¢ğ‘ğ‘™, ğ‘ˆğ‘ğ‘œğ‘›ğ‘‘ >, < ğ‘—, ğ¿ğ‘’ğ‘‘ğ‘”ğ‘’ğ‘Ÿ, ğ‘ˆğ‘ğ‘¡ğ‘¡ğ‘ğ‘ğ‘˜ >, ğœŒğ‘£1, ğœŒğ‘£2,

for some algorithm outputs ğœŒğ‘£1, ğœŒğ‘£2, or of the form

< ğ‘—, ğ¹ğ‘ğ‘ğ‘¡ğ‘¢ğ‘ğ‘™, ğ‘ˆğ‘ğ‘œğ‘›ğ‘‘ >, < ğ‘—, ğ¿ğ‘’ğ‘‘ğ‘”ğ‘’ğ‘Ÿ, ğ‘ˆğ‘ğ‘¡ğ‘¡ğ‘ğ‘ğ‘˜ >, ğœŒğ‘£1, < ğ‘—, ğ¹ğ‘ğ‘ğ‘¡ğ‘¢ğ‘ğ‘™, ğ‘ˆ >,
for some algorithm output ğœŒğ‘£1 and factual update ğ‘ˆ . We verify this signature matches the verbal de-
scription. If this signature appears, by our conclusion, the subsequent elements < ğ‘—, ğ¹ğ‘ğ‘ğ‘¡ğ‘¢ğ‘ğ‘™, ğ‘ˆğ‘ğ‘œğ‘›ğ‘‘ >
, < ğ‘—, ğ¿ğ‘’ğ‘‘ğ‘”ğ‘’ğ‘Ÿ, ğ‘ˆğ‘ğ‘¡ğ‘¡ğ‘ğ‘ğ‘˜ >, ğœŒğ‘£1 show that the condition to start attack was invoked. If we see a factual
update or an algorithm output after that, it can only result in the continuous protocol from some

24

agent receiving a factual update. Since these are the last elements in ğ‘‚ ğ‘— , and there is no additional
ledger update, the condition to end attack could not have previously been invoked since it only
happens after the condition to start attack was invoked and sends an additional Ledger update.

â–¡

Lemma 2. A sneak attack where ğ‘ˆğ‘ğ‘¡ğ‘¡ğ‘ğ‘ğ‘˜ âŠ† ğ‘ˆğ‘ğ‘œğ‘›ğ‘‘, ğ‘ˆğ‘Ÿğ‘’âˆ’ğ‘ ğ‘¦ğ‘›ğ‘ = ğ‘ˆğ‘ğ‘œğ‘›ğ‘‘ \ ğ‘ˆğ‘ğ‘¡ğ‘¡ğ‘ğ‘ğ‘˜ , and that moreover
after starting the attack and sending ğ‘ˆğ‘ğ‘¡ğ‘¡ğ‘ğ‘ğ‘˜ , satisfies
can infer the last algorithm output in ğ‘…I,ğ‘¡ğ‘Ÿğ‘¢ğ‘¡â„ ğ‘—
condition (ğ‘–ğ‘–).

Proof. For condition (ğ‘–ğ‘–), consider two nature-inputs I, I â€² with the same observed run

(9)

ğ‘‚ ğ‘— (ğ‘…I,ğ‘  ğ‘— ) = ğ‘‚ ğ‘— (ğ‘…Iâ€²,ğ‘  ğ‘— ).
If ğ‘‚ ğ‘— (ğ‘…I,ğ‘  ğ‘— ) = ğ‘‚ ğ‘— (ğ‘…I,ğ‘¡ğ‘Ÿğ‘¢ğ‘¡â„ ğ‘— ), it means that the condition to start attack (line 1) of ğ‘  ğ‘— was not
invoked during the run. Thus, there is no update < ğ‘—, ğ¹ğ‘ğ‘ğ‘¡ğ‘¢ğ‘ğ‘™, ğ‘ˆğ‘ğ‘œğ‘›ğ‘‘ > in ğ‘‚ ğ‘— (ğ‘…I,ğ‘  ğ‘— ), and by Eq. 9
also not in ğ‘‚ ğ‘— (ğ‘…Iâ€²,ğ‘  ğ‘— ). We therefore conclude that the condition to start attack is not invoked in
the run of I â€² with ğ‘  ğ‘— . Since the condition to end attack (line 3) is only invoked if at a previous
stage the condition to start attack was invoked, and so we conclude that all updates by ğ‘  ğ‘— for
I â€² are truthful, and therefore ğ‘‚ ğ‘— (ğ‘…Iâ€²,ğ‘  ğ‘— ) = ğ‘‚ ğ‘— (ğ‘…Iâ€²,ğ‘¡ğ‘Ÿğ‘¢ğ‘¡â„ ğ‘— ). All in all, the equations establish that
ğ‘‚ ğ‘— (ğ‘…I,ğ‘¡ğ‘Ÿğ‘¢ğ‘¡â„ ğ‘— ) = ğ‘‚ ğ‘— (ğ‘…Iâ€²,ğ‘¡ğ‘Ÿğ‘¢ğ‘¡â„ ğ‘— ).

If ğ‘‚ ğ‘— (ğ‘…I,ğ‘  ğ‘— ) â‰  ğ‘‚ ğ‘— (ğ‘…I,ğ‘¡ğ‘Ÿğ‘¢ğ‘¡â„ ğ‘— ), then the condition to start attack must have been invoked during
the run and there is a first update ğ‘ˆ =< ğ‘—, ğ¹ğ‘ğ‘ğ‘¡ğ‘¢ğ‘ğ‘™, ğ‘ˆğ‘ğ‘œğ‘›ğ‘‘ > in ğ‘‚ ğ‘— (ğ‘…I,ğ‘  ğ‘— ) such that the preceding
algorithm output is ğœŒğ‘ğ‘œğ‘›ğ‘‘ . Let the index of this element be ğ‘–. Before this update ğ‘ˆ , ğ‘  ğ‘— only responds
truthfully, and so the observed runs satisfy ğ‘‚ ğ‘— (ğ‘…I,ğ‘¡ğ‘Ÿğ‘¢ğ‘¡â„ ğ‘— )1:ğ‘–âˆ’1 = ğ‘‚ ğ‘— (ğ‘…I,ğ‘¡ğ‘Ÿğ‘¢ğ‘¡â„ ğ‘— )1:ğ‘–âˆ’1. Factual updates
are preserved across observed runs with different strategies (ğ‘  ğ‘— vs ğ‘¡ğ‘Ÿğ‘¢ğ‘¡â„ ğ‘— ), and so ğ‘‚ ğ‘— (ğ‘…I,ğ‘¡ğ‘Ÿğ‘¢ğ‘¡â„ ğ‘— )ğ‘– =
ğ‘‚ ğ‘— (ğ‘…I,ğ‘  ğ‘— )ğ‘– = ğ‘‚ ğ‘— (ğ‘…Iâ€²,ğ‘  ğ‘— )ğ‘– = ğ‘‚ ğ‘— (ğ‘…Iâ€²,ğ‘¡ğ‘Ÿğ‘¢ğ‘¡â„ ğ‘— )ğ‘– . Since ğ‘¡ğ‘Ÿğ‘¢ğ‘¡â„ ğ‘— follows each factual update of ğ‘— with a
ledger update with the same ğ‘ˆ , we have ğ‘‚ ğ‘— (ğ‘…I,ğ‘¡ğ‘Ÿğ‘¢ğ‘¡â„ ğ‘— )ğ‘–+1 =< ğ‘—, ğ¿ğ‘’ğ‘‘ğ‘”ğ‘’ğ‘Ÿ, ğ‘ˆğ‘ğ‘œğ‘›ğ‘‘ >= ğ‘‚ ğ‘— (ğ‘…Iâ€²,ğ‘¡ğ‘Ÿğ‘¢ğ‘¡â„ ğ‘— )ğ‘–+1.
Since we require that agent ğ‘— can infer the algorithm output ğœŒğ‘–ğ‘›ğ‘“ ğ‘’ğ‘Ÿ under ğ‘¡ğ‘Ÿğ‘¢ğ‘¡â„ ğ‘— immediately
after the start of the attack, and the observed histories up until this algorithm output identify for
ğ‘…I,ğ‘  ğ‘— , ğ‘…Iâ€²,ğ‘  ğ‘—

, it must hold that ğ‘‚ ğ‘— (ğ‘…I,ğ‘¡ğ‘Ÿğ‘¢ğ‘¡â„ ğ‘— )ğ‘–+2 = ğœŒğ‘–ğ‘›ğ‘“ ğ‘’ğ‘Ÿ = ğ‘‚ ğ‘— (ğ‘…Iâ€²,ğ‘¡ğ‘Ÿğ‘¢ğ‘¡â„ ğ‘— )ğ‘–+2.

We assume for simplicity that the first factual update after the factual update in index ğ‘– is an

update of ğ‘—. The argument can be extended to the case where it is not with more details.

As noted before factual updates are preserved in the observed histories of different strategies,
and so if there are at most ğ‘– + 2 elements in ğ‘‚ ğ‘— (ğ‘…I,ğ‘¡ğ‘Ÿğ‘¢ğ‘¡â„ ğ‘— ), then the factual update that is element ğ‘–
corresponds to the last element in I. Therefore it is also the last factual update in ğ‘‚ ğ‘— (ğ‘…I,ğ‘  ğ‘— ), and by
Eq 9 also in ğ‘‚ ğ‘— (ğ‘…Iâ€²,ğ‘  ğ‘— ), and by the same argument in ğ‘‚ ğ‘— (ğ‘…Iâ€²,ğ‘¡ğ‘Ÿğ‘¢ğ‘¡â„ ğ‘— ). Since in runs with ğ‘¡ğ‘Ÿğ‘¢ğ‘¡â„ ğ‘— each
factual update of ğ‘— is followed exactly by a ledger update of ğ‘— and an algorithm output, we conclude
that there are no more elements after ğ‘– + 2 for ğ‘‚ ğ‘— (ğ‘…Iâ€²,ğ‘  ğ‘— ), and so it identifies with ğ‘‚ ğ‘— (ğ‘…I,ğ‘  ğ‘— ) (as
weâ€™ve shown all elements are the same).

If there is a factual update at index ğ‘– +3 in ğ‘‚ ğ‘— (ğ‘…I,ğ‘¡ğ‘Ÿğ‘¢ğ‘¡â„ ğ‘— ), since we assume it is for ğ‘—, it identifies for
the two nature-inputsâ€™ observed histories with ğ‘  ğ‘— , and as factual updates do not depend on strategy,
we also have ğ‘‚ ğ‘— (ğ‘…I,ğ‘¡ğ‘Ÿğ‘¢ğ‘¡â„ ğ‘— )ğ‘–+3 = ğ‘‚ ğ‘— (ğ‘…Iâ€²,ğ‘¡ğ‘Ÿğ‘¢ğ‘¡â„ ğ‘— )ğ‘–+3. The subsequent ledger update and algorithm
output thus identify as well. Note that by the condition to end attack, for both I, I â€², the ledger
update at step ğ‘– + 4 by ğ‘  ğ‘— is ğ‘ˆğ‘Ÿğ‘’âˆ’ğ‘ ğ‘¦ğ‘›ğ‘ .

At any step after ğ‘– + 4, the union of points sent throughout the ledger history identifies with the
union of points in the factual history, by our requirement that ğ‘ˆğ‘Ÿğ‘’âˆ’ğ‘ ğ‘¦ğ‘›ğ‘ = ğ‘ˆğ‘ğ‘œğ‘›ğ‘‘ \ ğ‘ˆğ‘ğ‘¡ğ‘¡ğ‘ğ‘ğ‘˜ . All in all
this shows that observed histories under ğ‘  ğ‘— identify =â‡’ observed histories under ğ‘¡ğ‘Ÿğ‘¢ğ‘¡â„ ğ‘— identify,
which is logically equivalent to condition (ğ‘–ğ‘–).

â–¡

25

Lemma 3. The triangulation attack is well defined, i.e., the conditions in lines 1 and 2 can be
implemented using only information available in ğ‘‚ ğ‘— . The assignment in line 3 is valid, that is, given
that line 3 is executed there exists an algorithm output in ğ‘‚ ğ‘— .

Proof. We specify the way to implement the required predicates, without giving the full proof,

which goes by an argument similar to the proof structure of Lemma 1.

For line 1, there is a factual update after the last ledger update by agent ğ‘—, iff it is either a factual

update of ğ‘— or of another agent ğ‘ â‰  ğ‘—.

There is such factual update of an agent ğ‘ â‰  ğ‘— iff the last two elements in ğ‘‚ ğ‘— are both algorithm
outputs, or there is only one element in ğ‘‚ ğ‘— and it is an algorithm output (this is the case where there
are no ledger updates by agent ğ‘—). The case where it is a factual update of agent ğ‘— is immediately
visible in ğ‘‚ ğ‘— (agent ğ‘— can see its own factual updates).

For line 2, a triangulation attack is ongoing iff the pattern above is matched, followed by a series
of pairs of the form < ğ‘—, ğ¿ğ‘’ğ‘‘ğ‘”ğ‘’ğ‘Ÿ, ğ‘ˆ >, ğœŒğ‘£1 for some ledger update ğ‘ˆ and algorithm output ğœŒğ‘£1. If the
number of pairs is ğ‘–, then the triangulation attack previously executed ğ‘– steps and we are at step
ğ‘– + 1 of the attack.

For line 3, we reach it only given that ğ‘– is defined, and by the two possible signatures that make
it happen (either starting a new triangulation attack or continuing an ongoing triangulation attack)
assume that there are at least two algorithm outputs in ğ‘‚ ğ‘— , hence it is valid to define ğœŒğ‘–âˆ’1 as the
last algorithm output in ğ‘‚ ğ‘— .

â–¡

D MISSING PROOFS FOR ğ‘˜â€“CENTER AND ğ‘˜â€“MEDIAN
Corollary 2. ğ‘˜-median is not â„“-NCC-vulnerable* for any â„“.

Proof. ğ‘˜-median fits the definition of a set-choice algorithm. We show that it has forceable
winners. We show it for 3-median with the domain ğ‘…, but the proof for general ğ‘˜, ğ‘…ğ‘‘ is similar. Let
ğ‘¥ âˆˆ ğ‘… and a set ğ‘† with ğ‘¥ âˆˆ ğ‘†. Let the symmetric completion of ğ‘† around ğ‘¥ be ğ‘‚ = âˆªğ‘  âˆˆğ‘† {ğ‘ , 2ğ‘¥ âˆ’ ğ‘ }.
I.e., every point ğ‘  = ğ‘¥ + ğœ– is added the matching ğ‘  â€² = ğ‘¥ âˆ’ ğœ–. Let ğ¶ = max{(cid:205)ğ‘œ âˆˆğ‘‚ |ğ‘œ âˆ’ ğ‘¥ |, 1}. Let
Â¯ğ‘† = ğ‘‚ âˆª {ğ‘¥ + 10ğ¶, ğ‘¥ + 100ğ¶}. For this construction, the following claim holds and completes the
proof:
Claim 4. ğœŒ (ğ‘† âˆª Â¯ğ‘†) = {ğ‘¥, ğ‘¥ + 10ğ¶, ğ‘¥ + 100ğ¶}.

Proof. If {ğ‘¥ + 10ğ¶, ğ‘¥ + 100ğ¶} âŠ† ğœŒ (ğ‘† âˆª Â¯ğ‘†), then the remaining center ğœ will have all remaining
points closest to it. If we write ğœ = ğ‘¥ + ğ›¿, then the total cost attributed to this center must satisfy
|ğ›¿ | + (cid:205)ğ‘  âˆˆğ‘† |ğœ–=ğ‘ âˆ’ğ‘¥ >0 |(ğ‘¥ + ğœ–) âˆ’ (ğ‘¥ + ğ›¿)| + |(ğ‘¥ âˆ’ ğœ–) âˆ’ (ğ‘¥ + ğ›¿)| = |ğ›¿ | + (cid:205)ğ‘  âˆˆğ‘† |ğœ–=ğ‘ âˆ’ğ‘¥ >0(| âˆ’ ğœ– âˆ’ ğ›¿ | + |ğœ– âˆ’ ğ›¿ |) â‰¥
|ğ›¿ | + (cid:205)ğ‘  âˆˆğ‘† |ğœ–=ğ‘ âˆ’ğ‘¥ >0 2ğœ–. But the cost if ğ‘¥ is the remaining center is exactly (cid:205)ğ‘  âˆˆğ‘† |ğœ–=ğ‘ âˆ’ğ‘¥ >0 2ğœ–, and so it
is strictly better than the cost with |ğ›¿ | > 0. We conclude that in this case ğ‘¥ is the remaining center.
If either of {ğ‘¥ + 10ğ¶, ğ‘¥ + 100ğ¶} is not in ğœŒ (ğ‘† âˆª Â¯ğ‘†), then the cost of the solution is at least 9ğ¶. But
â–¡

the cost for {ğ‘¥, ğ‘¥ + 10ğ¶, ğ‘¥ + 100ğ¶} is exactly ğ¶, and ğ¶ > 0.

â–¡

Theorem 6. ğ‘˜-center is not vulnerable under the periodic communication protocol.

The proof of the theorem is two-fold using the distinction of explicitly lying and omission
strategies. As for explicitly-lying strategies, Claim 2 holds for periodic communication as well, with
minor adjustments. We are thus left to show:

Lemma 6. An omission strategy ğ‘  ğ‘— for ğ‘˜-center violates condition (ğ‘–ğ‘–) with the periodic protocol.

26

Proof. In the proof of Corollary 1 we show that ğ‘˜-center has forceable winners by a certain
construction. We now use similar ideas to get a construction with more detailed properties, as
formalized in the following claim:

Claim 5. For ğ‘˜-center, for every set ğ‘† with |ğ‘† | â‰¥ 2 and a point ğ‘¥ âˆˆ ğ‘†, there is such ğ‘¦ âˆˆ ğ‘†, and Â¯ğ‘†, Â¯ğ‘† â€² so
that

ğœŒ ((ğ‘† âˆª Â¯ğ‘† â€²) \ {ğ‘¥ }) = ğœŒ (ğ‘† âˆª Â¯ğ‘† â€²) = ğœŒ ((ğ‘† âˆª Â¯ğ‘†) \ {ğ‘¥ }) = {ğ‘¦, ğœ‚1, . . . , ğœ‚ğ‘˜âˆ’1}

for some ğœ‚1, . . . , ğœ‚ğ‘˜âˆ’1 âˆ‰ ğ‘† (in particular, not ğ‘¥).
In addition, Â¯ğ‘† satisfies the conditions of Definition 6.

Proof. We show an explicit construction. Let ğ‘¦ = arg minğ‘¥ â€² âˆˆğ‘†,ğ‘¥ â€²â‰ ğ‘¥ |ğ‘¥ â€² âˆ’ğ‘¥ |. Let Î” = maxğ‘  âˆˆğ‘† |ğ‘¥ âˆ’ğ‘  |.
Let Â¯ğ‘† = {ğ‘¥ + 2Î”, ğ‘¥ âˆ’ 2Î”, ğ‘¥ + 10Î”, ..., ğ‘¥ + 10ğ‘˜âˆ’1Î”, Â¯ğ‘† â€² = {ğ‘¦ + Î”, ğ‘¦ âˆ’ Î”, ğ‘¥ + 10Î”, . . . , ğ‘¥ + 10ğ‘˜âˆ’1Î”}. We
have ğœŒ (ğ‘† âˆª Â¯ğ‘†) = {ğ‘¥, ğ‘¥ + 10Î”, . . . , ğ‘¥ + 10ğ‘˜âˆ’1Î”}, ğœŒ ((ğ‘† âˆª Â¯ğ‘† â€²) \ {ğ‘¥ }) = ğœŒ (ğ‘† âˆª Â¯ğ‘† â€²) = ğœŒ ((ğ‘† âˆª Â¯ğ‘†) \ {ğ‘¥ }) =
{ğ‘¦, ğ‘¥ + 10Î”, . . . , ğ‘¥ + 10ğ‘˜âˆ’1Î”}.
â–¡

We now prove the lemma statement. Consider some nature-input I be the shortest (in terms of
number of elements) where ğ‘  ğ‘— sends a ledger update with an explicit lie ğ‘¥, and let ğ¿ğ‘… = ğ¿ğ‘— (ğ‘…I,ğ‘  ğ‘— ), ğ¹ğ‘… =
ğ¹ ğ‘— (ğ‘…I,ğ‘  ğ‘— ) be the union of all ledger, factual updates respectively by ğ‘—. Let ğ‘† = ğ¹ğ‘… âˆª ğ¿ğ‘… âˆª {ğ‘¥ + 1},
where we add the point ğ‘¥ + 1 to make sure there is some additional point besides ğ‘¥ in ğ‘† and have
|ğ‘† | â‰¥ 2.

Let ğ¸1 = (ğ‘† âˆª Â¯ğ‘†) \ {ğ‘¥ }, ğ¸2 = (ğ‘† âˆª Â¯ğ‘†ğœ– ) \ {ğ‘¥ }. As ğœŒ (ğ¸1 âˆª {ğ‘¥ }) â‰  ğœŒ (ğ¸2 âˆª {ğ‘¥ }), it must hold that ğ¸1 â‰  ğ¸2.
Let ğ‘Ÿ be the last round of I. Let I1, I2 be I with an additional last element < ğ‘–, ğ¸1, ğ‘Ÿ >, < ğ‘–, ğ¸2, ğ‘Ÿ >
respectively, for some agent ğ‘– â‰  ğ‘— (or, if all agents already have an element in this round, add
ğ¸1, ğ¸2 respectively to the element of one of them). We have that ğ‘‚ ğ‘— (ğ‘…I1,ğ‘  ğ‘— ) = ğ‘‚ ğ‘— (ğ‘…I2,ğ‘  ğ‘— ), since the
nature-inputs identify up until the last round, and the last round induces only an algorithm output
ğœŒ ((ğ‘† âˆª Â¯ğ‘†)\{ğ‘¥ })) = ğœŒ (ğ¿ğ‘… âˆªğ¸1).
observation by ğ‘—, which satisfies ğœŒ (ğ¿ğ‘… âˆªğ¸2) = ğœŒ ((ğ‘† âˆª Â¯ğ‘†ğœ– )\{ğ‘¥ }))
However, the last algorithm output in ğ‘‚ ğ‘— (ğ‘…I1,ğ‘¡ğ‘Ÿğ‘¢ğ‘¡â„ ğ‘— ) is ğœŒ (ğ¹ğ‘… âˆªğ¸1) = ğœŒ (ğ¹ğ‘… âˆª(ğ¹ğ‘… âˆªğ¿ğ‘… âˆª Â¯ğ‘†)) = ğœŒ (ğ‘†âˆª Â¯ğ‘†),
and thus has the element ğ‘¥ (By Claim 5 guarantee that Â¯ğ‘† satisfies the conditions of Definition 6).
On the other hand, the last algorithm output in ğ‘‚ ğ‘— (ğ‘…I2,ğ‘¡ğ‘Ÿğ‘¢ğ‘¡â„ ğ‘— ) is ğœŒ (ğ¹ğ‘… âˆª ğ¸2) = ğœŒ (ğ‘† âˆª Â¯ğ‘†ğœ– ), and does
â–¡
not contain ğ‘¥ as an element by Claim 5.

ğ¶ğ‘™ğ‘ğ‘–ğ‘š 5
=

E MISSING PROOFS FOR ğ‘‘â€“LINEAR REGRESSION
Example 4. 1 âˆ’ ğ¿ğ‘… is 1-NCC-vulnerable using an explicitly-lying sneak attack: Use Algorithm 1
with

ğ‘ˆğ‘ğ‘œğ‘›ğ‘‘ = (Xcond, ycond) = (

ï£¹
ï£º
ï£º
ï£º
ï£º
ï£»
ğ‘ˆğ‘Ÿğ‘’âˆ’ğ‘ ğ‘¦ğ‘›ğ‘ = (Xreâˆ’sync, yreâˆ’sync) = (

,

1
ï£®
ï£¯
1
ï£¯
ï£¯
1
ï£¯
ï£°

3
0
0

1
ï£®
ï£¹
ï£¯
ï£º
1
ï£¯
ï£º
ï£¯
ï£º
1
ï£¯
ï£º
ï£°
ï£»
(cid:20)1
2
1 âˆ’1

(cid:21)

,

(cid:21)

(cid:20)0
1

), ğœŒğ‘ğ‘œğ‘›ğ‘‘ = ğ›½ =

(cid:21)

(cid:20)1
0

.

), ğ‘ˆğ‘ğ‘¡ğ‘¡ğ‘ğ‘ğ‘˜ = (Xattack, yattack) = ( (cid:2)1

2(cid:3) , (cid:2)2(cid:3)),

Condition (ğ‘–) is satisfied since for nature-input I = (< 1, (

(cid:20)1
1

(cid:21)

1
0

,

with ğ‘¡ğ‘Ÿğ‘¢ğ‘¡â„2 yields algorithm outputs

(cid:21)

(cid:20)1
0

,

(cid:21)

(cid:20)1
0

but the run with ğ‘ 2 yields

(cid:21)

(cid:20)1
) >, < 2, ğ‘ˆğ‘ğ‘œğ‘›ğ‘‘ >), the run
1
(cid:21)
(cid:20)1
0

(cid:21)

,

.

(cid:20) 5
6
1
2

For condition (ğ‘–ğ‘–), the argument generally follows the proof of Lemma 2. We note two important

distinctions:

27

â€¢ Given ğœŒğ‘ğ‘œğ‘›ğ‘‘, ğ‘ˆğ‘ğ‘œğ‘›ğ‘‘ we can infer the algorithm output under ğ‘¡ğ‘Ÿğ‘¢ğ‘¡â„ ğ‘— is ğœŒğ‘ğ‘œğ‘›ğ‘‘ . That is since ğœŒğ‘ğ‘œğ‘›ğ‘‘
has the minimal cost function given all updates previous to ğ‘ˆğ‘ğ‘œğ‘›ğ‘‘ (for both I, I â€²): We know
that since it is the algorithm output before the factual update ğ‘ˆğ‘ğ‘œğ‘›ğ‘‘ . Moreover, it has cost 0 with
regards to ğ‘ˆğ‘ğ‘œğ‘›ğ‘‘ .

â€¢ At any step after ğ‘– +4 (the completion of the sneak attack), the union of points sent throughout the
ledger history does not identify anymore with the union of points in the factual history, since the
ledger history contains explicit lies (namely, any of the points in our choice of ğ‘ˆğ‘ğ‘¡ğ‘¡ğ‘ğ‘ğ‘˜, ğ‘ˆğ‘Ÿğ‘’âˆ’ğ‘ ğ‘¦ğ‘›ğ‘ ).
However, for the calculation of the algorithm output in 1 âˆ’ ğ¿ğ‘…, we have ğœŒ = (ğ‘‹ğ‘‡ ğ‘‹ )âˆ’1ğ‘‹ğ‘‡ğ‘¦. Since
updates aggregation is additive, as long as two different updates have the same ğ‘‹ğ‘‡ ğ‘‹, ğ‘‹ğ‘‡ğ‘¦, any
sequence of updates containing them would have the same algorithm outputs. In our case, we
have

(ğ‘‹ ğ‘ğ‘œğ‘›ğ‘‘ )ğ‘‡ ğ‘‹ ğ‘ğ‘œğ‘›ğ‘‘ =

(ğ‘‹ ğ‘ğ‘œğ‘›ğ‘‘ )ğ‘‡ğ‘¦ =

(cid:21)

(cid:20)3
3

(cid:20)3
3

(cid:21)

3
9

= (ğ‘‹ ğ‘ğ‘¡ğ‘¡ğ‘ğ‘ğ‘˜ )ğ‘‡ ğ‘‹ ğ‘ğ‘¡ğ‘¡ğ‘ğ‘ğ‘˜ + (ğ‘‹ ğ‘Ÿğ‘’âˆ’ğ‘ ğ‘¦ğ‘›ğ‘ )ğ‘‡ ğ‘‹ ğ‘Ÿğ‘’âˆ’ğ‘ ğ‘¦ğ‘›ğ‘,

= (ğ‘‹ ğ‘ğ‘¡ğ‘¡ğ‘ğ‘ğ‘˜ )ğ‘‡ğ‘¦ğ‘ğ‘¡ğ‘¡ğ‘ğ‘ğ‘˜ + (ğ‘‹ ğ‘Ÿğ‘’âˆ’ğ‘ ğ‘¦ğ‘›ğ‘ )ğ‘‡ğ‘¦ğ‘Ÿğ‘’âˆ’ğ‘ ğ‘¦ğ‘›ğ‘ .

By the construction of ğ‘  ğ‘— , any sequence of updates after the attack is â€œre-syncedâ€ behaves just as
if the agent has acted truthfully, and so the observed truthful histories identify subsequently. All
in all this shows that observed histories under ğ‘  ğ‘— identify =â‡’ observed histories under ğ‘¡ğ‘Ÿğ‘¢ğ‘¡â„ ğ‘—
identify, which is logically equivalent to condition (ğ‘–ğ‘–).

Claim 3. For every algorithm output ğœŒ =

, and a single point update ğ‘ˆ âˆ— = (ğ‘‹ âˆ— = (cid:2)1

ğ‘¥1

. . .

ğ‘¥ğ‘‘ (cid:3) , ğ‘¦âˆ—)

ğ›¼1
ï£®
ï£¯
. . .
ï£¯
ï£¯
ğ›¼ğ‘‘+1
ï£¯
ï£°

ï£¹
ï£º
ï£º
ï£º
ï£º
ï£»

so that ğ‘‹ âˆ— Â· ğœŒ â‰  ğ‘¦âˆ—, the new algorithm output ğœŒ â€² for the data with ğ‘ˆ âˆ— satisfies ğœŒ â€² â‰  ğœŒ, and has a
different value at ğ‘‹ âˆ— than ğ‘‹ âˆ— Â· ğœŒ.

Proof. Let ğœŒ =

ï£¹
ï£º
ï£º
ï£º
ï£º
ğ‘‘+1
ï£»
First, we show that it can not be that ğœŒ = ğœŒ â€². Assume otherwise, then by the extremal condition
over the optimization function,

, and let the algorithm output after adding the point ğ‘‹ âˆ—, ğ‘¦âˆ— be ğœŒ â€² =

ï£¹
ï£º
ï£º
ï£º
ï£º
ï£»

.

ğ›¼1
ï£®
ï£¯
. . .
ï£¯
ï£¯
ğ›¼ğ‘‘+1
ï£¯
ï£°

ğ›¼ â€²
ï£®
1
ï£¯
. . .
ï£¯
ï£¯
ğ›¼ â€²
ï£¯
ï£°

ğ›¼1 =

(cid:205)ğ‘›

ğ‘–=1(ğ‘¦ğ‘– âˆ’ (cid:205)ğ‘‘
ğ‘›

ğ‘—=1 ğ›¼ ğ‘—+1ğ‘¥ ğ‘—
ğ‘– )

,

and similarly for ğœŒ â€² = ğœŒ,

ğ›¼1 =

((cid:205)ğ‘›

ğ‘–=1 ğ‘¦ğ‘– âˆ’ (cid:205)ğ‘‘

ğ‘—=1 ğ›¼ ğ‘—+1ğ‘¥ ğ‘—

ğ‘– ) + (ğ‘¦âˆ— âˆ’ (cid:205)ğ‘‘
ğ‘› + 1

ğ‘—=1 ğ›¼ ğ‘—+1ğ‘¥ âˆ—
ğ‘— )

=

ğ‘›ğ›¼1 + ğ›¼1
ğ‘› + 1

+

ğ‘¦âˆ— âˆ’ (cid:205)ğ‘‘

ğ›¼1 +

ğ‘¦âˆ— âˆ’ (cid:205)ğ‘‘

ğ‘—=1 ğ›¼ ğ‘—+1ğ‘¥ âˆ—
ğ‘› + 1

ğ‘— âˆ’ ğ›¼1

=

ğ‘—=1 ğ›¼ ğ‘—+1ğ‘¥ âˆ—
ğ‘› + 1
ğ‘— âˆ’ ğ›¼1

â‰  ğ›¼1,

where the last inequality is since the point (ğ‘‹ âˆ—, ğ‘¦âˆ—) is outside the line and thus ğ‘¦âˆ—âˆ’(cid:205)ğ‘‘
ğ‘— âˆ’ğ›¼1 â‰ 
0. We arrived at a contradiction and so we may subsequently assume ğœŒ â‰  ğœŒ â€². Now, again assume by
contradiction that the lines intersect at the point (ğ‘‹ âˆ—, ğ‘¦âˆ—). Then, with respect to (ğ‘‹ âˆ—, ğ‘¦âˆ—) the two
lines would have the same cost function value 0, but overall with respect to all ğ‘¦ğ‘– with 1 â‰¤ ğ‘– â‰¤ ğ‘›, ğœŒ

ğ‘—=1 ğ›¼ ğ‘—+1ğ‘¥ âˆ—

is the unique optimal cost minimizer, so we conclude that ğœŒ has lower cost overall than ğœŒ â€² with
â–¡
respect to all the given points, in contradiction to ğœŒ â€² being optimal.

28

