Long-term Data Sharing under Exclusivity Attacks

YOTAM GAFNI yotam.gafni@campus.technion.ac.il
MOSHE TENNENHOLTZ moshet@ie.technion.ac.il

TECHNION - ISRAEL INSTITUTE OF TECHNOLOGY
The quality of learning generally improves with the scale and diversity of data. Companies and institutions
can therefore benefit from building models over shared data. Many cloud and blockchain platforms, as well as
government initiatives, are interested in providing this type of service.

These cooperative efforts face a challenge, which we call “exclusivity attacks”. A firm can share distorted
data, so that it learns the best model fit, but is also able to mislead others. We study protocols for long-term
interactions and their vulnerability to these attacks, in particular for regression and clustering tasks. We
conclude that the choice of protocol, as well as the number of Sybil identities an attacker may control, is
material to vulnerability.

2
2
0
2

n
a
J

2
2

]

R
C
.
s
c
[

1
v
7
3
1
9
0
.
1
0
2
2
:
v
i
X
r
a

 
 
 
 
 
 
1

1 THE WORK IN CONTEXT

1.1 Data Sharing among Firms
In today’s data-oriented economy [31], countless applications are based on the ability to extract
statistically significant models out of acquired user data. Still, firms are hesitant to share information
with other firms [8, 35], as data is viewed as a resource that must be protected. This is in tension
with the paradigm of the Wisdom of the crowds [38], which emphasizes the added predictive value of
aggregating multiple data sources. As early as 2001, the authors in [2] (note also a similar approach
in [17]) concluded that

“... a logical next step for the research community would be to direct efforts towards
increasing the size of annotated training collections, while deemphasizing the focus on
comparing different learning techniques trained only on small training corpora.”

Two popular frameworks to address issues arising in settings where data is shared are multi-party
computation [9] and differential privacy [11]. However, these paradigms are focused on addressing
the issue of privacy (whether of the individual user or the firm’s data bank), but do not answer
the basic conundrum of sharing data with competing firms: On one hand, cooperation enables the
firm to enrich its own models, but at the same time enable other firms to do so as well. A firm is
thus tempted to game the mechanism to allow itself better inference than other firms. We call this
behavior exclusivity attacks. Even if supplying intentionally false information could be a legal risk,
the nature of data processing (rich with outliers, spam accounts, natural biases), allows firms to
have “reasonable justification” to alter the data they share with others.

In this work, we present a model of collaborative information sharing between firms. The goal
of every firm is first to have the best available model given the aggregate data. As a secondary goal,
every firm wishes the others to have a downgraded version of the model. An appropriate framework
to address this objective is the Non-cooperative computation (NCC) framework, introduced in [37].
The framework was considered with respect to one-shot data aggregation tasks in [22].

1.2 Open and Long-term Environments
In our work, we present a general communication protocol for collaborative data sharing among
firms, that can be associated with any specific machine learning or data aggregation algorithm.
The protocol possesses an online nature, when any participating firm may send (additional) data
points at any time. This is in contrast with previous NCC literature, which focuses on one-shot
data-sharing procedures. The long-term setting yields two, somewhat contradicting, attributes:

• A firm may send multiple subsequent inputs to the protocol, using it to learn how the model’s
parameters change after each contribution. For an attacker, this allows better inference of
the true model’s parameters, without revealing its true data points, as we demonstrate in
Example 1 below.

• A firm is not only interested in attaining the current correct parameters of the model, but also
has a future interest to be able to attain correct answers, given that more data is later added
by itself and its competitors. This has a chilling effect on attacks, as even a successful attack
in the one-shot case could result in data corruption. For example, a possible short-term attack
could be for a firm to send its true data, attain the correct parameters, and then send additional
garbage data. Since we do not have built-in protection against such actions in the mechanism
(for reasons further explained in Remark 1), this would result in data corruption for the other
firms. Nevertheless, if the firm itself is interested in attaining meaningful information from
the mechanism in the future, it would be disincentivized to do so.

2

We now give an example demonstrating the first point. In [22], the authors consider the problem of
collaboratively calculating the average of data points. They show in their Theorem 4.6 and Theorem
4.7 that whether the number of different data points is known is essential to the truthfulness of the
mechanism. When the number of data points is unknown, the denominator of the average term is
unknown, and it is impossible for an attacker to know with certainty how to attain the true average
from the average the mechanism reports given a false input of the attacker. We now show that in
a model where it is possible to send multiple requests (in fact, two), it is possible to report false
information and attain the correct average:

Example 1. Consider a firm with some data points 𝐷𝐼 with a total sum 𝑆𝐼 and number of points 𝑁𝐼 .
Other firms have data points 𝐷𝑂 with a total sum 𝑆𝑂 and number of points 𝑁𝑂 . Assume 𝑆𝐼 ≠ 0, 𝑁𝐼 = 2.1
Instead of reporting 𝐷𝐼 , the firm first reports 𝐷 ′ = [0], receives an average 𝑎1, then reports 𝐷 ′′ = [0]
and receives the updated average 𝑎2. The average that others, following the mechanism as given, attain
is
, and they are different by our assumption on 𝑆𝐼 , 𝑁𝐼 . The firm is thus
successful in misleading others. Moreover, the firm can infer the true average. Given

𝑁𝑂 +2 , the true average is 𝑆𝐼 +𝑆𝑂
𝑆𝑂
𝑁𝐼 +𝑁𝑂

the firm2 can calculate

𝑎1 =

𝑆𝑂
𝑁𝑂 + 1

≠

𝑆𝑂
𝑁𝑂 + 2

= 𝑎2,

𝑁𝑂 =

𝑎1 − 2𝑎2
𝑎2 − 𝑎1

, 𝑆𝑂 = 𝑎1(𝑁𝑂 + 1),

and thus have all the information required to calculate the true average.

Remark 1. Why should we not consider simply forbidding multiple subsequent updates by a firm?
As noted in [1, 13, 40], modern internet-based environments lack clear identities and allow for multiple
inputs by the same agent using multiple identities. A common distinction in blockchain networks
separates public (“permissionless”) and private (“permissioned”) networks [24], where public networks
allow open access for everyone, while private networks require additional identification for participation.
In both cases, however, it is impossible to totally prevent false-name manipulation, where a firm uses
multiple identities to send her requests. Therefore, any “simple” solution of the problem demonstrated in
Example 1 is impossible. The mechanism does not know whether multiple subsequent updates are really
sent by different firms, or they are in fact “sock puppets” of a single firm. The mechanism therefore can
not adjust appropriately (e.g., drop any request after the first one). In this work, we assume a firm may
control up to ℓ identities, and so in the formal model, we allow up to ℓ subsequent updates of a single
firm. The false identities are not part of the formal model: They instead are encapsulated by giving
firms this ability to update ℓ times subsequently.

1.3 Our Results

• We define two long-term data-sharing protocols (the continuous and periodic communica-
tion protocols) for data sharing among firms. The models differ in how communication is
structured temporally (whether the agents can communicate at any time, or are asked for
their inputs at given times). Each model can be coupled with any choice of algorithm to
aggregate the data shared by the agents.

• We give a condition for NCC-vulnerability of an algorithm (given the communication model)
in Definition 1. A successful NCC attack is one that (i) Can mislead the other agents, and
(ii) Maintains the attacker’s ability to infer the true algorithm output. We give a stronger

1These assumptions are not required for the attack scheme to succeed, but make for a simpler demonstration.
2The only case where 𝑎1 = 𝑎2 is when 𝑆𝑂 = 0. In this case, upon having 𝑎1 = 0, we can choose 𝐷′′ = [1], and a similar
argument shows that we can infer the true average.

Vulnerable

Vulnerable*

𝑑-LinearRegression Yes, for any ℓ ≥ 1

𝑘-Center

Yes, for any ℓ ≥ 1

Yes

No





ℓ ≥ 𝑑 + 2
ℓ ≤ 𝑑 − 2
No

Table 1. A summary of vulnerability(*) results in the continuous communication protocol.

3

condition of NCC-vulnerability* that can moreover (i*) Mislead the other agents in every
possible scenario. As a simple example of using these definitions, we show in Appendix B
that finding the maximum over agent reports is NCC-vulnerable but not NCC-vulnerable*.
• For the 𝑘-center problem, we show that it is vulnerable under continuous communication but
not vulnerable under periodic communication. Moreover, we show that it is not vulnerable*
even in continuous communication, using a notion of explicitly-lying attacks.

• For Multiple Linear Regression, we show that it is vulnerable* under continuous communi-
cation but not vulnerable under periodic communication. The vulnerability* in continuous
communication depends on the number of identities an attacker can control: We show a form
of attack so that an attacker with 𝑑 + 2 identities (where 𝑑 is the dimension of the feature
space) is guaranteed to have an attack, and an attacker with less than 𝑑 − 2 identities can not
attack.

The vulnerability(*) results for the continuous communication protocol are summarized in Table 1.

Both algorithms are not vulnerable(*) under the periodic communication protocol.

We overview related work in Appendix A.

2 MODEL AND VULNERABILITY NOTIONS
We consider a system where agents receive factual updates containing data points or states of
the world. The agents apply their reporting strategy, performing ledger updates. Upon any ledger
update, the ledger distributes the latest aggregate parameter calculation using 𝜌, the computation
algorithm.

Formally, let [𝑛] = {1, . . . , 𝑛} be a set of 𝑛 agents. An update 𝑈 is of some type, depending on
the computational problem. An update with metadata ˆ𝑈 =< 𝑗, 𝑡, 𝑈 > complements an update 𝑈
with an agent 𝑗 ∈ N , and a type 𝑡 ∈ {𝐹𝑎𝑐𝑡𝑢𝑎𝑙, 𝐿𝑒𝑑𝑔𝑒𝑟 }, where “Factual” updates represent a factual
state of nature observed by an agent, and “Ledger” updates are what the agent shares with the
ledger, which may differ from what she factually observes. We note that the ledger (which for
simplicity we assume is a centralized third party) does not make the data public, but only shares the
algorithm’s updated outputs according to the protocol’s rules. The computation algorithm 𝜌 (Ut) is
an algorithm that receives a series of updates Ut = (𝑈1, ..., 𝑈𝑡 ) of any length 𝑡 and outputs a result.
In the continuous communication protocol, we have that algorithm outputs are shared with all
agents upon every ledger update.

In this section and Sections 3-4 we focus on the continuous communication protocol. The
continuous communication protocol simulates a system where agents may push updates at any
time, initiated by them and not by the system manager. We model this by allowing them to respond
to any change in the state of the system, including responding to their own ledger updates. The
only limit to an agent endlessly sending updates to the ledger is that we restrict it to update at most
ℓ times subsequently. The continuous communication protocol is a messaging protocol between
nature, the agents, and the ledger. A particular protocol run is instantiated with nature-input I,

4

Fig. 1. A continuous protocol run for I = (< 2, 90 >, < 2, 𝑦 >) with some 90 < 𝑦 < 𝑥 and the algorithm
𝜌 = max, as explained in the proof of Proposition 1. An agent’s observed history are all the nodes in her line,
or nodes that have an outgoing edge from a node in her line.

which is a series of some length |I| with each element being of the form < 𝑗, 𝑈 >, which is a tuple
comprised of agent 𝑗 ∈ N and an update 𝑈 .
Protocol 1: The continuous communication protocol
Input: Nature-input I, Parameter ℓ the maximum number of subsequent updates by an

agent

Output: Full Messaging History

1 for factual message < 𝑗, 𝑈𝑓 𝑎𝑐𝑡 > in I do
2

Nature sends a message to 𝑗 with < 𝑗, 𝐹𝑎𝑐𝑡𝑢𝑎𝑙, 𝑈𝑓 𝑎𝑐𝑡 >;
activeMessage ← True; // There is an active message
while activeMessage = True do

3

4

5

6

7

8

9

10

/* As long as some agent is responding */
activeMessage ← False;
for agent 𝑖 := 1 to 𝑛 do

if agent 𝑖 wishes to send a ledger update 𝑈𝑙𝑒𝑑𝑔 and last ℓ updates are not all of type
< 𝑖, 𝐿𝑒𝑑𝑔𝑒𝑟, 𝑈 >3 then

𝑖 sends a message to Ledger with < 𝑖, 𝐿𝑒𝑑𝑔𝑒𝑟, 𝑈𝑙𝑒𝑑𝑔 >;
Ledger sends a message to all with 𝜌’s algorithm output over all the past
ledger updates;
activeMessage ← True;

For the analysis, we extract some useful variables from the run of the protocol that will be used

in subsequent examples and proofs.

Let a run 𝑅 be all the messages sent in the system during the application of the continuous
communication protocol with nature-input I (where messages sent to ’all’ appear once, and the
messages appear in their order of sending).

Let 𝐿𝑗 (𝑅), 𝐹 𝑗 (𝑅) be the sub-sequences of all ledger, factual updates respectively in 𝑅 of agent 𝑗 (if
the index 𝑗 is omitted, then simply all such updates, regardless of an agent). Let 𝑂 𝑗 (𝑅) (“observed
history” of 𝑗) be all the messages in 𝑅 received or sent by 𝑗 during the run of the nature protocol:
These are all factual updates of 𝑗, ledger updates by 𝑗, and algorithm outputs shared by the ledger.
Let 𝑂 𝑗 (𝑅)𝑖1:𝑖2 be the the elements of 𝑂 𝑗 (𝑅) starting with index 𝑖1 and until (and including) index 𝑖2.

3We can perhaps question whether agent 𝑖 respects the condition that not all of the last ℓ updates are not all of type
< 𝑖, 𝐿𝑒𝑑𝑔𝑒𝑟, 𝑈 > for some 𝑈 . If she does not, she may send a message regardless of this constraint. But since nature can
choose not to accept/respond to it, we simplify the protocol by assuming the agents self-enforce the constraint.

An update strategy for 𝑗 is a mapping 𝑠 𝑗 from an observed history 𝑂 𝑗 (𝑅) to a ledger update 𝑈𝑙𝑒𝑑𝑔
by agent 𝑗. The truthful update strategy 𝑡𝑟𝑢𝑡ℎ 𝑗 is the following: If the last element in 𝑂 𝑗 (𝑅) is of
type < 𝑗, 𝐹𝑎𝑐𝑡𝑢𝑎𝑙, 𝑈 >, update with < 𝑗, 𝐿𝑒𝑑𝑔𝑒𝑟, 𝑈 >. Otherwise, do not update.

5

A full run of the protocol with nature input I and strategies 𝑠1, ..., 𝑠𝑛 is the run after completion
of the nature protocol where nature uses input I and each agent 𝑗 responds using strategy 𝑠 𝑗 .
Since we’re interested in the effect of one agent deviating from truthfulness, we say that we run
nature-input I with strategy 𝑠 𝑗 , where 𝑗 is the deviating agent, and it is assumed that all other
agents 𝑖 ≠ 𝑗 play 𝑡𝑟𝑢𝑡ℎ𝑖 . We denote the resulting run 𝑅I,𝑠 𝑗

We can now define an NCC-attack on the nature protocol given algorithm 𝜌 and updates

.

restriction ℓ.

Definition 1. An algorithm 𝜌 is ℓ − 𝑁𝐶𝐶 − 𝑣𝑢𝑙𝑛𝑒𝑟𝑎𝑏𝑙𝑒 if there exists an agent 𝑗 and update

strategy 𝑠 𝑗 such that:

i) There is a full run 𝑅I,𝑠 𝑗

of the protocol with some nature-input I and the strategy 𝑠 𝑗 such that its

.
last algorithm output is different from the last algorithm output in 𝑅I,𝑡𝑟𝑢𝑡ℎ 𝑗
ii) For any two nature-inputs I, I ′ such that the observed histories satisfy

𝑂 𝑗 (𝑅I,𝑡𝑟𝑢𝑡ℎ 𝑗 ) ≠ 𝑂 𝑗 (𝑅I′,𝑡𝑟𝑢𝑡ℎ 𝑗 ) =⇒ 𝑂 𝑗 (𝑅I,𝑠 𝑗 ) ≠ 𝑂 𝑗 (𝑅I′,𝑠 𝑗 ).

In words, to consider strategy 𝑠 𝑗 as a successful attack, the first condition requires that there is a
case where the rest of the agents other than 𝑗 observe something different than the factual truth.
Notice that we strictly require that the other agents (and not only the ledger) observe a different
outcome: If 𝑠 𝑗 updates with a ledger update that does not match its factual update, but this does
not affect future algorithm outputs, we do not consider it an attack (It is a “Tree that falls in a
forest unheard”). The second condition requires that the attacker is always able to infer (at least in
theory) the last true algorithm output. Under NCC utilities (which we omit formally defining, and
work instead directly with the logical formulation, similar to Definition 1 in [37]), failure to infer
the true algorithm output under strategy 𝑠 𝑗 makes it worse than 𝑡𝑟𝑢𝑡ℎ 𝑗 , no matter how much the
agent manages to mislead others (which is only its secondary goal).

We remark without formal discussion that being ℓ-NCC-vulnerable is enough to show that
truthfulness is not an ex-post Nash equilibrium if the agents were to play a non-cooperative game
using strategies 𝑠 𝑗 with NCC utilities. However, it does not suffice to show that truthfulness is not a
Bayesian-Nash equilibrium, as the cases where the deviation from truthfulness 𝑠 𝑗 satisfies condition
(𝑖) may be of measure 0. We give a stronger definition we call ℓ-NCC-vulnerable*, that would
guarantee the inexistence of the truthful Bayesian-Nash equilibrium for any possible probability
measure, by amending condition (𝑖) to hold for all cases:

Definition 2. An algorithm 𝜌 is ℓ − 𝑁𝐶𝐶 − 𝑣𝑢𝑙𝑛𝑒𝑟𝑎𝑏𝑙𝑒∗ if there exists an agent 𝑗 and update

strategy 𝑠 𝑗 with both condition (𝑖𝑖) of Definition 1, and:

i*) For every full run 𝑅I,𝑠 𝑗

of the protocol with some nature-input I, the last algorithm output is

different than the last algorithm output in 𝑅I,𝑡𝑟𝑢𝑡ℎ 𝑗

.

As long as there is at least one full run of the protocol, it is clear that being ℓ-NCC-vulnerable*
implies being ℓ-NCC-vulnerable. Similarly being ℓ-NCC-vulnerable(*) implies being (ℓ + 1)-NCC-
vulnerable(*) (i.e., the implication works for both the vulnerable and vulnerable* cases).

In Appendix B, we illustrate the difference between the two definitions, as well as simple proof

techniques, using a simple algorithm.

6

Fig. 2. A general template for the sneak attack. Until the special conditions are met, and after the re-sync is
done, the strategy behaves as 𝑡𝑟𝑢𝑡ℎ 𝑗 .

3 𝑘–CENTER AND 𝑘–MEDIAN IN THE CONTINUOUS COMMUNICATION

PROTOCOL

In this section, we analyze the performance of prominent clustering algorithms in terms of our
vulnerability(*) definitions. Together with Section 4 this demonstrates the applicability of the
approach for both unsupervised and supervised learning algorithms.

Definition 3. k-center: Each agent’s update 𝑈 is a set of data points, where each data point is of
the form 𝑥 ∈ R𝑑 . A possible output of the algorithm is some 𝑘 centers that are among the data points
𝑥1, . . . , 𝑥𝑘 ∈ (cid:208)𝑈 ∈Ut 𝑈 . Let 𝜂𝑖 = {𝑥 | arg min𝑘
𝑈 ∈Ut 𝑈 for 1 ≤ 𝑖 ≤ 𝑘 and some 𝐿𝑝
norm function ||v||𝑝 = 𝑝√︃
𝑖=1 𝑣𝑝
(cid:205)𝑑
𝑖 with 𝑝 ≥ 1. In words, 𝜂𝑖 is the set of all agents that have 𝑥𝑖 as their
closest point among 𝑥1, . . . , 𝑥𝑘 . Let 𝐶 (𝑥1, . . . , 𝑥𝑘 ) = max𝑘
𝑖=1 max𝑥 ∈𝜂𝑖 ||𝑥 − 𝑥𝑖 || be the cost function. In
words, the cost of a possible algorithm output 𝑥1, . . . , 𝑥𝑘 is the maximum distance between a point and
a center it is attributed to. We then have

𝑗=1 ||𝑥 − 𝑥 𝑗 ||𝑝 = 𝑖}𝑥 ∈(cid:208)

𝜌𝑘−𝑐𝑒𝑛𝑡𝑒𝑟 (Ut) = arg

min
𝑥1,...,𝑥𝑘 ∈(cid:208)

𝑈 ∈Ut 𝑈

𝐶 (𝑥1, . . . , 𝑥𝑘 ),

(1)

i.e., the 𝑘 centers are the 𝑘 points among the reported points that minimize the cost if chosen as
centers. Ties (both when determining 𝜂𝑖 and the final 𝑘 centers) are broken in favor of the candidate
with the smallest norm4.

3.1 Sneak Attacks and Vulnerability
In this subsection, we present a template for a class of attacks. We then show it is successful in
showing the vulnerability of the protocol for 𝑘-center.

4If this is not enough to determine, complement it with some arbitrary rule, e.g. over the radian coordinates of the points:
This does not matter for the argument.

7

Strategy Template 1: A template for a sneak attack
Input: Observed history 𝑂 𝑗 . Parameters 𝑈𝑐𝑜𝑛𝑑, 𝜌𝑐𝑜𝑛𝑑, 𝑈𝑎𝑡𝑡𝑎𝑐𝑘, 𝑈𝑟𝑒−𝑠𝑦𝑛𝑐
Output: A ledger update < 𝑗, 𝐿𝑒𝑑𝑔𝑒𝑟, 𝑈 >
/* Condition to start attack */

1 if The last element in 𝑂 𝑗 is < 𝑗, 𝐹𝑎𝑐𝑡𝑢𝑎𝑙, 𝑈𝑐𝑜𝑛𝑑 >, the last algorithm output in 𝑂 𝑗 is 𝜌𝑐𝑜𝑛𝑑 , and

the condition to start attack was not invoked before then

Return < 𝑗, 𝐿𝑒𝑑𝑔𝑒𝑟, 𝑈𝑎𝑡𝑡𝑎𝑐𝑘 >
/* Condition to end attack */

2

3 else if The condition to start attack was invoked, after that some agent (either 𝑗 or another)

received a factual update, but the condition to end attack was not yet invoked then
Let 𝑈 be the last update in 𝑂 𝑗 if it is a factual update for 𝑗, or ∅ otherwise.
Return < 𝑗, 𝐿𝑒𝑑𝑔𝑒𝑟, 𝑈 ∪ 𝑈𝑟𝑒−𝑠𝑦𝑛𝑐 >

4

5

/* If the special conditions do not hold, act as 𝑡𝑟𝑢𝑡ℎ 𝑗 */

6 else if Last update 𝑈 in 𝑂 𝑗 is factual for 𝑗 then
7

Return < 𝑗, 𝐿𝑒𝑑𝑔𝑒𝑟, 𝑈 >

Notice that when we defined strategies, we required them to be memory-less, i.e., only observe
𝑂 𝑗 and not their own past behavior (which by itself anyway only depends on the past observed
histories, which are contained in 𝑂 𝑗 ). However, the conditions in Strategy Template 1 require for
example to check whether the attack was initiated before. The technical lemma below shows that
this is possible to infer from 𝑂 𝑗 .
Lemma 1. If 𝑈𝑐𝑜𝑛𝑑 ≠ 𝑈𝑎𝑡𝑡𝑎𝑐𝑘 , the sneak attack is well defined, i.e., the conditions to start and end
attack can be implemented using only 𝑂 𝑗 .

We defer the proof details to Appendix C.
Strategy Template 1 presents the general sneak attack form, which requires four parameters:
𝑈𝑐𝑜𝑛𝑑 , 𝜌𝑐𝑜𝑛𝑑 , the factual update and last algorithm output that serve as a signal for the attacker to
send 𝑈𝑎𝑡𝑡𝑎𝑐𝑘 - the deviation from truth performs, and 𝑈𝑟𝑒−𝑠𝑦𝑛𝑐 , the update returning the ledger to a
synced state.

Two properties are important for a successful sneak attack. First, the attacker must know with
certainty the algorithm output given the counter-factual that it would have sent 𝑈𝑐𝑜𝑛𝑑 (as 𝑡𝑟𝑢𝑡ℎ 𝑗
would have), rather than 𝑈𝑎𝑡𝑡𝑎𝑐𝑘 . Second, after sending both 𝑈𝑎𝑡𝑡𝑎𝑐𝑘 and 𝑈𝑟𝑒−𝑠𝑦𝑛𝑐 , it should hold
that all future algorithm outputs are the same as if sending only 𝑈𝑐𝑜𝑛𝑑 . For example, if updates
are sets of data points and the algorithm outputs some calculation over their union (later formally
defined in Definition 4 as a set algorithm), this holds if 𝑈𝑐𝑜𝑛𝑑 = 𝑈𝑎𝑡𝑡𝑎𝑐𝑘 ∪ 𝑈𝑟𝑒−𝑠𝑦𝑛𝑐 .

We formalize this intuition in the following lemma:

Lemma 2. A sneak attack where 𝑈𝑎𝑡𝑡𝑎𝑐𝑘 ⊆ 𝑈𝑐𝑜𝑛𝑑, 𝑈𝑟𝑒−𝑠𝑦𝑛𝑐 = 𝑈𝑐𝑜𝑛𝑑 \ 𝑈𝑎𝑡𝑡𝑎𝑐𝑘 , and that moreover
can infer the last algorithm output in 𝑅I,𝑡𝑟𝑢𝑡ℎ 𝑗
after starting the attack and sending 𝑈𝑎𝑡𝑡𝑎𝑐𝑘 , satisfies
condition (𝑖𝑖).

The proof of the lemma is given in Appendix C.
We now give a sneak attack for 𝑘-center in R. The example can be extended to a general

dimension R𝑑 by setting the remaining coordinates in the attack parameters to 0.
Example 2. 𝑘-center with 𝑘 ≥ 3 is 1-NCC-vulnerable using a sneak attack: Use Strategy Template 1
with 𝑈𝑐𝑜𝑛𝑑 = {1, 2, 10, . . . , 10𝑘−1}, 𝑈𝑎𝑡𝑡𝑎𝑐𝑘 = {1}, 𝑈𝑟𝑒−𝑠𝑦𝑛𝑐 = 𝑈𝑐𝑜𝑛𝑑 \𝑈𝑎𝑡𝑡𝑎𝑐𝑘, 𝜌𝑐𝑜𝑛𝑑 = {−𝜖, 0,
with say 𝜖 = 1

𝜖
𝑘−2,

Condition (𝑖) is satisfied for nature-input I = (< 1, , 𝜌𝑐𝑜𝑛𝑑 >, < 2, 𝑈𝑐𝑜𝑛𝑑 >). The run with 𝑡𝑟𝑢𝑡ℎ2
𝑘−2 } ∪ {1}.

yields algorithm outputs 𝜌𝑐𝑜𝑛𝑑, {1, 10, . . . , 10𝑘−1} but the run with 𝑠2 yields 𝜌𝑐𝑜𝑛𝑑, 𝜌𝑐𝑜𝑛𝑑 \ { 𝜖

1000 .

𝜖
𝑘−3, . . . , 𝜖},

8

(a)

(b)

Fig. 3. An illustration of Example 2 with 𝑘 = 3. In (a), the fact that −𝜖, 0, 𝜖 is the algorithm output is enough
to show that all input elements are within [−2𝜖, 2𝜖], otherwise 𝑀 would be a better choice for a center. In (b),
which is displayed on a logarithmic scale, we see that given that all prior input elements are within [−2𝜖, 2𝜖],
and with additional elements 1, 2, 10, 100, the algorithm must output {1, 10, 100} as centers for a small enough
𝜖.

As for condition (𝑖𝑖): Let I be some nature-input, and let 𝑡 be the index of the element of I after
which the algorithm outputs 𝜌𝑐𝑜𝑛𝑑 (i.e., 𝑡 + 1 is < 2, 𝑈𝑐𝑜𝑛𝑑 >, upon where agent 2 starts the attack).
Let 𝑀+ = max𝑥 ∈(cid:208)
𝑈 ∈Ut 𝑈 . Assume for simplicity that |𝑀+| ≥ |𝑀−|, otherwise a
symmetric argument to the one we lay out follows. Given the algorithm output 𝜌𝑐𝑜𝑛𝑑 , we know that 𝜖
is the closest center to 𝑀+. Thus, 𝑀+ − 𝜖 ≤ 𝐶 (−𝜖, 0, 𝜖) ≤ 𝐶 (𝑀−, 0, 𝑀+) ≤ 𝑀+
2 . The last inequality is
due to that every point is either in [𝑀−, 0] or [0, 𝑀+], and so its distance from the closest center is at
most 1

2 . We thus have that 𝑀+ ≤ 2𝜖 (as illustrated in Figure 3).

2 max{|𝑀−|, |𝑀+|} = 𝑀+

𝑈 ∈Ut 𝑈 , 𝑀− = min𝑥 ∈(cid:208)

Therefore, under 𝑡𝑟𝑢𝑡ℎ2, after agent 2 sends 𝑈𝑐𝑜𝑛𝑑 = {1, 2, 10, . . . , 10𝑘−1}, we have 𝐶 ({1, 10, . . . , 10𝑘−1}) ≤

1+2𝜖. For any other choice of 𝑘 centers 𝑥1, . . . , 𝑥𝑘 (that may partially intersect), we have 𝐶 ({𝑥1, . . . , 𝑥𝑘 }) ≥
2 − 2𝜖 (as illustrated in Figure 3). Choosing 𝜖 < 1
4 we have that the algorithm output must be
{1, 10, . . . , 10𝑘−1}. This shows that agent 2 can infer with certainty the algorithm output under 𝑡𝑟𝑢𝑡ℎ2.
We thus satisfy the conditions of Lemma 2, which guarantees condition (𝑖𝑖) is satisfied.

3.2 𝑘–Center Vulnerability*
In the previous subsection, we have shown that 𝑘-Center is vulnerable. However, in this subsection,
we show it is not vulnerable*.

We note that a significant property of the 𝑘-center algorithm is that its output is a subset of its

input.

Definition 4. A set algorithm is an algorithm where each update is a set, and the algorithm is

defined over the union of all updates 𝑆 = (cid:208)𝑈 ∈Ut 𝑈 .

A multi-set algorithm is an algorithm where each update is a multi-set of data points, and the

algorithm is defined over the sum of all updates 𝑆 = (cid:210)𝑈 ∈Ut 𝑈 .

A set-choice algorithm is a set algorithm that satisfies 𝜌 (𝑆) ⊆ 𝑆, i.e., the algorithm output is a

subset of the input.

Many common algorithms such as max, min, or median, are set-choice algorithms, as well as

𝑘-center and 𝑘-median that we discuss.

We notice a property of the sneak attack in Example 2: 𝑈𝑎𝑡𝑡𝑎𝑐𝑘 deducts points that exist in the
factual update 𝑈𝑐𝑜𝑛𝑑 and does not include them in the ledger update. In fact, throughout the run
of 𝑠2 the union of ledger updates by agent 2 is a subset of the union of its factual updates. This
leads us to develop the following distinction. We partition the space of attack strategies (all attacks,
not necessarily just sneak attacks) into two types, explicitly-lying attacks and omission attacks.
This distinction has importance beyond the technical discussion, because of legal and regulatory

9

issues. Strategic firms may be willing to omit data (which can be excused as operational issues,
data cleaning, etc), but not to fabricate data.

Formally, for set and multi-set algorithms, we can partition all non-truthful strategies in the

following way:

Definition 5. An explicitly lying strategy 𝑠 𝑗 is a strategy that for some nature-input I has a point
𝑥 ∈ 𝐿𝑗 (𝑅I,𝑠 𝑗 ), 𝑥 ∉ 𝐹 𝑗 (𝑅I,𝑠 𝑗 ), i.e., the strategy sends a ledger update with a point that does not exist in
the union of all factual updates for that agent.

An omission strategy 𝑠 𝑗 is a a strategy that satisfies condition (𝑖) (i.e., misleads others) that is not

explicitly-lying.

For an omission strategy it must hold that for every run the agent past ledger updates are a subset

of its factual updates, i.e., 𝐿𝑗 (𝑅I,𝑠 𝑗 ) ⊆ 𝐹 𝑗 (𝑅I,𝑠 𝑗 ).

We now use the notion of explicitly-lying strategy to prove that 𝑘-center and 𝑘-median are not

vulnerable*. For this we need one more technical notion:

Definition 6. A set-choice algorithm has forceable winners if for any set 𝑆 and a point 𝑥 ∈ 𝑆,

there is a set ¯𝑆 with 𝑥 ∉ ¯𝑆 so that 𝑥 ∈ 𝜌 (𝑆 ∪ ¯𝑆).

In words, if the point 𝑥 is part of the algorithm input, it is always possible to send an update to force
the point 𝑥 to be an output of the algorithm. It is interesting to compare this requirement with axioms
of multi-winner social choice functions, as detailed e.g. in [12].

Theorem 1. A set-choice algorithm with forceable winners is not ℓ-NCC-vulnerable* for any ℓ.

We prove the theorem using the two following claims.

Claim 1. A strategy 𝑠 𝑗 that satisfies condition (𝑖∗) for a set-choice algorithm is explicitly-lying.

Proof. Consider a nature-input where agent 𝑗 receives no factual updates. To satisfy condition
(𝑖∗), it must send some ledger update for the algorithm output under 𝑠 𝑗 to differ from that under
𝑡𝑟𝑢𝑡ℎ 𝑗 . Since the union of all its factual updates is an empty set, it must hold that it sends a data
□
point that does not exist there.

Claim 2. An explicitly-lying strategy 𝑠 𝑗 for a set-choice algorithm with forceable winners violates
condition (𝑖𝑖).

Proof. Consider the shortest nature-input I (in terms of number of elements) where 𝑠 𝑗 sends
a ledger update with an explicit lie 𝑥, and let 𝐿𝑅 = 𝐿𝑗 (𝑅I,𝑠 𝑗 ), 𝐹𝑅 = 𝐹 𝑗 (𝑅I,𝑠 𝑗 ) be the union of all
ledger, factual updates respectively by 𝑗. Let 𝑆 = 𝐹𝑅 ∪ 𝐿𝑅, and < 𝑖, ¯𝑆 > the nature-input element
that generates a factual update of an agent 𝑖 ≠ 𝑗 that forces 𝑥 ∈ 𝜌 (𝑆 ∪ ¯𝑆) (such an element exist by
the forceable winners condition). Let 𝐸1 = 𝐹𝑅 ∪ 𝐿𝑅 ∪ ¯𝑆, 𝐸2 = 𝐸1 \ {𝑥 }. Notice that 𝑥 ∉ ¯𝑆 (as required
in Definition 6 of forceable winners), but 𝑥 ∈ 𝐿𝑅, and so 𝐸1 ≠ 𝐸2. Also note that 𝑥 ∉ 𝐹𝑅 (as it is an
explicit lie). Let I1, I2 be I with an additional last element 𝐸1, 𝐸2 respectively.

Now notice that 𝑂 𝑗 (𝑅I1,𝑠 𝑗 ), 𝑂 𝑗 (𝑅I2,𝑠 𝑗 ) are composed of the observed history 𝑂 𝑗 (𝑅I,𝑠 𝑗 ), together
with the observations following each of their different last elements. As the last element is a
factual update of an agent 𝑖 ≠ 𝑗, the agent sends a truthful ledger update. We then have 𝐿𝑅 ∪ 𝐸2 =
𝐿𝑅 ∪ ((𝐹𝑅 ∪ 𝐿𝑅 ∪ ¯𝑆) \ {𝑥 }) = (𝐿𝑅 ∪ {𝑥 }) ∪ ((𝐹𝑅 ∪ 𝐿𝑅 ∪ ¯𝑆) \ {𝑥 }) = 𝐿𝑅 ∪ (𝐹𝑅 ∪ 𝐿𝑅 ∪ ¯𝑆) = 𝐿𝑅 ∪ 𝐸1.
Thus, the immediate algorithm output, and any further algorithm output following some ledger
update by agent 𝑗 is taken over the same set, whether it is under I1 or I2, and so identifies. We
conclude that 𝑂 𝑗 (𝑅I1,𝑠 𝑗 ) = 𝑂 𝑗 (𝑅I2,𝑠 𝑗 ).

On the other hand, the last algorithm output in 𝑂 𝑗 (𝑅I1,𝑡𝑟𝑢𝑡ℎ 𝑗 ) is 𝜌 (𝐹𝑅∪𝐸1) = 𝜌 (𝐹𝑅∪(𝐹𝑅∪𝐿𝑅∪ ¯𝑆)) =
𝜌 (𝑆 ∪ ¯𝑆), and thus has the element 𝑥 by Definition 6. On the other hand, the last algorithm output

10

Fig. 4. Demonstration of the proof of Claim 2. 𝑥 is an explicit lie by agent 𝑗. 𝑆 is the state of the ledger
under 𝑡𝑟𝑢𝑡ℎ 𝑗 . 𝑆 ′ is the state of the ledger under 𝑠 𝑗 . ¯𝑆 is a complementary set to 𝑆 from Definition 6 (forceable
winners). Given that the next ledger update by a truthful agent is either ¯𝑆 or ¯𝑆 ∪ {𝑥 } (which is represented by
the rows), then the behavior under the different strategies (represented by the columns) is such that under 𝑠 𝑗 ,
the two underlying states of the world are the same, but not so under 𝑡𝑟𝑢𝑡ℎ 𝑗 .

in 𝑂 𝑗 (𝑅I2,𝑡𝑟𝑢𝑡ℎ 𝑗 ) is 𝜌 (𝐹𝑅 ∪ 𝐸2) = 𝜌 (𝐹𝑅 ∪ ((𝐹𝑅 ∪ 𝐿𝑅 ∪ ¯𝑆) \ {𝑥 })) = 𝜌 ((𝐹𝑅 ∪ 𝐿𝑅 ∪ ¯𝑆) \ {𝑥 }). Since 𝜌 is
□
a set-choice algorithm, it does not output 𝑥 since it does not appear in the input set.

Corollary 1. 𝑘-center is not ℓ-NCC-vulnerable* for any ℓ.

Proof. 𝑘-center is a set-choice algorithm. We show that it has forceable winners. We show
the construction for 𝑅, but the general 𝑅𝑑, 𝐿𝑝 is similar. Let some 𝑆 ⊆ 𝑅 with 𝑥 ∈ 𝑆. Let Δ =
max{max𝑠 ∈𝑆 |𝑥 − 𝑠 |, 1}. Let ¯𝑆 = {𝑥 + Δ, 𝑥 − Δ} ∪ {𝑥 + 10Δ, . . . , 𝑥 + 10𝑘−1Δ}. It must hold that
𝜌 (𝑆 ∪ ¯𝑆) = {𝑥, 𝑥 + 10Δ, . . . , 𝑥 + 10𝑘−1Δ}.
□

Corollary 2. 𝑘-median is not ℓ-NCC-vulnerable* for any ℓ.

The proof is given in Appendix D.

4 LINEAR REGRESSION UNDER CONTINUOUS COMMUNICATION
In this section, we study the vulnerability(*) of linear regression.

Definition 7. Multiple linear regression in 𝑑 features 𝑑 − 𝐿𝑅: Given a set of data points 𝑆 with 𝑛
points, where the data points features are a (𝑑 + 1) × 𝑛 matrix X with all elements of the first column
normalized to 1, the targets are a 1 × 𝑛 vector y, then

𝜌𝑑−𝐿𝑅 (Ut) =

𝜌𝑑−𝐿𝑅 (∪𝑡

𝑖=1𝑈𝑖 ) =

(cid:40)

(X𝑇 X)−1X𝑇 y X columns are linearly independent
𝑁𝑢𝑙𝑙

𝑂𝑡ℎ𝑒𝑟𝑤𝑖𝑠𝑒.

We slightly abuse notation by defining 𝜌𝑑−𝐿𝑅 both as a function on a series of updates Ut, as well as
on a set of data points. The latter satisfies, as long as the columns are linearly independent, 𝜌𝑑−𝐿𝑅 (𝑆) =
arg min𝛽 ∈R𝑑 (cid:205)𝑖 ∈ |𝑆 | (𝑦𝑖 − (cid:205)𝑑
𝑖 𝛽 𝑗 )2. We subsequently assume for simplicity that the columns are
always linearly independent (e.g., by having a first ledger update with 𝑑 linearly independent features.
The property is then automatically maintained with any future updates).

𝑗=1 𝑥 𝑗

It is not difficult to find omission sneak attacks for linear regression, as we demonstrate in
Figure 5. In Example 4 in Appendix E, we show a more complicated explicitly-lying sneak attack for
1 − 𝐿𝑅 (also called “simple linear regression”). The attack can be generalized for 𝑑 − 𝐿𝑅. This yields
Theorem 2. 𝑑-LR is 1-NCC-vulnerable.

11

Fig. 5. A sneak attack for simple linear regression. Since the points by others and the factual update of the
agent yield the same LR estimator ˆ𝜌, the result of running the regression on all points is ˆ𝜌 regardless of what
are the actual points by others.

Fig. 6. A general template for the triangulation attack, with 𝑘 = 3. Until the special conditions are met, and
after the re-sync is done, the strategy behaves as 𝑡𝑟𝑢𝑡ℎ 𝑗 .

4.1 Triangulation Attacks and Vulnerability*
To study vulnerability*, we now define a stronger type of attacks and show they exist for 𝑑 − 𝐿𝑅,
as long as ℓ ≥ 𝑑 + 2. We name this type of attacks triangulation attacks, and present a template
parameterized by functions 𝑓1, ..., 𝑓ℓ−1, ℎ in Strategy Template 2.
Strategy Template 2: A template for a triangulation attack
Input: Observed history 𝑂 𝑗 . Functions 𝑓1, . . . , 𝑓ℓ−1, ℎ
Output: A ledger update ˆ𝑈

1 Let 𝑖 = 1 if there is a factual update after the last ledger update by 𝑗.
2 Otherwise, if a triangulation attack is ongoing, let 2 ≤ 𝑖 ≤ ℓ be its current step or else exit.
3 Let 𝜌𝑖−1 be the last algorithm output in 𝑂 𝑗 .
4 if 1 ≤ 𝑖 ≤ ℓ − 1 then
5
6 else if 𝑖 = ℓ then
7

Return < 𝑗, 𝐿𝑒𝑑𝑔𝑒𝑟, 𝑓𝑖 (𝜌𝑖−1) >

Return < 𝑗, 𝐿𝑒𝑑𝑔𝑒𝑟, ℎ(𝜌ℓ−1) >

The idea of triangulation attacks is that for any state of the ledger, the attacker can find ℓ
subsequent updates so that it can both infer the algorithm output if it applied strategy 𝑡𝑟𝑢𝑡ℎ 𝑗 instead

12

of 𝑠 𝑗 (using 𝑓1, ..., 𝑓ℓ−1 the “triangulations”), and mislead others by the final update ℎ. Informally,
this attack has the desirable property that regardless of the state of the ledger (and how corrupted
it may be by previous updates of the attacker), the attacker can infer the true state.

As in the case of the sneak attack, we should show the strategy template can be implemented

using only the information in 𝑂 𝑗 .

Lemma 3. The triangulation attack is well defined, i.e., the conditions in lines 1 and 2 can be
implemented using only information available in 𝑂 𝑗 . The assignment in line 3 is valid, that is, given
that line 3 is executed there exists an algorithm output in 𝑂 𝑗 .

We defer the proof details to Appendix C.
We now prove there is a triangulation attack for 𝑑 − 𝐿𝑅 with ℓ ≥ 𝑑 + 2.

Theorem 3. 𝑑 − 𝐿𝑅 is (𝑑 + 2)-NCC-vulnerable* using a triangulation attack 𝑓1, ..., 𝑓𝑑+1, ℎ.

Proof. We shortly outline the overall flow of the proof. First, we give explicit construction of
the {𝑓𝑖 }1≤𝑖 ≤𝑑+1 functions. This suffices to show that condition (𝑖𝑖) is satisfied, which means there
is an inference function 𝑖 (𝑂 𝑗 ) that maps observed histories under 𝑠 𝑗 to the last algorithm output
under 𝑡𝑟𝑢𝑡ℎ 𝑗 . Given that inference function, we construct ℎ and show that with it condition (𝑖∗)
is satisfied. We give a formal treatment of inference function in Definition 10 and Lemma 5 of
Appendix B, but for our purpose in this proof it suffices that it is a map as specified.

Construction of {𝑓𝑖 }1≤𝑖 ≤𝑑+1 and condition (𝑖𝑖):
Let

be the last algorithm output before the application of 𝑓𝑖 . Define

𝜌𝑖−1 =

𝜌 1
𝑖−1
. . .
𝜌𝑑+1
𝑖−1















𝑓𝑖 (𝜌𝑖−1) = (𝑋𝑖, 𝑦𝑖 ),
where 𝑋𝑖 is the (𝑑 + 1) × 1 vector with 𝑋 1

𝑖 = 𝑋 𝑖

𝑖 = 1, and

𝑦𝑖 =

(cid:40)𝜌 1
𝑖−1 + 1
𝜌 1
𝑖−1 + 𝜌𝑖

𝑖−1 + 1

𝑖 = 1
2 ≤ 𝑖 ≤ 𝑑 + 1

.

Let 𝑅I,𝑠 𝑗

be a run with some nature-input I and 𝑠 𝑗 the triangulation attack with the specified
𝑓1, . . . , 𝑓𝑑+1 (and any function ℎ). Consider all the factual updates by agents ≠ 𝑗 induced by I. They
are each of the form of (𝑋 ′, 𝑦 ′), where 𝑋 ′ is of size 𝑛 × (𝑑 + 1) and 𝑦 ′ is 𝑛 × 1, and where 𝑛 is
the number of data points in the update. To consider all factual updates of the agents ≠ 𝑗, we
can vertically concatenate these matrices. Let this aggregate be denoted 𝑋𝐹,−𝑗, 𝑦𝐹,−𝑗 . Similarly, let
𝑋𝐹,𝑗, 𝑦𝐹,𝑗 be the concatenation of all factual updates by 𝑗 . Let the concatenation of all ledger updates
by 𝑗 before submission of any of the 𝑓𝑖 updates be 𝑋𝐿,𝑗, 𝑦𝐿,𝑗 . Recall that we denote by 𝜌0, . . . , 𝜌𝑑+1
the algorithm outputs (right before, and after each 𝑓𝑖 , e.g. 𝑓1 is applied after 𝜌0 and generates 𝜌1).
𝑖 be the (concatenated) inputs to the 𝑑 − 𝐿𝑅 algorithm that generate 𝜌𝑖 . In terms of the
Let 𝑋 ′
defined variables above, we can write:

𝑖 , 𝑦 ′

(𝑋 ′

𝑖 )𝑇 𝑋 ′

𝑖 = (𝑋𝐹,−𝑗 )𝑇 𝑋𝐹,−𝑗 + (𝑋𝐿,𝑗 )𝑇 𝑋𝐿,𝑗 +

𝑖
∑︁

(𝑋𝑖 )𝑇 𝑋𝑖,

(𝑋 ′

𝑖 )𝑇𝑦 ′

𝑖 = (𝑋𝐹,−𝑗 )𝑇𝑦𝐹,−𝑗 + (𝑋𝐿,𝑗 )𝑇𝑦𝐿,𝑗 +

𝑡 =1
𝑖
∑︁

(𝑋𝑖 )𝑇𝑦𝑖,

𝑡 =1

(2)

To show that condition (𝑖𝑖) holds, it suffices to show that we can infer the last algorithm output
. Let (𝑋𝐹 , 𝑦𝐹 ) be the concatenation of all factual updates of all agents,

𝜌𝑡𝑟𝑢𝑡ℎ of the run 𝑅I,𝑡𝑟𝑢𝑡ℎ 𝑗
then it is the input that generates 𝜌𝑡𝑟𝑢𝑡ℎ, and it holds that:

(𝑋𝐹 )𝑇 𝑋𝐹 = (𝑋𝐹,−𝑗 )𝑇 𝑋𝐹,−𝑗 + (𝑋𝐹,𝑗 )𝑇 𝑋𝐹,𝑗
(𝑋𝐹 )𝑇𝑦𝐹 = (𝑋𝐹,−𝑗 )𝑇𝑦𝐹,−𝑗 + (𝑋𝐹,𝑗 )𝑇𝑦𝐹,𝑗

(3)

13

Since in Equation 3, besides 𝑋𝐹,−𝑗, 𝑦𝐹,−𝑗 , all RHS variables are observed history under 𝑠 𝑗 , we
𝐹,−𝑗𝑦𝐹,−𝑗 in order to infer (𝑋𝐹 )𝑇 𝑋𝐹 , (𝑋𝐹 )𝑇𝑦𝐹 , and

conclude that it is enough to deduce 𝑋𝑇
thus also the last algorithm output under 𝑡𝑟𝑢𝑡ℎ 𝑗 which is ((𝑋𝐹 )𝑇 𝑋𝐹 )−1(𝑋𝐹 )𝑇𝑦𝐹 .

𝐹,−𝑗𝑋𝐹,−𝑗, 𝑋𝑇

Let (𝑋𝐹,−𝑗 )𝑇 𝑋𝐹,−𝑗

𝑑𝑒 𝑓
=

For every 0 ≤ 𝑖 ≤ 𝑑 + 1, we have

Σ1,1
. . .
Σ𝑑+1,1








. . .

Σ1,𝑑+1

. . . Σ𝑑+1,𝑑+1

, (𝑋𝐹,−𝑗 )𝑇𝑦𝐹,−𝑗 =








.

𝜎1


. . .


𝜎𝑑+1










𝑖 𝜌𝑖 = (𝑋 ′
(4)
By the construction of 𝑓𝑖 , we can rewrite these equations in the following way. Let 𝐷𝑖 be the
𝑖,𝑖 = 1, and all other elements zero. Let 𝑣𝑖 be the

1,1 = 𝐷𝑖

𝑖,1 = 𝐷𝑖

1,𝑖 = 𝐷𝑖

(𝑋 ′

𝑖 )𝑇𝑦 ′
𝑖 .

𝑖 )𝑇 𝑋 ′

(𝑑 + 1) × (𝑑 + 1) matrix with 𝐷𝑖
1 × (𝑑 + 1) vector with

1 = 𝑣𝑖
𝑣𝑖

𝑖 =

(cid:40)𝜌 1
0 + 1
𝜌 1
𝑖−1 + 𝜌𝑖

𝑖−1 + 1

𝑖 = 1
𝑖 > 1

,

and all other elements zero.
We have for 0 ≤ 𝑖 ≤ 𝑑 + 1:

. . .

Σ1,𝑑+1

𝑖
∑︁

+

𝐷𝑖 )𝜌𝑖 =

𝑖
∑︁

𝑣𝑖 .

+

(5)

𝑡 =1
If we examine the differences between the 𝑖 equation and the 𝑖 −1 equation, we get for 1 ≤ 𝑖 ≤ 𝑑 +1,

𝑡 =1

. . . Σ𝑑+1,𝑑+1

𝜎1


. . .


𝜎𝑑+1










. . .

Σ1,𝑑+1

𝑖−1
∑︁

+

𝐷𝑡 )(𝜌𝑖 − 𝜌𝑖−1) = 𝑣𝑖 − 𝐷𝑖 𝜌𝑖 .

(6)

𝑡 =1
Notice that for any 1 ≤ 𝑖 ≤ 𝑑 + 1, 𝑣𝑖 − 𝐷𝑖 𝜌𝑖 is not the zero vector. If it was, since (𝑋𝐹,−𝑗 )𝑇 𝑋𝐹,−𝑗 +
𝑡 =1 𝐷𝑡 is invertible, we will have that 𝜌𝑖 = 𝜌𝑖−1, which would contradict the following claim:

(cid:205)𝑖−1

. . . Σ𝑑+1,𝑑+1

Σ1,1
. . .
Σ𝑑+1,1

(








Σ1,1
. . .
Σ𝑑+1,1

(






















𝛼1


. . .


𝛼𝑑+1










Claim 3. For every algorithm output 𝜌 =

, and a single point update 𝑈 ∗ = (𝑋 ∗ = (cid:2)1

𝑥1

. . .

𝑥𝑑 (cid:3) , 𝑦∗)

so that 𝑋 ∗ · 𝜌 ≠ 𝑦∗, the new algorithm output 𝜌 ′ for the data with 𝑈 ∗ satisfies 𝜌 ′ ≠ 𝜌, and has a
different value at 𝑋 ∗ than 𝑋 ∗ · 𝜌.

𝑖−1 + 𝜌𝑖

𝑖−1 + 1 − 𝜌 1

The proof of the claim is given in Appendix E.
Moreover, 𝑣𝑖 − 𝐷𝑖 𝜌𝑖 by definition is a vector that has all elements 0 besides element 1 and 𝑖 that
are 𝜌 1
𝑖 ≠ 0 (since it is not a zero vector), and so the 𝑖-th element of the vector
is non-zero. Therefore, for the vector 𝑤𝑖 𝑑𝑒 𝑓
𝑡 =1 𝐷𝑡 (𝜌𝑡 − 𝜌𝑡 −1), the 𝑖-th element is
non-zero as well (Since (cid:205)𝑖−1
𝑡 =1 𝐷𝑡 𝜌𝑖 has all elements with index higher than 𝑖 − 1 as zero). For any 𝑤𝑡
with 𝑡 ≤ 𝑖 − 1, all elements with index higher than 𝑖 − 1 are zero. Therefore, the set {𝑤𝑖 }1≤𝑖 ≤𝑑+1 is
linearly independent, and the matrix 𝑊 where each column 𝑖 is 𝑤𝑖 is invertible. If we let 𝑀𝜌 be the

= 𝑣𝑖 − 𝐷𝑖 𝜌𝑖 − (cid:205)𝑖−1

𝑖 − 𝜌𝑖

14

Fig. 7. A script-run triangulation attack for 2-LR. The round red points represent an existing state of the ledger.
The yellow x points (in (1)) represent a new factual update for the strategic agent. The red line in (1) represents
the resulting linear regression estimator, if the agent reports truthfully. The four figures (2a)-(2d) show the
flow of our triangulation attack construction. In (2a) is the last state of the ledger before the triangulation,
with no triangulation point sent by the strategic agent. The rest of (2b)-(2d) consecutively add triangulation
points (blue triangles). At the end of the triangulation attack (after (2d)), the linear regression estimator is
different than in (1). It is possible to infer the estimator in (1) using knowledge of the triangulation points
and estimators of (2a)-(2d) (without knowledge of the red points).

matrix where each column 𝑖 is 𝜌𝑖 − 𝜌𝑖−1, we can rewrite Eq 6 as (𝑋𝐹,−𝑗 )𝑇 𝑋𝐹,−𝑗 𝑀𝜌𝑊 −1 = 𝐼 , where 𝐼 is
the (𝑑 + 1) × (𝑑 + 1) identity matrix. We conclude that 𝑀𝜌 is invertible and (𝑋𝐹,−𝑗 )𝑇 𝑋𝐹,−𝑗 = 𝑊 𝑀 −1
𝜌 .
We can directly calculate the RHS of this expression from the observed history under 𝑠 𝑗 , and by the
first equation of Eq 5 we can infer (𝑋𝐹,−𝑗 )𝑇𝑦𝐹,−𝑗 = (𝑋𝐹,−𝑗 )𝑇 𝑋𝐹,−𝑗 𝜌0, overall concluding the proof
for condition (𝑖𝑖).

Construction of ℎ and condition (i*). Let 𝑖 be the inference function (which existence is
guaranteed by the previous discussion) that matches observed histories running 𝑠 𝑗 with the true
algorithm outputs under 𝑡𝑟𝑢𝑡ℎ 𝑗 . I.e., we has 𝑖 (𝑂 𝑗 ) = 𝜌𝑡𝑟𝑢𝑡ℎ. Let the last algorithm output in 𝑂 𝑗 be
(cid:104)
𝜌 1
𝑙𝑎𝑠𝑡 + 1

. . .

(cid:40)

1

0

0

(cid:105)

(cid:104)

(cid:105)

)

(

,

𝜌𝑙𝑎𝑠𝑡 =

. Let ℎ(𝑂 𝑗 ) =

𝑖 (𝑂 𝑗 ) = 𝜌𝑙𝑎𝑠𝑡
𝑂𝑡ℎ𝑒𝑟𝑤𝑖𝑠𝑒.

.

No update

If 𝜌𝑙𝑎𝑠𝑡 ≠ 𝜌𝑡𝑟𝑢𝑡ℎ, ℎ does not send an update, and so for the nature-input that has observed history
𝑂 𝑗 the last algorithm output under 𝑠 𝑗 is different than that under 𝑡𝑟𝑢𝑡ℎ 𝑗 , as required by condition
(𝑖∗).

If 𝜌𝑙𝑎𝑠𝑡 = 𝜌𝑡𝑟𝑢𝑡ℎ, ℎ sends an update with a point (𝑋, 𝑦) that satisfies 𝑋 · 𝜌𝑙𝑎𝑠𝑡 = 𝜌 1

𝑙𝑎𝑠𝑡 ≠ 𝜌 1

𝑙𝑎𝑠𝑡 +1 = 𝑦.

By Claim 3, the resulting algorithm output is different from 𝑖 (𝑂 𝑗 ).

We demonstrate the construction and inference of the triangulation attack in an open-source
implementation https://github.com/yotam-gafni/triangulation_attack. Figure 7 shows a run of the
attack for a random example for 2-LR.

We show an asymptotically matching lower bound for triangulation attacks.

□

𝜌 1
𝑙𝑎𝑠𝑡
. . .
𝜌𝑑+1
𝑙𝑎𝑠𝑡















15

Theorem 4. There is no triangulation attack for 𝑑 − 𝐿𝑅 with 𝑑 − 2 or less functions (i.e., ℓ ≤ 𝑑 − 2).
Proof. Consider all nature-input elements that are of the form < 𝑖, (𝑋, 𝑦) >, < 𝑗, ( ¯𝑋 𝑗, ¯𝑦 𝑗 ) >,
where 𝑋 is a (𝑑 + 1) × (𝑑 + 1) matrix, and 𝑦 is the (𝑑 + 1) × 1 zero vector. ( ¯𝑋 𝑗, ¯𝑦 𝑗 ) of the same
sizes but without any restriction over ¯𝑦 𝑗 . We show that for any triangulation attack 𝑠 𝑗 , we can find
two nature-inputs among this family with different observed history under 𝑡𝑟𝑢𝑡ℎ 𝑗 , but the same
observed history under 𝑠 𝑗 .

By the choice of 𝑦, the first algorithm output satisfies 𝜌0 = (𝑋𝑇 𝑋 )−1𝑋𝑇𝑦 = 0. As we know
from the proof of Theorem 3, in particular Equation 5 (where it was done for a specific given
triangulation attack), that the attack generates 𝑑 − 1 vector equations for 𝑋𝑇 𝑋 (including the one
over 𝜌0). We also know that the first row of 𝑋𝑇 is all 1 elements. We can make it a stricter constraint
by demanding that the first row of 𝑋𝑇 𝑋 is of the form (cid:2)𝑑 + 1
0(cid:3). Then, the principal
sub-matrix of 𝑋𝑇 𝑋 (removing the first row and column) is a general PSD matrix (as a principal
submatrix of the 𝑋𝑇 𝑋 PSD matrix). To uniquely determine such a matrix of size 𝑑 × 𝑑, we need
𝑑 vector equations, but the triangulation equations only yield 𝑑 − 1 such equations. So there are
some 𝑋1 ≠ 𝑋2 that are in the family of nature-inputs and have the same observed history under 𝑠 𝑗 .
Fix some invertible ¯𝑋 𝑗 . Since (𝑋𝑇

¯𝑋 𝑗 ), there must be some 𝑣 so that

¯𝑋 𝑗 ) ≠ (𝑋𝑇

. . .

0

2 𝑋2 + ¯𝑋𝑇

1 𝑋1 + ¯𝑋𝑇

𝑗

𝑗

(𝑋𝑇

2 𝑋2 + ¯𝑋𝑇

𝑗

¯𝑋 𝑗 )−1𝑣 ≠ (𝑋𝑇

1 𝑋1 + ¯𝑋𝑇

𝑗

¯𝑋 𝑗 )−1𝑣.

If ¯𝑋 𝑗 ¯𝑦 𝑗 = 𝑣, then the last algorithm outputs under 𝑡𝑟𝑢𝑡ℎ 𝑗 are different for 𝑋1, 𝑋2, which holds
□

choosing ¯𝑦 𝑗 = ( ¯𝑋 𝑗 )−1𝑣.

5 THE PERIODIC COMMUNICATION PROTOCOL
The periodic communication protocol simulates a system where update rounds are initiated by
the system manager (or ledger), and not by the agents themselves. After each round, the ledger
shares the algorithm output with all agents. The definitions of section 2 remain consistent with
this periodic setting, with the following minor changes:

• Since all updates by different agents in a certain round are aggregated together, the distinction

of ℓ subsequent updates becomes irrelevant and we omit it.

• An identifier of the round number 𝑟 is added to each nature-input element. That is, each

element is < 𝑗, 𝑈 , 𝑟 >, with an agent 𝑗 ∈ N , an update 𝑈 , and a round number 𝑟 .5

Protocol 2: The periodic communication protocol
Input: Nature-input I
Output: Full Messaging History

1 Let 𝑟𝑚𝑎𝑥 = max< 𝑗,𝑈 ,𝑟 > ∈I 𝑟 .

/* For each round of updates */

2 for ¯𝑟 := 1 to 𝑟𝑚𝑎𝑥 do

/* For each update in round ¯𝑟 */
for Element < 𝑗, 𝑈𝑓 𝑎𝑐𝑡, 𝑟 > with 𝑟 = ¯𝑟 do

3

4

5

6

7

8

Nature sends a message to 𝑗 with < 𝑗, 𝐹𝑎𝑐𝑡𝑢𝑎𝑙, 𝑈𝑓 𝑎𝑐𝑡 >;

for agent 𝑖 := 1 to 𝑛 do

if agent 𝑖 wishes to send a ledger update 𝑈𝑙𝑒𝑑𝑔 then

𝑖 sends a message to Ledger with < 𝑖, 𝐿𝑒𝑑𝑔𝑒𝑟, 𝑈𝑙𝑒𝑑𝑔 >;

Ledger sends a message to all with 𝜌’s algorithm output over all the past ledger updates;

5Round numbers are assumed to have natural properties: They are monotonically increasing with later elements of the
nature-input series, each agent has at most one nature-input element assigned to it per round. The first round is 𝑟 = 1.

16

Fig. 8. A periodic protocol run for I = (< 1, 90, 1 >, < 2, 90, 1 >). Both agents are truthful.

We now show that indeed periodic communication is strictly less vulnerable to attacks, both for

𝑘-center and 𝑑 − 𝐿𝑅.
Theorem 5. 𝑑 − 𝐿𝑅 is not NCC-vulnerable in the periodic communication protocol.

We prove this theorem using a more general lemma. We first define three useful properties of a

minimization task:

Definition 8. A multi-set minimization problem 𝐶 is of the form 𝜌 (𝑆) = arg min𝜌′ 𝐶 (𝑆, 𝜌 ′),

where 𝑆 is the algorithm input, 𝐶 is a cost function and 𝜌 ′ is some possible algorithm output.

A minimization problem 𝐶 is separable if 𝐶 (𝑆1⊎𝑆2, 𝜌) = 𝐶 (𝑆1, 𝜌)+𝐶 (𝑆2, 𝜌). Separable minimization

problems are also homogeneous in the sense that: 𝐶 (𝑆 × 𝜆, 𝜌) = 𝜆𝐶 (𝑆, 𝜌).

A minimization problem has a unique solution if for every input 𝑆 it has a single algorithm output

𝜌 ′ that attains the optimal goal.

A minimization problem is non-negative if for every input 𝑆 and possible algorithm output 𝜌 ′,

𝐶 (𝑆, 𝜌 ′) ≥ 0.

We know that 𝑑−𝐿𝑅 under the restriction mentioned (independent columns) has a unique solution.
It is also immediate from its definition as an optimization problem that it satisfies separability.
Theorem 5 now follows on the following general lemma:

Lemma 4. Any multi-set algorithm 𝜌 that can be formalized as a minimization problem with
separable, non-negative minimization goal 𝐶 with a unique solution is not NCC-vulnerable in the
periodic communication protocol.

Proof. Assume the algorithm is NCC-vulnerable in periodic communication with some strategy
𝑠 𝑗 . By condition (𝑖), there is nature input I so that the last algorithm output under 𝑡𝑟𝑢𝑡ℎ 𝑗 is 𝜌
and under 𝑠 𝑗 is 𝜌 ′. Let 𝑆, 𝑆 ′ be some underlying input to generate 𝜌, 𝜌 ′ respectively. Since 𝜌, 𝜌 ′
are unique solutions, it must hold that 0 ≤ 𝐶 (𝑆 ′, 𝜌 ′) < 𝐶 (𝑆 ′, 𝜌), 0 ≤ 𝐶 (𝑆, 𝜌) < 𝐶 (𝑆, 𝜌 ′). Let
Δ = 𝐶 (𝑆, 𝜌 ′) − 𝐶 (𝑆, 𝜌), 𝛿 = 𝐶 (𝑆 ′, 𝜌) − 𝐶 (𝑆 ′, 𝜌 ′), 𝜆 = ⌈ Δ
𝛿 ⌉ + 1. Now assume that some agent ≠ 𝑗 sends
𝑆 ′ × 𝜆 in the last round of I (call this extension I ′. If all agents already send an update in this round,
add 𝑆 ′ × 𝜆 to one of these agents’ update). Under 𝑠 𝑗 , we have that 𝐶 (𝑆 ′ + (𝑆 ′ × 𝜆), ˆ𝜌) = (𝜆 + 1)𝐶 (𝑆 ′, ˆ𝜌)
and so 𝜌 remains the unique solution (The argmin does not change under multiplication of the cost
function). Under 𝑡𝑟𝑢𝑡ℎ 𝑗 , we have

𝐶 (𝑆 + (𝑆 ′ × 𝜆), 𝜌 ′) = 𝐶 (𝑆, 𝜌 ′) + 𝜆𝐶 (𝑆 ′, 𝜌 ′) = 𝐶 (𝑆, 𝜌) + Δ + 𝜆(𝐶 (𝑆 ′, 𝜌) − 𝛿) =
𝐶 (𝑆 ∪ (𝑆 ′ × 𝜆), 𝜌) + Δ − 𝜆𝛿 < 𝐶 (𝑆 ∪ (𝑆 ′ × 𝜆), 𝜌),

17

Fig. 9. Demonstration of the proof of Lemma 4. Under 𝑠 𝑗 , the estimator for 1-LR is the same whether the
other agent additionally submits ∅ or 3 × 𝑆 ′, but not so under 𝑡𝑟𝑢𝑡ℎ 𝑗 .

and so 𝜌 is not the optimal algorithm output.

We thus have a violation of condition (𝑖𝑖): There are two nature inputs (I, I ′) with the same
□

observed history under 𝑠 𝑗 but different under 𝑡𝑟𝑢𝑡ℎ 𝑗 .

In the appendix, we prove a similar result for 𝑘-center. The result also holds for 𝑘-median and is

done by extending the construction of Corollary 2.

Theorem 6. 𝑘-center is not vulnerable under the periodic communication protocol.

6 DISCUSSION
In this work, we lay the groundwork for the study of exclusivity attacks in long-term data sharing.
We present two protocols for long-term communication and show that the choice of protocol, as
well as the number of Sybil identities an attacker may control, matters for the safety of the system.
We do so by analyzing two representative and popular algorithms of supervised and unsupervised
learning, namely linear regression and k-center. We show that the distinction between omission
and explicitly-lying attacks has theoretical significance, and present two general attack templates
that are useful to consider against any possible algorithm. However, we believe that these are the
first steps and that there is much more to study regarding systems’ safety from exclusivity attacks.
We now expand on a few possible future directions.

6.1 Further Model Extensions
6.1.1 Varying Temporal Resilience. In our model, condition (𝑖𝑖) requires one pair of confounding
nature-inputs, i.e., one state of the world where the agent can not infer the true best model fit.
However, when dealing with collaborative computing, some organizations may have different
“temporal resilience”. While some depend daily on the learned parameters, others operate in longer

18

time scales such as issuing weekly or monthly reports. In such cases, an attacker 𝑗 may be willing
to incur being confounded, as long as the confusion is bounded within a small number of algorithm
outputs, after which it can again infer the true parameters. Adjusting the model to accommodate
such heterogeneous preferences and how they affect the results can be interesting.

6.1.2 Horizontal vs. Vertical Data Split. In multi-agent collaborative learning tasks, a common
distinction is between “Horizontal” and “Vertical” data split [39]. A horizontal split is when the set
of features is shared among agents, but the data points may differ. Vertical split is when the data
points are related to the same users, but the feature space is different among agents. While our
model is general and can accommodate both cases, our results largely deal with the horizontal case,
and it would be interesting to look into the vertical case as well.

6.1.3 Application to Silo-ed Federated Learning. A leading motivation for developing the theory in
this work is to apply it to federated learning, in particular in the context where the contributors are
a few large firms (referred to as Silo-ed federated learning in [21]). As we know from the case of the
Average algorithm [22], changing the amount of information shared with the agents can determine
the safety of the collaboration (In the Average case, whether the denominator of the number of
samples is shared alongside the average itself). Applied in the context of federated learning, design
choices such as split learning [15], keeping hyper-parameters at the aggregator level and not the
client level (Notice that this is in contrast with the design of the popular FederatedAveraging
algorithm [27]!), or varying the accuracy of the model supplied to agents [26], can be promising
ideas to deter NCC attacks. Another issue that needs to be addressed is that of learning being
resistant to permutations over the order of samples [34]. In the set and multi-set algorithms we
treat in this work, the order of the updates does not matter for the algorithm output, and so it is
possible to strategically control how and when to share factual data, for example in sneak attacks.
However, in training neural nets, the order of feeding samples can change the final model (See the
discussion in 1.4.2 in [28]).

6.1.4 Relaxing the NCC Requirements and Approximate Mechanisms. The requirement from exclu-
sivity attacks to be able to infer the exact true algorithm output seems harsh. This is especially true
when dealing with statistical estimators, that by their nature are prone to noise. So, it is interesting
to see how do the positive results of our work (in the sense of no-vulnerability of an algorithm
under some settings) hold when attackers are willing to suffer some 𝜖 degradation of the algorithm
output in comparison with the true result (under some appropriate metric). Such a discussion also
opens the gate to a mechanism design problem. Once agents are willing to suffer some degradation
of the model, it is possible to consider approximate algorithms that have better incentive-compatible
properties than the standard algorithm. However, simply adding noise to an algorithm does not
guarantee that it is safer. For example, consider that we take the one-shot sum algorithm and add
some 0-mean noise with expected variance 𝜖. Under 𝑡𝑟𝑢𝑡ℎ 𝑗 , agent 𝑗 will have a difference of 𝜖 from
the true sum in expectation. If agent 𝑗 attacks by adding 𝛿 to its true number in the ledger update,
and then reduces 𝛿 from the algorithm output, its expected deviation from the true sum remains
𝜖, but it is able (by choosing 𝛿 right) to mislead others on average by more than 𝜖. Therefore, we
remark that a good approximate algorithm to deter attacks should somehow guarantee that the
attack process amplifies the error to hurt the attacker.

Another interesting option that is possible once dealing with a relaxation of NCC is to have
different algorithm outputs sent to different agents, i.e., the protocol does not share a global
algorithm output each time with all agents, but gives a different response to each, hopefully in a
way that helps enforce incentive compatibility.

19

ACKNOWLEDGEMENTS
Yotam Gafni and Moshe Tennenholtz were supported by the European Research Council (ERC)
under the European Union’s Horizon 2020 research and innovation programme (Grant No. 740435).

REFERENCES
[1] Yehuda Afek, Shaked Rafaeli, and Moshe Sulamy. 2017. Cheating by duplication: Equilibrium requires global knowledge.

(2017). arXiv:1711.04728

[2] Michele Banko and Eric Brill. 2001. Scaling to Very Very Large Corpora for Natural Language Disambiguation.
In Proceedings of the 39th Annual Meeting on Association for Computational Linguistics (ACL ’01). Association for
Computational Linguistics, USA, 26–33. https://doi.org/10.3115/1073012.1073017

[3] Omer Ben-Porat and Moshe Tennenholtz. 2019. Regression Equilibrium. In Proceedings of the 2019 ACM Conference on
Economics and Computation (EC ’19). Association for Computing Machinery, New York, NY, USA, 173–191. https:
//doi.org/10.1145/3328526.3329560

[4] Arnaud Braud, Gaël Fromentoux, Benoit Radier, and Olivier Le Grand. 2021. The Road to European Digital Sovereignty

with Gaia-X and IDSA. IEEE Network 35, 2 (2021), 4–5. https://doi.org/10.1109/MNET.2021.9387709

[5] Yang Cai, Constantinos Daskalakis, and Christos Papadimitriou. 2015. Optimum Statistical Estimation with Strategic
Data Sources. In Proceedings of The 28th Conference on Learning Theory (July 3-6) (COLT ’15). PMLR, 280–296. https:
//proceedings.mlr.press/v40/Cai15.html

[6] Hau Chan, Aris Filos-Ratsikas, Bo Li, Minming Li, and Chenhao Wang. 2021. Mechanism Design for Facility Location
Problems: A Survey. In Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence (IJCAI ’21).
AAAI, 4356–4365. https://doi.org/10.24963/ijcai.2021/596

[7] Yiling Chen, Yang Liu, and Chara Podimata. 2020. Learning Strategy-Aware Linear Classifiers. In Advances in
Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020 (De-
cember 6-12) (NeurIPS ’20). Curran Associates, Inc., 15265–15276. https://proceedings.neurips.cc/paper/2020/hash/
ae87a54e183c075c494c4d397d126a66-Abstract.html

[8] European Commission, Content Directorate-General for Communications Networks, Technology, E Scaria, A Bergh-
mans, M Pont, C Arnaut, and S Leconte. 2018. Study on data sharing between companies in Europe : final report.
Publications Office. https://doi.org/10.2759/354943

[9] Ronald Cramer, Ivan Bjerre Damgård, and Jesper Buus Nielsen. 2015. Secure Multiparty Computation and Secret Sharing.

Cambridge University Press. https://doi.org/10.1017/CBO9781107337756

[10] Ofer Dekel, Felix Fischer, and Ariel D Procaccia. 2010. Incentive compatible regression learning. J. Comput. System Sci.

76, 8 (2010), 759–777.

[11] Cynthia Dwork. 2008. Differential Privacy: A Survey of Results. In Theory and Applications of Models of Computation,
Manindra Agrawal, Dingzhu Du, Zhenhua Duan, and Angsheng Li (Eds.). Springer Berlin Heidelberg, Berlin, Heidelberg,
1–19.

[12] Edith Elkind, Piotr Faliszewski, Piotr Skowron, and Arkadii Slinko. 2017. Properties of multiwinner voting rules. Social

Choice and Welfare 48, 3 (2017), 599–632.

[13] Yotam Gafni, Ron Lavi, and Moshe Tennenholtz. 2020. VCG under Sybil (False-Name) Attacks - A Bayesian Analysis.
In Proceedings of the 34th AAAI Conference on Artificial Intelligence (February 7-12) (AAAI ’20). AAAI, 1966–1973.
https://doi.org/10.1609/aaai.v34i02.5567

[14] Nicolas Gast, Stratis Ioannidis, Patrick Loiseau, and Benjamin Roussillon. 2020. Linear Regression from Strategic
Data Sources. ACM Transactions on Economics and Computation (TEAC) 8, 2, Article 10 (5 2020), 24 pages. https:
//doi.org/10.1145/3391436

[15] Otkrist Gupta and Ramesh Raskar. 2018. Distributed learning of deep neural network over multiple agents. Journal of

Network and Computer Applications 116 (2018), 1–8. https://doi.org/10.1016/j.jnca.2018.05.003

[16] S Louis Hakimi. 1964. Optimum locations of switching centers and the absolute centers and medians of a graph.

Operations research 12, 3 (1964), 450–459.

[17] Alon Halevy, Peter Norvig, and Fernando Pereira. 2009. The unreasonable effectiveness of data. IEEE Intelligent Systems

24, 2 (2009), 8–12.

[18] Justin D. Harris and Bo Waggoner. 2019. Decentralized and Collaborative AI on Blockchain. In Proceedings of
the Second IEEE International Conference on Blockchain (IEEE-Blockchain 2019). IEEE Computer Society, 368–375.
https://doi.org/10.1109/Blockchain.2019.00057

[19] Dorit S Hochbaum and David B Shmoys. 1985. A best possible heuristic for the k-center problem. Mathematics of

operations research 10, 2 (1985), 180–184.

[20] Nicole Immorlica, Adam Tauman Kalai, Brendan Lucier, Ankur Moitra, Andrew Postlewaite, and Moshe Tennenholtz.
2011. Dueling algorithms. In Proceedings of the 43rd ACM Symposium on Theory of Computing (June 6-8) (STOC ’11).

20

ACM, 215–224. https://doi.org/10.1145/1993636.1993666

[21] Peter Kairouz, H. Brendan McMahan, Brendan Avent, Aurélien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji, Kallista
Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, Rafael G. L. D’Oliveira, Hubert Eichner, Salim El
Rouayheb, David Evans, Josh Gardner, Zachary Garrett, Adrià Gascón, Badih Ghazi, Phillip B. Gibbons, Marco Gruteser,
Zaid Harchaoui, Chaoyang He, Lie He, Zhouyuan Huo, Ben Hutchinson, Justin Hsu, Martin Jaggi, Tara Javidi, Gauri
Joshi, Mikhail Khodak, Jakub Konecný, Aleksandra Korolova, Farinaz Koushanfar, Sanmi Koyejo, Tancrède Lepoint,
Yang Liu, Prateek Mittal, Mehryar Mohri, Richard Nock, Ayfer Özgür, Rasmus Pagh, Hang Qi, Daniel Ramage, Ramesh
Raskar, Mariana Raykova, Dawn Song, Weikang Song, Sebastian U. Stich, Ziteng Sun, Ananda Theertha Suresh, Florian
Tramèr, Praneeth Vepakomma, Jianyu Wang, Li Xiong, Zheng Xu, Qiang Yang, Felix X. Yu, Han Yu, and Sen Zhao.
2021. Advances and Open Problems in Federated Learning. Foundations and Trends® in Machine Learning 14, 1–2
(2021), 1–210. https://doi.org/10.1561/2200000083

[22] Murat Kantarcioglu and Wei Jiang. 2013. Incentive Compatible Privacy-Preserving Data Analysis. IEEE Transactions

on Knowledge and Data Engineering 25, 6 (2013), 1323–1335. https://doi.org/10.1109/TKDE.2012.61

[23] Ming Li, Jian Weng, Anjia Yang, Wei Lu, Yue Zhang, Lin Hou, Jia-Nan Liu, Yang Xiang, and Robert H. Deng. 2019.
IEEE Transactions on Parallel and

CrowdBC: A Blockchain-Based Decentralized Framework for Crowdsourcing.
Distributed Systems 30, 6 (2019), 1251–1266. https://doi.org/10.1109/TPDS.2018.2881735

[24] Manlu Liu, Kean Wu, and Jennifer Jie Xu. 2019. How will blockchain technology impact auditing and accounting:

Permissionless versus permissioned blockchain. Current Issues in Auditing 13, 2 (2019), A19–A29.

[25] Yuan Lu, Qiang Tang, and Guiling Wang. 2018. On Enabling Machine Learning Tasks atop Public Blockchains: A
Crowdsourcing Approach. In 2018 IEEE International Conference on Data Mining Workshops (November 17-20) (ICDMW).
IEEE Computer Society, 81–88. https://doi.org/10.1109/ICDMW.2018.00019

[26] L. Lyu, J. Yu, K. Nandakumar, Y. Li, X. Ma, J. Jin, H. Yu, and K. Ng. 2020. Towards Fair and Privacy-Preserving
IEEE Transactions on Parallel & Distributed Systems 31, 11 (11 2020), 2524–2541. https:

Federated Deep Models.
//doi.org/10.1109/TPDS.2020.2996273

[27] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Agüera y Arcas. 2017. Communication-
Efficient Learning of Deep Networks from Decentralized Data. In Proceedings of the 20th International Conference on
Artificial Intelligence and Statistics (April 20-22) (AISTATS ’17), Vol. 54. PMLR, 1273–1282. http://proceedings.mlr.press/
v54/mcmahan17a.html

[28] Grégoire Montavon, Geneviève Orr, and Klaus-Robert Müller. 2012. Neural networks: tricks of the trade. Vol. 7700.

Springer.

[29] Hervé Moulin. 1980. On strategy-proofness and single peakedness. Public Choice 35, 4 (1980), 437–455.
[30] Robert Nix and Murat Kantarciouglu. 2011. Incentive compatible privacy-preserving distributed classification. IEEE

Transactions on Dependable and Secure Computing 9, 4 (2011), 451–462.

[31] OECD. 2015. Data-Driven Innovation: Big Data for Growth and Well-Being. OECD Publishing. https://doi.org/10.1787/

9789264229358-en

[32] Ariel D. Procaccia and Moshe Tennenholtz. 2013. Approximate Mechanism Design without Money. ACM Transactions
on Economics and Computation (TEAC) 1, 4, Article 18 (12 2013), 26 pages. https://doi.org/10.1145/2542174.2542175

[33] Ocean Protocol. 2021. Tools for the Web3 Data Economy. Retrieved January 19, 2022 from https://oceanprotocol.com/tech-

whitepaper.pdf

[34] Siamak Ravanbakhsh, Jeff G. Schneider, and Barnabás Póczos. 2016. Deep Learning with Sets and Point Clouds. (2016).

arXiv:1611.04500

[35] Heiko Richter and Peter R Slowinski. 2019. The data sharing economy: on the emergence of new intermediaries.

IIC-International Review of Intellectual Property and Competition Law 50, 1 (2019), 4–29.
[36] George AF Seber and Alan J Lee. 2012. Linear regression analysis. Vol. 329. John Wiley & Sons.
[37] Yoav Shoham and Moshe Tennenholtz. 2005. Non-Cooperative Computation: Boolean Functions with Correctness and

Exclusivity. Theor. Comput. Sci. 343, 1–2 (10 2005), 97–113. https://doi.org/10.1016/j.tcs.2005.05.009

[38] James Surowiecki. 2005. The wisdom of crowds. Anchor.
[39] Qiang Yang, Yang Liu, Tianjian Chen, and Yongxin Tong. 2019. Federated machine learning: Concept and applications.

ACM Transactions on Intelligent Systems and Technology (TIST) 10, 2 (2019), 1–19.

[40] Makoto Yokoo, Yuko Sakurai, and Shigeo Matsubara. 2004. The effect of false-name bids in combinatorial auctions:
new fraud in internet auctions. Games and Economic Behavior 46, 1 (2004), 174–188. https://doi.org/10.1016/S0899-
8256(03)00045-9

21

A RELATED WORK

A.1 Collaborative Machine Learning
Data sharing between companies and institutions is an emerging phenomenon in the data economy
[8], still under-performing its full potential. Many companies, cloud services, and government
initiatives [4] offer frameworks and APIs to facilitate such exchange, as well as some decentralized
blockchain services [33]. However, the current focus of these services is in organizing the nuts and
bolts of such procedures (e.g. in terms of software, scale, and cyber-security), and not in ensuring
incentive-compatibility, in particular dealing with exclusivity attacks.

Mechanisms based on VCG and the Shapley-value were suggested as a method to construct
general incentive-compatible mechanisms for data collaboration in [30]. We highlight three main
aspects of that work that differ from our approach: They assume the existence of a test set for each
agent to compare other firms’ inputs (separate from the data set it communicates with others); They
consider a one-shot process rather than a continuous one, and they use monetary transfers while
we consider data sharing a barter between firms without exchanging money. The assumptions
we share with this work are that the true output of the machine learning algorithm is the best
parameter possible to learn and that the agents’ utilities are the NCC framework utilities.

In [18] the authors consider continuous data sharing implemented by a blockchain, with various
incentive mechanisms depending on the assumptions for agents’ incentives. An essential difference
with our work is that the data is assumed to be posted publicly and is thus known to all agents.
This is an issue both by itself in terms of privacy, but also when designing incentives. As we will
see, the uncertainty regarding other agents’ data is essential for the safety of certain mechanisms
under the NCC assumptions.

Federated learning is a popular framework for decentralized machine learning with private
information [27]. The general scheme has each agent perform stochastic gradient descent (SGD) by
itself and share the gradients with an aggregator, in order to train a global model. The global model
is public, and this is inherent to the operation of the mechanism since the agents are expected to
calculate the gradients. There is a natural free-rider attack (mentioned in [21]) in such cases where
the agent shares no data (or, possibly, a small amount of the data it has) and later completes the
training locally based on the global model and its remaining private data. This attack form fits
within our framework of exclusivity attacks, and we discuss in Section 6 how our insights may
apply to it.

There is a line of work that is orthogonal to ours [23, 25], which focuses on assigning model
training tasks to workers, in order to offload computation from being done by the central authority,
or on-chain in the case of a decentralized blockchain. We note that mechanisms built for this task
are different in nature and purpose from data sharing mechanisms.

A.2 Linear Regression and 𝑘–Center in Adversarial Settings
In this work, we use linear regression and the 𝑘-Center and 𝑘-Median problems to examine our
NCC utilities framework.

Linear regression [36] is a well-known regression mechanism. We study Multiple Linear Regres-

sion with 𝑑 features.

In [7] the authors study linear regression with users that have privacy concerns. In[5] the
authors suggest a mechanism using optimal monetary transfers to induce statistical estimation
using reports by workers that exert effort to attain more precise estimations. The mechanism is
shown to generalize to more general classes of regression than linear regression. Following the
framework of “Dueling algorithms” in [20], in [3] the authors consider firms optimizing their
regression models to better satisfy a subset of the users relative to the opponent. In [14] the authors

22

consider firms that control the level of noise they add to the dependent variable, and aim to balance
between privacy (more noise) and model accuracy (less). In [10] the authors consider a general
regression learning model where experts have strong opinions and wish to influence the resulting
model in their favor. As one can see, there are many strategic reasons to manipulate regression
tasks, but the NCC setting is a significant and understudied one.

The 𝑘-center and 𝑘-median problems [16, 19] are associated with clustering or facility location
algorithms. Facility location problems were studied extensively in strategic settings [6] [29]. The
main focus is usually on strategic users, that may manipulate reporting of their location to influence
the facility locations’ outcome [32]. For this purpose, strategy-proof mechanisms are developed,
with the goal of a small approximation ratio relative to the optimal (without strategic consideration)
algorithm. Our setting is different as we consider firms that acquired knowledge of users’ preferences
(or locations), and their goal of manipulation is not to benefit the users they have information
about but to know the resulting aggregate outcome better than the other firms.

B ILLUSTRATION OF PRELIMINARIES USING THE MAX ALGORITHM
We define the max algorithm:

Definition 9. Each update is a real number. 𝜌𝑚𝑎𝑥 (Ut) = max1≤𝑖 ≤𝑡 𝑈𝑖 .

Proposition 1. max is not ℓ-NCC-vulnerable* for any ℓ.

Proof. Consider w.l.o.g. agent 1 has a strategy 𝑠1 that satisfies conditions (𝑖∗) and (𝑖𝑖). For the
nature-input I = (< 2, 90 >), under 𝑡𝑟𝑢𝑡ℎ1 agent 2 receives a factual update < 2, 𝐹𝑎𝑐𝑡𝑢𝑎𝑙, 90 > and
then updates with < 2, 𝐿𝑒𝑑𝑔𝑒𝑟, 90 >, resulting in algorithm output 90. By condition (𝑖∗), under 𝑠1
the algorithm output after the full run must differ from 90. Agent 1 must thus update with at least
one update of the form < 1, 𝐿𝑒𝑑𝑔𝑒𝑟, 𝑥 > with 𝑥 larger than 90. Now consider the two nature-inputs
I ′ = (< 2, 90 >, < 2, 1
3 · 90 >), I ′′ = (< 2, 90 >, < 2, 2
3 · 90 >). Since 𝑥 ≠ 90,
the observed histories under 𝑡𝑟𝑢𝑡ℎ1 for I ′, I ′′ are not the same, as the last algorithm output are
3 · 𝑥 + 2
1

3 · 90 respectively. I.e.,

3 · 90 ≠ 2

3 · 𝑥 + 1

3 · 𝑥 + 2

3 · 𝑥 + 1

𝑂1(𝑅I′,𝑡𝑟𝑢𝑡ℎ1) ≠ 𝑂1(𝑅I′′,𝑡𝑟𝑢𝑡ℎ1 ).
Since the prefix of I ′, I ′′ is I, we know that under 𝑠1 by the end of the first round the algorithm
output is 𝑥. After agent 2 receives the second factual update and updates truthfully, the observed
history (in both cases) for agent 1 is (90, < 1, 𝐿𝑒𝑑𝑔𝑒𝑟, 𝑥 >, 𝑥, 𝑥) (where all but the second element
are algorithm outputs). Any strategy 𝑠1 response to this observed history will be the same for both
nature-inputs, and thus the observed histories of the full run satisfy

(7)

Equations 7,8 together contradict condition (𝑖𝑖).

𝑂1(𝑅I′,𝑠1) = 𝑂1(𝑅I′′,𝑠1).

(8)

□

Example 3. max is 1-NCC-vulnerable

Consider agent 1 with a strategy 𝑠1 that upon a factual update for agent 1, and given that there is a
previous algorithm output and the last algorithm output is 𝜌𝑣, updates with < 1, 𝐿𝑒𝑑𝑔𝑒𝑟, 𝜌𝑣 >, i.e., the
attacker repeats the last algorithm output as her own ledger update. Condition (𝑖) is satisfied: For the
nature-input I = (< 2, 100 >, < 1, 110 >), the last algorithm output for the run with 𝑠1 is 100, while
for the run with 𝑡𝑟𝑢𝑡ℎ1 it is 110. Condition (𝑖𝑖) is also satisfied: Consider two nature-inputs I, I ′ that
have the same observed run under 𝑠1. Notice that the last algorithm output is the maximum over the
other agents’ truthful ledger updates. The last algorithm output under 𝑡𝑟𝑢𝑡ℎ1 is the maximum between
other agents’ ledger updates and agent 1 maximum factual update, which is also observed under 𝑠1.
Therefore, the last algorithm output under 𝑡𝑟𝑢𝑡ℎ1 is determined by the observed history under 𝑠1, and

23

the natural way for the attacker to infer it is by taking the max over observed algorithm outputs and
its own factual updates.

We call such methods to construct the algorithm outputs under 𝑡𝑟𝑢𝑡ℎ 𝑗 out of the observed history

𝑂 𝑗 an inference function.

Definition 10. An inference function is a function from observed histories 𝑂 𝑗 to algorithm 𝜌

outputs.

Lemma 5. If there is an inference function 𝑖 𝑗 so that for every run 𝑅 of nature-input I with 𝑠 𝑗 ,
𝑖 𝑗 (𝑂 𝑗 (𝑅)) = 𝜌, where 𝜌 is the last algorithm output of the run of I with 𝑡𝑟𝑢𝑡ℎ 𝑗 , then condition (𝑖𝑖)
holds for 𝑠 𝑗 .

Proof. Assume by contradiction there are two nature-inputs I, I ′ with the same 𝑂 𝑗 when
running with 𝑠 𝑗 . The nature-inputs must be of the same length 𝑟 , otherwise, there would be a
different amount of total factual updates, and thus either a different amount of algorithm updates
not initiated by 𝑗 ledger updates, or a different amount of 𝑗 factual updates, both of which are
ℓ be the nature-inputs of length ℓ that start the same as I, I ′ but end after ℓ
observable. Let Iℓ, I ′
rounds. Since they have the same observed runs 𝑂 ℓ
𝑗 (parameterized by ℓ), running with 𝑡𝑟𝑢𝑒 𝑗 they
must have 𝑖 𝑗 (𝑂 ℓ
𝑗 ) as the algorithm output after the round ℓ. We conclude that all algorithm outputs
identify for the two nature-inputs running with 𝑡𝑟𝑢𝑡ℎ 𝑗 . The factual updates for 𝑗 also identify for
both nature-inputs since 𝑀𝑂
identify, and since running with 𝑡𝑟𝑢𝑡ℎ 𝑗 the ledger updates by 𝑗 are a
𝑗
copy of the factual updates of 𝑗, they also identify for both nature-inputs. We conclude that the
observable runs for both nature-inputs running with 𝑡𝑟𝑢𝑡ℎ 𝑗 identify, in compliance with condition
□
(𝑖𝑖).

C TECHNICAL LEMMAS FOR THE STRATEGY TEMPLATES
Lemma 1. If 𝑈𝑐𝑜𝑛𝑑 ≠ 𝑈𝑎𝑡𝑡𝑎𝑐𝑘 , the sneak attack is well defined, i.e., the conditions to start and end
attack can be implemented using only 𝑂 𝑗 .

Proof. We show that the condition to start attack (line 1) was previously invoked by 𝑠 𝑗 during
the run of the continuous protocol iff 𝑂 𝑗 contains three subsequent elements, < 𝑗, 𝐹𝑎𝑐𝑡𝑢𝑎𝑙, 𝑈𝑐𝑜𝑛𝑑 >
, < 𝑗, 𝐿𝑒𝑑𝑔𝑒𝑟, 𝑈𝑎𝑡𝑡𝑎𝑐𝑘 >, 𝜌𝑣1 for some 𝜌𝑣1: If the condition was invoked, then at that point the last
element in 𝑂 𝑗 was < 𝑗, 𝐹𝑎𝑐𝑡𝑢𝑎𝑙, 𝑈𝑐𝑜𝑛𝑑 >, and the agent updates with < 𝑗, 𝐿𝑒𝑑𝑔𝑒𝑟, 𝑈𝑎𝑡𝑡𝑎𝑐𝑘 >, and
finally the ledger updates all with some algorithm output 𝜌𝑣1. If it was not invoked before, then
the condition to end attack (in line 3) was not as well (as it depends on the condition to start
attack being previously invoked). Therefore all ledger updates by 𝑗 are of the form of an algorithm
output following some < 𝑗, 𝐿𝑒𝑑𝑔𝑒𝑟, 𝑈 > after < 𝑗, 𝐹𝑎𝑐𝑡𝑢𝑎𝑙, 𝑈 >. Since 𝑈𝑐𝑜𝑛𝑑 ≠ 𝑈𝑎𝑡𝑡𝑎𝑐𝑘 , the pattern
< 𝑗, 𝐹𝑎𝑐𝑡𝑢𝑎𝑙, 𝑈𝑐𝑜𝑛𝑑 >, < 𝑗, 𝐿𝑒𝑑𝑔𝑒𝑟, 𝑈𝑎𝑡𝑡𝑎𝑐𝑘 >, 𝜌𝑣1 can not appear.

We can thus use the above signature (together with the additional conditions given in line 1) to

decide whether to invoke the condition to start the attack.

The condition to end attack is invoked iff 𝑂 𝑗 last four elements are either of the form

< 𝑗, 𝐹𝑎𝑐𝑡𝑢𝑎𝑙, 𝑈𝑐𝑜𝑛𝑑 >, < 𝑗, 𝐿𝑒𝑑𝑔𝑒𝑟, 𝑈𝑎𝑡𝑡𝑎𝑐𝑘 >, 𝜌𝑣1, 𝜌𝑣2,

for some algorithm outputs 𝜌𝑣1, 𝜌𝑣2, or of the form

< 𝑗, 𝐹𝑎𝑐𝑡𝑢𝑎𝑙, 𝑈𝑐𝑜𝑛𝑑 >, < 𝑗, 𝐿𝑒𝑑𝑔𝑒𝑟, 𝑈𝑎𝑡𝑡𝑎𝑐𝑘 >, 𝜌𝑣1, < 𝑗, 𝐹𝑎𝑐𝑡𝑢𝑎𝑙, 𝑈 >,
for some algorithm output 𝜌𝑣1 and factual update 𝑈 . We verify this signature matches the verbal de-
scription. If this signature appears, by our conclusion, the subsequent elements < 𝑗, 𝐹𝑎𝑐𝑡𝑢𝑎𝑙, 𝑈𝑐𝑜𝑛𝑑 >
, < 𝑗, 𝐿𝑒𝑑𝑔𝑒𝑟, 𝑈𝑎𝑡𝑡𝑎𝑐𝑘 >, 𝜌𝑣1 show that the condition to start attack was invoked. If we see a factual
update or an algorithm output after that, it can only result in the continuous protocol from some

24

agent receiving a factual update. Since these are the last elements in 𝑂 𝑗 , and there is no additional
ledger update, the condition to end attack could not have previously been invoked since it only
happens after the condition to start attack was invoked and sends an additional Ledger update.

□

Lemma 2. A sneak attack where 𝑈𝑎𝑡𝑡𝑎𝑐𝑘 ⊆ 𝑈𝑐𝑜𝑛𝑑, 𝑈𝑟𝑒−𝑠𝑦𝑛𝑐 = 𝑈𝑐𝑜𝑛𝑑 \ 𝑈𝑎𝑡𝑡𝑎𝑐𝑘 , and that moreover
after starting the attack and sending 𝑈𝑎𝑡𝑡𝑎𝑐𝑘 , satisfies
can infer the last algorithm output in 𝑅I,𝑡𝑟𝑢𝑡ℎ 𝑗
condition (𝑖𝑖).

Proof. For condition (𝑖𝑖), consider two nature-inputs I, I ′ with the same observed run

(9)

𝑂 𝑗 (𝑅I,𝑠 𝑗 ) = 𝑂 𝑗 (𝑅I′,𝑠 𝑗 ).
If 𝑂 𝑗 (𝑅I,𝑠 𝑗 ) = 𝑂 𝑗 (𝑅I,𝑡𝑟𝑢𝑡ℎ 𝑗 ), it means that the condition to start attack (line 1) of 𝑠 𝑗 was not
invoked during the run. Thus, there is no update < 𝑗, 𝐹𝑎𝑐𝑡𝑢𝑎𝑙, 𝑈𝑐𝑜𝑛𝑑 > in 𝑂 𝑗 (𝑅I,𝑠 𝑗 ), and by Eq. 9
also not in 𝑂 𝑗 (𝑅I′,𝑠 𝑗 ). We therefore conclude that the condition to start attack is not invoked in
the run of I ′ with 𝑠 𝑗 . Since the condition to end attack (line 3) is only invoked if at a previous
stage the condition to start attack was invoked, and so we conclude that all updates by 𝑠 𝑗 for
I ′ are truthful, and therefore 𝑂 𝑗 (𝑅I′,𝑠 𝑗 ) = 𝑂 𝑗 (𝑅I′,𝑡𝑟𝑢𝑡ℎ 𝑗 ). All in all, the equations establish that
𝑂 𝑗 (𝑅I,𝑡𝑟𝑢𝑡ℎ 𝑗 ) = 𝑂 𝑗 (𝑅I′,𝑡𝑟𝑢𝑡ℎ 𝑗 ).

If 𝑂 𝑗 (𝑅I,𝑠 𝑗 ) ≠ 𝑂 𝑗 (𝑅I,𝑡𝑟𝑢𝑡ℎ 𝑗 ), then the condition to start attack must have been invoked during
the run and there is a first update 𝑈 =< 𝑗, 𝐹𝑎𝑐𝑡𝑢𝑎𝑙, 𝑈𝑐𝑜𝑛𝑑 > in 𝑂 𝑗 (𝑅I,𝑠 𝑗 ) such that the preceding
algorithm output is 𝜌𝑐𝑜𝑛𝑑 . Let the index of this element be 𝑖. Before this update 𝑈 , 𝑠 𝑗 only responds
truthfully, and so the observed runs satisfy 𝑂 𝑗 (𝑅I,𝑡𝑟𝑢𝑡ℎ 𝑗 )1:𝑖−1 = 𝑂 𝑗 (𝑅I,𝑡𝑟𝑢𝑡ℎ 𝑗 )1:𝑖−1. Factual updates
are preserved across observed runs with different strategies (𝑠 𝑗 vs 𝑡𝑟𝑢𝑡ℎ 𝑗 ), and so 𝑂 𝑗 (𝑅I,𝑡𝑟𝑢𝑡ℎ 𝑗 )𝑖 =
𝑂 𝑗 (𝑅I,𝑠 𝑗 )𝑖 = 𝑂 𝑗 (𝑅I′,𝑠 𝑗 )𝑖 = 𝑂 𝑗 (𝑅I′,𝑡𝑟𝑢𝑡ℎ 𝑗 )𝑖 . Since 𝑡𝑟𝑢𝑡ℎ 𝑗 follows each factual update of 𝑗 with a
ledger update with the same 𝑈 , we have 𝑂 𝑗 (𝑅I,𝑡𝑟𝑢𝑡ℎ 𝑗 )𝑖+1 =< 𝑗, 𝐿𝑒𝑑𝑔𝑒𝑟, 𝑈𝑐𝑜𝑛𝑑 >= 𝑂 𝑗 (𝑅I′,𝑡𝑟𝑢𝑡ℎ 𝑗 )𝑖+1.
Since we require that agent 𝑗 can infer the algorithm output 𝜌𝑖𝑛𝑓 𝑒𝑟 under 𝑡𝑟𝑢𝑡ℎ 𝑗 immediately
after the start of the attack, and the observed histories up until this algorithm output identify for
𝑅I,𝑠 𝑗 , 𝑅I′,𝑠 𝑗

, it must hold that 𝑂 𝑗 (𝑅I,𝑡𝑟𝑢𝑡ℎ 𝑗 )𝑖+2 = 𝜌𝑖𝑛𝑓 𝑒𝑟 = 𝑂 𝑗 (𝑅I′,𝑡𝑟𝑢𝑡ℎ 𝑗 )𝑖+2.

We assume for simplicity that the first factual update after the factual update in index 𝑖 is an

update of 𝑗. The argument can be extended to the case where it is not with more details.

As noted before factual updates are preserved in the observed histories of different strategies,
and so if there are at most 𝑖 + 2 elements in 𝑂 𝑗 (𝑅I,𝑡𝑟𝑢𝑡ℎ 𝑗 ), then the factual update that is element 𝑖
corresponds to the last element in I. Therefore it is also the last factual update in 𝑂 𝑗 (𝑅I,𝑠 𝑗 ), and by
Eq 9 also in 𝑂 𝑗 (𝑅I′,𝑠 𝑗 ), and by the same argument in 𝑂 𝑗 (𝑅I′,𝑡𝑟𝑢𝑡ℎ 𝑗 ). Since in runs with 𝑡𝑟𝑢𝑡ℎ 𝑗 each
factual update of 𝑗 is followed exactly by a ledger update of 𝑗 and an algorithm output, we conclude
that there are no more elements after 𝑖 + 2 for 𝑂 𝑗 (𝑅I′,𝑠 𝑗 ), and so it identifies with 𝑂 𝑗 (𝑅I,𝑠 𝑗 ) (as
we’ve shown all elements are the same).

If there is a factual update at index 𝑖 +3 in 𝑂 𝑗 (𝑅I,𝑡𝑟𝑢𝑡ℎ 𝑗 ), since we assume it is for 𝑗, it identifies for
the two nature-inputs’ observed histories with 𝑠 𝑗 , and as factual updates do not depend on strategy,
we also have 𝑂 𝑗 (𝑅I,𝑡𝑟𝑢𝑡ℎ 𝑗 )𝑖+3 = 𝑂 𝑗 (𝑅I′,𝑡𝑟𝑢𝑡ℎ 𝑗 )𝑖+3. The subsequent ledger update and algorithm
output thus identify as well. Note that by the condition to end attack, for both I, I ′, the ledger
update at step 𝑖 + 4 by 𝑠 𝑗 is 𝑈𝑟𝑒−𝑠𝑦𝑛𝑐 .

At any step after 𝑖 + 4, the union of points sent throughout the ledger history identifies with the
union of points in the factual history, by our requirement that 𝑈𝑟𝑒−𝑠𝑦𝑛𝑐 = 𝑈𝑐𝑜𝑛𝑑 \ 𝑈𝑎𝑡𝑡𝑎𝑐𝑘 . All in all
this shows that observed histories under 𝑠 𝑗 identify =⇒ observed histories under 𝑡𝑟𝑢𝑡ℎ 𝑗 identify,
which is logically equivalent to condition (𝑖𝑖).

□

25

Lemma 3. The triangulation attack is well defined, i.e., the conditions in lines 1 and 2 can be
implemented using only information available in 𝑂 𝑗 . The assignment in line 3 is valid, that is, given
that line 3 is executed there exists an algorithm output in 𝑂 𝑗 .

Proof. We specify the way to implement the required predicates, without giving the full proof,

which goes by an argument similar to the proof structure of Lemma 1.

For line 1, there is a factual update after the last ledger update by agent 𝑗, iff it is either a factual

update of 𝑗 or of another agent 𝑎 ≠ 𝑗.

There is such factual update of an agent 𝑎 ≠ 𝑗 iff the last two elements in 𝑂 𝑗 are both algorithm
outputs, or there is only one element in 𝑂 𝑗 and it is an algorithm output (this is the case where there
are no ledger updates by agent 𝑗). The case where it is a factual update of agent 𝑗 is immediately
visible in 𝑂 𝑗 (agent 𝑗 can see its own factual updates).

For line 2, a triangulation attack is ongoing iff the pattern above is matched, followed by a series
of pairs of the form < 𝑗, 𝐿𝑒𝑑𝑔𝑒𝑟, 𝑈 >, 𝜌𝑣1 for some ledger update 𝑈 and algorithm output 𝜌𝑣1. If the
number of pairs is 𝑖, then the triangulation attack previously executed 𝑖 steps and we are at step
𝑖 + 1 of the attack.

For line 3, we reach it only given that 𝑖 is defined, and by the two possible signatures that make
it happen (either starting a new triangulation attack or continuing an ongoing triangulation attack)
assume that there are at least two algorithm outputs in 𝑂 𝑗 , hence it is valid to define 𝜌𝑖−1 as the
last algorithm output in 𝑂 𝑗 .

□

D MISSING PROOFS FOR 𝑘–CENTER AND 𝑘–MEDIAN
Corollary 2. 𝑘-median is not ℓ-NCC-vulnerable* for any ℓ.

Proof. 𝑘-median fits the definition of a set-choice algorithm. We show that it has forceable
winners. We show it for 3-median with the domain 𝑅, but the proof for general 𝑘, 𝑅𝑑 is similar. Let
𝑥 ∈ 𝑅 and a set 𝑆 with 𝑥 ∈ 𝑆. Let the symmetric completion of 𝑆 around 𝑥 be 𝑂 = ∪𝑠 ∈𝑆 {𝑠, 2𝑥 − 𝑠}.
I.e., every point 𝑠 = 𝑥 + 𝜖 is added the matching 𝑠 ′ = 𝑥 − 𝜖. Let 𝐶 = max{(cid:205)𝑜 ∈𝑂 |𝑜 − 𝑥 |, 1}. Let
¯𝑆 = 𝑂 ∪ {𝑥 + 10𝐶, 𝑥 + 100𝐶}. For this construction, the following claim holds and completes the
proof:
Claim 4. 𝜌 (𝑆 ∪ ¯𝑆) = {𝑥, 𝑥 + 10𝐶, 𝑥 + 100𝐶}.

Proof. If {𝑥 + 10𝐶, 𝑥 + 100𝐶} ⊆ 𝜌 (𝑆 ∪ ¯𝑆), then the remaining center 𝜁 will have all remaining
points closest to it. If we write 𝜁 = 𝑥 + 𝛿, then the total cost attributed to this center must satisfy
|𝛿 | + (cid:205)𝑠 ∈𝑆 |𝜖=𝑠−𝑥 >0 |(𝑥 + 𝜖) − (𝑥 + 𝛿)| + |(𝑥 − 𝜖) − (𝑥 + 𝛿)| = |𝛿 | + (cid:205)𝑠 ∈𝑆 |𝜖=𝑠−𝑥 >0(| − 𝜖 − 𝛿 | + |𝜖 − 𝛿 |) ≥
|𝛿 | + (cid:205)𝑠 ∈𝑆 |𝜖=𝑠−𝑥 >0 2𝜖. But the cost if 𝑥 is the remaining center is exactly (cid:205)𝑠 ∈𝑆 |𝜖=𝑠−𝑥 >0 2𝜖, and so it
is strictly better than the cost with |𝛿 | > 0. We conclude that in this case 𝑥 is the remaining center.
If either of {𝑥 + 10𝐶, 𝑥 + 100𝐶} is not in 𝜌 (𝑆 ∪ ¯𝑆), then the cost of the solution is at least 9𝐶. But
□

the cost for {𝑥, 𝑥 + 10𝐶, 𝑥 + 100𝐶} is exactly 𝐶, and 𝐶 > 0.

□

Theorem 6. 𝑘-center is not vulnerable under the periodic communication protocol.

The proof of the theorem is two-fold using the distinction of explicitly lying and omission
strategies. As for explicitly-lying strategies, Claim 2 holds for periodic communication as well, with
minor adjustments. We are thus left to show:

Lemma 6. An omission strategy 𝑠 𝑗 for 𝑘-center violates condition (𝑖𝑖) with the periodic protocol.

26

Proof. In the proof of Corollary 1 we show that 𝑘-center has forceable winners by a certain
construction. We now use similar ideas to get a construction with more detailed properties, as
formalized in the following claim:

Claim 5. For 𝑘-center, for every set 𝑆 with |𝑆 | ≥ 2 and a point 𝑥 ∈ 𝑆, there is such 𝑦 ∈ 𝑆, and ¯𝑆, ¯𝑆 ′ so
that

𝜌 ((𝑆 ∪ ¯𝑆 ′) \ {𝑥 }) = 𝜌 (𝑆 ∪ ¯𝑆 ′) = 𝜌 ((𝑆 ∪ ¯𝑆) \ {𝑥 }) = {𝑦, 𝜂1, . . . , 𝜂𝑘−1}

for some 𝜂1, . . . , 𝜂𝑘−1 ∉ 𝑆 (in particular, not 𝑥).
In addition, ¯𝑆 satisfies the conditions of Definition 6.

Proof. We show an explicit construction. Let 𝑦 = arg min𝑥 ′ ∈𝑆,𝑥 ′≠𝑥 |𝑥 ′ −𝑥 |. Let Δ = max𝑠 ∈𝑆 |𝑥 −𝑠 |.
Let ¯𝑆 = {𝑥 + 2Δ, 𝑥 − 2Δ, 𝑥 + 10Δ, ..., 𝑥 + 10𝑘−1Δ, ¯𝑆 ′ = {𝑦 + Δ, 𝑦 − Δ, 𝑥 + 10Δ, . . . , 𝑥 + 10𝑘−1Δ}. We
have 𝜌 (𝑆 ∪ ¯𝑆) = {𝑥, 𝑥 + 10Δ, . . . , 𝑥 + 10𝑘−1Δ}, 𝜌 ((𝑆 ∪ ¯𝑆 ′) \ {𝑥 }) = 𝜌 (𝑆 ∪ ¯𝑆 ′) = 𝜌 ((𝑆 ∪ ¯𝑆) \ {𝑥 }) =
{𝑦, 𝑥 + 10Δ, . . . , 𝑥 + 10𝑘−1Δ}.
□

We now prove the lemma statement. Consider some nature-input I be the shortest (in terms of
number of elements) where 𝑠 𝑗 sends a ledger update with an explicit lie 𝑥, and let 𝐿𝑅 = 𝐿𝑗 (𝑅I,𝑠 𝑗 ), 𝐹𝑅 =
𝐹 𝑗 (𝑅I,𝑠 𝑗 ) be the union of all ledger, factual updates respectively by 𝑗. Let 𝑆 = 𝐹𝑅 ∪ 𝐿𝑅 ∪ {𝑥 + 1},
where we add the point 𝑥 + 1 to make sure there is some additional point besides 𝑥 in 𝑆 and have
|𝑆 | ≥ 2.

Let 𝐸1 = (𝑆 ∪ ¯𝑆) \ {𝑥 }, 𝐸2 = (𝑆 ∪ ¯𝑆𝜖 ) \ {𝑥 }. As 𝜌 (𝐸1 ∪ {𝑥 }) ≠ 𝜌 (𝐸2 ∪ {𝑥 }), it must hold that 𝐸1 ≠ 𝐸2.
Let 𝑟 be the last round of I. Let I1, I2 be I with an additional last element < 𝑖, 𝐸1, 𝑟 >, < 𝑖, 𝐸2, 𝑟 >
respectively, for some agent 𝑖 ≠ 𝑗 (or, if all agents already have an element in this round, add
𝐸1, 𝐸2 respectively to the element of one of them). We have that 𝑂 𝑗 (𝑅I1,𝑠 𝑗 ) = 𝑂 𝑗 (𝑅I2,𝑠 𝑗 ), since the
nature-inputs identify up until the last round, and the last round induces only an algorithm output
𝜌 ((𝑆 ∪ ¯𝑆)\{𝑥 })) = 𝜌 (𝐿𝑅 ∪𝐸1).
observation by 𝑗, which satisfies 𝜌 (𝐿𝑅 ∪𝐸2) = 𝜌 ((𝑆 ∪ ¯𝑆𝜖 )\{𝑥 }))
However, the last algorithm output in 𝑂 𝑗 (𝑅I1,𝑡𝑟𝑢𝑡ℎ 𝑗 ) is 𝜌 (𝐹𝑅 ∪𝐸1) = 𝜌 (𝐹𝑅 ∪(𝐹𝑅 ∪𝐿𝑅 ∪ ¯𝑆)) = 𝜌 (𝑆∪ ¯𝑆),
and thus has the element 𝑥 (By Claim 5 guarantee that ¯𝑆 satisfies the conditions of Definition 6).
On the other hand, the last algorithm output in 𝑂 𝑗 (𝑅I2,𝑡𝑟𝑢𝑡ℎ 𝑗 ) is 𝜌 (𝐹𝑅 ∪ 𝐸2) = 𝜌 (𝑆 ∪ ¯𝑆𝜖 ), and does
□
not contain 𝑥 as an element by Claim 5.

𝐶𝑙𝑎𝑖𝑚 5
=

E MISSING PROOFS FOR 𝑑–LINEAR REGRESSION
Example 4. 1 − 𝐿𝑅 is 1-NCC-vulnerable using an explicitly-lying sneak attack: Use Algorithm 1
with

𝑈𝑐𝑜𝑛𝑑 = (Xcond, ycond) = (







𝑈𝑟𝑒−𝑠𝑦𝑛𝑐 = (Xre−sync, yre−sync) = (

,

1


1


1



3
0
0

1




1




1




(cid:20)1
2
1 −1

(cid:21)

,

(cid:21)

(cid:20)0
1

), 𝜌𝑐𝑜𝑛𝑑 = 𝛽 =

(cid:21)

(cid:20)1
0

.

), 𝑈𝑎𝑡𝑡𝑎𝑐𝑘 = (Xattack, yattack) = ( (cid:2)1

2(cid:3) , (cid:2)2(cid:3)),

Condition (𝑖) is satisfied since for nature-input I = (< 1, (

(cid:20)1
1

(cid:21)

1
0

,

with 𝑡𝑟𝑢𝑡ℎ2 yields algorithm outputs

(cid:21)

(cid:20)1
0

,

(cid:21)

(cid:20)1
0

but the run with 𝑠2 yields

(cid:21)

(cid:20)1
) >, < 2, 𝑈𝑐𝑜𝑛𝑑 >), the run
1
(cid:21)
(cid:20)1
0

(cid:21)

,

.

(cid:20) 5
6
1
2

For condition (𝑖𝑖), the argument generally follows the proof of Lemma 2. We note two important

distinctions:

27

• Given 𝜌𝑐𝑜𝑛𝑑, 𝑈𝑐𝑜𝑛𝑑 we can infer the algorithm output under 𝑡𝑟𝑢𝑡ℎ 𝑗 is 𝜌𝑐𝑜𝑛𝑑 . That is since 𝜌𝑐𝑜𝑛𝑑
has the minimal cost function given all updates previous to 𝑈𝑐𝑜𝑛𝑑 (for both I, I ′): We know
that since it is the algorithm output before the factual update 𝑈𝑐𝑜𝑛𝑑 . Moreover, it has cost 0 with
regards to 𝑈𝑐𝑜𝑛𝑑 .

• At any step after 𝑖 +4 (the completion of the sneak attack), the union of points sent throughout the
ledger history does not identify anymore with the union of points in the factual history, since the
ledger history contains explicit lies (namely, any of the points in our choice of 𝑈𝑎𝑡𝑡𝑎𝑐𝑘, 𝑈𝑟𝑒−𝑠𝑦𝑛𝑐 ).
However, for the calculation of the algorithm output in 1 − 𝐿𝑅, we have 𝜌 = (𝑋𝑇 𝑋 )−1𝑋𝑇𝑦. Since
updates aggregation is additive, as long as two different updates have the same 𝑋𝑇 𝑋, 𝑋𝑇𝑦, any
sequence of updates containing them would have the same algorithm outputs. In our case, we
have

(𝑋 𝑐𝑜𝑛𝑑 )𝑇 𝑋 𝑐𝑜𝑛𝑑 =

(𝑋 𝑐𝑜𝑛𝑑 )𝑇𝑦 =

(cid:21)

(cid:20)3
3

(cid:20)3
3

(cid:21)

3
9

= (𝑋 𝑎𝑡𝑡𝑎𝑐𝑘 )𝑇 𝑋 𝑎𝑡𝑡𝑎𝑐𝑘 + (𝑋 𝑟𝑒−𝑠𝑦𝑛𝑐 )𝑇 𝑋 𝑟𝑒−𝑠𝑦𝑛𝑐,

= (𝑋 𝑎𝑡𝑡𝑎𝑐𝑘 )𝑇𝑦𝑎𝑡𝑡𝑎𝑐𝑘 + (𝑋 𝑟𝑒−𝑠𝑦𝑛𝑐 )𝑇𝑦𝑟𝑒−𝑠𝑦𝑛𝑐 .

By the construction of 𝑠 𝑗 , any sequence of updates after the attack is “re-synced” behaves just as
if the agent has acted truthfully, and so the observed truthful histories identify subsequently. All
in all this shows that observed histories under 𝑠 𝑗 identify =⇒ observed histories under 𝑡𝑟𝑢𝑡ℎ 𝑗
identify, which is logically equivalent to condition (𝑖𝑖).

Claim 3. For every algorithm output 𝜌 =

, and a single point update 𝑈 ∗ = (𝑋 ∗ = (cid:2)1

𝑥1

. . .

𝑥𝑑 (cid:3) , 𝑦∗)

𝛼1


. . .


𝛼𝑑+1










so that 𝑋 ∗ · 𝜌 ≠ 𝑦∗, the new algorithm output 𝜌 ′ for the data with 𝑈 ∗ satisfies 𝜌 ′ ≠ 𝜌, and has a
different value at 𝑋 ∗ than 𝑋 ∗ · 𝜌.

Proof. Let 𝜌 =






𝑑+1

First, we show that it can not be that 𝜌 = 𝜌 ′. Assume otherwise, then by the extremal condition
over the optimization function,

, and let the algorithm output after adding the point 𝑋 ∗, 𝑦∗ be 𝜌 ′ =








.

𝛼1


. . .


𝛼𝑑+1



𝛼 ′

1

. . .


𝛼 ′



𝛼1 =

(cid:205)𝑛

𝑖=1(𝑦𝑖 − (cid:205)𝑑
𝑛

𝑗=1 𝛼 𝑗+1𝑥 𝑗
𝑖 )

,

and similarly for 𝜌 ′ = 𝜌,

𝛼1 =

((cid:205)𝑛

𝑖=1 𝑦𝑖 − (cid:205)𝑑

𝑗=1 𝛼 𝑗+1𝑥 𝑗

𝑖 ) + (𝑦∗ − (cid:205)𝑑
𝑛 + 1

𝑗=1 𝛼 𝑗+1𝑥 ∗
𝑗 )

=

𝑛𝛼1 + 𝛼1
𝑛 + 1

+

𝑦∗ − (cid:205)𝑑

𝛼1 +

𝑦∗ − (cid:205)𝑑

𝑗=1 𝛼 𝑗+1𝑥 ∗
𝑛 + 1

𝑗 − 𝛼1

=

𝑗=1 𝛼 𝑗+1𝑥 ∗
𝑛 + 1
𝑗 − 𝛼1

≠ 𝛼1,

where the last inequality is since the point (𝑋 ∗, 𝑦∗) is outside the line and thus 𝑦∗−(cid:205)𝑑
𝑗 −𝛼1 ≠
0. We arrived at a contradiction and so we may subsequently assume 𝜌 ≠ 𝜌 ′. Now, again assume by
contradiction that the lines intersect at the point (𝑋 ∗, 𝑦∗). Then, with respect to (𝑋 ∗, 𝑦∗) the two
lines would have the same cost function value 0, but overall with respect to all 𝑦𝑖 with 1 ≤ 𝑖 ≤ 𝑛, 𝜌

𝑗=1 𝛼 𝑗+1𝑥 ∗

is the unique optimal cost minimizer, so we conclude that 𝜌 has lower cost overall than 𝜌 ′ with
□
respect to all the given points, in contradiction to 𝜌 ′ being optimal.

28

