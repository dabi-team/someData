1
2
0
2

t
c
O
5
1

]
I

N
.
s
c
[

1
v
4
6
9
7
0
.
0
1
1
2
:
v
i
X
r
a

Federated Route Leak Detection in Inter-domain
Routing with Privacy Guarantee

Man Zeng, Dandan Li, Pei Zhang, Kun Xie, Xiaohong Huang

1

Abstract—In the inter-domain network, a route leak occurs when a routing announcement is propagated outside of its intended scope,
which is a violation of the agreed routing policy. The route leaks can disrupt the internet trafﬁc and cause large outages. The accurately
detection of route leaks requires the share of AS business relationship information of ASes. However, the business relationship
information between ASes is conﬁdential due to economic issues. Thus, ASes are usually unwilling to revealing this information to the
other ASes, especially their competitors. Recent advancements in federated learning make it possible to share data while maintaining
privacy. Motivated by this, in this paper we study the route leak problem by considering the privacy of business relationships between
ASes, and propose a method for route leak detection with privacy guarantee by using blockchain-based federated learning framework,
in which ASes can train a global detection model without revealing their business relationships directly. Moreover, the proposed method
provides a self-validation scheme by labeling AS triples with local routing policies, which mitigates route leaks’ lack of ground truth. We
evaluate the proposed method under a variety of datasets including unbalanced and balanced datasets. The different deployment
strategies of the proposed method under different topologies are also examined. The results show that the proposed method has a
better performance in detecting route leaks than a single AS detection regardless of whether using balanced or unbalanced datasets.
In the analysis of the deployment, the results show that ASes with more peers have more possible route leaks and can contribute more
on the detection of route leaks with the proposed method.

Index Terms—BGP security, route leak detection, federated learning
(cid:70)

1 INTRODUCTION

Border Gateway Protocol (BGP) is used in the inter-domain
network for exchange of routing information between au-
tonomous systems (ASes). In BGP, each AS selects the best
route according to its routing policies and announces the
selected route to neighbors. Different from the shortest path
routing policy in the intra-domain, the routing policy in the
interdomain network is more complicated since it considers
business relationships between ASes, where the business
relationship can be categorized into two types according to
AS economic factors [1]: customer-to-provider (c2p) and peer-
to-peer (p2p).

The inter-domain routing policies have been extensively
investigated in a number of studies, such as [2], [3], [4]. Their
analysis suggests that a policy most commonly adopted is
valley-free rule [5]. In the valley-free rule, routes learned from
providers or peers should not be exported to other providers
or peers. However, due to misconﬁguration and malicious
attacks, the routing announcement may be propagated in
violation of their agreed routing policy, which is deﬁned as
route leak [6], [7].

Route leaks can cause major outages by redirecting
trafﬁc and bring a risk of encountering Man-in-the Middle
attacks [8]. For instance, during March 12, 2015, a broadband
provider of India (AS17488) wrongly announced over three
hundred Google’s preﬁxes to its provider AS9498, making
many of Google’s services inaccessible to their users [9].
Another incident occurred on February 11, 2021, AS28548
in Mexico leaked more than two thousand preﬁxes to its
neighbors and affected about 80 countries in the world
[10]. The detection of route leak is becoming increasingly

Corresponding author: Xiaohong Huang, huangxh@bupt.edu.cn

important, given the growing number of serious route leak
reports.

The major challenge of detecting route leaks is that the
business relationships of ASes are conﬁdential. Each AS
only knows the relationships between itself and its neigh-
bors but does not know the exact relationship of others due
to privacy issues. In order to detect route leaks, some studies
[1], [11], [12], [13] focus on inferring AS business relation-
ships. However, their inference techniques still suffer errors
on partial critical links [12]. Studies like [7], [14], [15], [16] do
not consider the privacy of AS relationship well and require
deployers directly revealing AS relationship information,
which makes the deployment hard to proceed.

Another challenge for route leak detection is the lack of
ground truth. Only a few destructive routing leak events
reported have been validated. Route leaks that are not
related to the customer-aware services, e.g. multi-media
services, are hard to be validated. For example, S. Abd
El Monem et al. [17] show there were only 13 validated
route leak incidents between 2006 and 2018. The lack of a
ground truth makes popular techniques such as traditional
machine learning techniques difﬁcult to be utilized in route
leak detection.

Federated learning is a distributed machine-learning
method that allows participants globally to train a model
without needing to transport their local training data to a
central server. Moreover, instead of aggregating local train-
ing data, federated learning aggregates local model updates
of participants, which can protect data privacy of partic-
ipants. However, traditional federated learning methods
require a third-party server for aggregating model updates,
which is vulnerable to single point of failure. To avoid this
problem, blockchain-based federated learning framework

 
 
 
 
 
 
[18], [19], [20], [21] is proposed, where blockchain can pro-
vide security management of participants, auditability, and
so on.

In this paper, we propose a method to route leak de-
tection using a blockchain-based federated learning frame-
work. As outlined previously, one AS has limited AS busi-
ness relationship information, while the detection of route
leaks needs as much relationship information as possible.
By using federated learning, ASes can globally train a model
to identify route leaks with sharing model updates instead
of directly sharing their business relationships with others.
Furthermore, in order to further strengthen the privacy pro-
tection of relationship information, we propose to replace
business relationships with AS triples to train the models.
By labeling AS triples as malicious or regular using local
routing policies, our method provides a solution to solve
the lack of ground truth in route leaks.

In the proposed method, AS participants ﬁrstly use their
known routing policies to generate local training data for
training. The local training data are composed of AS triples
and corresponding labels that show the triples are malicious
or regular. Then, each AS participant uses its local training
data to train a local model and exchange updates of the local
model with each other through the blockchain network. In
each global communication round, one AS can gather all
local model updates of participants and aggregate these
updates. After the training is ﬁnished, the ﬁnal global model
updates are stored in the blockchain. AS participants can
retrieve the global model from blockchain and utilize it for
further route leak detection.

The contributions of this paper are summarized as fol-

lows:

• We consider the problem of route leak detection and
propose a privacy-preserving method for sharing
routing policy information in AS triples compared
to methods that directly share business relationships
among ASes.

• We customize a blockchain-based federated learning
framework to learn routing policies in inter-domain
networks and globally train a model to detect route
leaks accurately.

• We evaluate the proposed method on different
datasets and analyze different deployment strategies
of the method under different topologies. Results
show that the proposed method can improve the
performance of a single AS in detecting route leaks.
The results also indicate that AS with more peers
appears to have more possible route leaks and can
contribute more to the detection of route leaks than
others if they deploy the method.

The remainder of the paper is organized as follows.
Section 2 gives an overview of route leak deﬁnitions in
inter-domain networks and related works in detecting route
leaks. In Section 3, we introduce the route leak detection
problem and the proposed solution method. Section 4 intro-
duces the conducted experiments and the analysis of results.
Section 5 gives the conclusion of this paper.

2

2 BACKGROUND AND RELATED WORKS
2.1 Route Leaks

Route leaks deﬁned in RFC 7908 [6] are categorized into
six types: Type1: a multihomed AS (has more than one
provider) leaks the routes learned from its one provider to
its another provider. Type2: an AS leaks the routes learned
from its one peer to its another peer. Type3: an AS leaks
learned routes from its provider to its peer. Type4: an AS
leaks of the routes learned from its peer to its provider.
Type5: a multihomed AS announces the routes learned from
its one upstream ISP as its origin routes to its another
upstream ISP. Type6: an AS leaks its internal routes to its
provider or peer.

The Type5 and Type6 leaks are more related to the
origin preﬁx hijacking and can be detected by Route Ori-
gin Validation (ROV) techniques, such as Resource Public
Key Infrastructure (RPKI) [22]. RPKI can build a trusted
repository for validating ownership certiﬁcates of preﬁxes
and origin ASN. Type1, Type2, Type3 and Type4 leaks are
primarily grouped by ASes business relationships. Fig.1
illustrates simple examples of these 4 types of route leaks,
where P is the announcing route. In this paper, we mainly
focus on the detection of Type1 to Type4 leaks.

2.2 Route leaks detection

The Internet Routing Registries (IRRs) are public distributed
databases that allow ASes to register and share their routing
policies. Using information in IRR, ASes can detect route
leaks that violate routing policies. However, IRR databases
are managed by different Regional Internet Registries (RIRs)
or private organizations [23], which pose challenges for
information synchronization and consistency.

Route-leak Protection (RLP) [16] adds a new BGP com-
munity attribute named Down-only to the BGP update
message for detecting and mitigating route leaks. When
receiving a new route, ASes will check the value in the
Down-only community of the route to decide whether to
forward it to upstream providers/peers or not. If the route
comes from the customer or peer ASes and the community
shows no forwarding to upstream ASes, then the route is
marked as route leak. However, the performance of RLP can
be easily affected by misconﬁguration of BGP communities
and maliciously modiﬁcations or discarding of Down-only
communities during route propagation [24].

S. Li et al. [5] conclude a relationship between route
loops and route leaks from analyzing local routing infor-
mation and develop an algorithm for detecting route leaks.
However, only some route leaks may cause route loops.
Therefore, it can only detect part of route leaks. Similar
to [5], M. Siddiqui et al. [25] also develop a theoretical
framework for detecting route leaks. However, the detection
method is valid in route leak initiations and is not suitable
for detecting route leak propagation [25].

Autonomous System Provider Authorization (ASPA)
[26] is based on the RPKI and adds a new type of object
to the RPKI repository. The new object includes the pair of
downstream AS and authorized upstream AS. The autho-
rized upstream AS is allowed to propagate the downstream
AS’s announcements. By validating the certiﬁcates of pairs,
ASes can detect route leaks. However, it cannot validate

3

Fig. 1: Route leak examples of Type1 to Type4 route leaks: a) Type1: AS4 leaks route P learned from its provider AS1 to its
another provider AS2; b) Type2: AS2 leaks route P learned from its peer AS1 to its another peer AS5; c) Type3: AS4 leaks
route P learned from its provider AS1 to its peer AS6; d) Type4: AS2 leaks route P learned from its peer AS1 to its provider
AS3. The p2p represents peer-to-peer relationship and c2p represents customer-to-provider relationship

complex relationships like mutual transit, where ASes pro-
vide transit service to each other [5].

There are studies [7], [14], [15] based on blockchain to
prevent route leaks. In the blockchain, ASes share their
relationships and store them in blocks. However, it also
direct expose conﬁdential AS business relationship, which
inﬂuences incentive of deployment. J. Yue et al. [27] consider
the privacy of AS policy when using blockchain for route
leak detection. They use Trusted Execution Environment
(TEE) [28] to implement the privacy protection. However,
it requires each chain node to maintain a global conﬁdential
and tamper-proof routing policy repository, which adds the
risk of routing policy leakage if there is a chain node that
acts maliciously. Moreover, since TEE uses a combination of
hardware and software for protecting the data privacy, it is
affected by hardware vulnerabilities and its updates require
hardware update.

In this paper, we propose a method for route leak de-
tection using blockchain-based federated learning, which
considers the privacy protection of AS business relationship
information and uses AS triples instead of AS business rela-
tionships to train models. The proposed method keeps the
data local and shares only model updates, which lowers the
risk of data leakage. With the help of blockchain technique,
the proposed method can audit and track data. Even if one
AS acts maliciously and leaks the updates, it does not leak
the direct business relationship information. Besides, since
the method uses AS triples, the complex relationships such
as mutual transit can also be handled.

3 METHODOLOGY

In this section, we ﬁrst give a description about the route
leak problem in inter-domain networks (Section 3.1) and an
overview of the proposed framework to solve the problem
(Section 3.2). Following that, we describe the data process-
ing details of how to generate local training data using
known local routing policies (Section 3.3). Then, we intro-
duce the model sharing and storing during the blockchain
(Section 3.4), and discuss the total theoretical cost of a train-
ing task (Section 3.5). Last, an analysis of factors affecting the
deployment of the proposed route leak detection method is
presented (Section 3.6).

3.1 Problem description

As introduced in Section 2.1, we can conclude that the route
learned from one provider or peer cannot be exported to
another provider or peer. For example, as illustrated in
Fig.1, AS4 leaks the route learned from its provider AS1
to its another provider AS2 due to misconﬁguration. The
AS2 then forwards the route to its other neighbors. For AS2
and its peer AS5, they only know the business relationship
between their direct neighbors and themselves, but they do
not know the accurate business relationship between AS1
and AS4, which makes it difﬁcult for both AS2 and AS5
to identify the leak route. If AS4 shares its relationship
with AS1 to AS2 or AS5, at least one of them can detect
the route leak and stop propagating the malicious route. In
other words, the challenge of route leak detection is to get as
accurate AS business relationships of other ASes as possible.
The inter-domain network is a distributed but coop-
erative system where each ASes exchange routes, which
indicates that AS business relationships of an individual
AS cannot be hidden completely in the real network [29].
Even though some business relationships have already been
exposed due to public BGP updates, there is still a hesitation
from ASes to announce business relationship information
proactively because of business competition. Therefore, in
order to facilitate the deployment of detection systems, we
consider increasing the difﬁculty of obtaining information
about relationships from the outside to protect privacy.
We do not require ASes to directly expose their business
relationships. Instead, we use AS triple [5] (vi−1, vi, vi+1)
with a malicious or regular label to hidden AS business
relationships, where vi−1 and vi+1 are both direct neighbors
of AS vi. That is, if the triple (vi−1, vi, vi+1) violates the
routing policy, then it is labeled as malicious. Thus, the
route leak detection problem is transformed into sharing AS
triples with labels as much as possible.

To solve the problem deﬁned above, we propose a
method called FL-RLD to share AS triples using blockchain-
based federated learning framework, where blockchain is
used to provide a secure system for federated learning [30].
In comparison with using a distributed repository to store
these AS triples, federated learning provides lower risk
of privacy leakage because it allows keeping private data

p2pc2pNormal updateMalicious update123546P {1}P {1}Leak P {1,4}Leak P {1,4,2}Leaking AS123546P {1}P {1}Leak P {1,2}Leaking AS123546P {1}P {1}Leak P {1,4}Leaking AS123546P {1}P {1}Leak P {1,2}Leaking AS(a) Type1(b) Type2(d) Type4(c) Type3Normal ASLeaking AS4

Fig. 2: Overview of the solution framework.

locally and uploading the updates of trained local models
instead of AS triples. As a result, we further transform the
problem from sharing AS triples into sharing local model
updates.

3.2 Overview of solution framework

The solution framework is shown in Fig.2. In the frame-
work, each AS deploys a blockchain node called ASChain
Manager. Assuming most ASes are honest but curious. AS-
chain Manager is responsible for managing the blockchain
network and federated learning tasks. As shown in Fig.2,
each ASchain Manager has at least two stages in one task.
The ﬁrst phase is to train its local model with local train-
ing data and upload updates of the local model to the
blockchain network. The local training data is generated by

local routing policies and the details about the generation
will be introduced in Section 3.3. The second phase is to
aggregate the received updates to a global model update,
and upload the aggregated updates to make consensus.
After the consensus, the winner of the consensus committee
has the third phase, which is to store the global model
update in the blockchain. In light of numerous studies
[19], [31], [32] have made achievements on the consensus
mechanism and other security issues of blockchain-based
federated frameworks, we move our focus on customizing
the framework for learning inter-domain routing policies.

The speciﬁc training process is illustrated in Fig.3. In
the initial state, assuming that an authorized organization
publishes a learning task in the blockchain network and
each ASchain Manager in the network can obtain all re-

Blockchain NetworkTrainingAggregatingTrainingAggregatingTrainingAggregatingAM 2AM 1AM 3AM 4Routing Information Base(cid:1153)(cid:53)(cid:44)(cid:37)(cid:1154)    (cid:335)(cid:335)…(cid:1153)(cid:69)(cid:19)(cid:15)(cid:69)(cid:20)(cid:15)(cid:335)(cid:15)(cid:69)(cid:80)(cid:1154)X2Y2X1(a0,a1,…,an)Y1PathNextHopPreﬁx……(v, bm)Peer-to-PeerProvider-to-Customer(v, an)RelathionshipAS LinkAS business relationships……(bm, v, am)Malicious❌Regular✔(an, v, bm)LabelAS TripleLocal training dataLocal routing  policesData processingAggregatingLocal training  dataModelLocal updateLocal routing policiesLocal updatesGlobal updateAggregatingASChain Manager (AM)Winner from the consensus  committee  Exchange local updatesConsensus LGloabl updateLocal updateModelLocal routing policesBlockchainLocal  training dataFinal gloabl updateLocal training dataModelLocal updateLocal routing policiesFinal Global updateBlockchainStoring global update in the blockLocal updatesGlobal updateAggregatingTrainingAggregatingStoringTrainingAlgorithm 1 Data Processing
1: Input: deployed AS m, RIB Sm, neighbors Nm, business

relationship Rm.

5

for ∀nj ∈ Nm do
if ni = nj then
Continue

2: Output: Training data Dm
3: Dm ← ∅
4: for ∀ni ∈ Nm do
5:
6:
7:
8:
9:
10:
11:
12:

end if
// Direct triples
if Rm(m, ni) = c2p or Rm(m, ni) = p2p then

if Rm(m, nj) = c2p or Rm(m, nj) = p2p then

/* AS triple (ni, m, nj) is labeled as malicious
*/
Dm ← Dm ∪ {(ni, m, nj, 0)}

end if
/* AS triple (ni, m, nj) is labeled as regular */
Dm ← Dm ∪ {(ni, m, nj, 1)}
// Inference triples
if link (ni, nj) in Sm then

Dm ← Dm ∪ {(ni, nj, m, 1)}
/* Reverse triple pattern */
Dm ← Dm ∪ {(m, nj, ni, 1)}

Dm ← Dm ∪ {(ni, nj, m, 0)}

end if

13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:
24:
25:
26:
27: end for

else

end if
end for

neighbors of vi. The ω is the label of direct AS triple
(vi−1, vi, vi+1). In a inferred sample (vi+1, vi−1, vi, ω) of AS
vi, the possible leaker is vi−1, and the vi+1, vi−1 are both
direct neighbors of vi. The ω is the label of inferred AS triple
(vi+1, vi−1, vi). The direct samples are based on the known
business relationship with neighbors. The inferred samples
are based on the permutations of known neighbors and are
used to broaden the vision of AS vi, because direct samples
can only include the scenarios about the one-hop vision of
vi and the inferred samples contain part of two-hop vision.
The main difference of the two types of triples is that in
the direct triples the possible leaker is AS itself vi and in
the inference triples the possible leaker is the neighbor of
vi. Details about the process for labeling AS triples are
described below.

Triple Labeling

1)

For each triple (vi−1, vi, vi+1) in the direct triples
DiT of vi:

a)

If vi is a customer or peer of vi−1, and vi+1
is the provider or peer of vi, then the triple
breaks the valley-free rule and the label ω for
(vi−1, vi, vi+1) is set as malicious.

b) Otherwise, the label ω for (vi−1, vi, vi+1) is

set as regular.

2)

For each triple (vi+1, vi−1, vi) in the inference triples
InT of vi:

Fig. 3: Overview of training process where AM represents
ASchain Manager.

quirements of the learning task, such as initial training
model and training epoch. (step (cid:172)) When the task starts,
ASchain Managers retrieve the task information from the
blockchain. (step (cid:173)) ASchain Managers use local training
data to train their local models. The local training data is
composed of AS triples with labels generated using local
routing policies. (step (cid:174)) In each global training epoch,
the ASchain Manager encrypts the update of a local model
and propagates it to the network. Other ASchain Manager
decrypt and verify the received local update. (step (cid:175)) After
all local updates are veriﬁed, the ASchain Manager begins to
aggregate these local updates to a global model update and
(step (cid:176)) propagates it to the network to make consensus
with others. (step (cid:177)) The winner of the consensus generate
a new block and store the ﬁnal global update to blockchain.
(step (cid:178)) If the training is not ﬁnished, the ASchain Manager
updates the local model using the agreed global model
update and repeats the step (cid:173) to (cid:177).

3.3 Data processing

In the next step, we process the original
local data of
ASes to training data that consists of AS triples and labels
indicating whether they are malicious or not. The labels are
generated according to local routing policies, which include
AS business relationships and stable Routing Information
Base (RIB) of AS. The RIB contains the information about
the selected routes. The combination of an AS triple and
its label is deﬁned as a sample. Therefore, for each AS, the
training data can be divided into two components, direct
samples based on relationship information and inferred
samples based on stable RIB.

Please note that in practice, in addition to the valley-free
rule, other routing strategies can also be used to label AS
triples. For instance, for an AS triple (a, b, c) where a is a
customer of b and b is a customer of c, AS b may not allow
the route learned from a to export to AS c even it does
not break valley-free rule. However, as the conﬁdentiality
of other individual routing policies and common use of
the valley-free rule, we mainly consider labeling AS triples
based on valley-free rule in the experiment.

In a direct sample (vi−1, vi, vi+1, ω) of AS vi,
the
possible leaker is vi, and the vi−1, vi+1 are both direct

…AM 1AM 2AM NStep 1 obtain task informationStep 2 local trainingStep 3 exchange local updatesStep 4 aggregate received local updates to a global updateStep 5 make consensus of  aggregated global updateStep 6 generate a new block andstore the ﬁnal global update to blockchainStep 7 if the training is not ﬁnished,  all AMs can download new global model update to update their local models and repeat step 2-6.Blockchain1234576a)

If the link (vi+1, vi−1) never appears in the
local stable RIB of vi, then the label ω for
(vi+1, vi−1, vi) is set as malicious.

b) Otherwise, the label ω for (vi+1, vi−1, vi) is

set as regular

After the above generation, each AS can obtain a variety
of triples including malicious and regular ones. In order
to further enrich the triples for training, we conclude a
relation pattern about the labels of triples and labels of their
reverse triples, which can be summarized as follows.

6

a transaction of
the block in the blockchain. For in-
stance, the kth global training results can be deﬁned as
Yk = [Hash(Yk−1), Ts, Texp, Γ, Mk, SIGx(Yk), x], where
Hash(Yk−1) is the hash of previous block. Blocks in the
blockchain are stored in a chain structure. The Ts and Texp
are the block generation time and expired time respectively.
The Γ is the global model update. The SIGx(Yk) is the
block signature of the winner participant x and Mk records
the ASN of participants. Therefore, once a global epoch
is ﬁnished, participants can download the global model
update from the blockchain to update their local models.

Reverse Triple Pattern. For each triple (vi+1, vi−1, vi)
in inference triples InT of vi:

1)

If the link (vi+1, vi−1) appears in the stable RIB:

Algorithm 2 FL-RLD Training

1: Input: deployed ASes M , model Q, local training epoch

ce, global training epoch e

a)

b)

if the triple’s label is regular, then the reverse
triple (vi, vi−1, vi+1) of (vi+1, vi−1, vi) is la-
beled as regular.
if the triple’s label is malicious and vi−1 is a
peer or customer of vi, then the reverse triple
(vi, vi−1, vi+1) of (vi+1, vi−1, vi) is labeled as
malicious.

2) Otherwise, break.

Using the reverse triple pattern, we extend more triples
from the initial generated triples. These triples and their la-
bels are combined as local training data for federated learn-
ing. For a clear expression, the use of the Triple Labeling
and Triple reverse pattern are summarized in Algorithm 1

3.4 Model sharing and storing during the blockchain

In the solution framework, excepet for avoiding single-point
failure in traditional federated learning, the blockchain is
used to share and store model updates, such as securely
exchanging information between participants and auditing
uploaded data.

Model sharing: For example, AS participants in the
blockchain can be authorized a public/private key pair (or
a set of key paris) with their Autonomous System Number
(ASN) by authorities (i.e., RIPE NCC, APNIC or large ISPs
) and use the key pair to sign/validate the updates. In
this way, the framework can verify the identities of AS
participants that generated the updates. To guarantee the
security of the transmitted updates, HTTPS connections can
be built between two participants to prevent attackers ac-
cessing the updates [33]. For meeting privacy requirements,
many methods are proposed for the blockchain-based feder-
ated learning framework, such as [34] use threshold paillier
algorithm, [19] use Shamir’s secret sharing scheme, and [35]
use differential privacy.

Model storing: Through the non-tamperable feature of
blockchain, we can track/validate the uploading records
and ensure the integrity of records. The initial block
records the initial training model and task information.
The consensus procedure is used to ensure that
the
global model update obtained by aggregation is unique,
which can be implemented by Proof of Work (PoW)
or Proof of Stake (PoS). After the consensus procedure,
the results of each global training epoch are stored as

2: Output: global model update Γ
3: for k = 1 to ge do
4:
5:

for ∀m ∈ M in Parallel do

Obtaining local training data Dm using algorithm 1

6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:

local model Q0
for i = 1 to ce do

m ← Q

m ← modelTraining(Qi−1

/* Training model */
Qi
/* Obtaining local model update */
m ← modelGetUpdate(Qi−1
γi
m)
end for
Uploading local model update γc
m

m , Dm)

m , Qi

end for
for ∀m ∈ M in Parallel do

m |∀m ∈ M })

/* Aggregating all local updates */
Γk
m ← modelAggregate({γce
end for
/* Making consensus and obtaining global model
update */
Γ ← consensus({Γk
Storing Γ in the blockchain

m|∀m ∈ M })

20:
21:
22: end for

3.5 Theoretical cost analysis

Combining the training process in Fig.3 and training algo-
rithm shown in Algorithm 2, the total cost of a training task
can be divided into three parts: local computation cost (step
(cid:172), (cid:173), (cid:175)), global communication cost (step (cid:174), (cid:176)) and storing
cost (step (cid:178)).

Local computation cost: Let Li(Dm) be the local training
cost of the participant m ∈ M in ith local training epoch
where Dm is the local training data of m and ak be the
model aggregation cost in kth global training. So, the local
computation cost is (cid:80)ce
i Li(Dm) + ak where ce is the local
training epochs, Dm is the local dataset of m.

Global communication cost: The global communication
cost includes two parts, exchanging local updates and mak-
ing consensus about aggregated global updates. First, let
δk
m be the local update of m in the kth global training epoch.
Assuming the communication cost between any two partici-
pants for m is ∆(δk
m). Thus, the cost of broadcasting the local
model update for m is deﬁned as (|M |−1)∆(δk
m) where |M |

is the number of participants. Then, let ζ k(|M |) be the con-
sensus cost of kth global epoch. Finally, the total global com-
munication cost is (cid:80)ge
m) + ζ k(|M |))
k
where ge is the global training epochs.

m∈M ((|M | − 1)∆(δk

(cid:80)

Storing cost: The global updates in each global training
epoch are stored in blocks. The storing cost is related to
the size of model updates, so the storing cost is (cid:80)ge
k f (Γk)
where the Γk is the global model update in kth global
training epoch.

Therefore, the total cost of a training task can be pre-

sented as following:

total cost =

ge
(cid:88)

(cid:88)

Local computation cost
(cid:125)(cid:124)

(cid:123)
Li(Dm)) + ak

(cid:122)
ce
(cid:88)
(
(

k

m∈M

i

7

vh+1 ∈ M and AS triple (vh−1, vh, vh+1) in D, then AS
vk+1 can identify that R is malicious.

When θ = 1, the global model can identify every AS
triple in D is malicious or not. So, once AS v ∈ M receives
an announcement with a malicious AS triple in D, it will
correctly detect the malicious announcement. If θ < 1, then
the number of malicious AS triples that the global model
can identify is θ · |D|.

Therefore, it can be concluded that the performance of
route leak detection depends on two parts, accuracy of the
global model and the number of malicious AS triples. Under
the same accuracy of the global model, more route leaks
will be detected as the number of malicious AS triples in D
increases.

4 EXPERIMENTS AND ANALYSIS

(1)

+

Exchanging update cost
(cid:122)
(cid:123)
(cid:125)(cid:124)
(|M | − 1)∆(δk
m) +
(cid:123)(cid:122)
(cid:124)
Global communication cost

Consensus cost
(cid:122) (cid:125)(cid:124) (cid:123)
ζ k(|M |)

(cid:125)

+ f (Γk)
(cid:124) (cid:123)(cid:122) (cid:125)
Storing cost

)

The parameters used in the Equation (1) are summarized in
Table 1.

TABLE 1: List of notations used in Equation (1)

Parameter Meaning

ge
ce
M, |M |
Dm
Li(Dm)
ak
δk
m
∆(δk
m)
ζk(|M |)
Γk
f (Γk)

Global training epochs
Local training epochs
Deployed AS set, the number of M
Training data of deployed AS m
Training cost of m in ith local training epoch
Aggregation cost in kth global training epoch
Local model update of m in kth global training epoch
Cost of broadcasting δk
m between any two participants
Consensus cost
Global model update in k the global training epoch
Cost of storing global model update in the blockchain

3.6 Factors affecting deployment

Here, to promote the deployment, we discuss which fac-
tors can affect the detection effectiveness of the proposed
method. The Internet is modeled as a graph G(V, E) where
V is a set of all ASes in the Internet and E represents the
direct links between ASes. In the graph, M is a set of ASes
that have deployed the proposed detection system, where
M ⊂ V . For each AS v ∈ M , the local training data of m
is represented as Dv where |Dv| ≥ 0. Thus, the shared local
training data of M can be deﬁned as D = {Dv|∀v ∈ M } and
is used to federally train a global model to identify the input
AS triple is malicious or regular. Thus, the aim of the global
model is to memorize the mappings of AS triples and their
label of local training data as much as possible. The accuracy
that the global model’s can correctly identify a malicious AS
triple in D is set as θ, where 0 ≤ θ ≤ 1.
Lemma 1. If θ = 1, consider AS vk+1 ∈ M receives a leaking
announcement R = {v1, ..., vh, ..., vk} and the leaking
AS is vh. If vh ∈ M , then the AS triple (vh−1, vh, vh+1)
and its label are in D. Therefore, AS vk+1 can identify
that R is a malicious announcement. For vh /∈ M , if

In this section, we give a description about experiment setup
and show the experiment results. To learn more about the
possible route leaks, we ﬁrst explore the features regarding
generated triples, such as the proportion of malicious triples
and regular triples, and how the two types of triples have
changed over the past four years. Then, experiments are
conducted to evaluate the performance of the proposed
detection method.

4.1 Experiment setup

Topology: The BGP topology data used in the evaluation
is collected from the CAIDA January 2021 AS relationship
dataset [36] of IPv6, which has 12,721 ASes and 173,462
AS links. We use the method introduced in Section 3.3 to
generate triples and their labels for ASes in the network.

Implementation details: The proposed method is imple-
mented by Python and Keras [37], and the initial training
model is a simple LSTM network. It includes a LSTM layer
with input size (1, 96) and output size (1, 128), a hidden layer
with output size (1, 64) and ReLU [38] activation function,
and an output layer with output size (1, 2) and Softmax
activation function. For each 2-bit vector of the output layer,
if the ﬁrst bit is larger than the second one, then it is
predicted as a regular triple. Otherwise it is predicted as a
malicious triple. The model uses Adam optimizer [39] as the
optimization process and the batch size is 32. The learning
rate of the model is set as 0.001 and the FedAvg algorithm
[40] is used to aggregate local updates. In the training data,
each ASN in the generated triples is embedded as a 32-bit
vector by converting decimal to binary. The local training
epoch is set as 2 and the global training epoch is around 70
to 100.

Training data details: In our experiments, we con-
sider two aspects of local training data that may inﬂu-
ence the results, balanced/unbalanced data size and bal-
anced/unbalanced class distribution. We select 4 groups
of federated learning participants to test. Each group has
5 participants. We represent each participant of a group
with Client1, Client2, Client3, Client4, and Client5. The
details about the groups are illustrated in Table 2. The
data size refers to the number of triples of the participant.
For example, in group 1, participants have different sizes
of local training data, and the number of malicious and

TABLE 2: The triple distribution of different groups

Data size Anomaly

Regular Anomaly % Regular %

8

Group 1 (unbalanced data size + unbalanced class distribution)
Client1 (51.19%)
Client2 (30.92%)
Client3 (0.51%)
Client4 (14.18%)
Client5 (3.20%)
Group 2 (balanced data size + unbalanced class distribution)
Client1 (19.77%)
Client2 (20.69%)
Client3 (19.25%)
Client4 (19.49%)
Client5 (20.79%)
Group 3 (unbalanced data size + balanced class distribution)
Client1 (8.58%)
Client2 (35.93%)
Client3 (43.45%)
Client4 (10.40%)
Client5 (1.64%)
Group 4 (balanced data size + balanced class distribution)
Client1 (20%)
Client2 (20%)
Client3 (20%)
Client4 (20%)
Client5 (20%)

13550
6936
4189
69
1922
434
63468
12549
13134
12218
12369
13198
416348
35712
149580
180904
43316
6836
17090
3418
3418
3418
3418
3418

12224
6192
3913
33
1680
406
51066
12099
12158
7606
10205
8998
208174
17856
74790
90452
21658
3418
8512
1761
1672
1724
1679
1676

1326
744
276
36
242
28
12402
450
976
4612
2164
4200
208174
17856
74790
90452
21658
3418
8578
1657
1746
1694
1739
1742

90.21%
89.27%
93.41%
47.83%
87.41%
93.55%
80.46%
96.41%
92.57%
62.25%
82.51%
68.18%
50.00%
50.00%
50.00%
50.00%
50.00%
50.00%
49.81%
51.52%
48.92%
50.44%
49.12%
49.04%

9.79%
10.73%
6.59%
52.17%
12.59%
6.45%
19.54%
3.59%
7.43%
37.75%
17.50%
31.82%
50.00%
50.00%
50.00%
50.00%
50.00%
50.00%
50.19%
48.48%
51.08%
49.56%
50.88%
50.97%

regular triples are not equal. In group 3, participants have
an equal number of malicious and regular triples but their
total number of triples are different. We run tests on the
above four groups. For a clear description, we use Dg,c to
denote the local training data of participant c in the group
g. For instance, the local training data of Client1 in Group
1 is D1,1. In each experiment, all local training data from a
group is used to test the trained model.

Evaluation metric: Four standard metrics, Accuracy, Pre-
cision, Recall, F1score are used as evaluation metrics for
detection performance. Accuracy shows the ratio of correctly
predicted triples to the total triples. Precision and Recall
display the ratio of correctly predicted malicious triples and
regular triples respectively. F1score is the average of Precision
and Recall. Their deﬁnitions are as follows.

Accuracy =

T P + T N
T P + F P + T N + F N

P recision =

T P
T P + F P

Recall =

T P
T P + F N

F 1score = 2

P recision ∗ Recall
P recision + Recall

(2)

(3)

(4)

(5)

where True Positives (TP) and False Positives (FP) are the
number of true malicious triples that the model predicts
as anomaly and regular respectively. The True Negatives
(TN) and False Positives (FP) are the number of true regular
triples that the model predicts as regular and anomaly
respectively.

Comparative methods: We use FL-RLD to represent the
proposed method, and CL to represent the central learning
method. The difference between CL and FL-RLD is that CL
transports all local training data of participants of a group to
a single server for model training while FL-RLD keeps the
data training local. The C1, C2, C3, C4, and C5 represent
the single AS learning method with participant Client1,

Client2, Client3, Client4, and Client5 respectively. So, the
single learning method can only utilize the local training
data of a single AS to train the model. For example, the
training data of C1 in group 1 uses the local training data
of corresponding AS is D1,1 and the training data of CL is
{D1,i|i = 1, 2, ..., 5}. The training model of FL-RLD, CL and
C1/C2/C3/C4 are all the same.

Traditional detection methods work by storing AS busi-
ness relationships in various ways and ﬁltering out routes
that aren’t matched (i.e., building a RPKI-like repository to
store the routing customer-provider authority objects [26],
marking the ”Down-only” routes [16] to prevent forwarding
routes to upstream providers or peers). Hence, we mod-
eled three different methods ML-random, ML-0, ML-1 based
on the above analysis to compare with FL-RLD. In these
three methods, they all build a global repository that all
participants of a group directly share their AS relationships.
Ideally, if all relationship information of the AS triple in test
data is in the global repository, output the correct result.
Otherwise, they respond differently: 1) ML-random will
randomly output a result. 2) ML- 0 will mark this AS triple
as malicious. 3) ML-1 will mark this AS triple as regular.

4.2 Performance

Multiple ASes vs. Single AS: First, a comparison of a global
model trained by multiple ASes and a model trained by a
single AS is carried out. The results are shown in Fig.4. The
Fig.4a and Fig.4b are results under datasets with unbalanced
class distribution, while Fig.4c and Fig.4d are results under
datasets with balanced class distribution. In Fig.4, FL-RLD
performs better than C1, C2, C3, C4 and C5 in all evaluation
metrics under different groups of datasets, which provides
an incentive for ASes to join federated learning. For exam-
ple, in Fig.4b, the Accuracy of C1 to C5 are all lower than 0.8
but when they join the federated learning, the Accuracy is
more than 0.95. The results also show that the difference of
FL-RLD and CL in the performance is small (e.g., less than
0.06 Accuracy in group 1).

9

(a) Unbalanced data size + un-
balanced class distribution

(b) Data size balance + unbal-
anced class distribution

(c) Unbalanced data size + bal-
anced class distribution

(d) Data size balance + bal-
anced class distribution

Fig. 4: The Performance of FL-RLD method compared with single AS learning method (C1, C2, C3, C4, C5) and Central
Learning (CL) method

(a) Anomaly 90.21% vs Regular
9.79%

(b) Anomaly 80.46% vs Regu-
lar 19.54%

(c) Anomaly 50% vs Regular
50%

(d) Anomaly 49.81% vs Regu-
lar 50.19%

Fig. 5: The performance comparison of FL-RLD and other methods.

Global repository vs. FL-RLD: In Fig.5, we compare
FL-RLD with ML-random, ML-0 and ML-1, where these
three comparative methods are based on sharing a global
repository of business relationships. As introduced previ-
ously, the main difference of these three methods is that they
respond differently when the AS relationship information is
not in the training data. The results in Fig.5 show that the
performance of FL-RLD and ML-0 is better than others and
FL-RLD performs better on average than ML-0. The ML-
1 performs the worst because the number of triples marked
as malicious are higher than that of regular triples, while the
ML-1 classiﬁes all unknown triples as regular, thus making
the Recall low. The ML-0 classiﬁes all unknown triples as
malicious, so its performance is better than ML-1. However,
the Precision of ML-1 is low, which makes high false alarms.
Thus, the FL-RLD that performs well on both Precision and
Recall is more recommended.

4.3 Deployment analysis

As analyzed in Section 3.6, the higher number of malicious
AS triples are contained in the training data, the more route
leaks can be detected by FL-RLD. Thus, to facilitate the
deployment, we ﬁrst analyze the distribution of AS triples
over time and study the relationship between the number
of deployed ASes and the number of route leaks that can
be detected under different deployment strategies if the
accuracy θ of the global model is 1.

Malicious AS triples vs. Regular AS triples: Except for
the 2021 topology used above, we also collect other three
topologies, 2020, 2019, 2018, to study the distribution of AS
triples over time. They are also from CAIDA AS relationship
dataset of IPv6. Fig.6 shows the proportion of malicious

and regular triples of the local training data under different
topology data grouped by year. As we can see, the results
show that the number of malicious triples (around 60%-
70%) are more than regular triples (around 30%-40%) in
the local training data. This is because the number of peer-
to-peer relationships are more than provider-to-customer
relationships, which makes extensive possible route leaks.
Please note that even the number of malicious AS triples are
higher than regular AS triples, but actually most of ASes in
the network act normally and only a small number of ASes
act maliciously. Therefore, there are not so many malicious
AS triples in the actual routing announcements.

Direct AS triples vs. Inference AS triples: Fig.6b depicts
CDFs of the percentage of inference triples to total triples.
In the about 80% ASes, around 30% triples are inference
triples where neighbors are the possible leakers. The data
of the past four years from 2018 to 2021 show a similar
result. It indicates that our generation method can enrich
training data well by extending properly proportioned AS
triples based on the known routing policies.

Deployment strategies: Fig.7 shows the distribution
of malicious triples in the training data to total mali-
cious triples under different deployment strategies, where
Peer/Customer/Provider selects ASes with the largest num-
ber of peers/customers/providers to deploy ﬁrst. The re-
sults show that Peer deployment strategy can cover the
most number of malicious triples than other two strategies
with the same deployment rate. For example, in the data
of 2021, if using Peer deployment strategy, it only needs to
deploy 875 ASes (0.06878 deployment rate) to cover 99%
total malicious triples while Customer deployment strategy
can only reach 72% coverage rate with the same deployment
rate. It also indicates that ASes with a large number of peers

C1C2C3C4C5FL-RLDCL0.40.60.81.0Group 1AccuracyPrecisionRecallF1scoreC1C2C3C4C5FL-RLDCL0.40.60.81.0Group 2AccuracyPrecisionRecallF1scoreC1C2C3C4C5FL-RLDCL0.20.40.60.81.0Group 3AccuracyPrecisionRecallF1scoreC1C2C3C4C5FL-RLDCL0.40.60.81.0Group 4AccuracyPrecisionRecallF1scoreFL-RLDML-randomML-0ML-10.000.250.500.751.001.251.50Group 1AccuracyPrecisionRecallF1scoreFL-RLDML-randomML-0ML-10.00.20.40.60.81.01.21.4Group 2AccuracyPrecisionRecallF1scoreFL-RLDML-randomML-0ML-10.00.20.40.60.81.01.21.4Group 3AccuracyPrecisionRecallF1scoreFL-RLDML-randomML-0ML-10.00.20.40.60.81.01.21.4Group 4AccuracyPrecisionRecallF1score10

Fig. 7: The proportion of malicious triples in the training
data to total malicious triples in the network using different
deployment strategies (Peer, Customer, and Provider).

more on route leak detection if they join the federated
learning.

ACKNOWLEDGEMENT
This work was supported by the National Key R&D Pro-
gram of China (No. 2018YFB1800404).

REFERENCES

[1] M. Luckie, B. Huffaker, A. Dhamdhere, V. Giotsas, and K. Claffy,
“As relationships, customer cones, and validation,” in Proceedings
of the 2013 conference on Internet measurement conference, 2013, pp.
243–256.

[2] L. Gao, “On inferring autonomous system relationships in the
internet,” IEEE/ACM Transactions on networking, vol. 9, no. 6, pp.
733–745, 2001.

[3] P. Gill, M. Schapira, and S. Goldberg, “A survey of interdomain
routing policies,” ACM SIGCOMM Computer Communication Re-
view, vol. 44, no. 1, pp. 28–34, 2013.

[4] R. Anwar, H. Niaz, D. Choffnes, ´I. Cunha, P. Gill, and E. Katz-
Bassett, “Investigating interdomain routing policies in the wild,”
in Proceedings of the 2015 Internet Measurement Conference, 2015, pp.
71–77.
S. Li, H. Duan, Z. Wang, and X. Li, “Route leaks identiﬁcation by
detecting routing loops,” in International Conference on Security and
Privacy in Communication Systems. Springer, 2015, pp. 313–329.

[5]

[6] K. Sriram, D. Montgomery, D. McPherson, E. Osterweil, and
B. Dickson, “Problem deﬁnition and classiﬁcation of bgp route
leaks,” RFC 7908, 2016.

[7] M. F. Galm´es, R. C. Aumatell, A. Cabellos-Aparicio, S. Ren,
X. Wei, and B. Liu, “Preventing route leaks using a decentralized
approach: An experimental evaluation,” in 2020 IEEE 28th Inter-
national Conference on Network Protocols (ICNP).
IEEE, 2020, pp.
1–6.

[8] C. Hepner and E. Zmijewski, “Defending against bgp man-in-the-

middle attacks,” Talk at BlackHat, vol. 2009, 2009.

[9] D. Madory, “Routing leak brieﬂy takes down google,” Online.

https://blogs.oracle.com/internetintelligence/routing-leak-brieﬂy-takes-
down-google, 2015.
Siddiqui,

as28548,” Online.

“Major

route

leak

by

[10] A.

https://www.manrs.org/2021/02/major-route-leak-by-as28548-another-
bgp-optimizer/, 2021.

(a) Distribution of proportion of malicious and regular
generated triples of local training data under different
years. Please note the results are distribution of possible triples
and are not distribution of triples appeared in the actual routing
announcements.

(b) CDF of the percentage of inference triples to total
triples in different years.

Fig. 6: Distribution of triples in different years.

have a large number of possible malicious triples.

5 CONCLUSION

This paper studied the problem of route leak detection in the
inter-domain network and proposed a privacy-preserving
method using blockchain-based federated learning to col-
laboratively train models with accurate routing policy infor-
mation for route leak detection. Compared with traditional
route leak detection methods, the proposed method consid-
ers the privacy of AS business relationships. To solve the
lack of a ground truth problem in route leaks, it provides
a self-validation scheme by labeling AS triples as mali-
cious or regular using local routing policies. The evaluation
results show that the proposed method can improve the
performance of a single AS in detecting route leaks, and
there is slight differences in results (e.g., around 0.06 in
Accuracy) between the federated learning method and the
central learning method, in which all local training data are
gathered and trained together. In the analysis of FL-RLD
deployment, it is found that AS with more peers are more
likely to have more possible route leaks and can contribute

2018201920202021Years0.00.20.40.60.81.0Proportion0.5960.690.650.6550.4040.310.350.345Malicious AS triplesRegular AS triples0.00.20.40.60.81.0Percentage of inference triplets0.00.20.40.60.81.0Proportion20212020201920180.000.250.500.751.00Deployment rate0.00.20.40.60.81.02021PeerCustomerProvider0.000.250.500.751.00Deployment rate0.00.20.40.60.81.02020PeerCustomerProvider0.000.250.500.751.00Deployment rate0.00.20.40.60.81.02019PeerCustomerProvider0.000.250.500.751.00Deployment rate0.00.20.40.60.81.0Anomalous triplets coverage rate2018PeerCustomerProvider11

blockchain,” in 2020 Second International Conference on Blockchain
Computing and Applications (BCCA).

IEEE, 2020, pp. 140–146.

[33] Q. Zhang, P. Palacharla, M. Sekiya, J. Suga, and T. Katagiri, “A
blockchain based protocol for federated learning,” in 2020 IEEE
28th International Conference on Network Protocols (ICNP).
IEEE,
2020, pp. 1–2.

[34] J. Weng, J. Weng, J. Zhang, M. Li, Y. Zhang, and W. Luo,
“Deepchain: Auditable and privacy-preserving deep learning with
blockchain-based incentive,” IEEE Transactions on Dependable and
Secure Computing, 2019.

[35] X. Chen, J. Ji, C. Luo, W. Liao, and P. Li, “When machine learning
meets blockchain: A decentralized, privacy-preserving and secure
design,” in 2018 IEEE International Conference on Big Data (Big
Data).

IEEE, 2018, pp. 1178–1187.

[36] CAIDA,

“As

relationship

dataset,”

Online.

http://www.caida.org/data/ as-relationships/, 2021.

[37] N. Ketkar, “Introduction to keras,” in Deep learning with Python.

Springer, 2017, pp. 97–111.

[38] V. Nair and G. E. Hinton, “Rectiﬁed linear units improve restricted

boltzmann machines,” in Icml, 2010.

[39] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimiza-

tion,” arXiv preprint arXiv:1412.6980, 2014.

[40] B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Ar-
cas, “Communication-efﬁcient learning of deep networks from
decentralized data,” in Artiﬁcial intelligence and statistics. PMLR,
2017, pp. 1273–1282.

[11] Y. Jin, C. Scott, A. Dhamdhere, V. Giotsas, A. Krishnamurthy, and
S. Shenker, “Stable and practical {AS} relationship inference with
problink,” in 16th {USENIX} Symposium on Networked Systems
Design and Implementation ({NSDI} 19), 2019, pp. 581–598.

[12] Z. Jin, X. Shi, Y. Yang, X. Yin, Z. Wang, and J. Wu, “Toposcope:
Recover as relationships from fragmentary observations,” in Pro-
ceedings of the ACM Internet Measurement Conference, 2020, pp. 266–
280.

[13] T. Shapira and Y. Shavitt, “Unveiling the type of relationship be-
tween autonomous systems using deep learning,” in NOMS 2020-
2020 IEEE/IFIP Network Operations and Management Symposium.
IEEE, 2020, pp. 1–6.

[14] G. He, W. Su, S. Gao, J. Yue, and S. K. Das, “Roachain: Securing
route origin authorization with blockchain for inter-domain rout-
ing,” IEEE Transactions on Network and Service Management, 2020.

[15] D. Chen, Y. Ba, H. Qiu, J. Zhu, and Q. Wang, “Isrchain: Achieving
efﬁcient interdomain secure routing with blockchain,” Computers
& Electrical Engineering, vol. 83, p. 106584, 2020.

[16] K. Sriram, D. Montgomery, B. Dickson, K. Patel,

and
A. Robachevsky, “Methods for detection and mitigation of bgp
route leaks,” draft-ietf-idr-route-leak-detection-mitigation-06, 2017.
[17] S. Abd El Monem, A. Khalafallah, and S. I. Shaheen, “Bgp route
leaks detection using supervised machine learning technique,” in
2020 2nd Novel Intelligent and Leading Emerging Sciences Conference
(NILES).

IEEE, 2020, pp. 15–20.

[18] D. Hou, J. Zhang, K. L. Man, J. Ma, and Z. Peng, “A systematic
literature review of blockchain-based federated learning: Architec-
tures, applications and issues,” in 2021 2nd Information Communi-
cation Technologies Conference (ICTC).

IEEE, 2021, pp. 302–307.

[19] M. Shayan, C. Fung, C. J. Yoon, and I. Beschastnikh, “Biscotti:
A blockchain system for private and secure federated learning,”
IEEE Transactions on Parallel and Distributed Systems, vol. 32, no. 7,
pp. 1513–1525, 2020.

[20] U. Majeed and C. S. Hong, “Flchain: Federated learning via mec-
enabled blockchain network,” in 2019 20th Asia-Paciﬁc Network
Operations and Management Symposium (APNOMS).
IEEE, 2019,
pp. 1–4.

[21] Y. Lu, X. Huang, Y. Dai, S. Maharjan, and Y. Zhang, “Blockchain
and federated learning for privacy-preserved data sharing in
industrial iot,” IEEE Transactions on Industrial Informatics, vol. 16,
no. 6, pp. 4177–4186, 2019.

[22] M. Lepinski and S. Kent, “Rfc 6480: an infrastructure to support
secure internet routing,” Internet Engineering Task Force (IETF),
2012.

[23] T. McDaniel, J. M. Smith, and M. Schuchard, “Flexsealing bgp
against route leaks: peerlock active measurement and analysis,”
arXiv preprint arXiv:2006.06576, 2020.

[24] J. Jia, Z.-w. YAN, G.-g. GENG, and J. Jian, “Study on bgp route
leak,” Chinese Journal of Network and Information Security, vol. 2,
no. 8, pp. 54–61, 2016.

[25] M. Siddiqui, D. Montero, R. Serral-Graci`a, and M. Yannuzzi, “Self-
reliant detection of route leaks in inter-domain routing,” Computer
Networks, vol. 82, pp. 135–155, 2015.

[26] A. Azimov, E. Bogomazov, R. Bush, K. Patel, and J. Snijders,
“Veriﬁcation of as path using the resource certiﬁcate public key
infrastructure and autonomous system provider authorization.”
2018.

[27] J. Yue, Y. Qin, S. Gao, W. Su, G. He, and N. Liu, “A privacy-
preserving route leak protection mechanism based on blockchain,”
in 2021 IEEE International Conference on Information Communication
and Software Engineering (ICICSE).

IEEE, 2021, pp. 264–269.

[28] M. Sabt, M. Achemlal, and A. Bouabdallah, “Trusted execution
environment: what it is, and what it is not,” in 2015 IEEE Trust-
com/BigDataSE/ISPA, vol. 1.

IEEE, 2015, pp. 57–64.

[29] Y. Xiang, X. Shi, J. Wu, Z. Wang, and X. Yin, “Sign what you really
care about–secure bgp as-paths efﬁciently,” Computer Networks,
vol. 57, no. 10, pp. 2250–2265, 2013.

[30] Z. Zheng, S. Xie, H.-N. Dai, X. Chen, and H. Wang, “Blockchain
challenges and opportunities: A survey,” International Journal of
Web and Grid Services, vol. 14, no. 4, pp. 352–375, 2018.

[31] Y. Li, C. Chen, N. Liu, H. Huang, Z. Zheng, and Q. Yan,
“A blockchain-based decentralized federated learning framework
with committee consensus,” IEEE Network, vol. 35, no. 1, pp. 234–
241, 2020.

[32] C. Korkmaz, H. E. Kocas, A. Uysal, A. Masry, O. Ozkasap, and
B. Akgun, “Chain ﬂ: Decentralized federated machine learning via

