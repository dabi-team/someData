2
2
0
2

y
a
M
7
1

]
T
G
.
s
c
[

1
v
7
0
4
8
0
.
5
0
2
2
:
v
i
X
r
a

Decentralised Update Selection with Semi-Strategic Experts∗

Georgios Amanatidis1, Georgios Birmpas2, Philip Lazos4, and Francisco
Marmolejo-Cossío3,4

1Department of Mathematical Sciences; University of Essex
georgios.amanatidis@essex.ac.uk
2Department of Computer, Control and Management Engineering; Sapienza University of Rome
birbas@diag.uniroma1.it
3School of Engineering and Applied Sciences; Harvard University
fjmarmol@seas.harvard.edu
4IOHK
{philip.lazos, francisco.marmolejo}@iohk.io

May 18, 2022

Abstract

Motivated by governance models adopted in blockchain applications, we study the problem
of selecting appropriate system updates in a decentralised way. Contrary to most existing
voting approaches, we use the input of a set of motivated experts of varying levels of expertise.
In particular, we develop an approval voting inspired selection mechanism through which the
experts approve or disapprove the diﬀerent updates according to their perception of the quality
of each alternative. Given their opinions, and weighted by their expertise level, a single update
is then implemented and evaluated, and the experts receive rewards based on their choices.
We show that this mechanism always has approximate pure Nash equilibria and that these
achieve a constant factor approximation with respect to the quality benchmark of the optimal
alternative. Finally, we study the repeated version of the problem, where the weights of the
experts are adjusted after each update, according to their performance. Under mild assumptions
about the weights, the extension of our mechanism still has approximate pure Nash equilibria
in this setting.

1

Introduction

In 2009, Satoshi Nakamoto published a landmark whitepaper outlining the core functionality of
Bitcoin [20], a decentralized blockchain-based ledger of transactions for a peer-to-peer digital cur-
rency. Indeed, a key feature of Bitcoin is precisely in its decentralized nature, whereby no single
entity controls the operation of the system, a feat which is achieved by an innovative amalgamation
of cryptography and carefully aligned incentives amongst participants in the protocol. In recent

∗ This work was supported by the ERC Advanced Grant 788893 AMDROMA “Algorithmic and Mechanism Design
Research in Online Markets”, the MIUR PRIN project ALGADIMAR “Algorithms, Games, and Digital Markets”,
and the NWO Veni project No. VI.Veni.192.153.

1

 
 
 
 
 
 
years the ecosystem for similar decentralized systems has grown drastically, and given that for each
of these systems no single entity holds control, users often ﬁnd themselves in a position where they
need to reach a consensus on critical decisions regarding the very platform they participate in.

A fundamental example of this governance dilemma is that of creating and implementing soft-
ware updates for the underlying infrastructure of a blockchain-based solution [6]. Such drastic
upgrades are known as hard forks in blockchain-based systems, and historically, there have been
scenarios where cryptocurrencies have split due to opinion diﬀerences regarding the infrastructure
of the blockchain (e.g., Ethereum vs. Ethereum Classic or Bitcoin vs. Bitcoin Cash). Beyond the
confusion and inconvenience for the users caused by such splits, these fragmentations have very
real implications for the security of blockchain-based systems, which is often strongly dependant
on the number of users within a system.

Software updates, as opposed to general collective decisions, are particularly interesting due
to two salient features of the problem structure: 1) adequately evaluating the relative merits of
software update proposals often requires a high degree of expertise 2) the overall stakes of the
software update process are incredibly high. Indeed, if a proposal which is collectively chosen for
uptake happens to have a fatal bug which has failed to be caught, its uptake can have catastrophic
implications for the underlying system.

In this work, we focus on providing a simple on-chain methodology whereby users from a
blockchain-based platform can collectively decide which software updates to implement, thereby
reducing the potential for aforementioned hard fork frictions. To do so, we assume the existence of
a set of users with diﬀering levels of expertise regarding software updates. These experts are then
faced with proposals for multiple potential software updates, where each expert not only formulates
independent opinions on whether a given proposal will succeed or not during implementation, but
may also harbor additional incentives for implementing one proposal over another. Each expert
casts votes in approval or disapproval of each proposal, and as a function of these votes and the
historical merit of each expert, a proposal is chosen for implementation. Ultimately, we assume the
success or failure of the proposal is recorded on-chain, and as a function of each expert’s votes and
the outcome of the proposal, the system pays experts for their participation.

1.1 Our Contributions

Our main contributions can be summarized as follows:

• In Section 2 we introduce our framework for encoding expert preferences amongst software
proposals, the notion of semi-strategic decision making amongst experts, and a benchmark
for proposal quality which we subsequently use to measure the performance of equilibria in
the semi-strategic voting setting.

• In Section 3, we present our approval voting inspired mechanism. We provide some suﬃcient
conditions that limit potentially damaging deviations and show existence and Price of Anarchy
(PoA) results for pure Nash equilibria. In Sections 3.1 and 3.2, we study approximate pure
Nash equilibria for semi-strategic experts and show that the PoA in this setting is exactly 2.

• Finally, in Section 4 we consider a repeated game setting that dynamically reﬂects expert
performance via weight updates in rounds of approval voting (a measure of “reputation”).
In this setting we show honest voting is an approximate pure Nash equilibrium and that
repeated voting has a Price of Stability of at most 2.

2

1.2 Related Work

Due to the fact that our work focuses on creating payment mechanisms whereby a blockchain
platform can elicit truthful expert opinions regarding potential software updates, it bears many
similarities to existing literature in the realm of scoring rules and preference elicitation [5, 15, 17, 24].
In the context of software updates, implementing a classic scoring rule for one proposal would involve
asking experts to report a probability p ∈ [0, 1] corresponding to their belief that a proposal would
succeed or fail. As a function of this reported probability and the outcome of the proposal, the
scoring rule pays experts in such a way that they always maximize their expected payment when
reporting truthfully. The works of Chen et al. [9], Chen and Kash [8], and Othman and Sandholm
[22], extend the scoring rule framework to incorporate decision-making between mutually exclusive
alternatives, as is the case with our proposal setting. Indeed only one proposal is chosen, against
which we must score expert performance.

Our work however also considers the very real possibility (especially in the semi-anonymous
and permissionless world of blockchain platforms) that expert incentives go beyond the payment
which the mechanism oﬀers them for providing their opinions, and that they instead exhibit distinct
utilities for the ultimate implementation of diﬀerent proposals. This assumption is similar to the
setting of Proper Decision Scoring Rules, as explored in the work of Oesterheld and Conitzer [21],
as well as Restricted Investment Contracts presented in the paper of Carroll [7], where potential
solutions involve experts earning rewards proportional to the principal’s earnings for the alternative
chosen and they each have potentially conﬂicting beliefs over the quality of the various proposals.
The mechanism we propose for eliciting expert beliefs builds oﬀ of a rich existing literature
[19] on Approval Voting (AV) ﬁrst introduced by Brams and Fishburn [2], and then extended in
[10–12]. As in AV, experts voting consists of providing a subset of proposals which they approve
(and, thus, a subset which they disapprove). Unlike typical AV, the underlying utility that experts
derive from the outcome of the mechanism is tied to their beliefs regarding the success of each
proposal. Furthermore, as mentioned in the work of Laslier and Sanver [18], a common technique
in AV involves restricting voter actions in natural ways (i.e., admissible and sincere voting proﬁles
from [12] for example) to restrict the set of equilibria considered in the mathematical analysis of
the model. Our work introduces a novel constraint of similar nature which we dub semi-strategic
voting (see Section 2). For semi-strategic experts we demonstrate theoretical guarantees on our
mechanism’s equilibria resulting from AV. Finally, it is worth mentioning that AV has additionally
seen much empirical work pointing to its practical success [3, 4, 23].

2 Preliminaries

Let N = {1, 2, . . . , n} be a set of experts (typically indexed by i) and U = {1, 2, . . . , k} a set of
proposed updates (typically indexed by j). Every update j has an associated quality qj ∈ {0, 1}
indicating whether it is beneﬁcial for the system; we call updates with qj = 1 “good” and those with
qj = 0 “bad”. These qj values are not initially known: only the value of the selected proposal is
revealed after the experts’ vote has concluded. Speciﬁcally, each expert has her own prior about the
qj’s. Expert i believes that qj has a probability pij of being good. This ‘opinion’ pij is unaﬀected
by what the other experts think or how they voted: no matter how some update j was selected,
expert i believes that it will be good with probability pij. Every expert i has a weight wi ≥ 0
indicating their “power” within the system. Additionally, each expert may have some (external)
personal gain, depending on the outcome of the vote. We denote this external reward that expert

3

i will receive if update j is implemented by gij; this value is known to expert i, but not to the
mechanism.

The strategy of each expert is a vector ri = (ri1, . . . , rik)⊺ ∈ {0, 1}k×1, where rij indicates the
binary vote of expert i on whether qj = 1 or not. Let r = (r1, r2, . . . , rn) ∈ {0, 1}k×n denote the
whole voting proﬁle. As is common, we write (r′
i, ri+1, . . . , rn), as
well as (r′

i, r−i) to denote (r1, . . . , ri−1, r′

ij, r−ij) to denote ((ri1, . . . , ri(j−1), r′

ij, ri(j+1), . . . , rik)⊺, r−i).

A mechanism M = (x, f ) consists of a (possibly randomised) selection rule x, which given r
and w = (w1, . . . , wn) returns x(r, w) ∈ U ∪ {0} (i.e., x could return the dummy proposal 0, if no
proposal is selected), and an expert reward function f , which given r, w, a winning proposal j⋆,
and its quality qj⋆ returns f (r, w, j⋆, qj⋆) = (f1(r, w, j⋆, qj⋆), . . . , fn(r, w, j⋆, qj⋆)) ∈ R1×n.

Given M, the expected reward of expert i conditioned on i’s own perspective p = (pi1, . . . , pin)
uses an estimate of fi(r, w, j⋆, qj⋆) which depends on the knowledge of r, w, and pij⋆, i.e., the
probability that j⋆ is a good proposal according to i only:

E [fi(r, w, j⋆, qj⋆) | p] = pij⋆fi(r, w, j⋆, 1) + (1 − pij⋆)fi(r, w, j⋆, 0) .

Thus, the expected utility of i conditioned on her perspective is

uM
i (r | w, p) = E [pij⋆ · gij⋆ + E [fi(r, w, j⋆, qj⋆) | p]] ,

(1)

where the outer expectation is over the winning proposal j⋆ = x(r, w). We adopt this approach for
the utilities because when an expert declares her preference, we assume she is agnostic about the
beliefs of other experts for any proposal.

Given the subjective evaluation of the quality of each proposal, we need a way to aggregate the
opinions of all experts that combines robustness and explainability. To this end, we introduce a
probability threshold T ∈ [0, 1] such that if expert i has pij ≥ T for proposal j then we consider i’s
honest response to be to vote in favour of j; otherwise i’s honest response is to vote against j.

Our metric can be viewed as the weighted average of the probabilities after these have been
rounded to 0 or 1 with respect to the threshold T , and it has an immediate meaning which is the
voting power that considers a proposal to be good enough. Note that using the raw probabilities
pij to deﬁne some measure of quality is a bit problematic:
it is not reasonable to expect that
experts would precisely and consistently report those, and possibly it would be considerably harder
to communicate the resulting notion of “quality” to non-experts in the system.

Deﬁnition 2.1 (Estimated Quality). Given a probability threshold T , the estimated quality of
proposal i is the sum of weights of experts i with pij ≥ T . That is:

Xi : pij≥T
For convenience, we will refer to the optimal quality as:

Qual[j] =

wi.

OPT(w, p) = argmax

Qual[j].

j

(2)

(3)

We consider experts that are strategic and strive to maximize their utility. However, we also
assume they are not malicious towards the system. That is, they only choose to lie when this results
in a net increase in their utility. If there is no strictly beneﬁcial deviation, they remain honest. We
call such experts semi-strategic.

4

Deﬁnition 2.2 (Semi-strategic Experts). An expert i is semi-strategic if for every mechanism M
and strategy vector r:

• If pij ≥ T and rij = 0, then

• If pij < T and rij = 1, then

i (1, r−ij | w, p) < uM
uM

i (r | w, p) .

uM
i (0, r−ij | w, p) < uM

i (r | w, p) .

The solution concept we use is the (multiplicatively) approximate pure Nash equilibrium. We
use the multiplicative, rather than the additive, version of approximate pure Nash equilibria as we
want our results to be mostly independent of scaling up or down the reward functions.1

Deﬁnition 2.3 ((1+ε)-Pure Nash Equilibrium). For ε ≥ 0, a strategy proﬁle r is a multiplicatively
(1 + ε)-approximate pure Nash equilibrium, or simply a (1 + ε)-PNE, for weight vector w, if for
every deviation r′

i we have:

(1 + ε) · uM

i (r | w, p) ≥ uM

i (r′

i, r−i | w, p) .

(4)

When ε = 0 we simply call r a pure Nash equilibrium (PNE).

We refer to the set strategies that are (1 + ε)-PNE of mechanism M given w and p as QM

ε (w, p).

To measure the ineﬃciency of diﬀerent equilibria compared to the proposal of highest quality,
we use the notions Price of Anarchy [16] and Price of Stability [1], which denote the ratios between
the quality of the worst or the best possible equilibrium produced by M = (x, f ) and the optimal
outcome, respectively. In particular, these are formally deﬁned as:

and

PoA(M) = sup
w,p

OPT(w, p)
ε (w,p) Qual[x(r)]

inf r∈QM

PoS(M) = sup
w,p

OPT(w, p)
ε (w,p) Qual[x(r)]

supr∈QM

.

3 Approval Voting

Although our deﬁnitions allow for randomized mechanisms, as a ﬁrst attempt of the problem we
focus on a natural deterministic mechanism. In particular, we study the mechanism induced by
approval voting, which we call MAV with an appropriately selected reward function f . Speciﬁcally,
the proposal with the highest amount of weighted approval is the winner, i.e.,

x(r, w) ∈ argmax

j

Xi : rij =1

wi .

(5)

1This is completely precise in the case where the external rewards gij are all 0, but it is still largely true whenever

the rewards of the mechanism are large compared to external rewards.

5

Ties can be broken arbitrarily, but in a deterministic manner, e.g., lexicographically. Hence, we
might abuse the notation and use ‘=’ with ‘argmax’.
It should be noted that while such naive
tie-breaking rules are standard in theoretical work, in practice we expect to have a large number
of experts at play with diﬀerent and dynamically adjusting weights, hence a tie is very improbable
anyway. Additionally, the reward given to each expert i is proportional to her weight:

fi(r, w, j⋆, qj⋆) = wi ·

a,
−s,
a′,
0,

if rij⋆ = 1 and qj⋆ = 1
if rij⋆ = 1 and qj⋆ = 0
if rij⋆ = 0 and qj⋆ = 0
if rij⋆ = 0 and qj⋆ = 1

.

(6)






That is, a is the reward in case the expert approved the winning proposal and it turned out to
have high quality, a′ is the reward in case the expert disapproved the winning proposal that turned
out bad, s is the penalty in case the expert approved the winning proposal and it turned out to be
bad and we assume there is no reward or penalty if the expert disapproved the winning proposal
that turned out good. Notice that the collected reward depends on the winning proposal j⋆ and
it’s quality. The other proposals are not implemented and their true nature is never revealed.

Remark. In the following analysis we will drop the wi multiplier. Indeed, all rewards are equally
scaled, except for the gij that do not depend on the weights. As such, to simplify notation (and
without loss of generality) we consider that external rewards are scaled down appropriately by wi
for each expert i.

To ﬁnd out the possible pure Nash equilibria of this scheme, we start by the simplest case for
MAV. Suppose everyone has already cast a vote and expert i has no way of changing the outcome
(which will usually be the most likely scenario). We need to check when the expected utility of
approving is higher than that of disapproving:

a · pij⋆ − s · (1 − pij⋆) + gij⋆ · pij⋆ ≥ a′ · (1 − pij⋆) + gij⋆ · pij⋆

⇒ a · pij⋆ − s + s · pij⋆ ≥ a′ − a′ · pij⋆
⇒ pij⋆ · (a + s + a′) ≥ a′ + s
a′ + s
a′ + s + a

⇒ pij⋆ ≥

.

So, the a, a′ and s parameters can be tuned so that approving the winning proposal is the best
option only for a conﬁdence equal or higher than a desired threshold, which we deﬁne as

T =

a′ + s
a′ + s + a

.

(7)

This threshold T is used for measuring quality and allows us to deﬁne the ‘honest strategy’ for this
voting scheme.

Deﬁnition 3.1. Expert i plays her honest strategy if she approves the proposals for which her
conﬁdence is greater that T and only those, i.e., if rij = 1 ⇔ pij ≥ T .

So far, we have shown that if every expert plays their honest strategy and it happens that no
single expert has the power to change the outcome, then this honest strategy proﬁle is a pure Nash

6

equilibrium. The next result gives some insights about the possible deviations from the honest
strategy, given the external rewards gij. Note that Theorem 3.2 does not ‘protect’ MAV against
all possible deviations, but only those where an expert votes for a proposal she considers bad
(amongst other things) to make it win. In the remaining possible deviations an expert determines
the winning proposal, not by changing her vote for it, but by disapproving a proposal she considers
good. As we shall see in Theorem 3.8, such deviations do not hurt the overall quality signiﬁcantly
for semi-strategic experts. The intuition is that when the winning proposal is determined like this
it necessarily has one of the highest number of honest votes and, thus, suﬃciently high quality.

Theorem 3.2. For any player i and voting proﬁle r−i, let j⋆ be the output of MAV if expert i
votes honestly and let j′ be any proposal that expert i voted against. Then, if

pij′ < min

a + s
a + s + gij′

,

T ·

(

a′ · (1 − T ) + a

a + s + gij′ )

,

(8)

then expert i cannot increase her payoﬀ by switching her vote in favour of proposal j′ (and possibly
also switching against j⋆). Moreover, for any choice of parameters, there are instances where an
expert i can increase her payoﬀ by switching her vote against proposal j⋆.

Proof. Suppose that for some outcome r the selected proposal is j⋆, but expert i could switch her
vote to change the winner to another proposal j′ (either by not approving j⋆ or by approving j′,
and possibly approving / disapproving other proposals as well). For each possible deviation, we
consider the expected utility of expert i and show under which conditions switching the winner to
j′ would be a better response, given the gij′ and pij′.

• For the ﬁrst group of cases, we assume that expert i considers the winning proposal j⋆ good
enough (i.e., pij⋆ ≥ T ). Therefore, we need to compare the utility of any deviation that makes
j′ the winner, to the utility obtained by voting ‘yes’ to j⋆. This is because voting ‘no’ for
j⋆ is is clearly not a best response unless the winner changes, since pij⋆ ≥ T implies that
a · pij⋆ − (1 − pij⋆) · s > a′ · (1 − pij⋆).

– Switches to approve j′, keeps approving j⋆: This deviation can only happen if the

utility for switching is greater than that of voting honestly for j⋆.

a · pij′ − s · (1 − pij′) + gij′ · pij′ > a · pij⋆ − s · (1 − pij⋆) + gij⋆ · pij⋆

⇒ a · pij′ − s · (1 − pij′) + gij′ · pij′ > a · pij⋆ − s · (1 − pij⋆)
⇒ a · pij′ − s · (1 − pij′) + gij′ · pij′ > a · T − s · (1 − T )
⇒ a · pij′ − s + s · pij′ + gij′ · pij′ > a · T − s + s · T
⇒ pij′ · (a + s + gij′) > T · (a + s)

⇒ pij′ > T ·

a + s
a + s + gij′

,

using pij⋆ > T in the ﬁrst implication.

– Switches to approve j′, switches to disapprove j⋆: The incentives here are identical
to the ﬁrst case: in the honest outcome the reward is at least a · T − s · (1 − T ) and in
the deviation it’s a · pij′ − s · (1 − pij′) + gij′ · pij′.

7

– Keeps approving j′, switches to disapprove j⋆: The incentives are identical to the
ﬁrst case, but since expert i wanted to approve j′ (i.e., pij′ ≥ T ), no matter how high
a, a′ and s are set she could better oﬀ disapproving j⋆ in this scenario, if pij⋆ < pij.

• For the last two cases, we consider that pij⋆ < T . As before, any deviation that does not
change the winner to something other than j⋆, needs contain a ‘no’ vote for j⋆: since pij⋆ < T
implies that a′ · pij⋆ > a · pij⋆ − (1 − s) · pij⋆.

– Switches to approve j′, keeps disapproving j⋆: Since the honest move is to disap-
prove j⋆, we have that pij⋆ ≤ T , therefore the ‘honest’ payoﬀ is at least a′ · (1 − T ).

a · pij′ − s · (1 − pij′) + gij′ · pij′ > a′ · (1 − pij⋆) + gij⋆ · pij⋆

⇒ a · pij′ − s · (1 − pij′) + gij′ · pij′ > a′ · (1 − T )
⇒ a · pij′ − s + s · pij′ + gij′ · pij′ > a′ · (1 − T )
⇒ pij′ · (a + s + gij′) > a′ · (1 − T ) + s

⇒ pij′ >

a′ · (1 − T ) + s
a + s + gij′

– Keeps disapproving j′, switches to disapprove j⋆: The rewards in the honest
outcome are the same as in the ﬁrst case, but the reward for deviating is diﬀerent:

a′ · (1 − pij′) + gij′ · pij′ > a · pij⋆ − s · (1 − pij⋆) + gij⋆ · pij⋆

⇒ a′ · (1 − pij′) + gij′ · pij′ > a · T − s · (1 − T )
⇒ a′ − a′ · pij′ + gij′ · pij′ > a · T − s + s · T
⇒ pij′ · (gij′ − a′) > a · T − a′ + s · T − s

If gij′ > a′, then we need

Otherwise, we have that:

pij′ >

a · T − a′ + s · T − s
gij′ − a′

.

pij′ <

a · T − a′ + s · T − s
gij′ − a′

.

However, since T > 0 and the ‘no’ branch of the reward function is decreasing in pij⋆ we
have that a · T − (1 − T ) · s < a′. Therefore, if gij′ > a′, there is no way to set the other
parameters and completely eliminate the possibility of deviating to disapproving j⋆.

Putting everything together, this mechanism can only protect from situations where the expert
needs to actively switch her vote to approve a proposal she knows is not good enough. Following
the previous cases, a necessary condition for this to happen is either:

or

pij′ > T ·

a + s
a + s + gij′

pij′ >

a′ · (1 − T ) + s
a + s + gij′

.

If pij′ is smaller than both, then there is no possibility of such a deviation and the claim holds.

8

Setting gij = 0 yields the following corollary, showing that experts without any external rewards

will never approve a perceived bad proposal.
Corollary 3.3. For any expert i, it is a dominant strategy to vote against any j′ such that gij′ = 0
and pij′ < T .

Proof. If gij′ = 0 (i.e., the expert in question has no external motivations), the previous probabilities
take on interesting values. Speciﬁcally, there is no deviation for:

pij′ < T ·

a + s
a + s

= T

and

pij′ <

a′ · (1 − T ) + s
a + s

· T =

a · T − (1 − T ) · s + s
a + s

· T < T .

The previous two results show that, assuming the a, a′ and s are all large enough, the experts
will be reluctant to vote in favour of a proposal they already know is bad: to do so, they still
need to have some faith in that proposal. It turns out that this mechanism always has pure Nash
equilibria, albeit with limited guarantees, as shown in the next two propositions.

Proposition 3.4. In the presence of strategic experts, the approval voting mechanism MAV always
has a PNE.

Proof. For i ∈ N and j ∈ U , let rij be the voting proﬁle where

• rij = 1

• ri′j′ = 0 for any i′ and j′ 6= j.

If no expert i has positive utility for some proposal j with respect to the voting proﬁle rij, then
clearly everyone voting ‘no’ to every proposal is a pure Nash equilibrium.

Assuming that the set N+ = {i ∈ N | ∃j ∈ U : uMAV

(rij | w, p) > 0} is nonempty, let
i⋆ = argmaxi∈N+ wi. Additionally, let j⋆ = argmaxj∈U uMAV
(ri⋆j | w, p). It is not hard to see that
i⋆
the proﬁle ri⋆j⋆
is a pure Nash equilibrium. Since the weight of every expert i 6= i⋆ is wi < wi⋆ (or
wi = wi⋆ but i is losing to i⋆ in the tie-breaking), there exists no possible deviation from i that
changes the winning proposal. In addition, because j⋆ maximizes the utility of expert i⋆, this is a
pure Nash equilibrium.

i

Proposition 3.5. The Price of Anarchy of MAV is Ω(n), even if for all experts i and all proposals
j we have that gij = 0.

Proof. Suppose that there are n + 1 experts and 2 proposals. For arbitrarily small ε > 0, we set:

• For expert 1: w1 = 1/n + ε, p11 = 1 and p12 = 0.

• For any expert i > 1: wi = 1/n, pi1 = 0 and pi2 = 1.

Following the construction of Proposition 3.4, there is a pure Nash equilibrium where expert 1
approves the ﬁrst proposal and every other expert votes ‘no’ for all proposals. The quality of
proposal 1 is 1/n + ε, while the quality of proposal 2 is n · 1/n = 1, leading to the claimed
result.

9

This equilibrium of Proposition 3.5, however, is unnatural: why would so many experts vote
against their favourite proposal? The intuition is that the assumption about the agents being semi-
strategic instead, should help us avoid such pitfalls. Unfortunately, if we assume the presence of
semi-strategic agents, there are combinations of pij’s for which no PNE exists.

Proposition 3.6. The mechanism MAV does not always have PNE for semi-strategic experts, even
when gij = 0 for all experts i and proposals j.

Proof. Suppose that we have 3 experts and 2 proposals, T = 0.9 (with a, a′ and s set appropriately),
and

• expert 1 has w1 = 0.49, p11 = 0.95, p12 = 1;

• expert 2 has w2 = 0.41, p21 = 1, p22 = 0.95;

• expert 3 has w3 = 0.1, p31 = 1, p32 = 0.

Clearly, expert 3 would always vote for proposal 1 only. The remaining experts would honestly
approve both proposals, but expert 1 has higher expected reward if proposal 2 wins, while expert
2 believes proposal 1 maximizes her utility. We present the following cycle of deviations. At ﬁrst
they all vote honestly. Then expert 1 says ‘no’ to the ﬁrst proposal, making proposal 2 the winner
and improving her utility. In response, expert 2 says rejects the second proposal, leading proposal 1
to reclaim the win. Then, expert ‘1’ has to switch her vote in favour of proposal 1 because they are
semi-strategic. Finally, expert 2 says ‘yes’ to the second proposal because they are semi-strategic
too.

Therefore, at any conﬁguration of votes, some expert wants to deviate and MAV has no PNE

for this instance.

Despite Proposition 3.6, our mechanism does have approximate PNE’s for semi-strategic experts
and an appropriate choice of parameters, as we show next. Moreover, these approximate equilibria
always lead to choosing approximately optimal proposals.

3.1 Approximate Equilibria of MAV

Since pure Nash equilibria may not always exist when dealing with semi-strategic experts, we have
to fall back to showing the existence of approximate PNE’s. This can be achieved by careful tuning
of the parameters a, a′ and s when deﬁning the reward function. Recall that a, a′, s and T are
related via Equation (7); an equivalent equation appears in the proof of the theorem below as (9).

Theorem 3.7. Suppose that for T ∈ [0, 1], we set ε ≥ 0 such that:

• 1/(ε + 1) < T

• a = (1 + ε) · a′ · (1 − T ) > a′

• s = a·(T ·(ε+1)−1)
(1−T )·(ε+1)

In addition, suppose that for every player i and proposal j we have that gij ≤ a · δ. Then the voting
proﬁle r where everyone votes honestly is (1 + ε) · (1 + δ)-approximate pure Nash equilibrium.

10

Proof. As always, the honest behaviour of expert i should be to approve proposal j if and only if
they have pij ≥ T . From the perspective of expert i, their reward for voting in favour of proposal
j is:

pij · a − (1 − pij) · s.

Notice that this expression is strictly increasing in pij. Voting against proposal j yields an expected
reward equal to a′·(1−pij), which is strictly decreasing in pij. As before, to ensure honest behaviour,
the two expressions need to be equal for pij = T :

T · a − (1 − T ) · s = a′ · (1 − T ) .

(9)

Additionally, we need that a, which is the payoﬀ for pij = 1 is also the maximum possible reward
and satisﬁes a = (1 + ε) · (a · T − (1 − T ) · s). Since T ∈ [0, 1], Equation (9) is actually the global
minimum of the honest response reward. Therefore, the condition that a is the maximum can be
replaced by a ≥ a′, since either a or a′ are the extreme points. Since T and ε are given, we can
solve for the remaining values using a′ ≥ 0 as a the free parameter. Moreover, these solutions are
non-negative for 1/(ε + 1) < T < 1.

Expected Reward

a
a′

a · T − (1 − T ) · s

0

pij = T

1

pij

Figure 1: The expected reward (as a function of pij) for honest voting, assuming gij = 0 and
proposal j won. The red line is a′ · (1 − pij), corresponding to voting against j, while the green line
is a · pij − (1 − pij) · s for voting in favour of it.

We are now ready to show that r, the honest voting proﬁle, is an approximate PNE. Let j⋆ be
the winning proposal. Clearly, any expert who cannot change the outcome is playing their best
i and changes the winner to j′. There
response. Suppose that expert i has a beneﬁcial deviation r′
are two cases:

• The expert i is honest about j′ (but possibly changed his vote on some other proposals): this
means that either pij′ ≥ T and rij′ = 1 or pij′ < T and rij′ = 0. In this case, the maximum
possible reward she could get is a+gij′ ≤ (1+δ)·a. On the other hand, the minimum possible
reward for an honest vote is a′ · (1 − T ) = a/(1 + ε). Therefore, this deviation can yield at
most (1 + ε) · (1 + δ) times the reward of the honest response.

11

• The expert i is dishonest about j′: in this case, the reward (without gij) is at most a′ · (1 − T ),
which is the minimum possible reward for honest voting. As with the previous case, the
addition of gij′ is not great enough to motivate the expert to deviate.

Therefore, the honest proﬁle is an (1 + ε)-pure Nash equilibrium.

Note that the existence of the approximate PNE of Theorem 3.7 is not guaranteed for an
arbitrarily small ε. So, it is natural to ask how ineﬃcient these equilibria are, with respect to
achieving our objective of maximizing Qual. We deal with this question in the following section.

3.2 Price of Anarchy of MAV

Here we study the Price of Anarchy of the approximate pure Nash equilibria of MAV. That is, we
bound the quality of a proposal returned by the mechanism in an approximate equilibrium in terms
of the best possible estimated quality. Surprisingly, we show that for any ε, (1 + ε)-approximate
PNE result in quality which is within a factor of 2 of the optimal estimated quality. Note that
although the statement of Theorem 3.8 does not mention the gij’s explicitly, these are taken into
consideration via the conditions of Theorem 3.2. Moreover, this bound on the Price of Anarchy is
tight.

Theorem 3.8. Suppose that a, a′ and s are chosen such that:

• a = (1 + ε) · (1 − T ) · a′.

• (1 − T ) · a′ = T · a − (1 − T ) · s.

In addition, the pij < T of every expert satisfy the conditions of Theorem 3.2. Then, the Price of
Anarchy of MAV over (1 + ε)-approximate pure Nash Equilibria is at most 2.

Proof. Let r be a (1 + ε) approximate PNE whose winner is j. Further, let j⋆ be the proposal with
highest quality and suppose that:

Each expert i belongs to one of the following categories:

Qual[j] <

1
2

· Qual[j⋆].

• Case 1: pij < T and pij⋆ < T : In this case, the expert has to disapprove both proposals
at the equilibrium r. By Theorem 3.2, expert i would gain no beneﬁt by voting in favour of
either j or j⋆.

• Either pij ≥ T or pij⋆ ≥ T :

– Case 2a: In the ﬁrst case, if expert i submits a ‘no’ vote for j and it remains the winner,
this ‘no’ vote also clearly reduces the reward of expert i compared to a ‘yes’ vote. Since
expert i is semi-strategic, they have to vote in favour of j.

– Case 2b: In the second case, by Theorem 3.2 they cannot approve j. Since r is an
equilibrium where j wins and they are semi-strategic, voting ‘no’ for j does not strictly
increase their reward. Therefore expert i votes only in favour of j⋆.

12

• Case 3: pij ≥ T and pij⋆ ≥ T : Similarly to Case 2, the expert has to approve j. However,
not every expert needs to vote in favour of j⋆. They only do so if permitted by the equilibrium
condition (i.e., if the winner stays j).

We partition the experts into sets C1, C2a, C2b and C3 respectively, indexed according to the afore-
mentioned cases. In addition, let ρ ∈ [0, 1] be the fraction of experts in C3 that voted for j⋆ as well
as j. Clearly, since proposal j is the winner we have that:

wi +

wi ≥

wi + ρ ·

wi ⇒

wi + (1 − ρ) ·

wi ≥

wi.

(10)

Xi∈C2a

Xi∈C2b
In addition, by since j⋆ maximizes the quality objective, we have:

Xi∈C2a

Xi∈C3

Xi∈C3

Xi∈C3

Xi∈C2b

wi +

wi

Combining Equation (10) with Equation (11) we get:

Xi∈C2a

Xi∈C2b

wi <

wi +

Xi∈C3

Xi∈C3





·

1
2

⇒ 2 ·

wi +

wi <

wi.

(11)

Xi∈C2a

Xi∈C3

Xi∈C2b

wi + (1 − ρ) ·

wi >

wi +

wi,

Xi∈C2a

Xi∈C3

Xi∈C2a

Xi∈C3

which is impossible for any ρ ∈ [0, 1], leading to a contradiction.

We complement the previous theorem with a matching lower bound.

Theorem 3.9. The Price of Anarchy of MAV is greater than or equal to 2.

Proof. Consider and instance with 2 experts and 2 proposals, with the following parameters:

• Expert 1 has p11 = T < p12 = 1 and w1 = 1 + ε.

• Expert 2 has p11 = 1, p12 = 0 and w2 = 1 − ε.

All gij are equal to zero.

The optimal outcome is to elect proposal 1, that has quality 2. However, it is a semi-strategic
deviation for expert 1 to vote against proposal 1, since she likes proposal 2 slightly more, even
though both meet the acceptance threshold T . In this case, expert 2 has no way to change the
outcome with her lower weight, leading to an exact PNE with quality 1 + ε.

4 The Repeated Game

In the previous sections we described a system which incentivizes the experts to only vote ‘yes’ for
proposals that they believe have a high chance of being good. While we have deﬁned the reward
of each expert to be proportional to her weight, this does not have any signiﬁcant impact on our
technical results so far. As mentioned in the introduction, however, we want the weight of an expert
to serve as a proxy for that expert’s demonstrated expertise level, capturing her “reputation” in the
system. This, of course, makes sense in a repeated game setting, where the weights are updated after
each round of proposals. We assume that every time we have a fresh set of proposals, independent

13

of any past decisions, but the diﬀerent parameters of the system (threshold T , reward parameters
a, a′ and s, etc.) remain the same and there is a known rule for updating the weights.

An analog of the various Folk Theorems (see, e.g., [13, 14]) would not apply in our setting
with the semi-strategic experts, since the notion of a “threat” used in their proofs cannot be used
anymore. Nevertheless, we show below that if rewards are smoothed out appropriately, then truth-
telling is an approximate pure Nash equilibrium. This, combined with Theorems 3.7 and 3.8 directly
gives us a Price of Stability (which is the ratio between the quality of the best possible equilibrium
and the optimal outcome) of 2 for this repeated game.

Let wt = (wt

1, wt

2, . . . , wt

n) be the weights after round t ≥ 1. In this context, a voting mechanism
will involve two components: a reward function f t(r, wt, j, qj) and a weight update rule wt+1 =
g(wt, r, j, qj ). The reward function will be the same to the single-shot game: f t
i (r, wt, j, qj) =
fi(r, wt, j, qj). For the sake of presentation, we will focus on a simple weight update rule here, so
that the weights converge to the percentage of correct predictions; the same argument, however,
could be made for any update rule.

In principle, we would not like the weights to ﬂuctuate widely from round to round, since then

they would not capture the empirical expertise level as intended. Suppose we deﬁne

ωt

i =

# of correct predictions
t

. Even if we assume that each expert has an inherent expertise level πi so that limt→∞ ωt
i = πi,
these weights can still ﬂuctuate a lot when t is small. Having these weights as a starting point,
however, for a small ζ > 0, we may deﬁne wt as follows:

w0

i = 1/2 ;

wt+1

i =

min{ωt
max{ωt

i, (1 + ζ) · wt
i},
i, (1 − ζ) · wt
i},

(

if t ≥ 1 and wt
if t ≥ 1 and wt

i ≤ ωt
i
i > ωt
i

Using these “delayed updates”, we still have limt→∞ wt
i = πi, but the weights never change more
than 100 · ζ % from round to round. Note that is is not necessary to start with a rule that converges
in any sense.

As usual, we assume that future rewards are discounted by a discount factor γ ∈ (0, 1). That
is, an amount of money x that is expected to be won τ rounds into the future, has value γτ x at
the present moment for any of the experts.
Theorem 4.1. Let ξ ∈ (0, 1). Also let T , ε, δ, a, a′, and s be like in Theorem 3.7, and suppose that
for every player i and any proposal j of any round t, we have gij ≤ wt
i · a · δ. Then the sequence of
voting proﬁles (rt)t∈N where everyone votes honestly in each round t is (1 + 3ε) · (1 + δ)-approximate
pure Nash equilibrium for the Repeated Update Selection game with delayed weight updates and
suﬃciently small discount factor γ.
Proof. Fix any expert i. Let wt
t, the expected reward of i (before the discount) is at least wt
i, rt
rt
sequence of voting proﬁles (
Now, in round t, the expected reward of i (before the discount) is at most (1 + δ) ·
calculating the expected overall rewards R((rt)t∈N) and R((
e

i be the weights of i for t ∈ N if (rt)t∈N is played. Then, in round
i · (1 − T ) · a′. Now, consider any
i be the weights of i if that sequence is played.
i · a. When

−1)t∈N, and let

−1)t∈N), we have

i, rt
rt

wt

wt

e

R((rt)t∈N) ≥

∞

t=0
X

[wt

i · (1 − T ) · a′ · γt]
e

14

e

∞

≥

[w0

i · (1 − ζ)t · (1 − T ) · a′ · γt]

t=0
X
= w0

i · (1 − T ) · a′ ·

=

i · (1 − T ) · a′
w0
1 − (1 − ζ) · γ

,

∞

t=0
X

[(1 − ζ) · γ]t

[(1 + δ) ·

wt

i · a · γt]

e
i · (1 + ζ)t · (1 + ε) · (1 − T ) · a′ · γt]
[(1 + δ) · w0

= (1 + δ) · (1 + ε) · w0

i · (1 − T ) · a′ ·

= (1 + δ) · (1 + ε) ·

i · (1 − T ) · a′
w0
1 − (1 + ζ) · γ

.

[(1 + ζ) · γ]t

∞

t=0
X

as well as

R((

i, rt
rt

−1)t∈N) ≤

e

≤

∞

t=0
X
∞

t=0
X

It is a matter of simple calculation to see that for small enough γ, 1−(1−ζ)·γ

1−(1+ζ)·γ ≤ (1 + ε), and thus,

R((

i, rt
rt

−1)t∈N) ≤ (1 + δ) · (1 + ε)2 · R((rt)t∈N)
≤ (1 + δ) · (1 + 3ε) · R((rt)t∈N) ,

as desired.

e

Given that repeated games introduce a large number of (approximate) pure Nash equilibria,
some of which may be of very low quality, it is not possible to replicate our Price of Anarchy
result from Theorem 3.8 here. However, that result, coupled with the fact that the sequence of
voting proﬁles in Theorem 4.1 consists of approximate pure Nash equilibria of the single-shot game
(Theorem 3.7), directly translate into the following Price of Stability result.

Corollary 4.2. Under the assumptions of Theorems 4.1 and 3.8 (where the original gij is replaced
by gij/wt
i if j is an update of round t), the Repeated Update Selection game with delayed weight
updates and suﬃciently small discount factor γ has Price of Stability at most 2.

5 Conclusions and Open Problems

In this work, we make the ﬁrst step in combining aspects of voting and eliciting truthful beliefs at
once. Typically voting applications do not involve payments dependent on votes and the chosen
outcome, whereas information elicitation (using peer prediction or proper scoring rules) do not
involve the experts aﬀecting the chosen action. Using the notion of semi-strategic experts and a
variant of approval voting, with appropriate rewards, we can prove the existence of approximate
pure Nash equilibria and show that they produce outcomes of good quality. This is only the ﬁrst
step however. The natural follow-up questions would be:

15

• To study randomized mechanisms, which might have better guarantees. For instance, rather
than always selecting the proposal with highest approval, the selection could be randomized
between the top proposals that exceed a threshold.
In this case, experts might be more
cautious about lying, as they have less inﬂuence on the ﬁnal selection. In the deterministic
case they can be sure that their deviation yielded some beneﬁt. But with randomization,
there is always the chance that a diﬀerent proposal is chosen and it might not be dishonest
with too many votes.

• To allow experts a richer strategy space, including reporting their prior (i.e., the pij) directly,

or even estimating how other experts might act.

Either of these approaches could produce a mechanism with improved performance, at a cost of
added complexity and perhaps lower robustness.

6 Acknowledgements

We would like to thank Nikos Karagiannidis for many enlightening meetings, helping us formulate
the model in early versions of this work.

References

[1] Elliot Anshelevich, Anirban Dasgupta, Jon M. Kleinberg, Éva Tardos, Tom Wexler, and Tim
Roughgarden. The price of stability for network design with fair cost allocation. SIAM J.
Comput., 38(4):1602–1623, 2008.

[2] Steven J Brams and Peter C Fishburn. Approval voting. American Political Science Review,

72(3):831–847, 1978.

[3] Steven J Brams and Peter C Fishburn. Going from theory to practice: the mixed success of

approval voting. In Handbook on approval voting, pages 19–37. Springer, 2010.

[4] Steven J Brams and Jack H Nagel. Approval voting in practice. Public Choice, 71(1):1–17,

1991.

[5] Glenn W Brier et al. Veriﬁcation of forecasts expressed in terms of probability. Monthly

weather review, 78(1):1–3, 1950.

[6] Vitalik Buterin. Moving beyond coin voting governance, 2021. Accessed on: October 1, 2021.

Available: https://vitalik.ca/general/2021/08/16/voting3.html.

[7] Gabriel Carroll. Robust incentives for information acquisition. Journal of Economic Theory,

181:382–420, 2019.

[8] Yiling Chen and Ian Kash. Information elicitation for decision making. 2011.

[9] Yiling Chen, Ian A Kash, Michael Ruberry, and Victor Shnayder. Eliciting predictions and
recommendations for decision making. ACM Transactions on Economics and Computation
(TEAC), 2(2):1–27, 2014.

16

[10] Ulle Endriss. Sincerity and manipulation under approval voting. Theory and Decision, 74(3):

335–355, 2013.

[11] Peter C Fishburn. A strategic analysis of nonranked voting systems. SIAM Journal on Applied

Mathematics, 35(3):488–495, 1978.

[12] Peter C Fishburn and Steven J Brams. Approval voting, condorcet’s principle, and runoﬀ

elections. Public Choice, 36(1):89–114, 1981.

[13] James W. Friedman. A non-cooperative equilibrium for supergames. The Review of Economic

Studies, 38(1):1–12, 1971.

[14] Drew Fudenberg and Eric Maskin. The Folk Theorem in repeated games with discounting or

with incomplete information. Econometrica, 54(3):533–554, 1986.

[15] Tilmann Gneiting and Adrian E Raftery. Strictly proper scoring rules, prediction, and esti-

mation. Journal of the American statistical Association, 102(477):359–378, 2007.

[16] Elias Koutsoupias and Christos H. Papadimitriou. Worst-case equilibria. Comput. Sci. Rev.,

3(2):65–69, 2009.

[17] Nicolas Lambert and Yoav Shoham. Eliciting truthful answers to multiple-choice questions.
In Proceedings of the 10th ACM conference on Electronic commerce, pages 109–118, 2009.

[18] Jean-François Laslier and M Remzi Sanver. The basic approval voting game. Handbook on

approval voting, pages 153–163, 2010.

[19] Jean-François Laslier and Remzi Sanver, editors. Handbook on Approval Voting. Springer,

2010.

[20] Satoshi Nakamoto. Bitcoin: A peer-to-peer electronic cash system. Decentralized Business

Review, page 21260, 2008.

[21] Caspar Oesterheld and Vincent Conitzer. Decision scoring rules. In WINE, page 468, 2020.

[22] Abraham Othman and Tuomas Sandholm. Decision rules and decision markets. In AAMAS,

pages 625–632. Citeseer, 2010.

[23] Michel Regenwetter and Bernard Grofman. Approval voting, borda winners, and condorcet

winners: Evidence from seven elections. Management Science, 44(4):520–533, 1998.

[24] Leonard J Savage. Elicitation of personal probabilities and expectations. Journal of the

American Statistical Association, 66(336):783–801, 1971.

17

